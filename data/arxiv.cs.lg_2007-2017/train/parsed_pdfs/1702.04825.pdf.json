{
  "name" : "1702.04825.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Learning to Use Learners’ Advice",
    "authors" : [ "Adish Singla", "Hamed Hassani", "Andreas Krause" ],
    "emails" : [ "ADISH.SINGLA@INF.ETHZ.CH", "HAMED@INF.ETHZ.CH", "KRAUSEA@ETHZ.CH" ],
    "sections" : [ {
      "heading" : null,
      "text" : "In the spirit of competing against the best action in hindsight in multi-armed bandits problem, our goal here is to be competitive w.r.t. the cumulative losses the algorithm could receive by following the policy of always selecting one expert. We prove the following hardness result: without any coordination between the forecaster and the experts, it is impossible to design a forecaster achieving no-regret guarantees. In order to circumvent this hardness result, we consider a practical assumption allowing the forecaster to “guide” the learning process of the experts by filtering/blocking some of the feedbacks observed by them from the environment, i.e., not allowing the selected expert it to learn at time t for some time steps. Then, we design a novel no-regret learning algorithm LEARNEXP for this problem setting by carefully guiding the feedbacks observed by experts. We prove that LEARNEXP achieves the worst-case expected cumulative regret ofO(T 1 2−β ) after T time steps and matches the regret bound of Θ(T 1 2 ) for the special case of multi-armed bandits."
    }, {
      "heading" : "1. Introduction",
      "text" : "Many real-world applications involve repeatedly making decisions under uncertainty—for instance, choosing one of the several items to recommend to the user, dynamically allocating resources among available stock options in a financial market, or sequentially deciding the next medical test in healthcare. Furthermore, the feedback is often limited in these settings in a sense that only the loss/reward associated with the action taken by the system is observed, referred to as the bandit feedback setting. Online learning using expert advice with bandit/limited feedback is a well-studied framework to model the above-mentioned application settings (Freund and Schapire, 1995; Auer et al., 2002; Cesa-Bianchi and Lugosi, 2006; Bubeck and Cesa-Bianchi, 2012) and addresses the fundamental question of how a learning algorithm should trade-off exploration (the cost of acquiring new information) versus exploitation (acting greedily based on current information to minimize instantaneous losses). In this paper, we investigate this framework with an important practical consideration:\nHow do we use the advice of experts when they themselves are learning entities?\n© A. Singla, H. Hassani & A. Krause.\nar X\niv :1\n70 2.\n04 82\n5v 2\n[ cs\n.L G\n] 1\n7 Fe\nb 20"
    }, {
      "heading" : "1.1. Motivating Applications",
      "text" : "Modeling experts as learning entities realistically captures many practical scenarios of how one would define/encounter these experts in real-world applications, such as seeking advice from fellow players or friends, aggregating prediction recommendations from trading agents or different marketplaces, product testing with human participants who might adapt over time, information acquisition from crowdsourcing participants who might learn over time, the problem of meta-learning and hyperparameter tuning whereby different learning algorithms are treated as experts (cf. Baram et al. (2004); Hsu and Lin (2015)), and many more.\nAs a concrete running example, we consider the problem of learning to offer personalized deals / discount coupons to users enabling new businesses to incentivize and attract more customers (Edelman et al., 2011; Singla et al., 2016). An emerging trend is deal-aggregator sites like Yipit1 providing personalized coupon recommendation services to their users by aggregating and selecting coupons from daily-deal marketplaces like Groupon and LivingSocial1. One of the primary goals of these recommendation systems like Yipit (corresponding to the central algorithm / forecaster in our setting) is to design better selection strategies for choosing coupons from different marketplaces (corresponding to the experts in our setting). However, these marketplaces (experts) themselves would be learning to optimize the coupons to offer, for instance, the discount price or the type of the coupon based on historic interactions with users (Edelman et al., 2011)."
    }, {
      "heading" : "1.2. Experts as Learning Entities: Challenges and Our Results",
      "text" : "We now provide an overview of our approach, the main challenges in designing a forecaster with no-regret guarantees, and our results.\nThe interaction model. We consider an online setting similar to that of adversarial online learning using experts’ advice with bandit feedback (Auer et al., 2002). However, to keep the presentation more general (e.g., we do not necessarily require that the sets of actions are shared across different experts), the forecaster selects / seeks advice from only one expert it at any time t (cf. Kale (2014) for more discussion about this aspect). More specifically, at time t, the forecaster selects an expert it, performs an action atit recommended by the expert i\nt, and incurs a loss lt(atit) set by the adversary.\nThe notion of regret. In the standard framework, i.e., when the experts are not learning entities, the EXP3 algorithm (Auer et al., 2002) is well-suited for this problem setting achieving the optimal regret bounds. However, it is important to note that the classical notion of external regret used in the literature (cf. Auer et al. (2002); Cesa-Bianchi and Lugosi (2006); Bubeck and Cesa-Bianchi (2012)) does not provide any meaningful guarantees in our setting in terms of competing against the “best” expert. Similar to the notion of competing against the best action in hindsight in multi-armed bandits problem, we want to be competitive w.r.t. the cumulative losses the algorithm could receive by following the policy of always selecting one expert (cf. Section 2.3 for a formal definition).\nExperts as no-regret learners and blackbox approach. In our setting, the experts themselves are learning entities. Formally, we assume that the experts are no-regret learners, i.e., the average regret of any expert j vanishes at a rate of at least O(tβ−1j ) with tj learning steps where β ∈ [0, 1] is a parameter known to the forecaster. We consider the following natural notion of bandit/limited feedback: only the selected expert it receives feedback and gets to learn at time t; all other experts that have not been selected at time t experience no change in their learning state at this time. We\n1. http://yipit.com/; http://www.groupon.com/; https://livingsocial.com/\nconsider a generic black-box approach in which the forecaster does not know and cannot control the internal learning dynamics of the experts.\nChallenges and hardness result. It turns out that modeling these experts as learning entities leads to a challenging twist in this well-studied and foundational online learning framework. In this paper, we prove the following hardness result for our problem setting: without any coordination between the forecaster and the experts, it is impossible to design a forecaster achieving noregret guarantees in the worst-case. Somewhat surprisingly, this hardness result holds when playing against an oblivious (non-adaptive) adversary and even if restricting the experts to be implementing some well-studied online learning algorithms, for instance, the HEDGE algorithm (Freund and Schapire, 1995). The fundamental challenge leading to this hardness result arises from the fact that the forecaster’s selection strategy affects the feedback sequences observed by the experts which in turn alters their learning process.\n“Guided” feedbacks and achieving no-regret guarantees. In order to circumvent this hardness result, we consider the following practical assumption: we allow the forecaster to “guide” the learning process of the experts by filtering/blocking some of the feedbacks observed by them from the environment, i.e., the selected expert it would not learn at time t for some time steps. For instance, in the motivating application of offering personalized deals to users, the deal-aggregator site (forecaster) often primarily interacts with users on behalf of the individual daily-deal marketplaces (experts) and hence can control the flow of feedback to these marketplaces. Alternatively, we note that this process of guiding and restricting the feedback can be achieved via coordination between the forecaster and the selected expert it with a 1-bit of communication at time t. Given this additional control, we design a novel algorithm LEARNEXP for the forecaster which carefully guides the feedbacks observed by experts. We prove that LEARNEXP achieves the worst-case expected cumulative regret of O(T 1 2−β ) after T time steps against an oblivious adversary for a rich family of no-regret learning algorithms that experts may be implementing. For the special case of multiarmed bandits, algorithm LEARNEXP is equivalent to that of the well-studied EXP3 algorithm and hence matches the optimal regret bound of Θ(T 1 2 ).\nConnections to the existing results. Maillard and Munos (2011) studied the problem of competing against an adaptive adversary when the adversary’s reward generation policy is restricted to a pre-specified set of known models. For this problem, the authors introduced the EXP4/EXP3 algorithm, i.e., EXP4 meta-algorithm with experts executing EXP3 algorithms proving a regret of O(T 2 3 ) (cf. Bubeck and Cesa-Bianchi (2012) for a variant of the algorithm). This EXP4/EXP3 algorithm is perhaps closest to ours, as it involves a forecaster where the experts are the learning entities. However, we note that our hardness result does not contradict their regret bounds—the key difference in their setting is that the forecaster has the power to modify the losses as seen by experts, and it provides an unbiased estimate of the losses to these experts. Moreover, their analysis is specific to the experts implementing the EXP3 or bandit algorithms, whereas the focus of this paper is to present a more generic learning framework in which experts as learning entities may implement a broad class of learning algorithms. Our work is also related to contemporary work by Agarwal et al. (2016), who study a variant of the problem tackled by Maillard and Munos (2011) and also prove a hardness result similar to that of ours. Note that, in applications where the experts directly receive feedback from the environment, implementing the strategies of Maillard and Munos (2011); Agarwal et al. (2016) would require the forecaster to communicate the probability p with which the expert it was selected at time t. However, our proposed idea of guiding the feedback\nProtocol 1: The interaction between adversary ADV, algorithm ALGO, and experts foreach t = 1, 2, . . . , T do\n/* Adversary generates the following */ 1 a private loss vector lt, i.e., lt(a) ∀ a ∈ A 2 a private feedback vector f t, i.e., f t(a) ∀ a ∈ A 3 a public context xt ∈ X\n/* Selecting an expert and performing an action */ 4 ALGO selects an expert it ∈ [N ] denoted as EXPit 5 ALGO performs the action atit recommended by EXPit /* Feedback and updates */ 6 ALGO incurs (and observes) loss lt(atit) and updates its selection strategy 7 ∀j ∈ [N ] : j 6= it, EXPj does not observe any feedback and makes no update 8 EXPit observes feedback f t(atit) from the environment and updates its learning state\nend\ncan be achieved via coordination between the forecaster and the selected expert it with a 1-bit of communication at time t."
    }, {
      "heading" : "2. The Model",
      "text" : "We have the following entities in our problem setting: (i) an algorithm ALGO as the forecaster; (ii) the adversary ADV acting on behalf of the environment; and (iii) N experts EXPj ∀j ∈ {1, . . . N} (henceforth denoted as [N ])."
    }, {
      "heading" : "2.1. Specification of the Interaction",
      "text" : "Protocol 1 provides a high-level specification of the interaction between the N + 2 entities. The sequential decision making process proceeds in rounds t = 1, 2, . . . , T (henceforth denoted as [T ]); for simplicity we assume that T is known in advance to the algorithm and the results in this paper can be extended to an unknown horizon via the usual doubling trick (Cesa-Bianchi and Lugosi, 2006). Each expert EXPj where j ∈ N is associated with a set of actions Aj and the action set of the algorithm ALGO is given by A = ∪j∈[N ]Aj . For the clarify of presentation in defining the loss and feedback vectors, we will consider that the action sets of experts are disjoint.2\nAt any time t, the adversary ADV generates a private loss vector lt (i.e., lt(a) ∀ a ∈ A) and a private feedback vector f t (i.e., f t(a) ∀ a ∈ A). Additionally, the adversary ADV generates a context xt ∈ X that is accessible to all the experts while recommending their actions at time t—this context essentially encodes all the side information from the environment accessible to the experts at time t (e.g., this context could represent preferences of a user arriving at time t in an online recommendation system). Simultaneously, the algorithm ALGO (possibly with some randomization) selects expert EXPit to seek advice. The selected expert EXPit recommends an action ait ∈ Ait ⊆ A (possibly with its internal randomization) which is then performed by the algorithm. As feedback, the algorithm ALGO observes the loss lt(atit) and updates its strategy on how to select experts in the future. All the experts apart from the one selected (i.e., EXPj ∀ j 6= it) observe no feedback and\n2. Note that assuming the disjoint action sets across experts is w.l.o.g., as we can still simulate the shared actions by enforcing a constraint that the losses generated by the adversary are same for the shared actions at any given time.\nmake no update at this time. The selected expert EXPit observes a feedback from the environment denoted as f t(atit) and updates its learning state. At the end of time t, the algorithm ALGO incurs a loss of lt(atit).\nSo far, we have considered a generic notion of the feedback received by the selected expert— this feedback essentially depends on the application setting and is supposed to be “compatible” with the learning algorithm used by an expert. As a concrete example, consider an expert EXPj implementing the EXP3 algorithm and taking action atj at time t, then the feedback f\nt(atj) received by this expert (if selected at time t) is the loss lt(atj); for the case of expert EXPj implementing the HEDGE algorithm, the feedback f t(atj) received by this expert (if selected at time t) is the set of losses lt(a) ∀ a ∈ Aj . The feedback could be more general, for instance, receiving a binary signal of rejection or acceptance of the offered deal when an expert is implementing a dynamic pricing based algorithm via the partial monitoring framework (Cesa-Bianchi and Lugosi, 2006; Bartók et al., 2014). Also, we note that the special case of standard multi-armed bandits is captured by the setting in which Aj is a singleton for every expert j ∈ [N ].\nWe assume that the losses are bounded in the range [0, lmax] for some known lmax ∈ R+; w.l.o.g. we will use lmax = 1 (Auer et al., 2002). We consider an oblivious (non-adaptive adversary) as is usual in the literature (Freund and Schapire, 1995; Auer et al., 2002), i.e., the loss vector lt, the feedback vector f t, and the context xt at any time t do not depend on the actions taken by ALGO, and hence can be considered to be fixed in advance. Apart from that, no other restrictions are put on the adversary, and it has complete knowledge about the algorithm ALGO and the learning dynamics of the experts."
    }, {
      "heading" : "2.2. Specification of the Experts",
      "text" : "We consider a generic black-box approach in which ALGO does not know and cannot control the internal dynamics of the experts. In order to formally state the objective and guarantees we seek, we now provide a generic specification of the experts. At time t, let us denote an instance of feedback received by EXPit by a tuple h = (atit , x\nt, f t(atit)). For any expert EXPj where j ∈ [N ], let Htj = (h1, h2, . . .) denote the feedback history for EXPj , i.e., an ordered sequence of feedback instances observed by EXPj up to time t. The length |Htj | denotes the number of learning steps for EXPj up to time t. At time t, the action atj recommended by EXPj to the algorithm, if this expert is selected, is given by atj = πj(x\nt,Htj) where πj is a (possibly randomized) function of EXPj , taking as input a context and a history of feedback sequence, and outputs an action a ∈ Aj . Importantly, this history Htj is dependent on the execution of the algorithm ALGO— for clarify of presentation, we denote it asHtj,ALGO.\nNo-regret learning dynamics. To be able to say anything meaningful in this setting, we introduce the constraint of no-regret learning dynamics on the experts.3 Let us consider any sequence of loss vector l, feedback vector f , and context x given by S = ( (lτ , f τ , xτ ) ) τ={1,2,...} generated arbitrarily by ADV and let |S| denotes its length. Consider a setting in which an expert EXPj for any j ∈ [N ] is selected at every time step. At every time step τ ∈ [|S|], EXPj recommends an action aτj , accumulates the loss l(a τ j ), and observes the feedback f(a τ j ). In this setting, EXPj observes feedback at every time step and we denote this “complete” history of feedback sequence at any time τ ∈ |S| as Htj,1 whereby 1 denotes the fact that this expert is selected and receives feedback with\n3. In order to prove the no-regret guarantees for our algorithm LEARNEXP in Section 4, this constraint is required to hold only for the best expert against which we want to be competitive, a less stringent requirement.\nprobability 1 at every time step. Then, the no-regret learning dynamics of EXPj parameterized by βj ∈ [0, 1] guarantees that the expected average regret vanishes as follows4:\nE [ 1\n|S| |S|∑ τ=1 lτ ( πj(x τ ,Hτj,1) )] − E [ 1 |S| |S|∑ τ=1 lτ ( πj(x τ ,H|S|j,1) )] ≤ O(|S|βj−1) (1)\nwhere the expectation is w.r.t. the randomization of function πj . We assume that parameter β ∈ [0, 1] upper bounds the regret rate parameters of individual experts and is a parameter known to the forecaster.5"
    }, {
      "heading" : "2.3. Our Objective: No-Regret Guarantees",
      "text" : "Intuitively, we want to be competitive w.r.t. the cumulative losses the algorithm could receive by following the policy of always using the advice of one single expert—such a policy ensures that the single expert gets more feedback to improve its learning state and hence incur less cumulative loss. This is a challenging problem when the experts are learning entities. For instance, what may go wrong is that the best expert could have a slow rate of learning/convergence thus incurring high losses in the beginning, misleading the algorithm to essentially “downweigh” this expert. This is turn further exacerbates the problem for the best expert in the bandit feedback setting as this expert will be selected less and will have fewer learning steps to improve its state. This adds new challenges to the classic trade-off between exploration and exploitation, suggesting the need to explore at higher rate to tackle this problem.\nLet us begin by looking at the classic notion of external regret used in the literature (Auer et al., 2002; Cesa-Bianchi and Lugosi, 2006; Bubeck and Cesa-Bianchi, 2012). Given that the experts are learning entities, naturally the losses incurred at any time step are dependent on the history of the forecaster’s actions as that history defines the current learning state of the individual experts. Given this subtle issue of history dependent losses, the usual notion of external regret does not provide any meaningful guarantees in terms of competing against the “best expert in hindsight” (see below for a formal definition); the bounds given by the external regret are only w.r.t. the post hoc sequence of actions performed and losses observed during the execution of the algorithm (cf. Maillard and Munos (2011); Arora et al. (2012); McMahan and Streeter (2009) for more discussion on this).\nWe consider the following natural notion of regret in this paper: our goal is to be competitive w.r.t. the best expert in hindsight, that is, competitive w.r.t. the cumulative loss that any expert could have received with the optimal actions it could have taken in hindsight. Formally, the expected cumulative regret of ALGO against the best expert in hindsight is given by:\nREG(T,ALGO) := T∑ t=1 E [ lt ( πit(x t,Htit,ALGO) )] − min j∈[N ] E [ T∑ t=1 lt ( πj(x t,HTj,1) )]\n(2)\nwhere the expectation is w.r.t. the randomization of the algorithm as well as any internal randomization of the experts. Our goal is to design an algorithm ALGO for the forecaster so that the regret REG(T,ALGO) grows sublinearly in time T .\n4. Note that this is a weaker notion of regret—any deterministic policy πj that always outputs a constant action has βj = 0. However, βj = 0 would be the right way to characterize the learning dynamics of this expert for our setting. 5. Again for our algorithm LEARNEXP in Section 4, β only needs to upper bound the regret rate for the best expert against which we want to be competitive."
    }, {
      "heading" : "3. Hardness Result",
      "text" : "We show in this section that, in the absence of any coordination between the forecaster and experts, it is impossible to design a forecaster that achieves no-regret guarantees in the worst-case. Somewhat surprisingly, we prove this hardness result when playing against an oblivious (non-adaptive) adversary and when restricting the experts to be implementing the well-studied HEDGE algorithm (Freund and Schapire, 1995). We formally state this hardness result in the Theorem 1 below.\nTheorem 1 There is a setting in which each of the experts has no-regret learning dynamics with parameter β = 12 ; however, any algorithm ALGO (forecaster) will suffer a positive average regret, i.e., REG(T,ALGO) = Ω(T ).\nThe proof is given in the Appendix, we briefly outline the main ideas below. Our setting for proving this theorem consists of two experts EXP1 and EXP2. The first expert EXP1 has two actions given by A1 = {a1, a2}, and the second expert EXP1 has only one action given by A2 = {b}. The action set of the algorithm ALGO is given by A = {a1, a2, b}. The expert EXP1 plays the HEDGE algorithm (Freund and Schapire, 1995), i.e., the regret rate parameter is β1 = 0.5; the expert EXP2 has only one action to play as in the standard multi-armed bandit with β2 = 0.6 Figures 1(a), 1(b), and 1(c) show the cumulative loss sequences L1, L2, and L3 for three different scenarios—the adversary at t = 0 uniformly at random picks one of these scenarios and uses that loss sequence.\nThe main idea of the proof uses the following arguments. We consider the case where the forecaster is facing the sequence L1 (chosen by the adversary with probability 13 at t = 0). We then divide the time horizon T into different slots and discuss the execution behavior of the forecaster and experts over these time slots. Specifically, our claim is that in the time slot t ∈ (T4 , T 2 ], the expert\n6. In fact, this hardness result holds even when considering a powerful forecaster which knows exactly the learning algorithms used by the experts, and is able to see the losses {lt(a1), lt(a2), lt(a3)} at every time t ∈ [T ].\nEXP1 would not be selected for T12 − o(T )) time steps. As a result, in the time slot t ∈ ( 11T 12 , T ], the expert EXP1 would select action a2 almost surely, and a1 would only be selected o(T ) number of times, leading to a positive average regret for the forecaster. Informally speaking, our negative example shows that the forecaster’s selection strategy could add “blind spots” in the feedback history seen by the experts and that they might not be able to “recover” from this. The key fundamental challenge leading to this hardness result is that the forecaster’s selection strategy affects the feedback sequences observed by the experts, which in turn alters the experts’ learning process."
    }, {
      "heading" : "4. Our Algorithm LEARNEXP",
      "text" : "In this section, we introduce a practical assumption that allows the forecaster to “guide” the learning process of experts, and then we design our main algorithm LEARNEXP with provable no-regret guarantees."
    }, {
      "heading" : "4.1. Guided Feedbacks",
      "text" : "In order to circumvent the hardness result proved in Section 3, we now consider a practical assumption motivated by the application setting of deal-aggregator sites, as discussed in Section 1. Usually, a deal-aggregator site interacts with users on behalf of the individual daily-deal marketplaces (experts) and hence could control the flow of feedback to these marketplaces. Hence, we allow the forecaster to “guide” the learning process of the experts by filtering/blocking some of the feedback the experts receive from the environment. Recall that at time t, as per the interaction model presented in Section 2, the selected expert EXPit observes feedback f t(atit) from the environment. We now consider the setting with the following additional power in the hands of the forecaster: In order to guide the learning process of the experts, the forecaster at time t could block the feedback, i.e., the expert EXPit would not observe feedback at time t and hence would not learn at this time (just like any other experts who were not selected at time t). Alternatively, we note that this process of guiding the feedback could be achieved via coordination between the forecaster and the selected expert EXPit with a 1-bit communication at time t."
    }, {
      "heading" : "4.2. Algorithm LEARNEXP",
      "text" : "With this additional power of the forecaster to guide feedback, we develop our main algorithm LEARNEXP, presented in Algorithm 2. The selection strategy of the algorithm LEARNEXP is similar to the EXP family of algorithms, and in particular is equivalent to the EXP3 algorithm by Auer et al. (2002). The core idea of guiding the feedbacks observed by experts is presented in Lines 10,11, and 12.\nBy default, as per the Protocol 1, the selected expert EXPit always observes feedback at time t—for this protocol, the hardness result of Theorem 1 applies. Our algorithm LEARNEXP instead decides whether the expert EXPit should observe/use the feedback based on the outcome ξt of a coin flip with probability η\nN ·pt it\n. By choosing this particular probability, the algorithm LEARNEXP\nensures that the probability that any expert EXPj observes feedback at time t is constant over time and is given by ηN . The key parameter of the algorithm η would be fixed in Theorem 3 based on the regret rate β to achieve the desired guarantees on the regret.\nThe guarantees in Theorem 3 mean that by adding this additional control/coordination in our model, we are able to circumvent the hardness result of Theorem 1. Interestingly, if we consider any\nAlgorithm 2: LEARNEXP 1 Parameters: η ∈ (0, 1] 2 Initialize: time t = 1, weights wtj = 1 ∀j ∈ [N ]\nforeach t = 1, 2, . . . , T do /* Selecting an expert and performing an action */\n3 ∀j ∈ [N ], define probability ptj = (1− η) · wtj(∑\nk∈[N ]w t k\n) + η N\n4 Draw it from the multinomial distribution (ptj)j∈[N ] 5 Perform action atit recommended by the expert EXPit /* Observing the loss and making updates */ 6 Observe loss lt(atit) 7 ∀j ∈ [N ], do the following:\n8 Set l̃tj as follows: l̃ t j =\nlt(atit)\nptit for j = it, else l̃tj = 0\n9 Update wt+1j ← wtj · exp(− η · l̃tj N )\n/* Guiding the feedback */\n10 ξt ∼ Bernoulli( η N · ptit ) 11 if (ξt = 1) then 12 EXPit observes feedback f t(atit) from the environment and updates its learning state\nend end\nexpert EXPj for j ∈ [N ], the historyHtj at any time t under this guided feedback setting would only contain a subset of the feedback instances that it would have received without guiding (i.e., where ξt = 1 ∀t ∈ [T ]). By carefully allowing the expert to observe a strictly smaller set of feedback instances allows us to ensure that the expert EXPj achieves low regret. Considering the example we use in the proof of Theorem 1 to show the hardness results, this means that by carefully guiding the feedback received by experts, our algorithm LEARNEXP ensures that there are no “blind spots” in the feedback history of any expert. However, in order to achieve this, the algorithm is required to explore at a higher rate, as is evident by the value of η in Theorem 3."
    }, {
      "heading" : "4.3. Theoretical Guarantees",
      "text" : "Next, we analyze the theoretical guarantees of our algorithm LEARNEXP. One approach to doing this is to consider a particular class of no-regret learning algorithms that experts implement and prove guarantees for that class. Instead, we introduce a novel, generic notion of “smooth” no-regret learning—our theoretical guarantees are then proven for the experts that have no-regret and smooth learning dynamics. Next, we introduce this notion and then discuss (see Proposition 2) the class of no-regret learning algorithms that also satisfy the constraint of smooth learning dynamics."
    }, {
      "heading" : "4.3.1. SMOOTH LEARNING DYNAMICS",
      "text" : "In our bandit feedback setting, not all the experts can observe feedback at a given time step, and hence the history of feedback instances received by any particular expert is naturally “sparse”. To formally state the behavior of the learning algorithm under this sparse feedback, we now introduce a new notion, termed smooth learning dynamics, to complement the no-regret learning dynamics defined in (1). Consider the same fixed sequence S as used in defining (1) and an expert EXPj . However, instead of observing feedback at every time step, let’s say that the expert EXPj only gets to observe the feedback sporadically at a rate of α ∈ (0, 1]—we call this an α-sparse history, denoted asHlj,α. Then, the constraint of smooth learning dynamics ensures that the expected regret of the expert EXPj when receiving the above-mentioned sparse feedback vanishes (smoothly w.r.t. rate α) as follows:\nE [ 1\n|S| |S|∑ τ=1 lτ ( πj(x τ ,Hτj,α) )] − E [ 1 |S| |S|∑ τ=1 lτ ( πj(x τ ,H|S|j,1) )] ≤ O((α · |S|)βj−1) (3)\nwhere the expectation is w.r.t. the randomization of function πj as well as w.r.t. the randomization in generating this sparse history. The following proposition states that a rich class of online learning algorithms indeed have smooth learning dynamics that can be used by the experts, cf. Appendix for the proof.\nProposition 2 A rich class of no-regret online learning algorithms based on gradient-descent style updates have smooth learning dynamics including the Online Mirror Descent family of algorithms with exact or estimated gradients (Shalev-Shwartz, 2011) and Online Convex Programming via greedy projections (Zinkevich, 2003)."
    }, {
      "heading" : "4.3.2. NO-REGRET GUARANTEES OF LEARNEXP",
      "text" : "Next, we prove the no-regret guarantees of our algorithm LEARNEXP, formally stated in Theorem 3. The following theorem (stating only the leading terms w.r.t. the T and dropping any other constants like N ) provides the no-regret guarantees of LEARNEXP against the best expert in hindsight as per (2). The proof is given in the Appendix.\nTheorem 3 Let T be the fixed time horizon. Consider that the best expert j∗ ∈ [N ] has no-regret smooth learning dynamics parameterized by βj∗ ∈ [0, 1] and LEARNEXP is invoked with input β ∈ [0, 1] such that β ≥ βj∗ . Set parameters η = Θ ( T − 1−β 2−β · N 1−β 2−β · (logN)( 1 2 ·1{β=0}) ) . Then, for sufficiently large T , the worst-case expected cumulative regret of LEARNEXP against the best expert in hindsight is:\nREG(T, LEARNEXP) ≤ O ( T 1 2−β ·N 1 2−β · (logN)( 1 2 ·1{β=0}) ) For the special case of multi-armed bandits (where β = 0), this regret bound matches the bound of Θ(T 1 2 )—in fact, for this special case, our algorithm LEARNEXP is exactly equivalent to EXP3. For an important case when experts are implementing algorithms like HEDGE or EXP3 (where β = 12 ), our algorithm LEARNEXP achieves the bound of O(T 2 3 )."
    }, {
      "heading" : "5. Background and Related Work",
      "text" : "In this section, we provide an overview of the relevant literature."
    }, {
      "heading" : "5.1. Background",
      "text" : "We begin with a background on the framework of learning using expert advice with bandit feedback. Using expert advice. The seminal work of Littlestone and Warmuth (1994); Cesa-Bianchi et al. (1997) initiated the study of using expert advice for prediction problems, and Freund and Schapire (1995) introduced the algorithm HEDGE for the general problem of dynamically allocating resources among a set of options using expert advice.\nUsing expert advice with bandit feedback. However, the feedback is often limited in these settings in a sense that only the loss/reward associated with the action taken by the system is observed, referred to as the bandit feedback setting. To tackle this, Auer et al. (2002) extended this framework to the limited feedback setting and introduced the EXP family of algorithms (EXP3, EXP4, and its variants) for multi-armed bandits and expert advice with bandit feedback. With limited feedback, this framework addresses the fundamental question of how a learning algorithm should trade-off exploration versus exploitation. This framework has been studied extensively by researchers in a variety of fields and the above mentioned algorithms provide minimax optimal noregret guarantees—we refer the reader to Bubeck and Cesa-Bianchi (2012) for the survey on bandit problems and monograph by Cesa-Bianchi and Lugosi (2006).\nFurthermore, this framework is very generic and versatile to capture many complex real-world scenarios. For instance, in the EXP family of algorithms (Auer et al., 2002; McMahan and Streeter, 2009; Beygelzimer et al., 2011), each expert could have access to an arbitrary context (e.g., information about user preferences) that may not be shared among experts and may not be available to the algorithm. Furthermore, no statistical assumptions are needed on the process generating context or losses/rewards over time. Consequently, this framework has been used in many diverse application settings including search engine ad placement (McMahan and Streeter, 2009), personalized news article recommendation (Beygelzimer et al., 2011), packet routing in networks, (Awerbuch and Kleinberg, 2004), and meta-learning with different learning algorithms as the experts (Baram et al., 2004; Hsu and Lin, 2015)."
    }, {
      "heading" : "5.2. Related Work",
      "text" : "Next, we review research work that is relevant to the problem studied in this paper. Markovian, rested, and restless bandits. The seminal work of Gittins (1979) considered Markovian bandits where each action/arm is associated with its own stochastic MDP and introduces the Gittins index to find an optimal sequential policy. Note that the arm changes its state only when it is pulled, hence also termed as rested bandits. Whittle (1988) considered an extension termed restless bandits where all the arms change their reward distributions at every time step according to their associated stochastic MDP. Restless bandits are notoriously difficult to tackle (cf. (Slivkins and Upfal, 2008)), thereby Slivkins and Upfal (2008) considered a type of restless bandits where the change in state is governed by a more gradual process with stochastic rewards depending upon the state. Besbes et al. (2014) considered another type of restless bandits with stochastic reward functions, however these distributions change adversarially with a budget on the allowed variation. Our approach is similar in spirit to the rested bandits; however, none of the frameworks above would model the learning dynamics of the experts in the adversarial setting we consider.\nNon-oblivious/adaptive adversary. As in our setting, the challenge of history-dependent expert rewards also arises in the case of non-oblivious/adaptive adversary (Maillard and Munos, 2011; Arora et al., 2012). Arora et al. (2012) studied online learning with bandit feedback against a nonoblivious adversary with bounded memory and introduced the notion of policy regret instead of the usual notion of external regret. Maillard and Munos (2011) studied competing against adaptive adversary when the adversary’s reward generation policy is restricted to a pre-specified set of known models. However, none of the frameworks of non-oblivious/adaptive adversary listed above model learning dynamics in our setting: It would require an adversary with unbounded memory to apply the results of Arora et al. (2012), and an adversary with unbounded number of models to apply the techniques of Maillard and Munos (2011).\nContextual bandits. Another perspective on tackling some of the applications we mentioned above is the contextual bandit framework (Li et al., 2010; Langford and Zhang, 2007; Agarwal et al., 2014). We refer the reader to the paper by McMahan and Streeter (2009) for more discussion on the connection between the framework of contextual bandits and learning using expert advice with bandit feedback.\nLearning in games. An orthogonal line of research studies the interaction of agents in multiplayer games where each agent uses a no-regret learning algorithm (Blum and Monsour, 2007; Syrgkanis et al., 2015). The questions tackled in this line of research are very different as it focuses on the interactions of the agents, their individual as well as social utilities, and the convergence of the game to equilibrium. This orthogonal line of research reassures that the no-regret learning dynamics that we consider in this paper are indeed important and natural dynamics that are also prevalent in other application domains."
    }, {
      "heading" : "6. Conclusions",
      "text" : "In this paper, we investigated the online learning framework using expert advice with bandit feedback with an important practical consideration: how do we use the advice of the experts when they themselves are learning entities? As our first contribution, we proved the hardness result stating that it is impossible to achieve no-regret guarantees when the experts receive feedback directly from the environment and there is no further coordination between forecaster/experts. Our hardness result sheds light on the complexity of the problem when applying this online learning framework to real-world applications whereby it is natural for experts to exhibit learning dynamics.\nThen, we considered a practical assumption of “guided” feedbacks whereby the forecaster can block/filter the feedback received by the selected expect from the environment. Under this setting, we proposed a novel algorithm LEARNEXP—we proved that LEARNEXP achieves the worst-case expected cumulative regret of O(T 1 2−β ) after T time steps where β is a parameter characterizing the individual no-regret learning dynamics of the best expert. This regret bound matches the bound of Θ(T 1 2 ) for the special case of multi-armed bandits.\nThere are a number of research directions for future work. An interesting question to tackle is whether it is possible to design a forecaster in our setting with a worst-case cumulative regret of Θ(T\n1 2 ) when the individual experts have no-regret learning dynamics with β = 12 . In this paper, in\norder to circumvent the hardness result, we considered the power of blocking/filtering the feedbacks, which can equivalently be achieved with a 1-bit of communication at every time step. An interesting direction would be to consider other practical ways of coordination and to understand the minimal coordination required to achieve no-regret guarantees."
    }, {
      "heading" : "Appendix A. Proof of Theorem 1",
      "text" : "In this section, we give a proof of the hardness result by discussing a generic and simple setting in which any forecaster suffers a positive average regret.\nThe setting. Our setting consists of two experts EXP1 and EXP2. The first expert EXP1 has two actions given by A1 = {a1, a2}, and the second expert EXP1 has only one action given by A2 = {b}. The action set of the forecaster, or algorithm ALGO, is given by A = {a1, a2, b}. The expert EXP1 plays the HEDGE algorithm (Freund and Schapire, 1995), i.e., the regret rate parameter is β1 = 0.5 (see (1)); the expert EXP2 has only one action to play as in the standard multi-armed bandit with β2 = 0 (see (1)). The forecaster knows parameter β = 0.5 which upper bounds the regret rate of the individual experts.7\nLoss sequences. Figures 1(a), 1(b), and 1(c) shows the cumulative loss sequences L1, L2, and L3 for three different scenarios—the adversary at t = 0 uniformly at random picks one of scenarios and uses that loss sequence. These plots show the cumulative losses of the three actions A = {a1, a2, b} for three different sequences. For the first scenario with cumulative loss sequences L1 shown in Figure 1(a), we show in Figure 2 the instantaneous losses of the different actions. Figures 2(a) and 2(b) show the losses of actions for EXP1; 2(c) shows the losses of action for EXP2.\nModel specification. To fully specify the model and Protocol 1, we specify now the feedback vector, and the context over time. The context xt is constant over time and plays no role in our setting. The experts receive the following feedback when selected: the expert EXP1 would observe the losses {lt(a1), lt(a2)} when it = 1; and the expert EXP2 would observe the loss {lt(b)} when it = 2.\nExecution behavior. We now divide the time horizon T into different slots and discuss the execution behavior of the forecaster and experts over these time slots. Specifically, let us consider the case where the forecaster is facing the sequence L1 (chosen by adversary with probability 13 at t = 0) with cumulative losses shown in Figure 1(a) and instantaneous losses of the actions shown in Figure 2. For a clarity of presentation, we shall use ∆ = 0.01 as a constant in rest of the proof below.\n• t ∈ [0, T4 ]: In this time slot, the expert EXP1 would be selected almost surely by the forecaster and the number of times EXP2 would be selected is o(T ). If this is not the case, then this forecaster would suffer positive average regret on the loss sequence L3 for third scenario in Figure 1(c). 8 The loss incurred by the forecaster at any time t in this time slot is at least 0.5.\n• t ∈ (T4 , T 2 ]: This is the first crucial time slot whereby forecaster’s selection strategy would\nadd “blind spots” to the feedback received by EXP1. The key argument is that in this time slot, the forecaster cannot select expert EXP1 for more than T6 + o(T ) timesteps—if this happens, than this forecaster would have a positive average regret on the loss sequence L2 for second scenario in Figure 1(b). In other words, in this period, the expert EXP1 has missed seeing the feedback for T12 − o(T )) timesteps. Clearly, the loss incurred by the forecaster at any time t in this time slot is at least 0.5− 3∆2 .\n7. In fact, this hardness result holds even when considering a powerful forecaster which knows exactly the learning algorithms used by the experts, and is able to see the losses {lt(a1), lt(a2), lt(b)} at every time t ∈ [T ]. 8. Note here that the loss sequence L3 is exactly equal to L1 up to time T/4. Furthermore, although we have assumed that the setting chosen by the adversary is L1, we should bear in mind that the forecaster (who can not distinguish between the losses at least up to time T/4) should play in a way that it does not suffer positive average regret for L3.\n• t ∈ (T2 , 11T 12 ]: In this time slot, the expert EXP1 would be selected almost surely and the\nnumber of times EXP2 would be selected is o(T ). The loss incurred by the forecaster at any time t in this time slot is at least 0.5.\n• t ∈ (11T12 , T ]: This is the second crucial time slot which would lead to the positive average regret for the forecaster. We note that the forecaster still does the “right” thing in this time slot, i.e., the expert EXP1 would be selected almost surely and the number of times EXP2 would be selected is o(T ). However, the expert EXP1 has missed observing feedback for ( T12 −o(T )) time steps in the slot ( T 4 , T 2 ]. Note also that no coordination is permitted between\nthe forecaster and the experts, and hence, EXP1 is not aware of the time steps that it misses the feedback. As a result, at the start of this time slot, the cumulative loss of action a1 (as perceived by EXP1 based on observed history) is at least 0.5 · T12 more than the cumulative loss of action a2 (as perceived by EXP1 based on observed history). The expert EXP1 who is playing HEDGE algorithm in our setting would select action a2 almost surely and a1 would be selected o(T ) number of times.\nPositive average regret. Let us now compute the regret of the forecaster when experiencing loss sequence L1 as discussed above. The cumulative loss of the “best expert in hindsight” is given by that of EXP1 always playing action a1. Based on Figure 2(a), this is given by:∑\nt∈[T ]\nlt(a1) = 1 · T\n4 + 0.5 · (11T 12 − T 2 ) = T · (1 2 − 1 24 ) The cumulative loss of the forecaster as per the execution behavior discussed above can be lower\nbounded as follows:∑ t∈[T ] lt(atit) ≥ 0.5 · T 4 + ( 0.5− 3∆ 2 ) · T 4 + 0.5 · (11T 12 − T 2 ) + 0.5 · ( T 12 − o(T ) ) = T ·\n(1 2 − 3∆ 8 − o(T ) 2 · T )\nHence, the total regret of the forecaster is lower bounded by:\nREG(ALGO, T ) ≥ T · (1\n2 − 3∆ 8 − o(T ) 2 · T ) − T · (1 2 − 1 24 ) = T · ( 1 24 − 3∆ 8 − o(T ) 2 · T )\nRecall that the constant ∆ = 0.01, hence the average regret of the forecaster is lower bounded by limT 7→∞ REG(ALGO,T ) T ≥ 91 2400 .\nAs this sequence L1 is selected by the adversary uniformly at random with probability 13 , this means that the forecaster would suffer a positive average regret. As we discussed above, any forecaster which doesn’t have the above-mentioned execution behavior in the timeslot t ∈ [0, T4 ] or t ∈ (T4 , T 2 ] would suffer a positive average regret for L3 and L2 loss sequences."
    }, {
      "heading" : "Appendix B. Proof of Proposition 2",
      "text" : "OCP algorithms. Assume that and expert EXPj is performing Online Convex Programming (OCP) via greedy projections. We will show that such an algorithm has smooth learning dynamics. Note that OCP has regret of size O( √ T ) (i.e. βj = 1/2). Consider the α-OCP algorithm that proceeds according to Algorithm 3. Proving smooth learning dynamics for OCP is equivalent to showing that α-OCP suffers a regret of size O( √ T/α). More precisely, we have the following Lemma.\nAlgorithm 3: α-OCP 1 Problem setting: Convex set S; sequence of convex loss functions f t : S → R+ 2 Parameters: Learning rates ηt for t ∈ [T ] 3 Initialize: w0 ∈ S arbitrarily\nforeach t = 1, 2, . . . , T do 4 wt+1/2 = wt − ηtBtzt where: (i) zt ∈ ∂f t(wt), and (ii) random variables Bt are independent\nBernoulli with parameter α (i.e. Pr(Bt = 1) = 1− Pr(Bt = 0) = α), and (iii) ηt = 1/ √ 1 + ∑t τ=1B τ\n5 wt+1 = ProjS(w t+1/2)\nend\nLemma 4 Let ‖S‖ denote the diameter of the convex set S and L denotes an upper bound on the magnitude of the gradient at any time t ∈ T . Then, the expected regret of the α-OCP algorithm is given by\nE[ T∑ t=1 (f t(wt)− f t(u))] ≤ ‖S‖ 2 2 · √ T α + L2 · √ T α (4)\nwhere the expectation is w.r.t. the sequence of Bernoulli random variables Bt for t ∈ [T ].\nProof We can equivalently write the updates in the α-OCP procedure as follows:\nwt+1/2 = wt − ηtBtzt\n= wt − (ηt · α) · (B t\nα )zt\n= wt − η̃t · z̃t\nwhere η̃t = (ηt · α) and z̃t = (Btα )z t. Note that E[z̃t|w1:t] = E[zt] ∈ ∂f t(wt). We have:\nE [ T∑ t=1 (f t(wt)− f t(u)) ]\n(5)\n= E [ T∑ t=1 E [ (f t(wt)− f t(u))|w1:t ]] (6)\n≤ E [ T∑ t=1 E [ < ∂f t(wt), wt − u > |w1:t ]] (7)\n= E [ T∑ t=1 E [ < z̃t, wt − u > |w1:t ]] (8)\n= E [ T∑ t=1 1 2αηt E [ ∥∥wt − w∥∥2 − ∥∥∥wt+1/2 − w∥∥∥2 + η2tα2 ∥∥∥z̃t∥∥∥2 |w1:t]] (9)\n≤ E [ T∑ t=1 1 2αηt E [ ∥∥wt − w∥∥2 − ∥∥wt+1 − w∥∥2 + η2tα2 ∥∥∥z̃t∥∥∥2 |w1:t]] (10)\n= E [ T∑ t=1 ∥∥wt − w∥∥2 2αηt − ∥∥wt+1 − w∥∥2 2αηt ] + α 2 E [ T∑ t=1 ηt||θt||2 ]\n(11)\n≤ E [∥∥w1 − w∥∥2\n2αη1 − ||w T+1 − w||2 2αηT + 1 2 T∑ t=2 ∥∥wt − w∥∥2 ( 1 αηt − 1 αηt−1 ) ] + α 2 E [ T∑ t=1 ηt||z̃t||2 ]\n(12) ≤ ||S||2E [ 1\n2αηT\n] + α 2 E [ T∑ t=1 ηt||z̃t||2 ]\n(13)\nNow note that E[ηt||z̃t||2] = E [ E[ηt||z̃t||2|w1:t] ] = E [ ηt||zt||2/α] ≤ L2E[ηt]/α. We thus obtain\nE [ T∑ t=1 (f t(wt)− f t(u)) ] ≤ ||S||2E [ 1 2αηT ] + L2 2 E [ T∑ t=1 ηt ] . (14)\nWe next recall that ηt = 1√ 1+ ∑t τ=1B τ . By using the multiplicative Chernoff bound (as Bτ ’s are Bernoulli random variables) we obtain\nPr(ηt ≥ √ 2/(tα)) = Pr( t∑\nτ=1\nBτ ≤ tα/2) ≤ exp(− tα 12 ).\nHence, we obtain E[ηt] ≤ √\n2/(αt)+exp(−tα/12). Also, due to concavity of the function h(x) =√ x, we have that E[1/ηT ] ≤ √ Tα. We finally obtain\nE [ T∑ t=1 (f t(wt)− f t(u)) ] ≤ ||S||2 √ T/α+ L2 √ T/α+ T∑ t=1 exp (−tα/12)\n≤ ||S||2 √ T/α+ L2 √ T/α+ 1/(1− exp (−α/12))\n≤ ||S||2 √ T/α+ L2 √ T/α+ 24/α,\nwhere the last line is because 1/(1− exp(−α/12)) ≤ 24/α for α ≤ 1.\nOMD Algorithms. We now consider the case that expert EXPj is performing an algorithm inside the Online Mirror Descent (OMD) family of algorithms. We assume that the algorithm has a regret of order O( √ T ) for any time horizon T (i.e. βj = 1/2). We also assume that the algorithm uses the doubling trick. Consider the standard online learning scenario where at any time t ∈ [T ] a convex function f t : S → R is assigned (S is assumed to be a convex region). The proofs proceeds in 3 steps.\nStep 1. α-OMD with a fixed time horizon We first analyze the algorithm α-OMD given in 4 which is run for a fixed (deterministic) number of steps.\nAlgorithm 4: α-OMD 1 Problem setting: Convex set S; sequence of convex loss functions f t : S → R+ 2 Parameters: a link function g : Rd → S; time horizon T 3 Initialize: time τ = 1, auxiliary variable θτ = 0 ∈ Rd\nforeach τ = 1, 2, . . . , T do 4 Predict vector wτ = g(θτ ) 5 Update θτ+1 = θτ −Bτzτ where: (a) zτ ∈ ∂f τ (wτ ), (b) Bτ is an independent Bernoulli\nrandom variable with parameter α (i.e. Pr(Bτ = 1) = 1− Pr(Bτ = 0) = α). end\nLemma 5 Let R be a 1/η- strongly convex function over S with respect to a norm || · ||. Assume that α-OMD is run on the sequence with a link function\ng(θ) = arg max w∈S\n(〈w, θ〉 −R(w))\nFurthermore, assume that f t is L-Lipshitz with respect to norm || · ||. Then\nE[ T∑ t=1 (f t(wt)− f t(u))] ≤ R(u)/α+ ηTL2. (15)\nProof For the sake of analysis, we introduce the following slightly modified procedure:\n1. Initialize θ̃1 = θ1/α.\n2. At time τ = 1, 2, · · · , T , let w̃τ = g̃(θ̃τ ), and θ̃τ+1 = θ̃τ − z̃τ . Here, we have z̃τ = Bτα z τ ,\nand the function g̃ is defined as g̃(θ) = arg maxw∈S(〈w, θ〉 −R(w)/α).\nIt is straight forward to justify for any τ ∈ [T ] that θ̃τ = θτ/α and w̃τ = wτ . Also note that E[z̃τ |z̃1:τ−1] = zτ ∈ ∂f τ (wτ ). Hence, the modified procedure (θ̃τ , w̃τ ) is precisely a stochastic\nOMD procedure with with link function g̃. By using Theorem 4.1 in Shalev-Shwartz (2011), w̃τ = wτ , and the fact that R(·)/α is a 1/(ηα)-strongly convex function, we obtain\nE[ T∑ t=1 (f t(wt)− f t(u))] ≤ sup u∈S R(u)/α+ ηα T∑ τ=1 E[||z̃τ ||2].\nWe finally note that E[||z̃τ ||2] = E [ E[||z̃τ ||2 | z̃1:τ−1] ] ≤ L2/α.\nThe result of the Lemma is now immediate.\nStep 2. α-OMD with a random time horizon From Lemma 5, for η = O(1/ √ T ), the algorithm α-OMD suffers a O( √ T/α) regret after any fixed time T . Recall now that at any time the algorithm is only given feedback with independent probability α. We are assuming that the algorithm used by the expert EXPj is performing the doubling trick, i.e., it runs in blocks whose size get doubled consecutively and within each block the learning rate is fixed. As a result, after the algorithm receives sufficient feedback to finish a block, it restarts OMD and changes the learning rate for the next block (which has twice the size). In order to analayze the regret suffered in each block, we need to consider a slightly different version of α-OMD which stops after a randomly chosen time.\nLemma 6 (α-OMD with a random time horizon) Assume that we run the α-OMD procedure until the time, call it Tstop, such that following stopping criterion has been fulfilled:\nTstop∑ τ=1 Bτ = M. (16)\nWe the have\nE[ Tstop∑ t=1 (f t(wt)− f t(u))] ≤ R(u)/α+ ηML2/α+ 14L||S|| √ M/α2, (17)\nwhere ‖S‖ denote the diameter of the convex set S and L denotes an upper bound on the Lipshitz parameter of all the functions ft.\nProof We can write\nE[ Tstop∑ t=1 (f t(wt)− f t(u))]\n= E [M/α∑ t=1 (f t(wt)− f t(u)) ] − ( E[ M/α∑ t=1 (f t(wt)− f t(u))]− E[ Tstop∑ t=1 (f t(wt)− f t(u))] ) ≤ E[ M/α∑ t=1 (f t(wt)− f t(u))] + L||S|| × E[|Tstop −M/α|],\nwhere the last step follows from the fact that for any two u, v ∈ S we have |f(u) − f(v)| ≤ L||S||. The first term above can be bounded using Lemma 5. We thus need to upper-bound the expected value of ||Tstop − M/α||. As Bτ ’s are Bernoulli(α) random variables, we expect that Tstop concentrates around M/α. By using the multiplicative Chernoff bound we have\nPr(Tstop ≥M/α+ β) = Pr( M/α+β∑ τ=1 Bτ ≤M)\n≤ Pr( M/α+β∑ τ=1 Bτ ≤ (M + αβ)(1− αβ M + αβ )) ≤ exp(− (αβ) 2\n3(M + αβ) ).\nSimilarly, we can show that\nPr(Tstop ≤M/α− β) ≤ exp(− (αβ)2\n3(M − αβ) ).\nWe thus obtain,\nE[|Tstop −M/α|] ≤ M/α∑ j=0 Pr(Tstop ≤M/α− j) + ∞∑ j=0 Pr(Tstop ≥M/α+ j)\n≤ M/α∑ j=0 exp(− (αj) 2 3(M − αj) ) + ∞∑ j=0 exp(− (αj) 2 3(M + αj) )\n≤ 2 ∞∑ j=0 exp(− (αj) 2 3(M + αj) )\n≤ 2 ∞∑ k=0 √ M/α2 exp(− Mk 2 3(M + k √ M) )\n≤ 2 √ M/α2 ∞∑ k=0 exp(−k 6 )\n≤ 14 √ M/α2.\nStep 3. Putting things together When the algorithm run by EXPj is using the doubling trick, for each round (with a block of size M ), the algorithm needs to be given M feedbacks (from the forecaster) until it switches to the next round (i.e. it doubles the block-length and restarts the algorithm). Therefore, due to the fact that feedback from the forecaster is sent with independent probability α, the total time needed for the algorithm to switch to the next round is as the one given in Lemma 6. As a result, the regret suffered in the current round is upper-bounded O( √ M/α2) (from Lemma 6). Note here that the time spent\nin each round to give M feedbacks to the algorithm (i.e. Tstop in Lemma 6) is roughly M/α. Now, assume that the total time taken by the algorithm is T . The algorithm (which is given feedback with probability α and plays according to the doubling trick) will be given feedback in Tα time units (on average). As a result, it is not hard to see that total regret (after summing up over all the rounds played by the algorithm and using Jensen) becomes √ T/α. Hence, the proposition is proved also for the OMD algorithms with regret O( √ T )."
    }, {
      "heading" : "Appendix C. Proof of Theorem 3",
      "text" : "In this section, we provide the proof of Theorem 3 for the no-regret guarantees of our algorithm LEARNEXP. We follow a step by step approach, beginning with the bounds on external regret of LEARNEXP.\nStep 1. Bounds on external regret of LEARNEXP By directly using the bounds of EXP3 algorithm, cf. Theorem 3.1 from Auer et al. (2002), we can state the following bounds on the external regret of our algorithm LEARNEXP against any expert EXPk where k ∈ [N ]. Note that these bounds given by the external regret are only w.r.t. to the post hoc sequence of actions performed and losses observed during the execution of the algorithm\nT∑ t=1 E [ lt ( πit(x t,Htit,LEARNEXP) )] − E [ T∑ t=1 lt ( πk(x t,Htk,LEARNEXP) )] ≤ c · η · T + (logN) ·N η\n(18)\nwhere c is a constant given by c = e− 1.\nStep2. No-regret and smooth learning dynamics of the experts We note that during the execution of LEARNEXP in Algorithm 2, we have sparse feedbacks whereby the experts receive feedback instances sporadically at rate defined by α = ηN , cf. Section 4. Hence, by definition, we have Htj,LEARNEXP ≡ Htj,α ∀j ∈ [N ] where α = η N , cf. Section 4. By definition, the no-regret smooth learning dynamics of the expert k guarantees:\nE [ T∑ t=1 lt ( πk(x t,Htk,LEARNEXP) )] − E [ T∑ t=1 lt ( πk(x t,HTk,1) )] ≤ O ( T · ( α · T )βk−1) = O (T βk ·N1−βk η1−βk ) , (19)\nwhere βk is the parameter defining the rate of growth of regret, cf. Section 4.\nStep3. Putting it together Let us rewrite the regret of the algorithm, copying from Equation 2:\nREG(T, LEARNEXP) := T∑ t=1 E [ lt ( πit(x t,Htit,LEARNEXP) )] − min j∈[N ] E [ T∑ t=1 lt ( πj(x t,HTj,1) )]\n(20)\nCombining Eq.18 and Eq.19 from above, and using the definition of REG from Equation 20 above, we get:\nREG(T, LEARNEXP) ≤ O ( η · T + (logN) ·N\nη + T βk ·N1−βk η1−βk\n) (21)\nStep4. Optimizing η Next, we will optimize the value of η in terms of T and N . Note that EXPk above corresponds to any expert. Hence, let us set k = j∗ where j∗ corresponds to the best expert EXPj∗ that we want to compete against. As per assumptions of the theorem, the best expert indeed has no-regret smooth learning dynamics with βj∗ ∈ [0, 1]. Stating this in terms k = j∗, we can write down the regret as follows:\nREG(T, LEARNEXP) ≤ O ( η · T + (logN) ·N\nη + T βj∗ ·N1−βj∗ η1−βj∗\n) (22)\nHowever, note that algorithm doesn’t know βj∗ and hence cannot directly optimize the value of η. As per the theorem statement, the LEARNEXP is invoked with input β ∈ [0, 1] such that β ≥ βj∗ .\nStep4.1 Optimizing η for known βj∗ , i.e., β = βj∗ To begin with, let us first optimize η for case when β = βj∗ . In order to find the optimal dependency of η on T , we set η ∼ T−z , and the value z will be found to minimize the external regret. By this choice of η, the following terms stated as the powers of T appear in (22):\n{T 1−z, T z, T z+βj∗ ·(1−z)} (23)\nSolving for optimal value of z to minimize the power of T in the leading term, we get z = 1−βj∗2−βj∗ . Next, we find the optimal dependency of η on N . Note that, when β = 0, we have optimal dependency of η on N as (N · log(N)) 1 2 . In general, the optimal dependency of η to N can be found by setting η ∼ N z , which gives us from (22) the following terms stated as the powers of N , where only the leading terms w.r.t. T are kept:\n{N z, N (1−βj∗ )·(1−z)} (24)\nSolving for optimal value of z to minimize the power of T in the leading term, we get z = 1−βj∗2−βj∗ . For any βj∗ ∈ [0, 1], we can thus write the optimal value of η as:\nη = T − 1−βj∗ 2−βj∗ ·N 1−βj∗ 2−βj∗ · (logN)( 1 2 ·1{βj∗=0}) (25)\nBy keeping only the leading term of T , we can write this as follows:\nREG(T, LEARNEXP) ≤ O ( T 1 2−βj∗ ·N 1 2−βj∗ · (logN)( 1 2 ·1{βj∗=0}) ) (26)\nStep4.2 Optimizing η for unknown βj∗ , i.e., β ≥ βj∗ When βj∗ is not known exactly, and β only upper bounds βj∗ , we can still optimize η w.r.t. β to get the same η as stated above, replacing βj∗ by βj (note that 1/(2− β) is increasing in β). By keeping only the leading term of T , we can write the regret as follows:\nREG(T, LEARNEXP) ≤ O ( T 1 2−β ·N 1 2−β · (logN)( 1 2 ·1{β=0}) ) (27)\nThis gives us the desired bound stated in Theorem 3."
    } ],
    "references" : [ {
      "title" : "Taming the monster: A fast and simple algorithm for contextual bandits",
      "author" : [ "Alekh Agarwal", "Daniel Hsu", "Satyen Kale", "John Langford", "Lihong Li", "Robert Schapire" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Agarwal et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Agarwal et al\\.",
      "year" : 2014
    }, {
      "title" : "Corralling a band of bandit algorithms",
      "author" : [ "Alekh Agarwal", "Haipeng Luo", "Behnam Neyshabur", "Robert E. Schapire" ],
      "venue" : null,
      "citeRegEx" : "Agarwal et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Agarwal et al\\.",
      "year" : 2016
    }, {
      "title" : "Online bandit learning against an adaptive adversary: from regret to policy regret",
      "author" : [ "Raman Arora", "Ofer Dekel", "Ambuj Tewari" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Arora et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Arora et al\\.",
      "year" : 2012
    }, {
      "title" : "The nonstochastic multiarmed bandit problem",
      "author" : [ "Peter Auer", "Nicolo Cesa-Bianchi", "Yoav Freund", "Robert E Schapire" ],
      "venue" : "SIAM Journal on Computing,",
      "citeRegEx" : "Auer et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Auer et al\\.",
      "year" : 2002
    }, {
      "title" : "Adaptive routing with end-to-end feedback: Distributed learning and geometric approaches",
      "author" : [ "Baruch Awerbuch", "Robert D Kleinberg" ],
      "venue" : "In STOC,",
      "citeRegEx" : "Awerbuch and Kleinberg.,? \\Q2004\\E",
      "shortCiteRegEx" : "Awerbuch and Kleinberg.",
      "year" : 2004
    }, {
      "title" : "Online choice of active learning algorithms",
      "author" : [ "Yoram Baram", "Ran El-Yaniv", "Kobi Luz" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Baram et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Baram et al\\.",
      "year" : 2004
    }, {
      "title" : "Partial monitoring – Classification, regret bounds, and algorithms",
      "author" : [ "Gábor Bartók", "Dean P Foster", "Dávid Pál", "Alexander Rakhlin", "Csaba Szepesvári" ],
      "venue" : "Mathematics of Operations Research,",
      "citeRegEx" : "Bartók et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Bartók et al\\.",
      "year" : 2014
    }, {
      "title" : "Optimal exploration-exploitation in a multi-armedbandit problem with non-stationary rewards",
      "author" : [ "Omar Besbes", "Yonatan Gur", "Assaf J. Zeevi" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Besbes et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Besbes et al\\.",
      "year" : 2014
    }, {
      "title" : "Contextual bandit algorithms with supervised learning guarantees",
      "author" : [ "Alina Beygelzimer", "John Langford", "Lihong Li", "Lev Reyzin", "Robert E Schapire" ],
      "venue" : "In AISTATS,",
      "citeRegEx" : "Beygelzimer et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Beygelzimer et al\\.",
      "year" : 2011
    }, {
      "title" : "Learning, regret minimization, and equilibria",
      "author" : [ "Avrim Blum", "Yishay Monsour" ],
      "venue" : null,
      "citeRegEx" : "Blum and Monsour.,? \\Q2007\\E",
      "shortCiteRegEx" : "Blum and Monsour.",
      "year" : 2007
    }, {
      "title" : "Regret analysis of stochastic and nonstochastic multiarmed bandit problems",
      "author" : [ "Sébastien Bubeck", "Nicolo Cesa-Bianchi" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Bubeck and Cesa.Bianchi.,? \\Q2012\\E",
      "shortCiteRegEx" : "Bubeck and Cesa.Bianchi.",
      "year" : 2012
    }, {
      "title" : "Prediction, learning, and games",
      "author" : [ "N. Cesa-Bianchi", "G. Lugosi" ],
      "venue" : null,
      "citeRegEx" : "Cesa.Bianchi and Lugosi.,? \\Q2006\\E",
      "shortCiteRegEx" : "Cesa.Bianchi and Lugosi.",
      "year" : 2006
    }, {
      "title" : "How to use expert advice",
      "author" : [ "N. Cesa-Bianchi", "Y. Freund", "D.P. Helmbold", "D. Haussler", "R. Schapire", "M. Warmuth" ],
      "venue" : "Journal of the ACM,",
      "citeRegEx" : "Cesa.Bianchi et al\\.,? \\Q1997\\E",
      "shortCiteRegEx" : "Cesa.Bianchi et al\\.",
      "year" : 1997
    }, {
      "title" : "To groupon or not to groupon: The profitability of deep discounts",
      "author" : [ "Benjamin Edelman", "Sonia Jaffe", "Scott Duke Kominers" ],
      "venue" : "Marketing Letters,",
      "citeRegEx" : "Edelman et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Edelman et al\\.",
      "year" : 2011
    }, {
      "title" : "A desicion-theoretic generalization of on-line learning and an application to boosting",
      "author" : [ "Yoav Freund", "Robert E Schapire" ],
      "venue" : "In COLT,",
      "citeRegEx" : "Freund and Schapire.,? \\Q1995\\E",
      "shortCiteRegEx" : "Freund and Schapire.",
      "year" : 1995
    }, {
      "title" : "Bandit processes and dynamic allocation indices",
      "author" : [ "John C Gittins" ],
      "venue" : "Journal of the Royal Statistical Society. Series B (Methodological),",
      "citeRegEx" : "Gittins.,? \\Q1979\\E",
      "shortCiteRegEx" : "Gittins.",
      "year" : 1979
    }, {
      "title" : "Active learning by learning",
      "author" : [ "Wei-Ning Hsu", "Hsuan-Tien Lin" ],
      "venue" : "In AAAI,",
      "citeRegEx" : "Hsu and Lin.,? \\Q2015\\E",
      "shortCiteRegEx" : "Hsu and Lin.",
      "year" : 2015
    }, {
      "title" : "Multiarmed bandits with limited expert advice",
      "author" : [ "Satyen Kale" ],
      "venue" : "In COLT, pages 107–122,",
      "citeRegEx" : "Kale.,? \\Q2014\\E",
      "shortCiteRegEx" : "Kale.",
      "year" : 2014
    }, {
      "title" : "The epoch-greedy algorithm for contextual multi-armed bandits",
      "author" : [ "J. Langford", "T. Zhang" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Langford and Zhang.,? \\Q2007\\E",
      "shortCiteRegEx" : "Langford and Zhang.",
      "year" : 2007
    }, {
      "title" : "A contextual-bandit approach to personalized news article recommendation",
      "author" : [ "Lihong Li", "Wei Chu", "John Langford", "Robert E Schapire" ],
      "venue" : "In WWW,",
      "citeRegEx" : "Li et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2010
    }, {
      "title" : "The weighted majority algorithm",
      "author" : [ "Nick Littlestone", "Manfred K. Warmuth" ],
      "venue" : "Info and Computation,",
      "citeRegEx" : "Littlestone and Warmuth.,? \\Q1994\\E",
      "shortCiteRegEx" : "Littlestone and Warmuth.",
      "year" : 1994
    }, {
      "title" : "Adaptive bandits: Towards the best history-dependent strategy",
      "author" : [ "Odalric-Ambrym Maillard", "Rémi Munos" ],
      "venue" : "In AISTATS,",
      "citeRegEx" : "Maillard and Munos.,? \\Q2011\\E",
      "shortCiteRegEx" : "Maillard and Munos.",
      "year" : 2011
    }, {
      "title" : "Tighter bounds for multi-armed bandits with expert advice",
      "author" : [ "H.B. McMahan", "M.J. Streeter" ],
      "venue" : "In COLT,",
      "citeRegEx" : "McMahan and Streeter.,? \\Q2009\\E",
      "shortCiteRegEx" : "McMahan and Streeter.",
      "year" : 2009
    }, {
      "title" : "Online learning and online convex optimization",
      "author" : [ "Shai Shalev-Shwartz" ],
      "venue" : "Foundations and Trends in Machine Learning,",
      "citeRegEx" : "Shalev.Shwartz.,? \\Q2011\\E",
      "shortCiteRegEx" : "Shalev.Shwartz.",
      "year" : 2011
    }, {
      "title" : "Actively learning hemimetrics with applications to eliciting user preferences",
      "author" : [ "Adish Singla", "Sebastian Tschiatschek", "Andreas Krause" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Singla et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Singla et al\\.",
      "year" : 2016
    }, {
      "title" : "Adapting to a changing environment: the brownian restless bandits",
      "author" : [ "Aleksandrs Slivkins", "Eli Upfal" ],
      "venue" : "In COLT, pages 343–354,",
      "citeRegEx" : "Slivkins and Upfal.,? \\Q2008\\E",
      "shortCiteRegEx" : "Slivkins and Upfal.",
      "year" : 2008
    }, {
      "title" : "Fast convergence of regularized learning in games",
      "author" : [ "Vasilis Syrgkanis", "Alekh Agarwal", "Haipeng Luo", "Robert E Schapire" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Syrgkanis et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Syrgkanis et al\\.",
      "year" : 2015
    }, {
      "title" : "Restless bandits: Activity allocation in a changing world",
      "author" : [ "P. Whittle" ],
      "venue" : "Journal of applied probability,",
      "citeRegEx" : "Whittle.,? \\Q1988\\E",
      "shortCiteRegEx" : "Whittle.",
      "year" : 1988
    }, {
      "title" : "Online convex programming and generalized infinitesimal gradient ascent",
      "author" : [ "M. Zinkevich" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Zinkevich.,? \\Q2003\\E",
      "shortCiteRegEx" : "Zinkevich.",
      "year" : 2003
    } ],
    "referenceMentions" : [ {
      "referenceID" : 14,
      "context" : "Online learning using expert advice with bandit/limited feedback is a well-studied framework to model the above-mentioned application settings (Freund and Schapire, 1995; Auer et al., 2002; Cesa-Bianchi and Lugosi, 2006; Bubeck and Cesa-Bianchi, 2012) and addresses the fundamental question of how a learning algorithm should trade-off exploration (the cost of acquiring new information) versus exploitation (acting greedily based on current information to minimize instantaneous losses).",
      "startOffset" : 143,
      "endOffset" : 251
    }, {
      "referenceID" : 3,
      "context" : "Online learning using expert advice with bandit/limited feedback is a well-studied framework to model the above-mentioned application settings (Freund and Schapire, 1995; Auer et al., 2002; Cesa-Bianchi and Lugosi, 2006; Bubeck and Cesa-Bianchi, 2012) and addresses the fundamental question of how a learning algorithm should trade-off exploration (the cost of acquiring new information) versus exploitation (acting greedily based on current information to minimize instantaneous losses).",
      "startOffset" : 143,
      "endOffset" : 251
    }, {
      "referenceID" : 11,
      "context" : "Online learning using expert advice with bandit/limited feedback is a well-studied framework to model the above-mentioned application settings (Freund and Schapire, 1995; Auer et al., 2002; Cesa-Bianchi and Lugosi, 2006; Bubeck and Cesa-Bianchi, 2012) and addresses the fundamental question of how a learning algorithm should trade-off exploration (the cost of acquiring new information) versus exploitation (acting greedily based on current information to minimize instantaneous losses).",
      "startOffset" : 143,
      "endOffset" : 251
    }, {
      "referenceID" : 10,
      "context" : "Online learning using expert advice with bandit/limited feedback is a well-studied framework to model the above-mentioned application settings (Freund and Schapire, 1995; Auer et al., 2002; Cesa-Bianchi and Lugosi, 2006; Bubeck and Cesa-Bianchi, 2012) and addresses the fundamental question of how a learning algorithm should trade-off exploration (the cost of acquiring new information) versus exploitation (acting greedily based on current information to minimize instantaneous losses).",
      "startOffset" : 143,
      "endOffset" : 251
    }, {
      "referenceID" : 13,
      "context" : "As a concrete running example, we consider the problem of learning to offer personalized deals / discount coupons to users enabling new businesses to incentivize and attract more customers (Edelman et al., 2011; Singla et al., 2016).",
      "startOffset" : 189,
      "endOffset" : 232
    }, {
      "referenceID" : 24,
      "context" : "As a concrete running example, we consider the problem of learning to offer personalized deals / discount coupons to users enabling new businesses to incentivize and attract more customers (Edelman et al., 2011; Singla et al., 2016).",
      "startOffset" : 189,
      "endOffset" : 232
    }, {
      "referenceID" : 13,
      "context" : "However, these marketplaces (experts) themselves would be learning to optimize the coupons to offer, for instance, the discount price or the type of the coupon based on historic interactions with users (Edelman et al., 2011).",
      "startOffset" : 202,
      "endOffset" : 224
    }, {
      "referenceID" : 5,
      "context" : "Baram et al. (2004); Hsu and Lin (2015)), and many more.",
      "startOffset" : 0,
      "endOffset" : 20
    }, {
      "referenceID" : 5,
      "context" : "Baram et al. (2004); Hsu and Lin (2015)), and many more.",
      "startOffset" : 0,
      "endOffset" : 40
    }, {
      "referenceID" : 3,
      "context" : "We consider an online setting similar to that of adversarial online learning using experts’ advice with bandit feedback (Auer et al., 2002).",
      "startOffset" : 120,
      "endOffset" : 139
    }, {
      "referenceID" : 3,
      "context" : ", when the experts are not learning entities, the EXP3 algorithm (Auer et al., 2002) is well-suited for this problem setting achieving the optimal regret bounds.",
      "startOffset" : 65,
      "endOffset" : 84
    }, {
      "referenceID" : 3,
      "context" : "We consider an online setting similar to that of adversarial online learning using experts’ advice with bandit feedback (Auer et al., 2002). However, to keep the presentation more general (e.g., we do not necessarily require that the sets of actions are shared across different experts), the forecaster selects / seeks advice from only one expert it at any time t (cf. Kale (2014) for more discussion about this aspect).",
      "startOffset" : 121,
      "endOffset" : 381
    }, {
      "referenceID" : 3,
      "context" : "We consider an online setting similar to that of adversarial online learning using experts’ advice with bandit feedback (Auer et al., 2002). However, to keep the presentation more general (e.g., we do not necessarily require that the sets of actions are shared across different experts), the forecaster selects / seeks advice from only one expert it at any time t (cf. Kale (2014) for more discussion about this aspect). More specifically, at time t, the forecaster selects an expert it, performs an action ait recommended by the expert i t, and incurs a loss l(ait) set by the adversary. The notion of regret. In the standard framework, i.e., when the experts are not learning entities, the EXP3 algorithm (Auer et al., 2002) is well-suited for this problem setting achieving the optimal regret bounds. However, it is important to note that the classical notion of external regret used in the literature (cf. Auer et al. (2002); Cesa-Bianchi and Lugosi (2006); Bubeck and Cesa-Bianchi (2012)) does not provide any meaningful guarantees in our setting in terms of competing against the “best” expert.",
      "startOffset" : 121,
      "endOffset" : 929
    }, {
      "referenceID" : 3,
      "context" : "We consider an online setting similar to that of adversarial online learning using experts’ advice with bandit feedback (Auer et al., 2002). However, to keep the presentation more general (e.g., we do not necessarily require that the sets of actions are shared across different experts), the forecaster selects / seeks advice from only one expert it at any time t (cf. Kale (2014) for more discussion about this aspect). More specifically, at time t, the forecaster selects an expert it, performs an action ait recommended by the expert i t, and incurs a loss l(ait) set by the adversary. The notion of regret. In the standard framework, i.e., when the experts are not learning entities, the EXP3 algorithm (Auer et al., 2002) is well-suited for this problem setting achieving the optimal regret bounds. However, it is important to note that the classical notion of external regret used in the literature (cf. Auer et al. (2002); Cesa-Bianchi and Lugosi (2006); Bubeck and Cesa-Bianchi (2012)) does not provide any meaningful guarantees in our setting in terms of competing against the “best” expert.",
      "startOffset" : 121,
      "endOffset" : 961
    }, {
      "referenceID" : 3,
      "context" : "We consider an online setting similar to that of adversarial online learning using experts’ advice with bandit feedback (Auer et al., 2002). However, to keep the presentation more general (e.g., we do not necessarily require that the sets of actions are shared across different experts), the forecaster selects / seeks advice from only one expert it at any time t (cf. Kale (2014) for more discussion about this aspect). More specifically, at time t, the forecaster selects an expert it, performs an action ait recommended by the expert i t, and incurs a loss l(ait) set by the adversary. The notion of regret. In the standard framework, i.e., when the experts are not learning entities, the EXP3 algorithm (Auer et al., 2002) is well-suited for this problem setting achieving the optimal regret bounds. However, it is important to note that the classical notion of external regret used in the literature (cf. Auer et al. (2002); Cesa-Bianchi and Lugosi (2006); Bubeck and Cesa-Bianchi (2012)) does not provide any meaningful guarantees in our setting in terms of competing against the “best” expert.",
      "startOffset" : 121,
      "endOffset" : 993
    }, {
      "referenceID" : 14,
      "context" : "Somewhat surprisingly, this hardness result holds when playing against an oblivious (non-adaptive) adversary and even if restricting the experts to be implementing some well-studied online learning algorithms, for instance, the HEDGE algorithm (Freund and Schapire, 1995).",
      "startOffset" : 244,
      "endOffset" : 271
    }, {
      "referenceID" : 11,
      "context" : "Somewhat surprisingly, this hardness result holds when playing against an oblivious (non-adaptive) adversary and even if restricting the experts to be implementing some well-studied online learning algorithms, for instance, the HEDGE algorithm (Freund and Schapire, 1995). The fundamental challenge leading to this hardness result arises from the fact that the forecaster’s selection strategy affects the feedback sequences observed by the experts which in turn alters their learning process. “Guided” feedbacks and achieving no-regret guarantees. In order to circumvent this hardness result, we consider the following practical assumption: we allow the forecaster to “guide” the learning process of the experts by filtering/blocking some of the feedbacks observed by them from the environment, i.e., the selected expert it would not learn at time t for some time steps. For instance, in the motivating application of offering personalized deals to users, the deal-aggregator site (forecaster) often primarily interacts with users on behalf of the individual daily-deal marketplaces (experts) and hence can control the flow of feedback to these marketplaces. Alternatively, we note that this process of guiding and restricting the feedback can be achieved via coordination between the forecaster and the selected expert it with a 1-bit of communication at time t. Given this additional control, we design a novel algorithm LEARNEXP for the forecaster which carefully guides the feedbacks observed by experts. We prove that LEARNEXP achieves the worst-case expected cumulative regret of O(T 1 2−β ) after T time steps against an oblivious adversary for a rich family of no-regret learning algorithms that experts may be implementing. For the special case of multiarmed bandits, algorithm LEARNEXP is equivalent to that of the well-studied EXP3 algorithm and hence matches the optimal regret bound of Θ(T 1 2 ). Connections to the existing results. Maillard and Munos (2011) studied the problem of competing against an adaptive adversary when the adversary’s reward generation policy is restricted to a pre-specified set of known models.",
      "startOffset" : 245,
      "endOffset" : 1973
    }, {
      "referenceID" : 8,
      "context" : "Bubeck and Cesa-Bianchi (2012) for a variant of the algorithm).",
      "startOffset" : 0,
      "endOffset" : 31
    }, {
      "referenceID" : 0,
      "context" : "Our work is also related to contemporary work by Agarwal et al. (2016), who study a variant of the problem tackled by Maillard and Munos (2011) and also prove a hardness result similar to that of ours.",
      "startOffset" : 49,
      "endOffset" : 71
    }, {
      "referenceID" : 0,
      "context" : "Our work is also related to contemporary work by Agarwal et al. (2016), who study a variant of the problem tackled by Maillard and Munos (2011) and also prove a hardness result similar to that of ours.",
      "startOffset" : 49,
      "endOffset" : 144
    }, {
      "referenceID" : 0,
      "context" : "Our work is also related to contemporary work by Agarwal et al. (2016), who study a variant of the problem tackled by Maillard and Munos (2011) and also prove a hardness result similar to that of ours. Note that, in applications where the experts directly receive feedback from the environment, implementing the strategies of Maillard and Munos (2011); Agarwal et al.",
      "startOffset" : 49,
      "endOffset" : 352
    }, {
      "referenceID" : 0,
      "context" : "Our work is also related to contemporary work by Agarwal et al. (2016), who study a variant of the problem tackled by Maillard and Munos (2011) and also prove a hardness result similar to that of ours. Note that, in applications where the experts directly receive feedback from the environment, implementing the strategies of Maillard and Munos (2011); Agarwal et al. (2016) would require the forecaster to communicate the probability p with which the expert it was selected at time t.",
      "startOffset" : 49,
      "endOffset" : 375
    }, {
      "referenceID" : 11,
      "context" : ", T (henceforth denoted as [T ]); for simplicity we assume that T is known in advance to the algorithm and the results in this paper can be extended to an unknown horizon via the usual doubling trick (Cesa-Bianchi and Lugosi, 2006).",
      "startOffset" : 200,
      "endOffset" : 231
    }, {
      "referenceID" : 11,
      "context" : "The feedback could be more general, for instance, receiving a binary signal of rejection or acceptance of the offered deal when an expert is implementing a dynamic pricing based algorithm via the partial monitoring framework (Cesa-Bianchi and Lugosi, 2006; Bartók et al., 2014).",
      "startOffset" : 225,
      "endOffset" : 277
    }, {
      "referenceID" : 6,
      "context" : "The feedback could be more general, for instance, receiving a binary signal of rejection or acceptance of the offered deal when an expert is implementing a dynamic pricing based algorithm via the partial monitoring framework (Cesa-Bianchi and Lugosi, 2006; Bartók et al., 2014).",
      "startOffset" : 225,
      "endOffset" : 277
    }, {
      "referenceID" : 3,
      "context" : "we will use lmax = 1 (Auer et al., 2002).",
      "startOffset" : 21,
      "endOffset" : 40
    }, {
      "referenceID" : 14,
      "context" : "We consider an oblivious (non-adaptive adversary) as is usual in the literature (Freund and Schapire, 1995; Auer et al., 2002), i.",
      "startOffset" : 80,
      "endOffset" : 126
    }, {
      "referenceID" : 3,
      "context" : "We consider an oblivious (non-adaptive adversary) as is usual in the literature (Freund and Schapire, 1995; Auer et al., 2002), i.",
      "startOffset" : 80,
      "endOffset" : 126
    }, {
      "referenceID" : 3,
      "context" : "Let us begin by looking at the classic notion of external regret used in the literature (Auer et al., 2002; Cesa-Bianchi and Lugosi, 2006; Bubeck and Cesa-Bianchi, 2012).",
      "startOffset" : 88,
      "endOffset" : 169
    }, {
      "referenceID" : 11,
      "context" : "Let us begin by looking at the classic notion of external regret used in the literature (Auer et al., 2002; Cesa-Bianchi and Lugosi, 2006; Bubeck and Cesa-Bianchi, 2012).",
      "startOffset" : 88,
      "endOffset" : 169
    }, {
      "referenceID" : 10,
      "context" : "Let us begin by looking at the classic notion of external regret used in the literature (Auer et al., 2002; Cesa-Bianchi and Lugosi, 2006; Bubeck and Cesa-Bianchi, 2012).",
      "startOffset" : 88,
      "endOffset" : 169
    }, {
      "referenceID" : 2,
      "context" : "Let us begin by looking at the classic notion of external regret used in the literature (Auer et al., 2002; Cesa-Bianchi and Lugosi, 2006; Bubeck and Cesa-Bianchi, 2012). Given that the experts are learning entities, naturally the losses incurred at any time step are dependent on the history of the forecaster’s actions as that history defines the current learning state of the individual experts. Given this subtle issue of history dependent losses, the usual notion of external regret does not provide any meaningful guarantees in terms of competing against the “best expert in hindsight” (see below for a formal definition); the bounds given by the external regret are only w.r.t. the post hoc sequence of actions performed and losses observed during the execution of the algorithm (cf. Maillard and Munos (2011); Arora et al.",
      "startOffset" : 89,
      "endOffset" : 817
    }, {
      "referenceID" : 2,
      "context" : "Maillard and Munos (2011); Arora et al. (2012); McMahan and Streeter (2009) for more discussion on this).",
      "startOffset" : 27,
      "endOffset" : 47
    }, {
      "referenceID" : 2,
      "context" : "Maillard and Munos (2011); Arora et al. (2012); McMahan and Streeter (2009) for more discussion on this).",
      "startOffset" : 27,
      "endOffset" : 76
    }, {
      "referenceID" : 14,
      "context" : "Somewhat surprisingly, we prove this hardness result when playing against an oblivious (non-adaptive) adversary and when restricting the experts to be implementing the well-studied HEDGE algorithm (Freund and Schapire, 1995).",
      "startOffset" : 197,
      "endOffset" : 224
    }, {
      "referenceID" : 14,
      "context" : "The expert EXP1 plays the HEDGE algorithm (Freund and Schapire, 1995), i.",
      "startOffset" : 42,
      "endOffset" : 69
    }, {
      "referenceID" : 3,
      "context" : "The selection strategy of the algorithm LEARNEXP is similar to the EXP family of algorithms, and in particular is equivalent to the EXP3 algorithm by Auer et al. (2002). The core idea of guiding the feedbacks observed by experts is presented in Lines 10,11, and 12.",
      "startOffset" : 150,
      "endOffset" : 169
    }, {
      "referenceID" : 23,
      "context" : "Proposition 2 A rich class of no-regret online learning algorithms based on gradient-descent style updates have smooth learning dynamics including the Online Mirror Descent family of algorithms with exact or estimated gradients (Shalev-Shwartz, 2011) and Online Convex Programming via greedy projections (Zinkevich, 2003).",
      "startOffset" : 228,
      "endOffset" : 250
    }, {
      "referenceID" : 28,
      "context" : "Proposition 2 A rich class of no-regret online learning algorithms based on gradient-descent style updates have smooth learning dynamics including the Online Mirror Descent family of algorithms with exact or estimated gradients (Shalev-Shwartz, 2011) and Online Convex Programming via greedy projections (Zinkevich, 2003).",
      "startOffset" : 304,
      "endOffset" : 321
    }, {
      "referenceID" : 3,
      "context" : "For instance, in the EXP family of algorithms (Auer et al., 2002; McMahan and Streeter, 2009; Beygelzimer et al., 2011), each expert could have access to an arbitrary context (e.",
      "startOffset" : 46,
      "endOffset" : 119
    }, {
      "referenceID" : 22,
      "context" : "For instance, in the EXP family of algorithms (Auer et al., 2002; McMahan and Streeter, 2009; Beygelzimer et al., 2011), each expert could have access to an arbitrary context (e.",
      "startOffset" : 46,
      "endOffset" : 119
    }, {
      "referenceID" : 8,
      "context" : "For instance, in the EXP family of algorithms (Auer et al., 2002; McMahan and Streeter, 2009; Beygelzimer et al., 2011), each expert could have access to an arbitrary context (e.",
      "startOffset" : 46,
      "endOffset" : 119
    }, {
      "referenceID" : 22,
      "context" : "Consequently, this framework has been used in many diverse application settings including search engine ad placement (McMahan and Streeter, 2009), personalized news article recommendation (Beygelzimer et al.",
      "startOffset" : 117,
      "endOffset" : 145
    }, {
      "referenceID" : 8,
      "context" : "Consequently, this framework has been used in many diverse application settings including search engine ad placement (McMahan and Streeter, 2009), personalized news article recommendation (Beygelzimer et al., 2011), packet routing in networks, (Awerbuch and Kleinberg, 2004), and meta-learning with different learning algorithms as the experts (Baram et al.",
      "startOffset" : 188,
      "endOffset" : 214
    }, {
      "referenceID" : 4,
      "context" : ", 2011), packet routing in networks, (Awerbuch and Kleinberg, 2004), and meta-learning with different learning algorithms as the experts (Baram et al.",
      "startOffset" : 37,
      "endOffset" : 67
    }, {
      "referenceID" : 5,
      "context" : ", 2011), packet routing in networks, (Awerbuch and Kleinberg, 2004), and meta-learning with different learning algorithms as the experts (Baram et al., 2004; Hsu and Lin, 2015).",
      "startOffset" : 137,
      "endOffset" : 176
    }, {
      "referenceID" : 16,
      "context" : ", 2011), packet routing in networks, (Awerbuch and Kleinberg, 2004), and meta-learning with different learning algorithms as the experts (Baram et al., 2004; Hsu and Lin, 2015).",
      "startOffset" : 137,
      "endOffset" : 176
    }, {
      "referenceID" : 25,
      "context" : "(Slivkins and Upfal, 2008)), thereby Slivkins and Upfal (2008) considered a type of restless bandits where the change in state is governed by a more gradual process with stochastic rewards depending upon the state.",
      "startOffset" : 0,
      "endOffset" : 26
    }, {
      "referenceID" : 9,
      "context" : "The seminal work of Littlestone and Warmuth (1994); Cesa-Bianchi et al.",
      "startOffset" : 20,
      "endOffset" : 51
    }, {
      "referenceID" : 5,
      "context" : "The seminal work of Littlestone and Warmuth (1994); Cesa-Bianchi et al. (1997) initiated the study of using expert advice for prediction problems, and Freund and Schapire (1995) introduced the algorithm HEDGE for the general problem of dynamically allocating resources among a set of options using expert advice.",
      "startOffset" : 52,
      "endOffset" : 79
    }, {
      "referenceID" : 5,
      "context" : "The seminal work of Littlestone and Warmuth (1994); Cesa-Bianchi et al. (1997) initiated the study of using expert advice for prediction problems, and Freund and Schapire (1995) introduced the algorithm HEDGE for the general problem of dynamically allocating resources among a set of options using expert advice.",
      "startOffset" : 52,
      "endOffset" : 178
    }, {
      "referenceID" : 3,
      "context" : "To tackle this, Auer et al. (2002) extended this framework to the limited feedback setting and introduced the EXP family of algorithms (EXP3, EXP4, and its variants) for multi-armed bandits and expert advice with bandit feedback.",
      "startOffset" : 16,
      "endOffset" : 35
    }, {
      "referenceID" : 3,
      "context" : "To tackle this, Auer et al. (2002) extended this framework to the limited feedback setting and introduced the EXP family of algorithms (EXP3, EXP4, and its variants) for multi-armed bandits and expert advice with bandit feedback. With limited feedback, this framework addresses the fundamental question of how a learning algorithm should trade-off exploration versus exploitation. This framework has been studied extensively by researchers in a variety of fields and the above mentioned algorithms provide minimax optimal noregret guarantees—we refer the reader to Bubeck and Cesa-Bianchi (2012) for the survey on bandit problems and monograph by Cesa-Bianchi and Lugosi (2006).",
      "startOffset" : 16,
      "endOffset" : 596
    }, {
      "referenceID" : 3,
      "context" : "To tackle this, Auer et al. (2002) extended this framework to the limited feedback setting and introduced the EXP family of algorithms (EXP3, EXP4, and its variants) for multi-armed bandits and expert advice with bandit feedback. With limited feedback, this framework addresses the fundamental question of how a learning algorithm should trade-off exploration versus exploitation. This framework has been studied extensively by researchers in a variety of fields and the above mentioned algorithms provide minimax optimal noregret guarantees—we refer the reader to Bubeck and Cesa-Bianchi (2012) for the survey on bandit problems and monograph by Cesa-Bianchi and Lugosi (2006). Furthermore, this framework is very generic and versatile to capture many complex real-world scenarios.",
      "startOffset" : 16,
      "endOffset" : 678
    }, {
      "referenceID" : 3,
      "context" : "To tackle this, Auer et al. (2002) extended this framework to the limited feedback setting and introduced the EXP family of algorithms (EXP3, EXP4, and its variants) for multi-armed bandits and expert advice with bandit feedback. With limited feedback, this framework addresses the fundamental question of how a learning algorithm should trade-off exploration versus exploitation. This framework has been studied extensively by researchers in a variety of fields and the above mentioned algorithms provide minimax optimal noregret guarantees—we refer the reader to Bubeck and Cesa-Bianchi (2012) for the survey on bandit problems and monograph by Cesa-Bianchi and Lugosi (2006). Furthermore, this framework is very generic and versatile to capture many complex real-world scenarios. For instance, in the EXP family of algorithms (Auer et al., 2002; McMahan and Streeter, 2009; Beygelzimer et al., 2011), each expert could have access to an arbitrary context (e.g., information about user preferences) that may not be shared among experts and may not be available to the algorithm. Furthermore, no statistical assumptions are needed on the process generating context or losses/rewards over time. Consequently, this framework has been used in many diverse application settings including search engine ad placement (McMahan and Streeter, 2009), personalized news article recommendation (Beygelzimer et al., 2011), packet routing in networks, (Awerbuch and Kleinberg, 2004), and meta-learning with different learning algorithms as the experts (Baram et al., 2004; Hsu and Lin, 2015). 5.2. Related Work Next, we review research work that is relevant to the problem studied in this paper. Markovian, rested, and restless bandits. The seminal work of Gittins (1979) considered Markovian bandits where each action/arm is associated with its own stochastic MDP and introduces the Gittins index to find an optimal sequential policy.",
      "startOffset" : 16,
      "endOffset" : 1759
    }, {
      "referenceID" : 3,
      "context" : "To tackle this, Auer et al. (2002) extended this framework to the limited feedback setting and introduced the EXP family of algorithms (EXP3, EXP4, and its variants) for multi-armed bandits and expert advice with bandit feedback. With limited feedback, this framework addresses the fundamental question of how a learning algorithm should trade-off exploration versus exploitation. This framework has been studied extensively by researchers in a variety of fields and the above mentioned algorithms provide minimax optimal noregret guarantees—we refer the reader to Bubeck and Cesa-Bianchi (2012) for the survey on bandit problems and monograph by Cesa-Bianchi and Lugosi (2006). Furthermore, this framework is very generic and versatile to capture many complex real-world scenarios. For instance, in the EXP family of algorithms (Auer et al., 2002; McMahan and Streeter, 2009; Beygelzimer et al., 2011), each expert could have access to an arbitrary context (e.g., information about user preferences) that may not be shared among experts and may not be available to the algorithm. Furthermore, no statistical assumptions are needed on the process generating context or losses/rewards over time. Consequently, this framework has been used in many diverse application settings including search engine ad placement (McMahan and Streeter, 2009), personalized news article recommendation (Beygelzimer et al., 2011), packet routing in networks, (Awerbuch and Kleinberg, 2004), and meta-learning with different learning algorithms as the experts (Baram et al., 2004; Hsu and Lin, 2015). 5.2. Related Work Next, we review research work that is relevant to the problem studied in this paper. Markovian, rested, and restless bandits. The seminal work of Gittins (1979) considered Markovian bandits where each action/arm is associated with its own stochastic MDP and introduces the Gittins index to find an optimal sequential policy. Note that the arm changes its state only when it is pulled, hence also termed as rested bandits. Whittle (1988) considered an extension termed restless bandits where all the arms change their reward distributions at every time step according to their associated stochastic MDP.",
      "startOffset" : 16,
      "endOffset" : 2035
    }, {
      "referenceID" : 3,
      "context" : "To tackle this, Auer et al. (2002) extended this framework to the limited feedback setting and introduced the EXP family of algorithms (EXP3, EXP4, and its variants) for multi-armed bandits and expert advice with bandit feedback. With limited feedback, this framework addresses the fundamental question of how a learning algorithm should trade-off exploration versus exploitation. This framework has been studied extensively by researchers in a variety of fields and the above mentioned algorithms provide minimax optimal noregret guarantees—we refer the reader to Bubeck and Cesa-Bianchi (2012) for the survey on bandit problems and monograph by Cesa-Bianchi and Lugosi (2006). Furthermore, this framework is very generic and versatile to capture many complex real-world scenarios. For instance, in the EXP family of algorithms (Auer et al., 2002; McMahan and Streeter, 2009; Beygelzimer et al., 2011), each expert could have access to an arbitrary context (e.g., information about user preferences) that may not be shared among experts and may not be available to the algorithm. Furthermore, no statistical assumptions are needed on the process generating context or losses/rewards over time. Consequently, this framework has been used in many diverse application settings including search engine ad placement (McMahan and Streeter, 2009), personalized news article recommendation (Beygelzimer et al., 2011), packet routing in networks, (Awerbuch and Kleinberg, 2004), and meta-learning with different learning algorithms as the experts (Baram et al., 2004; Hsu and Lin, 2015). 5.2. Related Work Next, we review research work that is relevant to the problem studied in this paper. Markovian, rested, and restless bandits. The seminal work of Gittins (1979) considered Markovian bandits where each action/arm is associated with its own stochastic MDP and introduces the Gittins index to find an optimal sequential policy. Note that the arm changes its state only when it is pulled, hence also termed as rested bandits. Whittle (1988) considered an extension termed restless bandits where all the arms change their reward distributions at every time step according to their associated stochastic MDP. Restless bandits are notoriously difficult to tackle (cf. (Slivkins and Upfal, 2008)), thereby Slivkins and Upfal (2008) considered a type of restless bandits where the change in state is governed by a more gradual process with stochastic rewards depending upon the state.",
      "startOffset" : 16,
      "endOffset" : 2322
    }, {
      "referenceID" : 3,
      "context" : "To tackle this, Auer et al. (2002) extended this framework to the limited feedback setting and introduced the EXP family of algorithms (EXP3, EXP4, and its variants) for multi-armed bandits and expert advice with bandit feedback. With limited feedback, this framework addresses the fundamental question of how a learning algorithm should trade-off exploration versus exploitation. This framework has been studied extensively by researchers in a variety of fields and the above mentioned algorithms provide minimax optimal noregret guarantees—we refer the reader to Bubeck and Cesa-Bianchi (2012) for the survey on bandit problems and monograph by Cesa-Bianchi and Lugosi (2006). Furthermore, this framework is very generic and versatile to capture many complex real-world scenarios. For instance, in the EXP family of algorithms (Auer et al., 2002; McMahan and Streeter, 2009; Beygelzimer et al., 2011), each expert could have access to an arbitrary context (e.g., information about user preferences) that may not be shared among experts and may not be available to the algorithm. Furthermore, no statistical assumptions are needed on the process generating context or losses/rewards over time. Consequently, this framework has been used in many diverse application settings including search engine ad placement (McMahan and Streeter, 2009), personalized news article recommendation (Beygelzimer et al., 2011), packet routing in networks, (Awerbuch and Kleinberg, 2004), and meta-learning with different learning algorithms as the experts (Baram et al., 2004; Hsu and Lin, 2015). 5.2. Related Work Next, we review research work that is relevant to the problem studied in this paper. Markovian, rested, and restless bandits. The seminal work of Gittins (1979) considered Markovian bandits where each action/arm is associated with its own stochastic MDP and introduces the Gittins index to find an optimal sequential policy. Note that the arm changes its state only when it is pulled, hence also termed as rested bandits. Whittle (1988) considered an extension termed restless bandits where all the arms change their reward distributions at every time step according to their associated stochastic MDP. Restless bandits are notoriously difficult to tackle (cf. (Slivkins and Upfal, 2008)), thereby Slivkins and Upfal (2008) considered a type of restless bandits where the change in state is governed by a more gradual process with stochastic rewards depending upon the state. Besbes et al. (2014) considered another type of restless bandits with stochastic reward functions, however these distributions change adversarially with a budget on the allowed variation.",
      "startOffset" : 16,
      "endOffset" : 2495
    }, {
      "referenceID" : 21,
      "context" : "As in our setting, the challenge of history-dependent expert rewards also arises in the case of non-oblivious/adaptive adversary (Maillard and Munos, 2011; Arora et al., 2012).",
      "startOffset" : 129,
      "endOffset" : 175
    }, {
      "referenceID" : 2,
      "context" : "As in our setting, the challenge of history-dependent expert rewards also arises in the case of non-oblivious/adaptive adversary (Maillard and Munos, 2011; Arora et al., 2012).",
      "startOffset" : 129,
      "endOffset" : 175
    }, {
      "referenceID" : 19,
      "context" : "Another perspective on tackling some of the applications we mentioned above is the contextual bandit framework (Li et al., 2010; Langford and Zhang, 2007; Agarwal et al., 2014).",
      "startOffset" : 111,
      "endOffset" : 176
    }, {
      "referenceID" : 18,
      "context" : "Another perspective on tackling some of the applications we mentioned above is the contextual bandit framework (Li et al., 2010; Langford and Zhang, 2007; Agarwal et al., 2014).",
      "startOffset" : 111,
      "endOffset" : 176
    }, {
      "referenceID" : 0,
      "context" : "Another perspective on tackling some of the applications we mentioned above is the contextual bandit framework (Li et al., 2010; Langford and Zhang, 2007; Agarwal et al., 2014).",
      "startOffset" : 111,
      "endOffset" : 176
    }, {
      "referenceID" : 9,
      "context" : "An orthogonal line of research studies the interaction of agents in multiplayer games where each agent uses a no-regret learning algorithm (Blum and Monsour, 2007; Syrgkanis et al., 2015).",
      "startOffset" : 139,
      "endOffset" : 187
    }, {
      "referenceID" : 26,
      "context" : "An orthogonal line of research studies the interaction of agents in multiplayer games where each agent uses a no-regret learning algorithm (Blum and Monsour, 2007; Syrgkanis et al., 2015).",
      "startOffset" : 139,
      "endOffset" : 187
    }, {
      "referenceID" : 0,
      "context" : "As in our setting, the challenge of history-dependent expert rewards also arises in the case of non-oblivious/adaptive adversary (Maillard and Munos, 2011; Arora et al., 2012). Arora et al. (2012) studied online learning with bandit feedback against a nonoblivious adversary with bounded memory and introduced the notion of policy regret instead of the usual notion of external regret.",
      "startOffset" : 156,
      "endOffset" : 197
    }, {
      "referenceID" : 0,
      "context" : "As in our setting, the challenge of history-dependent expert rewards also arises in the case of non-oblivious/adaptive adversary (Maillard and Munos, 2011; Arora et al., 2012). Arora et al. (2012) studied online learning with bandit feedback against a nonoblivious adversary with bounded memory and introduced the notion of policy regret instead of the usual notion of external regret. Maillard and Munos (2011) studied competing against adaptive adversary when the adversary’s reward generation policy is restricted to a pre-specified set of known models.",
      "startOffset" : 156,
      "endOffset" : 412
    }, {
      "referenceID" : 0,
      "context" : "As in our setting, the challenge of history-dependent expert rewards also arises in the case of non-oblivious/adaptive adversary (Maillard and Munos, 2011; Arora et al., 2012). Arora et al. (2012) studied online learning with bandit feedback against a nonoblivious adversary with bounded memory and introduced the notion of policy regret instead of the usual notion of external regret. Maillard and Munos (2011) studied competing against adaptive adversary when the adversary’s reward generation policy is restricted to a pre-specified set of known models. However, none of the frameworks of non-oblivious/adaptive adversary listed above model learning dynamics in our setting: It would require an adversary with unbounded memory to apply the results of Arora et al. (2012), and an adversary with unbounded number of models to apply the techniques of Maillard and Munos (2011).",
      "startOffset" : 156,
      "endOffset" : 774
    }, {
      "referenceID" : 0,
      "context" : "As in our setting, the challenge of history-dependent expert rewards also arises in the case of non-oblivious/adaptive adversary (Maillard and Munos, 2011; Arora et al., 2012). Arora et al. (2012) studied online learning with bandit feedback against a nonoblivious adversary with bounded memory and introduced the notion of policy regret instead of the usual notion of external regret. Maillard and Munos (2011) studied competing against adaptive adversary when the adversary’s reward generation policy is restricted to a pre-specified set of known models. However, none of the frameworks of non-oblivious/adaptive adversary listed above model learning dynamics in our setting: It would require an adversary with unbounded memory to apply the results of Arora et al. (2012), and an adversary with unbounded number of models to apply the techniques of Maillard and Munos (2011). Contextual bandits.",
      "startOffset" : 156,
      "endOffset" : 877
    }, {
      "referenceID" : 0,
      "context" : ", 2010; Langford and Zhang, 2007; Agarwal et al., 2014). We refer the reader to the paper by McMahan and Streeter (2009) for more discussion on the connection between the framework of contextual bandits and learning using expert advice with bandit feedback.",
      "startOffset" : 34,
      "endOffset" : 121
    }, {
      "referenceID" : 14,
      "context" : "The expert EXP1 plays the HEDGE algorithm (Freund and Schapire, 1995), i.",
      "startOffset" : 42,
      "endOffset" : 69
    }, {
      "referenceID" : 23,
      "context" : "1 in Shalev-Shwartz (2011), w̃τ = wτ , and the fact that R(·)/α is a 1/(ηα)-strongly convex function, we obtain",
      "startOffset" : 5,
      "endOffset" : 27
    }, {
      "referenceID" : 3,
      "context" : "1 from Auer et al. (2002), we can state the following bounds on the external regret of our algorithm LEARNEXP against any expert EXPk where k ∈ [N ].",
      "startOffset" : 7,
      "endOffset" : 26
    } ],
    "year" : 2017,
    "abstractText" : "In this paper, we study a variant of the framework of online learning using expert advice with limited/bandit feedback. We consider each expert as a learning entity, seeking to more accurately reflecting certain real-world applications. In our setting, the feedback at any time t is limited in a sense that it is only available to the expert i that has been selected by the central algorithm (forecaster), i.e., only the expert i receives feedback from the environment and gets to learn at time t. We consider a generic black-box approach whereby the forecaster does not control or know the learning dynamics of the experts apart from knowing the following no-regret learning property: the average regret of any expert j vanishes at a rate of at leastO(t j ) with tj learning steps where β ∈ [0, 1] is a parameter. In the spirit of competing against the best action in hindsight in multi-armed bandits problem, our goal here is to be competitive w.r.t. the cumulative losses the algorithm could receive by following the policy of always selecting one expert. We prove the following hardness result: without any coordination between the forecaster and the experts, it is impossible to design a forecaster achieving no-regret guarantees. In order to circumvent this hardness result, we consider a practical assumption allowing the forecaster to “guide” the learning process of the experts by filtering/blocking some of the feedbacks observed by them from the environment, i.e., not allowing the selected expert i to learn at time t for some time steps. Then, we design a novel no-regret learning algorithm LEARNEXP for this problem setting by carefully guiding the feedbacks observed by experts. We prove that LEARNEXP achieves the worst-case expected cumulative regret ofO(T 1 2−β ) after T time steps and matches the regret bound of Θ(T 1 2 ) for the special case of multi-armed bandits.",
    "creator" : "LaTeX with hyperref package"
  }
}