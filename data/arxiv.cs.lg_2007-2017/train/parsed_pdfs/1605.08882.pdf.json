{
  "name" : "1605.08882.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Optimal Learning for Multi-pass Stochastic Gradient Methods",
    "authors" : [ "Junhong Lin", "Lorenzo Rosasco" ],
    "emails" : [ "jhlin5@hotmail.com", "lrosasco@mit.edu" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Modern machine learning applications require computational approaches that are at the same time statistically accurate and numerically efficient [1]. This has motivated a recent interest in stochastic gradient methods (SGM), since on the one hand they enjoy good practical performances, especially in large scale scenarios, and on the other hand they are amenable to theoretical studies. In particular, unlike other learning approaches, such as empirical risk minimization or Tikhonov regularization, theoretical results on SGM naturally integrate statistical and computational aspects.\nMost generalization studies on SGM consider the case where only one pass over the data is allowed and the step-size is appropriately chosen, [2, 3, 4, 5, 6, 7] (possibly considering averaging [8]). In particular, recent works show how the step-size can be seen to play the role of a regularization parameter whose choice controls the bias and variance properties of the obtained solution [4, 5, 6]. These latter works show that balancing these contributions, it is possible to derive a step-size choice leading to optimal learning bounds. Such a choice typically depends on some unknown properties of the data generating distributions and in practice can be chosen by cross-validation.\nWhile processing each data point only once is natural in streaming/online scenarios, in practice SGM is often used as a tool for processing large data-sets and multiple passes over the data are typically considered. In this case, the number of passes over the data, as well as the step-size, need then to be determined. While the role of multiple passes is well understood if the goal is empirical risk minimization [9], its effect with respect to generalization is less clear and a few recent works have recently started to tackle this question. In particular, results in this direction have been derived in [10] and [11]. The former work considers a general stochastic optimization\nar X\niv :1\n60 5.\n08 88\n2v 1\n[ cs\n.L G\n] 2\n8 M\nay 2\nsetting and studies stability properties of SGM allowing to derive convergence results as well as finite sample bounds. The latter work, restricted to supervised learning, further develops these results to compare the respective roles of step-size and number of passes, and show how different parameter settings can lead to optimal error bounds. In particular, it shows that there are two extreme cases: one between the step-size or the number of passes is fixed a priori, while the other one acts as a regularization parameter and needs to be chosen adaptively. The main shortcoming of these latter results is that they are in the worst case, in the sense that they do not consider the possible effect of capacity assumptions [12, 13] shown to lead to faster rates for other learning approaches such as Tikhonov regularization. Further, these results do not consider the possible effect of mini-batches, rather than a single point in each gradient step [14, 15, 16, 17]. This latter strategy is often considered especially for parallel implementation of SGM.\nThe study in this paper, fills in these gaps in the case where the loss function is the least squares loss. We consider a variant of SGM for least squares, where gradients are sampled uniformly at random and mini-batches are allowed. The number of passes, the step-size and the mini-batch size are then parameters to be determined. Our main results highlight the respective roles of these parameters and show how can they be chosen so that the corresponding solutions achieve optimal learning errors. In particular, we show for the first time that multipass SGM with early stopping and a universal step-size choice can achieve optimal learning bounds, matching those of ridge regression [18, 13]. Further, our analysis shows how the minibatch size and the step-size choice are tightly related. Indeed, larger mini-batch sizes allow to consider larger step-sizes while keeping the optimal learning bounds. This result could give an insight on how to exploit mini-batches for parallel computations while preserving optimal statistical accuracy. Finally we note that a recent work [19] is tightly related to the analysis in the paper. The generalization properties of a multi-pass incremental gradient are analyzed in [19], for a cyclic, rather than a stochastic, choice of the gradients and with no mini-batches. The analysis in this latter case appears to be harder and results in [19] give good learning bounds only in restricted setting and considering iterates rather than the excess risk. Compared to [19] our results show how stochasticity can be exploited to get faster capacity dependent rates and analyze the role of mini-batches.\nThe rest of this paper is organized as follows. Section 2 introduces the learning setting and the SGM algorithm. Main results with discussions and proof sketches are presented in Section 3. Finally, simple numerical simulations are given in Section 4 to complement our theoretical results.\nNotation For any a, b ∈ R, a ∨ b denotes the maximum of a and b. N is the set of all positive integers. For any T ∈ N, [T ] denotes the set {1, · · · , T}. For any two positive sequences {at}t∈[T ] and {bt}t∈[T ], the notation at . bt for all t ∈ [T ] means that there exists a positive constant C ≥ 0 such that C is independent of t and that at ≤ Cbt for all t ∈ [T ]."
    }, {
      "heading" : "2 Learning with SGM",
      "text" : "We begin by introducing the learning setting we consider, and then describe the SGM learning algorithm. Following [19], the formulation we consider is close to the setting of functional regression, and covers the reproducing kernel Hilbert space (RKHS) setting as special cases. In\nparticular, it reduces to standard linear regression for finite dimensions."
    }, {
      "heading" : "2.1 Learning Problems",
      "text" : "Let H be a separable Hilbert space, with inner product and induced norm denoted by 〈·, ·〉H and ‖ · ‖H , respectively. Let the input space X ⊆ H and the output space Y ⊆ R. Let ρ be an unknown probability measure on Z = X × Y, ρX(·) the induced marginal measure on X, and ρ(·|x) the conditional probability measure on Y with respect to x ∈ X and ρ.\nConsidering the square loss function, the problem under study is the minimization of the\nrisk,\ninf ω∈H E(ω), E(ω) = ∫ X×Y (〈ω, x〉H − y)2dρ(x, y), (1)\nwhen the measure ρ is known only through a sample z = {zi = (xi, yi)}mi=1 of size m ∈ N, independently and identically distributed (i.i.d.) according to ρ. In the following, we measure the quality of an approximate solution ω̂ ∈ H (an estimator) considering the excess risk, i.e.,\nE(ω̂)− inf ω∈H E(ω). (2)\nThroughout this paper, we assume that there exists a constant κ ∈ [1,∞[, such that\n〈x, x′〉H ≤ κ2, ∀x, x′ ∈ X. (3)"
    }, {
      "heading" : "2.2 Stochastic Gradient Method",
      "text" : "We study the following SGM (with mini-batches).\nAlgorithm 1. Let b ∈ [m]. Given any sample z, the b-minibatch stochastic gradient method is defined by ω1 = 0 and\nωt+1 = ωt − ηt 1\nb bt∑ i=b(t−1)+1 (〈ωt, xji〉H − yji)xji , t = 1, . . . , T, (4)\nwhere {ηt > 0} is a step-size sequence. Here, j1, j2, · · · , jbT are independent and identically distributed (i.i.d.) random variables from the uniform distribution on [m] 1.\nDifferent choices for the (mini-)batch size b can lead to different algorithms. In particular, for b = 1, the above algorithm corresponds to a simple SGM, while for b = m, it is a stochastic version of the batch gradient descent. The aim of this paper is to derive excess risk bounds for the above algorithm under appropriate assumptions. Throughout this paper, we assume that {ηt}t is non-increasing, and T ∈ N with T ≥ 3. We denote by Jt the set {jl : l = b(t− 1) + 1, · · · , bt} and by J the set {jl : l = 1, · · · , bT}."
    }, {
      "heading" : "3 Main Results with Discussions",
      "text" : "In this section, we first state some basic assumptions. Then, we present and discuss our main results.\n1Note that, the random variables j1, · · · , jbT are conditionally independent given the sample z."
    }, {
      "heading" : "3.1 Assumptions",
      "text" : "We first make the following assumption.\nAssumption 1. There exists constants M ∈]0,∞[ and v ∈]1,∞[ such that∫ Y y2ldρ(y|x) ≤ l!M lv, ∀l ∈ N, (5) ρX-almost surely.\nAssumption (5) is related to a moment hypothesis on |y|2. It is weaker than the often considered bounded output assumption, and trivially verified in binary classification problems where Y = {−1, 1}. To present our next assumption, we introduce the operator L : L2(H, ρX) → L2(H, ρX),\ndefined by L(f) = ∫ X 〈x, ·〉Hf(x)ρX(x). Under Assumption (3), L can be proved to be positive trace class operators, and hence Lζ with ζ ∈ R can be defined by using the spectrum theory [20]. The Hilbert space of square integral functions from H to R with respect to ρX , with induced norm given by ‖f‖ρ = (∫ X |f(x)|2dρX(x) )1/2 , is denoted by (L2(H, ρX), ‖ · ‖ρ). It is well known\nthat the function minimizing ∫ Z\n(f(x)− y)2dρ(z) over all measurable functions f : H → R is the regression function, which is given by\nfρ(x) = ∫ Y ydρ(y|x), x ∈ X. (6)\nDefine another Hilbert space Hρ = {f : X → R|∃ω ∈ H with f(x) = 〈ω, x〉H , ρX -almost surely}. Under Assumption 3, it is easy to see that Hρ is a subspace of L 2(H, ρX). Let fH be the projection of the regression function fρ onto the closure of Hρ in L 2(H, ρX). It is easy to see that the search for a solution of Problem (1) is equivalent to the search of a linear function from Hρ to approximate fH. From this point of view, bounds on the excess risk of a learning algorithm naturally depend on the following assumption, which quantifies how well, the target function fH can be approximated by Hρ. Assumption 2. There exist ζ > 0 and R > 0, such that ‖L−ζfH‖ρ ≤ R.\nThe above assumption is fairly standard [20, 19] in non-parametric regression. The bigger ζ is, the more stringent the assumption is, since Lζ1(L2(H, ρX)) ⊆ Lζ2(L2(H, ρX)) when ζ1 ≥ ζ2. In particular, for ζ = 0, we are assuming ‖fH‖ρ < ∞, while for ζ = 1/2, we are requiring fH ∈ Hρ, since [21, 19]\nHρ = L1/2(L2(H, ρX)).\nFinally, the last assumption relates to the capacity of the hypothesis space.\nAssumption 3. For some γ ∈]0, 1] and cγ > 0, L satisfies\ntr(L(L+ λI)−1) ≤ cγλ−γ , for all λ > 0. (7)\nThe left-hand side of (7) is the so-called effective dimension, or the degrees of freedom [12, 13]. It can be related to covering/entropy number conditions, see [21] for further details. Assumption 3 is always true for γ = 1 and cγ = κ 2, since L is a trace class operator which implies the\neigenvalues of L, denoted as σi, satisfy tr(L) = ∑ i σi ≤ κ2. This is referred as the capacity independent setting. Assumption 3 with γ ∈]0, 1] allows to derive better error rates. It is satisfied, for example, if the eigenvalues of L satisfy a polynomial decaying condition σi ∼ i−γ , or with γ = 0 if L is finite rank."
    }, {
      "heading" : "3.2 Main Results",
      "text" : "We start with the following corollary, which is a simplified version of our main results stated next.\nCorollary 3.1. Under Assumptions 2 and 3, let ζ ≥ 1/2 and |y| ≤ M ρX-almost surely for some M > 0. Consider the SGM with 1) p∗ = dm 1\n2ζ+γ e, b = 1, ηt ' 1m for all t ∈ [(p∗m)], and ω̃p∗ = ωp∗m+1. If m is large enough, with high probability2, there holds\nEJ[E(ω̃p∗)]− inf ω∈H E . m− 2ζ 2ζ+γ .\nFurthermore, the above also holds for the SGM with3 2) or p∗ = dm 1 2ζ+γ e, b = √ m, ηt ' 1√m for all t ∈ [(p∗ √ m)], and ω̃p∗ = ωp∗ √ m+1.\nIn the above, p∗ is the number of ‘passes’ over the data, which is defined as d btme at t iterations. The above result asserts that, at p∗ passes over the data, the simple SGM with fixed step-size achieves optimal learning error bounds, matching those of ridge regression [13]. Furthermore, using mini-batch allows to use a larger step-size while achieving the same optimal error bounds.\nOur main theorem of this paper is stated next, and provides error bounds for the studied algorithm. For the sake of readability, we only consider the case ζ ≥ 1/2 in a fixed step-size setting. General results in a more general setting (ηt = η1t\n−θ with 0 ≤ θ < 1, and/or the case ζ ∈]0, 1/2]) can be found in the appendix.\nTheorem 3.2. Under Assumptions 1, 2 and 3, let ζ ≥ 1/2, δ ∈]0, 1[, ηt = ηκ−2 for all t ∈ [T ], with η ≤ 18(log T+1) . If m ≥ mδ, then the following holds with probability at least 1 − δ: for all t ∈ [T ],\nEJ[E(ωt+1)]− inf ω∈H E ≤ q1(ηt)−2ζ + q2m− 2ζ 2ζ+γ (1 +m− 1 2ζ+γ ηt)2 log2 T log2\n1 δ\n+q3ηb −1(1 ∨m− 1 2ζ+γ ηt) log T.\n(8)\nHere, mδ, q1, q2 and q3 are positive constants depending on κ 2, ‖T ‖,M, v, ζ, R, cγ , γ, and mδ also on δ (which will be given explicitly in the proof).\nThere are three terms in the upper bounds of (8). The first term depends on the regularity of the target function and it arises from bounding the bias, while the last two terms result from estimating the sample variance and the computational variance (due to the random choices of the points), respectively. To derive optimal rates, it is necessary to balance these three terms. Solving this trade-off problem leads to different choices on η, T , and b, corresponding to different regularization strategies, as shown in subsequent corollaries.\nThe first corollary gives generalization error bounds for SGM, with a universal step-size\ndepending on the number of sample points.\nCorollary 3.3. Under Assumptions 1, 2 and 3, let ζ ≥ 1/2 , b = 1 and ηt ' 1m for all t ∈ [T ], where T ≤ m2. If m ≥ m0, then with probability at least 1− 1/m, there holds\nEJ[E(ωt+1)]− inf ω∈H E . {(m t )2ζ +m− 2ζ+2 2ζ+γ ( t m )2} · log4m, ∀t ∈ [T ], (9)\n2Here, ‘high probability’ refers to the sample z. 3Here, we assume that √ m is an integer.\nand in particular,\nEJ[E(ωT∗+1)]− inf ω∈H E . m− 2ζ 2ζ+γ log4m, (10)\nwhere T ∗ = dm 2ζ+γ+1 2ζ+γ e. Here, m0 is a positive integer depending only on κ, ‖T ‖, ζ and γ, and will be given explicitly in the proof.\nRemark 3.4. Ignoring the logarithmic term and letting t = pm, Eq. (9) becomes\nEJ[E(ωpm+1)]− inf ω∈H E . p−2ζ +m− 2ζ+2 2ζ+γ p2."
    }, {
      "heading" : "A smaller p may lead to a larger bias, while a larger p may lead to a larger sample error. From",
      "text" : "this point of view, p has a regularization effect.\nThe second corollary provides error bounds for SGM with a fixed mini-batch size and a fixed\nstep-size (which depend on the number of sample points). Corollary 3.5. Under Assumptions 1, 2 and 3, let ζ ≥ 1/2, b = d √ me and ηt ' 1√m for all t ∈ [T ], where T ≤ m2. If m ≥ m0, then with probability at least 1− 1/m, there holds\nEJ[E(ωt+1)]− inf ω∈H E . {(√m t )2ζ +m− 2ζ+2 2ζ+γ ( t√ m )2} log4m, ∀t ∈ [T ], (11)\nand particularly,\nEJ[E(ωT∗+1)]− inf ω∈H E . m− 2ζ 2ζ+γ log4m, (12)\nwhere T ∗ = dm 1 2ζ+γ+ 1 2 e.\nThe above two corollaries follow from Theorem 3.2 with the simple observation that the dominating terms in (8) are the terms related to the bias and the sample variance, when a small step-size is chosen. The only free parameter in (9) and (11) is the number of iterations/passes. The ideal stopping rule is achieved by balancing the two terms related to the bias and the sample variance, showing the regularization effect of the number of passes. Since the ideal stopping rule depends on the unknown parameters ζ and γ, a hold-out cross-validation procedure is often used to tune the stopping rule in practice. Using an argument similar to that in Chapter 6 from [21], it is possible to show that this procedure can achieve the same convergence rate.\nWe give some further remarks. First, the upper bound in (10) is optimal up to a logarithmic factor, in the sense that it matches the minimax lower rate in [13]. Second, according to Corollaries 3.3 and 3.5, bT ∗\nm ' m 1 2ζ+γ passes over the data are needed to obtain optimal rates in both\ncases. Finally, in comparing the simple SGM and the mini-batch SGM, Corollaries 3.3 and 3.5 show that a larger step-size is allowed to use for the latter.\nIn the next result, both the step-size and the stopping rule are tuned to obtain optimal rates for simple SGM with multiple passes. In this case, the step-size and the number of iterations are the regularization parameters.\nCorollary 3.6. Under Assumptions 1, 2 and 3, let ζ ≥ 1/2, b = 1 and ηt ' m− 2ζ 2ζ+γ for all t ∈ [T ], where T ≤ m2. If m ≥ m0, and T ∗ = dm 2ζ+1 2ζ+γ e, then (10) holds with probability at least 1− 1/m.\nRemark 3.7. If we make no assumption on the capacity, i.e., γ = 1, Corollary 3.6 recovers the result in [4] for one pass SGM.\nThe next corollary shows that for some suitable mini-batch sizes, optimal rates can be achieved with a constant step-size (which is nearly independent of the number of sample points) by early stopping.\nCorollary 3.8. Under Assumptions 1, 2 and 3, let ζ ≥ 1/2, b = dm 2ζ\n2ζ+γ e and ηt ' 1logm for all t ∈ [T ], where T ≤ m2. If m ≥ m0, and T ∗ = dm 1 2ζ+γ e, then (10) holds with probability at least 1− 1/m.\nAccording to Corollaries 3.6 and 3.8, around m 1−γ 2ζ+γ passes over the data are needed to achieve the best performance in the above two strategies. In comparisons with Corollaries 3.3 and 3.5 where around m ζ+1 2ζ+γ passes are required, the latter seems to require fewer passes over the data. However, in this case, one might have to run the algorithms multiple times to tune the step-size, or the mini-batch size.\nFinally, the last result gives generalization error bounds for ‘batch’ SGM with a constant\nstep-size (nearly independent of the number of sample points).\nCorollary 3.9. Under Assumptions 1, 2 and 3, let ζ ≥ 1/2, b = m and ηt ' 1logm for all t ∈ [T ], where T ≤ m2. If m ≥ m0, and T ∗ = dm 1 2ζ+γ e, then (10) holds with probability at least 1− 1/m.\nAs will be seen in the proof from the appendix, the above result also holds when replacing the sequence {ωt} by the sequence {νt}t generated from real batch GM in (14). In this sense, we study the gradient-based learning algorithms simultaneously."
    }, {
      "heading" : "3.3 Discussions",
      "text" : "We compare our results with previous works. For non-parametric regression with the square loss, one pass SGM has been studied in, e.g., [4, 22, 5, 6]. In particular, [4] proved capacity independent rate of order O(m− 2ζ 2ζ+1 logm) with a fixed step-size η ' m− 2ζ 2ζ+1 , and [6] derived capacity dependent error bounds of order O(m− 2min(ζ,1) 2min(ζ,1)+γ ) (when 2ζ + γ > 1) for the average. Note also that a regularized version of SGM has been studied in [5], where the derived convergence rate there is of order O(m− 2ζ\n2ζ+1 ) assuming that ζ ∈ [ 12 , 1]. In comparison with these existing convergence rates, our rates from (10) are comparable, either involving the capacity condition, or allowing a broader regularity parameter ζ (which thus improves the rates).\nMore recently, [19] studied multiple passes SGM with a fixed ordering at each pass, also called incremental gradient method. Making no assumption on the capacity, rates of order O(m− ζ ζ+1 ) (in L2(H, ρX)-norm) with a universal step-size η ' 1m are derived. In comparisons, Corollary 3.3 achieves better rates, while considering the capacity assumption. Note also that [19] proved sharp rate in H-norm for ζ ≥ 1/2 in the capacity independent case. In fact, we can extend our analysis to the H-norm for Algorithm 4. We postpone this extension to a longer version of this paper.\nThe idea of using mini-batches (and parallel implements) to speed up SGM in a general stochastic optimization setting can be found, e.g., in [14, 15, 16, 17]. Our theoretical findings, especially the interplay between the mini-batch size and the step-size, can give further insights on parallelization learning. Besides, it has been shown in [23, 15] that for one pass mini-batch SGM with a fixed step-size η ' b/ √ m and a smooth loss function, assuming the existence of at least one solution in the hypothesis space for the expected risk minimization, the convergence\nrate is of order O( √ 1/m + b/m) by considering an averaging scheme. When adapting to the learning setting we consider, this reads as that if fH ∈ Hρ, i.e., ζ = 1/2, the convergence rate for the average is O( √ 1/m+ b/m). Note that, fH does not necessarily belongs to Hρ in general. Also, our derived convergence rate from Corollary 3.5 is better, when the regularity parameter ζ is greater than 1/2, or γ is smaller than 1."
    }, {
      "heading" : "3.4 Error Decomposition",
      "text" : "The key to our proof is a novel error decomposition, which may be also used in analysing other learning algorithms. We first introduce two sequences. The population iteration is defined by µ1 = 0 and\nµt+1 = µt − ηt ∫ X (〈µt, x〉H − fρ(x))xdρX(x), t = 1, . . . , T. (13)\nThe above iterated procedure is ideal and can not be implemented in practice, since the distribution ρX is unknown in general. Replacing ρX by the empirical measure and fρ(xi) by yi, we derive the sample iteration (associated with the sample z), i.e., ν1 = 0 and\nνt+1 = νt − ηt 1\nm m∑ i=1 (〈νt, xi〉H − yi)xi, t = 1, . . . , T. (14)\nClearly, µt is deterministic and νt is a H-valued random variable depending on z. Given the sample z, the sequence {νt}t has a natural relationship with the learning sequence {ωt}t, since\nEJ[ωt] = νt. (15)\nIndeed, taking the expectation with respect to Jt on both sides of (4), and noting that ωt depends only on J1, · · · ,Jt−1 (given any z), one has\nEJt [ωt+1] = ωt − ηt 1\nm m∑ i=1 (〈ωt, xi〉H − yi)xi,\nand thus,\nEJ[ωt+1] = EJ[ωt]− ηt 1\nm m∑ i=1 (〈EJ[ωt], xi〉H − yi)xi, t = 1, . . . , T,\nwhich satisfies the iterative relationship given in (14). By an induction argument, (15) can then be proved.\nLet Sρ : H → L2(H, ρX) be the linear map defined by (Sρω)(x) = 〈ω, x〉H ,∀ω, x ∈ H. We have the following error decomposition.\nProposition 3.10. We have\nEJ[E(ωt)]− inf f∈H E(f) ≤ 2‖Sρµt − fH‖2ρ + 2‖Sρνt − Sρµt‖2ρ + EJ[‖Sρωt − Sρνt‖2]. (16)\nProof. For any ω ∈ H, we have [21, 19]\nE(ω)− inf f∈H E(f) = ‖Sρω − fH‖2ρ. (17)\nThus, E(ωt)− inff∈H E(f) = ‖Sρωt − fH‖2ρ, and\nEJ[‖Sρωt − fH‖2ρ] = EJ[‖Sρωt − Sρνt + Sρνt − fH‖2ρ] = EJ[‖Sρωt − Sρνt‖2ρ + ‖Sρνt − fH‖2ρ] + 2EJ〈Sρωt − Sρνt,Sρνt − fH〉ρ.\nUsing (15) to the above, we get EJ[‖Sρωt − fH‖2ρ] = EJ[‖Sρωt − Sρνt‖2ρ + ‖Sρνt − fH‖2ρ]. Now the proof can be finished by considering\n‖Sρνt − fH‖2ρ = ‖Sρνt − Sρµt + Sρµt − fH‖2ρ ≤ 2‖Sρνt − Sρµt‖2ρ + 2‖Sρµt − SρfH‖2ρ.\nThere are three terms in the upper bound of the error decomposition (16). We refer to the deterministic term ‖Sρµt − fH‖2ρ as the bias, the term ‖Sρνt − Sρµt‖2ρ depending on z as the sample variance, and EJ[‖Sρωt −Sρνt‖2ρ] as the computational variance. These three terms will be estimated in the appendix, see Lemma B.2, Theorem C.6 and Theorem D.9. The bound in Theorem 3.2 thus follows plugging these estimations in the error decomposition."
    }, {
      "heading" : "4 Numerical Simulations",
      "text" : "In order to illustrate our theoretical results and the error decomposition, we first performed some simulations on a simple problem. We constructed m = 100 i.i.d. training examples of the form y = fρ(xi)+ωi. Here, the regression function is fρ(x) = |x−1/2|−1/2, the input point xi is uniformly distributed in [0, 1], and ωi is a Gaussian noise with zero mean and standard deviation 1, for each i ∈ [m]. We perform three experiments with the same H, a RKHS associated with a Gaussian kernel K(x, x′) = exp(−(x − x′)2/(2σ2)) where σ = 0.2. In the first experiment, we run mini-batch SGM, where the mini-batch size b = √ m, and the step-size ηt = 1/(8 √ m). In the second experiment, we run simple SGM where the step-size is fixed as ηt = 1/(8m), while in the third experiment, we run batch GM using the fixed step-size ηt = 1/8. For each experiment,\nwe run the algorithm 50 times. For mini-batch SGM and SGM, the total error ‖Sρωt − fρ‖2L2ρ̂ , the bias ‖Sρµ̂t − fρ‖2L2ρ̂ , the sample variance ‖Sρνt − Sρµ̂t‖ 2 L2ρ̂ and the computational variance ‖Sρωt − Sρνt‖2L2ρ̂ , averaged over 50 trials, are depicted in Figures 1a and 1b, respectively. For batch GM, the total error ‖Sρνt − fρ‖2L2ρ̂ , the bias ‖Sρµ̂t − fρ‖ 2 L2ρ̂ and the sample variance ‖Sρνt − µ̂t‖2L2ρ̂ , averaged over 50 trials are depicted in Figure 1c. Here, we replace the unknown marginal distribution ρX by an empirical measure ρ̂ = 1\n2000 ∑2000 i=1 δx̂i , where each x̂i is uniformly\ndistributed in [0, 1]. From Figure 1a or 1b, we see that as the number of passes increases4, the bias decreases, while the sample error increases. Furthermore, we see that in comparisons with the bias and the sample error, the computational error is negligible. In all these experiments, the minimal total error is achieved when the bias and the sample error are balanced. These empirical results show the effects of the three terms from the error decomposition, and complement the derived bound (8), as well as the regularization effect of the number of passes over the data. Finally, we tested the simple SGM, mini-batch SGM, and batch GM, using similar step-sizes as those in the first simulation, on the BreastCancer data-set [24]. The classification errors on the training set and the testing set of these three algorithms are depicted in Figure 2. We see that all of these algorithms perform similarly, which complement the bounds in Corollaries 3.3, 3.5 and 3.9."
    }, {
      "heading" : "Acknowledgments",
      "text" : "This material is based upon work supported by the Center for Brains, Minds and Machines (CBMM), funded by NSF STC award CCF-1231216. L. R. acknowledges the financial support of the Italian Ministry of Education, University and Research FIRB project RBFR12M3AC."
    }, {
      "heading" : "A Preliminary",
      "text" : ""
    }, {
      "heading" : "A.1 Notation",
      "text" : "We first introduce some notations. For t ∈ N, ΠTt+1(L) = ∏T k=t+1(I − ηkL) for t ∈ [T − 1] and ΠTT+1(L) = I, for any operator L : H → H, where H is a Hilbert space and I denotes the identity operator on H. E[ξ] denotes the expectation of a random variable ξ. For a given bounded operator L : L2(H, ρX) → H, ‖L‖ denotes the operator norm of L, i.e., ‖L‖ = supf∈L2(H,ρX),‖f‖ρ=1 ‖Lf‖H . We will use the conventional notations on summation and production: ∏t i=t+1 = 1 and ∑t i=t+1 = 0.\nWe next introduce some auxiliary operators. Let Sρ : H → L2(H, ρX) be the linear map ω → 〈ω, ·〉H , which is bounded by κ under Assumption (3). Furthermore, we consider the adjoint operator S∗ρ : L2(H, ρX)→ H, the covariance operator T : H → H given by T = S∗ρSρ, and the operator L : L2(H, ρX) → L2(H, ρX) given by SρS∗ρ . It can be easily proved that S∗ρg = ∫ X xg(x)dρX(x) and T = ∫ X 〈·, x〉HxdρX(x). The operators T and L can be proved to be positive trace class operators (and hence compact). For any ω ∈ H, it is easy to prove the following isometry property [21]\n‖Sρω‖ρ = ‖ √ T ω‖H . (18)\nWe define the sampling operator Sx : H → Rm by (Sxω)i = 〈ω, xi〉H , i ∈ [m], where the norm ‖ · ‖Rm in Rm is the Euclidean norm times 1/m. Its adjoint operator S∗x : Rm → H, defined by 〈S∗xy, ω〉H = 〈y,Sxω〉Rm for y ∈ Rm is thus given by S∗xy = 1m ∑m i=1 yixi. Moreover, we can define the empirical covariance operator Tx : H → H such that Tx = S∗xSx. Obviously,\nTx = 1\nm m∑ i=1 〈·, xi〉Hxi.\nWith these notations, (13) and (14) can be rewritten as\nµt+1 = µt − ηt(T µt − S∗ρfρ), t = 1, . . . , T, (19)\nand\nνt+1 = νt − ηt(Txνt − S∗xy), t = 1, . . . , T, (20)\nrespectively.\nUsing the projection theorem, one can prove that\nS∗ρfρ = S∗ρfH. (21)\nIndeed, since fH is the projection of the regression function fρ onto the closer of Hρ in L 2(H, ρX), according to the projection theorem, one has\n〈fH − fρ,Sρω〉ρ = 0, ∀ω ∈ H,\nwhich can be written as\n〈S∗ρfH − S∗ρfρ, ω〉H = 0, ∀ω ∈ H,\nand thus leads to (21)."
    }, {
      "heading" : "A.2 Concentration Inequality",
      "text" : "We need the following concentration result for Hilbert space valued random variable used in Caponnetto and De Vito [13] and based on the results in Pinelis and Sakhanenko [25].\nLemma A.1. Let w1, · · · , wm be i.i.d random variables in a Hilbert space with norm ‖ · ‖. Suppose that there are two positive constants B and σ2 such that\nE[‖w1 − E[w1]‖l] ≤ 1\n2 l!Bl−2σ2, ∀l ≥ 2. (22)\nThen for any 0 < δ < 1, the following holds with probability at least 1− δ,∥∥∥∥∥ 1m m∑ k=1 wm − E[w1] ∥∥∥∥∥ ≤ 2 ( B m + σ√ m ) log 2 δ ."
    }, {
      "heading" : "In particular, (22) holds if",
      "text" : "‖w1‖l ≤ B/2 a.s., and E[‖w1‖2] ≤ σ2. (23)"
    }, {
      "heading" : "A.3 Basic Estimates",
      "text" : "Lemma A.2. Let θ ∈ [0, 1[, and t ∈ N. Then\nt1−θ\n2 ≤ t∑ k=1 k−θ ≤ t 1−θ 1− θ .\nProof. Note that\nt∑ k=1 k−θ ≤ 1 + t∑ k=2 ∫ k k−1 u−θdu = 1 + ∫ t 1 u−θdu = t1−θ − θ 1− θ ,\nwhich leads to the first part of the desired result. Similarly,\nt∑ k=1 k−θ ≥ t∑ k=1 ∫ k+1 k u−θdu = ∫ t+1 1 u−θdu = (t+ 1)1−θ − 1 1− θ ,\nand by mean value theorem, (t+ 1)1−θ − 1 ≥ (1− θ)t(t+ 1)−θ ≥ (1− θ)t1−θ/2. This proves the second part of the desired result. The proof is complete.\nLemma A.3. Let θ ∈ R and t ∈ N. Then t∑\nk=1\nk−θ ≤ tmax(1−θ,0)(1 + log t).\nProof. Note that\nt∑ k=1 k−θ = t∑ k=1 k−1k1−θ ≤ tmax(1−θ,0) t∑ k=1 k−1,\nand\nt∑ k=1 k−1 ≤ 1 + t∑ k=2 ∫ k k−1 u−1du = 1 + log t.\nLemma A.4. Let q ∈ R and t ∈ N with t ≥ 3. Then\nt−1∑ k=1 1 t− k k−q ≤ 2t−min(q,1)(1 + log t).\nProof. Note that\nt−1∑ k=1 1 t− k k−q = t−1∑ k=1 k1−q (t− k)k ≤ tmax(1−q,0) t−1∑ k=1\n1\n(t− k)k ,\nand that by Lemma A.3,\nt−1∑ k=1\n1\n(t− k)k =\n1 t t−1∑ k=1 ( 1 t− k + 1 k ) = 2 t t−1∑ k=1 1 k ≤ 2 t (1 + log t)."
    }, {
      "heading" : "B Bias",
      "text" : "In this section, we develop upper bounds for the bias, i.e., ‖Sρµt − fH‖2ρ. Towards this end, we introduce the following lemma, whose proof borrows idea from [4, 5].\nLemma B.1. Let L be a compact self-adjoint operator on a separable Hilbert space H. Assume that η1‖L‖ ≤ 1. Then for t ∈ N and any non-negative integer k ≤ t− 1,\n‖Πtk+1(L)Lζ‖ ≤\n( ζ\ne ∑t j=k+1 ηj\n)ζ . (24)\nProof. Let {σi} be the sequence of eigenvalues of L. We have\n‖Πtk+1(L)Lζ‖ = sup i t∏ l=k+1 (1− ηlσi)σζi .\nUsing the basic inequality\n1 + x ≤ ex for all x ≥ −1, (25)\nwith ηl‖L‖ ≤ 1, we get\n‖Πtk+1(L)Lζ‖ ≤ sup i exp\n{ −σi\nt∑ l=k+1 ηl } σζi\n≤ sup x≥0 exp\n{ −x\nt∑ l=k+1 ηl\n} xζ .\nThe maximum of the function g(x) = e−cxxζ( with c > 0) over R+ is achieved at xmax = ζ/c, and thus\nsup x≥0\ne−cxxζ =\n( ζ\nec\n)ζ . (26)\nUsing this inequality, one can get the desired result (24).\nWith the above lemma and Lemma A.2 from the appendix, we can derive the following result\nfor the bias.\nProposition B.2. Under Assumption 2, let η1κ 2 ≤ 1. Then, for any t ∈ N,\n‖Sρµt+1 − fH‖ρ ≤ R\n( ζ\n2 ∑t j=1 ηj\n)ζ . (27)"
    }, {
      "heading" : "In particular, if ηt = ηt",
      "text" : "−θ for all t ∈ N, with η ∈]0, κ−2] and θ ∈ [0, 1[, then\n‖Sρµt+1 − fH‖ρ ≤ Rζζη−ζt(θ−1)ζ . (28)\nProof. The result is essentially proved in [26], see also [19]. For the sake of completeness, we provide a proof here. Since µt+1 is given by (19), introducing with (21),\nµt+1 = µt − ηt(T µt − S∗ρfH). (29)\nThus,\nSρµt+1 = Sρµt − ηtSρ(T µt − S∗ρfH) = Sρµt − ηtL(Sρµt − fH). (30)\nSubtracting both sides by fH,\nSρµt+1 − fH = (I − ηtL)(Sρµt − fH).\nUsing this equality iteratively, with µ1 = 0,\nSρµt+1 − fH = −Πt1(L)fH.\nTaking the L2(H, ρX)-norm, by Assumption 2,\n‖Sρµt+1 − fH‖ρ = ‖Πt1(L)fH‖ρ ≤ ‖Πt1(L)Lζ‖R.\nBy applying Lemma B.1, we get (27). Combining (27) with Lemma A.2, we get (28). The proof is complete.\nThe following lemma gives upper bounds for the sequence {µt}t∈N in H-norm. It will be used for the estimation on the sample variance in the next section.\nLemma B.3. Under Assumption 2, the following holds for all t ∈ N: 1) If ζ ≥ 1/2,\n‖µt‖H ≤ Rκ2ζ−1. (31)\n2) If ζ ∈]0, 1/2],\n‖µt‖H ≤ κ2ζ−1 ∨\n( t∑\nk=1\nηk\n) 1 2−ζ\n. (32)\nProof. The proof for the fixed step-size can be found in [19]. Following from (29), we have\nµt+1 = (I − ηtT )µt + ηtS∗ρfH.\nApplying this relationship iteratively, and introducing with µ1 = 0, we get\nµt+1 = t∑ k=1 ηkΠ t k+1(T )S∗ρfH = t∑ k=1 ηkS∗ρΠtk+1(L)fH.\nTherefore, using Assumption 2 and the spectrum theory,\n‖µt+1‖H ≤ ∥∥∥∥∥ t∑\nk=1\nηkS∗ρΠtk+1(L)Lζ ∥∥∥∥∥R ≤ R maxσ∈]0,κ2]σ1/2+ζ t∑ k=1 ηkΠ t k+1(σ).\nIf ζ ≥ 1/2, for any σ ∈]0, κ2],\nσ1/2+ζ t∑\nk=1\nηkΠ t k+1(σ) ≤ κ2ζ−1σ t∑ k=1 ηkΠ t k+1(σ) ≤ κ2ζ−1,\nwhere for the last inequality, we used\nt∑ k=1 ηkσΠ t k+1(σ) = t∑ k=1 (1− (1− ηkσ))Πtk+1(σ) = t∑ k=1 Πtk+1(σ)− t∑ k=1 Πtk(σ) = 1−Πt1(σ).\nThus,\n‖µt+1‖H ≤ Rκ2ζ−1.\nThe case for ζ ≥ 1/2 is similar to that in [19]. We omit it. The proof is complete."
    }, {
      "heading" : "C Sample Variance",
      "text" : "In this section, we aim to estimate the sample variance, i.e., E[‖Sρµt − Sρνt‖2ρ]. Towards this end, we need some preliminary analysis. We first introduce the following key inequality, which provides the hinge idea on estimating E[‖Sρµt − Sρνt‖2ρ].\nLemma C.1. For all t ∈ [T ], we have\n‖Sρνt+1 − Sρµt+1‖ρ ≤ t∑\nk=1\nηk ∥∥∥T 12 Πtk+1(Tx)Nk∥∥∥ H , (33)\nwhere\nNk = (T µk − S∗ρfρ)− (Txµk − S∗xy), ∀k ∈ [T ]. (34)\nProof. Since νt+1 and µt+1 are given by (20) and (19), respectively,\nνt+1 − µt+1 = νt − µt + ηt { (T µt − S∗ρfρ)− (Txνt − S∗xy) }\n= (I − ηtTx)(νt − µt) + ηt { (T µt − S∗ρfρ)− (Txµt − S∗xy) } ,\nwhich is exactly\nνt+1 − µt+1 = (I − ηtTx)(νt − µt) + ηtNt.\nApplying this relationship iteratively, with ν1 = µ1 = 0,\nνt+1 − µt+1 = Πt1(Tx)(ν1 − µ1) + t∑\nk=1\nηkΠ t k+1(Tx)Nk = t∑ k=1 ηkΠ t k+1(Tx)Nk.\nBy (18), we have\n‖Sρνt+1 − Sρµt+1‖ρ = ∥∥∥∥∥ t∑\nk=1\nηkT 1 2 Πtk+1(Tx)Nk ∥∥∥∥∥ H ,\nwhich leads to the desired result (33). The proof is complete.\nThe above lemma demonstrates that in order to upper bound E[‖Sρµt − Sρνt‖2ρ], one may only need to bound ∥∥∥T 12 Πtk+1(Tx)Nk∥∥∥ H . A detailed look at this latter term indicates that one may analysis the terms T 12 Πtk+1(Tx) and Nk separately, since Ez[Nk] = 0 and the properties of the deterministic sequence {µk}k are well developed in Section B.\nLemma C.2. Under Assumptions 2 and 3 , let ζ ≥ 1/2. Then for any fixed λ > 0, with probability at least 1− δ1, the following holds for all k ∈ N : 1) If ζ ≥ 1/2,\n‖(T + λ)− 12Nk‖H ≤ 4(Rκ2ζ + √ M)\n( κ\nm √ λ +\n√ 2 √ vcγ√\nmλγ\n) log 4\nδ1 . (35)\n2) If ζ ∈]0, 1/2],\n‖(T + λ)− 12Nk‖H ≤ 4 κ κ2ζ−1 ∨( k∑\ni=1\nηi\n) 1 2−ζ +√M ( κ m √ λ + √ 2 √ vcγ√ mλγ ) log 4 δ1 . (36)\nProof. We will apply Berstein inequality from Lemma A.1 to prove the result. Bounding ∥∥∥(T + λ)− 12 (S∗ρfρ − S∗xy)∥∥∥\nH\nFor all i ∈ [m], let wi = yi(T + λI)− 1 2xi. Obviously, from the definitions of fρ (see (6)) and Sρ,\nE[w1] = Ex1 [fρ(x1)(T + λI)− 1 2x1] = (T + λI)− 1 2S∗ρfρ.\nThus,\n(T + λ)− 12 ( S∗ρfρ − S∗xy ) = 1\nm ∑ i=1 (E[wi]− wi).\nWe next estimate the constants B and σ2(w1) in (22). Note that for any l ≥ 2,\nE[‖w1 − E[w1]‖lH ] ≤ E[(‖w1‖H + E[‖w1‖H ])l].\nBy using Hölder’s inequality twice,\nE[‖w1 − E[w1]‖lH ] ≤ 2l−1E[‖w1‖lH + (E[‖w1‖H ])l] ≤ 2l−1E[‖w1‖lH + E[‖w1‖lH ]].\nThe right-hand side is exactly 2lE[‖w1‖lH ]. Therefore, by recalling the definition of w1 and expanding the integration,\nE[‖w1 − E[w1]‖lH ] ≤ 2l ∫ Y yldρ(y|x) ∫ X ‖(T + λI)− 12x‖lHdρX(x). (37)\nNote that by using Hölder’s inequality,∫ Y yldρ(y|x) ∫ X ≤ (∫ Y |y|2ldρ(y|x) ) 1 2 .\nUsing Assumption 1 to the above,∫ Y yldρ(y|x) ∫ X ≤ √ l!M lv ≤ l!( √ M)l √ v.\nPlugging the above into (37), we reach\nE[‖w1 − E[w1]‖lH ] ≤ l!(2 √ M)l √ v ∫ X ‖(T + λI)− 12x‖lHdρX(x).\nUsing Assumption (3) which imples\n‖(T + λI)− 12x‖H ≤ ‖x‖H√ λ ≤ κ√ λ ,\nwe get that\nE[‖w1 − E[w1]‖lK ] ≤ l!(2 √ M)l √ v ( κ√ λ )l−2 ∫ X ‖(T + λI)− 12x‖2HdρX(x).\nUsing the fact that E[‖ξ‖2H ] = E[tr(ξ ⊗ ξ)] = tr(E[ξ ⊗ ξ]) and E[x⊗ x] = T , we know that∫ X ‖(T + λI)− 12x‖2HdρX(x) = tr((T + λI)− 1 2 T (T + λI)− 12 ) = tr((T + λI)−1T ),\nand as a result of the above and Assumption 3,∫ X ‖(T + λI)− 12x‖2HdρX(x) ≤ cγλ−γ .\nTherefore,\nE[‖w1 − E[w1]‖lH ] ≤ l!(2 √ M)l √ v ( κ√ λ )l−2 cγλ −γ = 1 2 l! ( 2κ √ M√ λ )l−2 8M √ vcγλ −γ .\nApplying Berstein inequality with B = 2κ √ M√ λ\nand σ = √ 8M √ vcγλ−γ , we get that with proba-\nbility at least 1− δ12 , there holds∥∥∥(T + λ)− 12 (S∗ρfρ − S∗xy)∥∥∥ H = ∥∥∥∥∥ 1m∑ i=1 (E[wi]− wi) ∥∥∥∥∥ H ≤ 4 √ M ( κ m √ λ + √ 2 √ vcγ√ mλγ ) log 4 δ1 .\n(38)\nBounding ‖(T + λ)− 12 (T − Tx)‖ Let ξi = (T + λ)− 1 2xi ⊗ xi, for all i ∈ [m]. It is easy to see that E[ξi] = (T + λ)− 1 2 T , and\nthat (T + λ)− 12 (T − Tx) = 1m ∑m i=1(E[ξi]− ξi). Denote the Hilbert-Schmidt norm of a bounded operator from H to H by ‖ · ‖HS . Note that\n‖ξ1‖2HS = ‖x1‖2HTrace((T + λ)−1/2x1 ⊗ x1(T + λ)−1/2) = ‖x1‖2HTrace((T + λ)−1x1 ⊗ x1).\nBy Assumption (3),\n‖ξ1‖HS ≤ √ κ2Trace((T + λ)−1x1 ⊗ x1) ≤ √ κ2Trace(x1 ⊗ x1)/λ ≤ κ2/ √ λ,\nand furthermore, by Assumption 3,\nE[‖ξ1‖2HS ] ≤ κ2ETrace((T + λ)−1x1 ⊗ x1) = κ2Trace((T + λ)−1T ) ≤ κ2cγλ−γ .\nAccording to Lemma A.1, we get that with probability at least 1− δ12 , there holds\n‖(T + λ)− 12 (T − Tx)‖HS ≤ 2κ ( 2κ\nm √ λ + √ cγ√ mλγ\n) log 4\nδ1 . (39)\nFinally, using the triangle inequality, we have,\n‖(T + λ)− 12Nk‖H ≤ ‖(T + λ)− 1 2 (T − Tx)‖‖µk‖H + ∥∥∥(T + λ)− 12 (S∗ρfρ − S∗xy)∥∥∥ H .\nApplying Lemma B.3 to the above, introducing with (38) and (39), and then noting that κ ≥ 1 and v ≥ 1, one can prove the desired results.\nThe next lemma is borrowed from [27], derived by applying a recent Bernstein inequality\nfrom [28, 29] for a sum of random operators.\nLemma C.3. Let δ2 ∈ (0, 1) and 9κ 2 m log m δ2 ≤ λ ≤ ‖T ‖. Then the following holds with probability at least 1− δ2, ‖(Tx + λI)− 1 2 T 12 ‖ ≤ ‖(Tx + λ)− 1 2 (T + λ) 12 ‖ ≤ 2. (40)\nNow we are in a position to estimate the sample variance.\nProposition C.4. Let η1κ 2 ≤ 1 and (35) for all k ∈ [T ]. Assume that (40) holds. Then the following holds for all t ∈ [T ] : 1) If ζ ≥ 1/2,\n‖Sρνt+1 − Sρµt+1‖ρ\n≤4(Rκ2ζ + √ M)\n( κ\nm √ λ +\n√ 2 √ vcγ√\nmλγ )( t−1∑ k=1 ηk/2∑t i=k+1 ηi + λ t−1∑ k=1 ηk + √ 2κ2ηt ) log 4 δ1 . (41)\n2) If ζ ≤ 1/2,\n‖Sρνt+1 − Sρµt+1‖ρ ≤ 4 κ κ2ζ−1 ∨( k∑\ni=1\nηi\n) 1 2−ζ +√M  ×\n( t−1∑ k=1 ηk/2∑t i=k+1 ηi + λ t−1∑ k=1 ηk + √ 2κ2ηt )( κ m √ λ + √ 2 √ vcγ√ mλγ ) log 4 δ1 . (42)\nProof. For notational simplicity, we let Tλ = T + λI and Tx,λ = Tx + λI. Note that by Lemma C.1, we have (33). When k ∈ [t− 1], by rewriting T 12 Πtk+1(Tx)Nk as\nT 12 T − 1 2 x,λ T 1 2 x,λΠ t k+1(Tx)T 1 2 x,λT − 12 x,λ T 1 2 λ T − 12 λ Nk,\nwe can upper bound ‖T 12 Πtk+1(Tx)Nk‖H as\n‖T 12 Πtk+1(Tx)Nk‖H ≤ ‖T 1 2 T − 1 2 x,λ ‖‖T 1 2 x,λΠ t k+1(Tx)T 1 2 x,λ‖‖T − 12 x,λ T 1 2 λ ‖‖T − 12 λ Nk‖H .\nApplying (40), the above can be relaxed as\n‖T 12 Πtk+1(Tx)Nk‖H ≤ 4‖T 1 2 x,λΠ t k+1(Tx)T 1 2 x,λ‖‖T − 12 λ Nk‖H ,\nwhich is equivalent to\n‖T 1 2\nλ Π t k+1(Tx)Nk‖H ≤ 4‖Tx,λΠtk+1(Tx)‖‖T − 12 λ Nk‖H .\nThus, following from ηkκ 2 ≤ 1 which implies ηk‖Tx‖ ≤ 1,\n‖Tx,λΠtk+1(Tx)‖ ≤ ‖TxΠtk+1(Tx)‖+ ‖λΠtk+1(Tx)‖\n≤ ‖TxΠtk+1(Tx)‖+ λ.\nApplying Lemma B.1 with ζ = 1 to bound ‖TxΠtk+1(Tx)‖, we get\n‖Tx,λΠtk+1(Tx)‖ ≤ 1 e ∑t j=k+1 ηj + λ.\nWhen k = t,\n‖T 12 Πtk+1(Tx)Nk‖H = ‖T 1 2Nt‖H ≤ ‖T 1 2 ‖‖T\n1 2 λ ‖‖T − 12 λ Nt‖H\n≤ ‖T ‖ 12 (‖T ‖+ λ) 12 ‖T − 1 2\nλ Nt‖H .\nSince λ ≤ ‖T ‖ ≤ tr(T ) ≤ κ2, we derive\n‖T 12 Πtk+1(Tx)Nt‖H ≤ √ 2κ2‖T − 1 2 λ Nt‖H .\nFrom the above analysis, we conclude that ∑t k=1 ηk ∥∥∥T 12 Πtk+1(Tx)Nk∥∥∥ H can be upper bounded by\n≤ sup k∈[t] ‖T − 1 2 λ Nk‖H ( t−1∑ k=1 ηk/2∑t i=k+1 ηi + λ t−1∑ k=1 ηk + √ 2κ2ηt ) .\nPlugging (35) (or (36)) into the above, and then combining with (33), we get the desired bound (41) (or (42)). The proof is complete.\nSetting ηt = η1t −θ in the above proposition, with some basic estimates from Appendix A,\nwe get the following explicit bounds for the sample variance.\nProposition C.5. Let ηt = η1t −θ and (35) for all t ∈ [T ], with η1 ∈]0, κ−2] and θ ∈ [0, 1[. Assume that (40) holds. Then the following holds for all t ∈ [T ]: 1) If ζ ≥ 1/2,\n‖Sρνt+1 − Sρµt+1‖ρ\n≤4(Rκ2ζ + √ M)\n( 2λη1t 1−θ\n1− θ + log t+ 1 +\n√ 2η1κ 2\n)( κ\nm √ λ +\n√ 2 √ vcγ√\nmλγ\n) log 4\nδ1 .\n(43)\n2) If ζ ≤ 1/2,\n‖Sρνt+1 − Sρµt+1‖ρ ≤ 4 ( κ ( κ2ζ−1 ∨ ( 2η1t 1−θ\n1− θ\n) 1 2−ζ )\n+ √ M\n)\n× ( 2λη1t 1−θ\n1− θ + log t+ 1 +\n√ 2η1κ 2\n)( κ\nm √ λ +\n√ 2 √ vcγ√\nmλγ\n) log 4\nδ1 . (44)\nProof. By Proposition C.4, we have (41). Note that\nt−1∑ k=1 ηk∑t i=k+1 ηi = t−1∑ k=1 k−θ∑t i=k+1 i −θ ≤ t−1∑ k=1\nk−θ\n(t− k)t−θ .\nApplying Lemma A.4, we get\nt−1∑ k=1 ηk∑t i=k+1 ηi ≤ 2 + 2 log t,\nand by Lemma A.2,\nt−1∑ k=1 ηk = η1 t−1∑ k=1 k−θ ≤ 2η1t 1−θ 1− θ .\nIntroducing the last two estimates into (41) and (43), one can get the desired results. The proof is complete.\nIn conclusion, we get the following result for the sample variance.\nTheorem C.6. Under Assumptions 1, 2 and 3, let δ1, δ2 ∈]0, 1[ and 9κ 2 m log m δ2 ≤ λ ≤ ‖T ‖. Let ηt = η1t −θ for all t ∈ [T ], with η1 ∈]0, κ−2] and θ ∈ [0, 1[. Then with probability at least 1− δ1 − δ2, the following holds for all t ∈ [T ] : 1) if ζ ≥ 1/2, we have (43). 2) if ζ < 1/2, we have (44)."
    }, {
      "heading" : "D Computational Variance",
      "text" : "In this section, we estimate the computational variance, E[‖Sρωt − Sρνt‖2ρ]. For this, a series of lemmas is necessarily introduced."
    }, {
      "heading" : "D.1 Bounding the Empirical Risk",
      "text" : "This subsection is devoted to upper bounding EJ[Ez(ωl)]. The process relies on some tools from convex analysis and a decomposition related to the weighted averages and the last iterates from [22, 30]. We begin by introducing the following lemma, a fact based on the square loss’ special properties.\nLemma D.1. Given any sample z, and l ∈ N, let ω ∈ H be independent from Jl, then\nηl (Ez(ωl)− Ez(ω)) ≤ ‖ωl − ω‖2H − EJl‖ωl+1 − ω‖2H + η2l κ2Ez(ωl). (45)\nProof. Since ωt+1 is given be (4), subtracting both sides of (4) by ω, taking the square H-norm, and expanding the inner product,\n‖ωl+1 − ω‖2H = ‖ωl − ω‖2H + η2l b2 ∥∥∥∥∥∥ bl∑\ni=b(l−1)+1\n(〈ωl, xji〉H − yji)xji ∥∥∥∥∥∥ 2\nH\n+ 2ηl b bl∑ i=b(l−1)+1 (〈ωl, xji〉H − yji)〈ω − ωl, xji〉H .\nBy Assumption (3), ‖xji‖H ≤ κ, and thus∥∥∥∥∥∥ bl∑\ni=b(l−1)+1\n(〈ωl, xji〉H − yji)xji ∥∥∥∥∥∥ 2\nH\n≤  bl∑ i=b(l−1)+1 |〈ωl, xji〉H − yji |κ 2 ≤ κ2b bl∑\ni=b(l−1)+1\n(〈ωl, xji〉H − yji)2,\nwhere for the last inequality, we used Cauchy-Schwarz inequality. Thus,\n‖ωl+1 − ω‖2H ≤ ‖ωl − ω‖2H + η2l κ 2\nb bl∑ i=b(l−1)+1 (〈ωl, xji〉H − yji)2\n+ 2ηl b bl∑ i=b(l−1)+1 (〈ωl, xji〉H − yji)(〈ω, xji〉H − 〈ωl, xji〉H).\nUsing the basic inequality a(b− a) ≤ (b2 − a2)/2,∀a, b ∈ R,\n‖ωl+1 − ω‖2H ≤ ‖ωl − ω‖2H + η2l κ 2\nb bl∑ i=b(l−1)+1 (〈ωl, xji〉H − yji)2\n+ ηl b bl∑ i=b(l−1)+1 ( (〈ω, xji〉H − yji)2 − (〈ωl, xji〉H − yji)2 ) .\nNoting that ωl and ω are independent from Jl, and taking the expectation on both sides with respect to Jl,\nEJl‖ωl+1 − ω‖2H ≤ ‖ωl − ω‖2H + η2l κ2Ez(ωl) + ηl (Ez(ω)− Ez(ωl)) ,\nwhich leads to the desired result by rearranging terms. The proof is complete.\nUsing the above lemma and a decomposition related to the weighted averages and the last\niterates from [22, 30], we can prove the following relationship.\nLemma D.2. Let η1κ 2 ≤ 1/2 for all t ∈ N. Then\nηtEJ[Ez(ωt)] ≤ 4Ez(0) 1\nt t∑ l=1 ηl + 2κ 2 t−1∑ k=1\n1\nk(k + 1) t−1∑ i=t−k η2i EJ[Ez(ωi)]. (46)\nProof. For k = 1, · · · , t− 1,\n1 k t∑ i=t−k+1 ηiEJ[Ez(ωi)]− 1 k + 1 t∑ i=t−k ηiEJ[Ez(ωi)]\n= 1\nk(k + 1)\n{ (k + 1)\nt∑ i=t−k+1 ηiEJ[Ez(ωi)]− k t∑ i=t−k ηiEJ[Ez(ωi)]\n}\n= 1\nk(k + 1) t∑ i=t−k+1 (ηiEJ[Ez(ωi)]− ηt−kEJ[Ez(ωt−k)]).\nSumming over k = 1, · · · , t− 1, and rearranging terms, we get [30]\nηtEJ[Ez(ωt)] = 1\nt t∑ i=1 ηiEJ[Ez(ωi)] + t−1∑ k=1\n1\nk(k + 1) t∑ i=t−k+1 (ηiEJ[Ez(ωi)]− ηt−kEJ[Ez(ωt−k)]).\nSince {ηt}t is decreasing and EJ[Ez(ωt−k)] is non-negative, the above can be relaxed as\nηtEJ[Ez(ωt)] ≤ 1\nt t∑ i=1 ηiEJ[Ez(ωi)] + t−1∑ k=1\n1\nk(k + 1) t∑ i=t−k+1 ηiEJ[Ez(ωi)− Ez(ωt−k)]. (47)\nIn the rest of the proof, we will upper bound the last two terms of the above.\nTo bound the first term of the right side of (47), we apply Lemma D.1 with ω = 0 to get\nηlEJ (Ez(ωl)− Ez(0)) ≤ EJ[‖ωl‖2H − ‖ωl+1‖2H ] + η2l κ2EJ[Ez(ωl)].\nRearranging terms,\nηl(1− ηlκ2)EJ[Ez(ωl)] ≤ EJ[‖ωl‖2H − ‖ωl+1‖2H ] + ηlEz(0).\nIt thus follows from the above and ηlκ 2 ≤ 1/2 that\nηlEJ[Ez(ωl)]/2 ≤ EJ[‖ωl‖2H − ‖ωl+1‖2H ] + ηlEz(0).\nSumming up over l = 1, · · · , t, t∑ l=1 ηlEJ[Ez(ωl)]/2 ≤ EJ[‖w1‖2H − ‖ωt+1‖2H ] + Ez(0) t∑ l=1 ηl. Introducing with ω1 = 0, ‖ωt+1‖2H ≥ 0, and then multiplying both sides by 2/t, we get\n1 t t∑ l=1 ηlEJ[Ez(ωl)] ≤ 2Ez(0) 1 t t∑ l=1 ηl. (48)\nIt remains to bound the last term of (47). Let k ∈ [t − 1] and i ∈ {t − k, · · · , t}. Note that given the sample z, ωi is depending only on J1, · · · ,Ji−1 when i > 1 and ω1 = 0. Thus, we can apply Lemma D.1 with ω = ωt−k to derive\nηi (Ez(ωi)− Ez(ωt−k)) ≤ ‖ωi − ωt−k‖2H − EJi‖ωi+1 − ωt−k‖2H + η2i κ2Ez(ωi).\nTherefore,\nηiEJ [Ez(ωi)− Ez(ωt−k)] ≤ EJ[‖ωi − ωt−k‖2H − ‖ωi+1 − ωt−k‖2H ] + η2i κ2EJ[Ez(ωi)].\nSumming up over i = t− k, · · · , t,\nt∑ i=t−k ηiEJ [Ez(ωi)− Ez(ωt−k)] ≤ κ2 t∑ i=t−k η2i EJ[Ez(ωi)].\nNote that the left hand side is exactly ∑t i=t−k+1 ηiEJ [Ez(ωi)− Ez(ωt−k)]. We thus know that the last term of (47) can be upper bounded by\nκ2 t−1∑ k=1\n1\nk(k + 1) t∑ i=t−k η2i EJ[Ez(ωi)]\n= κ2 t−1∑ k=1\n1\nk(k + 1) t−1∑ i=t−k η2i EJ[Ez(ωi)] + κ2η2tEJ[Ez(ωt)] t−1∑ k=1\n1\nk(k + 1) .\nUsing the fact that\nt−1∑ k=1\n1\nk(k + 1) = t−1∑ k=1 ( 1 k − 1 k + 1 ) = 1− 1 t ≤ 1,\nand κ2ηt ≤ 1/2, we get that the last term of (47) can be bounded as\nt−1∑ k=1\n1\nk(k + 1) t∑ i=t−k+1 ηi(EJ[Ez(ωi)]− EJ[Ez(ωt−k)])\n≤ κ2 t−1∑ k=1\n1\nk(k + 1) t−1∑ i=t−k η2i EJ[Ez(ωi)] + ηtEJ[Ez(ωt)]/2.\nPlugging the above and (48) into the decomposition (47), and rearranging terms\nηtEJ[Ez(ωt)]/2 ≤ 2M2 1\nt t∑ l=1 ηl + κ 2 t−1∑ k=1\n1\nk(k + 1) t−1∑ i=t−k η2i EJ[Ez(ωi)],\nwhich leads to the desired result by multiplying both sides by 2. The proof is complete.\nWe also need to the following lemma, whose proof can be done by using an induction argu-\nment.\nLemma D.3. Let {ut}Tt=1, {At}Tt=1 and {Bt}Tt=1 be three sequences of non-negative numbers such that u1 ≤ A1 and\nut ≤ At +Bt sup i∈[t−1] ui, ∀t ∈ {2, 3, · · · , T}. (49)\nLet supt∈[T ]Bt ≤ B < 1. Then for all t ∈ [T ],\nsup k∈[t]\nut ≤ 1\n1−B sup k∈[t] Ak. (50)\nProof. When t = 1, (50) holds trivially since u1 ≤ A1 and B < 1. Now assume for some t ∈ N with 2 ≤ t ≤ T,\nsup i∈[t−1]\nui ≤ 1\n1−B sup i∈[t−1] Ai.\nThen, by (49), the above hypothesis, and Bt ≤ B, we have\nut ≤ At +Bt sup i∈[t−1]\nui ≤ At + Bt\n1−B sup i∈[t−1] Ai ≤ sup i∈[t] Ai\n( 1 +\nBt 1−B ) ≤ sup i∈[t] Ai 1 1−B .\nConsequently,\nsup k∈[t]\nut ≤ 1\n1−B sup k∈[t] Ak,\nthereby showing that indeed (50) holds for t. By mathematical induction, (50) holds for every t ∈ [T ]. The proof is complete.\nNow we can bound EJ[Ez(fk)] as follows.\nLemma D.4. Let η1κ 2 ≤ 1/2 and for all t ∈ [T ] with t ≥ 2,\n1 ηt t−1∑ k=1\n1\nk(k + 1) t−1∑ i=t−k η2i ≤ 1 4κ2 . (51)\nThen for all t ∈ [T ],\nsup k∈[t] EJ[Ez(fk)] ≤ 8Ez(0) sup k∈[t]\n{ 1\nηkk k∑ l=1 ηl\n} . (52)\nProof. By Lemma D.2, we have (46). Dividing both sides by ηt, we can relax the inequality as\nEJ[Ez(ωt)] ≤ 4Ez(0) 1\nηtt t∑ l=1 ηl + 2κ 2 1 ηt t−1∑ k=1\n1\nk(k + 1) t−1∑ i=t−k η2i sup i∈[t−1] EJ[Ez(ωi)].\nIn Lemma D.3, we let ut = EJ[Ez(ωt)], At = 4Ez(0) 1ηtt ∑t l=1 ηl and\nBt = 2κ 2 1\nηt t−1∑ k=1\n1\nk(k + 1) t−1∑ i=t−k η2i .\nCondition (51) guarantees that supt∈[T ]Bt ≤ 1/2. Thus, (50) holds, and the desired result follows by plugging with B = 1/2. The proof is complete.\nFinally, we need the following lemma to bound Ez(0), whose proof follows from applying the Bernstein Inequality from Lemma A.1.\nLemma D.5. Under Assumption 1, with probability at least 1− δ3 (δ3 ∈]0, 1[), there holds\nEz(0) ≤Mv + 2M\n( 1\nm + √ 2v√ m\n) log 2\nδ3 .\nIn particular, if m ≥ 32 log2 2δ3 , then\nEz(0) ≤ 2Mv. (53)\nProof. Following from (5),∫ Z y2ldρ ≤ 1 2 l!M l−2 · (2M2v), ∀l ∈ N. Applying Lemma A.1, with ωi = y 2 i for all i ∈ [m], B = M and σ = M √ 2v, we know that with probability at least 1− δ3, there holds\n1 m m∑ i=1 y2i − ∫ Z y2dρ ≤ 2M ( 1 m + √ 2v√ m ) log 2 δ3 .\nBy setting l = 1 in (5), ∫ Z y2dρ ≤Mv. It thus follows that\n1 m m∑ i=1 y2i ≤ ∫ Z y2dρ+ 2M ( 1 m + √ 2v√ m ) log 2 δ3 ≤Mv + 2M ( 1 m + √ 2v√ m ) log 2 δ3 ,\nwhich leads to the desired results by noting that the left-hand side is exactly Ez(0) and ν ≥ 1. The proof is complete.\nD.2 Bounding ∥∥∥T 12 Πtk+1(Tx)∥∥∥\nLemma D.6. Assume (40) holds for some λ > 0 and η1κ 2 ≤ 1. Then\n‖T 12 Πtk+1(Tx)‖2 ≤ 1∑t\ni=k+1 ηi + 4λ.\nProof. Note that we have\n‖T 12 Πtk+1(Tx)‖ ≤ ‖T 1 2 (Tx + λI)− 1 2 ‖‖(Tx + λI) 1 2 Πtk+1(Tx)‖.\nUsing (40), we can relax the above as\n‖T 12 Πtk+1(Tx)‖ ≤ 2‖(Tx + λI) 1 2 Πtk+1(Tx)‖,\nwhich leads to\n‖T 12 Πtk+1(Tx)‖2 ≤ 4‖(Tx + λI) 1 2 Πtk+1(Tx)‖2.\nSince\n‖(Tx + λI) 1 2 Πtk+1(Tx)‖2 = ‖(Tx + λI)Πtk+1(Tx)Πtk+1(Tx)‖\n≤ ‖TxΠtk+1(Tx)Πtk+1(Tx)‖+ λ = ‖T 1 2\nx Π t k+1(Tx)‖2 + λ,\nand with ηtκ 2 ≤ 1, ‖Tx‖ ≤ tr(Tx) ≤ κ2, by Lemma B.1,\n‖T 1 2\nx Π t k+1(Tx)‖2 ≤\n1 2e ∑t i=k+1 ηi ≤ 1 4 ∑t i=k+1 ηi ,\nwe thus derive the desired result. The proof is complete."
    }, {
      "heading" : "D.3 Deriving Error Bounds",
      "text" : "With Lemmas D.4 and D.6, we are ready to estimate the computational variance , EJ‖ft− gt‖2ρ, as follows.\nProposition D.7. Assume (40) holds for some λ > 0, η1κ 2 ≤ 1/2, (51) and (53). Then, we have for all t ∈ [T ],\nEJ‖Sρωt+1 − Sρνt+1‖2ρ ≤ 16Mvκ2\nb sup k∈[t]\n{ 1\nηkk k∑ l=1 ηl }( t−1∑ k=1 η2k∑t i=k+1 ηi + 4λ t−1∑ k=1 η2k + η 2 t κ 2 ) .\n(54)\nProof. Since ωt+1 and νt+1 are given by (4) and (20), respectively,\nωt+1 − νt+1 = (ωt − νt) + ηt (Txνt − S∗xy)− 1b bt∑\ni=b(t−1)+1\n(〈ωt, xji〉H − yji)xji  = (I − ηtTx)(ωt − νt) +\nηt b bt∑ i=b(t−1)+1 {(Txωt − S∗xy)− (〈ωt, xji〉H − yji)xji} .\nApplying this relationship iteratively,\nωt+1 − νt+1 = Πt1(Tx)(ω1 − ν1) + 1\nb t∑ k=1 bk∑ i=b(k−1)+1 ηkΠ t k+1(Tx)Mk,i,\nwhere we denote\nMk,i = (Txωk − S∗xy)− (〈ωk, xji〉H − yji)xji . (55)\nIntroducing with ω1 = ν1 = 0,\nωt+1 − νt+1 = 1\nb t∑ k=1 bk∑ i=b(k−1)+1 ηkΠ t k+1(Tx)Mk,i.\nTherefore,\nEJ‖Sρωt+1 − Sρνt+1‖2ρ = 1\nb2 EJ ∥∥∥∥∥∥ t∑\nk=1 bk∑ i=b(k−1)+1 ηkΠ t k+1(Tx)Mk,i ∥∥∥∥∥∥ 2\nρ\n= 1\nb2 t∑ k=1 bk∑ i=b(k−1)+1 η2kEJ ∥∥Πtk+1(Tx)Mk,i∥∥2ρ , (56)\nwhere for the last equality, we use the fact that if k 6= k′, or k = k′ but i 6= i′5, then\nEJ〈Πtk+1(Tx)Mk,i,Πtk′+1(Tx)Mk′,i′〉ρ = 0.\nIndeed, if k 6= k′, without loss of generality, we consider the case k < k′. Recalling that Mk,i is given by (55) and that given any z, fk is depending only on J1, · · · ,Jk−1, we thus have\nEJ〈Πtk+1(Tx)Mk,i,Πtk′+1(Tx)Mk′,i′〉ρ = EJ1,··· ,Jk′−1〈Π t k+1(Tx)Mk,i,Πtl+1(Tx)EJk′ [Mk′,i′ ]〉ρ = 0.\nIf k = k′ but i 6= i′, without loss of generality, we assume i < i′. By noting that ωk is depending only on J1, · · · ,Jk−1 and Mk,i is depending only on ωk and zji (given any sample z),\nEJ〈Πtk+1(Tx)Mk,i,Πtk+1(Tx)Mk,i′〉ρ = EJ1,··· ,Jk−1〈Πtk+1(Tx)Eji [Mk,i],Πtl+1(Tx)Eji′ [Mk,i′ ]〉ρ = 0.\nUsing the isometry property (18) to (56),\nEJ ∥∥Πtk+1(Tx)Mk,i∥∥2ρ = EJ ∥∥∥T 12 Πtk+1(Tx)Mk,i∥∥∥2H ≤ ∥∥∥T 12 Πtk+1(Tx)∥∥∥2 EJ ‖Mk,i‖2H ,\nand by applying the inequality E[‖ξ − E[ξ]‖2H ] ≤ E[‖ξ‖2H ],\nEJ ‖Mk,i‖2H ≤ EJ ‖(〈ωk, xji〉H − yji)xji‖ 2 H ≤ κ 2EJ[(〈ωk, xji〉H − yji)2] = κ2EJ[Ez(ωk)], 5This is possible only when b ≥ 2.\nwhere for the last inequality we use (3). Therefore,\nEJ‖Sρωt+1 − Sρνt+1‖2ρ ≤ κ2\nb t∑ k=1 η2k ∥∥∥T 12 Πtk+1(Tx)∥∥∥2 EJ[Ez(ωk)]. According to Lemma D.4, we have (52). It thus follows that\nEJ‖Sρωt+1 − Sρνt+1‖2ρ ≤ 8Ez(0)κ2\nb sup k∈[t]\n{ 1\nηkk k∑ l=1 ηl\n} t∑\nk=1\nη2k ∥∥∥T 12 Πtk+1(Tx)∥∥∥2 . Now the proof can be finished by applying Lemma D.6 which tells us that\nt∑ k=1 η2k ∥∥∥T 12 Πtk+1(Tx)∥∥∥2 = t−1∑ k=1 η2k ∥∥∥T 12 Πtk+1(Tx)∥∥∥2 + η2t ∥∥∥T 12 ∥∥∥2 ≤\nt−1∑ k=1 η2k∑t i=k+1 ηi + 4λ t−1∑ k=1 η2k + η 2 t κ 2,\nand (53) to the above. The proof is complete.\nSetting ηt = η1t −θ for some appropriate η1 and θ in the above proposition, we get the\nfollowing explicitly upper bounds for EJ‖Sρωt − Sρωt‖2ρ.\nProposition D.8. Assume (40) holds for some λ > 0 and (53). Let ηt = η1t −θ for all t ∈ [T ], with θ ∈ [0, 1[ and\n0 < η1 ≤ tmin(θ,1−θ)\n8κ2(log t+ 1) , ∀t ∈ [T ]. (57)"
    }, {
      "heading" : "Then, for all t ∈ [T ],",
      "text" : "EJ‖ωt+1 − νt+1‖2ρ ≤ 16Mvκ2\nb(1− θ)\n( 5η1t −min(θ,1−θ) + 8λη21t (1−2θ)+ ) (1 ∨ log t). (58)\nProof. We will use Proposition D.7 to prove the result. Thus, we need to verify the condition (51). Note that\nt−1∑ k=1\n1\nk(k + 1) t−1∑ i=t−k η2i = t−1∑ i=1 η2i t−1∑ k=t−i\n1\nk(k + 1) = t−1∑ i=1 η2i ( 1 t− i − 1 t ) ≤ t−1∑ i=1 η2i t− i .\nSubstituting with ηi = ηi −θ, and by Lemma A.4,\nt−1∑ k=1\n1\nk(k + 1) t−1∑ i=t−k η2i ≤ η21 t−1∑ i=1 i−2θ t− i ≤ 2η21t−min(2θ,1)(log t+ 1).\nDividing both sides by ηt (= ηt −θ), and then using (57),\n1 ηt t−1∑ k=1\n1\nk(k + 1) t−1∑ i=t−k η2i ≤ 2η1t−min(θ,1−θ)(log t+ 1) ≤ 1 4κ2 .\nThis verifies (51). Note also that by taking t = 1 in (57), for all t ∈ [T ] ,\nηtκ 2 ≤ η1κ2 ≤\n1 8κ2 ≤ 1 2 .\nWe thus can apply Proposition D.7 to derive (54). What remains is to control the right hand side of (54). Since\nt−1∑ k=1 η2k∑t i=k+1 ηi = η1 t−1∑ k=1 k−2θ∑t i=k+1 i −θ ≤ η1 t−1∑ k=1 k−2θ (t− k)t−θ ,\ncombining with Lemma A.4,\nt−1∑ k=1 η2k∑t i=k+1 ηi ≤ 2η1t−min(θ,1−θ)(log t+ 1).\nAlso, by Lemma A.2,\n1\nηkk k∑ l=1 ηl = 1 k1−θ k∑ l=1 l−θ ≤ 1 1− θ ,\nand by Lemma A.3,\nt−1∑ k=1 η2k = η 2 1 t−1∑ k=1 k−2θ ≤ η21tmax(1−2θ,0)(log t+ 1).\nIntroducing the last three estimates into (54) and using that η2t κ 2 ≤ η1t−θ by (57), we get the desired result. The proof is complete.\nCollect some of the above analysis, we get the following result for the computational variance.\nTheorem D.9. Under Assumptions 1 and 3, let δ2 ∈]0, 1[, 9κ 2 m log m δ2 ≤ λ ≤ ‖T ‖, δ3 ∈]0, 1[, m ≥ 32 log2 2δ3 , and ηt = ηt −θ for all t ∈ [T ], with θ ∈ [0, 1[ and η such that (57). Then, with probability at least 1− δ2 − δ3, (58) holds for all t ∈ [T ]."
    }, {
      "heading" : "E Deriving Total Error Bounds",
      "text" : "The purpose of this section is to derive total error bounds."
    }, {
      "heading" : "E.1 Attainable Case",
      "text" : "We have the following general theorem for ζ ≥ 1/2, with which we prove our main results stated in Section 3.\nTheorem E.1. Under Assumptions 1, 2 and 3, let ζ ≥ 1/2, T ∈ N with T ≥ 3, δ ∈]0, 1[, ηt = ηκ −2t−θ for all t ∈ [T ], with θ ∈ [0, 1[ and η such that\n0 < η ≤ t min(θ,1−θ)\n8(log t+ 1) , ∀t ∈ [T ]. (59)\nIf for some ∈]0, 1],\nm ≥ ( 18κ2\n‖T ‖ log\n( 27κ2\n‖T ‖δ\n))1/ , (60)\nthen the following holds with probability at least 1− δ: for all t ∈ [T ],\nEJ[E(ωt+1)]− inf ω∈H E(ω) ≤ q1(ηt1−θ)−2ζ + q2mγ(1− )−1(1 ∨ η2m2 −2t2−2θ)(log T )2 log2\n12\nδ\n+q3ηb −1(t−min(θ,1−θ) ∨m −1ηt(1−2θ)+) log T.\n(61)"
    }, {
      "heading" : "Here, q1 = 2R",
      "text" : "2ζ2ζ , q2 =\n800(Rκ2ζ+ √ M)2(κ/ √ ‖T ‖+ √ 2 √ vcγ/‖T ‖γ)2\n(1−θ)2 , and q3 = 208Mv 1−θ .\nProof. Let λ = ‖T ‖m −1. Clearly, λ ≤ ‖T ‖. For any A ≥ 0 and B ≥ 1, by applying (26) with ζ = 1, x = (Bm) and c = 2AB ,\nA log(Bm) = A log((Bm) ) ≤ A log\n( 2AB\ne\n) + 1\n2 m ≤ A log\n( AB ) + 1\n2 m . (62)\nUsing the above inequality with A = 9κ 2\n‖T ‖ and B = 1 δ2 , one can prove that the condition (60)\nensures that 9κ 2\nm log m δ2 ≤ λ is satisfied with δ2 = δ3 , Therefore, by Lemma C.3, (40) holds with\nprobability at least 1 − δ2. Similarly the condition (60) implies that m ≥ 32 log2 2δ3 is satisfied with δ3 = δ 3 , and thus by Lemma D.5, (53) holds with probability at least 1−δ3. Combining with Lemma C.2, by taking the union bound, we know that with probability at least 1− δ1− δ2− δ3, (40), (53) and (35) hold for all k ∈ [T ]. Now, we can apply Propositions C.5 and D.8 to get (43) and (58). Noting that by (57), √ 2η ≤ 1, and by a simple calculation, we derive from (43) that\n‖Sρνt+1 − Sρµt+1‖2ρ\n≤ 400(Rκ2ζ +\n√ M)2(κ/ √ ‖T ‖+ √ 2 √ vcγ/‖T ‖γ)2\n(1− θ)2 mγ(1− )−1(1 ∨ λ2η2κ−4t2−2θ ∨ log2 t) log2 4\nδ1\n≤ 400(Rκ2ζ +\n√ M)2(κ/ √ ‖T ‖+ √ 2 √ vcγ/‖T ‖γ)2\n(1− θ)2 mγ(1− )−1(1 ∨ η2m2 −2t2−2θ)(log T )2 log2 4 δ1 ,\nwhere for the last inequality, we used ‖T ‖ ≤ κ2. Similarly, by a simple calculation, we get from (58) that\nEJ‖Sρωt+1 − Sρνt+1‖2ρ ≤ 208Mv\nb(1− θ) (ηt−min(θ,1−θ) ∨ λη2κ−2t(1−2θ)+)(1 ∨ log t)\n≤ 208Mv b(1− θ) (ηt−min(θ,1−θ) ∨m −1η2t(1−2θ)+) log T.\nLetting δ1 = δ 3 , and introducing the above estimates and (28) into (16), we get (61). The proof is complete.\nProof of Theorem 3.2. By choosing = 1− 12ζ+γ and θ = 0 in Theorem E.1, then the condition (60) reduces to m ≥ mδ, where\nmδ =\n( 18κ2p\n‖T ‖ log\n( 27κ2p\n‖T ‖δ\n))p , p = 2ζ + γ\n2ζ + γ − 1 . (63)\nThe desired result thus follows by applying Theorem E.1.\nProof of Corollary 3.3. We use Theorem 3.2 to prove the result. We first prove that the conditions of Theorem 3.2 are true. When δ = 1/m, the condition m ≥ mδ with mδ given by (63), is equivalent to\nm1/p ≥ 18κ 2p\n‖T ‖ log\n( 27κ2p\n‖T ‖ m\n) .\nUsing (62) wit = 1/p, A = 18κ 2p ‖T ‖ and B = 27κ2p ‖T ‖ , 6 we know that\n18κ2p\n‖T ‖ log\n( 27κ2p\n‖T ‖ m\n) ≤ 18κ 2p2\n‖T ‖ log\n( 486κ4p3\n‖T ‖2\n) + 1\n2 m1/p,\nand consequently we know that the condition m ≥ mδ is met if m ≥ m0, where\nm0 =\n( 36κ2p2\n‖T ‖ log\n( 486κ4p3\n‖T ‖2\n))1/p .\nLetting ηt = ηκ −2 with η = 116m (which is ηt ' 1 m ), we know that η ≤ 1 8(log T+1) , since 3 ≤ T ≤ m2 and logm ≤ m− 1. We thus verified the conditions of Theorem 3.2. We thus have (8). By introducing with δ = 1/m, η = 116m and T ≤ m 2,\nEJ‖Sρωt+1 − fH‖2ρ . ( m−1t )−2ζ +m− 2ζ 2ζ+γ (1 ∨m− 1 2ζ+γ−1t)2 log6m+m−1(1 ∨m− 1 2ζ+γ−1t) logm. Usingm−1 ≤ m− 2ζ 2ζ+γ ≤ ( m−1t )−2ζ +m− 2ζ 2ζ+γ (m− 1 2ζ+γ−1t)2, one can prove the desired result.\n6B is always greater than 1 since κ2 ≥ ‖T ‖ and p ≥ 1.\nThe proofs for the other corollaries parallelize to the above. We thus omit."
    }, {
      "heading" : "E.2 Non Attainable Case",
      "text" : "Theorem E.2. Under Assumptions 1, 2 and 3, let ζ ≤ 1/2, T ∈ N with T ≥ 3, δ ∈]0, 1[, ηt = ηκ\n−2t−θ for all t ∈ [T ], with θ ∈ [0, 1[ and η such that (59) and for some ∈]0, 1], (60) holds. Then the following holds with probability at least 1− δ: for all t ∈ [T ],\nEJ[E(ωt+1)]− inf ω∈H E(ω) . (ηt1−θ)−2ζ +mγ(1− )−1(1 ∨ η2m2 −2t2−2θ)\n( 1 ∨ ηt1−θ )1−2ζ log2 t log2 4\nδ1\n+ηb−1(t−min(θ,1−θ) ∨m −1ηt(1−2θ)+) log T. (64)\nProof. The proof is similar to that for Theorem E.1. We include the sketch only. Similar to the proof of Theorem E.1, one can prove that with probability at least 1 − δ1 − δ2 − δ3, (40), (53) and (36) hold for all k ∈ [T ]. Now, we can apply Propositions C.5 and D.8 to get (44) and (58). Noting that by (57), √ 2η ≤ 1, and by a simple calculation, we derive from (44) that\n‖Sρνt+1 − Sρµt+1‖2ρ ≤ 400\n( κ2ζ ( 1 ∨ 2ηt 1−θ\n1−θ\n) 1 2−ζ\n+ √ M )2 (κ/ √ ‖T ‖+ √ 2 √ vcγ/‖T ‖γ)2\n(1− θ)2\n×mγ(1− )−1(1 ∨ λ2η2κ−4t2−2θ ∨ log2 t) log2 4 δ1 .\nThe rest of the proof parallelizes to that for Theorem E.1.\nRemark E.3. Letting θ = 0 in the above theorem, and ignoring the logarithmic terms, the bound (64) reads as\nEJ[E(ωt+1)]− inf ω∈H E(ω) . (ηt)−2ζ +mγ(1− )−1(1 ∨m −1ηt)2 (1 ∨ ηt)1−2ζ + ηb−1(1 ∨m −1ηt).\nRemark E.4. Better bounds for the case ζ ≤ 1/2 will be proved in the longer version of this paper."
    } ],
    "references" : [ {
      "title" : "The tradeoffs of large scale learning",
      "author" : [ "Olivier Bousquet", "Léon Bottou" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2008
    }, {
      "title" : "On the generalization ability of on-line learning algorithms",
      "author" : [ "Nicolo Cesa-Bianchi", "Alex Conconi", "Claudio Gentile" ],
      "venue" : "IEEE Transactions on Information Theory,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2004
    }, {
      "title" : "Robust stochastic approximation approach to stochastic programming",
      "author" : [ "Arkadi Nemirovski", "Anatoli Juditsky", "Guanghui Lan", "Alexander Shapiro" ],
      "venue" : "SIAM Journal on Optimization,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2009
    }, {
      "title" : "Online gradient descent learning algorithms",
      "author" : [ "Yiming Ying", "Massimiliano Pontil" ],
      "venue" : "Foundations of Computational Mathematics,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2008
    }, {
      "title" : "Online learning as stochastic approximation of regularization paths: Optimality and almost-sure convergence",
      "author" : [ "Pierre Tarres", "Yuan Yao" ],
      "venue" : "IEEE Transactions on Information Theory,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2014
    }, {
      "title" : "Non-parametric stochastic approximation with large step sizes",
      "author" : [ "Aymeric Dieuleveut", "Francis Bach" ],
      "venue" : "arXiv preprint arXiv:1408.0361,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2014
    }, {
      "title" : "Simultaneous model selection and optimization through parameter-free stochastic learning",
      "author" : [ "Francesco Orabona" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2014
    }, {
      "title" : "Introduction to Optimization",
      "author" : [ "Boris T Poljak" ],
      "venue" : "Optimization Software,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 1987
    }, {
      "title" : "Stochastic subgradient methods",
      "author" : [ "Stephen Boyd", "Almir Mutapcic" ],
      "venue" : "Notes for EE364b,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2007
    }, {
      "title" : "Train faster, generalize better: Stability of stochastic gradient descent",
      "author" : [ "Moritz Hardt", "Benjamin Recht", "Yoram Singer" ],
      "venue" : "arXiv preprint arXiv:1509.01240,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2016
    }, {
      "title" : "Generalization properties and implicit regularization of multiple passes SGM",
      "author" : [ "Junhong Lin", "Raffaello Camoriano", "Lorenzo Rosasco" ],
      "venue" : "In Proceedings of the 33rd International Conference on Machine Learning,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2016
    }, {
      "title" : "Learning bounds for kernel regression using effective data dimensionality",
      "author" : [ "Tong Zhang" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2005
    }, {
      "title" : "Optimal rates for the regularized least-squares algorithm",
      "author" : [ "Andrea Caponnetto", "Ernesto De Vito" ],
      "venue" : "Foundations of Computational Mathematics,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2007
    }, {
      "title" : "Pegasos: Primal estimated sub-gradient solver for svm",
      "author" : [ "Shai Shalev-Shwartz", "Yoram Singer", "Nathan Srebro", "Andrew Cotter" ],
      "venue" : "Mathematical Programming,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2011
    }, {
      "title" : "Optimal distributed online prediction using mini-batches",
      "author" : [ "Ofer Dekel", "Ran Gilad-Bachrach", "Ohad Shamir", "Lin Xiao" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2012
    }, {
      "title" : "Optimization for Machine Learning",
      "author" : [ "Suvrit Sra", "Sebastian Nowozin", "Stephen J Wright" ],
      "venue" : null,
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2012
    }, {
      "title" : "Machine learning. Coursera",
      "author" : [ "Andrew Ng" ],
      "venue" : "Standford University,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2016
    }, {
      "title" : "Learning theory estimates via integral operators and their approximations",
      "author" : [ "Steve Smale", "Ding-Xuan Zhou" ],
      "venue" : "Constructive Approximation,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2007
    }, {
      "title" : "Learning with incremental iterative regularization",
      "author" : [ "Lorenzo Rosasco", "Silvia Villa" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2015
    }, {
      "title" : "Learning Theory: an Approximation",
      "author" : [ "Felipe Cucker", "Ding-Xuan Zhou" ],
      "venue" : "Theory Viewpoint,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2007
    }, {
      "title" : "Support Vector Machines",
      "author" : [ "Ingo Steinwart", "Andreas Christmann" ],
      "venue" : "Springer Science Business Media,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2008
    }, {
      "title" : "Stochastic gradient descent for non-smooth optimization: Convergence results and optimal averaging schemes",
      "author" : [ "Ohad Shamir", "Tong Zhang" ],
      "venue" : "In Proceedings of the 30th International Conference on Machine Learning,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2013
    }, {
      "title" : "Better mini-batch algorithms via accelerated gradient methods",
      "author" : [ "Andrew Cotter", "Ohad Shamir", "Nati Srebro", "Karthik Sridharan" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2011
    }, {
      "title" : "Remarks on inequalities for large deviation probabilities",
      "author" : [ "IF Pinelis", "AI Sakhanenko" ],
      "venue" : "Theory of Probability & Its Applications,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 1986
    }, {
      "title" : "On early stopping in gradient descent learning",
      "author" : [ "Yuan Yao", "Lorenzo Rosasco", "Andrea Caponnetto" ],
      "venue" : "Constructive Approximation,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2007
    }, {
      "title" : "Less is more: Nyström computational regularization",
      "author" : [ "Alessandro Rudi", "Raffaello Camoriano", "Lorenzo Rosasco" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2015
    }, {
      "title" : "User-friendly tools for random matrices: An introduction",
      "author" : [ "Joel A Tropp" ],
      "venue" : "Technical report, DTIC Document,",
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 2012
    }, {
      "title" : "On some extensions of bernstein’s inequality for self-adjoint operators",
      "author" : [ "Stanislav Minsker" ],
      "venue" : "arXiv preprint arXiv:1112.5448,",
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 2011
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Modern machine learning applications require computational approaches that are at the same time statistically accurate and numerically efficient [1].",
      "startOffset" : 145,
      "endOffset" : 148
    }, {
      "referenceID" : 1,
      "context" : "Most generalization studies on SGM consider the case where only one pass over the data is allowed and the step-size is appropriately chosen, [2, 3, 4, 5, 6, 7] (possibly considering averaging [8]).",
      "startOffset" : 141,
      "endOffset" : 159
    }, {
      "referenceID" : 2,
      "context" : "Most generalization studies on SGM consider the case where only one pass over the data is allowed and the step-size is appropriately chosen, [2, 3, 4, 5, 6, 7] (possibly considering averaging [8]).",
      "startOffset" : 141,
      "endOffset" : 159
    }, {
      "referenceID" : 3,
      "context" : "Most generalization studies on SGM consider the case where only one pass over the data is allowed and the step-size is appropriately chosen, [2, 3, 4, 5, 6, 7] (possibly considering averaging [8]).",
      "startOffset" : 141,
      "endOffset" : 159
    }, {
      "referenceID" : 4,
      "context" : "Most generalization studies on SGM consider the case where only one pass over the data is allowed and the step-size is appropriately chosen, [2, 3, 4, 5, 6, 7] (possibly considering averaging [8]).",
      "startOffset" : 141,
      "endOffset" : 159
    }, {
      "referenceID" : 5,
      "context" : "Most generalization studies on SGM consider the case where only one pass over the data is allowed and the step-size is appropriately chosen, [2, 3, 4, 5, 6, 7] (possibly considering averaging [8]).",
      "startOffset" : 141,
      "endOffset" : 159
    }, {
      "referenceID" : 6,
      "context" : "Most generalization studies on SGM consider the case where only one pass over the data is allowed and the step-size is appropriately chosen, [2, 3, 4, 5, 6, 7] (possibly considering averaging [8]).",
      "startOffset" : 141,
      "endOffset" : 159
    }, {
      "referenceID" : 7,
      "context" : "Most generalization studies on SGM consider the case where only one pass over the data is allowed and the step-size is appropriately chosen, [2, 3, 4, 5, 6, 7] (possibly considering averaging [8]).",
      "startOffset" : 192,
      "endOffset" : 195
    }, {
      "referenceID" : 3,
      "context" : "In particular, recent works show how the step-size can be seen to play the role of a regularization parameter whose choice controls the bias and variance properties of the obtained solution [4, 5, 6].",
      "startOffset" : 190,
      "endOffset" : 199
    }, {
      "referenceID" : 4,
      "context" : "In particular, recent works show how the step-size can be seen to play the role of a regularization parameter whose choice controls the bias and variance properties of the obtained solution [4, 5, 6].",
      "startOffset" : 190,
      "endOffset" : 199
    }, {
      "referenceID" : 5,
      "context" : "In particular, recent works show how the step-size can be seen to play the role of a regularization parameter whose choice controls the bias and variance properties of the obtained solution [4, 5, 6].",
      "startOffset" : 190,
      "endOffset" : 199
    }, {
      "referenceID" : 8,
      "context" : "While the role of multiple passes is well understood if the goal is empirical risk minimization [9], its effect with respect to generalization is less clear and a few recent works have recently started to tackle this question.",
      "startOffset" : 96,
      "endOffset" : 99
    }, {
      "referenceID" : 9,
      "context" : "In particular, results in this direction have been derived in [10] and [11].",
      "startOffset" : 62,
      "endOffset" : 66
    }, {
      "referenceID" : 10,
      "context" : "In particular, results in this direction have been derived in [10] and [11].",
      "startOffset" : 71,
      "endOffset" : 75
    }, {
      "referenceID" : 11,
      "context" : "The main shortcoming of these latter results is that they are in the worst case, in the sense that they do not consider the possible effect of capacity assumptions [12, 13] shown to lead to faster rates for other learning approaches such as Tikhonov regularization.",
      "startOffset" : 164,
      "endOffset" : 172
    }, {
      "referenceID" : 12,
      "context" : "The main shortcoming of these latter results is that they are in the worst case, in the sense that they do not consider the possible effect of capacity assumptions [12, 13] shown to lead to faster rates for other learning approaches such as Tikhonov regularization.",
      "startOffset" : 164,
      "endOffset" : 172
    }, {
      "referenceID" : 13,
      "context" : "Further, these results do not consider the possible effect of mini-batches, rather than a single point in each gradient step [14, 15, 16, 17].",
      "startOffset" : 125,
      "endOffset" : 141
    }, {
      "referenceID" : 14,
      "context" : "Further, these results do not consider the possible effect of mini-batches, rather than a single point in each gradient step [14, 15, 16, 17].",
      "startOffset" : 125,
      "endOffset" : 141
    }, {
      "referenceID" : 15,
      "context" : "Further, these results do not consider the possible effect of mini-batches, rather than a single point in each gradient step [14, 15, 16, 17].",
      "startOffset" : 125,
      "endOffset" : 141
    }, {
      "referenceID" : 16,
      "context" : "Further, these results do not consider the possible effect of mini-batches, rather than a single point in each gradient step [14, 15, 16, 17].",
      "startOffset" : 125,
      "endOffset" : 141
    }, {
      "referenceID" : 17,
      "context" : "In particular, we show for the first time that multipass SGM with early stopping and a universal step-size choice can achieve optimal learning bounds, matching those of ridge regression [18, 13].",
      "startOffset" : 186,
      "endOffset" : 194
    }, {
      "referenceID" : 12,
      "context" : "In particular, we show for the first time that multipass SGM with early stopping and a universal step-size choice can achieve optimal learning bounds, matching those of ridge regression [18, 13].",
      "startOffset" : 186,
      "endOffset" : 194
    }, {
      "referenceID" : 18,
      "context" : "Finally we note that a recent work [19] is tightly related to the analysis in the paper.",
      "startOffset" : 35,
      "endOffset" : 39
    }, {
      "referenceID" : 18,
      "context" : "The generalization properties of a multi-pass incremental gradient are analyzed in [19], for a cyclic, rather than a stochastic, choice of the gradients and with no mini-batches.",
      "startOffset" : 83,
      "endOffset" : 87
    }, {
      "referenceID" : 18,
      "context" : "The analysis in this latter case appears to be harder and results in [19] give good learning bounds only in restricted setting and considering iterates rather than the excess risk.",
      "startOffset" : 69,
      "endOffset" : 73
    }, {
      "referenceID" : 18,
      "context" : "Compared to [19] our results show how stochasticity can be exploited to get faster capacity dependent rates and analyze the role of mini-batches.",
      "startOffset" : 12,
      "endOffset" : 16
    }, {
      "referenceID" : 18,
      "context" : "Following [19], the formulation we consider is close to the setting of functional regression, and covers the reproducing kernel Hilbert space (RKHS) setting as special cases.",
      "startOffset" : 10,
      "endOffset" : 14
    }, {
      "referenceID" : 19,
      "context" : "Under Assumption (3), L can be proved to be positive trace class operators, and hence L with ζ ∈ R can be defined by using the spectrum theory [20].",
      "startOffset" : 143,
      "endOffset" : 147
    }, {
      "referenceID" : 19,
      "context" : "The above assumption is fairly standard [20, 19] in non-parametric regression.",
      "startOffset" : 40,
      "endOffset" : 48
    }, {
      "referenceID" : 18,
      "context" : "The above assumption is fairly standard [20, 19] in non-parametric regression.",
      "startOffset" : 40,
      "endOffset" : 48
    }, {
      "referenceID" : 20,
      "context" : "In particular, for ζ = 0, we are assuming ‖fH‖ρ < ∞, while for ζ = 1/2, we are requiring fH ∈ Hρ, since [21, 19] Hρ = L(L(H, ρX)).",
      "startOffset" : 104,
      "endOffset" : 112
    }, {
      "referenceID" : 18,
      "context" : "In particular, for ζ = 0, we are assuming ‖fH‖ρ < ∞, while for ζ = 1/2, we are requiring fH ∈ Hρ, since [21, 19] Hρ = L(L(H, ρX)).",
      "startOffset" : 104,
      "endOffset" : 112
    }, {
      "referenceID" : 11,
      "context" : "(7) The left-hand side of (7) is the so-called effective dimension, or the degrees of freedom [12, 13].",
      "startOffset" : 94,
      "endOffset" : 102
    }, {
      "referenceID" : 12,
      "context" : "(7) The left-hand side of (7) is the so-called effective dimension, or the degrees of freedom [12, 13].",
      "startOffset" : 94,
      "endOffset" : 102
    }, {
      "referenceID" : 20,
      "context" : "It can be related to covering/entropy number conditions, see [21] for further details.",
      "startOffset" : 61,
      "endOffset" : 65
    }, {
      "referenceID" : 12,
      "context" : "The above result asserts that, at p∗ passes over the data, the simple SGM with fixed step-size achieves optimal learning error bounds, matching those of ridge regression [13].",
      "startOffset" : 170,
      "endOffset" : 174
    }, {
      "referenceID" : 20,
      "context" : "Using an argument similar to that in Chapter 6 from [21], it is possible to show that this procedure can achieve the same convergence rate.",
      "startOffset" : 52,
      "endOffset" : 56
    }, {
      "referenceID" : 12,
      "context" : "First, the upper bound in (10) is optimal up to a logarithmic factor, in the sense that it matches the minimax lower rate in [13].",
      "startOffset" : 125,
      "endOffset" : 129
    }, {
      "referenceID" : 3,
      "context" : "6 recovers the result in [4] for one pass SGM.",
      "startOffset" : 25,
      "endOffset" : 28
    }, {
      "referenceID" : 3,
      "context" : ", [4, 22, 5, 6].",
      "startOffset" : 2,
      "endOffset" : 15
    }, {
      "referenceID" : 21,
      "context" : ", [4, 22, 5, 6].",
      "startOffset" : 2,
      "endOffset" : 15
    }, {
      "referenceID" : 4,
      "context" : ", [4, 22, 5, 6].",
      "startOffset" : 2,
      "endOffset" : 15
    }, {
      "referenceID" : 5,
      "context" : ", [4, 22, 5, 6].",
      "startOffset" : 2,
      "endOffset" : 15
    }, {
      "referenceID" : 3,
      "context" : "In particular, [4] proved capacity independent rate of order O(m− 2ζ 2ζ+1 logm) with a fixed step-size η ' m− 2ζ 2ζ+1 , and [6] derived capacity dependent error bounds of order O(m 2min(ζ,1) 2min(ζ,1)+γ ) (when 2ζ + γ > 1) for the average.",
      "startOffset" : 15,
      "endOffset" : 18
    }, {
      "referenceID" : 5,
      "context" : "In particular, [4] proved capacity independent rate of order O(m− 2ζ 2ζ+1 logm) with a fixed step-size η ' m− 2ζ 2ζ+1 , and [6] derived capacity dependent error bounds of order O(m 2min(ζ,1) 2min(ζ,1)+γ ) (when 2ζ + γ > 1) for the average.",
      "startOffset" : 124,
      "endOffset" : 127
    }, {
      "referenceID" : 4,
      "context" : "Note also that a regularized version of SGM has been studied in [5], where the derived convergence rate there is of order O(m− 2ζ 2ζ+1 ) assuming that ζ ∈ [ 1 2 , 1].",
      "startOffset" : 64,
      "endOffset" : 67
    }, {
      "referenceID" : 0,
      "context" : "Note also that a regularized version of SGM has been studied in [5], where the derived convergence rate there is of order O(m− 2ζ 2ζ+1 ) assuming that ζ ∈ [ 1 2 , 1].",
      "startOffset" : 155,
      "endOffset" : 165
    }, {
      "referenceID" : 1,
      "context" : "Note also that a regularized version of SGM has been studied in [5], where the derived convergence rate there is of order O(m− 2ζ 2ζ+1 ) assuming that ζ ∈ [ 1 2 , 1].",
      "startOffset" : 155,
      "endOffset" : 165
    }, {
      "referenceID" : 0,
      "context" : "Note also that a regularized version of SGM has been studied in [5], where the derived convergence rate there is of order O(m− 2ζ 2ζ+1 ) assuming that ζ ∈ [ 1 2 , 1].",
      "startOffset" : 155,
      "endOffset" : 165
    }, {
      "referenceID" : 18,
      "context" : "More recently, [19] studied multiple passes SGM with a fixed ordering at each pass, also called incremental gradient method.",
      "startOffset" : 15,
      "endOffset" : 19
    }, {
      "referenceID" : 18,
      "context" : "Note also that [19] proved sharp rate in H-norm for ζ ≥ 1/2 in the capacity independent case.",
      "startOffset" : 15,
      "endOffset" : 19
    }, {
      "referenceID" : 13,
      "context" : ", in [14, 15, 16, 17].",
      "startOffset" : 5,
      "endOffset" : 21
    }, {
      "referenceID" : 14,
      "context" : ", in [14, 15, 16, 17].",
      "startOffset" : 5,
      "endOffset" : 21
    }, {
      "referenceID" : 15,
      "context" : ", in [14, 15, 16, 17].",
      "startOffset" : 5,
      "endOffset" : 21
    }, {
      "referenceID" : 16,
      "context" : ", in [14, 15, 16, 17].",
      "startOffset" : 5,
      "endOffset" : 21
    }, {
      "referenceID" : 22,
      "context" : "Besides, it has been shown in [23, 15] that for one pass mini-batch SGM with a fixed step-size η ' b/ √ m and a smooth loss function, assuming the existence of at least one solution in the hypothesis space for the expected risk minimization, the convergence",
      "startOffset" : 30,
      "endOffset" : 38
    }, {
      "referenceID" : 14,
      "context" : "Besides, it has been shown in [23, 15] that for one pass mini-batch SGM with a fixed step-size η ' b/ √ m and a smooth loss function, assuming the existence of at least one solution in the hypothesis space for the expected risk minimization, the convergence",
      "startOffset" : 30,
      "endOffset" : 38
    }, {
      "referenceID" : 20,
      "context" : "For any ω ∈ H, we have [21, 19] E(ω)− inf f∈H E(f) = ‖Sρω − fH‖ρ.",
      "startOffset" : 23,
      "endOffset" : 31
    }, {
      "referenceID" : 18,
      "context" : "For any ω ∈ H, we have [21, 19] E(ω)− inf f∈H E(f) = ‖Sρω − fH‖ρ.",
      "startOffset" : 23,
      "endOffset" : 31
    }, {
      "referenceID" : 0,
      "context" : "Here, the regression function is fρ(x) = |x−1/2|−1/2, the input point xi is uniformly distributed in [0, 1], and ωi is a Gaussian noise with zero mean and standard deviation 1, for each i ∈ [m].",
      "startOffset" : 101,
      "endOffset" : 107
    }, {
      "referenceID" : 0,
      "context" : "Here, we replace the unknown marginal distribution ρX by an empirical measure ρ̂ = 1 2000 ∑2000 i=1 δx̂i , where each x̂i is uniformly distributed in [0, 1].",
      "startOffset" : 150,
      "endOffset" : 156
    }, {
      "referenceID" : 0,
      "context" : "[1] Olivier Bousquet and Léon Bottou.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 1,
      "context" : "[2] Nicolo Cesa-Bianchi, Alex Conconi, and Claudio Gentile.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 2,
      "context" : "[3] Arkadi Nemirovski, Anatoli Juditsky, Guanghui Lan, and Alexander Shapiro.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 3,
      "context" : "[4] Yiming Ying and Massimiliano Pontil.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 4,
      "context" : "[5] Pierre Tarres and Yuan Yao.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 5,
      "context" : "[6] Aymeric Dieuleveut and Francis Bach.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 6,
      "context" : "[7] Francesco Orabona.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 7,
      "context" : "[8] Boris T Poljak.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 8,
      "context" : "[9] Stephen Boyd and Almir Mutapcic.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 9,
      "context" : "[10] Moritz Hardt, Benjamin Recht, and Yoram Singer.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 10,
      "context" : "[11] Junhong Lin, Raffaello Camoriano, and Lorenzo Rosasco.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 11,
      "context" : "[12] Tong Zhang.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 12,
      "context" : "[13] Andrea Caponnetto and Ernesto De Vito.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 13,
      "context" : "[14] Shai Shalev-Shwartz, Yoram Singer, Nathan Srebro, and Andrew Cotter.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 14,
      "context" : "[15] Ofer Dekel, Ran Gilad-Bachrach, Ohad Shamir, and Lin Xiao.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 15,
      "context" : "[16] Suvrit Sra, Sebastian Nowozin, and Stephen J Wright.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 16,
      "context" : "[17] Andrew Ng.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 17,
      "context" : "[18] Steve Smale and Ding-Xuan Zhou.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 18,
      "context" : "[19] Lorenzo Rosasco and Silvia Villa.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 19,
      "context" : "[20] Felipe Cucker and Ding-Xuan Zhou.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 20,
      "context" : "[21] Ingo Steinwart and Andreas Christmann.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 21,
      "context" : "[22] Ohad Shamir and Tong Zhang.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 22,
      "context" : "[23] Andrew Cotter, Ohad Shamir, Nati Srebro, and Karthik Sridharan.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 23,
      "context" : "[25] IF Pinelis and AI Sakhanenko.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 24,
      "context" : "[26] Yuan Yao, Lorenzo Rosasco, and Andrea Caponnetto.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 25,
      "context" : "[27] Alessandro Rudi, Raffaello Camoriano, and Lorenzo Rosasco.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 26,
      "context" : "[28] Joel A Tropp.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 27,
      "context" : "[29] Stanislav Minsker.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 20,
      "context" : "For any ω ∈ H, it is easy to prove the following isometry property [21] ‖Sρω‖ρ = ‖ √ T ω‖H .",
      "startOffset" : 67,
      "endOffset" : 71
    }, {
      "referenceID" : 12,
      "context" : "2 Concentration Inequality We need the following concentration result for Hilbert space valued random variable used in Caponnetto and De Vito [13] and based on the results in Pinelis and Sakhanenko [25].",
      "startOffset" : 142,
      "endOffset" : 146
    }, {
      "referenceID" : 23,
      "context" : "2 Concentration Inequality We need the following concentration result for Hilbert space valued random variable used in Caponnetto and De Vito [13] and based on the results in Pinelis and Sakhanenko [25].",
      "startOffset" : 198,
      "endOffset" : 202
    }, {
      "referenceID" : 3,
      "context" : "Towards this end, we introduce the following lemma, whose proof borrows idea from [4, 5].",
      "startOffset" : 82,
      "endOffset" : 88
    }, {
      "referenceID" : 4,
      "context" : "Towards this end, we introduce the following lemma, whose proof borrows idea from [4, 5].",
      "startOffset" : 82,
      "endOffset" : 88
    }, {
      "referenceID" : 24,
      "context" : "The result is essentially proved in [26], see also [19].",
      "startOffset" : 36,
      "endOffset" : 40
    }, {
      "referenceID" : 18,
      "context" : "The result is essentially proved in [26], see also [19].",
      "startOffset" : 51,
      "endOffset" : 55
    }, {
      "referenceID" : 18,
      "context" : "The proof for the fixed step-size can be found in [19].",
      "startOffset" : 50,
      "endOffset" : 54
    }, {
      "referenceID" : 18,
      "context" : "The case for ζ ≥ 1/2 is similar to that in [19].",
      "startOffset" : 43,
      "endOffset" : 47
    }, {
      "referenceID" : 25,
      "context" : "The next lemma is borrowed from [27], derived by applying a recent Bernstein inequality from [28, 29] for a sum of random operators.",
      "startOffset" : 32,
      "endOffset" : 36
    }, {
      "referenceID" : 26,
      "context" : "The next lemma is borrowed from [27], derived by applying a recent Bernstein inequality from [28, 29] for a sum of random operators.",
      "startOffset" : 93,
      "endOffset" : 101
    }, {
      "referenceID" : 27,
      "context" : "The next lemma is borrowed from [27], derived by applying a recent Bernstein inequality from [28, 29] for a sum of random operators.",
      "startOffset" : 93,
      "endOffset" : 101
    }, {
      "referenceID" : 21,
      "context" : "The process relies on some tools from convex analysis and a decomposition related to the weighted averages and the last iterates from [22, 30].",
      "startOffset" : 134,
      "endOffset" : 142
    }, {
      "referenceID" : 21,
      "context" : "Using the above lemma and a decomposition related to the weighted averages and the last iterates from [22, 30], we can prove the following relationship.",
      "startOffset" : 102,
      "endOffset" : 110
    } ],
    "year" : 2016,
    "abstractText" : "We analyze the learning properties of the stochastic gradient method when multiple passes over the data and mini-batches are allowed. In particular, we consider the square loss and show that for a universal step-size choice, the number of passes acts as a regularization parameter, and optimal finite sample bounds can be achieved by early-stopping. Moreover, we show that larger step-sizes are allowed when considering mini-batches. Our analysis is based on a unifying approach, encompassing both batch and stochastic gradient methods as special cases.",
    "creator" : "LaTeX with hyperref package"
  }
}