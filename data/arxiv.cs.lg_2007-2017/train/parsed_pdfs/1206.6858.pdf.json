{
  "name" : "1206.6858.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Sequential Document Representations and Simplicial Curves",
    "authors" : [ "Guy Lebanon" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "The popular bag of words assumption represents a document as a histogram of word occurrences. While computationally efficient, such a representation is unable to maintain any sequential information. We present a continuous and differentiable sequential document representation that goes beyond the bag of words assumption, and yet is efficient and effective. This representation employs smooth curves in the multinomial simplex to account for sequential information. In contrast to n-grams the new representation is able to robustly model long rage sequential trends in the document. We discuss the representation and its geometric properties and demonstrate its applicability for the task of text classification."
    }, {
      "heading" : "1 Introduction",
      "text" : "Modeling of text documents is an essential part in a wide variety of applications such as the classification, segmentation, visualization and retrieval of text. A crucial part of the modeling process is choosing an appropriate representation for the documents. The original representation of documents as a sequence of words is extremely high-dimensional, discrete, and brittle making it an inconvenient representation for most models to work on. Instead, the words sequences are typically preprocessed by mapping them to a lower dimensional lossy representation such as n-gram, on which the modeling is carried out.\nThe n-gram representation keeps track of the number of n consecutive words of different types, called n-grams. In the case n = 1 this amounts to ignoring the order of the words in the document, keeping only the histogram of word occurrences. This specific case, also called bag of words (bow) representation, is the\nmost frequent one used due to its relative robustness in sparse situations. Assuming that the word counts are normalized by the document length, the bow representation of a word sequence y = 〈y1, . . . , yN 〉 over a vocabulary of integers V = {1, . . . , |V |} is x ∈ R|V |, where xj = 1 N ∑N\ni=1 δyi,j . Alternatively, to avoid zeros the bow representation is often defined using smoothed word counts\nxj = 1\nZ\nN ∑\ni=1\n(δyi,j + c), c ≥ 0 (1)\nwhere Z ensures normalization. The smoothed version (1) may also be derived as the maximum posterior estimate for multinomial model and Dirichlet prior and is equal to the non-smooth version for c = 0.\nClearly, the n-gram representation sacrifices sequential information in favor of reduced dimensionality. In the case of n = 1 or bow representation, all sequential information is lost. In the case of 1 < n ≪ N , frequently occurring word patterns are kept, but long range sequential information concerning the word sequence y is lost. Moreover, there is no way to tie the occurrences of word patterns to the specific locations in the document that they occurred. In terms of the applications mentioned above, the improvement that results from increasing n is largely disappointing due to the high dimensionality and discreteness of the ngram representation.\nIn this paper, we present a new representation called locally weighted bag of words (lowbow). This representation generalizes bow in a robust way that maintains long and short range sequential information. In contrast to n-gram that keeps track of frequently occurring patterns independent of their positions, lowbow keeps track of changes in the word histogram as it sweeps through the document from beginning to end.\nOne major advantage of lowbow is that it is a continuous and differentiable representation equivalent to smooth curves in the multinomial simplex. By naturally incorporating a smoothing kernel we are able to\nuse lowbow effectively, even in cases that n-gram performs poorly due to its high dimensionality. Moreover, the smooth representation facilitates the use of tools from differentiable calculus and geometry to characterize the sequential contents of documents.\nIn the next section we describe the multinomial simplex and its geometry and then proceed with a formal definition of the lowbow representation."
    }, {
      "heading" : "2 The Multinomial Simplex and its Geometry",
      "text" : "In this section, we present a brief description of the multinomial simplex and its geometry. Since the simplex is the space of bow representations, its geometry is crucial to the lowbow representation. The brief description below uses some concepts from Riemannian geometry. For additional information concerning the geometry of the simplex refer to (Amari & Nagaoka, 2000; Lafferty & Lebanon, 2005). A standard textbook on Riemannian geometry is (Spivak, 1975).\nThe multinomial simplex Pm for m > 0 is the mdimensional subset of Rm+1 of all positive probability vectors or histograms over m+ 1 objects\nPm =\n\n\n\nθ ∈ Rm+1 : ∀i θi > 0,\nm+1 ∑\nj=1\nθj = 1\n\n\n\n.\nIts connection to the multinomial distribution is that every θ ∈ Pm corresponds to a multinomial distribution over m + 1 items. The definition above does not include zero probabilities which lie on the boundary of the simplex closure Pm. As a result, Pm is a differentiable manifold and we can use the standard language of differentiable manifolds, rather than the more complicated notion of a geometry with corners. A discussion concerning the positivity restriction and its relative unimportance in practice may be found in (Lafferty & Lebanon, 2005).\nThe topological structure of Pm, which determines the notions of convergence and continuity, is naturally inherited from the standard topological structure of the embedding space Rm+1. The geometrical structure of Pm that determines the notions of distance, angle and curvature is determined by a local inner product gθ(·, ·), θ ∈ Pm, called the Riemannian metric. The most obvious choice, perhaps, is the standard Euclidean inner product gθ(u, v) = ∑\nuivi. However, such a choice is problematic from several aspects. Fortunately, a particular choice of a Riemannian metric is motivated by Čencov (1982) who proved that the\nFisher information metric\ngθ(u, v) = ∑\nij\nuivjEpθ\n(\n∂ log pθ(x)\n∂θi\n∂ log pθ(x)\n∂θj\n)\n(2)\n=\nm+1 ∑\ni=1\nuivi θi\n(3)\n(above, pθ(x) is the multinomial probability associated with the parameter θ) is the only invariant metric under sufficient statistics transformations. In addition, various recent results motivate the Fisher geometry from a practical perspective (Lafferty & Lebanon, 2005).\nThe inner product (3) defines the geometric properties of distance, angle and curvature on Pm in a way that is quite different from the Euclidean inner product. The distance function d : Pm×Pm → [0, π/2] corresponding to (3) is\nd(θ, η) = acos\n(\nm+1 ∑\ni=1\n√\nθiηi\n)\n. (4)\nThe distance function (4) and the Euclidean distance function d(θ, η) = √ ∑\n(θi − ηi)2 resulting from the inner product gθ(u, v) = ∑\nuivi on P2 are illustrated in Figures 1-2.\nBy determining geometric properties such as distance, the choice of metric for Pm is of direct importance to the bow representation of documents and its modeling. For example, while the Euclidean metric is homogenous across the simplex, the Fisher metric (3) emphasizes the area close to the boundary. The next section describes the lowbow representation which amounts to a parameterized curve in Pm. In addressing the question of modeling such curves, the simplex geometry plays a central role. It dictates notions such as the distance between two curves, the instantaneous direction of a curve, and the curvature or complexity of a curve. Understanding the relationship between these\ngeometric notions and gθ is the first step in modeling documents using the lowbow representation."
    }, {
      "heading" : "3 Sequential Document Representations",
      "text" : "We typically think of documents over a vocabulary V = {1, . . . , |V |} as a finite sequence 〈y1, . . . , yN 〉 of words represented as integers in V . However, we use the following broader definition for documents that will be more convenient later on.\nDefinition 1. A document x of length N is a function x : {1, . . . , N} × V → [0, 1] such that\n|V | ∑\nj=1\nx(i, j) = 1 ∀i ∈ {1, . . . , N}."
    }, {
      "heading" : "The set of documents (of all lengths) is denoted by X.",
      "text" : "For a document x ∈ X the value x(i, j) is the weight of the word j ∈ V at location i. The standard way to represent a word sequence as a document in X is to have each location host the appropriate single word with constant weight, which corresponds to the δc representation defined below with c = 0.\nDefinition 2. The standard representation δc(y) ∈ X, where c ≥ 0, of a word sequence y = 〈y1, . . . , yN〉 is\nδc(y)(i, j) =\n{\nc 1+c|V | yi 6= j\n1+c 1+c|V | yi = j\n.\nThe above is a legitimate representation since ∑\nj δc(y)(i, j) = 1+c|V | 1+c|V | = 1. The parameter c in the above definition is useful for avoiding zero counts in the obtained local histograms δc(y)(i, ·).\nThe standard representation δc assumes that each word yj in the sequence y occupies a single location 1, . . . , N . In general, however, Definition 1 lets several words occupy the same location by smoothing the influence of words yj across different document positions. Doing so is central in converting the discretetime standard representation to a continuous representation that is much more convenient for analysis and modeling.\nWe are interested in comparing different documents on the basis of their sequential contents. Definition 1 is problematic since it depends on the length of the document. We would like to treat two documents with similar sequential contents but different lengths in a similar fashion. Towards this goal we introduce length-normalized documents that abstract away from the document length by normalizing the sequential information to lie in the interval [0, 1].\nDefinition 3. A length-normalized document x is a function x : [0, 1]× V → [0, 1] such that\n|V | ∑\nj=1\nx(t, j) = 1, ∀t ∈ [0, 1]."
    }, {
      "heading" : "The set of length-normalized documents is denoted X∗.",
      "text" : "The procedure of converting a document x ∈ X to a\nlength-normalized document x′ ∈ X∗ is expressed by the length-normalization function defined below.\nDefinition 4. The length-normalization function for a document x ∈ X of length N is\nϕ : X → X∗ ϕ(x)(t, j) = x(⌈tN⌉, j)\nwhere ⌈r⌉ is the smallest integer greater than r.\nWith each length-normalized document we can associate the following global bow or histogram.\nDefinition 5. The global bow representation of a length-normalized document x ∈ X∗ is\nρ : X∗ → P|V |−1 [ρ(x)]j =\n∫ 1\n0\nx(t, j) dt.\nNote that the function ρ is well defined since\n|V | ∑\nj=1\n[ρ(x)]j =\n|V | ∑\nj=1\n∫ 1\n0\nx(t, j) dt =\n∫ 1\n0\n|V | ∑\nj=1\nx(t, j) dt\n=\n∫ 1\n0\n1 dt = 1 =⇒ ρ(x) ∈ P|V |−1.\nAs proved in Theorem 2, the global bow representation defined above for a document in its standard representation δc(y) is equivalent to the popular definition of bow expressed in equation (1).\nWe describe below a smoothed representation in X∗ that is analogous to a curve in the multinomial simplex. The smoothed representation is obtained by convolving the length-normalized standard representation ϕ(δc(y)) with a positive smoothing kernel. A positive smoothing kernel is a function Kµ,σ : [0, 1] → (0,∞), parameterized by location and scale parameters µ ∈ [0, 1], σ ∈ (0,∞). The parameter µ represents the (normalized) document location and σ represents the amount of smoothing. We further assume that a smoothing kernel is continuous and differentiable in µ, r and normalized, i.e., ∫ 1\n0 Kµ,σ(t) dt = 1.\nOne example for a smoothing kernel is the normal distribution pdf restricted to [0, 1] and normalized\nKµ,σ(x) =\n{\n1 Z N(x ;µ, σ) x ∈ [0, 1] 0 x 6∈ [0, 1] . (5)\nAnother example is the beta distribution pdf\nKµ,σ(x) = Beta\n(\nx ; c µ\nσ , c\n1− µ\nσ\n)\n(6)\nwhere c is selected so that the two parameters of the beta distribution will be greater than 1. The above Beta pdf has expectation µ and variance that is increasing in the scale parameter σ. The following definitions complete the description of the lowbow representation.\nDefinition 6. The µ-modulation function ψµ maps a document x ∈ X∗ to\nψµ(x)(t, j) = x(t, j)Kµ,σ(t) t ∈ [0, 1], j ∈ V.\nDefinition 7. The locally weighted bag of words (lowbow) representation at µ of the word sequence y is\nγµ(y) = ρ ◦ ψµ ◦ ϕ ◦ δc(y) ∈ P|V |−1, µ ∈ [0, 1].\nFirst note that lowbow indeed maps a word sequence y and µ into the simplex γµ(y) ∈ P|V |−1 since ∑ j [γµ(y)]j = ∑ j ∫ 1\n0 x(t, j)Kµ,σ(t) dt = ∫ 1\n0 Kµ,σ(t)\n∑ j x(t, j) dt = ∫ 1 0 Kµ,σ(t) · 1 dt = 1.\nTheorem 1. The lowbow representation is a continuous and differentiable parameterized curve in the simplex, in both the Euclidean and the Fisher geometry.\nProof. We prove below only the continuity of the lowbow representation. The proof of differentiability proceeds along similar lines. Fixing y, the mapping µ 7→ γµ(y) maps [0, 1] into the simplex P|V |−1. Since Kµ,σ(t) is continuous on a compact region (µ, t) ∈ [0, 1]2, it is also uniformly continuous and we have\nlim ǫ→0 |[γµ(y)]j − [γµ+ǫ(y)]j |\n= lim ǫ→0\n∣ ∣ ∣\n∫ 1\n0\nx(t, j)Kµ,σ(t)− x(t, j)Kµ+ǫ,σ(t)dt ∣ ∣ ∣\n≤ lim ǫ→0\n∫ 1\n0\nx(t, j)|Kµ,σ(t)−Kµ+ǫ,σ(t)| dt\n≤ lim ǫ→0 sup t∈[0,1] |Kµ,σ(t)−Kµ+ǫ,σ(t)|\n∫ 1\n0\nx(t, j) dt\n≤ lim ǫ→0 sup t∈[0,1] |Kµ,σ(t)−Kµ+ǫ,σ(t)| = 0.\nAs a result,\nlim ǫ→0 ‖γµ(y)− γµ+ǫ(y)‖ = √ ∑ |[γµ(t)]j − [γµ+ǫ(t)]j |2 → 0\nproving the continuity of γµ(y) in the Euclidean geometry. Since the Euclidean geometry and the Fisher geometry share the same topology, the lowbow curves are also continuous in the Fisher geometry.\nIt is important to note that the parameterized curve that corresponds to the lowbow representation consists of two parts: the geometric figure {γµ(y) : µ ∈ [0, 1]} ⊂ P|V |−1 and the parameterization function µ 7→ γµ(y) that ties the local histogram to a location µ in the document. A common mistake in dealing with parameterized curves is to focus on the geometric figure while ignoring the parameterization. This is a mistake since different lowbow representations may share\nsimilar geometric figures but possess different parameterization speeds indicating different local properties.\nThe geometric properties of the curve depend on the word sequence, the kernel shape and the kernel scale parameter. The kernel scale parameter is especially important as it determines the amount of temporal smoothing employed. If σ → ∞ the lowbow curve degenerates into a single point that is the global bow representation.\nTheorem 2. Let Kµ,σ be a smoothing kernel such that when σ → ∞, Kµ,σ(x) is constant in µ, x. Then for σ → ∞, the lowbow curve γ(y) degenerates into a single point equivalent to the bow representation (1).\nProof. Since the kernel is both constant and normalized, Kµ,σ(t) = 1, ∀µ, t ∈ [0, 1] 2. For all µ ∈ [0, 1],\n[γµ(y)]j =\n∫ 1\n0\nϕ(δc(y))(t, j)Kµ,σ(t) dt\n=\n∫ 1\n0\nϕ(δc(y))(t, j) dt\n=\nN ∑\ni=1\n1\nN\n(\nδyi,j 1 + c\n1 + c|V | + (1− δyi,j)\nc\n1 + c|V |\n)\n∝\nN ∑\ni=1\nδyi,j(1 + c) + (1− δyi,j)c ∝\nN ∑\ni=1\n(δyi,j + c).\nIn the other extreme, small σ will result in a curve figure that quickly moves between the different corners of the simplex as the words 〈y1, . . . , yN 〉 are encountered at times t = 1/N, . . . , N/N . Thus the lowbow representation as σ → 0 approaches a representation equivalent to the original word sequence. It is unlikely that either extreme cases σ → 0 or σ → ∞ will be optimal from a modeling perspective. By varying σ between 0 and ∞ we are able to interpolate between these two extreme cases and obtain a convenient sequential, yet robust, representation.\nFigure 3 illustrates the geometric figure resulting from the lowbow representation and its dependency on the kernel scale parameter and the smoothing coefficient. Notice how the figure shrinks as σ increases until it reaches the single point that is bow. Increasing c, on the other hand, pushes the geometric figure towards the center of the simplex.\nIt is useful to have a quantitative characterization of the complexity of the lowbow representation as a function of the chosen kernel and σ. Towards this end, the kernel’s complexity, defined below, serves as a bound for variations in the lowbow curve.\nDefinition 8. Let Kµ,σ(t) be a kernel that is Lipschitz continuous1 in µ with a Lipschitz constant CK(t). The kernel’s complexity is defined as\nO(K) = √ |V |\n∫ 1\n0\nCK(t) dt.\nThe theorem below proves that the lowbow curve is Lipschitz continuous with a Lipschitz constant O(K), thus connecting the curve complexity with the shape and the scale of the kernel.\nTheorem 3. The lowbow curve γ(y) satisfies\n‖γµ(y)− γτ (y)‖ ≤ |µ− τ | O(K)."
    }, {
      "heading" : "Proof.",
      "text" : "|[γµ(y)]j − [γµ+ǫ(y)]j |\n≤\n∫ 1\n0\nx(t, j)|Kµ,σ(t)−Kµ+ǫ,σ(t)| dt\n≤\n∫ 1\n0\n|Kµ,σ(t)−Kµ+ǫ,σ(t)| dt ≤ ǫ\n∫ 1\n0\nCK(t) dt\nand so ‖γµ(y)− γµ+ǫ(y)‖ ≤ ǫO(K).\nWhile Theorem 3 is expressed in terms of the Euclidean distance on the simplex, it is straightforward to extend it to the case of the Fisher geodesic distance d(γµ(y), γτ (y)) as it is a smooth function of the Euclidean distance."
    }, {
      "heading" : "4 Modeling of Simplicial Curves",
      "text" : "A lowbow representation is equivalent to a parameterized curve in the simplex. As such, it is a point in an infinite product of simplices P [0,1] m , that is naturally equipped with the product topology and geometry of the individual simplices. In practice, maintaining a continuous representation is difficult, and the straightforward solution is to sample the path at representative points t1, . . . , tl ∈ [0, 1] resulting in a discrete lowbow representation equivalent to a point in the finite dimensional product space Plm. We describe below geometrical features of lowbow and their use in modeling. The discussion focuses on continuous lowbow curves case, but the discrete case resulting from finite sampling may be obtained by replacing integrals with sums.\nThe distance between lowbow representations of two word sequences γ(y1), γ(y2) ∈ P [0,1] m , is the average distance between the corresponding time coordinates\nd(γ(y1), γ(y2)) =\n∫ 1\n0\nd(γt(y1), γt(y2)) dt (7)\n1A Lipschitz continuous function f satisfies |f(x) − f(y)| ≤ C|x − y| for some constant C called the Lipschitz constant.\nwhere d(γt(y1), γt(y2)) depends on the simplex geometry under consideration, e.g. Equation (4) or the Euclidean norm.\nThe distance between lowbow curves d(γ(y1), γ(y2)) (or its discrete analog) may be used in various modeling tasks. In k-nearest neighbors it simply replaces traditional bow-based distances such as tf similarity or the Euclidean norm. The distance (7) may also be used to construct the approximated heat kernel for use in SVM classification and regression as described by Lafferty and Lebanon (2005). The lowbow distance may also be used to construct generative models for text that generalizes the naive Bayes or multinomial model. Such language models serve an important role in applications such as machine translation, speech recognition and information retrieval.\nOther geometrical features may also be used in modeling text documents. The instantaneous direction of the curve γ(y) at t0 is given by its tangent vector γ̇t0(y). Since the curve is differentiable we can obtain a tangent vector field γ̇(y) along the curve that describes sequential topic trends and their change. The second derivative γ̈(y) vector field, together with the Riemannian metric, may be used to define the curvature at different points along the lowbow curve. Intuitively, curvature measures the amount of wigglyness or deviation from a straight line (or geodesic). Integrating the norm of the curvature tensor over t ∈ [0, 1] provides a measure of the sequential topic complexity or variability along the document. The tangent vector field and curvature of lowbow may be used to study local properties of documents that relate to tasks such as segmentation or summarization.\nThe geometric properties mentioned above are useful in constructing generative and conditional models for text applications such as retrieval, classification, filtering, segmentation and visualization. Due to lack of space we will briefly present in the next section a simple lowbow k-nearest neighbor classifier. The application of lowbow to additional tasks, and other classification models will be studied in future work."
    }, {
      "heading" : "5 Text Classification Experiments",
      "text" : "In this section, we examine lowbow and its properties in the context of text classification using a nearest neighbor classifier. We report experimental results for the WebKB faculty vs. course task and the Reuters top ten categories using the standard mod-apte split. In the WebKB task we repeatedly sampled subsets for training and testing with equal positive and negative examples. In the Reuters task we randomly sampled subsets of the mod-apte split for training (to examine the influence of the train set size) resulting in unbalanced train and test set containing more negative\nthan positive examples. The continuous quantities in the lowbow calculation were approximated by a discrete sample of 5 equally spaced points in the interval [0, 1] turning the integrals into efficiently computed Riemann sums. The kernel used was the bounded Gaussian kernel (5). Throughout the experiments we computed several alternatives for the kernel scale parameter and chose the best one. An important extension of the current experiments that we plan to conduct in the future is automatic selection of σ.\nFigure 4 (top) displays results for the WebKB data. The left graph is a standard train-set size vs. test set error rate comparing the lowbow geodesic using scale σ → ∞ (dashed) and the lowbow geodesic distance using an intermediate scale value. The right graph display the dependency of the test set error rate on the scale parameter indicating an optimal scale at around σ = 0.2 (for repeated samplings of 500 training examples). In both cases, the performances of standard bow techniques such as tf cosine similarity or Euclidean distance were significantly inferior (20-40% higher error rate) than the displayed curves and are not displayed.\nFigure 4 (bottom) displays test set error rates for the Reuters task. The 10 rows in the table indicate the classification task of identifying each of the 10 most popular classes in the Reuters collection. The columns represent varying training set sizes (sampled from the mod-apte split). The lowbow geodesic distance for an intermediate scale is denoted by err1 and for σ → ∞ is denoted by err2. tf-Cosine similarity and Euclidean distance for bow are denoted by err3 and err4.\nThe experiments indicate that lowbow geodesic clearly outperforms, for most values of σ, the standard tfcosine similarity and Euclidean distance for bow. In addition they also indicate that in general, the best scale parameter for lowbow is an intermediate one - and not σ → ∞ thus validating the hypothesis that we can leverage sequential information using the lowbow framework to improve on global bow models. Whether or not the optimal value of σ is small or large depends on several factors. It depends on the relationship between the sequential information and the response variable as some tasks are more amenable to such an approach than others. The size of the training set matters as well since excessive lowbow smoothing will be required for extremely small datasets resulting in an optimal scale of σ → ∞."
    }, {
      "heading" : "6 Related Work and Conclusion",
      "text" : "The use of n-gram and bow in document modeling has a long history. A geometric point of view considering the bow representation as a point in the multinomial simplex is expressed in (Lafferty & Lebanon, 2005), and a recent overview of the geometrical properties of\nprobability spaces is (Amari & Nagaoka, 2000). The use of simplicial curves in text modeling is a relatively new approach promoted by Gous (1998) and Hall and Hofmann (). ? (?) describes some related ideas that lead to a non-smooth multi-scale view of images.\nThe lowbow representation is a promising new direction in text modeling. By varying σ it interpolates between the standard word sequence representation 〈y1, . . . , yN〉 and bow. It is equivalent to a smooth curve that is amenable to techniques from real analysis and geometry. In contrast to n-gram, it captures topical trends and incorporates long range information. On the other hand, the lowbow novelty is orthogonal to n-gram as it is possible to combine the two.\nThe lowbow framework is aesthetically pleasing, and achieves good results in practice. Using a smoothing parameter, it naturally interpolates between bow and complete sequential information. The correspondence with smooth curves in the simplex enables the use of a wide array of tools from analysis and geometry in an otherwise discrete representation. It seems likely that the lowbow representation and its geometric properties will lead to improvements in many areas of text modeling, including retrieval, classification, visualization, segmentation, summarization, and language modeling."
    } ],
    "references" : [ {
      "title" : "Methods of information geometry. American Mathematical Society",
      "author" : [ "S. Amari", "H. Nagaoka" ],
      "venue" : null,
      "citeRegEx" : "Amari and Nagaoka,? \\Q2000\\E",
      "shortCiteRegEx" : "Amari and Nagaoka",
      "year" : 2000
    }, {
      "title" : "Statistical decision rules and optimal inference. American Mathematical Society",
      "author" : [ "N.N. Čencov" ],
      "venue" : null,
      "citeRegEx" : "Čencov,? \\Q1982\\E",
      "shortCiteRegEx" : "Čencov",
      "year" : 1982
    }, {
      "title" : "Exponential and spherical subfamily models",
      "author" : [ "A. Gous" ],
      "venue" : "Doctoral dissertation,",
      "citeRegEx" : "Gous,? \\Q1998\\E",
      "shortCiteRegEx" : "Gous",
      "year" : 1998
    }, {
      "title" : "Diffusion kernels on statistical manifolds",
      "author" : [ "J. Lafferty", "G. Lebanon" ],
      "venue" : "Journal of Machine Learning Research",
      "citeRegEx" : "Lafferty and Lebanon,? \\Q2005\\E",
      "shortCiteRegEx" : "Lafferty and Lebanon",
      "year" : 2005
    }, {
      "title" : "A comprehensive introduction to differential geometry, vol. 1-5",
      "author" : [ "M. Spivak" ],
      "venue" : "Publish or Perish",
      "citeRegEx" : "Spivak,? \\Q1975\\E",
      "shortCiteRegEx" : "Spivak",
      "year" : 1975
    } ],
    "referenceMentions" : [ {
      "referenceID" : 4,
      "context" : "A standard textbook on Riemannian geometry is (Spivak, 1975).",
      "startOffset" : 46,
      "endOffset" : 60
    }, {
      "referenceID" : 1,
      "context" : "Fortunately, a particular choice of a Riemannian metric is motivated by Čencov (1982) who proved that the Figure 1: The 2-simplex P2 may be visualized as a surface in R (left) or as a triangle in R (right).",
      "startOffset" : 72,
      "endOffset" : 86
    }, {
      "referenceID" : 3,
      "context" : "The distance (7) may also be used to construct the approximated heat kernel for use in SVM classification and regression as described by Lafferty and Lebanon (2005). The lowbow distance may also be used to construct generative models for text that generalizes the naive Bayes or multinomial model.",
      "startOffset" : 137,
      "endOffset" : 165
    } ],
    "year" : 2006,
    "abstractText" : "The popular bag of words assumption represents a document as a histogram of word occurrences. While computationally efficient, such a representation is unable to maintain any sequential information. We present a continuous and differentiable sequential document representation that goes beyond the bag of words assumption, and yet is efficient and effective. This representation employs smooth curves in the multinomial simplex to account for sequential information. In contrast to n-grams the new representation is able to robustly model long rage sequential trends in the document. We discuss the representation and its geometric properties and demonstrate its applicability for the task of text classification.",
    "creator" : "dvips(k) 5.94a Copyright 2003 Radical Eye Software"
  }
}