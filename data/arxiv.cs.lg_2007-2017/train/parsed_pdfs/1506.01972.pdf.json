{
  "name" : "1506.01972.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "UniVR: A Universal Variance Reduction Framework for Proximal Stochastic Gradient Method",
    "authors" : [ "Zeyuan Allen-Zhu", "Yang Yuan" ],
    "emails" : [ "zeyuan@csail.mit.edu", "yangyuan@cs.cornell.edu" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "In this paper, we consider the following composite convex minimization problem:\nmin x∈Rd\n{ F (x) def = f(x) + Ψ(x) def = 1\nn\nn∑\ni=1\nfi(x) + Ψ(x) } . (1.1)\nHere, f(x) = 1n ∑n i=1 fi(x) is the finite average of n smooth functions fi(x), and Ψ(x) is a relatively simple (but possibly non-differentiable) convex function, sometimes referred to as the proximal function. The goal of this paper is to find an approximate minimizer x ∈ Rd satisfying F (x) ≤ F (x∗) + ε, where x∗ is the minimizer of F (x). Problems of this form arise in many places in machine learning, statistics, and operations research. For instance, many regularized empirical risk minimization (ERM) problems naturally fall into this category. In such problems, we are given n training examples {(a1, `1), . . . (an, `n)}, where each ai ∈ Rd is the feature vector of example i, and each `i ∈ R is the label of example i. The following classification and regression problems are well-known examples of ERM:\n• Ridge Regression: fi(x) = 12 (〈ai, x〉 − `i)2 + σ2 ‖x‖22 and Ψ(x) = 0. • Lasso: fi(x) = 12 (〈ai, x〉 − `i)2 and Ψ(x) = σ‖x‖1. • Elastic Net: fi(x) = 12 (〈ai, x〉 − `i)2 + σ12 ‖x‖22 and Ψ(x) = σ2‖x‖1. • `1-Regularized Logistic Regression: fi(x) = log(1 + exp(−`i〈ai, x〉)) and Ψ(x) = σ‖x‖1.\nClassical full-gradient first-order methods often consider the following proximal steps:\nxt+1 ← arg min y∈Rd { 1 2η ‖y − xt‖22 + 〈∇f(xt), y〉+ Ψ(y) } .\nar X\niv :1\n50 6.\n01 97\n2v 1\n[ cs\n.L G\n] 5\nJ un\nAbove, η is the length of the gradient step, and if the proximal function Ψ(y) equals zero, then xt+1 simply reduces to the classical form of gradient descent: xt+1 ← xt − η∇f(xt). We remark here that the computation of the full gradient ∇f(·) is usually very expensive —for instance, for ERM problems it requires one to have a full pass of the possibly gigabyte-sized input data.\nIn the recent two decades, many researchers have started to consider stochastic update rules instead:\nxt+1 ← arg min y∈Rd { 1 2η ‖y − xt‖22 + 〈ξt, y〉+ Ψ(y) } ,\nwhere ξt is a random vector satisfying E[ξt] = ∇f(xt) and is referred to as the stochastic gradient. Given the “finite average” structure f(x) = 1n ∑n i=1 fi(x), a popular choice for the stochastic gradient is to let ξt = ∇fi(xt) for some randomly selected index i ∈ [n]. Methods based on this choice are known as stochastic gradient descent (SGD) methods. Since the computation of ∇fi(x) is usually n times faster than that of ∇f(x), SGD has been successfully applied to many large-scale learning problems, see for instance [1, 20].\nMore recently, the convergence speed of SGD has been improved to a next level [2, 3, 6, 8, 12, 15, 16, 19]. In all of these cited results, the authors have, in one way or another, shown that SGD can converge much faster if one makes a better choice of the stochastic gradient ξt, so that its variance E[‖ξt −∇f(xt)‖22] reduces as t increases. This is known as the variance reduction technique. One particular way to reduce the variance can be described as follows (see for instance Johnson and Zhang [6]). Keep a snapshot x̃ = xt after every m stochastic update steps (where m is some parameter), and compute the full gradient∇f(x̃) only for such snapshots. Then, set ξt = ∇fi(xt)− ∇fi(x̃) +∇f(x̃) as the stochastic gradient. One can verify that, under this choice of ξt, it satisfies E[ξt] = ∇f(xt) and limt→∞ E[‖ξt −∇f(xt)‖22] = 0. Although many variance-reduction based methods have been proposed, most of them only apply to Problem (1.1) when the objective function F (x) is strongly convex [3, 6, 15, 16, 19]. However, in many machine learning applications, F (x) is simply not strongly convex. This is particularly true for Lasso [18] and `1-Regularized Logistic Regression [11], two cornerstone problems extensively used for feature selections.\nOne way to get around this issue is to add a dummy regularizer λ2 ‖x‖22 to F (x), and then to apply any of the above strong-convexity methods. However, the weight of this regularizer, λ, needs to be chosen before the algorithm starts. This adds a lot of difficulty when applying such methods to real life: (1) one needs to tune λ by repeatedly executing the algorithm, and (2) the error of the algorithm does not converge to zero as time goes (in fact, it converges toO(λ) so one needs to know the desired accuracy before the algorithm starts). As we shall demonstrate in the experimental section, adding the dummy regularizer hurts the performance of the algorithm as well.\nAnother possible solution is to tackle the non-strongly convex case directly [2, 8, 12], without using any dummy regularizer. These methods are the so-called anytime algorithms: they can be interrupted at any time, and the error of the produced solution tends to zero as the number of steps increases. While direct methods are much more convenient for practical use, in the big-data scenario, existing direct methods are much slower than indirect methods (i.e., methods via dummy regularization).\nMore specifically, if the desired accuracy is ε and the smoothness of each fi(x) is L, then the gradient complexities (i.e., the number of full gradient evaluations divided by n)1 of the best known direct and indirect methods are respectively\nO ( n+L ε ) and O ( (n+ Lε ) log 1 ε ) .\nThus, direct and indirectly methods are incomparable both on the theoretical and practical side. On the theoretical side, in terms of gradient complexity, indirect methods have less dependency on n and slightly more dependency on L, while direct methods have slightly less dependency on L and more dependency on n. Meanwhile, in practice, when n is usually dominating, indirect methods are faster but less convenient, while direct methods are slower but more convenient.\n1Throughout this paper, we will use gradient complexity as an effective measure of an algorithm’s running time. Usually, the total running time of an algorithm is O(d) multiplied with its gradient complexity, because each∇fi(x) can be computed in O(d) time.\nAlgorithm 1 UniVR(xφ,m0, S, η) 1: x̃0 ← xφ, x10 ← xφ 2: for s← 1 to S do 3: µ̃s−1 ← ∇F (x̃s−1) 4: ms ← 2s ·m0 5: for t← 0 to ms − 1 do 6: Pick i uniformly at random in {1, · · · , n}. 7: ξ ← ∇fi(xst )−∇fi(x̃s−1) + µ̃s−1 8: xst+1 = arg miny∈Rd { 1 2η‖xst − y‖2 + Ψ(y) + 〈ξ, y〉 }\n9: end for 10: x̃s ← 1ms ∑ms t=1 x s t 11: xs+10 ← xsms 12: end for 13: return x̃S .\nOur Result. In this paper, we propose UniVR, a new method that can solve the non-strongly convex case of Problem (1.1) directly. On the theoretical side, UniVR admits a gradient complexity of only O(n log 1ε + L ε ), outperforming both the best known direct and indirect methods. On the practical side, UniVR is a direct anytime method, which is convenient to use.\nWith minor changes of parameters, our algorithm also matches the best known running time for the strongly-convex case. Therefore, UniVR unifies the two important cases of composite stochastic convex optimization theory, and provides an universal framework for solving (1.1). Interestingly enough, experimental results suggest that UniVR is faster than the previous methods for both cases.\nFinally, in both cases, UniVR demands a memory storage ofO(d), matching the best known memory requirement of SVRG [6], and is much cheaper thanO(nd) as required by many others (either direct or indirect methods). Small memory requirement is a demanding feature for machine learning in big data: it is often unrealistic to request a memory storage of O(nd), which is equivalent to the size of the large-scale input."
    }, {
      "heading" : "2 Algorithm Description",
      "text" : "Throughout this paper, we use ‖ · ‖ to denote the Euclidean norm. We assume that each fi(·) is convex, differentiable and L-smooth (or has L-Lipschitz continuous gradient):\n‖∇fi(x)−∇fi(y)‖ ≤ L‖x− y‖, ∀x, y ∈ Rd . In addition, we assume that Ψ(·) is convex and lower semicontinuous. Our algorithm for the non-strongly convex case is presented in Algorithm 1. Given an initial vector xφ, our algorithm is divided into S epochs. The s-th epoch consists of ms stochastic gradient steps (see Line 8 of UniVR), where ms doubles between every consecutive two epochs. This “doubling” feature distinguishes our method from all of the cited variance-reduction based methods.\nWithin each epoch, similar to SVRG [6, 19], we compute the full gradient µ̃s−1 = ∇f(x̃s−1) where x̃s−1 is the average point of the previous epoch. We then use µ̃s−1 to define the variance-reduced version of the stochastic gradient xst (see Line 7 of UniVR). Unlike SVRG, our starting vector x s 0 of each epoch is set to be the ending vector xs−1ms−1 of the previous epoch, rather than the average of the previous epoch. This difference turns out to be essential in order to obtain our improved running time, both in terms of theory (see the proof of Theorem B.2) and practice (see Section 5).\nWe prove in this paper that if m0 and S are positive integers and η = 1/7L, then the output satisfies\nE[F (x̃S)− F (x∗)] ≤ O (F (xφ)− F (x∗) 2S + L‖x∗ − xφ‖2 2Sm0 ) .\nIn addition, the gradient complexity of UniVR is O(n · S + 2S · m0). As a consequence, if we are given parameters Θ,∆ satisfying ‖xφ − x∗‖2 ≤ Θ and F (xφ) − F (x∗) ≤ ∆, then by setting S = log(∆/ε) and m0 = LΘ/∆, we have E[F (x̃S)−F (x∗)] ≤ O(ε), and the gradient complexity of UniVR is O ( n log ∆ε + LΘ ε ) .\nThe Strongly-Convex Case. If f(·) is also assumed to be σ-strongly convex, that is, f(y) ≥ f(x) + 〈∇f(x), y − x〉+ σ\n2 ‖y − x‖2, ∀x, y ∈ Rd ,\nour algorithm can be easily modified with running time matching the state of the arts.\nIn short, we need to choose the same length m = 7L/σ for all the epochs, as well as choose a weighted rather than a uniform average when computing x̃s. We defer the description and the analogous proof of this slightly different algorithm UniVRsc to the appendix. If we are given parameter ∆ satisfying F (xφ)− F (x∗) ≤ ∆, then the gradient complexity of UniVRsc is O (( n+ Lσ ) log ∆ε ) ."
    }, {
      "heading" : "3 Related Work",
      "text" : "If the full gradient is used at each step of the algorithm, a simple gradient descent algorithm converges in O(L/ε) steps and therefore has a gradient complexity O(nL/ε) (see for instance the textbook of Nesterov [10]). This has been improved to O(n √ L/ε) using Nesterov’s accelerated gradient method [9]. If f(·) is also σ-strongly convex, the standard and accelerated versions of the two methods have gradient complexities O(nL/σ log(1/ε)) and O(n √ L/σ log(1/ε)) respectively. However, in the big-data scenario (i.e., with large n), such performances are often unsatisfactory.\nIn the past two decades, a growing amount of attention has been paid towards stochastic gradient descent (SGD) algorithms. When the use of the full gradient ∇f(x) is simply replaced with a stochastic gradient ξt = ∇fi(xt), SGD achieves a convergence rate of O(1/ε2) [20, 22]. Later, a faster O(1/ε) convergence rate was discovered for functions that are strongly convex [5, 14]. Both of these convergence rates turn out to be quite inefficient when we need a more accurate solution.\nIn order to improve SGD, in the past three years, several attempts have been made with the idea of (explicitly or implicitly) reducing the variance of the stochastic gradient. This category of results is summarized in Table 1 and discussed below.\nThe first published method that reduces the variance and overcomes the previous barrier of SGD methods is due to Schmidt, Le Roux, and Bach [12]. Their proposed SAG method selects a random index it ∈ [n] for each iteration t, and adopts the choice ξt = 1n ∑n j=1∇fj(yj), where yj = xt′ and t′ ≤ t is essentially the largest index satisfying it′ = j. Using the idea that ξt becomes closer to the actual gradient ∇f(xt) when t increases, SAG obtains an O(log(1/ε)) convergence (i.e., linear convergence) for strongly convex and smooth objectives, comparing to the O(1/ε) convergence rate of the standard SGD [5, 14], matching that of the full gradient descent [10].\nThis O(log(1/ε)) convergence has also been obtained by several concurrent or subsequent works. For instance, the authors of MISO [8], Finito [3], and SAGA [2] have defined ξt to be of a form slightly different from SAG. The authors of SVRG [6] (and its follow-up work Prox-SVRG [19]) have adopted the idea of “epochs” and defined ξt = ∇if(xt) − ∇if(xt) + ∇f(x̃) like we do in this paper. The algorithm SDCA [16] has also been discovered to be intrinsically performing some “variance reduction” procedure [2, 6, 13].\nAmong the variance-reduction algorithms mentioned above, only SAG, MISO, and SAGA can provide theoretical guarantees for directly solving non-strongly convex objectives (i.e., without adding a dummy regularizer). The best gradient complexity for direct methods before our work is O(n+Lε ) due to SAG and SAGA. On the other hand, if one uses indirect methods, the best gradient complexity is O ( (n+ Lε ) log 1 ε ) , where the asymptotic dependence on ε is weakened to log(1/ε)ε .\nFinally, to stay in the general stochastic setting, in this paper we work directly with smooth functions fi(x), rather than the more structured fi(x) def = φi(〈x, ai〉). In this structured case, the accelerated SDCA method [17], along with subsequent works APCG [7] and SPDC [21], obtains a slightly better gradient complexity O (( n + min { L/ε, √ nL/ε }) log 1ε ) . However, this class of methods require one to work with the dual of the objective, cannot be directly applied to the non-strongly convex case, and run only faster than the primal-only methods when n < √ L/ε."
    }, {
      "heading" : "4 Analysis for the Non-Strongly Convex Case",
      "text" : "For each outer iteration s ∈ [S] and inner iteration t ∈ {0, 1, . . . ,ms−1} of UniVR, we denote by ist the selected random index i ∈ [n] and ξst the stochastic gradient ξ = ∇fist (xst )−∇fist (x̃s−1)+µ̃s−1.\naFollowing the tradition of machine learning literatures, we have assumed in this column that the initial objective distance to the minima, F (xφ)− F (x∗), is a constant for a clean comparison. bFollowing the tradition of machine learning literatures, we have assumed in this column that for the initial vector xφ, both F (xφ)− F (x∗) and ‖xφ − x∗‖2 are constants for a clean comparison. cThis can be reduced to O(d+ n) if T , the total number of iterations, is specified beforehand.\nThen, using the convexity and smoothness of our objective, as well as the definition of our stochastic gradient step, we obtain the following lemma:\nLemma 4.1. For every u ∈ Rd and t ∈ {0, 1, . . . ,ms − 1}, fixing xst and letting i = ist be the random variable, we have\nEist [ F (xst+1)− F (u) ] ≤ Eist [ η 2(1− ηL)‖ξ s t −∇f(xst )‖2 + ‖u− xst‖2 − ‖u− xst+1‖2 2η ] .\nProof. We first upper bound the left hand side: Eist [ F (xst+1)− F (u) ] = Eist [ f(xst+1)− f(u) + Ψ(xst+1)−Ψ(u) ]\n¬ ≤ Eist [ f(xst ) + 〈∇f(xst ), xst+1 − xst 〉+ L2 ‖xst − xst+1‖2 − f(u) + Ψ(xst+1)−Ψ(u) ]\n ≤ Eist [ 〈∇f(xst ), xst − u〉+ 〈∇f(xst ), xst+1 − xst 〉+ L2 ‖xst − xst+1‖2 + Ψ(xst+1)−Ψ(u) ]\n® = Eist [ 〈ξst , xst − u〉+ 〈∇f(xst ), xst+1 − xst 〉+ L2 ‖xst − xst+1‖2 + Ψ(xst+1)−Ψ(u) ] . (4.1) Above, inequalities ¬ and  are respectively due to the smoothness and convexity of f(·), and ® is because Eist [ξ s t ] = ∇f(xst ). Next, using the definition of xst+1 we have 〈ξst , xst − u〉+ Ψ(xst+1)−Ψ(u) = 〈ξst , xst − xst+1〉+ 〈ξst , xst+1 − u〉+ Ψ(xst+1)−Ψ(u) ¯ ≤ 〈ξst , xst − xst+1〉+ 〈− 1\nη (xst+1 − xst ), xst+1 − u〉\n° = 〈ξst , xst − xst+1〉+ ‖u− xst‖2 2η − ‖u− x s t+1‖2 2η − ‖x s t+1 − xst‖2 2η .\nAbove, inequality ¯ holds for the following reason. Recall that the minimality of xst+1 = arg miny∈Rd{ 12η‖y − xst‖2 + Ψ(y) + 〈ξst , y〉} implies the existence of some subgradient g ∈ ∂Ψ(xst+1) which satisfies 1 η (x s t+1 − xst ) + ξst + g = 0. Combining this with Ψ(u) − Ψ(xst+1) ≥ 〈g, u − xst+1〉, which is due to the convexity of Ψ(·), we immediately have Ψ(u) − Ψ(xst+1) + 〈 1η (xst+1 − xst ) + ξst , u − xst+1〉 ≥ 〈 1η (xst+1 − xst ) + ξst + g, u − xst+1〉 = 0. This gives inequality ¯. In addition, ° can be verified by expanding the Euclidean norms.\nCombining the above two inequalities, we have Eist [ F (xst+1)− F (u) ]\n≤ Eist [ 〈ξst −∇f(xst ), xst − xst+1〉 − 1− ηL 2η ‖xst − xst+1‖2 + ‖u− xst‖2 − ‖u− xst+1‖2 2η ]\n± ≤ Eist [ η 2(1− ηL)‖ξ s t −∇f(xst )‖2 + ‖u− xst‖2 − ‖u− xst+1‖2 2η ] .\nAbove, ± is by the Cauchy-Schwarz inequality. The next lemma is classical and analogous to most of the variance reduction literatures (cf. [2, 6, 19]). We include its proof in Appendix A for the sake of completeness. Lemma 4.2. Eist [ ‖ξst −∇f(xst )‖2 ] ≤ 4L · ( F (xst )− F (x∗) + F (x̃s−1)− F (x∗) )\nWe are now ready to state the main theorem for the convergence of UniVR:\nTheorem 4.3. UniVR(xφ,m0, S, η) satisfies if m0 and S are positive integers and η = 1/7L, then\nE[F (x̃S)− F (x∗)] ≤ O (F (xφ)− F (x∗) 2S + L‖x∗ − xφ‖2 2Sm0 ) . (4.2)\nIn addition, UniVR has a gradient complexity of O(S · n+ 2S ·m0). Proof. Combining Lemma 4.1 with u = x∗ and Lemma 4.2, we have\nEist [ F (xst+1)−F (x∗) ] ≤ 2ηL (1− ηL) ( F (xst )−F (x∗)+F (x̃s−1)−F (x∗) ) + ‖x∗ − xst‖2 − Eist ‖x∗ − xst+1‖2 2η .\nChoosing η = 1/7L in the above inequality, summing it up over t = 0, 1, . . . ,ms − 1, and dividing both sides by ms, we arrive at E [ms−1∑\nt=0\nF (xst+1)\nms −F (x∗)\n] ≤ E [1 3 (ms−1∑\nt=0\nF (xst )\nms −F (x∗)+F (x̃s−1)−F (x∗)\n) + ‖x∗ − xs0‖2 − ‖x∗ − xsms‖2\n2η ·ms\n] .\nAfter rearranging, this yields\n2E [ms−1∑\nt=0\nF (xst+1)\nms − F (x∗)\n] ≤ E [ (F (xs0)− F (x∗))− (F (xsms)− F (x∗)) ms + F (x̃s−1)− F (x∗)\n+ ‖x∗ − xs0‖2 − ‖x∗ − xsms‖2\n2η/3 ·ms\n] .\nNext, using the fact that F (x̃s) ≤ ∑ms−1t=0 F (xst+1) ms due to the convexity of F and the definition x̃s = ∑ms−1 t=0 xst+1 ms , as well as the choice xsms = x s+1 0 , we rewrite the above inequality as\n2E [ F (x̃s)− F (x∗) ] ≤ E [ (F (xs0)− F (x∗))− (F (xs+10 )− F (x∗)) ms + F (x̃s−1)− F (x∗) )\n+ ‖x∗ − xs0‖2 − ‖x∗ − xs+10 ‖2\n2η/3 ·ms\n] .\nAfter rearranging and using the fact ms = 2ms−1, we conclude that\n2E [ F (x̃s)− F (x∗) + ‖x ∗ − xs+10 ‖2 4η/3 ·ms + F (xs+10 )− F (x∗) 2ms ]\n≤ E [ F (x̃s−1)− F (x∗) + ‖x ∗ − xs0‖2 4η/3 ·ms−1 + F (xs0)− F (x∗) 2ms−1 ] .\nIn sum, after telescoping for s = 1, 2, . . . , S, we have2\nE[F (x̃S)− F (x∗)] ≤ 2−S · ( F (x̃0)− F (x∗) + ‖x ∗ − x10‖2 4η/3 ·m0 + F (x10)− F (x∗) 2m0 )\n2We can perform telescoping because we set our starting vector xs+10 of each epoch to equal the ending vector xsms of the previous epoch. This is different from SVRG, which chooses the average of the previous epoch as the starting vector. This difference is also beneficial in practice (see Section 5).\n≤ F (x φ)− F (x∗) 2S−1 + ‖x∗ − xφ‖2 2S · 4ηm03 .\nThis finishes the proof of (4.2) due to the choice of η = 1/7L. Finally, UniVR computes S times the full gradient ∇f(·), and ∑Ss=1ms = O(2Sm0) times the gradient ∇fi(·). This gives a total gradient complexity O(S · n+ 2S ·m0). Corollary 4.4. If we are given some parameters Θ,∆ ∈ R+ satisfying ‖xφ − x∗‖2 ≤ Θ and F (xφ)− F (x∗) ≤ ∆, then by setting S = log(∆/ε), m0 = LΘ/∆, and η = 1/7L, we have\nE[F (x̃S)− F (x∗)] ≤ O(ε) , and the gradient complexity of UniVR is O ( n log ( ∆ ε ) + LΘε ) ."
    }, {
      "heading" : "5 Experiment",
      "text" : "In this section, we confirm our theoretical findings using three real-life datasets: (1) the Adult dataset (32, 561 examples and 123 features), (2) the Covtype dataset (581, 012 examples and 54 features), and (3) the 2nd class of the MNIST dataset (60, 000 examples and 780 features) [4]. Following [17], we have normalized each feature vector to have Euclidean norm 1.\nWe perform 3 classification tasks: Lasso, Ridge Regression (RR), and `1-Regularized Logistic Regression (LR). As described in the introduction, Lasso and LR do not admit strongly convex objectives, while RR has a strongly convex objective. We have picked the weight σ of, either the regularizer σ2 ‖x‖22 for RR or the regularizer σ‖x‖21 for Lasso and LR, as follows: • Adult: σ = 0.001, 0.01 and 0.001 for Lasso, LR, and RR respectively. • Covtype: σ = 0.008, 0.005 and 0.006 for Lasso, LR, and RR respectively. • MNIST: σ = 0.0005, 0.003 and 0.00005 for Lasso, LR, and RR respectively.\nWe have implemented the following algorithms: • UniVR with initial epoch size m0 = n/4 and step size η = 0.3 (except for MNIST-Lasso and MNIST-LR, in which we choose η = 1 and η = 5 respectively). • SVRG [6, 19] with (their suggested) epoch size m = 2n and step size η = 0.3 (except for MNIST-Lasso and MNIST-LR, in which we choose η = 1 and η = 5 respectively). Recall that in theory, SVRG is not designed for non-strongly convex objectives, and F (·) needs to be added by a dummy regularizer for Lasso and LR. However, in our experiment, we noticed that the performances of SVRG with and without dummy regularizers are pretty similar. Thus, we have neglected the regularized version of SVRG for a clean comparison. • SAG [12] and SAGA [2] both with step size 0.1.3 • SDCA [15, 16] with both their Option I (steepest descent) and Option IV (constant step size).\nFor Option IV, we use step sizes 0.5 or 1 for all the experiments, which turn out to be the best after hand tuning. Since both these versions of SDCA work only with strongly convex objectives, a dummy regularizer needs to be introduced for Lasso and LR. We have therefore tuned the best regularizer weight for (at most) 16 different values of ε, and connected the points together when we plot the performance curves for SDCA in Figure 1.\nPerformance Comparison. It is clear from Figure 1 that UniVR outperforms all other methods we have considered in this paper. Interestingly enough, although SVRG is not designed for non-strongly convex objectives, it outperforms SAG and SAGA, and runs only twice slower than UniVR.\nFinally, indirect methods via dummy regularization (i.e., SDCA) perform poor. In addition to the large amount of parameter tuning effort of the dummy regularizer that is not reflected in Figure 1, for small ε, since the weight of the added dummy regularizer needs to be roughly proportional to ε, the accuracy of such a method can hardly go below 10−9 for Lasso or LR.\nEffectiveness of Our New Techniques. There are two main differences that distinguish UniVR from the previous works. First, we double the epoch length between every consecutive two epochs; and second, our starting vector of each epoch is set to be the ending vector of the previous one.\n3In our experiment we find out that η = 0.1 is a good choice for both SAG and SAGA for all the datasets.\nLet us first see what if in UniVR, the starting vector is set to be the average vector of the previous epoch (rather than the ending vector like in SVRG). We call this algorithm UniVR-broken. It is clear from Figure 2(a) that this algorithm is much slower than UniVR, and the choice of the average vector is not necessary: it slows down the algorithm by a constant factor.\nWe also notice that the doubling technique is very effective. For instance, in Figure 2(a), between the 40th and 72th pass of the dataset, our UniVR has stayed inside the same epoch (of length 32n) without ever recomputing the full gradient. Within this epoch, the objective decrease rate remains sharp, and thus a short epoch length (such as 2n in SVRG) is not really necessary. Our doubling technique is also robust against the choice of m0, as illustrated in Figure 2(b)."
    }, {
      "heading" : "A Proof of Lemma 4.2",
      "text" : "Lemma 4.2. Eist [ ‖ξst −∇f(xst )‖2 ] ≤ 4L · ( F (xst )− F (x∗) + F (x̃s−1)− F (x∗) )\nProof. The proof of this lemma is classical and is analogous to most of the variance reduction literatures (cf. [2, 6, 19]). Indeed,\nEist [ ‖ξst −∇f(xst )‖2 ] = Eist [∥∥(∇fist (xst )−∇fist (x̃s−1) ) − ( ∇f(xst )−∇f(x̃s−1) )∥∥2]\n¬ ≤ Eist [∥∥∇fist (xst )−∇fist (x̃s−1) ∥∥2] = Eist [∥∥(∇fist (xst )−∇fist (x∗) ) − ( ∇fist (x̃s−1)−∇fist (x∗) )∥∥2]\n ≤ 2 · Eist [∥∥∇fist (xst )−∇fist (x∗) ∥∥2 + ∥∥∇fist (x̃s−1)−∇fist (x∗) ∥∥2] .\nAbove, ¬ is because for any random vector ζ ∈ Rd, it holds that E‖ζ − Eζ‖2 = E‖ζ‖2 − ‖Eζ‖2, and  is because for any two vectors a, b ∈ Rd, it holds that ‖a− b‖2 ≤ 2‖a‖2 + 2‖b‖2. Next, the classical smoothness assumption on a function fi yields (see for instance [10, Theorem 2.1.5]) ‖∇fi(x) − ∇fi(x∗)‖2 ≤ 2L [ fi(x) − fi(x∗) − 〈∇fi(x∗), x − x∗)〉. Plugging this into the above inequality, we have\nEist [ ‖ξst −∇f(xst )‖2 ]\n≤ 4L · Eist [ fist (x s t )− fist (x∗)− 〈∇fist (x∗), xst − x∗〉+ fist (x̃s−1)− fist (x∗)− 〈∇fist (x∗), x̃s−1 − x∗〉 ] = 4L · ( f(xst )− f(x∗)− 〈∇f(x∗), xst − x∗〉+ f(x̃s−1)− f(x∗)− 〈∇f(x∗), x̃s−1 − x∗〉 ) = 4L · ( f(xst )− f(x∗) + 〈g∗, xst − x∗〉+ f(x̃s−1)− f(x∗) + 〈g∗, x̃s−1 − x∗〉 ) ≤ 4L · ( f(xst )− f(x∗) + Ψ(xst )−Ψ(x∗) + f(x̃s−1)− f(x∗) + Ψ(x̃s−1)−Ψ(x∗) ) = 4L · ( F (xst )− F (x∗) + F (x̃s−1)− F (x∗) ) .\nAbove, g∗ ∈ ∂Ψ(x∗) is the subgradient of Ψ at x∗ that satisfies∇f(x∗) + g∗ = 0."
    }, {
      "heading" : "B Analysis for the Strongly Convex Case",
      "text" : "In this section, we show that if f(·) is also assumed to be σ-strongly convex, our algorithm UniVR can be easily modified to have a gradient complexity matching the state of the arts for the stronglyconvex case.\nOur algorithm UniVRsc for the strongly convex case is presented in Algorithm 2. Given an initial vector xφ, our algorithm is again divided into S epochs, where each epoch is of length m for the same m. Unlike UniVR, we choose a weighted average x̃s ← 1∑m\nt=1(1−ση)−t ∑m t=1 xst (1−ση)t rather\nthan a uniform average x̃s ← 1m ∑m t=1 x s t in each epoch.\nAs in Section 4, for each outer iteration s ∈ [S] and inner iteration t ∈ {0, 1, . . . ,m − 1} of UniVRsc, we denote by ist the selected random index i ∈ [n] and ξst the stochastic gradient ξ = ∇fist (xst )−∇fist (x̃s−1) + µ̃s−1. Then, the following lemma is a counterpart of Lemma 4.1 where the only difference is the use of the strong convexity parameter σ:\nLemma B.1. For every u ∈ Rd and t ∈ {0, 1, . . . ,m − 1}, fixing xst and letting i = ist be the random variable, we have\nEist [ F (xst+1)−F (u) ] ≤ Eist [ η 2(1− ηL)‖ξ s t −∇f(xst )‖2 + (1− ση)‖u− xst‖2 − ‖u− xst+1‖2 2η ] .\nProof. We first upper bound the left hand side using the strong convexity and smoothness of f(·): Eist [ F (xst+1)− F (u) ]\n= Eist [ f(xst+1)− f(u) + Ψ(xst+1)−Ψ(u) ]\nAlgorithm 2 UniVRsc(xφ,m, S, η) 1: x̃0 ← xφ, x10 ← xφ 2: for s← 1 to S do 3: µ̃s−1 ← ∇F (x̃s−1) 4: for t← 0 to m− 1 do 5: Pick i uniformly at random in {1, · · · , n}. 6: ξ ← ∇fi(xst )−∇fi(x̃s−1) + µ̃s−1 7: xst+1 = arg miny∈Rd { 1 2η‖xst − y‖2 + Ψ(y) + 〈ξ, y〉 }\n8: end for 9: x̃s ← 1∑m t=1(1−ση)−t ∑m t=1 xst (1−ση)t\n10: xs+10 ← xsm 11: end for 12: return x̃S .\n≤ Eist [ f(xst ) + 〈∇f(xst ), xst+1 − xst 〉+ L\n2 ‖xst − xst+1‖2 − f(u) + Ψ(xst+1)−Ψ(u)\n]\n≤ Eist [ 〈∇f(xst ), xst − u〉 − σ\n2 ‖xst − u‖2 + 〈∇f(xst ), xst+1 − xst 〉+\nL 2 ‖xst − xst+1‖2 + Ψ(xst+1)−Ψ(u)\n]\n= Eist [ 〈ξst , xst − u〉 − σ\n2 ‖xst − u‖2 + 〈∇f(xst ), xst+1 − xst 〉+\nL 2 ‖xst − xst+1‖2 + Ψ(xst+1)−Ψ(u)\n]\n(B.1)\nAbove, the term σ2 ‖xst − u‖2 is due to the σ-strong convexity of f(·), and this is the only difference between the inequalities (B.1) and (4.1). Therefore, Lemma B.1 can be proven using exactly the identical rest of the proof of Lemma 4.1.\nWe are now ready to state the main theorem for the convergence of UniVRsc in the strongly convex case.\nTheorem B.2. UniVRsc(xφ,m, S, η) satisfies if S is a positive integer, η = 1/7L, and m = d 1ση e, then\nE[F (x̃S)− F (x∗)] ≤ O(2−S) · ( F (xφ)− F (x∗) ) . (B.2)\nIn addition, UniVRsc has a gradient complexity of O ( S · (n+ Lσ ) ) .\nProof. Combining Lemma B.1 with u = x∗ and Lemma 4.2, we have\nEist [ F (xst+1)− F (x∗) ] ≤ 2ηL (1− ηL) ( F (xst )− F (x∗) + F (x̃s−1)− F (x∗) )\n+ (1− ση)‖x∗ − xst‖2 − Eist ‖x∗ − xst+1‖2\n2η . (B.3)\nNext, let us define βt def = 1(1−ση)t for t = 0, 1, . . . ,m. Choosing η = 1/7L, and multiplying inequality (B.3) by βt+1 on both sides, we obtain that for every t ∈ {0, 1, . . . ,m− 1},\nβt+1Eist [ F (xst+1)− F (x∗) ] ≤ βt+1\n3\n( F (xst )− F (x∗) + F (x̃s−1)− F (x∗) )\n+ βt‖x∗ − xst‖2 − βt+1Eist ‖x∗ − xst+1‖2\n2η\n≤ βt 3\n( F (xst )− F (x∗) ) + βt+1\n3\n( F (x̃s−1)− F (x∗) )\n+ βt‖x∗ − xst‖2 − βt+1Eist ‖x∗ − xst+1‖2\n2η .\nSumming up the above inequality over t = 0, 1, . . . ,m−1, and dividing both sides by β def= ∑mt=1 βt, we arrive at\nE [m−1∑\nt=0\nβt+1F (x s t+1)\nβ − F (x∗)\n] ≤ E [1 3 (m−1∑\nt=0\nβtF (x s t )\nβ − F (x∗) + F (x̃s−1)− F (x∗)\n)\n+ β0‖x∗ − xs0‖2 − βm‖x∗ − xsm‖2 2η · β ] .\nAfter rearranging, we obtain\n2E [m−1∑\nt=0\nβt+1F (x s t+1)\nβ − F (x∗)\n] ≤ E [β0(F (xs0)− F (x∗))− βm(F (xsm)− F (x∗)) β + F (x̃s−1)− F (x∗)\n+ β0‖x∗ − xs0‖2 − βm‖x∗ − xsm‖2 2η/3 · β ] .\nNext, using the fact that F (x̃s) ≤ ∑m−1t=0 βt+1F (x s t+1) β due to the convexity of F and the definition x̃s def = 1β ∑m t=1 βtx s t , as well as the choice x s m = x s+1 0 , and the choice β0 = 1 and βm = (1 − ση)−m ≥ (1− ση)−1/ση > 2, we can rewrite the above inequality as\n2E [ F (x̃s)− F (x∗) ] ≤ E [ (F (xs0)− F (x∗))− 2(F (xs+10 )− F (x∗)) β + F (x̃s−1)− F (x∗) )\n+ ‖x∗ − xs0‖2 − 2‖x∗ − xs+10 ‖2 2η/3 · β ] .\nAfter rearranging, we conclude that\n2E [ F (x̃s)− F (x∗) + ‖x ∗ − xs+10 ‖2 2η/3 · β + F (xs+10 )− F (x∗) β ]\n≤ E [ F (x̃s−1)− F (x∗) + ‖x ∗ − xs0‖2 2η/3 · β + F (xs0)− F (x∗) β ] .\nIn sum, after telescoping for s = 1, 2, . . . , S, we have\nE[F (x̃S)− F (x∗)] ≤ 2−S · ( F (x̃0)− F (x∗) + ‖x ∗ − x10‖2 2η/3 · β + F (x 1 0)− F (x∗) )\n≤ F (x φ)− F (x∗) 2S−1 + ‖x∗ − xφ‖2\n2S · 2ηβ3 ≤ F (x\nφ)− F (x∗) 2S−1 + F (xφ)− F (x∗)\n2S · 2σηβ3 ≤ O(2−S) · ( F (xφ)− F (x∗) ) .\nAbove, the last inequality uses the fact that β ≥ m · 1 ≥ 1ση , and this finishes the proof of (B.2). Finally, it is clear that UniVRsc computes S times the full gradient∇f(·), and Sm times the gradient ∇fi(·). This gives a total gradient complexity O(S · (n+m)) = O(S · (n+ Lσ )). Corollary B.3. If we are given some parameter ∆ > 0 satisfying F (xφ) − F (x∗) ≤ ∆, then by setting S = log(∆/ε), η = 1/7L, and m = d 1ση e, we have\nE[F (x̃S)− F (x∗)] ≤ O(ε) , and the gradient complexity of UniVRsc is O ( log (\n∆ ε ) · ( n+ Lσ )) ."
    } ],
    "references" : [ {
      "title" : "SAGA: A Fast Incremental Gradient Method With Support for Non-Strongly Convex Composite Objectives",
      "author" : [ "Aaron Defazio", "Francis Bach", "Simon Lacoste-Julien" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2014
    }, {
      "title" : "Finito: A Faster, Permutable Incremental Gradient Method for Big Data Problems",
      "author" : [ "Aaron J. Defazio", "Tibério S. Caetano", "Justin Domke" ],
      "venue" : "In Proceedings of the 31st International Conference on Machine Learning,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2014
    }, {
      "title" : "LIBSVM Data: Classification, Regression and Multi-label",
      "author" : [ "Rong-En Fan", "Chih-Jen Lin" ],
      "venue" : null,
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2015
    }, {
      "title" : "Logarithmic regret algorithms for online convex optimization",
      "author" : [ "Elad Hazan", "Amit Agarwal", "Satyen Kale" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2007
    }, {
      "title" : "Accelerating stochastic gradient descent using predictive variance reduction",
      "author" : [ "Rie Johnson", "Tong Zhang" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2013
    }, {
      "title" : "An Accelerated Proximal Coordinate Gradient Method and its Application to Regularized Empirical Risk Minimization",
      "author" : [ "Qihang Lin", "Zhaosong Lu", "Lin Xiao" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2014
    }, {
      "title" : "Incremental Majorization-Minimization Optimization with Application to Large-Scale Machine Learning",
      "author" : [ "Julien Mairal" ],
      "venue" : "SIAM Journal on Optimization,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2015
    }, {
      "title" : "A method of solving a convex programming problem with convergence rate O(1/k)",
      "author" : [ "Yurii Nesterov" ],
      "venue" : "In Doklady AN SSSR (translated as Soviet Mathematics Doklady),",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 1983
    }, {
      "title" : "Introductory Lectures on Convex Programming Volume: A Basic course, volume I",
      "author" : [ "Yurii Nesterov" ],
      "venue" : "Kluwer Academic Publishers,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2004
    }, {
      "title" : "Feature selection, L1 vs. L2 regularization, and rotational invariance",
      "author" : [ "Andrew Y. Ng" ],
      "venue" : "In Proceedings of the 21st International Conference on Machine Learning,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2004
    }, {
      "title" : "Minimizing finite sums with the stochastic average gradient",
      "author" : [ "Mark Schmidt", "Nicolas Le Roux", "Francis Bach" ],
      "venue" : "arXiv preprint arXiv:1309.2388,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2013
    }, {
      "title" : "SDCA without Duality",
      "author" : [ "Shai Shalev-Shwartz" ],
      "venue" : "arXiv preprint arXiv:1502.06177,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2015
    }, {
      "title" : "Logarithmic regret algorithms for strongly convex repeated games",
      "author" : [ "Shai Shalev-Shwartz", "Yoram Singer" ],
      "venue" : "Technical report,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2007
    }, {
      "title" : "Proximal Stochastic Dual Coordinate Ascent",
      "author" : [ "Shai Shalev-Shwartz", "Tong Zhang" ],
      "venue" : "arXiv preprint arXiv:1211.2717,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2012
    }, {
      "title" : "Stochastic dual coordinate ascent methods for regularized loss minimization",
      "author" : [ "Shai Shalev-Shwartz", "Tong Zhang" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2013
    }, {
      "title" : "Accelerated Proximal Stochastic Dual Coordinate Ascent for Regularized Loss Minimization",
      "author" : [ "Shai Shalev-Shwartz", "Tong Zhang" ],
      "venue" : "In Proceedings of the 31st International Conference on Machine Learning,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2014
    }, {
      "title" : "Regression shrinkage and selection via the lasso",
      "author" : [ "Robert Tibshirani" ],
      "venue" : "Journal of the Royal Statistical Society. Series B (Methodological),",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 1996
    }, {
      "title" : "A Proximal Stochastic Gradient Method with Progressive Variance Reduction",
      "author" : [ "Lin Xiao", "Tong Zhang" ],
      "venue" : "SIAM Journal on Optimization,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2014
    }, {
      "title" : "Solving large scale linear prediction problems using stochastic gradient descent algorithms",
      "author" : [ "Tong Zhang" ],
      "venue" : "In Proceedings of the 21st International Conference on Machine Learning,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2004
    }, {
      "title" : "Stochastic Primal-Dual Coordinate Method for Regularized Empirical Risk Minimization",
      "author" : [ "Yuchen Zhang", "Lin Xiao" ],
      "venue" : "In Proceedings of the 32nd International Conference on Machine Learning,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 18,
      "context" : "Since the computation of ∇fi(x) is usually n times faster than that of ∇f(x), SGD has been successfully applied to many large-scale learning problems, see for instance [1, 20].",
      "startOffset" : 168,
      "endOffset" : 175
    }, {
      "referenceID" : 0,
      "context" : "More recently, the convergence speed of SGD has been improved to a next level [2, 3, 6, 8, 12, 15, 16, 19].",
      "startOffset" : 78,
      "endOffset" : 106
    }, {
      "referenceID" : 1,
      "context" : "More recently, the convergence speed of SGD has been improved to a next level [2, 3, 6, 8, 12, 15, 16, 19].",
      "startOffset" : 78,
      "endOffset" : 106
    }, {
      "referenceID" : 4,
      "context" : "More recently, the convergence speed of SGD has been improved to a next level [2, 3, 6, 8, 12, 15, 16, 19].",
      "startOffset" : 78,
      "endOffset" : 106
    }, {
      "referenceID" : 6,
      "context" : "More recently, the convergence speed of SGD has been improved to a next level [2, 3, 6, 8, 12, 15, 16, 19].",
      "startOffset" : 78,
      "endOffset" : 106
    }, {
      "referenceID" : 10,
      "context" : "More recently, the convergence speed of SGD has been improved to a next level [2, 3, 6, 8, 12, 15, 16, 19].",
      "startOffset" : 78,
      "endOffset" : 106
    }, {
      "referenceID" : 13,
      "context" : "More recently, the convergence speed of SGD has been improved to a next level [2, 3, 6, 8, 12, 15, 16, 19].",
      "startOffset" : 78,
      "endOffset" : 106
    }, {
      "referenceID" : 14,
      "context" : "More recently, the convergence speed of SGD has been improved to a next level [2, 3, 6, 8, 12, 15, 16, 19].",
      "startOffset" : 78,
      "endOffset" : 106
    }, {
      "referenceID" : 17,
      "context" : "More recently, the convergence speed of SGD has been improved to a next level [2, 3, 6, 8, 12, 15, 16, 19].",
      "startOffset" : 78,
      "endOffset" : 106
    }, {
      "referenceID" : 4,
      "context" : "One particular way to reduce the variance can be described as follows (see for instance Johnson and Zhang [6]).",
      "startOffset" : 106,
      "endOffset" : 109
    }, {
      "referenceID" : 1,
      "context" : "1) when the objective function F (x) is strongly convex [3, 6, 15, 16, 19].",
      "startOffset" : 56,
      "endOffset" : 74
    }, {
      "referenceID" : 4,
      "context" : "1) when the objective function F (x) is strongly convex [3, 6, 15, 16, 19].",
      "startOffset" : 56,
      "endOffset" : 74
    }, {
      "referenceID" : 13,
      "context" : "1) when the objective function F (x) is strongly convex [3, 6, 15, 16, 19].",
      "startOffset" : 56,
      "endOffset" : 74
    }, {
      "referenceID" : 14,
      "context" : "1) when the objective function F (x) is strongly convex [3, 6, 15, 16, 19].",
      "startOffset" : 56,
      "endOffset" : 74
    }, {
      "referenceID" : 17,
      "context" : "1) when the objective function F (x) is strongly convex [3, 6, 15, 16, 19].",
      "startOffset" : 56,
      "endOffset" : 74
    }, {
      "referenceID" : 16,
      "context" : "This is particularly true for Lasso [18] and `1-Regularized Logistic Regression [11], two cornerstone problems extensively used for feature selections.",
      "startOffset" : 36,
      "endOffset" : 40
    }, {
      "referenceID" : 9,
      "context" : "This is particularly true for Lasso [18] and `1-Regularized Logistic Regression [11], two cornerstone problems extensively used for feature selections.",
      "startOffset" : 80,
      "endOffset" : 84
    }, {
      "referenceID" : 0,
      "context" : "Another possible solution is to tackle the non-strongly convex case directly [2, 8, 12], without using any dummy regularizer.",
      "startOffset" : 77,
      "endOffset" : 87
    }, {
      "referenceID" : 6,
      "context" : "Another possible solution is to tackle the non-strongly convex case directly [2, 8, 12], without using any dummy regularizer.",
      "startOffset" : 77,
      "endOffset" : 87
    }, {
      "referenceID" : 10,
      "context" : "Another possible solution is to tackle the non-strongly convex case directly [2, 8, 12], without using any dummy regularizer.",
      "startOffset" : 77,
      "endOffset" : 87
    }, {
      "referenceID" : 4,
      "context" : "Finally, in both cases, UniVR demands a memory storage ofO(d), matching the best known memory requirement of SVRG [6], and is much cheaper thanO(nd) as required by many others (either direct or indirect methods).",
      "startOffset" : 114,
      "endOffset" : 117
    }, {
      "referenceID" : 4,
      "context" : "Within each epoch, similar to SVRG [6, 19], we compute the full gradient μ̃s−1 = ∇f(x̃s−1) where x̃s−1 is the average point of the previous epoch.",
      "startOffset" : 35,
      "endOffset" : 42
    }, {
      "referenceID" : 17,
      "context" : "Within each epoch, similar to SVRG [6, 19], we compute the full gradient μ̃s−1 = ∇f(x̃s−1) where x̃s−1 is the average point of the previous epoch.",
      "startOffset" : 35,
      "endOffset" : 42
    }, {
      "referenceID" : 8,
      "context" : "If the full gradient is used at each step of the algorithm, a simple gradient descent algorithm converges in O(L/ε) steps and therefore has a gradient complexity O(nL/ε) (see for instance the textbook of Nesterov [10]).",
      "startOffset" : 213,
      "endOffset" : 217
    }, {
      "referenceID" : 7,
      "context" : "This has been improved to O(n √ L/ε) using Nesterov’s accelerated gradient method [9].",
      "startOffset" : 82,
      "endOffset" : 85
    }, {
      "referenceID" : 18,
      "context" : "When the use of the full gradient ∇f(x) is simply replaced with a stochastic gradient ξt = ∇fi(xt), SGD achieves a convergence rate of O(1/ε) [20, 22].",
      "startOffset" : 142,
      "endOffset" : 150
    }, {
      "referenceID" : 3,
      "context" : "Later, a faster O(1/ε) convergence rate was discovered for functions that are strongly convex [5, 14].",
      "startOffset" : 94,
      "endOffset" : 101
    }, {
      "referenceID" : 12,
      "context" : "Later, a faster O(1/ε) convergence rate was discovered for functions that are strongly convex [5, 14].",
      "startOffset" : 94,
      "endOffset" : 101
    }, {
      "referenceID" : 10,
      "context" : "The first published method that reduces the variance and overcomes the previous barrier of SGD methods is due to Schmidt, Le Roux, and Bach [12].",
      "startOffset" : 140,
      "endOffset" : 144
    }, {
      "referenceID" : 3,
      "context" : ", linear convergence) for strongly convex and smooth objectives, comparing to the O(1/ε) convergence rate of the standard SGD [5, 14], matching that of the full gradient descent [10].",
      "startOffset" : 126,
      "endOffset" : 133
    }, {
      "referenceID" : 12,
      "context" : ", linear convergence) for strongly convex and smooth objectives, comparing to the O(1/ε) convergence rate of the standard SGD [5, 14], matching that of the full gradient descent [10].",
      "startOffset" : 126,
      "endOffset" : 133
    }, {
      "referenceID" : 8,
      "context" : ", linear convergence) for strongly convex and smooth objectives, comparing to the O(1/ε) convergence rate of the standard SGD [5, 14], matching that of the full gradient descent [10].",
      "startOffset" : 178,
      "endOffset" : 182
    }, {
      "referenceID" : 6,
      "context" : "For instance, the authors of MISO [8], Finito [3], and SAGA [2] have defined ξt to be of a form slightly different from SAG.",
      "startOffset" : 34,
      "endOffset" : 37
    }, {
      "referenceID" : 1,
      "context" : "For instance, the authors of MISO [8], Finito [3], and SAGA [2] have defined ξt to be of a form slightly different from SAG.",
      "startOffset" : 46,
      "endOffset" : 49
    }, {
      "referenceID" : 0,
      "context" : "For instance, the authors of MISO [8], Finito [3], and SAGA [2] have defined ξt to be of a form slightly different from SAG.",
      "startOffset" : 60,
      "endOffset" : 63
    }, {
      "referenceID" : 4,
      "context" : "The authors of SVRG [6] (and its follow-up work Prox-SVRG [19]) have adopted the idea of “epochs” and defined ξt = ∇if(xt) − ∇if(xt) + ∇f(x̃) like we do in this paper.",
      "startOffset" : 20,
      "endOffset" : 23
    }, {
      "referenceID" : 17,
      "context" : "The authors of SVRG [6] (and its follow-up work Prox-SVRG [19]) have adopted the idea of “epochs” and defined ξt = ∇if(xt) − ∇if(xt) + ∇f(x̃) like we do in this paper.",
      "startOffset" : 58,
      "endOffset" : 62
    }, {
      "referenceID" : 14,
      "context" : "The algorithm SDCA [16] has also been discovered to be intrinsically performing some “variance reduction” procedure [2, 6, 13].",
      "startOffset" : 19,
      "endOffset" : 23
    }, {
      "referenceID" : 0,
      "context" : "The algorithm SDCA [16] has also been discovered to be intrinsically performing some “variance reduction” procedure [2, 6, 13].",
      "startOffset" : 116,
      "endOffset" : 126
    }, {
      "referenceID" : 4,
      "context" : "The algorithm SDCA [16] has also been discovered to be intrinsically performing some “variance reduction” procedure [2, 6, 13].",
      "startOffset" : 116,
      "endOffset" : 126
    }, {
      "referenceID" : 11,
      "context" : "The algorithm SDCA [16] has also been discovered to be intrinsically performing some “variance reduction” procedure [2, 6, 13].",
      "startOffset" : 116,
      "endOffset" : 126
    }, {
      "referenceID" : 15,
      "context" : "In this structured case, the accelerated SDCA method [17], along with subsequent works APCG [7] and SPDC [21], obtains a slightly better gradient complexity O (( n + min { L/ε, √ nL/ε }) log 1ε ) .",
      "startOffset" : 53,
      "endOffset" : 57
    }, {
      "referenceID" : 5,
      "context" : "In this structured case, the accelerated SDCA method [17], along with subsequent works APCG [7] and SPDC [21], obtains a slightly better gradient complexity O (( n + min { L/ε, √ nL/ε }) log 1ε ) .",
      "startOffset" : 92,
      "endOffset" : 95
    }, {
      "referenceID" : 19,
      "context" : "In this structured case, the accelerated SDCA method [17], along with subsequent works APCG [7] and SPDC [21], obtains a slightly better gradient complexity O (( n + min { L/ε, √ nL/ε }) log 1ε ) .",
      "startOffset" : 105,
      "endOffset" : 109
    }, {
      "referenceID" : 4,
      "context" : "SVRG [6] Prox-SVRG [19] yes O(d) O ( (n+ L σ ) log 1 ε ) no O ( (n+ L ε ) log 1 ε )",
      "startOffset" : 5,
      "endOffset" : 8
    }, {
      "referenceID" : 17,
      "context" : "SVRG [6] Prox-SVRG [19] yes O(d) O ( (n+ L σ ) log 1 ε ) no O ( (n+ L ε ) log 1 ε )",
      "startOffset" : 19,
      "endOffset" : 23
    }, {
      "referenceID" : 1,
      "context" : "Finito [3] no O(nd) O ( n log L/σ ε ) (only when n ≥ L/σ) no -",
      "startOffset" : 7,
      "endOffset" : 10
    }, {
      "referenceID" : 14,
      "context" : "SDCA [16] Prox-SDCA [15] yes O(Td+ n) c O ( (n+ L σ ) log L/σ+n ε ) no O ( (n+ L ε ) log L+n ε )",
      "startOffset" : 5,
      "endOffset" : 9
    }, {
      "referenceID" : 13,
      "context" : "SDCA [16] Prox-SDCA [15] yes O(Td+ n) c O ( (n+ L σ ) log L/σ+n ε ) no O ( (n+ L ε ) log L+n ε )",
      "startOffset" : 20,
      "endOffset" : 24
    }, {
      "referenceID" : 6,
      "context" : "MISO [8] no O(nd) O ( nL σ log L/σ ε )",
      "startOffset" : 5,
      "endOffset" : 8
    }, {
      "referenceID" : 10,
      "context" : "SAG [12] no O(nd) O ( (n+ L σ ) log L/(σn)+1 ε ) yes O ( n+L ε )",
      "startOffset" : 4,
      "endOffset" : 8
    }, {
      "referenceID" : 0,
      "context" : "SAGA [2] yes O(nd) O ( (n+ L σ ) log min{L/σ,n} ε ) yes O ( n+L ε )",
      "startOffset" : 5,
      "endOffset" : 8
    }, {
      "referenceID" : 0,
      "context" : "[2, 6, 19]).",
      "startOffset" : 0,
      "endOffset" : 10
    }, {
      "referenceID" : 4,
      "context" : "[2, 6, 19]).",
      "startOffset" : 0,
      "endOffset" : 10
    }, {
      "referenceID" : 17,
      "context" : "[2, 6, 19]).",
      "startOffset" : 0,
      "endOffset" : 10
    }, {
      "referenceID" : 2,
      "context" : "In this section, we confirm our theoretical findings using three real-life datasets: (1) the Adult dataset (32, 561 examples and 123 features), (2) the Covtype dataset (581, 012 examples and 54 features), and (3) the 2nd class of the MNIST dataset (60, 000 examples and 780 features) [4].",
      "startOffset" : 284,
      "endOffset" : 287
    }, {
      "referenceID" : 15,
      "context" : "Following [17], we have normalized each feature vector to have Euclidean norm 1.",
      "startOffset" : 10,
      "endOffset" : 14
    }, {
      "referenceID" : 4,
      "context" : "• SVRG [6, 19] with (their suggested) epoch size m = 2n and step size η = 0.",
      "startOffset" : 7,
      "endOffset" : 14
    }, {
      "referenceID" : 17,
      "context" : "• SVRG [6, 19] with (their suggested) epoch size m = 2n and step size η = 0.",
      "startOffset" : 7,
      "endOffset" : 14
    }, {
      "referenceID" : 10,
      "context" : "• SAG [12] and SAGA [2] both with step size 0.",
      "startOffset" : 6,
      "endOffset" : 10
    }, {
      "referenceID" : 0,
      "context" : "• SAG [12] and SAGA [2] both with step size 0.",
      "startOffset" : 20,
      "endOffset" : 23
    }, {
      "referenceID" : 13,
      "context" : "3 • SDCA [15, 16] with both their Option I (steepest descent) and Option IV (constant step size).",
      "startOffset" : 9,
      "endOffset" : 17
    }, {
      "referenceID" : 14,
      "context" : "3 • SDCA [15, 16] with both their Option I (steepest descent) and Option IV (constant step size).",
      "startOffset" : 9,
      "endOffset" : 17
    } ],
    "year" : 2017,
    "abstractText" : "We revisit an important class of composite stochastic minimization problems that often arises from empirical risk minimization settings, such as Lasso, Ridge Regression, and Logistic Regression. We present a new algorithm UniVR based on stochastic gradient descent with variance reduction. Our algorithm supports non-strongly convex objectives directly, and outperforms all of the state-of-the-art algorithms, including both direct algorithms (SAG, MISO, and SAGA) and indirect algorithms (SVRG, ProxSVRG, SDCA, ProxSDCA, and Finito) for such objectives. Our algorithm supports strongly convex objectives as well, and matches the best known linear convergence rate. Experiments support our theory. As a result, UniVR closes an interesting gap in the literature because all the existing direct algorithms for the non-strongly convex case perform much slower than the indirect algorithms. We thus believe that UniVR provides a unification between the strongly and the non-strongly convex stochastic minimization theories.",
    "creator" : "LaTeX with hyperref package"
  }
}