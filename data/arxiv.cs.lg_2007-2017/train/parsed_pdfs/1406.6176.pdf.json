{
  "name" : "1406.6176.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Composite Likelihood Estimation for Restricted Boltzmann machines",
    "authors" : [ "Muneki Yasuda", "Shun Kataoka", "Yuji Waizumi" ],
    "emails" : [ "muneki@yz.yamagata-u.ac.jp" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n40 6.\n61 76\nv1 [\ncs .L\nG ]\n2 4\nJu n\n20 14"
    }, {
      "heading" : "1. Introduction",
      "text" : "Learning the parameters of graphical models using maximum likelihood (ML) estimation is generally hard due to the intractability of computing the normalizing constant and its gradients. Maximum pseudo-likelihood (PL) estimation [2] is a statistical approximation of the ML estimation. Unlike the ML estimation, the maximum PL estimation is computationally fast, but however, the estimates obtained by this method are not very accurate.\nComposite likelihoods (CLs) [6] are higher-order generalizations of the PL. Asymptotic analysis shows that maximum CL estimation is statistically more efficient than the maximum PL estimation [5]. It has been known that the maximum PL estimation is asymptotically consistent [2]. Like this, the maximum CL estimation is also asymptotically consistent [6]. Furthermore, the maximum CL estimation has an asymptotic variance that is smaller than the maximum PL estimation but larger than ML estimation [5, 3]. Recently, it has been found that the maximum CL estimation corresponds to a block-wise contrastive divergence learning [1].\nIn the maximum CL estimation, one can freely choose the size of “blocks” which contain several vari-\nables, and it is widely believed that by increasing the size of blocks, one can capture more dependency relations in the model and increase the accuracy of the estimates [1]. In the first part of this paper, we introduce a systematic choice of blocks in the maximum CL estimation. In our proposed choice of blocks, it is guaranteed that one can obtain quantitatively closer value to the true likelihood by increasing the size of blocks. In the latter part of this paper, we apply our maximum CL estimation to restricted Boltzmann machines (RBMs) [4] and show results of numerical experiments using synthetic data."
    }, {
      "heading" : "2. Composite Likelihood Estimation",
      "text" : "For the n dimensional discrete random variable x := {xi | i ∈ Ω = {1, 2, . . . , n}}, let us consider the probabilistic model expressed as\nP (x | θ) := Z(θ)−1 exp ( − E(x | θ) ) , (1)\nwhere E(x | θ) is the energy function having an arbitrary functional form and Z(θ) := ∑\nx exp\n(\n− E(x |\nθ) )\nis the normalizing constant. Let us suppose that the data set composed of M data, D := {d(µ) | µ = 1, 2, . . . ,M} is obtained. Each data is statisticallyindependent of each other. In the perspective of the ML estimation, we determine the optimal θ by maximizing the log-likelihood function defined by\nLML(θ) := ∑\nx\nQ(x) lnP (x | θ), (2)\nwhere Q(x) is the empirical distribution of the data set, i.e. the histogram of data set, expressed by Q(x) := M−1\n∑M µ=1 δ(x,d (µ)), where we define\nδ(x,d(µ)) :=\n{\n1 x = d(µ) 0 x 6= d(µ) .\nHowever, maximizing LML(θ) with respect to θ is computationally expensive. This generally requires the computational cost of O(en) due to multiple summations.\nThe maximum CL estimation is a statistical approximation technique of the ML estimation [6]. In the maximum CL estimation, one divides Ω into some different subsets termed blocks, c1, c2, . . . cr ⊆ Ω, with allowing overlaps among blocks. Note that the relation c1∪c2∪ . . .∪cr = Ω must be kept. We denote the family of these blocks, c1, c2, . . . cr, by F . For the family F , the CL is defined by\nLF(θ) := ΛF ∑\nc∈F\n∑\nx\nQ(x) lnP (xc | xc̄, θ), (3)\nwhere, for a set A ⊆ Ω, the expression xA is defined as xA := {xi | i ∈ A} and Ā := Ω \\ A. The notation ΛF is defined by ΛF := |F|−1, where the notation | · · · | denotes the size of the assigned set. From the Bayesian theorem, the conditional probability in the CL is obtained by P (xc | xc̄, θ) = P (x | θ) ( ∑\nxc P (x |\nθ) )−1\n. In the CL estimation, one maximizes the CL instead of the true log-likelihood. If each block is composed of just one variable, i.e. ci = {i} and r = n, the CL is reduced to the PL [2]. Hence, the CL can be regarded as a generalization of the PL. On the other hand, if r = 1 and c1 is composed of all variables, the CL is obviously equivalent to the true log-likelihoodLML(θ).\nProposition 1. The CL generally is an upper bound on the true log-likelihood LML(θ).\nProof. The relation between original log-likelihood and the CL can be expressed as LML(θ) = LF(θ)+RF (θ), where the remainder term is defined as\nRF (θ) := ΛF ∑\nc∈F\n∑\nx\nQ(x) ln ∑\nxc\nP (x | θ). (4)\nSince P (x | θ) is a discrete distribution and ΛF is positive, the remainder term, RF (θ), is less than or equal to zero. Therefore, the inequality LML(θ) ≤ LF (θ) is generally satisfied for any θ and for any choice of F ."
    }, {
      "heading" : "2.1. Systematic Choice of Blocks",
      "text" : "In this section, we introduce a particular choice of the blocks in which the CL has a good property. For 1 ≤ k ≤ n, we define the family Fk whose elements are all possible blocks composed of k different variables, i.e. Fk := {{i1, i2, . . . , ik} | i1 < i2 < · · · < ik ∈ Ω}. For example, when n = 4, F2 = {{1, 2}, {1, 3}, {1, 4}, {2, 3}, {2, 4}, {3, 4}} and\nF3 = {{1, 2, 3}, {1, 2, 4}, {1, 3, 4}, {2, 3, 4}}. For the family Fk, the CL is expressed as\nLFk(θ) = ΛFk ∑\nc∈Fk\n∑\nx\nQ(x) lnP (xc | xc̄, θ), (5)\nwhere ΛFk = |Fk| −1 = k!(n − k)!/n!. It is noteworthy that LF1(θ) is reduced to the PL and LFn(θ) = LML(θ).\nProposition 2. For 1 ≤ k ≤ n, the CLs for the family Fk is bounded as LF1(θ) ≥ LF2(θ) ≥ · · · ≥ LFn(θ) = LML(θ) for any θ.\nProof. The relation between LML(θ) and LFk(θ) is LML(θ) = LFk(θ) + RFk(θ), where, from equation (4) the remainder term is RFk(θ) = ΛFk ∑\nc∈Fk\n∑\nx Q(x) ln\n∑\nxc P (x | θ). Let us con-\nsider the difference between the the remainder terms Dk(θ) := RFk+1(θ)−RFk(θ). After a short manipulation, the difference Dk(θ) yields\nDk(θ)\n= ΛFk k − n ∑\nc∈Fk\n∑\ni∈c̄\n∑\nx\nQ(x) ln\n∑\nxc P (x | θ)\n∑\nxc,xi P (x | θ)\n.\nHence, for 1 ≤ k ≤ n − 1, the inequality Dk(θ) ≥ 0 holds, because P (x | θ) is a discrete distribution. Therefore, for 1 ≤ k ≤ n− 1, the inequality\nLFk(θ) ≥ LFk+1(θ) (6)\nis satisfied. From equation (6), we reach to the proposition.\nFrom propositions 1 and 2, we found that, for 1 ≤ k ≤ n − 1, the kth-order CL, LFk(θ), is always an upper bound on the true log-likelihood and it monotonically approaches the true log-likelihood with the increase of k. Therefore, it is guaranteed that a larger k gives quantitatively better approximation of the true log-likelihood."
    }, {
      "heading" : "3. Application to Restricted Boltzmann Machines",
      "text" : "In this section, we apply the CL estimation to RBMs [4]. RBMs are Boltzmann machines consisting of visible variables, whose states can be observed, and hidden variables, whose states are not specified by observed data. RBMs are defined on (complete) bipartite graphs consisting of two layers. One of them is a layer of visible variables, termed visible layer, and the other one is a layer of hidden variables, termed hidden\nlayer. There are connections between visible variables and hidden variables, and any interlayer connections are not allowed.\nWe denote the sets of labels of visible variables and hidden variables by Ω and H , respectively, and we denote the state variable of visible variable i ∈ Ω by xi and the state variable of hidden variable j ∈ H by hj . All state variables are binary random variables that take +1 or −1. The joint distribution of RBM is represented by\nPRBM(x,h | θ) ∝ exp ( ∑\ni∈Ω\nαixi + ∑\nj∈H\nβjhj\n+ ∑\ni∈Ω\n∑\nj∈H\nwi,jxihj\n)\n. (7)\nThe parameters α = {αi | i ∈ Ω} and β = {βj | j ∈ H} are biases (or sometimes called thresholds) for visible variables and hidden variables, respectively, and the parameters w = {wi,j | i ∈ Ω, j ∈ H} are weights of connections between the visible variables and the hidden variables. In equation (7), we denote θ = α∪β∪w for a short description.\nGiven an empirical distribution Q(x) for the visible variables, the log-likelihood of RBM in equation (7) is expressed as\nLML(θ) = ∑\nx\nQ(x) lnPRBM(x | θ), (8)\nwhere PRBM(x | θ) is the marginal distribution obtained by PRBM(x | θ) = ∑ h PRBM(x,h | θ). The marginal distribution can be explicitly expressed as PRBM(x | θ) ∝ exp ( − ERBM(x | θ) )\n, where ERBM(x | θ) := − ∑ i∈Ω αixi − ∑ j∈H ln Cj(x | θ) and Cj(x | θ) := cosh ( βj + ∑ i∈Ωwi,jxi )\n. The CL estimation can be applied to the RBM. Indeed, the PL estimation for the RBM was introduced [7]. By applying equation (5) to equation (8), we can express the kthorder CL for the RBM as\nLFk(θ)\n= ΛFk ∑\nc∈Fk\n∑\ni∈c\nαi〈xi〉D + ∑\nj∈H\n〈ln Cj(x, θ)〉D\n− ΛFk ∑\nc∈Fk\n〈\nln ∑\nxc\nexp ( − E (c) RBM(x | θ) )\n〉\nD\n,\n(9)\nwhere the notation 〈· · ·〉D denotes the average over the empirical distribution and E(c)RBM(x | θ) := − ∑\ni∈c αixi − ∑ j∈H ln Cj(x | θ). The gradients, ∆ (k) θ := ∂LFk(θ)/∂θ, with respect to the parameters\nαi, βj and wi,j are\n∆(k)αi ∝ 〈xi〉D − |Fk(i)| −1\n∑\nc∈Fk(i)\n〈xi〉c, (10)\n∆ (k) βj ∝ 〈Tj(x | θ)〉D − ΛFk ∑\nc∈Fk\n〈Tj(x | θ)〉c (11)\nand\n∆(k)wi,j ∝ 〈xiTj(x | θ)〉D − ΛFk ∑\nc∈Fk\n〈xiTj(x | θ)〉c,\n(12)\nrespectively, where the notation Fk(i) is the subset of Fk whose all blocks include i, i.e. Fk(i) := {c | i ∈ c ∈ Fk}, the notation 〈· · ·〉c is defined as\n〈· · ·〉c :=\n〈\n∑\nxc (· · · ) exp\n( − E (c) RBM(x | θ) )\n∑\nxc exp\n( − E (c) RBM(x | θ) )\n〉\nD\n,\nfor the block c, and Tj(x | θ) := tanh (\nβj + ∑\ni∈Ω wi,jxi )\n. The computational cost that is required to compute all of them is O(nkM |H |). Note that, when k = n, the gradients (10)–(12) yield the gradients of the true log-likelihood."
    }, {
      "heading" : "3.1. Numerical Experiments",
      "text" : "In this section, we show results of numerical experiments using synthetic data. We use an RBM consisting of 5 visible variables and 10 hidden variables as the learning machine, and we generate M = 70 data from an RBM consisting of 5 visible variables and 17 hidden variables by using the Markov chain Monte Carlo method. In the generative RBM, we set αi = 0.1, βj = −0.1 and wi,j = 0.2 for all i and j. We compare the first-, the second- and the third-order CL estimation with the exact ML estimation. We maximize the CLs, i.e. LF1(θ), LF2(θ) and LF3(θ), and the true log-likelihood, LML(θ), by using the gradient ascent method (GAM) with the update rate of 0.1. In the four different GAMs, the same initial values of parameters that are randomly generated are used.\nFigure 1 shows that the plot of the CLs shown in equation (9) and the true log-likelihood shown in (8) against the number of iterations of GAMs with the gradients (10)–(12). In this plot, the “ML” is the true log-likelihood obtained by the exact ML estimation and “CLk” are the kth-order CLs, LFk(θ), obtained by the kth-order CL estimations. One can see that the CL approach the true log-likelihood as k increase.\nFigure 2 shows the plot of the true log-likelihoods, LML(θ), against the number of iterations of GAMs with\nthe gradients (10)–(12). In the plot, the “ML” is the true log-likelihood with the parameters calculated by the exact ML estimation and the “CLk” are the true loglikelihoods with the parameters calculated by kth-order CL estimations. One can see that higher-order CL estimations give better and faster convergence.\nAfter 50000 iterations, the average values (averaged over 30 trials) of the true log-likelihood obtained by the exact ML estimation, the first-, the second- and the third-order CL estimation are −1.741, −1.796, −1.742 and −1.741, respectively. Table 1 shows the mean absolute errors (MADs) of the estimations, α, β and w, between obtained by the exact ML estimation and by the kth-order CL estimation after 50000 iterations. Each MAD is averaged over 30 trials. One can see that higher-order CL estimations give quantitatively better estimations."
    }, {
      "heading" : "4. Conclusion",
      "text" : "In this paper, we introduced the systematic choice of blocks for the maximum CL estimation which guarantees that the kth-order CL monotonically approaches the true log-likelihood with the increase of k. This property does not depend on details of graphical models.\nFurthermore, we applied our CL method to learning of RBMs and formulate learning algorithm explicitly. In our numerical experiments for synthetic data, we made sure that the higher-order CLs have better performances. As we have seen in section 3, the computational cost increases when higher-order CLs are employed. Nonetheless, it is possible to trade off computation time for increased accuracy by switching to higherorder CLs.\nacknowledgment\nThis work was partly supported by Grants-In-Aid (No. 21700247, No. 22300078 and No. 23500075) for Scientific Research from the Ministry of Education, Culture, Sports, Science and Technology, Japan."
    } ],
    "references" : [ {
      "title" : "Learning with blocks: composite likelihood and contrastive divergence",
      "author" : [ "A. Asuncion", "Q. Liu", "A.T. Ihler", "P. Smyth" ],
      "venue" : "Proceedings of 13th International Conference on AI and Statistics (AISTAT),",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2010
    }, {
      "title" : "Statistical analysis of non-lattice data",
      "author" : [ "J. Besag" ],
      "venue" : "Journal of the Royal Statistical Society D (The Statistician),",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 1975
    }, {
      "title" : "Stochastic composite likelihood",
      "author" : [ "J. Dillon", "G. Lebanon" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2010
    }, {
      "title" : "Training products of experts by minimizing contrastive divergence",
      "author" : [ "G.E. Hinton" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2002
    }, {
      "title" : "An asymptotic analysis of generative, discriminative, and pseudolikelihood estimators",
      "author" : [ "P. Liang", "M.I. Jordan" ],
      "venue" : "Proceedings of 25th International Conference on Machine Learning,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2008
    }, {
      "title" : "Composite likelihood methods",
      "author" : [ "B.G. Lindsay" ],
      "venue" : "Comtemporary Mathematics,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 1988
    }, {
      "title" : "Inductive principles for restricted boltzmann machine learning",
      "author" : [ "B. Marlin", "K. Swersky", "B. Chen", "N. de Freitas" ],
      "venue" : "Proceedings of the 13th International Conference on Ar-  tificial Intelligence and Statistics (AISTATS),",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2010
    } ],
    "referenceMentions" : [ {
      "referenceID" : 1,
      "context" : "Maximum pseudo-likelihood (PL) estimation [2] is a statistical approximation of the ML estimation.",
      "startOffset" : 42,
      "endOffset" : 45
    }, {
      "referenceID" : 5,
      "context" : "Composite likelihoods (CLs) [6] are higher-order generalizations of the PL.",
      "startOffset" : 28,
      "endOffset" : 31
    }, {
      "referenceID" : 4,
      "context" : "Asymptotic analysis shows that maximum CL estimation is statistically more efficient than the maximum PL estimation [5].",
      "startOffset" : 116,
      "endOffset" : 119
    }, {
      "referenceID" : 1,
      "context" : "It has been known that the maximum PL estimation is asymptotically consistent [2].",
      "startOffset" : 78,
      "endOffset" : 81
    }, {
      "referenceID" : 5,
      "context" : "Like this, the maximum CL estimation is also asymptotically consistent [6].",
      "startOffset" : 71,
      "endOffset" : 74
    }, {
      "referenceID" : 4,
      "context" : "Furthermore, the maximum CL estimation has an asymptotic variance that is smaller than the maximum PL estimation but larger than ML estimation [5, 3].",
      "startOffset" : 143,
      "endOffset" : 149
    }, {
      "referenceID" : 2,
      "context" : "Furthermore, the maximum CL estimation has an asymptotic variance that is smaller than the maximum PL estimation but larger than ML estimation [5, 3].",
      "startOffset" : 143,
      "endOffset" : 149
    }, {
      "referenceID" : 0,
      "context" : "Recently, it has been found that the maximum CL estimation corresponds to a block-wise contrastive divergence learning [1].",
      "startOffset" : 119,
      "endOffset" : 122
    }, {
      "referenceID" : 0,
      "context" : "In the maximum CL estimation, one can freely choose the size of “blocks” which contain several variables, and it is widely believed that by increasing the size of blocks, one can capture more dependency relations in the model and increase the accuracy of the estimates [1].",
      "startOffset" : 269,
      "endOffset" : 272
    }, {
      "referenceID" : 3,
      "context" : "In the latter part of this paper, we apply our maximum CL estimation to restricted Boltzmann machines (RBMs) [4] and show results of numerical experiments using synthetic data.",
      "startOffset" : 109,
      "endOffset" : 112
    }, {
      "referenceID" : 5,
      "context" : "The maximum CL estimation is a statistical approximation technique of the ML estimation [6].",
      "startOffset" : 88,
      "endOffset" : 91
    }, {
      "referenceID" : 1,
      "context" : "ci = {i} and r = n, the CL is reduced to the PL [2].",
      "startOffset" : 48,
      "endOffset" : 51
    }, {
      "referenceID" : 3,
      "context" : "Application to Restricted Boltzmann Machines In this section, we apply the CL estimation to RBMs [4].",
      "startOffset" : 97,
      "endOffset" : 100
    }, {
      "referenceID" : 6,
      "context" : "Indeed, the PL estimation for the RBM was introduced [7].",
      "startOffset" : 53,
      "endOffset" : 56
    } ],
    "year" : 2014,
    "abstractText" : "Learning the parameters of graphical models using the maximum likelihood estimation is generally hard which requires an approximation. Maximum composite likelihood estimations are statistical approximations of the maximum likelihood estimation which are higher-order generalizations of the maximum pseudolikelihood estimation. In this paper, we propose a composite likelihood method and investigate its property. Furthermore, we apply our composite likelihood method to restricted Boltzmann machines.",
    "creator" : "LaTeX with hyperref package"
  }
}