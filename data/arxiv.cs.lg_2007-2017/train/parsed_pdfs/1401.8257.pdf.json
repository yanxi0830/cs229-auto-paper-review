{
  "name" : "1401.8257.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Online Clustering of Bandits",
    "authors" : [ "Claudio Gentile", "Shuai Li", "Giovanni Zappella" ],
    "emails" : [ "CLAUDIO.GENTILE@UNINSUBRIA.IT", "SHUAILI.SLI@GMAIL.COM", "ZAPPELLA@AMAZON.COM" ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "Presenting personalized content to users is nowdays a crucial functionality for many online recommendation services. Due to the ever-changing set of available options, these services have to exhibit strong adaptation capabilities when trying to match users’ preferences. Coarsely speaking, the underlying systems repeatedly learn a mapping between available content and users, the mapping being based on context information (that is, sets of features) which is typically extracted from both users and contents. The need to focus on content that raises the users’ interest, combined with the need of exploring new content so as to globally improve users’ experience, generates a wellknown exploration-exploitation dilemma, which is commonly formalized as a multi-armed bandit problem (e.g., (Lai & Robbins, 1985; Auer et al., 2001; Audibert et al., 2009; Caron et al., 2012)). In particular, the contextual bandit methods (e.g., (Auer, 2002; Langford & Zhang, 2007; Li et al., 2010; Chu et al., 2011; Bogers, 2010; AbbasiYadkori et al., 2011; Crammer & Gentile, 2011; Krause & Ong, 2011; Seldin et al., 2011; Yue et al., 2012; Djolonga\nProceedings of the 31 st International Conference on Machine Learning, Beijing, China, 2014. JMLR: W&CP volume 32. Copyright 2014 by the author(s).\net al., 2013), and references therein) have rapidly become a reference algorithmic technique for implementing adaptive recommender systems. Within the above scenarios, the widespread adoption of online social networks, where users are engaged in technology-mediated social interactions (making product endorsement and word-of-mouth advertising a common practice), raises further challenges and opportunities to content recommendation systems: On one hand, because of the mutual influence among friends, acquaintances, business partners, etc., users having strong ties are more likely to exhibit similar interests, and therefore similar behavior. On the other hand, the nature and scale of such interactions calls for adaptive algorithmic solutions which are also computationally affordable. Incorporating social components into bandit algorithms can lead to a dramatic increase in the quality of recommendations. For instance, we may want to serve content to a group of users by taking advantage of an underlying network of social relationships among them. These social relationships can either be explicitly encoded in a graph, where adjacent nodes/users are deemed similar to one another, or implicitly contained in the data, and given as the outcome of an inference process that recognizes similarities across users based on their past behavior. Examples of the first approach are the recent works (Buccapatnam et al., 2013; Delporte et al., 2013; Cesa-Bianchi et al., 2013), where a social network structure over the users is assumed to be given that reflects actual interest similarities among users – see also (Caron & Bhagat, 2013; Valko et al., 2014) for recent usage of social information to tackle the so-called “cold-start” problem. Examples of the second approach are the more traditional collaborativefiltering (e.g., (Schafer et al., 1999)), content-based filtering, and hybrid approaches (e.g. (Burke, 2005)).\nBoth approaches have important drawbacks hindering their practical deployment. One obvious drawback of the “ex-\nar X\niv :1\n40 1.\n82 57\nv3 [\ncs .L\nG ]\n6 J\nun 2\nplicit network” approach is that the social network information may be misleading (see, e.g., the experimental evidence reported by (Delporte et al., 2013)), or simply unavailable. Moreover, even in the case when this information is indeed available and useful, the algorithmic strategies to implement the needed feedback sharing mechanisms might lead to severe scaling issues (Cesa-Bianchi et al., 2013), especially when the number of targeted users is large. A standard drawback of the “implicit network” approach of traditional recommender systems is that in many practically relevant scenarios (e.g., web-based), content universe and popularity often undergo dramatic changes, making these approaches difficult to apply. In such settings, most notably in the relevant case when the involved users are many, it is often possible to identify a few subgroups or communities within which users share similar interests (Rashid et al., 2006; Buscher et al., 2012), thereby greatly facilitating the targeting of users by means of group recommendations. Hence the system need not learn a different model for each user of the service, but just a single model for each group. In this paper, we carry out1 a theoretical and experimental investigation of adaptive clustering algorithms for linear (contextual) bandits under the assumption that we have to serve content to a set of n users organized into m << n groups (or clusters) such that users within each group tend to provide similar feedback to content recommendations. We give a O( √ T ) regret analysis holding in a standard stochastically linear setting for payoffs where, importantly, the hidden constants in the big-oh depend onm, rather than n, as well as on the geometry of the user models within the different clusters. The main idea of our algorithm is to use confidence balls of the users’ models to both estimate user similarity, and to share feedback across (deemed similar) users. The algorithm adaptively interpolates between the case when we have a single instance of a contextual bandit algorithm making the same predictions for all users and the case when we have n-many instances providing fully personalized recommendations. We show that our algorithm can be implemented efficiently (the large n scenario being of special concern here) by means of off-theshelf data-structures relying on random graphs. Finally, we test our algorithm on medium-size synthetic and real-world datasets, often reporting a significant increase in prediction performance over known state-of-the-art methods for bandit problems."
    }, {
      "heading" : "2. Learning Model",
      "text" : "We assume the user behavior similarity is encoded as an unknown clustering of the users. Specifically, let V = {1, . . . , n} represent the set of n users. Then V can be par-\n1 Due to space limitations, we postpone the discussion of related work to the supplementary material.\ntitioned into a small number m of clusters V1, V2, . . . , Vm, with m << n, such that users lying in the same cluster share similar behavior and users lying in different clusters have different behavior. The actual partition of V (including the number of clusters m) and the common user behavior within each cluster are unknown to the learner, and have to be inferred on the fly. Learning proceeds in a sequential fashion: At each round t = 1, 2, . . . , the learner receives a user index it ∈ V together with a set of context vectors Cit = {xt,1,xt,2, . . . ,xt,ct} ⊆ Rd. The learner then selects some x̄t = xt,kt ∈ Cit to recommend to user it, and observes some payoff at ∈ R, which is a function of both it and the recommended x̄t. The following assumptions are made on how index it, set Cit , and payoff at are generated in round t. Index it represents the user to be served by the system, and we assume it is selected uniformly at random2 from V . Once it is selected, the number of context vectors ct in Cit is generated arbitrarily as a function of past indices i1, . . . , it−1, payoffs a1, . . . , at−1, and sets Ci1 , . . . , Cit−1 , as well as the current index it. Then the sequence xt,1,xt,2, . . . ,xt,ct of context vectors within Cit is generated i.i.d. (conditioned on it, ct and all past indices i1, . . . , it−1, payoffs a1, . . . , at−1, and setsCi1 , . . . , Cit−1 ) from a random process on the surface of the unit sphere, whose process matrix E[XX>] is full rank, with minimal eigenvalue λ > 0. Further assumptions on the process matrix E[XX>] are made later on. Finally, payoffs are generated by noisy versions of unknown linear functions of the context vectors. That is, we assume each cluster Vj , j = 1, . . . ,m, hosts an unknown parameter vector uj ∈ Rd which is common to each user i ∈ Vj . Then the payoff value ai(x) associated with user i and context vector x ∈ Rd is given by the random variable\nai(x) = u > j(i)x+ j(i)(x) ,\nwhere j(i) ∈ {1, 2, . . . ,m} is the index of the cluster that node i belongs to, and j(i)(x) is a conditionally zero-mean and bounded variance noise term. Specifically, denoting by Et[ · ] the conditional expectation E [ · ∣∣ (i1, Ci1 , a1), . . . , (it−1, Cit−1 , at−1), it ], we assume that for any fixed j ∈ {1, . . . ,m} and x ∈ Rd, the variable j(x) is such that Et[ j(x)|x ] = 0 and Vt [ j(x)|x ] ≤ σ2, where Vt[ · ] is a shorthand for the conditional variance V [ · ∣∣ (i1, Ci1 , a1), . . . , (it−1, Cit−1 , at−1), it ] of the variable at argument. So we clearly have Et[ai(x)|x ] = u>j(i)x and Vt [ ai(x)|x ] ≤ σ2. Therefore, u>j(i)x is the expected payoff observed at user i for context vector x. In the special case when the noise j(i)(x) is a bounded random variable taking values in the range [−1, 1], this implies σ2 ≤ 1. We will make throughout the assumption\n2 Any other distribution that insures a positive probability of visiting each node of V would suffice here.\nthat ai(x) ∈ [−1, 1] for all i ∈ V and x. Notice that this implies −1 ≤ u>j(i)x ≤ 1 for all i ∈ V and x. Finally, we assume well-separatedness among the clusters, in that ||uj − uj′ || ≥ γ > 0 for all j 6= j′. We define the regret rt of the learner at time t as\nrt = ( max x∈Cit u>j(it)x ) − u>j(it)x̄t .\nWe are aimed at bounding with high probability (over the variables it, xt,k, k = 1, . . . , ct, and the noise variables j(it)) the cumulative regret ∑T t=1 rt . The kind of regret bound we would like to obtain (we call it the reference bound) is one where the clustering structure of V (i.e., the partition of V into V1, . . . , Vm) is known to the algorithm ahead of time, and we simply view each one of them clusters as an independent bandit problem. In this case, a standard contextual bandit analysis (Auer, 2002; Chu et al., 2011; Abbasi-Yadkori et al., 2011) shows that, as T grows large, the cumulative regret ∑T t=1 rt can be bounded with high probability as3∑T t=1 rt = Õ (∑m j=1 ( σ d+ ||uj || √ d ) √ T ) .\nFor simplicity, we shall assume that ||uj || = 1 for all j = 1, . . . ,m. Now, a more careful analysis exploiting our assumption about the randomness of it (see the supplementary material) reveals that one can replace the √ T term contributed by each bandit j by a term of the form √ T ( 1 m + √ |Vj | n ) , so that under our assumptions the ref-\nerence bound becomes\nT∑ t=1 rt = Õ\n(( σ d+ √ d )√ T ( 1 +\nm∑ j=1 √ |Vj | n )) . (1)\nObserve the dependence of this bound on the size of clusters Vj . The worst-case scenario is when we have m clusters of the same size nm , resulting in the bound∑T\nt=1 rt = Õ (( σ d+ √ d ) √ mT ) .\nAt the other extreme lies the easy case when we have a single big cluster and many small ones. For instance, |V1| = n − m + 1, and |V2| = |V3| = . . . |Vm| = 1, for m << n, gives∑T\nt=1 rt = Õ (( σ d+ √ d ) √ T ( 1 + m√ n )) .\nA relevant geometric parameter of the set of uj is the sum of distances SD(uj) of a given vector uj w.r.t. the set of vectors u1, . . . ,um, which we define as SD(uj) =∑m `=1 ||uj − u`||. If it is known that SD(uj) is small for all j, one can modify the abovementioned independent\n3 The Õ-notation hides logarithmic factors.\nbandit algorithm, by letting the bandits share signals, as is done, e.g., in (Cesa-Bianchi et al., 2013). This allows one to exploit the vicinity of theuj vectors, and roughly replace\n1 + ∑m j=1 √ |Vj | n in (1) by a quantity also depending on the mutual distances ||uj −uj′ || among cluster vectors. However, this improvement is obtained at the cost of a substantial increase of running time (Cesa-Bianchi et al., 2013). In our analysis (Theorem 1 in Section 3), we would like to leverage both the geometry of the clusters, as encoded by vectors uj , and the relative size |Vj | of the clusters, with no prior knowledge of m (or γ), and without too much extra computational burden."
    }, {
      "heading" : "3. The Algorithm",
      "text" : "Our algorithm, called Cluster of Bandits (CLUB), is described in Figure 1. In order to describe the algorithm we find it convenient to re-parameterize the problem described in Section 2, and introduce n parameter vectors u1,u2, . . . ,un, one per node, where nodes within the same cluster Vj share the same vector. An illustrative example is given in Figure 2. The algorithm maintains at time t an estimatewi,t for vector ui associated with user i ∈ V . Vectorswi,t are updated based on the payoff signals, similar to a standard linear bandit algorithm (e.g., (Chu et al., 2011)) operating on the context vectors contained in Cit . Every user i in V hosts a linear bandit algorithm like the one described in (CesaBianchi et al., 2013). One can see that the prototype vector wi,t is the result of a standard linear least-squares approximation to the corresponding unknown parameter vector ui. In particular, wi,t−1 is defined through the inverse correlation matrix M−1i,t−1, and the additively-updated vector bi,t−1. MatricesMi,t are initialized to the d×d identity matrix, and vectors bi,t are initialized to the d-dimensional zero vector. In addition, the algorithm maintains at time t an undirected graph Gt = (V,Et) whose nodes are precisely the users in V . The algorithm starts off from the complete graph, and progressively erases edges based on the evolution of vectors wi,t. The graph is intended to encode the current partition of V by means of the connected components of Gt. We denote by V̂1,t, V̂2,t, . . . , V̂mt,t the partition of V induced by the connected components of Gt. Initially, we have m1 = 1 and V̂1,1 = V . The clusters V̂1,1, V̂2,t, . . . , V̂mt,t (henceforth called the current clusters) are indeed meant to estimate the underlying true partition V1, V2, . . . , Vm, henceforth called the underlying or true clusters. At each time t = 1, 2, . . . , the algorithm receives the index it of the user to serve, and the associated context vectors xt,1, . . . ,xt,ct (the set Cit ), and must select one among them. In doing so, the algorithm first determines which cluster (among V̂1,1, V̂2,t, . . . , V̂mt,t) node it belongs to, call this cluster V̂ĵt,t, then builds the aggregate weight vec-\ntor w̄ĵt,t−1 by taking prior x̄s, s < t, such that is ∈ V̂ĵt,t, and computing the least squares approximation as if all nodes i ∈ V̂ĵt,t have been collapsed into one. It is weight vector w̄ĵt,t−1 that the algorithm uses to select kt. In particular,\nkt = argmax k=1,...,ct\n( w̄> ĵt,t−1 xt,k + CBĵt,t−1(xt,k) ) .\nThe quantity CBĵt,t−1(x) is a version of the upper confi-\ndence bound in the approximation of w̄ĵt,t−1 to a suitable combination of vectors ui, i ∈ V̂ĵt,t – see the supplementary material for details. Once this selection is done and the associated payoff at is observed, the algorithm uses the selected vector x̄t for updating Mit,t−1 to Mit,t via a rank-one adjustment, and for turning vector bit,t−1 to bit,t via an additive update whose learning rate is precisely at. Notice that the update is only performed at node it, since for all other i 6= it we have wi,t = wi,t−1. However, this update at it will also implicitly update the aggregate weight vector w̄ĵt+1,t associated with cluster V̂ĵt+1,t+1 that node it will happen to belong to in the next round. Finally, the cluster structure is possibly modified. At this point CLUB compares, for all existing edges (it, `) ∈ Et, the distance ||wit,t−1−w`,t−1|| between vectorswit,t−1 andw`,t−1 to the quantity C̃Bit,t−1+C̃B`,t−1 . If the above distance is significantly large (and wit,t−1 and w`,t−1 are good approximations to the respective underlying vectors uit and u`), then this is a good indication that uit 6= u` (i.e., that node it and node ` cannot belong to the same true cluster), so that edge (it, `) gets deleted. The new graph Gt+1, and the induced partitioning clusters V̂1,t+1, V̂2,t+1, . . . , V̂mt+1,t+1, are then computed, and a new round begins."
    }, {
      "heading" : "3.1. Implementation",
      "text" : "In implementing the algorithm in Figure 1, the reader should bear in mind that we are expecting n (the number of users) to be quite large, d (the number of features of each item) to be relatively small, and m (the number of true clusters) to be very small compared to n. With this in mind, the algorithm can be implemented by storing a leastsquares estimator wi,t−1 at each node i ∈ V , an aggregate least squares estimator w̄ĵt,t−1 for each current cluster ĵt ∈ {1, . . . ,mt}, and an extra data-structure which is able to perform decremental dynamic connectivity. Fast implementations of such data-structures are those studied by (Thorup, 1997; Kapron et al., 2013) (see also the research thread referenced therein). One can show (see the supplementary material) that in T rounds we have an overall (expected) running time\nO ( T ( d2 +\n|E1| n\nd ) +m (nd2 + d3) + |E1|\n+ min{n2, |E1| log n}+ √ n |E1| log2.5 n ) . (2)\nNotice that the above is n · poly(log n), if so is |E1|. In addition, if T is large compared to n and d, the average running time per round becomes O(d2 + d · poly(log n)). As for memory requirements, this implementation takes O(nd2 + md2 + |E1|) = O(nd2 + |E1|). Again, this is n · poly(log n) if so is |E1|."
    }, {
      "heading" : "3.2. Regret Analysis",
      "text" : "Our analysis relies on the high probability analysis contained in (Abbasi-Yadkori et al., 2011) (Theorems 1 and 2 therein). The analysis (Theorem 1 below) is carried out in the case when the initial graph G1 is the complete graph. However, if the true clusters are sufficiently large, then we can show (see Remark 4) that a formal statement can be made even if we start off from sparser random graphs, with substantial time and memory savings. The analysis actually refers to a version of the algorithm where the confidence bound functions CBj,t−1(·) and C̃Bi,t−1 in Figure 1 are replaced by their “theoretical” counterparts TCBj,t−1(·), and T̃CBi,t−1, respectively,4 which are defined as follows. Set for brevity\nAλ(T, δ)=\n( λT\n4 −8 log (T + 3 δ ) −2 √ T log (T + 3 δ )) +\nwhere (x)+ = max{x, 0}, x ∈ R. Then, for j = 1, . . . ,mt,\nTCBj,t−1(x) = √ x>M̄−1j,t−1x ( σ √ 2 log |M̄j,t−1| δ/2 + 1 ) ,\n(3) being | · | the determinant of the matrix at argument, and, for i ∈ V ,\nT̃CBi,t−1 = σ √\n2d log t+ 2 log(2/δ) + 1√ 1 +Aλ(Ti,t−1, δ/(2nd)) . (4)\nRecall the difference between true clusters V1, . . . , Vm and current clusters V̂1,t, . . . , V̂mt,t maintained by the algorithm at time t. Consistent with this difference, we let G = (V,E) be the true underlying graph, made up of them disjoint cliques over the sets of nodes V1, . . . , Vm ⊆ V , and Gt = (V,Et) be the one kept by the algorithm – see again Figure 2 for an illustration of how the algorithm works. The following is the main theoretical result of this paper,5 where additional conditions are needed on the process X generating the context vectors. Theorem 1. Let the CLUB algorithm of Figure 1 be run on the initial complete graph G1 = (V,E1), whose nodes V = {1, . . . , n} can be partitioned into m clusters V1, . . . , Vm where, for each j = 1, . . . ,m, nodes within cluster Vj host the same vector uj , with ||uj || = 1 for j = 1, . . . ,m, and ||uj − uj′ || ≥ γ > 0 for any j 6= j′. Denote by vj = |Vj | the cardinality of cluster Vj . Let the CBj,t(·) function in Figure 1 be replaced by the TCBj,t(·) function defined in (3), and C̃Bi,t be replaced by T̃CBi,t defined in (4). In both TCBj,t and T̃CBi,t, let δ therein be\n4Notice that, in all our notations, index i always ranges over nodes, while index j always ranges over clusters. Accordingly, the quantities C̃Bi,t and T̃CBi,t are always associates with node i ∈ V , while the quantities CBj,t−1(·) and TCBj,t−1(·) are always associates with clusters j ∈ {1, . . . ,mt}.\n5 The proof is provided in the supplementary material.\nreplaced by δ/10.5. Let, at each round t, context vectors Cit = {xt,1, . . . ,xt,ct} being generated i.i.d. (conditioned on it, ct and all past indices i1, . . . , it−1, payoffs a1, . . . , at−1, and sets Ci1 , . . . , Cit−1 ) from a random process X such that ||X|| = 1, E[XX>] is full rank, with minimal eigenvalue λ > 0. Moreover, for any fixed unit vector z ∈ Rd, let the random variable (z>X)2 be (conditionally) sub-Gaussian with variance parameter ν2 = Vt [ (z>X)2 | ct ] ≤ λ 2\n8 log(4c) , with ct ≤ c for all t. Then with probability at least 1 − δ the cumulative regret satisfies T∑ t=1 rt=Õ ( (σ √ d+ 1) √ m ( n λ2 + √ T ( 1 + m∑ j=1 √ vj λn ))\n+\n( n\nλ2 + nσ2 d λγ2\n) E[SD(uit)] +m )\n=Õ ( (σ √ d+ 1) √ mT ( 1 +\nm∑ j=1 √ vj λn )) , (5)\nas T grows large. In the above, the Õ-notation hides log(1/δ), logm, log n, and log T factors.\nRemark 1. A close look at the cumulative regret bound presented in Theorem 1 reveals that this bound is made up of three main terms: The first term is of the form\n(σ √ dm+ √ m) n\nλ2 +m .\nThis term is constant with T , and essentially accounts for the transient regime due to the convergence of the minimal eigenvalues of M̄j,t andMi,t to the corresponding minimal eigenvalue λ of E[XX>]. The second term is of the form( n\nλ2 + nσ2 d λγ2\n) E[SD(uit)] .\nThis term is again constant with T , but it depends through E[SD(uit)] on the geometric properties of the set of uj as\nwell as on the way such uj interact with the cluster sizes vj . Specifically,\nE[SD(uit)] = ∑m j=1 vj n ∑m j′=1 ||uj − uj′ || .\nHence this term is small if, say, among the m clusters, a few of them together cover almost all nodes in V (this is a typical situation in practice) and, in addition, the corresponding uj are close to one another. This term accounts for the hardness of learning the true underlying clustering through edge pruning. We also have an inverse dependence on γ2, which is likely due to an artifact of our analysis. Recall that γ is not known to our algorithm. Finally, the third term is the one characterizing the asymptotic behavior of our algorithm as T → ∞, its form being just (5). It is instructive to compare this term to the reference bound (1) obtained by assuming prior knowledge of the cluster structure. Broadly speaking, (5) has an extra √ m factor,6 and replaces a factor √ d in (1) by the larger factor √ 1 λ .\nRemark 2. The reader should observe that a similar algorithm as CLUB can be designed that starts off from the empty graph instead, and progressively draws edges (thereby merging connected components and associated aggregate vectors) as soon as two nodes host individual vectors wi,t which are close enough to one another. This would have the advantage to lean on even faster data-structures for maintaining disjoint sets (e.g., (Cormen et al., 1990)[Ch. 22]), but has also the significant drawback of requiring prior knowledge of the separation parameter γ. In fact, it would not be possible to connect two previously unconnected nodes without knowing something about this parameter. A regret analysis similar to the one in Theorem 1 exists, though our current understanding is that the cumulative regret would depend linearly on √ n instead of √ m. Intuitively, this algorithm is biased towards a large number of true clusters, rather than a small number.\nRemark 3. A data-dependent variant of the CLUB algorithm can be designed and analyzed which relies on datadependent clusterability assumptions of the set of users with respect to a set of context vectors. These datadependent assumptions allow us to work in a fixed design setting for the sequence of context vectors xt,k, and remove the sub-Gaussian and full-rank hypotheses regarding E[XX>]. On the other hand, they also require that the power of the adversary generating context vectors be suitably restricted. See the supplementary material for details.\nRemark 4. Last but not least, we would like to stress that the same analysis contained in Theorem 1 extends to the case when we start off from a p-random Erdos-Renyi initial graph G1 = (V,E1), where p is the independent probabil-\n6 This extra factor could be eliminated at the cost of having a higher second term in the bound, which does not leverage the geometry of the set of uj .\nity that two nodes are connected by an edge in G1. Translated into our context, a classical result on random graphs due to (Karger, 1994) reads as follows.\nLemma 1. Given V = {1, . . . , n}, let V1, . . . , Vm be a partition of V , where |Vj | ≥ s for all j = 1, . . . ,m. Let G1 = (V,E1) be a p-random Erdos-Renyi graph with p ≥ 12 log(6n2/δ)\ns−1 . Then with probability at least 1−δ (over the random draw of edges), all m subgraphs induced by true clusters V1, . . . , Vm on G1 are connected in G1.\nFor instance, if |Vj | = β nm , j = 1, . . . ,m, for some constant β ∈ (0, 1), then it suffices to have |E1| = O ( mn log(n/δ)\nβ\n) . Under these assumptions, if the initial\ngraph G1 is such a random graph, it is easy to show that Theorem 1 still holds. As mentioned in Section 3.1 (Eq. (2) therein), the striking advantage of beginning with a sparser connected graph than the complete graph is computational, since we need not handle anymore a (possibly huge) datastructure having n2-many items. In our experiments, described next, we set p = 3 lognn , so as to be reasonably confident that G1 is (at the very least) connected."
    }, {
      "heading" : "4. Experiments",
      "text" : "We tested our algorithm on both artificial and freely available real-world datasets against standard bandit baselines."
    }, {
      "heading" : "4.1. Datasets",
      "text" : "Artificial datasets. We firstly generated synthetic datasets, so as to have a more controlled experimental setting. We tested the relative performance of the algorithms along different axes: number of underlying clusters, balancedness of cluster sizes, and amount of payoff noise. We set ct = 10 for all t = 1, . . . , T , with time horizon T = 5, 000 + 50, 000, d = 25, and n = 500. For each cluster Vj of users, we created a random unit norm vector uj ∈ Rd. All d-dimensional context vectors xt,k have then been generated uniformly at random on the surface of the Euclidean ball. The payoff value associated with cluster vector uj and context vector xt,k has been generated by perturbing the inner product u>j xt,k through an additive white noise term drawn uniformly at random across the interval [−σ, σ]. It is the value of σ that determines the amount of payoff noise. The two remaining parameters are the number of clustersm and the clusters’ relative size. We assigned to cluster Vj a number of users |Vj | calculated as7 |Vj | = n j −z∑m\n`=1 ` −z , j = 1, . . . ,m, with z ∈ {0, 1, 2, 3},\nso that z = 0 corresponds to equally-sized clusters, and z = 3 yields highly unbalanced cluster sizes. Finally, the sequence of served users it is generated uniformly at random over the n users. LastFM & Delicious datasets. These datasets are ex-\n7 We took the integer part in this formula, and reassigned the remaining fractionary parts of users to the first cluster.\ntracted from the music streaming service Last.fm and the social bookmarking web service Delicious. The LastFM dataset contains n = 1,892 nodes, and 17,632 items (artists). This dataset contains information about the listened artists, and we used this information to create payoffs: if a user listened to an artist at least once the payoff is 1, otherwise the payoff is 0. Delicious is a dataset with n = 1,861 users, and 69,226 items (URLs). The payoffs were created using the information about the bookmarked URLs for each user: the payoff is 1 if the user bookmarked the URL, otherwise the payoff is 0.8 These two datasets are inherently different: on Delicious, payoffs depend on users more strongly than on LastFM, that is, there are more popular artists whom everybody listens to than popular websites which everybody bookmarks. LastFM is a “few hits” scenario, while Delicious is a “many niches” scenario, making a big difference in recommendation practice. Preprocessing was carried out by closely following previous experimental settings, like the one in (Cesa-Bianchi et al., 2013). In particular, we only retained the first 25 principal components of the context vectors resulting from a tf-idf representation of the available items, so that on both datasets d = 25. We generated random context sets Cit of size ct = 25 for all t by selecting index it at random over the n users, then picking 24 vectors at random from the available items, and one among those with nonzero payoff for user it.9 We repeated this process T = 5, 000 + 50, 000 times for the two datasets.\nYahoo dataset. We extracted two datasets from the one adopted by the “ICML 2012 Exploration and Exploitation 3 Challenge”10 for news article recommendation. Each user is represented by a 136-dimensional binary feature vector, and we took this feature vector as a proxy for the identity of the user. We operated on the first week of data. After removing “empty” users,11 this gave rise to a dataset of 8, 362, 905 records, corresponding to n = 713, 862 distinct users. The overall number of distinct news items turned out to be 323, ct changing from round to round, with a maximum of 51, and a median of 41. The news items have no features, hence they have been represented as ddimensional versors, with d = 323. Payoff values at are either 0 or 1 depending on whether the logged web system which these data refer to has observed a positive (click) or negative (no-click) feedback from the user in round t. We then extracted the two datasets “5k users” and “18k users”\n8 Datasets and their full descriptions are available at www.grouplens.org/node/462.\n9 This is done so as to avoid a meaningless comparison: With high probability, a purely random selection would result in payoffs equal to zero for all the context vectors in Cit .\n10 https://explochallenge.inria.fr/ 11 Out of the 136 Boolean features, the first feature is always 1 throughout all records. We call “empty” the users whose only nonzero feature is the first feature.\nby filtering out users that have occurred less than 100 times and less than 50 times, respectively. Since the system’s recommendation need not coincide with the recommendation issued by the algorithms we tested, we could only retain the records on which the two recommendations were indeed the same. Because records are discarded on the fly, the actual number of retained records changes across algorithms, but it is about 50, 000 for the “5k users” version and about 70, 000 for the “18k users” version."
    }, {
      "heading" : "4.2. Algorithms",
      "text" : "We compared CLUB with two main competitors: LinUCBONE and LinUCB-IND. Both competitors are members of the LinUCB family of algorithms (Auer, 2002; Chu et al., 2011; Li et al., 2010; Abbasi-Yadkori et al., 2011; Cesa-Bianchi et al., 2013). LinUCB-ONE allocates a single instance of LinUCB across all users (thereby making the same prediction for all users), whereas LinUCBIND (“LinUCB INDependent”) allocates an independent instance of LinUCB to each user, thereby making predictions in a fully personalised fashion. Moreover, on the synthetic experiments, we added two idealized baselines: a GOBLIN-like algorithm (Cesa-Bianchi et al., 2013) fed with a Laplacian matrix encoding the true underlying graph G, and a CLAIRVOYANT algorithm that knows the true clusters a priori, and runs one instance of LinUCB per cluster. Notice that an experimental comparison to multitasklike algorithms, like GOBLIN, or to the idealized algorithm that knows all clusters beforehand, can only be done on the artificial datasets, not in the real-world case where no cluster information is available. On the Yahoo dataset, we tested the featureless version of the LinUCB-like algorithm in (Cesa-Bianchi et al., 2013), which is essentially a version of the UCB1 algorithm of (Auer et al., 2001). The corresponding ONE and IND versions are denoted by UCBONE and UCB-IND, respectively. On this dataset, we also tried a single instance of UCB-V (Audibert et al., 2009) across all users, the winner of the abovementioned ICML Challenge. Finally, all algorithms have also been compared to the trivial baseline (denoted by RAN) that picks the item within Cit fully at random. As for parameter tuning, CLUB was run with p = 3 lognn , so as to be reasonably confident that the initial graph is at least connected. In fact, after each generation of the graph, we checked for its connectedness, and repeated the process until the graph happened to be connected.12 All algorithms (but RAN) require parameter tuning: an exploration-exploitation tradeoff parameter which is common to all algorithms (in Figure 1, this is the α parameter), and the edge deletion parameter α2 in CLUB. On the synthetic datasets, as well as on the LastFM and De-\n12 Our results are averaged over 5 random initial graphs, but this randomness turned out to be a minor source of variance.\nlicious datasets, we tuned these parameters by picking the best setting (as measured by cumulative regret) after the first t0 = 5, 000 rounds, and then sticked to those values for the remaining T − t0 = 50, 000 rounds. It is these 50, 000 rounds that our plots refer to. On the Yahoo dataset, this optimal tuning was done within the first t0 = 100, 000 records, corresponding to a number of retained records between 4, 350 and 4, 450 across different algorithms."
    }, {
      "heading" : "4.3. Results",
      "text" : "Our results are summarized in13 Figures 3, 4, and 5. On the synthetic datasets (Figure 3) and the LastFM and Delicious datasets (Figure 4) we measured the ratio of the cumulative regret of the algorithm to the cumulative regret of the random predictor RAN (so that the lower the better). On the synthetic datasets, we did so under combinations of number of clusters, payoff noise, and cluster size balancedness. On the Yahoo dataset (Figure 5), because the only available payoffs are those associated with the items recommended in the logs, we instead measured the Clickthrough Rate (CTR), i.e., the fraction of times we get at = 1 out of the number of retained records so far (so the higher the better). This experimental setting is in line with previous ones (e.g., (Li et al., 2010)) and, by the way data have been prepared, gives rise to a reliable estimation of actual CTR behavior under the tested experimental conditions (Li et al., 2011). Based on the experimental results, some trends can be spotted: On the synthetic datasets, CLUB always outperforms its uninformed competitors LinUCB-IND and LinUCB-ONE, the gap getting larger as we either decrease the number of underlying clusters or we make the clusters\n13Further plots can be found in the supplementary material.\nsizes more and more unbalanced. Moreover, CLUB can clearly interpolate between these two competitors taking, in a sense, the best of both. On the other hand (and unsurprisingly), the informed competitors GOBLIN and CLEARVOYANT outperform all uninformed ones. On the “few hits” scenario of LastFM, CLUB is again outperforming both of its competitors. However, this is not happening in the “many niches” case delivered by the Delicious dataset, where CLUB is clearly outperformed by LinUCBIND. The proposed alternative of CLUB that starts from an empty graph (Remark 2) might be an effective alternative in this case. On the Yahoo datasets we extracted, CLUB tends to outperform its competitors, when measured by CTR curves, thereby showing that clustering users solely based on past behavior can be beneficial. In general, CLUB seems to benefit from situations where it is not immediately clear which is the winner between the two extreme solutions (Lin)UCB-ONE and (Lin)UCB-IND, and an adaptive interpolation between these two is needed."
    }, {
      "heading" : "Acknowledgments",
      "text" : "We would like to thank the anonymous reviewers for their helpful and constructive comments. Also, the first and the second author acknowledge the support from Amazon AWS Award in Education Machine Learning Research Grant."
    }, {
      "heading" : "A. Appendix",
      "text" : "This supplementary material contains all proofs and technical details omitted from the main text, along with ancillary comments, discussion about related work, and extra experimental results."
    }, {
      "heading" : "A.1. Proof of Theorem 1",
      "text" : "The following sequence of lemmas are of preliminary importance. The first one needs extra variance conditions on the process X generating the context vectors.\nWe find it convenient to introduce the node counterpart to TCBj,t−1(x), and the cluster counterpart to T̃CBi,t−1. Given round t, node i ∈ V , and cluster index j ∈ {1, . . . ,mt}, we let TCBi,t−1(x) = √ x>M−1i,t−1x ( σ √ 2 log |Mi,t−1| δ/2 + 1 )\nT̃CBj,t−1 = σ √\n2d log t+ 2 log(2/δ) + 1√ 1 +Aλ(T̄j,t−1, δ/(2m+1d)) ,\nbeing T̄j,t−1 = ∑ i∈V̂j,t Ti,t−1 = |{s ≤ t− 1 : is ∈ V̂j,t}| ,\ni.e., the number of past rounds where a node lying in cluster V̂j,t was served. From a notational standpoint, notice the difference14 between T̃CBi,t−1 and TCBi,t−1(x), both referring to a single node i ∈ V , and T̃CBj,t−1 and TCBj,t−1(x) which refer to an aggregation (cluster) of nodes j among the available ones at time t.\nLemma 2. Let, at each round t, context vectors Cit = {xt,1, . . . ,xt,ct} being generated i.i.d. (conditioned on it, ct and all past indices i1, . . . , it−1, rewards a1, . . . , at−1, and sets Ci1 , . . . , Cit−1 ) from a random processX such that ||X|| = 1, E[XX>] is full rank, with minimal eigenvalue λ > 0. Let also, for any fixed unit vector z ∈ Rd, the random variable (z>X)2 be (conditionally) sub-Gaussian with variance parameter15\nν2 = Vt [ (z>X)2 | ct ] ≤ λ 2\n8 log(4ct) ∀t .\nThen TCBi,t(x) ≤ T̃CBi,t\n14 Also observe that 2nd has been replaced by 2m+1d inside the log’s.\n15 Random variable (z>X)2 is conditionally subGaussian with variance parameter σ2 > 0 when Et [ exp(γ (z>X)2)| ct ] ≤ exp ( σ2 γ2/2 ) for all γ ∈ R. The sub-Gaussian assumption can be removed here at the cost of assuming the conditional variance of (z>X)2 scales with ct like λ2\nct , instead of λ\n2\nlog(ct) .\nholds with probability at least 1− δ/2, uniformly over i ∈ V , t = 0, 1, 2 . . ., and x ∈ Rd such that ||x|| = 1.\nProof. Fix node i ∈ V and round t. By the very way the algorithm in Figure 1 is defined, we have\nMi,t = I + ∑\ns≤t : is=i\nx̄sx̄ > s = I + Si,t .\nFirst, notice that by standard arguments (e.g., (Dekel et al., 2010)) we have\nlog |Mi,t| ≤ d log(1 + Ti,t/d) ≤ d log(1 + t) .\nMoreover, denoting by λmax(·) and λmin(·) the maximal and the minimal eigenvalue of the matrix at argument we have that, for any fixed unit norm x ∈ Rd,\nx>M−1i,t x ≤ λmax(M −1 i,t ) =\n1\n1 + λmin(Si,t) .\nHence, we want to show with probability at least 1−δ/(2n) that\nλmin(Si,t) ≥ λTi,t/4− 8 log ( Ti,t + 3\nδ/(2nd) ) − 2 √ Ti,t log ( Ti,t + 3\nδ/(2nd) ) (6) holds for any fixed node i. To this end, fix a unit norm vector z ∈ Rd, a round s ≤ t, and consider the variable\nVs = z > (x̄sx̄>s − Es[x̄sx̄>s | cs]) z\n= (z>x̄s) 2 − Es[(z>x̄s)2 | cs] .\nThe sequence V1, V2, . . . , VTi,t is a martingale difference sequence, with optional skipping, where Ti,t is a stopping time.16 Moreover, the following claim holds.\nClaim 1. Under the assumption of this lemma,\nEs[(z>x̄s)2 | cs] ≥ λ/4 .\nProof of claim. Let17 in round s the context vectors be Cis = {xs,1, . . . ,xs,cs}, and consider the corresponding i.i.d. random variables Zi = (z>xs,i)2 − Es[(z>xs,i)2 | cs], i = 1, . . . , cs. Since by assumption these variables are (zero-mean) sub-Gaussian, we have that (see, e.g., (Massart, 2007)[Ch.2])\nPs (Zi < −a | ct) ≤ Ps (|Zi| > a | ct) ≤ 2e−a 2/2ν2 .\n16 More precisely, we are implicitly considering the sequence ηi,1V1, ηi,2V2, . . . , ηi,tVt, where ηi,s = 1 if is = i, and 0 otherwise, with Ti,t = ∑t s=1 ηi,s.\n17 This proof is based on standard arguments, and is reported here for the sake of completeness.\nholds for any i, where Ps(·) is the shorthand for the conditional probability\nP ( · ∣∣ (i1, Ci1 , a1), . . . , (is−1, Cis−1 , as−1), is ) .\nThe above implies\nPs (\nmin i=1,...,cs\n(z>xs,i) 2 ≥ λ− a ∣∣∣ ct) ≥ ( 1− 2e−a 2/2ν2 )cs .\nTherefore\nEs[(z>x̄s)2 | cs] ≥ Es [\nmin i=1,...,cs\n(z>xs,i) 2 ∣∣∣ cs]\n≥ (λ− a) ( 1− 2e−a 2/2ν2 )cs .\nSince this holds for all a ∈ R, we set a = √ 2ν2 log(4cs)\nto get ( 1− 2e−a2/2ν2 )cs\n= (1 − 12cs ) cs ≥ 1/2 (because\ncs ≥ 1), and λ − a ≥ λ/2 (because of the assumption on ν2). Putting together concludes the proof of the claim.\nWe are now in a position to apply a Freedman-like inequality for matrix martingales due to (Oliveira, 2010; Tropp, 2011) to the (matrix) martingale difference sequence\nE1[x̄1x̄>1 | c1]− x̄1x̄>1 , E2[x̄2x̄>2 | c2]− x̄2x̄>2 , . . .\nwith optional skipping. Setting for brevity Xs = x̄sx̄>s , and\nWt = ∑\ns≤t : is=i\n( Es[X2s | cs]− E2s[Xs | cs] ) ,\nTheorem 1.2 in (Tropp, 2011) implies P ( ∃t : λmin (Si,t) ≤ Ti,tλmin(E1[X1 | c1])− a, ||Wt|| ≤ σ2\n) ≤ d e− a2/2\nσ2+2a/3 . (7)\nwhere ||Wt|| denotes the operator norm of matrix Wt.\nWe apply Claim 1, so that λmin(E1[X1 | c1]) ≥ λ/4, and proceed as in, e.g., (Cesa-Bianchi & Gentile, 2008). We set for brevity A(x, δ) = 2 log (x+1)(x+3)δ , and f(A, r) =\n2A+ √ Ar. We can write\nP ( ∃t : λmin(Si,t) ≤ λminTi,t/4− f(A(||Wt||, δ), ||Wt||) ) ≤ ∞∑ r=0 P ( ∃t : λmin(Si,t) ≤ λminTi,t/4− f(A(r, δ), r),\nb||Wt||c = r )\n≤ ∞∑ r=0 P ( ∃t : λmin (Si,t) ≤ λminTi,t/4− f(A(r, δ), r),\n||Wt|| ≤ r + 1 )\n≤ d ∞∑ r=0 e− f2(A(r,δ),r)/2 r+1+2f(A(r,δ),r)/3 ,\nthe last inequality deriving from (7). Because f(A, r) satisfies f2(A, r) ≥ Ar + A + 23f(A, r)A, we have that the exponent in the last exponential is at least A(r, δ)/2, implying\n∞∑ r=0 e−A(r,δ)/2 = ∞∑ r=0\nδ\n(r + 1)(r + 3) < δ\nwhich, in turn, yields P ( ∃t : λmin(Si,t) ≤ Ti,tλmin/4\n− f(A(||Wt||, δ/d), ||Wt||) )\n≤ δ .\nFinally, observe that ||Wt|| ≤ ∑\ns≤t : is=i\n||Es[X2s | cs]||\n= ∑\ns≤t : is=i\n||Es[Xs | cs]||\n≤ ∑\ns≤t : is=i\nEs[||Xs | cs||]\n≤ Ti,t .\nTherefore we conclude P ( ∀t : λmin(Si,t) ≥ λminTi,t/4− f(A(Ti,t, δ/d), Ti,t) ) ≥ 1− δ .\nStratifying over i ∈ V , replacing δ by δ/(2n) in the last inequality, and overapproximating proves the lemma.\nLemma 3. Under the same assumptions as in Lemma 2, we have ||ui −wi,t|| ≤ T̃CBi,t holds with probability at least 1− δ, uniformly over i ∈ V , and t = 0, 1, 2, . . ..\nProof. From (Abbasi-Yadkori et al., 2011) it follows that\n|u>i x−w>i,tx| ≤ TCBi,t(x)\nholds with probability at least 1− δ/2, uniformly over i ∈ V , t = 0, 1, 2, . . .. and x ∈ Rd. Hence,\n||ui −wi,t|| ≤ max x∈Rd : ||x||=1 |u>i x−w>i,tx|\n≤ max x∈Rd : ||x||=1 TCBi,t(x)\n≤ T̃CBi,t ,\nthe last inequality holding with probability ≥ 1 − δ/2 by Lemma 2. This concludes the proof.\nLemma 4. Under the same assumptions as in Lemma 2:\n1. If ||ui − uj || ≥ γ and T̃CBi,t + T̃CBj,t < γ/2 then\n||wi,t −wj,t|| > T̃CBi,t + T̃CBj,t\nholds with probability at least 1 − δ, uniformly over i, j ∈ V and t = 0, 1, 2, . . .;\n2. if ||wi,t −wj,t|| > T̃CBi,t + T̃CBj,t then\n||ui − uj || ≥ γ\nholds with probability at least 1 − δ, uniformly over i, j ∈ V and t = 0, 1, 2, . . ..\nProof. 1. We have\nγ ≤ ||ui − uj || = ||ui −wi,t +wi,t −wj,t +wj,t − uj || ≤ ||ui −wi,t||+ ||wi,t −wj,t||+ ||wj,t − uj || ≤ T̃CBi,t + ||wi,t −wj,t||+ T̃CBj,t\n(from Lemma 3) ≤ ||wi,t −wj,t||+ γ/2,\ni.e., ||wi,t −wj,t|| ≥ γ/2 > T̃CBi,t + T̃CBj,t .\n2. Similarly, we have\nT̃CBi,t + T̃CBj,t < ||wi,t −wj,t|| ≤ ||ui −wi,t||+ ||ui − uj ||\n+ ||wj,t − uj || ≤ T̃CBi,t + ||ui − uj ||+ T̃CBj,t ,\nimplying ||ui − uj || > 0. By the well-separatedness assumption, it must be the case that ||ui − uj || ≥ γ.\nFrom Lemma 4, it follows that if any two nodes i and j belong to different true clusters and the upper confidence bounds T̃CBi,t and T̃CBj,t are both small enough, then it is very likely that edge (i, j) will get deleted by the algorithm (Lemma 4, Item 1). Conversely, if the algorithm deletes an edge (i, j), then it is very likely that the two involved nodes i and j belong to different true clusters (Lemma 4, Item 2). Notice that, we haveE ⊆ Et with high probability for all t. Because the clusters V̂1,t, . . . , V̂mt,t are induced by the connected components of Gt = (V,Et), every true cluster Vi must be entirely included (with high probability) in some cluster V̂j,t. Said differently, for all rounds t, the partition of V produced by V1, . . . , Vm is likely to be a refinement of the one produced by V̂1,t, . . . , V̂mt,t (in passing, this also shows that, with high probability,mt ≤ m for all t). This is a key property to all our analysis. See Figure 2 in the main text for reference. Lemma 5. Under the same assumptions as in Lemma 2, if ĵt is the index of the current cluster node it belongs to, then we have TCBĵt,t−1(x) ≤ T̃CBĵt,t−1 holds with probability at least 1 − δ/2, uniformly over all rounds t = 1, 2, . . ., and x ∈ Rd such that ||x|| = 1.\nProof. The proof is the same as the one of Lemma 2, except that at the very end we need to stratify over all possible shapes for cluster V̂ĵt,t, rather than over the n nodes. Now, since with high probability (Lemma 4), V̂ĵt,t is the union of true clusters, the set of all such unions is with the same probability upper bounded by 2m.\nThe next lemma is a generalization of Theorem 1 in (Abbasi-Yadkori et al., 2011), and shows a convergence result for aggregate vector w̄j,t−1. Lemma 6. Let t be any round, and assume the partition of V produced by true clusters V1, . . . , Vm is a refinement of the one produced by the current clusters V̂1,t, . . . , V̂mt,t. Let j = ĵt be the index of the current cluster node it belongs to. Let this cluster be the union of true clusters Vj1 , Vj2 , . . . , Vjk , associated with (distinct) parameter vectors uj1 ,uj2 , . . . ,ujk , respectively. Define\nūt = M̄ −1 j,t−1  k∑ `=1 1 k I + ∑ i∈Vj` (Mi,t−1 − I) uj`  ."
    }, {
      "heading" : "Then:",
      "text" : "1. Under the same assumptions as in Lemma 2,\n||ūt − w̄j,t−1|| ≤ √ 3m T̃CBj,t−1\nholds with probability at least 1 − δ, uniformly over cluster indices j = 1, . . . ,mt, and rounds t = 1, 2, . . . .\n2. For any fixed u ∈ Rd we have\n||ūt − u|| ≤ 2 k∑ `=1 ||uj` − u|| ≤ 2SD(u) .\nProof. Let X`,t−1 be the matrix whose columns are the ddimensional vectors x̄s, for all s < t : is ∈ Vj` , a`,t−1 be the column vector collecting all payoffs as, s < t : is ∈ Vj` , and η`,t−1 be the corresponding column vector of noise values. We have\nw̄j,t−1 = M̄ −1 j,t−1b̄j,t−1 ,\nwith\nb̄j,t−1 = k∑ `=1 X`,t−1a`,t−1\n= k∑ `=1 X`,t−1 ( X>`,t−1uj` + η`,t−1 ) =\nk∑ `=1 ∑ i∈Vj` (Mi,t−1 − I)uj` +X`,t−1 η`,t−1  . Thus\nw̄j,t−1 − ūt = M̄−1j,t−1 ( k∑ `=1 ( X`,t−1 η`,t−1 − 1 k uj` ))\nand, for any fixed x ∈ Rd : ||x|| = 1, we have( w̄>j,t−1x− ū>t x )2 =\n( k∑ `=1 ( X`,t−1 η`,t−1 − 1 k uj` ))> M̄−1j,t−1x 2\n≤ x>M̄−1j,t−1x ( k∑ `=1 ( X`,t−1 η`,t−1 − 1 k uj` ))> M̄−1j,t−1\n× ( k∑ `=1 ( X`,t−1 η`,t−1 − 1 k uj` )) ≤ 2x>M̄−1j,t−1x\n× (( k∑ `=1 X`,t−1 η`,t−1 )> M̄−1j,t−1 ( k∑ `=1 X`,t−1 η`,t−1 ) + 1\nk2 ( k∑ `=1 uj` )> M̄−1j,t−1 ( k∑ `=1 uj` )) (using (a+ b)2 ≤ 2a2 + 2b2) .\nWe focus on the two terms inside the big braces. Because V̂j,t is made up of the union of true clusters, we can stratify over the set of all such unions (which are at most 2m with\nhigh probability), and then apply the martingale result in (Abbasi-Yadkori et al., 2011) (Theorem 1 therein), showing that(\nk∑ `=1 X`,t−1 η`,t−1 )> M̄−1j,t−1 ( k∑ `=1 X`,t−1 η`,t−1 )\n≤ 2σ2 (\nlog |M̄j,t−1| δ/2m+1 ) holds with probability at least 1 − δ/2. As for the second term, we simply write\n1\nk2 ( k∑ `=1 uj` )> M̄−1j,t−1 ( k∑ `=1 uj` ) ≤ 1 k2 ∣∣∣∣∣∣ k∑ `=1 uj` ∣∣∣∣∣∣2≤ 1 . Putting together and overapproximating we conclude that\n|w̄>j,t−1x− ū>t x| ≤ √ 3m TCBj,t−1(x)\nand, since this holds for all unit-norm x, Lemma 5 yields\n||w̄j,t−1 − ūt|| ≤ √ 3m T̃CBj,t−1 ,\nthereby concluding the proof of part 1.\nAs for part 2, because\nM̄j,t−1 = I + k∑ `=1 ∑ i∈Vj` (Mi,t−1 − I) ,\nwe can rewrite u as\nu = M̄−1j,t−1 u+ k∑ `=1 ∑ i∈Vj` (Mi,t−1 − I)u  , so that\nūt − u = M̄−1j,t−1\n( 1\nk k∑ `=1 (uj` − u)\n+ k∑ `=1 ∑ i∈Vj` (Mi,t−1 − I) (uj` − u)\n) .\nHence\n||ūt − u|| ≤ 1\nk ∣∣∣∣∣∣M̄−1j,t−1 k∑ `=1 (uj` − u) ∣∣∣∣∣∣\n+ k∑ `=1 ∣∣∣∣∣∣M̄−1j,t−1 ∑ i∈Vj` (Mi,t−1 − I) (uj` − u) ∣∣∣∣∣∣\n≤ 1 k k∑ `=1 ||uj` − u)||+ k∑ `=1 ||uj` − u||\n≤ 2 k∑ `=1 ||uj` − u|| ,\nas claimed.\nThe next lemma gives sufficient conditions on Ti,t (or on T̄j,t) to insure that T̃CBi,t (or T̃CBj,t) is small. We state the lemma for T̃CBi,t, but the very same statement clearly holds when we replace T̃CBi,t by T̃CBj,t, Ti,t by T̄j,t, and n by 2m.\nLemma 7. The following properties hold for upper confidence bound T̃CBi,t:\n1. T̃CBi,t is nonincreasing in Ti,t; 2. Let A = σ √ 2d log(1 + t) + 2 log(2/δ) + 1. Then\nT̃CBi,t ≤ A√\n1 + λTi,t/8\nwhen\nTi,t ≥ 2 · 322\nλ2 log\n( 2nd\nδ\n) log ( 322\nλ2 log\n( 2nd\nδ\n)) ;\n3. We have T̃CBi,t ≤ γ/4\nwhen\nTi,t ≥ 32\nλ max\n{ A2\nγ2 ,\n64\nλ log\n( 2nd\nδ\n)\n× log ( 322\nλ2 log\n( 2nd\nδ\n))} .\nProof. The proof follows from simple but annoying calculations, and is therefore omitted.\nWe are now ready to combine all previous lemmas into the proof of Theorem 1.\nProof. Let t be a generic round, ĵt be the index of the current cluster node it belongs to, and jt be the index of the true cluster it belongs to. Also, let us define the aggregate vector w̄jt,t−1 as follows :\nw̄jt,t−1 = M̄ −1 jt,t−1b̄jt,t−1, M̄jt,t−1 = I + ∑ i∈Vjt (Mi,t−1 − I),\nb̄jt,t−1 = ∑ i∈Vjt bi,t−1 .\nAssume Lemma 4 holds, implying that the current cluster V̂ĵt,t is the (disjoint) union of true clusters, and define the aggregate vector ūt accordingly, as in the statement of Lemma 6. Notice that w̄jt,t−1 is the true cluster counterpart to w̄ĵt,t−1, that is, w̄jt,t−1 = w̄ĵt,t−1 if Vjt = V̂ĵt,t.\nAlso, observe that ūt = uit when Vjt = V̂ĵt,t. Finally, set for brevity\nx∗t = argmax x∈Cit u>itx\nWe can rewrite the time-t regret rt as follows:\nrt = u > itx ∗ t − u>it x̄t\n= u>itx ∗ t − w̄>jt,t−1x ∗ t + w̄ > jt,t−1x ∗ t − w̄>ĵt,t−1x ∗ t\n+ w̄> ĵt,t−1 x∗t − w̄>jt,t−1x̄t + w̄ > jt,t−1x̄t − u > it x̄t .\nCombined with\nw̄> ĵt,t−1 x∗t +TCBĵt,t−1(x ∗ t ) ≤ w̄>ĵt,t−1x̄t+TCBĵt,t−1(x̄t),\nand rearranging gives\nrt ≤ u>itx ∗ t − w̄>jt,t−1x ∗ t − TCBĵt,t−1(x ∗ t ) (8)\n+ w̄>jt,t−1x̄t − u > it x̄t + TCBĵt,t−1(x̄t) (9) + (w̄jt,t−1 − w̄ĵt,t−1) >(x∗t − x̄t) . (10)\nWe continue by bounding with high probability the three terms (8), (9), and (10).\nAs for (8), and (9), we simply observe that Lemma 3 allows18 us to write\nu>itx ∗ t − w̄>jt,t−1x ∗ t ≤ ||uit − w̄jt,t−1|| ≤ T̃CBjt,t−1 ,\nand\nw̄>jt,t−1x̄t − u > it x̄t ≤ ||uit − w̄jt,t−1|| ≤ T̃CBjt,t−1 .\nMoreover,\nTCBĵt,t−1(x̄t) ≤ T̃CBĵt,t−1 (by Lemma 5)\n≤ T̃CBjt,t−1 (by Lemma 4 and the definition of ĵt).\nHence,\n(8) + (9) ≤ 3T̃CBjt,t−1 (11)\nholds with probability at least 1− 2δ, uniformly over t.\nAs for (10), letting {·} be the indicator function of the pred18 This lemma applies here since, by definition, w̄jt,t−1 is built only from payoffs from nodes in Vjt , sharing the common unknown vector uit .\nicate at argument, we can write\n(w̄jt,t−1 − w̄ĵt,t−1) >(x∗t − x̄t)\n= (w̄jt,t−1 − uit)>(x∗t − x̄t) + (uit − ūt)>(x∗t − x̄t) + (ūt − w̄ĵt,t−1) >(x∗t − x̄t) ≤ 2 T̃CBjt,t−1 + 2 ||uit − ūt||+ 2 √ 3m T̃CBĵt,t−1\n(using Lemma 3, ||x∗t − x̄t|| ≤ 2, and Lemma 6, part 1) = 2 T̃CBjt,t−1 + 2 {Vjt 6= V̂ĵt,t} ||uit − ūt||\n+ 2 √\n3m T̃CBĵt,t−1\n≤ 2(1 + √\n3m) T̃CBjt,t−1 + 4 {Vjt 6= V̂ĵt,t}SD(uit) (by Lemma 4, and Lemma 6, part 2) .\nPiecing together we have so far obtained\nrt ≤ (5 + 2 √ 3m) T̃CBjt,t−1\n+ 4 {Vjt 6= V̂ĵt,t}SD(uit) . (12)\nWe continue by bounding {Vjt 6= V̂ĵt,t}. From Lemma 4, we clearly have\n{Vjt 6= V̂ĵt,t} ≤ {∃i ∈ Vjt ,∃j /∈ Vjt : (i, j) ∈ Et}\n≤ { ∃i ∈ Vjt ,∃j /∈ Vjt : ∀s < t ( (is 6= i)\n∨ (is = i, ||wi,s−1 +wj,s−1|| ≤ T̃CBi,s−1 + T̃CBj,s−1) )}\n≤ {∃i ∈ Vjt : ∀s < t is 6= i} + { ∃i ∈ Vjt ,∃j /∈ Vjt :\n∀s < t ||wi,s−1 +wj,s−1|| ≤ T̃CBi,s−1 + T̃CBj,s−1 }\n≤ {∃i ∈ Vjt : ∀s < t is 6= i} + {∃i ∈ Vjt ,∃j /∈ Vjt :\n∀s < t T̃CBi,s−1 + T̃CBj,s−1 ≥ γ/2} ≤ {∃i ∈ Vjt : ∀s < t is 6= i}\n+ {∃i ∈ V : ∀s < t T̃CBi,s−1 ≥ γ/4} .\nAt this point, we apply Lemma 7 to T̃CBi,t with A2 = ( σ √ 2d log(1 + T ) + 2 log(2/δ) + 1 )2\n≤ 4σ2(d log(1 + T ) + log(2/δ)) + 2,\nand set for brevity\nB = 32\nλ max\n{ A2\nγ2 ,\n64\nλ log\n( 2nd\nδ\n)\n× log ( 322\nλ2 log\n( 2nd\nδ\n))} ,\nC = 2 · 322\nλ2 log\n( 2m+1d\nδ\n) log ( 322\nλ2 log\n( 2m+1d\nδ\n)) .\nWe can write\n{∃i ∈ V : ∀s < t T̃CBi,s−1 ≥ γ/4} ≤ {∃i ∈ V : T̃CBi,t−2 ≥ γ/4} ≤ {∃i ∈ V : Ti,t−2 ≤ B} .\nMoreover,\n{∃i ∈ Vjt : ∀s < t is 6= i} ≤ {∃i ∈ Vjt \\ {it} : Ti,t−1 = 0} ≤ {∃i ∈ V : Ti,t−1 = 0} .\nThat is,\n{Vjt 6= V̂ĵt,t} ≤ {∃i ∈ V : Ti,t−2 ≤ B} + {∃i ∈ V : Ti,t−1 = 0} .\nFurther, using again Lemma 7 (applied this time to T̃CBj,t) combined with the fact that T̃CBj,t ≤ A for all j and t, we have\nT̃CBjt,t−1 = A {T̄jt,t−1 < C}+ A√\n1 + λ T̄jt,t−1/8 ,\nwhere T̄jt,t−1 = ∑ i∈Vjt Ti,t−1 = |{s ≤ t− 1 : is ∈ Vjt}| .\nPutting together as in (12), and summing over t = 1, . . . , T , we have shown so far that with probability at least 1− 7δ/2,\nT∑ t=1 rt ≤ (5 + 2 √ 3m)A T∑ t=1 {T̄jt,t−1 < C}\n+ (5 + 2 √ 3m)A T∑ t=1 1√ 1 + λ T̄jt,t−1/8\n+ 4 T∑ t=1 SD(uit) {∃i ∈ V : Ti,t−2 ≤ B}\n+ 4 T∑ t=1 SD(uit) {∃i ∈ V : Ti,t−1 = 0} ,\nwith Ti,t = 0 if t ≤ 0.\nWe continue by upper bounding with high probability the four terms in the right-hand side of the last inequality. First, observe that for any fixed i and t, Ti,t is a binomial random variable with parameters t and 1/n, and T̄jt,t−1 =∑ i∈Vjt\nTi,t−1 which, for fixed it, is again binomial with parameters t, and vjtn , where vjt is the size of the true cluster it falls into. Moreover, for any fixed t, the variables Ti,t, i ∈ V are indepedent.\nTo bound the third term, we use a standard Bernstein inequality twice: first, we apply it to sequences of independent Bernoulli variables, whose sum Ti,t−2 has average E[Ti,t−2] = t−2n (for t ≥ 3), and then to the sequence of variables SD(uit) whose average E[SD(uit)] = 1 n ∑ i∈V SD(ui) is over the random choice of it.\nSetting for brevity\nD(B) = 2n ( B + 5\n3 log(Tn/δ)\n) + 2,\nwhere B has been defined before, we can write\nT∑ t=1 SD(uit) {∃i ∈ V : Ti,t−2 ≤ B}\n= ∑\nt≤D(B)\nSD(uit) {∃i ∈ V : Ti,t−2 ≤ B}\n+ ∑\nt>D(B)\nSD(uit) {∃i ∈ V : Ti,t−2 ≤ B}\n≤ ∑\nt≤D(B)\nSD(uit)\n+m ∑\nt>D(B)\n{∃i ∈ V : Ti,t−2 ≤ B} .\nThen from Bernstein’s inequality,\nP (∃i ∈ V ∃t > D(B) : Ti,t−2 ≤ B) ≤ δ ,\nand\nP ( ∑ t≤D(B) SD(uit) ≥ 3 2 D(B)E[SD(uit)]\n+ 5\n3 m log(1/δ)\n) ≤ δ .\nThus with probability ≥ 1− 2δ\nT∑ t=1 SD(uit) {∃i ∈ V : Ti,t−2 ≤ B}\n≤ 3 2 D(B)E[SD(uit)] + 5 3 m log(1/δ) .\nSimilarly, to bound the fourth term we have, with probability ≥ 1− 2δ,\nT∑ t=1 SD(uit) {∃i ∈ V : Ti,t−1 = 0}\n≤ 3 2 D(0)E[SD(uit)] + 5 3 m log(1/δ) .\nNext, we crudely upper bound the first term as\n(5+2 √ 3m)A T∑ t=1 {T̄jt,t−1 < C}\n≤ (5 + 2 √ 3m)A T∑ t=1 {Tit,t−1 < C} ,\nand then apply a very similar argument as before to show that with probability ≥ 1− δ,\nT∑ t=1 {Tit,t−1 < C} ≤ n ( C + 5 3 log ( T δ )) + 1 .\nFinally, we are left to bound the second term. The following is a simple property of binomial random variables we be useful.\nClaim 2. Let X be a binomial random variable with parameters n and p, and λ ∈ (0, 1) be a constant. Then\nE [\n1√ 1 + λX\n] ≤ { 3√ 1+λn p if np ≥ 10 ;\n1 if np < 10 .\nProof of claim. The second branch of the inequality is clearly trivial, so we focus on the first one under the assumption np ≥ 10. Let then β ∈ (0, 1) be a parameter that will be set later on. We have\nE [\n1√ 1 + λX\n] ≤ P(X ≤ (1− β)n p)\n+ 1√\n1 + λ (1− β)n p P(X ≥ (1− β)n p)\n≤ e−β 2 n p/2 + 1√ 1 + λ (1− β)n p ,\nthe last inequality following from the standard Chernoff bounds. Setting β = √\nlog(1+λn p) n p gives\nE [\n1√ 1 + λX\n] ≤ 1√\n1 + λn p\n+ 1√ 1 + λ (np− √ np log(1 + λnp))\n≤ 1√ 1 + λn p\n+ 1√\n1 + λn p/2\n(using np ≥ 10)\n≤ 3√ 1 + λn p ,\ni.e., the claimed inequality\nNow,\nEt−1\n[ 1√\n1 + λ T̄jt,t−1/8\n] =\nm∑ j=1 vj n 1√ 1 + λ T̄j,t−1/8 ,\nbeing T̄j,t−1 = |{s < t : is ∈ Vj}| a binomial variable with parameters t − 1 and vjn , where vj = |Vj |. By the standard Hoeffding-Azuma inequality\nT∑ t=1 1√ 1 + λ T̄jt,t−1/8 ≤ T∑ t=1 m∑ j=1 vj n 1√ 1 + λ T̄j,t−1/8\n+ √ 2T log(1/δ)\nholds with probability at least 1 − δ, In turn, from Bernstein’s inequality, we have\nP ( ∃t∃j : T̄j,t−1 ≤\nt− 1 2n vj − 5 3 log(Tm/δ)\n) ≤ δ .\nTherefore, with probability at least 1− 2δ, T∑ t=1 1√ 1 + λ T̄jt,t−1/8\n≤ T∑ t=1 m∑ j=1 vj n 1√ 1 + λ8 ( t−1 2n vj − 5 3 log(Tm/δ) ) +\n+ √ 2T log(1/δ)\n≤ m∑ j=1 vj n 4n 5 3 log(Tm/δ) + 1 + T∑ t=1 1√ 1 + λ8 t−1 4n vj  + √ 2T log(1/δ)\n= 4n 5\n3 log(Tm/δ) + 1 + m∑ j=1 vj n T∑ t=1 1√ 1 + λ8 t−1 4n vj\n+ √ 2T log(1/δ) .\nIf we set for brevity rj = λ8 vj 4n , j = 1, . . . ,m, we have T∑ t=1 1√ 1 + λ8 t−1 4n vj ≤ ∫ T 0 dx√ 1 + (x− 1)rj\n= 2\nrj\n(√ 1 + T rj − rj − √ 1− rj ) ≤ 2 √ T\nrj ,\nso that T∑ t=1 1√ 1 + λ T̄jt,t−1/8 ≤ 4n 5 3 log(Tm/δ) + 1\n+ √ 2T log(1/δ) + 8 m∑ j=1 √ 2Tvj λn .\nFinally, we put all pieces together. In order for all claims to hold simultaneously with probability at least 1 − δ, we need to replace δ throughout by δ/10.5. Then we switch to a Õ-notation, and overapproximate once more to conclude the proof.\nA.2. Implementation\nAs we said in the main text, in implementing the algorithm in Figure 1, the reader should keep in mind that it is reasonable to expect n (the number of users) to be quite large, d (the number of features of each item) to be relatively small, and m (the number of true clusters) to be very small compared to n. Then the algorithm can be implemented by storing a least-squares estimator wi,t−1 at each node i ∈ V , an aggregate least squares estimator w̄ĵt,t−1 for each current cluster ĵt ∈ {1, . . . ,mt}, and an extra data-structure which is able to perform decremental dynamic connectivity. Fast implementations of such data-structures are those studied by (Thorup, 1997; Kapron et al., 2013) (see also the research thread referenced therein). In particular, in (Thorup, 1997) (Theorem 1.1 therein) it is shown that a randomized construction exists that maintains a spanning forerst which, given an initial undirected graph G1 = (V,E1), is able to perform edge deletions and answer connectivity queries of the form “Is node i connected to node j” in expected total time O ( min{|V |2, |E1| log |V |}+ √ |V | |E1| log2.5 |V | ) for\n|E1| deletions. Connectivity queries and deletions can be interleaved, the former being performed in constant time. Notice that when we start off from the full graph, we have |E1| = O(|V |2), so that the expected amortized time per query becomes constant. On the other hand, if our initial graph has |E1| = O(|V | log |V |) edges, then the expected amortized time per query is O(log2 |V |). This becomes O(log2.5 |V |) if the initial graph has |E1| = O(|V |). In addition, we maintain an n-dimensional vector CLUSTERINDICES containing, for each node i ∈ V , the index j of the current cluster i belongs to.\nWith these data-structures handy, we can implement our algorithm as follows. After receiving it, computing jt is O(1) (just by accessing CLUSTERINDICES). Then, computing kt can be done in time O(d2) (matrix-vector multiplication, executed ct times, assuming ct is a constant). Then the algorithm directly updates bit,t−1 and b̄ĵt,t−1, as well as the inverses of matrices Mit,t−1 and M̄ĵt,t−1, which is again O(d2), using standard formulas for rankone adjustment of inverse matrices. In order to prepare the ground for the subsequent edge deletion phase, it is convenient that the algorithm also stores at each node i matrix Mi,t−1 (whose time-t update is again O(d2)).\nLet DELETE(i, `) and IS-CONNECTED(i, `) be the two op-\nerations delivered by the decremental dynamic connectivity data-structure. Edge deletion at time t corresponds to cycling through all nodes ` such that (it, `) is an existing edge. The number of such edges is on average equal to the average degree of node it, which isO ( |E1| n ) , where |E1| is the number of edges in the initial graph G1. Now, if (it, `) has to be deleted (each the deletion test being O(d)), then we invoke DELETE(it, `), and then IS-CONNECTED(it, `). If IS-CONNECTED(it, `) = “no”, this means that the current cluster V̂jt,t−1 has to split into two new clusters as a consequence of the deletion of edge (it, `). The set of nodes contained in these two clusters correspond to the two sets\n{k ∈ V : IS-CONNECTED(it, k) = “yes”}, {k ∈ V : IS-CONNECTED(`, k) = “yes”}‘,\nwhose expected amortized computation per node isO(1) to O(log2.5 n) (depending on the density of the initial graph G1). We modify the CLUSTERINDICES vector accordingly, but also the aggregate least squares estimators. This is because w̄ĵt,t−1 (represented through M̄ −1 ĵt,t\nand b̄ĵt,t) has to be spread over the two newborn clusters. This operation can be performed by adding up all matrices Mi,t and all bi,t, over all i belonging to each of the two new clusters (it is at this point that we need to access Mi,t for each i), and then inverting the resulting aggregate matrices. This operation takes O(nd2 + d3). However, as argued in the comments following Lemma 4, with high probability the number of current clusters mt can never exceed m, so that with the same probability this operation is only performed at most m times throughout the learning process. Hence in T rounds we have an overall (expected) running time\nO ( T ( d2 +\n|E1| n d\n) +m (nd2 + d3) + |E1|\n+ min{n2, |E1| log n}+ √ n |E1| log2.5 n ) .\nNotice that the above is n · poly(log n), if so is |E1|. In addition, if T is large compared to n and d, the average running time per round becomes O(d2 + d · poly(log n)).\nAs for memory requirements, we need to store two d × d matrices and one d-dimensional vector at each node, one d×d matrix and one d-dimensional vector for each current cluster, vector CLUSTERINDICES, and the data-structures allowing for fast deletion and connectivity tests. Overall, these data-structures do not require more than O(|E1|) memory to be stored, so that this implementation takes O(nd2 + md2 + |E1|) = O(nd2 + |E1|), where we again relied upon the mt ≤ m condition. Again, this is n · poly(log n) if so is |E1|."
    }, {
      "heading" : "A.3. Further Plots",
      "text" : "This section contains a more thorough set of comparative plots on the synthetic datasets described in the main text. See Figure 6 and Figure 7."
    }, {
      "heading" : "A.4. Derivation of the Reference Bounds",
      "text" : "We now provide a proof sketch of the reference bounds mentioned in Section 2 of the main text.\nLet us start off from the single user bound for LINUCB (either ONE or IND) one can extract from (Abbasi-Yadkori et al., 2011). Let uj ∈ Rd be the profile vector of this user.\nThen, with probability at least 1− δ, we have T∑ t=1 rt = O (√ T ( σ2 d log T + σ2 log 1 δ + ||ui||2 ) d log T )\n= Õ (√ T (σ2 d+ ||uj ||2) d ) = Õ ( (σ d+ √ d) √ T ) ,\nthe last line following from assuming ||uj || = 1.\nThen, a straightforward way of turning this bound into a bound for the CLEARVOYANT algorithm that knows all clusters V1, . . . , Vm ahead of time and runs one instance of LINUCB per cluster is to sum the regret contributed by each cluster throughout the T rounds. Letting Tj,T denote the set of rounds t such that it ∈ Vj , we can write\nT∑ t=1 rt = Õ (σ d+√d) m∑ j=1 √ Tj,T  . However, because it is drawn uniformly at random over V , we also have E[Tj,T ] = T |Vj |n , so that we essentially have with high probability\nT∑ t=1 rt = Õ\n(σ d+√d)√T 1 + m∑\nj=1\n√ |Vj | n  , i.e., Eq. (1) in the main text."
    }, {
      "heading" : "A.5. Further Comments",
      "text" : "As we said in Remark 3, a data-dependent variant of the CLUB algorithm can be designed and analyzed which relies on data-dependent clusterability assumptions of the set of users with respect to a set of context vectors. These data-dependent assumptions allow us to work in a fixed design setting for the sequence of context vectors xt,k, and remove the sub-Gaussian and full-rank hypotheses regarding E[XX>]. To make this more precise, consider an adversary that generates (unit norm) context vectors in a (possibly adaptive) way that for all x so generated |u>j x − u>j′x| ≥ γ , whenever j 6= j′. In words, the adversary’s power is restricted in that it cannot generate two distict context vectors x and x′ such that |u>j x− u>j′x| is small and |u>j x′−u>j′x′| is large. The two quantities must either be both zero (when j = j′) or both bounded away from 0 (when j 6= j′). Under this assumption, one can show that a modification to the TCBi,t(x) and TCBj,t(x) functions exists that makes the CLUB algorithm in Figure 1 achieve a cumulative regret bound similar to the one in\n(5), where the √\n1 λ factor therein is turned back into\n√ d, as\nin the reference bound (1), but with a worse dependence on\nthe geometry of the set of uj , as compared to E[SD(uit)]. The analysis goes along the very same lines as the one of Theorem 1."
    }, {
      "heading" : "A.6. Related Work",
      "text" : "The most closely related papers are (Djolonga et al., 2013; Azar et al., 2013; Brunskill & Li, 2013; Maillard & Mannor, 2014).\nIn (Azar et al., 2013), the authors define a transfer learning problem within a stochastic multiarmed bandit setting, where a prior distribution is defined over the set of possible models over the tasks. More similar in spirit to our paper is the recent work (Brunskill & Li, 2013) that relies on clustering Markov Decision Processes based on their model parameter similarity. A paper sharing significant similarities with ours, in terms of both setting and technical tools is the very recent paper (Maillard & Mannor, 2014) that came to our attention at the time of writing ours. In that paper, the authors analyze a noncontextual stochastic bandit problem where model parameters can indeed be clustered in a few (unknown) types, thereby requiring the algorithm to learn the clusters rather than learning the parameters in isolation. Yet, the provided algorithmic solutions are completely different from ours. Finally, in (Djolonga et al., 2013), the authors work under the assumption that users are defined using a context vector, and try to learn a low-rank subspace under the assumption that variation across users is low-rank. The paper combines low-rank matrix recovery with high-dimensional Gaussian Process Bandits, but it gives rise to algorithms which do not seem easy to use in large scale practical scenarios."
    }, {
      "heading" : "A.7. Ongoing Research",
      "text" : "This work could be extended along several directions. First, we may rely on a softer notion of clustering than the one we adopted here: a cluster is made up of nodes where the “within distance” between associated profile vectors is smaller than their “between distance”. Yet, this is likely to require prior knowledge of either the distance threshold or the number of underlying clusters, which are assumed to be unknown in this paper. Second, it might be possible to handle partially overlapping clusters. Third, CLUB can clearly be modified so as to cluster nodes through off-theshelf graph clustering techniques (mincut, spectral clustering, etc.). Clustering via connected components has the twofold advantage of being computationally faster and relatively easy to analyze. In fact, we do not know how to analyze CLUB when combined with alternative clustering techniques, and we suspect that Theorem 1 already delivers the sharpest results (as T → ∞) when clustering is indeed based on connected components only. Fourth, from a practical standpoint, it would be important to incorporate fur-\nther side information, like must-link and cannot-link constraints. Fifth, in recommender systems practice, it is often relevant to provide recommendations to new users, even in the absence of past information (the so-called “cold start” problem). In fact, there is a way of tackling this problem through the machinery we developed here (the idea is to duplicate the newcomer’s node as many times as the current clusters are, and then treat each copy as a separate user). This would potentially allow CLUB to work even in the presence of (almost) idle users. We haven’t so far collected any experimental evidence on the effectiveness of this strategy. Sixth, following the comments we made in Remark 3, we are trying to see if the i.i.d. and other statistical assumptions we made in Theorem 1 could be removed."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2014,
    "abstractText" : "We introduce a novel algorithmic approach to content recommendation based on adaptive clustering of exploration-exploitation (“bandit”) strategies. We provide a sharp regret analysis of this algorithm in a standard stochastic noise setting, demonstrate its scalability properties, and prove its effectiveness on a number of artificial and real-world datasets. Our experiments show a significant increase in prediction performance over state-of-the-art methods for bandit problems.",
    "creator" : "LaTeX with hyperref package"
  }
}