{
  "name" : "1412.7525.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ "Dong-Hyun Lee", "Saizheng Zhang", "Antoine Biard", "Yoshua Bengio" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "Back-propagation has been the workhorse of recent successes of deep learning but it relies on infinitesimal effects (partial derivatives) in order to perform credit assignment. This could become a serious issue as one considers deeper and more non-linear functions, e.g., consider the extreme case of non-linearity where the relation between parameters and cost is actually discrete. Inspired by the biological implausibility of back-propagation, a few approaches have been proposed in the past that could play a similar credit assignment role as backprop. In this spirit, we explore a novel approach to credit assignment in deep networks that we call target propagation. The main idea is to compute targets rather than gradients, at each layer. Like gradients, they are propagated backwards. In a way that is related but different from previously proposed proxies for back-propagation which rely on a backwards network with symmetric weights, target propagation relies on auto-encoders at each layer. Unlike back-propagation, it can be applied even when units exchange stochastic bits rather than real numbers. We show that a linear correction for the imperfectness of the auto-encoders is very effective to make target propagation actually work, along with adaptive learning rates."
    }, {
      "heading" : "1 INTRODUCTION",
      "text" : "Recently, deep neural networks have achieved great success in hard AI tasks (Bengio, 2009; Hinton et al., 2012; Krizhevsky et al., 2012; Sutskever et al., 2014), mostly relying on back-propagation as the main way of performing credit assignment over the different sets of parameters associated with each layer of a deep net. Back-propagation exploits the chain rule of derivatives in order to convert a loss gradient on the activations over layer l (or time t, for recurrent nets) into a loss gradient on the activations over layer l− 1 (respectively, time t− 1). However, as we consider deeper networks – e.g., consider the recent best ImageNet competition entrants (Szegedy et al., 2014) with 19 or 22 layers – longer-term dependencies, or stronger non-linearities, the composition of many non-linear operations becomes more non-linear. To make this concrete, consider the composition of many hyperbolic tangent units. In general, this means that derivatives obtained by backprop are becoming either very small (most of the time) or very large (in a few places). In the extreme (very deep computations), one would get discrete functions, whose derivatives are 0 almost everywhere, and infinite where the function changes discretely. Clearly, back-propagation would fail in that regime. In addition, from the point of view of low-energy hardware implementation, the ability to train deep networks whose units only communicate via bits would also be interesting.\nThis limitation backprop to working with precise derivatives and smooth networks is the main machine learning motivation for this paper’s exploration into an alternative principle for credit assignment in deep networks. Another motivation arises from the lack of biological plausibility of back-propagation, for the following reasons: (1) the back-propagation computation is purely linear, whereas biological neurons interleave linear and non-linear operations, (2) if the feedback paths were used to propagate credit assignment by backprop, they would need precise knowledge of the derivatives of the non-linearities at the operating point used in the corresponding feedforward computation, (3) similarly, these feedback paths would have to use exact symmetric weights (with the same connectivity, transposed) of the feedforward connections, (4) real neurons communicate by (possibly stochastic) binary values (spikes), (5) the computation would have to be precisely clocked to alternate between feedforward and back-propagation phases, and (6) it is not clear where the output targets would come from.\nar X\niv :1\n41 2.\n75 25\nv1 [\ncs .L\nG ]\n2 3\nD ec\n2 01\n4\nThe main idea of target propagation is to associate with each feedforward unit’s activation value a target value rather than a loss gradient. The target value is meant to be close to the activation value while being likely to have provided a smaller loss (if that value had been obtained in the feedforward phase). In the limit where the target is very close to the feedforward value, target propagation should behave like back-propagation. This link was nicely made in (LeCun, 1986; 1987), which introduced the idea of target propagation and connected it to back-propagation via a Lagrange multipliers formulation (where the constraints require the output of one layer to equal the input of the next layer). A similar idea was recently proposed where the constraints are relaxed into penalties, yielding a different (iterative) way to optimize deep networks (Carreira-Perpinan and Wang, 2014). Once a good target is computed, a layer-local training criterion can be defined to update each layer separately, e.g., via the delta-rule (gradient descent update with respect to the cross-entropy loss).\nBy its nature, target propagation can in principle handle stronger (and even discrete) non-linearities, and it deals with biological plausibility issues 1, 2, 3 and 4 described above. Extensions of the precise scheme proposed here could handle 5 and 6, but this is left for future work.\nIn this paper, we provide several experimental results on rather deep neural networks as well as discrete and stochastic networks. The results show that the proposed form of target propagation is comparable to back-propagation with RMSprop (Tieleman and Hinton, 2012) - a very popular setting to train deep networks nowadays."
    }, {
      "heading" : "2 PROPOSED TARGET PROPAGATION IMPLEMENTATION",
      "text" : "Although many variants of the general principle of target propagation can be devised, this paper focuses on a specific approach, described below, which fixes a problem in the formulation introduced in an earlier technical report (Bengio, 2014)."
    }, {
      "heading" : "2.1 FORMULATING TARGETS",
      "text" : "Let us consider an ordinary deep network learning process. The unknown data distribution is p(x,y), from which the training data is sampled. The network structure is defined as\nhi = fi(hi−1) = si(Wihi−1), i = 1, . . . ,M (1)\nwhere hi is the i th hidden layer, hM is the output of network, h0 is the input x, si is the nonlinearity (e.g. tanh or sigmoid) and Wi corresponds to the weights for layer i, fi is the i-th layer feed-forward mapping. For simplicity (but an abuse) of notation, the bias term of each layer is included in Wi. We define θ i,j W as the subset of network parameters θ i,j W = {Wk, k = i+ 1, . . . , j}. By this notion, each hj is a function of hi where hj = hj(hi; θ i,j W ) for 0 ≤ i < j ≤ M . We define the global loss function for one sample (x,y) as L(x,y; θ0,MW ), where\nL(x,y; θ0,MW ) = loss(hM (x; θ 0,M W ),y)\n= loss(hM (hi(x; θ 0,i W ); θ i,M W ),y), i = 1, . . . ,M − 1 (2)\nHere loss(·) can be any kind of loss measure function (e.g. MSE, Binomial cross-entropy). Then the expected loss function over the whole data distribution p(x,y) is written\nL = E p {L(x,y; θ0,MW )}. (3)\nTraining a network with back-propagation corresponds to propagating error signals through the network, signals which indicate how the unit activations or parameters of the network could be updated to decrease the expected loss L. In very deep networks with strong non-linearities, error propagation could become useless in lower layers due to the difficulties associated with strong non-linearities, e.g. exploding or vanishing gradients, as explained above. Given a data sample (x,y) and the corresponding activations of the hidden layers hi(x; θ 0,i W ), a possible solution to avoid these issues could be to assign a nearby value ĥi for each hi(x; θ 0,i W ) that could lead to a lower global loss. For a sample (x,y), we name such value ĥi a target, with the objective that\nloss(hM (ĥi; θ i,M W ),y) < loss(hM (hi(x; θ 0,i W ); θ i,M W ),y) (4)\nIn local layer i, we hope to train the network to make hi move towards ĥi. As hi approaches ĥi, if the path leading from hi to ĥi is smooth enough, we expect that the global loss L(x,y; θ 0,M W ) would then decrease. To update the Wi, instead of using the error signals propagated from global loss L(x,y; θ0,MW ) with back-propagation, we define a layer-local target loss Li. For example, using a MSE loss gives :\nLi(ĥi,hi) = ||ĥi − hi(x; θ0,iW )|| 2 2. (5)\nIn such a case, Wi is updated locally within its layer via stochastic gradient descent, where ĥi is considered as a constant with respect to Wi\nW (t+1) i = W (t) i − ηfi\n∂Li(ĥi,hi)\n∂Wi = W\n(t) i − ηfi\n∂Li(ĥi,hi)\n∂hi\n∂hi(x; θ 0,i W )\n∂Wi . (6)\nIn this context, derivatives can be used within a local layer because they typically correspond to computation performed inside each neuron. The severe non-linearity that may originate from the chain rule arises mostly when it is applied through many layers. This motivates target propagation methods to serve as alternative credit assignment in the context of a composition of many nonlinearities. What a target propagation method requires is a way to compute the target ĥi−1 from the higher-level target ĥi and from hi, such that it is likely to respect the constraint defined by Eq.4 and at least satisfies weaker assumptions, like for example :\nLi(ĥi, fi(ĥi−1)) < Li(ĥi, fi(hi−1)) (7)"
    }, {
      "heading" : "2.2 HOW TO ASSIGN A PROPER TARGET TO EACH LAYER",
      "text" : "The problem of credit assignment is the following: how should each unit change its output so as to increase the likelihood of reducing the global loss?\nWith the back-propagation algorithm, we compute the gradient of the loss with respect to the output of each layer, and we can interpret that gradient as an error signal. That error signal is propagated recursively from the top layer to the bottom layer using the chain rule.\nδhi−1 = ∂L\n∂hi−1 =\n∂L\n∂hi ∂hi ∂hi−1 = δhi ∂hi ∂hi−1\n(8)\nIn the target-prop setting, the signal that gives the direction for the update is the difference ĥ − h. So we can rewrite the first and the last terms of the previous equation and we get :\nĥi−1 − hi−1 = (ĥi − hi) ∂hi ∂hi−1 = −1 2 ∂||ĥi − hi||22 ∂hi−1\n(9)\nStill in the target-prop framework, the parameter update at a specific layer is obtain by a stochastic gradient descent (sgd) step to minimize the layer wise cost and can be written :\nW (t+1) i = W (t) i − η ∂||ĥi − hi||22 ∂Wi\n(10)\nWith back-propagation to compute the gradients at each layer, we can consider that the target of a lower layer is computed from the target of an upper layer as if gradient descent had been applied (non-parametrically) to the layer’s activations, such that Li(ĥi, fi(ĥi−1)) < Li(ĥi, fi(hi−1)). This could be called “target propagation through optimization” and reminiscent of (CarreiraPerpinan and Wang, 2014).\nHowever, in order to avoid the chain of derivatives through many layers, another option, introduced in (Bengio, 2014), is to take advantage of an “approximate inverse”. For example, suppose that we have a function gi such that fi(gi(ĥi)) ≈ ĥi, (11) then choosing ĥi−1 = gi(ĥi) would have the consequence that the level i loss Li (to make the output match the target at level i) would be minimized. This is the vanilla target propagation introduced in (Bengio, 2014):\nĥi−1 = gi(ĥi) (12)\nNote that gi does not need to invert fi everywhere, only in the vicinity of the targets. If the feedback mappings were the perfect inverses of the feed-forward mappings (gi = f−1i ), we would get directly\nLi(ĥi, fi(ĥi−1)) = Li(ĥi, fi(gi(ĥi))) = Li(ĥi, ĥi) = 0. (13)\nThis would be ideal for target propagation. In fact, we have the following proposition for the case of a perfect inverse:\nProposition 1. Assume that gi is a perfect inverse of fi, where gi = f−1i , i = 1, ...,M − 1 and fi satisfies: 1. fi is a linear mapping or, 2. hi = fi(hi−1) = Wisi(hi−1), which is another way to obtain a non-linear deep network structure (here si can be any differentiable monotonically increasing element-wise function). Consider one update for both target propagation and back-propagation, with the target propagation update (with perfect inverse) in ith layer being δW tpi , and the backpropagation update being δW bpi . Then the angle αi between δW tp i and δW bp i is bounded by\n0 ≤ αi ≤ cos−1( λmin λmax ) (14)\nHere λmax and λmin are the largest and smallest singular values of (JfM−1 . . . Jfi+1) T , where Jfk is the Jacobian matrix of fk.\nSee proof in Appendix A1. Proposition 1 says that if fi has the assumed structures, the descent direction of target propagation with perfect inverse at least partly matches with the gradient descent direction, which makes the global loss always decrease. But a perfect inverse may be impractical for computational reasons and unstable (there is no guarantee that f−1i applied to a target would yield a value that is in the domain of fi−1). So here we prefer to learn an approximate inverse gi, making the fi / gi pair look like an auto-encoder. This suggests parametrizing gi as follows:\nĥi−1 = gi(ĥi) = si(Vihi), i = 0, ...,M (15)\nwhere si is a non-linearity associated with the decoder and Vi the matrix of feedback weights for layer i. With such a parametrization, it is unlikely that the auto-encoder will achieve zero reconstruction error. The decoder could be trained via an additional auto-encoder-like loss at each layer:\nLinvi = ||fi(gi(ĥi))− ĥi||22 (16)\nThis makes fi(ĥi−1) closer to ĥi, thus making Li(ĥi, fi(ĥi−1)) closer to zero. But we should get inverse mapping around the targets. This could help to compute targets which have never been seen before. For this, we can modify inverse loss using noise injection.\nLinvi = ||fi(gi(ĥi + ))− (ĥi + )||22, ∼ N(0, σ) (17)\nHowever, the imperfection of the inverse yields severe optimization problems which has brought us to propose the following linearly corrected formula for the target propagation:\nĥi−1 − hi−1 = gi(ĥi)− gi(hi) (18)\nWe call this variant “difference target propagation” and we found in the experiments described below that it can significantly reduce the optimization problems associated with Eq. 12. Note that if gi was an inverse of fi, then difference target propagation would be equivalent to the vanilla target propagation of Eq. 12. For the “difference target propagation”, we have following proposition:\nProposition 2. During the t + 1 th update in difference target propagation, we use Linvi (ĥ (t) i +\n;Vi,W (t) i ) to update V (t+1) i and we define L̄ inv i (Vi,W (t) i ) as the expected local auto-encoder-like\nloss function over all possible ĥ(t)i + with W (t) i fixed,\nL̄invi (Vi,W (t) i ) = E\nĥ (t) i ,\n{Linvi (ĥ (t) i + ;Vi,W (t) i )} (19)\n1In the arXiv version of this paper.\nIf 1.L̄invi (Vi,W (t) i ) has only one minimum with optimal V ∗ i (W (t) i ); 2. proper learning rates for Vi and Wi are given; 3. All the Jacobian and Hessian like matrices are bounded during learning; 4. ∇ViL̄invi (Vi,W (t) i ) always points towards optimal V ∗ i (W (t) i ); 5. E{V ∗i (W (t+1) i ) − V ∗i (W (t) i ) | W (t) i } = 0. Then V (t) i − V ∗i (W (t) i ) will almost surely converge to 0 at t th update when t goes to infinity. Condition 1, 2, 4 follow the settings of stochastic gradient descent convergence similar to (Bottou, 1998).\nSee proof in Appendix2. Proposition 2 says that in difference target propagation, gi can learn a good approximation of fi’s inverse, which will quickly minimize the auto-encoder-like error of each layer.\nThe top layer does not have a layer above it and it has its own loss function which is also the global loss function. In our experiments we chose to set the first target of the target-prop chain such that L(ĥM−1) < L(hM−1). This can be achieved for classification loss as follows:\nĥM−1 = hM−1 − η0 ∂L\n∂hM−1 (20)\nwhere η0 is a “target-prop” learning rate for making the first target – i.e. one of the hyper-parameters. Making the first target at layer M −1 with the specific output and loss function instead of the output layer can reduce algorithm’s dependence on specific type of output and loss function. So we can apply consistent formulation to compute target in lower layers. And then, once we have a method to assign proper targets to each layer, we only have to optimize layer-local target losses to decrease global loss function."
    }, {
      "heading" : "2.3 THE ADVANTAGE OF DIFFERENCE TARGET PROPAGATION",
      "text" : "In order to make optimization stable in target propagation, hi−1 should approach to ĥi−1 as hi approaches to ĥi . If not, even though optimization is finished in upper layers, the weights in lower layers would continue to be updated. As a result, the target losses in upper layers as well as the global loss can increase even after we reach the optimum situation. So we found the following condition to greatly improve the stability of the optimization.\nhi = ĥi → hi−1 = ĥi−1 (21) If we have the perfect inverse gi = f−1i , it holds with vanilla target propagation because\nhi−1 = f −1 i (hi) = gi(ĥi) = ĥi−1. (22)\nAlthough it is not guaranteed with an imperfect inverse mapping gi 6= f−1i in vanilla target propagation, with difference target propagation, it naturally holds by construction.\nĥi−1 − hi−1 = gi(ĥi)− gi(hi) (23)\nMore precisely, we can show that the when the input of a layer become the target of lower layer computed by difference target propagation, the output of the layer moves toward the side of its target\nfi(ĥi−1) = fi(hi−1 + gi(ĥi)− gi(hi)) ∼ hi + f ′i(hi−1)g′i(hi)(ĥi − hi) (24)\n(ĥi − hi)T (fi(ĥi−1)− hi) ∼ (ĥi − hi)T f ′i(hi−1)g′i(hi)(ĥi − hi)) > 0 (25) if ĥi ∼ hi and f ′i(hi−1)g′i(hi) = (fi(gi(hi)))′ is positive definite. It is far more flexible condition than the perfect inverseness. Even when gi is a random mapping, this condition can be satisfied. Actually, if fi and gi are linear mappings and gi has a random matrix, difference target propagation is equivalent to feedback alignment (Lillicrap et al., 2014) which works well on many datasets. As a target framework, we also can show that the output of the layer get closer to its target\n||ĥi − fi(ĥi−1)||22 < ||ĥi − hi||22 (26) if ĥi ∼ hi and the maximum eigenvalue of (I − f ′i(hi−1)g′i(hi))T (I − f ′i(hi−1)g′i(hi)) is less than 1 because ĥi − fi(ĥi−1) ∼ [I − f ′i(hi−1)g′i(hi)](ĥi − hi) . Moreover, as gi approaches to f −1 i , this approaches to vanilla target propagation formula in (Bengio, 2014). gi(hi) ∼ hi−1 → ĥi−1 = hi−1 − gi(hi) + gi(ĥi) ∼ gi(ĥi) (27)\n2In the arXiv version of this paper."
    }, {
      "heading" : "3 EXPERIMENTS",
      "text" : ""
    }, {
      "heading" : "3.1 VERY DEEP NETWORKS",
      "text" : "As a primary objective, we investigated whether one can train ordinary deep networks on the MNIST dataset. The network has 7 hidden layers and the number of hidden units is 240. The activation function is the hyperbolic tangent (tanh). we use RMSprop as a adaptive learning rate algorithm because we do not have a global loss to optimize. Instead, we have the local layer-wise target losses that might need their learning rates to be on different scales (this is actually what we find when we do hyper-parameter optimization over the separate learning rates for each layer). To get this result, we chose the optimal hyper-parameters for the best training cost using random search. And the weights are initialized with orthogonal random matrices.\nTo improve optimization results, layers are updated one at a time from the bottom layer to the top layer, thus avoiding issues with the current input of each layer being invalid if we update all layers at once.\nAs a baseline, back-propagation with RMSprop is used. The same weight initialization and adaptive learning rate and hyper-parameter searching method are used as with target-prop. We report our results in figure 1. We got test error 1.92% in target propagation, 1.88% in back propagation. And we got negative log-likelihood 3.38× 10−6 in target propagation, 1.81× 10−5 in back propagation. These results are averaged over 5 trials using chosen hyper-parameters."
    }, {
      "heading" : "3.2 NETWORKS WITH DISCRETIZED TRANSMISSION BETWEEN UNITS",
      "text" : "As an example of extremely non-linear networks, we investigated whether one can train even discrete networks on the MNIST dataset. The network architecture is 784-500-500-10 and only the 1st hidden layer is discretized. Instead of just using the step activation function, we have normal neural layers with tanh, and signals are discretized when transporting between layer 1 and layer 2, based on biological considerations and the objective of reducing the communication cost between neurons.\nh1 = f1(x) = tanh(W1x) (28)\nh2 = f2(h1) = tanh(W2sign(h1)) (29)\np(y|x) = f3(h2) = softmax(W3h2) (30) where sign(x) = 1 if x > 0, 0 if x ≤ 0. We also use feedback mapping with inverse loss. But in this case, we cannot optimize full auto-encoding loss because it is not differentiable. Instead, we can use only reconstruction loss given the input and the output of feed-forward mapping.\ng2(h2) = tanh(V2sign(h2)) (31)\nLinv2 = ||g2(f2(h1 + ))− (h1 + )||22, ∼ N(0, σ) (32)\nIf only feed-forward mapping is discrete, we can train the network using back-propagation with biased gradient estimator as if we train continuous networks with tanh. However, if training signals also should be discrete, it is very hard to train using back-propagation. So we compare our result to two backprop baselines. One baseline is to train the discrete networks directly so we cannot trainW1 using backprop. It still can make training error be zero but we cannot learn any meaningful representation on h1, so test error is poor in Figure 3 (left). Another baseline is to train continuous-activation networks with tanh and to test with the discrete networks (that is, indirect training). Though the estimated gradient is biased so training error does not converge to zero, generalization performance is fairly good, as seen in Figure 2 (right), 3 (left).\nHowever, with target propagation, because we can learn an inverse mapping with a discrete layer and we do not use derivatives through layers, we can successfully train discrete networks directly. Though training convergence is slower, training error approaches zero, unlike the biased gradient estimator with backprop and continuous networks. The remarkable thing is that test error is comparable to biased gradient estimator with backprop and continuous networks. We can train W1 properly, that is, training signals can go across the discrete region successfully. Of course, as shown on the figure, the generalization performance is much better than the vanilla backprop baseline."
    }, {
      "heading" : "3.3 STOCHASTIC NETWORKS",
      "text" : "Another interesting learning problem which backprop cannot deal with well is stochastic networks with discrete units. Recently such networks have attracted attention (Bengio, 2013; Tang and Salakhutdinov, 2013; Bengio et al., 2013) because a stochastic network can learn a multi-modal conditional distribution P (Y |X), which is important for structured output predictions. Training networks of stochastic binary units is also motivated from biology, i.e., they resemble networks of spiking neurons. Here, we investigate whether one can train networks of stochastic binary units on MNIST for classification using target propagation. Following Raiko et al. (2014), the network architecture is 784-200-200-10 and the hidden units are stochastic binary units with the probability of turning on given by a sigmoid activation.\nhpi = σ(Wihi−1), hi = sample(h p i ) (33)\nwhere sample(p) is a binary random variable which is 1 with probability p.\nAs a baseline, we consider a biased gradient estimator in which we do back-propagation as if it were just continuous sigmoid networks. This baseline showed the best performance in Raiko et al. (2014).\nδhpi−1 = δh p i ∂hpi ∂hpi−1 ∼ σ′(Wihi−1)WTi δh p i (34)\nIn target propagation, we can train this network directly.\nĥp2 = h p 2 − η\n∂L\n∂h2 , ĥp1 = h p 1 + g2(ĥ p 2)− g2(h p 2) (35)\ngi(h p i ) = tanh(Vih p i ), L inv i = ||gi(fi(hi−1 + ))− (hi−1 + )||22, ∼ N(0, σ) (36)\nUsing layer-local target losses Li = ||ĥpi − h p i ||22, we can update all the weights.\nWe obtained a test error of 1.51% using target propagation and 1.71% using the baseline method. In the evalution, we averaged the output probabilities of an example over 100 noise samples, and then classify the example accordingly, following Raiko et al. (2014) This suggests that target propagation can directly deal with networks of binary stochastic units."
    }, {
      "heading" : "3.4 BACKPROP-FREE AUTO-ENCODER",
      "text" : "Auto-encoders are interesting building blocks for learning representations, especially deep ones (Erhan et al., 2010). In addition, as we have seen, training an auto-encoder is also part of what is\nrequired for target propagation according to the approach presented here, in order to train the feedback paths that propagate the targets. We show here how a regularized auto-encoder can be trained using difference target propagation, without backprop.\nLike in the work on denoising auto-encoders (Vincent et al., 2010) and Generative Stochastic Networks (Bengio et al., 2014), we consider the denoising auto-encoder like a stochastic network with noise injected in input and hidden units, trained to minimize a reconstruction loss.\nh = f(x) = sigm(Wx + b) (37)\nz = g(h) = sigm(WT (h + ) + c), ∼ N(0, σ) (38)\nL = ||z− x||22 + ||f(x + )− h||22, ∼ N(0, σ) (39)\nwhere we also use regularization to obtain contractive mappings. In order to train this network without backprop (that is, chain rule), we can use difference target propagation. At first, the target of z is just x, so we can train reconstruction mapping g with Lg = ||g(h) − x||22 in which h is considered as a constant. And then, we compute the target ĥ of hidden units following difference target propagation.\nĥ = h + f(ẑ)− f(z) = 2h− f(z) (40)\nwhere f is used as a inverse mapping of g without additional functions, and f(ẑ) = f(x) = h. As a target loss for the hidden layer, we can use Lf = ||f(x + ) − ĥ||22 in which regularization for contractive mapping is also incorporated and ĥ is considered as a constant. Using layer-local target losses Lf and Lg , we train on MNIST a denoising auto-encoder whose architecture is 784- 1000-784. Stroke-like filters can be obtained (See Figure 4) and after supervised fine-tuning (using backprop), we get 1.35% test error. That is, our auto-encoder can train a good initial representation as good as the one obtained by regularized auto-encoders trained by backprop on the reconstruction error."
    }, {
      "heading" : "ACKNOWLEDGMENTS",
      "text" : "We would like to thank Junyoung Chung for providing RMSprop code, Caglar Gulcehre for general discussion and feedback, Jyri Kivinen for discussion of backprop-free auto-encoder, Mathias Berglund for explanation of his stochastic networks. We thank the developers of Theano (Bergstra et al., 2010; Bastien et al., 2012), a Python library which allowed us to easily develop a fast and optimized code for GPU. We also thank the developers of Pylearn2 (Goodfellow et al., 2013), a Python library built on the top of Theano which allowed us to easily interface the data sets with our Theano code. We are also grateful for funding from NSERC, the Canada Research Chairs, Compute Canada, and CIFAR."
    }, {
      "heading" : "A PROOF OF PROPOSITION 1",
      "text" : "Proof. During one update, the training sample is (x,y), with assumed fi we have:\nhi = fi(hi−1) = Wisi(hi−1), i = 1, . . . ,M (A-1)\nHere si is identity element-wise function if fi is linear mapping. According to the loss function in section 2.1, the back-propagation update δW bpi is then\nδW bpi = −ηbp ∂L(x,y; θ0,MW )\n∂Wi\n= −ηbp( ∂hi+1 ∂hi )T . . . ( ∂hM−1 ∂hM−2 )T ∂loss(hM , y) ∂hM−1 (si(hi−1)) T\n= −ηbpJTfi+1 . . . J T fM−1\n∂loss(hM , y)\n∂hM−1 (si(hi−1))\nT (A-2)\nwhere Jfk =\n∂hk ∂hk−1 = Wi · S′i(hk−1), k = i+ 1, . . . ,M − 1 (A-3)\nS′i(hk−1) is a diagonal matrix with each diagonal element being corresponding element-wise derivatives and Jfk is the Jocobian matirx of fk(hk−1) with respect to hi−1.\nTarget propagation update is more complicated. For layer M −1, as mentioned in the end of section 2.2, target ĥM−1 of hM−1 is assigned by\nĥM−1 = hM−1 − η0 ∂loss(hM , y)\n∂hM−1 (A-4)\nIf all hks are allocated in smooth areas and η0 is sufficiently small, then the target ĥi in layer i is achieved by perfect inverse gk = f−1k , k = i+ 1, . . . ,M − 1 that\nĥi = gi+1(. . . gM−1(ĥM−1) . . . )\n= gi+1(. . . gM−1(hM−1) . . . )− η0Jgi+1 . . . JgM−1 ∂loss(hM , y)\n∂hM−1 + o(η20)\n' hi − η0J−1fi+1 . . . J −1 fM−1\n∂loss(hM , y)\n∂hM−1 (A-5)\nNow for target propagation update δW tpi we have\nδW bpi = −ηtp ∂||hi(hi−1;Wi)− ĥi||22\n∂Wi\n= −ηtp(hi − (hi − η0J−1fi+1 . . . J −1 fM−1\n∂loss(hM , y)\n∂hM−1 ))(si(hi−1))\nT\n= −η̃tpJ−1fi+1 . . . J −1 fM−1\n∂loss(hM , y)\n∂hM−1 (si(hi−1))\nT (A-6)\nhere η̃tp = ηtp · η0 and we write ∂loss(hM ,y)∂hM−1 as l and si(hi−1) as v for short. Since δW bp i and δW tp i are in matrix form, the inner production of their vector forms vec(δW bpi ) and vec(δW tp i ) is\n〈vec(δW bpi ), vec(δW tp i )〉 = tr((−ηbpJ T fi+1 . . . J T fM−1 lv T )T (−η̃tpJ−1fi+1 . . . J −1 fM−1 lvT ))\n= ηbpη̃tptr(vlTJfM−1 . . . Jfi+1J −1 fi+1 . . . J−1fM−1 lv T )\n= ηbpη̃tptr(vlT lvT ) = ηbpη̃tp||v||22 · ||l||22 (A-7)\nAlso for ||vec(δW bpi )||22 and ||vec(δW tp i )||22 we have\n||vec(δW bpi )|| 2 2 = tr((−ηbpJTfi+1 . . . J T fM−1 lv T )T (−ηbpJTfi+1 . . . J T fM−1 lv T ))\n= η2bptr(v((JfM−1 . . . Jfi+1) T l)T ((JfM−1 . . . Jfi+1) T l)vT )\n= η2bp||v||22 · ||(JfM−1 . . . Jfi+1)T l||22 ≤ η2bp||v||22 · ||(JfM−1 . . . Jfi+1)T ||22 · ||l||22 (A-8)\nand similarly,\n||vec(δW tpi )|| 2 2 ≤ η̃2tp||v||22 · ||(JfM−1 . . . Jfi+1)−1||22 · ||l||22 (A-9)\nhere ||(JfM−1 . . . Jfi+1)T ||2 and ||(JfM−1 . . . Jfi+1)−1||2 are Euclidian norms, i.e. the largest singular value of (JfM−1 . . . Jfi+1) T , λmax, and the largest singular value of (JfM−1 . . . Jfi+1) −1, 1λmin (λmin is the smallest singular value of (JfM−1 . . . Jfi+1) T , because fk is invertable, so all the smallest singular values of Jacobians are larger than 0). Finally, the angle αi between vec(δW bp i ) and vec(δW tpi ) satisfies:\ncos(αi) = 〈vec(δW bpi ), vec(δW tp i )〉\n||vec(δW bpi )||2 · ||vec(δW tp i )||2\n≥ ηbpη̃tp||v|| 2 2 · ||l||22√ η2bp||v||22 · λ2max · ||l||22 √ η̃2bp||v||22 · (\n1 λ2min ) · ||l||22\n= λmin λmax\n(A-10)\nwe have 0 ≤ αi ≤ cos−1( λminλmax ), where αi ≥ 0 is trivial."
    }, {
      "heading" : "B PROOF OF PROPOSITION 2",
      "text" : "Proof. Let us first give detail explanation for condition 2, 3, 4. For condition 2, proper learning rates ηv and ηw satisfy\n∞∑ t=1 η(t)v (η (t) w ) = +∞, ∞∑ t=1 (η(t)v ) 2((η(t)w ) 2) < +∞ (A-11)\nNote that the the beginning learning rate η(1)v (η (1) w ) can be assigned as 1n0 to be sufficiently small to satisfy locally smooth condition if needed. Condition 3 basically says that the norm of first order terms like∇ViLinvi (ĥi+ ;Vi,Wi) and∇WiLi(Wi) which are special cases of Jacobians, and eigenvalues of second order terms like ∂ 2Linvi (ĥi+ ;Vi,Wi)\n∂V 2i , ∂ 2Linvi (ĥi+ ;Vi,Wi) ∂Vi∂Wi\nare bounded. Codition 4 is equavilent to the following\n∀ε > 0, inf ||Vi−V ∗i (W (t) i )||2>ε (Vi − V ∗i (W (t) i )) T∇ViL̄invi (Vi,W (t) i ) > 0 (A-12)\nabove condition basically says that oppoite of the gradient −∇ViL̄invi (Vi,W (t) i ) always at least partly points towards its minimum with optimal V ∗i (W (t) i ).\nNote that in the following proof, all Vis and Wis are in vector form. V (t) i and W (t) i follow their update rules like\nV (t+1) i = V (t) i − η (t) v ∇ViLinvi (ĥ (t) i + ;Vi,W (t) i ) (A-13)\nW (t+1) i = W (t) i − η (t) w δW (t) i (A-14)\nδW (t) i respects to∇WiLi(Wi) in difference target propagation. We define γt that\nγt = ||V (t)i − V ∗ i (W (t) i )|| 2 2 (A-15)\nThe γt measures how far the current V (t) i is from the optimum. During the learning process, the randomness is only introduced by every update’s sample (x,y) and the used in updating Vi. We care about whether γt converges and we check the following conditional expcation\nE{γt+1 − γt | V (t)i ,W (t) i }\n= E{||V (t)i − η (t) v ∇ViLinvi (ĥ (t) i + ;Vi,W (t) i )− V ∗ i (W (t+1) i )|| 2 2\n−||V (t)i − V ∗ i (W (t) i )|| 2 2 | V (t) i ,W (t) i }\n= −2η(t)v (V (t) i − V ∗ i (W (t) i )) T E ĥ\n(t) i ,\n{∇ViLinvi (ĥ (t) i + ;Vi,W (t) i )}\n+(η(t)v ) 2 E ĥ\n(t) i ,\n{||∇ViLinvi (ĥ (t) i + ;Vi,W (t) i )|| 2 2}\n+2(V (t) i − V ∗ i (W (t) i )) TE{V ∗i (W (t) i )− V ∗ i (W (t+1) i ) |W (t) i } (A-16)\n−2η(t)v E ĥ\n(t) i ,\n{(∇ViLinvi (ĥ (t) i + ;Vi,W (t) i )) T (V ∗i (W (t) i )− V ∗ i (W (t+1) i ) |W (t) i }\n+E{||V ∗i (W (t) i )− V ∗ i (W (t+1) i )|| 2 2 |W (t) i }\n= −2η(t)v (V (t) i − V ∗ i (W (t) i )) T∇ViL̄invi (V (t) i ,W (t) i ) (A-17)\n+(η(t)v ) 2 E ĥ\n(t) i ,\n{||∇ViLinvi (ĥ (t) i + ;Vi,W (t) i )|| 2 2} (A-18)\n−2η(t)v E ĥ\n(t) i ,\n{(∇ViLinvi (ĥ (t) i + ;Vi,W (t) i )) T (V ∗i (W (t) i )− V ∗ i (W (t+1) i ) |W (t) i }(A-19)\n+E{||V ∗i (W (t) i )− V ∗ i (W (t+1) i )|| 2 2 |W (t) i } (A-20)\nWe can see that term (A-16) is cancelled by condition 5. In term (A-18), because the norm of ∇ViLinvi (ĥi + ;Vi,Wi) is bounded by some non-negative constant αv , we have\nE ĥi,\n{||∇ViLinvi (ĥi + ;Vi,W (t) i )|| 2 2} ≤ α2v (A-21)\nLet us check term (A-19) and term (A-20) where both of them have the term\n∆∗ = V ∗i (W (t) i )− V ∗ i (W (t+1) i ) (A-22)\nBecause V ∗i (W (t) i ) is the optimum minimizing L̄ inv i (Vi,W (t) i ), we have\n0 = ∇ViL̄invi (V ∗i (W (t) i ),W (t) i )\n= ∇ViL̄invi (V ∗i (W (t+1) i ),W (t+1) i )\n= ∇ViL̄invi (V ∗i (W (t) i )−∆ ∗,W (t) i − η (t) w δW (t) i ) (A-23)\nIf the learning rate for Vi and Wi is small enough with local smoothness satisfied, we can transform Eq.A-23 like\n0 = −∂ 2L̄invi (Vi,Wi)\n∂V 2i ∆∗ − η(t)w\n∂2L̄invi (Vi,Wi)\n∂Vi∂Wi δW\n(t) i + o(||∆ ∗||22) + o(||η(t)w δW (t) i || 2 2) (A-24)\nHere ||o(||∆∗||22)||2 ≤ ε∆||∆∗||2 and ||o(||η (t) w δW (t) i ||22)||2 ≤ εw||η (t) w δW (t) i ||2 for local smoothness. With the fact that all the second order terms are bounded, based on Eq.A-24 we have\n||∆∗||2 ≤ α∆η(t)w ||δW (t) i ||2 (A-25)\nHere α∆ is some non-negative constant. Further more, because the first order term of Wi like ∇WiLi(Wi) is also bounded, we have\n||δW (t)i ||2 ≤ αw (A-26)\nHere αw is some non-negative constant. Now for term (A-19) we have\n| E ĥ\n(t) i ,\n{(∇ViLinvi (ĥ (t) i + ;Vi,W (t) i )) T (V ∗i (W (t) i )− V ∗ i (W (t+1) i ) |W (t) i }| ≤ α∆αvαwη (t) w (A-27)\nand the absolute value of the entire term (A-19) is then bounded by\n2α∆αvαwη (t) w η (t) v (A-28)\nBased on Cauthy-Schwarz inequality, η(t)w η (t) v satisfies\n( N∑ t=1 η(t)w η (t) v ) 2 ≤ ( N∑ t=1 (η(t)w ) 2)( N∑ t=1 (η(t)v ) 2) (A-29)\nFrom the learning rates condition, we know that\nlim N→∞ N∑ t=1 (η(t)w ) 2((η(t)v ) 2) < +∞ (A-30)\nBecause η(t)w and η (t) v are positive, then we can easily have\nlim N→∞ N∑ t=1 η(t)w η (t) v < +∞ (A-31)\nFor term (A-20) we have\nE{||V ∗i (W (t) i )− V ∗ i (W (t+1) i )|| 2 2 |W (t) i } ≤ α 2 ∆α 2 w(η (t) w ) 2 (A-32)\nFinally, E{γt+1 − γt | V (t)i ,W (t) i } satisfies\nE{γt+1 − γt | V (t)i ,W (t) i }\n≤ −2η(t)v (V (t) i − V ∗ i (W (t) i )) T∇ViL̄invi (V (t) i ,W (t) i )\n+(η(t)v ) 2(αv) 2 + 2α∆αvαwη (t) w η (t) v + α 2 ∆α 2 w(η (t) w ) 2\n≤ (η(t)v )2(αv)2 + 2α∆αvαwη(t)w η(t)v + α2∆α2w(η(t)w )2 (A-33)\nFrom Eq.A-11 and Eq.A-31, we know that the right side of Eq.A-33 is the summand of a convergent infinite sum. Since process {γt} always larger than 0, and Eq.A-33 gives the upper bound of positive expected variation of γt, from Quasi-martingale convergence theorem in section 4.4 of (Bottou, 1998), we have that γt converges amlost surely. The almost surely convergence of γt and Eq.A-33 imply that\n∞∑ t=1 η(t)v (V (t) i − V ∗ i (W (t) i )) T∇ViL̄invi (V (t) i ,W (t) i ) ≤ +∞, a.s. (A-34)\nHere a.s. means almost surely. With the learning rate η(t)v satisfies ∞∑ t=1 η(t)v = +∞ (A-35)\nand (V (t)i − V ∗i (W (t) i )) T∇ViL̄invi (V (t) i ,W (t) i ) is always positive because of condition 4, we have\nlim t→∞\n(V (t) i − V ∗ i (W (t) i )) T∇ViL̄invi (V (t) i ,W (t) i ) = 0, a.s. (A-36)\nAssume that γt converges to some positive constant rather than 0. It implies that when t is large enough, γt = ||V (t)i − V ∗i (W (t) i )||22 > ε > 0. This is incompatible with condition 3 and Eq.A-36. Therefore γt converges to 0 almost surely and we have\nlim t→∞\nV (t) i − V ∗ i (W (t) i ) = 0, a.s. (A-37)"
    } ],
    "references" : [ {
      "title" : "Theano: new features and speed improvements",
      "author" : [ "F. Bastien", "P. Lamblin", "R. Pascanu", "J. Bergstra", "I.J. Goodfellow", "A. Bergeron", "N. Bouchard", "Y. Bengio" ],
      "venue" : "Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop.",
      "citeRegEx" : "Bastien et al\\.,? 2012",
      "shortCiteRegEx" : "Bastien et al\\.",
      "year" : 2012
    }, {
      "title" : "Learning deep architectures for AI",
      "author" : [ "Y. Bengio" ],
      "venue" : "Now Publishers.",
      "citeRegEx" : "Bengio,? 2009",
      "shortCiteRegEx" : "Bengio",
      "year" : 2009
    }, {
      "title" : "Estimating or propagating gradients through stochastic neurons",
      "author" : [ "Y. Bengio" ],
      "venue" : "Technical Report arXiv:1305.2982, Universite de Montreal.",
      "citeRegEx" : "Bengio,? 2013",
      "shortCiteRegEx" : "Bengio",
      "year" : 2013
    }, {
      "title" : "How auto-encoders could provide credit assignment in deep networks via target propagation",
      "author" : [ "Y. Bengio" ],
      "venue" : "Technical report, arXiv preprint arXiv:1407.7906.",
      "citeRegEx" : "Bengio,? 2014",
      "shortCiteRegEx" : "Bengio",
      "year" : 2014
    }, {
      "title" : "Estimating or propagating gradients through stochastic neurons for conditional computation",
      "author" : [ "Y. Bengio", "N. Léonard", "A. Courville" ],
      "venue" : "arXiv preprint arXiv:1308.3432.",
      "citeRegEx" : "Bengio et al\\.,? 2013",
      "shortCiteRegEx" : "Bengio et al\\.",
      "year" : 2013
    }, {
      "title" : "Deep generative stochastic networks trainable by backprop",
      "author" : [ "Y. Bengio", "E. Thibodeau-Laufer", "J. Yosinski" ],
      "venue" : "ICML’2014.",
      "citeRegEx" : "Bengio et al\\.,? 2014",
      "shortCiteRegEx" : "Bengio et al\\.",
      "year" : 2014
    }, {
      "title" : "Theano: a CPU and GPU math expression compiler",
      "author" : [ "J. Bergstra", "O. Breuleux", "F. Bastien", "P. Lamblin", "R. Pascanu", "G. Desjardins", "J. Turian", "D. WardeFarley", "Y. Bengio" ],
      "venue" : "Proceedings of the Python for Scientific Computing Conference (SciPy). Oral Presentation.",
      "citeRegEx" : "Bergstra et al\\.,? 2010",
      "shortCiteRegEx" : "Bergstra et al\\.",
      "year" : 2010
    }, {
      "title" : "Online algorithms and stochastic approximations",
      "author" : [ "L. Bottou" ],
      "venue" : "Saad, D., editor, Online Learning and Neural Networks. Cambridge University Press, Cambridge, UK. revised, oct 2012.",
      "citeRegEx" : "Bottou,? 1998",
      "shortCiteRegEx" : "Bottou",
      "year" : 1998
    }, {
      "title" : "Distributed optimization of deeply nested systems",
      "author" : [ "M. Carreira-Perpinan", "W. Wang" ],
      "venue" : "AISTATS’2014, JMLR W&CP, volume 33, pages 10–19.",
      "citeRegEx" : "Carreira.Perpinan and Wang,? 2014",
      "shortCiteRegEx" : "Carreira.Perpinan and Wang",
      "year" : 2014
    }, {
      "title" : "Why does unsupervised pre-training help deep learning? In JMLR W&CP: Proc",
      "author" : [ "D. Erhan", "A. Courville", "Y. Bengio", "P. Vincent" ],
      "venue" : "AISTATS’2010, volume 9, pages 201–208.",
      "citeRegEx" : "Erhan et al\\.,? 2010",
      "shortCiteRegEx" : "Erhan et al\\.",
      "year" : 2010
    }, {
      "title" : "Pylearn2: a machine learning research library",
      "author" : [ "I.J. Goodfellow", "D. Warde-Farley", "P. Lamblin", "V. Dumoulin", "M. Mirza", "R. Pascanu", "J. Bergstra", "F. Bastien", "Y. Bengio" ],
      "venue" : "arXiv preprint arXiv:1308.4214.",
      "citeRegEx" : "Goodfellow et al\\.,? 2013",
      "shortCiteRegEx" : "Goodfellow et al\\.",
      "year" : 2013
    }, {
      "title" : "Deep neural networks for acoustic modeling in speech recognition",
      "author" : [ "G. Hinton", "L. Deng", "G.E. Dahl", "A. Mohamed", "N. Jaitly", "A. Senior", "V. Vanhoucke", "P. Nguyen", "T. Sainath", "B. Kingsbury" ],
      "venue" : "IEEE Signal Processing Magazine, 29(6):82–97.",
      "citeRegEx" : "Hinton et al\\.,? 2012",
      "shortCiteRegEx" : "Hinton et al\\.",
      "year" : 2012
    }, {
      "title" : "ImageNet classification with deep convolutional neural networks",
      "author" : [ "A. Krizhevsky", "I. Sutskever", "G. Hinton" ],
      "venue" : "NIPS’2012.",
      "citeRegEx" : "Krizhevsky et al\\.,? 2012",
      "shortCiteRegEx" : "Krizhevsky et al\\.",
      "year" : 2012
    }, {
      "title" : "Learning processes in an asymmetric threshold network",
      "author" : [ "Y. LeCun" ],
      "venue" : "Fogelman-Soulié, F., Bienenstock, E., and Weisbuch, G., editors, Disordered Systems and Biological Organization, pages 233–240. Springer-Verlag, Les Houches, France.",
      "citeRegEx" : "LeCun,? 1986",
      "shortCiteRegEx" : "LeCun",
      "year" : 1986
    }, {
      "title" : "Modèles connexionistes de l’apprentissage",
      "author" : [ "Y. LeCun" ],
      "venue" : "PhD thesis, Université de Paris VI.",
      "citeRegEx" : "LeCun,? 1987",
      "shortCiteRegEx" : "LeCun",
      "year" : 1987
    }, {
      "title" : "Random feedback weights support learningin deep neural networks",
      "author" : [ "T.P. Lillicrap", "D. Cownden", "D.B. Tweed", "C.J. Akerman" ],
      "venue" : "Technical report, arXiv preprint arXiv:1411.0247.",
      "citeRegEx" : "Lillicrap et al\\.,? 2014",
      "shortCiteRegEx" : "Lillicrap et al\\.",
      "year" : 2014
    }, {
      "title" : "Techniques for learning binary stochastic feedforward neural networks",
      "author" : [ "T. Raiko", "M. Berglund", "G. Alain", "L. Dinh" ],
      "venue" : "NIPS Deep Learning Workshop 2014.",
      "citeRegEx" : "Raiko et al\\.,? 2014",
      "shortCiteRegEx" : "Raiko et al\\.",
      "year" : 2014
    }, {
      "title" : "Sequence to sequence learning with neural networks",
      "author" : [ "I. Sutskever", "O. Vinyals", "Q.V. Le" ],
      "venue" : "Technical report, arXiv preprint arXiv:1409.3215.",
      "citeRegEx" : "Sutskever et al\\.,? 2014",
      "shortCiteRegEx" : "Sutskever et al\\.",
      "year" : 2014
    }, {
      "title" : "Going deeper with convolutions",
      "author" : [ "C. Szegedy", "W. Liu", "Y. Jia", "P. Sermanet", "S. Reed", "D. Anguelov", "D. Erhan", "V. Vanhoucke", "A. Rabinovich" ],
      "venue" : "Technical report, arXiv preprint arXiv:1409.4842.",
      "citeRegEx" : "Szegedy et al\\.,? 2014",
      "shortCiteRegEx" : "Szegedy et al\\.",
      "year" : 2014
    }, {
      "title" : "A new learning algorithm for stochastic feedforward neural nets",
      "author" : [ "Y. Tang", "R. Salakhutdinov" ],
      "venue" : "ICML’2013 Workshop on Challenges in Representation Learning.",
      "citeRegEx" : "Tang and Salakhutdinov,? 2013",
      "shortCiteRegEx" : "Tang and Salakhutdinov",
      "year" : 2013
    }, {
      "title" : "Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude",
      "author" : [ "T. Tieleman", "G. Hinton" ],
      "venue" : "COURSERA: Neural Networks for Machine Learning,",
      "citeRegEx" : "Tieleman and Hinton,? \\Q2012\\E",
      "shortCiteRegEx" : "Tieleman and Hinton",
      "year" : 2012
    }, {
      "title" : "Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion",
      "author" : [ "P. Vincent", "H. Larochelle", "I. Lajoie", "Y. Bengio", "Manzagol", "P.-A." ],
      "venue" : "J. Machine Learning Res., 11.",
      "citeRegEx" : "Vincent et al\\.,? 2010",
      "shortCiteRegEx" : "Vincent et al\\.",
      "year" : 2010
    } ],
    "referenceMentions" : [ {
      "referenceID" : 1,
      "context" : "Recently, deep neural networks have achieved great success in hard AI tasks (Bengio, 2009; Hinton et al., 2012; Krizhevsky et al., 2012; Sutskever et al., 2014), mostly relying on back-propagation as the main way of performing credit assignment over the different sets of parameters associated with each layer of a deep net.",
      "startOffset" : 76,
      "endOffset" : 160
    }, {
      "referenceID" : 11,
      "context" : "Recently, deep neural networks have achieved great success in hard AI tasks (Bengio, 2009; Hinton et al., 2012; Krizhevsky et al., 2012; Sutskever et al., 2014), mostly relying on back-propagation as the main way of performing credit assignment over the different sets of parameters associated with each layer of a deep net.",
      "startOffset" : 76,
      "endOffset" : 160
    }, {
      "referenceID" : 12,
      "context" : "Recently, deep neural networks have achieved great success in hard AI tasks (Bengio, 2009; Hinton et al., 2012; Krizhevsky et al., 2012; Sutskever et al., 2014), mostly relying on back-propagation as the main way of performing credit assignment over the different sets of parameters associated with each layer of a deep net.",
      "startOffset" : 76,
      "endOffset" : 160
    }, {
      "referenceID" : 17,
      "context" : "Recently, deep neural networks have achieved great success in hard AI tasks (Bengio, 2009; Hinton et al., 2012; Krizhevsky et al., 2012; Sutskever et al., 2014), mostly relying on back-propagation as the main way of performing credit assignment over the different sets of parameters associated with each layer of a deep net.",
      "startOffset" : 76,
      "endOffset" : 160
    }, {
      "referenceID" : 18,
      "context" : ", consider the recent best ImageNet competition entrants (Szegedy et al., 2014) with 19 or 22 layers – longer-term dependencies, or stronger non-linearities, the composition of many non-linear operations becomes more non-linear.",
      "startOffset" : 57,
      "endOffset" : 79
    }, {
      "referenceID" : 13,
      "context" : "This link was nicely made in (LeCun, 1986; 1987), which introduced the idea of target propagation and connected it to back-propagation via a Lagrange multipliers formulation (where the constraints require the output of one layer to equal the input of the next layer).",
      "startOffset" : 29,
      "endOffset" : 48
    }, {
      "referenceID" : 8,
      "context" : "A similar idea was recently proposed where the constraints are relaxed into penalties, yielding a different (iterative) way to optimize deep networks (Carreira-Perpinan and Wang, 2014).",
      "startOffset" : 150,
      "endOffset" : 184
    }, {
      "referenceID" : 20,
      "context" : "The results show that the proposed form of target propagation is comparable to back-propagation with RMSprop (Tieleman and Hinton, 2012) - a very popular setting to train deep networks nowadays.",
      "startOffset" : 109,
      "endOffset" : 136
    }, {
      "referenceID" : 3,
      "context" : "Although many variants of the general principle of target propagation can be devised, this paper focuses on a specific approach, described below, which fixes a problem in the formulation introduced in an earlier technical report (Bengio, 2014).",
      "startOffset" : 229,
      "endOffset" : 243
    }, {
      "referenceID" : 3,
      "context" : "However, in order to avoid the chain of derivatives through many layers, another option, introduced in (Bengio, 2014), is to take advantage of an “approximate inverse”.",
      "startOffset" : 103,
      "endOffset" : 117
    }, {
      "referenceID" : 3,
      "context" : "This is the vanilla target propagation introduced in (Bengio, 2014): ĥi−1 = gi(ĥi) (12)",
      "startOffset" : 53,
      "endOffset" : 67
    }, {
      "referenceID" : 7,
      "context" : "Condition 1, 2, 4 follow the settings of stochastic gradient descent convergence similar to (Bottou, 1998).",
      "startOffset" : 92,
      "endOffset" : 106
    }, {
      "referenceID" : 15,
      "context" : "Actually, if fi and gi are linear mappings and gi has a random matrix, difference target propagation is equivalent to feedback alignment (Lillicrap et al., 2014) which works well on many datasets.",
      "startOffset" : 137,
      "endOffset" : 161
    }, {
      "referenceID" : 3,
      "context" : "Moreover, as gi approaches to f −1 i , this approaches to vanilla target propagation formula in (Bengio, 2014).",
      "startOffset" : 96,
      "endOffset" : 110
    }, {
      "referenceID" : 2,
      "context" : "Recently such networks have attracted attention (Bengio, 2013; Tang and Salakhutdinov, 2013; Bengio et al., 2013) because a stochastic network can learn a multi-modal conditional distribution P (Y |X), which is important for structured output predictions.",
      "startOffset" : 48,
      "endOffset" : 113
    }, {
      "referenceID" : 19,
      "context" : "Recently such networks have attracted attention (Bengio, 2013; Tang and Salakhutdinov, 2013; Bengio et al., 2013) because a stochastic network can learn a multi-modal conditional distribution P (Y |X), which is important for structured output predictions.",
      "startOffset" : 48,
      "endOffset" : 113
    }, {
      "referenceID" : 4,
      "context" : "Recently such networks have attracted attention (Bengio, 2013; Tang and Salakhutdinov, 2013; Bengio et al., 2013) because a stochastic network can learn a multi-modal conditional distribution P (Y |X), which is important for structured output predictions.",
      "startOffset" : 48,
      "endOffset" : 113
    }, {
      "referenceID" : 1,
      "context" : "Recently such networks have attracted attention (Bengio, 2013; Tang and Salakhutdinov, 2013; Bengio et al., 2013) because a stochastic network can learn a multi-modal conditional distribution P (Y |X), which is important for structured output predictions. Training networks of stochastic binary units is also motivated from biology, i.e., they resemble networks of spiking neurons. Here, we investigate whether one can train networks of stochastic binary units on MNIST for classification using target propagation. Following Raiko et al. (2014), the network architecture is 784-200-200-10 and the hidden units are stochastic binary units with the probability of turning on given by a sigmoid activation.",
      "startOffset" : 49,
      "endOffset" : 545
    }, {
      "referenceID" : 1,
      "context" : "Recently such networks have attracted attention (Bengio, 2013; Tang and Salakhutdinov, 2013; Bengio et al., 2013) because a stochastic network can learn a multi-modal conditional distribution P (Y |X), which is important for structured output predictions. Training networks of stochastic binary units is also motivated from biology, i.e., they resemble networks of spiking neurons. Here, we investigate whether one can train networks of stochastic binary units on MNIST for classification using target propagation. Following Raiko et al. (2014), the network architecture is 784-200-200-10 and the hidden units are stochastic binary units with the probability of turning on given by a sigmoid activation. hpi = σ(Wihi−1), hi = sample(h p i ) (33) where sample(p) is a binary random variable which is 1 with probability p. As a baseline, we consider a biased gradient estimator in which we do back-propagation as if it were just continuous sigmoid networks. This baseline showed the best performance in Raiko et al. (2014). δhpi−1 = δh p i ∂hpi ∂hpi−1 ∼ σ(Wihi−1)W i δh p i (34)",
      "startOffset" : 49,
      "endOffset" : 1021
    }, {
      "referenceID" : 16,
      "context" : "In the evalution, we averaged the output probabilities of an example over 100 noise samples, and then classify the example accordingly, following Raiko et al. (2014) This suggests that target propagation can directly deal with networks of binary stochastic units.",
      "startOffset" : 146,
      "endOffset" : 166
    }, {
      "referenceID" : 16,
      "context" : "The second row shows the results from (Raiko et al., 2014).",
      "startOffset" : 38,
      "endOffset" : 58
    }, {
      "referenceID" : 16,
      "context" : "In our experiment, we used RMS-prop and maximum epochs is 1000 different from (Raiko et al., 2014).",
      "startOffset" : 78,
      "endOffset" : 98
    }, {
      "referenceID" : 9,
      "context" : "Auto-encoders are interesting building blocks for learning representations, especially deep ones (Erhan et al., 2010).",
      "startOffset" : 97,
      "endOffset" : 117
    }, {
      "referenceID" : 21,
      "context" : "Like in the work on denoising auto-encoders (Vincent et al., 2010) and Generative Stochastic Networks (Bengio et al.",
      "startOffset" : 44,
      "endOffset" : 66
    }, {
      "referenceID" : 5,
      "context" : ", 2010) and Generative Stochastic Networks (Bengio et al., 2014), we consider the denoising auto-encoder like a stochastic network with noise injected in input and hidden units, trained to minimize a reconstruction loss.",
      "startOffset" : 43,
      "endOffset" : 64
    } ],
    "year" : 2017,
    "abstractText" : "Back-propagation has been the workhorse of recent successes of deep learning but it relies on infinitesimal effects (partial derivatives) in order to perform credit assignment. This could become a serious issue as one considers deeper and more non-linear functions, e.g., consider the extreme case of non-linearity where the relation between parameters and cost is actually discrete. Inspired by the biological implausibility of back-propagation, a few approaches have been proposed in the past that could play a similar credit assignment role as backprop. In this spirit, we explore a novel approach to credit assignment in deep networks that we call target propagation. The main idea is to compute targets rather than gradients, at each layer. Like gradients, they are propagated backwards. In a way that is related but different from previously proposed proxies for back-propagation which rely on a backwards network with symmetric weights, target propagation relies on auto-encoders at each layer. Unlike back-propagation, it can be applied even when units exchange stochastic bits rather than real numbers. We show that a linear correction for the imperfectness of the auto-encoders is very effective to make target propagation actually work, along with adaptive learning rates.",
    "creator" : "LaTeX with hyperref package"
  }
}