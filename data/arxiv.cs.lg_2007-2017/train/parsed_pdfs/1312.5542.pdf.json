{
  "name" : "1312.5542.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Word Embeddings through Hellinger PCA",
    "authors" : [ "Rémi Lebret", "Ronan Collobert" ],
    "emails" : [ "rlebret@idiap.ch", "ronan@collobert.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Word embeddings resulting from neural language models have been shown to be successful for a large variety of NLP tasks. However, such architecture might be difficult to train and time-consuming. Instead, we propose to drastically simplify the word embeddings computation through a Hellinger PCA of the word co-occurence matrix. We compare those new word embeddings with some wellknown embeddings on NER and movie review tasks and show that we can reach similar or even better performance. Although deep learning is not really necessary for generating good word embeddings, we show that it can provide an easy way to adapt embeddings to specific tasks."
    }, {
      "heading" : "1 Introduction",
      "text" : "Building word embeddings have always generated much interest for linguists. Popular approaches such as Brown clustering algorithm [1] have been used with success in a wide variety of NLP tasks [2, 3, 4]. Those word embeddings are often seen as a low dimensional-vector space where the dimensions can be seen as features potentially describing syntactic or semantic properties. Recently, distributed approaches based on neural network language models (NNLM) have revived the field of learning word embeddings [5, 6, 7, 8, 9]. However, a neural network architecture can be hard to train. Finding the right parameters to tune the model is often a challenging task and the training phase is in general computationally expensive.\nThis paper aims to show that such good word embeddings can be obtained using simple linear operations. We show that similar word embeddings can be computed using the word co-occurrence statistics and a well-known dimensionality reduction operation such as Principal Component Analysis (PCA). We then compare our embeddings with the CW [5], Turian [7], HLBL [10] embeddings which come from deep architectures and the LR-MVL [11] embeddings which also come from a spectral method on several NLP tasks.\nWe claim that a simple spectral method as PCA (assuming an appropriate metric) can generate word embeddings as good as with deep-learning architectures. On the other hand, deep-learning architectures have shown their potential in several supervised NLP tasks by using these word embeddings. As they are usually generated over large corpora of unlabeled data, words are represented in a generic manner. Having generic embeddings, good performance can be achieved on NLP tasks where the syntactic aspect is dominant such as Part-Of-Speech, chunking and NER [7, 8, 11]. For supervised tasks relying more on the semantic aspect as sentiment classification, it is usually helpful to adapt the existing embeddings to improve performance [12]. We show in this paper that such embedding specialization can be easily done via neural network architectures and that helps to increase general performance.\nar X\niv :1\n31 2.\n55 42\nv1 [\ncs .C\nL ]\n1 9\nD ec"
    }, {
      "heading" : "2 Related Work",
      "text" : "As 80% of the meaning of English text comes from word choice and the remaining 20% comes from word order [13], it seems quite important to leverage word order to capture all the semantic information. Connectionist approaches have therefore been proposed to develop distributed representations which encode the structural relationships between words [14, 15, 16]. Most recently, a neural network language model was proposed in Bengio et al. [17] where word vector representations are simultaneously learned along with a statistical language model. This architecture inspired other authors: Collobert and Weston [5] designed a neural language model which eliminates the linear dependency on vocabulary size, Mnih and Hinton [10] proposed a hierarchical linear neural model, Mikolov et al. [18] investigated a recurrent neural network architecture for language modeling. Such architectures being trained over large corpora of unlabeled text with the aim to predict correct scores end up learning the co-occurence statistics.\nLinguists assumed long ago that words occurring in similar contexts tend to have similar meanings [19]. Using the word co-occurrence statistics is thus a natural choice to embed similar words into a common vector space [20]. Common approaches calculate the frequencies, apply some transformations (tf-idf, PPMI), reduce the dimensionality and calculate the similarities [21]. Considering a fixed-sized word vocabulary D, the co-occurence matrix is then vocabulary size dependent. To reduce the dimensionality of the co-occurence matrix F of size W × |D| by mapping F into a matrix f of size W × d, where d |D|, techniques such as Singular Valued Decomposition (SVD) are widely used (e.g. LSA [22], ICA [23]). However, word co-occurence statistics are discrete distributions. An information theory measure such as the Hellinger distance seems to be more appropriate than the Euclidean distance over a discrete distribution space. In this paper we will compare the Hellinger PCA against the classical Euclidean PCA and the Low Rank Multi-View Learning (LRMVL) method which is another spectral method based on Canonical Correlation Analysis (CCA) to learn word embeddings [11].\nIt has been proved that using word embeddings as features helps to improve general performance on many NLP tasks [7]. However these embeddings can be too generic to perform well on other tasks such as sentiment classification. For such task, word embeddings must capture the sentiment information. Maas et al. [24] proposed a model for jointly capturing semantic and sentiment components of words into vector spaces. More recently, Labutov and Lipson [12] presented a method which takes existing embeddings and, by using some labeled data, re-embed them in the same space. They showed that these new embeddings can be better predictors in a supervised task. In this paper, we consider word embedding-based linear and non-linear models for two NLP supervised tasks: Named Entity Recognition and IMDB movie review. We analyze the effect of fine-tuning existing embeddings over each task of interest."
    }, {
      "heading" : "3 Spectral Method for Word Embeddings",
      "text" : "A NNLM learns which words among the vocabulary appear more likely after a given context sequence of words. More formally, it learns the next word probability distribution. Instead, simply counting words on a large corpus of unlabeled text can be performed to retrieve those word distributions and to represent words [20]."
    }, {
      "heading" : "3.1 Word co-occurence statistics",
      "text" : "”You shall know a word by the company it keeps” [25]. It is a natural choice to use the word co-occurence statistics to acquire representations of word meanings. Raw word co-occurence frequencies are computed by counting the number of times each word w ∈ D occurs after a context sequence of words T :\np(w|T ) = p(w, T ) p(T ) = n(w, T )∑ w n(w, T ) , (1)\nwhere n(w, T ) is the number of times each context wordw occurs after the context T . The next word probability distribution p for each word or sequence of words is thus obtained. It is a multinomial distribution of |D| classes (words). A co-occurence matrix of size N × |D| is thus obtained by computing those frequencies over all the N possible sequences of words."
    }, {
      "heading" : "3.2 Hellinger distance",
      "text" : "Similarities between words can be derived by computing a distance between their corresponding word distributions. Several distances (or metric) over discrete distributions exist, such as the Bhattacharyya distance, the Hellinger distance or Kullback-Leibler divergence. We chose here the Hellinger distance for its simplicity and symmetry property (as it is a true distance). Considering two discrete probability distributions P = (p1, . . . , pk) and Q = (q1, . . . , qk), the Hellinger distance is formally defined as:\nH(P,Q) = − 1√ 2\n√√√√ k∑\ni=1\n( √ pi − √ qi)2 , (2)\nwhich is directly related to the Euclidean norm of the difference of the square root vectors:\nH(P,Q) = 1√ 2 ‖ √ P − √ Q‖2 . (3)\nNote that it makes more sense to take the Hellinger distance rather than the Euclidean distance for comparing discrete distributions, as P and Q are unit vectors according to the Hellinger distance ( √ P and √ Q are units vector according to the `2 norm)."
    }, {
      "heading" : "3.3 Dimensionality Reduction",
      "text" : "As discrete distributions are vocabulary size dependent, using directly the distribution as a word embedding is not really tractable for large vocabulary. We propose to perform a principal component analysis (PCA) of the word co-occurence probability matrix to represent words in a lower dimensional space while minimizing the reconstruction error according to the Hellinger distance."
    }, {
      "heading" : "4 Architectures for NLP tasks",
      "text" : "Traditional NLP approaches extract from documents a rich set of hand-designed features which are then fed to a standard classification algorithm. The choice of features is a task-specific empirical process. In contrast, we want to pre-process our features as little as possible. In that respect, a multilayer neural network architecture seems appropriate as it can be trained in an end-to-end fashion on the task of interest."
    }, {
      "heading" : "4.1 Sentence-level Approach",
      "text" : "The sentence-level approach aims at tagging with a label each word in a given sentence. Embeddings of each word in a sentence are fed to linear and non-linear classification models followed by a CRFtype sentence tag inference. We chose here neural networks as classifiers.\nSliding window Context is crucial to characterize word meanings. We thus consider n context words around each word xt to be tagged, leading to a window of N = (2n + 1) words [x]t = (xt−n, . . . , xt, . . . , xt+n). As each word is embedded into a dwrd-dimensional vector, it results a dwrd ×N vector to represent a window of N words which aims at characterizing the middle word xt in this window. Given a complete sentence of T words, we can obtain for each word a contextdependent representation by sliding over all the possible windows in the sentence. A same linear transformation is then applied on each window for each word to tag:\ng([x]t) =W [x]t + b , (4)\nwhere W ∈ RC×dN and b ∈ RC are the parameters to be trained with C the number of classes. Alternatively, a one hidden layer non-linear network can be considered:\ng([x]t) =Wh(U [x]t) + b , (5)\nwhere U ∈ Rnhu×dN with nhu the number of hidden units and h(.) a transfer function.\nCRF-type inference There exists strong dependencies between tags in a sentence: some tags cannot follow other tags. To take the sentence structure into account, we want to encourage valid paths of tags during training, while discouraging all other paths. Considering the matrix of scores outputs by the network, we train a simple conditional random field (CRF). At inference time, given a sentence to tag, the best path which minimizes the sentence score is inferred with the Viterbi algorithm. More formally, we denote θ to be all the trainable parameters of the network and fθ([x]T1 ) the matrix of scores. The element [fθ]i,t of the matrix is the score output by the network for the sentence [x]T1 and the i\nth tag, at the tth word. We introduce a transition score [A]i,j for jumping from i to j tags in successive words, and an initial score [A]i,0 for starting from the ith tag. As the transition scores are going to be trained, we define θ̃ = θ ∪ {[A]i,j∀i, j}. The score of a sentence [x]T1 along a path of tags [i] T 1 is then given by the sum of transition scores and networks scores:\ns([x]T1 , [i] T 1 , θ̃) =\nT∑\nt=1\n(A[i]t−1,[i]t + [fθ][i]t,t) . (6)\nWe normalize this score over all possible tag paths [j]T1 using a softmax, and we interpret the resulting ratio as a conditional tag path probability. Taking the log, the conditional probability of the true path [y]T1 is therefore given by:\nlog p([y]T1 , [x] T 1 , θ̃) = s([x] T 1 , [y] T 1 , θ̃)− logadd ∀[j]T1 s([x]T1 , [j] T 1 , θ̃) , (7)\nwhere we adopt the notation logadd\ni zi = log (\n∑\ni\nezi) . (8)\nComputing the log-likelihood efficiently is not straightforward, as the number of terms in the logadd grows exponentially with the length of the sentence. It can be computed in linear time with the Forward algorithm, which derives a recursion similar to the Viterbi algorithm (see [26]). We can thus maximize the log-likelihood over all the training pairs ([x]T1 , [y] T 1 ) to find, given a sentence [x]T1 , the best tag path which minimizes the sentence score (6):\nargmax [j]T1\ns([x]T1 , [j] T 1 , θ̃) . (9)\nIn contrast to classical CRF, all parameters θ are trained in a end-to-end manner, by backpropagation through the Forward recursion, following Collobert et al. [8]."
    }, {
      "heading" : "4.2 Document-level Approach",
      "text" : "The document-level approach is a document binary classifier. For each document, a set of (trained) filters is applied to the sliding window described in section 4.1. The maximum value obtained by the ith filter over the whole document is:\nmax t\n[ wi[x] t + bi ] i,t 1 ≤ i ≤ nfilter . (10)\nIt can be seen as a way to measure if the information represented by the filter has been captured in the document or not. We feed all these intermediate scores to a linear classifier leading to the following simple model:\nfθ(x) = αmax t\n[ W [x]t + b ] . (11)\nIn the case of movie reviews, the ith filter might capture positive or negative sentiment depending on the sign of αi. As in section 4.1, we will also consider a non-linear classifier in the experiments.\nTraining The neural network is trained using stochastic gradient ascent. We denote θ to be all the trainable parameters of the network. Using a training set T , we minimize the following soft margin loss function with respect to θ:\nθ ← ∑ (x,y)∈T log ( 1 + e−yfθ(x) ) . (12)"
    }, {
      "heading" : "4.3 Embedding Fine-Tuning",
      "text" : "As seen in section 3, the process to compute generic word embedding is quite straightforward. These embeddings can then be used as features for supervised NLP systems and help to improve the general performance [7, 8, 9]. However, most of these systems cannot tune these embeddings as they are not structurally able to. By leveraging the deep architecture of our system, we can define a lookup-table layer initialized with existing embeddings as the first layer of the network.\nLookup-Table Layer We consider a fixed-sized word dictionaryD. Given a sequence ofN words w1, w2, . . . , wN , each word wn ∈ W is first embedded into a dwrd-dimensional vector space, by applying a lookup-table operation:\nLTW (wn) =W\n( 0, . . . , 1 , . . . , 0\nat index wn\n) = 〈W 〉wn , (13)\nwhere the matrix W ∈ Rdwrd×|D| represents the embeddings to be tuned in this lookup layer. 〈W 〉wn ∈ Rdwrd is the wth column of W and dwrd is the word vector size. Given any sequence of N words [w]N1 inD, the lookup table layer applies the same operation for each word in the sequence, producing the following output matrix:\nLTW ([w] N 1 ) = ( 〈W 〉1[w]1 . . . 〈W 〉 1 [w]N ) . (14)\nTraining Given a task of interest, a relevant representation of each word is then given by the corresponding lookup table feature vector, which is trained by backpropagation. Word representations are initialized with existing embeddings."
    }, {
      "heading" : "5 Experimental Setup",
      "text" : "We evaluate the quality of our embeddings obtained on a large corpora of unlabeled text by comparing their performance against the CW [5], Turian [7], HLBL [10], and LR-MVL [11] embeddings on NER and movie review tasks. We also show that the general performance can be improved for these tasks by fine-tuning the word embeddings."
    }, {
      "heading" : "5.1 Building Word Representation over Large Corpora",
      "text" : "Our English corpus is composed of the entire English Wikipedia1 (where all MediaWiki markups have been removed), the Reuters corpus and the Wall Street Journal (WSJ) corpus. We chose to consider lower case words to limit the number of words in the vocabulary. Additionally, all occurrences of sequences of numbers within a word are replaced with the string “NUMBER”. The resulting text was tokenized using the Stanford tokenizer2. The data set contains about 1,652 million words. As vocabulary we considered all the words within our corpus which appear at least one hundred times. This results in a 178,080 words vocabulary. To build the co-occurence matrix we used only the 10,000 most frequent words within our vocabulary as context words. Each word is represented in a 50-dimensional vector as the other embeddings in the literature. The resulting embeddings will be referred as H-PCA in the following sections. To highlight the importance of the Hellinger distance, we also computed the PCA of the co-occurence probability matrix with respect to the Euclidean metric. The resulting embeddings are denoted E-PCA."
    }, {
      "heading" : "5.2 Existing Available Word Embeddings",
      "text" : "We choose to compare our H-PCA’s embeddings with the following publicly available embeddings:\n1Available at http://download.wikimedia.org. We took the May 2012 version. 2Available at http://nlp.stanford.edu/software/tokenizer.shtml\n• LR-MVL3: it covers 300,000 words with 50 dimensions for each word. They were trained on the RCV1 corpus using the Low Rank Multi-View Learning method. We only used their context oblivious embeddings coming from the eigenfeature dictionary.\n• CW4: it covers 130,000 words with 50 dimensions for each word. They were trained for about two months, over Wikipedia, using a neural network language model approach.\n• Turian5: it covers 268,810 words with 25, 50, 100 or 200 dimensions for each word. They were trained on the RCV1 corpus using the same system as the CW embeddings but with different parameters. We used only the 50 dimensions.\n• HLBL5 : it covers 246,122 words with 50 or 100 dimensions for each word. They were trained on the RCV1 corpus using a Hierarchical Log-Bilinear Model. We used only the 50 dimensions."
    }, {
      "heading" : "5.3 Supervised Evaluation Tasks",
      "text" : "Using word embeddings as feature proved that it can improve the generalization performance on several NLP tasks [7, 8, 9]. Using our word embeddings, we thus trained the sentence-level architecture described in section 4.1 on a NER task.\nNamed Entity Recognition (NER) It labels atomic elements in the sentence into categories such as “PERSON” or “LOCATION”. The CoNLL 2003 setup6 is a NER benchmark data set based on Reuters data. The contest provides training, validation and testing sets. The networks are fed with two raw features: word embeddings and a capital letter feature. The “caps” feature tells if each word was in lowercase, was all uppercase, had first letter capital, or had at least one non-initial capital letter. No other feature has been used to tune the models. This is a main difference with other systems which usually use more features as POS tags, prefixes and suffixes or gazetteers. Hyperparameters were tuned on the validation set. We picked n = 2 context words leading to a window of 5 words. We used a special “PADDING” word for context at the beginning and the end of each sentence. For the non-linear model, the number of hidden units was 300. As benchmark system, we report the system of Ando et al. [27] which reached 89.31% F1 with a semi-supervised approach and less specialized features than CoNLL 2003 challengers.\nThe NER evaluation task is mainly syntactic. As we wish to evaluate whether our word embeddings can also capture semantic, we trained the document-level architecture described in section 4.2 over a movie review task.\nIMDB Review Dataset We used a collection of 50,000 reviews from IMDB7. It allows no more than 30 reviews per movie. It contains an even number of positive and negative reviews, so randomly guessing yields 50% accuracy. Only highly polarized reviews have been considered. A negative review has a score ≤ 4 out of 10, and a positive review has a score ≥ 7 out of 10. It has been evenly divided into training and test sets (25,000 reviews each). For this task, we only used the word embeddings as features. We perform a simple cross-validation on the training set to choose the optimal hyper-parameters. The network had a window of 5 words and nfilter = 1000 filters. As benchmark system, we report the system of Maas et al. [24] which reached 88.90% accuracy with a mix of unsupervised and supervised techniques to learn word vectors capturing semantic term-document information as well as rich sentiment content."
    }, {
      "heading" : "5.4 Results",
      "text" : "H-PCA embeddings Results summarized in Table 1a reveal that performance on NER task can be as good with word embeddings from a word co-occurence matrix decomposition as with a neural network language model trained for weeks. The best F1 scores are indeed obtained using the H-PCA tuned embeddings. Results for the movie review task in Table 1b show that H-PCA’s embeddings\n3Available at http://www.cis.upenn.edu/ ungar/eigenwords/ 4From SENNA: http://ml.nec-labs.com/senna/ 5Available at http://metaoptimize.com/projects/wordreprs/ 6http://www.cnts.ua.ac.be/conll2003/ner/ 7Available at http://www.andrew-maas.net/data/sentiment\nalso perform as well as all the other embeddings on the movie review task. It is worth mentioning that on both tasks H-PCA’s embeddings outperform the LR-MVL’s and E-PCA’s embeddings, demonstrating the value of the Hellinger distance. When the embeddings are not tuned, the CW’s embeddings slightly outperform the H-PCA’s embeddings on NER task. The performance difference between both fixed embeddings on the movie review task is about 3%. Embeddings from the CW neural language model seems to capture more semantic information but we showed that this lack of semantic information can be offset by fine-tuning.\nEmbeddings fine-tuning We note that tuning the embeddings leads to increase the general performance on both NER and movie review tasks. The increase is, in general, higher for the movie review task which reveals the importance of embedding fine-tuning for NLP tasks with a high semantic component. We thus show in Table 1 that the embeddings after fine-tuning give a higher rank to words that are related to the task of interest which is movie-sentiment-based relations in this case.\nLinear vs nonlinear model We also report results with a linear version of our neural networks. Having non-linearity helps for NER. It seems important to extract non-linear features for such a\ntask. However, we note that the linear approach performs as well as the non-linear approach for the movie review task. Our approach capture all the necessary sentiment features to predict whether a review is positive or negative. That shows that a keyword matching method can perform well on this tasks as we don’t need to introduce non-linear feature extractors. As our method takes the whole review as input, we can clearly extract windows of words having the most discriminative power: it is a major advantage of our method compared to conventional bag-of-words based methods. We report in Table 2 some examples of windows of words extracted from the most discriminative filters αi (positive and negative). Note that there is an equal number of positive and negative filters after learning."
    }, {
      "heading" : "6 Conclusion",
      "text" : "We have demonstrated that appealing word embeddings can be obtained by computing a Hellinger PCA of the word co-occurence matrix. While a neural network language model can be painful and long to train, we can get a word co-occurence matrix by simply counting words over a large corpus. The resulting embeddings give similar results on NLP tasks even from a N × 10, 000 word cooccurence matrix. It reveals that having a significant but not too large set of common words seems sufficient for capturing most of the syntactic and semantic characteristics of words. As PCA of aN× 10, 000 matrix is really fast and not memory consuming, our method gives an interesting and practical alternative to neural language models for generating word embeddings. However, we showed that deep-learning is an interesting framework to fine-tune embeddings over specific NLP tasks. Our H-PCA’s embeddings are available online, here: http://www.lebret.ch/words/."
    }, {
      "heading" : "Acknowledgments",
      "text" : "This work was partly supported by the HASLER foundation through the grant “Information and Communication Technology for a Better World 2020” (SmartWorld)."
    } ],
    "references" : [ {
      "title" : "Class-based n-gram models of natural language",
      "author" : [ "P.F. Brown", "P.V. deSouza", "R.L. Mercer", "V.J.D. Pietra", "J C. Lai" ],
      "venue" : "Computational Linguistics,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 1992
    }, {
      "title" : "Distributional part-of-speech tagging",
      "author" : [ "H. Schütze" ],
      "venue" : "Proceedings of the Association for Computational Linguistics (ACL), pages 141–148. Morgan Kaufmann Publishers Inc.,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 1995
    }, {
      "title" : "Simple semi-supervised dependency parsing",
      "author" : [ "T. Koo", "X. Carreras", "M. Collins" ],
      "venue" : "Proceedings of the Association for Computational Linguistics (ACL), pages 595–603,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Design challenges and misconceptions in named entity recognition",
      "author" : [ "L. Ratinov", "D. Roth" ],
      "venue" : "Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL), pages 147–155. Association for Computational Linguistics,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "A unified architecture for natural language processing: Deep neural networks with multitask learning",
      "author" : [ "R. Collobert", "J. Weston" ],
      "venue" : "International Conference on Machine Learning, ICML,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Distributional representations for handling sparsity in supervised sequence-labeling",
      "author" : [ "F. Huang", "A. Yates" ],
      "venue" : "Proceedings of the Association for Computational Linguistics (ACL), pages 495–503. Association for Computational Linguistics,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Word representations: A simple and general method for semi-supervised learning",
      "author" : [ "J. Turian", "L. Ratinov", "Y. Bengio" ],
      "venue" : "ACL,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Natural language processing (almost) from scratch",
      "author" : [ "R. Collobert", "J. Weston", "L. Bottou", "M. Karlen", "K. Kavukcuoglu", "P. Kuksa" ],
      "venue" : "Journal of Machine Learning Research, 12:2493– 2537,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "The expressive power of word embeddings",
      "author" : [ "Y. Chen", "B. Perozzi", "R. Al-Rfou", "S. Skiena" ],
      "venue" : "CoRR, abs/1301.3226,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2013
    }, {
      "title" : "A Scalable Hierarchical Distributed Language Model",
      "author" : [ "A. Mnih", "G. Hinton" ],
      "venue" : "Advances in Neural Information Processing Systems, volume 21,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Multi-view learning of word embeddings via cca",
      "author" : [ "P.S. Dhillon", "D. Foster", "L. Ungar" ],
      "venue" : "Advances in Neural Information Processing Systems (NIPS), volume 24,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Re-embedding words",
      "author" : [ "I. Labutov", "H. Lipson" ],
      "venue" : "ACL,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "On the computational basis of learning and cognition: Arguments from lsa",
      "author" : [ "T.K. Landauer" ],
      "venue" : "N. Ross, editor, The psychology of learning and motivation, volume 41, pages 43–84. Academic Press, San Francisco, CA,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Learning distributed representations of concepts",
      "author" : [ "G.E. Hinton" ],
      "venue" : "Proceedings of the Eighth Annual Conference of the Cognitive Science Society, pages 1–12. Hillsdale, NJ: Erlbaum,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 1986
    }, {
      "title" : "Recursive distributed representations",
      "author" : [ "J.B. Pollack" ],
      "venue" : "Artificial Intelligence, 46:77–105,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 1990
    }, {
      "title" : "Distributed representations, simple recurrent networks, and grammatical structure",
      "author" : [ "J.L. Elman" ],
      "venue" : "Machine Learning, 7:195–225,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 1991
    }, {
      "title" : "A neural probabilistic language model",
      "author" : [ "Y. Bengio", "R. Ducharme", "P. Vincent", "C. Janvin" ],
      "venue" : "J. Mach. Learn. Res., 3:1137–1155, March",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Recurrent neural network based language model",
      "author" : [ "T. Mikolov", "M. Karafit", "L. Burget", "J. ernock", "Sanjeev Khudanpur" ],
      "venue" : "In Proceedings of the 11th Annual Conference of the International Speech Communication Association (INTERSPEECH",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2010
    }, {
      "title" : "Philosophical Investigations",
      "author" : [ "L. Wittgenstein" ],
      "venue" : "Blackwell,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 1953
    }, {
      "title" : "From frequency to meaning: Vector space models of semantics",
      "author" : [ "P.D. Turney", "P. Pantel" ],
      "venue" : "CoRR, abs/1003.1141,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Towards a theory of semantic space, pages 576–581",
      "author" : [ "W. Lowe" ],
      "venue" : null,
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2001
    }, {
      "title" : "A solution to Plato’s problem: The Latent Semantic Analysis theory of the acquisition, induction, and representation of knowledge",
      "author" : [ "T.K. Landauer", "S.T. Dumais" ],
      "venue" : "Psychological Review, 104:211–240,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 1997
    }, {
      "title" : "Word category maps based on emergent features created by ICA",
      "author" : [ "J.J. Väyrynen", "T. Honkela" ],
      "venue" : "Heikki Hyötyniemi, Pekka Ala-Siuru, and Jouko Seppänen, editors, Proceedings of the STeP’2004 Cognition + Cybernetics Symposium, number 19 in Publications of the Finnish Artificial Intelligence Society, pages 173–185. Finnish Artificial Intelligence Society,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Learning word vectors for sentiment analysis",
      "author" : [ "A.L. Maas", "R.E. Daly", "P.T. Pham", "D. Huang", "A.Y. Ng", "C. Potts" ],
      "venue" : "ACL, pages 142–150,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "A synopsis of linguistic theory 1930-55",
      "author" : [ "J.R. Firth" ],
      "venue" : "1952-59:1–32,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 1957
    }, {
      "title" : "A tutorial on hidden markov models and selected applications in speech recognition",
      "author" : [ "L.R. Rabiner" ],
      "venue" : "Proceedings of the IEEE, pages 257–286,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 1989
    }, {
      "title" : "A framework for learning predictive structures from multiple tasks and unlabeled data",
      "author" : [ "R.K. Ando", "T. Zhang", "P. Bartlett" ],
      "venue" : "Journal of Machine Learning Research, 6:1817–1853,",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 2005
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Popular approaches such as Brown clustering algorithm [1] have been used with success in a wide variety of NLP tasks [2, 3, 4].",
      "startOffset" : 54,
      "endOffset" : 57
    }, {
      "referenceID" : 1,
      "context" : "Popular approaches such as Brown clustering algorithm [1] have been used with success in a wide variety of NLP tasks [2, 3, 4].",
      "startOffset" : 117,
      "endOffset" : 126
    }, {
      "referenceID" : 2,
      "context" : "Popular approaches such as Brown clustering algorithm [1] have been used with success in a wide variety of NLP tasks [2, 3, 4].",
      "startOffset" : 117,
      "endOffset" : 126
    }, {
      "referenceID" : 3,
      "context" : "Popular approaches such as Brown clustering algorithm [1] have been used with success in a wide variety of NLP tasks [2, 3, 4].",
      "startOffset" : 117,
      "endOffset" : 126
    }, {
      "referenceID" : 4,
      "context" : "Recently, distributed approaches based on neural network language models (NNLM) have revived the field of learning word embeddings [5, 6, 7, 8, 9].",
      "startOffset" : 131,
      "endOffset" : 146
    }, {
      "referenceID" : 5,
      "context" : "Recently, distributed approaches based on neural network language models (NNLM) have revived the field of learning word embeddings [5, 6, 7, 8, 9].",
      "startOffset" : 131,
      "endOffset" : 146
    }, {
      "referenceID" : 6,
      "context" : "Recently, distributed approaches based on neural network language models (NNLM) have revived the field of learning word embeddings [5, 6, 7, 8, 9].",
      "startOffset" : 131,
      "endOffset" : 146
    }, {
      "referenceID" : 7,
      "context" : "Recently, distributed approaches based on neural network language models (NNLM) have revived the field of learning word embeddings [5, 6, 7, 8, 9].",
      "startOffset" : 131,
      "endOffset" : 146
    }, {
      "referenceID" : 8,
      "context" : "Recently, distributed approaches based on neural network language models (NNLM) have revived the field of learning word embeddings [5, 6, 7, 8, 9].",
      "startOffset" : 131,
      "endOffset" : 146
    }, {
      "referenceID" : 4,
      "context" : "We then compare our embeddings with the CW [5], Turian [7], HLBL [10] embeddings which come from deep architectures and the LR-MVL [11] embeddings which also come from a spectral method on several NLP tasks.",
      "startOffset" : 43,
      "endOffset" : 46
    }, {
      "referenceID" : 6,
      "context" : "We then compare our embeddings with the CW [5], Turian [7], HLBL [10] embeddings which come from deep architectures and the LR-MVL [11] embeddings which also come from a spectral method on several NLP tasks.",
      "startOffset" : 55,
      "endOffset" : 58
    }, {
      "referenceID" : 9,
      "context" : "We then compare our embeddings with the CW [5], Turian [7], HLBL [10] embeddings which come from deep architectures and the LR-MVL [11] embeddings which also come from a spectral method on several NLP tasks.",
      "startOffset" : 65,
      "endOffset" : 69
    }, {
      "referenceID" : 10,
      "context" : "We then compare our embeddings with the CW [5], Turian [7], HLBL [10] embeddings which come from deep architectures and the LR-MVL [11] embeddings which also come from a spectral method on several NLP tasks.",
      "startOffset" : 131,
      "endOffset" : 135
    }, {
      "referenceID" : 6,
      "context" : "Having generic embeddings, good performance can be achieved on NLP tasks where the syntactic aspect is dominant such as Part-Of-Speech, chunking and NER [7, 8, 11].",
      "startOffset" : 153,
      "endOffset" : 163
    }, {
      "referenceID" : 7,
      "context" : "Having generic embeddings, good performance can be achieved on NLP tasks where the syntactic aspect is dominant such as Part-Of-Speech, chunking and NER [7, 8, 11].",
      "startOffset" : 153,
      "endOffset" : 163
    }, {
      "referenceID" : 10,
      "context" : "Having generic embeddings, good performance can be achieved on NLP tasks where the syntactic aspect is dominant such as Part-Of-Speech, chunking and NER [7, 8, 11].",
      "startOffset" : 153,
      "endOffset" : 163
    }, {
      "referenceID" : 11,
      "context" : "For supervised tasks relying more on the semantic aspect as sentiment classification, it is usually helpful to adapt the existing embeddings to improve performance [12].",
      "startOffset" : 164,
      "endOffset" : 168
    }, {
      "referenceID" : 12,
      "context" : "As 80% of the meaning of English text comes from word choice and the remaining 20% comes from word order [13], it seems quite important to leverage word order to capture all the semantic information.",
      "startOffset" : 105,
      "endOffset" : 109
    }, {
      "referenceID" : 13,
      "context" : "Connectionist approaches have therefore been proposed to develop distributed representations which encode the structural relationships between words [14, 15, 16].",
      "startOffset" : 149,
      "endOffset" : 161
    }, {
      "referenceID" : 14,
      "context" : "Connectionist approaches have therefore been proposed to develop distributed representations which encode the structural relationships between words [14, 15, 16].",
      "startOffset" : 149,
      "endOffset" : 161
    }, {
      "referenceID" : 15,
      "context" : "Connectionist approaches have therefore been proposed to develop distributed representations which encode the structural relationships between words [14, 15, 16].",
      "startOffset" : 149,
      "endOffset" : 161
    }, {
      "referenceID" : 16,
      "context" : "[17] where word vector representations are simultaneously learned along with a statistical language model.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 4,
      "context" : "This architecture inspired other authors: Collobert and Weston [5] designed a neural language model which eliminates the linear dependency on vocabulary size, Mnih and Hinton [10] proposed a hierarchical linear neural model, Mikolov et al.",
      "startOffset" : 63,
      "endOffset" : 66
    }, {
      "referenceID" : 9,
      "context" : "This architecture inspired other authors: Collobert and Weston [5] designed a neural language model which eliminates the linear dependency on vocabulary size, Mnih and Hinton [10] proposed a hierarchical linear neural model, Mikolov et al.",
      "startOffset" : 175,
      "endOffset" : 179
    }, {
      "referenceID" : 17,
      "context" : "[18] investigated a recurrent neural network architecture for language modeling.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 18,
      "context" : "Linguists assumed long ago that words occurring in similar contexts tend to have similar meanings [19].",
      "startOffset" : 98,
      "endOffset" : 102
    }, {
      "referenceID" : 19,
      "context" : "Using the word co-occurrence statistics is thus a natural choice to embed similar words into a common vector space [20].",
      "startOffset" : 115,
      "endOffset" : 119
    }, {
      "referenceID" : 20,
      "context" : "Common approaches calculate the frequencies, apply some transformations (tf-idf, PPMI), reduce the dimensionality and calculate the similarities [21].",
      "startOffset" : 145,
      "endOffset" : 149
    }, {
      "referenceID" : 21,
      "context" : "LSA [22], ICA [23]).",
      "startOffset" : 4,
      "endOffset" : 8
    }, {
      "referenceID" : 22,
      "context" : "LSA [22], ICA [23]).",
      "startOffset" : 14,
      "endOffset" : 18
    }, {
      "referenceID" : 10,
      "context" : "In this paper we will compare the Hellinger PCA against the classical Euclidean PCA and the Low Rank Multi-View Learning (LRMVL) method which is another spectral method based on Canonical Correlation Analysis (CCA) to learn word embeddings [11].",
      "startOffset" : 240,
      "endOffset" : 244
    }, {
      "referenceID" : 6,
      "context" : "It has been proved that using word embeddings as features helps to improve general performance on many NLP tasks [7].",
      "startOffset" : 113,
      "endOffset" : 116
    }, {
      "referenceID" : 23,
      "context" : "[24] proposed a model for jointly capturing semantic and sentiment components of words into vector spaces.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 11,
      "context" : "More recently, Labutov and Lipson [12] presented a method which takes existing embeddings and, by using some labeled data, re-embed them in the same space.",
      "startOffset" : 34,
      "endOffset" : 38
    }, {
      "referenceID" : 19,
      "context" : "Instead, simply counting words on a large corpus of unlabeled text can be performed to retrieve those word distributions and to represent words [20].",
      "startOffset" : 144,
      "endOffset" : 148
    }, {
      "referenceID" : 24,
      "context" : "”You shall know a word by the company it keeps” [25].",
      "startOffset" : 48,
      "endOffset" : 52
    }, {
      "referenceID" : 25,
      "context" : "It can be computed in linear time with the Forward algorithm, which derives a recursion similar to the Viterbi algorithm (see [26]).",
      "startOffset" : 126,
      "endOffset" : 130
    }, {
      "referenceID" : 7,
      "context" : "[8].",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 6,
      "context" : "These embeddings can then be used as features for supervised NLP systems and help to improve the general performance [7, 8, 9].",
      "startOffset" : 117,
      "endOffset" : 126
    }, {
      "referenceID" : 7,
      "context" : "These embeddings can then be used as features for supervised NLP systems and help to improve the general performance [7, 8, 9].",
      "startOffset" : 117,
      "endOffset" : 126
    }, {
      "referenceID" : 8,
      "context" : "These embeddings can then be used as features for supervised NLP systems and help to improve the general performance [7, 8, 9].",
      "startOffset" : 117,
      "endOffset" : 126
    }, {
      "referenceID" : 4,
      "context" : "We evaluate the quality of our embeddings obtained on a large corpora of unlabeled text by comparing their performance against the CW [5], Turian [7], HLBL [10], and LR-MVL [11] embeddings on NER and movie review tasks.",
      "startOffset" : 134,
      "endOffset" : 137
    }, {
      "referenceID" : 6,
      "context" : "We evaluate the quality of our embeddings obtained on a large corpora of unlabeled text by comparing their performance against the CW [5], Turian [7], HLBL [10], and LR-MVL [11] embeddings on NER and movie review tasks.",
      "startOffset" : 146,
      "endOffset" : 149
    }, {
      "referenceID" : 9,
      "context" : "We evaluate the quality of our embeddings obtained on a large corpora of unlabeled text by comparing their performance against the CW [5], Turian [7], HLBL [10], and LR-MVL [11] embeddings on NER and movie review tasks.",
      "startOffset" : 156,
      "endOffset" : 160
    }, {
      "referenceID" : 10,
      "context" : "We evaluate the quality of our embeddings obtained on a large corpora of unlabeled text by comparing their performance against the CW [5], Turian [7], HLBL [10], and LR-MVL [11] embeddings on NER and movie review tasks.",
      "startOffset" : 173,
      "endOffset" : 177
    }, {
      "referenceID" : 6,
      "context" : "Using word embeddings as feature proved that it can improve the generalization performance on several NLP tasks [7, 8, 9].",
      "startOffset" : 112,
      "endOffset" : 121
    }, {
      "referenceID" : 7,
      "context" : "Using word embeddings as feature proved that it can improve the generalization performance on several NLP tasks [7, 8, 9].",
      "startOffset" : 112,
      "endOffset" : 121
    }, {
      "referenceID" : 8,
      "context" : "Using word embeddings as feature proved that it can improve the generalization performance on several NLP tasks [7, 8, 9].",
      "startOffset" : 112,
      "endOffset" : 121
    }, {
      "referenceID" : 26,
      "context" : "[27] which reached 89.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 23,
      "context" : "[24] which reached 88.",
      "startOffset" : 0,
      "endOffset" : 4
    } ],
    "year" : 2017,
    "abstractText" : "Word embeddings resulting from neural language models have been shown to be successful for a large variety of NLP tasks. However, such architecture might be difficult to train and time-consuming. Instead, we propose to drastically simplify the word embeddings computation through a Hellinger PCA of the word co-occurence matrix. We compare those new word embeddings with some wellknown embeddings on NER and movie review tasks and show that we can reach similar or even better performance. Although deep learning is not really necessary for generating good word embeddings, we show that it can provide an easy way to adapt embeddings to specific tasks.",
    "creator" : "LaTeX with hyperref package"
  }
}