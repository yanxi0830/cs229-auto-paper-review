{
  "name" : "1311.2547.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Learning Mixtures of Linear Classifiers",
    "authors" : [ "Yuekai Sun", "Andrea Montanari", "Yi" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n31 1.\n25 47\nv1 [\ncs .L\nG ]\n1 1\nN ov"
    }, {
      "heading" : "1. Introduction",
      "text" : "Since Pearson’s seminal contribution [17], and most notably after the introduction of the EM algorithm [7], mixture models and latent variable models have played a central role in statistics and machine learning, with numerous applications—see, e.g., McLachlan & Peel [14], Bishop [4], and Bartholomew et al. [3]. Despite their ubiquity, fitting the parameters of a mixture model remains a challenging task. The most popular methods (e.g., the EM algorithm or likelihood maximization by gradient ascent) are plagued by local optima and come with little or no guarantees. Computationally efficient algorithms with provable guarantees are an exception in this area. Even the idealized problem of learning mixtures of Gaussians has motivated a copious theoretical literature [2, 15].\nIn this paper we consider the problem of modeling a regression function as a mixture of k components. Namely, we are given labels Yi ∈ R and feature vectors Xi ∈ Rd, i ∈ [n] ≡ {1, 2, . . . , n}, and we seek estimates of the parameters of a mixture model\nYi ∣∣ Xi=xi ∼∑kℓ=1 pℓ f(yi|xi, uℓ) . (1)\nHere k is the number of components, (pℓ)ℓ∈[k] are weights of the components, and uℓ is a vector of parameters for the ℓ-th component. Models of this type have been intensely studied in the neural network literature since the early nineties [4, 10]. They have also found numerous applications ranging from object recognition [18] to machine translation [12]. These studies are largely based on learning algorithms without consistency guarantees.\nRecently, Chaganty and Liang [5] considered mixtures of linear regressions, whereby the relation between labels and feature vectors is linear within each\n1\ncomponent; i.e., Yi = 〈uℓ, Xi〉+noise (here and below 〈a, b〉 denotes the standard scalar product in Rm). Equivalently, f(yi|xi, uℓ) = f0(yi−〈xi, uℓ〉) with f0( · ) a density of mean zero. Building on a new approach developed by Hsu et al. [9] and Anandkumar et al. [1], these authors propose an algorithm for fitting mixtures of linear regressions with provable guarantees. The basic idea is to regress Y qi , for q ∈ {1, 2, 3} against the tensors Xi, Xi⊗Xi, Xi⊗Xi⊗Xi. The coefficients of these regressions are tensors whose decomposition gives rise to the parameters uℓ, pℓ.\nWhile the work of Chaganty and Liang [5] is a significant step forward, it leaves several open problems:\nStatistical efficiency. Let us assume a standard scaling of the feature vectors, whereby the components (Xi,j)j∈[p] are of order one. Then, the mathematical guarantees of Chaganty & Liang [5] require a sample size n ≫ d6. This is substantially larger than the ‘information-theoretic’ optimal scaling, and is an unrealistic requirement in high-dimension (large d). As noted in [5], this scaling is an intrinsic drawback of the tensor approach which requires working in a higher-dimensional space (tensor space) than the space in which data naturally live.\nLinear regression versus classification. In virtually all applications of the mixture model (1), the labels Yi are categorical—see, e.g., Jordan & Jacobs [10], Bishop [4], Quattoni et al. [18], Liang et al. [12]. In this case, the very first step of Chaganty & Liang, namely, regressing Y 2i on X ⊗2 i and Y 3 i onX ⊗3 i , breaks down. Consider—to be definite—the important case of binary labels (e.g., Yi ∈ {0, 1} or Yi ∈ {+1,−1}). Then powers of the labels do not provide any further information (e.g., if Yi ∈ {0, 1}, then Yi = Y 2i = Y 3i ). Also, since Yi is non-linearly related to uℓ, Y 2 i does not depend only on u ⊗2 ℓ .\nComputational complexity. The method of [5] requires solving a regularized linear regression in d3 dimensions and factorizing a tensor of third order in d dimensions. Even under optimistic assumptions (finite convergence of iterative schemes), this requres O(d3n+ d4) operations.\nIn this paper, we develop a spectral approach to learning mixtures of linear classifiers in high dimension. For the sake of simplicity, we shall focus on the case of binary labels Yi ∈ {+1,−1}, but we expect our ideas to be more broadly applicable. We consider regression functions of the form f(yi|xi, uℓ) = f(yi|〈xi, uℓ〉), i.e., each component corresponds to a generalized linear model with parameter vector uℓ ∈ Rd. In a nutshell, our method constructs a symmetric matrix Q̂ ∈ Rd×d by taking a suitable empirical average of the data. The matrix Q̂ has the following property: (d − k) of its eigenvalues are roughly degenerate. The remaining k eigenvalues correspond to eigenvectors that–approximately– span the same subspace as u1, . . . , uk. Once this space is accurately estimated, the problem dimensionality is reduced to k; as such, it is easy to come up with effective prediction methods (as a matter of fact, simple K-nearest neighbors works very well).\nThe resulting algorithm is computationally efficient, as its most expensive\nstep is computing the eigenvector decomposition of a d× d matrix (which takes O(d3) operations). Assuming Gaussian feature vectors Xi ∈ Rd, we prove that our method is also statistically efficient, i.e., it only requires n = Ω(d) samples to accurately reconstruct the subspace spanned by u1, . . . , uk. This is the same amount of data needed to estimate the covariance of the feature vectors Xi or a parameter vector u1 ∈ Rd in the trivial case of a mixture with a single component, k = 1. It is unlikely that a significantly better efficiency can be achieved without additional structure.\nThe assumption of Gaussian feature vectors Xi’s is admittedly restrictive. On one hand, as for the problem of learning mixtures of Gaussians [2, 15], we believe that useful insights can be gained by studying this simple setting. On the other, and as discussed below, our proof does not really require the distribution of the Xi’s to be Gaussian, and a strictly weaker assumption is sufficient. We expect that future work will succeed in further relaxing this assumption."
    }, {
      "heading" : "1.1. Technical contribution and related work",
      "text" : "Our approach is related to the principal Hessian directions (pHd) method proposed by Li [11] and further developed by Cook [6] and co-workers. PHd is an approach to dimensionality reduction and data visualization. It generalizes principal component analysis to the regression (discriminative) setting, whereby each data point consists of a feature vector Xi ∈ Rd and a label Yi ∈ R. Summarizing, the idea is to form the ‘Hessian’ matrix Ĥ = n−1 ∑n i=1 YiXiX T i ∈ Rd×d. (We assume here, for ease of exposition, that the Xi’s have zero mean and unit covariance.) The eigenvectors associated to eigenvalues with largest magnitude are used to identify a subspace in Rd onto which to project the feature vectors Xi’s.\nUnfortunately, the pHd approach fails in general for the mixture models of interest here, i.e., if Yi ∣∣ Xi=xi ∼∑kℓ=1 pℓ f(yi|〈uℓ, xi〉). For instance, it fails when each component is described by a logistic model f(yi = +1|z) = (1 + e−z)−1, when features are centered at E(Xi) = 0 (see Appendix D).\nOur approach overcomes this problem by constructing Q̂ = n−1 ∑n\ni=1 ZiXiX T i ∈\nR d×d. The Zi’s are pseudo-labels obtained by applying a ‘mirroring’ transformation to the Yi’s. Unlike with Ĥ , the eigenvector structure of Q̂ enables us to estimate the span of u1, . . . , uk.\nAs an additional technical contribution, we establish non-asymptotic bounds on the estimation error, that allows to characterize the trade-off between the data dimension d, and the sample size n. In contrast, rigorous analysis on pHd is limited to the low-dimensional regime of d fixed as n → ∞. It would be interesting to generalize the analysis developed here to characterize the highdimensional properties of pHd as well."
    }, {
      "heading" : "2. Problem Formulation",
      "text" : ""
    }, {
      "heading" : "2.1. Model",
      "text" : "Consider a dataset comprising n i.i.d. pairs (Xi, Yi) ∈ Rd × {−1,+1}, i ∈ [n]. We refer to the vectors Xi ∈ Rd as features and to the binary variables as labels. We assume that the features Xi ∈ Rd are sampled from a Gaussian distribution with mean µ ∈ Rd and a positive definite covariance Σ ∈ Rd×d. The labels Yi ∈ {−1,+1} are generated by a mixture of linear classifiers, i.e., labels are distributed as follows:\nPr(Yi = +1 | Xi) = ∑k\nℓ=1 pℓ f(〈uℓ, Xi〉) . (2) Here, k ≥ 2 is the number of components in the mixture; (pℓ)ℓ∈[k] are the weights, satisfying of course pℓ > 0, ∑k ℓ=1 pℓ = 1; and (uℓ)ℓ∈[k], uℓ ∈ Rd are the normals to the planes defining the k linear classifiers. We refer to each normal uℓ as the parameter profile of the ℓ-th classifier; we assume that the profiles uℓ, ℓ ∈ [k], are linearly independent, and that k < n/2.\nWe assume that the function f : R → [0, 1], characterizing the classifier response, is non-decreasing, strictly concave in [0,+∞), and satisfies:\nlim t→∞ f(t)=1, lim t→−∞\nf(t)=0, 1−f(t)=f(−t). (3)\nAs an example, it is useful to keep in mind the logistic function f(t) = (1 + e−t)−1. Fig. 1(a) illustrates a mixture of k = 2 classifiers over d = 3 dimensions."
    }, {
      "heading" : "2.2. Subspace Estimation, Prediction and Clustering",
      "text" : "Our main focus is the following task:\nSubspace Estimation: After observing (Xi, Yi), i ∈ [n], estimate the subspace spanned by the profiles of the k classifiers, i.e., U ≡ span(u1, . . . , uk).\nFor Û an estimate of U , we characterize performance via the principal angle between the two spaces, namely\ndP (U, Û) = max x∈U,y∈Û\narccos (\n〈x,y〉 ‖x‖‖y‖\n) .\nNotice that projecting the features Xi on U entails no loss of information w.r.t. (2); this can be exploited for dimensionality reduction, with the purpose of performing tasks beyond subspace estimation. We review two such tasks below.\nPrediction: Given a new feature vector Xn+1, predict the corresponding label Yn+1.\nClustering: Given a new feature vector and label pair (Xn+1, Yn+1), identify the classifier that generated the label.\nAs we will see in Section 5, our subspace estimate can be used to significantly improve the performance of both prediction and clustering."
    }, {
      "heading" : "2.3. Technical Preliminary",
      "text" : "We review here a few definitions that are used in our exposition. The subgaussian norm of a random variable X is:\n‖X‖ψ2 = sup p≥1 1√ p (E[|X |p])1/p.\nWe say that X is sub-gaussian if ‖X‖ψ2 < ∞. We say that a random vector X ∈ Rd is sub-gaussian if 〈y,X〉 is sub-Gaussian for any y ∈ Rd, and let ‖X‖ψ2 ≡ sup‖y‖2≤1 ‖〈y,X〉‖ψ2.\nWe use the following variant of Stein’s identity [13, 19]. Let X ∈ Rd, X ′ ∈ Rd′ be jointly Gaussian random vectors, and consider a function h : Rd\n′ → R that is almost everywhere (a.e.) differentiable and satisfies E[|∂h(X ′)/∂xi|] < ∞, i ∈ [d′]. Then, the following identity holds:\nCov(X,h(X ′)) = Cov(X,X ′)E[∇h(X ′)]. (4)"
    }, {
      "heading" : "3. Subspace Estimation",
      "text" : "In this section, we present our algorithm for subspace estimation, which we refer to as SpectralMirror. Our main technical contribution, stated formally below, is that the output Û of SpectralMirror is a consistent estimator of the subspace U spanned by the parameter vectors uℓ, as soon as n ≥ C d for a sufficiently large constant C.\nAlgorithm 1 SpectralMirror Require: Pairs (Xi, Yi), i ∈ [n] Ensure: Subspace estimate Û\n1: µ̂ ← 1 ⌊n/2⌋ ∑⌊n/2⌋ i=1 Xi 2: Σ̂ ← 1 ⌈n/2⌉ ∑⌊n/2⌋ i=1 (Xi − µ̂)(Xi − µ̂) T 3: r̂ ← 1 ⌊n/2⌋ ∑⌊n/2⌋ i=1 YiΣ̂ −1/2(Xi−µ̂) 4: for each i ∈ {⌊n/2⌋ + 1, . . . , n}: Zi ← Yi sgn〈r̂, Xi〉 5: Q̂← 1\n⌈n/2⌉\nn∑\ni=⌊n/2⌋+1\nZiΣ̂ −1/2(Xi−µ̂)(Xi−µ̂) T Σ̂−1/2\n6: Find eigendecomposition ∑d\nℓ=1 λℓwℓw T ℓ of Q̂\n7: Let λ(1), . . . , λ(k) be the k eigenvalues furthest from the median. 8: Û ← span ( Σ̂−1/2w(1), . . . , Σ̂ −1/2w(k) )"
    }, {
      "heading" : "3.1. Spectral Mirror Algorithm",
      "text" : "We begin by presenting our algorithm for estimating the subspace span U . Our algorithm consists of three main steps. First, as pre-processing, we estimate the mean and covariance of the underlying features Xi. Second, using these estimates, we identify a vector r̂ that concentrates near the convex cone spanned by the profiles (uℓ)ℓ∈[k]. We use this vector to perform an operation we call mirroring: we ‘flip’ all labels lying in the negative halfspace determined by r̂. Finally, we compute a weighted covariance matrix Q̂ over all Xi, where each point’s contribution is weighed by the mirrored labels: the eigenvectors of this matrix, appropriately rotated, yield the span U .\nThese operations are summarized in Algorithm 1. We discuss each of the main steps in more detail below: Pre-processing. (Lines 1–2) We split the dataset into two halves. Using the first half (i.e., all Xi with 1 ≤ i ≤ ⌊n2 ⌋), we construct estimates µ̂ ∈ Rd and Σ̂ ∈ Rd×d of the feature mean and covariance, respectively. Standard Gaussian (i.e., ‘whitened’) versions of features Xi can be constructed as Σ̂\n−1/2(Xi−µ̂). Mirroring. (Lines 3–4) We compute the vector:\nr̂ = 1 ⌊n/2⌋ ∑⌊n/2⌋ i=1 YiΣ̂ −1/2(Xi−µ̂) ∈ Rd. (5)\nWe refer to r̂ as the mirroring direction. In Section 4, we show that r̂ is tightly concentrated around its population (n = ∞) version:\nr ≡ E [ Σ−1(X − µ) · (∑k ℓ=1 pℓg(〈uℓ, X〉) )] , (6)\nwhere g(s) ≡ 2f(s) − 1, s ∈ R. Crucially, r lies in the interior of the convex cone spanned by the parameter profiles, i.e., r = ∑k ℓ=1 αℓuℓ, for some positive αℓ > 0, ℓ ∈ [k] (see Lemma 2 and Fig. 1(b)). Using this r̂, we ‘mirror’ the labels in the second part of the dataset:\nZi = Yi sgn〈r̂, Xi〉, for ⌊n/2⌋ < i ≤ n.\nIn words, Zi equals Yi for all i in the positive half-space defined by the mirroring direction; instead, all labels for points i in the negative half-space are flipped (i.e., Zi = −Yi). This is illustrated in Figure 1(c). Spectral Decomposition. (Lines 5–8) The mirrored labels are used to compute a weighted covariance matrix over whitened features as follows:\nQ̂ = 1\n⌈n2 ⌉\nn∑\ni=⌊n/2⌋+1 ZiΣ̂\n−1/2(Xi − µ̂)(Xi − µ̂)T Σ̂−1/2\nThe spectrum of Q̂ has a specific structure, that reveals the span U . In particular, as we will see in Section 4, Q̂ converges to a matrix Q that contains an eigenvalue with multiplicity n − k; crucially, the eigenvectors corresponding to the remaining k eigenvalues span the subspace U , subject to a rotation. As such, the final steps of the algorithm amount to discovering the eigenvalues that ‘stand out’ (i.e., are different from the eigenvalue with multiplicity n− k), and rotating the corresponding eigenvectors to obtain Û . More specifically, let (λℓ, wℓ)ℓ∈[d] be the eigenvalues and eigenvectors of Q̂. The algorithm computes the median of all eigenvalues, and identifies the k eigenvalues furthest from this median; these are the ‘outliers’. The corresponding k eigenvectors, multiplied by Σ̂−1/2, yield the subspace estimate Û .\nWe note that the algorithm does not require knowledge of the classifier response function f . While we assume knowledge of k, an eigenvalue/eigenvectors statistic (see, e.g., Zelnik-Manor and Perona [21]) can be used to estimate k as well."
    }, {
      "heading" : "3.2. Main Result",
      "text" : "Our main result states that SpectralMirror is a consistent estimator of the subspace spanned by u1, . . . , uℓ.\nTheorem 1. Denote by Û the output of SpectralMirror, and let P⊥r ≡ I − rrT /‖r‖2 be the projector orthogonal to r, given by (6). Then, if µ = 0, there exists ǫ0 > 0 such that, for all ǫ ∈ [0, ǫ0),\nPr(dP (P ⊥ r U, Û) > ǫ) ≤ C1 exp(−C2\nnǫ2\nd ).\nHere C1 is an absolute constant, and C2 > 0 depends on the distribution of Σ, f and u1, . . . , uℓ.\nIn other words, Û provides an accurate estimate of P⊥r U as soon as n is significantly larger than d. Note that this does not guarantee that Û spans the direction r ∈ U ; nevertheless, as shown below, the latter is accurately estimated by r̂ (see Lemma 1) and can be added to the span, if necessary. Moreover, our experiments suggest this is rarely the case in practice, as Û indeed includes the direction r (see Section 5).\nFor simplicity of exposition, our theorem statement refers to µ = 0. Nevertheless, our proof implies that, in fact, the theorem holds for ‘most’ µ ∈ Rd. Formally, it holds for generic µ: if we add an arbitrarily small random perturbation to µ, Theorem 1 holds with probability 1 w.r.t. this perturbation. We elaborate on this in Section 4, but leave a formal proof of this statement for an extended version of this paper."
    }, {
      "heading" : "4. Proof of Theorem 1",
      "text" : "We begin by characterizing the limit point of the mirroring direction r̂. Observe that E[Y | X = x] = ∑kℓ=1 pℓg(〈uℓ, x〉), where g(s) = 2f(s) − 1, for s ∈ R. Recall that we denote by r the population version of r̂—see Eq. (6).\nLemma 1. There exist an absolute constant C > 0 and c1, c ′ 1, c ′ 2 that depend on ‖X‖ψ2 such that:\nPr(‖r̂ − r‖2≥ǫ)≤C exp ( −min {c2nǫ2 d , ( c′1 √ nǫ− c′2 √ d )2}) .\nThe proof of Lemma 1 relies on a large deviation inequality for sub-Gaussian vectors, and is provided in Appendix A. Crucially, r lies in the interior of the convex cone spanned by the parameter profiles: Lemma 2. r = ∑k\nℓ=1 αℓuℓ for some αℓ > 0, ℓ ∈ [k]. Proof. Observe that\nr = ∑k\nℓ=1 pℓΣ −1 E[(X − µ)g(〈uℓ, X〉)].\nIt thus suffices to show that Σ−1E[(X − µ)g(〈u,X〉)] = αu, for some α > 0. Note that X ′ = 〈u,X〉 is normal with mean µ0 = uTµ and variance σ20 = uTΣu > 0. The monotonicity of f implies that it is a.e. differentiable, and so is g; moreover, g′ ≥ 0, wherever defined. This, and the fact that g is nonconstant, implies E[g′(X ′)] > 0. On the other hand, from Stein’s identity (4), E[g′(X ′)] = 1\nσ2 0 E[X ′g(X ′)] < ∞, as g is bounded. Hence, again from Stein’s identity (4):\nΣ−1E[(X − µ)g(〈u,X〉)] = Σ−1Cov(X, 〈u,X〉)E[g′(X ′)], where X ′ ∼ N (µ0, σ20) = Σ−1 · E[(X − µ)XTu] · E[g′(X ′)] = Σ−1 · Σu · E[g′(X ′)] = E[g′(X ′)] · u\nand the lemma follows.\nFor r and (αℓ)ℓ∈[k] as in Lemma 2, define\nz(x) = E[Y sgn(〈r,X〉) | X = x] = (∑k ℓ=1 pℓg(〈x, uℓ〉) ) · sgn (∑k ℓ=1 αℓ〈x, uℓ〉 ) .\nObserve that z(x) is the expectation of the mirrored label at a point x presuming that the mirroring direction is exactly r. Let Q ∈ Rd×d be the matrix:\nQ = E[z(X)Σ−1/2(X − µ)(X − µ)TΣ−1/2].\nThen Q̂ concentrates around Q, as stated below.\nLemma 3. Let ǫ0 ≡ min{α1, . . . , αk}σmin(U), where the αℓ > 0 are defined as per Lemma 2. Then\nPr(‖Q̂−Q‖2 > ǫ) ≤ C exp{−F (ǫ)},\nwhere F (ǫ) ≡ min { c1nǫ 2 d , ( c′1 √ nǫ− c′2 √ d )2} . Here C is an absolute constant, and c1, c ′ 1, c ′ 2 depend on the distribution of X.\nThe proof of Lemma 3 is also provided in Appendix C. We again rely on large deviation bounds for sub-gaussian random variables; nevertheless, our proof diverges from standard arguments because r̂, rather than r, is used as a mirroring direction. Additional care is needed to ensure that (a) when r̂ is close enough to r, its projection to U still lies in the interior of the convex cone spanned by the profiles, and (b) although r̂ may have a (vanishing) component outside the convex cone, the effect this has on Q̂ is negligible, for n large enough.\nAn immediate consequence of Lemma 2 is that r reveals a direction in the span U . The following lemma states that the eigenvectors of Q, subject to a rotation, yield the remaining k − 1 directions: Lemma 4. The rank of Q is at most k + 1. One eigenvalue, termed λ0, has multiplicity d − k. If µ = 0, the eigenvectors w1, . . . , wk corresponding to the remaining eigenvalues λ1, . . . , λk are such that\nP⊥r U = span(P ⊥ r Σ −1/2w1, . . . , P ⊥ r Σ −1/2wk),\nwhere P⊥r is the projection orthogonal to r.\nProof. Note that\nQ = E[z(X)Σ− 1 2X(X − µ)TΣ− 12 ] = E[z(Σ1/2W + µ)WWT ], where W ∼ N (0, I)\n= E [ k∑\nℓ=1\npℓg(〈Σ 1 2W+µ,uℓ〉) sgn(〈Σ 1 2W+µ,r〉)WWT ]\n= E [ k∑\nℓ=1\npℓg(〈W + µ̃, ũℓ〉) sgn(〈W + µ̃, r̃〉)WWT ]\nfor ũℓ ≡ Σ 1 2uℓ, r̃ ≡ Σ 1 2 r, and µ̃ ≡ Σ− 12µ. Hence Q =∑kℓ=1 pℓQℓ where\nQℓ = E[g(〈ũℓ,W + µ̃〉) sgn(〈r̃,W + µ̃〉)WWT ].\nBy a simple rotation invariance argument, Qℓ can be written as\nQℓ = aℓI + bℓ(ũℓr̃ T + r̃ũTℓ ) + cℓũℓũ T ℓ + dℓr̃r̃ T (7)\nfor some aℓ, bℓ, cℓ, dℓ ∈ R. Let a = ∑k ℓ=1 pℓaℓ. Then\nQ− aI = k∑\nℓ=1\npℓdℓr̃r̃ T + r̃(\nk∑\nℓ=1\npℓbℓũℓ) T+\n+ (\nk∑\nℓ=1\npℓbℓũℓ)r̃ T +\nk∑\nℓ=1\npℓcℓũℓũ T ℓ .\nLet P⊥r̃ be the projector orthogonal to r̃, i.e., P ⊥ r̃ = I − r̃r̃ T\n‖r̃‖2 2 . Let vℓ ≡ P⊥r̃ ũℓ. By Lemma 2, r̃ = ∑k ℓ=1 αℓũℓ, where αℓ > 0; this, and the linear independence of ũℓ, implies that vℓ 6= 0, for all ℓ ∈ [k]. Define\nR ≡ P⊥r̃ (Q− aI)P⊥r̃ = ∑k ℓ=1 γℓvℓv T ℓ ,\nwhere γℓ = pℓcℓ, ℓ ∈ [k]. We will show below that, if µ = 0, then γℓ 6= 0 for all ℓ ∈ [k]. This implies that rank(R) = k − 1. Indeed,\nR = P⊥r̃ ∑ γℓũℓũ T ℓ P ⊥ r̃ = P ⊥ r̃ R̃P ⊥ r̃ ,\nwhere R̃ has rank k by the linear independence of profiles. As P⊥ is a projector orthogonal to a 1-dimensional space,R has rank at least k−1. On the other hand, range(R) ⊆ Ũ , for Ũ = span(ũ1, . . . , ũℓ), and r̃TRr̃ = 0 where r̃ ∈ Ũ \\ {0}), so rank(R) = k− 1. The latter also implies that range(R) = P⊥r̃ Ũ , as range(R)⊥r̃, range(R) ⊆ Ũ , and dim(range(R)) = k − 1.\nThe above imply that Q has one eigenvalue of multiplicity n− k, namely a. Moreover, the eigenvectors w1, . . . , wk corresponding to the remaining eigenvalues (or, the non-zero eigenvalues of Q− aI) are such that\nP⊥r̃ Σ 1/2U = P⊥r̃ span(w1, . . . , wk).\nThe lemma thus follows by multiplying both sides of the above equality with P⊥r Σ −1/2, and using the fact that P⊥r Σ −1/2P⊥r̃ = P ⊥ r Σ\n−1/2. It thus remains to show that γℓ 6= 0, for all ℓ ∈ [k]. Note that,\ncℓ〈ũℓ, vℓ〉2 (7) = 〈vℓ, (Qℓ − aℓI)vℓ〉 = (8)\nCov(g(〈ũℓ,W + µ̃〉) sgn(〈r̃,W + µ̃〉); 〈W, vℓ〉2) ≡ c̃ℓ\nIt thus suffices to show that c̃ℓ < 0. Lemma 2 implies that ũℓ = vℓ+ cr̃ for some c > 0, so, for µ = 0,\nc̃ℓ = Cov[g(〈vℓ,W 〉+ c〈r̃,W 〉) sgn(r̃,W ); 〈W, vℓ〉2].\nLet X = 〈vℓ,W 〉 and Y = 〈r̃,W 〉; then, X and Y are independent Gaussians with zero mean. Moreover, c̃ℓ can be written as c̃ℓ = Cov[F (X);X\n2] where F (x) = EY [g(x + cY ) sgn(Y )]. The symmetry assumption (3) implies that g is antisymmetric, i.e., g(−x) = −g(x). This implies that F (−x) = EY [g(−x + cY ) sgn(Y )] Y ′≡−Y = EY ′ [g(−x − cY ′) sgn(−Y ′)] = F (x), i.e., F is symmetric. Further,\nF ′(x) = Ey[g ′(x+ cY ) sgn(Y )]\n=\n∫ ∞\n0\n(g′(x+ cy)− g′(x− cy))e −y2/2σY\nσ √ 2π dy\nThe strict concavity of g in [0,∞) implies that g′ is decreasing in [0,+∞), and the antisymmetry of g implies that g′ is symmetric. Take x > 0: if x > cy ≥ 0, g′(x+cy) > g′(x−cy), while if x ≤ cy, then g′(x−cy) = g′(cy−x) > g′(cy+x), so F ′(x) is negative for x > 0. By the symmetry of F , F ′(x) is positive for x < 0. As such, F (x) = G(x2) for some strictly decreasing G, and c̃ℓ = Cov(G(Z);Z) for Z = X2; hence, c̃ℓ < 0.\nDenote by λ0 the eigenvalue of multiplicity d − k in Lemma 4. Let ∆ = minℓ∈[k] |λ0 − λℓ| be the gap between λ0 and the remaining eigenvalues. Then, Lemmas 3 and 4 imply the following lemma, from which Theorem 1 readily follows:\nLemma 5. Let Û be our estimate for U . If λ1, . . . , λk are separated from λ0 by at least ∆, then for ǫ ≤ min(ǫ0/∆, 14 ), we have\nPr(dP (U, Û) > ǫ) ≤ C exp ( − F (∆ǫ) ) ,\nwhere ǫ0, F are defined as per Lemma 3.\nProof. If we ensure ‖Q̂−Q‖ ≤ ∆/4, then by Weyl’s theorem [8], d−k eigenvalues of Q̂ are contained in [λk+1 −∆/4, λk+1 +∆/4], and the remaining eigenvalues\nare outside this set, and will be detected by SpectralMirror. Moreover, by the Davis-Kahan sin(θ) theorem,\ndp(range(Q), range(Q̂)) ≤ 1\n∆− ‖Q̂−Q‖2 ‖Q̂−Q‖2\n= 1\n∆ ‖Q̂−Q‖2\n− 1 .\nThus the event dp(U, Û) ≤ ǫ is implied by ‖Q̂ − Q‖2 ≤ ∆ǫ1+ǫ ≤ ∆ǫ. Moreover, this implies that sufficient condition for ‖Q̂−Q‖2 ≤ ∆/4 (which is required for SpectralMirror to detect the correct eigenvalues) is that ǫ ≤ 14 . The lemma thus follows from Lemma 3."
    }, {
      "heading" : "4.1. Extensions",
      "text" : "Though our proof above is for µ = 0, this assumption is used only in establishing that the parameters c̃ℓ in (8) are non-zero and, hence, all directions in P ⊥ r U are present in the range of Q. In general, these parameters can be computed in closed form for any µ through (8), and will be non-zero for generic µ.\nMoreover, the Gaussianity of X is used to establish that the ‘whitened’ features W are uncorrelated, which in turn yields Eq. (7). We again believe that the Theorem can be extended to more general distributions, provided that this fact—i.e., that features can be decorrelated under the transform Σ− 1\n2—still holds."
    }, {
      "heading" : "5. Experiments",
      "text" : "We conduct computational experiments to validate the performance of our procedure on subspace estimation, prediction, and clustering. We generate synthetic\ndata as using profiles ui ∼ N (0, I), i = 1, 2 and mixture weights pℓ sampled uniformly at random from the k-dimensional simplex. Features are also Gaussian: Xi ∼ N (0, I), i = 1, . . . , n; labels generated by the ℓ-th classifier are given by yi = sign(u T ℓ Xi), i = 1, . . . , n. We use k = 2 in our experiments. Convergence. We study first how well SpectralMirror estimates the span U . Figure 2(a) shows the convergence of Û to U in terms of (the sin of) the largest principal angle between the subspaces versus the sample size n. We also plot the convergence versus the effective sample size n/d (Figure 2(a)). The curves for different values of d align in Figure 2, indicating that the upper bound in Thm. 1 correctly predicts the sample complexity as n ≈ Θ(d). Though not guaranteed by Theorem 1, in all experiments r was indeed spanned by Û , so the addition of r̂ to Û was not necessary. Prediction through K-NN. Next, we use the estimated subspace to aid in the prediction of expected labels. Given a new feature vector X , we use the average label of its K nearest neighbors (K-NN) in the training set to predict its expected label. We do this for two settings: once over the raw data (the ‘ambient’ space), and once over data for which the features X are first projected to Û , the estimated span. Note that projected features are of dimension 2. For each n, we repeat this procedure 25 times with K = √ n and K = logn. We record the average root mean squared error between the predicted and true labels over the 25 runs. Figures 3(a) and 3(b) show that, despite the error in the estimated subspace, using K-NN on the subspace outperforms K-NN on the ambient space. Prediction and Clustering through EM. We use the estimated subspace to predict the label of individual classifiers. We use the Expectation-Maximization (EM) to fit the individual profiles both over the training set, as well as on the dataset projected to the estimated subspace Û . When give a new feature vector X , we simply set sgn(uTi X) to be the predicted label of classifier i. We conducted two experiments in this setting: (a) initialize EM close to the true profiles ui, i ∈ [k], and (b) randomly initialize EM and choose the best set of profiles from 30 runs. For each n we run EM 10 times and evaluate fitted profiles by using\nthem to predict the label of new features. We record the average normalized 0-1 loss over 10 runs.\nThe first set of experiments, illustrated in Figure 4(a), measures the statistical efficiency of EM over the estimated subspace versus EM over the ambient space. The second set of experiments, illustrated in Figure 4(b), aims to capture the additional improvement due to the reduction in the number of local minima in the reduced space. In both cases we see that constraining the estimated profiles to lie in the estimated subspace improves the statistical efficiency of EM; in the more realistic random start experiments, enforcing the subspace constraint also improves the performance of EM by reducing the number of local minima, thereby allowing EM to reach better local minima.\nFinally, we use the fitted profiles ui to identify the classifier generating a label given the features and the label. To do this, once the profiles ui have been detected by EM, we use a logistic model margin condition to identify the classifier that generated a label, given the label and its features. Figure 4(c) shows the result for EM initialized at a random point, after choosing the best set of profiles from out of 30 runs. We evaluate the performance of this clustering procedure using the normalized 0-1 loss. Again, constraining the estimated profiles to the estimated subspace significantly improves the performance on this clustering task."
    }, {
      "heading" : "6. Conclusions",
      "text" : "We have proposed SpectralMirror, a method for discovering the span of a mixture of linear classifiers. Our method relies on a non-linear transform of the labels, which we refer to as ‘mirroring’. Moreover, we have provided consistency guarantees and non-asymptotic bounds, that also imply the near optimal statistical efficiency of the method. Finally, we have shown that, despite the fact that SpectralMirror discovers the span only approximately, this is sufficient to allow for a significant improvement in both prediction and clustering, when the features are projected to the estimated span.\nWe have already discussed several technical issues that remain open, and that we believe are amenable to further analysis. These include extending Theorem 1 to generic µ, amending the Gaussianity assumption, and applying our bounds to other pHd-inspired methods.\nAn additional research topic is to further improve the computational complexity of the estimation of the eigenvectors of the ‘mirrored’ matrix Q̂. This is of greatest interest in cases where the covariance Σ and mean µ are a priori known. This would be the case when, e.g., the method is applied repeatedly and, although the features X are sampled from the same distribution each time, labels Y are generated from a different mixture of classifiers. In this case, SpectralMirror lacks the preprocessing step, that requires estimating Σ and is thus computationally intensive; the remaining operations amount to discovering the spectrum of Q̂, an operation that can be performed more efficiently. For example, we can use a regularized M-estimator to exploit the fact that\nΣ−1/2Q̂Σ−1/2 should be the sum of a multiple of the identity and a low rank matrix—see, e.g., [16]."
    }, {
      "heading" : "Appendix A: A Large-Deviation Lemma",
      "text" : "Lemma 6. Let X ∈ Rd be a sub-Gaussian random vector, i.e. 〈a,X〉 is subGaussian for any a ∈ Rd. Then there exist universal constants c1, c2 such that\nPr(‖X‖2 ≥ t) ≤ c1 exp ( −min { c2(t\n2 − d ‖Σ‖2) 4d ‖X‖2ψ2 , (t2 − d ‖Σ‖2)2 64c2 ‖X‖4ψ2\n}) .\nProof. By the (exponential) Markov inequality, we have\nPr(‖X‖2 ≥ t) = Pr ( exp(λ ‖X‖22) ≥ exp(λt2) ) ≤ E[exp(λ ‖X‖ 2 2)]\nexp(λt2) . (9)\nLet Z be uniformly distributed on the unit sphere Sd−1. Then √ dZ is isotropic so dE[〈Z, a〉2] = ‖a‖22 for any a. This implies\nE[exp(λ ‖X‖22)] = EX [exp(λdEZ [〈Z,X〉2])] ≤ EX [EZ [exp(λd〈Z,X〉2)]].\nWe interchange the order of expectation to obtain\nE[exp(λ ‖X‖22)] ≤ EZ [EX [exp(λd〈X,Z〉2)]] ≤ sup {EX [exp(λd〈X,Z〉2)] | z ∈ Sd−1}.\n〈X, z〉 is sub-Gaussian (for a fixed z) so 〈X, z〉2 is (noncentered) sub-exponential.\nIf λd < c/ ∥∥〈X, z〉2 −E[〈X, z〉2] ∥∥ ψ1 , then\nE[exp(λd〈X, z〉2)] ≤ exp(λdE[〈X, z〉2])E[exp(λd(〈X, z〉2 −E[〈X, z〉2]))] ≤ exp(λdE[〈X, z〉2] + cd2λ2\n∥∥〈X, z〉2 −E[〈X, z〉2] ∥∥2 ψ1 )\n≤ exp(λdE[〈X, z〉2] + 4cd2λ2 ∥∥〈X, z〉2 ∥∥2 ψ1 ) ≤ exp(λdE[〈X, z〉2] + 16cd2λ2 ‖〈X, z〉‖4ψ2). (10)\nWe substitute this bound into (9) to obtain\nPr(‖X‖2 ≥ t) ≤ exp(16cd2λ2 ‖X‖ 4 ψ2 + λ(d ‖Σ‖2 − t2)), (11)\nwhere Σ is the covariance matrix of X . We optimize over λ to obtain\nPr(‖X‖2 ≥ t) ≤ exp ( − (t\n2 − d ‖Σ‖2)2 64cd2 ‖X‖4ψ2\n) .\nIf the optimum lies outside the region where (10) holds, we can set\nλ = c 4d ‖X‖2ψ2 ≤ c\nd ‖〈X, z〉2 −E[〈X, z〉2]‖ψ1 in (11) to obtain:\nPr(‖X‖2 ≥ t) ≤ exp ( c3 +\nc(d ‖Σ‖2 − t2) 4d ‖X‖2ψ2\n) .\nWe combine these two bounds to obtain\nPr(‖X‖2 ≥ t) ≤ c1 exp ( −min { c2(t\n2 − d ‖Σ‖2) 4d ‖X‖2ψ2 , (t2 − d ‖Σ‖2)2 64c2 ‖X‖4ψ2\n}) .\nNote that the t2 bound always holds. However, for small t, the t4 term yields a tighter bound."
    }, {
      "heading" : "Appendix B: Proof of Lemma 1 (Weak Convergence of r̂)",
      "text" : "Proof. We expand ‖r̂n − r‖2 to obtain\n‖r̂n − r‖2 = ∥∥∥ 1 n n∑\ni=1\nΣ̂−1Yi(Xi − µ̂)− Σ−1 E[s(X)(X − µ)] ∥∥∥ 2\n(12)\n≤ ∥∥Σ̂−1 ∥∥ 2 ∥∥∥ 1 n n∑\ni=1\nYi(Xi − µ̂)−E[s(X)(X − µ)] ∥∥∥ 2\n(13)\n+ ‖E[s(X)(X − µ)]‖2‖Σ̂−1 − Σ−1‖2 + (oP (1))2. (14)\nThe higher order terms generically look like\nPr(‖X −E[X ]‖2 ‖Y −E[Y ]‖2 > ǫ). (15)\nWe apply the union bound to deduce\nPr(‖X −E[X ]‖2 ‖Y −E[Y ]‖2 > ǫ) ≤ Pr(‖X −E[X ]‖2 > √ ǫ) +Pr(‖X −E[X ]‖2 > √ ǫ).\nFor any ǫ < 1, √ ǫ > ǫ and we have\nPr(‖X −E[X ]‖2 > √ ǫ) ≤ Pr(‖X −E[X ]‖2 > ǫ).\nSince terms of the form Pr(‖X −E[X ]‖2 > ǫ) appear in our tail bounds, we can handle terms like (15) with a constant factor (say 2). Our bounds involve multiplicative constants anyways, so we omit these terms to simplify our derivation.\nWe expand the first term to obtain\n∥∥∥ 1 n n∑\ni=1\nYi(Xi − µ̂)−E[Y (X − µ)] ∥∥∥ 2\n≤ |E[s(X)]| ‖µ̂− µ‖2 + ‖µ‖2 ∣∣∣ 1 n n∑\ni=1\nYi −E[s(X)] ∣∣∣ 2\n+ ∥∥∥ 1 n n∑\ni=1\nYi(Xi − µ)−E[s(X)(X − µ)] ∥∥∥ 2\n+ (oP (1)) 2.\nµ̂− µ is sub-Gaussian with sub-Gaussian norm ‖X‖ψ2√ n , so there exist universal c1 and c2 s.t.\nPr(‖µ̂− µ‖2 > t) ≤ c1 exp ( −c2n(t\n2 − d ‖Σ‖2) 4d ‖X‖2ψ2\n) .\nY is bounded between 1 and -1, so\n1. Chernoff’s inequality yields\nPr (∣∣∣ 1 n n∑\ni=1\nYi −E[s(X)] ∣∣∣ > t ) ≤ 2 exp(−nt2/2).\n2. Yi(Xi − µ) are also sub-Gaussian. Thus there exist universal c1 and c2 such that\nPr (∥∥∥ 1 n n∑\ni=1\nYi(Xi − µ)−E[s(X)(X − µ)] ∥∥∥ 2 > t ) ≤ c1 exp\n( −c2n(t\n2 − d ‖Σ‖2) 4d ‖X‖2ψ2\n) .\nWe expand the second term in (14) to obtain\n‖Σ̂−1 − Σ−1‖ = ‖Σ−1/2‖‖Σ1/2Σ̂−1Σ1/2 − I‖‖Σ−1/2‖.\nWe expand the middle term to obtain\n‖Σ−1/2Σ̂Σ−1/2 − I‖\n≤ ∥∥∥ ( 1 n n∑\ni=1\nΣ−1/2(Xi − µ)(Xi − µ)TΣ−1/2 )−1 − I ∥∥∥ 2\n+ 2 ‖µ‖2 ‖µ̂− µ‖2 + (oP (1))2\nWe use Theorem 5.39 in [20] to bound the first term:\nPr (∥∥∥ 1 n n∑\ni=1\nWiW T i − I ∥∥∥ 2 > t ) ≤ 2 exp(−c′1( √ nt− c′2 √ d)2),\nwhere c′1, c ′ 2 depend on the sub-Gaussian norm ofW . We substitute these bounds into our expression for ‖r̂ − r‖2 to deduce\nPr(‖r̂ − r‖2 ≥ ǫ) ≤ C exp ( −min { c1nǫ 2 d , ( c′1 √ nǫ − c′2 √ d )2}) ,\nwhere C is an absolute constant and c1, c ′ 1, c ′ 2 depend on the sub-Gaussian norm of X ."
    }, {
      "heading" : "Appendix C: Proof of Lemma 3 (Weak Convergence of Q̂)",
      "text" : "Let r̃ denote the projection of r onto span{u, v}. Lemma 7. If ‖r̂−r‖2 ≤ ǫ0 = min {α1, α2} sin(θ), then r̃ also lies in the positive quadrant.\nProof. r lies in interior of the conic hull of {u1, . . . , uk}, so we can express r as∑k i=1 αiui, where ri > 0. If r̃ also lies in the conic hull, then r̃ = ∑k i=1 βiui for some βi > 0.\nǫ0 = ‖r̃ − r‖2 = ∥∥∥∥∥ k∑\ni=1 (αi − βi)ui ∥∥∥∥∥ 2 = √ (α− β)UUT (α − β)\n≥ ‖α− β‖2 σmin(U) ≥ ‖α− β‖∞ σmin(U).\nTo ensure β is component-wise positive, we must have ‖α− β‖∞ < min{α1, . . . , αk}. A sufficient condition is ǫ0 ≤ min{α1, . . . , αk}σmin(U).\nWe are now ready to prove Lemma 3. We expand ‖Q̂n − Q‖2 (and neglect\nhigher order terms) to obtain\n‖Q̂n −Q‖2 ≤ ∥∥∥ 1 n n∑\ni=1\nZiΣ −1/2(Xi − µ)(Xi − µ)TΣ−1/2 −Q ∥∥∥ 2\n+ 2‖Σ̂−1/2 − Σ−1/2‖2 E [ ‖(X − µ)(X − µ)Σ−1/2‖2 ] + 2‖Σ−1/2‖2‖µ̂− µ‖2 E [ ‖(X − µ)Σ−1/2‖2 ] + (oP (1)) 2.\nThe second and third terms can be bounded using the same bounds used in the analysis of how fast r̂ converges to r. Thus we focus on how fast\nn∑\ni=1\nZiΣ −1/2(Xi − µ)(Xi − µ)TΣ−1/2\ncoverges to Q. First, we note that\nPr (∥∥∥ 1 n n∑\ni=1\nZiWiW T i −Q ∥∥∥ 2 > t )\n≤ Pr (∥∥∥ 1\nn\nn∑\ni=1\nZiWiW T i −Q ∥∥∥ 2 > t | r̂ ∈ Bǫ0(r) )\n+Pr(r̂ ∈ Bǫ0(r)). Let Z̃i denote the “corrected” version of the Zi’s, i.e. the Zi’s we obtain if we use the projection of r̂ onto the span{u, v} to flip the labels, and Wi denote Σ−1/2(Xi − µ). We have\n∥∥∥ 1 n n∑\ni=1\nZiWiW T i −Q ∥∥∥ 2\n(16)\n≤ ∥∥∥ 1 n n∑\ni=1\nZ̃iWiW T i −Q ∥∥∥ 2 + ∥∥∥ 1 n n∑\ni=1\n(Zi − Z̃i)WiWTi ∥∥∥ 2 . (17)\nThe probability the first term is large is bounded by\nPr (∥∥∥ 1 n n∑\ni=1\nZ̃iWiW T i −Q ∥∥∥ 2 > t | r̂ ∈ Bǫ0(r) )\n≤ sup r̂∈Bǫ0 (r) Pr\n(∥∥∥ 1 n n∑\ni=1\nZ̃iWiW T i −Q ∥∥∥ 2 > t | r̂ ∈ Bǫ0(r) )\nThe Zi’s are independent of Wi’s because the Zi’s were computed using r̂ that was in turn computed independently of the Xi’s. Thus this is a sum of i.i.d. r.v. and we can bound it by\nsup r̂∈Bǫr (r) Pr\n(∥∥∥ 1 n n∑\ni=1\nZ̃iWiW T i −Q ∥∥∥ 2 > t | r̂ ∈ Bǫ0(r) )\n≤ 2 exp(−c1( √ nt− c2 √ d)2),\nwhere c1, c2 depend on the sub-Gaussian norm of Z̃W . This is a consequence of Remark 5.40 in [20].\nWe now focus on bounding the second term in (17). Let Wi denote the spherically symmetric (whitened) version of X . We restrict ourselves to the 3d subspace spanned by u, v, r̂. Let Cr̂ (for cone) be the “bad” region, i.e. the region where Z 6= Z̃. We have\n∥∥∥ 1 n n∑\ni=1\n(Zi − Z̃i)WiWTi ∥∥∥ 2 = ∥∥∥ 2 n n∑\ni=1\n1Cr̂(Wi)WiW T i ∥∥∥ 2 .\n1Cr̂ is bounded, hence 1Cr̂(Wi)Wi is sub-Gaussian and\nPr (∥∥∥ 2 n n∑\ni=1\n1Cr̂(Wi)WiW T i − 2E[1Cr̂(W )WWT ] ∥∥∥ 2 > t | r̂ ∈ Bǫ0(r) )\n≤ 2 exp(−c1( √ nt− c2 √ d)2),\nwhere c1, c2 depend on the sub-Gaussian norm of X . It remains to bound∥∥E[1Cr̂(W )WWT ] ∥∥ 2 .\nWe use Jensen’s inequality to obtain ∥∥E[1Cr̂(W )WWT ] ∥∥ 2 ≤ E[1Cr̂(W ) ∥∥WWT ∥∥ 2 ]\n≤ CPr(W ∈ Cr̂), where the constant C depends on the number of mixture components. Finally, we bound Pr(W ∈ Cr̂).\nThe distribution of Wi is spherically symmetric so the probability that Wi ∈ Cr̂ is proportional to the surface area of the set\nS = {w ∈ Sk+1 | r̂Tw ≤ 0, uT1 w, . . . , uTkw ≥ 0}. This set is contained in the set\nS̄ = {w ∈ Sk+1 | ‖w − u1‖2 ≤ ‖r̂ − r‖2} This set is nothing but the spherical cap of radius ‖r̂ − r‖2 in k+1 dimensions. An upper bound is\nγ(S̄) ≤ exp(−(k + 1) cos(2− r 2\n2ab )2),\nwhere γ denotes the uniform measure on the unit sphere. We substitute these bounds into our expression for the second term to obtain\nE[1Cr̂(W )WW T ] ≤ C arccos ( 2− ǫ2r\n2\n) (1− cos∠(u, v))(1 + cos∠(u, v)+\ncos2 ∠(u, v))\n≤ C arccos ( 1− ǫ 2 r\n2\n) .\nWe combines these bounds to deduce\nPr(‖Q̂−Q‖2 > C arccos ( 1− ǫ 2 r\n2\n) + ǫ | r̂ ∈ Bǫ0(r))\n≤ C exp ( −min { c1nǫ 2\nd , ( c′1 √ nǫ − c′2\n√ d )2 }) .\nFinally, we have\nPr(‖Q̂−Q‖2 > ǫ | r̂ ∈ Bǫ0(r)) ≤ Pr(‖Q̂−Q‖2 > ǫ | ‖r̂ − r‖2 ≤ √ 1− cos(ǫ/2), r̂ ∈ Bǫ0(r))\n+Pr(‖r̂ − r‖2 ≤ √ 1− cos(ǫ/2)).\nLet\nf(ǫ) := min\n{ c1nǫ 2\nd , ( c′1 √ nǫ− c′2\n√ d )2 } .\nThe first term is bounded by C exp (−f(ǫ/2)) and the second term is bounded by C exp ( −f( √ 1− cos(ǫ/2)) ) . We deduce\nPr(‖Q̂−Q‖2 > ǫ | ‖r̂ − r‖2 ≤ ǫ0) ≤ C(exp(−f( √ 1− cos(ǫ/2))) + exp(−f(ǫ/2)))\n≤ C exp(−f(min{ √ 1− cos(ǫ/2), ǫ/2}))."
    }, {
      "heading" : "Appendix D: Principal Hessian Directions",
      "text" : "In this section, we apply the principal Hessian directions (pHd) [11] method to our setting, and demontrate its failure to discover the space spanned by parameter profile when µ = 0. Recall that pHd considers a setting in which features Xi ∈ Rd are i.i.d. and normally distributed with mean µ and covariance Σ, while labels Yi ∈ R lie, in expectation, in a k-dimensional manifold. In particular, some smooth h : Rk → R, k ≪ d:\nE[Y | X = x] = h(〈u1, x〉, . . . , 〈uk, x〉)\nwhere uℓ ∈ Rd, ℓ ∈ [k]. The method effectively creates an estimate\nĤ = n−1 n∑\ni=1\nYi Σ − 1 2XiX T i Σ − 1 2 ∈ Rd×d\nof the Hessian\nH = E[∇2xh(〈u1, X〉, . . . , 〈uk, X〉)] = UTE[∇2vh(〈u1, X〉, . . . , 〈uk, X〉)]U,\n(18)\nwhere ∇2vh is the Hessian of the mapping v 7→ h(v), for v ∈ Rk, and U the matrix of profiles. As in our case, the method discovers span(u1, . . . , uk) from the eigenvectors of the eigenvalues that “stand out”, after appropriate rotation in terms of Σ.\nUnfortunately, in the case of linear classifiers,\nh(v) =\nk∑\nℓ=1\npℓg(vℓ)\nfor g(s) = 2f(s) − 1 is anti-symmetric. As a result, ∇2vh is a diagonal matrix whose ℓ-th entry in the diagonal is g′′(vℓ). Since g is anti-symmetric, so is g′′. Hence, if µ = 0 we have that E[g′′(〈uℓ, X〉)] = 0; hence, the Hessian H given by (18) will in fact be zero, and the method will fail to discover any signal pertaining to span(U).\nThis calls for an application of the pHd method to a transform of the labels Y . This ought to be non-linear, as an affine transform would preserve the above property. Moreover, given that these labels are binary, polynomial transforms do not add any additional signal to Y , and are therefore not much help in accomplishing this task. In contrast, the ‘mirrorring’ approach that we propose provides a means of transforming the labels so that their expectation indeed carries sufficient information to extract the span U , as evidenced by Theorem 1."
    } ],
    "references" : [ {
      "title" : "Tensor decompositions for learning latent variable models",
      "author" : [ "Anima Anandkumar", "Rong Ge", "Daniel Hsu", "Sham M Kakade", "Matus Telgarsky" ],
      "venue" : "arXiv preprint arXiv:1210.7559,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2012
    }, {
      "title" : "Learning mixtures of arbitrary gaussians",
      "author" : [ "Sanjeev Arora", "Ravi Kannan" ],
      "venue" : "In Proceedings of the thirty-third annual ACM symposium on Theory of computing,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2001
    }, {
      "title" : "Latent variable models and factor analysis: A unified approach, volume 899",
      "author" : [ "David J Bartholomew", "Martin Knott", "Irini Moustaki" ],
      "venue" : "Wiley. com,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2011
    }, {
      "title" : "Latent variable models. In Learning in graphical models, pages 371–403",
      "author" : [ "Christopher M Bishop" ],
      "venue" : null,
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 1998
    }, {
      "title" : "Spectral experts for estimating mixtures of linear regressions",
      "author" : [ "Arun Tejasvi Chaganty", "Percy Liang" ],
      "venue" : "In ICML,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2013
    }, {
      "title" : "Principal hessian directions revisited",
      "author" : [ "R Dennis Cook" ],
      "venue" : "Journal of the American Statistical Association,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 1998
    }, {
      "title" : "Maximum likelihood from incomplete data via the em algorithm",
      "author" : [ "Arthur P Dempster", "Nan M Laird", "Donald B Rubin" ],
      "venue" : "Journal of the Royal Statistical Society. Series B (Methodological),",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 1977
    }, {
      "title" : "A spectral algorithm for learning hidden markov models",
      "author" : [ "Daniel Hsu", "Sham M Kakade", "Tong Zhang" ],
      "venue" : "Journal of Computer and System Sciences,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2012
    }, {
      "title" : "Hierarchical mixtures of experts and the em algorithm",
      "author" : [ "Michael I Jordan", "Robert A Jacobs" ],
      "venue" : "Neural computation,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 1994
    }, {
      "title" : "On principal hessian directions for data visualization and dimension reduction: another application of stein’s lemma",
      "author" : [ "Ker-Chau Li" ],
      "venue" : "Journal of the American Statistical Association,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 1992
    }, {
      "title" : "An end-to-end discriminative approach to machine translation",
      "author" : [ "Percy Liang", "Alexandre Bouchard-Côté", "Dan Klein", "Ben Taskar" ],
      "venue" : "In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2006
    }, {
      "title" : "Siegel’s formula via stein’s identities",
      "author" : [ "Jun S Liu" ],
      "venue" : "Statistics  Probability Letters,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 1994
    }, {
      "title" : "Finite mixture models",
      "author" : [ "Geoffrey McLachlan", "David Peel" ],
      "venue" : "Wiley. com,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2004
    }, {
      "title" : "Settling the polynomial learnability of mixtures of gaussians",
      "author" : [ "Ankur Moitra", "Gregory Valiant" ],
      "venue" : "In Foundations of Computer Science (FOCS),",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2010
    }, {
      "title" : "A unified framework for high-dimensional analysis of m-estimators with decomposable regularizers",
      "author" : [ "Sahand N Negahban", "Pradeep Ravikumar", "Martin J Wainwright", "Bin Yu" ],
      "venue" : "Statistical Science,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2012
    }, {
      "title" : "Conditional random fields for object recognition",
      "author" : [ "Ariadna Quattoni", "Michael Collins", "Trevor Darrell" ],
      "venue" : "In Advances in neural information processing systems,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2004
    }, {
      "title" : "Estimation of the mean of a multivariate normal distribution",
      "author" : [ "Charles M Stein" ],
      "venue" : "In Prague Symposium on Asymptotic Statistics,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 1973
    }, {
      "title" : "Introduction to the non-asymptotic analysis of random matrices",
      "author" : [ "R. Vershynin" ],
      "venue" : "arXiv preprint arXiv:1011.3027,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2010
    } ],
    "referenceMentions" : [ {
      "referenceID" : 6,
      "context" : "Since Pearson’s seminal contribution [17], and most notably after the introduction of the EM algorithm [7], mixture models and latent variable models have played a central role in statistics and machine learning, with numerous applications—see, e.",
      "startOffset" : 103,
      "endOffset" : 106
    }, {
      "referenceID" : 12,
      "context" : ", McLachlan & Peel [14], Bishop [4], and Bartholomew et al.",
      "startOffset" : 19,
      "endOffset" : 23
    }, {
      "referenceID" : 3,
      "context" : ", McLachlan & Peel [14], Bishop [4], and Bartholomew et al.",
      "startOffset" : 32,
      "endOffset" : 35
    }, {
      "referenceID" : 2,
      "context" : "[3].",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 1,
      "context" : "Even the idealized problem of learning mixtures of Gaussians has motivated a copious theoretical literature [2, 15].",
      "startOffset" : 108,
      "endOffset" : 115
    }, {
      "referenceID" : 13,
      "context" : "Even the idealized problem of learning mixtures of Gaussians has motivated a copious theoretical literature [2, 15].",
      "startOffset" : 108,
      "endOffset" : 115
    }, {
      "referenceID" : 3,
      "context" : "Models of this type have been intensely studied in the neural network literature since the early nineties [4, 10].",
      "startOffset" : 106,
      "endOffset" : 113
    }, {
      "referenceID" : 8,
      "context" : "Models of this type have been intensely studied in the neural network literature since the early nineties [4, 10].",
      "startOffset" : 106,
      "endOffset" : 113
    }, {
      "referenceID" : 15,
      "context" : "They have also found numerous applications ranging from object recognition [18] to machine translation [12].",
      "startOffset" : 75,
      "endOffset" : 79
    }, {
      "referenceID" : 10,
      "context" : "They have also found numerous applications ranging from object recognition [18] to machine translation [12].",
      "startOffset" : 103,
      "endOffset" : 107
    }, {
      "referenceID" : 4,
      "context" : "Recently, Chaganty and Liang [5] considered mixtures of linear regressions, whereby the relation between labels and feature vectors is linear within each 1",
      "startOffset" : 29,
      "endOffset" : 32
    }, {
      "referenceID" : 7,
      "context" : "[9] and Anandkumar et al.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 0,
      "context" : "[1], these authors propose an algorithm for fitting mixtures of linear regressions with provable guarantees.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 4,
      "context" : "While the work of Chaganty and Liang [5] is a significant step forward, it leaves several open problems: Statistical efficiency.",
      "startOffset" : 37,
      "endOffset" : 40
    }, {
      "referenceID" : 4,
      "context" : "Then, the mathematical guarantees of Chaganty & Liang [5] require a sample size n ≫ d.",
      "startOffset" : 54,
      "endOffset" : 57
    }, {
      "referenceID" : 4,
      "context" : "As noted in [5], this scaling is an intrinsic drawback of the tensor approach which requires working in a higher-dimensional space (tensor space) than the space in which data naturally live.",
      "startOffset" : 12,
      "endOffset" : 15
    }, {
      "referenceID" : 8,
      "context" : ", Jordan & Jacobs [10], Bishop [4], Quattoni et al.",
      "startOffset" : 18,
      "endOffset" : 22
    }, {
      "referenceID" : 3,
      "context" : ", Jordan & Jacobs [10], Bishop [4], Quattoni et al.",
      "startOffset" : 31,
      "endOffset" : 34
    }, {
      "referenceID" : 15,
      "context" : "[18], Liang et al.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 10,
      "context" : "[12].",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 4,
      "context" : "The method of [5] requires solving a regularized linear regression in d dimensions and factorizing a tensor of third order in d dimensions.",
      "startOffset" : 14,
      "endOffset" : 17
    }, {
      "referenceID" : 1,
      "context" : "On one hand, as for the problem of learning mixtures of Gaussians [2, 15], we believe that useful insights can be gained by studying this simple setting.",
      "startOffset" : 66,
      "endOffset" : 73
    }, {
      "referenceID" : 13,
      "context" : "On one hand, as for the problem of learning mixtures of Gaussians [2, 15], we believe that useful insights can be gained by studying this simple setting.",
      "startOffset" : 66,
      "endOffset" : 73
    }, {
      "referenceID" : 9,
      "context" : "Our approach is related to the principal Hessian directions (pHd) method proposed by Li [11] and further developed by Cook [6] and co-workers.",
      "startOffset" : 88,
      "endOffset" : 92
    }, {
      "referenceID" : 5,
      "context" : "Our approach is related to the principal Hessian directions (pHd) method proposed by Li [11] and further developed by Cook [6] and co-workers.",
      "startOffset" : 123,
      "endOffset" : 126
    }, {
      "referenceID" : 0,
      "context" : "We assume that the function f : R → [0, 1], characterizing the classifier response, is non-decreasing, strictly concave in [0,+∞), and satisfies:",
      "startOffset" : 36,
      "endOffset" : 42
    }, {
      "referenceID" : 11,
      "context" : "We use the following variant of Stein’s identity [13, 19].",
      "startOffset" : 49,
      "endOffset" : 57
    }, {
      "referenceID" : 16,
      "context" : "We use the following variant of Stein’s identity [13, 19].",
      "startOffset" : 49,
      "endOffset" : 57
    }, {
      "referenceID" : 14,
      "context" : ", [16].",
      "startOffset" : 2,
      "endOffset" : 6
    } ],
    "year" : 2017,
    "abstractText" : "We consider a regression (discriminative learning) problem, whereby the regression function is a convex combination of k linear classifiers. Existing approaches are based on the EM algorithm, or similar techniques, without provable guarantees. We develop a simple method based on spectral techniques and a ‘mirroring’ trick, that discovers the subspace spanned by the classifiers’ parameter vectors. Under a probabilistic assumption on the feature vector distribution, we prove that this approach has nearly optimal statistical efficiency.",
    "creator" : "LaTeX with hyperref package"
  }
}