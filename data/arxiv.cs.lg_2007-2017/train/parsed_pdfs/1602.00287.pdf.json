{
  "name" : "1602.00287.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Additive Approximations in High Dimensional Nonparametric Regression via the SALSA",
    "authors" : [ "Kirthevasan Kandasamy", "Yaoliang Yu" ],
    "emails" : [ "KANDASAMY@CS.CMU.EDU", "YAOLIANG@CS.CMU.EDU" ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "Given i.i.d samples (Xi, Yi)ni=1 from some distribution PXY , on X × Y ⊂ RD × R, the goal of least squares regression is to estimate the regression function f∗(x) = E[Y |X = x]. A popular approach is linear regression which models f∗ as a linear combination of the variables\nProceedings of the 33 rd International Conference on Machine Learning, New York, NY, USA, 2016. JMLR: W&CP volume 48. Copyright 2016 by the author(s).\nx, i.e. f(x) = β>x for some β ∈ RD. Linear Regression is typically solved by minimising the sum of squared errors on the training set subject to a complexity penalty on β. Such parametric methods are conceptually simple and have desirable statistical properties when the problem meets the assumption. However, the parametric assumption is generally too restrictive for many real problems.\nNonparametric regression refers to a suite of methods that typically only assume smoothness on f∗. They present a more compelling framework for regression since they encompass a richer class of functions than parametric models do. However they suffer from severe drawbacks in high dimensional settings. The excess risk of nonparametric methods has exponential dependence on dimension. Current lower bounds (Györfi et al., 2002) suggest that this dependence is unavoidable. Therefore, to make progress stronger assumptions on f∗ beyond just smoothness are necessary. In this light, a common simplification has been to assume that f∗ decomposes into the additive form f∗(x) = f (1) ∗ (x1)+f (2) ∗ (x2)+· · ·+f (D)∗ (xD) (Hastie & Tibshirani, 1990; Lafferty & Wasserman, 2005; Ravikumar et al., 2009). In this exposition, we refer to such models as first order additive models. Under this assumption, the excess risk improves significantly.\nThat said, the first order assumption is often too biased in practice since it ignores interactions between variables. It is natural to ask if we could consider additive models which permit interactions. For instance, a second order model has the expansion f∗(x) = f (1) ∗ (x1, x2) + f (2) ∗ (x1, x3) + . . . . In general, we may consider d orders of interaction which have ( D d ) terms in the expansion. If d D, we may allow for a richer class of functions than first order models, and hopefully still be able to control the excess risk.\nEven when f∗ is not additive, using an additive approximation has its advantages. It is a well understood statistical concept that when we only have few samples, using a simpler model to fit our data gives us a better trade-off for variance against bias. Since additive models are statistically simpler they may give us better estimates due to reduced\nar X\niv :1\n60 2.\n00 28\n7v 3\n[ st\nat .M\nL ]\n2 4\nM ay\n2 01\nvariance. In most nonparametric regression methods, the bias-variance trade-off is managed via a parameter such as the bandwidth of a kernel or a complexity penalty. In this work, we demonstrate that this trade-off can also be controlled via additive models with different orders of interaction. Intuitively, we might use low order interactions with few data points but with more data we can increase model capacity via higher order interactions. Indeed, our experiments substantiate this intuition: additive models do well on several datasets in which f∗ is not necessarily additive.\nThere are two key messages in this paper. The first is that we should use additive models in high dimensional regression to reduce the variance of the estimate. The second is that it is necessary to model beyond just first order models to reduce the bias. Our contributions in this paper are:\n1. We formulate additive models for nonparametric regression beyond first order models. Our method SALSA – for Shrunk Additive Least Squares Approximation– estimates a dth order additive function containing ( D d ) terms\nin its expansion. Despite this, the computational complexity of SALSA is O(Dd2).\n2. Our theoretical analysis bounds the excess risk for SALSA for (i) additive f∗ under reproducing kernel Hilbert space assumptions and (ii) non-additive f∗ in the agnostic setting. In (i), the excess risk has only polynomial dependence on D.\n3. We compare our method against 21 alternatives on synthetic and 15 real datasets. SALSA is more consistent and in many cases outperforms other methods. Our software and datasets are available at github.com/kirthevasank/salsa. Our implementation of locally polynomial regression is also released as part of this paper and is made available at github.com/kirthevasank/local-poly-reg.\nBefore we proceed we make an essential observation. When parametric assumptions are true, parametric regression methods can scale both statistically and computationally to possibly several thousands of dimensions. However, it is common knowledge in the statistics community that nonparametric regression can be reliably applied only in very low dimensions with reasonable data set sizes. Even D = 10 is considered “high” for nonparametric methods. In this work we aim to statistically scale nonparametric regression to dimensions on the order 10–100 while addressing the computational challenges in doing so."
    }, {
      "heading" : "Related Work",
      "text" : "A plurality of work in high dimensional regression focuses on first order additive models. One of the most popular techniques is the back-fitting algorithm (Hastie et al., 2001) which iteratively approximates f∗ via a sum of D one di-\nmensional functions. Some variants such as RODEO (Lafferty & Wasserman, 2005) and SpAM (Ravikumar et al., 2009) study first order models in variable selection/sparsity settings. MARS (Friedman, 1991) uses a sum of splines on individual dimensions but allows interactions between variables via products of hinge functions at selected knot points. Lou et al. (2013) model f∗ as a first order model plus a sparse collection of pairwise interactions. However, restricting ourselves to only to a sparse collection of second order interactions might be too biased in practice. COSSO (Lin & Zhang, 2006) study higher order models but when you need only a sparse collection of them. In Section 4 we list several other parametric and nonparametric methods used in regression.\nOur approach is based on additive kernels and builds on Kernel Ridge Regression (Steinwart & Christmann, 2008; Zhang, 2005). Using additive kernels to encode and identify structure in the problem is fairly common in Machine Learning literature. A large line of work, in what has to come to be known as Multiple Kernel Learning (MKL), focuses on precisely this problem (Bach, 2008; Gönen & Alpaydin, 2011; Xu et al., 2010). Additive models have also been studied in Gaussian process literature via additive kernels (Duvenaud et al., 2011; Plate, 1999). However, they treat the additive model just as a heuristic whereas we also provide a theoretical analysis of our methods."
    }, {
      "heading" : "2. Preliminaries",
      "text" : "We begin with a brief review of some background material. We are given i.i.d data (Xi, Yi)ni=1 sampled from some distribution PXY on a compact space X × Y ⊂ RD × R. Let the marginal distribution ofX onX be PX and the L2(PX) norm be ‖f‖22 = ∫ f2dPX . We wish to use the data to find a function f : X → R with small risk\nR(f) = ∫ X×Y (y−f(x))2dPXY (x, y) = E[(Y −f(X))2].\nIt is well known thatR is minimised by the regression function f∗(·) = EXY [Y |X = ·] and the excess risk for any f is R(f)−R(f∗) = ‖f − f∗‖22 (Györfi et al., 2002). Our goal is to develop an estimate that has low expected excess risk ER(f̂)−R(f∗) = E[‖f̂ − f∗‖22], where the expectation is taken with respect to realisations of the data (Xi, Yi)ni=1.\nSome smoothness conditions on f∗ are required to make regression tractable. A common assumption is that f∗ has bounded norm in the reproducing kernel Hilbert space (RKHS) Hκ of a continuous positive definite kernel κ : X × X → R. By Mercer’s theorem (Schölkopf & Smola, 2001), κ permits an eigenexpansion of the form κ(x, x′) =∑∞ j=1 µjφj(x)φj(x\n′) where µ1 ≥ µ2 ≥ · · · ≥ 0 are the eigenvalues of the expansion and φ1, φ2, . . . are an orthonormal basis for L2(PX).\nKernel Ridge Regression (KRR) is a popular technique for nonparametric regression. It is characterised as the solution of the following optimisation problem over the RKHS of some kernel κ.\nf̂ = argmin f∈Hκ\nλ‖f‖2Hκ + 1\nn n∑ i=1 (Yi − f(Xi))2. (1)\nHere λ is the regularisation coefficient to control the variance of the estimate and is decreasing with more data. Via the representer theorem (Schölkopf & Smola, 2001; Steinwart & Christmann, 2008), we know that the solution lies in the linear span of the canonical maps of the training points Xn1 – i.e. f̂(·) = ∑ i αiκ(·, Xi). This reduces the above objective to α̂ = argminα∈Rn λα >Kα + 1 n‖Y −Kα‖ 2 2 where K ∈ Rn×n is the kernel matrix with Kij = κ(Xi, Xj). The problem has the closed form solution α̂ = (K + λnI)−1Y . KRR has been analysed extensively under different assumptions on f∗; see (Steinwart & Christmann, 2008; Steinwart et al., 2009; Zhang, 2005) and references therein. Unfortunately, as is the case with many nonparametric methods, KRR suffers from the curse of dimensionality as its excess risk is exponential in D.\nAdditive assumption: To make progress in high dimensions, we assume that f∗ decomposes into the following additive form that contains interactions of d orders among the variables. (Later on, we will analyse non-additive f∗.)\nf∗(x) = ∑\n1≤i1<i2<···<id≤D\nf (j) ∗ (xi1 , xi2 , . . . , xid), (2)\nWe will write, f∗(x) = ∑Md j=1 f (j) ∗ (x\n(j)) where Md =( D d ) , and x(j) denotes the subset (xi1 , xi2 , . . . , xid). We are primarily interested in the setting d D. While there are a large number of f (j)∗ ’s, each of them only permits interactions of at most d variables. We will show that this assumption does in fact reduce the statistical complexity of the function to be estimated. The first order additive assumption is equivalent to setting d = 1 above. A potential difficulty with the above assumption is the combinatorial computational cost in estimating all f (j)∗ ’s when d > 1. We circumvent this bottleneck using two strategems: a classical result from RKHS theory, and a computational trick using elementary symmetric polynomials used before by Duvenaud et al. (2011); Shawe-Taylor & Cristianini (2004) in the kernel literature for additive kernels."
    }, {
      "heading" : "3. SALSA",
      "text" : "To extend KRR to additive models we first define kernels k(j) that act on each subset x(j). We then optimise the fol-\nlowing objective jointly over f̂ (j) ∈ Hk(j) , j = 1 . . . ,Md.\n{f̂ (j)}Mdj=1 = argmin f(j)∈H\nk(j) ,j=1,...,Md\nλ Md∑ j=1 ‖f (j)‖2H k(j) +\n1\nn n∑ i=1 ( Yi − Md∑ j=1 f (j)(X (j) i ) )2 . (3)\nOur estimate for f is then f̂(·) = ∑ j f̂\n(j)(·). At first, this appears troublesome since it requres optimising over nMd parameters (α(j)i ), j = 1, . . . ,Md, i = 1, . . . , n. However, from the work of Aronszajn (1950), we know that the solution of (3) lies in the RKHS of the sum kernel k\nk(x, x′) = Md∑ j=1 k(j)(x(j), x(j) ′ ) (4)\n= ∑\n1≤i1<···<id≤D\nk(j)([xi1 , . . . , xid ], [x ′ i1 , . . . , x ′ id ]).\nSee Remark 6 in Appendix A for a proof. Hence, the solution f̂ can be written in the form f̂(·) = ∑ i αik(·, Xi) This is convenient since we only need to optimise over n parameters despite the combinatorial number of kernels. Moreover, it is straightforward to see that the solution is obtained by solving (1) by plugging in the sum kernel k for κ. Consequently f̂ (j) = ∑ i α̂ik\n(j)(·, X(j)i ) and f̂ = ∑ i α̂ik(·, Xi) where α̂ is the solution of (1). While at first sight the differences with KRR might seem superficial, we will see that the stronger additive assumption will help us reduce the excess risk for high dimensional regression. Our theoretical results will be characterised directly via the optimisation objective (3)."
    }, {
      "heading" : "3.1. The ESP Kernel",
      "text" : "While the above formulation reduces the number of optimisation parameters, the kernel still has a combinatorial number of terms which can be expensive to compute. While this is true for arbitrary choices for k(j)’s, under some restrictions we can efficiently compute k. For this, we use the same trick used by Shawe-Taylor & Cristianini (2004) and Duvenaud et al. (2011). First consider a set of base kernels acting on each dimension k1, k2, . . . , . . . , kD. Define k(j) to be the product kernel of all kernels acting on each coordinate – k(j)(x(j), x(j) ′ ) = ki1(xi1 , x ′ i1 )ki2(xi2 , x ′ i2\n) · · · kid(xid , x′id). Then, the additive kernel k(x, x′) becomes the dth elementary symmetric polynomial (ESP) of the D variables k1(x1, x ′ 1), . . . , kD(xD, x ′ D). Concretely,\nk(x, x′) = ∑\n1≤i1<i2<···<id≤D ( d∏ `=1 ki`(xi` , x ′ i` ) ) . (5)\nWe refer to (5) as the ESP kernel. Using the GirardNewton identities (Macdonald, 1995) for ESPs, we can compute this summation efficiently. For the D variables sD1 = s1, . . . , sD and 1 ≤ m ≤ D, define the mth power sum pm and themth elementary symmetric polynomial em:\npm(s D 1 ) = D∑ i=1 smi ,\nem(s D 1 ) = ∑ 1≤i1<i2<···<im≤D si1 × si2 × · · · × sim .\nIn addition define e0(sn1 ) = 1. Then, the Girard-Newton formulae state,\nem(s D 1 ) =\n1\nm m∑ i=1 (−1)i−1em−i(sD1 )pi(sD1 ).\nStarting with m = 1 and proceeding up to m = d, ed can be computed iteratively in just O(Dd2) time. By treating si = ki, the kernel matrix can be computed in O(n2d2D) time. While the ESP trick restricts the class of kernels we can use in SALSA, it applies for important kernel choices. For example, if each k(j) is a Gaussian kernel, then it is an ESP kernel if we set the bandwidths appropriately.\nIn what follows, we refer to a kernel such as k (5) which permits only d orders of interaction as a dth order kernel. A kernel which permits interactions of all D variables is of Dth order. Note that unlike in MKL, here we do not wish to learn the kernel. We use additive kernels to explicitly reduce the complexity of the function class over which we optimise for f̂ . Next, we present our theoretical results."
    }, {
      "heading" : "3.2. Theoretical Analysis",
      "text" : "We first consider the setting when f (j)∗ is in Hk(j) over which we optimise for f̂ (j). Theorem 3 generally bounds the excess risk of f̂ (3) in terms of RKHS parameters. Then, we specialise it to specific RKHSs in Theorem 4 and show that in many cases, the dependence on D reduces from exponential to polynomial for additive f∗. We begin with some assumptions.\nAssumption 1. f∗ has a decomposition f∗(x) =∑Md j=1 g (j)(x(j)) where each g(j) ∈ Hk(j) .\nWe point out that the decomposition {g(j)} need not be unique. To enforce definiteness (by abusing notation) we define f (j)∗ ∈ Hk(j) , j = 1, . . . ,Md to be the set of functions which minimise ∑ j ‖g(j)‖2H\nk(j) . Denote the mini-\nmum value by ‖f∗‖2F . We denote it by a norm for reasons made clear in our proofs.\nLet k(j) have an eigenexpansion k(j)(x(j), x(j) ′ ) =∑∞\n`=1 µ (j) ` φ (j) ` (x (j))φ (j) ` (x (j)′) in L2(PX(j)). Here,\n{(φ(j)` )∞`=1} is an orthonormal basis for L2(PX(j)) and {(µ(j)` )∞`=1} are its eigenvalues. PX(j) is the marginal distribution of the coordinates X(j). We also need the following regularity condition on the tail behaviour of the basis functions {φ(j)` } for all k(j). Similar assumptions are made in (Zhang et al., 2013) and are satisfied for a large range of kernels including those in Theorem 4.\nAssumption 2. For some q ≥ 2, ∃ ρ < ∞ such that for all j = 1, . . . ,Md and ` ∈ N, E[φ(j)` (X)2q] ≤ ρ2q .\nWe also define the following,\nγ(j)(λ) = ∞∑ `=1\n1\n1 + λ/µ (j) `\n, γk(λ) = Md∑ j=1 γ(j)(λ). (6)\nThe first term is known as the effective data dimensionality of k(j) (Zhang, 2005; Zhang et al., 2013) and captures the statistical difficulty of estimating a function inHk(j) . γk is the sum of the γ(j)’s. Our first theorem below bounds the excess risk of f̂ in terms ‖f∗‖2F and γk.\nTheorem 3. Let Assumptions 1 and 2 hold. and Y have bounded conditional variance: E[(Y − f∗(X))2|X] ≤ σ2. Then the solution f̂ of (3) satisfies,\nE[R(f̂)]−R(f∗) ≤Md ( 20λ‖f∗‖2F + 12σ2γk(λ)\nn + χ(k)\n) .\nHere χ(k) are kernel dependent low order terms and are given in (11) in Appendix A. Our proof technique generalises the analysis of Zhang et al. (2013) for KRR to the additive case. We use ideas from Aronszajn (1950) to handle sum RKHSs. We consider a space F containing the tuple of functions f (j) ∈ Hk(j) and use first order optimality conditions of (3) in F . The proof is given in Appendix A.\nThe term γk(λ), which typically has exponential dependence on d, arises through the variance calculation. Therefore, by using small d we may reduce the variance of our estimate. However, this will also mean that we are only considering a smaller function class and hence suffer large bias if f∗ is not additive. In naive KRR, using a Dth order kernel (equivalent to setting Md = MD = 1) the excess risk depends exponentially in D. In contrast, for an additive dth order kernel, γk(λ) has polynomial dependence on D if f∗ is additive. We make this concrete via the following theorem.\nTheorem 4. Assume the same conditions as Theorem 3. Then, suppressing log(n) terms,\n• if each k(j) has eigendecay µ(j)` ∈ O(`−2s/d), then by choosing λ n −2s 2s+d , we have E[R(f̂)] − R(f∗) ∈\nO(D2dn −2s 2s+d ),\n• if each k(j) has eigendecay µ(j)` ∈ O(π̃d exp(−α`2)) for some constants π̃, α, then by choosing λ 1/n, we have E[R(f̂)]−R(f∗) ∈ O(D 2dπ̃d\nn ).\nWe bound γk via bounds for γ(j) and use it to derive the optimal rates for the problem. The proof is in Appendix B.\nIt is instructive to compare the rates for the cases above when we use aDth order kernel κ in KRR to estimate a nonadditive function. The first eigendecay is obtained if each k(j) is a Matérn kernel. Then f (j) belongs to the Sobolev class of smoothness s (Berlinet & Thomas-Agnan, 2004; Tsybakov, 2008). By following a similar analysis, we can show that if κ is in a Sobolev class, then the excess risk of KRR is O(n −2s 2s+D ) which is significantly slower than ours. In our setting, the rates are only exponential in d but we have an additional D2d term as we need to estimate several such functions. An example of the second eigendecay is the Gaussian kernel with π̃ = √ 2π (Williamson et al., 2001). In the nonadditve case, the excess risk is in the Gaussian RKHS is O ( (2π)D/2 n ) which is slower than SALSA whose dependence on D is just polynomial. D, d do not appear in the exponent of n because the Gaussian RKHS contains very smooth functions. KRR is slower since we are optimising over the very large class of non-additive functions and consequently it is a difficult statistical problem. The faster rates for SALSA should not be surprising since the class of additive functions is smaller. The advantage of SALSA is its ability to recover the function at a faster rate when f∗ is additive. Finally we note that by taking each base kernel ki in the ESP kernel to be a 1D Gaussian, each k(j) is a Gaussian. However, at this point it is not clear to us if it is possible to recover a s-smooth Sobolev class via the tensor product of s-smooth one dimensional kernels.\nFinally, we analyse SALSA under more agnostic assumptions. We will neither assume that f∗ is additive nor that it lies in any RKHS. First, define the functions f (j)λ , j = 1, . . . ,M which minimise the population objective.\n{f (j)λ } Md j=1 = argmin\nf(j)∈H k(j) ,j=1,...,M\nλ Md∑ j=1 ‖f (j)‖2H k(j) +\nE [( Y −\nMd∑ j=1\nf (j)(X(j)) )2] . (7)\nLet fλ = ∑ j f (j) λ , R (j) λ = ‖f (j) λ ‖Hk(j) and R\n2 d,λ =∑\nj R (j) λ\n2 . To bound the excess risk in the agnostic setting\nwe also define the class,\nHd,λ = { f : X → R; f(x) = ∑ j f (j)(x(j)), (8)\n∀j, f (j) ∈ Hk(j) , ‖f (j)‖Hk(j) ≤ R (j) λ\n} .\nTheorem 5. Let f∗ be an arbitrary measurable function and Y have bounded fourth moment E[Y 4] ≤ ν4. Further each k(j) satisfies Assumption 2. Then ∀ η > 0,\nE[R(f̂)]−R(f∗) ≤ (1 + η)AE + (1 + 1/η)EE,\nwhere, AE = inf f∈Hd,λ\n‖f − f∗‖22, EE ∈ O (Mdγk(λ)\nn\n) .\nThe proof, given in Appendix C, also follows the template in Zhang et al. (2013). Loosely, we may interpret AE and EE as the approximation and estimation errors1. We may use Theorem 5 to understand the trade-offs in approximaing a non-additive function via an additive model. We provide an intuitive “not-very-rigorous” explanation. Hd,λ is typically increasing with d since higher order additive functions contain lower order functions. Hence, AE is decreasing with d as the infimum is taken over a larger set. On the other hand, EE is increasing with d. With more data EE decreases due to the 1/n term. Hence, we can afford to use larger d to reduce AE and balance with EE. This results in an overall reduction in the excess risk.\nThe actual analysis would be more complicated sinceHd,λ is a bounded class depending intricately on λ. It also depends on the kernels k(j), which differ with d. To make the above intuition concrete and more interpretable, it is necessary to have a good handle on AE. However, if we are to overcome the exponential dependence in dimension, usual nonparametric assumptions such as Hölderian/ Sobolev conditions alone will not suffice. Current lower bounds suggest that the exponential dependence is unavoidable (Györfi et al., 2002; Tsybakov, 2008). Additional assumptions will be necessary to demonstrate faster convergence. Once we control AE, the optimal rates can be obtained by optimising the bound over η, λ. We wish to pursue this in future work."
    }, {
      "heading" : "3.3. Practical Considerations",
      "text" : "Choice of Kernels: The development of our algorithm and our analysis assume that the ki’s are known. This is hardly the case in reality and they have to be chosen properly for good empirical performance. Cross validation is not feasible here as there are too many hyper-parameters. In our experiments we set each ki to be a Gaussian kernel ki(xi, x ′ i) = σY exp(−(xi − x′i)2/2h2i ) with bandwidth hi = cσin −1/5. Here σi is the standard deviation of the ith covariate and σY is the standard deviation of Y . The choice of bandwidth was inspired by several other kernel methods which use bandwidths on the order σin−1/5 (Ravikumar et al., 2009; Tsybakov, 2008). The constant c was hand tuned – we found that performance was robust to choices between 5 and 60. In our experiments we use c = 20. c\n1Loosely (and not strictly) since f̂ need not be in Hd,λ.\nwas chosen by experimenting on a collection of synthetic datasets and then used in all our experiments. Both synthetic and real datasets used in experiments are independent of the data used to tune c.\nChoice of d, λ: If the additive order of f∗ is known and we have sufficient data then we can use that for d in (5). However, this is usually not the case in practice. Further, even in non-additive settings, we may wish to use an additive model to improve the variance of our estimate. In these instances, our approach to choose d uses cross validation. For a given d we solve (1) for different λ and pick the best one via cross validation. To choose the optimal d we cross validate on d. In our experiments we observed that the cross validation error had bi-monotone like behaviour with a unique local optimum on d. Since the optimal d was typically small we search by starting at d = 1 and keep increasing until the error begins to increase again. If d could be large and linear search becomes too expensive, a binary search like procedure on {1, . . . , D} can be used.\nWe conclude this section with a couple of remarks. First, we could have considered an alternative additive model which sums all interactions up to dth order instead of just the dth order. The excess risk of this model differs from Theorems 3, 4 and 5 only in subdominant terms and/or constant factors. The kernel can be computed efficiently using the same trick by summing all polynomials up to d. In our experiments we found that both our original model (2) and summing over all interactions performed equally well. For simplicity, results are presented only for the former.\nSecondly, as is the case with most kernel methods, SALSA requires O(n2) space to store the kernel matrix and O(n3) effort to invert it. Some recent advances in scalable kernel methods such as random features, divide and conquer techniques, stochastic gradients etc. (Dai et al., 2014; Le et al., 2013; Rahimi & Recht, 2007; 2009; Zhang et al., 2013) can be explored to scale SALSA with n. However, this is beyond the scope of this paper and is left to future work. For this reason, we also limit our experiments to moderate dataset sizes. The goal of this paper is primarily to introduce additive models of higher order, address the combinatorial cost in such models and theoretically demonstrate the improvements in the excess risk."
    }, {
      "heading" : "4. Experiments",
      "text" : "We compare SALSA to the following. Nonparametric models: Kernel Ridge Regression (KRR), k-Nearest Neighbors (kNN), Nadaraya Watson (NW), Locally Linear/ Quadratic interpolation (LL, LQ), -Support Vector Regression ( −SVR), ν-Support Vector Regression (ν−SVR), Gaussian Process Regression (GP), Regression Trees (RT), Gradient Boosted Regression Trees (GBRT) (Friedman,\n2000), RBF Interpolation (RBFI), M5’ Model Trees (M5’) (Wang & Witten, 1997) and Shepard Interpolation (SI). Nonparametric additive models: Back-fitting with cubic splines (BF) (Hastie & Tibshirani, 1990), Multivariate Adaptive Regression Splines (MARS) (Friedman, 1991), Component Selection and Smoothing (COSSO) (Lin & Zhang, 2006), Sparse Additive Models (SpAM) (Ravikumar et al., 2009) and Additive Gaussian Processes (AddGP) (Duvenaud et al., 2011). Parametric models: Ridge Regression (RR), Least Absolute Shrinkage and Selection (LASSO) (Tibshirani, 1994) and Least Angle Regression (LAR) (Efron et al., 2004). We used software from (Chang & Lin, 2011; Hara & Chellappa, 2013; Jakabsons, 2015; Lin & Zhang, 2006; Rasmussen & Williams, 2006) or from Matlab. In some cases we used our own implementation."
    }, {
      "heading" : "4.1. Synthetic Experiments",
      "text" : "We begin with a series of synthetic examples. We compare SALSA to some non-additive methods to convey intuition about our additive model. First we create a synthetic low order function of order d = 3 in D = 15 dimensions. We do so by creating a d dimensional function fd and add that function over all ( D d ) combinations of coordinates. We compare SALSA using order 3 and compare against others. The results are given in Figure 1(a). This setting is tailored to the assumptions of our method and, not surprisingly, it outperforms all alternatives.\nNext we demonstrate the bias variance trade-offs in using additive approximations on non-additive functions. We created a 15 dimensional (non-additive) function and fitted a SALSA model with d = 1, 2, 4, 8, 15 for difference choices of n. The results are given in Figure 1(b). The interesting observation here is that for small samples sizes small d performs best. However, as we increase the sample size we can also increase the capacity of the model by accommodating higher orders of interaction. In this regime, large d produces the best results. This illustrates our previous point that the order of the additive model gives us another way to control the bias and variance in a regression task. We posit that when n is extremely large, d = 15 will eventually beat all other models. Finally, we construct synthetic functions in D = 20 to 50 dimensions and compare against other methods in Figures 1(c) to 1(f). Here, we chose d via cross validation. Our method outperforms or is competitive with other methods."
    }, {
      "heading" : "4.2. Real Datasets",
      "text" : "Finally we compare SALSA against the other methods listed above on 16 datasets. The datasets were taken from the UCI repository, Bristol Multilevel Modeling and the following sources: (Guillame-Bert et al., 2014; Just et al., 2010; Paschou, 2007; Tegmark et al, 2006; Tu, 2012; Wehbe et al., 2014). Table 1 gives the average squared error\non a test set. For the Speech dataset we have indicated the training time (including cross validation for selecting hyper-parameters) for each method. For SALSA we have also indicated the order d chosen by cross validation. See the caption under the table for more details.\nSALSA performs best (or is very close to the best) in 5 of the datasets. Moreover it falls within the top 5 in all but two datasets, coming sixth in both instances. Observe that in many cases d chosen by SALSA is much smaller thanD, but importantly also larger than 1. This observation (along with Fig 1(b)) corroborates a key theme of this paper: while it is true that additive models improve the variance in high dimensional regression, it is often insufficient to confine ourselves to just first order models.\nIn Appendix D we have given the specifics on the datasets such as preprocessing, the predictors, features etc. We have also discussed some details on the alternatives used."
    }, {
      "heading" : "5. Conclusion",
      "text" : "SALSA finds additive approximations to the regression function in high dimensions. It has less bias than first order models and less variance than non-additive methods. Algorithmically, it requires plugging in an additive kernel to KRR. In computing the kernel, we use the Girard-Newton formulae to efficiently sum over a combinatorial number of terms. Our theorems show that the excess risk depends only\npolynomially on D when f∗ is additive, significantly better than the usual exponential dependence of nonparametric methods, albeit under stronger assumptions. Our analysis of the agnostic setting provides intuitions on the tradeoffs invovled with changing d. We demonstrate the efficacy of SALSA via a comprehensive empirical evaluation. Going forward, we wish to use techniques from scalable kernel methods to handle large datasets.\nTheorems 3,4 show polynomial dependence on D when f∗ is additive. However, these theorems are unsatisfying since in practice regression functions need not be additive. We believe our method did well even on non-additive settings since we could control model capacity via d. In this light, we pose the following open problem: identify suitable assumptions to beat existing lower bounds and prove faster convergence of additive models whose additive order d increases with sample size n. Our Theorem 5 might be useful in this endeavour."
    }, {
      "heading" : "Acknowledgements",
      "text" : "We thank Calvin McCarter, Ryan Tibshirani and Larry Wasserman for the insightful discussions and feedback on the paper. We also thank Madalina Fiterau for providing us with datasets. This work was partly funded by DOE grant DESC0011114."
    }, {
      "heading" : "Appendix",
      "text" : "A. Proof of Theorem 3: Convergence of SALSA Our analysis here is a brute force generalisation of the analysis in Zhang et al. (2013). We handle the additive case using ideas from Aronszajn (1950). As such we will try and stick to the same notation. Some intermediate technical results can be obtained directly from Zhang et al. (2013) but we repeat them (or provide an outline) here for the sake of completeness.\nIn addition to the definitions presented in the main text, we will also need the following quantities,\nβ (j) t = ∞∑ `=t+1 µ (j) ` , Ψ (j) = ∞∑ `=1 µ (j) ` , b(n, t, q) = max (√ max(q, log t) , max(q, log t) n1/2−1/q ) .\nHere Ψ(j) is the trace of k(j). β(j)t depends on some t ∈ N which we will pick later. Also define βt = ∑ j β (j) t and\nΨ = ∑ j Ψ (j).\nNote that the excess risk can be decomposed into bias and variance terms,R(f̂)−R(f∗) = E[‖f̂−f∗‖22] = ‖f∗−Ef̂‖22 + E[‖f̂ − Ef̂‖22]. In Sections A.2 and A.3 respectively, we will prove the following bounds which will yield in Theorem 3:\n‖f∗ − Ef̂‖22 ≤Md ( 8λ‖f∗‖2F +\n8Md 3/2ρ4‖f∗‖2F λ Ψβt + ‖f∗‖2F Md∑ j=1 µ (j) t+1 + (CMdb(n, t, q)ρ2γk(λ)√ n )q ‖f∗‖22 ) , (9)\nE[‖f̂ − Ef̂‖22] ≤Md ( 12λ‖f∗‖2F + 12σ2γk(λ)\nn + (10)\n( 2σ2\nλ + 4‖f∗‖2F )( Md∑ j=1 µ (j) t+1 + 12Mdρ 4 λ Ψβt + (CMdb(n, t, q)ρ2γk(λ)√ n )q) ‖f∗‖22 ) .\nAccordingly, this gives the following expression for χ(k),\nχ(k) = inf t\n[ 8Md\n3/2ρ4‖f∗‖2F λ Ψβt +\n( 2σ2\nλ + 4‖f∗‖2F + 1\n)( CMdb(n, t, q)ρ\n2γk(λ)√ n\n)q ‖f∗‖22 ) +\n( 2σ2\nλ + 4‖f∗‖2F )( Md∑ j=1 µ (j) t+1 + 12Mdρ 4 λ Ψβt ) + ‖f∗‖2F Md∑ j=1 µ (j) t+1 ] . (11)\nNote that the second term in χ(k) is usually low order for large enough q due to the n−q/2 term. Therefore if in our setting β\n(j) t and µ (j) t+1 are small enough, χ(k) is low order. We show this for the two kernel choices of Theorem 4 in Appendix B.\nFirst, we review some well known results on RKHS’s which we will use in our analysis. Let κ be a PSD kernel andHκ be its RKHS. Then κ acts as the representer of evaluation – i.e. for any f ∈ Hκ, 〈f, κ(·, x)〉Hκ = f(x). Denote the RKHS norm ‖f‖Hκ = √ 〈f, f〉Hκ and the L2 norm ‖f‖2 = √∫ f2.\nLet the kernel κ have an eigenexpansion κ(x, x′) = ∑∞ `=1 µ`φ`(x)φ`(x\n′). Denote the basis coefficients of f in {φ`} via {θ`}. That is, θ` = ∫ f · φ` dP and f = ∑∞ `=1 θ`φ`. The following results are well known (Schölkopf & Smola, 2001; Steinwart & Christmann, 2008),\n〈φ`, φ`〉 = 1/µ`, ‖f‖22 = ∞∑ `=1 θ2` , ‖f‖2Hκ = ∞∑ `=1 θ2` µ` .\nBefore we proceed, we make the following remark on the minimiser of (3). Remark 6. The solution of (3) takes the form f̂(·) = ∑n i=1 αik(·, Xi) where k is the sum kernel (4).\nProof. The key observation is that we only need to consider n (and not nMd) parameters even though we are optimising overMd RKHSs. The reasoning uses a powerful result from Aronszajn (1950). Consider the class of functionsH′ = {f =∑ j f\n(j); f (j) ∈ Hk(j)}. In (3) we are minimising over H′. Any f ∈ H′ need not have a unique additive decomposition. ConsiderH ⊂ H′ which only contains the minimisers in the expression below.\n‖f‖2H = inf g(j)∈H\nk(j) ;f=\n∑ g(j) M∑ j=1 ‖g(j)‖2H k(j)\nAronszajn (1950) showed that H is an RKHS with the sum kernel k = ∑ j k\n(j) and its RKHS norm is ‖ · ‖H. Clearly, the minimiser of (3) lies in H. For any g′ ∈ H′, we can pick a corresponding g ∈ H with the same sum of squared errors (as g = g′) but lower complexity penalty (as g minimises the sum of norms for any g′ = g). Therefore, we may optimise (3) just overH and notH′. An application of Mercer’s theorem concludes the proof."
    }, {
      "heading" : "A.1. Set up",
      "text" : "We first define the following function class of the product of all RKHS’s, F = Hk(1) × Hk(2) × · · · × Hk(Md) ={ f = (f (1), . . . , f (Md))\n∣∣f (j) ∈ Hk(j) ∀j} and equip it with the inner product 〈f1, f2〉 = 〈f (1)1 , f (1)2 〉Hk(1) + · · · + 〈f (Md)1 , f (Md) 2 〉Hk(Md) . Here, f (j) 1 are the elements of f1 and 〈·, ·〉Hk(j) is the RKHS inner product of Hk(j) . Therefore\nthe norm is ‖f‖2F = ∑Md j=1 ‖f (j)‖2H k(j) . Denote ξ(j)x = k(j)(x, ·) and ξx(·) = K(·, x). Observe that for an additive\nfunction f = ∑ j f (j)(x),\nf(x) = ∑ j f (j)(x) = ∑ j 〈f (j), ξ(j)x 〉Hk(j) = 〈f , ξx〉.\nRecall that the solution to (3) is denoted by f̂ and the individual functions of the solution are given by f̂ (j). We will also use f∗ and f̂ to denote the representations of f∗ and f̂ in F , i.e., f∗ = (f (1)∗ , . . . , f (Md)∗ ) and f̂ = (f̂ (1), . . . , f̂ (Md)). Note that ‖f∗‖2F is precisely the bound used in Theorem 3. We will also denote ∆(j) = f̂ (j) − f (j) ∗ ∈ Hk(j) , ∆ =\n(∆(1), . . . ,∆(Md)) ∈ F , and ∆ = ∑ j ∆ (j) = f̂ − f∗.\nFor brevity, from now on we will write k(j)(x, x′) instead of k(j)(x(j), x(j) ′ ). Further, since d is fixed in this analysis we will write M for Md.\nA.2. Bias (Proof of Bound (9)) Note that we need to bound ‖E[∆]‖2 which by Jensen’s inequality is less than E[‖E[∆|Xn1 ]‖2]. Since, ‖E[∆|Xn1 ]‖22 ≤ M ∑M j=1 ‖E[∆(j)|Xn1 ]‖22, we will focus on bounding ∑M j=1 ‖E[∆(j)|Xn1 ]‖22.\nWe can write the optimisation objective (3) as follows,\nf̂ = argmin f∈F\n1\nn n∑ i=1 (〈f , ξXi〉 − Yi) 2 + λ‖f‖2F (12)\nSince this is Fréchet differentiable in F in the metric induced by the inner product defined above, the first order optimality conditions for f̂ (j) give us,\n1\nn n∑ i=1 ( 〈ξXi , f̂ − f∗〉 − i ) ξ (j) Xi + 2λf̂ (j) = 0.\nHere, we have taken Yi = f∗(Xi) + i where E[ i|Xi] = 0. Doing this for all f̂ (j) we have,\n1\nn n∑ i=1 ξXi (〈ξXi ,∆〉 − i) + λf̂ = 0 (13)\nTaking expectations conditioned on Xn1 and rearranging we get,\n(Σ̂ + λI)E[∆|Xn1 ] = −λf∗, (14)\nwhere Σ̂ = 1n ∑ i ξXi ⊗ ξXi is the empirical covariance. Since Σ̂ 0,\n∀j′, ‖E[∆(j ′)|Xn1 ]‖2H\nk(j ′) ≤ M∑ j=1 ‖E[∆(j)|Xn1 ]‖2H k(j) = ‖E[∆|Xn1 ]‖2F ≤ ‖f∗‖2F (15)\nLet E[∆(j)|Xn1 ] = ∑∞ `=1 δ (j) ` φ (j) ` where φ (j) ` are the eigenfunctions in the expansion of k (j). Denote δ(j)↓ = (δ (j) 1 , . . . , δ (j) t ) and δ (j) ↑ = (δ (j) t+1, δ (j) t+2, . . . ). We will set t later. Since ‖E[∆(j)|Xn1 ]‖22 = ‖δ (j) ↓ ‖22 + ‖δ (j) ↑ ‖22 we will bound the two terms. The latter term is straightforward,\n‖δ(j)↑ ‖ 2 2 ≤ µ (j) t+1 ∞∑ `=t+1 δ (j) ` 2 µ (j) ` ≤ µ(j)t+1‖E[∆(j)|Xn1 ]‖2H k(j) ≤ µ(j)t+1‖f∗‖2F (16)\nTo control ‖δ(j)↓ ‖, let f (j) ∗ = ∑ ` θ (j) ` φ (j) ` . Also, define the following: θ (j) ↓ = (θ (j) 1 , . . . , θ (j) t ), Φ\n(j) ∈ Rn×t, Φ(j)i` = φ\n(j) ` (Xi), Φ (j) ` = (φ (j) ` (X1), . . . , φ (j) ` (Xn)) ∈ Rn, M(j) = diag(µ (j) 1 , . . . , µ (j) t ) ∈ Rt×t+ and v(j) ∈ Rn where v (j) i =∑\n`>t δ (j) ` φ (j) ` (Xi) = E[∆ (j) ↑ (Xi)|Xn1 ].\nFurther define, Φ = [Φ(1) . . .Φ(M)] ∈ Rn×tM , M = diag(M(1), . . . ,M(M)) ∈ RtM×tM , vi = ∑ j v (j), δ↓ = [δ (1) ↓ ; . . . ; δ (M) ↓ ] ∈ RtM and θ↓ = [θ (1) ↓ ; . . . ; θ (M) ↓ ] ∈ RtM .\nNow compute the F-inner product between (0, . . . , φ(j)` , . . . ,0) with equation (14) to obtain,\n1\nn n∑ i=1 〈φ(j)` , ξ (j) Xi 〉H k(j) 〈ξXi ,E[∆|Xn1 ]〉+ λ〈φ (j) ` ,E[∆ (j)|Xn1 ]〉Hk(j) = −λ〈φ (j) ` , f (j) ∗ 〉H k(j)\n1\nn n∑ i=1 φ (j) ` (Xi) M∑ j=1 ∑ `′≤t φ (j) `′ (Xi)δ (j) `′ + ∑ `′>t φ (j) `′ (Xi)δ (j) `′ + λ δ(j)` µ (j) ` = −λ θ (j) ` µ (j) `\nAfter repeating this for all j and for all ` = 1, . . . , t, and arranging the terms appropriately this reduces to( 1\nn Φ>Φ + λM−1\n) δ↓ = −λM−1θ↓ − 1\nn Φ>v\nBy writing Q = (I + λM−1)1/2, we can rewrite the above expression as( I +Q−1 ( 1\nn Φ>Φ− I\n) Q−1 ) Qδ↓ = −λQ−1M−1θ↓ − 1\nn Q−1Φ>v.\nWe will need the following technical lemmas. The proofs are given at the end of this section. These results correspond to Lemma 5 in Zhang et al. (2013).\nLemma 7. ‖λQ−1M−1θ↓‖22 ≤ λ‖f∗‖2F .\nLemma 8. E [ ‖ 1nQ −1Φ>v‖22 ] ≤ 1λM 3/2ρ4‖f∗‖2FΨβt.\nLemma 9. Define the event E = {‖Q−1( 1nΦ >Φ− I)Q−1‖op ≤ 1/2}. Then, there exists a constant C s.t.\nP(Ec) ≤ ( max (√ max(q, log t) , max(q, log t)\nn1/2−1/q\n) × MCρ\n2γk(λ)√ n\n)q .\nWhen E holds, by Lemma 9 and noting that Q I ,\n‖δ↓‖22 ≤ ‖Qδ↓‖22 = ∥∥∥(I +Q−1( 1\nn Φ>Φ− I\n) Q−1 )−1( −λQ−1M−1θ↓ − 1\nn Q−1Φ>v )∥∥∥2 ≤ 4‖λQ−1M−1θ↓ + 1\nn Q−1Φ>v‖22. ≤ 8‖λQ−1M−1θ↓‖22 + 8‖\n1 n Q−1Φ>v‖22\nNow using Lemmas 7 and 8,\nE[‖δ↓‖22|E ] ≤ 8 ( λ‖f∗‖2F +\nM3/2ρ4‖f∗‖2FΨβt λ ) Since E[‖δ↓‖22] = P(E)E[‖δ↓‖22|E ] + P(Ec)E[‖δ↓‖22|Ec] and by using the fact that ‖δ↓‖2 ≤ ‖E[∆|Xn1 ]‖22 ≤ ‖f∗‖22, we have\nE[‖δ↓‖22] ≤ 8λ‖f∗‖2F + 8Mρ4‖f∗‖2FΨβt\nλ +(\nmax (√ max(q, log t) , max(q, log t)\nn1/2−1/q\n) × MCρ\n2γk(λ)√ n\n)q ‖f∗‖22\nFinally using (16) and by noting that\n‖E[∆|Xn1 ]‖22 ≤M M∑ j=1 ‖E[∆(j)|Xn1 ]‖22 = M ( ‖δ↓‖22 + ∑ j ‖δ(j)↑ ‖ 2 2 ) ≤M ( ‖δ↓‖22 + ‖f∗‖2F ∑ j µ (j) t+1 ) and then taking expectation over Xn1 , we obtain the bound for the bias in (9)."
    }, {
      "heading" : "Proofs of Technical Lemmas",
      "text" : ""
    }, {
      "heading" : "A.2.1. PROOF OF LEMMA 7",
      "text" : "Lemma 7 is straightforward.\n‖Q−1M−1θ↓‖22 = M∑ j=1 ‖Q(j) −1 M(j) −1 θ (j) ↓ ‖ 2 2 = M∑ j=1 θ (j) ↓ > (M(j) 2 + λM(j))−1θ(j)↓\n≤ M∑ j=1 θ (j) ↓ > (λM(j))−1θ(j)↓ = 1 λ M∑ j=1 t∑ `=1 θ (j) ` 2 µ (j) ` ≤ 1 λ ‖f∗‖2F"
    }, {
      "heading" : "A.2.2. PROOF OF LEMMA 8",
      "text" : "We first decompose the LHS as follows,∥∥∥∥ 1nQ−1Φ>v ∥∥∥∥2\n2\n= ∥∥∥∥(M + λI)−1/2( 1nM1/2Φ>v )∥∥∥∥2\n2\n≤ 1 λ ∥∥∥∥ 1nM1/2Φ>v ∥∥∥∥2\n2\n(17)\nThe last step follows by noting that ‖(M + λI)−1/2‖2op = maxj,` 1/(µ (j) ` + λ) ≤ 1/λ. Further,\nE [ ‖M1/2Φ>v‖22 ] = M∑ j=1 t∑ `=1 µ (j) ` E[(Φ (j) ` > v)2] ≤ M∑ j=1 t∑ `=1 µ (j) ` E[‖Φ (j) ` ‖ 2 2‖v‖22] (18)\nNote that the term inside the summation in the RHS can be bounded by, √ E[‖Φ(j)` ‖42]E[‖v‖42]. We bound the first expectation via,\nE [ ‖Φ(j)` ‖ 4 ] = E [( n∑ i=1 φ (j) ` (Xi) 2 )2] ≤ E [ n n∑ i=1 φ (j) ` (Xi) 4 ] ≤ n2ρ4\nwhere the last step follows from Assumption 2. For the second expectation we first bound ‖v‖4,\n‖v‖42 =  n∑ i=1 ( M∑ j=1 v (j) i )22 ≤ M n∑ i=1 M∑ j=1 v (j) i 2 2 ≤M3n n∑ i=1 M∑ j=1 v (j) i 4\nNow by the Cauchy Schwarz inequality,\nv (j) i\n2 = (∑ `>t δ (j) ` φ (j) ` (Xi) )2 ≤ (∑ `>t δ (j) ` 2 µ (j) ` )(∑ `>t µ (j) ` φ (j) ` (Xi) 2 ) .\nTherefore,\nE [ ‖v‖4 ] ≤M3n n∑ i=1 M∑ j=1 E [ ‖E[∆(j)|Xn1 ]‖4H k(j) (∑ `>t µ (j) ` φ (j) ` (Xi) 2 )2]\n≤M3n‖f∗‖4F M∑ j=1 n∑ i=1 ∑ `,`′>t E[µ(j)` µ (j) `′ φ (j) ` (Xi) 2φ (j) `′ (Xi) 2] ≤M3nρ4‖f∗‖4F M∑ j=1 n∑ i=1 (∑ `>t µ (j) ` )2 ≤M3n2ρ4‖f∗‖4F M∑ j=1 β (j) t 2\nHere, in the first step we have used the definition of ‖E[∆(j)|Xn1 ]‖Hk(j) , in the second step, equation (15), in the third step assumption 2 and Cauchy Schwarz, and in the last step, the definition of βt. Plugging this back into (18), we get\nE [ ‖M1/2Φ>v‖2 ] ≤M3/2n2ρ4‖f∗‖2F √√√√ M∑ j=1 β (j) t 2 M∑ j=1 t∑ `=1 µ (j) ` ≤M 3/2n2ρ4‖f∗‖2FΨβt\nThis bound, along with equation (17) gives us the desired result."
    }, {
      "heading" : "A.2.3. PROOF OF LEMMA 9",
      "text" : "Define π(j)i = {φ (j) ` (xi)}t`=1 ∈ Rt, πi = [π (1) i ; . . . ;π (M) i ] ∈ RtM and the matricesAi = Q−1(πiπ>i −I)Q−1 ∈ Rtm×tM . Note that Ai = A>i and E[Ai] = Q−1(E[πiπ>i ]− I)Q−1 = 0.\nThen, if i, i = 1, . . . , n are i.i.d Rademacher random variables, by a symmetrization argument we have, E [∥∥∥Q−1( 1\nn Φ>Φ− I\n) Q−1 ∥∥∥k op ] = E [∥∥∥ 1 n n∑ i=1 Ai ∥∥∥k op ] ≤ 2kE [∥∥∥ 1 n n∑ i=1 iAi ∥∥∥k op ] (19)\nThe above term can be bounded by the following expression.\n2q √emax(q, log(t))ρ2√M√ n √√√√ M∑ `=1 γ(j)(λ)2 + 4emax(q, log(t))ρ2 ( M n )1−1/q ( M∑ `=1 γ(j)(λ)q )1/qq\n≤ ( C\n2\n)q max (√ M(max(q, log t)), M1−1/q max(q, log t)\nn1/2−1/q\n)q ( ρ2γk(λ)√\nn )q The proof mimics Lemma 6 in (Zhang et al., 2013) by performing essentially the same steps over F instead of the usual Hilbert space. In many of the steps, M terms appear (instead of the one term for KRR) which is accounted for via Jensen’s inequality.\nFinally, by Markov’s inequality, P(Ec) ≤ 2kE [∥∥∥Q−1( 1\nn Φ>Φ− I\n) Q−1 ∥∥∥q op ] ≤ Cq max (√ M(max(q, log t)), M1−1/q max(q, log t)\nn1/2−1/q\n)q ( ρ2γk(λ)√\nn\n)q\nA.3. Variance (Proof of Bound (10))\nOnce again, we follow Zhang et al. (2013). The tricks we use to generalise it to the additive case (i.e. over F) are the same as that for the bias. Note that since E[‖f̂−Ef̂‖22] ≤ E[‖f̂−g‖22] for all g, it is sufficient to bound E[‖f̂−f∗‖22] = E[‖∆‖22].\nFirst note that,\nλE[‖f̂‖2F |Xn1 ] ≤ E\n[ 1\nn n∑ i=1 ( f̂(Xi)− Yi )2 + λ‖f̂‖2F ∣∣∣∣Xn1 ] ≤ 1 n n∑ i=1 E[ 2i |Xn1 ] + λ‖f∗‖2F ≤ σ2 + λ‖f∗‖2F\nThe second step follows by the fact that f̂ is the minimiser of (12). Then, for all j,\nE[‖∆(j)‖2H k(j) |Xn1 ] ≤ E[‖∆‖2F |Xn1 ] ≤ 2‖f∗‖2F + 2E[‖f̂‖22|Xn1 ] ≤\n2σ2\nλ + 4‖f∗‖2F (20)\nLet ∆(j) = ∑∞ `=1 δ (j) ` φ (j) ` . Note that the definition of δ (j) ` is different here. Define δ (j) ↓ , δ (j) ↑ ,∆ (j) ↓ ,∆ (j) ↑ , δ↓ analogous to the definitions in Section A.2. Then similar to before we have,\nE[‖δ(j)↑ ‖ 2 2] ≤ µ (j) t+1E[‖∆ (j) ↑ ‖ 2 H k(j)\n] ≤ µ(j)t+1 ( 2σ2\nλ + 4‖f∗‖2F ) We may use this to obtain a bound on E[‖∆↑‖2]. To obtain a bound on E[‖∆↓‖2], take the F inner product of (0, . . . , φ\n(j) ` , . . . ,0) with the first order optimality condition (13) and following essentially the same procedure to the\nbias we get, ( 1\nn Φ>Φ + λM−1\n) δ↓ = −λM−1θ↓ − 1\nn Φ>v +\n1 n Φ>\nwhere Φ,M, θ↓ are the same as in the bias calculation. v(j) ∈ Rn where v(j)i = ∑ `>t δ (j) ` φ (j) ` (Xi) = E[∆ (j) ↑ (Xi)|Xn1 ] (recall that δ(j)` is different to the definition in the bias) and ∈ Rn, i = Yi − f∗(Xi) is the vector of errors. Then we write, (\nI +Q−1 ( 1\nn Φ>Φ− I\n) Q−1 ) Qδ↓ = −λQ−1M−1θ↓ − 1\nn Q−1Φ>v +\n1 n Q−1Φ>\nwhere Q = (I + λM−1)1/2 as before. Following a similar argument to the bias, when the event E holds,\n‖δ↓‖22 ≤ ‖Qδ↓‖22 ≤ 4‖λQ−1M−1θ↓ + 1\nn Q−1Φ>v +\n1 n Q−1Φ> ‖22\n≤ 12‖λQ−1M−1θ↓‖2 + 12‖ 1 n Q−1Φ>v‖2 + 12‖ 1 n Q−1Φ> ‖22 (21)\nBy Lemma 7, the first term can be bounded via 12λ‖f∗‖2F . For the second and third terms we use the following two lemmas, the proofs of which are given at the end of this subsection. Lemma 10. E [ ‖ 1nQ −1Φ>v‖22 ] ≤ 1λMρ 4Ψβt(2σ 2/λ+ 4‖f∗‖2F ).\nLemma 11. E [∥∥ 1\nnQ −1Φ> ∥∥2 2 ] ≤ σ 2 n γk(λ)\nNote that E[‖δ↓‖22] ≤ P(E)E[‖δ↓‖22|E ] + E[1(Ec)‖δ↓‖22]. The bound on the first term comes via equation (21) and Lemmas 7, 10 and 11. The second term can be bound via,\nE[1(Ec)‖δ↓‖22] ≤ E[1(Ec)E[‖∆‖2F |Xn1 ] ≤ ( max (√ max(q, log t) , max(q, log t)\nn1/2−1/q\n) × MCρ\n2γk(λ)√ n\n)q ( 2σ2\nλ + 4‖f∗‖2F\n) (22)\nHere, we have used equation (20) and Lemma 9. Finally, note that\nE[‖∆‖22] ≤M ∑ j E[‖∆(j)‖22] = M ( E‖δ↓‖22 + ∑ j E‖δ(j)↑ ‖ 2 2 ) ≤M ( E‖δ↓‖22 + (2σ2 λ + 4‖f∗‖2F )∑\nj\nµ (j) t+1\n) (23)\nWhen we combine (21), (22) and (23) we get the bound in equation (10)."
    }, {
      "heading" : "Proofs of Technical Lemmas",
      "text" : ""
    }, {
      "heading" : "A.3.1. PROOF OF LEMMA 10",
      "text" : "Note that following an argument similar to equation (25) in Lemma 8, it is sufficient to bound E‖M1/2Φ>v‖22. We expand this as,\nE [ ‖M1/2Φ>v‖22 ] = M∑ j=1 t∑ `=1 µ (j) ` E[(Φ (j) ` > v)2] ≤ M∑ j=1 t∑ `=1 µ (j) ` E[‖Φ (j) ` ‖ 2‖v‖2]\nTo bound this term, first note that\n‖v‖2 = n∑ i=1 ( M∑ j=1 v (j) i )2 ≤M n∑ i=1 M∑ j=1 v (j) i 2 ≤M n∑ i=1 M∑ j=1 (∑ `>t δ (j) ` 2 µ (j) ` )(∑ `>t µ (j) ` φ (j) ` (Xi) 2 ) Therefore,\nE [ ‖M1/2Φ>v‖2 ] ≤ M∑ j=1 t∑ `=1 µ (j) ` M n∑ i=1 M∑ j′=1 E [ E[‖∆(j ′)‖2H k(j ′) |Xn1 ]‖Φ (j) ` ‖ 2 ∑ `′>t µ (j′) `′ φ (j′) `′ (Xi) 2 ] (24)\n≤M ( 2σ2\nλ + 4‖f∗‖2F ) M∑ j=1 t∑ `=1 µ (j) ` n∑ i=1 M∑ j′=1 ∑ `′>t µ (j′) `′ E [ ‖Φ(j)` ‖ 2φ (j′) `′ (Xi) 2 ]\nFor all i, the inner expectation can be bounded using assumption 2 and Jensen’s inequality via,\nE [ ‖Φ(j)` ‖ 2φ (j′) `′ (Xi) 2 ] ≤ √ E [ ‖Φ(j)` ‖4 ] E [ φ (j′) `′ (Xi) 4 ] ≤ ρ2 √√√√E[( n∑ i=1 φ (j) ` (Xi) 2 )2]\n≤ ρ2 √√√√E[n n∑\ni=1\nφ (j) ` (Xi) 4\n] ≤ ρ2 √ n2ρ4 = nρ4.\nThis yields,\nE [ ‖M1/2Φ>v‖2 ] ≤Mn2ρ4 ( 2σ2\nλ + 4‖f∗‖2F ) M∑ j=1 t∑ `=1\nµ (j) `︸ ︷︷ ︸\n≤Ψ\nM∑ j′=1 ∑ `′>t\nµ (j′) `′︸ ︷︷ ︸\n=βt\nFinally, we have\nE [∥∥∥∥ 1nQ−1Φ>v ∥∥∥∥2\n2\n] ≤ E [ 1\nλ ∥∥∥∥ 1nM1/2Φ>v ∥∥∥∥2\n2\n] ≤ 1 λ Mρ4Ψβt ( 2σ2 λ + 4‖f∗‖2F ) (25)"
    }, {
      "heading" : "A.3.2. PROOF OF LEMMA 11",
      "text" : "We expand the LHS as follows to obtain the result.\nE [∥∥ 1 n Q−1Φ> ∥∥2] = 1 n2 M∑ j=1 t∑ `=1 n∑ i=1\n1\n1 + λ/µ (j) `\nE[φ(j)` (Xi) 2 2i ] ≤\nσ2\nn M∑ j=1 γ(j)(λ) = σ2 n γk(λ)\nThe first step is just an expansion of the matrix. In the second step we have used E[φ(j)` (Xi) 2 2i ] = E[φ (j) ` (Xi) 2 E[ 2i |Xi]] ≤ σ2 since E[φ(j)` (X)2] = 1. In the last two steps we have used the definitions of γ(j)(λ) and γk(λ)."
    }, {
      "heading" : "B. Proof of Theorem 4: Rate of Convergence in Different RKHSs",
      "text" : "Our strategy will be to choose λ so as to balance the dependence on n in the first two terms in the RHS of the bound in Theorem 3.\nProof of Theorem 4-1. Polynomial Decay: The quantity γk(λ) can be bounded via Md ∑∞ `=1 1/(1 + λ/µ̃`). If we set λ = n −2s 2s+d , then\nγk(λ)\nMd = ∞∑ `=1\n1\n1 + n −2s 2s+d /µ̃`\n≤ n d 2s+d + ∑\n`>n d 2s+d\n1\n1 + n 2s 2s+d ` 2s d\n≤ n d 2s+d + n− 2s 2s+d ∑ `>n d 2s+d\n1\nn −2s 2s+d + ` 2s d\n≤ n d 2s+d + n −2s 2s+d ( n d 2s+d + ∫ ∞ n d 2s+d u−2s/ddu ) ∈ O(n d 2s+d ).\nTherefore, γk(λ)/n ∈ O(Mdn −2s 2s+d ) giving the correct dependence on n as required. To show that χ(k) is negligible, set t = n 3d 2s−d . Ignoring the poly(D) terms, both µ̃t+1, βt ∈ O(n −6s 2s−d ) and χ(k) is low order. Therefore, by Thereom 3 the excess risk is in O(M2dn −2s 2s+d ).\nProof of Theorem 4-2. Exponential Decay: By setting λ = 1/n and following a similar argument to above we have,\nγk(λ) Md ≤ √ log n α + 1 λ ∑ `> √ logn/α µ̃` ≤ √ log n α + nπ̃d ∑ `> √ logn/α exp(−α`2)\n≤ √ log n\nα + nπ̃d\n( 1\nn +\n∫ ∞ √\nlogn/α\nexp(−α`2) ) = √ log n\nα + π̃d\n( 1 + √ π\n2 (1− Φ(\n√ log n) ) ,\nwhere Φ is the Gaussian cdf. In the first step we have bounded the first √\nlogn α terms by 1 and then bounded the second\nterm by a constant. Note that the last term is o(1). Therefore ignoring log n terms, γk(λ) ∈ O(Mdπ̃d) which gives excess risk O(M2d π̃d/n). χ(k) can be shown to be low order by choosing t = n2 which results in µ̃t+1, βt ∈ O(n−4)."
    }, {
      "heading" : "C. Proof of Theorem 5: Analysis in the Agnostic Setting",
      "text" : "As before, we generalise the analysis by Zhang et al. (2013) to the tuple RKHS F . We begin by making the following crucial observation about the population minimiser (7) fλ = ∑M j=1 f (j) λ ,\nfλ = argmin g∈Hd,λ\n‖g − f∗‖22. (26)\nTo prove this, consider any g = ∑M j=1 g\n(j) ∈ Hd,λ. Using the fact that R(g) = R(f∗) + ‖g − f∗‖22 for any g and that ‖g‖F ≤ Rd,λ we obtain the above result as follows.\nE [ (f∗(X)− Y )2 ] + ‖fλ − f∗‖22 + λR2d,λ = E[(fλ(X)− Y )2] + λR2d,λ\n≤ E[(g(X)− Y )2] + λ M∑ j=1 ‖g(j)‖2H k(j) ≤ E [ (f∗(X)− Y )2 ] + ‖g − f∗‖22 + λR2d,λ.\nBy using the above, we get for all η > 0,\nE [ ‖f̂ − f∗‖22 ] ≤ (1 + η)E [ ‖fλ − f∗‖22 ] + (1 + 1/η)E [ ‖f̂ − fλ‖22 ] = (1 + η) inf\ng∈Hd,λ ‖g − f∗‖22︸ ︷︷ ︸ AE\n+(1 + 1/η)E [ ‖f̂ − fλ‖22 ]︸ ︷︷ ︸ EE\nFor the first step, by the AM-GM inequality we have 2 ∫ (f̂ − fλ)(fλ − f∗) ≤ 1/η ∫ (f̂ − fλ)2 + η ∫\n(fλ − f∗)2. In the second step we have used (26). The term AE is exactly as in Theorem 5 so we just need to bound EE.\nAs before, we consider the RKHS F . Denote the representation of fλ in F by fλ = (f (1)λ , . . . , f (M) λ ). Note that Rd,λ = ‖fλ‖F . Analogous to the analysis in Appendix A we define ∆(j) = f̂ (j) − f (j)λ , ∆ = ∑ j ∆\n(j) = f̂ − fλ and ∆ = (∆(1), . . . ,∆(M)). Note that EE = E[‖∆‖22].\nLet ∆(j) = ∑∞ `=1 δ (j) ` φ (j) ` be the expansion of ∆\n(j) in L2(PX). For t ∈ N, which we will select later, define ∆(j)↓ =∑t `=1 δ (j) ` φ (j) ` , ∆ (j) ↑ = ∑ `>t δ (j) ` φ (j) ` , δ (j) ↓ = (δ (1), . . . , δ(t)) ∈ Rt and δ(j)↑ = (δ (j) ` )`>t. Let ∆↓ = ∑ j ∆\n(j) ↓ and ∆↑ =∑\nj ∆ (j) ↑ . Continuing the analogy, let f (j) λ = ∑M j=1 θ (j) ` φ (j) ` be the expansion of f (j) λ . Let θ (j) ↓ = (θ (j) 1 , . . . , θ (j) t ) ∈ Rt\nand θ↓ = [θ (1) ↓ ; . . . ; θ (M) ↓ ] ∈ RtM . Let v ∈ Rn such that v (j) i = ∑ `>t δ (j) ` φ (j) ` (Xi) and vi = ∑ j v (j) i . Let ∈ Rn,\ni = Yi − fλ(Xi). Also define the following quantities:\nς2λ(x) = E[(Y − fλ(X))2|X = x], B4λ = 32‖fλ‖4F + 8E[ς4λ(X)]/λ2.\nWe begin with the following lemmas.\nLemma 12. E[ς4λ(X)] ≤ 8Ψ2‖fλ‖4Fρ4 + 8ν4.\nLemma 13. E [( E[‖∆‖2F |Xn1 ] )2] ≤ B4λ. We first bound E[‖∆(j)↑ ‖22] = ∑ `>t Eδ (j) ` 2 using Lemma 13 and Jensen’s inequality.\nE [ ‖δ(j)↑ ‖ 2 2 ] = ∑ `>t E[δ(j)` 2 ] ≤ µ(j)t+1E ∑ `>t δ (j) ` 2 µ (j) `  ≤ µ(j)t+1E [‖∆(j)‖2H k(j) ] ≤ µ(j)t+1E [ ‖∆‖2F ] ≤ µ(j)t+1B2λ (27)\nNext we proceed to bound E[‖∆↓‖22]. For this we will use Φ(j),Φ (j) ` ,M(j),M, Q from Appendix A. The first order optimality condition can be written as,\n1\nn n∑ i=1 ξXi (〈ξXi ,∆〉 − i) + λf̂ = 0.\nThis has the same form as (13) but the definitions of ∆ and i have changed. Now, just as in the variance calculation, when we take the F-inner product of the above with (0, . . . , φ(j)` , . . . ,0) and repeat for all j we get,(\nI +Q−1 ( 1\nn Φ>Φ− I\n) Q−1 ) Qδ↓ = −λQ−1M−1θ↓ − 1\nn Q−1Φ>v +\n1 n Q−1Φ>\nSince Φ,M, Q are the same as before we may reuse Lemma 9. Then, as Q I when the event E holds,\n‖δ↓‖22 ≤ ‖Qδ↓‖22 ≤ 4‖λQ−1M−1θ↓ + 1\nn Q−1Φ>v +\n1 n Q−1Φ> ‖22\n≤ 8‖ 1 n Q−1Φ>v‖2 + 8‖λQ−1M−1θ↓ − 1 n Q−1Φ> ‖22 (28)\nWe now bound the two terms in the RHS in expectation via the following lemmas.\nLemma 14. E[‖ 1nQ −1Φ>v‖2] ≤ 1λMB 2 λρ 4Ψβt\nLemma 15. E[‖λQ−1M−1θ↓ − 1nQ −1Φ> ‖22] ≤ 1nρ 2γk(λ) √ E[ς4λ(X)]\nNow by Lemma 13 we have, E[‖δ↓‖22] = P(E)E[‖δ↓‖22|E ] + E[1(Ec)‖δ↓‖22] ≤ E[‖δ↓‖22|E ] +B2λP(Ec). E[‖δ↓‖22|E ] can be bounded using Lemmas 14 and 15 while P(Ec) can be bounded using Lemma 9. Combining these results along with (27) we have the following bound for EE = E[‖∆‖22],\nE[‖∆‖22] ≤ E [∥∥∥∥ M∑ j=1 ∆(j) ∥∥∥∥2 2 ] ≤M M∑ j=1 E [ ‖∆(j)‖22 ] = M E[‖δ↓‖22] + M∑ j=1 E[‖δ(j)↓ ‖ 2 2]  ≤ 8 n Mρ2γk(λ) √ E[ς4λ(X)] + 8 λ M2B2λρ 4Ψβt +B 2 λM ( CMdb(n, t, q)ρ 2γk(λ)√ n )q +B2λM ∑ j µ (j) t+1\nNow we choose t large enough so that the following are satisfied,\nβt ≤ λ\nM2nB4λ ,\nM∑ j=1 µ (j) t+1 ≤\n1\nMnB4λ ,\n( CMdb(n, t, q)ρ\n2γk(λ)√ n )q ≤ 1 MnB4λ .\nThen the last three terms are O(1/nB2λ) and the first term dominates. Using Lemma 12 and recalling that R2d,λ =∑ j R (j) λ 2 = ‖fλ‖2F we get EE ∈ O ( n−1Mγk(λ)R 2 d,λ ) as given in the theorem."
    }, {
      "heading" : "Proofs of Technical Lemmas",
      "text" : ""
    }, {
      "heading" : "C.1. Proof of Lemma 13",
      "text" : "Since f̂ is the minimiser of the empirical objective,\nE [ λ‖f̂‖2F |Xn1 ] ≤ E λ M∑ j=1 ‖f̂ (j)‖2H k(j) + 1 n n∑ i=1  M∑ j=1 f̂ (j)(X (j) i )− Yi 2 ∣∣∣∣∣Xn1 \n≤ E λ M∑ j=1 ‖f (j)λ ‖ 2 H k(j) + 1 n n∑ i=1  M∑ j=1 f (j) λ (X (j) i )− Yi 2 ∣∣∣∣∣Xn1  ≤ λ‖fλ‖2F + 1n n∑ i=1 ς2λ(Xi)\nNoting that ∆ = f̂ − fλ and using the above bound and Jensen’s inequality yields,\nE[‖∆‖2F |Xn1 ] ≤ 2‖fλ‖2F + 2E[‖f̂‖2F |Xn1 ] ≤ 4‖fλ‖2F + 2\nnλ n∑ i=1 ς2λ(Xi)\nApplying Jensen’s inequality once again yields,\nE[(E[∆‖2F |Xn1 ])2] ≤ E  8 n2λ2 ( n∑ i=1 ς2λ )2 + 32‖fλ‖4F  ≤ 8 nλ2 n∑ i=1 E[ς4λ] + 32‖fλ‖4F = B4λ"
    }, {
      "heading" : "C.2. Proof of Lemma 12",
      "text" : "First, using Jensen’s inequality twice we have E[ς4λ(X)] = E [ E[(Y − fλ(X))2|X]2 ] ≤ E [ (Y − fλ(X))4 ] ≤ 8E[f4λ(X)] + 8E[Y 4] (29)\nConsider any f (j)λ ,\nf (j) λ (x) = ∞∑ `=1 θ (j) ` φ (j) ` (x) (a) ≤ ( ∞∑ `=1 µ (j) ` 1/3 θ (j) ` 2/3 )3/4 ∞∑ `=1 θ (j) ` 2 φ (j) ` (x) 4 µ (j) ` 1/4\n(b) ≤  M∑ j=1 µ (j) ` 1/2 M∑ j=1 θ (j) ` 2 µ (j) ` 1/4 ∞∑ `=1 θ (j) ` 2 φ (j) ` (x) 4 µ (j) ` 1/4 = Ψ(j)1/2‖f (j)λ ‖1/2H k(j)  ∞∑ `=1 θ (j) ` 2 φ (j) ` (x) 4 µ (j) ` 1/4\nIn (a), we used Hölder’s inequality on µ(j)` 1/4 θ (j) ` 1/2 and θ(j)` 1/2 φ (j) ` (x)/µ (j) ` 1/4 with conjugates 4/3 and 4 respectively. In (b) we used Hölder’s inequality once again on µ(j)` 2/3 and (θ(j)` 2 /µ (j) ` )\n1/3 with conjugates 3/2 and 3. Now we expand fλ in terms of the f (j) λ ’s as follows,\nfλ(x) ≤ M∑ j=1 Ψ(j) 1/2 ‖f (j)λ ‖ 1/2 H k(j)  ∞∑ `=1 θ (j) ` 2 φ (j) ` (x) 4 µ (j) ` 1/4 ≤  M∑ j=1 Ψ(j) 1/2  M∑ j=1 ‖f (j)λ ‖Hk(j)  ∞∑ `=1 θ (j) ` 2 φ (j) ` (x) 4 µ (j) ` 1/2  1/2\nwhere we have applied Cauchy-Schwarz in the last step. Using Cauchy-Schwarz once again,\nf2λ(X) ≤ Ψ  M∑ j=1 ‖f (j)λ ‖ 2 H k(j) 1/2 M∑ j=1 ∞∑ `=1 θ (j) ` 2 φ (j) ` (X) 4 µ (j) ` 1/2\nUsing Cauchy-Schwarz for one last time, we obtain\nE[f4λ(x)] ≤ Ψ2‖fλ‖2F M∑ j=1 ∞∑ `=1 θ (j) ` 2 E[φ(j)` (x)]4 µ (j) ` ≤ Ψ2‖fλ‖4Fρ2\nwhere we have used Assumption 2 in the last step. When we combine this with (29) and use the fact that E[Y 4] ≤ ν4 we get the statement of the lemma."
    }, {
      "heading" : "C.3. Proof of Lemma 14",
      "text" : "The first part of the proof will mimic that of Lemma 10. By repeating the arguments for (24), we get\nE [ ‖M1/2Φ>v‖2 ] ≤ M∑ j=1 t∑ `=1 µ (j) ` M n∑ i=1 M∑ j′=1 E [ E[‖∆(j ′)‖2H k(j ′) |Xn1 ]‖Φ (j) ` ‖ 2 ∑ `′>t µ (j′) `′ φ (j′) `′ (Xi) 2 ]\n≤M M∑ j=1 t∑ `=1 n∑ i=1 M∑ j′=1 ∑ `′>t µ (j) ` µ (j′) `′ E [ E[‖∆(j ′)‖2H k(j ′) |Xn1 ]‖Φ (j) ` ‖ 2φ (j′) `′ (Xi) 2 ]\nUsing Cauchy-Schwarz the inner expectation can be bounded via √ E [( E[‖∆(j′)‖2H\nk(j ′)\n] )2]E [‖Φ(j)` ‖4φ(j′)`′ (Xi)4].\nLemma 13 bounds the first expectation by B4λ. To bound the second expectation we use Assumption 2.\nE [ ‖Φ(j)` ‖ 4φ (j′) `′ (Xk) 4 ] = E [( n∑ i=1 φ (j) ` (Xi) 2 )2 φ (j) ` (Xk) 4 ] = E ∑ i,i′ φ (j) ` (Xi) 2φ (j) ` (Xi′) 2φ (j) ` (Xk) 4  ≤ n2ρ8\nFinally once again reusing some calculations from Lemma 10,\nE [∥∥∥∥ 1nQ−1Φ>v ∥∥∥∥2\n2\n] ≤ E [ 1\nλ ∥∥∥∥ 1nM1/2Φ>v ∥∥∥∥2\n2\n] ≤ M n2λ ( n∑ i=1 nρ4 ) ︸ ︷︷ ︸\nn2ρ4\n M∑ j=1 t∑ `=1 µ (j) `  ︸ ︷︷ ︸\nΨ\n M∑ j′=1 ∑ `′>t µ (j′) `′  ︸ ︷︷ ︸\nβt"
    }, {
      "heading" : "C.4. Proof of Lemma 15",
      "text" : "First note that we can write the LHS of the lemma as,\nE [∥∥∥∥λQ−1M−1θ↓ − 1nQ−1Φ> ∥∥∥∥2 ] = M∑ j=1 t∑ `=1\n1\n1 + λ/µ (j) `\nE (λθ(j)` µ\n(j) `\n− 1 n n∑ i=1 φ (j) ` (X (j) i ) i )2 To bound the inner expectation we use the optimality conditions of the population minimiser (7). We have,\n2E ( M∑ j=1 f (j) λ (X (j) i )− Y ) ξ (j) Xi + 2λf (j)λ = 0 =⇒ E [ξ(j)Xi i] = λf (j)λ =⇒ E [φ(j)` (X(j)i ) i] = λ θ(j)` µ (j) ` . (30)\nIn the last step we have taken the F-inner product with (0, . . . , φ(j)` , . . . ,0). Therefore the term inside the expectation is the variance of n−1 ∑ i φ (j) ` (X (j) i ) i and can be bounded via,\nV\n[ 1\nn n∑ i=1 φ (j) ` (X (j)) i ] ≤ 1 n E [ φ (j) ` (X (j))2 2i ] ≤ 1 n √ E [ φ (j) ` (X (j))4 ] E [ 4i ] ≤ 1 n ρ2 √ E[ς4λ(X)]\nHence the LHS can be bounded via,\n1 n ρ2 √ E[ς4λ(X)] M∑ j=1 t∑ `=1\n1\n1 + λ/µ (j) `\n= 1\nn ρ2γk(λ)\n√ E[ς4λ(X)]"
    }, {
      "heading" : "D. Some Details on Experimental Setup",
      "text" : "The function fd used in Figure 1(a) is the log of three Gaussian bumps,\nfd(x) = log ( α1 1\nhdd exp\n( ‖x− v1‖2\n2h2d\n) + α1 1\nhdd exp\n( ‖x− v2‖2\n2h2d\n) + (1− α1 − α2) 1\nhdd exp\n( ‖x− v3‖2\n2h2d\n)) (31)\nwhere hd = 0.01 √ d, α1, α2 ∈ [0, 1] and vi ∈ Rd are constant vectors. For figures 1(b)-1(f) we used fD where D is given in the figures. In all experiments, we used a test set of 2000 points and plot the mean squared test error.\nFor the real datasets, we normalised the training data so that the X, y values have zero mean and unit variance along each dimensions. We split the given dataset roughly equally to form a training set and testing set. We tuned hyper-parameters via 5-fold cross validation on the training set and report the mean squared error on the test set. For some datasets the test prediction error is larger than 1. Such datasets turned out to be quite noisy. In fact, when we used a constant predictor at 0 (i.e. the mean of the training instances) the mean squared error on the test set was typically much larger than 1.\nBelow, we list details on the dataset: the source, the used predictor and features.\n1. Housing: (UCI), Predictor: CRIM Features: All other attributes except CHAS which is a binary feature.\n2. Galaxy: (SDSS data on Luminous Red Galaxies from Tegmark et al (2006)), Predictor: Baryonic Density Features: All other attributes.\n3. fMRI: (From (Just et al., 2010)), Predictor: Noun representation Features: Voxel Intensities. Since the actual dimensionality was very large, we use a random projection to bring it down to 100 dimensions.\n4. Insulin: (From (Tu, 2012)), Predictor: Insulin levels. Features: SNP features\n5. Skillcraft: (UCI), Predictor: TotalMapExplored Features: All other attributes. The usual predictor for this dataset is LeagueIndex but its an ordinal attribute and not suitable for real valued prediction.\n6. School: (From Bristol Multilevel Modelling), Predictor: Given output Features: Given features. We don’t know much about its attributes. We used the given features and labels.\n7. CCPP*: (UCI), Predictor: Hourly energy output EP Features: The other 4 features and 55 random features for the other 55 dimensions.\n8. Blog: (UCI Blog Feedback Dataset), Predictor: Number of comments in 24 hrs Features: The dataset had 280 features. The first 50 features were not used since they were just summary statistics. Our features included features 51-62 given in the UCI website and the word counts of 38 of the most frequently occurring words.\n9. Bleeding: (From (Guillame-Bert et al., 2014)), Predictor: Given output Features: Given features reduced to 100 dimensions via a random projection. We got this dataset from a private source and don’t know much about its attributes. We used the given features and labels.\n10. Speech: (Parkinson Speech dataset from UCI), Predictor: Median Pitch Features: All other attributes except the mean pitch, standard deviation, minimum pitch and maximum pitches which are not actual features but statistics of the pitch.\n11. Music: (UCI), Predictor: Year of production Features: All other attributes: 12 timbre average and 78 timbre covariance\n12. Telemonit: (Parkinson’s Telemonitoring dataset from UCI), Predictor: total-UPDRS Features: All other features except subject-id and motor-UPDRS (since it was too correlated with total-UPDRS). We only consider the female subjects in the dataset.\n13. Propulsion: (Naval Propulsion Plant dataset from UCI), Predictor: Lever Position Features: All other attributes. We picked a random attribute as the predictor since no clear predictor was specifified.\n14. Airfoil*: (Airfoil Self-Noise dataset from UCI), Predictor: Sound Pressure Level Features: The other 5 features and 35 random features.\n15. Forestfires: (UCI), Predictor: DC Features: All other attributes. We picked a random attribute as the predictor since no clear predictor was specifified.\n16. Brain: (From Wehbe et al. (2014)), Predictor: Story feature at a given time step Features: Other attributes\nSome experimental details: GP is the Bayesian interpretation of KRR. However, the results are different in Table 1. We believe this is due to differences in hyper-parameter tuning. For GP, the GPML package (Rasmussen & Williams, 2006) optimises the GP marginal likelihood via L-BFGS. In contrast, our KRR implementation minimises the least squares cross validation error via grid search. Some Add-GP results are missing since it was very slow compared to other methods. On the Blog dataset, SALSA took less than 35s to train and all other methods were completed in under 22 minutes. In contrast Add-GP was not done training even after several hours. Even on the relatively small speech dataset Add-GP took about 80 minutes. Among the others, BF, MARS, and SpAM were the more expensive methods requiring several minutes on datasets with large D and n whereas other methods took under 2-3 minutes. We also experimented with locally cubic and quartic interpolation but exclude them from the table since LL, LQ generally performed better. Appendix D has more details on the synthetic functions and test sets."
    } ],
    "references" : [ {
      "title" : "Consistency of the Group Lasso and Multiple Kernel Learning",
      "author" : [ "Bach", "Francis R" ],
      "venue" : null,
      "citeRegEx" : "Bach and R.,? \\Q2008\\E",
      "shortCiteRegEx" : "Bach and R.",
      "year" : 2008
    }, {
      "title" : "Reproducing kernel Hilbert spaces in Probability and Statistics",
      "author" : [ "Berlinet", "Alain", "Thomas-Agnan", "Christine" ],
      "venue" : "Kluwer Academic,",
      "citeRegEx" : "Berlinet et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Berlinet et al\\.",
      "year" : 2004
    }, {
      "title" : "LIBSVM: A library for support vector machines",
      "author" : [ "Chang", "Chih-Chung", "Lin", "Chih-Jen" ],
      "venue" : "ACM Transactions on Intelligent Systems and Technology,",
      "citeRegEx" : "Chang et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Chang et al\\.",
      "year" : 2011
    }, {
      "title" : "Scalable Kernel Methods via Doubly Stochastic Gradients",
      "author" : [ "Dai", "Bo", "Xie", "He", "Niao", "Liang", "Yingyu", "Raj", "Anant", "Balcan", "Maria-Florina F", "Song", "Le" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Dai et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Dai et al\\.",
      "year" : 2014
    }, {
      "title" : "Additive Gaussian Processes",
      "author" : [ "Duvenaud", "David K", "Nickisch", "Hannes", "Rasmussen", "Carl Edward" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Duvenaud et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Duvenaud et al\\.",
      "year" : 2011
    }, {
      "title" : "Least Angle Regression",
      "author" : [ "Efron", "Bradley", "Hastie", "Trevor", "Johnstone", "Iain", "Tibshirani", "Robert" ],
      "venue" : "Annals of Statistics,",
      "citeRegEx" : "Efron et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Efron et al\\.",
      "year" : 2004
    }, {
      "title" : "Multivariate Adaptive Regression Splines",
      "author" : [ "Friedman", "Jerome H" ],
      "venue" : "Ann. Statist.,",
      "citeRegEx" : "Friedman and H.,? \\Q1991\\E",
      "shortCiteRegEx" : "Friedman and H.",
      "year" : 1991
    }, {
      "title" : "Greedy Function Approximation: A Gradient Boosting Machine",
      "author" : [ "Friedman", "Jerome H" ],
      "venue" : "Annals of Statistics,",
      "citeRegEx" : "Friedman and H.,? \\Q2000\\E",
      "shortCiteRegEx" : "Friedman and H.",
      "year" : 2000
    }, {
      "title" : "Multiple Kernel Learning Algorithms",
      "author" : [ "Gönen", "Mehmet", "Alpaydin", "Ethem" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Gönen et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Gönen et al\\.",
      "year" : 2011
    }, {
      "title" : "Utility of Empirical Models of Hemorrhage in Detecting and Quantifying Bleeding",
      "author" : [ "M Guillame-Bert", "A Dubrawski", "L Chen", "A Holder", "MR Pinsky", "G. Clermont" ],
      "venue" : "In Intensive Care Medicine,",
      "citeRegEx" : "Guillame.Bert et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Guillame.Bert et al\\.",
      "year" : 2014
    }, {
      "title" : "A Distribution Free Theory of Nonparametric Regression",
      "author" : [ "Györfi", "László", "Kohler", "Micael", "Krzyzak", "Adam", "Walk", "Harro" ],
      "venue" : "Springer Series in Statistics,",
      "citeRegEx" : "Györfi et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Györfi et al\\.",
      "year" : 2002
    }, {
      "title" : "Computationally efficient regression on a dependency graph for human pose estimation",
      "author" : [ "Hara", "Kentaro", "Chellappa", "Rama" ],
      "venue" : "In Computer Vision and Pattern Recognition (CVPR),",
      "citeRegEx" : "Hara et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Hara et al\\.",
      "year" : 2013
    }, {
      "title" : "Generalized Additive Models",
      "author" : [ "T.J. Hastie", "R.J. Tibshirani" ],
      "venue" : null,
      "citeRegEx" : "Hastie and Tibshirani,? \\Q1990\\E",
      "shortCiteRegEx" : "Hastie and Tibshirani",
      "year" : 1990
    }, {
      "title" : "Open source regression software for Matlab/Octave",
      "author" : [ "Jakabsons", "Gints" ],
      "venue" : null,
      "citeRegEx" : "Jakabsons and Gints.,? \\Q2015\\E",
      "shortCiteRegEx" : "Jakabsons and Gints.",
      "year" : 2015
    }, {
      "title" : "A neurosemantic theory of concrete noun representation based on the underlying brain",
      "author" : [ "Just", "Marcel Adam", "Cherkassky", "Vladimir L", "S Aryal", "Mitchell", "Tom M", "Aryal", "Esh" ],
      "venue" : "codes. PLoS ONE,",
      "citeRegEx" : "Just et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Just et al\\.",
      "year" : 2010
    }, {
      "title" : "Rodeo: Sparse Nonparametric Regression in High Dimensions",
      "author" : [ "Lafferty", "John D", "Wasserman", "Larry A" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Lafferty et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Lafferty et al\\.",
      "year" : 2005
    }, {
      "title" : "Fastfood Approximating Kernel Expansions in Loglinear Time",
      "author" : [ "Le", "Quoc", "Sarlos", "Tamas", "Smola", "Alex" ],
      "venue" : "In 30th International Conference on Machine Learning (ICML),",
      "citeRegEx" : "Le et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Le et al\\.",
      "year" : 2013
    }, {
      "title" : "Component selection and smoothing in smoothing spline analysis of variance models",
      "author" : [ "Lin", "Yi", "Zhang", "Hao Helen" ],
      "venue" : "Annals of Statistics,",
      "citeRegEx" : "Lin et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Lin et al\\.",
      "year" : 2006
    }, {
      "title" : "Accurate Intelligible Models with Pairwise Interactions",
      "author" : [ "Lou", "Yin", "Caruana", "Rich", "Gehrke", "Johannes", "Hooker", "Giles" ],
      "venue" : "In KDD,",
      "citeRegEx" : "Lou et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Lou et al\\.",
      "year" : 2013
    }, {
      "title" : "Symmetric functions and Hall polynomials",
      "author" : [ "Macdonald", "Ian Grant" ],
      "venue" : null,
      "citeRegEx" : "Macdonald and Grant.,? \\Q1995\\E",
      "shortCiteRegEx" : "Macdonald and Grant.",
      "year" : 1995
    }, {
      "title" : "PCA-correlated SNPs for Structure Identification",
      "author" : [ "P. Paschou" ],
      "venue" : "PLoS Genetics,",
      "citeRegEx" : "Paschou,? \\Q2007\\E",
      "shortCiteRegEx" : "Paschou",
      "year" : 2007
    }, {
      "title" : "Accuracy versus Interpretability in flexible modeling:implementing a tradeoff using Gaussian process models",
      "author" : [ "Plate", "Tony A" ],
      "venue" : "Behaviourmetrika, Interpreting Neural Network Models”,",
      "citeRegEx" : "Plate and A.,? \\Q1999\\E",
      "shortCiteRegEx" : "Plate and A.",
      "year" : 1999
    }, {
      "title" : "Random Features for Large-Scale Kernel Machines",
      "author" : [ "Rahimi", "Ali", "Recht", "Benjamin" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Rahimi et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Rahimi et al\\.",
      "year" : 2007
    }, {
      "title" : "Weighted Sums of Random Kitchen Sinks: Replacing minimization with randomization in learning",
      "author" : [ "Rahimi", "Ali", "Recht", "Benjamin" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Rahimi et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Rahimi et al\\.",
      "year" : 2009
    }, {
      "title" : "Gaussian Processes for Machine Learning",
      "author" : [ "C.E. Rasmussen", "C.K.I. Williams" ],
      "venue" : null,
      "citeRegEx" : "Rasmussen and Williams,? \\Q2006\\E",
      "shortCiteRegEx" : "Rasmussen and Williams",
      "year" : 2006
    }, {
      "title" : "Sparse Additive Models",
      "author" : [ "Ravikumar", "Pradeep", "Lafferty", "John", "Liu", "Han", "Wasserman", "Larry" ],
      "venue" : "Journal of the Royal Statistical Society: Series B,",
      "citeRegEx" : "Ravikumar et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Ravikumar et al\\.",
      "year" : 2009
    }, {
      "title" : "Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond",
      "author" : [ "Schölkopf", "Bernhard", "Smola", "Alexander J" ],
      "venue" : null,
      "citeRegEx" : "Schölkopf et al\\.,? \\Q2001\\E",
      "shortCiteRegEx" : "Schölkopf et al\\.",
      "year" : 2001
    }, {
      "title" : "Kernel Methods for Pattern Analysis",
      "author" : [ "Shawe-Taylor", "John", "Cristianini", "Nello" ],
      "venue" : "Cambridge university press,",
      "citeRegEx" : "Shawe.Taylor et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Shawe.Taylor et al\\.",
      "year" : 2004
    }, {
      "title" : "Optimal Rates for Regularized Least Squares Regression",
      "author" : [ "Steinwart", "Ingo", "Hush", "Don R", "Scovel", "Clint" ],
      "venue" : "In COLT,",
      "citeRegEx" : "Steinwart et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Steinwart et al\\.",
      "year" : 2009
    }, {
      "title" : "Cosmological Constraints from the SDSS Luminous Red Galaxies",
      "author" : [ "M. Tegmark et al" ],
      "venue" : "Physical Review,",
      "citeRegEx" : "al,? \\Q2006\\E",
      "shortCiteRegEx" : "al",
      "year" : 2006
    }, {
      "title" : "Regression Shrinkage and Selection Via the Lasso",
      "author" : [ "Tibshirani", "Robert" ],
      "venue" : "Journal of the Royal Statistical Society, Series B,",
      "citeRegEx" : "Tibshirani and Robert.,? \\Q1994\\E",
      "shortCiteRegEx" : "Tibshirani and Robert.",
      "year" : 1994
    }, {
      "title" : "Introduction to Nonparametric Estimation",
      "author" : [ "Tsybakov", "Alexandre B" ],
      "venue" : null,
      "citeRegEx" : "Tsybakov and B.,? \\Q2008\\E",
      "shortCiteRegEx" : "Tsybakov and B.",
      "year" : 2008
    }, {
      "title" : "Integrative Analysis of a cross-locci regulation Network identifies App as a Gene regulating Insulin Secretion from Pancreatic Islets",
      "author" : [ "Tu", "Zhidong" ],
      "venue" : "PLoS Genetics,",
      "citeRegEx" : "Tu and Zhidong.,? \\Q2012\\E",
      "shortCiteRegEx" : "Tu and Zhidong.",
      "year" : 2012
    }, {
      "title" : "Inducing Model Trees for Continuous Classes",
      "author" : [ "Wang", "Yong", "Witten", "Ian H" ],
      "venue" : "In ECML,",
      "citeRegEx" : "Wang et al\\.,? \\Q1997\\E",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 1997
    }, {
      "title" : "Simultaneously uncovering the patterns of brain regions involved in different story reading",
      "author" : [ "L. Wehbe", "B. Murphy", "P. Talukdar", "A. Fyshe", "A. Ramdas", "T. Mitchell" ],
      "venue" : "PLoS ONE,",
      "citeRegEx" : "Wehbe et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Wehbe et al\\.",
      "year" : 2014
    }, {
      "title" : "Generalization Performance of Regularization Networks and Support Vector Machines via Entropy Numbers of Compact Operators",
      "author" : [ "Williamson", "Robert C", "Smola", "Alex J", "Schölkopf", "Bernhard" ],
      "venue" : "IEEE Transactions on Information Theory,",
      "citeRegEx" : "Williamson et al\\.,? \\Q2001\\E",
      "shortCiteRegEx" : "Williamson et al\\.",
      "year" : 2001
    }, {
      "title" : "Simple and Efficient Multiple Kernel Learning by Group Lasso",
      "author" : [ "Xu", "Zenglin", "Jin", "Rong", "Yang", "Haiqin", "King", "Irwin", "Lyu", "Michael R" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Xu et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2010
    }, {
      "title" : "Learning Bounds for Kernel Regression Using Effective Data Dimensionality",
      "author" : [ "Zhang", "Tong" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "Zhang and Tong.,? \\Q2005\\E",
      "shortCiteRegEx" : "Zhang and Tong.",
      "year" : 2005
    }, {
      "title" : "Divide and Conquer Kernel Ridge Regression",
      "author" : [ "Zhang", "Yuchen", "Duchi", "John C", "Wainwright", "Martin J" ],
      "venue" : "In COLT,",
      "citeRegEx" : "Zhang et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2013
    }, {
      "title" : "Shrunk Additive Least Squares Approximation Appendix A. Proof of Theorem 3: Convergence of SALSA Our analysis here is a brute force generalisation of the analysis",
      "author" : [ "Zhang" ],
      "venue" : null,
      "citeRegEx" : "Zhang,? \\Q2013\\E",
      "shortCiteRegEx" : "Zhang",
      "year" : 2013
    }, {
      "title" : "but we repeat them (or provide an outline) here for the sake of completeness",
      "author" : [ "Zhang" ],
      "venue" : null,
      "citeRegEx" : "Zhang,? \\Q2013\\E",
      "shortCiteRegEx" : "Zhang",
      "year" : 2013
    }, {
      "title" : "Shrunk Additive Least Squares Approximation A.3",
      "author" : [ "Zhang" ],
      "venue" : "Variance (Proof of Bound",
      "citeRegEx" : "Zhang,? \\Q2013\\E",
      "shortCiteRegEx" : "Zhang",
      "year" : 2013
    }, {
      "title" : "χ(k) can be shown to be low order by choosing t = n which results in μ̃t+1, βt ∈ O(n−4). C. Proof of Theorem 5: Analysis in the Agnostic Setting As before, we generalise the analysis by Zhang et al. (2013) to the tuple RKHS F . We begin by making the following crucial observation about the population",
      "author" : [ "d π̃/n" ],
      "venue" : null,
      "citeRegEx" : "π̃.n..,? \\Q2013\\E",
      "shortCiteRegEx" : "π̃.n..",
      "year" : 2013
    }, {
      "title" : "R are constant vectors. For figures 1(b)-1(f) we used fD where D is given in the figures. In all experiments, we used a test set of 2000 points and plot the mean squared test error",
      "author" : [ "√ d" ],
      "venue" : null,
      "citeRegEx" : "d,? \\Q2000\\E",
      "shortCiteRegEx" : "d",
      "year" : 2000
    }, {
      "title" : "Predictor: Story feature at a given time step Features: Other attributes",
      "author" : [ "Brain: (From Wehbe" ],
      "venue" : null,
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2014
    } ],
    "referenceMentions" : [ {
      "referenceID" : 10,
      "context" : "Current lower bounds (Györfi et al., 2002) suggest that this dependence is unavoidable.",
      "startOffset" : 21,
      "endOffset" : 42
    }, {
      "referenceID" : 25,
      "context" : "In this light, a common simplification has been to assume that f∗ decomposes into the additive form f∗(x) = f (1) ∗ (x1)+f (2) ∗ (x2)+· · ·+f (D) ∗ (xD) (Hastie & Tibshirani, 1990; Lafferty & Wasserman, 2005; Ravikumar et al., 2009).",
      "startOffset" : 153,
      "endOffset" : 232
    }, {
      "referenceID" : 25,
      "context" : "Some variants such as RODEO (Lafferty & Wasserman, 2005) and SpAM (Ravikumar et al., 2009) study first order models in variable selection/sparsity settings.",
      "startOffset" : 66,
      "endOffset" : 90
    }, {
      "referenceID" : 36,
      "context" : "A large line of work, in what has to come to be known as Multiple Kernel Learning (MKL), focuses on precisely this problem (Bach, 2008; Gönen & Alpaydin, 2011; Xu et al., 2010).",
      "startOffset" : 123,
      "endOffset" : 176
    }, {
      "referenceID" : 4,
      "context" : "Additive models have also been studied in Gaussian process literature via additive kernels (Duvenaud et al., 2011; Plate, 1999).",
      "startOffset" : 91,
      "endOffset" : 127
    }, {
      "referenceID" : 17,
      "context" : "Lou et al. (2013) model f∗ as a first order model plus a sparse collection of pairwise interactions.",
      "startOffset" : 0,
      "endOffset" : 18
    }, {
      "referenceID" : 10,
      "context" : "It is well known thatR is minimised by the regression function f∗(·) = EXY [Y |X = ·] and the excess risk for any f is R(f)−R(f∗) = ‖f − f∗‖2 (Györfi et al., 2002).",
      "startOffset" : 142,
      "endOffset" : 163
    }, {
      "referenceID" : 28,
      "context" : "KRR has been analysed extensively under different assumptions on f∗; see (Steinwart & Christmann, 2008; Steinwart et al., 2009; Zhang, 2005) and references therein.",
      "startOffset" : 73,
      "endOffset" : 140
    }, {
      "referenceID" : 4,
      "context" : "We circumvent this bottleneck using two strategems: a classical result from RKHS theory, and a computational trick using elementary symmetric polynomials used before by Duvenaud et al. (2011); Shawe-Taylor & Cristianini (2004) in the kernel literature for additive kernels.",
      "startOffset" : 169,
      "endOffset" : 192
    }, {
      "referenceID" : 4,
      "context" : "We circumvent this bottleneck using two strategems: a classical result from RKHS theory, and a computational trick using elementary symmetric polynomials used before by Duvenaud et al. (2011); Shawe-Taylor & Cristianini (2004) in the kernel literature for additive kernels.",
      "startOffset" : 169,
      "endOffset" : 227
    }, {
      "referenceID" : 43,
      "context" : "At first, this appears troublesome since it requres optimising over nMd parameters (α i ), j = 1, . . . ,Md, i = 1, . . . , n. However, from the work of Aronszajn (1950), we know that the solution of (3) lies in the RKHS of the sum kernel k",
      "startOffset" : 70,
      "endOffset" : 170
    }, {
      "referenceID" : 28,
      "context" : "The ESP Kernel While the above formulation reduces the number of optimisation parameters, the kernel still has a combinatorial number of terms which can be expensive to compute. While this is true for arbitrary choices for k’s, under some restrictions we can efficiently compute k. For this, we use the same trick used by Shawe-Taylor & Cristianini (2004) and Duvenaud et al.",
      "startOffset" : 124,
      "endOffset" : 356
    }, {
      "referenceID" : 4,
      "context" : "For this, we use the same trick used by Shawe-Taylor & Cristianini (2004) and Duvenaud et al. (2011). First consider a set of base kernels acting on each dimension k1, k2, .",
      "startOffset" : 78,
      "endOffset" : 101
    }, {
      "referenceID" : 38,
      "context" : "Similar assumptions are made in (Zhang et al., 2013) and are satisfied for a large range of kernels including those in Theorem 4.",
      "startOffset" : 32,
      "endOffset" : 52
    }, {
      "referenceID" : 38,
      "context" : "The first term is known as the effective data dimensionality of k (Zhang, 2005; Zhang et al., 2013) and captures the statistical difficulty of estimating a function inHk(j) .",
      "startOffset" : 66,
      "endOffset" : 99
    }, {
      "referenceID" : 29,
      "context" : "Our proof technique generalises the analysis of Zhang et al. (2013) for KRR to the additive case.",
      "startOffset" : 25,
      "endOffset" : 68
    }, {
      "referenceID" : 29,
      "context" : "Our proof technique generalises the analysis of Zhang et al. (2013) for KRR to the additive case. We use ideas from Aronszajn (1950) to handle sum RKHSs.",
      "startOffset" : 25,
      "endOffset" : 133
    }, {
      "referenceID" : 35,
      "context" : "An example of the second eigendecay is the Gaussian kernel with π̃ = √ 2π (Williamson et al., 2001).",
      "startOffset" : 74,
      "endOffset" : 99
    }, {
      "referenceID" : 10,
      "context" : "Current lower bounds suggest that the exponential dependence is unavoidable (Györfi et al., 2002; Tsybakov, 2008).",
      "startOffset" : 76,
      "endOffset" : 113
    }, {
      "referenceID" : 28,
      "context" : "The proof, given in Appendix C, also follows the template in Zhang et al. (2013). Loosely, we may interpret AE and EE as the approximation and estimation errors1.",
      "startOffset" : 32,
      "endOffset" : 81
    }, {
      "referenceID" : 25,
      "context" : "The choice of bandwidth was inspired by several other kernel methods which use bandwidths on the order σin (Ravikumar et al., 2009; Tsybakov, 2008).",
      "startOffset" : 107,
      "endOffset" : 147
    }, {
      "referenceID" : 3,
      "context" : "(Dai et al., 2014; Le et al., 2013; Rahimi & Recht, 2007; 2009; Zhang et al., 2013) can be explored to scale SALSA with n.",
      "startOffset" : 0,
      "endOffset" : 83
    }, {
      "referenceID" : 16,
      "context" : "(Dai et al., 2014; Le et al., 2013; Rahimi & Recht, 2007; 2009; Zhang et al., 2013) can be explored to scale SALSA with n.",
      "startOffset" : 0,
      "endOffset" : 83
    }, {
      "referenceID" : 38,
      "context" : "(Dai et al., 2014; Le et al., 2013; Rahimi & Recht, 2007; 2009; Zhang et al., 2013) can be explored to scale SALSA with n.",
      "startOffset" : 0,
      "endOffset" : 83
    }, {
      "referenceID" : 25,
      "context" : "Nonparametric additive models: Back-fitting with cubic splines (BF) (Hastie & Tibshirani, 1990), Multivariate Adaptive Regression Splines (MARS) (Friedman, 1991), Component Selection and Smoothing (COSSO) (Lin & Zhang, 2006), Sparse Additive Models (SpAM) (Ravikumar et al., 2009) and Additive Gaussian Processes (AddGP) (Duvenaud et al.",
      "startOffset" : 256,
      "endOffset" : 280
    }, {
      "referenceID" : 4,
      "context" : ", 2009) and Additive Gaussian Processes (AddGP) (Duvenaud et al., 2011).",
      "startOffset" : 48,
      "endOffset" : 71
    }, {
      "referenceID" : 5,
      "context" : "Parametric models: Ridge Regression (RR), Least Absolute Shrinkage and Selection (LASSO) (Tibshirani, 1994) and Least Angle Regression (LAR) (Efron et al., 2004).",
      "startOffset" : 141,
      "endOffset" : 161
    }, {
      "referenceID" : 9,
      "context" : "The datasets were taken from the UCI repository, Bristol Multilevel Modeling and the following sources: (Guillame-Bert et al., 2014; Just et al., 2010; Paschou, 2007; Tegmark et al, 2006; Tu, 2012; Wehbe et al., 2014).",
      "startOffset" : 104,
      "endOffset" : 217
    }, {
      "referenceID" : 14,
      "context" : "The datasets were taken from the UCI repository, Bristol Multilevel Modeling and the following sources: (Guillame-Bert et al., 2014; Just et al., 2010; Paschou, 2007; Tegmark et al, 2006; Tu, 2012; Wehbe et al., 2014).",
      "startOffset" : 104,
      "endOffset" : 217
    }, {
      "referenceID" : 20,
      "context" : "The datasets were taken from the UCI repository, Bristol Multilevel Modeling and the following sources: (Guillame-Bert et al., 2014; Just et al., 2010; Paschou, 2007; Tegmark et al, 2006; Tu, 2012; Wehbe et al., 2014).",
      "startOffset" : 104,
      "endOffset" : 217
    }, {
      "referenceID" : 34,
      "context" : "The datasets were taken from the UCI repository, Bristol Multilevel Modeling and the following sources: (Guillame-Bert et al., 2014; Just et al., 2010; Paschou, 2007; Tegmark et al, 2006; Tu, 2012; Wehbe et al., 2014).",
      "startOffset" : 104,
      "endOffset" : 217
    }, {
      "referenceID" : 29,
      "context" : "Proof of Theorem 3: Convergence of SALSA Our analysis here is a brute force generalisation of the analysis in Zhang et al. (2013). We handle the additive case using ideas from Aronszajn (1950).",
      "startOffset" : 47,
      "endOffset" : 130
    }, {
      "referenceID" : 29,
      "context" : "Proof of Theorem 3: Convergence of SALSA Our analysis here is a brute force generalisation of the analysis in Zhang et al. (2013). We handle the additive case using ideas from Aronszajn (1950). As such we will try and stick to the same notation.",
      "startOffset" : 47,
      "endOffset" : 193
    }, {
      "referenceID" : 29,
      "context" : "Proof of Theorem 3: Convergence of SALSA Our analysis here is a brute force generalisation of the analysis in Zhang et al. (2013). We handle the additive case using ideas from Aronszajn (1950). As such we will try and stick to the same notation. Some intermediate technical results can be obtained directly from Zhang et al. (2013) but we repeat them (or provide an outline) here for the sake of completeness.",
      "startOffset" : 47,
      "endOffset" : 332
    }, {
      "referenceID" : 43,
      "context" : "The key observation is that we only need to consider n (and not nMd) parameters even though we are optimising overMd RKHSs. The reasoning uses a powerful result from Aronszajn (1950). Consider the class of functionsH′ = {f = ∑ j f ; f (j) ∈ Hk(j)}.",
      "startOffset" : 39,
      "endOffset" : 183
    }, {
      "referenceID" : 29,
      "context" : "We will need the following technical lemmas. The proofs are given at the end of this section. These results correspond to Lemma 5 in Zhang et al. (2013).",
      "startOffset" : 34,
      "endOffset" : 153
    }, {
      "referenceID" : 38,
      "context" : "The proof mimics Lemma 6 in (Zhang et al., 2013) by performing essentially the same steps over F instead of the usual Hilbert space.",
      "startOffset" : 28,
      "endOffset" : 48
    }, {
      "referenceID" : 29,
      "context" : "Variance (Proof of Bound (10)) Once again, we follow Zhang et al. (2013). The tricks we use to generalise it to the additive case (i.",
      "startOffset" : 62,
      "endOffset" : 73
    }, {
      "referenceID" : 29,
      "context" : "Proof of Theorem 5: Analysis in the Agnostic Setting As before, we generalise the analysis by Zhang et al. (2013) to the tuple RKHS F .",
      "startOffset" : 22,
      "endOffset" : 114
    }, {
      "referenceID" : 29,
      "context" : "In all experiments, we used a test set of 2000 points and plot the mean squared test error. For the real datasets, we normalised the training data so that the X, y values have zero mean and unit variance along each dimensions. We split the given dataset roughly equally to form a training set and testing set. We tuned hyper-parameters via 5-fold cross validation on the training set and report the mean squared error on the test set. For some datasets the test prediction error is larger than 1. Such datasets turned out to be quite noisy. In fact, when we used a constant predictor at 0 (i.e. the mean of the training instances) the mean squared error on the test set was typically much larger than 1. Below, we list details on the dataset: the source, the used predictor and features. 1. Housing: (UCI), Predictor: CRIM Features: All other attributes except CHAS which is a binary feature. 2. Galaxy: (SDSS data on Luminous Red Galaxies from Tegmark et al (2006)), Predictor: Baryonic Density Features: All other attributes.",
      "startOffset" : 3,
      "endOffset" : 966
    }, {
      "referenceID" : 14,
      "context" : "fMRI: (From (Just et al., 2010)), Predictor: Noun representation Features: Voxel Intensities.",
      "startOffset" : 12,
      "endOffset" : 31
    }, {
      "referenceID" : 9,
      "context" : "Bleeding: (From (Guillame-Bert et al., 2014)), Predictor: Given output Features: Given features reduced to 100 dimensions via a random projection.",
      "startOffset" : 16,
      "endOffset" : 44
    }, {
      "referenceID" : 9,
      "context" : "Bleeding: (From (Guillame-Bert et al., 2014)), Predictor: Given output Features: Given features reduced to 100 dimensions via a random projection. We got this dataset from a private source and don’t know much about its attributes. We used the given features and labels. 10. Speech: (Parkinson Speech dataset from UCI), Predictor: Median Pitch Features: All other attributes except the mean pitch, standard deviation, minimum pitch and maximum pitches which are not actual features but statistics of the pitch. 11. Music: (UCI), Predictor: Year of production Features: All other attributes: 12 timbre average and 78 timbre covariance 12. Telemonit: (Parkinson’s Telemonitoring dataset from UCI), Predictor: total-UPDRS Features: All other features except subject-id and motor-UPDRS (since it was too correlated with total-UPDRS). We only consider the female subjects in the dataset. 13. Propulsion: (Naval Propulsion Plant dataset from UCI), Predictor: Lever Position Features: All other attributes. We picked a random attribute as the predictor since no clear predictor was specifified. 14. Airfoil*: (Airfoil Self-Noise dataset from UCI), Predictor: Sound Pressure Level Features: The other 5 features and 35 random features. 15. Forestfires: (UCI), Predictor: DC Features: All other attributes. We picked a random attribute as the predictor since no clear predictor was specifified. 16. Brain: (From Wehbe et al. (2014)), Predictor: Story feature at a given time step Features: Other attributes Some experimental details: GP is the Bayesian interpretation of KRR.",
      "startOffset" : 17,
      "endOffset" : 1422
    } ],
    "year" : 2016,
    "abstractText" : "High dimensional nonparametric regression is an inherently difficult problem with known lower bounds depending exponentially in dimension. A popular strategy to alleviate this curse of dimensionality has been to use additive models of first order, which model the regression function as a sum of independent functions on each dimension. Though useful in controlling the variance of the estimate, such models are often too restrictive in practical settings. Between non-additive models which often have large variance and first order additive models which have large bias, there has been little work to exploit the trade-off in the middle via additive models of intermediate order. In this work, we propose SALSA, which bridges this gap by allowing interactions between variables, but controls model capacity by limiting the order of interactions. SALSA minimises the residual sum of squares with squared RKHS norm penalties. Algorithmically, it can be viewed as Kernel Ridge Regression with an additive kernel. When the regression function is additive, the excess risk is only polynomial in dimension. Using the Girard-Newton formulae, we efficiently sum over a combinatorial number of terms in the additive expansion. Via a comparison on 15 real datasets, we show that our method is competitive against 21 other alternatives.",
    "creator" : "TeX"
  }
}