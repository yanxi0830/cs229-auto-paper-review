{
  "name" : "1603.08037.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "On the Detection of Mixture Distributions with applications to the Most Biased Coin Problem",
    "authors" : [ "Kevin Jamieson" ],
    "emails" : [ "kjamieson@eecs.berkeley.edu", "dhaas@eecs.berkeley.edu", "brecht@eecs.berkeley.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n60 3.\n08 03\n7v 1\n[ cs\n.L G\n] 2\n5 M"
    }, {
      "heading" : "1 Introduction",
      "text" : "The trade-off between exploration and exploitation has been an ever-present trope in the online learning literature. In contrast, this paper studies the trade-off between two different kinds of pure exploration: breadth versus depth. Consider a magic bag that contains an infinite number of two kinds of biased coins: “heavy” coins with mean θ1 ∈ (0, 1) and “light” coins with mean θ0 ∈ (0, θ1). When a player picks a coin from the bag, with probability α the coin is “heavy” and with probability (1 − α) the coin is “light.” The player can flip any coin she picks from the bag as many times as she wants, and the goal is to identify a heavy coin. The key difficulty of this problem lies in distinguishing whether the two kinds of coins have very similar means, or whether heavy coins are just extremely rare. That is, how does one balance flipping an individual coin many times to better estimate its mean against considering many new coins to maximize the probability of observing a heavy one. It turns out that this toy problem is a useful abstraction to characterize the inherent difficulty of real-world problems including automated hiring of crowd workers for data processing tasks, anomaly and intrusion detection, and discovery of vacant frequencies in the radio spectrum.\nThe most biased coin problem first came to the attention of the authors of this work when it was presented at COLT 2014 (Chandrasekaran and Karp, 2014). In that work, it was shown that if α, θ1, and θ0 were known\nthen there exists an algorithm based on the sequential probability ratio test (SPRT) that is optimal in that it minimizes the expected number of total flips to find a “heavy” coin whose posterior probability of being heavy is at least 1− δ, and the expected sample complexity of this algorithm was upper-bounded by\n16 (θ1 − θ0)2 ( 1− α α + log ( (1− α)(1 − δ) αδ )) . (1)\nHowever, the practicality of the proposed algorithm is severely limited as it relies critically on knowing α, θ1, and θ0 exactly. In addition, the algorithm requires more than one coin to be outside the bag at a time ruling out some applications.\nMalloy et al. (2012) addressed some of the shortcomings of Chandrasekaran and Karp (2012) (a preprint of Chandrasekaran and Karp (2014)) by considering both an alternative SPRT procedure and a sequential thresholding procedure. Both of these proposed algorithms consider one coin at a time and never return to previous coins. However, the former requires knowledge of all relevant parameters α, θ0, θ1, and the latter requires knowledge of α, θ0. Moreover, these results are only presented for the asymptotic case where δ → 0.\nIn this work we propose algorithms that are adaptive to partial or even no knowledge of α, θ0, θ1, are guaranteed to return a heavy coin with probability at least 1− δ, and support the setting where just one coin is allowed outside the bag at any given time. In addition, we present lower bounds that nearly match the upper bounds shown for our algorithms.\nWhile coins are a useful analogy, all of our lower and upper bounds extend beyond Bernoulli coins to other distributions (e.g. distributions supported on the interval [0, 1]), though we return to the coin analogy throughout for concreteness. Indeed, in pursuit of bounds for the coin problem, we derive upper and lower bounds for a related problem, the detection of mixture distributions with applications to anomaly detection. As a concrete example of that kind of lower bound shown in this work, suppose we observe a sequence of random variables X1, . . . ,Xn and consider the following hypothesis test:\nProblem 1.\nH0 : ∀i X1, . . . ,Xn ∼ N (θ, σ2) for some θ ∈ R, H1 : ∀i X1, . . . ,Xn ∼ (1− α)N (θ0, σ2) + α N (θ1, σ2)\nWe can show that if θ0, θ1, α are known and θ = θ0, then it suffices to observe just max{1/α, σ2\nα2(θ1−θ0)2 log(1/δ)} samples to determine the correct hypothesis with probability at least 1− δ. However, if θ0, θ1, α are unknown (and hence we cannot assume a value for θ), we show that whenever (θ1−θ0)2\nσ2 ≤ 1, at least max { 1/α, ( σ2 α(θ1−θ0)2 )2 log(1/δ) } samples in expectation are necessary to determine the correct hypothesis with probability at least 1 − δ (see Appendix C). The unknown parameter case has a simple interpretation for anomaly detection with a fixed mixing component α that gets at the key insights of this work: if the anomalous distribution is well separated from the null distribution, then detecting an anomalous component is only about as hard as observing just one anomalous sample (i.e. 1/α—no harder than if the parameters were known) since detection is nearly certain between well-separated distributions. However, when the two distributions are not well separated then the sample complexity to detect an anomaly scales like the inverse of the KL divergence squared!\nIn this work, we formally prove the above observations as special cases of more general statements about detecting mixtures. Our main contributions are the following:\n• We characterize the difficulty of distinguishing between a single-parameter distribution and a mixture of two such distributions. When the parameters are known, detecting the presence of a mixture requires a sample complexity that scales as the expected number of samples to differentiate between\nthe two distributions if given samples from each (i.e. the inverse KL divergence). However, when the distribution parameters are unknown, we prove lower bounds showing that detecting a mixture is quadratically harder if the distributions are not well-separated. We then show that this bound applies to any algorithm that solves the most biased coin problem by flipping each coin a fixed number of times (as in Malloy et al., 2012).\n• We propose and analyze the sample complexity of several algorithms for the most biased coin problem that are adaptive to partial or no knowledge of the distribution parameters, all of which come within log factors of the information-theoretic lower bound (see Table 1). These algorithms actually detect any heavy distribution supported on [0, 1], not just Bernoulli coins, and solve a particular instance of the infinite armed bandit problem. We believe both that our algorithms are the first fully adaptive solution to the most biased coin problem, and that the same approach can be reworked to solve more general instances of the infinite-armed bandit problem in the important case when the arm mean distributions are not fully known."
    }, {
      "heading" : "1.1 Motivation and Related Work",
      "text" : "Data labeling for machine learning applications is often performed by humans, and recent work in the crowdsourcing literature accelerates labeling by organizing workers into pools of labelers and paying them to wait for incoming data (Bernstein et al., 2011; Haas et al., 2015). Because workers hired on marketplaces such as Amazon’s Mechanical Turk (MTurk) vary widely in skill, identifying high-quality workers is an important challenge. If we model each worker’s performance (e.g. accuracy or speed) on a set of tasks as drawn from some distribution on [0, 1], then selecting a good worker is equivalent to identifying a worker with a high mean by taking as few total samples as possible from all workers. Note that we do not observe a worker’s inherent skill or mean directly, we must give them tasks from which we estimate it (like repeatedly flipping a biased coin). That is, the identification of good workers is well-modeled by the most biased coin problem.\nOne can interpret the most biased coin problem as an infinite armed bandit problem where each coin is an arm. In that setting, Berry et al. (1997), Wang et al. (2009) and Bonald and Proutiere (2013) prove and refine bounds on the expected cumulative regret of the player, whereas Carpentier and Valko (2015) focus on the pure exploration setting. All of this work relies on the assumption that the distribution of the means is parametric and known (though Carpentier and Valko (2015) describes a method to estimate the relevant parameters first). Our setting relies on a different parameterization of the means (i.e. (1−α)δθ0+αδθ1 where δx is a Dirac delta located at x), and we focus on settings in which the relevant parameters are unknown.\nOur lower bounds are based on the detection of the presence of a mixture of two parametric distributions versus just a single distribution of the same family. There has been extensive work in the estimation of mixture distributions (Hardt and Price, 2014; Freund and Mansour, 1999). This literature usually assumes that the mixture coefficient α is bounded away from 0 and 1 to ensure that a sufficient amount of samples are observed from each distribution in the mixture. In contrast, we highlight the challenging regime when α is arbitrarily small, as is the case in statistical anomaly detection (Eskin, 2000; Thatte et al., 2011; Agarwal, 2006). The current work differs primarily in that we are in an online setting where we choose to keep sampling or stop, and for the coin problem we must decide how many times to flip each coin, not just a stopping time."
    }, {
      "heading" : "1.2 Preliminaries",
      "text" : "Let P and Q be two probability distributions with a common measurable space. For simplicity, assume P and Q have the same support.\nDefinition 1. Define the KL Divergence between P and Q as KL(P,Q) = ∫ log ( dP dQ ) dP . Definition 2. Define the χ2 Divergence between P and Q as χ2(P,Q) = ∫ (\ndP dQ − 1\n)2 dQ = ∫ (dP (x)−dQ(x))2 dQ(x) dx.\nNote that by Jensen’s inequality\nKL(P,Q) = EP\n[ log ( dP\ndQ\n)] ≤ log ( EP [ dP\ndQ\n]) = log ( χ2(P,Q) + 1 ) ≤ χ2(P,Q). (2)\nExample 1 (Gaussian). Let P = N (θ1, σ2) and Q = N (θ0, σ2). Then\nKL(P,Q) = (θ1−θ0) 2\n2σ2 and χ 2(P,Q) = e\n(θ1−θ0) 2\nσ2 − 1.\nExample 2 (Bernoulli). Let P = Bernoulli(θ1) and Q = Bernoulli(θ0). Then\nKL(P,Q) = θ1 log( θ1 θ0 ) + (1− θ1) log(1−θ11−θ0 ), and χ 2(P,Q) = (θ1−θ0) 2 θ0(1−θ0) .\n≤ (θ1−θ0)2/2θ0(1−θ0)−[(θ1−θ0)(2θ0−1)]+"
    }, {
      "heading" : "1.3 The Most Biased Coin Problem Statement",
      "text" : "Let θ ∈ Θ index a family of single-parameter probability density functions gθ and fix θ0, θ1 ∈ Θ, α ∈ [0, 1/2]. For any θ ∈ Θ assume that gθ is known to the procedure. Consider a sequence of iid Bernoulli random variables ξi ∈ {0, 1} for i = 1, 2, . . . where each P(ξi = 1) = 1 − P(ξi = 0) = α. Let Xi,j for j = 1, 2, . . . be a sequence of random variables drawn from gθ1 if ξi = 1 and gθ0 otherwise, and let {{Xi,j}Mij=1}Ni=1 represent the sampling history generated by a procedure for some N ∈ N and (M1, . . . ,MN ) ∈ NN . For any procedure, let N(α, θ0, θ1) be the random variable denoting the number of distributions each sampled Mi(α, θ0, θ1) times for all i when the procedure is applied to the problem defined by fixed (α, θ0, θ1).\nDefinition 3. We say a procedure is δ-probably correct if for all (α, θ0, θ1) it identifies a “heavy” distribution with probability at least 1− δ.\nFor all procedures that are δ-probably correct and follow Algorithm 1, our goal is to provide lower and upper bounds on the quantity E[T (α, θ0, θ1)] = E[ ∑N(α,θ0,θ1) i=1 Mi(α, θ0, θ1)] for any (α, θ0, θ1). Note that if gθ = Bernoulli(θ), then E[T (α, θ0, θ1)] is equivalent to the expected number of total coin flips needed to find a most biased coin. To emphasize this, our results are stated generally, then tied to the special case of Bernoulli coins by way of corollaries. All proofs appear in the appendix."
    }, {
      "heading" : "2 Lower bounds",
      "text" : "In this section, we derive lower bounds on the sample complexity of valid procedures. Section 2.1 provides a lower bound for any adaptive procedure that may choose how many times to sample from each distribution independently, and Section 2.2 derives bounds for fixed sample size procedures that select an m ≥ 1 and sample from each distribution exactly m times. The results in Section 2.2.1 apply to procedures with full knowledge of α, θ0, θ1, and Section 2.2.2 demonstrates that without knowledge of these parameters, the sample complexity becomes much higher.\nInitialize an empty history (N = 0,M = {}). Repeat until heavy distribution declared:\nChoose one of 1. obtain an additional sample from distribution i = N so that Mi ← Mi + 1 2. draw a sample from the (N + 1)st distribution so that N ← N + 1, MN = 1 3. declare distribution i = N as heavy\nAlgorithm 1: Sequential procedure for identifying a heavy distribution. Only the last distribution drawn may be sampled or declared heavy, enforcing the rule that only one coin may be outside the bag at a time."
    }, {
      "heading" : "2.1 Fully adaptive strategies",
      "text" : "The following theorem, reproduced from Malloy et al. (2012), describes the sample complexity of any δprobably correct algorithm for the most biased coin identification problem. Note that this lower bound holds for any procedure, regardless of how adaptive it is or if it returns to previously seen distributions to draw additional samples.\nTheorem 1. (Malloy et al., 2012, Theorem 2) Fix δ ∈ (0, 1). Let T be the total number of samples taken of any procedure that is δ-probably correct in identifying a heavy distribution. Then\nE[T ] ≥ c1 max { 1− δ α , (1− δ)\nαKL(gθ0 |gθ1)\n}\nwhenever α ≤ c2δ where c1, c2 ∈ (0, 1) are absolute constants. The above theorem is directly applicable to the special case where gθ is a Bernoulli distribution, implying\na lower bound of max {\n1−δ α , 2min{θ0(1−θ0),θ1(1−θ1)} α(θ1−θ0)2\n} on the most biased coin problem. Our upper bounds\nfor adaptive procedures presented later should be compared to this result."
    }, {
      "heading" : "2.2 The fixed sample size strategy and the detection of mixtures",
      "text" : "The lower bounds of this section are based on two simple observations. The first observation is that identifying that a specific distribution i ≤ N is heavy (i.e. ξi = 1) is at least as hard as detecting that any of the distributions up to time N is heavy. Thus, a lower bound on E[T (α, θ0, θ1)] for this strictly easier detection problem is also a lower bound for the identification problem. Thus, we’ve reduced the problem to a sequential hypothesis test of whether all the observed samples all came from a single distribution or from a mixture of two distributions:\nProblem 2.\nH0 : ∀i, j Xi,j ∼ gθ for some θ ∈ Θ̃ ⊆ Θ, H1 : ∀i ξi ∼ Bernoulli(α), ∀i, j Xi,j ∼ { gθ0 if ξi = 0\ngθ1 if ξi = 1\nIf θ0 and θ1 are close to each other, or if α is very small, or both, it can be very difficult to decide between H0 and H1 even if α, θ0, θ1 are known a priori. Note that if Θ̃ = {θ0} and the parameters are known, any lower bound on the problem also bounds the most biased coin problem with known α, θ0, θ1. In what follows, for any event A, let Pi(A) and Ei[A] denote probability and expectation of A under hypothesis Hi for i ∈ {0, 1} (the specific value of θ in H0 will be clear from context).\nThe second observation is characterized in the following claim:\nClaim 1. Any procedure that is δ-probably correct also satisfies P(N(0, θ0, θ1) < ∞) ≤ δ for all θ0 < θ1. Claim 1 allows us to restrict our analysis of Problem 2 to procedures that in addition to deciding the hypothesis test, satisfy P0(N < ∞) ≤ δ. This property is instrumental in our ability to prove tight bounds on the sample complexity of the procedures.\nThe fixed sample size strategy fixes an m ∈ N prior to starting the game and samples each distribution exactly m times, i.e. Mi = m for all i ≤ N . To simplify notation let fθ = gθ ⊗ · · · ⊗ gθ be the m-wise product distribution for any θ ∈ Θ. Now our problem is more succinctly described as: Problem 3.\nH0 : ∀i Xi ∼ fθ for some θ ∈ Θ̃ ⊆ Θ, H1 : ∀i ξi ∼ Bernoulli(α), ∀i Xi ∼ { fθ0 if ξi = 0\nfθ1 if ξi = 1\nIn the special case where gθ is a Bernoulli distribution, fθ can be represented by a Binomial distribution with parameters (m, θ)."
    }, {
      "heading" : "2.2.1 Sample complexity when parameters are known",
      "text" : "Theorem 2 characterizes the sample complexity of Problem 3 for any valid procedure. Note that when Θ̃ = {θ0} and θ0, θ1, and α are known, then lower bounding the problem also bounds any fixed sample size procedure that solves the most biased coin problem.\nTheorem 2. Fix δ ∈ (0, 1). Consider the hypothesis test of Problem 3 for any fixed θ ∈ Θ̃ ⊆ Θ. Let N be the random number of distributions considered before stopping and declaring a hypothesis. If a procedure satisfies P0(N < ∞) ≤ δ and P1(∪Ni=1{ξi = 1}) ≥ 1− δ, then\nE1[N ] ≥ max { 1− δ α , log(1δ ) KL(P1|P0) } ≥ max { 1− δ α , log(1δ ) χ2(P1|P0) } ."
    }, {
      "heading" : "In addition, if Θ̃ = {θ0} then",
      "text" : "E1[N ] ≥ max { 1− δ α , log(1δ )\nα2χ2(fθ1 |fθ0)\n} .\nThe next corollary relates Theorem 2 to the special case where distributions are Bernoulli coins and the objective is to find a heavy coin. The second result of the corollary is similar to that of Malloy et al. (2012, Theorem 4) that considers the limit as α → 0 and assumes m is sufficiently large (specifically, large enough for the Chernoff-Stein lemma to apply). In contrast, our result holds for all finite δ, α,m.\nCorollary 1. Fix δ ∈ (0, 1), m ∈ N and consider the class of algorithms that flips each coin exactly m times and outputs a coin i ≤ N as its estimate for a heavy coin. If an algorithm in this class is δ-probably correct then\nE[Nm] ≥ max    1− δ α , log(1δ )\nα2(e m\n(θ1−θ0) 2 θ0(1−θ0) − 1)\n   ≥ θ0(1− θ0) log(1δ ) mα2(θ1 − θ0)2 1 m≤ θ0(1−θ0)\n2(θ1−θ0) 2\n,\nhowever, if we pick the best-case m:\nmin m∈N\nE[mNm] ≥ (1− δ) log\n( log(1/δ)\nα\n)\nα\nθ0(1− θ0) (θ1 − θ0)2 .\nRemark 1. For all sufficiently small (θ1−θ0) 2\nθ0(1−θ0) , the expected number of flips of the fixed strategy to identify a\nheavy coin scales like Ω( θ0(1−θ0)α2(θ1−θ0)2 log(1/δ)), a factor 1/α more than (1) and the best adaptive algorithms we propose in Section 3 that can identify a heavy coin with just O( log(1/δ) α(θ1−θ0)2 ) total flips in expectation. Indeed, even the lower bound for the best case m is a factor of log(1/α) from the best upper bounds."
    }, {
      "heading" : "2.2.2 Sample complexity when parameters are unknown",
      "text" : "If α, θ0, and θ1 are unknown, we cannot test fθ0 against the mixture (1 − α)fθ0 + αfθ1 . Instead, we have the general composite test of any individual distribution against any mixture, which is at least as hard as the hypothesis test of Problem 3 with Θ̃ = {θ} for some particular worst-case setting of θ. Without any specific form of fθ, it is difficult to pick a worst case θ that will produce a tight bound. Consequently, in this section we appeal to single parameter exponential families (defined formally below) to provide us with a class of distributions in which we can reason about different possible values for θ. Since exponential families include Bernoulli, Gaussian, exponential, and many other distributions, the following theorem is general enough to be useful in a wide variety of settings.\nTheorem 3. Suppose fθ for θ ∈ Θ ⊂ R is a single parameter exponential family so that fθ(x) = h(x) exp(η(θ)x − b(η(θ))) for some scalar functions h, b, η where η is strictly increasing. If Eθ[X] =∫ xfθ(x)dx then let Mk(θ) = ∫ (x − Eθ[X])kfθ(x)dx denote the kth centered moment under distribution fθ. Define\nθ∗ = η −1((1− α)η(θ0) + αη(θ1) )\nθ− = η −1(η(θ0)− α(η(θ1)− η(θ0)) ) θ+ = η −1(η(θ1) + (1− α)(η(θ1)− η(θ0)) )\nand assume there exist finite κ, γ such that\nsup y∈[θ0,θ1]\nb(2η(y) − η(θ∗))− [2b(η(y)) − b(η(θ∗))] ≤ κ,\nsup x∈[ḃ(η(θ−)),ḃ(η(θ+))]\nφx(ḃ −1(x)) ≤ γ,\nwhere φx(η(θ)) = fθ(x). Then\nχ2((1 − α)fθ0(x) + αfθ1(x)|fθ∗(x)) ≤ c ( 1\n2 α(1− α)(η(θ1)− η(θ0))2\n)2\nwhere\nc = eκ (\nsup θ∈[θ0,θ1]\nM2(θ) 2 ( 2 + γ ( ḃ(η(θ+))− ḃ(η(θ−)) ))\n+ 8M4(θ−) + 8M4(θ+) + 16 ( ḃ(η(θ+))− ḃ(η(θ−)) )4 + 25γ ( ḃ(η(θ+))− ḃ(η(θ−)) )5 ) .\nThus, if Θ̃ = {θ∗} and N is the stopping time of any procedure that satisfies P0(N < ∞) ≤ δ and P1(∪Ni=1{ξi = 1}) ≥ 1− δ, then\nE1[N ] ≥ max { 1− δ α , log(1δ )\nc ( 1 2α(1− α)(η(θ1)− η(θ0))2 )2\n} .\nTheorem 3 is difficult to interpret, so the following remark and corollary consider the special cases of Gaussian mixture model detection and the most biased coin problem, respectively.\nRemark 2. Recall that when α, θ0, θ1 are unknown, any procedure does not know how to choose Θ̃ in Problem 3 and consequently it cannot rule out θ = θ∗ for H0 where θ∗ is defined in Theorem 3. If fθ = N (θ, σ2) for known σ, then whenever (θ1−θ0)2σ2 ≤ 1 the constant c in Theorem 3 is an absolute constant and consequently, E1[N ] = Ω (( σ2 α(θ1−θ0)2 )2 log(1/δ) ) . Conversely, when α, θ0, θ1 are known, then we simply need to determine whether samples came from N (θ0, σ2) or (1−α)N (θ0, σ2)+αN (θ1, σ2), and we show that it is sufficient to take just O ( σ2\nα2(θ1−θ0)2 log(1/δ) ) samples (see Appendix C).\nCorollary 2. Fix δ ∈ [0, 1],m ∈ N and consider the class of algorithms that flips each coin exactly m times. Assume θ0, θ1 are bounded sufficiently far from {0, 1} such that 2(θ1 − θ0) ≤ min{θ0(1− θ0), θ1(1− θ1)}. If an algorithm in this class is δ-probably correct then\nE[N ] ≥ c ′ min{ 1m , θ∗(1− θ∗)}\nm ( α(1 − α) (θ1−θ0)2θ∗(1−θ∗) )2 log( 1 δ ) whenever m ≤\nθ∗(1− θ∗) (θ1 − θ0)2 .\nwhere c′ is an absolute constant and θ∗ = η−1 ((1− α)η(θ0) + αη(θ1)) ∈ [θ0, θ1].\nRemark 3. We recall that if α, θ0, θ1 are unknown, then any fixed sample strategy would not know how to pick m sufficiently large a priori. Thus, the above corollary states that for any fixed m, whenever (θ1−θ0) 2\nθ∗(1−θ∗) is sufficiently small the number of samples necessary for this simple and intuitive strategy to identify the most biased coin scales like (\nθ∗(1−θ∗) α(θ1−θ0)2\n)2 log(1/δ). However, in the next section we show that when α, θ0, θ1 are\nknown and m can be chosen by the algorithm, this same fixed sample strategy can identify the most biased coin using just log(1/(αδ))\nα(θ1−θ0)2 total flips in expectation, nearly matching the lower bound of Corollary 1. This is a striking example of the difference when parameters are known versus when they are not."
    }, {
      "heading" : "3 Upper bounds and algorithms",
      "text" : "Above we presented lower bounds on the difficulty of identifying a heavy distribution. In this section we prove the existence of algorithms that nearly match the lower bounds, even with only partial side knowledge. Table 1 summarizes the algorithms and their bounds. Our main result in Section 3.3 is Theorem 8 which describes the performance of an algorithm that has no prior knowledge of the parameters α, θ0, θ1 yet yields an upper bound that matches the lower bound of Theorem 1 up to logarithmic factors. In what follows, we assume that samples from heavy or light distributions are supported on [0, 1], and that drawn samples are independent and unbiased estimators of the mean, i.e., E[Xi,j] = µi for µi ∈ {θ0, θ1}. All results can be easily extended to sub-Gaussian distributions. We begin with a fixed sample strategy and then turn our attention to adaptive sampling procedures."
    }, {
      "heading" : "3.1 Fixed sample strategy for known α, θ0, θ1",
      "text" : "A lower bound on α tells us how many distributions we must consider and knowledge of the difference (θ1 − θ0) tells us how many times we should sample each distribution. The below theorem comes within a log(1/δ) factor of the lower bound proved in Corollary 1 in general and is tight when α ≤ δ. Theorem 4 (Fixed sample size, known α and θ0, θ1). Fix δ ∈ (0, 1/4) and set n̂ = ⌈ 1 α log( 2 δ ) ⌉\nand m =⌈ 2 log(4n̂/δ) (θ1−θ0)2 ⌉ . There exists a fixed sample size strategy with stopping time Nm ≤ n̂ that is δ-probably correct and satisfies\nE[mNm] ≤ 3 log(1/α) + log(12 log(6/δ)/δ) α(θ1 − θ0)2 ≤ 12 log(\n2 δα )\nα(θ1 − θ0)2 ."
    }, {
      "heading" : "3.2 Fully adaptive strategies when α and/or θ0, θ1 are known",
      "text" : "While the previous section considered a strategy that takes a constant number of samples from each distribution, this section allows the procedure to determine the number of times to sample a particular distribution adaptively based on the samples from that distribution. This section also shows that there exist simple procedures that adapt to the case when only a subset of α, θ0, θ1 are known using just a small number of samples more than if they had been known.\nConsider Algorithm 2, an SPRT-like procedure for finding a heavy distribution given δ and lower bounds on α and ǫ.\nTheorem 5. If Algorithm 2 is run with δ ∈ (0, 1/4), α0 ∈ (0, 1/2), ǫ0 ∈ (0, 1), then the expected number of total samples taken by the algorithm is no more than\nc′α log(1/α0) + c′′ log ( 1 δ )\nα0ǫ20\nfor some absolute constants c′,c′′, and all of the following hold: 1) with probability at least 1 − δ, a light distribution is not returned, 2) if ǫ0 ≤ θ1 − θ0 and α0 ≤ α, then with probability 45 a heavy distribution is returned, and 3) the procedure takes no more than c log(1/(α0δ))\nα0ǫ20 total samples.\nClearly, Theorem 5 applies when α, θ0, θ1 are known. The third claim of the theorem follows from a trivial bound of nm for the values of n and m stated in the algorithm (i.e. it holds with probability 1). The second claim holds only with constant probability (versus with probability 1− δ) since the probability of observing a heavy distribution among the n = ⌈2 log(4)/α0⌉ distributions considered only occurs with constant probability. One can boost this probability to 1−δ by repeated application of the algorithm log(1/δ)\nGiven δ ∈ (0, 1/4), α0 ∈ (0, 1/2), ǫ0 ∈ (0, 1). Initialize n = ⌈2 log(9)/α0⌉,m = ⌈64ǫ−20 log(14n/δ)⌉, A = −8ǫ−10 log(21),\nB = 8ǫ−10 log(14n/δ), k1 = 5, k2 = ⌈8ǫ−20 log(2k1/min{δ/8,m−1ǫ−20 })⌉. Draw k1 distributions and sample them each k2 times. Estimate θ̂0 = mini=1,...,k1 µ̂i,k2 , γ̂ = θ̂0 + ǫ0/2. Repeat for i = 1, . . . , n:\nDraw distribution i. Repeat for j = 1, . . . ,m:\nSample distribution i and observe Xi,j . If ∑j\nk=1(Xi,k − γ̂) > B: Declare distribution i to be heavy and Output distribution i.\nElse if ∑j\nk=1(Xi,k − γ̂) < A: break.\nOutput null.\nAlgorithm 2: Adaptive strategy for heavy distribution identification with inputs α0, ǫ0, δ\ntimes or alternatively, one can run the algorithm with n = Θ( log(1/δ)α ) (with a straightforward modification of the proof). Moreover, with a slightly more sophisticated argument, one can show that if the algorithm is run with θ̂0 = θ0 (and the estimation step is skipped) and n = ∞ then the algorithm is nearly equivalent to the SPRT of Malloy et al. (2012) which succeeds with probability at least 1 − δ and achieves an expected sample complexity equivalent to (1).\nGiven δ ∈ (0, 1), α ∈ (0, 1/2). Initialize k = 1 While Algorithm 2 run with inputs δ/(2k2), α0 = α, ǫ0 = 2\n−k returns null: Set k = k + 1.\nOutput distribution k.\nAlgorithm 3: Algorithm for unknown θ1 − θ0.\nGiven δ ∈ (0, 1), ǫ ∈ (0, 1]. Initialize k = 1 While Algorithm 2 run with inputs δ/(2k2), α0 = 2\n−k, ǫ0 = ǫ returns null: Set k = k + 1.\nOutput distribution k.\nAlgorithm 4: Algorithm for unknown α.\nWe now leverage Theorem 5 to design procedures that do not have knowledge of these parameters using the “doubling trick.”. First we consider the case when α is known but a lower bound on θ1 − θ0 is not. The following theorem characterizes the performance of Algorithm 3.\nTheorem 6 (Known α, unknown θ0, θ1). Fix δ ∈ (0, 1). If Algorithm 3 is run with δ, α then with probability at least 1 − δ a heavy distribution is returned and the expected number of total samples taken is no more than\nc log ( log ( 1 (θ1−θ0)2 ) /δ )\nα(θ1 − θ0)2 .\nfor an absolute constant c.\nNow we consider the case when θ1− θ0 is known but a lower bound on α is not. The following theorem characterizes the performance of Algorithm 4.\nTheorem 7 (Unknown α, known θ0, θ1). Fix δ ∈ (0, 1). If Algorithm 4 is run with δ, θ1 − θ0 then with probability at least 1−δ a heavy distribution is returned and the the expected number of total samples taken is no more than\nc log ( log ( 1 α ) /δ )\nα(θ1 − θ0)2\nfor an absolute constant c."
    }, {
      "heading" : "3.3 Fully adaptive strategies when α, θ0, θ1 are unknown",
      "text" : "We now consider the most difficult setting in which no prior knowledge about α, θ0, θ1 are known. The algorithm for this setting, Algorithm 5, requires a more sophisticated argument than the simple “doubling trick” used above when partial information was available. As far as we are aware this is the first result of its kind that does not require any prior estimation or knowledge of the unknown mean distribution parameters. We also remark that the placing of “landmarks” (αk, ǫk) throughout the search space as is done in Algorithm 5 can also be generalized to generic infinite armed bandit problems, perhaps providing a simple alternative to the two-stage approach of estimation then exploration of Carpentier and Valko (2015).\nGiven δ > 0. Initialize ℓ = 1, heavy distribution h = null. Repeat until h is not null:\nSet γℓ = 2ℓ, δℓ = δ/(2ℓ3) Repeat for k = 0, . . . , ℓ:\nSet αk = 2 k\nγℓ , ǫk =\n√ 1\n2αkγℓ\nRun Algorithm 2 with α0 = αk, ǫ0 = ǫk, δ = δℓ and Set h to its output. If h is not null break\nSet ℓ = ℓ+ 1 Output h\nAlgorithm 5: Adaptive strategy for heavy distribution identification with unknown parameters\nTheorem 8 (Unknown α, θ0, θ1). Fix δ ∈ (0, 1). If Algorithm 5 is run with δ then with probability at least 1− δ a heavy distribution is returned and the expected number of total samples taken is bounded by\nc log2(\n1 αǫ2 )\nαǫ2 (α log2(\n1 ǫ2 ) + log(log2( 1 αǫ2 )) + log(1/δ))\nfor an absolute constant c."
    }, {
      "heading" : "4 Conclusion",
      "text" : "In this work, we prove upper and lower bounds on the complexity of detecting mixture distributions with partial or missing knowledge of the distribution parameters. We note that there is still a log-factor gap between several of our upper and lower bounds, and investigating whether either can be tightened remains an interesting problem. Importantly, in this work we considered mixtures of only two components, whereas the literature on infinite-armed bandits considers a continuous mixture. Extending the algorithms developed\nfor our upper bounds to the continuous mixture case is a promising direction, as it would represent the first such algorithm that does not rely on knowledge of the distribution parameters or estimating them first with a two-stage approach."
    }, {
      "heading" : "Acknowledgments",
      "text" : "Kevin Jamieson is generously supported by ONR awards N00014-15-1-2620, and N00014-13-1-0129. This research is supported in part by NSF CISE Expeditions Award CCF-1139158, DOE Award SN10040 de-sc0012463, and DARPA XData Award FA8750-12-2-0331, and gifts from Amazon Web Services, Google, IBM, SAP, The Thomas and Stacey Siebel Foundation, Apple Inc., Arimo, Blue Goji, Bosch, Cisco, Cray, Cloudera, Ericsson, Facebook, Fujitsu, Guavus, HP, Huawei, Intel, Microsoft, Pivotal, Samsung, Schlumberger, Splunk, State Farm and VMware."
    }, {
      "heading" : "A Proofs of Lower Bounds",
      "text" : ""
    }, {
      "heading" : "A.1 Proof of Claim 1",
      "text" : "Proof. Suppose there exists a δ-probably correct procedure with P(N(0, θ0, θ1) < ∞) > δ. Then there exists a finite n̂ ∈ N such that P(N(0, θ0, θ1) ≤ n̂) > δ. For some ǫ ∈ (0, 1) to be defined later, define α̂ = log( 1\n1−ǫ ) 2n̂ and note that for this α̂, P( ⋂n̂ i=1{ξi = 0}) = (1− α̂)n̂ ≥ e−2n̂α̂ ≥ 1− ǫ. Thus, the probability\nthat the procedure terminates with a light distribution under α = α̂ is at least\nP(N(α̂, θ0, θ1) ≤ n̂,∩n̂i=1{ξi = 0}) = P(N(α̂, θ0, θ1) ≤ n̂| ∩n̂i=1 {ξi = 0})P(∩n̂i=1{ξi = 0}) = P(N(0, θ0, θ1) ≤ n̂)P(∩n̂i=1{ξi = 0}) > δ(1 − ǫ).\nBecause we can make ǫ arbitrarily small, the above display implies that the procedure makes a mistake with probability at least δ, but this is a contradiction as the procedure is δ-probably correct."
    }, {
      "heading" : "A.2 Proof of Theorem 2",
      "text" : "Proof. First, let N be the number of distributions considered at the stopping time T . Note that T ≥ N . By assumption the procedure satisfies P1(N ≥ n| ∩n−1i=1 {ξi = 0}) ≥ 1− δ for all n ∈ N. And\nP1(N ≥ n) ≥ P1(N ≥ n,∩n−1i=1 {ξi = 0}) = P1(N ≥ n| ∩n−1i=1 {ξi = 0})P1(∩n−1i=1 {ξi = 0}) ≥ (1− δ)(1 − α)n−1\nThus, E1[N ] = ∑∞ n=1 P1(N ≥ n) ≥ (1 − δ) ∑∞\nn=1(1 − α)n−1 = 1−δα which results in the first argument of the max.\nApplying Theorem 2.38 of Siegmund (2013) we have\nE1[N ]χ 2 (P1|P0)\nEqn. (2) ≥ E1[N ]KL (P1|P0) Thm. 2.38 ≥ log( 1P0(N<∞)) assumption ≥ log(1δ ),\nwhich results in the second argument of the max. If Θ̃ = {θ0} then χ2(P1|P0) = χ2((1− α)fθ0 + αfθ1 |fθ0) and\nχ2((1− α)fθ0 + αfθ1 |fθ0) = ∫\n((1− α)fθ0(x) + αfθ1(x)− fθ0(x))2 fθ0(x) dx = α2χ2(fθ1 |fθ0)\nThus, E1[N ] ≥ log(\n1 δ )\nα2χ2(fθ1 |fθ0) which results in the second part of the theorem."
    }, {
      "heading" : "A.3 Proof of Corollary 1",
      "text" : "Proof. For k = 0, 1 let gθk be a Bernoulli distribution with parameter θk and let fθk = gθk ⊗ · · · ⊗ gθk be a product distribution composed of m gθk distributions. Then\nχ2(gθ1 |gθ0) = (θ1 − θ0)2 θ0(1− θ0) ≤ e (θ1−θ0) 2 θ0(1−θ0) − 1\nand\nχ2(fθ1 |fθ0) = ( 1 + χ2(gθ1 |gθ0) )m − 1 ≤ em (θ1−θ0) 2 θ0(1−θ0) − 1.\nMoreover, e m\n(θ1−θ0) 2 θ0(1−θ0) − 1 ≤ m (θ1−θ0)2θ0(1−θ0) whenever m ≤ θ0(1−θ0) 2(θ1−θ0)2 since e\nx/2 − 1 ≤ x for all x ∈ [0, 1]. Applying Theorem 2 obtains the first result. The second result follows from loosening the integer constraint on m and minimizing the the lower bound on E[Nm] multiplied by m. To perform the minimization, we\nnote that the function max{1−δα , 2 log(1δ )/[α2(e m\n(θ1−θ0) 2\nθ0(1−θ0) − 1)]} reaches its minimum at the intersection of the two arguments and solve for m at that point."
    }, {
      "heading" : "A.4 Proof of Theorem 3",
      "text" : "Proof. Define φx(η) = h(x) exp(ηx − b(η)). By the properties of scalar exponential families, note that b′(η) and b′′(η) ≥ 0 represent the mean and variance of the distribution. We deduce that b′ is monotonically increasing. Define η0 = η(θ0), η1 = η(θ1), and µ = (1− α)η0 + αη1. Noting that\nχ2((1 − α)φx(η0) + αφx(η1)|φx(µ)) = ∫ φx(µ) ( (1− α)φx(η0) + αφx(η1)− φx(µ)\nφx(µ)\n)2 dx\nwe will use a technique that was used in Pollard (2000) to approximate the divergence between a single Gaussian distribution and a mixture of them. Essentially, we will take the Taylor series of each φx(·) centered at µ and bound. We have\nφx(η) = h(x) exp(ηx− b(η)) φ′x(η) = (x− b′(η))φx(η) φ′′x(η) = (−b′′(η) + (x− b′(η))2)φx(η)\nso that\nφx(y) = φx(µ) [ 1 + (x− b′(µ))(y − µ) + 12(−b′′(µ) + (x− b′(µ))2)(y − µ)2 . . . ] .\nNoting that (η0−µ) = −α(η1−η0), (η1−µ) = (1−α)(η1−η0), and (1−α)α2+α(1−α)2 = α(1−α), we have\n∣∣∣∣ (1− α)φx(η0) + αφx(η1)− φx(µ)\nφx(µ)\n∣∣∣∣\n= ∣∣∣∣ φ′x(µ) φx(µ) [(1− α)(η0 − µ) + α(η1 − µ)] + 1 2 φ′′x(µ) φx(µ) [(1− α)(η0 − µ)2 + α(η1 − µ)2] + . . . ∣∣∣∣\n= ∣∣∣∣ 1\n2 φ′′x(µ) φx(µ)\nα(1− α)(η1 − η0)2 + . . . ∣∣∣∣\n≤ sup z∈[η0,η1] |φ′′x(z)| φx(µ) 1 2 α(1 − α)(η1 − η0)2.\nThus,\nχ2((1 − α)φx(η0) + αφx(η1)|φx(µ)) = ∫ φx(µ) ( (1− α)φx(η0) + αφx(η1)− φx(µ)\nφx(µ)\n)2 dx\n≤ ( 1\n2 α(1 − α)(η1 − η0)2\n)2 ∫ sup\nz∈[η0,η1]\n|φ′′x(z)|2 φx(µ)2 φx(µ)dx.\nBy distributing the square and noting that b′′(η) ≥ 0, we have ∫\nsup z∈[η0,η1] |φ′′x(z)|2 φx(µ)2 φx(µ)dx =\n∫ sup\nz∈[η0,η1]\n( φx(z)\nφx(µ)\n)2 (−b′′(z) + (x− b′(z))2)2 φx(µ)dx\n≤ ∫\nsup z∈[η0,η1]\n( φx(z)\nφx(µ)\n)2 b′′(z)2 φx(µ)dx+ ∫ sup\nz∈[η0,η1]\n( φx(z)\nφx(µ)\n)2 (x− b′(z))4 φx(µ)dx\n≤ sup y∈[η0,η1]\nb′′(y)2 ∫\nsup z∈[η0,η1]\n( φx(z)\nφx(µ)\n)2 φx(µ)dx+ ∫ sup\nz∈[η0,η1]\n( φx(z)\nφx(µ)\n)2 (x− b′(z))4 φx(µ)dx.\nThe remainder of the proof bounds the integrals. Define η− = 2η0−µ = η(θ−) and η− = 2η1−µ = η(θ+). Observe that\nsup z∈[η0,η1]\n( φx(z)\nφx(µ)\n)2 φx(µ)\n= sup z∈[η0,η1]\nh(x) exp ( (2z − µ)x− (2b(z) − b(µ)) )\n= sup z∈[η0,η1]\nh(x) exp ( (2z − µ)x− b(2z − µ) ) exp ( b(2z − µ)− (2b(z) − b(µ)) )\n≤ eκ sup z∈[η0,η1]\nh(x) exp ( (2z − µ)x− b(2z − µ) )\n= eκ sup z∈[2η0−µ,2η1−µ]\nh(x) exp ( zx− b(z) )\n= eκ sup z∈[η−,η+]\nh(x) exp ( zx− b(z) )\n≤ eκ ( φx(η−) + φx(η+) + φx(ḃ\n−1(x))1x∈[ḃ(η−),ḃ(η+)]\n)\n≤ eκ ( φx(η−) + φx(η+) + γ1x∈[ḃ(η−),ḃ(η+)] )\nwhere the second inequality follows by observing that the maximum of the function φx(z) will occur either at an endpoint of the interval z ∈ [η(θ−), η(θ+)] or at the point where ∂∂zg(z) = 0 (if that point occurs inside the interval), and loosely bounding the maximum by simply adding the function values at all three points.\nConsequently,\nsup y∈[η0,η1]\nb′′(y)2 ∫\nsup z∈[η0,η1]\n( φx(z)\nφx(µ)\n)2 φx(µ)dx ≤ sup\nθ∈[θ0,θ1] M2(θ)\n2eκ ( 2 + γ(ḃ(η+)− ḃ(η−)) ) .\nBy Jensen’s inequality, (a+ b)4 = 16(12a+ 1 2b) 4 ≤ 8(a4 + b4), so ∫\nsup z∈[η0,η1]\nφx(η−)(x− ḃ(z))4dx = ∫\nsup z∈[η0,η1]\nφx(η−)(x− ḃ(η−) + ḃ(η−)− ḃ(z))4dx\n≤ ∫\n8φx(η−)[(x− ḃ(η−))4 + sup z∈[η0,η1] (ḃ(η−)− ḃ(z))4]dx\n≤ ∫ 8φx(η−)[(x− ḃ(η−))4 + (ḃ(η−)− ḃ(η1))4]dx\n= 8[M4(θ−)− (ḃ(η−)− ḃ(η1))4].\nRepeating an analogous series of steps for η+, we have\n∫ sup\nz∈[η0,η1]\n( φx(z)\nφx(µ)\n)2 (x− b′(z))4 φx(µ)dx\n≤ eκ ∫ (\nφx(η−) + φx(η+) + γ1x∈[ḃ(η−),ḃ(η+)]\n) sup\nz∈[η0,η1] (x− ḃ(z))4dx\n≤ eκ ( 8M4(θ−) + 8(ḃ(η1)− ḃ(η−))4 + 8M4(θ+) + 8(ḃ(η+)− ḃ(η0))4 + 25γ(ḃ(η+)− ḃ(η−))5 ) ≤ eκ ( 8M4(θ−) + 8M4(θ+) + 16(ḃ(η+)− ḃ(η−))4 + 25γ(ḃ(η+)− ḃ(η−))5 ) .\nThe final result holds by Theorem 2."
    }, {
      "heading" : "A.5 Proof of Corollary 2",
      "text" : "Proof. A binomial distribution for fixed m is an exponential family fθ(x) = h(x) exp(η(θ)x − b(η(θ))) with h(x) = (m x ) , η(θ) = log( θ1−θ ), and b(τ) = m log(1 + e τ ). Note that η is monotonically increasing, b is m-Lipschitz, and ḃ(τ) = m(1 + e−τ )−1 so that ḃ(η(θ)) = mθ. Step 1: Relating θ+, θ− to θ1, θ0 We will make repeated use of the fact that if f is convex then f(y) ≥ f(x)+ f ′(x)T (y−x). Since x1−x and 1−x x are both convex, we have\ny 1− y ≥ x 1− x + y − x (1− x)2 and 1− y y ≥ 1− x x − y − x x2\nfor all x, y ∈ [0, 1]. To begin, note η−1(ν) = (1+e−ν)−1 so that for any θ we have θ(1−θ) = η−1(η(θ))(1−η−1(η(θ))) =\ne−η(θ)\n(1+e−η(θ))2 . Observe that\n1 4 e−|η(θ)| ≤ e\n−η(θ)\n(1 + e−η(θ))2 ≤ e−|η(θ)|\nand recalling that θ∗ = η−1((1− α)θ0 + αθ1) ∈ [θ0, θ1] we have\nθ+(1− θ+) ≥ 1 4 e−|η(θ+)| = 1 4 e−|2η(θ1)−η(θ∗)|\n= 141θ+≤1/2\n( θ1\n1− θ1 )2 (1− θ∗ θ∗ ) + 141θ+>1/2 ( 1− θ1 θ1 )2( θ∗ 1− θ∗ )\n≥ 141θ+≤1/2 (\nθ1 1− θ1 )2 (1− θ1 θ1 ) + 141θ+>1/2 ( 1− θ1 θ1 )2( θ0 1− θ0 )\n≥ 141θ+≤1/2 (\nθ1 1− θ1\n) + 141θ+>1/2 ( 1− θ1 θ1 )2( θ1 1− θ1 − θ1 − θ0 (1− θ1)2 )\n≥ 141θ+≤1/2 (\nθ1 1− θ1\n) + 181θ+>1/2 ( 1− θ1 θ1 ) ≥ 1 8 θ1(1− θ1)\nwhere the last line follows from the assumption that θ1(1− θ1) ≥ 2(θ1 − θ0). Analogously,\nθ−(1− θ−) ≥ 1 4 e−|η(θ−)| = 1 4 e−|2η(θ0)−η(θ∗)|\n= 141θ−≤1/2\n( θ0\n1− θ0 )2 (1− θ∗ θ∗ ) + 141θ−>1/2 ( 1− θ0 θ0 )2( θ∗ 1− θ∗ )\n≥ 141θ−≤1/2 (\nθ0 1− θ0 )2 (1− θ1 θ1 ) + 141θ−>1/2 ( 1− θ0 θ0 )2( θ0 1− θ0 )\n≥ 141θ−≤1/2 (\nθ0 1− θ0 )2 (1− θ0 θ0 − θ1 − θ0 θ20 ) + 141θ−>1/2 ( 1− θ0 θ0 )\n≥ 181θ−≤1/2 (\nθ0 1− θ0\n) + 141θ−>1/2 ( 1− θ0 θ0 ) ≥ 1 8 θ0(1− θ0)\nwhere the last line follows from the assumption that θ0(1− θ0) ≥ 2(θ1 − θ0). We conclude that\ninf θ∈[θ−,θ+] θ(1− θ) ≥ 1 8 inf θ∈[θ0,θ1] θ(1− θ). (3)\nConversely,\nsup θ∈[θ−,θ+]\nθ(1− θ) ≤ 11/2∈[θ−,θ+] 1\n4 + θ+(1− θ+)1θ+≤1/2 + θ−(1− θ−)1θ−>1/2.\nWe consider these three cases in turn. If θ+ ≤ 1/2:\nθ+(1− θ+) ≤ e−|η(θ+)| = e−|2η(θ1)−η(θ∗)|\n=\n( θ1\n1− θ1 )2 (1− θ∗ θ∗ ) ≤ ( θ1 1− θ1 )2 (1− θ0 θ0 ) ≤ ( θ1 1− θ1 )2(1− θ1 θ1 + θ1 − θ0 θ20 )\n=\n( θ1\n1− θ1\n)( 1 +\nθ1(θ1 − θ0) (1− θ1)θ20\n) ≤ ( θ1\n1− θ1\n)( 1 +\nθ1(1− θ0) 2(1 − θ1)θ0\n)\n=\n( θ1\n1− θ1\n)( 1 +\nθ0(1− θ0) + (θ1 − θ0)(1 − θ0) 2(1 − θ1)θ0\n)\n≤ (\nθ1 1− θ1\n)( 1 +\nθ0(1− θ0) + θ0(1− θ0)2/2 2(1− θ1)θ0\n) ≤ 5\n2\n( θ1\n1− θ1\n) ≤ 10θ1(1− θ1)\nusing the convexity of 1−xx , the assumption that 2(θ1 − θ0) ≤ θ0(1 − θ0), that θ1 ≤ θ+ ≤ 1/2, and that 1− θ0 ≤ 1. If θ− > 1/2:\nθ−(1− θ−) ≤ e−|η(θ−)| = e−|2η(θ0)−η(θ∗)|\n= ( 1− θ0 θ0 )2( θ∗ 1− θ∗ ) ≤ ( 1− θ0 θ0 )2 ( θ1 1− θ1 ) ≤ ( 1− θ0 θ0 )2 ( θ0 1− θ0 + θ1 − θ0 (1− θ1)2 ) ≤ ( 1− θ0 θ0 )( 1 + (1− θ0)(θ1 − θ0) θ0(1− θ1)2 ) ≤ ( 1− θ0 θ0 )( 1 + (1− θ0)θ1/2 θ0(1− θ1) )\n= ( 1− θ0 θ0 )( 1 + (1− θ1)θ1 + (θ1 − θ0)θ1 2θ0(1− θ1) ) ≤ ( 1− θ0 θ0 )( 1 + (1− θ1)θ1 + (1− θ1)θ21/2 2θ0(1− θ1) ) ≤ 5 2 ( 1− θ0 θ0 ) ≤ 10θ0(1− θ0)\nusing the same methods as above. From these two cases, we can conclude that if 1/2 /∈ [θ−, θ+],\nsup θ∈[θ−,θ+] θ(1− θ) ≤ 10 sup θ∈[θ0,θ1] θ(1− θ). (4)\nThe remaining case, when 1/2 ∈ [θ−, θ+], also satisfies (4), which we now demonstrate. When θ+ = 1/2 we have 1/4 = θ+(1 − θ+) ≤ 10θ1(1 − θ1) so that θ1(1 − θ1) ≥ 1/40. Because θ1 is monotonically increasing in θ+ and supθ∈[θ−,θ+] θ(1− θ) ≤ 1/4 we conclude that (4) holds whenever θ1 ≤ 1/2. A similar argument follows for all θ0 ≥ 1/2. Finally, if 1/2 ∈ [θ0, θ1], it must be true that supθ∈[θ−,θ+] θ(1 − θ) ≤ supθ∈[θ0,θ1] θ(1 − θ) because θ− ≤ θ0 ≤ 12 ≤ θ1 ≤ θ+ and the function θ(1 − θ) is concave taking its maximum at 12 . Thus, (4) holds for all θ−, θ+.\nWe now turn our attention to bounding θ+ − θ−. Let g(y) = η−1(y) then g(y) = (1 + e−y)−1 and ġ(y) = e−y(1 + e−y)−2. Observing that ġ(η(θ)) = θ(1− θ) we have by Taylor’s remainder theorem\nθ+ − θ− = η−1(η(θ+))− η−1(η(θ−)) ≤ (η(θ+)− η(θ−)) sup y∈[η(θ−),η(θ+)] e−y(1 + e−y)−2\n= (η(θ+)− η(θ−)) sup θ∈[θ−,θ+] θ(1− θ) = 2 (η(θ1)− η(θ0)) sup θ∈[θ−,θ+] θ(1− θ) ≤ 20 (η(θ1)− η(θ0)) sup θ∈[θ0,θ1] θ(1− θ).\nSince η(θ) = log( θ1−θ ) and η ′(θ) = 1θ + 1 1−θ = 1 θ(1−θ) , we have\nθ+ − θ− ≤ 20 (η(θ1)− η(θ0)) sup θ∈[θ0,θ1] θ(1− θ) ≤ 20 (θ1 − θ0) supθ∈[θ0,θ1] θ(1− θ) infθ∈[θ0,θ1] θ(1− θ) .\nIf θ1(1− θ1) ≥ θ0(1− θ0):\nθ1(1− θ1) θ0(1− θ0) = θ0(1− θ1) + (θ1 − θ0)(1− θ1) θ0(1− θ0)\n≤ θ0(1− θ1) + θ0(1− θ0)(1 − θ1)/2 θ0(1− θ0) ≤ 1 + (1− θ1)/2 ≤ 3/2,\nelse if θ0(1− θ0) ≥ θ1(1− θ1)\nθ0(1− θ0) θ1(1− θ1) = θ0(1− θ1) + θ0(θ1 − θ0) θ1(1− θ1)\n≤ θ0(1− θ1) + θ0θ1(1− θ1)/2 θ1(1− θ1) ≤ 1 + θ0/2 ≤ 3/2.\nFinally, if 1/2 ∈ [θ0, θ1] then supθ∈[θ0,θ1] θ(1− θ) = 1/4 taking its maximum at 1/4. To maximize the ratio of the sup to the inf, it suffices to just consider the case when θ0 = 1/2 or θ1 = 1/2. Thus, the above two bounds suffice for this case and we observe that\nsupθ∈[θ0,θ1] θ(1− θ) infθ∈[θ0,θ1] θ(1− θ) ≤ 3/2. (5)\nThus, putting the pieces together, we conclude that\nθ+ − θ− ≤ 30(θ1 − θ0). (6)\nStep 2: Bounding γ, κ, c In what follows, define θh = arg supθ∈[θ0,θ1] θ(1− θ) and θl = arg infθ∈[θ0,θ1] θ(1− θ). We now continue to bound the terms of the theorem. Note\nsup x∈[ḃ(η(θ−)),ḃ(η(θ+))]\nφx(ḃ −1(x)) = sup\nx∈[mθ−,mθ+] φx(η(x/m))\n≤ sup x∈[mθ−,mθ+] sup y∈[0,1] φx(η(y))\n= sup x∈[mθ−,mθ+] sup y∈[0,1]\nΓ(m+ 1)\nΓ(m− x+ 1)Γ(x+ 1)y x(1 − y)m−x\n= sup θ∈[θ−,θ+] sup y∈[0,1]\nΓ(m+ 1)\nΓ(m(1− θ) + 1)Γ(mθ + 1)y mθ(1− y)m(1−θ)\n≤ sup θ∈[θ−,θ+] sup y∈[0,1] e/2π√ mθ(1− θ) ymθ(1− y)m(1−θ) θmθ(1− θ)m(1−θ)\n= sup θ∈[θ−,θ+] e/2π√ mθ(1− θ) ≤ 2√ mθl(1− θl) =: γ\nby Stirling’s approximation: √ 2π ≤ Γ(s+1)\ne−sss+1/2 ≤ e (Spira, 1971) and (3). And for any y ∈ [θ0, θ1]\nb(2η(y) − η(θ∗))− (2b(η(y)) − b(η(θ∗))) = m log(1 + e2η(y)−η(θ∗))− 2m log(1 + eη(y)) +m log(1 + eη(θ∗))\n= m log\n( (1 + e2η(y)−η(θ∗))(1 + eη(θ∗))\n(1 + eη(y))2\n)\n= m log (( 1 + ( y\n1− y )2 1− θ∗ θ∗ )( 1 1− θ∗ ) (1− y)2 )\n= m log ( (1− y)2 1\n1− θ∗ + y2\n1\nθ∗\n)\n= m log ( (1− 2y + y2) θ∗\nθ∗(1− θ∗) + y2 1− θ∗ θ∗(1− θ∗)\n)\n= m log ( (1− 2y) θ∗\nθ∗(1− θ∗) + y2\n1\nθ∗(1− θ∗)\n)\n= m log ( 1 +\n(y − θ∗)2 θ∗(1− θ∗)\n)\nso\nsup y∈[θ0,θ1]\nb(2η(y) − η(θ∗))− (2b(η(y)) − b(η(θ∗)))\n≤ sup y∈[θ0,θ1] m log\n( 1 +\n(y − θ∗)2 θ∗(1− θ∗)\n) ≤ m ( (θ1 − θ0)2 θ∗(1− θ∗) ) =: κ.\nNoting that M2(θ) = mθ(1− θ),\nsup y∈[θ0,θ1]\nM2(y) 2(2 + γ(ḃ(η(θ+))− ḃ(η(θ−)))) ≤ m2 (θh(1− θh))2 (2 + γm(θ+ − θ−))\n≤ m2 (θh(1− θh))2 ( 2 +\n2m√ mθl(1− θl)\n30(θ1 − θ0) )\n≤ m2 (θh(1− θh))2 ( 2 + 60 √ m (θ1 − θ0)2 θl(1− θl) ) .\nSince for any θ ∈ [0, 1]\nM4(θ) = mθ(1− θ) (3θ(1− θ)(m− 2) + 1) < 3m2 (θ(1− θ))2 +mθ(1− θ),\nwe have\n8M4(θ−) + 8M4(θ+) + 16 ( ḃ(η(θ+))− ḃ(η(θ−)) )4 + 25γ ( ḃ(η(θ+))− ḃ(η(θ−)) )5\n≤24m2 (θ−(1− θ−))2 + 8mθ−(1− θ−) + 24m2 (θ+(1− θ+))2 + 8mθ+(1− θ+)\n+ 16m4(θ+ − θ−)4 + 4/5√\nmθl(1− θl) m5(θ+ − θ−)5\n≤4800m2 (θh(1− θh))2 + 160mθh(1− θh)\n+ 3240000m4(θ1 − θ0)4 + 19440000 √\nm (θ1 − θ0)2 θl(1− θl) m4(θ1 − θ0)4\nwhere we have applied (4) and (6). Finally, recall from above that\nη(θ1)− η(θ0) ≤ θ1 − θ0 θl(1− θl) ≤ 3 2 θ1 − θ0 θ∗(1− θ∗) .\nStep 3: Putting the pieces together Noting that θl(1 − θl) ≤ θ∗(1 − θ∗) ≤ θh(1 − θh) and θh(1−θh)θl(1−θl) ≤ 3/2 by (5), we can use θ∗(1 − θ∗) throughout at the cost of a constant. Putting it altogether, if m (θ1−θ0) 2\nθ∗(1−θ∗) ≤ 1 then κ ≤ 1 and\nc ≤ c′ ( m2 (θ∗(1− θ∗))2 +mθ∗(1− θ∗) +m4(θ1 − θ0)4 )\n≤ c′ ( m2 (θ∗(1− θ∗))2 +mθ∗(1− θ∗) )\nfor some absolute constant c′. Thus,\nc ( 1 2α(1 − α) (η(θ1)− η(θ0)) 2 )2\n≤ c′ ( m2 (θ∗(1− θ∗))2 +mθ∗(1− θ∗) )(9 8 α(1− α) (θ1 − θ0) 2 (θ∗(1− θ∗))2 )2 ≤ c′ ( m2 + m\nθ∗(1− θ∗)\n)( 9\n8 α(1 − α) (θ1 − θ0) 2 θ∗(1− θ∗)\n)2 ."
    }, {
      "heading" : "B Proofs of Upper Bounds",
      "text" : ""
    }, {
      "heading" : "B.1 Proof of Theorem 4",
      "text" : "Proof. Let µ̂i be the empirical mean of the ith distribution sampled m times with mean µi ∈ {θ0, θ1}. Let N be the minimum of n̂ and the first i ∈ N such that µ̂i ≥ θ0+θ12 . Declare distribution N to be heavy. The total number of flips this procedure makes equals mN .\nDefine the events\nξ1 = n̂⋃\ni=1\n{µi = θ1}, and ξ2 = n̂⋂\ni=1\n{|µ̂i − µi| < θ1−θ02 }.\nNote that P(ξc1) = P(µ1 = θ0) n̂ = (1 − α)n̂ ≤ exp(−αn̂) ≤ δ/2. And, by a union bound and Chernoff’s inequality P (ξc2) ≤ 2n̂e−m(θ1−θ0) 2/2 ≤ δ/2. Thus, the probability that ξ1 or ξ2 fail to occur is less than δ, so in what follows assume they succeed. Under ξ1 at least one of the n̂ distributions is heavy. Under ξ2, for any i ∈ [n̂] with µi = θ0 we have µ̂i < µi + θ1−θ0 2 = θ0+θ1\n2 which implies that the procedure will never exit with a light distribution unless N = n̂. On the other hand, for the first i ∈ [n̂] with µi = θ1 we have µ̂i > µi − θ1−θ02 = θ0+θ12 which means the algorithm will output distribution i at time N = i. Thus, N is equal to the first distribution that is heavy and\nE[N ] = n̂∑\nn=1\nP(N ≥ n) = n̂∑\nn=1\nP(N ≥ n, max i=1,...,n−1 µi 6= θ1) + P(N ≥ n, max i=1,...,n−1 µi = θ1)\n≤ n̂∑\nn=1\nP( max i=1,...,n−1 µi 6= θ1) + P(∪n−1i=1 {|µ̂i − µi| > θ1−θ02 }| maxi=1,...,n−1µi = θ1)P( maxi=1,...,n−1µi = θ1)\n≤ n̂∑\nn=1\nP( max i=1,...,n−1\nµi 6= θ1) + P(∪n−1i=1 {|µ̂i − µi| > θ1−θ02 })\n≤ n̂∑\nn=1\n(1− α)n−1 + n−1n̂ δ 2 ≤ 1 α + n̂δ/4 = 1 α (1 + δ log(2e/δ)4 ) ≤ 3/2 α .\nMultiplying E[N ] by m yields the result."
    }, {
      "heading" : "B.2 Proof of Theorem 5",
      "text" : "First, we prove several technical lemmas necessary to analyze our algorithm.\nLemma 1. For i ∈ N, let Xi ∈ [ai, bi] for |bi − ai| ≤ 1 be a random variable with E[Xi] = 0. Then\nP\n( ∞⋃\nn=1\n{ n∑\ni=1\nXi ≥ αn+ β }) ≤ 7 exp(−αβ/2)\nwhenever αβ ≥ 1.\nProof. First we will break the bound into two pieces:\nP\n( ∞⋃\nn=1\n{ n∑\ni=1\nXi ≥ αn+ β })\n≤ min n0 P\n( n0⋃\nn=1\n{ n∑\ni=1\nXi ≥ β }) + P ( ∞⋃\nn=n0+1\n{ n∑\ni=1\nXi ≥ αn })\nwhere P ( ⋃n0 n=1 { ∑n\ni=1 Xi ≥ β}) ≤ exp(−2β2/n0) by Doob-Hoeffding’s maximal inequality. For any fixed k ∈ N:\nP\n  2k∑\ni=1\nXi ≥ α2k/2   ≤ exp(−α22k/2)\nand\nP\n  2k+1⋃\nn=2k+1\n   n∑\ni=2k+1\nXi ≥ αn/2      ≤ P   2k+1⋃\nn=2k+1\n   n∑\ni=2k+1\nXi ≥ α2k/2     \n= P\n  2k⋃\nℓ=1\n{ ℓ∑\ni=1\nXi ≥ α2k/2 }  ≤ exp(−α22k/2)\nby Hoeffding’s and Doob-Hoeffding’s maximal inequality, respectively. Thus\nP\n( ∞⋃\nn=n0\n{ n∑\ni=1\nXi ≥ αn }) = P   ∞⋃\nn=n0\n   2⌈log2(n)⌉∑\ni=1\nXi +\nn∑\ni=2⌈log2(n)⌉+1\nXi ≥ αn     \n= P\n \n∞⋃\nk=log2(n0)\n2k+1⋃\nn=2k+1\n   2k∑\ni=1\nXi + n∑\ni=2k+1\nXi ≥ αn     \n≤ ∞∑\nk=log2(n0)\nP\n  2k∑\ni=1\nXi ≥ α2k/2   + P   2k+1⋃\nn=2k+1\n   n∑\ni=2k+1\nXi ≥ αn/2     \n≤ ∞∑\nk=log2(n0)\n2 exp(−α22k/2) ≤ 2 ∫ ∞\nlog2(n0) exp(−(α/2)22x)dx\n= 2\nlog(2)\n∫ ∞\nn0\nu−1 exp(−(α/2)2u)du ≤ 8 exp(−(α/2) 2n0)\nn0α2 log(2) .\nPutting the pieces together we have\nP\n( ∞⋃\nn=1\n{ n∑\ni=1\nXi ≥ αn+ β })\n≤ min n0\nexp(−2β2/n0) + 8 exp(−(α/2)2n0)\nn0α2 log(2)\n≤ exp(−βα) + 4 exp(−βα/2) βα log(2) ≤ 7 exp(−βα/2)\nwhere the last inequality holds with βα ≥ 1.\nLemma 2. Given θ1 − γ̂ ≥ 2Bm ,\nP ( max\nj=1,...,m\nj∑\ns=1\n(Xi,s − γ̂) > B ∣∣µi = θ1 ) ≥ 1− exp ( −m(θ1 − γ̂)2/2 ) .\nSimilarly, given γ̂ − θ0 ≥ 2|A|m ,\nP ( min\nj=1,...,m\nj∑\ns=1\n(Xi,s − γ̂) < A ∣∣µi = θ0 ) ≥ 1− exp ( −m(γ̂ − θ0)2/2 ) .\nProof. We analyze the left hand side of the lemma:\nP ( max\nj=1,...,m\nj∑\ns=1\n(Xi,s − γ̂) > B ∣∣∣∣µi = θ1\n)\n= P\n  m⋃\nj=1\n{ j∑\ns=1\n(Xi,s − γ̂) > B ∣∣∣∣µi = θ1\n} \n≥ P ( m∑\ns=1\n(Xi,s − γ̂) > B ∣∣∣∣µi = θ1\n)\n= 1− P ( 1\nm\nm∑\ns=1\n(Xi,s − µi) ≤ B\nm − (µi − γ̂)\n∣∣∣∣µi = θ1 )\n= 1− P ( 1\nm\nm∑\ns=1\n(µi −Xi,s) ≥ (µi − γ̂)− B\nm\n∣∣∣∣µi = θ1\n)\n≥ 1− exp ( −2m [ (θ1 − γ̂)− B\nm\n]2)\n≥ 1− exp (−m(θ1 − γ̂)2\n2\n)\nWhere the second to last statement holds by Hoeffding’s inequality, and the last uses the bound on B/m given in the lemma. A nearly identical argument yields the second half of the lemma.\nLemma 3. If θ1 − γ̂ ≥ 2Bm then\nP\n( n⋃\ni=1\n{ µi = θ1, max\nj=1,...,m\nj∑\ns=1\n(Xi,s − γ̂) > B, min j=1,...,m\nj∑\ns=1\n(Xi,s − γ̂) > A })\n≥ 1− exp [−αn(1− exp (−B(θ1 − γ̂))− 7 exp(−|A|(γ̂ − θ0)/2))]\nProof. Consider iid events Ωi for i = 1, . . . , n. Then P( ⋃n i=1Ωi) = 1 − P( ⋂n i=1Ω c i ) = 1 − P(Ωc1)n = 1− (1− P(Ωi))n ≥ 1− exp(−nP(Ωi)). We follow the same line of reasoning:\nP\n( n⋃\ni=1\n{ µi = θ1, max\nj=1,...,m\nj∑\ns=1\n(Xi,s − γ̂) > B, min j=1,...,m\nj∑\ns=1\n(Xi,s − γ̂) > A })\n= 1− ( 1− P ( µi = θ1, max\nj=1,...,m\nj∑\ns=1\n(Xi,s − γ̂) > B, min j=1,...,m\nj∑\ns=1\n(Xi,s − γ̂) > A ))n\n= 1− ( 1− αP ( max\nj=1,...,m\nj∑\ns=1\n(Xi,s − γ̂) > B, min j=1,...,m\nj∑\ns=1\n(Xi,s − γ̂) > A ∣∣∣µi = θ1\n))n\n= 1− ( 1− α ( 1− P ( max\nj=1,...,m\nj∑\ns=1\n(Xi,s − γ̂) < B ∣∣∣µi = θ1 ) − P ( min\nj=1,...,m\nj∑\ns=1\n(Xi,s − γ̂) < A ∣∣∣µi = θ1\n)))n\n≥ 1− ( 1− α ( 1− exp ( −m(θ1 − γ̂)2/2 ) − 7 exp(−|A|(γ̂ − θ0)/2) ))n ≥ 1− exp [ −αn(1− exp ( −m(θ1 − γ̂)2/2 ) − 7 exp(−|A|(γ̂ − θ0)/2)) ] ≥ 1− exp [−αn(1− exp (−B(θ1 − γ̂))− 7 exp(−|A|(γ̂ − θ0)/2))]\nWhere the third-to-last inequality applies Lemmas 2 and 1.\nNow, we are ready to prove Theorem 5.\nProof. First, we consider the estimation of θ̂0 of Algorithm 2, then consider the sample complexity of the algorithm, and then prove correctness.\nLet ξ0 = {θ̂0 − θ0 ≥ − ǫ04 } and ξ1 = {θ̂0 − θ0 ≤ ǫ04 } be the events that we accurately estimate the parameter θ0. We will show that P(ξ0) ≥ 1 − δ′ and P(ξ1) ≥ 3/4 where δ′ = min{δ/8, 1mǫ20}. Let k1 = 5 and k2 = 8ǫ −2 0 log( 2k1 δ′ ). First note that\nP\n( k1⋃\ni=1\n{ |µ̂i,k2 − µi| ≥\nǫ0 4\n}) ≤ 2k1 exp(−2k2(ǫ0/4)2) ≤ δ′\nso that with probability at least 1−δ′ we have θ̂0 = mini=1,...,k1 µ̂i,k2 ≥ mini=1,...,k1 µi−ǫ0/4 ≥ θ0−ǫ0/4, and in particular, P(ξ0) ≥ 1−δ′. Let E = { ⋃k1 i=1{µi = θ0}} be the event that at least one of the distributions is light. Then\nP (E) = 1− αk1 ≥ 1− 2−k1 ≥ 31/32,\nso that under E ∩ ξ0, we have θ̂0 = mini=1,...,k1 µ̂i,k2 ≤ mini=1,...,k1 µi + ǫ0/4 = θ0 + ǫ0/4 which means P(ξc1) ≤ P(ξc0 ∪ Ec) ≤ δ/8 + 1/32 ≤ 1/16. Moreover, the total number of samples is bounded by k1k2 = cǫ −2 0 log(1/δ\n′) ≤ cǫ−20 log(max{1δ , log( 1α0δ )}) which is clearly dominated by log(1/δ) α0ǫ20 .\nWe now turn our attention to the sample complexity. By Wald’s identitity (Siegmund, 2013, Proposition 2.18),\nE[T ] = E\n[ N∑\ni=1\nMi ] = E[N ]E[M1] = E[N ]((1− α)E[M1|µ1 = θ0] + αE[M1|µ1 = θ1]).\nTrivially, E[N ] ≤ n and E[M1|µ1 = θ1] ≤ m, so we only need to bound E[M1|µ1 = θ0]. Clearly we have that\nE[M1|µ1 = θ0] = E[M1|ξ0, µ1 = θ0]P(ξ0) + E[M1|ξc0, µ1 = θ0]P(ξc0) ≤ E[M1|ξ0, µ1 = θ0] + δ′m\nso\nE[M1|ξ0, µ1 = θ0] ≤ ∞∑\nt=1\nP ( argmin\nj\n{ j∑\ns=1\n(X1,s − γ̂) < A ∣∣∣ξ0, µ1 = θ0 } ≥ t )\n=\n∞∑\nt=1\n1− P (\nmin j=1,...,t−1\nj∑\ns=1\n(X1,s − γ̂) < A ∣∣∣ξ0, µ1 = θ0\n)\n=\n∞∑\nt=0\n1− P (\nmin j=1,...,t\nj∑\ns=1\n(X1,s − γ̂) < A ∣∣∣ξ0, µ1 = θ0\n)\n≤ ∞∑\nt=0\n1− 1 γ̂−θ0≥ 2|A|t\n(1− exp ( −t(γ̂ − θ0)2/2 )\n≤ 2|A| γ̂ − θ0 + 2e1/2(γ̂ − θ0)−2 exp (−|A|(γ̂ − θ0)) ≤ 3|A| γ̂ − θ0 ≤ 293 ǫ20 .\nwhere the second inequality follows by applying Lemma 2 and the last inequality holds by ξ0 and the value of |A| since if ξ0 holds, γ̂ − θ0 = θ̂0 − θ0 + ǫ02 ≥ ǫ04 . Thus\nE[M1] ≤ (1− α) [( 293\nǫ20\n) + δ′m ] + αm ≤ δ′m+ 1\nǫ20\n( 293 + 64α log ( 14n δ )) ≤\ncα log (\n1 α0δ\n)\nǫ20\nfor some c where we use the fact that δ′m ≤ ǫ−20 . So we have\nE[T ] ≤ nE[M1] ≤ c′α log(1/α0) + c′′ log\n( 1 δ )\nα0ǫ20 .\nNow, we analyze the correctness claims. Under ξ0, γ̂ − θ0 ≥ ǫ04 . Note that this event fails to occur with probability less than δ/2, and if it is used in conjunction with some other event that fails to occur with probability δ/2, we may conclude that either of these events fail with probability less than δ.\nTo justify Claim 1, we apply Lemma 1 to observe that the probability that we output a light distribution is no greater than\nP(ξc0) + P\n( n⋃\ni=1\n{ max\nj=1,...,m\nj∑\ns=1\n(Xi,s − γ̂) > B,µi = θ0 } ∣∣∣ξ0 ) P(ξ0)\n≤ P(ξc0) + n(1− α)P (\nmax j=1,...,m\nj∑\ns=1\n(Xi,s − γ̂) > B ∣∣µi = θ0, ξ0\n)\n≤ δ/2 + 7n exp(−B(γ̂ − θ0)/2) ≤ δ\nwhere we have used γ̂ − θ0 ≥ ǫ04 and plugged in the values of B and n. To justify Claim 2, assume α0 ≤ α and ǫ0 ≤ θ1− θ0. We apply Lemma 3 to observe that the probability that we return a heavy distribution is at least\nP ( ξ0 ∩ ξ1 ∩ n⋃\ni=1\n{ µi = θ1, max\nj=1,...,m\nj∑\ns=1\n(Xi,s − γ̂) > B, min j=1,...,m\nj∑\ns=1\n(Xi,s − γ̂) > A })\n= P(ξ0 ∩ ξ1)P ( n⋃\ni=1\n{ µi = θ1, max\nj=1,...,m\nj∑\ns=1\n(Xi,s − γ̂) > B, min j=1,...,m\nj∑\ns=1\n(Xi,s − γ̂) > A }∣∣∣∣ξ0, ξ1 )\n≥ P(ξ0 ∩ E)(1− exp [−αn(1− exp (−B(ǫ0/4)) − 7 exp(−|A|(ǫ0/4)/2))]) ≥ (15/16)(1 − exp [ −αn(1− ( δ14n )2 − 1/3) ] ) ≥ (15/16)(8/9) ≥ 4/5\nwhere we have used P(ξ0 ∩ E) ≥ 1− P(ξc0) − P(Ec) ≥ 15/16, ( δ14n)2 ≤ 1/6, αn ≥ 2 log(9) and plugged in the values for A and B.\nTo justify Claim 3, we simply observe that the algorithm always terminates after n×m steps."
    }, {
      "heading" : "B.3 Proof of Theorem 6",
      "text" : "Proof. On each stage k, Algorithm 2 is called with δ/(2k2). By the guarantees of Theorem 5, the probability that Algorithm 3 ever outputs a light distribution is less than ∑∞ k=1 δ/(2k\n2) ≤ δ. Thus, if a distribution is output, it is heavy with probability at least 1−δ. We now show that the expected number of samples taken before outputting a distribution is bounded.\nLet K be the random stage in which Algorithm 3 outputs a distribution and let k∗ be the smallest k ∈ N that satisfies 2−k ≤ θ1 − θ0. By the guarantees of Theorem 5 and the independence of the stages k,\nP(K ≥ k∗ + i) ≤ ∑∞ ℓ=i( 1 5) ℓ = (54)( 1 5 )\ni. Moreover, if Mk is the number of measurements taken at stage k, then by Wald’s identity the expected number of measurements is bounded by\nE\n[ K∑\nk=1\nMk\n] = ∞∑\nk=1\nE[Nk]P(K ≥ k) ≤ ∞∑\nk=1\nc′α log(1/α) + c′′ log ( 2k2\nδ\n)\nα2−2k max{1, (54 )(15 )k−k∗}\n≤ k∗∑\nk=1\nc′′′ log ( 2k2∗ δ )\nα 4k + 5k∗ c\n′′′ α\n∞∑\nk=k∗+1\n( 2 log(k) + log(2δ ) ) (45) k\n≤ c′′′ log\n( 2k2∗ δ )\nα 4k∗+1 + 5k∗ c\n′′′ α\n∞∑\nk=k∗+1\n( 2 log(k) + log(2δ ) ) (45) k ≤ c ′′′′ log\n( k∗ δ )\nα (2k∗)2\nsince supα α log(1/α) ≤ e−1 and\n∞∑\nk=k∗\nlog(k)(45 ) k =\n2k∗−1∑\nk=k∗\nlog(k)(45 ) k +\n∞∑\nk=2k∗\nlog(k)(45 ) k/2(45 ) k/2\n≤ log(2k∗) 2k∗−1∑\nk=k∗\n(45) k +\n∞∑\nk=2k∗\n(45) k/2 ≤ (log(2k∗) + 2)\n∞∑\nk=k∗\n(45 ) k = 5 log(2e2k∗)( 4 5 ) k∗\nsince supk log(k)( 4 5 ) k/2 ≤ 1. Noting that k∗ ≤ log2( 1θ1−θ0 ) + 1 completes the proof."
    }, {
      "heading" : "B.4 Proof of Theorem 7",
      "text" : "Proof. The proof of this result is nearly identical to that of Theorem 6 except the following changes. Let K be the random stage in which Algorithm 4 outputs a distribution and let k∗ be the smallest k ∈ N that satisfies 2−k ≤ α. Moreover, if Mk is the number of measurements taken at stage k, then by Wald’s identity expected number of measurements is bounded by\nE\n[ K∑\nk=1\nMk\n] = ∞∑\nk=1\nE[Nk]P(K ≥ k) ≤ ∞∑\nk=1\nc′α log(2k) + c′′ log ( 2k2\nδ\n)\n2−kǫ2 max{1, (54 )(15 )k−k∗}\n≤ k∗∑\nk=1\nc′′′ log ( 2k2∗ δ )\nǫ2 2k + 5k∗ c\n′′′\nǫ2\n∞∑\nk=k∗+1\n( αk log(2) + 2 log(k) + log(2δ ) ) (25 ) k\n≤ c ′′′′ (αk∗ + log ( k∗ δ ))\nǫ2 2k∗ ≤ c ′′′′′ log (log(1/α)/δ) αǫ2\nby the same series of steps as the proof of Theorem 6 and the fact that ∑∞\nk=n ka k ≤ nan (1−a)2 for any a ∈ (0, 1). The final inequality follows from k∗ ≤ log2(1/α) + 1 and that αk∗ = α log2(2/α) ≤ 2."
    }, {
      "heading" : "B.5 Proof of Theorem 8",
      "text" : "Proof. The proof is broken up into a few steps, summarized as follows. For any given α0, ǫ0, Theorem 5\ntakes just O ( α log(1/α0)+log(1/δ)\nα0ǫ20\n) samples in expectation and the procedure makes an error (i.e. returns a\nlight distribution) with probability less than δ. Define ǫ = θ1 − θ0. In addition, ifǫ = θ1 − θ0, α ≥ α0, and ǫ ≥ ǫ0 then with probability at least 4/5 a heavy distribution is returned after the same expected\nnumber of samples. We will leverage this result to show that if we are given an upper bound γ0 such that 1αǫ2 ≤ γ0 then it is possible to identify a heavy distribution with probability at least 4/5 using just O (log2(γ0)γ0 [α log2(γ0) + log(log2(γ0)/δ)]) samples in expectation. Finally, we apply the “doubling trick” to γ so that even though the tightest γ is not known a priori, we can adapt to it using only twice the number of samples as if we had known it. Because each of the stages is independent of one another, the probability that the procedure requires more than ℓ∗+ i stages is less than (1/5)i , which yields our expected sample complexity.\nFor all ℓ ∈ N define δℓ = δ2ℓ3 and γℓ = 2ℓ. Fix some ℓ and consider the set {(α, ǫ) : 1αǫ2 = γℓ}. Clearly, in this set, α ∈ [1/γℓ, 1/2]. For all k ∈ {0, . . . , ℓ− 1}, define αk = 2 k\nγℓ and ǫk =\n√ 1\n2αkγℓ . The key\nobservation is that\n{(α, ǫ) : 1 αǫ2\n≤ γℓ} ⊆ log2 γℓ−1⋃\nk=0\n{(α, ǫ) : α ≥ αk, ǫ ≥ ǫk}. (7)\nTo see this, fix any (α′, ǫ′) such that 1 α′ǫ′2 ≤ γℓ. Let k∗ be the integer that satisfies αk∗ ≤ α′ < 2αk∗ . Such a k∗ must exist since αℓ−1 =\n1 2 ≥ α′ ≥ 1γℓǫ′2 ≥ 1 γℓ = α0. Then γℓ ≥ 1α′ǫ′2 ≥ 12αk∗ ǫ′2 which means ǫ′ ≥ √ 1\n2αk∗γℓ = ǫk∗ which proves the claim of (7). Consequently, even if no information about α or ǫ\nindividually is known but 1 αǫ2 ≤ γℓ, one can cover the entire range of valid (α, ǫ) with just log2(γℓ) = ℓ landmarks (αk, ǫk).\nFor any ℓ ∈ N and k ∈ {0, . . . , ℓ − 1}, if Algorithm 2 is used with α0 = αk, ǫ0 = ǫk and δ = δℓ then the probability that a light distribution is returned, declared heavy is less than δℓ. And the probability that a light distribution is returned, declared heavy for any ℓ ∈ N and k ∈ {0, . . . , ℓ − 1} is less than∑∞\nℓ=1 ℓδℓ = δ ∑∞ ℓ=1 ℓ/(2ℓ 3) ≤ δ. Thus, given that Algorithm 5 terminates with a non-null distribution h, h is heavy with probability at least 1 − δ. This proves correctness. We next bound the expected number of samples taken before the procedure terminates.\nWith the inputs given in the last paragraph for any k, ℓ, Algorithm 2 takes an expected number samples bounded by cγℓ(α log(1/αk)+log(1/δℓ)). Let L ∈ N be the random stage at which Algorithm 5 terminates with a non-null distribution h. Let ℓ∗ be the first integer such that there exists a k ∈ {0, . . . , ℓ∗ − 1} with α ≥ αk and ǫ ≥ ǫk (recall that in this case 1αkǫ2k ≤ γℓ∗). Then by the end of stage ℓ ≥ ℓ∗, at most cℓγℓ(α log(γℓ) + log(1/δℓ)) samples in expectation were taken on stage ℓ and with probability at least 4/5 the procedure terminated with a heavy coin. By the independence of samples between rounds, observe that P(L ≥ ℓ∗ + i) = ∑∞ j=i P(L = ℓ∗ + j) ≤ (54 )(15 )i. Thus, if Mℓ is the number of samples taken at stage ℓ\nthen by Wald’s identify, the total expected number of samples taken before termination is bounded by\nE\n[ L∑\nℓ=1\ncℓγℓ(α log(γℓ) + log(1/δℓ))\n] = ∞∑\nℓ=1\nE[Mℓ]P(L ≥ ℓ) ≤ ∞∑\nℓ=1\ncℓγℓ(α log(γℓ) + log(1/δℓ))P(L ≥ ℓ)\n≤ ℓ∗∑\nℓ=1\ncℓγℓ(α log(γℓ) + log(1/δℓ)) +\n∞∑\nℓ=ℓ∗+1\ncℓγℓ(α log(γℓ) + log(1/δℓ))( 5 4 )( 1 5 )\nℓ−ℓ∗\n≤ ℓ∗∑\nℓ=1\ncℓ2ℓ(αℓ+ log(2ℓ3/δ)) +\n∞∑\nℓ=ℓ∗+1\ncℓ2ℓ(αℓ+ log(2ℓ3/δ))(54 )( 1 5 )\nℓ−ℓ∗\n≤ cℓ∗(αℓ∗ + log(2ℓ3∗/δ)) ℓ∗∑\nℓ=1\n2ℓ + c(54 )5 ℓ∗\n∞∑\nℓ=ℓ∗+1\n( αℓ2(25) ℓ + 3ℓ log(ℓ)(25 ) ℓ + log(2/δ)ℓ(25 ) ℓ )\n≤ 2cℓ∗2ℓ∗(αℓ∗ + log(2ℓ3∗/δ)) + c(54 )5 ℓ∗ ( 2α(ℓ∗ + 1) 2(25 ) ℓ∗ + 12 log(2e2ℓ∗)(ℓ∗ + 1)( 2 5 ) ℓ∗ + 4 log(2/δ)(ℓ∗ + 1)( 2 5 ) ℓ∗ ) ≤ c′ℓ∗2ℓ∗(αℓ∗ + log(ℓ∗) + log(1/δ))\nfor some absolute constant c′ since ∑∞\nk=n ka k ≤ nan(1−a)2 , ∑∞ k=n k\n2ak ≤ n2an(1−a)3 , and ∞∑\nℓ=ℓ∗+1\nℓ log(ℓ)(25 ) ℓ ≤ log(2ℓ∗)\n2ℓ∗∑\nℓ∗+1\nℓ(25) ℓ +\n∞∑\n2ℓ∗+1\nℓ(25) ℓ/2 ( log(ℓ)(25 ) ℓ/2 )\n≤ log(2e2ℓ∗) ∞∑\nℓ∗+1\nℓ(25 ) ℓ ≤ 4 log(2e2ℓ∗)(ℓ∗ + 1)(25 )ℓ∗\nsince maxx≥1 log(x)(25 ) x/2 ≤ 1. Noting that ℓ∗ ≤ log2( 1αǫ2 )+ 1, we have that the total number of samples, in expectation, is bounded by\nc′ℓ∗2 ℓ∗(αℓ∗ + log(ℓ∗) + log(1/δ)) ≤ c′′\nlog2( 1 αǫ2 )\nαǫ2 (α log2(\n1 αǫ2 ) + log(log2( 1 αǫ2 )) + log(1/δ))\n≤ c′′′ log2( 1 αǫ2 )\nαǫ2 (α log2(\n1 ǫ2 ) + log(log2( 1 αǫ2 )) + log(1/δ))\nwhere we’ve used the fact that supα∈[0,1] α log(1/α) ≤ e−1."
    }, {
      "heading" : "C Gaussians",
      "text" : ""
    }, {
      "heading" : "C.1 On the detection of a mixture of Gaussians",
      "text" : "For known σ2, consider the hypothesis test of Problem 1 . In what follows, let χ2(θ1, θ0) and KL(θ1, θ0) be the chi-squared and KL divergences of the two distributions of H1. Note that for (θ1−θ0)2\nσ ≤ 1, we have that χ2(θ1, θ0) = e (θ1−θ0) 2\nσ2 − 1 ≤ 2 (θ1−θ0)2σ2 = 4KL(θ1, θ0) Theorem 2 says that for (θ1−θ0) 2\nσ2 ≤ 1, a procedure that has maximum probability of error less than δ\nrequires at least max {\n1−δ α , log(1/δ) 4α2KL(θ1,θ0)\n} samples to decide the above hypohesis test, even if α, θ0, θ1 are\nknown. The next subsection shows that if α, θ0, θ1 are unknown then one requires at least log(1/δ)\n2[αKL(θ1,θ0)]2\nsamples to decide the above hypothesis test correctly with probability at least 1−δ. This is likely achievable using the method of moments (Hardt and Price, 2014)."
    }, {
      "heading" : "C.2 Lower bounds",
      "text" : "Theorem 9. For known σ2, consider the hypothesis test of Problem 1. If θ∗ = (1 − α)θ0 + αθ1 and θ1−θ0\nσ ≤ 1 then\nχ2((1− α)fθ0(x) + αfθ1(x)|fθ∗(x)) ≤ c′ ( α(1− α)(θ1 − θ0) 2\nσ2\n)2\nfor some absolute constant c′.\nProof. If fθ = N (θ, σ2) then fθ(x) = h(x) exp(η(θ)x − b(θ)) where h(x) = 1√2πσ2 e − x2 2σ2 , η(θ) = θ σ2 , and b(η(θ)) = η(θ) 2σ2\n2 = θ2 2σ2 . Thus,\nθ∗ = η −1((1− α)η(θ0) + αη(θ1) ) = (1− α)θ0 + αθ1\nand\nsup y∈[θ0,θ1] b(2η(y) − η(θ∗))− (2b(η(y)) − b(η(θ∗))) = sup y∈[θ0,θ1] (y − θ∗)2 σ2 ≤ (θ1 − θ0) 2 σ2 =: κ\nand\nsup x∈[ḃ(η(θ−)),ḃ(η(θ+))] fḃ−1(x)(x) = sup x∈[θ−,θ+] sup θ∈R 1√ 2πσ2 e− (x−θ)2 2σ2 ≤ 1√ 2πσ2 =: γ.\nNote that for any θ < θ′ we have ḃ(η(θ′)) − ḃ(η(θ)) = θ′ − θ, M2(θ) = σ2, and M4(θ) = 3σ4. Plugging these values into the theorem we have\nc = eκ (\nsup θ∈[θ0,θ1]\nM2(θ) 2 ( 2 + γ ( ḃ(η(θ+))− ḃ(η(θ−)) ))\n+ 8M4(θ−) + 8M4(θ+) + 16 ( ḃ(η(θ+))− ḃ(η(θ−)) )4 + 25γ ( ḃ(η(θ+))− ḃ(η(θ−))\n)5)\n=e (θ1−θ0)\n2\nσ2 ( σ4 ( 2 +\n2(θ1 − θ0)√ 2πσ ) + 48σ4 + 256 (θ1 − θ0)4 + 645√2π (θ1 − θ0)5 σ )\nnoting that θ+ − θ− = 2(θ1 − θ0). If θ1−θ0σ ≤ 1 then c = c′σ4 for some absolute constant c′ and (η(θ1)− η(θ0))2 = (θ1−θ0) 2\nσ4 which yields the final result.\nC.3 Gaussian Upper bound for known α, θ0, θ1\nFor known σ2, consider the hypothesis test of Problem 1 with θ = θ0. We observe a sample X1, . . . ,Xn and are trying to establish whether it came from H0 or H1.\nConsider the test\n1\nn\nn∑\ni=1\n1Xi>θ1\nH1 ≷ H0 P1(X1 > θ1) + P0(X1 > θ1) 2 =: γ.\nIf ǫ = P1(X1 > θ1)− P0(X1 > θ1) then\nP1\n( 1\nn\nn∑\ni=1\n1Xi>θ1 ≤ γ ) = P1 ( 1\nn\nn∑\ni=1\n1Xi>θ1 ≤ P1(X1 > θ1)− ǫ/2 ) ≤ e−nǫ2/2\nand\nP0\n( 1\nn\nn∑\ni=1\n1Xi>θ1 ≥ γ ) = P0 ( 1\nn\nn∑\ni=1\n1Xi>θ1 ≥ P0(X1 > θ1) + ǫ/2 ) ≤ e−nǫ2/2\nby sub-Gaussian tail bounds. If Q(x) = ∫∞ x 1√ 2π e−z 2/2dz and ∆ = θ1−θ0σ then\nP0(X1 > θ1) = Q (∆) P1(X1 > θ1) = (1− α)Q (∆) + α 1\n2\nso\nǫ = α\n( 1\n2 −Q (∆)\n) = α ∫ ∆\n0\n1√ 2π e−x 2/2dx ≥ min{ α∆ 4 √ 2π , 1 4 α}.\nThus, the test fails with probability at most\nexp [ −nα2min { (θ1 − θ0)2 64πσ2 , 1 32 }] .\nWe conclude that if ∆ = θ1−θ0σ ≤ 1 and n ≥ (θ1−θ0)2 log(1/δ) 64πα2σ2 = KL(Pθ1 ,Pθ0) log(1/δ)\n64πα2 the correct hypothesis is selected. The 1/α sufficiency result holds for large enough ∆ since one merely needs to observe just one sample since the probability of it coming from θ0 is negligible."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2016,
    "abstractText" : "This paper studies the trade-off between two different kinds of pure exploration: breadth versus depth. The most biased coin problem asks how many total coin flips are required to identify a “heavy” coin from an infinite bag containing both “heavy” coins with mean θ1 ∈ (0, 1), and “light” coins with mean θ0 ∈ (0, θ1), where heavy coins are drawn from the bag with probability α ∈ (0, 1/2). The key difficulty of this problem lies in distinguishing whether the two kinds of coins have very similar means, or whether heavy coins are just extremely rare. This problem has applications in crowdsourcing, anomaly detection, and radio spectrum search. Chandrasekaran and Karp (2014) recently introduced a solution to this problem but it required perfect knowledge of θ0, θ1, α. In contrast, we derive algorithms that are adaptive to partial or absent knowledge of the problem parameters. Moreover, our techniques generalize beyond coins to more general instances of infinitely many armed bandit problems. We also prove lower bounds that show our algorithm’s upper bounds are tight up to log factors, and on the way characterize the sample complexity of differentiating between a single parametric distribution and a mixture of two such distributions. As a result, these bounds have surprising implications both for solutions to the most biased coin problem and for anomaly detection when only partial information about the parameters is known.",
    "creator" : "LaTeX with hyperref package"
  }
}