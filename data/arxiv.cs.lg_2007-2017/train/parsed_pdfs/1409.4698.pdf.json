{
  "name" : "1409.4698.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "A Mixtures-of-Experts Framework for Multi-Label Classification",
    "authors" : [ "Charmgil Hong" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "We develop a novel probabilistic approach for multi-label classification that is based on the mixtures-of-experts architecture combined with recently introduced conditional tree-structured Bayesian networks. Our approach captures different input-output relations from multi-label data using the efficient tree-structured classifiers, while the mixtures-of-experts architecture aims to compensate for the tree-structured restrictions and build a more accurate model. We develop and present algorithms for learning the model from data and for performing multi-label predictions on future data instances. Experiments on multiple benchmark datasets demonstrate that our approach achieves highly competitive results and outperforms the existing state-of-the-art multi-label classification methods."
    }, {
      "heading" : "1 INTRODUCTION",
      "text" : "Mutli-Label Classification (MLC) refers to a classification problem in which data instances are associated with multiple class variables that may reflect different views, functions or components describing the data. MLC naturally arises in many real world problems, such as text categorization (Kazawa et al., 2005; Zhang and Zhou, 2006) where a document may be associated with different topics reflecting its content; semantic scene and video classification (Boutell et al., 2004; Qi et al., 2007a) where different images or videos are assigned to different categories or tagged based on their content; or in genomics where individual genes may be associated with multiple functions (Clare and King, 2001; Zhang and Zhou, 2006).\nFormally speaking, the MLC problem is specified by learning a function h : Rm → Y that maps each data instance, represented by a feature vector x = (x1, ..., xm), to class assignments, represented by a vector of d binary values y = (y1, ..., yd), such that\nyi = {0, 1} indicates the absence or presence of the i-th class. However, the application of the model in practice raises tow important questions: how to define/represent such a function for high-dimensional feature and label spaces; and how to learn this function from data.\nThe problem of learning MLC classifiers from data has been studied extensively by the machine learning community in recent years. One of the key challenges in solving the problem is how to efficiently model and learn dependences among class variables given the fact that the number of possible class assignments is exponential in d. A simple solution to this is to assume that all class variables Yi for i = 1, ..., d are conditionally independent of each other and, hence, learn d functions to predict each class separately (Clare and King, 2001; Boutell et al., 2004). However, this may not suffice when dependences among labels exist. To overcome this limitation, more advanced machine learning methods that model class relations have been proposed. These include two-layer classification models (Godbole and Sarawagi, 2004; Cheng and Hüllermeier, 2009), classifier chains (Read et al., 2009; Zhang and Zhang, 2010; Dembczynski et al., 2010), multi-dimensional Bayesian network classifiers (van der Gaag and de Waal, 2006; Bielza et al., 2011; Antonucci et al., 2013) and output compression and coding methods (Hsu et al., 2009; Tai and Lin, 2010; Zhang and Schneider, 2011, 2012).\nHowever, the above mentioned methods are still rather limited especially when the relations among features and labels become more complex. More specifically, if the relations tend to change across the dataset, these methods may fail to respond with correct classification, since they are designed to capture only one dependence structure from data. For example, in automated image tagging, an object can be tagged as {cat, pet} or {cat, wild animal} according to its context; Similarly, in medical informatics, patients who are suffering from the same disease may receive different sets of medications due to their medical history or allergic reactions. To address such issues, ensemble techniques have been recently adopted to MLC settings (Read et al., 2009; Dembczynski et al., 2010; An-\nar X\niv :1\n40 9.\n46 98\nv1 [\ncs .L\nG ]\n1 6\nSe p\n20 14\ntonucci et al., 2013). However, these approaches are still limited in that they are relying on randomization for obtaining multiple dependence relations and using simple averaging to make ensemble predictions. As a result, the improvement we could obtain from them is not significant or consistent.\nIn this paper, we propose and study a new probabilistic MLC approach that attempts to remedy the limitations of the existing approaches. Our approach relies on the mixtures-of-experts (ME) framework (Jacobs et al., 1991; Yuksel et al., 2012) and conditional tree-structured Bayesian network (CTBN) classifiers proposed recently in (Batal et al., 2013). Briefly, CTBN defines a multi-label classifier that is modeling P (Y|X) where dependences among class variables for different inputs are modeled by a collection of classifiers (for example, logistic regression models) linked together in directed tree structures. The model comes with a number of computational advantages such as efficient learning, and efficient MAP inference that lets us find the best set of class assignments for a given data instance x.\nA limitation of CTBN is that dependences among class variables are restricted to tree structures which may not reflect all existing dependences in the data. Our new framework based on the ME architecture aims to take advantage of the computational benefits of CTBNs and remedy its limitation by learning and combining multiple CTBNs, where each CTBN can cover a different region of the input space and/or can help to model the different dependences among class variables. We develop and present an EM algorithm for learning the new model from data and an algorithm for making the MAP inferences for predicting class assignments for future data instances.\nThe rest of the paper is organized as follows. Section 2 formally defines the MLC problem. Section 3 gives summary on related MLC research. Section 4 provides the necessary definitions for ME and CTBN, which are needed to understand our model. Section 5 describes our proposed solution by going over its model representation (Section 5.1) and the supporting algorithms for parameter learning (Section 5.2), structure learning (Section 5.3) and prediction (Section 5.4). Section 6 presents the experimental results and evaluation. Section 7 concludes the paper."
    }, {
      "heading" : "2 PROBLEM DEFINITION",
      "text" : "Multi-Label Classification (MLC) is a classification problem in which each data instance is associated with a subset of labels from a set of possible labels L. Let d = |L|. We can define d binary class variables Y1, ..., Yd, where the value of Yi in instance x indicates whether or not the i-th label in L is present in x. We are given labeled training data D = {x(n),y(n)}Nn=1, where x(n) = (x\n(n) 1 , ..., x (n) m ) is the m-dimensional\nfeature vector of the n-th instance (the input) and\ny(n) = (y (n) 1 , ..., y (n) d ) is its d -dimensional class vector (the output). We want to learn a function h that fits D and assigns to each instance, represented by its feature vector, a class vector:\nh : Rm → {0, 1}d\nOne way to approach this task is to model and learn the conditional joint distribution P (Y|X), where Y is a random variable for the class vector and X is a random variable for the feature vector. Assuming the 0-1 loss function, the optimal classifier h∗ assigns to each instance x the maximum a posteriori (MAP) assignment of class variables:\nh∗(x) = arg max y P (Y=y|X=x) (1)\n= arg max y1,...,yd\nP (Y1 =y1, ..., Yd=yd|X=x)\nA key challenge for modeling and learning P (Y|X) from data, as well as for defining the corresponding MAP classifier, is that the number of all possible class assignments for a given x is exponential in d (there are 2d different assignments). Our goal is to develop a parametric model that allows us to efficiently model and learn P (Y|X) from data. Notation: For notational convenience, we will omit the index superscript (n) when it is not necessary. We may also abbreviate the expressions by omitting variable names; e.g., P (Y1 = y1, ..., Yd = yd|X=x) = P (y1, ..., yd|x)."
    }, {
      "heading" : "3 RELATED RESEARCH",
      "text" : "In this section, we review the related research in MLC and outline the main differences from our approach.\nEarlier MLC methods ignore the relations between classes and learn to predict each class separately (Clare and King, 2001; Boutell et al., 2004). Zhang and Zhou (2007) presented the multi-label k-nearest neighbor method, which predicts each class label by combining KNN with Bayesian inference. An approach that enriches the feature space by incorporating an intermediate layer of classifiers was proposed by (Godbole and Sarawagi, 2004) and later on by (Cheng and Hüllermeier, 2009). The main drawback of these methods is that class dependences are either not represented at all, or represented indirectly in a limited way.\nSeveral methods have been proposed to probabilistically model MLC. The classifier chains (CC) method (Read et al., 2009) decomposes the relations among the class variables using the chain rule of probability:\nP (Y1, ..., Yd|X) = d∏ i=1 P (Yi|X, Y1, ..., Yi−1)\nEach component in the chain is a classifier that is learned separately by incorporating the (0/1) predictions of preceding classifiers as additional features.\nZhang and Zhang (2010) further studied the influence of the chain order and presented a method to learn an effective ordering from data. The main disadvantage of CC, however, is that they do not perform proper probabilistic inference for classification. Instead, they simply propagate the predictions according to the class order defined by the chain. To overcome this shortcoming, Dembczynski et al. (2010) presented the probabilistic classifier chains method by extending CC to estimate the entire class posterior distribution. However, this method needs to evaluate all possible 2d label configurations, which greatly limits its applicability.\nOther probabilistic MLC methods are based on multidimensional Bayesian networks (van der Gaag and de Waal, 2006; Bielza et al., 2011; Antonucci et al., 2013). These methods attempt to build a generative model of P (X,Y) using a restricted Bayesian network structure, which assumes all class variables are top nodes and all feature variables are their descendants. The limitation of this approach is that it must learn the structure of the full joint distribution over both features and classes, which can be very complex. Besides, it requires all features to be a priori discretized. In contrast, our approach directly learns the conditional distribution P (Y|X) and takes advantage of modern discriminative classifiers.\nAn alternative approach for MLC is based on output coding. The idea is to project the output space Y into a lower dimensional space Y′, learn to predict Y′, and then reconstruct the original output from the noisy predictions. Methods that fall into this category use different dimensionality reduction techniques, such as compressed sensing (Hsu et al., 2009), principal component analysis (Tai and Lin, 2010) and canonical correlation analysis (Zhang and Schneider, 2011). The state-of-the-art in output coding utilizes a maximum margin formulation (Zhang and Schneider, 2012) that promotes both discriminative and predictable codes. The limitation of output coding methods is that they can only predict the single “best” output for a given input, and they cannot compute probabilities for different input-output pairs.\nSeveral researchers proposed using ensemble methods for MLC. In general, the objective was to compensate for the restrictions the base MLC models introduce using a set of models and their combinations. Read et al. (2009) presented a simple method that averages the predictions of multiple randomly ordered CCs trained on random subsets of the data. Antonucci et al. (2013) proposed an ensemble of multi-dimensional Bayesian networks combined via simple averaging. In this work, we develop an approach based on the mixtures-ofexperts architecture. The difference from the previous work is that our approach optimizes the structures and parameters of base classifiers in a principled way, and that our approach can help to overcome the restriction of the base MLC classifier by modeling better the relations among inputs and their labels, as well as, mutual relations among different labels."
    }, {
      "heading" : "4 PRELIMINARY",
      "text" : "The MLC solution we propose in this work combines multiple base MLC classifiers using the mixtures-ofexperts (ME) (Jacobs et al., 1991) architecture. The base classifiers we use are based on the conditional treestructured Bayesian networks (CTBN) (Batal et al., 2013). To start with, we briefly review the basics of ME and CTBN.\nME is a mixture model that consists of a set of experts that are combined via gating (or switching) module to represent the conditional distribution P (y|x). The model is defined by the following decomposition:\nP (y|x) = K∑ k=1 P (Ek|x)P (y|x, Ek), (2)\nwhere P (y|x, Ek) is the distribution of outputs defined by the k-th expert Ek and P (Ek|x) is the context sensitive prior of the k-th expert that is implemented by the gating function gk(x). In general, depending on the choice of the expert model, ME can be used for either regression or classification (Yuksel et al., 2012).\nThe ME model defines a soft-partitioning of the input space (via the gating module and its functions), on which the K experts represent different input-output relations. ME is especially useful when individual expert models are good in representing local inputoutput relations but fail to accurately capture the relations for the complete input space. The ability to switch among the experts in different regions of the input space allows to compensate for the limitation of individual experts and improve the overall model and its accuracy. In general, depending on the choice of the expert model, ME can be used for either regression or classification (Yuksel et al., 2012).\nME has been successfully adopted in a range of applications such as handwriting recognition (Ebrahimpour et al., 2009), text classification (Estabrooks and Japkowicz, 2001), climate prediction (Lu, 2006) and bioinformatics (Qi et al., 2007b; Cao et al., 2010). In addition, ME has been vigorously used in time series analysis, including speech recognition (Mossavat et al., 2010), financial forecasting (Weigend and Shi, 2000) and dynamic control systems (Jacobs and Jordan, 1993; Weigend et al., 1995). Recently, ME has been used in social network analysis, in which different social behavior patterns are modeled through mixtures (Gormley and Murphy, 2011).\nIn this work, we apply the ME approach in context of MLC. We would like to note that although modeling of the joint conditional probability P (y1, ..., yd|x) has been attempted in context of multiple target regression (Jordan and Xu, 1995; Waterhouse, 1997), its application to MLC is new to the best of our knowledge.\nIn particular, we combine ME with CTBN to model individual experts. CTBN is a recently proposed prob-\nabilistic MLC method that has been shown to be competitive and efficient on a range of domains. CTBN defines P (Y|X) using a collection of classifiers modeling relations in between features and individual labels that are tied together using a special Bayesian network structure that approximates the dependence relations among the class variables. In modeling of the dependences, it allows each class variable to have at most one other class variable as a parent (without creating a cycle) besides the feature vector X.\nA CTBN T defines the joint distribution of class vector (y1, ..., yd) conditioned on feature vector x as:\nP (y1, ..., yd|x, T ) = d∏ i=1 P (yi|x, yπ(i,T )), (3)\nwhere π(i, T ) denotes the parent class of class Yi in T (by convention, π(i, T ) = {} if Yi does not have a parent class). For example, the conditional joint distribution of class assignment (y1, y2, y3, y4) given x according to the network T in Figure 1 is defined as:\nP (y1, y2, y3, y4|x, T ) = P (y3|x) · P (y2|x, y3) · P (y1|x, y2) · P (y4|x, y2)"
    }, {
      "heading" : "5 PROPOSED SOLUTION",
      "text" : "In this section, we develop a Multi-Label Mixtures-ofExperts (ML-ME) model, which uses the ME framework in combination with the CTBN classifiers to improve the classification accuracy of MLC tasks, and develop algorithms for its learning and predictions. Our key motivation is to exploit the divide and conquer principle, which states that a large, complex problem can be decomposed and effectively solved using simpler sub-problems. More specifically, we want to accurately model relations among inputs X and class variables Y by learning multiple CTBN models and by improving their predictive ability by combining their outputs. In section 5.1, we describe the mixture defined by the ML-ME model. In section 5.2 through 5.4, we present the learning and prediction algorithms for the ML-ME model."
    }, {
      "heading" : "5.1 REPRESENTATION",
      "text" : "By following the definition of ME in Equation (2), MLME defines the multivariate posterior distribution of\nclass vector y = (y1, ..., yd) as:\nP (y|x) = K∑ k=1 gk(x)P (y|x, Ek), (4)\nwhere P (y|x, Ek) is the joint conditional distribution defined by the k-th expert Ek and gk(x) = P (Ek|x) is the gating function reflecting how much the k-th expert should contribute to predict classes for input x. In this work, we model the gating functions for the different experts with the help of the softmax function, which is also know as normalized exponential:\ngk(x) = exp(θGkx)∑K\nk′=1 exp(θGk′ x) , (5)\nwhere ΘG = {θGk}Kk=1 is a set of the softmax parameters.\nTo model the different experts and their probabilistic MLC predictions P (y|x, Ek) in Equation (4), we rely on the CTBN model introduced in the previous section. Employing multiple CTBN models Tk : k ∈ {1, ...,K} as experts in the mixture, our ML-ME model defines the joint distribution of class vector y as:\nP (y|x) = K∑ k=1 gk(x)P (y|x, Tk) (6)\n= K∑ k=1 gk(x) d∏ i=1 P (yi|x, yπ(i,Tk)),\nFigure 2 depicts an example ML-ME model, which consists of K CTBNs and whose output is probabilistically mixed with the help of the gating network.\nParameters Let Θ = {ΘG,ΘT } denote the set of all the parameters of the ML-ME model, where ΘG = {θGk}Kk=1 are parameters of the the gating model, and ΘT = {θTk}Kk=1 are parameters of CTBNs defining individual experts. Since the gating function for each expert is defined by a linear combination of inputs, there are |θGk | = (m + 1) parameters\nper expert. Consequently, the total number of parameters needed to represent the full gating network is: |ΘG| = K(m+ 1). The parameterization of a CTBN expert is done by modeling the conditional probability distributions (CPDs) of class variable Yi conditioned on its parents. For each Yi, we learn two logistic regression classifier functions according to the parent class: P (yi|x, yπ(i,Tk) = 0) and P (yi|x, yπ(i,Tk) = 1). Let θTk denote the parameters for the k-th CTBN expert. Then, for |θTk | = 2d(m+ 1), since two different classifiers are defined on each class variable. Hence, |ΘT | = 2dK(m+ 1). Table 1 summarizes the notation and parameterization of our ML-ME model. In summary, the total number of parameters for our model is K(1 + 2d)(m+ 1)."
    }, {
      "heading" : "5.2 PARAMETER LEARNING FOR FIXED STRUCTURES",
      "text" : "In this section, we describe how to learn the parameters of ML-ME, when the structures of individual CTBN experts are known and fixed. We return to the structure learning problem in Section 5.3.\nΘ = {ΘG,ΘT } denotes the set of all the parameters of the ML-ME model. Our objective is to find the parameters that optimize the log-likelihood of the training data:\nl(D; Θ) = N∑ n=1 logP (y(n)|x(n))\nBy substituting the joint probability with the definition of ML-ME (Equation (6)), we obtain:\nl(D; Θ) = N∑ n=1 log K∑ k=1 gk(x (n))P (y(n)|x(n), Tk) (7)\nWe refer to Equation (7) as the observed log-likelihood. However, optimizing this function is very difficult because there is a summation inside the log, which results in a non-convex function. To overcome this difficulty, we instead optimize the complete log-likelihood in the expectation-maximization (EM) framework.\nThe complete log-likelihood is defined by associating each instance (x(n),y(n)) with a hidden variable z(n) ∈ {1, ...,K} indicating to which expert it belongs to:\nlc(D; Θ) = N∑ n=1 logP (y(n), z(n)|x(n)) (8)\n= N∑ n=1 log K∏ k=1 P (y(n), Tk|x(n))1[z (n)=k]\n= N∑ n=1 log K∏ k=1 [ gk(x (n))P (y(n)|x(n), Tk) ]1[z(n)=k]\n= N∑ n=1 K∑ k=1 1[z(n) = k] [ log gk(x (n))P (y(n)|x(n), Tk) ] ,\nwhere 1[z(n) = k] is the indicator function that evaluates to one if the n-th instance belongs to the k-th expert and to zero otherwise.\nThe EM framework iteratively optimizes the expected complete log-likelihood (E [lc(D; Θ)]), which is always a lower bound of the observed log-likelihood (Dempster et al., 1977). In the E-step, the expectation is computed using the current set of parameters; in the M-step, the parameters of the model are relearned by maximizing the expected complete log-likelihood. In the following, we explain our parameter learning algorithm by deriving the E-step and M-step for ML-ME.\nE-step In the E-step, we compute the expectation of the complete log-likelihood, which reduces to computing the expectation of the hidden variable.\nE [ 1[z(n) = k] ] = P (z(n) = k|y(n),x(n))\n= P (Tk|y(n),x(n))\nNotice that this becomes the posterior of the k-th expert given the observation and the current set of parameters. Let h (n) k denote P (Tk|y(n),x(n)). We can write h (n) k using Bayes rule as:\nh (n) k = gk(x (n))P (y(n)|x(n), Tk)∑K\nk′=1 gk′(x (n))P (y(n)|x(n), Tk′)\n(9)\nM-step In the M-step, we learn the model parameters {ΘG,ΘT } that maximize the expected complete log-likelihood. Let us first rewrite Equation (8) using h (n) k and by switching the order of summations:\nK∑ k=1 N∑ n=1 h (n) k log gk(x (n)) + h (n) k logP ( y(n)|x(n), Tk ) (10)\nFor the fixed h (n) k we can decompose Equation (10) into two parts, each of which respectively involves the\ngating model parameters ΘG and the CTBN parameters ΘT :\nf1(D; ΘG) = K∑ k=1 N∑ n=1 h (n) k log gk(x (n))\nf2(D; ΘT ) = K∑ k=1 N∑ n=1 h (n) k logP ( y(n)|x(n), Tk ) Notice that, in order to optimize the gate parameters ΘG, we only need to optimize f1(D; ΘG); whereas to optimize the CTBN parameters ΘT , we only need to optimize f2(D; ΘT ).\nWe first show how to learn the parameters ΘG = {θG1 , ...,θGK} of the gating model. We can rewrite f1(D; ΘG) using Equation (5) as:\nf1(D; ΘG)\n= K∑ k=1 N∑ n=1 h (n) k θGkx (n) − h(n)k log K∑ k′=1 exp(θGk′ x (i))\nSince f1(D; ΘG) is concave in ΘG, gradient methods are guaranteed to find its optimal solution. The derivative of the log-likelihood with respect to θGj is calculated as:\n∇θjf1(D; ΘG) = N∑ n=1 { h (n) j − gj(x (n)) } x(n) (11)\nNote that the derivative becomes zero when gj(x (n)) = P (Tk|x(n)) and h(n)j = P (Tk|y(n),x(n)) are equal.\nWe can solve the optimization of f1(D; ΘG) using any gradient method. However, in practice the dimensionality of the input space can be high which may result in model overfitting. To prevent the overfitting problem, we add L2-regularization penalty R(ΘG) = λ 2 ∑K k=1 ||θGk ||22 to the optimization function to penalize model complexity. In our experiments, we use the L-BFGS algorithm (Liu and Nocedal, 1989) to optimize this regularized objective function. L-BFGS is a quasi-Newton optimization method that uses a sparse approximation to the inverse Hessian matrix to steer its search through parameter space. The algorithm is known to provide faster convergence rate and be well suited for optimization problems with a large number of variables.\nTo optimize the parameters of the CTBN experts ΘT = {θT1 , ...,θTK}, we optimize f2(D; ΘT ). This optimization decomposes into optimization of parameters of individual CTBN Tk. Note that f2 is the weighted log-likelihood where h\n(n) k serves as\nthe instance weight. Following (Batal et al., 2013), we use the logistic regression model to represent P (y(n)|x(n), Tk) in all our experiments. To prevent overfitting, we apply L2-regularized instance-weighted logistic regression to optimize f2. Algorithm 1 summarizes our parameter learning algorithm.\nAlgorithm 1 learn-mixture-parameters\nInput: Training data D; base CTBN experts T1, ..., TK Output: Model parameters {ΘG,ΘT } 1: repeat 2: E-step: 3: for k = 1 to K, n = 1 to N do\n4: Compute h (n) k using Equation (9) 5: end for 6: M-step: 7: ΘG = arg maxΘG l1(D; ΘG)−R(ΘG) 8: for k = 1 to K do 9: θTk = arg max ∑N n=1 h (n) k logP (y\n(n)|x(n), Tk) 10: end for 11: until convergence"
    }, {
      "heading" : "5.2.1 Complexity",
      "text" : "E-step We compute h (n) k for each instance on every CTBN expert. This requires O(md) multiplications. Hence, the complexity of a single E-step is O(KNmd).\nM-step Computing the derivative in Equation (11) requires O(mN) multiplications, therefore optimizing ΘG requires O(mNl) operations, where l is the number of L-BFGS steps. To learn ΘT , we optimize an instance-weighted logistic regression for every node of each of the K experts, which requires learning of O(Kd) logistic regression models."
    }, {
      "heading" : "5.3 STRUCTURE LEARNING",
      "text" : "In the previous section, we described the parameter learning of ML-ME by assuming we have fixed the individual CTBN structures. In this section, we present how to automatically learn CTBN structures from data. In a nutshell, we apply a sequential boostinglike heuristic; i.e., on each iteration, we learn a structure that focuses on “hard instances” that previous CTBNs tend to misclassify. In the following, we first describe how to learn a single CTBN structure from instance-weighted data. After that, we describe how to re-weight the instances and incrementally add new structures to the ML-ME model."
    }, {
      "heading" : "5.3.1 Learning a Single CTBN Structure on Weighted Data",
      "text" : "To learn the CTBN structure that best approximates weighted data, we find the structure that maximizes the weighted conditional log-likelihood (WCLL) on {D,Ω}, where D = {x(n),y(n)}Nn=1 is the data and Ω = {ω(n)}Nn=1 is the instance weight. Note that we further split D into training data Dtr and hold-out data Dh.\nGiven a CTBN structure T , we train its parameters using Dtr, which corresponds to learning instanceweighted logistic regression using Dtr and the corresponding instance weights. On the other hand, we use WCLL of Dh to define the score that measures the\nquality of T .\nScore(T ) = ∑ n∈Dh ω(n) logP (y(n)|x(n), T )\n= ∑ n∈Dh d∑ i=1 ω(n) logP (y (n) i |x (n), y (n) π(i,T )) (12)\nBelow we describe our algorithm for obtaining the CTBN structure that optimizes Equation (12) without having to evaluate all of the exponentially many possible tree structures.\nLet us first define a weighted directed graph G = (V,E) as follows:\n• There is one vertex Vi for each class variable Yi : i ∈ {1, ..., d}.\n• There is a directed edge Ej→i from each vertex Vj to each vertex Vi (i.e., G is complete). In addition, each vertex Vi has a self-loop Ei→i. • The weight of edge Ej→i, denoted as Wj→i, is the WCLL of class Yi conditioned on X and Yj :\nWj→i = ∑ n∈Dh ω(n) logP (y (n) i |x (n), y (n) j ) (13)\n• The weight of self-loop Ei→i, denoted as Wφ→i, is the WCLL of class Yi conditioned only on X.\nUsing the definition of edge weights (Equation (13)) and by switching the order of the summations in Equation (12), we can rewrite the score of T simply as the sum of its edge weights:\nScore(T ) = d∑ n=1 Wπ(i,T )→i\nNow we have transformed the problem of finding the optimal tree structure into the problem of finding the tree in G that has the maximum sum of edge weights. The solution can be obtained by solving the maximum branching (arborescence) problem (Edmonds, 1967), which finds the maximum weight tree in a weighted directed graph."
    }, {
      "heading" : "5.3.2 Learning Multiple CTBN Structures",
      "text" : "In order to obtain multiple, effective CTBN structures for the ML-ME model, we apply the above described algorithm multiple times with different sets of instance weights. We assign the weights such that we give poorly predicted instances higher weights; and give well-predicted instances lower weights.\nWe start with assigning all instances uniform weights (i.e., all instances are equally important a priori).\nω(n) = 1/N : n = 1, ..., N\nUsing this initial set of weights, we find the initial CTBN structure T1 (and its parameters θT1) and set\nthe current model M to be T1. We then estimate the prediction error margin ω(n) = 1−P (y(n)|x(n),M) for each instance and renormalize such that ∑N n=1 ω\n(n) = 1. We repeat our structure learning algorithm with {ω(n)} and find another CTBN structure T2. After that, we set the current model to be the mixture of T1 and T2 and learn the ML-ME parameters {ΘG,ΘT } according to Algorithm 1.\nWe incrementally add trees to the mixture by repeating this process. To stop the process, we use internal validation approach. Specifically, the data used for learning are split to internal train and test sets. The structure of the trees and parameters are always learned on the internal train set. The quality of the current mixture is evaluated on the internal test set. The mixture growth stops when the log-likelihood on the internal test set for the new mixture is worse than for the previous mixture. The trees included in the previous mixture are then fixed, and the parameters of the mixture are relearned on the full training data."
    }, {
      "heading" : "5.3.3 Complexity",
      "text" : "To learn a single CTBN structure, we need to compute edge weights for the complete graph G, which requires estimating P (Yi|X, Yj) for all d2 pairs of classes. Finding the maximum branching in G can be obtained in O(d2) using (Tarjan, 1977). To learn K CTBN structures for the mixture, we repeat these steps K times. Therefore, the overall complexity is O(Kd2) times the complexity of learning logistic regression."
    }, {
      "heading" : "5.4 PREDICTION",
      "text" : "In order to make a prediction for a new instance x, we want to find the MAP assignment of the class variables (see Equation (1)). In general, this requires to evaluate all possible assignments of values to d class variables, which is exponential in d.\nOne important advantage of the CTBN model is that the MAP inference can be done more efficiently by avoiding blind enumeration of all possible assignments. More specifically, the MAP inference on a CTBN is linear in the number of classes (O(d)) when it is implemented using a variant of the max-sum algorithm on a tree structure (Batal et al., 2013).\nHowever, our ML-ME model consists of multiple CTBNs and the MAP solution may, at the end, require enumeration of exponentially many class assignments. To address this problem, we rely on approximate MAP inference. The two commonly applied MAP approximation approaches in the literature are: convex programming relaxation via dual decomposition (Sontag, 2010), and simulated annealing using a Markov chain (Yuan et al., 2004). In this work, we use the latter approach. Briefly, we search the space of all assignments by defining a Markov chain that is induced by local changes to individual class labels. The annealed version of the exploration procedure (Yuan et al., 2004)\nis then used to speed up the search. We initialize our MAP algorithm using the following heuristic: first, we identify the MAP assignments for each CTBN in the mixture individually, and after that, we pick the best assignment from among these candidates. We have found this (efficient) heuristic to work very well and often results in the true MAP assignment."
    }, {
      "heading" : "6 EXPERIMENTS",
      "text" : ""
    }, {
      "heading" : "6.1 DATA",
      "text" : "We use ten publicly available MLC datasets obtained from different domains including music recognition, semantic image labeling, biology and text classification. Table 2 summarizes the characteristics of the datasets. It shows the number of instances (N), the number of feature variables (m) and the number of class variables (d). In addition, it shows two statistics: 1) label cardinality (LC), which is the average number of labels per instance and 2) distinct label sets (DLS), which is the number of distinct class configurations that appear in the data. Note that for RCV1 datasets we have used the 10 most common labels."
    }, {
      "heading" : "6.2 METHODS",
      "text" : "We compare the performance of our proposed method, which we refer to as ML-ME, with the following MLC methods: • Binary Relevance (BR) (Boutell et al., 2004; Clare\nand King, 2001)\n• Classification with Heterogeneous Features (CHF) (Godbole and Sarawagi, 2004)\n• Multi-Label k-Nearest Neighbor (MLKNN) (Zhang and Zhou, 2007)\n• Instance-Based Learning by Logistic Regression (IBLR) (Cheng and Hüllermeier, 2009)\n• Classifier Chains (CC) (Read et al., 2009) • Ensemble of Classifier Chains (ECC) (Read et al.,\n2009)\n• Probabilistic Classifier Chains (PCC) (Dembczynski et al., 2010)\n• Maximum Margin Output Coding (MMOC) (Zhang and Schneider, 2012)\n• Single CTBN (CTBN) (Batal et al., 2013)\nFor all methods, we use the same parameter settings as suggested in the papers that introduced them: For MLKNN and IBLR, we use Euclidean distance to measure similarity of instances and we set the number of nearest neighbors to 10 (Zhang and Zhou, 2007; Cheng and Hüllermeier, 2009); for CC, we set the order of classes to Y1<Y2, ... <Yd (Read et al., 2009); for ECC, we use 10 CCs in the ensemble Read et al. (2009); and for MMOC, we set λ (the decoding parameter) to 1 (Zhang and Schneider, 2012). Also note that all of these methods except MMOC are considered as metalearners because they can work with several base classifiers. To eliminate additional effects that may bias the results, we use L2-penalized logistic regression for all of these methods and choose their regularization parameters by cross validation. Lastly, for our MLME model, we set the number of mixture components to 5 and we use 150 iterations of simulated annealing for prediction."
    }, {
      "heading" : "6.3 EVALUATION MEASURES",
      "text" : "Evaluating the performance of MLC methods is more challenging than that of traditional single-label classification methods. The most appropriate measure is the exact match accuracy (EMA), which computes the percentage of instances whose predicted output vectors are exactly the same as their true class vectors (i.e., all classes are predicted correctly). This measure is proper for MLC because it evaluates the success of the method in finding the mode of P (X|Y) (see Section 2). However, EMA could be too harsh, especially when the output dimensionality is high. An alternative evaluation measure is the conditional log-likelihood loss (CLL-loss), which computes the negative conditional log-likelihood of the test instances:\nCLL-loss = N∑ n=1 − log ( P (y(n)|x(n)) ) CLL-loss evaluates how much probability mass is given to the true label vectors (the higher the probability, the smaller the loss). Note that CLL-loss is only defined for probabilistic methods.\nMicro F1 and macro F1 have been used for evaluating MLC methods (Tsoumakas et al., 2009). Micro F1 aggregates the number of true positives, false positives and false negatives for all classes and then calculates the overall F1 score. On the other hand, macro F1 computes the F1 score for each class separately and then averages these scores. Note that both measures are not quite appropriate for MLC because they do not account for the correlations between classes (Dembczynski et al., 2010). However, we report them in our results as they have been used in other MLC literature."
    }, {
      "heading" : "6.4 RESULTS",
      "text" : "Tables 3, 4, 5 and 6 show the performance of all methods in terms of EMA, CLL-loss, micro F1 and macro F1, respectively. All results are obtained using tenfold cross valiation. To evaluate the statistical significance in the performance measure differences, we use paired t-test at 0.05 significance level. We use markers ∗/~ to indicate whether ML-ME is statistically superior/inferior to the compared method.\nNote that we report the results of MMOC only on four datasets (emotions, image, scene and yeast) because it did not finish on the rest of the datasets (MMOC did not finish one round of the learning within 24 hours). Also, PCC did not finish on the enron dataset be causes it has to evaluate all 253 possible class assignments, which is clearly infeasible. Lastly, we do not report CLL-loss for for MMOC and ECC because they do not compute the probabilistic score for a giving class assignment.\nIn terms of EMA (Table 3), ML-ME clearly outperforms the other methods on most datasets. For example, ML-ME is significantly better than BR, CHF, MLKNN, CC and ECC on all ten datasets, significantly better than IBLR on nine datasets, significantly better than PCC on six datasets and significantly better than MMOC on three datasets. In addition, MLME shows significant improvement over a single CTBN on six datasets, which demonstrates the ability of the mixture to compensate for the tree structure restric-\ntion of a single CTBN.\nIn terms of CLL-loss (Table 4), ML-ME again shows consistent improvement over the other methods. This is because ML-ME is learned to optimize the loglikelihood of the data and its prediction is based on exact MAP inference. Also note that ML-ME shows a consistent improvement over CTBN because combining multiple CTBNs allows us to account for different relations in the data and, hence, improves the generalization of the model.\nLastly, Table 5 and 6 show that ML-ME is also very competitive in terms of micro and macro F1 scores, although optimizing them was not our main objective.\nIn summary, the experimental results show that our ME-ML method with CTBN experts is able to outperform or match the existing state-of-the-art methods across a broad range of benchmark MLC datasets. We attribute this performance to the ability of CTBN mixtures to simultaneously compensate for the restricted dependences modeled by an individual CTBN, and for its ability to fit better the different regions of the input space with new expert models."
    }, {
      "heading" : "7 CONCLUSION",
      "text" : "In this paper, we have proposed a new probabilistic approach for the multi-label classification problem. Our approach models different input-output relations using\nconditional tree-structured Bayesian networks, while the mixtures-of-experts architecture aims to compensate for the tree-structured restrictions and as a result achieves a more accurate model. We have formulated and developed the algorithms for learning the model from data and for performing multi-label predictions on future data instances. Our experiments on a broad range of datasets showed that our approach outperforms several state-of-the-art methods and produces more reliable probabilistic estimates.\nReferencess\nAlessandro Antonucci, Giorgio Corani, Denis Deratani Mauá, and Sandra Gabaglio. An ensemble of bayesian networks for multilabel classification. In IJCAI, pages 1220–1225, 2013.\nIyad Batal, Charmgil Hong, and Milos Hauskrecht. An efficient probabilistic framework for multi-dimensional classification. In Proceedings of the 22nd ACM International Conference on Information and Knowledge Management, CIKM ’13, pages 2417–2422. ACM, 2013.\nC. Bielza, G. Li, and P. Larrañaga. Multi-dimensional classification with bayesian networks. International Journal of Approximate Reasoning, 52(6):705 – 727, 2011.\nMatthew R. Boutell, Jiebo Luo, Xipeng Shen, and Christopher M. Brown. Learning multi-label scene classification. Pattern Recognition, 37(9):1757 – 1771, 2004.\nKim-Anh L Cao, Emmanuelle Meugnier, and Geoffrey J. McLachlan. Integrative mixture of experts to combine\nclinical factors and gene markers. Bioinformatics, 26(9): 1192–1198, 2010.\nWeiwei Cheng and Eyke Hüllermeier. Combining instancebased learning and logistic regression for multilabel classification. Machine Learning, 76(2-3):211–225, 2009.\nAmanda Clare and Ross D. King. Knowledge discovery in multi-label phenotype data. In In: Lecture Notes in Computer Science, pages 42–53. Springer, 2001.\nKrzysztof Dembczynski, Weiwei Cheng, and Eyke Hüllermeier. Bayes optimal multilabel classification via probabilistic classifier chains. In Proceedings of the 27th International Conference on Machine Learning (ICML10), pages 279–286. Omnipress, 2010.\nA. P. Dempster, N. M. Laird, and D. B. Rubin. Maximum likelihood from incomplete data via the EM algorithm. Journal of the Royal Statistical Society: Series B, 39: 1–38, 1977.\nReza Ebrahimpour, Mohammad R. Moradian, Alireza Esmkhani, and Farzad M. Jafarlou. Recognition of persian handwritten digits using characterization loci and mixture of experts. JDCTA, 3(3):42–46, 2009.\nJack Edmonds. Optimum branchings. Research of the National Bureau of Standards, 71B:233–240, 1967.\nAndrew Estabrooks and Nathalie Japkowicz. A mixture-ofexperts framework for text classification. In Proceedings of the 2001 Workshop on Computational Natural Language Learning - Volume 7, ConLL ’01, pages 9:1–9:8, Stroudsburg, PA, USA, 2001. Association for Computational Linguistics.\nShantanu Godbole and Sunita Sarawagi. Discriminative\nmethods for multi-labeled classification. In PAKDD’04, pages 22–30, 2004.\nIsobel Claire Gormley and Thomas Brendan Murphy. Mixture of Experts Modelling with Social Science Applications, pages 101–121. John Wiley & Sons, Ltd, 2011.\nDaniel Hsu, Sham Kakade, John Langford, and Tong Zhang. Multi-label prediction via compressed sensing. In NIPS, pages 772–780, 2009.\nRobert A. Jacobs and Michael I. Jordan. Learning piecewise control strategies in a modular neural network architecture. IEEE Transactions on Systems, Man, and Cybernetics, 23(2):337–345, 1993.\nRobert A. Jacobs, Michael I. Jordan, Steven J. Nowlan, and Geoffrey E. Hinton. Adaptive mixtures of local experts. Neural Comput., 3(1):79–87, March 1991. ISSN 0899-7667.\nMichael I. Jordan and Lei Xu. Convergence results for the em approach to mixtures of experts architectures. Neural Networks, 8(9):1409–1431, 1995.\nHideto Kazawa, Tomonori Izumitani, Hirotoshi Taira, and Eisaku Maeda. Maximal margin labeling for multi-topic text categorization. In Advances in Neural Information Processing Systems 17, pages 649–656. MIT Press, 2005.\nD. C. Liu and J. Nocedal. On the limited memory bfgs method for large scale optimization. Math. Program., 45(3):503–528, December 1989.\nZhiwu Lu. A regularized minimum cross-entropy algorithm on mixtures of experts for time series prediction and curve detection. Pattern Recogn. Lett., 27(9):947–955, July 2006. ISSN 0167-8655.\nS. I. Mossavat, O. Amft, B. De Vries, Petko Petkov, and W. Bastiaan Kleijn. A bayesian hierarchical mixture of experts approach to estimate speech quality. In 2010 2nd International Workshop on Quality of Multimedia Experience, volume QoMEX 2010 - Proceedings, pages 200–205, 2010.\nGuo-Jun Qi, Xian-Sheng Hua, Yong Rui, Jinhui Tang, Tao Mei, and Hong-Jiang Zhang. Correlative multilabel video annotation. In Proceedings of the 15th international conference on Multimedia, MULTIMEDIA ’07, pages 17–26. ACM, 2007a.\nY. Qi, J. Klein-Seetharaman, and Z. Bar-Joseph. A mixture of feature experts approach for protein-protein interaction prediction. BMC bioinformatics, 8(Suppl 10): S6, 2007b. ISSN 1471-2105.\nJesse Read, Bernhard Pfahringer, Geoff Holmes, and Eibe Frank. Classifier chains for multi-label classification. In Proceedings of the European Conference on Machine Learning and Knowledge Discovery in Databases: Part II, ECML PKDD ’09, pages 254–269. Springer-Verlag, 2009.\nDavid Sontag. Approximate Inference in Graphical Models using LP Relaxations. PhD thesis, Massachusetts Institute of Technology, Department of Electrical Engineering and Computer Science, 2010.\nFarbound Tai and Hsuan-Tien Lin. Multi-label classification with principle label space transformation. In Proceedings of the 2nd International Workshop on MultiLabel Learning, 2010.\nRobert Endre Tarjan. Finding optimum branchings. Networks, 7(1):25–35, 1977.\nGrigorios Tsoumakas, Min-Ling Zhang, and Zhi-Hua Zhou. Learning from multi-label data. ECML PKDD Tutorial, 2009.\nLinda C. van der Gaag and Peter R. de Waal. Multidimensional bayesian network classifiers. In Probabilistic Graphical Models, pages 107–114, 2006.\nSteven Richard Waterhouse. Classification and Regression Using Mixtures of Experts. PhD thesis, University of Cambridge, Department of Engineering, 1997.\nA. S. Weigend, M. Mangeas, and A. N. Srivastava. Nonlinear gated experts for time series: Discovering regimes and avoiding overfitting. International Journal of Neural Systems, 6:373–399, 1995.\nAndreas S. Weigend and Shanming Shi. Predicting daily probability distributions of S&P500 returns. Journal of Forecasting, 19(4), July 2000.\nChanghe Yuan, Tsai-Ching Lu, and Marek J. Druzdzel. Annealed map. In UAI, pages 628–635. AUAI Press, 2004.\nSeniha Esen Yuksel, Joseph N. Wilson, and Paul D. Gader. Twenty years of mixture of experts. IEEE Trans. Neural Netw. Learning Syst., 23(8):1177–1193, 2012.\nMin-Ling Zhang and Kun Zhang. Multi-label learning by exploiting label dependency. In Proceedings of the 16th ACM SIGKDD international conference on Knowledge discovery and data mining, KDD ’10, pages 999–1008. ACM, 2010.\nMin-Ling Zhang and Zhi-Hua Zhou. Multilabel neural networks with applications to functional genomics and text categorization. IEEE Transactions on Knowledge and Data Engineering, 18(10):1338–1351, 2006.\nMin-Ling Zhang and Zhi-Hua Zhou. Ml-knn: A lazy learning approach to multi-label learning. Pattern Recogn., 40(7):2038–2048, July 2007.\nYi Zhang and Jeff Schneider. Multi-label output codes using canonical correlation analysis. In AISTATS 2011, 2011.\nYi Zhang and Jeff Schneider. Maximum margin output coding. In Proceedings of the 29th International Conference on Machine Learning (ICML-12), ICML ’12, pages 1575–1582. Omnipress, 2012."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2014,
    "abstractText" : "We develop a novel probabilistic approach for multi-label classification that is based on the mixtures-of-experts architecture combined with recently introduced conditional tree-structured Bayesian networks. Our approach captures different input-output relations from multi-label data using the efficient tree-structured classifiers, while the mixtures-of-experts architecture aims to compensate for the tree-structured restrictions and build a more accurate model. We develop and present algorithms for learning the model from data and for performing multi-label predictions on future data instances. Experiments on multiple benchmark datasets demonstrate that our approach achieves highly competitive results and outperforms the existing state-of-the-art multi-label classification methods.",
    "creator" : "TeX"
  }
}