{
  "name" : "1611.00898.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Low Rank Approximation with Entrywise `1-Norm Error",
    "authors" : [ "Zhao Song", "David P. Woodruff", "Peilin Zhong" ],
    "emails" : [ "zhaos@utexas.edu", "dpwoodru@us.ibm.com", "peilin.zhong@columbia.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "‖A− Â‖1 ≤ α · min rank-k matrices A′ ‖A−A′‖1,\nwhere for an n× d matrix C, we let ‖C‖1 = ∑n\ni=1 ∑d j=1 |Ci,j |. This error measure is known to\nbe more robust than the Frobenius norm in the presence of outliers and is indicated in models where Gaussian assumptions on the noise may not apply. The problem was shown to be NP-hard by Gillis and Vavasis and a number of heuristics have been proposed. It was asked in multiple places if there are any approximation algorithms.\nWe give the first provable approximation algorithms for `1-low rank approximation, showing that it is possible to achieve approximation factor α = (log d)·poly(k) in nnz(A)+(n+d) poly(k) time, where nnz(A) denotes the number of non-zero entries of A. If k is constant, we further improve the approximation ratio toO(1) with a poly(nd)-time algorithm. Under the Exponential Time Hypothesis, we show there is no poly(nd)-time algorithm achieving a (1 + 1\nlog1+γ(nd) )-\napproximation, for γ > 0 an arbitrarily small constant, even when k = 1. We give a number of additional results for `1-low rank approximation: nearly tight upper and lower bounds for column subset selection, CUR decompositions, extensions to low rank approximation with respect to `p-norms for 1 ≤ p < 2 and earthmover distance, low-communication distributed protocols and low-memory streaming algorithms, algorithms with limited randomness, and bicriteria algorithms. We also give a preliminary empirical evaluation.\n∗Work done while visiting IBM Almaden.\nar X\niv :1\n61 1.\n00 89\n8v 1\n[ cs\n.D S]\n3 N\nov 2\n01 6"
    }, {
      "heading" : "1 Introduction",
      "text" : "Two well-studied problems in numerical linear algebra are regression and low rank approximation. In regression, one is given an n× d matrix A, and an n× 1 vector b, and one seeks an x ∈ Rd which minimizes ‖Ax − b‖ under some norm. For example, for least squares regression one minimizes ‖Ax − b‖2. In low rank approximation, one is given an n × d matrix A, and one seeks a rank-k matrix Â which minimizes ‖A − Â‖ under some norm. For example, in Frobenius norm low rank\napproximation, one minimizes ‖A − Â‖F = (∑ i,j(Ai,j − Âi,j)2 )1/2\n. Algorithms for regression are often used as subroutines for low rank approximation. Indeed, one of the main insights of [DMM06b, DMM06a, Sar06, DMM08, CW09] was to use results for generalized least squares regression for Frobenius norm low rank approximation. Algorithms for `1-regression, in which one minimizes ‖Ax − b‖1 = ∑ i |(Ax)i − bi|, were also used [BD13, SW11] to fit a set of points to a hyperplane, which is a special case of entrywise `1-low rank approximation, the more general problem being to find a rank-k matrix Â minimizing ∑ i,j |Ai,j − Âi,j |.\nRandomization and approximation were introduced to significantly speed up algorithms for these problems, resulting in algorithms achieving relative error approximation with high probability. Such algorithms are based on sketching and sampling techniques; we refer to [Woo14b] for a survey. For least squares regression, a sequence of work [Sar06, CW13, MM13, NN13, LMP13, BDN15, Coh16] shows how to achieve algorithms running in nnz(A) + poly(d) time. For Frobenius norm low rank approximation, using the advances for regression this resulted in nnz(A) + (n + d) poly(k) time algorithms. For `1-regression, sketching and sampling-based methods [Cla05, SW11, CDMI+13, CW13, MM13, LMP13, WZ13, CW15b, CP15] led to an nnz(A) + poly(d) time algorithm.\nJust like Frobenius norm low rank approximation is the analogue of least squares regression, entrywise `1-low rank approximation is the analogue of `1-regression. Despite this analogy, no non-trivial upper bounds with provable guarantees are known for `1-low rank approximation. Unlike Frobenius norm low rank approximation, which can be solved exactly using the singular value decomposition, no such algorithm or closed-form solution is known for `1-low rank approximation. Moreover, the problem was recently shown to be NP-hard [GV15]. A major open question is whether there exist approximation algorithms, sketching-based or otherwise, for `1-low rank approximation. Indeed, the question of obtaining betters algorithms was posed in section 6 of [GV15], in [Exc13], and as the second part of open question 2 in [Woo14b], among other places. The earlier question of NP-hardness was posed in Section 1.4 of [KV09], for which the question of obtaining approximation algorithms is a natural followup. The goal of our work is to answer this question.\nWe now formally define the `1-low rank approximation problem: we are given an n× d matrix A and approximation factor α ≥ 1, and we would like, with large constant probability, to output a rank-k matrix Â for which\n‖A− Â‖1 ≤ α · min rank-k matrices A′\n‖A−A′‖1, (1)\nwhere for an n × d matrix C, we let ‖C‖1 = ∑n\ni=1 ∑d j=1 |Ci,j |. This notion of low rank approx-\nimation has been proposed as a more robust alternative to Frobenius norm low rank approximation [KK03, KK05, KLC+15, Kwa08, ZLS+12, BJ12, BD13, BDB13, MXZZ13, MKP13, MKP14, MKCP16, PK16], and is sometimes referred to as `1-matrix factorization or robust PCA. `1-low rank approximation gives improved results over Frobenius norm low rank approximation since outliers are less exaggerated, as one does not square their contribution in the objective. The outlier values are often erroneous values that are far away from the nominal data, appear only a few times in the data matrix, and would not appear again under normal system operation. These works also argue `1-low rank approximation can better handle missing data, is appropriate in noise models for which\nthe noise is not Gaussian, e.g., it produces the maximum likelihood estimator for Laplacian noise [Gao08, KAC+08, VT01], and can be used in image processing to prevent image occlusion [YZD12].\nTo see that `1-low rank approximation and Frobenius norm low rank approximation can give very different results, consider the n × n matrix A = [ n 0 0 B ] , where B is any (n − 1) × (n − 1) matrix with ‖B‖F < n. The best rank-1 approximation with Frobenius norm error is given by Â = n · e1e>1 , where e1 is the first standard unit vector. Here Â ignores all but the first row and column of A, which may be undesirable in the case that this row and column represent an outlier. Note ‖A − Â‖1 = ‖B‖1. If, for example, B is the all 1s matrix, then Â = [0, 0; 0, B] is a rank-1 approximation for which ‖A− Â‖1 = n, and therefore this solution is a much better solution to the `1-low rank approximation problem than n · e1e>1 , for which ‖A− n · e1e>1 ‖1 = (n− 1)2.\nDespite the advantages of `1-low rank approximation, its main disadvantage is its computationally intractability. It is not rotationally invariant and most tools for Frobenius low rank approximation do not apply. To the best of our knowledge, all previous works only provide heuristics. We provide hard instances for previous work in Section L, showing these algorithms at best give a poly(nd)-approximation (though even this is not shown in these works). We also mention why a related objective function, robust PCA [WGR+09, CLMW11, NNS+14, NYH14, CHD16, ZZL15], does not give a provable approximation factor for `1-low rank approximation. Using that for an n×d matrix C, ‖C‖F ≤ ‖C‖1 ≤ √ nd‖C‖F , a Frobenius norm low rank approximation gives a √ nd approximation for `1-low rank approximation. A bit better is to use algorithms for low rank approximation with respect to the sum of distances, i.e., to find a rank-k matrix Â minimizing ‖A− Â‖1,2, where for an n × d matrix C, ‖C‖1,2 = ∑n i=1 ‖Ci‖2, where Ci is the i-th row of C. A sequence of work [DV07, FMSW10, FL11, SV12, CW15a] shows how to obtain an O(1)-approximation to this problem in nnz(A) + (n + d) poly(k) + exp(k) time, and using that ‖C‖1,2 ≤ ‖C‖1 ≤ √ d‖C‖1,2\nresults in an O( √ d)-approximation.\nThere are also many variants of Frobenius norm low rank approximation for which nothing is known for `1-low rank approximation, such as column subset selection and CUR decompositions, distributed and streaming algorithms, algorithms with limited randomness, and bicriteria algorithms. Other interesting questions include low rank approximation for related norms, such as `p-low rank approximation in which one seeks a rank-k matrix Â minimizing ∑n i=1 ∑d j=1(Ai,j − Âi,j)p. Note for 1 ≤ p < 2 these are also more robust than the SVD."
    }, {
      "heading" : "1.1 Our Results",
      "text" : "We give the first efficient algorithms for `1-low rank approximation with provable approximation guarantees. By symmetry of the problem, we can assume d ≤ n. We first give an algorithm which runs in O(nnz(A)) + n · poly(k) time and solves the `1-low rank approximation problem with approximation factor (log d) · poly(k). This is an exponential improvement over the previous approximation factor of O( √ d), provided k is not too large, and is polynomial time for every k. Moreover, provided nnz(A) ≥ n ·poly(k), our time is optimal up to a constant factor as any relative error algorithm must spend nnz(A) time. We also give a hard instance for our algorithm ruling out log d k log k + k\n1/2−γ approximation for arbitrarily small constant γ > 0, and hard instances for a general class of algorithms based on linear sketches, ruling out k1/2−γ approximation.\nVia a different algorithm, we show how to achieve an Õ(k)-approximation factor in poly(n)dÕ(k)2Õ(k\n2) time. This is useful for constant k, for which it gives an O(1)-approximation in poly(n) time, improving the O(log d)-approximation for constant k of our earlier algorithm. The approximation ratio of this algorithm, although O(1) for constant k, depends on k. We also show one can find a rank-2k matrix Â in poly(n) time for constant k for which ‖A − Â‖1 ≤\nC minrank-k matrices A′ ‖A − A′‖1, where C > 1 is an absolute constant independent of k. We refer to this as a bicriteria algorithm. Finally, one can output a rank-k matrix Â, instead of a rank2k matrix Â, in poly(n) time with the same absolute constant C approximation factor, under an additional assumption that the entries of Â are integers in the range {−b,−b+1, . . . , b} for an integer b ≤ poly(n). Unlike our previous algorithms, this last algorithm has a bit complexity assumption.\nUnder the Exponential Time Hypothesis (ETH), we show there is no poly(n)-time algorithm achieving a (1 + 1\nlog1+γ(n) )-approximation, for γ > 0 an arbitrarily small constant, even when k = 1.\nThe latter strengthens the NP-hardness result of [GV15]. We also give a number of results for variants of `1-low rank approximation which are studied for Frobenius norm low rank approxiation; prior to our work nothing was known about these problems. Column Subset Selection and CUR Decomposition: In the column subset selection problem, one seeks a small subset C of columns of A for which there is a matrix X for which ‖CX − A‖ is small, under some norm. The matrix CX provides a low rank approximation to A which is often more interpretable, since it stores actual columns of A, preserves sparsity, etc. These have been extensively studied when the norm is the Frobenius or operator norm (see, e.g., [BMD09, DR10, BDM11] and the references therein). We initiate the study of this problem with respect to the `1-norm. We first prove an existence result, namely, that there exist matrices A for which any subset C of poly(k) columns satisfies minX ‖CX−A‖1 ≥ k1/2−γ ·minrank-k matrices A′ ‖A−A′‖1, where γ > 0 is an arbitrarily small constant. This result is in stark contrast to the Frobenius norm for which for every matrix there exist O(k ) columns for which the approximation factor is 1 + . We also show that our bound is nearly optimal in this regime, by showing for every matrix there exists a subset of O(k log k) columns providing an O( √ k log k)-approximation. One can find such columns in poly(n)dO(k log k) time by enumerating and evaluating the cost of each subset. Although this is exponential in k, we show it is possible to find O(k log k) columns providing an O( √ k log k log d)-approximation in polynomial time for every k. We extend these results to the CUR decomposition problem (see, e.g., [DMM08, BW14]), in which one seeks a factorization CUR for which C is a subset of columns of A, R is a subset of rows of A, and ‖CUR−A‖ is as small as possible. In the case of Frobenius norm, one can choose O(k/ ) columns and rows, have rank(U) = k, have ‖CUR−A‖F be at most (1 + ) times the optimal cost, and find the factorization in nnz(A) log n + n · poly(k/ ) time [BW14]. Using our column subset selection results, we give an nnz(A) + n · poly(k) time algorithm choosing O(k log k) columns and rows, for which rank(U) = k, and for which ‖CUR − A‖1 is poly(k) log d times the cost of any rank-k approximation to A.\n`p-Low Rank Approximation and EMD-Low Rank Approximation: We also give the first algorithms with provable approximation guarantees for the `p-low rank approximation problem, 1 ≤ p < 2, in which we are given an n × d matrix A and approximation factor α ≥ 1, and would like, with large constant probability, to output a rank-k matrix Â for which\n‖A− Â‖pp ≤ α · minrank-k matrices A′ ‖A−A ′‖pp, (2)\nwhere for an n × d matrix C, ‖C‖pp = ∑n\ni=1 ∑d j=1 |Ci,j |p. We obtain similar algorithms for this\nproblem as for `1-low rank approximation. For instance, we obtain an nnz(A) + n · poly(k) time algorithm with approximation ratio (log d) · poly(k). We also provide the first low rank approximation with respect to sum of earthmover distances (of the n rows of A and Â) with a (log2 d) poly(k) approximation factor. This low rank error measure was used, e.g., in [SL09]. Sometimes such applications also require a non-negative factorization, which we do not provide.\nDistributed/Streaming Algorithms, and Algorithms with Limited Randomness: There is a growing body of work on low rank approximation in the distributed (see, e.g., [TD99, QOSG02,\nBCL05, BRB08, MBZ10, FEGK13, PMvdG+13, KVW14, BKLW14, BLS+16, BWZ16, WZ16]) and streaming models (see, e.g., [McG06, CW09, KL11, GP13, Lib13, KLM+14, Woo14a]), though almost exclusively for the Frobenius norm. One distributed model is the arbitrary partition model [KVW14] in which there are s servers, each holding an n×dmatrix Ai, and they would like to output a k× d matrix V > for which minU ‖UV >−A‖ is as small as possible (or, a centralized coordinator may want to output this). We give Õ(snk)-communication algorithms achieving a poly(k, log(n))approximation for `1-low rank approximation in the arbitrary partition model, which is optimal for this approximation factor (see [BW14] where lower bounds for Frobenius norm approximation with poly(n) multiplicative approximation were shown - such lower bounds apply to `1 low rank approximation). We also consider the turnstile streaming model [Mut05] in which we receive positive or negative updates to its entries and wish to output a rank-k factorization at the end of the stream. We give an algorithm using Õ(nk)+poly(k) space to achieve a poly(k, log(n))-approximation, which is space-optimal for this approximation factor, up to the degree of the poly(k) factor. To obtain these results, we show our algorithms can be implemented using Õ(dk) random bits.\nWe stress for all of our results, we do not make assumptions on A such as low coherence or condition number; our results hold for any n× d input matrix A.\nWe report a promising preliminary empirical evaluation of our algorithms in Section L.\nRemark 1.1. We were just informed of the concurrent and independent work [CGK+16], which also obtains approximation algorithms for `1-low rank approximation. That paper obtains a 2O(k) log dapproximation in (log d)k poly(nd) time. Their algorithm is not polynomial time once k = Ω̃(log d), whereas we obtain a polynomial time algorithm for every k (in fact nnz(A) + (n+ d) poly(k) time). Our approximation factor is also poly(k) log d, which is an exponential improvement over theirs in terms of k. In [CGK+16] they also obtain a 2k-approximation in poly(nd)dO(k) time. In contrast, we obtain an Õ(k)-approximation in poly(nd)dÕ(k)2Õ(k2) time. The dependence in [CGK+16] on k in the approximation ratio is exponential, whereas ours is polynomial."
    }, {
      "heading" : "1.2 Technical Overview",
      "text" : "Initial Algorithm and Optimizations: Let A∗ be a rank-k matrix for which ‖A − A∗‖1 = minrank-k matrices A′ ‖A − A′‖1. Let A∗ = U∗V ∗ be a factorization for which U∗ is n × k and V ∗ is k × d. Suppose we somehow knew U∗ and consider the multi-response `1-regression problem minV ‖U∗V − A‖1 = minV ∑d i=1 ‖U∗Vi − Ai‖1, where Vi, Ai denote the i-th columns of V and A, respectively. We could solve this with linear programming though this is not helpful for our argument here.\nInstead, inspired by recent advances in sketching for linear algebra (see, e.g., [Woo14b] for a survey), we could choose a random matrix S and solve minV ‖SU∗V −SA‖1 = minV ∑d i=1 ‖(SU∗)Vi− SAi‖1. If V is an approximate minimizer of the latter problem, we could hope V is an approximate minimizer of the former problem. If also S has a small number t of rows, then we could instead solve minV ∑d i=1 ‖(SU∗)Vi − SAi‖2, that is, minimize the sum of Euclidean norms rather than the sum of `1-norms. Since t−1/2‖(SU∗)Vi−SAi‖1 ≤ ‖(SU∗)Vi−SAi‖2 ≤ ‖(SU∗)Vi−SAi‖1, we would obtain a √ t-approximation to the problem minV ‖SU∗V − SA‖1. A crucial observation is that the\nsolution to minV ∑d\ni=1 ‖(SU∗)Vi−SAi‖2 is given by V = (SU∗)†SA, which implies that V is in the row span of SA. If also S were oblivious to U∗, then we could compute SA without ever knowing U∗. Having a low-dimensional space containing a good solution in its span is our starting point.\nFor this to work, we need a distribution on oblivious matrices S with a small number of rows, for which an approximate minimizer V to minV ‖SU∗V − SA‖1 is also an approximate minimizer to minV ‖U∗V − A‖1. It is unknown if there exists a distribution on S with this property. What\nis known is that if S has O(d log d) rows, then the Lewis weights (see, e.g., [CP15] and references therein) of the concatenated matrix [U∗, A] give a distribution for which the optimal V for the latter problem is a (1+ )-approximation to the former problem; see also earlier work on `1-leverage scores [Cla05, DDH+09] which have poly(d) rows and the same (1 + )-approximation guarantee. Such distributions are not helpful here as (1) they are not oblivious, and (2) the number O(d log d) of rows gives an O( √ d log d) approximation factor, which is much larger than what we want.\nThere are a few oblivious distributions S which are useful for single-response `1-regression min ‖U∗v − a‖1 for column vectors v, a ∈ Rk [SW11, CDMI+13, WZ13]. In particular, if S is an O(k log k)×n matrix of i.i.d. Cauchy random variables, then the solution v to min ‖SU∗v−Sa‖1 is an O(k log k)-approximation to min ‖U∗v−a‖1 [SW11]. The important property of Cauchy random variables is that if X and Y are independent Cauchy random variables, then αX+βY is distributed as a Cauchy random variable times |α|+ |β|, for any scalars α, β ∈ R. The O(k log k) approximation arises because all possible regression solutions are in the column span of [U∗, a] which is (k + 1)- dimensional, and the sketch S gives an approximation factor of O(k log k) to preserve every vector norm in this subspace. If we instead had a multi-response regression problem min ‖SU∗V ∗ − SA‖1 the dimension of the column span of [U∗, A] would be d + k, and this approach would give an O(d log d)-approximation. Unlike Frobenius norm multi-response regression min ‖SU∗V ∗ − SA‖F , which can be bounded if S is a subspace embedding for U∗ and satisfies an approximate matrix product theorem [Sar06], there is no convenient linear-algebraic analogue for the `1-norm.\nWe first note that since regression is a minimization problem, to obtain an O(α)-approximation by solving the sketched version of the problem, it suffices that (1) for the optimal V ∗, we have ‖SU∗V ∗−SA‖1 ≤ O(α)‖U∗V ∗−A‖1, and (2) for all V , we have ‖SU∗V −SA‖1 ≥ Ω(1)·‖U∗V −A‖1.\nWe show (1) holds for α = O(log d) and any number of rows of S. Our analysis follows by truncating the Cauchy random variables (SU∗V ∗j − SAj)i for i ∈ [O(k log k)] and j ∈ [d], so that their expectation exists, and applying linearity of expectation across the d columns. This is inspired from an argument of Indyk [Ind06] for embedding a vector into a lower-dimensional vector while preserving its `1-norm; for single-response regression this is the statement that ‖SU∗v∗ − Sa‖1 = Θ(1)‖U∗v − a‖1, implied by [Ind06]. However, for multi-response regression we have to work entirely with expectations, rather than the tail bounds in [Ind06], since the Cauchy random variables (SU∗Vj − SAj)i, while independent across i, are dependent across j. Moreover, our O(log d)-approximation factor is not an artifact of our analysis - we show in Section G that there is an n × d input matrix A for which with probability 1 − 1/ poly(k), there is no k-dimensional space in the span of SA achieving a ( log d t log t + k 1/2−γ ) -approximation, for S a Cauchy matrix with t rows, where γ > 0 is an arbitrarily small constant. This shows (k log d)Ω(1)-inapproximability. Thus, the fact that we achieve O(log d)-approximation instead of O(1) is fundamental for a matrix S of Cauchy random variables or any scaling of it.\nWhile we cannot show (2), we instead show for all V , ‖SU∗V − SA‖1 ≥ ‖U∗V − A‖1/2 − O(log d)‖U∗V ∗−A‖1 if S has O(k log k) rows. This suffices for regression, since the only matrices V for which the cost is much smaller in the sketch space are those providing an O(log d) approximation in the original space. The guarantee follows from the triangle inequality: ‖SU∗V −SA‖1 ≥ ‖SU∗V − SU∗V ∗‖1−‖SU∗V ∗−SA‖1 and the fact that S is known to not contract any vector in the column span of U∗ if S has O(k log k) rows [SW11]. Because of this, we have ‖SU∗V − SU∗V ∗‖1 = Ω(1)‖U∗V −U∗V ∗‖1 = Ω(1)(‖U∗V −A‖1−‖U∗V ∗−A‖1), where we again use the triangle inequality. We also bound the additive term ‖SU∗V ∗ − SA‖1 by O(log d)‖U∗V ∗ −A‖1 using (1) above.\nGiven that SA contains a good rank-k approximation in its row span, our algorithm with a slightly worse poly(n) time and poly(k log(n))-approximation can be completely described here. Let S and T1 be independent O(k log k)× n matrices of i.i.d. Cauchy random variables, and let R\nand T2 be independent d×O(k log k) matrices of i.i.d. Cauchy random variables. Let\nX = (T1AR) †((T1AR)(T1AR) †(T1AT2)(SAT2)(SAT2) †)k(SAT2) †,\nwhich is the rank-k matrix minimizing ‖T1ARXSAT2 − T1AT2‖F , where for a matrix C, Ck is its best rank-k approximation in Frobenius norm. Output Â = ARXSA as the solution to `1-low rank approximation of A. We show with constant probability that Â is a poly(k log(n))-approximation.\nTo improve the approximation factor, after computing SA, we `1-project each of the rows of A onto SA using linear programming or fast algorithms for `1-regression [CW13, MM13], obtaining an n× d matrix B of rank O(k log k). We then apply the algorithm in the previous paragraph with A replaced by B. This ultimately leads to a log d · poly(k)-approximation.\nTo improve the running time from poly(n) to nnz(A) + n · poly(k), we show a similar analysis holds for the sparse Cauchy matrices of [MM13]; see also the matrices in [WZ13].\nCUR Decompositions: To obtain a CUR decomposition, we first find a log d·poly(k)-approximate rank-k approximation Â as above. Let B1 be an n× k matrix whose columns span those of Â, and consider the regression minV ‖B1V − A‖1. Unlike the problem minV ‖U∗V − A‖1 where U∗ was unknown, we know B1 so can compute its Lewis weights efficiently, sample by them, and obtain a regression problem minV ‖D1(B1V −A)‖1 where D1 is a sampling and rescaling matrix. Since\n‖D1(B1V −A)‖1 ≤ ‖D1(B1V −B1V ∗)‖1 + ‖D1(B1V ∗ −A)‖1,\nwhere V ∗ = argminV ‖B1V − A‖1, we can bound the first term by O(‖B1V − B1V ∗‖1) using that D1 is a subspace embedding if it has O(k log k) rows, while the second term is O(1)‖B1V ∗ − A‖1 by a Markov bound. Note that ‖B1V ∗ − A‖1 ≤ (log d) · poly(k) minrank-k matrices A′ ‖A − A′‖1. By switching to `2 as before, we see that V̂ = (D1B1)†D1A contains a (log d) poly(k)-approximation in its span. Here D1A is an actual subset of rows of A, as required in a CUR decomposition. Moreover the subset size is O(k log k). We can sample by the Lewis weights of V̂ to obtain a subset C of O(k log k) rescaled columns of A, together with a rank-k matrix U for which ‖CUR−A‖1 ≤ (log d) poly(k) minrank-k matrices A′ ‖A−A′‖1.\nAlgorithm for Small k: Our CUR decomposition shows how we might obtain anO(1)-approximation for constant k in poly(n) time. If we knew the Lewis weights of U∗, an α-approximate solution to the problem minV ‖D1(U∗V − A)‖1 would be an O(α)-approximate solution to the problem minV ‖U∗V − A‖1, where D1 is a sampling and rescaling matrix of O(k log k) rows of A. Moreover, an O( √ k log k)-approximate solution to minV ‖D1(U∗V −A)‖1 is given by V = (D1U∗)†D1A, which implies the O(k log k) rows of D1A contain an O( √ k log k)-approximation. For small k, we can guess every subset of O(k log k) rows of A in nO(k log k) time (if d n, by taking transposes at the beginning one can replace this with dO(k log k) time). For each guess, we set up the problem minrank-k U ‖U(D1A)−A‖1. If D2 is a sampling and rescaling matrix according to the Lewis weights of D1A, then by a similar triangle inequality argument as for our CUR decomposition, minimizing ‖U(D1A)D2−AD2‖1 gives an O( √ k log k) approximation. By switching to `2, this implies there is an O(k log k)-approximation of the form AD2WD1A, whereW is an O(k log2 k)×O(k log k) matrix of rank k. By setting up the problem minrank-k W ‖AD2WD1A− A‖1, one can sample from Lewis weights on the left and right to reduce this to a problem independent of n and d, after which one can use polynomial optimization to solve it in exp(poly(k)) time. One of our guesses D1A will be correct, and for this guess we obtain an Õ(k)-approximation. For each guess we can compute its cost and take the best one found. This gives an O(1)-approximation for constant k, removing the O(log d)-factor from the approximation of our earlier algorithm.\nExistential Results for Subset Selection: In our algorithm for small k, the first step was to show there exist O(k log k) rows of A which contain a rank-k space which is an O( √ k log k)approximation. While for Frobenius norm one can find O(k) rows with an O(1)-approximation in their span, one of our main negative results for `1-low rank approximation is that this is impossible, showing that the best approximation one can obtain with poly(k) rows is k1/2−γ for an arbitrarily small constant γ > 0. Our hard instance is an r× (r+ k) matrix A in which the first k columns are i.i.d. Gaussian, and the remaining r columns are an identity matrix. Here, r can be twice the number of rows one is choosing. The optimal `1-low rank approximation has cost at most r, obtained by choosing the first k columns.\nLet R ∈ Rr/2×k denote the first k entries of the r/2 chosen rows, and let y denote the first k entries of an unchosen row. For r/2 > k, there exist many solutions x ∈ Rr/2 for which x>R = y. However, we can show the following tradeoff:\nwhenever ‖x>R− y‖1 < √ k poly(log k) , then ‖x‖1 > √ k poly(log k) .\nThen no matter which linear combination x> of the rows of R one chooses to approximate y by, either one incurs a √ k\npoly(log k) cost on the first k coordinates, or since A contains an identity matrix,\none incurs cost ‖x‖1 > √ k poly(log k) on the last r coordinates of x >R.\nTo show the tradeoff, consider an x ∈ Rr/2. We decompose x = x0 + ∑\nj≥1 x j , where xj agrees\nwith x on coordinates which have absolute value in the range 1√ k logc k · [2−j , 2−j+1], and is zero otherwise. Here, c > 0 is a constant, and x0 denotes the restriction of x to all coordinates of absolute value at least 1√\nk logc k . Then ‖x‖1 <\n√ k\nlogc k , as otherwise we are done. Hence, x 0 has\nsmall support. Thus, one can build a small net for all x0 vectors by choosing the support, then placing a net on it. For xj for j > 0, the support sizes are increasing so the net size needed for all xj vectors is larger. However, since xj has all coordinates of roughly the same magnitude on its support, its `2-norm is decreasing in j. Since (xj)>R ∼ N(0, ‖xj‖22Ik), this makes it much less likely that individual coordinates of (xj)>R can be large. Since this probability goes down rapidly, we can afford to union bound over the larger net size. What we show is that for any sum of the form ∑ j≥1 x j , at most k10 of its coordinates are at least 1 log k in magnitude.\nFor ‖x>R − y‖1 to be at most √ k logc k , for at least k 2 coordinates i, we must have |(x\n>R − y)i| < 2√k logc k . With probability 1 − 2 −Ω(k), |yi| ≥ 1100 on at least 2k 3 coordinates. From the previous paragraph, it follows there are at least k2 − k 10 − k 3 = Ω(k) coordinates i of x for which (1)\n|(x>R − y)i| < 2√k logc k , (2) | ∑ j≥1 x j i | < 1 log k , and (3) |yi| ≥ 1 100 . On these i, (x 0)>Ri must be in an interval of width 1log k at distance at least 1 100 from the origin. Since (x 0)>R ∼ N(0, ‖x0‖22Ik), for any value of ‖x0‖22 the probability this happens on Ω(k) coordinates is at most 2−Θ(k). Since the net size for x0 is small, we can union bound over every sequence x0, x1, . . . , coming from our nets.\nSome care is needed to union bound over all possible subsets R of rows which can be chosen. We handle this by conditioning on a few events of A itself, which imply corresponding events for every subset of rows. These events are such that if R is the chosen set of half the rows, and S the remaining set of rows of A, then the event that a constant fraction of rows in S are close to the row span of R is 2−Θ(kr), which is small enough to union bound over all choices of R.\nCuriously, we also show there are some matrices A ∈ Rn×d for which any `1 rank-k approximation in the entire row span of A cannot achieve better than a (2−Θ(1/d))-approximation.\nBicriteria Algorithm: Our algorithm for small k gives an O(1)-approximation in poly(n) time for constant k, but the approximation factor depends on k. We show how one can find a rank2k matrix Â for which ‖A − Â‖1 ≤ C · OPT, where C is an absolute constant, and OPT = minrank-k matrices A′ ‖A−A′‖1. We first find a rank-k matrix B1 for which ‖A−B1‖1 ≤ p ·OPT for a factor 1 ≤ p ≤ poly(n). We can use any of our algorithms above for this.\nNext consider the problem minV ∈Rk×d ‖U∗V − (A−B1)‖1, and let U∗V ∗ be a best `1-low rank approximation to A− B1; we later explain why we look at this problem. We can assume V ∗ is an `1 well-conditioned basis [Cla05, DDH+09], since we can replace U∗ with U∗R−1 and V ∗ with RV ∗ for any invertible linear transformation R. For any vector x we then have ‖x‖1f ≤ ‖x >V ∗‖1 ≤ e‖x‖1, where 1 ≤ e, f ≤ poly(k). This implies all entries of U∗ are at most 2f‖A − B‖1, as otherwise one could replace U∗ with 0n×k and reduce the cost. Also, any entry of U∗ smaller than ‖A−B‖1100enkp can be replaced with 0 as this incurs additive error OPT100 . If we round the entries of U\n∗ to integer multiples of ‖A−B‖1100enkp , then we only have O(enkpf) possibilities for each entry of U\n∗, and still obtain an O(1)-approximation. We refer to the rounded U∗ as U∗, abusing notation.\nLet D be a sampling and rescaling matrix with O(k log k) non-zero diagonal entries, corresponding to sampling by the Lewis weights of U∗. We do not know D, but handle this below. By the triangle inequality, for any V ,\n‖D(U∗V − (A−B1))‖1 = ‖D(U∗V − U∗V ∗)‖1 ± ‖D(U∗V ∗ − (A−B1))‖1 = Θ(1)‖U∗V − U∗V ∗‖1 ±O(1)‖U∗V ∗ − (A−B1)‖1,\nwhere the Lewis weights give ‖D(U∗V −U∗V ∗)‖1 = Θ(1)‖U∗V −U∗V ∗‖1 and a Markov bound gives ‖D(U∗V ∗ − (A − B1))‖1 = O(1)‖U∗V ∗ − (A − B1)‖1. Thus, minimizing ‖DU∗V −D(A − B1)‖1 gives a fixed constant factor approximation to the problem minV ∈Rk×d ‖U∗V − (A − B1)‖1. The non-zero diagonal entries of D can be assumed to be integers between 1 and n2.\nWe guess the entries ofDU∗ and note for each entry there are only O(enkpf log(n2)) possibilities. One of our guesses corresponds to Lewis weight sampling by U∗. We solve for V and by the guarantees of Lewis weights, the row span of this V provides an O(1)-approximation. We can find the corresponding U via linear programming. As mentioned above, we do not know D, but can enumerate over all D and all possible DU∗. The total time is npoly(k).\nAfter finding U , which has k columns, we output the rank-2k space formed by the column span of [U,B1]. By including the column span of B1, we ensure our original transformation of the problem minV ∈Rk×d ‖U∗ ·V −A‖1 to the problem minV ∈Rk×d ‖U∗ ·V − (A−B1)‖1 is valid, since we can first use the column span of B1 to replace A with A − B1. Replacing A with A − B1 ultimately results in a rank-2k output. Had we used A instead of A − B1 our output would have been rank k but would have additive error ‖A‖1poly(k/ ) . If we assume the entries of A are in {−b,−b+ 1, . . . , b}, then we can lower bound the cost ‖U∗V −A‖1, given that it is non-zero, by (ndb)−O(k) (if it is zero then we output A) using Lemma 4.1 in [CW09] and relating entrywise `1-norm to Frobenius norm. We can go through the same arguments above with A−B replaced by A and our running time will now be (ndb)poly(k).\nHard Instances for Cauchy Matrices and More General Sketches: We consider a d × d matrix A = Id+(log d)e>1 e, where e1 = (1, 0, . . . , 0) and e = (1, 1, . . . , 1) and Id is the d×d identity. For an O(k log k)×d matrix S of i.i.d. Cauchy random variables, SA = S+ (log d)S>1 e, where S1 is the first column of S. For a typical column of SA, all entries are at most poly(k) log d in magnitude. Thus, in order to approximate the first row of A, which is (log d)e, by x>SA for an x ∈ Rk log k, we\nneed ‖x‖1 ≥ 1poly(k) . Also ‖x >S‖1 = Ω(‖x‖1d log d) with 1 − exp(−k log k) probability, for d large enough, so by a net argument ‖x‖1 ≤ poly(k) for all x. However, there are entries of SA that are very large, i.e., about one which is r = Θ(dk log k) in magnitude, and in general about 2i entries about r2−i in magnitude. These entries typically occur in columns Cj of SA for which all other entries in the column are bounded by poly(k) in magnitude. Thus, |x>Cj | ≈ r2−i for about 2i columns j. For each such column, if r2−i log d, then we incur cost r2 −i\npoly(k) in approximating the first row of A. In total the cost is r log r poly(k) = d log d poly(k) , but the optimal\ncost is at most d, giving a log dpoly(k) lower bound. We optimize this to a log d k log2 k lower bound.\nWhen k is large this bound deteriorates, but we also show a k1/2−γ lower bound for arbitrarily small constant γ > 0. This bound applies to any oblivious sketching matrix. The idea is similar to our row subset selection lower bound. Let A be as in our row subset selection lower bound, consider SA, and write S = UΣV > in its full SVD. Then SA is in the row span of the top O(k log k) rows of V >A, since Σ only has O(k log k) non-zero singular values. Since the first k columns of A are rotationally invariant, V >A has first k columns i.i.d. Gaussian and remaining columns equal to V >. Call the first O(k log k) rows of V >A the matrix B. We now try to approximate a row of A by a vector in the row span of B. There are two issues that make this setting different from row subset selection: (1) B no longer contains an identity submatrix, and (2) the rows of B depend on the rows of A. We handle the first issue by building nets for subsets of coordinates of x>V > rather than x as before; since ‖x>V >‖2 = ‖x‖2 similar arguments can be applied. We handle the second issue by observing that if the number of rows of B is considerably smaller than that of A, then the distribution of B had we replaced a random row of A with zeros would be statistically close to i.i.d. Gaussian. Hence, typical rows of A can be regarded as being independent of B.\nLimited Independence, Distributed, and Streaming Algorithms: We show for an n × d matrix A, if we left-multiply by an O(k log k)×n matrix S in which each row is an independent vector of Õ(d)-wise independent Cauchy random variables, SA contains a poly(k) log d-approximation in its span. This allows players in a distributed model to share a common S by exchanging Õ(kd) bits, independent of n. We use Lemma 2.2 of [KNW10] which shows for a reasonably smooth approximation f to an indicator function, E[f(X)] = E[f(Y )] + O( ), where X = ∑ i aiXi, Y = ∑ i aiYi, a ∈ Rn is fixed, X is a vector of i.i.d. Cauchy random variables, and Y is a vector of Õ(1/ )-wise independent random variables.\nTo show the row span of SA contains a good rank-k approximation, we argue ‖Sy‖1 = Ω(‖y‖1) for a fixed y ∈ Rn with 1−exp(−k log k) probability. We apply the above lemma with = Θ(1). We also need for an n×d matrix A with unit-`1 columns, that ‖SA‖1 = Õ(kd). We fool the expectation of a truncated Cauchy by taking a weighted sum of O(log(dk)) indicator functions and applying the above lemma with = Θ(1/d). An issue is there are Θ̃(kd) Cauchy random variables corresponding to the entries of SA, some of which can be as large as Θ̃(kd), so to fool their expectation (after truncation) we need = Θ̃(1/(dk)), resulting in Õ(dk2) seed length and ruining our optimal Õ(dk) communication. We show we can instead pay a factor of k in our approximation and maintain Õ(dk)-wise independence. The distributed and streaming algorithms, given this, follow algorithms for Frobenius norm low rank approximation in [KVW14, BWZ16].\nHardness Assuming Exponential Time Hypothesis: By inspecting the proof of NP-hardness of [GV15], it at best gives a (1 + 1nγ )-inapproximability for an arbitrarily small constant γ > 0. We considerably strengthen this to (1 + 1\nlog1+γ n )-inapproximability by taking a modified version of the\nn × n hard instance of [GV15] and planting it in a 2o(n) × 2o(n) matrix padded with tiny values.\nUnder the ETH, the maximum cut problem that [GV15] and that we rely on cannot be solved in 2o(n) time, so our transformation is efficient. Although we use the maximum cut problem as in [GV15] for our n× n hard instance, in order to achieve our inapproximability we need to use that under the ETH this problem is hard to approximate even if the input graph is sparse and even up to a constant factor; such additional conditions were not needed in [GV15].\n`p-Low Rank Approximation and EMD-Low Rank Approximation: Our algorithms for entrywise `p-Norm Error are similar to our algorithms for `1. We use p-stable random variables in place of Cauchy random variables, and note that the p-th power of a p-stable random variable has similar tails to that of a Cauchy, so many of the same arguments apply. Our algorithm for EMD low rank approximation immediately follows by embedding EMD into `1.\nCounterexamples to Heuristics: Let A = diag(n2+γ , n1.5+ , B,B) ∈ R(2n+2)×(2n+2) where ∈ (0, .5), γ > 0, and B is the n × n all 1s matrix. For this A we show the four heuristic algorithms [KK05, DZHZ06, Kwa08, BDB13] cannot achieve an nmin(γ,0.5− ) approximation ratio when the rank parameter k = 3."
    }, {
      "heading" : "1.3 Several Theorem Statements, an Algorithm, and a Roadmap",
      "text" : "Algorithm 1 Main Meta-Algorithm 1: procedure L1LowRankApprox(A,n, d, k) . Theorem 1.2 2: Choose sketching matrix S (a Cauchy matrix or a sparse Cauchy matrix.) 3: Compute SA, form C by Ci ← arg minx ‖xSA−Ai‖1. Form B = C · SA. 4: Choose sketching matrices T1, R,D, T2 (Cauchy matrices or sparse Cauchy matrices.) 5: Solve minX,Y ‖T1BRXYDBT2 − T1BT2‖F . 6: return BRX,Y DB. 7: end procedure\nTheorem 1.2 (Informal Version of Theorem C.6). Given A ∈ Rn×d, there is an algorithm which in nnz(A) + (n + d) · poly(k) time, outputs a (factorization of a) rank-k matrix A′ such that with probability 9/10, ‖A′ −A‖1 ≤ (log d) poly(k) min\nU∈Rn×k,V ∈Rk×d ‖UV −A‖1.\nTheorem 1.3 (Informal Version of Theorem C.7). Given A ∈ Rn×d, there is an algorithm that takes poly(n)dÕ(k)2Õ(k2) time and outputs a rank-k matrix A′ such that, with probability 9/10, ‖A′ −A‖1 ≤ Õ(k) min\nU∈Rn×k,V ∈Rk×d ‖UV −A‖1. In addition, A′ is a CUR decomposition.\nTheorem 1.4 (Informal Version of Theorem G.28). For any k ≥ 1, and any constant c ≥ 1, let n = kc. There exists a matrix A such that for any matrix A′ in the span of n/2 rows of A, ‖A′ −A‖1 = Ω(k0.5−α) min\nU∈Rn×k,V ∈Rk×d ‖UV −A‖1, where α > 0 is an arbitrarily small constant.\nRoad map Section A introduces some notation and definitions. Section B includes several useful tools. We provide several `1-low rank approximation algorithms in Section C. Section D contains the no contraction and no dilation analysis for our main algorithm. The results for `p and earth mover distance are presented in Section E and F. We provide our existential hardness results for Cauchy matrices, row subset selection and oblivious subspace embeddings in Section G. We provide our computational hardness results in Section H. We analyze limited independent random Cauchy variables in Section I. Section K presents the results for the distributed setting. Section J presents the results for the streaming setting. Section L contains the experimental results of our algorithm and several heuristic algorithms, as well as counterexamples to heuristic algorithms."
    }, {
      "heading" : "Contents",
      "text" : ""
    }, {
      "heading" : "1 Introduction 1",
      "text" : "1.1 Our Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2 1.2 Technical Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 1.3 Several Theorem Statements, an Algorithm, and a Roadmap . . . . . . . . . . . . . . 10"
    }, {
      "heading" : "A Notation 22",
      "text" : ""
    }, {
      "heading" : "B Preliminaries 22",
      "text" : "B.1 Polynomial system verifier . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22 B.2 Cauchy and p-stable transform . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23 B.3 Lewis weights . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23 B.4 Frobenious norm and `2 relaxation . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24 B.5 Converting entry-wise `1 and `p objective functions into polynomials . . . . . . . . . 25 B.6 Converting entry-wise `1 objective function into a linear program . . . . . . . . . . . 25\nC `1-Low Rank Approximation 26 C.1 Existence results via dense Cauchy transforms, sparse Cauchy transforms, Lewis weights 26 C.2 Input sparsity time, poly(k, log n, log d)-approximation for an arbitrary matrix A . . 28 C.3 poly(k, log d)-approximation for an arbitrary matrix A . . . . . . . . . . . . . . . . . 31 C.4 Õ(k)-approximation for an arbitrary matrix A . . . . . . . . . . . . . . . . . . . . . . 32 C.5 Rank-2k and O(1)-approximation algorithm for an arbitrary matrix A . . . . . . . . 35 C.6 CUR decomposition for an arbitrary matrix A . . . . . . . . . . . . . . . . . . . . . . 37 C.7 Rank-r matrix B . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39\nC.7.1 Properties . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39 C.7.2 poly(k, r)-approximation for rank-r matrix B . . . . . . . . . . . . . . . . . . 41"
    }, {
      "heading" : "D Contraction and Dilation Bound for `1 44",
      "text" : "D.1 Definitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44 D.2 Properties . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45 D.3 Cauchy embeddings, no dilation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48 D.4 Cauchy embeddings, no contraction . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50 D.5 Cauchy embeddings, k-dimensional subspace . . . . . . . . . . . . . . . . . . . . . . . 51 D.6 Sparse Cauchy transform . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53 D.7 `1-Lewis weights . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55\nE `p-Low Rank Approximation 57 E.1 Definitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57 E.2 Properties . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58 E.3 Tools and inequalities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62 E.4 Dense p-stable transform . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63 E.5 Sparse p-stable transform . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64 E.6 `p-Lewis weights . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66\nF EMD-Low Rank Approximation 66 F.1 Definitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66 F.2 Analysis of no contraction and no dilation bound . . . . . . . . . . . . . . . . . . . . 67"
    }, {
      "heading" : "G Hardness results for Cauchy matrices, row subset selection, OSE 68",
      "text" : "G.1 Hard instance for Cauchy matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . 68 G.2 Hard instance for row subset selection . . . . . . . . . . . . . . . . . . . . . . . . . . 72 G.3 Hard instance for oblivious subspace embedding and more row subset selection . . . 77\nG.3.1 Definitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 77 G.3.2 Main results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 79"
    }, {
      "heading" : "H Hardness 91",
      "text" : "H.1 Previous results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 92 H.2 Extension to multiplicative error `1-low rank approximation . . . . . . . . . . . . . . 92 H.3 Usin the ETH assumption . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 94 H.4 Extension to the rank-k case . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 97"
    }, {
      "heading" : "I Limited independent Cauchy random variables 101",
      "text" : "I.1 Notations and Tools . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 101 I.2 Analysis of limited independent random Cauchy variables . . . . . . . . . . . . . . . 101"
    }, {
      "heading" : "J Streaming Setting 104",
      "text" : "J.1 Definitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 105 J.2 Turnstile model, poly(k, log(d), log(n)) approximation . . . . . . . . . . . . . . . . . 105 J.3 Row-update model, poly(k) log d approximation . . . . . . . . . . . . . . . . . . . . . 106"
    }, {
      "heading" : "K Distributed Setting 108",
      "text" : "K.1 Definitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 110 K.2 Arbitrary-partition model, subspace, poly(k, log(d), log(n)) approximation . . . . . . 110 K.3 Arbitrary-partition model, decomposition, poly(k, log(d), log(n)) approximation . . . 112 K.4 Row-partition model, subspace, poly(k) log d approximation . . . . . . . . . . . . . . 112 K.5 Row-partition model, decomposition, poly(k) log d approximation . . . . . . . . . . . 114"
    }, {
      "heading" : "L Experiments and Discussions 115",
      "text" : "L.1 Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 115 L.2 Counterexample for [DZHZ06] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 115 L.3 Counterexample for [BDB13] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 116 L.4 Counterexample for [Kwa08] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 118 L.5 Counterexample for [KK05] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 119 L.6 Counterexample for all . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 121 L.7 Discussion for Robust PCA [CLMW11] . . . . . . . . . . . . . . . . . . . . . . . . . . 121\nM Acknowledgments 122"
    }, {
      "heading" : "A Notation",
      "text" : "Let N+ denote the set of positive integers. For any n ∈ N+, let [n] denote the set {1, 2, · · · , n}. For any p ∈ [1, 2], the `p-norm of a vector x ∈ Rd is defined as\n‖x‖p = ( d∑ i=1 |xi|p )1/p .\nFor any p ∈ [1, 2), the `p-norm of a matrix A ∈ Rn×d is defined as\n‖A‖p = ( n∑ i=1 d∑ j=1 |Aij |p )1/p .\nLet ‖A‖F denote the Frobenius norm of matrix A. Let nnz(A) denote the number of nonzero entries of A. Let det(A) denote the determinant of a square matrix A. Let A> denote the transpose of A. Let A† denote the Moore-Penrose pseudoinverse of A. Let A−1 denote the inverse of a full rank square matrix. We use Aj to denote the jth column of A, and Ai to denote the ith row of A. For an n× d matrix A, for S a subset of [n] and T a subset of [d], we let AS denote the |S| × d submatrix of A with rows indexed by S, while AT denotes the n × |T | submatrix of A with columns indexed by T , and AST denote the |S| × |T | submatrix A with rows in S and columns in T .\nFor any function f , we define Õ(f) to be f · logO(1)(f). In addition to O(·) notation, for two functions f, g, we use the shorthand f . g (resp. &) to indicate that f ≤ Cg (resp. ≥) for an absolute constant C. We use f h g to mean cf ≤ g ≤ Cf for constants c, C. We use OPT to denote minrank−k Ak ‖Ak −A‖1, unless otherwise specified."
    }, {
      "heading" : "B Preliminaries",
      "text" : ""
    }, {
      "heading" : "B.1 Polynomial system verifier",
      "text" : "Renegar [Ren92a, Ren92b] and Basu et al. [BPR96] independently provided an algorithm for the decision problem for the existential theory of the reals, which is to decide the truth or falsity of a sentence (x1, · · · , xv)F (f1, · · · , fm) where F is a quantifier-free Boolean formula with atoms of the form sign(fi) = σ with σ ∈ {0, 1,−1}. Note that this problem is equivalent to deciding if a given semi-algebraic set is empty or not. Here we formally state that theorem. For a full discussion of algorithms in real algebraic geometry, we refer the reader to [BPR05] and [Bas14].\nTheorem B.1 (Decision Problem [Ren92a, Ren92b, BPR96]). Given a real polynomial system P (x1, x2, · · · , xv) having v variables and m polynomial constraints fi(x1, x2, · · · , xv)∆i0, ∀i ∈ [m], where ∆i is any of the “standard relations”: {>,≥,=, 6=,≤, <}, let d denote the maximum degree of all the polynomial constraints and let H denote the maximum bitsize of the coefficients of all the polynomial constraints. Then in\n(md)O(v) poly(H),\ntime one can determine if there exists a solution to the polynomial system P .\nRecently, this technique has been used to solve a number of low-rank approximation and matrix factorization problems [AGKM12, Moi13, CW15a, BDL16, RSW16].\nB.2 Cauchy and p-stable transform\nDefinition B.2 (Dense Cauchy transform). Let S = σ · C ∈ Rm×n where σ is a scalar, and each entry of C ∈ Rm×n is chosen independently from the standard Cauchy distribution. For any matrix A ∈ Rn×d, SA can be computed in O(m · nnz(A)) time.\nDefinition B.3 (Sparse Cauchy transform). Let Π = σ ·SC ∈ Rm×n, where σ is a scalar, S ∈ Rm×n has each column chosen independently and uniformly from the m standard basis vectors of Rm, and C ∈ Rn×n is a diagonal matrix with diagonals chosen independently from the standard Cauchy distribution. For any matrix A ∈ Rn×d, ΠA can be computed in O(nnz(A)) time.\nDefinition B.4 (Dense p-stable transform). Let p ∈ (1, 2). Let S = σ · C ∈ Rm×n where σ is a scalar, and each entry of C ∈ Rm×n is chosen independently from the standard p-stable distribution. For any matrix A ∈ Rn×d, SA can be computed in O(m nnz(A)) time.\nDefinition B.5 (Sparse p-stable transform). Let p ∈ (1, 2). Let Π = σ · SC ∈ Rm×n, where σ is a scalar, S ∈ Rm×n has each column chosen independently and uniformly from the m standard basis vectors of Rm, and C ∈ Rn×n is a diagonal matrix with diagonals chosen independently from the standard p-stable distribution. For any matrix A ∈ Rn×d, ΠA can be computed in O(nnz(A)) time."
    }, {
      "heading" : "B.3 Lewis weights",
      "text" : "We follow the exposition of Lewis weights from [CP15].\nDefinition B.6. For a matrix A, let ai denote ith row of A, and ai(= (Ai)>) is a column vector. The statistical leverage score of a row ai is\nτi(A) def = a>i (A >A)−1ai = ‖(A>A)−1/2ai‖22.\nFor a matrix A and norm p, the `p Lewis weights w are the unique weights such that for each row i we have\nwi = τi(W 1/2−1/pA).\nor equivalently\na>i (A >W 1−2/pA)−1ai = w 2/p i .\nLemma B.7 (Lemma 2.4 of [CP15] and Lemma 7 of [CLM+15]). Given a matrix A ∈ Rn×d, n ≥ d, for any constant C > 0, 4 > p ≥ 1, there is an algorithm which can compute C-approximate `p Lewis weights for every row i of A in O((nnz(A) + dω log d) log n) time, where ω < 2.373 is the matrix multiplication exponent[Str69, CW87, Wil12].\nLemma B.8 (Theorem 7.1 of [CP15]). Given matrix A ∈ Rn×d (n ≥ d) with `p (4 > p ≥ 1) Lewis weights w, for any set of sampling probabilities pi, ∑ i pi = N ,\npi ≥ f(d, p)wi,\nif S ∈ RN×n has each row chosen independently as the ith standard basis vector, times 1/p1/pi , with probability pi/N , then with probability at least 0.999,\n∀x ∈ Rd, 1 2 ‖Ax‖1 ≤ ‖SAx‖1 ≤ 2‖Ax‖1\nFurthermore, if p = 1, N = O(d log d). If 1 < p < 2, N = O(d log d log log d). If 2 ≤ p < 4, N = O(dp/2 log d)."
    }, {
      "heading" : "B.4 Frobenious norm and `2 relaxation",
      "text" : "Theorem B.9 (Generalized rank-constrained matrix approximations, Theorem 2 in [FT07]). Given matrices A ∈ Rn×d, B ∈ Rn×p, and C ∈ Rq×d, let the SVD of B be B = UBΣBV >B and the SVD of C be C = UCΣCV >C . Then,\nB†(UBU > BAVCC > C )kC † = arg min rank−k X∈Rp×q ‖A−BXC‖F ,\nwhere (UBU>BAVCV > C )k ∈ Rp×q is of rank at most k and denotes the best rank-k approximation to UBU > BAVCV > C ∈ Rp×d in Frobenious norm.\nClaim B.10 (`2 relaxation of `p-regression). Let p ∈ [1, 2). For any A ∈ Rn×d and b ∈ Rn, define x∗ = arg min\nx∈Rd ‖Ax− b‖p and x′ = arg min x∈Rd ‖Ax− b‖2. Then,\n‖Ax∗ − b‖p ≤ ‖Ax′ − b‖p ≤ n1/p−1/2 · ‖Ax∗ − b‖p.\nProof. The lower bound trivially holds by definition; we will focus on proving the upper bound. Because Ax− b is an n-dimensional vector, ∀x,\n1\nn1/p−1/2 ‖Ax− b‖p ≤ ‖Ax− b‖2 ≤ ‖Ax− b‖p. (3)\nThen,\n‖Ax′ − b‖p ≤ √ n‖Ax′ − b‖2 by LHS of Equation (3) ≤ √ n‖Ax∗ − b‖2 by x′ = arg min\nx ‖Ax− b‖2\n≤ √ n‖Ax∗ − b‖p by RHS of Equation (3)\nThis completes the proof.\nClaim B.11 (Frobenius norm relaxation of `p-low rank approximation). Let p ∈ [1, 2) and for any matrix A ∈ Rn×d, define A∗ = arg min\nrank−k B∈Rn×d ‖B −A‖p and A′ = arg min rank−k B∈Rn×d ‖B −A‖F . Then\n‖A∗ −A‖p ≤ ‖A′ −A‖p ≤ (nd)1/p−1/2‖A∗ −A‖p. (4)\nProof. The lower bound of ‖A′ − A‖p trivially holds by definition. We show an upper bound of ‖A′ −A‖p in the rest of the proof. For any A′ −A ∈ Rn×d, we have\n1\n(nd)1/p−1/2 ‖A′ −A‖p ≤ ‖A′ −A‖F ≤ ‖A′ −A‖p. (5)\nThen,\n‖A′ −A‖p ≤ (nd)1/p−1/2‖A′ −A‖F by LHS of Equation (5) ≤ (nd)1/p−1/2‖A∗ −A‖F by A′ = arg min\nrank−k B ‖B −A‖p\n≤ (nd)1/p−1/2‖A∗ −A‖p. by RHS of Equation (5)\nB.5 Converting entry-wise `1 and `p objective functions into polynomials\nClaim B.12 (Converting absolute value constraints into variables). Givenm polynomials f1(x), f2(x), · · · , fm(x) where x ∈ Rv, solving the problem\nmin x∈Rv m∑ i=1 |fi(x)|, (6)\nis equivalent to solving another minimization problem with O(m) extra constraints and m extra variables,\nmin x∈Rv ,σ∈Rm m∑ i=1 σifi(x)\ns.t. σ2i = 1,∀i ∈ [m] fi(x)σi ≥ 0, ∀i ∈ [m].\nClaim B.13. (Handling `p) Given m polynomials f1(x), f2(x), · · · , fm(x) where x ∈ Rv and p = a/b for positive integers a and b, solving the problem\nmin x∈Rv m∑ i=1 |fi(x)|p, (7)\nis equivalent to solving another minimization problem with O(m) extra constraints and O(m) extra variables,\nmin x∈Rv ,σ∈Rm m∑ i=1 yi\ns.t. σ2i = 1,∀i ∈ [m] fi(x)σi ≥ 0, ∀i ∈ [m] (σifi(x))\na = ybi , ∀i ∈ [m] yi ≥ 0, ∀i ∈ [m].\nB.6 Converting entry-wise `1 objective function into a linear program\nClaim B.14. Given any matrix A ∈ Rn×d and matrix B ∈ Rk×d, the problem minU∈Rn×k ‖UB−A‖1 can be solved by solving the following linear program,\nmin U∈Rn×k,x∈Rn×d n∑ i=1 m∑ j=1 xi,j\nUiB j −Ai,j ≤ xi,j , ∀i ∈ [n], j ∈ [d]\nUiB j −Ai,j ≥ −xi,j ,∀i ∈ [n], j ∈ [d]\nxi,j ≥ 0, ∀i ∈ [n], j ∈ [d],\nwhere the number of constraints is O(nd) and the number of variables is O(nd).\nC `1-Low Rank Approximation\nThis section presents our main `1-low rank approximation algorithms. Section C.1 provides our three existence results. Section C.2 shows an input sparsity algorithm with poly(k) log2 d log napproximation ratio. Section C.3 improves the approximation ratio to poly(k) log d. Section C.4 explains how to obtain Õ(k) approximation ratio. Section C.5 improves the approximation ratio to O(1) by outputting a rank-2k solution. Section C.6 presents our algorithm for CUR decomposition. Section C.7 includes some useful properties. Our `1-low rank approximation algorithm for a rank-r (where k ≤ r ≤ (n, d) ) matrix is used as a black box (by setting r = poly(k)) in several other algorithms.\nC.1 Existence results via dense Cauchy transforms, sparse Cauchy transforms, Lewis weights\nThe goal of this section is to present the existence results in Corollary C.2. We first provide some bicriteria algorithms in Theorem C.1 which can be viewed as a “warmup”. Then the proof of our bicriteria algorithm actually implies the existence results.\nTheorem C.1. Given matrix A ∈ Rn×d, for any k ≥ 1, there exist bicriteria algorithms with running time T (specified below), which output two matrices U ∈ Rn×m, V ∈ Rm×d such that, with probability 9/10,\n‖UV −A‖1 ≤ α min rank−k Ak ‖Ak −A‖1.\n(I). Using a dense Cauchy transform, T = poly(n, d, k), m = O(k log k), α = O( √ k log k log d).\n(II). Using a sparse Cauchy transform, T = poly(n, d, k),m = O(k5 log5 k), α = O(k4.5 log4.5 k log d).\n(III). Sampling by Lewis weights, T = (nd)Õ(k), m = O(k log k), α = O( √ k log k).\nThe matrices in (I), (II), (III) here, are the same as those in (I), (II), (III), (IV) of Lemma D.11. Thus, they have the properties shown in Section D.2.\nProof. We define\nOPT := min rank−k Ak\n‖Ak −A‖1.\nWe define U∗ ∈ Rn×k, V ∗ ∈ Rk×d to be the optimal solution such that ‖U∗V ∗ −A‖1 = OPT . Part (I). Apply the dense Cauchy transform S ∈ Rm×n with m = O(k log k) rows, and β = O(log d). Part (II). Apply the sparse Cauchy transform S(= Π ∈ Rm×n) with m = O(k5 log5 k) rows, and β = O(σ log d) = O(k2 log2 k log d). Part (III). Use S (= D ∈ Rn×k) to denote an n × n matrix which is a sampling and rescaling diagonal matrix according to the Lewis weights of matrix U∗. It has m = O(k log k) rows, and β = O(1). Sometimes we abuse notation, and should regard D as a matrix which has size m × n, where m = O(k log k).\nWe can just replace M in Lemma D.11 with U∗V ∗−A, replace U in Lemma D.11 with U∗, and replace c1c2 with O(β). So, we can apply Lemma D.11 for S. Then we can plug it in Lemma D.8,\nwe have: with constant probability, for any c ≥ 1, for any V ′ ∈ Rk×d which satisfies\n‖SU∗V ′ − SA‖1 ≤ c · min V ∈Rk×d ‖SU∗V − SA‖1, (8)\nit has\n‖U∗V ′ −A‖1 ≤ c ·O(β)‖U∗V ∗ −A‖1. (9)\nDefine V̂i = arg min Vi∈Rk\n‖SU∗Vi − SAi‖2 for each i ∈ [d]. By using Claim B.10 with n = m and\nd = k, it shows\n‖SU∗V̂ − SA‖1 = d∑ i=1 ‖SU∗V̂i − SAi‖1 ≤ d∑ i=1 √ m‖SU∗Ṽi − SAi‖1 = √ m min V ∈Rk×d ‖SU∗V − SA‖1.\nwhich means V̂ is a √ m-approximation solution to problem, min\nV ∈Rk×d ‖SU∗V − SA‖1.\nNow, let us look into Equation (8) and Equation (9), we can obtain that\n‖U∗V̂ −A‖1 ≤ √ mO(β) OPT .\nBecause V̂i is the optimal solution of the `2 regression problem, we have\nV̂i = (SU ∗)†SAi ∈ Rk,∀i ∈ [d], which means V̂ = (SU∗)†SA ∈ Rk×d.\nPlugging V̂ into original problem, we obtain\n‖U∗(SU∗)† · SA−A‖1 ≤ √ mO(β) OPT .\nIt means\nmin rank−k X∈Rn×m\n‖XSA−A‖1 ≤ √ mO(β) OPT . (10)\nIf we ignore the constraint on the rank of X, we can get a bicriteria solution: For part (I), notice that X is an n ×m matrix which can be found by using a linear program, because matrices SA ∈ Rm×d and A ∈ Rn×d are known. For part (II), notice that X is an n×m matrix which can be found by using a linear program, because matrices SA ∈ Rm×d and A ∈ Rn×d are known. For part (III), notice that X is an n×m matrix which can be found by using a linear program, when the span of rows of DA ∈ Rm×d is known. We assume that D is known in all the above discussions. But D is actually unknown. So we need to try all the possible choices of the row span of DA. Since D samples at most m = O(k log k) rows of A, then the total number of choices of selecting m rows from n rows is ( n m ) = nO(k log k). This completes the proof.\nEquation (10) in the proof of our bicriteria solution implies the following result,\nCorollary C.2. Given A ∈ Rn×d, there exists a rank-k matrix A′ ∈ Rn×d such that A′ ∈ rowspan(S′A) ⊆ rowspan(A) and ‖A′ − A‖1 ≤ α · min\nrank−k Ak ‖A − Ak‖1, where S′ ∈ Rm×n is a\nsketching matrix. If S′\n(I). indicates the dense Cauchy transform, then α = O( √ k log k log d). (II). indicates the sparse Cauchy transform, then α = O(k4.5 log4.5 k log d). (III). indicates sampling by Lewis weights, then α = O( √ k log k).\nProof. Define OPT = min U∈Rn×k,V ∈Rk×d ‖UV −A‖1.\nProof of (I). Choose S to be a dense Cauchy transform matrix with m rows, then\nmin U∈Rn×k,Z∈Rk×m\n‖UZSA−A‖1 ≤ O( √ m log d) OPT,\nwhere m = O(k log k). Choosing A′ = UZSA completes the proof. Proof of (II). Choose Π = SD ∈ Rm×n where S ∈ Rm×n has each column chosen independently and uniformly from the m standard basis vectors of Rm, and where D is a diagonal matrix with diagonals chosen independently from the standard Cauchy distribution, then\nmin U∈Rn×k,Z∈Rk×m\n‖UZΠA−A‖1 ≤ O( √ mσ log d) OPT,\nwhere m = O(k5 log5 k) and σ = O(k2 log2 k). Choosing A′ = UZΠA completes the proof. Proof of (III). Choose D to be the sampling and rescaling matrix corresponding to the Lewis weights of U∗, and let it have m = O(k log k) nonzero entries on the diagonal, then\nmin U∈Rn×k,Z∈Rk×m\n‖UZDA−A‖1 ≤ O( √ m) OPT .\nChoosing A′ = UZDA completes the proof.\nC.2 Input sparsity time, poly(k, log n, log d)-approximation for an arbitrary matrix A\nThe algorithm described in this section is actually worse than the algorithm described in the next section. But this algorithm is easy to extend to the distributed and streaming settings (See Section K and Section J).\nAlgorithm 2 Input Sparsity Time Algorithm 1: procedure L1LowRankApproxInputSparsity(A,n, d, k) . Theorem C.3 2: Set s← r ← t1 ← Õ(k5), t2 ← Õ(k). 3: Choose sparse Cauchy matrices S ∈ Rs×n, R ∈ Rd×r, T1 ∈ Rt1×n. 4: Choose dense Cauchy matrices T2 ∈ Rd×t2 . 5: Compute S ·A, A ·R and T1 ·A · T2. 6: Compute XY = arg minX,Y ‖T1ARXY SAT2 − T1AT2‖F . 7: return ARX,Y SA. 8: end procedure\nTheorem C.3. Given matrix A ∈ Rn×d, for any k ≥ 1, there exists an algorithm which takes O(nnz(A)) + (n+ d) · poly(k) time and outputs two matrices U ∈ Rn×k, V ∈ Rk×d such that\n‖UV −A‖1 ≤ O(poly(k) log n log2 d) min rank−k Ak ‖Ak −A‖1\nholds with probability 9/10.\nProof. Choose a Cauchy matrix S ∈ Rs×n (notice that S can be either a dense Cauchy transform matrix or a sparse Cauchy transform matrix). Using Corollary C.2, we have\nmin U∈Rn×k,Z∈Rk×s\n‖UZSA−A‖1 ≤ αs OPT,\nwhere αs is the approximation by using matrix S. If S is a dense Cauchy transform matrix, then due to Part (I) of Corollary C.2, αs = O( √ k log k log d), s = O(k log k), and computing SA takes O(snnz(A)) time. If S is a sparse Cauchy transform matrix, then due to Part II of Corollary C.2, αs = Õ(k\n4.5 log d), s = Õ(k5), and computing SA takes nnz(A) time. We define U∗, Z∗ = arg minU,Z ‖UZSA−A‖1. For the fixed Z∗ ∈ Rk×s, choose a Cauchy matrix R ∈ Rd×r (note that R can be either a dense Cauchy transform matrix or a sparse Cauchy transform matrix) and sketch on the right of (UZSA − A). If R is a dense Cauchy transform matrix, then αr = O(log n), r = O(k log k), computing AR takes O(r · nnz(A)) time. If R is a sparse Cauchy transform matrix, then αr = Õ(k2) log n, r = Õ(k5), computing AR takes O(nnz(A)) time.\nDefine a row vector Û j = AjR((Z∗SA)R)† ∈ Rk. Then\n∀j ∈ [n], ‖Û jZ∗SAR−AjR‖2 = min x∈Rk ‖x>Z∗SAR−AjR‖2.\nRecall that r is the number of columns of R. Due to Claim B.10,\nn∑ j=1 ‖AjR((Z∗SA)R)†Z∗SAR−AjR‖1 ≤ O( √ r) n∑ j=1 min Uj∈Rk ‖U jZ∗SAR−AjR‖1,\nwhich is equivalent to\n‖AR((Z∗SA)R)†Z∗SAR−AR‖1 ≤ O( √ r) min\nU∈Rn×k ‖UZ∗SAR−AR‖1,\nwhere AR is an n× r matrix and SA is an s× d matrix. Using Lemma D.8, we obtain,\n‖AR((Z∗SA)R)†Z∗SA−A‖1 ≤ O( √ rαr) min\nU∈Rn×k ‖UZ∗SA−A‖1.\nWe define X∗ ∈ Rr×k, Y ∗ ∈ Rk×s,\nX∗, Y ∗ = arg min X∈Rr×k,Y ∈Rk×s ‖ARXY SA−A‖1.\nThen,\n‖ARX∗Y ∗SA−A‖1 ≤ ‖AR((Z∗SA)R)†Z∗SA−A‖1 ≤ O( √ rαr) min\nU∈Rn×k ‖UZ∗SA−A‖1\n= O( √ rαr) min\nU∈Rn×k,Z∈Rk×s ‖UZSA−A‖1\n≤ O( √ rαrαs) OPT .\nIt means that ARX∗, Y ∗SA gives an O(αrαs √ r)-approximation to the original problem.\nThus it suffices to use Lemma C.4 to solve\nmin X∈Rr×k,Y ∈Rk×s\n‖ARXY SA−A‖1,\nby losing an extra poly(k) log d factor in the approximation ratio. By using a sparse Cauchy transform (for the place discussing the two options), combining the approximation ratios and running times all together, we can get poly(k) log(n) log2(d)-approximation ratio with O(nnz(A)) + (n+ d) poly(k) running time. This completes the proof.\nLemma C.4. Given matrices A ∈ Rn×d, SA ∈ Rs×n,RA ∈ Rr×d where S ∈ Rs×n, R ∈ Rd×r with min(n, d) ≥ max(r, s). For any 1 ≤ k ≤ min(r, s), there exists an algorithm that takes O(nnz(A)) + (n+ d) poly(s, r, k) time to output two matrices X ′ ∈ Rr×k, Y ′ ∈ Rk×s such that\n‖ARX ′ · Y ′SA−A‖1 ≤ poly(r, s) log(d) min X∈Rr×k,Y ∈Rk×s ‖ARXY SA−A‖1\nholds with probability at least .999.\nProof. Choose sketching matrices T1 ∈ Rt1×n to sketch on the left of (ARXY SA − A) (note that S can be either a dense Cauchy transform matrix or a sparse Cauchy transform matrix). If T1 is a dense Cauchy transform matrix, then t1 = O(r log r), αt1 = O(log d), and computing T1A takes O(t1 · nnz(A)) time. If T1 is a sparse Cauchy transform matrix, then t1 = Õ(r5), αt1 = Õ(r2) log d, and computing T1A takes nnz(A) time.\nChoose dense Cauchy matrices T>2 ∈ Rt2×d to sketch on the right of T1(ARXY SA − A) with t2 = O((t1 + s) log(t1 + s)). We get the following minimization problem,\nmin X∈Rr×k,Y ∈Rk×s\n‖T1ARXY SAT2 − T1AT2‖1. (11)\nDefine X ′, Y ′ to be the optimal solution of\nmin X∈Rr×k,Y ∈Rk×s\n‖T1ARXY SAT2 − T1AT2‖F .\nDue to Claim B.11,\n‖T1ARX ′Y ′SAT2 − T1AT2‖1 ≤ √ t1t2 min\nX∈Rr×k,Y ∈Rk×s ‖T1ARXY SAT2 − T1AT2‖1.\nDue to Lemma D.10\n‖ARX ′Y ′SA−A‖1 ≤ √ t1t2αt1 log t1 min\nX∈Rr×k,Y ∈Rk×s ‖ARXY SA−A‖1.\nIt remains to solve\nmin X∈Rr×k,Y ∈Rk×s\n‖T1ARXY SAT2 − T1AT2‖F .\nBy using Theorem B.9 and choosing T1 to be a sparse Cauchy transform matrix, we have that the optimal rank-k solution X ′Y ′ is (T1AR)†(UBU>B (T1AT2)VCV > C )k(SAT2) which can be computed in O(nnz(A)) + (n + d) poly(s, r, k) time. Here, UB are the left singular vectors of T1AR. VC are the right singular vectors of SAT2.\nAn alternative way of solving Equation (11) is using a polynomial system verifier. Note that a polynomial system verifier does not allow absolute value constraints. Using Claim B.12, we are able to remove these absolute value constraints by introducing new constraints and variables. Thus, we can get a better approximation ratio but by spending exponential running time in k. In the previous step, we should always use a dense Cauchy transform to optimize the approximation ratio.\nCorollary C.5. Given A ∈ Rn×d, there exists an algorithm which takes nd ·poly(k)+(n+d) ·2Õ(k2) time and outputs two matrices U ∈ Rn×k, V ∈ Rk×d such that\n‖UV −A‖1 ≤ O(poly(k) log n log2 d) min rank−k Ak ‖Ak −A‖1\nholds with probability 9/10.\nThe poly(k) factor in the above corollary is much smaller than that in Theorem C.3.\nC.3 poly(k, log d)-approximation for an arbitrary matrix A\nIn this section, we explain how to get an O(log d) · poly(k) approximation.\nAlgorithm 3 poly(k) log d-approximation Algorithm\n1: procedure L1LowRankApproxPolykLogd(A,n, d, k) . Theorem C.6 2: Set s← Õ(k5). 3: Choose sparse Cauchy matrices S ∈ Rs×n and compute S ·A. 4: Implicitly obtain B = UBVB by finding VB = SA ∈ Rs×d and UB ∈ Rn×s where ∀i ∈ [n],\nrow vector (UB)i gives an O(1) approximation to minx∈R1×s ‖xSA−Ai‖1. 5: U, V ←L1LowRankApproxB(UB, VB, n, d, k, s). . Theorem C.19 6: return U, V . 7: end procedure\nIntuitively, our algorithm has two stages. In the first stage, we just want to find a low rank matrix B which is a good approximation to A. Then, we can try to find a rank-k approximation to B. Since now B is a low rank matrix, it is much easier to find a rank-k approximation to B. The procedure L1LowRankApproxB(UB, VB, n, d, k, s) corresponds to Theorem C.19.\nTheorem C.6. Given matrix A ∈ Rn×d, for any k ≥ 1, there exists an algorithm which takes nnz(A) + (n+ d) · poly(k) time to output two matrices U ∈ Rn×k, V ∈ Rk×d such that\n‖UV −A‖1 ≤ poly(k) log d min rank−k Ak ‖Ak −A‖1\nholds with probability 9/10.\nProof. We define\nOPT := min rank−k Ak\n‖Ak −A‖1.\nThe main idea is to replace the given n × d matrix A with another low rank matrix B which also has size n × d. Choose S ∈ Rs×n to be a Cauchy matrix, where s ≤ poly(k) (note that, if S is a dense Cauchy transform matrix, computing SA takes O(snnz(A)) time, while if S is a sparse Cauchy transform matrix, computing SA takes O(nnz(A)) time). Then B is obtained by taking each row of A and replacing it with its closest point (in `1-distance) in the row span of SA. By using Part II of Corollary C.2, we have,\nmin U∈Rn×k,Z∈Rk×s\n‖UZSA−A‖1 ≤ O( √ spoly(k) log d) OPT .\nWe define B to be the product of two matrices UB ∈ Rn×s and VB ∈ Rs×d. We define VB to be SA and UB to be such that for any i ∈ [n], (UB)i gives an O(1)-approximation to problem\nmin x∈R1×s ‖xSA − Ai‖1. This can be solved using an `1-regression solver in nnz(A) + (n + d) poly(s) time.\nBy the definition of B, it is an n×d matrix. Naïvely we can write down B after finding UB and VB. The time for writing down B is O(nd). To avoid this, we can just keep a factorization UB and VB. We are still able to run algorithm L1LowRankApproxB. Because s = poly(k), the running time of algorithm L1LowRankApproxB is still O(nnz(A)) + (n+ d) poly(k).\nBy the definition of B, we have that B has rank at most s. Suppose we then solve `1-low rank approximation problem for rank-s matrix B, finding a rank-k g-approximation matrix UV . Due to Lemma C.15, we have that if B is an f -approximation solution to A, then UV is also an O(fg)-approximation solution to A,\n‖UV −A‖1 ≤ O(log d) · poly(k) · gOPT .\nUsing Theorem C.19 we have that g = poly(s), which completes the proof."
    }, {
      "heading" : "C.4 Õ(k)-approximation for an arbitrary matrix A",
      "text" : "Algorithm 4 Õ(k)-approximation Algorithm\n1: procedure L1LowRankApproxK(A,n, d, k) . Theorem C.7 2: r ← O(k log k),m← t1 ← O(r log r), t2 ← O(m logm). 3: Guess a diagonal matrix R ∈ Rd×d with only r 1s. . R selects r columns of A ∈ Rn×d. 4: Compute a sampling and rescaling matrix D ∈ Rn×n, T1 ∈ Rn×n corresponding to the Lewis\nweights of AR, and let them have m, t1 nonzero entries on the diagonals, respectively. 5: Compute a sampling and rescaling matrix T>2 ∈ Rd×d according to the Lewis weights of\n(DA)>, and let it have t2 nonzero entries on the diagonal. 6: Solve minX,Y ‖T1ARXYDAT2 − T1AT2‖1. 7: Take the best solution X,Y over all guesses of R. 8: return ARX, Y DA. 9: end procedure\nTheorem C.7. Given matrix A ∈ Rn×d, there exists an algorithm that takes poly(n) · dÕ(k) · 2Õ(k2) time and outputs two matrices U ∈ Rn×k, V ∈ Rk×d such that\n|UV −A‖1 ≤ Õ(k) min rank−k Ak ‖Ak −A‖1\nholds with probability 9/10.\nProof. We define\nOPT := min rank−k Ak\n‖Ak −A‖1."
    }, {
      "heading" : "Let U∗ ∈ Rn×k, V ∗ ∈ Rk×d satisfy",
      "text" : "‖U∗V ∗ −A‖1 = OPT .\nLet S> ∈ Rd×d denote the sampling and rescaling matrix corresponding to the Lewis weights of (V ∗)>, where the number of nonzero entries on the diagonal of S is s = r = O(k log k). Let\nR> ∈ Rd×d denote a diagonal matrix such that ∀i ∈ [d], if Si,i 6= 0, then Ri,i = 1, and if Si,i = 0, then Ri,i = 0. Since rowspan(R>A>) = rowspan(S>A>),\nmin Z∈Rm×k,V ∈Rk×d ‖ASZV −A‖1 = min Z∈Rm×k,V ∈Rk×d ‖ARZV −A‖1.\nCombining with Part III of Corollay C.2, there exists a rank-k solution in the column span of AR, which means,\nmin Z∈Rm×k,V ∈Rk×d\n‖ARZV −A‖1 ≤ O( √ r) OPT . (12)\nBecause the number of 1s of R is r, and the size of the matrix is d×d, there are ( d r ) = dÕ(k) different choices for locations of 1s on the diagonal of R. We cannot compute R directly, but we can guess all the choices of locations of 1s. Regarding R as selecting r columns of A, then there are dÕ(k) choices. There must exist a “correct” way of selecting a subset of columns over all all choices. After trying all of them, we will have chosen the right one.\nFor a fixed guess R, we can compute D ∈ Rn×n, which is a sampling and rescaling matrix corresponding to the Lewis weights of AR, and let m = O(k log2 k) be the number of nonzero entries on the diagonal of D.\nBy Equation (12), there exists a W ∈ Rr×k such that,\nmin V ∈Rk×d\n‖ARWV −A‖1 ≤ O( √ r) OPT . (13)\nWe define V̂i = arg min Vi∈Rk×d\n‖DARWVi−DAi‖2, ∀i ∈ [d], which means V̂i = (DARW )†DAi ∈ Rk. Then\nV̂ = (DARW )†DA ∈ Rk×d. We define V ∗ = arg min V ∈Rk×d ‖ARWV −A‖1. Then, by Claim B.10, it has\n‖DARWV̂ −DA‖1 ≤ O( √ m) min\nV ∈Rk×d ‖DARWV −DA‖1.\nBy applying Lemma D.11, Lemma D.8 and Equation (13), we can show\n‖ARWV̂ −A‖1 ≤ O( √ m)‖ARWV ∗ −A‖1 ≤ O( √ mr) OPT ≤ Õ(k) OPT .\nPlugging V̂ = (DARW )†DA into ‖ARWV̂ −A‖1, we obtain that\n‖ARW (DARW )†DA−A‖1 ≤ Õ(k) OPT .\nand it is clear that,\nmin X∈Rr×k,Y ∈Rk×m\n‖ARXYDA−A‖1 ≤ ‖ARW (DARW )†DA−A‖1 ≤ Õ(k) OPT .\nRecall that we guessed R, so it is known. We can compute T1 ∈ Rn×n, which is a sampling and rescaling diagonal matrix corresponding to the Lewis weights of AR, and t1 = O(r log r) is the number of nonzero entries on the diagonal of T1.\nAlso, DA is known, and the number of nonzero entries in D ism = O(k log2 k). We can compute T>2 ∈ Rd×d, which is a sampling and rescaling matrix corresponding to the Lewis weights of (DA)>, and t2 = O(m logm) is the number of nonzero entries on the diagonal of T2.\nDefine X∗, Y ∗ = arg min X∈Rr×k,Y ∈Rk×m ‖T1(AR)XY (DA)T2 − T1AT2‖1. Thus, using Lemma D.10, we\nhave\n‖(AR)X∗Y ∗(DA)−A‖1 ≤ Õ(k) OPT .\nTo find X∗, Y ∗, we need to solve this minimization problem\nmin X∈Rr×k,Y ∈Rk×m\n‖T1(AR)XY (DA)T2 − T1AT2‖1,\nwhich can be solved by a polynomial system verifier (see more discussion in Section B.1 and B.5). In the next paragraphs, we explain how to solve the above problem by using a polynomial system verifier. Notice that T1(AR) is known and (DA)T2 is also known. First, we create r × k variables for the matrix X, i.e., one variable for each entry of X. Second, we create k × m variables for matrix Y , i.e., one variable for each entry of Y . Putting it all together and creating t1× t2 variables σi,j , ∀i ∈ [t1], j ∈ [t2] for handling the unknown signs, we write down the following optimization poblem\nmin X,Y t1∑ i=1 t2∑ j=1 σi,j(T1(AR)XY (DA)T2)i,j\ns.t σ2i,j = 1, ∀i ∈ [t1], j ∈ [t2] σi,j · (T1(AR)XY (DA)T2)i,j ≥ 0, ∀i ∈ [t1], j ∈ [t2].\nNotice that the number of constraints is O(t1t2) = Õ(k2), the maximum degree is O(1), and the number of variables O(t1t2 + km+ rk) = Õ(k2). Thus the running time is,\n(# constraints ·# degree)O(# variables) = 2Õ(k2).\nTo use a polynomial system verifier, we need to discuss the bit complexity. Suppose that all entries are multiples of δ, and the maximum is ∆, i.e., each entry ∈ {−∆, · · · ,−2δ,−δ, 0, δ, 2δ, · · · ,∆}, and ∆/δ = 2poly(nd). Then the running time is O(poly(∆/δ)) · 2Õ(k2) = poly(nd)2Õ(k2).\nAlso, a polynomial system verifier is able to tell us whether there exists a solution in a semialgebraic set. In order to find the solution, we need to do a binary search over the cost C. In each step of the binary search we use a polynomial system verifier to determine if there exists a solution in,\nt1∑ i=1 t2∑ j=1 σi,j(T1(AR)XY (DA)T2)i,j ≤ C\nσ2i,j = 1, ∀i ∈ [t1], j ∈ [t2] σi,j · (T1(AR)XY (DA)T2)i,j ≥ 0, ∀i ∈ [t1], j ∈ [t2].\nIn order to do binary search over the cost, we need to know an upper bound on the cost and also a lower bound on the minimum nonzero cost. The upper bound on the cost is Cmax = O(nd∆), and the minimum nonzero cost is Cmin = 2−Ω(poly(nd)). Thus, the total number of steps for binary search is O(log(Cmax/Cmin)). Overall, the running time is\nndÕ(k) · 2Õ(k2) · log(∆/δ) · log(Cmax/Cmin) = poly(n)dÕ(k)2Õ(k 2).\nThis completes the proof.\nInstead of solving an `1 problem at the last step of L1LowRankApproxK by using a polyonomial system verifier, we can just solve a Frobenious norm minimization problem. This slighly improves the running time and pays an extra poly(k) factor in the approximation ratio. Thus, we obtain the following corollary,\nCorollary C.8. Given matrix A ∈ Rn×d, there exists an algorithm that takes poly(n) · dÕ(k) time which outputs two matrices U ∈ Rn×k, V ∈ Rk×d such that\n|UV −A‖1 ≤ poly(k) min rank−k Ak ‖Ak −A‖1\nholds with probability 9/10.\nC.5 Rank-2k and O(1)-approximation algorithm for an arbitrary matrix A\nIn this section, we show how to output a rank-2k solution that is able to achieve anO(1)-approximation.\nAlgorithm 5 Bicriteria O(1)-approximation Algorithm\n1: procedure L1LowRankApproxBicriteria(A,n, d, k) . Theorem C.9 2: UB, VB ← min\nU∈Rn×k,V ∈Rk×d ‖UV −A‖F .\n3: r ← O(k log k). 4: Guess a diagonal matrix D ∈ Rn×n with r nonzero entries. 5: Guess matrix DU ∈ Rr×k. 6: Find VA by solving minV ‖DUV −D(A−B)‖1. 7: Find UA by solving minU ‖UVA − (A−B)‖1.\n8: Take the best solution [ UA UB ] , [ VA VB ] over all guesses.\n9: return [ UA UB ] , [ VA VB ] .\n10: end procedure\nTheorem C.9. Given matrix A ∈ Rn×d, for any k ≥ 1, there exists an algorithm which takes (nd)Õ(k 2) time to output two matrices U ∈ Rn×2k, V ∈ R2k×d,\n‖UV −A‖1 . min rank−k Ak ‖Ak −A‖1,\nholds with probability 9/10.\nProof. We define U∗ ∈ Rn×k, V ∗ ∈ Rk×d to be the optimal solution, i.e.,\nU∗V ∗ = arg min U∈Rn×k,V ∈Rk×d ‖UV −A‖1,\nand define OPT = ‖U∗V ∗ −A‖1. Solving the Frobenious norm problem, we can find a factorization of a rank-k matrix B = UBVB\nwhere UB ∈ Rn×k, VB ∈ Rk×d, and B satisfies ‖A−B‖1 ≤ αB OPT for an αB = √ nd.\nLet D be a sampling and rescaling diagonal matrix corresponding to the Lewis weights of U∗, and let the number of nonzero entries on the diagonal be t = O(k log k).\nBy Lemma D.11 and Lemma D.8, the solution of min V ∈Rk×d\n‖DU∗V −D(A − B)‖1 together with\nU∗ gives an O(1)-approximation to A−B. In order to compute D we need to know U∗. Although\nwe do not know U∗, there still exists a way to figure out the Lewis weights. The idea is the same as in the previous discussion Lemma C.10 “Guessing Lewis weights”. By Claim C.11 and Claim C.12, the total number of possible D is nO(t).\nLemma C.10 tries to find a rank-k solution when all the entries in A are integers at most poly(nd). Here we focus on a bicriteria algorithm that outputs a rank-2k matrix without such a strong bit complexity assumption. We can show a better claim C.13, which is that the total numer of possible DU∗ is N Õ(k2) where N = poly(n) is the number of choices for a single entry in U∗.\nWe explain how to obtain an upper bound on N . Consider the optimum ‖U∗V ∗ − (A − B)‖1. We can always change the basis so assume V ∗ is an Auerbach basis (i.e., an `1-well-conditioned basis discussed in Section 1), so e‖x‖1 ≥ ‖xV ∗‖1 ≥ ‖x‖1/f , where e, f = poly(k). Then no entry of U∗ is larger than 2f‖A−B‖1, otherwise we could replace U∗ with 0 and get a better solution. Also any entry smaller than ‖A − B‖1/(enkαB100) can be replaced with 0 as this will incur additive error at most OPT /100. So if we round to integer multiples of ‖A−B‖1/(enkαB100) we only have O(enkαBf) possibilities for each entry of U∗ and still have an O(1)-approximation. We will just refer to this rounded U∗ as U∗, abusing notation.\nLet U denote the set of all the matrices U that we guess. From the above discussion, we conclude that, there exists a U ∈ U such that ‖UV ∗ − (A−B)‖1 ≤ O(OPT).\nFor each guess of DU∗ and D, we find VA, UA in the following way. We find VA by using a linear program to solve,\nmin V ∈Rk×d\n‖DU∗V −D(A−B)‖1.\nGiven VA and A, we write down a linear program to solve this problem,\nmin U∈Rn×k\n‖UVA − (A−B)‖1,\nwhich takes poly(ndk) time. Then we obtain UA. Recall that VB, UB are the two factors of B and it is a rank-k, αB-approximation solution to min U,V ‖UV −A‖1. Then we have\n∥∥∥∥[UA UB] [VAVB ] −A ∥∥∥∥ 1 = ∥∥∥∥UAVA − (A− UBVB)∥∥∥∥ 1 = ∥∥∥∥UAVA − (A−B)∥∥∥∥ 1 .\nBecause there must exist a pair UA, VA satifying ‖UAVA− (A−B)‖1 ≤ O(OPT), it follows that by taking the best solution [ UA UB ] [VA VB ] over all guesses, we obtain an O(1)-approximation solution.\nOverall, the running time is (nd)Õ(k2).\nLemma C.10. Given an n × d matrix A with integers bounded by poly(n), for any k ≥ 1, there exists an algorithm which takes (nd)Õ(k3) time to output two matrices U ∈ Rn×k, V ∈ Rk×d, such that\n‖UV −A‖1 . min rank−k Ak ‖Ak −A‖1,\nProof. We define U∗ ∈ Rn×k, V ∗ ∈ Rk×d to be the optimal solution, i.e.,\nU∗, V ∗ = arg min U∈Rn×k,V ∈Rk×d ‖UV −A‖1,\nand define OPT = ‖U∗V ∗ −A‖1. Let D denote a sampling and rescaling diagonal matrix corresponding to the Lewis weights of U∗, and let the number of nonzero entries on the diagonal be t = O(k log k). By Lemma D.11 and Lemma D.8, the solution to min\nV ∈Rk×d ‖DU∗V −DA‖1 together with U∗ gives\nan O(1)-approximation to A. In order to compute D, we need to know U∗. Although we do not know U∗, there still exists a way to figure out the Lewis weights. We call the idea “Guessing Lewis weights”. We will explain this idea in the next few paragraphs.\nFirst, we can guess the nonzero entries on the diagonal, because the number of choices is small.\nClaim C.11. The number of possibile choice of supp(D) is at most nO(t).\nProof. The matrix has dimension n × n and the number of nonzero entries is t. Thus the number of possible choices is at most ∑t i=1 ( n t ) = nO(t).\nSecond, we can guess the value of each probability. For each probability, it is trivially at most 1. If the probability is less than 1/(poly(n)k log k), then we will never sample that row with high probability. It means that we can truncate the probability if it is below that threshold. We can also round each probability to 2−i which only loses another constant factor in the approximation ratio. Thus, we have:\nClaim C.12. The total number of possible D is nO(t) = nÕ(k).\nSince ∆/δ ≤ poly(n) and the entries of A are in {−∆,−∆+δ, . . . ,−2δ,−δ, 0, δ, 2δ, · · · ,∆−δ,∆}, we can lower bound the cost of ‖U∗V −A‖1 given that it is non-zero by (nd∆/δ)−O(k) (if it is zero then A has rank at most k and we output A) using Lemma 4.1 in [CW09] and relating entrywise `1-norm to Frobenius norm. We can assume V is an `1 well-conditioned basis, since we can replace U∗ with U∗R−1 and V with RV for any invertible linear transformation R. By properties of such basis, we can discretize the entries of U∗ to integer multiples of (nd∆/δ)−O(k) while preserving relative error. Hence we can correctly guess each entry of DU∗ in ( nO(k) ) time.\nClaim C.13. The total numer of possible DU∗ is nÕ(k3).\nIn the following, let DU denote a guess of DU∗. Now the problem remaining is to solve min\nV ∈Rk×d ‖DUV − DA‖1. Since we already know DA can be computed, and we know DU , we can solve this multiple regression problem by running linear programming. Thus the running time of this step is in poly(nd). After we get such a solution V , we use a linear program to solve\nmin U∈Rn×k\n‖DUV −DA‖1. Then we can get U .\nAfter we guess all the choices of D and DU∗, we must find a solution U, V which gives an O(1) approximation. The total running time is nÕ(k) · nÕ(k3) · poly(nd) = (nd)Õ(k3)."
    }, {
      "heading" : "C.6 CUR decomposition for an arbitrary matrix A",
      "text" : "Theorem C.14. Given matrix A ∈ Rn×d, for any k ≥ 1, there exists an algorithm which takes O(nnz(A))+(n+d) poly(k) time to output three matrices C ∈ Rn×c with columns from A, U ∈ Rc×r, and R ∈ Rr×d with rows from A, such that rank(CUR) = k, c = O(k log k), r = O(k log k), and\n‖CUR−A‖1 ≤ poly(k) log d min rank−k Ak ‖Ak −A‖1,\nholds with probability 9/10.\nAlgorithm 6 CUR Decomposition Algorithm 1: procedure L1LowRankApproxCUR(A,n, d, k) . Theorem C.14 2: UB, VB ←L1LowRankApproxPolykLogd(A,n, d, k). 3: Let D1 ∈ Rn×n be the sampling and rescaling diagonal matrix coresponding to the Lewis\nweights of B1 = UB ∈ Rn×k, and let D1 have d1 = O(k log k) nonzero entries. 4: Let D>2 ∈ Rd×d be the sampling and rescaling diagonal matrix coresponding to the Lewis\nweights of B>2 = ( (D1B1) †D1A )> ∈ Rd×k, and let D2 have d2 = O(k log k) nonzero entries.\n5: C ← AD2, U ← (B2D2)†(D1B1)†, and R← D1A. 6: return C,U,R. 7: end procedure\nProof. We define\nOPT := min rank−k Ak\n‖Ak −A‖1.\nDue to Theorem C.6, we can output two matrices UB ∈ Rn×k, VB ∈ Rk×d such that UBVB gives a rank-k, and poly(k) log d-approximation solution to A, i.e.,\n‖UBVB −A‖1 ≤ poly(k) log dOPT . (14)\nBy Section B.3, we can compute D1 ∈ Rn×n which is a sampling and rescaling matrix corresponding to the Lewis weights of B1 = UB in O(n poly(k)) time, and there are d1 = O(k log k) nonzero entries on the diagonal of D1.\nDefine V ∗ ∈ Rk×d to be the optimal solution of min V ∈Rk×d ‖B1V −A‖1, V̂ = (D1B1)†D1A ∈ Rk×d,\nU1 ∈ Rn×k to be the optimal solution of min U∈Rn×k ‖UV̂ − A‖1, and V ′ to be the optimal solution of\nmin V ∈Rk×d ‖D1A−D1B1V ‖1. By Claim B.10, we have\n‖D1B1V̂ −D1A‖1 ≤ √ d1‖D1B1V ′ −D1A‖1.\nDue to Lemma D.11 and Lemma D.8, with constant probability, we have ‖B1V̂ −A‖1 ≤ √ d1αD1‖B1V ∗ −A‖1,\nwhere αD1 = O(1). Now, we can show,\n‖U1V̂ −A‖1 ≤ ‖B1V̂ −A‖1 by U1 = arg min U∈Rn×k ‖UV̂ −A‖1\n. √ d1‖B1V ∗ −A‖1\n≤ √ d1‖UBVB −A‖1\n≤ poly(k) log dOPT . by Equation (14) (15)\nWe define B2 = V̂ , then we replace V̂ by B2 ∈ Rk×d and look at this objective function,\nmin U∈Rn×k\n‖UB2 −A‖1,\nwhere U∗ denotes the optimal solution. We use a sketching matrix to sketch the RHS of matrix UB2 − A ∈ Rn×d. Let D>2 ∈ Rd×d denote a sampling and rescaling diagonal matrix corresponding to the Lewis weights of B>2 ∈ Rd×k, and let the number of nonzero entries on the diagonal of D2 be d2 = O(k log k). We define Û = AD2(B2D2)† ∈ Rn×k, U ′ ∈ Rn×k to be the optimal solution of\nmin U∈Rn×k ‖(UB2 −A)D2‖1. Recall that U1 ∈ Rn×k is the optimal of min U∈Rn×k ‖UB2 −A‖1. By Claim B.10, we have\n‖ÛB2D2 −AD2‖1 ≤ √ d2‖U ′B2D2 −AD2‖1.\nAccording to Lemma D.8 and Lemma D.11, with constant probability, ‖ÛB2 −A‖1 ≤ αD2 √ d2‖U1B2 −A‖1,\nwhere αB2 = O(1). We have\n‖ÛB2 −A‖1 ≤ √ d2αD2‖U1B2 −A‖1\n= √ d2αD2‖U1V̂ −A‖1 by B2 = V̂ ≤ poly(k) log(d) OPT . by Equation (15)"
    }, {
      "heading" : "Notice that ÛB2 = AD2(B2D2)†(D1B1)†D1A. Setting",
      "text" : "C = AD2 ∈ Rn×d2 , U = (B2D2)†(D1B1)† ∈ Rd2×d1 , and R = D1A ∈ Rd1×d,\nwe get the desired CUR decomposition,\n‖AD2︸︷︷︸ C · (B2D2)†(D1B1)†︸ ︷︷ ︸ U ·D1A︸︷︷︸ R −A‖1 ≤ poly(k) log(d) OPT .\nwith rank(CUR) = k. Overall, the running time is O(nnz(A)) + (n+ d) poly(k).\nC.7 Rank-r matrix B"
    }, {
      "heading" : "C.7.1 Properties",
      "text" : "Lemma C.15. Given matrix A ∈ Rn×d, let OPT = min rank−k Ak ‖A−Ak‖1. For any r ≥ k, if rank-r matrix B ∈ Rn×d is an f -approximation to A, i.e.,\n‖B −A‖1 ≤ f ·OPT,\nand U ∈ Rn×k, V ∈ Rk×d is a g-approximation to B, i.e.,\n‖UV −B‖1 ≤ g · min rank−k Bk ‖Bk −B‖1,\nthen, ‖UV −A‖1 . gf ·OPT .\nProof. We define Ũ ∈ Rn×k, Ṽ ∈ Rk×d to be two matrices, such that\n‖Ũ Ṽ −B‖1 ≤ g min rank−k Bk ‖Bk −B‖1,\nand also define,\nÛ , V̂ = arg min U∈Rn×k,V ∈Rk×d ‖UV −B‖1 and U∗, V ∗ = arg min U∈Rn×k,V ∈Rk×d ‖UV −A‖1,\nThen,\n‖Ũ Ṽ −A‖1 ≤ ‖Ũ Ṽ −B‖1 + ‖B −A‖1 by triangle inequality\n≤ g‖Û V̂ −B‖1 + ‖B −A‖1 by definition ≤ g‖U∗V ∗ −B‖1 + ‖B −A‖1 by ‖Û V̂ −B‖1 ≤ ‖U∗V ∗ −B‖1 ≤ g‖U∗V ∗ −A‖1 + g‖B −A‖1 + ‖B −A‖1 by triangle inequality = gOPT +(g + 1)‖B −A‖1 by definition of OPT ≤ gOPT +(g + 1)f ·OPT by B is f -approximation to A . gf OPT .\nThis completes the proof.\nLemma C.16. Given a matrix B ∈ Rn×d with rank r, for any 1 ≤ k < r, for any fixed U∗ ∈ Rn×k, choose a Cauchy matrix S with m = O(r log r) rows and rescaled by Θ(1/m). With probability .999 for all V ∈ Rk×d, we have\n‖SU∗V − SB‖1 ≥ ‖U∗V −B‖1.\nProof. This follows by definitions in Section D and Lemma D.23.\nLemma C.17. Given a matrix B ∈ Rn×d with rank r, for any 1 ≤ k < r, for any fixed U∗ ∈ Rn×k, V ∗ ∈ Rk×d, choose a Cauchy matrix S with m rows and rescaled by Θ(1/m). We have\n‖SU∗V ∗ − SB‖1 ≤ O(r log r)‖U∗V ∗ −B‖1,\nwith probabiliaty .999.\nProof. Let U denote a well-conditioned basis of [U∗V ∗, B], then U has d̃ = O(r) columns. We have\n‖SU‖1 = d̃∑ i=1 m∑ j=1 |(SUi)j |\n= d̃∑ i=1 m∑ j=1 | 1 m n∑ l=1 Sj,lUl,i| by Sj,l ∼ C(0, 1)\n= 1\nm d̃∑ i=1 m∑ j=1 |ci,j | by ci,j ∼ C(0, ‖Ui‖1)\n= 1\nm d̃∑ i=1 m∑ j=1 ‖Ui‖1 · wi+(j−1)d, by wi,j ∼ |C(0, 1)|\nwhere the last step follows since each wi can be thought of as a clipped half-Cauchy random variable. Define d′ = md̃. Define event ξi to be the situation when wi < D (we will choose D later), and define event ξ = ξ1 ∩ ξ2 ∩ · · · ∩ ξd′ . Using a similar proof as Lemma D.12, which is also similar to previous work [Ind06, SW11, CDMI+13], we obtain that\nPr [ m∑ i=1 ‖SUi‖1 ≥ m∑ i=1 ‖Ui‖1t ] . log d′ t + d′ D .\nChoosing t = Θ(log d′) and D = Θ(d′), we have\nPr [ m∑ i=1 ‖SUi‖1 ≥ m∑ i=1 ‖Ui‖1O(log d′) ] ≤ 1 C ,\nfor a constant C. Condition on the above event. Let y = Ux, for some x ∈ Rd. Then for any y,\n‖Sy‖1 = ‖SUx‖1\n≤ d′∑ j=1 ‖SUjxj‖1 by triangle inequality = d′∑ j=1 |xj | · ‖SUj‖1 . ‖x‖∞ log(d′) d′∑ j=1 ‖Uj‖1\n. r log r‖y‖1,\nwhere the last step follows by ∑d′\nj=1 ‖Uj‖1 ≤ d′ and ‖x‖∞ ≤ ‖Ux‖1 = ‖y‖1. Chossing C = 1000 completes the proof.\nLemma C.18. Given a matrix M ∈ Rn×d with rank O(r), choose a random matrix S ∈ Rm×n with each entry drawn from a standard Cauchy distribution and scaled by Θ(1/m). We have that\n‖SM‖1 ≤ O(r log r)‖M‖1,\nholds with probability .999.\nProof. Let U ∈ RO(r) be the well-condtioned basis of M . Then each column of M can be expressed by Ux for some x. We then follow the same proof as that of Lemma C.17.\nC.7.2 poly(k, r)-approximation for rank-r matrix B\nTheorem C.19. Given a factorization of a rank-r matrix B = UBVB ∈ Rn×d, where UB ∈ Rn×r, VB ∈ Rr×d, for any 1 ≤ k ≤ r there exists an algorithm which takes (n + d) · poly(k) time to output two matrices U ∈ Rn×k, V ∈ Rk×d such that\n‖UV −B‖1 ≤ poly(r) min rank−k Bk ‖Bk −B‖1,\nholds with probability 9/10.\nAlgorithm 7 poly(r, k)-approximation Algorithm for Rank-r matrix B\n1: procedure L1LowRankApproxB(UB, VB, n, d, k, r) . Theorem C.19 2: Set s← Õ(r), r′ ← Õ(r), t1 ← Õ(r), t2 ← Õ(r). 3: Choose dense Cauchy matrices S ∈ Rs×n, R ∈ Rd×r′ , T1 ∈ Rt1×n, T2 ∈ Rd×t2 . 4: Compute S · UB · VB, UB · VB ·R and T1 · UB · VB · T2. 5: Compute XY = arg minX,Y ‖T1UBVBRXY SUBVBT2 − T1UBVBT2‖F . 6: return UBVBRX,Y SUBVB. 7: end procedure\nProof. We define\nOPT = min rank−k Bk\n‖Bk −B‖1.\nChoose S ∈ Rs×n to be a dense Cauchy transform matrix with s = O(r log r). Using Lemma C.18, Lemma D.23, and combining with Equation (10), we have\nmin U∈Rn×k,Z∈Rk×s\n‖UZSB −B‖1 ≤ √ sO(r log r) OPT = O(r1.5 log1.5 r) OPT .\nLet αs = O(r1.5 log1.5 r). We define U∗, Z∗ = arg min\nU∈Rn×k,Z∈Rk×d ‖UZSB − B‖1. For the fixed Z∗ ∈ Rk×s, choose a dense\nCauchy transform matrix R ∈ Rd×r′ with r′ = O(r log r) and sketch on the right of (UZSB − B). We obtain the minimization problem, min\nU∈Rn×k ‖UZ∗SBR−BR‖1.\nDefine Û j = BjR((Z∗SB)R)† ∈ Rk,∀j ∈ [n]. Then Û = BR((Z∗SB)R)† ∈ Rn×k. Due to Claim B.10,\nn∑ j=1 ‖BjR((Z∗SB)R)†Z∗SBR−BjR‖1 ≤ O( √ r′) n∑ j=1 min Uj∈Rk ‖U jZ∗SBR−BjR‖1,\nwhich is equivalent to\n‖BR((Z∗SB)R)†Z∗SBR−BR‖1 ≤ O( √ r′) min\nU∈Rn×k ‖UZ∗SBR−BR‖1,\nwhere BR is an n × r′ matrix and SB is an s × d matrix. Both of them can be computed in (n+ d) poly(r) time.\nUsing Lemma D.8, Lemma D.23, Lemma C.18, we obtain,\n‖BR((Z∗SB)R)†Z∗SB −B‖1 ≤ O( √ r′αr′) min\nU∈Rn×k ‖UZ∗SB −B‖1\nwhere αr′ = Õ(r). We define X∗ ∈ Rr×k, Y ∗ ∈ Rk×s,\nX∗, Y ∗ = arg min X∈Rr′×k,Y ∈Rk×s ‖BRXY SB −B‖1.\nThen,\n‖BRX∗Y ∗SB −B‖1 ≤ ‖BR((Z∗SB)R)†Z∗SB −B‖1 ≤ O( √ r′αr′) min\nU∈Rn×k ‖UZ∗SB −B‖1\n= O( √ r′αr′) min\nU∈Rn×k,Z∈Rk×s ‖UZSB −B‖1\n≤ O( √ r′αr′αs) OPT .\nIt means that BRX, Y SB gives an O(αr′αs √ r′)-approximation to the original problem.\nThus it suffices to use Lemma C.20 to solve\nmin X∈Rr×k,Y ∈Rk×s\n‖BRXY SB −B‖1,\nby losing an extra poly(r) approximation ratio. Therefore, we finish the proof.\nLemma C.20. Suppose we are given S ∈ Rs×n, R ∈ Rd×r′, and a factorization of a rank-r matrix B = UBVB ∈ Rn×d, where UB ∈ Rn×r, VB ∈ Rr×d. Then for any 1 ≤ k ≤ r, there exists an algorithm which takes (n+ d) poly(r, r′, s) time to output two matrices X ′ ∈ Rr′×k, Y ′ ∈ Rk×s such that\n‖BRX ′ · Y ′SB −B‖1 ≤ poly(r) min X∈Rr′×k,Y ∈Rk×s ‖BRXY SB −B‖1\nholds with probability at least .999.\nProof. Choosing dense Cauchy matrices T1 ∈ Rt1×n, T>2 ∈ Rt2×d to sketch on both sides, we get the problem\nmin X∈Rr′×k,Y ∈Rk×s\n‖T1BRXY SBT2 − T1BT2‖1, (16)\nwhere t1 = Õ(r) and t2 = Õ(r). Define X ′, Y ′ to be the optimal solution of\nmin X∈Rr′×k,Y ∈Rk×s\n‖T1BRXY SBT2 − T1BT2‖F .\nDefine X̃, Ỹ to the be the optimal solution of\nmin X∈Rr′×k,Y ∈Rk×s\n‖BRXY SB −B‖1.\nBy Claim B.11,\n‖T1BRX ′Y ′SBT2 − T1BT2‖1 ≤ √ t1t2 min\nX∈Rr′×k,Y ∈Rk×s ‖T1BRXY SBT2 − T1BT2‖1.\nBy Lemma D.10, Lemma C.18 and Lemma D.23, we have\n‖BRX ′ · Y ′SB −B‖1 ≤ √ t1t2 · Õ(r2)‖BRX̃ · Ỹ SB −B‖1.\nThis completes the proof."
    }, {
      "heading" : "D Contraction and Dilation Bound for `1",
      "text" : "This section presents the essential lemmas for `1-low rank approximation. Section D.1 gives some basic definitions. Section D.2 shows some properties implied by contraction and dilation bounds. Section D.3 presents the no dilation lemma for a dense Cauchy transform. Section D.4 and D.5 presents the no contraction lemma for dense Cauchy transforms. Section D.6 and D.7 contains the results for sparse Cauchy transforms and Lewis weights."
    }, {
      "heading" : "D.1 Definitions",
      "text" : "Definition D.1. Given a matrix M ∈ Rn×d, if matrix S ∈ Rm×n satisfies\n‖SM‖1 ≤ c1‖M‖1,\nthen S has at most c1-dilation on M .\nDefinition D.2. Given a matrix U ∈ Rn×k, if matrix S ∈ Rm×n satisfies\n∀x ∈ Rk, ‖SUx‖1 ≥ 1\nc2 ‖Ux‖1,\nthen S has at most c2-contraction on U .\nDefinition D.3. Given matrices U ∈ Rn×k, A ∈ Rn×d, let V ∗ = arg minV ∈Rk×d ‖UV − A‖1. If matrix S ∈ Rm×n satisfies\n∀V ∈ Rk×d, ‖SUV − SA‖1 ≥ 1 c3 ‖UV −A‖1 − c4‖UV ∗ −A‖1,\nthen S has at most (c3, c4)-contraction on (U,A).\nDefinition D.4. A (c5, c6) `1-subspace embedding for the column space of an n× k matrix U is a matrix S ∈ Rm×n for which all x ∈ Rk\n1 c5 ‖Ux‖1 ≤ ‖SUx‖1 ≤ c6‖Ux‖1.\nDefinition D.5. Given matrices U ∈ Rn×k, A ∈ Rn×d, let V ∗ = arg minV ∈Rk×d ‖UV − A‖1. Let S ∈ Rm×n. If for all c ≥ 1, and if for any V̂ ∈ Rk×d which satisfies\n‖SUV̂ − SA‖1 ≤ c · min V ∈Rk×d ‖SUV − SA‖1,\nit holds that\n‖UV̂ −A‖1 ≤ c · c7 · ‖UV ∗ −A‖1,\nthen S provides a c7-multiple-regression-cost preserving sketch of (U,A).\nDefinition D.6. Given matrices L ∈ Rn×m1 , N ∈ Rm2×d, A ∈ Rn×d, k ≥ 1, let\nX∗ = arg min rank−k X ‖LXN −A‖1.\nLet S ∈ Rm×n. If for all c ≥ 1, and if for any rank−k X̂ ∈ Rm1×m2 which satisfies\n‖SLX̂N − SA‖1 ≤ c · min rank−k X ‖SLXN − SA‖1,\nit holds that\n‖LX̂N −A‖1 ≤ c · c8 · ‖LX∗N −A‖1,\nthen S provides a c8-restricted-multiple-regression-cost preserving sketch of (L,N,A, k)."
    }, {
      "heading" : "D.2 Properties",
      "text" : "Lemma D.7. Given matrices A ∈ Rn×d, U ∈ Rn×k, let V ∗ = arg minV ∈Rk×d ‖UV − A‖1. If S ∈ Rm×n has at most c1-dilation on UV ∗ −A, i.e.,\n‖S(UV ∗ −A)‖1 ≤ c1‖UV ∗ −A‖1,\nand it has at most c2-contraction on U , i.e.,\n∀x ∈ Rk, ‖SUx‖1 ≥ 1\nc2 ‖Ux‖1,\nthen S has at most (c2, c1 + 1c2 )-contraction on (U,A), i.e.,\n∀V ∈ Rk×d, ‖SUV − SA‖1 ≥ 1\nc2 ‖UV −A‖1 − (c1 +\n1\nc2 )‖UV ∗ −A‖1,\nProof. Let A ∈ Rn×d, U ∈ Rn×k, S ∈ Rm×n be the same as that described in the lemma. Then ∀V ∈ Rk×d\n‖SUV − SA‖1 ≥ ‖SUV − SUV ∗‖1 − ‖SUV ∗ − SA‖1 ≥ ‖SUV − SUV ∗‖1 − c1‖UV ∗ −A‖1 = ‖SU(V − V ∗)‖1 − c1‖UV ∗ −A‖1\n= d∑ j=1 ‖SU(V − V ∗)j‖1 − c1‖UV ∗ −A‖1 ≥ d∑ j=1 1 c2 ‖U(V − V ∗)j‖1 − c1‖UV ∗ −A‖1\n= 1\nc2 ‖UV − UV ∗‖1 − c1‖UV ∗ −A‖1\n≥ 1 c2 ‖UV −A‖1 − 1 c2 ‖UV ∗ −A‖1 − c1‖UV ∗ −A‖1 = 1\nc2 ‖UV −A‖1 −\n( ( 1\nc2 + c1)‖UV ∗ −A‖1\n) .\nThe first inequality follows by the triangle inequality. The second inequality follows since S has at most c1 dilation on UV ∗ − A. The third inequality follows since S has at most c2 contraction on U . The fourth inequality follows by the triangle inequality.\nLemma D.8. Given matrices A ∈ Rn×d, U ∈ Rn×k, let V ∗ = arg minV ∈Rk×d ‖UV − A‖1. If S ∈ Rm×n has at most c1-dilation on UV ∗ −A, i.e.,\n‖S(UV ∗ −A)‖1 ≤ c1‖UV ∗ −A‖1,\nand has at most c2-contraction on U , i.e.,\n∀x ∈ Rk, ‖SUx‖1 ≥ 1\nc2 ‖Ux‖1,\nthen S provides a (2c1c2 + 1)-multiple-regression-cost preserving sketch of (U,A), i.e., for all c ≥ 1, for any V̂ ∈ Rk×d which satisfies\n‖SUV̂ − SA‖1 ≤ c · min V ∈Rk×d ‖SUV − SA‖1,\nit has\n‖UV̂ −A‖1 ≤ c · (2c1c2 + 1) · ‖UV ∗ −A‖1,\nProof. Let S ∈ Rm×n, A ∈ Rn×d, U ∈ Rn×k, V ∗, V̂ ∈ Rk×d, and c be the same as stated in the lemma.\n‖UV̂ −A‖1 ≤ c2‖SUV̂ − SA‖1 + (1 + c1c2)‖UV ∗ −A‖1 ≤ c2c min\nV ∈Rk×d ‖SUV − SA‖1 + (1 + c1c2)‖UV ∗ −A‖1\n≤ c2c‖SUV ∗ − SA‖1 + (1 + c1c2)‖UV ∗ −A‖1 ≤ c1c2c‖UV ∗ −A‖1 + (1 + c1c2)‖UV ∗ −A‖1 ≤ c · (1 + 2c1c2)‖UV ∗ −A‖1.\nThe first inequality follows by Lemma D.7. The second inequality follows by the guarantee of V̂ . The fourth inequality follows since S has at most c1-dilation on UV ∗ − A. The fifth inequality follows since c ≥ 1.\nLemma D.9. Given matrices L ∈ Rn×m1 , N ∈ Rm2×d, A ∈ Rn×d, k ≥ 1, let\nX∗ = arg min rank−k X ‖LXN −A‖1.\nIf S ∈ Rm×n has at most c1-dilation on LX∗N −A, i.e.,\n‖S(LX∗N −A)‖1 ≤ c1‖LX∗N −A‖1,\nand has at most c2-contraction on L, i.e.,\n∀x ∈ Rm1‖SLx‖1 ≥ ‖Lx‖1,\nthen S provides a (2c1c2 + 1)-restricted-multiple-regression-cost preserving sketch of (L,N,A, k), i.e., for all c ≥ 1, for any rank−k X̂ ∈ Rm1×m2 which satisfies\n‖SLX̂N − SA‖1 ≤ c · min rank−k X ‖SLXN − SA‖1,\nit holds that\n‖LX̂N −A‖1 ≤ c · (2c1c2 + 1) · ‖LX∗N −A‖1.\nProof. Let S ∈ Rm×n, L ∈ Rn×m1 , X̂ ∈ Rm1×m2 , X∗ ∈ Rm1×m2 , N ∈ Rm2×d, A ∈ Rn×d, and c ≥ 1 be the same as stated in the lemma.\n‖SLX̂N − SA‖1 ≥ ‖SLX̂N − SLX∗N‖1 − ‖SLX∗N − SA‖1\n≥ 1 c2 ‖L(X̂N −X∗N)‖1 − c1‖LX∗N −A‖1 ≥ 1 c2 ‖LX̂N −A‖1 − 1 c2 ‖LX∗N −A‖1 − c1‖LX∗N −A‖1 = 1\nc2 ‖LX̂N −A‖1 − (\n1 c2 + c1)‖LX∗N −A‖1.\nThe inequality follows by the triangle inequality. The second inequality follows since S has at most c2-contraction on L, and it has at most c1-dilation on LX∗N − A. The third inequality follows by the triangle inequality.\nIt follows that\n‖LX̂N −A‖1 ≤ c2‖SLX̂N − SA‖1 + (1 + c1c2)‖LX∗N −A‖1 ≤ c2c · min\nrank−k X ‖SLXN − SA‖1 + (1 + c1c2)‖LX∗N −A‖1\n≤ c2c · ‖SLX∗N − SA‖1 + (1 + c1c2)‖LX∗N −A‖1 ≤ cc1c2 · ‖LX∗N −A‖1 + (1 + c1c2)‖LX∗N −A‖1 ≤ c · (1 + 2c1c2)‖LX∗N −A‖1.\nThe first inequality directly follows from the previous one. The second inequality follows from the guarantee of X̂. The fourth inequality follows since S has at most c1 dilation on LX∗N − A. The fifth inequality follows since c ≥ 1.\nLemma D.10. Given matrices L ∈ Rn×m1 , N ∈ Rm2×d, A ∈ Rn×d, k ≥ 1, let\nX∗ = arg min rank−k X ‖LXN −A‖1.\nLet T1 ∈ Rt1×n have at most c1-dilation on LX∗N −A, and at most c2-contraction on L. Let\nX̃ = arg min rank−k X\n‖T1LXN − T1A‖1.\nLet T>2 ∈ Rt2×d have at most c′1-dilation on (T1LX̃N − T1A)>, and at most c′2-contraction on N>. Then, for all c ≥ 1, for any rank−k X̂ ∈ Rm1×m2 which satisfies\n‖T1(LX̂N − SA)T2‖1 ≤ c · min rank−k X ‖T1(LXN −A)T2‖1,\nit has\n‖LX̂N −A‖1 ≤ c · (2c1c2 + 1)(2c′1c′2 + 1) · ‖LX∗N −A‖1.\nProof. Apply Lemma D.9 for sketch matrix T2. Then for any c ≥ 1, any rank−k X̂ ∈ Rm1×m2 which satisfies\n‖T1(LX̂N −A)T2‖1 ≤ c · min rank−k X ‖T1(LXN −A)T2‖1,\nhas\n‖T1(LX̂N −A)‖1 ≤ c · (2c′1c′2 + 1) · ‖T1(LX̃N −A)‖1.\nApply Lemma D.9 for sketch matrix T1. Then for any c ≥ 1, any rank−k X̂ ∈ Rm1×m2 which satisfies\n‖T1(LX̂N −A)‖1 ≤ c(2c′1c′2 + 1) · min rank−k X ‖T1(LX̃N −A)‖1,\nhas\n‖LX̂N −A‖1 ≤ c · (2c1c2 + 1)(2c′1c′2 + 1) · ‖LX∗N −A‖1.\nLemma D.11. Given matrices M ∈ Rn×d, U ∈ Rn×t, d ≥ t = rank(U), n ≥ d ≥ r = rank(M), if sketching matrix S ∈ Rm×n is drawn from any of the following probability distributions of matrices, with .99 probability S has at most c1-dilation on M , i.e.,\n‖SM‖1 ≤ c1‖M‖1,\nand S has at most c2-contraction on U , i.e.,\n∀x ∈ Rt, ‖SUx‖1 ≥ 1\nc2 ‖Ux‖1,\nwhere c1, c2 are parameters depend on the distribution over S.\n(I) S ∈ Rm×n is a dense Cauchy matrix: a matrix with i.i.d. entries from the standard Cauchy distribution. If m = O(t log t), then c1c2 = O(log d). If m = O((t + r) log(t + r)), then c1c2 = O(min(log d, r log r)).\n(II) S ∈ Rm×n is a sparse Cauchy matrix: S = TD, where T ∈ Rm×n has each column i.i.d. from the uniform distribution on standard basis vectors of Rm, and D ∈ Rn×n is a diagonal matrix with i.i.d. diagonal entries following a standard Cauchy distribution. If m = O(t5 log5 t), then c1c2 = O(t2 log2 t log d). If m = O((t + r)5 log5(t + r)), then c1c2 = O(min(t2 log2 t log d, r3 log3 r)).\n(III) S ∈ Rm×n is a sampling and rescaling matrix (notation S ∈ Rn×n denotes a diagonal sampling and rescaling matrix with m non-zero entries): If S samples and reweights m = O(t log t) rows of U , selecting each with probability proportional to the ith row’s `1 Lewis weight and reweighting by the inverse probability, then c1c2 = O(1).\n(IV) S ∈ Rm×n is a dense Cauchy matrix with limited independence: S is a matrix with each entry drawn from a standard Cauchy distribution. Entries from different rows of S are fully independent, and entries from the same row of S are W -wise independent. If m = O(t log t), and W = Õ(d), then c1c2 = O(t log d). If m = O(t log t), and W = Õ(td), then c1c2 = O(log d).\nIn the above, if we replace S with σ · S where σ ∈ R\\{0} is any scalar, then the relation between m and c1c2 can be preserved.\nFor (I), if m = O(t log t), then c1c2 = O(log d) is implied by Lemma D.23 and Lemma D.13. If m = O((t+ r) log(t+ r)), c1c2 = O(r log r) is implied by [SW11].\nFor (II), if m = O(t5 log5 t), then c1c2 = O(t2 log2 t log d) is implied by Corollary D.27 and Lemma D.25. If m = O((t+ r)5 log5(t+ r)), c1c2 = O(r3 log3 r) is implied by [MM13].\nFor (III), it is implied by [CP15] and Lemma D.29. For (IV), it is implied by Lemma I.4, Corollary I.5."
    }, {
      "heading" : "D.3 Cauchy embeddings, no dilation",
      "text" : "Lemma D.12. Define U∗ ∈ Rn×k, V ∗ ∈ Rk×d to be the optimal solution of min U∈Rn×k,V ∈Rk×d ‖UV − A‖1. Choose a Cauchy matrix S with m rows and rescaled by Θ(1/m). We have that\n‖SU∗V ∗ − SA‖1 ≤ O(log d)‖U∗V ∗ −A‖1\nholds with probability at least 99/100.\nProof. The proof technique has been used in [Ind06] and [CDMI+13]. Fix the optimal U∗ and V ∗, then\n‖SU∗V ∗ − SA‖1 = d∑ i=1 ‖S(U∗V ∗i −Ai)‖1\n= d∑ i=1 m∑ j=1 | n∑ l=1 1 m Sj,l · (U∗V ∗i −Ai)l| where Sj,l ∼ C(0, 1)\n= d∑ i=1 m∑ j=1 1 m |ci,j | where ci,j ∼ C(0, ‖U∗V ∗i −Ai‖1)\n= 1\nm d∑ i=1 m∑ j=1 ‖U∗V ∗i −Ai‖1 · wi+d(j−1). where wi+d(j−1) ∼ |C(0, 1)| (17)\nwhere the last step follows since each wi can be thought of as a clipped half-Cauchy random variable. Define d′ = md. Define event ξi to be the situation in which wi < D (we will decide upon D later), and define event ξ = ξ1 ∩ ξ2 ∩ · · · ∩ ξd′ . Then it is clear that ξ ∩ ξi = ξ, ∀i ∈ [d′].\nUsing the probability density function (pdf) of a Cauchy and because tan−1 x ≤ x, we can lower bound Pr[event ξi holds] in the following sense,\nPr[ξi] = 2 π tan−1(D) = 1− 2 π tan−1(1/D) ≥ 1− 2 πD .\nBy a union bound over all i ∈ [d′], we can lower bound Pr[event ξ holds],\nPr[ξ] ≥ 1− d′∑ i=1 Pr[ξi] ≥ 1− 2d′ πD . (18)"
    }, {
      "heading" : "By Bayes rule and ξ = ξ ∩ ξi, Pr[ξ|ξi] Pr[ξi] = Pr[ξ ∩ ξi] = Pr[ξ], which implies that Pr[ξ|ξi] =",
      "text" : "Pr[ξ]/Pr[ξi]. First, we can lower bound E[wi|ξi],\nE[wi|ξi] = E[wi|ξi ∩ ξ] Pr[ξ|ξi] + E[wi|ξ ∩ ξ] Pr[ξ|ξi] ≥ E[wi|ξi ∩ ξ] Pr[ξ|ξi] by wi ≥ 0 and Pr[] ≥ 0 = E[wi|ξ] Pr[ξ|ξi] by ξ = ξ ∩ ξi.\nThe above equation implies that\nE[wi|ξ] ≤ E[wi|ξi] Pr[ξ|ξi]\n= E[wi|ξi] Pr[ξi]\nPr[ξ ∩ ξi] by Bayes rule Pr[ξ|ξi] Pr[ξi] = Pr[ξ ∩ ξi]\n= E[wi|ξi] Pr[ξi]\nPr[ξ] by ξ = ξ ∩ ξi.\nUsing the pdf of a Cauchy, E[wi|ξi] = 1π log(1 +D 2)/Pr[ξi] and plugging it into the lower bound\nof E[wi|ξ],\nE[wi|ξ] ≤ E[wi|ξi] Pr[ξi]\nPr[ξ] =\n1 π log(1 +D 2)\nPr[ξ] ≤\n1 π log(1 +D 2)\n1− 2dπD . log(D),\nwhere the third step follows since Pr[ξ] ≥ 1− 2d′πD and the last step follows by choosing D = Θ(d ′).\nWe can conclude\nE[‖SU∗V ∗ − SA‖1|ξ] = 1\nm d∑ i=1 m∑ j=1 ‖U∗V ∗i −Ai‖1 ·E[wi+d(j−1)|ξ] . (log d′) · ‖U∗V ∗ −A‖1. (19)\nFor simplicity, define X = ‖SU∗V ∗ − SA‖1 and γ = ‖U∗V ∗ − A‖1. By Markov’s inequality and because Pr[X ≥ γt|ξ] ≤ 1, we have\nPr[X ≥ γt] = Pr[X ≥ γt|ξ] Pr[ξ] + Pr[X ≥ γt|ξ] Pr[ξ] ≤ Pr[X ≥ γt|ξ] + Pr[ξ]\n≤ E[X|ξ] γt + Pr[ξ] by Markov’s inequality ≤ E[X|ξ] γt + 2d′ πD by Equation (18) . log d′\nt +\n2d′ πD by Equation (19)\n≤ .01,\nwhere choosing t = Θ(log d′) and D = Θ(d′). Since k ≤ d and m = poly(k), we have t = Θ(log d), which completes the proof.\nLemma D.13. Given any matrix M ∈ Rn×d, if matrix S ∈ Rm×n has each entry drawn from an i.i.d. standard Cauchy distribution and is rescalsed by Θ(1/m), then\n‖SM‖1 ≤ O(log d)‖M‖1\nholds with probability at least 99/100.\nProof. Just replace the matrix U∗V ∗ − A in the proof of Lemma D.12 with M . Then we can get the result directly."
    }, {
      "heading" : "D.4 Cauchy embeddings, no contraction",
      "text" : "We prove that if we choose a Cauchy matrix S, then for a fixed optimal solution U∗ of minU,V ‖UV − A‖1, and for all V , we have that with high probability ‖SU∗V −SA‖1 is lower bouned by ‖U∗V −A‖1 up to some constant.\nLemma D.14. Define U∗ ∈ Rn×k, V ∗ ∈ Rk×d to be the optimal solution of min U∈Rn×kV ∈Rk×d ‖UV−A‖1, where A ∈ Rn×d. Let m = O(k log k), S ∈ Rm×n be a random matrix with each entry an i.i.d. standard Cauchy random variable, scaled by Θ(1/m). Then with probability at least 0.95,\n∀V ∈ Rk×d, ‖U∗V −A‖1 . ‖SU∗V − SA‖1 +O(log(d))‖U∗V ∗ −A‖1.\nProof. This follows by Lemmas D.12, D.23, D.7.\nD.5 Cauchy embeddings, k-dimensional subspace\nThe goal of this section is to prove Lemma D.23. Before getting into the details, we first give the formal definition of an (α, β) `1 well-conditioned basis and -net.\nDefinition D.15 (`1 Well-conditioned basis). [DDH+09] A basis U for the range of A is (α, β)condtioned if ‖U‖1 ≤ α and for all x ∈ Rk, ‖x‖∞ ≤ β‖Ux‖1. We will say U is well-conditioned if α and β are low-degree polynomials in k, independent of n.\nNote that a well-conditoned basis implies the following result.\nFact D.16. There exist α, β ≥ 1 such that,\n∀x ∈ Rk, 1 kβ ‖x‖1 ≤ ‖Ux‖1 ≤ α‖x‖1.\nProof. The lower bound can be proved in the following sense,\n‖Ux‖1 ≥ 1\nβ ‖x‖∞ ≥\n1\nβ\n1 k ‖x‖1,\nwhere the first step follows by the properties of a well-conditioned basis, and the second step follows since k‖x‖∞ ≥ ‖x‖1. Then we can show an upper bound,\n‖Ux‖1 ≤ ‖U‖1 · ‖x‖1 ≤ α‖x‖1,\nwhere the first step follows by ‖Ux‖1 ≤ ‖U‖1‖x‖1, and the second step follows using ‖U‖1 ≤ α.\nDefinition D.17 ( -net). Define N to an -net where, for all the x ∈ Rk that ‖x‖1 = 1, for any vectors two x, x′ ∈ N , ‖x − x′‖1 ≥ , for any vector y /∈ N , there exists an x ∈ N such that ‖x− y‖1 ≤ . Then the size of N is (1 ) O(k) = 2O(k log(1/ )).\nLemma D.18 (Lemma 6 in [SW11]). There is a constant c0 > 0 such that for any t ≥ 1 and any constant c > c0, if S is a t × n matrix whose entries are i.i.d. standard Cauchy random variables scaled by c/t, then, for any fixed y ∈ Rn,\nPr[‖Sy‖1 < ‖y‖1] ≤ 1/2t\nLemma D.19. Suppose we are given a well-conditioned basis U ∈ Rn×k. If S is a t × n matrix whose entries are i.i.d. standard Cauchy random variables scaled by Θ(1/t), then with probability 1− 2−Ω(t), for all vectors x ∈ N we have that ‖SUx‖1 ≥ ‖Ux‖1.\nProof. First, using Lemma D.18, we have for any fixed vector y ∈ Rn, Pr[‖Sy‖ < ‖y‖1] ≤ 1/2t. Second, we can rewrite y = Ux. Then for any fixed x ∈ N , Pr[‖SUx‖ < ‖Ux‖1] ≤ 1/2t. Third, choosing t & k log(1/ ) and taking a union bound over all the vectors in the -net N completes the proof.\nLemma D.20 (Lemma 7 in [SW11]). Let S be a t×n matrix whose entries are i.i.d standard Cauchy random variables, scaled by c/t for a constant c, and t ≥ 1. Then there is a constant c′ = c′(c) > 0 such that for any fixed set of {y1, y2, · · · , yk} of d vectors in {y ∈ Rn : x ∈ Rk, Ux = y},\nPr[ k∑ i=1 ‖Syi‖1 ≥ c′ log(tk) k∑ i=1 ‖yi‖1] ≤ 1 1000 .\nUsing Lemma D.20 and the definition of an (α, β) well-conditioned basis, we can show the following corollary.\nCorollary D.21. Suppose we are given an (α, β) `1 well-conditioned basis U ∈ Rn×k. Choose S to be an i.i.d. Cauchy matrix with t = O(k log k) rows, and rescale each entry by Θ(1/t). Then with probability 99/100, for all vectors x ∈ Rk,\n‖SUx‖1 ≤ O(αβ log(k)) · ‖Ux‖1,\nProof. Define event E to be the situation when k∑ i=1 ‖SUi‖1 < c′ log(tk) k∑ i=1 ‖Ui‖1\nholds, and U is a well-conditioned basis. Using Lemma D.20, we can show that event E holds with probability 999/1000. We condition on Event E holding. Then for any y = Ux for an x ∈ Rk, we have\n‖Sy‖1 = ‖SUx‖1\n≤ k∑ j=1 ‖SUjxj‖1 by triangle inequality = k∑ j=1 |xj | · ‖SUj‖1 ≤ ‖x‖∞c′ log(tk) k∑ j=1 ‖Uj‖1 by Lemma D.20 = ‖x‖∞c′ log(tk)α by ‖U‖1 = k∑ j=1 ‖Uj‖1 ≤ α ≤ β‖Ux‖1c′ log(tk)α by ‖x‖∞ ≤ β‖Ux‖1 ≤ β‖y‖1c′ log(tk)α by Ux = y.\nThis completes the proof.\nLemma D.22. Given an (α, β) `1 well-conditioned basis, condition on the following two events, 1. For all x ∈ N , ‖SUx‖1 ≥ ‖Ux‖1. (Lemma D.18) 2. For all x ∈ Rk, ‖SUx‖1 ≤ O(αβ log k)‖Ux‖1. (Corollary D.21) Then, for all w ∈ Rk, ‖SUw‖1 & ‖Uw‖1.\nProof. For any w ∈ Rk we can write it as w = ` · z where ` is some scalar and z has ‖z‖1 = 1. Define y = arg min\ny′∈N ‖y′ − z‖1.\nWe first show that if U is an (α, β) well-conditioned basis for `1, then ‖U(y−z)‖1 ≤ αβk ‖Uy‖1,\n‖U(y − z)‖1 ≤ α‖y − z‖1 by ‖U(y − z)‖1 ≤ α‖y − z‖1 ≤ α by ‖y − z‖1 ≤ = α ‖y‖1 by ‖y‖1 = 1\n≤ α ‖Uy‖1βk by ‖Uy‖1 ≥ 1\nβk ‖y‖1.\nBecause < 1/(αβkc+1), we have\n‖U(y − z)‖1 ≤ 1\nkc ‖Uy‖1. (20)\nUsing the triangle inequality, we can lower bound ‖Uy‖1 by ‖Uz‖1 up to some constant,\n‖Uy‖1 ≥ ‖Uz‖1 − ‖U(y − z)‖1 ≥ ‖Uz‖1 − 1\nkc ‖Uy‖1, (21)\nwhich implies ‖Uy‖1 ≥ .99‖Uz‖1. (22)\nThus,\n‖SUz‖1 ≥ ‖SUy‖1 − ‖SU(z − y)‖1 by triangle inequality ≥ ‖Uy‖1 − ‖SU(z − y)‖1 by Lemma D.18 ≥ ‖Uy‖1 − αβ log(k) · ‖U(z − y)‖1 by Corollary D.21\n≥ ‖Uy‖1 − αβ log(k) · 1\nkc ‖Uy‖1 by Equation (20)\n& ‖Uy‖1 by kc & αβ log(k) & ‖Uz‖1, by Equation (22)\nby rescaling z to w, we complete the proof.\nLemma D.23. Given matrix U ∈ Rn×k, let t = O(k log k), and let S ∈ Rt×n be a random matrix with entries drawn i.i.d. from a standard Cauchy distribution, where each entry is rescaled by Θ(1/t). With probability .99,\n∀x ∈ Rk, ‖SUx‖1 & ‖Ux‖1.\nProof. We can compute a well-conditioned basis for U , and denote it U ′. Then, ∀x ∈ Rk, there exists y ∈ Rk such that Ux = U ′y. Due to Lemma D.22, with probability .99, we have\n∀y ∈ Rk, ‖SU ′y‖1 & ‖U ′y‖1."
    }, {
      "heading" : "D.6 Sparse Cauchy transform",
      "text" : "This section presents the proof of two lemmas related to the sparse Cauchy transform. We first prove the no dilation result in Lemma D.24. Then we show how to get the no contraction result in Lemma D.26.\nLemma D.24. Given matrix A ∈ Rn×d, let U∗, V ∗ be the optimal solutions of minU,V ‖UV −A‖1. Let Π = σ · SC ∈ Rm×n, where S ∈ Rm×n has each column vector chosen independently and uniformly from the m standard basis vectors of Rm, where C is a diagonal matrix with diagonals chosen independently from the standard Cauchy distribution, and σ is a scalar. Then\n‖ΠU∗V ∗ −ΠA‖1 . σ · log(md) · ‖U∗V ∗ −A‖1\nholds with probability at least .999.\nProof. We define Π = σ · SC ∈ Rm×n, as in the statement of the lemma. Then,\n‖Π(U∗V ∗ −A)‖1\n= d∑ i=1 ‖SD(U∗V ∗i −Ai)‖1 = d∑ i=1 ∥∥∥∥  S11 S12 · · · S1n S21 S22 · · · S2n · · · · · · · · · · · · Sm1 Sm2 · · · Smn  ·  c1 0 0 0 0 c2 0 0 0 0 · · · 0 0 0 0 cn  · (U∗V ∗i −Ai)∥∥∥∥ 1\n= d∑ i=1 ∥∥∥∥  c1S11 c2S12 · · · cnS1n c1S21 c2S22 · · · cnS2n · · · · · · · · · · · · c1Sm1 c2Sm2 · · · cnSmn  · (U∗V ∗i −Ai)∥∥∥∥ 1\n= d∑ i=1 ∥∥∥∥ n∑ l=1 clS1l · (U∗V ∗i −Ai)l, n∑ l=1 clS2l · (U∗V ∗i −Ai)l, · · · , n∑ l=1 clSml · (U∗V ∗i −Ai)l ∥∥∥∥ 1\n= d∑ i=1 m∑ j=1 | n∑ l=1 clSjl · (U∗V ∗i −Ai)l|\n= d∑ i=1 m∑ j=1 |w̃ij · n∑ l=1 |Sjl(U∗V ∗i −Ai)l|| where w̃ij ∼ C(0, 1)\n= d∑ i=1 m∑ j=1 n∑ l=1 |Sjl(U∗V ∗i −Ai)l| · |w̃ij |\n= d∑ i=1 m∑ j=1 n∑ l=1 |Sjl(U∗V ∗i −Ai)l| · wi+(j−1)d where wi+(j−1)d ∼ |C(0, 1)|,\nwhere the last step follows since each wi can be thought of as a clipped half-Cauchy random variable. Define d′ = md. Define event ξi to be the situation when wi < D (we will decide upon D later). Define event ξ = ξ1 ∩ ξ2 ∩ · · · ∩ ξd′ . By choosing D = Θ(d′), we can conclude that,\nE[‖ΠU∗V ∗ −ΠA‖1|ξ] = d∑ i=1 m∑ j=1 n∑ l=1 |Sjl(U∗V ∗i −Ai)l| ·E[wi+(j−1)d|ξ]\n. d∑ i=1 m∑ j=1 n∑ l=1 |Sjl(U∗V ∗i −Ai)l| · log(d′)\n= d∑ i=1 n∑ l=1 |(U∗V ∗i −Ai)l|1 · log(d′) by m∑ j=1 |Sjl| = 1, ∀l ∈ [n] = log(d′) · ‖U∗V ∗ −A‖1.\nThus, we can show that\nPr[‖ΠU∗V ∗ −ΠA‖1 . log(d′)‖U∗V ∗ −A‖1] ≥ 0.999.\nLemma D.25. Given any matrix M ∈ Rn×d. Let Π = σ · SC ∈ Rm×n, where S ∈ Rm×n has each column vector chosen independently and uniformly from the m standard basis vectors of Rm, where C is a diagonal matrix with diagonals chosen independently from the standard Cauchy distribution, and σ is a scalar. Then\n‖ΠM‖1 . σ · log(md) · ‖M‖1\nholds with probability at least .999.\nProof. Just replace the matrix U∗V ∗ − A in the proof of Lemma D.24 with M . Then we can get the result directly.\nWe already provided the proof of Lemma D.24. It remains to prove Lemma D.26.\nLemma D.26. Given matrix A ∈ Rn×d, let U∗, V ∗ be the optimal solution of minU,V ‖UV −A‖1. Let Π = σ · SC ∈ Rm×n, where S ∈ Rm×n has each column vector chosen independently and uniformly from the m standard basis vectors of Rm, and where C is a diagonal matrix with diagonals chosen independently from the standard Cauchy distribution. Then with probability at least .999, for all V ∈ Rk×d,\n‖ΠU∗V −ΠA‖1 ≥ ‖U∗V −A‖1 −O(σ · log(md))‖U∗V ∗ −A‖1.\nNotice that m = O(k5 log5 k) and σ = O(k2 log2 k) according to Theorem 2 in [MM13].\nWe start by using Theorem 2 in [MM13] to generate the following Corollary.\nCorollary D.27. Given U ∈ Rn×k with full column rank, let Π = σ ·SC ∈ Rm×n where S ∈ Rm×n has each column chosen independently and uniformly from the m standard basis vectors of Rm, and where C ∈ Rn×n is a diagonal matrix with diagonals chosen independently from the standard Cauchy distribution. Then with probability .999, for all x ∈ Rk, we have\n‖ΠUx‖1 ≥ ‖Ux‖1.\nNotice that m = O(k5 log5 k) and σ = O(k2 log2 k) according to Theorem 2 in [MM13].\nWe give the proof of Lemma D.26.\nProof. Follows by Corollary D.27, Lemma D.24, Lemma D.7.\nD.7 `1-Lewis weights\nIn this section we show how to use Lewis weights to get a better dilation bound. The goal of this section is to prove Lemma D.32 and Lemma D.31. Notice that our algorithms in Section C use Lewis weights in several different ways. The first way is using Lewis weights to show an existence result, which means we only need an existential result for Lewis weights. The second way is only guessing the nonzero locations on the diagonal of a sampling and rescaling matrix according to the Lewis weights. The third way is guessing the values on the diagonal of a sampling and rescaling matrix according to the Lewis weights. The fourth way is computing the Lewis weights for a known low dimensional matrix(n×poly(k) or poly(k)×d). We usually do not need to optimize the running time of computing Lewis weights for a low-rank matrix to have input-sparsity time.\nClaim D.28. Given matrix A ∈ Rn×d, let B = U∗V ∗−A. For any distribution p = (p1, p2, . . . , pn) define random variable X such that X = ‖Bi‖1/pi with probability pi. Then take any m independent samples X1, X2, . . . , Xm, let Y = 1m ∑m j=1X j. We have\nPr[Y ≤ 1000‖B‖1] ≥ .999\nProof. We can compute the expectation of Xj , for any j ∈ [m]\nE[Xj ] = n∑ i=1 ‖Bi‖1 pi · pi = ‖B‖1.\nThen, E[Y ] = 1m ∑m j=1 E[X j ] = ‖B‖1. Using Markov’s inequality, we have\nPr[Y ≥ 1000‖B‖1] ≤ .001\nLemma D.29. Given matrix M ∈ Rn×d, let S ∈ Rn×n be any sampling and rescaling diagonal matrix. Then with probability at least .999,\n‖SM‖1 . ‖M‖1.\nProof. Just replace the matrix B in the proof of Claim D.28 with M . Then we can get the result directly.\nUsing Theorem 1.1 of [CP15], we have the following result,\nClaim D.30. Given matrix A ∈ Rn×d, for any fixed U∗ ∈ Rn×k and V ∗ ∈ Rk×d, choose D ∈ Rn×n to be the sampling and rescaling diagonal matrix with m = O(k log k) nonzeros according to the Lewis weights of U∗. Then with probability .999, for all V ,\n‖U∗V ∗ − U∗V ‖1 ≤ ‖DU∗V ∗ −DU∗V ‖1 . ‖U∗V ∗ − U∗V ‖1.\nLemma D.31. Given matrix A ∈ Rn×d, U∗ ∈ Rn×k, define V ∗ ∈ Rk×d to be the optimal solution of min\nV ∈Rk×d ‖U∗V −A‖1. Choose a sampling and rescaling diagonal matrix D ∈ Rn×n with m = O(k log k) non-zero entries according to the Lewis weights of U∗. Then with probability at least .99, we have: for all V ∈ Rk×d,\n‖DU∗V −DA‖1 . ‖U∗V ∗ − U∗V ‖1 +O(1)‖U∗V ∗ −A‖1 . ‖U∗V −A‖1,\nholds with probability at least .99.\nProof. Using the above two claims, we have with probability at least .99, for all V ∈ Rk×d,\n‖DU∗V −DA‖1 ≤ ‖DU∗V −DU∗V ∗‖1 + ‖DU∗V ∗ −DA‖1 by triangle inequality . ‖DU∗V −DU∗V ∗‖1 +O(1)‖U∗V ∗ −A‖1 by Claim D.28 . ‖U∗V − U∗V ∗‖1 +O(1)‖U∗V ∗ −A‖1 by Claim D.30 ≤ ‖U∗V −A‖1 + ‖U∗V ∗ −A‖1 +O(1)‖U∗V ∗ −A‖1 by triangle inequality . ‖U∗V −A‖1.\nLemma D.32. Given matrix A ∈ Rn×d, define U∗ ∈ Rn×k, V ∗ ∈ Rk×d to be the optimal solution of min\nU∈Rn×k,V ∈Rk×d ‖UV − A‖1. Choose a sampling and rescaling diagonal matrix D ∈ Rn×n with\nm = O(k log k) non-zero entries according to the Lewis weights of U∗. For all V ∈ Rk×d we have\n‖U∗V −A‖1 . ‖DU∗V −DA‖1 +O(1)‖U∗V ∗ −A‖1,\nholds with probability at least .99.\nProof. Follows by Claim D.28, Lemma D.31, Lemma D.7.\nE `p-Low Rank Approximation\nThis section presents some fundamental lemmas for `p-low rank approximation problems. Using these lemmas, all the algorithms described for `1-low rank approximation problems can be extended to `p-low rank approximation directly. We only state the important Lemmas in this section, due to most of the proofs in this section being identical to the proofs in Section D."
    }, {
      "heading" : "E.1 Definitions",
      "text" : "This section is just a generalization of Section D.1 to the `p setting when 1 < p < 2.\nDefinition E.1. Given a matrix M ∈ Rn×d, if matrix S ∈ Rm×n satisfies\n‖SM‖pp ≤ c1‖M‖pp,\nthen S has at most c1-dilation on M .\nDefinition E.2. Given a matrix U ∈ Rn×k, if matrix S ∈ Rm×n satisfies\n∀x ∈ Rk, ‖SUx‖pp ≥ 1\nc2 ‖Ux‖pp,\nthen S has at most c2-contraction on U .\nDefinition E.3. Given matrices U ∈ Rn×k, A ∈ Rn×d, denote V ∗ = arg minV ∈Rk×d ‖UV −A‖ p p. If matrix S ∈ Rm×n satisfies\n∀V ∈ Rk×d, ‖SUV − SA‖pp ≥ 1 c3 ‖UV −A‖pp − c4‖UV ∗ −A‖pp,\nthen S has at most (c3, c4)-contraction on (U,A).\nDefinition E.4. A (c5, c6) `p-subspace embedding for the column space of an n × k matrix U is a matrix S ∈ Rm×n for which all x ∈ Rk\n1 c5 ‖Ux‖pp ≤ ‖SUx‖pp ≤ c6‖Ux‖pp.\nDefinition E.5. Given matrices U ∈ Rn×k, A ∈ Rn×d, denote V ∗ = arg minV ∈Rk×d ‖UV − A‖ p p. Let S ∈ Rm×n. If for all c ≥ 1, and if for any V̂ ∈ Rk×d which satisfies\n‖SUV̂ − SA‖pp ≤ c · min V ∈Rk×d ‖SUV − SA‖pp,\nit holds that\n‖UV̂ −A‖pp ≤ c · c7 · ‖UV ∗ −A‖pp,\nthen S provides a c7-multiple-regression-cost preserving sketch of (U,A).\nDefinition E.6. Given matrices L ∈ Rn×m1 , N ∈ Rm2×d, A ∈ Rn×d, k ≥ 1, let\nX∗ = arg min rank−k X ‖LXN −A‖pp.\nLet S ∈ Rm×n. If for all c ≥ 1, and if for any rank−k X̂ ∈ Rm1×m2 which satisfies\n‖SLX̂N − SA‖pp ≤ c · min rank−k X ‖SLXN − SA‖pp,\nit holds that\n‖LX̂N −A‖pp ≤ c · c8 · ‖LX∗N −A‖pp,\nthen S provides a c8-restricted-multiple-regression-cost preserving sketch of (L,N,A, k)."
    }, {
      "heading" : "E.2 Properties",
      "text" : "Lemma E.7. Given matrices A ∈ Rn×d, U ∈ Rn×k, let V ∗ = arg minV ∈Rk×d ‖UV − A‖ p p. If S ∈ Rm×n has at most c1-dilation on UV ∗ −A, i.e.,\n‖S(UV ∗ −A)‖pp ≤ c1‖UV ∗ −A‖pp,\nand it has at most c2-contraction on U , i.e.,\n∀x ∈ Rk, ‖SUx‖pp ≥ 1\nc2 ‖Ux‖pp,\nthen S has at most (22p−2c2, c1 + 21−p 1c2 )-contraction on (U,A), i.e.,\n∀V ∈ Rk×d, ‖SUV − SA‖pp ≥ 1\nc2 ‖UV −A‖pp − (c1 +\n1 c2 )‖UV ∗ −A‖pp,\nProof. Let A ∈ Rn×d, U ∈ Rn×k, and S ∈ Rm×n be the same as that described in the lemma. Then\n∀V ∈ Rk×d\n‖SUV − SA‖pp ≥ 21−p‖SUV − SUV ∗‖pp − ‖SUV ∗ − SA‖pp ≥ 21−p‖SUV − SUV ∗‖pp − c1‖UV ∗ −A‖pp = 21−p‖SU(V − V ∗)‖pp − c1‖UV ∗ −A‖pp\n= 21−p d∑ j=1 ‖SU(V − V ∗)j‖pp − c1‖UV ∗ −A‖pp ≥ 21−p d∑ j=1 1 c2 ‖U(V − V ∗)j‖pp − c1‖UV ∗ −A‖pp\n= 21−p 1\nc2 ‖UV − UV ∗‖pp − c1‖UV ∗ −A‖pp\n≥ 22−2p 1 c2 ‖UV −A‖pp − 21−p 1 c2 ‖UV ∗ −A‖pp − c1‖UV ∗ −A‖pp = 22−2p 1\nc2 ‖UV −A‖pp −\n( (21−p 1\nc2 + c1)‖UV ∗ −A‖pp\n) .\nThe first inequality follows by Fact E.14. The second inequality follows since S has at most c1 dilation on UV ∗ − A. The third inequality follows since S has at most c2 contraction on U . The fourth inequality follows by Fact E.14.\nLemma E.8. Given matrices A ∈ Rn×d, U ∈ Rn×k, let V ∗ = arg minV ∈Rk×d ‖UV − A‖ p p. If S ∈ Rm×n has at most c1-dilation on UV ∗ −A, i.e.,\n‖S(UV ∗ −A)‖pp ≤ c1‖UV ∗ −A‖pp,\nand has at most c2-contraction on U , i.e.,\n∀x ∈ Rk, ‖SUx‖pp ≥ 1\nc2 ‖Ux‖pp,\nthen S provides a 2p−1(2c1c2 + 1)-multiple-regression-cost preserving sketch of (U,A), i.e., for all c ≥ 1, for any V̂ ∈ Rk×d which satisfies\n‖SUV̂ − SA‖pp ≤ c · min V ∈Rk×d ‖SUV − SA‖pp,\nit has\n‖UV̂ −A‖pp ≤ c · 2p−1(2c1c2 + 1) · ‖UV ∗ −A‖pp,\nProof. Let S ∈ Rm×n, A ∈ Rn×d, U ∈ Rn×k, V ∗, V̂ ∈ Rk×d, and c be the same as stated in the lemma.\n‖UV̂ −A‖pp ≤ 22p−2c2‖SUV̂ − SA‖pp + (2p−1 + 22p−2c1c2)‖UV ∗ −A‖pp ≤ 22p−2c2c min\nV ∈Rk×d ‖SUV − SA‖pp + (2p−1 + 22p−2c1c2)‖UV ∗ −A‖pp\n≤ 22p−2c2c‖SUV ∗ − SA‖pp + (2p−1 + 22p−2c1c2)‖UV ∗ −A‖pp ≤ 22p−2c1c2c‖UV ∗ −A‖pp + (2p−1 + 22p−2c1c2)‖UV ∗ −A‖pp ≤ c · 2p−1(1 + 2c1c2)‖UV ∗ −A‖pp.\nThe first inequality follows by Lemma E.7. The second inequality follows by the guarantee of V̂ . The fourth inequality follows since S has at most c1-dilation on UV ∗ − A. The fifth inequality follows since c ≥ 1.\nLemma E.9. Given matrices L ∈ Rn×m1 , N ∈ Rm2×d, A ∈ Rn×d, k ≥ 1, let\nX∗ = arg min rank−k X ‖LXN −A‖pp.\nIf S ∈ Rm×n has at most c1-dilation on LX∗N −A, i.e.,\n‖S(LX∗N −A)‖pp ≤ c1‖LX∗N −A‖pp,\nand has at most c2-contraction on L, i.e.,\n∀x ∈ Rm1‖SLx‖pp ≥ ‖Lx‖pp,\nthen S provides a 2p−1(2c1c2+1)-restricted-multiple-regression-cost preserving sketch of (L,N,A, k), i.e., for all c ≥ 1, for any rank−k X̂ ∈ Rm1×m2 which satisfies\n‖SLX̂N − SA‖pp ≤ c · min rank−k X ‖SLXN − SA‖pp,\nit has\n‖LX̂N −A‖pp ≤ c · 2p−1(2c1c2 + 1) · ‖LX∗N −A‖pp.\nProof. Let S ∈ Rm×n, L ∈ Rn×m1 , X̂ ∈ Rm1×m2 , X∗ ∈ Rm1×m2 , N ∈ Rm2×d, A ∈ Rn×d and c ≥ 1 be the same as stated in the lemma.\n‖SLX̂N − SA‖pp ≥ 21−p‖SLX̂N − SLX∗N‖pp − ‖SLX∗N − SA‖pp\n≥ 21−p 1 c2 ‖L(X̂N −X∗N)‖pp − c1‖LX∗N −A‖pp ≥ 22−2p 1 c2 ‖LX̂N −A‖pp − 21−p 1 c2 ‖LX∗N −A‖1 − c1‖LX∗N −A‖pp = 22−2p 1\nc2 ‖LX̂N −A‖pp − (21−p\n1\nc2 + c1)‖LX∗N −A‖pp.\nThe inequality follows from the Fact E.14. The second inequality follows since S has at most c2contraction on L, and it has at most c1-dilation on LX∗N − A. The third inequality follows by Fact E.14.\nIt follows that\n‖LX̂N −A‖pp ≤ 22p−2c2‖SLX̂N − SA‖pp + (2p−1 + 22p−2c1c2)‖LX∗N −A‖pp ≤ 22p−2c2c · min\nrank−k X ‖SLXN − SA‖pp + (2p−1 + 22p−2c1c2)‖LX∗N −A‖pp\n≤ 22p−2c2c · ‖SLX∗N − SA‖pp + (2p−1 + 22p−2c1c2)‖LX∗N −A‖pp ≤ 22p−2cc1c2 · ‖LX∗N −A‖pp + (2p−1 + 22p−2c1c2)‖LX∗N −A‖pp ≤ c · 2p−1(1 + 2c1c2)‖LX∗N −A‖pp.\nThe first inequality directly follows from the previous one. The second inequality follows from the guarantee of X̂. The fourth inequality follows since S has at most c1 dilation on LX∗N − A. The fifth inequality follows since c ≥ 1.\nLemma E.10. Given matrices L ∈ Rn×m1 , N ∈ Rm2×d, A ∈ Rn×d, k ≥ 1, let\nX∗ = arg min rank−k X ‖LXN −A‖pp.\nLet T1 ∈ Rt1×n have at most c1-dilation on LX∗N −A, and have at most c2-contraction on L. Let\nX̃ = arg min rank−k X\n‖T1LXN − T1A‖pp.\nLet T>2 ∈ Rt2×d have at most c′1-dilation on (T1LX̃N − T1A)>, and at most c′2-contraction on N>. Then, for all c ≥ 1, for any rank−k X̂ ∈ Rm1×m2 which satisfies\n‖T1(LX̂N − SA)T2‖pp ≤ c · min rank−k X ‖T1(LXN −A)T2‖pp,\nit holds that\n‖LX̂N −A‖pp ≤ c · 22p−2(2c1c2 + 1)(2c′1c′2 + 1) · ‖LX∗N −A‖pp.\nProof. Apply Lemma D.9 for sketching matrix T2. Then for any c ≥ 1, any rank−k X̂ ∈ Rm1×m2 which satisfies\n‖T1(LX̂N −A)T2‖pp ≤ c · min rank−k X ‖T1(LXN −A)T2‖pp,\nit has\n‖T1(LX̂N −A)‖pp ≤ c · 2p−1(2c′1c′2 + 1) · ‖T1(LX̃N −A)‖pp.\nApply Lemma D.9 for sketch matrix T1. Then for any c ≥ 1, any rank−k X̂ ∈ Rm1×m2 which satisfies\n‖T1(LX̂N −A)‖pp ≤ c2p−1(2c′1c′2 + 1) · min rank−k X ‖T1(LX̃N −A)‖pp,\nit has\n‖LX̂N −A‖pp ≤ c · 22p−2(2c1c2 + 1)(2c′1c′2 + 1) · ‖LX∗N −A‖pp.\nLemma E.11. Given matrices M ∈ Rn×d, U ∈ Rn×t, d ≥ t = rank(U), n ≥ d ≥ r = rank(M). If sketching matrix S ∈ Rm×n is drawn from any of the following probability distributions on matrices, with .99 probability, S has at most c1-dilation on M , i.e.,\n‖SM‖pp ≤ c1‖M‖pp,\nand S has at most c2-contraction on U , i.e.,\n∀x ∈ Rt, ‖SUx‖pp ≥ 1\nc2 ‖Ux‖pp,\nwhere c1, c2 are parameters depend on the distribution over S.\n(I) S ∈ Rm×n is a dense matrix with entries drawn from a p-stable distribution: a matrix with i.i.d. standard p-stable random variables. If m = O(t log t), then c1c2 = O(log d).\n(II) S ∈ Rm×n is a sparse matrix with some entries drawn from a p-stable distribution: S = TD, where T ∈ Rm×n has each column drawn i.i.d. from the uniform distribution over standard basis vectors of Rm, and D ∈ Rn×n is a diagonal matrix with each diagonal entry drawn from i.i.d. from the standard p-stable distribution. If m = O(t5 log5 t), then c1c2 = O(t2/p log2/p t log d). If m = O((t+ r)5 log5(t+ r)), then c1c2 = O(min(t 2/p log2/p t log d, r3/p log3/p r)).\n(III) S ∈ Rm×n is a sampling and rescaling matrix (notation S ∈ Rn×n denotes a diagonal sampling and rescaling matrix with m non-zero entries): If S samples and reweights m = O(t log t log log t) rows of U , selecting each with probability proportional to the ith row’s `p Lewis weight and reweighting by the inverse probability, then c1c2 = O(1).\nIn the above, if we replace S with σ · S where σ ∈ R\\{0} is any scalar, then the relation between m and c1c2 can be preserved.\nFor (I), it is implied by Lemma E.17, Lemma E.19. Also see from [SW11]1. For (II), if m = O(t5 log5 t), then c1c2 = O(t2/p log2/p t log d) is implied by Corollary D.27 and Lemma E.20 and Theorem 4 in [MM13]. If m = O((t + r)5 log5(t + r)), c1c2 = O(r3/p log3/p r) is implied by [MM13].\nFor (III), it is implied by [CP15] and Lemma D.29."
    }, {
      "heading" : "E.3 Tools and inequalities",
      "text" : "Lemma E.12 (Lemma 9 in [MM13], Upper Tail Inequality for p-stable Distributions). Let p ∈ (1, 2) and m ≥ 3. ∀i ∈ [m], let Xi be m(not necessarily independent) random variables sampled from Dp, and let γi > 0 with γ = ∑m i=1 γi. Let X = ∑m i=1 γi|Xi|p. Then for any t ≥ 1,\nPr[X ≥ tαpγ] ≤ 2 log(mt)\nt .\nWe first review some facts about the p-norm and q-norm,\nFact E.13. For any p ≥ q > 0 and any x ∈ Rk,\n‖x‖p ≤ ‖x‖q ≤ k 1 q − 1 p ‖x‖p.\nWe provide the triangle inequality for the p-norm,\nFact E.14. For any p ∈ (1, 2), for any x, y ∈ Rk,\n‖x+ y‖p ≤ ‖x‖p + ‖y‖p, and ‖x+ y‖pp ≤ 2p−1(‖x‖pp + ‖y‖pp).\nFact E.15 (Hölder’s inequality). For any x, y ∈ Rk, if 1p + 1 q = 1, then |x >y| ≤ ‖x‖p‖y‖q.\nWe give the definition of a well-conditioned basis for `p,\nDefinition E.16. Let p ∈ (1, 2). A basis U for the range of A is (α, β, p)-conditioned if ‖U‖p ≤ α and for all x ∈ Rk, ‖x‖q ≤ β‖Ux‖p. We will say U is well-conditioned if α and β are low-degree polynomials in k, independent of n.\n1Full version.\nProof. We first show an upper bound,\n‖Ux‖p ≤ ‖U‖p · ‖x‖p ≤ α‖x‖p\nThen we show a lower bound,\n‖Ux‖p ≥ 1\nβ ‖x‖q\nFor any p and q with 1/p+ 1/q = 1, by Hölder’s inequality we have\n|x>y| ≤ ‖x‖p · ‖y‖q\nchoosing y to be the vector that has 1 everywhere, ‖x‖1 ≤ ‖x‖pk1/q\nE.4 Dense p-stable transform\nThis section states the main tools for the dense p-stable transform. The proof is identical to that for the dense Cauchy transform.\nLemma E.17. Given matrix A ∈ Rn×d and p ∈ (1, 2), define U∗ ∈ Rn×k, V ∗ ∈ Rk×d to be an optimal solution of min\nU∈Rn×k,V ∈Rk×d ‖UV − A‖p. Choose a p-stable distribution matrix S ∈ Rm×n,\nrescaled by Θ(1/m1/p). Then we have\n‖SU∗V ∗ − SA‖pp . log(md)‖U∗V ∗ −A‖pp\nwith probability at least 99/100.\nProof. Let P (0, 1) denote the p-stable distribution. Then,\n‖SU∗V ∗ − SA‖pp ≤ d∑ i=1 ‖S(U∗V ∗i −Ai)‖pp\n= d∑ i=1 m∑ j=1 | n∑ l=1 1 m Sj,l(U ∗V ∗i −Ai)l|p where Sj,l ∼ P (0, 1)\n= 1\nm d∑ i=1 m∑ j=1 |w̃ij( n∑ l=1 |(U∗V ∗i −Ai)l|p)1/p|p where w̃ij ∼ P (0, 1)\n= 1\nm d∑ i=1 m∑ j=1 n∑ l=1 |(U∗V ∗i −Ai)l|p · |w̃ij |p\n= 1\nm d∑ i=1 m∑ j=1 ‖U∗V ∗i −Ai‖pp · w p i+(j−1)d, where wi+(j−1)d ∼ |P (0, 1)|\nwhere the last step follows since each wi can be thought of as a clipped half-p-stable random variable. Define X to be ∑d i=1 ∑m j=1 ‖U∗V ∗i −Ai‖ p p · wpi+(j−1)d and γ to be ∑d i=1 ∑m j=1 ‖U∗V ∗i −Ai‖ p p. Then applying Lemma E.12,\nPr[X ≥ tαpγ] ≤ 2 log(mdt)\nt .\nChoosing t = Θ(log(md)), we have with probability .999,\nX . log(md)αpγ = log(md)αp d∑ i=1 ‖U∗V ∗i −Ai‖pp,\nwhere the last steps follows by definition of γ. Thus, we can conclude that with probability .999, ‖Π(U∗V ∗ −A)‖pp . log(md)‖U∗V ∗ −A‖pp.\nLemma E.18. Given matrix A ∈ Rn×d and p ∈ (1, 2), define U∗ ∈ Rn×k to be the optimal solution of min\nU∈Rn×k,V ∈Rk×d ‖UV −A‖p. Choose a matrix of i.i.d. p-stable random variables S ∈ Rm×n. Then\nfor all V ∈ Rk×n, we have\n‖SU∗V − SA‖pp & ‖U∗V −A‖pp −O(log(md))‖U∗V ∗ −A‖pp.\nLemma E.19. Let p ∈ (1, 2). Given an (α, β) `p well-conditioned basis, condition on the following two events,\n1. For all x ∈ N , ‖SUx‖p & ‖Ux‖p. 2. For all x ∈ Rk, ‖SUx‖p ≤ poly(k)‖Ux‖p. Then for all x ∈ Rk, ‖SUx‖p & ‖Ux‖p.\nProof. The proof is identical to Lemma D.22 in Section D.\nE.5 Sparse p-stable transform\nThis section states the main tools for the sparse p-stable transform. The proof is identical to that of the sparse Cauchy transform.\nLemma E.20. Let p ∈ (1, 2). Given matrix A ∈ Rn×d with U∗, V ∗ an optimal solution of minU,V ‖UV − A‖p, let Π = σ · SC ∈ Rm×n, where S ∈ Rm×n has each column vector chosen independently and uniformly from the m standard basis vectors of Rm, where C is a diagonal matrix with diagonals chosen independently from the standard p-stable distribution, and σ is a scalar. Then\n‖ΠU∗V ∗ −ΠA‖pp . σ · log(md) · ‖U∗V ∗ −A‖pp\nholds with probability at least .999.\nProof. We define Π = σ · SC ∈ Rm×n as in the statement of the lemma. Then by the definition of\nΠ, we have,\n‖Π(U∗V ∗ −A)‖pp\n= d∑ i=1 ‖SC(U∗V ∗i −Ai)‖pp\n= d∑ i=1 ∥∥∥∥  S11 S12 · · · S1n S21 S22 · · · S2n · · · · · · · · · · · · Sm1 Sm2 · · · Smn  ·  c1 0 0 0 0 c2 0 0 0 0 · · · 0 0 0 0 cn  · (U∗V ∗i −Ai)∥∥∥∥p p = d∑ i=1 ∥∥∥∥  c1S11 c2S12 · · · cnS1n c1S21 c2S22 · · · cnS2n · · · · · · · · · · · · c1Sm1 c2Sm2 · · · cnSmn  · (U∗V ∗i −Ai)∥∥∥∥p p\n= d∑ i=1 ∥∥∥∥ n∑ l=1 clS1l · (U∗V ∗i −Ai)l, n∑ l=1 clS2l · (U∗V ∗i −Ai)l, · · · , n∑ l=1 clSml · (U∗V ∗i −Ai)l ∥∥∥∥p p\n= d∑ i=1 m∑ j=1 ∣∣∣∣ n∑ l=1 clSjl · (U∗V ∗i −Ai)l ∣∣∣∣p by aX + bY and (|a|p + |b|p)1/pZ are indentically distributed\n= d∑ i=1 m∑ j=1 ∣∣∣∣w̃ij · ( n∑ l=1 |Sjl(U∗V ∗i −Ai)l|p)1/p ∣∣∣∣p where w̃ij ∼ P (0, 1)\n= d∑ i=1 m∑ j=1 n∑ l=1 |Sjl(U∗V ∗i −Ai)l|p · |w̃ij |p\n= d∑ i=1 m∑ j=1 n∑ l=1 |Sjl(U∗V ∗i −Ai)l|p · w p i+(j−1)d, where wi+(j−1)d ∼ |P (0, 1)|\nwhere the last step follows since each wi can be thought of as a clipped half-p-stable random variable. DefineX to be ∑d i=1 ∑m j=1 ∑n l=1 |Sjl(U∗V ∗i −Ai)l|p·w p i+(j−1)d and γ to be ∑d i=1 ∑m j=1 ∑n l=1 |Sjl(U∗V ∗i − Ai)l|p. Then applying Lemma E.12,\nPr[X ≥ tαpγ] ≤ 2 log(mdt)\nt .\nChoosing t = Θ(log(md)), we have with probability .999,\nX . log(md)αpγ = log(md)αp d∑ i=1 n∑ l=1 |(U∗V ∗i −Ai)l|p,\nwhere the last steps follows by\nγ = d∑ i=1 m∑ j=1 n∑ l=1 |Sjl(U∗V ∗i −Ai)l|p = d∑ i=1 m∑ j=1 n∑ l=1 |Sjl|p|(U∗V ∗i −Ai)l|p = d∑ i=1 n∑ l=1 |(U∗V ∗i −Ai)l|p.\nThus, we can conclude that with probability .999, ‖Π(U∗V ∗ −A)‖pp . log(md)‖U∗V ∗ −A‖pp.\nLemma E.21. Given matrix A ∈ Rn×d with U∗, V ∗ an optimal solution of minU,V ‖UV −A‖p, let Π = σ · SC ∈ Rm×n, where S ∈ Rm×n has each column vector chosen independently and uniformly from the m standard basis vectors of Rm, and where C is a diagonal matrix with diagonals chosen independently from the standard p-stable distribution. Then with probability at least .999, for all V ∈ Rk×d,\n‖ΠU∗V −ΠA‖pp ≥ ‖U∗V −A‖pp −O(σ log(md))‖U∗V ∗ −A‖pp.\nNotice that m = O(k5 log5 k) and σ = O((k log k)2/p) according to Theorem 4 in [MM13].\nE.6 `p-Lewis weights\nThis section states the main tools for `p-Lewis weights. The proof is identical to `1-Lewis weights.\nLemma E.22. For any p ∈ (1, 2). Given matrix A ∈ Rn×d, define U∗ ∈ Rn×k, V ∗ ∈ Rk×d to be an optimal solution of min\nU∈Rn×k,V ∈Rk×d ‖UV −A‖p. Choose a diagonal matrix D ∈ Rn×n according to\nthe Lewis weights of U∗. We have that\n‖DU∗V ∗ −DA‖pp . ‖U∗V ∗ −A‖pp,\nholds with probability at least .99.\nLemma E.23. Let p ∈ (1, 2). Given matrix A ∈ Rn×d, define U∗ ∈ Rn×k, V ∗ ∈ Rk×d to be an optimal solution of min\nU∈Rn×k,V ∈Rk×d ‖UV − A‖p. Choose a sampling and rescaling matrix D ∈ Rn×n\naccording to the Lewis weights of U∗. For all V ∈ Rk×d we have\n‖DU∗V −DA‖pp & ‖U∗V −A‖pp −O(1)‖U∗V ∗ −A‖pp,\nholds with probability at least .99.\nF EMD-Low Rank Approximation\nIn this section we explain how to embed EMD to `1. For more detailed background on the EarthMover Distance(EMD) problem, we refer the reader to [IT03, AIK08, ABIW09, IP11, BIRW16] and [SL09, LOG16]. Section F.1 introduces some necessary notation and definitions for Earth-Mover Distance. Section F.2 presents the main result for the Earth-Mover distance low rank approximation problem."
    }, {
      "heading" : "F.1 Definitions",
      "text" : "Consider any two non-negative vectors x, y ∈ R[∆] 2\n+ such that ‖x‖1 = ‖y‖1. Let Γ(x, y) be the set of functions γ : [∆]2 × [∆]2 → R+, such that for any i, j ∈ [∆]2 we have ∑ l γ(i, l) = xi and∑\nl γ(l, j) = yj ; that is, Γ is the set of possible “flows” from x to y. Then we define\nEMD(x, y) = min γ∈Γ ∑ i,j∈[∆]2 γ(i, j)‖i− j‖1\nto be the min cost flow from x to y, where the cost of an edge is its `1 distance.\nUsing the EMD(·, ·) metric, for general vectors w, we define ‖ · ‖EEMD distance (which is the same as [SL09]),\n‖w‖EEMD = min x−y+z=w ‖x‖1=‖y‖1 x,y≥0 EMD(x, y) + 2∆‖z‖1.\nUsing ‖ · ‖EEMD distance, for general matrices X ∈ Rn×d, we define the ‖ · ‖1,EEMD distance,\n‖X‖1,EEMD = d∑ i=1 ‖Xi‖EEMD,\nwhere Xi denotes the j-th column of matrix X.\nF.2 Analysis of no contraction and no dilation bound\nLemma F.1. Given matrix A ∈ Rn×d and U∗, V ∗ = arg min U∈Rn×k,V ∈Rk×d ‖UV − A‖1,EEMD, there exist sketching matrices S ∈ Rm×n such that, with probability .999, for all V ∈ Rk×d,\n‖S(U∗V −A)‖1 ≥ ‖U∗V −A‖1,EEMD holds.\nProof. Using Lemma 1 in [IT03], there exists a constant C > 0 such that for all i ∈ [d],\nC‖SU∗Vi − SAi‖1 ≥ ‖U∗Vi −Ai‖EEMD. (23)\nThen taking a summation over all d terms and rescaling the matrix S, we obtain, d∑ i=1 ‖S(U∗Vi −Ai)‖1 ≥ ‖U∗Vi −Ai‖EEMD\nwhich completes the proof.\nLemma F.2. Given matrix A ∈ Rn×d and U∗, V ∗ = arg min U∈Rn×k,V ∈Rk×d ‖UV − A‖1,EEMD, there exist sketching matrices S ∈ Rm×n such that\n‖S(U∗V ∗ −A)‖1 ≤ O(log n)‖U∗V ∗ −A‖1,EEMD holds with probability at least .999.\nProof. Using Lemma 2 in [IT03], we have for any i ∈ [d],\nE[‖SU∗V ∗i − SAi‖1] ≤ O(log n)‖U∗V ∗i −Ai‖EEMD. (24)\nThen using that the expectation is linear, we have\nE[‖SU∗V ∗ − SA‖1] = E[ d∑ i=1 ‖SU∗V ∗i − SAi‖1]\n= d∑ i=1 E[‖SU∗V ∗i − SAi‖1]\n≤ d∑ i=1 O(log n)‖U∗V ∗i −Ai‖EEMD by Equation (24) = O(log n)‖U∗V ∗ −A‖1,EEMD.\nUsing Markov’s inequality, we can complete the proof.\nTheorem F.3. Given a matrix A ∈ Rn×d, there exists an algorithm running in poly(k, n, d) time that is able to output U ∈ Rn×k and V ∈ Rk×d such that\n‖UV −A‖1,EEMD ≤ poly(k) · log d · log n min rank−k Ak ‖Ak −A‖1,EEMD\nholds with probability .99.\nProof. First using Lemma F.1 and Lemma F.2, we can reduce the original problem into an `1- low rank approximation problem by choosing m = poly(n). Second, we can use our `1-low rank approximation algorithm to solve it. Notice that all of our `1-low rank approximation algorithms can be applied here. If we apply Theorem C.6, we complete the proof.\nOur current ‖ · ‖1,EEMD is column-based. We can also define it to be row-based. Then we get a slightly better result by applying the `1-low rank algorithm.\nCorollary F.4. Given a matrix A ∈ Rn×d, there exists an algorithm running in poly(k, n, d) time that is able to output U ∈ Rn×k and V ∈ Rk×d such that\n‖UV −A‖1,EEMD ≤ poly(k) · log2 d min rank−k Ak ‖Ak −A‖1,EEMD\nholds with probability .99."
    }, {
      "heading" : "G Hardness results for Cauchy matrices, row subset selection, OSE",
      "text" : "Section G.1 presents some inapproximability results by using random Cauchy matrices. Section G.2 is a warmup for inapproximability results for row subset selection problems. Section G.3 shows the inapproximability results by using any linear oblivious subspace embedding(OSE), and also shows inapproximability results for row subset selection.\nG.1 Hard instance for Cauchy matrices\nThe goal of this section is to prove Theorem G.2. Before stating the result, we first introduce some useful tools in our analysis.\nLemma G.1 (Cauchy Upper Tail Inequality, Lemma 3 of [CDMI+13]). For i ∈ [m], let Ci be m random Cauchy variables from C(0, 1) (not necessarily independent), and γi > 0 with γ = ∑ i∈[m] γi.\nLet X = ∑\ni∈[m] γi|Ci|. Then, for any t ≥ 1,\nPr[X > γt] ≤ O(log(mt)/t).\nTheorem G.2. Let k ≥ 1. There exist matrices A ∈ Rd×d such that for any o(log d) ≥ t ≥ 1, where c can be any constant smaller than 1/3, for random Cauchy matrices S ∈ Rt×d where each entry is sampled from an i.i.d. Cauchy distribution C(0, γ) where γ is an arbitrary real number, with probability .99 we have\nmin U∈Rd×t ‖USA−A‖1 ≥ Ω(log d/(t log t)) min rank−k A′\n‖A′ −A‖1.\nProof. We define matrix A ∈ Rd×d\nA = B + I = α ·  1 1 1 · · · 1 0 0 0 · · · 0 0 0 0 · · · 0 · · · · · · · · · · · · · · · 0 0 0 · · · 0 +  1 0 0 · · · 0 0 1 0 · · · 0 0 0 1 · · · 0 · · · · · · · · · · · · · · · 0 0 0 · · · 1  , where α = Θ(log d).So, if we only fit the first row of A, we can get approximation cost at most O(d).\nFor t > 0, let S ∈ Rt×d denote a random Cauchy matrix where Si,j denotes the entry in the ith row and jth column. Then SA is\nSA = SB + SI = α ·  S1,1 S1,1 S1,1 · · · S1,1 S2,1 S2,1 S2,1 · · · S2,1 S3,1 S3,1 S3,1 · · · S3,1 · · · · · · · · · · · · · · · St,1 St,1 St,1 · · · St,1 +  S1,1 S1,2 S1,3 · · · S1,d S2,1 S2,2 S2,3 · · · S2,d S3,1 S3,2 S3,3 · · · S3,d · · · · · · · · · · · · · · · St,1 St,2 St,3 · · · St,d  . Since ∀γ,\nmin U∈Rd×t ‖USA−A‖1 = min U∈Rd×t ‖γUSA−A‖1,\nwithout loss of generality, we can let Si,j ∼ C(0, 1). Then we want to argue that, if we want to use SA to fit the first row of A, with high probability, the cost will be Ω(d log d).\nLet b ∈ Rd denote the first row of A. Then, we want to use SA to fit the first row of A, which is a d-dimensional vector that has entry Θ(log d) on each position. The problem is equiavlent to\nmin x∈Rt ‖(SA)>x− b‖1.\nFirst, we want to show that for any x in Rt, if x>SA fits the first row of A very well, then the `1 and `2 norm of vector x must have reasonable size.\nClaim G.3. Define A1 to be the first row of matrix A. With probability .999, for any column vector x ∈ Rt×1, if ‖x>SA−A1‖1 ≤ o(d log d), then\nProperty (I) : ‖x‖2 ≥ Ω(1/t log t); Property (II) : ‖x‖1 ≤ O(log d).\nProof. Consider the absolute value of the i-th coordinate of x>SA. We can rewrite |〈(SA)i, x〉| in the following sense,\n|〈(SA)i, x〉| = |〈(SB)i, x〉+ 〈(SI)i, x〉| = |〈(SB)1, x〉+ 〈(SI)i, x〉| = |〈(SB)1, x〉+ 〈Si, x〉|, (25)\nwhere the second step follows because (SB)i = (SB)1, ∀i ∈ [n], and the last step follows because (SI)i = Si,∀i ∈ [n].\nWe start by proving Property I. Using the triangle inequality and Equation (25),\n|〈(SA)i, x〉| ≤ |〈(SB)1, x〉|+ |〈Si, x〉| ≤ ‖(SB)1‖2‖x‖2 + ‖Si‖2‖x‖2 by Cauchy-Schwarz inequality ≤ ‖(SB)1‖1‖x‖2 + ‖Si‖1‖x‖2. by ‖ · ‖2 ≤ ‖ · ‖1\nThen, according to Lemma G.1, with probability .99999, we have ‖(SB)1‖1 ≤ O(t log t log d) and for a fixed i ∈ [d], ‖Si‖1 ≤ O(t log t). Applying the Chernoff bound, with probability 1 − 2−Ω(d), there are a constant fraction of i such that ‖Si‖1 = O(t log t). Taking the union bound, with probability .9999, there exists a constant fraction of i such that |〈(SA)i, x〉| ≤ O(t log t log d)‖x‖2. Because A1,i ≥ α, ∀i ∈ [d] where α = Θ(log d), we need ‖x‖2 ≥ Ω(1/t log t). Otherwise, the total cost on this constant fraction of coordinates will be at least Ω(d log d).\nFor Property II, for a fixed x ∈ Rt, we have\n|〈(SA)i, x〉| =|〈(SB)1, x〉+ 〈Si, x〉| by Equation (25)\n= ∣∣∣∣Θ(log d)‖x‖1w′1(x) + ‖x‖1w′i(x)∣∣∣∣. where w′i(x) ∼ C(0, 1), and for different x, w′i(x) are different. Then for a fixed x ∈ Rt with probability at least 0.9, |w′i(x)| = Ω(1), and with probability 0.5, w′1(x) and w′i(x) have the same sign. Since these two events are independent, with probability at least 0.45, we have\n|〈(SA)i, x〉| ≥ ‖x‖1 · Ω(1).\nApplying the Chernoff bound, with probability at least 1 − 2Θ(d), there exists a 3/10 fraction of i such that |〈(SA)i, x〉| ≥ ‖x‖1Ω(1).\nWe build an -net N for x ∈ Rt on an `1-norm unit ball, where = 1/(t2 log2 d). Thus the size of the net is |N | = 2Θ̃(t). Consider y to be an arbitrary vector, let y/‖y‖1 = x + δ, where x is the closest point to y/‖y‖1 and x ∈ N . For any δ ∈ Rt,\n|〈(SA)i, δ〉| = |〈(SB)1, δ〉+ 〈(SI)i, δ〉| ≤ |〈(SB)1, δ〉|+ |〈(SI)i, δ〉| by triangle inequality ≤ ‖(SB)1‖2‖δ‖2 + ‖Si‖2‖δ‖2 by Cauchy-Schwarz inequality ≤ ‖(SB)1‖2‖δ‖1 + ‖Si‖2‖δ‖1. by ‖ · ‖2 ≤ ‖ · ‖1\nAs we argued before, With probability .99999, we have\n‖(SB)1‖2 ≤ ‖(SB)1‖1 ≤ O(t log t log d), (26)\nand with probability 1 − 2−Θ(d), there is a 9/10 fraction of i ∈ [d], ‖Si‖2 ≤ ‖Si‖1 = O(t log t). Therefore, with probability .999, for any δ ∈ Rt, there exists a 9/10 fraction of i such that |〈(SA)i, δ〉| ≤ Θ(t log t log d)‖δ‖1.\nTherefore, with probability .99, ∀y ∈ Rt, due to the pigeonhole principle, there is a 3/10 + 9/10− 1 = 1/5 fraction of i such that\n|〈(SA)i, y〉| = ‖y‖1 · |〈(SA)i, y/‖y‖1〉| = ‖y‖1 · |〈(SA)i, x+ δ〉| by y/‖y‖1 = x+ δ ≥ ‖y‖1 · ( |〈(SB)i, x〉+ 〈(SI)i, x〉| − |〈(SB)1, δ〉+ 〈(SI)i, δ〉| ) by triangle inequality\n≥ ‖y‖1 ( Ω(1)− O(t log t log d) ) ≥ ‖y‖1Ω(1).\nSo ‖y‖1 should be O(log d). Otherwise, the total cost on this 1/5 fraction of coordinates is at least Ω(d log d).\nCombining Property (I) and (II) completes the proof.\nNext, we need to show the following claim is true,\nClaim G.4. For any d independent Cauchy random variables x1, x2, · · · , xd from C(0, 1), with probability 1 − 1/ poly(t log d), for any j ∈ [1, 2, · · · , log d − Θ(log log(t log d))], there are Ω(d/2j) variables belonging to (2j , 2j+1].\nProof. For each Cauchy random variable xi, we have for any j ∈ [1, 2, · · · ,Θ(log d)],\nPr [ |xi| ∈ (2j , 2j+1] ] = Θ(1/2j).\nWe define the indicator random variable zi,j\nzi,j = { 1 if |xi| ∈ (2j , 2j+1], 0 otherwise.\nWe define zj = ∑d i=1 zi,j . It is clear that E[zi,j ] = Θ(1/2 j). We use a Chernoff bound,\nPr[X < (1− δ)µ] < (\ne−δ\n(1− δ)1−δ\n)µ ,\nand set X = zj , δ = 1/2, µ = E[zj ]. Then, this probability is at most 2−Ω(µ) = 2−Ω(E[zj ]). For any j ∈ [1, log d − Θ(log log(t log d))], we have E[zj ] = dE[zij ] = Ω(d/2j) = Ω(log(t log d)). Thus, this probability is at most 1/ poly(t log d). Overall, we have\nPr [ zj & d/2 j ] ≥ 1− 1/ poly(t log d).\nTaking a union bound over Θ(log d) such j, we complete the proof.\nClaim G.5. Let 1 > c1 > c2 > 1/3 > 0 be three arbitrary constants. We fix the first column of (SI)> ∈ Rd×t. All the rows are grouped together according to the value in the first column. Let Rj be the set of rows for which the entry in the first column is ∈ (2j , 2j+1]. With probability at least 1 − O(1/ poly(t log(d))), for any j ∈ [c2 log d, c1 log d], the following event holds. There exist Ω(d/2j) rows such that the first coordinate ∈ (2j , 2j+1] and all the other coordinates are at most O(d1/3).\nProof. Let Rj be a subset of rows of S>, such that for any row in Rj , the first coordinate is ∈ (2j , 2j+1]. By Claim G.4, we have that with probability 1 − 1/ poly(t log d), for any j ∈ [c2 log d, c1 log d], |Rj | ≥ Ω(d/2j). We want to show that for any j ∈ [c2 log d, c1 log d], there exists a constant fraction of rows in Rj such that, the remaining coordinates are at most O(d1/3).\nFor a fixed row in Rj and a fixed coordinate in that row, the probability that the absolute value of this entry is at least Ω(d1/3) is at most O(1/d1/3). By taking a union bound, with probability at least 1−O(t/d1/3), the row has every coordinate of absolute value at most O(d1/3).\nBy applying a Chernoff bound, for a fixed subset Rj of rows, the probability that there is a constant fraction of these rows for which every coordinate except the first one in absolute value is at most O(d1/3), is at least 1− 2−Θ(|Rj |) ≥ 1− 2−Θ(d1−c1 ) ≥ 1−O(1/poly(t log(d))).\nAfter taking a union over all the j, we complete the proof.\nClaim G.6. With probability at least 1 − O(1/t), the absolute value of any coordinate in column vector (SB)1 ∈ Rt×1 is at most O(t2 log d).\nProof. Because each entry is sampled from a Cauchy distribution C(0, 1) scaled by O(log d), then with probability at most O(1/t2), the absolute value of one coordinate is at least Ω(t2 log d). Taking a union over all the coordinates, we complete the proof.\nBecause ‖x‖1 ≤ O(log d), ‖x‖2 ≥ Ω(1/t log t), there exists one coordinate i such that the absolute value of it is at least Ω(1/t log t) and all the other coordinates are most O(log d). We can assume that i = 1 for now. (To remove this assumption, we can take a union bound over all the possibilities for i ∈ [t].) According to Claim G.5 and G.6, let R̂j denote a subset of Rj , which is a “good” constant fraction of Rj . Considering the `1-norm of all coordinates l ∈ R̂j ⊂ Rj ⊂ [d], we have∑\nl∈R̂j\n|((SA)>x)l −O(log d)|\n≥ ∑ l∈R̂j |〈(SA)l, x〉 −O(log d)| ≥ ∑ l∈R̂j |(SI)1,l · x1| − t∑ j=2 |(SI)j,l · xj | − |〈(SB)l, x〉| −O(log d)\n ≥ ∑ l∈R̂j ( Ω( 2j t log t )−O(td1/3 log d)− |〈(SB)1, x〉| −O(log d) )\n≥ ∑ l∈R̂j ( Ω( 2j t log t )−O(td1/3 log d)− ‖(SB)1‖1‖x‖1 −O(log d) )\n≥ ∑ l∈R̂j ( Ω( 2j t log t )−O(td1/3 log d)−O(t log t log2 d)−O(log d) ) & ∑ l∈R̂j 2j/(t log t)\n&d/2j · 2j/(t log t) &d/(t log t).\nThe second inequality follows by the triangle inequality. The third inequality follows by (SB)1 = (SB)l, |x1| = Ω(1/t log t), (SI)1,l ∈ [2j , 2j+1), and ∀j 6= 1, |xj | < O(log d), (SI)j,l ≤ O(d1/3). The fourth inequality follows by Cauchy-Schwarz and ‖ · ‖2 ≤ ‖ · ‖1. The fifth inequality follows by Equation (26) and Claim G.3. The sixth inequality follows by t = o(log d) where c is a constant smaller than 1/3 and 2j ≥ dc2 > poly(t). The seventh inequality follows from |R̂j | ≥ Ω(d/2j).\nSince there are c1 − c2 different j, the total cost is Ω(d log d/(t log t)). The gap then is Ω(log d/(t log t)). This completes the proof of Theorem G.2.\nG.2 Hard instance for row subset selection\nTheorem G.7. Let ∈ (0, 1). There exists a value k ≥ 1 and matrix A ∈ R(d−1)×d such that, for any subset R of rows of A, letting B be the `1-projection of each row of A onto R, we have\n‖A−B‖1 > (2− ) min rank−k A′\n‖A′ −A‖1,\nunless |R| & d.\nProof. We construct the (d− 1)× d matrix A in the following way. The first column is all 1s, and then the remaining (d− 1)× (d− 1) matrix is the identity matrix.\nA =  1 1 0 0 · · · 0 1 0 1 0 · · · 0 1 0 0 1 · · · 0 · · · · · · · · · · · · · · · · · · 1 0 0 0 · · · 1  .\nNote that min rank−1 A′ ‖A − A′‖1 = OPT ≤ d − 1, since one could choose A′ to have the first column all 1s and remaining entries 0. On the other hand, consider any subset of r rows. We can permute the columns and preserve the entrywise `1-norm so w.l.o.g., we can take the first r rows for R.\nBecause we are taking the first r rows, we do not pay any cost on the first rows. To minimize the cost on the i-th row, where i ∈ {r + 1, r + 2, . . . , d}, let vi denote the row vector we use for the i-th row. Then vi can be written as a linear combination of the first r rows {A1, A2, . . . , Ar},\nvi = r∑ j=1 αi,jA j .\nThen the cost of using vi to approximate the i-th row of A is:\n‖vi −Ai‖1 = (cost in 1st col) + ( cost in 2nd,3rd, . . . , r + 1th cols) + ( cost in i+ 1th col)\n= | r∑ j=1 αi,j − 1|+ r∑ j=1 |αi,j |+ 1\n≥ | r∑ j=1 αi,j − 1− r∑ j=1 αi,j |+ 1 by triangle inequality\n= 2.\nHence, the cost of using vi to approximate the i-th row of A is at least 2. So in total, across these (d− 1− r) rows, the algorithm pays at least 2(d− 1− r) cost, which needs to be at most C(d− 1), and therefore r ≥ (d− 1)(1− C/2) = Ω( d). Choosing C = 2− completes the proof.\nTheorem G.8. There exists a value k ≥ 1 and matrix A ∈ R(d−1)×d such that, there is no algorithm that is able to output a rank−k matrix B in the row span of A satisfying\n‖A−B‖1 < 2(1−Θ( 1\nd )) min rank−k A′ ‖A′ −A‖1.\nProof. We use the same matrix A as in the previous theorem.\nA =  1 1 0 0 · · · 0 1 0 1 0 · · · 0 1 0 0 1 · · · 0 · · · · · · · · · · · · · · · · · · 1 0 0 0 · · · 1  .\nLet vector v be ( ∑d−1\ni=1 βi, β1, . . . , βd−1). For the i-th row of A, we use αj · v to cancel the cost of that row, where αj is a scalar. Then for any β1, . . . , βd−1, α1, . . . , αd−1, we can compute the entire residual cost,\nf(α, β) = d−1∑ j=1 ‖Aj − vαj‖1 = d−1∑ j=1 ( |1− αj( d−1∑ i=1 βi)|+ |1− αjβj |+ ∑ i 6=j |αjβi| ) .\nIn the next few paragraphs, we will show that optimizing f(α, β) over some extra constraints for α, β does not change the optimality. Without loss of generality, we can assume that ∑d−1 i=1 βi = 1. If not we can just rescale all the αi. Consider a fixed index j. All the terms related to αj and βj are,\n|1− αj( d−1∑ i=1 βi)|+ |1− αjβj |+ ∑ i 6=j |αjβi|+ |αiβj |.\nWe first show a simple proof by assuming βj ≥ 0. Later, we will prove the general version which does not have any assumptions.\nHandling a special case We can optimize f(α, β) in the following sense,\nmin f(α, β)\ns.t. d−1∑ j=1 βj = 1\nαj ≥ 0, βj ≥ 0, ∀j ∈ [d− 1].\nFor each j, we consider three cases. Case I, if αj ≥ 1βj , then\n|1− αj |+ |1− αjβj |+ ∑ i 6=j αjβi = αj − 1 + αjβj − 1 + ∑ i 6=j αjβi = 2αj − 2 ≥ 2(1/βj − 1) ≥ 2(1− βj),\nwhere the last step follows by βj + 1/βj ≥ 2. Case II, if 1 ≤ αj < 1/βj , then\n|1− αj |+ |1− αjβj |+ ∑ i 6=j αjβi = αj − 1 + 1− αjβj + ∑ i 6=j αjβi = 2αj(1− βj) ≥ 2(1− βj).\nCase III, if αj < 1, then\n|1− αj |+ |1− αjβj |+ ∑ i 6=j αjβi = 1− αj + 1− αjβj + ∑ i 6=j αjβi = 2(1− αjβj) ≥ 2(1− βj).\nPutting it all together, we have\nf(α, β) ≥ d−1∑ j=1 2(1− βj) = 2(d− 1)− 2 d−1∑ j=1 βj = 2(d− 2).\nTo handle the case where βi can be negative Without loss of generality, we can assume that∑d−1 i=1 βi = 1. Notice that we can also assume βi 6= 0, otherwise it means we do not choose that row. We split all βi into two disjoint sets S and S. For any i ∈ S, βi > 0 and for any i ∈ S, βi < 0. As a first step, we discuss the case when all the j are in set S. Case I, if 1−αj ∑d−1 i=1 βi < 0 and\n1− αjβj < 0, then it means αj ≥ max( 1∑d−1 i=1 βi , 1β j ). The cost of that row is,\n= αj d−1∑ i=1 βi − 1 + αjβj − 1 + ∑ i 6=j |αjβi|\n= 2αjβj + 2αj ∑ i∈S\\j βi − 2.\nIf βj ≥ 1, then αj ≥ 1. The cost is at least 2 ∑\ni∈S\\j βi. If βj < 1, then αj ≥ 1/βj , and the cost is at least 2 1βj ∑ i∈S\\j βi. If there are C such j in Case I, then the total cost of Case I is at least 0 if C = 1, and 2(C − 1) if C ≥ 2. Case II, if 1− αj ∑d−1 i=1 βi < 0 and 1− αjβj > 0, then it means 1 < αj < 1/βj . The cost of that row is,\n= αj d−1∑ i=1 βi − 1 + 1− αjβj + |αj | ∑ i 6=j |βj |\n= 2αj ∑ i∈S\\j βi\n≥ 2 ∑ i∈S\\j βi.\nSimilarly to before, if there are C such j in Case II, then the total cost of Case II is at least 0 if C = 1, and 2(C − 1) if C ≥ 2.\nCase III, if 1− αj ∑d−1 i=1 βi > 0 and 1− αjβj < 0, then it means 1/βj < αj < 1∑d−1\ni=1 βi . The cost\nof the row is,\n= 1− αj d−1∑ i=1 βi + αjβj − 1 + |αj | ∑ i 6=j |βj |\n= 2|αj | ∑ i∈S |βi|\n= 2|αj |( ∑ i∈S |βi| − 1) ≥ 2 1 βj ∑ i∈S\\j βi.\nIf there are C such j in Case III, then the total cost of Case III is at least 0 if C = 1, and 2(C ·(C−1)) if C ≥ 2.\nCase IV, if 1 − αj ∑d−1 i=1 βi > 0 and 1 − αjβj > 0, then it means αj ≤ min(1/βj , 1∑d−1\ni=1 βi ). The\ncost for the case αj < 0 is larger than the cost for case α > 0. Thus we can ignore the case αj < 0.\nThe cost of the row is,\n= 1− αj d−1∑ i=1 βi + 1− αjβj + |αj | ∑ i 6=j |βj |\n= 2− 2αjβj + 2αj ∑ i∈S |βi|.\nIf βj < ∑\ni∈S |βi|, we know that the cost is at least 2. Otherwise, using αj ≤ 1, we have the cost is at least 2− 2βj + ∑ i∈S |βi|. Let T denote the set of those j with βj ≥ ∑ i∈S |βi|. If |T | = 1, we know that cost is at least 0. If |T | ≥ 2, then the cost is at least\n= ∑ j∈T (2− 2βj + 2 ∑ i∈S |βi|)\n≥ 2|T | − 2 ∑ j∈S βj + 2|T | ∑ i∈S |βi|\n≥ 2|T | − 2 + 2(|T | − 1) ∑ i∈S |βi|\n≥ 2(C − 1).\nNow we discuss the case where j ∈ S, which means βj < 0. Case V if 1− αj ∑d−1 i=1 βi < 0 and 1− αjβj < 0, then it means αj > 1/ ∑d−1 i=1 βi and αj < 1/βj .\nNotice that this case will never happen, because ∑d−1\ni=1 βi = 1 and αj < 0. Case VI if 1 − αj ∑d−1 i=1 βi < 0 and 1 − αjβj < 0, then it means αj ≥ max(1/ ∑d−1 i=1 βi, 1/βj).\nBecause of βj < 0, then αj ≥ 1. The cost of that is,\n= αj d−1∑ i=1 βi − 1 + 1− αjβj + |αj | ∑ i 6=j |βi|\n= 2αj ∑ i∈S |βi|\n≥ 2. by αj ≥ 1 and ∑ i∈S |βi| ≥ 1.\nCase VII if 1 − αj ∑d−1 i=1 βi > 0 and 1 − αjβj < 0, then it means αj < min(1/βj , 1/ ∑d−1\ni=1 βi). Because βj < 0 and ∑d−1 i=1 βi = 1, thus αj < 1/βj . The cost of that row is,\n= 1− αj d−1∑ i=1 βi + αjβj − 1 + |αj | ∑ i 6=j |βi|\n= 2|αj | ∑ i∈S\\j |βi|\n≥ 2 1 |βj | ∑ i∈S\\j |βi|.\nIf there are C such j in Case VII, then the total cost of Case VII is at least 0 if C = 1, and 2(C · (C − 1)) if C ≥ 2.\nCase VIII if 1− αj ∑d−1 i=1 βi > 0 and 1− αjβj > 0, then it means 1/βj < αj < 1/ ∑d−1\ni=1 βi. The cost of that row,\n= 1− αj d−1∑ i=1 βi + 1− αjβj + |αj | ∑ i 6=j |βi|\n= 2− 2αjβj + 2|αj | ∑ i∈S\\j |βi|.\nIf αj > 0, then the cost is always at least 2. If αj < 0, the cost is at least,\n2− 2|αjβj |+ 2|αj | ∑ i∈S\\j |βi|.\nIf ∑\ni∈S\\j |βi| > |βj |, we also have cost at least 2. Otherwise, we have\n2− 2|αj |(|βj | − ∑ i∈S\\j |βi|) =\n{ 2− 2|βj |+ 2 ∑ i∈S\\j |βi| if 1/|βj | ≤ 1,\n2 1|βj | ∑ i∈S\\j |βi| if 1/|βj | > 1.\nLet C denote the number of such j with 1/|βj | ≤ 1. If C = 1, the cost is at least 0. If C ≥ 2, the cost is at least 2C. Let C ′ denote the number of such j with 1/|βj | > 1. If C ′ = 1, the cost is at least 0, if C ′ ≥ 2, the cost is at least 2(C ′(C ′− 1)). Overall, putting all the eight cases together, we complete the proof.\nG.3 Hard instance for oblivious subspace embedding and more row subset selection\nThe goal in this section is to prove Theorem G.31. By applying Yao’s minmax principle, it suffices to prove Theorem G.27."
    }, {
      "heading" : "G.3.1 Definitions",
      "text" : "We first give the definition of total variation distance and Kullback-Leibler divergence.\nDefinition G.9. [LPW09, Ver14] The total variation distance between two probability measures P and Q on the measurable space (X , F) is defined as,\nDTV(P,Q) = sup A∈F |P (A)−Q(A)|.\nThe Kullback-Leibler( KL) divergence of P and Q is defined as,\nDKL(P ||Q) = E P\n( log dP\ndQ\n) = ∫ X ( log dP dQ ) dP.\nLemma G.10. [Pin60, Tsy09, CK11] Pinsker’s inequality states that, if P and Q are two probability distributions on a measurable (X , F), then\nDTV(P,Q) ≤ √ 1\n2 DKL(P ||Q).\nLemma G.11. For any Gaussian random variable x ∼ N(0, σ2), we have, for any a > 0\nPr[|x| < a] . a σ .\nProof.\nPr[|x| < a] = erf( a σ √ 2 )\n≤ (1− e− −4a2 2σ2π ) 1 2 by 1− exp(−4x2/π) ≥ erf(x)2\n≤ ( 4a 2\n2σ2π ) 1 2 by 1− e−x ≤ x\n. a\nσ .\nLemma G.12. Let V ∈ Rn×m be a matrix with orthonormal columns. Let H′ be a distribution over V >A, where A ∈ Rn×k is a random matrix with each entry i.i.d. Gaussian N(0, 1). Denote H as a distribution over H ∈ Rm×k, where each entry of H is drawn from i.i.d. Gaussian N(0, 1). Then, H and H′ are the same distribution.\nProof. It is clear that each entry of V >A is a random Gaussian variable from N(0, 1), so our goal is to prove the entries of V >A are fully independent. Since the the jth column of V >A only depends on Aj , the variables from different columns are fully independent. Now, we look at one column of x = V >Aj . The density function is\nf(x) = 1√\n(2π)m|V >V | exp\n( −1\n2 x>V >V x\n) =\n1√ (2π)m exp\n( −1\n2 x>x\n) ,\nwhich is exactly the density function of N(0, Im). Thus x ∈ N(0, Im). Therefore, all the entries of V >A are fully independent.\nLemma G.13 (Matrix Determinant Lemma). Suppose A ∈ Rn×n is an invertible matrix, and u, v ∈ Rn. Then,\n|A+ uv>| = (1 + v>A−1u)|A|.\nLemma G.14 (KL divergence between two multivariate Gaussians [PP+08]2). Given two d-dimensional multivariate Gaussian distribution N(µ1,Σ1) and N(µ2,Σ2), then\nDKL(N(µ1||Σ1), N(µ2,Σ2)) = 1\n2 ( log |Σ2| |Σ1| − d+ tr(Σ−12 Σ1) + (µ2 − µ1) >Σ−12 (µ2 − µ1) ) .\nLemma G.15 (Sherman-Morrison formula). Suppose A ∈ Rn×n is an invertible matrix and u, v ∈ Rn. Suppose furthermore that 1 + v>A−1u 6= 0. Then the Sherman-Morrison formula states that\n(A+ uv>)−1 = A−1 − A −1uv>A−1\n1 + v>A−1u . (27)\n2http://stats.stackexchange.com/questions/60680/kl-divergence-between-two-multivariate-gaussians"
    }, {
      "heading" : "G.3.2 Main results",
      "text" : "Lemma G.16. Let V ∈ Rn×m be a matrix with orthonormal columns, and let A ∈ Rn×k be a random matrix with each entry drawn from i.i.d. Gaussian N(0, 1). We denote the distribution Di over Di ∈ R(m+1)×k where\nDi = [ V >A Ai ] ."
    }, {
      "heading" : "If ‖(V >)i‖22 < 12 , then",
      "text" : "DTV(Di,G) ≤ O(k‖V i‖2) + 2−Θ(k),\nwhere G is a distribution over G ∈ R(m+1)×k, where each entry of G is drawn from the i.i.d. Gaussian N(0, 1).\nProof. Let H be a distribution over H ∈ Rm×k, where each entry of H is an i.i.d. Gaussian N(0, 1). Let H′ be a distribution over V >A. According to Lemma G.12, H and H′ are the same distribution.\nWe define matrix V ¬i ∈ Rn×m as\nV ¬i = [ (V 1)> (V 2)> · · · (V i−1)> 0 (V i+1)> · · · (V n)> ]> ,\nwhere V i denotes the ith row of V ∈ Rn×m, ∀i ∈ [n]. Let G be a distribution over G ∈ R(m+1)×k, where each entry of G is an i.i.d. Gaussian N(0, 1). Let Pi be a distribution over Pi ∈ R(m+1)×k, where\nPi =\n[ (V ¬i)>A\nAi\n] .\nLet P̂i be a distribution over P̂i ∈ Rm×k, where P̂i = (V ¬i)>A. Then we have:\nDTV(Pi,G) = DTV(P̂i,H′) = DTV(P̂i,H). (28)\nThe first equality is because (V ¬i)>A is independent from Ai. The second equality follows the Lemma G.12.\nClaim G.17. If ‖V i‖22 ≤ 12 ,\nDKL(P̂i||H) = − k\n2\n( log(1− ‖V i‖22) + ‖V i‖22 ) ≤ k‖V i‖22.\nProof. Let P̂i ∼ P̂i, H ∼ H. Notice that different columns of P̂i are i.i.d, and all entries of H are fully independent. We can look at one column of P̂i and H. Since P̂i = (V ¬i)>A, it is easy to see that its column is drawn from N(0,Σ1) where Σ1 = Im − (V i)>V i. Since H is fully independent, each column of H is drawn from N(0,Σ2) where Σ2 = Im. Let p(x) be the pdf of the column of P̂i,\nand let q(x) be the pdf of the column of H. We have the following calculation [PP+08]3,\nDKL(P̂i||H) = k\n2 ( log |Σ2| |Σ1| −m+ tr(Σ−12 Σ1) )\n= k\n2\n( log\n|Im| |Im − (V i)>V i|\n−m+ tr(Im − (V i)>V i) )\n= k\n2\n( − log |Im − (V i)>V i| −m+m− ‖V i‖22 ) = k\n2\n( − log(1− ‖V i‖22)−m+m− ‖V i‖22 ) = −k\n2\n( log(1− ‖V i‖22) + ‖V i‖22 ) ≤ −k\n2 · 2‖V i‖22\n= k‖V i‖22.\nThe first equality is due to Lemma G.14. The sixth equality follows by Σ2 = Im and Σ1 = Im − (V i)>V i. The eighth equality follows by Lemma G.13. The first inequality follows by log(1−x)+x ≥ −2x, when 0 < x < 1/2.\nAccording to Lemma G.10, we have\nDTV(P̂i,H) ≤ √ 1\n2 DKL(P̂i||H) ≤\n√ k‖V i‖2. (29)\nNow, we want to argue that DTV(Di,Pi) is small, where Di is a distribution over Di ∈ R(m+1)×k that\nDi = [ V >A Ai ] .\nFor a fixed x ∈ Rk, let D̂i(x) be a distribution over D̂i(x) ∈ Rm×k, where D̂i(x) = (V ¬i)>A+(V i)>x. Let p(x) be the pdf of (Ai)>, then\nDTV(Di,Pi) = ∫ DTV ( D̂i(x), P̂i ) p(x)dx. (30)\nNow we look at the jth column of D̂i(x) and the jth column of P̂i. The distribution of the previous one is over N((V i)>xj ,Σ2), and the latter distribution as we said before is N(0,Σ2),\n3http://stats.stackexchange.com/questions/60680/kl-divergence-between-two-multivariate-gaussians\nwhere Σ2 = Im − (V i)>V i. Now we can argue that the KL divergence between them is bounded:\nDKL(N((V i)>xj ,Σ2)||N(0,Σ2))\n= 1\n2 ( log |Σ2| |Σ2| −m+ tr(Σ−12 Σ2) + x 2 jV iΣ−12 (V i)> )\n= 1\n2\n( −m+ tr(Im) + x2jV iΣ−12 (V i)> )\n= 1\n2 x2jV iΣ−12 (V i)>\n= 1\n2 x2jV i(Im − (V i)>V i)−1(V i)>\n= 1\n2 x2jV i\n( Im + (V i)>V i\n1− V i(V i)>\n) (V i)>\n= 1\n2 x2j\n( ‖V i‖22 +\n‖V i‖22 1− ‖V i‖22 ) = 1\n2 x2j ‖V i‖22\n1− ‖V i‖22 .\nThe first equality is due to Lemma G.14. The fourth equality follows by Σ2 = Im − (V i)>V i. The fifth equality follows by Lemma G.15.\nBy summing the KL divergence on all the columns up,\nDKL(D̂i(x)||P̂i) = 1\n2 ‖x‖22 ‖V i‖22 1− ‖V i‖22 .\nApplying Lemma G.10 again, we get DTV(D̂i(x), P̂i) ≤ √ 1\n2 DKL(D̂i(x)||P̂i) = ‖x‖2‖V i‖2 2 √ 1− ‖V i‖22 .\nPlugging it into Equation (30), we get DTV(Di,Pi) = ∫ DTV ( D̂i(x), P̂i ) p(x)dx\n= ∫ ‖x‖2≤10k DTV ( D̂i(x), P̂i ) p(x)dx+ ∫ ‖x‖2>10k DTV ( D̂i(x), P̂i ) p(x)dx\n≤ ∫ ‖x‖2≤10k DTV ( D̂i(x), P̂i ) p(x)dx+ ∫ ‖x‖2>10k p(x)dx\n≤ ∫ ‖x‖2≤10k ‖x‖2‖V i‖2 2 √ 1− ‖V i‖22 p(x)dx+ ∫ ‖x‖2>10k p(x)dx ≤ 10k‖V i‖2\n2 √ 1− ‖V i‖22 + ∫ ‖x‖2>10k p(x)dx\n≤ 10k‖V i‖2 2 √ 1− ‖V i‖22 + 2−Θ(k).\nThe first inequality just follows from the fact that total variation distance is smaller than 1. The second inequality is what we plugged in. The last inequality follows from the fact that x ∼ N(0, Ik) and from the tail bounds of a Gaussian.\nTogether with Equation (28) and Equation (29), we can get\nDTV(Di,G) ≤ DTV(Di,Pi) +DTV(G,Pi)\n= DTV(Di,Pi) +DTV(P̂i,H)\n≤ 10k‖V i‖2 2 √ 1− ‖V i‖22 + 2−Θ(k) + √ k‖V i‖2 ≤ 10k‖V i‖2 + 2−Θ(k) + √ k‖V i‖2.\nThe last inequality follows by ‖V i‖22 ≤ 12 . Then, we have completed the proof.\nLemma G.18. A ∈ Rr×k (r ≥ k) is a random matrix for which each entry is i.i.d. N(0, 1). With probability at least 1− e−Θ(r), the maximum singular value ‖A‖2 is at most O( √ r).\nProof. Since A ∈ Rk×r is a random matrix with each entry i.i.d. N(0, 1), this follows by standard arguments (Proposition 2.4 in [RV10]). Since r ≥ k, with probability at least 1− e−Θ(r), ‖A‖2 is at most O( √ r).\nDefinition G.19. Let V ∈ Rn×r be a matrix with orthonormal columns, and let each entry of A ∈ Rk×r be a random variable drawn from an i.i.d. Gaussian N(0, 1). Define event Ê(A, V, β, γ) to be: ∀y ∈ Rn, ‖y‖1 ≤ O(kγ) and each coordinate of y has absolute value at most 1/kβ, and also AV >y has at most O(k/ log k) coordinates with absolute value at least Ω(1/ log k), and ‖A‖2 ≤ O( √ r).\nLemma G.20. For any k ≥ 1, and any constants c2 ≥ c1 ≥ 1, let k ≤ r = O(kc1), r ≤ n = O(kc2), let V ∈ Rn×r be a matrix with orthonormal columns, and let each entry of A ∈ Rk×r be a random variable drawn from i.i.d. Gaussian N(0, 1). Furthermore, if β and γ are two constants which satisfy β > γ > 0 and β + γ < 1,\nPr [ Ê(A, V, β, γ) ] ≥ 1− 2−Θ(k).\nProof. Due to Lemma G.18, with probability at least 1− 2−Θ(r), ‖A‖2 ≤ O( √ r), so we can restrict attention to ‖A‖2 ≤ O( √ r) in the following proof.\nTake any y ∈ Rn which has each non-zero coordinate with absolute value at most 1/kβ , and write it as y = ∑+∞ j=j0\nyj , where the coordinates in yj have absolute value in the range [2−j−1, 2−j), and the supports of different yj are disjoint. Since each coordinate of y is at most 1/kβ , 2−j0 = 1/kβ . Since ‖y‖1 ≤ O(kγ), the support size of yj ∈ Rn is at most sj ≤ O(2j+1kγ), so it follows that\n‖yj‖2 ≤ √ sj · 2−2j ≤ √ 2−j+1kγ .\nWe also know ‖yj‖2 ≥ √ sj · 2−2j−2. Then we can conclude that yj has 2-norm Θ(2−j √ sj). Now we state an ε-net for all the possible yj : let ε = O(1/(nrk3)) = O(1/(kc1+c2+3)). Let Nj ⊂ Rn be the following:\nNj = { p ∈ Rn | ∃q ∈ Zn, s.t. p = εq, ‖p‖1 ≤ O(kγ),∀i ∈ [n], either |pi| ∈ [2−j−1, 2−j) or pi = 0 } .\nObviously, for any yj , there exists p ∈ Nj such that ‖yj − p‖∞ ≤ ε = O(1/(kc1+c2+3)), since n ≤ O(kc2), ‖yj − p‖2 ≤ ‖yj − p‖1 ≤ n‖yj − p‖∞ ≤ O(1/kc1+3). Now let us consider the size of Nj . If p ∈ Nj , the choice of one coordinate of p is at most 2/ε + 1. And since ‖p‖1 ≤ O(kγ) and each\ncoordinate of p has absolute value at least 2−j−1, the number of supports of p is at most O(2j+1kγ). Therefore,\n|Nj | ≤ (n+ 1)O(2 j+1kγ) · (2/ε+ 1)O(2j+1kγ)\n≤ 2O(2j+1kγ(logn+log(2/ε+1))\n≤ 2O(2j+1kγ log k).\nThe last inequality follows from n ≤ O(kc2), r ≤ O(kc1), ε = O(1/(rnk3)). We define an event E(yj) to be: AV >yj has k/ log2 k coordinates which are each at least 2/ log2 k in absolute value. Now, we want to show,\nClaim G.21. For any j ≥ j0, for a fixed yj ∈ Rn,\nPr [ E(yj) happens ] ≤ 2ke−Θ(k/(‖yj‖22 log 6 k)) ≤ e−Θ(2j−1k1−γ/ log 6 k).\nProof. We let p be the probability that the absolute value of a single coordinate of AV >yj is at least 1/ log2 k. Notice that each coordinate of AV >yj is i.i.d. Gaussian N(0, ‖V >yj‖22) and because for any Gaussian random variable g, Pr[|g| ≥ t] ≤ exp(−Θ(t2/σ2)), then p ≤ exp(−1/(‖V >yj‖22 log4 k)), by plugging σ2 = ‖V >yj‖22 and t = 1/ log2 k. So the probability AV >yj has k/ log2 k coordinates which are each at least 1/ log2 k in absolute value is,\nk∑ i=k/ log2 k pi(1− p)k−i ( k i ) ≤ k∑ i=k/ log2 k pi ( k k/2 ) ≤ 2pk/ log 2 k2k\n≤ e−Θ(kt2/(σ2 log 2 k))2k by p ≤ exp(−Θ(t2/σ2))\n≤ e−Θ(k/(‖V >yj‖22 log 6 k))2k\n≤ e−Θ(k/(‖yj‖22 log 6 k))2k by ‖V >yj‖22 ≤ ‖V ‖22‖yj‖22 ≤ ‖yj‖22 ≤ e−Θ(k/(2−2jsj log 6 k))2k by ‖yj‖22 ≤ 2−2jsj ≤ e−Θ(k/(2−j+1kγ log 6 k))2k by sj ≤ O(2j+1kγ) = e−Θ(k/(2 −j+1kγ log6 k))\n≤ e−Θ(2j−1k1−γ/ log 6 k).\nThe first equality follows from 2−j+1kγ log6 k ≤ 2−j0+1kγ log6 k ≤ 2kγ−β log6 k = o(1).\nFor j1 = d100(c1 + c2 + 1) log ke = Θ(log k), consider j ∈ [j1,∞). We have ‖ ∑∞\nj=j1 yj‖2 is at\nmost Θ(2−j1 √ n) ≤ 1/k100(1+c1), and so\n‖AV > ∞∑ j=j1 yj‖1 ≤ √ k‖AV > ∞∑ j=j1 yj‖2 ≤ √ k‖A‖2‖V ‖2‖ ∞∑ j=j1 yj‖2 ≤ √ k · √ r · 1/k100(1+c1) ≤ 1/k50.\nThe last inequality follows from r = O(kc1). So the contribution of yj to ‖AV >yj‖1 for all j ≥ j1 is at most 1/k50. Thus, if we only consider those j which contribute, i.e., j0 ≤ j ≤ j1, we have\nO(log k) values of j. Then we can only construct O(log k) nets Nj0 ,Nj0+1, · · · ,Nj1 . Since the size of net Nj is 2Θ(2 j+1kγ log k), by combining Claim G.21 and taking a union bound, we have\nPr ∃yj ∈ j1⋃ j=j0 Nj , E(yj) happens  ≤ j1∑ j=j0 2Θ(2 j+1kγ log k) · e−Θ(2j−1k1−γ/ log 6 k)\n≤ e−Θ(2j0−1k1−γ/ log 6 k)\n≤ e−Θ(k1+β−γ/ log 6 k)\n≤ 2−Θ(k).\nThe second inequality follows since kγ = o(k1−γ). The third inequality follows since 2j0 ≥ kβ . The fourth inequality follows since 1 + β − γ > 1.\nThen, ∀j0 ≤ j ≤ j1, ∀ỹj 6∈ Nj , there exists a vector ŷj in Nj , such that ‖ŷj − ỹj‖2 ≤ 1/k3+c1 . We can upper bound the `∞ norm of AV >ŷj −AV >ỹj in the following sense,\n‖AV >ŷj −AV >ỹj‖∞ ≤ ‖AV >ŷj −AV >ỹj‖2 by ‖ · ‖∞ ≤ ‖ · ‖2 ≤ ‖A‖2 · ‖ŷj − ỹj‖2 ≤ √ r/k3+c1 by Claim G.18 and ‖ŷj − ỹj‖2 ≤ 1/k3+c1\n= 1/k2 by r ≤ O(kc1).\nWe let Y = {y ∈ Rn | ‖y‖1 ≤ O(kγ) and each coordinate of y ≤ 1/kβ}. Since 1/k2 < 1/ log2 k we can conclude that,\nPr [ ∃y ∈ Y, j ≥ j0, E(yj) happens ] ≤ 2−Θ(k). (31)\nRecalling y = ∑\nj y j , by Equation (31), for any y, with probability at most 2−Θ(k), there are at most O(log k) · k/ log2 k ≤ O(k/ log k) coordinates for which AV > ∑j1\nj=j0 yj (the same statement also holds for AV > ∑∞\nj=j0 yj = AV >y since we argued that there is negligible “contribution” for\nthose j > j1) is at least O(log k)/ log2 k = O(1/ log k) on that coordinate. Summarizing,\nPr [ Ê(A, V, β, γ) ] ≥ 1− 2−Θ(k).\nLemma G.22. For any t, k ≥ 1, and any constants c2 ≥ c1 ≥ 1, let k ≤ r = O(kc1), r ≤ n = O(kc2), let V ∈ Rn×r be a matrix with orthonormal columns, and let each entry of A ∈ Rk×r, v1, v2, · · · , vt ∈ Rk be an i.i.d. Gaussian N(0, 1) random variable. For a constant α ∈ (0, 0.5) which can be arbitrarily small, if Ê(A, V, 0.5+α/2, 0.5−α) happens, then with probability at least 1−2−Θ(tk), there are at least dt/10e such j ∈ [t] that ∀x ∈ Rr either ‖Ax− vj‖1 ≥ Ω(k0.5−α) or ‖V x‖1 ≥ Ω(k0.5−α) holds.\nProof. For convenience, we define γ = 0.5 − α which can be an arbitrary constant in (0, 0.5). We let constant β = 0.5 + α/2. Then we have β + γ < 1 and β > γ. Let v ∈ Rk be a random vector with each entry drawn from i.i.d. Gaussian N(0, 1). Suppose Ê(A, V, β, γ) happens.\nFor any x ∈ Rr, we can find a y ∈ Rn such that y = V x. Since V ∈ Rn×r has orthonormal columns, x = V >V x = V >y. Then our goal is to argue that with high probability if ‖AV >y−v‖1 ≤ O(kγ), then ‖y‖1 > Ω(kγ). Take any y ∈ Rn which can be expressed as V x, and decompose it as y = y0 +y1, where y0 ∈ Rn and y1 ∈ Rn have disjoint supports, y0 has each coordinate with absolute value greater than 1/kβ , and y1 has each coordinate with absolute value at most 1/kβ .\nNow we create an ε-net for y0: let ε = O(1/(rnk3)) = O(1/kc1+c2+3), we denote N ⊂ Rn as follows:\nN = { p ∈ Rn | ∃q ∈ Zn, s.t. p = εq, ‖p‖1 ≤ O(kγ), ∀i ∈ [n], either |pi| > 1/kβ or pi = 0 } .\nObviously, for any y0, there exists p ∈ N such that ‖y0 − p‖∞ ≤ ε = O(1/(kc1+c2+3)), since n ≤ O(kc2), ‖y0 − p‖2 ≤ ‖y0 − p‖1 ≤ n‖y0 − p‖∞ ≤ O(1/kc1+3). Now let us consider the size of N . If p ∈ N , the number of choices of one coordinate of p is at most O(kγ/ε). And since ‖p‖1 ≤ O(kγ) and each coordinate of p has absolute value at least 1/kβ , the number of supports of p is at most O(kγ+β). Therefore,\n|N | ≤ (n+ 1)O(kγ+β) ·O(kγ/ε)O(kγ+β)\n≤ 2O(kγ+β log k).\nThe last inequality follows from n ≤ O(kc2), r ≤ O(kc1), ε = O(1/(rnk3)). For y0 ∈ Rn, we define event E1(y0) as: ∃ valid y1 ∈ Rn, ‖AV >y−v‖1 ≤ O(kγ), where y = y0+y1 is the decomposition of a possible y ∈ Rn. Here y1 is valid means that there exists y ∈ Rn such that ‖y‖1 ≤ O(kγ) and the decomposition of y is y = y0 + y1. We define event E2(y0) as: ∃ valid y1, the absolute value of AV >y − v is at most O(1/kγ) for at least k − O(k2γ) coordinates i in [k] . We define event E3(y0) as: at least k−O(k2γ)−O(k/ log k) coordinates of Ay0− v have absolute value at most O(1/ log k).\nClaim G.23. For y0 ∈ Rn,\nPr[E3(y0) happens ] ≥ Pr[E2(y0) happens ] ≥ Pr[E1(y0) happens ].\nProof. If E1(y0) happens, then there exists a valid y1 ∈ Rn such that y = y0 +y1 and ‖AV >y−v‖1 ≤ O(kγ). For this y, there are at least k −O(1/k2γ) coordinates of AV >y − v with absolute value at most O(1/kγ). Otherwise, ‖AV >y − v‖1 > Ω(kγ). Thus, E1(y0) implies E2(y0).\nNow we want to show E2(y0) implies E3(y0). We suppose E2(y0) happens. Then there is a valid y1 such that there are at least k −O(1/k2γ) coordinates of AV >y − v with absolute value at most O(1/kγ), where y = y0 + y1. Recall that the event Ê(A, V, β, γ) happens, for any valid y1 there are at most O(k/ log k) coordinates of AV >y1 are at least Ω(1/ log k). Therefore,\nAV >y0 − v = AV >y − v︸ ︷︷ ︸ ≥k−O(k2γ) coordinates\neach ≤O(1/kγ)\n− AV >y1︸ ︷︷ ︸ ≤O(k/ log k) coordinates\neach≥Ω(1/ log k)\n.\nTherefore, at least k − O(k2γ) − O(k/ log k) coordinates of Ay0 − v in absolute value is at most O(1/ log k) +O(1/kγ) = O(1/ log k)\nClaim G.24. Define event F1 to be the situation for which there exists 1/2 of the coordinates of v ∈ Rk that are at least 1/100. The probability this event F1 holds is at least 1− 2−Θ(k).\nProof. Note that F1 means there exist k/2 of the coordinates of v ∈ Rk which are at most 1/100. Using Lemma G.11, for each single coordinate the probability it is smaller than 1/100 is 1/200. Then the probability more than half the coordinates are no more than 1/100 is\nk∑ i=k/2 pi(1− p)k−i ( k i ) ≤ k∑ i=k/2 pi ( k k/2 ) ≤ 2pk/22k ≤ (1/200)Θ(k) = 2−Θ(k).\nConditioned on F1 happening, if E3(y0) happens, then due to the pigeonhole principle, there are at least k2 − O(k\n2γ) − O(k/ log k) coordinates of AV >y0 − v that are at most O(1/ log k) and the corresponding coordinate of v is larger than 1/100. Now, let us look at the probability on a single coordinate of AV >y0 − v.\nClaim G.25. If the ith coordinate of v ∈ Rk is at least 1/100. Then Pr[|(AV >y0)i − (v)i| ≤ O(1/ log k)] ≤ O(1/ log k).\nProof. Since |(v)i| > 1/100 and v is independent from A ∈ Rk×r, for 0 < η < 1/100, Pr[|(AV >y0)i− (v)i| ≤ η] is always upper bounded by Pr[(Ax0)i ∈ [1/100− η, 1/100 + η]. Thus, it suffices to prove an upper bound for Pr[(AV >x0)i ∈ [1/100− η, 1/100 + η]. Let f(x) be the pdf of N(0, σ2), where σ2 = ‖V >y0‖22, V ∈ Rn×r, y0 ∈ Rn. Then\nPr[(AV >y0)i ∈ [1/100− η, 1/100 + η]]\n= ∫ 1/100+η 1/100−η f(x)dx\n≤ f(1/200) ∫ 1/100+η\n1/100−η dx\n≤O(η).\nwhere the last step follows since f(1/200) ≤ 200. We set η = O(1/ log k), then we get the statement.\nClaim G.26. Conditioned on F1, for a fixed y0 ∈ Rn, with probability at most 2−Θ(k), there are at least k10 coordinates of AV\n>y0 − v ∈ Rk which are at most O(1/ log k) and the corresponding coordinate of v is larger than 1/100.\nProof. We look at the coordinate i ∈ [k] which has |(v)i| > 1/100. The probability ‖(AV >y0)i − (v)i‖ ≤ O(1/ log k) is at most O(1/ log k). Due to the independence between different coordinates of v, since there are at least k/2 coordinates of v satisfying that they have absolute value greater than 1/100, with probability at most 2−Θ(k), there are at least 15 · k 2 = k/10 coordinates of AV\n>y0 − v which are at most O(1/ log k).\nBecause E3(y0) implies the event described in the above claim when conditioning on F1, for a fixed y0, the probability that E3(y0) holds is at most 2−Θ(k). Since γ + β < 1, the |N | ≤ 2k\no(1) , we can take a union bound over the N :\nPr[∃y0 ∈ N , E1(y0) happens ] ≤ 2−Θ(k).\nIt means that with probability at least 1−2−Θ(k), for any y = y0 +y1 with y0 ∈ N , ‖AV >y−v‖1 > Ω(kγ). Let ỹ = ỹ0 + ∑ ỹj , where ỹ0 6∈ N . We can find ŷ0 ∈ N which is the closest to ỹ0. Denote\nŷ = ŷ + ∑ ỹj . Then,\n‖AV >ŷ − v‖1 ≤ ‖AV >ỹ − v‖1 + ‖AV >ỹ −AV >ŷ‖1 by triangle inequality\n≤ ‖AV >ỹ − v‖1 + √ k‖AV >ỹ −AV >ŷ‖2 by ‖ · ‖1 ≤ √ dim‖ · ‖2 ≤ ‖AV >ỹ − v‖1 + √ k‖A‖2‖ỹ − ŷ‖2 ≤ ‖AV >ỹ − v‖1 + √ k · √ r · ‖ỹ0 − ŷ0‖2 by ‖A‖2 ≤ √ r ≤ ‖AV >ỹ − v‖1 + √ k · √ r · ε by ‖ỹ0 − ŷ0‖2 ≤ ε = ‖AV >ỹ − v‖1 + √ k · √ r · 1/kc1+3 by ε′ = 1/kc1+3 = ‖AV >ỹ − v‖1 + 1/k. by r = O(kc1)\nand so if ‖AV >ỹ − v‖1 is at most O(kγ) then ‖Aŷ − v‖1 is at most O(kγ). For j ∈ [t], we now use notation E4(vj) to denote the event: ∃y0 ∈ Rn with ‖y0‖1 ≤ O(kγ) and each non-zero coordinate of y0 is greater than 1/kβ , at least k−O(k2γ)−O(k/ log k) coordinates of AV >y0−vj in absolute value are at most O(1/ log k). Based on the previous argument, conditioned on Ê(A, V, β, γ),\nPr[E4(vj)] ≤ 2−Θ(k).\nAlso notice that, conditioned on Ê(A, V, β, γ), ∀j ∈ [t], E4(vj) are independent. Thus, conditioned on Ê(A, V, β, γ), due to the Chernoff bound, the probability that there are dt/10e such j that E4(vj) happens is at most 2−Θ(kt). We define E5(vj) to be the event: ∃y ∈ Rn, ‖AV >y − vj‖ ≤ O(kγ) with ‖y‖1 ≤ O(kγ). Similar to the proof of Claim G.23, conditioned on Ê(A, V, β, γ), E5(vj) implies E4(vj). Thus, conditioned on Ê(A, V, β, γ), the probability that there are dt/10e such j that E5(vj) happens is at most 2−Θ(tk). Then, we complete the proof.\nTheorem G.27. For any k ≥ 1, and any constants c1, c2 which satisfy c2 − 2 > c1 > 1, let r = Θ(kc1), n = Θ(kc2), and let A(k, n) denote a distribution over n× (k + n) matrices where each entry of the first n × k matrix is i.i.d. Gaussian N(0, 1) and the next n × n matrix is an identity matrix. For any fixed r × n matrix S and a random matrix Â ∼ A(k, n), with probability at least 1−O(k1+ c1−c2 2 )− 2−Θ(k), there is no algorithm that is able to output a matrix B ∈ Rn×r such that\n‖BSÂ− Â‖1 ≤ O(k0.5−ε) min rank−k A′ ‖A′ − Â‖1,\nwhere ε > 0 is a constant which can be arbitrarily small.\nProof. For convenience, we let γ = 0.5− ε be a constant which can be arbitrarily close to 0.5. Since the last n columns of Â is an identity matrix, we can fit the first k columns of Â, so we have\nmin rank−k A′\n‖A′ − Â‖1 ≤ n.\nNow, we want to argue that, for a fixed S, with high probability, for any rank-k n × r matrix B, the cost\n‖BSÂ− Â‖1 ≥ Ω(n · kγ).\nThus, the approximation gap will be at least Ω(kγ). We denote the SVD of S = USΣSV >S where US ∈ Rr×r,ΣS ∈ Rr×r, VS ∈ Rn×r. Then, we can rewrite\n‖BSÂ− Â‖1 = ‖BUSΣSV >S Â− Â‖1\n= n∑ l=1 ‖(BUSΣS)l(V >S Â)− Âl‖1\n≥ ∑\nl:‖V lS‖ 2 2≤2r/n\n‖(BUSΣS)l(V >S Â)− Âl‖1. (32)\nThe first equality follows from the SVD of S. The second equality follows from the fact that the `1-norm of a matrix is the sum of `1-norms of rows. The third inequality follows since we just look at the cost on a part of the rows.\nWe use βl to denote (BUSΣS)l. We look at a fixed row l, then the cost on this row is:\n‖βl(V >S Â)− Âl‖1 =‖βl(V >S Â)[1:k] − (Âl)[1:k]‖1 + ‖βl(V >S Â)[k+1:k+n] − (Âl)[k+1:n+k]‖1 ≥‖βl(V >S Â)[1:k] − (Âl)[1:k]‖1 + ‖βl(V >S Â)[k+1:k+n]‖1 − ‖(Âl)[k+1:n+k]‖1 ≥‖βl(V >S Â)[1:k] − (Âl)[1:k]‖1 + ‖βl(V >S Â)[k+1:k+n]‖1 − 1 ≥‖βl(V >S Â)[1:k] − (Âl)[1:k]‖1 + ‖βlV >S ‖1 − 1. (33)\nwhere (V >S Â)[1:k] denotes the first k columns of (V > S Â), and similarly, (V > S Â)[k+1:k+n] denotes the last n columns of (V >S Â). The first equality is because we can compute the sum of `1 norms on the first k coordinates and `1 norm on the last n coordinates. The first inequality follows from the triangle inequality. The second inequality follows since the last n columns of Â form an identity, so there is exactly one 1 on the last n columns in each row. The third inequality follows since the last n columns of Â form an identity. Let Dl be a distribution over Dl ∈ R(r+1)×k, where\nDl =\n[ (V >S Â)[1:k]\n(Âl)[1:k]\n] .\nLet G be a distribution over G ∈ R(r+1)×k where each entry of G is drawn from i.i.d. N(0, 1). According to Lemma G.16, we have\nDTV(Dl,G) ≤ O(k‖(VS)l‖2) + 2−Θ(k). (34)\nLet A = G[1:r], v = Gr+1. Due to Lemma G.20, with probability at least 1−2−Θ(k), Ê(A>, VS , 0.75− γ/2, γ) happens. Then conditioned on Ê(A>, VS , 0.75−γ/2, γ), due to Lemma G.22, with probability at most 2−Θ(k), there exists βl such that\n‖βlA− v‖1 + ‖βlV >S ‖1 = o(kγ).\nCombined with Equation (34), we can get that for a fixed l, with probability at most O(k‖(VS)l‖2)+ 2−Θ(k), there exists βl such that\n‖βl(V >S Â)[1:k] − (Âl)[1:k]‖1 + ‖βlV >S ‖1 = o(kγ).\nWhen ‖(VS)l‖22 ≤ 2r/n = Θ(kc1−c2), this probability is at most Θ(k1+(c1−c2)/2) + 2−Θ(k). Since∑n l=1 ‖(VS)l‖22 = r, there are at most n/2 such l that ‖(VS)l‖22 > 2r/n which means that there are at least n/2 such l that ‖(VS)l‖22 ≤ 2r/n. Let s be the number of l such that ‖(VS)l‖22 ≤ 2r/n, then s > n/2. Let t be a random variable which denotes that the number of l which satisfies ‖(VS)l‖22 ≤ 2r/n and achieve\n‖βl(V >S Â)[1:k] − (Âl)[1:k]‖1 + ‖βlV >S ‖1 = o(kγ),\nat the same time. Then, E[t] ≤ (O(k‖(VS)l‖2) + 2−Θ(k))s = (O(k √ 2r/n) + 2−Θ(k))s.\nDue to a Markov inequality, Pr[t > s/2 > n/4] ≤ O(k √ 2r/n) + 2−Θ(k) = O(k1+(c1−c2)/2) + 2−Θ(k).\nThe equality follows since r = Θ(kc1), n = Θ(kc2). Plugging it into Equation (32), now we can conclude, with probability at least 1−O(k1+(c1−c2)/2)− 2−Θ(k), ∀B ∈ Rn×r\n‖BSÂ− Â‖1 ≥ ∑\nl:‖V lS‖ 2 2≤2r/n\n‖(BUSΣS)l(V >S Â)− Âl‖1 ≥ n/4 · Ω(kγ) = Ω(n · kγ).\nTheorem G.28 (Hardness for row subset selection). For any k ≥ 1, any constant c ≥ 1, let n = O(kc), and let A(k, n) denote the same distribution stated in Theorem G.27. For matrix Â ∼ A(k, n), with positive probability, there is no algorithm that is able to output B ∈ Rn×(n+k) in the row span of any r = n/2 rows of Â such that\n‖Â−B‖1 ≤ O(k0.5−α) min rank−k A′ ‖A′ − Â‖1,\nwhere α ∈ (0, 0.5) is a constant which can be arbitrarily small.\nProof. For convenience, we define γ = 0.5− α which can be an arbitrary constant in (0, 0.5). Since the last n columns of Â is an identity matrix, we can fit the first k columns of Â, so we have\nmin rank−k A′\n‖A′ − Â‖1 ≤ n.\nWe want to argue that ∀B ∈ Rn×(k+n) in the row span of any r = n/2 rows of Â,\n‖Â−B‖1 ≥ Ω(n · kγ).\nLet A> ∈ Rn×k be the first k columns of Â, and let S ⊂ [n] be a set of indices of chosen rows of Â with k ≤ |S| ≤ r. Let MS ∈ Rk×n with the ith column MSi = Ai if i ∈ S and MSi = 0 otherwise. We use M̂S ∈ Rk×r to be MS without those columns of zeros, so it is a random matrix with each entry i.i.d. Gaussian N(0, 1). Then the minimum cost of using a matrix in the span of rows of Â with index in S to fit Â is at least:∑\nl 6∈S min xl∈Rn\n( ‖MSxl −Al‖1 + ‖xl‖1 − 1 ) .\nThe part of ‖MSxl − Al‖1 is just the cost on the lth row of the first k columns of Â, and the part of ‖xl‖1 − 1 is just the lower bound of the cost on the lth row of the last n columns of Â.\nClaim G.29. A, M̂S ∈ Rk×n, γ ∈ (0, 0.5),\nPr [ Ê(M̂S , Ir, 0.75− γ/2, γ) ∣∣∣∣ Ê(A, In, 0.75− γ/2, γ)] = 1. Proof. Suppose Ê(A, In, 0.75 − γ/2, γ) happens. Since MS has just a subset of columns of A, ‖MS‖2 ≤ ‖A‖2 ≤ √ n. Notice that ∀x ∈ Rn with ‖x‖1 ≤ O(kγ) and each non-zero coordinate of x is at most O(1/k0.75−γ/2), MSx ≡ MSxS ≡ AxS , where xS ∈ Rn has xSi = xi if i ∈ S and xSi = 0 otherwise. Because ‖xS‖1 ≤ O(kγ) and xS has each coordinate in absolute value at most O(1/k0.75−γ/2), AxS has at most O(k/ log k) coordinates in absolute value at least Ω(1/ log k). So, MSx = AxS has at most O(k/ log k) coordinates in absolute value at least Ω(1/ log k).\nWe denote cost(S, l) = minxl∈Rn ( ‖MSxl −Al‖1 + ‖xl‖1 − 1 ) . Since ∀l 6∈ S, Al are independent,\nand they are independent from MS , due to Lemma G.22,\nPr ∑ l 6∈S cost(S, l) ≤ O(n · kγ) ∣∣∣∣ Ê(M̂S , Ir, 0.75− γ/2, γ)  ≤ 2−Θ(rk). (35) Now we just want to upper bound the following:\nPr ∃S ⊂ [n], |S| ≤ r,∑ l 6∈S cost(S, l) ≤ O(n · kγ)  ≤ Pr\n∃S ⊂ [n], |S| ≤ r,∑ l 6∈S cost(S, l) ≤ O(n · kγ) ∣∣∣∣ Ê(A, In, 0.75− γ/2, γ)  + Pr [ ¬Ê(A, In, 0.75− γ/2, γ)\n] ≤\n∑ S⊂[n],|S|≤r Pr ∑ l 6∈S cost(S, l) ≤ O(n · kγ) ∣∣∣∣ Ê(A, In, 0.75− γ/2, γ)  + Pr [ ¬Ê(A, In, 0.75− γ/2, γ)\n] ≤\n∑ S⊂[n],|S|≤r Pr ∑ l 6∈S cost(S, l) ≤ O(n · kγ) ∣∣∣∣ Ê(A, In, 0.75− γ/2, γ) + 2−Θ(k) ≤\n∑ S⊂[n],|S|≤r Pr ∑ l 6∈S cost(S, l) ≤ O(n · kγ) ∣∣∣∣ Ê(M̂S , Ir, 0.75− γ/2, γ) + 2−Θ(k) ≤ (n+ 1)r2−Θ(rk) + 2−Θ(k)\n≤ 2−Θ(rk) + 2−Θ(k)\n≤ 2−Θ(k).\nThe second inequality follows by a union bound. The third inequality follows by Lemma G.20. The fourth inequality follows by Claim G.29. The fifth inequality is due to Equation (35). The sixth inequality follows by n ≤ O(kc), r = n/2. Thus, with probability at least 1−2−Θ(k), ∀B ∈ Rn×(n+k) which is in the span of any r ≤ n/2 rows of Â,\n‖B − Â‖1 ≥ Ω(n · kγ).\nThen, we have completed the proof.\nDefinition G.30. Given a matrix A ∈ Rn×d, a matrix S ∈ Rr×n, k ≥ 1 and γ ∈ (0, 12), we say that an algorithmM(A,S, k, γ) which outputs a matrix B ∈ Rn×r “succeeds”, if\n‖BSA−A‖1 ≤ kγ · min rank−k A′ ‖A′ −A‖1,\nholds.\nTheorem G.31 (Hardness for oblivious embedding). Let Π denote a distribution over matrices S ∈ Rr×n. For any k ≥ 1, any constant γ ∈ (0, 12), arbitrary constants c1, c2 > 0 and min(n, d) ≥ Ω(kc2), if for all A ∈ Rn×d, it holds that\nPr S∼Π\n[M(A,S, k, γ) succeeds ] ≥ Ω(1/kc1).\nThen r must be at least Ω(kc2−2c1−2).\nProof. We borrow the idea from [NN14, PSW16]. We use Yao’s minimax principle [Yao77] here. Let D be an arbitrary distribution over Rn×d, then\nPr A∼D,S∼Π\n[M(A,S, k, γ)] ≥ 1− δ.\nIt means that there is a fixed S0 such that\nPr A∼D\n[M(A,S0, k, γ)] ≥ 1− δ.\nTherefore, we want to find a hard distribution Dhard that if\nPr A∼Dhard\n[M(A,S0, k, γ)] ≥ 1− δ,\nS0 must have at least some larger poly(k) rows. Here, we just use the distribution A(k,Ω(kc2)) described in Theorem G.27 as our hard distribution. We can just fill zeros to expand the size of matrix to n× d. We can complete the proof by using Theorem G.27.\nRemark G.32. Actually, in Lemma G.20 and Lemma G.22, the reason we need β > γ > 0 is that we want kβ−γ = ω(poly(log k)), and the reason we need β+γ < 1 is that we want kβ+γ poly(log k) = o(k). Thus we can replace all the kγ by √ k/ poly(log k), e.g., √ k/ log20 k, and replace all the kβ by √ k poly(log k) with a smaller poly(log k), e.g., √ k log10 k. Our proofs still work. Therefore, if we replace the approximation ratio in Theorem G.27, Theorem G.28, and Theorem G.31 to be√ k/ logc k where c is a sufficiently large constant, the statements are still correct."
    }, {
      "heading" : "H Hardness",
      "text" : "This section presents our hardness results. Section H.1 states several useful tools from literature. Section H.2 shows that, it is NP-hard to get some multiplicative error. Assuming ETH is true, we provide a stronger hardness result in Section H.3. Section H.4 extends the result from the rank-1 case to the rank-k case."
    }, {
      "heading" : "H.1 Previous results",
      "text" : "Definition H.1 (‖A‖∞→1,[GV15]). Given matrix A ∈ Rn×d,\n‖A‖∞→1 = min x∈{−1,+1}n,y∈{−1,+1}d\nx>Ay.\nThe following lemma says that computing ‖A‖∞→1 for matrix A with entries in {−1,+1} is equivalent to computing a best {−1,+1} matrix which is an `1 norm rank-1 approximation to A.\nLemma H.2 (Lemma 3 of [GV15]). Given matrix A ∈ {−1,+1}n×d,\n‖A‖∞→1 + min x∈{−1,+1}n,y∈{−1,+1}d\n‖A− xy>‖1 = nd.\nLemma H.3 (Theorem 2 of [GV15]). Given A ∈ {−1,+1}n×d, we have\nmin x∈{−1,+1}n,y∈{−1,+1}d ‖A− xy>‖1 = min x∈Rn,y∈Rd ‖A− xy>‖1.\nCombining with Lemma H.2 and Lemma H.3, it implies that computing ‖A‖∞→1 for A ∈ {−1,+1}n×d is equivalent to computing the best `1 norm rank-1 approximation to the matrix A:\n‖A‖∞→1 + min x∈Rn,y∈Rd\n‖A− xy>‖1 = nd.\nTheorem H.4 (NP-hard result, Theorem 1 of [GV15]). Computing ‖A‖∞→1 for matrix A ∈ {−1,+1}n×d is NP-hard.\nThe proof of the above theorem in [GV15] is based on the reduction from MAX-CUT problem. The above theorem implies that computing minx∈Rn,y∈Rd ‖A− xy>‖1 is also NP-hard.\nH.2 Extension to multiplicative error `1-low rank approximation\nThe previous result only shows that solving the exact problem is NP-hard. This section presents a stronger hardness result, which says that, it is still NP-hard even if the goal is to find a solution that is able to achieve some multiplicative error. The proof in this section and the next section are based on the reduction from the MAX-CUT problem. For recent progress on MAX-CUT problem, we refer the readers to [GW95, BGS98, TSSW00, Hås01, KKMO07, FLP15].\nTheorem H.5. Given A ∈ {−1,+1}n×d, computing an x̂ ∈ Rn, ŷ ∈ Rd s.t.\n‖A− x̂>ŷ‖1 ≤ (1 + 1\nnd ) min x∈Rn,y∈Rd\n‖A− x>y‖1\nis NP-hard.\nMAX-CUT decision problem: Given a positive integer c∗ and an unweighted graph G = (V,E) where V is the set of vertices of G and E is the set of edges of G, the goal is to determine whether there is a cut of G has at least c∗ edges.\nLemma H.6. MAX-CUT decision problem is NP-hard.\nWe give the definition for the Hadamard matrix,\nDefinition H.7. The Hadamard matrix Hp of size p × p is defined recursively : [ Hp/2 Hp/2 Hp/2 −Hp/2 ] with H2 = [ +1 +1 +1 −1 ] .\nFor simplicity, we use H to denote Hp in the rest of the proof. Recall the reduction shown in [GV15] which is from MAX-CUT to computing ‖ · ‖∞→1 for {−1,+1} matrices. We do the same thing: for a given graph G = (V,E), we construct a matrix A ∈ {−1,+1}n×d where n = p|E| and d = p|V |. Notice that p = poly(|E|, |V |) is a parameter which will be determined later, and also p is a power of 2.\nWe divide the matrix A into |E| × |V | blocks, and each block has size p× p. For e ∈ [|E|], if the eth edge has endpoints i ∈ [|V |], j ∈ [|V |] and i < j, let all the p× p elements of (e, i) block of A be 1, all the p× p elements of (e, j) block of A be −1, and all the (e, l) block of A be p× p Hadamard matrix H for l 6= i, j.\nClaim H.8 (Lower bound of ‖A‖∞→1, proof of Theorem 1 of [GV15]). If there is a cut of G with cut size at least c,\n‖A‖∞→1 ≥ 2p2c− |E||V |p3/2.\nClaim H.9 (Upper bound of ‖A‖∞→1, proof of Theorem 1 of [GV15]). If the max cut of G has fewer than c edges,\n‖A‖∞→1 ≤ 2p2(c− 1) + |E||V |p3/2.\nRemark H.10. In [GV15], they set p as a power of 2 and p > |E|2|V |2. This implies\n∀c ∈ [|E|], 2p2(c− 1) + |E||V |p3/2 < 2p2c− |E||V |p3/2.\nTherefore, according to Claim H.8 and Claim H.9, if we can know the precise value of ‖A‖∞→1, we can decide whether G has a cut with cut size at least c∗.\nFor convenience, we use T ∗ to denote ‖A‖∞→1 and use L∗ to denote\nmin x∈Rn,y∈Rd\n‖A− x>y‖1.\nAlso, we use L to denote a (1 + 1nd) relative error approximation to L ∗, which means:\nL∗ ≤ L ≤ (1 + 1 nd )L∗.\nWe denote T as nd− L.\nProof of Theorem H.5. Because L∗ ≤ L ≤ (1 + 1nd)L ∗, we have:\nnd− L∗ ≥ nd− L ≥ nd− (1 + 1 nd )L∗.\nDue to Lemma H.2 and the definition of T , it has:\nT ∗ ≥ T ≥ T ∗ − 1 nd L∗.\nNotice that A is a {−1,+1} matrix, we have\nL∗ ≤ ‖A‖1 ≤ 2nd.\nThus,\nT ∗ ≥ T ≥ T ∗ − 2.\nIt means\nT + 2 ≥ T ∗ ≥ T.\nAccording to Claim H.16, if G has a cut with cut size at least c, we have:\nT + 2 ≥ T ∗ ≥ 2p2c− |E||V |p3/2.\nThat is\nT ≥ 2p2c− |E||V |p3/2 − 2.\nAccording to Claim H.17, if the max cut of G has fewer than c edges,\nT ≤ T ∗ ≤ 2p2(c− 1) + |E||V |p3/2.\nLet p be a power of 2 and p > |E|3|V |3, we have\n2p2(c− 1) + |E||V |p3/2 < 2p2c− |E||V |p3/2 − 2.\nTherefore, we can decide whether G has a cut with size at least c based on the value of T . Thus, if we can compute x̂ ∈ Rn, ŷ ∈ Rd s.t.\n‖A− x̂>ŷ‖1 ≤ (1 + 1\nnd ) min x∈Rn,y∈Rd\n‖A− x>y‖1,\nin polynomial time, it means we can compute L and T in polynomial time, and we can solve MAX-CUT decision problem via the value of T , which leads to a contradiction.\nH.3 Usin the ETH assumption\nThe goal of this section is to prove Theorem H.13. We first introduce the definition of 3-SAT and Exponential Time Hypothesis(ETH). For the details and background of 3-SAT problem, we refer the readers to [AB09].\nDefinition H.11 (3-SAT problem). Given an r variables and m clauses conjunctive normal form CNF formula with size of each clause at most 3, the goal is to decide whether there exists an assignment for the r boolean variables to make the CNF formula be satisfied.\nHypothesis H.12 (Exponential Time Hypothesis (ETH) [IPZ98]). There is a δ > 0 such that 3-SAT problem defined in Definition H.11 cannot be solved in O(2δr) running time.\nThe main lower bound is stated as follows:\nTheorem H.13. Unless ETH(see Hypothesis H.12) fails, for arbitrarily small constant γ > 0, given some matrix A ∈ {−1,+1}n×d, there is no algorithm can compute x̂ ∈ Rn, ŷ ∈ Rd s.t.\n‖A− x̂>ŷ‖1 ≤ (1 + 1\nlog1+γ nd ) min x∈Rn,y∈Rd\n‖A− x>y‖1,\nin (nd)O(1) running time.\nBefore we prove our lower bound, we introduce the following theorem which is used in our proof.\nDefinition H.14 (MAX-CUT decision problem). Given a positive integer c∗ and an unweighted graph G = (V,E) where V is the set of vertices of G and E is the set of edges of G, the goal is to determine whether there is a cut of G has at least c∗ edges.\nTheorem H.15 (Theorem 6.1 in [FLP15]). There exist constants a, b ∈ (0, 1) and a > b, such that, for a given MAX-CUT (see Definition H.14) instance graph G = (E, V ) which is an n-vertices 5-regular graph, if there is an algorithm in time 2o(n) which can distinguish the following two cases:\n1. At least one cut of the instance has at least a|E| edges,\n2. All cuts of the instance have at most b|E| edges,\nthen ETH(see Hypothesis H.12) fails.\nProof of Theorem H.13. We prove it by contradiction. We assume, for any given A ∈ {−1,+1}n×d, there is an algorithm can compute x̂ ∈ Rn, ŷ ∈ Rd s.t.\n‖A− x̂>ŷ‖1 ≤ (1 + 1\nW ) min x∈Rn,y∈Rd\n‖A− x>y‖1,\nin time poly(nd), where W = log1+γ d for arbitrarily small constant γ > 0. Then, we show the following. There exist constants a, b ∈ [0, 1], a > b, for a given MAX-CUT instance G = (V,E) with |E| = O(|V |), such that we can distinguish whether G has a cut with size at least a|E| or all the cuts of G have size at most b|E| in 2o(|V |) time, which leads to a contradiction to Theorem H.15.\nRecall the reduction shown in [GV15] which is from MAX-CUT to computing ‖ · ‖∞→1 for {−1,+1} matrices. We do similar things here: for a given graph G = (V,E) where |E| = O(|V |), we construct a matrix A ∈ {−1,+1}n×d where n = p|E| and d = p|V |. Notice that p is a parameter which will be determined later, and also p is a power of 2.\nWe divide the matrix A into |E| × |V | blocks, and each block has size p× p. For e ∈ [|E|], if the eth edge has endpoints i ∈ [|V |], j ∈ [|V |] and i < j, let all the p× p elements of (e, i) block of A be 1, all the p× p elements of (e, j) block of A be −1, and all the (e, l) block of A be p× p Hadamard matrix H for l 6= i, j.\nWe can construct the matrix in nd time, which is p2|E||V |. We choose p to be the smallest number of power of 2 which is larger than 2 2 a−b |V |\n1− γ10 for some γ > 0. Thus, the time for construction of the matrix A is O(nd) = 2O(|V |\n1− γ10 ). We will show, if we can compute a (1 + 1/W )-approximation to A, we can decide whether G has a cut with size at least a|E| or has no cut with size larger than b|E|. For convenience, we use T ∗ to denote ‖A‖∞→1 and use L∗ to denote\nmin x∈Rn,y∈Rd\n‖A− x>y‖1.\nAlso, we use L to denote a (1 + 1W ) relative error approximation to L ∗, which means:\nL∗ ≤ L ≤ (1 + 1 W )L∗.\nWe denote T as nd− L. Because L∗ ≤ L ≤ (1 + 1W )L ∗, we have:\nnd− L∗ ≥ nd− L ≥ nd− (1 + 1 W )L∗.\nDue to Lemma H.2 and the definition of T , it has:\nT ∗ ≥ T ≥ T ∗ − 1 W L∗.\nNotice that A is a {−1,+1} matrix, we have\nL∗ ≤ ‖A‖1 ≤ 2nd.\nThus,\nT ∗ ≥ T ≥ T ∗ − 2nd/W.\nIt means\nT + 2nd/W ≥ T ∗ ≥ T.\nClaim H.16 (Lower bound of ‖A‖∞→1, proof of Theorem 1 of [GV15]). If there is a cut of G with cut size at least c,\n‖A‖∞→1 ≥ 2p2c− |E||V |p3/2.\nClaim H.17 (Upper bound of ‖A‖∞→1, proof of Theorem 1 of [GV15]). If the max cut of G has fewer than c edges,\n‖A‖∞→1 ≤ 2p2(c− 1) + |E||V |p3/2.\nAccording to Claim H.16, if G has a cut with cut size at least a|E|, we have:\nT + 2nd/W ≥ T ∗ ≥ 2p2a|E| − |E||V |p3/2.\nThat is\nT ≥ 2p2a|E| − |E||V |p3/2 − 2nd/W. (36)\nAccording to Claim H.17, if the max cut of G has fewer than b|E| edges,\nT ≤ T ∗ ≤ 2p2b|E|+ |E||V |p3/2. (37)\nUsing these conditions p ≥ 2 2 a−b |V | 1− γ10 , d = p|V |,W ≥ log1+γ d, we can lower bound |W | by |V |\nup to some constant,\nW ≥ log1+γ d by W ≥ log1+γ d = log1+γ(p|V |) by d = p|V | = (log |V |+ log p)1+γ\n≥ (log |V |+ 2 a− b |V |1− γ 10 )1+γ by p ≥ 2 2 a−b |V |\n1− γ10\n≥ 2 a− b |V |. by (1− γ/10)(1 + γ) > 1 for γ small enough (38)\nThus, we can upper bound 1/W in the following sense,\n1 W ≤ a− b 2|V | ≤ a− b |V | − p− 1 2 , (39)\nwhere the first inequality follows by Equation (38) and the second inequality follows by p ≥ 2 2 a−b |V |\n1− γ10 , γ is sufficient small, and |V | is large enough. Now, we can conclude,\n1 W ≤ a− b |V | − p− 1 2\n⇐⇒ p2|E||V |/W ≤ (a− b)p2|E| − |E||V |p 3 2\nby multiplying p2|E||V | on both sides\n⇐⇒ 2nd/W ≤ 2(a− b)p2|E| − 2|E||V |p 3 2\nby multiplying 2 on both sides and p2|E||V | = nd ⇐⇒ 2p2b|E|+ |E||V |p3/2 < 2p2a|E| − |E||V |p3/2 − 2nd/W, (40)\nby adding 2p2b|E|+ |E||V |p3/2 − 2nd/W on both sides\nwhich implies that Equation (40) is equivalent to Equation (39). Notice that the LHS of (40) is exactly the RHS of (37) and the RHS of (40) is exactly the RHS of (36). Therefore, we can decide whether G has a cut with size larger than a|E| or has no cut with size larger than b|E|.\nThus, if we can compute x̂ ∈ Rn, ŷ ∈ Rd s.t.\n‖A− x̂>ŷ‖1 ≤ (1 + 1\nlog1+γ d ) min x∈Rn,y∈Rd\n‖A− x>y‖1,\nin poly(nd) time, which means we can compute L and T in poly(nd) time. Notice that nd = p2|E||V |, |E| = O(|V |), p ≥ 2 2 a−b |V | 1− γ10 , it means poly(nd) = 2O(|V | 1− γ10 ). Because we decide whether G has a cut with size larger than a|E| or has no cut with size larger than b|E| via the value of T , we can solve it in 2O(|V | 1− γ10 ) time which leads to a contradiction to Theorem H.15.\nH.4 Extension to the rank-k case\nThis section presents a way of reducing the rank-k case to the rank-1 case. Thus, we can obtain a lower bound for general k ≥ 1 under ETH.\nTheorem H.18. For any constants c1 > 0, c2 > 0 and c3 > 0, and any constant c4 ≥ 10(c1 + c2 + c3 + 1), given any matrix A ∈ Rn×n with absolute value of each entry bounded by nc1, we define a block diagonal matrix Ã ∈ R(n+k−1)×(n+k−1) as\nÃ =  A 0 0 · · · 0 0 B 0 · · · 0 0 0 B · · · 0 · · · · · · · · · · · · · · · 0 0 0 · · · B  ,\nwhere B = nc4. If Â is an `1-norm rank-k C-approximation solution to Ã, i.e.,\n‖Â− Ã‖1 ≤ C · min rank−k Â′\n‖Â′ − Ã‖1,\nwhere C ∈ [1, nc3 ], then there must exist j∗ ∈ [n] such that\nmin v∈Rn\n‖Â[1:n]j∗ v > −A‖1 ≤ C · min u,v∈Rn ‖uv> −A‖1 + 1/nc2 ,\ni.e., the first n coordinates of the column j∗ of Â can give an `1-norm rank-1 C-approximation to A.\nProof. The first observation is that because we can use a rank-1 matrix to fit A and use a rank-(k−1) matrix to fit other Bs, we have\nmin rank−k Â′ ‖Â′ − Ã‖1 ≤ min u,v∈Rn ‖uv> −A‖1 ≤ ‖A‖1. (41)\nClaim H.19. Let Â denote the rank-k C-approximate solution to Ã. Let Z ∈ R(n+k−1)×(k−1) denote the rightmost k − 1 columns of Â, then, rank(Z) = k − 1.\nProof. Consider the (k − 1) × (k − 1) submatrix Z [n+1:n+k−1] of Z. Each element on the diagonal of this submatrix should be at least B − C‖A‖1, and each element not on the diagonal of the submatrix should be at most C‖A‖1. Otherwise ‖Â − Ã‖1 > C‖A‖1 which will lead to a contradiction. Since B = nc4 is sufficiently large, Z [n+1:n+k−1] is diagonally dominant. Thus rank(Z) ≥ rank(Z [n+1:n+k−1]) = k − 1. Because Z only has k − 1 columns, rank(Z) = k − 1.\nClaim H.20. ∀x ∈ Rk−1, i ∈ [n], ∃j ∈ {n+ 1, n+ 2, · · · , n+ k − 1} such that,\n|(Zx)j | |(Zx)i| ≥ B 2(k − 1)C‖A‖1 .\nProof. Without loss of generality, we can let ‖x‖1 = 1. Thus, there exists j such that |xj | ≥ 1k−1 . So we have\n|(Zx)n+j | = | k−1∑ i=1 Zn+j,ixi|\n≥ |Zn+j,jxj | − ∑ i 6=j |Zn+j,ixi|\n≥ (B − C‖A‖1)|xj | − ∑ i 6=j |xi|C‖A‖1\n≥ (B − C‖A‖1)/(k − 1)− C‖A‖1\n≥ B 2(k − 1) .\nThe second inequality follows because |Zn+j,j | ≥ B−C‖A‖1 and ∀i 6= j, |Zn+j,i| ≤ C‖A‖1 (otherwise ‖Â−Ã‖1 > C‖A‖1 which leads to a contradiction.) The third inequality follows from |xj | > 1/(k−1) and ‖x‖1 = 1. The fourth inequality follows since B is large enough such that B2(k−1) ≥ C‖A‖1 k−1 + C‖A‖1. Now we consider any q ∈ [n]. We have\n|(Zx)q| = k−1∑ i=1 |Zq,ixi| ≤ max i∈[k−1] |Zq,i| · k−1∑ i=1 |xi| ≤ C‖A‖1.\nThe last inequality follows that ‖x‖1 = 1 and ∀i ∈ [k − 1], Zq,i ≤ C‖A‖1. Otherwise, ‖Â − Ã‖1 > C‖A‖1 which will lead to a contradiction.\nLook at |(Zx)n+j |/|(Zx)q|, it is greater than B2(k−1)C‖A‖1 .\nWe now look at the submatrix Â[1:n][1:n], we choose i ∗, j∗ ∈ [n] such that\n|Âi∗,j∗ | ≥ 1/nc2−2.\nIf there is no such (i∗, j∗), it means that we already found a good rank-1 approximation to A\n‖0−A‖1 ≤ ‖0− Â[1:n][1:n]‖1 + ‖Â [1:n] [1:n] −A‖1\n≤ ‖0− Â[1:n][1:n]‖1 + ‖Â− Ã‖1\n≤ 1/nc2 + ‖Â− Ã‖1 ≤ 1/nc2 + C min\nrank−k Â′ ‖Â′ − Ã‖1\n≤ 1/nc2 + C min u,v∈Rn ‖uv> −A‖1,\nwhere 0 is an n× n all zeros matrix. The second inequality follows since Â[1:n][1:n] − A is a submatrix of Â− Ã. The third inequality follows since each entry of Â[1:n][1:n] should be no greater than 1/n c2−2 (otherwise, we can find (i∗, j∗)). The last inequality follows from equation 41.\nClaim H.21. Âj∗ is not in the column span of Z, i.e.,\n∀x ∈ Rk−1, Âj∗ 6= Zx.\nProof. If there is an x such that Âj∗ = Zx, it means (Zx)i∗ = Âi∗,j∗ ≥ 1/nc2−2. Due to Claim H.20, there must exist i′ ∈ {n+ 1, n+ 2, · · · , n+ k − 1} such that (Zx)i′ ≥ B2(k−1)C‖A‖1nc2−2 . Since B is sufficiently large, Âi′,j∗ = (Zx)i′ > C‖A‖1 which implies that ‖Â− Ã‖1 > C‖A‖1, and so leads to a contradiction.\nDue to Claim H.19 and Claim H.21, the dimension of the subspace spanned by Âj∗ and the column space of Z is k. Since Â has rank at most k, it means that each column of Â can be written as a linear combination of Âj∗ and the columns of Z.\nNow consider the jth column Âj of Â for j ∈ [n]. We write it as\nÂj = αj · Âj∗ + Zxj .\nClaim H.22. ∀j ∈ [n], αj ≤ 2C‖A‖1nc2+2.\nProof. Otherwise, suppose αj > 2C‖A‖1nc2+2. We have\n|(Zxj)i∗ | ≥ αj · |Âi∗,j∗ | − |Âi∗,j |\n≥ αj · 1\nnc2−2 − |Âi∗,j |\n≥ 1 2 αj · 1 nc2−2 .\nThe second inequality follows from |Âi∗,j∗ | ≥ 1/nc2−2. The third inequality follows from |Âi∗,j | ≤ ‖A‖1 and 12αj · 1 nc2−2\n≥ C‖A‖1 ≥ ‖A‖1. Due to Claim H.20, there exists i ∈ {n+1, n+2, · · · , n+k−1} such that |(Zxj)i| ≥ B2(k−1)C‖A‖1 ·\n1 2αj · 1 nc2−2 . For sufficiently large B, we can have |(Zxj)i| ≥ αjB1/2. Then we look at\n|Âi,j | ≥ |(Zxj)i| − αj |Âi,j∗ | ≥ αj(B1/2 − C‖A‖1)\n≥ αj 1\n2 B1/2.\nThe second inequality follows by |Âi,j∗ | ≤ C‖A‖1, otherwise ‖Â− Ã‖1 > C‖A‖1 which will lead to a contradiction. The third inequality follows that B is sufficient large that 12B\n1/2 > C‖A‖1. Since |Âi,j | ≥ αj 12B\n1/2 > C‖A‖1, it contradicts to the fact ‖Â− Ã‖1 ≤ C‖A‖1. Therefore, ∀j ∈ [n], αj ≤ 2C‖A‖1nc2+2.\nClaim H.23. ∀j ∈ [n], i ∈ {n+ 1, n+ 2, · · · , n+ k − 1}, |(Zxj)i| ≤ 4C2‖A‖21nc2+2\nProof. Consider j ∈ [n], i ∈ {n+ 1, n+ 2, · · · , n+ k − 1}, we have\n|(Zxj)i| ≤ |Âi,j |+ αj |Âi,j∗ | ≤ C‖A‖1 + αj · C‖A‖1 ≤ 4C2‖A‖21nc2+2.\nThe second inequality follows by |Âi,j | ≤ C‖A‖1 and |Âi,j∗ | ≤ C‖A‖1, otherwise the ‖Â− Ã‖1 will be too large and leads to a contradiction. The third inequality is due to αj + 1 ≤ 4C‖A‖1nc2+2 via Claim H.22.\nClaim H.24. ∀j ∈ [n], ‖Â[1:n]j − αj · Â1:nj∗ ‖∞ ≤ 1/nc2−2\nProof. Due to Claim H.23 and Claim H.20, ∀i, j ∈ [n], we have\n|(Zxj)i| ≤ 4C2‖A‖21nc2+2\nB/(2(k − 1)C‖A‖1) ≤ 1/B1/2.\nThe second inequality follows for a large enough B. Therefore, ∀i, j ∈ [n],\n|Âi,j − αj · Âi,j∗ | ≤ |(Zxj)i| ≤ 1/B1/2 ≤ 1/nc2−2.\nThe last inequality follows since B is large enough.\nNow, let us show that Â1:nj∗ can provide a good rank-1 approximation to A:\n‖Â1:nj∗ α> −A‖1 ≤ ‖Â1:nj∗ α> − Â [1:n] [1:n]‖1 + ‖Â [1:n] [1:n] −A‖1\n= n∑ j=1 ‖αjÂj∗ − Â[1:n]j ‖1 + ‖Â [1:n] [1:n] −A‖1\n≤ n2 · 1/nc2−2 + ‖Â[1:n][1:n] −A‖1\n≤ 1/nc2 + ‖Â− Ã‖1 ≤ 1/nc2 + C min\nu,v∈Rn ‖uv> −A‖1.\nThe first inequality follows by triangle inequality. The first equality is due to the linearity of `1 norm. The second inequality is due to Claim H.24. The third inequality follows since Â[1:n][1:n] − A is a submatrix of Â− Ã. The fourth inequality is due to the equation 41."
    }, {
      "heading" : "I Limited independent Cauchy random variables",
      "text" : "This section presents the fundamental lemmas with limited independent Cauchy variables, which will be used in Section J and K. In Section I.1, we provide some notation, definitions and tools from previous work. Section I.2 includes the main result."
    }, {
      "heading" : "I.1 Notations and Tools",
      "text" : "For a function f : R → R and nonnegative integer `, f (`) denotes the `th derivative of f , with f (0) = f . We also often use x ≈ y to state that |x−y| = O( ). We use I[a,b] to denote the indicator function of the interval [a, b].\nTo optimize the communication complexity of our ditributed algorithm, we show that instead of using fully independent Cauchy variables, poly(k, d)-wise independent Cauchy variables suffice.\nWe start by stating two useful Lemmas from previous work [KNW10].\nLemma I.1 (Lemma 2.2 in [KNW10]). There exists an 0 > 0 such that the following holds. Let n be a positive integer and 0 < < 0, 0 < p < 2 be given. Let f : R→ R satisfy ‖f (`)‖∞ = O(α`) for all ` ≥ 0, for some α satisfying αp ≥ log(1/ ). Let k = αp. Let a ∈ Rn satisfy ‖a‖p = O(1). Let Xi be a 3Ck-independent family of p-stable random variables. Let X = ∑ i aiXi and Y = ∑ i aiYi. Then E[f(x)] = E[f(Y )] +O( ).\nLemma I.2 (Lemma 2.5 in [KNW10]). There exist constants c′, 0 > 0 such that for all c > 0 and 0 < < 0, and for all [a, b] ⊆ R, there exists a function Jc[a,b] : R→ R satisfying:\ni. ‖(Jc[a,b]) (`)‖∞ = O(c`) for all ` ≥ 0.\nii. For all x such that a, b /∈ [x− , x+ ], and as long as c > c′ −1 log3(1/ ), |Jc[a,b](x)−I[a,b](x)| < .\nI.2 Analysis of limited independent random Cauchy variables\nLemma I.3. Given a vector y ∈ Rn, choose Z to be the t × n random Cauchy matrices with 1/t rescaling and t = O(k log k). The variables from different rows are fully independent, and the variables from the same rows are O(1)-wise independent. Then, we have\n‖Zy‖1 & ‖y‖1\nholds with probability at least 1− 2−Ω(t).\nProof. Let S denote the original fully independent matrix and Z denote the matrix for which the entries in the same row are w-wise independent, and the entries from different rows are fully independent. Notice we define the random matrices without rescaling by 1/t and it will be added back at the end. (We will decide w later)\nWe define random variable X such that X = 1 if |(Zy)i| ≤ 150 and X = 0 otherwise. We also define random variable Y such that Y = 1 if |(Sy)i| ≤ 150 and Y = 0 otherwise. Then, we have\nE[X] = Pr [ |(Zy)i| ≤ 1\n50\n] = E [ I[− 1\n50 , 1 50\n]((Zy)i) ] E[Y ] = Pr [ |(Sy)i| ≤ 1\n50\n] = E [ I[− 1\n50 , 1 50\n]((Sy)i) ] The goal is to show that E[X] ≈ E[Y ]. Following the same idea from [KNW10], we need to argue this chain of inequalities,\nE[I[a,b](X)] ≈ E[Jc[a,b](X)] ≈ E[J c [a,b](Y )] ≈ E[I[a,b](Y )]\nUsing Lemma 2.2 and Lemma 2.5 from [KNW10], choosing sufficiently small constant (which implies w = O(1)), it follows that for each i ∈ [t], we still have\nPr [ |(Zy)i| > 1\n50 ‖y‖1\n] & Pr [ |(Sy)i| > 1\n50 ‖y‖1\n] ≥ 0.9\nBecause all rows of Z are fully independent, using the Chernoff bound we can get that\nPr [ ‖Zy‖1 . t‖y‖1 ] ≤ exp(−Ω(t))\nas we needed for the “no contraction” part of the net argument.\nFor the no dilation, we need to argue that\nLemma I.4. Given a set of vectors {y1, y2, . . . , yd} where yi ∈ Rn, ∀i ∈ [d], choose Z to be the t×n random Cauchy matrices with 1/t rescaling and t = O(k log k), where the variables from different rows are fully independent, and the variables from the same rows are w-wise independent.\nI. If w = Õ(dk), we have d∑ i=1 ‖Zyi‖1 ≤ O(log d) d∑ i=1 ‖yi‖1\nholds with probability at least .999. II. If If w = Õ(d), we have\nd∑ i=1 ‖Zyi‖1 ≤ O(k log d) d∑ i=1 ‖yi‖1\nholds with probability at least .999.\nProof. Let m = t. Let S ∈ Rm×n denote the original fully independent matrix and Z denote the matrix that for each entry in the same row are w-wise independent, where the entries from different rows are fully independent. (We will decide onw later)\nApplying matrix S to those fixed set of vectors, we have\nd∑ i=1 ‖Syi‖1 = d∑ i=1 m∑ j=1 | n∑ l=1 1 m Sj,l · (yi)l| = 1 m d∑ i=1 m∑ j=1 | n∑ l=1 Sj,l · (yi)l|\nApplying matrix Z to those fixed set of vectors, we have a similar thing,\nd∑ i=1 ‖Zyi‖1 = d∑ i=1 m∑ j=1 | n∑ l=1 1 m Zj,l · (yi)l| = 1 m d∑ i=1 m∑ j=1 | n∑ l=1 Zj,l · (yi)l|\nThe goal is to argue that, for any i ∈ [d], j ∈ [m],\nE [ | n∑ l=1 Zj,l · (yi)l| ∣∣∣∣ξ] . E[| n∑ l=1 Sj,l · (yi)l| ∣∣∣∣ξ]+ δ\nAs long as δ is small enough, we are in a good shape. Let X = 1‖yi‖1 ∑n l=1 Zj,l(yi)l, and Y = 1 ‖yi‖1 ∑m l=1 Sj,l(yi)l. Let D be the truncating threshold of each Cauchy random variable. Define T = O(logD). On one hand, we have\nE[|X||ξ] ≤ 2 Pr[X ∈ [0, 1]|ξ] · 1 + T∑ j=0 Pr[X ∈ (2j , 2j+1]|ξ] · 2j+1 \n≤ 2 1 + T∑ j=0 Pr[I(2j ,2j+1)(X) = 1|ξ] · 2j+1 \n= 2 1 + T∑ j=0 E[I(2j ,2j+1)(X)|ξ] · 2j+1  (42)\nOn the other hand, we can show\nE[|Y ||ξ] ≥ 2 Pr[X ∈ [0, 1]|ξ] · 0 + T∑ j=0 Pr[Y ∈ (2j , 2j+1]|ξ] · 2j \n≥ 2 T∑ j=0 Pr[I(2j ,2j+1](Y ) = 1|ξ] · 2j ≥ 2 T∑ j=0 E[I(2j ,2j+1](Y )|ξ] · 2j (43)\nThus, we need to show that, for each j,\nE[I(2j ,2j+1)(X)|ξ] ≈ E[I(2j ,2j+1](Y )|ξ] (44)\nFollowing the same idea from [KNW10], we need to argue this chain of inequalities,\nE[I[a,b](X)] ≈ E[Jc[a,b](X)] ≈ E[J c [a,b](Y )] ≈ E[I[a,b](Y )]\nWe first show E[I[a,b](X)] ≈ E[Jc[a,b](X)]. Notice that I[a,b] and J[a,b] are within everywhere except for two intervals of length O( ). Also the Cauchy distribution is anticoncentrated (any length-O( ) interval contains O( ) probability mass) and ‖I[a,b]‖∞, ‖Jc[a,b]‖∞ = O(1), these intervals contribute O( ) to the difference.\nSecond, we show E[Jc[a,b](X)] ≈ E[J c [a,b](Y )]. This directly follows by Lemma I.1 by choosing\nα = O( −1 log3(1/ )).\nThird, we show E[Jc[a,b](Y )] ≈ E[I[a,b](Y )]. The argument is similar as the first step, but we need to show anticoncentration of Y . Suppose for any t ∈ R we had a nonnegative function f ,t : R→ R symmetric about t satisfying:\nI. ‖f (`)t, ‖∞ = O(α`) for all ` ≥ 0, with α = O(1/ ) II. E[ft, (z)] = O( ) for z ∼ D1 III. ft, (t+ ) = Ω(1) IV. ft, (x) is strictly decreasing as |x− t| → ∞\nBy I, II and Lemma I.1 we could have E[f ,t(Y )] ≈ E[ft, (z)] = O( ). Then, E[ft, (Y )] ≥ ft, (t+ ) · Pr[Y ∈ [t− , t+ ]] = Ω(Pr[Y ∈ [t− , t+ ]]) by III and IV, implying anticoncentration in [t− , t+ ] as desired. For the details of function ft, , we refer the readers to Section A.4 in [KNW10].\nNow, combining Equation (42), (43) and (44) gives\nE [ | n∑ l=1 Zj,l · (yi)l| ∣∣∣∣ξ] . E[| n∑ l=1 Sj,l · (yi)l| ∣∣∣∣ξ]+ T∑ j=0 2j · · ‖yi‖1\n. E [ | n∑ l=1 Sj,l · (yi)l| ∣∣∣∣ξ]+D · · ‖yi‖1\nOverall, for the fixed j, Sj,l is Õ(1/ )-independent family of Cauchy random variable. Choosing D = O(dk) and = O(1/D), we can show\n1\nm d∑ i=1 m∑ j=1 E [ | n∑ l=1 Sj,l · (yi)l| ∣∣∣∣ξ] ≤ O(log d) d∑ i=1 ‖yi‖1\nas before. Notice that D ‖yi‖1 = O(‖yi‖1). Thus, we complete the proof of the first result. Choosing D = O(dk) and = O(1/d), the dominant term becomes D ‖yi‖1 = O(k‖yi‖1). Thus, we complete the proof of second result.\nCorollary I.5. Given U ∈ Rn×k, let Z ∈ Rt×n be the same as the matrix stated in the Lemma I.3, then with probability at least .95,\n∀x ∈ Rk, ‖ZUx‖1 & ‖Ux‖1.\nThe proof is very similar to the proof of Lemma D.22. Without loss of generality, we can suppose U is a well-conditioned basis. Due to Lemma I.4, with arbitrarily high constant probability ‖ZU‖1 is bounded by poly(t, k). By simply applying the net argument and using Lemma I.3 to take a union bound over net points, we can get the above corollary."
    }, {
      "heading" : "J Streaming Setting",
      "text" : "Section J.1 provides some notation and definitions about row-update streaming model and the turnstile streaming model. For some recent developments of row-update streaming and turnstile streaming models, we refer the readers to [CW09, KL11, GP13, Lib13, KLM+14, BWZ16] and the references therein. Section J.2 presents our turnstile streaming algorithm. Section J.3 presents our row-update streaming algorithm."
    }, {
      "heading" : "J.1 Definitions",
      "text" : "Definition J.1 (Row-update model). Let matrix A ∈ Rn×d be a set of rows A1, · · · , An. In the row-update streaming model, each row of A will occur in the stream exactly once. But the rows can be in arbitrary order. An algorithm in this model is only allowed a single pass over these rows. At the end of the stream, the algorithm stores some information of A. The space of the algorithm is the total number of words required to store this information during the stream. Here, each word is O(log(nd)) bits.\nDefinition J.2 (Turnstile model). At the beginning, let matrix A ∈ Rn×d be a zero matrix. In the turnstile streaming model, there is a stream of update operations, and the ith operation has the form (xi, yi, ci) which means that Axi,yi should be incremented by ci. An algorithm in this model is only allowed a single pass over the stream. At the end of the stream, the algorithm stores some information of A. The space complexity of the algorithm is the total number of words required to store this information during the stream. Here, each word is O(log(nd)) bits.\nJ.2 Turnstile model, poly(k, log(d), log(n)) approximation\nDefinition J.3 (Turnstile model `1-low rank approximation - rank-k subspace version). Given matrix A ∈ Rn×d and k ∈ N+, the goal is to propose an algorithm in the streaming model of Definition J.2 such that\n1. Upon termination, the algorithm outputs a matrix V ∗ ∈ Rk×d.\n2. V ∗ satisfies that\nmin U∈Rn×k ‖A− UV ∗‖1 ≤ poly(k, log(d), log(n)) · min U∈Rn×k,V ∈Rk×d ‖A− UV ‖1.\n3. The space complexity is as small as possible\nTheorem J.4. Suppose A ∈ Rn×d is given in the turnstile streaming model (See Definition J.2), there is an algorithm(in Algorithm 8 without decomposition) which solves the problem in Definition J.3 with constant probability. Further, the space complexity of the algorithm is poly(k) + Õ(kd) words.\nProof. Correctness. The correctness is implied by (IV) of Lemma D.11, and the proof of Theorem C.3. Notice that L = T1AR,N = SAT2,M = T1AT2, so X̂ ∈ RO(k log k)×O(k log k) minimizes\nmin rank−k X\n‖T1ARXSAT2 − T1AT2‖F .\nAccording to the proof of Theorem C.3, ARX̂SA gives a `1 rank-k poly(k, log(d), log(n))-approximation to A. Because X̂ = Û Σ̂V̂ >, V ∗ = Σ̂V̂ >SA satisfies:\nmin U∈Rn×k ‖A− UV ∗‖1 ≤ poly(k, log(d), log(n)) · min U∈Rn×k,V ∈Rk×d ‖A− UV ‖1.\nSpace complexity. Generating Õ(kd)-wise independent random Cauchy variables needs Õ(kd) bits. The size of L,N and M are k2 log2 k, k2 log3 k and k2 log3 k words separately. So the space of maintaining them is O(k2 log3 k) words. The size of D is O(k log k)× d, so maintaining it needs O(kd log k) words. Therefore, the total space complexity of the algorithm is poly(k)+ Õ(kd) words.\nIt is easy to extend our algorithm to output a decomposition. The formal definition of the decomposition problem is as the following:\nDefinition J.5 (Turnstile model `1-low rank approximation - rank-k decomposition version). Given matrix A ∈ Rn×d and k ∈ N+, the goal is to propose an algorithm in the streaming model of Definition J.2 such that\n1. Upon termination, the algorithm outputs a matrix U∗ ∈ Rn×k, V ∗ ∈ Rk×d.\n2. U∗, V ∗ satisfies\n‖A− U∗V ∗‖1 ≤ poly(k, log(d), log(n)) · min U∈Rn×k,V ∈Rk×d ‖A− UV ‖1.\n3. The space complexity is as small as possible\nTheorem J.6. Suppose A ∈ Rn×d is given by the turnstile streaming model (See Definition J.2). There is an algorithm( in Algorithm 8 with decomposition) which solves the problem in Definition J.5 with constant probability. Further, the space complexity of the algorithm is poly(k) + Õ(k(d + n)) words.\nProof. Correctness. The only difference from the Algorithm 8 (without decomposition) is that the algorithm maintains C. Thus, finally it can compute U∗ = ARÛ . Notice that U∗V ∗ = ARX̂SA, according to the proof of Theorem C.3, U∗V ∗ gives a `1 rank-k poly(k, log(d), log(n))-approximation to A.\nSpace complexity. Since the size of C is O(nk log k) words, the total space is poly(k)+Õ(k(d+ n)) words.\nJ.3 Row-update model, poly(k) log d approximation\nDefinition J.7 (Row-update model `1-low rank approximation - rank-k subspace version). Given matrix A ∈ Rn×d and k ∈ N+, the goal is to propose an algorithm in the streaming model of Definition J.1 such that\n1. Upon termination, the algorithm outputs a matrix V ∗ ∈ Rk×d.\n2. V ∗ satisfies that\nmin U∈Rn×k ‖A− UV ∗‖1 ≤ poly(k) log(d) · min U∈Rn×k,V ∈Rk×d ‖A− UV ‖1.\n3. The space complexity is as small as possible\nTheorem J.8. Suppose A ∈ Rn×d is given by the row-update streaming model (See Definition J.1), there is an algorithm( in Algorithm 9 without decomposition ) which solves the problem in Definition J.7 with constant probability. Further, the space complexity of the algorithm is poly(k) + Õ(kd) words.\nAlgorithm 8 Turnstile Streaming Algorithm 1: procedure TurnstileStreaming(k,S) 2: Construct sketching matrices S ∈ RO(k log k)×n, R ∈ Rd×O(k log k), T1 ∈ RO(k log k)×n, T2 ∈\nRd×O(k log2 k) where R, T2 are fully independent random Cauchy matrices, and S, T1 are random Cauchy matrices with fully independent variables across different rows and Õ(d)-wise independent variables from the same row.\n3: Initialize matrices: 4: L← {0}O(k log k)×O(k log k), N ← {0}O(k log k)×O(k log2 k). 5: M ← {0}O(k log k)×O(k log2 k), D ← {0}O(k log k)×d. 6: if need decomposition then 7: C ← {0}n×O(k log k). 8: end if 9: for i ∈ [l] do\n10: Receive update operation (xi, yi, ci) from the data stream S. 11: for r = 1→ O(k log k), s = 1→ O(k log k) do 12: Lr,s ← Lr,s + T1r,xi · ci ·Ryi,s. 13: end for 14: for r = 1→ O(k log k), s = 1→ O(k log2 k) do 15: Nr,s ← Nr,s + Sr,xi · ci · T2yi,s. 16: end for 17: for r = 1→ O(k log k), s = 1→ O(k log2 k) do 18: Mr,s ←Mr,s + T1r,xi · ci · T2yi,s. 19: end for 20: for r = 1→ O(k log k) do 21: Dr,yi ← Dr,s + Sr,xi · ci. 22: end for 23: if need decomposition then 24: for s = 1→ O(k log k) do 25: Cxi,s ← Cxi,s + ci ·Ryi,s. 26: end for 27: end if 28: end for 29: Compute the SVD of L = ULΣLV >L . 30: Compute the SVD of N = UNΣNV >N . 31: Compute X̂ = L†(ULU>LMVNV > N )kN\n†. 32: Compute the SVD of X̂ = Û Σ̂V̂ >. 33: if need decomposition then 34: return V ∗ = Σ̂V̂ >D,U∗ = CÛ . 35: else 36: return V ∗ = Σ̂V̂ >D. 37: end if 38: end procedure\nProof. Correctness. Notice that L = T1BR,N = SBT2,M = T1BT2. Thus, X̂ ∈ RO(k log k)×O(k log k) actually minimizes\nmin rank−k X\n‖T1BRXSBT2 − T1BT2‖F .\nAlso notice that B is just taking each row of A and replacing it with its nearest point in the row span of S′A. According to the proof of Theorem C.6 and (IV) of Lemma D.11 BRX̂SB gives a poly(k) log d `1 norm rank-k approximation to A. Since X̂ = Û Σ̂V̂ >, V ∗ = Σ̂V̂ >SB satisfies:\nmin U∈Rn×k ‖A− UV ∗‖1 ≤ poly(k, log(d), log(n)) · min U∈Rn×k,V ∈Rk×d ‖A− UV ‖1.\nSpace complexity. Constructing sketching matrices needs Õ(kd) bits to store random seeds. Maintaining L,N,M needs O(k2 log3 k) words. The cost of storing X̂ is also O(k2 log2 k) words. Maintaining D needs O(kd log k) words. Therefore, the total space cost of the algorithm is poly(k)+ Õ(kd) words.\nIt is easy to extend our algorithm to output a decomposition. The formal definition of the decomposition problem is as the following:\nDefinition J.9 (Row-update model `1-low rank approximation - rank-k decomposition version). Given matrix A ∈ Rn×d and k ∈ N+, the goal is to propose an algorithm in the streaming model of Definition J.1 such that\n1. Upon termination, the algorithm outputs matrices U∗ ∈ Rn×k, V ∗ ∈ Rk×d.\n2. U∗, V ∗ satisfies that\n‖A− U∗V ∗‖1 ≤ poly(k) log d · min U∈Rn×k,V ∈Rk×d ‖A− UV ‖1.\n3. The space complexity is as small as possible.\nTheorem J.10. Suppose A ∈ Rn×d is given by the row-update streaming model (See Definition J.1), there is an algorithm(in Algorithm 9 with decomposition ) which solves the problem in Definition J.9 with constant probability. Further, the space complexity of the algorithm is poly(k) + Õ(k(n + d)) words.\nProof. Correctness. The only difference is that the above algorithm maintains C. Thus, We can compute U∗ = CÛ in the end. Notice that U∗V ∗ = BRX̂SB, according to the proof of Theorem C.6, U∗V ∗ gives a poly(k) log d `1 norm rank-k approximation to A.\nSpace complexity. Since the size of C is nk log k words, the total space is poly(k)+Õ(k(n+d)) words."
    }, {
      "heading" : "K Distributed Setting",
      "text" : "Section K.1 provides some notation and definitions for the Row-partition distributed model and the Arbitrary-partition model. These two models were recently studied in a line of works such as [TD99, QOSG02, BCL05, BRB08, MBZ10, FEGK13, PMvdG+13, KVW14, BKLW14, BLS+16, BWZ16, WZ16]. Section K.2 and K.3 presents our distributed protocols for the Arbitrary-partition distributed model. Section K.4 and K.5 presents our distributed protocols for the Row-partition distributed model.\nAlgorithm 9 Row Update Streaming Algorithm 1: procedure RowUpdateStreaming(k,S) 2: Construct sketching matrices S′ ∈ RO(k log k)×n, S ∈ RO(k log k)×n, R ∈ Rd×O(k log k), T1 ∈\nRO(k log k)×n, T2 ∈ Rd×O(k log 2 k) where R, T2 are fully independent random Cauchy variables, and S , S′ , T1 are random Cauchy matrices with fully independent random variables from different rows and Õ(d)-wise independent in the same row.\n3: Initialize matrices: 4: L← {0}O(k log k)×O(k log k), N ← {0}O(k log k)×O(k log2 k). 5: M ← {0}O(k log k)×O(k log2 k), D ← {0}O(k log k)×d. 6: if need decomposition then 7: C ← {0}n×O(k log k). 8: end if 9: for i ∈ [n] do\n10: Recieve a row update (i, Ai) from the data stream S. 11: Compute Y ∗i ∈ R1×O(k log k) which minimizes minY ∈R1×O(k log k) ‖Y S′:,iAi −Ai‖1. 12: Compute Bi = Y ∗i S ′ :,iAi. 13: for r = 1→ O(k log k), s = 1→ O(k log k), j = 1→ d do 14: Lr,s ← Lr,s + T1r,i ·Bi,j ·Rj,s. 15: end for 16: for r = 1→ O(k log k), s = 1→ O(k log2 k), j = 1→ d do 17: Nr,s ← Nr,s + Sr,i ·Bi,j · T2j,s. 18: end for 19: for r = 1→ O(k log k), s = 1→ O(k log2 k), j = 1→ d do 20: Mr,s ←Mr,s + T1r,i ·Bi,j · T2j,s. 21: end for 22: for r = 1→ O(k log k), j = 1→ d do 23: Dr,j ← Dr,j + Sr,i ·Bi,j . 24: end for 25: if need decomposition then 26: for s = 1→ O(k log k), j = 1→ d do 27: Ci,s := Ci,s +Bi,j ·Rj,s. 28: end for 29: end if 30: end for 31: Compute the SVD of L = ULΣLV >L . 32: Compute the SVD of N = UNΣNV >N . 33: Compute X̂ = L†(ULU>LMVNV > N )kN\n†. 34: Compute the SVD of X̂ = Û Σ̂V̂ >. 35: if need decomposition then 36: return V ∗ = Σ̂V̂ >D,U∗ = CÛ . 37: else 38: return V ∗ = Σ̂V̂ >D. 39: end if 40: end procedure"
    }, {
      "heading" : "K.1 Definitions",
      "text" : "Definition K.1 (Row-partition model [BWZ16]). There are s machines, and the ith machine has a matrix Ai ∈ Rni×d as input. Suppose n = ∑s i=1 ni, and the global data matrix A ∈ Rn×d is denoted\nas  A1 A2 · · · As  , we say A is row-partitioned into these s matrices distributed in s machines respectively. Furthermore, there is a machine which is a coordinator. The model only allows communication between the machines and the coordinator. The communication cost in this model is the total number of words transferred between machines and the coordinator. Each word is O(log(snd)) bits.\nDefinition K.2 (Arbitrary-partition model [BWZ16]). There are s machines, and the ith machine has a matrix Ai ∈ Rn×d as input. Suppose the global data matrix A ∈ Rn×d is denoted as A = ∑s i=1Ai. We say A is arbitrarily partitioned into these s matrices distributed in s machines respectively. Furthermore, there is a machine which is a coordinator. The model only allows communication between the machines and the coordinator. The communication cost in this model is the total number of words transferred between machines and the coordinator. Each word is O(log(snd)) bits.\nK.2 Arbitrary-partition model, subspace, poly(k, log(d), log(n)) approximation\nDefinition K.3 (Arbitrary-partition model `1-low rank approximation - rank-k subspace version). Given matrix A ∈ Rn×d arbitrarily partitioned into s matrices A1, A2, · · · , As distributed in s machines respectively, and k ∈ N+, the goal is to propose a protocol in the model of Definition K.2 such that\n1. Upon termination, the protocol leaves a matrix V ∗ ∈ Rk×d on the coordinator.\n2. V ∗ satisfies that\nmin U∈Rn×k ‖A− UV ∗‖1 ≤ poly(k, log(d), log(n)) · min U∈Rn×k,V ∈Rk×d ‖A− UV ‖1.\n3. The communication cost is as small as possible\nTheorem K.4. Suppose A ∈ Rn×d is partitioned in the arbitrary partition model (See Definition K.2). There is a protocol(in Algorithm 10) which solves the problem in Definition K.3 with constant probability. Further, the communication complexity of the protocol is s(poly(k) + Õ(kd)) words.\nProof. Correctness. The correctness is shown by the proof of Theorem C.3 and (IV) of Lemma D.11. Notice that X̂ ∈ RO(k log k)×O(k log k) minimizes\nmin rank−k X\n‖LXN −M‖F .\nwhich is min\nrank−k X ‖T1ARXSAT2 − T1AT2‖F .\nAlgorithm 10 Arbitrary Partition Distributed Protocol 1: procedure ArbitraryPartitionDistributedProtocol(k,s,A) 2: A ∈ Rn×d was arbitrarily partitioned into s matrices A1, · · · , As ∈ Rn×d distributed in s\nmachines. 3: Coordinator Machines i 4: Chooses a random seed. 5: Sends it to all machines. 6: −−−−−−−−− > 7: Agrees on R, T2 which are fully 8: independent random Cauchy matrices. 9: Agrees on S, T1 which are random Cauchy 10: matrices with fully independent entries 11: from different rows, and Õ(d)-wise indepen12: dent variables from the same row. 13: Computes Li = T1AiR,Ni = SAiT2. 14: Computes Mi = T1AiT2. 15: Sends Li, Ni,Mi to the coordinator. 16: < −−−−−−−−− 17: Computes L =\ns∑ i=1 Li, N = s∑ i=1 Ni.\n18: Computes M = s∑ i=1 Mi. 19: Computes the SVD of L = ULΣLV >L . 20: Computes the SVD of N = UNΣNV >N . 21: Computes X̂ = L†(ULU>LMVNV > N )kN\n†. 22: Sends X̂ to machines. 23: −−−−−−−−− > 24: Computes the SVD of X̂ = Û Σ̂V̂ >. 25: Computes V ∗i = Σ̂V̂\n>SAi. 26: If need decomposition 27: U∗i = AiRÛ . 28: Sends U∗i , V ∗ i to the coordinator. 29: Else 30: Sends V ∗i to the coordinator. 31: Endif 32: < −−−−−−−−− 33: If need decomposition, 34: return V ∗ = ∑s i=1 V ∗ i , U ∗ = ∑s i=1 U ∗ i . 35: Else 36: return V ∗ = ∑s i=1 V ∗ i . 37: Endif 38: end procedure\nAccording to the proof of Theorem C.3, ARX̂SA gives an `1 rank-k poly(k, log(d), log(n))-approximation to A. Because X̂ = Û Σ̂V̂ >, V ∗ = Σ̂V̂ >SA satisfies:\nmin U∈Rn×k ‖A− UV ∗‖1 ≤ poly(k, log(d), log(n)) · min U∈Rn×k,V ∈Rk×d ‖A− UV ‖1.\nCommunication complexity. Since the random seed generates Õ(kd)-wise independent random Cauchy variables, the cost of line 5 is Õ(skd) bits. The size of Li, Ni andMi are k2 log2 k, k2 log3 k and k2 log3 k words separately. So the cost of line 15 is O(sk2 log3 k) words. Because the size of X̂ is O(k2 log2 k), the cost of line 22 is O(sk2 log2 k) words. line 30 needs skd words of communication. Therefore, the total communication of the protocol is s(poly(k) + Õ(kd)) words.\nK.3 Arbitrary-partition model, decomposition, poly(k, log(d), log(n)) approximation\nDefinition K.5 (Arbitrary-partition model `1-low rank approximation - rank-k decomposition version). Given matrix A ∈ Rn×d arbitrarily partitioned into s matrices A1, A2, · · · , As distributed in s machines respectively, and k ∈ N+, the goal is to propose a protocol in the model of Definition K.2 such that\n1. Upon termination, the protocol leave matrices U∗ ∈ Rn×k, V ∗ ∈ Rk×d on the coordinator.\n2. U∗, V ∗ satisfies that\n‖A− U∗V ∗‖1 ≤ poly(k, log(d), log(n)) · min U∈Rn×k,V ∈Rk×d ‖A− UV ‖1.\n3. The communication cost is as small as possible.\nTheorem K.6. Suppose A ∈ Rn×d is partitioned in the arbitrary partition model (See Definition K.2). There is a protocol(in Algorithm 10 with decomposition) which solves the problem in Definition K.5 with constant probability. Further, the communication complexity of the protocol is s(poly(k) + Õ(k(d+ n))) words.\nProof. Correctness. The only difference from the protocol (without decomposition) in Section K.2 is that the protocol sends Ui. Thus, the coordinator can compute U∗ = ARÛ . Notice that U∗V ∗ = ARX̂SA. According to the proof of Theorem C.3, U∗V ∗ gives a `1 rank-k poly(k, log(d), log(n))approximation to A.\nCommunication complexity. Since the size of Ui is kn words, the total communication is s(poly(k) + Õ(k(d+ n))) words.\nK.4 Row-partition model, subspace, poly(k) log d approximation\nDefinition K.7 (Row-partition model `1-low rank approximation - rank-k subspace version). Given matrix A ∈ Rn×d row-partitioned into s matrices A1, A2, · · · , As distributed in s machines respectively, and k ∈ N+, the goal is to propose a protocol in the model of Definition K.1 such that\n1. Upon termination, the protocol leaves a matrix V ∗ ∈ Rk×d on the coordinator.\n2. V ∗ satisfies that\nmin U∈Rn×k ‖A− UV ∗‖1 ≤ poly(k) log d · min U∈Rn×k,V ∈Rk×d ‖A− UV ‖1.\n3. The communication cost is as small as possible\nTheorem K.8. Suppose A ∈ Rn×d is partitioned in the row partition model (See Definition K.1). There is a protocol(in Algorithm 11 without decomposition) which solves the problem in Definition K.7 with constant probability. Further, the communication complexity of the protocol is s(poly(k)+ Õ(kd)) words.\nAlgorithm 11 Row Partition Distributed Protocol 1: procedure RowPartitionDistributedProtocol(k,s,A) 2: A ∈ Rn×d was row partitioned into s matrices A1 ∈ Rn1×d, · · · , As ∈ Rns×d distributed in s\nmachines. 3: Coordinator Machines i 4: Chooses a random seed. 5: Sends it to all machines. 6: −−−−−−−−− > 7: Agrees on R, T2 which are fully 8: independent random Cauchy variables. 9: Generates random Cauchy matrices\n10: S′i ∈ RO(k log k)×ni , Si, T1i ∈ RO(k log 2 k)×ni . 11: Computes Y ∗i = arg min Y ∈Rni×O(k log k) ‖Y S′iAi −Ai‖1. 12: Computes Bi = Y ∗i S ′ iAi. 13: Computes Li = T1iBiR,Ni = SiBiT2. 14: Computes Mi = T1iBiT2. 15: Sends Li, Ni,Mi to the coordinator. 16: < −−−−−−−−− 17: Computes L = ∑s i=1 Li, N = ∑s i=1Ni.\n18: Computes M = ∑s\ni=1Mi. 19: Computes the SVD of L = ULΣLV >L . 20: Computes the SVD of N = UNΣNV >N . 21: Computes X̂ = L†(ULU>LMVNV > N )kN\n†. 22: Sends X̂ to machines. 23: −−−−−−−−− > 24: Computes the SVD of X̂ = Û Σ̂V̂ >. 25: Computes V ∗i = Σ̂V̂\n>SiBi. 26: If need decomposition 27: Computes U∗i = BiRÛ . 28: Sends U∗i , V ∗ i to the coordinator. 29: Else 30: Sends V ∗i to the coordinator. 31: Endif 32: < −−−−−−−−− 33: If need decomposition 34: return V ∗ = ∑s i=1 V ∗ i , U ∗ = ∑s i=1 U ∗ i . 35: Else 36: return V ∗ = ∑s i=1 V ∗ i . 37: Endif 38: end procedure\nProof. Correctness. For convenience, we denote matrices B ∈ Rn×d, S ∈ RO(k log2 k)×n and T1 ∈ RO(k log2 k)×n as\nB =  B1 B2 · · · Bs  S = ( S1 S2 · · · Ss ) T1 = ( T11 T12 · · · T1s ) . Notice that L = T1BR,N = SBT2,M = T1BT2. Thus, X̂ ∈ RO(k log k)×O(k log k) actually minimizes\nmin rank−k X\n‖T1BRXSBT2 − T1BT2‖F .\nAlso notice that B is just taking each row of A and replacing it with its nearest point in the row span of S′A. According to the proof of Theorem C.6, BRX̂SB gives a poly(k) log d `1 norm rank-k approximation to A. Since X̂ = Û Σ̂V̂ >, V ∗ = Σ̂V̂ >SB satisfies:\nmin U∈Rn×k ‖A− UV ∗‖1 ≤ poly(k, log(d), log(n)) · min U∈Rn×k,V ∈Rk×d ‖A− UV ‖1.\nCommunication complexity. Since the R and T2 are Õ(kd)-wise independent, line 6 needs O(sW ) bits of communication. Line 16 needsO(sk2 log3 k) words. The cost of line 23 isO(sk2 log2 k) words. Line 32 needs skd words of communication. Therefore, the total communication of the protocol is s(poly(k) + Õ(kd)) words.\nK.5 Row-partition model, decomposition, poly(k) log d approximation\nDefinition K.9 (Row-partition model `1-low rank approximation - rank-k decomposition version). Given matrix A ∈ Rn×d row partitioned into s matrices A1, A2, · · · , As distributed in s machines respectively, and a positive integer k < rank(A), the goal is to propose a protocol in the model of Definition K.1 such that\n1. Upon termination, the protocol leaves matrices U∗ ∈ Rn×k, V ∗ ∈ Rk×d on the coordinator.\n2. U∗, V ∗ satisfies that\n‖A− U∗V ∗‖1 ≤ poly(k) log d · min U∈Rn×k,V ∈Rk×d ‖A− UV ‖1.\n3. The communication cost is as small as possible.\nTheorem K.10. Suppose A ∈ Rn×d is partitioned in the row partition model (See Definition K.1). There is a protocol(in Algorithm 11 with decomposition) which solves the problem in Definition K.9 with constant probability. Further, the communication complexity of the protocol is s(poly(k) + Õ(k(n+ d))) words.\nProof. Correctness. The only difference is that the above protocol sends Ui. Thus, the coordinator can compute U∗ = BRÛ . Notice that U∗V ∗ = BRX̂SB, according to the proof of Theorem C.6, U∗V ∗ gives a poly(k) log d `1 norm rank-k approximation to A.\nCommunication complexity. Since the size of Ui is kn words, the total communication is s(poly(k) + Õ(k(n+ d))) words."
    }, {
      "heading" : "L Experiments and Discussions",
      "text" : "In this section, we provide some counterexamples for the other heuristic algorithms such that, for those examples, the heuristic algorithms can output a solution with a very “bad” approximation ratio, i.e., nc, where c > 0 and the input matrix has size n× n. We not only observe that heuristic algorithms sometimes have very bad performance in practice, but also give a proof in theory."
    }, {
      "heading" : "L.1 Setup",
      "text" : "We provide some details of our experimental setup. We obtained the R package of [KK05, Kwa08, BDB13] from https://cran.r-project.org/web/packages/pcaL1/index.html. We also implemented our algorithm and the r1-pca algorithm [DZHZ06] using the R language. The version of the R language is 3.0.2. We ran experiments on a machine with Intel X5550@2.67GHz CPU and 24G memory. The operating system of that machine is Linux Ubuntu 14.04.5 LTS. All the experiments were done in single-threaded mode."
    }, {
      "heading" : "L.2 Counterexample for [DZHZ06]",
      "text" : "The goal is to find a rank k = 1 approximation for matrix A. For any ∈ [0, 0.5), we define A ∈ Rn×n as\nA =\n[ n1.5+ 0\n0 0\n] + [ 0 0 0 B ] , (45)\nwhere B ∈ R(n−1)×(n−1) is all 1s matrix. It is immediate that the optimal cost is at most n1.5+ . However, using the algorithm in [DZHZ06], the cost is at least Ω(n2). Thus, we can conclude,\nusing algorithm [DZHZ06] to solve `1 low rank approximation problem on A cannot achieve an approximation ratio better than n0.5− ."
    }, {
      "heading" : "L.3 Counterexample for [BDB13]",
      "text" : "The goal is to find a rank k = 1 approximation for matrix A. The input matrix A ∈ Rd×d for algorithm [BDB13] is defined to be,\nA =\n[ n1.5 0\n0 0\n] + [ 0 0 0 B ] , (46)\nwhere B ∈ R(n−1)×(n−1) is an all 1s matrix. It is immediate that the optimal cost is at most n1.5. Now, let us look at the procedure of [BDB13]. Basically, the algorithm described in [BDB13] is that they first find a rank n − 1 approximation via a best `1-fit hyperplane algorithm, then they rotate it based on the right singular vectors of the rank n− 1 approximation matrix, and next they recursively do the same thing for the rotated matrix which has only n− 1 columns.\nWhen running their algorithm on A, they will fit an arbitrary column except the first column of A. Without loss of generality, it just fits the last column of A. After the rotation, the matrix will be an n× (n− 1) matrix:  n1.5 0 0 · · · 0 0 √ n− 2 0 · · · 0 0 √ n− 2 0 · · · 0\n· · · · · · · · · · · · · · · 0 √ n− 2 0 · · · 0\n .\nThen, after the tth iteration for t < (n− 1), they will get an n× (n− t) matrix: n1.5 0 0 · · · 0 0 √ n− 2 0 · · · 0 0 √ n− 2 0 · · · 0\n· · · · · · · · · · · · · · · 0 √ n− 2 0 · · · 0  . This means that their algorithm will run on an n× 2 matrix: n1.5 0 0 √ n− 2 0 √ n− 2\n· · · · · · 0 √ n− 2\n ,\nin the last iteration. Notice that n × √ n− 2 < n1.5. This means that their algorithm will fit the first column which implies that their algorithm will output a rank-1 approximation to A by just fitting the first column of A. But the cost of this rank-1 solution is (n− 1)2. Since the optimal cost is at most n1.5, their algorithm cannot achieve an approximation ratio better than n0.5."
    }, {
      "heading" : "L.4 Counterexample for [Kwa08]",
      "text" : "We show that the algorithm [Kwa08] cannot achieve an approximation ratio better than Θ(n) on the matrix A ∈ R(2n+1)×(2n+1) defined as,\nA = n1.5 0 00 B 0 0 0 B  , (47) where B is an n×n matrix that contains all 1s. We consider the rank-2 approximation problem for this input matrix A. The optimal cost is at most n1.5.\nWe run their algorithm. Let x0 denote the initial random unit vector. Consider the sign vector s ∈ {±1}2n+1 where each entry is the sign of the inner product between x0 and each column of A. There are only three possibilities,\ns =  (1, {1}n, {1}n) (1, {1}n, {−1}n) (1, {−1}n, {1}n) .\nCase I, s = (1, {1}n, {1}n). Define û = ∑2n+1\ni=1 si · Ai = (n1.5, n, · · · , n). Let u = û/‖u‖2 = û/( √ 3n1.5) = (1/ √ 3, 1/ √ 3n, · · · , 1/ √\n3n). Define matrix D to be A−uu>A. Then, we can compute D,\nD = A− uu>A\n= A−  1/3 1 3 √ n 1> 1 3 √ n 1> 1 3 √ n 1 13nB 1 3nB\n1 3 √ n 1 13nB\n1 3nB A =\nn1.5 0 00 B 0 0 0 B − n1.53 √ n 3 1 > √ n 3 1 > n 31 1 3B 1 3B\nn 31\n1 3B\n1 3B  = 2n1.5/3 − √ n 3 1 > − √ n 3 1 >\n−n31 2 3B − 1 3B −n31 − 1 3B 2 3B  . Now we need to take the linear combination of columns D = A − uu>A. Let w denote another sign vector {−1,+1}2n+1. Then let v denote the basis vector, v = ∑2n+1 i=1 Di. There are three possibilities, Case I(a), if w = (1, {1}n, {1}n), then v is the all 0 vector. Using vector v to interpolate each column of D, the cost we obtain is at least 2n2. Case I(b), if w = (1, {1}n, {−1}n), then v = (2n1.5/3, {2/3}n, {−4/3}n). We also obtain at least 2n2 cost if we use that v to interpolate each column of D. Case I(c), if w = (1, {−1}n, {−1}n), then v = (0, {−2/3}n, {−2/3}n). The cost is also at least 2n2. Case II, s = (1, {1}n, {−1}n). Define 1 to be a length n all 1s column vector. Define û =∑2n+1 i=1 si·Ai = (n1.5, {n}n, {−n}n). Let u = û/‖u‖2 = û/( √ 3n1.5) = (1/ √ 3, {1/ √ 3n}n, {−1/ √ 3n}n). Define (2n+ 1)× (2n+ 1) matrix D to be A− uu>A. Then, we can compute D,\nD = A− uu>A\n= A−  1/3 1 3 √ n 1> − 1 3 √ n 1> 1 3 √ n 1 13nB − 1 3nB\n− 1 3 √ n 1 − 13nB\n1 3nB A =\nn1.5 0 00 B 0 0 0 B − n1.5/3 √ n 3 1 > − √ n 3 1 > n 31 1 3B − 1 3B\n−n31 − 1 3B\n1 3B  = 2n1.5/3 − √ n 3 1 > √ n 3 1 >\n−n31 2 3B\n1 3B\nn 31\n1 3B\n2 3B  . Similarly to the previous case, we can also discuss three cases. Case III, s = (1, {1}n, {−1}n). Define 1 to be a length n all 1s column vector. Define û =∑2n+1 i=1 si·Ai = (n1.5, {−n}n, {−n}n). Let u = û/‖u‖2 = û/( √ 3n1.5) = (1/ √ 3, {−1/ √ 3n}n, {−1/ √ 3n}n). Define matrix D to be A− uu>A. Then, we can compute D,\nD = A− uu>A\n= A−  1/3 − 1 3 √ n )1> − 1 3 √ n 1> − 1 3 √ n 1 13nB 1 3nB\n− 1 3 √ n 1 13nB\n1 3nB A =\nn1.5 0 00 B 0 0 0 B − n1.5/3 − √ n 3 1 > − √ n 3 1 > −n31 1 3B 1 3B\n−n31 1 3B\n1 3B  = 2n1.5/3 √ n 3 1 > √ n 3 1 >\nn 31\n2 3B\n1 3B\nn 31\n1 3B\n2 3B  . Similarly to the previous case, we can also discuss three cases."
    }, {
      "heading" : "L.5 Counterexample for [KK05]",
      "text" : "We show that there exist matrices such that the algorithm of [KK05] cannot achieve an approximation ratio better than Θ(n). Their algorithm has two different ways of initialization. We provide counterexamples for each of the initialization separately.\nRandom vector initialization We provide a counterexample matrix A ∈ Rn×n defined as,\nA =  nc 0 · · · 0 0 0 · · · 0 · · · · · · · · · · · · 0 0 · · · 0 + I, (48) where c ≥ 2. Consider the rank-1 approximation problem for this matrix A. The optimal cost is at most n− 1.\nRun their algorithm. The starting vectors are u(0) ∼ N(0, I) and v(0) ∼ N(0, 1). We define two properties for a given vector y ∈ Rn. Property I is for all i ∈ [n], |yi| ≤ n/8, and Property II is there exist half of the i such that |yi| ≥ 1/2. We can show that with probability 1 − 2−Ω(n), both u(0) and v(0) satisfy Property I and II. After 1 iteration, we can show that u(1)1 = v(1)1 = 0\nNow let us use column vector u(0) ∈ Rn to interpolate the first column of matrix A. We simplify u(0) to be u. Let ui denote the i-th coordinate of vector u, ∀i ∈ [n]. Let A1 denote the first column of matrix A. We define α = v(1)1 = arg minα ‖αu(0)−A1‖1. For any scalar α, the cost we pay on the first column of matrix A is,\n|α · u1 − nc|+ n∑ i=2 |αui| ≥ |α · u1 − nc|+ n 2 |α · 1 2 | ≥ |α · u1 − nc|+ n 4 |α|. (49)\nNotice that c ≥ 2. Then |α · u1 − nc| + n4 |α| ≥ |α · n 8 − n c| + n4 |α|. If α ≤ 0, then the cost is minimized when α = 0. If α ∈ [0, 8nc−1], the cost is minimized when α = 0. If α ≥ 8nc−1, the cost is minimized when α = 8nc−1. Putting it all together, to achieve the minimum cost, there is only one choice for α, which is α = 0. The optimal cost is at least nc.\nThen after T iterations(for any T ≥ 1), u(T )1 = v(T )1 = 0. Thus, we always pay at least nc cost on the first entry.\nTherefore, their algorithm cannot achieve any approximation ratio better than nc−1. Because c ≥ 2, we complete the proof.\nTop singular vector initialization The counterexample input matrix A ∈ Rn×n is defined as,\nA = [ n 0 0 B ] , (50)\nwhere matrix B ∈ R(n−1)×(n−1) contains all 1s. Consider the rank-1 approximation problem for this matrix A. The optimal cost is at most n. Run their algorithm. The starting vectors u(0) and v(0) will be set to (1, 0, · · · , 0) ∈ Rn. After T iterations(for any T > 0), the support of u(T )(resp. v(T ))\nis the same as u(0)(resp. v(T )). Thus, the cost is at least ‖B‖1 = (n − 1)2. Therefore, we can conclude that their algorithm cannot achieve any approximation ratio better than (n−1)2/n = Θ(n)."
    }, {
      "heading" : "L.6 Counterexample for all",
      "text" : "For any ∈ (0, 0.5) and γ > 0, we construct the input matrix A ∈ R(2n+2)×(2n+2) as follows\nA =  n2+γ 0 0 0\n0 n1.5+ 0 0 0 0 B 0 0 0 0 B  , where B is n×n all 1s matrix. We want to find a rank k = 3 solution for A. Then any of those four heuristic algorithms [KK05, DZHZ06, Kwa08, BDB13] is not able to achieve better than nmin(γ,0.5− ) approximation ratio. We present our main experimental results in Figure 5. Both [KK05] and [Kwa08] have two different ways of initialization. In Figure 5 we use KK05r(resp. Kwak08r) to denote the way that uses random vector as initialization, and use KK05s(resp. Kwak08s) to denote the way that uses top singular vector as initialization. Figure 5(a) shows the performance of all the algorithms and Figure 5(b) presents the running time. The `1 residual cost of all the other algorithm is growing much faster than our algorithm. Most of the algorithms (including ours) are pretty efficient, i.e., the running time is always below 3 seconds. The running time of [BDB13, KK05] is increasing very fast when the matrix dimension n is growing.\nIn Figure 5(a), the cost of KK05r at {82, · · · , 142} is in [105, 106], at {162, · · · , 302} is in [106, 107], and at {322, · · · , 402} is in [107, 108]. In Figure 5(b), the time of KK05r at {382, 482} is 64s and 160s. The running time of BDB13 at {82, · · · , 222} is between 1 minute and 1 hour. The running time of BDB13 at {242, · · · , 322} is between 1 hour and 20 hours. The running time of BDB13 at {342, · · · , 402} is more than 20 hours."
    }, {
      "heading" : "L.7 Discussion for Robust PCA [CLMW11]",
      "text" : "A popular method is robust PCA [CLMW11], which given a matrix A, tries to find a matrix L for which λ‖A− L‖1 + ‖L‖∗ is minimized, where λ > 0 is a tuning parameter and ‖L‖∗ is the nuclear norm of L. This is a convex program, but it need not return a low rank matrix L with relative error. As a simple example, suppose λ = 1, and the n × n matrix A is a block-diagonal matrix of rank k and n = k2 (b + 1). Further, the first k/2 blocks are b × b matrices of all 1s, while the next k/2 blocks are just a single value b on the diagonal.\nThen the solution to the above problem may return L to be the first k/2 blocks of A. The total cost of λ‖A− L‖1 + ‖L‖∗is (k/2)b+ (k/2)b = kb.\nAlso, the solution to the above problem may return L to be A, which has cost 0 + ‖A‖∗ = kb. Because this solution has the same cost, it means that their algorithm might output a rank-k solution, and also might output a rank-k/2 solution.\nTherefore, the relative error of the output matrix may be arbitrarily bad for `1-low rank approximation.\nWe also consider the following example. Suppose λ = 1/ √ n, and let n× n matrix A denote the Hadamard matrix Hn. Recall that the Hadamard matrix Hp of size p × p is defined recursively :[ Hp/2 Hp/2 Hp/2 −Hp/2 ] with H2 = [ +1 +1 +1 −1 ] . Notice that every singular values of A is √ n. We consider the objective function λ‖A− L‖1 + ‖L‖∗.\nThen the solution to the above problem may return L to be the first n/2 rows of A. The total cost of λ‖A−L‖1 +‖L‖∗ is (1/ √ n)n2/2 + (n/2) √ n = n1.5. Also, the solution to the above problem may return L to be A, which has cost 0 + ‖A‖∗ = n √ n = n1.5. For any i, if the solution takes i rows of A, the cost is (1/ √ n)(n − i)n + i √ n = n1.5. Because this solution has the same cost, it means that their algorithm might output a rank-n solution, and also might output a rank-n/2 solution. Therefore, the relative error of the output matrix may be arbitrarily bad for `1-low rank approximation."
    }, {
      "heading" : "M Acknowledgments",
      "text" : "The authors would like to thank Alexandr Andoni, Saugata Basu, Cho-Jui Hsieh, Daniel Hsu, Chi Jin, Fu Li, Ankur Moitra, Cameron Musco, Richard Peng, Eric Price, Govind Ramnarayan, James Renegar, and Clifford Stein for useful discussions. The authors also thank Jiyan Yang, Yinlam Chow, Christopher Ré, and Michael Mahoney for sharing the code."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2016,
    "abstractText" : "<lb>We study the `1-low rank approximation problem, where for a given n × d matrix A and<lb>approximation factor α ≥ 1, the goal is to output a rank-k matrix Â for which<lb>‖A− Â‖1 ≤ α · min<lb>rank-k matrices A′ ‖A−A‖1,<lb>where for an n× d matrix C, we let<lb>‖C‖1 =<lb>∑n<lb>i=1<lb>∑d<lb>j=1 |Ci,j |. This error measure is known to<lb>be more robust than the Frobenius norm in the presence of outliers and is indicated in models<lb>where Gaussian assumptions on the noise may not apply. The problem was shown to be NP-hard<lb>by Gillis and Vavasis and a number of heuristics have been proposed. It was asked in multiple<lb>places if there are any approximation algorithms.<lb>We give the first provable approximation algorithms for `1-low rank approximation, showing<lb>that it is possible to achieve approximation factor α = (log d)·poly(k) in nnz(A)+(n+d) poly(k)<lb>time, where nnz(A) denotes the number of non-zero entries of A. If k is constant, we further<lb>improve the approximation ratio toO(1) with a poly(nd)-time algorithm. Under the Exponential<lb>Time Hypothesis, we show there is no poly(nd)-time algorithm achieving a (1 + 1<lb>log1+γ(nd) )-<lb>approximation, for γ > 0 an arbitrarily small constant, even when k = 1.<lb>We give a number of additional results for `1-low rank approximation: nearly tight upper and<lb>lower bounds for column subset selection, CUR decompositions, extensions to low rank approx-<lb>imation with respect to `p-norms for 1 ≤ p < 2 and earthmover distance, low-communication<lb>distributed protocols and low-memory streaming algorithms, algorithms with limited random-<lb>ness, and bicriteria algorithms. We also give a preliminary empirical evaluation. ∗Work done while visiting IBM Almaden.<lb>ar<lb>X<lb>iv<lb>:1<lb>61<lb>1.<lb>00<lb>89<lb>8v<lb>1<lb>[<lb>cs<lb>.D<lb>S]<lb>3<lb>N<lb>ov<lb>2<lb>01<lb>6",
    "creator" : "LaTeX with hyperref package"
  }
}