{
  "name" : "1411.8003.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Guaranteed Matrix Completion via Non-convex Factorization",
    "authors" : [ "Ruoyu Sun", "Zhi-Quan Luo" ],
    "emails" : [ "sunxx394@umn.edu." ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "In the era of big data, there has been an increasing need for handling the enormous amount of data generated by mobile devices, sensors, online merchants, social networks, etc. Exploiting low-rank structure of the data matrix is a powerful method to deal with “big data”. One prototype example is the low rank matrix completion problem in which the goal is to recover an unknown low rank matrix M ∈ Rm×n for which only a subset of its entries Mi j, (i, j) ∈ Ω ⊆ {1, 2, . . . ,m} × {1, 2, . . . , n} are specified. Matrix completion has found numerous applications in various fields such as recommender systems [1], computer vision [2] and system identification [3], to name a few.\nThere are two popular approaches to impose the low-rank structure: the nuclear norm based approach and the matrix factorization (MF) based approach. In the first approach, the whole matrix is the optimization variable and the nuclear norm (denoted as ‖ · ‖∗) of this matrix variable, which can be viewed as a convex approximation of its rank, serves as the objective function or a regularization term. For the matrix completion problem, the nuclear norm based formulation becomes either a linearly constrained minimization problem [4]\nmin Z∈Rm×n ‖Z‖∗, s.t. Zi j = Mi j, ∀ (i, j) ∈ Ω, (1)\na quadratically constrained minimization problem\nmin Z∈Rm×n\n‖Z‖∗, s.t. ∑\n(i, j)∈Ω (Zi j − Mi j)2 ≤ , (2)\n∗R. Sun is with the Department of Electrical and Computer Engineering, University of Minnesota, Minneapolis, MN 55455. Email: sunxx394@umn.edu. †Z.-Q. Luo is with the Chinese University of HongKong, Shenzhen, China. He is also affiliated with the Department of Electrical and Computer Engineering, University of Minnesota, Minneapolis, MN 55455. Email: luozq@cuhk.edu.cn.\nar X\niv :1\n41 1.\n80 03\nv3 [\ncs .L\nG ]\n1 1\nO ct\n2 01\n6\nor a regularized unconstrained problem\nmin Z∈Rm×n\n‖Z‖∗ + λ ∑\n(i, j)∈Ω (Zi j − Mi j)2. (3)\nOn the theoretical side, it has been shown that given a rank-r matrix M satisfying an incoherence condition, solving (1) will exactly reconstruct M with high probability provided that O(r(m + n) log2(m + n)) entries are uniformly randomly revealed [4–7]. This result was later generalized to noisy matrix completion, whereby the optimization formulation (2) is adopted [8]. Using a different proof framework, reference [9] provided theoretical guarantee for a variant of the formulation (3). On the computational side, problems (1) and (2) can be reformulated as a semidefinite program (SDP) and solved to global optima by standard SDP solvers when the matrix dimension is smaller than 500. To solve problems with larger size, researchers have developed first order algorithms, including the SVT (singular value thresholding) algorithm for the formulation (1) [10], and several variants of the proximal gradient method for the formulation (3) [11, 12] . Although linear convergence of the proximal gradient method has been established for the formulation (3) under certain conditions [13, 14], the per-iteration cost of computing SVD (Singular Value Decomposition) may increase rapidly as the dimension of the problem increases, making these algorithms rather slow or even useless for problems of huge size. The other major drawback is the memory requirement of storing a large m by n matrix.\nIn the second approach, the unknown rank r matrix is expressed as the product of two much smaller matrices XYT , where X ∈ Rm×r,Y ∈ Rn×r, so that the low-rank requirement is automatically fulfilled. Such a matrix factorization model has long been used in PCA (principle component analysis) and many other applications [15]. It has gained great popularity in the recommender systems field and served as the basic building block of many competing algorithms for the Netflix Prize [1, 16] due to several reasons. First, the compact representation of the unknown matrix greatly reduces the per-iteration computation cost as well as the storage space (requiring essentially linear storage of O((m + n)r) for small r). Second, the per-iteration computation cost is rather small and people have found in practice that huge size optimization problems based on the factorization model can be solved very fast. Third, as elaborated in [1], the factorization model can be easily modified to incorporate additional application-specific requirements.\nA popular factorization based formulation for matrix completion takes the form of an unconstrained regularized square-loss minimization problem [1]:\nP0 : min X∈Rm×r ,Y∈Rn×r 1 2 ∑ (i, j)∈Ω [Mi j − (XYT )i j]2 + λ(‖X‖2F + ‖Y‖2F). (4)\nThere are a few variants of this formulation: the coefficient λ can be zero [17–20] or different for each row of X,Y [21]; each square loss term [Mi j−(XYT )i j]2 can have different weights [1]; an additional matrix variable Z ∈ Rn×r can be introduced [22]. Problem (4) is a non-convex fourth-order polynomial optimization problem, and can be solved to stationary points by standard nonlinear optimization algorithms such as gradient descent method, alternating minimization [1, 18, 19, 21] and SGD (stochastic gradient descent) [1, 16, 23, 24]. Alternating minimization is easily parallelizable but has higher per-iteration computation cost than SGD; in contrast, SGD requires little computation per iteration, but its parallelization is challenging. Recently several parallelizable variants of the SGD [25–27] and variants of the block coordinate descent method with very low per-iteration cost [28,29] have been developed. Some of these algorithms have been tested in distributed computation platforms and can achieve good performance and high efficiency, solving very large problems with more than a million rows and columns in just a few minutes."
    }, {
      "heading" : "1.1 Our contributions",
      "text" : "Despite the great empirical success, the theoretical understanding of the algorithms for the factorization based formulation is fairly limited. More specifically, the fundamental question of whether these algorithms (including\nmany recently proposed ones) can recover the true low-rank matrix remains largely open. In this paper, we partially answer this question by showing that under similar conditions to those used in previous works, many standard optimization algorithms for a factorization based formulation (see (18)) indeed converge to the true low-rank matrix (see Theorem 3.1). Our result applies to a large class of algorithms including gradient descent, SGD and many block coordinate descent type methods such as two-block alternating minimization and block coordinate gradient descent. We also show the linear convergence of some of these algorithms (see Theorem 3.2 and Corollary 3.2).\nTo the best of our knowledge, our result is the first one that analyzes the geometry of matrix factorization in Euclidean space for matrix completion. In addition, our result also provides the first recovery guarantee for alternating minimization without resampling (i.e. without using independent samples in different iterations). Below we elaborate these two contributions in light of the existing works.\n1) We analyze the local geometry of the matrix factorization formation (in Euclidean space). We argue that the success of many algorithms attributes mostly (or at least partially) to the geometry of the problem, rather than the specific algorithms being used. The geometrical property we establish is that the local gradient direction −∇ f (x) is aligned with the global descent direction x∗ − x. For the classical matrix factorization formulation ‖M − XYT ‖2F , we develop a novel perturbation analysis to deal with the ambiguity of the factorization. For the sampling loss ‖PΩ(M − XYT )‖2F , an incoherence regularizer (or constraint) is needed, which causes an extra difficulty of analyzing nonconvex constrained optimization. Unfortunately, projection to the constraint (or the gradient of the regularizer) is not aligned with the global direction, and we add one more regularizer to “correct” the local descent direction. A high-level lesson is that regularization may change the geometry of the problem.\n2) Our result applies to the standard forms of the algorithms (though our optimization formulation is a bit different), which do not require the additional resampling scheme used in other works [17–20]. We obtain a sample complexity bound that is independent of the recovery error , while all previous sample complexity bounds for the matrix factorization based formulation (in Euclidean space) depend on . There is a subtle theoretical issue for the resampling scheme; see more discussions in Section 1.2 and [30, Sec. 1.5.3]."
    }, {
      "heading" : "1.2 Related works",
      "text" : "Factorization models. The first recovery guarantee for the factorization based matrix completion is provided in [31], where Keshavan, Montanari and Oh considered a factorization model in Grassmannian manifold and showed that the matrix can be recovered by a proper initialization and a gradient descent method on Grassmannian manifold. Besides being quite complicated, this model is not as flexible as the factorization model in Euclidean space, and it is not easy to solve by many advanced large-scale optimization algorithms. Moreover, most algorithms in Grassmann manifold require line search, and little is known about the convergence rate.\nThe factorization model in Euclidean space was first analyzed in an unpublished work [17] of Keshavan 1, as well as a later work of Jain et al. [18]. Both works considered alternating minimization with resampling scheme, a special variant of the original alternating minimization. The sample complexity bounds were later improved by Hardt [19] and Hardt and Wooters [20], where in the latter work, notably, the authors devised an algorithm with a corresponding sample complexity bound independent of the condition number. However, these improvements are obtained for more sophisticated versions of resampling-based alternating minimization, not the typical alternating minimization algorithm.\nResampling. The issues of resampling have been discussed in a recent work on phase retrieval by Candès et al. [32]. We will point out a subtle theoretical issue not mentioned in [32], as well as some other practical issues.\n1 Reference [17] is a PhD thesis that discusses various algorithms including the algorithm proposed in [31] and alternating minimization. In this paper when we refer to [17], we are only referring to [17, Ch. 5] which presents resampling-based alternating minimization and the corresponding result.\nThe resampling scheme (a.k.a. golfing scheme [6]) can be used at almost no cost for the nuclear norm approach [6, 7, 33], but for the alternating minimization it causes many issues. At first, it may seem that for both approaches resampling is a cheap way to get around a common difficulty: the dependency of the iterates on the sample set. However, there is a crucial difference: for the nuclear norm approach, resampling is just a proof technique used in a “conceptual” algorithm for constructing the dual certificate, while for the alternating minimization, resampling is used in the actual algorithm. This difference causes some issues of resampling-based alternating minimization at conceptual, practical and theoretical levels.\n1) Gap between theory and algorithm. Algorithmically, an easy resampling scheme is to randomly partition the given set Ω into non-overlapping subsets Ωk, k = 1, . . . , L, as proposed in [17,18] 2. However, the results in [17–20] actually require a generative model of independent Ωk’s, instead of sampling Ωk’s based on a given Ω. Therefore, the results in [17–20] do not directly apply to the partition based resampling scheme that is easy to use. See [30, Sec. 1.5.3] for more discussions on this subtle issue.\nThis issue has been discussed by Hardt and Wooters in [20, Appendix D], and they proposed a new resampling scheme [20, Algorithm 6] to which the results in [17–20] can apply, provided that the generative model of Ω is exactly known. In practice, the underlying generative model of Ω is usually unknown, in which case the scheme [20, Algorithm 6] does not work. In contrast, the classical results in [4–7] and our result herein are robust to the generative model of Ω: these results actually state that for an overwhelming portion of Ω with a given size, one can recover M through a certain algorithm, thus for many reasonable probability distributions of Ω a high probability result holds.\n2) Impracticality. As argued previously, assuming a generative model of Ωk’s is not practical since Ω is usually given. For given Ω, the only known validated resampling scheme [20, Algorithm 6], besides not being robust to the underlying generative model of Ω, might be a bit complicated to use in practice. Even the simple resampling scheme of partitioning Ω (which has not been validated yet) is rather unrealistic since each sample is used only once during the algorithm.\n3) Inexact recovery. A theoretical consequence of the resampling scheme is that the required sample complexity |Ω| becomes dependent on the desired accuracy , and goes to infinity as goes to zero. This is different from the classical results (and ours) where exact reconstruction only requires finite samples. While it is common to see the dependency of time complexity on the accuracy , it is relatively uncommon to see the dependency of sample complexity on .\nIn a recent work [34] the authors have managed to remove the dependency of the required sample size on by using a singular value projection algorithm. However, [34] considers a matrix variable of the same size as the original matrix, which requires significantly more memory than the matrix factorization approach considered in this paper. Moreover, it requires resampling at a number of iterations (though not all), which may suffer from the same issues we mentioned earlier. The resampling is also required in the recent work of [35]; see [30, Sec. 1.5.3] for more discussions.\nOther works on non-convex formulations. Non-convex formulation has also been studied for the phase retrieval problem in some recent works [32, 36]. These works provide theoretical guarantee for some algorithms specially tailored to certain non-convex formulations and with specific initializations. The major difference between [36] and [32] is that the former requires independent samples in each iteration, while the latter uses the same samples throughout in the proposed algorithm. As mentioned earlier, such a difference also exists between all previous works on alternating minimization for matrix completion [17–20] and our work.\nFinally, we note that there is a growing list of works on the theoretical guarantee of non-convex formulations for various problems, such as sparse regression (e.g. [37–39]), sparse PCA [40,41], robust PCA [42] and EM (Expected-\n2The description in [18] has some ambiguity and it might refer to the scheme of sampling Ωk’s with replacement; anyhow, under this model Ωk’s are still dependent. See [30, Sec. 1.5.3] for more discussions.\nMaximization) algorithm [43, 44]. We emphasize several aspects that distinguish our paper from other recent works on non-convex optimization. First, our paper is one of the first to analyze the (local) geometry of the problem. Second, we deal with non-symmetric matrix factorization which has a more bizarre geometry than symmetric matrix factorization and some other models. Third, one difficulty of our problem essentially lies in nonconvex constrained optimization (though we consider the closely related regularized form)."
    }, {
      "heading" : "1.3 Proof Overview and Techniques",
      "text" : "Basic idea: local geometry. The very first question is what kind of property can ensure global convergence for non-convex optimization. We will establish a local geometrical property of a regularized objective such that any stationary point in a local region is globally optimal. This is achieved in three steps: (i) study the local geometry of the fully observed objective ‖M − XYT ‖2F ; (ii) study the local geometry of the matrix completion objective ‖PΩ(M − XYT )‖2F ; (iii) study the local geometry of a regularized objective. Next, we will discuss the difficulties involved in each step and describe how we address these difficulties.\nLocal geometry of ‖M − XYT ‖2F . We start by considering a simple case that M is fully observed and the objective function is f (X,Y) = ‖M − XYT ‖2F . What is the geometrical landscape of this function? In the simplest case m = n = r = 1 and f (x, y) = (xy − 1)2, the set of stationary points is {(x, y) | xy = 1} ∪ {(0, 0)}, in which (0, 0) is a saddle point and the curve xy = 1 consists of global optima. We plot the function around the curve xy = 1 in the positive orthant in Figure 1.\nClearly a certain geometrical property prevents bad local minima in the neighborhood of the global optima, but what kind of property? We emphasize that the property can not be local convexity because the set of global optima is non-convex in R2. Due to the intrinsic symmetry that f (x, y) = f (xq, yq−1), only the product z = xy affects the value of f . We hope that the strong convexity of (1 − z)2 can be partially preserved when z is reparameterized into z = xy. It turns out we can prove the following local convexity-type property: for any (x, y) such that xy is close to 1 and |x|, |y| are upper bounded, there exists uv = 1 such that\n〈∇ f (x, y), (x, y) − (u, v)〉 ≥ c‖(x, y) − (u, v)‖2.\nAn interpretation is that the negative gradient direction −∇ f should be aligned with the global direction (u, v)−(x, y); a convex function has a similar property, but the difference is that here the global direction is adjusted according to the position of (x, y).\nFor general m, n, r, the geometrical landscape is probably much more complicated than the scalar case. Nevertheless, we can still prove that the convexity of ‖M − Z‖2 is partially preserved when reparameterizing Z as Z = XY . The exact expression is a variant of (6) which we will discuss in more detail later. Technically, we need to connect the Euclidean space and the quotient manifold via “coupled perturbation analysis”: given X,Y such that ‖XYT −M‖F is small, find decomposition M = UVT such that U,V are close to X and Y respectively (a simpler version of Proposition 4.1). The difference from traditional perturbation analysis of Wedin [45] (i.e. if two matrices are close then their row/column spaces are close) is that in [45] the row/column spaces are fixed while in our problem U,V are up to our choice.\nLocal geometry of ‖PΩ(M − XYT )‖2F . Let us come back to the original matrix completion problem, in which an additional sampling operator PΩ is introduced. Similarly, we hope that fΩ(Z) = 12 ‖PΩ(M − Z)‖2 is strongly convex and this strong convexity can be partially preserved after reparametrization Z = XYT . However, one issue is that the function fΩ(Z) is possibly non-strongly-convex (though still convex). In fact, if fΩ is locally strongly convex around M, then we should have\nfΩ(Z) − fΩ(M) ≥ O(‖Z − M‖2F),∀ Z close to M.\nAssuming Z is rank-r, this inequality can be rewritten as\n‖PΩ(M − XYT )‖2F ≥ Cp‖M − XYT ‖2F , ∀(X,Y) ∈ K(δ), (5)\nwhere K(δ) is a neighborhood of M defined as {(X,Y) | ‖XYT −M‖F ≤ δ} and C is a numerical constant. We wish (5) to hold with high probability (w.h.p.) for random Ω in which each position in M is chosen with probability p. This inequality is closely related to matrix RIP (restricted isometry property) in [8] (see equation (III.4) therein). If X,Y are independent of Ω, then (5) follows easily from the concentration inequalities. Unfortunately, if X,Y are chosen arbitrarily instead of independently from Ω, the bound (5) may fail to hold.\nA solution, as employed in [31], is to utilize a random graph lemma in [46] which provides a bound on ‖PΩ(A)‖F for any rank-1 matrix A (possibly dependent on Ω). This lemma, combined with another probability result in [4], implies a bound on ‖PΩ(M − XYT )‖F . However, this bound is not good enough since it only leads to (5) when δ = O(1/n). The underlying reason is that the bound given by the random graph lemma is actually quite loose if X or Y have unbalanced rows, i.e. certain row has large norm. One solution is to force the iterates to have bounded row norms (a.k.a. incoherent), by adding a constraint or regularizer. With the incoherence requirement on X,Y , now (5) can be shown to be hold for δ = O(1), or more precisely, δ = O(Σmin), where Σmin is the minimum eigenvalue of M. With such a δ, it is possible to find an initial point in the region K(δ).\nIn summary, although fΩ(Z) = 12 ‖PΩ(Z − M)‖2F is possibly non-strongly-convex, by restricting to an incoherent neighborhood of M it is “relative” strongly convex (called “relative” since we fix M in (5)). More specifically, we have that w.h.p.\n‖PΩ(M − XYT )‖2F ≥ Cp‖M − XYT ‖2F , ∀(X,Y) ∈ B , K(δ) ∩ K1. (6)\nwhere K1 denotes the set of (X,Y) with bounded row norms. Note that this inequality also implies that global optimally in B leads to exact recovery; or equivalently, zero training error leads to zero generalization error.\nHaving established the geometry of fΩ(Z), we can use the same technique for the fully observed case to show the local geometry 3 of\nF(X,Y) , fΩ(X,Y) = 1 2 ‖PΩ(M − XYT )‖2F .\n3 For illustration purpose, we present a two-step approach: first establish a geometrical property of fΩ(Z), then extend the property to fΩ(XYT ). However, our current proof does not follow the two-step approach but directly establish the property of fΩ(XYT ). In fact, although we establish the property of fΩ(Z) in Claim 3.1, the proof of this claim is very similar to the proof of (7).\nMore specifically, we can prove that for any (X,Y) ∈ B, there exists (U,V) ∈ X∗ = {(U,V) ∈ Rm×r × Rn×r | UVT = M} such that\n〈∇XF(X,Y), X − U〉 + 〈∇Y F(X,Y),Y − V〉 ≥ c(‖X − U‖2F + ‖Y − V‖2F). (7)\nDenoting x = (X,Y),x∗ = (U,V) and utilizing ∇F(x∗) = 0, (7) becomes\n∀ x ∈ B, ∃ x∗ ∈ X∗, s.t. 〈∇F(x) − ∇F(x∗),x − x∗〉 ≥ c‖x − x∗‖2. (8)\nIt links the local optimality measure ‖∇F(x)‖ with the global optimality measure dist(x,X∗) = minx∗∈X∗ ‖x − x∗‖, and implies that any stationary point of F in B is a global minimum.\nIf (8) holds for arbitrary x,x∗ then F would be strongly convex in x. Let us emphasize again two differences of (8) with local strong convexity: i) since x∗ is not arbitrary but has to be one global minimum, (8) indicates local “relative convexity” of F; ii) due to the ambiguity of factorization, x∗ should be chosen according to x, thus (8) indicates local relative convexity up to a group transformation (it might be conceptually helpful to view it as a property in the quotient manifold, but we do not explicitly exploit its structure).\nLocal geometry with regularizers/constraints. The property (8) is still not desirable. The original purpose of studying geometry is to show there is no spurious “1st order local-min” (point that satisfies 1st order optimality conditions). To establish the geometrical property with sampling, we restrict to an incoherent set K1, but this restriction changes the meaning of the 1st order local-min. In fact, to ensure the iterates stay in the incoherent region K1, we need to solve a constrained optimization problem minx∈K1 F(x) or a regularized problem minx F(x) +G1(x) where G1 is a regularizer forcing x to be in K1. Standard optimization algorithms converge to the KKT points of minx∈K1 F(x) or the stationary points of F + G1, which may not be the stationary points of F. The property (8) only implies any stationary point of F in B is globally optimal.\nWe shall focus on the regularized problem min F + G1; the constrained problem minx∈K1 F is similar. Because of the extra regularizer, the property (8) is not enough. We need to prove a result similar to (8), but with ∇F replaced by ∇F + ∇G1:\n∀ x ∈ B, ∃ x∗ ∈ X∗, s.t. 〈∇F(x) + ∇G1(x),x − x∗〉 ≥ c‖x − x∗‖2. (9)\nIf it happens to be the case that 〈∇G1(x),x − x∗〉 ≥ 0, (10)\nthen combining with the existing result (8) we are done; unfortunately, we do not know how to prove (10). Intuitively, (10) means that −∇G1(x), which is almost the same direction as the projection to the incoherent region K1, is positively correlated with the global direction x∗ − x. At first sight, this seems trivially true because for any point x̄ ∈ K1 we have 〈∇G1(x),x − x̄〉 ≥ 0 (as illutrated in Fig. 2). However, a rather strange issue is that x∗ is chosen to be a point in {(U,V) | UVT = M} that is close to x, thus there is no guarantee that x∗ lies in K1. An underlying reason is that the global optimum set {(U,V) | UVT = M} is unbounded and thus not a subset of K1. If we enforce (U,V) to be in K1, we may not be able to find (U,V) that is close enough to (X,Y).\nTechnically, the issue is that (U,V) chosen in Proposition 4.1 have row-norms bounded above by quantities proportional to the norms of X,Y , and can be higher than the row-norms of X,Y (threshold of K1). To resolve this issue, we add an extra regularizer G2(X,Y) to force (X,Y) to lie in K2, a set of matrix pairs with bounded norms. This extra bound makes 〈∇G1(x),x − x∗〉 ≥ 0 straightforward to prove, but a similar issue arises: now we need to prove (8) for F + G1 + G2 instead of F. Again, it suffices to prove that for any x ∈ K(δ) ∩ K1 ∩ K2 there exists x∗ such that\n〈∇G2(x),x − x∗〉 ≥ 0. (11)\nThis is what we prove as outlined next.\nConstrained perturbation analysis. The desired inequality (11) is implied by the following condition on U,V: ‖U‖F ≤ ‖X‖F , ‖V‖F ≤ ‖Y‖F when ‖X‖F , ‖Y‖F are large. Recall that previously we try to find U,V that are close to X,Y; see Proposition 4.1. Now we need to impose extra constraints on U,V , giving rise to Proposition 4.2. The extra constraints make the perturbation analysis significantly more involved; in fact, we apply a sophisticated iterative procedure to construct the factorization M = UVT . The main steps of the proof are briefly given in Appendix C.2.\nOne crucial component of our proof can be viewed as the perturbation analysis for “preconditioning”. Roughly speaking, the basic problem is: given an r × r matrix X̂ with a large condition number, find another matrix Û with the same Frobenius norm as X̂ but smaller inverse Frobenious norm (i.e. ‖Û−1‖F ≤ 11−δ‖X̂−1‖F). In other words, we want to reduce ∑r i=1\n1 σ2i\nwith ∑r\ni=1 σ 2 i fixed, where σi’s are all singular values. Intuitively, by reducing ∑r i=1\n1 σ2i we reduce the discrepancy of singular values. This process is somewhat similar to preconditioning in numerical algebra that reduces the gap between the largest and smallest eigenvalue. The precise statement of the basic problem and its relation with the key technical result Proposition 4.2 are provided in Appendix C.2.1.\nAlgorithm requirements. We provide three conditions and show that if an algorithm satisfies either of them, then with specific initialization the iterates will stay in the desired basin (see Proposition 5.1). A special case of the third condition has been used in [31] for Grassmann manifold optimization. Together, these three conditions cover a wide spectrum of algorithms including GD, SGD and block coordinate descent type methods.\nProof outline. The overall proof can be divided into two parts: the geometrical property (Lemma 3.1) and the algorithm property (Lemma 3.2). For the geometrical property, Lemma 3.1 states that the regularized objective function F + G1 + G2 enjoys some nice geometrical property in a certain local region around the global optima, thus there is no other stationary point in this region. For the algorithm property, Lemma 3.2 states that starting from an easily computable initial point, many standard algorithms generate a sequence that are inside the desired region and these algorithms also converge to stationary points. Since these stationary points must be global optima by Lemma 3.1, we obtain that these algorithms converge to the global optima."
    }, {
      "heading" : "1.4 Other Remarks",
      "text" : "Difference with previous works. As discussed earlier, one major challenge is to bound PΩ(A) when A may be dependent on Ω. One simple strategy as adopted in [17–20] is to use a resampling scheme to decouple A and the observation set. This strategy artificially avoids this difficulty, and causes a few issues discussed earlier in Section 1.2. Another strategy, as employed in [31], is to use a random graph lemma in [46].\nWe apply the random graph lemma of [46] when extending the local geometry of ‖M−XYT ‖2F to ‖PΩ(M−XYT )‖2F .\nThe difference of our work with [31] is that we study the local geometry in Euclidean space (and, indirectly, the geometry of the quotient manifold), which is quite different from the local geometry in Grassmann manifold studied in [31]. Technically, the complications of the proof in [31] are mostly due to heavy computation of various quantities in Grassmann manifold; in addition, much effort is spent in estimating the terms related to the extra factor S which enables the decoupling of X and Y ( [31] actually uses a three-factor decomposition XS YT ). For our problem, one difficulty is to “pull back” the distance in the quotient manifold to the Euclidean space, by the coupled perturbation analysis. Another difficulty is to align the gradient of the regularizer with the global direction (this is not an issue for Grassman manifold), which requires a more sophisticated perturbation analysis. The difficulties have been discussed in detail in Section 1.3.\nSymmetric PSD or rank-1 case. The symmetric PSD (positive semi-definite) case or the rank-1 case are easier to deal with, because in the 3-step study of the local geometry the third step is not necessary. When M is rank-1 (possibly non-symmetric), the regularizer G2(·) may still be needed, but Proposition 4.2 is trivial since its assumptions cannot hold for r = 1. When M is symmetric PSD, a popular approach is to use a symmetric factorization M = XXT instead of the non-symmetric factorization, and the loss function becomes ‖PΩ(M−XXT )‖2F . The same proof in our paper can be translated to this symmetric PSD case, except that the third step is not necessary. In fact, it is possible to show that (10) holds without any additional requirement on x. As a result, the regularizer G2 and a major technical result Proposition 4.2 are not needed. In both the symmetric PSD and rank-1 case, we only need to establish the intermediate result (7) and the proof can be greatly simplified. Stronger sample complexity and time complexity bounds may be established in these two cases.\nSimulation Results The regularizers are introduced due to theoretical purposes; interestingly, they turn out to be helpful in the numerical experiments (the comments below are extracted from the thesis [30, Chapter 2]).\nFirst, the simulation suggests that the imbalance of the rows of X or Y is an important issue for matrix completion in practice, a phenomenon not reported before to our knowledge. The table in Figure 2.10 of [30] shows that when |Ω| is small, in all successful instances the iterates are balanced, while in all failed instances the iterates are unbalanced. This contrast occurs for many standard algorithms such as AltMin,GD and SGD.\nSecond, adding only the regularizer G1 helps, but not too much. Adding an extra regularizer G2 can push the sample complexity to be very close to the fundamental limit, at least for the synthetic Gaussian data. These experiments seem to indicate that the new regularizers do change the geometry of the problem.\nNecessity of incoherence? While our regularizers are helpful when |Ω| is small, an open question is whether the row-norm requirement is needed for the local geometry when |Ω| is large. We observe that the row-norms can be automatically controlled by standard algorithms for the synthetic Gaussian data when there are, say, 5rn samples for n × n matrices. There are two possible explanations (assuming a large |Ω|): (i) the local geometrical property (7) holds without the incoherence requirement; (ii) (7) still requires incoherence, but there is an unknown mechanism for many algorithms to control the row-norms.\nTo exclude the first possibility, we need to find (X,Y) ∈ K(δ) such that ∇F(X,Y) = 0 but XYT , M; since (7) holds, such (X,Y) must have unbalanced row-norms. Such an example would validate the necessity of the incoherence restriction for the local geometry. Note that the necessity of incoherence for the local geometry is different from the necessity of an incoherence regularizer/constraint for a specific algorithm. Even if the local geometry requires incoherence, it remains an interesting question why many algorithms can automatically control row-norms when |Ω| is large."
    }, {
      "heading" : "1.5 Notations and organization",
      "text" : "Notations. Throughout the paper, M ∈ Rm×n denotes the unknown data matrix we want to recover, and r min{m, n} is the rank of M. The SVD of M is M = ÛΣV̂T , where Û ∈ Rm×r, V̂ ∈ Rn×r and Σ ∈ Rr×r is a diagonal matrix with diagonal entries Σ1 ≥ Σ2 ≥ · · · ≥ Σr. We denote the maximum and minimum singular value as Σmax and Σmin, respectively, and denote κ , Σmax/Σmin as the condition number of M. Define α = m/n, which is assumed to be bounded away from 0 and∞ as n −→ ∞. Without loss of generality, assume m ≥ n, then α ≥ 1.\nDefine the short notations [m] , {1, 2, . . . ,m}, [n] , {1, 2, . . . , n}. Let Ω ⊆ [m] × [n] be the set of observed positions, i.e. {Mi j | (i, j) ∈ Ω} is the set of all observed entries of M, and define p , |Ω|mn which can be viewed as the probability that each entry is observed. For a linear subspace S, denote PS as the projection onto S. By a slight abuse of notation, we denote PΩ as the projection onto the subspace {W ∈ Rm×n : Wi, j = 0,∀(i, j) < Ω}. In other words, PΩ(A) is a matrix where the entries in Ω are the same as A while the entries outside of Ω are zero.\nFor a vector x ∈ Rn, denote ‖x‖ as its Euclidean norm. For a matrix X, denote ‖X‖F as its Frobenius norm, and ‖X‖2 as its spectral norm (i.e. the largest singular value). Denote σmax(X), σmin(X) as the largest and smallest singular values of X, respectively. Let X† denote the pseudo inverse of a matrix X. The standard inner product between vectors or matrices are written as 〈x, y〉 or 〈X,Y〉, respectively. Denote A(i) as the ith row of a matrix A. We will use C,C1,CT ,Cd, etc. to denote universal numerical constants.\nOrganization. The rest of the paper is organized as follows. In Section 2 we introduce the problem formulation and four typical algorithms. In Section 3, we present the main results and the main lemmas used in the proofs of these results. The proof of the two lemmas used in proving Theorem 3.1 are given in Section 4 and Section 5 respectively. The proof of the first lemma depends on two “coupled perturbation analysis” results Proposition 4.1 and Proposition 4.2, the proofs of which are given in Appendix B and Appendix C respectively. The proof of a lemma used in proving Theorem 3.2 is given in Appendix E."
    }, {
      "heading" : "2 Problem Formulation and Algorithms",
      "text" : ""
    }, {
      "heading" : "2.1 Assumptions",
      "text" : "Incoherence condition. The incoherence condition for the matrix completion problem is first introduced by Candès and Recht in [4] and has become a standard assumption for low-rank matrix recovery problems (except a few recent works such as [47, 48]). We will define an incoherence condition for an m × n matrix M which is the same as that in [31].\nDefinition 2.1 We say a matrix M = ÛΣV̂T (compact SVD of M) is µ-incoherent if: r∑\nk=1\nÛ2ik ≤ µr m , r∑ k=1 V̂2jk ≤ µr n , 1 ≤ i ≤ m, 1 ≤ j ≤ n. (12)\nIt can be shown that µ ∈ [1, max{m,n}r ]. For some popular random models for generating M, the incoherence condition holds with a parameter scaling as √ r log n (see [31]). In this paper, we just assume that M is µ-incoherent. Note that the incoherence condition implies that Û, V̂ have bounded row norm. Throughout the paper, we also use the terminology “incoherent” to (imprecisely) describe m × r or n × r matrices that have bounded row norm (see the definition of set K1 in (30)).\nRandom sampling model. In the statement of the results in this paper, the probability is taken with respect to the uniform random model of Ω ⊆ [m]× [n] with fixed size |Ω| = S (i.e. Ω is generated uniformly at random from set\n{Ω′ ⊆ [m]× [n] : the size of Ω′ is S } ). We remark that this model is “equivalent to” a Bernolli model that each entry of M is included into Ω independently with probability p = Smn in the sense that if the success of an algorithm holds for the Bernolli model with a certain p with high probability, then the success also holds for the uniform random model with |Ω| = pmn with high probability (see [4] or [31, Sec. 1D] for more details). Thus in the proofs we will instead use the Bernolli model."
    }, {
      "heading" : "2.2 Problem formulation",
      "text" : "We consider a variant of (P0) with incoherence-control regularizers. In particular, we introduce two types of regularization terms besides the square loss function: the first type is designed to force the iterates Xk,Yk to be incoherent (i.e. with bounded row norm), and the second type is designed to upper bound the norm of Xk and Yk. Note that (P0) is related to the Lagrangian method, while our regularizer is based on the penalty function method for constrained optimization problems. We can also view the regularizer λ(‖X‖2F + ‖Y‖2F) as a “soft regularizer”, and our new regularizer as a “hard regularizer”. The advantage of the hard regularizer is that it does not distort the optimal solution.\nOur regularizers are smooth functions with simple gradients, thus the algorithms for our formulation have similar per-iteration computation cost as the algorithms for the formulation without regularizers. In the numerical experiments, we find that when |Ω| is large, the iterates are always incoherent and bounded, and our algorithms are the same as the traditional algorithms for the unregularized formulation; when |Ω| is relatively small, the traditional algorithms may produce high error, and our regularizer becomes active and significantly reduce the error. In some sense, our algorithms for the new formulation are “better” versions of the traditional algorithms, and our theoretical results can be viewed as a validation of the traditional algorithms in the “large-|Ω| regime” and a validation of the modified algorithm in the “small-Ω” regime. Preliminary simulation results show that many algorithms for the proposed formulation can recover the matrix when |Ω| is very close to the fundamental limit, significantly improving upon the traditional algorithms; see [30, Chapter 3].\nThe regularization function G is defined as follows:\nG(X,Y) , ρ m∑\ni=1\nG0 3‖X(i)‖2 2β21  + ρ n∑ j=1 G0 3‖Y ( j)‖2 2β22  +ρG0\n3‖X‖2F 2β2T  + ρG0 3‖Y‖2F 2β2T  , (13)\nwhere A(i) denotes the ith row of a matrix A,\nG0(z) , I[1,∞](z)(z − 1)2 = max{0, z − 1}2, (14)\nβT , √ CT rΣmax, β1 , βT √ 3µr m = √ CT rΣmax √ 3µr m ,\nβ2 , βT √ 3µr n = √ CT rΣmax √ 3µr n .\n(15)\nHere, IC is the indicator function of a set C, i.e. IC(z) equals 1 when z ∈ C and 0 otherwise. ρ is a constant specified shortly. Throughout the paper, δ and δ0 are defined as\nδ , Σmin\nCdr1.5κ , δ0 ,\nδ 6 , (16)\nwhere Cd is some numerical constant. The coefficient ρ is defined as (a larger ρ also works)\nρ , 2pδ20\nG0(3/2) = 8pδ20. (17)\nThe numerical constant CT > 5 will be specified in the proof of our main result. The parameter βT is chosen to be of the same order as ‖ÛΣ1/2‖F and ‖V̂Σ1/2‖F , and β1, β2 are chosen to be of the same order as √ r‖(ÛΣ1/2)(i)‖, √ r‖(V̂Σ1/2)( j)‖. The additional factor √ 3r is due to technical consideration (to prove (256)). Our regularizer G involves Σmax and\nµ which depend on the unknown matrix M; in practice, we can estimate Σmax by c1 √ ‖PΩ(M)‖2F\npr , and estimate µ by\nc2 √\nmn rΣmax max(i, j)∈Ω |Mi j| (according to (203)) where c1, c2 are numerical constants to tune.\nIt is easy to verify that G0 is continuously differentiable. The choice of function G0 is not unique; in fact, we can choose any G0 that satisfies the following requirements: a) G0 is convex and continuously differentiable; b) G0(z) = 0, z ∈ [0, 1]. In [31], G0 is chosen as G0(z) = I[1,∞](z)(e(z−1)\n2 − 1), which also satisfies these two requirements. Choosing different G0 does not affect the proof except the change of numerical constants (which depend on G0(3/2),G′0(3/2),G ′′ 0 (3/2)). Note that the requirement of G0 being non-decreasing and convex guarantees the convexity of G(X,Y). In fact, according to the well-known result that the composition of a non-decreasing convex function and a convex function is a convex function, and notice that ‖X(i)‖2, ‖Y ( j)‖2, ‖X‖2F , ‖Y‖2F are convex, we have that each component of G is convex and thus G is convex.\nDenote the square loss term in (P0) as F(X,Y) , ∑\n(i, j)∈Ω[Mi j − (XYT )i j]2 = ‖PΩ(M − XYT )‖2F . Replacing the objective function of (P0) by F̃(X,Y) , F(X,Y) + G(X,Y), we obtain the following problem:\nP1 : min X∈Rm×r ,Y∈Rn×r 1 2 ‖PΩ(M − XYT )‖2F + G(X,Y). (18)\nWe remark that (P1) can be interpreted as the penalized version of the following constrained problem (see, e.g. [49])\nmin X,Y 1 2 ‖PΩ(M − XYT )‖2F , s.t. ‖X‖2F ≤ 2 3 β2T , ‖Y‖2F ≤ 2 3 β2T ;\n‖X(i)‖2 ≤ 2 3 β21, ∀ i, ‖Y ( j)‖2 ≤ 2 3 β22, ∀ j.\n(19)\nTo illustrate this, note that the constraint f1(X) , 3‖X‖2F 2β2T − 1 ≤ 0 corresponds to the penalty term ρG0( f1(X) + 1) = ρmax{0, f1(X)}2 which appears as the third term in G(X,Y), and similarly other constraints correspond to other terms in G(X,Y). In other words, the regularization function G(X,Y) is just a penalty function for the constraints of the problem (19). The function max{0, ·}2 is a popular choice for the penalty function in optimization (see, e.g. [49]), which motivates our choice of G0 in (14). Our result can be extended to cover the algorithms for the constrained version (19), or a partially regularized formulation (e.g. only penalize the violation of the constraint ‖X‖2F ≤ 23β2T , ‖Y‖2F ≤ 2 3β 2 T ).\nIt is easy to check that the optimal value of (P1) is zero and (X,Y) = (ÛΣ1/2, V̂Σ1/2) is an optimal solution to (P1), provided that M is µ-incoherent. In fact, since F̃ is a nonnegative function, we only need to show F̃(X,Y) = 0 for this choice of (X,Y). As XYT = M implies ‖PΩ(M − XYT )‖2F = 0, we only need to show G(X,Y) = G(ÛΣ1/2, V̂Σ1/2) equals zero. In the expression of G(X,Y), the third and fourth terms G0(\n3‖X‖2F 2β2T ) and G0( 3‖Y‖2F 2β2T ) equal zero because\n‖X‖2F = ‖Y‖2F ≤ rΣmax < 23β2T . The first and second terms ∑ i G0( 3‖X(i)‖2\n2β21 ) and\n∑ j G0(\n3‖Y ( j)‖2 2β22 ) equal zero because\n‖X(i)‖2 ≤ Σmax‖Û(i)‖2 ≤ Σmax µrm ≤ 2 3β 2 1, for all i and, similarly, ‖Y ( j)‖2 ≤ 2 3β 2 2, for all j, where we have used the incoherence condition (12). This verifies our previous claim that the “hard regularizer” G(X,Y) does not distort the optimal solution of the original formulation.\nOne commonly used assumption in the optimization literature is that the gradient of the objective function is Lipschitz continuous. For any positive number β, define a bounded set\nΓ(β) , {(X,Y)|X ∈ Rm×r,Y ∈ Rn×r, ‖X‖F ≤ β, ‖Y‖F ≤ β}. (20)\nThe following result shows that this assumption (Lipschitz continuous gradients) holds for our objective function within a bounded set.\nClaim 2.1 Suppose β0 ≥ βT and\nL(β0) , 4β20 + 54ρ β20\nβ41 . (21)\nThen ∇F̃(X,Y) is Lipschitz continuous over the set Γ(β0) with Lipschitz constant L(β0), i.e.\n‖∇F̃(X,Y) − ∇F̃(U,V)‖F ≤ L(β0)‖(X,Y) − (U,V)‖F , ∀(X,Y), (U,V) ∈ Γ(β0),\nwhere ‖(X,Y) − (U,V)‖F = √ ‖X − U‖2F + ‖Y − V‖2F .\nThe proof of Claim 2.1 is given in Appendix A.1."
    }, {
      "heading" : "2.3 Row-scaled Spectral Initialization",
      "text" : "Our results require the initial point to be close enough to the global optima. To be more precise, we want the initial point to be in an incoherent neighborhood of the original matrix M (this neighborhood will be specified later). Special initialization is also required in other works on non-convex formulations [17–20, 31, 32, 36].\nWe will show that such an initial point can be found through a simple procedure. This procedure consists of two steps: first, using the spectral method (see, e.g. [31]), we obtain M0 = X̂0ŶT0 which is close to M; second, we scale the rows of (X̂0, Ŷ0) to make it incoherent (i.e. with bounded row-norm). Denote the best rank-r approximation of a matrix A as Pr(A). Define an operation SVDr that maps a matrix A to the SVD components (X,D,Y) of its best rank-r approximation Pr(A), i.e.\nSVDr(A) , (X,D,Y), where XDYT is compact SVD of Pr(A). (22)\nThe initialization procedure is given in Table 1. The property of the initial point generated by this procedure will be presented in Claim 5.2.\nIn the numerical experiments, we find that the proposed initialization is not better than random initialization if we use the proposed formulation with the incoherence-control regularizer. In contrast, for traditional formulations (either unregularized or with a regularizer λ(‖X‖2F + ‖Y‖2F)) the proposed initialization does lead to better recovery performance (lower sample complexity). We also notice that the row-scaling step is crucial for this improvement since simply initializing via the spectral method does not help too much. See [30, Chapter 3] for the simulation results and discussions."
    }, {
      "heading" : "2.4 Algorithms",
      "text" : "Our result applies to many standard algorithms such as gradient descent, SGD and block coordinate descent type methods (including alternating minimization, block coordinate gradient descent, block successive upper bound minimization, etc.). We will describe several typical algorithms in this subsection.\nwhere G′0(z) = I[1,∞](z)2(z − 1), and X̄(i) (resp. Ȳ ( j)) denotes a matrix with the i-th (resp. j-th) row being X(i) (resp. Y ( j)) and the other rows being zero.\nWe first present a gradient descent algorithm in Table 2. There are many choices of stepsizes such as constant stepsize, exact line search, limited line search, diminishing stepsize and Armijo rule [50]. We present three stepsize rules here: constant stepsize, restricted Armijo rule and restricted line search (the latter two are the variants of Armijo rule and exact line search). Note that the restricted line search rule is similar to that used in [31] for the gradient descent method over Grassmannian manifolds. To simplify the notations, we denote xk(η) , (Xk(η),Yk(η))\nand d(xk(η),x0) , √ ‖Xk(η) − X0‖2F + ‖Yk(η) − Y0‖2F .\nAltMin (alternating minimization) belongs to the class of block coordinate descent (BCD) type methods. One can update the blocks in different orders (e.g. cyclic [51–53], randomized [54] or parallel) and solve the subproblem inexactly. Commonly used inexact BCD type algorithms include BCGD (block coordinate gradient descent, which updates each variable by a single gradient step [54]) and BSUM (block successive upper bound minimization, which updates each variable by minimizing an upper bound of the objective function [55]). BCD-type methods have been widely used in engineering (e.g. [56, 57]). In the context of matrix completion, Hastie et al. [58] proposed an algorithm that could be viewed as a BSUM algorithm. Just considering different choices of the blocks will lead to different algorithms for the matrix completion problem [29]. Our result applies to many BCD type methods, including the two-block alternating minimization, BCGD and BSUM. While it is not very interesting to list all possible algorithms to which our results are applicable, we just present two specific algorithms for illustration.\nThe first BCD type algorithm we present is (two-block) AltMin, which, in the context of matrix completion,\nusually refers to the algorithm that alternates between X and Y by updating one factor at a time with the other factor fixed. Although the overall objective function is non-convex, each subproblem of X or Y is convex and thus can be solved efficiently. The details are given in Table 3.\nFor the case without the regularization term G(X,Y), the objective function becomes F(X,Y) and is quadratic with respect to X or Y . Thus Xk,Yk have closed form update. Suppose XT = (x1, . . . , xm) and YT = (y1, . . . , yn), where xi, y j ∈ Rr×1. Then (x∗1, . . . , x∗m) , (arg minX F(X,Y))T and (y∗1, . . . , y∗n) , (arg minY F(X,Y))T are given by\nx∗i = ( ∑ j∈Ωxi y jyTj ) †( ∑ j∈Ωxi Mi jy j), i = 1, . . . ,m,\ny∗j = ( ∑ i∈Ωyj xixTi ) †( ∑ i∈Ωyj Mi jxi), j = 1, . . . , n, (25)\nwhere Ωxi = { j | (i, j) ∈ Ω},Ω y j = {i | (i, j) ∈ Ω}, and A† denotes the pseudo inverse of a matrix A. For our problem with the regularization term G(X,Y), we no longer have closed form update of Xk,Yk. One way to solve the convex subproblems is to start from the solution given in (25) and then apply the gradient descent method until convergence. The details for solving minX F̃(X,Y) is given in Table 4 (the stepsize can be chosen by one of the standard rules of the gradient descent method), and the other subproblem minY F̃(X,Y) can be solved in a similar fashion.\nTheoretically speaking, AltMin for our formulation (P1) is not as efficient as the vanilla AtlMin for (P0) since an extra inner loop is needed to solve the subproblem. However, we remark that in the regimes of |Ω| that the vanilla AltMin works, the least square solution X (resp. Y) is always bounded and incoherent (empirical observation), in which case the regularizer G is inactive; therefore, the gradient updates in Table 4 do not happen. In the regimes of |Ω| that the vanilla AltMin fails, G is active and the gradient updates do happen; however, instead of solving the subproblem exactly, one could perform one gradient step and the algorithm becomes the popular variant BCGD [54]. Our main result of exact recovery still holds for BCGD (the proof for Algorithm 3 in Claim 5.3 can be applied to BCGD since BCGD is a special case of BSUM).\nIn the second BCD type algorithm called row BSUM, we update the rows of X and Y cyclically by minimizing\nan upper bound of the objective function; see Table 5. The extra terms λ02 ‖X(i) − X (i) k−1‖2 or λ0 2 ‖Y ( j) − Y ( j) k−1‖2 are added to make the subproblems strongly convex, which help prove convergence to stationary points. Such a technique has also been used in the alternating least square algorithm for tensor decomposition [55]. Note that for the two-block BCD algorithm, convergence to stationary points can be guaranteed even when the subproblems are not strongly convex [59], thus in Algorithm 2 we do not add the extra terms. The benefit of cyclically updating the rows is that each subproblem can be solved efficiently using a simple binary search; see Appendix A.2 for the details. We remark again that instead of solving the subproblem exactly, one could just perform one gradient step to update each row of X and Y (with λ = 0) and our result still holds.\nThe fourth algorithm we present is SGD (stochastic gradient descent) [1, 23] tailored for our problem (P1). In the optimization literature, this algorithm for minimizing the sum of finitely many functions is more commonly referred to as “incremental gradient method”, while SGD represents the algorithm for minimizing the expectation of a function; nevertheless, in this paper we follow the convention in the computer science literature and still call it “SGD”. In SGD, at each iteration we pick a component function and perform a gradient update. Similar to the BCD type methods where the blocks can be chosen in different orders, one can pick the component functions in a cyclic order, in an essentially cyclic order, or in a random order (either sampling with replacement or without replacement). In practice, the version of sampling without replacement converges much faster than the version of sampling with replacement (see [30, Chapter 2] for simulation results). In general, the understanding of sampling without replacement for optimization algorithms is quite limited (see, e.g., [60] for one example of such analysis).\nIn this paper we only consider the cyclic order, and use a standard stepsize rule for SGD [61, 62] which requires the stepsizes {ηk} to go to zero as k → ∞, but neither too fast nor too slow (this choice guarantees convergence to stationary points even for nonconvex problems). One such choice of stepsizes is ηk = O(1/k). We remark that our results also apply to other versions of SGD with different update orders or stepsize rules as long as they converge to stationary points.\nTo apply SGD to our problem, we decompose the objective function F̃(X,Y) as follows:\nF̃(X,Y) = ∑\n(i, j)∈Ω Fi j(X,Y) + m∑ i=1 G1i(X) + n∑ j=1 G2 j(Y) + G3(X) + G4(Y) = |Ω|+m+n+2∑ k=1 fk(X,Y),\nwhere the component functions\nFi j(X,Y) = [(XYT − M)i j]2 = [(X(i))T Y ( j) − Mi j]2, (i, j) ∈ Ω,\nG1i(X) = ρG0( 3‖X(i)‖2\n2β21 ), 1 ≤ i ≤ m, G2 j(Y) = ρG0(\n3‖Y ( j)‖2\n2β22 ), 1 ≤ j ≤ n,\nG3(X) = ρG0( 3‖X‖2F 2β2T ), G4(Y) = ρG0( 3‖Y‖2F 2β2T )\n(26)\nand { fk(X,Y)}|Ω|+m+n+2k=1 denotes the collection of all component functions. With these definitions, the SGD algorithm is given in Table 6."
    }, {
      "heading" : "3 Main Results",
      "text" : "The main result of this paper is that Algorithms 1-4 (standard optimization algorithms) will converge to the global optima of problem (P1) given in (18) and reconstruct M exactly with high probability, provided that the number of revealed entries is large enough. Similar to the results for nuclear norm minimization [4–7], the probability is taken with respect to the random choice of Ω, and the result also applies to a uniform random model of Ω.\nTheorem 3.1 (Exact Recovery) Assume a rank-r matrix M ∈ Rm×n is µ-incoherent. Suppose the condition number of M is κ and α = m/n ≥ 1. Then there exists a numerical constant C0 such that: if Ω is uniformly generated at random with size\n|Ω| ≥ C0αnrκ2 max{µ log n, √ αµ2r6κ4}, (27)\nthen with probability at least 1 − 2/n4, each of Algorithms 1-4 reconstructs M exactly. Here, we say an algorithm reconstructs M if each limit point (X∗,Y∗) of the sequence {Xk,Yk} generated by this algorithm satisfies X∗(Y∗)T = M.\nThis result shows that although (18) is a non-convex optimization problem, many standard algorithms can converge to the global optima with certain initialization. Different from all previous works on alternating minimization for matrix completion, our result does not require the algorithm to use independent samples in different iterations. To the best of our knowledge, our result is the first one that provides theoretical guarantee for alternating minimization without resampling. In addition, this result also provides the first exact recovery guarantee for many algorithms such as gradient descent, SGD and BSUM.\nAs demonstrated in [4] (and proved in [5, Theorem 1.7]), O(nr log n) entries are the minimum requirement to recover the original matrix: O(nr) is the number of degrees of freedom of a rank r matrix M, and the additional\nlog n factor is due to the coupon collector effect [4]. For r = O(1) and κ bounded, Theorem 3.1 is order optimal in terms of the sample complexity since only O(n log n) entries are needed to exactly recover M. For r = O(log n), however, our result is suboptimal by a polylogarithmic factor. The initialization has contributed r4κ4 to the sample complexity bound, and we expect that using other initialization procedures (e.g. the one proposed in [19]) can reduce the exponents of r and κ.\nTheorem 3.1 only establishes the convergence, but not the convergence speed. With some extra effort, we can prove the linear convergence of the gradient descent method (see Theorem 3.2 below). Again, this result can be extended beyond the gradient descent method. In fact, by a standard optimization argument, we can prove the linear convergence of any algorithm that satisfies “sufficient decrease” (i.e. F̃(xk) − F̃(xk+1) ≥ O(‖∇F̃(xk)‖2F)) and the requirements in Lemma 3.2; see Corollary 3.2. Many first order methods, including alternating type methods (e.g. BCGD, two-block BCD), can be shown to have the sufficient decrease property under mild conditions. For space reason, we do not verify all the methods considered in this paper, but only present the linear convergence result for the gradient descent method. The proof of Theorem 3.2 is given in Section 3.2.\nTheorem 3.2 (Linear convergence) Under the same condition of Theorem 3.1, with probability at least 1 − 2/n4, Algorithm 1a (gradient descent with constant stepsize) converges linearly; more precisely, the sequence {Xk,Yk} generated by Algorithm 1a satisfies\nF̃(Xk,Yk) ≤ (1 − 1 2 η1ξ)k, (28)\nwhere ξ = 1Cgr5κ3 pΣmin (here Cg is a numerical constant), η1 is the stepsize and η1ξ < 1.\nThe linear convergence will immediately lead to a time complexity of Õ(poly(n) log 1 ) for achieving any - optimal solution, where the Õ notation hides factors polynomial in r, κ, α. We conjecture that the time complexity bound can be improved to Õ(|Ω| log(1/ )) as observed in practice. However, finding the optimal time complexity bound is not the focus of this paper, and is left as future work.\nThe above result shows that F̃(Xk,Yk) converges to zero at a linear speed. Note that F̃(X,Y) = 0 (global convergence) only implies PΩ(M − XYT ) = 0, not necessarily M = XYT (exact reconvery). The following lemma implies that with high probability (for random Ω) the global convergence implies the exact recovery. In fact, it shows that the observed loss ‖PΩ(M − XYT )‖2F is on the order of the recovery error p‖M − XYT ‖2F if (X,Y) lies in an incoherent neighborhood of M. As discussed in the introduction, this lemma can also be viewed as a geometrical property of fΩ(Z) = ‖PΩ(M − Z)‖2F in a local incoherent region (view PΩ(Z − M) as the gradient of fΩ(Z)).\nClaim 3.1 Under the same condition of Theorem 3.1, with probability at least 1 − 1/(2n4), we have\n1 3 p‖M − XYT ‖2F ≤ ‖PΩ(M − XYT )‖2F ≤ 2p‖M − XYT ‖2F ,\n∀(X,Y) ∈ K1 ∩ K2 ∩ K(δ). (29)\nThe proof of this claim is given in Appendix D.2. This result is a simple corollary of several intermediate bounds established in the proof of Lemma 3.1."
    }, {
      "heading" : "3.1 Proof of Theorem 3.1 and main lemmas",
      "text" : "To prove Theorem 3.1, we only need to prove two lemmas which describe the local geometry of the regularized objective in (P1) and the properties of the algorithms respectively. Roughly speaking, the first lemma shows that any stationary point of (P1) in a certain region is globally optimal, and the second lemma shows that each of Algorithms\n1-4 converges to stationary points in that region. This region can be viewed as an “incoherent neighborhood” of M, and can be formally defined as K1 ∩ K2 ∩ K(δ), where K1,K2 are defined as\nK1 , {(X,Y)|X ∈ Rm×r,Y ∈ Rn×r, ‖X(i)‖ ≤ β1, ‖Y ( j)‖ ≤ β2,∀i, j}, K2 , {(X,Y)|X ∈ Rm×r,Y ∈ Rn×r, ‖X‖F ≤ βT , ‖Y‖F ≤ βT }.\n(30)\nand K(δ) is defined as K(δ) , {(X,Y)|X ∈ Rm×r,Y ∈ Rn×r, ‖M − XYT ‖F ≤ δ}. (31)\nNote that K2 = Γ(βT ) by our definition of Γ in (20). As mentioned in Section 2.1, we only need to consider a Bernolli model of Ω where each entry is included into Ω with probability p = Smn , where S satisfies (27).\nThe first lemma describes the local geometry and implies that any stationary point (X,Y) in K1 ∩ K2 ∩ K(δ) satisfies XYT = M. The main steps to derive this geometrical property is described in Section 1.3. The formal proof will be given in Section 4.\nLemma 3.1 There exist numerical constants C0,Cd such that the following holds. Assume δ is defined by (16) and Ω is generated by a Bernolli model with expected cardinality S satisfying (27) (i.e. S is lower bounded by the right hand side of (27)). Then, with probability at least 1 − 1/n4, the following holds: for all (X,Y) ∈ K1 ∩ K2 ∩ K(δ), there exist U ∈ Rm×r,V ∈ Rn×r, such that UVT = M and\n〈∇X F̃(X,Y), X − U〉 + 〈∇Y F̃(X,Y),Y − V〉 ≥ p 4 ‖M − XYT ‖2F . (32)\nThe second lemma describes the properties of the algorithms we presented. Throughout the paper, “under the same condition of Lemma 3.1” means “assume δ is defined by (16) and Ω is generated by a Bernolli model with expected cardinality S satisfying (27), where C0,Cd are the same numerical constants as those in Lemma 3.1”. The proof of Lemma 3.2 will be given in Section 5.\nLemma 3.2 Under the same conditions of Lemma 3.1, with probability at least 1 − 1/n4, the sequence (Xk,Yk) generated by either of Algorithms 1-4 has the following properties: (a) Each limit point of (Xk,Yk) is a stationary point of (P1). (b) (Xk,Yk) ∈ K1 ∩ K2 ∩ K(δ), ∀k ≥ 0.\nIntuitively, ‖X(i)k ‖, ‖Y ( j) k ‖, ‖Xk‖F , ‖Yk‖F are bounded because of the regularization terms we introduced and that the objective function is decreasing, and ‖M−XkYTk ‖F is bounded because the objective function is decreasing (however, the intuition is not enough and the proof requires some extra effort). In Section 5 we provide some easily verifiable conditions for Property (b) to hold (see Proposition 5.1), so that Lemma 3.2 and Theorem 3.1 can be extended to other algorithms.\nWith these two lemmas, the proof of Theorem 3.1 is quite straightforward and presented below.\nProof of Theorem 3.1: Consider any limit point (X∗,Y∗) of sequence {(Xk,Yk)} generated by either of Algorithms 1-4. According to Property (a) of Lemma (3.2), (X∗,Y∗) is a stationary point of problem (P1), i.e. ∇X F̃(X∗,Y∗) = 0,∇Y F̃(X∗,Y∗) = 0. According to Property (b) of Lemma 3.2, with probability at least 1− 1/n4, (Xk,Yk) ∈ K1 ∩K2 ∩ K(δ) for all k, implying (X∗,Y∗) ∈ K1 ∩K2 ∩K(δ). Then we can apply Lemma 3.1 by plugging (X,Y) = (X∗,Y∗) into (32) to conclude that with probability at least 1 − 2/n4, ‖M − X∗YT∗ ‖F ≤ 0, i.e. X∗YT∗ = M.\nRemark: Note that X∗YT∗ = M does not necessarily imply the global optimality of (X∗,Y∗) since we have not proved G(X∗,Y∗) = 0. Nevertheless, the global optimality can be easily proved using a different version of Lemma 3.1 (see the discussion before Lemma 3.3); in other words, Theorem 3.1 can be slightly strengthened to “Algorithm 1-4 converge to the global optima of problem (P1)”, instead of “Algorithm 1-4 recover M”.\nThe same argument can be used to show a more general result than Theorem 3.1, as stated in the following corollary.\nCorollary 3.1 Under the same conditions of Theorem 3.1, any algorithm satisfying Properties (a) and (b) in Lemma 3.2 reconstructs M exactly with probability at least 1 − 2/n4."
    }, {
      "heading" : "3.2 Proof of Theorem 3.2",
      "text" : "The proof of Theorem 3.2 applies a standard framework for first order methods: the convergence rate (or iteration complexity) can be derived from the “cost-to-go estimate” and the “sufficient descent” condition. For instance, the linear convergence f (xk)− f ∗ ≤ (1−c1c2)k is a direct corollary of the cost-to-go estimate ‖∇ f (xk)‖2 ≥ c1[ f (xk)− f ∗] and the sufficient descent condition f (xk) − f (xk+1) ≥ c2‖∇ f (xk)‖2, where f ∗ is the minimum value of f , and c1, c2 are certain constants. We remark that using other optimization frameworks may lead to stronger time complexity bounds; this is left as future work.\nFor our problem, a variant of Lemma 3.1 can be viewed as the cost-to-go estimate; see Lemma 3.3 below. One difference with Lemma 3.1 is the following: for a stationary point (X∗,Y∗) that ∇F̃(X∗,Y∗) = 0, Lemma 3.3 implies F̃(X∗,Y∗) = 0 (global optimality), but Lemma 3.1 implies M = X∗YT∗ (exact recovery). The relation between these two lemmas is that Lemma 3.3 is a direct consequence of (251), a slightly stronger version of Lemma 3.1. The main difficulties of proving the two lemmas are the same and lie in Proposition 4.1 and Proposition 4.2; see the formal proof in Appendix E.\nLemma 3.3 (Cost-to-go estimate) Under the same conditions of Lemma 3.1, with probability at least 1 − 1/n4, the following holds:\n‖∇F̃(X,Y)‖2F ≥ ξF̃(X,Y), ∀ (X,Y) ∈ K1 ∩ K2 ∩ K(δ), (33)\nwhere ξ = 1Cgr5κ3 pΣmin (here Cg ≥ 1 is a numerical constant).\nThe following claim shows that Algorithm 1a satisfies the sufficient descent condition. It is easy to prove: it is well known that for minimizing a function (possibly non-convex) with Lipschitz continuous gradient, the gradient descent method with constant step-size satisfies the sufficient decrease condition.\nClaim 3.2 (Sufficient descent) For the sequence xk = (Xk,Yk) generated by Algorithm 1a (gradient descent with constant stepsize), we have\nF̃(xk) − F̃(xk+1) ≥ η1 2 ‖∇F̃(xk)‖2F , (34)\nwhere η1 is the stepsize bounded above by η̄1 defined in (238).\nThe linear convergence can be easily derived from Lemma 3.1 and Claim 3.2. For completeness, we present the proof below.\nProof of Theorem 3.2: According to Property (b) of Lemma 3.2, with probability at least 1 − 1/n4, (Xk,Yk) ∈ K1 ∩ K2 ∩ K(δ) for all k. According to Lemma 3.3 and Claim 3.2, we have (with probability at least 1 − 2/n4)\nF̃(xk) − F̃(xk+1) ≥ η1 2 ‖∇F̃(xk)‖2F ≥ η1 2 ξF̃(xk), ∀k.\nThis relation can be rewritten as F̃(xk+1) ≤ (1 −\n1 2 η1ξ)F̃(xk), ∀ k. (35)\nThe stepsize η1 can be bounded as 0 < η1 ≤ η̄1 (235) ≤ 14β2T = 1 4CT rΣmax ≤ 1 Σmax\n. Since 0 < ξ = 1Cgr5κ3 pΣmin ≤ Σmin, we have 0 < η1ξ ≤ ΣminΣmax ≤ 1, which implies 0 < 1 − 1 2η1ξ < 1. Then the relation (35) leads to\nF̃(xk) ≤ (1 − 1 2 η1ξ)kF̃(x0), ∀ k,\nwhich finishes the proof.\nThe same argument can be used to show a more general result than Theorem 3.2, as stated in the following corollary. Corollary 3.2 Under the same conditions of Theorem 3.1, any algorithm satisfying Properties (a) and (b) in Lemma 3.2 and the sufficient decrease condition (34) has the linear convergence property, i.e. generates a sequence (Xk,Yk) that satisfies (28)."
    }, {
      "heading" : "4 Proof of Lemma 3.1",
      "text" : "In Section 4.1, we will show that to prove Lemma 3.1, we only need to construct U,V to satisfy three inequalities that ‖PΩ((U − X)(V − Y)T )‖F and ‖((U − X)(V − Y)T )‖F are bounded above and 〈∇XG, X − U〉 + 〈∇YG,Y − V〉 is bounded below. In Section 4.2 we describe two propositions that specify the choice of U,V , and then we show that such U,V satisfy the three desired inequalities in Section 4.2 and subsequent subsections."
    }, {
      "heading" : "4.1 Preliminary analysis",
      "text" : "Since (X,Y) ∈ K(δ), we have d , ‖M − XYT ‖F ≤ δ (16) = Σmin\nCdr1.5κ . (36)\nTo ensure (32) holds, we only need to ensure that the following two inequalities hold:\nφF = 〈∇XF, X − U〉 + 〈∇Y F,Y − V〉 ≥ p 4 d2, (37a)\nφG = 〈∇XG, X − U〉 + 〈∇YG,Y − V〉 ≥ 0. (37b)\nDefine a , U(Y − V)T + (X − U)VT , b , (U − X)(V − Y)T . (38)\nThen XYT − M = a + b, (X − U)YT + X(Y − V)T = a + 2b.\nUsing the expressions of ∇XF,∇Y F in (24), we bound φF as follows:\nφF =〈∇XF, X − U〉 + 〈∇Y F,Y − V〉 =〈PΩ(XYT − M), (X − U)YT + X(Y − V)T 〉 =〈PΩ(a + b),PΩ(a + 2b)〉 =‖PΩ(a)‖2F + 2‖PΩ(b)‖2F + 3〈PΩ(a),PΩ(b)〉 ≥‖PΩ(a)‖2F + 2‖PΩ(b)‖2F − 3‖PΩ(a)‖F‖PΩ(b)‖F .\n(39)\nThe reason to decompose M − XYT as a + b is the following. In order to bound ‖PΩ(M − XYT )‖F , we notice E(PΩ(M − XYT )) = p(M − XYT ) and wish to prove ‖PΩ(M − XYT )‖2F ≈ O(pd2). However, ‖PΩ(A)‖F could be\nas large as ‖A‖F if the matrix A is not independent of the random subset Ω (e.g. choose A s.t. A = PΩ(A)). This issue can be resolved by decomposing XYT − M as a + b and bounding ‖PΩ(a)‖F and ‖PΩ(b)‖F separately. In fact, ‖PΩ(a)‖F can be bounded because a lies in a space spanned by the matrices with the same row space or column space as M, which is independent of Ω (Theorem 4.1 in [4]). ‖PΩ(b)‖F can be bounded according to a random graph lemma of [31, 46], which requires U,V, X,Y to be incoherent (i.e. have bounded row norm).\nWe claim that (37a) is implied by the following two inequalities:\n‖PΩ(b)‖F = ‖PΩ((U − X)(V − Y)T )‖F ≤ 1 5 √ pd; (40a) ‖b‖F = ‖(U − X)(V − Y)T ‖F ≤ 1\n10 d. (40b)\nIn fact, assume (40a) and (40b) are true, we prove φF ≥ pd2/4 as follows. By XYT − M = a + b we have\n‖a‖F ≥ ‖M − XYT ‖F − ‖b‖F (40b) ≥ 9\n10 d. (41)\nRecall that the SVD of M is M = ÛΣV̂T and M satisfies the incoherence condtion (12). It follows from M = UVT = ÛΣV̂T that M,U, Û have the same column space, thus there exists some matrix B1 ∈ Rr×r such that U = ÛB1; similarly, there exists B2 ∈ Rr×r such that V = V̂B2. Therefore, by the definition of a in (38) we have\na ∈ T , {ÛWT2 + W1V̂T | W1 ∈ Rm×r,W2 ∈ Rn×r}. (42)\nBy Theorem 4.1 in [4], for |Ω| satisfying (27) with large enough C0, we have that with probability at least 1−1/(2n4), ‖PTPΩPT (a) − pPT (a)‖F ≤ 16 p‖a‖F (note that this bound holds uniformly for all a ∈ T , thus also holds when a is dependent on Ω). Since a ∈ T , this inequality can be simplified to\n‖PTPΩ(a) − pa‖F ≤ 1 6 p‖a‖F . (43)\nFollowing the analysis of [4, Corollary 4.3], we have\n‖PΩ(a)‖2F = ‖PΩPT (a)‖2F = 〈a,PTP2ΩPT (a)〉 = 〈a,PTPΩ(a)〉 = 〈a, pa〉 + 〈a,PTPΩ(a) − pa〉.\n(44)\nThe absolute value of the second term can be bounded as\n|〈a,PTPΩ(a) − pa〉| ≤ ‖a‖F‖PTPΩ(a) − pa‖F (43) ≤ 1\n6 p‖a‖2F ,\nwhich implies − 16 p‖a‖2F ≤ 〈a,PTPΩ(a) − pa〉 ≤ 1 6 p‖a‖2F . Substituting into (44), we obtain that with probability at least 1 − 1/(2n4), 5 6 ‖a‖2F ≤ ‖PΩ(a)‖2F ≤ 7 6 ‖a‖2F . (45)\nThe first inequality of the above relation implies\n‖PΩ(a)‖2F ≥ 5 6 ‖a‖2F (41) ≥ 27 40 pd2. (46)\nAccording to (39) and the bounds (46) and (40a), we have φF/(pd2) ≥ 2740 + 2( 1 5 ) 2 − 35 √ 27 40 ≥ 1 4 , which proves (37a).\nIn summary, to find a factorization M = UVT such that (32) holds, we only need to ensure that the factorization satisfies (40b), (40a) and (37b). In the following three subsections, we will show that such a factorization M = UVT exists. Specifically, U,V will be defined in Table 7 and the three desired inequalities will be proved in Corollary 4.2, Proposition 4.3 and Claim 4.1 respectively.\n4.2 Definitions of U,V and key technical results\nWe construct U,V according to two propositions, which will be stated in this subsection and proved in the appendix. The first proposition states that if XYT is close to M, then there exists a factorization M = UVT such that U (resp. V) is close to X (resp. Y), and U,V are incoherent. Roughly speaking, this proposition shows the continuity of the factorization map Z = XYT 7→ (X,Y) near a low-rank matrix M. The condition X,Y ∈ K1 ∩ K2 ∩ K(δ) and (16) implies that d , ‖M − XYT ‖F ≤ δ = ΣminCdr1.5κ and ‖X‖F ≤ βT , ‖Y‖F ≤ βT , thus for large enough Cd, the assumptions of Proposition 4.1 hold. Similarly, the assumptions of the other results in this subsection also hold.\nProposition 4.1 Suppose M ∈ Rm×n is a rank-r matrix with Σmax (Σmin) being the largest (smallest) non-zero singular value, and M is µ-incoherent. There exists a numerical constant CT such that the following holds: If\nd , ‖M − XYT ‖F ≤ Σmin\n11r , (47a)\n‖X‖F ≤ βT , ‖Y‖F ≤ βT , (47b)\nwhere βT = √ CT rΣmax, then there exist U ∈ Rm×r,V ∈ Rn×r such that\nUVT = M, (48a)\n‖U‖F ≤ (1 − d\nΣmin )‖X‖F , (48b)\n‖U − X‖F ≤ 6βT\n5Σmin d, ‖V − Y‖F ≤ 3βT Σmin d, (48c)\n‖U(i)‖2 ≤ rµ m β2T , ‖V ( j)‖2 ≤ 3rµ 2n β2T . (48d)\nThe proof of Proposition 4.1 is given in Appendix B.\nRemark 1: A symmetric result that switches X,U and Y,V in the above proposition holds: under the conditions of Proposition (4.1), there exist U,V satisfying (48) with U,V reversed, i.e. UVT = M, ‖V‖F(1 − dΣmin ) ≤ ‖Y‖F , ‖U − X‖F ≤ 3βTΣmin d, ‖V − Y‖F ≤ 6βT 5Σmin d, and ‖U(i)‖2 ≤ 3rµ2m β2T , ‖V ( j)‖2 ≤ rµ n β 2 T .\nRemark 2: To prove Theorem 3.1 (convergence), we only need ‖U‖F ≤ ‖X‖F ; here the slightly stronger requirement ‖U‖F ≤ (1 − dΣmin )‖X‖F is for the purpose of proving Theorem 3.2 (linear convergence).\nRemark 3: Without the incoherence assumption on M, by the same proof we can show that there still exist U,V satisfying (48a) and (48c), i.e. M = UVT and U,V are close to X,Y respectively. Such a result bears some similarity with the classical perturbation theory for singular value decomposition [45]. In particular, [45] proved that for two low-rank matrices4 that are close, the spaces spanned by the left (resp. right) singular vectors of the two matrices are also close. Note that the singular vectors themselves may be very sensitive to perturbations and no such perturbation bounds can be established (see [63, Sec. 6]). The difference of our work with the classical perturbation theory is that we do not consider SVD of two matrices; instead, we allow one matrix to have an arbitrary factorization, and the factorization of the other matrix can be chosen accordingly. Since we do not have any restriction on the factorization XYT (except the dimensions) and the norms of X and Y can be arbitrarily large, the distance between two corresponding factors has to be proportional to the norm of one single factor, which explains the coefficient βT in (48c).\nUnfortunately, Proposition 4.1 is not strong enough to prove φG ≥ 0 when both ‖X‖F and ‖Y‖F are large (see an analysis in Section 4.4). To resolve this issue, we need to prove the second proposition in which there is an additional\n4The result in [45] also covered the case of two approximately low-rank matrices, but we only consider the case of exact low-rank matrices here.\nassumption that both ‖X‖F and ‖Y‖F are large, and an additional requirement that both ‖U‖F and ‖V‖F are bounded (by the norms of original factors ‖X‖F and ‖Y‖F respectively). More specifically, the proposition states that if M is close to XYT , and both ‖X‖F and ‖Y‖F are large, then there is a factorization M = UVT such that U (resp. V) is close to X (resp. Y), and ‖U‖F ≤ ‖X‖F , ‖V‖F ≤ ‖Y‖F . For the purpose of proving linear convergence, we prove a slightly stronger result that ‖V‖F ≤ (1 − d/Σmin)‖Y‖F . The previous result Proposition 4.1 can be viewed as a perturbation analysis for an arbitrary factorization, while Proposition 4.2 can be viewed as an enhanced perturbation analysis for a constrained factorization. Although Proposition 4.2 is just a simple variant of Proposition 4.1, it seems to require a much more involved proof than Proposition 4.1. See the formal proof of Proposition 4.2 in Appendix C.\nProposition 4.2 Suppose M ∈ Rm×n is a rank-r matrix with Σmax (Σmin) being the largest (smallest) non-zero singular value, and M is µ-incoherent. There exist numerical constants Cd,CT such that the following holds: if\nd , ‖M − XYT ‖F ≤ Σmin\nCdr , (49a)√\n2 3 βT ≤ ‖X‖F ≤ βT , √ 2 3 βT ≤ ‖Y‖F ≤ βT , (49b)\nwhere βT = √ CT rΣmax, then there exist U ∈ Rm×r,V ∈ Rn×r such that\nUVT = M, (50a)\n‖U‖F ≤ ‖X‖F , ‖V‖F ≤ (1 − d\nΣmin )‖Y‖F , (50b)\n‖U − X‖F‖V − Y‖F ≤ 65 √ r β2T\nΣ2min d2,\nmax{‖U − X‖F , ‖V − Y‖F} ≤ 17 2 √ r βT Σmin d, (50c)\n‖U(i)‖2 ≤ rµ m β2T , ‖V ( j)‖2 ≤ rµ n β2T . (50d)\nRemark: A symmetric result that switches X,U and Y,V in the above proposition still holds; the only change is that (50b) will become ‖U‖F ≤ (1 − dΣmin )‖X‖F , ‖V‖F ≤ ‖Y‖F . It is easy to prove a variant of the above proposition in which (50b) is changed to ‖U‖F ≤ (1− d2Σmin )‖X‖F , ‖V‖F ≤ (1− d 2Σmin\n)‖Y‖F ; in other words, the asymmetry of X,U and Y,V in (50b) is artificial. Nevertheless, Proposition 4.2 is enough for our purpose.\nThroughout the proof of Lemma 3.1, U,V are defined in Table 4.2.\nAccording to Proposition 4.1 and Proposition 4.2 (and their symmetric results), the properties of U,V defined in Tabel 7 are summarized in the following corollary. For simplicity, we only present the case that ‖X‖F ≤ ‖Y‖F ; in the other case that ‖X‖F > ‖Y‖F , a symmetric result of Corollary 4.1 holds.\nCorollary 4.1 Suppose d , ‖XYT − M‖F ≤ ΣminCdr and ‖X‖F ≤ ‖Y‖F , then U,V defined in Table 7 satisfy:\nUVT = M; (51a)\n‖U − X‖F‖V − Y‖F ≤ 65 √ r β2T\nΣ2min d2;\nmax{‖U − X‖F , ‖V − Y‖F} ≤ 17 2 √ r βT Σmin d, (51b)\n‖U(i)‖2 ≤ 3 2 rµ m β2T , ‖V ( j)‖2 ≤ 3 2 rµ n β2T ; (51c)\n‖V‖F ≤ (1 − d\nΣmin )‖Y‖F ; if ‖X‖F > √ 2 3 βT , then ‖U‖F ≤ ‖X‖F . (51d)\nIn (51b), we bound ‖U −X‖F‖V −Y‖F by O(d2) with a rather complicated coefficient, but to prove (40b) we need a bound O(d) with a coefficient 1/10. Under a slightly stronger condition on d than that of Corollary 4.1, which still holds for (X,Y) ∈ K(δ) with δ defined in (16), we can prove the bound (40b) by (51b).\nCorollary 4.2 There exists a numerical constant Cd such that if\nd , ‖M − XYT ‖F ≤ Σmin\nCdr1.5κ , (52)\nthen U,V defined in Table 7 satisfy (40b).\nProof of Corollary 4.2: According to (51b) , we have\n‖U − X‖F‖V − Y‖F ≤ 65 β2T\nΣ2min\n√ rd2 = 65CT r1.5 Σmax\nΣ2min d2\n= 65CT r1.5κ d Σmin d ≤ 1 10 d,\nwhere the last inequliaty follows from (52) with Cd ≥ 650CT .\nIn the next two subsections, we will use the properties in Corollary 4.1 to prove (40a) and (37b).\n4.3 Upper bound on ‖PΩ((U − X)(V − Y)T )‖F\nThe following result states that for U,V defined in Table 7, (40a) holds.\nProposition 4.3 Under the same conditions as Lemma 3.1, with probability at least 1 − 1/(2n4), the following is true. For any (X,Y) ∈ K1 ∩ K2 ∩ K(δ) and U,V defined in Table 7, we have\n‖PΩ((U − X)(V − Y)T )‖2F ≤ p\n25 ‖M − XYT ‖2F . (53)\nProof of Proposition 4.3: We need the following random graph lemma [31, Lemma 7.1].\nLemma 4.1 There exist numerical constants C0,C1 such that if |Ω| ≥ C0 √ αn log n, then with probability at least 1 − 1/(2n4), for all x ∈ Rm, y ∈ Rn, ∑ (i, j)∈Ω xiy j ≤ C1 p‖x‖1‖y‖1 + C1α 3 4 √ np‖x‖2‖y‖2. (54)\nLet Z = U − X,W = V − Y and zi = ‖Z(i)‖2, w j = ‖W ( j)‖2. We have\n‖PΩ((U − X)(V − Y)T )‖2F = ∑\n(i, j)∈Ω (ZWT )2i j\n≤ ∑\n(i, j)∈Ω ‖Z(i)‖2‖W ( j)‖2 = ∑ (i, j)∈Ω ziw j. (55)\nInvoking Lemma 4.1, we have\n‖PΩ((U − X)(V − Y)T )‖2F ≤ C1 p‖z‖1‖w‖1 + C1α 3 4 √ np‖z‖2‖w‖2. (56)\nAnalogous to the proof of (40b) in Corollary 4.2, we can prove that ‖U − X‖F‖V − Y‖F ≤ d/(10 √\nC1) for large enough Cd (in fact, Cd ≥ 650CT √ C1 suffices). Therefore, we have\n‖z‖1‖w‖1 = ‖Z‖2F‖W‖2F = ‖U − X‖2F‖V − Y‖2F ≤ 1\n100C1 d2. (57)\nWe still need to bound ‖z‖2 and ‖w‖2. We have\n‖z‖2 = √∑\ni\n‖Z(i)‖4 ≤ √\nmax i ‖Z(i)‖2 ∑ j ‖Z( j)‖2\n≤ max i (‖U(i)‖ + ‖X(i)‖)‖U − X‖F ≤ ( √\n3rµ 2m βT + β1)‖U − X‖F\n≤ √ 8 √\nrµ m βT ‖U − X‖F .\n(58)\nHere, the third inequliaty follows from the property (51c) in Corollary 4.1 and the condition (X,Y) ∈ K1 (which implies ‖X(i)‖ ≤ β1), and the fourth inequliaty follows from the definition of β1 in (15). Similarly,\n‖w‖2 ≤max j (‖V ( j)‖ + ‖Y ( j)‖)‖V − Y‖F\n≤ √ 8 √\nrµ n βT ‖V − Y‖F .\n(59)\nMultiplying (58) and (59), we get\n‖z‖2‖w‖2 ≤ 8 rµ √\nmn β2T ‖U − X‖F‖V − Y‖F\n(51b) ≤ 8 rµ√\nmn β2T 65\n√ r β2T\nΣ2min d2\n(15) = 520C2T 1 √\nmn µr3.5κ2d2.\nThus the second term in (56) can be bounded as\nC1α 3 4 √ np‖z‖2‖w‖2 ≤ 520C1C2T α\n3 4 √\nnp √ mn µr3.5κ2d2 ≤ 3 100 pd2, (60)\nwhere the last inequality is equivalent to 5202C21C 4 Tα 3 2 µ2r7κ4 ≤ 91002 |Ω|/n, which holds due to (27) with large enough numerical constant C0. Plugging (57) and (60) into (56), we get ‖PΩ((U − X)(V − Y)T )‖2F ≤ p 25 d 2 = p 25‖M − XYT ‖2F ."
    }, {
      "heading" : "4.4 Lower bound on φG",
      "text" : "In this subsection, we prove the following claim.\nClaim 4.1 U,V defined in Table 7 satisfy (37b), i.e. φG = 〈∇XG, X − U〉 + 〈∇YG,Y − V〉 ≥ 0.\nProof of Claim 4.1:\nBy the expressions of ∇XG,∇YG in (24), we have\nφG = 〈∇XG, X − U〉 + 〈∇YG,Y − V〉 =\nρ m∑ i=1 G′0( 3‖X(i)‖2 2β21 ) 3 β21 〈X(i), X(i) − U(i)〉 + ρG′0( 3‖X‖2F 2β2T ) 3 β2T 〈X, X − U〉\n+ρ n∑ j=1 G′0( 3‖Y ( j)‖2 2β22 ) 3 β22 〈Y ( j),Y ( j) − V ( j)〉 + ρG′0( 3‖Y‖2F 2β2T ) 3 β2T 〈Y,Y − V〉,\n(61)\nwhere G′0(z) = I[1,∞](z)2(z − 1).\nFirstly, we prove\nh1i , G′0( 3‖X(i)‖2\n2β21 ) 3 β21 〈X(i), X(i) − U(i)〉 ≥ 0, ∀ i, (62a)\nh3 j , G′0( 3‖Y ( j)‖2\n2β22 ) 3 β22 〈Y ( j),Y ( j) − V ( j)〉 ≥ 0, ∀ j. (62b)\nWe only need to prove (62a); the proof of (62b) is similar. We consider two cases.\nCase 1: ‖X(i)‖2 ≤ 2β 2 1 3 . Note that 3‖X(i)‖2 2β21 ≤ 1 implies G′0( 3‖X(i)‖2 2β21 ) = 0, thus h1i = 0.\nCase 2: ‖X(i)‖2 > 2β 2 1\n3 . By Corollary 4.1 and the fact that β 2 1 = β 2 T 3µr m , we have\n‖U(i)‖2 ≤ 3rµ 2m β2T ≤ 2β21 3 < ‖X(i)‖2. (63)\nAs a result, 〈X(i), X(i)〉 = ‖X(i)‖‖X(i)‖ > ‖X(i)‖‖U(i)‖ ≥ 〈X(i),U(i)〉, which implies 〈X(i), X(i) −U(i)〉 ≥ 0. Combining this inequality with the fact that G′0( 3‖X(i)‖2 2β21 ) ≥ 0, we get h1i ≥ 0.\nSecondly, we prove\nh2 + h4 ≥ 0,\nwhere h2 , G′0( 3‖X‖2F 2β2T ) 3 β2T 〈X, X − U〉,\nh4 , G′0( 3‖Y‖2F 2β2T ) 3 β2T 〈Y,Y − V〉.\n(64)\nWithout loss of generality, we can assume ‖X‖F ≤ ‖Y‖F , and we will apply Corollary 4.1 to prove (64). If ‖Y‖F < ‖X‖F , we can apply a symmetric result of Corollary 4.1 to prove (64). We further consider three cases.\nCase 1: ‖X‖F ≤ ‖Y‖F ≤ √ 2 3βT . In this case G ′ 0( 3‖X‖2F 2β2T ) = G′0( 3‖Y‖2F 2β2T\n) = 0, which implies h2 = h4 = 0, thus (64) holds.\nCase 2: ‖X‖F ≤ √ 2 3βT < ‖Y‖F . Then G′0( 3‖X‖2F 2β2T\n) = 0, which implies h2 = 0. By (51d) in Corollary 4.1 we have ‖V‖F ≤ ‖Y‖F , which implies 〈Y,Y〉 ≥ ‖Y‖F‖V‖F ≥ 〈Y,V〉, i.e. 〈Y,Y − V〉 ≥ 0. Combined with the nonnegativity of G′0(·), we get h4 ≥ 0. Thus h2 + h4 = h4 ≥ 0.\nCase 3: √\n2 3βT < ‖X‖F ≤ ‖Y‖F . By (51d) in Corollary 4.1, we have ‖U‖F ≤ ‖X‖F and ‖V‖F ≤ ‖Y‖F . Similar to\nthe argument in Case 2 we can prove h2 ≥ 0, h4 ≥ 0 and (64) follows.\nIn all three cases, we have proved (64), thus (64) holds.\nWe conclude that for U,V defined in Table 7,\nφG (61) = ρ ∑ i h1i + ∑ j h3 j + h2 + h4  (62),(64)≥ 0, which finishes the proof of Claim 4.1.\nRemark: Based on the above proof, we can explain why Proposition 4.1 is not enough to prove φG ≥ 0. Note that h2 = 0 when ‖X‖F > √ 2 3βT and h4 = 0 when ‖Y‖F > √ 2 3βT . To prove h2 ≥ 0, h4 ≥ 0, it suffices to prove: (i)\n‖U‖F ≤ ‖X‖F when ‖X‖F > √ 2 3βT ; (ii) ‖V‖F ≤ ‖Y‖F when ‖Y‖F > √ 2 3βT . For the choice of U,V in Proposition 4.1, we have ‖U‖F ≤ ‖X‖F , but there is no guarantee that (ii) holds. Similarly, for the choice of U,V in the symmetric result of Proposition 4.1, we have ‖V‖F ≤ ‖Y‖F , but there is no guarantee that (i) holds. Thus, Proposition 4.1 is not enough to prove φG ≥ 0. To guarantee that (i) and (ii) hold simultaneously, we need a complementary result for the case ‖X‖F > √ 2 3βT , ‖Y‖F > √ 2 3βT . This motivates our Proposition 4.2."
    }, {
      "heading" : "5 Proof of Lemma 3.2",
      "text" : "Property (a) in Lemma 3.2 (convergence to stationary points) is a basic requirement for many reasonable algorithms and can be proved using classical results in optimization, so the difficulty mainly lies in how to prove Property (b). We will give some easily verifiable conditions for Property (b) to hold and then show that Algorithms 1-4 satisfy these conditions. This proof framework can be used to extend Theorem 3.1 to many other algorithms.\nThe following claim states that Algorithms 1-4 satisfy Property (a). The proof of this claim is given in Appendix D.5.\nClaim 5.1 Suppose Ω satisfies (29), then each limit point of the sequence generated by Algorithms 1-4 is a stationary point of problem (P1).\nFor Property (b), we first show that the initial point (X0,Y0) lies in an incoherent neighborhood ( √ 2 3 K1) ∩\n( √\n2 3 K2) ∩ Kδ0 , where cKi denotes the set {(cX, cY) | (X,Y) ∈ Ki}, i = 1, 2. The proof of Claim 5.2 will be given in Appendix D.1. The purpose of proving (X0,Y0) ∈ ( √ 2 3 K1) ∩ ( √ 2 3 K2) rather than (X0,Y0) ∈ K1 ∩ K2 is to guarantee that G(X0,Y0) = 0, where G is the regularizer defined in (13).\nClaim 5.2 Under the same condition of Lemma 3.1, with probability at least 1 − 1/(2n4), (X0,Y0) given by the procedure Initialize belongs to ( √ 2 3 K1) ∩ ( √ 2 3 K2) ∩ Kδ0 , where δ0 is defined by (16), i.e.\n(a) ‖X(i)0 ‖ ≤ √ 2 3β1, i = 1, 2, . . . ,m; ‖Y ( j) 0 ‖ ≤ √ 2 3β2, j = 1, . . . , n;\n(b) ‖X0‖F ≤ √ 2 3βT , ‖Y0‖F ≤ √ 2 3βT ; (c) ‖M − X0YT0 ‖F ≤ δ0.\nThe next result provides some general conditions for (Xt,Yt) to lie in K1 ∩ K2 ∩ K(δ). To simplify the notations, denote xt , (Xt,Yt) and\nu∗ , (ÛΣ1/2, V̂Σ1/2),\nwhere ÛΣV̂ is the SVD of M. Recall that F̃(u∗) = 0 (proved in the paragraph after (19)). We say a function ψ(x̄,∆; λ) is a convex tight upper bound of F̃(x) along the direction ∆ at x̄ if\nψ(x̄,∆; λ) is convex over λ ∈ R; (65a) ψ(x̄,∆; λ) ≥ F̃(x̄ + λ∆), ∀ λ ∈ R; ψ(x̄,∆; 0) = F̃(x̄). (65b)\nFor example, ψ(x̄,∆; λ) = F̃(x̄ + λ∆) satisfies (65) for either ∆ = (X, 0) or ∆ = (0,Y), where X ∈ Rm×r and Y ∈ Rn×r are arbitrary matrices. This definition is motivated by the block successive upper bound minimization method [55]. The proof of Proposition 5.1 is given in Appendix D.3.\nProposition 5.1 Suppose the sample set Ω satisfies (29) and δ, δ0 are defined by (16). Consider an algorithm that starts from a point x0 = (X0,Y0) and generates a sequence {xt} = {(Xt,Yt)}. Suppose x0 satisfies\nx0 ∈ ( √\n2 3\nK1) ∩ ( √\n2 3 K2) ∩ K(δ0), (66)\nand {xt} satisfies either of the following three conditions:\n1) F̃(xt + λ∆t) ≤ 2F̃(x0),∀ λ ∈ [0, 1], where ∆t = xt+1 − xt, ∀ t; (67a)\n2) 1 = arg min λ∈R ψ(xt,∆t; λ),\nwhere ψ satisfies (65),∆t = xt+1 − xt, ∀ t; (67b)\n3) F̃(xt) ≤ 2F̃(x0), d(xt,x0) ≤ 5 6 δ, ∀ t. (67c)\nThen xt = (Xt,Yt) ∈ K1 ∩ K2 ∩ K(2δ/3), for all t ≥ 0.\nThe first condition means that F̃ is bounded above by 2F̃(x0) over the line segment between xt and xt+1 for any t. This condition holds for gradient descent or SGD with small enough stepsize (see Claim 5.3). The second condition means that the new point xt+1 is the minimum of a convex tight upper bound of the original function along the direction xt+1 −xt, and holds for BCD type methods such as Algorithm 2 and Algorithm 3 (see Claim 5.3). Note that the gradient descent method with exact line search stepsize does not satisfy this condition since F̃ is not jointly convex in the variable (X,Y). The third condition means that F̃(xt) is bounded above and xt is not far from x0 for any t. For standard nonlinear optimization algorithms, it is not easy to prove that xt is not far from x0. However, as done by Algorithm 1 with restricted Armijo rule or restricted line search, we can force d(xt,x0) ≤ 56δ to hold when computing the new point xt.\nThe following claim shows that each of Algorithm 1-4 satisfies one of the three conditions in (67). The proof of Claim 5.3 is given in Appendix D.4.\nClaim 5.3 The sequence {xt} generated by Algorithm 1 with either restricted Armijo rule or restricted line search satisfies (67c). The sequence {xt} generated by either Algorithm 2 or Algorithm 3 satisfies (67b). Suppose the sample set Ω satisfies (29), then the sequence {xt} generated by either Algorithm 1 with constant stepsize or Algorithm 4 satisfies (67a).\nTo put things together, Claim 5.1 shows Algorithms 1-4 satisfy Property (a), and Proposition 5.1 together with Claim 5.2 and Claim 5.3 shows that Algorithms 1-4 satisfy Property (b). Therefore, we have proved Lemma 3.2."
    }, {
      "heading" : "A Supplemental Material for Section 2",
      "text" : ""
    }, {
      "heading" : "A.1 Proof of Claim 2.1",
      "text" : "This proof is quite straightforward and we mainly use the triangular inequalities and the boundedness of the considered region Γ(β0). In this proof, f ′(x) denotes the derivative of a function f at x.\nSince (X,Y), (U,V) belong to Γ(β0), we have\n‖X‖F ≤ β0, ‖Y‖F ≤ β0, ‖U‖F ≤ β0, ‖V‖F ≤ β0. (68)\nWe first prove ‖∇F(X,Y) − ∇F(U,V)‖F ≤ 4β20‖(X,Y) − (U,V)‖F . (69)\nBy the triangular inequality, we have\n‖∇XF(X,Y) − ∇XF(U,V)‖F ≤ ‖∇XF(X,Y) − ∇XF(U,Y)‖F +‖∇XF(U,Y) − ∇XF(U,V)‖F .\n(70)\nThe first term of (70) can be bounded as follows\n‖∇XF(X,Y) − ∇XF(U,Y)‖F = ‖PΩ(XYT − M)Y − PΩ(UYT − M)Y‖F ≤ ‖PΩ(XYT − M) − PΩ(UYT − M)‖F‖Y‖F = ‖PΩ[(X − U)YT ]‖F‖Y‖F ≤ ‖(X − U)YT ‖F‖Y‖F ≤ ‖X − U‖F‖Y‖2F ≤ ‖X − U‖Fβ20.\nThe second term of (70) can be bounded as\n‖∇XF(U,Y) − ∇XF(U,V)‖F = ‖PΩ(UYT − M)Y − PΩ(UVT − M)V‖F ≤ ‖PΩ(M)(V − Y)‖F + ‖PΩ(UYT )Y − PΩ(UVT )V‖F ≤ ‖PΩ(M)(V − Y)‖F + ‖PΩ(UYT )Y − PΩ(UYT )V‖F\n+ ‖PΩ(UYT )V − PΩ(UVT )V‖F ≤ ‖PΩ(M)‖F‖V − Y‖F + ‖PΩ(UYT )‖F‖Y − V‖F\n+ ‖PΩ[U(Y − V)T ]‖F‖V‖F ≤ ‖M‖F‖V − Y‖F + ‖U‖F‖Y‖F‖Y − V‖F + ‖U‖F‖Y − V‖F‖V‖F ≤ 3β20‖Y − V‖F ,\nwhere the last inequliaty follows from (68) and the fact that ‖M‖F ≤ √\nrΣmax (15) = 1CT √ rβ 2 T ≤ β2T ≤ β20 (here the second\nlast inequality follows from the fact that the numerical constant CT ≥ 1, and the last inequality follows from the assumption of Claim 2.1).\nPlugging the above two bounds into (70), we obtain\n‖∇XF(X,Y) − ∇XF(U,V)‖F ≤ β20(‖X − U‖F + 3‖Y − V‖F).\nSimilarly, we have ‖∇Y F(X,Y) − ∇Y F(U,V)‖F ≤ β20(3‖X − U‖F + ‖Y − V‖F).\nCombining the above two relations, we have (denote ω1 , ‖X − U‖F , ω2 , ‖Y − V‖F)\n‖∇F(X,Y) − ∇F(U,V)‖F\n= √ ‖∇XF(X,Y) − ∇XF(U,V)‖2F + ‖∇Y F(X,Y) − ∇Y F(U,V)‖2F\n≤ β20 √ (ω1 + 3ω2)2 + (3ω1 + ω2)2\n≤ 4β20 √ ω21 + ω 2 2 = 4β20‖(X,Y) − (U,V)‖F ,\nwhich proves (69).\nNext we prove\n‖∇G(X,Y) − ∇G(U,V)‖F ≤ 54ρ β20\nβ41 ‖(X,Y) − (U,V)‖F . (71)\nDenote\nG1i(X) , G0 3‖X(i)‖2 2β21  , G2(X) , G0 3‖X‖2F 2β2T  , (72) then we have\n∇G1i(X) = G′0 3‖X(i)‖2\n2β21  3X̄(i) β21 , ∇G2(X) = G′0 3‖X‖2F 2β2T  3X β2T , (73)\nwhere G′0(z) = I[1,∞](z)2(z − 1) and X̄(i) denotes a matrix with the i-th row being X(i) and the other rows being zero. Obviously G1i(X) is a matrix with all but the i-th row being zero. Recall that\nG(X,Y) = ρ ∑\ni\nG1i(X) + ρG2(X) + f0(Y),\nwhere f0(Y) is a certain function of Y which we can ignore for now. Then we have ∇XG(X,Y) = ρ ∑\ni\n∇G1i(X) + ρ∇G2(X)\n= ρ m∑ i=1 G′0 3‖X(i)‖2 2β21  3X̄(i) β21 + ρG′0 3‖X‖2F 2β2T  3X β2T ,\n(74)\nand, similarly, ∇XG(U,V) = ρ ∑ i ∇G1i(U) + ρG2(U).\nTherefore, we have\n‖∇XG(X,Y) − ∇XG(U,V)‖F = ‖ρ ∑ i [∇G1i(X) − ∇G1i(U)] + ρ[∇G2(X) − ∇G2(U)]‖F\n≤ ‖ρ ∑\ni\n[∇G1i(X) − ∇G1i(U)]‖F + ρ‖∇G2(X) − ∇G2(U)‖F\n= ρ √∑ i ‖∇G1i(X) − ∇G1i(U)‖2F + ρ‖∇G2(X) − ∇G2(U)‖F ,\n(75)\nwhere the last equality is due to the fact that each ∇G1i(X)−∇G1i(U) is a matrix with all but the i-th row being zero. Denote\nz1 , 3‖X‖2F 2β2T , z2 , 3‖U‖2F 2β2T . (76)\nThen by (76), (73) and the triangle inequality we have\nβ2T 3 ‖∇G2(X) − ∇G2(U)‖F =‖G′0(z1)X −G′0(z2)U‖F ≤ |G′0(z1)|‖X − U‖F + |G′0(z1) −G′0(z2)|‖U‖F .\n(77)\nBy the definitions of z1, z2 in (76) and using ‖X‖F ≤ β0, ‖Y‖F ≤ β0, we have\n|z1 − z2| = 3\n2β2T (‖X‖2F − ‖U‖2F)\n= 3\n2β2T (‖X‖F + ‖U‖F)(‖X‖F − ‖U‖F)\n≤ 3β0 β2T ‖X − U‖F .\n(78)\nAccording to (68) and the definitions of z1, z2 in (76), we have\nmax{z1, z2} ≤ 3 2 β20\nβ2T . (79)\nWe can bound the first and second order derivative of G0 as follows:\nG′0(z) = I[1,∞](z)2(z − 1) ≤ 3 β20 β2T , ∀z ∈ [0, 3 2 β20 β2T ], (80)\nG′′0 (z) = 2I[1,∞](z) ≤ 2, ∀z ∈ [0,∞). (81)\nBy the mean value theorem and (81), we have\n|G′0(z1) −G′0(z2)| ≤ 2|z1 − z2| (78) ≤ 6β0\nβ2T ‖X − U‖F . (82)\nPlugging (80) (with z = z1) and (82) into (77), we obtain\nβ2T 3 ‖∇G2(X) − ∇G2(U)‖F ≤ 3 β20\nβ2T ‖X − U‖F + 6β0 β2T ‖X − U‖F‖U‖F\n≤ 9 β20\nβ2T ‖X − U‖F\n=⇒ ‖∇G2(X) − ∇G2(U)‖F ≤ 27 β20\nβ4T ‖X − U‖F . (83)\nSince ‖X(i)‖F ≤ ‖X‖F ≤ β0, ‖U(i)‖ ≤ ‖U‖F ≤ β0, by an argument analogous to that for (83), we can prove\n‖∇G1i(X) − ∇G1i(U)‖F ≤ 27 β20\nβ41 ‖X(i) − U(i)‖, ∀ i,\nwhich further implies √∑ i ‖∇G1i(X) − ∇G1i(U)‖2\n≤ 27 β20\nβ41 √∑ i ‖X(i) − U(i)‖2 = 27 β20 β41 ‖X − U‖F .\n(84)\nPlugging (83) and (84) into (75), we obtain\n‖∇XG(X,Y) − ∇XG(U,V)‖F ≤ 54ρ β20\nβ41 ‖X − U‖F .\nSimilarly, we can prove\n‖∇YG(X,Y) − ∇YG(U,V)‖F ≤ 54ρ β20\nβ42 ‖Y − V‖F ≤ 54ρ\nβ20 β41 ‖Y − V‖F ,\nwhere the last inequality is due to β1 = βT √ 3µr m ≤ βT √ 3µr n = β2. Combining the above two relations yields (71).\nFinally, we combine (69) and (71) to obtain\n‖∇F̃(X,Y) − ∇F̃(U,V)‖F ≤ ‖∇F(X,Y) − ∇F(U,V)‖F + ‖∇G(X,Y) − ∇G(U,V)‖F\n≤ 4β20 + 54ρβ20β41  ‖(X,Y) − (U,V)‖F , which finishes the proof of Claim 2.1.\nRemark: If we further assume that the norm of each X(i) (resp. Y ( j)) is bounded by O(β1) (resp. O(β2)), the Lipschitz constant can be improved to 4β20 + 54ρ β20 β4T ."
    }, {
      "heading" : "A.2 Solving the Subproblem of Algorithm 3",
      "text" : "The subproblem of Algorithm 3 for the row vector X(i) is\nmin X(i)\nF̃(X(1)k , . . . , X (i−1) k , X (i), X(i+1)k−1 . . . , X (m) k−1,Yk−1) + λ0 2 ‖X(i) − X(i)k−1‖ 2.\nFor simplicity, denote X(i) = xi, X (i) k−1 = x̄i, X ( j) k = x j, 1 ≤ j ≤ i− 1, X ( j) k−1 = x j, i + 1 ≤ j ≤ m, and Y ( j) k−1 = y j, 1 ≤ j ≤ n. Then the above problem becomes\nmin xi F̃(x1, . . . , xi−1, xi, xi+1, . . . , xm, y1, . . . , yn) + λ0 2 ‖xi − x̄i‖2.\nThe optimal solution x∗i to this subproblem satisfies the equation ∇xi F̃ = 0, i.e.\nAxi − b + g(‖xi‖)xi = 0, (85)\nwhere A = ∑\nj∈Ωxi y jy T j + λ0I is a symmetric PD (positive definite) matrix, b = ∑ j∈Ωxi Mi jy j + λ0 x̄i, and g is a function\ndefined as\ng(z) = ρ 3 β21 G′0( 3z2 2β21 ) + ρ 3 β2T G′0( 3(z2 + ξi) 2β2T ),\nin which ξi = ∑ j,i ‖x j‖2 is a constant. Note that g has the following properties: a) g(z) = 0 when z2 ≤ min{ 2β21 3 , 2β2T 3 − ξi} ; b) g is an increasing function in [0,∞). The equation (85) is equivalent to\nxi = (A + g(‖xi‖)I)−1b. (86)\nSuppose the eigendecomposition of A is BΛBT and let Φ = BT bbT B, then (86) implies\n‖xi‖2 = ‖(A + g(‖xi‖)I)−1b‖2 = Tr((A + g(‖xi‖)I)−2bbT )\n= Tr((Λ + g(‖xi‖)I)−2Φ) = r∑\nk=1\nΦkk\n(Λkk + g(‖xi‖))2 ,\n=⇒ 1 = 1‖xi‖2 r∑\nk=1\nΦkk\n(Λkk + g(‖xi‖))2 , (87)\nwhere Zkk denotes the (k, k)-th entry of matrix Z. Since A and Φ are PSD (positive semidefinite) matrices, we have Φkk ≥ 0,Λkk ≥ 0. The righthand side of (87) is a decreasing function of ‖xi‖, thus the equation (87) can be solved via a simple bisection procedure. After obtaining the norm of the optimal solution z∗ = ‖x∗i ‖, the optimal solution x∗i can be obtained by (86), i.e.\nx∗i = (A + g(z ∗)I)−1b. (88)\nSimilarly, the subproblem for Y ( j) can also be solved by a bisection procedure."
    }, {
      "heading" : "B Proof of Proposition 4.1",
      "text" : ""
    }, {
      "heading" : "B.1 Matrix norm inequalities",
      "text" : "We first prove some basic inequalities related to the matrix norms. These simple results will be used in the proof of Propositions 4.1 and 4.2.\nProposition B.1 If A, B ∈ Rn1×n2 , then\n‖A − B‖2 ≥ σmin(A) − σmin(B). (89)\nProof: σmin(A) = min‖v‖=1 ‖Av‖ ≤ min‖v‖=1(‖Bv‖ + ‖(A − B)v‖) ≤ min‖v‖=1 ‖Bv‖ + ‖A − B‖ = σmin(B) + ‖A − B‖.\nProposition B.2 For any A ∈ Rn1×n2 , B ∈ Rn2×n3 , we have\nσmin(AB) ≤ σmin(A)‖B‖2. (90)\nProof: σmin(AB) = minv∈Rn1×1,‖v‖=1 ‖vT AB‖ ≤ minv∈Rn1×1,‖v‖=1 ‖vT A‖‖B‖2 = σmin(A)‖B‖2.\nProposition B.3 Suppose A, B ∈ Rn1×n2 and ciA(i) = B(i), where ci ∈ R and |ci| ≤ 1, for i = 1, . . . , n1 (recall that Z(i) denotes the i-th row of Z). Then\n‖B‖2 ≤ ‖A‖2.\nProof: For simplicity, denote ai , (A(i))T , bi , (B(i))T . Then\n‖B‖22 = max‖v‖=1 ‖Bv‖ 2 = max ‖v‖=1 ∑ i (bTi v) 2\n= max ‖v‖=1 ∑ i c2i (a T i v) 2 ≤ max ‖v‖=1 ∑ i (aTi v) 2 = ‖A‖22.\nCorollary B.1 Suppose B ∈ Rn1×n2 is a submatrix of A ∈ Rm1×m2 , then\n‖B‖2 ≤ ‖A‖2. (91)\nProof: By Proposition B.3, we have ‖(X1, X2)‖2 ≥ ‖(X1, 0)‖2 = ‖X1‖2.\nWithout loss of generality, suppose A = [\nB B1 B2 B3\n] . Applying the above inequality twice, we get\n‖A‖2 ≥ ‖(B, B1)‖2 ≥ ‖B‖2.\nProposition B.4 For any A ∈ Rn1×n2 , B ∈ Rn2×n3 , we have\n‖AB‖F ≤ ‖A‖2‖B‖F , (92a) ‖AB‖2 ≤ ‖A‖2‖B‖2. (92b)\nFurther, if n1 ≥ n2, then\nσmin(A)‖B‖F ≤ ‖AB‖F , (93a) σmin(A)‖B‖2 ≤ ‖AB‖2. (93b)\nProof: Assume the SVD of A is A1DA2, where A1 ∈ Rn1×n1 , A2 ∈ Rn2×n2 are orthonormal matrices and D ∈ Rn1×n2 has nonzero entries Dii, i = 1, . . . ,min{n1, n2}. Note that\nσmin(A) ≤ Dii ≤ ‖A‖2,∀ i.\nLet B′ = A2B and suppose the i-th row of B′ is bi, i = 1, . . . , n2, then\n‖AB‖2F = ‖DA2B‖2F = ‖DB′‖2F = min{n1,n2}∑\ni=1\nD2ii‖bi‖2. (94)\nThe the RHS (right hand side) can be bounded from above as\nmin{n1,n2}∑ i=1 D2ii‖bi‖2 ≤ ‖A‖22 min{n1,n2}∑ i=1 ‖bi‖2\n≤ ‖A‖22 n2∑ i=1 b2i = ‖A‖22‖B′‖2F = ‖A‖22‖B‖2F .\nCombining the above relation and (94) leads to (92a).\nIf n1 ≥ n2, then min{n1, n2} = n2, and the RHS of (94) can be bounded from below as min{n1,n2}∑\ni=1 D2ii‖bi‖2 = n2∑ i=1 D2ii‖bi‖2 ≥ σmin(A)2 n2∑ i=1 ‖bi‖2\n= σmin(A)2‖B′‖2F = σmin(A)2‖B‖2F .\nCombining the above relation and (94) leads to (93a).\nNext we prove the inequalities related to the spectral norm. We have\n‖AB‖2 = ‖DA2B‖2 = ‖DB′‖2 = max ‖v‖≤1,v∈Rn1×1 ‖vT DB′‖. (95)\nNote that {vT D | ‖v‖ ≤ 1, v ∈ Rn1×1} ⊆ {uT | u ∈ Rn2×1, ‖u‖ ≤ ‖A‖2}, thus the RHS of (95) can be bounded from above as\nmax ‖v‖≤1,v∈Rn1×1 ‖vT DB′‖ ≤ max u∈Rn2×1,‖u‖≤‖A‖2 ‖uT B′‖\n= ‖A‖2‖B′‖2 = ‖A‖2‖B‖2.\nCombining the above relation and (95) leads to (92b).\nIf n1 ≥ n2, then {uT | u ∈ Rn2×1, ‖u‖ ≤ σmin(A)} ⊆ {vT D | ‖v‖ ≤ 1, v ∈ Rn1×1} (in fact, for any ‖u‖ ≤ σmin(A), let vi = ui/Dii, i = 1, . . . , n2 and vi = 0, n2 < i ≤ n1, where vi denotes the i-th entry of v, then vT D = uT and ‖v‖ ≤ 1). Thus the RHS of (95) can be bounded from below as\nmax ‖v‖≤1,v∈Rn1×1 ‖vT DB′‖ ≥ max u∈Rn2×1,‖u‖≤σmin(A) ‖uT B′‖\n= σmin(A)‖B′‖2 = σmin(A)‖B‖2.\nCombining the above relation and (95) leads to (93b)."
    }, {
      "heading" : "B.2 Proof of Proposition 4.1",
      "text" : "Let M, X,Y satisfy the condition (47). First, we specify the choice of U,V . Suppose the SVD of M is M = ÛΣV̂ = Q1Σ̃QT2 , where Q1 ∈ Rm×m,Q2 ∈ Rn×n are unitary matrices, and Σ̃ = ( Σ 0 0 0 ) . Suppose Q1 = (Q11,Q12), Q2 = (Q21,Q22), where Q11 = Û ∈ Rm×r,Q21 = V̂ ∈ Rn×r are incoherent matrices, and Q12 ∈ Rm×(m−r),Q22 ∈ Rn×(n−r). Let us write X,Y as\nX = Q1 ( X′1 X′2 ) , Y = Q2 ( Y ′1 Y ′2 ) , (96)\nwhere X′1,Y ′ 1 ∈ Rr×r, X′2 ∈ R(m−r)×r,Y ′2 ∈ R(n−r)×r. Define\nU , Q1 ( U′1 0 ) , V , Q2 ( V ′1 0 ) , (97)\nwhere U′1 = (1 − η̄)X′1, V ′1 =\n1 1 − η̄Σ(X ′ 1) −T ,\nin which η̄ ,\nd Σmin ≤ 1 11 .\nThe definition of V ′1 is valid since X ′ 1 is invertible (otherwise, rank(X ′ 1(Y ′ 1) T ) ≤ rank(X′1) ≤ r − 1, thus d ≥ ‖Σ − X′1(Y ′ 1) T ‖F (89) ≥ Σmin − σmin(X′1(Y ′1)T ) = Σmin, which contradicts (47a).By this definition, we have\nU′1(V ′ 1) T = (1 − η̄)X′1(V ′1)T = Σ. (98)\nNow, we prove that U,V defined in (97) satisfy the requirement (48). The requirement (48a) UVT = M follows from (98) and (97). The requirement (48b) ‖U‖F ≤ (1 − dΣmin )‖X‖F can be proved as follows:\n‖U‖F = ‖U′1‖F = (1 − d\nΣmin )‖X′1‖F ≤ (1 − d Σmin )‖X‖F .\nAs a side remark, the following variant of the requirement (48b) also holds:\n‖U‖2 ≤ (1 − d\nΣmin )‖X‖2. (99)\nIn fact, ‖U‖2 = ‖U′1‖2 = (1 − d Σmin )‖X′1‖2\n(91) ≤ (1 − d\nΣmin ) ∥∥∥∥∥∥ ( X′1 X′2 )∥∥∥∥∥∥ 2 = (1 − d Σmin )‖X‖2.\nTo prove the requirement (48c), we first provide the bounds on ‖X′2‖F , ‖V ′1 − Y ′1‖F , ‖Y ′2‖F . Note that\nd2 =‖M − XYT ‖2F\n= ∥∥∥∥∥∥ ( Σ 0 0 0 ) − QT1 XYT Q2 ∥∥∥∥∥∥ 2\nF\n(96) = ∥∥∥∥∥∥ ( Σ 0 0 0 ) − ( X′1(Y ′ 1) T X′1(Y ′ 2) T\nX′2(Y ′ 1) T X′2(Y ′ 2) T\n)∥∥∥∥∥∥ 2\nF\n=‖Σ − X′1(Y ′1)T ‖2F + ‖X′1(Y ′2)T ‖2F + ‖X′2(Y ′1)T ‖2F + ‖X′2(Y ′2)T ‖2F . (98) = ‖X′1((1 − η̄)V ′1 − Y ′1)T ‖2F + ‖X′1(Y ′2)T ‖2F + ‖X′2(Y ′1)T ‖2F\n+ ‖X′2(Y ′2)T ‖2F .\n(100)\nIntuitively, since ‖X′1‖F , ‖Y ′1‖F are O(1), we can upper bound ‖(1 − η̄)V ′1 − Y ′1‖F , ‖Y ′2‖F , ‖X′2‖F as O(d). More rigorously, it follows from (100) that d ≥ ‖X′1((1 − η̄)V ′1 − Y ′1)T ‖F (93a) ≥ σmin(X′1)‖(1 − η̄)V ′1 − Y ′1‖F and, similarly, d ≥ σmin(X′1)‖(Y ′2)T ‖F , d ≥ σmin(Y ′1)‖(X′2)T ‖F . These three inequalities imply\n‖(1 − η̄)V ′1 − Y ′1‖F ≤ d\nσmin(X′1) ,\n‖Y ′2‖F ≤ d\nσmin(X′1) , ‖X′2‖F ≤ d σmin(Y ′1) .\n(101)\nWe can lower bound σmin(X′1) and σmin(Y ′ 1) as\nσmin(X′1) ≥ 10Σmin 11βT , σmin(Y ′1) ≥ 10Σmin 11βT . (102)\nTo prove (102), notice that (100) implies that d ≥ ‖Σ− X′1(Y ′1)T ‖F ≥ ‖Σ− X′1(Y ′1)T ‖2 (89) ≥ Σmin −σmin(X′1(Y ′1)T ), which further implies\nσmin(X′1(Y ′ 1) T ) ≥ Σmin − d ≥ 10 11 Σmin.\nAccording to Proposition B.2, we have σmin(X′1(Y ′ 1) T ) ≤ σmin(X′1)‖Y ′1‖2. Combining this inequality with the above relation, we get σmin(X′1)‖Y ′1‖2 ≥ σmin(X′1(Y ′1)T ) ≥ 5Σmin/6, which further implies\nσmin(X′1) ≥ 10Σmin 11‖Y ′1‖2 . (103)\nSimilarly, we have\nσmin(Y ′1) ≥ 10Σmin\n11‖X′1‖2 . (104)\nPlugging ‖Y ′1‖2 ≤ ‖Y ′1‖F ≤ ‖Y‖F ≤ βT and similarly ‖X′1‖2 ≤ βT into (103) and (104), we obtain (102).\nCombining (102) and (101), we obtain\nmax{‖(1 − η̄)V ′1 − Y ′1‖F , ‖X′2‖F , ‖Y ′2‖F} ≤ 11 10 d Σmin βT ≤ 1 10 βT . (105)\nWe can bound the norm of V ′1 as\n‖V ′1‖F = 1\n1 − η̄‖(1 − η̄)V ′ 1‖F ≤ 1 1 − η̄ (‖(1 − η̄)V ′ 1 − Y ′1‖F + ‖Y ′1‖F)\n(105) ≤ 11\n10 ( 1 10 βT + βT ) ≤ ( 11 10 )2 βT .\n(106)\nCombining this relation with (105), we have\n‖V ′1 − Y ′1‖F ≤ ‖(1 − η̄)V ′1 − Y ′1‖F + η̄‖V ′1‖F\n≤ 11 10 d Σmin βT + η̄ ( 11 10 )2 βT ≤ 7βT 3Σmin d.\nFrom (105) and the above relation we obtain\n‖U − X‖F =‖X′2‖F ≤ 11βT 10Σmin d ≤ 6βT 5Σmin d,\n‖V − Y‖F = √ ‖V ′1 − Y ′1‖2F + ‖Y ′2‖2F\n≤ √( 7 3 )2 + ( 11 10 )2 βT Σmin d ≤ 3βT Σmin d,\nwhich finishes the proof of the requirement (48c).\nAs a side remark, the requirement (48c) can be slightly improved to\n‖U − X‖F ≤ 6‖Y‖2 5Σmin d, ‖V − Y‖F ≤ 3‖X‖2 Σmin d. (107)\nIn fact, plugging ‖X′1‖2 (91) ≤ ‖ ( X′1 X′2 ) ‖2 = ‖X‖2 and similarly ‖Y ′1‖2 ≤ ‖Y‖2 into (103) and (104), we obtain σmin(X′1) ≥ 5Σmin 6‖Y‖2 , σmin(Y ′ 1) ≥ 5Σmin 6‖X‖2 . Combining with (101), we obtain (107). This inequality will be used in the proof of Claim 5.2 in Appendix D.1.\nAt last, we prove the requirement (48d). By the definitions of U,V in (97), we have U = (Q11,Q12) (\nU′1 0 ) = Q11U′1,\nV = (Q21,Q22) (\nV ′1 0 ) = Q21V ′1.\n(108)\nThe assumption that M is µ-incoherent implies\n‖Q(i)11‖ 2 = ‖Û(i)‖2 ≤ rµ\nm , ‖Q(i)21‖ 2 = ‖V̂ ( j)‖2 ≤ rµ n , ∀ i, j.\nNotice the following fact: for any matrix A ∈ RK×r, B ∈ Rr×r, where K ∈ {m, n}, we have\n‖(AB)(i)‖2 = ‖A(i)B‖2 ≤ ‖A(i)‖2‖B‖2F .\nTherefore, we have (using the fact ‖U′1‖F ≤ ‖X′1‖F ≤ ‖X‖F ≤ βT and (106))\n‖U(i)‖2 = ‖(Q11U′1)(i)‖2 ≤ ‖Q (i) 11‖ 2‖U′1‖2F ≤ rµ m β2T ;\n‖V ( j)‖2 = ‖(Q21V ′1)( j)‖2 ≤ rµ n ‖V ′1‖2F (106) ≤ ( 11 10 )4 rµ n β2T ≤ 3 2 rµ n β2T ,\n(109)\nwhich finishes the proof the requirement (48d)."
    }, {
      "heading" : "C Proof of Proposition 4.2",
      "text" : "We will first reduce Proposition 4.2 to Proposition C.1 for r × r matrices in Section C.1. This reduction is rather trivial, and the major difficulty lies in Proposition C.1. For general r, the proof of Proposition C.1 is rather involved. We will give the overview of the main proof ideas in Section C.2. Most readers can skip Section C.1."
    }, {
      "heading" : "C.1 Transformation to a simpler problem",
      "text" : "We first transform the problem to a simpler problem that only involves r × r matrices. In particular, we will show that to prove Proposition 4.2 we only need to prove Proposition C.1.\nSimilar to the proof of Proposition 4.1, we use Q1 ∈ Rm×m,Q2 ∈ Rn×n to denote the SVD factors of M (Q1 and Q2 are unitary matrices), and write X,Y as\nX = Q1 ( X′1 X′2 ) , Y = Q2 ( Y ′1 Y ′2 ) .\nDefine\nU = Q1 ( U′1 0 ) , V = Q2 ( V ′1 0 ) , (110)\nwhere U′1 ∈ Rr×r and V ′1 ∈ Rr×r are to be determined.\nWe can convert the conditions on U,V to the conditions on U′1,V ′ 1. As proved in Appendix B (combining (101)\nand (102)),\n‖X′2‖F ≤ 6βT\n5Σmin d, ‖Y ′2‖F ≤ 6βT 5Σmin d. (111)\nObviously, the condition (49a) implies the following condition on X′1,Y ′ 1:\nd′ , ‖Σ − (X′1)(Y ′1)T ‖ ≤ Σmin\nCdr . (112)\nUsing (111) and the facts ‖X‖F = √ ‖X′1‖2F + ‖X′2‖2F and ‖Y‖F = √ ‖Y ′1‖2F + ‖Y ′2‖2F , the condition (49b) implies the following condition on X′1,Y ′ 1: √\n3 5 βT ≤ ‖X′1‖F ≤ βT , √ 3 5 βT ≤ ‖Y ′1‖F ≤ βT . (113)\nWe have the following proposition.\nProposition C.1 There exist numerical constants Cd,CT such that: if X′1,Y ′ 1 ∈ Rr×r satisfy (112) and (113), where\nβT = √ CT rΣmax, then there exist U′1 ∈ Rr×r,V ′1 ∈ Rr×r such that\nU′1(V ′ 1) T = Σ, (114a)\n‖U′1‖F ≤ ‖X′1‖F , ‖V ′1‖F ≤ (1 − d\nΣmin )‖Y ′1‖F , (114b)\n‖U′1 − X′1‖F‖V ′1 − Y ′1‖F ≤ 63 √ r β2T\nΣ2min d2,\nmax{‖U′1 − X′1‖F , ‖V ′1 − Y ′1‖F} ≤ 58 7 √ r βT Σmin d. (114c)\nWe claim that Proposition C.1 implies Proposition 4.2. Since we have already proved that the conditions of Proposition 4.2 imply the conditions of Proposition C.1, we only need to prove that the conclusion of Proposition C.1 implies the conclusion of Proposition 4.2. In other words, we only need to show that if U′1,V ′ 1 satisfy (114), then they satisfy the requirements (50).\nThe requirement (50a) UVT = M follows directly from (114a) and the definition of U,V in (110). The requirement (50b) can be proved as ‖V‖F = ‖V ′1‖F ≤ (1 − d Σmin )‖Y ′1‖F ≤ (1 − d Σmin )‖Y‖F and ‖U‖F = ‖U′1‖F ≤ ‖X‖F .\nAnalogous to (109), the requirement (50d) can be proved as ‖V ( j)‖2 = ‖(Q21V ′1)( j)‖2 ≤ rµ n ‖V ′1‖2F ≤ rµ n β 2 T and, similarly, ‖U(i)‖2 ≤ rµm β2T . At last, we prove the requirement (50c). The first relation in (50c) can be proved as\n‖U − X‖F‖V − Y‖F\n= √ ‖U′1 − X′1‖2F + ‖X′2‖2F √ ‖V ′1 − Y ′1‖2F + ‖Y ′2‖2F = (‖ U′1 − X′1‖2F‖V ′1 − Y ′1‖2F + ‖X′2‖2F‖V ′1 − Y ′1‖2F + ‖U′1 − X′1‖2F‖Y ′2‖2F + ‖X′2‖2F‖Y ′2 ‖2F\n)1/2 (111),(114c) ≤ √ r β2T\nΣ2min d2\n√ 632 + (\n6 5 )2( 58 7 )2 + ( 58 7 )2( 6 5 )2 + ( 6 5 )4,\n< 65 √ r β2T\nΣ2min d2,\nwhere in the second last inequality we also use the fact d′ ≤ d. The second relation in (50c) can be proved by\n‖U − X‖F = √ ‖U′1 − X′1‖2F + ‖X′2‖2F\n(111),(114c) ≤ √ ( 6 5 )2 + ( 58 7 )2 √ r βT Σmin d ≤ 17 2 √ r βT Σmin d\nand a similar inequality for ‖V − Y‖F .\nC.2 Preliminary analysis for the proof of Proposition C.1\nWe first give a more intuitive explanation of what we want to prove, by relating the result to “preconditioning”. Then we analyze two simple examples for r = 2 to get some ideas on how to approach the problem. Next we discuss how to extend the ideas to general r. To simplify the notations, from now on, we use X,Y,U,V, d to replace X′1,Y ′ 1,U ′ 1,V ′ 1, d ′ in Proposition (C.1)."
    }, {
      "heading" : "C.2.1 Perturbation Analysis for Preconditioning.",
      "text" : "We claim that Proposition C.1 is closely related to “preconditioning”, which refers to reducing the condition number (by preprocessing) in numerical linear algebra.\nProposition C.2 (Informal) Suppose X ∈ Rr×r is non-singular and ‖X‖F = ‖X−1‖F ≥ C √\nr where C ≥ 10 is a constant. For any d′ ≤ O(1/r1.5), there exists U ∈ Rr×r such that ‖U‖F = ‖X‖F , ‖U−1‖F ≤ (1 − d′)‖X−1‖F and max{‖U − X‖F , ‖U−1 − X−1‖F} ≤ O(d′r1.5).\nWe will argue later that Proposition C.2 is a simple version of Proposition C.1.\nWe explain why this proposition can be understood as perturbation analysis for perconditioning. Assume X has singular values σ1 ≥ · · · ≥ σr > 0, then ‖X‖2F = ∑ i σ 2 i and ‖X−1‖2F = ∑ i\n1 σ2i . By Cauchy-Schwartz inequality\n‖X‖2F‖X−1‖2F ≥ r2, and the equality holds iff σ1 = · · · = σr, i.e., X has a condition number 1. In other words, if ‖X‖F = ‖X−1‖F = √ r, then X has the minimal condition number 1. In the assumption ‖X‖F = ‖X−1‖F ≥ C √ r, C can be viewed as a measure of the ill-conditioned-ness of X (different from the condition number σ1/σr but related). Prop. C.2 simply says that we can perturb X to make X better-conditioned.\nProp. C.2 itself is not difficult to prove. In fact, without loss of generality we can assume X is a diagonal matrix (by left and right multiplying X by its singular vector matrices). Then the problem reduces to the following problem:\nassume ∑\ni σ 2 i = ∑ i\n1 σ2i ≥ C2r, perturb σi’s so that the\n∑ i σ 2 i does not change while ∑ i\n1 σ2i increases. This is a rather easy problem. Nevertheless, for the original desired result Prop. C.1 we cannot assume X is diagonal. In Section C.2.2 we will analyze the problem without assuming X is diagonal.\nTo show the connection of Prop. C.2 and Prop. C.1, we first simplify the statement of Prop. C.1.\nProposition C.3 (Simpler version of Proposition C.1) Suppose X,Y,Σ ∈ Rr×r are non-singular and Σ is diagonal. If ‖XYT − Σ‖F = d ≤ O(Σmin/r) and ‖X‖F = ‖Y‖F = β ≥ C √ rΣmax, then we can find a factorization Σ = UVT such that max{‖U − X‖F , ‖V − Y‖F} ≤ O( √ rdβ/Σmin) and ‖U‖F ≤ ‖X‖F , ‖V‖F ≤ ‖Y‖F .\nThere are a few differences with Prop. C.1: i) In Prop. C.1 we assume ‖X‖F , ‖Y‖F ∈ [ √\n0.6βT , βT ], but by simply scaling X,U,Y,V we can assume ‖X‖F = ‖Y‖F as in the above proposition; ii) here we only require ‖V‖F ≤ ‖Y‖F , instead of ‖V‖F ≤ (1 − d/Σmin)‖Y‖F in (114b); iii) in Prop. C.1 there is an extra bound of ‖U − X‖F‖V − Y‖F . Nevertheless, these differences are not essential and do not affect the proof too much.\nNow let us consider a special case and show how to reduce Prop. C.3 to Prop. C.2. This part is mainly for the purpose of rigorous derivation and we suggest first-time readers jump to Section C.2.2. The special case we consider is Σ = I and XYT = (1 − d/ √ r)I, where d ≤ O(1/r). Let d′ = d/ √ r ≤ O(1/r1.5), then YT = (1 − d/ √ r)X−1 = (1 − d′)X−1. The condition of Prop. C.3 becomes\n‖X‖F = ‖X−1‖F(1 − d′) = β. (115)\nOne requirement of Prop. C.3 becomes ‖U‖F ≤ ‖X‖F , ‖U−1‖F ≤ ‖Y‖F = ‖X−1‖F(1−d′). The distance bound in Prop. C.3 is O( √ rdβ/Σmin), which becomes O(d′r1.5) under the new parameter setting. By a similar scaling technique, i.e. scaling X,U by 1/ √ 1 − d′ and Y,V by √ 1 − d′, we can replace the condition (115) by\n‖X‖F = ‖Y‖F = β √\n1 1 − d′ ≥ C √ r.\nNote that rigorously speaking the bound should be C √ r/ √\n1 − d′, but since 1/(1 − d′) ≤ 1/(1 − 1/r1.5) ∈ [1/(1 − 1/21.5), 1], the contribution of 1/ √ 1 − d′ is just a numerical constant which can be absorbed into C. Now the problem becomes: assume ‖X‖F = ‖X−1‖F ≥ C √\nr, find U such that ‖U‖F ≤ ‖X‖F , ‖U−1‖F ≤ ‖X−1‖F(1 − d′) and max{‖U − X‖F , ‖U−1 − X−1‖F} ≤ O(d′r1.5), where d′ ≤ O(1/r1.5). By slightly strengthening the requirement ‖U‖F ≤ ‖X‖F to ‖U‖F = ‖X‖F , we obtain Prop. C.2."
    }, {
      "heading" : "C.2.2 Two Motivating Examples",
      "text" : "We denote the i-th row of X,Y as xi, yi, respectively. In the first example (see Figure 3), we set r = 2, Σ = I (which implies Σmin = Σmax = 1), d = 1/(Cdr) and\nX = Diag (x11, x22) = Diag C, 1 − d/√2C  , Y = Diag (y11, y22) = Diag 1 − d/√2C ,C  , (116)\nwhere C > 1 is to be determined, and Diag(w1,w2) denotes a 2 × 2 diagonal matrix with diagonal entries w1,w2. In this setting βT = √ rCT Σmax = √ 2CT is a large constant. Condition (112) holds since ‖XYT − Σ‖F = ‖(1− d/ √ 2)I −\nI‖F = d = 1/(Cdr). Note that ‖X‖F = ‖Y‖F = √ C2 + (1−d/ √ 2)2 C2 ≈ C, thus there exists C ∈ [ √ 3/5βT , βT ] so that (113) holds.\nHow should we define U = Diag(u11, u22),V = Diag(v11, v22) so that (114) holds? Due to the “symmetry” of X and Y in this example (by symmetry we mean x11 = y22, x22 = y11), we choose U,V such that u11 = v22, u22 = v11. Then the requirements (114a) and (114b) reduce to:\nu11u22 = 1 = x11x22\n1 − d/ √ 2 ,\nu211 + u 2 22 ≤ x211 + x222.\n(117)\nIt can be easily shown that there exist u11, u22 satisfying (117). In fact, define R = ‖X‖F = √ x211 + x 2 22 and let a point (w1,w2) move along the circle {(w1,w2) | w21 + w22 = R2} from (x11, x22) to (R/ √ 2,R/ √\n2). During this process, the norm of (w1,w2) does not change and the product w1w2 monotonically increases from x11x22 to R2/2. Therefore, there exist u11, u22 satisfying (117) as long as R2/2 > x11x22/(1 − d/ √ 2). This inequality is equivalent to (1−d/ √\n2)(x211 + x 2 22)/2 > x11x22, which can be simplified to (1−d/ √ 2)(x11− x22)2 > √ 2dx11x22 = √ 2d(1−d/ √ 2),\nor equivalently, (x11 − x22)2 > √ 2d. The last inequality holds when x11 − x22 = C − (1 − d/ √\n2)/C is large enough (i.e. C is large enough).\nTo summarize, we will increase the small entry x22 (resp. y11) and decrease the large entry x11 (resp. y22) to obtain a more balanced diagonal matrix U (resp. V), which has the same norm as X (resp. Y). The percentage of increase in the small entry x22 (resp. y11) will be much larger than the percentage of decrease in the large entry x11 (resp. y22), thus the products x22y22 and x11y11 will increase; in other words, the product UVT of the more balanced matrices U,V will have larger entries than XYT .\nNote that the above idea of shrinking/extending works when there is a large imbalance in the lengths of the rows of X,Y , regardless of whether X,Y are diagonal matrices or not. By the assumption that ‖X‖F and ‖Y‖F are large, we know that there must be a row of X (resp. Y) that has large norm (here “large” means much larger than 1/ √ r); however, it is possible that all rows of X and Y have large norm and there is no imbalance in terms of the lengths of the rows. See below for such an example.\nIn the second example (see Figure 4), we still set r = 2, Σ = I, d = 1/(Cdr). Suppose X = (xT1 , x T 2 ), Y = (y T 1 , y T 2 ). We define x1 = (C, 0), x2 = (−C sinα,C cosα) and y1 = (C cosα,C sinα), y2 = (0,C), where C is a large constant, and α ∈ (0, π/2) is chosen so that\nC2 cosα = 1 − d/ √ 2. (118)\nWhen C is large, α ≈ arccos(1/C2) is also large (i.e. close to π/2). Condition (112) holds since ‖XYT − Σ‖F = ‖C2 cosαI − I‖F = ‖(1 − d/ √ 2)I − I‖F = d = 1/(Cdr). Note that ‖X‖F = ‖Y‖F = √ 2C, so we can choose C = βT / √ 2 = √ 2CT / √ 2 = √ CT so that (113) holds.\nHow should we choose U = (uT1 , u T 2 ), V = (v T 1 , v T 2 ) so that (114) holds? The idea for the first example no longer works since it requires that the difference of ‖x1‖ and ‖x2‖ (resp. ‖y1‖ and ‖y2‖) is large; however, in this example,\n‖x1‖ − ‖x2‖ = ‖y1‖ − ‖y2‖ = 0. The key idea for this example is to use rotation. Rotating a vector does not change the norm, so requirement (113) will not be violated if ui (resp. vi) is obtained by rotating xi(resp. yi). For simplicity, we rotate y1, x2 to obtain v1, u2 respectively and let u1 = x1, v2 = y2 (see Figure 4). Note that y1 and x2 should be rotated by the same angle as v1 should be orthogonal to u2 (since the off-diagonal entries of UVT are zero). To increase the inner product 〈xi, yi〉 from 1 − d/ √ 2 to 1, we need to decrease the angle of xi and yi, thus y1 (resp. x2) should be rotated towards x1(resp. y2). Finally, let us specify the angle of rotation θ , ∠(y1, v1) = ∠(x2, u2). The requirement 〈u1, v1〉 = 1 is equivalent to 1 = ‖u1‖‖v1‖ cos ∠(u1, v1) = ‖x1‖‖y1‖ cos(α − θ), which can be rewritten as\n1 = C2 cos(α − θ). (119)\nThe right-hand side of (119) is an increasing function of θ, ranging from C2 cos(α) (118) = 1−d/ √ 2 to C2 for θ ∈ [0, α]. Since 1 lies in the range [1 − d/ √\n2,C2], there exists a unique θ so that (119) holds. One can further verify the requirement (114c), i.e. the difference of X (resp. Y) and U (resp. V) is small. As a rough summary, we rotate xi, yi to obtain ui, vi when the angle of xi and yi is large. This operation does not change the norm and can increase the inner product 〈xi, yi〉 to the desired amount (1 in this case)."
    }, {
      "heading" : "C.2.3 Proof Ideas of Proposition C.1",
      "text" : "In the above two examples, we have used two different operations: one is based on shrinking/extending, and the other is based on rotation. As we mentioned before, the first operation cannot deal with the second example; also, it is obvious that the second operation cannot deal with the first example (the angle between xi and yi is zero, so rotation only decreases the inner product). Therefore, both operations are necessary.\nAre these two operations sufficient? Fortunately, the answer is yes for the case that XYT is diagonal and 〈xi, yi〉 ≤ Σi (we need extra effort to reduce the general problem to this case). When all the angles between xi and yi are smaller than a constant ᾱ, there must be some kind of imbalance in the lengths of xi, yi’s (to illustrate this, if all ‖xi‖ = ‖yi‖, then ‖xi‖2 = ‖xi‖‖yi‖ ≈ Σi/ cos ∠(xi, yi) ≤ Σi/ cos(ᾱ), which implies ‖X‖2F . rΣmax/ cos(ᾱ) 35CT rΣmax = 3 5β 2 T for large enough CT , a contradiction to (112)). Thus we can use the first operation (i.e. shrinking/extending the vectors xi, yi’s) to obtain the desired U,V . When all the angles between xi and yi are larger than a constant ᾱ, we can use the second operation (i.e. rotating the vectors xi, yi’s) to obtain the desired U,V . In general, some angles may be larger than ᾱ and others may be smaller, then a natural solution is to use the two operations simultaneously: use the first operation for the pairs (xi, yi) with small angles and the second operation for those with large angles.\nWe had a proof using the two operations simultaneously, but the bounds on ‖U − X‖F , ‖V − Y‖F have a large exponent of r. In the following subsection, we present a different proof that does not use the two operations simultaneously, but only use one of the two operations. The basic proof framework is summarized as follows. We first\ndefine Ŷ so that XŶ = Σ; in other words, we try to satisfy the requirement (114a) first. Then we try to modify Ŷ to satisfy the requirement (114b). In particular, we need to reduce the norm of Ŷ and keep the norm of X unchanged, while maintaining the relation XŶT = Σ. We consider two cases: in Case 1, “most” angles between X and Ŷ are smaller than ᾱ, and using the first operation (shrinking/extending) can obtain the desired U,V; in Case 2, “most” angles between X and Ŷ are larger than ᾱ, and using the second operation (rotation) can obtain the desired U,V (see (127) for a precise definition of Case 1 and Case 2). The difference of this proof framework and the previous one is the following. In our previous proof framework, we need to take into account every pair xi, yi so that its inner product is modified to Σi, thus two operations have to be applied simultaneously. In contrast, in this new proof framework, 〈xi, ŷi〉 is already Σi, and we only need to worry about the “overall” requirement that ‖Ŷ‖F should be reduced, thus dealing only with the pairs with small angles (or only with the pairs with large angles) is enough to satisfy the requirement.\nFinally, we would like to mention that when Σ is an identity matrix, the proof can be rather simple. In fact, in this case one can assume X to be diagonal by proper orthonormal transformation, and then assume Y to be diagonal since the off-diagonal entries are small. By just using the first operation (scaling of the diagonal entries), we can construct the desired U,V and the proof is similar to that in Appendix C.3.1. When Σ is not a diagonal matrix, we can replace X,Y by XQ,Q−1Y where Q is orthonormal, but that only simplifies X to a upper triangular matrix, a condition seems not very helpful. It seems that the second operation has to be used and the proof becomes more involved."
    }, {
      "heading" : "C.3 Proof of Proposition C.1",
      "text" : "As mentioned earlier, to simplify the notations, we use X,Y,U,V, d to replace X′1,Y ′ 1,U ′ 1,V ′ 1, d ′ in Proposition (C.1). Throughout the proof, we choose CT = 20, (120)\nand Cd = 108, which implies d\nΣmin ≤ 1 108r . (121)\nThere are two “hard” requirements on U,V: (114a) and (114b). Our construction of U,V can be viewed as a two-step approach, whereby we satisfy one requirement in each step. In Step 1, we construct\nŶ = Σ(Σ + D)−T Y, where D , XYT − Σ,\nthen XŶT = (XYT )(XYT )−1Σ = Σ,\ni.e. the first requirement is satisfied. Since the new Ŷ may have higher norm than ‖Y‖F , in Step 2 we modify X, Ŷ to U,V so that the product does not change, and ‖V‖F ≤ ‖Y‖F , ‖U‖F ≤ ‖X‖F .\nClaim C.1 Let Ŷ = Σ(Σ + D)−T Y, then\nη , 1 − ‖Y‖F ‖Ŷ‖F ≤ d Σmin , (122a)\n‖Y − Ŷ‖F ≤ d\nΣmin − d ‖Y‖F . (122b)\nProof of Claim C.1: By the definition of Ŷ we have Y = (Σ + D)T Σ−1Ŷ , then we have\n‖Y − Ŷ‖F = ‖(Σ + D)T Σ−1Ŷ − Ŷ‖F = ‖DT Σ−1Ŷ‖F\n≤ ‖DT Σ−1‖F‖Ŷ‖F ≤ ‖DT ‖FΣ−1min‖Ŷ‖F = d\nΣmin ‖Ŷ‖F .\n(123)\nUsing the triangular inequality and (123), we have\n‖Ŷ‖F ≤ ‖Y − Ŷ‖F + ‖Y‖F ≤ d\nΣmin ‖Ŷ‖F + ‖Y‖F ,\n=⇒ ‖Y‖F ≥ (1 − d\nΣmin )‖Ŷ‖F . (124)\nThe first desired inequality (122a) follows immediately from (124), and the second desired inequality (122b) is proved by combining (124) and (123).\nCombining (122a) and (121), we obtain\nη ≤ 1 108r . (125)\nIf η ≤ 0, i.e. ‖Ŷ‖F ≤ ‖Y‖F , then U = X,V = Ŷ already satisfy (114). From now on, we assume η > 0, i.e. ‖Ŷ‖F > ‖Y‖F . Denote xTi , ŷTi , uTi , vTi as the i-th row of X, Ŷ ,U,V , respectively. Denote αi , ∠(xi, ŷi), i.e. the angle between the two vectors xi and ŷi. Since 〈xi, ŷi〉 = Σi > 0, we have αi ∈ [0, π2 ). Without loss of generality, assume\nα1, . . . , αs > 3 8 π, αs+1, . . . , αr ≤ 3 8 π, (126)\nwhere s ∈ {0, 1, . . . , r}. We consider three cases and construct U,V that satisfy the desired properties in the subsequent three subsections.\nCase 1 : r∑\ni=s+1\n‖ŷi‖2 ≥ 2 3 ‖Ŷ‖2F , r∑ i=s+1 ‖xi‖2 ≥ 2 3 ‖X‖2F . (127a)\nCase 2a : s∑\ni=1\n‖ŷi‖2 > 1 3 ‖Ŷ‖2F . (127b)\nCase 2b : s∑\ni=1\n‖xi‖2 > 1 3 ‖X‖2F . (127c)"
    }, {
      "heading" : "C.3.1 Proof of Case 1",
      "text" : "Without loss of generality, assume ‖xs+1‖ ≤ ‖xs+2‖ ≤ · · · ≤ ‖xr‖. (128)\nLet K be the smallest integer in {s + 1, s + 2, . . . , r} so that\nK∑ i=s+1 ‖ŷi‖2 ≥ 2 r∑ j=K+1 ‖ŷ j‖2. (129)\nBy this definition of K, we have K−1∑\ni=s+1\n‖ŷi‖2 < 2 r∑\nj=K\n‖ŷ j‖2. (130)\nWe will shrink and extend xi, ŷi to obtain U,V . The precise definition of U = (u1, u2, . . . , ur)T ,V = (v1, . . . , vr)T\nis given in Table 8.\nWe will show that such U,V satisfy the requirements (114). The requirement (114a) follows directly from the definition of U,V and the fact XŶT = Σ.\nTable 8: Operation 1\nOperation 1: Shrinking and Extending Input: xk, ŷk, k = 1, . . . , r. Output: uk, vk, k = 1, . . . , r. Procedure:\n(i) For each j ≤ s, keep x j, ŷ j unchanged, i.e.\nu j , x j, v j , ŷ j, j = 1, . . . , s. (131)\n(ii) For each i ∈ {s + 1, . . . ,K}, extend xi to obtain ui and shrink ŷi to obtain vi. For each i ≥ K + 1, shrink xi to obtain ui and extend ŷi to obtain vi. More specifically,\nui , xi\n1 − i , vi , ŷi(1 − i), where i = 7η̄ i ≤ K,−4.5η̄ i ≥ K + 1, i = s + 1, s + 2, . . . , r, (132) in which\nη̄ , d\nΣmin ≥ η. (133)\nWe then prove the requirement (114c). We can bound ‖U − X‖F as\n‖U − X‖F = √∑\ni>s\n‖ 1 1 − i xi − xi‖2 = √∑ i>s ( i 1 − i )2 ‖xi‖2\n≤ 7η̄ 1 − 7η̄ √∑ i>s ‖xi‖2 ≤ 7η̄ 1 − 7η̄‖X‖F ≤ 15 2 η̄βT .\n(134)\nThe bound of ‖V − Ŷ‖F is given as\n‖V − Ŷ‖F = √∑\ni>s\n‖(1 − i)ŷi − ŷi‖2 ≤ √∑\ni>s\n2i ‖ŷi‖2 ≤ 7η̄‖Ŷ‖F .\nCombining with the bound (123), we can bound ‖V − Y‖F as\n‖V − Y‖F ≤ ‖V − Ŷ‖F + ‖Ŷ − Y‖F ≤ 7η̄‖Ŷ‖F + d\nΣmin ‖Ŷ‖F\n= 8η̄‖Ŷ‖F (122a) ≤ 8η̄ 1 − η̄‖Y‖F ≤ 58 7 η̄βT .\n(135)\nThe first part of the requirement (114c) now follows by multiplying (134) and (135), and the second part of the requirement (114c) follows directly from (134) and (135).\nAt last, we prove that U,V satisfy the requirement (114b). Let\nS 1 , K∑\ni=s+1\n‖ŷi‖2, S 2 , r∑\nj=K+1\n‖ŷ j‖2, S 3 , s∑\nk=1\n‖ŷk‖2,\nthen (129) and (127a) imply S 2 ≤ S 1/2, S 3 ≤ (S 1 + S 2)/2 ≤ 3S 1/4. (136)\nSince η̄ = d/Σmin ≥ η, we have (1 − η)2(1 − η̄)2 ≥ (1 − 2η)(1 − 2η̄) ≥ (1 − 2η̄)2. Then\n(1 − η)2(1 − η̄)2‖Ŷ‖2F − ‖V‖2F ≥ (1 − 2η̄)2‖Ŷ‖2F − ‖V‖2F =\n∑ i≥s+1 ((1 − 2η̄)2‖ŷi‖2 − ‖vi‖2) + ∑ k≤s ((1 − 2η̄)2‖ŷk‖2 − ‖vk‖2)\n= ∑\ni≥s+1 ((1 − 2η̄)2‖ŷi‖2 − (1 − i)2‖ŷi‖2)\n+ ∑ k≤s ((1 − 2η̄)2‖ŷk‖2 − ‖ŷk‖2)\n= ∑\ni≥s+1 ( i − 2η̄)(2 − i − 2η̄)‖ŷi‖2 − ∑ k≤s 4η̄(1 − η̄)‖ŷk‖2\n(132) = ∑ s+1≤i≤K 5η̄(2 − 5η̄ − 2η̄)‖ŷi‖2\n+ ∑\nK< j≤r (−6.5η̄)(2 + 4.5η̄ − η̄)‖ŷ j‖2 − ∑ k≤s 4η̄(1 − η̄)‖ŷk‖2\n= 5η̄(2 − 7η̄)S 1 − 6.5η̄(2 + 2.5η̄)S 2 − 4η̄(1 − η̄)S 3 (136) ≥ 5η̄(2 − 7η̄)S 1 − 6.5η̄(2 + 2.5η̄)\n1 2 S 1 − 4η̄(1 − η̄) 3 4 S 1\n≥ (0.5 − 41η̄)η̄S 1 ≥ 0,\n(137)\nwhere the last inequliaty follows from (121). Note that (1 − η)‖Ŷ‖F = ‖Y‖F , thus (137) implies\n‖V‖F ≤ (1 − η)(1 − η̄)‖Ŷ‖F = (1 − d\nΣmin )‖Y‖F ,\nwhich proves the second part of (114b).\nWe then prove the first part of (114b), i.e. ‖U‖F ≤ ‖X‖F . Let\nT1 , K∑\ni=s+1\n‖xi‖2, T2 , r∑\nj=K\n‖x j‖2.\nWe claim that T2 ≥ 2T1. (138)\nWe prove (138) by contradiction. Assume the contrary that T2 < 2T1, then 13 (T2 + T1) < T1, i.e.\n1 3 r∑ k=s+1 ‖xk‖2 < K∑ i=s+1 ‖xi‖2 (128) ≤ (K − s)‖xK‖2. (139)\nPlugging the second inequality of (127a), i.e. ∑r\nk=s+1 ‖xk‖2 ≥ 23 ‖X‖2F , into the above relation, we obtain\n‖X‖2F ≤ 9 2 (K − s)‖xK‖2 ≤ 9 2 K‖xK‖2. (140)\nWhen j ∈ {K,K + 1, . . . , r}, we have\nΣmax ≥ Σ j = 〈x j, ŷ j〉 = ‖x j‖‖ŷ j‖ cos(α j) (128),(126) ≥ ‖xK‖‖ŷ j‖ cos(3π/8).\nwhich implies\n‖ŷ j‖ ≤ ω, where ω , 1 cos(3π/8) Σmax ‖xK‖ , j = K,K + 1, . . . , s. (141)\nTherefore,\n‖Ŷ‖2F (127a) ≤ 3\n2 r∑ j=s+1 ‖ŷ j‖2 (130) ≤ 9 2 r∑ j=K ‖ŷ j‖2 (141) ≤ 9 2 (r − K + 1)ω2. (142)\nCombining (140) and (142), and using K(r − K + 1) ≤ 14 (r + 1)2 ≤ r2, we get\n‖X‖2F‖Ŷ‖2F ≤ 81 4 r2‖xK‖2ω2\n(141) = 81 4 r2‖xK‖2 1 cos(3π/8)2 Σ2max ‖xK‖2 < 140r2Σ2max.\n(143)\nAccording to (113), we have ‖X‖2F‖Ŷ‖2F ≥ ‖X‖2F‖Y‖2F ≥ ( 35 )2β4T = 9 25C 2 T r 2Σ2max; combining with (143), we get 140 > 925C 2 T , which implies C 2 T < 389. This contradicts the definition (120) that CT = 20, thus (138) is proved.\nNow we are ready to prove the first part of (114b) as follows: ‖X‖2F − ‖U‖2F = ∑\ni≥s+1 (‖xi‖2 − ‖ui‖2) + ∑ k≤s (‖xk‖2 − ‖uk‖2)\n= ∑\ni≥s+1 (‖xi‖2 − 1 (1 − i)2 ‖xi‖2) + 0\n= ∑\ni≥s+1\ni( i − 2) (1 − i)2 ‖xi‖2\n= ∑\nK< j≤r\n4.5η̄(4.5η̄ + 2) (1 + 4.5η̄)2\n‖x j‖2 − ∑\ns+1≤i≤K\n7η̄(2 − 7η̄) (1 − 7η̄)2 ‖xi‖ 2\n(138) ≥ T2η̄ [ 4.5(4.5η̄ + 2) (1 + 4.5η̄)2 − 1 2 7(2 − 7η̄) (1 − 7η̄)2 ] ≥ T2η̄ [ 9\n(1 + 4.5η̄)2 − 7 (1 − 7η̄)2\n] ≥ 0,\nwhere the last inequality is because (1−7η̄) 2\n(1+4.5η̄)2 > 0.79 > 7 9 when η̄ ≤ 1/(108r) < 1/100. Thus the first part of (114b) is\nproved."
    }, {
      "heading" : "C.3.2 Proof of Case 2a",
      "text" : "Denote X0 = X,Y0 = Ŷ , x0k = xk, y 0 k = ŷk, α 0 k = αk, k = 1, . . . , r. (144)\nWe will define Xi = (xi1, . . . , x i r) T ,Y i = (yi1, . . . , y i r) T recursively. In specific, at the i-th iteration, we will adjust Xi−1,Y i−1 to Xi,Y i so that ‖Xi‖F ≤ ‖Xi−1‖F , ‖Y i‖F < ‖Y i−1‖F while keeping the first requirement satisfied, i.e. Xi(Y i)T = Σ. The angle αik is defined accordingly, i.e. α i k , 〈xik, yik〉.\nTo adjust Xi−1,Y i−1 to Xi,Y i, we will define an operation that consists of rotation and shrinking. The basic idea is the following: since the angle between xi−1i and y i−1 i is large, we can rotate x i−1 i to x i i and shrink y i−1 i to y i i to keep the inner product invariant, i.e. 〈xi−1i , yi−1i 〉 = 〈xii, yii〉. However, rotating xi−1i may destroy the orthogonal relationship between xi−1i and y i−1 j ,∀ j , i, thus we further rotate and shrink yi−1j to yij for all j , i so that yij is orthogonal to the new vector xii. Fortunately, we can prove that using such an operation we still have 〈xi−1j , yij〉 = Σ j,∀ j , i.\nA complete description of this operation is given in Table 9. Without loss of generality, we can make the assumption (145). In fact, if (145) does not hold, we can switch i and mi , arg mink∈{i,i+1,...,s} αi−1k and then apply Operation 2.\nWe will prove that Operation 2 is valid (for Di that is small enough), i.e. Xi,Y i defined in Operation 2 indeed exist. The properties of Xi,Y i obtained by Operation 2 are summarized in the following claim, which will be proved in Appendix C.4.\nClaim C.2 Consider i ∈ {1, 2, . . . , s}. Suppose\nαi−1i ≤ αi−1j , ∀ j ∈ {i + 1, i + 2, . . . , s}, (145)\nand Di > 0 satisfies Di Σi ≤ 1 12r , (146) then Xi = (xi1, . . . , x i r) T ,Y i = (yi1, . . . , y i r) T described in Operation 2 exist and satisfy the following properties:\nXi(Y i)T = Σ, (147a)\n‖xik‖ = ‖xi−1k ‖,∀k, ‖Y i − Y i−1‖2F ≤ 4 5 Di Σi (‖Y i−1‖2F − ‖Y i‖2F), (147b)\n‖Xi − Xi−1‖F = ‖xii − xi−1i ‖ ≤ 1 √\n3 Di Σi ‖xi−1i ‖\n‖Y i − Y i−1‖F ≤ 2 √\n3 Di Σi ‖Y i−1‖F , (147c)\nαil ≥ αi−1l − 1 r π 24 ≥ 1 3 π, l = i, i + 1, . . . , s. (147d)\n‖yi−1k ‖ ≥ ‖yik‖ ≥ ‖yi−1k ‖ − 1\n10r ‖yi−1k ‖, k = 1, 2, . . . , s. (147e)\n‖Y i−1‖2F − ‖Y i‖2F ≥ 5 3 Di Σi ‖yii‖2. (147f)\nWe continue to prove Proposition C.1 using Claim C.2. Given any D1, . . . ,Ds that satisfy (146), we can apply a sequence of Operation 2 for i = 1, 2, . . . , s to define two sequences of matrices Y1, . . . ,Y s and X1, . . . , Xs. Since Y1, . . . ,Y s depend on D1, . . . ,Ds, thus we can use Y s(D1, . . . ,Ds) to denote the obtained Y s by applying Operation 2 for D1, . . . ,Ds. Obviously Y s(0, . . . , 0) = Y0. We can also view ‖Y s‖2F as a function of D1, . . . ,Ds, denoted as\nf (D1, . . . ,Ds) , ‖Y s(D1, . . . ,Ds)‖2F . (148)\nIt can be easily seen that f is a continuous function with respect to D1, . . . ,Ds.\nDefine5\nη̄ , d\nΣmin\n(122a) ≥ η, D̄i , 9η̄Σi, i = 1, . . . , s. (149)\n5In the first version of the paper, we define D̄i , 92 ηΣi ≤ 9 2 η̄Σi ≤ 9 d Σmin Σi, which is enough for proving Theorem 3.1. Here we use a slightly different definition of D̄i for the purpose of proving Theorem 3.2 (linear convergence of the algorithm.)\nWe prove that f (D̄1, . . . , D̄s) ≤ (1 − 4η̄)‖Ŷ‖2F . (150)\nSuppose X̄i, Ȳ i, i = 1, . . . , s are recursively defined by Operation 2 for the choices of Di = D̄i and denote X̄0 = X, Ȳ0 = Ŷ . Since\nη̄ = d/Σmin (121) ≤ 1/(108r),\nwe know that Di = D̄i, i = 1, . . . , s as defined in (149) satisfy the condition (146), thus the property (147) holds for X̄i, Ȳ i. Suppose the k-th row of Ȳ i is (ȳik) T , k = 1, . . . , r. By (147f) and the fact Ŷ = Ȳ0, we have\n‖Ŷ‖2F − f (D̄1, . . . , D̄s) = ‖Ȳ0‖2F − ‖Ȳ s‖2F\n= s∑ i=1 (‖Ȳ i−1‖2F − ‖Ȳ i‖2F) ≥ s∑ i=1 5 3 D̄i Σi ‖ȳii‖2.\n(151)\nWe can bound ‖ȳii‖ according to (147e) as\n‖ȳii‖ ≥ ‖ȳi−1i ‖ − 1\n10r ‖ȳi−1i ‖ ≥ ‖ȳi−1i ‖ − 1 10r ‖ȳ0i ‖\n≥ · · · ≥ ‖ȳ0i ‖ − i\n10r ‖ȳ0i ‖ ≥ 9 10 ‖ȳ0i ‖.\nPlugging into (151), we get\n‖Ŷ‖2F − f (D̄1, . . . , D̄s) ≥ s∑\ni=1\n5 3 D̄i Σi ( 9 10 )2‖ȳ0i ‖2\n(149) = 15 81 100 η̄ s∑ i=1 ‖ŷi‖2 (127b) > 12η̄ 1 3 ‖Ŷ‖2F = 4η̄‖Ŷ‖2F ,\nwhich immediately leads to (150).\nCombining (150) and the fact f (0, . . . , 0) = ‖Y0‖2F = ‖Ŷ‖2F , we have\nf (0, . . . , 0) = ‖Ŷ‖2F > (1 − 4η̄)‖Ŷ‖2F = f (D̄1, . . . , D̄s).\nSince f is continuous (in the proof of Claim C.2 in Appendix C.4, all new vectors depend continuously on Di), and notice that 1 − 4η̄ < (1 − η̄)4 ≤ (1 − η̄)2(1 − η)2 ≤ 1, there must exist\n0 ≤ Di ≤ D̄i = 9η̄Σi, i = 1, . . . , s (152)\nsuch that f (D1, . . . ,Ds) = (1 − η̄)2(1 − η)2‖Ŷ‖2F . (153)\nSuppose Xi,Y i, i = 1, . . . , s are recursively defined by Operation 2 for these choices of Di, where Y s is the simplified notation for Y s(D1, . . . ,Ds). Define\nV , Y s, U , Xs, (154)\nBy this definition of V and (148), the relation (153) can be rewritten as\n‖V‖2F = (1 − η̄)2(1 − η)2‖Ŷ‖2F . (155)\nWe show that U,V defined by (154) satisfy the requirements (114). The requirement (114a) follows by the property (147a) for i = s. The requirement (114b) is proved as follows. Combining (155) with (122a) leads to\n‖V‖F = (1 − η̄)(1 − η)‖Ŷ‖F = (1 − η̄)‖Y‖F = (1 − d\nΣmin )‖Y‖F . (156)\nAccording to the property (147b), we have ‖Xi‖F = ‖Xi−1‖F , i = 1, . . . , s. Thus ‖Xs‖F = ‖Xs−1‖F = · · · = ‖X0‖F = ‖X‖F , which implies\n‖U‖F = ‖X‖F . (157)\nCombining (157) and (156) leads to the requirement (114b) .\nIt remains to show that U,V satisfy the requirement (114c). By the property (147b), we have ‖xi−1k ‖ = ‖xik‖,∀1 ≤ k ≤ r, 1 ≤ i ≤ s, which implies\n‖xik‖ = ‖x0k‖ = ‖xk‖, ∀1 ≤ k ≤ r, 1 ≤ i ≤ s. (158)\nNote that Xi differs from Xi−1 only in the i-th row (according to (147c)), thus\n‖U − X‖F = ‖Xs − X0‖F =\n√ s∑\ni=1\n‖xii − xi−1i ‖2\n(147c) ≤ 1√\n3 Di Σi\n√ s∑\ni=1\n‖xi−1i ‖2 (158) = 1 √\n3 Di Σi\n√ s∑\ni=1\n‖xi‖2\n≤ 1√ 3 Di Σi ‖X‖F (152) ≤ 3 √ 3η̄‖X‖F .\n(159)\nPlugging η̄ = d/Σmin and ‖X‖F ≤ βT into the above inequality, we get\n‖U − X‖F ≤ 3 √ 3 βT\nΣmin d. (160)\nWe then bound ‖V − Ŷ‖2F as\n‖V − Ŷ‖2F = ‖Y s − Y0‖2F\n≤ s s∑\ni=1\n‖Y i − Y i−1‖2F (147b) ≤ s4 5 Di Σi s∑ i=1 (‖Y i−1‖2F − ‖Y i‖2F)\n= s 4 5 Di Σi (‖Y0‖2F − ‖Y s‖2F) = s 4 5 Di Σi (‖Ŷ‖2F − ‖V‖2F) (152) ≤ 36\n5 sη̄(‖Ŷ‖2F − ‖V‖2F)\n(155) ≤ 36\n5 sη̄(2η + 2η̄)‖Ŷ‖2F ≤ 144 5 rη̄2‖Ŷ‖2F ,\nwhich leads to\n‖V − Ŷ‖F ≤ 12 √ 5 η̄ √ r‖Ŷ‖F . (161)\nThen we can bound ‖V − Y‖F as\n‖V − Y‖F ≤ ‖V − Ŷ‖F + ‖Y − Ŷ‖ (161),(123) ≤ 12√\n5 η̄ √ r‖Ŷ‖F + d Σmin ‖Ŷ‖F = ( 12 √ 5 + 1) d Σmin √ r‖Ŷ‖F\n(122a) = ( 12 √\n5 + 1) d Σmin √ r‖Y‖F 1 1 − η < 13d 2Σmin √ r‖Y‖F ≤ 13βT 2Σmin √ rd,\n(162)\nwhere the second last inequality is due to ( 12√ 5\n+ 1)/(1 − η) (121) ≤ ( 12√\n5 + 1)/(1 − 1108 ) < 6.5. The first part of the\nrequirement (114c) now follows by multiplying (160) and (162), and the second part of the requirement (114c) follows directly from (160) and (162)."
    }, {
      "heading" : "C.3.3 Proof of Case 2b",
      "text" : "Similar to Case 2a, denote X0 = X,Y0 = Ŷ , x0k = xk, y 0 k = ŷk, α 0 k = αk.\nBy a symmetric argument to that for Case 2a (switch the role of U, X j, j = 0, . . . , s and V,Y j, j = 0, . . . , s), we can prove that there exist Ū, V̄ that satisfy properties analogous to (114a), (156), (157), (159) and (161), i.e.\nŪV̄T = Σ, (163a)\n‖Ū‖F = (1 − η)(1 − η̄)‖X0‖F , ‖V̄‖F = ‖Y0‖F , (163b)\n‖V̄ − Y0‖F ≤ 3 √ 3η̄‖Y0‖F , ‖Ū − X0‖F ≤ 12 √ 5 η̄ √ r‖X0‖F . (163c)\nWe will show that the following U,V satisfy the requirements (114):\nU , Ū\n(1 − η)(1 − η̄) , V , V̄(1 − η)(1 − η̄). (164)\nThe requirement (114a) follows directly from (163a) and (164). According to (163b), (164) and the facts X0 = X, ‖Y0‖F = ‖Ŷ‖F = ‖Y‖F/(1 − η), we have ‖U‖F = ‖Ū‖F(1−η)(1−η̄) = ‖X0‖F = ‖X‖F , ‖V‖F = ‖V̄‖F(1 − η)(1 − η̄) = ‖Y0‖F(1 − η)(1 − η̄) = ‖Y‖F(1 − η̄), thus the requirement (114b) is proved.\nIt remains to prove the requirement (114c). We bound ‖U − X‖F as\n‖U − X‖F ≤ ‖U − Ū‖F + ‖Ū − X‖F (164) ≤ 2η̄‖U‖F + ‖Ū − X0‖F\n(114b),(163c) ≤ 2η̄‖X‖F + 12 √ 5 η̄ √ r‖X0‖F ≤ 15 2 η̄ √ r‖X‖F ≤ 15 2 βT Σmin √ rd.\n(165)\nUsing the fact Ŷ = Y0, we bound ‖V − Y‖F as\n‖V − Y‖F ≤ ‖V − V̄‖F + ‖V̄ − Ŷ‖F + ‖Ŷ − Y‖F (164),(123) ≤ 2η̄‖V̄‖F + ‖V̄ − Y0‖F +\nd Σmin ‖Ŷ‖F\n(163b),(163c) ≤ 2η̄‖Ŷ‖F + 3 √ 3η̄‖Y0‖F + d Σmin ‖Ŷ‖F = (3 + 3 √ 3) d\nΣmin ‖Ŷ‖F\n(122a) =\n3 + 3 √\n3 1 − η d Σmin ‖Y‖F ≤ 58βT 7Σmin d.\n(166)\nThe first part of the requirement (114c) now follows by multiplying (165) and (166), and the second part follows directly from (165) and (166)."
    }, {
      "heading" : "C.4 Proof of Claim C.2",
      "text" : "Suppose Claim C.2 holds for 1, 2, . . . , i − 1, we prove Claim (C.2) for i. By the property (147a) and (147d) of Claim C.2 for i − 1, we have\nXi−1(Y i−1)T = Σ. (167a)\nαi−1i ≥ α [0] i − i − 1 r 1 24 π ≥3 8 π − 1 24 π + 1 24r π = 1 3 π + 1 24r π ≥ 1 3 π. (167b)\nTo simplify the notations, throughout the proof of Claim C.2, we denote Xi−1,Y i−1 as X,Y and denote Xi,Y i as X′,Y ′. The notations αi−1k , α i k are changed accordingly to αk, α ′ k. Then (167a) and (167b) become\nXYT = Σ, (168a)\nαi ≥ 1 3 π + 1 24r π ≥ 1 3 π. (168b)\nWe need to prove that X′,Y ′ exist and satisfy the properties in Claim (C.2), i.e. (with the simplification of notations)\nX′(Y ′)T = Σ. (169a)\n‖x′k‖ = ‖xk‖,∀k, ‖Y ′ − Y‖2F ≤ 4 5 Di Σi (‖Y‖2F − ‖Y ′‖2F). (169b)\n‖X′ − X‖F = ‖x′i − xi‖ ≤ 1 √\n3 Di Σi ‖xi‖, ‖Y ′ − Y‖F ≤ 2 √ 3 Di Σi ‖Y‖F . (169c)\nα′l ≥ αl − 1 r π 24 ≥ 1 3 π, l = i, i + 1, . . . , s. (169d)\n‖yk‖ ≥ ‖y′k‖ ≥ ‖yk‖ − 1\n10r ‖yk‖, k = 1, 2, . . . , s. (169e)\n‖Y‖2F − ‖Y ′‖2F ≥ 5 3 Di Σi ‖yi‖2. (169f)\nC.4.1 Ideas of the proof of Claim (C.2)\nBefore presenting the formal proof, we briefly describe its idea. The goal of Operation 2 is to reduce the norm of Y while keeping 〈X,Y〉 and ‖X‖F invariant, by rotating and shrinking xi, yk, k = 1, . . . ,K (note that x j,∀ j , i, do no change). We first rotate xi and shrink yi at the same time so that the new inner product 〈x′i , y′i〉 equals the previous one 〈xi, yi〉 (this step can be viewed as a combination of two steps: first rotate xi to increase the inner product, then shrink yi to reduce the inner product). In order to preserve the orthogonality of X and Y , we need to rotate y j,∀ j , i, so that the new y′j is orthogonal to x ′ i .\nAlthough the above procedure is simple, there are two questions to be answered. The first question is: will the inner product 〈x j, y j〉 increase as we rotate y j, for all j , i? If yes, we could first rotate and then shrink y j to obtain y′j so that the new inner product 〈x j, y′j〉 equals 〈x j, y j〉, which achieves the goal of Operation 2. By resorting to the geometry (in a rigourous way) we are able to provide an affirmative answer to the above question. To gain an intuition why this is possible, we use Figure 5 to illustrate. Consider the case i = 2 and rotate x2 towards y2 to obtain x′2, then y1 has to be rotated so that y ′ 1 is orthogonal to x ′ 2. It is clear from this figure that the angle between y1 and x1 also decreases, or equivalently, the inner product 〈x1, y1〉 also increases. One might ask whether we have utilized additional assumptions on the relative positions of xi, yi’s. In fact, we do not utilize additional assumptions; what\nwe implicitly utilize is the fact that 〈xi, yi〉 > 0,∀i (see Figure 6, Figure 7 and the paragraph after (176) for detailed explanations).\nThe second question is: will the angle α′j = ∠(x j, y ′ j) still be larger than, say, 1 3π, for all j > i? If yes, then we can apply Operation 2 repeatedly for all i = 1, 2, . . . , s. To provide an affirmative answer, we should guarantee that each angle decreases at most 1s ( 3 8π− 1 3π) = 1 24sπ, i.e. ∠(x j, y ′ j) ≥ ∠(x j, y j)− 124sπ,∀ i < j ≤ s. Unlike the first question which can be answered by reading Figure 6 and Figure 7, this question cannot be answered by just reading figures. We make some algebraic computation to obtain the following result: under the assumption that αi is no less than α j, during Operation 2 the amount of decrease in α j is upper bounded by the amount of decrease in αi, which can be further bounded above by 124sπ. This result explains why our proof requires the assumption αi ≥ α j,∀ i < j ≤ s, i.e. (145).\nC.4.2 Formal proof of Claim (C.2)\nWe first show how to define x′i and y ′ i . Note that\n‖xi‖‖yi‖ = 〈xi, yi〉 cosαi ≥ Σi cos( π3 ) = 2Σi. (170)\nSince (170) implies Σi+Di‖xi‖‖yi‖ ≤ 2Σi ‖xi‖‖yi‖ ≤ 1, we can define\nα′i , arccos( Σi + Di ‖xi‖‖yi‖ ) ∈ [0, π 2 ].\nThere is a unique x′i in the plane span{xi, yi} which satisfies\n‖x′i‖ = ‖xi‖ (171)\nand ∠(x′i , yi) = α ′ i . By the definition of α ′ i above, we have\n〈x′i , yi〉 = Σi + Di.\nThe existence of x′i is proved. We define\ny′i , Σi\nΣi + Di yi, (172)\nthen 〈x′i , y′i〉 = Σi\nΣi + Di 〈x′i , yi〉 = Σi. (173)\nThe existence of y′i is also proved.\nSince 0 < 〈xi, yi〉 = Σi < 〈x′i , yi〉, we have π2 > αi > α′i > 0, thus we can define\nθ , αi − α′i = ∠(x′i , xi) ∈ (0, αi). (174)\nFix any j , i, we then show how to define y′j. Define\nAi , span j,i{x j}⊥yi, Bi , span j,i{y j}⊥xi, Ti , Ai ∩ Bi.\nLet −−→ OY j = y j, K j , PAi (Y j),H j , PTi (Y j). Then ∠Y jH jK j = min{∠(xi, yi), π − ∠(xi, yi)} = ∠(xi, yi) = αi. Since αi > θ, there exists a unique point Y ′j in the line segment Y jK j such that\n∠Y jH jY ′j = θ. (175)\nSince K j = PAi (Y j) and xk ∈ Ai,∀k , i, we have −−−→ Y jK j⊥xk,∀k , i, thus\n−−−→ Y jY ′j⊥xk, ∀k , i. (176)\nSee Figure 6 and Figure 7 for the geometrical interpretation; note that Ti in general is not a line but a r−2 dimensional space. The righthand side subfigures represents the 2 dimensional subspace T⊥i ; since span{H jY j,H jK j} = T⊥i = span{xi, yi}, we can draw xi, yi, y′i as the vectors starting from H j and lying in the plane H jY jK j = T⊥i in the figures. Figure 6 and Figure 7 differ in the relative position of xi and K j: xi and K j lie in the same side of line H jY j in Figure 6 but in different sides in Figure 7. Given the positions of xi and H j,Y j,K j, the position of yi is determined since yi⊥ −−−→ H jK j and ∠(xi, yi) < π2 .\nIn both figures, we have\n∠( −−−→ H jY ′j, x ′ i ) = ∠( −−−→ H jY j, xi) − ∠(x′i , xi) + ∠Y jH jY ′j\n(174),(175) ========\nπ 2 − θ + θ = π 2 ,\n=⇒ −−−→ H jY ′j⊥x′i . (177)\nNow we are ready to define y′j and establish its properties. Define\ny′j , −−→ OY ′j. (178)\nSince Y ′j lies in the line segment K jY j and ∠Y jK jO = π/2, we have\n‖y′j‖ ≤ ‖y j‖. (179)\nWe also have y′j = y j + −−−→ Y jY ′j ∈ span{y j, yi} ⊥xk, ∀k , i, j. (180) According to the fact −−−→ OH j⊥x′i and (177), we have\ny′j = −−−→ OH j + −−−→ H jY ′j ⊥ x′i . (181)\nLet k = j in (176), we obtain\n0 = 〈 −−−→ Y jY ′j, x j〉 = 〈y′j − y j, x j〉 = 0 =⇒ 〈x j, y′j〉 = 〈x j, y j〉. (182)\nWe have shown that y′j defined in (178) satisfies (180), (181) and (182), thus the existence of y ′ j in Operation 2 is proved.\nHaving defined x′i , y ′ i and y ′ j,∀ j , i, we further define\nx′j , x j,∀ j , i, (183)\nwhich completes the definition of X′,Y ′. In the rest, we prove that X′,Y ′ satisfy the desired property (169).\nThe property (169a) can be directly proved by the definitions of X′,Y ′. In specific, according to (173), (182) and the definition (183), we have 〈x′k, y′k〉 = Σk,∀k. According to the definitions (183), (172) and the fact yi⊥x j,∀ j , i, we have y′i⊥x′j,∀ j , i. Together with (180) and (181), we obtain 〈x′k, y′l〉 = 0,∀k , l. Thus X′(Y ′)T = Σ.\nNext, we prove the property (169d). We first prove\nα′i − αi = θ ≤ 1 r π 24 . (184)\nDefine hi , x′i − xi, then ‖hi‖ = 2‖xi‖ sin( θ\n2 ). (185)\nFrom 〈x′i , yi〉 = Σi + Di = 〈xi, yi〉 + Di, we obtain 〈hi, yi〉 = Di. Note that 〈hi, yi〉 = ‖hi‖‖yi‖ cos(∠(hi, yi)) and ∠(hi, yi) = π2 − αi + θ 2 , thus\n‖hi‖ = Di\n‖yi‖ sin(αi − θ2 ) . (186)\nAccording to (185) and (186), we have\nDi ‖xi‖‖yi‖ = 2 sin(αi − θ 2 ) sin( θ 2 ) ≥ 2 sin(αi 2 ) sin( θ 2 )\n≥ 2 sin(π 6 ) sin( θ 2 ) = sin( θ 2 ) ≥ θ π ,\nwhere the last equality follows from the fact that sin(t)t is decreasing in t ∈ (0, π 2 ]. Note that Di ‖xi‖‖yi‖ can be upper bounded as Di\n‖xi‖‖yi‖ (170) ≤ Di 2Σi (146) ≤ 1 24r .\nCombining the above two relations, we get (184).\nTo prove α j − α′j ≤ π\n24r ,∀ j ∈ {i + 1, . . . , s}, (187)\nwe only need to prove θ j , α j − α′j ≤ θ, ∀ j ∈ {i + 1, . . . , s} (188)\nand then use (184). The equality (182) implies that ‖x j‖‖y j‖ cos(α j) = ‖x j‖‖y′j‖ cos(α′j), which leads to\ncos(α j) cos(α j − θ j) = cos(α j) cos(α′j) = ‖y′j‖ ‖y j‖ .\nFor any two points P1, P2, we use |P1P2| to denote the length of the line segment P1P2. Since −−−→ OH j is orthogonal to plane H jK jY j, we have ‖y′j‖2\n‖y j‖2 = |OH j|2 + |H jY ′j |2 |OH j|2 + |H jY j|2 ≥ |H jY ′j |2 |H jY j|2 ,\nwhere the last inequality follows from the fact that |H jY ′j | ≤ |H jY j|. Since ∠Y jH jK j = αi, ∠Y ′jH jK j = α′i and ∠Y jK jH j = π2 , we have\n|H jY ′j | |H jY j| = sin ∠Y ′jY jH j sin ∠Y jY ′jH j = sin(π/2 − αi) sin(π/2 + α′i) = cos(αi) cos(α′i) .\nAccording to the assumption (145) and i < j ≤ s, we have 0 ≤ αi ≤ α j ≤ π2 . Since cos(x)/ cos(x − θ) is decreasing in [0, π2 ], we can get\ncos(αi) cos(α′i) = cos(αi) cos(αi − θ) ≥ cos(α j) cos(α j − θ) .\nCombining the above four relations, we get\ncos(α j) cos(α j − θ j) ≥ cos(α j) cos(α j − θ) ,\nwhich implies cos(α j − θ) ≥ cos(α j − θ j) that immediately leads to (188). Thus we have proved (187), which combined with (184) establishes the property (169d).\nThen we prove the property (169c). Since x′j = x j,∀ j , i, we have ‖X′ − X‖F = ‖x′i − xi‖, which can be bounded as\n‖x′i − xi‖ = ‖hi‖ (186) = Di ‖yi‖ sin(αi − θ2 ) ≤ ‖xi‖Di‖xi‖‖yi‖ sin( π3 ) (170) ≤ ‖xi‖Di\n2Σi sin( π3 ) <\n1 √\n3 ‖xi‖ Σi Di,\nwhere the first inequality is due to\nαi − θ/2 ≥ αi − θ (168b) ≥ π/3 + π/24 − θ (184) ≥ π/3. (189)\nThus the first part of (169c) is proved.\nAccording to (185) and (186), we have\n2 sin( θ\n2 ) = Di ‖xi‖‖yi‖ sin(αi − θ2 )\n(190)\nNow we upper bound ‖y′j − y j‖ as\n‖y′j − y j‖ = |Y ′jY j|\n= sin(θ)\ncos(αi − θ) |H jY j|\n= 2 sin( θ\n2 ) cos(\nθ 2 ) 1 cos(αi − θ) |H jY j|\n(190) = Di ‖xi‖‖yi‖ sin(αi − θ2 ) cos( θ 2 ) 1 cos(αi − θ) |H jY j| ≤ Di ‖xi‖‖yi‖ sin(αi − θ2 ) 1 cos(αi) |H jY j| (189) ≤ Di\nsin( π3 )〈xi, yi〉 |H jY j|\n≤ 2√ 3 Di Σi |H jY j|,\n(191)\nwhere the last inequality is due to the fact 〈xi, yi〉 = Σi. Using |H jY j| ≤ ‖y j‖, we obtain\n‖y′j − y j‖ ≤ 2 √\n3 Di Σi ‖y j‖. (192)\nAccording to the definition (172), we have\n‖yi − y′i‖ = (1 − Σi\nΣi + Di )‖yi‖ = Di Σi + Di ‖yi‖ ≤ Di Σi ‖yi‖. (193)\nAccording to (192) (which holds for any j ∈ {1, . . . , r}\\{i}) and (193), we get\n‖Y − Y ′‖F =\n√ r∑\nk=1\n‖yk − y′k‖2 ≤ 2 √\n3 Di Σi\n√ r∑\nk=1\n‖yk‖2 = 2 √\n3 Di Σi ‖Y‖F ,\nwhich proves the second part of (169c).\nThe property (169e) can be proved as follows. By the definition (172), we have ‖y′i‖ ≤ ‖yi‖, which combined with (179) (for all j , i) leads to\n‖y′k‖ ≤ ‖yk‖, k = 1, . . . , s.\nAccording to (192) (for all j , i) and (193), we have ‖y′k − yk‖ ≤ 2√ 3 Di Σi ‖yk‖,∀k, which implies\n‖y′k‖ ≥ ‖yk‖ − ‖y′k − yk‖ ≥ ‖yk‖ − 2 √\n3 Di Σi ‖yk‖ (146) ≥ ‖yk‖ − 1 10r ‖yk‖, ∀k.\nCombining the above two relations we obtain the property (169e).\nThe property (169f) can be easily proved by (172). In fact, we have\n‖yi‖2 − ‖y′i‖2 = (‖yi‖ − ‖y′i‖)(‖yi‖ + ‖y′i‖)\n≥ 2‖y′i‖(‖yi‖ − ‖y′i‖) (172) = 2‖y′i‖( Σi + Di Σi − 1)‖y′i‖\n= 2 Di Σi ‖y′i‖2 ≥ 2 Di Σi ( 11 12 )2‖yi‖2 ≥ 5 3 Di Σi ‖yi‖2.\n(194)\nwhere the second last inequliaty follows from ‖y′i‖ ≥ ‖yi‖ − ‖yi − y′i‖ (193) ≥ ‖yi‖ − Di‖yi‖/Σi (146) ≥ 11‖yi‖/12. According to (179) (for all j , i), we have ‖Y‖2F −‖Y ′‖2F ≥ ‖yi‖2−‖y′i‖2, which combined with (194) leads to the property (169f).\nAt last, we prove the property (169b). The first part ‖X′‖F = ‖X‖F follows from (171) and (183), thus it remains to prove the second part. Denote ϕ j , ∠Y jOY ′j, β j , ∠Y jOK j as shown in Figure 8. Pick a point Z j in the line\nsegment OY j so that |OZ j| = |OY ′j |, then |Y jZ j| = ‖y j‖ − ‖y′j‖. Thus we have\n‖y j − y′j‖ ‖y j‖ − ‖y′j‖ = |Y jY ′j | |Y jZ j| = sin(∠Y jZ jY ′j) sin(∠Y jY ′jZ j)\n= sin(π/2 − ϕ j/2) sin(β j − ϕ j/2) ≤ 1 sin(β j − ϕ j) .\n(195)\nIn order to bound 1/ sin(β j − ϕ j) 6, we use the following bound:\nsin β j sin(β j − ϕ j) = |Y jK j| ‖y j‖ ‖y′j‖ |Y ′jK j| ≤ |Y jK j| |Y ′jK j| = tanαi tan(αi − θ) .\nThen we have\nsin β j sin(β j − ϕ j) sin(αi − θ) sin(αi) cos(αi − θ) cos(αi)\n= cosαi cos θ + sinαi sin θ cos(αi) ≤ sin(θ) cos(αi) + 1.\n(196)\nAccording to (190) and the fact cos(αi) = 〈xi, yi〉/(‖xi‖‖yi‖) = Σi/(‖xi‖‖yi‖), we have sin(θ)\ncos(αi) ≤ 2 sin(θ/2) cos(αi) = Di ‖xi‖‖yi‖ sin(αi − θ/2) ‖xi‖‖yi‖ Σi\n= Di Σi 1 sin(αi − θ/2) (146),(189) ≤ 1 12 1 sin(π/3) = 1 6 √ 3 .\n6The part from (195) to (197) can be replaced by a simpler bound sin(β j − ϕ j) ≥ sin(β j/2) ≥ sin(β j)/2 and we can still obtain a similar bound as (199); however, by using this simpler yet looser bound, the constant coefficient 7/8 will be replaced by a larger constant.\nPlugging the above relation into (196), we obtain\nsin β j sin(β j − ϕ j) sin(αi − θ) sin(αi)\n≤ 6 √ 3 + 1\n6 √ 3 . (197)\nCombining (195) and (191), we obtain\n‖y j − y′j‖ ‖y j‖ − ‖y′j‖ ‖y j − y′j‖ ‖y j‖ ≤ 1 sin(β j − ϕ j) 2 √ 3 Di Σi |H jY j| ‖y j‖\n(197) ≤ 2√\n3 Di Σi\n6 √ 3 + 1\n6 √ 3 |H jY j| ‖y j‖ sin(αi) sin(β j) 1 sin(αi − θ)\n= 6 √\n3 + 1 9 Di Σi 1 sin(αi − θ) (189) ≤ 6 √ 3 + 1 9 2 √ 3 Di Σi ≤ 3 2 Di Σi ,\n(198)\nwhere the last equality is due to |H jY j| sin(αi) = |Y jK j| = ‖y j‖ sin(β j).\nAccording to (192) and (146), we obtain that ‖y j − y′j‖ ≤ 2√3 1 12‖y j‖ ≤ 1 8‖y j‖, which further implies ‖y′j‖ + ‖y j‖ ≥\n2‖y j‖ − ‖y j − y′j‖ ≥ 158 ‖y j‖. Then by (198) we have\n‖y j − y′j‖2 ≤ 5 √\n3 + 1 6 Di Σi (‖y j‖ − ‖y′j‖)‖y j‖\n≤ 3 2 Di Σi (‖y j‖ − ‖y′j‖)(‖y′j‖ + ‖y j‖) 8 15 = 4 5 Di Σi\n(‖y j‖2 − ‖y′j‖2). (199)\nAccording to the definition (172), we have\n‖yi‖2 − ‖y′i‖2\n‖yi − y′i‖2 = 1 − (Σi)2/(Σi + Di)2 [1 − Σi/(Σi + Di)]2\n= (Σi + Di)2 − Σ2i\nD2i = D2i + 2DiΣi D2i ≥ 2 Σi Di ,\nwhich implies\n‖yi − y′i‖2 ≤ 1 2 Di Σi (‖yi‖2 − ‖y′i‖2). (200)\nSumming up (199) for j ∈ {1, . . . , r}\\{i} and (200), we obtain\n‖Y − Y ′‖2F ≤ 4 5 Di Σi (‖Y‖2F − ‖Y ′‖2F),\nwhich proves the second part of (169b)."
    }, {
      "heading" : "D Proofs of the results in Section 5",
      "text" : ""
    }, {
      "heading" : "D.1 Proof of Claim 5.2",
      "text" : "The proof of this claim consists of two parts: first, by a classical result we have that M0, the best rank-r approximation of 1pPΩ(M), is close to M; second, show that the scaling does not change the closeness.\nWe first present the following result.\nLemma D.1 Assume M is a rank r matrix of dimension m×n with m ≥ n, and denote Mmax = ‖M‖∞ as the maximum magnitude of the entries of M. Suppose each entry of M is included in Ω with probability p ≥ C0 log(m+n)m , and M0 is the best rank-r approximation of 1pPΩ(M). Then with probability larger than 1 − 1/(2n4),\n1 mnM2max ‖M − M0‖2F ≤ C2 α\n3 2 r\npm , (201)\nfor some numerical constant C2.\nRemark: Lemma D.1 can be found in [31]. The original version [31, Theorem 1.1] holds for M0 = Pr(Tr(PΩ(M))/p), where Tr(·) denotes a trimming operator which sets to zero all rows and columns that have too many observed entries, and Pr(·) denotes the best rank-r approximation. By standard Chernoff bound one can show that none of the rows and columns have too many observed entries with high probability, thus the conclusion of [31, Theorem 1.1] holds for M0 = Pr(PΩ(M))/p. The key to establish Lemma D.1 is a bound on ‖M− 1pPΩ(M)‖2, which can be simply proved by matrix concentration inequalities; see [17, Remark 6.1.2], [4, Theorem 6.3] or [7, Theorem 3.5]. The proof of [31, Theorem 1.1] is more complicated than applying matrix concentration inequalities since it holds for a weaker condition |Ω| ≥ O(n).\nNote that X̂0, Ŷ0 defined in Table 1 satisfy\nX̂0Ŷ0 T = Pr(PΩ(M)/p) = M0. (202)\nRecall that the SVD of M is M = ÛΣV̂ , where Û, V̂ satisfies (12). We have\n|Mi j| = r∑\nk=1\n|ÛikV̂ jkΣk | ≤ Σmax r∑\nk=1\n|ÛikV̂ jk |\n≤ Σmax\n√ r∑\nk=1\nÛ2ik\n√ r∑\nk=1\nV̂2jk (12) ≤ Σmax µr √\nmn , ∀ i, j.\n(203)\nThe above relation implies Mmax ≤ Σmax µr√mn . Plugging this inequality and p = |Ω|/(mn) into (201), we get\n‖M − M0‖2F ≤ C2 mnα\n3 2 r\npm Σ2max\nµ2r2\nmn = C2n\nα 3 2 r3κ2µ2\n|Ω| Σ 2 min. (204)\nPlugging (202) and the assumption (27) into (204), we get\nδ̂0 , ‖M − X̂0Ŷ0 T ‖F ≤ √ C2 C0 Σmin r1.5κ2 . (205)\nThe property (a), i.e. (X0,Y0) ∈ ( √\n2/3K1) follows directly from the definitions of X0 and Y0 in (23). We then prove the property (b), i.e. (X0,Y0) ∈ ( √ 2/3K2). By (205) we have ‖M − M0‖F ≤ Σmin/5 ≤ Σmax/5 for large enough C0. This inequality combined with ‖M − M0‖F ≥ ‖M − M0‖2 ≥ ‖M0‖2 − Σmax yields\n‖M0‖2 ≤ 6 5 Σmax. (206)\nBy the definitions of X̂0, Ŷ0 (i.e. X̂0 = X̄0D 1 2 0 , Ŷ0 = Ȳ0D 1 2 0 , where X̄0D0Ȳ T 0 is the SVD of M0), we have\n‖X̂0‖2 = ‖Ŷ0‖2 = √ ‖M0‖2 (206) ≤ √ 6 5 √ Σmax. (207)\nThen we have ‖X̂0‖2F ≤ r‖X̂0‖22 ≤\n6 5 rΣmax (15) < 2 3 β2T , (208)\nwhere the last inequality follows from CT > 9/5. By the definition of X0 in (23), we have ‖X0‖2F ≤ ‖X̂0‖2F ≤ 23β2T . Similarly, we can prove ‖Y0‖2F ≤ 23β2T . Thus the property (b) is proved.\nNext we prove the property (c), i.e. ‖M − X0YT0 ‖F ≤ δ0. Since X̂0, Ŷ0 satisfy max{‖X̂0‖F , ‖Ŷ0‖F} ≤ βT (due to (208) and the analogous inequality for Ŷ0) and (205), it follows from Proposition 4.1 that there exist U0,V0 such that\nU0VT0 = M; (209a) ‖U0‖2 ≤ ‖X0‖2; (209b)\n‖U0 − X̂0‖F ≤ 6‖Ŷ0‖2 5Σmin δ̂0, ‖V0 − Ŷ0‖F ≤ 3‖X̂0‖2 Σmin δ̂0; (209c)\n‖U(i)0 ‖ 2 ≤ rµ\nm β2T , ‖V ( j) 0 ‖ 2 ≤ 3rµ 2n β2T . (209d)\nNote that the above inequalities (209b) and (209c) are not due to (48b) and (48c) of Proposition 4.1, but stronger results (99) and (107) established during the proof of Proposition 4.1.\nNote that\n‖M − X0YT0 ‖F = ‖U0(V0 − Y0)T + (U0 − X0)YT0 ‖F ≤ ‖U0(V0 − Y0)T ‖F + ‖(U0 − X0)YT0 ‖F ≤ ‖U0‖2‖V0 − Y0‖F + ‖U0 − X0‖F‖Y0‖2,\n(210)\nwhere the last inequality follows from Proposition B.4. Since X(i)0 and X̂ (i) 0 has the same direction and ‖X (i) 0 ‖ ≤ ‖X̂ (i) 0 ‖, by Proposition B.3 we have\n‖X0‖2 ≤ ‖X̂0‖2 ≤ √\n6 5\n√ Σmax. (211)\nCombining (209b) and (211), we get\n‖U0‖2 ≤ √\n6 5\n√ Σmax. (212)\nSimilar to (211), we have\n‖Y0‖2 ≤ √\n6 5\n√ Σmax. (213)\nIt remains to bound ‖V0 − Y0‖F and ‖U0 − X0‖F . Let us prove the following inequality:\n‖U(i)0 − X (i) 0 ‖ ≤ ‖U (i) 0 − X̂ (i) 0 ‖, ∀ i. (214) If ‖X̂(i)0 ‖ ≤ √ 2 3β1, then (214) becomes equality since X̂ (i) 0 = X (i) 0 . Thus we only need to consider the case ‖X̂\n(i) 0 ‖ >√\n2 3β1. In this case by the definition of X0 in (23) we have ‖X (i) 0 ‖ = √ 2 3β1. From (209d), we get\n‖U(i)0 ‖ 2 < 3 2 rµ m β2T ≤ 2 3 β21 < ‖X̂ (i) 0 ‖ 2. (215)\nFor simplicity, denote u , U(i)0 , x , X (i) 0 , τ , ‖X̂(i)0 ‖√ 2/3β1 = ‖X̂(i)0 ‖ ‖x‖ > 1. Then (215) becomes ‖u‖ ≤ ‖x‖ and (214) becomes ‖u − x‖ ≤ ‖u − τx‖. The latter can be transformed as follows:\n‖u − x‖ ≤ ‖u − τx‖ ⇐⇒ ‖x‖2 − 2〈u, x〉 ≤ τ2‖x‖2 − 2τ〈u, x〉 ⇐⇒ 2(τ − 1)〈u, x〉 ≤ (τ2 − 1)‖x‖2\n⇐⇒ 2〈u, x〉 ≤ (τ + 1)‖x‖2. (216)\nSince 〈u, x〉 ≤ ‖u‖‖x‖ ≤ ‖x‖2 (here we use ‖u‖ ≤ ‖x‖ which is equivalent to (215)) and 2 < τ + 1, the last inequality of (216) holds, which implies that ‖u − x‖ ≤ ‖u − τx‖ holds and, consequently, (214) holds.\nAn immediate consequence of (214) is\n‖U0 − X0‖F ≤ ‖U0 − X̂0‖F (209c) ≤ 5‖Ŷ0‖2\n4Σmin δ̂0\n(207) ≤ 5\n4 √ 6 5 √ Σmax δ̂0 Σmin . (217)\nSimilarly, we have\n‖V0 − Y0‖F (209c) ≤ 3 √ 6 5 √ Σmax δ̂0 Σmin . (218)\nPlugging (212), (213), (217) and (218) into (210), we get\n‖M − X0YT0 ‖F ≤ √\n6 5\n√ Σmax\n5 4 √ 6 5 √ Σmax δ̂0 Σmin + √ 6 5 √ Σmax3 √ 6 5 √ Σmax δ̂0 Σmin\n=( 3 2 + 18 5 )κδ̂0\n(205) ≤ 51\n10 √ C2 C0 Σmin\nr1.5κ (16) ≤ δ0,\nwhere the last inequality holds for Cd ≥ 5153 √ C0 C2 . Therefore property (c) is proved."
    }, {
      "heading" : "D.2 Proof of Claim 3.1",
      "text" : "As mentioned in Section 2.1, in this proof we only need to consider the Bernolli model that Ω includes each entry of M with probability p and the expected size S satisfies (27). Denote d , ‖M−XYT ‖F . Let a = U(V−Y)T +(U−X)VT , b = (U − X)(V − Y), where U,V are defined with the properties in Corollary 4.1.\nAccording to (46) we have ‖PΩ(a)‖2F ≥ 2740 pd2. According to (40a), we have ‖PΩ(b)‖F ≤ 1 5 √ pd. Therefore, ‖PΩ(M − XYT )‖F = ‖PΩ(a − b)‖F ≥ ‖PΩ(a)‖F − ‖PΩ(b)‖F ≥ √ 27 40 √ pd − 15 √ pd ≥ 35 √ pd ≥ 1√ 3 √ pd.\nAccording to (40b), we have ‖b‖F ≤ 110 d. According to (45) (which is a corollary of [4, Theorem 4.1]), we have ‖PΩ(a)‖2F ≤ 76 p‖a‖2F ≤ 7 6 p(‖M − XYT ‖F + ‖b‖F)2 ≤ 7 6 p(1 + 1 10 )\n2d2 ≤ 1712 pd2. Thus, ‖PΩ(a − b)‖F ≤ ‖PΩ(a)‖F + ‖PΩ(b)‖F ≤ ( √ 17 12 + 1 5 ) √ pd ≤ √ 2pd."
    }, {
      "heading" : "D.3 Proof of Proposition 5.1",
      "text" : "We first provide a general condition for (X,Y) ∈ K1 ∩ K2 (i.e. incoherent and bounded) based on the function value F̃(X,Y).\nProposition D.1 Suppose the sample set Ω satisfies (29) and ρ = 2pδ20/G0(3/2), where δ0 is defined in (16). Suppose (X0,Y0) satisfies (66) and\nF̃(X,Y) ≤ 2F̃(X0,Y0). (219)\nThen (X,Y) ∈ K1 ∩ K2.\nProof of Proposition D.1: We prove by contradiction. Assume the contrary that (X,Y) < K1 ∩ K2. By the definition of K1,K2 in (30), we have either ‖X(i)‖2 > β21 for some i, ‖Y ( j)‖2 > β22 for some j, ‖X‖2F > β2T or ‖Y‖2F > β2T . Hence at least one term of G(X,Y) = ρ ∑m i=1 G0(\n3‖X(i)‖2 2β21\n) + ρ ∑n\nj=1 G0( 3‖Y ( j)‖2\n2β22 ) + ρG0( 3‖X‖2F 2β2T ) + ρG0( 3‖Y‖2F 2β2T ) is larger than\nG0( 32 ). In addition, all the other terms in the expression of G(X,Y) are nonnegative, thus we have G(X,Y) > ρG0( 3 2 ). Therefore,\nF̃(X,Y) ≥ G(X,Y) > ρG0( 3 2 ) = 2pδ20. (220)\nWe have F̃(X0,Y0) =\n1 2 ‖PΩ(M − X0YT0 )‖2F ≤ p‖M − X0YT0 ‖2F ≤ pδ20, (221)\nwhere the first equality is due to G(X0,Y0) = 0 which follows from (X0,Y0) ∈ ( √ 2 3 K1) ∩ ( √ 2 3 K2), the second\ninequality follows from (29) and the fact (X0,Y0) ∈ ( √ 2 3 K1) ∩ ( √ 2 3 K2) ∩ K(δ0) ⊆ K1 ∩ K2 ∩ K(δ), and the last inequality is due to (X0,Y0) ∈ K(δ0). Combining (220) and (221), we get\nF̃(X,Y) > 2F̃(X0,Y0),\nwhich contradicts (219).\nWe can prove that (67) implies F̃(xi) ≤ 2F̃(x0), ∀ i. (222)\nIn fact, when (67c) holds, as the first inequality in (67c) the above relation also holds. When (67a) holds, let λ = 0 in (67a) we get (222). When (67b) holds, we have\nψ(xi,∆i; 1) (67b) ≤ ψ(xi,∆i; 0) (65b) = F̃(xi), (223)\nwhich implies F̃(xi+1) = F̃(xi +∆i) (65b) ≤ ψ(xi,∆i; 1) ≤ F̃(xi). This relation holds for any i, thus F̃(xi+1) ≤ F̃(xi) ≤ · · · ≤ F̃(x0) ≤ 2F̃(x0).\nSince (67) implies implies F̃(xt) ≤ 2F̃(x0) (see (222)), by Proposition D.1 we have xt ∈ K1 ∩ K2. The rest of the proof is devoted to establish\nxt ∈ K( 2 3 δ), ∀ t. (224)\nDefine the distance of x = (X,Y) and u = (U,V) as\nd(x,u) = ‖XYT − UVT ‖F ,\nthen (Xt,Yt) ∈ K(δ)⇐⇒ ‖XtYTt − M‖F ≤ δ can be expressed as\nd(xt,u∗) ≤ δ.\nWe first prove the following result:\nLemma D.2 If F̃(x) ≤ 2F̃(x0), then d(u∗,x) < [ 23δ, δ].\nProof of Lemma D.2: We prove by contradiction. Assume the contrary that\nd(u∗,x) ∈ [2 3 δ, δ]. (225)\nSince x0 satisfies (66), according to the proof of Proposition D.1 we have (221), i.e.\nF̃(x0) ≤ pδ20. (226)\nAccording to Proposition D.1 and the assumption F̃(x) ≤ 2F̃(x0), we have x ∈ K1 ∩K2. Together with (225) we get x ∈ K1 ∩ K2 ∩ K(δ). Then we have\nF̃(x) ≥ 1 2 ‖PΩ(M − XYT )‖2 (29) ≥ 1 6 p‖M − XYT ‖2 = 1 6 pd(u∗,x)2. (227)\nPlugging d(u∗,x)2 ≥ ( 23 )2δ2 (16) = 16δ20 (226) ≥ 16F̃(x0)/p into (227), we get F̃(x) ≥ 83 F̃(x0), which together with the assumption F̃(x) ≤ 2F̃(x0) leads to F̃(x) = F̃(x0) = 0. Then by (227) we get d(u∗,x) = 0, which contradicts (225) since δ > 0. Thus Lemma D.2 is proved.\nNow we get back to the proof of (224). We prove (224) by induction on t. The basis of the induction holds due to (66) and the fact δ0 = δ/6. Suppose xt ∈ K(2δ/3), we need to prove xt+1 ∈ K(2δ/3). Assume the contrary that xt+1 < K(2δ/3), i.e.\nd(u∗,xt+1) > 2 3 δ. (228)\nLet i = t + 1 in (222), we get F̃(xt+1) ≤ 2F̃(x0). Then by Lemma D.2 we have\nd(xt+1,u∗) < [ 2 3 δ, δ]; (229)\nCombining (229) and (228), we get d(xt+1,u∗) > δ. (230)\nIn the rest of the proof, we will derive a contradiction for the three cases (67a), (67b) and (67c) separately.\nCase 1: (67a) holds. By the induction hypothesis, d(xt,u∗) ≤ 23δ. Since d(x,u∗) is a continuous function over x, the relation d(xt,u∗) ≤ 23δ and (230) imply that there must exist some x′ = (1− λ)xt+1 + λxt, λ ∈ [0, 1] such that\nd(x′,u∗) = δ. (231)\nAccording to (67a), we have F̃(x′) ≤ 2F̃(x0). By Lemma D.2, we have d(u∗,x′) < [ 23δ, δ], which contradicts (231).\nCase 2: (67b) holds. Define λ′ = arg min\nλ∈R,d(xt+λ∆t ,u∗)≤δ ψ(xt,∆t; λ). (232)\nBy the induction hypothesis, d(xt,u∗) ≤ δ, thus 0 lies in the feasible region of the optimization problem in (232), which implies\nψ(xt,∆t; λ′) ≤ ψ(xt,∆t; 0) (65b) = F̃(xt). (233)\nDefine x′ = xt + λ′∆t, then the feasibility of λ′ for the optimization problem in (232) implies δ ≥ d(x′,u∗). Since d(x,u∗) is a continuous function over x and d(x′,u∗) ≤ δ (230) < d(xt+1,u∗), there must exist some x′′ = (1 − )xt+1 + x′= xt + (1 − + λ′)∆t, ∈ [0, 1] such that\nd(x′′,u∗) = δ. (234)\nThen we have\nF̃(x′′) (65b) ≤ ψ(xt,∆t; 1 − + λ′)\n(65a) ≤ (1 − )ψ(xt,∆t; 1) + ψ(xt,∆t; λ′)\n(223),(233) ≤ F̃(xt) (222) ≤ 2F̃(x0).\nAgain we apply Lemma D.2 to obtain d(u∗,x′′) < [ 23δ, δ], which contradicts (234).\nCase 3: (67c) holds. By (66) and the fact δ0 = δ/6 we get d(x0,u∗) ≤ δ/6. Then we have\nd(xt+1,u∗) ≤ d(xt+1,x0) + d(x0,u∗) (67c) ≤ 5\n6 δ + 1 6 δ = δ,\nwhich contradicts (230).\nIn all three cases we have arrived at a contradiction, thus the assumption (228) does not hold, which finishes the induction step for t + 1. Therefore, (224) holds for all t."
    }, {
      "heading" : "D.4 Proof of Claim 5.3",
      "text" : "The sequence {xt} generated by Algorithm 1 with either restricted Armijo rule or restricted line search satisfies (67c) because the sequence F̃(xt) is decreasing and the requirement d(xt,x0) ≤ 5δ/6 is enforced throughout computation.\nAlgorithm 2 and Algorithm 3 satisfy (67b) since all of them perform exact minimization of a convex upper bound of the objective function along some directions. Note that xt should be understood as the produced solution after t “iterations” (one block of variables is updated in one “iteration”). In contrast, (Xk,Yk) defined in these algorithms is the produced solution after k “loops” (all variables are updated once in one “loop”). For (Xk,Yk) generated by Algorithm 2, we define x2k = (Xk,Yk),x2k+1 = (Xk+1,Yk) and ψ(xt,∆t; λ) = F̃(xt + λ∆t), then ψ satisfies (65) and {xt}∞t=0 = {(Xk,Yk), (Xk+1,Yk)}∞k=0 satisfies (67b). Similarly, for (Xk,Yk) generated by Algorithm 3, define\nx(m+n)k+i = (X (1) k+1, . . . , X (i−1) k+1 , X (i), X(i+1)k , . . . , X (m) k ,Yk),\ni = 1, . . . ,m,\nx(m+n)k+m+ j = (Xk+1,Y (1) k+1, . . . ,Y ( j−1) k+1 ,Y ( j),Y ( j+1)k , . . . ,Y (m) k ),\nj = 1, . . . , n,\nand ψ(xt,∆t; λ) = F̃(xt + λ∆t) + λ0‖λ∆t‖2/2, then ψ satisfies (65) and {xt}∞t=0 satisfies (67b).\nWe then show that Algorithm 1 with constant stepsize η < η̄1 satisfies (67a) for some η̄1 when Ω satisfies (29). We prove by induction on t. Define x−1 = x0, then (67a) holds for t = 0. Assume (67a) holds for t − 1, i.e., F̃(xt−1 + λ∆t−1) ≤ 2F̃(x0),∀λ ∈ [0, 1], where ∆t = xt − xt−1. In particular, we have F̃(xt) ≤ 2F̃(x0), which together with the assumption that Ω satisfies (29) leads to (by Proposition (D.1))\nxt ∈ K1 ∩ K2.\nThus max{‖Xt‖F , ‖Yt‖F} ≤ βT , ‖X(i)t ‖ ≤ β1,∀i, and ‖Y ( j) t ‖ ≤ β2,∀ j. Then we have\n‖∇X F̃(xt)‖F = ‖∇XF(xt) + ∇XG(xt)‖F\n≤ ‖PΩ(XtYTt − M)Yt‖F + ∥∥∥∥∥∥∥ρ m∑ i=1 G′0( 3‖X(i)t ‖2 2β21 ) 3X̄(i)t β21 ∥∥∥∥∥∥∥ F\n+ ∥∥∥∥∥∥ρG′0(3‖Xt‖2F2β2T )3Xtβ2T ∥∥∥∥∥∥ F\n≤ ‖PΩ(XtYTt − M)‖F‖Yt‖F + 3ρ‖Xt‖F β21 + 3ρ‖Xt‖F β2T\n≤ √\nF̃(xt)βT + 6ρ‖Xt‖F β21\n≤ √\n2F̃(x0)βT + 6ρβT β21 ,\nwhere in the second inequality we use G′0( 3‖X(i)t ‖2\n2β21 ) ≤ G′0( 3 2 ) = 1 and G ′ 0( 3‖X‖2F 2β2T ) ≤ G′0( 3 2 ) = 1. Assume\nη̄1 ≤ 1\n4β2T . (235)\nRecall that η ≤ η̄1, thus we have\n‖Xt+1‖F ≤ ‖Xt‖F + η‖∇X F̃(xt)‖F\n≤ βT + 1\n4β2T √2F̃(x0)βT + 6ρβT β21  (221) ≤ βT +\n1 4βT √2pδ0 + 6ρ β21  , c1. (236)\nBy a similar argument, we can prove ‖Yt+1‖F ≤ c1, thus xt+1 = (Xt+1,Yt+1) ∈ Γ(c1) (recall the definition of Γ(·) in (20) is Γ(β) = {(X,Y) | ‖X‖F ≤ β, ‖Y‖F ≤ β}). Since (Xt,Yt) ∈ Γ(βT ) ⊆ Γ(c1) and Γ(c1) is a convex set, we have that the line segment connecting xt and xt+1, denoted as [xt,xt+1], lies in Γ(c1). Then by Claim 2.1 we have that ∇F̃ is Lipschitz continuous in [xt,xt+1] with Lipschitz constant\nL1 = L(c1) = 4c21 + 54ρ c21 β41 ≥ L(βT ) ≥ 4β2T , (237)\nwhere the last inequality is due to the fact c1 ≥ βT . Define (note c1 is defined by (236))\nη̄1 , 1 L1 = 1\n4c21 + 54ρ c21 β41\n, (238)\nthen η̄1 ≤ 1L(βT ) ≤ 1 4β2T = 14β2T , which is consistent with (235).\nIt follows from a classical descent lemma (see, e.g., [50, Prop. A.24]) that\nF̃(xt − λη∇F̃(xt))\n≤ F̃(xt) − 〈λη∇F̃(xt),∇F̃(xt)〉 + L1 2 ‖λη∇F̃(xt)‖2 = F̃(xt) + ‖∇F̃(xt)‖2( L1 2 λ2η2 − λη) ≤ F̃(xt) − λη\n2 ‖∇F̃(xt)‖2\n≤ F̃(xt) ≤ 2F̃(x0), ∀ λ ∈ [0, 1],\n(239)\nwhere the second inequality follows from the fact that λη ≤ η ≤ η̄1 = 1/L1. This finishes the induction step (note that ∆t = xt+1 − xt = −η∇F̃(xt)), thus (67a) is proved.\nFinally, we show that Algorithm 4 (SGD) satisfies (67a) with xt = (Xk,Yk) representing the produced solution after the t-th loop, provided that Ω satisfies (29). Denote N = |Ω| + m + n + 2 and xk,i = (Xk,i,Yk,i), i = 1, . . . ,N. We prove (67a) by induction on t. Define x−1 = x0, then (67a) holds for t = 0. Assume (67a) holds for 0, 1, . . . t−1, i.e., F̃(xk + λ∆k) ≤ 2F̃(x0),∀λ ∈ [0, 1], where ∆k = xk+1 − xk, 0 ≤ k ≤ t − 1. In particular, we have F̃(xt) ≤ 2F̃(x0), which together with the assumption that Ω satisfies (29) leads to (by Proposition (D.1))\nxt ∈ K1 ∩ K2. (240)\nNow we show that there exist constants c1,i, c2,i, i = 0, 1, . . . ,N (independent of t) so that\nmax{‖Xt,i‖F ,‖Yt,i‖F} ≤ c1,i, (241a) max{‖∇X fi+1(xt,i)‖F , ‖∇Y fi+1(xt,i−1)‖F} ≤ c2,i. (241b)\nWe prove (241) by induction on i. When i = 0, since by (240) we have max{‖Xt,0‖F , ‖Yt,0‖F} = max{‖Xt‖F , ‖Yt‖F} ≤ βT , thus (241a) holds for c1,0 = βT .\nSuppose (241a) holds for i, we prove (241b) holds for i with suitably chosen c2,i. Note that fi+1 can be one of the five different functions in (26). When fi+1 equals some F jl, we have\n‖∇X fi+1(xt,i)‖F = ‖∇XF j,l(xt,i)‖F = |(X( j)t,i ) T Y (l)t,i − M jl|‖Y (l) t,i ‖\n≤ (‖Xt,i‖F‖Yt,i‖F + Mmax)‖Yt,i‖F ≤ (c21,i + Mmax)c1,i.\nWhen fi+1(X,Y) equals some G1 j(X), we have (see (24) for the expression of ∇XG1 j)\n‖∇X fi+1(xt,i)‖F = ‖∇XG1 j(Xt,i)‖F = ρG′0( 3‖X( j)t,i ‖2 2β21 ) 3‖X( j)t,i ‖ β21\n≤ ρG′0( 3c21,i 2β21 ) 3c1,i β21 ≤ ρG′0( 3c21,i 2β2T ) 3c1,i β2T .\nWhen fi+1(X,Y) equals some G3(X), we have\n‖∇X fi+1(xt,i)‖F = ‖∇XG3(Xt,i)‖F = ρG′0( 3‖Xt,i‖2F 2β2T ) 3‖Xt,i‖F β2T\n≤ ρG′0( 3c21,i 2β2T ) 3c1,i β2T .\nWhen fi+1(X,Y) equals some G2 j(Y) or G4(Y) that only depend on Y , we have ∇X fi+1(xt,i) = 0. Let\nc2,i , max (c21,i + Mmax)c1,i, ρG′0(3c21,i2β2T )3c1,iβ2T  ,\nthen no matter what kind of function fi+1 is, we always have ‖∇X fi+1(xt,i)‖F ≤ c2,i. Similarly, ‖∇Y fi+1(xt,i)‖F ≤ c2,i. Thus (241b) holds for i.\nSuppose (241b) holds for i − 1, we prove that (241a) holds for i with suitably chosen c1,i. In fact,\n‖Xt,i‖F = ‖Xt,i−1 − ηt∇X fi(xt,i−1)‖F ≤ ‖Xt,i−1‖F + ηt‖∇X fi(xt,i−1)‖F ≤ c1,i−1 + η̄c2,i−1,\nthus (241a) holds for c1,i = c1,i−1 + η̄c2,i−1. This finishes the induction proof of (241).\nIn Claim 2.1, we have proved that ∇F̃ is Lipschitz continuous with Lipschitz constant L(β0) = 4β0 + 54ρ β20 β41\nin the set Γ(β0) (the definition of Γ(·) is given in (20)). By a similar argument (or set irrelevant rows of X,Y,U,V to zero in the proof of Claim (2.1)), we can prove that each ∇ fi is also Lipschitz continuous with Lipschitz constant L(β0) = 4β0 + 54ρ\nβ20 β41 in the set Γ(β0). Then we have\n‖∇ fi(xt,i−1) − ∇ fi(xt)‖F ≤ c′i−1‖xt,i−1 − xt‖F , i = 1, . . . ,N, (242)\nwhere c′i−1 = L(c1,i−1).\nNote that xt+1 = xt + ∑N i=1(xt,i − xt,i−1) = xt − ηt ∑N\ni=1 ∇ fi(xt,i−1). We can express SGD as an approximate gradient descent method: xt+1 = xt − ηt(∇F̃(xt) + wt), (243) where the error\nwt = N∑\ni=1\n∇ fi(xt,i−1) − ∇F̃(xt) = N∑\ni=1\n(∇ fi(xt,i−1) − ∇ fi(xt)).\nFollowing the analysis in [61, Lemma 1], we can bound each term ∇ fi(xt,i−1) − ∇ fi(xt) as\n‖∇ fi(xt,i−1) − ∇ fi(xt)‖F (242) ≤ c′i−1‖xt,i−1 − xt‖F\n= ηtc′i−1‖ i−1∑ l=1 ∇ fl(xt,l−1)‖F (241b) ≤ ηtc′i−1 i−1∑ l=1 √ 2c2,l.\nPlugging this inequality for i = 1, . . . ,N into the expression of wt, we obtain an upper bound of the error wt:\n‖wt‖F ≤ ηtc0, (244)\nwhere c0 , ∑N i=1(c ′ i−1 ∑i−1 l=1 √ 2c2,l) is a constant.\nApplying (241a) for i = N, we get max{‖Xt+1‖F , ‖Yt+1‖F} ≤ c1,N , thus xt+1 ∈ Γ(c1,N). Since xt ∈ Γ(βT ) ⊆ Γ(c1,N) and Γ(c1,N) is a convex set, we have that the line segment connecting xt and xt+1 lies in Γ(c1,N). Then by Claim 2.1 we have that ∇F̃ is Lipschitz continuous over this line segment with Lipschitz constant L′ = L(c1,N). It follows from a classical descent lemma (see, e.g., [50, Prop. A.24]) that\nF̃(xt+1) ≤ F̃(xt) + 〈xt+1 − xt,∇F̃(xt)〉 + L′\n2 ‖xt+1 − xt‖2F .\nUsing the expression (243), the above relation becomes\nF̃(xt+1) − F̃(xt) ≤ −ηt〈∇F̃(xt) + wt,∇F̃(xt)〉 + L′\n2 η2t ‖∇F̃(xt) + wt‖2F . (245)\nPlugging\n−ηt〈wt,∇F̃(xt)〉 ≤ ηt‖wt‖F‖∇F̃(xt)‖F (244) ≤ η2t c0‖∇F̃(xt)‖F ≤\n1 2 η2t c0(1 + ‖∇F̃(xt)‖2F)\nand 1 2 ‖∇F̃(xt) + wt‖2F ≤ ‖∇F̃(xt)‖2F + ‖wt‖2F (244) ≤ ‖∇F̃(xt)‖2F + η2t c20\ninto (245), we get\nF̃(xt+1) − F̃(xt)\n≤ −ηt‖∇F̃(xt)‖2F + 1 2 η2t c0(1 + ‖∇F̃(xt)‖2F) + L′η2t (‖∇F̃(xt)‖2F + η2t c20) = ( 1 2 η2t c0 + η 2 t L ′ − ηt)‖∇F̃(xt)‖2F + η2t ( 1 2 c0 + L′η2t c 2 0).\n(246)\nPick η̄ ,\n1 c0 + 2L′ .\nSince ηt ≤ η̄, we have 12η2t c0 + η2t L′ − ηt ≤ −ηt/2 and L′η2t c20 ≤ L′c20 1 (c0+2L′)2 ≤ c08 (the last inequality follows from (c0 + 2L′)2 ≥ 8c0L′). Plugging these two inequalities into (246), we obtain\nF̃(xt+1) − F̃(xt) ≤ η2t c0.\nBy the same argument we can prove\nF̃(xk+1) − F̃(xk) ≤ η2kc0, k = 0, 1, . . . , t.\nSumming up these inequalities, we get\nF̃(xt+1) ≤ F̃(x0) + t∑\nk=0\nη2kc0 ≤ F̃(x0) + ηsumc0.\nwhere the last inequality follows from the assumption ∑∞\nk=0 η 2 k ≤ ηsum. Pick\nηsum , F̃(x0)\nc0 ,\nthe above relation becomes F̃(xt+1) ≤ 2F̃(x0).\nBy a similar argument, we can prove\nF̃(xt + λ(xt+1 − xt)) ≤ 2F̃(x0), ∀ λ ∈ [0, 1],\nwhich completes the induction. Thus we have proved that Algorithm 4 (SGD) satisfies (67a) with suitably chosen η̄ and ηsum."
    }, {
      "heading" : "D.5 Proof of Claim 5.1",
      "text" : "For Algorithm 1 with constant stepsize η < η̄1 (defined in (238)), since the objective value F̃(xt) is decreasing, we have F̃(xt) ≤ F̃(x0). By Proposition D.1 this implies that the algorithm generates a sequence in K1 ∩ K2. By Claim 2.1 and the fact K2 = Γ(βT ) (see the definitions of K2 in (30) and the definition of Γ(·) in (20)), ∇F̃ is Lipschitz continuous with Lipschitz constant L(βT ) over the set K2. According to [50, Proposition 1.2.3], each limit point of the sequence generated by Algorithm 1 with constant stepsize η < η̄1 (238) ≤ 2/L(βT ) is a stationary point of problem (P1).\nWe then consider Algorithm 1 with stepsize chosen by the restricted Armijo rule. The proof of [50, Proposition 1.2.1] for the standard Armijo rule can not be directly applied, and some extra effort is needed. For the restricted Armijo rule, the procedure of picking the stepsize ηk can be viewed as a two-phase approach. In the first phase, we find the smallest nonnegative integer so that the distance requirement is fulfilled, i.e.\ni1 , min{i ∈ Z+ | d(xk(ξis0),x0) ≤ 5 6 δ}, (247)\nwhere Z+ denotes the set of nonnegative integers, and let s̄k = ξi1 s0. Since\nd(xk(0), s0) = d(xk−1,x0) ≤ 2 3 δ, (248)\n(according to Proposition 5.1 and Claim 5.3), such an integer i1 must exist. In the second phase, find the smallest nonnegative integer so that the reduction requirement is fulfilled, i.e.\ni2 , min{i ∈ Z+ | F̃(xk(ξi s̄k)) ≤ F̃(xk−1) − σξi s̄k‖∇F̃(xk−1)‖2F}, (249)\nand let ηk = ξi2 s̄k = ξi1+i2 s0.\nNote that the second phase follows the same procedure as the standard Armijo rule (see (1.11) of [50]). Hence the difference between the standard Armijo rule and the restricted Armijo rule can be viewed as the following: in each iteration the former starts from a fixed initial stepsize s while the latter starts from a varying initial stepsize s̄k. We notice that the proof of [50, Proposition 1.2.1] does not require the initial stepsizes to be constant, but rather the\nfollowing property: if the final stepsize ηk goes to zero for a subsequence k ∈ K , then for large enough k ∈ K the initial stepsize must be reduced at least once (see the remark after (1.17) in [50]). This property also holds when the initial stepsize is lower bounded (asymptotically). In the following, we will prove that for the restricted Armijo rule the initial stepsize s̄k is lower bounded (asymptotically), and then show how to apply the proof of [50, Proposition 1.2.1] to the restricted Armijo rule.\nWe first prove that the sequence {s̄k} is lower bounded (asymptotically), i.e.\nlim inf k→∞ s̄k > 0. (250)\nAssume the contrary that lim infk→∞ s̄k = 0, i.e. there exists a subsequence {s̄k}k∈K that converges to zero. Since s0 is a fixed scalar, we can assume s̄k < s0,∀k ∈ K , thus the corresponding i1 > 0 for all k ∈ K . By the definition of i1 in (247), we know that i1 − 1 does not satisfy the distance requirement; in other words, we have\nd(xk(ξ−1 s̄k),x0) > 5 6 δ.\nDenote gk−1 , ∇F̃(xk−1), then the above relation becomes\n5 6 δ < d(xk−1 − ξ−1 s̄kgk−1,x0) ≤ d(xk−1, x0) + ξ−1 s̄k‖gk−1‖F\n(248) ≤ 2\n3 δ + ξ−1 s̄k‖gk−1‖F ,\nimplying 1 6 ξδ ≤ s̄k‖gk−1‖F . Since 16ξδ is a constant and {s̄k}k∈K converges to zero, the above relation implies that {‖gk−1‖F}k∈K goes to infinity. However, it is easy to verify that ‖gk−1‖F = ‖∇F̃(xk−1)‖F is bounded above by a universal constant when ‖xk−1‖F ≤ βT (note that ‖xk−1‖F ≤ βT holds due to Proposition 5.1 and Claim 5.3)), which is a contradiction. Therefore, (250) is proved.\nNow we prove that each limit point of the sequence {xk} generated by Algorithm 1 with restricted Armijo rule is a stationary point. Assume the contrary that there exists a limit point x̄ with ∇F̃(x̄) , 0, and suppose the subsequence {xk}k∈K converges to x̄. By the same argument as that for [50, Proposition 1.2.1], we can prove that the subsequence of final stepsizes {ηk}k∈K → 0 (see the inequality before (1.17) in [50]). Since {s̄k} is lower bounded (asymptotically), we must have that s̄k > ηk, ∀ k ∈ K , k ≥ k̄ for large enough k̄. Thus the corresponding i2 > 0 for all k ∈ K , k ≥ k̄. By the definition of i2 in (249), we know that i2 − 1 does not satisfy the reduction requirement; in other words, we have F̃(xk(ηkξ−1)) > F̃(xk−1) − σηkξ−1‖∇F̃(xk−1)‖2F , or equivalently,\nF̃(xk−1) − F̃(xk−1 − ηkξ−1∇F̃(xk−1))) < σηkξ −1‖∇F̃(xk−1)‖2F , ∀ k ∈ K , k ≥ k̄.\nThis relation is the same as (1.17) in [50] (except that (1.17) in [50] considers a more general descent direction), and the rest of the proof is also the same as [50] and is omitted here.\nFor Algorithm 1 with stepsize chosen by the restricted line search rule, since it “gives larger reduction in cost at each iteration” than the restricted Armijo rule, it “inherits the convergence properties” of the restricted Armijo rule (as remarked in the last paragraph of the proof of [50, Proposition 1.2.1]). The rigorous proof is similar to that in the second last paragraph of the proof of [50, Proposition 1.2.1]) and is omitted here.\nAlgorithm 2 is a two-block BCD method to solve problem (P1). According to [59, Corollary 2], each limit point of the sequence generated by Algorithm 2 is a stationary point of problem (P1).\nAlgorithm 3 belongs to the class of BSUM methods [55]. According to Proposition D.1, the level set X0 = {x | F̃(x) ≤ F̃(x0)} is a subset of the bounded set K1 ∩ K2, thus X0 is bounded. Moreover, X0 is a closed set, thus X0 is compact. It is easy to verify that the objective function of each subproblem in Algorithm 3 is a convex tight upper bound of F̃(x) (more precisely, satisfies Assumption 2 in [55]). It is also obvious that the objective function of each subproblem is strongly convex, thus each subproblem of Algorithm 3 has a unique solution. Based on these facts, it follows from [55, Theorem 2] that each limit point of the sequence generated by Algorithm 3 is a stationary point.\nAlgorithm 4 is a SGD method (or more precisely, incremental gradient method) with a specific stepsize rule. According to (243) and (244) in Appendix (D.4), Algorithm 4 can be viewed as an approximate gradient descent method with bounded error. By [62, Proposition 1], each limit point of the sequence generated by Algorithm 4 is a stationary point."
    }, {
      "heading" : "E Proof of Lemma 3.3",
      "text" : "We will prove a statement that is stronger than Lemma 3.1: with probability at least 1 − 1/n4, for any (X,Y) ∈ K1 ∩ K2 ∩ K(δ) and U,V defined in Table 7, we have\n〈∇X F̃(X,Y), X − U〉 + 〈∇Y F̃(X,Y),Y − V〉 ≥ p 4\nd2 + 2 √ ρ Σmin d √ G(X,Y), (251)\nwhere d = ‖M − XYT ‖F .\nWe have already proved (37a), i.e. with probability at least 1 − 1/n4,\nφF = 〈∇XF, X − U〉 + 〈∇Y F,Y − V〉 ≥ p 4 d2.\nIt remains to prove a bound on φG, which is stronger than the bound φG ≥ 0. Note that φF depends on the observed set Ω, thus the bound on φF holds with high probability; in contrast, φG does not depend on Ω, thus the bound on φG always holds.\nClaim E.1 For any (X,Y) ∈ K1 ∩ K2 ∩ K(δ) and U,V defined in Table 7, we have\nφG = 〈∇XG, X − U〉 + 〈∇YG,Y − V〉 ≥ 2 √ ρ Σmin d √ G(X,Y). (252)\nProof of Claim E.1: By the definition of G in (13), G(X,Y) = ρ( ∑ i G1i(X) + G2(X) + ∑\nj G3 j(Y) + G4(Y)), where the component functions\nG1i(X) = G0 3‖X(i)‖2 2β21  , G2(X) = G0 3‖X‖2F 2β2T  , G3 j(Y) , G0\n3‖Y ( j)‖2 2β22  , G4(Y) , G0 3‖Y‖2F 2β2T  . (253)\nBy the expressions of ∇XG,∇YG in (24), we have\nφG = 〈∇XG, X − U〉 + 〈∇YG,Y − V〉 =\nρ m∑ i=1 G′0( 3‖X(i)‖2 2β21 ) 3 β21 〈X(i), X(i) − U(i)〉 + ρG′0( 3‖X‖2F 2β2T ) 3 β2T 〈X, X − U〉\n+ρ n∑ j=1 G′0( 3‖Y ( j)‖2 2β22 ) 3 β22 〈Y ( j),Y ( j) − V ( j)〉 + ρG′0( 3‖Y‖2F 2β2T ) 3 β2T 〈Y,Y − V〉,\n(254)\nwhere G′0(z) = I[1,∞](z)2(z − 1) = 2 √ G0(z).\nFirstly, we prove\nh1i , G′0( 3‖X(i)‖2\n2β21 ) 3 β21 〈X(i), X(i) − U(i)〉 ≥ 1 2\n√ G1i(X), ∀ i, (255a)\nh3 j , G′0( 3‖Y ( j)‖2\n2β22 ) 3 β22 〈Y ( j),Y ( j) − V ( j)〉 ≥ 1 2\n√ G3 j(Y), ∀ j. (255b)\nWe only need to prove (255a); the proof of (255b) is similar. We consider two cases.\nCase 1: ‖X(i)‖2 ≤ 2β 2 1 3 . Note that 3‖X(i)‖2 2β21 ≤ 1 implies G0( 3‖X (i)‖2 2β21 ) = G′0( 3‖X(i)‖2 2β21 ) = 0, thus h1i = G1i = 0, in which\ncase (255a) holds.\nCase 2: ‖X(i)‖2 > 2β 2 1\n3 . By Corollary 4.1 and the fact that β 2 1 = β 2 T 3µr m , we have\n‖U(i)‖2 ≤ 3rµ 2m β2T (15) = 3 4 2β21 3 < 3 4 ‖X(i)‖2. (256)\nAs a result, √\n3 2 〈X(i), X(i)〉 =\n√ 3\n2 ‖X(i)‖‖X(i)‖ > ‖X(i)‖‖U(i)‖ ≥ 〈X(i),U(i)〉, which implies 〈X(i), X(i) − U(i)〉 ≥ (1 − √\n3 2 )‖X(i)‖2 > (1 −\n√ 3 2 ) 2 3β 2 1 > 1 12β 2 1. Combining this inequality with the fact that G ′ 0( 3‖X(i)‖2 2β21\n) = 2 √ G0 (\n3‖X(i)‖2 2β21\n) =\n2 √ G1i(X), we get (255a).\nSecondly, we prove\nh2 + h4 ≥ 2d\nΣmin\n( √ G2(X) + √ G4(Y) ) ,\nwhere h2 , G′0( 3‖X‖2F 2β2T ) 3 β2T 〈X, X − U〉,\nh4 , G′0( 3‖Y‖2F 2β2T ) 3 β2T 〈Y,Y − V〉.\n(257)\nWithout loss of generality, we can assume ‖Y‖F ≥ ‖X‖F , and we will apply Corollary 4.1 to prove (257). If ‖Y‖F < ‖X‖F , we can apply a symmetric result of Corollary 4.1 to prove (257). We consider three cases.\nCase 1: ‖X‖F ≤ ‖Y‖F ≤ √ 2 3βT . In this case G0( 3‖X‖2F 2β2T ) = G′0( 3‖X‖2F 2β2T ) = G0( 3‖Y‖2F 2β2T ) = G′0( 3‖Y‖2F 2β2T\n) = 0, which implies h2 = h4 = G2(X) = G4(Y) = 0, thus (257) holds.\nCase 2: ‖X‖F ≤ √ 2 3βT < ‖Y‖F . Then we have 3‖X‖2F 2β2T ≤ 1, which implies h2 = 0 = G2(X). By (51d) in Corollary\n4.1 we have ‖V‖F ≤ (1− dΣmin )‖Y‖F , which implies (1− d Σmin )〈Y,Y〉 = (1− d Σmin )‖Y‖2F ≥ ‖Y‖F‖V‖F ≥ 〈Y,V〉. This further\nimplies 〈Y,Y −V〉 ≥ d Σmin ‖Y‖2F ≥ dΣmin 2β2T 3 . Combined with the fact that G ′ 0( 3‖Y‖2F 2β2T\n) = 2 √\nG0( 3‖Y‖2F 2β2T\n) = 2 √ G4(Y), we get\nh4 = G′0( 3‖Y‖2F 2β2T ) 3 β2T 〈Y,Y − V〉\n≥ 2 √\nG4(Y) 3 β2T d Σmin 2β2T 3 = 4d Σmin\n√ G4(Y).\nThus h2 + h4 = h4 ≥ 4dΣmin √ G4(Y) = 4dΣmin (√ G4(Y) + √ G2(X) ) ≥ 2d Σmin (√ G4(Y) + √ G2(X) ) .\nCase 3: √\n2 3βT < ‖X‖F ≤ ‖Y‖F . Since ‖Y‖F ≥ ‖X‖F , we have G4(Y) = G0 ( 3‖Y‖2F 2β2T ) ≥ G0 ( 3‖X‖2F 2β2T ) = G2(X). By\nCorollary 4.1, we have ‖U‖F ≤ ‖X‖F and ‖V‖F ≤ (1 − dΣmin )‖Y‖F . Similar to the argument in Case 2 we can prove h2 ≥ 0, h4 ≥ 4dΣmin √ G4(Y); thus h2 + h4 ≥ 4dΣmin √ G4(Y) ≥ 2dΣmin (√ G4(Y) + √ G2(X) ) .\nIn all three cases, we have proved (257), thus (257) holds.\nWe conclude that for U,V defined in Table 7,\nφG (254) = ρ ∑ i h1i + ∑ j h3 j + h2 + h4  (255),(257) ≥ ρ ( 1 2 ∑ i √ G1i(X) + 1 2 ∑ j √ G2 j(Y)\n+ 2d\nΣmin\n√ G2(X) +\n2d Σmin\n√ G4(Y) ) ≥ ρ 2d\nΣmin ∑ i √ G1i(X) + ∑ j √ G2 j(Y) + √ G2(X) + √ G4(Y)  ≥ ρ 2d\nΣmin √∑ i G1i(X) + ∑ j G2 j(Y) + G2(X) + G4(Y)\n= ρ 2d\nΣmin\n√ 1 ρ G(X,Y) = 2 √ ρ Σmin d √ G(X,Y).\n(258)\nwhich finishes the proof of Claim E.1.\nLet us come back to the proof of Lemma 3.3. The rest of the proof is just algebraic computation. According to (251), we have\np 4\nd2 + 2 √ ρ Σmin d √ G(X,Y)\n≤ 〈∇X F̃(X,Y), X − U〉 + 〈∇Y F̃(X,Y),Y − V〉 ≤ (‖∇X F̃(X,Y)‖F + ‖∇Y F̃(X,Y)‖F) max{‖X − U‖F , ‖Y − V‖F} (51b) ≤ √ 2 √ ‖∇X F̃(X,Y)‖2F + ‖∇Y F̃(X,Y)‖2F 17 2 √ r βT Σmin d = ‖∇F̃(X,Y)‖F 17 √\n2\n√ r βT\nΣmin d.\nEliminating a factor of d from both sides and taking square, we get\n‖∇F̃(X,Y)‖2F 289 2 r β2T\nΣ2min ≥\n( p 4 d + 2 √ ρ\nΣmin\n√ G(X,Y) )2 ≥ pd 2\n16 + 4ρ Σ2min G(X,Y).\n(259)\nBy the definition of βT in (15), we have\nr β2T\nΣ2min = r CT rΣmax Σ2min = CT r2κ Σmin .\nAccording to Claim 3.1, we have\npd2 = p‖M − XYT ‖2F ≥ 1 2 ‖PΩ(M − XYT )‖2F = F(X,Y).\nBy the definition of ρ in (17) and the definition of δ0 in (16), we have\n4ρ Σ2min = 4 Σ2min 8pδ20 = 32p Σ2min 1 36\nΣ2min\nC2dr 3κ2\n= 8 9 1 C2dr 3κ2 p.\nSubstituting the above three relations into (259), we get (when Cd ≥ 32/3)\n‖∇F̃(X,Y)‖2F 289 2 CT r2κ Σmin ≥ p 32 F(X,Y) + 8 9 1 C2dr 3κ2 pG(X,Y)\n≥ 8 9 1 C2dr 3κ2 p(F(X,Y) + G(X,Y)) = 8 9 1 C2dr 3κ2 pF̃(X,Y).\nThis can be further simplified to\n‖∇F̃(X,Y)‖2F ≥ Σmin\nCgr5κ3 pF̃(X,Y),\nwhere the numerical constant Cg = 260116 CT C 2 d. This finishes the proof of Lemma 3.3."
    } ],
    "references" : [ {
      "title" : "Matrix factorization techniques for recommender systems",
      "author" : [ "Yehuda Koren", "Robert Bell", "Chris Volinsky" ],
      "venue" : "Computer, vol. 42, no. 8, pp. 30–37, 2009.",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Recovering the missing components in a large noisy low-rank matrix: Application to SFM",
      "author" : [ "Pei Chen", "David Suter" ],
      "venue" : "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 26, no. 8, pp. 1051–1063, 2004.",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Interior-point method for nuclear norm approximation with application to system identification",
      "author" : [ "Zhang Liu", "Lieven Vandenberghe" ],
      "venue" : "SIAM Journal on Matrix Analysis and Applications, vol. 31, no. 3, pp. 1235–1256, 2009.",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Exact matrix completion via convex optimization",
      "author" : [ "Emmanuel J Candès", "Benjamin Recht" ],
      "venue" : "Foundations of Computational mathematics, vol. 9, no. 6, pp. 717–772, 2009.",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "The power of convex relaxation: Near-optimal matrix completion",
      "author" : [ "Emmanuel J Candès", "Terence Tao" ],
      "venue" : "IEEE Transactions on Information Theory, vol. 56, no. 5, pp. 2053–2080, 2010.",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Recovering low-rank matrices from few coefficients in any basis",
      "author" : [ "David Gross" ],
      "venue" : "IEEE Transactions on Information Theory, vol. 57, no. 3, pp. 1548–1566, 2011.",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "A simpler approach to matrix completion",
      "author" : [ "Benjamin Recht" ],
      "venue" : "The Journal of Machine Learning Research, vol. 12, pp. 3413–3430, 2011.",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Matrix completion with noise",
      "author" : [ "Emmanuel J Candès", "Yaniv Plan" ],
      "venue" : "Proceedings of the IEEE, vol. 98, no. 6, pp. 925–936, 2010.",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Restricted strong convexity and weighted matrix completion: Optimal bounds with noise",
      "author" : [ "Sahand Negahban", "Martin J Wainwright" ],
      "venue" : "The Journal of Machine Learning Research, vol. 13, no. 1, pp. 1665–1697, 2012.",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "A singular value thresholding algorithm for matrix completion",
      "author" : [ "Jian-Feng Cai", "Emmanuel J Candès", "Zuowei Shen" ],
      "venue" : "SIAM Journal on Optimization, vol. 20, no. 4, pp. 1956–1982, 2010.",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 1956
    }, {
      "title" : "Fixed point and bregman iterative methods for matrix rank minimization",
      "author" : [ "Shiqian Ma", "Donald Goldfarb", "Lifeng Chen" ],
      "venue" : "Mathematical Programming, vol. 128, no. 1-2, pp. 321–353, 2011.",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "An accelerated proximal gradient algorithm for nuclear norm regularized linear least squares problems",
      "author" : [ "Kim-Chuan Toh", "Sangwoon Yun" ],
      "venue" : "Pacific Journal of Optimization, vol. 6, no. 615-640, pp. 15, 2010.",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Fast global convergence of gradient methods for high-dimensional statistical recovery",
      "author" : [ "Alekh Agarwal", "Sahand Negahban", "Martin Jordan Wainwright" ],
      "venue" : "The Annals of Statistics, vol. 40, no. 5, pp. 2452–2482, 2012.",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "On the linear convergence of the proximal gradient method for trace norm regularization",
      "author" : [ "Ke Hou", "Zirui Zhou", "Anthony Man-Cho So", "Zhi-Quan Luo" ],
      "venue" : "Advances in Neural Information Processing Systems (NIPS), 2013, pp. 710–718.",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "A unified view of matrix factorization models",
      "author" : [ "Ajit P Singh", "Geoffrey J Gordon" ],
      "venue" : "Machine Learning and Knowledge Discovery in Databases, pp. 358–373. Springer, 2008.",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Major components of the gravity recommendation system",
      "author" : [ "Gábor Takács", "István Pilászy", "Bottyán Németh", "Domonkos Tikk" ],
      "venue" : "ACM SIGKDD Explorations Newsletter, vol. 9, no. 2, pp. 80–83, 2007.",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Efficient algorithms for collaborative filtering",
      "author" : [ "Hulikal Keshavan" ],
      "venue" : "Ph.D. thesis, Stanford University,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2012
    }, {
      "title" : "Low-rank matrix completion using alternating minimization",
      "author" : [ "Prateek Jain", "Praneeth Netrapalli", "Sujay Sanghavi" ],
      "venue" : "Proceedings of the forty-fifth annual ACM symposium on Theory of computing (STOC). ACM, 2013, pp. 665–674.",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Understanding alternating minimization for matrix completion",
      "author" : [ "Moritz Hardt" ],
      "venue" : "2014 IEEE 55th Annual Symposium on Foundations of Computer Science (FOCS). IEEE, 2014, pp. 651–660.",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Fast matrix completion without the condition number",
      "author" : [ "Moritz Hardt", "Mary Wootters" ],
      "venue" : "Proceedings of The 27th Conference on Learning Theory (COLT), 2014, pp. 638–678.",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Large-scale parallel collaborative filtering for the netflix prize",
      "author" : [ "Yunhong Zhou", "Dennis Wilkinson", "Robert Schreiber", "Rong Pan" ],
      "venue" : "Algorithmic Aspects in Information and Management, pp. 337–348. Springer, 2008.",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Solving a low-rank factorization model for matrix completion by a nonlinear successive overrelaxation algorithm",
      "author" : [ "Zaiwen Wen", "Wotao Yin", "Yin Zhang" ],
      "venue" : "Mathematical Programming Computation, vol. 4, no. 4, pp. 333–361, 2012.",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Netflix update: Try this at home",
      "author" : [ "Simon Funk" ],
      "venue" : "http://sifter.org/ simon/journal/20061211.html.",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Improving regularized singular value decomposition for collaborative filtering",
      "author" : [ "Arkadiusz Paterek" ],
      "venue" : "Proceedings of KDD cup and workshop, 2007, vol. 2007, pp. 5–8.",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Large-scale matrix factorization with distributed stochastic gradient descent",
      "author" : [ "Rainer Gemulla", "Erik Nijkamp", "Peter J Haas", "Yannis Sismanis" ],
      "venue" : "Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, 2011, pp. 69–77.",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Parallel stochastic gradient algorithms for large-scale matrix completion",
      "author" : [ "Benjamin Recht", "Christopher Ré" ],
      "venue" : "Mathematical Programming Computation, vol. 5, no. 2, pp. 201–226, 2013.",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "A fast parallel sgd for matrix factorization in shared memory systems",
      "author" : [ "Yong Zhuang", "Wei-Sheng Chin", "Yu-Chin Juan", "Chih-Jen Lin" ],
      "venue" : "Proceedings of the 7th ACM Conference on Recommender Systems. ACM, 2013, pp. 249–256.",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Fast als-based matrix factorization for explicit and implicit feedback datasets",
      "author" : [ "István Pilászy", "Dávid Zibriczky", "Domonkos Tikk" ],
      "venue" : "Proceedings of the fourth ACM conference on Recommender systems. ACM, 2010, pp. 71–78.",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Scalable coordinate descent approaches to parallel matrix factorization for recommender systems",
      "author" : [ "Hsiang-Fu Yu", "Cho-Jui Hsieh", "Si Si", "Inderjit S Dhillon" ],
      "venue" : "ICDM, 2012, pp. 765–774.",
      "citeRegEx" : "29",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Matrix Completion via Nonconvex Factorization: Algorithms and Theory, Ph.D",
      "author" : [ "Ruoyu Sun" ],
      "venue" : "thesis, University of Minnesota,",
      "citeRegEx" : "30",
      "shortCiteRegEx" : "30",
      "year" : 2015
    }, {
      "title" : "Matrix completion from a few entries",
      "author" : [ "Raghunandan H Keshavan", "Andrea Montanari", "Sewoong Oh" ],
      "venue" : "IEEE Transactions on Information Theory, vol. 56, no. 6, pp. 2980–2998, 2010.",
      "citeRegEx" : "31",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Phase retrieval via wirtinger flow: Theory and algorithms",
      "author" : [ "Emmanuel Candès", "Xiaodong Li", "Mahdi Soltanolkotabi" ],
      "venue" : "arXiv preprint arXiv:1407.1065, 2014.",
      "citeRegEx" : "32",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Quantum state tomography via compressed sensing",
      "author" : [ "David Gross", "Yi-Kai Liu", "Steven T Flammia", "Stephen Becker", "Jens Eisert" ],
      "venue" : "arXiv preprint, http://arxiv.org/abs/0909.3304v1, 2009.",
      "citeRegEx" : "33",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Fast exact matrix completion with finite samples",
      "author" : [ "Prateek Jain", "Praneeth Netrapalli" ],
      "venue" : "arXiv preprint arXiv:1411.1087, 2014.",
      "citeRegEx" : "34",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Global convergence of stochastic gradient descent for some nonconvex matrix problems",
      "author" : [ "Christopher De Sa", "Kunle Olukotun", "Christopher Ré" ],
      "venue" : "arXiv preprint arXiv:1411.1134, 2014.",
      "citeRegEx" : "35",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Phase retrieval using alternating minimization",
      "author" : [ "Praneeth Netrapalli", "Prateek Jain", "Sujay Sanghavi" ],
      "venue" : "Advances in Neural Information Processing Systems (NIPS), 2013, pp. 2796–2804.",
      "citeRegEx" : "36",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "A general theory of concave regularization for high-dimensional sparse estimation problems",
      "author" : [ "Cun-Hui Zhang", "Tong Zhang" ],
      "venue" : "Statistical Science, vol. 27, no. 4, pp. 576–593, 2012.",
      "citeRegEx" : "37",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Regularized m-estimators with nonconvexity: Statistical and algorithmic theory for local optima",
      "author" : [ "Po-Ling Loh", "Martin Wainwright" ],
      "venue" : "Advances in Neural Information Processing Systems, 2013, pp. 476–484.",
      "citeRegEx" : "38",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Strong oracle optimality of folded concave penalized estimation",
      "author" : [ "Jianqing Fan", "Lingzhou Xue", "Hui Zou" ],
      "venue" : "The Annals of Statistics, vol. 42, no. 3, pp. 819–849, 2014.",
      "citeRegEx" : "39",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Truncated power method for sparse eigenvalue problems",
      "author" : [ "Xiao-Tong Yuan", "Tong Zhang" ],
      "venue" : "The Journal of Machine Learning Research, vol. 14, no. 1, pp. 899–925, 2013.",
      "citeRegEx" : "40",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Nonconvex statistical optimization: Minimax-optimal sparse pca in polynomial time",
      "author" : [ "Zhaoran Wang", "Huanran Lu", "Han Liu" ],
      "venue" : "arXiv preprint arXiv:1408.5352, 2014.",
      "citeRegEx" : "41",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Non-convex robust pca",
      "author" : [ "Praneeth Netrapalli", "UN Niranjan", "Sujay Sanghavi", "Animashree Anandkumar", "Prateek Jain" ],
      "venue" : "Advances in Neural Information Processing Systems, 2014, pp. 1107–1115.",
      "citeRegEx" : "42",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Statistical guarantees for the EM algorithm: From population to sample-based analysis",
      "author" : [ "Sivaraman Balakrishnan", "Martin Wainwright", "Bin Yu" ],
      "venue" : "arXiv preprint arXiv:1408.2156, 2014.",
      "citeRegEx" : "43",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "High dimensional expectation-maximization algorithm: Statistical optimization and asymptotic normality",
      "author" : [ "Zhaoran Wang", "Quanquan Gu", "Yang Ning", "Han Liu" ],
      "venue" : "arXiv preprint arXiv:1412.8729, 2014. 76",
      "citeRegEx" : "44",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Perturbation bounds in connection with singular value decomposition",
      "author" : [ "Per-Åke Wedin" ],
      "venue" : "BIT Numerical Mathematics, vol. 12, no. 1, pp. 99–111, 1972.",
      "citeRegEx" : "45",
      "shortCiteRegEx" : null,
      "year" : 1972
    }, {
      "title" : "Spectral techniques applied to sparse random graphs",
      "author" : [ "Uriel Feige", "Eran Ofek" ],
      "venue" : "Random Structures & Algorithms, vol. 27, no. 2, pp. 251–275, 2005.",
      "citeRegEx" : "46",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Coherent matrix completion",
      "author" : [ "Yudong Chen", "Srinadh Bhojanapalli", "Sujay Sanghavi", "Rachel Ward" ],
      "venue" : "Proceedings of The 31st International Conference on Machine Learning (ICML), 2014, pp. 674–682.",
      "citeRegEx" : "47",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Universal matrix completion",
      "author" : [ "Srinadh Bhojanapalli", "Prateek Jain" ],
      "venue" : "arXiv preprint arXiv:1402.2324, 2014.",
      "citeRegEx" : "48",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Non-linear programming via penalty functions",
      "author" : [ "Willard I Zangwill" ],
      "venue" : "Management science, vol. 13, no. 5, pp. 344–358, 1967.",
      "citeRegEx" : "49",
      "shortCiteRegEx" : null,
      "year" : 1967
    }, {
      "title" : "Nonlinear programming",
      "author" : [ "Dimitri P Bertsekas" ],
      "venue" : "1999.",
      "citeRegEx" : "50",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "Convergence of a block coordinate descent method for nondifferentiable minimization",
      "author" : [ "Paul Tseng" ],
      "venue" : "Journal of optimization theory and applications, vol. 109, no. 3, pp. 475–494, 2001.",
      "citeRegEx" : "51",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Improved iteration complexity bounds of cyclic block coordinate descent for convex problems",
      "author" : [ "Ruoyu Sun", "Mingyi Hong" ],
      "venue" : "Advances in Neural Information Processing Systems, 2015, pp. 1306–1314.",
      "citeRegEx" : "52",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Worst-case complexity of cyclic coordinate descent: O(n2) gap with randomized version",
      "author" : [ "Ruoyu Sun", "Yinyu Ye" ],
      "venue" : "arXiv preprint arXiv:1604.07130, 2016.",
      "citeRegEx" : "53",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Efficiency of coordinate descent methods on huge-scale optimization problems",
      "author" : [ "Yu. Nesterov" ],
      "venue" : "SIAM Journal on Optimization, vol. 22, no. 2, pp. 341–362, 2012.",
      "citeRegEx" : "54",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "A unified convergence analysis of block successive minimization methods for nonsmooth optimization",
      "author" : [ "Meisam Razaviyayn", "Mingyi Hong", "Zhi-Quan Luo" ],
      "venue" : "SIAM Journal on Optimization, vol. 23, no. 2, pp. 1126–1153, 2013.",
      "citeRegEx" : "55",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Cross-layer provision of future cellular networks: A WMMSE-based approach",
      "author" : [ "Hadi Baligh", "Mingyi Hong", "Wei-Cheng Liao", "Zhi-Quan Luo", "Meisam Razaviyayn", "Maziar Sanjabi", "Ruoyu Sun" ],
      "venue" : "IEEE Signal Processing Magazine, vol. 31, no. 6, pp. 56–68, 2014.",
      "citeRegEx" : "56",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Joint base station clustering and beamformer design for partial coordinated transmission in heterogeneous networks",
      "author" : [ "Mingyi Hong", "Ruoyu Sun", "H. Baligh", "Zhi-Quan Luo" ],
      "venue" : "IEEE Journal on Selected Areas in Communications (JSAC), vol. 31, no. 2, pp. 226–240, February 2013.",
      "citeRegEx" : "57",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Matrix completion and low-rank svd via fast alternating least squares",
      "author" : [ "Trevor Hastie", "Rahul Mazumder", "Jason Lee", "Reza Zadeh" ],
      "venue" : "arXiv preprint arXiv:1410.2596, 2014.",
      "citeRegEx" : "58",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "On the convergence of the block nonlinear gauss–seidel method under convex constraints",
      "author" : [ "Luigi Grippo", "Marco Sciandrone" ],
      "venue" : "Operations Research Letters, vol. 26, no. 3, pp. 127–136, 2000.",
      "citeRegEx" : "59",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "On the expected convergence of randomly permuted ADMM",
      "author" : [ "Ruoyu Sun", "Zhi-Quan Luo", "Yinyu Ye" ],
      "venue" : "arXiv preprint arXiv:1503.06387, 2015.",
      "citeRegEx" : "60",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Analysis of an approximate gradient projection method with applications to the backpropagation algorithm",
      "author" : [ "Zhi-Quan Luo", "Paul Tseng" ],
      "venue" : "Optimization Methods and Software, vol. 4, no. 2, pp. 85–101, 1994.",
      "citeRegEx" : "61",
      "shortCiteRegEx" : null,
      "year" : 1994
    }, {
      "title" : "Gradient convergence in gradient methods with errors",
      "author" : [ "Dimitri P Bertsekas", "John N Tsitsiklis" ],
      "venue" : "SIAM Journal on Optimization, vol. 10, no. 3, pp. 627–642, 2000.",
      "citeRegEx" : "62",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "Perturbation theory for the singular value decomposition",
      "author" : [ "Gilbert W Stewart" ],
      "venue" : "1998. 77",
      "citeRegEx" : "63",
      "shortCiteRegEx" : null,
      "year" : 1998
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Matrix completion has found numerous applications in various fields such as recommender systems [1], computer vision [2] and system identification [3], to name a few.",
      "startOffset" : 96,
      "endOffset" : 99
    }, {
      "referenceID" : 1,
      "context" : "Matrix completion has found numerous applications in various fields such as recommender systems [1], computer vision [2] and system identification [3], to name a few.",
      "startOffset" : 117,
      "endOffset" : 120
    }, {
      "referenceID" : 2,
      "context" : "Matrix completion has found numerous applications in various fields such as recommender systems [1], computer vision [2] and system identification [3], to name a few.",
      "startOffset" : 147,
      "endOffset" : 150
    }, {
      "referenceID" : 3,
      "context" : "For the matrix completion problem, the nuclear norm based formulation becomes either a linearly constrained minimization problem [4] min Z∈Rm×n ‖Z‖∗, s.",
      "startOffset" : 129,
      "endOffset" : 132
    }, {
      "referenceID" : 3,
      "context" : "On the theoretical side, it has been shown that given a rank-r matrix M satisfying an incoherence condition, solving (1) will exactly reconstruct M with high probability provided that O(r(m + n) log2(m + n)) entries are uniformly randomly revealed [4–7].",
      "startOffset" : 248,
      "endOffset" : 253
    }, {
      "referenceID" : 4,
      "context" : "On the theoretical side, it has been shown that given a rank-r matrix M satisfying an incoherence condition, solving (1) will exactly reconstruct M with high probability provided that O(r(m + n) log2(m + n)) entries are uniformly randomly revealed [4–7].",
      "startOffset" : 248,
      "endOffset" : 253
    }, {
      "referenceID" : 5,
      "context" : "On the theoretical side, it has been shown that given a rank-r matrix M satisfying an incoherence condition, solving (1) will exactly reconstruct M with high probability provided that O(r(m + n) log2(m + n)) entries are uniformly randomly revealed [4–7].",
      "startOffset" : 248,
      "endOffset" : 253
    }, {
      "referenceID" : 6,
      "context" : "On the theoretical side, it has been shown that given a rank-r matrix M satisfying an incoherence condition, solving (1) will exactly reconstruct M with high probability provided that O(r(m + n) log2(m + n)) entries are uniformly randomly revealed [4–7].",
      "startOffset" : 248,
      "endOffset" : 253
    }, {
      "referenceID" : 7,
      "context" : "This result was later generalized to noisy matrix completion, whereby the optimization formulation (2) is adopted [8].",
      "startOffset" : 114,
      "endOffset" : 117
    }, {
      "referenceID" : 8,
      "context" : "Using a different proof framework, reference [9] provided theoretical guarantee for a variant of the formulation (3).",
      "startOffset" : 45,
      "endOffset" : 48
    }, {
      "referenceID" : 9,
      "context" : "To solve problems with larger size, researchers have developed first order algorithms, including the SVT (singular value thresholding) algorithm for the formulation (1) [10], and several variants of the proximal gradient method for the formulation (3) [11, 12] .",
      "startOffset" : 169,
      "endOffset" : 173
    }, {
      "referenceID" : 10,
      "context" : "To solve problems with larger size, researchers have developed first order algorithms, including the SVT (singular value thresholding) algorithm for the formulation (1) [10], and several variants of the proximal gradient method for the formulation (3) [11, 12] .",
      "startOffset" : 252,
      "endOffset" : 260
    }, {
      "referenceID" : 11,
      "context" : "To solve problems with larger size, researchers have developed first order algorithms, including the SVT (singular value thresholding) algorithm for the formulation (1) [10], and several variants of the proximal gradient method for the formulation (3) [11, 12] .",
      "startOffset" : 252,
      "endOffset" : 260
    }, {
      "referenceID" : 12,
      "context" : "Although linear convergence of the proximal gradient method has been established for the formulation (3) under certain conditions [13, 14], the per-iteration cost of computing SVD (Singular Value Decomposition) may increase rapidly as the dimension of the problem increases, making these algorithms rather slow or even useless for problems of huge size.",
      "startOffset" : 130,
      "endOffset" : 138
    }, {
      "referenceID" : 13,
      "context" : "Although linear convergence of the proximal gradient method has been established for the formulation (3) under certain conditions [13, 14], the per-iteration cost of computing SVD (Singular Value Decomposition) may increase rapidly as the dimension of the problem increases, making these algorithms rather slow or even useless for problems of huge size.",
      "startOffset" : 130,
      "endOffset" : 138
    }, {
      "referenceID" : 14,
      "context" : "Such a matrix factorization model has long been used in PCA (principle component analysis) and many other applications [15].",
      "startOffset" : 119,
      "endOffset" : 123
    }, {
      "referenceID" : 0,
      "context" : "It has gained great popularity in the recommender systems field and served as the basic building block of many competing algorithms for the Netflix Prize [1, 16] due to several reasons.",
      "startOffset" : 154,
      "endOffset" : 161
    }, {
      "referenceID" : 15,
      "context" : "It has gained great popularity in the recommender systems field and served as the basic building block of many competing algorithms for the Netflix Prize [1, 16] due to several reasons.",
      "startOffset" : 154,
      "endOffset" : 161
    }, {
      "referenceID" : 0,
      "context" : "Third, as elaborated in [1], the factorization model can be easily modified to incorporate additional application-specific requirements.",
      "startOffset" : 24,
      "endOffset" : 27
    }, {
      "referenceID" : 0,
      "context" : "A popular factorization based formulation for matrix completion takes the form of an unconstrained regularized square-loss minimization problem [1]:",
      "startOffset" : 144,
      "endOffset" : 147
    }, {
      "referenceID" : 16,
      "context" : "There are a few variants of this formulation: the coefficient λ can be zero [17–20] or different for each row of X,Y [21]; each square loss term [Mi j−(XY )i j] can have different weights [1]; an additional matrix variable Z ∈ Rn×r can be introduced [22].",
      "startOffset" : 76,
      "endOffset" : 83
    }, {
      "referenceID" : 17,
      "context" : "There are a few variants of this formulation: the coefficient λ can be zero [17–20] or different for each row of X,Y [21]; each square loss term [Mi j−(XY )i j] can have different weights [1]; an additional matrix variable Z ∈ Rn×r can be introduced [22].",
      "startOffset" : 76,
      "endOffset" : 83
    }, {
      "referenceID" : 18,
      "context" : "There are a few variants of this formulation: the coefficient λ can be zero [17–20] or different for each row of X,Y [21]; each square loss term [Mi j−(XY )i j] can have different weights [1]; an additional matrix variable Z ∈ Rn×r can be introduced [22].",
      "startOffset" : 76,
      "endOffset" : 83
    }, {
      "referenceID" : 19,
      "context" : "There are a few variants of this formulation: the coefficient λ can be zero [17–20] or different for each row of X,Y [21]; each square loss term [Mi j−(XY )i j] can have different weights [1]; an additional matrix variable Z ∈ Rn×r can be introduced [22].",
      "startOffset" : 76,
      "endOffset" : 83
    }, {
      "referenceID" : 20,
      "context" : "There are a few variants of this formulation: the coefficient λ can be zero [17–20] or different for each row of X,Y [21]; each square loss term [Mi j−(XY )i j] can have different weights [1]; an additional matrix variable Z ∈ Rn×r can be introduced [22].",
      "startOffset" : 117,
      "endOffset" : 121
    }, {
      "referenceID" : 0,
      "context" : "There are a few variants of this formulation: the coefficient λ can be zero [17–20] or different for each row of X,Y [21]; each square loss term [Mi j−(XY )i j] can have different weights [1]; an additional matrix variable Z ∈ Rn×r can be introduced [22].",
      "startOffset" : 188,
      "endOffset" : 191
    }, {
      "referenceID" : 21,
      "context" : "There are a few variants of this formulation: the coefficient λ can be zero [17–20] or different for each row of X,Y [21]; each square loss term [Mi j−(XY )i j] can have different weights [1]; an additional matrix variable Z ∈ Rn×r can be introduced [22].",
      "startOffset" : 250,
      "endOffset" : 254
    }, {
      "referenceID" : 0,
      "context" : "Problem (4) is a non-convex fourth-order polynomial optimization problem, and can be solved to stationary points by standard nonlinear optimization algorithms such as gradient descent method, alternating minimization [1, 18, 19, 21] and SGD (stochastic gradient descent) [1, 16, 23, 24].",
      "startOffset" : 217,
      "endOffset" : 232
    }, {
      "referenceID" : 17,
      "context" : "Problem (4) is a non-convex fourth-order polynomial optimization problem, and can be solved to stationary points by standard nonlinear optimization algorithms such as gradient descent method, alternating minimization [1, 18, 19, 21] and SGD (stochastic gradient descent) [1, 16, 23, 24].",
      "startOffset" : 217,
      "endOffset" : 232
    }, {
      "referenceID" : 18,
      "context" : "Problem (4) is a non-convex fourth-order polynomial optimization problem, and can be solved to stationary points by standard nonlinear optimization algorithms such as gradient descent method, alternating minimization [1, 18, 19, 21] and SGD (stochastic gradient descent) [1, 16, 23, 24].",
      "startOffset" : 217,
      "endOffset" : 232
    }, {
      "referenceID" : 20,
      "context" : "Problem (4) is a non-convex fourth-order polynomial optimization problem, and can be solved to stationary points by standard nonlinear optimization algorithms such as gradient descent method, alternating minimization [1, 18, 19, 21] and SGD (stochastic gradient descent) [1, 16, 23, 24].",
      "startOffset" : 217,
      "endOffset" : 232
    }, {
      "referenceID" : 0,
      "context" : "Problem (4) is a non-convex fourth-order polynomial optimization problem, and can be solved to stationary points by standard nonlinear optimization algorithms such as gradient descent method, alternating minimization [1, 18, 19, 21] and SGD (stochastic gradient descent) [1, 16, 23, 24].",
      "startOffset" : 271,
      "endOffset" : 286
    }, {
      "referenceID" : 15,
      "context" : "Problem (4) is a non-convex fourth-order polynomial optimization problem, and can be solved to stationary points by standard nonlinear optimization algorithms such as gradient descent method, alternating minimization [1, 18, 19, 21] and SGD (stochastic gradient descent) [1, 16, 23, 24].",
      "startOffset" : 271,
      "endOffset" : 286
    }, {
      "referenceID" : 22,
      "context" : "Problem (4) is a non-convex fourth-order polynomial optimization problem, and can be solved to stationary points by standard nonlinear optimization algorithms such as gradient descent method, alternating minimization [1, 18, 19, 21] and SGD (stochastic gradient descent) [1, 16, 23, 24].",
      "startOffset" : 271,
      "endOffset" : 286
    }, {
      "referenceID" : 23,
      "context" : "Problem (4) is a non-convex fourth-order polynomial optimization problem, and can be solved to stationary points by standard nonlinear optimization algorithms such as gradient descent method, alternating minimization [1, 18, 19, 21] and SGD (stochastic gradient descent) [1, 16, 23, 24].",
      "startOffset" : 271,
      "endOffset" : 286
    }, {
      "referenceID" : 24,
      "context" : "Recently several parallelizable variants of the SGD [25–27] and variants of the block coordinate descent method with very low per-iteration cost [28,29] have been developed.",
      "startOffset" : 52,
      "endOffset" : 59
    }, {
      "referenceID" : 25,
      "context" : "Recently several parallelizable variants of the SGD [25–27] and variants of the block coordinate descent method with very low per-iteration cost [28,29] have been developed.",
      "startOffset" : 52,
      "endOffset" : 59
    }, {
      "referenceID" : 26,
      "context" : "Recently several parallelizable variants of the SGD [25–27] and variants of the block coordinate descent method with very low per-iteration cost [28,29] have been developed.",
      "startOffset" : 52,
      "endOffset" : 59
    }, {
      "referenceID" : 27,
      "context" : "Recently several parallelizable variants of the SGD [25–27] and variants of the block coordinate descent method with very low per-iteration cost [28,29] have been developed.",
      "startOffset" : 145,
      "endOffset" : 152
    }, {
      "referenceID" : 28,
      "context" : "Recently several parallelizable variants of the SGD [25–27] and variants of the block coordinate descent method with very low per-iteration cost [28,29] have been developed.",
      "startOffset" : 145,
      "endOffset" : 152
    }, {
      "referenceID" : 16,
      "context" : "2) Our result applies to the standard forms of the algorithms (though our optimization formulation is a bit different), which do not require the additional resampling scheme used in other works [17–20].",
      "startOffset" : 194,
      "endOffset" : 201
    }, {
      "referenceID" : 17,
      "context" : "2) Our result applies to the standard forms of the algorithms (though our optimization formulation is a bit different), which do not require the additional resampling scheme used in other works [17–20].",
      "startOffset" : 194,
      "endOffset" : 201
    }, {
      "referenceID" : 18,
      "context" : "2) Our result applies to the standard forms of the algorithms (though our optimization formulation is a bit different), which do not require the additional resampling scheme used in other works [17–20].",
      "startOffset" : 194,
      "endOffset" : 201
    }, {
      "referenceID" : 19,
      "context" : "2) Our result applies to the standard forms of the algorithms (though our optimization formulation is a bit different), which do not require the additional resampling scheme used in other works [17–20].",
      "startOffset" : 194,
      "endOffset" : 201
    }, {
      "referenceID" : 30,
      "context" : "The first recovery guarantee for the factorization based matrix completion is provided in [31], where Keshavan, Montanari and Oh considered a factorization model in Grassmannian manifold and showed that the matrix can be recovered by a proper initialization and a gradient descent method on Grassmannian manifold.",
      "startOffset" : 90,
      "endOffset" : 94
    }, {
      "referenceID" : 16,
      "context" : "The factorization model in Euclidean space was first analyzed in an unpublished work [17] of Keshavan 1, as well as a later work of Jain et al.",
      "startOffset" : 85,
      "endOffset" : 89
    }, {
      "referenceID" : 17,
      "context" : "[18].",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 18,
      "context" : "The sample complexity bounds were later improved by Hardt [19] and Hardt and Wooters [20], where in the latter work, notably, the authors devised an algorithm with a corresponding sample complexity bound independent of the condition number.",
      "startOffset" : 58,
      "endOffset" : 62
    }, {
      "referenceID" : 19,
      "context" : "The sample complexity bounds were later improved by Hardt [19] and Hardt and Wooters [20], where in the latter work, notably, the authors devised an algorithm with a corresponding sample complexity bound independent of the condition number.",
      "startOffset" : 85,
      "endOffset" : 89
    }, {
      "referenceID" : 31,
      "context" : "[32].",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 31,
      "context" : "We will point out a subtle theoretical issue not mentioned in [32], as well as some other practical issues.",
      "startOffset" : 62,
      "endOffset" : 66
    }, {
      "referenceID" : 16,
      "context" : "1 Reference [17] is a PhD thesis that discusses various algorithms including the algorithm proposed in [31] and alternating minimization.",
      "startOffset" : 12,
      "endOffset" : 16
    }, {
      "referenceID" : 30,
      "context" : "1 Reference [17] is a PhD thesis that discusses various algorithms including the algorithm proposed in [31] and alternating minimization.",
      "startOffset" : 103,
      "endOffset" : 107
    }, {
      "referenceID" : 16,
      "context" : "In this paper when we refer to [17], we are only referring to [17, Ch.",
      "startOffset" : 31,
      "endOffset" : 35
    }, {
      "referenceID" : 5,
      "context" : "golfing scheme [6]) can be used at almost no cost for the nuclear norm approach [6, 7, 33], but for the alternating minimization it causes many issues.",
      "startOffset" : 15,
      "endOffset" : 18
    }, {
      "referenceID" : 5,
      "context" : "golfing scheme [6]) can be used at almost no cost for the nuclear norm approach [6, 7, 33], but for the alternating minimization it causes many issues.",
      "startOffset" : 80,
      "endOffset" : 90
    }, {
      "referenceID" : 6,
      "context" : "golfing scheme [6]) can be used at almost no cost for the nuclear norm approach [6, 7, 33], but for the alternating minimization it causes many issues.",
      "startOffset" : 80,
      "endOffset" : 90
    }, {
      "referenceID" : 32,
      "context" : "golfing scheme [6]) can be used at almost no cost for the nuclear norm approach [6, 7, 33], but for the alternating minimization it causes many issues.",
      "startOffset" : 80,
      "endOffset" : 90
    }, {
      "referenceID" : 16,
      "context" : ", L, as proposed in [17,18] 2.",
      "startOffset" : 20,
      "endOffset" : 27
    }, {
      "referenceID" : 17,
      "context" : ", L, as proposed in [17,18] 2.",
      "startOffset" : 20,
      "endOffset" : 27
    }, {
      "referenceID" : 16,
      "context" : "However, the results in [17–20] actually require a generative model of independent Ωk’s, instead of sampling Ωk’s based on a given Ω.",
      "startOffset" : 24,
      "endOffset" : 31
    }, {
      "referenceID" : 17,
      "context" : "However, the results in [17–20] actually require a generative model of independent Ωk’s, instead of sampling Ωk’s based on a given Ω.",
      "startOffset" : 24,
      "endOffset" : 31
    }, {
      "referenceID" : 18,
      "context" : "However, the results in [17–20] actually require a generative model of independent Ωk’s, instead of sampling Ωk’s based on a given Ω.",
      "startOffset" : 24,
      "endOffset" : 31
    }, {
      "referenceID" : 19,
      "context" : "However, the results in [17–20] actually require a generative model of independent Ωk’s, instead of sampling Ωk’s based on a given Ω.",
      "startOffset" : 24,
      "endOffset" : 31
    }, {
      "referenceID" : 16,
      "context" : "Therefore, the results in [17–20] do not directly apply to the partition based resampling scheme that is easy to use.",
      "startOffset" : 26,
      "endOffset" : 33
    }, {
      "referenceID" : 17,
      "context" : "Therefore, the results in [17–20] do not directly apply to the partition based resampling scheme that is easy to use.",
      "startOffset" : 26,
      "endOffset" : 33
    }, {
      "referenceID" : 18,
      "context" : "Therefore, the results in [17–20] do not directly apply to the partition based resampling scheme that is easy to use.",
      "startOffset" : 26,
      "endOffset" : 33
    }, {
      "referenceID" : 19,
      "context" : "Therefore, the results in [17–20] do not directly apply to the partition based resampling scheme that is easy to use.",
      "startOffset" : 26,
      "endOffset" : 33
    }, {
      "referenceID" : 16,
      "context" : "This issue has been discussed by Hardt and Wooters in [20, Appendix D], and they proposed a new resampling scheme [20, Algorithm 6] to which the results in [17–20] can apply, provided that the generative model of Ω is exactly known.",
      "startOffset" : 156,
      "endOffset" : 163
    }, {
      "referenceID" : 17,
      "context" : "This issue has been discussed by Hardt and Wooters in [20, Appendix D], and they proposed a new resampling scheme [20, Algorithm 6] to which the results in [17–20] can apply, provided that the generative model of Ω is exactly known.",
      "startOffset" : 156,
      "endOffset" : 163
    }, {
      "referenceID" : 18,
      "context" : "This issue has been discussed by Hardt and Wooters in [20, Appendix D], and they proposed a new resampling scheme [20, Algorithm 6] to which the results in [17–20] can apply, provided that the generative model of Ω is exactly known.",
      "startOffset" : 156,
      "endOffset" : 163
    }, {
      "referenceID" : 19,
      "context" : "This issue has been discussed by Hardt and Wooters in [20, Appendix D], and they proposed a new resampling scheme [20, Algorithm 6] to which the results in [17–20] can apply, provided that the generative model of Ω is exactly known.",
      "startOffset" : 156,
      "endOffset" : 163
    }, {
      "referenceID" : 3,
      "context" : "In contrast, the classical results in [4–7] and our result herein are robust to the generative model of Ω: these results actually state that for an overwhelming portion of Ω with a given size, one can recover M through a certain algorithm, thus for many reasonable probability distributions of Ω a high probability result holds.",
      "startOffset" : 38,
      "endOffset" : 43
    }, {
      "referenceID" : 4,
      "context" : "In contrast, the classical results in [4–7] and our result herein are robust to the generative model of Ω: these results actually state that for an overwhelming portion of Ω with a given size, one can recover M through a certain algorithm, thus for many reasonable probability distributions of Ω a high probability result holds.",
      "startOffset" : 38,
      "endOffset" : 43
    }, {
      "referenceID" : 5,
      "context" : "In contrast, the classical results in [4–7] and our result herein are robust to the generative model of Ω: these results actually state that for an overwhelming portion of Ω with a given size, one can recover M through a certain algorithm, thus for many reasonable probability distributions of Ω a high probability result holds.",
      "startOffset" : 38,
      "endOffset" : 43
    }, {
      "referenceID" : 6,
      "context" : "In contrast, the classical results in [4–7] and our result herein are robust to the generative model of Ω: these results actually state that for an overwhelming portion of Ω with a given size, one can recover M through a certain algorithm, thus for many reasonable probability distributions of Ω a high probability result holds.",
      "startOffset" : 38,
      "endOffset" : 43
    }, {
      "referenceID" : 33,
      "context" : "In a recent work [34] the authors have managed to remove the dependency of the required sample size on by using a singular value projection algorithm.",
      "startOffset" : 17,
      "endOffset" : 21
    }, {
      "referenceID" : 33,
      "context" : "However, [34] considers a matrix variable of the same size as the original matrix, which requires significantly more memory than the matrix factorization approach considered in this paper.",
      "startOffset" : 9,
      "endOffset" : 13
    }, {
      "referenceID" : 34,
      "context" : "The resampling is also required in the recent work of [35]; see [30, Sec.",
      "startOffset" : 54,
      "endOffset" : 58
    }, {
      "referenceID" : 31,
      "context" : "Non-convex formulation has also been studied for the phase retrieval problem in some recent works [32, 36].",
      "startOffset" : 98,
      "endOffset" : 106
    }, {
      "referenceID" : 35,
      "context" : "Non-convex formulation has also been studied for the phase retrieval problem in some recent works [32, 36].",
      "startOffset" : 98,
      "endOffset" : 106
    }, {
      "referenceID" : 35,
      "context" : "The major difference between [36] and [32] is that the former requires independent samples in each iteration, while the latter uses the same samples throughout in the proposed algorithm.",
      "startOffset" : 29,
      "endOffset" : 33
    }, {
      "referenceID" : 31,
      "context" : "The major difference between [36] and [32] is that the former requires independent samples in each iteration, while the latter uses the same samples throughout in the proposed algorithm.",
      "startOffset" : 38,
      "endOffset" : 42
    }, {
      "referenceID" : 16,
      "context" : "As mentioned earlier, such a difference also exists between all previous works on alternating minimization for matrix completion [17–20] and our work.",
      "startOffset" : 129,
      "endOffset" : 136
    }, {
      "referenceID" : 17,
      "context" : "As mentioned earlier, such a difference also exists between all previous works on alternating minimization for matrix completion [17–20] and our work.",
      "startOffset" : 129,
      "endOffset" : 136
    }, {
      "referenceID" : 18,
      "context" : "As mentioned earlier, such a difference also exists between all previous works on alternating minimization for matrix completion [17–20] and our work.",
      "startOffset" : 129,
      "endOffset" : 136
    }, {
      "referenceID" : 19,
      "context" : "As mentioned earlier, such a difference also exists between all previous works on alternating minimization for matrix completion [17–20] and our work.",
      "startOffset" : 129,
      "endOffset" : 136
    }, {
      "referenceID" : 36,
      "context" : "[37–39]), sparse PCA [40,41], robust PCA [42] and EM (Expected2The description in [18] has some ambiguity and it might refer to the scheme of sampling Ωk’s with replacement; anyhow, under this model Ωk’s are still dependent.",
      "startOffset" : 0,
      "endOffset" : 7
    }, {
      "referenceID" : 37,
      "context" : "[37–39]), sparse PCA [40,41], robust PCA [42] and EM (Expected2The description in [18] has some ambiguity and it might refer to the scheme of sampling Ωk’s with replacement; anyhow, under this model Ωk’s are still dependent.",
      "startOffset" : 0,
      "endOffset" : 7
    }, {
      "referenceID" : 38,
      "context" : "[37–39]), sparse PCA [40,41], robust PCA [42] and EM (Expected2The description in [18] has some ambiguity and it might refer to the scheme of sampling Ωk’s with replacement; anyhow, under this model Ωk’s are still dependent.",
      "startOffset" : 0,
      "endOffset" : 7
    }, {
      "referenceID" : 39,
      "context" : "[37–39]), sparse PCA [40,41], robust PCA [42] and EM (Expected2The description in [18] has some ambiguity and it might refer to the scheme of sampling Ωk’s with replacement; anyhow, under this model Ωk’s are still dependent.",
      "startOffset" : 21,
      "endOffset" : 28
    }, {
      "referenceID" : 40,
      "context" : "[37–39]), sparse PCA [40,41], robust PCA [42] and EM (Expected2The description in [18] has some ambiguity and it might refer to the scheme of sampling Ωk’s with replacement; anyhow, under this model Ωk’s are still dependent.",
      "startOffset" : 21,
      "endOffset" : 28
    }, {
      "referenceID" : 41,
      "context" : "[37–39]), sparse PCA [40,41], robust PCA [42] and EM (Expected2The description in [18] has some ambiguity and it might refer to the scheme of sampling Ωk’s with replacement; anyhow, under this model Ωk’s are still dependent.",
      "startOffset" : 41,
      "endOffset" : 45
    }, {
      "referenceID" : 17,
      "context" : "[37–39]), sparse PCA [40,41], robust PCA [42] and EM (Expected2The description in [18] has some ambiguity and it might refer to the scheme of sampling Ωk’s with replacement; anyhow, under this model Ωk’s are still dependent.",
      "startOffset" : 82,
      "endOffset" : 86
    }, {
      "referenceID" : 42,
      "context" : "Maximization) algorithm [43, 44].",
      "startOffset" : 24,
      "endOffset" : 32
    }, {
      "referenceID" : 43,
      "context" : "Maximization) algorithm [43, 44].",
      "startOffset" : 24,
      "endOffset" : 32
    }, {
      "referenceID" : 44,
      "context" : "The difference from traditional perturbation analysis of Wedin [45] (i.",
      "startOffset" : 63,
      "endOffset" : 67
    }, {
      "referenceID" : 44,
      "context" : "if two matrices are close then their row/column spaces are close) is that in [45] the row/column spaces are fixed while in our problem U,V are up to our choice.",
      "startOffset" : 77,
      "endOffset" : 81
    }, {
      "referenceID" : 7,
      "context" : "This inequality is closely related to matrix RIP (restricted isometry property) in [8] (see equation (III.",
      "startOffset" : 83,
      "endOffset" : 86
    }, {
      "referenceID" : 30,
      "context" : "A solution, as employed in [31], is to utilize a random graph lemma in [46] which provides a bound on ‖PΩ(A)‖F for any rank-1 matrix A (possibly dependent on Ω).",
      "startOffset" : 27,
      "endOffset" : 31
    }, {
      "referenceID" : 45,
      "context" : "A solution, as employed in [31], is to utilize a random graph lemma in [46] which provides a bound on ‖PΩ(A)‖F for any rank-1 matrix A (possibly dependent on Ω).",
      "startOffset" : 71,
      "endOffset" : 75
    }, {
      "referenceID" : 3,
      "context" : "This lemma, combined with another probability result in [4], implies a bound on ‖PΩ(M − XYT )‖F .",
      "startOffset" : 56,
      "endOffset" : 59
    }, {
      "referenceID" : 30,
      "context" : "A special case of the third condition has been used in [31] for Grassmann manifold optimization.",
      "startOffset" : 55,
      "endOffset" : 59
    }, {
      "referenceID" : 16,
      "context" : "One simple strategy as adopted in [17–20] is to use a resampling scheme to decouple A and the observation set.",
      "startOffset" : 34,
      "endOffset" : 41
    }, {
      "referenceID" : 17,
      "context" : "One simple strategy as adopted in [17–20] is to use a resampling scheme to decouple A and the observation set.",
      "startOffset" : 34,
      "endOffset" : 41
    }, {
      "referenceID" : 18,
      "context" : "One simple strategy as adopted in [17–20] is to use a resampling scheme to decouple A and the observation set.",
      "startOffset" : 34,
      "endOffset" : 41
    }, {
      "referenceID" : 19,
      "context" : "One simple strategy as adopted in [17–20] is to use a resampling scheme to decouple A and the observation set.",
      "startOffset" : 34,
      "endOffset" : 41
    }, {
      "referenceID" : 30,
      "context" : "Another strategy, as employed in [31], is to use a random graph lemma in [46].",
      "startOffset" : 33,
      "endOffset" : 37
    }, {
      "referenceID" : 45,
      "context" : "Another strategy, as employed in [31], is to use a random graph lemma in [46].",
      "startOffset" : 73,
      "endOffset" : 77
    }, {
      "referenceID" : 45,
      "context" : "We apply the random graph lemma of [46] when extending the local geometry of ‖M−XYT ‖F to ‖PΩ(M−XY )‖F .",
      "startOffset" : 35,
      "endOffset" : 39
    }, {
      "referenceID" : 30,
      "context" : "The difference of our work with [31] is that we study the local geometry in Euclidean space (and, indirectly, the geometry of the quotient manifold), which is quite different from the local geometry in Grassmann manifold studied in [31].",
      "startOffset" : 32,
      "endOffset" : 36
    }, {
      "referenceID" : 30,
      "context" : "The difference of our work with [31] is that we study the local geometry in Euclidean space (and, indirectly, the geometry of the quotient manifold), which is quite different from the local geometry in Grassmann manifold studied in [31].",
      "startOffset" : 232,
      "endOffset" : 236
    }, {
      "referenceID" : 30,
      "context" : "Technically, the complications of the proof in [31] are mostly due to heavy computation of various quantities in Grassmann manifold; in addition, much effort is spent in estimating the terms related to the extra factor S which enables the decoupling of X and Y ( [31] actually uses a three-factor decomposition XS YT ).",
      "startOffset" : 47,
      "endOffset" : 51
    }, {
      "referenceID" : 30,
      "context" : "Technically, the complications of the proof in [31] are mostly due to heavy computation of various quantities in Grassmann manifold; in addition, much effort is spent in estimating the terms related to the extra factor S which enables the decoupling of X and Y ( [31] actually uses a three-factor decomposition XS YT ).",
      "startOffset" : 263,
      "endOffset" : 267
    }, {
      "referenceID" : 29,
      "context" : "10 of [30] shows that when |Ω| is small, in all successful instances the iterates are balanced, while in all failed instances the iterates are unbalanced.",
      "startOffset" : 6,
      "endOffset" : 10
    }, {
      "referenceID" : 3,
      "context" : "The incoherence condition for the matrix completion problem is first introduced by Candès and Recht in [4] and has become a standard assumption for low-rank matrix recovery problems (except a few recent works such as [47, 48]).",
      "startOffset" : 103,
      "endOffset" : 106
    }, {
      "referenceID" : 46,
      "context" : "The incoherence condition for the matrix completion problem is first introduced by Candès and Recht in [4] and has become a standard assumption for low-rank matrix recovery problems (except a few recent works such as [47, 48]).",
      "startOffset" : 217,
      "endOffset" : 225
    }, {
      "referenceID" : 47,
      "context" : "The incoherence condition for the matrix completion problem is first introduced by Candès and Recht in [4] and has become a standard assumption for low-rank matrix recovery problems (except a few recent works such as [47, 48]).",
      "startOffset" : 217,
      "endOffset" : 225
    }, {
      "referenceID" : 30,
      "context" : "We will define an incoherence condition for an m × n matrix M which is the same as that in [31].",
      "startOffset" : 91,
      "endOffset" : 95
    }, {
      "referenceID" : 30,
      "context" : "For some popular random models for generating M, the incoherence condition holds with a parameter scaling as √ r log n (see [31]).",
      "startOffset" : 124,
      "endOffset" : 128
    }, {
      "referenceID" : 3,
      "context" : "We remark that this model is “equivalent to” a Bernolli model that each entry of M is included into Ω independently with probability p = S mn in the sense that if the success of an algorithm holds for the Bernolli model with a certain p with high probability, then the success also holds for the uniform random model with |Ω| = pmn with high probability (see [4] or [31, Sec.",
      "startOffset" : 359,
      "endOffset" : 362
    }, {
      "referenceID" : 0,
      "context" : "The choice of function G0 is not unique; in fact, we can choose any G0 that satisfies the following requirements: a) G0 is convex and continuously differentiable; b) G0(z) = 0, z ∈ [0, 1].",
      "startOffset" : 181,
      "endOffset" : 187
    }, {
      "referenceID" : 30,
      "context" : "In [31], G0 is chosen as G0(z) = I[1,∞](z)(e 2 − 1), which also satisfies these two requirements.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 48,
      "context" : "[49])",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 48,
      "context" : "[49]), which motivates our choice of G0 in (14).",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 16,
      "context" : "Special initialization is also required in other works on non-convex formulations [17–20, 31, 32, 36].",
      "startOffset" : 82,
      "endOffset" : 101
    }, {
      "referenceID" : 17,
      "context" : "Special initialization is also required in other works on non-convex formulations [17–20, 31, 32, 36].",
      "startOffset" : 82,
      "endOffset" : 101
    }, {
      "referenceID" : 18,
      "context" : "Special initialization is also required in other works on non-convex formulations [17–20, 31, 32, 36].",
      "startOffset" : 82,
      "endOffset" : 101
    }, {
      "referenceID" : 19,
      "context" : "Special initialization is also required in other works on non-convex formulations [17–20, 31, 32, 36].",
      "startOffset" : 82,
      "endOffset" : 101
    }, {
      "referenceID" : 30,
      "context" : "Special initialization is also required in other works on non-convex formulations [17–20, 31, 32, 36].",
      "startOffset" : 82,
      "endOffset" : 101
    }, {
      "referenceID" : 31,
      "context" : "Special initialization is also required in other works on non-convex formulations [17–20, 31, 32, 36].",
      "startOffset" : 82,
      "endOffset" : 101
    }, {
      "referenceID" : 35,
      "context" : "Special initialization is also required in other works on non-convex formulations [17–20, 31, 32, 36].",
      "startOffset" : 82,
      "endOffset" : 101
    }, {
      "referenceID" : 30,
      "context" : "[31]), we obtain M0 = X̂0Ŷ 0 which is close to M; second, we scale the rows of (X̂0, Ŷ0) to make it incoherent (i.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 49,
      "context" : "There are many choices of stepsizes such as constant stepsize, exact line search, limited line search, diminishing stepsize and Armijo rule [50].",
      "startOffset" : 140,
      "endOffset" : 144
    }, {
      "referenceID" : 30,
      "context" : "Note that the restricted line search rule is similar to that used in [31] for the gradient descent method over Grassmannian manifolds.",
      "startOffset" : 69,
      "endOffset" : 73
    }, {
      "referenceID" : 50,
      "context" : "cyclic [51–53], randomized [54] or parallel) and solve the subproblem inexactly.",
      "startOffset" : 7,
      "endOffset" : 14
    }, {
      "referenceID" : 51,
      "context" : "cyclic [51–53], randomized [54] or parallel) and solve the subproblem inexactly.",
      "startOffset" : 7,
      "endOffset" : 14
    }, {
      "referenceID" : 52,
      "context" : "cyclic [51–53], randomized [54] or parallel) and solve the subproblem inexactly.",
      "startOffset" : 7,
      "endOffset" : 14
    }, {
      "referenceID" : 53,
      "context" : "cyclic [51–53], randomized [54] or parallel) and solve the subproblem inexactly.",
      "startOffset" : 27,
      "endOffset" : 31
    }, {
      "referenceID" : 53,
      "context" : "Commonly used inexact BCD type algorithms include BCGD (block coordinate gradient descent, which updates each variable by a single gradient step [54]) and BSUM (block successive upper bound minimization, which updates each variable by minimizing an upper bound of the objective function [55]).",
      "startOffset" : 145,
      "endOffset" : 149
    }, {
      "referenceID" : 54,
      "context" : "Commonly used inexact BCD type algorithms include BCGD (block coordinate gradient descent, which updates each variable by a single gradient step [54]) and BSUM (block successive upper bound minimization, which updates each variable by minimizing an upper bound of the objective function [55]).",
      "startOffset" : 287,
      "endOffset" : 291
    }, {
      "referenceID" : 55,
      "context" : "[56, 57]).",
      "startOffset" : 0,
      "endOffset" : 8
    }, {
      "referenceID" : 56,
      "context" : "[56, 57]).",
      "startOffset" : 0,
      "endOffset" : 8
    }, {
      "referenceID" : 57,
      "context" : "[58] proposed an algorithm that could be viewed as a BSUM algorithm.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 28,
      "context" : "Just considering different choices of the blocks will lead to different algorithms for the matrix completion problem [29].",
      "startOffset" : 117,
      "endOffset" : 121
    }, {
      "referenceID" : 53,
      "context" : "In the regimes of |Ω| that the vanilla AltMin fails, G is active and the gradient updates do happen; however, instead of solving the subproblem exactly, one could perform one gradient step and the algorithm becomes the popular variant BCGD [54].",
      "startOffset" : 240,
      "endOffset" : 244
    }, {
      "referenceID" : 54,
      "context" : "Such a technique has also been used in the alternating least square algorithm for tensor decomposition [55].",
      "startOffset" : 103,
      "endOffset" : 107
    }, {
      "referenceID" : 58,
      "context" : "Note that for the two-block BCD algorithm, convergence to stationary points can be guaranteed even when the subproblems are not strongly convex [59], thus in Algorithm 2 we do not add the extra terms.",
      "startOffset" : 144,
      "endOffset" : 148
    }, {
      "referenceID" : 0,
      "context" : "The fourth algorithm we present is SGD (stochastic gradient descent) [1, 23] tailored for our problem (P1).",
      "startOffset" : 69,
      "endOffset" : 76
    }, {
      "referenceID" : 22,
      "context" : "The fourth algorithm we present is SGD (stochastic gradient descent) [1, 23] tailored for our problem (P1).",
      "startOffset" : 69,
      "endOffset" : 76
    }, {
      "referenceID" : 59,
      "context" : ", [60] for one example of such analysis).",
      "startOffset" : 2,
      "endOffset" : 6
    }, {
      "referenceID" : 60,
      "context" : "In this paper we only consider the cyclic order, and use a standard stepsize rule for SGD [61, 62] which requires the stepsizes {ηk} to go to zero as k → ∞, but neither too fast nor too slow (this choice guarantees convergence to stationary points even for nonconvex problems).",
      "startOffset" : 90,
      "endOffset" : 98
    }, {
      "referenceID" : 61,
      "context" : "In this paper we only consider the cyclic order, and use a standard stepsize rule for SGD [61, 62] which requires the stepsizes {ηk} to go to zero as k → ∞, but neither too fast nor too slow (this choice guarantees convergence to stationary points even for nonconvex problems).",
      "startOffset" : 90,
      "endOffset" : 98
    }, {
      "referenceID" : 3,
      "context" : "Similar to the results for nuclear norm minimization [4–7], the probability is taken with respect to the random choice of Ω, and the result also applies to a uniform random model of Ω.",
      "startOffset" : 53,
      "endOffset" : 58
    }, {
      "referenceID" : 4,
      "context" : "Similar to the results for nuclear norm minimization [4–7], the probability is taken with respect to the random choice of Ω, and the result also applies to a uniform random model of Ω.",
      "startOffset" : 53,
      "endOffset" : 58
    }, {
      "referenceID" : 5,
      "context" : "Similar to the results for nuclear norm minimization [4–7], the probability is taken with respect to the random choice of Ω, and the result also applies to a uniform random model of Ω.",
      "startOffset" : 53,
      "endOffset" : 58
    }, {
      "referenceID" : 6,
      "context" : "Similar to the results for nuclear norm minimization [4–7], the probability is taken with respect to the random choice of Ω, and the result also applies to a uniform random model of Ω.",
      "startOffset" : 53,
      "endOffset" : 58
    }, {
      "referenceID" : 3,
      "context" : "As demonstrated in [4] (and proved in [5, Theorem 1.",
      "startOffset" : 19,
      "endOffset" : 22
    }, {
      "referenceID" : 3,
      "context" : "log n factor is due to the coupon collector effect [4].",
      "startOffset" : 51,
      "endOffset" : 54
    }, {
      "referenceID" : 18,
      "context" : "the one proposed in [19]) can reduce the exponents of r and κ.",
      "startOffset" : 20,
      "endOffset" : 24
    }, {
      "referenceID" : 3,
      "context" : "1 in [4]).",
      "startOffset" : 5,
      "endOffset" : 8
    }, {
      "referenceID" : 30,
      "context" : "‖PΩ(b)‖F can be bounded according to a random graph lemma of [31, 46], which requires U,V, X,Y to be incoherent (i.",
      "startOffset" : 61,
      "endOffset" : 69
    }, {
      "referenceID" : 45,
      "context" : "‖PΩ(b)‖F can be bounded according to a random graph lemma of [31, 46], which requires U,V, X,Y to be incoherent (i.",
      "startOffset" : 61,
      "endOffset" : 69
    }, {
      "referenceID" : 3,
      "context" : "1 in [4], for |Ω| satisfying (27) with large enough C0, we have that with probability at least 1−1/(2n4), ‖PTPΩPT (a) − pPT (a)‖F ≤ 6 p‖a‖F (note that this bound holds uniformly for all a ∈ T , thus also holds when a is dependent on Ω).",
      "startOffset" : 5,
      "endOffset" : 8
    }, {
      "referenceID" : 44,
      "context" : "Such a result bears some similarity with the classical perturbation theory for singular value decomposition [45].",
      "startOffset" : 108,
      "endOffset" : 112
    }, {
      "referenceID" : 44,
      "context" : "In particular, [45] proved that for two low-rank matrices4 that are close, the spaces spanned by the left (resp.",
      "startOffset" : 15,
      "endOffset" : 19
    }, {
      "referenceID" : 44,
      "context" : "To resolve this issue, we need to prove the second proposition in which there is an additional 4The result in [45] also covered the case of two approximately low-rank matrices, but we only consider the case of exact low-rank matrices here.",
      "startOffset" : 110,
      "endOffset" : 114
    }, {
      "referenceID" : 54,
      "context" : "This definition is motivated by the block successive upper bound minimization method [55].",
      "startOffset" : 85,
      "endOffset" : 89
    }, {
      "referenceID" : 0,
      "context" : "and {xt} satisfies either of the following three conditions: 1) F̃(xt + λ∆t) ≤ 2F̃(x0),∀ λ ∈ [0, 1], where ∆t = xt+1 − xt, ∀ t; (67a) 2) 1 = arg min λ∈R ψ(xt,∆t; λ), where ψ satisfies (65),∆t = xt+1 − xt, ∀ t; (67b) 3) F̃(xt) ≤ 2F̃(x0), d(xt,x0) ≤ 5 6 δ, ∀ t.",
      "startOffset" : 93,
      "endOffset" : 99
    }, {
      "referenceID" : 30,
      "context" : "1 can be found in [31].",
      "startOffset" : 18,
      "endOffset" : 22
    }, {
      "referenceID" : 0,
      "context" : "Since d(x,u∗) is a continuous function over x, the relation d(xt,u) ≤ 3δ and (230) imply that there must exist some x′ = (1− λ)xt+1 + λxt, λ ∈ [0, 1] such that d(x′,u∗) = δ.",
      "startOffset" : 143,
      "endOffset" : 149
    }, {
      "referenceID" : 0,
      "context" : "Since d(x,u∗) is a continuous function over x and d(x′,u∗) ≤ δ (230) < d(xt+1,u), there must exist some x′′ = (1 − )xt+1 + x′= xt + (1 − + λ)∆t, ∈ [0, 1] such that d(x′′,u∗) = δ.",
      "startOffset" : 147,
      "endOffset" : 153
    }, {
      "referenceID" : 0,
      "context" : ", F̃(xt−1 + λ∆t−1) ≤ 2F̃(x0),∀λ ∈ [0, 1], where ∆t = xt − xt−1.",
      "startOffset" : 34,
      "endOffset" : 40
    }, {
      "referenceID" : 0,
      "context" : "= F̃(xt) + ‖∇F̃(xt)‖( L1 2 λ2η2 − λη) ≤ F̃(xt) − λη 2 ‖∇F̃(xt)‖ ≤ F̃(xt) ≤ 2F̃(x0), ∀ λ ∈ [0, 1], (239)",
      "startOffset" : 90,
      "endOffset" : 96
    }, {
      "referenceID" : 0,
      "context" : ", F̃(xk + λ∆k) ≤ 2F̃(x0),∀λ ∈ [0, 1], where ∆k = xk+1 − xk, 0 ≤ k ≤ t − 1.",
      "startOffset" : 30,
      "endOffset" : 36
    }, {
      "referenceID" : 0,
      "context" : "F̃(xt + λ(xt+1 − xt)) ≤ 2F̃(x0), ∀ λ ∈ [0, 1],",
      "startOffset" : 39,
      "endOffset" : 45
    }, {
      "referenceID" : 49,
      "context" : "11) of [50]).",
      "startOffset" : 7,
      "endOffset" : 11
    }, {
      "referenceID" : 49,
      "context" : "17) in [50]).",
      "startOffset" : 7,
      "endOffset" : 11
    }, {
      "referenceID" : 49,
      "context" : "17) in [50]).",
      "startOffset" : 7,
      "endOffset" : 11
    }, {
      "referenceID" : 49,
      "context" : "17) in [50] (except that (1.",
      "startOffset" : 7,
      "endOffset" : 11
    }, {
      "referenceID" : 49,
      "context" : "17) in [50] considers a more general descent direction), and the rest of the proof is also the same as [50] and is omitted here.",
      "startOffset" : 7,
      "endOffset" : 11
    }, {
      "referenceID" : 49,
      "context" : "17) in [50] considers a more general descent direction), and the rest of the proof is also the same as [50] and is omitted here.",
      "startOffset" : 103,
      "endOffset" : 107
    }, {
      "referenceID" : 54,
      "context" : "Algorithm 3 belongs to the class of BSUM methods [55].",
      "startOffset" : 49,
      "endOffset" : 53
    }, {
      "referenceID" : 54,
      "context" : "It is easy to verify that the objective function of each subproblem in Algorithm 3 is a convex tight upper bound of F̃(x) (more precisely, satisfies Assumption 2 in [55]).",
      "startOffset" : 165,
      "endOffset" : 169
    } ],
    "year" : 2016,
    "abstractText" : "Matrix factorization is a popular approach for large-scale matrix completion. The optimization formulation based on matrix factorization can be solved very efficiently by standard algorithms in practice. However, due to the non-convexity caused by the factorization model, there is a limited theoretical understanding of this formulation. In this paper, we establish a theoretical guarantee for the factorization formulation to correctly recover the underlying low-rank matrix. In particular, we show that under similar conditions to those in previous works, many standard optimization algorithms converge to the global optima of a factorization formulation, and recover the true lowrank matrix. We study the local geometry of a properly regularized factorization formulation and prove that any stationary point in a certain local region is globally optimal. A major difference of our work from the existing results is that we do not need resampling in either the algorithm or its analysis. Compared to other works on nonconvex optimization, one extra difficulty lies in analyzing nonconvex constrained optimization when the constraint (or the corresponding regularizer) is not “consistent” with the gradient direction. One technical contribution is the perturbation analysis for non-symmetric matrix factorization.",
    "creator" : "LaTeX with hyperref package"
  }
}