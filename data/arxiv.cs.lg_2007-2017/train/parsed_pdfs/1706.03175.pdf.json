{
  "name" : "1706.03175.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Recovery Guarantees for One-hidden-layer Neural Networks∗",
    "authors" : [ "Kai Zhong", "Zhao Song", "Peter L. Bartlett", "Inderjit S. Dhillon" ],
    "emails" : [ "zhongkai@ices.utexas.edu", "zhaos@utexas.edu", "prajain@microsoft.com", "bartlett@cs.berkeley.edu", "inderjit@cs.utexas.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "∗A preliminary version of this paper appears in Proceedings of the Thirty-fourth International Conference on Machine Learning (ICML 2017). †Supported in part by NSF grants CCF-1320746, IIS-1546452 and CCF-1564000, and part of the work was done while interning in Microsoft research, India. ‡Supported in part by UTCS TAship (CS361 Spring 17 Introduction to Computer Security). §Supported in part by Australian Research Council through an Australian Laureate Fellowship (FL110100281) and through the Australian Research Council Centre of Excellence for Mathematical and Statistical Frontiers (ACEMS), and NSF grants IIS-1619362. ¶Supported in part by NSF grants CCF-1320746, IIS-1546452 and CCF-1564000.\nar X\niv :1\n70 6.\n03 17\n5v 1\n[ cs\n.L G\n] 1\n0 Ju"
    }, {
      "heading" : "Contents",
      "text" : ""
    }, {
      "heading" : "1 Introduction 3",
      "text" : ""
    }, {
      "heading" : "2 Related Work 4",
      "text" : "2.1 Expressive Power . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 2.2 Achievability of Global Optima . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 2.3 Generalization Bound / Recovery Guarantees . . . . . . . . . . . . . . . . . . . . . . 5"
    }, {
      "heading" : "3 Problem Formulation 6",
      "text" : ""
    }, {
      "heading" : "4 Positive Definiteness of Hessian 7",
      "text" : ""
    }, {
      "heading" : "5 Tensor Methods for Initialization 9",
      "text" : "5.1 Preliminary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9 5.2 Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10 5.3 Theoretical Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11"
    }, {
      "heading" : "6 Global Convergence 11",
      "text" : ""
    }, {
      "heading" : "7 Numerical Experiments 12",
      "text" : ""
    }, {
      "heading" : "8 Conclusion 13",
      "text" : "References 14"
    }, {
      "heading" : "A Notation 19",
      "text" : ""
    }, {
      "heading" : "B Preliminaries 19",
      "text" : "B.1 Useful Facts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19 B.2 Matrix Bernstein . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22"
    }, {
      "heading" : "C Properties of Activation Functions 25",
      "text" : ""
    }, {
      "heading" : "D Local Positive Definiteness of Hessian 26",
      "text" : "D.1 Main Results for Positive Definiteness of Hessian . . . . . . . . . . . . . . . . . . . . 26\nD.1.1 Bounding the Spectrum of the Hessian near the Ground Truth . . . . . . . . 26 D.1.2 Local Linear Convergence of Gradient Descent . . . . . . . . . . . . . . . . . 27\nD.2 Positive Definiteness of Population Hessian at the Ground Truth . . . . . . . . . . . 28 D.2.1 Lower Bound on the Eigenvalues of Hessian for the Orthogonal Case . . . . . 29 D.2.2 Lower Bound on the Eigenvalues of Hessian for Non-orthogonal Case . . . . . 33 D.2.3 Upper Bound on the Eigenvalues of Hessian for Non-orthogonal Case . . . . . 37 D.3 Error Bound of Hessians near the Ground Truth for Smooth Activations . . . . . . . 38 D.3.1 Second-order Smoothness near the Ground Truth for Smooth Activations . . 38 D.3.2 Empirical and Population Difference for Smooth Activations . . . . . . . . . . 42 D.4 Error Bound of Hessians near the Ground Truth for Non-smooth Activations . . . . . 48 D.5 Positive Definiteness for a Small Region . . . . . . . . . . . . . . . . . . . . . . . . . 51"
    }, {
      "heading" : "E Tensor Methods 55",
      "text" : "E.1 Tensor Initialization Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55 E.2 Main Result for Tensor Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57 E.3 Error Bound for the Subspace Spanned by the Weight Matrix . . . . . . . . . . . . . 59\nE.3.1 Error Bound for the Second-order Moment in Different Cases . . . . . . . . . 59 E.3.2 Error Bound for the Second-order Moment . . . . . . . . . . . . . . . . . . . . 63 E.3.3 Subspace Estimation Using Power Method . . . . . . . . . . . . . . . . . . . . 63\nE.4 Error Bound for the Reduced Third-order Moment . . . . . . . . . . . . . . . . . . . 66 E.4.1 Error Bound for the Reduced Third-order Moment in Different Cases . . . . . 66 E.4.2 Final Error Bound for the Reduced Third-order Moment . . . . . . . . . . . . 70 E.5 Error Bound for the Magnitude and Sign of the Weight Vectors . . . . . . . . . . . . 71 E.5.1 Robustness for Solving Linear Systems . . . . . . . . . . . . . . . . . . . . . . 71 E.5.2 Error Bound for the First-order Moment . . . . . . . . . . . . . . . . . . . . . 71 E.5.3 Linear System for the First-order Moment . . . . . . . . . . . . . . . . . . . . 73 E.5.4 Linear System for the Second-order Moment . . . . . . . . . . . . . . . . . . . 74\nF Acknowledgments 75"
    }, {
      "heading" : "1 Introduction",
      "text" : "Neural Networks (NNs) have achieved great practical success recently. Many theoretical contributions have been made very recently to understand the extraordinary performance of NNs. The remarkable results of NNs on complex tasks in computer vision and natural language processing inspired works on the expressive power of NNs [CSS16, CS16, RPK+16, DFS16, PLR+16, MPCB14, Tel16]. Indeed, several works found NNs are very powerful and the deeper the more powerful. However, due to the high non-convexity of NNs, knowing the expressivity of NNs doesn’t guarantee that the targeted functions will be learned. Therefore, several other works focused on the achievability of global optima. Many of them considered the over-parameterized setting, where the global optima or local minima close to the global optima will be achieved when the number of parameters is large enough, including [FB16, HV15, LSSS14, DPG+14, SS16, HM17]. This, however, leads to overfitting easily and can’t provide any generalization guarantees, which are actually the essential goal in most tasks.\nA few works have considered generalization performance. For example, [XLS17] provide generalization bound under the Rademacher generalization analysis framework. Recently [ZBH+17] describe some experiments showing that NNs are complex enough that they actually memorize the training data but still generalize well. As they claim, this cannot be explained by applying generalization analysis techniques, like VC dimension and Rademacher complexity, to classification loss (although it does not rule out a margins analysis—see, for example, [Bar98]; their experiments involve the unbounded cross-entropy loss).\nIn this paper, we don’t develop a new generalization analysis. Instead we focus on parameter recovery setting, where we assume there are underlying ground-truth parameters and we provide recovery guarantees for the ground-truth parameters up to equivalent permutations. Since the parameters are exactly recovered, the generalization performance will also be guaranteed.\nSeveral other techniques are also provided to recover the parameters or to guarantee generalization performance, such as tensor methods [JSA15] and kernel methods [AGMR17]. These methods require sample complexity O(d3) or computational complexity Õ(n2), which can be intractable in practice. We propose an algorithm that has recovery guarantees for 1NN with sample complexity Õ(d) and computational time Õ(dn) under some mild assumptions.\nRecently [Sha16] show that neither specific assumptions on the niceness of the input distribution or niceness of the target function alone is sufficient to guarantee learnability using gradient-based methods. In this paper, we assume data points are sampled from Gaussian distribution and the parameters of hidden neurons are linearly independent.\nOur main contributions are as follows,\n1. We distill some properties for activation functions, which are satisfied by a wide range of activations, including ReLU, squared ReLU, sigmoid and tanh. With these properties we show positive definiteness (PD) of the Hessian in the neighborhood of the ground-truth parameters given enough samples (Theorem 4.2). Further, for activations that are also smooth, we show local linear convergence is guaranteed using gradient descent.\n2. We propose a tensor method to initialize the parameters such that the initialized parameters fall into the local positive definiteness area. Our contribution is that we reduce the sample/computational complexity from cubic dependency on dimension to linear dependency (Theorem 5.6).\n3. Combining the above two results, we provide a globally converging algorithm (Algorithm 2) for smooth homogeneous activations satisfying the distilled properties. The whole procedure\nrequires sample/computational complexity linear in dimension and logarithmic in precision (Theorem 6.1)."
    }, {
      "heading" : "2 Related Work",
      "text" : "The recent empirical success of NNs has boosted their theoretical analyses [FZK+16, Bal16, BMBY16, SBL16, APVZ14, AGMR17, GKKT17]. In this paper, we classify them into three main directions."
    }, {
      "heading" : "2.1 Expressive Power",
      "text" : "Expressive power is studied to understand the remarkable performance of neural networks on complex tasks. Although one-hidden-layer neural networks with sufficiently many hidden nodes can approximate any continuous function [Hor91], shallow networks can’t achieve the same performance in practice as deep networks. Theoretically, several recent works show the depth of NNs plays an essential role in the expressive power of neural networks [DFS16]. As shown in [CSS16, CS16, Tel16], functions that can be implemented by a deep network of polynomial size require exponential size in order to be implemented by a shallow network. [RPK+16, PLR+16, MPCB14, AGMR17] design some measures of expressivity that display an exponential dependence on the depth of the network. However, the increasing of the expressivity of NNs or its depth also increases the difficulty of the learning process to achieve a good enough model. In this paper, we focus on 1NNs and provide recovery guarantees using a finite number of samples."
    }, {
      "heading" : "2.2 Achievability of Global Optima",
      "text" : "The global convergence is in general not guaranteed for NNs due to their non-convexity. It is widely believed that training deep models using gradient-based methods works so well because the error surface either has no local minima, or if they exist they need to be close in value to the global minima. [SCP16] present examples showing that for this to be true additional assumptions on the data, initialization schemes and/or the model classes have to be made. Indeed the achievability of global optima has been shown under many different types of assumptions.\nIn particular, [CHM+15] analyze the loss surface of a special random neural network through spin-glass theory and show that it has exponentially many local optima, whose loss is small and close to that of a global optimum. Later on, [Kaw16] eliminate some assumptions made by [CHM+15] but still require the independence of activations as [CHM+15], which is unrealistic. [SS16] study the geometric structure of the neural network objective function. They have shown that with high probability random initialization will fall into a basin with a small objective value when the network is over-parameterized. [LSSS14] consider polynomial networks where the activations are square functions, which are typically not used in practice. [HV15] show that when a local minimum has zero parameters related to a hidden node, a global optimum is achieved. [FB16] study the landscape of 1NN in terms of topology and geometry, and show that the level set becomes connected as the network is increasingly over-parameterized. [HM17] show that products of matrices don’t have spurious local minima and that deep residual networks can represent any function on a sample, as long as the number of parameters is larger than the sample size. [SC16] consider over-specified NNs, where the number of samples is smaller than the number of weights. [DPG+14] propose a new approach to second-order optimization that identifies and attacks the saddle point problem in high-dimensional non-convex optimization. They apply the approach to recurrent neural networks and show practical performance. [AGMR17] use results from tropical geometry to show global optimality of an algorithm, but it requires (2n)k poly(n) computational complexity.\nAlmost all of these results require the number of parameters is larger than the number of points, which probably overfits the model and no generalization performance will be guaranteed. In this paper, we propose an efficient and provable algorithm for 1NNs that can achieve the underlying ground-truth parameters."
    }, {
      "heading" : "2.3 Generalization Bound / Recovery Guarantees",
      "text" : "The achievability of global optima of the objective from the training data doesn’t guarantee the learned model to be able to generalize well on unseen testing data. In the literature, we find three main approaches to generalization guarantees.\n1) Use generalization analysis frameworks, including VC dimension/Rademacher complexity, to bound the generalization error. A few works have studied the generalization performance for NNs. [XLS17] follow [SC16] but additionally provide generalization bounds using Rademacher complexity. They assume the obtained parameters are in a regularization set so that the generalization performance is guaranteed, but this assumption can’t be justified theoretically. [HRS16] apply stability analysis to the generalization analysis of SGD for convex and non-convex problems, arguing early stopping is important for generalization performance.\n2) Assume an underlying model and try to recover this model. This direction is popular for many non-convex problems including matrix completion/sensing [JNS13, Har14, SL15, BLWZ17], mixed linear regression [ZJD16], subspace recovery [EV09] and other latent models [AGH+14].\nWithout making any assumptions, those non-convex problems are intractable [AGKM12, GV15, SWZ17a, GG11, RSW16, SR11, HM13, AGM12, YCS14]. Recovery guarantees for NNs also need assumptions. Several different approaches under different assumptions are provided to have recovery guarantees on different NN settings.\nTensor methods [AGH+14, WTSA15, WA16, SWZ16] are a general tool for recovering models with latent factors by assuming the data distribution is known. Some existing recovery guarantees for NNs are provided by tensor methods [SA15, JSA15]. However, [SA15] only provide guarantees to recover the subspace spanned by the weight matrix and no sample complexity is given, while [JSA15] require O(d3/ 2) sample complexity. In this paper, we use tensor methods as an initialization step so that we don’t need very accurate estimation of the moments, which enables us to reduce the total sample complexity from 1/ 2 to log(1/ ).\n[ABGM14] provide polynomial sample complexity and computational complexity bounds for learning deep representations in unsupervised setting, and they need to assume the weights are sparse and randomly distributed in [−1, 1].\n[Tia17] analyze 1NN by assuming Gaussian inputs in a supervised setting, in particular, regression and classification with a teacher. This paper also considers this setting. However, there are some key differences. a) [Tia17] require the second-layer parameters are all ones, while we can learn these parameters. b) In [Tia17], the ground-truth first-layer weight vectors are required to be orthogonal, while we only require linear independence. c) [Tia17] require a good initialization but doesn’t provide initialization methods, while we show the parameters can be efficiently initialized by tensor methods. d) In [Tia17], only the population case (infinite sample size) is considered, so there is no sample complexity analysis, while we show finite sample complexity.\nRecovery guarantees for convolution neural network with Gaussian inputs are provided in [BG17], where they show a globally converging guarantee of gradient descent on a one-hidden-layer nooverlap convolution neural network. However, they consider population case, so no sample complexity is provided. Also their analysis depends on ReLU activations and the no-overlap case is very unlikely to be used in practice. In this paper, we consider a large range of activation functions, but for one-hidden-layer fully-connected NNs.\n3) Improper Learning. In the improper learning setting for NNs, the learning algorithm is not restricted to output a NN, but only should output a prediction function whose error is not much larger than the error of the best NN among all the NNs considered. [ZLJ16, ZLW16] propose kernel methods to learn the prediction function which is guaranteed to have generalization performance close to that of the NN. However, the sample complexity and computational complexity are exponential. [AZS14] transform NNs to convex semi-definite programming. The works by [Bac14] and [BRV+05] are also in this direction. However, these methods are actually not learning the original NNs. Another work by [ZLWJ17] uses random initializations to achieve arbitrary small excess risk. However, their algorithm has exponential running time in 1/ .\nRoadmap. The paper is organized as follows. In Section 3, we present our problem setting and show three key properties of activations required for our guarantees. In Section 4, we introduce the formal theorem of local strong convexity and show local linear convergence for smooth activations. Section 5 presents a tensor method to initialize the parameters so that they fall into the basin of the local strong convexity region."
    }, {
      "heading" : "3 Problem Formulation",
      "text" : "We consider the following regression problem. Given a set of n samples\nS = {(x1, y1), (x2, y2), · · · (xn, yn)} ⊂ Rd × R,\nlet D denote a underlying distribution over Rd × R with parameters\n{w∗1, w∗2, · · ·w∗k} ⊂ Rd, and {v∗1, v∗2, · · · , v∗k} ⊂ R\nsuch that each sample (x, y) ∈ S is sampled i.i.d. from this distribution, with\nD : x ∼ N (0, I), y = k∑ i=1 v∗i · φ(w∗>i x), (1)\nwhere φ(z) is the activation function, k is the number of nodes in the hidden layer. The main question we want to answer is: How many samples are sufficient to recover the underlying parameters?\nIt is well-known that, training one hidden layer neural network is NP-complete [BR88]. Thus, without making any assumptions, learning deep neural network is intractable. Throughout the paper, we assume x follows a standard normal distribution; the data is noiseless; the dimension of input data is at least the number of hidden nodes; and activation function φ(z) satisfies some reasonable properties.\nActually our results can be easily extended to multivariate Gaussian distribution with positive definite covariance and zero mean since we can estimate the covariance first and then transform the input to a standard normal distribution but with some loss of accuracy. Although this paper focuses on the regression problem, we can transform classification problems to regression problems if a good teacher is provided as described in [Tia17]. Our analysis requires k to be no greater than d, since the first-layer parameters will be linearly dependent otherwise.\nFor activation function φ(z), we assume it is continuous and if it is non-smooth let its first derivative be left derivative. Furthermore, we assume it satisfies Property 3.1, 3.2, and 3.3. These properties are critical for the later analyses. We also observe that most activation functions actually satisfy these three properties.\nProperty 3.1. The first derivative φ′(z) is nonnegative and homogeneously bounded, i.e., 0 ≤ φ′(z) ≤ L1|z|p for some constants L1 > 0 and p ≥ 0.\nProperty 3.2. Let αq(σ) = Ez∼N (0,1)[φ′(σ · z)zq],∀q ∈ {0, 1, 2}, and βq(σ) = Ez∼N (0,1)[φ′2(σ · z)zq],∀q ∈ {0, 2}. Let ρ(σ) denote min{β0(σ) − α20(σ) − α21(σ), β2(σ) − α21(σ) − α22(σ), α0(σ) · α2(σ)− α21(σ)} The first derivative φ′(z) satisfies that, for all σ > 0, we have ρ(σ) > 0.\nProperty 3.3. The second derivative φ′′(z) is either (a) globally bounded |φ′′(z)| ≤ L2 for some constant L2, i.e., φ(z) is L2-smooth, or (b) φ′′(z) = 0 except for e (e is a finite constant) points.\nRemark 3.4. The first two properties are related to the first derivative φ′(z) and the last one is about the second derivative φ′′(z). At high level, Property 3.1 requires φ to be non-decreasing with homogeneously bounded derivative; Property 3.2 requires φ to be highly non-linear; Property 3.3 requires φ to be either smooth or piece-wise linear.\nTheorem 3.5. ReLU φ(z) = max{z, 0}, leaky ReLU φ(z) = max{z, 0.01z}, squared ReLU φ(z) = max{z, 0}2 and any non-linear non-decreasing smooth functions with bounded symmetric φ′(z), like the sigmoid function φ(z) = 1/(1 + e−z), the tanh function and the erf function φ(z) = ∫ z 0 e −t2dt, satisfy Property 3.1,3.2,3.3. The linear function, φ(z) = z, doesn’t satisfy Property 3.2 and the quadratic function, φ(z) = z2, doesn’t satisfy Property 3.1 and 3.2.\nThe proof can be found in Appendix C."
    }, {
      "heading" : "4 Positive Definiteness of Hessian",
      "text" : "In this section, we study the Hessian of empirical risk near the ground truth. We consider the case when v∗ is already known. Note that for homogeneous activations, we can assume v∗i ∈ {−1, 1} since vφ(z) = v|v|φ(|v|\n1/pz), where p is the degree of homogeneity. As v∗i only takes discrete values for homogeneous activations, in the next section, we show we can exactly recover v∗ using tensor methods with finite samples.\nFor a set of samples S, we define the Empirical Risk,\nf̂S(W ) = 1 2|S| ∑\n(x,y)∈S ( k∑ i=1 v∗i φ(w > i x)− y )2 . (2)\nFor a distribution D, we define the Expected Risk,\nfD(W ) = 1\n2 E (x,y)∼D ( k∑ i=1 v∗i φ(w > i x)− y )2 . (3) Let’s calculate the gradient and the Hessian of f̂S(W ) and fD(W ). For each j ∈ [k], the partial gradient of fD(W ) with respect to wj can be represented as\n∂fD(W )\n∂wj = E (x,y)∼D [( k∑ i=1 v∗i φ(w > i x)− y ) v∗jφ ′(w>j x)x ] .\nFor each j, l ∈ [k] and j 6= l, the second partial derivative of fD(W ) for the (j, l)-th off-diagonal block is,\n∂2fD(W )\n∂wj∂wl = E (x,y)∼D\n[ v∗j v ∗ l φ ′(w>j x)φ ′(w>l x)xx > ] ,\nand for each j ∈ [k], the second partial derivative of fD(W ) for the j-th diagonal block is\n∂2fD(W )\n∂w2j = E (x,y)∼D [( k∑ i=1 v∗i φ(w > i x)− y ) v∗jφ ′′(w>j x)xx > + (v∗jφ ′(w>j x)) 2xx> ] .\nIf φ(z) is non-smooth, we use the Dirac function and its derivatives to represent φ′′(z). Replacing the expectation E(x,y)∼D by the average over the samples |S|−1 ∑ (x,y)∈S , we obtain the Hessian of the empirical risk. Considering the case when W = W ∗ ∈ Rd×k, for all j, l ∈ [k], we have,\n∂2fD(W ∗)\n∂wj∂wl = E (x,y)∼D\n[ v∗j v ∗ l φ ′(w∗>j x)φ ′(w∗>l x)xx > ] .\nIf Property 3.3(b) is satisfied, φ′′(z) = 0 almost surely. So in this case the diagonal blocks of the empirical Hessian can be written as,\n∂2f̂S(W )\n∂w2j =\n1 |S| ∑\n(x,y)∈S\n(v∗jφ ′(w>j x)) 2xx>.\nNow we show the Hessian of the objective near the global optimum is positive definite.\nDefinition 4.1. Given the ground truth matrix W ∗ ∈ Rd×k, let σi(W ∗) denote the i-th singular value of W ∗, often abbreviated as σi. Let κ = σ1/σk, λ = ( ∏k i=1 σi)/σ k k . Let vmax denote maxi∈[k] |v∗i | and vmin denote mini∈[k] |v∗i | . Let ν = vmax/vmin. Let ρ denote ρ(σk). Let τ = (3σ1/2) 4p/minσ∈[σk/2,3σ1/2]{ρ 2(σ)}.\nTheorem 4.2 (Informal version of Theorem D.1). For any W ∈ Rd×k with ‖W −W ∗‖ ≤ poly(1/k, 1/λ, 1/ν, ρ/σ2p1 ) · ‖W ∗‖, let S denote a set of i.i.d. samples from distribution D (defined in (1)) and let the activation function satisfy Property 3.1,3.2,3.3. Then for any t ≥ 1, if |S| ≥ d · poly(log d, t, k, ν, τ, λ, σ2p1 /ρ), we have with probability at least 1− d−Ω(t),\nΩ(v2minρ(σk)/(κ 2λ))I ∇2f̂S(W ) O(kv2maxσ 2p 1 )I.\nRemark 4.3. As we can see from Theorem 4.2, ρ(σk) from Property 3.2 plays an important role for positive definite (PD) property. Interestingly, many popular activations, like ReLU, sigmoid and tanh, have ρ(σk) > 0, while some simple functions like linear (φ(z) = z) and square (φ(z) = z2) functions have ρ(σk) = 0 and their Hessians are rank-deficient. Another important numbers are κ and λ, two different condition numbers of the weight matrix, which directly influences the positive definiteness. If W ∗ is rank deficient, λ → ∞, κ → ∞ and we don’t have PD property. In the best case when W ∗ is orthogonal, λ = κ = 1. In the worse case, λ can be exponential in k. Also W should be close enough to W ∗. In the next section, we provide tensor methods to initialize w∗i and v∗i such that they satisfy the conditions in Theorem 4.2.\nFor the PD property to hold, we need the samples to be independent of the current parameters. Therefore, we need to do resampling at each iteration to guarantee the convergence in iterative algorithms like gradient descent. The following theorem provides the linear convergence guarantee of gradient descent for smooth activations.\nTheorem 4.4 (Linear convergence of gradient descent, informal version of Theorem D.2). Let W be the current iterate satisfying ‖W −W ∗‖ ≤ poly(1/ν, 1/k, 1/λ, ρ/σ2p1 )‖W ∗‖. Let S denote a set\nof i.i.d. samples from distribution D (defined in (1)) with |S| ≥ d ·poly(log d, t, k, ν, τ, λ, σ2p1 /ρ) and let the activation function satisfy Property 3.1,3.2 and 3.3(a). Define m0 := Θ(v2minρ(σk)/(κ 2λ)) and M0 := Θ(kv2maxσ 2p 1 ). If we perform gradient descent with step size 1/M0 on f̂S(W ) and obtain the next iterate,\nW̃ = W − 1 M0 ∇f̂S(W ),\nthen with probability at least 1− d−Ω(t),\n‖W̃ −W ∗‖2F ≤ (1− m0 M0 )‖W −W ∗‖2F .\nWe provide the proofs in the Appendix D.1"
    }, {
      "heading" : "5 Tensor Methods for Initialization",
      "text" : "In this section, we show that Tensor methods can recover the parameters W ∗ to some precision and exactly recover v∗ for homogeneous activations.\nIt is known that most tensor problems are NP-hard [Hås90, HL13] or even hard to approximate [SWZ17b]. However, by making some assumptions, tensor decomposition method becomes efficient [AGH+14, WTSA15, WA16, SWZ16]. Here we utilize the noiseless assumption and Gaussian inputs assumption to show a provable and efficient tensor methods."
    }, {
      "heading" : "5.1 Preliminary",
      "text" : "Let’s define a special outer product ⊗̃ for simplification of the notation. If v ∈ Rd is a vector and I is the identity matrix, then v⊗̃I = ∑d j=1[v⊗ ej ⊗ ej + ej ⊗ v⊗ ej + ej ⊗ ej ⊗ v]. If M is a symmetric\nrank-r matrix factorized as M = ∑r\ni=1 siviv > i and I is the identity matrix, then M⊗̃I = r∑ i=1 si d∑ j=1 6∑ l=1 Al,i,j ,\nwhere A1,i,j = vi⊗vi⊗ej⊗ej , A2,i,j = vi⊗ej⊗vi⊗ej , A3,i,j = ej⊗vi⊗vi⊗ej , A4,i,j = vi⊗ej⊗ej⊗vi, A5,i,j = ej ⊗ vi ⊗ ej ⊗ vi and A6,i,j = ej ⊗ ej ⊗ vi ⊗ vi.\nDenote w = w/‖w‖. Now let’s calculate some moments.\nDefinition 5.1. We define M1,M2,M3,M4 and m1,i,m2,i,m3,i,m4,i as follows : M1 = E(x,y)∼D[y · x]. M2 = E(x,y)∼D[y · (x⊗ x− I)]. M3 = E(x,y)∼D[y · (x⊗3 − x⊗̃I)]. M4 = E(x,y)∼D[y · (x⊗4 − (x⊗ x)⊗̃I + I⊗̃I)]. γj(σ) = Ez∼N (0,1)[φ(σ · z)zj ], ∀j = 0, 1, 2, 3, 4. m1,i = γ1(‖w∗i ‖). m2,i = γ2(‖w∗i ‖)− γ0(‖w∗i ‖). m3,i = γ3(‖w∗i ‖)− 3γ1(‖w∗i ‖). m4,i = γ4(‖w∗i ‖) + 3γ0(‖w∗i ‖)− 6γ2(‖w∗i ‖).\nAccording to Definition 5.1, we have the following results, Claim 5.2. For each j ∈ [4], Mj = ∑k i=1 v ∗ imj,iw ∗⊗j i .\nNote that some mj,i’s will be zero for specific activations. For example, for activations with symmetric first derivatives, i.e., φ′(z) = φ′(−z), like sigmoid and erf, we have φ(z) + φ(−z) being a constant and M2 = 0 since γ0(σ) = γ2(σ). Another example is ReLU. ReLU functions have vanishing M3, i.e., M3 = 0, as γ3(σ) = 3γ1(σ). To make tensor methods work, we make the following assumption.\nAssumption 5.3. Assume the activation function φ(z) satisfies the following conditions: 1. If Mj 6= 0, then mj,i 6= 0 for all i ∈ [k]. 2. At least one of M3 and M4 is non-zero. 3. If M1 = M3 = 0, then φ(z) is an even function, i.e., φ(z) = φ(−z). 4. If M2 = M4 = 0, then φ(z) is an odd function, i.e., φ(z) = −φ(−z).\nIf φ(z) is an odd function then φ(z) = −φ(−z) and vφ(w>x) = −vφ(−w>x). Hence we can always assume v > 0. If φ(z) is an even function, then vφ(w>x) = vφ(−w>x). So if w recovers w∗ then −w also recovers w∗. Note that ReLU, leaky ReLU and squared ReLU satisfy Assumption 5.3. We further define the following non-zero moments.\nDefinition 5.4. Let α ∈ Rd denote a randomly picked vector. We define P2 and P3 as follows: P2 = Mj2(I, I, α, · · · , α) , where j2 = min{j ≥ 2|Mj 6= 0} and P3 = Mj3(I, I, I, α, · · · , α), where j3 = min{j ≥ 3|Mj 6= 0}.\nAccording to Definition 5.1 and 5.4, we have, Claim 5.5. P2 = ∑k i=1 v ∗ imj2,i(α >w∗i ) j2−2w∗⊗2i and P3 = ∑k i=1 v ∗ imj3,i(α >w∗i ) j3−3w∗⊗3i .\nIn other words for the above definition, P2 is equal to the first non-zero matrix in the ordered sequence {M2,M3(I, I, α),M4(I, I, α, α)}. P3 is equal to the first non-zero tensor in the ordered sequence {M3,M4(I, I, I, α)}. Since α is randomly picked up, w∗>i α 6= 0 and we view this number as a constant throughout this paper. So by construction and Assumption 5.3, both P2 and P3 are rank-k. Also, let P̂2 ∈ Rd×d and P̂3 ∈ Rd×d×d denote the corresponding empirical moments of P2 ∈ Rd×d and P3 ∈ Rd×d×d respectively."
    }, {
      "heading" : "5.2 Algorithm",
      "text" : "Now we briefly introduce how to use a set of samples with size linear in dimension to recover the ground truth parameters to some precision. As shown in the previous section, we have a rank-k 3rd-order moment P3 that has tensor decomposition formed by {w∗1, w∗2, · · · , w∗k}. Therefore, we can use the non-orthogonal decomposition method [KCL15] to decompose the corresponding estimated tensor P̂3 and obtain an approximation of the parameters. The precision of the obtained parameters depends on the estimation error of P3, which requires Ω(d3/ 2) samples to achieve error. Also, the time complexity for tensor decomposition on a d× d× d tensor is Ω(d3).\nIn this paper, we reduce the cubic dependency of sample/computational complexity in dimension [JSA15] to linear dependency. Our idea follows the techniques used in [ZJD16], where they first used a 2nd-order moment P2 to approximate the subspace spanned by {w∗1, w∗2, · · · , w∗k}, denoted as V , then use V to reduce a higher-dimensional third-order tensor P3 ∈ Rd×d×d to a lower-dimensional tensor R3 := P3(V, V, V ) ∈ Rk×k×k. Since the tensor decomposition and the tensor estimation are conducted on a lower-dimensional Rk×k×k space, the sample complexity and computational complexity are reduced.\nThe detailed algorithm is shown in Algorithm 1. First, we randomly partition the dataset into three subsets each with size Õ(d). Then apply the power method on P̂2, which is the estimation\nAlgorithm 1 Initialization via Tensor Method 1: procedure Initialization(S) . Theorem 5.6 2: S2, S3, S4 ← Partition(S, 3) 3: P̂2 ← ES2 [P2] 4: V ← PowerMethod(P̂2, k) 5: R̂3 ← ES3 [P3(V, V, V )] 6: {ûi}i∈[k] ← KCL(R̂3) 7: {w(0)i , v (0) i }i∈[k] ← RecMagSign(V, {ûi}i∈[k], S4)\n8: Return {w(0)i , v (0) i }i∈[k] 9: end procedure\nAlgorithm 2 Globally Converging Algorithm 1: procedure Learning1NN(S, d, k, ) . Theorem 6.1 2: T ← log(1/ ) · poly(k, ν, λ, σ2p1 /ρ). 3: η ← 1/(kv2maxσ 2p 1 ).\n4: S0, S1, · · · , Sq ← Partition(S, q + 1). 5: W (0), v(0) ← Initialization(S0). 6: Set v∗i ← v (0) i in Eq. (2) for all f̂Sq(W ), q ∈ [T ] 7: for q = 0, 1, 2, · · · , T − 1 do 8: W (q+1) = W (q) − η∇f̂Sq+1(W (q)) 9: end for\n10: Return {w(T )i , v (0) i }i∈[k] 11: end procedure\nof P2 from S2, to estimate V . After that, the non-orthogonal tensor decomposition (KCL)[KCL15] on R̂3 outputs ûi which estimates siV >w∗i for i ∈ [k] with unknown sign si ∈ {−1, 1}. Hence w∗i can be estimated by siV ûi. Finally we estimate the magnitude of w∗i and the signs si, v ∗ i in the RecMagSign function for homogeneous activations. We discuss the details of each procedure and provide PowerMethod and RecMagSign algorithms in Appendix E."
    }, {
      "heading" : "5.3 Theoretical Analysis",
      "text" : "We formally present our theorem for Algorithm 1, and provide the proof in the Appendix E.2.\nTheorem 5.6. Let the activation function be homogeneous satisfying Assumption 5.3. For any 0 < < 1 and t ≥ 1, if |S| ≥ −2 ·d ·poly(t, k, κ, log d), then there exists an algorithm (Algorithm 1) that takes |S|k · Õ(d) time and outputs a matrix W (0) ∈ Rd×k and a vector v(0) ∈ Rk such that, with probability at least 1− d−Ω(t),\n‖W (0) −W ∗‖F ≤ · poly(k, κ)‖W ∗‖F , and v(0)i = v ∗ i ."
    }, {
      "heading" : "6 Global Convergence",
      "text" : "Combining the positive definiteness of the Hessian near the global optimal in Section 4 and the tensor initialization methods in Section 5, we come up with the overall globally converging algorithm Algorithm 2 and its guarantee Theorem 6.1.\nTheorem 6.1 (Global convergence guarantees). Let S denote a set of i.i.d. samples from distribution D (defined in (1)) and let the activation function be homogeneous satisfying Property 3.1, 3.2, 3.3(a) and Assumption 5.3. Then for any t ≥ 1 and any > 0, if |S| ≥ d log(1/ )·poly(log d, t, k, λ), T ≥ log(1/ ) · poly(k, ν, λ, σ2p1 /ρ) and 0 < η ≤ 1/(kv2maxσ 2p 1 ), then there is an Algorithm (procedure Learning1NN in Algorithm 2) taking |S| · d · poly(log d, k, λ) time and outputting a matrix W (T ) ∈ Rd×k and a vector v(0) ∈ Rk satisfying\n‖W (T ) −W ∗‖F ≤ ‖W ∗‖F , and v(0)i = v ∗ i .\nwith probability at least 1− d−Ω(t).\nThis follows by combining Theorem 4.4 and Theorem 5.6."
    }, {
      "heading" : "7 Numerical Experiments",
      "text" : "In this section we use synthetic data to verify our theoretical results. We generate data points {xi, yi}i=1,2,··· ,n from Distribution D(defined in Eq. (1)). We set W ∗ = UΣV >, where U ∈ Rd×k and V ∈ Rk×k are orthogonal matrices generated from QR decomposition of Gaussian matrices, Σ is a diagonal matrix whose diagonal elements are 1, 1+κ−1k−1 , 1+ 2(κ−1) k−1 , · · · , κ. In this experiment, we set κ = 2 and k = 5. We set v∗i to be randomly picked from {−1, 1} with equal chance. We use squared ReLU φ(z) = max{z, 0}2, which is a smooth homogeneous function. For non-orthogonal tensor methods, we directly use the code provided by [KCL15] with the number of random projections fixed as L = 100. We pick the stepsize η = 0.02 for gradient descent. In the experiments, we don’t do the resampling since the algorithm still works well without resampling.\nFirst we show the number of samples required to recover the parameters for different dimensions. We fix k = 5, change d for d = 10, 20, · · · , 100 and n for n = 1000, 2000, · · · , 10000. For each pair of d and n, we run 10 trials. We say a trial successfully recovers the parameters if there exists a permutation π : [k]→ [k], such that the returned parameters W and v satisfy\nmax j∈[k] {‖w∗j − wπ(j)‖/‖w∗j‖} ≤ 0.01 and vπ(j) = v∗j .\nWe record the recovery rates and represent them as grey scale in Fig. 1(a). As we can see from Fig. 1(a), the least number of samples required to have 100% recovery rate is about proportional to the dimension.\nNext we test the tensor initialization. We show the error between the output of the tensor method and the ground truth parameters against the number of samples under different dimensions\nin Fig 1(b). The pure dark blocks indicate, in at least one of the 10 trials, ∑k\ni=1 v (0) i 6= ∑k i=1 v ∗ i ,\nwhich means v(0)i is not correctly initialized. Let Π(k) denote the set of all possible permutations π : [k]→ [k]. The grey scale represents the averaged error,\nmin π∈Π(k) max j∈[k] {‖w∗j − w (0) π(j)‖/‖w ∗ j‖},\nover 10 trials. As we can see, with a fixed dimension, the more samples we have the better initialization we obtain. We can also see that to achieve the same initialization error, the sample complexity required is about proportional to the dimension.\nWe also compare different initialization methods for gradient descent in Fig. 1(c). We fix d = 10, k = 5, n = 10000 and compare three different initialization approaches, (I) Let both v and W be initialized from tensor methods, and then do gradient descent for W while v is fixed; (II) Let both v and W be initialized from random Gaussian, and then do gradient descent for both W and v; (III) Let v = v∗ and W be initialized from random Gaussian, and then do gradient descent for W while v is fixed. As we can see from Fig 1(c), Approach (I) is the fastest and Approach (II) doesn’t converge even if more iterations are allowed. Both Approach (I) and (III) have linear convergence rate when the objective value is small enough, which verifies our local linear convergence claim."
    }, {
      "heading" : "8 Conclusion",
      "text" : "As shown in Theorem 6.1, the tensor initialization followed by gradient descent will provide a globally converging algorithm with linear time/sample complexity in dimension, logarithmic in precision and polynomial in other factors for smooth homogeneous activation functions. Our distilled properties for activation functions include a wide range of non-linear functions and hopefully provide an intuition to understand the role of non-linear activations played in optimization. Deeper neural networks and convergence for SGD will be considered in the future."
    }, {
      "heading" : "A Notation",
      "text" : "For any positive integer n, we use [n] to denote the set {1, 2, · · · , n}. For random variable X, let E[X] denote the expectation of X (if this quantity exists). For any vector x ∈ Rn, we use ‖x‖ to denote its `2 norm.\nWe provide several definitions related to matrix A. Let det(A) denote the determinant of a square matrix A. Let A> denote the transpose of A. Let A† denote the Moore-Penrose pseudoinverse of A. Let A−1 denote the inverse of a full rank square matrix. Let ‖A‖F denote the Frobenius norm of matrix A. Let ‖A‖ denote the spectral norm of matrix A. Let σi(A) to denote the i-th largest singular value of A. We often use capital letter to denote the stack of corresponding small letter vectors, e.g.,W = [w1 w2 · · · wk]. For two same-size matrices, A,B ∈ Rd1×d2 , we use A◦B ∈ Rd1×d2 to denote element-wise multiplication of these two matrices.\nWe use ⊗ to denote outer product and · to denote dot product. Given two column vectors u, v ∈ Rn, then u⊗v ∈ Rn×n and (u⊗v)i,j = ui ·vj , and u>v = ∑n i=1 uivi ∈ R. Given three column vectors u, v, w ∈ Rn, then u⊗ v⊗w ∈ Rn×n×n and (u⊗ v⊗w)i,j,k = ui · vj ·wk. We use u⊗r ∈ Rn r to denote the vector u outer product with itself r − 1 times. Tensor T ∈ Rn×n×n is symmetric if and only if for any i, j, k, Ti,j,k = Ti,k,j = Tj,i,k = Tj,k,i = Tk,i,j = Tk,j,i. Given a third order tensor T ∈ Rn1×n2×n3 and three matrices A ∈ Rn1×d1 , B ∈ Rn2×d2 , C ∈ Rn3×d3 , we use T (A,B,C) to denote a d1 × d2 × d3 tensor where the (i, j, k)-th entry is,\nn1∑ i′=1 n2∑ j′=1 n3∑ k′=1 Ti′,j′,k′Ai′,iBj′,jCk′,k.\nWe use ‖T‖ to denote the operator norm of the tensor T , i.e.,\n‖T‖ = max ‖a‖=1 |T (a, a, a)|.\nFor tensor T ∈ Rn1×n2×n3 , we use matrix T (1) ∈ Rn1×n2n3 to denote the flattening of tensor T along the first dimension, i.e., [T (1)]i,(j−1)n3+k = Ti,j,k, ∀i ∈ [n1], j ∈ [n2], k ∈ [n3]. Similarly for matrices T (2) ∈ Rn2×n3n1 and T (3) ∈ Rn3×n1n2 .\nWe use 1f to denote the indicator function, which is 1 if f holds and 0 otherwise. Let Id ∈ Rd×d denote the identity matrix. We use φ(z) to denote an activation function. We define (z)+ := max{0, z}. We use D to denote a Gaussian distribution N (0, Id) or to denote a joint distribution of (X,Y ) ∈ Rd × R, where the marginal distribution of X is N (0, Id).\nFor any function f , we define Õ(f) to be f · logO(1)(f). In addition to O(·) notation, for two functions f, g, we use the shorthand f . g (resp. &) to indicate that f ≤ Cg (resp. ≥) for an absolute constant C. We use f h g to mean cf ≤ g ≤ Cf for constants c, C."
    }, {
      "heading" : "B Preliminaries",
      "text" : "In this section, we introduce some lemmata and corollaries that will be used in the proofs."
    }, {
      "heading" : "B.1 Useful Facts",
      "text" : "We provide some facts that will be used in the later proofs.\nFact B.1. Let z denote a fixed d-dimensional vector, then for any C ≥ 1 and n ≥ 1, we have\nPr x∼N (0,Id)\n[|〈x, z〉|2 ≤ 5C‖z‖2 log n] ≥ 1− 1/(ndC).\nProof. This follows by Proposition 1.1 in [HKZ12].\nFact B.2. For any C ≥ 1 and n ≥ 1, we have\nPr x∼N (0,Id)\n[‖x‖2 ≤ 5Cd log n] ≥ 1− 1/(ndC).\nProof. This follows by Proposition 1.1 in [HKZ12].\nFact B.3. Given a full column-rank matrix W = [w1, w2, · · · , wk] ∈ Rd×k, let W = [ w1‖w1‖ , w2 ‖w2‖ , · · · , wk‖wk‖ ]. Then, we have: (I) for any i ∈ [k], σk(W ) ≤ ‖wi‖ ≤ σ1(W ); (II) 1/κ(W ) ≤ σk(W ) ≤ σ1(W ) ≤ √ k.\nProof. Part (I). We have, σk(W ) ≤ ‖Wei‖ = ‖wi‖ ≤ σ1(W )\nPart (II). We first show how to lower bound σk(W ),\nσk(W ) = min ‖s‖=1\n‖Ws‖\n= min ‖s‖=1 ∥∥∥∥∥ k∑ i=1 si ‖wi‖ wi ∥∥∥∥∥ by definition of W ≥ min ‖s‖=1 σk(W ) ( k∑ i=1 ( si ‖wi‖ )2 ) 1 2 by ‖wi‖ ≥ σk(W )\n≥ min ‖s‖2=1 σk(W ) ( k∑ i=1 ( si maxj∈[k] ‖wj‖ )2 ) 1 2\nby max j∈[k] ‖wj‖ ≥ ‖wi‖\n= σk(W )/max j∈[k] ‖wj‖ by ‖s‖ = 1\n≥ σk(W )/σ1(W ). by max j∈[k] ‖wj‖ ≤ σ1(W )\n= 1/κ(W ).\nIt remains to upper bound σ1(W ),\nσ1(W ) ≤ ( k∑ i=1 σ2i (W ) ) 1 2 = ‖W‖F ≤ √ k.\nFact B.4. Let U ∈ Rd×k and V ∈ Rd×k (k ≤ d) denote two orthogonal matrices. Then ‖UU> − V V >‖ = ‖(I − UU>)V ‖ = ‖(I − V V >)U‖ = √ 1− σ2k(U>V ).\nProof. Let U⊥ ∈ Rd×(d−k) and V⊥ ∈ Rd×(d−k) be the orthogonal complementary matrices of U, V ∈ Rd×k respectively.\n‖UU> − V V >‖ = ‖(I − V V >)UU> − V V >(I − UU>)‖ = ‖V⊥V >⊥ UU> − V V >U⊥U>⊥ ‖\n= ∥∥∥∥[V⊥ V ] [V >⊥ U 00 −V >U⊥ ] [ U> U>⊥ ]∥∥∥∥ = max(‖V >⊥ U‖, ‖V >U⊥‖).\nWe show how to simplify ‖V >⊥ U‖, ‖V >⊥ U‖ = ‖(I − V V >)U‖ = √ ‖U>(I − V V >)U‖ = max\n‖a‖=1\n√ 1− ‖V >Ua‖2 = √ 1− σ2k(V >U).\nSimilarly we can simplify ‖U>⊥V ‖, ‖U>⊥V ‖ = √ 1− σ2k(U>V ) = √ 1− σ2k(V >U).\nFact B.5. Let C ∈ Rd1×d2 , B ∈ Rd2×d3 be two matrices. Then ‖CB‖ ≤ ‖C‖‖B‖F and ‖CB‖ ≥ σmin(C)‖B‖F . Proof. For each i ∈ [d3], let bi denote the i-th column of B. We can upper bound ‖CB‖,\n‖CB‖F = ( d2∑ i=1 ‖Cbi‖2 )1/2 ≤ ( d2∑ i=1 ‖C‖2‖bi‖2 )1/2 = ‖C‖‖B‖F .\nWe show how to lower bound ‖CB‖,\n‖CB‖ = ( d2∑ i=1 ‖Cbi‖2 )1/2 ≥ ( d2∑ i=1 σ2k(C)‖bi‖2 )1/2 = σmin(C)‖B‖F .\nFact B.6. Let a, b, c ≥ 0 denote three constants, let u, v, w ∈ Rd denote three vectors, let Dd denote Gaussian distribution N (0, Id) then\nE x∼Dd\n[ |u>x|a|v>x|b|w>x|c ] h ‖u‖a‖v‖b‖w‖c.\nProof.\nE x∼Dd\n[ |u>x|a|v>x|b|w>x|c ] ≤ (\nE x∼Dd\n[|u>x|2a] )1/2 · (\nE x∼Dd\n[|u>x|4b] )1/4 · (\nE x∼Dd\n[|u>x|4c] )1/4\n. ‖u‖a‖v‖b‖w‖c,\nwhere the first step follows by Hölder’s inequality, i.e., E[|XY Z|] ≤ (E[|X|2])1/2 · (E[|Y |4])1/4 · (E[|Z|4])1/4, the third step follows by calculating the expectation and a, b, c are constants.\nSince all the three components |u>x|, |v>x|, |w>x| are positive and related to a common random vector x, we can show a lower bound,\nE x∼Dd\n[ |u>x|a|v>x|b|w>x|c ] & ‖u‖a‖v‖b‖w‖c."
    }, {
      "heading" : "B.2 Matrix Bernstein",
      "text" : "In many proofs we need to bound the difference between some population matrices/tensors and their empirical versions. Typically, the classic matrix Bernstein inequality requires the norm of the random matrix be bounded almost surely (e.g., Theorem 6.1 in [Tro12]) or the random matrix satisfies subexponential property (Theorem 6.2 in [Tro12]) . However, in our cases, most of the random matrices don’t satisfy these conditions. So we derive the following lemmata that can deal with random matrices that are not bounded almost surely or follow subexponential distribution, but are bounded with high probability.\nLemma B.7 (Matrix Bernstein for unbounded case (A modified version of bounded case, Theorem 6.1 in [Tro12])). Let B denote a distribution over Rd1×d2. Let d = d1 + d2. Let B1, B2, · · ·Bn be i.i.d. random matrices sampled from B. Let B = EB∼B[B] and B̂ = 1n ∑n i=1Bi. For parameters m ≥ 0, γ ∈ (0, 1), ν > 0, L > 0, if the distribution B satisfies the following four properties,\n(I) Pr B∼B\n[‖B‖ ≤ m] ≥ 1− γ;\n(II)\n∥∥∥∥ EB∼B[B] ∥∥∥∥ > 0;\n(III) max (∥∥∥∥ EB∼B[BB>] ∥∥∥∥ , ∥∥∥∥ EB∼B[B>B] ∥∥∥∥) ≤ ν; (IV) max\n‖a‖=‖b‖=1\n( E\nB∼B\n[( a>Bb )2])1/2 ≤ L.\nThen we have for any 0 < < 1 and t ≥ 1, if\nn ≥ (18t log d) · (ν + ‖B‖2 +m‖B‖ )/( 2‖B‖2) and γ ≤ ( ‖B‖/(2L))2\nwith probability at least 1− 1/d2t − nγ,\n‖B̂ −B‖ ≤ ‖B‖.\nProof. Define the event ξi = {‖Bi‖ ≤ m},∀i ∈ [n].\nDefine Mi = 1‖Bi‖≤mBi. Let M = EB∼B[1‖B‖≤mB] and M̂ = 1 n ∑n i=1Mi. By triangle inequality, we have\n‖B̂ −B‖ ≤ ‖B̂ − M̂‖+ ‖M̂ −M‖+ ‖M −B‖. (4)\nIn the next a few paragraphs, we will upper bound the above three terms. The first term in Eq. (4). Denote ξc as the complementary set of ξ, thus Pr[ξci ] ≤ γ. By a union bound over i ∈ [n], with probability 1− nγ, ‖Bi‖ ≤ m for all i ∈ [n]. Thus M̂ = B̂. The second term in Eq. (4). For a matrix B sampled from B, we use ξ to denote the event\nthat ξ = {‖B‖ ≤ m}. Then, we can upper bound ‖M −B‖ in the following way,\n‖M −B‖\n= ∥∥∥∥ EB∼B[1‖B‖≤m ·B]− EB∼B[B] ∥∥∥∥\n= ∥∥∥∥ EB∼B[B · 1ξc ] ∥∥∥∥\n= max ‖a‖=‖b‖=1 E B∼B\n[a>Bb1ξc ]\n≤ max ‖a‖=‖b‖=1 E B∼B [(a>Bb)2]1/2 · E B∼B [1ξc ] 1/2 by Hölder’s inequality\n≤ L E B∼B [1ξc ] 1/2 by Property (IV) ≤ Lγ1/2, by Pr[ξc] ≤ γ\n≤ 1 2 ‖B‖, by γ ≤ ( ‖B‖/(2L))2\nwhich implies\n‖M −B‖ ≤ 2 ‖B‖.\nSince < 1, we also have ‖M −B‖ ≤ 12‖B‖ and 3 2‖B‖ ≥ ‖M‖ ≥ 1 2‖B‖.\nThe third term in Eq. (4). We can bound ‖M̂−M‖ by Matrix Bernstein’s inequality [Tro12]. We define Zi = Mi −M . Thus we have E\nBi∼B [Zi] = 0, ‖Zi‖ ≤ 2m, and∥∥∥∥ EBi∼B[ZiZ>i ] ∥∥∥∥ = ∥∥∥∥ EBi∼B[MiM>i ]−M ·M> ∥∥∥∥ ≤ ν + ‖M‖2 ≤ ν + 3‖B‖2.\nSimilarly, we have ∥∥∥∥ EBi∼B[Z>i Zi] ∥∥∥∥ ≤ ν + 3‖B‖2. Using matrix Bernstein’s inequality, for any > 0, Pr\nB1,··· ,Bn∼B\n[ 1\nn ∥∥∥∥∥ n∑ i=1 Zi ∥∥∥∥∥ ≥ ‖B‖ ] ≤ d exp ( − 2‖B‖2n/2 ν + 3‖B‖2 + 2m‖B‖ /3 ) .\nBy choosing\nn ≥ (3t log d) · ν + 3‖B‖ 2 + 2m‖B‖ /3\n2‖B‖2/2 ,\nfor t ≥ 1, we have with probability at least 1− 1/d2t,∥∥∥∥∥ 1n n∑ i=1 Mi −M ∥∥∥∥∥ ≤ 2‖B‖ Putting it all together, we have for 0 < < 1, if\nn ≥ (18t log d) · (ν + ‖B‖2 +m‖B‖ )/( 2‖B‖2) and γ ≤ ( ‖B‖/(2L))2\nwith probability at least 1− 1/d2t − nγ,∥∥∥∥∥ 1n n∑ i=1 Bi − E B∼B [B] ∥∥∥∥∥ ≤ ∥∥∥∥ EB∼B[B] ∥∥∥∥ .\nCorollary B.8 (Error bound for symmetric rank-one random matrices). Let x1, x2, · · ·xn denote n i.i.d. samples drawn from Gaussian distribution N (0, Id). Let h(x) : Rd → R be a function satisfying the following properties (I), (II) and (III).\n(I) Pr x∼N (0,Id)\n[|h(x)| ≤ m] ≥ 1− γ\n(II) ∥∥∥∥ Ex∼N (0,Id)[h(x)xx>] ∥∥∥∥ > 0;\n(III) ( E\nx∼N (0,Id) [h4(x)]\n)1/4 ≤ L.\nDefine function B(x) = h(x)xx> ∈ Rd×d, ∀i ∈ [n]. Let B = E x∼N (0,Id) [h(x)xx>]. For any\n0 < < 1 and t ≥ 1, if\nn & (t log d) · (L2d+ ‖B‖2 + (mtd log n)‖B‖ )/( 2‖B‖2), and γ + 1/(nd2t) . ( ‖B‖/L)2\nthen\nPr x1,··· ,xn∼N (0,Id) [∥∥∥∥∥B − 1n n∑ i=1 B(xi) ∥∥∥∥∥ ≤ ‖B‖ ] ≥ 1− 2/(d2t)− nγ.\nProof. We show that the four Properties in Lemma B.7 are satisfied. Define function B(x) = h(x)xx>.\n(I) ‖B(x)‖ = ‖h(x)xx>‖ = |h(x)|‖x‖2. By using Fact B.2, we have\nPr x∼N (0,Id)\n[‖x‖2 ≤ 10td log n] ≥ 1− 1/(nd2t)\nTherefore,\nPr x∼N (0,Id)\n[‖B(x)‖ ≤ m · 10td log(n)] ≥ 1− γ − 1/(nd2t).\n(II) ∥∥∥∥ EB∼B[B] ∥∥∥∥ = ∥∥∥∥ Ex∼N (0,Id)[h(x)xx>] ∥∥∥∥ > 0. (III)\nmax (∥∥∥∥ EB∼B[BB>] ∥∥∥∥ , ∥∥∥∥ EB∼B[B>B] ∥∥∥∥) = max ‖a‖=1 E x∼N (0,Id) [(h(x))2‖x‖2(a>x)2]\n≤ (\nE x∼N (0,Id) [(h(x))4]\n)1/2 · (\nE x∼N (0,Id)\n[‖x‖8] )1/4\n· max ‖a‖=1\n( E\nx∼N (0,Id) [(a>x)8] )1/4 . L2d.\n(IV)\nmax ‖a‖=‖b‖=1\n( E\nB∼B [(a>Bb)2] )1/2 = max ‖a‖=1 ( E x∼N (0,Id) [h2(x)(a>x)4]\n)1/2 ≤ (\nE x∼N (0,Id) [h4(x)] )1/4 · max ‖a‖=1 ( E x∼N (0,Id) [(a>x)8] )1/4 . L.\nApplying Lemma B.7, we obtain, for any 0 < < 1 and t ≥ 1, if\nn & (t log d) · (L2d+ ‖B‖2 + (mtd log n)‖B‖ )/( 2‖B‖2), and γ + 1/(nd2t) . ( ‖B‖/L)2\nthen\nPr x1,··· ,xn∼N (0,Id) [∥∥∥∥∥B − 1n n∑ i=1 B(xi) ∥∥∥∥∥ ≤ ‖B‖ ] ≥ 1− 2/(d2t)− nγ."
    }, {
      "heading" : "C Properties of Activation Functions",
      "text" : "Theorem 3.5. ReLU φ(z) = max{z, 0}, leaky ReLU φ(z) = max{z, 0.01z}, squared ReLU φ(z) = max{z, 0}2 and any non-linear non-decreasing smooth functions with bounded symmetric φ′(z), like the sigmoid function φ(z) = 1/(1 + e−z), the tanh function and the erf function φ(z) = ∫ z 0 e −t2dt, satisfy Property 3.1,3.2,3.3. The linear function, φ(z) = z, doesn’t satisfy Property 3.2 and the quadratic function, φ(z) = z2, doesn’t satisfy Property 3.1 and 3.2.\nProof. We can easily verify that ReLU , leaky ReLU and squared ReLU satisfy Property 3.2 by calculating ρ(σ) in Property 3.2, which is shown in Table 1. Property 3.1 for ReLU , leaky ReLU and squared ReLU can be verified since they are non-decreasing with bounded first derivative. ReLU and leaky ReLU are piece-wise linear, so they satisfy Property 3.3(b). Squared ReLU is smooth so it satisfies Property 3.3(a).\nSmooth non-decreasing activations with bounded first derivatives automatically satisfy Property 3.1 and 3.3. For Property 3.2, since their first derivatives are symmetric, we have E[φ′(σ ·z)z] = 0. Then by Hölder’s inequality and φ′(z) ≥ 0, we have\nE z∼D1\n[φ′2(σ · z)] ≥ (\nE z∼D1\n[φ′(σ · z)] )2 ,\nE z∼D1 [φ′2(σ · z)z2] · E z∼D1\n[z2] ≥ (\nE z∼D1\n[φ′(σ · z)z2] )2 ,\nE z∼D1 [φ′(σ · z)z2] · E z∼D1 [φ′(σ · z)] = E z∼D1\n[( √ φ′(σ · z)z)2] · E z∼D1 [( √ φ′(σ · z))2] ≥ ( E z∼D1 [φ′(σ · z)z] )2 .\nThe equality in the first inequality happens when φ′(σ · z) is a constant a.e.. The equality in the second inequality happens when |φ′(σ ·z)| is a constant a.e., which is invalidated by the non-linearity and smoothness condition. The equality in the third inequality holds only when φ′(z) = 0 a.e., which leads to a constant function under non-decreasing condition. Therefore, ρ(σ) > 0 for any smooth non-decreasing non-linear activations with bounded symmetric first derivatives. The statements about linear activations and quadratic activation follow direct calculations."
    }, {
      "heading" : "D Local Positive Definiteness of Hessian",
      "text" : ""
    }, {
      "heading" : "D.1 Main Results for Positive Definiteness of Hessian",
      "text" : ""
    }, {
      "heading" : "D.1.1 Bounding the Spectrum of the Hessian near the Ground Truth",
      "text" : "Theorem D.1 (Bounding the spectrum of the Hessian near the ground truth). For any W ∈ Rd×k with ‖W −W ∗‖ . v4minρ2(σk)/(k2κ5λ2v4maxσ 4p 1 ) · ‖W ∗‖, let S denote a set of i.i.d. samples from distribution D (defined in (1)) and let the activation function satisfy Property 3.1,3.2,3.3. Then for any t ≥ 1, if |S| ≥ d · poly(log d, t) · k2v4maxτκ8λ2σ 4p 1 /(v 4 minρ\n2(σk)), we have with probability at least 1− d−Ω(t),\nΩ(v2minρ(σk)/(κ 2λ))I ∇2f̂S(W ) O(kv2maxσ 2p 1 )I.\nProof. The main idea of the proof follows the following inequalities,\n∇2fD(W ∗)− ‖∇2f̂S(W )−∇2fD(W ∗)‖I ∇2f̂S(W ) ∇2fD(W ∗) + ‖∇2f̂S(W )−∇2fD(W ∗)‖I\nThe proof sketch is first to bound the range of the eigenvalues of ∇2fD(W ∗) (Lemma D.3) and then bound the spectral norm of the remaining error, ‖∇2f̂S(W )−∇2fD(W ∗)‖. ‖∇2f̂S(W )−∇2fD(W ∗)‖ can be further decomposed into two parts, ‖∇2f̂S(W ) − H‖ and ‖H − ∇2fD(W ∗)‖, where H is ∇2fD(W ) if φ is smooth, otherwise H is a specially designed matrix . We can upper bound them whenW is close enough toW ∗ and there are enough samples. In particular, if the activation satisfies Property 3.3(a), see Lemma D.10 for bounding ‖H − ∇2fD(W ∗)‖ and Lemma D.11 for bounding ‖H −∇2f̂S(W )‖. If the activation satisfies Property 3.3(b), see Lemma D.15.\nFinally we can complete the proof by setting δ = O(v2minρ(σ1)/(kv 2 maxκ 2λσ2p1 )) in Lemma D.11 and Lemma D.15, setting ‖W −W ∗‖ . v2minρ(σk)/(kκ2λv2maxσ p 1) in Lemma D.10 and setting ‖W − W ∗‖ ≤ v4minρ2(σk)σk/(k2κ4λ2v4maxσ 4p 1 ) in Lemma D.15."
    }, {
      "heading" : "D.1.2 Local Linear Convergence of Gradient Descent",
      "text" : "Although Theorem D.1 gives upper and lower bounds for the spectrum of the Hessian w.h.p., it only holds when the current set of parameters W are independent of samples. When we use iterative methods, like gradient descent, to optimize this objective, the next iterate calculated from the current set of samples will depend on this set of samples. Therefore, we need to do resampling at each iteration. Here we show that for activations that satisfies Properties 3.1, 3.2 and 3.3(a), linear convergence of gradient descent is guaranteed. To the best of our knowledge, there is no linear convergence guarantees for general non-smooth objective. So the following proposition also applies to smooth objectives only, which excludes ReLU.\nTheorem D.2 (Linear convergence of gradient descent, formal version of Theorem 4.4). Let W c ∈ Rd×k be the current iterate satisfying\n‖W c −W ∗‖ . v4minρ2(σk)/(k2κ5λ2v4maxσ 4p 1 )‖W ∗‖.\nLet S denote a set of i.i.d. samples from distribution D (defined in (1)) Let the activation function satisfy Property 3.1,3.2 and 3.3(a). Define\nm0 = Θ(v 2 minρ(σk)/(κ 2λ)) and M0 = Θ(kv2maxσ 2p 1 ).\nFor any t ≥ 1, if we choose\n|S| ≥ d · poly(log d, t) · k2v4maxτκ8λ2σ 4p 1 /(v 4 minρ 2(σk)) (5)\nand perform gradient descent with step size 1/M0 on f̂S(W c) and obtain the next iterate,\nW̃ = W c − 1 M0 ∇f̂S(W c),\nthen with probability at least 1− d−Ω(t),\n‖W̃ −W ∗‖2F ≤ (1− m0 M0 )‖W c −W ∗‖2F .\nProof. To prove Theorem D.2, we need to show the positive definite properties on the entire line between the current iterate and the optimum by constructing a set of anchor points, which are independent of the samples. Then we apply traditional analysis for the linear convergence of gradient descent.\nIn particular, given a current iterate W c, we set d(p+1)/2 anchor points {W a}a=1,2,··· ,d(p+1)/2 equally along the line ξW ∗ + (1− ξ)W c for ξ ∈ [0, 1].\nAccording to Theorem D.1, by setting t ← t + (p + 1)/2, we have with probability at least 1− d−(t+(p+1)/2) for each anchor point {W a},\nm0I ∇2f̂S(W a) M0I.\nThen given an anchor point W a, according to Lemma D.16, we have with probability 1 − 2d−(t+(p+1)/2), for any points W between (W a−1 +W a)/2 and (W a +W a+1)/2,\nm0I ∇2f̂S(W ) M0I. (6)\nFinally by applying union bound over these d(p+1)/2 small intervals, we have with probability at least 1− d−t for any points W on the line between W c and W ∗,\nm0I ∇2f̂S(W ) M0I.\nNow we can apply traditional analysis for linear convergence of gradient descent. Let η denote the stepsize.\n‖W̃ −W ∗‖2F = ‖W c − η∇f̂S(W c)−W ∗‖2F = ‖W c −W ∗‖2F − 2η〈∇f̂S(W c), (W c −W ∗)〉+ η2‖∇f̂S(W c)‖2F\nWe can rewrite f̂S(W c),\n∇f̂S(W c) = (∫ 1\n0 ∇2f̂S(W ∗ + γ(W c −W ∗))dγ\n) vec(W c −W ∗).\nWe define function ĤS : Rd×k → Rdk×dk such that\nĤS(W c −W ∗) = (∫ 1 0 ∇2f̂S(W ∗ + γ(W c −W ∗))dγ ) .\nAccording to Eq. (6), m0I Ĥ M0I. (7)\nWe can upper bound ‖∇f̂S(W c)‖2F ,\n‖∇f̂S(W c)‖2F = 〈ĤS(W c −W ∗), ĤS(W c −W ∗)〉 ≤M0〈W c −W ∗, ĤS(W c −W ∗)〉.\nTherefore,\n‖W̃ −W ∗‖2F ≤ ‖W c −W ∗‖2F − (−η2M0 + 2η)〈W c −W ∗, Ĥ(W c −W ∗)〉 ≤ ‖W c −W ∗‖2F − (−η2M0 + 2η)m0‖W c −W ∗‖2F = ‖W c −W ∗‖2F −\nm0 M0 ‖W c −W ∗‖2F\n≤ (1− m0 M0 )‖W c −W ∗‖2F\nwhere the third equality holds by setting η = 1M0 ."
    }, {
      "heading" : "D.2 Positive Definiteness of Population Hessian at the Ground Truth",
      "text" : "The goal of this Section is to prove Lemma D.3.\nLemma D.3 (Positive definiteness of population Hessian at the ground truth). If φ(z) satisfies Property 3.1,3.2 and 3.3, we have the following property for the second derivative of function fD(W ) at W ∗,\nΩ(v2minρ(σk)/(κ 2λ))I ∇2fD(W ∗) O(kv2maxσ 2p 1 )I.\nProof. The proof directly follows Lemma D.6 (Section D.2.2) and Lemma D.7(Section D.2.3)."
    }, {
      "heading" : "D.2.1 Lower Bound on the Eigenvalues of Hessian for the Orthogonal Case",
      "text" : "Lemma D.4. Let D1 denote Gaussian distribution N (0, 1). Let α0 = Ez∼D1 [φ′(z)], α1 = Ez∼D1 [φ′(z)z], α2 = Ez∼D1 [φ′(z)z2], β0 = Ez∼D1 [φ′2(z)] ,β2 = Ez∼D1 [φ′2(z)z2]. Let ρ denote min{(β0 − α20 − α21), (β2 − α21 − α22)}. Let P = [ p1 p2 · · · pk ] ∈ Rk×k. Then we have,\nE u∼Dk ( k∑ i=1 p>i u · φ′(ui) )2 ≥ ρ‖P‖2F (8) Proof. The main idea is to explicitly calculate the LHS of Eq (8), then reformulate the equation and find a lower bound represented by α0, α1, α2, β0, β2.\nE u∼Dk ( k∑ i=1 p>i u · φ′(ui) )2 =\nk∑ i=1 k∑ l=1 E u∼Dk [p>i (φ ′(ul)φ ′(ui) · uu>)pl]\n= k∑ i=1 E u∼Dk [p>i (φ ′(ui)\n2 · uu>)pi]︸ ︷︷ ︸ A\n+ ∑ i 6=l E u∼Dk [p>i (φ ′(ul)φ\n′(ui) · uu>)pl]︸ ︷︷ ︸ B\nFurther, we can rewrite the diagonal term in the following way,\nA = k∑ i=1 E u∼Dk [p>i (φ ′(ui) 2 · uu>)pi]\n= k∑ i=1 E u∼Dk\np>i φ′(ui)2 · u2i eie>i +∑ j 6=i uiuj(eie > j + eje > i ) + ∑ j 6=i ∑ l 6=i ujuleje > l  pi \n= k∑ i=1 E u∼Dk\np>i φ′(ui)2 · u2i eie>i +∑ j 6=i u2jeje > j  pi \n= k∑ i=1 p>i  E u∼Dk [φ′(ui) 2u2i ]eie > i + ∑ j 6=i E u∼Dk [φ′(ui) 2u2j ]eje > j  pi  = k∑ i=1 p>i β2eie>i +∑ j 6=i β0eje > j  pi \n= k∑ i=1 p>i ((β2 − β0)eie>i + β0Ik)pi\n= (β2 − β0) k∑ i=1 p>i eie > i pi + β0 k∑ i=1 p>i pi = (β2 − β0)‖diag(P )‖2 + β0‖P‖2F ,\nwhere the second step follows by rewriting uu> = k∑ i=1 k∑ j=1 uiujeie > j , the third step follows by\nE u∼Dk [φ′(ui) 2uiuj ] = 0, ∀j 6= i and E u∼Dk [φ′(ui) 2ujul] = 0, ∀j 6= l, the fourth step follows by pushing expectation, the fifth step follows by E u∼Dk [φ′(ui) 2u2i ] = β2 and E u∼Dk [φ′(ui) 2u2j ] = E u∼Dk [φ′(ui) 2] = β0,\nand the last step follows by k∑ i=1 p2i,i = ‖diag(P )‖2 and k∑ i=1 p>i pi = k∑ i=1 ‖pi‖2 = ‖P‖2F .\nWe can rewrite the off-diagonal term in the following way,\nB = ∑ i 6=l E u∼Dk [p>i (φ ′(ul)φ ′(ui) · uu>)pl]\n= ∑ i 6=l E u∼Dk\np>i φ′(ul)φ′(ui) · u2i eie>i + u2l ele>l + uiul(eie>l + ele>i ) +∑ j 6=l uiujeie > j\n+ ∑ j 6=i ujuleje > l + ∑ j 6=i,l ∑ j′ 6=i,l ujuj′eje > j′\n pl \n= ∑ i 6=l E u∼Dk\np>i φ′(ul)φ′(ui) · u2i eie>i + u2l ele>l + uiul(eie>l + ele>i ) + ∑ j 6=i,l u2jeje > j  pl \n= ∑ i 6=l [ p>i ( E u∼Dk [φ′(ul)φ ′(ui)u 2 i ]eie > i + E u∼Dk [φ′(ul)φ ′(ui)u 2 l ]ele > l\n+ E u∼Dk\n[φ′(ul)φ ′(ui)uiul](eie > l + ele > i ) + ∑ j 6=i,l E u∼Dk [φ′(ul)φ ′(ui)u 2 j ]eje > j\n pl \n= ∑ i 6=l\np>i α0α2(eie>i + ele>l ) + α21(eie>l + ele>i ) + ∑\nj 6=i,l α20eje > j\n pl \n= ∑ i 6=l [ p>i ( (α0α2 − α20)(eie>i + ele>l ) + α21(eie>l + ele>i ) + α20Ik ) pl ] = (α0α2 − α20)\n∑ i 6=l p>i (eie > i + ele\n> l )pl︸ ︷︷ ︸\nB1\n+α21 ∑ i 6=l p>i (eie > l + ele\n> i )pl︸ ︷︷ ︸\nB2\n+α20 ∑ i 6=l\np>i pl︸ ︷︷ ︸ B3 ,\nwhere the third step follows by E u∼Dk [φ′(ul)φ ′(ui)uiuj ] = 0 and E u∼Dk [φ′(ul)φ ′(ui)uj′uj ] = 0 for\nj′ 6= j.\nFor the term B1, we have B1 = (α0α2 − α20) ∑ i 6=l p>i (eie > i + ele > l )pl\n= 2(α0α2 − α20) ∑ i 6=l p>i eie > i pl = 2(α0α2 − α20) k∑ i=1 p>i eie > i ( k∑ l=1 pl − pi )\n= 2(α0α2 − α20) ( k∑ i=1 p>i eie > i k∑ l=1 pl − k∑ i=1 p>i eie > i pi ) = 2(α0α2 − α20)(diag(P )> · P · 1− ‖diag(P )‖2)\nFor the term B2, we have\nB2 = α 2 1 ∑ i 6=l p>i (eie > l + ele > i )pl\n= α21 ∑ i 6=l p>i eie > l pl + ∑ i 6=l p>i ele > i pl  = α21  k∑ i=1 k∑ l=1 p>i eie > l pl − k∑ j=1 p>j eje > j pj + k∑ i=1 k∑ l=1 p>i ele > i pl − k∑ j=1 p>j eje > j pj\n = α21((diag(P ) >1)2 − ‖diag(P )‖2 + 〈P, P>〉 − ‖diag(P )‖2)\nFor the term B3, we have\nB3 = α 2 0 ∑ i 6=l p>i pl\n= α20 ( k∑ i=1 p>i k∑ l=1 pl − k∑ i=1 p>i pi )\n= α20 ∥∥∥∥∥ k∑ i=1 pi ∥∥∥∥∥ 2 − k∑ i=1 ‖pi‖2 \n= α20(‖P · 1‖2 − ‖P‖2F )\nLet diag(P ) denote a length k column vector where the i-th entry is the (i, i)-th entry of\nP ∈ Rk×k. Furthermore, we can show A+B is,\nA+B\n= A+B1 +B2 +B3 = (β2 − β0)‖diag(P )‖2 + β0‖P‖2F︸ ︷︷ ︸ A + 2(α0α2 − α20)(diag(P )> · P · 1− ‖diag(P )‖2)︸ ︷︷ ︸ B1 + α21((diag(P ) > · 1)2 − ‖diag(P )‖2 + 〈P, P>〉 − ‖diag(P )‖2)︸ ︷︷ ︸\nB2\n+α20(‖P · 1‖2 − ‖P‖2F )︸ ︷︷ ︸ B3\n= ‖α0P · 1 + (α2 − α0)diag(P )‖2︸ ︷︷ ︸ C1 +α21(diag(P ) > · 1)2︸ ︷︷ ︸ C2 + α21 2 ‖P + P> − 2diag(diag(P ))‖2F︸ ︷︷ ︸\nC3\n+ (β0 − α20 − α21)‖P − diag(diag(P ))‖2F︸ ︷︷ ︸ C4 + (β2 − α21 − α22)‖diag(P )‖2︸ ︷︷ ︸ C5 ≥ (β0 − α20 − α21)‖P − diag(diag(P ))‖2F + (β2 − α21 − α22)‖diag(P )‖2 ≥ min{(β0 − α20 − α21), (β2 − α21 − α22)} · (‖P − diag(diag(P ))‖2F + ‖diag(P )‖2) = min{(β0 − α20 − α21), (β2 − α21 − α22)} · (‖P − diag(diag(P ))‖2F + ‖diag(diag(P ))‖2) ≥ min{(β0 − α20 − α21), (β2 − α21 − α22)} · ‖P‖2F = ρ‖P‖2F ,\nwhere the first step follows by B = B1 + B2 + B3, and the second step follows by the definition of A,B1, B2, B3 the third step follows by A + B1 + B2 + B3 = C1 + C2 + C3 + C4 + C5, the fourth step follows by C1, C2, C3 ≥ 0, the fifth step follows a ≥ min(a, b), the sixth step follows by ‖diag(P )‖2 = ‖diag(diag(P ))‖2, the seventh step follows by triangle inequality, and the last step follows the definition of ρ.\nClaim D.5. A+B1 +B2 +B3 = C1 + C2 + C3 + C4 + C5.\nProof. The key properties we need are, for two vectors a, b, ‖a+ b‖2 = ‖a‖2 + 2〈a, b〉+ ‖b‖2; for two\nmatrices A,B, ‖A+B‖2F = ‖A‖2F + 2〈A,B〉+ ‖B‖2F . Then, we have\nC1 + C2 + C3 + C4 + C5\n= (‖α0P · 1‖)2 + 2(α0α2 − α20)〈P · 1,diag(P )〉+ (α2 − α0)2‖diag(P )‖2︸ ︷︷ ︸ C1 +α21(diag(P ) > · 1)2︸ ︷︷ ︸ C2 + α21 2\n(2‖P‖2F + 4‖diag(diag(P ))‖2F + 2〈P, P>〉 − 4〈P,diag(diag(P ))〉 − 4〈P>, diag(diag(P ))〉)︸ ︷︷ ︸ C3\n+ (β0 − α20 − α21)(‖P‖2F − 2〈P,diag(diag(P ))〉+ ‖diag(diag(P ))‖2F )︸ ︷︷ ︸ C4 + (β2 − α21 − α22)‖diag(P )‖2︸ ︷︷ ︸ C5 = α20‖P · 1‖2 + 2(α0α2 − α20)〈P · 1, diag(P )〉+ (α2 − α0)2‖diag(P )‖2︸ ︷︷ ︸ C1 +α21(diag(P ) > · 1)2︸ ︷︷ ︸ C2 + α21 2\n(2‖P‖2F + 4‖diag(P )‖2 + 2〈P, P>〉 − 8‖diag(P )‖2)︸ ︷︷ ︸ C3\n+ (β0 − α20 − α21)(‖P‖2F − 2‖diag(P )‖2 + ‖diag(P )‖2)︸ ︷︷ ︸ C4 + (β2 − α21 − α22)‖diag(P )‖2︸ ︷︷ ︸ C5 = α20‖P · 1‖2 + 2(α0α2 − α20)diag(P )> · P · 1 + α21(diag(P )> · 1)2 + α21〈P, P>〉 + (β0 − α20)‖P‖2F + ((α2 − α0)2 − 2α21 − β0 + α20 + α21 + β2 − α21 − α22)︸ ︷︷ ︸\nβ2−β0−2(α2α0−α20+α21)\n‖diag(P )‖2\n= 0︸︷︷︸ part of A + 2(α2α0 − α20) · diag(P )>P · 1︸ ︷︷ ︸ part of B1 +α21 · ((diag(P )>1)2 + 〈P, P>〉)︸ ︷︷ ︸ part of B2 +α20 · ‖P · 1‖2︸ ︷︷ ︸ part of B3 + (β0 − α20) · ‖P‖2F︸ ︷︷ ︸ proportional to ‖P‖2F + (β2 − β0 − 2(α2α0 − α20 + α21)) · ‖diag(P )‖2︸ ︷︷ ︸ proportional to ‖diag(P )‖2 = (β2 − β0)‖diag(P )‖2 + β0‖P‖2F︸ ︷︷ ︸ A + 2(α0α2 − α20)(diag(P )> · P · 1− ‖diag(P )‖2)︸ ︷︷ ︸ B1 + α21((diag(P ) > · 1)2 − ‖diag(P )‖2 + 〈P, P>〉 − ‖diag(P )‖2)︸ ︷︷ ︸\nB2\n+α20(‖P · 1‖2 − ‖P‖2F )︸ ︷︷ ︸ B3\n=A+B1 +B2 +B3\nwhere the second step follows by 〈P,diag(diag(P ))〉 = ‖diag(P )‖2 and ‖diag(diag(P ))‖2F = ‖diag(P )‖2."
    }, {
      "heading" : "D.2.2 Lower Bound on the Eigenvalues of Hessian for Non-orthogonal Case",
      "text" : "First we show the lower bound of the eigenvalues. The main idea is to reduce the problem to a k-by-k problem and then lower bound the eigenvalues using orthogonal weight matrices.\nLemma D.6 (Lower bound). If φ(z) satisfies Property 3.1,3.2 and 3.3, we have the following property for the second derivative of function fD(W ) at W ∗,\nΩ(v2minρ(σk)/(κ 2λ))I ∇2fD(W ∗).\nProof. Let a ∈ Rdk denote vector [ a>1 a > 2 · · · a>k ]>, let b ∈ Rdk denote vector [b>1 b>2 · · · b>k ]> and let c ∈ Rdk denote vector [ c>1 c > 2 · · · c>k\n]>. The smallest eigenvalue of the Hessian can be calculated by\n∇2f(W ∗) min ‖a‖=1 a>∇2f(W ∗)a Idk = min ‖a‖=1 E x∼Dd ( k∑ i=1 v∗i a > i x · φ′(w∗>i x) )2 Idk (9) Note that\nmin ‖a‖=1 E x∼Dd ( k∑ i=1 (v∗i ai) >x · φ′(w∗>i x) )2 = min ‖a‖6=0 E x∼Dd ( k∑ i=1 (v∗i ai) >x · φ′(w∗>i x)\n)2 /‖a‖2 = min∑\ni ‖bi/v∗i ‖2 6=0 E x∼Dd ( k∑ i=1 b>i x · φ′(w∗>i x) )2 /( k∑ i=1 ‖bi/v∗i ‖2 )\nby ai = bi/v∗i\n= min∑ i ‖bi‖2 6=0 E x∼Dd ( k∑ i=1 b>i x · φ′(w∗>i x) )2 /( k∑ i=1 ‖bi/v∗i ‖2 )\n≥ v2min min∑ i ‖bi‖2 6=0 E x∼Dd ( k∑ i=1 b>i x · φ′(w∗>i x) )2 /( k∑ i=1 ‖bi‖2 )\nby vmin = min i∈[k] |v∗i |\n= v2min min‖a‖=1 E x∼Dd ( k∑ i=1 a>i x · φ′(w∗>i x) )2 (10) Let U ∈ Rd×k be the orthonormal basis of W ∗ and let V = [ v1 v2 · · · vk ] = U>W ∗ ∈ Rk×k. Also note that V and W ∗ have same singular values and W ∗ = UV . We use U⊥ ∈ Rd×(d−k) to denote the complement of U . For any vector aj ∈ Rd, there exist two vectors bj ∈ Rk and cj ∈ Rd−k such that\naj = Ubj + U⊥cj .\nWe useDd to denote Gaussian distributionN (0, Id), Dd−k to denote Gaussian distributionN (0, Id−k), and Dk to denote Gaussian distribution N (0, Ik). Then we can rewrite formulation (10) (removing v2min) as\nE x∼Dd ( k∑ i=1 a>i x · φ′(w∗>i x) )2 = E x∼Dd ( k∑ i=1 (b>i U > + c>i U > ⊥ )x · φ′(w∗>i x) )2 = A+B + C\nwhere\nA = E x∼Dd ( k∑ i=1 b>i U >x · φ′(w∗>i x) )2 , B = E\nx∼Dd ( k∑ i=1 c>i U > ⊥x · φ′(w∗>i x) )2 , C = E\nx∼Dd\n[ 2 ( k∑ i=1 b>i U >x · φ′(w∗>i x) ) · ( k∑ i=1 c>i U > ⊥x · φ′(w∗>i x) )] .\nWe calculate A,B,C separately. First, we can show\nA = E x∼Dd ( k∑ i=1 b>i U >x · φ′(w∗>i x) )2 = E z∼Dk ( k∑ i=1 b>i z · φ′(v>i z) )2 . Second, we can show\nB = E x∼Dd ( k∑ i=1 c>i U > ⊥x · φ′(w∗>i x) )2 = E\ns∼Dd−k,z∼Dk ( k∑ i=1 c>i s · φ′(v∗>i z) )2 = E\ns∼Dd−k,z∼Dk [(y>s)2] by defining y = k∑ i=1 φ′(v∗>i z)ci ∈ Rd−k\n= E z∼Dk\n[ E\ns∼Dd−k [(y>s)2]\n]\n= E z∼Dk  E s∼Dd−k d−k∑ j=1 s2jy 2 j  by E[sjsj′ ] = 0 = E\nz∼Dk d−k∑ j=1 y2j  by sj ∼ N (0, 1) = E\nz∼Dk ∥∥∥∥∥ k∑ i=1 φ′(v∗>i z)ci ∥∥∥∥∥ 2  by definition of y\nThird, we have C = 0 since U>⊥x is independent of w ∗> i x and U >x. Thus, putting them all together,\nE x∼Dd ( k∑ i=1 a>i x · φ′(w∗>i x) )2 = E z∼Dk ( k∑ i=1 b>i z · φ′(v>i z) )2+ E z∼Dk ∥∥∥∥∥ k∑ i=1 φ′(v>i z)ci ∥∥∥∥∥ 2 \nLet us lower bound A,\nA = E z∼Dk ( k∑ i=1 b>i z · φ′(v>i z) )2 = ∫ (2π)−k/2\n( k∑ i=1 b>i z · φ′(v>i z) )2 e−‖z‖ 2/2dz\n= ξ1\n∫ (2π)−k/2 ( k∑ i=1 b>i V †>s · φ′(si) )2 e−‖V †>s‖2/2 · | det(V †)|ds\n≥ ξ2\n∫ (2π)−k/2 ( k∑ i=1 b>i V †>s · φ′(si) )2 e−σ 2 1(V †)‖s‖2/2 · | det(V †)|ds\n= ξ3\n∫ (2π)−k/2 ( k∑ i=1 b>i V †>u/σ1(V †) · φ′(ui/σ1(V †)) )2 e−‖u‖ 2/2| det(V †)|/σk1 (V †)du\n= ∫ (2π)−k/2 ( k∑ i=1 p>i u · φ′(σk · ui) )2 e−‖u‖ 2/2 1 λ du\n= 1\nλ E u∼Dk ( k∑ i=1 p>i u · φ′(σk · ui) )2 where V † ∈ Rk×k is the inverse of V ∈ Rk×k, i.e., V †V = I, p>i = b>i V †>/σ1(V †) and σk = σk(W ∗). ξ1 replaces z by z = V †>s, so v∗>i z = si. ξ2 uses the fact ‖V †>s‖ ≤ σ1(V †)‖s‖. ξ3 replaces s by s = u/σ1(V\n†). Note that φ′(σk ·ui)’s are independent of each other, so we can simplify the analysis. In particular, Lemma D.4 gives a lower bound in this case in terms of pi. Note that ‖pi‖ ≥ ‖bi‖/κ. Therefore,\nE z∼Dk ( k∑ i=1 b>i z · φ′(v>i z) )2 ≥ ρ(σk) 1 κ2λ ‖b‖2.\nFor B, similar to the proof of Lemma D.4, we have,\nB = E z∼Dk ∥∥∥∥∥ k∑ i=1 φ′(v>i z)ci ∥∥∥∥∥ 2 \n= ∫ (2π)−k/2 ∥∥∥∥∥ k∑ i=1 φ′(v>i z)ci ∥∥∥∥∥ 2 e−‖z‖ 2/2dz\n= ∫ (2π)−k/2 ∥∥∥∥∥ k∑ i=1 φ′(σk · ui)ci ∥∥∥∥∥ 2 e−‖V †>u/σ1(V †)‖2/2 · det(V †/σ1(V †))du\n= ∫ (2π)−k/2 ∥∥∥∥∥ k∑ i=1 φ′(σk · ui)ci ∥∥∥∥∥ 2 e−‖V †>u/σ1(V †)‖2/2 · 1 λ du\n≥ ∫ (2π)−k/2 ∥∥∥∥∥ k∑ i=1 φ′(σk · ui)ci ∥∥∥∥∥ 2 e−‖u‖ 2/2 · 1 λ du\n= 1\nλ E u∼Dk ∥∥∥∥∥ k∑ i=1 φ′(σk · ui)ci ∥∥∥∥∥ 2 \n= 1\nλ  k∑ i=1 E u∼Dk [φ′(σk · ui)φ′(σk · ui)c>i ci] + ∑ i 6=l E u∼Dk [φ′(σk · ui)φ′(σk · ul)c>i cl]  = 1\nλ  E z∼D1 [φ′(σk · ui)2] k∑ i=1 ‖ci‖2 + ( E z∼D1 [φ′(σk · z)] )2∑ i 6=l c>i cl  = 1\nλ ( E z∼D1 [φ′(σk · z)] )2 ∥∥∥∥∥ k∑ i=1 ci ∥∥∥∥∥ 2\n2\n+ ( E\nz∼D1 [φ′(σk · z)2]−\n( E\nz∼D1 [φ′(σk · z)]\n)2) ‖c‖2  ≥ 1 λ ( E z∼D1 [φ′(σk · z)2]− ( E z∼D1 [φ′(σk · z)] )2) ‖c‖2\n≥ ρ(σk) 1\nλ ‖c‖2,\nwhere the first step follows by definition of Gaussian distribution, the second step follows by replacing z by z = V †>u/σ1(V †), and then v>i z = ui/σ1(V †) = uiσk(W ∗), the third step follows by ‖u‖2 ≥ ‖ 1 σ1(V †) V † > u‖2 , the fourth step follows by det(V †/σ1(V †)) = det(V †)/σk1 (V †) = 1/λ, the fifth step follows by definition of Gaussian distribution, the ninth step follows by x2 ≥ 0 for any x ∈ R, and the last step follows by Property 3.2.\nNote that 1 = ‖a‖2 = ‖b‖2 + ‖c‖2. Thus, we finish the proof for the lower bound."
    }, {
      "heading" : "D.2.3 Upper Bound on the Eigenvalues of Hessian for Non-orthogonal Case",
      "text" : "Lemma D.7 (Upper bound). If φ(z) satisfies Property 3.1,3.2 and 3.3, we have the following property for the second derivative of function fD(W ) at W ∗,\n∇2fD(W ∗) O(kv2maxσ 2p 1 )I.\nProof. Similarly, we can calculate the upper bound of the eigenvalues by\n‖∇2f(W ∗)‖ = max ‖a‖=1 a>∇2f(W ∗)a\n= v2max max‖a‖=1 E x∼Dk ( k∑ i=1 a>i x · φ′(w∗>i x) )2 = v2max max‖a‖=1 k∑ i=1 k∑ l=1 E x∼Dk [a>i x · φ′(w∗>i x) · a>l x · φ′(w∗>l x)]\n≤ v2max max‖a‖=1 k∑ i=1 k∑ l=1 ( E x∼Dk [(a>i x) 4] · E x∼Dk [(φ′(w∗>i x)) 4] · E x∼Dk [(a>l x) 4] · E x∼Dk [(φ′(w∗>l x)) 4] )1/4\n. v2max max‖a‖=1 k∑ i=1 k∑ l=1 ‖ai‖ · ‖al‖ · ‖w∗i ‖p · ‖w∗l ‖p\n≤ v2max max‖a‖=1 k∑ i=1 k∑ l=1 ‖ai‖ · ‖al‖ · σ2p1 ≤ kv2maxσ 2p 1 ,\nwhere the first inequality follows Hölder’s inequality, the second inequality follows by Property 3.1, the third inequality follows by ‖w∗i ‖ ≤ σ1(W ∗), and the last inequality by Cauchy-Schwarz inequality."
    }, {
      "heading" : "D.3 Error Bound of Hessians near the Ground Truth for Smooth Activations",
      "text" : "The goal of this Section is to prove Lemma D.8\nLemma D.8 (Error bound of Hessians near the ground truth for smooth activations). Let φ(z) satisfy Property 3.1, Property 3.2 and Property 3.3(a). Let W satisfy ‖W − W ∗‖ ≤ σk/2. Let S denote a set of i.i.d. samples from the distribution defined in (1). Then for any t ≥ 1 and 0 < < 1/2, if\n|S| ≥ −2dκ2τ · poly(log d, t)\nthen we have, with probability at least 1− 1/dΩ(t),\n‖∇2f̂S(W )−∇2fD(W ∗)‖ . v2maxkσ p 1( σ p 1 + ‖W −W ∗‖).\nProof. This follows by combining Lemma D.10 and Lemma D.11 directly."
    }, {
      "heading" : "D.3.1 Second-order Smoothness near the Ground Truth for Smooth Activations",
      "text" : "The goal of this Section is to prove Lemma D.10.\nFact D.9. Let wi denote the i-th column ofW ∈ Rd×k, and w∗i denote the i-th column ofW ∗ ∈ Rd×k. If ‖W −W ∗‖ ≤ σk(W ∗)/2, then for all i ∈ [k],\n1 2 ‖w∗i ‖ ≤ ‖wi‖ ≤ 3 2 ‖w∗i ‖.\nProof. Note that if ‖W −W ∗‖ ≤ σk(W ∗)/2, we have σk(W ∗)/2 ≤ σi(W ) ≤ 32σ1(W ∗) for all i ∈ [k] by Weyl’s inequality. By definition of singular value, we have σk(W ∗) ≤ ‖w∗i ‖ ≤ σ1(W ∗). By definition of spectral norm, we have ‖wi − w∗i ‖ ≤ ‖W −W ∗‖. Thus, we can lower bound ‖wi‖,\n‖wi‖ ≤ ‖w∗i ‖+ ‖wi − w∗i ‖ ≤ ‖w∗i ‖+ ‖W −W ∗‖ ≤ ‖w∗i ‖+ σk/2 ≤ 3 2 ‖w∗i ‖.\nSimilarly, we have ‖wi‖ ≥ 12‖w ∗ i ‖.\nLemma D.10 (Second-order smoothness near the ground truth for smooth activations). If φ(z) satisfies Property 3.1, Property 3.2 and Property 3.3(a), then for any W with ‖W −W ∗‖ ≤ σk/2, we have\n‖∇2fD(W )−∇2fD(W ∗)‖ . k2v2maxσ p 1‖W −W ∗‖.\nProof. Let ∆ = ∇2fD(W )−∇2fD(W ∗). For each (i, l) ∈ [k]× [k], let ∆i,l ∈ Rd×d denote the (i, l)-th block of ∆. Then, for i 6= l, we have\n∆i,l = E x∼Dd\n[ v∗i v ∗ l ( φ′(w>i x)φ ′(w>l x)− φ′(w∗>i x)φ′(w∗>l x) ) xx> ] ,\nand for i = l, we have\n∆i,i\n= E x∼Dd [( k∑ r=1 v∗rφ(w > r x)− y ) v∗i φ ′′(w>i x)xx > + v∗2i ( φ′2(w>i x)− φ′2(w∗>i x) ) xx> ]\n= E x∼Dd [( k∑ r=1 v∗rφ(w > r x)− y ) v∗i φ ′′(w>i x)xx > ] + E x∼Dd [ v∗2i ( φ′2(w>i x)− φ′2(w∗>i x) ) xx> ] . (11)\nIn the next a few paragraphs, we first show how to bound the off-diagonal term, and then show how to bound the diagonal term.\nFirst, we consider off-diagonal terms.\n‖∆i,l‖\n= ∥∥∥∥ Ex∼Dd [ v∗i v ∗ l ( φ′(w>i x)φ ′(w>l x)− φ′(w∗>i x)φ′(w∗>l x) ) xx> ]∥∥∥∥ ≤ v2max max‖a‖=1 E x∼Dd\n[∣∣∣φ′(w>i x)φ′(w>l x)− φ′(w∗>i x)φ′(w∗>l x)∣∣∣ · (x>a)2] ≤ v2max max‖a‖=1 E x∼Dd [( |φ′(w>i x)− φ′(w∗>i x)||φ′(w>l x)|+ |φ′(w∗>i x)||φ′(w>l x)− φ′(w∗>l x)| ) (x>a)2\n] = v2max max‖a‖=1 ( E x∼Dd [ |φ′(w>i x)− φ′(w∗>i x)||φ′(w>l x)|(x>a)2\n] + E\nx∼Dd\n[ |φ′(w∗>i x)||φ′(w>l x)− φ′(w∗>l x)|(x>a)2 ]) ≤ v2max max‖a‖=1 ( E x∼Dd [ L2|(wi − w∗i )>x||φ′(w>l x)|(x>a)2 ] + E x∼Dd [ |φ′(w∗>i x)|L2|(wl − w∗l )>x|(x>a)2\n]) ≤ v2max max‖a‖=1 ( E x∼Dd [ L2|(wi − w∗i )>x|L1|w>l x|p(x>a)2 ] + E x∼Dd [ L1|w∗>i x|pL2|(wl − w∗l )>x|(x>a)2\n]) ≤ v2maxL1L2 max‖a‖=1 ( E x∼Dd [ |(wi − w∗i )>x||w>l x|p(x>a)2 ] + E x∼Dd [ |(wl − w∗l )>x||w∗>i x|p(x>a)2\n]) . v2maxL1L2 max‖a‖=1 (‖wi − w∗i ‖‖wl‖p‖a‖2 + ‖wl − w∗l ‖‖w∗i ‖p‖a‖2)\n. v2maxL1L2σ p 1(W ∗)‖W −W ∗‖ (12)\nwhere the first step follows by definition of ∆i,l, the second step follows by definition of spectral norm and v∗i v ∗ l ≤ v2max, the third step follows by triangle inequality, the fourth step follows by linearity of expectation, the fifth step follows by Property 3.3(a), i.e., |φ′(w>i x)−φ′(w∗>i x)| ≤ L2|(wi−w∗i )>x|, the sixth step follows by Property 3.1, i.e., φ′(z) ≤ L1|z|p, the seventh step follows by Fact B.6, and the last step follows by ‖a‖2 = 1, ‖wi − w∗i ‖ ≤ ‖W −W ∗‖, ‖wi‖ ≤ 32‖w ∗ i ‖, and ‖w∗i ‖ ≤ σ1(W ∗).\nNote that the proof for the off-diagonal terms also applies to bounding the second-term in the diagonal block Eq. (11). Thus we only need to show how to bound the first term in the diagonal\nblock Eq. (11). ∥∥∥∥∥ Ex∼Dd [( k∑ r=1 v∗rφ(w > r x)− y ) v∗i φ ′′(w>i x)xx > ]∥∥∥∥∥ = ∥∥∥∥∥ Ex∼Dd [( k∑ r=1 v∗r (φ(w > r x)− φ(w∗>r x)) ) v∗i φ ′′(w>i x)xx >\n]∥∥∥∥∥ ≤ v2max\nk∑ r=1 max ‖a‖=1 E x∼Dd [|φ(w>r x)− φ(w∗>r x)||φ′′(w>i x)|(x>a)2]\n≤ v2max k∑ r=1 max ‖a‖=1 E x∼Dd [|φ(w>r x)− φ(w∗>r x)|L2(x>a)2]\n≤ v2maxL2 k∑ r=1 max ‖a‖=1 E x∼Dd [ max z∈[w>r x,w∗>r x] |φ′(z)| · |(wr − w∗r)>x| · (x>a)2 ]\n≤ v2maxL2 k∑ r=1 max ‖a‖=1 E x∼Dd [ max z∈[w>r x,w∗>r x] L1|z|p · |(wr − w∗r)>x| · (x>a)2 ]\n≤ v2maxL1L2 k∑ r=1 max ‖a‖=1 E x∼Dd [(|w>r x|p + |w∗>r x|p) · |(wr − w∗r)>x| · (x>a)2]\n. v2maxL1L2 k∑ r=1 [(‖wr‖p + ‖w∗r‖p)‖wr − w∗r‖] . kv2maxL1L2σ p 1(W ∗)‖W −W ∗‖, (13)\nwhere the first step follows by y = ∑k\nr=1 v ∗ rφ(w ∗> r x), the second step follows by definition of spectral\nnorm and v∗rv∗i ≤ |vmax|2, the third step follows by Property 3.3(a), i.e., |φ′′(w>i x)| ≤ L2, the fourth step follows by |φ(w>r x) − φ(w∗>r x) ≤ maxz∈[w>r x,w∗>r x] |φ\n′(z)||(wr − w∗r)>x|, the fifth step follows Property 3.1, i.e., |φ′(z)| ≤ L1|z|p, the sixth step follows by maxz∈[w>r x,w∗>r x] |z|\np ≤ (|w>r x|p + |w∗>r x|p), the seventh step follows by Fact B.6.\nPutting it all together, we can bound the error by\n‖∇2fD(W )−∇2fD(W ∗)‖ = max ‖a‖=1 a>(∇2fD(W )−∇2fD(W ∗))a\n= max ‖a‖=1 k∑ i=1 k∑ l=1 a>i ∆i,lal\n= max ‖a‖=1  k∑ i=1 a>i ∆i,iai + ∑ i 6=l a>i ∆i,lal  ≤ max ‖a‖=1  k∑ i=1 ‖∆i,i‖‖ai‖2 + ∑ i 6=l ‖∆i,l‖‖ai‖‖al‖\n ≤ max ‖a‖=1  k∑ i=1 C1‖ai‖2 + ∑ i 6=l C2‖ai‖‖al‖\n = max ‖a‖=1 C1 k∑ i=1 ‖ai‖2 + C2 ( k∑ i=1 ‖ai‖ )2 − k∑ i=1 ‖ai‖2 \n≤ max ‖a‖=1\n( C1\nk∑ i=1 ‖ai‖2 + C2\n( k\nk∑ i=1 ‖ai‖2 − k∑ i=1\n‖ai‖2 ))\n= max ‖a‖=1\n(C1 + C2(k − 1))\n. k2v2maxL1L2σ p 1(W ∗)‖W −W ∗‖.\nwhere the first step follows by definition of spectral norm and a denotes a vector ∈ Rdk, the first inequality follows by ‖A‖ = max‖x‖6=0,‖y‖6=0 x >Ay ‖x‖‖y‖ , the second inequality follows by ‖∆i,i‖ ≤ C1 and ‖∆i,l‖ ≤ C2, the third inequality follows by Cauchy-Scharwz inequality, the eighth step follows by∑ i=1 ‖ai‖2 = 1, and the last step follows by Eq (12) and (13)."
    }, {
      "heading" : "D.3.2 Empirical and Population Difference for Smooth Activations",
      "text" : "The goal of this Section is to prove Lemma D.11. For each i ∈ [k], let σi denote the i-th largest singular value of W ∗ ∈ Rd×k.\nNote that Bernstein inequality requires the spectral norm of each random matrix to be bounded almost surely. However, since we assume Gaussian distribution for x, ‖x‖2 is not bounded almost surely. The main idea is to do truncation and then use Matrix Bernstein inequality. Details can be found in Lemma B.7 and Corollary B.8.\nLemma D.11 (Empirical and population difference for smooth activations). Let φ(z) satisfy Property 3.1,3.2 and 3.3(a). Let W satisfy ‖W −W ∗‖ ≤ σk/2. Let S denote a set of i.i.d. samples from distribution D (defined in (1)). Then for any t ≥ 1 and 0 < < 1/2, if\n|S| ≥ −2dκ2τ · poly(log d, t)\nthen we have, with probability at least 1− d−Ω(t),\n‖∇2f̂S(W )−∇2fD(W )‖ .v2maxkσ p 1( σ p 1 + ‖W −W ∗‖).\nProof. Define ∆ = ∇2fD(W )−∇2f̂S(W ). Let’s first consider the diagonal blocks. Define\n∆i,i = E x∼Dd [( k∑ r=1 v∗rφ(w > r x)− y ) v∗i φ ′′(w>i x)xx > + v∗2i φ ′2(w>i x)xx > ]\n−  1 n n∑ j=1 ( k∑ r=1 v∗rφ(w > r xj)− y ) v∗i φ ′′(w>i xj)xjx > j + v ∗2 i φ ′2(w>i xj)xjx > j  . Let’s further decompose ∆i,i into ∆i,i = ∆ (1) i,i + ∆ (2) i,i , where\n∆ (1) i,i\n= E x∼Dd [( k∑ r=1 v∗rφ(w > r x)− y ) v∗i φ ′′(w>i x)xx > ] − 1 n n∑ j=1 ( k∑ r=1 v∗rφ(w > r xj)− y ) v∗i φ ′′(w>i xj)xjx > j\n= v∗i k∑ r=1 ( v∗r E x∼Dd [ (φ(w>r x)− φ(w∗>r x))φ′′(w>i x)xx> ] − 1 n n∑ j=1 (φ(w>r xj)− φ(w∗>r xj))φ′′(w>i xj)xjx>j ) ,\nand\n∆ (2) i,i = E x∼Dd [v∗2i φ ′2(w>i x)xx >]− 1 n n∑ j=1 [v∗2i φ ′2(w>i xj)xjx > j ]. (14)\nThe off-diagonal block is\n∆i,l = v ∗ i v ∗ l  E x∼Dd [φ′(w>i x)φ ′(w>l x)xx >]− 1 n n∑ j=1 φ′(w>i xj)φ ′(w>l xj)xjx > j  Combining Claims. D.12, D.13 and D.14, and taking a union bound over k2 different ∆i,j , we\nobtain if n ≥ −2κ2(W ∗)τd · poly(t, log d) for any ∈ (0, 1/2), with probability at least 1− 1/dt,\n‖∇2f̂S(W )−∇2f(W )‖ . v2maxkσ p 1(W ∗)( σp1(W ∗) + ‖W −W ∗‖)\nClaim D.12. For each i ∈ [k], if n ≥ dpoly(log d, t)\n‖∆(1)i,i ‖ . kv 2 maxσ p 1(W ∗)‖W −W ∗‖\nholds with probability 1− 1/d4t.\nProof. For each r ∈ [k], we define function B̂r : Rd → Rd×d,\nB̂r(x) = L1L2 · (|w>r x|p + |w∗>r x|p) · |(wr − w∗r)>x| · xx>.\nAccording to Properties 3.1,3.2 and 3.3(a), we have for each x ∈ S,\n−B̂r(x) (φ(w>r x)− φ(w∗>r x))φ′′(w>i x)xx> B̂r(x)\nTherefore,\n∆ (1) i,i v 2 max k∑ r=1\n( E\nx∼Dd [B̂r(x)] +\n1 |S| ∑ x∈S B̂r(x)\n) .\nLet hr(x) = L1L2|w>r x|p · |(wr − w∗r)>x|. Let Br = Ex∼Dd [hr(x)xx>]. Define function Br(x) = hr(x)xx\n>. (I) Bounding |hr(x)|. According to Fact B.1, we have for any constant t ≥ 1, with probability 1− 1/(nd4t),\n|hr(x)| = L1L2|w>r x|p|(wr − w∗r)>x| . L1L2‖wr‖p‖wr − w∗r‖(t log n)(p+1)/2.\n(II) Bounding ‖Br‖.\n‖Br‖ ≥ E x∼Dd\n[ L1L2|w>r x|p|(wr − w∗r)>x| ( (wr − w∗r)>x ‖wr − w∗r‖ )2] & L1L2‖wr‖p‖wr − w∗r‖,\nwhere the first step follows by definition of spectral norm, and last step follows by Fact B.6. Using Fact B.6, we can also prove an upper bound ‖Br‖, ‖Br‖ . L1L2‖wr‖p‖wr − w∗r‖.\n(III) Bounding (Ex∼Dd [h4(x)])1/4 Using Fact B.6, we have(\nE x∼Dd [h4(x)]\n)1/4 = L1L2 ( E\nx∼Dd\n[( |w>r x|p|(wr − w∗r)>x| )4])1/4 . L1L2‖wr‖p‖wr − w∗r‖.\nBy applying Corollary B.8, if n ≥ −2dpoly(log d, t), then with probability 1− 1/d4t,\n∥∥∥∥∥ Ex∼Dd [ |w>r x|p · |(wr − w∗r)>x| · xx> ] − 1 |S| (∑ x∈S |w>r xj |p · |(wr − w∗r)>x| · xx> )∥∥∥∥∥ =\n∥∥∥∥∥Br − 1|S|∑ x∈S Br(x) ∥∥∥∥∥ ≤ ‖Br‖ . ‖wr‖p‖wr − w∗r‖. (15)\nIf ≤ 1/2, we have\n‖∆(1)i,i ‖ . k∑ i=1 v2max‖Br‖ . kv2maxσ p 1(W ∗)‖W −W ∗‖\nClaim D.13. For each i ∈ [k], if n ≥ −2dτ poly(log d, t) , then\n‖∆(2)i,i ‖ . v 2 maxσ 2p 1\nholds with probability 1− 1/d4t.\nProof. Recall the definition of ∆(2)i,i .\n∆ (2) i,i = E x∼Dd [v∗2i φ ′2(w>i x)xx >]− 1 |S| ∑ x∈S [v∗2i φ ′2(w>i x)xx >]\nLet hi(x) = φ′2(w>i x). Let Bi = Ex∼Dd [hi(x)xx>] Define function Bi(x) = hi(x)xx>. (I) Bounding |hi(x)|. For any constant t ≥ 1, (φ′(w>i x))2 ≤ L21|w>i x|2p . L21‖wi‖2ptp log\np(n) with probability 1 − 1/(nd4t) according to Fact B.1.\n(II) Bounding ‖Bi‖.∥∥∥∥ Ex∼Dd[φ′2(w>i x)xx>] ∥∥∥∥ = max‖a‖=1 Ex∼Dd [ φ′2(w>i x)(x >a)2 ]\n= max ‖a‖=1 E x∼Dd\n[ φ′2(w>i x) ( αw>i x+ βx >v )2]\n= max α2+β2=1,‖v‖=1 E x∼Dd\n[ φ′2(w>i x) ( (αw>i x) 2 + (βx>v)2 )]\n= max α2+β2=1\n( α2 E\nz∼D1 [φ′2(‖wi‖z)z2] + β2 E z∼D1 [φ′2(‖wi‖z)] ) = max ( E\nx∼D1 [φ′2(‖wi‖z)z2], E x∼D1 [φ′2(‖wi‖z)] ) where wi = wi/‖wi‖ and v is a unit vector orthogonal to wi such that a = αwi + βv. Now from Property 3.2, we have\nρ(‖wi‖) ≤ ∥∥∥∥ Ex∼Dd[φ′2(w>i x)xx>] ∥∥∥∥ . L21‖wi‖2p. (III) Bounding (Ex∼Dd [h4i (x)])1/4.(\nE x∼Dd [h4i (x)]\n)1/4 = ( E\nx∼Dd [φ′8(w>i x)]\n)1/4 . L21‖wi‖2p.\nBy applying Corollary B.8, we have, for any 0 < < 1, if n ≥ −2d ‖wi‖ 4p\nρ2(‖wi‖) poly(log d, t) the following bound holds ∥∥∥∥∥Bi − 1|S|∑\nx∈S Bi(x) ∥∥∥∥∥ ≤ ‖Bi‖, with probability at least 1− 1/d4t.\nTherefore, if n ≥ −2dτ poly(log d, t), where τ = (3σ1/2) 4p\nminσ∈[σk/2,3σ1/2] ρ 2(σ)\n, we have with probability\n1− 1/d4t\n‖∆(2)i,i ‖ . v 2 maxσ 2p 1\nClaim D.14. For each i ∈ [k], j ∈ [k], i 6= j, if n ≥ −2κ2τd poly(log d, t), then\n‖∆i,j‖ ≤ v2maxσ 2p 1 (W ∗)\nholds with probability 1− 1/d4t.\nProof. Recall the definition of off-diagonal blocks ∆i,l,\n∆i,l = v ∗ i v ∗ l\n( E\nx∼Dd [φ′(w>i x)φ ′(w>l x)xx >]− 1 |S| ∑ x∈S φ′(w>i x)φ ′(w>l x)xx >\n) (16)\nLet hi,l(x) = φ′(w>i x)φ ′(w>l x). Define functionBi,l(x) = hi,l(x)xx >. LetBi,l = Ex∼Dd [hi,l(x)xx>]. (I) Bounding |hi,l(x)|. For any constant t ≥ 1, we have with probability 1− 1/(nd4t)\n|hi,l(x)| = |φ′(w>i x)φ′(w>l x)| ≤ L21‖w>i x‖p‖w>l x‖p\n≤ L21‖wi‖p‖wl‖p(t log n)p . L21σ 2p 1 (t log n) p\nwhere the third step follows by Fact B.1. (II) Bounding ‖Bi,l‖. Let U ∈ Rd×2 be the orthogonal basis of span{wi, wl} and U⊥ ∈ Rd×(d−2) be the complementary matrix of U . Let matrix V ∈ R2×2 denote U>[wi wl], then UV = [wi wl] ∈ Rd×2. Given any vector a ∈ Rd, there exist vectors b ∈ R2 and c ∈ Rd−2 such that a = Ub + U⊥c. We can simplify ‖Bi,l‖ in the following way,\n‖Bi,l‖ = ∥∥∥∥ Ex∼Dd[φ′(w>i x)φ′(w>l x)xx>] ∥∥∥∥ = max ‖a‖=1 E x∼Dd [φ′(w>i x)φ ′(w>l x)(x >a)2]\n= max ‖b‖2+‖c‖2=1 E x∼Dd\n[φ′(w>i x)φ ′(w>l x)(b >U>x+ c>U>⊥x) 2]\n= max ‖b‖2+‖c‖2=1 E x∼Dd\n[φ′(w>i x)φ ′(w>l x)((b >U>x)2 + (c>U>⊥x) 2)]\n= max ‖b‖2+‖c‖2=1  Ez∼D2[φ′(v>1 z)φ′(v>2 z)(b>z)2]︸ ︷︷ ︸ A1 + E z∼D2,s∼Dd−2 [φ′(v>1 z)φ ′(v>2 z)(c >s)2]︸ ︷︷ ︸ A2 \nWe can lower bound the term A1,\nA1 = E z∼D2\n[φ′(v>1 z)φ ′(v>2 z)(b >z)2]\n= ∫ (2π)−1φ′(v>1 z)φ ′(v>2 z)(b >z)2e−‖z‖ 2/2dz\n= ∫ (2π)−1φ′(s1)φ ′(s2)(b >V †>s)2e−‖V †>s‖2/2 · | det(V †)|ds\n≥ ∫ (2π)−1(φ′(s1)φ ′(s2))(b >V †>s)2e−σ 2 1(V †)‖s‖2/2 · | det(V †)|ds\n= ∫ (2π)−1(φ′(u1/σ1(V †))φ′(u2/σ1(V †))) · (b>V †>u/σ1(V †))2e−‖u‖ 2/2|det(V †)|/σ21(V †)du\n= σ2(V )\nσ1(V ) E u∼D2\n[ (p>u)2φ′(σ2(V ) · u1)φ′(σ2(V ) · u2) ] = σ2(V )\nσ1(V ) E u∼D2\n[( (p1u1) 2 + (p2u2) 2 + 2p1p2u1u2 ) φ′(σ2(V ) · u1)φ′(σ2(V ) · u2) ] = σ2(V )\nσ1(V )\n( ‖p‖2 E\nz∼D1 [φ′(σ2(V ) · z)z2] · E z∼D1 [φ′(σ2(V ) · z)]\n+ ((p>1)2 − ‖p‖2) E z∼D1\n[φ′(σ2(V ) · z)z]2 )\nwhere p = V †b · σ2(V ) ∈ R2. Since we are maximizing over b ∈ R2, we can choose b such that ‖p‖ = ‖b‖. Then\nA1 = E z∼D2\n[φ′(v>1 z)φ ′(v>2 z)(b >z)2]\n≥ σ2(V ) σ1(V )\n‖b‖2 (\nE z∼D1 [φ′(σ2(V ) · z)z2] · E z∼D1 [φ′(σ2(V ) · z)]− E z∼D1\n[φ′(σ2(V ) · z)z]2 )\nFor the term A2, similarly we have\nA2 = E z∼D2,s∼Dd−2\n[φ′(v>1 z)φ ′(v>2 z)(c >s)2]\n= E z∼D2\n[φ′(v>1 z)φ ′(v>2 z)] E s∼Dd−2 [(c>s)2]\n= ‖c‖2 E z∼D2 [φ′(v>1 z)φ ′(v>2 z)]\n≥ ‖c‖2σ2(V ) σ1(V )\n( E\nz∼D1 [φ′(σ2(V ) · z)] )2 For simplicity, we just set ‖b‖ = 1 and ‖c‖ = 0 to lower bound,∥∥∥∥ Ex∼Dd [ φ′(w>i x)φ ′(w>l x)xx > ]∥∥∥∥\n≥ σ2(V ) σ1(V )\n( E\nz∼D1 [φ′(σ2(V ) · z)z2] · E z∼D1 [φ′(σ2(V ) · z)]−\n( E\nz∼D1 [φ′(σ2(V ) · z)z]\n)2)\n≥ σ2(V ) σ1(V ) ρ(σ2(V )) ≥ 1 κ(W ∗) ρ(σ2(V ))\nwhere the second step is from Property 3.2 and the fact that σk/2 ≤ σ2(V ) ≤ σ1(V ) ≤ 3σ1/2. For the upper bound of ‖Ex∼Dd [φ′(w>i x)φ′(w>l x)xx>]‖, we have\nE z∼D2\n[φ′(v>1 z)φ ′(v>2 z)(b >z)2] ≤ L21 E z∼D2 [|v>1 z|p · |v>2 z|p · |b>z|2]\n. L21‖v1‖p‖v2‖p‖b‖2 . L21σ 2p 1\nwhere the first step follows by Property 3.1, the second step follows by Fact B.6, and the last step follows by ‖v1‖ ≤ σ1, ‖v2‖ ≤ σ1 and ‖b‖ ≤ 1. Similarly, we can upper bound,\nE z∼D2,s∼Dd−2\n[φ′(v>1 z)φ ′(v>2 z)(c >s)2] = ‖c‖2 E z∼D2 [φ′(v>1 z)φ ′(v>2 z)] . L 2 1σ 2p 1\nThus, we have ∥∥∥∥ Ex∼Dd[φ′(w>i x)φ′(w>l x)xx>] ∥∥∥∥ . L21σ2p1 . σ2p1 .\n(III) Bounding (Ex∼Dd [h4i,l(x)])1/4.\n( E\nx∼Dd [h4i,l(x)]\n)1/4 = ( E\nx∼Dd [φ′4(w>i x) · φ′4(w>l x)]\n)1/4 . L21‖wi‖p‖wl‖p . L21σ 2p 1 .\nTherefore, applying Corollary B.8, we have, if n ≥ −2κ2(W ∗)τd poly(log d, t), then\n‖∆i,j‖ ≤ v2maxσ 2p 1 (W ∗).\nholds with probability at least 1− 1/d4t.\nD.4 Error Bound of Hessians near the Ground Truth for Non-smooth Activations\nThe goal of this Section is to prove Lemma D.15,\nLemma D.15 (Error bound of Hessians near the ground truth for non-smooth activations). Let φ(z) satisfy Property 3.1,3.2 and 3.3(b). Let W satisfy ‖W −W ∗‖ ≤ σk/2. Let S denote a set of i.i.d. samples from the distribution defined in (1). Then for any t ≥ 1 and 0 < < 1/2, if\n|S| ≥ −2κ2τd poly(log d, t)\nwith probability at least 1− d−Ω(t),\n‖∇2f̂S(W )−∇2fD(W ∗)‖ . v2maxkσ 2p 1 ( + (σ −1 k · ‖W −W ∗‖)1/2).\nProof. As we noted previously, when Property 3.3(b) holds, the diagonal blocks of the empirical Hessian can be written as, with probability 1,\n∂2f̂S(W )\n∂w2i =\n1 |S| ∑\n(x,y)∈S\n(v∗i φ ′(w>i x)) 2xx>\nWe construct a matrix H ∈ Rdk×dk with i, l-th block as\nHi,l = v ∗ i v ∗ l E x∼Dd\n[ φ′(w>i x)φ ′(w>l x)xx > ] ∈ Rd×d, ∀i ∈ [k], l ∈ [k].\nNote that H 6= ∇2fD(W ). However we can still bound ‖H −∇2f̂S(W )‖ and ‖H −∇2fD(W ∗)‖ when we have enough samples and ‖W −W ∗‖ is small enough. The proof for ‖H − ∇2f̂S(W )‖ basically follows the proof of Lemma D.11 as ∆(2)ii in Eq. (14) and ∆il in Eq. (16) forms the blocks of H −∇2f̂S(W ) and we can bound them without smoothness of φ(·).\nNow we focus on H −∇2fD(W ∗). We again consider each block.\n∆i,l = E x∼Dd\n[ v∗i v ∗ l (φ ′(w>i x)φ ′(w>l x)− φ′(w∗>i x)φ′(w∗>l x))xx> ] .\nWe used the boundedness of φ′′(z) to prove Lemma D.10. Here we can’t use this condition. Without smoothness, we will stop at the following position.∥∥∥∥ Ex∼Dd[v∗i v∗l (φ′(w>i x)φ′(w>l x)− φ′(w∗>i x)φ′(w∗>l x))xx>]\n∥∥∥∥ ≤ |v∗i v∗l | max‖a‖=1 E x∼Dd [ |φ′(w>i x)φ′(w>l x)− φ′(w∗>i x)φ′(w∗>l x)|(x>a)2\n] ≤ |v∗i v∗l | max‖a‖=1 E x∼Dd [ |φ′(w>i x)− φ′(w∗>i x)||φ′(w>l x)|\n+ |φ′(w∗>i x)||φ′(w>l x)− φ′(w∗>l x)|(x>a)2 ]\n= |v∗i v∗l | max‖a‖=1\n( E\nx∼Dd\n[ |φ′(w>i x)− φ′(w∗>i x)||φ′(w>l x)|(x>a)2 ] + E\nx∼Dd\n[ |φ′(w∗>i x)||φ′(w>l x)− φ′(w∗>l x)|(x>a)2 ]) . (17)\nwhere the first step follows by definition of spectral norm, the second step follows by triangle inequality, and the last step follows by linearity of expectation.\nWithout loss of generality, we just bound the first term in the above formulation. Let U be the orthogonal basis of span(wi, w∗i , wl). If wi, w ∗ i , wl are independent, U is d-by-3. Otherwise it can be d-by-rank(span(wi, w∗i , wl)). Without loss of generality, we assume U = span(wi, w ∗ i , wl) is d-by-3. Let [vi v∗i vl] = U >[wi w ∗ i wl] ∈ R3×3, and [ui u∗i ul] = U>⊥ [wi w∗i wl] ∈ R(d−3)×3. Let a = Ub+U⊥c, where U⊥ ∈ Rd×(d−3) is the complementary matrix of U .\nE x∼Dd\n[ |φ′(w>i x)− φ′(w∗>i x)||φ′(w>l x)|(x>a)2 ] = E\nx∼Dd\n[ |φ′(w>i x)− φ′(w∗>i x)||φ′(w>l x)|(x>(Ub+ U⊥c))2 ] . E\nx∼Dd\n[ |φ′(w>i x)− φ′(w∗>i x)||φ′(w>l x)| ( (x>Ub)2 + (x>U⊥c) 2 )]\n= E x∼Dd\n[ |φ′(w>i x)− φ′(w∗>i x)||φ′(w>l x)|(x>Ub)2 ] + E\nx∼Dd\n[ |φ′(w>i x)− φ′(w∗>i x)||φ′(w>l x)|(x>U⊥c)2 ] = E\nz∼D3\n[ |φ′(v>i z)− φ′(v∗>i z)||φ′(v>l z)|(z>b)2 ] + E\ny∼Dd−3\n[ |φ′(u>i y)− φ′(u∗>i y)||φ′(u>l y)|(y>c)2 ] (18)\nwhere the first step follows by a = Ub+ U⊥c, the last step follows by (a+ b)2 ≤ 2a2 + 2b2. By Property 3.3(b), we have e exceptional points which have φ′′(z) 6= 0. Let these e points be p1, p2, · · · , pe. Note that if v>i z and v∗>i z are not separated by any of these exceptional points, i.e., there exists no j ∈ [e] such that v>i z ≤ pj ≤ v∗>i z or v∗>i z ≤ pj ≤ v>i z, then we have φ′(v>i z) = φ ′(v∗>i z) since φ ′′(s) are zeros except for {pj}j=1,2,··· ,e. So we consider the probability that v>i z, v ∗> i z are separated by any exception point. We use ξj to denote the event that v > i z, v ∗> i z\nare separated by an exceptional point pj . By union bound, 1− ∑e\nj=1 Pr[ξj ] is the probability that v>i z, v ∗> i z are not separated by any exceptional point. The first term of Equation (18) can be bounded as,\nE z∼D3\n[ |φ′(v>i z)− φ′(v∗>i z)||φ′(v>l z)|(z>b)2 ] = E\nz∼D3\n[ 1∪ej=1ξj |φ ′(v>i z) + φ ′(v∗>i z)||φ′(v>l z)|(z>b)2 ] ≤ (\nE z∼D3\n[ 1∪ej=1ξj ])1/2( E\nz∼D3\n[ (φ′(v>i z) + φ ′(v∗>i z)) 2φ′(v>l z) 2(z>b)4 ])1/2\n≤  e∑ j=1 Pr z∼D3 [ξj ] 1/2( E z∼D3 [ (φ′(v>i z) + φ ′(v∗>i z)) 2φ′(v>l z) 2(z>b)4 ])1/2\n.  e∑ j=1 Pr z∼D3 [ξj ] 1/2 (‖vi‖p + ‖v∗i ‖p)‖vl‖p‖b‖2 where the first step follows by if v>i z, v ∗> i z are not separated by any exceptional point then φ\n′(v>i z) = φ′(v∗>i z) and the last step follows by Hölder’s inequality and Property 3.1.\nIt remains to upper bound Prz∼D3 [ξj ]. First note that if v>i z, v ∗> i z are separated by an excep-\ntional point, pj , then |v∗>i z − pj | ≤ |v>i z − v∗>i z| ≤ ‖vi − v∗i ‖‖z‖. Therefore,\nPr z∼D3 [ξj ] ≤ Pr z∼D3 [ |v>i z − pj | ‖z‖ ≤ ‖vi − v∗i ‖ ] .\nNote that ( v ∗> i z\n‖z‖‖v∗i ‖ + 1)/2 follows Beta(1,1) distribution which is uniform distribution on [0, 1].\nPr z∼D3 [ |v∗>i z − pj | ‖z‖‖v∗i ‖ ≤ ‖vi − v ∗ i ‖ ‖v∗i ‖ ] ≤ Pr z∼D3 [ |v∗>i z| ‖z‖‖v∗i ‖ ≤ ‖vi − v ∗ i ‖ ‖v∗i ‖ ] . ‖vi − v∗i ‖ ‖v∗i ‖ . ‖W −W ∗‖ σk(W ∗) ,\nwhere the first step is because we can view v ∗> i z ‖z‖ and pj ‖z‖ as two independent random variables: the former is about the direction of z and the later is related to the magnitude of z. Thus, we have\nE z∈D3\n[|φ′(v>i z)− φ′(v∗>i z)||φ′(v>l z)|(z>b)2] . (e‖W −W ∗‖/σk(W ∗))1/2σ 2p 1 (W ∗)‖b‖2. (19)\nSimilarly we have\nE y∈Dd−3\n[|φ′(u>i y)− φ′(u∗>i y)||φ′(u>l y)|(y>c)2] . (e‖W −W ∗‖/σk(W ∗))1/2σ 2p 1 (W ∗)‖c‖2. (20)\nFinally combining Eq. (17), Eq. (19) and Eq. (20), we have\n‖H −∇2fD(W ∗)‖ . kv2max(e‖W −W ∗‖/σk(W ∗))1/2σ 2p 1 (W ∗),\nwhich completes the proof."
    }, {
      "heading" : "D.5 Positive Definiteness for a Small Region",
      "text" : "Here we introduce a lemma which shows that the Hessian of any W , which may be dependent on the samples but is very close to an anchor point, is close to the Hessian of this anchor point.\nLemma D.16. Let S denote a set of samples from Distribution D defined in Eq. (1). Let W a ∈ Rd×k be a point (respect to function f̂S(W )), which is independent of the samples S, satisfying ‖W a −W ∗‖ ≤ σk/2. Assume φ satisfies Property 3.1, 3.2 and 3.3(a). Then for any t ≥ 1, if\n|S| ≥ dpoly(log d, t),\nwith probability at least 1− d−t, for any W (which is not necessarily to be independent of samples) satisfying ‖W a −W‖ ≤ σk/4, we have\n‖∇2f̂S(W )−∇2f̂S(W a)‖ ≤ kv2maxσ p 1(‖W a −W ∗‖+ ‖W −W a‖d(p+1)/2).\nProof. Let ∆ = ∇2f̂S(W )−∇2f̂S(W a) ∈ Rdk×dk, then ∆ can be thought of as k2 blocks, and each block has size d× d. The off-diagonal blocks are,\n∆i,l = v ∗ i v ∗ l\n1 |S| ∑ x∈S ( φ′(w>i x)φ ′(w>l x)− φ′(wa>i x)φ′(wa>l x) ) xx>\nFor diagonal blocks,\n∆i,i = 1 |S| ∑ x∈S  k∑ q=1 v∗qφ(w > q x)− y  v∗i φ′′(w>i x)xx> + v∗2i φ′2(w>i x)xx> \n− 1 |S| ∑ x∈S  k∑ q=1 v∗qφ(w a> q x)− y  v∗i φ′′(wa>i x)xx> + v∗2i φ′2(wa>i x)xx> \nWe further decompose ∆i,i into ∆i,i = ∆ (1) i,i + ∆ (2) i,i , where\n∆ (1) i,i = v ∗ i\n1 |S| ∑ x∈S  k∑ q=1 v∗qφ(w > q x)− y φ′′(w>i x)−  k∑ q=1 v∗qφ(w a> q x)− y φ′′(wa>i x) xx>,\nand\n∆ (2) i,i =v ∗2 i\n1 |S| ∑\n(x,y)∈S\nφ′2(w>i x)xx > − φ′2(wa>i x)xx>. (21)\nWe can further decompose ∆(1)i,i into ∆ (1,1) i,i and ∆ (1,2) i,i ,\n∆ (1) i,i = v ∗ i\n1 |S| ∑ x∈S  k∑ q=1 v∗qφ(w > q x)− y φ′′(w>i x)−  k∑ q=1 v∗qφ(w a> q x)− y φ′′(wa>i x)xx> = v∗i 1\n|S| ∑ x∈S  k∑ q=1 v∗qφ(w > q x)− k∑ q=1 v∗qφ(w a> q x) φ′′(w>i x)xx> + v∗i 1\n|S| ∑ x∈S  k∑ q=1 v∗qφ(w a> q x)− y  (φ′′(w>i x)− φ′′(wa>i x))xx> = v∗i 1\n|S| ∑ x∈S k∑ q=1 v∗q (φ(w > q x)− φ(wa>q x))φ′′(w>i x)xx>\n+ v∗i 1 |S| ∑ x∈S k∑ q=1 v∗q (φ(w a> q x)− φ(w∗>q x))(φ′′(w>i x)− φ′′(wa>i x))xx>\n= ∆ (1,1) i,i + ∆ (1,2) i,i .\nCombining Claim D.17 and Claim D.18 , we have if\nn ≥ dpoly(log d, t)\nwith probability at least 1− 1/d4t,\n‖∆(1)i,i ‖ . kv 2 maxσ p 1(‖W a −W ∗‖+ ‖W a −W‖d(p+1)/2). (22)\nTherefore, combining Eq. (22), Claim D.19 and Claim D.20, we complete the proof.\nClaim D.17. For each i ∈ [k], if n ≥ dpoly(log d, t), then\n‖∆(1,1)i,i ‖ . kv 2 maxσ p 1‖W a −W‖d(p+1)/2\nProof. Define function h1(x) = ‖x‖p+1 and h2(x) = |w∗>q x|p|(w∗q − waq )>x|. Note that h1 and h2 don’t contain W which maybe depend on the samples. Therefore, we can use the modified matrix Bernstein inequality (Corollary B.8) to bound ∆(1)i,i .\n(I) Bounding |h1(x)|. By Fact B.2, we have h1(x) . (td log n)(p+1)/2 with probability at least 1− 1/(nd4t). (II) Bounding ‖Ex∼Dd [‖x‖p+1xx>]‖.\nLet g(x) = (2π)−d/2e−‖x‖2/2. Note that xg(x)dx = −dg(x).\nE x∼Dd\n[ ‖x‖p+1xx> ] = ∫ ‖x‖p+1g(x)xx>dx\n= − ∫ ‖x‖p+1d(g(x))x>\n= − ∫ ‖x‖p+1d(g(x)x>) + ∫ ‖x‖p+1g(x)Iddx\n= ∫ d(‖x‖p+1)g(x)x> + ∫ ‖x‖p+1g(x)Iddx\n= ∫ (p+ 1)‖x‖p−1g(x)xx>dx+ ∫ ‖x‖p+1g(x)Iddx\n∫ ‖x‖p+1g(x)Iddx\n= E x∼Dd [‖x‖p+1]Id.\nSince ‖x‖2 follows χ2 distribution with degree d, Ex∼Dd [‖x‖q] = 2q/2 Γ((d+q)/2) Γ(d/2) for any q ≥ 0. So, dq/2 . Ex∼Dd [‖x‖q] . dq/2. Hence, ‖Ex∼Dd [h1(x)xx>]‖ & d(p+1)/2. Also∥∥∥∥ Ex∼Dd [ h1(x)xx > ]∥∥∥∥ ≤ max‖a‖=1 Ex∼Dd [ h1(x)(x >a)2 ]\n≤ max ‖a‖=1\n( E\nx∼Dd\n[ h21(x) ])1/2( E\nx∼Dd\n[ (x>a)4 ])1/2 . d(p+1)/2.\n(III) Bounding (Ex∼Dd [h41(x)])1/4.\n( E\nx∼Dd [h41(x)]\n)1/4 . d(p+1)/2.\nDefine function B(x) = h(x)xx> ∈ Rd×d, ∀i ∈ [n]. Let B = Ex∼Dd [h(x)xx>]. Therefore by applying Corollary B.8, we obtain for any 0 < < 1, if\nn ≥ −2dpoly(log d, t)\nwith probability at least 1− 1/dt,∥∥∥∥∥ 1|S|∑ x∈S ‖x‖p+1xx> − E x∼Dd [ ‖x‖p+1xx> ]∥∥∥∥∥ . δd(p+1)/2. Therefore we have with probability at least 1− 1/dt,∥∥∥∥∥ 1|S|∑\nx∈S ‖x‖p+1xx>\n∥∥∥∥∥ . d(p+1)/2. (23)\nClaim D.18. For each i ∈ [k], if n ≥ dpoly(log d, t), then\n‖∆(1,2)i,i ‖ . kσ p 1‖W a −W ∗‖,\nholds with probability at least 1− 1/d4t.\nProof. Recall the definition of ∆(1,2)i,i ,\n∆ (1,2) i,i = v ∗ i\n1 |S| ∑ x∈S k∑ q=1 v∗q (φ(w a> q x)− φ(w∗>q x))(φ′′(w>i x)− φ′′(wa>i x))xx>\nIn order to upper bound the ‖∆(1,2)i,i ‖, it suffices to upper bound the spectral norm of this quantity, 1 |S| ∑ x∼S (φ(wa>q x)− φ(w∗>q x))(φ′′(w>i x)− φ′′(wa>i x))xx>.\nBy Property 3.1, we have\n|φ(wa>q x)− φ(w∗>q x)| . L1(|wa>q x|p + |w∗>q x|)|(w∗q − waq )>x|.\nBy Property 3.3, we have |φ′′(w>i x)− φ′′(wa>i x)| ≤ 2L2. For the second part |w∗>q x|p|(w∗q − waq )>x|xx>, according to Eq. (15), we have with probability 1− d−t, if n ≥ d poly(log d, t),∥∥∥∥∥ Ex∼Dd [ |w∗>q x|p|(w∗q − waq )>x|xx> ] − 1 |S| ∑ x∈S |w∗>q x|p|(w∗q − waq )>x|xx>\n∥∥∥∥∥ . δ‖w∗q‖p‖w∗q − waq‖. Also, note that ∥∥∥∥ Ex∼Dd [ |w∗>q x|p|(w∗q − waq )>x|xx>\n]∥∥∥∥ . ‖w∗q‖p‖w∗q − waq‖. Thus, we obtain ∥∥∥∥∥ 1|S|∑\nx∈S |w∗>q x|p|(w∗q − waq )>x|xx> ∥∥∥∥∥ . ‖W a −W ∗‖σp1 . (24) Claim D.19. For each i ∈ [k], if n ≥ dpoly(log d, t), then\n‖∆(2)i,i ‖ . kv 2 maxσ p 1‖W −W a‖d(p+1)/2\nholds with probability 1− 1/d4t. Proof. We have\n‖∆(2)i,i ‖ ≤ v ∗2 i ∥∥∥∥∥ 1|S|∑ x∈S ( (φ′(w>i xj)− φ′(wa>i x)) · (φ′(w>i x) + φ′(wa>i x)) ) xx> ∥∥∥∥∥ ≤ v∗2i\n∥∥∥∥∥ 1|S|∑ x∈S ( L2|(wi − wai )>x| · L1(|w>i x|p + |wa>i x|p) ) xx> ∥∥∥∥∥ ≤ v∗2i ‖W −W a‖\n∥∥∥∥∥ 1|S|∑ x∈S ( L2‖x‖ · L1(‖wi‖p‖x‖p + |wa>i x|p) ) xx> ∥∥∥∥∥ . Applying Corollary B.8 finishes the proof.\nClaim D.20. For each i ∈ [k], j ∈ [k], i 6= j, if n ≥ dpoly(log d, t), then\n‖∆i,l‖ . v2maxσ p 1‖W a −W‖\nholds with probability 1− d4t.\nProof. Recall the definition of ∆i,l,\n∆i,l = v ∗ i v ∗ l\n1 |S| ∑ x∈S ( φ′(w>i x)φ ′(w>l x)− φ′(w>i x)φ′(wa>l x)\n+φ′(w>i x)φ ′(wa>l x)− φ′(wa>i x)φ′(wa>l x)\n) xx>\n= v∗i v ∗ l\n1 |S| ∑ x∈S ( φ′(w>i x)φ ′(w>l x)− φ′(w>i x)φ′(wa>l x) )\n+ v∗i v ∗ l\n1 |S| ∑ x∈S ( φ′(w>i x)φ ′(wa>l x)− φ′(wa>i x)φ′(wa>l x) ) xx>\n|v∗i v∗l | 1 |S| ∑ x∈S ( L1‖wi‖pL2‖wl − wal ‖‖x‖p+1 + L2‖wi − wai ‖‖x‖L1‖wa>l x‖p ) xx>\nApplying Corollary B.8 completes the proof."
    }, {
      "heading" : "E Tensor Methods",
      "text" : ""
    }, {
      "heading" : "E.1 Tensor Initialization Algorithm",
      "text" : "We describe the details of each procedure in Algorithm 1 in this section. a) Compute the subspace estimation from P̂2 (Algorithm 3). Note that the eigenvalues of P2 and P̂2 are not necessarily nonnegative. However, only k of the eigenvalues will have large magnitude. So we can first compute the top-k eigenvectors/eigenvalues of both C · I + P̂2 and C · I − P̂2, where C is large enough such that C ≥ 2‖P2‖. Then from the 2k eigen-pairs, we pick the top-k eigenvectors with the largest eigenvalues in magnitude, which is executed in TopK in Algorithm 3. For the outputs of TopK, k1, k2 are the numbers of picked largest eigenvalues from C · I + P̂2 and C · I − P̂2 respectively and π1(i) returns the original index of i-th largest eigenvalue from C · I + P̂2 and similarly π2 is for C · I − P̂2. Finally orthogonalizing the picked eigenvectors leads to an estimation of the subspace spanned by {w∗1 w∗2 · · · w∗k}. Also note that forming P̂2 takes O(n · d2) time and each step of the power method doing a multiplication between a d × d matrix and a d × k matrix takes k · d2 time by a naive implementation. Here we reduce this complexity from O((k + n)d2) to O(knd). The idea is to compute each step of the power method without explicitly forming P̂2. We take P2 = M2 as an example; other cases are similar. In Algorithm 3, let the step P̂2V be calculated as P̂2V = 1|S| ∑ (x,y)∈S y(x(x\n>V ) − V ). Now each iteration only needs O(knd) time. Furthermore, the number of iterations required will be a small number, since the power method has a linear convergence rate and as an initialization method we don’t need a very accurate solution. The detailed algorithm is shown in Algorithm 3. The approximation error bound of P̂2 to P2 is provided in Lemma E.5. Lemma E.6 provides the theoretical bound for Algorithm 3.\nb) Form and decompose the 3rd-moment R̂3 (Algorithm 1 in [KCL15]). We apply the non-orthogonal tensor factorization algorithm, Algorithm 1 in [KCL15], to decompose R̂3. According to Theorem 3 in [KCL15], when R̂3 is close enough to R3, the output of the algorithm, ûi will\nclose enough to siV >w∗i , where si is an unknown sign. Lemma E.10 provides the error bound for ‖R̂3 −R3‖.\nc) Recover the magnitude of w∗i and the signs si, v ∗ i (Algorithm 4). For Algorithm 4, we only consider homogeneous functions. Hence we can assume v∗i ∈ {−1, 1} and there exist some universal constants cj such that mj,i = cj‖w∗i ‖p+1 for j = 1, 2, 3, 4, where p + 1 is the degree of homogeneity. Note that different activations have different situations even under Assumption 5.3. In particular, ifM4 = M2 = 0, φ(·) is an odd function and we only need to know siv∗i . IfM3 = M1 = 0, φ(·) is an even function, so we don’t care about what si is.\nLet’s describe the details for Algorithm 4. First define two quantities Q1 and Q2,\nQ1 = Ml1(I, α, · · · , α︸ ︷︷ ︸ (l1−1) α’s ) = k∑ i=1 v∗i cl1‖w∗i ‖p+1(α>w∗i )l1−1w∗i , (25)\nQ2 = Ml2(V, V, α, · · · , α︸ ︷︷ ︸ (l2−2) α’s ) = k∑ i=1 v∗i cl2‖w∗i ‖p+1(α>w∗i )l2−2(V >w∗i )(V >w∗i )>, (26)\nwhere l1 ≥ 1 such that Ml1 6= 0 and l2 ≥ 2 such that Ml2 6= 0. There are possibly multiple choices for l1 and l2. We will discuss later on how to choose them. Now we solve two linear systems.\nz∗ = argmin z∈Rk ∥∥∥∥∥ k∑ i=1 zisiw ∗ i −Q1 ∥∥∥∥∥ , and r∗ = argminr∈Rk ∥∥∥∥∥ k∑ i=1 riV >w∗i (V >w∗i ) > −Q2 ∥∥∥∥∥ F . (27)\nThe solutions of the above linear systems are\nz∗i = v ∗ i s l1 i cl1‖w ∗ i ‖p+1(α>siw∗i )l1−1, and r∗i = v∗i s l2 i cl2‖w ∗ i ‖p+1(α>siw∗i )l2−2.\nWe can approximate siw∗i by V ûi and approximate Q1 and Q2 by their empirical versions Q̂1 and Q̂2 respectively. Hence, in practice, we solve\nẑ = argmin z∈Rk ∥∥∥∥∥ k∑ i=1 ziV ûi − Q̂1 ∥∥∥∥∥ , and r̂ = argminr∈Rk ∥∥∥∥∥ k∑ i=1 riûiû > i − Q̂2 ∥∥∥∥∥ F\n(28)\nSo we have the following approximations,\nẑi ≈ v∗i s l1 i cl1‖w ∗ i ‖p+1(α>V ûi)l1−1, and r̂i ≈ v∗i s l2 i cl2‖w ∗ i ‖p+1(α>V ûi)l2−2, ∀i ∈ [k].\nIn Lemma E.13 and Lemma E.14, we provide robustness of the above two linear systems, i.e., the solution errors, ‖ẑ − z∗‖ and ‖r̂ − r∗‖, are bounded under small perturbations of w∗i , Q1 and Q2. Recall that the final goal is to approximate ‖w∗i ‖ and the signs v∗i , si. Now we can approximate ‖w∗i ‖ by (|ẑi/(cl1(α>V ûi)l1−1)|)1/(p+1). To recover v∗i , si, we need to note that if l1 and l2 are both odd or both even, we can’t recover both v∗i and si. So we consider the following situations,\n1. If M1 = M3 = 0, we choose l1 = l2 = min{j ∈ {2, 4}|Mj 6= 0}. Return v(0)i = sign(r̂icl2) and s\n(0) i being −1 or 1.\n2. If M2 = M4 = 0, we choose l1 = min{j ∈ {1, 3}|Mj 6= 0}, l2 = 3. Return v(0)i being −1 or 1 and s(0)i = sign(v (0) i ẑicl1).\nAlgorithm 3 Power Method via Implicit Matrix-Vector Multiplication\n1: procedure PowerMethod(P̂2, k) 2: C ← 3‖P̂2‖, T ← a large enough constant. 3: Initial guess V̂ (0)1 ∈ R d×k, V̂ (0) 2 ∈ R d×k 4: for t = 1→ T do 5: V̂\n(t) 1 ← QR(CV̂ (t−1) 1 + P̂2V̂ (t−1) 1 ) . P̂2V̂ (t−1) 1 is not calculated directly, see Sec. E.1(a)\n6: V̂ (t) 2 ← QR(CV̂ (t−1) 2 − P̂2V̂ (t−1)\n2 ) 7: end for 8: for j = 1, 2 do 9: V̂\n(T ) j ← [ v̂j,1 v̂j,2 · · · v̂j,k ] 10: for i = 1→ k do 11: λj,i ← |v̂>j,iP̂2v̂j,i| . Calculate the absolute of eigenvalues 12: end for 13: end for 14: π1, π2, k1, k2 ← TopK(λ, k) . πj : [kj ]→ [k] and k1 + k2 = k, see Sec. E.1(a) 15: for j = 1, 2 do 16: Vj ← [ v̂j,πj(1) v̂1,πj(2) · · · v̂j,πj(kj)\n] 17: end for 18: Ṽ2 ← QR((I − V1V >1 )V2) 19: V ← [V1, Ṽ2] 20: return V 21: end procedure\n3. Otherwise, we choose l1 = min{j ∈ {1, 3}|Mj 6= 0}, l2 = min{j ∈ {2, 4}|Mj 6= 0}. Return v\n(0) i = sign(r̂icl2) and s (0) i = sign(v (0) i ẑicl1).\nThe 1st situation corresponds to part 3 of Assumption 5.3,where si doesn’t matter, and the 2nd situation corresponds to part 4 of Assumption 5.3, where only siv∗i matters. So we recover ‖w∗i ‖ to some precision and v∗i , si exactly provided enough samples. The recovery of w ∗ i and v ∗ i then follows.\nSample complexity: We use matrix Bernstein inequality to bound the error between P̂2 and P2, which requires Ω̃(d) samples (Lemma E.5). To bound the estimation error between R3 and R̂3, we flatten the tensor to a matrix and then use matrix Bernstein inequality to bound the error, which requires Ω̃(k3) samples (Lemma E.10). In Algorithm 4, we also need to approximate a Rd vector and a Rk×k matrix, which also requires Ω̃(d). Thus, taking Õ(d) + Õ(k3) samples is sufficient.\nTime complexity: In Part a), by using a specially designed power method, we only need O(knd) time to compute the subspace estimation V . Part b) needs O(knd) to form R̂3 and the tensor factorization needs O(k3) time. Part c) requires calculation of d×k and k2×k linear systems in Eq. (28), which takes at most O(knd) running time. Hence, the total time complexity is O(knd)."
    }, {
      "heading" : "E.2 Main Result for Tensor Methods",
      "text" : "The goal of this Section is to prove Theorem 5.6.\nTheorem 5.6. Let the activation function be homogeneous satisfying Assumption 5.3. For any 0 < < 1 and t ≥ 1, if |S| ≥ −2 ·d ·poly(t, k, κ, log d), then there exists an algorithm (Algorithm 1) that takes |S|k · Õ(d) time and outputs a matrix W (0) ∈ Rd×k and a vector v(0) ∈ Rk such that, with\nAlgorithm 4 Recovery of the Ground Truth Parameters of the Neural Network, i.e., w∗i and v ∗ i\n1: procedure RecMagSign(V, {ûi}i∈[k], S) 2: if M1 = M3 = 0 then 3: l1 ← l2 ← min{j ∈ {2, 4}|Mj 6= 0} 4: else if M2 = M4 = 0 then 5: l1 ← min{j ∈ {1, 3}|Mj 6= 0}, l2 ← 3 6: else 7: l1 ← min{j ∈ {1, 3}|Mj 6= 0}, l2 ← min{j ∈ {2, 4}|Mj 6= 0}. 8: end if 9: S1, S2 ← Partition(S, 2) . |S1|, |S2| = Ω̃(d) 10: Choose α to be a random unit vector 11: Q̂1 ← ES1 [Q1] . Q̂1 is the empirical version of Q1(defined in Eq.(25)) 12: Q̂2 ← ES2 [Q2] . Q̂2 is the empirical version of Q2(defined in Eq.(26)) 13: ẑ ← argminz\n∥∥∥∑ki=1 ziV ûi − Q̂1∥∥∥ 14: r̂ ← argminr ∥∥∥∑ki=1 riûiû>i − Q̂2∥∥∥ F 15: v (0) i ← sign(r̂icl2) 16: s (0) i ← sign(v (0) i ẑicl1) 17: w (0) i ← s (0) i (|ẑi/(cl1(α>V ûi)l1−1)|)1/(p+1)V ûi 18: return v(0)i ,w (0) i 19: end procedure\nprobability at least 1− d−Ω(t),\n‖W (0) −W ∗‖F ≤ · poly(k, κ)‖W ∗‖F , and v(0)i = v ∗ i .\nProof. The success of Algorithm 1 depends on two approximations. The first is the estimation of the normalized w∗i up to some unknown sign flip, i.e., the error ‖w∗i − siV ûi‖ for some si ∈ {−1, 1}. The second is the estimation of the magnitude of w∗i and the signs v ∗ i , si which is conducted in Algorithm 4. For the first one,\n‖w∗i − siV ûi‖ ≤ ‖V V >w∗i − w∗i ‖+ ‖V V >w∗i − V siûi‖ = ‖V V >w∗i − w∗i ‖+ ‖V >w∗i − siûi‖, (29)\nwhere the first step follows by triangle inequality, the second step follows by V >V = I. We can upper bound ‖V V >w∗i − w∗i ‖,\n‖V V >w∗i − w∗i ‖ ≤ (‖P̂2 − P2‖/σk(P2) + )\n≤ (poly(k, κ)‖P̂2 − P2‖+ ) ≤ poly(k, κ) , (30)\nwhere the first step follows by Lemma E.6, the second step follows by σk(P2) ≥ 1/poly(k, κ), and the last step follows by ‖P̂2 − P2‖ ≤ poly(k, κ) if the number of samples is proportional to Õ(d/ 2) as shown in Lemma E.5.\nWe can upper bound ‖V >w∗i − siûi‖,\n‖V >w∗i − siûi‖ ≤ poly(k, κ)‖R̂3 −R3‖ ≤ poly(k, κ), (31)\nwhere the first step follows by Theorem 3 in [KCL15], and the last step follows by ‖R̂3 − R3‖ ≤ poly(k, κ) if the number of samples is proportional to Õ(k2/ 2) as shown in Lemma E.10.\nCombining Eq. (29), (30) and (31) together,\n‖w∗i − siV ûi‖ ≤ poly(k, κ).\nFor the second one, we can bound the error of the estimation of moments, Q1 and Q2, using number of samples proportional to Õ(d) by Lemma E.12 and Lemma E.5 respectively. The error of the solutions of the linear systems Eq.(28) can be bounded by ‖Q1− Q̂1‖, ‖Q2− Q̂2‖, ‖ûi− V >w∗i ‖ and ‖(I − V V >)w∗i ‖ according to Lemma E.13 and Lemma E.14. Then we can bound the error of the output of Algorithm 4. Furthermore, since v∗i ’s are discrete values, they can be exactly recovered. All the sample complexities mentioned in the above lemmata are linear in dimension and polynomial in other factors to achieve a constant error. So accumulating all these errors we complete our proof.\nRemark E.1. The proofs of these lemmata for Theorem 1 can be found in the following sections. Note that these lemmata also hold for any activations satisfying Property 3.1 and Assumption 5.3. However, since we are unclear how to implement the last step of Algorithm 1 (Algorithm 4) for general non-homogeneous activations, we restrict our theorem to homogeneous activations only."
    }, {
      "heading" : "E.3 Error Bound for the Subspace Spanned by the Weight Matrix",
      "text" : ""
    }, {
      "heading" : "E.3.1 Error Bound for the Second-order Moment in Different Cases",
      "text" : "Lemma E.2. Let M2 be defined as in Definition 5.1. Let M̂2 be the empirical version of M2, i.e.,\nM̂2 = 1 |S| ∑\n(x,y)∈S\ny · (x⊗ x− Id),\nwhere S denote a set of samples from Distribution D defined in Eq. (1). Assume M2 6= 0, i.e., m\n(2) i 6= 0 for any i. Then for any 0 < < 1, t ≥ 1, if\n|S| ≥ max i∈[k] (‖w∗i ‖p+1/|m2,i|+ 1) · −2dpoly(log d, t)\nwith probability at least 1− d−t,\n‖M2 − M̂2‖ ≤ k∑ i=1 |v∗im2,i|.\nProof. Recall that, for each sample (x, y), y = ∑k\ni=1 v ∗ i φ(w ∗> i x). We consider each component\ni ∈ [k]. Define function Bi(x) : Rd → Rd×d such that\nBi(x) = φ(w ∗> i x) · (x⊗ x− Id). Define g(z) = φ(z)−φ(0), then |g(z)| = | ∫ z\n0 φ ′(s)ds| ≤ L1/(p+1)|z|p+1, which follows Property 3.1.\n(I) Bounding ‖Bi(x)‖.\n‖Bi(x)‖ . ( L1 p+ 1 |w∗>i x|p+1 + |φ(0)|)(‖xj‖2 + 1)\n. ( L1 p+ 1 ‖w∗i ‖p+1 + |φ(0)|)dpoly(log d, t)\nwhere the last step follows by Fact B.1 and Fact B.2. (II) Bounding ‖Ex∼Dd [Bi(x)]‖. Note that Ex∼Dd [Bi(x)] = m2,iw∗iw∗>i . Therefore, ‖Ex∼Dd [Bi(x)]‖ = |m2,i|. (III) Bounding max(Ex∼Dd ‖Bi(x)Bi(x)>‖,Ex∼Dd ‖Bi(x)>Bi(x)‖). Note that Bi(x) is a symmetric matrix, thus it suffices to only bound one of them.∥∥∥∥ Ex∼Dd[B2i (x)] ∥∥∥∥ . ( Ex∼Dd[φ(w∗>i x)4] )1/2( E x∼Dd [‖x‖4] )1/2 . ( L1 p+ 1 ‖w∗i ‖p+1 + |φ(0)|)2d.\n(IV) Bounding max‖a‖=‖b‖=1(Ex∼Dd [(a>Bi(x)b)2]). Note that Bi(x) is a symmetric matrix, thus it suffices to consider the case where a = b.\nmax ‖a‖=1\n( E\nx∼Dd\n[ (a>Bi(x)a) 2 ])1/2 . ( E\nx∼Dd [φ4(w∗>i x)]\n)1/4 .\nL1 p+ 1 ‖w∗i ‖p+1 + |φ(0)|.\nDefine L = ‖w∗i ‖p+1 + |φ(0)|. Then we have for any 0 < < 1, if\nn & L2d+ |m2,i|2 + L|m2,i|d · poly(log d, t)\n2|m2,i|2 t log d\nwith probability at least 1− 1/dt,∥∥∥∥∥ Ex∼Dd[Bi(x)]− 1|S|∑ x∈S Bi(x) ∥∥∥∥∥ ≤ |m2,i|.\nLemma E.3. Let M3 be defined as in Definition 5.1. Let M̂3 be the empirical version of M3, i.e.,\nM̂3 = 1 |S| ∑\n(x,y)∈S\ny · (x⊗3 − x⊗̃I),\nwhere S denote a set of samples (each sample is i.i.d. sampled from Distribution D defined in Eq. (1)). Assume M3 6= 0, i.e., m3,i 6= 0 for any i. Let α be a fixed unit vector. Then for any 0 < < 1, t ≥ 1, if\n|S| ≥ max i∈[k] (‖w∗i ‖p+1/|m3,i(w∗>i α)|2 + 1) · −2dpoly(log d, t)\nwith probability at least 1− 1/dt,\n‖M3(I, I, α)− M̂3(I, I, α)‖ ≤ k∑ i=1 |v∗im3,i(w∗>i α)|.\nProof. Since y = ∑k\ni=1 v ∗ i φ(w ∗> i x). We consider each component i ∈ [k].\nDefine function Bi(x) : Rd → Rd×d such that\nBi(x) = [φ(w ∗> i x) · (x⊗3 − x⊗̃I)](I, I, α) = φ(w∗>i x) · ((x>α)x⊗2 − α>xI − αx> − xα>).\nDefine g(z) = φ(z) − φ(0), then |g(z)| = | ∫ z\n0 φ ′(s)ds| ≤ L1p+1 |z| p+1 . |z|p+1, which follows Property 3.1. In order to apply Lemma B.7, we need to bound the following four quantities,\n(I) Bounding ‖Bi(x)‖.\n‖Bi(x)‖ = ‖φ(w∗>i x) · ((x>α)x⊗2 − α>xId − αx> − xα>)‖ ≤ |φ(w∗>i x)| · ‖(x>α)x⊗2 − α>xI − αx> − xα>‖ . (|w∗>i x|p+1 + |φ(0)|)‖(x>α)x⊗2 − α>xI − αx> − xα>‖ . (|w∗>i x|p+1 + |φ(0)|)(|x>α|‖x‖2 + 3|α>x|),\nwhere the third step follows by definition of g(z), and last step follows by definition of spectral norm and triangle inequality.\nUsing Fact B.1 and Fact B.2, we have for any constant t ≥ 1, with probability 1− 1/(nd4t),\n‖Bi(x)‖ . (‖w∗i ‖p+1 + |φ(0)|)dpoly(log d, t).\n(II) Bounding ‖Ex∼Dd [Bi(x)]‖. Note that Ex∼Dd [Bi(x)] = m3,i(w∗>i α)w∗iw∗>i . Therefore, ‖Ex∼Dd [Bi(x)]‖ = |m3,i(w∗>i α)|. (III) Bounding max(‖Ex∼Dd [Bi(x)Bi(x)>]‖, ‖Ex∼Dd [Bi(x)>Bi(x)]‖). Because matrix Bi(x) is symmetric, thus it suffices to bound one of them,∥∥∥∥ Ex∼Dd[B2i (x)] ∥∥∥∥ . ( Ex∼Dd [ φ(w∗>i x) 4 ])1/2( E x∼Dd [ (x>α)4 ])1/2( E x∼Dd [‖x‖4]\n)1/2 . (‖w∗i ‖p+1 + |φ(0)|)2d.\n(IV) Bounding max‖a‖=‖b‖=1(Ex∼Dd [(a>Bi(x)b)2])1/2.\nmax ‖a‖=1\n( E\nx∼Dd\n[ (a>Bi(x)a) 2 ])1/2 . ( E\nx∼Dd\n[ φ4(w∗>i x) ])1/4 . ‖w∗i ‖p+1 + |φ(0)|.\nDefine L = ‖w∗i ‖p+1 + |φ(0)|. Then we have for any 0 < < 1, if\n|S| & L 2d+ |m3,i(w∗>i α)|2 + L|m3,i(w∗>i α)|d · poly(log d, t)\n2|m3,i(w∗>i α)|2 · t log d\nwith probability at least 1− d−t,∥∥∥∥∥ Ex∼Dd[Bi(x)]− 1|S|∑ x∈S Bi(x) ∥∥∥∥∥ ≤ |m3,i(w∗>i α)|.\nLemma E.4. Let M4 be defined as in Definition 5.1. Let M̂4 be the empirical version of M4, i.e.,\nM̂4 = 1 |S| ∑\n(x,y)∈S\ny · (x⊗4 − (x⊗ x)⊗̃I + I⊗̃I),\nwhere S denote a set of samples (where each sample is i.i.d. sampled are sampled from Distribution D defined in Eq. (1)). Assume M4 6= 0, i.e., m4,i 6= 0 for any i. Let α be a fixed unit vector. Then for any 0 < < 1, t ≥ 1, if\n|S| ≥ max i∈[k] (‖w∗i ‖p+1/|m4,i|(w∗>i α)2 + 1)2 · −2 · dpoly(log d, t)\nwith probability at least 1− 1/dt,\n‖M4(I, I, α, α)− M̂4(I, I, α, α)‖ ≤ k∑ i=1 |v∗im4,i|(w∗>i α)2.\nProof. Since y = ∑k\ni=1 v ∗ i φ(w ∗> i x). We consider each component i ∈ [k].\nDefine function Bi(x) : Rd → Rd×d such that\nBi(x) = [φ(w ∗> i x) · (x⊗4 − (x⊗ x)⊗̃I + I⊗̃I)](I, I, α, α)\n= φ(w∗>i x) · ((x>α)2x⊗2 − (α>x)2I − 2(α>x)(xα> + αx>)− xx> + 2αα> + I).\nDefine g(z) = φ(z) − φ(0), then |g(z)| = | ∫ z\n0 φ ′(s)ds| ≤ L1/(p + 1)|z|p+1 . |z|p+1, which follows\nProperty 3.1. (I) Bounding ‖Bi(x)‖.\n‖Bi(x)‖ .|φ(w∗>i x)| · ((x>α)2‖x‖2 + 1 + ‖x‖2 + (α>x)2) .(|w∗>i x|p+1 + |φ(0)|) · ((x>α)2‖x‖2 + 1 + ‖x‖2 + (α>x)2)\nUsing Fact B.1 and Fact B.2, we have for any constant t ≥ 1, with probability 1− 1/(nd4t),\n‖Bi(x)‖ . (‖w∗i ‖p+1 + |φ(0)|)d poly(log d, t).\n(II) Bounding ‖Ex∼Dd [Bi(x)]‖. Note that Ex∼Dd [Bi(x)] = m4,i(w∗>i α)2w∗iw∗>i . Therefore, ‖Ex∼Dd [Bi(x)]‖ = |m4,i|(w∗>i α)2. (III) Bounding max(Ex∼Dd ‖Bi(x)Bi(x)>‖,Ex∼Dd ‖Bi(x)>Bi(x)‖).\n∥∥∥∥ Ex∼Dd[Bi(x)2] ∥∥∥∥ . ( Ex∼Dd[φ(w∗>i x)4] )1/2( E x∼Dd [(x>α)8] )1/2( E x∼Dd [‖x‖4] )1/2 . (‖w∗i ‖p+1 + |φ(0)|)2d.\n(IV) Bounding max‖a‖=‖b‖=1(Ex∼Dd [(a>Bi(x)b)2])1/2.\nmax ‖a‖=1\n( E\nx∼Dd\n[ (a>Bi(x)a) 2 ])1/2 . ( E\nx∼Dd\n[ φ4(w∗>i x) ])1/4 . ‖w∗i ‖p+1 + |φ(0)|.\nDefine L = ‖w∗i ‖p+1 + |φ(0)|. Then we have for any 0 < < 1, if\nn & L2d+ |m4,i|2(w∗>i α)4 + L|m4,i|(w∗>i α)2dpoly(log d, t)\n2|m4,i|2(w∗>i α)4 · t log d\nwith probability at least 1− d−t,∥∥∥∥∥ Ex∼Dd[Bi(x)]− 1|S|∑ x∈S Bi(x) ∥∥∥∥∥ ≤ |m4,i|(w∗>i α)2."
    }, {
      "heading" : "E.3.2 Error Bound for the Second-order Moment",
      "text" : "The goal of this section is to prove Lemma E.5, which shows we can approximate the second order moments up to some precision by using linear sample complexity in d.\nLemma E.5 (Estimation of the second order moment). Let P2 and j2 be defined in Definition 5.4. Let S denote a set of i.i.d. samples generated from distribution D(defined in (1)). Let P̂2 be the empirical version of P2 using dataset S, i.e., P̂2 = ES [P2]. Assume the activation function satisfies Property 3.1 and Assumption 5.3. Then for any 0 < < 1 and t ≥ 1, and m0 = mini∈[k]{|mj2,i|2(w∗>i α)2(j2−2)}, if\n|S| & σ2p+21 · d · poly(t, log d)/( 2m0),\nthen with probability at least 1− d−Ω(t),\n‖P2 − P̂2‖ ≤ k∑ i=1 |v∗imj2,i(w∗>i α)j2−2|.\nProof. We have shown the bound for j2 = 2, 3, 4 in Lemma E.2, Lemma E.3 and Lemma E.4 respectively. To summarize, for any 0 < < 1 we have if\n|S| ≥ max i∈[k]\n{ (‖w∗i ‖p+1 + |φ(0)|+ |mj2,i(w∗>i α)(j2−2)|)2\n|mj2,i|2(w∗>i α)2(j2−2)\n} · −2dpoly(log d, t)\nwith probability at least 1− d−t,\n‖P2 − P̂2‖ ≤ k∑ i=1 |v∗imj2,i(w∗>i α)j2−2|."
    }, {
      "heading" : "E.3.3 Subspace Estimation Using Power Method",
      "text" : "Lemma E.6 shows a small number of power iterations can estimate the subspace of {w∗i }i∈[k] to some precision, which provides guarantees for Algorithm 3.\nLemma E.6 (Bound on subspace estimation). Let P2 be defined as in Definition. 5.4 and P̂2 be its empirical version. Let U ∈ Rd×k be the orthogonal column span of W ∗ ∈ Rd×k. Assume ‖P̂2 − P2‖ ≤ σk(P2)/10. Let C be a large enough positive number such that C > 2‖P2‖. Then after T = O(log(1/ )) iterations, the output of Algorithm 3, V ∈ Rd×k, will satisfy\n‖UU> − V V >‖ . ‖P̂2 − P2‖/σk(P2) + ,\nwhich implies\n‖(I − V V >)w∗i ‖ . (‖P̂2 − P2‖/σk(P2) + )‖w∗i ‖.\nProof. According to Weyl’s inequality, we are able to pick up the correct numbers of positive eigenvalues and negative eigenvalues in Algorithm 3 as long as P̂2 and P2 are close enough.\nLet U = [U1 U2] ∈ Rd×k be the eigenspace of span{w∗1 w∗2 · · · w∗k}, where U1 ∈ R d×k1 corre-\nsponds to positive eigenvalues of P2 and U2 ∈ Rd×k2 is for negatives.\nLet V 1 be the top-k1 eigenvectors of CI + P̂2. Let V 2 be the top-k2 eigenvectors of CI − P̂2. Let V = [V 1 V 2] ∈ Rd×k.\nAccording to Lemma 9 in [HK13], we have ‖(I−U1U>1 )V 1‖ . ‖P̂2−P2‖/σk(P2),‖(I−U2U>2 )V 2‖ . ‖P̂2 − P2‖/σk(P2) . Using Fact B.4, we have ‖(I − UU>)V ‖ = ‖UU> − V V\n>‖. Let be the precision we want to achieve using power method. Let V1 be the top-k1 eigenvectors returned after O(log(1/ )) iterations of power methods on CI + P̂2 and V2 ∈ Rd×k2 for CI − P̂2 similarly.\nAccording to Theorem 7.2 in [AKZ12], we have ‖V 1V > 1 − V1V >1 ‖ ≤ and ‖V 2V > 2 − V2V >2 ‖ ≤ . Let U⊥ be the complementary matrix of U . Then we have,\n‖(I − U1U>1 )V 1‖ = max‖a‖=1 ‖(I − U1U>1 )V 1a‖\n= max ‖a‖=1\n‖(U⊥U>⊥ + U2U>2 )V 1a‖\n= max ‖a‖=1\n√ ‖U⊥U>⊥V 1a‖2 + ‖U2U>2 V 1a‖2\n≥ max ‖a‖=1 ‖U2U>2 V 1a‖ = ‖U2U>2 V 1‖, (32)\nwhere the first step follows by definition of spectral norm, the second step follows by I = U1U>1 + U2U > 2 + U > ⊥U > ⊥ , the third step follows by U > 2 U⊥ = 0, and last step follows by definition of spectral norm. We can upper bound ‖(I − UU>)V ‖,\n‖(I − UU>)V ‖ ≤ (‖(I − U1U>1 )V 1‖+ ‖(I − U2U>2 )V 2‖+ ‖U2U>2 V 1‖+ ‖U1U>1 V 2‖) ≤ 2(‖(I − U1U>1 )V 1‖+ ‖(I − U2U>2 )V 2‖)\n. ‖P̂2 − P2‖/σk(P2), (33)\nwhere the first step follows by triangle inequality, the second step follows by Eq. (32), and the last step follows by Lemma 9 in [HK13].\nWe define matrix R such that V 2R = (I − V1V >1 )V2 is the QR decomposition of (I − V1V >1 )V2, then we have\n‖(I − V 2V > 2 )V 2‖\n= ‖(I − V 2V > 2 )(I − V1V >1 )V2R−1‖ = ‖(I − V 2V > 2 )(I − V 1V > 1 + V 1V > 1 − V1V >1 )V2R−1‖ ≤ ‖(I − V 2V > 2 )(I − V 1V > 1 )V2R\n−1‖︸ ︷︷ ︸ α + ‖(I − V 2V > 2 )‖‖R−1‖‖V 1V > 1 − V1V >1 ‖︸ ︷︷ ︸\nβ\n,\nwhere the first step follows by V 2 = (I−V1V >1 )V2R−1, and the last step follows by triangle inequality.\nFurthermore, we have,\nα+ β\n= ‖(I − V 2V > 2 − V 1V > 1 )V2R −1‖+ ‖(I − V 2V > 2 )‖‖R−1‖‖V 1V > 1 − V1V >1 ‖ ≤ ‖(I − V 2V > 2 )V2R −1‖+ ‖V 1V > 1 V2R −1‖+ ‖(I − V 2V > 2 )‖‖R−1‖‖V 1V > 1 − V1V >1 ‖ ≤ ‖V 2V > 2 − V2V >2 ‖‖R−1‖+ ‖V 1V > 1 V2‖‖R−1‖+ ‖(I − V 2V > 2 )‖‖R−1‖‖V 1V > 1 − V1V >1 ‖ = ‖V 2V > 2 − V2V >2 ‖‖R−1‖+ ‖V 1V > 1 V2‖‖R−1‖+ ‖R−1‖‖V 1V > 1 − V1V >1 ‖ ≤ ‖V 2V > 2 − V2V >2 ‖‖R−1‖+ ‖(I − V 2V > 2 )V2‖‖R−1‖+ ‖R−1‖‖V 1V > 1 − V1V >1 ‖ ≤ (2‖V 2V > 2 − V2V >2 ‖+ ‖V 1V > 1 − V1V >1 ‖)‖R−1‖ ≤ 3 ‖R−1‖ ≤ 6 ,\nwhere the first step follows by definition of α, β, the second step follows by triangle inequality, the third step follows by ‖AB‖ ≤ ‖A‖‖B‖, the fourth step follows by ‖(I − V 2V > 2 )‖ = 1, the fifth step follows by Eq. (32), the sixth step follows by Fact B.4, the seventh step follows by ‖V 1V > 1 −V1V >1 ‖ ≤ and ‖V 2V > 2 −V2V >2 ‖ ≤ , and the last step follows by ‖R−1‖ ≤ 2 (Claim E.7).\nFinally,\n‖UU> − V V >‖ ≤ ‖UU> − V V >‖+ ‖V V > − V V >‖\n= ‖(I − UU>)V ‖+ ‖V V > − V V >‖\n≤ ‖P̂2 − P2‖/σk(P2) + ‖V V > − V V >‖ ≤ ‖P̂2 − P2‖/σk(P2) + ‖V 1V > 1 − V1V >1 ‖+ ‖V 2V > 2 − V2V >2 ‖\n≤ ‖P̂2 − P2‖/σk(P2) + 2 ,\nwhere the first step follows by triangle inequality, the second step follows by Fact B.4, the third step follows by Eq. (33), the fourth step follows by triangle inequality, and the last step follows by ‖V 1V > 1 − V1V >1 ‖ ≤ and ‖V 2V > 2 − V2V >2 ‖ ≤ .\nTherefore we finish the proof.\nIt remains to prove Claim E.7.\nClaim E.7. σk(R) ≥ 1/2.\nProof. First, we can rewrite R>R in the follow way,\nR>R = V >2 (I − V1V >1 )V2 = I − V >2 V1V >1 V2\nSecond, we can upper bound ‖V >2 V1‖ by 1/4,\n‖V >2 V1‖ = ‖V2V >2 V1‖\n≤ ‖(V2V >2 − V 2V > 2 )V1‖+ ‖V 2V > 2 V1‖ ≤ ‖(V2V >2 − V 2V > 2 )V1‖+ ‖V > 2 (V1V > 1 − V 1V > 1 )‖+ ‖V > 2 V 1V > 1 ‖ = ‖(V2V >2 − V 2V > 2 )V1‖+ ‖V > 2 (V1V > 1 − V 1V > 1 )‖ ≤ ‖V2V >2 − V 2V > 2 ‖ · ‖V1‖+ ‖V > 2 ‖ · ‖V1V >1 − V 1V > 1 ‖ ≤ + ≤ 1/4,\nwhere the first step follows by V >2 V2 = I, the second step follows by triangle inequality, the third step follows by triangle inequality, the fourth step follows by ‖V >2 V 1V > 1 ‖ = 0, the fifth step follows by ‖AB‖ ≤ ‖A‖·‖B‖, and the last step follows by ‖V 1V > 1 −V1V >1 ‖ ≤ , ‖V1‖ = 1, ‖V 2V > 2 −V2V >2 ‖ ≤ and ‖V >2 ‖ = 1, and the last step follows by < 1/8. Thus, we can lower bound σ2k(R),\nσ2k(R) = λmin(R >R)\n= min ‖a‖=1\na>R>Ra\n= min ‖a‖=1\na>Ia− ‖V >2 V1a‖2\n= 1− max ‖a‖=1 ‖V >2 V1a‖2 = 1− ‖V >2 V1‖2 ≥ 3/4\nwhich implies σk(R) ≥ 1/2."
    }, {
      "heading" : "E.4 Error Bound for the Reduced Third-order Moment",
      "text" : ""
    }, {
      "heading" : "E.4.1 Error Bound for the Reduced Third-order Moment in Different Cases",
      "text" : "Lemma E.8. Let M3 be defined as in Definition 5.1. Let M̂3 be the empirical version of M3, i.e.,\nM̂3 = 1 |S| ∑\n(x,y)∈S\ny · (x⊗3 − x⊗̃I),\nwhere S denote a set of samples (where each sample is i.i.d. sampled from Distribution D defined in Eq. (1)). Assume M3 6= 0, i.e., m3,i 6= 0 for any i. Let V ∈ Rd×k be an orthogonal matrix satisfying ‖UU> − V V >‖ ≤ 1/4, where U is the orthogonal basis of span{w∗1, w∗2, · · · , w∗k}. Then for any 0 < < 1, t ≥ 1, if\n|S| ≥ max i∈[k] (‖w∗i ‖p+1/|m3,i|2 + 1) · −2 · k2 poly(log d, t)\nwith probability at least 1− 1/dt,\n‖M3(V, V, V )− M̂3(V, V, V )‖ ≤ k∑ i=1 |v∗im3,i|.\nProof. Since y = ∑k\ni=1 v ∗ i φ(w ∗> i x). We consider each component i ∈ [k]. We define function\nTi(x) : Rd → Rk×k×k such that,\nTi(x) = φ(w ∗> i x) · ((V >x)⊗3 − (V >x)⊗̃I).\nWe flatten tensor Ti(x) along the first dimension into matrix Bi(x) ∈ Rk×k 2 . Define g(z) = φ(z)− φ(0), then |g(z)| = | ∫ z\n0 φ ′(s)ds| ≤ L1/(p+ 1)|z|p+1, which follows Property 3.1.\n(I) Bounding ‖Bi(x)‖.\n‖Bi(x)‖ ≤ |φ(w∗>i x)| · (‖V >x‖3 + 3k‖V >x‖) . (|w∗>i x|p+1 + |φ(0)|) · (‖V >x‖3 + 3k‖V >xj‖)\nNote that V >x ∼ N (0, Ik). According to Fact B.1 and Fact B.2, we have for any constant t ≥ 1, with probability 1− 1/(ndt),\n‖Bi(x)‖ . (‖w∗i ‖p+1 + |φ(0)|)k3/2 poly(log d, t)\n(II) Bounding ‖Ex∼Dd [Bi(x)]‖. Note that Ex∼Dd [Bi(x)] = m3,i(V >w∗i )vec((V >w∗i )(V >w∗i )>)>. Therefore, ‖Ex∼Dd [Bi(x)]‖ = |m3,i|‖V >w∗i ‖3. Since ‖V V > − UU>‖ ≤ 1/4, ‖V V >w∗i − w∗i ‖ ≤ 1/4 and 3/4 ≤ ‖V >w∗i ‖ ≤ 5/4. So 1 4 |m3,i| ≤ ‖B‖ ≤ 2|m3,i|.\n(III) Bounding max(Ex∼Dd ‖Bi(x)Bi(x)>‖,Ex∼Dd ‖Bi(x)>Bi(x)‖).\n∥∥∥∥ Ex∼Dd[Bi(x)Bi(x)>] ∥∥∥∥ . ( Ex∼Dd[φ(w∗>i x)4] )1/2( E x∼Dd [‖V >x‖6] )1/2 . (‖w∗i ‖p+1 + |φ(0)|)2k3/2.∥∥∥∥ Ex∼Dd[Bi(x)>Bi(x)]\n∥∥∥∥ . ( E\nx∼Dd [φ(w∗>i x) 4]\n)1/2( E\nx∼Dd [‖V >x‖4] )1/2 max ‖A‖F=1 ( E x∼Dd [〈A, (V >x)(V >x)>〉4] )1/2 . (‖w∗i ‖p+1 + |φ(0)|)2k2.\n(IV) Bounding max‖a‖=‖b‖=1(Ex∼Dd [(a>Bi(x)b)2]).\nmax ‖a‖=‖b‖=1\n( E\nx∼Dd [(a>Bi(x)b) 2] )1/2 . ( E\nx∼Dd [(φ(w∗>i x)) 4] )1/4 max ‖a‖=1 ( E x∼Dd [(a>V >x)4] )1/2 max ‖A‖F=1 ( E x∼Dd [〈A, (V >x)(V >x)>〉4] )1/2 . (‖w∗i ‖p+1 + |φ(0)|)k\nDefine L = ‖w∗i ‖p+1 + |φ(0)|. Then we have for any 0 < < 1, if\n|S| & L 2k2 + |m3,i|2 + k3/2 poly(log d, t)|m3,i|\n2|m3,i|2 t log(k)\nwith probability at least 1− k−t,∥∥∥∥∥ Ex∼Dd[Bi(x)]− 1|S|∑ x∈S Bi(x) ∥∥∥∥∥ ≤ |m3,i|. We can set t = T log(d)/ log(k), then if\n|S| ≥ −2(1 + 1/|m3,i|2) poly(T, log d)\nwith probability at least 1− d−T ,∥∥∥∥∥ Ex∼Dd[Bi(x)]− 1|S|∑ x∈S Bi(x) ∥∥∥∥∥ ≤ |m3,i|. Also note that for any symmetric 3rd-order tensor R, the operator norm of R,\n‖R‖ = max ‖a‖=1 |R(a, a, a)| ≤ max ‖a‖=1 ‖R(a, I, I)‖F = ‖R(1)‖.\nLemma E.9. Let M4 be defined as in Definition 5.1. Let M̂4 be the empirical version of M4, i.e.,\nM̂4 = 1 |S| ∑\n(x,y)∈S\ny · ( x⊗4 − (x⊗ x)⊗̃I + I⊗̃I ) ,\nwhere S is a set of samples (where each sample is i.i.d. sampled from Distribution D defined in Eq. (1)). Assume M4 6= 0, i.e., m4,i 6= 0 for any i. Let α be a fixed unit vector. Let V ∈ Rd×k be an orthogonal matrix satisfying ‖UU> − V V >‖ ≤ 1/4, where U is the orthogonal basis of span{w∗1, w∗2, · · · , w∗k}. Then for any 0 < < 1, t ≥ 1, if\n|S| ≥ max i∈[k] (1 + ‖w∗i ‖p+1/|m4,i(α>w∗i )|2) · −2 · k2 poly(log d, t)\nwith probability at least 1− d−t,\n‖M4(V, V, V, α)− M̂4(V, V, V, α)‖ ≤ k∑ i=1 |v∗im4,i(α>w∗i )|.\nProof. Recall that for each (x, y) ∈ S, we have y = ∑k\ni=1 v ∗ i φ(w ∗> i x). We consider each component\ni ∈ [k]. We define function r(x) : Rd → Rk such that\nr(x) = V >x.\nDefine function Ti(x) : Rd → Rk×k×k such that\nTi(x) = φ(w ∗> i x)\n( x>α · r(x)⊗ r(x)⊗ r(x)− (V >α)⊗̃(r(x)⊗ r(x))− α>x · r(x)⊗̃I + (V >α)⊗̃I ) .\nWe flatten Ti(x) : Rd → Rk×k×k along the first dimension to obtain functionBi(x) : Rd → Rk×k 2 . Define g(z) = φ(z)−φ(0), then |g(z)| = | ∫ z\n0 φ ′(s)ds| ≤ L1/(p+1)|z|p+1, which follows Property 3.1.\n(I) Bounding ‖Bi(x)‖.\n‖Bi(x)‖ .(|w∗>i x|p+1 + |φ(0)|) · (|(x>α)|‖V >x‖3 + 3‖V >α‖‖V >x‖2\n+ 3|(x>α)|‖V >xj‖ √ k + 3‖V >α‖ √ k)\nNote that V >x ∼ N (0, Ik). According to Fact B.1 and Fact B.2, we have for any constant t ≥ 1, with probability 1− 1/(ndt),\n‖Bi(x)‖ . (‖w∗i ‖p+1 + |φ(0)|)k3/2 poly(log d, t)\n(II) Bounding ‖Ex∼Dd [Bi(x)]‖. Note that Ex∼Dd [Bi(x)] = m4,i(α>w∗i )(V >w∗i )vec((V >w∗i )(V >w∗i )>)>. Therefore,∥∥∥∥ Ex∼Dd[Bi(x)]\n∥∥∥∥ = |m4,i(α>w∗i )|‖V >w∗i ‖3. Since ‖V V >−UU>‖ ≤ 1/4, ‖V V >w∗i −w∗i ‖ ≤ 1/4 and 3/4 ≤ ‖V >w∗i ‖ ≤ 5/4. So 14 |m4,i(α\n>w∗i )| ≤ ‖Ex∼Dd [Bi(x)]‖ ≤ 2|m4,i(α>w∗i )|.\n(III) Bounding max(Ex∼Dd [Bi(x)Bi(x)>],Ex∼Dd [Bi(x)>Bi(x)]).∥∥∥∥ Ex∼Dd[Bi(x)Bi(x)>] ∥∥∥∥ . ( Ex∼Dd [ φ(w∗>i x) 4 ])1/2( E x∼Dd [ (α>x)4 ])1/2( E x∼Dd [ ‖V >x‖6 ])1/2 . (‖w∗i ‖p+1 + |φ(0)|)2k3/2.\n∥∥∥∥ Ex∼Dd[Bi(x)>Bi(x)] ∥∥∥∥\n. ( E\nx∼Dd [φ(w∗>i x) 4]\n)1/2( E\nx∼Dd [(α>x)4]\n)1/2( E\nx∼Dd [‖V >x‖4] )1/2 · (\nmax ‖A‖F=1 E x∼Dd\n[ 〈A, (V >x)(V >x)>〉4 ])1/2 . (‖w∗i ‖p+1 + |φ(0)|)2k2.\n(IV) Bounding max‖a‖=‖b‖=1(Ex∼Dd [ (a>Bi(x)b) 2 ] )1/2.\nmax ‖a‖=‖b‖=1\n( E\nx∼Dd\n[ (a>Bi(x)b) 2 ])1/2\n. ( E\nx∼Dd [φ4(w∗>i x)]\n)1/4( E\nx∼Dd\n[ (α>x)4 ])1/4 max ‖a‖=1 ( E x∼Dd [ (a>V >x)4 ])1/2 · max ‖A‖F=1 ( E x∼Dd [ 〈A, (V >x)(V >x)>〉4\n])1/2 . (‖w∗i ‖p+1 + |φ(0)|)k.\nDefine L = ‖w∗i ‖p+1 + |φ(0)|. Then we have for any 0 < < 1, if\n|S| ≥ L 2k2 + |m4,i(α>w∗i )|2 + k3/2 poly(t, log d)|m4,i(α>w∗i )|\n2(m4,i(α>w∗i )) 2\n· t log k\nwith probability at least 1− k−t,∥∥∥∥∥ Ex∼Dd[Bi(x)]− 1|S| n∑ x∈S Bi(x) ∥∥∥∥∥ ≤ |m4,i(α>w∗i )|. (34) We can set t = T log(d)/ log(k), then if\n|S| ≥ (L+ |m4,i(α >w∗i )|)2k2 poly(T, log d)\n2|m4,i(α>w∗i )|2 · T log2 d\nwith probability at least 1−d−T , Eq. (34) holds. Also note that for any symmetric 3rd-order tensor R, the operator norm of R,\n‖R‖ = max ‖a‖=1 |R(a, a, a)| ≤ max ‖a‖=1 ‖R(a, I, I)‖F = ‖R(1)‖."
    }, {
      "heading" : "E.4.2 Final Error Bound for the Reduced Third-order Moment",
      "text" : "Lemma E.10 shows R̂3 can approximate R3 to some small precision with poly(k) samples.\nLemma E.10 (Estimation of the reduced third order moment). Let U ∈ Rd×k denote the orthogonal column span of W ∗. Let α be a fixed unit vector and V ∈ Rd×k denote an orthogonal matrix satisfying ‖V V >−UU>‖ ≤ 1/4. Define R3 := P3(V, V, V ), where P3 is defined as in Definition 5.4 using α. Let R̂3 be the empirical version of R3 using dataset S, where each sample of S is i.i.d. sampled from distribution D(defined in (1)). Assume the activation function satisfies Property 3.1 and Assumption 5.3. Then for any 0 < < 1 and t ≥ 1, define j3 = min{j ≥ 3|Mj 6= 0} and m0 = mini∈[k]{(m (j3) i (α >w∗i ) j3−3)2}, if\n|S| ≥ σ2p+21 · k 2 · poly(log d, t)/( 2m0)\nthen we have ,\n‖R3 − R̂3‖ ≤ k∑ i=1 |v∗imj3,i(w∗>i α)j3−3|,\nholds with probability at least 1− d−Ω(t).\nProof. The main idea is to use matrix Bernstein bound after matricizing the third-order tensor. Similar to the proof of Lemma E.5, we consider each node component individually and then sum up the errors and apply union bound.\nWe have shown the bound for j3 = 3, 4 in Lemma E.8 and Lemma E.9 respectively. To summarize, for any 0 < < 1 we have if\n|S| ≥ max i∈[k]\n( 1 + ‖w∗i ‖p+1/|mj3,i(w∗>i α)(j3−3)|2 ) · −2 · k2 poly(log d, t)\nwith probability at least 1− d−t,\n‖R3 − R̂3‖ ≤ k∑ i=1 |v∗imj3,i(w∗>i α)j3−3|."
    }, {
      "heading" : "E.5 Error Bound for the Magnitude and Sign of the Weight Vectors",
      "text" : "The lemmata in this section together with Lemma E.5 provide guarantees for Algorithm 4. In particular, Lemma E.12 shows with linear sample complexity in d, we can approximate the 1storder moment to some precision. And Lemma E.13 and Lemma E.14 provide the error bounds of linear systems Eq. (28) under some perturbations."
    }, {
      "heading" : "E.5.1 Robustness for Solving Linear Systems",
      "text" : "Lemma E.11 (Robustness of linear system). Given two matrices A, Ã ∈ Rd×k, and two vectors b, b̃ ∈ Rd. Let z∗ = argminz∈Rk ‖Az−b‖ and ẑ = argminz∈Rk ‖(A+Ã)z−(b+ b̃)‖. If ‖Ã‖ ≤ 1 4κσk(A) and ‖b̃‖ ≤ 14‖b‖, then, we have\n‖z∗ − ẑ‖ .(σ−4k (A)σ 2 1(A) + σ −2 k (A))‖b‖‖Ã‖+ σ −2 k (A)σ1(A)‖b̃‖.\nProof. By definition of z and ẑ, we can rewrite z and ẑ,\nz = A†b = (A>A)−1A>b\nẑ = (A+ Ã)†(b+ b̃) = ((A+ Ã)>(A+ Ã))−1(A+ Ã)>(b+ b̃).\nAs ‖Ã‖ ≤ 14κσk(A), we have ‖Ã >A + A>Ã‖‖(A>A)−1‖ ≤ 1/4. Together with ‖b̃‖ ≤ 14‖b‖, we can ignore the high-order errors. So we have\n‖ẑ − z∗‖\n. ‖(A>A)−1(Ã>b+A>b̃) + (A>A)−1(A>Ã+ Ã>A)(A>A)−1A>b‖\n. ‖(A>A)−1‖(‖Ã‖‖b‖+ ‖A‖‖b̃‖) + ‖(A>A)−2‖ · ‖A‖‖Ã‖‖A‖‖b‖\n. σ−2k (A)(‖Ã‖‖b‖+ σ1(A)‖b̃‖) + σ −4 k (A) · σ 2 1(A)‖Ã‖‖b‖."
    }, {
      "heading" : "E.5.2 Error Bound for the First-order Moment",
      "text" : "Lemma E.12 (Error bound for the first-order moment). Let Q1 be defined as in Eq. (25) and Q̂1 be the empirical version of Q1 using dataset S, where each sample of S is i.i.d. sampled from distribution D(defined in (1)). Assume the activation function satisfies Property 3.1 and Assumption 5.3. Then for any 0 < < 1 and t ≥ 1, define j1 = min{j ≥ 1|Mj 6= 0} and m0 = mini∈[k](mj1,i(w ∗> i α) j1−1)2 if\n|S| ≥ σ2p+21 d poly(t, log d)/( 2m0)\nwe have with probability at least 1− d−Ω(t),\n‖Q1 − Q̂1‖ ≤ k∑ i=1 |v∗imj1,i(w∗>i α)j1−1|.\nProof. We consider the case when l1 = 3, i.e.,\nQ1 = M3(I, α, α) = k∑ i=1 v∗im3,i(α >w∗i ) 3w∗i .\nAnd other cases are similar. Since y = ∑k i=1 v ∗ i φ(w ∗> i x). We consider each component i ∈ [k].\nDefine function Bi(x) : Rd → Rd such that\nBi(x) = [φ(w ∗> i x) · (x⊗3 − x⊗̃I)](I, α, α) = φ(w∗>i x) · ((x>α)2x− 2(x>α)α− x).\nDefine g(z) = φ(z) − φ(0), then |g(z)| = | ∫ z\n0 φ ′(s)ds| ≤ L1/(p + 1)|z|p+1, which follows Prop-\nerty 3.1. (I) Bounding ‖Bi(x)‖.\n‖Bi(x)‖ ≤ |φ(w∗>i x)| · ‖((x>α)2x− 2α>xα− x)‖ ≤ (|w∗>i x|p+1 + |φ(0)|)(((x>α)2 + 1)‖x‖+ 2|α>x|)\nAccording to Fact B.1 and Fact B.2, we have for any constant t ≥ 1, with probability 1− 1/(ndt),\n‖Bi(x)‖ . (‖w∗i ‖p+1 + |φ(0)|) √ dpoly(log d, t)\n(II) Bounding ‖Ex∼Dd [Bi(x)]‖. Note that Ex∼Dd [Bi(x)] = m3,i(w∗>i α)2w∗i . Therefore, ‖Ex∼Dd [Bi(x)]‖ = |m3,i(w∗>i α)2|. (III) Bounding max(Ex∼Dd ‖Bi(x)Bi(x)>‖,Ex∼Dd ‖Bi(x)>Bi(x)‖).\n∥∥∥∥ Ex∼Dd [ Bi(x) >Bi(x) ]∥∥∥∥ . ( Ex∼Dd [ φ(w∗>i x) 4 ])1/2( E x∼Dd [ (x>α)8 ])1/2( E x∼Dd [ ‖x‖4 ])1/2 . (‖w∗i ‖p+1 + |φ(0)|)2d.\n∥∥∥∥ Ex∼Dd [ Bi(x)Bi(x) > ]∥∥∥∥ . ( Ex∼Dd [ φ(w∗>i x) 4 ])1/2( E x∼Dd [ (x>α)8 ])1/2( max ‖a‖=1 E x∼Dd [ (x>a)4 ])1/2 . (‖w∗i ‖p+1 + |φ(0)|)2.\n(IV) Bounding max‖a‖=‖b‖=1(Ex∼Dd [(a>Bi(x)b)2]).\nmax ‖a‖=1\n( E\nx∼Dd\n[ (a>Bi(x)a) 2 ])1/2 . ( E\nx∼Dd\n[ φ4(w∗>i x) ])1/4 . ‖w∗i ‖p+1 + |φ(0)|.\nDefine L = ‖w∗i ‖p+1 + |φ(0)|. Then we have for any 0 < < 1, if\n|S| & L 2d+ |m3,i(w∗>i α)2|2 + L|m3,i(w∗>i α)2|\n√ dpoly(log d, t)\n2|m3,i(w∗>i α)2|2 · t log d\nwith probability at least 1− 1/dt,∥∥∥∥∥ Ex∼Dd[Bi(x)]− 1|S| n∑\nx∼S Bi(x) ∥∥∥∥∥ ≤ |m3,i(w∗>i α)2|. Summing up all k components, we obtain if\n|S| ≥ max i∈[k]\n{ (‖w∗i ‖p+1 + |φ(0)|+ |m3,i(w∗>i α)2|)2\n|m3,i(w∗>i α)2|2\n} · −2dpoly(log d, t)\nwith probability at least 1− 1/dt,\n‖M3(I, α, α)− M̂3(I, α, α)‖ ≤ k∑ i=1 |v∗im3,i(w∗>i α)2|.\nOther cases (j1 = 1, 2, 4) are similar, so we complete the proof."
    }, {
      "heading" : "E.5.3 Linear System for the First-order Moment",
      "text" : "The following lemma provides estimation error bound for the first linear system in Eq. (28).\nLemma E.13 (Solution of linear system for the first order moment). Let U ∈ Rd×k be the orthogonal column span of W ∗. Let V ∈ Rd×k denote an orthogonal matrix satisfying that ‖V V > − UU>‖ ≤ δ̂2 . 1/(κ2 √ k). For each i ∈ [k], let ûi denote the vector satisfying ‖ûi − V >w∗i ‖ ≤ δ̂3 . 1/(κ2 √ k). Let Q1 be defined as in Eq. (25) and Q̂1 be the empirical version of Q1 such that ‖Q1 − Q̂1‖ ≤ δ̂4‖Q1‖ ≤ 14‖Q1‖. Let z ∗ ∈ Rk and ẑ ∈ Rk be defined as in Eq. (27) and Eq. (28). Then\n|ẑi − z∗i | ≤ (κ4k3/2(δ̂2 + δ̂3) + κ2k1/2δ̂4)‖z∗‖1.\nProof. Let A ∈ Rk×k denote the matrix where the i-th column is siw∗i . Let Ã ∈ Rk×k denote the matrix where the i-th column is V ûi. Let b ∈ Rk denote the vector Q1, let b̃ denote the vector Q̂1 −Q1. Then we have\n‖A‖ ≤ √ k.\nUsing Fact B.3, we can lower bound σk(A),\nσk(A) ≥ 1/κ.\nWe can upper bound ‖Ã‖ in the following way,\n‖Ã‖ ≤ √ kmax i∈[k] {‖V ûi − siw∗i ‖}\n≤ √ kmax i∈[k] {‖V ûi − siV V >w∗i + siV V >w∗i − siUU>w∗i ‖} ≤ √ k(δ̂3 + δ̂2).\nWe can upper bound ‖b‖ and ‖b̃‖,\n‖b‖ = ‖Q1‖ ≤ k k∑ i=1 |z∗i |, and ‖b̃‖ ≤ δ̂4‖Q1‖.\nTo apply Lemma E.11, we need δ̂4 ≤ 1/4 and δ̂2 . 1/( √ kκ2), δ̂3 . 1/( √ kκ2). So we have,\n‖ẑi − z∗i ‖ ≤ (κ4k3/2(δ̂2 + δ̂3) + κ2k1/2δ̂4)‖Q1‖\n≤ (κ4k3/2(δ̂2 + δ̂3) + κ2k1/2δ̂4) k∑ i=1 |z∗i |."
    }, {
      "heading" : "E.5.4 Linear System for the Second-order Moment",
      "text" : "The following lemma provides estimation error bound for the second linear system in Eq. (28).\nLemma E.14 (Solution of linear system for the second order moment). Let U ∈ Rd×k be the orthogonal column span of W ∗ denote an orthogonal matrix satisfying that ‖V V > − UU>‖ ≤ δ̂2 . 1/(κ √ k). For each i ∈ [k], let ûi denote the vector satisfying ‖ûi−V >w∗i ‖ ≤ δ̂3 . 1/( √ kκ3). Let Q2 be defined as in Eq. (26) and Q̂2 be the estimation of Q2 such that ‖Q2−Q̂2‖F ≤ δ̂4‖Q2‖F ≤ 14‖Q2‖F . Let r∗ ∈ Rk and r̂ ∈ Rk be defined as in Eq. (27) and Eq. (28). Then\n|r̂i − r∗i | ≤ (k3κ8δ̂3 + κ2k2δ̂4)‖r∗‖.\nProof. For each i ∈ [k], let ui = V >w∗i . Let A ∈ Rk 2×k denote the matrix where the i-th column is vec(uiu>i ). Let Ã ∈ Rk 2×k denote the matrix where the i-th column is vec(uiu>i − ûiû>i ). Let b ∈ Rk2 denote the vector vex(Q2), let b̃ ∈ Rk 2 denote the vector vec(Q2 − Q̂2).\nLet ◦ be the element-wise matrix product (a.k.a. Hadamard product), W = [w∗1 w∗2 · · · w∗k] and U = [u1 u2 · · · uk] = V >W . We can upper bound ‖A‖ and ‖Ã‖ as follows,\n‖A‖ = max ‖x‖=1 ∥∥∥∥∥ k∑ i=1 xivec(uiu>i ) ∥∥∥∥∥ = max ‖x‖=1 ‖Udiag(x)U>‖F\n≤ ‖U‖2\n≤ σ21(V >W ),\nand\n‖Ã‖ = √ kmax i∈[k] ‖Ãi‖\n≤ √ kmax i∈[k] ‖uiu>i − ûiû>i ‖F ≤ √ kmax i∈[k] 2‖ui − ûi‖2 ≤ 2 √ kδ̂3.\nWe can lower bound σk(A),\nσk(A) = √ σk(A>A)\n= √ σk((U>U) ◦ (U>U))\n= min ‖x‖=1\n√ x>((U>U) ◦ (U>U))x\n= min ‖x‖=1\n‖(U>U)1/2diag(x)(U>U)1/2‖F\n≥ σ2k(V >W )\nwhere fourth step follows Schur product theorem, the last step follows by the fact that ‖CB‖F ≥ σmin(C)‖B‖F and ◦ is the element-wise multiplication of two matrices.\nWe can upper bound ‖b‖ and ‖b̃‖,\n‖b‖ ≤‖Q2‖F ≤ ‖r∗‖,\n‖b̃‖ =‖Q2 − Q̂2‖F ≤ δ̂4‖r∗‖.\nSince ‖V V >W −W‖ ≤ √ kδ̂2, we have for any x ∈ Rk,\n‖V V >Wx‖ ≥ ‖Wx‖ − ‖(V V >W −W )x‖\n≥ σk(W )‖x‖ − δ̂2 √ k‖x‖\nNote that according to Fact B.3, σk(W ) ≥ 1/κ. Therefore, if δ̂2 ≤ 1/(2κ √ k), we will have\nσk(V >W ) ≥ 1/(2κ). Similarly, we have σ1(V >W ) ≤ ‖V ‖‖W‖ ≤ √ k. Then applying Lemma E.11 and setting δ̂2 . 1√kκ3 , we complete the proof."
    }, {
      "heading" : "F Acknowledgments",
      "text" : "The authors would like to thank Surbhi Goel, Adam Klivans, Qi Lei, Eric Price, David P. Woodruff, Peilin Zhong, Hongyang Zhang and Jiong Zhang for useful discussions."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2017,
    "abstractText" : "<lb>In this paper, we consider regression problems with one-hidden-layer neural networks (1NNs).<lb>We distill some properties of activation functions that lead to local strong convexity in the<lb>neighborhood of the ground-truth parameters for the 1NN squared-loss objective. Most popular<lb>nonlinear activation functions satisfy the distilled properties, including rectified linear units<lb>(ReLUs), leaky ReLUs, squared ReLUs and sigmoids. For activation functions that are also<lb>smooth, we show local linear convergence guarantees of gradient descent under a resampling rule.<lb>For homogeneous activations, we show tensor methods are able to initialize the parameters to<lb>fall into the local strong convexity region. As a result, tensor initialization followed by gradient<lb>descent is guaranteed to recover the ground truth with sample complexity d · log(1/ ) ·poly(k, λ)<lb>and computational complexity n · d · poly(k, λ) for smooth homogeneous activations with high<lb>probability, where d is the dimension of the input, k (k ≤ d) is the number of hidden nodes, λ<lb>is a conditioning property of the ground-truth parameter matrix between the input layer and<lb>the hidden layer, is the targeted precision and n is the number of samples. To the best of our<lb>knowledge, this is the first work that provides recovery guarantees for 1NNs with both sample<lb>complexity and computational complexity linear in the input dimension and logarithmic in the<lb>precision. ∗A preliminary version of this paper appears in Proceedings of the Thirty-fourth International Conference on<lb>Machine Learning (ICML 2017).<lb>†Supported in part by NSF grants CCF-1320746, IIS-1546452 and CCF-1564000, and part of the work was done<lb>while interning in Microsoft research, India.<lb>‡Supported in part by UTCS TAship (CS361 Spring 17 Introduction to Computer Security).<lb>Supported in part by Australian Research Council through an Australian Laureate Fellowship (FL110100281) and<lb>through the Australian Research Council Centre of Excellence for Mathematical and Statistical Frontiers (ACEMS),<lb>and NSF grants IIS-1619362.<lb>¶Supported in part by NSF grants CCF-1320746, IIS-1546452 and CCF-1564000.<lb>ar<lb>X<lb>iv<lb>:1<lb>70<lb>6.<lb>03<lb>17<lb>5v<lb>1<lb>[<lb>cs<lb>.L<lb>G<lb>]<lb>1<lb>0<lb>Ju<lb>n<lb>20<lb>17",
    "creator" : "LaTeX with hyperref package"
  }
}