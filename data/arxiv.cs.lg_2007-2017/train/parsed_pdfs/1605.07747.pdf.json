{
  "name" : "1605.07747.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "NESTT: A Nonconvex Primal-Dual Splitting Method for Distributed and Stochastic Optimization",
    "authors" : [ "Davood Hajinezhad", "Mingyi Hong", "Tuo Zhao", "Zhaoran Wang" ],
    "emails" : [ "dhaji@iastate.edu", "mingyi@iastate.edu", "tzhao5@jhu.edu", "zhaoran@princeton.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "∑N i=1 √ Li/N)\n2/ ) gradient evaluations, which can be up to O(N) times better than the (proximal) gradient descent methods. It also achieves Q-linear convergence rate for nonconvex `1 penalized quadratic problems with polyhedral constraints. Further, we reveal a fundamental connection between primal-dual based methods and a few primal only methods such as IAG/SAG/SAGA.\n1 Introduction\nConsider the following nonconvex and nonsmooth constrained optimization problem\nmin z∈Z\nf(z) := 1\nN N∑ i=1 gi(z) + g0(z) + p(z), (1)\nwhere Z ⊆ Rd; for each i ∈ {0, · · · , N}, gi : Rd → R is a smooth possibly nonconvex function which has Li-Lipschitz continuous gradient; p(z) : Rd → R is a lower semi-continuous convex but possibly nonsmooth function. Define g(z) := 1N ∑N i=1 gi(z) for notational simplicity.\nProblem (1) is quite general. It arises frequently in applications such as machine learning and signal processing; see a recent survey [8]. In particular, each smooth functions {gi}Ni=1 can represent: 1) a minibatch of loss functions modeling data fidelity, such as the `2 loss, the logistic loss, etc; 2) nonconvex activation functions for neural networks, such as the logit or the tanh functions; 3) nonconvex utility functions used in signal processing, machine learning, and resource allocation, see [5], and [12]. The smooth function g0 can represent smooth nonconvex regularizers such as the non-quadratic penalties [2], or the smooth part of the SCAD or MCP regularizers (which is a concave function) [30]. The convex function p can take the following form: 1) nonsmooth convex regularizers such as `1 and `2 functions; 2) an indicator function for convex and closed feasible set Z, denoted as ιZ(·); 3) convex functions without global Lipschitz continuous gradient, such as p(z) = z4 or p(z) = 1/z + ιz≥0(z).\n∗Department of Industrial & Manufacturing Systems Engineering and Department of Electrical & Computer Engineering, Iowa State University, Emails: {dhaji,mingyi}@iastate.edu †Department of Computer Science, Johns Hopkins University, Email: tzhao5@jhu.edu ‡Department of Operations Research and Financial Engineering, Princeton University, Email: zhaoran@princeton.edu\nar X\niv :1\n60 5.\n07 74\n7v 2\n[ m\nat h.\nO C\n] 7\nN ov\nIn this work we solve (1) in a stochastic and distributed manner. We consider the setting in which N distributed agents each having the knowledge of one smooth function {gi}Ni=1, and they are connected to a cluster center which handles g0 and p. At any given time, a randomly selected agent is activated and performs computation to optimize its local objective. Such distributed computation model has been popular in large-scale machine learning and signal processing [7]. Such model is also closely related to the (centralized) stochastic finite-sum optimization problem [17, 10, 15, 25, 1, 26], in which each time the iterate is updated based on the gradient information of a random component function. One of the key differences between these two problem types is that in the distributed setting there can be disagreement between local copies of the optimization variable z, while in the centralized setting only one copy of z is maintained.\nOur Contributions. We propose a class of NonconvEx primal-dual SpliTTing (NESTT) algorithms for problem (1). We split z ∈ Rd into local copies of xi ∈ Rd, while enforcing the equality constraints xi = z for all i. That is, we consider the following reformulation of (1)\nmin x,z∈Rd\n`(x, z) := 1\nN N∑ i=1 gi(xi) + g0(z) + h(z), s.t. xi = z, i = 1, · · · , N, (2)\nwhere h(z) := ιZ(z) + p(z), x := [x1; · · · ;xN ]. Our algorithm uses the Lagrangian relaxation of the equality constraints, and at each iteration a (possibly non-uniformly) randomly selected primal variable is optimized, followed by an approximate dual ascent step. Note that such splitting scheme has been popular in the convex setting [7], but not so when the problem becomes nonconvex.\nThe NESTT is one of the first stochastic algorithms for distributed nonconvex nonsmooth optimization, with provable and nontrivial convergence rates. Our main contribution is given below. First, in terms of some primal and dual optimality gaps, NESTT converges sublinearly to a point belongs to stationary solution set of (2). Second, NESTT converges Q-linearly for certain nonconvex `1 penalized quadratic problems. To the best of our knowledge, this is the first time that linear convergence is established for stochastic and distributed optimization of such type of problems. Third, we show that a gradient-based NESTT with non-uniform sampling achieves an -stationary solution of (1) using O(( ∑N i=1 √ Li/N)\n2/ ) gradient evaluations. Compared with the classical gradient descent, which in the worst case requires O( ∑N\ni=1 Li/ ) gradient evaluation to achieve -stationarity [23], our obtained rate can be up to O(N) times better in the case where the Li’s are not equal.\nOur work also reveals a fundamental connection between primal-dual based algorithms and the primal only average-gradient based algorithm such as SAGA/SAG/IAG [10, 25, 26, 6]. With the key observation that the dual variables in NESTT serve as the “memory” of the past gradients, one can specialize NESTT to SAGA/SAG/IAG. Therefore, NESTT naturally generalizes these algorithms to the nonconvex nonsmooth setting. It is our hope that by bridging the primal-dual splitting algorithms and primal-only algorithms (in both the convex and nonconvex setting), there can be significant further research developments benefiting both algorithm classes. Related Work. Many stochastic algorithms have been designed for (2) when it is convex. In these algorithms the component functions gi’s are randomly sampled and optimized. Popular algorithms include the SAG/SAGA [10, 26], the SDCA [27], the SVRG [15], the RPDG [17] and so on. When the problem becomes nonconvex, the well-known incremental based algorithm can be used [28, 3], but these methods generally lack convergence rate guarantees. The SGD based method has been studied in [11], with O(1/ 2) convergence rate. Recent works [1] and [25] develop algorithms based on SVRG and SAGA for a special case of (1) where the entire problem is smooth and unconstrained. To the best of our knowledge there\nhas been no stochastic algorithms with provable, and non-trivial, convergence rate guarantees for solving problem (1). On the other hand, distributed stochastic algorithms for solving problem (1) in the nonconvex setting has been proposed in [14], [13], in which each time a randomly picked subset of agents update their local variables. However there has been no convergence rate analysis for such distributed stochastic scheme. There has been some recent distributed algorithms designed for (1) [19], but again without global convergence rate guarantee. Preliminaries. The augmented Lagrangian function for problem (1) is given by:\nL (x, z;λ) = N∑ i=1 ( 1 N gi(xi) + 〈λi, xi − z〉+ ηi 2 ‖xi − z‖2 ) + g0(z) + h(z), (3)\nwhere λ := {λi}Ni=1 is the set of dual variables, and η := {ηi > 0}Ni=1 are penalty parameters. We make the following assumptions about problem (1) and the function (3).\nA-(a) The function f(z) is bounded from below over Z ∩ int(dom f): f := minz∈Z f(z) > −∞. p(z) is a convex lower semi-continuous function; Z is a closed convex set.\nA-(b) The gi’s and g have Lipschitz continuous gradients, i.e.,\n‖∇g(y)−∇g(z)‖ ≤ L‖y − z‖, and ‖∇gi(y)−∇gi(z)‖ ≤ Li‖y − z‖, ∀ y, z\nClearly L ≤ 1/N ∑N\ni=1 Li, and the equality can be achieved in the worst case. For simplicity of analysis we will further assume that L0 ≤ 1N ∑N i=1 Li.\nA-(c) Each ηi in (3) satisfies ηi > Li/N ; if g0 is nonconvex, then ∑N i=1 ηi > 3L0.\nAssumption A-(c) implies that L (x, z;λ) is strongly convex w.r.t. each xi and z, with modulus γi := ηi − Li/N and γz = ∑N i=1 ηi − L0, respectively [31, Theorem 2.1].\nWe then define the prox-gradient (pGRAD) for (1), which will serve as a measure of stationarity. It can be checked that the pGRAD vanishes at the set of stationary solutions of (1) [24].\nDefinition 1.1. The proximal gradient of problem (1) is given by (for any γ > 0)\n∇̃fγ(z) := γ ( z − proxγp+ιZ [z − 1/γ∇(g(z) + g0(z))] ) , with proxγp+ιZ [u] := argmin\nu∈Z p(u) +\nγ 2 ‖z − u‖2.\n2 The NESTT-G Algorithm\nAlgorithm Description. We present a primal-dual splitting scheme for the reformulated problem (2). The algorithm is referred to as the NESTT with Gradient step (NESTT-G) since each agent only requires to know the gradient of each component function. To proceed, let us define the following function (for some constants {αi > 0}Ni=1):\nVi(xi, z;λi) = 1\nN gi(z) +\n1\nN 〈∇gi(z), xi − z〉+ 〈λi, xi − z〉+ αiηi 2 ‖xi − z‖2.\nNote that Vi(·) is related to L(·) in the following way: it is a quadratic approximation (approximated at the point z) of L(x, y;λ) w.r.t. xi. The parameters α := {αi}Ni=1 give some freedom to the algorithm\nAlgorithm 1 NESTT-G Algorithm\n1: for r = 1 to R do 2: Pick ir ∈ {1, 2, · · · , N} with probability pir and update (x, λ)\nxr+1ir = arg minxir Vir ( xir , z r, λrir ) ; (4) λr+1ir = λ r ir + αirηir ( xr+1ir − z r ) ; (5) λr+1j = λ r j , x r+1 j = z\nr, ∀ j 6= ir; (6) Update z: zr+1 = arg min\nz∈Z L({xr+1i }, z;λ r). (7)\n3: end for 4: Output: (zm, xm, λm) where m randomly picked from {1, 2, · · · , R}.\ndesign, and they are critical in improving convergence rates as well as in establishing connection between NESTT-G with a few primal only stochastic optimization schemes.\nThe algorithm proceeds as follows. Before each iteration begins the cluster center broadcasts z to everyone. At iteration r + 1 a randomly selected agent ir ∈ {1, 2, · · ·N} is picked, who minimizes Vir(·) w.r.t. its local variable xir , followed by a dual ascent step for λir . The rest of the agents update their local variables by simply setting them to z. The cluster center then minimizes L(x, z;λ) with respect to z. See Algorithm 1 for details. We remark that NESTT-G is related to the popular ADMM method for convex optimization [7]. However our particular update schedule (randomly picking (xi, λi) plus deterministic updating z), combined with the special x-step (minimizing an approximation of L(·) evaluated at a different block variable z) is not known before. These features are critical in our following rate analysis.\n2.1 Convergence Analysis.\nTo proceed, let us define r(j) as the last iteration in which the jth block is picked before iteration r + 1. i.e.r(j) := max{t | t < r+ 1, j = i(t)}. Define yrj := zr(j) if j 6= ir, and yrir = z\nr. Define the filtration Fr as the σ-field generated by {i(t)}r−1t=1 .\nA few important observations are in order. Combining the (x, z) updates (4) – (7), we have\nxr+1q = z r − 1\nαqηq (λrq +\n1\nN ∇gq(zr)),\n1\nN ∇gq(zr) + λrq + αqηq(xr+1q − zr) = 0, with q = ir (8a)\nλr+1ir = − 1\nN ∇gir (zr), λr+1j = −\n1\nN ∇gj(zr(j)), ∀ j 6= ir, ⇒ λr+1i = −\n1\nN ∇gi(yri ), ∀ i (8b)\nxr+1j (6) = zr (8b) = zr − 1\nαjηj (λrj +\n1\nN ∇gj(zr(j))), ∀ j 6= ir. (8c)\nThe key here is that the dual variables serve as the “memory” for the past gradients of gi’s. To proceed,\nwe first construct a potential function using an upper bound of L(x, y;λ). Note that\n1\nN gj(x\nr+1 j ) + 〈λ r j , x r+1 j − z r〉+ ηj 2 ‖xr+1j − z r‖2 = 1 N gj(z r), ∀ j 6= ir (9)\n1\nN gir (x\nr+1 ir ) + 〈λrir , x r+1 ir − zr〉+ ηi\n2 ‖xr+1ir − z r‖2\n(i) ≤ 1 N gir (z r) + ηir + Lir/N 2 ‖xr+1ir − z r‖2 (ii) = 1\nN gir (z\nr) + ηir + Lir/N\n2(αirηir ) 2 ‖1/N(∇gir (yr−1ir )−∇gir (z r))‖2 (10)\nwhere (i) uses (8b) and applies the descent lemma on the function 1/Ngi(·); in (ii) we have used (5) and (8b). Since each i is picked with probability pi, we have\nEir [L(xr+1, zr;λr) | Fr]\n≤ N∑ i=1 1 N gi(z r) + N∑ i=1 pi(ηi + Li/N) 2(αiηi)2 ‖1/N(∇gi(yr−1i )−∇gi(z r))‖2 + g0(zr) + h(zr)\n≤ N∑ i=1 1 N gi(z r) + N∑ i=1 3piηi (αiηi)2 ‖1/N(∇gi(yr−1i )−∇gi(z r))‖2 + g0(zr) + h(zr) := Qr,\nwhere in the last inequality we have used Assumption [A-(c)]. In the following, we will use EFr [Qr] as the potential function, and show that it decreases at each iteration.\nLemma 2.1. Suppose Assumption A holds, and pick\nαi = pi = βηi, where β := 1∑N i=1 ηi , and ηi ≥ 9Li Npi , i = 1, · · ·N. (11)\nThen the following descent estimate holds true for NESTT-G E[Qr −Qr−1|Fr−1] ≤ − ∑N\ni=1 ηi 8 Ezr‖zr − zr−1‖2 − N∑ i=1 1 2ηi ‖ 1 N (∇gi(zr−1)−∇gi(yr−2i ))‖ 2. (12)\nSublinear Convergence. Define the optimality gap as the following: E[Gr] := E [ ‖∇̃1/βf(zr)‖2 ] = 1 β2 E [ ‖zr − prox1/βh [z r − β∇(g(zr) + g0(zr))]‖2 ] . (13)\nNote that when h, g0 ≡ 0, E[Gr] reduces to E[‖∇g(zr)‖2]. We have the following result.\nTheorem 2.1. Suppose Assumption A holds, and pick (for i = 1, · · · , N)\nαi = pi = √ Li/N∑N\ni=1\n√ Li/N , ηi = 3 ( N∑ i=1 √ Li/N )√ Li/N, β =\n1 3( ∑N\ni=1\n√ Li/N)2 . (14)\nThen every limit point generated by NESTT-G is a stationary solution of problem (2). Further,\n1) E[Gm] ≤ 80 3 ( N∑ i=1 √ Li/N )2E[Q1 −QR+1] R ;\n2) E[Gm] + E [ N∑ i=1 3η2i ∥∥xmi − zm−1∥∥2 ] ≤ 80 3 ( N∑ i=1 √ Li/N )2 E[Q1 −QR+1] R .\nNote that Part (1) is useful in the centralized finite-sum minimization setting, as it shows the sublinear convergence of NESTT-G, measured only by the primal optimality gap evaluated at zr. Meanwhile, part (2) is useful in the distributed setting, as it also shows that the expected constraint violation, which measures the consensus among agents, shrinks in the same order. We also comment that the above result suggests\nthat to achieve an -stationary solution, the NESTT-G requires about O ((∑N i=1 √ Li/N )2 / ) number\nof gradient evaluations (for simplicity we have ignored an additive N factor for evaluating the gradient of the entire function at the initial step of the algorithm).\nIt is interesting to observe that our choice of pi is proportional to the square root of the Lipschitz constant of each component function, rather than to Li. Because of such choice of the sampling probability, the derived convergence rate has a mild dependency on N and Li’s. Compared with the conventional gradientbased methods, our scaling can be up to N times better. Detailed discussion and comparison will be given in Section 4.\nNote that similar sublinear convergence rates can be obtained for the case αi = 1 for all i (with different scaling constants). However due to space limitation, we will not present those results here.\n2.2 Linear Convergence.\nIn this section we show that the NESTT-G is capable of linear convergence for a family of nonconvex quadratic problems, which has important applications, for example in high-dimensional statistical learning [18]. To proceed, we will assume the following.\nB-(a) Each function gi(z) is a quadratic function of the form gi(z) = 1/2z TAiz + 〈b, z〉, where Ai is a\nsymmetric matrix but not necessarily positive semidefinite;\nB-(b) The feasible set Z is a closed compact polyhedral set;\nB-(c) The nonsmooth function p(z) = µ‖z‖1, for some µ ≥ 0.\nOur linear convergence result is based upon certain error bound condition around the stationary solutions set, which has been shown in [21] for smooth quadratic problems and has been extended to including `1 penalty in [29, Theorem 4].\nLemma 2.2. Suppose Assumptions A and B hold. Let Z∗ denotes the set of stationary solutions of problem (1), and dist (z, Z∗) := minu∈Z∗ ‖z − u‖. Then we have the following\n1. (Error Bound Condition) For any ξ ≥ minz f(z), exists a positive scalar τ such that the following error bound holds\ndist (z, Z∗) ≤ τ‖∇̃1/βf(z)‖ (15)\nfor all z ∈ (Z ∩ dom h) and z ∈ {z : f(z) ≤ ξ}.\n2. (Separation of Isocost Surfaces) There exists a scalar δ > 0 such that\n‖z − v‖ ≥ δ whenever z ∈ Z∗, v ∈ Z∗, f(z) 6= f(v). (16)\nWe note that the first statement holds true largely due to [29, Theorem 4], and the second statement holds true due to [20, Lemma 2.1]; see detailed discussion after [29, Assumption 2]. Here the only difference\nAlgorithm 2 NESTT-E Algorithm\n1: for r = 1 to R do 2: Update z by minimizing the augmented Lagrangian:\nzr+1 = arg min z L(xr, z;λr). (17)\n3: Randomly pick ir ∈ {1, 2, · · ·N} with probability pir :\nxr+1ir = argminxir Uir(xir , z r+1;λrir); (18) λr+1ir = λ r ir + αirηir ( xr+1ir − z r+1 ) ; (19) xr+1j = x r j , λ r+1 j = λ r j ∀ j 6= ir. (20)\n4: end for 5: Output: (zm, xm, λm) where m randomly picked from {1, 2, · · · , R}.\nwith the statement [29, Theorem 4] is that the error bound condition (15) holds true globally. This is by the assumption that Z is a compact set. The proof will be provided in the Appendix.\nUtilizing the above result, we have the following linear convergence claim.\nTheorem 2.2. Suppose that Assumptions A, B are satisfied. Then the sequence {E[Qr+1]}∞r=1 converges Q-linearly 1 to some Q∗ = f(z∗), where z∗ is a stationary solution for problem (1). That is, there exists a finite r̄ > 0, ρ ∈ (0, 1) such that for all r ≥ r̄, E[Qr+1 −Q∗]≤ ρE[Qr −Q∗].\nLinear convergence of this type for problems satisfying Assumption B has been shown for (deterministic) proximal gradient based methods [29, Theorem 2, 3]. To the best of our knowledge, this is the first result that shows the same linear convergence for a stochastic and distributed algorithm. There has been some recent works showing linear convergence for nonconvex problems satisfying certain quadratic growth condition [16, 25, 1]. However the problems considered in [16, 25, 1] are smooth unconstrained problems, and every stationary point is a global minimum, therefore they do no cover our nonconvex quadratic problems, whose stationary solutions are not global minimizers.\n3 The NESTT-E Algorithm\n3.1 Algorithm Description\nIn this section, we present a variant of NESTT-G, which is named NESTT with Exact minimization (NESTT-E). Our motivation is the following. First, in NESTT-G every agent should update its local variable at every iteration [cf. (4) or (6)]. In practice this may not be possible, for example at any given time a few agents can be in the sleeping mode so they cannot perform (6). Second, in the distributed setting it has been generally observed (e.g., see [9, Section V]) that performing exact minimization (whenever possible) instead of taking the gradient steps for local problems can significantly speed up the algorithm. The NESTT-E algorithm to be presented in this section is designed to address these issues. To proceed,\n1A sequence {xr} is said to converge Q-linearly to some x̄ if lim supr ‖xr+1 − x̄‖/‖xr − x̄‖ ≤ ρ, where ρ ∈ (0, 1) is some constant; cf [29] and references therein.\nlet us define a new function as follows:\nU(x, z;λ) := N∑ i=1 Ui(xi, z;λi) := N∑ i=1 ( 1 N gi(xi) + 〈λi, xi − z〉+ αiηi 2 ‖xi − z‖2 ) .\nNote that if αi = 1 for all i, then the L(x, z;λ) = U(x, z;λ) + p(z) + h(z). The algorithm details are presented in Algorithm 2. The algorithm proceeds as follows. At each iteration the cluster center minimizes L(x, z;λ) with respect to z. Then the updated z is sent to a randomly selected agent ir ∈ {1, 2, · · ·N}, who minimizes U(x, z;λ) w.r.t. its local variable xir , followed by a dual ascent step for λir .\n3.2 Convergence Analysis\nWe begin analyzing NESTT-E. The proof technique is quite different from that for NESTT-G, and it is based upon using the expected value of the Augmented Lagrangian function as the potential function. For the ease of description we define the following quantities:\nw := (x, z, λ), β := 1∑N i=1 ηi , ci := L2i αiηiN2 − γi 2 + 1− αi αi Li N , α := {αi}Ni=1.\nTo measure the optimality of NESTT-E, define the prox-gradient of L(x, z;λ) as: ∇̃L(w) = [ (z − proxh[z −∇z(L(w)− h(z))]);∇x1L(w); · · · ;∇xNL(w) ] ∈ R(N+1)d. (21)\nWe define the optimality gap by adding to ‖∇̃L(w)‖2 the size of the constraint violation [14]:\nH(wr) := ‖∇̃L(wr)‖2 + N∑ i=1 L2i N2 ‖xri − zr‖2.\nIt can be verified that H(wr) → 0 implies that wr reaches a stationary solution for problem (2). We have the following theorem regarding the convergence properties of NESTT-E.\nTheorem 3.1. Suppose Assumption A holds, and that (ηi, αi) are chosen such that ci < 0 . Then for some constant f , we have\nE[L(wr)] ≥ E[L(wr+1)] ≥ f > −∞, ∀ r ≥ 0.\nFurther, almost surely every limit point of {wr} is a stationary solution of problem (2). Finally, for some function of α denoted as C(α) = σ1(α)/σ2(α), we have the following:\nE[H(wm)] ≤ C(α)E[L(w 1)− L(wR+1)] R , (22)\nwhere σ1 := max(σ̂1(α), σ̃1) and σ2 := max(σ̂2(α), σ̃2), and these constants are given by\nσ̂1(α) = max i\n{ 4 ( L2i N2 + η2i + ( 1 αi − 1 )2 L2i N2 ) + 3 ( L4i αiη2iN 4 + L2i N2 )} ,\nσ̃1 = N∑ i=1 4η2i + (2 + N∑ i=1 ηi + L0) 2 + 3 N∑ i=1 L2i N2 ,\nσ̂2(α) = max i\n{ pi ( γi 2 − L 2 i N2αiηi − 1− αi αi Li N )} , σ̃2 = ∑N i=1 ηi − L0 2 .\nWe remark that the above result shows the sublinear convergence of NESTT-E to the set of stationary solutions. Note that γi = ηi − Li/N , to satisfy ci < 0, a simple derivation yields\nηi > Li\n( (2− αi) + √ (αi − 2)2 + 8αi ) 2Nαi .\nFurther, the above result characterizes the dependency of the rates on various parameters of the algorithm. For example, to see the effect of α on the convergence rate, let us set pi = Li∑N i=1 Li , and ηi = 3Li/N , and assume L0 = 0, then consider two different choices of α: α̂i = 1, ∀ i and α̃i = 4, ∀ i. One can easily check that applying these different choices leads to following results:\nC(α̂) = 49 N∑ i=1 Li/N, C(α̃) = 28 N∑ i=1 Li/N.\nThe key observation is that increasing αi’s reduces the constant in front of the rate. Hence, we expect that in practice larger αi’s will yield faster convergence. This phenomenon will be later confirmed by the numerical results.\nNext let us briefly present the linear convergence of NESTT-E algorithm under Assumption B. The proof again utilizes the error bound condition in Lemma 2.2.\nTheorem 3.2. Suppose that Assumptions A, B are satisfied. Then the sequence {E[Lr+1]}∞r=1 converges Q-linearly to some L∗ = f(z∗), where z∗ is a stationary solution for problem (1). That is, there exists a finite r̄ > 0, ρ ∈ (0, 1) such that for all r ≥ r̄, E[Lr+1 − L∗] ≤ ρE[Lr − L∗].\n4 Connections and Comparisons with Existing Works\nIn this section we compare NESTT-G/E with a few existing algorithms in the literature. First, we present a somewhat surprising observation, that NESTT-G takes the same form as some well-known algorithms for convex finite-sum problems. To formally state such relation, we show in the following result that NESTT-G in fact admits a compact primal-only characterization.\nProposition 4.1. The NESTT-G can be written into the following compact form:\nzr+1 = arg min z\nh(z) + g0(z) + 1\n2β ‖z − ur+1‖2 (23a)\nwith ur+1 := zr − β ( 1 Nαir (∇gir(zr)−∇gir(yr−1ir )) + 1 N N∑ i=1 ∇gi(yr−1i ) ) . (23b)\nBased on this observation, the following comments are in order.\n(1) Suppose h ≡ 0, g0 ≡ 0 and αi = 1, pi = 1/N for all i. Then (23) takes the same form as the SAG presented in [26]. Further, when the component functions gi’s are picked cyclically in a Gauss-Seidel manner, the iteration (23) takes the same form as the IAG algorithm [6].\n(2) Suppose h 6= 0 and g0 6= 0, and αi = pi = 1/N for all i. Then (23) is the same as the SAGA algorithm [10], which is design for optimizing convex nonsmooth finite sum problems.\nCase I: Li = 1, ∀i O(N/ ) O(N/ ) Case II : O(N2/3) terms with Li = N2/3 the rest with Li = 1 O(N/ ) O(N4/3/ ) Case II : O( √ N) terms with Li = N the rest with Li = 1 O(N/ ) O(N3/2/ ) Case IV : O(1) terms with Li = N2 the rest with Li = 1 O(N/ ) O(N2/ )\nNote that SAG/SAGA/IAG are all designed for convex problems. Through the lens of primal-dual splitting, our work shows that they can be generalized to nonconvex nonsmooth problems as well.\nSecondly, NESTT-E is related to the proximal version of the nonconvex ADMM [14, Algorithm 2]. However, the introduction of αi’s is new, which can significantly improve the practical performance but complicates the analysis. Further, there has been no counterpart of the sublinear and linear convergence rate analysis for the stochastic version of [14, Algorithm 2].\nThirdly, we note that a recent paper [25] has shown that SAGA works for smooth and unconstrained nonconvex problem. Suppose that h ≡ 0, g0 6= 0, Li = Lj , ∀ i, j and αi = pi = 1/N , the authors show that SAGA achieves -stationarity using O(N2/3( ∑N i=1 Li/N)/ ) gradient evaluations. Compared with GD,\nwhich achieves -stationarity using O( ∑N\ni=1 Li/ ) gradient evaluations in the worse case (in the sense that∑N i=1 Li/N = L), the rate in [25] is O(N1/3) times better. However, the algorithm in [25] is different from NESTT-G in two aspects: 1) it does not generalize to the nonsmooth constrained problem (1); 2) it samples two component functions at each iteration, while NESTT-G only samples once. Further, the analysis and the scaling are derived for the case of uniform Li’s, therefore it is not clear how the algorithm and the rates can be adapted for the non-uniform case. On the other hand, our NESTT works for the general nonsmooth constrained setting. The non-uniform sampling used in NESTT-G is well-suited for problems with non-uniform Li’s, and our scaling can be up to N times better than GD (or its proximal version) in the worst case. Note that problems with non-uniform Li’s for the component functions are common in applications such as sparse optimization and signal processing. For example in LASSO problem the data matrix is often normalized by feature (or “column-normalized” [22]), therefore the `2 norm of each row of the data matrix (which corresponds to the Lipschitz constant for each component function) can be dramatically different.\nIn Table 1 we list the comparison of the number of gradient evaluations for NESTT-G and GD, in the worst case (in the sense that ∑N i=1 Li/N = L). For simplicity, we omitted an additive constant of O(N) for computing the initial gradients.\n5 Numerical Results\nIn this section we evaluate the performance of NESTT. Consider the high dimensional regression problem with noisy observation [18], where M observations are generated by y = Xν + . Here y ∈ RM is the observed data sample; X ∈ RM×P is the covariate matrix; ν ∈ RP is the ground truth, and ∈ RM is\nthe noise. Suppose that the covariate matrix is not perfectly known, i.e., we observe A = X + W where W ∈ RM×P is the noise matrix with known covariance matrix ΣW . Let us define Γ̂ := 1/M(A>A)−ΣW , and γ̂ := 1/M(A>y). To estimate the ground truth ν, let us consider the following (nonconvex) optimization problem posed in [18, problem (2.4)] (where R > 0 controls sparsity):\nmin z\nz>Γ̂z − γ̂z s.t. ‖z‖1 ≤ R. (24)\nDue to the existence of noise, Γ̂ is not positive semidefinite hence the problem is not convex. Note that this problem satisfies Assumption A– B, then by Theorem 2.2 NESTT-G converges Q-linearly.\nTo test the performance of the proposed algorithm, we generate the problem following similar setups as [18]. Let X = (X1; · · · , XN ) ∈ RM×P with ∑ iNi = M and each Xi ∈ RNi×P corresponds to Ni data points, and it is generated from i.i.d Gaussian. Here Ni represents the size of each mini-batch of samples. Generate the observations yi = Xi × ν∗ + i ∈ RNi , where ν∗ is a K-sparse vector to be estimated, and i ∈ RNi is the random noise. Let W = [W1; · · · ;WN ], with Wi ∈ RNi×P generated with i.i.d Gaussian. Therefore we have z>Γ̂z = 1N ∑N i=1 N M z > (X>i Xi −W>i Wi) z. We set M = 100.000, P = 5000, N = 50,\nK = 22 ≈ √ P ,and R = ‖ν∗‖1. In simulation, we perform a mini-batch version of the algorithms, meaning that we split the data matrix A and labels y into M submatrices and store them into different nodes. Therefore, in each node we have Γ̂i ∈ Rni×p, and γ̂i ∈ Rp such that ∑M i=1 ni = n. Here we set M = 30. We implement NESTT-G/E, the SGD, and the nonconvex SAGA proposed in [25] with stepsize β = 1 3LmaxN2/3 (with Lmax := maxi Li). Note that the SAGA proposed in [25] only works for the unconstrained problems with uniform Li, therefore when applied to (24) it is not guaranteed to converge. Here we only include it for comparison purposes.\nIn Fig. 1 we compare different algorithms in terms of the gap ‖∇̃1/βf(zr)‖2. In the left figure we consider the problem with Ni = Nj for all i, j, and we show performance of the proposed algorithms with uniform sampling (i.e., the probability of picking ith block is pi = 1/N). On the right one we consider problems in which approximately half of the component functions have twice the size of Li’s as the rest, and consider the non-uniform sampling (pi = √ Li/N/ ∑N i=1 √ Li/N). Clearly in both cases the proposed algorithms perform quite well. Furthermore, it is clear that the NESTT-E performs well with large α := {αi}Ni=1, which confirms our theoretical rate analysis. Also it is worth mentioning that when the Ni’s are non-uniform, the proposed algorithms [NESTT-G and NESTT-E (with α = 10)] significantly outperform SAGA and SGD. In Table 2 we further compare different algorithms when changing the number of component functions (i.e., the number of mini-batches N) while the rest of the setup is as above. We run each algorithm with 100 passes over the dataset. Similarly as before, our algorithms perform well, while SAGA seems to be sensitive to the uniformity of the size of the mini-batch [note that there is no convergence guarantee for SAGA applied to the nonconvex constrained problem (24)].\ndataset. Left: Uniform Sampling pi = 1/N ; Right: Non-uniform Sampling (pi =\n√\nLi/N∑N\ni=1\n√ Li/N ).\nAppendix\n5.1 Some Key Properties of NESTT-G\nTo facilitate the following derivation, in this section we collect some key properties of NESTT-G. First, from the optimality condition of the x update we have\nxr+1ir = z r − 1\nαirηir\n( λrir + 1\nN ∇gir(zr)\n) , (25a)\nxr+1j (6) = zr (8b) = zr − 1\nαjηj (λrj +\n1\nN ∇gj(zr(j))), ∀ j 6= ir. (25b)\nThen using the update scheme of the λ we can further obtain\nλr+1ir = − 1\nN ∇gir(zr), (26a)\nλr+1j = − 1\nN ∇gj(zr(j)), ∀ j 6= ir. (26b)\nTherefore, using the definition of yri we have the following compact forms\nλr+1i = − 1\nN ∇gi(yri ), i = 1, · · · , N. (27)\nxr+1i = z r − 1\nαiηi\n( λri + 1\nN ∇gi(yri )\n) , i = 1, · · · , N. (28)\nSecond, let us look at the optimality condition for the z update. The z-update (7) is given by\nzr+1 = arg min z L({xr+1i }, z;λ r)\n= arg min z N∑ i=1 ( 〈λri , xr+1i − z〉+ ηi 2 ‖xr+1i − z‖ 2 ) + g0(z) + h(z). (29)\nNote that this problem is strongly convex because we have assumed that ∑\ni=1 ηi > 3L0; cf. Assumption [A-(c)].\nLet us define\nur+1 :=\n∑N i=1 ηix r+1 i + ∑N i=1 λ\nr i∑N\ni=1 ηi\n=\n∑N i=1 ηiz\nr − ηir(zr − xr+1ir )∑N i=1 ηi +\n∑N i=1 λ\nr i∑N\ni=1 ηi\n(25a) =\n∑N i=1 ηiz r − ηirαirηir (λ r ir\n+ 1/N∇gir(zr))∑N i=1 ηi +\n∑N i=1 λ\nr i∑N\ni=1 ηi\n(27) = zr −\n1 αir (−∇gir(yr−1ir ) +∇gir(z r)) N ∑N i=1 ηi − ∑N i=1∇gi(y r−1 i ) N ∑N i=1 ηi\n(i) = zr − β\nNαir (−∇gir(yr−1ir ) +∇gir(z\nr))− β ∑N i=1∇gi(y r−1 i )\nN (30)\n(ii) : = zr − βvr+1ir (31)\nwhere in (i) we have defined β := 1/ ∑N\ni=1 ηi; in (ii) we have defined\nvr+1ir := 1\nN N∑ i=1 ∇gi(yr−1i ) + 1 αir ( − 1 N ∇gir(yr−1ir ) + 1 N ∇gir(zr) ) . (32)\nClearly if we pick αi = pi for all i, then we have\nEir [ur+1 | Fr] = zr − β\nN N∑ i=1 ∇gi(zr). (33)\nUsing the definition of ur+1, it is easy to check that\nzr+1 = arg min z\n1\n2β ‖z − ur+1‖2 + h(z) + g0(z)\n= prox 1/β h [u r+1 − β∇g0(zr+1)]. (34)\nThe optimality condition for the z subproblem is given by:\nzr+1 − ur+1 + β∇g0(zr+1) + βξr+1 = 0 (35)\nwhere, ξr+1 ∈ ∂h(zr+1) is a subgradient of h(zr+1). Using the definition of vir in (32), we obtain\nzr+1 = zr − β(vr+1ir +∇g0(z r+1) + ξr+1). (36)\nThird, if αi = pi, then we have:\nEir ∥∥∥∥∥−λrir + 1/N∇gir(zr)αir + 1N N∑ i=1 ∇gi(zr)− N∑ i=1 1 N ∇gi(yr−1i ) ∥∥∥∥∥ 2 \n(a) = Var [ − λrir + 1/N∇gir(z r)\nαir ] (b)\n≤ N∑ i=1 1 αi ∥∥∥∥ 1N∇gi(zr)− 1N∇gi(yr−1i ) ∥∥∥∥2 , (37)\nwhere (a) is true because whenever αi = pi for all i, then\nEir [ − λrir + 1/N∇gir(z r)\nαir\n] = 1\nN N∑ i=1 ∇gi(zr)− N∑ i=1 1 N ∇gi(yr−1i );\nThe inequality in (b) is true because for a random variable x we have Var(x) ≤ E[x2].\n5.2 Proof of Lemma 2.1\nStep 1). Using the definition of potential function Qr, we have:\nE[Qr −Qr−1 | Fr−1]\n= E [ N∑ i=1 1 N ( gi(z r)− gi(zr−1) ) + g0(z r)− g0(zr−1) + h(zr)− h(zr−1) | Fr−1 ]\n+ E [ N∑ i=1 3pi α2i ηi ∥∥∥∥ 1N∇gi(zr)− 1N∇gi(yr−1i ) ∥∥∥∥2 − 3piα2i ηi ∥∥∥∥ 1N∇gi(zr−1)− 1N∇gi(yr−2i ) ∥∥∥∥2 | Fr−1 ] . (38)\nStep 2). The first term in (38) can be bounded as follows (omitting the subscript Fr).\nE [ N∑ i=1 1 N ( gi(z r)− gi(zr−1) ) + g0(z r)− g0(zr−1) + h(zr)− h(zr−1) | Fr−1 ] (i)\n≤ E [ 1\nN N∑ i=1 〈∇gi(zr−1), zr − zr−1〉+ 〈∇g0(zr−1), zr − zr−1〉\n+ 〈ξr, zr − zr−1〉+ ∑N\ni=1 Li/N + L0 2\n‖zr − zr−1‖2 | Fr−1 ]\n(ii) = E\n[〈 1\nN N∑ i=1 ∇gi(zr−1) + ξr +∇g0(zr) + 1 β (zr − zr−1), zr − zr−1\n〉 | Fr−1 ]\n−\n( 1 β − ∑N i=1 Li/N + 3L0 2 ) Ezr‖zr − zr−1‖2\n(36) = E\n[〈 1\nN N∑ i=1 ∇gi(zr−1)− vri(r−1), z r − zr−1\n〉 | Fr−1 ]\n−\n( 1 β − ∑N i=1 Li/N + 3L0 2 ) Ezr‖zr − zr−1‖2\n(iii)\n≤ 1 2`1 E ∥∥∥∥∥1/N N∑ i=1 ∇gi(zr−1)− vri(r−1) ∥∥∥∥∥ 2 | Fr−1 + `1 2 Ezr‖zr − zr−1‖2\n−\n( 1 β − ∑N i=1 Li/N + 3L0 2 ) Ezr‖zr − zr−1‖2 (39)\nwhere in (i) we have used the Lipschitz continuity of the gradients of gi’s as well as the convexity of h; in (ii) we have used the fact that\n〈∇g0(zr−1), zr − zr−1〉 ≤ 〈∇g0(zr), zr − zr−1〉+ L0‖zr − zr−1‖2; (40)\nin (iii) we have applied the Young’s inequality for some `1 > 0. Choosing `1 = 1 2β , we have:\n1\n2`1 E ∥∥∥∥∥ 1N N∑ i=1 ∇gi(zr−1)− vri(r−1) ∥∥∥∥∥ 2\n(32) = βE ∥∥∥∥∥ 1N N∑ i=1 ∇gi(zr−1)− λr−1i(r−1) + 1/N∇gi(r−1)(z r−1) αi(r−1) − N∑ i=1 1 N ∇gi(yr−2i ) ∥∥∥∥∥ 2 \n(37) ≤ β N∑ i=1 1 αi ∥∥∥∥ 1N∇gi(zr−1)− 1N∇gi(yr−2i ) ∥∥∥∥2 .\nOverall we have the following bound for the first term in (38):\nE [ N∑ i=1 1 N ( gi(z r)− gi(zr−1) ) + g0(z r)− g0(zr−1) + h(zr)− h(zr−1) | Fr−1 ] (41)\n≤ N∑ i=1 β αi ∥∥∥∥ 1N∇gi(zr−1)− 1N∇gi(yr−2i ) ∥∥∥∥2 − ( 3 4β − ∑N i=1 Li/N + 3L0 2 ) Ezr‖zr − zr−1‖2.\nStep 3). We bound the second term in (38) in the following way: E [ ‖∇gi(zr)−∇gi(yr−1i )‖ 2 | Fr−1 ]\n= E [ ‖∇gi(zr)−∇gi(yr−1i ) +∇gi(z r−1)−∇gi(zr−1)‖2 | Fr−1 ] (i)\n≤ (1 + ξi)Ezr‖∇gi(zr)−∇gi(zr−1)‖2 + ( 1 + 1\nξi\n) Eyr−1i ‖∇gi(y r−1 i )−∇gi(z r−1)‖2\n(ii) = (1 + ξi)Ezr‖∇gi(zr)−∇gi(zr−1)‖2 + (1− pi) ( 1 + 1\nξi\n) ‖∇gi(yr−2i )−∇gi(z r−1)‖2 (42)\nwhere in (i) we have used the fact that the randomness of zr−1 comes from ir−2, so fixing Fr−1, zr−1 is deterministic; we have also applied the following inequality:\n(a+ b)2 ≤ (1 + ξ)a2 + (1 + 1 ξ )b2 ∀ ξ > 0.\nThe equality (ii) is true because the randomness of yr−1i comes from ir−1, and for each i there is a probability pi such that x r i is updated, so that ∇gi(y r−1 i ) = ∇gi(zr−1), otherwise xi is not updated so that ∇gi(yr−1i ) = ∇gi(y r−2 i ).\nStep 4). Applying (42) and set αi = pi, the second part of (38) can be bounded as\nE [ N∑ i=1 3pi α2i ηi ∥∥∥∥ 1N∇gi(zr)− 1N∇gi(yr−1i ) ∥∥∥∥2 − 3piα2i ηi ∥∥∥∥ 1N∇gi(zr−1)− 1N∇gi(yr−2i ) ∥∥∥∥2 | Fr−1 ]\n≤ N∑ i=1 3L2i αiηiN2 (1 + ξi)Ezr‖zr − zr−1‖2\n+ 3\nαiηi\n( (1− pi)(1 + 1\nξi )− 1 )∥∥∥∥ 1N∇gi(yr−2i )− 1N∇gi(zr−1) ∥∥∥∥2 . (43)\nCombining (41) and (43) eventually we have\nE[Qr −Qr−1 | Fr]\n≤ N∑ i=1 { β αi + 3 αiηi ( (1− pi)(1 + 1 ξi )− 1 )}∥∥∥∥ 1N∇gi(zr−1)− 1N∇gi(yr−2i ) ∥∥∥∥2\n+ { − 3\n4β +\n∑N i=1 Li/N + 3L0\n2 + N∑ i=1 3L2i αiηiN2 (1 + ξi)\n} Ezr‖zr − zr−1‖2. (44)\nLet us define {c̃i} and ĉ as following:\nc̃i = β\nαi +\n3\nαiηi\n( (1− pi)(1 + 1\nξi )− 1 ) ĉ = − 3\n4β +\n∑N i=1 Li/N + 3L0\n2 + N∑ i=1 3L2i αiηiN2 (1 + ξi) .\nIn order to prove the lemma it is enough to show that c̃i < − 12ηi ∀ i, and ĉ < − ∑N i=1 ηi 8 . Let us pick\nαi = pi, ξi = 2\npi , pi = ηi∑N i=1 ηi . (45)\nRecall that β = 1∑N i=1 ηi .These values yield the following\nc̃i = 1 ηi − 3 ηi\n( pi + 1\n2 ) ≤ 1 ηi − 3 2ηi = − 1 2ηi < 0.\nTo show that ĉ ≤ − ∑N\ni=1 ηi 8 let us assume that ηi = diLi for some di > 0. Note that by assumption we\nhave N∑ i=1 ηi ≥ 3L0.\nTherefore we have the following expression for ĉ:\nĉ ≤ − N∑ i=1 1 4 diLi + Li 2N + 3Li pidiN2 ( 1 + 2 pi )\n< N∑ i=1 Li di ( −1 4 d2i + di 2N + 9 p2iN 2 ) .\nAs a result, to have ĉ < − ∑N\ni=1 ηi 8 , we need\nLi di\n( 1\n4 d2i − di 2N − 9 p2iN 2\n) ≥ diLi\n8 , ∀ i. (46)\nOr equivalently\n1 8 d2i − di 2N − 9 p2iN 2 ≥ 0, ∀ i. (47)\nBy finding the root of the above quadratic inequality, we need di ≥ 9Npi , which is equivalent to choosing the following parameters\nηi ≥ 9Li Npi . (48)\nThe lemma is proved. Q.E.D.\n5.3 Proof of Theorem 2.1\nFirst, using the fact that f(z) is lower bounded [cf. Assumption A-(a)], it is easy to verify that {Qr} is a bounded sequence. Denote its lower bound to be Q. From Lemma 2.1, it is clear that {Qr − Q} is a nonnegative supermartingale. Apply the Supermartigale Convergence Theorem [4, Proposition 4.2] we conclude that {Qr} converges almost surely (a.s.), and that∥∥∇gi(zr−1)−∇gi(yr−2i )∥∥2 → 0, Ezr‖zr − zr−1‖ → 0, a.s., ∀ i. (49) The first inequality implies that ‖λrir−λ r−1 ir ‖ → 0. Combining this with equation (5) yields ‖xrir−z\nr−1‖ → 0, which further implies that ‖zr − zr−1‖ → 0. By utilizing (8b) – (8c), we can conclude that\n‖xri − xr−1i ‖ → 0, ‖λ r i − λr−1i ‖ → 0, a.s., ∀ i. (50)\nThat is, almost surely the successive differences of all the primal and dual variables go to zero. Then it is easy to show that every limit point of the sequence (xr, zr, λr) converge to a stationary solution of problem (2) (for example, see the argument in [14, Theorem 2]. Here we omit the full proof.\nPart 1). We bound the gap in the following way (where the expectation is taking over the nature history of the algorithm):\nE [ ‖zr − prox1/βh [z r − β∇(g(zr) + g0(zr))]‖2 ]\n(a) = E [ ‖zr − zr+1 + prox1/βh [u r+1 − β∇g0(zr+1)]− prox1/βh [z r − β∇(g(zr) + g0(zr))]‖2 ] (b) ≤ 3E‖zr − zr+1‖2 + 3E‖ur+1 − zr + β∇g(zr)‖2 + 3L20β2‖zr+1 − zr‖2\n(c) ≤ 10 3 E‖zr − zr+1‖2 + 3β2E\n[ ‖∇g(zr)− λrir + 1/N∇gir(z r)\nαir − N∑ i=1 1/N∇gi(yr−1i )‖ 2 ] (37)\n≤ 10 3 E‖zr − zr+1‖2 + 3β2 N∑ i=1 1 αi E ∥∥∥∥ 1N∇gi(zr)− 1N∇gi(yr−1i ) ∥∥∥∥2\n≤ 10 3 E‖zr − zr+1‖2 + 3 N∑ i=1 β ηi E ∥∥∥∥ 1N∇gi(zr)− 1N∇gi(yr−1i ) ∥∥∥∥2 (51) where (a) is due to (34); (b) is true due to the nonexpansivness of the prox operator, and the Cauchy-\nSwartz inequality; in (c) we have used the definition of u in (31) and the fact that 3L0 ≤ ∑N i=1 ηi = 1 β [cf. Assumption A-(c)]. In the last inequality we have applied (45), which implies that\nβ αi =\n1 pi ∑N j=1 ηj = 1 ηi . (52)\nNote that ηi’s has to satisfy (48). Let us follow (11) and choose\nηi = 9Li piN\n= 9 ∑N j=1 ηj\nNηi Li.\nWe have\nηi = √√√√9Li/N N∑ j=1 ηj = √ 9Li/N √√√√ N∑ j=1 ηj (53)\nSumming i from 1 to N we have √√√√ N∑ i=1 ηi = N∑ i=1 √ 9Li/N (54)\nThen we conclude that\n1 β = N∑ i=1 ηi = ( N∑ i=1 √ 9Li/N )2 . (55)\nSo plugging the expression of β into (52) and (53), we conclude\nαi = pi = √ Li/N∑N\ni=1\n√ Li/N\n, ηi = √ 9Li/N N∑ j=1 √ 9Lj/N. (56)\nAfter plugging in the above inequity into (13), we obtain:\nE[Gr] (51) ≤ 10 3β2 E‖zr − zr+1‖2 + N∑ i=1 3 βηi E ∥∥∥∥ 1N∇gi(zr)− 1N∇gi(yr−1i ) ∥∥∥∥2 (57) (12)\n≤ 80 3β E[Qr −Qr+1] = 80 3 ( N∑ i=1 √ Li/N )2 E[Qr −Qr+1]\nIf we sum both sides over r = 1, · · · , R, we obtain:\nR∑ r=1 E[Gr] ≤ 80 3 ( N∑ i=1 √ Li/N )2 E[Q1 −QR+1].\nUsing the definition of zm, we have\nE[Gm] = EFr [Em[Gm | Fr]] = 1/R R∑ r=1 EFr [Gr].\nTherefore, we can finally conclude that:\nE[Gm] ≤ 80 3 ( N∑ i=1 √ Li/N )2 E[Q1 −QR+1] R (58)\nwhich proves the first part.\nPart 2). In order to prove the second part let us recycle inequality in (57) and write\nE [ Gr +\nN∑ i=1 3 βηi ∥∥∥∥ 1N∇gi(zr)− 1N∇gi(yr−1i ) ∥∥∥∥2 ]\n≤ 10 3β2 E‖zr+1 − zr‖2 + N∑ i=1 6 βηi E ∥∥∥∥ 1N∇gi(zr)− 1N∇gi(yr−1i ) ∥∥∥∥2\n≤ 80 3β E[Qr −Qr+1] = 48 ( N∑ i=1 √ Li/N )2 E[Qr −Qr+1].\nAlso note that\nExr [∥∥xr+1i − zr∥∥2 | Fr] = N∑\ni=1\n1\nαiη2i\n∥∥∥∥ 1N∇gi(zr)− 1N∇gi(yr−1i ) ∥∥∥∥2 (59)\nCombining the above two inequalities, we conclude EFr [Gr] + EFr [ N∑ i=1 3η2i ∥∥xr+1i − zr∥∥2 ]\n= EFr [Gr] + EFr [ N∑ i=1 3ηiαi β ∥∥xr+1i − zr∥∥2 ]\n= E [ Gr +\nN∑ i=1 3 βηi ∥∥∥∥ 1N∇gi(zr)− 1N∇gi(yr−1i ) ∥∥∥∥2 ]\n≤ 80 3 ( N∑ i=1 √ Li/N )2 EFr [Qr −Qr+1] (60)\nwhere in the first equality we have used the relation αiβ = ηi [cf. (52)]. Using a similar argument as in first part, we conclude that\nE[Gm] + E [ N∑ i=1 3η2i ∥∥xmi − zm−1∥∥2 ] ≤ 80 3 ( N∑ i=1 √ Li/N )2 E[Q1 −QR+1] R . (61)\nThis completes the proof. Q.E.D.\n5.4 Proof of Theorem 2.2\nProof of Lemma 2.2 The first statement holds true largely due to [29, Theorem 4], and the second statement holds true due to [20, Lemma 2.1]; see detailed discussion after [29, Assumption 2]. Here the only difference with the statement [29, Theorem 4] is that the error bound condition (15) holds true globally. This is by the assumption that Z is a compact set. Below we provide a brief argument.\nFrom [29, Theorem 4], we know that when Assumption B is satisfied, we have that for any ξ ≥ minz f(z), there exists scalars τ and such that the following error bound holds\ndist (z, Z∗) ≤ τ‖∇̃1/βf(z)‖, whenever ‖∇̃1/βf(z)‖ ≤ , f(z) ≤ ξ. (62)\nTo argue that when Z is compact, the above error bound is independent of , we use the following two steps: (1) for all z ∈ Z ∩dom(h) such that ‖∇̃1/βf(z)‖ ≤ δ, it is clear that the error bound (15) holds true; (2) for all z ∈ Z ∩ dom(h) such that ‖∇̃1/βf(z)‖ ≥ δ, the ratio\ndist (z,Z∗) ‖∇̃1/βf(z)‖ is a continuous function and well defined over the compact set Z ∩ dom(h)∩ { z | ‖∇̃1/βf(z)‖ ≥ δ } . Thus, the above ratio must be bounded from above by a constant τ ′ (independent of b, and no greater than maxz,z′∈Z ‖z − z′‖/δ). Combining (1) and (2) yields the desired error bound over the set Z ∩ dom(h). Q.E.D.\nProof of Theorem 2.2 From Theorem 2.1 we know that (xr, zr, λr) converges to the set of stationary solutions of problem (2). Let (x∗, z∗, λ∗) be one of such stationary solution. Then by the definition of the Q function and the fact that the successive differences of the gradients goes to zero (cf. (49)), we have\nQ∗ = f(z∗) = N∑ i=1 1/Ngi(z ∗) + g0(z ∗) + p(z∗). (63)\nThen by Lemma 2.2 - (2) we know that f(zr) = ∑N\ni=1 1/Ngi(z r) + g0(z r) + p(zr) will finally settle at some isocost surface of f , i.e., there exists some finite r̄ > 0 such that for all r > r̄ and v̄ ∈ R such that\nf(z̄r) = v̄, ∀ r ≥ r̄ (64)\nwhere z̄r = arg minz∈Z∗ ‖zr − z‖. Therefore, combining the fact that ‖xr+1 − xr‖ → 0, ‖zr+1 − zr‖ → 0, ‖xr+1i − zr+1‖ → 0 and ‖λr+1 − λr‖ → 0 (cf. (87), (88)), it is easy to see that\nL(z̄r, x̄r, λ̄r) = f(z̄r) = v̄, ∀ r ≥ r̄, (65)\nwhere x̄r, λ̄r are defined similarly as z̄r. Now we prove that the expectation of ∆r+1 := Qr+1 − v̄ diminishes Q-linearly. All the expectation below is w.r.t. the natural history of the algorithm. The proof consists of the following steps: Step 1: There exists σ1 > 0 such that\nE[Qr −Qr+1] ≥ σ1 ( E‖zr+1 − zr‖2 +\nN∑ i=1 E‖1/N∇gi(zr)− 1/N∇gi(yr−1i )‖ 2\n) ;\nStep 2: There exists τ > 0 such that\nE‖zr − z̄r‖2 ≤ τ‖E[∇1/β f̃(zr)]‖2;\nStep 3: There exists σ2 > 0 such that\n‖E[∇1/β f̃(zr)]‖2 ≤ σ2\n( E‖zr+1 − zr‖2 +\nN∑ i=1 E‖1/N∇gi(zr)− 1/N∇gi(yr−1i )‖ 2\n) ;\nStep 4: There exists σ3 > 0 such that the following relation holds true for all r ≥ r̄\nE[Qr+1 − v̄] ≤ σ3 ( E‖zr − z̄r‖2 + E‖zr+1 − zr‖2 +\nN∑ i=1 E‖1/N∇gi(zr)− 1/N∇gi(yr−1i )‖ 2\n) .\nThese steps will be verified one by one shortly. But let us suppose that they all hold true. Below we show that linear convergence can be obtained.\nCombining step 4 and step 2 we conclude that there exists σ3 > 0 such that for all r ≥ r̄\nE[Qr+1 − v̄] ≤ σ3 ( τ‖E[∇1/β f̃(zr−1)]‖2 + E‖zr+1 − zr‖2 +\nN∑ i=1 E‖1/N∇gi(zr)− 1/N∇gi(yr−1i )‖ 2\n) .\nThen if we bound ‖E(Gr)‖2 using step 3, we can simply make a σ4 > 0 such that\nE[Qr+1 − v̄] ≤ σ4 ( E‖zr+1 − zr‖2 +\nN∑ i=1 E‖1/N∇gi(zr)− 1/N∇gi(yr−1i )‖ 2\n) .\nFinally, applying step 1 we reach the following bound for E[Qr+1 − v̄]:\nE[Qr+1 − v̄] ≤ σ4 σ1 E[Qr −Qr+1], ∀ r ≥ r̄,\nwhich further implies that for σ5 = σ4 σ1 > 0, we have\nE[∆r+1] ≤ σ5 1 + σ5 E[∆r], ∀ r ≥ r̄.\nNow let us verify the correctness of each step. Step 1 can be directly obtained from equation (12). Step 2 is exactly Lemma (2.2). Step 3 can be verified using a similar derivation as in (51)2.\nBelow let us prove the step 4, which is a bit involved. From (7) we know that\nzr+1 = arg min z h(z) + g0(z) + N∑ i=1 〈λri , xr+1i − z〉+ ηi 2 ‖xr+1i − z‖ 2.\nThis implies that\nh(zr+1) + g0(z r+1) + N∑ i=1 〈λri , xr+1i − z r+1〉+ ηi 2 ‖xr+1i − z r+1‖2\n≤ h(z̄r) + g0(z̄r) + N∑ i=1 〈λri , xr+1i − z̄ r〉+ ηi 2 ‖xr+1i − z̄ r‖2. (66)\nRearranging the terms, we obtain\nh(zr+1) + g0(z r+1)− h(z̄r)− g0(z̄r) ≤ N∑ i=1 〈λri , zr+1 − z̄r〉+ ηi 2 ‖xr+1i − z̄ r‖2.\n2We simply need to replace −zr−1 + prox1/βh [u r−1 − β∇g0(zr−1)] in step (a) of (51) by −zr + prox1/βh [u r − β∇g0(zr)] and using the same derivation.\nUsing this inequality we have:\nQr+1 − v̄ ≤ N∑ i=1 1/N ( gi(z r+1)− gi(z̄r) ) + 〈λri , zr+1 − z̄r〉\n+ N∑ i=1 ηi 2 ‖xr+1i − z̄ r‖2 + ‖1/N(∇gi(zr)−∇gi(yr−1i )‖ 2. (67)\nThe first term in RHS can be bounded as follows: N∑ i=1 1/N ( gi(z r+1)− gi(z̄r) )\n(a) ≤ N∑ i=1 1/N〈∇gi(z̄r), zr+1 − z̄r〉+ Li/2N‖zr+1 − z̄r‖2\n≤ N∑ i=1 1/N〈∇gi(z̄r) +∇gi(zr+1)−∇gi(zr+1), zr+1 − z̄r〉+ Li/2N‖zr+1 − z̄r‖2\n(b) ≤ N∑ i=1 1/N〈∇gi(zr+1), zr+1 − z̄r〉+ 3Li/2N‖zr+1 − z̄r‖2,\nwhere (a) is true due to the descent lemma; and (b) comes from the Lipschitz continuity of the ∇gi. Plugging the above bound into (67), we further have:\nQr+1 − v̄ ≤ N∑ i=1 1/N〈∇gi(zr+1)−∇gi(yr−1i ), z r+1 − z̄r〉+ 3Li/2N‖zr+1 − z̄r‖2\n+ ηi 2 ‖xr+1i − z̄ r‖2 + ‖1/N(∇gi(zr)−∇gi(yr−1i )‖ 2\n= N∑ i=1 1/N〈∇gi(zr+1) +∇gi(zr)−∇gi(zr)−∇gi(yr−1i ), z r+1 − z̄r〉 + ηi 2 ‖xr+1i − z̄ r‖2 + ‖1/N(∇gi(zr)−∇gi(yr−1i )‖ 2 + 3Li/2N‖zr+1 − z̄r‖2,\nwhere in the first inequality we have used the fact that λri = − 1N∇gi(y r−1 i ); cf . (27). Applying the Cauchy-Schwartz inequality we further have:\nQr+1 − v̄ ≤ N∑ i=1 1/2‖1/N ( ∇gi(zr+1) +∇gi(zr) ) ‖2 + 1/2‖zr+1 − z̄r‖2\n+ N∑ i=1 1/2‖1/N ( ∇gi(zr)−∇gi(yr−1i ) ) ‖2 + 1/2‖zr+1 − z̄r‖2 + ηi 2 ‖xr+1i − z̄ r‖2 + ‖1/N(∇gi(zr)−∇gi(yr−1i )‖ 2 + 3Li/2N‖zr+1 − z̄r‖2\n≤ N∑ i=1 [ L2i 2N2 ‖zr+1 − zr‖2 + 3 2N2 ‖gi(zr)−∇gi(yr−1i )‖ 2 + ηi 2 ‖xr+1i − z̄ r‖2 ] + (1 + 3Li/2N) ‖zr+1 − z̄r‖2. (68)\nNow let us bound ∑N\ni=1 ηi 2 ‖x r+1 i − z̄r‖2 in the above inequality:\nN∑ i=1 ηi 2 ‖xr+1i − z̄ r‖2 = N∑ i=1 ηi 2 ‖xr+1i − z r+1 + zr+1 − z̄r‖2\n≤ N∑ i=1 ηi‖xr+1i − z r+1‖2 + ηi‖zr+1 − z̄r‖2\n= N∑ i=1 ηi‖xr+1i − z r + zr − zr+1‖2 + ηi‖zr+1 − z̄r‖2\n≤ N∑ i=1 2ηi‖xr+1i − z r‖2 + 2ηi‖zr − zr+1‖2 + ηi‖zr+1 − z̄r‖2.\nUsing the fact that xr+1i = z r when i 6= ir we further have:\nN∑ i=1 ηi 2 ‖xr+1i − z̄ r‖2 ≤ 2ηir‖xr+1ir − z r‖2 + N∑ i=1 2ηi‖zr − zr+1‖2 + ηi‖zr+1 − z̄r‖2\n= 2\nα2irηir ‖λir + 1/N∇gir(zr)‖2 + N∑ i=1 2ηi‖zr − zr+1‖2 + ηi‖zr+1 − z̄r‖2\n= 2\nα2irηirN 2 ‖∇gir(zr)−∇gir(yr−1ir )‖ 2\n+ N∑ i=1 2ηi‖zr − zr+1‖2 + ηi‖zr+1 − zr + zr − z̄r‖2 ≤ 2 α2irηirN 2 ‖∇gir(zr)−∇gir(yr−1ir )‖ 2\n+ N∑ i=1 4ηi‖zr − zr+1‖2 + 2ηi‖zr − z̄r‖2. (69)\nTake expectation on both sides of the above equation and set pi = αi, we obtain:\nN∑ i=1 ηi 2 E‖xr+1i − z̄ r‖2 ≤ N∑ i=1 2 αiηi E‖∇gi(zr)−∇gi(yr−1i )‖ 2\n+ N∑ i=1 4ηiE‖zr − zr+1‖2 + 2ηiE‖zr − z̄r‖2.\nCombining equations (68) and (69), eventually one can find σ3 > 0 such that\nE[Qr+1 − v̄] ≤ σ3 ( E‖zr − z̄‖2 + E‖zr+1 − zr‖2 +\nN∑ i=1 E‖1/N∇gi(zr)− 1/N∇gi(yr−1i )‖ 2\n) ,\nwhich completes the proof of Step 4. In summary, we have shown that Step 1 - 4 all hold true. Therefore we have shown that the NESTT-G converges Q-linearly. Q.E.D.\n5.5 Some Key Properties of NESTT-E\nTo facilitate the following derivation, in this section we collect some key properties of NESTT-E. First, for i = ir, using the optimality condition for xi update step (18) we have the following identity:\n1\nN ∇gir(xr+1ir ) + λ r ir + αirηir(x r+1 ir − zr+1) = 0. (70)\nCombined with the dual variable update step (19) we obtain\n1\nN ∇gir(xr+1ir ) = −λ r+1 ir . (71)\nSecond, the optimality condition for the z-update is given by: zr+1 = proxh [ zr+1 −∇z(L(xr, z, λr)− h(z)) ] (72)\n= proxh\n[ zr+1 −\nN∑ i=1 ηi ( zr+1 − xri − λri ηi ) −∇g0(zr+1) ] . (73)\n5.6 Proof of Theorem 3.1\nTo prove this result, we need a few lemmas. For notational simplicity, define new variables {x̂r+1i }, {λ̂ r+1 i } by\nx̂r+1i := arg minxi Ui(xi, z r+1, λri ), λ̂ r+1 i := λ r i + αiηi\n( x̂r+1i − z r+1 ) , ∀i. (74)\nThese variables are the virtual variables generated by updating all variables at iteration r+ 1. Also define:\nLr := L(xr, zr;λr), w := (x, z, λ), β := 1∑N i=1 ηi , ci := L2i αiηiN2 − γi 2 + 1− αi αi Li N\nFirst, we need the following lemma to show that the size of the successive difference of the dual variables can be upper bounded by that of the primal variables. This is a simple consequence of (71); also see [R2, Lemma 2.1]. We include the proof for completeness.\nLemma 5.1. Suppose assumption A holds. Then for NESTT-E algorithm, the following are true:\n‖λr+1i − λ r i ‖2 ≤ L2i N2 ‖xr+1i − x r i ‖2, ‖λ̂r+1i − λ r i ‖2 ≤ L2i N2 ‖x̂r+1i − x r i ‖2, ∀ i. (75a)\nProof. We only show the first inequality. The second one follows an analogous argument. To prove (75a), first note that the case for i 6= ir is trivial, as both sides of (75a) are zero. For the index\nir, we have a closed-form expression for λ r ir\nfollowing (71). Notice that for any given i, the primal-dual pair (xi, λi) is always updated at the same iteration. Therefore, if for each i we choose the initial solutions in a way such that λ0i = −∇gi(x0i ), then we have\n1\nN ∇gi(xr+1i ) = −λ r+1 i ∀ i = 1, 2, · · ·N. (76)\nCombining (76) with Assumption A-(a) yields the following:\n‖λr+1i − λ r i ‖ =\n1\nN ‖∇gi(xr+1i )−∇gi(x r i )‖ ≤ Li N ‖xr+1i − x r i ‖.\nThe proof is complete. Q.E.D.\nSecond, we bound the successive difference of the potential function.\nLemma 5.2. Suppose Assumption A holds true. Then the following holds for NESTT-E\nE[Lr+1 − Lr|xr, zr] ≤ −γz 2 ‖zr+1 − zr‖2 + N∑ i=1 pici‖xri − x̂r+1i ‖ 2. (77)\nProof. First let us split Lr+1 − Lr in the following way:\nLr+1 − Lr = Lr+1 − L(xr+1, zr+1;λr) + L(xr+1, zr+1;λr)− Lr. (78)\nThe first two terms in (78) can be bounded by\nLr+1 − L(xr+1, zr+1;λr) = N∑ i=1 〈λr+1i − λ r i , x r+1 i − z r+1〉\n(a) =\n1\nαirηir ‖λr+1ir − λ r ir‖\n2 (b) ≤ L2ir\nN2αirηir ‖xr+1ir − x r ir‖ 2 (79)\nwhere in (a) we have used (19), and the fact that λr+1i − λri = 0 for all variable blocks except irth block; (b) is true because of Lemma 5.1.\nThe last two terms in (78) can be written in the following way:\nL({xr+1i }, z r+1;λr)− Lr\n= L(xr+1, zr+1;λr)− L(xr, zr+1;λr) + L(xr, zr+1;λr)− Lr. (80)\nThe first two terms in (80) characterizes the change of the Augmented Lagrangian before and after the update of x. Note that x updates do not directly optimize the augmented Lagrangian. Therefore the characterization of this step is a bit involved. We have the following:\nL(xr+1, zr+1;λr)− L(xr, zr+1;λr) (a)\n≤ N∑ i=1 (〈 ∇iL(xr+1, zr+1;λr), xr+1i − x r i 〉 − γi 2 ‖xr+1i − x r i ‖2 )\n(b) = 〈 ∇irL(xr+1, zr+1;λr), xr+1ir − x r ir 〉 − γir\n2 ‖xr+1ir − x r ir‖ 2\n(c) = 〈 ηir(1− αir)(xr+1ir − z r+1), xr+1ir − x r ir 〉 − γir\n2 ‖xr+1ir − x r ir‖ 2\n(d) = 〈 1− αir αir (λr+1ir − λ r ir), x r+1 ir − xrir 〉 − γir 2 ‖xr+1ir − x r ir‖ 2 ≤ 1− αir αir ( 1 2Lir/N ‖λr+1ir − λ r ir‖ 2 + Lir 2N ‖xr+1ir − x r ir‖ 2 ) − γir 2 ‖xr+1ir − x r ir‖ 2 (e)\n≤ 1− αir αir Lir N ‖xr+1ir − x r ir‖ 2 − γir 2 ‖xr+1ir − x r ir‖ 2 (81)\nwhere\n• (a) is true because L(x, z, λ) is strongly convex with respect to xi.\n• (b) is true because when i 6= ir, we have xr+1i = xri .\n• (c) is true because xr+1ir is optimal solution for the problem minUir(xir , z r+1, λrir) (satisfying (70)),\nand we have used the optimality of such xr+1ir .\n• (d) and (e) are due to Lemma 5.1.\nSimilarly, the last two terms in (80) can be bounded using equation (70) and the strong convexity of function L with respect to the variable z. Therefor We have:\nL(xr, zr+1, λr)− Lr ≤ −γz 2 ‖zr+1 − zr‖2. (82)\nCombining equations (79), (81) and (82), eventually we have:\nLr+1 − L(xr, zr+1, λr) ≤ cir‖xrir − x r+1 ir ‖2 (83) Lr+1 − Lr ≤ −γz 2 ‖zr+1 − zr‖2 + cir‖xrir − x r+1 ir ‖2 (84)\nTaking expectation on both side of this inequality with respect to ir, we can conclude that:\nE[Lr+1 − Lr | zr, xr] ≤ −γz 2 ‖zr+1 − zr‖2 + N∑ i=1 pici‖xri − x̂r+1i ‖ 2 (85)\nwhere pi is the probability of picking ith block. The lemma is proved. Q.E.D.\nLemma 5.3. Suppose that Assumption A is satisfied, then Lr ≥ f .\nProof. Using the definition of the augmented Lagrangian function we have:\nLr+1 = N∑ i=1 ( 1 N gi(x r+1 i ) + 〈λ r+1 i , x r+1 i − z r+1〉+ ηi 2 ‖xr+1i − z r+1‖2 ) + g0(z r+1) + p(zr+1)\n(a) = N∑ i=1 ( 1 N gi(x r+1 i ) + 1 N 〈∇gi(xr+1i ), z r+1 − xr+1i 〉+ ηi 2 ‖xr+1i − z r+1‖2 ) + g0(z r+1) + p(zr+1)\n(b) ≥ N∑ i=1 1 N gi(z r+1) + ( ηi 2 − Li 2N ) ‖zr+1 − xr+1i ‖ 2 + g0(z r+1) + p(zr+1)\n(c) ≥ N∑ i=1 1 N gi(z r+1) + g0(z r+1) + p(zr+1) ≥ f (86)\nwhere (a) is true because of equation (71); (b) follows Assumption A-(b); (c) follows Assumption A-(d). The desired result is proven. Q.E.D.\nProof of Theorem 3.1. We first show that the algorithm converges to the set of stationary solutions, and then establish the convergence rate.\nStep 1. Convergence to Stationary Solutions. Combining the descent estimate in Lemma 5.2 as well as the lower bounded condition in Lemma 5.3, we can again apply the Supermartigale Convergence Theorem [4, Proposition 4.2] and conclude that\n‖xr+1i − x r i ‖ → 0, ‖zr+1 − zr‖ → 0,with probability 1. (87)\nFrom Lemma 5.1 we have that the constraint violation is satisfied\n‖λr+1 − λr‖ → 0, ‖xr+1i − z r‖ → 0. (88)\nThe rest of the proof follows similar lines as in [14, Theorem 2.4]. Due to space limitations we omit the proof.\nStep 2. Convergence Rate. We first show that there exists a σ1(α) > 0 such that\n‖∇̃L(wr)‖2 + N∑ i=1 L2i N2 ‖xri − zr‖2 ≤ σ1(α)\n( ‖zr − zr+1‖2 +\nN∑ i=1 ‖xri − x̂r+1i ‖ 2\n) . (89)\nUsing the definition of ‖∇̃Lr(wr)‖ we have:\n‖∇̃Lr(wr)‖2 = ‖zr − proxh [zr −∇z(Lr − h(zr))] ‖2\n+ N∑ i=1 ∥∥∥∥ 1N∇gi(xri ) + λri + ηi(xri − zr) ∥∥∥∥2 . (90)\nFrom the optimality condition of the z update (73) we have:\nzr+1 = proxh\n[ zr+1 −\nN∑ i=1 ηi ( zr+1 − xri − λri ηi ) −∇g0(zr+1) ] .\nUsing this, the first term in equation (90) can be bounded as:\n‖zr − proxh [zr −∇z(Lr − h(zr))]‖\n= ∥∥∥∥∥zr − zr+1 + zr+1 − proxh [ zr − N∑ i=1 ηi(z r − xri − λri ηi )−∇g0(zr) ]∥∥∥∥∥ ≤ ‖zr − zr+1‖+ ∥∥∥∥∥proxh [ zr+1 − N∑ i=1 ηi ( zr+1 − xri − λri ηi ) −∇g0(zr+1) ]\n− proxh\n[ zr −\nN∑ i=1 ηi(z r − xri − λri ηi )−∇g0(zr) ]∥∥∥∥∥ ≤ 2‖zr+1 − zr‖+\n( N∑ i=1 ηi + L0 ) ‖zr − zr+1‖, (91)\nwhere in the last inequality we have used the nonexpansiveness of the proximity operator. Similarly, the optimality condition of the xi subproblem is given by\n1\nN ∇gi(x̂r+1i ) + λ r i + αiηi(x̂ r+1 i − z r+1) = 0. (92)\nApplying this identity, the second term in equation (90) can be written as follows:\nN∑ i=1 ∥∥∥∥ 1N∇gi(xri ) + λri + ηi(xri − zr) ∥∥∥∥2\n(a) = N∑ i=1 ∥∥∥∥ 1N∇gi(xri )− 1N∇gi(x̂r+1i ) + ηi(xri − zr)− αiηi(x̂r+1i − zr+1) ∥∥∥∥2\n= N∑ i=1 ∥∥∥∥ 1N∇gi(xri )− 1N∇gi(x̂r+1i ) + ηi(xri − x̂r+1i + x̂r+1i − zr+1 + zr+1 − zr)− αiηi(x̂r+1i − zr+1) ∥∥∥∥2\n(b) ≤ 4 N∑ i=1 [( L2i N2 + η2i + (1− αi)2L2i N2α2i ) ‖x̂r+1i − x r i ‖2 + η2i ‖zr+1 − zr‖2 ] , (93)\nwhere (a) holds because of equation (92); (b) holds because of Lemma 5.1. Finally, combining (91) and (93) leads to the following bound for proximal gradient\n‖∇̃Lr‖2 ≤ 4 N∑ i=1 η2i + ( 2 + L0 + N∑ i=1 ηi )2 ‖zr − zr+1‖2 +\nN∑ i=1 4 ( L2i N2 + η2i + (1− αi)2Li N2α2i ) ‖xri − x̂r+1i ‖ 2. (94)\nAlso note that:\nN∑ i=1 L2i N2 ‖xri − zr‖2 ≤ N∑ i=1 3 L2i N2 [ ‖xri − x̂r+1i ‖ 2 + ‖x̂r+1i − z r+1‖2 + ‖zr+1 − zr‖2 ] =\nN∑ i=1 3 L2i N2 [ ‖xri − x̂r+1i ‖ 2 + 1 α2i η 2 i ‖λ̂r+1i − λ r i ‖2 + ‖zr+1 − zr‖2 ]\n≤ N∑ i=1 3 L2i N2 [ ‖xri − x̂r+1i ‖ 2 + L2i α2i η 2 iN 2 ‖x̂r+1i − x r i ‖2 + ‖zr+1 − zr‖2 ] . (95)\nThe two inequalities (94) – (95) imply that:\n‖∇̃Lr‖2 + N∑ i=1 L2i N2 ‖xri − zr‖2\n≤ ( N∑ i=1 4η2i + (2 + N∑ i=1 ηi + L0) 2 + 3 N∑ i=1 L2i N2 ) ‖zr − zr+1‖2\n+ N∑ i=1 ( 4 ( L2i N2 + η2i + ( 1 αi − 1)2 L 2 i N2 ) + 3 ( L4i αiN4η2i + L2i N2 )) ‖xri − x̂r+1i ‖ 2. (96)\nDefine the following quantities:\nσ̂1(α) = max i\n{ 4 ( L2i N2 + η2i + ( 1 αi − 1 )2 L2i N2 ) + 3 ( L4i αiη2iN 4 + L2i N2 )}\nσ̃1 = N∑ i=1 4η2i + (2 + N∑ i=1 ηi + L0) 2 + 3 N∑ i=1 L2i N2 .\nSetting σ1(α) = max(σ̂1(α), σ̃1) > 0, we have\n‖∇̃Lr‖2 + N∑ i=1 L2i N2 ‖xri − zr‖2 ≤ σ1(α)\n( ‖zr − zr+1‖2 +\nN∑ i=1 ‖xri − x̂r+1i ‖ 2\n) . (97)\nFrom Lemma 5.2 we know that\nE[Lr+1 − Lr|zr, xr] ≤ −γz 2 ‖zr+1 − zr‖2 + N∑ i=1 pici‖xri − x̂r+1i ‖ 2 (98)\nNote that γz = ∑N i=1 ηi − L0, then define σ̂2 and σ̃2 as\nσ̂2(α) = max i\n{ pi ( γi 2 − L 2 i αiηiN2 − 1− αi αi Li N )} σ̃2 = ∑N i=1 ηi − L0\n2 .\nWe can set σ2(α) = max(σ̂2(α), σ̃2) to obtain\nE[Lr − Lr+1|xr, zr] ≥ σ2(α) ( N∑ i=1 ‖x̂r+1i − x r i ‖2 + ‖zr+1 − zr‖2 ) . (99)\nCombining (89) and (99) we have\nH(wr) = ‖∇̃Lr‖2 + N∑ i=1 L2i /N‖xri − zr‖2 ≤ σ1(α) σ2(α) E[Lr − Lr+1|F r].\nLet us set C(α) = σ1(α)σ2(α) and take expectation on both side of the above equation to obtain:\nE[H(wr)] ≤ C(α)E[Lr − Lr+1]. (100)\nSumming both sides of the above inequality over r = 1, · · · , R, we obtain: R∑ r=1 E[H(wr)] ≤ C(α)E[L1 − LR+1]. (101)\nUsing the definition of wm = (xm, zm, λm), and following the same line of argument as Theorem (2.1) we eventually conclude that\nE[H(wm)] ≤ C(α)E[L 1 − LR+1] R . (102)\nThe proof is complete. Q.E.D.\n5.7 Proof of Theorem 3.2\nFollowing similar line of proof as Theorem 2.2, we conclude that there exists some finite r̄ > 0 such that for all r > r̄ and v̄ ∈ R such that\nf(z̄r) = v̄, ∀ r ≥ r̄ (103)\nwhere z̄r = arg minz∈Z∗ ‖zr − z‖. Therefore, combining the fact that ‖xr+1 − xr‖ → 0, ‖zr+1 − zr‖ → 0, ‖xr+1i − zr+1‖ → 0 and ‖λr+1 − λr‖ → 0 [cf. (87), (88)], it is easy to see that\nL(z̄r, x̄r, λ̄r) = f(z̄r) = v̄, ∀ r ≥ r̄ (104)\nwhere x̄r, λ̄r are defined similarly as z̄r. In what follows we will show that ∆ := Lr+1 − v̄ decreases Q-linearly. From the optimality condition for z subproblem (73), we know that:\nzr+1 = proxh\n[ zr+1 − ( N∑ i=1 −λri − ηi(zr+1 − xri ) ) −∇g0(zr+1) ] . (105)\nTherefore, ‖∇̃f(zr+1)‖ can be bounded in the following way:\n‖∇̃f(zr+1)‖ = ∥∥∥∥proxh [ zr+1 − ( N∑ i=1 −λri − ηi(zr+1 − xri ) ) −∇g0(zr+1) ]\n− proxh\n( zr+1 −\nN∑ i=1 ∇gi(zr+1)−∇g0(zr+1) )∥∥∥∥ (a) ≤\n∥∥∥∥∥− N∑ i=1 ∇gi(zr+1) + ( N∑ i=1 −λri − ηi(zr+1 − xri ) )∥∥∥∥∥ (b) =\n∥∥∥∥∥ N∑ i=1 [ −∇gi(zr+1) +∇gi(xri )− ηi(zr+1 − xri ) ]∥∥∥∥∥ ≤\nN∑ i=1 (Li/N + ηi)‖zr+1 − xri ‖\n≤ N∑ i=1 (Li/N + ηi) ( ‖zr+1 − x̂r+1i ‖+ ‖x̂ r+1 i − x r i ‖ )\n= N∑ i=1 (Li/N + ηi) ( 1 αiηi ‖λ̂r+1i − λ r i ‖+ ‖x̂r+1i − x r i ‖ )\n(c) ≤ N∑ i=1 (Li/N + ηi) ( Li Nαiηi + 1 ) ‖x̂r+1i − x r i ‖ (106)\nwhere in (a) we have used the fact that poximity operator is nonexpansive, and in (b) we plugged in equation (76); in (c) we have used Lemma 5.1. Therefore, there exists C > 0 such that we can bound the\n‖∇̃f(zr+1)‖2 as the following:\n‖∇̃f(zr+1)‖2 ≤ C ( ‖zr+1 − zr‖2 +\nN∑ i=1 ‖x̂r+1i − x r i ‖2 ) . (107)\nIn what follows we bound L(zr+1, xr, λr)− v̄. Assume that r > r̄, therefore z̄r+1 = arg minz∈Z∗ ‖zr+1− z‖ satisfies f(z̄) = v̄. We have the following\nL(zr+1, xr, λr)− v̄ = N∑ i=1 gi(x r i ) + g0(z r+1) + p(zr+1) + N∑ i=1 〈λri , xri − zr+1〉+ N∑ i=1 ηi 2 ‖xri − zr+1‖2\n− N∑ i=1 gi(z̄ r+1)− p(z̄r+1)− g0(z̄r+1). (108)\nFrom the optimality condition of the z subproblem we know that:\np(zr+1) + g0(z r+1) + N∑ i=1 〈λri , xri − zr+1〉+ N∑ i=1 ηi 2 ‖xri − zr+1‖2\n≤ p(z̄r+1) + g0(z̄r+1) + N∑ i=1 〈λri , xri − z̄r+1〉+ N∑ i=1 ηi 2 ‖xri − z̄r+1‖2. (109)\nRearranging the terms in the previous equation we have:\np(zr+1) + g0(z r+1)− p(z̄r+1)− g0(z̄r+1)\n≤ N∑ i=1 [ 〈λri , zr+1 − z̄r+1〉+ ηi 2 ‖xri − z̄r+1‖2 − ηi 2 ‖xri − zr+1‖2 ] . (110)\nPlugging in (110) in (108) yields the following:\nL(zr+1, xr, λr)− v̄ ≤ N∑ i=1 [ 1 N (gi(x r i )− gi(z̄r+1)) + ηi 2 ‖xri − z̄r+1‖2 + 〈λri , xri − z̄r+1〉 ] (a) =\nN∑ i=1 [ 1 N 〈∇gi(x̃i), xri − z̄r+1〉+ ηi 2 ‖xri − z̄r+1‖2 + 〈λri , xri − z̄r+1〉 ] (b) =\nN∑ i=1 [ 1 N 〈∇gi(x̃i)−∇gi(xri ), xri − z̄r+1〉+ ηi 2 ‖xri − z̄r+1‖2 ] (c)\n≤ N∑ i=1 ( Li/N 2 + ηi 2 ) ‖xri − z̄r+1‖2\n≤ N∑ i=1 3(Li/N + ηi) [ ‖xri − x̂r+1i ‖ 2 + ‖x̂r+1i − z r+1‖2 + ‖zr+1 − z̄r+1‖2 ] ≤\nN∑ i=1 3(Li/N 2 + ηi) [( 1 + L2i α2i η 2 iN 2 ) ‖xri − x̂r+1i ‖ 2 + ‖zr+1 − z̄r+1‖2 ] . (111)\nIn the above series of inequalities, (a) is the result of applying mean value theorem on function gi, where x̃i is a point lies in between the line segment (x r i , z̄\nr+1); In (b) we have used (76); (c) is true because of assumption A-(c) and the fact that ‖x̃i − z̄‖ ≤ ‖xri − z̄r+1‖; the last inequality comes from Lemma 5.1.\nOverall, there exists σ2 > 0 such that\nL(zr+1, xr, λr)− v̄ ≤ σ2 ( N∑ i=1 ‖x̂r+1i − x r i ‖2 + ‖zr+1 − z̄r+1‖2 ) ∀ r ≥ r̄. (112)\nFrom Lemma 2.2, we know that\ndist (zr+1, Z∗) = ‖zr+1 − z̄r+1‖ ≤ τ‖∇̃f(zr+1)‖. (113)\nAs a result we have: ∆r+1 = Lr+1 − v̄ ≤ L(zr+1, xr, λr)− v̄,\nwhere the inequality comes from the fact that Lr+1 − L(zr+1, xr, λr) ≤ 0 [by (83) and the fact that ci < 0 for all i]. Considering the above equations we obtain:\n∆r+1 ≤ σ2 ( N∑ i=1 ‖x̂r+1i − x r i ‖2 + ‖zr+1 − z̄r+1‖2 ) (113)\n≤ σ2 N∑ i=1 ‖x̂r+1i − x r i ‖2 + σ2τ‖∇̃f(zr+1)‖2\n(107) ≤ (σ2 + σ2τC) N∑ i=1 ‖x̂r+1i − x r i ‖2 + σ2τC‖zr+1 − zr‖2, ∀ r ≥ r̄ (114)\nFrom (99) and (114) we can further construct a σ3 > 0 such that E[∆r+1] ≤ σ3E[Lr−Lr+1], which further implies the following relationship:\nE[∆r+1] ≤ σ3 1 + σ3 E[∆r], ∀ r ≥ r̄. (115)\nLet us set ρ = σ31+σ3 < 1. Thus concluding the proof. Q.E.D.\n5.8 Proof of Proposition 4.1\nApplying the optimality condition on z subproblem in (34) we have:\nzr+1 = argmin z\nh(z) + g0(z) + β\n2 ‖z − ur+1‖2 (116)\nwhere the variable ur+1 is given by (cf. (31))\nur+1 = β N∑ i=1 (λri + ηix r+1 i ). (117)\nNow from one of the key properties of NESTT-G [cf. Section 5.1, equation (30)], we have that\nur+1 = zr − β\n( 1\nNαir\n( ∇gir(zr)−∇gir(yr−1ir ) ) + 1\nN N∑ i=1 ∇gi(yr−1i )N\n) . (118)\nThis verifies the claim. Q.E.D.\nReferences\n[1] Z. A.-Zhu and E. Hazan. Variance reduction for faster non-convex optimization. 2016. Preprint, available on arXiv, arXiv:1603.05643.\n[2] A. Antoniadis, I. Gijbels, and M. Nikolova. Penalized likelihood regression for generalized linear models with non-quadratic penalties. Annals of the Institute of Statistical Mathematics, 63(3):585–615, 2009.\n[3] D. Bertsekas. Incremental gradient, subgradient, and proximal methods f or convex optimization: A survey. 2000. LIDS Report 2848.\n[4] D. P. Bertsekas and J. N. Tsitsiklis. Parallel and Distributed Computation: Numerical Methods, 2nd ed. Athena Scientific, Belmont, MA, 1997.\n[5] E. Bjornson and E. Jorswieck. Optimal resource allocation in coordinated multi-cell systems. Foundations and Trends in Communications and Information Theory, 9, 2013.\n[6] D. Blatt, A. O. Hero, and H. Gauchman. A convergent incremental gradient method with a constant step size. SIAM Journal on Optimization, 18(1):29–51, 2007.\n[7] S. Boyd, N. Parikh, E. Chu, B. Peleato, and J. Eckstein. Distributed optimization and statistical learning via the alternating direction method of multipliers. Foundations and Trends in Machine Learning, 3(1):1–122, 2011.\n[8] V. Cevher, S. Becker, and M. Schmidt. Convex optimization for big data: Scalable, randomized, and parallel algorithms for big data analytics. IEEE Signal Processing Magazine, 31(5):32–43, Sept 2014.\n[9] T.-H. Chang, M. Hong, and X. Wang. Multi-agent distributed optimization via inexact consensus admm. IEEE Transactions on Signal Processing, 63(2):482–497, Jan 2015.\n[10] A. Defazio, F. Bach, and S. Lacoste-Julien. Saga: A fast incremental gradient method with support for non-strongly convex composite objectives. In The Proceeding of NIPS, 2014.\n[11] S. Ghadimi and G. Lan. Stochastic first- and zeroth-order methods for nonconvx stochastic programming. SIAM Journal on Optimizatnoi, 23(4):2341–2368, 2013.\n[12] D. Hajinezhad, T.-H. Chang, X. Wang, Q. Shi, and M. Hong. Nonnegative matrix factorization using admm: Algorithm analysis and convergence guarantees. In the Proceedings of ICASSP 2016, 2016.\n[13] D. Hajinezhad and M. Hong. Nonconvex alternating direction method of multipliers for distributed sparse principal component analysis. In the Proceedings of GlobalSIPT, 2015.\n[14] M. Hong, Z.-Q. Luo, and M. Razaviyayn. Convergence analysis of alternating direction method of multipliers for a family of nonconvex problems. SIAM Journal On Optimization, 26(1):337–364, 2016.\n[15] R. Johnson and T. Zhang. Accelerating stochastic gradient descent using predictive variance reduction. In the Proceedings of the Neural Information Processing (NIPS). 2013.\n[16] H. Karimi and M. Schmidt. Linear convergence of proximal-gradient methods under the polyaklojasiewicz condition. In NIPS workshop on optimization, 2016.\n[17] G. Lan. An optimal randomized incremental gradient method. 2015. Preprint.\n[18] P.-L. Loh and M. Wainwright. High-dimensional regression with noisy and missing data: Provable guarantees with nonconvexity. The Annals of Statistics, 40(3):1637–1664, 2012.\n[19] P. D. Lorenzo and G. Scutari. Next: In-network nonconvex optimization. 2016. Preprint.\n[20] Z.-Q. Luo and P. Tseng. Error bounds and the convergence analysis of matrix splitting algorithms for the affine variational inequality problem. SIAM Journal on Optimization, pages 43–54, 1992.\n[21] Z.-Q. Luo and P. Tseng. On the linear convergence of descent methods for convex essentially smooth minimization. SIAM Journal on Control and Optimization, 30(2):408–425, 1992.\n[22] S. N. Negahban, P. Ravikumar, M. J. Wainwright, and B. Yu. A unified framework for highdimensional analysis of m-estimators with decomposable regularizers. Statist. Sci., 27(4):538–557, 11 2012.\n[23] Y. Nesterov. Introductory lectures on convex optimization: A basic course. Springer, 2004.\n[24] M. Razaviyayn, M. Hong, Z.-Q. Luo, and J. S. Pang. Parallel successive convex approximation for nonsmooth nonconvex optimization. In the Proceedings of NIPS, 2014.\n[25] S. J. Reddi, S. Sra, B. Poczos, and A. Smola. Fast incremental method for nonconvex optimization. 2016. Preprint, available on arXiv: arXiv:1603.06159.\n[26] M. Schmidt, N. L. Roux, and F. Bach. Minimizing finite sums with the stochastic average gradient. 2013. Technical report, INRIA.\n[27] S. Shalev-Shwartz and T. Zhang. Proximal stochastic dual coordinate ascent methods for regularzied loss minimization. Journal of Machine Learning Rsearch, 14:567–599, 2013.\n[28] S. Sra. Scalable nonconvex inexact proximal splitting. In Advances in Neural Information Processing Systems (NIPS), 2012.\n[29] P. Tseng and S. Yun. A coordinate gradient descent method for nonsmooth separable minimization. Mathematical Programming, 117:387–423, 2009.\n[30] Z. Wang, H. Liu, and T. Zhang. Optimal computational and statistical rates of convergence for sparse nonconvex learning problems. Annals of Statistics, 42(6):2164–2201, 2014.\n[31] S. Zlobec. On the Liu - Floudas convexification of smooth programs. Journal of Global Optimization, 32:401 – 407, 2005."
    } ],
    "references" : [ {
      "title" : "Variance reduction for faster non-convex optimization. 2016",
      "author" : [ "Z.A.-Zhu", "E. Hazan" ],
      "venue" : "Preprint, available on arXiv,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2016
    }, {
      "title" : "Penalized likelihood regression for generalized linear models with non-quadratic penalties",
      "author" : [ "A. Antoniadis", "I. Gijbels", "M. Nikolova" ],
      "venue" : "Annals of the Institute of Statistical Mathematics,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2009
    }, {
      "title" : "Incremental gradient, subgradient, and proximal methods f or convex optimization: A survey",
      "author" : [ "D. Bertsekas" ],
      "venue" : "LIDS Report",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2000
    }, {
      "title" : "Parallel and Distributed Computation: Numerical Methods, 2nd ed",
      "author" : [ "D.P. Bertsekas", "J.N. Tsitsiklis" ],
      "venue" : "Athena Scientific,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 1997
    }, {
      "title" : "Optimal resource allocation in coordinated multi-cell systems",
      "author" : [ "E. Bjornson", "E. Jorswieck" ],
      "venue" : "Foundations and Trends in Communications and Information Theory,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2013
    }, {
      "title" : "A convergent incremental gradient method with a constant step size",
      "author" : [ "D. Blatt", "A.O. Hero", "H. Gauchman" ],
      "venue" : "SIAM Journal on Optimization,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2007
    }, {
      "title" : "Distributed optimization and statistical learning via the alternating direction method of multipliers",
      "author" : [ "S. Boyd", "N. Parikh", "E. Chu", "B. Peleato", "J. Eckstein" ],
      "venue" : "Foundations and Trends in Machine Learning,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2011
    }, {
      "title" : "Convex optimization for big data: Scalable, randomized, and parallel algorithms for big data analytics",
      "author" : [ "V. Cevher", "S. Becker", "M. Schmidt" ],
      "venue" : "IEEE Signal Processing Magazine,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2014
    }, {
      "title" : "Multi-agent distributed optimization via inexact consensus admm",
      "author" : [ "T.-H. Chang", "M. Hong", "X. Wang" ],
      "venue" : "IEEE Transactions on Signal Processing,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2015
    }, {
      "title" : "Saga: A fast incremental gradient method with support for non-strongly convex composite objectives",
      "author" : [ "A. Defazio", "F. Bach", "S. Lacoste-Julien" ],
      "venue" : "In The Proceeding of NIPS,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2014
    }, {
      "title" : "Stochastic first- and zeroth-order methods for nonconvx stochastic programming",
      "author" : [ "S. Ghadimi", "G. Lan" ],
      "venue" : "SIAM Journal on Optimizatnoi,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2013
    }, {
      "title" : "Nonnegative matrix factorization using admm: Algorithm analysis and convergence guarantees",
      "author" : [ "D. Hajinezhad", "T.-H. Chang", "X. Wang", "Q. Shi", "M. Hong" ],
      "venue" : "In the Proceedings of ICASSP",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2016
    }, {
      "title" : "Nonconvex alternating direction method of multipliers for distributed sparse principal component analysis",
      "author" : [ "D. Hajinezhad", "M. Hong" ],
      "venue" : "In the Proceedings of GlobalSIPT,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2015
    }, {
      "title" : "Convergence analysis of alternating direction method of multipliers for a family of nonconvex problems",
      "author" : [ "M. Hong", "Z.-Q. Luo", "M. Razaviyayn" ],
      "venue" : "SIAM Journal On Optimization,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2016
    }, {
      "title" : "Accelerating stochastic gradient descent using predictive variance reduction",
      "author" : [ "R. Johnson", "T. Zhang" ],
      "venue" : "In the Proceedings of the Neural Information Processing (NIPS)",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2013
    }, {
      "title" : "Linear convergence of proximal-gradient methods under the polyaklojasiewicz condition",
      "author" : [ "H. Karimi", "M. Schmidt" ],
      "venue" : "In NIPS workshop on optimization,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2016
    }, {
      "title" : "An optimal randomized incremental gradient method",
      "author" : [ "G. Lan" ],
      "venue" : null,
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2015
    }, {
      "title" : "High-dimensional regression with noisy and missing data: Provable guarantees with nonconvexity",
      "author" : [ "P.-L. Loh", "M. Wainwright" ],
      "venue" : "The Annals of Statistics,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2012
    }, {
      "title" : "Next: In-network nonconvex optimization. 2016",
      "author" : [ "P.D. Lorenzo", "G. Scutari" ],
      "venue" : null,
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2016
    }, {
      "title" : "Error bounds and the convergence analysis of matrix splitting algorithms for the affine variational inequality problem",
      "author" : [ "Z.-Q. Luo", "P. Tseng" ],
      "venue" : "SIAM Journal on Optimization,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 1992
    }, {
      "title" : "On the linear convergence of descent methods for convex essentially smooth minimization",
      "author" : [ "Z.-Q. Luo", "P. Tseng" ],
      "venue" : "SIAM Journal on Control and Optimization,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 1992
    }, {
      "title" : "A unified framework for highdimensional analysis of m-estimators with decomposable regularizers",
      "author" : [ "S.N. Negahban", "P. Ravikumar", "M.J. Wainwright", "B. Yu" ],
      "venue" : "Statist. Sci., 27(4):538–557,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2012
    }, {
      "title" : "Introductory lectures on convex optimization: A basic course",
      "author" : [ "Y. Nesterov" ],
      "venue" : null,
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2004
    }, {
      "title" : "Parallel successive convex approximation for nonsmooth nonconvex optimization",
      "author" : [ "M. Razaviyayn", "M. Hong", "Z.-Q. Luo", "J.S. Pang" ],
      "venue" : "In the Proceedings of NIPS,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2014
    }, {
      "title" : "Fast incremental method for nonconvex optimization. 2016",
      "author" : [ "S.J. Reddi", "S. Sra", "B. Poczos", "A. Smola" ],
      "venue" : null,
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2016
    }, {
      "title" : "Minimizing finite sums with the stochastic average gradient",
      "author" : [ "M. Schmidt", "N.L. Roux", "F. Bach" ],
      "venue" : null,
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2013
    }, {
      "title" : "Proximal stochastic dual coordinate ascent methods for regularzied loss minimization",
      "author" : [ "S. Shalev-Shwartz", "T. Zhang" ],
      "venue" : "Journal of Machine Learning Rsearch,",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2013
    }, {
      "title" : "Scalable nonconvex inexact proximal splitting",
      "author" : [ "S. Sra" ],
      "venue" : "In Advances in Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 2012
    }, {
      "title" : "A coordinate gradient descent method for nonsmooth separable minimization",
      "author" : [ "P. Tseng", "S. Yun" ],
      "venue" : "Mathematical Programming,",
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 2009
    }, {
      "title" : "Optimal computational and statistical rates of convergence for sparse nonconvex learning problems",
      "author" : [ "Z. Wang", "H. Liu", "T. Zhang" ],
      "venue" : "Annals of Statistics,",
      "citeRegEx" : "30",
      "shortCiteRegEx" : "30",
      "year" : 2014
    }, {
      "title" : "On the Liu - Floudas convexification of smooth programs",
      "author" : [ "S. Zlobec" ],
      "venue" : "Journal of Global Optimization,",
      "citeRegEx" : "31",
      "shortCiteRegEx" : "31",
      "year" : 2005
    } ],
    "referenceMentions" : [ {
      "referenceID" : 7,
      "context" : "It arises frequently in applications such as machine learning and signal processing; see a recent survey [8].",
      "startOffset" : 105,
      "endOffset" : 108
    }, {
      "referenceID" : 4,
      "context" : "In particular, each smooth functions {gi}i=1 can represent: 1) a minibatch of loss functions modeling data fidelity, such as the `2 loss, the logistic loss, etc; 2) nonconvex activation functions for neural networks, such as the logit or the tanh functions; 3) nonconvex utility functions used in signal processing, machine learning, and resource allocation, see [5], and [12].",
      "startOffset" : 363,
      "endOffset" : 366
    }, {
      "referenceID" : 11,
      "context" : "In particular, each smooth functions {gi}i=1 can represent: 1) a minibatch of loss functions modeling data fidelity, such as the `2 loss, the logistic loss, etc; 2) nonconvex activation functions for neural networks, such as the logit or the tanh functions; 3) nonconvex utility functions used in signal processing, machine learning, and resource allocation, see [5], and [12].",
      "startOffset" : 372,
      "endOffset" : 376
    }, {
      "referenceID" : 1,
      "context" : "The smooth function g0 can represent smooth nonconvex regularizers such as the non-quadratic penalties [2], or the smooth part of the SCAD or MCP regularizers (which is a concave function) [30].",
      "startOffset" : 103,
      "endOffset" : 106
    }, {
      "referenceID" : 29,
      "context" : "The smooth function g0 can represent smooth nonconvex regularizers such as the non-quadratic penalties [2], or the smooth part of the SCAD or MCP regularizers (which is a concave function) [30].",
      "startOffset" : 189,
      "endOffset" : 193
    }, {
      "referenceID" : 6,
      "context" : "Such distributed computation model has been popular in large-scale machine learning and signal processing [7].",
      "startOffset" : 106,
      "endOffset" : 109
    }, {
      "referenceID" : 16,
      "context" : "Such model is also closely related to the (centralized) stochastic finite-sum optimization problem [17, 10, 15, 25, 1, 26], in which each time the iterate is updated based on the gradient information of a random component function.",
      "startOffset" : 99,
      "endOffset" : 122
    }, {
      "referenceID" : 9,
      "context" : "Such model is also closely related to the (centralized) stochastic finite-sum optimization problem [17, 10, 15, 25, 1, 26], in which each time the iterate is updated based on the gradient information of a random component function.",
      "startOffset" : 99,
      "endOffset" : 122
    }, {
      "referenceID" : 14,
      "context" : "Such model is also closely related to the (centralized) stochastic finite-sum optimization problem [17, 10, 15, 25, 1, 26], in which each time the iterate is updated based on the gradient information of a random component function.",
      "startOffset" : 99,
      "endOffset" : 122
    }, {
      "referenceID" : 24,
      "context" : "Such model is also closely related to the (centralized) stochastic finite-sum optimization problem [17, 10, 15, 25, 1, 26], in which each time the iterate is updated based on the gradient information of a random component function.",
      "startOffset" : 99,
      "endOffset" : 122
    }, {
      "referenceID" : 0,
      "context" : "Such model is also closely related to the (centralized) stochastic finite-sum optimization problem [17, 10, 15, 25, 1, 26], in which each time the iterate is updated based on the gradient information of a random component function.",
      "startOffset" : 99,
      "endOffset" : 122
    }, {
      "referenceID" : 25,
      "context" : "Such model is also closely related to the (centralized) stochastic finite-sum optimization problem [17, 10, 15, 25, 1, 26], in which each time the iterate is updated based on the gradient information of a random component function.",
      "startOffset" : 99,
      "endOffset" : 122
    }, {
      "referenceID" : 6,
      "context" : "Note that such splitting scheme has been popular in the convex setting [7], but not so when the problem becomes nonconvex.",
      "startOffset" : 71,
      "endOffset" : 74
    }, {
      "referenceID" : 22,
      "context" : "Compared with the classical gradient descent, which in the worst case requires O( ∑N i=1 Li/ ) gradient evaluation to achieve -stationarity [23], our obtained rate can be up to O(N) times better in the case where the Li’s are not equal.",
      "startOffset" : 140,
      "endOffset" : 144
    }, {
      "referenceID" : 9,
      "context" : "Our work also reveals a fundamental connection between primal-dual based algorithms and the primal only average-gradient based algorithm such as SAGA/SAG/IAG [10, 25, 26, 6].",
      "startOffset" : 158,
      "endOffset" : 173
    }, {
      "referenceID" : 24,
      "context" : "Our work also reveals a fundamental connection between primal-dual based algorithms and the primal only average-gradient based algorithm such as SAGA/SAG/IAG [10, 25, 26, 6].",
      "startOffset" : 158,
      "endOffset" : 173
    }, {
      "referenceID" : 25,
      "context" : "Our work also reveals a fundamental connection between primal-dual based algorithms and the primal only average-gradient based algorithm such as SAGA/SAG/IAG [10, 25, 26, 6].",
      "startOffset" : 158,
      "endOffset" : 173
    }, {
      "referenceID" : 5,
      "context" : "Our work also reveals a fundamental connection between primal-dual based algorithms and the primal only average-gradient based algorithm such as SAGA/SAG/IAG [10, 25, 26, 6].",
      "startOffset" : 158,
      "endOffset" : 173
    }, {
      "referenceID" : 9,
      "context" : "Popular algorithms include the SAG/SAGA [10, 26], the SDCA [27], the SVRG [15], the RPDG [17] and so on.",
      "startOffset" : 40,
      "endOffset" : 48
    }, {
      "referenceID" : 25,
      "context" : "Popular algorithms include the SAG/SAGA [10, 26], the SDCA [27], the SVRG [15], the RPDG [17] and so on.",
      "startOffset" : 40,
      "endOffset" : 48
    }, {
      "referenceID" : 26,
      "context" : "Popular algorithms include the SAG/SAGA [10, 26], the SDCA [27], the SVRG [15], the RPDG [17] and so on.",
      "startOffset" : 59,
      "endOffset" : 63
    }, {
      "referenceID" : 14,
      "context" : "Popular algorithms include the SAG/SAGA [10, 26], the SDCA [27], the SVRG [15], the RPDG [17] and so on.",
      "startOffset" : 74,
      "endOffset" : 78
    }, {
      "referenceID" : 16,
      "context" : "Popular algorithms include the SAG/SAGA [10, 26], the SDCA [27], the SVRG [15], the RPDG [17] and so on.",
      "startOffset" : 89,
      "endOffset" : 93
    }, {
      "referenceID" : 27,
      "context" : "When the problem becomes nonconvex, the well-known incremental based algorithm can be used [28, 3], but these methods generally lack convergence rate guarantees.",
      "startOffset" : 91,
      "endOffset" : 98
    }, {
      "referenceID" : 2,
      "context" : "When the problem becomes nonconvex, the well-known incremental based algorithm can be used [28, 3], but these methods generally lack convergence rate guarantees.",
      "startOffset" : 91,
      "endOffset" : 98
    }, {
      "referenceID" : 10,
      "context" : "The SGD based method has been studied in [11], with O(1/ 2) convergence rate.",
      "startOffset" : 41,
      "endOffset" : 45
    }, {
      "referenceID" : 0,
      "context" : "Recent works [1] and [25] develop algorithms based on SVRG and SAGA for a special case of (1) where the entire problem is smooth and unconstrained.",
      "startOffset" : 13,
      "endOffset" : 16
    }, {
      "referenceID" : 24,
      "context" : "Recent works [1] and [25] develop algorithms based on SVRG and SAGA for a special case of (1) where the entire problem is smooth and unconstrained.",
      "startOffset" : 21,
      "endOffset" : 25
    }, {
      "referenceID" : 13,
      "context" : "On the other hand, distributed stochastic algorithms for solving problem (1) in the nonconvex setting has been proposed in [14], [13], in which each time a randomly picked subset of agents update their local variables.",
      "startOffset" : 123,
      "endOffset" : 127
    }, {
      "referenceID" : 12,
      "context" : "On the other hand, distributed stochastic algorithms for solving problem (1) in the nonconvex setting has been proposed in [14], [13], in which each time a randomly picked subset of agents update their local variables.",
      "startOffset" : 129,
      "endOffset" : 133
    }, {
      "referenceID" : 18,
      "context" : "There has been some recent distributed algorithms designed for (1) [19], but again without global convergence rate guarantee.",
      "startOffset" : 67,
      "endOffset" : 71
    }, {
      "referenceID" : 23,
      "context" : "It can be checked that the pGRAD vanishes at the set of stationary solutions of (1) [24].",
      "startOffset" : 84,
      "endOffset" : 88
    }, {
      "referenceID" : 6,
      "context" : "We remark that NESTT-G is related to the popular ADMM method for convex optimization [7].",
      "startOffset" : 85,
      "endOffset" : 88
    }, {
      "referenceID" : 17,
      "context" : "In this section we show that the NESTT-G is capable of linear convergence for a family of nonconvex quadratic problems, which has important applications, for example in high-dimensional statistical learning [18].",
      "startOffset" : 207,
      "endOffset" : 211
    }, {
      "referenceID" : 20,
      "context" : "Our linear convergence result is based upon certain error bound condition around the stationary solutions set, which has been shown in [21] for smooth quadratic problems and has been extended to including `1 penalty in [29, Theorem 4].",
      "startOffset" : 135,
      "endOffset" : 139
    }, {
      "referenceID" : 15,
      "context" : "There has been some recent works showing linear convergence for nonconvex problems satisfying certain quadratic growth condition [16, 25, 1].",
      "startOffset" : 129,
      "endOffset" : 140
    }, {
      "referenceID" : 24,
      "context" : "There has been some recent works showing linear convergence for nonconvex problems satisfying certain quadratic growth condition [16, 25, 1].",
      "startOffset" : 129,
      "endOffset" : 140
    }, {
      "referenceID" : 0,
      "context" : "There has been some recent works showing linear convergence for nonconvex problems satisfying certain quadratic growth condition [16, 25, 1].",
      "startOffset" : 129,
      "endOffset" : 140
    }, {
      "referenceID" : 15,
      "context" : "However the problems considered in [16, 25, 1] are smooth unconstrained problems, and every stationary point is a global minimum, therefore they do no cover our nonconvex quadratic problems, whose stationary solutions are not global minimizers.",
      "startOffset" : 35,
      "endOffset" : 46
    }, {
      "referenceID" : 24,
      "context" : "However the problems considered in [16, 25, 1] are smooth unconstrained problems, and every stationary point is a global minimum, therefore they do no cover our nonconvex quadratic problems, whose stationary solutions are not global minimizers.",
      "startOffset" : 35,
      "endOffset" : 46
    }, {
      "referenceID" : 0,
      "context" : "However the problems considered in [16, 25, 1] are smooth unconstrained problems, and every stationary point is a global minimum, therefore they do no cover our nonconvex quadratic problems, whose stationary solutions are not global minimizers.",
      "startOffset" : 35,
      "endOffset" : 46
    }, {
      "referenceID" : 28,
      "context" : "To proceed, A sequence {x} is said to converge Q-linearly to some x̄ if lim supr ‖x − x̄‖/‖x − x̄‖ ≤ ρ, where ρ ∈ (0, 1) is some constant; cf [29] and references therein.",
      "startOffset" : 142,
      "endOffset" : 146
    }, {
      "referenceID" : 13,
      "context" : "(21) We define the optimality gap by adding to ‖∇̃L(w)‖2 the size of the constraint violation [14]: H(w) := ‖∇̃L(w)‖ + N ∑",
      "startOffset" : 94,
      "endOffset" : 98
    }, {
      "referenceID" : 25,
      "context" : "Then (23) takes the same form as the SAG presented in [26].",
      "startOffset" : 54,
      "endOffset" : 58
    }, {
      "referenceID" : 5,
      "context" : "Further, when the component functions gi’s are picked cyclically in a Gauss-Seidel manner, the iteration (23) takes the same form as the IAG algorithm [6].",
      "startOffset" : 151,
      "endOffset" : 154
    }, {
      "referenceID" : 9,
      "context" : "Then (23) is the same as the SAGA algorithm [10], which is design for optimizing convex nonsmooth finite sum problems.",
      "startOffset" : 44,
      "endOffset" : 48
    }, {
      "referenceID" : 24,
      "context" : "Thirdly, we note that a recent paper [25] has shown that SAGA works for smooth and unconstrained nonconvex problem.",
      "startOffset" : 37,
      "endOffset" : 41
    }, {
      "referenceID" : 24,
      "context" : "Compared with GD, which achieves -stationarity using O( ∑N i=1 Li/ ) gradient evaluations in the worse case (in the sense that ∑N i=1 Li/N = L), the rate in [25] is O(N1/3) times better.",
      "startOffset" : 157,
      "endOffset" : 161
    }, {
      "referenceID" : 24,
      "context" : "However, the algorithm in [25] is different from NESTT-G in two aspects: 1) it does not generalize to the nonsmooth constrained problem (1); 2) it samples two component functions at each iteration, while NESTT-G only samples once.",
      "startOffset" : 26,
      "endOffset" : 30
    }, {
      "referenceID" : 21,
      "context" : "For example in LASSO problem the data matrix is often normalized by feature (or “column-normalized” [22]), therefore the `2 norm of each row of the data matrix (which corresponds to the Lipschitz constant for each component function) can be dramatically different.",
      "startOffset" : 100,
      "endOffset" : 104
    }, {
      "referenceID" : 17,
      "context" : "Consider the high dimensional regression problem with noisy observation [18], where M observations are generated by y = Xν + .",
      "startOffset" : 72,
      "endOffset" : 76
    }, {
      "referenceID" : 17,
      "context" : "To test the performance of the proposed algorithm, we generate the problem following similar setups as [18].",
      "startOffset" : 103,
      "endOffset" : 107
    }, {
      "referenceID" : 24,
      "context" : "We implement NESTT-G/E, the SGD, and the nonconvex SAGA proposed in [25] with stepsize β = 1 3LmaxN (with Lmax := maxi Li).",
      "startOffset" : 68,
      "endOffset" : 72
    }, {
      "referenceID" : 24,
      "context" : "Note that the SAGA proposed in [25] only works for the unconstrained problems with uniform Li, therefore when applied to (24) it is not guaranteed to converge.",
      "startOffset" : 31,
      "endOffset" : 35
    } ],
    "year" : 2016,
    "abstractText" : "We study a stochastic and distributed algorithm for nonconvex problems whose objective consists of a sum of N nonconvex Li/N -smooth functions, plus a nonsmooth regularizer. The proposed NonconvEx primal-dual SpliTTing (NESTT) algorithm splits the problem into N subproblems, and utilizes an augmented Lagrangian based primal-dual scheme to solve it in a distributed and stochastic manner. With a special non-uniform sampling, a version of NESTT achieves -stationary solution using O(( ∑N i=1 √ Li/N) / ) gradient evaluations, which can be up to O(N) times better than the (proximal) gradient descent methods. It also achieves Q-linear convergence rate for nonconvex `1 penalized quadratic problems with polyhedral constraints. Further, we reveal a fundamental connection between primal-dual based methods and a few primal only methods such as IAG/SAG/SAGA.",
    "creator" : "LaTeX with hyperref package"
  }
}