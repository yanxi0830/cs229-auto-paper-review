{
  "name" : "1605.06296.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "On the Robustness of Decision Tree Learning under Label Noise",
    "authors" : [ "Aritra Ghosh", "Naresh Manwani" ],
    "emails" : [ "aritraghosh.iem@gmail.com", "nareshmanwani@gmail.com", "sastry@ee.iisc.ernet.in" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n60 5.\n06 29\n6v 2\n[ cs\n.L G\n] 2\nKeywords: Robust learning, Decision trees, Label noise"
    }, {
      "heading" : "1. Introduction",
      "text" : "Decision tree is among the most widely used machine learning approaches (Wu et al., 2007). Interpretability, applicability to all types of features, less demands on data pre-processing and scalability are some of the reasons for its popularity. In general, decision tree is learnt in a top down greedy fashion where, at each node, a split rule is learnt by minimizing some objective function.\nFor learning a decision tree classifier, we make use of labeled training data. When the class labels in the training data may be incorrect, it is referred to as label noise. Subjectivity and other errors in human labeling, measurement errors, insufficient feature space are some of the main reasons behind label noise. In many large data problems, labeled samples are often obtained through crowd sourcing and the unreliability of such labels is another reason for label noise. Learning from positive and unlabeled samples can also be cast as a problem of learning under label noise (du Plessis et al., 2014). Thus, learning classifiers in the presence of label noise is an important problem (Frénay and Verleysen, 2014). It is generally accepted that among all the classification methods, decision tree is probably closest to ‘off-the-shelf’ method which has all the desirable properties including robustness to outliers (Hastie et al., 2005).\nWhile there are many results about generalization bounds for decision trees (Mansour and McAllester, 2000; Kearns and Mansour, 1998), not many theoretical results are known about the robustness of decision tree learning in presence of label noise. It is observed that label noise in\nc© A. Ghosh, N. Manwani & P.S. Sastry.\nthe training data increases size of the learnt tree; detecting and removing noisy examples improves the learnt tree (Brodley and Friedl, 1999). Recently, Nettleton et al. (2010) empirically studied robustness of different classifiers under label noise. While decision tree learning is better than SVM or logistic regression in terms of robustness to label noise, it is also seen that naive Bayes is more robust than decision trees. In this paper, we present a theoretical study of such robustness properties of decision trees.\nRecently, many analytical results are reported on robust learning of classifiers, using the framework of risk minimization. The robustness or noise tolerance of risk minimization depends on the loss function used. Long and Servedio (2010) proved that any convex potential loss is not robust to uniform or symmetric label noise. Another result is that some of the standard convex losses are not robust to symmetric label noise while the 0-1 loss is (Manwani and Sastry, 2013). It is noted by du Plessis et al. (2014) that convex surrogates losses are not good for learning from positive and unlabeled data. A general sufficient condition on the loss function for risk minimization to be robust is derived in (Ghosh et al., 2015). The 0-1 loss, sigmoid loss and ramp loss are shown to satisfy this condition while convex losses such as hinge loss (used in SVM) and the logistic loss do not satisfy this condition. Interestingly, it is possible to have a convex loss (which is not a convex potential) that satisfies this sufficient condition and the corresponding risk minimization essentially amounts to a highly regularized SVM (van Rooyen et al., 2015). Robust risk minimization strategies under the so called class-conditional (or asymmetric) label noise are also proposed (Natarajan et al., 2013; Scott et al., 2013). Some sufficient conditions for robustness of risk minimization under 0-1 loss, ramp loss and sigmoid loss when the training data is corrupted with most general non-uniform label noise are also presented in (Ghosh et al., 2015). None of these results are applicable for decision trees because the popular decision tree learning algorithms cannot be cast as risk minimization.\nIn this paper, we analyze learning of decision trees under label noise. We consider some of the popular impurity function based methods for learning of decision trees. We show, in the large sample limit, that under symmetric or uniform label noise the split rule that optimizes the objective function under noisy data is the same as that under noise-free data. We explain how this results in the learning algorithm being robust to label noise, under the assumption that the number of samples at every node is large. We also derive some sample complexity bounds to indicate how large a sample we need at a node. We also explain how these results indicate robustness of random forest also. We present empirical results to show that trees learnt with noisy data give accuracies that are comparable with those learnt with noise-free data. We also show empirically that the random forests algorithm is robust to label noise. For comparison we also present results obtained with SVM algorithm."
    }, {
      "heading" : "2. Label Noise and Decision Tree Robustness",
      "text" : "In this paper, we only consider binary decision trees for binary classification. We use the same notion of noise tolerance as in (Manwani and Sastry, 2013; van Rooyen et al., 2015)."
    }, {
      "heading" : "2.1. Label Noise",
      "text" : "Let X ⊂ Rd be the feature space and let Y = {1,−1} be the class labels. Let S = {(x1, yx1), (x2, yx2), . . . , (xN , yxN )} ∈ (X ×Y)N be the ideal noise-free data drawn iid from\na fixed but unknown distribution D over X×Y. The learning algorithm does not have access to this data. The noisy training data given to the algorithm is Sη = {(xi, ỹxi), i = 1, · · · , N}, where ỹxi = yxi with probability (1 − ηxi) and ỹxi = −yxi with probability ηxi . As a notation, for any x, yx denotes its ‘true’ label while ỹx denotes the noisy label. Thus, ηx = Pr[yx 6= ỹx | x]. We use Dη to denote the joint probability distribution of x and ỹx.\nWe say that the noise is uniform or symmetric if ηx = η, ∀x. Note that, under symmetric noise, a sample having wrong label is independent of the feature vector and the ‘true’ class of the sample. Noise is said to be class conditional or asymmetric if ηx = η+, for all patterns of class +1 and ηx = η−, for all patterns of class −1. When noise rate ηx is a general function of x, it is termed as non-uniform noise. Note that the value of η is unknown to the learning algorithm."
    }, {
      "heading" : "2.2. Criteria for Learning Split Rule at a Node of Decision Trees",
      "text" : "Most decision tree learning algorithms grow the tree in top down fashion starting with all training data at the root node. At any node, the algorithm selects a split rule to optimize a criterion and uses that split rule to split the data into the left and right children of this node; then the same process is recursively applied to the children nodes till the node satisfies the criterion to become a leaf. Let F denote a set of split rules. Suppose, a split rule f ∈ F at a node v, sends a fraction a of the samples at v to the left child vl and the remaining fraction (1− a) to the right child vr. Then many algorithms select a f ∈ F to maximize a criterion C(f) = G(v)− (aG(vl) + (1− a)G(vr)) (1) where G(·) is a so called impurity measure. There are many such impurity measures. Of the samples at any node v, suppose a fraction p are of positive class and a fraction q = (1− p) are of negative class. Then the gini impurity is defined by GGini = 2pq (Breiman et al., 1984); entropy based impurity is defined as GEntropy = −p log p− q log q (Quinlan, 1986); and misclassification impurity is defined as GMC = min{p, q}. Often the criterion C is called the gain. Hence, we also use gainGini(f) to refer to C(f) when G is GGini and similarly for other impurity measures.\nA split criterion different from impurity is twoing rule, first proposed by Breiman et al. (1984). Consider a split rule f at a node v. Let pl (pr), ql (qr) be the fraction of positive and negative class samples at the left (right) child vl (vr). (We have, apl + (1 − a)pr = p, aql + (1 − a)qr = q, p and q are the fractions for parent node v). Then twoing rule selects f ∈ F which maximizes GTwoing(f) = a(1− a)[|pl − pr|+ |ql − qr|]2/4."
    }, {
      "heading" : "2.3. Noise Tolerance of Decision Tree",
      "text" : "By noise tolerance we desire the following. A decision tree learnt with noisy labels in training data should have the same test error (on noise-free test set) as that of the tree learnt using noise-free training data. One way of achieving such robustness is if the decision tree learning algorithm learns the same tree in presence of label noise as it would learn with noise free data.1 Since label noise is random, on any specific noise-corrupted training data, the tree learnt would also be random. Hence, we say the learning method is robust if, in\n1. For simplicity, we do not consider pruning of the tree.\nthe limit as training set size goes to infinity, the algorithm learns the same tree with noisy as well as noise-free training data. We then argue that this implies we learn the same tree (with a high probability) if given sufficient number of samples. We also provide sample complexity results for this. Below, we formalize this notion.\nDefinition 1 A split criterion C is said to be noise-tolerant if\nargmin f∈F C(f) = argmin f∈F\nCη(f)\nwhere C(f) is the value of the split criterion C for a split rule f ∈ F on noise free data and Cη(f) is the value of the criterion function for f on noisy data, in the limit as the data size goes to infinity.\nLet the decision tree learnt from training sample S be represented as LearnTree(S) and let the classification of any x by this tree be represented as LearnTree(S)(x).\nDefinition 2 A decision tree learning algorithm LearnTree is said to be noise-tolerant if the probability of misclassification, under the noise-free distribution, of the tree learnt with noisy samples is same as that learnt with noise-free samples. That is,\nPD(LearnTree(S)(x) 6= yx) = PD(LearnTree(Sη)(x) 6= yx)\nNote that for the above to hold it is sufficient if LearnTree(S) is same as LearnTree(Sη)."
    }, {
      "heading" : "3. Theoretical Results",
      "text" : "Robustness of decision tree learning requires the robustness of the split criterion at each non-leaf node and robustness of the labeling rule at each leaf node. We consider each of these in turn."
    }, {
      "heading" : "3.1. Robustness Of Split Rules",
      "text" : "As mentioned earlier, most decision tree algorithms select a split rule, f , by maximizing C(f) defined by (1). Hence we are interested in comparing, for any specific f , the value of C(f) with its value, in the large sample limit, when labels are flipped under symmetric label noise.\nLet the noise-free samples at a node v be {(xi, yi), i = 1, · · · , n}. Under label noise, the samples at this node would become {(xi, ỹi), i = 1, · · · , n}. Suppose in the noise-free case a split rule f sends nl of these n samples to the left child, vl, and nr = n− nl to right child, vr. Note that a split rule is a function of only the feature vector. (For example, in an oblique decision tree the split rule could be: send a x to left child if wTx+ w0 > 0). Since the split rule depends only on the feature vector x and not the labels, the points that go to vl and vr would be the same for the noisy samples also. Thus, nl and a = nl/n would be same in both cases. However, what changes with label noise are the class labels on examples and hence the number of examples of different classes at a node.\nLet n+ and n− = n − n+ be the number of samples of the two classes at node v in the noise-free case. Similarly, let n+l and n − l = nl − n+l be the number of samples of the two classes at vl and define n + r , n − r similarly. Let the corresponding quantities in the noisy\ncase be ñ+, ñ−, ñ+l , ñ − l etc. Define random variables, Zi, i = 1, · · · , n by Zi = 1 if ỹi 6= yi and Zi = 0 otherwise. Thus, Zi are indicators of whether or not label on the i th example is corrupted. By definition of symmetric label noise, Zi are iid Bernoulli random variables with expectation η.\nLet p = n+/n, q = n−/n = (1 − p) be the fractions of the two classes at v under noisefree samples. Let pl, ql and pr, qr be these fractions for vl and vr. Let the corresponding quantities for the noisy samples case be p̃, q̃, p̃l, q̃l etc. Let p\nη, qη be the values of p̃, q̃ in the large sample limit and similarly define pηl , q η l ,p η r , q η r .\nThe value of ñ+ is the number of i such that ỹi = +1. Similarly, the value of n + l would\nbe the number of i such that xi is in vl and ỹi = +1. Hence we have\np̃ = ñ+\nn =\n1\nn\n\n\n∑\ni:ỹi=+1\n1\n\n = 1\nn\n\n\n∑\ni:yi=+1\n(1− Zi) + ∑\ni:yi=−1\nZi\n\n (2)\np̃l = ñ+l nl = 1 nl\n\n\n∑\ni:xi∈vl,ỹi=+1\n1\n\n = 1\nnl\n\n\n∑\ni:xi∈vl,yi=+1\n(1− Zi) + ∑\ni:xi∈vl,yi=−1\nZi\n\n (3)\nAll the above expressions involve sums of independent random variables. Hence the values of the above quantities in the large sample limit can be calculated, by laws of large numbers, by essentially replacing each Zi by its expected value. Thus, from the above, we get\npη = p(1− η) + qη = p(1− 2η) + η; pηl = pl(1− η) + qlη = pl(1 − 2η) + η (4) We emphasize here that, under symmetric label noise, the corruption of label is independent of feature vector and true label and thus we have Pr[Zi = 1] = Pr[Zi = 1|yi] = Pr[Zi = 1|xi ∈ B, yi] = η, for any subset B of the feature space. We have used this fact in deriving the eq.(4). Comparing the expressions for pη and pηl , we see that, essentially, at any node (in the large sample limit) the fraction of examples whose labels are corrupted is the same. This is intuitively clear because under symmetric label noise the corruption of class label does not depend on the feature vector.\nTo find the large sample limit of criterion C(f) under label noise, we need values of the impurity function in the large sample limit which in turn needs pη, qη, pηl etc. which are as given above. For example, the Gini impurity is given by G(v) = 2pq for the noise free case. For the noisy sample, its value can be written as G̃(v) = 2p̃q̃. Its value in the large sample limit would be Gη(v) = 2pηqη. Another way this can be seen is as follows. Using eq.(2) one can show that Eη[p̃q̃] = p ηqη − η(1−η)n which is pηqη as n goes to infinity.\nUsing the above we can now prove the following theorem about robustness of split criteria.\nTheorem 3 Splitting criterion based on gini impurity, mis-classification rate and twoing rule are noise-tolerant (as per definition 1) to symmetric label noise given η 6= 0.5.\nProof\nAs in the above, let p and q be the fractions of the two classes at v. For any split f , let a be the fraction of points at the left child (vl). Recall from above that the fraction a is\nsame for noisy and noise-free data. • Gini Impurity For a node v, the gini impurity is Ggini(v) = 2pq. Under symmetric label noise, gini impurity (under large sample limit) becomes (using eq.(4)),\nGη Gini (v) = 2pηqη = 2[((1 − 2η)p + η)((1 − 2η)q + η)] = 2pq(1− 2η)2 + (η − η2) = GGini(v)(1 − 2η)2 + (η − η2)\nSimilar expressions hold for Gη Gini (vl) and G η Gini (vr). The (large sample) value of criterion or impurity gain of f under label noise can be written as\ngainηGini(f) = G η Gini(v)− [a G η Gini(vl) + (1− a)G η Gini(vr)]\n= (1− 2η)2[GGini(v)− a GGini(vl)− (1− a)Gini(vr)] = (1− 2η)2gainGini(f)\nThus for any η 6= 0.5, if gainGini(f1) > gainGini(f2), then gain η Gini (f1) > gainη Gini (f2). Which means that a maximizer of impurity gain based on gini index under noise-free samples will be also a maximizer of gain under symmetric label noise, under large sample limit.\n•Misclassification rate For node v, misclassification impurity is, GMC(v) = min{p, q}. Under symmetric label noise with η < 0.5, in the large sample limit, value of impurity is, (using eq.(4)),\nGηMC(v) = min{p η, qη} = min{(1 − 2η)p + η, (1 − 2η)q + η}\n= (1− 2η)GMC(v) + η\nIn presence of symmetric label noise, expected impurity gain for a split f can be written as\ngainηMC(f) = G η MC(v)− [a G η MC(vl) + (1− a)G η MC(vr)]\n= (1− 2η)[GηMC(v)− a G η MC(vl)− (1− a)G η MC(vr)] = (1− 2η)gainMC(f)\nwhere (1−2η) > 0 because we are considering the case η < 0.5. When η > 0.5, one can similarly show that gainη\nMC (f) = (2η − 1)gainmc(f). This completes proof of noise-tolerance\nof impurity based on misclassification rate.\n• Twoing rule Using the same notation defined Sec 2.2 for twoing criterion, for a split f , objective can be rewritten as\nGTwoing(f) = a(1− a)\n4\n[ |pl − pr|+ |ql − qr| ]2 = a(1− a)[pl − pr]2\nWhen there is symmetric label noise, pηl = (1− 2η)pl + η and p η r = (1− 2η)pr + η.\nGηTwoing(f) = a(1− a)[p η l − pηr ]2 = a(1− a)(1− 2η)2[pl − pr]2\n= (1− 2η)2GTwoing(f)\nThus, the maximizer of twoing rule does not change when there is symmetric label noise.\nThe above theorem shows that impurity gain (using gini or misclassification rate) based criteria are noise-tolerant for symmetric label noise as per Definition 1.\nRemark 4 Impurity based on entropy Another popular criterion is impurity gain based on entropy which is not considered in the above theorem. The impurity gain based on entropy is not noise-tolerant as per definition 1 as shown by the following counterexample.\nConsider a case where a node has n samples (n is large). Suppose, under split rule f1 we get nl = nr = 0.5n, n + l = 0.05n and n + r = 0.25n. Suppose there is another split rule f2 under which we get nl = 0.3n and nr = 0.7n with n + l = 0.003n and n + r = 0.297n. Then it can be easily shown that gainEntropy(f1) < gainEntropy(f2); but, under symmetric label noise with η = 40%, gainη Entropy (f2) < gain η Entropy (f1).\nHowever, we would like to emphasize that the above example may be a non-generic one. In large number of simulations we have seen that the split rule that maximizes the criterion is same under noisy and noise-free cases. Thus, impurity gain based on entropy for learning decision trees is also fairly robust to label noise."
    }, {
      "heading" : "3.2. Robustness of Labeling Rule at Leaf Nodes",
      "text" : "We next consider the robustness of criterion to assign a class label to a leaf node. A popular approach is to take majority vote at the leaf node. We prove that, majority voting is robust to symmetric label noise in the sense that (in the large sample limit) the fraction of positive examples would be more under label noise if the fraction of positive examples is higher in noise-free case. We also show that it can be robust to non-uniform noise also under a restrictive condition.\nTheorem 5 Let ηx < 0.5,∀x. (a). Then, majority voting at a leaf node is robust to symmetric label noise. (b). It is also robust to nonuniform label noise if all the points at the leaf node belong to one class in the noise free data.\nProof\nLet p and q = 1− p be the fraction of positive and negative samples at leaf node v. (a) Under symmetric label noise, the relevant fractions are pη = (1 − η)p + ηq and qη = (1− η)q + ηp. Thus, pη − qη = (1− 2η)(p − q). Since η < 0.5, (pη − qη) will have the same sign as (p− q), proving robustness of the majority voting.\n(b) Let v contain all the points from the positive class. Thus, p = 1, q = 0. Let x1, · · · ,xn be the samples at v. Under non-uniform noise (with ηx < 0.5,∀x),\npη = 1\nn\nn ∑\ni=1\n(1− ηxi) > 0.5\nn\nn ∑\ni=1\n1 = 0.5 (5)\nThus, the majority vote will assign positive label to the leaf node v. This proves the second part of the theorem."
    }, {
      "heading" : "3.3. Robustness of Decision Tree Learning Under Symmetric Label Noise : Large Sample Analysis",
      "text" : "We have proved that some of the popular split criteria are noise-tolerant. What we have shown is that the split rule that maximizes the criterion under noise-free samples is same as that which maximizes the value of criterion under symmetric label noise (under large sample limit). This means, under large sample assumption, the same split rule would be learnt at any node irrespective of whether the labels come from noise-free data or noisy data. (Here we assume for simplicity that there is a unique split rule maximizing the criterion at each node. Otherwise we need some prefixed rule to break ties).2\nOur result for leaf node labeling implies that, under large sample assumption, with majority rule a leaf node would get the same label under noisy or noise-free data. To conclude that we learn the same tree, we need to examine the rule for deciding when a node becomes a leaf. If this is determined by the depth of the node or number of samples at the node then it is easy to see that the same tree would be learnt with noisy and noise-free data. In many algorithms one makes a node as leaf if no split rule gives positive value to the gain. This will also lead to learning of the same tree with noisy samples as with noise-free samples, because we showed that the gain under noisy case is a linear function of the gain under noise-free case.\nRemark 6 Robustness under general noise: In our analysis so far, we have only considered symmetric label noise. In the simplest case of asymmetric noise, namely, classconditional noise, noise rate is same for all feature vectors of a class though it may be different for different classes. In the risk minimization framework, class conditional noise can be taken care when the noise rates are known (or can be estimated) (Natarajan et al., 2013; Scott et al., 2013; Ghosh et al., 2015). We can extend the analysis presented in Sec.3.1 to relate expected fraction of examples of a class in the noisy and noise-free cases using the two noise rates. Thus, if the noise rates are assumed known (or can be reliably estimated) it should be possible to extend the analysis here to the case of class-conditional noise. In the general case when noise rates are not known (and cannot be reliably estimated), it appears difficult to establish robustness of impurity based split criteria."
    }, {
      "heading" : "3.4. Sample Complexity under Noise",
      "text" : "We established robustness of decision tree learning algorithms under large sample limit. Hence an interesting question is that of how large the sample size should be for our assertions about robustness to hold with a large probability. We provide some sample complexity bounds in this subsection. (Proofs of Lemmas 7 and 8 are given in Appendix).\nLemma 7 Let leaf node v have n samples. Under symmetric label noise with η < 0.5, majority voting will not fail with probability at least 1 − δ when n ≥ 2\nρ2(1−2η)2 ln(1δ ), where\nρ is the difference between fraction of positive and negative samples in the noise-free case.\n2. Here we are assuming that the xi at the node are same in the noisy and noise-free cases. These are same at the root. If in the two cases we learn the same split at the root, then at both its children the samples would be same in the noise and noise-free cases and so on.\nThe sample size needed increases with increasing η, which is intuitive. It also increases with decreasing ρ. The value of ρ tells us the ‘margin of majority’ in the noise-free case and hence when ρ is small we should expect to need more examples in the noisy case.\nLemma 8 Let there be n samples at a non-leaf node v and given two splits f1 and f2, suppose gain (gini, misclassification, twoing rule) for f1 is higher than that for f2. Under symmetric label noise with η 6= 0.5, gain from f1 will be higher with probability 1 − δ when n ≥ O( 1ρ2(1−2η)2 ln( 1 δ )), where ρ denotes the difference between gain of the two splits in the noise-free case.\nWhile these results, shed some lights on sample complexity, we emphasize that these bounds are loose and are obtained using concentration inequalities. Also we want to point out, large sample in leaf implies large sample in non-leaf nodes. In practice, sample size needed is not high. In experimental section, we provide results on how many training samples are needed for robust learning of decision trees on a synthetic dataset."
    }, {
      "heading" : "3.5. Noise Robustness in Random Forest",
      "text" : "A random forest (Breiman, 2001) is a collection of randomized tree classifiers. We represent the set of trees as gn = {gn(x, π1), · · · , gn(x, πm)}. Here π1, · · · , πm are iid random variables, conditioned on data, which are used for partitioning the nodes. Finally, majority vote is taken among the random tree classifiers for prediction. We denote this classifier as ḡn.\nIn a purely random forest classifier, partitioning does not depend on the class labels. At each step, a node is chosen randomly and a feature is selected randomly for the split. A split threshold is chosen uniformly randomly from the interval of the selected feature. This procedure is done k times. A greedily grown random forest classifier is a set of randomized tree classifiers. Each tree is grown greedily by improving impurity with some randomization. At each node, a random subset of features are chosen. Tree is grown by computing the best split among those random features only. Breiman’s random forest classifier uses gini impurity gain (Breiman, 2001).\nRemark 9 A purely random forest classifier/ greedily grown random forest, ḡn, is robust to symmetric label noise with η < 0.5 under large sample assumption.\nWe need to show each randomized tree is robust to label noise in both cases. In purely random forest, randomization is on the partitions and the partitions do not depend on class labels (which may be noisy). We proved robustness of majority vote at leaf nodes under symmetric label noise. Thus, for a purely random forest, ḡ∗η = ḡ∗. That is, the classifier learnt with noisy labels would be same as that learnt with noise-free samples. Similarly for a greedily grown trees with gini impurity measure, we showed that each tree is robust because of both split rule robustness and majority voting robustness. Thus when large sample assumption holds, greedily grown random forest will also be robust to symmetric label noise.\nRemark 10 Sample complexity of Random forest: Empirically we observe that, often random forest has better robustness than a single decision tree in finite sample cases. For a classifier, generalization error can be written as,\nerrorgen = errorbias + errvariance + σ 2 noise\nUnder symmetric label noise, errorbias is same for single decision tree as well as random forest. Thus generalization error is controlled by errorvariance. If pairwise correlation of each trees is ρ and variance is σ2 for each tree, then random forest, consisting N trees, has variance, (Hastie et al., 2005)\nerrorvariance = ρσ 2 + 1− ρ N σ2\nIntuitively, if a single decision tree is learnt with noisy samples, our results imply that its classification decision on a new point would be same as noise free case in an expected sense. If we have many independent decision trees, variance in the classification will decrease. If the decision trees are highly correlated, then the variance reduction might not be significant."
    }, {
      "heading" : "4. Empirical Illustration",
      "text" : "In this section, we illustrate our robustness results for learning of decision trees and random forest. We also present results with SVM. While, SVM has been proved to be non-robust even under symmetric label noise, its sensitivity towards noise widely varies (Long and Servedio, 2010; Nettleton et al., 2010; Manwani and Sastry, 2013; van Rooyen et al., 2015). We also provide results on sample complexity for robust learning of decision trees and random forest."
    }, {
      "heading" : "4.1. Dataset Description",
      "text" : "We used four 2D synthetic datasets. Details are given below. (Here n denotes total number of samples, p+, p− represent the class conditional densities, and U(A) denotes uniform distribution over set A).\n• Dataset 1: Checker board 2by2 Pattern: Data uniform over [0, 2]× [0, 2] and one class region being ([0, 1] × [0, 1]) ∪ ([1, 2] × [1, 2]) and n = 30000\n• Dataset 2: Checker board 4by4 Pattern: Extension of the above to a 4× 4 grid.\n• Dataset 3: Imbalance Linear Data. p+ = U([0, 0.5]×[0, 1]) and p− = U([0.5, 1]×[0, 1]). Prior probabilities of classes are 0.9 & 0.1, and n = 40000.\n• Dataset 4: Imbalance and Asymmetric Linear Data. p+ = U([0, 0.5] × [0, 1]) and p− = U([0.5, 0.7] × [0.4, 0.6]). Prior probabilities are 0.8 & 0.2, and n = 40000.\nWe also present results for 6 UCI datasets (Lichman, 2013)."
    }, {
      "heading" : "4.2. Experimental Setup",
      "text" : "We used decision tree implementation in scikit learn library (Pedregosa et al., 2011). We present results only with gini impurity based decision tree classifier. (We observed that decision trees learnt using twoing rule and misclassification rate have similar performance). For random forest classifier (RF) we used scikit learn library. Number of trees in random forest was set to 100. For SVM we used libsvm package (Chang and Lin, 2011).\nIn subsection 4.3 we present results to illustrate sample complexity for robust learning where training set size and size of leaf nodes is varied as explained there.\nIn subsection 4.4, we compare accuracies of decision tree learning, random forest and SVM for which the following setup is used. Minimum leaf size is the only user-chosen parameter in random forest and decision trees. For synthetic datasets, minimum samples in leaf node was restricted to 250. For UCI datasets, it was restricted to 50. For SVM, we used linear kernel (l) for Synthetic Datasets 3, 4 and quadratic kernel (p) for Checker board 2by2 data. In all other datasets we used gaussian kernel (g). For SVM, we selected hyper-parameters using validation data. (Validation range for C is 0.01-500 and for γ in the Gaussian kernel it is 0.001-10). We used 20% data for testing and 20% for validation. Symmetric label noise was varied from 0% − 40%. As synthetic datasets are separable, we also experimented with class conditional noise with the two noise rates for the two classes being 40% and 20%. In all experiments, noise was introduced only on training and validation data. Test set was noise free."
    }, {
      "heading" : "4.3. Effect of sample size on robustness of learning",
      "text" : "Here we discuss sensitivity of decision tree learning (under label noise) on sample size. We present experimental results on the test accuracy for different sample sizes using the 2by2 checker board data.\nTo study effect of sample size in leaf nodes, we choose a leaf sample size and learn decision tree and random forest with different noise levels. (The training set size is fixed at 20000). We do this for a number of choices for leaf sample size. The test accuracies in all these cases are shown in Figure 1(a). As can be seen from the figure, even when training data size is huge, we do not get robustness if leaf sample size is small. This is in accordance with our analysis (as in Lemma 7) because minimum sample size is needed for the majority rule to be correct with a large probability. A leaf sample size of 50 seems sufficient to take care of even 30% noise. As expected, random forest has better robustness.\nNext we experiment with varying the (noisy) training data size. The results are shown in Figure 1(b). It can be seen that with 400/4000 sample size decision tree learnt has good test accuracy (95%) at 20%/40% noise (the sample ratio is close to (1−2×0.4) 2\n(1−2×0.2)2 = 1/9 as\nprovided in lemma. 7). We need larger sample size for higher level of noise. This is also as expected from our analysis."
    }, {
      "heading" : "4.4. Comparison of accuracies of learnt classifiers",
      "text" : "The average test accuracy and standard deviation (over 10 runs) on different data sets under different levels of noise are shown in Table 1 for synthetic datasets and in Table 2 for UCI datasets. In table 2 we also indicate the dimension of feature vector (d), the number of positive and negative samples in the data (n+, n−).\nFor synthetic datasets the sample sizes are large and hence we expect good robustness. As can be seen from Table 1, for noise-free data, decision tree, random forest and SVM have all similar accuracies. However, with 30% or 40% noise, the accuracies of SVM are much poorer than those of decision tree and random forest. For example on datasets 3 and 4, the accuracies of decision tree and random forest continue to be 99% even at 40% noise while those of SVM drop to about 90% and 80% respectively. This illustrates the robustness of\ndecision tree learning as indicated by our analysis. It can be seen that decision tree and random forest are robust to class conditional noise also, even without knowledge about noise rate (as indicated by last column in the table). Our current analysis does not prove this robustness; as remarked earlier, this is one possible extension of the theoretical analysis presented here.\nSimilar performance is seen on UCI data sets also as shown in Table 2. For breast cancer dataset, accuracy of decision tree also drops with noise while for random forest the drop is significantly less. This is also expected because the total sample size here is less. Although SVM has significantly higher accuracy than decision tree in 0% noise, at 40% noise its accuracy drops more than that of decision tree. In all other data sets also, decision tree and random forest are more robust than SVM as can be seen from the table.\nAs explained earlier, our analysis shows that decision tree learning is robust in large sample case. Thus, though decision tree learning may not be robust to label noise when training set size is small, the robustness improves with increasing training set size. This is demonstrated by our results on synthetic data sets. However, this is not true of a standard algorithm such as SVM. For example, Datasets 3 and 4 represent very simple two dimensional problems. Though we have 40000 samples here, SVM does not learn well under label noise. On the other hand, the accuracies of decision tree and random forest at 30% noise are as good as their accuracies at 0% noise and these accuracies are very high."
    }, {
      "heading" : "5. Conclusion",
      "text" : "In this paper, we investigated the robustness of decision tree learning under label noise. In many current applications one needs to take care of label noise in training data. Hence, it is very desirable to have learning algorithms that are not affected by label noise. Since most impurity based top-down decision tree algorithms learns split rules based on fractions of positive and negative samples at a node, one can expect that they should have some\nrobustness. We proved that decision tree algorithms based on gini or misclassification impurity and the twoing rule algorithm are all robust to symmetric label noise. We showed that, under large sample assumption, with a high probability, the same tree would be learnt with noise-free data as with noisy data. We also provided some sample complexity results for the robustness. Through extensive empirical investigations we illustrated the robust learning of decision tree and random forest. Decision tree approach is very popular in many practical applications. Hence, the robustness results presented in this paper are\ninteresting. All the results we proved are for symmetric noise. Extending these results to class conditional and non-uniform noise is an important direction for future research."
    }, {
      "heading" : "Appendix A. Sample Complexity Bounds",
      "text" : "Proof [of Lemma 7] Let n+ and n− denote the positive and negative samples at the node under noise-free case. (Note n = n+ + n−). Without loss of generality assume that positive class is in majority and hence, by definition, ρ = (n+ − n−)/n. Let ñ+ and ñ− be the positive and negative samples under the noisy case.\nLet Xi, i = 1, · · · , n+ be random variables with Pr[Xi = 1] = 1 − Pr[Xi = 0] = η. Let Xi, i = n\n+ + 1, · · · , n be random variables with Pr[Xi = −1] = 1 − Pr[Xi = 0] = η. Let Sn = ∑n i=1 Xi. Then, under symmetric label noise, we have ñp − ñn = (np − nn)− 2Sn = ρn− 2Sn. Also, note that ESn = ηn+ − ηn− = ηρn. Now we have\nPr[ñ+ − ñ− < 0] = Pr[ρn − 2Sn < 0] = Pr[2Sn − 2ESn > ρn(1− 2η)]\n≤ exp ( −ρ 2n(1− 2η)2\n2\n)\nwhere the last line follows from hoeffding’s inequality. If we want this probability to be less than δ then we would need n > 2\nρ2(1−2η)2 ln(1δ ). This completes the proof.\nProof [Of Lemma 8] Lets assume parent node v contains n samples whereas left child vl (right child vr) contains nl = na (nr = n − na) samples. Note under noise, for a split rule f at node v, for both parent as well as child, these numbers remain same as noise free case. For a\nparent node v, suppose, p (p̃) and q (q̃) are the positive and negative fraction under noisefree (noisy) data with n samples. Similarly pl, ql, p̃l, q̃l (pr, qr, p̃r, q̃r) is defined for left (and right child). Thus under symmetric label noise η, we can write for any node (note, Eη(p̃) = p η), Pr[|p̃− pη| > ǫ] ≤ 2e−2nǫ2 (6) We want to bound how finite sample estimates of different impurity gain differs from the large sample assumption (or the expectation). We use ǫ1, ǫ2 and ǫ3 to denote the finite sample error (from the expectation) for positive fraction in parent, left and right child respectively (note this in turn bounds negative fraction also). We set ǫ1 = ǫ, ǫ2 = ǫ/ √ a and\nǫ3 = ǫ/ √ 1− a. The probability can be upper bounded using hoeffding bound in eq. (6) as,\nPr [ ( |p̃−pη| ≥ ǫ1 ) ∪ ( |p̃l−pηl | ≥ ǫ2 ) ∪ ( |p̃r−pηr | ≥ ǫ3 ) ] ≤ 2(e−2nǫ21+e−2nlǫ22+e−2nrǫ23) = 6e−2nǫ2 (7) Note that, this probability does not depend on any split and can be applied to any arbitrary split. Also note, for twoing rule, first term is not required in RHS and LHS. Given the complement of this event (lets call it as ‘all fractions are ǫ-accurate’ event), we compute how finite sample impurity gain deviates from the large sample limit.\n• Gini Impurity: For a node v, after some simplification, using eq. 6,7, we can bound the finite sample noise estimate as (for gini G̃ = 2p̃q̃),\n|p̃q̃ − pηqη| ≤ |ǫ(pη − qη)|\nThus we can bound finite noisy sample gain from gini impurity as,\n| ˆgainηGini(f)− gain η Gini (f)| ≤ 2|ǫ1(pη − qη)|+ 2a|ǫ2(pηl − q η l )|+ 2(1− a)|ǫ3(pηr − qηr )|\n≤ 2(1 − 2η) [ |ǫ1(p− q)|+ a|ǫ2(pl − ql)|+ (1− a)|ǫ3(pr − qr)| ]\n≤ 2(1 − 2η)[|ǫ1(p− q)|+ |aǫ2|+ |(1− a)ǫ3|] ≤ 6(1 − 2η)ǫ\nUnder noise free case, we assume the difference of gini gain between two splits is ρ. Under noise corrupted signal label, expected difference is ρη = (1− 2η)2ρ.\nSetting ǫ = ρη/12(1 − 2η) = ρ(1− 2η)/12 for both the splits in eq. 7, we get the upper bound on probability of ordering change as, 12e−nρ 2(1−2η)2/72.\n•Misclassification Impurity: For misclassification impurity, for a node v, we have\n|min(p̃, q̃)−min(pη, qη)| ≤ |ǫ|\nThus we can bound finite noisy sample gain for misclassification impurity as,\n| ˆgainηMC(f)− gain η MC (f)| ≤ |ǫ1|+ a|ǫ2|+ (1− a)|ǫ3|\n≤ |ǫ|+ |ǫ √ a|+ |ǫ √ 1− a| ≤ 3ǫ\nIf ρ is the difference in gain in noise free case, under noise, difference in gain becomes, ρ(1− 2η). Thus we can set ǫ = ρ(1− 2η)/6 in eq. 7 for both of the splits to get the probability bound.\n•Twoing Rule: Similarly for twoing rule we bound the gain assuming ‘all fractions are ǫ-accurate’ event. We get, after simplification,\n|ĜηTwoing(f)−G η Twoing(f)| ≤ a(1− a)(|ǫ2 − ǫ3|)(|p η l − pηr |)\n≤ (1− 2η)(|ǫ(1 − a) √ a|+ |ǫa √ 1− a|)(|pl − pr|) ≤ (1− 2η) 2 (|ǫ|+ |ǫ|) ≤ (1− 2η)ǫ\nNote √ a √ 1− a ≤ 1/2. Under noise, difference of gain becomes (1− 2η)2ρ. Here we can set ǫ = ρ(1− 2η)/2 to bound the probability of ordering change.\nThus for all cases, required sample size in parent node is n ≥ O( 1 ρ2(1−2η)2 ln(1δ ))"
    } ],
    "references" : [ {
      "title" : "Classification and Regression Trees",
      "author" : [ "L. Breiman", "J. Friedman", "R. Olshen", "C. Stone" ],
      "venue" : null,
      "citeRegEx" : "Breiman et al\\.,? \\Q1984\\E",
      "shortCiteRegEx" : "Breiman et al\\.",
      "year" : 1984
    }, {
      "title" : "Identifying mislabeled training data",
      "author" : [ "Carla E. Brodley", "Mark A. Friedl" ],
      "venue" : "Journal of Artificial Intelligence Research,",
      "citeRegEx" : "Brodley and Friedl.,? \\Q1999\\E",
      "shortCiteRegEx" : "Brodley and Friedl.",
      "year" : 1999
    }, {
      "title" : "LIBSVM: A library for support vector machines",
      "author" : [ "Chih-Chung Chang", "Chih-Jen Lin" ],
      "venue" : "ACM Transactions on Intelligent Systems and Technology,",
      "citeRegEx" : "Chang and Lin.,? \\Q2011\\E",
      "shortCiteRegEx" : "Chang and Lin.",
      "year" : 2011
    }, {
      "title" : "Analysis of learning from positive and unlabeled data",
      "author" : [ "Marthinus C du Plessis", "Gang Niu", "Masashi Sugiyama" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Plessis et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Plessis et al\\.",
      "year" : 2014
    }, {
      "title" : "Classification in the presence of label noise: a survey",
      "author" : [ "Benôıt Frénay", "Michel Verleysen" ],
      "venue" : "Neural Networks and Learning Systems, IEEE Transactions on,",
      "citeRegEx" : "Frénay and Verleysen.,? \\Q2014\\E",
      "shortCiteRegEx" : "Frénay and Verleysen.",
      "year" : 2014
    }, {
      "title" : "Making risk minimization tolerant to label",
      "author" : [ "Aritra Ghosh", "Naresh Manwani", "PS Sastry" ],
      "venue" : "noise. Neurocomputing,",
      "citeRegEx" : "Ghosh et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Ghosh et al\\.",
      "year" : 2015
    }, {
      "title" : "The elements of statistical learning: data mining, inference and prediction",
      "author" : [ "Trevor Hastie", "Robert Tibshirani", "Jerome Friedman", "James Franklin" ],
      "venue" : "The Mathematical Intelligencer,",
      "citeRegEx" : "Hastie et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Hastie et al\\.",
      "year" : 2005
    }, {
      "title" : "A fast, bottom-up decision tree pruning algorithm with near-optimal generalization",
      "author" : [ "Michael J Kearns", "Yishay Mansour" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Kearns and Mansour.,? \\Q1998\\E",
      "shortCiteRegEx" : "Kearns and Mansour.",
      "year" : 1998
    }, {
      "title" : "Random classification noise defeats all convex potential boosters",
      "author" : [ "Philip M Long", "Rocco A Servedio" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Long and Servedio.,? \\Q2010\\E",
      "shortCiteRegEx" : "Long and Servedio.",
      "year" : 2010
    }, {
      "title" : "Generalization bounds for decision trees",
      "author" : [ "Yishay Mansour", "David A McAllester" ],
      "venue" : "In COLT,",
      "citeRegEx" : "Mansour and McAllester.,? \\Q2000\\E",
      "shortCiteRegEx" : "Mansour and McAllester.",
      "year" : 2000
    }, {
      "title" : "Noise tolerance under risk minimization",
      "author" : [ "Naresh Manwani", "PS Sastry" ],
      "venue" : "Cybernetics, IEEE Transactions on,",
      "citeRegEx" : "Manwani and Sastry.,? \\Q2013\\E",
      "shortCiteRegEx" : "Manwani and Sastry.",
      "year" : 2013
    }, {
      "title" : "Learning with noisy labels",
      "author" : [ "Nagarajan Natarajan", "Inderjit S Dhillon", "Pradeep K Ravikumar", "Ambuj Tewari" ],
      "venue" : "In Advances in neural information processing systems,",
      "citeRegEx" : "Natarajan et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Natarajan et al\\.",
      "year" : 2013
    }, {
      "title" : "A study of the effect of different types of noise on the precision of supervised learning techniques",
      "author" : [ "David F Nettleton", "Albert Orriols-Puig", "Albert Fornells" ],
      "venue" : "Artificial intelligence review,",
      "citeRegEx" : "Nettleton et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Nettleton et al\\.",
      "year" : 2010
    } ],
    "referenceMentions" : [ {
      "referenceID" : 4,
      "context" : "Thus, learning classifiers in the presence of label noise is an important problem (Frénay and Verleysen, 2014).",
      "startOffset" : 82,
      "endOffset" : 110
    }, {
      "referenceID" : 6,
      "context" : "It is generally accepted that among all the classification methods, decision tree is probably closest to ‘off-the-shelf’ method which has all the desirable properties including robustness to outliers (Hastie et al., 2005).",
      "startOffset" : 200,
      "endOffset" : 221
    }, {
      "referenceID" : 9,
      "context" : "While there are many results about generalization bounds for decision trees (Mansour and McAllester, 2000; Kearns and Mansour, 1998), not many theoretical results are known about the robustness of decision tree learning in presence of label noise.",
      "startOffset" : 76,
      "endOffset" : 132
    }, {
      "referenceID" : 7,
      "context" : "While there are many results about generalization bounds for decision trees (Mansour and McAllester, 2000; Kearns and Mansour, 1998), not many theoretical results are known about the robustness of decision tree learning in presence of label noise.",
      "startOffset" : 76,
      "endOffset" : 132
    }, {
      "referenceID" : 1,
      "context" : "the training data increases size of the learnt tree; detecting and removing noisy examples improves the learnt tree (Brodley and Friedl, 1999).",
      "startOffset" : 116,
      "endOffset" : 142
    }, {
      "referenceID" : 10,
      "context" : "Another result is that some of the standard convex losses are not robust to symmetric label noise while the 0-1 loss is (Manwani and Sastry, 2013).",
      "startOffset" : 120,
      "endOffset" : 146
    }, {
      "referenceID" : 5,
      "context" : "A general sufficient condition on the loss function for risk minimization to be robust is derived in (Ghosh et al., 2015).",
      "startOffset" : 101,
      "endOffset" : 121
    }, {
      "referenceID" : 11,
      "context" : "Robust risk minimization strategies under the so called class-conditional (or asymmetric) label noise are also proposed (Natarajan et al., 2013; Scott et al., 2013).",
      "startOffset" : 120,
      "endOffset" : 164
    }, {
      "referenceID" : 5,
      "context" : "Some sufficient conditions for robustness of risk minimization under 0-1 loss, ramp loss and sigmoid loss when the training data is corrupted with most general non-uniform label noise are also presented in (Ghosh et al., 2015).",
      "startOffset" : 206,
      "endOffset" : 226
    }, {
      "referenceID" : 1,
      "context" : "the training data increases size of the learnt tree; detecting and removing noisy examples improves the learnt tree (Brodley and Friedl, 1999). Recently, Nettleton et al. (2010) empirically studied robustness of different classifiers under label noise.",
      "startOffset" : 117,
      "endOffset" : 178
    }, {
      "referenceID" : 1,
      "context" : "the training data increases size of the learnt tree; detecting and removing noisy examples improves the learnt tree (Brodley and Friedl, 1999). Recently, Nettleton et al. (2010) empirically studied robustness of different classifiers under label noise. While decision tree learning is better than SVM or logistic regression in terms of robustness to label noise, it is also seen that naive Bayes is more robust than decision trees. In this paper, we present a theoretical study of such robustness properties of decision trees. Recently, many analytical results are reported on robust learning of classifiers, using the framework of risk minimization. The robustness or noise tolerance of risk minimization depends on the loss function used. Long and Servedio (2010) proved that any convex potential loss is not robust to uniform or symmetric label noise.",
      "startOffset" : 117,
      "endOffset" : 766
    }, {
      "referenceID" : 1,
      "context" : "the training data increases size of the learnt tree; detecting and removing noisy examples improves the learnt tree (Brodley and Friedl, 1999). Recently, Nettleton et al. (2010) empirically studied robustness of different classifiers under label noise. While decision tree learning is better than SVM or logistic regression in terms of robustness to label noise, it is also seen that naive Bayes is more robust than decision trees. In this paper, we present a theoretical study of such robustness properties of decision trees. Recently, many analytical results are reported on robust learning of classifiers, using the framework of risk minimization. The robustness or noise tolerance of risk minimization depends on the loss function used. Long and Servedio (2010) proved that any convex potential loss is not robust to uniform or symmetric label noise. Another result is that some of the standard convex losses are not robust to symmetric label noise while the 0-1 loss is (Manwani and Sastry, 2013). It is noted by du Plessis et al. (2014) that convex surrogates losses are not good for learning from positive and unlabeled data.",
      "startOffset" : 117,
      "endOffset" : 1043
    }, {
      "referenceID" : 10,
      "context" : "We use the same notion of noise tolerance as in (Manwani and Sastry, 2013; van Rooyen et al., 2015).",
      "startOffset" : 48,
      "endOffset" : 99
    }, {
      "referenceID" : 0,
      "context" : "Then the gini impurity is defined by GGini = 2pq (Breiman et al., 1984); entropy based impurity is defined as GEntropy = −p log p− q log q (Quinlan, 1986); and misclassification impurity is defined as GMC = min{p, q}.",
      "startOffset" : 49,
      "endOffset" : 71
    }, {
      "referenceID" : 0,
      "context" : "Then the gini impurity is defined by GGini = 2pq (Breiman et al., 1984); entropy based impurity is defined as GEntropy = −p log p− q log q (Quinlan, 1986); and misclassification impurity is defined as GMC = min{p, q}. Often the criterion C is called the gain. Hence, we also use gainGini(f) to refer to C(f) when G is GGini and similarly for other impurity measures. A split criterion different from impurity is twoing rule, first proposed by Breiman et al. (1984). Consider a split rule f at a node v.",
      "startOffset" : 50,
      "endOffset" : 465
    }, {
      "referenceID" : 11,
      "context" : "In the risk minimization framework, class conditional noise can be taken care when the noise rates are known (or can be estimated) (Natarajan et al., 2013; Scott et al., 2013; Ghosh et al., 2015).",
      "startOffset" : 131,
      "endOffset" : 195
    }, {
      "referenceID" : 5,
      "context" : "In the risk minimization framework, class conditional noise can be taken care when the noise rates are known (or can be estimated) (Natarajan et al., 2013; Scott et al., 2013; Ghosh et al., 2015).",
      "startOffset" : 131,
      "endOffset" : 195
    }, {
      "referenceID" : 6,
      "context" : "If pairwise correlation of each trees is ρ and variance is σ2 for each tree, then random forest, consisting N trees, has variance, (Hastie et al., 2005)",
      "startOffset" : 131,
      "endOffset" : 152
    }, {
      "referenceID" : 8,
      "context" : "While, SVM has been proved to be non-robust even under symmetric label noise, its sensitivity towards noise widely varies (Long and Servedio, 2010; Nettleton et al., 2010; Manwani and Sastry, 2013; van Rooyen et al., 2015).",
      "startOffset" : 122,
      "endOffset" : 222
    }, {
      "referenceID" : 12,
      "context" : "While, SVM has been proved to be non-robust even under symmetric label noise, its sensitivity towards noise widely varies (Long and Servedio, 2010; Nettleton et al., 2010; Manwani and Sastry, 2013; van Rooyen et al., 2015).",
      "startOffset" : 122,
      "endOffset" : 222
    }, {
      "referenceID" : 10,
      "context" : "While, SVM has been proved to be non-robust even under symmetric label noise, its sensitivity towards noise widely varies (Long and Servedio, 2010; Nettleton et al., 2010; Manwani and Sastry, 2013; van Rooyen et al., 2015).",
      "startOffset" : 122,
      "endOffset" : 222
    }, {
      "referenceID" : 2,
      "context" : "For SVM we used libsvm package (Chang and Lin, 2011).",
      "startOffset" : 31,
      "endOffset" : 52
    } ],
    "year" : 2016,
    "abstractText" : "In most practical problems of classifier learning, the training data suffers from the label noise. Hence, it is important to understand how robust is a learning algorithm to such label noise. This paper presents some theoretical analysis to show that many popular decision tree algorithms are robust to symmetric label noise under large sample size. We also present some sample complexity results which provide some bounds on the sample size for the robustness to hold with a high probability. Through extensive simulations we illustrate this robustness.",
    "creator" : "LaTeX with hyperref package"
  }
}