{
  "name" : "1507.02482.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Differentially Private Ordinary Least Squares",
    "authors" : [ "Or Sheffet" ],
    "emails" : [ "<osheffet@ualberta.ca>." ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "Since the early days of differential privacy, its main goal was to design privacy preserving versions of existing techniques for data analysis. It is therefore no surprise that several of the first differentially private algorithms were machine learning algorithms, with a special emphasis on the ubiquitous problem of linear regression (Kasiviswanathan et al., 2008; Chaudhuri et al., 2011; Kifer et al., 2012; Bass-\n1Computing Science Dept., University of Alberta, Edmonton AB, Canada. This work was done when the author was at Harvard University, supported by NSF grant CNS-123723. Correspondence to: Or Sheffet <osheffet@ualberta.ca>.\nily et al., 2014). However, all existing body of work on differentially private linear regression measures utility by bounding the distance between the linear regressor found by the standard non-private algorithm and the regressor found by the privacy-preserving algorithm. This is motivated from a machine-learning perspective, since bounds on the difference in the estimators translate to error bounds on prediction (or on the loss function). Such bounds are (highly) interesting and non-trivial, yet they are of little use in situations where one uses linear regression to establish correlations rather than predict labels.\nIn the statistics literature, Ordinary Least Squares (OLS) is a technique that uses linear regression in order to infer the correlation between a variable and an outcome, especially in the presence of other factors. And so, in this paper, we draw a distinction between “linear regression,” by which we refer to the machine learning technique of finding a specific estimator for a specific loss function; and “Ordinary Least Squares,” by which we refer to the statistical inference done assuming a specific model for generating the data and that uses linear regression. Many argue that OLS is the most prevalent technique in social sciences (Agresti & Finlay, 2009). Such works make no claim as to the labels of a new unlabeled batch of samples. Rather they aim to establish the existence of a strong correlation between the label and some feature. Needless to say, in such works, the privacy of individuals’ data is a concern.\nIn order to determine that a certain variable xj is positively (resp. negatively) correlated with an outcome y, OLS assumes a model where the outcome y is a noisy version of a linear mapping of all variables: y = β · x + e (with e denoting random Gaussian noise) for some predetermined and unknown β . Then, given many samples (xi, yi) OLS establishes two things: (i) when fitting a linear function to best predict y from x over the sample (via computing β̂ = (∑ i xix T i )−1 ( ∑ i yixi)) the coefficient β̂j is positive (resp. negative); and (ii) inferring, based on β̂j , that the true βj is likely to reside in R>0 (resp. R<0). In fact, the crux in OLS is by describing βj using a probability distribution over the reals, indicating where βj is likely to fall, derived by computing t-values. These values take into account both the variance in the data as well as the variance of the noise e.1 Based on this probability distribution one can\n1For example, imagine we run linear regression on a certain (X,y) which results in a vector β̂ with coordinates β̂1 = β̂2 =\nar X\niv :1\n50 7.\n02 48\n2v 4\n[ cs\n.D S]\n2 1\nA ug\n2 01\n7\ndefine the α-confidence interval — an interval I centered at β̂j whose likelihood to contain βj is 1−α. Of particular importance is the notion of rejecting the null-hypothesis, where the interval I does not contain the origin, and so one is able to say with high confidence that βj is positive (resp. negative). Further details regarding OLS appear in Section 2.\nIn this work we give the first analysis of statistical inference for OLS using differentially private estimators. We emphasize that the novelty of our work does not lie in the differentially-private algorithms, which are, as we discuss next, based on the Johnson-Lindenstrauss Transform (JLT) and on additive Gaussian noise and are already known to be differentially private (Blocki et al., 2012; Dwork et al., 2014). Instead, the novelty of our work lies in the analyses of the algorithms and in proving that the output of the algorithms is useful for statistical inference.\nThe Algorithms. Our first algorithm (Algorithm 1) is an adaptation of Gaussian JLT. Proving that this adaptation remains ( , δ)-differentially private is straightforward (the proof appears in Appendix A.1). As described, the algorithm takes as input a parameter r (in addition to the other parameters of the problem) that indicates the number of rows in the JL-matrix. Later, we analyze what should one set as the value of r. Our second algorithm is taken\nAlgorithm 1 Outputting a private Johnson-Lindenstrauss projection of a matrix.\nInput: A matrix A ∈ Rn×d and a bound B > 0 on the l2-norm of any row in A. Privacy parameters: , δ > 0. Parameter r indicating the number of rows in the resulting matrix.\nSet w s.t. w2 = 8B 2 (√ 2r ln(8/δ) + 2 ln(8/δ) ) .\nSample Z ∼ Lap(4B2/ ) and let σmin(A) denote the smallest singular value of A. if σmin(A)2 > w2 + Z + 4B 2 ln(1/δ) then\nSample a (r×n)-matrixRwhose entries are i.i.d samples from a normal Gaussian. return RA and “matrix unaltered”. else LetA′ denote the result of appendingAwith the d×dmatrix wId×d. Sample a (r × (n + d))-matrix R whose entries are i.i.d samples from a normal Gaussian. returnRA′ and “matrix altered”. end if\nverbatim from the work of Dwork et al (2014). We de-\n0.1. Yet while the column X1 contains many 1s and (−1)s, the column X2 is mostly populated with zeros. In such a setting, OLS gives that it is likely to have β1 ≈ 0.1, whereas no such guarantees can be given for β2.\nliberately focus on algorithms that approximate the 2ndmoment matrix of the data and then run hypothesis-testing by post-processing the output, for two reasons. First, they enable sharing of data2 and running unboundedly many hypothesis-tests. Since, we do not deal with OLS based on the private single-regression ERM algorithms (Chaudhuri et al., 2011; Bassily et al., 2014) as such inference requires us to use the Fisher-information matrix of the loss function — but these algorithms do not minimize a private loss-function but rather prove that outputting the minimizer of the perturbed loss-function is private. This means that differentially-private OLS based on these ERM algorithms requires us to devise new versions of these algorithms, making this a second step in this line of work... (After first understanding what we can do using existing algorithms.) We leave this approach — as well as performing private hypothesis testing using a PTR-type algorithm (Dwork & Lei, 2009) (output merely reject / don’t-reject decision without justification), or releasing only relevant tests judging by their p-values (Dwork et al., 2015) — for future work.\nOur Contribution and Organization. We analyze the performances of our algorithms on a matrix A of the form A = [X;y], where each coordinate yi is generated according to the homoscedastic model with Gaussian noise, which is a classical model in statistics. We assume the existence of a vector β s.t. for every i we have yi = βTxi + ei and ei is sampled i.i.d from N (0, σ2).3\nWe study the result of running Algorithm 1 on such data in the two cases: where A wasn’t altered by the algorithm and when A was appended by the algorithm. In the former case, Algorithm 1 boils down to projecting the data under a Gaussian JLT. Sarlos (2006) has already shown that the JLT is useful for linear regression, yet his work bounds the l2-norm of the difference between the estimated regression before and after the projection. Following Sarlos’ work, other works in statistics have analyzed compressed\n2Researcher A collects the data and uses the approximation of the 2nd-moment matrix to test some OLS hypothesis; but once the approximation is published researcher B can use it to test for a completely different hypothesis.\n3This model may seem objectionable. Assumptions like the noise independence, 0-meaned or sampled from a Gaussian distribution have all been called into question in the past. Yet due to the prevalence of this model we see fit to initiate the line of work on differentially private Least Squares with this Ordinary model.\nAlgorithm 2 “Analyze Gauss” Algorithm of Dwork et al (2014).\nInput: A matrix A ∈ Rn×d and a bound B > 0 on the l2-norm of any row in A. Privacy parameters: , δ > 0. N ← symmetric (d × d)-matrix with upper triangle entries sampled i.i.d from N ( 0, 2B 4 ln(2/δ) 2 ) .\nreturn ATA+N .\nlinear regression (Zhou et al., 2007; Pilanci & Wainwright, 2014a;b). However, none of these works give confidence intervals based on the projected data, presumably for three reasons. Firstly, these works are motivated by computational speedups, and so they use fast JLT as opposed to our analysis which leverages on the fact that our JL-matrix is composed of i.i.d Gaussians. Secondly, the focus of these works is not on OLS but rather on newer versions of linear regression, such as Lasso or when β lies in some convex set. Lastly, it is evident that the smallest confidence interval is derived from the data itself. Since these works do not consider privacy applications, (actually, (Zhou et al., 2007; Pilanci & Wainwright, 2014a) do consider privacy applications of the JLT, but quite different than differential privacy) they assume the analyst has access to the data itself, and so there was no need to give confidence intervals for the projected data. Our analysis is therefore the first, to the best of our knowledge, to derive t-values — and therefore achieve all of the rich expressivity one infers from tvalues, such as confidence bounds and null-hypotheses rejection — for OLS estimations without having access to X itself. We also show that, under certain conditions, the sample complexity for correctly rejecting the null-hypothesis increases from a certain bound N0 (without privacy) to a bound ofN0 + Õ( √ N0 ·κ( 1nA\nTA)/ ) with privacy (where κ(M) denotes the condition number of the matrixM .) This appears in Section 3.\nIn Section 4 we analyze the case Algorithm 1 does append the data and the JLT is applied to A′. In this case, solving the linear regression problem on the projected A′ approximates the solution for Ridge Regression (Tikhonov, 1963; Hoerl & Kennard, 1970). In Ridge Regression we aim to solve minz (∑ i(yi − zTxi)2 + w2‖z‖2 ) , which means we penalize vectors whose l2-norm is large. In general, it is not known how to derive t-values from Ridge regression, and the literature on deriving confidence intervals solely from Ridge regression is virtually non-existent. Indeed, prior to our work there was no need for such calculations, as access to the data was (in general) freely given, and so deriving confidence intervals could be done by appealing back to OLS. We too are unable to derive approximated t-values in the general case, but under additional assumptions about the data — which admittedly depend in part on ‖β‖ and so cannot be verified solely from the data — we show that solving the linear regression problem on RA′ allows us to give confidence intervals for βj , thus correctly determining the correlation’s sign.\nIn Section 5 we discuss the “Analyze Gauss” algorithm (Dwork et al., 2014) that outputs a noisy version of a covariance of a given matrix using additive noise rather than multiplicative noise. Empirical work (Xi et al., 2011) shows that Analyze Gauss’s output might be non-PSD if the input has small singular values, and this results in truly bad regressors. Nonetheless, under additional conditions (that imply that the output is PSD), we derive confidence\nbounds for Dwork et al’s “Analyze Gauss” algorithm. Finally, in Section 6 we experiment with the heuristic of computing the t-values directly from the outputs of Algorithms 1 and 2. We show that Algorithm 1 is more “conservative” than Algorithm 2 in the sense that it tends to not reject the null-hypothesis until the number of examples is large enough to give a very strong indication of rejection. In contrast, Algorithm 2 may wrongly rejects the null-hypothesis even when it is true.\nDiscussion. Some works have already looked at the intersection of differentially privacy and statistics (Dwork & Lei, 2009; Smith, 2011; Chaudhuri & Hsu, 2012; Duchi et al., 2013; Dwork et al., 2015) (especially focusing on robust statistics and rate of convergence). But only a handful of works studied the significance and power of hypotheses testing under differential privacy, without arguing that the noise introduced by differential privacy vanishes asymptotically (Vu & Slavkovic, 2009; Uhler et al., 2013; Wang et al., 2015; Rogers et al., 2016). These works are experimentally promising, yet they (i) focus on different statistical tests (mostly Goodness-of-Fit and Independence testing), (ii) are only able to prove results for the case of simple hypothesis-testing (a single hypothesis) with an efficient data-generation procedure through repeated simulations — a cumbersome and time consuming approach. In contrast, we deal with a composite hypothesis (we simultaneously reject all βs with sign(βj) 6= sign(β̂j)) by altering the confidence interval (or the critical region).\nOne potential reason for avoiding confidence-interval analysis for differentially private hypotheses testing is that it does involve re-visiting existing results. Typically, in statistical inference the sole source of randomness lies in the underlying model of data generation, whereas the estimators themselves are a deterministic function of the dataset. In contrast, differentially private estimators are inherently random in their computation. Statistical inference that considers both the randomness in the data and the randomness in the computation is highly uncommon, and this work, to the best of our knowledge, is the first to deal with randomness in OLS hypothesis testing. We therefore strive in our analysis to separate the two sources of randomness — as in classic hypothesis testing, we use α to denote the bound on any bad event that depends solely on the homoscedastic model, and use ν to bound any bad event that depends on the randomized algorithm.4 (Thus, any result which is originally of the form “α-reject the null-hypothesis” is now converted into a result “(α+ν)-reject the null hypothesis”.)\n4Or any randomness in generating the feature matrixX which standard OLS theory assumes to be fixed, see Theorems 2.2 and 3.3."
    }, {
      "heading" : "2. Preliminaries and OLS Background",
      "text" : "Notation. Throughout this paper, we use lower-case letters to denote scalars (e.g., yi or ei); bold characters to denote vectors; and UPPER-case letters to denote matrices. The l-dimensional all zero vector is denoted 0l, and the l × m-matrix of all zeros is denoted 0l×m. We use e to denote the specific vector y − Xβ in our model; and though the reader may find it a bit confusing but hopefully clear from the context — we also use ej and ek to denote elements of the natural basis (unit length vector in the direction of coordinate j or k). We use , δ to denote the privacy parameters of Algorithms 1 and 2, and use α and ν to denote confidence parameters (referring to bad events that hold w.p. ≤ α and ≤ ν resp.) based on the homoscedastic model or the randomized algorithm resp. We also stick to the notation from Algorithm 1 and usew to denote the positive scalar for which w2 = 8B 2 (√ 2r ln(8/δ) + ln(8/δ)\n) throughout this paper. We use standard notation for SVD composition of a matrix (M = UΣV T), its singular values and its Moore-Penrose inverse (M+).\nThe Gaussian distribution. A univariate Gaussian N (µ, σ2) denotes the Gaussian distribution whose mean is µ and variance σ2. Standard concentration bounds on Gaussians give that Pr[x > µ + 2σ √ ln(2/ν)] < ν for any ν ∈ (0, 1e ). A multivariate Gaussian N (µ,Σ) for some positive semi-definite Σ denotes the multivariate Gaussian distribution where the mean of the j-th coordinate is the µj and the covariance between coordinates j and k is Σj,k. The PDF of such Gaussian is defined only on the subspace colspan(Σ). A matrix Gaussian distribution, denoted N (Ma×b, Ia×a, V ) has mean M , independence among its rows and variance V for each of its columns. We also require the following property of Gaussian random variables: Let X and Y be two random Gaussians s.t. X ∼ N (0, σ2) and Y ∼ N (0, λ2) where 1 ≤ σ 2\nλ2 ≤ c 2 for some c, then for any S ⊂ R we have\n1 cPrx←Y [x ∈ S] ≤ Prx←X [x ∈ S] ≤ cPrx←Y [x ∈ S/c] (see Proposition A.2).\nAdditional Distributions. We denote by Lap(σ) the Laplace distribution whose mean is 0 and variance is 2σ2. The χ2k-distribution, where k is referred to as the degrees of freedom of the distribution, is the distribution over the l2-norm squared of the sum of k independent normal Gaussians. That is, given i.i.d X1, . . . , Xk ∼ N (0, 1) it holds that ζ def= (X1, X2, . . . , Xk) ∼ N (0k, Ik×k), and ‖ζ‖2 ∼ χ2k. Existing tail bounds on the χ2k distribution (Laurent & Massart, 2000) give that\nPr [ ‖ζ‖2 ∈ ( √ k ± √ 2 ln(2/ν))2 ] ≥ 1 − ν. The Tk-\ndistribution, where k is referred to as the degrees of freedom of the distribution, denotes the distribution over the reals created by independently sampling Z ∼ N (0, 1) and\n‖ζ‖2 ∼ χ2k, and taking the quantity Z/ √ ‖ζ‖2/k. It is a known fact that Tk k→∞→ N (0, 1), thus it is a common practice to apply Gaussian tail bounds to the Tk-distribution when k is sufficiently large.\nDifferential Privacy. In this work, we deal with input in the form of a n × d-matrix with each row bounded by a l2-norm of B. Two inputs A and A′ are called neighbors if they differ on a single row. Definition 2.1 ((Dwork et al., 2006a)). An algorithm ALG which maps (n × d)-matrices into some range R is ( , δ)differential privacy it holds that Pr[ALG(A) ∈ S] ≤ e Pr[ALG(A′) ∈ S] + δ for all neighboring inputs A and A′ and all subsets S ⊂ R.\nBackground on OLS. For the unfamiliar reader, we give here a very brief overview of the main points in OLS. Further details, explanations and proofs appear in Section A.3.\nWe are given n observations {(xi, yi)}ni=1 where ∀i,xi ∈ Rp and yi ∈ R. We assume the existence of β ∈ Rp s.t. the label yi was derived by yi = βTxi + ei where ei ∼ N (0, σ2) independently (also known as the homoscedastic Gaussian model). We use the matrix notation where X denotes the (n × p)- feature matrix and y denotes the labels. We assume X has full rank.\nThe parameters of the model are therefore β and σ2, which we set to discover. To that end, we minimize minz ‖y − Xz‖2 and have\nβ̂ = (XTX)−1XTy = (XTX)−1XT(Xβ + e) = β +X+e (1) ζ = y −Xβ̂ = (Xβ + e)−X(β +X+e) = (I −XX+)e (2)\nAnd then for any coordinate j the t-value, which is the quantity t(βj) def =\nβ̂j−βj√ (XTX)−1j,j · ‖ζ‖√ n−p , is\ndistributed according to Tn−p-distribution. I.e., Pr [ β̂ and ζ satisfying t(βj) ∈ S ] = ∫ S PDFTn−p(x)dx for any measurable S ⊂ R. Thus t(βj) describes the likelihood of any βj — for any z ∈ R we can now give an estimation of how likely it is to have βj = z (which is PDFTn−p(t(z))), and this is known as t-test for the value z. In particular, given 0 < α < 1, we denote cα as the number for which the interval (−cα, cα) contains a probability mass of 1 − α from the Tn−p-distribution. And so we derive a corresponding confidence interval Iα centered at β̂j where βj ∈ Iα with confidence of level of 1− α.\nOf particular importance is the quantity t0 def = t(0) =\nβ̂j √ n−p ‖ζ‖ √\n(XTX)−1j,j\n,since if there is no correlation between xj\nand y then the likelihood of seeing β̂j depends on the ratio of its magnitude to its standard deviation. As mentioned earlier, since Tk k→∞→ N (0, 1), then rather than viewing\nthis t0 as sampled from a Tn−p-distribution, it is common to think of t0 as a sample from a normal GaussianN (0, 1). This allows us to associate t0 with a p-value, estimating the event “βj and β̂j have different signs.” Specifically, given α ∈ (0, 1/2), we α-reject the null hypothesis if p0 < α. Let τα be the number s.t. Φ(τα) = ∫∞ τα 1√ 2π e−x\n2/2dx = α. This means we α-reject the null hypothesis when |t0| > τα. We now lower bound the number of i.i.d sample points needed in order to α-reject the null hypothesis. This bound is our basis for comparison between standard OLS and the differentially private version.5\nTheorem 2.2. Fix any positive definite matrix Σ ∈ Rp×p and any ν ∈ (0, 12 ). Fix parameters β ∈ R p and σ2 and a coordinate j s.t. βj 6= 0. Let X be a matrix whose n rows are i.i.d samples from N (0,Σ), and y be a vector where yi − (Xβ)i is sampled i.i.d from N (0, σ2). Fix α ∈ (0, 1). Then w.p. ≥ 1 − α − ν we have that OLS’s (1 − α)-confidence interval has length O(cα √ σ2/(nσmin(Σ))) provided n ≥ C1(p + ln(1/ν)) for some sufficiently large constant C1. Furthermore, there exists a constant C2 such that w.p. ≥ 1 − α − ν OLS (correctly) rejects the null hypothesis provided n ≥ max { C1(p+ ln(1/ν)), p+ C2 σ2\nβ2j · c\n2 α+τ 2 α\nσmin(Σ)\n} , where cα\nis the number for which ∫ cα −cα PDFTn−p(x)dx = 1− α."
    }, {
      "heading" : "3. OLS over Projected Data",
      "text" : "In this section we deal with the output of Algorithm 1 in the special case where Algorithm 1 outputs matrix unaltered and so we work with RA.\nTo clarify, the setting is as follows. We denote A = [X;y] the column-wise concatenation of the (n× (d− 1))-matrix X with the n-length vector y . (Clearly, we can denote any column of A as y and any subset of the remaining columns as the matrix X .) We therefore denote the output RA = [RX;Ry] and for simplicity we denote M = RX and p = d − 1. We denote the SVD decomposition of X = UΣV T. So U is an orthonormal basis for the columnspan of X and as X is full-rank V is an orthonormal basis for Rp. Finally, in our work we examine the linear regression problem derived from the projected data. That is, we denote\nβ̃ = (XTRTRX)−1(RX)T(Ry) = β + (RX)+Re (3)\nσ̃2 = r\nr − p ‖ζ̃‖2 , with ζ̃ = 1√ r Ry − 1√ r (RX)β̃ (4)\nWe now give our main theorem, for estimating the t-values based on β̃ and σ̃.\n5Theorem 2.2 also illustrates how we “separate” the two sources of privacy. In this case, ν bounds the probability of bad events that depend to sampling the rows of X , and α bounds the probability of a bad event that depends on the sampling of the y coordinates.\nTheorem 3.1. Let X be a (n × p)-matrix, and parameters β ∈ Rp and σ2 are such that we generate the vector y = Xβ + e with each coordinate of e sampled independently from N (0, σ2). Assume Algorithm 1 projects the matrix A = [X;y] without altering it. Fix ν ∈ (0, 1/2) and r = p + Ω(ln(1/ν)). Fix coordinate j. Then we have that w.p. ≥ 1 − ν deriving β̃ and σ̃2 as in Equations (3) and (4), the pivot quantity t̃(βj) =\nβ̃j−βj σ̃ √\n(XTRTRX)−1j,j\nhas a\ndistribution D satisfying e−aPDFTr−p(x) ≤ PDFD(x) ≤ eaPDFTr−p(e\n−ax) for any x ∈ R, where we denote a = r−p n−p .\nThe implications of Theorem 3.1 are immediate: all estimations one can do based on the t-values from the true data X,y , we can now do based on t̃ modulo an approximation factor of exp( r−pn−p ). In particular, Theorem 3.1 enables us to deduce a corresponding confidence interval based on β̃ .\nCorollary 3.2. In the same setting as in Theorem 3.1, w.p. ≥ 1− ν we have the following. Fix any α ∈ (0, 12 ). Let c̃α denote the number s.t. the interval (c̃α,∞) contains α2 e −a\nprobability mass of the Tr−p-distribution. Then Pr[βj ∈( β̃j ± ea · c̃α · σ̃ √ (XTRTRX)−1j,j ) ] ≥ 1− α. 6\nWe compare the confidence interval of Corollary 3.2 to the confidence interval of the standard OLS model, whose length is cα ‖ζ‖√ n−p √ (XTX)−1j,j . As R is a JLmatrix, known results regarding the JL transform give\nthat ‖ζ̃‖ = Θ (‖ζ‖), and that √\n(r − p)(XTRTRX)−1j,j = Θ (√\n(XTX)−1j,j\n) . We therefore have that\nσ̃ √ (XTRTRX)−1j,j = ‖ζ̃‖ √ r√\nr−p √ (XTRTRX)−1j,j =√\nr·(n−p) (r−p)2 · Θ ( ‖ζ‖√ n−p √ (XTX)−1j,j ) . So for values of r for which rr−p = Θ(1) we get that the confidence interval\nof Theorem 3.1 is a factor of Θ ( c̃α cα √ n−p r−p ) -larger than the standard OLS confidence interval. Observe that when α = Θ(1), which is the common case, the dominating factor is √ (n− p)/(r − p). This bound intuitively makes sense: we have contracted n observations to r observations, hence our model is based on confidence intervals derived from Tr−p rather than Tn−p.\nIn the supplementary material we give further discussion, in which we compare our work to the more straight-forward bounds one gets by “plugging in” Sarlos’ work (2006); and we also compare ourselves to the bounds derived from alternative works in differentially private linear regression.\n6Moreover, this interval is essentially optimal: denote d̃α s.t the interval (d̃α,∞) contains α2 e r−p n−p prob-\nability mass of the Tr−p-distribution. Then Pr[βj ∈( β̃j ± d̃α · σ̃ √ (XTRTRX)−1j,j ) ] ≤ 1− α.\nRejecting the Null Hypothesis. Due to Theorem 3.1, we can mimic OLS’ technique for rejecting the null hypothesis. I.e., we denote t̃0 = β̃j\nσ̃ √\n(XTRTRX)−1j,j\nand re-\nject the null-hypothesis if indeed the associated p̃0, denoting p-value of the slightly truncated e− r−p n−p t̃0, is below α · e− r−p n−p . Much like Theorem 2.2 we now establish a lower bound on n so that w.h.p we end up (correctly) rejecting the null-hypothesis.\nTheorem 3.3. Fix a positive definite matrix Σ ∈ Rp×p. Fix parameters β ∈ Rp and σ2 > 0 and a coordinate j s.t. βj 6= 0. Let X be a matrix whose n rows are sampled i.i.d from N (0p,Σ). Let y be a vector s.t. yi − (Xβ)i is sampled i.i.d from N (0, σ2). Fix ν ∈ (0, 1/2) and α ∈ (0, 1/2). Then there exist constants C1, C2, C3 and C4 such that when we run Algorithm 1 over [X;y] with parameter r w.p. ≥ 1 − α − ν we (correctly) reject the null hypothesis using p̃0 (i.e., Algorithm 1 returns matrix unaltered and we can estimate t̃0 and verify that indeed p̃0 < α · e − r−pn−p ) provided\nr ≥ p + max { C1 σ2(c̃2α+τ̃ 2 α)\nβ2jσmin(Σ) , C2 ln(1/ν)\n} , and n ≥\nmax { r, C3\nw2 min{σmin(Σ),σ2} , C4p ln(1/ν) }\nwhere\nc̃α, τ̃α defined s.t. PrX∼Tr−p [X > c̃α/e r−p n−p ] = PrX∼N (0,1)[X > τ̃α/e r−p n−p ] = α2 e − r−pn−p ."
    }, {
      "heading" : "3.1. Setting the Value of r, Deriving a Bound on n",
      "text" : "Comparing the lower bound on n given by Theorem 3.3 to the bound of Theorem 2.2, we have that the data-dependent bound of Ω ( (c̃α+τ̃α) 2σ2\nβ2jσmin(Σ)\n) should now hold for r rather than\nn. Yet, Theorem 3.3 also introduces an additional dependency between n and r: we require n = Ω(w 2\nσ2 + w2 σmin(Σ) )\n(since otherwise we do not have σmin(A) w and Algorithm 1 might alterA before projecting it) and by definition w2 is proportional to √ r ln(1/δ)/ . This is precisely the focus of our discussion in this subsection. We would like to set r’s value as high as possible — the larger r is, the more observations we have in RA and the better our confidence bounds (that depend on Tr−p) are — while satisfying n = Ω( √ r\n·min{σ2,σmin(Σ)} ).\nRecall that if each sample point is drawn i.i.d x ∼ N (0p,Σ), then each sample (xi ◦ yi) is sampled from N (0p+1,ΣA) for ΣA defined in the proof of Theorem 3.3,\nthat is: ΣA = (\nΣ Σβ\nβTΣ σ2+βTΣβ\n) . So, Theo-\nrem 3.3 gives the lower bound r − p = Ω ( σ2(c̃α+τ̃α) 2\nβ2jσmin(Σ) ) and the following lower bounds on n: n ≥ r and\nn = Ω\n( B2( √ r ln(1/δ)+ln(1/δ))\nσmin(ΣA)\n) , which means r =\nmin { n,\n2σ2min(ΣA) B4 ln(1/δ) (n− ln(1/δ))\n2 }\n. This discussion culminates in the following corollary.\nCorollary 3.4. Denoting L̃B2.2 = σ 2(c̃α+τ̃α) 2\nβ2jσmin(Σ) , we thus conclude that if n − p ≥ Ω ( L̃B2.2 ) and n =\nΩ ( B2 ln(1/δ) σmin(ΣA) · √ L̃B2.2 ) , then the result of Theorem 3.3\nholds by setting r = min { n,\n2σ2min(ΣA) B4 ln(1/δ) (n− ln(1/δ))\n2 }\n.\nIt is interesting to note that when we know ΣA, we also have a bound on B. Recall ΣA, the variance of the Gaussian (x ◦ y). Since every sample is an independent draw from N (0p+1,ΣA) then we have an upper bound of B2 ≤ log(np)σmax(ΣA). So our lower bound on n (using κ(ΣA) to denote the condition number of ΣA) is given by\nn ≥ max { Ω ( L̃B2.2 ) , Ω̃ ( κ(ΣA) ln(1/δ) · √ L̃B2.2 )} .\nObserve, overall this result is similar in nature to many other results in differentially private learning (Bassily et al., 2014) which are of the form “without privacy, in order to achieve a total loss of ≤ η we have a sample complexity bound of some Nη; and with differential privacy the sample complexity increases to Nη + Ω( √ Nη/ ).” However,\nthere’s a subtlety here worth noting. L̃B2.2 is proportional to 1σmin(ΣA) but not to κ(ΣA) = σmax(ΣA) σmin(ΣA)\n. The additional dependence on σmax follows from the fact that differential privacy adds noise proportional to the upper bound on the norm of each row."
    }, {
      "heading" : "4. Projected Ridge Regression",
      "text" : "We now turn to deal with the case that our matrix does not pass the if-condition of Algorithm 1. In this case, the matrix is appended with a d × d-matrix which is wId×d. De-\nnoting A′ = [\nA w · Id×d\n] we have that the algorithm’s\noutput is RA′. Similarly to before, we are going to denote d = p+ 1 and decompose A = [X;y] with X ∈ Rn×p and y ∈ Rn, with the standard assumption of y = Xβ + e and ei sampled i.i.d from N (0, σ2). We now need to introduce some additional notation. We denote the appended matrix and vectors X ′ and y ′ s.t. A′ = [X ′;y ′]. And so, using the output RA′ of Algorithm 1, we solve the linear regression problem derived from 1√\nr RX ′ and 1√ r Ry ′. I.e., we set\nβ ′ = (X ′TRTRX ′)−1(RX ′)T(Ry ′) ζ ′ = 1√\nr (Ry ′ −RX ′β ′) (5)\nSarlos’ results (2006) regarding the Johnson Lindenstrauss transform give that, when R has sufficiently many rows, solving the latter optimization problem gives a good approximation for the solution of the optimization problem βR = arg minz ‖y ′ − X ′z‖2 = arg minz ( ‖y −Xz‖2 + w2‖z‖2 ) . The latter problem is\nknown as the Ridge Regression problem. Invented in the 60s (Tikhonov, 1963; Hoerl & Kennard, 1970), Ridge Regression is often motivated from the perspective of penalizing linear vectors whose coefficients are too large. It is also often applied in the case where X doesn’t have full rank or is close to not having full-rank: one can show that the minimizer βR = (XTX + w2Ip×p)−1XTy is the unique solution of the Ridge Regression problem and that the RHS is always well-defined.\nWhile the solution of the Ridge Regression problem might have smaller risk than the OLS solution, it is not known how to derive t-values and/or reject the null hypothesis under Ridge Regression (except for using X to manipulate βR back into β̂ = (XTX)−1XTy and relying on OLS). In fact, prior to our work there was no need for such analysis! For confidence intervals one could just use the standard OLS, because access to X and y was given.\nTherefore, much for the same reason, we are unable to derive t-values under projected Ridge Regression.7 Clearly, there are situations where such confidence bounds simply cannot be derived.Nonetheless, under additional assumptions about the data, our work can give confidence intervals for βj , and in the case where the interval doesn’t intersect the origin — assure us that sign(β′j) = sign(βj) w.h.p. This is detailed in the supplementary material.\nTo give an overview of our analysis, we first discuss a model where e = y − Xβ is fixed (i.e., the data is fixed and the algorithm is the sole source of randomness), and prove that in this model β′ is as an approximation to β̂ .\nTheorem 4.1. Fix X ∈ Rn×p and y ∈ R. Define β̂ = X+y and ζ = (I − XX+)y . Let RX ′ = M ′ and Ry ′ denote the result of applying Algorithm 1 to the matrixA = [X;y] when the algorithm appends the data with a w · I matrix. Fix a coordinate j and any α ∈ (0, 1/2). When computing β ′ and ζ ′ as in (5), we have that w.p. ≥ 1 − α it holds that β̂j ∈ ( β′j ± c′α‖ζ ′‖ √ r r−p · (M ′TM ′) −1 j,j\n) where c′α denotes the number such that (−c′α, c′α) contains 1− α mass of the Tr−p-distribution.\nHowever, our goal remains to argue that β′j serves as a good approximation for βj . To that end, we combine the standard OLS confidence interval — which says that w.p. ≥ 1−α over the randomness of picking e in the homoscedas-\ntic model we have |βj − β̂j | ≤ cα‖ζ‖ √ (XTX)−1j,j n−p — with the confidence interval of Theorem 4.1 above, and denoting I = cα ‖ζ‖√ n−p √ (XTX)−1j,j + c ′ α ‖ζ ′‖√ r−p √ r(M ′TM ′)−1j,j we have that Pr[|β′j − βj | = O(I)] ≥ 1 − α. And 7Note: The naı̈ve approach of using RX ′ and Ry ′ to interpolate RX and Ry and then apply Theorem 3.1 using these estimations of RX and Ry ignores the noise added from appending the matrix A into A′, and therefore leads to inaccurate estimations of the t-values.\nso, in summary, in Section C we give conditions under which the length of the interval I is dominated by the c′α ‖ζ ′‖√ r−p √ r(M ′TM ′)−1j,j factor derived from Theorem 4.1."
    }, {
      "heading" : "5. Confidence Intervals for “Analyze Gauss”",
      "text" : "In this section we analyze the “Analyze Gauss” algorithm of Dwork et al (2014). Algorithm 2 works by adding random Gaussian noise to ATA, where the noise is symmetric with each coordinate above the diagonal sampled i.i.d from N (0,∆2) with ∆2 = O ( B4 log(1/δ) 2 ) . Using the same no-\ntation for a sub-matrix of A as [X;y] as before, we denote\nthe output of Algorithm 2 as  X̃TX X̃Ty ỹTX ỹTy . Thus, we approximate β and ‖ζ‖ by β̃ = ( X̃TX )−1 X̃Ty and\n‖̃ζ‖2 = ỹTy − 2 ỹTX β̃ + β̃ T X̃TX β̃ resp. We now argue\nthat it is possible to use β̃j and ‖̃ζ‖2 to get a confidence interval for βj under certain conditions. Theorem 5.1. Fix α, ν ∈ (0, 12 ). Assume that there exists η ∈ (0, 12 ) s.t. σmin(X TX) > ∆ √ p ln(1/ν)/η. Under the homoscedastic model, given β and σ2, if we assume also that ‖β‖ ≤ B and ‖β̂‖ = ‖(XTX)−1XTy‖ ≤ B, then w.p. ≥ 1− α− ν it holds that\n∣∣∣βj − β̃j∣∣∣ is at most O ( ρ · √( X̃TX −1 j,j + ∆ √ p ln(1/ν) · X̃TX −2 j,j ) ln(1/α)\n+ ∆ √ X̃TX −2 j,j · ln(1/ν) · (B √ p+ 1) ) where ρ is w.h.p an upper bound on σ (details appear in the Supplementary material).\nNote that the assumptions that ‖β‖ ≤ B and ‖β̂‖ ≤ B are fairly benign once we assume each row has bounded l2-norm. The key assumption is that XTX is well-spread. Yet in the model where each row in X is sampled i.i.d fromN (0,Σ), this assumption merely means that n is large enough — namely, that n = Ω̃(∆ √ p ln(1/ν)\nη·σmin(Σ) ).\n6. Experiment: t-Values of Output\nGoal. We set to experiment with the outputs of Algorithms 1 and 2. While Theorem 3.1 guarantees that computing the t-value from the output of Algorithm 1 in the matrix unaltered case does give a good approximation of the t-value – we were wondering if by computing the t-value directly from the output we can (a) get a good approximation of the true (non-private) t-value and (b) get the same “higher-level conclusion” of rejecting the nullhypothesis. The answers are, as ever, mixed. The two main\nobservations we do notice is that both algorithms improve as the number of examples increases, and that Algorithm 1 is more conservative then Algorithm 2.\nSetting. We tested both algorithms in two settings. The first is over synthetic data. Much like the setting in Theorems 2.2 and 3.3, X was generated using p = 3 independent normal Gaussian features, and y was generated using the homoscedastic model. We chose β = (0.5,−0.25, 0) so the first coordinate is twice as big a the second but of opposite sign, and moreover, y is independent of the 3rd feature. The variance of the label is also set to 1, and so the variance of the homosedastic noise equals to σ2 = 1− (0.5)2 − (−0.25)2. The number of observations n ranges from n = 1000 to n = 100000.\nThe second setting is over real-life data. We ran the two algorithms over diabetes dataset collected over ten years (1999-2008) taken from the UCI repository (Strack et al., 2014). We truncated the data to 4 attributes: sex (binary), age (in buckets of 10 years), number medications (numeric, 0-100), and a diagnosis (numeric, 0-1000). Naturally, we added a 5th column of all-1 (intercept). Omitting any entry with missing or non-numeric values on these nine attributes we were left with N = 91842 entries, which we shuffled and fed to the algorithm in varying sizes — from n = 30, 000 to n = 90, 000. Running OLS over the entire N observation yields β ≈ (14.07, 0.54,−0.22, 482.59), and t-Values of (10.48, 1.25,−2.66, 157.55).\nThe Algorithms. We ran a version of Algorithm 1 that uses a DP-estimation of σmin, and finds the largest r the we can use without altering the input, yet if this r is below 25 then it does alter the input and approximates Ridge regression. We ran Algorithm 2 verbatim. We set = 0.25 and δ = 10−6. We repeated each algorithm 100 times.\nResults. We plot the t-values we get from Algorithms 1 and 2 and decide to reject the null-hypothesis based on tvalue larger than 2.8 (which corresponds to a fairly conservative p-value of 0.005). Not surprisingly, as n increases, the t-values become closer to their expected value – the tvalue of Analyze Gauss is close to the non-private t-value and the t-value from Algorithm 1 is a factor of √ r n smaller as detailed above (see after Corollary 3.2). As a result, when the null-hypothesis is false, Analyze Gauss tends to produce larger t-values (and thus reject the null-hypothesis) for values of n under which Algorithm 1 still does not reject, as shown in Figure 1a. This is exacerbated in real data setting, where its actual least singular value (≈ 500) is fairly small in comparison to its size (N = 91842).\nHowever, what is fairly surprising is the case where the null-hypothesis should not be rejected — since βj = 0 (in the synthetic case) or its non-private t-value is close to 0 (in the real-data case). Here, the Analyze Gauss’ tvalue approximation has fairly large variance, and we still\nget fairly high (in magnitude) t-values. As the result, we falsely reject the null-hypothesis based on the t-value of Analyze Gauss quite often, even for large values of n. This is shown in Figure 1b. Additional figures (including plotting the distribution of the t-value approximations) appear in the supplementary material.\nThe results show that t-value approximations that do not take into account the inherent randomness in the DPalgorithms lead to erroneous conclusions. One approach would be to follow the more conservative approach we advocate in this paper, where Algorithm 1 may allow you to get true approximation of the t-values and otherwise reject the null-hypothesis only based on the confidence interval (of Algorithm 1 or 2) not intersecting the origin. Another approach, which we leave as future work, is to replace the T -distribution with a new distribution, one that takes into account the randomness in the estimator as well. This, however, has been an open and long-standing challenge since the first works on DP and statistics (see (Vu & Slavkovic, 2009; Dwork & Lei, 2009)) and requires we move into non-asymptotic hypothesis testing."
    }, {
      "heading" : "Acknowledgements",
      "text" : "The bulk of this work was done when the author was a postdoctoral fellow at Harvard University, supported by NSF grant CNS-123723; and also an unpaid collaborator on NSF grant 1565387. The author wishes to wholeheartedly thank Prof. Salil Vadhan, for his tremendous help in shaping this paper. The author would also like to thank Prof. Jelani Nelson and the members of the “Privacy Tools for Sharing Research Data” project at Harvard University (especially James Honaker, Vito D’Orazio, Vishesh Karwa, Prof. Kobbi Nissim and Prof. Gary King) for many helpful discussions and suggestions; as well as Abhradeep Thakurta for clarifying the similarity between our result. Lastly the author thanks the anonymous referees for many helpful suggestions in general and for a reference to (Ullman, 2015) in particular."
    }, {
      "heading" : "A. Extended Introductory Discussion",
      "text" : "Due to space constraint, a few details from the introductory parts (Sections 1,2) were omitted. We bring them in this appendix. We especially recommend the uninformed reader to go over the extended OLS background we provide in Appendix A.3."
    }, {
      "heading" : "A.1. Proof Of Privacy of Algorithm 1",
      "text" : "Theorem A.1. Algorithm 1 is ( , δ)-differentially private.\nProof. The proof of the theorem is based on the fact the Algorithm 1 is the result of composing the differentially private Propose-Test-Release algorithm of (Dwork & Lei, 2009) with the differentially private analysis of the Johnson-Lindenstrauss transform of (Sheffet, 2015).\nMore specifically, we use Theorem B.1 from (Sheffet, 2015) that states that given a matrix A whose all of its singular values at greater than T ( , δ) where T ( , δ)2 = 2B2 (√ 2r ln(4/δ) + 2 ln(4/δ) ) , publishing RA is ( , δ)-\ndifferentially private for a r-row matrix R whose entries sampled are i.i.d normal Gaussians. Since we have that all of the singular values of A′ are greater than w (as specified in Algorithm 1), outputtingRA′ is ( /2, δ/2)-differentially private. The rest of the proof boils down to showing that (i) the if-else-condition is ( /2, 0)-differentially private and that (ii) w.p. ≤ δ/2 any matrix A whose smallest singular value is smaller than w passes the if-condition (step 3). If both these facts hold, then knowing whether we pass the if-condition or not is ( /2)-differentially private and the output of the algorithm is ( /2, δ)-differentially private, hence basic composition gives the overall bound of ( , δ)differential privacy.\nTo prove (i) we have that for any pair of neighboring matricesA andB that differ only on the i-th row, denoted ai and bi resp., we have BTB − bibTi = ATA − aiaTi . Applying Weyl’s inequality we have\nσmin(B TB) ≤ σmin(BTB − bibTi ) + σmax(bibTi )\n≤ σmin(ATA) + σmax(aiaTi ) + σmax(bibTi ) ≤ σmin(ATA) + 2B2\nhence |σmin(A)2−σmin(B)2| ≤ 2B2, so addingLap( 4B 2\n) is ( /2)-differentially private.\nTo prove (ii), note that by standard tail-bounds on the Laplace distribution we have that Pr[Z < − 4B2 ln(1/δ)\n] ≤ δ 2 . Therefore, w.p. 1 − δ/2 it holds that\nany matrix A that passes the if-test of the algorithm must have σmin(A)2 > w2. Also note that a similar argument shows that for any 0 < β < 1, any matrix A s.t. σmin(A) 2 > w2 + 4B 2 ln(1/β) passes the if-condition of the algorithm w.p. 1− β."
    }, {
      "heading" : "A.2. Omitted Preliminary Details",
      "text" : "Linear Algebra and Pseudo-Inverses. Given a matrix M we denote its SVD as M = USV T with U and V being orthonormal matrices and S being a non-negative diagonal matrix whose entries are the singular values of M . We use σmax(M) and σmin(M) to denote the largest and smallest singular value resp. Despite the risk of confusion, we stick to the standard notation of using σ2 to denote the variance of a Gaussian, and use σj(M) to denote the j-th singular value of M . We use M+ to denote the Moore-Penrose inverse of M , defined as M+ = V S−1UT where S−1 is a matrix with S−1j,j = 1/Sj,j for any j s.t. Sj,j > 0.\nThe Gaussian Distribution. A univariate Gaussian N (µ, σ2) denotes the Gaussian distribution whose mean is µ and variance σ2, with PDF(x) = ( √\n2πσ2)−1 exp(−x−µ2σ2 ). Standard concentration bounds on Gaussians give that Pr[x > µ + 2σ √ ln(1/ν)] < ν for any ν ∈ (0, 1e ). A multivariate Gaussian N (µ,Σ) for some positive semi-definite Σ denotes the multivariate Gaussian distribution where the mean of the j-th coordinate is the µj and the co-variance between coordinates j and k is Σj,k. The PDF of such Gaussian is defined only on the subspace colspan(Σ), where for every x ∈ colspan(Σ) we have PDF(x) =(\n(2π)rank(Σ) · d̃et(Σ) )−1/2 exp ( − 12 (x −µ) TΣ+(x −µ) )\nand d̃et(Σ) is the multiplication of all non-zero singular values of Σ. A matrix Gaussian distribution denoted N (Ma×b, U, V ) has mean M , variance U on its rows and variance V on its columns. For full rank U and V it holds that PDFN (M,U,V )(X) = (2π)−ab/2(det(U))−b/2(det(V ))−a/2 · exp(− 12 trace ( V −1(X −M)TU−1(X −M) ) ). In our case, we will only use matrix Gaussian distributions with N (Ma×b, Ia×a, V ) and so each row in this matrix is an i.i.d sample from a b-dimensional multivariate Gaussian N ((M)j→, V ).\nWe will repeatedly use the rules regarding linear operations on Gaussians. That in, for any c, it holds that cN (µ, σ2) = N (c · µ, c2σ2). For any C it holds that C · N (µ,Σ) = N (Cµ,CΣCT). And for any C is holds thatN (M,U, V ) · C = N (MC,U,CTV C). In particular, for any c (which can be viewed as a b×1-matrix) it holds thatN (M,U, V ) · c = N (Mc,U,cTV c) = N (Mc,cTV c · U).\nWe will also require the following proposition.\nProposition A.2. Given σ2, λ2 s.t. 1 ≤ σ 2\nλ2 ≤ c 2 for some\nconstant c, letX and Y be two random Gaussians s.t. X ∼ N (0, σ2) and Y ∼ N (0, λ2). It follows that 1cPDFY (x) ≤ PDFX(x) ≤ cPDFcY (x) for any x. Corollary A.3. Under the same notation as in Proposition A.2, for any set S ⊂ R it holds that 1cPrx←Y [x ∈ S] ≤ Prx←X [x ∈ S] ≤ cPrx←cY [x ∈ S] =\ncPrx←Y [x ∈ S/c]\nProof. The proof is mere calculation.\nPDFX(x) PDFcY (x) =\n√ c2λ2\nσ2 ·\nexp(− x 2\n2σ2 )\nexp(− x 2\n2c2λ2 )\n≤ c · exp(x 2\n2 (\n1 c2λ2 − 1 σ2 )) ≤ c · exp(0) = c\nPDFX(x) PDFY (x) =\n√ λ2\nσ2 ·\nexp(− x 2\n2σ2 )\nexp(− x 2\n2λ2 )\n≥ c−1 exp(x 2\n2 ( 1 λ2 − 1 σ2 )) ≥ exp(0) c = c −1\nThe Tk-Distribution. The Tk-distribution, where k is referred to as the degrees of freedom of the distribution, denotes the distribution over the reals created by independently sampling Z ∼ N (0, 1) and ‖ζ‖2 ∼ χ2k, and taking the quantity Z√\n‖ζ‖2/k . Its PDF is given by\nPDFTk(x) ∝ ( 1 + x 2\nk )−k+12 . It is a known fact that as k increases, Tk becomes closer and closer to a normal Gaussian. The T -distribution is often used to determine suitable bounds on the rate of converges, as we illustrate in Section A.3. As the T -distribution is heavy-tailed, existing tail bounds on the T -distribution (which are of the form: if τν = C √ k((1/ν)2/k − 1) for some constant C\nthen ∫∞ τν\nPDFTk(x)dx < ν) are often cumbersome to work with. Therefore, in many cases in practice, it common to assume ν = Θ(1) (most commonly, ν = 0.05) and use existing tail-bounds on normal Gaussians.\nDifferential Privacy facts. It is known (Dwork et al., 2006b) that if ALG outputs a vector in Rd such that for any A and A′ it holds that ‖ALG(A) − ALG(A′)‖1 ≤ B, then adding Laplace noise Lap(1/ ) to each coordinate of the output of ALG(A) satisfies -differential privacy. Similarly, (2006b) showed that if for any neighboring A and A′ it holds that ‖ALG(A)−ALG(A′)‖22 ≤ ∆2 then adding Gaussian noise N (0,∆2 · 2 ln(2/δ) 2 ) to each coordinate of the output of ALG(A) satisfies ( , δ)-differential privacy.\nAnother standard result (Dwork et al., 2006a) gives that the composition of the output of a ( 1, δ1)-differentially private algorithm with the output of a ( 2, δ2)-differentially private algorithm results in a ( 1+ 2, δ1+δ2)-differentially private algorithm."
    }, {
      "heading" : "A.3. Detailed Background on Ordinary Least Squares",
      "text" : "For the unfamiliar reader, we give a short description of the model under which OLS operates as well as the confidence bounds one derives using OLS. This is by no means an ex-\nhaustive account of OLS and we refer the interested reader to (Rao, 1973; Muller & Stewart, 2006).\nGiven n observations {(xi, yi)}ni=1 where for all i we have xi ∈ Rp and yi ∈ R, we assume the existence of a pdimensional vector β ∈ Rp s.t. the label yi was derived by yi = β\nTxi + ei where ei ∼ N (0, σ2) independently (also known as the homoscedastic Gaussian model). We use the matrix notation whereX denotes the (n×p)-matrix whose rows are xi, and use y,e ∈ Rn to denote the vectors whose i-th entry is yi and ei resp. To simplify the discussion, we assume X has full rank.\nThe parameters of the model are therefore β and σ2, which we set to discover. To that end, we minimize minz ‖y − Xz‖2 and solve\nβ̂ = (XTX)−1XTy = (XTX)−1XT(Xβ+e) = β+X+e\nAs e ∼ N (0n, σ2In×n), it holds that β̂ ∼ N (β, σ2(XTX)−1), or alternatively, that for every coordinate j it holds that β̂j = eTj β̂ ∼ N (βj , σ2(XTX) −1 j,j ). Hence we get β̂j−βj σ √\n(XTX)−1j,j\n∼ N (0, 1). In addition, we de-\nnote the vector\nζ = y−Xβ̂ = (Xβ +e)−X(β +X+e) = (I −XX+)e\nand since XX+ is a rank-p (symmetric) projection matrix, we have ζ ∼ N (0, σ2(I − XX+)). Therefore, ‖ζ‖2 is equivalent to summing the squares of (n− p) i.i.d samples from N (0, σ2). In other words, the quantity ‖ζ‖2/σ2 is sampled from a χ2-distribution with (n − p) degrees of freedom.\nWe sidetrack from the OLS discussion to give the following bounds on the l2-distance between β and β̂ , as the next claim shows.\nClaim A.4. For any 0 < ν < 1/2, the following holds w.p. ≥ 1−ν over the randomness of the model (the randomness over e)\n‖β − β̂‖2 = ‖X+e‖2 = O ( σ2 log(p/ν) · ‖X+‖2F ) (6)\n‖β̂‖2 = ‖β +X+e‖2 = O( ( ‖β‖+ σ · ‖X+‖F · √ log(p/ν) )2 )∣∣∣ 1n−p‖ζ‖2 − σ2∣∣∣ = O(√ ln(1/ν)n−p )\nProof. Since e ∼ N (0n, σ2In×n) then X+e ∼ N (0n, σ2(XTX)−1). Denoting the SVD decomposition (XTX)−1 = V SV T with S denoting the diagonal matrix whose entries are σ−2max(X), . . . , σ −2 min(X), we have that V TX+e ∼ N (0n, σ2S). And so, each coordinate of V TX+e is distributed like an i.i.d Gaussian. So\nw.p. ≥ 1 − ν/2 non of these Gaussians is a factor of O(σ √ ln(p/ν)) greater than its standard deviation. And so w.p. ≥ 1 − ν/2 it holds that ‖X+e‖2 = ‖V TX+e‖2 ≤ O(σ2 log(p/ν) (∑ i σ −2 i (X) ) ). Since ∑ i σ −2 i (X) = trace((XTX)−1) = trace(X+(X+)T) = ‖X+‖2F , the bound of (6) is proven.\nThe bound on ‖β̂‖2 is an immediate corollary of (6) using the triangle inequality.8 The bound on ‖ζ‖2 follows from tail bounds on the χ2n−p distribution, as detailed in Section 2.\nReturning to OLS, it is important to note that β̂ and ζ are independent of one another. (Note, β̂ depends solely on X+e = (X+X)X+e = X+PUe, whereas ζ depends on (I −XX+)e = PU⊥e. As e is spherically symmetric, the two projections are independent of one another and so β̂ is independent of ζ .) As a result of the above two calculations, we have that the quantity\ntβ̂j (βj) def = β̂j−βj√ (XTX)−1j,j · ‖ζ‖√ n−p = β̂j−βj σ √ (XTX)−1j,j\n/ ‖ζ‖\nσ √ n−p\nis distributed like a T -distribution with (n − p) degrees of freedom. Therefore, we can compute an exact probability estimation for this quantity. That is, for any measurable S ⊂ R we have Pr [ β̂ and ζ satisfying tβ̂j (βj) ∈ S ] = ∫ S PDFTn−p(x)dx\nThe importance of the t-value t(βj) lies in the fact that it can be fully estimated from the observed data X and y (for any value of βj), which makes it a pivotal quantity. Therefore, given X and y , we can use t(βj) to describe the likelihood of any βj — for any z ∈ R we can now give an estimation of how likely it is to have βj = z (which is PDFTn−p(t(z))). The t-values enable us to perform multitude of statistical inferences. For example, we can say which of two hypotheses is more likely and by how much (e.g., we are 5-times more likely that the hypothesis βj = 3 is true than the hypothesis βj = 14 is true); we can compare between two coordinates j and j′ and report we are more confident that βj > 0 than βj′ > 0; or even compare among the t-values we get across multiple datasets (such as the datasets we get from subsampling rows from a single dataset).\nIn particular, we can use t(βj) to α-reject unlikely values of βj . Given 0 < α < 1, we denote cα as the number for which the interval (−cα, cα) contains a probability mass of 1 − α from the Tn−p-distribution. And so we derive a\n8Observe, though e is spherically symmetric, and is likely to be approximately-orthogonal to β , this does not necessarily hold for X+e which isn’t spherically symmetric. Therefore, we result to bounding the l2-norm of β̂ using the triangle bound.\ncorresponding confidence interval Iα centered at β̂j where βj ∈ Iα with confidence of level of 1− α.\nWe comment as to the actual meaning of this confidence interval. Our analysis thus far applied w.h.p to a vector y derived according to this model. Such X and y will result in the quantity tβ̂j (βj) being distributed like a Tn−pdistribution — where βj is given as the model parameters and β̂j is the random variable. We therefore have that guarantee that for X and y derived according to this model, the\nevent Eα def = β̂j ∈ ( βj ± cα · √ (XTX)−1j,j · ‖ζ‖2 n−p ) hap-\npens w.p. 1 − α. However, the analysis done over a given dataset X and y (once y has been drawn) views the quantity tβ̂j (βj) with β̂j given and βj unknown. Therefore the event Eα either holds or does not hold. That is why the alternative terms of likelihood or confidence are used, instead of probability. We have a confidence level of 1 − α that indeed βj ∈ β̂j±cα · √ (XTX)−1j,j · ‖ζ‖2 n−p , because this event does happen in 1−α fraction of all datasets generated according to our model.\nRejecting the Null Hypothesis. One important implication of the quantity t(βj) is that we can refer specifically to the hypothesis that βj = 0, called the null hypothesis. This quantity, t0 def = tβ̂j (0) = β̂j √ n−p ‖ζ‖ √\n(XTX)−1j,j\n, represents how\nlarge is β̂j relatively to the empirical estimation of standard deviation σ. Since it is known that as the number of degrees of freedom of a T -distribution tends to infinity then the T - distribution becomes a normal Gaussian, it is common to think of t0 as a sample from a normal Gaussian N (0, 1). This allows us to associate t0 with a p-value, estimating the event “βj and β̂j have different signs.” Formally, we define p0 = ∫∞ |t0| 1√ 2π e−x\n2/2dx. It is common to reject the null hypothesis when p0 is sufficiently small (typically, below 0.05).9\nSpecifically, given α ∈ (0, 1/2), we say we α-reject the null hypothesis if p0 < α. Let τα be the number s.t. Φ(τα) = ∫∞ τα 1√ 2π e−x 2/2dx = α. (Standard bounds\ngive that τα < 2 √\nln(1/α).) This means we α-reject the null hypothesis if t0 > τα or t0 < −τα, meaning if |β̂j | > τα √ (XTX)−1j,j ‖ζ‖√ n−p .\nWe can now lower bound the number of i.i.d sample points needed in order to α-reject the null hypothesis. This bound will be our basis for comparison — between standard OLS and the differentially private version.10 9Indeed, it is more accurate to associate with t0 the value∫∞ |t0|\nPDFTn−p(x)dx and check that this value is < α. However, as most uses take α to be a constant (often α = 0.05), asymptotically the threshold we get for rejecting the null hypothesis are the same.\n10This theorem is far from being new (except for maybe fo-\nTheorem A.5 (Theorem 2.2 restated.). Fix any positive definite matrix Σ ∈ Rp×p and any ν ∈ (0, 12 ). Fix parameters β ∈ Rp and σ2 and a coordinate j s.t. βj 6= 0. Let X be a matrix whose n rows are i.i.d samples from N (0,Σ), and y be a vector where yi − (Xβ)i is sampled i.i.d from N (0, σ2). Fix α ∈ (0, 1). Then w.p. ≥ 1 − ν we have that the (1 − α)-confidence interval is of length O(cα √ σ2/(nσmin(Σ))) provided n ≥ C1(p + ln(1/ν)) for some sufficiently large constant C1. Furthermore, there exists a constant C2 such that w.p. ≥ 1 − α − ν we (correctly) reject the null hypothesis provided\nn ≥ max { C1(p+ ln(1/ν)), C2 σ2\nβ2j · c\n2 α + τ 2 α\nσmin(Σ)\n}"
    }, {
      "heading" : "Here cα denotes the number for which∫ cα",
      "text" : "−cα PDFTn−p(x)dx = 1 − α. (If we are content with approximating Tn−p with a normal Gaussian than one can set cα ≈ τα < 2 √ ln(1/α).)\nProof. The discussion above shows that w.p. ≥ 1 − α we have |βj − β̂j | ≤ cα √ (XTX)−1j,j ‖ζ‖2 n−p ; and in order to α-reject the null hypothesis we must have |β̂j | > τα √ (XTX)−1j,j ‖ζ‖2 n−p . Therefore, a sufficient condition for OLS to α-reject the null-hypothesis is to have n large\nenough s.t. |βj | > (cα + τα) √ (XTX)−1j,j ‖ζ‖2 n−p . We therefore argue that w.p.≥ 1− ν this inequality indeed holds.\nWe assume each row of X i.i.d vector xi ∼ N (0p,Σ), and recall that according to the model ‖ζ‖2 ∼ σ2χ2(n − p). Straightforward concentration bounds on Gaussians and on the χ2-distribution give: (i) W.p. ≤ α it holds that ‖ζ‖ > σ ( √ n− p+ 2 ln(2/α))). (This is part of the standard OLS analysis.) (ii) W.p. ≤ ν it holds that σmin(XTX) ≤ σmin(Σ)( √ n − ( √ p+ √ 2 ln(2/ν)))2. (Rudelson & Vershynin, 2009) Therefore, due to the lower bound n = Ω(p + ln(1/ν)), w.p.≥ 1 − ν − α we have that none of these events hold. In such a case we have√\n(XTX)−1j,j ≤ √ σmax((XTX)−1) = O(\n1√ nσmin(Σ) )\nand ‖ζ‖ = O(σ √ n− p). This implies that the confidence interval of level 1 − α has length of cα √ (XTX)−1j,j · ‖ζ‖2 n−p = O ( cα √ σ2\nnσmin(Σ)\n) ; and that in\norder to α-reject that null-hypothesis it suffices to have |βj | = Ω ( (cα + τα) √ σ2\nnσmin(Σ)\n) . Plugging in the lower\nbound on n, we see that this inequality holds.\nWe comment that for sufficiently large constants C1, C2,\ncusing on the setting where every row in X is sampled from an i.i.d multivariate Gaussians), it is just stated in a non-standard way, discussing solely the power of the t-test in OLS. For further discussions on sample size calculations see (Muller & Stewart, 2006).\nit holds that all the constants hidden in the O- and Ωnotations of the proof are close to 1. I.e., they are all within the interval (1 ± η) for some small η > 0 given C1, C2 ∈ Ω(η−2)."
    }, {
      "heading" : "B. Projecting the Data using Gaussian Johnson-Lindenstrauss Transform",
      "text" : ""
    }, {
      "heading" : "B.1. Main Theorem Restated and Further Discussion",
      "text" : "Theorem B.1 (Theorem 3.1 restated.). Let X be a n × p matrix, and parameters β ∈ Rp and σ2 are such that we generate the vector y = Xβ + e with each coordinate of e sampled independently from N (0, σ2). Assume σmin(X) ≥ C · w and that n is sufficiently large s.t. all of the singular values of the matrix [X;y] are greater than C · w for some large constant C, and so Algorithm 1 projects the matrixA = [X;y] without altering it, and publishes [RX;Ry]. Fix ν ∈ (0, 1/2) and r = p + Ω(ln(1/ν)). Fix coordinate j. Then w.p. ≥ 1− ν we have that deriving β̃ , ζ̃ and σ̃2 as follows\nβ̃ = (XTRTRX)−1(RX)T(Ry) = β + (RX)+Re\nζ̃ = 1√ r Ry − 1√ r (RX)β̃\n= 1√ r\n( I − (RX)(XTRTRX)−1(RX)T) ) Re\nσ̃2 = r\nr − p ‖ζ̃‖2\nthen the pivot quantity\nt̃(βj) = β̃j − βj σ̃ √\n(XTRTRX)−1j,j\nhas a distribution D satisfying e−aPDFTr−p(x) ≤ PDFD(x) ≤ eaPDFTr−p(e−ax) for any x ∈ R, where we denote a = r−pn−p .\nComparison with Existing Bounds. Sarlos’ work (2006) utilizes the fact that when r, the numbers of rows in R, is large enough, then 1√\nr R is a Johnson-Lindenstrauss\nmatrix. Specifically, given r and ν ∈ (0, 1) we denote η = Ω(\n√ p ln(p) ln(1/ν)\nr ), and so r = O( p ln(p) ln(1/ν) η2 ).\nLet us denote β̃ = arg minz 1r‖RXz − Ry‖ 2. In this setting, Sarlos’ work (Sarlós, 2006) (Theorem 12(3)) guarantees that w.p. ≥ 1 − ν we have ‖β̂ − β̃‖2 ≤ η‖ζ‖/σmin(X) = O (√ p log(p) log(1/ν) rσmin(XTX) ‖ζ‖ ) . Naı̈vely bounding |β̂j − β̃j | ≤ ‖β̂ − β̃‖ and using the confidence interval for β̂j − βj from Section A.311\n11Where we approximate cα, the tail bound of the Tn−pdistribution with the tail bound on a Gaussian, i.e., use the approximation cα ≈ O( √ ln(1/α)).\ngives a confidence interval of level 1 − (α + ν) centered at β̃j with length of O (√ p ln(p) log(1/ν) rσmin(XTX) ‖ζ‖ ) +\nO (√ (XTX)−1j,j log(1/α) n−p ‖ζ‖ ) =\nO (√\np ln(p) log(1/ν)+log(1/α) rσmin(XTX)\n‖ζ‖ )\n. This implies that our confidence interval has decreased its degrees of freedom from n− p to roughly r/p ln(p), and furthermore, that it no longer depends on (XTX)−1j,j but rather on 1/σmin(X\nTX). It is only due to the fact that we rely on Gaussians and by mimicking carefully the original proof that we can deduce that the t̃-value has (roughly) r − p degrees of freedom and depends solely on (XTX)−1j,j .\n(In the worst case, we have that (XTX)−1j,j is proportional to σmin(XTX)−1, but it is not uncommon to have matrices where the former is much larger than the latter.) As mentioned in the introduction, alternative techniques ((Chaudhuri et al., 2011; Bassily et al., 2014; Ullman, 2015)) for finding a DP estimator βdp of the linear regression give a data-independent12 bound of ‖βdp − β̂‖ = Õ(p/ ). Such bounds are harder to compare with the interval length given by Corollary 3.2. Indeed, as we discuss in Section 3 under “Rejecting the null-hypothesis,” enough samples from a multivariate Gaussian whose covariance-matrix is well conditioned give a bound which is well below the worstupper bound of O(p/ ). (Yet, it is possible that these techniques also do much better on such “well-behaved” data.) What the works of Sarlos and alternative works regrading differentially private linear regression do not take into account are questions such as generating a likelihood for βj nor do they discuss rejecting the null hypothesis."
    }, {
      "heading" : "B.2. Proof of Theorem 3.1",
      "text" : "We now turn to our analysis of β̃ and ζ̃ , where our goal is to show that the distribution of the t̃-values as specified in Theorem 3.1 is well-approximated by the Tr−pdistribution. For now, we assume the existence of fixed vectors β ∈ Rp and e ∈ Rn s.t. y = Xβ + e. (Later, we will return to the homoscedastic model where each coordinate of e is sampled i.i.d from N (0, σ2) for some σ2.) In other words, we first examine the case where R is the sole source of randomness in our estimation. Based on the assumption that e is fixed, we argue the following.\nClaim B.2. In our model, given X and the output M = RX , we have that β̃ ∼ N ( β +X+e, ‖PU⊥e‖2(MTM)−1 ) and ζ̃ ∼\nN ( 0n, ‖P U⊥e‖ 2 r (Ir×r −M(M TM)−1MT) ) . Where PU⊥ denotes the projection operator onto the subspace orthogonal to colspan(X); i.e., PU = XX+ and PU⊥ = (Ir×r −XX+).\n12In other words, independent of X,ζ .\nProof. The matrix R is sampled from N (0r×p, Ir×r, Ip×p). Given X and RX = M , we learn the projection of each row in R onto the subspace spanned by the columns of X . That is, denoting uT as the i-th row of R and vT as the i-th row of M , we have that XTu = v . Recall, initially u ∼ N (0n, In×n) – a spherically symmetric Gaussian. As a result, we can denote u = PUu × PU⊥u where the two projections are independent samples from N (0n, PU ) and N (0n, PU⊥) resp. However, once we know that v = XTu we have that PUu = X(X\nTX)−1XTu = X(XTX)−1v so we learn PUu exactly, whereas we get no information about PU⊥ so PU⊥u is still sampled from a Gaussian N (0n, PU⊥). As we know for each row of R that uTPU = vTX+, we therefore have that\nR = RPU +RPU⊥ = MX + +RPU⊥\nwhere RPU⊥ ∼ N (0r×n, Ir×r, PU⊥). From here on, we just rely on the existing results about the linearity of Gaussians.\nR ∼ N (MX+, Ir×r, PU⊥) ⇒ Re ∼ N (MX+e, ‖PU⊥e‖2Ir×r) ⇒M+Re ∼ N (X+e, ‖PU⊥e‖2(MTM)−1)\nso β̃ = β + M+Re implies β̃ ∼ N (β + X+e, ‖PU⊥e‖2(MTM)−1). And as ζ̃ = 1√ r (Ir×r − M(MTM)−1MT)Re then we have ζ̃ ∼ N (0r, ‖P U⊥e‖ 2 r (Ir×r − MM +)) as (Ir×r −MM+)M = 0r×p.\nClaim B.2 was based on the assumption that e is fixed. However, given X and y there are many different ways to assign vectors β and e s.t. y = Xβ + e. However, the distributions we get in Claim B.2 are unique. To see that, recall Equations (1) and (2): β + X+e = X+y = β̂ and PU⊥e = PU⊥y = (I − XX+)y = ζ . We therefore have β̃ ∼ N (β̂ , ‖ζ‖2(MTM)−1) and ζ̃ ∼ N (0n, ‖ζ‖ 2\nr (I − MM+)). We will discuss this further, in Section 4, where we will not be able to better analyze the explicit distributions of our estimators. But in this section, we are able to argue more about the distributions of β̃ and ζ̃ .\nSo far we have considered the case that e is fixed, whereas our goal is to argue about the case where each coordinate of e is sampled i.i.d from N (0, σ2). To that end, we now switch to an intermediate model, in which PUe is sampled from a multivariate Gaussian while PU⊥e is fixed as some arbitrary vector of length l. Formally, let Dl denote the distribution where PUe ∼ N (0, σ2PU ) and PU⊥e is fixed as some specific vector whose length is denoted by ‖PU⊥e‖ = l. Claim B.3. Under the same assumptions as in Claim B.2, given that e ∼ Dl, we have that\nβ̃ ∼ N ( β, σ2(XTX)−1 + l2(MTM)−1 ) and\nζ̃ ∼ N ( 0n, l2 r (I −MM +) ) .\nProof. Recall, β̃ = β + M+Re = β + M+(MX+ + RPU⊥)e = β + X\n+e + M+R(PU⊥e). Now, under the assumption e ∼ Dl we have that β is the sum of two independent Gaussians:\nβ +X+e ∼ N (β, σ2 ( X+ · PU · (X+)T ) ) = N (β, σ2(XTX)−1) RPU⊥e ∼ N (0r, ‖PU⊥e‖2Ir×r) ⇒M+Re ∼ N (0p, ‖PU⊥e‖2(MTM)−1)\nSumming the two independent Gaussians’ means and variances gives the distribution of β̃ . Furthermore, in Claim B.2 we have already established that for any fixed e we have ζ̃ ∼ N ( 0n, ‖P U⊥e‖ 2 r (I −MM +) ) . Hence, for\ne ∼ Dl we still have ζ̃ ∼ N ( 0n, l2 r (I −MM +) )\n. (It is easy to verify that the same chain of derivations is applicable when e ∼ Dl.)\nCorollary B.4. Given that e ∼ Dl we have that β̃j ∼ N (βj , σ2(XTX)−1j,j + l2(MTM) −1 j,j ) for any coordinate j, and that ‖ζ̃‖2 ∼ l 2\nr · χ 2 r−p.\nProof. The corollary follows immediately from the fact that βj = eTj β̃ , and from the definition of the χ\n2- distribution, as ζ̃ is a spherically symmetric Gaussian defined on the subspace colspan(M)⊥ of dimension r − p.\nTo continue, we need the following claim.\nClaim B.5. GivenX andM = RX , and given that e ∼ Dl we have that β̃ and ζ̃ are independent.\nProof. Recall, β̃ = β + X+e + M+R(PU⊥e). And so, given X , M and a specific vector PU⊥e we have that the distribution of β̃ depends on (i) the projection of e on U = colspan(X) and on (ii) the projection of each row in R onto Ũ = colspan(M). The distribution of ζ̃ = 1√\nr PŨ⊥Re = 1√ r PŨ⊥(MX + + RPU⊥)e = 1√ r PŨ⊥RPU⊥e depends on (i) the projection of e onto U ⊥ (which for the time being is fix to some specific vector of length l) and on (ii) the projection of each row in R onto Ũ⊥. Since PUe is independent from PU⊥e, and since for any rowuT ofRwe have thatPŨu is independent ofPŨ⊥u, and since e and R are chosen independently, we have that β̃ and ζ̃ are independent.\nFormally, consider any pair of coordinates β̃j and ζ̃k, and we have\nβ̃j − βj = eTjX+e + eTjM+(RPU⊥e)\nζ̃k = e T kPŨ⊥(RPU⊥e)\nRecall, we are givenX andM = RX . Therefore, we know PU and PŨ . And so\nCov[β̃j , ζ̃k]\n= E[(β̃j − βj)(ζ̃k − 0)] = E[eTjX +e(RPU⊥e) TPŨ⊥ek]\n+ E[eTjM +(RPU⊥e)(RPU⊥e) TPŨ⊥ek]\n= eTjX +E[eeTPU⊥ ]E[R T]PŨ⊥ek\n+ eTjM +E[(RPU⊥e)(RPU⊥e) T]PŨ⊥ek\n= eTjX +E[eeTPU⊥ ] ( (MX+)T + E[(RPU⊥) T] ) PŨ⊥ek\n+ eTjM + ( ‖PU⊥e‖2Ir×r ) PŨ⊥ek\n= eTjX +E[eeTPU⊥ ](X +)T ( MTPŨ⊥ ) ek + 0\n+ l2 · eTj ( M+PŨ⊥ ) ek\n= 0 + 0 + 0 = 0\nAnd as β̃ and ζ̃ are Gaussians, having their covariance = 0 implies independence.\nHaving established that β̃ and ζ̃ are independent Gaussians and specified their distributions, we continue with the proof of Theorem 3.1. We assume for now that there exists some small a > 0 s.t.\nl2(MTM)−1j,j ≤ σ 2(XTX)−1j,j + l 2(MTM)−1j,j ≤ e2a · l2(MTM)−1j,j (7)\nThen, due to Corollary A.3, denoting the distributions N1 = N (0, l2(MTM)−1j,j ) and N2 = N (0, σ2(XTX)−1j,j + l2(MTM) −1 j,j ), we have that for any S ⊂ R it holds that13\ne−aPrβ̃j∼N1 [S] ≤ Prβ̃j∼N2 [S] ≤ e aPrβ̃j∼N1 [S/e a]\n(8)\nMore specifically, denote the function\nt̃(ψ, ‖ξ‖, βj) = ψ − βj ‖ξ‖ √\nr r−p (M TM)−1j,j\n= ψ − βj l √\n(MTM)−1j,j\n/‖ξ‖√ rr−p l\nand observe that when we sample ψ,ξ independently s.t. ψ ∼ N (βj , l2(MTM)−1j,j ) and ‖ξ‖2 ∼ l 2 r χ 2 r−p then t̃(ψ, ‖ξ‖, βj) is distributed like a T -distribution with r − p 13In fact, it is possible to use standard techniques from differential privacy, and argue a similar result — that the probabilities of any event that depends on some function f(βj) under βj ∼ N1 and under βj ∼ N2 are close in the differential privacy sense.\ndegrees of freedom. And so, for any τ > 0 we have that under such way to sample ψ,ξ we have Pr[t̃(ψ, ‖ξ‖, βj) > τ ] = 1− CDFTr−p(τ).\nFor any τ ≥ 0 and for any non-negative real value z let Sτz denote the suitable set of values s.t.\nPrψ∼N (βj , l 2(MTM)−1j,j )\n‖ξ‖2∼ l 2\nr χ 2 r−p\n [t̃(ψ, ‖ξ‖, βj) > τ ]\n= ∞∫ 0 PDF l2 r χ 2 r−p (z) · Pr {ψ−βj∼N (0, l2(MTM)−1j,j )} [Sτz ] dz\nThat is, Sτz = ( τ · z √ r r−p (M TM)−1j,j , ∞ ) .\nWe now use Equation (8) (Since N (0, l2(MTM)−1j,j ) is precisely N1) to deduce that Prψ∼N (βj , l 2(MTM)−1j,j+σ 2(XTX)−1j,j )\n‖ξ‖2∼ l 2\nr χ 2 r−p\n [t̃(ψ, ‖ξ‖, βj) > τ ]\n= ∫ ∞ 0 PDF l2 r χ 2 r−p (z) Pr ψ − βj ∼ N (0, l2(MTM)−1j,j + σ2(XTX) −1 j,j ) [Sτz ]dz\n≤ ea ∫ ∞\n0 PDF l2 r χ 2 r−p (z) Pr ψ−βj∼N (0, l2(MTM)−1j,j )\n[Sτz /ea]dz\n(∗) = ea ∫ ∞ 0 PDF l2 r χ 2 r−p (z) Pr ψ−βj∼N (0, l2(MTM)−1j,j ) [Sτ/e a z ]dz = eaPrψ∼N (βj , l 2(MTM)−1j,j )\n‖ξ‖2∼ l 2\nr χ 2 r−p\n [t̃(ψ, ‖ξ‖, βj) > τ/ea]\n= ea ( 1− CDFTr−p(τ/ea) ) where the equality (∗) follows from the fact that Sτz /c = S τ/c z for any c > 0, since it is a non-negative interval. Analogously, we can also show that Prψ∼N (βj , l 2(MTM)−1j,j+σ 2(XTX)−1j,j )\n‖ξ‖2∼ l 2\nr χ 2 r−p\n [t̃(ψ, ‖ξ‖, βj) > τ ]\n≥ e−aPrψ∼N (βj , l 2(MTM)−1j,j )\n‖ξ‖2∼ l 2\nr χ 2 r−p\n [t̃(ψ, ‖ξ‖, βj) > τ ]\n= e−a ( 1− CDFTr−p(τ) ) In other words, we have just shown that for any interval I = (τ,∞) with τ ≥ 0 we have that Prψ∼N (βj , l 2(MTM)−1j,j+σ 2(XTX)−1j,j )\n‖ξ‖2∼ l 2\nr χ 2 r−p\n [t̃(ψ, ‖ξ‖, βj) ∈ I]\nis lower bounded by ea ∫ I PDFTr−p(z)dz and upper\nbounded by ea ∫\nI/ea PDFTr−p(z)dz. We can now repeat\nthe same argument for I = (τ1, τ2) with 0 ≤ τ1 < τ2 (using an analogous definition of Sτ1,τ2z ), and again\nfor any I = (τ1, τ2) with τ1 < τ2 ≤ 0, and deduce that the PDF of the function t̃(ψ, ‖ξ‖, βj) at x — where we sample ψ ∼ N (βj , l2(MTM)−1j,j + σ2(XTX) −1 j,j ) and ‖ξ‖2 ∼ l 2\nr χ 2 r−p independently — lies in the range(\ne−aPDFTr−p(x), e aPDFTr−p(x/e\na) ) . And so, using\nCorollary B.4 and Claim B.5, we have that when e ∼ Dl, the distributions of β̃j and ‖ζ̃‖2 are precisely as stated above, and so we have that the distribution of t̃(βj) def = t̃(β̃j , ‖ζ̃‖, βj) has a PDF that at the point x is “sandwiched” between e−aPDFTr−p(x) and e aPDFTr−p(x/e a).\nNext, we aim to argue that this characterization of the PDF of t̃(βj) still holds when e ∼ N (0n, σ2In×n). It would be convenient to think of e as a sample in N (0n, σ2PU ) ×N (0n, σ2PU⊥). (So while in Dl we have PUe ∼ N (0n, σ2PU ) butPU⊥e is fixed, now bothPUe and PU⊥e are sampled from spherical Gaussians.) The reason why the above still holds lies in the fact that t̃(βj) does not depend on l. In more details:\nPre∼N (0n,σ2In×n) [ t̃(βj) ∈ I ] =\n∫ v Pre∼N (0n,σ2In×n) [ t̃(βj) ∈ I | PU⊥e = v ] PDFP U⊥e (v)dv\n= ∫ v Pr e∼Dl [ t̃(βj) ∈ I | l = ‖v‖ ] PDFP U⊥e (v)dv\n≤ ∫ v ( ea ∫ I/ea PDFTr−p(z)dz ) PDFP U⊥e (v)dv\n= ( ea ∫ I/ea PDFTr−p(z)dz )∫ v PDFP U⊥e (v)dv\n= ea ∫ I/ea PDFTr−p(z)dz\nwhere the last transition is possible precisely because t̃ is independent of l (or ‖v‖) — which is precisely what makes this t-value a pivot quantity. The proof of the lower bound is symmetric.\nTo conclude, we have shown that if Equation (7) holds, then for every interval I ⊂ R we have that Pre∼N (0n,σ2In×n) [ t̃(βj) ∈ I ] is lower bounded by e−aPrz∼Tr−p [z ∈ I] and upper bounded by eaPrz∼Tr−p [z ∈ (I/ea)]. So to conclude the proof of Theorem 3.1, we need to show that w.h.p such a as in Equation (7) exists. Claim B.6. In the homoscedastic model with Gaussian noise, if both n and r satisfy n, r ≥ p + Ω(log(1/ν)), then we have that σ2(XTX)−1j,j +l 2(MTM)−1j,j ≥ l2(MTM) −1 j,j and\nσ2(XTX)−1j,j +l 2(MTM)−1j,j ≤ (1+ 2(r−p) n−p )·l 2(MTM)−1j,j\nUsing (1 + 2(r−p)n−p ) ≤ e 2(r−p) n−p , Theorem 3.1 now follows from plugging a = r−pn−p to our above discussion.\nProof. The lower bound is immediate from non-negativity of σ2 and of (XTX)−1j,j = ‖(XTX)−1/2ej‖2. We therefore prove the upper bound.\nFirst, observe that l2 = ‖PU⊥e‖2 is sampled from σ2·χ2n−p as U⊥ is of dimension n − p. Therefore, it holds that w.p. ≥ 1− ν/2 that\nσ2 (√ n− p− √ 2 ln(2/ν) )2 ≤ l2\nand assuming n > p+100 ln(2/ν) we therefore have σ2 ≤ 4 3(n−p) l 2.\nSecondly, we argue that when r > p + 300 ln(4/ν) we have that w.p. ≥ 1 − ν/2 it holds that 3 4 (X TX)−1j,j ≤ (r − p)(XTRTRX) −1 j,j . To see this, first observe that by picking R ∼ N (0r×n, Ir×r, In×n) the distribution of the product RX ∼ N (0r×d, Ir×r, XTX) is identical to picking Q ∼ N (0r×d, Ir×r, Id×d) and taking the product Q(XTX)1/2. Therefore, the distribution of (XTRTRX)−1 is identical to ( (XTX)1/2QTQ(XTX)1/2 )−1 = (XTX)−1/2(QTQ)−1(XTX)−1/2. Denoting v = (XTX)−1/2ej we have ‖v‖2 = (XTX)−1j,j . Claim A.1 from (Sheffet, 2015) gives that w.p. ≥ 1 − ν/2 we have\n(r − p) · eTj ( (XTX)1/2QTQ(XTX)1/2 )−1 ej\n= vT( 1r−pQ TQ)−1v ≥ 34v Tv = 34 (X TX)−1j,j\nwhich implies the required.\nCombining the two inequalities we get:\nσ2(XTX)−1j,j ≤ 16l2(r−p) n−p (X TRTRX)−1j,j\n≤ 2(r−p)n−p l 2(XTRTRX)−1j,j\nand as we denote M = RX we are done.\nWe comment that our analysis in the proof of Claim B.6 implicitly assumes r n (as we do think of the projection R as dimensionality reduction), and so the ratio r−p n−p is small. However, a similar analysis holds for r which is comparable to n — in which we would argue that σ2(XTX)−1j,j+l\n2(MTM)−1j,j σ2(XTX)−1 ∈ [1, 1 + η] for some small η."
    }, {
      "heading" : "B.3. Proof of Theorem 3.3",
      "text" : "Theorem B.7 (Theorem 3.3 restated.). Fix a positive definite matrix Σ ∈ Rp×p. Fix parameters β ∈ Rp and σ2 > 0 and a coordinate j s.t. βj 6= 0. Let X be a matrix whose n rows are sampled i.i.d fromN (0p,Σ). Let y be a vector s.t. yi−(Xβ)i is sampled i.i.d fromN (0, σ2). Fix ν ∈ (0, 1/2) and α ∈ (0, 1/2). Then there exist constants C1, C2, C3 and C4 such that when we run Algorithm 1 over [X;y] with\nparameter r w.p. ≥ 1−ν we correctly α-reject the null hypothesis using p̃0 (i.e., w.p. ≥ 1 − ν Algorithm 1 returns matrix unaltered and we can estimate t̃0 and verify that indeed p̃0 < α · e − r−pn−p ) provided\nr ≥ p+ max { C1 σ2(c̃2α + τ̃ 2 α)\nβ2jσmin(Σ) , C2 ln(1/ν)\n}\nand n ≥ max { r, C3\nw2\nmin{σmin(Σ), σ2} , C4(p+ ln(1/ν)) } where c̃α, τ̃α denote the numbers s.t. ∞∫\nc̃α/e\nr−p n−p\nPDFTr−p(x)dx = α 2 e − r−pn−p and\n∞∫ τ̃α/e r−p n−p PDFN (0,1)(x)dx = α 2 e − r−pn−p resp.\nProof. First we need to use the lower bound on n to show that indeed Algorithm 1 does not alter A, and that various quantities are not far from their expected values. Formally, we claim the following.\nProposition B.8. Under the same lower bounds on n and r as in Theorem 3.3, w.p. 1−α−ν we have that Theorem 3.1 holds and also that\nζ̃‖2 = Θ( r−pr ‖PU⊥e‖ 2) = Θ( r−pr (n− p)σ 2)\nand (XTRTRX)−1j,j = Θ( 1 r−p (X TX)−1j,j )\nProof of Proposition B.8. First, we need to argue that we have enough samples as to have the gap σ2min([X; y])−w2 sufficiently large.\nSince xi ∼ N (0,Σ), and yi = βTxi + ei with ei ∼ N (0, σ2), we have that the concatenation (xi ◦ yi) is also sampled from a Gaussian. Clearly, E[yi] = βTE[xi] + E[ei] = 0. Similarly, E[xi,jyi] = E[xi,j · (βTxi + ei)] = (Σβ)j and E[y2i ] = E[e 2 i ] + E[‖Xβ‖2] = σ2 + E[βTXTXβ ] = σ2 + βTΣβ . Therefore, each row of A is an i.i.d sample of N (0p+1,ΣA), with\nΣA =\n( Σ Σβ\nβTΣ σ2+βTΣβ ) Denote λ2 = σmin(Σ). Then, to argue that σmin(ΣA) is large we use the lower bound from (Ma & Zarowski, 1995) (Theorem 3.1) combining with some simple arithmetic manipulations to deduce that σmin(ΣA) ≥ min{σmin(Σ), σ2}.\nHaving established a lower bound on σmin(ΣA), it follows that with n = Ω(p ln(1/ν)) i.i.d draws from N (0p+1,ΣA) we have w.p. ≤ ν/4 that σmin(ATA) = o(n) · min{σmin(Σ), σ2}. Conditioned on σmin(ATA) = Ω(nσmin(ΣA)) = Ω(w\n2) being large enough, we have that w.p. ≤ ν/4 over the randomness of Algorithm 1 the matrix A does not pass the if-condition and the output of the algorithm is not RA. Conditioned on Algorithm 1 outputting RA, and due to the lower bound r = p + Ω(ln(1/ν)), we have that the result of Theorem 3.1 does not hold w.p. ≤ α+ ν/4. All in all we deduce that w.p. ≥ 1−α− 3ν/4 the result of Theorem 3.1 holds. And since we argue Theorem 3.1 holds, then the following two bounds that are used in the proof14 also hold:\n(XTRTRX)−1j,j = Θ( 1 r−p (X TX)−1j,j )\n‖PU⊥e‖2 = Θ((n− p)σ2)\nLastly, in the proof of Theorem 3.1 we argue that for a given PU⊥e the length ‖ζ̃‖2 is distributed like ‖P U⊥e‖ 2\nr χ 2 r−p. Appealing again to the fact that r = p + Ω(ln(1/ν) we have that w.p. ≥ ν/4 it holds that ‖ζ̃‖2 > 2(r − p)‖PU⊥e‖ 2\nr . Plugging in the value of ‖PU⊥e‖ 2 con-\ncludes the proof of the proposition.\nBased on Proposition B.8, we now show that we indeed reject the null-hypothesis (as we should). When Theorem 3.1 holds, reject the null-hypothesis iff p̃0 < α · e − r−pn−p which holds iff |t̃0| > e r−p n−p τ̃α. This implies we reject that null-hypothesis when |β̃j | > e r−p n−p τ̃α ·\nσ̃ √\n(XTRTRX)−1j,j ). Note that this bound is based\non Corollary 3.2 that determines that |β̃j − βj | =\nO ( e r−p n−p c̃α · σ̃ √ (XTRTRX)−1j,j ) ) . And so we have that\nw.p. ≥ 1− ν we α-reject the null hypothesis when it holds that |βj | > 3(c̃α+ τ̃α) · σ̃ √ (XTRTRX)−1j,j ) ≥ e r−p n−p (c̃α+\nτ̃α)σ̃ √ (XTRTRX)−1j,j ) (due to the lower bound n ≥ r).\nBased on the bounds stated above we have that σ̃ = ‖ζ̃‖ √\nr r−p = Θ(σ\n√ n− p √ r−p r √ r r−p ) = Θ(σ √ n− p)\nand that\n(XTRTRX)−1j,j = Θ( 1 r−p (X TX)−1j,j ) = O ( 1 r−p · 1 nσmin(Σ) ) And so, a sufficient condition for rejecting the nullhypothesis is to have\n|βj | = Ω ( (c̃α + τ̃α)σ √ n− p r − p · √ 1 nσmin(Σ) ) 14More accurately, both are bounds shown in Claim B.6.\n= Ω(e r−p n−p (c̃α + τ̃α)σ̃ √ (XTRTRX)−1j,j ))\nwhich, given the lower bound r = p + Ω ( (c̃α+τ̃α) 2σ2\nβ2jσmin(Σ) ) indeed holds."
    }, {
      "heading" : "C. Projected Ridge Regression",
      "text" : "In this section we deal with the case that our matrix does not pass the if-condition of Algorithm 1. In this case, the matrix is appended with a d × d-matrix which is wId×d.\nDenoting A′ = [\nA w · Id×d\n] we have that the algorithm’s\noutput is RA′.\nSimilarly to before, we are going to denote d = p + 1 and decompose A = [X;y] with X ∈ Rn×p and y ∈ Rn, with the standard assumption of y = Xβ + e and ei sampled i.i.d from N (0, σ2).15 We now need to introduce some additional notation. We denote the appended matrix and vectors X ′ and y ′ s.t. A′ = [X ′;y ′]. Meaning:\nX ′ =  XwIp×p 0Tp  and\ny ′ =  y0p w  = X ′β +  e−wβ w  def= X ′β + e′ And so we respectively denote R = [R1;R2;R3] with R1 ∈ Rr×n, R2 ∈ Rr×p and R3 ∈ Rr×1 (so R3 is a vector denoted as a matrix). Hence:\nM ′ = RX ′ = R1X + wR2\nand\nRy ′ = RX ′β+Re′ = R1y+wR3 = R1Xβ+R1e+wR3\nAnd so, using the output RA′ of Algorithm 1, we solve the linear regression problem derived from 1√\nr RX ′ and\n1√ r Ry ′. I.e., we set\nβ ′ = arg min z 1 r‖Ry ′ −RX ′z‖2\n= (X ′TRTRX ′)−1(RX ′)T(Ry ′)\nSarlos’ results (2006) regarding the Johnson Lindenstrauss transform give that, when R has sufficiently many rows, solving the latter optimization problem gives a good approximation for the solution of the optimization problem βR = arg minz ‖y ′ −X ′z‖2 = arg minz ( ‖y −Xz‖2 + w2‖z‖2\n) 15Just as before, it is possible to denote any single column as y\nand any subset of the remaining columns as X .\nThe latter problem is known as the Ridge Regression problem. Invented in the 60s (Tikhonov, 1963; Hoerl & Kennard, 1970), Ridge Regression is often motivated from the perspective of penalizing linear vectors whose coefficients are too large. It is also often applied in the case where X doesn’t have full rank or is close to not having full-rank. That is because the Ridge Regression problem is always solvable. One can show that the minimizer βR = (XTX + w2Ip×p)\n−1XTy is the unique solution of the Ridge Regression problem and that the RHS is always defined (even when X is singular).\nThe original focus of Ridge Regression is on penalizing βR for having large coefficients. Therefore, Ridge Regression actually poses a family of linear regression problems: minz ‖y−Xz‖+ λ‖z‖2, where one may set λ to be any non-negative scalar. And so, much of the literature on Ridge Regression is devoted to the art of fine-tuning this penalty term — either empirically or based on the λ that yields the best risk: ‖E[βR] − β‖2 + Var(βR).16 Here we propose a fundamentally different approach for the choice of the normalization factor — we set it so that solution of the regression problem would satisfy ( , δ)-differential privacy (by projecting the problem onto a lower dimension).\nWhile the solution of the Ridge Regression problem might have smaller risk than the OLS solution, it is not known how to derive t-values and/or reject the null hypothesis under Ridge Regression (except for using X to manipulate βR back into β̂ = (XTX)−1XTy and relying on OLS). In fact, prior to our work there was no need for such analysis! For confidence intervals one could just use the standard OLS, because access to X and y was given.\nTherefore, much for the same reason, we are unable to derive t-values under projected Ridge Regression.17 Clearly, there are situations where such confidence bounds simply cannot be derived. (Consider for example the case where X = 0n×p and y is just i.i.d draws from N (0, σ2), so obviously [X; y] gives no information about β .) Nonetheless, under additional assumptions about the data, our work can give confidence intervals for βj , and in the case where the interval doesn’t intersect the origin — assure us that sign(β′j) = sign(βj) w.h.p.\nClearly, Sarlos’ work (2006) gives an upper bound on the distance ‖β ′−βR‖. However, such distance bound doesn’t come with the coordinate by coordinate confidence guarantee we would like to have. In fact, it is not even clear from Sarlos’ work that E[β ′] = βR (though it is obvious to see that E[(X ′TRTRX ′)]βR = E[(RX ′)TRy ′]). Here,\n16Ridge Regression, as opposed to OLS, does not yield an unbiased estimator. I.e., E[βR] 6= β .\n17Note: The naı̈ve approach of using RX ′ and Ry ′ to interpolate RX and Ry and then apply Theorem 3.1 using these estimations of RX and Ry ignores the noise added from appending the matrix A into A′, and it is therefore bound to produce inaccurate estimations of the t-values.\nwe show that E[β ′] = β̂ which, more often than not, does not equal βR.\nComment about notation. Throughout this section we assume X is of full rank and so (XTX)−1 is well-defined. If X isn’t full-rank, then one can simply replace any occurrence of (XTX)−1 with X+(X+)T. This makes all our formulas well-defined in the general case."
    }, {
      "heading" : "C.1. Running OLS on the Projected Data",
      "text" : "In this section, we analyze the projected Ridge Regression, under the assumption (for now) that e is fixed. That is, for now we assume that the only source of randomness comes from picking the matrix R = [R1;R2;R3]. As before, we analyze the distribution over β ′ (see Equation (9)), and the value of the function we optimize at β ′. Denoting M ′ = RX ′, we can formally express the estimators:\nβ ′ = (M ′TM ′)−1M ′TRy ′ (9) ζ ′ = 1√\nr (Ry ′ −RX ′β ′) (10)\nClaim C.1. Given that y = Xβ+e for a fixed e, and given X and M ′ = RX ′ = R1X + wR2 we have that β ′ ∼ N ( β +X+e,\n(w2(‖β +X+e‖2 + 1) + ‖PU⊥e‖2)(M ′TM ′)−1 )\nζ ′ ∼ N ( 0r,\nw2(‖β+X+e‖2+1)+‖P U⊥e‖ 2\nr (Ir×r −M ′M ′+) ) and furthermore, β ′ and ζ ′ are independent of one another.\nProof. First, we write β ′ and ζ ′ explicitly, based on e and projection matrices:\nβ ′ = (M ′TM ′)−1M ′TRy ′\n= M ′+(R1X)β +M ′+(R1e + wR3)\nζ ′ = 1√ r (Ry ′ −RX ′β ′)\n= 1√ r (Ir×r −M ′M ′+)Re′ = 1√ r PU ′⊥(R1e − wR2β + wR3)\nwith U ′ denoting colspan(M ′) and PU ′⊥ denoting the projection onto the subspace U ′⊥.\nAgain, we break e into an orthogonal composition: e = PUe + PU⊥e with U = colspan(X) (hence PU = XX+) and U⊥ = colspan(X)⊥. Therefore,\nβ ′ = M ′+(R1X)β +M ′+(R1XX +e +R1PU⊥e + wR3) = M ′+(R1X)(β +X +e) +M ′+(R1PU⊥e + wR3)\nwhereas ζ ′ is essentially 1√ r (Ir×r −M ′M ′+)(R1XX+e +R1PU⊥e − wR2β + wR3)\n(∗) = 1√\nr (Ir×r −M ′M ′+)·\n(R1XX +e +R1PU⊥e + (M ′ − wR2)β + wR3) = 1√\nr (Ir×r −M ′M ′+)·\n(R1X(β +X +e) +R1PU⊥e + wR3)\nwhere equality (∗) holds because (I −M ′M ′+)M ′v = 0 for any v .\nWe now aim to describe the distribution of R given that we know X ′ and M ′ = RX ′. Since\nM ′ = R1X + wR2 + 0 ·R3 = R1X(X+X) + wR2 = (R1PU )X + wR2\nthen M ′ is independent of R3 and independent of R1PU⊥ . Therefore, given X and M ′ the induced distribution over R3 remainsR3 ∼ N (0r, Ir×r), and similarly, givenX and M ′ we have R1PU⊥ ∼ N (0r×n, Ir×r, PU⊥) (rows remain independent from one another, and each row is distributed like a spherical Gaussian in colspan(X)⊥). And so, we have that R1X = R1PUX = M ′ − wR2, which in turn implies:\nR1X ∼ N ( M ′, Ir×r, w 2 · Ip×p )\nmultiplying this random matrix with a vector, we get\nR1X(β+X +e) ∼ N (M ′β +M ′X+e, w2‖β +X+e‖2Ir×r)\nand multiplying this random vector with a matrix we get\nM ′+R1X(β+X +e) ∼ N (β +X+e, w2‖β +X+e‖2(M ′TM)−1)\nI.e.,\nM ′+R1X(β+X +e) ∼ ‖β+X+e‖·N (u,w2(M ′TM)−1)\nwhere u denotes a unit-length vector in the direction of β+ X+e.\nSimilar to before we have\nRPU⊥ ∼ N (0r×n, Ir×r, PU⊥) ⇒M ′+(RPU⊥e) ∼ N (0d, ‖PU⊥e‖2(M ′TM ′)−1)\nwR3 ∼ N (0r, w2Ir×r) ⇒M ′+(wR3) ∼ N (0d, w2(M ′+M ′)−1)\nTherefore, the distribution of β ′, which is the sum of the 3 independent Gaussians, is as required.\nAlso, ζ ′ = 1√ r PU ′⊥ (R1X(β +X +e) +R1PU⊥e + wR3) is the sum of 3 independent Gaussians, which implies its distribution is\nN (\n1√ r PU ′⊥M ′(β +X+e),\n1 r (w 2(‖β +X+e‖2 + 1) + ‖PU⊥e‖2)PU ′⊥ )\nI.e., N ( 0r, 1 r (w 2(‖β +X+e‖2 + 1) + ‖PU⊥e‖2)PU ′⊥ ) as PU ′⊥M ′ = 0r×r.\nFinally, observe that β ′ and ζ ′ are independent as the former depends on the projection of the spherical Gaussian R1X(β + X\n+e) + R1PU⊥e + wR3 on U ′, and the latter depends on the projection of the same multivariate Gaussian on U ′⊥.\nObserve that Claim C.1 assumes e is given. This may seem somewhat strange, since without assuming anything about e there can be many combinations of β and e for which y = Xβ + e. However, we always have that β + X+e = X+y = β̂ . Similarly, it is always the case the PU⊥e = (I − XX+)y = ζ . (Recall OLS definitions of β̂ and ζ in Equation (1) and (2).) Therefore, the distribution of β ′ and ζ ′ is unique (once y is set):\nβ ′ ∼ N ( β̂ , (w2(‖β̂‖2 + 1) + ‖ζ‖2)(M ′TM ′)−1 ) ζ ′ ∼ N ( 0r, w2(‖β̂‖2 + 1) + ‖ζ‖2\nr (Ir×r −M ′M ′+)\n)\nAnd so for a given dataset [X;y] we have that β ′ serves as an approximation for β̂ .\nAn immediate corollary of Claim C.1 is that for any fixed e it holds that the quantity t′(βj) =\nβ′j−(βj+(X +e)j) ‖ζ ′‖ √ r r−p ·(M ′TM ′)−1j,j = β′j−β̂j ‖ζ ′‖ √ r r−p ·(M ′TM ′)−1j,j is distributed like a Tr−p-distribution. Therefore, the following theorem follows immediately.\nTheorem C.2. Fix X ∈ Rn×p and y ∈ R. Define β̂ = X+y and ζ = (I − XX+)y . Let RX ′ and Ry ′ denote the result of applying Algorithm 1 to the matrix A = [X;y] when the algorithm appends the data with a w · I matrix. Fix a coordinate j and any α ∈ (0, 1/2). When computing β ′ and ζ ′ as in Equations (9) it and (10), we have that w.p. ≥ 1− α it holds that\nβ̂j ∈ ( β′j ± c′α‖ζ ′‖ √ r r−p · (M ′TM ′) −1 j,j ) where c′α denotes the number such that (−c′α, c′α) contains 1− α mass of the Tr−p-distribution.\nNote that Theorem C.2, much like the rest of the discussion in this Section, builds on y being fixed, which means β′j serves as an approximation for β̂j . Yet our goal is to argue about similarity (or proximity) between β′j and βj . To that end, we combine the standard OLS confidence interval — which says that w.p. ≥ 1 − α over the randomness of picking e in the homoscedastic model we have\n|βj − β̂j | ≤ cα‖ζ‖ √ (XTX)−1j,j n−p — with the confidence interval of Theorem C.2 above, and deduce that w.p. ≥ 1−α\nwe have that |β′j − βj | is at most\nO cα ‖ζ‖ √\n(XTX)−1j,j √ n− p + c′α ‖ζ ′‖ √ r(M ′TM ′)−1j,j √ r − p  (11)\n18And so, in the next section, our goal is to give conditions under which the interval of Equation (11) isn’t much larger in comparison to the interval length of c′α ‖ζ ′‖√ r−p √ r(M ′TM ′)−1j,j we get from Theorem C.2; and more importantly — conditions that make the interval of Theorem C.2 useful and not too large. (Note, in expectation ‖ζ ′‖√ r−p is about √ (w2 + w2‖β̂‖2 + ‖ζ‖2)/r. So, for example, in situations where ‖β̂‖ is very large, this interval isn’t likely to inform us as to the sign of βj .)\nMotivating Example. A good motivating example for the discussion in the following section is when [X;y] is a strict submatrix of the dataset A. That is, our data contains many variables for each entry (i.e., the dimensionality d of each entry is large), yet our regression is made only over a modest subset of variables out of the d. In this case, the least singular value of A might be too small, causing the algorithm to alter A; however, σmin(XTX) could be sufficiently large so that had we run Algorithm 1 only on [X;y] we would not alter the input. (Indeed, a differentially private way for finding a subset of the variables that induce a submatrix with high σmin is an interesting open question, partially answered — for a single regression — in the work of Thakurta and Smith (Thakurta & Smith, 2013).) Indeed, the conditions we specify in the following section depend on σmin( 1nX\nTX), which, for a zero-mean data, the minimal variance of the data in any direction. For this motivating example, indeed such variance isn’t necessarily small."
    }, {
      "heading" : "C.2. Conditions for Deriving a Confidence Interval for Ridge Regression",
      "text" : "Looking at the interval specified in Equation (11), we now give an upper bound on the the random quantities in this interval: ‖ζ‖, ‖ζ ′‖, and (M ′TM ′)−1j,j . First, we give bound that are dependent on the randomness in R (i.e., we continue to view e as fixed).\nProposition C.3. For any ν ∈ (0, 1/2), if we have r = p + Ω(ln(1/ν)) then with probability ≥\n18Observe that w.p. ≥ 1 − α over the randomness of e we have that |βj − β̂j | ≤ cα‖ζ‖ √ (XTX)−1j,j n−p , and w.p. ≥ 1 − α over the randomness of R we have that |β′j − β̂j | ≤ c′α‖ζ ′‖ √ r r−p · (M ′TM ′) −1 j,j . So technically, to give a (1 − α)confidence interval around β′j that contains βj w.p. ≥ 1− α, we need to use cα/2 and c′α/2 instead of cα and c ′ α resp. To avoid overburdening the reader with what we already see as too many parameters, we switch to asymptotic notation.\n1 − ν over the randomness of R we have (r − p)(M ′TM)−1j,j = Θ ( (w2Ip×p +X TX)−1j,j ) and ‖ζ ′‖2 r−p = Θ(w 2+w2‖β̂‖2+‖ζ‖2\nr ).\nProof. The former bound follows from known results on the Johnson-Lindenstrauss transform (as were shown in the proof of Claim B.6). The latter bound follows from standard concentration bounds of the χ2-distribution.\nPlugging in the result of Proposition C.3 to Equation (11) we get that w.p. ≥ 1− ν the difference |β′j − βj | is at most O ( cα ‖ζ‖√ n− p √ (XTX)−1j,j\n+ c′α\n√ w2 + w2‖β̂‖2 + ‖ζ‖2\nr − p\n√ (w2Ip×p +XTX) −1 j,j ) (12)\nWe will also use the following proposition.\nProposition C.4.\n(XTX)−1j,j ≤ ( 1 + w2\nσmin(XTX)\n) (w2Ip×p +X TX)−1j,j\nProof. We have that\n(XTX)−1\n= (XTX)−1(XTX + w2Ip×p)(X TX + w2Ip×p) −1\n= (XTX + w2Ip×p) −1 + w2(XTX)−1(XTX + w2Ip×p) −1\n= (Ip×p + w 2(XTX)−1)(XTX + w2Ip×p) −1\n= (XTX + w2Ip×p) −1/2·\n(Ip×p + w 2(XTX)−1)·\n(XTX + w2Ip×p) −1/2\nwhere the latter holds because (Ip×p + w2(XTX)−1) and (XTX + w2Ip×p)\n−1 are diagonalizable by the same matrix V (the same matrix for which (XTX) = V S−1V T). Since we have ‖Ip×p +w2(XTX)−1‖ = 1 + w 2\nσ2min(X) , it is\nclear that (Ip×p + w2(XTX)−1) (1 + w 2\nσ2min(X) )Ip×p.\nWe deduce that (XTX)−1j,j = e T j (X TX)−1ej ≤ (1 + w2 σ2min(X) )(XTX + w2Ip×p) −1 j,j .\nBased on Proposition C.4 we get from Equation (12) that\n|β′j − βj | is at most\nO( ( cα √√√√‖ζ‖2(1 + w2σmin(XTX) ) n− p +\nc′α\n√ w2 + w2‖β̂‖2 + ‖ζ‖2\nr − p\n)√ (w2Ip×p +XTX) −1 j,j )\n(13)\nAnd so, if it happens to be the case that exists some small η > 0 for which β̂ , ζ and w2 satisfy\n‖ζ‖2(1 + w 2\nσmin(XTX) )\nn− p ≤ η2\n( w2 + w2‖β̂‖2 + ‖ζ‖2\nr − p\n) (14)\nthen we have that Pr[βj ∈( β′j ±O((1 + η) · c′α‖ζ ′‖ √ r r−p · (M ′TM ′) −1 j,j ) )\n] ≥ 1 − α.19 Moreover, if in this case |βj | >\nc′α(1 + η)\n√ w2+w2‖β̂‖2+‖ζ‖2\nr−p\n√ (w2Ip×p +XTX) −1 j,j\nthen Pr[sign(β′j) = sign(βj)] ≥ 1− α. This is precisely what Claims C.5 and C.6 below do. Claim C.5. If there exists η > 0 s.t. n − p ≥ 2η2 (r − p) and n 2 =\nΩ ( r3/2 · B 2 ln(1/δ) ·\n1\nη2σmin( 1 nX TX) ) , then Pr[βj ∈(\nβ′j ±O((1 + η) · c′α‖ζ ′‖ √ r r−p · (M ′TM ′) −1 j,j ) )\n] ≥ 1− α.\nProof. Based on the above discussion, it is enough to argue that under the conditions of the claim, the constraint of Equation (14) holds. Since we require η 2\n2 ≥ r−p n−p then\nit is evident that ‖ζ‖ 2 n−p ≤ η2‖ζ‖2 2(r−p) . So we now show that ‖ζ‖2 n−p · w2 σmin(XTX) ≤ η 2‖ζ‖2 2(r−p) under the conditions of the claim, and this will show the required. All that is left is some algebraic manipulations. It suffices to have:\nη2 2 · n−p r−pσmin(X TX) ≥ η 2 2 · n2 r σmin( 1 nX TX)\n≥ 32B 2 √ r ln(8/δ) ≥ w2\nwhich holds for n2 ≥ r3/2 · 64B 2 ln(1/δ) η2 σmin( 1 nX TX)−1, as we assume to hold.\nClaim C.6. Fix ν ∈ (0, 12 ). If (i) n = p + Ω(ln(1/ν)), (ii) ‖β‖2 = Ω(σ2‖X+‖2F ln( p ν )) and (iii) r − p =\nΩ\n( (c′α) 2(1+η)2\nβ2j\n( 1 + ‖β‖2 + σ 2\nσmin( 1 nX TX)\n)) , then in the\nhomoscedastic model, with probability≥ 1−ν−α we have that sign(βj) = sign(β′j).\n19We assume n ≥ r so cα < c′α as the Tn−p-distribution is closer to a normal Gaussian than the Tr−p-distribution.\nProof. Based on the above discussion, we aim to show that in the homoscedastic model (where each coordinate ei ∼ N (0, σ2) independently) w.p. ≥ 1 − ν it holds that the magnitude of βj is greater than\nc′α(1+η)\n√ w2 + w2‖β̂‖2 + ‖ζ‖2\nr − p\n√ (w2Ip×p +XTX) −1 j,j\nTo show this, we invoke Claim A.4 to argue that w.p. ≥ 1 − ν we have (i) ‖ζ‖2 ≤ 2σ2(n − p) (since n = p + Ω(ln(1/ν))), and (ii) ‖β̂‖2 ≤ 2‖β‖2 (since ‖β − β̂‖2 ≤ σ2‖X+‖2F ln( p ν ) whereas ‖β‖ 2 = Ω(σ2‖X+‖2F ln( p ν ))). We also use the fact that (w2Ip×p + XTX)−1j,j ≤ (w2 + σ−1min(X TX)), and then deduce that\n(1 + η)c′α\n√ w2 + w2‖β̂‖2 + ‖ζ‖2\nr − p\n√ (w2Ip×p +XTX) −1 j,j\n≤ (1 + η)c ′ α√\nr − p\n√ 2 w2(1 + ‖β‖2) + σ2(n− p)\nw2 + σmin(XTX)\n≤ (1 + η)c ′ α√\nr − p\n√ 2(1 + ‖β‖2) + 2σ\n2(n− p) σmin(XTX) ≤ |βj |\ndue to our requirement on r − p.\nObserve, out of the 3 conditions specified in Claim C.6, condition (i) merely guarantees that the sample is large enough to argue that estimations are close to their expect value; and condition (ii) is there merely to guarantee that ‖β̂‖ ≈ ‖β‖. It is condition (iii) which is non-trivial to hold, especially together with the conditions of Claim C.5 that pose other constraints in regards to r, n, η and the various other parameters in play. It is interesting to compare the requirements on r to the lower bound we get in Theorem 3.3 — especially the latter bound. The two bounds are strikingly similar, with the exception that here we also require r − p to be greater than 1+‖β‖ 2\nβ2j . This is part of the\nunfortunate effect of altering the matrix A: we cannot give confidence bounds only for the coordinates j for which β2j is very small relative to ‖β‖2.\nIn summary, we require to have n = p + Ω(ln(1/ν)) and that X contains enough sample points to have ‖β̂‖ comparable to ‖β‖, and then set r and η such that (it is convenient to think of η as a small constant, say, η = 0.1)\n• r − p = O(η2(n− p)) (which implies r = O(n)) • r = O( ( η2 n 2\nB2 ln(1/δ)σmin( 1 nX\nTX) ) 2 3 )\n• r − p = Ω( 1+‖β‖ 2\nβ2j + σ\n2 β2j · σ−1min( 1nX TX))\nto have that the (1 − α)-confidence interval around β′j does not intersect the origin. Once again, we comment that these conditions are sufficient but not necessary, and furthermore — even with these conditions holding — we do not make any claims of optimality of our confidence bound. That is because from Proposition C.4 onwards our discussion uses upper bounds that do not have corresponding lower bounds, to the best of our knowledge."
    }, {
      "heading" : "D. Confidence Intervals for “Analyze Gauss” Algorithm",
      "text" : "To complete the picture, we now analyze the “Analyze Gauss” algorithm of Dwork et al (Dwork et al., 2014). Algorithm 2 works by adding random Gaussian noise to ATA, where the noise is symmetric with each coordinate above the diagonal sampled i.i.d from N (0,∆2) with ∆2 = O ( B4 log(1/δ) 2 ) .20 Using the same notation for a sub-matrix of A as [X;y] as before, with X ∈ Rn×p and y ∈ Rn, we denote the output of Algorithm 2 as X̃TX X̃Ty\nỹTX ỹTy\n =  XTX +N XTy +n\nyTX +nT yTy +m  (15)\nwhere N is a symmetric p× p-matrix, n is a p-dimensional vector and m is a scalar, whose coordinates are sampled i.i.d from N (0,∆2).\nUsing the output of Algorithm 2, it is simple to derive analogues of β̂ and ‖ζ‖2 (Equations (1) and (2))\nβ̃ = ( X̃TX )−1 X̃Ty = ( XTX +N )−1 (XTy +n)\n(16)\n‖̃ζ‖2 = ỹTy − 2 ỹTX β̃ + β̃ T X̃TX β̃\n= ỹTy − ỹTX X̃TX −1 X̃Ty (17)\nWe now argue that it is possible to use β̃j and ‖̃ζ‖2 to get a confidence interval for βj under certain conditions. Theorem D.1. Fix α, ν ∈ (0, 12 ). Assume that there exists η ∈ (0, 12 ) s.t. σmin(X TX) > ∆ √ p ln(1/ν)/η. Under the homoscedastic model, given β and σ2, if we assume also that ‖β‖ ≤ B and ‖β̂‖ = ‖(XTX)−1XTy‖ ≤ B, then w.p. ≥ 1− α− ν it holds that |βj − β̃j | it at most O ( ρ · √( X̃TX −1 j,j + ∆ √ p ln(1/ν) · X̃TX −2 j,j ) ln(1/α)\n20It is easy to see that the l2-global sensitivity of the mapping A 7→ ATA is ∝ B4. Fix any A1, A2 that differ on one row which is some vector v with ‖v‖ = B in A1 and the all zero vector in A2. Then GS22 = ‖AT1A1 − AT2A2‖2F = ‖vvT ‖2F = trace(vvT · vvT) = (vTv)2 = B4.\n+ ∆ √ X̃TX −2 j,j · ln(1/ν) · (B √ p+ 1) ) where ρ is such that ρ2 is w.h.p an upper bound on σ2, defined as\nρ2 def = ( 1√\nn−p−2 √ ln(4/α) )2 ·(\n‖̃ζ‖2 − C · ( ∆ B2 √ p\n1−η\n√ ln(1/ν) + ∆2‖X̃TX −1 ‖F · ln(p/ν) )) for some large constant C.\nWe comment that in practice, instead of using ρ, it might be better to use the MLE of σ2, namely:\nσ2 def = 1n−p\n( ‖̃ζ‖2 + ∆2‖X̃TX −1 ‖F )\ninstead of ρ2, the upper bound we derived for σ2. (Replacing an unknown variable with its MLE estimator is a common approach in applied statistics.) Note that the assumption that ‖β‖ ≤ B is fairly benign once we assume each row has bounded l2-norm. The assumption ‖β̂‖ ≤ B simply assumes that β̂ is a reasonable estimation of β , which is likely to hold if we assume that XTX is well-spread. The assumption about the magnitude of the least singular value of XTX is therefore the major one. Nonetheless, in the case we considered before where each row inX is sampled i.i.d from N (0,Σ), this assumption merely means that n is large enough s.t. n = Ω̃(∆ √ p ln(1/ν)\nη·σmin(Σ) ).\nIn order to prove Theorem D.1, we require the following proposition.\nProposition D.2. Fix any ν ∈ (0, 12 ). Fix any matrix M ∈ Rp×p. Let v ∈ Rp be a vector with each coordinate sampled independently from a Gaussian N (0,∆2). Then we have that Pr [ ‖Mv‖ > ∆ · ‖M‖F √ 2 ln(2p/ν) ] < ν.\nProof. Given M , we have that Mv ∼ N (0,∆2 ·MMT). Denoting M ’s singular values as sv1, . . . , svp, we can rotate Mv without affecting its l2-norm and infer that ‖Mv|2 is distributed like a sum on p independent Gaussians, each sampled fromN (0,∆2 · sv2i ). Standard union bound gives that w.p. ≥ 1 − ν non of the p Gaussians exceeds its standard deviation by a factor of √ 2 ln(2p/ν). Hence, w.p.\n≥ 1 − ν it holds that ‖Mv‖2 ≤ 2∆2 ∑ i sv 2 i ln(2p/ν) = 2∆2 · trace(MMT) · ln(2p/ν).\nOur proof also requires the use of the following equality, that holds for any invertible A and any matrix B s.t. I + B ·A−1 is invertible:\n(A+B) −1 = A−1 −A−1 ( I +BA−1 )−1 BA−1\nIn our case, we have\nX̃TX −1\n= (XTX +N)−1\n= (XTX)−1 − (XTX)−1 ( I +N(XTX)−1 )−1 N(XTX)−1\n= (XTX)−1 ( I − ( I +N(XTX)−1 )−1 N(XTX)−1 ) def = (XTX)−1 ( I − Z · (XTX)−1 ) (18)\nProof of Theorem D.1. Fix ν > 0. First, we apply to standard results about Gaussian matrices, such as (Tao, 2012) (used also by (Dwork et al., 2014) in their analysis), to see that w.p. ≥ 1 − ν/6 we have ‖N‖ = O(∆ √ p ln(1/ν)). And so, for the remainder of the proof we fix N subject to having bounded operator norm. Note that by fixing N we\nfix X̃TX .\nRecall that in the homoscedastic model, y = Xβ + e with each coordinate of e sampled i.i.d from N (0, σ2). We therefore have that\nβ̃ = X̃TX −1 (XTy +n) = X̃TX −1 (XTXβ +XTe +n)\n= X̃TX −1 (X̃TX −N)β + X̃TX −1 XTe + X̃TX −1 n\n= β − X̃TX −1 Nβ + X̃TX −1 XTe + X̃TX −1 n\nDenoting the j-th row of X̃TX −1 as X̃TX −1\nj→ we deduce:\nβ̃j = βj − X̃TX −1 j→Nβ + X̃ TX −1 j→X Te + X̃TX −1 j→n\n(19)\nWe naı̈vely bound the size of the term X̃TX −1 j→Nβ by ∥∥∥∥X̃TX −1j→∥∥∥∥ ‖N‖‖β‖ =\nO (∥∥∥∥X̃TX −1j→∥∥∥∥ ·B∆√p ln(1/ν)). To bound X̃TX −1\nj→X Te note that e is cho-\nsen independently of X̃TX and since e ∼ N (0, σ2I) we have X̃TX −1\nj→X Te ∼ N ( 0, σ2 · eTj X̃TX −1 ·XTX · X̃TX −1 ej ) . Since\nwe have\nX̃TX −1 ·XTX · X̃TX −1\n= X̃TX −1 · (X̃TX −N) · X̃TX −1\n= X̃TX −1 − X̃TX −1 ·N · X̃TX −1\nwe can bound the variance of X̃TX −1\nj→X Te by\nσ2 ( X̃TX −1 j,j + ‖N‖ · ∥∥∥∥X̃TX −1j→∥∥∥∥2 ) . Appealing to\nGaussian concentration bounds, we have that w.p. ≥ 1 − α/2 the absolute value of this Gaussian is at most\nO √√√√(X̃TX −1j,j + ∆√p ln(1/ν) · ∥∥∥∥X̃TX −1j→∥∥∥∥2 ) σ2 ln(1/α) . To bound X̃TX −1\nj→n note that n ∼ N (0,∆2I) is sampled independently of X̃TX . We therefore have that X̃TX −1 j→n ∼ N (0,∆2 ∥∥∥∥X̃TX −1j→∥∥∥∥2). Gaussian concentra-\ntion bounds give that w.p≥ 1−ν/6 we have |X̃TX −1\nj→n| =\nO ( ∆ ∥∥∥∥X̃TX −1j→∥∥∥∥√ln(1/ν)). Plugging this into our above bounds on all terms that appear in Equation (19) we have that w.p. ≥ 1 − ν/2 − α/2 we have that\n∣∣∣β̃j − βj∣∣∣ is at most O\n(∥∥∥∥X̃TX −1j→∥∥∥∥ ·B∆√p ln(1/ν)) +O σ √√√√(X̃TX −1j,j + ∆√p ln(1/ν) · ∥∥∥∥X̃TX −1j→∥∥∥∥2 ) ln(1/α)\n +O ( ∆ ∥∥∥∥X̃TX −1j→∥∥∥∥√ln(1/ν))\nNote that due to the symmetry of X̃TX we have∥∥∥∥X̃TX −1j→∥∥∥∥2 = X̃TX −2j,j (the (j, j)-coordinate of the matrix X̃TX −2 ), thus |β̃j − βj | is at most\nO ( σ · √( X̃TX −1 j,j + ∆ √ p ln(1/ν) · X̃TX −2 j,j ) ln(1/α)\n+ ∆ √ X̃TX −2 j,j · ln(1/ν) · (B √ p+ 1) ) (20)\nAll of the terms appearing in Equation (20) are known\ngiven X̃TX , except for σ — which is a parameter of the model. Next, we derive an upper bound on σ which we can then plug into Equation (20) to complete the proof of the theorem and derive a confidence interval for βj .\nRecall Equation (17), according to which we have\n‖̃ζ‖2 = ỹTy − ỹTX X̃TX −1 X̃Ty\n(18) = yTy +m\n− (yTX +nT)(XTX)−1(I − Z · (XTX)−1)(XTy +n) = yTy +m\n− yTX(XTX)−1XTy + yTX(XTX)−1Z(XTX)−1XTy\n− 2yTX(XTX)−1n + 2yTX(XTX)−1Z(XTX)−1n\n−nT(XTX)−1(I − Z · (XTX)−1)n\nRecall that β̂ = (XTX)−1XTy , and so we have\n= yT ( I −X(XTX)−1XT ) y +m− β̂ T Zβ̂\n− 2β̂ T (I − Z(XTX)−1)n −nTX̃TX −1 n (21)\nand of course, both n and m are chosen independently of\nX̃TX and y .\nBefore we bound each term in Equation (21), we first give a bound on ‖Z‖. Recall, Z = ( I +N(XTX)−1 )−1 N . Recall our assumption (given in the statement of Theorem D.1) that σmin(XTX) ≥ ∆η √ p ln(1/ν). This implies that ‖N(XTX)−1‖ ≤ ‖N‖·σmin(XTX)−1 = O(η). Hence\n‖Z‖ ≤ (‖I+N(XTX)−1‖)−1 ·‖N‖ = O ( ∆ √ p ln(1/ν)\n1−η ) Moreover, this implies that ‖Z(XTX)−1‖ ≤ O ( η\n1−η ) and that ‖I − Z(XTX)−1‖ ≤ O ( 1\n1−η\n) .\nArmed with these bounds on the operator norms of Z and (I−Z(XTX)−1) we bound the magnitude of the different terms in Equation (21).\n• The term yT (I −XX+)y is the exact term from the standard OLS, and we know it is distributed like σ2 · χ2n−p distribution. Therefore, it is greater than σ2( √ n− p − 2 √ ln(4/α))2 w.p. ≥ 1− α/2.\n• The scalar m sampled from m ∼ N (0,∆2) is bounded by O(∆ √ ln(1/ν)) w.p. ≥ 1− ν/8.\n• Since we assume ‖β̂‖ ≤ B, the term β̂ T Zβ̂ is upper bounded by B2‖Z‖ = O ( B2∆ √ p ln(1/ν)\n1−η\n) .\n• Denote zTn = 2β̂ T\n(I−Z(XTX)−1)n. We thus have that zTn ∼ N (0,∆2‖z‖2) and that its magnitude is at\nmost O(∆ · ‖z‖ √\nln(1/ν)) w.p. ≥ 1 − ν/8. We can upper bound ‖z‖ ≤ 2‖β̂‖ ‖I − Z(XTX)−1‖ = O( B1−η ), and so this term’s magnitude is upper\nbounded by O ( ∆·B √ ln(1/ν)\n1−η\n) .\n• Given our assumption about the least singular value of XTX and with the bound on ‖N‖, we have that σmin(X̃TX) ≥ σmin(XTX) − ‖N‖ > 0 and so the symmetric matrix X̃TX is a PSD. Therefore,\nthe term nTX̃TX −1 n = ‖X̃TX −1/2 n‖2 is strictly positive. Applying Proposition D.2 we have that w.p. ≥ 1 − ν/8 it holds that nTX̃TX −1 n ≤\nO ( ∆2‖X̃TX −1 ‖F · ln(p/ν) ) .\nPlugging all of the above bounds into Equation (21) we get that w.p. ≥ 1− ν/2− α/2 it holds that σ2 ≤ (\n1√ n−p−2 √ ln(4/α) )2 ·(\n‖̃ζ‖2 +O ( (1 + B2 √ p+B 1−η )∆ √ ln(1/ν) + ∆2‖X̃TX −1 ‖F · ln(p/ν) ))\nand indeed, the RHS is the definition of ρ2 in the statement of Theorem D.1."
    }, {
      "heading" : "E. Experiment: Additional Figures",
      "text" : "To complete our discussion about the experiments we have conducted, we attach here additional figures, plotting both the t-value approximations we get from both algorithms, and the “high-level decision” of whether correctly reject or not-reject the null hypothesis (and with what sign). First, we show the distribution of the t-value approximation for coordinates that should be rejected, in Figure 2, and then the decision of whether to reject or not based on this t-value — and whether it was right, conservative (we didn’t reject while we needed to) or wrong (we rejected with the wrong sign, or rejected when we shouldn’t have rejected) in Figure 3. As one can see, Algorithm 1 has far lower t-values (as expected) and therefore is much more conservative. In fact, it tends to not-reject coordinate 1 of the real-data even on the largest value of n (Figure 3c).\nHowever, because Algorithm 1 also has much smaller variance, it also does not reject when it ought to notreject, whereas Algorithm 2 erroneiously rejects the nullhypotheses. This can be seen in Figures 4 and 5."
    } ],
    "references" : [ {
      "title" : "Statistical Methods for the Social Sciences",
      "author" : [ "A. Agresti", "B. Finlay" ],
      "venue" : null,
      "citeRegEx" : "Agresti and Finlay,? \\Q2009\\E",
      "shortCiteRegEx" : "Agresti and Finlay",
      "year" : 2009
    }, {
      "title" : "Private empirical risk minimization: Efficient algorithms and tight error bounds",
      "author" : [ "R. Bassily", "A. Smith", "A. Thakurta" ],
      "venue" : "In FOCS,",
      "citeRegEx" : "Bassily et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Bassily et al\\.",
      "year" : 2014
    }, {
      "title" : "The Johnson-Lindenstrauss transform itself preserves differential privacy",
      "author" : [ "J. Blocki", "A. Blum", "A. Datta", "O. Sheffet" ],
      "venue" : "In FOCS,",
      "citeRegEx" : "Blocki et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Blocki et al\\.",
      "year" : 2012
    }, {
      "title" : "Convergence rates for differentially private statistical estimation",
      "author" : [ "Chaudhuri", "Kamalika", "Hsu", "Daniel J" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Chaudhuri et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Chaudhuri et al\\.",
      "year" : 2012
    }, {
      "title" : "Differentially private empirical risk minimization",
      "author" : [ "Chaudhuri", "Kamalika", "Monteleoni", "Claire", "Sarwate", "Anand D" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Chaudhuri et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Chaudhuri et al\\.",
      "year" : 2011
    }, {
      "title" : "Local privacy and statistical minimax rates",
      "author" : [ "Duchi", "John C", "Jordan", "Michael I", "Wainwright", "Martin J" ],
      "venue" : "In FOCS, pp",
      "citeRegEx" : "Duchi et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Duchi et al\\.",
      "year" : 2013
    }, {
      "title" : "Differential privacy and robust statistics",
      "author" : [ "C. Dwork", "J. Lei" ],
      "venue" : "In STOC,",
      "citeRegEx" : "Dwork and Lei,? \\Q2009\\E",
      "shortCiteRegEx" : "Dwork and Lei",
      "year" : 2009
    }, {
      "title" : "Our data, ourselves: Privacy via distributed noise generation",
      "author" : [ "Dwork", "Cynthia", "Kenthapadi", "Krishnaram", "McSherry", "Frank", "Mironov", "Ilya", "Naor", "Moni" ],
      "venue" : "In EUROCRYPT,",
      "citeRegEx" : "Dwork et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Dwork et al\\.",
      "year" : 2006
    }, {
      "title" : "Calibrating noise to sensitivity in private data analysis",
      "author" : [ "Dwork", "Cynthia", "Mcsherry", "Frank", "Nissim", "Kobbi", "Smith", "Adam" ],
      "venue" : "In TCC,",
      "citeRegEx" : "Dwork et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Dwork et al\\.",
      "year" : 2006
    }, {
      "title" : "Analyze gauss - optimal bounds for privacy preserving principal component analysis",
      "author" : [ "Dwork", "Cynthia", "Talwar", "Kunal", "Thakurta", "Abhradeep", "Zhang", "Li" ],
      "venue" : "In STOC,",
      "citeRegEx" : "Dwork et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Dwork et al\\.",
      "year" : 2014
    }, {
      "title" : "Private false discovery rate control",
      "author" : [ "Dwork", "Cynthia", "Su", "Weijie", "Zhang", "Li" ],
      "venue" : "CoRR, abs/1511.03803,",
      "citeRegEx" : "Dwork et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Dwork et al\\.",
      "year" : 2015
    }, {
      "title" : "Ridge regression: Biased estimation for nonorthogonal problems",
      "author" : [ "A.E. Hoerl", "R.W. Kennard" ],
      "venue" : "Technometrics, 12:55–67,",
      "citeRegEx" : "Hoerl and Kennard,? \\Q1970\\E",
      "shortCiteRegEx" : "Hoerl and Kennard",
      "year" : 1970
    }, {
      "title" : "What can we learn privately",
      "author" : [ "S. Kasiviswanathan", "H. Lee", "K. Nissim", "S. Raskhodnikova", "A. Smith" ],
      "venue" : "In FOCS,",
      "citeRegEx" : "Kasiviswanathan et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Kasiviswanathan et al\\.",
      "year" : 2008
    }, {
      "title" : "Private convex optimization for empirical risk minimization with applications to high-dimensional regression",
      "author" : [ "Kifer", "Daniel", "Smith", "Adam D", "Thakurta", "Abhradeep" ],
      "venue" : "In COLT,",
      "citeRegEx" : "Kifer et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Kifer et al\\.",
      "year" : 2012
    }, {
      "title" : "Adaptive estimation of a quadratic functional by model selection",
      "author" : [ "B. Laurent", "P. Massart" ],
      "venue" : "The Annals of Statistics, 28(5),",
      "citeRegEx" : "Laurent and Massart,? \\Q2000\\E",
      "shortCiteRegEx" : "Laurent and Massart",
      "year" : 2000
    }, {
      "title" : "On lower bounds for the smallest eigenvalue of a hermitian positivedefinite matrix",
      "author" : [ "E.M. Ma", "Zarowski", "Christopher J" ],
      "venue" : "IEEE Transactions on Information Theory,",
      "citeRegEx" : "Ma et al\\.,? \\Q1995\\E",
      "shortCiteRegEx" : "Ma et al\\.",
      "year" : 1995
    }, {
      "title" : "Linear Model Theory: Univariate, Multivariate, and Mixed Models",
      "author" : [ "Muller", "Keith E", "Stewart", "Paul W" ],
      "venue" : null,
      "citeRegEx" : "Muller et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Muller et al\\.",
      "year" : 2006
    }, {
      "title" : "Randomized sketches of convex programs with sharp guarantees",
      "author" : [ "M. Pilanci", "M. Wainwright" ],
      "venue" : "In ISIT,",
      "citeRegEx" : "Pilanci and Wainwright,? \\Q2014\\E",
      "shortCiteRegEx" : "Pilanci and Wainwright",
      "year" : 2014
    }, {
      "title" : "Iterative hessian sketch: Fast and accurate solution approximation for constrained least-squares",
      "author" : [ "Pilanci", "Mert", "Wainwright", "Martin J" ],
      "venue" : "CoRR, abs/1411.0347,",
      "citeRegEx" : "Pilanci et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Pilanci et al\\.",
      "year" : 2014
    }, {
      "title" : "Linear statistical inference and its applications",
      "author" : [ "Rao", "C. Radhakrishna" ],
      "venue" : null,
      "citeRegEx" : "Rao and Radhakrishna.,? \\Q1973\\E",
      "shortCiteRegEx" : "Rao and Radhakrishna.",
      "year" : 1973
    }, {
      "title" : "Differentially private chi-squared hypothesis testing: Goodness of fit and independence testing",
      "author" : [ "Rogers", "Ryan M", "Vadhan", "Salil P", "Lim", "Hyun-Woo", "Gaboardi", "Marco" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Rogers et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Rogers et al\\.",
      "year" : 2016
    }, {
      "title" : "Smallest singular value of a random rectangular matrix",
      "author" : [ "Rudelson", "Mark", "Vershynin", "Roman" ],
      "venue" : "Comm. Pure Appl. Math, pp",
      "citeRegEx" : "Rudelson et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Rudelson et al\\.",
      "year" : 2009
    }, {
      "title" : "Improved approx. algs for large matrices via random projections",
      "author" : [ "T. Sarlós" ],
      "venue" : "In FOCS,",
      "citeRegEx" : "Sarlós,? \\Q2006\\E",
      "shortCiteRegEx" : "Sarlós",
      "year" : 2006
    }, {
      "title" : "Private approximations of the 2nd-moment matrix using existing techniques in linear regression",
      "author" : [ "O. Sheffet" ],
      "venue" : "CoRR, abs/1507.00056,",
      "citeRegEx" : "Sheffet,? \\Q2015\\E",
      "shortCiteRegEx" : "Sheffet",
      "year" : 2015
    }, {
      "title" : "Privacy-preserving statistical estimation with optimal convergence rates",
      "author" : [ "Smith", "Adam D" ],
      "venue" : "In STOC, pp",
      "citeRegEx" : "Smith and D.,? \\Q2011\\E",
      "shortCiteRegEx" : "Smith and D.",
      "year" : 2011
    }, {
      "title" : "Impact of HbA1c measurement on hospital readmission rates: Analysis of 70,000 clinical database patient",
      "author" : [ "B. Strack", "J. DeShazo", "C. Gennings", "J. Olmo", "S. Ventura", "K. Cios", "J. Clore" ],
      "venue" : "records. BioMed Research International,",
      "citeRegEx" : "Strack et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Strack et al\\.",
      "year" : 2014
    }, {
      "title" : "Topics in Random Matrix Theory",
      "author" : [ "T. Tao" ],
      "venue" : "American Mathematical Soc.,",
      "citeRegEx" : "Tao,? \\Q2012\\E",
      "shortCiteRegEx" : "Tao",
      "year" : 2012
    }, {
      "title" : "Differentially private feature selection via stability arguments, and the robustness of the lasso",
      "author" : [ "Thakurta", "Abhradeep", "Smith", "Adam" ],
      "venue" : "In COLT,",
      "citeRegEx" : "Thakurta et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Thakurta et al\\.",
      "year" : 2013
    }, {
      "title" : "Solution of incorrectly formulated problems and the regularization method",
      "author" : [ "A.N. Tikhonov" ],
      "venue" : "Soviet Math. Dokl.,",
      "citeRegEx" : "Tikhonov,? \\Q1963\\E",
      "shortCiteRegEx" : "Tikhonov",
      "year" : 1963
    }, {
      "title" : "Privacy-preserving data sharing for genome-wide association studies",
      "author" : [ "Uhler", "Caroline", "Slavkovic", "Aleksandra B", "Fienberg", "Stephen E" ],
      "venue" : "Journal of Privacy and Confidentiality,",
      "citeRegEx" : "Uhler et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Uhler et al\\.",
      "year" : 2013
    }, {
      "title" : "Private multiplicative weights beyond linear queries",
      "author" : [ "J. Ullman" ],
      "venue" : "In PODS,",
      "citeRegEx" : "Ullman,? \\Q2015\\E",
      "shortCiteRegEx" : "Ullman",
      "year" : 2015
    }, {
      "title" : "Differential privacy for clinical trial data: Preliminary evaluations",
      "author" : [ "D. Vu", "A. Slavkovic" ],
      "venue" : "In ICDM,",
      "citeRegEx" : "Vu and Slavkovic,? \\Q2009\\E",
      "shortCiteRegEx" : "Vu and Slavkovic",
      "year" : 2009
    }, {
      "title" : "Differentially private hypothesis testing, revisited",
      "author" : [ "Wang", "Yue", "Lee", "Jaewoo", "Kifer", "Daniel" ],
      "venue" : "CoRR, abs/1511.03376,",
      "citeRegEx" : "Wang et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2015
    }, {
      "title" : "Mixture of gaussian models and bayes error under differential privacy",
      "author" : [ "B. Xi", "M. Kantarcioglu", "A. Inan" ],
      "venue" : "In CODASPY. ACM,",
      "citeRegEx" : "Xi et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Xi et al\\.",
      "year" : 2011
    }, {
      "title" : "Comparison with Existing Bounds. Sarlos’ work (2006) utilizes the fact that when r, the numbers of rows in R, is large enough",
      "author" : [ "n−p" ],
      "venue" : null,
      "citeRegEx" : ".,? \\Q2006\\E",
      "shortCiteRegEx" : ".",
      "year" : 2006
    } ],
    "referenceMentions" : [ {
      "referenceID" : 9,
      "context" : "First, we show that for wellspread data, the Gaussian Johnson-Lindenstrauss Transform (JLT) gives a very good approximation of t-values; secondly, when JLT approximates Ridge regression (linear regression with l2-regularization) we derive, under certain conditions, confidence intervals using the projected data; lastly, we derive, under different conditions, confidence intervals for the “Analyze Gauss” algorithm (Dwork et al., 2014).",
      "startOffset" : 415,
      "endOffset" : 435
    }, {
      "referenceID" : 2,
      "context" : "We emphasize that the novelty of our work does not lie in the differentially-private algorithms, which are, as we discuss next, based on the Johnson-Lindenstrauss Transform (JLT) and on additive Gaussian noise and are already known to be differentially private (Blocki et al., 2012; Dwork et al., 2014).",
      "startOffset" : 261,
      "endOffset" : 302
    }, {
      "referenceID" : 9,
      "context" : "We emphasize that the novelty of our work does not lie in the differentially-private algorithms, which are, as we discuss next, based on the Johnson-Lindenstrauss Transform (JLT) and on additive Gaussian noise and are already known to be differentially private (Blocki et al., 2012; Dwork et al., 2014).",
      "startOffset" : 261,
      "endOffset" : 302
    }, {
      "referenceID" : 4,
      "context" : "Since, we do not deal with OLS based on the private single-regression ERM algorithms (Chaudhuri et al., 2011; Bassily et al., 2014) as such inference requires us to use the Fisher-information matrix of the loss function — but these algorithms do not minimize a private loss-function but rather prove that outputting the minimizer of the perturbed loss-function is private.",
      "startOffset" : 85,
      "endOffset" : 131
    }, {
      "referenceID" : 1,
      "context" : "Since, we do not deal with OLS based on the private single-regression ERM algorithms (Chaudhuri et al., 2011; Bassily et al., 2014) as such inference requires us to use the Fisher-information matrix of the loss function — but these algorithms do not minimize a private loss-function but rather prove that outputting the minimizer of the perturbed loss-function is private.",
      "startOffset" : 85,
      "endOffset" : 131
    }, {
      "referenceID" : 10,
      "context" : ") We leave this approach — as well as performing private hypothesis testing using a PTR-type algorithm (Dwork & Lei, 2009) (output merely reject / don’t-reject decision without justification), or releasing only relevant tests judging by their p-values (Dwork et al., 2015) — for future work.",
      "startOffset" : 252,
      "endOffset" : 272
    }, {
      "referenceID" : 28,
      "context" : "In this case, solving the linear regression problem on the projected A′ approximates the solution for Ridge Regression (Tikhonov, 1963; Hoerl & Kennard, 1970).",
      "startOffset" : 119,
      "endOffset" : 158
    }, {
      "referenceID" : 9,
      "context" : "In Section 5 we discuss the “Analyze Gauss” algorithm (Dwork et al., 2014) that outputs a noisy version of a covariance of a given matrix using additive noise rather than multiplicative noise.",
      "startOffset" : 54,
      "endOffset" : 74
    }, {
      "referenceID" : 33,
      "context" : "Empirical work (Xi et al., 2011) shows that Analyze Gauss’s output might be non-PSD if the input has small singular values, and this results in truly bad regressors.",
      "startOffset" : 15,
      "endOffset" : 32
    }, {
      "referenceID" : 5,
      "context" : "Some works have already looked at the intersection of differentially privacy and statistics (Dwork & Lei, 2009; Smith, 2011; Chaudhuri & Hsu, 2012; Duchi et al., 2013; Dwork et al., 2015) (especially focusing on robust statistics and rate of convergence).",
      "startOffset" : 92,
      "endOffset" : 187
    }, {
      "referenceID" : 10,
      "context" : "Some works have already looked at the intersection of differentially privacy and statistics (Dwork & Lei, 2009; Smith, 2011; Chaudhuri & Hsu, 2012; Duchi et al., 2013; Dwork et al., 2015) (especially focusing on robust statistics and rate of convergence).",
      "startOffset" : 92,
      "endOffset" : 187
    }, {
      "referenceID" : 29,
      "context" : "But only a handful of works studied the significance and power of hypotheses testing under differential privacy, without arguing that the noise introduced by differential privacy vanishes asymptotically (Vu & Slavkovic, 2009; Uhler et al., 2013; Wang et al., 2015; Rogers et al., 2016).",
      "startOffset" : 203,
      "endOffset" : 285
    }, {
      "referenceID" : 32,
      "context" : "But only a handful of works studied the significance and power of hypotheses testing under differential privacy, without arguing that the noise introduced by differential privacy vanishes asymptotically (Vu & Slavkovic, 2009; Uhler et al., 2013; Wang et al., 2015; Rogers et al., 2016).",
      "startOffset" : 203,
      "endOffset" : 285
    }, {
      "referenceID" : 20,
      "context" : "But only a handful of works studied the significance and power of hypotheses testing under differential privacy, without arguing that the noise introduced by differential privacy vanishes asymptotically (Vu & Slavkovic, 2009; Uhler et al., 2013; Wang et al., 2015; Rogers et al., 2016).",
      "startOffset" : 203,
      "endOffset" : 285
    }, {
      "referenceID" : 1,
      "context" : "Observe, overall this result is similar in nature to many other results in differentially private learning (Bassily et al., 2014) which are of the form “without privacy, in order to achieve a total loss of ≤ η we have a sample complexity bound of some Nη; and with differential privacy the sample complexity increases to Nη + Ω( √ Nη/ ).",
      "startOffset" : 107,
      "endOffset" : 129
    }, {
      "referenceID" : 28,
      "context" : "Invented in the 60s (Tikhonov, 1963; Hoerl & Kennard, 1970), Ridge Regression is often motivated from the perspective of penalizing linear vectors whose coefficients are too large.",
      "startOffset" : 20,
      "endOffset" : 59
    }, {
      "referenceID" : 25,
      "context" : "We ran the two algorithms over diabetes dataset collected over ten years (1999-2008) taken from the UCI repository (Strack et al., 2014).",
      "startOffset" : 115,
      "endOffset" : 136
    }, {
      "referenceID" : 30,
      "context" : "Lastly the author thanks the anonymous referees for many helpful suggestions in general and for a reference to (Ullman, 2015) in particular.",
      "startOffset" : 111,
      "endOffset" : 125
    }, {
      "referenceID" : 23,
      "context" : "The proof of the theorem is based on the fact the Algorithm 1 is the result of composing the differentially private Propose-Test-Release algorithm of (Dwork & Lei, 2009) with the differentially private analysis of the Johnson-Lindenstrauss transform of (Sheffet, 2015).",
      "startOffset" : 253,
      "endOffset" : 268
    }, {
      "referenceID" : 23,
      "context" : "1 from (Sheffet, 2015) that states that given a matrix A whose all of its singular values at greater than T ( , δ) where T ( , δ) = 2B (√ 2r ln(4/δ) + 2 ln(4/δ) ) , publishing RA is ( , δ)differentially private for a r-row matrix R whose entries sampled are i.",
      "startOffset" : 7,
      "endOffset" : 22
    }, {
      "referenceID" : 7,
      "context" : "It is known (Dwork et al., 2006b) that if ALG outputs a vector in R such that for any A and A′ it holds that ‖ALG(A) − ALG(A)‖1 ≤ B, then adding Laplace noise Lap(1/ ) to each coordinate of the output of ALG(A) satisfies -differential privacy. Similarly, (2006b) showed that if for any neighboring A and A′ it holds that ‖ALG(A)−ALG(A)‖2 ≤ ∆ then adding Gaussian noise N (0,∆ · 2 ln(2/δ) 2 ) to each coordinate of the output of ALG(A) satisfies ( , δ)-differential privacy.",
      "startOffset" : 13,
      "endOffset" : 263
    }, {
      "referenceID" : 22,
      "context" : "In this setting, Sarlos’ work (Sarlós, 2006) (Theorem 12(3)) guarantees that w.",
      "startOffset" : 30,
      "endOffset" : 44
    }, {
      "referenceID" : 4,
      "context" : ") As mentioned in the introduction, alternative techniques ((Chaudhuri et al., 2011; Bassily et al., 2014; Ullman, 2015)) for finding a DP estimator β of the linear regression give a data-independent12 bound of ‖β − β̂‖ = Õ(p/ ).",
      "startOffset" : 60,
      "endOffset" : 120
    }, {
      "referenceID" : 1,
      "context" : ") As mentioned in the introduction, alternative techniques ((Chaudhuri et al., 2011; Bassily et al., 2014; Ullman, 2015)) for finding a DP estimator β of the linear regression give a data-independent12 bound of ‖β − β̂‖ = Õ(p/ ).",
      "startOffset" : 60,
      "endOffset" : 120
    }, {
      "referenceID" : 30,
      "context" : ") As mentioned in the introduction, alternative techniques ((Chaudhuri et al., 2011; Bassily et al., 2014; Ullman, 2015)) for finding a DP estimator β of the linear regression give a data-independent12 bound of ‖β − β̂‖ = Õ(p/ ).",
      "startOffset" : 60,
      "endOffset" : 120
    }, {
      "referenceID" : 23,
      "context" : "1 from (Sheffet, 2015) gives that w.",
      "startOffset" : 7,
      "endOffset" : 22
    }, {
      "referenceID" : 28,
      "context" : "Invented in the 60s (Tikhonov, 1963; Hoerl & Kennard, 1970), Ridge Regression is often motivated from the perspective of penalizing linear vectors whose coefficients are too large.",
      "startOffset" : 20,
      "endOffset" : 59
    }, {
      "referenceID" : 9,
      "context" : "To complete the picture, we now analyze the “Analyze Gauss” algorithm of Dwork et al (Dwork et al., 2014).",
      "startOffset" : 85,
      "endOffset" : 105
    }, {
      "referenceID" : 26,
      "context" : "First, we apply to standard results about Gaussian matrices, such as (Tao, 2012) (used also by (Dwork et al.",
      "startOffset" : 69,
      "endOffset" : 80
    }, {
      "referenceID" : 9,
      "context" : "First, we apply to standard results about Gaussian matrices, such as (Tao, 2012) (used also by (Dwork et al., 2014) in their analysis), to see that w.",
      "startOffset" : 95,
      "endOffset" : 115
    } ],
    "year" : 2017,
    "abstractText" : "Linear regression is one of the most prevalent techniques in machine learning; however, it is also common to use linear regression for its explanatory capabilities rather than label prediction. Ordinary Least Squares (OLS) is often used in statistics to establish a correlation between an attribute (e.g. gender) and a label (e.g. income) in the presence of other (potentially correlated) features. OLS assumes a particular model that randomly generates the data, and derives tvalues — representing the likelihood of each real value to be the true correlation. Using t-values, OLS can release a confidence interval, which is an interval on the reals that is likely to contain the true correlation; and when this interval does not intersect the origin, we can reject the null hypothesis as it is likely that the true correlation is non-zero. Our work aims at achieving similar guarantees on data under differentially private estimators. First, we show that for wellspread data, the Gaussian Johnson-Lindenstrauss Transform (JLT) gives a very good approximation of t-values; secondly, when JLT approximates Ridge regression (linear regression with l2-regularization) we derive, under certain conditions, confidence intervals using the projected data; lastly, we derive, under different conditions, confidence intervals for the “Analyze Gauss” algorithm (Dwork et al., 2014).",
    "creator" : "LaTeX with hyperref package"
  }
}