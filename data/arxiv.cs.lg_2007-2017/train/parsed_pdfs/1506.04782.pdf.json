{
  "name" : "1506.04782.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Cheap Bandits",
    "authors" : [ "Manjesh Kumar Hanawal", "Venkatesh Saligrama", "Michal Valko" ],
    "emails" : [ "MHANAWAL@BU.EDU", "SRV@BU.EDU", "MICHAL.VALKO@INRIA.FR", "REMI.MUNOS@INRIA.FR" ],
    "sections" : [ {
      "heading" : null,
      "text" : "√ dT ) lower bound on the\ncumulative regret of spectral bandits for a class of graphs with effective dimension d."
    }, {
      "heading" : "1. Introduction",
      "text" : "In many online learning and bandit problems, the learner is asked to select a single action for which it obtains a (possibly contextual) feedback. However, in many scenarios such as surveillance, monitoring and exploration of a large area or network, it is often cheaper to obtain an average reward for a group of actions rather than a reward for a single\nProceedings of the 32nd International Conference on Machine Learning, Lille, France, 2015. JMLR: W&CP volume 37. Copyright 2015 by the author(s).\none. In this paper, we therefore study group actions and formalize this setting as cheap bandits on graph structured data. Nodes and edges in our graph model the geometric structure of the data and we associate signals (rewards) with each node. We are interested in problems where the actions are a collection of nodes. Our objective is to locate nodes with largest rewards.\nThe cost-aspect of our problem arises in sensor networks (SNETs) for target localization and identification. In SNETs sensors have limited sensing range (Ermis & Saligrama, 2010; 2005)and can reliably sense/identify targets only in their vicinity. To conserve battery power, sleep/awake scheduling is used (Fuemmeler & Veeravalli, 2008; Aeron et al., 2008), wherein a group of sensors is woken up sequentially based on probable locations of target. The group of sensors minimize transmit energy through coherent beamforming of sensed signal, which is then received as an average reward/signal at the receiver. While coherent beam forming is cheaper, it nevertheless increases target ambiguity since the sensed field degrades with distance from target. A similar scenario arises in aerial reconnaissance as well: Larger areas can be surveilled at higher altitudes more quickly (cheaper) but at the cost of more target ambiguity.\nMoreover, sensing average rewards through group actions, in the initial phases, is also meaningful. Rewards in many applications are typically smooth band-limited graph signals (Narang et al., 2013) with the sensing field decaying smoothly with distance from the target. In addition to SNETs (Zhu & Rabbat, 2012), smooth graph signals also arise in social networks (Girvan & Newman, 2002), and recommender systems. Signals on graphs is an emerging area in signal processing (SP) but the emphasis is on reconstruction through sampling and interpolation from a\nar X\niv :1\n50 6.\n04 78\n2v 2\n[ cs\n.L G\n] 1\n8 Ju\nsmall subset of nodes (Shuman et al., 2013). In contrast, our goal is in locating the maxima of graph signals rather than reconstruction. Nevertheless, SP does provide us with the key insight that whenever the graph signal is smooth, we can obtain information about a location by sampling its neighborhood.\nOur approach is to sequentially discover the nodes with optimal reward. We model this problem as an instance of linear bandits (Auer, 2002; Dani et al., 2008; Li et al., 2010) that links the reward of nodes through an unknown parameter. A bandit setting for smooth signals was recently studied by Valko et al. (2014), however neglecting the signal cost. While typically bandit algorithms aim to minimize the regret, we aim to minimize both regret and the signal cost. Nevertheless, we do not want to tradeoff the regret for cost. In particular, we are not compromising regret for cost, neither we seek a Pareto frontier of two objectives. We seek algorithms that minimize the cost of sensing and at the same time attain, the state-of-the-art regret guarantees.\nNotice that our setting directly generalizes the traditional setting with single action per time step as the arms themselves are graph signals. We define cost of each arm in terms of their graph Fourier transform. The cost is quadratic in nature and assigns higher cost to arms that collect average information from a smaller set of neighbors. Our goal is to collect higher reward from the nodes while keeping the total cost small. However, there is a tradeoff in choosing low cost signals and higher reward collection: The arms collecting reward from individual nodes cost more, but give more specific information about node’s reward and hence provide better estimates. On other hand, arms that collect average reward from subset of its neighbors cost less, but only give crude estimate of the reward function. In this paper, we develop an algorithm maximizing the reward collection while keeping the cost low."
    }, {
      "heading" : "2. Related Work",
      "text" : "There are several other bandit and online learning settings that consider costs (Tran-Thanh et al., 2012; Badanidiyuru et al., 2013; Ding et al., 2013; Badanidiyuru et al., 2014; Zolghadr et al., 2013; Cesa-Bianchi et al., 2013a). The first set is referred to as budgeted bandits (Tran-Thanh et al., 2012) or bandits with knapsacks (Badanidiyuru et al., 2013), where each single arm is associated with a cost. This cost can be known or unknown (Ding et al., 2013) and can depend on a given context (Badanidiyuru et al., 2014). The goal there is in general to minimize the regret as a function of budget instead of time or to minimize regret under budget constraints, where there is no advantage in not spending all the budget. Our goal is different as we care both about minimizing the budget and minimizing the regret as a function of time. Another cost setting considers\ncost for observing features from which the learner can build its prediction (Zolghadr et al., 2013). This is different from our consideration of cost, which is inversely proportional to the sensing area. Finally, in the adversarial setting (CesaBianchi et al., 2013a), considers cost for switching actions.\nThe most related graph bandits setting to ours is by Valko et al. (2014) on which we build this paper. Another graph bandit setting considers side information, when the learner obtains besides the reward of the node it chooses, also the rewards of the neighbors (Mannor & Shamir, 2011; Alon et al., 2013; Caron et al., 2012; Kocák et al., 2014). Finally a different graph bandit setup is gang of (multiple) bandits considered in (Cesa-Bianchi et al., 2013b) and online clustering of bandits in (Gentile et al., 2014).\nOur main contribution is the incorporation of sensing cost into learning in linear bandit problems while simultaneously minimizing two performance metrics: cumulative regret and the cumulative sensing cost. We develop CheapUCB, the algorithm that guarantees regret bound of the order d √ T , where d is the effective dimension and T is the number of rounds. This regret bound is of the same order as SpectralUCB (Valko et al., 2014) that does not take cost into consideration. However, we show that our algorithm provides a cost saving that is linear in T compared to the cost of SpectralUCB. The effective dimension d that appears in the bound is a dimension typically smaller in realworld graphs as compared to number of nodes N . This is in contrast with linear bandits that can achieve in this graph setting the regret of N √ T or √ NT . However, our ideas of cheap sensing are directly applicable to the linear bandit setting as well. As a by-product of our analysis, we establish a Ω( √ dT ) lower bound on the cumulative regret for a class of graphs with effective dimension d."
    }, {
      "heading" : "3. Problem Setup",
      "text" : "Let G = (V, E) denote an undirected graph with number of nodes |V| = N . We assume that degree of all the nodes is bounded by κ. Let s : V → R denote a signal on G, and S the set of all possible signals on G. Let L = D −A denote the unnormalized Laplacian of the graph G, where A = {aij} is the adjacency matrix and D is the diagonal matrix with Dii = ∑ j aij . We emphasize that our main results extend to weighted graphs if we replace the matrix Awith the edge weight matrixW . We work with matrixA for simplicity of exposition. We denote the eigenvalues of L as 0 = λ1 ≤ λ2 ≤ · · · ≤ λN , and the corresponding eigenvectors as q1,q2, · · · ,qN. Equivalently, we write L = QΛLQ\n′, where ΛL = diag(λ1, λ2, · · · , λN ) and Q is the N × N orthonormal matrix with eigenvectors in columns. We denote transpose of a as a′, and all vectors are by default column vectors. For a given matrix V , we denote V -norm of a vector a as ‖a‖V = √ a′V a."
    }, {
      "heading" : "3.1. Reward function",
      "text" : "We define a reward function on a graph G as a linear combination of the eigenvectors. For a given parameter vector α ∈ RN , let fα : V → R denote the reward function on the nodes defined as\nfα = Qα.\nThe parameter α can be suitably penalized to control the smoothness of the reward function. For instance, if we choose α such that large coefficients correspond to the eigenvectors associated with small eigenvalues then fα is a smooth function of G (Belkin et al., 2008). We denote the unknown parameter that defines the true reward function as α∗. We denote the reward of node i as fα∗(i).\nIn our setting, the arms are nodes and the subsets of their neighbors. When an arm is selected, we observe only the average of the rewards of the nodes selected by that arm. To make this notion formal, we associate arms with probe signals on graphs."
    }, {
      "heading" : "3.2. Probes",
      "text" : "Let S ⊆ { s ∈ [0, 1]N : ∑N i=1 si = 1 } denote the set of probes. We use the word probe and action interchangeably. A probe is a signal with its width corresponding to the support of the signal s. For instance, it could correspond to the region-of-coverage or region-of-interest probed by a radar pulse. Thus each s ∈ S is of the form si = 1/supp(s), for all i = 1, 2, · · · , N , where supp(s) denotes the number of positive elements in s. The inner product of fα∗ and a probe s is the average reward of supp(s) number of nodes.\nWe parametrize a probe in terms of its width w ∈ [N ] and let the set of probes of width w to be S̃w = {s ∈ S : supp(s) = w}. For a given w > 0, our focus in this paper is on probes with uniformly weighted components, which are limited to neighborhoods of each node on the graph. We denote the collection of these probes as Sw ⊂ S̃w, which has N elements. We denote the element in Sw associated with node i as swi . Suppose node i has neighbors at {j1, j2, · · · jw−1}, then swi is described as:\nswik =  1/w if k = i 1/w if k = ji, i = 1, 2, · · · , w − 1 0 otherwise. (1)\nIf node i has more than w neighbors, there can be multiple ways to define swi depending on the choice of its neighbors. When w is less than degree of node i, in defining swi we only consider neighbors with larger edge weights. If all the weights are the same, then we select w neighbors arbitrarily. Note that |Sw| = N for all w. In the following we write ‘probing with s’ to mean that s is used to get information from nodes of graph G.\nWe define the arms as the set\nSD := {Sw : w = 1, 2, · · · , N}.\nCompared to multi-arm and linear bandits, the number of arms K is O(N2) and the contexts have dimension N ."
    }, {
      "heading" : "3.3. Cost of probes",
      "text" : "The cost of the arms are defined using the spectral properties of their associated graph probes. Let s̃ denote the graph Fourier transform (GFT) of probe s ∈ S . Analogous to Fourier transform of a continuous function, GFT gives amplitudes associated with graph frequencies. The GFT coefficient of a probe on frequency λi, i = 1, 2 · · · , N is obtained by projecting it on qi, i.e.,\ns̃ = Q′s,\nwhere s̃i, i = 1, 2, · · · , N is the GFT coefficient associated with frequency λi. Let C : S → R+ denote the cost function. Then the cost of the probe s is described by\nC(s) = ∑ i∼j (si − sj)2,\nwhere the summation is over all the unordered node pairs {i, j} for which node i is adjacent to node j. We motivate this cost function from the SNET perspective where probes with large width are relatively cheap. We first observe that the cost of a constant probe is zero. For a probe, swi ∈ Sw, of width w it follows that1,\nC(swi ) = w − 1 w2\n( 1− 1\nN\n) + 1\nw2 . (2)\nNote that the cost of w- width probe associated with node i depends only on its width w. For w = 1, C(s1i ) = 1 for all i = 1, 2, · · · , N . That is, the cost of probing individual nodes of the graph is the same. Also note that C(swi ) is decreasing in w, implying that probing a node is more costly than probing a subset of its neighbors.\nAlternatively, we can associate probe costs with eigenvalues of the graph Laplacian. Constant probes corresponds to the zero eigenvalue of the graph Laplacian. More generally, we see that,\nC(s) = ∑ i∼j (si − sj)2 = s′Ls = N∑ i=1 λis̃ 2 i = s̃ ′ΛLs̃.\nIt follows that C(s) = ‖s‖2L. The operation of pulling an arm and observing a reward is equivalent to probing the\n1We symmetrized the graph by adding self loops to all the nodes to make their degree (number of neighbors) N , and normalized the cost by N .\ngraph with a probe. This results in a value that is the inner product of the probe signal and graph reward function. We write the reward in the probe space SD as follows. Let FG : S → R defined as\nFG(s) = s ′Qα∗ = s̃′α∗\ndenote the reward obtained from probe s. Thus, each arm gives a reward that is linear, and has quadratic cost, in its GFT coefficients. In terms of the linear bandit terminology, the GFT coefficients in SD constitute the set of arms.\nWith the rewards defined in terms of the probes, the optimization of reward function is over the action space. Let s∗ = arg maxs∈SD FG(s) denote the probe that gives the maximum reward. This is a straightforward linear optimization problem if the function parameter α∗ is known. When α∗ is unknown we can learn the function through a sequence of measurements."
    }, {
      "heading" : "3.4. Learning setting and performance metrics",
      "text" : "Our learning setting is the following. The learner uses a policy π : {1, 2, · · · , T} → SD that assigns at step t ≤ T , probe π(t). In each step t, the recommender incurs a cost C(π(t)) and obtains a noisy reward such that\nrt = FG(π(t)) + εt,\nwhere εt is independent R-sub Gaussian for any t.\nThe cumulative regret of policy π is defined as\nRT = TFG(s∗)− T∑ t=1 FG(π(t)) (3)\nand the total cost incurred up to time T is given by\nCT = T∑ t=1 C(π(t)). (4)\nThe goal of the learner is to learn a policy π that minimizes total cost CT while keeping the cumulative (pseudo) regret RT as low as possible.\nNode vs. Group actions: The set SD allows actions that can probe a node (node-action) or a subset of nodes (groupaction). Though the group actions have smaller cost, they only provide average reward information for the selected nodes. In contrast, node actions provide crisper information of the reward for the selected node, but at a cost premium. Thus, an algorithm that uses only node actions can provide a better regret performance compared to the one that takes group actions. But if the algorithms use only node actions, the cumulative cost can be high.\nIn the following, we first state the regret performance of the SpectralUCB algorithm (Valko et al., 2014) that uses only\nnode actions. We then develop an algorithm that aims to achieve the same order of regret using group actions and reducing the total sensing cost."
    }, {
      "heading" : "4. Node Actions: Spectral Bandits",
      "text" : "If we restrict the action set to SD = {ei : i = 1, 2, · · · , n}, where ei denotes a binary vector with ith component set to 1 and all the other components set to 0, then only node actions are allowed in each step. In this setting, the cost is the same for all the actions, i.e., C(ei) = 1 for all i.\nUsing these node actions, Valko et al. (2014) developed SpectralUCB that aims to minimize the regret under the assumption that the reward function is smooth. The smoothness condition is characterized as follows:\n∃ c > 0 such that ‖α∗‖Λ ≤ c. (5)\nHere Λ = ΛL + λI , and λ > 0 is used to make ΛL invertible. The bound c characterizes the smoothness of the reward. When c is small, the rewards on the neighboring nodes are more similar. In particular, when the reward function is a constant, then c = 0. To characterize the regret performance of SpectralUCB, Valko et al. (2014) introduced the notion of effective dimension defined as follows:\nDefinition 1 (Effective dimension) For graph G, let us denote λ = λ1 ≤ λ2 · · · ≤ λN the diagonal elements of Λ. Given T , effective dimension is the largest d such that:\n(d− 1)λd ≤ T\nlog(T/λ+ 1) < dλd+1. (6)\nTheorem 1 (Valko et al., 2014) The cumulative regret of SpectralUCB is bounded with probability at least 1− δ as:\nRT ≤ ( 8R √ d log(1 + T/λ) + 2 log(1/δ) + 4c ) × √ dT log(1 + T/λ),\nLemma 1 The total cost of the SpectralUCB is CT = T .\nNote that effective dimension depends on T and also on how fast the eigenvalues grow. The regret performance of SpectralUCB is good when d is small, which occurs when the eigenspectrum exhibits large gaps. For these situations, SpectralUCB performance has a regret that scales as O(d √ T ) for a large range of values of T . To see this, notice that in relation (6) when λd+1/λd is large, the value of effective dimension remains unchanged over a large range of T implying that the regret bound of O(d √ T ) is valid for a large range of values of T with the same d.\nThere are many graphs for which the effective dimension is small. For example, random graphs are good expanders for\nwhich eigenvalues grow fast. Another setting are stochastic block models (Girvan & Newman, 2002), that exhibit large eigenvalue gap and are popular in the analysis of social, biological, citation, and information networks."
    }, {
      "heading" : "5. Group Actions: Cheap Bandits",
      "text" : "Recall (Section 3.3) that group actions are cheaper than the node actions. Furthermore, that the cost of group actions is decreasing in group size. In this section, we develop a learning algorithm that aims to minimize the total cost without compromising on the regret using group actions. Specifically, given T and a graph with effective dimension d our objective is as follows:\nmin π\nCT subject to RT . d √ T . (7)\nwhere optimization is over policies defined on the action set SD given in subsection 3.2."
    }, {
      "heading" : "5.1. Lower bound",
      "text" : "The action set used in the above optimization problem is larger than the set used in the SpectralUCB. This raises the question of whether or not the regret order of d √ T is too loose particularly when SpectralUCB can realize this bound using a much smaller set of probes. In this section we derive a √ dT lower bound on the expected regret (worst-case) for any algorithm using action space SD on graphs with effective dimension d. While this implies that our target in (7) should be √ dT , we follow Valko et al. (2014) and develop a variation of SpectralUCB that obtains the target regret of d √ T . We leave it as a future work to develop an algorithm that meets the target regret of√ dT while minimizing the cost.\nLet Gd denote a set of graphs with effective dimension d. For a given policy π,α∗, T and graph G. Define expected cumulative reward as\nRegret(T, π,α∗, G) = E [ T∑ t=1 s̃∗α ∗ − s̃tα∗ ∣∣∣α∗]\nwhere s̃t = π′(t)Q.\nProposition 1 For any policy π and time period T , there exists a graph G ∈ Gd and a α∗ ∈ Rd representing a smooth reward such that\nRegret(T, π,α∗, G) = Ω( √ dT )\nThe proof follows by construction of a graph with d disjoint cliques and restricting the rewards to be piecewise constant on the cliques. The problem then reduces to identifying the\nclique with the highest reward. We then reduce the problem to the multi-arm case, using Theorem 5.1 of Auer et al. (2003) and lower bound the minimax risk. See the supplementary material for a detailed proof."
    }, {
      "heading" : "5.2. Local smoothness",
      "text" : "In this subsection we show that a smooth reward function on a graph with low effective dimension implies local smoothness of the reward function around each node. Specifically, we establish that the average reward around the neighborhood of a node provides good information about the reward of the node itself. Then, instead of probing a node, we can use group actions to probe its neighborhood and get good estimates of the reward at low cost.\nFrom the discussion in Section 4, when d is small and there is a large gap between the λd and λd+1, SpectralUCB enjoys a small bound on the regret for a large range of values in the interval [(d − 1)λd, dλd+1]. Intuitively, a large gap between the eigenvalues implies that there is a good partitioning of the graph into tight clusters. Furthermore, the smoothness assumption implies that the reward of a node and its neighbors within each cluster are similar.\nLet Ni denote a set of neighbors of node i. The following result provides a relation between the reward of node i and the average reward from Ni of its neighbors.\nProposition 2 Let d denote the effective dimension and λd+1/λd ≥ O(d2). Let α∗ satisfy (5). For any node i∣∣∣∣∣∣fα∗(i)− 1|Ni| ∑ j∈Ni fα∗(j)\n∣∣∣∣∣∣ ≤ c′d/λd+1 (8) for all Ni, and c′ = 56κ √ 2κc.\nThe full proof is given in the supplementary material. It is based on k-way expansion constant together with bounds on higher order Cheeger inequality (Gharan & Trevisan, 2014). Note that (8) holds for all i. However, we only need this to hold for the node with the optimal reward to establish regret performance our algorithm. We rewrite (8) for the optimal i∗ node using group actions as follows:\n|FG(s∗)− FG(sw∗ )| ≤ c′d/λd+1 for all w ≤ |Ni∗ |. (9)\nThough we give the proof of the above result under the technical assumption λd+1/λd ≥ O(d2), it holds in cases where eigenvalues grow fast. For example, for graphs with strong connectivity property this inequality is trivially satisfied. We can show that |FG(s∗)− FG(sw∗ )| ≤ c/ √ λ2 through a standard application of Cauchy-Schwartz inequality. For the model of Barabási-Albert we get λ2 = Ω(Nγ) with γ > 0 and for the cliques we get λ2 = N .\nGeneral graphs: When λd+1 is much larger than λd, the above proposition gives a tight relationship between the optimal reward and the average reward from its neighborhood. However, for general graphs this eigenvalue gap assumption is not valid. Motivated by (9), we assume that the smooth reward function satisfies the following weaker version for the general graphs. For all w ≤ |Ni∗ |\n|FG(s∗)− FG(sw∗ )| ≤ c′ √ Tw/λd+1. (10)\nThese inequalities get progressively weaker in T and w and can be interpreted as follows. For small values of T , we have few rounds for exploration and require stronger assumptions on smoothness. On the other hand, as T increases we have the opportunity to explore and consequently the inequalities are more relaxed. This relaxation of the inequality as a function of the width w characterizes the fact that close neighborhoods around the optimal node provide better information about the optimal reward than a wider neighborhood."
    }, {
      "heading" : "5.3. Algorithm: CheapUCB",
      "text" : "Below we present an algorithm similar to LinUCB (Li et al., 2010) and SpectralUCB (Valko et al., 2014) for regret minimization. The main difference between our algorithm and the SpectralUCB algorithm is the enlarged action space, which allows for selection of subsets of nodes and associated realization of average rewards. Note that when we probe a specific node instead of probing a subset of nodes, we get a more precise information (though noisy) about the node, but this results in higher cost.\nAs our goal is to minimize the cost while maintaining a low regret, we handle this requirement by moving sequentially from the least costly probes to expensive ones as we progress. In particular, we split the time horizon into J stages, and as we move from state j to j + 1 we use more expensive probes. That means, we use probes with smaller widths as we progress through the different stages of learning. The algorithm uses the probes of different widths in each stage as follows. Stage j = 1, . . . , J consists of time steps from 2j−1 to 2j − 1 and uses of probes of weight j only.\nAt each time step t = 1, 2, . . . , T, we estimate the value of α∗ by using l2-regularized least square as follows. Let {si := π(i), i = 1, 2, . . . , t} denote the probe selected till time t and {ri, i = 1, 2, . . . , t} denote the corresponding rewards. The estimate of α∗ denoted α̂t is computed as\nα̂t = arg min α ( t∑ i=1 [s′iQα− rt] 2 + ‖α‖2Λ ) .\nAlgorithm 1 CheapUCB 1: Input: 2: G: graph 3: T : number of steps 4: λ, δ: regularization and confidence parameters 5: R, c: upper bound on noise and norm of α 6: Initialization: 7: d← argmax{d : (d− 1)λd ≤ T/ log(1 + T/λ)} 8: β ← 2R √ d log(1 + T/λ) + 2 log(1/δ) + c\n9: V 0 ← ΛL + λI,S0 ← 0, r0 ← 0 10: for j = 1→ J do 11: for t = 2j−1 → min{2j − 1, T} do 12: St ← St−1 + rt−1s̃t−1 13: V t ← V t−1 + s̃t−1s̃′t−1 14: α̂t ← V −1t St 15: st ← argmaxs∈ SJ−j+1 ( s̃′α̂t + β‖s̃‖V −1t\n) 16: end for 17: end for\nTheorem 2 Set J = dlog T e in the algorithm. Let d be the effective dimension and λ be the smallest eigenvalue of Λ. Let s̃′tα\n∗ ∈ [−1, 1] for all s ∈ S, the cumulative regret of the algorithm is with probability at least 1− δ bounded as:\n(i) If (5) holds and λd+1/λd ≥ O(d2), then RT ≤ (8R √ d log(1 + T/λ) + 2 log(1/δ) + 4c)\n× √ dT log(1 + T/λ) + c′d2 log2(T/2) log(T/λ+ 1),\n(ii) If (5) and (10) hold, then RT ≤ (8R √ d log(1 + T/λ) + 2 log(1/δ) + 4c)\n× √ dT log(1 + T/λ) + c′d √ T/4 log2(T/2) log(T/λ+ 1),\nMoreover, the cumulative cost of CheapUCB is bounded as\nCT ≤ J−1∑ j=1 2j−1 J − j + 1 ≤ 3T 4 − 1 2\nRemark 1 Observe that when the eigenvalue gap is large, we get the regret to order d √ T within a constant factor satisfying the constraint (7). For the general case, compared to SpectralUCB, the regret bound of our algorithm increases by an amount of cd √ T/2 log2(T/2) log(T/λ+1),\nbut still it is of the order d √ T . However, the total cost in CheapUCB is smaller than in SpectralUCB by an amount of at least T/4 + 1/2, i.e., cost reduction of the order of T is achieved by our algorithm.\nCorollary 1 CheapUCB matches the regret performance of SpectralUCB and provides a cost gain of O(T )."
    }, {
      "heading" : "5.4. Computational complexity and scalability",
      "text" : "The computational and scalability issues of CheapUCB are essentially those associated with the SpectralUCB, i.e., obtaining eigenbasis of the graph Laplacian, matrix inversion\nand computation of the UCBs. Though CheapUCB uses larger sets of arms or probes at each step, it needs to compute onlyN UCBs as |Sw| = N for allw. The i-th probe in the set Sw can be computed by sorting the elements of the edge weights W (i, :) and assigning weight 1/w to the first w components can be done in orderN logN computations. As Valko et al. (2014), we speed up matrix inversion using iterative update (Zhang, 2005), and compute the eigenbasis of symmetric Laplacian matrix using fast symmetric diagonally dominant solvers as CMG (Koutis et al., 2011)."
    }, {
      "heading" : "6. Experiments",
      "text" : "We evaluate and compare our algorithm with SpectralUCB which is shown to outperform its competitor LinUCB for learning on graphs with large number of nodes. To demonstrate the potential of our algorithm in a more realistic scenario we also provide experiments on Forest Cover Type dataset. We set δ = 0.001, R = 0.01, and λ = 0.01."
    }, {
      "heading" : "6.1. Random graphs models",
      "text" : "We generated graphs from two graph models that are widely used to analyze connectivity in social networks. First, we generated a Erdős-Rényi (ER) graph with each edge sampled with probability 0.05 independent of others. Second, we generated a Barabási-Albert (BA) graph with degree parameter 3. The weights of the edges of these graphs we assigned uniformly at random.\nTo obtain a reward function f , we randomly generate a sparse vector α∗ with a small k N and use it to linearly combine the eigenvectors of the graph Laplacian as f = Qα∗, whereQ is the orthonormal matrix derived from the eigendecomposition of the graph Laplacian. We ran our algorithm on each graph in the regime T < N . In the plots displayed we used N = 250, T = 150 and k = 5. We averaged the experiments over 100 runs.\nFrom Figure 1, we see that the cumulative regret performance of CheapUCB is slightly worse than for SpectralUCB, but significantly better than for LinUCB. However, in terms of the cost CheapUCB provides a gain of at least 30 % as compared to both SpectralUCB and LinUCB."
    }, {
      "heading" : "6.2. Stochastic block models",
      "text" : "Community structure commonly arises in many networks. Many nodes can be naturally grouped together into a tightly knit collection of clusters with sparse connections among the different clusters. Graph representation of such networks often exhibit dense clusters with sparse connection between them. Stochastic block models are popular in modeling such community structure in many real-world networks (Girvan & Newman, 2002).\nThe adjacency matrix of SBMs exhibits a block triangular behavior. A generative model for SBM is based on connecting nodes within each block/cluster with high probability and nodes that are in two different blocks/clusters with low probability. For our simulations, we generated an SBM as follows. We grouped N = 250 nodes into 4 blocks of size 100, 60, 40 and 50, and connected nodes within each block with probability of 0.7. The nodes from the different blocks are connected with probability 0.02. We generated the reward function as in the previous subsection. The first 6 eigenvalues of the graph are 0, 3, 4, 5, 29, 29.6, . . ., i.e., there is a large gap between 4th and 5th eigenvalues, which confirms with our intuition that there should be 4 clusters (see Prop. 2). As seen from (a) and (b) in Figure 2, in this regime CheapUCB gives the same performance as SpectralUCB at a significantly lower cost, which confirms Theorem 2 (i) and Proposition 2."
    }, {
      "heading" : "6.3. Forest Cover Type data",
      "text" : "As our motivation for cheap bandits comes from the scenario involving sensing costs, we performed experiments on the Forest Cover Type data, a collection of 581021 labeled samples each providing observations on 30m× 30m region of a forest area. This dataset was chosen to match the radar motivation from the introduction, namely, we can view sensing the forest area from above, when vague sensing is cheap and specific sensing on low altitudes is costly. This dataset was already used to evaluate a bandit setting by Filippi et al. (2010).\nThe labels in Forest Cover Type data indicate the dominant species of trees (cover type) in a given region region.\nThe observations are 12 ‘cartographic’ measures of the regions and are used as independent variables to derive the cover types. Ten of the cartographic measures are quantitative and indicate the distance of the regions with respect to some reference points. The other two are qualitative binary variables indicating presence of certain characteristics.\nIn a forest area, the cover type of a region depends on the geographical conditions which mostly remain similar in the neighboring regions. Thus, the cover types change smoothly over the neighboring regions and likely to be concentrated in some parts of forest. Our goal is to find the region where a particular cover type has the highest concentrated. For example, such requirement arises in aerial reconnaissance, where an air borne vehicle (like UAV) collects ground information through a series of measurements to identify the regions of interests. In such applications, larger areas can be sensed at higher altitudes more quickly (lower cost) but this sensing suffers a lower resolution. On the other hand, smaller areas can be sensed at lower altitudes but at much higher costs.\nTo find the regions of high concentration of a given cover type, we first clustered the samples using only the quantitative attributes ignoring all the qualitative measurements as done in (Filippi et al., 2010). We generated 2000 clusters (after normalizing the data to lie in the intervals [0 1]) using k-means with Euclidean distance as a distance metric. For each cover type, we defined reward on clusters as the fraction of samples in the cluster that have the given cover type. We then generated graphs taking cluster centers as nodes and connected them with edge weight 1 that have similar rewards using 10 nearest-neighbors method. Note that neighboring clusters are geographically closer and will have similar cover types making their rewards similar.\nWe first considered the ‘Cottonwood/Willow’ cover type for which nodes’ rewards varies from 0 to 0.068. We plot the cumulative regret and cost in (c) and (d) in Figure 2 for T = 100. As we can see, the cumulative regret of the CheapUCB saturates faster than LinUCB and its performance is similar to that of SpectralUCB. And compared to both Lin-\nUCB and SpectralUCB total cost of CheapUCB is less by 35 %. We also considered reward functions for all the 7 cover types and the cumulative regret is shown in Figure 3. Again, the cumulative regret of CheapUCB is smaller than LinUCB and close to that of SpectralUCB with the cost gain same as in Figure 2(d) for all the cover types."
    }, {
      "heading" : "7. Conclusion",
      "text" : "We introduced cheap bandits, a new setting that aims to minimize sensing cost of the group actions while attaining the state-of-the-art regret guarantees in terms of effective dimension. The main advantage over typical bandit settings is that it models situations where getting the average reward from a set of neighboring actions is less costly than getting a reward from a single one. For the stochastic rewards, we proposed and evaluated CheapUCB, an algorithm that guarantees a cost gain linear in time. In future, we plan to extend this new sensing setting to other settings with limited feedback, such as contextual, combinatorial and non-stochastic bandits. As a by-product of our analysis, we establish a Ω( √ dT ) lower bound on the cumulative regret for a class of graphs with effective dimension d."
    }, {
      "heading" : "Acknowledgment",
      "text" : "This material is based upon work partially supported by NSF Grants CNS-1330008, CIF-1320566, CIF-1218992, and the U.S. Department of Homeland Security, Science and Technology Directorate, Office of University Programs, under Grant Award 2013-ST-061-ED0001. The views and conclusions contained in this document are those of the authors and should not be interpreted as necessarily representing the official policies, either expressed or implied, of the U.S. Department of Homeland Security or the National Science Foundation."
    }, {
      "heading" : "8. Proof of Proposition 1",
      "text" : "For a given policy π,α∗, T , and a graph G define expected cumulative reward as\nRegret(T, π,α∗, G) = E [ T∑ t=1 s̃∗α∗ − s̃tα∗|α∗ ]\nwhere s̃t = π′(t)Q, andQ is the orthonormal basis matrix corresponding to Laplacian of G. Let Gd denote the family of graphs with effective dimension d. Define T - period risk of the policy π\nRisk(T, π) = max G∈Gd max α∗∈RN ‖α∗‖Λ<c\n[Regret(T, π,α∗, G)]\nWe first establish that their exists a graph with effective dimension d, and a class of smooth reward functions defined over it with parameters α∗’s in a d-dimensional vector space.\nLemma 2 Given T , there exists a graph Ĝ ∈ Gd such that\nmax α∗∈Rd ‖α∗‖Λ<c\n[ Regret(T, π,α∗, Ĝ) ] ≤ Risk(T, π)\nProof: We prove the lemma by explicit construction of a graph. Consider a graph G consisting of d disjoint connected subgraphs denoted as Gj : j = 1, 2 . . . , d. Let the nodes in each subgraph have the same reward. The set of eigenvalues of the graph are {0, λ̂1, · · · , λ̂N−d}, where eigenvalue 0 is repeated d times. Note that the set of eigenvalues of the graph is the union of the set of eigenvalues of the individual subgraphs. Without loss of generality, assume that λ̂1 > T/d log(T/λ+ 1) (this is always possible, for example if subgraphs are cliques). Then, the effective dimension of the graph G is d. Since the graph separates into d disjoint subgraphs, we can split the reward function fα = Qα into d parts, one corresponding to each subgraph. We write f j = Qjαj for j = 1, 2, . . . , d, where f i is the reward function associated with Gj , Qj is the orthonormal matrix corresponding to Laplacian of Gj , and αi is a sub-vector of α corresponding to Gj .\nWrite αj = Q′jf j . Since f j is a constant vector and, except for one , all the columns in Qj are orthogonal to f j , it is clear that αj has only one non-zero component. We conclude that for the reward functions that is constant on each subgraphs α has only d non-zero components and lies d-dimensional space. The proof is complete by setting Ĝ = G\nNote that a graph with effective dimension d cannot have more than d disjoint connected subgraphs. Next, we restrict our attention to graph Ĝ and rewards that are piecewise constant on each clique. That means that the nodes\nin each clique have the same reward. Recall that action set SD consists of actions that can probe a node or a group of neighboring nodes. Therefore, any group action will only allow us to observe average reward from a group of nodes within a clique but not across the cliques. Then, all node and group actions used to observe reward from within a clique are indistinguishable. Hence, the SD collapses to set of d distinct actions one associated with each clique, and the problem reduces to that of selecting a clique with the highest reward. We henceforth treat each clique as an arm where all the nodes within the same clique share the same reward value.\nWe now provide a lower bound on the expected regret defined as follows\nR̃isk(T, π, Ĝ) = E [ Regret ( T, π,α∗, Ĝ )] , (11)\nwhere expectation is over the reward function on the arms.\nTo lower bound the regret we follow the argument of Auer et al. (2002) and their Theorem 5.1, where an adversarial setting is considered and the expectation in (11) is over the reward functions generated randomly according to Bernoulli distributions. We generalize this construction to our case with Gaussian noise. The reward generation process is as follows:\nWithout loss of generality choose cluster 1 to be the good cluster. At each time step t, sample reward of cluster 1 from the Gaussian distribution with mean 12 + ξ and unit variance. For all other clusters, sample reward from the Gaussian distribution with mean 12 and unit variance.\nThe rest of the proof of the arguments follows exactly as in the proof of Theorem 5.1(Auer et al., 2002) except at their Equation 29. To obtain an equivalent version for Gaussian rewards, we use the relationship between the L1 distance of Gaussian distributions and their KL divergence. We then apply the formula for the KL divergence between the Gaussian random variables to obtain equivalent version of their Equation 30. Now note that, log(1 − ξ2) ∼ −ξ2 (within a constant). Then the proof follows silmilarly by setting ξ = √ d/T and noting that the L2 norm of the mean rewards is bounded by c for an appropriate choice of λ."
    }, {
      "heading" : "9. Proof of Proposition 2",
      "text" : "In the following, we first we give some definitions and related results.\nDefinition 2 (k-way expansion constant (Lee et al., 2012)) Consider a graph G and X ⊂ V let\nφG(X ) := φ(X ) = |∂X| V (X ) ,\nwhere V (X ) denote the sum of the degree of nodes in X\nand |∂X| denote the number of edges between the nodes in X and V\\X . For all k > 0, k−way expansion constant is defined as\nρG(k) = min { maxφ(Vi) : ∩ki=1Vi = ∅, |Vi| 6= 0 } .\nLet µ1 ≤ µ2, . . . ,≤ µN denote the eigenvalues of the normalized Laplacian of G.\nTheorem 3 ((Gharan & Trevisan, 2014)) Let ε > 0 and ρ(k + 1) > (1 + ε)ρ(k) holds for some k > 0. Then the following holds:\nµk/2 ≤ ρ(k) ≤ O(k2) √ µk (12)\nThere exits a k partitions {Vi : i = 1, 2, · · · , k} of V such that forall i = 1, 2, · · · k\nφ(Vi) ≤ kρ(k) and (13) φ(G[Vi]) ≥ ερ(k + 1)/14k (14)\nwhere φ(G[X ]) denotes the Cheeger’s constant (conduntance) of the subgraph induced by X .\nDefinition 3 (Isoperimetric number)\nθ(G) = { min\n∂X |X |\n: |X | ≤ X/2 } .\nLet λ1 ≤ λ2, . . . ,≤ λN denotes the eigenvalues of the unnormalized Lapalcian of G.The following is a standard result.\nλ2/2 ≤ θ(G) ≤ √ 2κλ2. (15)\nProof: The relation λk+1/λk ≥ O(k2) implies that µk+1/µk ≥ O(k2). Using the upper and lower bounds on the eigenvalues in (12), the relation ρk+1 ≥ (1 + ε)ρk holds for some ε > 1/2. Then, applying Theorem 3 we get k-partitions satisfying (13)-(14). Let Li denote the Laplacian induced by the subgraph G[Vj ] = (Vj , Ej) for j = 1, 2, · · · k. By the quadratic property of the graph Laplacian we have\nf ′Lf = ∑\n(u,v)∈E\n(fu − fv)2 (16)\n= k∑ j=1 ∑ (u,v)∈Ej (fu − fv)2 (17)\n= k∑ j=1 f ′jLjfj (18)\nwhere f j denote the reward vector on the induced subgraph Gj := G[Vj ] In the following we just focus on the optimal node. The same arguments holds for any other node. Without loss of generality assume that the node with optimal reward lies in subgraph Gl for some 1 ≤ l ≤ d. From the last relation we have f ′lllf l ≤ c. The reward functions on the subgraphGl can be represented as f l = Qlαl for some αl, where Ql satisfies Li = Q ′ lΛLQl and Λl denotes the diagonal matrix with eigenvalues of Λl. We have\n|FG(s∗)− FG((sw∗ )| = |FGl(s∗)− FGl(sw∗ )| ≤ ‖s∗ − sw∗ ‖‖Qlαl‖\n≤ (\n1− 1 w\n) ‖QlΛ −1/2 l ‖‖Λ 1/2 l αl‖\n≤ c√ λ2(Gl) From Chauchy-Schwarz ≤ √ 2κc\nθ(Gl) From (15)\n≤ √ 2κc\nφ(Gl) Using θ(Gl) ≥ φ(Gl)\n≤ 14k √ 2κc\nερ(k + 1) From Th.1, Eq. (14)\n≤ 56k √ 2κc\nµk+1 From Th.1, Eq. (12)\n≤ 56kκ √ 2κc\nλk+1 Using µk+1 ≥ λk+1/κ.\nThis completes the proof."
    }, {
      "heading" : "10. Analysis of CheapUCB",
      "text" : "For a given confidence parameter δ define\nβ = 2R √ d log ( 1 + T\nλ\n) + 2 log 1\nδ + c,\nand consider the ellipsoid around the estimate α̂t\nCt = {α : ‖α̂t −α‖Vt ≤ β}.\nWe first state the following results from (Abbasi-Yadkori et al., 2011), (Dani et al., 2008), and (Valko et al., 2014)\nLemma 3 (Self-Normalized Bound) Let ξt = ∑t i=1 s̃iεi and λ > 0. Then, for any δ > 0, with probability at least 1− δ and for all t > 0,\n‖ξt‖V −1t ≤ β.\nLemma 4 Let V0 = λI . We have:\nlog det(Vt)\ndet(λI) ≤ t∑ i=1 ‖s̃i‖V −1 i−1 ‖s̃i‖V −1i−1 ≤ 2 log det(Vt+1)\ndet(λI) .\nLemma 5 Let ‖α∗‖2 ≤ c. Then, with probability at least 1− δ, for all t ≥ 0 and for any x ∈ Rn we have α∗ ∈ Ct and\n|x · (α̂t −α∗)| ≤ ‖x‖V−1t β.\nLemma 6 Let d be the effective dimension and T be the time horizon of the algorithm. Then,\nlog det(VT+1)\ndet(Λ) ≤ 2d log\n( 1 + T\nλ\n) ."
    }, {
      "heading" : "10.1. Proof of Theorem 2",
      "text" : "We first prove the case where degree of each node is at least log T .\nConsider step t ∈ [2j−1, 2j−1] in stage j = 1, 2, · · · J−1. Recall that in this step a probe of width J−j+1 is selected. Write wj := J − j + 1, and denote the probe of width J − j + 1 associated with the optimal probe s∗ as simply s wj ∗ and the corresponding GFT as s̃ wj ∗ . The probe selected at time t is denoted as st. Note that both st and s wj ∗ lie in the set SJ−j+1. For notational convenience let us denote\nh(j) :=\n{ c′ √ T (J − j + 1)/λd+1 when (10) holds\nc′d/λd+1 when (9) holds.\nThe instantaneous regret in step t is\nrt = s̃∗ ·α∗ − s̃t ·α∗\n≤ s̃wj∗ ·α∗ + h(j)− s̃t ·α∗ = s̃ wj ∗ · (α∗ − α̂t) + s̃j∗ · α̂t + β‖s̃\nwj ∗ ‖V−1t\n−β‖s̃wj∗ ‖V−1t − s̃t ·α ∗ + h(j)\n≤ s̃wj∗ · (α∗ − α̂t) + s̃t · α̂t + β‖s̃t‖V−1t −β‖s̃wj∗ ‖V−1t − s̃t ·α ∗ + h(j) = s̃ wj ∗ · (α∗ − α̂t) + s̃t · (α̂t −α∗) + β‖s̃t‖V −1t −β‖s̃wj∗ ‖V−1t + h(j) ≤ β‖s̃wj∗ ‖V−1t + β‖s̃t‖V−1t + β‖s̃t‖V−1t −β‖s̃wj∗ ‖V−1t + h(j) = 2β‖s̃t‖V−1t + h(j).\nWe used (9)/(10) in the first inequality. The second inequality follows from the algorithm design and the third inequality follows from Lemma 5. Now, the cumulative regret of\nthe algorithm is given by\nRT\n≤ J∑ j=1 2j−1∑ t=2j−1 min{2, 2β‖s̃t‖V −1t + h(j)} ≤ J∑ j=1 2j−1∑ t=2j−1 min{2, 2βt‖s̃t‖V −1t }+ J−1∑ j=1 2j−1∑ t=2j−1 h(j)\n≤ T∑ t=1 min{2, 2βt‖s̃t‖V −1t }+ J−1∑ j=1 h(j)2j−1.\nNote that the summation in the second term includes only the first J − 1 stages. In the last stage J , we use probes of width 1 and hence we do not need to use (9)/(10) in bounding the instantaneous regret. Next, we bound each term in the regret separately.\nTo bound the first term we use the same steps as in the proof of Theorem 1 (Valko et al., 2014). We repeat the steps below.\nT∑ t=1 min{2, 2β‖s̃t‖V −1t }\n≤ (2 + 2β) T∑ t=1 min{1, ‖s̃t‖V−1t }\n≤ (2 + 2β) √√√√T T∑ t=1 min{1, βt‖s̃t‖V−1t } 2\n≤ 2(1 + β) √\n2T log(|V T+1|/|Λ|) (19) ≤ 4(1 + β) √ Td log(1 + T/λ) (20)\n≤ ( 8R √ 2 log 1\nδ + d log\n( 1 + T\nλ\n) + 4c+ 4 )\n× √ Td log ( 1 + T\nλ\n) .\nWe used Lemma 4 and 6 in inequalities (19) and (20) respectively. The final bound follows from plugging the value of β."
    }, {
      "heading" : "10.2. For the case when (10) holds:",
      "text" : "For this case we use h(j) = c′ √ T (J − j + 1)/λd+1. First observe that 2j−1h(j) is increasing in 1 ≤ j ≤ J − 1. We\nhave J−1∑ j=1 2j−1c′ √ T (J − j + 1) λd+1 ≤ (J − 1)2 J−1 √ Tc′ λd+1\n≤ (J − 1)2 log2 T−1c′\n√ T\nλd+1 ≤ (J − 1) c\n′ √ T (T/2)\n(T/d log(T/λ+ 1)) ≤ dc′ √ T/4 log2(T/2) log(T/λ+ 1).\nIn the second line we applied the definition of effective dimension.\n10.3. For the case when λd+1/λd ≥ O(d2)\nFor the case λd+1/λd ≥ O(d2) we use h(j) = c′d/λd+1.\nJ−1∑ j=1 2j−1c′d λd+1 ≤ 2 J−1c′d λd+1\n≤ c′d2 log2(T/2) log(T/λ+ 1).\nNow consider the case where minimum degree of the nodes is 1 < a ≤ log T . In this case we modify the algorithm to use only signals of width a in the first log T − a + 1 stages and subsequently the signal width is reduced by one in each of the following stages. The previous analysis holds for this case and we get the same bounds on the cumulative regret and cost. When a = 1, CheapUCB is same as the SpectralUCB, hence total cost and regret is same as that of SpectralUCB.\nTo bound the total cost, note that in stage j we use signals of width J − j + 1. Also, the cost of a signal given in (2) can be upper bounded as C(swi ) ≤ 1w . Then, we can upper bound total cost of signals used till step T as\nJ∑ j=1 2j−1 J − j + 1\n≤ 1 2 J−1∑ j=1 2j−1 + T 2\n≤ 1 2\n( T 2 − 1 ) + T 2 = 3T 4 − 1 2 ."
    } ],
    "references" : [ {
      "title" : "Improved algorithms for linear stochastic bandits",
      "author" : [ "Y. Abbasi-Yadkori", "D. Pal", "C. Szepesvari" ],
      "venue" : "In Proceeding of NIPS,",
      "citeRegEx" : "Abbasi.Yadkori et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Abbasi.Yadkori et al\\.",
      "year" : 2011
    }, {
      "title" : "Efficient sensor management policies for distributed target tracking in multihop sensor networks",
      "author" : [ "Aeron", "Shuchin", "Saligrama", "Venkatesh", "Castanon", "David A" ],
      "venue" : "IEEE Transactions on Signal Processing (TSP),",
      "citeRegEx" : "Aeron et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Aeron et al\\.",
      "year" : 2008
    }, {
      "title" : "From Bandits to Experts: A Tale of Domination and Independence",
      "author" : [ "Alon", "Noga", "Cesa-Bianchi", "Nicolò", "Gentile", "Claudio", "Mansour", "Yishay" ],
      "venue" : "In Neural Information Processing Systems,",
      "citeRegEx" : "Alon et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Alon et al\\.",
      "year" : 2013
    }, {
      "title" : "The non-stochastic multi-armed bandit problem",
      "author" : [ "P. Auer", "N. Cesa-Bianchi", "Robert", "Y. Freund", "E. Schapire" ],
      "venue" : "SIAM Journal on Computing,",
      "citeRegEx" : "Auer et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Auer et al\\.",
      "year" : 2003
    }, {
      "title" : "Using confidence bounds for exploitationexploration trade-offs",
      "author" : [ "Auer", "Peter" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Auer and Peter.,? \\Q2002\\E",
      "shortCiteRegEx" : "Auer and Peter.",
      "year" : 2002
    }, {
      "title" : "The Nonstochastic Multiarmed Bandit Problem",
      "author" : [ "Auer", "Peter", "Cesa-Bianchi", "Nicolò", "Freund", "Yoav", "Schapire", "Robert E" ],
      "venue" : "SIAM Journal on Computing,",
      "citeRegEx" : "Auer et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Auer et al\\.",
      "year" : 2002
    }, {
      "title" : "Resourceful contextual bandits",
      "author" : [ "A. Badanidiyuru", "J. Langford", "A. Slivkins" ],
      "venue" : "In Proceeding of Conference on Learning Theory,",
      "citeRegEx" : "Badanidiyuru et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Badanidiyuru et al\\.",
      "year" : 2014
    }, {
      "title" : "Bandits with knapsacks",
      "author" : [ "Badanidiyuru", "Ashwinkumar", "Kleinberg", "Robert", "Slivkins", "Aleksandrs" ],
      "venue" : "In Proceedings - Annual IEEE Symposium on Foundations of Computer Science,",
      "citeRegEx" : "Badanidiyuru et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Badanidiyuru et al\\.",
      "year" : 2013
    }, {
      "title" : "Manifold regularization: A geometric framework for learning from labeled and unlabeled examples",
      "author" : [ "M. Belkin", "P. Niyogi", "V. Sindhwani" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Belkin et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Belkin et al\\.",
      "year" : 2008
    }, {
      "title" : "Leveraging Side Observations in Stochastic Bandits",
      "author" : [ "Caron", "Stéphane", "Kveton", "Branislav", "Lelarge", "Marc", "Bhagat", "Smriti" ],
      "venue" : "In Uncertainty in Artificial Intelligence,",
      "citeRegEx" : "Caron et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Caron et al\\.",
      "year" : 2012
    }, {
      "title" : "Online Learning with Switching Costs and Other Adaptive Adversaries",
      "author" : [ "Cesa-Bianchi", "Nicolò", "Dekel", "Ofer", "Shamir", "Ohad" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Cesa.Bianchi et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Cesa.Bianchi et al\\.",
      "year" : 2013
    }, {
      "title" : "A Gang of Bandits",
      "author" : [ "Cesa-Bianchi", "Nicolò", "Gentile", "Claudio", "Zappella", "Giovanni" ],
      "venue" : "In Neural Information Processing Systems,",
      "citeRegEx" : "Cesa.Bianchi et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Cesa.Bianchi et al\\.",
      "year" : 2013
    }, {
      "title" : "Stochastic linear optimization under bandit feedback",
      "author" : [ "V. Dani", "T.P. Hayes", "S.M. Kakade" ],
      "venue" : "In Proceeding of Conference on Learning Theory, COLT, Helsinki,",
      "citeRegEx" : "Dani et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Dani et al\\.",
      "year" : 2008
    }, {
      "title" : "Multi-Armed Bandit with Budget Constraint and Variable Costs",
      "author" : [ "Ding", "Wenkui", "Qin", "Tao", "Zhang", "Xu-dong", "Liu", "Tieyan" ],
      "venue" : "In Proceedings of the Twenty-Seventh AAAI Conference on Artificial Intelligence,",
      "citeRegEx" : "Ding et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Ding et al\\.",
      "year" : 2013
    }, {
      "title" : "Adaptive statistical sampling methods for decentralized estimation and detection of localized phenomena",
      "author" : [ "Ermis", "Erhan Baki", "Saligrama", "Venkatesh" ],
      "venue" : "Proceedings of Information Processing in Sensor Networks (IPSN),",
      "citeRegEx" : "Ermis et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Ermis et al\\.",
      "year" : 2005
    }, {
      "title" : "Distributed detection in sensor networks with limited range multimodal sensors",
      "author" : [ "Ermis", "Erhan Baki", "Saligrama", "Venkatesh" ],
      "venue" : "IEEE Transactions on Signal Processing,",
      "citeRegEx" : "Ermis et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Ermis et al\\.",
      "year" : 2010
    }, {
      "title" : "Parametric bandits: The generalized linear case",
      "author" : [ "L. Filippi", "O. Cappe", "A. Garivier", "C. Szepesvari" ],
      "venue" : "In Proceeding of NIPS,",
      "citeRegEx" : "Filippi et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Filippi et al\\.",
      "year" : 2010
    }, {
      "title" : "Smart sleeping policies for energy efficient tracking in sensor networks",
      "author" : [ "Fuemmeler", "Jason A", "Veeravalli", "Venugopal V" ],
      "venue" : "IEEE Transactions on Signal Processing,",
      "citeRegEx" : "Fuemmeler et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Fuemmeler et al\\.",
      "year" : 2008
    }, {
      "title" : "Online Clustering of Bandits",
      "author" : [ "Gentile", "Claudio", "Li", "Shuai", "Zappella", "Giovanni" ],
      "venue" : "In International Conference on Machine Learning,",
      "citeRegEx" : "Gentile et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Gentile et al\\.",
      "year" : 2014
    }, {
      "title" : "Partitioning into expanders",
      "author" : [ "S.O. Gharan", "L. Trevisan" ],
      "venue" : "In Proceeding of Symposium of Discrete Algorithms,",
      "citeRegEx" : "Gharan and Trevisan,? \\Q2014\\E",
      "shortCiteRegEx" : "Gharan and Trevisan",
      "year" : 2014
    }, {
      "title" : "Community structure in social and biological networks",
      "author" : [ "M. Girvan", "M.E. Newman" ],
      "venue" : "In Proceedings of Natl Acad Sci USA,",
      "citeRegEx" : "Girvan and Newman,? \\Q2002\\E",
      "shortCiteRegEx" : "Girvan and Newman",
      "year" : 2002
    }, {
      "title" : "Efficient learning by implicit exploration in bandit problems with side observations",
      "author" : [ "Kocák", "Tomáš", "Neu", "Gergely", "Valko", "Michal", "Munos", "Rémi" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Kocák et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Kocák et al\\.",
      "year" : 2014
    }, {
      "title" : "Combinatorial preconditioners and multilevel solvers for problems in computer vision and image processing",
      "author" : [ "Koutis", "Ioannis", "Miller", "Gary L", "Tolliver", "David" ],
      "venue" : "Computer Vision and Image Understanding,",
      "citeRegEx" : "Koutis et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Koutis et al\\.",
      "year" : 2011
    }, {
      "title" : "Multi-way spectral partitioning and higher-order cheeger inequalities",
      "author" : [ "Lee", "James R", "Gharan", "Shayan Oveis", "Trevisan", "Luca" ],
      "venue" : "In Proceeding of STOC,",
      "citeRegEx" : "Lee et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Lee et al\\.",
      "year" : 2012
    }, {
      "title" : "A contextual-bandit approach to personalized news article recommendation",
      "author" : [ "L. Li", "C. Wei", "J. Langford", "R.E. Schapire" ],
      "venue" : "In Proceeding of International Word Wide Web conference,",
      "citeRegEx" : "Li et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2010
    }, {
      "title" : "From Bandits to Experts: On the Value of Side-Observations",
      "author" : [ "Mannor", "Shie", "Shamir", "Ohad" ],
      "venue" : "In Neural Information Processing Systems,",
      "citeRegEx" : "Mannor et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Mannor et al\\.",
      "year" : 2011
    }, {
      "title" : "Signal processing techniques for interpolation in graph structured data",
      "author" : [ "S.K. Narang", "A. Gadde", "A. Ortega" ],
      "venue" : "In Proceedings of International Conference of Acoustics, Speech and Signal Processing,",
      "citeRegEx" : "Narang et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Narang et al\\.",
      "year" : 2013
    }, {
      "title" : "The emerging filed of signal processing on graphs",
      "author" : [ "D.I. Shuman", "S.K. Narang", "P. Frossard", "A. Ortega", "P. Vanderghenyst" ],
      "venue" : "In IEEE Signal Processing Magazine,",
      "citeRegEx" : "Shuman et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Shuman et al\\.",
      "year" : 2013
    }, {
      "title" : "Knapsack Based Optimal Policies for Budget-Limited Multi-Armed Bandits",
      "author" : [ "Tran-Thanh", "Long", "Chapman", "Archie C", "Rogers", "Alex", "Jennings", "Nicholas R" ],
      "venue" : null,
      "citeRegEx" : "Tran.Thanh et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Tran.Thanh et al\\.",
      "year" : 2012
    }, {
      "title" : "Spectral Bandits for Smooth Graph Functions",
      "author" : [ "Valko", "Michal", "Munos", "Rémi", "Kveton", "Branislav", "Kocák", "Tomáš" ],
      "venue" : "In 31th International Conference on Machine Learning,",
      "citeRegEx" : "Valko et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Valko et al\\.",
      "year" : 2014
    }, {
      "title" : "The schur complement and its",
      "author" : [ "F. Zhang" ],
      "venue" : "application. Springer,",
      "citeRegEx" : "Zhang,? \\Q2005\\E",
      "shortCiteRegEx" : "Zhang",
      "year" : 2005
    }, {
      "title" : "Graph spectral compressed sensing for sensor networks",
      "author" : [ "X. Zhu", "M. Rabbat" ],
      "venue" : "In Proceedings of International Conference of Acoustics, Speech and Signal Processing,",
      "citeRegEx" : "Zhu and Rabbat,? \\Q2012\\E",
      "shortCiteRegEx" : "Zhu and Rabbat",
      "year" : 2012
    }, {
      "title" : "Online Learning with Costly Features and Labels",
      "author" : [ "Zolghadr", "Navid", "Bartok", "Gabor", "Greiner", "Russell", "György", "András", "Szepesvari", "Csaba" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Zolghadr et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Zolghadr et al\\.",
      "year" : 2013
    } ],
    "referenceMentions" : [ {
      "referenceID" : 1,
      "context" : "To conserve battery power, sleep/awake scheduling is used (Fuemmeler & Veeravalli, 2008; Aeron et al., 2008), wherein a group of sensors is woken up sequentially based on probable locations of target.",
      "startOffset" : 58,
      "endOffset" : 108
    }, {
      "referenceID" : 26,
      "context" : "Rewards in many applications are typically smooth band-limited graph signals (Narang et al., 2013) with the sensing field decaying smoothly with distance from the target.",
      "startOffset" : 77,
      "endOffset" : 98
    }, {
      "referenceID" : 27,
      "context" : "small subset of nodes (Shuman et al., 2013).",
      "startOffset" : 22,
      "endOffset" : 43
    }, {
      "referenceID" : 12,
      "context" : "We model this problem as an instance of linear bandits (Auer, 2002; Dani et al., 2008; Li et al., 2010) that links the reward of nodes through an unknown parameter.",
      "startOffset" : 55,
      "endOffset" : 103
    }, {
      "referenceID" : 24,
      "context" : "We model this problem as an instance of linear bandits (Auer, 2002; Dani et al., 2008; Li et al., 2010) that links the reward of nodes through an unknown parameter.",
      "startOffset" : 55,
      "endOffset" : 103
    }, {
      "referenceID" : 12,
      "context" : "We model this problem as an instance of linear bandits (Auer, 2002; Dani et al., 2008; Li et al., 2010) that links the reward of nodes through an unknown parameter. A bandit setting for smooth signals was recently studied by Valko et al. (2014), however neglecting the signal cost.",
      "startOffset" : 68,
      "endOffset" : 245
    }, {
      "referenceID" : 28,
      "context" : "There are several other bandit and online learning settings that consider costs (Tran-Thanh et al., 2012; Badanidiyuru et al., 2013; Ding et al., 2013; Badanidiyuru et al., 2014; Zolghadr et al., 2013; Cesa-Bianchi et al., 2013a).",
      "startOffset" : 80,
      "endOffset" : 229
    }, {
      "referenceID" : 7,
      "context" : "There are several other bandit and online learning settings that consider costs (Tran-Thanh et al., 2012; Badanidiyuru et al., 2013; Ding et al., 2013; Badanidiyuru et al., 2014; Zolghadr et al., 2013; Cesa-Bianchi et al., 2013a).",
      "startOffset" : 80,
      "endOffset" : 229
    }, {
      "referenceID" : 13,
      "context" : "There are several other bandit and online learning settings that consider costs (Tran-Thanh et al., 2012; Badanidiyuru et al., 2013; Ding et al., 2013; Badanidiyuru et al., 2014; Zolghadr et al., 2013; Cesa-Bianchi et al., 2013a).",
      "startOffset" : 80,
      "endOffset" : 229
    }, {
      "referenceID" : 6,
      "context" : "There are several other bandit and online learning settings that consider costs (Tran-Thanh et al., 2012; Badanidiyuru et al., 2013; Ding et al., 2013; Badanidiyuru et al., 2014; Zolghadr et al., 2013; Cesa-Bianchi et al., 2013a).",
      "startOffset" : 80,
      "endOffset" : 229
    }, {
      "referenceID" : 32,
      "context" : "There are several other bandit and online learning settings that consider costs (Tran-Thanh et al., 2012; Badanidiyuru et al., 2013; Ding et al., 2013; Badanidiyuru et al., 2014; Zolghadr et al., 2013; Cesa-Bianchi et al., 2013a).",
      "startOffset" : 80,
      "endOffset" : 229
    }, {
      "referenceID" : 28,
      "context" : "The first set is referred to as budgeted bandits (Tran-Thanh et al., 2012) or bandits with knapsacks (Badanidiyuru et al.",
      "startOffset" : 49,
      "endOffset" : 74
    }, {
      "referenceID" : 7,
      "context" : ", 2012) or bandits with knapsacks (Badanidiyuru et al., 2013), where each single arm is associated with a cost.",
      "startOffset" : 34,
      "endOffset" : 61
    }, {
      "referenceID" : 13,
      "context" : "This cost can be known or unknown (Ding et al., 2013) and can depend on a given context (Badanidiyuru et al.",
      "startOffset" : 34,
      "endOffset" : 53
    }, {
      "referenceID" : 6,
      "context" : ", 2013) and can depend on a given context (Badanidiyuru et al., 2014).",
      "startOffset" : 42,
      "endOffset" : 69
    }, {
      "referenceID" : 32,
      "context" : "Another cost setting considers cost for observing features from which the learner can build its prediction (Zolghadr et al., 2013).",
      "startOffset" : 107,
      "endOffset" : 130
    }, {
      "referenceID" : 2,
      "context" : "Another graph bandit setting considers side information, when the learner obtains besides the reward of the node it chooses, also the rewards of the neighbors (Mannor & Shamir, 2011; Alon et al., 2013; Caron et al., 2012; Kocák et al., 2014).",
      "startOffset" : 159,
      "endOffset" : 241
    }, {
      "referenceID" : 9,
      "context" : "Another graph bandit setting considers side information, when the learner obtains besides the reward of the node it chooses, also the rewards of the neighbors (Mannor & Shamir, 2011; Alon et al., 2013; Caron et al., 2012; Kocák et al., 2014).",
      "startOffset" : 159,
      "endOffset" : 241
    }, {
      "referenceID" : 21,
      "context" : "Another graph bandit setting considers side information, when the learner obtains besides the reward of the node it chooses, also the rewards of the neighbors (Mannor & Shamir, 2011; Alon et al., 2013; Caron et al., 2012; Kocák et al., 2014).",
      "startOffset" : 159,
      "endOffset" : 241
    }, {
      "referenceID" : 18,
      "context" : ", 2013b) and online clustering of bandits in (Gentile et al., 2014).",
      "startOffset" : 45,
      "endOffset" : 67
    }, {
      "referenceID" : 23,
      "context" : "The most related graph bandits setting to ours is by Valko et al. (2014) on which we build this paper.",
      "startOffset" : 53,
      "endOffset" : 73
    }, {
      "referenceID" : 29,
      "context" : "This regret bound is of the same order as SpectralUCB (Valko et al., 2014) that does not take cost into consideration.",
      "startOffset" : 54,
      "endOffset" : 74
    }, {
      "referenceID" : 8,
      "context" : "For instance, if we choose α such that large coefficients correspond to the eigenvectors associated with small eigenvalues then fα is a smooth function of G (Belkin et al., 2008).",
      "startOffset" : 157,
      "endOffset" : 178
    }, {
      "referenceID" : 29,
      "context" : "In the following, we first state the regret performance of the SpectralUCB algorithm (Valko et al., 2014) that uses only node actions.",
      "startOffset" : 85,
      "endOffset" : 105
    }, {
      "referenceID" : 29,
      "context" : "Using these node actions, Valko et al. (2014) developed SpectralUCB that aims to minimize the regret under the assumption that the reward function is smooth.",
      "startOffset" : 26,
      "endOffset" : 46
    }, {
      "referenceID" : 29,
      "context" : "To characterize the regret performance of SpectralUCB, Valko et al. (2014) introduced the notion of effective dimension defined as follows:",
      "startOffset" : 55,
      "endOffset" : 75
    }, {
      "referenceID" : 29,
      "context" : "Theorem 1 (Valko et al., 2014) The cumulative regret of SpectralUCB is bounded with probability at least 1− δ as:",
      "startOffset" : 10,
      "endOffset" : 30
    }, {
      "referenceID" : 29,
      "context" : "While this implies that our target in (7) should be √ dT , we follow Valko et al. (2014) and develop a variation of SpectralUCB that obtains the target regret of d √ T .",
      "startOffset" : 69,
      "endOffset" : 89
    }, {
      "referenceID" : 3,
      "context" : "1 of Auer et al. (2003) and lower bound the minimax risk.",
      "startOffset" : 5,
      "endOffset" : 24
    }, {
      "referenceID" : 24,
      "context" : "Below we present an algorithm similar to LinUCB (Li et al., 2010) and SpectralUCB (Valko et al.",
      "startOffset" : 48,
      "endOffset" : 65
    }, {
      "referenceID" : 29,
      "context" : ", 2010) and SpectralUCB (Valko et al., 2014) for regret minimization.",
      "startOffset" : 24,
      "endOffset" : 44
    }, {
      "referenceID" : 30,
      "context" : "(2014), we speed up matrix inversion using iterative update (Zhang, 2005), and compute the eigenbasis of symmetric Laplacian matrix using fast symmetric diagonally dominant solvers as CMG (Koutis et al.",
      "startOffset" : 60,
      "endOffset" : 73
    }, {
      "referenceID" : 22,
      "context" : "(2014), we speed up matrix inversion using iterative update (Zhang, 2005), and compute the eigenbasis of symmetric Laplacian matrix using fast symmetric diagonally dominant solvers as CMG (Koutis et al., 2011).",
      "startOffset" : 188,
      "endOffset" : 209
    }, {
      "referenceID" : 28,
      "context" : "As Valko et al. (2014), we speed up matrix inversion using iterative update (Zhang, 2005), and compute the eigenbasis of symmetric Laplacian matrix using fast symmetric diagonally dominant solvers as CMG (Koutis et al.",
      "startOffset" : 3,
      "endOffset" : 23
    }, {
      "referenceID" : 16,
      "context" : "This dataset was already used to evaluate a bandit setting by Filippi et al. (2010).",
      "startOffset" : 62,
      "endOffset" : 84
    }, {
      "referenceID" : 16,
      "context" : "To find the regions of high concentration of a given cover type, we first clustered the samples using only the quantitative attributes ignoring all the qualitative measurements as done in (Filippi et al., 2010).",
      "startOffset" : 188,
      "endOffset" : 210
    } ],
    "year" : 2015,
    "abstractText" : "We consider stochastic sequential learning problems where the learner can observe the average reward of several actions. Such a setting is interesting in many applications involving monitoring and surveillance, where the set of the actions to observe represent some (geographical) area. The importance of this setting is that in these applications, it is actually cheaper to observe average reward of a group of actions rather than the reward of a single action. We show that when the reward is smooth over a given graph representing the neighboring actions, we can maximize the cumulative reward of learning while minimizing the sensing cost. In this paper we propose CheapUCB, an algorithm that matches the regret guarantees of the known algorithms for this setting and at the same time guarantees a linear cost again over them. As a by-product of our analysis, we establish a Ω( √ dT ) lower bound on the cumulative regret of spectral bandits for a class of graphs with effective dimension d.",
    "creator" : "LaTeX with hyperref package"
  }
}