{
  "name" : "1304.1574.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Generalization Bounds for Domain Adaptation",
    "authors" : [ "Chao Zhang", "Lei Zhang", "Jieping Ye" ],
    "emails" : [ "zhangchao1015@gmail.com;", "jieping.ye@asu.edu", "zhanglei.njust@yahoo.com.cn" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n30 4.\n15 74\nv1 [\ncs .L\nIn particular, we use the integral probability metric to measure the difference between two domains. For either kind of domain adaptation, we develop a related Hoeffding-type deviation inequality and a symmetrization inequality to achieve the corresponding generalization bound based on the uniform entropy number. We also generalized the classical McDiarmid’s inequality to a more general setting where independent random variables can take values from different domains. By using this inequality, we then obtain generalization bounds based on the Rademacher complexity. Afterwards, we analyze the asymptotic convergence and the rate of convergence of the learning process for such kind of domain adaptation. Meanwhile, we discuss the factors that affect the asymptotic behavior of the learning process and the numerical experiments support our theoretical findings as well."
    }, {
      "heading" : "1 Introduction",
      "text" : "The generalization bound measures the probability that a function, chosen from a function class by an algorithm, has a sufficiently small error and plays an important role in statistical learning theory (see Vapnik, 1999; Bousquet et al., 2004). The generalization bounds have been widely used to study the consistency of the ERM-based learning process (Vapnik, 1999), the asymptotic convergence of empirical process (Van der Vaart and Wellner, 1996) and the learnability of learning models (Blumer et al., 1989). Generally, there are three essential aspects to obtain the generalization bounds of a specific learning process: complexity measures of function classes, deviation (or concentration) inequalities and symmetrization inequalities related to the learning process. For example, Van der Vaart and Wellner (1996) presented the generalization bounds\nbased on the Rademacher complexity and the covering number, respectively. Vapnik (1999) gave the generalization bounds based on the Vapnik-Chervonenkis (VC) dimension. Bartlett et al. (2005) proposed the local Rademacher complexity and obtained a sharp generalization bound for a particular function class {f ∈ F : Ef 2 < βEf, β > 0}. Hussain and Shawe-Taylor (2011) showed improved loss bounds for multiple kernel learning.\nIt is noteworthy that the aforementioned results of statistical learning theory are all built under the assumption that training and test data are drawn from the same distribution (or briefly called the assumption of same distribution). This assumption may not be valid in the situation that training and test data have different distributions, which will arise in many practical applications including speech recognition (Jiang and Zhai, 2007) and natural language processing (Blitzer et al., 2007). Domain adaptation has recently been proposed to handle this situation and it is aimed to apply a learning model, trained by using the samples drawn from a certain domain (source domain), to the samples drawn from another domain (target domain) with a different distribution (see Bickel et al., 2007; Wu and Dietterich, 2004; Blitzer et al., 2006; Ben-David et al., 2010; Bian et al., 2012).\nWithout loss of generality, this paper is mainly concerned with two types of representative domain adaptation. In the first type, the learner receives training data from several source domains, known as domain adaptation with multiple sources (see Crammer et al., 2006, 2008; Mansour et al., 2008, 2009a). In the second type, the learner minimizes a convex combination of the source and the target empirical risks, termed as domain adaptation combining source and target data (see Ben-David et al., 2010; Blitzer et al., 2008)."
    }, {
      "heading" : "1.1 Overview of Main Results",
      "text" : "In this paper, we present a new framework to obtain the generalization bounds of the learning process for the aforementioned two kinds of representative domain adaptation, respectively. Based on the resultant bounds, we then analyze the asymptotical properties of the learning processes for the two types of domain adaptation. There are four major aspects in the framework:\n• the quantity measuring the difference between two domains;\n• the complexity measure of function class;\n• the deviation inequalities of the learning process for domain adaptation;\n• the symmetrization inequality of the learning precess for domain adaptation. Generally, in order to obtain the generalization bounds of a learning process, one needs to develop the related deviation (or concentration) inequalities of the learning process. For either kind of domain adaptation, we use a martingale method to develop the related Hoeffding-type deviation inequality. Moreover, in the situation of domain adaptation, since the source domain differs from the target domain, the desired symmetrization inequality for domain adaptation should incorporate some quantity to reflect the difference. From the point of this view, we then obtain the related symmetrization inequality incorporating the integral probability metric that measures the difference between the distributions of the source and the target domains. Next, we present the generalization bounds based on the uniform entropy number for both kinds of domain adaptation. Also, we generalize the classical McDiarmid’s inequality to a more general setting, where independent random variables take values from different domains. By using the derived\ninequality, we obtain the generalization bounds based on the Rademacher complexity. Following the resultant bounds, we study the asymptotic convergence and the rate of convergence of the learning process in addition to a discussion on factors that affect the asymptotic behaviors. The numerical experiments support our theoretical findings as well. Meanwhile, we give a comparison with the related results under the assumption of same distribution."
    }, {
      "heading" : "1.2 Organization of the Paper",
      "text" : "The rest of this paper is organized as follows. Section 2 introduces the problems studied in this paper. Section 3 introduces the integral probability metric to measure the difference between two domains. In Section 4, we introduce two kinds of complexity measures of function classes including the uniform entropy number and the Rademacher complexity. In Section 5 (resp. Section 6), we present the generalization bounds of the learning process for domain adaptation with multiple sources (resp. combining source and target data), and then analyze the asymptotic behavior of the learning process in addition to the related numerical experiment supporting our findings. In Section 7, we list the existing works on the theoretical analysis of domain adaptation as a comparison and the last section concludes the paper. In the appendices, we prove main results of this paper. For clarity of presentation, we also postpone the discussion of the deviation inequalities and the symmetrization inequalities in the appendices."
    }, {
      "heading" : "2 Problem Setup",
      "text" : "In this section, we formalize the main issues of this paper by introducing some necessary notations"
    }, {
      "heading" : "2.1 Domain Adaptation with Multiple Sources",
      "text" : "We denote Z(Sk) := X (Sk)×Y (Sk) ⊂ RI ×RJ (1 ≤ k ≤ K) and Z(T ) := X (T )×Y (T ) ⊂ RI ×RJ as the k-th source domain and the target domain, respectively. Set L = I + J . Let D(Sk) and D(T ) stand for the distributions of the input spaces X (Sk) (1 ≤ k ≤ K) and X (T ), respectively. Denote g (Sk) ∗ : X (Sk) → Y (Sk) and g(T )∗ : X (T ) → Y (T ) as the labeling functions of Z(Sk) (1 ≤ k ≤ K) and Z(T ), respectively. In the situation of domain adaptation with multiple sources, the input-space distributions D(Sk) (1 ≤ k ≤ K) and D(T ) differ from each other, or g(Sk)∗ (1 ≤ k ≤ K) and g(T )∗ differ from each other, or both of the cases occur. There are sufficient amounts of i.i.d. samples ZNk1 = {z(k)n }Nkn=1 drawn from each source domain Z(Sk) (1 ≤ k ≤ K) but little or no labeled samples drawn from the target domain Z(T ).\nGiven w = (w1, · · · , wK) ∈ [0, 1]K with ∑K\nk=1wk = 1, let gw ∈ G be the function that minimizes the empirical risk\nE(S) w\n(ℓ ◦ g) = K∑\nk=1\nwkE (Sk) Nk\n(ℓ ◦ g) = K∑\nk=1\nwk Nk\nNk∑\nn=1\nℓ(g(x(k)n ),y (k) n ) (1)\nover G with respect to sample sets {ZNk1 }Kk=1, and it is expected that gw will perform well on the target expected risk:\nE(T )(ℓ ◦ g) := ∫ ℓ(g(x(T )),y(T ))dP(z(T )), g ∈ G, (2)\ni.e., gw approximates the labeling g (T ) ∗ as precisely as possible.\nIn the learning process of domain adaptation with multiple sources, we are mainly interested in the following two types of quantities:\n• E(T )(ℓ ◦ gw)−E(S)w (ℓ ◦ gw), which corresponds to the estimation of the expected risk in the target domain Z(T ) from the empirical quantity that is the weighted combination of the empirical risks in the multiple sources {Z(Sk)}Kk=1;\n• E(T )(ℓ◦gw)−E(T )(ℓ◦ g̃∗), which corresponds to the performance of the algorithm for domain adaptation with multiple sources,\nwhere g̃∗ ∈ G is the function that minimizes the expected risk E(T )(ℓ ◦ g) over G. Recalling (1) and (2), since\nE(S) w (ℓ ◦ g̃∗)− E(S)w (ℓ ◦ gw) ≥ 0,\nwe have\nE(T )(ℓ ◦ gw) =E(T )(ℓ ◦ gw)− E(T )(ℓ ◦ g̃∗) + E(T )(ℓ ◦ g̃∗) ≤E(S)\nw (ℓ ◦ g̃∗)− E(S)w (ℓ ◦ gw) + E(T )(ℓ ◦ gw)− ET (ℓ ◦ g̃∗) + ET (ℓ ◦ g̃∗)\n≤2 sup g∈G\n∣∣E(T )(ℓ ◦ g)− E(S) w (ℓ ◦ g) ∣∣+ E(T )(ℓ ◦ g̃∗), (3)\nand thus\n0 ≤ E(T )(ℓ ◦ gw)− E(T )(ℓ ◦ g̃∗) ≤ 2 sup g∈G\n∣∣E(T )(ℓ ◦ g)− E(S) w (ℓ ◦ g) ∣∣.\nThis shows that the asymptotic behaviors of the aforementioned two quantities when the sample numbers N1, · · · , NK go to infinity can both be described by the supremum\nsup g∈G\n∣∣E(T )(ℓ ◦ g)− E(S) w (ℓ ◦ g) ∣∣, (4)\nwhich is the so-called generalization bound of the learning process for domain adaptation with multiple sources.\nFor convenience, we define the loss function as class\nF := {z 7→ ℓ(g(x),y) : g ∈ G}, (5)\nand call F as the function class in the rest of this paper. By (1) and (2), given sample sets {ZNk1 }Kk=1 drawn from {Z(Sk)}Kk=1 respectively, we briefly denote for any f ∈ F ,\nE(T )f := ∫ f(z(T ))dP(z(T )) , (6)\nand\nE(S) w f :=\nK∑\nk=1\nwk Nk\nNk∑\nn=1\nf(z(k)n ). (7)\nThus, we rewrite the generalization bound (4) for domain adaptation with multiple sources as\nsup f∈F\n∣∣E(T )f − E(S) w f ∣∣. (8)"
    }, {
      "heading" : "2.2 Domain Adaptation Combining Source and Target Data",
      "text" : "Denote Z(S) := X (S)×Y (S) ⊂ RI×RJ and Z(T ) := X (T )×Y (T ) ⊂ RI×RJ as the source domain and the target domain, respectively. Let D(S) and D(T ) stand for the distributions of the input spaces X (S) and X (T ), respectively. Denote g(S)∗ : X (S) → Y (S) and g(T )∗ : X (T ) → Y (T ) as the labeling functions of Z(S) and Z(T ), respectively. In the situation of domain adaptation combining source and target data (see Blitzer et al., 2008; Ben-David et al., 2010), the input-space distributions D(S) and D(T ) differ from each other, or the labeling functions g(S)∗ and g(T )∗ differ from each other, or both cases occur. There are some (but not enough) samples ZNT1 := {z(T )n }NTn=1 drawn from the target domain Z(T ) in addition to a large amount of samples ZNS1 := {z(S)n }NSn=1 drawn from the source domain Z(S) with N (T ) ≪ N (S). Given a τ ∈ [0, 1), we denote gτ ∈ G as the function that minimizes the convex combination of the source and the target empirical risks over G:\nEτ (ℓ ◦ g) := τE(T )NT (ℓ ◦ g) + (1− τ)E (S) NS (ℓ ◦ g), (9) and it is expected that gτ will perform well for any pair z\n(T ) = (x(T ),y(T )) ∈ Z(T ), i.e., gτ approximates the labeling function g (T ) ∗ as precisely as possible.\nAs mentioned by Blitzer et al. (2008); Ben-David et al. (2010), setting τ involves a tradeoff between the source data that are sufficient but not accurate and the target data that are accurate but not sufficient. Especially, setting τ = 0 provides a learning process of the basic domain adaptation with one single source (see Ben-David et al., 2006).\nSimilar to the situation of domain adaptation with multiple sources, two types of quantities: E(T )(ℓ ◦ gτ )−Eτ (ℓ ◦ gτ ) and E(T )(ℓ ◦ gτ )−E(T )(ℓ ◦ g̃∗) also play an essential role in analyzing the asymptotic behavior of the learning process for domain adaptation combining source and target data. By the similar way of (3), we need to consider the supremum\nsup g∈G\n∣∣E(T )(ℓ ◦ g)− Eτ (ℓ ◦ g) ∣∣, (10)\nwhich is the so-called generalization bound of the learning process for domain adaptation combining source and target data. Following the notation of (5) and taking f = ℓ ◦ g, we can equivalently rewrite the generalization bound (10) as\nsup f∈F\n∣∣E(T )f − Eτf ∣∣. (11)"
    }, {
      "heading" : "3 Integral Probability Metric",
      "text" : "As shown in some existing works (see Mansour et al., 2008, 2009a; Ben-David et al., 2010, 2006), one of major challenges in the theoretical analysis of domain adaptation is to find a quantity to measure the difference between the source domain Z(S) and the target domain Z(T ). Then, one can use the quantity to achieve generalization bounds for domain adaptation. In this section, we use the integral probability metric to measure the difference between the distributions of Z(S) and Z(T ), and then discuss the relationship between the integral probability metric and other quantities proposed in existing works, e.g., the H-divergence and the discrepancy distance (see Ben-David et al., 2010; Mansour et al., 2009b). Moreover, we will show that there is a special situation of domain adaptation, where the integral probability metric performs better than other quantities (see Remark 3.1)"
    }, {
      "heading" : "3.1 Integral Probability Metric",
      "text" : "In Ben-David et al. (2010, 2006), the H-divergence was introduced to derive the generalization bounds based on the VC dimension under the condition of “λ-close”. Mansour et al. (2009b) obtained the generalization bounds based on the Rademacher complexity by using the discrepancy distance. Both quantities are aimed to measure the difference between two input-space distributions D(S) and D(T ). Moreover, Mansour et al. (2009a) used the Rényi divergence to measure the distance between two distributions. In this paper, we use the following quantity to measure the difference between the distributions of the source and the target domains:\nDefinition 3.1 Given two domains Z(S),Z(T ) ⊂ RL, let z(S) and z(T ) be the random variables taking values from Z(S) and Z(T ), respectively. Let F ⊂ RZ be a function class. We define\nDF(S, T ) := sup f∈F\n|E(S)f − E(T )f |, (12)\nwhere the expectations E(S) and E(T ) are taken on the distributions Z(S) and Z(T ), respectively.\nThe quantity DF(S, T ) is termed as the integral probability metric that has played an important role in probability theory for measuring the difference between the two probability distributions (see Zolotarev, 1984; Rachev, 1991; Müller, 1997; Reid and Williamson, 2011). Recently, Sriperumbudur et al. (2009, 2012) gave the further investigation and proposed an empirical method to compute the integral probability metric. As mentioned by Müller (1997)[page 432], the quantity DF(S, T ) is a semimetric and it is a metric if and only if the function class F separates the set of all signed measures with µ(Z) = 0. Namely, according to Definition 3.1, given a non-trivial function class F , the integral probability metric DF(S, T ) is equal to zero if the domains Z(S) and Z(T ) have the same distribution.\nBy (5), the quantity DF(S, T ) can be equivalently rewritten as\nDF(S, T ) = sup g∈G\n∣∣∣E(S)ℓ(g(x(S)),y(S))− E(T )ℓ(g(x(T )),y(T )) ∣∣∣\n=sup g∈G\n∣∣∣E(S)ℓ ( g(x(S)), g(S)∗ (x (S)) ) − E(T )ℓ ( g(x(T )), g(T )∗ (x (T )) )∣∣∣. (13)\nNext, based on the equivalent form (13), we discuss the relationships between the quantity DF(S, T ) and other quantities including the H-divergence and the discrepancy distance."
    }, {
      "heading" : "3.2 Relationship with Other Quantities",
      "text" : "Before the formal discussion, we briefly introduce the related quantities proposed in existing works (see Ben-David et al., 2010; Mansour et al., 2009b).\n3.2.1 H-Divergence and Discrepancy Distance In classification tasks, by setting ℓ as the absolute-value loss function (ℓ(x,y) = |x − y|), Ben-David et al. (2010) introduced a variant of the H-divergence:\ndH△H(D(S),D(T )) = sup g1,g2∈H\n∣∣∣E(S)ℓ ( g1(x (S)), g2(x (S)) ) − E(T )ℓ ( g1(x (T )), g2(x (T )) )∣∣∣ (14)\nto achieve VC-dimension-based generalization bounds for domain adaptation under the condition of “λ-close”: there exists a λ > 0 such that\nλ ≥ inf g∈G\n{∫ ℓ(g(x(S)),y(S))dP(z(S)) + ∫ ℓ(g(x(T )),y(T ))dP(z(T )) } .\nIn both of the classification and regression tasks, given a function class G and a loss function ℓ, Mansour et al. (2009b) defined the discrepancy distance as\ndiscℓ(D(S),D(T )) = sup g1,g2∈G\n∣∣∣E(S)ℓ ( g1(x (S)), g2(x (S)) ) − E(T )ℓ ( g1(x (T )), g2(x (T )) )∣∣∣, (15)\nand then used this quantity to obtain the generalization bounds based on the Rademacher complexity.\nAs mentioned by Mansour et al. (2009b), the quantities (14) and (15) match in the setting of classification tasks by setting ℓ as the absolute-value loss function, while the usage of (15) does not require the condition of “λ-close” but the usage of (14) does. Recalling Definition 3.1, since there is no limitation on the function class F , the integral probability metric DF(S, T ) can be used in both classification and regression tasks. Therefore, we only consider the relationship between the integral probability metric DF(S, T ) and the discrepancy distance discℓ(D(S),D(T )).\n3.2.2 Relationship between DF(S, T ) and discℓ(D(S),D(T )) From Definition 3.1 and (13), we can find that the integral probability metric DF(S, T ) measures the difference between the distributions of the two domains Z(S) and Z(T ). However, as addressed in Section 2, if a domain Z(S) differs from another domain Z(T ), there are three possibilities: the input-space distribution D(S) differs from D(T ), or g(S)∗ differs from g(T )∗ , or both of them occur. Therefore, it is necessary to consider two kinds of differences: the difference between the input-space distributions D(S) and D(T ) and the difference between the labeling functions g(S)∗ and g (T ) ∗ . Next, we will show that the integral probability metric DF(S, T ) can be bounded by using two separate quantities that can measure the difference between D(S) and D(T ) and the difference between g (S) ∗ and g (T ) ∗ , respectively.\nAs shown in (15), the quantity discℓ(D(S),D(T )) actually measures the difference between the input-space distributions D(S) and D(T ). Moreover, we introduce another quantity to measure the difference between the labeling functions g (S) ∗ and g (T ) ∗ :\nDefinition 3.2 Given a loss function ℓ and a function class G, we define\nQ (T ) G (g (S) ∗ , g (T ) ∗ ) := sup\ng1∈G\n∣∣∣E(T )ℓ ( g1(x (T )), g(T )∗ (x (T )) ) − E(T )ℓ ( g1(x (T )), g(S)∗ (x (T )) )∣∣∣. (16)\nNote that if the loss function ℓ and the function class G are both non-trivial (i.e., F is nontrivial), the quantity Q\n(T ) G (g (S) ∗ , g (T ) ∗ ) is a (semi)metric between the labeling functions g (S) ∗ and\ng (T ) ∗ . In fact, it is not hard to verify that Q (T ) G (g (S) ∗ , g (T ) ∗ ) satisfies the triangle inequality and is equal to zero if g (S) ∗ and g (T ) ∗ match.\nBy combining (13), (15) and (16), we have\ndiscℓ(D(S),D(T )) = sup g1,g2∈G\n∣∣∣E(S)ℓ ( g1(x (S)), g2(x (S)) ) − E(T )ℓ ( g1(x (T )), g2(x (T )) )∣∣∣\n≥ sup g1∈G\n∣∣∣E(S)ℓ ( g1(x (S)), g(S)∗ (x (S)) ) − E(T )ℓ ( g1(x (T )), g(S)∗ (x (T )) )∣∣∣\n= sup g1∈G\n∣∣∣E(S)ℓ ( g1(x (S)), g(S)∗ (x (S)) ) − E(T )ℓ ( g1(x (T )), g(T )∗ (x (T )) )\n+ E(T )ℓ ( g1(x (T )), g(T )∗ (x (T )) ) − E(T )ℓ ( g1(x (T )), g(S)∗ (x (T )) )∣∣∣\n≥ sup g1∈G\n∣∣∣E(S)ℓ ( g1(x (S)), g(S)∗ (x (S)) ) − E(T )ℓ ( g1(x (T )), g(T )∗ (x (T )) )∣∣∣\n− sup g1∈G\n∣∣∣E(T )ℓ ( g1(x (T )), g(T )∗ (x (T )) ) − E(T )ℓ ( g1(x (T )), g(S)∗ (x (T )) )∣∣∣\n=DF(S, T )−Q(T )G (g(S)∗ , g(T )∗ ), (17)\nand thus DF(S, T ) ≤ discℓ(D(S),D(T )) +Q(T )G (g(S)∗ , g(T )∗ ), (18)\nwhich implies that the integral probability metric DF(S, T ) can be bounded by the summation of the discrepancy distance discℓ(D(S),D(T )) and the quantity Q(T )G (g (S) ∗ , g (T ) ∗ ), which measure the difference between the input-space distributions D(S) and D(T ) and the difference between the labeling functions g (S) ∗ and g (T ) ∗ , respectively.\nRemark 3.1 Note that there is a specific case in the situation of domain adaptation: D(S) differs from D(T ) and meanwhile g(S)∗ differs from g(T )∗ , while the distribution of the domain Z(S) matches with that of the domain Z(T ). In this case, the integral probability metric DF(S, T ) equals to zero, but discℓ(D(S),D(T )) or Q(T )G (g(S)∗ , g(T )∗ ) neither equals to zero. Therefore, the integral probability metric DF(S, T ) is more suitable for this case than the discrepancy distance discℓ(D(S),D(T ))."
    }, {
      "heading" : "4 Complexity Measures of Function Classes",
      "text" : "Generally, the generalization bound of a certain learning process is achieved by incorporating some complexity measure of the function class, e.g., the covering number, the VC dimension and the Rademacher complexity. In this paper, we are mainly concerned with the uniform entropy number and the Rademacher complexity."
    }, {
      "heading" : "4.1 Uniform Entropy Number",
      "text" : "The uniform entropy number is derived from the concept of the covering number and we refer to Mendelson (2003) for details. The covering number of a function class F is defined as follows:\nDefinition 4.1 Let F be a function class and d be a metric on F . For any ξ > 0, the covering number of F at radius ξ with respect to the metric d, denoted by N (F , ξ, d) is the minimum size of a cover of radius ξ.\nIn some classical results of statistical learning theory, the covering number is applied by letting d be the distribution-dependent metric. For example, as shown in Theorem 2.3 of Mendelson (2003), one can set d as the norm ℓ1(Z N 1 ) and then derive the generalization bound of the i.i.d. learning process by incorporating the expectation of the covering number, i.e., EN (F , ξ, ℓ1(ZN1 )). However, in the situation of domain adaptation, we only know the information of the source domain, while the expectation EN (F , ξ, ℓ1(ZN1 )) is dependent on the distributions of the source and the target domains because z = (x,y). Therefore, the covering number is no longer suitable for our framework to obtain the generalization bounds for domain adaptation. In contrast, the uniform entropy number is distribution-free and thus we choose it as the complexity measure of function classes to derive the generalization bounds for domain adaptation.\nNext, we will consider the uniform entropy number of F in the situations of two types of domain adaptation: (i) domain adaptation with multiple sources; (ii) domain adaptation combining source and target data, respectively."
    }, {
      "heading" : "4.1.1 Domain Adaptation with Multiple Sources",
      "text" : "For clarity of presentation, we give a useful notation for the following discussion. Let {ZNk1 }Kk=1 := {{z(k)n }Nkn=1}Kk=1 be the collection of sample sets drawn from multiple sources {Z(Sk)}Kk=1, respectively. Denote {Z′Nk1 }Kk=1 := {{z′(k)n }Nkn=1}Kk=1 as the collection of the ghost sample sets drawn from {Z(Sk)}Kk=1 such that the ghost sample z′(k)n has the same distribution as z(k)n for any 1 ≤ k ≤ K and any 1 ≤ n ≤ Nk. Denote Z2Nk1 := {ZNk1 ,Z′Nk1 } for any 1 ≤ k ≤ K. Moreover, given an f ∈ F and a w = (w1, · · · , wK) ∈ [0, 1]K with ∑K k=1wk = 1, we introduce a variant of the ℓ1 norm:\n‖f‖ ℓw1 ({Z 2Nk 1 } K k=1) :=\nK∑\nk=1\nwk Nk\nNk∑\nn=1\n|f(z(k)n )|. (19)\nIt is noteworthy that the variant ℓw1 of the ℓ1 norm is still a norm on the functional space, which can be directly verified by using the definition of norm, so we omit it here.\nIn the situation of domain adaptation with multiple sources, by setting the metric d as ℓw1 ({Z2Nk1 }Kk=1), we then define the uniform entropy number of F with respect to the metric ℓw1 ({Z2Nk1 }Kk=1) as\nlnNw1 ( F , ξ, 2 K∑\nk=1\nNk ) := sup\n{Z 2Nk 1 } K k=1\nlnN ( F , ξ, ℓw1 ({Z2Nk1 }Kk=1) ) . (20)"
    }, {
      "heading" : "4.1.2 Domain Adaptation Combining Source and Target Data",
      "text" : "In the situation of domain adaptation combining source and target data, we have to introduce another variant of the ℓ1 norm on F . Let ZNS1 = {z(S)n }NSn=1 and Z NT 1 = {z(T )n }NTn=1 be two sets of samples drawn from the domains Z(S) and Z(T ), respectively. Given an f ∈ F , we define for any τ ∈ [0, 1),\n‖f‖ ℓτ1(Z NS 1 ,Z NT 1 )\n:= τ\nNT\nNT∑\nn=1\n|f(z(T )n )|+ 1− τ NS\nNS∑\nn=1\n|f(z(S)n )|. (21)\nNote that the variant ℓτ1 (τ ∈ [0, 1)) of the norm ℓ1 is still a norm on the functional space, which can be easily verified by using the definition of norm, so we omit it here.\nMoreover, let Z′NS1 and Z ′NT 1 be the ghost sample sets of Z NS 1 and Z NT 1 , respectively. Denote\nZ2NS1 := {ZNS1 ,Z′NS1 } and Z 2NT 1 := {Z NT 1 ,Z\n′NT\n1 }, respectively. Then, the uniform entropy number of F with respect to the metric ℓτ1(Z) is defined as\nlnN τ1 (F , ξ, 2(NS +NT )) := sup Z lnN (F , ξ, ℓτ1(Z)) , (22)\nwhere Z := {Z2NS1 ,Z 2NT 1 }."
    }, {
      "heading" : "4.2 Rademacher Complexity",
      "text" : "The Rademacher complexity is one of the most frequently used complexity measures of function classes and we refer to Van der Vaart and Wellner (1996); Mendelson (2003) for details.\nDefinition 4.2 Let F be a function class and {zn}Nn=1 be a sample set drawn from Z. Denote {σn}Nn=1 be a set of random variables independently taking either value from {−1, 1} with equal probability. Rademacher complexity of F is defined as\nR(F) := E sup f∈F\n{ 1\nN\n∣∣ N∑\nn=1\nσnf(zn) ∣∣ }\n(23)\nwith its empirical version\nRN(F) := Eσ sup f∈F\n{ 1\nN\n∣∣ N∑\nn=1\nσnf(zn) ∣∣ } , (24)\nwhere E stands for the expectation taken with respect to all random variables {zn}Nn=1 and {σn}Nn=1, and Eσ stands for the expectation only taken with respect to the random variables {σn}Nn=1."
    }, {
      "heading" : "5 Learning Processes of Domain Adaptation with Multi-",
      "text" : "ple Sources\nIn this section, we present two generalization bounds of the learning process for domain adaptation with multiple sources. They are based on the uniform entropy number and the Rademacher complexity, respectively. By using the derived bounds based on the uniform entropy number, we then analyze the asymptotic convergence and the rate of convergence of the learning process. The numerical experiment supports our theoretical analysis as well."
    }, {
      "heading" : "5.1 Generalization Bounds",
      "text" : "Based on the uniform entropy number defined in (20), a generalization bound for domain adaptation with multiple sources is presented in the following theorem.\nTheorem 5.1 Assume that F is a function class consisting of bounded functions with the range [a, b]. Let w = (w1, · · · , wK) ∈ [0, 1]K with ∑K k=1wk = 1. Then, given an arbitrary ξ > D (w) F (S, T ), we have for any (∏K k=1Nk ) ≥ 8(b−a)2 (ξ′)2 and any ǫ > 0, with probability at least 1− ǫ,\nsup f∈F\n∣∣E(S) w f − E(T )f ∣∣ ≤ D(w)F (S, T ) +   ( lnNw1 ( F , ξ′/8, 2∑Kk=1Nk ) − ln(ǫ/8) ) (∏K k=1 Nk )\n32(b−a)2 (∑K\nk=1 w 2 k ( ∏ i6=k Ni) )\n  1 2 , (25)\nwhere ξ′ = ξ −D(w)F (S, T ) and\nD (w) F (S, T ) :=\nK∑\nk=1\nwkDF(Sk, T ). (26)\nIn the above theorem, we show that the generalization bound supf∈F |E(T )f − E(S)w f | can be bounded by the right-hand side of (25). Compared to the classical result under the assumption of same distribution (see Mendelson, 2003, Theorem 2.3 and Definition 2.5): with probability at least 1− ǫ,\nsup f∈F\n∣∣ENf − Ef ∣∣ ≤ O\n  ( lnN1 ( F , ξ, N ) − ln(ǫ/8)\nN\n) 1 2   (27)\nwith ENf being the empirical risk with respect to the sample set Z N 1 , there is a discrepancy quantity D (w) F (S, T ) that is determined by the two factors: the choice of w and the integral probability metrics DF(Sk, T ) (1 ≤ k ≤ K). The two results will coincide if any source domain and the target domain match, i.e., DF(Sk, T ) = 0 holds for any 1 ≤ k ≤ K.\nIn order to prove this result, we develop the specific Hoeffding-type deviation inequality and the symmetrization inequality for domain adaptation with multiple sources, respectively. The detailed proof is arranged in Appendix A. Subsequently, we give another generalization bound based on the Rademacher complexity:\nTheorem 5.2 Assume that F is a function class consisting of bounded functions with the range [a, b]. Let w = (w1, · · · , wK) ∈ [0, 1]K with ∑K k=1wk = 1. Then, we have with probability at least 1− ǫ,\nsup f∈F\n∣∣E(S) w f − E(T )f ∣∣ ≤ D(w)F (S, T ) + 2 K∑\nk=1\nwkR(k)(F) +\n√√√√ K∑\nk=1\n(b− a)2w2k ln(1/ǫ) 2Nk , (28)\nwhere D (w) F (S, T ) is defined in (26) and R(k)(F) (1 ≤ k ≤ K) are the Rademacher complexities on the source domains Z(Sk), respectively.\nSimilarly, the derived bound (28) coincides with the related classical result under the assumption of same distribution (see Bousquet et al., 2004, Theorem 5), when any source domain of\n{Z(Sk)}Kk=1 and the target domain Z(T ) match, i.e., D(w)F (S, T ) = DF(Sk, T ) = 0 holds for any 1 ≤ k ≤ K. The proof of this theorem is processed by introducing a generalized version of McDiarmid’s inequality which allows independent random variables to take values from different domains (see Appendix C).\nSubsequently, based on the derived bound (25), we can analyze the asymptotic behavior of the learning process for domain adaptation with multiple sources."
    }, {
      "heading" : "5.2 Asymptotic Convergence",
      "text" : "In statistical learning theory, it is well-known that the complexity of function class is one of main factors to the asymptotic convergence of the learning process under the assumption of same distribution (Vapnik, 1999; Van der Vaart and Wellner, 1996; Mendelson, 2003).\nFrom Theorem 5.1, we can directly arrive at the following result showing that the asymptotic convergence of the learning process for domain adaptation with multiple sources is affected by the three aspects: the choice of w, the discrepancy quantity D\n(w) F (S, T ) and the uniform entropy\nnumber lnNw1 ( F , ξ/8, 2∑Kk=1Nk ) . Theorem 5.3 Assume that F is a function class consisting of the bounded functions with the range [a, b]. Let w = (w1, · · · , wK) ∈ [0, 1]K with ∑K k=1wk = 1. If the following condition holds:\nlim N1,··· ,NK→+∞\nlnNw1 ( F , ξ/8, 2∑Kk=1Nk ) (∏K\nk=1 Nk\n)\n32(b−a)2 (∑K\nk=1 w 2 k ( ∏ i6=k Ni) )\n< +∞, (29)\nthen we have for any ξ > D (w) F (S, T ),\nlim N1,··· ,NK→+∞ Pr { sup f∈F ∣∣E(T )f − E(S) w f ∣∣ > ξ } = 0. (30)\nAs shown in Theorem 5.3, if the choice of w ∈ [0, 1]K and the uniform entropy number lnNw1 ( F , ξ′/8, 2∑Kk=1Nk ) satisfy the condition (29) with ∑K k=1wk = 1, the probability of the\nevent that supf∈F ∣∣E(T )f − E(S)w f ∣∣ > ξ will converge to zero for any ξ > D(w)F (S, T ), when the sample numbers N1, · · · , NK of multiple sources go to infinity, respectively. This is partially in accordance with the classical result of the asymptotic convergence of the learning process under the assumption of same distribution (cf. Theorem 2.3 and Definition 2.5 of [22]): the probability of the event that supf∈F ∣∣Ef − ENf ∣∣ > ξ will converge to zero for any ξ > 0, if the uniform entropy number lnN1 (F , ξ, N) satisfies the following:\nlim N→+∞ lnN1 (F , ξ, N) N < +∞. (31)\nNote that in the learning process of domain adaptation with multiple sources, the uniform convergence of the empirical risk on the source domains to the expected risk on the target domain may not hold, because the limit (30) does not hold for any ξ > 0 but for any ξ > D\n(w) F (S, T ).\nBy contrast, the limit (30) holds for all ξ > 0 in the learning process under the assumption of same distribution, if the condition (31) is satisfied. Again, these two results coincide when any source domain and the target domain match, i.e., D (w) F (S, T ) = DF(Sk, T ) = 0 holds for any 1 ≤ k ≤ K. Next, we study the rate of convergence of the learning process for domain adaptation with multiple sources."
    }, {
      "heading" : "5.3 Rate of Convergence",
      "text" : "Recalling (25), we can find that the rate of convergence is affected by the choice of w. According to the Cauchy-Schwarz inequality, setting wk = Nk/ ∑K k=1Nk (1 ≤ k ≤ K), we have\nmax\n{ (∏K k=1Nk )\n32(b− a)2 (∑K k=1w 2 k( ∏ i 6=k Ni) ) } = N1 +N2 + · · ·+NK 32(b− a)2 , (32)\nwhich minimizes the second term of the right-hand side of (25). Thus, by (25), (27) and (32), we find that the fastest rate of convergence of the learning process is up to O(1/ √ N) which is the same as the classical result (27) of the learning process under the assumption of same distribution if the discrepancy D (w) F (S, T ) is ignored.\nIn addition, the bound (28) based on the Rademacher complexity also implies that the rate of convergence of the learning process is affected by the choice of w. Again, according to CauchySchwarz inequality, setting wk =\nNk∑K k=1 Nk (1 ≤ k ≤ K) leads to the fastest rate of convergence: √\n(b− a)2 ln(1/ǫ) 2 ∑K k=1Nk = O(1/ √ N),\nwhich is in accordance with the aforementioned analysis. The following numerical experiments support our theoretical findings (see Fig. 1)."
    }, {
      "heading" : "5.4 Numerical Experiment",
      "text" : "We have performed the numerical experiments to verify the theoretic analysis of the asymptotic convergence of the learning processes for domain adaptation with multiple sources. Without loss of generality, we only consider the case of K = 2, i.e., there are two source domains and one target domain. The experiment data are generated in the following way:\nFor the target domain Z(T ) = X (T ) × Y (T ) ⊂ R100 × R, we consider X (T ) as a Gaussian distribution N(0, 1) and draw {x(T )n }NTn=1 (NT = 4000) from X (T ) randomly and independently. Let β ∈ R100 be a random vector of a Gaussian distribution N(1, 5), and let the random vector R ∈ R100 be a noise term with R ∼ N(0, 0.5). For any 1 ≤ n ≤ NT , we randomly draw β and R from N(1, 5) and N(0, 0.01) respectively, and then generate y (T ) n ∈ Y as follows:\ny(T )n = 〈x(T )n , β〉+R. (33)\nThe derived {(x(T )n , y(T )n )}NTn=1 (NT = 4000) are the samples of the target domain Z(T ) and will be used as the test data.\nIn the similar way, we generate the sample set {(x(1)n , y(1)n )}N1n=1 (N1 = 2000) of the source domain Z(S1) = X (1) ×Y (1) ⊂ R100 × R: for any 1 ≤ n ≤ N1,\ny(1)n = 〈x(1)n , β〉+R, (34)\nwhere x (1) n ∼ N(0.5, 1), β ∼ N(1, 5) and R ∼ N(0, 0.5).\nFor the source domain Z(S2) = X (2) × Y (2) ⊂ R100 × R, the samples {(x(2)n , y(2)n )}N2n=1 (N2 = 2000) are derived in the following way: for any 1 ≤ n ≤ N2,\ny(2)n = 〈x(2)n , β〉+R, (35)\nwhere x (2) n ∼ N(2, 5), β ∼ N(1, 5) and R ∼ N(0, 0.5).\nIn this experiment, we use the method of Least Square Regression1 to minimize the empirical risk\nE(S)w (ℓ ◦ g) = w\nN1\nN1∑\nn=1\nℓ(g(x(1)n ), y (1) n ) + (1− w) N2\nN2∑\nn=1\nℓ(g(x(2)n ), y (2) n ) (36)\nfor different combination coefficients w ∈ {0.1, 0.3, 0.5, 0.9} and then compute the discrepancy |E(S)w f −E(T )NT f | for each N1+N2. The initial N1 and N2 both equal to 200. Each test is repeated 30 times and the final result is the average of the 30 results. After each test, we increment N1 and N2 by 200 until N1 = N2 = 2000. The experiment results are shown in Fig. 1.\nFrom Fig. 1, we can find that for any choice of w, the curve of |E(S)w f −E(T )NT f | is decreasing when N1+N2 increases, which is in accordance with the results presented in Theorems 5.1 & 5.3. Moreover, when w = 0.5, the discrepancy |E(S)w f − E(T )NT f | has the fastest rate of convergence, and the rate becomes slower as w is further away from 0.5. In this experiment, we set N1 = N2 that implies that N2/(N1 + N2) = 0.5. Recalling (25), we have shown that w = N2/(N1 + N2) will provide the fastest rate of convergence and this proposition is supported by the experiment results shown in Fig. 1."
    }, {
      "heading" : "6 Learning Process of Domain Adaptation Combining",
      "text" : "Source and Target Data\nIn this section, we present two generalization bounds of the learning process for domain adaptation combining source and target data, which are based on the uniform entropy number and\n1SLEP Package: http://www.public.asu.edu/∼jye02/Software/SLEP/index.htm\nthe Rademacher complexity, respectively. We then analyze the asymptotic convergence and the rate of convergence of the learning process in addition to the numerical experiments supporting our theoretical analysis."
    }, {
      "heading" : "6.1 Generalization Bounds",
      "text" : "The following theorem provides a generalization bound based on the uniform entropy number with respect to the metric ℓτ1 defined in (22). Similar to the situation of domain adaptation with multiple sources, the proof of this theorem is achieved by using a specific Hoeffding-type deviation inequality and a symmetrization inequality for domain adaptation combining source and target data (see Appendix B).\nTheorem 6.1 Assume that F is a function class consisting of the bounded functions with the range [a, b]. Let ZNS1 = {z(S)n }NSn=1 and Z NT 1 = {z(T )n }NTn=1 be two sets of i.i.d. samples drawn from domains Z(S) and Z(T ), respectively. Then, for any τ ∈ [0, 1) and given an arbitrary ξ > (1− τ)DF (S, T ), we have for any NSNT ≥ 8(b−a) 2 (ξ′)2 , with probability at least 1− ǫ,\nsup f∈F\n∣∣Eτf − E(T )f ∣∣ ≤ (1− τ)DF(S, T ) +\n( lnN τ1 (F , ξ′/8, 2(NS +NT ))− ln(ǫ/8)\nNSNT 32(b−a)2((1−τ)2NT+τ2NS)\n) 1 2\n, (37)\nwhere DF(S, T ) is defined in (12) and ξ ′ := ξ − (1− τ)DF(S, T ).\nCompared to the classical result (27) under the assumption of same distribution, the derived bound (37) contains a term of discrepancy quantity (1 − τ)DF (S, T ) that is determined by two factors: the combination coefficient τ and the quantity DF(S, T ). The two results coincide when the source domain Z(S) and the target domain Z(T ) match, i.e., DF (S, T ) = 0.\nBased on the Rademacher complexity, we then get another generalization bound of the learning process for domain adaptation combining source and target data. Its proof is postponed in Appendix C.\nTheorem 6.2 Assume that F is a function class consisting of the bounded functions with the range [a, b]. Let ZNS1 = {z(S)n }NSn=1 and Z NT 1 = {z(T )n }NTn=1 be two sets of i.i.d. samples drawn from the domains Z(S) and Z(T ), respectively. Then, given τ ∈ [0, 1) and for any ǫ > 0, we have with probability at least 1− ǫ,\nsup f∈F\n∣∣Eτf − E(T )f ∣∣ ≤(1− τ)DF (S, T ) + 2(1− τ)R(S)(F)\n+ 2τR(T )NT (F) + 3τ √ (b− a) ln(4/ǫ) 2NT + (1− τ) √\n(b− a)2 ln(2/ǫ) 2\n( τ 2\nNT + (1− τ)2 NS\n) , (38)\nwhere DF(S, T ) is defined in (12).\nNote that in the derived bound (38), we adopt an empirical Rademacher complexity R(T )NT (F) that is based on the data drawn from the target domain Z(T ), because the distribution of Z(T ) is unknown in the situation of domain adaptation. Similar to the aforementioned discussion, the generalization bound (38) coincides with the result under the assumption of same distribution (see Bousquet et al., 2004, Theorem 5), when the source domain of Z(S) and the target domain Z(T ) match, i.e., DF(S, T ) = 0.\nThe two results (37) and (38) exhibit a tradeoff between the sample numbers NS and NT , which is associated with the choice of τ . Although the tradeoff has been mentioned in some previous works (see Blitzer et al., 2008; Ben-David et al., 2010), the following will show a rigorous theoretical analysis of the tradeoff."
    }, {
      "heading" : "6.2 Asymptotic Convergence",
      "text" : "Following Theorem 6.1, we can directly obtain the concerning result pointing out that the asymptotic convergence of the learning process for domain adaptation combining source and target data is affected by three factors: the uniform entropy number lnN τ1 (F , ξ/8, 2(NS +NT )), the integral probability metric DF(S, T ) and the choice of τ ∈ [0, 1).\nTheorem 6.3 Assume that F is a function class consisting of bounded functions with the range [a, b]. Given a τ ∈ [0, 1), if the following condition holds:\nlim NS→+∞\nlnN τ1 (F , ξ′/8, 2(NS +NT )) NSNT\n((1−τ)2NT+τ2NS)\n< +∞ (39)\nwith ξ′ := ξ − (1− τ)DF (S, T ), then we have for any ξ > (1− τ)DF (S, T ),\nlim NS→+∞ Pr { sup f∈F ∣∣E(T )f − Eτf ∣∣ > ξ } = 0. (40)\nAs shown in Theorem 6.3, if the choice of τ ∈ [0, 1) and the uniform entropy number lnN τ1 (F , ξ′/8, 2(NS+NT )) satisfy the condition (39), the probability of the event supf∈F ∣∣E(T )f− Eτf\n∣∣ > ξ will converge to zero for any ξ > (1 − τ)DF (S, T ), when NS goes to infinity. This is partially in accordance with the classical result under the assumption of same distributions derived from the combination of Theorem 2.3 and Definition 2.5 of Mendelson (2003).\nNote that in the learning process for domain adaptation combining source and target data, the uniform convergence of the empirical risk Eτf to the expected risk E\n(T )f may not hold, because the limit (40) does not hold for any ξ > 0 but for any ξ > (1 − τ)DF (S, T ). By contrast, the limit (40) holds for all ξ > 0 in the learning process under the assumption of same distribution, if the condition (31) is satisfied. The two results coincide when the source domain Z(S) and the target domain Z(T ) match, i.e., DF(S, T ) = 0."
    }, {
      "heading" : "6.3 Rate of Convergence",
      "text" : "We consider the choice of τ that is an essential factor to the rate of convergence for the learning process and is associated with the tradeoff between the sample numbers NS and NT . Recalling\n(37), if we fix the value of lnN τ1 (F , ξ′/8, 2(NS +NT )), setting τ = NTNT+NS minimizes the second term of the right-hand side of (37) and then we arrive at\nsup f∈F\n∣∣Eτf − E(T )f ∣∣ ≤ NSDF(S, T )\nNS +NT +\n( (lnN τ1 (F , ξ′/8, 2(NS +NT ))− ln(ǫ/8))\nNS+NT 32(b−a)2\n) 1 2\n, (41)\nwhich implies that setting τ = NT NT+NS\ncan result in the fastest rate of convergence, while it can also cause the relatively larger discrepancy between the empirical risk Eτf and the expected risk E(T )f , because the situation of domain adaptation is set up in the condition that NT ≪ NS, which implies that NS\nNS+NT ≈ 1. Moreover, this choice of τ associated with a trade off between\nsample numbers NS and NT is also suitable to the Rademacher-complexity-based bound (38). It is noteworthy that the value τ = NT\nNT+NS has been mentioned in the section of “Experimental\nResults” in Blitzer et al. (2008). Here, we show a rigorous theoretical analysis of this value and the following numerical experiment also supports this finding (see Fig. 2)."
    }, {
      "heading" : "6.4 Numerical Experiments",
      "text" : "In the situation of domain adaptation combining source and target data, the samples {(x(T )n , y(T )n )}NTn=1 (NT = 4000) of the target domain Z(T ) are generated in the aforementioned way (see (33)). We randomly pick N ′T = 100 samples from them to form the objective function and the rest N ′′T = 3900 are used to test.\nIn the similar way, the samples {(x(S)n , y(S)n )}NSn=1 (NS = 4000) of the source domain Z(S) are generated as follows: for any 1 ≤ n ≤ NS,\ny(S)n = 〈x(S)n , β〉+R, (42)\nwhere x (S) n ∼ N(1, 2), β ∼ N(1, 5) and R ∼ N(0, 0.5).\nWe also use the method of Least Square Regression to minimize the empirical risk\nEτ (ℓ ◦ g) = 1− τ NS\nNS∑\nn=1\nℓ(g(x(1)n ), y (1) n ) +\nτ\nN ′T\nN ′ T∑\nn=1\nℓ(g(x(T )n ), y (T ) n )\nfor different combination coefficients τ ∈ {0.1, 0.3, 0.5, 0.9} and then compute the discrepancy |Eτf −E(T )N ′′ T f | for each NS. Since it has to be satisfied that NS ≫ N ′T , the initial NS is set to be 200. Each test is repeated 100 times and the final result is the average of the 100 results. After each test, we increment NS by 200 until NS = 4000. The experiment results are shown in Fig. 2.\nFigure (2) illustrates that for any choice of τ ∈ {0.1, 0.3, 0.5, 0.9}, the curve of |Eτf −E(T )N ′′ T | is decreasing as NS increases. This is in accordance with our results of the asymptotic convergence of the learning process for domain adaptation with multiple sources (see Theorems 6.1 and 6.3). Furthermore, Fig. 2 also shows that when τ ≈ N ′T /(NS +N ′T ), the discrepancy |E(S)τ f − E(T )N ′′ T f | has the fastest rate of convergence, and the rate becomes slower as τ is further away from N ′T/(NS + N ′ T ). Thus, this is in accordance with the theoretical analysis of the asymptotic convergence presented above."
    }, {
      "heading" : "7 Prior Works",
      "text" : "There have been some previous works on the theoretical analysis of domain adaptation with multiple sources (see Ben-David et al., 2010; Crammer et al., 2006, 2008; Mansour et al., 2008, 2009a) and domain adaptation combining source and target data (see Blitzer et al., 2008; Ben-David et al., 2010).\nIn Crammer et al. (2006, 2008), the function class and the loss function are assumed to satisfy the conditions of “α-triangle inequality” and “uniform convergence bound”. Moreover, one has to get some prior information about the disparity between any source domain and the target domain. Under these conditions, some generalization bounds were obtained by using the classical techniques developed under the assumption of same distribution.\nMansour et al. (2008) proposed another framework to study the problem of domain adaptation with multiple sources. In this framework, one has to know some prior knowledge including the exact distributions of the source domains and the hypothesis function with a small loss on each source domain. Furthermore, the target domain and the hypothesis function on the target domain were deemed as the mixture of the source domains and the mixture of the hypothesis functions on the source domains, respectively. Then, by introducing the Rényi divergence, Mansour et al. (2009a) extended their previous work (Mansour et al., 2008) to a more general setting, where the distribution of the target domain can be arbitrary and one only needs to know an approximation of the exact distribution of each source domain. Ben-David et al. (2010) also discussed the situation of domain adaptation with the mixture of source domains.\nIn Ben-David et al. (2010); Blitzer et al. (2008), domain adaptation combining source and target data was originally proposed and meanwhile a theoretical framework was presented to analyze its properties for the classification tasks by introducing the H-divergence. Under the condition of “λ-close”, the authors applied the classical techniques developed under the assumption of same distribution to achieve the generalization bounds based on the VC dimension.\nMansour et al. (2009b) introduced the discrepancy distance discℓ(D(S),D(T )) to capture the difference between domains and this quantity can be used in both classification and regression tasks. By applying the classical results of statistical learning theory, the authors obtained the generalization bounds based on the Rademacher complexity."
    }, {
      "heading" : "8 Conclusion",
      "text" : "In this paper, we propose a new framework to obtain generalization bounds of the learning process for two representative types of domain adaptation: domain adaptation with multiple sources and domain adaptation combining source and target data. This framework is suitable for a variant of learning tasks including classification and regression. Based on the derived bounds, we theoretically analyze the asymptotic convergence and the rate of convergence of the learning process for domain adaptation. There are four important aspects of this framework: the quantity measuring the difference between two domains; the complexity measure of function class, the deviation inequality and the symmetrization inequality for domain adaptation.\n• We use the integral probability metric DF(S, T ) to measure the difference between two domains Z(S) and Z(T ). We show that the integral probability metric is well-defined and is a (semi)metric on the space of the probability distributions. It can be bounded by the\nsummation of the discrepancy distance discℓ(D(S),D(T )) and the quantity Q(T )G (g (S) ∗ , g (T ) ∗ ), which measure the difference between the input-space distributions D(S) and D(T ) and the difference between labeling functions g (S) ∗ and g (T ) ∗ , respectively. Note that there is a special case that is more suitable to the integral probability metric DF(S, T ) than other quantities (see Remark 3.1).\n• The uniform entropy number and the Rademacher complexity are adopted to achieved the generalization bounds (25); (37) and (28); (38), respectively. It is noteworthy that the generalization bounds (25) and (37) can lead to the results based on the fat-shattering dimension, respectively (see Mendelson, 2003, Theorem 2.18). According to Theorem 2.6.4 of Van der Vaart and Wellner (1996), the bounds based on the VC dimension can also be obtained from the results (25) and (37), respectively.\n• Instead of directly applying the classical techniques, we present the specific deviation inequalities for the learning process of domain adaptation. In order to obtain the generalization bounds based on the uniform entropy numbers, we develop the specific Hoeffding-type deviation inequalities for the two types of domain adaptation, respectively (see Appendices A & B). Furthermore, we also generalize the classical McDiarmid’s inequality to a more general setting where the independent random variables can take value from different domains (see Appendix C).\n• We also develop the related symmetrization inequalities of the learning process for domain adaptation. The derived inequalities incorporate the discrepancy term that is determined by the difference between the source and the target domains and reflects the learningtransfering from the source to the target domains.\nBased on the derived generalization bounds, we provide a rigorous theoretical analysis of the asymptotic convergence and the rate of convergence of the learning process for either kind of\ndomain adaptation. We also consider the choices of w and τ that affect the rate of convergence of the learning processes for the two types of domain adaptation, respectively. Moreover, we give a comparison with the previous works Ben-David et al. (2010); Crammer et al. (2006, 2008); Mansour et al. (2008, 2009a); Blitzer et al. (2008) as well as the related results of the learning process under the assumption of same distribution (see Bousquet et al., 2004; Mendelson, 2003). The numerical experiments support our theoretical findings as well.\nIn our future work, we will attempt to find a new distance between distributions to develop the generalization bounds based on other complexity measures, and analyze other theoretical properties of domain adaptation."
    }, {
      "heading" : "A Proof of Theorem 5.1",
      "text" : "In this appendix, we provide the proof of Theorem 5.1. In order to achieve the proof, we need to develop the specific Hoeffding-type deviation inequality and the symmetrization inequality for domain adaptation with multiple sources.\nA.1 Hoeffding-Type Deviation Inequality for Multiple Sources\nDeviation (or concentration) inequalities play an essential role in obtaining the generalization bounds for a certain learning process. Generally, specific deviation inequalities need to be developed for different learning processes. There are many popular deviation and concentration inequalities, e.g., Hoeffding’s inequality, McDiarmid’s inequality, Bennett’s inequality, Bernstein’s inequality and Talagrand’s inequality. These results are all built under the assumption of same distribution, and thus they are not applicable (or at least cannot be directly applied) to the setting of multiple sources. Next, based on Hoeffding’s inequality (Hoeffding, 1963), we present a deviation inequality for multiple sources.\nTheorem A.1 Assume that F is a function class consisting of bounded functions with the range [a, b]. Let ZNk1 = {z(k)n }Nkn=1 be the set of i.i.d. samples drawn from the source domain Z(Sk) ⊂ RL (1 ≤ k ≤ K). Given w ∈ [0, 1]K with ∑Kk=1wk = 1 and for any f ∈ F , we define a function Fw : R L ∑K k=1 Nk → R as\nFw ( {XNk1 }Kk=1 ) := K∑\nk=1\nwk\n(∏\ni 6=k\nNi\n) Nk∑\nn=1\nf(x(k)n ), (43)\nwhere for any 1 ≤ k ≤ K and given Nk ∈ N, the set XNk1 is denoted as\nXNk1 := {x(k)1 ,x(k)2 , · · · ,x(k)Nk} ∈ (R L)Nk .\nThen, we have for any ξ > 0,\nPr {∣∣E(S)Fw − Fw ( {ZNk1 }Kk=1 )∣∣ > ξ }\n≤2 exp   −\n2ξ2\n(b− a)2 (∏K k=1Nk )(∑K k=1w 2 k (∏ i 6=k Ni ))\n   , (44)\nwhere E(S) stands for the expectation taken on all source domains {Z(Sk)}Kk=1.\nThis result is an extension of the classical Hoeffding-type deviation inequality under the assumption of same distribution (see Bousquet et al., 2004, Theorem 1). Compared to the classical result, the resultant deviation inequality (44) is suitable to the setting of multiple sources. These two inequalities coincide when there is only one source, i.e., K = 1\nThe proof of Theorem A.1 is processed by a martingale method. Before the formal proof, we introduce some essential notations.\nLet {ZNk1 }Kk=1 be sample sets drawn from multiple sources {Z(Sk)}Kk=1, respectively. Define a random variable\nS(k)n := E (S) { Fw({ZNk1 }Kk=1)|ZN11 ,ZN21 , · · · ,Z Nk−1 1 ,Z n 1 } , 1 ≤ k ≤ K, 0 ≤ n ≤ Nk, (45)\nwhere Zn1 = {z(k)1 , z(k)2 , · · · , z(k)n } ⊆ ZNk1 , and Z01 = ∅.\nIt is clear that S (1) 0 = E (S)Fw and S (K) NK\n= Fw({ZNk1 }Kk=1), where E(S) stands for the expectation taken on all source domains {Z(Sk)}Kk=1.\nThen, according to (43) and (45), we have for any 1 ≤ k ≤ K and 1 ≤ n ≤ Nk,\nS(k)n − S(k)n−1 =E(S) { Fw({ZNk1 }Kk=1) ∣∣ZN11 ,ZN21 , · · · ,Z Nk−1 1 ,Z n 1 }\n− E(S) { Fw({ZNk1 }Kk=1) ∣∣ZN11 ,ZN21 , · · · ,ZNk−11 ,Zn−11 }\n=E(S)\n{ K∑\nk=1\nwk\n(∏\ni 6=k\nNi\n) Nk∑\nn=1\nf(z(k)n ) ∣∣ZN11 ,ZN21 , · · · ,Z Nk−1 1 ,Z n 1\n}\n− E(S) { K∑\nk=1\nwk\n(∏\ni 6=k\nNi\n) Nk∑\nn=1\nf(z(k)n ) ∣∣ZN11 ,ZN21 , · · · ,ZNk−11 ,Zn−11\n}\n= k−1∑\nl=1\nwl\n(∏\ni 6=l\nNi\n) Nl∑\nj=1\nf(z (l) j ) + wk\n(∏\ni 6=k\nNi\n) n∑\nj=1\nf(z (k) j )\n+ E(S)\n{ K∑\nl=k+1\nwl\n(∏\ni 6=l\nNi\n) Nl∑\nj=1\nf(z (l) j ) + wk\n(∏\ni 6=k\nNi\n) Nk∑\nj=n+1\nf(z (k) j )\n}\n− k−1∑\nl=1\nwl\n(∏\ni 6=l\nNi\n) Nl∑\nj=1\nf(z (l) j )− wk\n(∏\ni 6=k\nNi\n) n−1∑\nj=1\nf(z (k) j )\n− E(S) { K∑\nl=k+1\nwl\n(∏\ni 6=l\nNi\n) Nl∑\nj=1\nf(z (l) j ) + wk\n(∏\ni 6=k\nNi\n) Nk∑\nj=n\nf(z (k) j )\n}\n=wk\n(∏\ni 6=k\nNi ) ( f(z(k)n )− E(Sk)f ) . (46)\nTo prove Theorem A.1, we need the following inequality resulted from Hoeffding’s lemma.\nLemma A.1 Let f be a function with the range [a, b]. Then, the following holds for any α > 0:\nE { eα(f(z (S))−E(S)f) } ≤ eα 2(b−a)2 8 . (47)\nProof. We consider (f(z(S))− E(S)f)\nas a random variable. Then, it is clear that\nE{f(z(S))− E(S)f} = 0.\nSince the value of E(S)f is a constant denoted as e, we have\na− e ≤ f(z(S))− E(S)f ≤ b− e.\nAccording to Hoeffding’s lemma, we then have\nE { eα(f(z (S))−E(S)f) } ≤ eα 2(b−a)2 8 . (48)\nThis completes the proof. We are now ready to prove Theorem A.1. Proof of Theorem A.1. According to (43), (46), Lemma A.1, Markov’s inequality, Jensen’s inequality and the law of iterated expectation, we have for any α > 0,\nPr { Fw ( {ZNk1 }Kk=1 ) − E(S)Fw > ξ }\n≤e−αξE { e α ( Fw ( {Z Nk 1 } K k=1 ) −E(S)Fw )}\n=e−αξE { E { e α ∑K k=1 ∑Nk n=1 ( S (k) n −S (k) n−1 )∣∣ZN11 , · · · ,ZNK−11 ,ZNK−11 }}\n=e−αξE { e α (∑K k=1 ∑Nk n=1 ( S (k) n −S (k) n−1 ) − ( S (K) NK −S (K) NK−1 )) E { e α ( S (K) NK −S (K) NK−1 )∣∣ZN11 , · · · ,ZNK−11 ,ZNK−11 }}\n=e−αξE { e α (∑K k=1 ∑Nk n=1 ( S (k) n −S (k) n−1 ) − ( S (K) NK −S (K) NK−1 )) E { eαwK( ∏ i6=K Ni)(f(z (K) N )−E(SK )f) }} ≤e−αξE { e α (∑K k=1 ∑Nk n=1 ( S (k) n −S (k) n−1 ) − ( S (K) NK −S (K) NK−1 ))} e α2w2 K ( ∏ i6=K Ni) 2(b−a)2 8 , (49)\nwhere ZNK−11 := {z(K)1 , · · · , z(K)NK−1} ⊂ Z NK 1 . Therefore, we have\nPr { Fw ( {ZNk1 }Kk=1 ) − E(S)Fw > ξ } ≤ eΦ(α)−αξ , (50)\nwhere\nΦ(α) = α2(b− a)2\n(∏K k=1Nk )(∑K k=1w 2 k (∏ i 6=k Ni ))\n8 . (51)\nSimilarly, we can obtain\nPr { E(S)Fw − Fw ( {ZNk1 }Kk=1 ) > ξ } ≤ eΦ(α)−αξ . (52)\nNote that Φ(α)− αξ is a quadratic function with respect to α > 0 and thus the minimum value “minα>0 {Φ(α)− αξ}” is achieved when\nα = 4ξ\n(b− a)2 (∏K k=1Nk )(∑K k=1w 2 k (∏ i 6=k Ni )) .\nBy combining (50), (51) and (52), we arrive at\nPr {∣∣E(S)Fw − Fw ( {ZNk1 }Kk=1 )∣∣ > ξ }\n≤2 exp   −\n2ξ2\n(b− a)2 (∏K k=1Nk )(∑K k=1w 2 k (∏ i 6=k Ni ))\n   .\nThis completes the proof. In the following subsection, we present a symmetrization inequality for domain adaptation with multiple sources.\nA.2 Symmetrization Inequality\nSymmetrization inequalities are mainly used to replace the expected risk by an empirical risk computed on another sample set that is independent of the given sample set but has the same distribution. In this manner, the generalization bounds can be achieved by applying some kinds of complexity measures, e.g., the covering number and the VC dimension. However, the classical symmetrization results are built under the assumption of same distribution (see Bousquet et al., 2004). The symmetrization inequality for domain adaptation with multiple sources is presented in the following theorem:\nTheorem A.2 Assume that F is a function class with the range [a, b]. Let sample sets {ZNk1 }Kk=1 and {Z′Nk1 }Kk=1 be drawn from the source domains {Z(Sk)}Kk=1. Then, given an arbitrary ξ > D\n(w) F (S, T ) and w = (w1, · · · , wK) ∈ [0, 1]K with ∑K k=1wk = 1, we have for any (∏K k=1Nk ) ≥\n8(b−a)2\n(ξ′)2 ,\nPr { sup f∈F ∣∣E(T )f − E(S) w f ∣∣ > ξ } ≤ 2Pr { sup f∈F ∣∣E′(S) w f − E(S) w f ∣∣ > ξ ′ 2 } , (53)\nwhere ξ′ = ξ −D(w)F (S, T ).\nThis theorem shows that given ξ > D (w) F (S, T ), the probability of the event:\nsup f∈F\n∣∣E(T )f − E(S) w f ∣∣ > ξ\ncan be bounded by using the probability of the event:\nsup f∈F\n∣∣E′(S) w f − E(S) w f ∣∣ > ξ −D (w) F (S, T )\n2 (54)\nthat is only determined by the characteristics of the source domains {Z(Sk)}Kk=1 when ∏K\nk=1Nk ≥ 8(b−a)2\n(ξ′)2 with ξ′ = ξ −D(w)F (S, T ). Compared to the classical symmetrization result under the assumption of same distribution (see Bousquet et al., 2004), there is a discrepancy term D (w) F (S, T ) in the derived inequality. Especially, the two results coincide when any source domain and the target domain match, i.e., DF(Sk, T ) = 0 holds for any 1 ≤ k ≤ K. The following is the proof of Theorem A.2.\nProof of Theorem A.2. Let f̂ be the function achieving the supremum:\nsup f∈F\n∣∣E(T )f − E(S) w f ∣∣\nwith respect to the sample set {ZNk1 }Kk=1. According to (6), (7), (12) and (26), we arrive at\n|E(T )f̂ − E(S) w f̂ | = |E(T )f̂ − E(S)f̂ + E(S)f̂ − E(S) w f̂ | ≤ D(w)F (S, T ) + ∣∣E(S)f̂ − E(S) w f̂ ∣∣, (55)\nand thus,\nPr { |E(T )f̂ − E(S)\nw f̂ | > ξ\n} ≤ Pr { D\n(w) F (S, T ) + ∣∣E(S)f̂ − E(S) w f̂ ∣∣ > ξ } , (56)\nwhere the expectation E(S)f̂ is defined as\nE (S) f̂ :=\nK∑\nk=1\nwkE (Sk)f̂ . (57)\nLet\nξ′ := ξ −D(w)F (S, T ), (58) and denote ∧ as the conjunction of two events. According to the triangle inequality, we have\n( |E(S)f̂ − E(S)\nw f̂ | − |E′(S) w f̂ − E(S)f̂ |\n) ≤ |E′(S)\nw f̂ − E(S) w f̂ |,\nand thus for any ξ′ > 0, ( 1 |E (S) f̂−E (S) w f̂ |>ξ′ )( 1 |E (S) f̂−E′ (S) w f̂ |< ξ′\n2\n)\n=1{ |E (S) f̂−E (S) w f̂ |>ξ′ } ∧ { |E (S) f̂−E′ (S) w f̂ |< ξ ′\n2\n}\n≤1 |E′ (S) w f̂−E (S) w f̂ |> ξ′\n2\n.\nThen, taking the expectation with respect to {Z′Nk1 }Kk=1 gives ( 1 |E (S) f̂−E (S) w f̂ |>ξ′ ) Pr′ { |E(S)f̂ − E′(S) w f̂ | < ξ ′\n2\n}\n≤Pr′ { |E′(S)\nw f̂ − E(S) w f̂ | > ξ\n′\n2\n} . (59)\nBy Chebyshev’s inequality, since {Z′Nk1 }Kk=1 are the sets of i.i.d. samples drawn from the multiple sources {Z(Sk)}Kk=1 respectively, we have for any ξ′ > 0,\nPr′ {∣∣E(S)f̂ − E′(S) w f̂ ∣∣ ≥ ξ ′\n2\n} ≤Pr′ { K∑\nk=1\nwk Nk\nNk∑\nn=1\n|E(Sk)f̂ − f̂(z′(k)n )| ≥ ξ′\n2\n}\n=Pr′\n{ K∑\nk=1\nwk\n(∏\ni 6=k\nNi\n) Nk∑\nn=1\n|E(Sk)f̂ − f̂(z′(k)n )| ≥ ξ′ ∏K\nk=1Nk 2\n}\n≤ 4E\n{∑K k=1wk (∏ i 6=k Ni )∑Nk n=1 ∣∣E(Sk)f̂ − f̂(z′(k)n ) ∣∣2 }\n(∏K k=1Nk )2 (ξ′)2\n= 4E\n{∑K k=1wk (∏ i 6=k Ni ) Nk (b− a)2 }\n(∏K k=1Nk )2 (ξ′)2\n= 4 (∏K k=1Nk ) (b− a)2\n(∏K k=1Nk )2 (ξ′)2\n= 4 (b− a)2\n(ξ′)2 (∏K k=1Nk ) . (60)\nSubsequently, according to (59) and (60), we have for any ξ′ > 0,\nPr′ { |E′(S)\nw f̂ − E(S) w f̂ | > ξ\n′\n2\n} ≥ ( 1 |E (S) f̂−E (S) w f̂ |>ξ′ )( 1− 4 (b− a) 2 (ξ′)2 (∏K k=1Nk ) ) . (61)\nBy combining (56), (58) and (61), taking the expectation with respect to {ZNk1 }Kk=1 and letting 4 (b− a)2\n(ξ′)2 (∏K k=1Nk ) ≤ 1 2\ncan lead to: for any ξ > D (w) F (S, T ),\nPr { |E(T )f̂ − E(S)\nw f̂ | > ξ\n} ≤Pr { |E(S)f̂ − E(S)\nw f̂ | > ξ′\n}\n≤2Pr { |E′(S)\nw f̂ − E(S) w f̂ | > ξ\n′\n2\n} (62)\nwith ξ′ = ξ −D(w)F (S, T ). This completes the proof. By using the resultant deviation inequality and the symmetrization inequality, we can achieve the proof of Theorem 5.1.\nA.3 Proof of Theorem 5.1\nProof of Theorem 5.1. Consider ǫ as an independent Rademacher random variables, i.e., an independent {−1, 1}-valued random variable with equal probability of taking either value. Given sample sets {Z2Nk1 }Kk=1, denote for any f ∈ F and 1 ≤ k ≤ K,\n−→ǫ (k) := (ǫ(k)1 , · · · , ǫ(k)Nk ,−ǫ (k) 1 , · · · ,−ǫ(k)Nk) ∈ {−1, 1} 2Nk , (63)\nand for any f ∈ F , −→ f (Z2Nk1 ) := ( f(z′ (k) 1 ), · · · , f(z′(k)Nk), f(z (k) 1 ), · · · , f(z(k)Nk) ) . (64)\nAccording to (6), (7) and Theorem A.2, given an arbitrary ξ > D (w) F (S, T ), we have for any\n{Nk}Kk=1 ∈ NK such that ∏K k=1Nk ≥ 8(b−a) 2 (ξ′)2 with ξ′ = ξ −D(w)F (S, T ),\nPr { sup f∈F ∣∣E(T )f − E(S) w f ∣∣ > ξ }\n≤2Pr { sup f∈F ∣∣E′(S) w f − E(S) w f ∣∣ > ξ ′ 2 } (by Theorem A.2)\n=2Pr { sup f∈F ∣∣∣ K∑\nk=1\nwk Nk\nNk∑\nn=1\n( f(z′\n(k) n )− f(z(k)n ) )∣∣∣ > ξ ′\n2\n}\n=2Pr { sup f∈F ∣∣∣ K∑\nk=1\nwk Nk\nNk∑\nn=1\nǫ(k)n ( f(z′ (k) n )− f(z(k)n ) )∣∣∣ > ξ ′\n2\n}\n=2Pr { sup f∈F ∣∣∣ K∑\nk=1\nwk 2Nk\n〈−→ǫ (k),−→f (Z2Nk1 ) 〉∣∣∣ > ξ ′\n4\n} . (by (63) and (64)) (65)\nFix a realization of {Z2Nk1 }Kk=1 and let Λ be a ξ′/8-radius cover of F with respect to the ℓw1 ({Z2Nk1 }Kk=1) norm. Since F is composed of the bounded functions with the range [a, b], we assume that the same holds for any h ∈ Λ. If f0 is the function that achieves the following supremum\nsup f∈F\n∣∣∣ K∑\nk=1\nwk 2Nk\n〈−→ǫ (k),−→f (Z2Nk1 ) 〉∣∣∣ > ξ ′\n4 ,\nthere must be an h0 ∈ Λ that satisfies K∑\nk=1\nwk 2Nk\n( |f0(z′(k)n )− h0(z′(k)n )|+ |f0(z(k)n )− h0(z(k)n )| ) < ξ′\n8 ,\nand meanwhile, ∣∣∣ K∑\nk=1\nwk 2Nk\n〈−→ǫ (k),−→h0(Z2Nk1 ) 〉∣∣∣ > ξ ′\n8 .\nTherefore, for the realization of {Z2Nk1 }Kk=1, we arrive at\nPr { sup f∈F ∣∣∣ K∑\nk=1\nwk 2Nk\n〈−→ǫ (k),−→f (Z2Nk1 ) 〉∣∣∣ > ξ ′\n4\n}\n≤Pr { sup h∈Λ ∣∣∣ K∑\nk=1\nwk 2Nk\n〈−→ǫ (k),−→h (Z2Nk1 ) 〉∣∣∣ > ξ ′\n8\n} . (66)\nMoreover, we denote the event\nA := { Pr { sup h∈Λ ∣∣∣ K∑\nk=1\nwk 2Nk\n〈−→ǫ (k),−→h (Z2Nk1 ) 〉∣∣∣ > ξ ′\n8\n}} ,\nand let 1A be the characteristic function of the event A. By Fubini’s Theorem, we have\nPr{A} = E { E−→ǫ { 1A }∣∣ {Z2Nk1 }Kk=1 }\n= E { Pr { sup h∈Λ ∣∣∣ K∑\nk=1\nwk 2Nk\n〈−→ǫ (k),−→h (Z2Nk1 ) 〉∣∣∣ > ξ ′\n8\n}∣∣∣ {Z2Nk1 }Kk=1 } . (67)\nFix a realization of {Z2Nk1 }Kk=1 again. According to (63), (64) and Theorem A.1, we have\nPr { sup h∈Λ ∣∣∣ K∑\nk=1\nwk 2Nk\n〈−→ǫ (k),−→h (Z2Nk1 ) 〉∣∣∣ > ξ ′\n8\n}\n≤|Λ|max h∈Λ Pr\n{∣∣∣ K∑\nk=1\nwk 2Nk\n〈−→ǫ (k),−→h (Z2Nk1 ) 〉∣∣∣ > ξ ′\n8\n}\n=N ( F , ξ′/8, ℓw1 ({Z2Nk1 }Kk=1) ) max h∈Λ Pr {∣∣E′(S) w h− E(S) w h ∣∣ > ξ ′ 4 } ≤N ( F , ξ′/8, ℓw1 ({Z2Nk1 }Kk=1) ) max h∈Λ Pr { |E(S)h− E′(S) w h|+ |E(S)h− E(S) w h| > ξ ′ 4 } ≤2N ( F , ξ′/8, ℓw1 ({Z2Nk1 }Kk=1) ) max h∈Λ Pr {∣∣E(S)h− E(S) w h ∣∣ > ξ ′ 8 }\n≤4N ( F , ξ′/8, ℓw1 ({Z2Nk1 }Kk=1) ) exp    − (∏K k=1Nk ) ( ξ −D(w)F (S, T ) )2 32(b− a)2 (∑K k=1w 2 k( ∏ i 6=k Ni) )    , (68)\nwhere the expectation E (S) is defined in (57).\nThe combination of (65), (66) and (68) leads to the result: given an arbitrary ξ > D (w) F (S, T )\nand for any ∏K\nk=1Nk ≥ 8(b−a) 2 (ξ′)2 with ξ′ = ξ −D(w)F (S, T ),\nPr { sup f∈F ∣∣E(T )f − E(S) w f ∣∣ > ξ }\n≤8EN ( F , ξ′/8, ℓw1 ({Z2Nk1 }Kk=1) ) exp    − (∏K k=1Nk ) ( ξ −D(w)F (S, T ) )2 32(b− a)2 (∑K k=1w 2 k( ∏ i 6=k Ni) )   \n≤8Nw1\n( F , ξ′/8, 2 K∑\nk=1\nNk\n) exp    − (∏K k=1Nk ) ( ξ −D(w)F (S, T ) )2 32(b− a)2 (∑K k=1w 2 k( ∏ i 6=k Ni) )    . (69)\nAccording to (69), letting\nǫ := 8Nw1\n( F , ξ′/8, 2 K∑\nk=1\nNk\n) exp    − (∏K k=1Nk ) ( ξ −D(w)F (S, T ) )2 32(b− a)2 (∑K k=1w 2 k( ∏ i 6=k Ni) )    ,\nwe then arrive at with probability at least 1− ǫ,\nsup f∈F\n∣∣E(S) w f − E(T )f ∣∣ ≤ D(w)F (S, T ) +   lnNw1 ( F , ξ′/8, 2∑Kk=1Nk ) − ln(ǫ/8)(∏K\nk=1 Nk\n)\n32(b−a)2 (∑K\nk=1 w 2 k ( ∏ i6=k Ni) )\n  1 2 ,\nwhere ξ′ = ξ −D(w)F (S, T ). This completes the proof."
    }, {
      "heading" : "B Proof of Theorem 6.1",
      "text" : "Here, we provide the proof of Theorem 6.1. Similar to the situation of domain adaptation with multiple sources, we need to develop the related Hoeffding-type deviation inequality and the symmetrization inequality for domain adaptation combining source and target data.\nB.1 Hoeffding-Type Deviation Inequality\nBased on Hoeffding’s inequality (Hoeffding, 1963), we derive a deviation inequality for the combination of the source and the target domains.\nTheorem B.1 Assume that F is a function class consisting of bounded functions with the range [a, b]. Let ZNS1 := {z(S)n }NSn=1 and Z NT 1 := {z(T )n }NTn=1 be sets of i.i.d. samples drawn from the source domain Z(S) ⊂ RL and the target domain Z(T ) ⊂ RL, respectively. For any τ ∈ [0, 1), define a function Fτ : R L(NS+NT ) → R as\nFτ ( XNT1 ,Y NS 1 ) := τNS NT∑\nn=1\nf(xn) + (1− τ)NT NS∑\nn=1\nf(yn), (70)\nwhere\nXNT1 := {x1, · · · ,xNT } ∈ (RL)NT ; YNS1 := {y1, · · · ,yNS} ∈ (RL)NS .\nThen, we have for any τ ∈ [0, 1) and any ξ > 0,\nPr {∣∣Fτ ( ZNS1 ,Z NT 1 ) − E(∗)Fτ ∣∣ > ξ }\n≤2 exp { − 2ξ 2\n(b− a)2NSNT ((1− τ)2NT + τ 2NS)\n} , (71)\nwhere the expectation E(∗) is taken on both of the source domain Z(S) and the target domain Z(T ).\nIn this theorem, we present a deviation inequality for the combination of source and target domains, which is an extension of the classical Hoeffding-type deviation inequality under the assumption of same distribution (see Bousquet et al., 2004, Theorem 1). Compare to the classical result, the resultant deviation inequality (71) allows the random variables to take values from different domains. The two inequalities coincide when the source domain Z(S) and the target domain Z(T ) match, i.e., DF(S, T ) = 0.\nThe proof of Theorem B.1 is also processed by a martingale method. Before the formal proof, we introduce some essential notations.\nFor any τ ∈ [0, 1), we denote\nFS(Z NS 1 ) := (1− τ)NT\nNS∑\nn=1\nf(z(S)n ); FT (Z NT 1 ) := τNS\nNT∑\nn=1\nf(z(T )n ). (72)\nRecalling (70), it is evident that Fτ ( ZNS1 ,Z NT 1 ) = FS(Z NS 1 ) + FT (Z NT 1 ). We then define two random variables:\nSn :=E (S) { FS(Z NS 1 )|Zn1 } , 0 ≤ n ≤ NS;\nTn :=E (T ) { FT (Z NT 1 )|Z n 1 } , 0 ≤ n ≤ NT , (73)\nwhere\nZn1 = {z(S)1 , · · · , z(S)n } ⊆ ZNS1 with Z01 := ∅; Z n\n1 = {z(T )1 , · · · , z(T )n } ⊆ Z NT 1 with Z 0 1 := ∅.\nIt is clear that S0 = E (S)FS; SNS = FS(Z NS 1 ) and T0 = E (T )FT ; TNT = FT (Z NT 1 ).\nAccording to (70) and (73), we have for any 1 ≤ n ≤ NS and any τ ∈ [0, 1),\nSn − Sn−1 =E(S) { FS(Z NS 1 )|Zn1 } − E(S) { FS(Z NS 1 )|Zn−11 }\n=E(S) { (1− τ)NT NS∑\nn=1\nf(z(S)n ) ∣∣∣Zn1 } − E(S) { (1− τ)NT NS∑\nn=1\nf(z(S)n ) ∣∣∣Zn−11\n}\n=(1− τ)NT n∑\nm=1\nf(z(S)m ) + E (S) { (1− τ)NT NS∑\nm=n+1\nf(z(S)m )\n}\n− ( (1− τ)NT n−1∑\nm=1\nf(z(S)m ) + E (S) { (1− τ)NT NS∑\nm=n\nf(z(S)m )\n})\n=(1− τ)NT ( f(z(S)n )− E(S)f ) . (74)\nSimilarly, we also have for any 1 ≤ n ≤ NT ,\nTn − Tn−1 = τNS ( f(z(T )n )− E(T )f ) . (75)\nWe are now ready to prove Theorem B.1. Proof of Theorem B.1. According to (70) and (72), we have\nFτ (Z N 1 )− E(∗)Fτ =FS(ZNS1 ) + FT (Z NT 1 )− E(∗){FS + FT}\n=FS(Z NS 1 )− E(S)FS + FT (Z NT 1 )− E(T )FT . (76)\nAccording to Lemma A.1, (74), (75), (76), Markov’s inequality, Jensen’s inequality and the law of iterated expectation, we have for any α > 0 and any τ ∈ [0, 1),\nPr { Fτ (Z N 1 )− E(∗)Fτ > ξ }\n=Pr { FS(Z NS 1 )− E(S)FS + FT (Z NT 1 )− E(T )FT > ξ } ≤e−αξE { e α ( FS(Z NS 1 )−E (S)FS+FT (Z NT 1 )−E (T )FT )}\n=e−αξE { eα (∑NS n=1(Sn−Sn−1)+ ∑NT n=1(Tn−Tn−1) )}\n=e−αξE { E { eα (∑NS n=1(Sn−Sn−1)+ ∑NT n=1(Tn−Tn−1) )∣∣∣ZNS−11 }}\n=e−αξE { eα (∑NS−1 n=1 (Sn−Sn−1)+ ∑NT n=1(Tn−Tn−1) ) E { eα(SNS−SNS−1) ∣∣∣ZNS−11 }} ≤e−αξE { eα (∑NS−1 n=1 (Sn−Sn−1)+ ∑NT n=1(Tn−Tn−1) )} e (1−τ)2N2 T α2(b−a)2 8\n=e−αξE { eα (∑NS−1 n=1 (Sn−Sn−1)+ ∑NT −1 n=1 (Tn−Tn−1) ) E { eα(TNT −TNT−1) ∣∣∣ZNT−11 }}\n× e (1−τ)2N2 T α2(b−a)2 8 ≤e−αξE { eα (∑NS−1 n=1 (Sn−Sn−1)+ ∑NT −1 n=1 (Tn−Tn−1) )} e τ2N2 S α2(b−a)2 8 e (1−τ)2N2 T α2(b−a)2 8 . (77)\nThen, we have\nPr { Fτ ( ZNS1 ,Z NT 1 ) − E(∗)Fτ > ξ } ≤ eΦ(α)−αξ , (78)\nwhere\nΦ(α) = α2(1− τ)2(b− a)2NSN2T\n8 + α2τ 2(b− a)2N2SNT 8 . (79)\nSimilarly, we can arrive at\nPr { E(∗)Fτ − Fτ ( ZNS1 ,Z NT 1 ) > ξ } ≤ eΦ(α)−αξ . (80)\nNote that Φ(α)− αξ is a quadratic function with respect to α > 0 and thus the minimum value\nmin α>0\n{Φ(α)− αξ}\nis achieved when\nα = 4ξ\n(b− a)2NSNT ((1− τ)2NT + τ 2NS) .\nBy combining (78), (79) and (80), we arrive at\nPr { |Fτ ( ZNS1 ,Z NT 1 ) − E(∗)Fτ | > ξ } ≤ 2 exp { − 2ξ 2\n(b− a)2NSNT ((1− τ)2NT + τ 2NS)\n} .\nThis completes the proof.\nB.2 Symmetrization Inequality\nIn the following theorem, we present the symmetrization inequality for domain adaptation combining source and target data.\nTheorem B.2 Assume that F is a function class with the range [a, b]. Let ZNS1 and Z′NS1 be drawn from the source domain Z(S), and ZNT1 and Z′ NT 1 be drawn from the target domain Z(T ). Then, for any τ ∈ [0, 1) and given an arbitrary ξ > (1 − τ)DF (S, T ), we have for any NSNT ≥ 8(b−a) 2\n(ξ′)2 ,\nPr { sup f∈F ∣∣E(T )f − Eτf ∣∣ > ξ } ≤ 2Pr { sup f∈F ∣∣E′τf − Eτf ∣∣ > ξ ′ 2 } (81)\nwith ξ′ = ξ − (1− τ)DF (S, T ).\nThis theorem shows that for any ξ > (1− τ)DF (S, T ), the probability of the event:\nsup f∈F\n∣∣E(T )f − Eτf ∣∣ > ξ\ncan be bounded by using the probability of the event:\nsup f∈F\n∣∣E′τf − Eτf ∣∣ > ξ ′\n2\nthat is only determined by the samples drawn from the source domain Z(S) and the target domain Z(T ), when NSNT ≥ 8(b−a) 2\n(ξ′)2 . Compared to the classical symmetrization result under\nthe assumption of same distribution (see Bousquet et al., 2004), there is a discrepancy term (1 − τ)DF (S, T ). The two results will coincide when the source and the target domains match, i.e., DF(S, T ) = 0. The following is the proof of Theorem B.2.\nProof of Theorem B.2. Let f̂ be the function achieving the supremum:\nsup f∈F\n|E(T )f − Eτf |\nwith respect to ZNS1 and Z NT 1 . According to (9) and (12), we arrive at\n∣∣E(T )f̂ − Eτ f̂ ∣∣ = ∣∣τE(T )f̂ + (1− τ)E(T )f̂ − (1− τ)E(S)f̂ + (1− τ)E(S)f̂ − Eτ f̂ ∣∣\n= ∣∣τ(E(T )f̂ − E(T )NT f̂) + (1− τ)(E (T )f̂ − E(S)f̂) + (1− τ)(E(S)f̂ − E(S)NS f̂) ∣∣ ≤(1− τ)DF(S, T ) + ∣∣τ(E(T )f̂ − E(T )NT f̂) + (1− τ)(E (S)f̂ − E(S)NS f̂) ∣∣, (82)\nand thus\nPr {∣∣E(T )f̂ − Eτ f̂ ∣∣ > ξ }\n≤Pr { (1− τ)DF(S, T ) + ∣∣τ(E(T )f̂ − E(T )NT f̂) + (1− τ)(E (S)f̂ − E(S)NS f̂) ∣∣ > ξ } , (83)\nwhere\nE (T ) NT\nf̂ := 1\nNT\nNT∑\nn=1\nf̂(z(T )n ); E (S) NS\nf̂ := 1\nNS\nNS∑\nn=1\nf̂(z(S)n ). (84)\nLet ξ′ = ξ − (1− τ)DF(S, T ) (85)\nand denote ∧ as the conjunction of two events. According to the triangle inequality, we have ( 1{\n|τ(E(T )f̂−E (T ) NT f̂)+(1−τ)(E(S) f̂−E (S) NS\nf̂)|>ξ′ } )( 1{ |τ(E(T )f̂−E′\n(T ) NT f̂)+(1−τ)(E(S)f̂−E′ (S) NS f̂)|< ξ ′ 2\n} )\n=1{ |τ(E(T )f̂−E\n(T ) NT f̂)+(1−τ)(E(S) f̂−E (S) NS\nf̂)|>ξ′ } ∧ { |τ(E(T )f̂−E′\n(T ) NT f̂)+(1−τ)(E(S)f̂)−E′ (S) NS f̂ |< ξ ′ 2\n}\n≤1{ |τ(E′\n(T ) NT f̂−E (T ) NT f̂)+(1−τ)(E′ (S) NS f̂−E (S) NS f̂)|> ξ ′ 2\n} .\nThen, taking the expectation with respect to Z′NS1 and Z ′ NT 1 gives\n( 1{\n|τ(E(T )f̂−E (T ) NT f̂)+(1−τ)(E(S) f̂−E (S) NS\nf̂)|>ξ′ } )\n× Pr′ {∣∣τ(E(T )f̂ − E′(T )NT f̂) + (1− τ)(E (S)f̂ − E′(S)NS f̂) ∣∣ < ξ ′\n2\n}\n≤ Pr′ {∣∣τ(E′(T )NT f̂ − E (T ) NT f̂) + (1− τ)(E′(S)NS f̂ − E (S) NS f̂) ∣∣ > ξ ′\n2\n} . (86)\nBy Chebyshev’s inequality, since Z′NS1 = {z′(S)n }NSn=1 and Z′ NT 1 = {z′(T )n }NTn=1 are sets of i.i.d. samples drawn from the source domain Z(S) and the target domain Z(T ) respectively, we have for any ξ′ > 0 and any τ ∈ [0, 1),\nPr′ {∣∣τ(E(T )f̂ − E′(T )NT f̂) + (1− τ)(E (S)f̂ − E′(S)NS f̂) ∣∣ ≥ ξ ′\n2\n}\n≤Pr′ { τ\nNT\nNT∑\nn=1\n|E(T )f̂ − f̂(z′(T )n )|+ 1− τ NS\nNS∑\nn=1\n|E(S)f̂ − f̂(z′(S)n )| ≥ ξ′\n2\n}\n≤ 4E\n{ τNSNT (E (T )f̂ − f̂(z′(T )))2 + (1− τ)NSNT (E(S)f̂ − f̂(z′(S)))2 }\nN2SN 2 T (ξ ′)2\n≤4E {τNSNT (b− a) 2 + (1− τ)NSNT (b− a)2}\nN2SN 2 T (ξ ′)2\n= 4(b− a)2 NSNT (ξ′)2 , (87)\nwhere z′(S) and z′(T ) stand for the ghost random variables taking values from the source domain Z(S) and the target domain Z(T ), respectively.\nSubsequently, according to (86) and (87), we have for any ξ′ > 0,\nPr′ {∣∣τ(E′(T )NT f̂ − E (T ) NT f̂) + (1− τ)(E′(S)NS f̂ − E (S) NS f̂) ∣∣ > ξ ′\n2\n}\n≥ ( 1{∣∣τ(E(T )f̂−E(T )\nNT f̂)+(1−τ)(E(S)f̂−E (S) NS f̂)\n∣∣>ξ′ }\n)( 1− 4(b− a) 2\nNSNT (ξ′)2\n) . (88)\nAccording to (83), (85) and (88), by letting\n4(b− a)2 NSNT (ξ′)2 ≤ 1 2 ,\nand taking the expectation with respect to ZNS1 and Z NT 1 , we have for any ξ ′ > 0,\nPr { |E(T )f̂ − Eτ f̂ | > ξ }\n≤Pr {∣∣τ(E(T )f̂ − E(T )NT f̂) + (1− τ)(E (S)f̂ − E(S)NS f̂) ∣∣ > ξ′ } ≤2Pr {∣∣τ(E′(T )NT f̂ − E (T ) NT f̂) + (1− τ)(E′(S)NS f̂ − E (S) NS f̂) ∣∣ > ξ ′\n2\n}\n=2Pr {∣∣E′τ f̂ − Eτ f̂ ∣∣ > ξ ′\n2\n} (89)\nwith ξ′ = ξ − (1− τ)DF(S, T ). This completes the proof. We are now ready to prove Theorem 6.1.\nB.3 Proof of Theorem 6.1\nProof of Theorem 6.1. Consider {ǫn}Nn=1 as independent Rademacher random variables, i.e., independent {±1}-valued random variables with equal probability of taking either value. Given {ǫn}NSn=1, {ǫn}NTn=1, Z2NS1 and Z 2NT 1 , denote\n−→ǫ S :=(ǫ1, · · · , ǫNS ,−ǫ1, · · · ,−ǫNS) ∈ {±1}2NS ; −→ǫ T :=(ǫ1, · · · , ǫNT ,−ǫ1, · · · ,−ǫNT ) ∈ {±1}2NT , (90)\nand for any f ∈ F , −→ f (Z2NS1 ) := ( f(z′1), · · · , f(z′NS), f(z1), · · · , f(zNS) ) ∈ [a, b]2NS ;\n−→ f (Z2NT1 ) := ( f(z′1), · · · , f(z′NT ), f(z1), · · · , f(zNT ) ) ∈ [a, b]2NT . (91)\nWe also denote\nZ := { Z\n2NT 1 ,Z 2NS 1\n} ∈ ( Z(T ) )2NT × ( Z(S) )2NS ; −→ǫ :=\n(−→ǫ T , · · · ,−→ǫ T︸ ︷︷ ︸ NS ,−→ǫ S, · · · ,−→ǫ S︸ ︷︷ ︸ NT ) ∈ {±1}4NSNT ;\n−→ f ( Z ) := (−→ f (Z\n2NT 1 ), · · · ,\n−→ f (Z\n2NT 1 )︸ ︷︷ ︸\nNS\n, −→ f (Z2NS1 ), · · · , −→ f (Z2NS1 )︸ ︷︷ ︸\nNT\n) ∈ [a, b]4NSNT . (92)\nAccording to (6), (85) and Theorem B.2, for any τ ∈ [0, 1) and given an arbitrary ξ > (1− τ)DF (S, T ), we have for any NSNT ≥ 8(b−a) 2\nξ′2 with ξ′ = ξ − (1− τ)DF(S, T ),\nPr { sup f∈F ∣∣E(T )f − Eτf ∣∣ > ξ }\n≤2Pr { sup f∈F ∣∣E′τf − Eτf ∣∣ > ξ ′ 2 } (by Theorem B.2)\n=2Pr { sup f∈F ∣∣∣ τ NT NT∑\nn=1\n( f(z′\n(T ) n )− f(z(T )n )\n) + 1− τ NS NS∑\nn=1\n( f(z′\n(S) n )− f(z(S)n ) )∣∣∣ > ξ ′\n2\n}\n=2Pr { sup f∈F ∣∣∣ τ NT NT∑\nn=1\nǫn ( f(z′ (T ) n )− f(z(T )n ) ) + 1− τ NS NS∑\nn=1\nǫn ( f(z′ (S) n )− f(z(S)n ) )∣∣∣ > ξ ′\n2\n}\n=2Pr { sup f∈F ∣∣∣ τ 2NT 〈−→ǫ T , −→ f (Z 2NT 1 ) 〉 + 1− τ 2NS 〈−→ǫ S, −→ f (Z2NS1 ) 〉∣∣∣ > ξ ′ 4 } . (93)\nGiven a τ ∈ [0, 1), fix a realization of Z and let Λ be a ξ′/8-radius cover of F with respect to the ℓτ1(Z) norm. Since F is composed of bounded functions with the range [a, b], we assume that the same holds for any h ∈ Λ. If f0 is the function that achieves the following supremum\nsup f∈F ∣∣∣ τ 2NT 〈−→ǫ T , −→ f (Z 2NT 1 ) 〉 + 1− τ 2NS 〈−→ǫ S, −→ f (Z2NS1 ) 〉∣∣∣ > ξ ′ 4 ,\nthere must be an h0 ∈ Λ that satisfies that\nτ\n2NT\nNT∑\nn=1\n( |f0(z′(T )n )− h0(z′Tn )|+ |f0(z(T )n )− h0(z(T )n )| )\n+ 1− τ 2NS\nNS∑\nn=1\n( |f0(z′(S)n )− h0(z′Sn)|+ |f0(z(S)n )− h0(z(S)n )| ) < ξ′\n8 ,\nand meanwhile, ∣∣∣ τ 2NT 〈−→ǫ T , −→ h 0(Z 2NT 1 ) 〉 + 1− τ 2NS 〈−→ǫ S, −→ h 0(Z 2NS 1 ) 〉∣∣∣ > ξ ′ 8 .\nTherefore, for the realization of Z, we arrive at\nPr { sup f∈F ∣∣∣ τ 2NT 〈−→ǫ T , −→ f (Z 2NT 1 ) 〉 + 1− τ 2NS 〈−→ǫ S, −→ f (Z2NS1 ) 〉∣∣∣ > ξ ′ 4 }\n≤Pr { sup h∈Λ ∣∣∣ τ 2NT 〈−→ǫ T , −→ h (Z 2NT 1 ) 〉 + 1− τ 2NS 〈−→ǫ S, −→ h (Z2NS1 ) 〉∣∣∣ > ξ ′ 8 } . (94)\nMoreover, we denote the event\nA := { Pr { sup h∈Λ ∣∣∣ τ 2NT 〈−→ǫ T , −→ h (Z 2NT 1 ) 〉 + 1− τ 2NS 〈−→ǫ S, −→ h (Z2NS1 ) 〉∣∣∣ > ξ ′ 8 }} ,\nand let 1A be the characteristic function of the event A. By Fubini’s Theorem, we have\nPr{A} = E { E−→ǫ { 1A }∣∣ Z }\n=E { Pr { sup h∈Λ ∣∣∣ τ 2NT 〈−→ǫ T , −→ h (Z 2NT 1 ) 〉 + 1− τ 2NS 〈−→ǫ S, −→ h (Z2NS1 ) 〉∣∣∣ > ξ ′ 8 } ∣∣ Z } . (95)\nFix a realization of Z again. According to (21), (90), (91) and Theorem B.1, for any τ ∈ [0, 1) and given an arbitrary ξ′ > 0, we have for any NSNT ≥ 8(b−a) 2\n(ξ′)2 ,\nPr { sup h∈Λ ∣∣∣ τ 2NT 〈−→ǫ T , −→ h (Z 2NT 1 ) 〉 + 1− τ 2NS 〈−→ǫ S, −→ h (Z2NS1 ) 〉∣∣∣ > ξ ′ 8 }\n≤|Λ|max h∈Λ Pr {∣∣∣ τ 2NT 〈−→ǫ T , −→ h (Z 2NT 1 ) 〉 + 1− τ 2NS 〈−→ǫ S, −→ h (Z2NS1 ) 〉∣∣∣ > ξ ′ 8 }\n=N (F , ξ′/8, ℓτ1(Z))max h∈Λ Pr\n{∣∣E′τh− Eτh ∣∣ > ξ ′\n4\n}\n≤N (F , ξ′/8, ℓτ1(Z))max h∈Λ Pr\n{ |Ẽh− E′τh|+ |Ẽh− Eτh| > ξ′\n4\n}\n≤2N (F , ξ′/8, ℓτ1(Z))max h∈Λ Pr\n{∣∣Ẽh− Eτh ∣∣ > ξ ′\n8\n}\n≤4N (F , ξ′/8, ℓτ1(Z)) exp { − NSNT (ξ − (1− τ)DF(S, T )) 2\n32(b− a)2 ((1− τ)2NT + τ 2NS)\n} , (96)\nwhere Ẽh := τE(T )h + (1− τ)E(S)h. The combination of (22), (93), (94) and (96) leads to the following result: for any τ ∈ [0, 1) and given an arbitrary ξ > (1− τ)DF(S, T ), we have for any NSNT ≥ 8(b−a) 2\n(ξ′)2 ,\nPr { sup f∈F ∣∣E(T )f − Eτf ∣∣ > ξ }\n≤8EN (F , ξ′/8, ℓτ1(Z)) exp { − NSNT (ξ − (1− τ)DF(S, T )) 2\n32(b− a)2 ((1− τ)2NT + τ 2NS)\n}\n≤8N τ1 (F , ξ′/8, 2(NS +NT )) exp { − NSNT (ξ − (1− τ)DF (S, T )) 2\n32(b− a)2 ((1− τ)2NT + τ 2NS)\n} . (97)\nAccording to (97), letting\nǫ := 8N τ1 (F , ξ′/8, 2(NS +NT )) exp { − NSNT (ξ − (1− τ)DF (S, T )) 2\n32(b− a)2 ((1− τ)2NT + τ 2NS)\n} ,\nwe have given an arbitrary ξ > (1 − τ)DF(S, T ) and for any NSNT ≥ 8(b−a) 2\n(ξ′)2 , with probability\nat least 1− ǫ,\nsup f∈F\n∣∣Eτf − E(T )f ∣∣ ≤(1− τ)DF(S, T ) +\n( lnN τ1 (F , ξ′/8, 2(NS +NT ))− ln(ǫ/8)\nNSNT 32(b−a)2((1−τ)2NT+τ2NS)\n) 1 2\n.\nThis completes the proof."
    }, {
      "heading" : "C Proofs of Theorems 5.2 & 6.2",
      "text" : "In this appendix, we will prove Theorem 5.2 and Theorem 6.2. In order to achieve the proofs, we need to generalize the classical McDiarmid’s inequality (see Bousquet et al., 2004, Theorem 6) to a more general setting where independent random variables can independently take values from different domains.\nC.1 Generalized McDiarmid’s inequality\nThe following is the classical McDiarmid’s inequality that is one of the most frequently used deviation inequalities in statistical learning theory and has been widely used to obtain generalization bounds based on the Rademacher complexity under the assumption of same distribution (see Bousquet et al., 2004, Theorem 6).\nTheorem C.1 (McDiamid’s Inequality) Let z1, · · · , zN be N independent random variables taking value from the domain Z. Assume that the function H : Z → R satisfies the condition of bounded difference: for all 1 ≤ n ≤ N ,\nsup z1,··· ,zN ,z′n\n∣∣∣H ( z1, · · · , zn, · · · , zN ) −H ( z1, · · · , z′n, · · · , zN )∣∣∣ ≤ cn. (98)\nThen, for any ξ > 0\nPr { H ( z1, · · · , zn, · · · , zN ) − E { H ( z1, · · · , zn, · · · , zN )} ≥ ξ } ≤ exp { −2ξ2/ N∑\nn=1\nc2n\n} .\nAs shown in Theorem C.1, the classical McDiarmid’s inequality is valid under the condition that random variables z1, · · · , zN are independent and drawn from the same domain. Next, we generalize this inequality to a more general setting, where the independent random variables can take values from different domains.\nTheorem C.2 Given independent domains Z(Sk) (1 ≤ k ≤ K), for any 1 ≤ k ≤ K, let ZNk1 := {z(Sk)n }Nkn=1 be Nk independent random variables taking values from the domain Z(Sk). Assume that the function H : ( Z(S1) )N1 ×· · ·× ( Z(SK )\n)NK → R satisfies the condition of bounded difference: for all 1 ≤ k ≤ K and 1 ≤ n ≤ Nk,\nsup Z\nN1 1 ,··· ,Z NK 1 ,z ′(Sk) n\n∣∣∣H ( ZN11 , · · · ,ZNk−11 , z(Sk)1 , · · · , z(Sk)n , · · · , z(Sk)Nk ,Z Nk+1 1 , · · · ,ZNK1 )\n−H ( ZN11 , · · · ,ZNk−11 , z(Sk)1 , · · · , z′(Sk)n , · · · , z(Sk)Nk ,Z Nk+1 1 , · · · ,ZNK1 )∣∣∣ ≤ c(k)n . (99)\nThen, for any ξ > 0\nPr { H ( ZN11 , · · · ,ZNK1 ) − E { H ( ZN11 , · · · ,ZNK1 )} ≥ ξ } ≤ exp { −2ξ2/ K∑\nk=1\nNk∑\nn=1\n(c(k)n ) 2\n} .\nProof. Define a random variable\nT (k)n := E { H({ZNk1 }Kk=1)|ZN11 ,ZN21 , · · · ,ZNk−11 ,Zn1 } , 1 ≤ k ≤ K, 0 ≤ n ≤ Nk, (100)\nwhere Zn1 = {z(k)1 , z(k)2 , · · · , z(k)n } ⊆ ZNk1 , and Z01 = ∅.\nIt is clear that T\n(1) 0 = E { H({ZNk1 }Kk=1) } and T (K) NK\n= H({ZNk1 }Kk=1), and thus\nH({ZNk1 }Kk=1)− E { H({ZNk1 }Kk=1) } = T (K) NK − T (1)0 = K∑\nk=1\nNk∑\nn=1\n(T (k)n − T (k)n−1). (101)\nDenote for any 1 ≤ k ≤ K and 1 ≤ n ≤ Nk,\nU (k)n = sup µ\n{ T (k)n ∣∣ z (k) n =µ − T (k)n−1 } ;\nL(k)n = inf ν\n{ T (k)n ∣∣ z (k) n =ν − T (k)n−1 } .\nIt follows from the definition of (100) that L (k) n ≤ (T (k)n − T (k)n−1) ≤ U (k)n and thus results in\nT (k)n − T (k)n−1 ≤ U (k)n − L(k)n = sup µ,ν\n{ T (k)n ∣∣ z (k) n =µ − T (k)n ∣∣ z (k) n =ν } ≤ c(k)n . (102)\nMoreover, by the law of iterated expectation, we also have for any 1 ≤ k ≤ K and 1 ≤ n ≤ Nk\nE { T (k)n − T (k)n−1|ZN11 ,ZN21 , · · · ,Z Nk−1 1 ,Z n−1 1 } = 0. (103)\nAccording to Hoeffding inequality (see Hoeffding, 1963), given an α > 0, the condition (99) leads to for any 1 ≤ k ≤ K and 1 ≤ n ≤ Nk,\nE { eα(T (k) n −T (k) n−1)|ZN11 ,ZN21 , · · · ,ZNk−11 ,Zn−11 } ≤ eα2(c(k)n )2/8. (104)\nSubsequently, according to Markov’s inequality, (101), (102), (103) and (104), we have for any α > 0,\nPr { H ( ZN11 , · · · ,ZNK1 ) − E { H ( ZN11 , · · · ,ZNK1 )} ≥ ξ }\n≤e−αξE { eα ( H ( Z N1 1 ,··· ,Z NK 1 ) −E { H ( Z N1 1 ,··· ,Z NK 1 )})} =e−αξE { eα (∑K k=1 ∑Nk n=1(T (k) n −T (k) n−1) )}\n=e−αξE { E { eα (∑K k=1 ∑Nk n=1(T (k) n −T (k) n−1) )} |ZN11 , · · · ,ZNk−11 ,ZNK−11 } =e−αξE { eα (∑K−1 k=1 ∑Nk n=1(T (k) n −T (k) n−1)+ ∑NK−1 n=1 (T (K) n −T (K) n−1) ){ e α ( T (K) NK −T (K) NK−1 ) |ZN11 , · · · ,ZNK−11 ,ZNK−11 }} ≤e−αξE { eα (∑K−1 k=1 ∑Nk n=1(T (k) n −T (k) n−1)+ ∑NK−1 n=1 (T (K) n −T (K) n−1) )} e α2(c (K) NK )2/8\n≤e−αξ K∏\nk=1\nNk∏\nn=1\nexp\n{ α2(c (k) n )2\n8\n}\n=exp { −αξ + α2 K∑\nk=1\nNk∑\nn=1\n(c (k) n )2\n8\n} .\nThe above bound is minimized by setting\nα∗ = 4ξ\n∑K k=1 ∑NK n=1(c (k) n )2 ,\nand its minimum value is\nexp { −2ξ2/ K∑\nk=1\nNk∑\nn=1\n(c(k)n ) 2\n} .\nThis completes the proof.\nC.2 Proof of Theorem 5.2\nBy using Theorem C.2, we prove Theorem 5.2 as follows: Proof of Theorem 5.2 Assume that F is a function class F consisting of bounded functions with the range [a, b]. Let sample sets {ZNk1 }Kk=1 := {{z(k)n }Nkn=1}Kk=1 be drawn from multiple sources Z(Sk) (1 ≤ k ≤ K), respectively. Given a choice of w ∈ [0, 1]K with ∑Kk=1wk = 1, denote\nH ( ZN11 , · · · ,ZNK1 ) := sup\nf∈F\n∣∣E(S) w f − E(T )f ∣∣. (105)\nBy (1), we have\nH ( ZN11 , · · · ,ZNK1 ) = sup\nf∈F\n∣∣∣ K∑\nk=1\nwk ( E (S) Nk f − E(T )f )∣∣∣, (106)\nwhere E (S) Nk f = 1 Nk ∑Nk n=1 f(z (k) n ). Therefore, it is clear that such H ( ZN11 , · · · ,ZNK1 ) satisfies the condition of bounded difference with\nc(k)n = (b− a)wk\nNk\nfor all 1 ≤ k ≤ K and 1 ≤ n ≤ Nk. Thus, according to Theorem C.2, we have for any ξ > 0,\nPr { H ( ZNk1 , · · · ,ZNk1 ) − E(S) { H ( ZNk1 , · · · ,ZNk1 )} ≥ ξ } ≤ exp    −2ξ2 ∑K\nk=1\n(b−a)2w2 k\nNk\n   ,\nwhich can be equivalently rewritten as with probability at least 1− ǫ,\nH ( ZNk1 , · · · ,ZNk1 ) ≤E(S) { H ( ZNk1 , · · · ,ZNk1 )} +\n√√√√ K∑\nk=1\n(b− a)2w2k ln(1/ǫ) 2Nk\n=E(S) { sup f∈F ∣∣∣ K∑\nk=1\nwk ( E (S) Nk f − E(T )f )∣∣∣ } + √√√√ K∑\nk=1\n(b− a)2w2k ln(1/ǫ) 2Nk\n=E(S) { sup f∈F ∣∣∣ K∑\nk=1\nwk ( E (S) Nk f − E(Sk)f + E(Sk)f − E(T )f )∣∣∣ }\n+\n√√√√ K∑\nk=1\n(b− a)2w2k ln(1/ǫ) 2Nk\n≤E(S) { sup f∈F ∣∣∣ K∑\nk=1\nwk ( E (S) Nk f − E(Sk)f )∣∣∣ } + K∑\nk=1\nwk sup f∈F\n∣∣E(Sk)f − E(T )f ∣∣\n+\n√√√√ K∑\nk=1\n(b− a)2w2k ln(1/ǫ) 2Nk\n≤E(S) { sup f∈F ∣∣∣ K∑\nk=1\nwk ( E (S) Nk f − E(Sk)f )∣∣∣ } +D (w) F (S, T ) [see (26)]\n+\n√√√√ K∑\nk=1\n(b− a)2w2k ln(1/ǫ) 2Nk . (107)\nNext, according to (23) and (106), we have\nE(S) sup f∈F\n∣∣∣ K∑\nk=1\nwk ( E (S) Nk f − E(Sk)f )∣∣∣\n=E(S) sup f∈F\n∣∣∣ K∑\nk=1\nwk ( E (S) Nk f − E′(Sk){E′(S)Nk f} )∣∣∣\n≤E(S)E′(S) sup f∈F\n∣∣∣ K∑\nk=1\nwk ( E (S) Nk f − E′(S)Nk f )∣∣∣\n=E(S)E′ (S)\nsup f∈F\n∣∣∣ K∑\nk=1\nwk 1\nNk\nNk∑\nn=1\n( f(z(k)n )− f(z′(k)n ) )∣∣∣\n≤E(S)E′(S)Eσ sup f∈F\n∣∣∣ K∑\nk=1\nwk 1\nNk\nNk∑\nn=1\nσ(k)n ( f(z(k)n )− f(z′(k)n ) )∣∣∣\n≤2E(S)Eσ sup f∈F\n∣∣∣ K∑\nk=1\nwk 1\nNk\nNk∑\nn=1\nσ(k)n f(z (k) n )\n∣∣∣\n≤2E(S)Eσ K∑\nk=1\nwk 1\nNk sup f∈F\n∣∣∣ Nk∑\nn=1\nσ(k)n f(z (k) n )\n∣∣∣\n=2\nK∑\nk=1\nwkR(k)(F). (108)\nBy combining (105), (107) and (108), we obtain with probability at least 1− ǫ\nsup f∈F\n∣∣E(S) w f − E(T )f ∣∣ ≤ D(w)F (S, T ) + 2 K∑\nk=1\nwkR(k)(F) +\n√√√√ K∑\nk=1\n(b− a)2w2k ln(1/ǫ) 2Nk .\nThis completes the proof.\nC.3 Proof of Theorem 6.2\nIn order to prove Theorem 6.2, we also need the following result (see Bousquet et al., 2004, Theorem 5):\nTheorem C.3 Let F ⊆ [a, b]Z . For any ǫ > 0, with probability at least 1 − ǫ, there holds that for any f ∈ F ,\nEf ≤ENf + 2R(F) + √\n(b− a) ln(1/ǫ) 2N\n≤ENf + 2RN (F) + 3 √\n(b− a) ln(2/ǫ) 2N . (109)\nAgain, we prove Theorem 6.2 by using Theorems C.2 and C.3. Proof of Theorem 6.2 We only consider the result of Theorem C.2 in a special setting of\nK = 2, N1 = NT , N2 = NS, w1 = τ and w2 = 1− τ . Given a choice of τ ∈ [0, 1), denote\nH ( ZNS1 ,Z NT 1 ) := sup\nf∈F\n∣∣Eτf − E(T )f ∣∣. (110)\nBy (9), we have\nH ( ZNS1 ,Z NT 1 ) = sup\nf∈F\n∣∣∣τE(T )NT f + (1− τ)E (S) NS f − E(T )f ∣∣∣, (111)\nAccording to Theorem C.2, we have for any ξ > 0,\nPr { H ( ZNS1 ,Z NT 1 ) − E(S) { H ( ZNS1 ,Z NT 1 )} ≥ ξ } ≤ exp   \n−2ξ2\n(b− a)2 ( τ2\nNT + (1−τ)\n2\nNS\n)    ,\nwhich can be equivalently rewritten as with probability at least 1− (ǫ/2),\nH ( ZNS1 ,Z NT 1 )\n≤E(S) { H ( ZNS1 ,Z NT 1 )} +\n√ (b− a)2 ln(2/ǫ)\n2\n( τ 2\nNT + (1− τ)2 NS\n)\n=E(S) { sup f∈F ∣∣∣τE(T )NT f + (1− τ)E (S) NS f − E(T )f ∣∣∣ } +\n√ (b− a)2 ln(2/ǫ)\n2\n( τ 2\nNT + (1− τ)2 NS\n)\n=E(S) { sup f∈F ∣∣∣τ ( E (T ) NT f − E(T )f) + (1− τ) ( E (S) NS f − E(T )f )∣∣∣ }\n+\n√ (b− a)2 ln(2/ǫ)\n2\n( τ 2\nNT + (1− τ)2 NS\n)\n≤τ sup f∈F\n∣∣∣E(T )NT f − E (T )f ∣∣∣+ (1− τ)E(S) { sup f∈F ∣∣∣E(S)NSf − E (T )f ∣∣∣ }\n+\n√ (b− a)2 ln(2/ǫ)\n2\n( τ 2\nNT + (1− τ)2 NS\n)\n=τ sup f∈F\n∣∣∣E(T )NT f − E (T )f ∣∣∣+ (1− τ)E(S) { sup f∈F ∣∣∣E(S)NSf − E (S)f + E(S)f − E(T )f ∣∣∣ }\n+\n√ (b− a)2 ln(2/ǫ)\n2\n( τ 2\nNT + (1− τ)2 NS\n)\n≤τ sup f∈F\n∣∣∣E(T )NT f − E (T )f ∣∣∣+ (1− τ)E(S) { sup f∈F ∣∣∣E(S)NSf − E (S)f ∣∣∣ }\n+ (1− τ) sup f∈F\n∣∣∣E(S)f − E(T )f ∣∣∣+\n√ (b− a)2 ln(2/ǫ)\n2\n( τ 2\nNT + (1− τ)2 NS\n)\n=τ sup f∈F\n∣∣∣E(T )NT f − E (T )f ∣∣∣+ (1− τ)E(S) { sup f∈F ∣∣∣E(S)NSf − E (S)f ∣∣∣ } + (1− τ)DF(S, T )\n+\n√ (b− a)2 ln(2/ǫ)\n2\n( τ 2\nNT + (1− τ)2 NS\n) . (112)\nAccording to Theorem C.3, for any ǫ > 0, we have with at least 1− (ǫ/2),\nsup f∈F\n∣∣∣E(T )NT f − E (T )f ∣∣∣ ≤ 2R(T )NT (F) + 3 √\n(b− a) ln(4/ǫ) 2NT . (113)\nNext, according to (23), we have\nE(S) { sup f∈F ∣∣∣E(S)NSf − E (S)f ∣∣∣ }\n=E(S) sup f∈F\n∣∣∣E(S)NSf − E ′(S){E′(S)NSf} ∣∣∣\n≤E(S)E′(S) sup f∈F\n∣∣∣E(S)Nk f − E ′(S) Nk f ∣∣∣\n=E(S)E′ (S)\nsup f∈F ∣∣∣ 1 NS NS∑\nn=1\n( f(z(S)n )− f(z′(S)n ) )∣∣∣\n≤E(S)E′(S)Eσ sup f∈F ∣∣∣ 1 NS NS∑\nn=1\nσn ( f(z(S)n )− f(z′(S)n ) )∣∣∣\n≤2E(S)Eσ sup f∈F ∣∣∣ 1 NS NS∑\nn=1\nσnf(z (S) n ) ∣∣∣\n=2R(S)(F). (114)\nBy combining (110), (112), (113) and (114), we obtain with probability at least 1− ǫ,\nsup f∈F\n∣∣Eτf − E(T )f ∣∣ ≤(1− τ)DF (S, T ) + 2(1− τ)R(S)(F)\n+ 2τR(T )NT (F) + 3τ √ (b− a) ln(4/ǫ) 2NT + (1− τ) √\n(b− a)2 ln(2/ǫ) 2\n( τ 2\nNT + (1− τ)2 NS\n) .\nThis completes the proof."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2013,
    "abstractText" : "In this paper, we provide a new framework to obtain the generalization bounds of the<lb>learning process for domain adaptation, and then apply the derived bounds to analyze the<lb>asymptotical convergence of the learning process. Without loss of generality, we consider<lb>two kinds of representative domain adaptation: one is with multiple sources and the other<lb>is combining source and target data.<lb>In particular, we use the integral probability metric to measure the difference between<lb>two domains. For either kind of domain adaptation, we develop a related Hoeffding-type<lb>deviation inequality and a symmetrization inequality to achieve the corresponding gener-<lb>alization bound based on the uniform entropy number. We also generalized the classical<lb>McDiarmid’s inequality to a more general setting where independent random variables can<lb>take values from different domains. By using this inequality, we then obtain generaliza-<lb>tion bounds based on the Rademacher complexity. Afterwards, we analyze the asymptotic<lb>convergence and the rate of convergence of the learning process for such kind of domain<lb>adaptation. Meanwhile, we discuss the factors that affect the asymptotic behavior of the<lb>learning process and the numerical experiments support our theoretical findings as well.",
    "creator" : "LaTeX with hyperref package"
  }
}