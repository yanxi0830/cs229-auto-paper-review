{
  "name" : "1704.02712.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Adaptive Relaxed ADMM: Convergence Theory and Practical Implementation",
    "authors" : [ "Zheng Xu", "Mário A. T. Figueiredo", "Xiaoming Yuan", "Christoph Studer", "Tom Goldstein" ],
    "emails" : [ "xuzh@cs.umd.edu" ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "Modern methods in computer vision and machine learning often require solving difficult optimization problems involving non-differentiable objective functions and constraints. Some popular applications include sparse models [48, 54, 8, 36], low-rank models [47, 23, 53, 31], and support vector machines (SVMs) [4, 3]. The alternating direction method of multiplier (ADMM) is one of the most prominent optimization tools to solve such problems, and tackles problems in the following form:\nmin u∈Rn,v∈Rm h(u) + g(v), subject to Au+Bv = b. (1)\nHere, h : Rn → R and g : Rm → R are closed, proper, and convex functions, A ∈ Rp×n, B ∈ Rp×m, and b ∈ Rp. ADMM was first introduced in [16] and [12], and has found\n∗xuzh@cs.umd.edu\napplications in a variety of optimization problems in machine learning, image processing, computer vision, wireless communications, and many other areas [2, 21].\nRelaxed ADMM is a popular practical variant of ADMM, and proceeds with the following steps:\nuk+1 = arg min u h(u) + τk 2 ∥∥∥∥b−Au−Bvk + λkτk ∥∥∥∥2 (2)\nũk+1 = γkAuk+1 + (1− γk)(b−Bvk) (3)\nvk+1 = arg min v g(v) + τk 2 ∥∥∥∥b− ũk+1 −Bv + λkτk ∥∥∥∥2 (4)\nλk+1 = λk + τk(b− ũk+1 −Bvk+1). (5)\nHere, λk ∈Rp denotes the dual variables (Lagrange multipliers) on iteration k, and (τk, γk) are sequences of penalty and relaxation parameters. Relaxed ADMM coincides with the original non-relaxed version if γk = 1.\nConvergence of (relaxed) ADMM is guaranteed under fairly general assumptions [6, 25, 26, 10], if the penalty and relaxation parameters are held constant. However, the practical performance of ADMM depends strongly on the choice of these parameters, as well as on the problem being solved. Good penalty choices are known for certain ADMM formulations, such as strictly convex quadratic problems [40, 14], and for the gradient descent parameter in the “linearized” ADMM [32, 34].\nAdaptive penalty methods (in which the penalty parameters are tuned automatically as the algorithm proceeds) achieve good performance without user oversight. For nonrelaxed ADMM, the authors of [24] propose methods that modulate the penalty parameter so that the primal and dual residuals (i.e., derivatives of the Lagrangian with respect to primal and dual variables) are of approximately equal size. This “residual balancing” approach has been generalized to work with preconditioned variants of ADMM [20] and distributed ADMM [44]. In [51], a spectral penalty parameter method is proposed that uses the local curvature of the objective to achieve fast convergence. All of these methods are\n1\nar X\niv :1\n70 4.\n02 71\n2v 1\n[ cs\n.C V\n] 1\n0 A\npr 2\nspecific to (non-relaxed) vanilla ADMM, and do not apply to the more general case involving a relaxation parameter."
    }, {
      "heading" : "1.1. Overview & contributions",
      "text" : "In this paper, we study adaptive parameter choices for the relaxed ADMM that jointly and automatically tune both the penalty parameter τk and relaxation parameter γk. In Section 3, we address theoretical questions about the convergence of ADMM with non-constant penalty and relaxation parameters. In Section 4, we discuss practical methods for choosing these parameters. In Section 6, we apply the proposed ARADMM to several problems in machine learning, computer vision, and image processing. Finally, in Section 7, we compare ARADMM to other ADMM variants and examine the benefits of the proposed approach for real-world regression, classification, and image processing problems."
    }, {
      "heading" : "2. Related work",
      "text" : "Sparse and low rank methods are widely used in computer vision [48, 54, 8, 47, 23, 36, 53, 31], machine learning [7, 57, 43, 9, 33], and image processing [42, 21]. ADMM has been extensively applied to solve such problems [2, 21, 51, 50], and has recently found applications in neural networks [56, 45], tensor decomposition [18, 35, 52], structure from motion [19], and other vision problems.\nThe O(1/k) convergence rate of non-relaxed ADMM is established under mild conditions for convex problems [25, 26]. The O(1/k2) convergence rate is discussed in [17, 21, 27, 46], where at least one of the functions is assumed either strongly convex or smooth. For the general relaxed ADMM formulation, a O(1/k) convergence rate is provided under mild conditions [10]. Linear convergence can be achieved with strong convexity assumptions [5, 38, 15]. All of these results assume constant parameters—it is considerably harder to prove convergence when the algorithm parameters are adaptive.\nFixed optimal parameters are discussed in the literature. For the specific case in which the objective is quadratic, a criterion is proposed in [40, 14]. The authors of [38] suggest a grid search and semidefinite programming based method to determine the optimal relaxation and penalty parameters. These methods, however, make strong assumptions about the objective and require knowledge of condition numbers.\nAdaptive penalty methods are proposed to accelerate the practical convergence of non-relaxed ADMM [24, 51]. For the relaxation parameter, it has been suggested in [6] that over-relaxation (γ ∈ (1, 2)) may accelerate convergence and γ = 1.5 achieves faster convergence in a specific distributed computing application. The proposed ARADMM simultaneously adapts both the penalty and the relaxation parameter, thus being fully automated."
    }, {
      "heading" : "3. Convergence theory",
      "text" : "We study conditions under which ADMM converges with adaptive penalty and relaxation parameters. Our approach utilizes the variational inequality (VI) methods put forward in [24, 25, 26]. Our results measure convergence using the primal and dual “residuals,” which are defined as\nrk = b−Auk −Bvk and dk = τkATB(vk − vk−1). (6)\nIt has been observed that these residuals approach zero as the algorithm approaches a true solution [2]. Typically, the iterative process is stopped if\n‖rk‖ ≤ tol max{‖Auk‖, ‖Bvk‖, ‖b‖} and ‖dk‖ ≤ tol‖ATλk‖,\n(7)\nwhere tol > 0 is the stopping tolerance [2]. For this reason, it is important to know that the method converges in the sense that the residuals approach zero as k →∞.\nIn the sequel, we prove that relaxed ADMM converges in the residual sense, provided that the algorithm parameters satisfy one of the following two assumptions.\nAssumption 1. The relaxation sequence γk and penalty sequence τk satisfy\n1 ≤ γk < 2, lim k→∞ 1/τ2k <∞, ∞∑ k=1 η2k <∞,\nwhere η2k = γk\n(2− γk) max\n( τ2k/τ 2 k−1, 1 ) − 1. (8)\nAssumption 2. The relaxation sequence γk and penalty sequence τk satisfy\n1 ≤ γk < 2, lim k→∞ τ2k <∞, ∞∑ k=1 θ2k <∞,\nwhere θ2k = γk\n(2− γk) max\n( τ2k−1/τ 2 k , 1 ) − 1. (9)\nIn Section 5, we prove adaptive relaxed ADMM converges if the algorithm parameters satisfy either Assumption 1 or Assumption 2. Before presenting the proof, we show how to choose the relaxation parameters that lead to efficient performance in practice."
    }, {
      "heading" : "4. ARADMM: Adaptive relaxed ADMM",
      "text" : "Spectral stepsize selection methods for vanilla ADMM were discussed in [51]. Here, we modify the adaptive ADMM framework in two important ways. First, we discuss the selection of penalty parameters in the presence of the relaxation term. Second, we discuss adaptive methods also for automatically selecting the relaxation parameter.\nThe proposed method works by assuming a local linear model for the dual optimization problem, and then selecting\nan optimal stepsize under this assumption. A safeguarding method is adopted to ensure that bad stepsizes are not chosen in case these linearity assumptions fail to hold."
    }, {
      "heading" : "4.1. Dual interpretation of relaxed ADMM",
      "text" : "We derive our adaptive stepsize rules by examining the close relationship between relaxed ADMM and the relaxed Douglas-Rachford Splitting (DRS) [6, 5, 15]. The dual of the general constrained problem (1) is\nmin ζ∈Rp h∗(AT ζ)− 〈ζ, b〉︸ ︷︷ ︸ ĥ(ζ) + g∗(BT ζ)︸ ︷︷ ︸ ĝ(ζ) , (10)\nwith f∗ denoting the Fenchel conjugate of f , defined as f∗(y) = supx〈x, y〉 − f(x) [41].\nThe relaxed DRS algorithm solves (10) by generating two sequences, (ζk)k∈N and (ζ̂k)k∈N, according to\n0 ∈ ζ̂k+1 − ζk τk + ∂ĥ(ζ̂k+1) + ∂ĝ(ζk), (11) 0 ∈ζk+1 − ζk τk + γk ∂ĥ(ζ̂k+1)\n− (1− γk)∂ĝ(ζk) + ∂ĝ(ζk+1), (12)\nwhere γk is a relaxation parameter, and ∂f(x) denotes the subdifferential of f evaluated at x [41]. Referring back to ADMM in (2)–(5), and defining λ̂k+1 = λk + τk(b − Auk+1−Bvk), the sequences (λk)k∈N and (λ̂k)k∈N satisfy the same conditions (11) and (12) as (ζk)k∈N and (ζ̂k)k∈N, thus ADMM for the problem (1) is equivalent to DRS on its dual (10). A detailed proof of this is provided in the supplementary material."
    }, {
      "heading" : "4.2. Spectral adaptive stepsize rule",
      "text" : "Adaptive stepsize rules of the “spectral” type were originally proposed for simple gradient descent on smooth problems by Barzilai and Borwein [1], and have been found to dramatically outperform constant stepsizes in many applications [11, 49]. Spectral stepsize methods work by modeling the gradient of the objective as a linear function, and then selecting the optimal stepsize for this simplified linear model.\nSpectral methods were recently used to determine the penalty parameter for the non-relaxed ADMM in [51]. Inspired by that work, we derive spectral stepsize rules assuming a linear model/approximation for ∂ĥ(ζ̂) and ∂ĝ(ζ) at iteration k given by\n∂ĥ(ζ̂) = αk ζ̂ + Ψk and ∂ĝ(ζ) = βk ζ + Φk, (13)\nwhere αk > 0, βk > 0 are local curvature estimates of ĥ and ĝ, respectively, and Ψk,Φk ⊂ Rp. Once we obtain these curvature estimates, we will exploit the following simple proposition whose proof is given in the supplementary material.\nProposition 1. Suppose the DRS steps (11)–(12) are applied to problem (10), where (omitting iteration k from αk, βk,Ψk,Φk to lighten the notation in what follows)\n∂ĥ(ζ̂) = α ζ̂ + Ψ and ∂ĝ(ζ) = β ζ + Φ. (14)\nThen, the residual of ĥ(ζk+1) + ĝ(ζk+1) will be zero if τ and γ are chosen to satisfiy γk = 1 + 1+αβτ2k (α+β)τk .\nOur adaptive method works by fitting a linear model to the gradient (or subgradient) of our objective, and then using Proposition 1 to select an optimal stepsize pair that obtains zero residual on the model problem. For our convergence theory to hold, we need γ < 2. For fixed values of α and β, the minimal value of γk that is still optimal for the linear model occurs if we choose\nτk = arg min τ\n1 + αβτ2 (α+ β)τ = 1/\n√ αβ. (15)\nNote this is the same “optimal” penalty parameter proposed for non-relaxed ADMM in [51]. Under this choice of τk, we then have the “optimal” relaxation parameter\nγk = 1 + 1 + αβτ2\n(α+ β)τ = 1 +\n2 √ αβ α+ β ≤ 2. (16)"
    }, {
      "heading" : "4.3. Estimation of stepsizes",
      "text" : "We now propose a simple method for fitting a linear model to the dual objective terms so that the formulas in Section 4.2 can be used to obtain stepsizes. Once these linear models are formed, the optimal penalty parameter and relaxation term can be calculated by (15) and (16), thanks to the equivalence of relaxed ADMM and DRS.\nIn what follows, we let α̂k = 1/αk and β̂k = 1/βk to simplify notation. The optimal stepsize choice is then written as τk = (α̂k β̂k)1/2 and γk = 1 + 2 √ α̂kβ̂k\nα̂k+β̂k .\nThe estimation of α̂k and β̂k for the dual components ĥ(λ̂k) and ĝ(λk) at the k-th iteration of primal ADMM has been described in [51]. It is easy to verify that the model parameters α̂k and β̂k of relaxed ADMM can be estimated based on the results from iteration k and an older iteration k0 < k in a similar way. If we define\n∆λ̂k := λ̂k − λ̂k0 and ∆ĥk := A(uk − uk0), (17)\nthen the parameter α̂k is obtained from the formula\nα̂k =\n{ α̂MGk if 2 α̂ MG k > α̂ SD k\nα̂SDk − α̂MGk /2 otherwise, (18)\nα̂SDk = 〈∆λ̂k,∆λ̂k〉 〈∆ĥk,∆λ̂k〉 and α̂MGk = 〈∆ĥk,∆λ̂k〉 〈∆ĥk,∆ĥk〉 . (19)\nFor a detailed derivation of these formulas, see [51].\nThe spectral stepsize β̂k of ĝ(λk) is similarly estimated with ∆ĝk :=B(vk − vk0), and ∆λk := λk − λk0 . It is important to note that α̂k and β̂k are obtained from the iterates of ADMM alone, i.e., our scheme does not require the user to supply the dual problem."
    }, {
      "heading" : "4.4. Safeguarding",
      "text" : "Spectral stepsize methods for simple gradient descent are paired with a backtracking line search to guarantee convergence in case the linear model assumptions break down and an unstable stepsize is produced. ADMM methods have no analog of backtracking. Rather, we adopt the correlation criterion proposed in [51] to test the validity of the local linear assumption, and only rely on the adaptive model when the assumptions are deemed valid. To this end, we define\nαcork = 〈∆ĥk,∆λ̂k〉 ‖∆ĥk‖ ‖∆λ̂k‖ and βcork = 〈∆ĝk,∆λk〉 ‖∆ĝk‖ ‖∆λk‖ . (20)\nWhen the model assumptions (14) hold perfectly, the vectors ∆ĥk and ∆λ̂k should be highly correlated and we get αcork = 1. When αcork or β cor k is small, the model assumptions are invalid and the spectral stepsize may not be effective. The proposed method uses the following update rules\nτk+1 =  √ α̂kβ̂k if αcork > cor and βcork > cor α̂k if αcork > cor and βcork ≤ cor β̂k if αcork ≤ cor and βcork > cor\nτk otherwise,\n(21)\nγk+1 =  1 + 2 √ α̂kβ̂k α̂k+β̂k if αcork > cor and βcork > cor 1.9 if αcork > cor and βcork ≤ cor 1.1 if αcork ≤ cor and βcork > cor\n1.5 otherwise,\n(22)\nwhere cor is a quality threshold for the curvature estimates, while α̂k and β̂k are the spectral stepsizes estimated in Section 4.3. The update for τk+1 only uses model parameters that have been accurately estimated. When the model is effective for h but not g, we use a large γk = 1.9 to make the v update conservative relative to the u update. When the model is effective for g but not h, we use a small γk = 1.1 to make the v update aggressive relative to the u update."
    }, {
      "heading" : "4.5. Applying convergence guarantee",
      "text" : "Our convergence theory requires either Assumption 1 or Assumption 2 to be satisfied, which suggests that convergence is guaranteed under “bounded adaptivity” for both penalty and relaxation parameters. These conditions can be guaranteed by explicitly adding constraints to the stepsize choice in ARADMM.\nAlgorithm 1 Adaptive relaxed ADMM (ARADMM) Input: initialize v0, λ0, τ0, γ0, and k0 =0\n1: while not converge by (7) and k < maxiter do 2: Perform relaxed ADMM, as in (2)–(5) 3: if mod(k, Tf ) = 1 then 4: λ̂k+1 = λk + τk(b−Auk+1 −Bvk) 5: Compute spectral stepsizes α̂k, β̂k using (18) 6: Estimate correlations αcork , β cor k using (20) 7: Update τk+1, γk+1 using (21) and (22) 8: Bound τk+1, γk+1 using (23) 9: k0 ← k\n10: else 11: τk+1 ← τk and γk+1 ← γk 12: end if 13: k ← k + 1 14: end while\nTo guarantee convergence, we simply replace the parameter updates (21) and (22) with\nτ̂k+1 = min {τk+1, (1 + Ccg/k2) τk} γ̂k+1 = min {γk+1, 1 + Ccg/k2},\n(23)\nwhere Ccg is some (large) constant. It is easily verified that the parameter sequence (τ̂k, γ̂k) satisfies Assumption 1. In practice, the update schemes (21) and (22) converges reliably without explicitly enforcing these conditions. We use a very large Ccg such that the conditions are not triggered in the first few thousand iterations and provide these constraints for theoretical interests."
    }, {
      "heading" : "4.6. ARADMM algorithm",
      "text" : "The complete adaptive relaxed ADMM (ARADMM) is shown in Algorithm 1. We suggest only updating the stepsize every Tf = 2 iterations. We suggest a fixed safeguarding threshold cor = 0.2, which is used in all the experiments in Section 6. The overhead of the adaptive scheme is modest, requiring only a few inner product calculations."
    }, {
      "heading" : "5. Proofs of convergence theorems",
      "text" : "We now prove that relaxed ADMM converges under Assumption 1 or 2. Let\ny = ( u v ) ∈ Rn+m, z = uv λ  ∈ Rn+m+p. (24) We use yk = (uk, vk)T and zk = (uk, vk, λk)T to denote iterates, and y∗ = (u∗, v∗)T and z∗ = (u∗, v∗, λ∗)T denote optimal solutions. Set ∆z+k = (∆u + k ,∆v + k ,∆λ + k ) := zk+1 − zk, and ∆z∗k = (∆u∗k,∆v∗k,∆λ∗k) := z∗ − zk, and\ndefine\nf(y) = h(u) + g(v), F (z) =  −ATλ−BTλ Au+Bv − b . (25) Notice that F (z) is monotone, which means ∀z, z′, (z − z′)T (F (z)− F (z′)) ≥ 0.\nProblem formulation (1) can be reformulated as a variational inequality (VI). The optimal solution z∗ satisfies\n∀z, f(y)− f(y∗) + (z − z∗)TF (z∗) ≥ 0. (26)\nLikewise, the ADMM iterates produced by steps (2) and (4) satisfy the variational inequalities\n∀u, h(u)− h(uk+1) + (u− uk+1)T\n(τkA T (Auk+1 +Bvk − b)−ATλk) ≥ 0, (27)\n∀v, g(v)− g(vk+1) + (v − vk+1)T\n(τkB T (ũk+1 +Bvk+1 − b)−BTλk) ≥ 0. (28)\nUsing the definitions of y, z, f(y), and F (z) in (24, 25), λ in (5), and ũ in (3), VI (27) and (28) combine to yield\nf(y)− f(yk+1) + (z − zk+1)T ( F (zk+1) + Ω(∆z + k , τk, γk) ) ≥ 0,\nΩ(∆z+k , τk, γk) =  γk−1 γk AT∆λ+k − τk γk ATB∆v+k\n0 1\nγkτk ∆λ+k − γk−1 γk B∆v+k . (29) We then apply VI (26), (28), and (29) in order to prove the following lemmas for our contraction proof, which show that the difference between iterates decreases as the iterates approach the true solution. ‘The remaining details of the proof are in the supplementary material.\nLemma 1. The iterates zk = (uk, vk, λk)T generated by ADMM satisfy\n(B∆v+k ) T∆λ+k ≥ 0. (30)\nLemma 2. Let γk ≥ 1. The optimal solution z∗ and iterates zk generated by ADMM satisfy\n2− γk γk ‖τkB∆v+k + ∆λ + k ‖ 2\n≤γk(‖τkB∆v∗k‖2 + ‖∆λ∗k‖2) − (2− γk)(‖τkB∆v∗k+1‖2 + ‖∆λ∗k+1‖2).\n(31)"
    }, {
      "heading" : "5.1. Convergence with adaptivity",
      "text" : "We are now ready to state our main convergence results. The proof of Theorem 1 is shown here in full, and leverages Lemma 2 to produce a contraction argument. The proof of Theorem 2 is extremely similar, and is shown in the supplementary material.\nTheorem 1. Suppose Assumption 1 holds. Then, the iterates zk = (uk, vk, λk) T generated by ADMM satisfy\nlim k→∞ ‖rk‖ = 0 and lim k→∞ ‖dk‖ = 0. (32)\nProof. Assumption 1 implies γk\n2− γk τ2k ≤ (1 + η2k)τ2k−1 and γk 2− γk ≤ (1 + η2k). (33)\nIf γk < 2 as in Assumption 1, then Lemma 2 shows\n1\nγk ‖τkB∆v+k + ∆λ + k ‖ 2\n≤ γk 2− γk (τ2k‖B∆v∗k‖2 + ‖∆λ∗k‖2)\n− (τ2k‖B∆v∗k+1‖2 + ‖∆λ∗k+1‖2) (34) ≤(1 + η2k)(τ2k−1‖B∆v∗k‖2 + ‖∆λ∗k‖2) − (τ2k‖B∆v∗k+1‖2 + ‖∆λ∗k+1‖2), (35)\nwhere (33) is used to get from (34) to (35). Accumulating inequality (35) from k = 0 to N shows\nN∑ k=0 N∏ t=k+1 (1 + η2t ) 1 γk ‖τkB∆v+k + ∆λ + k ‖ 2\n≤ N∏ k=1 (1 + η2t )(τ 2 0 ‖B∆v∗0‖2 + ‖∆λ∗0‖2). (36)\nAssumption 1 also implies ∏∞ t=1(1 + η\n2 t ) < ∞, and∏N\nt=k+1(1 + η 2 t ) 1 γk ≥ 1γk > 1/2. Then, (36) indicates∑∞\nk=0 ‖τkB∆v + k + ∆λ + k ‖2 <∞, and\nlim k→∞\n‖τkB∆v+k + ∆λ + k ‖ 2 = 0. (37)\nNow, from Lemma 1, (B∆v+k ) T∆λ+k ≥ 0, and so\nlim k→∞\n‖∆λ+k ‖ 2 ≤ lim k→∞ ‖τkB∆v+k + ∆λ + k ‖ 2 = 0, (38)\nlim k→∞\n‖τkB∆v+k ‖ 2 ≤ lim k→∞ ‖τkB∆v+k + ∆λ + k ‖ 2 = 0. (39)\nThe residuals rk, dk in (6) satisfy\nrk = 1\nγkτk ∆λ+k−1 − γk − 1 γk B∆v+k−1, (40)\ndk = τkA TB∆v+k−1, (41)\nfrom which we get\nlim k→∞ ‖rk‖ ≤ lim k→∞\n1\nγkτk ‖∆λ+k−1‖\n+ γk − 1 γkτ2k−1 ‖τk−1B∆v+k−1‖ = 0, and\nlim k→∞ ‖dk‖ ≤ lim k→∞ ‖A‖‖τkB∆v+k−1‖\n≤ lim k→∞\n√ 1 + η2k‖A‖ ‖τk−1B∆v + k−1‖ = 0.\nSimilar methods can be used to prove the following about convergence under Assumption 2. The proof of the following theorem is given in the supplementary material.\nTheorem 2. Suppose Assumption 2 holds. Then, the iterates zk = (uk, vk, λk) T generated by ADMM satisfy\nlim k→∞ ‖rk‖ = 0 and lim k→∞ ‖dk‖ = 0. (42)"
    }, {
      "heading" : "6. Applications",
      "text" : "We focus on the following statistical and image processing problems involving non-differentiable objectives: linear regression with elastic net regularization (EN), low-rank least squares (LRLS), quadratic programming (QP), consensus `1-regularized logistic regression, support vector machine (SVM), total variation image restoration (TVIR), and robust principle component analysis (RPCA). We study several vision benchmark datasets such as the extended Yale B face dataset [13], MNIST digital images [29], and CIFAR10 object images1 [28]. We also use synthetic and benchmark datasets from [7, 57, 30, 43, 33, 21], which are obtained from the UCI repository and the LIBSVM page. The experimental setups for each problem are briefly described here, and the implementation details are provided in the supplementary material.\nLinear regression with EN regularization Elastic net (EN) is a modification of the `1-norm (or LASSO) regularizer that helps dealing with highly correlated variables [57, 21], and requires solving\nmin x\n1 2 ‖Dx− c‖22 + ρ1‖x‖1 + ρ2 2 ‖x‖22, (43)\nwhere ‖ · ‖1 denotes the `1-norm, D is the data matrix, c contains measurements, and x is the vector of regression coefficient.\nLow-rank least squares (LRLS) The nuclear norm (the `1-norm of the matrix singular values) is a convex surrogate for matrix rank. ADMM has been applied to solve low rank least squares problems [55, 53]\nmin X\n1 2 ‖DX − C‖2F + ρ1‖X‖∗ + ρ2 2 ‖X‖2F , (44)\nwhere ‖ · ‖∗ denotes the nuclear norm, ‖ · ‖F denotes the Frobenius norm, D ∈ Rn×m is a data matrix, C ∈ Rn×d contains measurements, and X ∈ Rm×d contains variables.\nADMM is applied by splitting the regression term and the non-differentiable regularizer composed of nuclear and Frobenius norm. LRLS has been used to formulate exemplar classifiers and discover visual subcategories [53].\n1We use the first batch of CIFAR10 that contains 10000 samples.\nSVM and QP Support vector machine (SVM) is one of the most successful binary classifiers for computer vision. The dual of the SVM is a QP problem,\nmin z\n1 2 zTQz − eT z\nsubject to cT z = 0 and 0 ≤ z ≤ C,\nwhere z is the SVM dual variable, Q is the kernel matrix, c is a vector of labels, e is a vector of ones, and C > 0 [3]. The canonical QP is also considered,\nmin x\n1 2 xTQx+ qTx subject to Dx ≤ c. (45)\nConsensus `1-regularized logistic regression ADMM has become an important tool for solving distributed optimization problems [2]. A typical problem is the consensus `1-regularized logistic regression\nmin xi,z N∑ i=1 ni∑ j=1 log(1 + exp(−cjDjxi)) + ρ‖z‖1\nsubject to xi − z = 0, i = 1, . . . , N,\n(46)\nwhere xi ∈ Rm represents the local variable on the ith distributed node, z is the global variable, ni is the number of samples in the ith block, Dj ∈ Rm is the jth sample, and cj ∈ {−1,+1} is the corresponding label. Unwrapped SVM The unwrapped formulation of SVM [22], which can be used in distributed computing environments via “transpose reduction” tricks, applies ADMM to the primal form of SVM to solve\nmin x\n1 2 ‖x‖22 + C n∑ j=1 max{1− cjDTj x, 0}, (47)\nwhere Dj ∈ Rm is the jth sample of training data, and cj ∈ {−1, 1} is the corresponding label. ADMM is applied by splitting the `2-norm regularizer and the non-differentiable hinge loss term. Total variation image denoising (TVID) Total variation image denoising is often performed by solving [42]\nmin x\n1 2 ‖x− c‖22 + ρ‖∇x‖1 (48)\nwhere c represents given noisy image, and ∇ is the discrete gradient operator, which computes differences between adjacent image pixels. ADMM is applied by splitting the `2-norm term and the non-differentiable total variation term. RPCA Robust principal component analysis (RPCA) has broad applications in computer vision and imaging [47, 37, 39]. RPCA recovers a low-rank matrix and a sparse matrix by solving\nmin Z,E ‖Z‖∗ + ρ‖E‖1 subject to Z + E = C, (49)\nwhere the nuclear norm ‖ · ‖∗ is used to obtain a low rank matrix Z, and ‖ · ‖1 is used to obtain a sparse error E."
    }, {
      "heading" : "7. Experiments",
      "text" : "The proposed AADMM is implemented as shown in Algorithm 1. We also implemented vanilla ADMM, (nonadaptive) relaxed ADMM, ADMM with residual balancing (RB), and adaptive ADMM (AADMM) for comparison.\nThe relaxation parameter for the non-adaptive relaxed ADMM is fixed at γk=1.5 as suggested in [6]. The parameters of RB and AADMM are selected as in [24, 2, 51]. The initial penalty τ0 =1/10 and initial relaxation γ0 =1 are used for all problems except the canonical QP problem, where initial parameters are set to the geometric mean of the maximum and minimum eigenvalues of matrix Q, as proposed for quadratic problems in [40].\nFor each problem, the same randomly generated initial variables v0, λ0 are used for ADMM and its variant methods. As suggested by [24, 51], the adaptivity of RB and AADMM is stopped after 1000 iterations to guarantee convergence."
    }, {
      "heading" : "7.1. Convergence results",
      "text" : "Table 1 reports the convergence speed of ADMM and its variants for the applications described in Section 6. More experimental results including the table of more test cases, the convergence curves, and visual results of image restoration and robust PCA for face decomposition are provided in the supplementary material. Relaxed ADMM often outperforms vanilla ADMM, but does not compete with adaptive methods like RB, AADMM and ARADMM. The proposed ARADMM performs best in all the test cases."
    }, {
      "heading" : "7.2. Sensitivity to initialization",
      "text" : "We study the sensitivity of the different ADMM variants to the initial penalty (τ0) and initial relaxation parameter (γ0). Fig. 1 presents iteration counts for a wide range of values of τ0, γ0, for elastic net regression with synthetic datasets. In the left and center plots we fix one of τ0, γ0 and vary the other. The number of iterations needed to convergence is plotted as the algorithm parameters vary. In the right plot, we use a grid search to find the optimal τ0 for different\n10 -5 10 -4 10 -3 10 -2 10 -1 10 0 10 1 10 2 10 3 10 4 10 510\n0\n10 1\n10 2 10 3\nInitial penalty parameter\nIte ra\ntio ns\nVanilla ADMM Relaxed ADMM Residual balance Adaptive ADMM ARADMM\n1 1.1 1.2 1.3 1.4 1.5 1.6 1.7 1.8 1.9 2 10\n0\n10 1\n10 2 10 3\nInitial relaxation parameter\nIte ra\ntio ns\nVanilla ADMM Relaxed ADMM Residual balance Adaptive ADMM ARADMM\n1 1.1 1.2 1.3 1.4 1.5 1.6 1.7 1.8 1.9 2 10\n0\n10 1\n10 2 10 3\nInitial relaxation parameter\nIte ra\ntio ns\nVanilla ADMM Relaxed ADMM Residual balance Adaptive ADMM ARADMM\nvalues of γ0. Fig. 1 (left) shows that adaptive methods are relatively stable with respect to the initial penalty τ0, while ARADMM outperforms RB and AADMM in all choices of initial τ0. Fig. 1 (middle) suggests that the relaxation γ0 is generally less important than τ0. When a bad value of τ is chosen, it is unlikely that a good choice of γ can compensate. The proposed ARADMM that jointly adjusts τ, γ is generally better than simply adding the relaxation to the existing adaptive methods RB and AADMM.\nFig. 1 (right) shows the sensitivity to γ when using a grid search to choose the optimal τ0. This optimal τ0 significantly improves the performance of vanilla ADMM and relaxed ADMM (which use the same τ0 for all iterations). Even when using the optimal stepsize for the non-adaptive methods, ARADMM is superior to or competitive with the non-adaptive methods. Note that this experiment is meant to show a best-case scenario for the non-adaptive methods; in practice the user generally has no knowledge of the optimal value of τ. Adaptive methods achieve optimal or nearoptimal performance without an expensive grid search."
    }, {
      "heading" : "7.3. Sensitivity to safeguarding",
      "text" : "Finally, Fig. 2 presents iteration counts when applying ARADMM with various safeguarding correlation thresholds cor. When cor = 0, the calculated adaptive parameters\nbased on curvature estimations are always accepted, and when cor = 1 the parameters are never changed. The proposed AADMM method is insensitive to cor and performs well for a wide range of cor ∈ [0.1, 0.4] for various applications, except for unwrapping SVM and RPCA. Though tuning such “hyper-parameters” may improve the performance of ARADMM for some applications, the fixed cor = 0.2 performs well in all our experiments (seven applications and over fifty test cases, a full list is in the supplementary material). The proposed ARADMM is fully automated and performs well without parameter tuning."
    }, {
      "heading" : "8. Conclusion",
      "text" : "We have proposed an adaptive method for jointly tuning the penalty and relaxation parameters of relaxed ADMM without user oversight. We have analyzed adaptive relaxed ADMM schemes, and provided conditions for which convergence is guaranteed. Experiments on a wide range of machine learning, computer vision, and image processing benchmarks have demonstrated that the proposed adaptive method (often significantly) outperforms other ADMM variants without user oversight or parameter tuning. The new adaptive method improves the applicability of relaxed ADMM by facilitating fully automated solvers that exhibit fast convergence and are usable by non-expert users."
    }, {
      "heading" : "Acknowledgments",
      "text" : "TG and ZX were supported by the US Office of Naval Research under grant N00014-17-1-2078 and by the US National Science Foundation (NSF) under grant CCF-1535902. MF was partially supported by the Fundação para a Ciência e Tecnologia, grant UID/EEA/5008/2013. XY was supported by the General Research Fund from Hong Kong Research Grants Council under grant HKBU-12313516. CS was supported in part by Xilinx Inc., and by the US NSF under grants ECCS-1408006, CCF-1535897, and CAREER CCF1652065."
    } ],
    "references" : [ {
      "title" : "Two-point step size gradient methods",
      "author" : [ "J. Barzilai", "J. Borwein" ],
      "venue" : "IMA J. Num. Analysis, 8:141–148,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 1988
    }, {
      "title" : "Distributed optimization and statistical learning via the alternating direction method of multipliers",
      "author" : [ "S. Boyd", "N. Parikh", "E. Chu", "B. Peleato", "J. Eckstein" ],
      "venue" : "Found. and Trends in Mach. Learning, 3:1–122,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "LIBSVM: a library for support vector machines",
      "author" : [ "C.-C. Chang", "C.-J. Lin" ],
      "venue" : "ACM Transactions on Intelligent Systems and Technology (TIST), 2(3):27,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Support-vector networks",
      "author" : [ "C. Cortes", "V. Vapnik" ],
      "venue" : "Machine learning, 20(3):273–297,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 1995
    }, {
      "title" : "Faster convergence rates of relaxed Peaceman-Rachford and ADMM under regularity assumptions",
      "author" : [ "D. Davis", "W. Yin" ],
      "venue" : "arXiv preprint arXiv:1407.5210,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "On the Douglas-Rachford splitting method and the proximal point algorithm for maximal monotone operators",
      "author" : [ "J. Eckstein", "D. Bertsekas" ],
      "venue" : "Mathematical Programming, 55(1- 3):293–318,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 1992
    }, {
      "title" : "Least angle regression",
      "author" : [ "B. Efron", "T. Hastie", "I. Johnstone", "R. Tibshirani" ],
      "venue" : "The Annals of statistics, 32(2):407–499,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Sparse subspace clustering",
      "author" : [ "E. Elhamifar", "R. Vidal" ],
      "venue" : "Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on, pages 2790–2797. IEEE,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Liblinear: A library for large linear classification",
      "author" : [ "R.-E. Fan", "K.-W. Chang", "C.-J. Hsieh", "X.-R. Wang", "C.- J. Lin" ],
      "venue" : "Journal of machine learning research,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2008
    }, {
      "title" : "Generalized alternating direction method of multipliers: new theoretical insights and applications",
      "author" : [ "E.X. Fang", "B. He", "H. Liu", "X. Yuan" ],
      "venue" : "Mathematical Programming Computation, 7(2):149–187,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "On the Barzilai-Borwein method",
      "author" : [ "R. Fletcher" ],
      "venue" : "Optimization and control with applications, pages 235–256. Springer,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "A dual algorithm for the solution of nonlinear variational problems via finite element approximation",
      "author" : [ "D. Gabay", "B. Mercier" ],
      "venue" : "Computers & Mathematics with Applications, 2(1):17–40,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 1976
    }, {
      "title" : "From few to many: Illumination cone models for face recognition under variable lighting and pose",
      "author" : [ "A.S. Georghiades", "P.N. Belhumeur", "D.J. Kriegman" ],
      "venue" : "IEEE transactions on pattern analysis and machine intelligence, 23(6):643–660,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Optimal parameter selection for the alternating direction method of multipliers: quadratic problems",
      "author" : [ "E. Ghadimi", "A. Teixeira", "I. Shames", "M. Johansson" ],
      "venue" : "IEEE Trans. Autom. Control, 60:644–658,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Linear convergence and metric selection in Douglas-Rachford splitting and ADMM",
      "author" : [ "P. Giselsson", "S. Boyd" ],
      "venue" : null,
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2016
    }, {
      "title" : "Sur l’approximation, par éléments finis d’ordre un, et la résolution, par pénalisationdualité d’une classe de problémes de Dirichlet non linéaires",
      "author" : [ "R. Glowinski", "A. Marroco" ],
      "venue" : "ESAIM: Modélisation Mathématique et Analyse Numérique, 9:41–76,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 1975
    }, {
      "title" : "Fast alternating linearization methods for minimizing the sum of two convex functions",
      "author" : [ "D. Goldfarb", "S. Ma", "K. Scheinberg" ],
      "venue" : "Mathematical Programming, 141(1-2):349–382,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Robust low-rank tensor recovery: Models and algorithms",
      "author" : [ "D. Goldfarb", "Z. Qin" ],
      "venue" : "SIAM Journal on Matrix Analysis and Applications, 35(1):225–253,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Shapefit and shapekick for robust, scalable structure from motion",
      "author" : [ "T. Goldstein", "P. Hand", "C. Lee", "V. Voroninski", "S. Soatto" ],
      "venue" : "European Conference on Computer Vision, pages 289–304. Springer,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Adaptive primal-dual splitting methods for statistical learning and image processing",
      "author" : [ "T. Goldstein", "M. Li", "X. Yuan" ],
      "venue" : "Advances in Neural Information Processing Systems, pages 2080–2088,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Fast alternating direction optimization methods",
      "author" : [ "T. Goldstein", "B. O’Donoghue", "S. Setzer", "R. Baraniuk" ],
      "venue" : "SIAM Journal on Imaging Sciences,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2014
    }, {
      "title" : "Unwrapping ADMM: efficient distributed computing via transpose reduction",
      "author" : [ "T. Goldstein", "G. Taylor", "K. Barabin", "K. Sayre" ],
      "venue" : "AISTATS,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Large-scale image classification with trace-norm regularization",
      "author" : [ "Z. Harchaoui", "M. Douze", "M. Paulin", "M. Dudik", "J. Malick" ],
      "venue" : "Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on, pages 3386–3393. IEEE,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Alternating direction method with self-adaptive penalty parameters for monotone variational inequalities",
      "author" : [ "B. He", "H. Yang", "S. Wang" ],
      "venue" : "Jour. Optim. Theory and Appl., 106(2):337– 356,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "On the o(1/n) convergence rate of the Douglas-Rachford alternating direction method",
      "author" : [ "B. He", "X. Yuan" ],
      "venue" : "SIAM Journal on Numerical Analysis, 50(2):700–709,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "On non-ergodic convergence rate of Douglas-Rachford alternating direction method of multipliers",
      "author" : [ "B. He", "X. Yuan" ],
      "venue" : "Numerische Mathematik, 130:567–577,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Accelerated alternating direction method of multipliers",
      "author" : [ "M. Kadkhodaie", "K. Christakopoulou", "M. Sanjabi", "A. Banerjee" ],
      "venue" : "ACM SIGKDD, pages 497–506,",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Learning multiple layers of features from tiny images",
      "author" : [ "A. Krizhevsky", "G. Hinton" ],
      "venue" : null,
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 2009
    }, {
      "title" : "Gradientbased learning applied to document recognition",
      "author" : [ "Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner" ],
      "venue" : "Proceedings of the IEEE, 86(11):2278–2324,",
      "citeRegEx" : "29",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "Efficient L1 regularized logistic regression",
      "author" : [ "S.-I. Lee", "H. Lee", "P. Abbeel", "A. Ng" ],
      "venue" : "AAAI, volume 21, page 401,",
      "citeRegEx" : "30",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Domain generalization and adaptation using low rank exemplar svms",
      "author" : [ "W. Li", "Z. Xu", "D. Xu", "D. Dai", "L.V. Gool" ],
      "venue" : "IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI),",
      "citeRegEx" : "31",
      "shortCiteRegEx" : null,
      "year" : 2017
    }, {
      "title" : "Linearized alternating direction method with adaptive penalty for low-rank representation",
      "author" : [ "Z. Lin", "R. Liu", "Z. Su" ],
      "venue" : "NIPS, pages 612–620,",
      "citeRegEx" : "32",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Large-scale sparse logistic regression",
      "author" : [ "J. Liu", "J. Chen", "J. Ye" ],
      "venue" : "ACM SIGKDD, pages 547–556,",
      "citeRegEx" : "33",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Linearized alternating direction method with parallel splitting and adaptive penalty for separable convex programs in machine learning",
      "author" : [ "R. Liu", "Z. Lin", "Z. Su" ],
      "venue" : "ACML, pages 116–132,",
      "citeRegEx" : "34",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Tensor robust principal component analysis: Exact recovery of corrupted low-rank tensors via convex optimization",
      "author" : [ "C. Lu", "J. Feng", "Y. Chen", "W. Liu", "Z. Lin", "S. Yan" ],
      "venue" : "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
      "citeRegEx" : "35",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Sparse modeling for image and vision processing",
      "author" : [ "J. Mairal", "F. Bach", "J. Ponce" ],
      "venue" : "Foundations and Trends R  © in Computer Graphics and Vision, 8(2-3):85–283,",
      "citeRegEx" : "36",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Informative feature selection for object recognition via sparse PCA",
      "author" : [ "N. Naikal", "A.Y. Yang", "S.S. Sastry" ],
      "venue" : "2011 International Conference on Computer Vision, pages 818– 825. IEEE,",
      "citeRegEx" : "37",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "A general analysis of the convergence of ADMM",
      "author" : [ "R. Nishihara", "L. Lessard", "B. Recht", "A. Packard", "M. Jordan" ],
      "venue" : "ICML,",
      "citeRegEx" : "38",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Robust camera location estimation by convex programming",
      "author" : [ "O. Ozyesil", "A. Singer" ],
      "venue" : "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2674–2683,",
      "citeRegEx" : "39",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Alternating direction method of multipliers for strictly convex quadratic programs: Optimal parameter selection",
      "author" : [ "A. Raghunathan", "S. Di Cairano" ],
      "venue" : "American Control Conf., pages 4324–4329,",
      "citeRegEx" : "40",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Convex Analysis",
      "author" : [ "R. Rockafellar" ],
      "venue" : "Princeton University Press,",
      "citeRegEx" : "41",
      "shortCiteRegEx" : null,
      "year" : 1970
    }, {
      "title" : "Nonlinear total variation based noise removal algorithms",
      "author" : [ "L.I. Rudin", "S. Osher", "E. Fatemi" ],
      "venue" : "Physica D: Nonlinear Phenomena, 60(1):259–268,",
      "citeRegEx" : "42",
      "shortCiteRegEx" : null,
      "year" : 1992
    }, {
      "title" : "Fast optimization methods for l1 regularization: A comparative study and two new approaches",
      "author" : [ "M. Schmidt", "G. Fung", "R. Rosales" ],
      "venue" : "ECML, pages 286–297. Springer,",
      "citeRegEx" : "43",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Fast ADMM algorithm for distributed optimization with adaptive penalty",
      "author" : [ "C. Song", "S. Yoon", "V. Pavlovic" ],
      "venue" : "arXiv preprint arXiv:1506.08928,",
      "citeRegEx" : "44",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Training neural networks without gradients: A scalable ADMM approach",
      "author" : [ "G. Taylor", "R. Burmeister", "Z. Xu", "B. Singh", "A. Patel", "T. Goldstein" ],
      "venue" : "ICML,",
      "citeRegEx" : "45",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Faster alternating direction method of multipliers with a worst-case o (1/n) convergence rate. 2016",
      "author" : [ "W. Tian", "X. Yuan" ],
      "venue" : null,
      "citeRegEx" : "46",
      "shortCiteRegEx" : "46",
      "year" : 2016
    }, {
      "title" : "Robust principal component analysis: Exact recovery of corrupted low-rank matrices via convex optimization",
      "author" : [ "J. Wright", "A. Ganesh", "S. Rao", "Y. Peng", "Y. Ma" ],
      "venue" : "Advances in neural information processing systems, pages 2080–2088,",
      "citeRegEx" : "47",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Robust face recognition via sparse representation",
      "author" : [ "J. Wright", "A. Yang", "A. Ganesh", "S. Sastry", "Y. Ma" ],
      "venue" : "IEEE Trans. Pattern Analysis and Machine Intelligence, 31:210–227,",
      "citeRegEx" : "48",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Sparse reconstruction by separable approximation",
      "author" : [ "S. Wright", "R. Nowak", "M. Figueiredo" ],
      "venue" : "IEEE Trans. Signal Processing, 57:2479–2493,",
      "citeRegEx" : "49",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "An empirical study of ADMM for nonconvex problems",
      "author" : [ "Z. Xu", "S. De", "M.A.T. Figueiredo", "C. Studer", "T. Goldstein" ],
      "venue" : "NIPS workshop on nonconvex optimization,",
      "citeRegEx" : "50",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Adaptive ADMM with spectral penalty parameter selection",
      "author" : [ "Z. Xu", "M.A. Figueiredo", "T. Goldstein" ],
      "venue" : "AISTATS,",
      "citeRegEx" : "51",
      "shortCiteRegEx" : null,
      "year" : 2017
    }, {
      "title" : "Non-negative factorization of the occurrence tensor from financial contracts",
      "author" : [ "Z. Xu", "F. Huang", "L. Raschid", "T. Goldstein" ],
      "venue" : "NIPS workshop on tensor methods,",
      "citeRegEx" : "52",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Exploiting low-rank structure for discriminative sub-categorization",
      "author" : [ "Z. Xu", "X. Li", "K. Yang", "T. Goldstein" ],
      "venue" : "BMVC, Swansea, UK, September 7-10, 2015,",
      "citeRegEx" : "53",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Linear spatial pyramid matching using sparse coding for image classification",
      "author" : [ "J. Yang", "K. Yu", "Y. Gong", "T. Huang" ],
      "venue" : "Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on, pages 1794–1801. IEEE,",
      "citeRegEx" : "54",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Linearized augmented lagrangian and alternating direction methods for nuclear norm minimization",
      "author" : [ "J. Yang", "X. Yuan" ],
      "venue" : "Mathematics of Computation, 82(281):301–329,",
      "citeRegEx" : "55",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Efficient training of very deep neural networks for supervised hashing",
      "author" : [ "Z. Zhang", "Y. Chen", "V. Saligrama" ],
      "venue" : "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1487–1495,",
      "citeRegEx" : "56",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Regularization and variable selection via the elastic net",
      "author" : [ "H. Zou", "T. Hastie" ],
      "venue" : "Journal of the Royal Statistical Society: Series B (Statistical Methodology), 67(2):301–320,",
      "citeRegEx" : "57",
      "shortCiteRegEx" : null,
      "year" : 2005
    } ],
    "referenceMentions" : [ {
      "referenceID" : 47,
      "context" : "Some popular applications include sparse models [48, 54, 8, 36], low-rank models [47, 23, 53, 31], and support vector machines (SVMs) [4, 3].",
      "startOffset" : 48,
      "endOffset" : 63
    }, {
      "referenceID" : 53,
      "context" : "Some popular applications include sparse models [48, 54, 8, 36], low-rank models [47, 23, 53, 31], and support vector machines (SVMs) [4, 3].",
      "startOffset" : 48,
      "endOffset" : 63
    }, {
      "referenceID" : 7,
      "context" : "Some popular applications include sparse models [48, 54, 8, 36], low-rank models [47, 23, 53, 31], and support vector machines (SVMs) [4, 3].",
      "startOffset" : 48,
      "endOffset" : 63
    }, {
      "referenceID" : 35,
      "context" : "Some popular applications include sparse models [48, 54, 8, 36], low-rank models [47, 23, 53, 31], and support vector machines (SVMs) [4, 3].",
      "startOffset" : 48,
      "endOffset" : 63
    }, {
      "referenceID" : 46,
      "context" : "Some popular applications include sparse models [48, 54, 8, 36], low-rank models [47, 23, 53, 31], and support vector machines (SVMs) [4, 3].",
      "startOffset" : 81,
      "endOffset" : 97
    }, {
      "referenceID" : 22,
      "context" : "Some popular applications include sparse models [48, 54, 8, 36], low-rank models [47, 23, 53, 31], and support vector machines (SVMs) [4, 3].",
      "startOffset" : 81,
      "endOffset" : 97
    }, {
      "referenceID" : 52,
      "context" : "Some popular applications include sparse models [48, 54, 8, 36], low-rank models [47, 23, 53, 31], and support vector machines (SVMs) [4, 3].",
      "startOffset" : 81,
      "endOffset" : 97
    }, {
      "referenceID" : 30,
      "context" : "Some popular applications include sparse models [48, 54, 8, 36], low-rank models [47, 23, 53, 31], and support vector machines (SVMs) [4, 3].",
      "startOffset" : 81,
      "endOffset" : 97
    }, {
      "referenceID" : 3,
      "context" : "Some popular applications include sparse models [48, 54, 8, 36], low-rank models [47, 23, 53, 31], and support vector machines (SVMs) [4, 3].",
      "startOffset" : 134,
      "endOffset" : 140
    }, {
      "referenceID" : 2,
      "context" : "Some popular applications include sparse models [48, 54, 8, 36], low-rank models [47, 23, 53, 31], and support vector machines (SVMs) [4, 3].",
      "startOffset" : 134,
      "endOffset" : 140
    }, {
      "referenceID" : 15,
      "context" : "ADMM was first introduced in [16] and [12], and has found",
      "startOffset" : 29,
      "endOffset" : 33
    }, {
      "referenceID" : 11,
      "context" : "ADMM was first introduced in [16] and [12], and has found",
      "startOffset" : 38,
      "endOffset" : 42
    }, {
      "referenceID" : 1,
      "context" : "edu applications in a variety of optimization problems in machine learning, image processing, computer vision, wireless communications, and many other areas [2, 21].",
      "startOffset" : 157,
      "endOffset" : 164
    }, {
      "referenceID" : 20,
      "context" : "edu applications in a variety of optimization problems in machine learning, image processing, computer vision, wireless communications, and many other areas [2, 21].",
      "startOffset" : 157,
      "endOffset" : 164
    }, {
      "referenceID" : 5,
      "context" : "Convergence of (relaxed) ADMM is guaranteed under fairly general assumptions [6, 25, 26, 10], if the penalty and relaxation parameters are held constant.",
      "startOffset" : 77,
      "endOffset" : 92
    }, {
      "referenceID" : 24,
      "context" : "Convergence of (relaxed) ADMM is guaranteed under fairly general assumptions [6, 25, 26, 10], if the penalty and relaxation parameters are held constant.",
      "startOffset" : 77,
      "endOffset" : 92
    }, {
      "referenceID" : 25,
      "context" : "Convergence of (relaxed) ADMM is guaranteed under fairly general assumptions [6, 25, 26, 10], if the penalty and relaxation parameters are held constant.",
      "startOffset" : 77,
      "endOffset" : 92
    }, {
      "referenceID" : 9,
      "context" : "Convergence of (relaxed) ADMM is guaranteed under fairly general assumptions [6, 25, 26, 10], if the penalty and relaxation parameters are held constant.",
      "startOffset" : 77,
      "endOffset" : 92
    }, {
      "referenceID" : 39,
      "context" : "Good penalty choices are known for certain ADMM formulations, such as strictly convex quadratic problems [40, 14], and for the gradient descent parameter in the “linearized” ADMM [32, 34].",
      "startOffset" : 105,
      "endOffset" : 113
    }, {
      "referenceID" : 13,
      "context" : "Good penalty choices are known for certain ADMM formulations, such as strictly convex quadratic problems [40, 14], and for the gradient descent parameter in the “linearized” ADMM [32, 34].",
      "startOffset" : 105,
      "endOffset" : 113
    }, {
      "referenceID" : 31,
      "context" : "Good penalty choices are known for certain ADMM formulations, such as strictly convex quadratic problems [40, 14], and for the gradient descent parameter in the “linearized” ADMM [32, 34].",
      "startOffset" : 179,
      "endOffset" : 187
    }, {
      "referenceID" : 33,
      "context" : "Good penalty choices are known for certain ADMM formulations, such as strictly convex quadratic problems [40, 14], and for the gradient descent parameter in the “linearized” ADMM [32, 34].",
      "startOffset" : 179,
      "endOffset" : 187
    }, {
      "referenceID" : 23,
      "context" : "For nonrelaxed ADMM, the authors of [24] propose methods that modulate the penalty parameter so that the primal and dual residuals (i.",
      "startOffset" : 36,
      "endOffset" : 40
    }, {
      "referenceID" : 19,
      "context" : "This “residual balancing” approach has been generalized to work with preconditioned variants of ADMM [20] and distributed ADMM [44].",
      "startOffset" : 101,
      "endOffset" : 105
    }, {
      "referenceID" : 43,
      "context" : "This “residual balancing” approach has been generalized to work with preconditioned variants of ADMM [20] and distributed ADMM [44].",
      "startOffset" : 127,
      "endOffset" : 131
    }, {
      "referenceID" : 50,
      "context" : "In [51], a spectral penalty parameter method is proposed that uses the local curvature of the objective to achieve fast convergence.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 47,
      "context" : "Sparse and low rank methods are widely used in computer vision [48, 54, 8, 47, 23, 36, 53, 31], machine learning [7, 57, 43, 9, 33], and image processing [42, 21].",
      "startOffset" : 63,
      "endOffset" : 94
    }, {
      "referenceID" : 53,
      "context" : "Sparse and low rank methods are widely used in computer vision [48, 54, 8, 47, 23, 36, 53, 31], machine learning [7, 57, 43, 9, 33], and image processing [42, 21].",
      "startOffset" : 63,
      "endOffset" : 94
    }, {
      "referenceID" : 7,
      "context" : "Sparse and low rank methods are widely used in computer vision [48, 54, 8, 47, 23, 36, 53, 31], machine learning [7, 57, 43, 9, 33], and image processing [42, 21].",
      "startOffset" : 63,
      "endOffset" : 94
    }, {
      "referenceID" : 46,
      "context" : "Sparse and low rank methods are widely used in computer vision [48, 54, 8, 47, 23, 36, 53, 31], machine learning [7, 57, 43, 9, 33], and image processing [42, 21].",
      "startOffset" : 63,
      "endOffset" : 94
    }, {
      "referenceID" : 22,
      "context" : "Sparse and low rank methods are widely used in computer vision [48, 54, 8, 47, 23, 36, 53, 31], machine learning [7, 57, 43, 9, 33], and image processing [42, 21].",
      "startOffset" : 63,
      "endOffset" : 94
    }, {
      "referenceID" : 35,
      "context" : "Sparse and low rank methods are widely used in computer vision [48, 54, 8, 47, 23, 36, 53, 31], machine learning [7, 57, 43, 9, 33], and image processing [42, 21].",
      "startOffset" : 63,
      "endOffset" : 94
    }, {
      "referenceID" : 52,
      "context" : "Sparse and low rank methods are widely used in computer vision [48, 54, 8, 47, 23, 36, 53, 31], machine learning [7, 57, 43, 9, 33], and image processing [42, 21].",
      "startOffset" : 63,
      "endOffset" : 94
    }, {
      "referenceID" : 30,
      "context" : "Sparse and low rank methods are widely used in computer vision [48, 54, 8, 47, 23, 36, 53, 31], machine learning [7, 57, 43, 9, 33], and image processing [42, 21].",
      "startOffset" : 63,
      "endOffset" : 94
    }, {
      "referenceID" : 6,
      "context" : "Sparse and low rank methods are widely used in computer vision [48, 54, 8, 47, 23, 36, 53, 31], machine learning [7, 57, 43, 9, 33], and image processing [42, 21].",
      "startOffset" : 113,
      "endOffset" : 131
    }, {
      "referenceID" : 56,
      "context" : "Sparse and low rank methods are widely used in computer vision [48, 54, 8, 47, 23, 36, 53, 31], machine learning [7, 57, 43, 9, 33], and image processing [42, 21].",
      "startOffset" : 113,
      "endOffset" : 131
    }, {
      "referenceID" : 42,
      "context" : "Sparse and low rank methods are widely used in computer vision [48, 54, 8, 47, 23, 36, 53, 31], machine learning [7, 57, 43, 9, 33], and image processing [42, 21].",
      "startOffset" : 113,
      "endOffset" : 131
    }, {
      "referenceID" : 8,
      "context" : "Sparse and low rank methods are widely used in computer vision [48, 54, 8, 47, 23, 36, 53, 31], machine learning [7, 57, 43, 9, 33], and image processing [42, 21].",
      "startOffset" : 113,
      "endOffset" : 131
    }, {
      "referenceID" : 32,
      "context" : "Sparse and low rank methods are widely used in computer vision [48, 54, 8, 47, 23, 36, 53, 31], machine learning [7, 57, 43, 9, 33], and image processing [42, 21].",
      "startOffset" : 113,
      "endOffset" : 131
    }, {
      "referenceID" : 41,
      "context" : "Sparse and low rank methods are widely used in computer vision [48, 54, 8, 47, 23, 36, 53, 31], machine learning [7, 57, 43, 9, 33], and image processing [42, 21].",
      "startOffset" : 154,
      "endOffset" : 162
    }, {
      "referenceID" : 20,
      "context" : "Sparse and low rank methods are widely used in computer vision [48, 54, 8, 47, 23, 36, 53, 31], machine learning [7, 57, 43, 9, 33], and image processing [42, 21].",
      "startOffset" : 154,
      "endOffset" : 162
    }, {
      "referenceID" : 1,
      "context" : "ADMM has been extensively applied to solve such problems [2, 21, 51, 50], and has recently found applications in neural networks [56, 45], tensor decomposition [18, 35, 52], structure from motion [19], and other vision problems.",
      "startOffset" : 57,
      "endOffset" : 72
    }, {
      "referenceID" : 20,
      "context" : "ADMM has been extensively applied to solve such problems [2, 21, 51, 50], and has recently found applications in neural networks [56, 45], tensor decomposition [18, 35, 52], structure from motion [19], and other vision problems.",
      "startOffset" : 57,
      "endOffset" : 72
    }, {
      "referenceID" : 50,
      "context" : "ADMM has been extensively applied to solve such problems [2, 21, 51, 50], and has recently found applications in neural networks [56, 45], tensor decomposition [18, 35, 52], structure from motion [19], and other vision problems.",
      "startOffset" : 57,
      "endOffset" : 72
    }, {
      "referenceID" : 49,
      "context" : "ADMM has been extensively applied to solve such problems [2, 21, 51, 50], and has recently found applications in neural networks [56, 45], tensor decomposition [18, 35, 52], structure from motion [19], and other vision problems.",
      "startOffset" : 57,
      "endOffset" : 72
    }, {
      "referenceID" : 55,
      "context" : "ADMM has been extensively applied to solve such problems [2, 21, 51, 50], and has recently found applications in neural networks [56, 45], tensor decomposition [18, 35, 52], structure from motion [19], and other vision problems.",
      "startOffset" : 129,
      "endOffset" : 137
    }, {
      "referenceID" : 44,
      "context" : "ADMM has been extensively applied to solve such problems [2, 21, 51, 50], and has recently found applications in neural networks [56, 45], tensor decomposition [18, 35, 52], structure from motion [19], and other vision problems.",
      "startOffset" : 129,
      "endOffset" : 137
    }, {
      "referenceID" : 17,
      "context" : "ADMM has been extensively applied to solve such problems [2, 21, 51, 50], and has recently found applications in neural networks [56, 45], tensor decomposition [18, 35, 52], structure from motion [19], and other vision problems.",
      "startOffset" : 160,
      "endOffset" : 172
    }, {
      "referenceID" : 34,
      "context" : "ADMM has been extensively applied to solve such problems [2, 21, 51, 50], and has recently found applications in neural networks [56, 45], tensor decomposition [18, 35, 52], structure from motion [19], and other vision problems.",
      "startOffset" : 160,
      "endOffset" : 172
    }, {
      "referenceID" : 51,
      "context" : "ADMM has been extensively applied to solve such problems [2, 21, 51, 50], and has recently found applications in neural networks [56, 45], tensor decomposition [18, 35, 52], structure from motion [19], and other vision problems.",
      "startOffset" : 160,
      "endOffset" : 172
    }, {
      "referenceID" : 18,
      "context" : "ADMM has been extensively applied to solve such problems [2, 21, 51, 50], and has recently found applications in neural networks [56, 45], tensor decomposition [18, 35, 52], structure from motion [19], and other vision problems.",
      "startOffset" : 196,
      "endOffset" : 200
    }, {
      "referenceID" : 24,
      "context" : "The O(1/k) convergence rate of non-relaxed ADMM is established under mild conditions for convex problems [25, 26].",
      "startOffset" : 105,
      "endOffset" : 113
    }, {
      "referenceID" : 25,
      "context" : "The O(1/k) convergence rate of non-relaxed ADMM is established under mild conditions for convex problems [25, 26].",
      "startOffset" : 105,
      "endOffset" : 113
    }, {
      "referenceID" : 16,
      "context" : "The O(1/k) convergence rate is discussed in [17, 21, 27, 46], where at least one of the functions is assumed either strongly convex or smooth.",
      "startOffset" : 44,
      "endOffset" : 60
    }, {
      "referenceID" : 20,
      "context" : "The O(1/k) convergence rate is discussed in [17, 21, 27, 46], where at least one of the functions is assumed either strongly convex or smooth.",
      "startOffset" : 44,
      "endOffset" : 60
    }, {
      "referenceID" : 26,
      "context" : "The O(1/k) convergence rate is discussed in [17, 21, 27, 46], where at least one of the functions is assumed either strongly convex or smooth.",
      "startOffset" : 44,
      "endOffset" : 60
    }, {
      "referenceID" : 45,
      "context" : "The O(1/k) convergence rate is discussed in [17, 21, 27, 46], where at least one of the functions is assumed either strongly convex or smooth.",
      "startOffset" : 44,
      "endOffset" : 60
    }, {
      "referenceID" : 9,
      "context" : "For the general relaxed ADMM formulation, a O(1/k) convergence rate is provided under mild conditions [10].",
      "startOffset" : 102,
      "endOffset" : 106
    }, {
      "referenceID" : 4,
      "context" : "Linear convergence can be achieved with strong convexity assumptions [5, 38, 15].",
      "startOffset" : 69,
      "endOffset" : 80
    }, {
      "referenceID" : 37,
      "context" : "Linear convergence can be achieved with strong convexity assumptions [5, 38, 15].",
      "startOffset" : 69,
      "endOffset" : 80
    }, {
      "referenceID" : 14,
      "context" : "Linear convergence can be achieved with strong convexity assumptions [5, 38, 15].",
      "startOffset" : 69,
      "endOffset" : 80
    }, {
      "referenceID" : 39,
      "context" : "For the specific case in which the objective is quadratic, a criterion is proposed in [40, 14].",
      "startOffset" : 86,
      "endOffset" : 94
    }, {
      "referenceID" : 13,
      "context" : "For the specific case in which the objective is quadratic, a criterion is proposed in [40, 14].",
      "startOffset" : 86,
      "endOffset" : 94
    }, {
      "referenceID" : 37,
      "context" : "The authors of [38] suggest a grid search and semidefinite programming based method to determine the optimal relaxation and penalty parameters.",
      "startOffset" : 15,
      "endOffset" : 19
    }, {
      "referenceID" : 23,
      "context" : "Adaptive penalty methods are proposed to accelerate the practical convergence of non-relaxed ADMM [24, 51].",
      "startOffset" : 98,
      "endOffset" : 106
    }, {
      "referenceID" : 50,
      "context" : "Adaptive penalty methods are proposed to accelerate the practical convergence of non-relaxed ADMM [24, 51].",
      "startOffset" : 98,
      "endOffset" : 106
    }, {
      "referenceID" : 5,
      "context" : "For the relaxation parameter, it has been suggested in [6] that over-relaxation (γ ∈ (1, 2)) may accelerate convergence and γ = 1.",
      "startOffset" : 55,
      "endOffset" : 58
    }, {
      "referenceID" : 23,
      "context" : "Our approach utilizes the variational inequality (VI) methods put forward in [24, 25, 26].",
      "startOffset" : 77,
      "endOffset" : 89
    }, {
      "referenceID" : 24,
      "context" : "Our approach utilizes the variational inequality (VI) methods put forward in [24, 25, 26].",
      "startOffset" : 77,
      "endOffset" : 89
    }, {
      "referenceID" : 25,
      "context" : "Our approach utilizes the variational inequality (VI) methods put forward in [24, 25, 26].",
      "startOffset" : 77,
      "endOffset" : 89
    }, {
      "referenceID" : 1,
      "context" : "It has been observed that these residuals approach zero as the algorithm approaches a true solution [2].",
      "startOffset" : 100,
      "endOffset" : 103
    }, {
      "referenceID" : 1,
      "context" : "where tol > 0 is the stopping tolerance [2].",
      "startOffset" : 40,
      "endOffset" : 43
    }, {
      "referenceID" : 50,
      "context" : "Spectral stepsize selection methods for vanilla ADMM were discussed in [51].",
      "startOffset" : 71,
      "endOffset" : 75
    }, {
      "referenceID" : 5,
      "context" : "We derive our adaptive stepsize rules by examining the close relationship between relaxed ADMM and the relaxed Douglas-Rachford Splitting (DRS) [6, 5, 15].",
      "startOffset" : 144,
      "endOffset" : 154
    }, {
      "referenceID" : 4,
      "context" : "We derive our adaptive stepsize rules by examining the close relationship between relaxed ADMM and the relaxed Douglas-Rachford Splitting (DRS) [6, 5, 15].",
      "startOffset" : 144,
      "endOffset" : 154
    }, {
      "referenceID" : 14,
      "context" : "We derive our adaptive stepsize rules by examining the close relationship between relaxed ADMM and the relaxed Douglas-Rachford Splitting (DRS) [6, 5, 15].",
      "startOffset" : 144,
      "endOffset" : 154
    }, {
      "referenceID" : 40,
      "context" : "with f∗ denoting the Fenchel conjugate of f , defined as f∗(y) = supx〈x, y〉 − f(x) [41].",
      "startOffset" : 83,
      "endOffset" : 87
    }, {
      "referenceID" : 40,
      "context" : "where γk is a relaxation parameter, and ∂f(x) denotes the subdifferential of f evaluated at x [41].",
      "startOffset" : 94,
      "endOffset" : 98
    }, {
      "referenceID" : 0,
      "context" : "Adaptive stepsize rules of the “spectral” type were originally proposed for simple gradient descent on smooth problems by Barzilai and Borwein [1], and have been found to dramatically outperform constant stepsizes in many applications [11, 49].",
      "startOffset" : 143,
      "endOffset" : 146
    }, {
      "referenceID" : 10,
      "context" : "Adaptive stepsize rules of the “spectral” type were originally proposed for simple gradient descent on smooth problems by Barzilai and Borwein [1], and have been found to dramatically outperform constant stepsizes in many applications [11, 49].",
      "startOffset" : 235,
      "endOffset" : 243
    }, {
      "referenceID" : 48,
      "context" : "Adaptive stepsize rules of the “spectral” type were originally proposed for simple gradient descent on smooth problems by Barzilai and Borwein [1], and have been found to dramatically outperform constant stepsizes in many applications [11, 49].",
      "startOffset" : 235,
      "endOffset" : 243
    }, {
      "referenceID" : 50,
      "context" : "Spectral methods were recently used to determine the penalty parameter for the non-relaxed ADMM in [51].",
      "startOffset" : 99,
      "endOffset" : 103
    }, {
      "referenceID" : 50,
      "context" : "Note this is the same “optimal” penalty parameter proposed for non-relaxed ADMM in [51].",
      "startOffset" : 83,
      "endOffset" : 87
    }, {
      "referenceID" : 50,
      "context" : "The estimation of α̂k and β̂k for the dual components ĥ(λ̂k) and ĝ(λk) at the k-th iteration of primal ADMM has been described in [51].",
      "startOffset" : 130,
      "endOffset" : 134
    }, {
      "referenceID" : 50,
      "context" : "For a detailed derivation of these formulas, see [51].",
      "startOffset" : 49,
      "endOffset" : 53
    }, {
      "referenceID" : 50,
      "context" : "Rather, we adopt the correlation criterion proposed in [51] to test the validity of the local linear assumption, and only rely on the adaptive model when the assumptions are deemed valid.",
      "startOffset" : 55,
      "endOffset" : 59
    }, {
      "referenceID" : 12,
      "context" : "We study several vision benchmark datasets such as the extended Yale B face dataset [13], MNIST digital images [29], and CIFAR10 object images1 [28].",
      "startOffset" : 84,
      "endOffset" : 88
    }, {
      "referenceID" : 28,
      "context" : "We study several vision benchmark datasets such as the extended Yale B face dataset [13], MNIST digital images [29], and CIFAR10 object images1 [28].",
      "startOffset" : 111,
      "endOffset" : 115
    }, {
      "referenceID" : 27,
      "context" : "We study several vision benchmark datasets such as the extended Yale B face dataset [13], MNIST digital images [29], and CIFAR10 object images1 [28].",
      "startOffset" : 144,
      "endOffset" : 148
    }, {
      "referenceID" : 6,
      "context" : "We also use synthetic and benchmark datasets from [7, 57, 30, 43, 33, 21], which are obtained from the UCI repository and the LIBSVM page.",
      "startOffset" : 50,
      "endOffset" : 73
    }, {
      "referenceID" : 56,
      "context" : "We also use synthetic and benchmark datasets from [7, 57, 30, 43, 33, 21], which are obtained from the UCI repository and the LIBSVM page.",
      "startOffset" : 50,
      "endOffset" : 73
    }, {
      "referenceID" : 29,
      "context" : "We also use synthetic and benchmark datasets from [7, 57, 30, 43, 33, 21], which are obtained from the UCI repository and the LIBSVM page.",
      "startOffset" : 50,
      "endOffset" : 73
    }, {
      "referenceID" : 42,
      "context" : "We also use synthetic and benchmark datasets from [7, 57, 30, 43, 33, 21], which are obtained from the UCI repository and the LIBSVM page.",
      "startOffset" : 50,
      "endOffset" : 73
    }, {
      "referenceID" : 32,
      "context" : "We also use synthetic and benchmark datasets from [7, 57, 30, 43, 33, 21], which are obtained from the UCI repository and the LIBSVM page.",
      "startOffset" : 50,
      "endOffset" : 73
    }, {
      "referenceID" : 20,
      "context" : "We also use synthetic and benchmark datasets from [7, 57, 30, 43, 33, 21], which are obtained from the UCI repository and the LIBSVM page.",
      "startOffset" : 50,
      "endOffset" : 73
    }, {
      "referenceID" : 56,
      "context" : "Linear regression with EN regularization Elastic net (EN) is a modification of the `1-norm (or LASSO) regularizer that helps dealing with highly correlated variables [57, 21], and requires solving",
      "startOffset" : 166,
      "endOffset" : 174
    }, {
      "referenceID" : 20,
      "context" : "Linear regression with EN regularization Elastic net (EN) is a modification of the `1-norm (or LASSO) regularizer that helps dealing with highly correlated variables [57, 21], and requires solving",
      "startOffset" : 166,
      "endOffset" : 174
    }, {
      "referenceID" : 54,
      "context" : "ADMM has been applied to solve low rank least squares problems [55, 53]",
      "startOffset" : 63,
      "endOffset" : 71
    }, {
      "referenceID" : 52,
      "context" : "ADMM has been applied to solve low rank least squares problems [55, 53]",
      "startOffset" : 63,
      "endOffset" : 71
    }, {
      "referenceID" : 52,
      "context" : "LRLS has been used to formulate exemplar classifiers and discover visual subcategories [53].",
      "startOffset" : 87,
      "endOffset" : 91
    }, {
      "referenceID" : 2,
      "context" : "where z is the SVM dual variable, Q is the kernel matrix, c is a vector of labels, e is a vector of ones, and C > 0 [3].",
      "startOffset" : 116,
      "endOffset" : 119
    }, {
      "referenceID" : 1,
      "context" : "Consensus `1-regularized logistic regression ADMM has become an important tool for solving distributed optimization problems [2].",
      "startOffset" : 125,
      "endOffset" : 128
    }, {
      "referenceID" : 21,
      "context" : "Unwrapped SVM The unwrapped formulation of SVM [22], which can be used in distributed computing environments via “transpose reduction” tricks, applies ADMM to the primal form of SVM to solve",
      "startOffset" : 47,
      "endOffset" : 51
    }, {
      "referenceID" : 41,
      "context" : "Total variation image denoising (TVID) Total variation image denoising is often performed by solving [42]",
      "startOffset" : 101,
      "endOffset" : 105
    }, {
      "referenceID" : 46,
      "context" : "RPCA Robust principal component analysis (RPCA) has broad applications in computer vision and imaging [47, 37, 39].",
      "startOffset" : 102,
      "endOffset" : 114
    }, {
      "referenceID" : 36,
      "context" : "RPCA Robust principal component analysis (RPCA) has broad applications in computer vision and imaging [47, 37, 39].",
      "startOffset" : 102,
      "endOffset" : 114
    }, {
      "referenceID" : 38,
      "context" : "RPCA Robust principal component analysis (RPCA) has broad applications in computer vision and imaging [47, 37, 39].",
      "startOffset" : 102,
      "endOffset" : 114
    }, {
      "referenceID" : 5,
      "context" : "5 as suggested in [6].",
      "startOffset" : 18,
      "endOffset" : 21
    }, {
      "referenceID" : 23,
      "context" : "The parameters of RB and AADMM are selected as in [24, 2, 51].",
      "startOffset" : 50,
      "endOffset" : 61
    }, {
      "referenceID" : 1,
      "context" : "The parameters of RB and AADMM are selected as in [24, 2, 51].",
      "startOffset" : 50,
      "endOffset" : 61
    }, {
      "referenceID" : 50,
      "context" : "The parameters of RB and AADMM are selected as in [24, 2, 51].",
      "startOffset" : 50,
      "endOffset" : 61
    }, {
      "referenceID" : 39,
      "context" : "The initial penalty τ0 =1/10 and initial relaxation γ0 =1 are used for all problems except the canonical QP problem, where initial parameters are set to the geometric mean of the maximum and minimum eigenvalues of matrix Q, as proposed for quadratic problems in [40].",
      "startOffset" : 262,
      "endOffset" : 266
    }, {
      "referenceID" : 23,
      "context" : "As suggested by [24, 51], the adaptivity of RB and AADMM is stopped after 1000 iterations to guarantee convergence.",
      "startOffset" : 16,
      "endOffset" : 24
    }, {
      "referenceID" : 50,
      "context" : "As suggested by [24, 51], the adaptivity of RB and AADMM is stopped after 1000 iterations to guarantee convergence.",
      "startOffset" : 16,
      "endOffset" : 24
    } ],
    "year" : 2017,
    "abstractText" : "Many modern computer vision and machine learning applications rely on solving difficult optimization problems that involve non-differentiable objective functions and constraints. The alternating direction method of multipliers (ADMM) is a widely used approach to solve such problems. Relaxed ADMM is a generalization of ADMM that often achieves better performance, but its efficiency depends strongly on algorithm parameters that must be chosen by an expert user. We propose an adaptive method that automatically tunes the key algorithm parameters to achieve optimal performance without user oversight. Inspired by recent work on adaptivity, the proposed adaptive relaxed ADMM (ARADMM) is derived by assuming a BarzilaiBorwein style linear gradient. A detailed convergence analysis of ARADMM is provided, and numerical results on several applications demonstrate fast practical convergence.",
    "creator" : "LaTeX with hyperref package"
  }
}