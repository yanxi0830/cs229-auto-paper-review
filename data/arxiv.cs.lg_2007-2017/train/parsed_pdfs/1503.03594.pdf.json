{
  "name" : "1503.03594.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Efficient Learning of Linear Separators under Bounded Noise",
    "authors" : [ "Pranjal Awasthi", "Maria-Florina Balcan", "Nika Haghtalab", "Ruth Urner" ],
    "emails" : [ "pawashti@cs.princeton.edu", "ninamf@cs.cmu.edu", "nhaghtal@cs.cmu.edu", "rurner@tuebingen.mpg.de" ],
    "sections" : [ {
      "heading" : null,
      "text" : "We additionally provide lower bounds showing that popular algorithms such as hinge loss minimization and averaging cannot lead to arbitrarily small excess error under Massart noise, even under the uniform distribution. Our work instead, makes use of a margin based technique developed in the context of active learning. As a result, our algorithm is also an active learning algorithm with label complexity that is only a logarithmic the desired excess error ."
    }, {
      "heading" : "1 Introduction",
      "text" : "Overview Linear separators are the most popular classifiers studied in both the theory and practice of machine learning. Designing noise tolerant, polynomial time learning algorithms that achieve arbitrarily small excess error rates for linear separators is a long-standing question in learning theory. In the absence of noise (when the data is realizable) such algorithms exist via linear programming [11]. However, the problem becomes significantly harder in the presence of label noise. In particular, in this work we are concerned with designing algorithms that can achieve error OPT + which is arbitrarily close to OPT, the error of the best linear separator, and run in time polynomial in 1 and d (as usual, we call the excess error). Such strong guarantees are only known for the well studied random classification noise model [7]. In this work, we provide the first algorithm that can achieve arbitrarily small excess error, in truly polynomial time, for bounded noise, also called Massart noise [28], a much more realistic and widely studied noise model in statistical learning theory [9]. We additionally show strong lower bounds under the same noise model for two other computationally efficient learning algorithms (hinge loss minimization and the averaging algorithm), which could be of independent interest. Motivation The work on computationally efficient algorithms for learning halfspaces has focused on two different extremes. On one hand, for the very stylized random classification noise model (RCN), where each\nar X\niv :1\n50 3.\n03 59\n4v 1\n[ cs\n.L G\n] 1\n2 M\nar 2\nexample x is flipped independently with equal probability η, several works have provided computationally efficient algorithms that can achieve arbitrarily small excess error in polynomial time [7, 30, 5] — note that all these results crucially exploit the high amount of symmetry present in the RCN noise. At the other extreme, there has been significant work on much more difficult and adversarial noise models, including the agnostic model [25] and malicious noise models [24]. The best results here however, not only require additional distributional assumptions about the marginal over the instance space, but they only achieve much weaker multiplicative approximation guarantees [23, 27, 2]; for example, the best result of this form for the case of uniform distribution over the unit sphere Sd−1 achieves excess error cOPT [2], for some large constant c. While interesting from a technical point of view, guarantees of this form are somewhat troubling from a statistical point of view, as they are inconsistent, in the sense there is a barrier O(OPT), after which we cannot prove that the excess error further decreases as we get more and more samples. In fact, recent evidence shows that this is unavoidable for polynomial time algorithms for such adversarial noise models [12].\nOur Results In this work we identify a realistic and widely studied noise model in the statistical learning theory, the so called Massart noise [9], for which we can prove much stronger guarantees. Massart noise can be thought of as a generalization of the random classification noise model where the label of each example x is flipped independently with probability η(x) < 1/2. The adversary has control over choosing a different noise rate η(x) ≤ η for every example x with the only constraint that η(x) ≤ η. From a statistical point of view, it is well known that under this model, we can get faster rates compared to worst case joint distributions [9]. In computational learning theory, this noise model was also studied, but under the name of malicious misclassification noise [29, 31]. However due to its highly unsymmetric nature, til date, computationally efficient learning algorithms in this model have remained elusive. In this work, we provide the first computationally efficient algorithm achieving arbitrarily small excess error for learning linear separators.\nFormally, we show that there exists a polynomial time algorithm that can learn linear separators to error OPT+ and run in poly(d, 1 ) when the underlying distribution is the uniform distribution over the unit ball in <d and the noise of each example is upper bounded by a constant η (independent of the dimension).\nAs mentioned earlier, a result of this form was only known for random classification noise. From a technical point of view, as opposed to random classification noise, where the error of each classifier scales uniformly under the observed labels, the observed error of classifiers under Masasart noise could change drastically in a non-monotonic fashion. This is due to the fact that the adversary has control over choosing a different noise rate η(x) ≤ η for every example x. As a result, as we show in our work (see Section 4), standard algorithms such as the averaging algorithm [30] which work for random noise can only achieve a much poorer excess error (as a function of η) under Massart noise. Technically speaking, this is due to the fact that Massart noise can introduce high correlations between the observed labels and the component orthogonal to the direction of the best classifier.\nIn face of these challenges, we take an entirely different approach than previously considered for random classification noise. Specifically, we analyze a recent margin based algorithm of [2]. This algorithm was designed for learning linear separators under agnostic and malicious noise models, and it was shown to achieve an excess error of cOPT for a constant c. By using new structural insights, we show that there exists a constant η (independent of the dimension), so that if we use Massart noise where the flipping probability is upper bounded by η, we can use a modification of the algorithm in [2] and achieve arbitrarily small excess error. One way to think about this result is that we define an adaptively chosen sequence of hinge loss minimization problems around smaller and smaller bands around the current guess for the target. We show by relating the hinge loss and 0/1-loss together with a careful localization analysis that these will\ndirect us closer and closer to the optimal classifier, allowing us to achieve arbitrarily small excess error rates in polynomial time.\nGiven that our algorithm is an adaptively chosen sequence of hinge loss minimization problems, one might wonder what guarantee one-shot hinge loss minimization could provide. In Section 5, we show a strong negative result: for every τ , and η ≤ 1/2, there is a noisy distribution D̃ over <d × {0, 1} satisfying Massart noise with parameter η and an > 0, such that τ -hinge loss minimization returns a classifier with excess error Ω( ). This result could be of independent interest. While there exists earlier work showing that hinge loss minimization can lead to classifiers of large 0/1-loss [6], the lower bounds in that paper employ distributions with significant mass on discrete points with flipped label (which is not possible under Massart noise) at a very large distance from the optimal classifier. Thus, that result makes strong use of the hinge loss’s sensitivity to errors at large distance. Here, we show that hinge loss minimization is bound to fail under much more benign conditions.\nOne appealing feature of our result is the algorithm we analyze is in fact naturally adaptable to the active learning or selective sampling scenario (intensively studied in recent years [19, 13, 20], where the learning algorithms only receive the classifications of examples when they ask for them. We show that, in this model, our algorithms achieve a label complexity whose dependence on the error parameter is polylogarithmic (and thus exponentially better than that of any passive algorithm). This provides the first polynomial-time active learning algorithm for learning linear separators under Massart noise. We note that prior to our work only inefficient algorithms could achieve the desired label complexity under Massart noise [4, 20].\nRelated Work The agnostic noise model is notoriously hard to deal with computationally and there is significant evidence that achieving arbitrarily small excess error in polynomial time is hard in this model [1, 18, 12]. For this model, under our distributional assumptions, [23] provides an algorithm that learns linear separators in <d to excess error at most , but whose running time poly(dexp(1/ )). Recent work show evidence that the exponential dependence on 1/ is unavoidable in this case [26] for the agnostic case. We side-step this by considering a more structured, yet realistic noise model.\nMotivated by the fact that many modern machine learning applications have massive amounts of unannotated or unlabeled data, there has been significant interest in designing active learning algorithms that most efficiently utilize the available data, while minimizing the need for human intervention. Over the past decade there has been substantial progress on understanding the underlying statistical principles of active learning, and several general characterizations have been developed for describing when active learning could have an advantage over the classical passive supervised learning paradigm both in the noise free settings and in the agnostic case [17, 13, 3, 4, 19, 15, 10, 14, 20]. However, despite many efforts, except for very simple noise models (random classification noise [5] and linear noise [16]), to date there are no known computationally efficient algorithms with provable guarantees in the presence of Massart noise that can achieve arbitrarily small excess error.\nWe note that work of [21] provides computationally efficient algorithms for both passive and active learning under the assumption that the hinge loss (or other surrogate loss) minimizer aligns with the minimizer of the 0/1-loss. In our work (Section 5), we show that this is not the case under Massart noise even when the marginal over the instance space is uniform, but still provide a computationally efficient algorithm for this much more challenging setting."
    }, {
      "heading" : "2 Preliminaries",
      "text" : "We consider the binary classification problem; that is, we work on the problem of predicting a binary label y for a given instance x. We assume that the data points (x, y) are drawn from an unknown underlying\ndistribution D̃ over X × Y , where X = <d is the instance space and Y = {−1, 1} is the label space. For the purpose of this work, we consider distributions where the marginal of D̃ over X is a uniform distribution on a d-dimensional unit ball. We work with the class of all homogeneous halfspaces, denoted byH = {sign(w · x) : w ∈ <d}. For a given halfspace w ∈ H, we define the error of w with respect to D̃, by errD̃(w) = Pr(x,y)∼D̃[sign(w · x) 6= y].\nWe examine learning halfspaces in the presence of Massart noise. In this setting, we assume that the Bayes optimal classifier is a linear separator w∗. Note that w∗ can have a non-zero error. Then Massart noise with parameter β > 0 is a condition such that for all x, the conditional label probability is such that\n|Pr(y = 1|x)− Pr(y = −1|x)| ≥ β. (1)\nEquivalently, we say that D̃ satisfies Massart noise with parameter β, if an adversary construct D̃ by first taking the distribution D over instances (x, sign(w∗ · x)) and then flipping the label of an instance x with probability at most 1−β2 .\n1 Also note that under distribution D̃, w∗ remains the Bayes optimal classier. In the remainder of this work, we refer to D̃ as the “noisy” distribution and to distribution D over instances (x, sign(w∗ · x)) as the “clean” distribution.\nOur goal is then to find a halfspace w that has small excess error, as compared to the Bayes optimal classifier w∗. That is, for any > 0, find a halfspace w, such that errD̃(w) − errD̃(w\n∗) ≤ . Note that the excess error of any classifier w only depends on the points in the region where w and w∗ disagree. So, errD̃(w)− errD̃(w ∗) ≤ θ(w,w ∗)\nπ . Additionally, under Massart noise the amount of noise in the disagreement region is also bounded by 1−β2 . It is not difficult to see that under Massart noise,\nβ θ(w,w∗)\nπ ≤ errD̃(w)− errD̃(w\n∗). (2)\nIn our analysis, we frequently examine the region within a certain margin of a halfspace. For a halfspace w and margin b, let Sw,b be the set of all points that fall within a margin b from w, i.e., Sw,b = {x : |w · x| ≤ b}. For distributions D̃ and D, we indicate the distribution conditioned on Sw,b by D̃w,b and Dw,b, respectively. In the remainder of this work, we refer to the region Sw,b as “the band”.\nIn our analysis, we use hinge loss, as a convex surrogate function for the 0/1-loss. For a halfspace w, we use τ -normalized hinge loss that is defined as `(w, x, y) = max{0, 1 − (w·x)yτ }. For a labeled sample set W , let `(w,W ) = 1|W | ∑ (x,y)∈W `(w, x, y) be the empirical hinge loss of a vector w with respect to W ."
    }, {
      "heading" : "3 Computationally Efficient Algorithm for Massart Noise",
      "text" : "In this section, prove our main result for learning half-spaces in presence of Massart noise. We focus on the case where D is the uniform distribution on the d-dimensional unit ball. Our main Theorem is as follows.\nTheorem 1. Let the optimal bayes classifier be a half-space denoted by w∗. Assume that the massart noise condition holds for some β > 1 − 3.6 × 10−6. Then for any , δ > 0, Algorithm 1 with λ = 10−8, αk = 0.038709π(1−λ)k−1, bk−1 = 2.3463αk√d , and τk = √ 2.50306 (3.6×10−6)1/4bk−1, runs in polynomial time, proceeds in s = O(log 1 ) rounds, where in round k it takes nk = poly(d, exp(k), log( 1 δ )) unlabeled samples and mk = O(d(d+ log(k/δ))) labels and with probability (1− δ) returns a linear separator that has excess error (compared to w∗) of at most .\n1Note that the relationship between Massart noise parameter β, and the maximum flipping probability discussed in the introduction η, is η = 1−β\n2 .\nNote that in the above theorem and Algorithm 1, the value of β is unknown to the algorithm, and therefore, our results are adaptive to values of β within the acceptable range defined by the theorem.\nThe algorithm described above is similar to that of [2] and uses an iterative margin-based approach. The algorithm runs for s = log 1\n1−λ (1 ) rounds for a constant λ ∈ (0, 1]. By induction assume that our algorithm\nproduces a hypothesis wk−1 at round k − 1 such that θ(wk−1, w∗) ≤ αk. We satisfy the base case by using an algorithm of [27]. At round k, we sample mk labeled examples from the conditional distribution D̃wk−1,bk−1 which is the uniform distribution over {x : |wk−1 · x| ≤ bk−1}. We then choose wk from the set of all hypothesis B(wk−1, αk) = {w : θ(w,wk−1) ≤ αk} such that wk minimizes the empirical hinge loss over these examples. Subsequently, as we prove in detail later, θ(wk, w∗) ≤ αk+1. Note that for any w, the excess error of w is at most the error of w on D̃ when the labels are corrected according to w∗, i.e., errD̃(w) − errD̃(w ∗) ≤ errD(w). Moreover, when D is uniform, errD(w) = θ(w ∗,w) π . Hence, θ(ws, w ∗) ≤ π implies that ws has excess error of at most .\nThe algorithm described below was originally introduced to achieve an error of c ·err(w∗) for some constant c in presence of adversarial noise. Achieving a small excess error err(w∗)+ is a much more ambitious goal – one that requires new technical insights. Our two crucial technical innovations are as follow: We first make a key observation that under Massart noise, the noise rate over any conditional distribution D̃ is still at most 1−β2 . Therefore, as we focus on the distribution within the band, our noise rate does not increase. Our second technical contribution is a careful choice of parameters. Indeed the choice of parameters, upto a constant, plays an important role in tolerating a constant amount of Massart noise. Using these insights, we show that the algorithm by [2] can indeed achieve a much stronger guarantee, namely arbitrarily small excess error in presence of Massart noise. That is, for any , this algorithm can achieve error of err(w∗) + in the presence of Massart noise.\nAlgorithm 1 EFFICIENT ALGORITHM FOR ARBITRARILY SMALL EXCESS ERROR FOR MASSART NOISE Input: A distribution D̃. An oracle that returns x and an oracle that returns y for a (x, y) sampled from D̃. Permitted excess error and probability of failure δ. Parameters: A learning rate λ; a sequence of sample sizes mk; a sequence of angles of the hypothesis space αk; a sequence of widths of the labeled space bk; a sequence of thresholds of hinge-loss τk. Algorithm:\n1. Take poly(d, 1δ ) samples and run poly(d, 1 δ )-time algorithm by [27] to find a half-spacew0 with excess\nerror 0.0387089 such that θ(w∗, w0) ≤ 0.038709π (Refer to Appendix C)\n2. Draw m1 examples (x, y) from D̃ and put them into a working set W .\n3. For k = 1, . . . , log( 1 1−λ )\n(1 ) = s.\n(a) Find vk such that ‖vk −wk−1‖ < αk (as a result vk ∈ B(wk−1, αk)), that minimizes the empirical hinge loss over W using threshold τk. That is `τk(vk,W ) ≤ minw∈B(wk−1,αk) `τk(w,W ) + 10−8.\n(b) Clear the working set W .\n(c) Normalize vk to wk = vk‖vk‖2 . Until mk+1 additional examples are put in W , draw an example x\nfrom D̃. If |wk · x| ≥ bk, then reject x, else put (x, y) into W .\nOutput: Return ws, which has excess error with probability 1− δ.\nOverview of our analysis: Similar to [2], we divide errD(wk) to two categories; error in the band, i.e., on x ∈ Swk−1,bk−1 , and error outside the band, on x 6∈ Swk−1,bk−1 . We choose bk−1 and αk such that, for every hypothesis w ∈ B(wk−1, αk) that is considered at step k, the probability mass outside the band such that w and w∗ also disagree is very small (Lemma 5). Therefore, the error associated with the region outside the band is also very small. This motivates the design of the algorithm to only minimize the error in the band. Furthermore, the probability mass of the band is also small enough such that for errD(wk) ≤ αk+1 to hold, it suffices for wk to have a small constant error over the clean distribution restricted to the band, namely Dwk−1,bk−1 .\nThis is where minimizing hinge loss in the band comes in. As minimizing the 0/1-loss is NP-hard, an alternative method for finding wk with small error in the band is needed. Hinge loss that is a convex loss function can be efficiently minimized. So, we can efficiently find wk that minimizes the empirical hinge loss of the sample drawn from D̃wk−1,bk−1 . To allow the hinge loss to remain a faithful proxy of 0/1-loss as we focus on bands with smaller widths, we use a normalized hinge loss function defined by `τ (w, x, y) = max{0, 1− w·xyτ }.\nA crucial part of our analysis involves showing that if wk minimizes the empirical hinge loss of the sample set drawn from D̃wk−1,bk−1 , it indeed has a small 0/1-error on Dwk−1,bk−1 . To this end, we first show that when τk is proportional to bk, the hinge loss of w∗ on Dwk−1,bk−1 , which is an upper bound on the 0/1-error of wk in the band, is itself small (Lemma 1). Next, we notice that under Massart noise, the noise rate in any marginal of the distribution is still at most 1−β2 . Therefore, focusing the distribution in the band does not increase the probability of noise in the band. Moreover, the noise points in the band are close to the decision boundary so intuitively speaking, they can not increase the hinge loss too much. Using these insights we can show that the hinge loss of wk on D̃wk−1,bk−1 is close to its hinge loss on Dwk−1,bk−1 (Lemma 2).\nProof of Theorem 1 and related lemmas\nTo prove Theorem 1, we first introduce a series of lemmas concerning the behavior of hinge loss in the band. These lemmas build up towards showing that wk has error of at most a fixed small constant in the band.\nFor ease of exposition, for any k, let Dk and D̃k represent Dwk−1,bk−1 and D̃wk−1,bk−1 , respectively, and `(·) represent `τk(·). Furthermore, let c = 2.3463, such that bk−1 = cαk√ d\n. Our first lemma, whose proof appears in Appendix B, provides an upper bound on the true hinge error\nof w∗ on the clean distribution in the band.\nLemma 1. E(x,y)∼Dk`(w ∗, x, y) ≤ 0.665769 τb .\nThe next Lemma compares the true hinge loss of any w ∈ B(wk−1, αk) on two distributions, D̃k and Dk. It is clear that the difference between the hinge loss on these two distributions is entirely attributed to the noise points and their margin from w. A key insight in the proof of this lemma is that as we concentrate in the band, the probability of seeing a noise point remains under 1−β2 . This is due to the fact that under Massart noise, each label can be changed with probability at most 1−β2 . Furthermore, by concentrating in the band all points are close to the decision boundary of wk−1. Since w is also close in angle to wk−1, then points in the band are also close to the decision boundary of w. Therefore the hinge loss of noise points in the band can not increase the total hinge loss of w by too much.\nLemma 2. For any w such that w ∈ B(wk−1, αk), we have\n|E(x,y)∼Dk`(w, x, y)− E(x,y)∼D̃k`(w, x, y)| ≤ 1.092 √\n2 √\n1− β bk−1 τk .\nProof. Let N be the set of noise points. We have,\n|E(x,y)∼D̃k`(w, x, y)− E(x,y)∼Dk`(w, x, y)| = |E(x,y)∈D̃k (`(w, x, y)− `(w, x, sign(w ∗ · x)) |\n≤ E(x,y)∼D̃k (1x∈N (`(w, x, y)− `(w, x,−y)))\n≤ 2E(x,y)∼D̃k\n( 1x∈N\n|w · x| τk ) ≤ 2 τk √ Pr (x,y)∼D̃k (x ∈ N)× √ E(x,y)∼D̃k(w · x) 2 (By Cauchy Shwarz)\n≤ 2 τk\n√ 1− β\n2 √ α2k d− 1 + b2k−1 (By Definition 4.1 of [2] for uniform)\n≤ √ 2 √\n1− β bk−1 τk\n√ d\n(d− 1)c2 + 1\n≤ 1.092 √ 2 √\n1− β bk−1 τk\n(for d > 20, c > 1)\nFor a labeled sample set W drawn at random from D̃k, let cleaned(W ) be the set of samples with the labels corrected by w∗, i.e., cleaned(W ) = {(x, sign(w∗ · x)) : for all (x, y) ∈ W}. Then by standard VC-dimension bounds (Proof included in Appendix B) there is mk ∈ O(d(d + log(k/d))) such that for any randomly drawn set W of mk labeled samples from D̃k, with probability 1 − δ2(k+k2) , for any w ∈ B(wk−1, αk),\n|E(x,y)∼D̃k`(w, x, y)− `(w,W )| ≤ 10 −8, (3)\n|E(x,y)∼Dk`(w, x, y)− `(w, cleaned(W ))| ≤ 10 −8. (4)\nOur next lemma is a crucial step in our analysis of Algorithm 1. This lemma proves that ifwk ∈ B(wk−1, αk) minimizes the empirical hinge loss on the sample drawn from the noisy distribution in the band, namely D̃wk−1,bk−1 , then with high probability wk also has a small 0/1-error with respect to the clean distribution in the band, i.e., Dwk−1,bk−1 .\nLemma 3. There exists mk ∈ O(d(d+ log(k/d))), such that for a randomly drawn labeled sampled set W of size mk from D̃k, and for wk such that wk has the minimum empirical hinge loss on W between the set of all hypothesis in B(wk−1, αk), with probability 1− δ2(k+k2) ,\nerrDk(wk) ≤ 0.757941 τk bk−1\n+ 3.303 √\n1− β bk−1 τk + 3.28× 10−8.\nProof Sketch First, we note that the true 0/1-error of wk on any distribution is at most its true hinge loss on that distribution. Lemma 1 provides an upper bound on the true hinge loss on distribution Dk. Therefore, it remains to create a connection between the empirical hinge loss of wk on the sample drawn from D̃k to its true hinge loss on distribution Dk. This, we achieve by using the generalization bounds of Equations 3 and 4 to connect the empirical and true hinge loss of wk and w∗, and using Lemma 2 to connect the hinge of wk and w∗ in the clean and noisy distributions.\nProof of Theorem 1 For ease of exposition, let c = 2.3463. Recall that λ = 10−8, αk = 0.038709π(1 − λ)k−1, bk−1 = cαk√d , τk = √ 2.50306 (3.6× 10−6)1/4bk−1, and β > 1− 3.6× 10−6.\nNote that for any w, the excess error of w is at most the error of w on the clean distribution D, i.e., errD̃(w) − errD̃(w ∗) ≤ errD(w). Moreover, for uniform distribution D, errD(w) = θ(w ∗,w) π . Hence, to show that w has excess error, it suffices to show that errD(w) ≤ . Our goal is to achieve excess error of 0.038709(1− λ)k at round k. This we do indirectly by bounding errD(wk) at every step. We use induction. For k = 0, we use the algorithm for adversarial noise model by [27], which can achieve excess error of if errD̃(w ∗) < 2 256 log(1/ ) (Refer to Appendix C for more details). For Massart noise, errD̃(w ∗) ≤ 1−β2 . So, for our choice of β, this algorithm can achieve excess error of 0.0387089 in poly(d, 1δ ) samples and run-time. Furthermore, using Equation 2, θ(w0, w ∗) < 0.038709π.\nAssume that at round k−1, errD(wk−1) ≤ 0.038709(1−λ)k−1. We will show that wk, which is chosen by the algorithm at round k, also has errD(wk) ≤ 0.038709(1− λ)k.\nFirst note that errD(wk−1) ≤ 0.038709(1 − λ)k−1 implies θ(wk−1, w∗) ≤ αk. Let S = Swk−1,bk−1 indicate the band at round k. We divide the error of wk to two parts, error outside the band and error inside of the band. That is\nerrD(wk) = Pr x∼D [x /∈ S and (wk · x)(w∗ · x) < 0] + Pr x∼D [x ∈ S and (wk · x)(w∗ · x) < 0].\nFor the first part, i.e., error outside of the band, Prx∼D[x /∈ S and (wk · x)(w∗ · x) < 0] is at most\nPr x∼D [x /∈ S and (wk · x)(wk−1 · x) < 0] + Pr x∼D [x /∈ S and (wk−1 · x)(w∗ · x) < 0] ≤ 2αk π e− c2(d−2) 2d ,\nwhere this inequality holds by the application of Lemma 5 and the fact that θ(wk−1, wk) ≤ αk and θ(wk−1, w\n∗) ≤ αk. For the second part, i.e., error inside the band\nPr x∼D [x ∈ S and (wk · x)(w∗ · x) < 0] = errDk(wk) Pr x∼D [x ∈ S]\n≤ errDk(wk) Vd−1 Vd 2 bk−1 (By Lemma 4)\n≤ errDk(wk) c αk\n√ 2(d+ 1)\nπd , where the last transition holds by the fact that Vd−1Vd ≤ √ d+1 2π [8]. Replacing an upper bound on errDk(wk) from Lemma 3, to show that errD(wk) ≤ αk+1π , it suffices to show that the following inequality holds.( 0.757941\nτk bk−1\n+ 3.303 √\n1− β bk−1 τk\n+ 3.28× 10−8 ) c αk √ 2(d+ 1)\nπd + 2αk π e− c2(d−2) 2d ≤ αk+1 π .\nWe simplify this inequality as follows.( 0.757941\nτk bk−1\n+ 3.303 √\n1− β bk−1 τk\n+ 3.28× 10−8 ) c √ 2π(d+ 1)\nd + 2e−\nc2(d−2) 2d ≤ 1− λ.\nReplacing in the r.h.s., the values of c = 2.3463, and τk = √\n2.50306(3.6× 10−6)1/4bk−1, we have(√ 2.50306(3.6× 10−6)1/4 + √ 2.50306 √ 1− β\n(3.6× 10−6)1/4 + 3.28× 10−8\n) c √ 2π(d+ 1)\nd + 2e−\nc2(d−2) 2d\n≤ 5.88133 ( 2 √ 2.50306(3.6× 10−6)1/4 + 3.28× 10−8 ) √21\n20 + 0.167935 (For d > 20)\n≤ 0.998573 < 1− λ\nTherefore, errD(wk) ≤ 0.038709(1− λ)k. Sample complexity analysis: We require mk labeled samples in the band Swk−1,bk−1 at round k. By Lemma 4, the probability that a randomly drawn sample from D̃ falls in Swk−1,bk−1 is at leastO(bk−1 √ d) = O((1 − λ)k−1). Therefore, we need O((1 − λ)k−1mk) unlabeled samples to get mk examples in the band with probability 1− δ\n8(k+k2) . So, the total unlabeled sample complexity is at most\ns∑ k=1 O ( (1− λ)k−1mk ) ≤ s s∑ k=1 mk ∈ O ( 1 log ( d )( d+ log log(1/ ) δ )) .\n4 Average Does Not Work\nOur algorithm described in the previous section uses convex loss minimization (in our case, hinge loss) in the band as an efficient proxy for minimizing the 0/1 loss. The Average algorithm introduced by [30] is another computationally efficient algorithm that has provable noise tolerance guarantees under certain noise models and distributions. For example, it achieves arbitrarily small excess error in the presence of random classification noise and monotonic noise when the distribution is uniform over the unit sphere. Furthermore, even in the presence of a small amount of malicious noise and less symmetric distributions, Average has been used to obtain a weak learner, which can then be boosted to achieve a non-trivial noise tolerance [27]. Therefore it is natural to ask, whether the noise tolerance that Average exhibits could be extended to the case of Massart noise under the uniform distribution? We answer this question in the negative. We show that the lack of symmetry in Massart noise presents a significant barrier for the one-shot application of Average, even when the marginal distribution is completely symmetric. Additionally, we also discuss obstacles in incorporating Average as a weak learner with the margin-based technique.\nIn a nutshell, Average takesm sample points and their respective labels,W = {(x1, y1), . . . , (xm, ym)}, and returns 1m ∑m i=1 x\niyi. Our main result in this section shows that for a wide range of distributions that are very symmetric in nature, including the Gaussian and the uniform distribution, there is an instance of Massart noise under which Average can not achieve an arbitrarily small excess error.\nTheorem 2. For any continuous distributionD with a p.d.f. that is a function of the distance from the origin only, there is a noisy distribution D̃ over X ×{0, 1} that satisfies Massart noise condition in Equation 1 for some parameter β > 0 and Average returns a classifier with excess error Ω(β(1−β)1+β ).\nProof. Let w∗ = (1, 0, . . . , 0) be the target halfspace. Let the noise distribution be such that for all x, if x1x2 < 0 then we flip the label of x with probability 1−β2 , otherwise we keep the label. Clearly, this satisfies Massart noise with parameter β. Let w be expected vector returned by Average. We first show that w is far from w∗ in angle. Then, using Equation 2 we show that w has large excess error.\nFirst we examine the expected component of w that is parallel to w∗, i.e., w · w∗ = w1. For ease of exposition, we divide our analysis to two cases, one for regions with no noise (first and third quadrants)\nand second for regions with noise (second and fourth quadrants). Let E be the event that x1x2 > 0. By symmetry, it is easy to see that Pr[E] = 1/2. Then\nE[w · w∗] = Pr(E) E[w · w∗|E] + Pr(Ē) E[w · w∗|Ē] For the first term, for x ∈ E the label has not changed. So, E[w · w∗|E] = E[|x1| |E] = ∫ 1 0 zf(z). For the second term, the label of each point stays the same with probability 1+β2 and is flipped with probability 1−β 2 . Hence, E[w · w ∗|E] = β E[|x1| |E] = β ∫ 1 0 zf(z). Therefore, the expected parallel component of w\nis E[w · w∗] = 1+β2 ∫ 1 0 zf(z)\nNext, we examine w2, the orthogonal component of w on the second coordinate. Similar to the previous case for the clean regions E[w2|E] = E[|x2| |E] = ∫ 1 0 zf(z). Next, for the second and forth quadrants, which are noisy, we have\nE(x,y)∼D̃[x2y|x1x2 < 0] = ( 1 + β\n2 ) ∫ 0 −1 z f(z) 2 + ( 1− β 2 ) ∫ 0 −1 (−z)f(z) 2\n(Fourth quadrant)\n+ ( 1 + β\n2 ) ∫ 1 0 (−z)f(z) 2 + ( 1− β 2 ) ∫ 1 0 z f(z) 2 (Second quadrant)\n= −(1 + β 2 ) ∫ 1 0 z f(z) 2 + ( 1− β 2 ) ∫ 1 0 z f(z) 2\n− (1 + β 2 ) ∫ 1 0 z f(z) 2 + ( 1− β 2 ) ∫ 1 0 z f(z) 2 (By symmetry)\n= −β ∫ 1 0 zf(z).\nSo, w2 = ( 1−β 2 ) ∫ 1 0 zf(z). Therefore θ(w,w ∗) = arctan(1−β1+β ) ≥ 1−β (1+β) . By Equation 2, we have errD̃(w)− errD̃(w ∗) ≥ β θ(w,w ∗) π ≥ β 1−β π(1+β) .\nOur margin-based analysis from Section 3 relies on using hinge-loss minimization in the band at every round to efficiently find a halfspace wk that is a weak learner for Dk, i.e., errDk(wk) is at most a small constant, as demonstrated in Lemma 3. Motivated by this more lenient goal of finding a weak learner, one might ask whether Average, as an efficient algorithm for finding low error halfspaces, can be incorporated with the margin-based technique in the same way as hinge loss minimization? We argue that the marginbased technique is inherently incompatible with Average.\nThe Margin-based technique maintains two key properties at every step: First, the angle between wk and wk−1 and the angle between wk−1and w∗ are small, and as a result θ(w∗, wk) is small. Second, wk is a weak learner with errDk−1(wk) at most a small constant. In our work, hinge loss minimization in the band guarantees both of these properties simultaneously by limiting its search to the halfspaces that are close in angle to wk−1 and limiting its distribution to Dwk−1,bk−1 . However, in the case of Average as we concentrate in the band Dwk−1,bk−1 we bias the distributions towards its orthogonal component with respect to wk−1. Hence, an upper bound on θ(w∗, wk−1) only serves to assure that most of the data is orthogonal to w∗ as well. Therefore, informally speaking, we lose the signal that otherwise could direct us in the direction of w∗. More formally, consider the construction from Theorem 2 such that wk−1 = w∗ = (1, 0, . . . , 0). In distribution Dwk−1,bk−1 , the component of wk that is parallel to wk−1 scales down by the width of the band, bk−1. However, as most of the probability stays in a band passing through the origin in any log-concave (including Gaussian and uniform) distribution, the orthogonal component of wk remains almost unchanged. Therefore, θ(wk, w∗) = θ(wk, wk−1) ∈ Ω( 1−βbk−1(1+β)) ≥ ( (1−β) √ d (1+β)αk−1 ) ."
    }, {
      "heading" : "5 Hinge Loss Minimization Does Not Work",
      "text" : "Hinge loss minimization is a widely used technique in Machine Learning. In this section, we show that, perhaps surprisingly, hinge loss minimization does not lead to arbitrarily small excess error even under very small noise condition, that is it is not consistent. (Note that in our setting of Massart noise, consistency is the same as achieving arbitrarily small excess error, since the Bayes optimal classifier is a member of the class of halfspaces).\nIt has been shown earlier that hinge loss minimization can lead to classifiers of large 0/1-loss [6]. However, the lower bounds in that paper employ distributions with significant mass on discrete points with flipped label (which is not possible under Massart noise) at a very large distance from the optimal classifier. Thus, that result makes strong use of the hinge loss’s sensitivity to errors at large distance. Here, we show that hinge loss minimization is bound to fail under much more benign conditions. More concretely, we show that for every parameter τ , and arbitrarily small bound on the probability of flipping a label, η = 1−β2 , hinge loss minimization is not consistent even on distributions with a uniform marginal over the unit ball in <2, with the Bayes optimal classifier being a halfspace and the noise satisfying the Massart noise condition with bound η. That is, there exists a constant ≥ 0 and a sample size m( ) such that hinge loss minimization returns a classifier of excess error at least with high probability over sample size of at least m( ).\nHinge loss minimization does approximate the optimal hinge loss. We show that this does not translate into an agnostic learning guarantee for halfspaces with respect to the 0/1-loss even under very small noise conditions. Let Pβ be the class of distributions D̃ with uniform marginal over the unit ball B1 ⊆ <2, the Bayes classifier being a halfspace w, and satisfying the Massart noise condition with parameter β. Our lower bound for hinge loss minimization is stated as follows.\nTheorem 3. For every hinge-loss parameter τ ≥ 0 and every Massart noise parameter 0 ≤ β < 1, there exists a distribution D̃τ,β ∈ Pβ (that is, a distribution over B1 × {−1, 1} with uniform marginal over B1 ⊆ <2 satisfying the β-Massart condition) such that τ -hinge loss minimization is not consistent on D̃τ,β with respect to the class of halfspaces. That is, there exists an ≥ 0 and a sample size m( ) such that hinge loss minimization will output a classifier of excess error larger (with high probability over samples of size at least m( )).\nProof idea To prove the above result, we define a subclass of Pα,η ⊆ Pβ consisting of well structured distributions. We then show that for every hinge parameter τ and every bound on the noise η, there is a distribution D̃ ∈ Pα,η on which τ -hinge loss minimization is not consistent.\nw*\nw\n⍺\n⍺\nA\nA\n⍺/2\nhw hw*\nB\nBD\nD\nFigure 1: Pα,η\nIn the remainder of this section, we use the notation hw for the classifier associated with a vector w ∈ B1, that is hw(x) = sign(w · x), since for our geometric construction it is convenient to differentiate between the two. We define a family Pα,η ⊆ Pβ of distributions D̃α,η, indexed by an angle α and a noise parameter η as follows. Let the Bayes optimal classifier be linear h∗ = hw∗ for a unit vector w∗. Let hw be the classifier that is defined by the unit vector w at angle α from w∗. We partition the unit ball into areas A, B and D as in the Figure 5. That is A consists of the two wedges of disagreement between hw and hw∗ and the wedge where the two classifiers agree is divided into B (points that are closer to hw than to hw∗) and D\n(points that are closer to hw∗ than to hw). We now flip the labels of all points in A and B with probability η = 1−β2 and leave the labels deterministic according to hw∗ in the area D.\nMore formally, points at angle between α/2 and π/2 and points at angle between π + α/2 and −π/2 from w∗ are labeled per hw∗(x) with conditional label probability 1. All other points are labeled −hw∗(x)\nwith probability η and hw∗(x) with probability (1 − η). Clearly, this distribution satisfies Massart noise conditions in Equation 1 with parameter β.\nThe goal of the above construction is to design distributions where vectors along the direction of w have smaller hinge loss of those along the direction of w∗. Observe that the noise in the are A will tend to “even out” the difference in hinge loss between w and w∗ (since are A is symmetric with respect to these two directions). The noise in area B however will “help w”: Since all points in area B are closer to the hyperplane defined by w than to the one defined by w∗, vector w∗ will pay more in hinge loss for the noise in this area. In the corresponding area D of points that are closer to the hyperplane defined by w∗ than to the one defined by w we do not add noise, so the cost for both w and w∗ in this area is small.\nWe show that for every α, from a certain noise level η on, w∗(or any other vector in its direction) is not the expected hinge minimizer on D̃α,η. We then argue that thereby hinge loss minimization will not approximate w∗ arbitrarily close in angle and can therefore not achieve arbitrarily small excess 0/1-error. Overall, we show that for every (arbitrarily small) bound on the noise η0 and hinge parameter τ0, we can choose an angle α such that τ0-hinge loss minimization is not consistent for distribution D̃α,η0 . The details of the proof can be found in the Appendix, Section D."
    }, {
      "heading" : "6 Conclusions",
      "text" : "Our work is the first to provide a computationally efficient algorithm under the Massart noise model, a distributional assumption that has been identified in statistical learning to yield fast (statistical) rates of convergence. While both computational and statistical efficiency is crucial in machine learning applications, computational and statistical complexity have been studied under disparate sets of assumptions and models. We view our results on the computational complexity of learning under Massart noise also as a step towards bringing these two lines of research closer together. We hope that this will spur more work identifying situations that lead to both computational and statistical efficiency to ultimately shed light on the underlying connections and dependencies of these two important aspects of automated learning.\nAcknowledgments This work was supported in part by NSF grants CCF-0953192, CCF-1451177, CCF1422910, a Sloan Research Fellowshp, a Microsoft Research Faculty Fellowship, and a Google Research Award."
    }, {
      "heading" : "A Probability Lemmas For The Uniform Distribution",
      "text" : "The following probability lemmas are used throughout this work. Variation of these lemmas are presented in previous work in terms of their asymptotic behavior [2, 4, 22]. Here, we focus on finding bounds that are tight even when the constants are concerned. Indeed, the improved constants in these bounds are essential to tolerating Massart noise with β > 1− 3.6× 10−6.\nThroughout this section, let D be the uniform distribution over a d-dimensional ball. Let f(·) indicate the p.d.f. of D. For any d, let Vd be the volume of a d-dimensional unit ball. Ratios between volumes of the unit ball in different dimensions are commonly used to find the probability mass of different regions under the uniform distribution. Note that for any d\nVd−2 Vd = d 2π .\nThe following bound due to [8] proves useful in our analysis.√ d\n2π ≤ Vd−1 Vd ≤ √ d+ 1 2π\nThe next lemma provides an upper and lower bound for the probability mass of a band in uniform distribution.\nLemma 4. Let u be any unit vector in <d. For all a, b ∈ [− C√ d , C√ d ], such that C < d/2, we have\n|b− a|2−C Vd−1 Vd ≤ Pr x∼D [u · x ∈ [a, b]] ≤ |b− a|Vd−1 Vd .\nProof. We have\nPr x∼D [u · x ∈ [a, b]] = Vd−1 Vd ∫ b a (1− z2)(d−1)/2 dz.\nFor the upper bound, we note that the integrant is at most 1, so Prx∼D[u · x ∈ [a, b]] ≤ Vd−1Vd |b − a| . For the lower bound, note that since a, b ∈ [− C√\nd , C√ d ], the integrant is at least (1 − Cd ) (d−1)/2. We know that\nfor any x ∈ [0, 0.5], 1 − x > 4−x. So, assuming that d > 2C, (1 − Cd ) (d−1)/2 ≥ 4− C d (d−1)/2 ≥ 2−C Prx∼D[u · x ∈ [a, b]] ≥ |b− a|2−C Vd−1Vd .\nLemma 5. Let u and v be two unit vectors in <d and let α = θ(u, v). Then,\nPr x∼D [sign(u · x) 6= sign(w · x) and |u · x| > c α√ d ] ≤ α π e− c2(d−2) 2d\nProof. Without the loss of generality, we can assume u = (1, 0, . . . , 0) and w = (cos(α), sin(α), 0, . . . , 0). Consider the projection of D on the first 2 coordinates. Let E be the event we are interested in. We first show that for any x = (x1, x2) ∈ E, ‖x‖2 > c/ √ d. Consider x1 ≥ 0 (the other case is symmetric). If x ∈ E, it must be that ‖x‖2 sin(α) ≥ cα√d . So, ‖x‖2 = c α sin(α) √ d ≥ c√ d .\nNext, we consider a circle of radius c√ d < r < 1 around the center, indicated by S(r). Let A(r) = S(r) ∩ E be the arc of such circle that is in E. Then the length of such arc is the arc-length that falls in the disagreement region, i.e., rα, minus the arc-length that falls in the band of width cα√\nd . Note, that for every\nx ∈ A(r), ‖x‖2 = r, so f(x) = Vd−2Vd (1− ‖x‖ 2)(d−2)/2 = Vd−2 Vd (1− r2)(d−2)/2.\nPr x∼D [sign(u · x) 6=sign(w · x) and |u · x| > α√ d ] = 2 ∫ 1 c√ d (rα− cα√ d )f(r) dr\n= 2 ∫ √d/c 1 ( rc√ d α− cα√ d )f( cr√ d ) c√ d dr (change of variable z = r √ d/c )\n= 2 Vd−2 Vd\nc2α\nd ∫ √d/c 1 (r − 1)(1− c 2r2 d )(d−2)/2 dr\n= c2α\nπ ∫ √d/c 1 (r − 1)e− r2(d−2) 2d dr\n≤ c 2α\nπ ∫ √d 1 (r − 1) (d−2)c2r\nd\n(−1)(−(d− 2)c 2r\nd )e−\n(d−2)c2r2 2d dr\n≤ α π ∫ √d/c 1 (−1)(−(d− 2)c 2r d )e− (d−2)c2r2 2d dr\n≤ α π\n[ − e− (d−2)r2 2d ]r=√d/c r=1\n≤ α π (e− c2(d−2) 2d − e−(d−2)/2) ≤ α π e− c2(d−2) 2d"
    }, {
      "heading" : "B Proofs of Margin-based Lemmas",
      "text" : "Proof of Lemma 1 Let L(w∗) = E(x,y)∼Dk`(w ∗, x, y), τ = τk, and b = bk−1. First note that for our choice of b ≤ 2.3463× 0.0121608 1√ d , using Lemma 4 we have that\nPr x∼D\n[|wk−1 · x| < b] ≥ 2 b× 2−0.285329.\nNote that L(w∗) is maximized when w∗ = wk−1. Then\nL(w∗) ≤ 2 ∫ τ 0 (1− a τ )f(a) da Prx∼D[|wk−1 · x| < b] ≤ ∫ τ 0 (1− a τ )(1− a 2)−(d−1)/2 da b 2−0.285329 .\nFor the numerator:∫ τ 0 (1− a τ )(1− a2)−(d−1)/2 da ≤ ∫ τ 0 (1− a τ )e−a 2(d−1)/2 da\n≤ 1 2 ∫ τ −τ e−a 2(d−1)/2 da− 1 τ ∫ τ 0 ae−a 2(d−1)/2 da\n≤ √\nπ\n2(d− 1) erf\n( τ √ d− 1\n2\n) − 1\n(d− 1)τ (1− e−(d−1)τ2/2)\n≤ √\nπ\n2(d− 1)\n√ 1− e−τ2(d−1) − 1\n(d− 1)τ\n( (d− 1)τ2\n2 − 1 2 ( (d− 1)τ2 2 )2 )\n(By Taylor expansion)\n≤ τ √ π\n2 − τ 2 + 1 8 (d− 1)τ3\n≤ τ(0.5462 + 1 8 (d− 1)τ2) ≤ 0.5463τ (By 1 8 (d− 1)τ2 < 2× 10−4)\nWhere the last inequality follows from the fact that for our choice of parameters τ ≤ √ 2.50306(3.6×10−6)1/4b√\nd <\n0.003√ d , so 18(d− 1)τ 2 < 10−5. Therefore,\nL(w∗) ≤ 0.5463× 20.285329 τ b ≤ 0.665769τ b .\nProof of Lemma 3 Note that the convex loss minimization procedure returns a vector vk that is not necessarily normalized. To consider all vectors in B(wk−1, αk), at step k, the optimization is done over all\nvectors v (of any length) such that ‖wk−1 − v‖ < αk. For all k, αk < 0.038709π (or 0.0121608), so ‖vk‖2 ≥ 1− 0.0121608, and as a result `(wk,W ) ≤ 1.13844 `(vk,W ). We have,\nerrDk(wk) ≤ E(x,y)∼Dk`(wk, x, y) ≤ E(x,y)∼D̃k`(wk, x, y) + ( 1.092 √ 2 √ 1− β bk−1 τk ) (By Lemma 2)\n≤ `(wk,W ) + 1.092 √ 2 √\n1− β bk−1 τk + 10−8 (By Equation 3)\n≤ 1.13844 `(vk,W ) + 1.092 √ 2 √\n1− β bk−1 τk + 10−8 (By ‖vk‖2 ≥ 1− 0.0121608)\n≤ 1.13844 `(w∗,W ) + 1.092 √ 2 √\n1− β bk−1 τk + 2.14× 10−8 (By vk minimizing the hinge-loss)\n≤ 1.13844 E(x,y)∼D̃k`(w ∗, x, y) + 1.092\n√ 2 √\n1− β bk−1 τk + 3.28× 10−8 (By Equation 3)\n≤ 1.13844 E(x,y)∼Dk`(w ∗, x, y) + 2.13844\n( 1.092 √ 2 √\n1− β bk−1 τk\n) + 3.28× 10−6 (By Lemma 2)\n≤ 0.757941 τk bk−1\n+ 3.303 √\n1− β bk−1 τk + 3.28× 10−8 (By Lemma 1)\nLemma 6. For any constant c′, there is mk ∈ O(d(d + log(k/d))) such that for a randomly drawn set W of mk labeled samples from D̃k, with probability 1− δk+k2 , for any w ∈ B(wk−1, αk),\n|E(x,y)∼D̃k (`(w, x, y)− `(w,W )) | ≤ c ′,\n|E(x,y)∼Dk (`(w, x, y)− `(w, cleaned(W ))) | ≤ c ′.\nProof. By Lemma H.3 of [2], `(w, x, y) = O( √ d) for all (x, y) ∈ Swk−1,bk−1 and θ(w,wk−1) ≤ rk. We get the result by applying Lemma H.2 of [2].\nC Initialization\nWe initialize our margin based procedure with the algorithm from [27]. The guarantees mentioned in [27] hold as long as the noise rate is η ≤ c 2log 1/ . [27] do not explicitly compute the constant but it is easy to check that c ≤ 1256 . This can be computed from inequality 17 in the proof of Lemma 16 in [27]. We need the l.h.s. to be at least 2/2. On the r.h.s., the first term is lower bounded by 2/512. Hence, we need the second term to be at most 255512 2. The second term is upper bounded by 4c2 2. This implies that c ≤ 1/256."
    }, {
      "heading" : "D Hinge Loss Minimization",
      "text" : "In this section, we show that hinge loss minimization is not consistent in our setup, that is, that it does not lead to arbitrarily small excess error. We let Bd1 denote the unit ball in R\nd. In this section, we will only work with d = 2, thus we set B1 = B21 .\nRecall that the τ -hinge loss of a vector w ∈ <d on an example (x, y) ∈ <d × {−1, 1} is defined as follows:\n`τ (w, x, y) = max\n{ 0, 1− y(w · x)\nτ } For a distribution D̃ over <d × {−1, 1}, we let LD̃τ denote the expected hinge loss over D, that is\nLD̃τ (w) = E(x,y)∼D̃`τ (w, x, y).\nIf clear from context, we omit the superscript and write Lτ (w) for LD̃τ (w). Let Aτ be the algorithm that minimizes the empirical τ -hinge loss over a sample. That is, for W = {(x1, y1), . . . , (xm, ym)}, we have\nAτ (W ) ∈ argminw∈B1 1 |W | ∑\n(x,y)∈W\n`τ (w, x, y).\nHinge loss minimization over halfspaces converges to the optimal hinge loss over all halfspace (it is “hinge loss consistent”). That is, for all > 0 there is a sample size m( ) such that for all distributions D̃, we have\nEW∼D̃m [L D̃ τ (Aτ (W ))] ≤ min w∈B1 LD̃τ (w) + .\nIn this section, we show that this does not translate into an agnostic learning guarantee for halfspaces with respect to the 0/1-loss. Moreover, hinge loss minimization is not even consistent with respect to the 0/1-loss even when restricted to a rather benign classes of distributions P . Let Pβ be the class of distributions D̃ with uniform marginal over the unit ball in <2, the Bayes classifier being a halfspace w, and satisfying the Massart noise condition with parameter β. We show that there is a distribution D̃ ∈ Pβ and an ≥ 0 and a sample size m0 such that hinge loss minimization will output a classifier of excess error larger than on expectation over samples of size larger than m0. More precisely, for all m ≥ m0:\nEW∼D̃m [L D̃ τ (Aτ (W ))] > min\nw∈B1 errD̃(w) + .\nFormally, our lower bound for hinge loss minimization is stated as follows.\nTheorem 3 (Restated). For every hinge-loss parameter τ ≥ 0 and every Massart noise parameter 0 ≤ β < 1, there exists a distribution D̃τ,β ∈ Pβ (that is, a distribution overB1×{−1, 1} with uniform marginal over B1 ⊆ <2 satisfying the β-Massart condition) such that τ -hinge loss minimization is not consistent on Pτ,β with respect to the class of halfspaces. That is, there exists an ≥ 0 and a sample size m( ) such that hinge loss minimization will output a classifier of excess error larger than (with high probability over samples of size at least m( )).\nIn the section, we use the notation hw for the classifier associated with a vector w ∈ B1, that is hw(x) = sign(w · x), since for our geometric construction it is convenient to differentiate between the two. The rest of this section is devoted to proving the above theorem.\nA class of distributions\nLet η = 1−β2 . We define a family Pα,η ⊆ Pβ of distributions D̃α,η, indexed by an angle α and a noise parameter η as follows. We let the marginal be uniform over the unit ball B1 ⊆ <2 and let the Bayes optimal classifier be linear h∗ = hw∗ for a unit vector w∗. Let hw be the classifier that is defined by the unit vector w at angle α from w∗. We partition the unit ball into areas A, B and D as in the Figure 2. That is A consists of the two wedges of disagreement between hw and hw∗ and the wedge where the two classifiers agree is divided in B (points that are closer to hw than to hw∗) and D (points that are closer to hw∗ than to hw). We now “add noise η” at all points in areas A and B and leave the labels deterministic according to hw∗ in the area D.\nMore formally, points at angle between α/2 and π/2 and points at angle between π + α/2 and −π/2 from w∗ are labeled with hw∗(x) with (conditional) probability 1. All other points are labeled −hw∗(x) with probability η and hw∗(x) with probability (1− η).\nUseful lemmas\nThe following lemma relates the τ -hinge loss of unit length vectors to the hinge loss of arbitrary vectors in the unit ball. It will allow us to focus our attention to comparing the τ -hinge loss of unit vectors for τ > τ0, instead of having to argue about the τ0 hinge loss of vectors of arbitrary norms in B1.\nLemma 7. Let τ > 0 and 0 < λ ≤ 1. Letw andw∗ be two vectors of unit length. ThenLτ (λw) < Lτ (λw∗) if and only if Lτ/λ(w) < Lτ/λ(w∗).\nProof. By the definition of the hinge loss, we have\n`τ (λw, x, y) = max\n( 0, 1− y(λw · x)\nτ\n) = max ( 0, 1− y(w · x)\nτ/λ\n) = `τ/λ(w, x, y).\nLemma 8. Let τ > 0, for any D̃ ∈ Pα,η let wτ denote the halfspace that minimizes the τ -hinge loss with respect to D̃. If θ(w∗, wτ ) > 0, then hinge loss minimization is not consistent for the 0/1-loss.\nProof. First we show that the hinge loss minimizer is never the vector 0. Note that LD̃τ (0) = 1 (for all τ > 0). Consider the case τ ≥ 1, we show that w∗ has τ -hinge loss strictly smaller than 1. Integrating the hinge loss over the unit ball using polar coordinates, we get\nLD̃τ (w∗) < 2\nπ\n( (1− η) ∫ 1 0 ∫ π 0 (1− z τ sin(ϕ)) z dϕ dz + η ∫ 1 0 ∫ π 0 (1 + z τ sin(ϕ)) z dϕ dz ) = 2\nπ\n( (1− η) ∫ 1 0 ∫ π 0 z − z 2 τ sin(ϕ) dϕ dz + η ∫ 1 0 ∫ π 0 z + z2 τ sin(ϕ) dϕ dz ) = 1 + 2\nπ\n( (1− 2η) ∫ 1 0 ∫ π 0 −z 2 τ sin(ϕ) dϕ dz ) = 1− 2\nπ\n( (1− 2η) ∫ 1 0 ∫ π 0 z2 τ sin(ϕ) dϕ dz ) < 1.\nFor the case of τ < 1, we have Lτ (τw∗) = L1(w∗) < 1.\nThus, (0, 0) is not the hinge-minimizer. Then, by the assumption of the lemma wτ has some positive angle γ to the w∗. Furthermore, for all 0 ≤ λ ≤ 1, LD̃τ (wτ ) < LD̃τ (λw∗). Since w 7→ LD̃τ (w) is a continuous function we can choose an > 0 such that\nLD̃τ (wτ ) + /2 < LD̃τ (λw∗)− /2.\nfor all 0 ≤ λ ≤ 1 (note that the set {λw∗ | 0 ≤ λ ≤ 1} is compact). Now, we can choose an angle µ < γ such that for all vectors v at angle at most µ from w∗, we have\nLD̃τ (v) ≥ min 0≤λ≤1 LD̃τ (λw∗)− /2\nSince hinge loss minimization will eventually (in expectation over large enough samples) output classifiers of hinge loss strictly smaller than LD̃τ (wτ ) + /2, it will then not output classifiers of angle smaller than µ to w∗. By Equation 2, for all w, errD̃(w)− errD̃(w ∗) > β θ(w,w ∗)\nπ , therefore, the excess error of a the classfier returned by hinge loss minimization is lower bounded by a constant β µπ . Thus, hinge loss minimization is not consistent with respect to the 0/1-loss.\nProof of Theorem 3\nWe will show that, for every bound on the noise η0 and for every every τ0 ≥ 0 there is an α0 > 0, such that the unit length vector w has strictly lower τ -hinge loss than the unit length vector w∗ for all τ ≥ τ0. By Lemma 7, this implies that for every bound on the noise η0 and for every τ0 there is an α0 > 0 such that for all 0 < λ ≤ 1 we have Lτ0(λw) < Lτ0(λw∗). This implies that the hinge minimizer is not a multiple of w∗ and so is at a positive angle to w∗. Now Lemma 8 tells us that hinge loss minimization is not consistent for the 0/1-loss.\nw*\nw\n⍺\n⍺\nA\nA\n⍺/2\nhw hw*\nB\nBD\nD\nIn the sequel, we will now focus on the unit length vectors w and w∗ and show how to choose α0 as a function of τ0 and η0. We let cA denote the hinge loss of hw∗ on one wedge (one half of) area A when the labels are correct and dA that hinge loss on that same area when the labels are not correct. Analogously, we define cB,dB, cD and dD. For example, for τ ≥ 1, we have (integrating the hinge loss over the unit ball using polar coordinates)\nNow we can express the hinge loss of both hw∗ and hw in terms of these quantities. For hw∗ we have"
    }, {
      "heading" : "Lτ (hw∗) = 2 · (η(dA + dB) + (1− η)(cA + cB) + cD) .",
      "text" : "For hw, note that area B relates to hw as area D relates to hw∗ (and vice versa). Thus, the roles of B and D are exchanged for hw. That is, for example, for the noisy version of area B the classifier hw pays dD. We have\nLτ (hw) = 2 · (η(cA + dD) + (1− η)(dA + cD) + cB) .\nThis yields\nLτ (hw)− Lτ (hw∗) = 2 · ((1− 2η)(dA− cA)− η((dB− cB)− (dD− cD))) .\nWe now define area C as the points at angle between π−α/2 and π+α/2 from w∗ (See Figure 3). We let cC and dC be defined analogously to the above.\nNote that dA + dB− dD = dC and cA + cB− cD = cC. Thus we get\nLτ (hw)− Lτ (hw∗) =2 · ((1− 2η)(dA− cA)− η((dB− cB)− (dD− cD))) =2 · ((1− η)(dA− cA)− η((dB− cB) + (dA− cA)− (dD− cD))) =2 · ((1− η)(dA− cA)− η((dC− cC))) .\nIf η > η(α, τ) := (dA−cA)(dA−cA)+(dC−cC) , then we get Lτ (hw) − Lτ (hw∗) < 0 and thus hw having smaller hinge loss than hw∗ . Thus, η(α, τ) signifies the amount of noise from which onward, w will have smaller hinge loss than w∗\nGiven τ0 ≥ 0, choose α small enough (we can always choose the angle α sufficiently small for this) so that the area A is included in the τ0-band around w∗. We have for all τ ≥ τ0:\n(dA− cA) = 2 π ∫ 1 0 ∫ α 0 z2 τ sin(ϕ) dϕ dz\n= 2\n3π ∫ α 0 1 τ sin(ϕ) dϕ\n= 2\n3πτ [− cos(ϕ)]α0\n= 2\n3πτ (1− cos(α)).\nFor the area C we now consider the case of τ ≥ 1 and τ < 1 separately. For τ ≥ 1 we get\n(dC− cC) = 4 π ∫ 1 0 ∫ π 2\nπ−α 2\nz2\nτ sin(ϕ) dϕ dz\n= 4\n3π\n∫ π 2\nπ−α 2\n1 τ sin(ϕ) dϕ\n= 4\n3πτ cos\n( π − α\n2 ) = 4 3πτ sin (α 2 ) .\nThus, for τ ≥ 1 we get\nη(α, τ) = (dA− cA)\n(dA− cA) + (dC + cC) = 1− cos(α) 1− cos(α) + 2 sin(α/2) .\nWe call this quantity η1(α) since, given that τ ≥ 1, it does not depend on τ :\nη1(α) = (dA− cA)\n(dA− cA) + (dC + cC) = 1− cos(α) 1− cos(α) + 2 sin(α/2) .\nObserve that limα→0 η1(α) = 0. This will yield the first condition on the angle α: Given some bound on the allowed noise η0, we can choose an α small enough so that η1(α) ≤ η0/2. Then, for the distribution D̃α,η0 we have Lτ (w) < Lτ (w∗) for all τ ≥ 1.\nWe now consider the case τ < 1. For this case we lower bound (dC− cC) as follows. We have\ndC = 2\nπ ∫ 1 0 ∫ π 2\nπ−α 2\nz + z2\nτ sin(ϕ) dϕ dz\n= α\n2π +\n2\nπ ∫ 1 0 ∫ π 2\nπ−α 2\nz2\nτ sin(ϕ) dϕ dz\n= α\n2π +\n2 3τπ sin (α 2 ) .\nhw*\n⍺/2C\n\uD835\uDF0F T\nFigure 5: Area T\nWe now provide an upper bound on cC by integrating over a the triangular shape T (see Figure 4). Note that this bound on cC is actually exact if τ ≤ cos(α/2) and only a strict upper bound for cos(α/2) < τ < 1. We have\ncC ≤ (cT ) = 2 π · ∫ τ 0 (1− z τ )(z tan(α/2)) dz\n= 2 π · ∫ τ 0 z tan(α/2)− z 2 τ tan(α/2) dz = τ2\n3π tan (α 2 ) .\nThus we get\n(dC− cC) ≥ (dC− (cT )) = 1 π\n( α\n2 +\n2 3τ sin (α 2 ) − τ 2 3 tan (α 2 )) .\nThis yields, for the case τ ≤ 1\nη(α, τ) = 2 3(1− cos(α))\n2 3(1− cos(α)) + 2 3 sin(α) + ατ 2 −\nτ3\n3 tan( α 2 )\nWe call this quantity η2(α, τ) to differentiate it from η1(α). Again, it is easy to show that we have limα→0 η2(α, τ) = 0 for every τ . Thus, for a fixed τ0, we can choose an angle α small enough so that Lτ0(w) ≤ Lτ0(w∗).\nTo argue that we will then also have Lτ (w) ≤ Lτ (w∗) for all τ ≥ τ0, we show that, for a fixed angle α, the function η(α, τ) gets smaller as τ grows. For this, it suffices to show that g(τ) = τ α2 − τ3 3 tan( α 2 ) is monotonically increasing with τ for τ ≤ 1. We have\ng′(τ) = α 2 − τ\n2\n2 tan (α 2 ) .\nSince we have τ2 ≤ 1 and 2α tan(α2 ) ≥ 1 for 0 ≤ α ≤ π/3, we get that (for sufficiently small α) g′(τ) ≥ 0 and thus g(τ) is monotonically increasing for 0 ≤ τ ≤ 1 as desired.\nSummarizing, for a given τ0 and η0, we can always choose α0 sufficiently small so that both η1(α0) < η02 and η2(α0, τ) < η02 for all τ ≥ τ0 and thus L D̃α0,η0 τ (w) < L D̃α0,η0 τ (w∗) for all τ ≥ τ0. This completes the proof."
    } ],
    "references" : [ {
      "title" : "The hardness of approximate optima in lattices, codes, and systems of linear equations",
      "author" : [ "Sanjeev Arora", "László Babai", "Jacques Stern", "Z. Sweedyk" ],
      "venue" : "In Proceedings of the 34th IEEE Annual Symposium on Foundations of Computer Science (FOCS),",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 1993
    }, {
      "title" : "The power of localization for efficiently learning linear separators with noise",
      "author" : [ "Pranjal Awasthi", "Maria Florina Balcan", "Philip M. Long" ],
      "venue" : "In Proceedings of the 46th Annual ACM Symposium on Theory of Computing (STOC),",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2014
    }, {
      "title" : "Agnostic active learning",
      "author" : [ "Maria-Florina Balcan", "Alina Beygelzimer", "John Langford" ],
      "venue" : "In Proceedings of the 23rd International Conference on Machine Learning (ICML),",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2006
    }, {
      "title" : "Margin based active learning",
      "author" : [ "Maria-Florina Balcan", "Andrei Z. Broder", "Tong Zhang" ],
      "venue" : "In Proceedings of the 20th Annual Conference on Learning Theory (COLT),",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2007
    }, {
      "title" : "Statistical active learning algorithms",
      "author" : [ "Maria-Florina Balcan", "Vitaly Feldman" ],
      "venue" : "In Advances in Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2013
    }, {
      "title" : "Minimizing the misclassification error rate using a surrogate convex loss",
      "author" : [ "Shai Ben-David", "David Loker", "Nathan Srebro", "Karthik Sridharan" ],
      "venue" : "In Proceedings of the 29th International Conference on Machine Learning (ICML),",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2012
    }, {
      "title" : "A polynomial-time algorithm for learning noisy linear threshold functions",
      "author" : [ "Avrim Blum", "Alan Frieze", "Ravi Kannan", "Santosh Vempala" ],
      "venue" : null,
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 1998
    }, {
      "title" : "The simplex method, volume 1 of Algorithms and Combinatorics: Study and Research Texts",
      "author" : [ "Karl-Heinz Borgwardt" ],
      "venue" : null,
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 1987
    }, {
      "title" : "Theory of classification: a survey of recent advances",
      "author" : [ "Olivier Bousquet", "Stéphane Boucheron", "Gabor Lugosi" ],
      "venue" : "ESAIM: Probability and Statistics,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2005
    }, {
      "title" : "Minimax bounds for active learning",
      "author" : [ "Rui M. Castro", "Robert D. Nowak" ],
      "venue" : "In Proceedings of the 20th Annual Conference on Learning Theory, (COLT),",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2007
    }, {
      "title" : "An Introduction to Support Vector Machines and Other Kernel-based Learning Methods",
      "author" : [ "Nello Cristianini", "John Shawe-Taylor" ],
      "venue" : null,
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2000
    }, {
      "title" : "From average case complexity to improper learning complexity",
      "author" : [ "Amit Daniely", "Nati Linial", "Shai Shalev-Shwartz" ],
      "venue" : "In Proceedings of the 46th Annual ACM Symposium on Theory of Computing (STOC),",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2014
    }, {
      "title" : "Coarse sample complexity bounds for active learning",
      "author" : [ "Sanjoy Dasgupta" ],
      "venue" : "In Advances in Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2005
    }, {
      "title" : "Active learning",
      "author" : [ "Sanjoy Dasgupta" ],
      "venue" : "Encyclopedia of Machine Learning,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2011
    }, {
      "title" : "A general agnostic active learning algorithm",
      "author" : [ "Sanjoy Dasgupta", "Daniel Hsu", "Claire Monteleoni" ],
      "venue" : "In Advances in Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2007
    }, {
      "title" : "Selective sampling and active learning from single and multiple teachers",
      "author" : [ "Ofer Dekel", "Claudio Gentile", "Karthik Sridharan" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2012
    }, {
      "title" : "Selective sampling using the query by committee algorithm",
      "author" : [ "Yoav Freund", "H. Sebastian Seung", "Eli Shamir", "Naftali Tishby" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 1997
    }, {
      "title" : "Hardness of learning halfspaces with noise",
      "author" : [ "Venkatesan Guruswami", "Prasad Raghavendra" ],
      "venue" : "In Proceedings of the 47th Annual IEEE Symposium on Foundations of Computer Science (FOCS),",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2006
    }, {
      "title" : "A bound on the label complexity of agnostic active learning",
      "author" : [ "Steve Hanneke" ],
      "venue" : "In Proceedings of the 24rd International Conference on Machine Learning (ICML),",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2007
    }, {
      "title" : "Theory of disagreement-based active learning",
      "author" : [ "Steve Hanneke" ],
      "venue" : "Foundations and Trends in Machine Learning,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2014
    }, {
      "title" : "Surrogate losses in passive and active learning",
      "author" : [ "Steve Hanneke", "Liu Yang" ],
      "venue" : "CoRR, abs/1207.3772,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2014
    }, {
      "title" : "Agnostically learning halfspaces",
      "author" : [ "Adam Tauman Kalai", "Adam R. Klivans", "Yishay Mansour", "Rocco A. Servedio" ],
      "venue" : "SIAM Journal on Computing,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2008
    }, {
      "title" : "On agnostic boosting and parity learning",
      "author" : [ "Adam Tauman Kalai", "Yishay Mansour", "Elad Verbin" ],
      "venue" : "In Proceedings of the 40th Annual ACM Symposium on Theory of Computing (STOC),",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2008
    }, {
      "title" : "Learning in the presence of malicious errors (extended abstract)",
      "author" : [ "Michael J. Kearns", "Ming Li" ],
      "venue" : "In Proceedings of the 20th Annual ACM Symposium on Theory of Computing (STOC),",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 1988
    }, {
      "title" : "Toward efficient agnostic learning",
      "author" : [ "Michael J. Kearns", "Robert E. Schapire", "Linda Sellie" ],
      "venue" : "In Proceedings of the 5th Annual Conference on Computational Learning Theory (COLT),",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 1992
    }, {
      "title" : "Embedding hard learning problems into gaussian space. In Approximation, Randomization, and Combinatorial Optimization. Algorithms and Techniques",
      "author" : [ "Adam R. Klivans", "Pravesh Kothari" ],
      "venue" : "(AP- PROX/RANDOM),",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2014
    }, {
      "title" : "Learning halfspaces with malicious noise",
      "author" : [ "Adam R. Klivans", "Philip M. Long", "Rocco A. Servedio" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2009
    }, {
      "title" : "Risk bounds for statistical learning",
      "author" : [ "Pascal Massart", "lodie Ndlec" ],
      "venue" : "The Annals of Statistics, 34(5):2326–2366,",
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 2006
    }, {
      "title" : "A formal model of hierarchical concept learning",
      "author" : [ "Ronald L. Rivest", "Robert H. Sloan" ],
      "venue" : "Information and Computation,",
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 1994
    }, {
      "title" : "Efficient algorithms in computational learning theory",
      "author" : [ "Rocco A. Servedio" ],
      "venue" : "Harvard University,",
      "citeRegEx" : "30",
      "shortCiteRegEx" : "30",
      "year" : 2001
    } ],
    "referenceMentions" : [ {
      "referenceID" : 10,
      "context" : "In the absence of noise (when the data is realizable) such algorithms exist via linear programming [11].",
      "startOffset" : 99,
      "endOffset" : 103
    }, {
      "referenceID" : 6,
      "context" : "Such strong guarantees are only known for the well studied random classification noise model [7].",
      "startOffset" : 93,
      "endOffset" : 96
    }, {
      "referenceID" : 27,
      "context" : "In this work, we provide the first algorithm that can achieve arbitrarily small excess error, in truly polynomial time, for bounded noise, also called Massart noise [28], a much more realistic and widely studied noise model in statistical learning theory [9].",
      "startOffset" : 165,
      "endOffset" : 169
    }, {
      "referenceID" : 8,
      "context" : "In this work, we provide the first algorithm that can achieve arbitrarily small excess error, in truly polynomial time, for bounded noise, also called Massart noise [28], a much more realistic and widely studied noise model in statistical learning theory [9].",
      "startOffset" : 255,
      "endOffset" : 258
    }, {
      "referenceID" : 6,
      "context" : "example x is flipped independently with equal probability η, several works have provided computationally efficient algorithms that can achieve arbitrarily small excess error in polynomial time [7, 30, 5] — note that all these results crucially exploit the high amount of symmetry present in the RCN noise.",
      "startOffset" : 193,
      "endOffset" : 203
    }, {
      "referenceID" : 29,
      "context" : "example x is flipped independently with equal probability η, several works have provided computationally efficient algorithms that can achieve arbitrarily small excess error in polynomial time [7, 30, 5] — note that all these results crucially exploit the high amount of symmetry present in the RCN noise.",
      "startOffset" : 193,
      "endOffset" : 203
    }, {
      "referenceID" : 4,
      "context" : "example x is flipped independently with equal probability η, several works have provided computationally efficient algorithms that can achieve arbitrarily small excess error in polynomial time [7, 30, 5] — note that all these results crucially exploit the high amount of symmetry present in the RCN noise.",
      "startOffset" : 193,
      "endOffset" : 203
    }, {
      "referenceID" : 24,
      "context" : "At the other extreme, there has been significant work on much more difficult and adversarial noise models, including the agnostic model [25] and malicious noise models [24].",
      "startOffset" : 136,
      "endOffset" : 140
    }, {
      "referenceID" : 23,
      "context" : "At the other extreme, there has been significant work on much more difficult and adversarial noise models, including the agnostic model [25] and malicious noise models [24].",
      "startOffset" : 168,
      "endOffset" : 172
    }, {
      "referenceID" : 22,
      "context" : "The best results here however, not only require additional distributional assumptions about the marginal over the instance space, but they only achieve much weaker multiplicative approximation guarantees [23, 27, 2]; for example, the best result of this form for the case of uniform distribution over the unit sphere Sd−1 achieves excess error cOPT [2], for some large constant c.",
      "startOffset" : 204,
      "endOffset" : 215
    }, {
      "referenceID" : 26,
      "context" : "The best results here however, not only require additional distributional assumptions about the marginal over the instance space, but they only achieve much weaker multiplicative approximation guarantees [23, 27, 2]; for example, the best result of this form for the case of uniform distribution over the unit sphere Sd−1 achieves excess error cOPT [2], for some large constant c.",
      "startOffset" : 204,
      "endOffset" : 215
    }, {
      "referenceID" : 1,
      "context" : "The best results here however, not only require additional distributional assumptions about the marginal over the instance space, but they only achieve much weaker multiplicative approximation guarantees [23, 27, 2]; for example, the best result of this form for the case of uniform distribution over the unit sphere Sd−1 achieves excess error cOPT [2], for some large constant c.",
      "startOffset" : 204,
      "endOffset" : 215
    }, {
      "referenceID" : 1,
      "context" : "The best results here however, not only require additional distributional assumptions about the marginal over the instance space, but they only achieve much weaker multiplicative approximation guarantees [23, 27, 2]; for example, the best result of this form for the case of uniform distribution over the unit sphere Sd−1 achieves excess error cOPT [2], for some large constant c.",
      "startOffset" : 349,
      "endOffset" : 352
    }, {
      "referenceID" : 11,
      "context" : "In fact, recent evidence shows that this is unavoidable for polynomial time algorithms for such adversarial noise models [12].",
      "startOffset" : 121,
      "endOffset" : 125
    }, {
      "referenceID" : 8,
      "context" : "Our Results In this work we identify a realistic and widely studied noise model in the statistical learning theory, the so called Massart noise [9], for which we can prove much stronger guarantees.",
      "startOffset" : 144,
      "endOffset" : 147
    }, {
      "referenceID" : 8,
      "context" : "From a statistical point of view, it is well known that under this model, we can get faster rates compared to worst case joint distributions [9].",
      "startOffset" : 141,
      "endOffset" : 144
    }, {
      "referenceID" : 28,
      "context" : "In computational learning theory, this noise model was also studied, but under the name of malicious misclassification noise [29, 31].",
      "startOffset" : 125,
      "endOffset" : 133
    }, {
      "referenceID" : 29,
      "context" : "As a result, as we show in our work (see Section 4), standard algorithms such as the averaging algorithm [30] which work for random noise can only achieve a much poorer excess error (as a function of η) under Massart noise.",
      "startOffset" : 105,
      "endOffset" : 109
    }, {
      "referenceID" : 1,
      "context" : "Specifically, we analyze a recent margin based algorithm of [2].",
      "startOffset" : 60,
      "endOffset" : 63
    }, {
      "referenceID" : 1,
      "context" : "By using new structural insights, we show that there exists a constant η (independent of the dimension), so that if we use Massart noise where the flipping probability is upper bounded by η, we can use a modification of the algorithm in [2] and achieve arbitrarily small excess error.",
      "startOffset" : 237,
      "endOffset" : 240
    }, {
      "referenceID" : 5,
      "context" : "While there exists earlier work showing that hinge loss minimization can lead to classifiers of large 0/1-loss [6], the lower bounds in that paper employ distributions with significant mass on discrete points with flipped label (which is not possible under Massart noise) at a very large distance from the optimal classifier.",
      "startOffset" : 111,
      "endOffset" : 114
    }, {
      "referenceID" : 18,
      "context" : "One appealing feature of our result is the algorithm we analyze is in fact naturally adaptable to the active learning or selective sampling scenario (intensively studied in recent years [19, 13, 20], where the learning algorithms only receive the classifications of examples when they ask for them.",
      "startOffset" : 186,
      "endOffset" : 198
    }, {
      "referenceID" : 12,
      "context" : "One appealing feature of our result is the algorithm we analyze is in fact naturally adaptable to the active learning or selective sampling scenario (intensively studied in recent years [19, 13, 20], where the learning algorithms only receive the classifications of examples when they ask for them.",
      "startOffset" : 186,
      "endOffset" : 198
    }, {
      "referenceID" : 19,
      "context" : "One appealing feature of our result is the algorithm we analyze is in fact naturally adaptable to the active learning or selective sampling scenario (intensively studied in recent years [19, 13, 20], where the learning algorithms only receive the classifications of examples when they ask for them.",
      "startOffset" : 186,
      "endOffset" : 198
    }, {
      "referenceID" : 3,
      "context" : "We note that prior to our work only inefficient algorithms could achieve the desired label complexity under Massart noise [4, 20].",
      "startOffset" : 122,
      "endOffset" : 129
    }, {
      "referenceID" : 19,
      "context" : "We note that prior to our work only inefficient algorithms could achieve the desired label complexity under Massart noise [4, 20].",
      "startOffset" : 122,
      "endOffset" : 129
    }, {
      "referenceID" : 0,
      "context" : "Related Work The agnostic noise model is notoriously hard to deal with computationally and there is significant evidence that achieving arbitrarily small excess error in polynomial time is hard in this model [1, 18, 12].",
      "startOffset" : 208,
      "endOffset" : 219
    }, {
      "referenceID" : 17,
      "context" : "Related Work The agnostic noise model is notoriously hard to deal with computationally and there is significant evidence that achieving arbitrarily small excess error in polynomial time is hard in this model [1, 18, 12].",
      "startOffset" : 208,
      "endOffset" : 219
    }, {
      "referenceID" : 11,
      "context" : "Related Work The agnostic noise model is notoriously hard to deal with computationally and there is significant evidence that achieving arbitrarily small excess error in polynomial time is hard in this model [1, 18, 12].",
      "startOffset" : 208,
      "endOffset" : 219
    }, {
      "referenceID" : 22,
      "context" : "For this model, under our distributional assumptions, [23] provides an algorithm that learns linear separators in <d to excess error at most , but whose running time poly(dexp(1/ )).",
      "startOffset" : 54,
      "endOffset" : 58
    }, {
      "referenceID" : 25,
      "context" : "Recent work show evidence that the exponential dependence on 1/ is unavoidable in this case [26] for the agnostic case.",
      "startOffset" : 92,
      "endOffset" : 96
    }, {
      "referenceID" : 16,
      "context" : "Over the past decade there has been substantial progress on understanding the underlying statistical principles of active learning, and several general characterizations have been developed for describing when active learning could have an advantage over the classical passive supervised learning paradigm both in the noise free settings and in the agnostic case [17, 13, 3, 4, 19, 15, 10, 14, 20].",
      "startOffset" : 363,
      "endOffset" : 397
    }, {
      "referenceID" : 12,
      "context" : "Over the past decade there has been substantial progress on understanding the underlying statistical principles of active learning, and several general characterizations have been developed for describing when active learning could have an advantage over the classical passive supervised learning paradigm both in the noise free settings and in the agnostic case [17, 13, 3, 4, 19, 15, 10, 14, 20].",
      "startOffset" : 363,
      "endOffset" : 397
    }, {
      "referenceID" : 2,
      "context" : "Over the past decade there has been substantial progress on understanding the underlying statistical principles of active learning, and several general characterizations have been developed for describing when active learning could have an advantage over the classical passive supervised learning paradigm both in the noise free settings and in the agnostic case [17, 13, 3, 4, 19, 15, 10, 14, 20].",
      "startOffset" : 363,
      "endOffset" : 397
    }, {
      "referenceID" : 3,
      "context" : "Over the past decade there has been substantial progress on understanding the underlying statistical principles of active learning, and several general characterizations have been developed for describing when active learning could have an advantage over the classical passive supervised learning paradigm both in the noise free settings and in the agnostic case [17, 13, 3, 4, 19, 15, 10, 14, 20].",
      "startOffset" : 363,
      "endOffset" : 397
    }, {
      "referenceID" : 18,
      "context" : "Over the past decade there has been substantial progress on understanding the underlying statistical principles of active learning, and several general characterizations have been developed for describing when active learning could have an advantage over the classical passive supervised learning paradigm both in the noise free settings and in the agnostic case [17, 13, 3, 4, 19, 15, 10, 14, 20].",
      "startOffset" : 363,
      "endOffset" : 397
    }, {
      "referenceID" : 14,
      "context" : "Over the past decade there has been substantial progress on understanding the underlying statistical principles of active learning, and several general characterizations have been developed for describing when active learning could have an advantage over the classical passive supervised learning paradigm both in the noise free settings and in the agnostic case [17, 13, 3, 4, 19, 15, 10, 14, 20].",
      "startOffset" : 363,
      "endOffset" : 397
    }, {
      "referenceID" : 9,
      "context" : "Over the past decade there has been substantial progress on understanding the underlying statistical principles of active learning, and several general characterizations have been developed for describing when active learning could have an advantage over the classical passive supervised learning paradigm both in the noise free settings and in the agnostic case [17, 13, 3, 4, 19, 15, 10, 14, 20].",
      "startOffset" : 363,
      "endOffset" : 397
    }, {
      "referenceID" : 13,
      "context" : "Over the past decade there has been substantial progress on understanding the underlying statistical principles of active learning, and several general characterizations have been developed for describing when active learning could have an advantage over the classical passive supervised learning paradigm both in the noise free settings and in the agnostic case [17, 13, 3, 4, 19, 15, 10, 14, 20].",
      "startOffset" : 363,
      "endOffset" : 397
    }, {
      "referenceID" : 19,
      "context" : "Over the past decade there has been substantial progress on understanding the underlying statistical principles of active learning, and several general characterizations have been developed for describing when active learning could have an advantage over the classical passive supervised learning paradigm both in the noise free settings and in the agnostic case [17, 13, 3, 4, 19, 15, 10, 14, 20].",
      "startOffset" : 363,
      "endOffset" : 397
    }, {
      "referenceID" : 4,
      "context" : "However, despite many efforts, except for very simple noise models (random classification noise [5] and linear noise [16]), to date there are no known computationally efficient algorithms with provable guarantees in the presence of Massart noise that can achieve arbitrarily small excess error.",
      "startOffset" : 96,
      "endOffset" : 99
    }, {
      "referenceID" : 15,
      "context" : "However, despite many efforts, except for very simple noise models (random classification noise [5] and linear noise [16]), to date there are no known computationally efficient algorithms with provable guarantees in the presence of Massart noise that can achieve arbitrarily small excess error.",
      "startOffset" : 117,
      "endOffset" : 121
    }, {
      "referenceID" : 20,
      "context" : "We note that work of [21] provides computationally efficient algorithms for both passive and active learning under the assumption that the hinge loss (or other surrogate loss) minimizer aligns with the minimizer of the 0/1-loss.",
      "startOffset" : 21,
      "endOffset" : 25
    }, {
      "referenceID" : 1,
      "context" : "The algorithm described above is similar to that of [2] and uses an iterative margin-based approach.",
      "startOffset" : 52,
      "endOffset" : 55
    }, {
      "referenceID" : 26,
      "context" : "We satisfy the base case by using an algorithm of [27].",
      "startOffset" : 50,
      "endOffset" : 54
    }, {
      "referenceID" : 1,
      "context" : "Using these insights, we show that the algorithm by [2] can indeed achieve a much stronger guarantee, namely arbitrarily small excess error in presence of Massart noise.",
      "startOffset" : 52,
      "endOffset" : 55
    }, {
      "referenceID" : 26,
      "context" : "Take poly(d, 1δ ) samples and run poly(d, 1 δ )-time algorithm by [27] to find a half-spacew0 with excess error 0.",
      "startOffset" : 66,
      "endOffset" : 70
    }, {
      "referenceID" : 1,
      "context" : "Overview of our analysis: Similar to [2], we divide errD(wk) to two categories; error in the band, i.",
      "startOffset" : 37,
      "endOffset" : 40
    }, {
      "referenceID" : 1,
      "context" : "1 of [2] for uniform)",
      "startOffset" : 5,
      "endOffset" : 8
    }, {
      "referenceID" : 26,
      "context" : "For k = 0, we use the algorithm for adversarial noise model by [27], which can achieve excess error of if errD̃(w ∗) < 2 256 log(1/ ) (Refer to Appendix C for more details).",
      "startOffset" : 63,
      "endOffset" : 67
    }, {
      "referenceID" : 7,
      "context" : "where the last transition holds by the fact that Vd−1 Vd ≤ √ d+1 2π [8].",
      "startOffset" : 68,
      "endOffset" : 71
    }, {
      "referenceID" : 29,
      "context" : "The Average algorithm introduced by [30] is another computationally efficient algorithm that has provable noise tolerance guarantees under certain noise models and distributions.",
      "startOffset" : 36,
      "endOffset" : 40
    }, {
      "referenceID" : 26,
      "context" : "Furthermore, even in the presence of a small amount of malicious noise and less symmetric distributions, Average has been used to obtain a weak learner, which can then be boosted to achieve a non-trivial noise tolerance [27].",
      "startOffset" : 220,
      "endOffset" : 224
    }, {
      "referenceID" : 5,
      "context" : "It has been shown earlier that hinge loss minimization can lead to classifiers of large 0/1-loss [6].",
      "startOffset" : 97,
      "endOffset" : 100
    }, {
      "referenceID" : 0,
      "context" : "[1] Sanjeev Arora, László Babai, Jacques Stern, and Z.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 1,
      "context" : "[2] Pranjal Awasthi, Maria Florina Balcan, and Philip M.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 2,
      "context" : "[3] Maria-Florina Balcan, Alina Beygelzimer, and John Langford.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 3,
      "context" : "[4] Maria-Florina Balcan, Andrei Z.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 4,
      "context" : "[5] Maria-Florina Balcan and Vitaly Feldman.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 5,
      "context" : "[6] Shai Ben-David, David Loker, Nathan Srebro, and Karthik Sridharan.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 6,
      "context" : "[7] Avrim Blum, Alan Frieze, Ravi Kannan, and Santosh Vempala.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 7,
      "context" : "[8] Karl-Heinz Borgwardt.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 8,
      "context" : "[9] Olivier Bousquet, Stéphane Boucheron, and Gabor Lugosi.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 9,
      "context" : "[10] Rui M.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 10,
      "context" : "[11] Nello Cristianini and John Shawe-Taylor.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 11,
      "context" : "[12] Amit Daniely, Nati Linial, and Shai Shalev-Shwartz.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 12,
      "context" : "[13] Sanjoy Dasgupta.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 13,
      "context" : "[14] Sanjoy Dasgupta.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 14,
      "context" : "[15] Sanjoy Dasgupta, Daniel Hsu, and Claire Monteleoni.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 15,
      "context" : "[16] Ofer Dekel, Claudio Gentile, and Karthik Sridharan.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 16,
      "context" : "[17] Yoav Freund, H.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 17,
      "context" : "[18] Venkatesan Guruswami and Prasad Raghavendra.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 18,
      "context" : "[19] Steve Hanneke.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 19,
      "context" : "[20] Steve Hanneke.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 20,
      "context" : "[21] Steve Hanneke and Liu Yang.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 21,
      "context" : "[22] Adam Tauman Kalai, Adam R.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 22,
      "context" : "[23] Adam Tauman Kalai, Yishay Mansour, and Elad Verbin.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 23,
      "context" : "[24] Michael J.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 24,
      "context" : "[25] Michael J.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 25,
      "context" : "[26] Adam R.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 26,
      "context" : "[27] Adam R.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 27,
      "context" : "[28] Pascal Massart and lodie Ndlec.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 28,
      "context" : "[29] Ronald L.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 29,
      "context" : "[30] Rocco A.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 1,
      "context" : "Variation of these lemmas are presented in previous work in terms of their asymptotic behavior [2, 4, 22].",
      "startOffset" : 95,
      "endOffset" : 105
    }, {
      "referenceID" : 3,
      "context" : "Variation of these lemmas are presented in previous work in terms of their asymptotic behavior [2, 4, 22].",
      "startOffset" : 95,
      "endOffset" : 105
    }, {
      "referenceID" : 21,
      "context" : "Variation of these lemmas are presented in previous work in terms of their asymptotic behavior [2, 4, 22].",
      "startOffset" : 95,
      "endOffset" : 105
    }, {
      "referenceID" : 7,
      "context" : "The following bound due to [8] proves useful in our analysis.",
      "startOffset" : 27,
      "endOffset" : 30
    }, {
      "referenceID" : 1,
      "context" : "3 of [2], `(w, x, y) = O( √ d) for all (x, y) ∈ Swk−1,bk−1 and θ(w,wk−1) ≤ rk.",
      "startOffset" : 5,
      "endOffset" : 8
    }, {
      "referenceID" : 1,
      "context" : "2 of [2].",
      "startOffset" : 5,
      "endOffset" : 8
    }, {
      "referenceID" : 26,
      "context" : "We initialize our margin based procedure with the algorithm from [27].",
      "startOffset" : 65,
      "endOffset" : 69
    }, {
      "referenceID" : 26,
      "context" : "The guarantees mentioned in [27] hold as long as the noise rate is η ≤ c 2 log 1/ .",
      "startOffset" : 28,
      "endOffset" : 32
    }, {
      "referenceID" : 26,
      "context" : "[27] do not explicitly compute the constant but it is easy to check that c ≤ 1 256 .",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 26,
      "context" : "This can be computed from inequality 17 in the proof of Lemma 16 in [27].",
      "startOffset" : 68,
      "endOffset" : 72
    } ],
    "year" : 2015,
    "abstractText" : "We study the learnability of linear separators in < in the presence of bounded (a.k.a Massart) noise. This is a realistic generalization of the random classification noise model, where the adversary can flip each example xwith probability η(x) ≤ η. We provide the first polynomial time algorithm that can learn linear separators to arbitrarily small excess error in this noise model under the uniform distribution over the unit ball in <, for some constant value of η. While widely studied in the statistical learning theory community in the context of getting faster convergence rates, computationally efficient algorithms in this model had remained elusive. Our work provides the first evidence that one can indeed design algorithms achieving arbitrarily small excess error in polynomial time under this realistic noise model and thus opens up a new and exciting line of research. We additionally provide lower bounds showing that popular algorithms such as hinge loss minimization and averaging cannot lead to arbitrarily small excess error under Massart noise, even under the uniform distribution. Our work instead, makes use of a margin based technique developed in the context of active learning. As a result, our algorithm is also an active learning algorithm with label complexity that is only a logarithmic the desired excess error .",
    "creator" : "LaTeX with hyperref package"
  }
}