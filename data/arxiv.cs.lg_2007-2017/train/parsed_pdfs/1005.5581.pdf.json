{
  "name" : "1005.5581.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Multi-View Active Learning in the Non-Realizable Case",
    "authors" : [ "Wei Wang", "Zhi-Hua Zhou" ],
    "emails" : [ "zhouzh@nju.edu.cn" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n00 5.\n55 81\nv2 [\ncs .L\nG ]\n2 9\nO ct\n2 01\nThe sample complexity of active learning under the realizability assumption has been well-studied. The realizability assumption, however, rarely holds in practice. In this paper, we theoretically characterize the sample complexity of active learning in the non-realizable case under multiview setting. We prove that, with unbounded Tsybakov noise, the sample complexity of multiview active learning can be Õ(log 1ǫ ), contrasting to single-view setting where the polynomial improvement is the best possible achievement. We also prove that in general multi-view setting the sample complexity of active learning with unbounded Tsybakov noise is Õ(1ǫ ), where the order of 1/ǫ is independent of the parameter in Tsybakov noise, contrasting to previous polynomial bounds where the order of 1/ǫ is related to the parameter in Tsybakov noise. Key words: active learning, non-realizable case"
    }, {
      "heading" : "1. Introduction",
      "text" : "In active learning [10, 13, 16], the learner draws unlabeled data from the unknown distribution defined on the learning task and actively queries some labels from an oracle. In this way, the active learner can achieve good performance with much fewer labels than passive learning. The number of these queried labels, which is necessary and sufficient for obtaining a good leaner, is well-known as the sample complexity of active learning.\nMany theoretical bounds on the sample complexity of active learning have been derived based on the realizability assumption (i.e., there exists a hypothesis perfectly separating the data in\n∗Corresponding author. Email: zhouzh@nju.edu.cn\nPreprint submitted for review November 1, 2010\nthe hypothesis class) [4, 5, 11, 12, 14, 16]. The realizability assumption, however, rarely holds in practice. Recently, the sample complexity of active learning in the non-realizable case (i.e., the data cannot be perfectly separated by any hypothesis in the hypothesis class because of the noise) has been studied [2, 13, 17]. It is worth noting that these bounds obtained in the non-realizable case match the lower bound Ω(η 2\nǫ2 ) [19], in the same order as the upper bound O( 1 ǫ2 ) of passive\nlearning (η denotes the generalization error rate of the optimal classifier in the hypothesis class and ǫ bounds how close to the optimal classifier in the hypothesis class the active learner has to get). This suggests that perhaps active learning in the non-realizable case is not as efficient as that in the realizable case. To improve the sample complexity of active learning in the non-realizable case remarkably, the model of the noise or some assumptions on the hypothesis class and the data distribution must be considered. Tsybakov noise model [21] is more and more popular in theoretical analysis on the sample complexity of active learning. However, existing result [8] shows that obtaining exponential improvement in the sample complexity of active learning with unbounded Tsybakov noise is hard.\nInspired by [23] which proved that multi-view setting [6] can help improve the sample complexity of active learning in the realizable case remarkably, we have an insight that multi-view setting will also help active learning in the non-realizable case. In this paper, we present the first analysis on the sample complexity of active learning in the non-realizable case under multi-view setting, where the non-realizability is caused by Tsybakov noise. Specifically:\n-We define α-expansion, which extends the definition in [3] and [23] to the non-realizable case,\nand β-condition for multi-view setting.\n-We prove that the sample complexity of active learning with Tsybakov noise under multi-view setting can be improved to Õ(log 1ǫ ) when the learner satisfies non-degradation condition. 1 This exponential improvement holds no matter whether Tsybakov noise is bounded or not, contrasting to single-view setting where the polynomial improvement is the best possible achievement for active learning with unbounded Tsybakov noise.\n-We also prove that, when non-degradation condition does not hold, the sample complexity of active learning with unbounded Tsybakov noise under multi-view setting is Õ(1ǫ ), where the order\n1The Õ notation is used to hide the factor log log( 1 ǫ ).\nof 1/ǫ is independent of the parameter in Tsybakov noise, i.e., the sample complexity is always Õ(1ǫ ) no matter how large the unbounded Tsybakov noise is. While in previous polynomial bounds, the order of 1/ǫ is related to the parameter in Tsybakov noise and is larger than 1 when unbounded Tsybakov noise is larger than some degree (see Section 2). This discloses that, when non-degradation condition does not hold, multi-view setting is still able to lead to a faster convergence rate and our polynomial improvement in the sample complexity is better than previous polynomial bounds when unbounded Tsybakov noise is large.\nThe rest of this paper is organized as follows. After introducing related work in Section 2 and preliminaries in Section 3, we define α-expansion in the non-realizable case in Section 4. Then we analyze the sample complexity of active learning with Tsybakov noise under multi-view setting with and without the non-degradation condition in Section 5 and Section 6, respectively, and verify the improvement in the sample complexity empirically in Section 7. Finally we conclude the paper in Section 8."
    }, {
      "heading" : "2. Related Work",
      "text" : "Generally, the non-realizability of learning task is caused by the presence of noise. For learning the task with arbitrary forms of noise, Balcan et al. [2] proposed the agnostic active learning algorithm A2 and proved that its sample complexity is Ô(η 2\nǫ2 ). 2 Hoping to get tighter bound on\nthe sample complexity of the algorithm A2, Hanneke [17] defined the disagreement coefficient θ, which depends on the hypothesis class and the data distribution, and proved that the sample complexity of the algorithm A2 is Ô(θ2 η 2\nǫ2 ). Later, Dasgupta et al. [13] developed a general\nagnostic active learning algorithm which extends the scheme in [10] and proved that its sample complexity is Ô(θ η 2\nǫ2 ).\nRecently, the popular Tsybakov noise model [21] was considered in theoretical analysis on active learning and there have been some bounds on the sample complexity. For some simple cases, where Tsybakov noise is bounded, it has been proved that the exponential improvement in the sample complexity is possible [4, 7, 18]. As for the situation where Tsybakov noise is unbounded,\n2The Ô notation is used to hide the factor polylog( 1 ǫ ).\nonly polynomial improvement in the sample complexity has been obtained. Balcan et al. [4] assumed that the samples are drawn uniformly from the the unit ball in Rd and proved that the sample complexity of active learning with unbounded Tsybakov noise is O ( ǫ− 2 1+λ ) (λ > 0 depends on Tsybakov noise). This uniform distribution assumption, however, rarely holds in practice. Castro and Nowak [8] showed that the sample complexity of active learning with unbounded Tsybakov noise is Ô ( ǫ − 2µω+d−2ω−1 µω ) (µ > 1 depends on another form of Tsybakov noise, ω ≥ 1 depends on the Hölder smoothness and d is the dimension of the data). This result is also based on the strong uniform distribution assumption. Cavallanti et al. [9] assumed that the labels of examples are generated according to a simple linear noise model and indicated that the sample complexity of active learning with unbounded Tsybakov noise is O ( ǫ − 2(3+λ) (1+λ)(2+λ) ) . Hanneke [18] proved that the algorithms or variants thereof in [2] and [13] can achieve the polynomial sample complexity Ô ( ǫ− 2 1+λ ) for active learning with unbounded Tsybakov noise. For active learning with unbounded Tsybakov noise, Castro and Nowak [8] also proved that at least Ω(ǫ−ρ) labels are requested to learn an ǫ-approximation of the optimal classifier (ρ ∈ (0, 2) depends on Tsybakov noise). This result shows that the polynomial improvement is the best possible achievement for active learning with unbounded Tsybakov noise in single-view setting. Wang [22] introduced smooth assumption to active learning with approximate Tsybakov noise and proved that if the classification boundary and the underlying distribution are smooth to ξ-th order and ξ > d, the sample complexity of active learning is Ô ( ǫ− 2d ξ+d ) ; if the boundary and the distribution are infinitely smooth, the sample complexity of active learning is O ( polylog(1ǫ ) ) . Nevertheless, this result is for approximate Tsybakov noise and the assumption on large smoothness order (or infinite smoothness order) rarely holds for data with high dimension d in practice."
    }, {
      "heading" : "3. Preliminaries",
      "text" : "In multi-view setting, the instances are described with several different disjoint sets of features. For the sake of simplicity, we only consider two-view setting in this paper. Suppose that X = X1 ×X2 is the instance space, X1 and X2 are the two views, Y = {0, 1} is the label space and D is the distribution over X × Y . Suppose that c = (c1, c2) is the optimal Bayes classifier, where c1 and c2 are the optimal Bayes classifiers in the two views, respectively. Let H1 and H2 be the hypothesis class in each view and suppose that c1 ∈ H1 and c2 ∈ H2. For any instance\nx = (x1, x2), the hypothesis hv ∈ Hv (v = 1, 2) makes that hv(xv) = 1 if xv ∈ Sv and hv(xv) = 0 otherwise, where Sv is a subset ofXv . In this way, any hypothesis hv ∈ Hv corresponds to a subset Sv of Xv (as for how to combine the hypotheses in the two views, see Section 5). Considering that x1 and x2 denote the same instance x in different views, we overload Sv to denote the instance set {x = (x1, x2) : xv ∈ Sv} without confusion. Let S ∗ v correspond to the optimal Bayes classifier cv. It is well-known [15] that S ∗ v = {xv : ϕv(xv) ≥ 1 2}, where ϕv(xv) = P (y = 1|xv). Here, we also overload S∗v to denote the instances set {x = (x1, x2) : xv ∈ S ∗ v}. The error rate of a hypothesis Sv under the distribution D is R(hv) = R(Sv) = Pr(x1,x2,y)∈D ( y 6= I(xv ∈ Sv) ) . In general, R(S∗v ) 6= 0 and the excess error of Sv can be denoted as follows, where Sv∆S ∗ v = (Sv −S ∗ v )∪ (S ∗ v −Sv) and d(Sv , S ∗ v) is a pseudo-distance between the sets Sv and S ∗ v .\nR(Sv)−R(S ∗ v ) =\n∫\nSv∆S∗v\n|2ϕv(xv)− 1|pxvdxv , d(Sv, S ∗ v ) (1)\nLet ηv denote the error rate of the optimal Bayes classifier cv which is also called as the noise rate in the non-realizable case. In general, ηv is less than 1 2 . In order to model the noise, we assume that the data distribution and the Bayes decision boundary in each view satisfies the popular Tsybakov noise condition [21] that Prxv∈Xv (|ϕv(xv) − 1/2| ≤ t) ≤ C0t λ for some finite C0 > 0, λ > 0 and all 0 < t ≤ 1/2, where λ = ∞ corresponds to the best learning situation and the noise is called bounded [8]; while λ = 0 corresponds to the worst situation. When λ < ∞, the noise is called unbounded [8]. According to Proposition 1 in [21], it is easy to know that (2) holds.\nd(Sv, S ∗ v ) ≥ C1d k ∆(Sv, S ∗ v ) (2)\nHere k = 1+λλ , C1 = 2C −1/λ 0 λ(λ + 1) −1−1/λ, d∆(Sv, S ∗ v) = Pr(Sv − S ∗ v) + Pr(S ∗ v − Sv) is also a pseudo-distance between the sets Sv and S ∗ v , and d(Sv , S ∗ v) ≤ d∆(Sv, S ∗ v ) ≤ 1. We will use the following lamma [1] which gives the standard sample complexity for non-realizable learning task.\nLemma 1 Suppose that H is a set of functions from X to Y = {0, 1} with finite VC-dimension V ≥ 1 and D is the fixed but unknown distribution over X × Y . For any ǫ, δ > 0, there is a positive constant C, such that if the size of sample {(x1, y1), . . . , (xN , yN )} from D is N(ǫ, δ) = C ǫ2 ( V + log(1δ ) ) , then with probability at least 1− δ, for all h ∈ H, the following holds.\n| 1\nN ∑N i=1 I ( h(xi) 6= yi ) −E(x,y)∈DI ( h(x) 6= y ) | ≤ ǫ"
    }, {
      "heading" : "4. α-Expansion in the Non-realizable Case",
      "text" : "Multi-view active learning first described in [20] focuses on the contention points (i.e., unlabeled instances on which different views predict different labels) and queries some labels of them. It is motivated by that querying the labels of contention points may help at least one of the two views to learn the optimal classifier. Let S1 ⊕ S2 = (S1 − S2) ∪ (S2 − S1) denote the contention points between S1 and S2, then Pr(S1 ⊕ S2) denotes the probability mass on the contentions points. “∆” and “⊕” mean the same operation rule. In this paper, we use “∆” when referring the excess error between Sv and S ∗ v and use “⊕” when referring the difference between the two views S1 and S2. In order to study multi-view active learning, the properties of contention points should be considered. One basic property is that Pr(S1 ⊕ S2) should not be too small, otherwise the two views could be exactly the same and two-view setting would degenerate into single-view setting.\nIn multi-view learning, the two views represent the same learning task and generally are consistent with each other, i.e., for any instance x = (x1, x2) the labels of x in the two views are the same. Hence we first assume that S∗1 = S ∗ 2 = S ∗. As for the situation where S∗1 6= S ∗ 2 , we will discuss on it further in Section 5.2. The instances agreed by the two views can be denoted as (S1 ∩ S2) ∪ (S1 ∩ S2). However, some of these agreed instances may be predicted different label by the optimal classifier S∗, i.e., the instances in (S1 ∩ S2 − S ∗) ∪ (S1 ∩ S2 − S∗). Intuitively, if the contention points can convey some information about (S1 ∩ S2 − S ∗) ∪ (S1 ∩ S2 − S∗), then querying the labels of contention points could help to improve S1 and S2. Based on this intuition and that Pr(S1 ⊕S2) should not be too small, we give our definition on α-expansion in the non-realizable case.\nDefinition 1 D is α-expanding if for some α > 0 and any S1 ⊆ X1, S2 ⊆ X2, (3) holds.\nPr ( S1 ⊕ S2 ) ≥ α ( Pr ( S1 ∩ S2 − S ∗ ) + Pr ( S1 ∩ S2 − S∗ )) (3)\nWe say that D is α-expanding with respect to hypothesis class H1 ×H2 if the above holds for all S1 ∈ H1∩X1, S2 ∈ H2 ∩X2 (here we denote by Hv ∩Xv the set {h∩Xv : h ∈ Hv} for v = 1, 2).\nBalcan et al. [3] also gave a definition of expansion, Pr(T1⊕T2) ≥ αmin [ Pr(T1∩T2), P r(T1∩T2) ] , for realizable learning task under the assumptions that the learner in each view is never “confident but wrong” and the learning algorithm is able to learn from positive data only. Here Tv denotes\nthe instances which are classified as positive confidently in each view. Generally, in realizable learning tasks, we aim at studying the asymptotic performance and assume that the performance of initial classifier is better than guessing randomly, i.e., Pr(Tv) > 1/2. This ensures that Pr(T1 ∩ T2) is larger than Pr(T1 ∩ T2). In addition, in [3] the instances which are agreed by the two views but are predicted different label by the optimal classifier can be denoted as T1 ∩ T2. So, it can be found that Definition 1 and the definition of expansion in [3] are based on the same intuition that the amount of contention points is no less than a fraction of the amount of instances which are agreed by the two views but are predicted different label by the optimal classifiers."
    }, {
      "heading" : "5. Multi-view Active Learning with Non-degradation Condition",
      "text" : "In this section, we first consider the multi-view learning in Table 1 and analyze whether multiview setting can help improve the sample complexity of active learning in the non-realizable case remarkably. In multi-view setting, the classifiers are often combined to make predictions and many strategies can be used to combine them. In this paper, we consider the following two combination schemes, h+ and h−, for binary classification:\nhi+(x) =    1 if hi1(x1) = h i 2(x2) = 1\n0 otherwise hi−(x) =\n   0 if hi1(x1) = h i 2(x2) = 0\n1 otherwise (4)\n5.1. The Situation Where S∗1 = S ∗ 2\nWith (4), the error rate of the combined classifiers hi+ and h i − satisfy (5) and (6), respectively.\nR(hi+)−R(S ∗) = R(Si1 ∩ S i 2)−R(S ∗) ≤ d∆(S i 1 ∩ S i 2, S ∗) (5) R(hi−)−R(S ∗) = R(Si1 ∪ S i 2)−R(S ∗) ≤ d∆(S i 1 ∪ S i 2, S ∗) (6)\nHere Siv ⊂ Xv (v = 1, 2) corresponds to the classifier h i v ∈ Hv in the i-th round. In each round of multi-view active learning, labels of some contention points are queried to augment the training data set L and the classifier in each view is then refined. As discussed in [23], we also assume that the learner in Table 1 satisfies the non-degradation condition as the amount of labeled training examples increases, i.e., (7) holds, which implies that the excess error of Si+1v is no larger than that of Siv in the region of S i 1 ⊕ S i 2.\nPr ( Si+1v ∆S ∗ ∣∣Si1 ⊕ Si2 ) ≤ Pr(Siv∆S ∗ ∣∣Si1 ⊕ Si2) (7)\nTo illustrate the non-degradation condition, we give the following example: Suppose the data in Xv (v = 1, 2) fall into n different clusters, denoted by π v 1 , . . . , π v n, and every cluster has the same probability mass for simplicity. The positive class is the union of some clusters while the negative class is the union of the others. Each positive (negative) cluster πvξ in Xv is associated with only 3 positive (negative) clusters π3−vς (ξ, ς ∈ {1, . . . , n}) in X3−v (i.e., given an instance xv in π v ξ , x3−v will only be in one of these π3−vς ). Suppose the learning algorithm will predict all instances in each cluster with the same label, i.e., the hypothesis class Hv consists of the hypotheses which do not split any cluster. Thus, the cluster πvξ can be classified according to the posterior probability P (y = 1|πvξ ) and querying the labels of instances in cluster π v ξ will not influence the estimation of the posterior probability for cluster πvς (ς 6= ξ). It is evident that the non-degradation condition holds in this task. Note that the non-degradation assumption may not always hold, and we will discuss on this in Section 6. Now we give Theorem 1.\nTheorem 1 For data distribution D α-expanding with respect to hypothesis class H1 × H2 according to Definition 1, when the non-degradation condition holds, if s = ⌈ 2 log 1 8ǫ\nlog 1 C2\n⌉ and mi =\n256kC C21 ( V +log(16(s+1)δ ) ) , the multi-view active learning in Table 1 will generate two classifiers hs+ and hs−, at least one of which is with error rate no larger than R(S ∗) + ǫ with probability at least 1− δ. Here, V = max[V C(H1), V C(H2)] where V C(H) denotes the VC-dimension of the hypothesis class H, k = 1+λλ , C1 = 2C −1/λ 0 λ(λ+ 1) −1−1/λ and C2 = 5α+8 6α+8 .\nProof: Let Qi = S i 1 ⊕ S i 2. First we prove that if each view Xv (v = 1, 2) satisfies Tsybakov noise condition, i.e., Prxv∈Xv(|ϕv(xv)− 1/2| ≤ t) ≤ C3t λ3 for some finite C3 > 0, λ3 > 0 and all 0 < t ≤ 1/2, Tsybakov noise condition can also be met in Qi, i.e., Prxv∈Qi(|ϕv(xv)−1/2|≤t)\nPr(Qi) ≤ C4t\nλ4\nfor some finite C4 > 0, λ4 > 0 and all 0 < t ≤ 1/2. Suppose Tsybakov noise condition cannot be met in Qi, then for C∗ = C3\nPr(Qi) and λ∗ = λ3, there exists some 0 < t∗ ≤ 1/2 to satisfy that\nPrxv∈Qi (|ϕv(xv)−1/2|≤t)\nPr(Qi) > C∗t\nλ∗ ∗ . So we get\nPrxv∈Xv(|ϕv(xv)− 1/2| ≤ t) ≥ Prxv∈Qi(|ϕv(xv)− 1/2| ≤ t) > C3t λ3 ∗ .\nIt is in contradiction with that Xv satisfies Tsybakov noise condition. Thus, we get that Tsybakov noise condition can also be met in Qi. Without loss of generality, suppose that Tsybakov noise condition in all Qi and Xv can be met for the same finite C0 and λ.\nSince m0 = 256kC C21 ( V + log(16(s+1)δ ) ) , according to Lemma 1 we know that d(S0v , S ∗) ≤ C1 16k with probability at least 1 − δ16(s+1) . With d(Sv, S ∗ v ) ≥ C1d k ∆(Sv, S ∗ v ), we get d∆(S 0 v , S ∗) ≤ 116 . It is easy to find that d∆(S 0 1 ∩S 0 2 , S ∗) ≤ d∆(S 0 1 , S ∗)+d∆(S 0 2 , S ∗) ≤ 1/8 holds with probability at least 1− δ8(s+1) .\nFor i ≥ 0, mi+1 number of labels are queried randomly from Qi. Thus, similarly according to Lemma 1 we have d∆(S i+1 1 ∩ S i+1 2 | Qi, S ∗ | Qi) ≤ 1/8 with probability at least 1 − δ 8(s+1) . Let T i+1v = S i+1 v ∩Qi and τi+1 = Pr(T i+11 ⊕T i+1 2 −S ∗)\nPr(T i+11 ⊕T i+1 2 )\n− 12 , it is easy to get\nPr ( S∗ ∩ (Si+11 ⊕ S i+1 2 )|Qi ) − Pr ( S∗ ∩ (Si+11 ⊕ S i+1 2 )|Qi ) = −2τi+1Pr(S i+1 1 ⊕ S i+1 2 |Qi).\nConsidering the non-degradation condition and d∆(S i 1 ∩ S i 2|Qi, S ∗|Qi) = d∆(S i v|Qi, S ∗|Qi), we calculate that\nd∆(S i+1 1 ∩ S i+1 2 |Qi, S ∗|Qi)\n= 1\n2\n( d∆(S i+1 1 |Qi, S ∗|Qi) + d∆(S i+1 2 |Qi, S ∗|Qi) ) + 1\n2 Pr\n( S∗ ∩ (Si+11 ⊕ S i+1 2 )|Qi )\n− 1\n2 Pr\n( S∗ ∩ (Si+11 ⊕ S i+1 2 )|Qi )\n≤ 1\n2\n( d∆(S i 1|Qi, S ∗|Qi) + d∆(S i 2|Qi, S ∗|Qi) ) − τi+1Pr(S i+1 1 ⊕ S i+1 2 |Qi)\n= d∆(S i 1 ∩ S i 2|Qi, S ∗|Qi)− τi+1Pr(S i+1 1 ⊕ S i+1 2 |Qi).\nSo we have\nd∆(S i+1 1 ∩ S i+1 2 , S ∗)\n= d∆(S i+1 1 ∩ S i+1 2 |Qi, S ∗|Qi)Pr(Qi) + d∆(S i+1 1 ∩ S i+1 2 |Qi, S ∗|Qi)Pr(Qi) ≤ 1\n8 Pr(Qi) + d∆(S\ni 1 ∩ S i 2|Qi, S ∗|Qi)Pr(Qi)− τi+1Pr ( (Si+11 ⊕ S i+1 2 ) ∩Qi ) .\nConsidering d∆(S i 1 ∩ S i 2|Qi, S ∗|Qi)Pr(Qi) = Pr(S i 1 ∩ S i 2 − S ∗) + Pr(Si1 ∩ S i 2 − S ∗), we have\nd∆(S i+1 1 ∩ S i+1 2 , S ∗)\n≤ Pr(Si1 ∩ S i 2 − S ∗) + Pr(Si1 ∩ S i 2 − S\n∗) + 1\n8 Pr(Si1 ⊕ S i 2)− τi+1Pr\n( (Si+11 ⊕ S i+1 2 ) ∩Qi ) .\nSimilarly, we get\nd∆(S i+1 1 ∪ S i+1 2 , S ∗)\n≤ Pr(Si1 ∩ S i 2 − S ∗) + Pr(Si1 ∩ S i 2 − S\n∗) + 1\n8 Pr(Si1 ⊕ S i 2) + τi+1Pr\n( (Si+11 ⊕ S i+1 2 ) ∩Qi ) .\nLet γi = Pr(Si1⊕S i 2−S ∗)\nPr(Si1⊕S i 2)\n− 12 , we have\nd∆(S i 1 ∩ S i 2, S ∗) = d∆(S i 1 ∩ S i 2|Qi, S ∗|Qi)Pr(Qi) + d∆(S i 1 ∩ S i 2|Qi, S ∗|Qi)Pr(Qi)\n= (1/2 − γi)Pr(S i 1 ⊕ S i 2) + Pr(S i 1 ∩ S i 2 − S ∗) + Pr(Si1 ∩ S i 2 − S ∗)\nand d∆(S i 1 ∪ S i 2, S ∗) = (1/2 + γi)Pr(S i 1 ⊕ S i 2) + Pr(S i 1 ∩ S i 2 − S ∗) + Pr(Si1 ∩ S i 2 − S ∗).\nAs in each round of the multi-view active learning some contention points of the two views are queried and added into the training set, the difference between the two views is decreasing, i.e., Pr(Si+11 ⊕ S i+1 2 ) is no larger than Pr(S i 1 ⊕ S i 2).\nCase 1: If |τi+1| ≤ γi, with respect to Definition 1, we have\nd∆(S i+1 1 ∪ S i+1 2 , S ∗)\nd∆(S i 1 ∪ S i 2, S\n∗) ≤\n1 8Pr(S i 1 ⊕ S i 2) + |τi+1|Pr(S i+1 1 ⊕ S i+1 2 ) + 1 αPr(S i 1 ⊕ S i 2)\n(12 + γi)Pr(S i 1 ⊕ S i 2) + 1 αPr(S i 1 ⊕ S i 2)\n≤ (18 + γi)Pr(S i 1 ⊕ S i 2) + 1 αPr(S i 1 ⊕ S i 2)\n(12 + γi)Pr(S i 1 ⊕ S i 2) + 1 αPr(S i 1 ⊕ S i 2)\n≤ 5α+ 8\n8α+ 8 ;\nCase 2: If −|τi+1| > γi, with respect to Definition 1, we have\nd∆(S i+1 1 ∩ S i+1 2 , S ∗)\nd∆(Si1 ∩ S i 2, S\n∗) ≤\n1 8Pr(S i 1 ⊕ S i 2) + |τi+1|Pr(S i+1 1 ⊕ S i+1 2 ) + 1 αPr(S i 1 ⊕ S i 2)\n(12 + |γi|)Pr(S i 1 ⊕ S i 2) + 1 αPr(S i 1 ⊕ S i 2)\n≤ 5α+ 8\n8α+ 8 ;\nCase 3: If τi+1 ≥ γi and 0 ≤ γi ≤ 1 4 , with respect to Definition 1, we have\nd∆(S i+1 1 ∩ S i+1 2 , S ∗)\nd∆(Si1 ∩ S i 2, S\n∗) ≤\n1 8Pr(S i 1 ⊕ S i 2) + 1 αPr(S i 1 ⊕ S i 2)\n(12 − γi)Pr(S i 1 ⊕ S i 2) + 1 αPr(S i 1 ⊕ S i 2)\n≤ α+ 8\n2α+ 8 ;\nCase 4: If τi+1 ≥ γi and 1 4 < γi ≤ 1 2 , with respect to Definition 1, we have\nd∆(S i+1 1 ∪ S i+1 2 , S ∗)\nd∆(Si1 ∪ S i 2, S\n∗) ≤\n1 8Pr(S i 1 ⊕ S i 2) + τi+1Pr(S i+1 1 ⊕ S i+1 2 ) + 1 αPr(S i 1 ⊕ S i 2)\n(12 + γi)Pr(S i 1 ⊕ S i 2) + 1 αPr(S i 1 ⊕ S i 2)\n≤ 5α+ 8\n6α+ 8 ;\nCase 5: If τi+1 < γi and − 1 4 ≤ γi ≤ 0, with respect to Definition 1, we have\nd∆(S i+1 1 ∪ S i+1 2 , S ∗)\nd∆(Si1 ∪ S i 2, S\n∗) ≤\n1 8Pr(S i 1 ⊕ S i 2) + 1 αPr(S i 1 ⊕ S i 2)\n(12 + γi)Pr(S i 1 ⊕ S i 2) + 1 αPr(S i 1 ⊕ S i 2)\n≤ α+ 8\n2α+ 8 ;\nCase 6: If τi+1 < γi and − 1 2 ≤ γi < − 1 4 , with respect to Definition 1, we have\nd∆(S i+1 1 ∩ S i+1 2 , S ∗)\nd∆(Si1 ∩ S i 2, S\n∗) ≤\n1 8Pr(S i 1 ⊕ S i 2) + |τi+1|Pr(S i+1 1 ⊕ S i+1 2 ) + 1 αPr(S i 1 ⊕ S i 2)\n(12 + |γi|)Pr(S i 1 ⊕ S i 2) + 1 αPr(S i 1 ⊕ S i 2)\n≤ 5α+ 8\n6α+ 8 ;\nCase 7: If τi+1 ≤ −γi and 0 ≤ γi ≤ 1 2 , with respect to Definition 1, we have\nd∆(S i+1 1 ∪ S i+1 2 , S ∗)\nd∆(S i 1 ∪ S i 2, S\n∗) ≤\n1 8Pr(S i 1 ⊕ S i 2) + 1 αPr(S i 1 ⊕ S i 2)\n(12 + γi)Pr(S i 1 ⊕ S i 2) + 1 αPr(S i 1 ⊕ S i 2)\n≤ α+ 8\n4α+ 8 ;\nCase 8: If τi+1 > −γi and − 1 2 ≤ γi ≤ 0, with respect to Definition 1, we have\nd∆(S i+1 1 ∩ S i+1 2 , S ∗)\nd∆(Si1 ∩ S i 2, S\n∗) ≤\n1 8Pr(S i 1 ⊕ S i 2) + 1 αPr(S i 1 ⊕ S i 2)\n(12 + |γi|)Pr(S i 1 ⊕ S i 2) + 1 αPr(S i 1 ⊕ S i 2)\n≤ α+ 8\n4α+ 8 .\nThus, after the (i+1)-th round, either d∆(S\ni+1 1 ∩S i+1 2 ,S ∗)\nd∆(Si1∩S i 2,S\n∗) ≤ 5α+86α+8 or\nd∆(S i+1 1 ∪S i+1 2 ,S ∗)\nd∆(Si1∪S i 2,S\n∗) ≤ 5α+86α+8 holds.\nHence, we have d∆(S s 1 ∩S s 2, S ∗) ≤ 18 ( 5α+8 6α+8 )s/2 or d∆(S s 1 ∪S s 2, S ∗) ≤ 18 ( 5α+8 6α+8 )s/2 with probability\nat least 1 − δ. When s = ⌈ 2 log 1 8ǫ\nlog 1 C2\n⌉, where C2 = 5α+8 6α+8 is a constant less than 1, we have either\nd∆(S s 1 ∩ S s 2, S ∗) ≤ ǫ or d∆(S s 1 ∪ S s 2, S ∗) ≤ ǫ with probability at least 1 − δ. Thus, considering R(hi+)−R(S ∗) = R(Si1∩S i 2)−R(S ∗) ≤ d∆(S i 1∩S i 2, S ∗) and R(hi−)−R(S ∗) = R(Si1∪S i 2)−R(S ∗) ≤ d∆(S i 1 ∪ S i 2, S ∗), we have either R(hs+) ≤ R(S ∗) + ǫ or R(hs−) ≤ R(S ∗) + ǫ.\nFrom Theorem 1 we know that we only need to request ∑s\ni=0mi = Õ(log 1 ǫ ) labels to learn h s +\nand hs−, at least one of which is with error rate no larger than R(S ∗) + ǫ with probability at least 1− δ. If we choose hs+ and it happens to satisfy R(h s +) ≤ R(S ∗) + ǫ, we can get a classifier whose error rate is no larger than R(S∗) + ǫ. Fortunately, there are only two classifiers and the probability of getting the right classifier is no less than 12 . To study how to choose between h s + and hs−, we give Definition 2 at first.\nDefinition 2 The multi-view classifiers S1 and S2 satisfy β-condition if (8) holds for some β > 0.\n∣∣∣ Pr\n( {x : x ∈ S1 ⊕ S2 ∧ y(x) = 1} )\nPr(S1 ⊕ S2) −\nPr ( {x : x ∈ S1 ⊕ S2 ∧ y(x) = 0} )\nPr(S1 ⊕ S2)\n∣∣∣ ≥ β (8)\n(8) implies the difference between the examples belonging to positive class and that belonging to negative class in the contention region of S1 ⊕ S2. Based on Definition 2, we give Lemma 2 which provides information for deciding how to choose between h+ and h−. This helps to get Theorem 2.\nLemma 2 If the multi-view classifiers Ss1 and S s 2 satisfy β-condition, with the number of\n2 log( 4 δ )\nβ2\nlabels we can decide correctly whether Pr ( {x : x ∈ Ss1 ⊕ S s 2 ∧ y(x) = 1} ) or Pr ( {x : x ∈ Ss1 ⊕ S s 2 ∧ y(x) = 0} ) ) is smaller with probability at least 1− δ.\nProof: We apply Ss1 and S s 2 to the unlabeled instances set and identify the contention point set. Then we query for labels of 2 log( 4 δ )\nβ2 instances drawn randomly from the contention points set.\nWith these labels we estimate the empirical value P̂1 of Pr({x:x∈Ss1⊕S s 2∧y(x)=1})\nPr(Ss1⊕S s 2)\nand the empirical\nvalue P̂2 of Pr({x:x∈Ss1⊕S s 2∧y(x)=0})\nPr(Ss1⊕S s 2)\n. By Chernoff bound, with number of 2 log( 4 δ )\nβ2 labels we have the\nfollowing two equations with probability at least 1− δ.\nP̂1 ∈ [Pr\n( {x : x ∈ Ss1 ⊕ S s 2 ∧ y(x) = 1} )\nPr(Ss1 ⊕ S s 2)\n− β 2 , P r\n( {x : x ∈ Ss1 ⊕ S s 2 ∧ y(x) = 1} )\nPr(Ss1 ⊕ S s 2)\n+ β\n2\n]\nP̂2 ∈ [Pr\n( {x : x ∈ Ss1 ⊕ S s 2 ∧ y(x) = 0} )\nPr(Ss1 ⊕ S s 2)\n− β 2 , P r\n( {x : x ∈ Ss1 ⊕ S s 2 ∧ y(x) = 0} )\nPr(Ss1 ⊕ S s 2)\n+ β\n2\n]\nIf P̂1 ≤ P̂2, we get Pr ( {x : x ∈ Ss1 ⊕ S s 2 ∧ y(x) = 1} ) ≤ Pr ( {x : x ∈ Ss1 ⊕ S s 2 ∧ y(x) = 0} ) with probability at least 1 − δ; otherwise, we get Pr ( {x : x ∈ Ss1 ⊕ S s 2 ∧ y(x) = 1} ) > Pr ( {x : x ∈ Ss1 ⊕ S s 2 ∧ y(x) = 0} ) with probability at least 1− δ.\nTheorem 2 For data distribution D α-expanding with respect to hypothesis class H1 × H2 according to Definition 1, when the non-degradation condition holds, if the multi-view classifiers satisfy β-condition, by requesting Õ(log 1ǫ ) labels the multi-view active learning in Table 1 will generate a classifier whose error rate is no larger than R(S∗) + ǫ with probability at least 1− δ.\nProof: According to Theorem 1, by requesting Õ(log 1ǫ ) labels the multi-view active learning in Table 1 can get either R(hs+) ≤ R(S ∗) + ǫ or R(hs−) ≤ R(S ∗) + ǫ with probability at least 1 − δ2 . According to Lemma 2, by requesting 2 log( 8 δ ) β2 labels we can decide correctly whether Pr ( {x : x ∈ Ss1 ⊕S s 2 ∧ y(x) = 1} ) or Pr ( {x : x ∈ Ss1 ⊕S s 2 ∧ y(x) = 0} ) is smaller with probability at least 1− δ2 . Case 1: If Pr ( {x : x ∈ Ss1 ⊕ S s 2 ∧ y(x) = 1} ) ≤ Pr ( {x : x ∈ Ss1 ⊕ S s 2 ∧ y(x) = 0} ) , we have R(hs−) ≤ R(h s +). Thus, we get R(h s −) ≤ R(S ∗) + ǫ with probability at least 1− δ. Case 2: If Pr ( {x : x ∈ Ss1 ⊕ S s 2 ∧ y(x) = 1} ) > Pr ( {x : x ∈ Ss1 ⊕ S s 2 ∧ y(x) = 0} ) , we have R(hs+) < R(h s −). Thus, we get R(h s +) ≤ R(S ∗) + ǫ with probability at least 1− δ.\nThe total number of labels to be requested is Õ(log 1ǫ ) + 2 log( 8 δ ) β2 = Õ(log 1ǫ ).\nFrom Theorem 2 we know that we only need to request Õ(log 1ǫ ) labels to learn a classifier with error rate no larger than R(S∗)+ǫ with probability at least 1−δ. Thus, we achieve an exponential improvement in sample complexity of active learning in the non-realizable case under multi-view setting. Sometimes, the difference between the examples belonging to positive class and that belonging to negative class in Ss1 ⊕ S s 2 may be very small, i.e., (9) holds.\n∣∣∣ Pr\n( {x : x ∈ Ss1 ⊕ S s 2 ∧ y(x) = 1} )\nPr(Ss1 ⊕ S s 2)\n− Pr\n( {x : x ∈ Ss1 ⊕ S s 2 ∧ y(x) = 0} )\nPr(Ss1 ⊕ S s 2)\n∣∣∣ = O(ǫ) (9)\nIf so, we need not to estimate whether R(hs+) or R(h s −) is smaller and Theorem 3 indicates that both hs+ and h s − are good approximations of the optimal classifier.\nTheorem 3 For data distribution D α-expanding with respect to hypothesis class H1×H2 according to Definition 1, when the non-degradation condition holds, if (9) is satisfied, by requesting Õ(log 1ǫ ) labels the multi-view active learning in Table 1 will generate two classifiers h s + and hs− which satisfy either (a) or (b) with probability at least 1 − δ. (a) R(h s +) ≤ R(S ∗) + ǫ and R(hs−) ≤ R(S ∗) +O(ǫ); (b) R(hs+) ≤ R(S ∗) +O(ǫ) and R(hs−) ≤ R(S ∗) + ǫ.\nProof: Since Pr(Ss1 ⊕ S s 2) ≤ 1, with the following equation\n∣∣∣ Pr\n( {x : x ∈ Ss1 ⊕ S s 2 ∧ y(x) = 1} )\nPr(Ss1 ⊕ S s 2)\n− Pr\n( {x : x ∈ Ss1 ⊕ S s 2 ∧ y(x) = 0} )\nPr(Ss1 ⊕ S s 2)\n∣∣∣ = O(ǫ)\nwe have |Pr ( {x : x ∈ Ss1 ⊕ S s 2 ∧ y(x) = 1} ) − Pr ( {x : x ∈ Ss1 ⊕ S s 2 ∧ y(x) = 0} ) | = O(ǫ). So it is easy to get |R(hs+)−R(h s −)| = O(ǫ). According to Theorem 1, by requesting Õ(log 1 ǫ ) labels we can get either R(hs+) ≤ R(S ∗) + ǫ or R(hs−) ≤ R(S ∗) + ǫ with probability at least 1 − δ. Thus, we get that hs+ and h s − satisfy either (a) or (b) with probability at least 1− δ.\n5.2. The Situation Where S∗1 6= S ∗ 2\nAlthough the two views represent the same learning task and generally are consistent with each other, sometimes S∗1 may be not equal to S ∗ 2 . Therefore, the α-expansion assumption in Definition 1 should be adjusted to the situation where S∗1 6= S ∗ 2 . To analyze this theoretically, we replace S∗ by S∗1 ∩ S ∗ 2 in Definition 1 and get (10). Similarly to Theorem 1, we get Theorem 4.\nPr ( S1 ⊕ S2 ) ≥ α ( Pr ( S1 ∩ S2 − S ∗ 1 ∩ S ∗ 2 ) + Pr ( S1 ∩ S2 − S ∗ 1 ∩ S ∗ 2 )) (10)\nTheorem 4 For data distribution D α-expanding with respect to hypothesis class H1 × H2 according to (10), when the non-degradation condition holds, if s = ⌈ 2 log 1 8ǫ\nlog 1 C2\n⌉ and mi = 256kC C21\n( V +\nlog(16(s+1)δ ) ) , the multi-view active learning in Table 1 will generate two classifiers hs+ and h s −, at least one of which is with error rate no larger than R(S∗1 ∩ S ∗ 2) + ǫ with probability at least 1− δ. (V , k, C1 and C2 are given in Theorem 1.)\nProof: Since S∗v is the optimal Bayes classifier in the v-th view, obviously, R(S ∗ 1 ∩S ∗ 2) is no less than R(S∗v ), (v = 1, 2). So, learning a classifier with error rate no larger than R(S ∗ 1 ∩ S ∗ 2) + ǫ is not harder than learning a classifier with error rate no larger than R(S∗v) + ǫ. Now we aim at\nlearning a classifier with error rate no larger than R(S∗1 ∩ S ∗ 2) + ǫ. Without loss of generality, we assume R(Siv) > R(S ∗ 1 ∩ S ∗ 2) for i = 0, 1, . . . , s. If R(S i v) ≤ R(S ∗ 1 ∩ S ∗ 2), we get a classifier with error rate no larger than R(S∗1 ∩ S ∗ 2) + ǫ. Thus, we can neglect the probability mass on the hypothesis whose error rate is less than R(S∗1 ∩S ∗ 2) and regard S ∗ 1 ∩S ∗ 2 as the optimal. Replacing S∗ by S∗1 ∩ S ∗ 2 in the discussion of Section 5.1, with the proof of Theorem 1 we get Theorem 4 proved.\nTheorem 4 shows that for the situation where S∗1 6= S ∗ 2 , by requesting Õ(log 1 ǫ ) labels we can learn two classifiers hs+ and h s −, at least one of which is with error rate no larger than R(S ∗ 1 ∩ S ∗ 2) + ǫ with probability at least 1− δ. With Lemma 2, we get Theorem 5 from Theorem 4.\nTheorem 5 For data distribution D α-expanding with respect to hypothesis class H1 × H2 according to (10), when the non-degradation condition holds, if the multi-view classifiers satisfy β-condition, by requesting Õ(log 1ǫ ) labels the multi-view active learning in Table 1 will generate a classifier whose error rate is no larger than R(S∗1 ∩ S ∗ 2) + ǫ with probability at least 1− δ.\nProof: According to Theorem 4, by requesting Õ(log 1ǫ ) labels the multi-view active learning in Table 1 can get either R(hs+) ≤ R(S ∗ 1 ∩ S ∗ 2) + ǫ or R(h s −) ≤ R(S ∗ 1 ∩ S ∗ 2) + ǫ with probability at least 1− δ2 . According to Lemma 2, by requesting 2 log( 8 δ ) β2 labels we can decide correctly whether Pr ( {x : x ∈ Ss1 ⊕S s 2 ∧ y(x) = 1} ) or Pr ( {x : x ∈ Ss1 ⊕S s 2 ∧ y(x) = 0} ) is smaller with probability at least 1− δ2 . Case 1: If Pr ( {x : x ∈ Ss1 ⊕ S s 2 ∧ y(x) = 1} ) ≤ Pr ( {x : x ∈ Ss1 ⊕ S s 2 ∧ y(x) = 0} ) , we have R(hs−) ≤ R(h s +). Thus, we get R(h s −) ≤ R(S ∗ 1 ∩ S ∗ 2) + ǫ with probability at least 1− δ. Case 2: If Pr ( {x : x ∈ Ss1 ⊕ S s 2 ∧ y(x) = 1} ) > Pr ( {x : x ∈ Ss1 ⊕ S s 2 ∧ y(x) = 0} ) , we have R(hs+) < R(h s −). Thus, we get R(h s +) ≤ R(S ∗ 1 ∩ S ∗ 2) + ǫ with probability at least 1− δ. The total number of labels to be requested is Õ(log 1ǫ ) + 2 log( 8 δ ) β2 = Õ(log 1ǫ ).\nGenerally, R(S∗1∩S ∗ 2) is larger than R(S ∗ 1) and R(S ∗ 2). When S ∗ 1 is not too much different from S ∗ 2 , i.e., Pr(S∗1 ⊕ S ∗ 2) ≤ ǫ/2, we have Corollary 1 which indicates that the exponential improvement in the sample complexity of active learning with Tsybakov noise is still possible.\nCorollary 1 For data distribution D α-expanding with respect to hypothesis class H1 ×H2 according to (10), when the non-degradation condition holds, if the multi-view classifiers satisfy β-condition and Pr(S∗1 ⊕ S ∗ 2) ≤ ǫ/2, by requesting Õ(log 1 ǫ ) labels the multi-view active learning in Table 1 will generate a classifier with error rate no larger than R(S∗v) + ǫ (v = 1, 2) with probability at least 1− δ.\nProof: According to Theorem 5 we know that by requesting Õ(log 1ǫ ) labels the multi-view active learning in Table 1 will generate a classifier whose error rate is no larger than R(S∗1 ∩ S ∗ 2) + ǫ 2 with probability at least 1− δ. Considering that\nR(S∗1 ∩ S ∗ 2)−R(S ∗ v) =\n∫\n(S∗1∩S ∗ 2 )∆S ∗ v\n|2ϕv(xv)− 1|pxvdxv ≤ Pr(S ∗ 1 ⊕ S ∗ 2),\nwe have R(S∗1 ∩ S ∗ 2) ≤ R(S ∗ v) + ǫ 2 . Thus, we get that R(S ∗ 1 ∩ S ∗ 2) + ǫ 2 is no larger than R(S ∗ v ) + ǫ."
    }, {
      "heading" : "6. Multi-view Active Learning without Non-degradation Condition",
      "text" : "Section 5 considers situations when the non-degradation condition holds, there are cases, however, the non-degradation condition (7) does not hold. In this section we focus on the multi-view active learning in Table 2 and give an analysis with the non-degradation condition waived. Firstly, we give Theorem 6 for the sample complexity of multi-view active learning in Table 2 when S∗1 = S ∗ 2 = S ∗.\nTheorem 6 For data distribution D α-expanding with respect to hypothesis class H1 × H2 according to Definition 1, if s = ⌈ 2 log 1 8ǫ\nlog 1 C2\n⌉ and mi = 256kC C21 ( V + log(16(s+1)δ ) ) , the multi-view active\nlearning in Table 2 will generate two classifiers hs+ and h s −, at least one of which is with error rate no larger than R(S∗) + ǫ with probability at least 1 − δ. (V , k, C1 and C2 are given in Theorem 1.)\nProof: After the i-th round in Table 2, the number of training examples in L is ∑i\nb=0 2 bmi =\n(2i+1 − 1)mi. While in the (i + 1)-th round, we randomly query (2 i+1 − 1)mi labels from the region of Qi and add them into L. So in the (i + 1)-th round, the number of training examples\nfor Si+1v (v = 1, 2) drawn randomly from region of Qi is larger than the number of whole training examples for Siv. Since the optimal Bayes classifier cv belongs to Hv, according to the standard PAC-model, it is easy to know that d(Si+1v |Qi, S ∗|Qi) ≤ d(S i v|Qi, S ∗|Qi) can be met for any ϕv, where d(Sv|Qi, S ∗|Qi) is defined as\nd(Sv|Qi, S ∗|Qi) , R(Sv|Qi)−R(S ∗|Qi) =\n∫\n(Sv∩Qi)∆(S∗∩Qi) |2ϕv(xv)− 1|pxvdxv\n/ Pr(Qi).\nSo, by setting ϕv ∈ {0, 1}, we get d∆(S i+1 v |Qi, S ∗|Qi) ≤ d∆(S i v|Qi, S ∗|Qi), which implies the non-degradation condition. Thus, with the proof of Theorem 1, we get Theorem6 proved.\nTheorem 6 shows that we can request ∑s\ni=0 2 imi = Õ( 1 ǫ ) labels to learn two classifiers h s + and\nhs−, at least one of which is with error rate no larger than R(S ∗) + ǫ with probability at least 1 − δ. To guarantee the non-degradation condition (7), we only need to query (2i − 1)mi more labels in the i-th round. With Lemma 2, we get Theorem 7.\nTheorem 7 For data distribution D α-expanding with respect to hypothesis class H1 × H2 according to Definition 1, if the multi-view classifiers satisfy β-condition, by requesting Õ(1ǫ ) labels the multi-view active learning in Table 2 will generate a classifier whose error rate is no larger than R(S∗) + ǫ with probability at least 1− δ.\nProof: According to Theorem 6, by requesting Õ(1ǫ ) labels the multi-view active learning in Table 2 will generate two classifiers hs+ and h s −, at least one of which is with error rate no larger\nthan R(S∗) + ǫ with probability at least 1 − δ. Similarly to the proof of Theorem 2, we get Theorem 7 proved.\nTheorem 7 shows that, without the non-degradation condition, we need to request Õ(1ǫ ) labels to learn a classifier with error rate no larger than R(S∗) + ǫ with probability at least 1− δ. The order of 1/ǫ is independent of the parameter in Tsybakov noise. Similarly to Theorem 3, we get Theorem 8 which indicates that both hs+ and h s − are good approximations of the optimal classifier.\nTheorem 8 For data distribution D α-expanding with respect to hypothesis class H1 × H2 according to Definition 1, if (9) holds, by requesting Õ(1ǫ ) labels the multi-view active learning in Table 2 will generate two classifiers hs+ and h s − which satisfy either (a) or (b) with probability at least 1− δ. (a) R(hs+) ≤ R(S ∗) + ǫ and R(hs−) ≤ R(S ∗) +O(ǫ); (b) R(hs+) ≤ R(S ∗) +O(ǫ) and R(hs−) ≤ R(S ∗) + ǫ.\nProof: According to Theorem 6, by requesting Õ(1ǫ ) labels the multi-view active learning in Table 2 will generate two classifiers hs+ and h s −, at least one of which is with error rate no larger than R(S∗) + ǫ with probability at least 1 − δ. Similarly to the proof of Theorem 3, we get Theorem 8 proved.\nAs for the situation where S∗1 6= S ∗ 2 , similarly to Theorem 5 and Corollary 1, we have Theorem 9 and Corollary 2.\nTheorem 9 For data distribution D α-expanding with respect to hypothesis class H1 × H2 according to (10), if the multi-view classifiers satisfy β-condition, by requesting Õ(1ǫ ) labels the multi-view active learning in Table 2 will generate a classifier whose error rate is no larger than R(S∗1 ∩ S ∗ 2) + ǫ with probability at least 1− δ.\nProof: Similarly to the proof of Theorem 4 and Theorem 6, we know that by requesting Õ(1ǫ ) labels the multi-view active learning in Table 2 can get either R(h s +) ≤ R(S ∗ 1 ∩ S ∗ 2) + ǫ or R(hs−) ≤ R(S ∗ 1 ∩ S ∗ 2) + ǫ with probability at least 1 − δ 2 . According to Lemma 2, by requesting 2 log( 8 δ )\nβ2 labels we can decide correctly whether R(hs+) or R(h s −) is smaller with probability at\nleast 1− δ2 . Thus, we can get a classifiers whose error rate is no larger than R(S ∗ 1 ∩ S ∗ 2) + ǫ with probability at least 1− δ. The total number of labels to be requested is Õ(1ǫ ) + 2 log( 8 δ ) β2 = Õ(1ǫ ).\nCorollary 2 For data distribution D α-expanding with respect to hypothesis class H1×H2 according to (10), if the multi-view classifiers satisfy β-condition and Pr(S∗1 ⊕S ∗ 2) ≤ ǫ/2, by requesting Õ(1ǫ ) labels the multi-view active learning in Table 2 will generate a classifier with error rate no larger than R(S∗v) + ǫ (v = 1, 2) with probability at least 1− δ.\nProof: According to Theorem 9 we know that by requesting Õ(1ǫ ) labels the multi-view active learning in Table 2 will generate a classifier whose error rate is no larger than R(S∗1 ∩ S ∗ 2) + ǫ 2 with probability at least 1− δ. With the proof of Corollary 1, we get that R(S∗1 ∩ S ∗ 2) + ǫ 2 is no larger than R(S∗v ) + ǫ."
    }, {
      "heading" : "7. Empirical Verification",
      "text" : "In this section we empirically verify that whether multi-view setting can improve the sample complexity of active learning in the non-realizable case remarkably.\nIn the experiment we use the semi-artificial data set [20] and the course data set [6]. The semiartificial data set has two artificial views which are created by randomly pairing two examples from the same class and contains 800 examples. In order to control the correlation between the two views, the number of clusters per class can be set as a parameter. We use 1 cluster, 2 clusters and 4 clusters in the experiments, respectively. The course data set has two natural views: pages view (i.e., the text appearing on the page) and links view (i.e., the anchor text attached to hyper-links pointing to the page) and contains 1,051 examples. We randomly use 25% data as the test set and use the remaining 75% data to generate the unlabeled data set U . We use Random Sampling as the baseline. In each round, we fix the number of examples to be queried in Multi-View Active Learning and that in Random Sampling. Thus, we can study their performances under the same number of queried examples. In the experiments, we query two\nexamples in each round of the two methods and implement the classifiers with NaiveBayes in WEKA. The experiments are repeated for 20 runs and Figure 1 plots the average error rates of the two methods against the number of examples that have been queried. From Figure 1 it can be found that the performance of Multi-View Active Learning is far better than the performance of Random Sampling with the same number of queried examples. In other words, multi-view setting can help improve the sample complexity of active learning in the non-realizable case remarkably."
    }, {
      "heading" : "8. Conclusion",
      "text" : "We present the first study on active learning in the non-realizable case under multi-view setting in this paper. We prove that the sample complexity of multi-view active learning with unbounded Tsybakov noise can be improved to Õ(log 1ǫ ), contrasting to single-view setting where only polynomial improvement is proved possible with the same noise condition. In general multi-view\nsetting, we prove that the sample complexity of active learning with unbounded Tsybakov noise is Õ(1ǫ ), where the order of 1/ǫ is independent of the parameter in Tsybakov noise, contrasting to previous polynomial bounds where the order of 1/ǫ is related to the parameter in Tsybakov noise. Generally, the non-realizability of learning task can be caused by many kinds of noise, e.g., misclassification noise and malicious noise. It would be interesting to extend our work to more general noise model."
    } ],
    "references" : [ {
      "title" : "editors",
      "author" : [ "M. Anthony", "P.L. Bartlett" ],
      "venue" : "Neural Network Learning: Theoretical Foundations. Cambridge University Press, Cambridge, UK",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "Agnostic active learning",
      "author" : [ "M.-F. Balcan", "A. Beygelzimer", "J. Langford" ],
      "venue" : "ICML, pages 65–72",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Co-training and expansion: Towards bridging theory and practice",
      "author" : [ "M.-F. Balcan", "A. Blum", "K. Yang" ],
      "venue" : "In NIPS",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2005
    }, {
      "title" : "Margin based active learning",
      "author" : [ "M.-F. Balcan", "A.Z. Broder", "T. Zhang" ],
      "venue" : "COLT, pages 35–50",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "The true sample complexity of active learning",
      "author" : [ "M.-F. Balcan", "S. Hanneke", "J. Wortman" ],
      "venue" : "COLT, pages 45–56",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Combining labeled and unlabeled data with co-training",
      "author" : [ "A. Blum", "T. Mitchell" ],
      "venue" : "COLT, pages 92–100",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "Upper and lower error bounds for active learning",
      "author" : [ "R.M. Castro", "R.D. Nowak" ],
      "venue" : "Allerton Conference, pages 225–234",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Minimax bounds for active learning",
      "author" : [ "R.M. Castro", "R.D. Nowak" ],
      "venue" : "IEEE Transactions on Information Theory, 54(5):2339–2353",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Linear classification and selective sampling under low noise conditions",
      "author" : [ "G. Cavallanti", "N. Cesa-Bianchi", "C. Gentile" ],
      "venue" : "In NIPS",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2009
    }, {
      "title" : "Improving generalization with active learning",
      "author" : [ "D.A. Cohn", "L.E. Atlas", "R.E. Ladner" ],
      "venue" : "Machine Learning, 15(2):201–221",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 1994
    }, {
      "title" : "Analysis of a greedy active learning strategy",
      "author" : [ "S. Dasgupta" ],
      "venue" : "In NIPS",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2005
    }, {
      "title" : "Coarse sample complexity bounds for active learning",
      "author" : [ "S. Dasgupta" ],
      "venue" : "In NIPS",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2006
    }, {
      "title" : "A general agnostic active learning algorithm",
      "author" : [ "S. Dasgupta", "D. Hsu", "C. Monteleoni" ],
      "venue" : "In NIPS",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2008
    }, {
      "title" : "Analysis of perceptron-based active learning",
      "author" : [ "S. Dasgupta", "A.T. Kalai", "C. Monteleoni" ],
      "venue" : "COLT, pages 249–263",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "editors",
      "author" : [ "L. Devroye", "L. Györfi", "G. Lugosi" ],
      "venue" : "A Probabilistic Theory of Pattern Recognition. Springer, New York",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 1996
    }, {
      "title" : "Selective sampling using the query by committee algorithm",
      "author" : [ "Y. Freund", "H.S. Seung", "E. Shamir", "N. Tishby" ],
      "venue" : "Machine Learning, 28(2-3):133–168",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 1997
    }, {
      "title" : "A bound on the label complexity of agnostic active learning",
      "author" : [ "S. Hanneke" ],
      "venue" : "ICML, pages 353–360",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Adaptive rates of convergence in active learning",
      "author" : [ "S. Hanneke" ],
      "venue" : "COLT",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Active learning in the non-realizable case",
      "author" : [ "M. Kääriäinen" ],
      "venue" : "ACL, pages 63–77",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Active + semi-supervised learning = robust multi-view learning",
      "author" : [ "I. Muslea", "S. Minton", "C.A. Knoblock" ],
      "venue" : "ICML, pages 435–442",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Optimal aggregation of classifiers in statistical learning",
      "author" : [ "A. Tsybakov" ],
      "venue" : "The Annals of Statistics, 32(1):135–166",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Sufficient conditions for agnostic active learnable",
      "author" : [ "L. Wang" ],
      "venue" : "In NIPS 22,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 1999
    }, {
      "title" : "On multi-view active learning and the combination with semisupervised learning",
      "author" : [ "W. Wang", "Z.-H. Zhou" ],
      "venue" : "ICML, pages 1152–1159",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2008
    } ],
    "referenceMentions" : [ {
      "referenceID" : 9,
      "context" : "In active learning [10, 13, 16], the learner draws unlabeled data from the unknown distribution defined on the learning task and actively queries some labels from an oracle.",
      "startOffset" : 19,
      "endOffset" : 31
    }, {
      "referenceID" : 12,
      "context" : "In active learning [10, 13, 16], the learner draws unlabeled data from the unknown distribution defined on the learning task and actively queries some labels from an oracle.",
      "startOffset" : 19,
      "endOffset" : 31
    }, {
      "referenceID" : 15,
      "context" : "In active learning [10, 13, 16], the learner draws unlabeled data from the unknown distribution defined on the learning task and actively queries some labels from an oracle.",
      "startOffset" : 19,
      "endOffset" : 31
    }, {
      "referenceID" : 3,
      "context" : "the hypothesis class) [4, 5, 11, 12, 14, 16].",
      "startOffset" : 22,
      "endOffset" : 44
    }, {
      "referenceID" : 4,
      "context" : "the hypothesis class) [4, 5, 11, 12, 14, 16].",
      "startOffset" : 22,
      "endOffset" : 44
    }, {
      "referenceID" : 10,
      "context" : "the hypothesis class) [4, 5, 11, 12, 14, 16].",
      "startOffset" : 22,
      "endOffset" : 44
    }, {
      "referenceID" : 11,
      "context" : "the hypothesis class) [4, 5, 11, 12, 14, 16].",
      "startOffset" : 22,
      "endOffset" : 44
    }, {
      "referenceID" : 13,
      "context" : "the hypothesis class) [4, 5, 11, 12, 14, 16].",
      "startOffset" : 22,
      "endOffset" : 44
    }, {
      "referenceID" : 15,
      "context" : "the hypothesis class) [4, 5, 11, 12, 14, 16].",
      "startOffset" : 22,
      "endOffset" : 44
    }, {
      "referenceID" : 1,
      "context" : ", the data cannot be perfectly separated by any hypothesis in the hypothesis class because of the noise) has been studied [2, 13, 17].",
      "startOffset" : 122,
      "endOffset" : 133
    }, {
      "referenceID" : 12,
      "context" : ", the data cannot be perfectly separated by any hypothesis in the hypothesis class because of the noise) has been studied [2, 13, 17].",
      "startOffset" : 122,
      "endOffset" : 133
    }, {
      "referenceID" : 16,
      "context" : ", the data cannot be perfectly separated by any hypothesis in the hypothesis class because of the noise) has been studied [2, 13, 17].",
      "startOffset" : 122,
      "endOffset" : 133
    }, {
      "referenceID" : 18,
      "context" : "It is worth noting that these bounds obtained in the non-realizable case match the lower bound Ω( 2 ǫ2 ) [19], in the same order as the upper bound O( 1 ǫ2 ) of passive learning (η denotes the generalization error rate of the optimal classifier in the hypothesis class and ǫ bounds how close to the optimal classifier in the hypothesis class the active learner has to get).",
      "startOffset" : 105,
      "endOffset" : 109
    }, {
      "referenceID" : 20,
      "context" : "Tsybakov noise model [21] is more and more popular in theoretical analysis on the sample complexity of active learning.",
      "startOffset" : 21,
      "endOffset" : 25
    }, {
      "referenceID" : 7,
      "context" : "However, existing result [8] shows that obtaining exponential improvement in the sample complexity of active learning with unbounded Tsybakov noise is hard.",
      "startOffset" : 25,
      "endOffset" : 28
    }, {
      "referenceID" : 22,
      "context" : "Inspired by [23] which proved that multi-view setting [6] can help improve the sample complexity of active learning in the realizable case remarkably, we have an insight that multi-view setting will also help active learning in the non-realizable case.",
      "startOffset" : 12,
      "endOffset" : 16
    }, {
      "referenceID" : 5,
      "context" : "Inspired by [23] which proved that multi-view setting [6] can help improve the sample complexity of active learning in the realizable case remarkably, we have an insight that multi-view setting will also help active learning in the non-realizable case.",
      "startOffset" : 54,
      "endOffset" : 57
    }, {
      "referenceID" : 2,
      "context" : "-We define α-expansion, which extends the definition in [3] and [23] to the non-realizable case, and β-condition for multi-view setting.",
      "startOffset" : 56,
      "endOffset" : 59
    }, {
      "referenceID" : 22,
      "context" : "-We define α-expansion, which extends the definition in [3] and [23] to the non-realizable case, and β-condition for multi-view setting.",
      "startOffset" : 64,
      "endOffset" : 68
    }, {
      "referenceID" : 1,
      "context" : "[2] proposed the agnostic active learning algorithm A2 and proved that its sample complexity is Ô( 2 ǫ2 ).",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 16,
      "context" : "2 Hoping to get tighter bound on the sample complexity of the algorithm A2, Hanneke [17] defined the disagreement coefficient θ, which depends on the hypothesis class and the data distribution, and proved that the sample complexity of the algorithm A2 is Ô(θ2 η 2 ǫ2 ).",
      "startOffset" : 84,
      "endOffset" : 88
    }, {
      "referenceID" : 12,
      "context" : "[13] developed a general agnostic active learning algorithm which extends the scheme in [10] and proved that its sample complexity is Ô(θ η 2 ǫ2 ).",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 9,
      "context" : "[13] developed a general agnostic active learning algorithm which extends the scheme in [10] and proved that its sample complexity is Ô(θ η 2 ǫ2 ).",
      "startOffset" : 88,
      "endOffset" : 92
    }, {
      "referenceID" : 20,
      "context" : "Recently, the popular Tsybakov noise model [21] was considered in theoretical analysis on active learning and there have been some bounds on the sample complexity.",
      "startOffset" : 43,
      "endOffset" : 47
    }, {
      "referenceID" : 3,
      "context" : "For some simple cases, where Tsybakov noise is bounded, it has been proved that the exponential improvement in the sample complexity is possible [4, 7, 18].",
      "startOffset" : 145,
      "endOffset" : 155
    }, {
      "referenceID" : 6,
      "context" : "For some simple cases, where Tsybakov noise is bounded, it has been proved that the exponential improvement in the sample complexity is possible [4, 7, 18].",
      "startOffset" : 145,
      "endOffset" : 155
    }, {
      "referenceID" : 17,
      "context" : "For some simple cases, where Tsybakov noise is bounded, it has been proved that the exponential improvement in the sample complexity is possible [4, 7, 18].",
      "startOffset" : 145,
      "endOffset" : 155
    }, {
      "referenceID" : 3,
      "context" : "[4] assumed that the samples are drawn uniformly from the the unit ball in Rd and proved that the sample complexity of active learning with unbounded Tsybakov noise is O ( ǫ 2 1+λ ) (λ > 0 depends on Tsybakov noise).",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 7,
      "context" : "Castro and Nowak [8] showed that the sample complexity of active learning with unbounded Tsybakov noise is Ô ( ǫ − 2μω+d−2ω−1 μω ) (μ > 1 depends on another form of Tsybakov noise, ω ≥ 1 depends on the Hölder smoothness and d is the dimension of the data).",
      "startOffset" : 17,
      "endOffset" : 20
    }, {
      "referenceID" : 8,
      "context" : "[9] assumed that the labels of examples are generated according to a simple linear noise model and indicated that the sample complexity of active learning with unbounded Tsybakov noise is O ( ǫ − 2(3+λ) (1+λ)(2+λ) ) .",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 17,
      "context" : "Hanneke [18] proved that the algorithms or variants thereof in [2] and [13] can achieve the polynomial sample complexity Ô ( ǫ 2 1+λ ) for active learning with unbounded Tsybakov noise.",
      "startOffset" : 8,
      "endOffset" : 12
    }, {
      "referenceID" : 1,
      "context" : "Hanneke [18] proved that the algorithms or variants thereof in [2] and [13] can achieve the polynomial sample complexity Ô ( ǫ 2 1+λ ) for active learning with unbounded Tsybakov noise.",
      "startOffset" : 63,
      "endOffset" : 66
    }, {
      "referenceID" : 12,
      "context" : "Hanneke [18] proved that the algorithms or variants thereof in [2] and [13] can achieve the polynomial sample complexity Ô ( ǫ 2 1+λ ) for active learning with unbounded Tsybakov noise.",
      "startOffset" : 71,
      "endOffset" : 75
    }, {
      "referenceID" : 7,
      "context" : "For active learning with unbounded Tsybakov noise, Castro and Nowak [8] also proved that at least Ω(ǫ−ρ) labels are requested to learn an ǫ-approximation of the optimal classifier (ρ ∈ (0, 2) depends on Tsybakov noise).",
      "startOffset" : 68,
      "endOffset" : 71
    }, {
      "referenceID" : 21,
      "context" : "Wang [22] introduced smooth assumption to active learning with approximate Tsybakov noise and proved that if the classification boundary and the underlying distribution are smooth to ξ-th order and ξ > d, the sample complexity of active learning is Ô ( ǫ 2d ξ+d ) ; if the boundary and the distribution are infinitely smooth, the sample complexity of active learning is O ( polylog(1ǫ ) ) .",
      "startOffset" : 5,
      "endOffset" : 9
    }, {
      "referenceID" : 14,
      "context" : "It is well-known [15] that S ∗ v = {xv : φv(xv) ≥ 1 2}, where φv(xv) = P (y = 1|xv).",
      "startOffset" : 17,
      "endOffset" : 21
    }, {
      "referenceID" : 20,
      "context" : "In order to model the noise, we assume that the data distribution and the Bayes decision boundary in each view satisfies the popular Tsybakov noise condition [21] that Prxv∈Xv (|φv(xv) − 1/2| ≤ t) ≤ C0t λ for some finite C0 > 0, λ > 0 and all 0 < t ≤ 1/2, where λ = ∞ corresponds to the best learning situation and the noise is called bounded [8]; while λ = 0 corresponds to the worst situation.",
      "startOffset" : 158,
      "endOffset" : 162
    }, {
      "referenceID" : 7,
      "context" : "In order to model the noise, we assume that the data distribution and the Bayes decision boundary in each view satisfies the popular Tsybakov noise condition [21] that Prxv∈Xv (|φv(xv) − 1/2| ≤ t) ≤ C0t λ for some finite C0 > 0, λ > 0 and all 0 < t ≤ 1/2, where λ = ∞ corresponds to the best learning situation and the noise is called bounded [8]; while λ = 0 corresponds to the worst situation.",
      "startOffset" : 343,
      "endOffset" : 346
    }, {
      "referenceID" : 7,
      "context" : "When λ < ∞, the noise is called unbounded [8].",
      "startOffset" : 42,
      "endOffset" : 45
    }, {
      "referenceID" : 20,
      "context" : "According to Proposition 1 in [21], it is easy to know that (2) holds.",
      "startOffset" : 30,
      "endOffset" : 34
    }, {
      "referenceID" : 0,
      "context" : "We will use the following lamma [1] which gives the standard sample complexity for non-realizable learning task.",
      "startOffset" : 32,
      "endOffset" : 35
    }, {
      "referenceID" : 19,
      "context" : "Multi-view active learning first described in [20] focuses on the contention points (i.",
      "startOffset" : 46,
      "endOffset" : 50
    }, {
      "referenceID" : 2,
      "context" : "[3] also gave a definition of expansion, Pr(T1⊕T2) ≥ αmin [ Pr(T1∩T2), P r(T1∩T2) ] , for realizable learning task under the assumptions that the learner in each view is never “confident but wrong” and the learning algorithm is able to learn from positive data only.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 2,
      "context" : "In addition, in [3] the instances which are agreed by the two views but are predicted different label by the optimal classifier can be denoted as T1 ∩ T2.",
      "startOffset" : 16,
      "endOffset" : 19
    }, {
      "referenceID" : 2,
      "context" : "So, it can be found that Definition 1 and the definition of expansion in [3] are based on the same intuition that the amount of contention points is no less than a fraction of the amount of instances which are agreed by the two views but are predicted different label by the optimal classifiers.",
      "startOffset" : 73,
      "endOffset" : 76
    }, {
      "referenceID" : 22,
      "context" : "As discussed in [23], we also assume that the learner in Table 1 satisfies the non-degradation condition as the amount of labeled training examples increases, i.",
      "startOffset" : 16,
      "endOffset" : 20
    }, {
      "referenceID" : 19,
      "context" : "In the experiment we use the semi-artificial data set [20] and the course data set [6].",
      "startOffset" : 54,
      "endOffset" : 58
    }, {
      "referenceID" : 5,
      "context" : "In the experiment we use the semi-artificial data set [20] and the course data set [6].",
      "startOffset" : 83,
      "endOffset" : 86
    } ],
    "year" : 2010,
    "abstractText" : "The sample complexity of active learning under the realizability assumption has been well-studied. The realizability assumption, however, rarely holds in practice. In this paper, we theoretically characterize the sample complexity of active learning in the non-realizable case under multiview setting. We prove that, with unbounded Tsybakov noise, the sample complexity of multiview active learning can be Õ(log 1ǫ ), contrasting to single-view setting where the polynomial improvement is the best possible achievement. We also prove that in general multi-view setting the sample complexity of active learning with unbounded Tsybakov noise is Õ(1ǫ ), where the order of 1/ǫ is independent of the parameter in Tsybakov noise, contrasting to previous polynomial bounds where the order of 1/ǫ is related to the parameter in Tsybakov noise.",
    "creator" : "LaTeX with hyperref package"
  }
}