{
  "name" : "1108.6296.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "InfTucker: t-Process based Infinite Tensor Decomposition",
    "authors" : [ "Zenglin Xu" ],
    "emails" : [ "xu218@purdue.edu", "yan12@purdue.edu", "alanqi@cs.purdue.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n10 8.\n62 96\nv1 ["
    }, {
      "heading" : "1 Introduction",
      "text" : "Many real world datasets are naturally described by multiway arrays or tensors. For example, histories of email correspondences can be represented by a tensor with 4 modes (sender, receiver, date, content); and user customer ratings can be represented by a tenor with four modes (user, item, rating, time). Therefore there is a critical need for principled multiway data analysis tools.\nTraditional multiway factor models— such as the Tucker decomposition [21] and CANDECOMP/PARAFAC (CP) [6]—have been widely applied in many areas (e.g. , network traffic analysis [24], computer vision [17] and social network analysis [18, 14, 19], etc). These models, however, face serious challenges for modeling complex multiway interactions. First the interactions between entities in each mode may be coupled together and highly nonlinear. The classical multi-linear models cannot capture these intricate relationships. Second, the tensor data are often noisy and contain many outliers; classical models are not designed to handle outliers. Third, the data may not be restricted to real values; they can be binary as in dynamic network data or they may have ordinal values for user-movie-ratings. The classical models treat all these data as real-valued and this treatment would lead to degenerated predictive performance.\nTo address these challenges, we propose a robust nonparametric multiway analysis model, InfTucker . It conducts the Tucker decomposition in an infinite dimensional feature space, which captures nonlinear interactions between different tensor modes. Compared with the tensor-variate Gaussian process approaches [7, 23], the t-processbased InfTucker is more robust (not sensitive to data outliers). Furthermore, it handles various data types (e.g. , incomplete or binary data) by adopting suitable probabilistic data likelihoods.\nAlthough our nonparametric models offer elegant solutions to robust multiway analysis, computationally it is difficult to train them from multi-aspect data. To overcome this computational difficulty, we develop an efficient variational Bayesian approach that explores the tensor structures to reduce the computational cost.\nIn summary, the major contributions of this paper include: i. We propose a tensor-variate latent t process model, InfTucker , for robust multiway\ndata analysis. Recently several nonparametric Bayesian models on matrices and tensors [5, 23, 4] have been developed, but they are either designed for different tasks (e.g. , multitask learning)—-rather than multiway analysis—or use a linear kernel to make the computation feasible in practice. By contrast, based on latent t-processes, our model conducts robust nonlinear Tucker decomposition. ii. We present a novel technique that reduces the computational complexity of variational inference on tensors. Specifically, for a K-mode tensor with nk entities in each mode, our algorithm has O(\n∑K k=1 n 3 k + ( ∑K k=1 nk) ∏K k=1 nk) time and\nO( ∑K k=1 n 2 k + ∏K k=1 nk) space complexities, instead of the O( ∏K k=1 n 3 k) time and O( ∏K\nk=1 n 2 k) space complexities of a naı̈ve inference technique. The efficient\ninference technique enables us to use tensor-variate t-processes with nonlinear covariance functions. iii. Our experimental results on chemometrics and social network datasets demonstrate that the InfTucker achieves significantly higher prediction accuracy than\nseveral state-of-art tensor decomposition approaches including High Order Singular Value Decomposition (HOSVD) [10], Weighted CP [1] and nonnegative tensor decomposition [17]."
    }, {
      "heading" : "2 Preliminary",
      "text" : "Notations. Throughout this paper, we denote scalars by lower case letters (e.g. a), vectors by bold lower case letters (e.g. a), matrices by bold upper case letters (e.g. A), and tensors by calligraphic upper case letters (e.g. A). Calligraphic upper case letters are also used for probability distributions, e.g., N (µ,Σ). We use uij to represent the (i, j) entry of a matrix U, yi to represent the i = (i1, . . . , iK) entry of a tensor Y . U ⊗ V denotes the Kronecker product of the two matrices there. We define the vectorization operation, denoted by vec(Y), to stack the tensor entries into a\n∏K k=1 nk\nby 1 vector. The entry i = (i1, . . . , ıK) of Y is mapped to the entry at position j = iK + ∑K−1 i=1 (ik − 1) ∏K k+1 nk of vec(Y)\n1. The mode-k product of a tensor W ∈ R\nr1×...×rK with a matrix U ∈ Rn×rk is denoted as W ×k U and it is of size r1 × . . .× rk−1 × n× rk+1 × . . .× rK . The corresponding entry-wise definition is (W ×k U)i1...ik−1jik+1...iK = ∑rk j=1 wi1...iKujik .\nTensor decomposition: There are two families of tensor decomposition, the Tucker family and the CP family. The Tucker family extends bilinear factorization models to handle tensor datasets. For an observed K-mode tensor Y ∈ Rn1×...×nK , the general form of Tucker decomposition is\nY = W ×1 U (1) ×2 . . .×K U (K) (1)\nwhere W ∈ Rr1×...×rK is the core tensor, and U(k) ∈ Rnk×rk are K latent factor matrices. As in Kolda and Bader [9], We collectively denote the group of K matrices as a Tucker tensor with a identity core U = [ U(1), . . . ,U(K) ]\n—this allows us to compactly represent the Tucker decomposition as Y = W ×U . The vector form of (1) is\nvec(W ×U) = U(1) ⊗U(2) ⊗ . . .⊗U(K) · vec(W) (2)\nThe CP family is a restricted form of the Tucker family. The entry-wise definition of CP is yi1...iK = ∑r l=1 λlui1l . . . uiK l. The alternating least square (ALS) method has been used to solve both Tucker decomposition and CP [9]."
    }, {
      "heading" : "3 Robust infinite Tucker decomposition by t processes",
      "text" : "In this section we present the robust infinite Tucker decomposition based on latent t processes. We extend classical Tucker decomposition in three aspects: i) flexible noise models for both continuous and binary observations; ii) an infinite core tensor to model complex interactions; and iii) latent t process prior, which makes the model robust to outliers.\n1Unlike the usual column-wise vec-operation, our definition of vec() on matrices is row-wise, which avoids the use of transpose in many equations throughout this paper.\nMore specifically, we assume the observed tensor Y is sampled from a latent realvalued tensor M via a probabilistic noise model p(Y|M) = ∏\ni p(yi|mi). We conduct Tucker decomposition of M with a core tensor W of infinite size. To do so, we use a countably infinite feature mapping for the rows of the component matrix U(k) ∈ Rnk×r, k = 1, . . . ,K . Let u(k)i denotes the i-th row of U (k), A feature mapping φ : Rr → Rℵ0 maps each u(k)i to the infinite feature space φ(u (k) i ), where ℵ0 denotes the countable infinity. The inner product of the feature mapping is denoted as Σ(k)ij = 〈φ(u (k) i ), φ(u (k) j )〉. Let φ (r)(u (k) i ) = [φ1(u (k) i ), . . . , φr(u (k) i )] denote the first r coordinates of φ(u(k)i ), W ∈ R ℵK0 denote an infinite K-mode core tensor, and W(r) = (wi)rik=1 ∈ R rK denote the first r dimensions in every mode of W . The infinite Tucker decomposition “M = W × φ(U)” for the latent tensor M can be formally defined as the limit of a series of finite Tucker decompositions.\nM = lim r→∞\nW(r) ×1 φ (r)(U(1))×2 . . .×K φ (r)(U(K)) (3)\nwhere φr(U(k)) = [φ(r)(u(k)1 ) ⊤, . . . , φ(r)(u (k) nk ) ⊤]⊤. As shown in the next Section, we use a latent tensor-variate t process prior on W and then marginalize it out to obtain a t process over M—this makes the estimation of M to the presence of strong noise and outliers."
    }, {
      "heading" : "3.1 Tensor-variate t processes",
      "text" : "Before formally defining the tensor-variate t process, we denote the domain of the mode k by Uk, the K covariance functions by Σ(k) : Uk × Uk → R, the covariance matrices by a Tucker tensor S− 1 2 = [ ( Σ(1) )− 1 2 , . . . , ( Σ(K) )− 1 2 ] and n = ∏K\nk=1 nk. The norm of the a tensor ‖A‖ is defined as √ ∑\ni a 2 i . Then we define tensor-variate t\nprocesses as follows.\nDefinition 1 (Tensor-variate t Distributions and Tensor-variate t Processes) Given K location sets Uk, k = 1, . . . ,K , let b : U1 × . . .× UK → R be the mean function. M = {f(u(1), . . . ,u(K))|u(k) ∈ Uk} is a set of random tensor variables where f : U1 × . . .×UK → R is a random function. For any finite sets {u (k) 1 , . . . ,u (k) nk } K k=1, let M = [f(u (1) j1 , . . . ,u (K) jK )]∀j ∈ Rn1×...×nK , where jk = 1, . . . , nk, be a random tensor and B = [b(u(1)j1 , . . . ,u (K) jK )]∀j ∈ Rn1×...×nK be the mean tensor. Let Γ(x) be the Gamma function. The set M follows a tensor-variate t process T T P(ν, b, {Σ(k)}Kk=1) with degree of freedom ν > 2, if M follows tensor t distribution with the following density\nT T (M|ν,B, {Σ(k)}Kk=1) = Γ(n+ν2 ) ∏K k=1 |Σ (k)| − n 2nk\nΓ(ν2 )(νπ) n 2\n(\n1 + 1\nν ‖(M−B)× S−\n1 2 ‖2 )− 1 2 (n+ν) (4)\nIn this paper, we set the mean function to be zero, i.e. B = 0. Let T (ν,µ,Σ) denotes t distribution with mean µ, covariance Σ and degree of freedom ν. If the latent tensor M is drawn from a tensor-variate t process, then vec(M) ∼ T (0,Ψ), where Ψ =\nΣ(1) ⊗ . . . ⊗ Σ(K). We choose the prior on the truncated core tensor W(r) to be T T (ν,0, {Ir}Kk=1), where Ir denotes the identity matrix. The next theorem proves that the limit defined in (3) is the corresponding tensor process.\nTheorem 2 Let Uk ⊂ Rr, and Σ (k) r (u (k) i ,u (k) j ) = 〈φ (r)(u (k) i ), φ (r)(u (k) j )〉 be a series of covariance functions. Define a multi-linear function by\nt(r)(u(1), . . . ,u(K)) = W(r) ×1 φ (r)(u(1)) . . .×K φ (r)(u(K)), u(k) ∈ Uk (5)\nIf W(r) ∼ T T (ν,0, {Ir}Kk=1), then t (r)(u(1), . . . ,u(K)) follows a tensor-variate t distribution T T P(ν,0, {Σ(k)r }Kk=1), and it converges to T T P(ν,0, {Σ (k)}Kk=1) in distribution as r → ∞.\nThe proof can be found in the appendix. The above theorem shows that probabilistic infinite Tucker decomposition of M can be realized by modelingM as a draw from a tensor-variate t process on the location vectors induced from the unknown component matrices U(k). Our definition of tensor-variate t processes generalizes matrix-variate t process defined in [25]. Theorem 2 also suggests a constructive definition of tensorvariate processes for general covariance functions.\nFinally, to encourage sparsity in estimated u(k)i —for easy model interpretation—\nwe use Laplace prior u(k)i ∼ L(λ) ∝ exp(−λ‖u (k) i ‖1)."
    }, {
      "heading" : "3.2 Noise models",
      "text" : "We use a noise model p(Y|M) to link the infinite Tucker decomposition and the tensor observation Y .\nProbit model: In this case, each entry of the observation is binary; that is, yi ∈ {0, 1}. A probit function p(yi|mi) = Φ(mi)yi(1 − Φ(mi))1−yi models the binary observation. Note that Φ(·) is the standard normal cumulative distribution function.\nGaussian model: We use a Gaussian likelihood p(yi|mi) = N (yi|mi, σ2) to model the real-valued observation yi.\nMissing values: We allow missing values in the observation. Let O denote the indices of the observed entries in Y . Then we have p(YO|MO) as the likelihood.\nOther noise models include modified probit models for ordinal regression and multiclass classification [2], null category noise models for semi-supervised classification [12]. In this paper we focus on probit and Gaussian models."
    }, {
      "heading" : "4 Inference",
      "text" : "Given the observed tensor Y , we aim to estimate the component matrices U(k) by maximizing the marginal likelihood p(Y|{U(k)}Kk=1)p({U\n(k)}Kk=1). Integrating out M in the above equation is intractable however. Therefore, we resort to approximate inference; more specifically, we develop a variational expectation maximization (EM) algorithm. In the following paragraphs, we first present the inference algorithms for both of the noise models, and then describe an efficient algebraic approach to significantly reduce the computation complexity."
    }, {
      "heading" : "4.1 Variational EM for binary classification",
      "text" : "We follow the data augmentation scheme by Albert and Chib [2] to decompose the probit model into p(yi|mi) = ∫\np(yi|zi)p(zi|mi)dzi . Let δ(·) be the indicator function, we have\np(yi|zi) = δ(yi = 1)δ(zi > 0) + δ(yi = 0)δ(zi ≤ 0), p(zi|mi) = N (zi|mi, 1) (6)\nIt is well known that a t distribution can be factorized into a normal distribution convolved with a Gamma distribution, such that\nT T (M|ν,0, {Σ(k)}Kk=1) =\n∫\nT N (M|0, {η−1/KΣ(k)}Kk=1)Gam(η|ν/2, ν/2)dη (7)\nT N (M|B, {Σ(k)}Kk=1) = (2π) −n 2\nK ∏\nk=1\n|Σ(k)| − n 2nk exp\n(\n− 1\n2 ‖(M−B)× S−\n1 2 ‖2 ) (8)\nwhere T N denotes the tensor-variate normal distribution. The joint probability likelihood with data augmentation is\np(Y,Z,M, η,U) = p(Y|Z)p(Z|M)p(M|η,U)p(η)p(U). (9)\nwhere p(M|η,U) and p(η) is the tensor-variate normal distribution and the Gamma distribution in (7). p(U) is the Laplace prior.\nOur variational EM algorithm consists of a variational E-step and a gradient-based M-step. In the E-step, we approximate the posterior distribution p(Z,M, η|Y,U) by a fully factorized distribution q(Z,M, η) = q(Z)q(M)q(η). Variational inference minimizes the Kullback-Leibler (KL) divergence between the approximate posterior and the true posterior.\nmin q KL (q(Z)q(M)q(η)‖p(Z,M, η|Y,U)) . (10)\nThe variational approach optimizes one approximate distribution, e.g. , q(Z), in (10) at a time, while having all the other approximate distributions fixed [3]. We loop over q(Z), q(M) and q(η) to iteratively optimize the KL divergence until convergence.\nGiven q(M) and q(η), the q(zi) is a truncated normal distribution\nq(zi) ∝ N (Eq [mi] , 1)δ(zi > 1), Eq [zi] = Eq [mi] + (2yi − 1)N (Eq [mi] |0, 1)\nΦ((2yi − 1)Eq [mi]) .\n(11)\nGiven q(Z) and q(η), it is more convenient to write the optimized approximate distribution for M in its vectorized form. Let Σp = Σ(1) ⊗ . . .⊗Σ(K), we have\nq(vec(M)) = N (vec(M)|µ,Υ), µ = vec(Eq [M]) = Υ vec(Eq [Z]) (12)\nΥ = Eq [η] −1 Σp\n(\nI+ Eq [η] −1 Σp\n)−1\n. (13)\nThe optimized q(η) is also a Gamma distribution:\nq(η) = Gam(η|β1, β2), Eq [η] = β1 β2 , β1 = ν + n 2 , β2 =\nν + µ⊤Σ−1p µ+ tr(Σ −1 p Υ)\n2 .\n(14)\nBased on the variational approximate distribution obtained in the E-step, we maximize the expected log likelihood over U = [ U(1), . . . ,U(K) ] in the M-step.\nmax U Eq [log p(Y,Z,M, η|U)p(U)] . (15)\nAfter eliminating constant terms, we need to solve the following optimization problem\nmin U\nf(U) = K ∑\nk=1\nn\nnk log |Σ(k)|+ τ‖Eq [M]× S −1/2‖2 + τ tr ( Σ−1p Υ ) + λ\nK ∑\nk=1\n‖Uk‖1,\n(16)\nwhere τ = Eq [η]. In the above equation (16), Σ(k) = Σ(k)(U(k),U(k)) is considered as a function of Uk, and S−1/2 is a function of U . Υ and τ are the statistics computed in the E-step, and they have fixed values. Omitting the last ℓ1 term in (16), the gradient of f(U) w.r.t. to a scalar u(k)ij is given in the appendix.\nWith an ℓ1 penalty on f(U), we choose a projected scaled subgradient L-BFGS algorithm for optimization—due to its excellent performance [16]."
    }, {
      "heading" : "4.2 Regression",
      "text" : "Inference: The inference for the regression case follows the same format as the binary classification case. The only changes are: 1) replacing Eq [Z] by Y and skipping updating q(Z). 2) The variational EM algorithm are only applied to the observed entries.\nPrediction: Given a missing value index i = (i1, . . . , iK), the predictive distribution is\np(yi|YO) ≈\n∫\np(yi|mi)p(mi|M, η)q(M)q(η)dMdη\nThe above integral is intractable, so we replace η integral q(η)dη by the mode of its approximate posterior distribution τ∗ = (β1 − 1)/β2, thus the predictive distribution is approximated by\n∫\np(yi|mi)p(mi|M, τ ∗)q(M)dM\n= N (yi|k ⊤(Σp + σ 2τ∗I)−1 vec(Y), 1\nτ∗ [k(i, i)− k⊤(Σp + σ 2τ∗I)−1k]) (17)\nwhere k(i, j) = ∏K k=1 Σ (k)(u (k) ik ,u (k) jk ) and k = [k(i, j)]⊤j∈O."
    }, {
      "heading" : "4.3 Efficient Algorithms",
      "text" : "A naı̈ve implementation of the above algorithm requires prohibitive O( ∏K k=1 n 3 k) time complexity and O( ∏K\nk=1 n 2 k) space complexity for each EM iteration. The key com-\nputation bottlenecks are the operations involving Υ defined in equation (13). To avoid this high complexity, we can make use of the Kronecker product structure. We assume Eq [η] = 1 to simplify the computation, it is easy to adapt our computation strategies to Eq [η] 6= 1. Let Σ(k) = V(k)Λ(k)V(k)⊤ be the singular value decomposition of the covariance matrix Σ(k), V(k) is an orthogonal matrix and Λ(k) is a diagonal matrix. Υ can be represented as\nΥ = V(1)Λ(1)V(1)⊤ ⊗ . . .⊗V(K)Λ(K)V(K)⊤(V(1)V(1)⊤ ⊗ . . .⊗V(K)V(K)⊤+\nV(1)Λ(1)V(1)⊤ ⊗ . . .⊗V(K)Λ(K)V(K)⊤)−1\n= V(1)Λ(1)(I+Λ(1))−1V(1)⊤ ⊗ . . .⊗V(K)Λ(K)(I+Λ(K))−1V(K)⊤.\nLet V = V(1) ⊗ . . .⊗V(K), Λ = Λ(1)(I+Λ(1))−1 ⊗ . . .⊗Λ(K)(I+Λ(K))−1. It is obvious that V is an orthogonal matrix and Λ is a diagonal matrix. The above relation implies that we can actually compute the singular value decomposition of Υ = VΛV from covariance matrices Σ(k).\nIn order to efficiently compute tr(Σ−1p Υ) appearing in equation (16), we use the following relations\ntr(Σ−1p Υ) = tr(Σ −1 p V ⊤ΛV) = tr(ΛVΣ−1p V ⊤) = diag(VΣ−1p V ⊤)⊤ diag(Λ)\n= d1 ⊗ . . .⊗ dK diag(Λ) = d1 ⊗ . . .⊗ dK vec(D)\n= D ×1 d1 . . .×K dK , (18)\nwhere dk = diag(V(k)(Σ(k))−1V(k) ⊤ )⊤ with Σ(k) being a computed statistics in the E-step, diag(Λ) denotes the diagonal elements of Λ, and D is a tensor of size n1 × . . .× nK , such that vec(D) = diag(Λ). Both time and space complexities of the last formula (18) is O(\n∏K k=1 nk).\nWe denote V = [V(1), . . . ,V(K)] and V⊤ = [V(1)⊤, . . . ,V(K)⊤]. For any tensor A of the same size as D, Λ vec(A) means multiplying the j-th element of vec(A) by the Λjj , which is the j-th element of vecD. So we have Λ vec(A) = vec(D ⊙ A), where ⊙ denotes the Hadamard product, i.e. entry-wise product. In light of this relation, we can efficiently compute (12) by\nΥ vec(Eq [Z]) = vec [ ((Eq [Z]× V ⊤)⊙D)× V ] . (19)\nThe right-hand side of Equation (19) effectively reduce the time and space complexities of the left-hand side operations toO(\n∑K k=1 n 3 k+( ∑K k=1 nk) ∏K k=1 nk) andO( ∑K k=1 n 2 k+\n∏K k=1 nk), respectively.\nWe can further reduce the complexities by approximating the covariance matrices via truncated SVD. If the first tk leading eigenvalues and the corresponding eigenvectors are preserved, the time complexity of our algorithm is O(\n∑K k=1 n 2 ktk+( ∑K k=1 nk) ∏K k=1 tk),\nand the space complexity is O( ∑K k=1 nktk + ∏K k=1 tk)."
    }, {
      "heading" : "5 Related Works",
      "text" : "The InfTucker model extends Probabilistic PCA (PPCA) [20] and Gaussian process latent variable models (GPLVMs) [11]: while PPCA and GPLVM model interactions of one mode of a matrix and ignore the joint interactions of two modes, InfTucker does.\nOur model is related to previous tensor-variate GPs Yu et al. [23], Yu and Chu [22]. The main difference lies in the fact they used linear covariance functions to reduce the computational complexities and dealt with matrix-variate data for online recommendation and link prediction.\nOur model is closely related to the probabilistic Tucker-3 (pTucker) model [5]; actually InfTucker reduces to pTucker as a special case as ν → ∞. Also, Hoff [7] proposed a hierarchical Bayesian extension to CANDECOMP/PARAFAC that captures the interaction of component matrices. Unlike both [5]’s and Hoff [7]’s approach, ours handles non-Gaussian noise and uses nonlinear covariance functions to model complex interactions. In addition, [7] used a Gibbs sampler for inference—its convergence incurs high computational cost and makes this approach infeasible for tensors with moderate and large sizes.\nTo handle missing data, enhance model interpretability, and avoid overfitting, several extensions to non-probabilistic tensor decomposition have been proposed, including nonnegative tensor decomposition (NTD) [17, 8, 13, 15] and Weighted tensor decomposition (WTD) [1]. These models solve the core tensors explicitly and do not handle nonlinear multiway interactions.\nFinally, note that the inference technique described in Section 4 can be adopted for Gaussian process or t-process multi-task learning [4, 25]. Let M be the number of tasks and N be the number of data points in each task. Our inference technique can be used to reduce their time complexity from O(M3N3) to O(M3 + N3) and the space complexity from O(M2N2) to O(N2 +M2)."
    }, {
      "heading" : "6 Experiments",
      "text" : "To evaluate the proposed InfTucker decomposition method, we conduct two batches of experiments: one for the setting with the Gaussian noise and the other for the probit noise. For both settings, we compare InfTucker with the following conventional and state-of-the-arts tensor decomposition methods: CP, Tucker decomposition (TD), Nonnegative CP (NCP), High Order SVD (HOSVD), and Weighted CP (WCP). All of\nthese comparison methods are implemented in the tensor toolbox2. Experiment on Gaussian Noise Model Experimental setting: We employ four continuous chemometrics datasets3, namely amino, bread, flow injection, and suger. Their dimensions are 5×201×61, 10×11×8, 12× 100× 89, and 268× 571× 7, respectively. All the above datasets are normalized to have zero mean and unite variance. For each dataset, we randomly split all the tensor coordinates to 5 folds via cross-validation: each time one fold used for testing and the other four folds for training. This procedure is repeated for 10 times. Thus in total, for each dataset, all algorithms are repeated 50 times. In InfTucker , the degree of freedom ν in the tensor-variate t process is fixed to 10. We employ the Gaussian/exponential covariance functions Σ(k)(ui,uj) = e−γ‖ui−uj‖ t\n, where t = 1, 2 and γ is selected from [0.01 : 0.05 : 1] by 5-fold cross validation. The regularization parameter λ for InfTucker is chosen from {1, 10, 100}.\nResults: Since for all these datasets we do not know the true latent factors, we use the prediction accuracy on hold out edges to compare all these models. For easy comparison of all algorithms, we set the same number of latent factors for all modes. We varied the number of the latent factors, r, from 3 to 5 and computed the averaged MSE values and standard deviations for all algorithms. Due to the relatively low dimensionality of certain modes in the above mentioned tensors, we find that when r = 3, the MSEs are all near optimal. Thus we only reported the MSEs for r = 3 in Table 1, where the lowest MSE value and those not significantly worse than it (achieved by t-test with 95% confidence level) are highlighted. It is not hard to find that InfTucker achieves the lowest MSEs on all data. This may be due to the so-called “kernel trick” that the non-linear covariance functions provide more flexibility in modeling the interaction among all modes. Among the comparison methods, WCP ranks the second best, while HOSVD is the worst due to the strong constraint on the non-negativity of latent factors.\nExperiment on Probit Noise Model Experimental setting: We extracted three binary multi-way social network datasets,\nnoted as Enron, Digg1, and Digg2, as our evaluation data. Enron is a relational dataset\n2http://csmr.ca.sandia.gov/˜tgkolda/TensorToolbox/ 3Available from http://www.models.kvl.dk/datasets\ndescribing the three-way relation: sender-receiver-email. This dataset, extracted from the Enron email4, has the dimensionality of 203 × 203 × 200 with 0.01% non-zero elements. The Digg1 and Digg2 datasets are all extracted from a social news website digg.com5. Digg1 describes a three-way relation: news-keyword-topic, and Digg2 describes a four-way relation: user-news-keyword-topic. Digg1 has the dimensionality of 581× 124× 48 with 0.024% non-zero elements, and Digg2 has the dimensionality of 22 × 109 × 330 × 30 with 0.002% non-zero elements. All these datasets are very sparse.\nResults: We adopt similar settings to the Gaussian noise case except that r is varied from {3,5,8,10,15,20} due to the larger dimension of each mode. Since the data are binary, MSE cannot well reveal the existence of a missing link in social networks. Therefore, we evaluate all the algorithms by Area Under Curve (AUC) values averaged over 50 runs. The larger the AUC values, the better. In InfTucker , we use Eq [M] as predictions to calculate the AUC values. We report the averaged AUC values for all algorithms in Figure 1. It is observed that the proposed InfTucker model sufficiently outperforms all the other models. The failure of the traditional models like CP and TD is that the least square minimization used in these models easily leads to almost all zero predictions since the tensor datasets are highly sparse. This experiment indicates the necessity of probit noise models for binary data."
    }, {
      "heading" : "7 Conclusion",
      "text" : "We proposed a new nonparametric Bayesian tensor decomposition framework, InfTucker , where the observation tensor is modeled as a draw from a tensor-variate t process. This framework is able to flexibly model the interdependence among multimodes by conducting tensor decomposition in the infinite feature space. It can also deal with several types of data, e.g. incomplete data and binary data, via various noise modeling approaches, while most traditional methods rely on least square estimations. As a significant contribution, we also proposed an efficient variational approach to estimate the component matrices. Experimental results on chemometrics and social network datasets demonstrate that the new InfTucker model achieves significantly higher prediction accuracy than several state-of-art tensor decomposition approaches.\n4Available from http://www.cs.cmu.edu/˜enron/ 5Available from http://www.public.asu.edu/˜ylin56/kdd09sup.html"
    }, {
      "heading" : "A Proof of Theorem 2",
      "text" : "Proof Sketch If W(r) ∼ T T (ν,0, {Ir}Kk=1), then vec(W (r)) ∼ T (ν,0, IrK). Let U(k), k = 1, . . . ,K be K location sets (matrices) as used in (3), we have\nvec ( W(r) × φ(r)(U) ) = φ(r)(U(1))⊗ . . .⊗ φ(r)(U(K)) vec(W(r)) (20)\nThus, vec ( W(r) × φ(r)(U) ) ∼ T (ν,0,Σ (1) r ⊗ . . . ⊗ Σ (K) r ), where Σ (k) r (i, j) = Σ (k) r (u (k) i ,u (k) j ) are the covariance matrix. Inverting (20) gives W (r) × φ(r)(U) ∼ T T (0, {Σ (k) r }Kk=1), which proves t\n(r) follows the tensor Gaussian process. From the definition of inner product in ℓ2, we have the following identity on the\nconvergence of covariance function.\nΣ(k)(u (k) i ,u (k) j ) = limr→∞ Σ(k)r (u (k) i ,u (k) j ), ∀u (k) i ,u (k) j ∈ Uk\nConvergence in distribution follows from this convergence result.\nB Gradient of f(U)\n∂f\n∂u (k) ij\n= n\nnk tr\n(\n(Σ(k))−1 ∂Σ(k)\n∂u (k) ij\n)\n+ τµ⊤∆(k)µ+ τ tr ( ∆(k)Υ )\n(21)\n∆(k) = (Σ(1))−1 ⊗ . . .⊗ (Σ(k−1))−1 ⊗ (Σ(k))−1 ∂Σ(k)\n∂u (k) ij\n(Σ(k))−1\n⊗ (Σ(k+1))−1 ⊗ . . .⊗ (Σ(K))−1"
    }, {
      "heading" : "C Original figures",
      "text" : ""
    } ],
    "references" : [ {
      "title" : "Scalable tensor factorizations for incomplete data",
      "author" : [ "E. Acar", "D.M. Dunlavy", "T.G. Kolda", "M. Mørup" ],
      "venue" : "Chemometrics and Intelligent Laboratory Systems,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2011
    }, {
      "title" : "Bayesian analysis of binary and polychotomous response data",
      "author" : [ "James H Albert", "Siddhartha Chib" ],
      "venue" : "Journal of the American Statistical Association,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 1993
    }, {
      "title" : "Pattern Recognition and Machine Learning",
      "author" : [ "Christopher M. Bishop" ],
      "venue" : null,
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2007
    }, {
      "title" : "Multi-task Gaussian process prediction",
      "author" : [ "Edwin Bonilla", "Kian Ming Chai", "Chris Williams" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2008
    }, {
      "title" : "Probabilistic models for incomplete multi-dimensional arrays",
      "author" : [ "Wei Chu", "Zoubin Ghahramani" ],
      "venue" : "Journal of Machine Learning Research - Proceedings Track,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2009
    }, {
      "title" : "Foundations of the PARAFAC procedure: Model and conditions for an”explanatory”multi-mode factor analysis",
      "author" : [ "R.A. Harshman" ],
      "venue" : "UCLA Working Papers in Phonetics,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 1970
    }, {
      "title" : "Hierarchical multilinear models for multiway data",
      "author" : [ "Peter D. Hoff" ],
      "venue" : "Computational Statistics & Data Analysis,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2011
    }, {
      "title" : "Nonnegative Tucker decomposition",
      "author" : [ "Yong-Deok Kim", "Seungjin Choi" ],
      "venue" : "In Computer Vision and Pattern Recognition,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2007
    }, {
      "title" : "Tensor decompositions and applications",
      "author" : [ "Tamara G. Kolda", "Brett W. Bader" ],
      "venue" : "SIAM Review,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2009
    }, {
      "title" : "A multilinear singular value decomposition",
      "author" : [ "Lieven De Lathauwer", "Bart De Moor", "Joos Vandewalle" ],
      "venue" : "SIAM J. Matrix Anal. Appl,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2000
    }, {
      "title" : "The Gaussian process latent variable model",
      "author" : [ "Neil Lawrence" ],
      "venue" : "Technical Report CS-06-03,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2006
    }, {
      "title" : "Semi-supervised learning via gaussian processes",
      "author" : [ "Neil D. Lawrence", "Michael I. Jordan" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2005
    }, {
      "title" : "Temporal collaborative filtering with bayesian probabilistic tensor factorization",
      "author" : [ "Xi Chen" ],
      "venue" : "In Proceedings of SDM,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2010
    }, {
      "title" : "Multi-HDP: A non-parametric Bayesian model for tensor factorization",
      "author" : [ "Ian Porteous", "Evgeniy Bart", "Max Welling" ],
      "venue" : "In Proc. AAAI Conference on Artificial Intelligence,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2008
    }, {
      "title" : "Graphical Model Structure Learning with L1-Regularization",
      "author" : [ "Mark Schmidt" ],
      "venue" : "PhD thesis, University of British Columbia,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2010
    }, {
      "title" : "Non-negative tensor factorization with applications to statistics and computer vision",
      "author" : [ "Amnon Shashua", "Tamir Hazan" ],
      "venue" : "In Proceedings of the 22nd ICML,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2005
    }, {
      "title" : "Beyond streams and graphs: Dynamic tensor analysis",
      "author" : [ "Jimeng Sun", "Dacheng Tao", "Christos Faloutsos" ],
      "venue" : "In KDD,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2006
    }, {
      "title" : "Multivis: Content-based social network exploration through multi-way visual analysis",
      "author" : [ "Jimeng Sun", "Spiros Papadimitriou", "Ching-Yung Lin", "Nan Cao", "Shixia Liu", "Weihong Qian" ],
      "venue" : "In SDM’09,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2009
    }, {
      "title" : "Probabilistic principal component analysis",
      "author" : [ "Michael E. Tipping", "Christopher M. Bishop" ],
      "venue" : "Journal of The Royal Statistical Society Series B-statistical Methodology,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 1999
    }, {
      "title" : "Some mathematical notes on three-mode factor analysis",
      "author" : [ "Ledyard Tucker" ],
      "venue" : "Psychometrika, 31:279–311,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 1966
    }, {
      "title" : "Gaussian process models for link analysis and transfer learning",
      "author" : [ "Kai Yu", "Wei Chu" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2008
    }, {
      "title" : "Stochastic relational models for discriminative link prediction",
      "author" : [ "Kai Yu", "Wei Chu", "Shipeng Yu", "Volker Tresp", "Zhao Xu" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2007
    }, {
      "title" : "Spatio-temporal compressive sensing and internet traffic matrices",
      "author" : [ "Yin Zhang", "Matthew Roughan", "Walter Willinger", "Lili Qiu" ],
      "venue" : "In Proceedings of the ACM SIGCOMM 2009 conference on Data communication",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2009
    } ],
    "referenceMentions" : [ {
      "referenceID" : 19,
      "context" : "Traditional multiway factor models— such as the Tucker decomposition [21] and CANDECOMP/PARAFAC (CP) [6]—have been widely applied in many areas (e.",
      "startOffset" : 69,
      "endOffset" : 73
    }, {
      "referenceID" : 5,
      "context" : "Traditional multiway factor models— such as the Tucker decomposition [21] and CANDECOMP/PARAFAC (CP) [6]—have been widely applied in many areas (e.",
      "startOffset" : 101,
      "endOffset" : 104
    }, {
      "referenceID" : 22,
      "context" : ", network traffic analysis [24], computer vision [17] and social network analysis [18, 14, 19], etc).",
      "startOffset" : 27,
      "endOffset" : 31
    }, {
      "referenceID" : 15,
      "context" : ", network traffic analysis [24], computer vision [17] and social network analysis [18, 14, 19], etc).",
      "startOffset" : 49,
      "endOffset" : 53
    }, {
      "referenceID" : 16,
      "context" : ", network traffic analysis [24], computer vision [17] and social network analysis [18, 14, 19], etc).",
      "startOffset" : 82,
      "endOffset" : 94
    }, {
      "referenceID" : 17,
      "context" : ", network traffic analysis [24], computer vision [17] and social network analysis [18, 14, 19], etc).",
      "startOffset" : 82,
      "endOffset" : 94
    }, {
      "referenceID" : 6,
      "context" : "Compared with the tensor-variate Gaussian process approaches [7, 23], the t-processbased InfTucker is more robust (not sensitive to data outliers).",
      "startOffset" : 61,
      "endOffset" : 68
    }, {
      "referenceID" : 21,
      "context" : "Compared with the tensor-variate Gaussian process approaches [7, 23], the t-processbased InfTucker is more robust (not sensitive to data outliers).",
      "startOffset" : 61,
      "endOffset" : 68
    }, {
      "referenceID" : 4,
      "context" : "Recently several nonparametric Bayesian models on matrices and tensors [5, 23, 4] have been developed, but they are either designed for different tasks (e.",
      "startOffset" : 71,
      "endOffset" : 81
    }, {
      "referenceID" : 21,
      "context" : "Recently several nonparametric Bayesian models on matrices and tensors [5, 23, 4] have been developed, but they are either designed for different tasks (e.",
      "startOffset" : 71,
      "endOffset" : 81
    }, {
      "referenceID" : 3,
      "context" : "Recently several nonparametric Bayesian models on matrices and tensors [5, 23, 4] have been developed, but they are either designed for different tasks (e.",
      "startOffset" : 71,
      "endOffset" : 81
    }, {
      "referenceID" : 9,
      "context" : "several state-of-art tensor decomposition approaches including High Order Singular Value Decomposition (HOSVD) [10], Weighted CP [1] and nonnegative tensor decomposition [17].",
      "startOffset" : 111,
      "endOffset" : 115
    }, {
      "referenceID" : 0,
      "context" : "several state-of-art tensor decomposition approaches including High Order Singular Value Decomposition (HOSVD) [10], Weighted CP [1] and nonnegative tensor decomposition [17].",
      "startOffset" : 129,
      "endOffset" : 132
    }, {
      "referenceID" : 15,
      "context" : "several state-of-art tensor decomposition approaches including High Order Singular Value Decomposition (HOSVD) [10], Weighted CP [1] and nonnegative tensor decomposition [17].",
      "startOffset" : 170,
      "endOffset" : 174
    }, {
      "referenceID" : 8,
      "context" : "As in Kolda and Bader [9], We collectively denote the group of K matrices as a Tucker tensor with a identity core U = [ U, .",
      "startOffset" : 22,
      "endOffset" : 25
    }, {
      "referenceID" : 8,
      "context" : "The alternating least square (ALS) method has been used to solve both Tucker decomposition and CP [9].",
      "startOffset" : 98,
      "endOffset" : 101
    }, {
      "referenceID" : 1,
      "context" : "Other noise models include modified probit models for ordinal regression and multiclass classification [2], null category noise models for semi-supervised classification [12].",
      "startOffset" : 103,
      "endOffset" : 106
    }, {
      "referenceID" : 11,
      "context" : "Other noise models include modified probit models for ordinal regression and multiclass classification [2], null category noise models for semi-supervised classification [12].",
      "startOffset" : 170,
      "endOffset" : 174
    }, {
      "referenceID" : 1,
      "context" : "1 Variational EM for binary classification We follow the data augmentation scheme by Albert and Chib [2] to decompose the probit model into p(yi|mi) = ∫",
      "startOffset" : 101,
      "endOffset" : 104
    }, {
      "referenceID" : 2,
      "context" : ", q(Z), in (10) at a time, while having all the other approximate distributions fixed [3].",
      "startOffset" : 86,
      "endOffset" : 89
    }, {
      "referenceID" : 14,
      "context" : "With an l1 penalty on f(U), we choose a projected scaled subgradient L-BFGS algorithm for optimization—due to its excellent performance [16].",
      "startOffset" : 136,
      "endOffset" : 140
    }, {
      "referenceID" : 18,
      "context" : "The InfTucker model extends Probabilistic PCA (PPCA) [20] and Gaussian process latent variable models (GPLVMs) [11]: while PPCA and GPLVM model interactions of one mode of a matrix and ignore the joint interactions of two modes, InfTucker does.",
      "startOffset" : 53,
      "endOffset" : 57
    }, {
      "referenceID" : 10,
      "context" : "The InfTucker model extends Probabilistic PCA (PPCA) [20] and Gaussian process latent variable models (GPLVMs) [11]: while PPCA and GPLVM model interactions of one mode of a matrix and ignore the joint interactions of two modes, InfTucker does.",
      "startOffset" : 111,
      "endOffset" : 115
    }, {
      "referenceID" : 21,
      "context" : "[23], Yu and Chu [22].",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 20,
      "context" : "[23], Yu and Chu [22].",
      "startOffset" : 17,
      "endOffset" : 21
    }, {
      "referenceID" : 4,
      "context" : "Our model is closely related to the probabilistic Tucker-3 (pTucker) model [5]; actually InfTucker reduces to pTucker as a special case as ν → ∞.",
      "startOffset" : 75,
      "endOffset" : 78
    }, {
      "referenceID" : 6,
      "context" : "Also, Hoff [7] proposed a hierarchical Bayesian extension to CANDECOMP/PARAFAC that captures the interaction of component matrices.",
      "startOffset" : 11,
      "endOffset" : 14
    }, {
      "referenceID" : 4,
      "context" : "Unlike both [5]’s and Hoff [7]’s approach, ours handles non-Gaussian noise and uses nonlinear covariance functions to model complex interactions.",
      "startOffset" : 12,
      "endOffset" : 15
    }, {
      "referenceID" : 6,
      "context" : "Unlike both [5]’s and Hoff [7]’s approach, ours handles non-Gaussian noise and uses nonlinear covariance functions to model complex interactions.",
      "startOffset" : 27,
      "endOffset" : 30
    }, {
      "referenceID" : 6,
      "context" : "In addition, [7] used a Gibbs sampler for inference—its convergence incurs high computational cost and makes this approach infeasible for tensors with moderate and large sizes.",
      "startOffset" : 13,
      "endOffset" : 16
    }, {
      "referenceID" : 15,
      "context" : "To handle missing data, enhance model interpretability, and avoid overfitting, several extensions to non-probabilistic tensor decomposition have been proposed, including nonnegative tensor decomposition (NTD) [17, 8, 13, 15] and Weighted tensor decomposition (WTD) [1].",
      "startOffset" : 209,
      "endOffset" : 224
    }, {
      "referenceID" : 7,
      "context" : "To handle missing data, enhance model interpretability, and avoid overfitting, several extensions to non-probabilistic tensor decomposition have been proposed, including nonnegative tensor decomposition (NTD) [17, 8, 13, 15] and Weighted tensor decomposition (WTD) [1].",
      "startOffset" : 209,
      "endOffset" : 224
    }, {
      "referenceID" : 12,
      "context" : "To handle missing data, enhance model interpretability, and avoid overfitting, several extensions to non-probabilistic tensor decomposition have been proposed, including nonnegative tensor decomposition (NTD) [17, 8, 13, 15] and Weighted tensor decomposition (WTD) [1].",
      "startOffset" : 209,
      "endOffset" : 224
    }, {
      "referenceID" : 13,
      "context" : "To handle missing data, enhance model interpretability, and avoid overfitting, several extensions to non-probabilistic tensor decomposition have been proposed, including nonnegative tensor decomposition (NTD) [17, 8, 13, 15] and Weighted tensor decomposition (WTD) [1].",
      "startOffset" : 209,
      "endOffset" : 224
    }, {
      "referenceID" : 0,
      "context" : "To handle missing data, enhance model interpretability, and avoid overfitting, several extensions to non-probabilistic tensor decomposition have been proposed, including nonnegative tensor decomposition (NTD) [17, 8, 13, 15] and Weighted tensor decomposition (WTD) [1].",
      "startOffset" : 265,
      "endOffset" : 268
    }, {
      "referenceID" : 3,
      "context" : "Finally, note that the inference technique described in Section 4 can be adopted for Gaussian process or t-process multi-task learning [4, 25].",
      "startOffset" : 135,
      "endOffset" : 142
    } ],
    "year" : 2017,
    "abstractText" : "Tensor decomposition is a powerful tool for multiway data analysis. Many popular tensor decomposition approaches—such as the Tucker decomposition and CANDECOMP/PARAFAC (CP)—conduct multi-linear factorization. They are insufficient to model (i) complex interactions between data entities, (ii) various data types (e.g. missing data and binary data), and (iii) noisy observations and outliers. To address these issues, we propose a tensor-variate latent t process model, InfTucker , for robust multiway data analysis: it conducts robust Tucker decomposition in an infinite feature space. Unlike classical tensor decomposition models, it handles both continuous and binary data in a probabilistic framework. Unlike previous nonparametric Bayesian models on matrices and tensors, our latent t-process model focuses on multiway analysis and uses nonlinear covariance functions. To efficiently learn InfTucker from data, we develop a novel variational inference technique on tensors. Compared with classical implementation, the new technique reduces both time and space complexities by several orders of magnitude. This technique can be easily adopted in other contexts (e.g. , multitask learning) where we encounter tensor-variate t processes or Gaussian processes. Our experimental results on chemometrics and social network datasets demonstrate that the new InfTucker model achieves significantly higher prediction accuracy than several state-of-art tensor decomposition approaches including High Order Singular Value Decomposition (HOSVD), Weighted CP, and nonnegative tensor decomposition.",
    "creator" : "LaTeX with hyperref package"
  }
}