{
  "name" : "1402.5596.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Exact Post Model Selection Inference for Marginal Screening",
    "authors" : [ "Jason D. Lee", "Jonathan E. Taylor" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Consider the model\nyi = µ(xi) + i, i ∼ N (0, σ2I), (1)\nwhere µ(x) is an arbitrary function, and xi ∈ Rp. Our goal is to perform inference on (XTX)−1XTµ, which is the best linear predictor of µ. In the classical setting of n > p , the least squares estimator\nβ̂ = (XTX)−1XT y (2)\nar X\niv :1\n40 2.\n55 96\nv2 [\nst at\n.M E\n] 2\n8 Fe\nis a commonly used estimator for (XTX)−1XTµ. Under the linear model assumption µ = Xβ0, the exact distribution of β̂ is\nβ̂ ∼ N (β0, σ2(XTX)−1). (3)\nUsing the normal distribution, we can test the hypothesis H0 : β 0 j = 0 and form confidence intervals for β0j using the z-test. However in the high-dimensional p > n setting, the least squares estimator is an underdetermined problem, and the predominant approach is to perform variable selection or model selection [3]. There are many approaches to variable selection including AIC/BIC, greedy algorithms such as forward stepwise regression, orthogonal matching pursuit, and regularization methods such as the Lasso. The focus of this paper will be on the model selection procedure known as marginal screening, which selects the k most correlated features xj with the response y.\nMarginal screening is the simplest and most commonly used of the variable selection procedures [13, 28, 20]. Marginal screening requires only O(np) computation and is several orders of magnitude faster than regularization methods such as the Lasso; it is extremely suitable for extremely large datasets where the Lasso may be computationally intractable to apply. Furthermore, the selection properties are comparable to the Lasso [12]. In the ultrahigh dimensional setting p = O(en k ), marginal screening is shown to have the SURE screening property, P (S ⊂ Ŝ), that is marginal screening selects a superset of the truly relevant variables [9, 11, 10]. Marginal screening can also be combined with a second variable selection procedure such as the Lasso to further reduce the dimensionality; our statistical inference methods extend to the Marginal Screening+Lasso method.\nSince marginal screening utilizes the response variable y, the confidence intervals and statistical tests based on the distribution in (3) are not valid; confidence intervals with nominal 1−α coverage may no longer cover at the advertised level:\nPr ( β0j ∈ C1−α(x) ) < 1− α.\nSeveral authors have previously noted this problem including recent work in [17, 18, 19, 2]. A major line of work [17, 18, 19] has described the difficulty of inference post model selection: the distribution of post model selection estimates is complicated and cannot be approximated in a uniform sense by their asymptotic counterparts.\nIn this paper, we describe how to form exact confidence intervals for linear regression coefficients post model selection. We assume the model\n(1), and operate under the fixed design matrix X setting. The linear regression coefficients constrained to a subset of variables S is linear in µ, eTj (X T SXS) −1XTS µ = η Tµ for some η. We derive the conditional distribution of ηT y for any vector η, so we are able to form confidence intervals and test regression coefficients.\nIn Section 2 we discuss related work on high-dimensional statistical inference, and Section 3 introduces the marginal screening algorithm and shows how z intervals may fail to have the correct coverage properties. Section 4 and 5 show how to represent the marginal screening selection event as constraints on y, and construct pivotal quantities for the truncated Gaussian. Section 6 uses these tools to develop valid hypothesis tests and confidence intervals.\nAlthough the focus of this paper is on marginal screening, the “condition on selection” framework, first proposed for the Lasso in [16], is much more general; we use marginal screening as a simple and clean illustration of the applicability of this framework. In Section 7, we discuss several extensions including how to apply the framework to other variable/model selection procedures and to nonlinear regression problems. Section 7 covers\n1. marginal screening+Lasso, a screen and clean procedure that first uses marginal screening and cleans with the Lasso,\n2. orthogonal matching pursuit (OMP)\n3. non-negative least squares (NNLS)."
    }, {
      "heading" : "2 Related Work",
      "text" : "Most of the theoretical work on high-dimensional linear models focuses on consistency. Such results establish, under restrictive assumptions on X, the Lasso β̂ is close to the unknown β0 [24] and selects the correct model [33, 30, 15]. We refer to the reader to [3] for a comprehensive discussion about the theoretical properties of the Lasso.\nThere is also recent work on obtaining confidence intervals and significance testing for penalized M-estimators such as the Lasso. One class of methods uses sample splitting or subsampling to obtain confidence intervals and p-values [31, 23]. In the post model selection literature, the recent work of [2] proposed the POSI approach, a correction to the usual t-test confidence intervals by controlling the familywise error rate for all parameters in any possible submodel. The POSI approach will produce valid confidence intervals for any possible model selection procedure; however for a given model\nselection procedure such as marginal regression, it will be conservative. In addition, the POSI methodology is extremely computationally intensive and currently only applicable for p ≤ 30.\nA separate line of work establishes the asymptotic normality of a corrected estimator obtained by “inverting” the KKT conditions [29, 32, 14]. The corrected estimator b̂ has the form b̂ = β̂ + λΘ̂ẑ, where ẑ is a subgradient of the penalty at β̂ and Θ̂ is an approximate inverse to the Gram matrix XTX. The two main drawbacks to this approach are 1) the confidence intervals are valid only when the M-estimator is consistent, and thus require restricted eigenvalue conditions on X, 2) obtaining Θ̂ is usually much more expensive than obtaining β̂, and 3) the method is specific to regularized estimators, and does not extend to marginal screening, forward stepwise, and other variable selection methods.\nMost closely related to our work is the “condition on selection” framework laid out in [16] for the Lasso. Our work extends this methodology to other variable selection methods such as marginal screening, marginal screening followed by the Lasso (marginal screening+Lasso), orthogonal matching pursuit, and non-negative least squares. The primary contribution of this work is the observation that many model selection methods, including marginal screening and Lasso, lead to “selection events” that can be represented as a set of constraints on the response variable y. By conditioning on the selection event, we can characterize the exact distribution of ηT y. This paper focuses on marginal screening, since it is the simplest of variable selection methods, and thus the applicability of the “conditioning on selection event” framework is most transparent. However, this framework is not limited to marginal screening and can be applied to a wide a class of model selection procedures including greedy algorithms such as matching pursuit and orthogonal matching pursuit. We discuss some of these possible extensions in Section 7, but leave a thorough investigation to future work.\nA remarkable aspect of our work is that we only assume X is in general position, and the test is exact, meaning the distributional results are true even under finite samples. By extension, we do not make any assumptions on n and p, which is unusual in high-dimensional statistics [3]. Furthermore, the computational requirements of our test are negligible compared to computing the linear regression coefficients.\nOur test assumes that the noise variance σ2 is known. However, there are many methods for estimating σ2 in high dimensions. A data splitting technique is used in [8], while [27] proposes a method that computes the regression estimate and an estimate of the variance simultaneously. We refer the reader to [25] for a survey and comparison of the various methods,\nand assume σ2 is known for the remainder of the paper."
    }, {
      "heading" : "3 Marginal Screening",
      "text" : "Let X ∈ Rn×p be the design matrix, y ∈ Rn the response variable, and assume the model\nyi = µ(xi) + i, i ∼ N (0, σ2I).\nWe will assume that X is in general position and has unit norm columns. The algorithm estimates β̂ via Algorithm 1. The marginal screening algo-\nAlgorithm 1 Marginal screening algorithm\n1: Input: Design matrix X, response y, and model size k. 2: Compute |XT y|. 3: Let Ŝ be the index of the k largest entries of |XT y|. 4: Compute β̂Ŝ = (X T Ŝ XŜ) −1XT Ŝ y\nrithm chooses the k variables with highest absolute dot product with y, and then fits a linear model over those k variables. We will assume k ≤ min(n, p). For any fixed subset of variables S, the distribution of β̂S = (X T SXS)\n−1XTS y is\nβ̂S ∼ N (β?S , σ2(XTSXS)−1) (4) β?S := (X T SXS) −1XTS µ. (5)\nWe will use the notation β?j∈S := (β ? S)j , where j is indexing a variable in the set S. The z-test intervals for a regression coefficient are\nC(α, j, S) :=( β̂j∈S − σz1−α/2(XTSXS)jj , β̂j∈S + σz1−α/2(XTSXS)jj ) (6)\nand each interval has 1−α coverage, meaning Pr ( β?j∈S ∈ C(α, j, S) ) = 1−α.\nHowever if Ŝ is chosen using a model selection procedure that depends on y, the distributional result (5) no longer holds and the z-test intervals will not cover at the 1− α level. It is possible that\nPr ( β? j∈Ŝ ∈ C(α, j, Ŝ) ) < 1− α.\nSimilarly, the test of the hypothesis H0 : β ? j∈Ŝ = 0 will not control type I error at level α, meaning Pr (reject H0|H0) > α."
    }, {
      "heading" : "3.1 Failure of z-test confidence intervals",
      "text" : "We will illustrate empirically that the z-test intervals do not cover at 1− α when Ŝ is chosen by marginal screening in Algorithm 1. For this experiment\nwe generated X from a standard normal with n = 20 and p = 200. The signal vector is 2 sparse with β01 , β 0 2 = SNR, y = Xβ\n0 + , and ∼ N(0, 1). The confidence intervals were constructed for the k = 2 variables selected by the marginal screening algorithm. The z-test intervals were constructed via (6) with α = .1, and the adjusted intervals were constructed using Algorithm 3. The results are described in Figure 1. The y-axis plots the coverage proportion or the fraction of times the true parameter value fell in the confidence interval. Each point represents 500 independent trials. The x-axis varies the SNR parameter over the values 0.1, .2, .5, 1, 2, 5, 10. From the figure, we see that the z intervals can have coverage proportion drastically less than the nominal level of 1 − α = .9, and only for SNR=10 does the coverage tend to .9. This motivates the need for intervals that have the correct coverage proportion after model selection."
    }, {
      "heading" : "4 Representing the selection event",
      "text" : "Since Equation (5) does not hold for a selected Ŝ when the selection procedure depends on y, the z-test intervals are not valid. Our strategy will be to understand the conditional distribution of y and contrasts (linear functions of y) ηT y, then construct inference conditional on the selection event Ê. We will use Ê(y) to represent a random variable, and E to represent an element of the range of Ê(y). In the case of marginal screening, the selection event Ê(y) corresponds to the set of selected variables Ŝ and signs s:\nÊ(y) = { y : sign(xTi y)x T i y > ±xTj y for all i ∈ Ŝ and j ∈ Ŝc } = { y : ŝix T i y > ±xTj y and ŝixTi y ≥ 0 for all i ∈ Ŝ and j ∈ Ŝc\n} = { y : A(Ŝ, ŝ)y ≤ b(Ŝ, ŝ) } (7)\nfor some matrix A(Ŝ, ŝ) and vector b(Ŝ, ŝ)1. We will use the selection event Ê and the selected variables/signs pair (Ŝ, ŝ) interchangeably since they are in bijection.\nThe space Rn is partitioned by the selection events, Rn = ⊔\n(S,s)\n{y : A(S, s)y ≤ b(S, s)}.\nThe vector y can be decomposed with respect to the partition as follows y = ∑ S,s y 1 (A(S, s)y ≤ b(S, s)) (8)\nThe previous equation establishes that y is a different constrained Gaussian for each element of the partition, where the partition is specified by a possible subset of variables and signs (S, s). The above discussion can be summarized in the following theorem.\nTheorem 4.1. The distribution of y conditional on the selection event is a constrained Gaussian,\ny|{Ê(y) = E} d= z ∣∣{A(S, s)z ≤ b}, z ∼ N (µ, σ2I).\nProof. The event E is in bijection with a pair (S, s), and y is unconditionally Gaussian. Thus the conditional y ∣∣{A(S, s)y ≤ b(S, s)} is a Gaussian constrained to the set {A(S, s)y ≤ b(S, s)}.\n1b can be taken to be 0 for marginal screening, but this extra generality is needed for other model selection methods"
    }, {
      "heading" : "5 Truncated Gaussian test",
      "text" : "This section summarizes the recent tools developed in [16] for testing contrasts2 ηT y of a constrained Gaussian y. The results are stated without proof and the proofs can be found in [16].\nThe distribution of a constrained Gaussian y ∼ N(µ,Σ) conditional on affine constraints {Ay ≤ b} has density 1Pr(Ay≤b)f(y;µ,Σ)1 (Ay ≤ b), involves the intractable normalizing constant Pr(Ay ≤ b). In this section, we derive a one-dimensional pivotal quantity for ηTµ. This pivot relies on characterizing the distribution of ηT y as a truncated normal. The key step to deriving this pivot is the following lemma:\nLemma 5.1. The conditioning set can be rewritten in terms of ηT y as follows: {Ay ≤ b} = {V−(y) ≤ ηT y ≤ V+(y),V0(y) ≥ 0} where\nα = AΣη\nηTΣη (9)\nV− = V−(y) = max j: αj<0 bj − (Ay)j + αjηT y αj\n(10)\nV+ = V+(y) = min j: αj>0 bj − (Ay)j + αjηT y αj . (11)\nV0 = V0(y) = min j: αj=0 bj − (Ay)j (12)\nMoreover, (V+,V−,V0) are independent of ηT y.\nThe geometric picture gives more intuition as to why V+ and V− are independent of ηT y. Without loss of generality, we assume ||η||2 = 1 and y ∼ N(µ, I) (otherwise we could replace y by Σ− 1 2 y). Now we can decompose y into two independent components, a 1-dimensional component ηT y and an (n− 1)-dimensional component orthogonal to η:\ny = ηT y + Pη⊥y.\nThe case of n = 2 is illustrated in Figure 2. Since the two components are independent, the distribution of ηT y is the same as ηT y|{Pη⊥y}. If we condition on Pη⊥y, it is clear from Figure 2 that in order for y to lie in the set, it is necessary for V− ≤ ηT y ≤ V+, where V− and V+ are functions of Pη⊥y.\n2A contrast of y is a linear function of the form ηT y.\nCorollary 5.2. The distribution of ηT y conditioned on {Ay ≤ b,V+(y) = v+,V−(y) = v−} is a (univariate) Gaussian truncated to fall between V− and V+, i.e.\nηT y | {Ay ≤ b,V+(y) = v+,V−(y) = v−} d= W\nwhere W ∼ TN(ηTµ, ηTΣη, v−, v+). TN(µ, σ, a, b) is the normal distribution truncated to lie between a and b.\nIn Figure 3, we plot the density of the truncated Gaussian, noting that its shape depends on the location of µ relative to [a, b] as well as the width relative to σ.\nThe following pivotal quantity3 follows from Corollary 5.2 via the probability integral transform.\nTheorem 5.3. Let Φ(x) denote the CDF of a N(0, 1) random variable, and let F [a,b] µ,σ2 denote the CDF of TN(µ, σ, a, b), i.e.:\nF [a,b] µ,σ2 (x) = Φ((x− µ)/σ)− Φ((a− µ)/σ) Φ((b− µ)/σ)− Φ((a− µ)/σ) . (13)\n3The distribution of a pivotal quantity does not depend on unobserved parameters.\nThen F [V−,V+] ηTµ, ηTΣη (ηT y) is a pivotal quantity, conditional on {Ay ≤ b}:\nF [V−,V+] ηTµ, ηTΣη\n(ηT y) ∣∣ {Ay ≤ b} ∼ Unif(0, 1) (14)\nwhere V− and V+ are defined in (10) and (11)."
    }, {
      "heading" : "6 Inference for marginal screening",
      "text" : "In this section, we apply the theory summarized in Sections 4 and 5 to marginal screening. In particular, we will construct confidence intervals for the selected variables.\nTo summarize the developments so far, recall that our model (1) says that y ∼ N(µ, σ2I). The distribution of interest is y|{Ê(y) = E}, and by Theorem 4.1, this is equivalent to y|{A(S, s)z ≤ b(S, s)}, where y ∼ N(µ, σ2I). By applying Theorem 5.3, we obtain the pivotal quantity\nF [V−,V+] ηTµ, σ2||η||22\n(ηT y) ∣∣ {Ê(y) = E} ∼ Unif(0, 1) (15)\nfor any η, where V− and V+ are defined in (10) and (11)."
    }, {
      "heading" : "6.1 Hypothesis tests for selected variables",
      "text" : "In this section, we describe how to form confidence intervals for the components of β?\nŜ = (XT Ŝ XŜ) −1XT Ŝ µ. The best linear predictor of µ that uses\nonly the selected variables is β? Ŝ , and β̂Ŝ = (X T Ŝ XŜ) −1XT Ŝ y is an unbiased estimate of β? Ŝ\n. In this section, we propose hypothesis tests and confidence intervals for β?\nŜ . If we choose\nηj = ((X T Ŝ XŜ) −1XT Ŝ ej) T , (16)\nthen ηTj µ = β ? j∈Ŝ , so the above framework provides a method for inference about the jth variable in the model Ŝ. This choice of η is not fixed before marginal screening selects Ŝ, but it is measurable with respect to the σalgebra generated by the partition. Since it is measurable, η is constant on each partition, so the pivot is uniformly distributed on each element of the partition, and thus uniformly distributed for all y.\nIf we assume the linear model µ = Xβ0 for some β0 ∈ Rp, S0 := support(β0) ⊂ Ŝ, and XŜ is full rank, then by the following computation β? Ŝ = β0 Ŝ :\nβ? Ŝ = (XT Ŝ XŜ) −1XT Ŝ XSβ 0 S\n= (XT Ŝ XŜ) −1XT Ŝ XŜβ 0 Ŝ = β0 Ŝ\nIn [9], the screening property S0 ⊂ Ŝ for the marginal screening algorithm is\nestablished under mild conditions. Thus under the screening property, our method provides hypothesis tests and confidence intervals for β0\nŜ .\nBy applying Theorem 5.3, we obtain the following (conditional) pivot for β?\nj∈Ŝ :\nF [V−,V+] β? j∈Ŝ , σ2||ηj ||2(η T j y) ∣∣∣{Ê(y) = E} ∼ Unif(0, 1). (17) The quantities j and ηj are both random through Ê, a quantity which is fixed after conditioning, therefore Theorem 5.3 holds even for this choice of η.\nConsider testing the hypothesis H0 : β ? j∈Ŝ = βj . A valid test statis-\ntic is given by F [V−,V+] βj , σ2||ηj ||2(η T j y), which is uniformly distributed under the null hypothesis and y|{Ê(y) = E}. Thus, this test would reject when F\n[V−,V+] βj , σ2||ηj ||2(η T j y) > 1− α2 or F [V−,V+] βj , σ2||ηj ||2(η T j y) < α 2 .\nTheorem 6.1. The test of H0 : β ? j∈Ŝ = βj that accepts when\nα 2 < F [V−,V+] βj , σ2||ηj ||2(η T j y) < 1− α 2\nis an α level test of H0.\nProof. Under H0, we have β ? j∈Ŝ = βj , so by (17) F [V−,V+] βj , σ2||ηj ||2(η T j y) ∣∣{Ê(y) = E} is uniformly distributed. Thus\nPr( α\n2 < F\n[V−,V+] βj , σ2||ηj ||2(η T j y) ≤ 1− α\n2 ∣∣{Ê(y) = E,H0)} = 1− α, and the type 1 error is exactly α. Under H0, but not conditional on selection event Ê, we have\nPr( α\n2 < F\n[V−,V+] βj , σ2||ηj ||2(η T j y) ≤ 1− α\n2 ∣∣H0)} = ∑ E Pr( α 2 < F [V−,V+] βj , σ2||ηj ||2(η T j y) ≤ 1− α 2\n∣∣{Ê(y) = E,H0)}Pr(Ê(y) = E|H0) = ∑ E (1− α)Pr(Ê(y) = E|H0)\n= (1− α) ∑ E Pr(Ê(y) = E|H0) = 1− α.\nFor each element of the partition E, the conditional (on selection) hypothesis test is level 1 − α, so by summing over the partition the unconditional test is level 1− α.\nOur hypothesis test is not conservative, in the sense that the type 1 error is exactly α; also, it is non-asymptotic, since the statement holds for fixed n and p. We summarize the hypothesis test in this section in the following algorithm.\nAlgorithm 2 Hypothesis test for selected variables\n1: Input: Design matrix X, response y, model size k. 2: Use Algorithm 1 to select a subset of variables Ŝ and signs ŝ =\nsign(XT Ŝ y).\n3: Specify the null hypothesis H0 : β ? j∈Ŝ = βj . 4: Let A = A(Ŝ, ŝ) and b = b(Ŝ, ŝ) using (7). Let ηj = (X T Ŝ )†ej . 5: Compute F [V−,V+] βj , σ2||ηj ||2(η T j y), where V− and V+ are computed via (10)\nand (11) using the A, b, and η previously defined.\n6: Output: Reject if F [V−,V+] βj , σ2||ηj ||2(η T j y) > α 2 or F [V−,V+] βj , σ2||ηj ||2(η T j y) < 1− α2 ."
    }, {
      "heading" : "6.2 Confidence intervals for selected variables",
      "text" : "Next, we discuss how to obtain confidence intervals for β? j∈Ŝ . The standard way to obtain an interval is to invert a pivotal quantity [4]. In other words, since\nPr\n( α\n2 ≤ F [V −,V+] β? j∈Ŝ , σ2||ηj ||2(η T j y) ≤ 1− α 2 ∣∣ {Ê = E}) = α, one can define a (1− α) (conditional) confidence interval for β?\nj,Ê as{\nx : α 2 ≤ F [V −,V+] x, σ2||ηj ||2(η T j y) ≤ 1− α 2\n} . (18)\nIn fact, F is monotone decreasing in x, so to find its endpoints, one need only solve for the root of a smooth one-dimensional function. The monotonicity is a consequence of the fact that the truncated Gaussian distribution is a natural exponential family and hence has monotone likelihood ratio in µ [21].\nWe now formalize the above observations in the following result, an immediate consequence of Theorem 5.3.\nCorollary 6.2. Let ηj be defined as in (16), and let Lα = Lα(ηj , (Ŝ, ŝ)) and Uα = Uα(ηj , (Ŝ, ŝ)) be the (unique) values satisfying\nF [V−,V+] Lα, σ2||ηj ||2(η T j y) = 1− α 2 F [V−,V+] Uα, σ2||ηj ||2(η T j y) = α 2 (19)\nThen [Lα, Uα] is a (1− α) confidence interval for β?j∈Ŝ, conditional on Ê:\nP ( β? j∈Ŝ ∈ [Lα, Uα] ∣∣ {Ê = E}) = 1− α. (20) Proof. The confidence region of β?\nj∈Ŝ is the set of βj such that the test\nof H0 : β ? j∈Ŝ accepts at the 1 − α level. The function F [V−,V+] x, σ2||ηj ||2(η T j y) is monotone in x, so solving for Lα and Uα identify the most extreme values where H0 is still accepted. This gives a 1− α confidence interval.\nIn relation to the literature on False Coverage Rate (FCR) [1], our procedure also controls the FCR.\nLemma 6.3. For each j ∈ Ŝ,\nPr ( β? j∈Ŝ ∈ [L j α, U j α] ) = 1− α. (21)\nFurthermore, the FCR of the intervals { [Ljα, U j α] } j∈Ê is α.\nProof. By (20), the conditional coverage of the confidence intervals are 1−α. The coverage holds for every element of the partition {Ê(y) = E}, so\nPr ( β? j∈Ŝ ∈ [L j α, U j α] )\n= ∑ E Pr ( β? j∈Ŝ ∈ [Lα, Uα] ∣∣ {Ê = E})Pr(Ê = E) = ∑ E (1− α)Pr(Ê = E)\n= (1− α) ∑ E Pr(Ê = E) = 1− α.\nWe summarize the algorithm for selecting and constructing confidence intervals below."
    }, {
      "heading" : "6.3 Experiments on Diabetes dataset",
      "text" : "In Figure 1, we have already seen that the confidence intervals constructed using Algorithm 3 have exactly 1−α coverage proportion. In this section, we\nperform an experiment on real data where the linear model does not hold, the noise is not Gaussian, and the noise variance is unknown. The diabetes dataset contains n = 442 diabetes patients measured on p = 10 baseline variables [6]. The baseline variables are age, sex, body mass index, average blood pressure, and six blood serum measurements, and the response y is a quantitative measure of disease progression measured one year after the baseline. The goal is to use the baseline variables to predict y, the measure of disease progression after one year, and determine which baseline variables are statistically significant for predicting y.\nSince the noise variance σ2 is unknown, we estimate it by σ2 = ‖y−ŷ‖n−p , where ŷ = Xβ̂ and β̂ = (XTX)−1XT y. For each trial we generated new responses ỹi = Xβ̂ + ̃, and ̃ is bootstrapped from the residuals ri = yi − ŷi. This is known as the residual bootstrap, and is a standard method for assessing statistical procedures when the underlying model is unknown [7]. We used marginal screening to select k = 2 variables, and then fit linear regression on the selected variables. The adjusted confidence intervals were\nAlgorithm 3 Confidence intervals for selected variables\n1: Input: Design matrix X, response y, model size k. 2: Use Algorithm 1 to select a subset of variables Ŝ and signs ŝ =\nsign(XT Ŝ y).\n3: Let A = A(Ŝ, ŝ) and b = b(Ŝ, ŝ) using (7). Let ηj = (X T Ŝ )†ej . 4: Solve for Ljα and U j α using Equation (19) where V− and V+ are computed\nvia (10) and (11) using the A, b, and ηj previously defined.\n5: Output: Return the intervals [Ljα, U j α] for j ∈ Ŝ.\nconstructed using Algorithm 3 with the estimated σ2. The nominal coverage level is varied across 1 − α ∈ {.5, .6, .7, .8, .9, .95, .99}. From Figure 6, we observe that the adjusted intervals always cover at the nominal level, whereas the z-test is always below. The experiment was repeated 2000 times."
    }, {
      "heading" : "7 Extensions",
      "text" : "The purpose of this section is to illustrate the broad applicability of the condition on selection framework. This framework was first proposed in [16]\nto form valid hypothesis tests and confidence intervals after model selection via the Lasso. However, the framework is not restricted to the Lasso, and we have shown how to apply it to marginal screening. For expository purposes, we focused the paper on marginal screening where the framework is particularly easy to understand. In the rest of this section, we show how to apply the framework to marginal screening+Lasso, orthogonal matching pursuit, and non-negative least squares. This is a non-exhaustive list of selection procedures where the condition on selection framework is applicable, but we hope this incomplete list emphasizes the ease of constructing tests and confidence intervals post-model selection via conditioning."
    }, {
      "heading" : "7.1 Marginal screening + Lasso",
      "text" : "The marginal screening+Lasso procedure was introduced in [9] as a variable selection method for the ultra-high dimensional setting of p = O(en k ). Fan et al. [9] recommend applying the marginal screening algorithm with k = n − 1, followed by the Lasso on the selected variables. This is a two-stage procedure, so to properly account for the selection we must encode the selection event of marginal screening followed by Lasso. This can be done by representing the two stage selection as a single event. Let (Ŝm, ŝm) be the variables and signs selected by marginal screening, and the (ŜL, ẑL) be the variables and signs selected by Lasso [16]. In Proposition 2.2 of [16], it is shown how to encode the Lasso selection event (ŜL, ẑL) as a set of constraints {ALy ≤ bL} 4, and in Section 4 we showed how to encode the marginal screening selection event (Ŝm, ŝm) as a set of constraints {Amy ≤ bm}. Thus the selection event of marginal screening+Lasso can be encoded as {ALy ≤ bL, Amy ≤ bm}. Using these constraints, the hypothesis test and confidence intervals described in Algorithms 2 and 3 are valid for marginal screening+Lasso."
    }, {
      "heading" : "7.2 Orthogonal Matching Pursuit",
      "text" : "Orthogonal matching pursuit (OMP) is a commonly used variable selection method. At each iteration, OMP selects the variable most correlated with the residual r, and then recomputes the residual using the residual of least squares using the selected variables. The description of the OMP algorithm is given in Algorithm 4.\n4The Lasso selection event is with respect to the Lasso optimization problem after marginal screening.\nAlgorithm 4 Orthogonal matching pursuit (OMP)\n1: Input: Design matrix X, response y, and model size k. 2: for: i = 1 to k 3: pi = arg maxj=1,...,p |rTi xj |. 4: Ŝi = ∪ij=1 {pi}. 5: ri+1 = (I −XŜiX † Ŝi )y. 6: end for 7: Output: Ŝ := {p1, . . . , pk}, and β̂Ŝ = (X T Ŝ XŜ) −1XT Ŝ y\nSimilar to Section 4, we can represent the OMP selection event as a set of linear constraints on y.\nÊ(y) = { y : sign(xTpiri)x T piri > ±x T j ri, for all j 6= pi and all i ∈ [k] } = {y : ŝixTpi(I −XŜi−1X † Ŝi−1 )y > ±xTj (I −XŜi−1X † Ŝi−1 )y and\nŝix T pi(I −XŜi−1X † Ŝi−1 )y > 0, for all j 6= pi, and all i ∈ [k] } = { y : A(Ŝ1, . . . , Ŝk, ŝ1, . . . , ŝk) ≤ b(Ŝ1, . . . , Ŝk, ŝ1, . . . , ŝk) } .\nThe selection event encodes that OMP selected a certain variable and the sign of the correlation of that variable with the residual, at steps 1 to k. The primary difference between the OMP selection event and the marginal screening selection event is that the OMP event also describes the order at which the variables were chosen. The marginal screening event only describes that the variable was among the top k most correlated, and not whether a variable was the most correlated or kth most correlated.\nSince the selection event can be represented as constraints on y, the hypothesis test and confidence intervals described in Algorithms 2 and 3 are valid for OMP selected β̂Ŝ ."
    }, {
      "heading" : "7.3 Nonnegative Least Squares",
      "text" : "Non-negative least squares (NNLS) is a simple modification of the linear regression estimator with non-negative constraints on β:\narg min β:β≥0\n1 2 ‖y −Xβ‖2 . (22)\nUnder a positive eigenvalue conditions on X, several authors [26, 22] have shown that NNLS is comprable to the Lasso in terms of prediction and\nestimation errors. The NNLS estimator also does not have any tuning parameters, since the sign constraint provides a natural form of regularization. NNLS has found applications when modeling non-negative data such as prices, incomes, count data. Non-negativity constraints arise naturally in non-negative matrix factorization, signal deconvolution, spectral analysis, and network tomography; we refer to [5] for a comprehensive survey of the applications of NNLS.\nWe show how our framework can be used to form exact hypothesis tests and confidence intervals for NNLS estimated coefficients. The primal dual solution pair (β̂, λ̂) is a solution iff the KKT conditions are satisfied,\nλ̂i := −xTi (y −Xβ̂) ≥ 0 for all i β̂ ≥ 0.\nLet Ŝ = {i : −xTi (y − Xβ̂) = 0}. By complementary slackness β̂−Ŝ = 0, where −Ŝ is the complement to the “active” variables Ŝ chosen by NNLS. Given the active set we can solve the KKT equation for the value of β̂Ŝ ,\n−XT Ŝ (y −Xβ̂) = 0\n−XT Ŝ (y −XŜ β̂Ŝ) = 0\nβ̂Ŝ = X † Ŝ y,\nwhich is a linear contrast of y. The NNLS selection event is\nÊ(y) = {y : XT Ŝ (y −Xβ̂) = 0, XT−Ŝ(y −Xβ̂) > 0}\n= {y : XT Ŝ (y −Xβ̂) ≥ 0,−XT Ŝ (y −Xβ̂) ≥ 0, XT−Ŝ(y −Xβ̂) > 0} = {y : XT Ŝ (I −XŜX † Ŝ )y ≥ 0,−XT Ŝ (I −XŜX † Ŝ )y ≥ 0, XT−Ŝ(I −XŜX † Ŝ )y > 0} = {y : A(Ŝ)y ≤ 0}.\nThe selection event encodes that for a given y the NNLS optimization program will select a subset of variables Ŝ(y). Similar to the case in OMP and marginal screening, we can use Algorithms 2 and 3, since the selection event is represented by a set of linear constraints {y : A(Ŝ)y ≤ 0}."
    }, {
      "heading" : "8 Conclusion",
      "text" : "Due to the increasing size of datasets, marginal screening has become an important method for fast variable selection. However, the standard hypothesis tests and confidence intervals used in linear regression are invalid\nafter using marginal screening to select important variables. We have described a method to perform hypothesis and form confidence intervals after marginal screening. The conditional on selection framework is not restricted to marginal screening, and also applies to OMP, marginal screening + Lasso, and NNLS."
    }, {
      "heading" : "Acknowledgements",
      "text" : "Jonathan Taylor was supported in part by NSF grant DMS 1208857 and AFOSR grant 113039. Jason Lee was supported by a NSF graduate fellowship, and a Stanford Graduate Fellowship."
    } ],
    "references" : [ {
      "title" : "False discovery rate–adjusted multiple confidence intervals for selected parameters",
      "author" : [ "Yoav Benjamini", "Daniel Yekutieli" ],
      "venue" : "Journal of the American Statistical Association,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2005
    }, {
      "title" : "Valid post-selection inference",
      "author" : [ "Richard Berk", "Lawrence Brown", "Andreas Buja", "Kai Zhang", "Linda Zhao" ],
      "venue" : "Annals of Statistics,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2013
    }, {
      "title" : "Statistics for Highdimensional Data",
      "author" : [ "Peter Lukas Bühlmann", "Sara A van de Geer" ],
      "venue" : null,
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2011
    }, {
      "title" : "Statistical inference, volume 70",
      "author" : [ "George Casella", "Roger L Berger" ],
      "venue" : null,
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 1990
    }, {
      "title" : "Nonnegativity constraints in numerical analysis",
      "author" : [ "Donghui Chen", "Robert J Plemmons" ],
      "venue" : "In Symposium on the Birth of Numerical Analysis,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2009
    }, {
      "title" : "Least angle regression",
      "author" : [ "Bradley Efron", "Trevor Hastie", "Iain Johnstone", "Robert Tibshirani" ],
      "venue" : "The Annals of statistics,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2004
    }, {
      "title" : "An introduction to the bootstrap, volume 57",
      "author" : [ "Bradley Efron", "Robert Tibshirani" ],
      "venue" : "CRC press,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 1993
    }, {
      "title" : "Variance estimation using refitted cross-validation in ultrahigh dimensional regression",
      "author" : [ "Jianqing Fan", "Shaojun Guo", "Ning Hao" ],
      "venue" : "Journal of the Royal Statistical Society. Series B (Methodological),",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2012
    }, {
      "title" : "Sure independence screening for ultrahigh dimensional feature space",
      "author" : [ "Jianqing Fan", "Jinchi Lv" ],
      "venue" : "Journal of the Royal Statistical Society: Series B (Statistical Methodology),",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2008
    }, {
      "title" : "Ultrahigh dimensional feature selection: beyond the linear model",
      "author" : [ "Jianqing Fan", "Richard Samworth", "Yichao Wu" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2009
    }, {
      "title" : "Sure independence screening in generalized linear models with np-dimensionality",
      "author" : [ "Jianqing Fan", "Rui Song" ],
      "venue" : "The Annals of Statistics,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2010
    }, {
      "title" : "A comparison of the lasso and marginal regression",
      "author" : [ "Christopher R Genovese", "Jiashun Jin", "Larry Wasserman", "Zhigang Yao" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2012
    }, {
      "title" : "An introduction to variable and feature selection",
      "author" : [ "Isabelle Guyon", "André Elisseeff" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2003
    }, {
      "title" : "Confidence intervals and hypothesis testing for high-dimensional regression",
      "author" : [ "Adel Javanmard", "Andrea Montanari" ],
      "venue" : "arXiv preprint arXiv:1306.3171,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2013
    }, {
      "title" : "On model selection consistency of penalized m-estimators: a geometric theory",
      "author" : [ "Jason Lee", "Yuekai Sun", "Jonathan E Taylor" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2013
    }, {
      "title" : "Exact inference after model selection via the lasso",
      "author" : [ "Jason D Lee", "Dennis L Sun", "Yuekai Sun", "Jonathan E Taylor" ],
      "venue" : "arXiv preprint arXiv:1311.6238,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2013
    }, {
      "title" : "The finite-sample distribution of post-model-selection estimators and uniform versus nonuniform approximations",
      "author" : [ "Hannes Leeb", "Benedikt M Pötscher" ],
      "venue" : "Econometric Theory,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2003
    }, {
      "title" : "Pötscher. Model selection and inference: Facts and fiction",
      "author" : [ "Hannes Leeb", "Benedikt M" ],
      "venue" : "Econometric Theory,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2005
    }, {
      "title" : "Can one estimate the conditional distribution of post-model-selection estimators",
      "author" : [ "Hannes Leeb", "Benedikt M Pötscher" ],
      "venue" : "The Annals of Statistics,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2006
    }, {
      "title" : "Sign-constrained least squares estimation for high-dimensional regression",
      "author" : [ "Nicolai Meinshausen" ],
      "venue" : "Electronic Journal of Statistics,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2013
    }, {
      "title" : "P-values for high-dimensional regression",
      "author" : [ "Nicolai Meinshausen", "Lukas Meier", "Peter Bühlmann" ],
      "venue" : "Journal of the American Statistical Association,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2009
    }, {
      "title" : "A unified framework for high-dimensional analysis of m-estimators with decomposable regularizers",
      "author" : [ "Sahand N Negahban", "Pradeep Ravikumar", "Martin J Wainwright", "Bin Yu" ],
      "venue" : "Statistical Science,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2012
    }, {
      "title" : "A study of error variance estimation in lasso regression",
      "author" : [ "Stephen Reid", "Robert Tibshirani", "Jerome Friedman" ],
      "venue" : null,
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2013
    }, {
      "title" : "Non-negative least squares for high-dimensional linear models: Consistency and sparse recovery without regularization",
      "author" : [ "Martin Slawski", "Matthias Hein" ],
      "venue" : "Electronic Journal of Statistics,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2013
    }, {
      "title" : "Scaled sparse linear regression",
      "author" : [ "Tingni Sun", "Cun-Hui Zhang" ],
      "venue" : "Biometrika, 99(4):879–898,",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2012
    }, {
      "title" : "Significance analysis of microarrays applied to the ionizing radiation response",
      "author" : [ "Virginia Goss Tusher", "Robert Tibshirani", "Gilbert Chu" ],
      "venue" : "Proceedings of the National Academy of Sciences,",
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 2001
    }, {
      "title" : "On asymptotically optimal confidence regions and tests for high-dimensional models",
      "author" : [ "Sara van de Geer", "Peter Bühlmann", "Ya’acov Ritov" ],
      "venue" : "arXiv preprint arXiv:1303.0518,",
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 2013
    }, {
      "title" : "Sharp thresholds for high-dimensional and noisy sparsity recovery using `1-constrained quadratic programming (lasso)",
      "author" : [ "M.J. Wainwright" ],
      "venue" : null,
      "citeRegEx" : "30",
      "shortCiteRegEx" : "30",
      "year" : 2009
    }, {
      "title" : "High dimensional variable selection",
      "author" : [ "Larry Wasserman", "Kathryn Roeder" ],
      "venue" : "Annals of statistics,",
      "citeRegEx" : "31",
      "shortCiteRegEx" : "31",
      "year" : 2009
    }, {
      "title" : "Confidence intervals for lowdimensional parameters with high-dimensional data",
      "author" : [ "Cun-Hui Zhang", "S Zhang" ],
      "venue" : "arXiv preprint arXiv:1110.2563,",
      "citeRegEx" : "32",
      "shortCiteRegEx" : "32",
      "year" : 2011
    }, {
      "title" : "On model selection consistency of lasso",
      "author" : [ "P. Zhao", "B. Yu" ],
      "venue" : null,
      "citeRegEx" : "33",
      "shortCiteRegEx" : "33",
      "year" : 2006
    } ],
    "referenceMentions" : [ {
      "referenceID" : 2,
      "context" : "However in the high-dimensional p > n setting, the least squares estimator is an underdetermined problem, and the predominant approach is to perform variable selection or model selection [3].",
      "startOffset" : 187,
      "endOffset" : 190
    }, {
      "referenceID" : 12,
      "context" : "Marginal screening is the simplest and most commonly used of the variable selection procedures [13, 28, 20].",
      "startOffset" : 95,
      "endOffset" : 107
    }, {
      "referenceID" : 25,
      "context" : "Marginal screening is the simplest and most commonly used of the variable selection procedures [13, 28, 20].",
      "startOffset" : 95,
      "endOffset" : 107
    }, {
      "referenceID" : 11,
      "context" : "Furthermore, the selection properties are comparable to the Lasso [12].",
      "startOffset" : 66,
      "endOffset" : 70
    }, {
      "referenceID" : 8,
      "context" : "In the ultrahigh dimensional setting p = O(en k ), marginal screening is shown to have the SURE screening property, P (S ⊂ Ŝ), that is marginal screening selects a superset of the truly relevant variables [9, 11, 10].",
      "startOffset" : 205,
      "endOffset" : 216
    }, {
      "referenceID" : 10,
      "context" : "In the ultrahigh dimensional setting p = O(en k ), marginal screening is shown to have the SURE screening property, P (S ⊂ Ŝ), that is marginal screening selects a superset of the truly relevant variables [9, 11, 10].",
      "startOffset" : 205,
      "endOffset" : 216
    }, {
      "referenceID" : 9,
      "context" : "In the ultrahigh dimensional setting p = O(en k ), marginal screening is shown to have the SURE screening property, P (S ⊂ Ŝ), that is marginal screening selects a superset of the truly relevant variables [9, 11, 10].",
      "startOffset" : 205,
      "endOffset" : 216
    }, {
      "referenceID" : 16,
      "context" : "Several authors have previously noted this problem including recent work in [17, 18, 19, 2].",
      "startOffset" : 76,
      "endOffset" : 91
    }, {
      "referenceID" : 17,
      "context" : "Several authors have previously noted this problem including recent work in [17, 18, 19, 2].",
      "startOffset" : 76,
      "endOffset" : 91
    }, {
      "referenceID" : 18,
      "context" : "Several authors have previously noted this problem including recent work in [17, 18, 19, 2].",
      "startOffset" : 76,
      "endOffset" : 91
    }, {
      "referenceID" : 1,
      "context" : "Several authors have previously noted this problem including recent work in [17, 18, 19, 2].",
      "startOffset" : 76,
      "endOffset" : 91
    }, {
      "referenceID" : 16,
      "context" : "A major line of work [17, 18, 19] has described the difficulty of inference post model selection: the distribution of post model selection estimates is complicated and cannot be approximated in a uniform sense by their asymptotic counterparts.",
      "startOffset" : 21,
      "endOffset" : 33
    }, {
      "referenceID" : 17,
      "context" : "A major line of work [17, 18, 19] has described the difficulty of inference post model selection: the distribution of post model selection estimates is complicated and cannot be approximated in a uniform sense by their asymptotic counterparts.",
      "startOffset" : 21,
      "endOffset" : 33
    }, {
      "referenceID" : 18,
      "context" : "A major line of work [17, 18, 19] has described the difficulty of inference post model selection: the distribution of post model selection estimates is complicated and cannot be approximated in a uniform sense by their asymptotic counterparts.",
      "startOffset" : 21,
      "endOffset" : 33
    }, {
      "referenceID" : 15,
      "context" : "Although the focus of this paper is on marginal screening, the “condition on selection” framework, first proposed for the Lasso in [16], is much more general; we use marginal screening as a simple and clean illustration of the applicability of this framework.",
      "startOffset" : 131,
      "endOffset" : 135
    }, {
      "referenceID" : 21,
      "context" : "Such results establish, under restrictive assumptions on X, the Lasso β̂ is close to the unknown β0 [24] and selects the correct model [33, 30, 15].",
      "startOffset" : 100,
      "endOffset" : 104
    }, {
      "referenceID" : 30,
      "context" : "Such results establish, under restrictive assumptions on X, the Lasso β̂ is close to the unknown β0 [24] and selects the correct model [33, 30, 15].",
      "startOffset" : 135,
      "endOffset" : 147
    }, {
      "referenceID" : 27,
      "context" : "Such results establish, under restrictive assumptions on X, the Lasso β̂ is close to the unknown β0 [24] and selects the correct model [33, 30, 15].",
      "startOffset" : 135,
      "endOffset" : 147
    }, {
      "referenceID" : 14,
      "context" : "Such results establish, under restrictive assumptions on X, the Lasso β̂ is close to the unknown β0 [24] and selects the correct model [33, 30, 15].",
      "startOffset" : 135,
      "endOffset" : 147
    }, {
      "referenceID" : 2,
      "context" : "We refer to the reader to [3] for a comprehensive discussion about the theoretical properties of the Lasso.",
      "startOffset" : 26,
      "endOffset" : 29
    }, {
      "referenceID" : 28,
      "context" : "One class of methods uses sample splitting or subsampling to obtain confidence intervals and p-values [31, 23].",
      "startOffset" : 102,
      "endOffset" : 110
    }, {
      "referenceID" : 20,
      "context" : "One class of methods uses sample splitting or subsampling to obtain confidence intervals and p-values [31, 23].",
      "startOffset" : 102,
      "endOffset" : 110
    }, {
      "referenceID" : 1,
      "context" : "In the post model selection literature, the recent work of [2] proposed the POSI approach, a correction to the usual t-test confidence intervals by controlling the familywise error rate for all parameters in any possible submodel.",
      "startOffset" : 59,
      "endOffset" : 62
    }, {
      "referenceID" : 26,
      "context" : "A separate line of work establishes the asymptotic normality of a corrected estimator obtained by “inverting” the KKT conditions [29, 32, 14].",
      "startOffset" : 129,
      "endOffset" : 141
    }, {
      "referenceID" : 29,
      "context" : "A separate line of work establishes the asymptotic normality of a corrected estimator obtained by “inverting” the KKT conditions [29, 32, 14].",
      "startOffset" : 129,
      "endOffset" : 141
    }, {
      "referenceID" : 13,
      "context" : "A separate line of work establishes the asymptotic normality of a corrected estimator obtained by “inverting” the KKT conditions [29, 32, 14].",
      "startOffset" : 129,
      "endOffset" : 141
    }, {
      "referenceID" : 15,
      "context" : "Most closely related to our work is the “condition on selection” framework laid out in [16] for the Lasso.",
      "startOffset" : 87,
      "endOffset" : 91
    }, {
      "referenceID" : 2,
      "context" : "By extension, we do not make any assumptions on n and p, which is unusual in high-dimensional statistics [3].",
      "startOffset" : 105,
      "endOffset" : 108
    }, {
      "referenceID" : 7,
      "context" : "A data splitting technique is used in [8], while [27] proposes a method that computes the regression estimate and an estimate of the variance simultaneously.",
      "startOffset" : 38,
      "endOffset" : 41
    }, {
      "referenceID" : 24,
      "context" : "A data splitting technique is used in [8], while [27] proposes a method that computes the regression estimate and an estimate of the variance simultaneously.",
      "startOffset" : 49,
      "endOffset" : 53
    }, {
      "referenceID" : 22,
      "context" : "We refer the reader to [25] for a survey and comparison of the various methods,",
      "startOffset" : 23,
      "endOffset" : 27
    }, {
      "referenceID" : 15,
      "context" : "This section summarizes the recent tools developed in [16] for testing contrasts2 ηT y of a constrained Gaussian y.",
      "startOffset" : 54,
      "endOffset" : 58
    }, {
      "referenceID" : 15,
      "context" : "The results are stated without proof and the proofs can be found in [16].",
      "startOffset" : 68,
      "endOffset" : 72
    }, {
      "referenceID" : 8,
      "context" : "In [9], the screening property S0 ⊂ Ŝ for the marginal screening algorithm is",
      "startOffset" : 3,
      "endOffset" : 6
    }, {
      "referenceID" : 3,
      "context" : "The standard way to obtain an interval is to invert a pivotal quantity [4].",
      "startOffset" : 71,
      "endOffset" : 74
    }, {
      "referenceID" : 0,
      "context" : "In relation to the literature on False Coverage Rate (FCR) [1], our procedure also controls the FCR.",
      "startOffset" : 59,
      "endOffset" : 62
    }, {
      "referenceID" : 5,
      "context" : "The diabetes dataset contains n = 442 diabetes patients measured on p = 10 baseline variables [6].",
      "startOffset" : 94,
      "endOffset" : 97
    }, {
      "referenceID" : 6,
      "context" : "This is known as the residual bootstrap, and is a standard method for assessing statistical procedures when the underlying model is unknown [7].",
      "startOffset" : 140,
      "endOffset" : 143
    }, {
      "referenceID" : 15,
      "context" : "This framework was first proposed in [16]",
      "startOffset" : 37,
      "endOffset" : 41
    }, {
      "referenceID" : 8,
      "context" : "1 Marginal screening + Lasso The marginal screening+Lasso procedure was introduced in [9] as a variable selection method for the ultra-high dimensional setting of p = O(en k ).",
      "startOffset" : 86,
      "endOffset" : 89
    }, {
      "referenceID" : 8,
      "context" : "[9] recommend applying the marginal screening algorithm with k = n − 1, followed by the Lasso on the selected variables.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 15,
      "context" : "Let (Ŝm, ŝm) be the variables and signs selected by marginal screening, and the (ŜL, ẑL) be the variables and signs selected by Lasso [16].",
      "startOffset" : 134,
      "endOffset" : 138
    }, {
      "referenceID" : 15,
      "context" : "2 of [16], it is shown how to encode the Lasso selection event (ŜL, ẑL) as a set of constraints {ALy ≤ bL} 4, and in Section 4 we showed how to encode the marginal screening selection event (Ŝm, ŝm) as a set of constraints {Amy ≤ bm}.",
      "startOffset" : 5,
      "endOffset" : 9
    }, {
      "referenceID" : 23,
      "context" : "Under a positive eigenvalue conditions on X, several authors [26, 22] have shown that NNLS is comprable to the Lasso in terms of prediction and",
      "startOffset" : 61,
      "endOffset" : 69
    }, {
      "referenceID" : 19,
      "context" : "Under a positive eigenvalue conditions on X, several authors [26, 22] have shown that NNLS is comprable to the Lasso in terms of prediction and",
      "startOffset" : 61,
      "endOffset" : 69
    }, {
      "referenceID" : 4,
      "context" : "Non-negativity constraints arise naturally in non-negative matrix factorization, signal deconvolution, spectral analysis, and network tomography; we refer to [5] for a comprehensive survey of the applications of NNLS.",
      "startOffset" : 158,
      "endOffset" : 161
    } ],
    "year" : 2014,
    "abstractText" : "We develop a framework for post model selection inference, via marginal screening, in linear regression. At the core of this framework is a result that characterizes the exact distribution of linear functions of the response y, conditional on the model being selected (“condition on selection” framework). This allows us to construct valid confidence intervals and hypothesis tests for regression coefficients that account for the selection procedure. In contrast to recent work in highdimensional statistics, our results are exact (non-asymptotic) and require no eigenvalue-like assumptions on the design matrix X. Furthermore, the computational cost of marginal regression, constructing confidence intervals and hypothesis testing is negligible compared to the cost of linear regression, thus making our methods particularly suitable for extremely large datasets. Although we focus on marginal screening to illustrate the applicability of the condition on selection framework, this framework is much more broadly applicable. We show how to apply the proposed framework to several other selection procedures including orthogonal matching pursuit, non-negative least squares, and marginal screening+Lasso.",
    "creator" : "LaTeX with hyperref package"
  }
}