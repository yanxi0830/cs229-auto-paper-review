{
  "name" : "1205.4891.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Clustering is difficult only when it does not matter∗",
    "authors" : [ "Amit Daniely", "Michael Saks" ],
    "emails" : [ "amit.daniely@math.huji.ac.il", "nati@cs.huji.ac.il", "saks@math.rutgers.edu." ],
    "sections" : [ {
      "heading" : null,
      "text" : "We introduce a theoretical framework of clustering in metric spaces that revolves around a notion of ”good clustering”. We show that if a good clustering exists, then in many cases it can be efficiently found. Our conclusion is that contrary to popular belief, clustering should not be considered a hard task.\nKeywords: Cluster Analysis, Hardness of clustering, Theoretical Framework for clustering, Stability.\n∗Credit for this title goes to Tali Tishby who stated this in a conversation with one of us many years ago. †Department of Mathematics, Hebrew University, Jerusalem 91904, Israel. Supported in part by a binational Israel-USA grant 2008368. amit.daniely@math.huji.ac.il ‡School of Computer Science and Engineering, Hebrew University, Jerusalem 91904, Israel. Supported in part by a binational Israel-USA grant 2008368. nati@cs.huji.ac.il §Department of Mathematics, Rutgers University, Piscataway, NJ 08854. Supported in part by NSF under grant CCF-0832787 and by a binational Israel-USA grant 2008368. saks@math.rutgers.edu.\nar X\niv :1\n20 5.\n48 91\nv1 [\ncs .L\nG ]\n2 2\nM ay"
    }, {
      "heading" : "1 Introduction",
      "text" : "Clustering is the task of partitioning a set of objects in a meaningful way. Notwithstanding several recent attempts to develop a theory of clustering (e.g. [1, 4, 9]), our foundational understanding of the matter is still quite unsatisfactory.\nThe clustering problem deals with a set of objects X that is equipped with some additional structure, such as a dissimilarity (or similarity) function w : X×X → [0,∞). Informally, we are seeking a partition of X into clusters, such that objects are placed in the same cluster iff they are sufficiently similar. Here are some concrete popular manifestations of this general problem.\n1. A very popular optimization criterion is k-means. Aside from X and w one is given an integer k. The goal is partition X into k parts C1, . . . , Ck and find a center xi ∈ Ci in each part so as to minimize ∑ i ∑ y∈Ci w\n2(y, xi). Other popular criteria of similar nature are k-medians, min-sum and others.\n2. Many clustering algorithms work “bottom up”. Initially, every singleton in X is considered as a separate cluster, and the algorithm proceeds by repeatedly merging nearby clusters. Other popular algorithms work “top down”: Here we start with a single cluster that consists of the whole space. Subsequently, existing clusters get split to improve some objective function.\n3. Several successful methods use spectral methods. One associates a matrix (e.g. a Laplacian) to (X,w), and partitions X according to the eigenvectors of this matrix.\nApproaches to the clustering problem that focus on some objective function, usually result in NP -hard optimization problems. Consequently, most existing theoretical studies concentrate on designing approximation algorithms for such optimization problems and proving appropriate hardness results.\nHowever, the practical purpose of clustering is not to optimize such objectives. Rather, our goal is to find a meaningful partition of the data (provided, of course, that such a partition exists). The point that we advocate is that a satisfactory theory of clustering, should start with a definition of a good clustering and proceed to determine when a good clustering can be found efficiently. In this paper, we follow this approach when the underlying space in a metric1 space.\nThis perspective leads to conclusions which are at odds with common beliefs regarding clustering. This applies, in particular, to the computational hardness of clustering. The infeasibility of optimizing most of the popular objectives led many theoreticians, to the bleak view that clustering is hard. However, we show that in many circumstances a good clustering can be efficiently found, leading to the opposite conclusion. From the practitioner’s viewpoint, ”clustering is either easy or pointless” – that is, whenever\n1The assumption that d is a metric is not too strict. E.g., much of what we do applies even if we weaken the triangle inequality to λ · d(x, z) ≤ d(x, y) + d(y, z) for λ bounded away from zero.\nthe input admits a good clustering, finding it is feasible. Our analysis provides some support to this view.\nThis work is one of several recent attempts to develop a mathematical theory of clustering. For more on the relevant literature, see Section 4."
    }, {
      "heading" : "1.1 A Theoretical Framework for Clustering in Metric Spaces",
      "text" : "There are numerous notions of clusters in data sets and clustering methods to be found in the literature. Although not necessarily stated explicitly, these methods are guided by an ideal (in the Platonic sense) notion of a good cluster in a space X. This is a subset C ⊆ X such that if x ∈ C and y 6∈ C, then x is substantially closer to C than y is. To rule out trivialities we usually require C to be big enough. This, in particular, eliminates the possibility of trivial singleton clusters. Even more emphasis is put on problems of clustering. Here we seek partitions of the space X into clusters such that every x ∈ X is substantially closer to the cluster containing it than to any other cluster. The problem is specified in terms of a proximity measure ∆(x,A) between elements x ∈ X and subsets A ⊆ X. Numerous natural choices for ∆(·, ·) suggest themselves. For example, if X is a metric space, it is reasonable to define ∆(x,A) in terms of x’s distances from members of A.\nIn the present paper we consider a metric space (X, d) from which data points are sampled2 according to a probability distribution P . The definition we adopt here is ∆(x,A) = Ey∼P [d(x, y)|y ∈ A]. Other interesting definitions suggest themselves, e.g., ∆′(x,A) = infy∈A\\{x} d(x, y).\nA technical comment: The definition of ∆(x,A) depends on the distribution P . To simplify notations we omit subscripts such as P when they are clear from the context.\nFormally, we say that C ⊂ X is an (α, γ)-cluster for α > 0, γ > 1 if P (C) ≥ α and for (almost-)every3 x ∈ C, y /∈ C,\n∆(y, C) ≥ γ ·∆(x,C).\nLikewise, a partition C = {C1, . . . , Ck} of X is an (α, γ)-clustering for some α > 0, γ > 1 if\n∆(x,Cj) ≥ γ ·∆(x,Ci)\nfor every i 6= j and (almost-)every x ∈ Ci and, in addition, P (Ci) ≥ α for every i. A few technical points are in place.\n• We study (α, γ)-clusterings of a space as well as partitions of a space into (α, γ)clusters. We note that although these two notions are similar, they are not identical.\n2In certain cases it is inappropriate to assume that points of X are drawn at random. It is also possible that we do not know how X is sampled. In such circumstances, we consider P as the uniform distribution on X.\n3Almost means, as usual, that we are allowing an exceptional set of measure zero.\n• Our results hold if we choose instead to define ∆(x,A) as E[d(x, y)|y ∈ A \\ {x}]. This definition is perfectly reasonable, but it leads to certain minor technical complications that the current definition avoids. Moreover, the difference between the two definitions is rather insignificant, since our main interest is in cases where P ({x}) P (A).\nOur main focus here is on efficient algorithms for finding (α, γ)-clusters and clusterings. The analysis of these algorithms rely on the structural properties of such clusters. We can now present our main results. To simplify matters without compromising the big picture, we state our theorems in the case when X is a given finite metric space.\nTheorem 1.1 For every fixed γ > 1, α > 0 there is an algorithm that finds all (α, γ)clusterings of a given finite metric space X and runs in time poly(|X|).\nTheorem 1.2 There is a polynomial time algorithm that on input a finite metric space X and α > 0 finds all γ-clusters in X with γ > 3 and a partition of X into (α, γ)clusters with γ > 3, provided one exists. Moreover, the latter problem is NP -hard for γ = 5/2."
    }, {
      "heading" : "1.2 An overview",
      "text" : "Our discussion splits according to the value of the parameter α. When α is bounded away from zero we work by exhaustive sampling (e.g. as in [2]). We first sample a small set of points S from the space. Since |S| is small (logarithmic in an error parameter), it is computationally feasible to consider all possible partitions Π of S. To each partition Π of S we associate a clustering that can be viewed as the corresponding “Voronoi diagram”. If the space has an (α, γ)-clustering C, let Π∗ be the partition of S that is consistent with C. We show that the “Voronoi diagram” of Π∗ nearly coincides with C provided that γ is bounded away from 1. Concretely, Lemma 2.2 controls the distances between points that reside in distinct clusters in an (α, γ)-clustering. Together with Hoeffding’s inequality this yields Lemma 2.3 and Corollary 2.4 which show that the “Voronoi diagram” of an appropriate partition of a small sample is nearly an (α, γ)clustering. Lemma 2.5 speaks about the collection of all possible (α, γ)-clusterings of the space. It shows that every two distinct (α, γ)-clusterings must differ substantially. Consequently (Corollary 2.6) there is a bound on the number of (α, γ)-clusterings that any space can have. All of this is then used to derive an efficient algorithm that can find all (α, γ)-clusterings of the space, proving Theorem 1.1.\nIn section 3 we deal with the case of small α. This affects the analysis, since we require that the dependency of the algorithm’s runtime on α be poly( 1\nα ). We show that\n(α, 3+ )-clusters are very simple: Such a cluster is a ball and any two such clusters that intersect are (inclusion) comparable. These structural properties are used to derive an efficient algorithm that partitions the space into (α, 3 + )-clusters (provided that such a partition exists), proving the positive part of Theorem 1.2. To match this result, we\nshow that finding a partition of the space into (α, 2.5)-clusters is NP-Hard, proving Theorem 1.2 in full.\nLastly, in section 4 we discuss some connection to other work, both old and new, as well as some open questions arising from our work."
    }, {
      "heading" : "2 Clustering into Few Clusters – α is bounded away",
      "text" : "from zero\nThroughout the section, X is a metric space endowed with a probability measure P . To avoid confusion, other probability measures that are used throughout, are denoted by Pr. We define a metric d between two collections of subsets of X, say C = {C1, . . . , Ck} and C ′ = {C ′1, . . . , C ′k}. Namely, d(C, C ′) = minP (∪ki=1Ci⊕C ′σ(i)) where A⊕B denotes symmetric difference, and the minimum is over all permutations σ ∈ Sk. The definition of d(C, C ′) extends naturally to the case where C and C ′ have k resp. l sets and, say l ≤ k. The only change is that now σ : [l]→ [k] is 1 : 1.\nWe define ∆ also on sets. If A,B ⊆ X, we define ∆(A,B) as the expectation of d(x, y) where x and y are drawn from the distribution P restricted to A and B respectively. It is easily verified that ∆ is symmetric and satisfies the triangle inequality. It is usually not a metric, since ∆(A,A) is usually positive.\nProposition 2.1 For every A,B,C ⊂ X,\n∆(A,B) = ∆(B,A) and ∆(A,B) ≤ ∆(A,C) + ∆(C,B)\nAs the following lemma shows, distances in an (α, γ)-clustering are fairly regular\nLemma 2.2 Let C1, . . . , Ck be an (α, γ)-clustering and let i 6= j. Then\n1. For almost every x ∈ Ci, y ∈ Cj, γ−1γ ∆(y, Ci) ≤ d(x, y) ≤ γ2+1 γ(γ−1)∆(y, Ci)\n2. For almost every x, y ∈ Ci, d(x, y) ≤ 2γ−1 ·∆(x,Cj)\nProof. Let x ∈ Ci, y ∈ Cj. For the left inequality in part 1, note that\nd(x, y) ≥ ∆(y, Ci)−∆(x,Ci)\n≥ ∆(y, Ci)− 1\nγ ·∆(x,Cj)\n≥ ∆(y, Ci)− 1\nγ · [d(x, y) + ∆(y, Cj)]\n≥ ∆(y, Ci)− 1 γ · [d(x, y) + 1 γ ·∆(y, Ci)]\nFor the right inequality,\nd(x, y) ≤ ∆(x,Ci) + ∆(y, Ci)\n≤ 1 γ ·∆(x,Cj) + ∆(y, Ci) ≤ 1 γ · (d(x, y) + ∆(y, Cj)) + ∆(y, Ci) ≤ 1 γ · (d(x, y) + 1 γ ·∆(y, Ci)) + ∆(y, Ci)\nFor part 2,\nd(x, y) ≤ ∆(x,Ci) + ∆(y, Ci)\n≤ 1 γ · [∆(x,Cj) + ∆(y, Cj)] ≤ 1 γ · [2 ·∆(x,Cj) + d(x, y)]\nNote that for γ →∞ all distances d(x, y) with x ∈ Ci and y ∈ Cj are roughly equal and d(x1, x2) d(x1, y) for all x1, x2 ∈ Ci and y ∈ Cj with i 6= j. We show next how to recover an (α, γ)-clustering by sampling. For x ∈ X and A ⊆ X finite, we denote the average distance from x to A’s elements by ∆U(x,A) := 1 |A| ∑ y∈A d(x, y). A finite sample set S provides us with an estimate for the distance of a point x from a (not necessarily finite) C ⊆ X. Namely, we define the empirical proximity of x to C as ∆emp(x,C) := ∆U(x,C ∩ S).\nWe turn to explain how we recover an unknown (α, γ)-clustering of X with α > 0 and γ > 1. Consider a collection C = {C1, . . . ,Ck} ⊆ X of disjoint subsets of X. We define a “Voronoi diagram” corresponding to S, denoted Cγ = {Cγ1 , . . . , C γ k}. Here\nCγi = {x ∈ X : ∀j 6= i, γ ·∆emp(x,Ci) < ∆emp(x,Cj)}. If C is a (α, γ)-clustering of X, we expect Cγ to be a good approximation of C.\nLemma 2.3 Let C = {C1, . . . , Ck} be an (α, γ)-clustering of X. Let S = {Z1, . . . , Zm} be an i.i.d. sample with distribution P and let q 6= p. Then, for every x ∈ Cq, > 0,\nP (∆emp(x,Cp) ≥ (γ − ) ·∆emp(x,Cq)) ≥ 1− 3 exp ( − (\n(γ − 1)α√ 8γ(γ2 + 1)\n)2 ·m ) The proof follows by a standard application of the Hoeffding bound and is deferred to the appendix.\nCorollary 2.4 Let S = {Z1, . . . , Zm} be an i.i.d. sample with distribution P . Then,\nfor every (α, γ)-clustering C, Pr(d(C, Cγ−δ) > t) ≤ 3 tα · exp\n( − (\n(γ−1)δα√ 8γ(γ2+1)\n)2 ·m ) .\nProof. Denote C = {C1, . . . , Ck}. By lemma 2.3, with = δ, we have\nE[d(C,Cγ−δ)] = E[P (∪ki=1Ci ⊕ C γ−δ i )]\n= k∑ i=1 ∫ Ci Pr(x /∈ Cγ−δi )dP (x)\n= k∑ i=1 ∑ j 6=i ∫ Ci Pr(x ∈ Cγ−δj )dP (x)\n≤ k∑ i=1 (k − 1) · P (Ci) · 3 · exp\n( − (\n(γ − 1)δα√ 8γ(γ2 + 1)\n)2 ·m )\n= (k − 1) · 3 · exp ( − (\n(γ − 1)δα√ 8γ(γ2 + 1)\n)2 ·m )\nThus, the lemma follows from Markov’s inequality and the fact that k − 1 ≤ k ≤ 1 α\nWe next turn to investigate the collection of all (α, γ)-clusterings of the given space. We observe first that every two distinct (α, γ)-clusterings must differ substantially.\nLemma 2.5 If C, C ′ are two (α, γ)-clusterings with d(C, C ′) > 0, then d(C, C ′) ≥ α·(γ−1)2 2γ2−γ+1 .\nProof. Denote C = {C1, . . . , Ck}, C ′ = {C ′1, . . . , C ′k′} and = d(C, C ′). By adding empty clusters if needed, we can assume that k = k′. By reordering the clusters, if necessary, we can assume that P (∪ki=1Ci ⊕ C ′i) = and P (C ′1 ⊕ C1) > 0. Again by selecting the ordering we can assume the existence of some point x that is in C ′1 \\ C1 and in C2 \\ C ′2.\n∆(x,C ′1) = 1 P (C ′1) · ∫ C′1 d(x, y)dP (y)\n≥ 1 P (C ′1) · ∫ C1 d(x, y)dP (y)− 1 P (C ′1) · ∫ C1\\C′1 d(x, y)dP (y) ≥ P (C1) P (C ′1) ·∆(x,C1)− P (C1 \\ C ′1) α · max y∈C1\\C′1 d(x, y) (1)\n≥ (\n1− α\n) ·∆(x,C1)−\nα · γ\n2 + 1\nγ(γ − 1) ∆(x,C1) ≥ (\n1− α · 2γ 2 − γ + 1 γ(γ − 1)\n) · γ ·∆(x,C2)\nFor the second inequality note that P (C′1) P (C1) ≥ P (C1)−P (C1\\C ′ 1) P (C1) ≥ 1− α . The third inequality follows from lemma 2.2.\nAs we just saw ∆(x,C′1) ∆(x,C2) ≥ ( 1− α · 2γ2−γ+1 γ(γ−1) ) · γ. The same argument yields as well\n∆(x,C2) ∆(x,C′1)\n≥ (\n1− α · 2γ2−γ+1 γ(γ−1)\n) · γ. Consequently 1 ≥ ( 1−\nα · 2γ2−γ+1 γ(γ−1)\n) · γ which proves\nthe lemma. As we observe next, for every α > 0 and γ > 1 the number of (α, γ)-clusterings that any space can have does not exceed f(α, γ), where f depends only on α and γ but not on the space. We find this somewhat surprising, although the proof is fairly easy.\nCorollary 2.6 There is a function f = f(α, γ) defined for α > 0 and γ > 1 with the following property. The number of (α, γ)-clusterings of any metric probability space X is at most f(α, γ). This works in particular with f(α, γ) = 2 ·(\n12(2γ2−γ+1) α2(γ−1)2\n)(√8γ(γ2+1) (γ−1)2α )2 ·ln( 1 α )\nProof. Consider the following experiment. We take an i.i.d. sample Z1, . . . , Zm of points from the distribution P with\nm >\n(√ 8γ(γ2 + 1)\n(γ − 1)2α\n)2 · ln (\n12(2γ2 − γ + 1) α2(γ − 1)2\n) .\nand partition them randomly into k ≤ ( 1 α\n) parts T1, . . . , Tk. This induces a partition C∗ = {C1, . . . , Ck} of the space X defined by\nCi = {x ∈ X : ∀j 6= i, ∆U(x, Ti) < ∆U(x, Tj)}\nFor every (α, γ)-clustering C of X we consider the event AC that the induced partition of X satisfies d(C, C∗) < α · (γ−1) 2\n2·(2γ2−γ+1) . Let us consider the events AC over distinct\n(α, γ)-clusterings of the space. By Lemma 2.5, these events AC are disjoint. Now consider the event B that the Ti’s are consistent with C. There are at most ( 1α)\nm ways to partition the sampled points into 1\nα parts or less, so that Pr(B) ≥ αm. By the choice\nof m and by Corollary 2.4 Pr(AC|B) ≥ 12 . Thus, Pr(AC) ≥ Pr(B) · Pr(AC|B) ≥ 1 2 αm. Consequently, X has at most f(α, γ) = 2( 1 α )m distinct (α, γ)-clusterings, as claimed.\nNote 2.7 Fix α > 0. The number of (α, γ)-clusterings might be quite large when γ is close to 1. For example, let X be an n-point space, with uniform metric and uniform probability measure. Every partition in which each part has cardinality ≥ α · n is an (α, n\nn−1)-clustering 4.\n4Note that this example is not valid if we define ∆(x,A) = E[d(x, y)|y ∈ A \\ {x}]. To overcome this point, we can replace every point x ∈ X by many copies, where two copies of x are distance and a copy of x and a copy of y 6= x are at distance d(x, y)."
    }, {
      "heading" : "Algorithmic Aspects",
      "text" : "Fix α > 0, γ > 1. We shall now show that an (α, γ)-clustering can be well approximated efficiently. By lemma 2.4, (α, γ)-clustering can be approximated by a small sample, where the approximation is with respect to the symmetric difference metric. A major flaw of this approximation scheme is that we have no verification method to accompany it. We do not know how to check whether a given partition is close to an (α, γ)clustering w.r.t. the symmetric difference metric. To this end, we introduce another notion of approximation. A family of subsets of X, C = {C1, . . . , Ck}, is an ( , α, γ)clustering if\n• For every i ∈ [k], P (Ci) ≥ α\n• There is a set N ⊂ X with P (N) ≤ such that every x ∈ X \\ N , belongs to exactly one Ci and for every j 6= i, ∆(x,Cj) ≥ γ ·∆(x,Ci).\nWe consider next a partition that is attained by the method of Corollary 2.4. We show that if it is -close to an (α, γ)-clustering w.r.t. symmetric differences, then it is necessarily an (α− , γ −O( ), )-clustering.\nWe associate with every collection A = {A1, . . . , Ak} of finite subsets5 of X the following collection of subsets Cγ(A) = {Cγ1 (A), . . . , C γ k (A)}:\nCγi (A) = {x ∈ X : ∀j 6= i, γ ·∆U(x,Ai) < ∆U(x,Aj)} (2)\nwhere, as above, ∆U(x,A) := 1 |A| ∑ z∈A d(x, z).\nProposition 2.8 Let C = {C1, . . . , Ck} be an (α, γ)-clustering. Let A = {A1, . . . , Ak} where ∀i, Ai ⊂ Ci and d(Cγ(A), C) < . Then Cγ(A) is an (α− , γ−O( ), )-clustering. The unspecified coefficients in the O-term depend on α and γ.\nThe main idea of the proof is rather simple: The assumption d(Cγ(A), C) < implies that for all i the set Ci⊕Cγi (A) is small. This suggests that ∆(x,Ci) ≈ ∆(x,C γ i (A)) for most points x ∈ X. The only difficulty in realizing this idea is that points in Ci⊕Cγi (A) might have a large effect on either ∆(x,Ci) or ∆(x,C γ i (A)). But the assumption that Ai ⊂ Ci gives us control over the distances between x to these points. The full proof can be found in the appendix.\nTo recap, the above discussion suggests a randomized algorithm that for a given > 0 runs in time poly(1 ) and finds w.h.p. an (α − , γ − O( ), )-clustering of X provided that X has an (α, γ)-clustering C. We take m = Θ(log(1 )) i.i.d. samples from X and go over all possible partitions of the sample points into at most 1 α sets.\nThere are only ( 1 )O(log( 1 α )) such partitions. We next check whether the clustering of X that is induced as in Equation (2) is an (α − , γ − O( ), )-clustering (this can be easily done by standard statistical estimates).\n5In fact, we will allow A1, . . . , Ak to have multiple points. Formally, then, A1, . . . , Ak are multisets.\nTo see that the algorithm accomplishes what it should, note that the failure probability in corollary 2.4 with δ = γ−1 can be ≤ 1\n2 for m = Θ(log(1 )). Thus, w.p. > 1 2 one\nof the considered partitions induces a partition of X which is -close in the symmetric difference sense to C. By Proposition 2.8, this partition is an (α − , γ − O( ), )- clustering.\nThis also proves Theorem 1.1: If our input is a finite metric space X, we can apply the above algorithm with = 1|X|+1 and examples that are being sampled from X uniformly at random. As explained, w.h.p., the algorithm will consider every partition which is -close in the symmetric difference sense to any of X’s (α, γ)-clusterings. However, since = 1|X|+1 , two -close partitions must be identical. This proves Theorem 1.1.\nNote that by corollary 2.6, all the (α, γ)-clusterings can be approximated. A similar algorithm can efficiently find an approximate (α, γ, )-clustering, provided that one exists6. Also, similar techniques yield an algorithm to approximate an individual (α, γ)cluster."
    }, {
      "heading" : "3 Clustering into Many Clusters",
      "text" : "To simplify matters we consider only finite metric spaces endowed with a uniform probability distribution7.\nLemma 3.1 Let X be a metric space and let > 0.\n1. Let C1, C2 ⊆ X be two (3 + )-clusters. Then C1 ∩C2 = ∅, C1 ⊂ C2 or C2 ⊂ C1.\n2. Every (3 + )-cluster is a ball around one of its points.\n3. The claim is sharp and the above claims need not hold for = 0.\nProof. We prove the first claim by contradiction and assume that P (C1 \\C2), P (C2 \\ C1), P (C1 ∩ C2) are positive. Let x ∈ C1 ∩ C2, y ∈ C1 ⊕ C2 be such that d(x, y) is as small as possible. Say that y ∈ C2. Clearly, ∆(x,C1 \\ C2) ≥ d(x, y).\n6The main difference is that here we do not consider partitions of the whole sample set. Rather, we seek first those sample points that belong to the exceptional set, and only partitions of the remaining sample points are considered.\n7As in the previous section, it’s a fairly easy matter to accommodate general metric spaces and arbitrary probability distributions.\nWe first deal with the case P (C1 \\C2) ≥ P (C1 ∩C2), and arrive at a contradiction as follows:\n∆(x,C1) = P (C1 \\ C2) P (C1) ∆(x,C1 \\ C2) + P (C1 ∩ C2) P (C1) ∆(x,C1 ∩ C2)\n≥ 1 2 ∆(x,C1 \\ C2) ≥ 1 2 d(x, y) ≥ 1 2 [∆(y, C1)−∆(x,C1)] ≥ 3 + − 1 2 ∆(x,C1)\nWhen P (C1 \\ C2) ≤ P (C1 ∩ C2), a contradiction is reached as follows. By the choice of x, y, for every z ∈ C1 \\ C2, there holds ∆(z, C1 ∩ C2) ≥ d(x, y). Therefore,\n∆(z, C1) = P (C1 \\ C2) P (C1) ∆(z, C1 \\ C2) + P (C1 ∩ C2) P (C1) ∆(z, C1 ∩ C2)\n≥ 1 2 ∆(z, C1 ∩ C2) ≥ 1 2 d(x, y) ≥ 1 2 [∆(y, C1)−∆(x,C1)] ≥ 1 2 · (1− 1 3 + ) ·∆(y, C1) ≥ 1 2 · (1− 1 3 + ) · (3 + ) ·∆(z, C1)\nTo prove the second part, let C be a (3 + )-cluster of diameter r, and let x, y ∈ C satisfy d(x, y) = r. Since d(x, y) ≤ ∆(x,C) + ∆(y, C), we may assume w.l.o.g. that ∆(x,C) ≥ d(x,y)\n2 . We show now that C = B(x, r) and C is a ball, as claimed. Indeed\nd(x, z) ≤ r for every z ∈ C, and if z /∈ C, then d(x, z) ≥ ∆(z, C) − ∆(x,C) ≥ (3 + − 1)∆(x,C) > d(x, y) = r. The conclusion follows.\nTo show that the result is sharp, consider the graph G that is a four-vertex cycle and its graph metric. It is not hard to check that every two consecutive vertices in G constitute a 3-cluster which is not a ball. Moreover a pair of intersecting edges in G yield an example for which the first part of the lemma fails to hold.\nAn (α, γ)-cluster in a space X is called minimal if it contains no (α, γ)-cluster other than itself. Such clusters are of interest, since they can be viewed as “atoms” in clustering X.\nCorollary 3.2 For every α, > 0 and every space X there is at most one partition of X into minimal (α, 3 + )-clusters.\nTo see this, consider two (α, 3 + )-clusters C and C ′ that belong to two different such partitions and have a nonempty intersection. By Lemma 3.1, they must be comparable. By the minimality assumption, C = C ′ which proves the claim.\nNote 3.3 We note that the previous Corollary may fail badly without the minimality assumption. Let X = {x1, . . . , xn}∪̇{y1, . . . , yn}, where d(xi, yi) = 1 for all i and all other distance equal γ. It is not hard to see that the following are (α, γ)-clusters in X where α = 1 2n : A singleton and a pair {xi, yi}. There are 2n = 2 1 2α ways to partition"
    }, {
      "heading" : "X into such clusters.",
      "text" : ""
    }, {
      "heading" : "Algorithmic Aspects",
      "text" : "We next discuss several algorithmic aspects of clustering into arbitrarily many clusters. Our input consists of a finite metric space X and the parameter α > 0. Lemma 3.1 suggests an algorithm for finding (α, 3 + )-clusters and for partitioning the space into (α, 3+ )-clusters. The runtime of this algorithm is polynomial in |X|, and independent of α. The second part of the lemma suggests how to find all the (α, 3 + )-clusters. As the first part of the lemma shows, the inclusion relation among the (α, 3 + )-clusters has a tree structure. Thus, we can use dynamic programming to find a partition of the space into (α, 3 + )-clusters, provided that such a partition exists. This proves the positive part of Theorem 1.2.\nTo match the above positive result, we show\nTheorem 3.4 The following problems are NP-Hard.\n1. (α, 2.5)-CLUSTERING: Given an n-point metric space X and α > 0, decide whether X has a (α, 2.5)-clustering.\n2. PARTITION-INTO-(α, 2.5)-CLUSTERS: Given an n-point metric space X and α > 0, decide whether X has a partition into (α, 2.5)-clusters.\nThe proof of this Theorem, which also proves the negative part of Theorem 1.2, is deferred to the appendix."
    }, {
      "heading" : "4 Conclusion",
      "text" : ""
    }, {
      "heading" : "4.1 Relation to other work",
      "text" : "As we explain below, our work is inspired by the classical VC/PAC theory. In addition we refer to several recent papers that contribute to the development of a theory of clustering."
    }, {
      "heading" : "VC/PAC theory",
      "text" : "The VC/PAC setting offers the following formal description of the classification problem. We are dealing with a space X of instances. The problem is to recover an unknown member h∗ in a known class H of hypotheses. Here H ⊂ YX , where Y is a finite set of labels. We seek to recover the unknown h∗ by observing a sample S = {(xi, h∗(xi)}mi=1 ⊂ X × Y . These samples come from some fixed but unknown distribution over X .\nOur description of the clustering problem is similar. We consider a space X of instances and a class G of good clusterings (P, C) of X, where P is probability measure over X and C is a partition of X. We are given a sample {X1, . . . , Xm} ⊂ X that comes from some unknown P , where (P, C) ∈ G for some partition C, and our purpose is to recover C. Specifically, here X is a metric space, G is the class of probability measures P that admit a partition which is a (α, γ)-clustering and the corresponding partition is the associated (α, γ)-clustering.\nBoth theories seek conditions on G or H under which there are no information theoretic or computational obstacles that keep us from performing the above mentioned tasks."
    }, {
      "heading" : "Alternative Notions of Good Clustering",
      "text" : "Our approach is somewhat close in spirit to [4], see also [6]. These papers assume that the space under consideration has a clustering with some structural properties, and show how to find it efficiently. In particular, a key notion in these papers is the γ-average attraction property, which is conceptually similar to our notion of γclustering. Given a partition C = {C1, . . . , Ck} of a space X it is possible to compare between clusters either additively or through multiplication. In [4] the requirement is that ∆(x,Ci) + γ ≤ ∆(x,Cj) for every x ∈ Ci and j 6= i, whereas our condition is ∆(x,Ci) · γ ≤ ∆(x,Cj). A clear advantage of our notion is its scale invariance. On the other hand, their algorithms work even if X is not a metric space and is only endowed with an arbitrary dissimilarity function.\nWe mention two more papers that share a similar spirit. Consider a data set that resides in the unit ball of a Hilbert Space. It is shown in [8] how to efficiently find a large margin classifier for the data provided that one exists. In [1] several additional possible notions of good clustering are introduced and analyzed."
    }, {
      "heading" : "Stability",
      "text" : "The notion of instance stability was introduced in [5] (See also [3]). An instance for an optimization problem in called stable if the optimal solution does not change (or changes only slightly) upon a small perturbation of the input. The point is made that instances of clustering problems are of practical interest only if they are stable. The notion of an (α, γ)-clustering has a similar stability property. Namely, if we slightly perturb a metric, an (α, γ)-clustering is still (α′, γ′)-clustering for α′ ≈, α, γ′ ≈ γ.\nThus, a good clustering remains a good clustering under a slight perturbation of the input\nIn fact, the present paper is an outgrowth of our work on stable instances for MAXCUT, which we view as a clustering problem. We recall that the input to the MAXCUT problem is an n×n nonnegative symmetric matrix W . We seek an S ⊆ [n] which maximizes ∑ i∈S,j 6∈S wij. Even METRIC-MAXCUT problem (i.e., when wij form a metric) is NP -Hard []. We say that W ′ is a γ-perturbation of W some γ > 1 if ∀i, j, γ− 12wij ≤ w′i,j ≤ γ 1 2wij. The instance W of MAXCUT is called γ-stable if the optimal solution S for W coincides with the optimal solution for every γ-perturbation W ′ of W . The methods presented in this paper can be used to give, for every > 0, an efficient algorithm that correctly solves all (1 + )-stable instances of METRICMAXCUT.\nThese developments will be elaborated in a future publication."
    }, {
      "heading" : "4.2 Future Work and Open Questions",
      "text" : "In view of this article and papers such as [1, 8, 4] it is clear that there is still much interest in new notions of a good clustering and the relevant algorithms. Still, on the subjects discussed here several natural questions remain open.\n1. We believe that it should be possible to improve the dependence on α and γ of the run time of the algortihm in Theorem 1.1.\n2. We gave an efficient method for partitioning a space into 3-clusters, and showed (theorem 3.4) that it is NP -Hard to find a partition into 2.5-clusters. Can this gap be closed?\n3. As Lemma 3.1 shows, (3 + )-clusters are just balls. It is not hard to see that Lemma 2.3 implies that given an (α, γ)-clustering of an n-point metric space, it is possible to find Oγ(log n) representative points in every cluster so that the clustering is nothing but the Voronoi diagram of the (bunched) representative sets. Presumably, there is still some interesting structural theory of (α, γ)-clustering waiting to be discovered here. Specifically, can the above Oγ(log n) be replaced by Oγ(1)? A positive answer would give a deterministic version of our algorithm from section 2, with no dependency of α, but only on the maximal number of clusters.\n4. Consider the following statement “Every n-point metric space X has a partition X = A∪̇B such that for every x ∈ A, y ∈ B, it holds that γ(n)·∆(x,A) ≤ ∆(x,B) and γ(n) ·∆(y,B) ≤ ∆(y, A)”. How large can γ(n) be for this statement to be true?"
    }, {
      "heading" : "A Proofs omitted from the text",
      "text" : "Proof. (of Lemma 2.3) For A ⊂ X, denote IA = 1m |{j : Zj ∈ A}|, JA =∑ j:Zj∈A d(x, Zj). For every j ∈ [m] define\nYj =  1 P (Cp) · d(x, Zj) Zj ∈ Cp − γ− 2 P (Cq) · d(x, Zj) Zj ∈ Cq\n0 otherwise\nWe have EYj = ∆(x,Cp)− (γ− 2) ·∆(x,Cq) ≥ 2γ ·∆(x,Cp). Moreover, by lemma 2.2, |Yj| ≤ (γ− 2 ) α · γ2+1 γ(γ−1) ·∆(x,Cp) ≤ γ2+1 α(γ−1) ·∆(x,Cp). Thus, by Hoeffding’s bound,\nP ( JCp P (Cp) ≤ ( γ − 2 ) · JCq P (Cq) ) = P ( m∑ j=1 Yj ≤ 0 ) ≤ exp ( − ( (γ − 1)α√ 8γ(γ2 + 1) )2 ·m )\nAgain by Hoeffding’s bound, we have\nP\n( ICq\nP (Cq) ≤ 1− 4γ\n) ≤ exp ( − ( α√ 8γ )2 ·m )\nP\n( ICp\nP (Cp) ≥ 1 + 4γ\n) ≤ exp ( − ( α√ 8γ )2 ·m ) Combining the inequalities, we conclude that, with probability ≥ 1 −\n3 exp ( − (\n(γ−1)α√ 8γ(γ2+1)\n)2 ·m ) ,\nJCp ICp\n(γ − )JCq ICq\n=\nJCp P (Cp)\n(γ − 2 ) JCq P (Cq)\n· P (Cp) ICp\nP (Cq)\nICq\n· γ − 2\nγ −\n≥ 1− 4γ\n1 + 4γ\n· γ − 2\nγ − ≥ 1\nProof (of Proposition 2.8) It is very suggestive how to select the exceptional set in the (α− , γ−O( ), )-clustering that we seek. Namely, let N = ∪i (Ci \\ Cγi (A)). As needed, P (N) < , since d(Cγ(A), C) < . To prove our claim, note that ∀i, P (Cγi ) ≥ α − since d(C, Cγ∗ (A)) < . Consider some x ∈ X \\ N and the unique index i for which x ∈ Cγi (A) ∩ Ci. If j 6= i, we need to show that\n∆(x,Cγj (A)) ≥ (γ −O( ))∆(x,C γ i (A))\nAs in the proof of lemma 2.5, we have ∆(x,Cγj (A)) ≥ (\n1− α\n) ∆(x,Cj)−\nα max y∈Cj\\Cγj (A) d(x, y) ≥ (\n1− α · 2γ 2 − γ + 1 γ(γ − 1)\n) ·∆(x,Cj) (3)\n=: (1− a1 · ) ·∆(x,Cj)\nSimilarly, again as in the proof of lemma 2.5, we have ∆(x,Ci) ≥ (\n1− α\n) ∆(x,Cγi (A))−\nα max y∈Cγi (A)\\Ci d(x, y) (4)\nNow, for y ∈ Cγi (A), we have\nd(x, y) ≤ ∆U(x,Ai) + ∆U(y, Ai)\n≤ 1 γ ∆U(x,Aj) + 1 γ ∆U(y, Aj) ≤ 2 γ ∆U(x,Aj) + 1 γ d(x, y)\nNow, since Aj ⊂ Cj, by lemma 2.2, ∆U(x,Aj) ≤ γ 2+1\nγ(γ−1)∆(x,Cj) and we have,\nd(x, y) ≤ γ γ − 1 · γ 2 + 1 γ(γ − 1) · 2 γ ∆(x,Cj)\nSo, by equation (4) we have,\n∆(x,Ci) ≥ (1− a2 · ) ·∆(x,Cγi (A))− a3 · ·∆(x,Cj) (5)\nFor some positive constants a2, a3 which depend only of γ and α. Now by equations (3) and (5) we conclude that\n∆(x,Cγj (A)) ≥ (1− (a1 + γa3) · ) ·∆(x,Cj) + γa3 · ·∆(x,Cj) ≥ (1− (a1 + γa3) · ) · γ ·∆(x,Ci) + γa3 · ·∆(x,Cj) ≥ (1− (a1 + γa3) · )(1− a2 · )γ ·∆(x,Cγi (A)) = (γ −O( )) ·∆(x,Cγi (A))\nProof. (of Theorem 3.4) Both claims are proved by the same reduction from 3- DIMENSIONAL-MATCHING (e.g., [7] pp. 221). The input to this problem is a subset M ⊂ Y ×Z×W , where Y, Z,W are three disjoint q-element sets. A three dimensional matching (=3DM) is a q-element subset M ′ ⊂M that covers all elements in Y ∪̇Z∪̇W . The problem is to decide whether a 3DM exists.\nWe associate with this instance of the problem a graph on vertex set Y ∪̇Z∪̇W , and edge set the union of all triangles {y, z, w} over (y, z, w) ∈ M . It is not hard to see that 3DM remains NP -Hard under the restriction that this graph is connected.\nHere is our reduction. Given an instance M ⊂ Y × Z ×W of 3DM , we construct a graph GM = (V M , EM) as follows: Associated with every m = (y, z, w) ∈ M is a gadget below. We consider the clustering problem on GM with its natural graph metric.\n•y\n•m1 •m2\n•m3 •m4\n•m5 •m6 •m7\n•m8 •m9•z •w We say that a triangle T in a graph is isolated if every vertex outside it has at most one neighbor in T . The above gadget is useful for the reduction since it’s easy to verify that:\nClaim 1 The graph GM can be partitioned into isolated triangles iff M has a 3DM .\nProof(sketch). If M has a 3DM , we can construct a partition of V into isolated triangle by taking the triangles\n{y,m1,m2}, {z,m6,m8}, {w,m7,m9}, {m3,m4,m5} (6)\nfor every m in the 3DM and the triangles\n{m1,m3,m6}, {m2,m4,m7}, {m5,m8,m9} (7)\nform outside it. On the other hand, consider any partition ofGM into isolated triangles. Its restriction to every gadget must coincide with one of the above two choices, so that the corresponding 3DM is readily apparent\nBoth NP -Hardness claims in Theorem 3.4 follow from the above discussion and the following claim\nClaim 2 Let G = (V,E) be a connected graph in which all vertex degrees are ≥ 2. For every partition of the vertex set V = ∪̇k1Ci, the following are equivalent\n1. Each Ci induces an isolated triangle.\n2. Each Ci is a ( 3 |V | , 2.5)-cluster.\n3. The partition C1, . . . , Ck is a ( 3 |V | , 2.5)-clustering.\nProof The implication 1. ⇒ 2. and 1. ⇒ 3. are easily verified. We turn to prove 3. ⇒ 1. Let i ∈ [k]. We need to show that each Ci is an isolated triangle. Clearly, |Ci| ≥ 3 by definition of ( 3|V | , 2.5)-clustering. But G is connected, so there are two neighbors xy with x ∈ Ci, y 6∈ Ci. By proposition 2.2 we have\n1 = d(x, y) ≥ (2.5− 1)∆(x,Ci) ≥ 1.5 · |Ci| − 1 |Ci| ,\nso that |Ci| = 3. Consider now x, y ∈ Ci which are nonadjacent. Since d(x) ≥ 2, it has a neighbor z 6∈ Ci. Using Proposition 2.2 we arrive at the following contradiction: 1 = d(x, z) ≥ (2.5 − 1)∆(x,Ci) ≥ 1.5 · 2+13 = 1.5. We already know that each Ci is a triangle, but why is it isolated? If z ∈ Cj, j 6= i has at least two neighbors in Ci, then\n2.5 ·∆(z, Cj) ≤ ∆(z, Ci) ≤ 4 3 < 2.5 · 2 3 = 2.5 ·∆(z, Cj).\nThe proof of 2.⇒ 1. is similar. Let Ci be cluster in the partition. Using the same argument as before, where the the fact that ∀x ∈ Ci, y /∈ Ci, d(x, y) ≥ ∆(y, Ci) − ∆(y, Ci) ≥ (2.5 − 1)∆(x,Ci) replace Proposition 2.2, we deduce that Ci induces a triangle. To show that Ci is isolated, suppose that there exists a vertex z /∈ Ci with ≥ 2 neighbors in Ci. Let x ∈ Ci be an arbitrary vertex. To obtain a contradiction, we note that\n2.5∆(x,Ci) ≤ ∆(z, Ci) ≤ 4 3 < 2.5 · 2 3 = 2.5∆(x,Ci)\nNote A.1 Theorem 3.4 is tight in the following sense: As the proof shows, the above problems are hard even for graph metrics. On the other hand, given a graph G = (V,E), the following polynomial time algorithms find (i) A partition into (α, 2.5 + )-clusters, and (ii) A (α, 2.5 + )-clustering. (provided, of course, that one exists).\n1. If α > 1|V | then, as in the proof of theorem 3.4, one shows that a partition into\n(α, 2.5 + )-clusters / (α, 2.5 + )-clustering is equivalent to a perfect matching, no edge of which is contained in a triangle. This can be done by first eliminating every edge that belongs to a triangle and then running an arbitrary matching algorithm.\n2. If α < 1|V | then clearly there is no partition into (α, 2.5+ )-clusters / (α, 2.5+ )-\nclustering. If α = 1|V | , the singletons constitute a partition of V into (α, 2.5 + )- clusters and a (α, 2.5 + )-clustering.\nNote A.2 As in Note 2.7, by replacing each vertex with many points at distance from each other, the above reduction applies as well with the definition ∆(x,A) = E[d(x, y)|y ∈ A \\ {x}]."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2012,
    "abstractText" : "Numerous papers ask how difficult it is to cluster data. We suggest that the<lb>more relevant and interesting question is how difficult it is to cluster data sets<lb>that can be clustered well. More generally, despite the ubiquity and the great<lb>importance of clustering, we still do not have a satisfactory mathematical theory<lb>of clustering. In order to properly understand clustering, it is clearly necessary to<lb>develop a solid theoretical basis for the area. For example, from the perspective<lb>of computational complexity theory the clustering problem seems very hard.<lb>Numerous papers introduce various criteria and numerical measures to quantify<lb>the quality of a given clustering. The resulting conclusions are pessimistic, since<lb>it is computationally difficult to find an optimal clustering of a given data set, if<lb>we go by any of these popular criteria. In contrast, the practitioners’ perspective<lb>is much more optimistic. Our explanation for this disparity of opinions is that<lb>complexity theory concentrates on the worst case, whereas in reality we only care<lb>for data sets that can be clustered well.<lb>We introduce a theoretical framework of clustering in metric spaces that<lb>revolves around a notion of ”good clustering”. We show that if a good clustering<lb>exists, then in many cases it can be efficiently found. Our conclusion is that<lb>contrary to popular belief, clustering should not be considered a hard task.<lb>",
    "creator" : "LaTeX with hyperref package"
  }
}