{
  "name" : "1402.0480.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Efficient Gradient-Based Inference through  Transformations between Bayes Nets and Neural Nets",
    "authors" : [ "Diederik P. Kingma", "Max Welling" ],
    "emails" : [ "D.P.KINGMA@UVA.NL", "M.WELLING@UVA.NL" ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "Bayesian networks (also called belief networks) are probabilistic graphical models where the conditional dependencies within a set of random variables are described by a directed acyclic graph (DAG). Many supervised and unsupervised models can be considered as special cases of Bayesian networks.\nIn this paper we focus on the problem of efficient inference in Bayesian networks with multiple layers of continuous latent variables, where exact posterior inference is intractable (e.g. the conditional dependencies between variables are nonlinear) but the joint distribution is differentiable. Algorithms for approximate inference in Bayesian networks can be roughly divided into two categories: sampling approaches and parametric approaches. Parametric approaches include Belief Propagation (Pearl, 1982) or the\nProceedings of the 31 st International Conference on Machine Learning, Beijing, China, 2014. JMLR: W&CP volume 32. Copyright 2014 by the author(s).\nmore recent Expectation Propagation (EP) (Minka, 2001). When it is not reasonable or possible to make assumptions about the posterior (which is often the case), one needs to resort to sampling approaches such as Markov Chain Monte Carlo (MCMC) (Neal, 1993). In high-dimensional spaces, gradient-based samplers such as Hybrid Monte Carlo (Duane et al., 1987) and the recently proposed noU-turn sampler (Hoffman & Gelman, 2011) are known for their relatively fast mixing properties. When just interested in finding a mode of the posterior, vanilla gradient-based optimization methods can be used. The alternative parameterizations suggested in this paper can dramatically improve the efficiency of any of these algorithms."
    }, {
      "heading" : "1.1. Outline of the paper",
      "text" : "After reviewing background material in 2, we introduce a generally applicable differentiable reparameterization of continuous latent variables into a differentiable noncentered form in section 3. In section 4 we analyze the posterior dependencies in this reparameterized form. Experimental results are shown in section 6."
    }, {
      "heading" : "2. Background",
      "text" : "Notation. We use bold lower case (e.g. x or y) notation for random variables and instantiations (values) of random variables. We write pθ(x|y) and pθ(x) to denote (conditional) probability density (PDF) or mass (PMF) functions of variables. With θ we denote the vector containing all parameters; each distribution in the network uses a subset of θ’s elements. Sets of variables are capitalized and bold, matrices are capitalized and bold, and vectors are written in bold and lower case."
    }, {
      "heading" : "2.1. Bayesian networks",
      "text" : "A Bayesian network models a set of random variables V and their conditional dependencies as a directed acyclic graph, where each variable corresponds to a vertex and each edge to a conditional dependency. Let the distribu-\nar X\niv :1\n40 2.\n04 80\nv5 [\ncs .L\nG ]\n2 2\nJa n\n20 15\ntion of each variable vj be pθ(vj |paj), where we condition on vj’s (possibly empty) set of parents paj . Given the factorization property of Bayesian networks, the joint distribution over all variables is simply:\npθ(v1, . . . ,vN ) = N∏ j=1 pθ(vj |paj) (1)\nLet the graph consist of one or more (discrete or continuous) observed variables xj and continuous latent variables zj , with corresponding conditional distributions pθ(xj |paj) and pθ(zj |paj). We focus on the case where both the marginal likelihood pθ(x) = ∫ z pθ(x, z) dz and the posterior pθ(z|x) are intractable to compute or differentiate directly w.r.t. θ (which is true in general), and where the joint distribution pθ(x, z) is at least once differentiable, so it is still possible to efficiently compute first-order partial derivatives ∇θ log pθ(x, z) and∇z log pθ(x, z)."
    }, {
      "heading" : "2.2. Conditionally deterministic variables",
      "text" : "A conditionally deterministic variable vj with parents paj is a variable whose value is a (possibly nonlinear) deterministic function gj(.) of the parents and the parameters: vj = gj(paj ,θ). The PDF of a conditionally deterministic variable is a Dirac delta function, which we define as a Gaussian PDF N (.;µ, σ) with infinitesimal σ:\npθ(vj |paj) = lim σ→0 N (vj ; gj(paj ,θ), σ) (2)\nwhich equals +∞ when vj = gj(paj ,θ) and equals 0 everywhere else such that ∫ vj pθ(vj |paj) dvj = 1."
    }, {
      "heading" : "2.3. Inference problem under consideration",
      "text" : "We are often interested in performing posterior inference, which most frequently consists of either optimization (finding a mode argmaxz pθ(z|x)) or sampling from the posterior pθ(z|x). Gradients of the log-posterior w.r.t. the latent\nvariables can be easily acquired using the equality:\n∇z log pθ(z|x) = ∇z log pθ(x, z)\n= N∑ j=1 ∇z log pθ(vj |paj) (3)\nIn words, the gradient of the log-posterior w.r.t. the latent variables is simply the sum of gradients of individual factors w.r.t. the latent variables. These gradients can then be followed to a mode if one is interested in finding a MAP solution. If one is interested in sampling from the posterior then the gradients can be plugged into a gradient-based sampler such as Hybrid Monte Carlo (Duane et al., 1987); if also interested in learning parameters, the resulting samples can be used for the E-step in Monte Carlo EM (Wei & Tanner, 1990) (MCEM).\nProblems arise when strong posterior dependencies exist between latent variables. From eq. (3) we can see that the Hessian H of the posterior is:\nH = ∇z∇Tz log pθ(z|x) = N∑ j=1 ∇z∇Tz log pθ(vj |paj)\n(4)\nSuppose a factor log pθ(zi|zj) connecting two scalar latent variables zi and zj exists, and zi is strongly dependent on zj , then the Hessian’s corresponding element ∂2 log pθ(z|x) ∂zi∂zj will have a large (positive or negative) value. This is bad for gradient-based inference since it means that changes in zj have a large effect on the gradient\n∂ log pθ(zi|zj) ∂zi\nand changes in zi have a large effect on the gradient ∂ log pθ(zi|zj)\n∂zj . In general, strong conditional dependencies lead to ill-conditioning of the posterior, resulting in smaller optimal stepsizes for first-order gradient-based optimization or sampling methods, making inference less efficient."
    }, {
      "heading" : "3. The differentiable non-centered parameterization (DNCP)",
      "text" : "In this section we introduce a generally applicable transformation between continuous latent random variables and deterministic units with auxiliary parent variables. In rest of the paper we analyze its ramifications for gradient-based inference."
    }, {
      "heading" : "3.1. Parameterizations of latent variables",
      "text" : "Let zj be some continuous latent variable with parents paj , and corresponding conditional PDF:\nzj |paj ∼ pθ(zj |paj) (5) This is also known in the statistics literature as the centered parameterization (CP) of the latent variable zj . Let the\ndifferentiable non-centered parameterization (DNCP) of the latent variable zj be:\nzj = gj(paj , j ,θ) where j ∼ p( j) (6)\nwhere gj(.) is some differentiable function. Note that in the DNCP, the value of zj is deterministic given both paj and the newly introduced auxiliary variable j which is distributed as p( j). See figure 1 for an illustration of the two parameterizations.\nBy the change of variables, the relationship between the original PDF pθ(zj |paj), the function gj(paj , j) and the PDF p( j) is:\np( j) = pθ(zj = gj(paj , j ,θ)|paj) |det(J)| (7)\nwhere det(J) is the determinant of Jacobian of gj(.) w.r.t. j . If zj is a scalar variable, then j is also scalar and |det(J)| = |∂zj∂ j |. In the DNCP, the original latent variable zj has become deterministic, and its PDF pθ(zj |paj , j) can be described as a Dirac delta function (see section 2.2).\nThe joint PDF over the random and deterministic variables can be integrated w.r.t. the determinstic variables. If for simplicity we assume that observed variables are always leaf nodes of the network, and that all latent variables are reparameterized such that the only random variables left are the observed and auxiliary variables x and , then the marginal joint pθ(x, ) is obtained as follows:\npθ(x, ) = ∫ z pθ(x, z, ) dz\n= ∫ z ∏ j pθ(xj |paj) ∏ j pθ(zj |paj , j) ∏ j p( j) dz\n= ∏ j pθ(xj |paj) ∏ j p( j) ∫ z ∏ j pθ(zj |paj , j) dz\n= ∏ j pθ(xj |paj) ∏ j p( j)\nwhere zk = gk(pak, k,θ) (8)\nIn the last step of eq. (8), the inputs paj to the factors of observed variables pθ(xj |paj) are defined in terms of functions zk = gk(.), whose values are all recursively computed from auxiliary variables ."
    }, {
      "heading" : "3.2. Approaches to DNCPs",
      "text" : "There are a few basic approaches to transforming CP of a latent variable zj to a DNCP:\n1. Tractable and differentiable inverse CDF. In this case, let j ∼ U(0, 1), and let gj(zj ,paj ,θ) =\nF−1(zj |paj ;θ) be the inverse CDF of the conditional distribution. Examples: Exponential, Cauchy, Logistic, Rayleigh, Pareto, Weibull, Reciprocal, Gompertz, Gumbel and Erlang distributions.\n2. For any ”location-scale” family of distributions (with differentiable log-PDF) we can choose the standard distribution (with location = 0, scale = 1) as the auxiliary variable j , and let gj(.) = location+ scale · j . Examples: Gaussian, Uniform, Laplace, Elliptical, Student’s t, Logistic and Triangular distributions.\n3. Composition: It is often possible to express variables as functions of component variables with different distributions. Examples: Log-Normal (exponentiation of normally distributed variable), Gamma (a sum over exponentially distributed variables), Beta distribution, Chi-Squared, F distribution and Dirichlet distributions.\nWhen the distribution is not in the families above, accurate differentiable approximations to the inverse CDF can be constructed, e.g. based on polynomials, with time complexity comparable to the CP (see e.g. (Devroye, 1986) for some methods).\nFor the exact approaches above, the CP and DNCP forms have equal time complexities. In practice, the difference in CPU time depends on the relative complexity of computing derivatives of log pθ(zj |paj) versus computing gj(.) and derivatives of log p( j), which can be easily verified to be similar in most cases below. Iterations with the DNCP form were slightly faster in our experiments."
    }, {
      "heading" : "3.3. DNCP and neural networks",
      "text" : "It is instructive to interpret the DNCP form of latent variables as ”hidden units” of a neural network. The network of hidden units together form a neural network with inserted noise , which we can differentiate efficiently using the backpropagation algorithm (Rumelhart et al., 1986).\nThere has been recent increase in popularity of deep neural networks with stochastic hidden units (e.g. (Krizhevsky et al., 2012; Goodfellow et al., 2013; Bengio, 2013)). Often, the parameters θ of such neural networks are optimized towards maximum-likelihood objectives. In that case, the neural network can be interpreted as a probabilistic model log pθ(t|x, ) computing a conditional distribution over some target variable t (e.g. classes) given some input x. In (Bengio & Thibodeau-Laufer, 2013), stochastic hidden units are used for learning the parameters of a Markov chain transition operator that samples from the data distribution.\nFor example, in (Hinton et al., 2012) a ’dropout’ regularization method is introduced where (in its basic ver-\nsion) the activation of hidden units zj is computed as zj = j · f(paj) with j ∼ p( j) = Bernoulli(0.5), and where the parameters are learned by following the gradient of the log-likelihood lower bound: ∇θE [ log pθ(t (i)|x(i), ) ] ; this gradient can sometimes be computed exactly (Maaten et al., 2013) and can otherwise be approximated with a Monte Carlo estimate (Hinton et al., 2012). The two parameterizations explained in section 3.1 offer us a useful new perspective on ’dropout’. A ’dropout’ hidden unit (together with its injected noise ) can be seen as the DNCP of latent random variables, whose CP is zj |paj ∼ pθ(zj = j · f(paj)|paj)). A practical implication is that ’dropout’-type neural networks can therefore be interpreted and treated as hierarchical Bayes nets, which opens the door to alternative approaches to learning the parameters, such as Monte Carlo EM or variational methods.\nWhile ’dropout’ is designed as a regularization method, other work on stochastic neural networks exploit the power of stochastic hidden units for generative modeling, e.g. (Frey & Hinton, 1999; Rezende et al., 2014; Tang & Salakhutdinov, 2013) applying (partially) MCMC or (partically) factorized variational approaches to modelling the posterior. As we will see in sections 4 and 6, the choice of parameterization has a large impact on the posterior dependencies and the efficiency of posterior inference. However, current publications lack a good justification for their choice of parameterization. The analysis in section 4 offers some important insight in where the centered or noncentered parameterizations of such networks are more appropriate."
    }, {
      "heading" : "3.4. A differentiable MC likelihood estimator",
      "text" : "We showed that many hierarchical continuous latentvariable models can be transformed into a DNCP pθ(x, ), where all latent variables (the introduced auxiliary variables ) are root nodes (see eq. (8)). This has an important implication for learning since (contrary to a CP) the DNCP can be used to form a differentiable Monte Carlo estimator of the marginal likelihood:\nlog pθ(x) ' log 1\nL L∑ l=1 ∏ j pθ(xj |pa(l)j )\nwhere the parents pa(l)j of the observed variables are either root nodes or functions of root nodes whose values are sampled from their marginal: (l) ∼ p( ). This MC estimator can be differentiated w.r.t. θ to obtain an MC estimate of the log-likelihood gradient ∇θ log pθ(x), which can be plugged into stochastic optimization methods such as Adagrad for approximate ML or MAP. When performed one datapoint at a time, we arrive at our on-line Maximum Monte Carlo Likelihood (MMCL) algorithm."
    }, {
      "heading" : "4. Effects of parameterizations on posterior dependencies",
      "text" : "What is the effect of the proposed reparameterization on the efficiency of inference? If the latent variables have linearGaussian conditional distributions, we can use the metric of squared correlation between the latent variable and any of its children in their posterior distribution. If after reparameterization the squared correlation is decreased, then in general this will also result in more efficient inference.\nFor non-linear Gaussian conditional distributions, the logPDF can be locally approximated as a linear-Gaussian using a second-order Taylor expansion. Results derived for the linear case can therefore also be applied to the nonlinear case; the correlation computed using this approximation is a local dependency between the two variables.\nDenote by z a scalar latent variable we are going to reparameterize, and by y its parents, where yi is one of the parents. The log-PDF of the corresponding conditional dis-\ntribution is\nlog pθ(z|y) = logN (z|wTy + b, σ2) = −(z −wTy − b)2/(2σ2) + constant\nA reparameterization of z using an auxiliary variable is z = g(.) = (wTy + b) + σ where ∼ N (0, 1). With (7) it can be confirmed that this change of variables is correct:\npθ(z|y) · ∣∣∣∣∂z∂ ∣∣∣∣ = pθ(z = g(.)|y) · ∣∣∣∣∂z∂ ∣∣∣∣\n= N (wTy + b+ σ |wTy + b, σ2) · σz = − exp( 2/2)/ √ 2π = N (0, 1)\n= p( ) (9)\nFirst we will derive expressions for the squared correlations between z and its parents, for the CP and DNCP case, and subsequently show how they relate.\nThe covarianceC between two jointly Gaussian distributed variables A and B equals the negative inverse of the Hessian matrix of the log-joint PDF:\nC =\n( σ2A σ 2 AB\nσ2AB σ 2 B\n) = −H−1 = 1\ndet(H) ( −HB HAB HAB −HA ) The correlation ρ between two jointly Gaussian distributed variables A and B is given by: ρ = σ2AB/(σAσB). Using\nthe equation above, the squared correlation can be computed from the elements of the Hessian matrix:\nρ2 = (σ2AB) 2/(σ2Aσ 2 B)\n= (HAB/det(H)) 2/((−HA/det(H))(−HB/det(H))\n= H2AB/(HAHB) (10)\nImportant to note is that derivatives of the log-posterior w.r.t. the latent variables are equal to the derivatives of logjoint w.r.t. the latent variables, therefore,"
    }, {
      "heading" : "H = ∇z∇Tz log pθ(z|x) = ∇z∇Tz log pθ(x, z)",
      "text" : "The following shorthand notation is used in this section:\nL = log pθ(x, z) (sum of all factors) z = the variable to be reparameterized y = z’s parents\nL(z) = log pθ(z|y) (z’s factor) L(\\z) = L− L(z) (all factors minus z’s factor) L(z→) = the factors of z’s children\nα = ∂2L(\\z)\n∂yi∂yi\nβ = ∂2L(z→)\n∂z∂z"
    }, {
      "heading" : "4.1. Squared Correlations",
      "text" : ""
    }, {
      "heading" : "4.1.1. CENTERED CASE",
      "text" : "In the CP case, the relevant Hessian elements are as follows:\nHyiyi = ∂2L\n∂yi∂yi = α+\n∂2L(z) ∂yi∂yi = α− w2i /σ2\nHzz = ∂2L\n∂z∂z = β +\n∂2L(z)\n∂z∂z = β − 1/σ2\nHyiz = ∂2L ∂yi∂z = ∂2L(z) ∂yi∂z = wi/σ 2 (11)\nTherefore, using eq. (10), the squared correlation between yi and z is:\nρ2yi,z = (Hyiz)\n2\nHyiyiHzz =\nw2i /σ 4\n(α− w2i /σ2)(β − 1/σ2) (12)"
    }, {
      "heading" : "4.1.2. NON-CENTERED CASE",
      "text" : "In the DNCP case, the Hessian elements are:\nHyiyi = ∂2L\n∂yi∂yi = α+\n∂\n∂yi\n∂L(z→)\n∂yi\n= α+ ∂\n∂yi\n( wi ∂L(z→)\n∂z\n) = α+ w2i β\nH = ∂2L ∂ ∂ = ∂2L(z→) ∂ ∂ + ∂2 log p( ) ∂ ∂ = σ2β − 1\nHyi = ∂2L\n∂yi∂ = σwiβ (13)\nThe squared correlation between yi and is therefore:\nρ2yi, = (Hyi )\n2\nHyiyiH =\nσ2w2i β 2\n(α+ w2i β)(σ 2β − 1) (14)"
    }, {
      "heading" : "4.2. Correlation inequality",
      "text" : "We can now compare the squared correlation, between z and some parent yi, before and after the reparameterization. Assuming α < 0 and β < 0 (i.e. L(\\z) and L(z→) are concave, e.g. exponential families):\nρ2yi,z > ρ 2 yi,\nw2i /σ 4\n(α− w2i /σ2)(β − 1/σ2) >\nσ2w2i β 2\n(α+ w2i β)(σ 2β − 1)\nw2i /σ 4\n(α− w2i /σ2)(β − 1/σ2) >\nw2i β 2\n(α+ w2i β)(β − 1/σ2) 1/σ4\n(α− w2i /σ2) >\nβ2\n(α+ w2i β)\nσ−2 > −β (15)\nThus we have shown the surprising fact that the correlation inequality takes on an extremely simple form where the parent-dependent values α and wi play no role; the inequality only depends on two properties of z: the relative strenghts of σ (its noisiness) and β (its influence on children’s factors). Informally speaking, if the noisiness of z’s conditional distribution is large enough compared to other factors’ dependencies on z, then the reparameterized form is beneficial for inference."
    }, {
      "heading" : "4.3. A beauty-and-beast pair",
      "text" : "Additional insight into the properties of the CP and DNCP can be gained by taking the limits of the squared correlations (12) and (14). Limiting behaviour of these correlations is shown in table 1. As becomes clear in these limits, the CP and DNCP often form a beauty-and-beast pair: when posterior correlations are high in one parameterization, they are low in the other. This is especially true in the\nlimits of σ → 0 and β → −∞, where squared correlations converge to either 0 or 1, such that posterior inference will be extremely inefficient in either CP or DNCP, but efficient in the other. This difference in shapes of the log-posterior is illustrated in figure 3."
    }, {
      "heading" : "4.4. Example: Simple Linear Dynamical System",
      "text" : "Take a simple model with scalar latent variables z1 and z2, and scalar observed variables x1 and x2. The joint PDF is defined as p(x1, x2, z1, z2) = p(z1)p(x1|z1)p(z2|z1)p(x2|z2), where p(z1) = N (0, 1), p(x1|z1) = N (z1, σ2x), p(z2|z1) = N (z1, σ2z) and p(x2|z2) = N (z2, σ2x). Note that the parameter σz determines the dependency between the latent variables, and σx determines the dependency between latent and observed variables.\nWe reparameterize z2 such that it is conditionally deterministic given a new auxiliary variable 2. Let p( 2) = N (0, 1). let z2 = g2(z1, 2, σz) = z1 + σz · 2 and let 1 = z1. See figure 3 for plots of the original and auxiliary posterior log-PDFs, for different choices of σz , along with the resulting posterior correlation ρ.\nFor what choice of parameters does the reparameterization yield smaller posterior correlation? We use equation (15) and plug in σ ← σz and −β ← σ−2x , which results in:\nρ2z1,z2 > ρ 2 1, 2 ⇒ σ2z < σ2x\ni.e. the posterior correlation in DNCP form ρ2 1, 2 is smaller when the latent-variable noise parameter σ2z is smaller than the oberved-variable noise parameter σ2x. Less formally, this means that the DNCP is preferred when the latent variable is more strongly coupled to the data (likelihood) then to its parents."
    }, {
      "heading" : "5. Related work",
      "text" : "This is, to the best of our knowledge, the first work to investigate the implications of the different differentiable noncentered parameterizations on the efficiency of gradientbased inference. However, the topic of centered vs noncentered parameterizations has been investigated for efficient (non-gradient based) Gibbs Sampling in work by Papaspiliopoulos et al. (2003; 2007), which also discusses some strategies for constructing parameterization for those cases. There have been some publications for parameterizations of specific models; (Gelfand et al., 1995), for example, discusses parameterizations of mixed models, and (Meng & Van Dyk, 1998) investigate several rules for choosing an appropriate parameterization for mixed-effects models for faster EM. In the special case where Gibbs sam-\npling is tractable, efficient sampling is possible by interleaving between centered and non-centered parameterizations, as was shown in (Yu & Meng, 2011).\nAuxiliary variables are used for data augmentation (see (Van Dyk & Meng, 2001) or slice sampling (Neal, 2003)) where, in contrast with our method, sampling is performed in a higher-dimensional augmented space. Auxiliary variables are used in a similar form under the name exogenous variables in Structural Causal Models (SCMs) (Pearl, 2000). In SCMs the functional form of exogenous variables is more restricted than our auxiliary variables. The concept of conditionally deterministic variables has been used earlier in e.g. (Cobb & Shenoy, 2005), although not as a tool for efficient inference in general Bayesian networks with continuous latent variables. Recently, (Raiko et al., 2012) analyzed the elements of the Hessian w.r.t. the parameters in neural network context.\nThe differentiable reparameterization of latent variables in this paper was introduced earlier in (Kingma, 2013) and independently in (Bengio, 2013), but these publications lack a theoretic analysis of the impact on the efficiency of inference. In (Kingma & Welling, 2013), the reparameterization trick was used in an efficient algorithm for stochastic variational inference and learning."
    }, {
      "heading" : "6. Experiments",
      "text" : ""
    }, {
      "heading" : "6.1. Nonlinear DBN",
      "text" : "From the derived posterior correlations in the previous sections we can conclude that depending on the parameters of the model, posterior sampling can be extremely inefficient in one parameterization while it is efficient in the other. When the parameters are known, one can choose the best parameterization (w.r.t. posterior correlations) based on the correlation inequality (15).\nIn practice, model parameters are often subject to change, e.g. when optimizing the parameters with Monte Carlo EM; in these situations where there is uncertainty over the value of the model parameters, it is impossible to choose the best parameterization in advance. The ”beauty-beast” duality from section 4.3 suggests a solution in the form of a very simple sampling strategy: mix the two parameterizations. Let QCP (z′|z) be the MCMC/HMC proposal distribution based on pθ(z|x) (the CP), and let QDNCP (z′|z) be the proposal distribution based on pθ( |x) (the DNCP). Then the new MCMC proposal distribution based on the mixture is:\nQ(z′|z) = ρ ·QCP (z′|z) + (1− ρ) ·QDNCP (z′|z) (16)\nwhere we use ρ = 0.5 in experiments. The mixing efficiency might be half that of the oracle solution (where the\noptimal parameterization is known), nonetheless when taking into account the uncertainty over the parameters, the expected efficiency of the mixture proposal can be better than a single parameterization chosen ad hoc.\nWe applied a Hybrid Monte Carlo (HMC) sampler to a Dynamic Bayesian Network (DBN) with nonlinear transition probabilities with the same structure as the illustrative model in figure 2. The prior and conditional probabilities are: z1 ∼ N (0, I), zt|zt−1 ∼ N (tanh(Wzzt−1 + bz), σ 2 zI) and xt|zt ∼ Bernoulli(sigmoid(Wxzt−1)). The parameters were intialized randomly by sampling from N (0, I). Based on the derived limiting behaviour (see table 1, we can expect that such a network in CP can have very large posterior correlations if the variance of the latent variables σ2z is very small, resulting in slow sampling.\nTo validate this result, we performed HMC inference with different values of σ2z , sampling the latent variables while holding the parameters fixed. For HMC we used 10 leapfrog steps per sample, and the stepsize was automatically adjusted while sampling to obtain a HMC acceptance rate of around 0.9. At each sampling run, the first 1000 HMC samples were thrown away (burn-in); the subsequent 4000 HMC samples were kept. To estimate the efficiency of sampling, we computed the effective sample size (ESS); see e.g. (Kass et al., 1998) for a discussion on ESS.\nResults. See table 2 and figure 4 for results. It is clear that the choice of parameterization has a large effect on posterior dependencies and the efficiency of inference. Sampling was very inefficient for small values of σz in the CP, which can be understood from the limiting behaviour in table 1."
    }, {
      "heading" : "6.2. Generative multilayer neural net",
      "text" : "As explained in section 3.4, a hierarchical model in DNCP form can be learned using a MC likelihood estimator which can be differentiated and optimized w.r.t. the parameters θ. We compare this Maximum Monte Carlo Likelihood (MMCL) method with the MCEM method for learning the parameters of a 4-layer hierarchical model of the MNIST dataset, where x|z3 ∼ Bernoulli(sigmoid(Wxz3 + bx)) and zt|zt−1 ∼ N (tanh(Wizt−1+bi), σ2ztI). For MCEM, we used HMC with 10 leapfrog steps followed by a weight update using Adagrad (Duchi et al., 2010). For MMCL, we used L ∈ {10, 100, 500}. We observed that DNCP was a better parameterization than CP in this case, in terms of fast mixing. However, even in the DNCP, HMC mixed very slowly when the dimensionality of latent space become too high. For this reason, z1 and z2 were given a dimensionality of 3, while z3 was 100-dimensional but noiseless (σ2z1 = 0) such that only z3 and z2 are random variables that require posterior inference by sampling. The model was trained on a small (1000 datapoints) and large (50000 datapoints) version of the MNIST dataset.\nResults. We compared train- and testset marginal likelihood. See figure 5 for experimental results. As was expected, MCEM attains asymptotically better results. However, despite its simplicity, the on-line nature of MMCL means it scales better to large datasets, and (contrary to MCEM) is trivial to implement."
    }, {
      "heading" : "7. Conclusion",
      "text" : "We have shown how Bayesian networks with continuous latent variables and generative neural networks are related through two different parameterizations of the latent variables: CP and DNCP. A key result is that the differentiable non-centered parameterization (DNCP) of a latent variable is preferred, in terms of its effect on decreased posterior correlations, when the variable is more strongly linked to its parents than its children. Through theoretical analysis we have also shown that the two parameterizations are complementary to each other: when posterior correlations are large in one form, they are small in the other. We have also illustrated that this theoretical result can be exploited in practice by designing a MCMC strategy that mixes between both parameterizations, making it robust to situations where MCMC can otherwise be inefficient."
    }, {
      "heading" : "Acknowledgments",
      "text" : "The authors thank the reviewers for their excellent feedback and Joris Mooij, Ted Meeds and Taco Cohen for invaluable discussions and input."
    } ],
    "references" : [ {
      "title" : "Estimating or propagating gradients through stochastic neurons",
      "author" : [ "Bengio", "Yoshua" ],
      "venue" : "arXiv preprint arXiv:1305.2982,",
      "citeRegEx" : "Bengio and Yoshua.,? \\Q2013\\E",
      "shortCiteRegEx" : "Bengio and Yoshua.",
      "year" : 2013
    }, {
      "title" : "Deep generative stochastic networks trainable by backprop",
      "author" : [ "Bengio", "Yoshua", "Thibodeau-Laufer", "Éric" ],
      "venue" : "arXiv preprint arXiv:1306.1091,",
      "citeRegEx" : "Bengio et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Bengio et al\\.",
      "year" : 2013
    }, {
      "title" : "Nonlinear deterministic relationships in Bayesian networks. In Symbolic and Quantitative Approaches to Reasoning with Uncertainty",
      "author" : [ "Cobb", "Barry R", "Shenoy", "Prakash P" ],
      "venue" : null,
      "citeRegEx" : "Cobb et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Cobb et al\\.",
      "year" : 2005
    }, {
      "title" : "Sample-based non-uniform random variate generation",
      "author" : [ "Devroye", "Luc" ],
      "venue" : "In Proceedings of the 18th conference on Winter simulation,",
      "citeRegEx" : "Devroye and Luc.,? \\Q1986\\E",
      "shortCiteRegEx" : "Devroye and Luc.",
      "year" : 1986
    }, {
      "title" : "Hybrid Monte Carlo",
      "author" : [ "Duane", "Simon", "Kennedy", "Anthony D", "Pendleton", "Brian J", "Roweth", "Duncan" ],
      "venue" : "Physics letters B,",
      "citeRegEx" : "Duane et al\\.,? \\Q1987\\E",
      "shortCiteRegEx" : "Duane et al\\.",
      "year" : 1987
    }, {
      "title" : "Adaptive subgradient methods for online learning and stochastic optimization",
      "author" : [ "Duchi", "John", "Hazan", "Elad", "Singer", "Yoram" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Duchi et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Duchi et al\\.",
      "year" : 2010
    }, {
      "title" : "Variational learning in nonlinear Gaussian belief networks",
      "author" : [ "Frey", "Brendan J", "Hinton", "Geoffrey E" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "Frey et al\\.,? \\Q1999\\E",
      "shortCiteRegEx" : "Frey et al\\.",
      "year" : 1999
    }, {
      "title" : "Efficient parameterisations for normal linear mixed models",
      "author" : [ "AE Gelfand", "SK Sahu", "Carlin", "BP" ],
      "venue" : null,
      "citeRegEx" : "Gelfand et al\\.,? \\Q1995\\E",
      "shortCiteRegEx" : "Gelfand et al\\.",
      "year" : 1995
    }, {
      "title" : "Improving neural networks by preventing co-adaptation of feature detectors",
      "author" : [ "Hinton", "Geoffrey E", "Srivastava", "Nitish", "Krizhevsky", "Alex", "Sutskever", "Ilya", "Salakhutdinov", "Ruslan R" ],
      "venue" : "arXiv preprint arXiv:1207.0580,",
      "citeRegEx" : "Hinton et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Hinton et al\\.",
      "year" : 2012
    }, {
      "title" : "The no-U-turn sampler: Adaptively setting path lengths in Hamiltonian Monte Carlo",
      "author" : [ "Hoffman", "Matthew D", "Gelman", "Andrew" ],
      "venue" : "arXiv preprint arXiv:1111.4246,",
      "citeRegEx" : "Hoffman et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Hoffman et al\\.",
      "year" : 2011
    }, {
      "title" : "Markov chain Monte Carlo in practice: A roundtable discussion",
      "author" : [ "Kass", "Robert E", "Carlin", "Bradley P", "Gelman", "Andrew", "Neal", "Radford M" ],
      "venue" : "The American Statistician,",
      "citeRegEx" : "Kass et al\\.,? \\Q1998\\E",
      "shortCiteRegEx" : "Kass et al\\.",
      "year" : 1998
    }, {
      "title" : "Fast gradient-based inference with continuous latent variable models in auxiliary form",
      "author" : [ "Kingma", "Diederik P" ],
      "venue" : "arXiv preprint arXiv:1306.0733,",
      "citeRegEx" : "Kingma and P.,? \\Q2013\\E",
      "shortCiteRegEx" : "Kingma and P.",
      "year" : 2013
    }, {
      "title" : "Auto-Encoding Variational Bayes",
      "author" : [ "Kingma", "Diederik P", "Welling", "Max" ],
      "venue" : "arXiv preprint arXiv:1312.6114,",
      "citeRegEx" : "Kingma et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Kingma et al\\.",
      "year" : 2013
    }, {
      "title" : "Imagenet classification with deep convolutional neural networks",
      "author" : [ "Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoff" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Krizhevsky et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Krizhevsky et al\\.",
      "year" : 2012
    }, {
      "title" : "Learning with marginalized corrupted features",
      "author" : [ "Maaten", "Laurens", "Chen", "Minmin", "Tyree", "Stephen", "Weinberger", "Kilian Q" ],
      "venue" : "In Proceedings of the 30th International Conference on Machine Learning",
      "citeRegEx" : "Maaten et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Maaten et al\\.",
      "year" : 2013
    }, {
      "title" : "Fast EM-type implementations for mixed effects models",
      "author" : [ "Meng", "X-L", "Van Dyk", "David" ],
      "venue" : "Journal of the Royal Statistical Society: Series B (Statistical Methodology),",
      "citeRegEx" : "Meng et al\\.,? \\Q1998\\E",
      "shortCiteRegEx" : "Meng et al\\.",
      "year" : 1998
    }, {
      "title" : "Expectation propagation for approximate bayesian inference",
      "author" : [ "Minka", "Thomas P" ],
      "venue" : "In Proceedings of the Seventeenth conference on Uncertainty in artificial intelligence,",
      "citeRegEx" : "Minka and P.,? \\Q2001\\E",
      "shortCiteRegEx" : "Minka and P.",
      "year" : 2001
    }, {
      "title" : "Probabilistic inference using Markov Chain Monte Carlo methods",
      "author" : [ "Neal", "Radford M" ],
      "venue" : null,
      "citeRegEx" : "Neal and M.,? \\Q1993\\E",
      "shortCiteRegEx" : "Neal and M.",
      "year" : 1993
    }, {
      "title" : "Non-centered parameterisations for hierarchical models and data augmentation",
      "author" : [ "Papaspiliopoulos", "Omiros", "Roberts", "Gareth O", "Sköld", "Martin" ],
      "venue" : "In Bayesian Statistics 7: Proceedings of the Seventh Valencia International Meeting,",
      "citeRegEx" : "Papaspiliopoulos et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Papaspiliopoulos et al\\.",
      "year" : 2003
    }, {
      "title" : "A general framework for the parametrization of hierarchical models",
      "author" : [ "Papaspiliopoulos", "Omiros", "Roberts", "Gareth O", "Sköld", "Martin" ],
      "venue" : "Statistical Science,",
      "citeRegEx" : "Papaspiliopoulos et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Papaspiliopoulos et al\\.",
      "year" : 2007
    }, {
      "title" : "Reverend Bayes on inference engines: A distributed hierarchical approach",
      "author" : [ "Pearl", "Judea" ],
      "venue" : "Cognitive Systems Laboratory, School of Engineering and Applied Science,",
      "citeRegEx" : "Pearl and Judea.,? \\Q1982\\E",
      "shortCiteRegEx" : "Pearl and Judea.",
      "year" : 1982
    }, {
      "title" : "Causality: models, reasoning and inference, volume 29",
      "author" : [ "Pearl", "Judea" ],
      "venue" : null,
      "citeRegEx" : "Pearl and Judea.,? \\Q2000\\E",
      "shortCiteRegEx" : "Pearl and Judea.",
      "year" : 2000
    }, {
      "title" : "Deep learning made easier by linear transformations in perceptrons",
      "author" : [ "Raiko", "Tapani", "Valpola", "Harri", "LeCun", "Yann" ],
      "venue" : "In International Conference on Artificial Intelligence and Statistics,",
      "citeRegEx" : "Raiko et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Raiko et al\\.",
      "year" : 2012
    }, {
      "title" : "Stochastic back-propagation and variational inference in deep latent gaussian models",
      "author" : [ "Rezende", "Danilo Jimenez", "Mohamed", "Shakir", "Wierstra", "Daan" ],
      "venue" : "arXiv preprint arXiv:1401.4082,",
      "citeRegEx" : "Rezende et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Rezende et al\\.",
      "year" : 2014
    }, {
      "title" : "Learning stochastic feedforward neural networks",
      "author" : [ "Tang", "Yichuan", "Salakhutdinov", "Ruslan" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Tang et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Tang et al\\.",
      "year" : 2013
    }, {
      "title" : "The art of data augmentation",
      "author" : [ "Van Dyk", "David A", "Meng", "Xiao-Li" ],
      "venue" : "Journal of Computational and Graphical Statistics,",
      "citeRegEx" : "Dyk et al\\.,? \\Q2001\\E",
      "shortCiteRegEx" : "Dyk et al\\.",
      "year" : 2001
    }, {
      "title" : "A Monte Carlo implementation of the EM algorithm and the poor man’s data augmentation algorithms",
      "author" : [ "Wei", "Greg CG", "Tanner", "Martin A" ],
      "venue" : "Journal of the American Statistical Association,",
      "citeRegEx" : "Wei et al\\.,? \\Q1990\\E",
      "shortCiteRegEx" : "Wei et al\\.",
      "year" : 1990
    }, {
      "title" : "To Center or Not to Center: That Is Not the Question–An Ancillarity-Sufficiency Interweaving Strategy (ASIS) for Boosting MCMC Efficiency",
      "author" : [ "Yu", "Yaming", "Meng", "Xiao-Li" ],
      "venue" : "Journal of Computational and Graphical Statistics,",
      "citeRegEx" : "Yu et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Yu et al\\.",
      "year" : 2011
    } ],
    "referenceMentions" : [ {
      "referenceID" : 4,
      "context" : "In high-dimensional spaces, gradient-based samplers such as Hybrid Monte Carlo (Duane et al., 1987) and the recently proposed noU-turn sampler (Hoffman & Gelman, 2011) are known for their relatively fast mixing properties.",
      "startOffset" : 79,
      "endOffset" : 99
    }, {
      "referenceID" : 4,
      "context" : "If one is interested in sampling from the posterior then the gradients can be plugged into a gradient-based sampler such as Hybrid Monte Carlo (Duane et al., 1987); if also interested in learning parameters, the resulting samples can be used for the E-step in Monte Carlo EM (Wei & Tanner, 1990) (MCEM).",
      "startOffset" : 143,
      "endOffset" : 163
    }, {
      "referenceID" : 13,
      "context" : "(Krizhevsky et al., 2012; Goodfellow et al., 2013; Bengio, 2013)).",
      "startOffset" : 0,
      "endOffset" : 64
    }, {
      "referenceID" : 8,
      "context" : "For example, in (Hinton et al., 2012) a ’dropout’ regularization method is introduced where (in its basic ver-",
      "startOffset" : 16,
      "endOffset" : 37
    }, {
      "referenceID" : 14,
      "context" : "5), and where the parameters are learned by following the gradient of the log-likelihood lower bound: ∇θE [ log pθ(t (i)|x(i), ) ] ; this gradient can sometimes be computed exactly (Maaten et al., 2013) and can otherwise be approximated with a Monte Carlo estimate (Hinton et al.",
      "startOffset" : 181,
      "endOffset" : 202
    }, {
      "referenceID" : 8,
      "context" : ", 2013) and can otherwise be approximated with a Monte Carlo estimate (Hinton et al., 2012).",
      "startOffset" : 70,
      "endOffset" : 91
    }, {
      "referenceID" : 23,
      "context" : "(Frey & Hinton, 1999; Rezende et al., 2014; Tang & Salakhutdinov, 2013) applying (partially) MCMC or (partically) factorized variational approaches to modelling the posterior.",
      "startOffset" : 0,
      "endOffset" : 71
    }, {
      "referenceID" : 7,
      "context" : "There have been some publications for parameterizations of specific models; (Gelfand et al., 1995), for example, discusses parameterizations of mixed models, and (Meng & Van Dyk, 1998) investigate several rules for choosing an appropriate parameterization for mixed-effects models for faster EM.",
      "startOffset" : 76,
      "endOffset" : 98
    }, {
      "referenceID" : 22,
      "context" : "Recently, (Raiko et al., 2012) analyzed the elements of the Hessian w.",
      "startOffset" : 10,
      "endOffset" : 30
    }, {
      "referenceID" : 10,
      "context" : "(Kass et al., 1998) for a discussion on ESS.",
      "startOffset" : 0,
      "endOffset" : 19
    }, {
      "referenceID" : 5,
      "context" : "For MCEM, we used HMC with 10 leapfrog steps followed by a weight update using Adagrad (Duchi et al., 2010).",
      "startOffset" : 87,
      "endOffset" : 107
    } ],
    "year" : 2015,
    "abstractText" : "Hierarchical Bayesian networks and neural networks with stochastic hidden units are commonly perceived as two separate types of models. We show that either of these types of models can often be transformed into an instance of the other, by switching between centered and differentiable non-centered parameterizations of the latent variables. The choice of parameterization greatly influences the efficiency of gradient-based posterior inference; we show that they are often complementary to eachother, we clarify when each parameterization is preferred and show how inference can be made robust. In the non-centered form, a simple Monte Carlo estimator of the marginal likelihood can be used for learning the parameters. Theoretical results are supported by experiments.",
    "creator" : "LaTeX with hyperref package"
  }
}