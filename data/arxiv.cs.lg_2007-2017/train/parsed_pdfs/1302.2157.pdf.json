{
  "name" : "1302.2157.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Passive Learning with Target Risk",
    "authors" : [ "Mehrdad Mahdavi", "Rong Jin" ],
    "emails" : [ "mahdavim@cse.msu.edu", "rongjin@cse.msu.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n30 2.\n21 57\nv2 [\ncs .L\nG ]\n1 9\n( 1\nǫ\n) ), an exponential improvement compared to\nthe sample complexity O( 1 ǫ ) for learning with strongly convex loss functions. Furthermore, our proof is constructive and is based on a computationally efficient stochastic optimization algorithm for such settings which demonstrate that the proposed algorithm is practically useful."
    }, {
      "heading" : "1 Introduction",
      "text" : "In the standard passive supervised learning setting, the learning algorithm is given a set of labeled examples S = ((x1, y1), · · · , (xn, yn)) drawn i.i.d. from a fixed but unknown distribution D. The goal, with the help of labeled examples, is to output a classifier h from a predefined hypothesis class H that does well on unseen examples coming from the same distribution. The sample complexity of an algorithm is the number of examples which is sufficient to ensure that, with probability at least 1− δ (w.r.t. the random choice of S), the algorithm picks a hypothesis with an error that is at most ǫ from the optimal one. Sample complexity of passive learning is well established and goes back to early works in the learning theory where the lower bounds Ω ( 1 ǫ (log 1 ǫ + log 1 δ ) ) and Ω ( 1 ǫ2 (log 1 ǫ + log 1 δ ) ) were obtained in classic PAC and general agnostic PAC settings, respectively [9, 5, 1]. In light of no free lunch theorem, learning is impossible unless we make assumptions regarding the nature of the problem at hand. Therefore, when approaching a particular learning problem, it is desirable to take into account some prior knowledge we might have about our problem and use a specialized algorithm that exploits this knowledge into a learning process or theoretical analysis. A key issue in this regard is the formalization of prior knowledge. Such prior knowledge can be expressed by restricting our hypothesis class, making assumptions on the nature of unknown distribution D or formalization of the data space, analytical\nproperties of the loss function being used to evaluate the performance, sparsity, and margin– to name a few.\nThere has been an upsurge of interest over the last decade in finding tight upper bounds on the sample complexity by utilizing prior knowledge on the analytical properties of the loss function, that led to stronger generalization bounds in agnostic PAC setting. In [17] fast rates obtained for squared loss, exploiting the strong convexity of this loss function, which only holds under pseudo-dimensionality assumption. With the recent development in online strongly convex optimization [11], fast rates approaching O(1\nǫ log 1 δ ) for convex Lipschitz\nstrongly convex loss functions has been obtained in [29, 15]. For smooth non-negative loss functions, [27] improved the sample complexity to optimistic rates\nO ( 1\nǫ\n( ǫopt + ǫ\nǫ\n)( log3 1\nǫ + log\n1\nδ\n))\nfor non-parametric learning using the notion of local Rademacher complexity [3], where ǫopt is the optimal risk.\nIn this work, we consider a slightly different setup for passive learning. We assume that before the start of the learning process, the learner has in mind a target expected loss, also referred to as target risk, denoted by ǫprior\n1, and tries to learn a classifier with the expected risk of O(ǫprior) by labeling a small number of training examples. We further assume the target risk ǫprior is feasible, i.e., ǫprior ≥ ǫopt. To address this problem, we develop an efficient algorithm, based on stochastic optimization, for passive learning with target risk. The most surprising property of the proposed algorithm is that when the loss function is both smooth and strongly convex, it only needs O(d log(1/ǫprior)) labeled examples to find a classifier with the expected risk of O(ǫprior), where d is the dimension of data. This is a significant improvement compared to the sample complexity for empirical risk minimization.\nThe key intuition behind our algorithm is that by knowing target risk as prior knowledge, the learner has better control over the variance in stochastic gradients, which contributes mostly to the slow convergence in stochastic optimization and consequentially large sample complexity in passive learning. The trick is to run the stochastic optimization in multistages with a fixed size and decrease the variance of stochastically perturbed gradients at each iteration by a properly designed mechanism. Another crucial feature of the proposed algorithm is to utilize the target risk ǫprior to gradually refine the hypothesis space as the algorithm proceeds. Our algorithm differs significantly from standard stochastic optimization algorithms and is able to achieve a geometric convergence rate with the knowledge of target risk ǫprior.\nWe note that our work does not contradict the lower bound in [27] because a feasible target risk ǫprior is given in our learning setup and is fully exploited by the proposed algorithm. Knowing that the target risk ǫprior is feasible makes it possible to improve the sample complexity from O(1/ǫprior) to O(log(1/ǫprior)). We also note that although the logarithmic sample complexity is known for active learning [10, 2], we are unaware of any existing passive learning algorithm that is able to achieve a logarithmic sample complexity by incorporating any kind of prior knowledge.\n1We use ǫprior instead of ǫ to emphasize the fact that this parameter is known to the learner in advance."
    }, {
      "heading" : "1.1 More Related Work",
      "text" : "Stochastic Optimization and Learnability Our work is related to the recent studies that examined the learnability from the viewpoint of stochastic convex optimization. In [28, 26], the authors presented learning problems that are learnable by stochastic convex optimization but not by empirical risk minimization (ERM). Our work follows this line of research. The proposed algorithm achieves the sample complexity of O(d log(1/ǫprior)) by explicitly incorporating the target expected risk ǫprior into the stochastic convex optimization algorithm. It is however difficult to incorporate such knowledge into the framework of ERM. Furthermore, it is worth noting that in [23, 28, 22, 4], the authors explored the connection between online optimization and statistical learning in the opposite direction. This was done by exploring the complexity measures developed in statistical learning for the learnability of online learning.\nOnline and Stochastic Optimization The proposed algorithm is closely related to the recent works that stated O(1/n) is the optimal convergence rate for stochastic optimization when the objective function is strongly convex [14, 12, 21]. In contrast, the proposed algorithm is able to achieve a geometric convergence rate for a target optimization error. Similar to the previous argument, our result does not contradict the lower bound given in [12] because of the knowledge of a feasible optimization error. Moreover, in contrast to the multistage algorithm in [12] where the size of stages increases exponentially, in our algorithm, the size of each stage is fixed to be a constant.\nOutline The remainder of the paper is organized as follows: In Section 2, we set up notation, describe the setting, and discuss the assumptions on which our algorithm relies. Section 3 motivates the problem and discusses the main intuition of our algorithm. The proposed algorithm and main result are discussed in Section 4. We prove the main result in Section 5. Section 6 concludes the paper and the appendix contains the omitted proofs."
    }, {
      "heading" : "2 Preliminaries",
      "text" : "As usual in the framework of statistical learning theory, we consider a domain Z := X × Y where X ⊆ Rd is the space for instances and Y is the set of labels, and H is a hypothesis class. We assume that the domain space Z is endowed with an unknown Borel probability measure D. We measure the performance of a specific hypothesis h by defining a nonnegative loss function ℓ : H×Z → R+. We denote the risk of a hypothesis h by L(h) = Ez∼D[ℓ(h, z)]. Given a sample S = (z1, · · · , zn) = ((x1, y1), · · · , (xn, yn)) ∼ Dn, the goal of a learning algorithm is to pick a hypothesis h : X → Y from H in such a way that its risk L(h) is close to the minimum possible risk of a hypothesis in H.\nThroughout this paper we pursue stochastic optimization viewpoint for risk minimization as detailed in Section 3. Precisely, we focus on the convex learning problems for which we assume that the hypothesis class H is a parametrized convex set H = {hw : x 7→ 〈w,x〉 : w ∈ Rd, ‖w‖ ≤ R} and for all z = (x, y) ∈ Z, the loss function ℓ(·, z) is a non-negative convex function. Thus, in the remainder we simply use vector w to represent hw, rather than working with hypothesis hw. We will assume throughout that X ⊆ Rd is the unit ball so\nthat ‖x‖ ≤ 1. Finally, the conditions under which we can get the desired result on sample complexity depend on analytic properties of the loss function. In particular, we assume that the loss function is strongly convex and smooth [20].\nDefinition 1 (Strong convexity). A loss function ℓ(w) is said to be α-strongly convex w.r.t a norm ‖ · ‖2, if there exists a constant α > 0 (often called the modulus of strong convexity) such that, for any λ ∈ [0, 1] and for all w1,w2 ∈ H, it holds that\nℓ(λw1 + (1− λ)w2) ≤ αℓ(w1) + (1 − λ)ℓ(w2)− 1\n2 λ(1− λ)α‖w1 −w2‖2.\nWhen ℓ(w) is differentiable, the strong convexity is equivalent to\nℓ(w1) ≥ ℓ(w2) + 〈∇ℓ(w2),w1 −w2〉+ α\n2 ‖w1 −w2‖2, ∀ w1,w2 ∈ H.\nWe would like to emphasize that in our setting, we only need that the expected loss function L(w) be strongly convex, without having to assume strong convexity for individual loss functions. Another property of loss function that underline our analysis is its smoothness. Smooth functions arise, for instance, in logistic and least-squares regression, and in general for learning linear predictors where the loss function has a Lipschitz-continuous gradient.\nDefinition 2 (Smoothness). A differentiable loss function ℓ(w) is said to be β-smooth with respect to a norm ‖ · ‖, if it holds that\nℓ(w1) ≤ ℓ(w2) + 〈∇ℓ(w2),w1 −w2〉+ β\n2 ‖w1 −w2‖2, ∀ w1,w2 ∈ H. (1)"
    }, {
      "heading" : "3 The Curse of Stochastic Oracle",
      "text" : "We begin by discussing stochastic optimization for risk minimization, convex learnability, and then the main intuition that motivates this work.\nMost existing learning algorithms follow the framework of empirical risk minimizer (ERM) or regularized ERM, which was developed to great extent by Vapnik and Chervonenkis [30]. Essentially, ERM methods use the empirical loss over S, i.e., L̂(w) = 1 n ∑n i=1 ℓ(w, zi), as a criterion to pick a hypothesis. In regularized ERM methods, the learner picks a hypothesis that jointly minimizes L̂(w) and a regularization function over w. We note that ERM resembles the widely used Sample Average Approximation (SAA) method in the optimization community when the hypothesis space and the loss function are convex. If uniform convergence holds, then the empirical risk minimizer is consistent, i.e., the population risk of the ERM converges to the optimal population risk, and the problem is learnable using ERM.\nA rather different paradigm for risk minimization is stochastic optimization. Recall that the goal of learning is to approximately minimize the risk L(w) = Ez∼D [ℓ(w, z)]. However, since the distribution D is unknown to the learner, we can not utilize standard gradient methods to minimize the expected loss. Stochastic optimization methods circumvent this problem by allowing the optimization method to take a step which is only in expectation along the\n2Throughout this paper, we only consider the ℓ2-norm.\nnegative of the gradient. To motivate stochastic optimization as an alternative to the ERM method, [25, 24] challenged the ERM method and showed that there is a real gap between learnability and uniform convergence by investigating non-trivial problems where no uniform convergence holds, but they are still learnable using Stochastic Gradient Descent (SGD) algorithm [18]. These results uncovered an important relationship between learnability and stability, and showed that stability together with approximate empirical risk minimization, assures learnability [26]. We note that Lipschitzness or smoothness of loss function is necessary for an algorithm to be stable, and boundedness and convexity alone are not sufficient for ensuring that the convex learning problem is learnable.\nTo directly solve minw∈H L(w) = Ez∼D[ℓ(w, z)], a typical stochastic optimization algorithm initially picks some point in the feasible set H and iteratively updates these points based on first order perturbed gradient information about the function at those points. For instance, the widely used SGD algorithm starts with w0 = 0; at each iteration t, it queries the stochastic oracle (SO) at wt to obtain a perturbed but unbiased gradient ĝt and updates the current solution by wt+1 = ΠH (wt − ηtĝt) , where ΠH(w) projects the solution w into the domain H. To capture the efficiency of optimization procedures in a general sense, one can use oracle complexity of the algorithm which, roughly speaking, is the minimum number of calls to any oracle needed by any method to achieve desired accuracy [20]. We note that the oracle complexity corresponds to the sample complexity of learning from the stochastic optimization viewpoint previously discussed. The following theorem states a lower bound on the sample complexity of stochastic optimization algorithms [19].\nTheorem 3 (Lower Bound on Oracle Complexity). Suppose L(w) = Ez∼D[ℓ(w, z)] is αstrongly and β-smooth convex function defined over convex domain H. Let SO be a stochastic oracle that for any point w ∈ H returns an unbiased estimate ĝ, i.e., E[ĝ] = ∇L(w), such that E [ ‖ĝ−∇L(w)‖2 ] ≤ σ2 holds. Then for any stochastic optimization algorithm A to find a solution ŵ with ǫ accuracy respect to the optimal solution w∗, i.e., E [L(ŵ)− L(w∗)] ≤ ǫ, the number of calls to SO is lower bounded by\nO(1) (√ β\nα log\n( β‖w0 −w∗‖2\nǫ\n) + σ2\nαǫ\n) . (2)\nThe first term in (2) comes from deterministic oracle complexity and the second term is due to noisy gradient information provided by SO. As indicated in (2), the slow convergence rate for stochastic optimization is due to the variance in stochastic gradients, leading to at least O ( σ2/ǫ ) queries to be issued. We note that the idea of mini-batch [7, 8], although it reduces the variance in stochastic gradients, does not reduce the oracle complexity. We close this section by informally presenting why logarithmic sample complexity is, in principle, possible, under the assumption that target risk is known to the learner A. To this end, consider the setting of Theorem 3 and assume that the learner A is given the prior accuracy ǫprior and is asked to find an ǫprior-accurate solution. If it happens that the variance of SO has the same magnitude as ǫprior, i.e., E [ ‖ĝ−∇L(w)‖2 ] ≤ ǫprior, then from (2) it\nfollows that the second term vanishes and the learner A needs to issue only O (log 1/ǫprior) queries to find the solution. But, since there is no control on SO, except that the variance of stochastic gradients are bounded, A needs a mechanism to manage the variance of perturbed gradients at each iteration in order to alleviate the influence of noisy gradients. One strategy is to replace the unbiased estimate of gradient with a biased one, which unfortunately may yield loose bounds. To overcome this problem, we introduce a strategy that shrinks the solution space with respect to the target risk ǫprior to control the damage caused by biased estimates."
    }, {
      "heading" : "4 Algorithm and Main Result",
      "text" : "In this section we proceed to describe the proposed algorithm and state the main result on its sample complexity."
    }, {
      "heading" : "4.1 Description of Algorithm",
      "text" : "We now turn to describing our algorithm. Interestingly, our algorithm is quite dissimilar to the classic stochastic optimization methods. It proceeds by running the algorithm online on fixed chunks of examples, and using the intermediate hypotheses and target risk ǫprior to gradually refine the hypothesis space. As mentioned above, we assume in our setting that the target expected risk ǫprior is provided to the learner a priori. We further assume the target risk ǫprior is feasible for the solution within the domain H, i.e., ǫprior ≥ ǫopt. The proposed algorithm explicitly takes advantage of the knowledge of expected risk ǫprior to attain an O (log(1/ǫprior)) sample complexity.\nThroughout we shall consider linear predictors of form 〈w,x〉 and assume that the loss function of interest ℓ(〈w,x〉, y) is β-smooth. It is straightforward to see that L(w) = E(x,y)∼D [ℓ(〈w,x〉, y)] is also β-smooth. In addition to the smoothness of the loss function, we also assume that L(w) to be α-strongly convex. We denote by w∗ the optimal solution that minimizes L(w), i.e., w∗ = argminw∈H L(w), and denote its optimal value by ǫopt.\nLet (xt, yt), t = 1, . . . , T be a sequence of i.i.d. training examples. The proposed algorithm divides the T iterations into the m stages, where each stage consists of T1 training examples, i.e., T = mT1. Let (x t k, y t k) be the t-th training example received at stage k, and let η be the step size used by all the stages. At the beginning of each stage k, we initialize the solution w by the average solution ŵk obtained from the last stage, i.e.,\nŵk = 1\nT1\nT1∑\nt=1\nŵtk, (3)\nwhere ŵtk denotes the tth solution at stage k. Another feature of the proposed algorithm is a domain shrinking strategy that adjusts the domain as the algorithm proceeds using intermediate hypotheses and target risk. We define the domain Hk used at stage k as\nHk = {w ∈ H : ‖w− ŵk‖ ≤ ∆k} , (4)\nwhere ∆k is the domain size, whose value will be discussed later. Similar to the SGD method, at each iteration of stage k, we receive a training example (xtk, y t k), and compute the gradient\nAlgorithm 1 Convex Learning with Target Risk\n1: Input: step size η, stage size T1, number of stages m, target expected risk ǫprior, parameters ε ∈ (0, 1) and τ ∈ (0, 1) used for updating domain size ∆k, and parameter ξ ≥ 1 used to clip the gradients 2: Initialization: ŵ1 = 0, ∆1 = R, and H1 = H 3: for k = 1, . . . ,m do 4: Set wtk = ŵk and γk = 2ξβ∆k 5: for t = 1, . . . , T1 do 6: Receive training example (xt, yt) 7: Compute the gradient ĝtk and the clipped version of the gradient v t k using Eq. (5) 8: Update the solution wtk using Eq. (6). 9: end for 10: Update ∆k using Eq. (7). 11: Compute the average solution ŵk+1 according to Eq. (3), and update the domain Hk+1 using the expression in (4). 12: end for\nĝtk = ℓ ′ (〈wtk,xtk〉, yt)xtk. Instead of using the gradient directly, following [13], a clipped version of the gradient, denoted by vtk = clip (γk, ĝ t k), will be used for updating the solution. More specifically, the clipped vector vtk ∈ Rd is defined as\n[vtk]i = clip ( γk, [ ĝtk ] i ) = sign ([ ĝtk ] i ) min ( γk, ∣∣[ĝtk ] i ∣∣) , i = 1, . . . , d (5)\nwhere γk = 2ξβ∆k with ξ ≥ 1. Given the clipped gradient vtk, we follow the standard framework of stochastic gradient descent, and update the solution by\nwt+1k = ΠHk ( wtk − ηvtk ) . (6)\nThe purpose of introducing the clipped version of the gradient is to effectively control the variance in stochastic gradients, an important step toward achieving the geometric convergence rate. At the end of each stage, we will update the domain size by explicitly exploiting the target expected risk ǫprior as\n∆k+1 = √ ε∆2k + τǫprior , (7)\nwhere ε ∈ (0, 1) and τ ∈ (0, 1) are two parameters, both of which will be discussed later. Algorithm 1 gives the detailed steps for the proposed method. The three important aspects of Algorithm 1, all crucial to achieve a geometric convergence rate, are highlighted as follows:\n• Each stage of the proposed algorithm is comprised of the same number of training examples. This is in contrast to the epoch gradient algorithm [12] which divides m iterations into exponentially increasing epochs, and runs SGD with averaging on each epoch. Also, in our case the learning rate is fixed for all iterations.\n• The proposed algorithm uses a clipped gradient for updating the solution in order to better control the variance in stochastic gradients; this stands in contrast to the SGD method, which uses original gradients to update the solution.\n• The proposed algorithm takes into account the targeted expected risk and intermediate hypotheses when updating the domain size at each stage. The purpose of domain shrinking is to reduce the damage caused by biased gradients that resulted from clipping operation."
    }, {
      "heading" : "4.2 Main Result on Sample Complexity",
      "text" : "The main theoretical result of Algorithm 1 is given in the following theorem.\nTheorem 4 (Convergence Rate). Assume that the hypothesis space H is compact and the loss function ℓ is α-strongly convex and β-smooth. Let T = mT1 be the size of the sample and ǫprior be the target expected loss given to the learner in advance such that ǫopt ≤ ǫprior holds. Given ε ∈ (0, 1) and τ ∈ (0, 1), set ξ, η, and T1 as\nξ = 4β\nατ , T1 = 4max\n{ ξ3βd+ 2ξβ √ d\nεα ln\nms δ , 16ξ2β2 α2ε2\n} , η =\n1\n2ξβ √ T1 ,\nwhere\ns = ⌈ log2 ξβR2\nǫprior\n⌉ . (8)\nAfter running Algorithm 1 over m stages, we have, with a probability 1− δ,\nL(ŵm+1) ≤ βR2\n2 εm +\n( 1 + τ\n1− ε\n) ǫprior,\nimplying that only O(d log[1/ǫprior]) training examples are needed in order to achieve a risk of O(ǫprior).\nWe note that comparing to the bound in Theorem 3, for Algorithm 1 the level of error to which the linear convergence holds is not determined by the noise level in stochastic gradients, but by the target risk. In other words, the algorithm is able to tolerate the noise by knowing the target risk as prior knowledge and achieves a linear convergence to the level of the target risk even when the variance of stochastic gradients is much larger than the target risk. In addition, although the result given in Theorem 4 assumes a bounded domain with ‖w‖ ≤ R, however, this assumption can be lifted by effectively exploring the strong convexity of the loss function and further assuming that the loss function is Lipschitz continuous with constant G, i.e., |L(w1) − L(w2)| ≤ G‖w1 − w2‖, ∀ w1,w2 ∈ H. More specifically, the fact that the L(w) is α-strongly convex with first order optimality condition, for the optimal solution w∗ = argminw∈H L(w), we have\nL(w) − L(w∗) ≥ α\n2 ‖w−w∗‖2, ∀w ∈ H.\nThis inequality combined with Lipschitz continuous assumption implies that for any w ∈ H the inequality ‖w − w∗‖ ≤ R∗ := 2G/α holds, and therefore we can simply set R = R∗. We also note that this dependency can be resolved with a weaker assumption than Lipschitz continuity, which only depends on the gradient of loss function at origin. To this end, we define |ℓ′(0, y)| = G. Using the fact that L(w) is α-strongly, it is easy to verify that α2 ‖w∗‖2− G‖w∗‖ ≤ 0, leading to ‖w∗‖ ≤ R∗ := 2αG and, therefore, we can simply set R = R∗.\nWe now use our analysis of Algorithm 1 to obtain a sample complexity analysis for learning smooth strongly convex problems with a bounded hypothesis class. To make it easier to parse, we only keep the dependency on the main parameters d, α, β, T , and ǫprior and hide the dependency on other constants in O(·) notation. Let ŵ denote the output of Algorithm 1. By setting ε = 0.5 and letting c = O(τ) to be an arbitrary small number, Theorem 4 yields the following:\nCorollary 5 (Sample Complexity). Under the same conditions as Theorem 4, by running Algorithm 1 for minimizing L(w) with a number of iterations (i.e., number of training examples) T , if it holds that,\nT ≥ O ( dκ4 ( log 1\nǫprior log log\n1\nǫprior + log\n1\nδ\n))\nwhere κ = β/α denotes the condition number of the loss function and d is the dimension of data, then with a probability 1− δ, ŵ attains a risk of O(ǫprior), i.e., L(ŵ) ≤ (1 + c)ǫprior.\nAs an example of a concrete problem that may be put into the setting of the present work is the regression problem with squared loss. It is easy to show that average square loss function is Lipschitz continuous with a Lipschitz constant β = λmax(X\n⊤X) which denotes the largest eigenvalue of matrix X⊤X where X is the data matrix. The strong convexity is guaranteed as long as the population data covariance matrix is not rank-deficient and its minimum eigenvalue is lower bounded by a constant α > 0. For this problem, the optimal minimax sample complexity is known to be O(1\nǫ ), but as it implies from Corollary 5, by the knowledge\nof target risk ǫprior, it is possible to reduce the sample complexity to O(log(1/ǫprior)).\nRemark 6. It is indeed remarkable that the sample complexity of Theorem 4 has κ4 = (β/α)4 dependency on the condition number of the loss function, which is worse than the √ β/α dependency in the lower bound in (2). Also, the explicit dependency of sample complexity on dimension d makes the proposed algorithm inappropriate for non-parametric settings."
    }, {
      "heading" : "5 Analysis",
      "text" : "Now we turn to proving the main theorem. The proof will be given in a series of lemmas and theorems where the proof of few are given in the appendix. The proof makes use of the Bernstein inequality for martingales, idea of peeling process, self-bounding property of smooth loss functions, standard analysis of stochastic optimization, and novel ideas to derive the claimed sample complexity for the proposed algorithm.\nThe proof of Theorem 4 is by induction and we start with the key step given in the following theorem.\nTheorem 7. Assume ǫprior ≥ ǫopt. For a fixed stage k, if ‖ŵk − w∗‖ ≤ ∆k, then, with a probability 1− δ, we have ‖ŵk+1 −w∗‖2 ≤ a∆2k + bǫprior where\na = 2\nαT1\n( 2ξβ √ T1 + [ ξ3βd+ 2ξβ √ d ] ln s\nδ\n) , b = 8\nαξ (9)\nand s is given in (8), provided that ξ ≥ 16β/α and η = 1/(2ξβ √ T1) hold.\nTaking this statement as given for the moment, we proceed with the proof of Theorem 4, returning later to establish the claim stated in Theorem 7.\nof Theorem 4. By setting a and b in (9) in Theorem 7 as a ≤ ε and b ≤ 2τ/β, we have ξ ≥ 4β/(ατ) and\nT1 ≤ 2\nαε\n( 2ξβ √ T1 + [ ξ3βd+ 2ξβ √ d ] ln s\nδ\n)\nimplying that\nT1 ≥ 4max { ξ3βd+ 2ξβ √ d\nεα ln\ns δ , 16ξ2β2 α2ε2\n} .\nThus, using Theorem 7 and the definition of ξ and T1, we have, with a probability 1− δ,\n∆2k+1 ≤ ε∆2k + 2τ\nβ ǫprior.\nAfter m stages, with a probability 1−mδ, we have\n∆2m+1 ≤ εm∆21 + 2τ\nβ ǫprior\nm−1∑\ni=0\nεi ≤ εm∆21 + 2τ\nβ(1− ε)ǫprior.\nBy the β-smoothness of L(w), it implies that\nL(ŵm+1)− L(w∗) ≤ β\n2 ‖ŵm+1 −w∗‖2 ≤\nβ 2 εm∆21 + τ 1− εǫprior,\n≤ βR 2\n2 εm +\nτ\n1− εǫprior,\nwhere the last inequality follows from ∆1 ≤ R. The bound stated in the theorem follows the assumption that L(w∗) = ǫopt ≤ ǫprior."
    }, {
      "heading" : "5.1 Proof of Theorem 7",
      "text" : "To bound ‖ŵk+1−w∗‖ in terms of ∆k, we start with the standard analysis of online learning. In particular, from the strong convexity assumption of L(w) and updating rule (6) we have,\nL(wtk)− L(w∗) ≤ 〈∇L(wtk),wtk −w∗〉 − α\n2 ‖wtk −w∗‖2\n= 〈vtk,wtk −w∗〉+ 〈∇L(wtk)− vtk,wtk −w∗〉 − α\n2 ‖wt −w∗‖2\n≤ ‖w t+1 k −w∗‖2 − ‖wt+1k −w∗‖2\n2η +\nηd\n2 γ2k\n+ 〈∇L(wtk)− vtk,wtk −w∗〉︸ ︷︷ ︸ ,vt\nk\n−α 2 ‖wt −w∗‖2, (10)\nwhere the last step follows from ‖vtk‖ ≤ γk √ d. By adding all the inequalities of (10) at stage k, we have\nT1∑\nt=1\nL(wtk)− L(w∗) ≤ ‖ŵk −w∗‖2\n2η +\ndη\n2 γ2kT1 +\nT1∑\nt=1\nvtk − α\n2\nT1∑\nt=1\n‖wt −w∗‖2\n≤ ∆ 2 k\n2η +\ndη\n2 γ2kT1 + Vk −\nα 2 Wk, (11)\nwhere Vk and Wk are defined as Vk = ∑T1 t=1 v t k and Wk = ∑T1 t=1 ‖wtk −w∗‖2, respectively. In order to bound Vk, using the fact that ∇L(wtk) = Et[ĝtk], we rewrite Vk as\nVk =\nT1∑\nt=1 〈−vtk + Et[vtk],wtk −w∗〉︸ ︷︷ ︸ ,dt\nk\n+\nT1∑\nt=1\n〈Et [ ĝtk ] − Et[vtk],wtk −w∗〉︸ ︷︷ ︸\n,et k\n= Dk + Ek,\nwhere Dk = ∑T1 t=1 d t k and Ek = ∑T1 t=1 e t k which represent the variance and bias of the clipped gradient vtk, respectively. We now turn to separately upper bound each term. The following lemma bounds the variance term Dk using the Bernstein inequality for martingale. Its proof can be found in Appendix A.\nLemma 1. For any L > 0 and µ > 0, we have\nPr ( Wk ≤\nǫpriorT1 2µβ\n) + Pr ( Dk ≤ 1\nL Wk +\n( Lγ2kd+ γk∆k √ d ) ln s\nδ\n) ≥ 1− δ\nwhere s is given by\ns = ⌈ log2 8βµR2\nǫprior\n⌉ .\nThe following lemma bounds Ek using the self-bounding property of smooth functions and the proof is deferred to Appendix B.\nLemma 2.\nEk ≤ 4T1 ξ ǫopt + 4β ξ Wk ≤ 4T1 ξ ǫprior + 4β ξ Wk.\nNote that without the knowledge of ǫprior, we have to bound ǫopt by Ω(1), resulting in a very loose bound for the bias term Ek. It is knowledge of the target expected risk ǫprior that allows us to come up with a significantly more accurate bound for the bias term Ek, which consequentially leads to a geometric convergence rate.\nWe now proceed to bound ∑T1\nt=1 L(wtk) − L(w∗) using the two bounds in Lemma 1 and 2. To this end, based on the result obtained in Lemma 1, we consider two scenarios. In the first scenario, we assume\nWk ≤ ǫpriorT1 2µβ\n(12)\nIn this case, we have\nT1∑\nt=1\nL(wtk)− L(w∗) ≤ β\n2 Wk ≤ ǫprior 2µ T1. (13)\nIn the second scenario, we assume\nDk ≤ 1\nL WT +\n( Lγ2kd+ γk∆k √ d ) ln s\nδ . (14)\nIn this case, by combining the bounds for Dk and Ek and setting L = ξ 4β , we have\nVk ≤ 8β\nξ Wk +\n( ξd\n4β γ2k + γk∆k\n√ d ) ln s\nδ + 4T1 ξ ǫprior\n= 8β\nξ Wk +\n( ξ3βd+ 2ξβ √ d ) ∆2k ln s\nδ + 4T1 ξ ǫprior,\nwhere the last equality follows from the fact γk = 2ξβ∆k. If we choose ξ such that 8β ξ ≤ α2 or ξ ≥ 16β\nα > 1 holds, we get\nVk ≤ α\n2 Wk +\n( ξ3βd+ 2ξβ √ d ) ∆2k ln s\nδ + 4T1 ξ ǫprior\nSubstituting the above bound for Vk into the inequality of (11), we have\nT1∑\nt=1\nL(wtk)− L(w∗) ≤ ∆2k 2η + η 2 γ2kT1 +\n( ξ3βd+ 2ξβ √ d ) ∆2k ln s\nδ + 4T1 ξ ǫprior\nBy choosing η as η = ∆k γk √ T1 = 1 2ξβ √ T1 , we have\nL(ŵk+1)− L(w∗) ≤ 1\nT1\n( 2ξβ √ T1 + [ ξ3βd+ 2ξβ √ d ] ln s\nδ\n) ∆2k + 4\nξ ǫprior. (15)\nBy combining the bounds in (13) and (15), under the assumption that at least one of the two conditions in (12) and (14) is true, by setting µ = B/8, we have\nL(ŵk+1)− L(w∗) ≤ 1\nT1\n( 2ξβ √ T1 + [ ξ3βd+ 2ξβ √ d ] ln s\nδ\n) ∆2k + 4\nξ ǫprior,\nimplying\n‖ŵk+1 −w∗‖ ≤ 2\nαT1\n( 2ξβ √ T1 + [ ξ3βd+ 2ξβ √ d ] ln s\nδ\n) ∆2k + 8\nαξ ǫprior.\nWe complete the proof by using Lemma 1, which states that the probability for either of the two conditions hold is no less than 1− δ."
    }, {
      "heading" : "6 Conclusions",
      "text" : "In this paper, we have studied the sample complexity of passive learning when the target expected risk is given to the learner as prior knowledge. The crucial fact about target risk assumption is that, it can be fully exploited by the learning algorithm and stands in contrast to most common types of prior knowledges that usually enter into the generalization bounds and are often perceived as a rather crude way to incorporate such assumptions. We showed that by explicitly employing the target risk ǫprior in a properly designed stochastic optimization algorithm, it is possible to attain the given target risk ǫprior with a logarithmic\nsample complexity log (\n1 ǫprior\n) , under the assumption that the loss function is both strongly\nconvex and smooth. There are various directions for future research. The current study is restricted to the parametric setting where the hypothesis space is of finite dimension. It would be interesting to see how to achieve a logarithmic sample complexity in a non-parametric setting where hypotheses lie in a functional space of infinite dimension. Evidently, it is impossible to extend the current algorithm for the non-parametric setting; therefore additional analysis tools are needed to address the challenge of infinite dimension arising from the non-parametric setting. It is also an interesting problem to relate target risk assumption we made here to the low noise margin condition which is often made in active learning for binary classification since both settings appear to share the same sample complexity. However it is currently unclear how to derive a connection between these two settings. We believe this issue is worthy of further exploration and leave it as an open problem."
    }, {
      "heading" : "A Proof of Lemma 1",
      "text" : "The proof is based on the Bernstein inequality for martingales (see, e.g., [6]).\nLemma 3. (Bernstein inequality for martingales). Let X1, . . . , Xn be a bounded martingale difference sequence with respect to the filtration F = (Fi)1≤i≤n and with ‖Xi‖ ≤ M . Let Si = ∑i j=1 Xj be the associated martingale. Denote the sum of the conditional variances by\nΣ2n =\nn∑\nt=1\nE [ X2t |Ft−1 ]\nThen for all constants κ, ν > 0,\nPr [ max\ni=1,...,n Si > ρ and Σ\n2 n ≤ ν\n] ≤ exp ( − ρ 2\n2(ν +Mρ/3)\n)\nand therefore,\nPr [ max\ni=1,...,n Si >\n√ 2νρ+\n√ 2\n3 Mρ and Σ2n ≤ ν\n] ≤ e−ρ.\nof Lemma 1. Define martingale difference dtk = 〈wtk −w∗,Et[vtk]− vtk〉 and martingale Dk =∑T1 t=1 d t k. Let Σ 2 T denote the conditional variance as\nΣ2T =\nT1∑\nt=1\nEt [ (dtk) 2 ] ≤ T1∑\nt=1\nEt [∥∥Et[vtk]− vtk ∥∥2 ] ‖wtk −w∗‖2\n≤ T∑\nt=1\ndγ2k‖wtk −w‖2 = dγ2kWk,\nwhich follows from the Cauchy’s Inequality and the definition of clipping. Define M = max\nt |dtk| ≤ 2\n√ dγk∆k. To prove the inequality in Lemma 1, we follow the idea of peeling\nprocess [16]. Since Wk ≤ 4R2T1, we have Pr ( Dk ≥ 2γk √ Wkdρ+ √ 2Mρ/3 )\n= Pr ( Dk ≥ 2γk √ Wkdρ+ √ 2Mρ/3,Wk ≤ 4R2T1 ) = Pr ( Dk ≥ 2γk √ Wkdρ+ √ 2Mρ/3,Σ2T ≤ γ2kdWk,Wk ≤ 4R2T1 ) ≤ Pr ( Dk ≥ 2γk √ Wkdρ+ √ 2Mρ/3,Σ2T ≤ γ2kdWk,Wk ≤ ǫpriorT1/(2βµ) )\n+\ns∑\ni=1\nPr ( Dk ≥ 2γk √ Wkdρ+ √ 2Mρ/3,Σ2T ≤ γ2kdWk, ǫprior2 i−1T1\n2βµ < Wk ≤\nǫprior2 iT1\n2βµ\n)\n≤ Pr ( Wk ≤\nǫpriorT1 2βµ\n) + s∑\ni=1\nPr  Dk ≥ √ ǫprior2i+1T1γ2kd\n2βµ ρ+\n√ 2\n3 Mρ,Σ2T ≤\nǫprior2 iT1γ 2 kd\n2βµ\n \n≤ Pr ( Wk ≤\nǫpriorT1 2βµ\n) + se−ρ,\nwhere s is given by\ns = ⌈ log2 8βµR2\nǫprior\n⌉ .\nThe last step follows the Bernstein inequality for martingales. We complete the proof by setting ρ = ln(s/δ) and using the fact that\n2γk √ Wkρd ≤ 1\nL Wk + γ\n2 kρdL."
    }, {
      "heading" : "B Proof of Lemma 2",
      "text" : "To bound Ek, we need the following two lemmas. The first lemma bounds the deviation of the expected value of a clipped random variable from the original variable, in terms of its variance (Lemma A.2 from [13]).\nLemma 4. Let X be a random variable, let X̃ = clip(X,C) and assume that |E[X ]| ≤ C/2 for some C > 0. Then\n|E[X̃ ]− E[X ]| ≤ 2 C |Var[X]|\nAnother key observation used for bounding Ek is the fact that for any non-negative βsmooth convex function, we have the following self-bounding property. We note that this self-bounding property has been used in [27] to get better (optimistic) rates of convergence for non-negative smooth losses. Lemma 5. For any β-smooth non-negative function f : R → R, we have |f ′(w)| ≤ √ 4βf(w)\nAs a simple proof, first from the smoothness assumption, by setting w1 = w2 − 1β f ′(w2) in (1) and rearranging the terms we obtain f(w2)− f(w1) ≥ 12β |f ′(w2)|2. On the other hand, from the convexity of loss function we have f(w1) ≥ f ′(w2) + 〈f ′(w1), w1 − w2〉. Combining these inequalities and considering the fact that the function is non-negative gives the desired inequality.\nof Lemma 2. To apply the above lemmas, we write etk as\netk =\nd∑\ni=1\nEt [ ℓ′(〈wtk,xtk〉, yt)[xtk]i − clip ( γk, ℓ ′(〈wtk,xtk〉, yt)[xtk]i )] [wtk −w∗]i\nIn order to apply Lemma 4, we check if the following condition holds\nγk ≥ 2 ∣∣Et [ ℓ′ ( 〈wtk,xtk〉, yt ) [xtk]i ]∣∣ (16)\nSince ∣∣Et [ ℓ′ ( 〈wtk,xtk〉, yt ) [xtk]i\n]∣∣ ≤ ∣∣Et [{ ℓ′ ( 〈wtk,xtk〉, yt ) − ℓ′ ( 〈w∗,xtk〉, yt )} [xtk]i ]∣∣+ ∣∣Et [ ℓ′ ( 〈w∗,xtk〉, yt ) [xtk]i\n]∣∣ ≤ β‖wtk −w∗‖ ≤ β∆k\nwhere the last inequality follows from Et [ℓ ′ (〈w∗,xtk〉, yt) [xtk]i] = 0 since w∗ is the minimizer of L(w), we thus have\nγk = 2ξβ∆k ≥ 2β∆k ≥ 2 ∣∣Et [ ℓ′ ( 〈wtk,xtk〉, yt ) [xtk]i ]∣∣\nwhere ξ ≥ 1, implying that the condition in (16) holds. Thus, using Lemma 4, we have\netk ≤ d∑\ni=1\n∣∣[wtk −w∗]i ∣∣ 1 γk Et [( ℓ′(〈wtk,xtk〉, yt)[xtk]i )2]\n≤ 2‖w t k −w∗‖∞ γk Et\n[( ℓ′(〈wtk,xtk〉, yt) )2]\nUsing Lemma 5 to upper bound the right hand side, we further simplify the above bound for etk as\netk ≤ 8β‖wtk −w∗‖∞\nγk Et\n[ ℓ ( 〈wtk,xtk〉, yt )]\n= 8β‖wtk −w∗‖∞\nγk L(wtk)\n≤ 8β∆k γk L(wtk) = 4\nξ L(wtk)\nwhere the second inequality follows from ‖wtk − w∗‖∞ ≤ ‖wtk − w∗‖ ≤ ∆k. Therefore we obtain\nEk =\nT1∑\nt=1\netk ≤ 4\nξ\nT1∑\nt=1\nL(wtk) = 4\nξ\nT1∑\nt=1\nL(w∗) + 4\nξ\nT1∑\nt=1\nL(wtk)− L(w∗)\n≤ 4T1 ξ L(w∗) + 4β ξ\nT1∑\nt=1\n‖wtk −w∗‖2\n= 4T1 ξ L(w∗) + 4β ξ Wk,\nwhere the second inequality follows from the smoothness assumption of L(w)."
    } ],
    "references" : [ {
      "title" : "Neural network learning: Theoretical foundations",
      "author" : [ "M. Anthony", "P.L. Bartlett" ],
      "venue" : null,
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 1999
    }, {
      "title" : "The true sample complexity of active learning",
      "author" : [ "Maria-Florina Balcan", "Steve Hanneke", "Jennifer Wortman Vaughan" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2010
    }, {
      "title" : "Local rademacher complexities",
      "author" : [ "P.L. Bartlett", "O. Bousquet", "S. Mendelson" ],
      "venue" : "The Annals of Statistics,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2005
    }, {
      "title" : "Agnostic online learning",
      "author" : [ "Shai Ben-David", "David Pal", "Shai Shalev-Shwartz" ],
      "venue" : "In COLT,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2009
    }, {
      "title" : "Learnability and the vapnik-chervonenkis dimension",
      "author" : [ "Anselm Blumer", "A. Ehrenfeucht", "David Haussler", "Manfred K. Warmuth" ],
      "venue" : "J. ACM,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 1989
    }, {
      "title" : "Prediction, learning, and games",
      "author" : [ "Nicolò Cesa-Bianchi", "Gábor Lugosi" ],
      "venue" : null,
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2006
    }, {
      "title" : "Better mini-batch algorithms via accelerated gradient methods",
      "author" : [ "Andrew Cotter", "Ohad Shamir", "Nati Srebro", "Karthik Sridharan" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2011
    }, {
      "title" : "Randomized smoothing for stochastic optimization",
      "author" : [ "John C. Duchi", "Peter L. Bartlett", "Martin J. Wainwright" ],
      "venue" : "SIAM Journal on Optimization,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2012
    }, {
      "title" : "A general lower bound on the number of examples needed for learning",
      "author" : [ "A. Ehrenfeucht", "D. Haussler", "M. Kearns", "L. Valiant" ],
      "venue" : "Information and Computation,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 1989
    }, {
      "title" : "Theoretical Foundations of Active Learning",
      "author" : [ "Steve Hanneke" ],
      "venue" : "PhD thesis,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2009
    }, {
      "title" : "Logarithmic regret algorithms for online convex optimization",
      "author" : [ "Elad Hazan", "Adam Kalai", "Satyen Kale", "Amit Agarwal" ],
      "venue" : "In COLT,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2006
    }, {
      "title" : "Beyond the regret minimization barrier: an optimal algorithm for stochastic strongly-convex optimization",
      "author" : [ "Elad Hazan", "Satyen Kale" ],
      "venue" : null,
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2011
    }, {
      "title" : "Optimal algorithms for ridge and lasso regression with partially observed",
      "author" : [ "Elad Hazan", "Tomer Koren" ],
      "venue" : null,
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2011
    }, {
      "title" : "Primal-dual subgradient methods for minimizing uniformly convex functions",
      "author" : [ "Anatoli Iouditski", "Yuri Nesterov" ],
      "venue" : null,
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2010
    }, {
      "title" : "On the complexity of linear prediction: Risk bounds, margin bounds, and regularization",
      "author" : [ "Sham M. Kakade", "Karthik Sridharan", "Ambuj Tewari" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2008
    }, {
      "title" : "Oracle Inequalities in Empirical Risk Minimization and Sparse Recovery Problems",
      "author" : [ "Vladimir Koltchinskii" ],
      "venue" : "Lecture Notes in mathematics. Springer,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2011
    }, {
      "title" : "The importance of convexity in learning with squared loss",
      "author" : [ "Wee Sun Lee", "Peter L. Bartlett", "Robert C. Williamson" ],
      "venue" : "IEEE Transactions on Information Theory,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 1980
    }, {
      "title" : "Robust stochastic approximation approach to stochastic programming",
      "author" : [ "A. Nemirovski", "A. Juditsky", "G. Lan", "A. Shapiro" ],
      "venue" : "SIAM Journal on Optimization,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2009
    }, {
      "title" : "Problem complexity and method efficiency in optimization",
      "author" : [ "A.S. Nemirovsky", "D.B. Yudin" ],
      "venue" : "Wiley Interscience Series in Discrete Mathematics,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 1983
    }, {
      "title" : "Introductory Lectures on Convex Optimization: A Basic Course",
      "author" : [ "Yurii Nesterov" ],
      "venue" : "Kluwer Academic Publishers,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2004
    }, {
      "title" : "Making gradient descent optimal for strongly convex stochastic optimization",
      "author" : [ "Alexander Rakhlin", "Ohad Shamir", "Karthik Sridharan" ],
      "venue" : "In ICML,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2012
    }, {
      "title" : "Online learning: Random averages, combinatorial parameters, and learnability",
      "author" : [ "Alexander Rakhlin", "Karthik Sridharan", "Ambuj Tewari" ],
      "venue" : "CoRR, abs/1006.1138,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2010
    }, {
      "title" : "Optimal stochastic convex optimization through the lens of active learning",
      "author" : [ "Aaditya Ramdas", "Aarti Singh" ],
      "venue" : "In ICML,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2013
    }, {
      "title" : "Learnability and stability in the general learning setting",
      "author" : [ "S. Shalev-Shwartz", "O. Shamir", "K. Sridharan", "N. Srebro" ],
      "venue" : null,
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2009
    }, {
      "title" : "Stochastic convex optimization",
      "author" : [ "S. Shalev-Shwartz", "O. Shamir", "K. Sridharan", "N. Srebro" ],
      "venue" : "COLT,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2009
    }, {
      "title" : "Learnability, stability and uniform convergence",
      "author" : [ "Shai Shalev-Shwartz", "Ohad Shamir", "Nathan Srebro", "Karthik Sridharan" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2010
    }, {
      "title" : "Smoothness, low noise and fast rates",
      "author" : [ "Nathan Srebro", "Karthik Sridharan", "Ambuj Tewari" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2010
    }, {
      "title" : "Learning from an optimization viewpoint",
      "author" : [ "Karthik Sridharan" ],
      "venue" : "PhD Thesis,",
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 2012
    }, {
      "title" : "Fast rates for regularized objectives",
      "author" : [ "Karthik Sridharan", "Shai Shalev-Shwartz", "Nathan Srebro" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 2008
    }, {
      "title" : "On the uniform convergence of relative frequencies of events to their probabilities",
      "author" : [ "V.N. Vapnik", "A.Y. Chervonenkis" ],
      "venue" : "Theory of Probability and Its Applications,",
      "citeRegEx" : "30",
      "shortCiteRegEx" : "30",
      "year" : 1971
    } ],
    "referenceMentions" : [ {
      "referenceID" : 8,
      "context" : "Sample complexity of passive learning is well established and goes back to early works in the learning theory where the lower bounds Ω ( 1 ǫ (log 1 ǫ + log 1 δ ) ) and Ω ( 1 ǫ2 (log 1 ǫ + log 1 δ ) ) were obtained in classic PAC and general agnostic PAC settings, respectively [9, 5, 1].",
      "startOffset" : 277,
      "endOffset" : 286
    }, {
      "referenceID" : 4,
      "context" : "Sample complexity of passive learning is well established and goes back to early works in the learning theory where the lower bounds Ω ( 1 ǫ (log 1 ǫ + log 1 δ ) ) and Ω ( 1 ǫ2 (log 1 ǫ + log 1 δ ) ) were obtained in classic PAC and general agnostic PAC settings, respectively [9, 5, 1].",
      "startOffset" : 277,
      "endOffset" : 286
    }, {
      "referenceID" : 0,
      "context" : "Sample complexity of passive learning is well established and goes back to early works in the learning theory where the lower bounds Ω ( 1 ǫ (log 1 ǫ + log 1 δ ) ) and Ω ( 1 ǫ2 (log 1 ǫ + log 1 δ ) ) were obtained in classic PAC and general agnostic PAC settings, respectively [9, 5, 1].",
      "startOffset" : 277,
      "endOffset" : 286
    }, {
      "referenceID" : 16,
      "context" : "In [17] fast rates obtained for squared loss, exploiting the strong convexity of this loss function, which only holds under pseudo-dimensionality assumption.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 10,
      "context" : "With the recent development in online strongly convex optimization [11], fast rates approaching O( ǫ log 1 δ ) for convex Lipschitz strongly convex loss functions has been obtained in [29, 15].",
      "startOffset" : 67,
      "endOffset" : 71
    }, {
      "referenceID" : 28,
      "context" : "With the recent development in online strongly convex optimization [11], fast rates approaching O( ǫ log 1 δ ) for convex Lipschitz strongly convex loss functions has been obtained in [29, 15].",
      "startOffset" : 184,
      "endOffset" : 192
    }, {
      "referenceID" : 14,
      "context" : "With the recent development in online strongly convex optimization [11], fast rates approaching O( ǫ log 1 δ ) for convex Lipschitz strongly convex loss functions has been obtained in [29, 15].",
      "startOffset" : 184,
      "endOffset" : 192
    }, {
      "referenceID" : 26,
      "context" : "For smooth non-negative loss functions, [27] improved the sample complexity to optimistic rates",
      "startOffset" : 40,
      "endOffset" : 44
    }, {
      "referenceID" : 2,
      "context" : "for non-parametric learning using the notion of local Rademacher complexity [3], where ǫopt is the optimal risk.",
      "startOffset" : 76,
      "endOffset" : 79
    }, {
      "referenceID" : 26,
      "context" : "We note that our work does not contradict the lower bound in [27] because a feasible target risk ǫprior is given in our learning setup and is fully exploited by the proposed algorithm.",
      "startOffset" : 61,
      "endOffset" : 65
    }, {
      "referenceID" : 9,
      "context" : "We also note that although the logarithmic sample complexity is known for active learning [10, 2], we are unaware of any existing passive learning algorithm that is able to achieve a logarithmic sample complexity by incorporating any kind of prior knowledge.",
      "startOffset" : 90,
      "endOffset" : 97
    }, {
      "referenceID" : 1,
      "context" : "We also note that although the logarithmic sample complexity is known for active learning [10, 2], we are unaware of any existing passive learning algorithm that is able to achieve a logarithmic sample complexity by incorporating any kind of prior knowledge.",
      "startOffset" : 90,
      "endOffset" : 97
    }, {
      "referenceID" : 27,
      "context" : "In [28, 26], the authors presented learning problems that are learnable by stochastic convex optimization but not by empirical risk minimization (ERM).",
      "startOffset" : 3,
      "endOffset" : 11
    }, {
      "referenceID" : 25,
      "context" : "In [28, 26], the authors presented learning problems that are learnable by stochastic convex optimization but not by empirical risk minimization (ERM).",
      "startOffset" : 3,
      "endOffset" : 11
    }, {
      "referenceID" : 22,
      "context" : "Furthermore, it is worth noting that in [23, 28, 22, 4], the authors explored the connection between online optimization and statistical learning in the opposite direction.",
      "startOffset" : 40,
      "endOffset" : 55
    }, {
      "referenceID" : 27,
      "context" : "Furthermore, it is worth noting that in [23, 28, 22, 4], the authors explored the connection between online optimization and statistical learning in the opposite direction.",
      "startOffset" : 40,
      "endOffset" : 55
    }, {
      "referenceID" : 21,
      "context" : "Furthermore, it is worth noting that in [23, 28, 22, 4], the authors explored the connection between online optimization and statistical learning in the opposite direction.",
      "startOffset" : 40,
      "endOffset" : 55
    }, {
      "referenceID" : 3,
      "context" : "Furthermore, it is worth noting that in [23, 28, 22, 4], the authors explored the connection between online optimization and statistical learning in the opposite direction.",
      "startOffset" : 40,
      "endOffset" : 55
    }, {
      "referenceID" : 13,
      "context" : "Online and Stochastic Optimization The proposed algorithm is closely related to the recent works that stated O(1/n) is the optimal convergence rate for stochastic optimization when the objective function is strongly convex [14, 12, 21].",
      "startOffset" : 223,
      "endOffset" : 235
    }, {
      "referenceID" : 11,
      "context" : "Online and Stochastic Optimization The proposed algorithm is closely related to the recent works that stated O(1/n) is the optimal convergence rate for stochastic optimization when the objective function is strongly convex [14, 12, 21].",
      "startOffset" : 223,
      "endOffset" : 235
    }, {
      "referenceID" : 20,
      "context" : "Online and Stochastic Optimization The proposed algorithm is closely related to the recent works that stated O(1/n) is the optimal convergence rate for stochastic optimization when the objective function is strongly convex [14, 12, 21].",
      "startOffset" : 223,
      "endOffset" : 235
    }, {
      "referenceID" : 11,
      "context" : "Similar to the previous argument, our result does not contradict the lower bound given in [12] because of the knowledge of a feasible optimization error.",
      "startOffset" : 90,
      "endOffset" : 94
    }, {
      "referenceID" : 11,
      "context" : "Moreover, in contrast to the multistage algorithm in [12] where the size of stages increases exponentially, in our algorithm, the size of each stage is fixed to be a constant.",
      "startOffset" : 53,
      "endOffset" : 57
    }, {
      "referenceID" : 19,
      "context" : "In particular, we assume that the loss function is strongly convex and smooth [20].",
      "startOffset" : 78,
      "endOffset" : 82
    }, {
      "referenceID" : 0,
      "context" : "t a norm ‖ · ‖2, if there exists a constant α > 0 (often called the modulus of strong convexity) such that, for any λ ∈ [0, 1] and for all w1,w2 ∈ H, it holds that l(λw1 + (1− λ)w2) ≤ αl(w1) + (1 − λ)l(w2)− 1 2 λ(1− λ)α‖w1 −w2‖.",
      "startOffset" : 120,
      "endOffset" : 126
    }, {
      "referenceID" : 29,
      "context" : "Most existing learning algorithms follow the framework of empirical risk minimizer (ERM) or regularized ERM, which was developed to great extent by Vapnik and Chervonenkis [30].",
      "startOffset" : 172,
      "endOffset" : 176
    }, {
      "referenceID" : 24,
      "context" : "To motivate stochastic optimization as an alternative to the ERM method, [25, 24] challenged the ERM method and showed that there is a real gap between learnability and uniform convergence by investigating non-trivial problems where no uniform convergence holds, but they are still learnable using Stochastic Gradient Descent (SGD) algorithm [18].",
      "startOffset" : 73,
      "endOffset" : 81
    }, {
      "referenceID" : 23,
      "context" : "To motivate stochastic optimization as an alternative to the ERM method, [25, 24] challenged the ERM method and showed that there is a real gap between learnability and uniform convergence by investigating non-trivial problems where no uniform convergence holds, but they are still learnable using Stochastic Gradient Descent (SGD) algorithm [18].",
      "startOffset" : 73,
      "endOffset" : 81
    }, {
      "referenceID" : 17,
      "context" : "To motivate stochastic optimization as an alternative to the ERM method, [25, 24] challenged the ERM method and showed that there is a real gap between learnability and uniform convergence by investigating non-trivial problems where no uniform convergence holds, but they are still learnable using Stochastic Gradient Descent (SGD) algorithm [18].",
      "startOffset" : 342,
      "endOffset" : 346
    }, {
      "referenceID" : 25,
      "context" : "These results uncovered an important relationship between learnability and stability, and showed that stability together with approximate empirical risk minimization, assures learnability [26].",
      "startOffset" : 188,
      "endOffset" : 192
    }, {
      "referenceID" : 19,
      "context" : "To capture the efficiency of optimization procedures in a general sense, one can use oracle complexity of the algorithm which, roughly speaking, is the minimum number of calls to any oracle needed by any method to achieve desired accuracy [20].",
      "startOffset" : 239,
      "endOffset" : 243
    }, {
      "referenceID" : 18,
      "context" : "The following theorem states a lower bound on the sample complexity of stochastic optimization algorithms [19].",
      "startOffset" : 106,
      "endOffset" : 110
    }, {
      "referenceID" : 6,
      "context" : "We note that the idea of mini-batch [7, 8], although it reduces the variance in stochastic gradients, does not reduce the oracle complexity.",
      "startOffset" : 36,
      "endOffset" : 42
    }, {
      "referenceID" : 7,
      "context" : "We note that the idea of mini-batch [7, 8], although it reduces the variance in stochastic gradients, does not reduce the oracle complexity.",
      "startOffset" : 36,
      "endOffset" : 42
    }, {
      "referenceID" : 12,
      "context" : "Instead of using the gradient directly, following [13], a clipped version of the gradient, denoted by vtk = clip (γk, ĝ t k), will be used for updating the solution.",
      "startOffset" : 50,
      "endOffset" : 54
    }, {
      "referenceID" : 11,
      "context" : "This is in contrast to the epoch gradient algorithm [12] which divides m iterations into exponentially increasing epochs, and runs SGD with averaging on each epoch.",
      "startOffset" : 52,
      "endOffset" : 56
    }, {
      "referenceID" : 5,
      "context" : ", [6]).",
      "startOffset" : 2,
      "endOffset" : 5
    }, {
      "referenceID" : 15,
      "context" : "To prove the inequality in Lemma 1, we follow the idea of peeling process [16].",
      "startOffset" : 74,
      "endOffset" : 78
    }, {
      "referenceID" : 12,
      "context" : "2 from [13]).",
      "startOffset" : 7,
      "endOffset" : 11
    }, {
      "referenceID" : 26,
      "context" : "We note that this self-bounding property has been used in [27] to get better (optimistic) rates of convergence for non-negative smooth losses.",
      "startOffset" : 58,
      "endOffset" : 62
    } ],
    "year" : 2017,
    "abstractText" : "In this paper we consider learning in passive setting but with a slight modification. We assume that the target expected loss, also referred to as target risk, is provided in advance for learner as prior knowledge. Unlike most studies in the learning theory that only incorporate the prior knowledge into the generalization bounds, we are able to explicitly utilize the target risk in the learning process. Our analysis reveals a surprising result on the sample complexity of learning: by exploiting the target risk in the learning algorithm, we show that when the loss function is both strongly convex and smooth, the sample complexity reduces to O(log ( 1 ǫ ) ), an exponential improvement compared to the sample complexity O( 1 ǫ ) for learning with strongly convex loss functions. Furthermore, our proof is constructive and is based on a computationally efficient stochastic optimization algorithm for such settings which demonstrate that the proposed algorithm is practically useful.",
    "creator" : "LaTeX with hyperref package"
  }
}