{
  "name" : "1605.09066.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Distributed Asynchronous Stochastic Dual Coordinate Ascent without Duality",
    "authors" : [ "Zhouyuan Huo" ],
    "emails" : [ "zhouyuan.huo@mavs.uta.edu", "heng@uta.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n60 5.\n09 06\n6v 1\n[ cs\n.L G\n] 2\n1 Introduction\nWe consider the following ℓ2-norm regularized loss minimization problem:\nmin w∈Rd\nP (w) = 1\nn\nn∑\ni=1\nφi(w) + λ\n2 ‖w‖2 . (1)\nMany optimization methods have been proposed to solve this problem including [6, 15, 2, 12, 13, 14, 16, 18]. Experimental results in [18] verify that SDCA method enjoys strong theoretical convergence guarantee properties and often has better performances than stochastic gradient descent (SGD) based methods. In [6], the paper points out that SDCA is a variation of SGD method, and its update is based on an unbiased estimate of gradient. Unlike most of the SGD methods which solve primal problem directly, as its name indicates, SDCA is derived by considering a dual problem of (1). However, the dual problem of φi is meaningless sometimes. In [13], a variation of SDCA was proposed and applied to problems in which individual φi is non-convex.\nRecently, as the size of data and model grows larger and larger, many distributed optimization algorithms have been proposed to solve large-scale problems [9, 21, 22, 11, 8, 1, 3, 10, 4, 20, 7, 19, 5, 17]. There are mainly two architectures in distributed system: one is shared-memory architecture, and the other one is distributed-memory\narchitecture. In this paper, we only consider distributed-memory architecture. In [19, 5, 17], distributed SDCA method was proposed with proved linear convergence when φi is smooth and convex.\nIn this paper, we propose a Distributed Asynchronous Dual-Free Coordinate Ascent (Asy-df SDCA) method. The corresponding convergence analysis is provided on two different assumptions: one is that φi is L-smooth and convex, and the other one is that φi is L-smooth and non-convex, but the average of φi is strongly convex.\n2 Asynchronous Dual Free Stochastic Dual Coordinate Ascent Method\nDetails of our proposed Distributed Asynchronous Dual-Free Coordinate Ascent method (Asy-df SDCA) are described in Algorithms (1) and (2). Algorithm (1) presents the pseudo code of Asy-df SDCA on each worker node. αi ∈ Rd, i ∈ {1, · · · , n} denotes pseudo-dual vector for each sample, and they are maintained by workers. We assume datasets are evenly distributed in K workers, and there are nk samples in worker k. Algorithm (1) summarizes the pseudo code on server node. Parameter w is maintained in the server, and vi represents update received from workers in each iteration.\nAlgorithm 1 Asy-df SDCA (Worker k)\nInitialize α0,0i ∈ R d, i ∈ {1, · · · , nk} for s = 1, 2, · · · , S do for t = 1, 2, · · · , nk do\nPull ws,t−τ from server. Randomly select sample i from {1, · · · , nk}; v s,t i = ∇φi(w\ns,t−τ ) + αs,t−τi Update αs,ti = α s,t−1 i − ληnv s,t i\nPush vs,ti to server. end for\nend for\nAlgorithm 2 Asy-df SDCA (Server)\nInitialize w0,0 ∈ Rd. for s = 1, 2, · · · , S do\nfor t = 1, 2, · · · , n do Receive vs,ti from worker. Update ws,t = ws,t−1 − ηvs,ti end for ws+1,0 = ws,n\nend for\n3 Convergence Analysis\nWe provide convergence analysis of our proposed method on two different cases: (1) φi is L-smooth and convex, and (2) φi is L-smooth and non-convex, but the average of φi is strongly convex.\n3.1 Convex Case\nFor further analysis, in this section, we make the following assumptions for problem (1). All of them are common assumptions in the theoretical analysis of distributed methods and stochastic gradient method.\nAssumption 1 We assume the following conditions hold:\n• φi is L-smooth,\n‖∇φ(x) −∇φi(y)‖ 2 ≤ L2‖x− y‖2 . (2)\n• φi is convex,\nφi(x) ≥ φi(y) +∇φi(y) T (x − y) . (3)\n• Time delay τ is no larger than ∆.\nFollowing the above assumptions, we know that our method is able to have linear convergence rate with the following theorem.\nTheorem 1 When the above assumptions satisfy, and let w∗ be the minimizer of P (w), α∗i = −∇φi(w ∗). If η ≤ 1 2L+nλ+4L∆ , then we have\nE\n[\n‖ws,0 − w∗‖2 + 1 2λL\nn∑\ni=1\n‖αs,0i − α ∗ i ‖ 2\n]\n≤ e−ηλs [\n‖w0,0 − w∗‖2 + 1 2λL\nn∑\ni=1\n‖α0,0i − α ∗ i ‖ 2\n]\n. (4)\n3.2 Non-Convex Case\nFor further analysis, in this section, we make the following assumptions for problem (1).\nAssumption 2 We assume the following conditions holds:\n• φi is L-smooth,\n‖∇φ(x) −∇φi(y)‖ 2 ≤ L2‖x− y‖2 . (5)\n• φi is non-convex, and the average of φi is γ-strongly convex,\n1\nn\nn∑\ni=1\nφi(x) ≥ 1\nn\nn∑\ni=1\nφi(y) + 1\nn\nn∑\ni=1\n∇φi(y) T (x − y) +\nγ 2 ‖x− y‖2 . (6)\n• Time delay τ is no larger than ∆.\nFollowing the above assumptions, we know that our method is able to have linear convergence rate with the following theorem.\nTheorem 2 When the above assumptions satisfy, and let w∗ be the minimizer of P (w), α∗i = −∇φi(w\n∗). If η satisfies that: (\n∆2 + 2γ∆2\nγ − λ\n)\nη2 +\n( 2∆+ 1\nγ − λ +\nn\n2L2\n)\nη − 1\n2L2 ≤ 0 , (7)\nthen we have the final conclusion:\nE\n[\n1 γ−λ ‖ws,0 − w∗‖2 + 1 2λL2n\nn∑\ni=1\n‖αs,0i − α ∗ i ‖ 2\n]\n≤ e−ηλs [ 1\nγ−λ ‖w0,0 − w∗‖2 + 1 2λL2n\nn∑\ni=1\n‖α0,0i − α ∗ i ‖ 2\n]\n. (8)\nReferences\n[1] Alekh Agarwal and John C Duchi. Distributed delayed stochastic optimization. In Advances in Neural Information Processing Systems, pages 873–881, 2011.\n[2] Aaron Defazio, Francis Bach, and Simon Lacoste-Julien. Saga: A fast incremental gradient method with support for non-strongly convex composite objectives. In Advances in Neural Information Processing Systems, pages 1646–1654, 2014.\n[3] Mingyi Hong. A distributed, asynchronous and incremental algorithm for nonconvex optimization: An admm based approach. arXiv preprint arXiv:1412.6058, 2014.\n[4] Zhouyuan Huo and Heng Huang. Asynchronous stochastic gradient descent with variance reduction for non-convex optimization. arXiv preprint arXiv:1604.03584, 2016.\n[5] Martin Jaggi, Virginia Smith, Martin Takác, Jonathan Terhorst, Sanjay Krishnan, Thomas Hofmann, and Michael I Jordan. Communication-efficient distributed dual coordinate ascent. In Advances in Neural Information Processing Systems, pages 3068–3076, 2014.\n[6] Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance reduction. In Advances in Neural Information Processing Systems, pages 315–323, 2013.\n[7] John Langford, Alexander Smola, and Martin Zinkevich. Slow learners are fast. arXiv preprint arXiv:0911.0491, 2009.\n[8] Mu Li, David G Andersen, Alex J Smola, and Kai Yu. Communication efficient distributed machine learning with the parameter server. In Advances in Neural Information Processing Systems, pages 19–27, 2014.\n[9] Xiangru Lian, Yijun Huang, Yuncheng Li, and Ji Liu. Asynchronous parallel stochastic gradient for nonconvex optimization. In Advances in Neural Information Processing Systems, pages 2719–2727, 2015.\n[10] Ji Liu, Stephen J Wright, and Srikrishna Sridhar. An asynchronous parallel randomized kaczmarz algorithm. arXiv preprint arXiv:1401.4780, 2014.\n[11] Benjamin Recht, Christopher Re, Stephen Wright, and Feng Niu. Hogwild: A lock-free approach to parallelizing stochastic gradient descent. In Advances in Neural Information Processing Systems, pages 693–701, 2011.\n[12] Mark Schmidt, Nicolas Le Roux, and Francis Bach. Minimizing finite sums with the stochastic average gradient. arXiv preprint arXiv:1309.2388, 2013.\n[13] Shai Shalev-Shwartz. Sdca without duality. arXiv preprint arXiv:1502.06177, 2015.\n[14] Shai Shalev-Shwartz and Tong Zhang. Accelerated mini-batch stochastic dual coordinate ascent. In Advances in Neural Information Processing Systems, pages 378–385, 2013.\n[15] Shai Shalev-Shwartz and Tong Zhang. Stochastic dual coordinate ascent methods for regularized loss. The Journal of Machine Learning Research, 14(1):567–599, 2013.\n[16] Martin Takáč, Avleen Bijral, Peter Richtárik, and Nathan Srebro. Mini-batch primal and dual methods for svms. arXiv preprint arXiv:1303.2314, 2013.\n[17] Martin Takác, Peter Richtárik, and Nathan Srebro. Distributed mini-batch sdca. arXiv preprint arXiv:1507.08322, 2015.\n[18] Lin Xiao and Tong Zhang. A proximal stochastic gradient method with progressive variance reduction. SIAM Journal on Optimization, 24(4):2057–2075, 2014.\n[19] Tianbao Yang. Trading computation for communication: Distributed stochastic dual coordinate ascent. In Advances in Neural Information Processing Systems, pages 629–637, 2013.\n[20] Ruiliang Zhang and James Kwok. Asynchronous distributed admm for consensus optimization. In Proceedings of the 31st International Conference on Machine Learning (ICML-14), pages 1701–1709, 2014.\n[21] Ruiliang Zhang, Shuai Zheng, and James T Kwok. Fast distributed asynchronous sgd with variance reduction. arXiv preprint arXiv:1508.01633, 2015.\n[22] Shen-Yi Zhao and Wu-Jun Li. Fast asynchronous parallel stochastic gradient descent: A lock-free approach with convergence guarantee. 2016.\nA Proof of Theorem 1\nLemma 3 Assuming that each φi is L-smooth and convex, for every w, we have:\n1\nn\nn∑\ni=1\n‖∇φi(w) −∇φi(w ∗)‖2 ≤ 2L\n(\nP (w)− P (w∗)− λ\n2 ‖w − w∗‖2\n)\n(9)\nThis lemma was proved in [13].\nProof 1 (Proof of Theorem 1) As per Lemma 3, we know that:\nE[‖ ∇φi(w t−τ ) + α∗i ‖ 2] ≤ E[‖ ∇φi(w t−τ )−∇φi(w t−1) +∇φi(w t−1)−∇φi(w ∗)‖2]\n≤ 2E[‖ ∇φi(w t−τ )−∇φi(w t−1)‖2] + 2E[‖ ∇φi(w t−1)−∇φi(w ∗)‖2]\n≤ 4L\n(\nP (wt−τ )− P (wt−1)− λ\n2 E[‖wt−τ − wt−1‖2]\n)\n+ 4L\n(\nP (wt−1)− P (w∗)− λ\n2 E[‖wt−1 − w∗‖2]\n)\n≤ 4L(P (wt−τ )− P (w∗))− 2λLE[‖wt−1 − w∗‖2] (10)\nE[(wt−1 − w∗)T vti ] = E[(w t−1 − wt−τ + wt−τ − w∗)T vti ]\n= η\nt−1∑\nj=t−τ+1\nE[(vjij ) T vti ] + (w t−τ − w∗)∇P (wt−τ )\n≥ η\nt−1∑\nj=t−τ+1\nE[(vjij ) T vti ] + P (w t−τ )− P (w∗) (11)\nwhere the final inequality follows from convexity of P (w). Let’s define Ct = caAt + cbBt, and take expectation over i:\nE[Ct] = E\n[\nca(1− ηλ)At−1 + caηλ‖∇φi(w t−τ ) + α∗i ‖ 2 − caηλ(1 − β)‖v t i‖ 2\n+ cbBt−1 − 2cbη(w t−1 − w∗)T vti + cbη 2‖vti‖ 2\n]\n≤ E\n[\nca(1− ηλ)At−1 + caηλ\n(\n4L(P (wt−τ )− P (w∗))− 2λLE[‖wt−1 − w∗‖2]\n)\n− caηλ(1 − β)‖v t i‖ 2 + cbBt−1 + cbη 2‖vti‖ 2\n− 2cbη\n\nη\nt−1∑\nj=t−τ+1\nE[(vjij ) T vti ] + P (w t−τ )− P (w∗)\n\n\n]\n≤ ca(1 − ηλ)E[At−1] + (cb − 2caηLλ 2)E[Bt−1] + ( cbη 2 − caηλ(1 − β) + cb∆η 2 ) E[‖vti‖ 2]\n+ (4caηλL − 2cbη) ( P (wt−τ )− P (w∗) ) + cbη 2\nt−1∑\nj=t−τ+1\nE[‖vjij‖ 2] (12)\nSumming over E[Ct], we have:\nn∑\nt=1\nE[Ct] ≤\nn∑\nt=1\n( ca(1− ηλ)E[At−1] + (cb − 2caηLλ 2)E[Bt−1] )\n+ n∑\nt=1\n(4caηλL − 2cbη) ( P (wt−τ )− P (w∗) )\n+\nn∑\nt=1\n( cbη 2 − caηλ(1 − β) + 2cb∆η 2 ) E[‖vti‖ 2] (13)\nWe denote\ncb − 2caηLλ 2 = cb(1− ηλ) (14)\ncbη 2 − caηλ(1 − β) + 2cb∆η 2 ≤ 0 . (15)\nTherefore, if cb = 2caλL and η ≤ 12L+nλ+4L∆ , we have:\nn∑\nt=1\nE[Ct] ≤ (1− ηλ) n∑\nt=1\nE[Ct − 1]\n≤\nn∑\nt=2\nE[Ct − 1] + (1− ηλ)E[C0] (16)\nThus\nE[Cn] ≤ (1− ηλ)E[C0] (17)\nBecause E[Cn] = E[Cs+1,0] and E[C0] = E[Cs,0], we have:\nE[Cs,0] ≤ (1− ηλ)E[Cs−1,0]\n≤ (1− ηλ)sC0,0 ≤ e−ηλsC0,0 (18)\nLet ca = 12Lλ and cb = 1, then we have the final conclusion:\nE\n[\n‖ws,0 − w∗‖2 + 1 2λL\nn∑\ni=1\n‖αs,0i − α ∗ i ‖ 2\n]\n≤ e−ηλs [\n‖w0,0 − w∗‖2 + 1 2λL\nn∑\ni=1\n‖α0,0i − α ∗ i ‖ 2\n]\n(19)\nB Proof of Theorem 2\nProof 2 (Proof of Theorem 2) Let w∗ be the minimizer ofP (w) and letα∗i = −∇φi(w ∗).\nu s,t i = ∇φi(w s,t−1) + αs,t−1i (20)\nv s,t i = ∇φi(w s,t−τ ) + αs,t−τi (21)\nBecause αi will be not updated in the process from t− τ to t− 1, so α s,t−τ i = α s,t−1 i . In an epoch s, we use wt to denote ws,t, αti to denote α s,t i , At, Bt, Ct to denote As,t, Bs,t, Cs,t.\nAt = 1\nn\nn∑\ni=1\n‖αti − α ∗ i ‖ 2 (22)\nBt = ‖w t − w∗‖2 (23)\nLet β = ηλn, so in iteration t, αti = (1− β)α t−1 i + β(−∇φi(w t−τ )).\nAt −At−1 = 1\nn ‖αti − α ∗ i ‖ 2 −\n1 n ‖αt−1i − α ∗ i ‖ 2\n= 1\nn ‖(1− β)(αt−1i − α ∗ i ) + β(−∇φi(w t−τ )− α∗i )‖\n2 − 1\nn ‖αt−1i − α ∗ i ‖ 2\n= 1\nn\n(\n(1− β)‖αt−1i − α ∗ i ‖ 2 + β‖ − ∇φi(w t−τ )− α∗i ‖ 2\n− β(1 − β)‖αt−1i +∇φi(w t−τ )‖2 − ‖αt−1i − α ∗ i ‖ 2\n)\n= β\nn\n(\n−‖αt−1i − α ∗ i ‖ 2 + ‖∇φi(w t−τ ) + α∗i ‖ 2 − (1 − β)‖vti‖ 2\n)\n= ηλ\n(\n−‖αt−1i − α ∗ i ‖ 2 + ‖∇φi(w t−τ ) + α∗i ‖ 2 − (1− β)‖vti‖ 2\n)\n(24)\nIn addition,\nBt − Bt−1 = ‖w t − w∗‖2 − ‖wt−1 − w∗‖2\n= ‖wt−1 − ηvti − w ∗‖2 − ‖wt−1 − w∗‖2 = −2η(wt−1 − w∗)T vti + η 2‖vti‖ 2 (25)\n‖∇φi(w t−τ ) + α∗i ‖ 2 = ‖∇φi(w t−τ )−∇φi(w t−1) +∇φi(w t−1)−∇φi(w ∗)‖2\n≤ 2‖∇φi(w t−τ )−∇φi(w t−1)‖2 + 2‖∇φi(w t−1)−∇φi(w ∗)‖2 ≤ 2L2‖wt−τ − wt−1‖2 + 2L2‖wt−1 − w∗‖2\n≤ 2L2‖\nt−1∑\nj=t−τ+1\n(wj − wj−1)‖2 + 2L2‖wt−1 − w∗‖2\n≤ 2L2∆\nt−1∑\nj=t−τ+1\n‖wj − wj−1‖2 + 2L2‖wt−1 − w∗‖2\n≤ 2∆η2L2 t−1∑\nj=t−τ+1\n‖vjij‖ 2 + 2L2‖wt−1 − w∗‖2 (26)\nwhere the first and fourth inequality follows from that ‖ n∑\ni=1\nai‖ 2 ≤ n\nn∑\ni=1\n‖ai‖ 2. The\nsecond inequality follows that φi is L-smooth. ∆ is the upper bound of time delay.\n(wt−1 − w∗)T vti = (w t−1 − wt−τ + wt−τ − w∗)T vti\n= (wt−1 − wt−τ )T vti ︸ ︷︷ ︸\nT1\n+(wt−τ − w∗)T vti ︸ ︷︷ ︸\nT2\n(27)\nBecause E[vti ] = ∇P (w t−τ ), we have:\nE[T2] = E[(w t−τ − w∗)T vti ]\n= (wt−τ − w∗)T∇P (wt−τ )\n≥ γ‖wt−τ − w∗‖2\n≥ γ( 1\n2 ‖wt−1 − w∗‖2 − ‖wt−τ − wt−1‖2)\n≥ γ\n2 ‖wt−1 − w∗‖2 −∆γη2\nt−1∑\nj=t−τ+1\n‖vjij‖ 2 (28)\nwhere the first inequality follows from the strong convexity of P (w),\n(w∗ − wt−τ )T∇P (wt−τ ) ≥ P (wt−τ )− P (w∗) + γ\n2 ‖wt−τ − w∗‖2 (29)\nP (wt−τ )− P (w∗) ≥ γ\n2 ‖wt−τ − w∗‖2 (30)\nThe second inequality follows from the inequality,\n‖wt−1 − w∗‖2 = ‖wt−1 − wt−τ + wt−τ − w∗‖2\n≤ 2‖wt−1 − wt−τ‖2 + 2‖wt−τ − w∗‖2 (31)\nT1 = (w t−1 − wt−τ )T vti\n= (\nt−1∑\nj=t−τ+1\n(wj − wj−1))T vti\n= η t−1∑\nj=t−τ+1\n(vjij ) T vti (32)\nWe define Ct = caAt + cbBt, and take expectation over i,\nE[Ct] = E\n[\nca ( At−1 + ηλ ( −‖αt−1i − α ∗ i ‖ 2 + ‖∇φi(w t−τ ) + α∗i ‖ 2 − (1− β)‖vti‖ 2 ))\n+ cb ( Bt−1 − 2η(w t−1 − w∗)T vti + η 2‖vti‖\n2 ) ]\n= E\n[\nca(1− ηλ)At−1 + caηλ‖∇φi(w t−τ ) + α∗i ‖ 2 − caηλ(1 − β)‖v t i‖ 2\n+ cbBt−1 − 2cbη(w t−1 − w∗)T vti + cbη 2‖vti‖ 2\n]\n≤ E\n[\nca(1− ηλ)At−1 + caηλ\n 2∆η2L2 t−1∑\nj=t−τ+1\n‖vjij‖ 2 + 2L2‖wt−1 − w∗‖2\n\n\n− caηλ(1 − β)‖v t i‖ 2 + cbBt−1 + cbη 2‖vti‖ 2\n− 2cbη\n\n γ\n2 ‖wt−1 − w∗‖2 −∆γη2\nt−1∑\nj=t−τ+1\n‖vjij‖ 2 + η\nt−1∑\nj=t−τ+1\n(vjij ) T vti\n\n\n]\n≤ ca(1 − ηλ)E[At−1] + (2caηλL 2 + cb − cbηγ)E[Bt−1] + ( −caηλ(1 − β) + cbη 2 + cb∆η 2 ) E[‖vti‖ 2]\n+ (2ca∆λL 2η3 + 2cb∆γη 3 + cbη 2)\nt−1∑\nj=t−τ+1\nE[‖vjij‖ 2] (33)\nwhere the first equality follows from E[‖αt−1i − α ∗ i ‖ 2] = At−1.\nSumming over E[Ct] from t = 1 to n, we obtain:\nn∑\nt=1\nE[Ct] ≤\nn∑\nt=1\n( ca(1− ηλ)E[At−1] + (2caηλL 2 + cb − cbηγ)E[Bt−1] )\n(34)\n+\nn∑\nt=1\n( 2caλ∆ 2L2η3 + 2cbγ∆ 2η3 + 2cb∆η 2 + cbη 2 − caηλ(1 − β) ) E[‖vti‖ 2]\nIf two following inequalities hold\n2caηλL 2 + cb − cbηγ = cb(1 − ηλ) (35)\n2caλ∆ 2L2η2 + 2cbγ∆ 2η2 + 2cb∆η + cbη − caλ(1 − nλη) ≤ 0 (36)\nthen n∑\nt=1\nE[Ct] ≤ (1− ηλ)\nn∑\nt=1\nE[Ct−1]\n≤ n∑\nt=2\nE[Ct−1] + (1− ηλ)E[C0] (37)\nBecause E[Cn] = E[Cs+1,0] and E[C0] = E[Cs,0], thus,\nE[Cs,0] ≤ (1− ηλ)E[Cs−1,0]\n≤ (1− ηλ)sC0,0 ≤ e−ηλsC0,0 (38)\nWe set ca = 12λL2 and cb = 1 γ−λ . If η = 0, the left-side value in (36) is −caλ, and it is smaller than 0. Thus there exists a value η > 0 to make the inequality (36) hold. Thus, we have the final conclusion. If η satisfies the following inequality:\n(\n∆2 + 2γ∆2\nγ − λ\n)\nη2 +\n( 2∆+ 1\nγ − λ +\nn\n2L2\n)\nη − 1\n2L2 ≤ 0 , (39)\nthen\nE\n[\n1 γ−λ ‖ws,0 − w∗‖2 + 1 2λL2n\nn∑\ni=1\n‖αs,0i − α ∗ i ‖ 2\n]\n≤ e−ηλs [ 1\nγ−λ ‖w0,0 − w∗‖2 + 1 2λL2n\nn∑\ni=1\n‖α0,0i − α ∗ i ‖ 2\n]\n. (40)"
    } ],
    "references" : [ {
      "title" : "Distributed delayed stochastic optimization",
      "author" : [ "Alekh Agarwal", "John C Duchi" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2011
    }, {
      "title" : "Saga: A fast incremental gradient method with support for non-strongly convex composite objectives",
      "author" : [ "Aaron Defazio", "Francis Bach", "Simon Lacoste-Julien" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2014
    }, {
      "title" : "A distributed, asynchronous and incremental algorithm for nonconvex optimization: An admm based approach",
      "author" : [ "Mingyi Hong" ],
      "venue" : "arXiv preprint arXiv:1412.6058,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2014
    }, {
      "title" : "Asynchronous stochastic gradient descent with variance reduction for non-convex optimization",
      "author" : [ "Zhouyuan Huo", "Heng Huang" ],
      "venue" : "arXiv preprint arXiv:1604.03584,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2016
    }, {
      "title" : "Communication-efficient distributed dual coordinate ascent",
      "author" : [ "Martin Jaggi", "Virginia Smith", "Martin Takác", "Jonathan Terhorst", "Sanjay Krishnan", "Thomas Hofmann", "Michael I Jordan" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2014
    }, {
      "title" : "Accelerating stochastic gradient descent using predictive variance reduction",
      "author" : [ "Rie Johnson", "Tong Zhang" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2013
    }, {
      "title" : "Slow learners are fast",
      "author" : [ "John Langford", "Alexander Smola", "Martin Zinkevich" ],
      "venue" : "arXiv preprint arXiv:0911.0491,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2009
    }, {
      "title" : "Communication efficient distributed machine learning with the parameter server",
      "author" : [ "Mu Li", "David G Andersen", "Alex J Smola", "Kai Yu" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2014
    }, {
      "title" : "Asynchronous parallel stochastic gradient for nonconvex optimization",
      "author" : [ "Xiangru Lian", "Yijun Huang", "Yuncheng Li", "Ji Liu" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2015
    }, {
      "title" : "An asynchronous parallel randomized kaczmarz algorithm",
      "author" : [ "Ji Liu", "Stephen J Wright", "Srikrishna Sridhar" ],
      "venue" : "arXiv preprint arXiv:1401.4780,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2014
    }, {
      "title" : "Hogwild: A lock-free approach to parallelizing stochastic gradient descent",
      "author" : [ "Benjamin Recht", "Christopher Re", "Stephen Wright", "Feng Niu" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2011
    }, {
      "title" : "Minimizing finite sums with the stochastic average gradient",
      "author" : [ "Mark Schmidt", "Nicolas Le Roux", "Francis Bach" ],
      "venue" : "arXiv preprint arXiv:1309.2388,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2013
    }, {
      "title" : "Sdca without duality",
      "author" : [ "Shai Shalev-Shwartz" ],
      "venue" : "arXiv preprint arXiv:1502.06177,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2015
    }, {
      "title" : "Accelerated mini-batch stochastic dual coordinate ascent",
      "author" : [ "Shai Shalev-Shwartz", "Tong Zhang" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2013
    }, {
      "title" : "Stochastic dual coordinate ascent methods for regularized loss",
      "author" : [ "Shai Shalev-Shwartz", "Tong Zhang" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2013
    }, {
      "title" : "Mini-batch primal and dual methods for svms",
      "author" : [ "Martin Takáč", "Avleen Bijral", "Peter Richtárik", "Nathan Srebro" ],
      "venue" : "arXiv preprint arXiv:1303.2314,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2013
    }, {
      "title" : "Distributed mini-batch sdca",
      "author" : [ "Martin Takác", "Peter Richtárik", "Nathan Srebro" ],
      "venue" : "arXiv preprint arXiv:1507.08322,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2015
    }, {
      "title" : "A proximal stochastic gradient method with progressive variance reduction",
      "author" : [ "Lin Xiao", "Tong Zhang" ],
      "venue" : "SIAM Journal on Optimization,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2014
    }, {
      "title" : "Trading computation for communication: Distributed stochastic dual coordinate ascent",
      "author" : [ "Tianbao Yang" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2013
    }, {
      "title" : "Asynchronous distributed admm for consensus optimization",
      "author" : [ "Ruiliang Zhang", "James Kwok" ],
      "venue" : "In Proceedings of the 31st International Conference on Machine Learning",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2014
    }, {
      "title" : "Fast distributed asynchronous sgd with variance reduction",
      "author" : [ "Ruiliang Zhang", "Shuai Zheng", "James T Kwok" ],
      "venue" : "arXiv preprint arXiv:1508.01633,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 5,
      "context" : "Many optimization methods have been proposed to solve this problem including [6, 15, 2, 12, 13, 14, 16, 18].",
      "startOffset" : 77,
      "endOffset" : 107
    }, {
      "referenceID" : 14,
      "context" : "Many optimization methods have been proposed to solve this problem including [6, 15, 2, 12, 13, 14, 16, 18].",
      "startOffset" : 77,
      "endOffset" : 107
    }, {
      "referenceID" : 1,
      "context" : "Many optimization methods have been proposed to solve this problem including [6, 15, 2, 12, 13, 14, 16, 18].",
      "startOffset" : 77,
      "endOffset" : 107
    }, {
      "referenceID" : 11,
      "context" : "Many optimization methods have been proposed to solve this problem including [6, 15, 2, 12, 13, 14, 16, 18].",
      "startOffset" : 77,
      "endOffset" : 107
    }, {
      "referenceID" : 12,
      "context" : "Many optimization methods have been proposed to solve this problem including [6, 15, 2, 12, 13, 14, 16, 18].",
      "startOffset" : 77,
      "endOffset" : 107
    }, {
      "referenceID" : 13,
      "context" : "Many optimization methods have been proposed to solve this problem including [6, 15, 2, 12, 13, 14, 16, 18].",
      "startOffset" : 77,
      "endOffset" : 107
    }, {
      "referenceID" : 15,
      "context" : "Many optimization methods have been proposed to solve this problem including [6, 15, 2, 12, 13, 14, 16, 18].",
      "startOffset" : 77,
      "endOffset" : 107
    }, {
      "referenceID" : 17,
      "context" : "Many optimization methods have been proposed to solve this problem including [6, 15, 2, 12, 13, 14, 16, 18].",
      "startOffset" : 77,
      "endOffset" : 107
    }, {
      "referenceID" : 17,
      "context" : "Experimental results in [18] verify that SDCA method enjoys strong theoretical convergence guarantee properties and often has better performances than stochastic gradient descent (SGD) based methods.",
      "startOffset" : 24,
      "endOffset" : 28
    }, {
      "referenceID" : 5,
      "context" : "In [6], the paper points out that SDCA is a variation of SGD method, and its update is based on an unbiased estimate of gradient.",
      "startOffset" : 3,
      "endOffset" : 6
    }, {
      "referenceID" : 12,
      "context" : "In [13], a variation of SDCA was proposed and applied to problems in which individual φi is non-convex.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 8,
      "context" : "Recently, as the size of data and model grows larger and larger, many distributed optimization algorithms have been proposed to solve large-scale problems [9, 21, 22, 11, 8, 1, 3, 10, 4, 20, 7, 19, 5, 17].",
      "startOffset" : 155,
      "endOffset" : 204
    }, {
      "referenceID" : 20,
      "context" : "Recently, as the size of data and model grows larger and larger, many distributed optimization algorithms have been proposed to solve large-scale problems [9, 21, 22, 11, 8, 1, 3, 10, 4, 20, 7, 19, 5, 17].",
      "startOffset" : 155,
      "endOffset" : 204
    }, {
      "referenceID" : 10,
      "context" : "Recently, as the size of data and model grows larger and larger, many distributed optimization algorithms have been proposed to solve large-scale problems [9, 21, 22, 11, 8, 1, 3, 10, 4, 20, 7, 19, 5, 17].",
      "startOffset" : 155,
      "endOffset" : 204
    }, {
      "referenceID" : 7,
      "context" : "Recently, as the size of data and model grows larger and larger, many distributed optimization algorithms have been proposed to solve large-scale problems [9, 21, 22, 11, 8, 1, 3, 10, 4, 20, 7, 19, 5, 17].",
      "startOffset" : 155,
      "endOffset" : 204
    }, {
      "referenceID" : 0,
      "context" : "Recently, as the size of data and model grows larger and larger, many distributed optimization algorithms have been proposed to solve large-scale problems [9, 21, 22, 11, 8, 1, 3, 10, 4, 20, 7, 19, 5, 17].",
      "startOffset" : 155,
      "endOffset" : 204
    }, {
      "referenceID" : 2,
      "context" : "Recently, as the size of data and model grows larger and larger, many distributed optimization algorithms have been proposed to solve large-scale problems [9, 21, 22, 11, 8, 1, 3, 10, 4, 20, 7, 19, 5, 17].",
      "startOffset" : 155,
      "endOffset" : 204
    }, {
      "referenceID" : 9,
      "context" : "Recently, as the size of data and model grows larger and larger, many distributed optimization algorithms have been proposed to solve large-scale problems [9, 21, 22, 11, 8, 1, 3, 10, 4, 20, 7, 19, 5, 17].",
      "startOffset" : 155,
      "endOffset" : 204
    }, {
      "referenceID" : 3,
      "context" : "Recently, as the size of data and model grows larger and larger, many distributed optimization algorithms have been proposed to solve large-scale problems [9, 21, 22, 11, 8, 1, 3, 10, 4, 20, 7, 19, 5, 17].",
      "startOffset" : 155,
      "endOffset" : 204
    }, {
      "referenceID" : 19,
      "context" : "Recently, as the size of data and model grows larger and larger, many distributed optimization algorithms have been proposed to solve large-scale problems [9, 21, 22, 11, 8, 1, 3, 10, 4, 20, 7, 19, 5, 17].",
      "startOffset" : 155,
      "endOffset" : 204
    }, {
      "referenceID" : 6,
      "context" : "Recently, as the size of data and model grows larger and larger, many distributed optimization algorithms have been proposed to solve large-scale problems [9, 21, 22, 11, 8, 1, 3, 10, 4, 20, 7, 19, 5, 17].",
      "startOffset" : 155,
      "endOffset" : 204
    }, {
      "referenceID" : 18,
      "context" : "Recently, as the size of data and model grows larger and larger, many distributed optimization algorithms have been proposed to solve large-scale problems [9, 21, 22, 11, 8, 1, 3, 10, 4, 20, 7, 19, 5, 17].",
      "startOffset" : 155,
      "endOffset" : 204
    }, {
      "referenceID" : 4,
      "context" : "Recently, as the size of data and model grows larger and larger, many distributed optimization algorithms have been proposed to solve large-scale problems [9, 21, 22, 11, 8, 1, 3, 10, 4, 20, 7, 19, 5, 17].",
      "startOffset" : 155,
      "endOffset" : 204
    }, {
      "referenceID" : 16,
      "context" : "Recently, as the size of data and model grows larger and larger, many distributed optimization algorithms have been proposed to solve large-scale problems [9, 21, 22, 11, 8, 1, 3, 10, 4, 20, 7, 19, 5, 17].",
      "startOffset" : 155,
      "endOffset" : 204
    }, {
      "referenceID" : 18,
      "context" : "In [19, 5, 17], distributed SDCA method was proposed with proved linear convergence when φi is smooth and convex.",
      "startOffset" : 3,
      "endOffset" : 14
    }, {
      "referenceID" : 4,
      "context" : "In [19, 5, 17], distributed SDCA method was proposed with proved linear convergence when φi is smooth and convex.",
      "startOffset" : 3,
      "endOffset" : 14
    }, {
      "referenceID" : 16,
      "context" : "In [19, 5, 17], distributed SDCA method was proposed with proved linear convergence when φi is smooth and convex.",
      "startOffset" : 3,
      "endOffset" : 14
    }, {
      "referenceID" : 0,
      "context" : "References [1] Alekh Agarwal and John C Duchi.",
      "startOffset" : 11,
      "endOffset" : 14
    }, {
      "referenceID" : 1,
      "context" : "[2] Aaron Defazio, Francis Bach, and Simon Lacoste-Julien.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 2,
      "context" : "[3] Mingyi Hong.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 3,
      "context" : "[4] Zhouyuan Huo and Heng Huang.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 4,
      "context" : "[5] Martin Jaggi, Virginia Smith, Martin Takác, Jonathan Terhorst, Sanjay Krishnan, Thomas Hofmann, and Michael I Jordan.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 5,
      "context" : "[6] Rie Johnson and Tong Zhang.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 6,
      "context" : "[7] John Langford, Alexander Smola, and Martin Zinkevich.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 7,
      "context" : "[8] Mu Li, David G Andersen, Alex J Smola, and Kai Yu.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 8,
      "context" : "[9] Xiangru Lian, Yijun Huang, Yuncheng Li, and Ji Liu.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 9,
      "context" : "[10] Ji Liu, Stephen J Wright, and Srikrishna Sridhar.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 10,
      "context" : "[11] Benjamin Recht, Christopher Re, Stephen Wright, and Feng Niu.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 11,
      "context" : "[12] Mark Schmidt, Nicolas Le Roux, and Francis Bach.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 12,
      "context" : "[13] Shai Shalev-Shwartz.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 13,
      "context" : "[14] Shai Shalev-Shwartz and Tong Zhang.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 14,
      "context" : "[15] Shai Shalev-Shwartz and Tong Zhang.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 15,
      "context" : "[16] Martin Takáč, Avleen Bijral, Peter Richtárik, and Nathan Srebro.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 16,
      "context" : "[17] Martin Takác, Peter Richtárik, and Nathan Srebro.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 17,
      "context" : "[18] Lin Xiao and Tong Zhang.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 18,
      "context" : "[19] Tianbao Yang.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 19,
      "context" : "[20] Ruiliang Zhang and James Kwok.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 20,
      "context" : "[21] Ruiliang Zhang, Shuai Zheng, and James T Kwok.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 12,
      "context" : "i=1 ‖∇φi(w) −∇φi(w )‖ ≤ 2L ( P (w)− P (w)− λ 2 ‖w − w‖ ) (9) This lemma was proved in [13].",
      "startOffset" : 86,
      "endOffset" : 90
    } ],
    "year" : 2017,
    "abstractText" : "In this paper, we propose new Distributed Asynchronous Dual-Free Coordinate Ascent method (Asy-df SDCA), and provide the proof of convergence rate for two cases: the individual loss is convex and the individual loss is non-convex but its expected loss is convex. Stochastic Dual Coordinate Ascent (SDCA) model is a popular method and often has better performances than stochastic gradient descent methods in solving regularized convex loss minimization problems. Dual-Free Stochastic Dual Coordinate Ascent method is a variation of SDCA, and can be applied to non-convex problem when its dual problem is meaningless. We extend Dual-Free Stochastic Dual Coordinate Ascent method to the distributed mode with considering the star network in this paper.",
    "creator" : "LaTeX with hyperref package"
  }
}