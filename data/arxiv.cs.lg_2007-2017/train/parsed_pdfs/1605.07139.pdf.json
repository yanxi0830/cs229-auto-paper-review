{
  "name" : "1605.07139.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Fairness in Learning: Classic and Contextual Bandits∗",
    "authors" : [ "Matthew Joseph", "Michael Kearns", "Jamie Morgenstern", "Aaron Roth" ],
    "emails" : [ "majos@cis.upenn.edu.", "mkearns@cis.upenn.edu.", "jamiemor@cis.upenn.edu.", "aaroth@cis.upenn.edu." ],
    "sections" : [ {
      "heading" : null,
      "text" : "First, in the important special case of the classic stochastic bandits problem (i.e. in which there are no contexts), we provide a provably fair algorithm based on chained confidence intervals, and prove a cumulative regret bound with a cubic dependence on the number of arms. We further show that any fair algorithm must have such a dependence. When combined with regret bounds for standard non-fair algorithms such as UCB, this proves a strong separation between fair and unfair learning, which extends to the general contextual case.\nIn the general contextual case, we prove a tight connection between fairness and the KWIK (Knows What It Knows) learning model: a KWIK algorithm for a class of functions can be transformed into a provably fair contextual bandit algorithm, and conversely any fair contextual bandit algorithm can be transformed into a KWIK learning algorithm. This tight connection allows us to provide a provably fair algorithm for the linear contextual bandit problem with a polynomial dependence on the dimension, and to show (for a different class of functions) a worst-case exponential gap in regret between fair and non-fair learning algorithms.\n∗Department of Computer and Information Sciences, University of Pennsylvania. {majos,mkearns,jamiemor,aaroth}@cis.upenn.edu. AR is supported in part by an NSF CAREER award, a Sloan Foundation Fellowship, and a Google Faculty Research Award.\nar X\niv :1\n60 5.\n07 13\n9v 1\n[ cs\n.L G\n] 2\n3 M\nay 2\n01 6\nContents"
    }, {
      "heading" : "1 Introduction 3",
      "text" : "1.1 Fairness and Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 1.2 Our Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 1.3 Other Related Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6"
    }, {
      "heading" : "2 Preliminaries 6",
      "text" : "2.1 Specializing to Classic Stochastic Bandits . . . . . . . . . . . . . . . . . . . . . . . . 7"
    }, {
      "heading" : "3 Fair Classic Stochastic Bandits: An Algorithm 8",
      "text" : ""
    }, {
      "heading" : "4 Fair Classic Stochastic Bandits: A Lower Bound 11",
      "text" : ""
    }, {
      "heading" : "5 KWIK Learnability Implies Fair Bandit Learnability 15",
      "text" : ""
    }, {
      "heading" : "6 Fair Bandit Learnability Implies KWIK Learnability 18",
      "text" : "6.1 An Exponential Separation Between Fair and Unfair Learning . . . . . . . . . . . . . 20"
    }, {
      "heading" : "A Missing Proofs for the Classic Stochastic Bandits Upper Bound 23",
      "text" : "A.1 Missing Derivation of R(T ) for Theorem 2 . . . . . . . . . . . . . . . . . . . . . . . . 24"
    }, {
      "heading" : "B Missing Proofs for the Classic Stochastic Bandits Lower Bound 24",
      "text" : "C Missing Proofs for the Contextual Bandit Setting 26"
    }, {
      "heading" : "1 Introduction",
      "text" : "Automated techniques from statistics and machine learning are increasingly being used to make decisions that have important consequences on people’s lives, including hiring [Miller, 2015], lending [Byrnes, 2016], policing [Rudin, 2013], and even criminal sentencing [Barry-Jester et al., 2015]. These high stakes uses of machine learning have led to increasing concern in law and policy circles about the potential for (often opaque) machine learning techniques to be discriminatory or unfair [Coglianese and Lehr, 2016, Barocas and Selbst, 2016]. Moreover, these concerns are not merely hypothetical: Sweeney [2013] observed that contextual ads for public record services shown in response to Google searches for stereotypically African American names were more likely to contain text referring to arrest records, compared to comparable ads shown in response to searches for stereotypically Caucasian names, which showed more neutral text. She confirmed that this was not because of stated preferences of the advertisers, but rather the automated outcome of Google’s targeting algorithms. Despite the recognized importance of this problem, very little is known about technical solutions to the problem of “unfairness”, or the extent to which “fairness” is in conflict with the goals of learning.1\nIn this paper, we consider the extent to which a natural fairness notion is compatible with learning in a general setting (the contextual bandit setting), which can be used to model many of the applications mentioned above in which machine learning is currently employed. In this model, the learner is a sequential decision maker, which must choose at each time step t which decision to make, out of a finite set of k choices (for example, which of k loan applicants – potentially from different populations or racial groups – to give a loan to). Before the learner makes its decision at round t, it observes some context xtj for each choice of arm j (x t j could, for example, represent the contents of the loan application of an individual from population j at round t). When the learner chooses arm j at time t, it obtains a stochastic reward rtj whose expectation is determined\nby some unknown function of the context: E [ rtj ] = fj(x t j). The goal of the learning algorithm is to maximize its expected reward – i.e. to approximate the optimal policy, which at each round,\nchooses arm j to maximize E [ rtj ] . The difficulty in this task stems from the unknown functions fj which map contexts to rewards; these functions must be learned. Despite this, there are many known algorithms for learning the optimal policy (in the absence of any fairness constraint)."
    }, {
      "heading" : "1.1 Fairness and Learning",
      "text" : "Our notion of individual fairness is very simple: it states that it is unfair to preferentially choose one individual (e.g. for a loan, a job, admission to college, etc.) over another if he or she is not as qualified as the other individual. This definition of fairness is apt for our setting, since in contextual learning, the quality of an arm is clear: its expected reward. We view different arms\n1 For example, a 2014 White House report [Podesta et al., 2014] notes that “[t]he increasing use of algorithms to make eligibility decisions must be carefully monitored for potential discriminatory outcomes for disadvantaged groups, even absent discriminatory intent. . . additional research in measuring adverse outcomes due to the use of scores or algorithms is needed to understand the impacts these tools are having and will have in both the private and public sector as their use grows.” Along the same lines, a 2016 White House report [Munoz et al., 2016] observes that “[a]s improvements in the uses of big data and machine learning continue, it will remain important not to place too much reliance on these new systems without questioning and continuously testing the inputs and mechanics behind them and the results they produce.” Similarly, in a recent speech FTC Commissioner Julie Brill [Julie Brill, 2015] observed, “. . . a lot remains unknown about how big data-driven decisions may or may not use factors that are proxies for race, sex, or other traits that U.S. laws generally prohibit from being used in a wide range of commercial decisions . . . What can be done to make sure these products and services–and the companies that use them treat consumers fairly and ethically?”\nj as representing different populations (e.g. different ethnic groups, cultures, or other divisions within society), and view the context xtj at round t as representing information about a particular individual from that population. Each population has its own underlying function fj which maps contexts to expected payoff2. At each time step t, the algorithm is asked to choose between specific members of each population, represented by the contexts xtj . The quality of an individual is thus\nexactly E [ rtj ] = fj(x t j). Our fairness condition translates thus: for any pair of arms j, j ′ at time t, if fj(x t j) ≥ fj′(xtj′), then an algorithm is said to be discriminatory if it preferentially chooses the lower quality arm j′. Said another way, an algorithm is fair if it guarantees the following: with high probability, over all rounds t, and for all pairs of arms j, j′, whenever fj(x t j) ≥ fj′(xtj′), the algorithm chooses arm j with probability at least that with which it chooses arm j′3. It is worth noting that this definition of fairness (formalized in the preliminaries) is entirely consistent with the optimal policy, which can simply choose at each round to play uniformly at\nrandom from the arms arg maxj ( E [ rtj ]) which maximize the expected reward. This is because – it seems – the goal of fairness as enunciated above is entirely consistent with the goal of maximizing expected reward. Indeed, the fairness constraint exactly states that the algorithm cannot favor low reward arms!\nOur main conceptual result is that this intuition is incorrect in the face of unknown reward functions. Even though the constraint of fairness is consistent with implementing the optimal policy, it is not necessarily consistent with learning the optimal policy. We show that fairness always has a cost, in terms of the achievable learning rate of the algorithm. For some problems, the cost is mild, but for others, the cost is large."
    }, {
      "heading" : "1.2 Our Results",
      "text" : "We divide our results into two parts. First, we study the classic stochastic multi-armed bandit problem [Lai and Robbins, 1985, Katehakis and Robbins, 1995]. In this case, there are no contexts, and each arm i has a fixed but unknown average reward µi. Note that this is a special case of the contextual bandit problem in which the contexts are the same every day. In this setting, our fairness constraint specializes to require that with probability 1 − δ, for any pair of arms i, j for which µi ≥ µj , at no round t does the algorithm play arm j with probability higher than that with which it plays arm i. Note that even this special case models interesting scenarios from the point of view of fairness in learning. It models, for example, the case in which choices are made by a loan officer after applicants have been categorized into k internally indistinguishable equivalence classes based on their applications.\nWithout a fairness constraint, it is known that it is possible to guarantee non-trivial regret to the optimal policy after only T = O(k) many rounds [Auer et al., 2002]. In Section 3, we give an algorithm that satisfies our fairness constraint and is able to guarantee non-trivial regret after T = O(k3) rounds. We then show in Section 4 that it is not possible to do better – any\n2It is natural that different populations should have different underlying functions – for example, in a college admissions setting, the function mapping applications to college success probability might weight SAT scores less in a wealthy population that employs SAT tutors, and more in a working-class population that does not – see Dwork et al. [2012] for more discussion of this issue and Munoz et al. [2016] for examples.\n3Note that the definition does not require equality of outcomes on a population wide basis, also known as statistical parity. If some population j is less credit-worthy on average than another population j′, we do not necessarily say that an algorithm is discriminatory if it ends up giving fewer loans to individuals from population j. Our notion of discrimination is on an individual basis – it requires that even if population j is less credit worthy on average than population j′, if it happens that on some day, an individual appears from population j who is at least as credit worthy as the individual from population j′, then the algorithm cannot favor the individual from population j′.\nfair learning algorithm can be forced to endure constant per-round regret for T = Ω(k3) rounds. Thus, we tightly characterize the optimal regret attainable by fair algorithms in this setting, and formally separate it from the regret attainable by algorithms absent a fairness constraint. Note that this already shows a separation between the best possible learning rates for contextual bandit learning with and without the fairness constraint – the stochastic multi-armed bandit problem is a special case of every contextual bandit problem, and for general contextual bandit problems, it is also known how to get non-trivial regret after only T = O(k) many rounds [Agarwal et al., 2014, Beygelzimer et al., 2011, Chu et al., 2011].\nWe then move on to the general contextual bandit setting and prove a broad characterization result, relating fair contextual bandit learning to KWIK learning [Li et al., 2011]. The KWIK model, which stands for Knows What it Knows and has a close relationship with reinforcement learning, is a model of sequential supervised classification in which the learning algorithm must be confident in its predictions. Informally, a KWIK learning algorithm receives a sequence of unlabeled examples, whose true labels are defined by some unknown function in a class C. For each example, the algorithm may either predict a label, or announce “I Don’t Know”. The KWIK requirement is that with high probability, for each example, if the algorithm predicts a label, then its prediction must be very close to the true label. The quality of a KWIK learning algorithm is characterized by its “KWIK bound”, which provides an upper bound on the maximum number of times the algorithm can be forced to announce “I Don’t Know”. For any contextual bandit problem (defined by the set of functions C from which the payoff functions fj may be selected), we show that the optimal learning rate of any fair algorithm is determined by the best KWIK bound for the class C. We prove this constructively – we give a reduction showing how to convert a KWIK learning algorithm into a fair contextual bandit algorithm in Section 5, and vice versa in Section 6. Both reductions show that the KWIK bound of the KWIK algorithm is polynomially related to the regret of the fair algorithm.\nThis general connection has immediate implications, because it allows us to import known results for KWIK learning [Li et al., 2011]. For example, it implies that some fair contextual bandit problems are easy, in that there are fair algorithms which can obtain non-trivial regret guarantees after polynomially many rounds. This is the case, for example, for the important linear special case in which the contexts xtj ∈ Rd are real valued vectors and the unknown functions fj are linear: fj(x t j) = 〈θj , xtj〉4. In this case, the KWIK-learnability of noisy linear regression problems [Strehl and Littman, 2008, Li et al., 2011] implies that we can construct a fair contextual bandit algorithm whose per-round regret is polynomial in d. Conversely, it also implies that some contextual bandit problems which are easy without the fairness constraint become hard once we impose the fairness constraint, in that any fair algorithm must suffer constant per-round regret for exponentially many rounds. This is the case, for example, when the context consists of boolean vectors xtj ∈ {0, 1}d, and the unknown functions fj : {0, 1}d → {0, 1} are conjunctions – the “and”s of some unknown set of features5. The impossibility of non-trivial KWIK-learning of conjunctions [Li, 2009, Li et al., 2011] implies that no fair learner in the contextual bandit setting can achieve non-trivial regret before exponentially many (in d) rounds.\n4This corresponds to the case in which the probability that an individual pays back his or her loan is determined by a standard linear regression model.\n5For example, a conjunction might predict that an individual is likely to pay back his loan if all of the following conditions are satisfied: he or she has graduated from college, has a clean driving history, and has not previously defaulted on any loans."
    }, {
      "heading" : "1.3 Other Related Work",
      "text" : "Several papers study the problem of fairness in machine learning. One line of work aims to give algorithms for batch classification which achieve group fairness otherwise known as equality of outcomes, statistical parity – or algorithms that avoid disparate impact (see e.g. Calders and Verwer [2010], Luong et al. [2011], Kamishima et al. [2011], Feldman et al. [2015], Fish et al. [2016] and Adler et al. [2016] for a study of auditing existing algorithms for disparate impact). While statistical parity is sometimes a desirable goal – indeed, it is sometimes required by law – as observed by Dwork et al. [2012] and others, it suffers from two problems. First, if different populations indeed have different statistical properties, then it can be at odds with accurate classification. Second, even in cases when statistical parity is attainable with an optimal classifier, it does not prevent discrimination at an individual level – see Dwork et al. [2012] for a catalog of ways in which statistical parity can be insufficient from the perspective of fairness. In contrast, we study a notion aimed at guaranteeing fairness at the individual level.\nOur definition of fairness is most closely related to that of Dwork et al. [2012], who proposed and explored the basic properties of a technical definition of individual fairness formalizing the idea that “similar individuals should be treated similarly”. Specifically, their work presupposes the existence of a task-specific metric on individuals, and proposes that fair algorithms should satisfy a Lipschitz condition with respect to this metric. Our definition of fairness is similar, in that the expected reward of each arm is a natural metric through which we define fairness. The main conceptual distinction between our work and Dwork et al. [2012] is that their work operates under the assumption that the metric is known to the algorithm designer, and hence in their setting, the fairness constraint binds only insofar as it is in conflict with the desired outcome of the algorithm designer. The most challenging aspect of this approach (as they acknowledge) is that it requires that some third party design a “fair” metric on individuals, which in a sense encodes much of the relevant challenge. The question of how to design such a metric was considered by Zemel et al. [2013], who study methods to learn representations that encode the data, while obscuring protected attributes. Our fairness constraint, conversely, is entirely aligned with the goal of the algorithm designer in that it is satisfied by the optimal policy; nevertheless, it affects the space of feasible learning algorithms, because it interferes with learning an optimal policy, which depends on the unknown reward functions.\nAt a technical level, our work is related to Amin et al. [2012] and Amin et al. [2013], which also relate KWIK learning to bandit learning in a different context, unrelated to fairness (when the arm space is very large)."
    }, {
      "heading" : "2 Preliminaries",
      "text" : "We study the stochastic contextual bandit setting, which is defined by a domain X , a set of “arms” [k] := {1, . . . , k} and a class C of functions of the form f : X → [0, 1]. For each arm j there is some function fj ∈ C, unknown to the learner. In rounds t = 1, . . . , T , an adversary reveals to the algorithm a context xtj for each arm\n6. An algorithm A then chooses an arm it, and observes reward rtit for the arm it chose. We assume r t j ∼ Dtj , E [ rtj ] = fj(x t j), for some distribution Dtj over [0, 1].\nLet Π be the set of policies mapping contexts to distributions over arms Xk → ∆k, and π∗ the optimal policy which selects a distribution over arms as a function of contexts to maximize the expected reward of those arms. The pseudo-regret of an algorithm A on contexts x1, . . . , xT is\n6Often, the contextual bandit problem is defined such that there is a single context xt every day. Our model is equivalent – we could take xtj := x t for each j.\ndefined as follows, where πt represents A’s distribution on arms at round t:\n∑ t Eit∗∼π∗(xt) [ fit∗(x t it∗ ) ] − Eit∼πt [∑ t fit(x t it) ] = Regret(x1, . . . , xT ).\nWe hereafter refer to this as the regret of A. The optimal policy π∗ pulls arms with highest expectation at each round, so:\nRegret(x1, . . . , xT ) = ∑ t max j ( fj(x t j) ) − Eit∼πt [∑ t fit(x t it) ] .\nWe say that A satisfies regret bound R(T ) if maxx1,...,xT Regret(x1, . . . , xt) ≤ R(T ). Let the history ht ∈ ( X k × [k]× [0, 1] )t−1 be a record of t − 1 rounds experienced by A, t − 1 3-tuples which encode for each t the realization of the contexts, arm chosen, and reward observed. We write πtj|ht to denote the probability that A chooses arm j after observing contexts x t, given ht. For notational simplicity, we will often drop the superscript t on the history when referring to the distribution over arms: πtj|h := π t j|ht .\nWe now define what it means for a contextual bandit algorithm to be δ-fair with respect to its arms. Informally, this will mean that A will play arm i with higher probability than arm j in round t only if i has higher mean than j in round t, for all i, j ∈ [k], and in all rounds t.\nDefinition 1 (δ-fair). A is δ-fair if, for all sequences of contexts x1, . . . , xt and all payoff distributions Dt1, . . . ,Dtk, with probability at least 1− δ over the realization of the history h, for all rounds t ∈ [T ] and all pairs of arms j, j′ ∈ [k],\nπtj|h > π t j′|h only if fj(x t j) > fj′(x t j′).\nRemark 1. Definition 1 prohibits favoring lower payoff arms over higher payoff arms. One relaxed definition only requires that πtj|h = π t j′|h when fj(x t j) = fj′(x t j′) – requiring only identical individuals (concerning expected payoff) be treated identically. This relaxation is a special case of Dwork et al. [2012]’s proposed family of definitions, which require that “similar individuals be treated similarly”. We use Definition 1 as it is better motivated in its implications for fair treatment of individuals, but all of our results – including our lower bounds – apply also to this relaxation.\nKWIK learning Let B be an algorithm which takes as input a sequence of examples x1, . . . , xT , and when given some xt ∈ X , outputs either a prediction ŷt ∈ [0, 1] or else outputs ŷt = ⊥, representing “I don’t know”. When ŷt = ⊥, B receives feedback yt such that E [ yt ]\n= f(xt). B is an ( , δ)-KWIK learning algorithm for C : X → [0, 1], with KWIK bound m( , δ) if for any sequence of examples x1, x2, . . . and any target f ∈ C, with probability at least 1− δ, both:\n1. Its numerical predictions are accurate: for all t, ŷt ∈ {⊥} ∪ [f(xt)− , f(xt) + ], and\n2. B rarely outputs “I Don’t Know”: ∑∞ t=1 I [ ŷt = ⊥ ] ≤ m( , δ)."
    }, {
      "heading" : "2.1 Specializing to Classic Stochastic Bandits",
      "text" : "In Sections 3 and 4, we study the classic stochastic bandit problem, an important special case of the contextual bandit setting described above. Here we specialize our notation to this setting, in which there are no contexts. For each arm j ∈ [k], there is an unknown distribution Dj over [0, 1]\nwith unknown mean µj . A learning algorithm A chooses an arm it in round t, and observes the reward rtit ∼ Dit for the arm that it chose. Let i\n∗ ∈ [k] be the arm with highest expected reward: i∗ ∈ arg maxi∈[k] µi. The pseudo-regret of an algorithm A on D1, . . . ,Dk is now just:\nT · µi∗ − Eit∼πt  ∑ 0≤t≤T µit  = Regret(T,D1, . . . ,Dk) Let ht ∈ ([k]× [0, 1])t−1 denote a record of the t − 1 rounds experienced by the algorithm so far, represented by t − 1 2-tuples encoding the previous arms chosen and rewards observed. We write πtj|ht to denote the probability that A chooses arm j given history h t. Again, we will often drop the superscript t on the history when referring to the distribution over arms: πtj|h := π t j|ht .\nδ-fairness in the classic bandit setting specializes as follows:\nDefinition 2 (δ-fairness in the classic bandits setting). A is δ-fair if, for all distributionsD1, . . . ,Dk, with probability at least 1− δ over the history h, for all t ∈ [T ] and all j, j′ ∈ [k]:\nπtj|h > π t j′|h only if µj > µj′ ."
    }, {
      "heading" : "3 Fair Classic Stochastic Bandits: An Algorithm",
      "text" : "In this section, we describe a simple and intuitive modification of the standard UCB algorithm [Auer et al., 2002], called FairBandits, prove that it is fair, and analyze its regret bound. The algorithm and its analysis highlight a key idea that is important to the design of fair algorithms in this setting: that of chaining confidence intervals. Intuitively, as a δ-fair algorithm explores different arms it must play two arms j1 and j2 with equal probability until it has sufficient data to deduce, with confidence 1 − δ, either that µj1 > µj2 or vice versa. FairBandits does this by maintaining empirical estimates of the means of both arms, together with confidence intervals around those means. To be safe, the algorithm must play the arms with equal probability while their confidence intervals overlap. The same reasoning applies simultaneously to every pair of arms. Thus, if the confidence intervals of each pair of arms ji and ji+1 overlap for each i ∈ [k], the algorithm is forced to play all arms j with equal probability. This is the case even if the confidence intervals around arm jk and arm j1 are far from overlapping – i.e. when the algorithm can be confident that µj1 > µjk .\nThis approach initially seems naive: in an attempt to achieve fairness, it seems overly conservative when ruling out arms, and can be forced to play arms uniformly at random for long periods of time. This is reflected in its regret bound, which is only non-trivial after T k3, whereas the UCB algorithm [Auer et al., 2002] achieves non-trivial regret after T = O(k) rounds. However, our lower bound in Section 4 shows that any fair algorithm must suffer constant per-round regret for T k3 rounds on some instances.\nWe now give an overview of the behavior of FairBandits. At every round t, FairBandits identifies the arm it∗ = arg maxi u t i that has the largest upper confidence interval amongst the active arms. At each round t, we say i is linked to j if [`ti, u t i] ∩ [`tj , utj ] 6= ∅, and i is chained to j if i and j are in the same component of the transitive closure of the linked relation. FairBandits plays uniformly at random among all active arms chained to arm it∗.\nInitially, the active set contains all arms. The active set of arms at each subsequent round is defined to be the set of arms that are chained to the arm with highest upper confidence bound at the previous round. The algorithm can be confident that arms that have become unchained to the\narm with the highest upper confidence bound at any round have means that are lower than the means of any chained arms, and hence such arms can be safely removed from the active set, never to be played again. This has the useful property that the active set of arms can only shrink: at any round t, St ⊆ St−1; see Figure 1 for an example of active set evolution over time.\n1: procedure FairBandits(δ) 2: S0 ← {1, . . . , k} . Initialize the active set 3: for i = 1, . . . k do 4: µ̂0i ← 12 , u 0 i ← 1, `0i ← 0, n0i ← 0 . Initialize each arm 5: for t = 1 to T do 6: it∗ ← arg maxi∈St uti . Find arm with highest ucb 7: St ← {j | j chains to it∗, j ∈ St−1} . Update active set 8: j∗ ← (x ∈R St) . Select active arm at random 9: nt+1j∗ ← ntj∗ + 1\n10: µ̂t+1j∗ ← 1\nnt+1 j∗\n(µ̂tj∗ · ntj∗ + rtj∗) . Pull arm j∗, update its mean estimate 11: B ← √ ln((π[t+1])2/3δ)\n2nt+1 j∗\n12:\n[ `t+1j∗ , u t+1 j∗ ] ← [ µ̂t+1j∗ −B, µ̂ t+1 j∗ +B ] . Update interval for pulled arm\n13: for j ∈ St, j 6= j∗ do 14: µ̂t+1j ← µ̂tj , n t+1 j ← ntj , u t+1 j ← utj , ` t+1 j ← `tj\nWe first observe that with probability 1 − δ, all of the confidence intervals maintained by FairBandits (δ) contain the true means of their respective arms over all rounds. We prove this claim, along with all other claims in this section without proofs, in Appendix A.\nLemma 1. With probability at least 1− δ, for every arm i and round t `ti ≤ µi ≤ uti.\nThe fairness of FairBandits follows almost immediately from this guarantee.\nTheorem 1. FairBandits (δ) is δ-fair.\nProof. By Lemma 1, with probability at least 1−δ all confidence intervals contain their true means across all rounds. Thus, with probability 1−δ, at every round t, for every i ∈ St, j /∈ St, it must be that µj < µi – the arms not in the active set have strictly smaller means than those in the active set; if not, utj ≥ µj ≥ µi ≥ `ti implies j would be chained to it∗ if i is. Finally, all arms in St are played uniformly at random – but since all such arms are played with the same probability, this does not cause the fairness constraint to bind for any pair i, i′ ∈ St, for any realization of µi, µ′i which lie within their confidence intervals.\nNext, we upper bound the regret of FairBandits.\nTheorem 2. If δ < 1/ √ T , then FairBandits has regret\nR(T ) = O (√ k3T ln Tk\nδ\n) .\nRemark 2. Before proving Theorem 2, we highlight two points. First, this bound becomes nontrivial (i.e. the average per-round regret is 1) for T = Ω(k3). As we show in the next section, it is not possible to improve on this. Second, the bound may appear to have suboptimal dependence on T when compared to unconstrained regret bounds (where the dependence on T is often described\nas logarithmic). However, it is known that Ω (√ kT ) regret is necessary even in the unrestricted\nsetting (without fairness) if one does not make data-specific assumptions on an instance [Bubeck and Cesa-Bianchi, 2012] (e.g. that there is a lower bound on the gap between the best and second best arm). It would be possible to state a logarithmic dependence on T in our setting as well while making assumptions on the gaps between arms, but since our fairness constraint manifests itself as a cost that depends on k, we choose for clarity to avoid such assumptions. Without such assumptions, our dependence on T is also optimal.\nWe now prove Theorem 2. Lemma 2 upper bounds the probability any arm i active in round t has been pulled substantially fewer times than its expectation, i.e. nti tk . Lemma 3 upper bounds the width of any confidence interval used by FairBandits in round t by η(t), conditioned on i being pulled the number of times guaranteed by Lemma 2. Finally, we stitch this together to prove Theorem 2 by upper bounding the total regret incurred for T rounds by noticing that the regret of any arm active in round t is at most kη(t).\nWe begin by lower bounding the probability that any arm active in round t has been pulled substantially fewer times than its expectation.\nLemma 2. With probability at least 1− δ 2t2 ,\nnti ≥ t\nk −\n√ t\n2 ln ( 2k · t2 δ ) for all i ∈ St (for all active arms in round t).\nWe now use this lower bound on the number of pulls of active arm i in round t to upper-bound η(t), an upper bound on the confidence interval width FairBandits uses for any active arm i in round t.\nLemma 3. Consider any round t and any arm i ∈ St. Condition on nti ≥ tk −\n√ t ln( 2kt 2\nδ )\n2 . Then,\nuti − `ti ≤ 2 √√√√√√ ln ( (π · t)2 /3δ )\n2 · tk −\n√ t ln( 2kt 2\nδ )\n2\n= η(t).\nFinally, we prove the bound on the total regret of the algorithm, using the bound on the width of any active arm’s confidence interval in round t provided by Lemma 3.\nProof of Theorem 2. We condition on µi ∈ [`ti, uti] for all i, t. This occurs with probability at least 1− δ, by Lemma 1. We claim that this implies that arm i∗ with highest expected reward is always in the active set. This follows from the fact that µi∗ ∈ [`ti∗ , u t i∗ ] and µj ∈ [` t j , u t j ] for all j, t; thus, if µi∗ > µj , it must be that u t i∗ ≥ ` t j . Thus, this holds for i t ∗, the arm with highest upper confidence bound in round t, so i∗ must be chained to i t ∗ in round t for all t.\nWe further condition on the event that for all j, t,\nntj ≥ t\nk −\n√ t\n2 ln\n( 2kt2\nδ\n) ,\nwhich holds with probability at least 1− πδ2 by Lemma 2 and a union bound over all times t. This implies that, for all rounds t, for every active arm j ∈ St, Lemma 3 applies, and therefore\nutj − `tj ≤ η(t).\nFinally, we upper-bound the per-round regret of pulling any active arm i ∈ St at round t. Since i∗ is active, any i ∈ St is chained to arm i∗. Since all active arms have confidence interval width at most η(t) and i must be chained using at most k arms’ confidence intervals, we have that\n`ti ≥ uti∗ − k · η(t).\nSince µi ≥ `ti and uti∗ ≥ µi∗ , it follows that |µi − µi∗ | ≤ k · η(t) for any i ∈ S t. Finally, summing up over all rounds t ∈ T , we know that\nR(T ) ≤ T∑ t:0 min(1, k · η(t)) + ( 1 + π 2 ) δT\n≤ k  T∑ t: t k >2 √ t ln 2tk δ √ ln tδ t 2k + T∑ t: t k ≤2 √ t ln 2tk δ 1 + (1 + π2) δT = Õ(k 32 √ T ln kT δ + k3)\nwhere this bound is derived in Appendix A.1."
    }, {
      "heading" : "4 Fair Classic Stochastic Bandits: A Lower Bound",
      "text" : "We now show that the regret bound for FairBandits has an optimal dependence on k: no fair algorithm has diminishing regret before T = Ω(k3) rounds. All missing proofs are in Appendix B. The main result of this section is the following.\nTheorem 3. There is a distribution P over k-arm instances of the stochastic multi-armed bandit problem such that any fair algorithm run on P experiences constant per-round regret for at least\nT = Ω ( k3 ln 1\nδ ) rounds.\nDespite the fact that regret is defined in a prior-free way, the proof of Theorem 3 proceeds via Bayesian reasoning. We construct a family of lower bound instances such that arms have payoffs drawn from Bernoulli distributions, denoted B(µ) for mean µ. So, to specify a problem instance, it suffices to specify a mean for each of k arms: µ1, . . . , µk. The proof formalizes the following outline.\n1. We define an instance distribution P = P1× . . .×Pk over means µi (Definition 3). P will have two important properties. First, we will draw means from P such that for any i ∈ [k − 1], µi = µi+1 with probability at least 1/4. Second, for any realization of means drawn from P , if an algorithm plays uniformly at random over [k], it will suffer constant per-round regret.\n2. We treat Pi as a prior distribution over mean µi, and analyze the posterior distribution Pi(r 1 i , , . . . , r t i) over means that results after applying Bayes’ rule to the payoff observations\nr1i , . . . , r t i made by the algorithm. Bayes’ rule implies (Lemma 4) the joint distribution over rewards and means drawn from P is identical to the distribution which first draws means according to P , then draws rewards conditioned on those means, and finally resamples the means from the posterior distribution on means. Thus, we can reason about fairness (a frequentist quantity) by analyzing the Bayesian posterior distribution on means conditioned on the observed rewards.\n3. A δ-fair algorithm, for any set of means realized from the instance (prior) distribution, must not play arm i + 1 with lower probability than arm i if µi = µi+1, except with probability δ. By the above change of perspective, therefore, any δ-fair algorithm must play arms i and i+1 with equal probability until the posterior distribution on means given observed rewards, satisfies P [µi = µi+1|h] < δ (Lemmas 5 and 6).\n4. We finally lower bound the number of reward observations necessary before the posterior distribution on means given payoffs is such that P [µi = µi+1|h] < δ for any pair of adjacent arms i, i + 1. We show that this is Ω(k2) (Lemma 7). Since fair algorithms must play from among the k arms uniformly at random until this point, with high probability, no arm accumulates sufficiently many reward observations until T = Ω(k3) rounds of play.\nWe begin by describing our distribution over instances. Each arm i’s payoff distribution will be Bernoulli with mean µi ∼ Pi independently of each other arm.\nDefinition 3 (Prior Distribution over µi). For each arm i, µi is distributed according to the distribution with the following probability mass function:\nPi(x) =\n{ 1 2 if x = 1 3 + i 3k\n1 2 if x = 1 3 + i+1 3k . Let P = ∏ i Pi denote the joint distribution on arms’ expected payoffs.\nWe treat P as a prior distribution over instances, and analyze the posterior distribution on instances given the realized rewards. Lemma 4 justifies this reasoning.\nLemma 4. Consider the following two experiments: In the first, let µi ∼ Pi and r1i , . . . , rti ∼ B(µi), and W denote the joint distribution on (µi, r 1 i , . . . , r t i). In the second, let µi ∼ Pi, and r1i , . . . , r t i ∼ B(µi), and then re-draw the mean µ′i ∼ Pi(r1i , . . . , rti) from its posterior distribution given the rewards. Let (µ′i, r 1 i , . . . , r t i) ∼W ′. Then, W and W ′ are identical distributions.\nNext, we lower-bound the number of reward observations necessary such that for some i ∈ [k]: P [ µi = µi+1|ht ] < δ with respect to the posterior. It will be useful to refer to an algorithm’s histories as distinguishing the mean of an arm given that history with high probability.\nDefinition 4 (δ-distinguishing). We will say ht δ-distinguishes arm i for A if, for some α ∈ [0, 1],\nPµ′i∼Pi(ht) [ µ′i = α ] ≥ 1− δ.\nThe next lemma shows that if no arm is √ δ-distinguished by a history, all pairs of arms i, i+ 1\nhave posterior probability strictly greater than δ of having equal means. Lemma 5. Suppose A has history ht, and that ht does not √ δ-distinguish any arm i. Then, for all arms i, i+ 1, P [ µi = µi+1|ht ] > δ.\nNow, we prove that for any fair algorithm, with probability ≥ 12 over the draw of histories h t, ht\nmust √\n2δ-distinguish some arm, or the algorithm must play uniformly across all k arms conditioned on ht.\nLemma 6. Suppose an algorithm A is δ-fair. Then:\nPht∼A [ ht does not √ 2δ-distinguish any i ∧ ∃t′ ≤ t, i ∈ [k] : πt′ i′|ht′ 6= 1\nk\n] ≤ 1\n2 .\nWe now lower-bound the number of observations from arm i which are required to δ-distinguish it. Lemma 7. Fix any δ < 18 . Let µi ∼ Pi as in Definition 3. Then, arm i is √ 2δ-distinguishable by ht only if Ti = Ω(k 2 ln 1δ ), where Ti = |{t ′ : ht ′ 2 = i, t ′ ≤ t}| is the number of times arm i is played.\nProof. Write p, p + 13k to represent the two possible realizations that µi might take, when drawn from the distribution over instances given in Definition 3. Let A represent the event that µi = p and B the event that µi = p+ 1 3k . Let δ ′ = √\n2δ throughout. Fix a history ht, and let m = Ti represent the number of observations of arm i’s reward. We will abuse notation and use hti to refer to the payoff sequence of arm i observed in history h t. hti is therefore a binary sequence of length m; let ||hti||0 = s denote the number of 1s in the sequence. We will calculate conditions under which hti,m, s will imply that either 1−δ′ δ′ ≤\nP[B|hti] P[A|hti] or P[B|hti] P[A|hti] ≤ δ′1−δ′ holds, implying that one of A or B has posterior probability at least 1 − δ′, conditioned on the observed rewards. If neither of these is the case, i is not δ′-distinguished by ht.\nWe begin by rearranging our definition of this ratio X = P[B|hti] P[A|hti] = P[hti|B] P[hti|A] ,which follows from Bayes’ rule and the fact that P [A] = P [B]. We wish to upper and lower bound X in terms of hti’s value. By definition of the Bernoulli distribution, we have that\nX = P [ hti|B ] P [hti|A] = ( p+ 13k )s ( 1− p− 13k )m−s (p)s (1− p)m−s = ( 1 + 1 3kp )s( 1− 1 3k(1− p) )m−s .\nWe now calculate under what conditions either (a) X ≤ δ′1−δ′ , or (b) X ≥ 1−δ′ δ′ . One of these must hold if i is δ′-distinguished. Before we do so, we mention that a Chernoff bound implies that with probability 1− δ′, for events A and B, Equations 1 and 2, respectively:\n|s−mp| ≤ √ 2m ln 2\nδ′ (1) |s−mp− m 3k | ≤\n√ 2m ln 2\nδ′ (2)\nsince the mean of m Bernoulli trials with mean p ( or p+ 13k ) is mp (or mp+ m 3k ).\nWe begin by analyzing case (a), where δ′ = √ 2δ < 1/2 implies\n2δ′ > δ′\n1− δ′ ≥ X =\n( 1 + 1\n3kp\n)s( 1− 1\n3k(1− p)\n)m−s .\nTaking logarithms on both sides, we have that\nln(2δ′) > s ln ( 1 + 1\n3kp\n) + (m− s) ln ( 1− 1\n3k(1− p)\n) ≥ s 1\n3kp+ 1 − (m− s) 1 3k(1− p)− 1\nwhere the inequality follows from ln(1 + x) ≥ xx+1 for x ∈ [−1,∞]. Then, this implies that\n(3kp+ 1)(3k(1− p)− 1) ln(2δ′) > s(3k(1− p)− 1)− (m− s)(3kp+ 1) = 3ks− 3kpm−m.\nMultiplying both sides by −1, this implies that\n(3kp+ 1)(3k(1− p)− 1) ln 1 2δ′ < m+ 3kpm− 3ks.\nEquation 1 implies |3ks− 3kmp−m| ≤ m+ 3k √\n2m ln 2δ′ , which with the previous line implies\n(3kp+ 1)(3k(1− p)− 1) ln 1 2δ′ < m+ 3k\n√ 2m ln 2\nδ′ .\nSince p, 1− p ∈ [1/3, 2/3] and δ′ = √\n2δ, solving for m implies that m = Ω(k2 ln 1δ′ ). In case (b), we have\n1\n2δ′ <\n1− δ′\nδ′ ≤ X =\n( 1 + 1\n3kp\n)s( 1− 1\n3k(1− p)\n)m−s ≤ e s 3kp e −(m−s) 3k(1−p)\nwhere we used the fact that 1 + x ≤ ex for all x. Taking logarithms, this will imply that\ns 3kp − m− s 3k(1− p) > ln 1 2δ′ ⇒ s−mp = s(1− p)− (m− s)p > 3kp(1− p) ln 1 2δ′ ≥ 6k 9 ln 1 2δ′ (3)\nwhose last inequality comes the range of p. Combining this inequality with Equation 2, this implies√ 2m ln 1\n2δ′ + m k > 6k 9 ln 1 2δ′\nand solving for m and substituting for δ′ gives that m = Ω(k2 ln 1δ ).\nThus, if either X ≥ 1−δ′δ′ or X ≤ δ′ 1−δ′ , it must be that m = Ω(k 2 ln 1δ ).\nWe now have the tools in hand to prove Theorem 3.\nProof of Theorem 3. Assume A is some δ-fair algorithm where δ < 1/8. Fix T ; we claim that with probability at least 12 , for any t = o(k 3 ln 1δ ), t ≤ T , that π t j|ht = 1 k for all j. Since the payoff for uniformly random play is ≤ 12 + 1 k , while the best arm has payoff ≥ 2 3 , in any round t where πti|ht = π t i′|ht for all i, i\n′ ∈ [k], the algorithm suffers Ω(1) regret in that round. Lemma 6 implies that, with probability at least 12 over the distribution over histories h t, either\n(a) πt ′ i|ht′ = π t′ i′|ht′ for all i, i ′ ∈ [k], t′ ≤ t or (b) ht must\n√ 2δ-distinguish some arm i. Case (a) implies\nour claim. In case (b), Lemma 7 states than an arm i is √\n2δ-distinguishable only if Ti = Ω(k 2 ln 1δ ).\nWe now argue that unless t = Ω(k3 ln 1δ ), Ti = o(k 2 ln 1δ ), which will imply our claim for case (b).\nFix some i, t. We lower-bound t for which, with probability at least 1− δ′k over histories h t, it\nwill be the case that nti ≥ c · k2 ln 1δ when π t′ i|ht′ = π t′ i′|ht′ for all i, i ′ ∈ [k], t′ ≤ t. Let X1, . . . , Xt be indicator variables of arm i being played in round t′ ≤ t. Note that for all t′ ≤ t, E[Xt′ ] = 1k , since in all rounds prior to t, we have all arms are played with equal probability. For any ∈ [0, 1], as nt ′ i are nondecreasing in t ′, an additive Chernoff bound implies\nP [ ∃t′ ≤ t : nt′i ≥ t\nk + t\n] ≤ P ∑ t′≤t Xt′ > t k + t  ≤ e−2t 2 which, for t = √ t ln 2k\nδ′ 2 , becomes P [∑ t′≤tXt′ > t k + t ] ≤ δ′k . So, using a union bound over all k\narms, with probability 1 − δ′, for some fixed t and all i, nti ≤ tk + √ t ln 2k δ′\n2 . We condition on the event that nti satisfies this inequality for a fixed t and all i. If n t i ≥ c · k2 ln 1δ , this implies\nt k +\n√ t ln 2kδ′\n2 ≥ c · k2 ln 1 δ ⇒ t ≥ −k\n√ t ln 2kδ′\n2 + c · k3 ln 1 δ .\nIf k\n√ t ln 2k\nδ′ 2 ≤ c 2 · k 3 ln 1δ , then t ≥ c 2 · k 3 ln 1δ ; if not, then t ≥ c2 2 k4 ln2 1 δ\nln 2k δ′\n. Thus, nti < c · k2 ln 1δ with probability 1− δ′ for all i unless t ≥ min ( c 2 · k 3 ln 1δ , c2 2 k4 ln2 1 δ\nln 2k δ′\n) = Ω(k3 ln 1δ ) for δ ′ ∈ [12 , 1]."
    }, {
      "heading" : "5 KWIK Learnability Implies Fair Bandit Learnability",
      "text" : "In this section, we show if a class of functions is KWIK learnable, then there is a fair algorithm for learning the same class of functions in the contextual bandit setting, with a regret bound polynomially related to the function class’ KWIK bound. Intuitively, KWIK-learnability of a class of functions guarantees we can learn the function’s behavior to a high degree of accuracy with a high degree of confidence. As fairness constrains an algorithm most before the algorithm has determined the payoff functions’ behavior accurately, this guarantee enables us to learn fairly without incurring much additional regret. Formally, we prove the following polynomial relationship.\nTheorem 4. For an instance of the contextual multi-armed bandit problem where fj ∈ C for all j ∈ [k], if C is ( , δ)-KWIK learnable with bound m( , δ), KWIKToFair (δ, T ) is δ-fair and achieves regret bound:\nR(T ) = O ( max ( k2 ·m ( ∗, min (δ, 1/T )\nT 2k\n) , k3 ln k\nδ )) for δ ≤ 1√\nT where ∗ = arg min (max( · T, k ·m( , min(δ,1/T )kT 2 ))).\nFirst, we construct an algorithm KWIKToFair(δ, T ) that uses the KWIK learning algorithm as a subroutine, and prove that it is δ-fair. A call to KWIKToFair(δ, T ) will initialize a KWIK learner for each arm, and in each of the T rounds will implicitly construct a confidence interval around the prediction of each learner. If a learner makes a numeric valued prediction, we will interpret this as a confidence interval centered at the prediction with width ∗. If a learner outputs ⊥, we interpret this as a trivial confidence interval (covering all of [0, 1]). We use the same chaining technique that we use in the classic stochastic setting. In every round t, KWIKToFair (δ, T ) identifies the arm it∗ = arg maxi u t i that has the largest upper confidence bound. At each round t, we will say i is linked to j if [`ti, u t i] ∩ [`tj , utj ] 6= ∅, and i is chained to j if they are in the same component of the transitive closure of the linked relation. Then, it plays uniformly at random amongst all arms chained to arm it∗. Whenever all learners output predictions, they need no feedback. When a learner for j outputs ⊥, if j is selected then we have feedback rtj to give it; on the other hand, if j isn’t selected, we “roll back” the learning algorithm for j to before this round by not updating the algorithm’s state.\n1: procedure KWIKToFair(δ, T ) 2: δ∗ ← min(δ, 1 T )\nkT 2 , ∗ ← arg min (max( · T, k ·m( , δ∗)))\n3: Initialize KWIK( ∗, δ∗)-learner Li, hi ← [ ] ∀i ∈ [k] 4: for 1 ≤ t ≤ T do 5: S ← ∅ . Initialize set of predictions S 6: for i = 1, . . . , k do 7: sti ← Li(xti, hi) 8: S ← S ∪ sti . Store prediction sti 9: if ⊥∈ S then 10: Pull j∗ ← (x ∈R [k]), receive reward rtj∗ . Pick arm at random from all arms 11: else 12: it∗ ← arg maxi sti 13: St ← {j | (stj − ∗, stj + ∗) chains to (stit∗ − ∗, stit∗ + ∗)} 14: Pull j∗ ← (x ∈R St), receive reward rtj∗ . Pick arm at random from active set sti∗ 15: hj∗ ← hj∗ :: (xtj∗ , rtj∗) . Update the history for Li\nWe begin by bounding the probability of certain failures of KWIKToFair in Lemma 8, proven in Appendix C. This in turn lets us prove the fairness of KWIKToFair in Theorem 5.\nLemma 8. With probability at least 1 −min(δ, 1T ), for all rounds t and all arms j, (a) if s t i ∈ R then |sti − fi(xti)| ≤ ∗ and (b) ∑ t I [ sti = ⊥ ] ≤ m( ∗, δ∗).\nTheorem 5. KWIKToFair(δ, T ) is δ-fair.\nProof of Theorem 5. We condition on both (a) and (b) holding for all arms i and rounds t from Lemma 8, which occur with probability 1 − δ for all arms and all times t. Therefore, we proceed by conditioning on the event that for all arms i and all rounds t, if Li = s t i for s t i 6= ⊥ then |sti − fi(xti)| ≤ ∗. Having done so, there are two possibilities for each round t. In case 1, for each i we have that Li(x t i) = s t i 6= ⊥. By the condition above, for any arms i and j, fi(x t i) ≥ fj(xtj) implies that sti + ∗ ≥ stj − ∗. Since in this case no learner outputs ⊥, arm j chains to the top arm only if arm i does. Therefore πti|h ≥ π t j|h. In case 2, there exists some i such that Li(x t i) = ⊥. Then we choose uniformly at random across all arms, so πti|h = π t j|h for all i and j. Thus, with probability at least 1−δ, for each round t, fi(xti) ≥ fj(xtj) implies that πti|h ≥ π t j|h.\nWe now use the KWIK bounds of the KWIK learners to upper-bound the regret of KWIKToFair(δ, T ).\nLemma 9. KWIKToFair(δ, T ) achieves regret O(max(k2 ·m( ∗, δ∗), k3 ln Tkδ )).\nProof. We first condition on the event that both (a) and (b) from Lemma 8 hold for all t, i, which holds with probability 1 − min(δ, 1T ), and bound the regret when they both hold. Choose an arbitrary round t in the execution of KWIKToFair(δ, T ). As above, there are two cases. In the first case, Li(x t i) = s t i 6= ⊥ for all i and we choose uniformly at random from the arms chained by\n∗-intervals to the arm with the highest prediction. Since we have conditioned on the event that all KWIK learners are correct, i∗ ∈ St. Furthermore, for any i, j ∈ St, we have that |sti − stj | ≤ 2k ∗, and in particular that |sti − sti∗ | ≤ 2k ∗. Thus, the regret is at most 2k ∗ in such a round. In the second case some arm outputs ⊥, so we choose randomly from all k arms, and the worst-case regret is 1. Thus, the total regret will be at most 2k ∗+n+ δT where n is the number of rounds in which some Li outputs ⊥.\nWe now upper bound ni, the number of rounds in which some arm outputs ⊥. Fix some arm i which outputs ⊥ in ni rounds. Arm i is played and therefore receives feedback every time it outputs ⊥ with probability at least 1/k. Thus, using a Chernoff bound, with probability 1 − δ′, arm i receives feedback for ni outputs of ⊥ in at least nik − √ 2ni ln 2 δ′ rounds. Li has the guarantee that there can be at most m( ∗, δ∗) many such rounds (in which it outputs ⊥ and receives feedback). Thus,\nm( ∗, δ∗) ≥ ni k − √ 2ni ln 2 δ′ .\nIf ni ≥ ck ·m( ∗, δ∗), this implies\nm( ∗, δ∗) ≥ c ·m( ∗, δ∗)− √\n2ck ·m( ∗, δ∗) ln 2 δ′ .\nWe now analyze cases in which (1) k ln 2δ′ ≤ m( ∗, δ∗) and (2) k ln 2δ′ > m( ∗, δ∗). Case (1) this implies\nm( ∗, δ∗) ≥ c ·m( ∗, δ∗)− √ 2c ·m( ∗, δ∗).\nFor c > 4, this leads to contradiction. Thus, in this case, if we set δ′ = δk , we know that with probability 1 − δ, ni ≤ 4k ·m( ∗, δ∗) which summing up over all i implies ∑ i ni ≤ 4k2 ·m( ∗, δ∗), as desired. In case (2), we have that\nln 2 δ′ ≥ ni k − √ 2ni ln 2 δ′\nwhich solving for ni implies that ni = O(k 2 ln 1δ′ ), so ∑ i ni = O(k 3 ln kδ ) by setting δ ′ = δk and taking a union bound. Thus, there are at most n = O(max(k2 ·m( , δ∗), k3 ln kδ )) rounds in expectation during the execution of KWIKToFair(δ, T ) in which some arm outputs ⊥.\nCombining both cases, the total regret incurred by KWIKToFair(δ, T ) across all T rounds is\nR(T ) = 4k2 ·m( ∗, δ∗) + k3 ln k δ ) + T · 2k ∗ + T ·min(δ, 1 T ) = O(max(k2 ·m( ∗, δ∗), k3 ln k δ )).\nOur presentation of KWIKToFair(δ, T ) has a known time horizon T . Its guarantees extend to the case in which T is unknown via the standard “doubling trick” to prove Theorem 4 in Appendix C.\nAn important instance of the contextual bandit problem is the linear case, where C consists of the set of all linear functions of bounded norm in d dimensions. This captures the natural setting in which the rewards of each arm are governed by an underlying linear regression model on a d-dimensional real valued feature space. The linear case is well studied, and there are known KWIK algorithms [Strehl and Littman, 2008] for the set of linear functions C, which allows us via our reduction to give a fair contextual bandit algorithm for this setting with a polynomial regret bound.\nLemma 10 ([Strehl and Littman, 2008]). Let C = {fθ|fθ(x) = 〈θ, x〉, θ ∈ Rd, ||θ|| ≤ 1} and X = {x ∈ Rd : ||x|| ≤ 1}. C is KWIK learnable with KWIK bound m( , δ) = Õ(d3/ 4).\nThen, an application of Theorem 4 implies that KWIKToFair has a polynomial regret guarantee for the class of linear functions. This proof can be found in Appendix C.\nCorollary 1. Let C and X be as in Lemma 10, and fj ∈ C for each j ∈ [k]. Then, KWIKToFair(T, δ) using the learner from [Strehl and Littman, 2008] has regret:\nR(T ) = Õ ( max ( T 4/5k6/5d3/5, k3 ln k\nδ\n)) ."
    }, {
      "heading" : "6 Fair Bandit Learnability Implies KWIK Learnability",
      "text" : "In this section, we show how to use a fair, no-regret contextual bandit algorithm to construct a KWIK learning algorithm whose KWIK bound has logarithmic dependence on the number of rounds T . Intuitively, any fair algorithm which achieves low regret must both be able to find and exploit an optimal arm (since the algorithm is no-regret) and can only exploit that arm once it has a tight understanding of the qualities of all arms (since the algorithm is fair). Thus, any fair no-regret algorithm will ultimately have tight (1− δ)-confidence about each arm’s reward function.\nTheorem 6. Suppose A is a δ-fair algorithm for the contextual bandit problem over the class of functions C, with regret bound R(T, δ). Suppose also there exists f ∈ C, x(`) ∈ X such that for every ` ∈ [d1 e], f(x(`)) = ` · . Then, FairToKWIK is an ( , δ)-KWIK algorithm for C with KWIK bound m( , δ), with m( , δ) the solution to m( ,δ) 4 = R(m( , δ), δ 2T ).\nRemark 3. The condition that C should contain a function that can take on values that are multiples of is for technical convenience; C can always be augmented by adding a single such function.\nOur aim is to construct a KWIK algorithm B to predict labels for a sequence of examples labeled with some unknown function f∗ ∈ C. To do this, we will run our fair contextual bandit algorithm A on an instance that we construct online as examples xt arrive for B. The idea is to simulate a two arm instance, in which one arm’s rewards are governed by f∗ (the function to be KWIK learned), and the other arm’s rewards are governed by a function f that we can set to take any value in {0, , 2 , . . . , 1}. For each input xt, we perform a thought experiment and consider A’s probability distribution over arms when facing a context which forces arm 2’s payoff to take each of the values 0, ∗, 2 ∗, . . . , 1. Since A is fair, A will play arm 1 with weakly higher probability than arm 2 for those ` : ` ∗ ≤ f(xt); analogously, A will play arm 1 with weakly lower probability than arm 2 for those ` : ` ∗ ≥ f(xt). If there are at least 2 values of ` for which arm 1 and arm 2 are\nplayed with equal probability, one of those contexts will force A to suffer ∗ regret, so we continue the simulation of A on one of those instances selected at random, forcing at least ∗/2 regret in expectation, and at the same time have B return ⊥. B receives f∗(xt) on such a round, which is used to construct feedback for A. Otherwise, A must transition from playing arm 1 with strictly higher probability to playing 2 with strictly higher probability as ` increases: the point at which that occurs will “sandwich” the value of f(xt), since A’s fairness implies this transition must occur when the expected payoff of arm 2 exceeds that of arm 1. B uses this value to output a numeric prediction.\nAn important fact we exploit is that we can query A’s behavior on (xt, x(`)), for any xt and ` ∈ [ d 1 ∗ e ] without providing it feedback (and instead “roll back” its history to ht not including the query (xt, x(`))). We update A’s history by providing it feedback only in rounds where B outputs ⊥.\n1: procedure FairToKWIK( , δ, T ) 2: ∗ ← 2 , δ ∗ ← δ· ∗T , h← [ ], initialize fair A(δ ∗, T ) for class C 3: for 1 ≤ t ≤ T do 4: ht ← h 5: for ` ∈ [ d 1 ∗ e ] do 6: Let (pt,`1 , p t,` 2 ) = (π1|ht , π2|ht)) be A(h t, (xt, x(`)))’s dist. over arms given h\n7: if ∃` 6= `′ : `, `′ ∈ [ d 1 ∗ e ] : pt,`1 = p t,` 2 = p t,`′ 1 = p t,`′\n2 then 8:\n9: Choose x(ˆ̀) ∈R {x(`), x(`′)} . One of x(`), x(`′) must cause ∗ regret 10: Select at ∼ A(ht, xt, x) . Run A to get a predicted arm 11: Predict ŷt ← ⊥ 12: if at = 1 then 13: rt1 ← yt and h← ht :: ((xt, x(ˆ̀)), 1, rt1) . Use KWIK feedback 14: else 15: rt2 ← ˆ̀ ∗ 16: h← ht :: ((xt, x(ˆ̀)), 2, rt2) . Construct feedback for arm 2 17: else . A’s history is not updated 18: if pt,`1 ≤ p t,` 2 for all ` ∈ [ d 1 ∗ e ] then 19: ˆ̀← 0 20: else Let ˆ̀ be the largest index for which pt, ˆ̀\n1 > p t,ˆ̀ 2\n21: Predict ŷt ← ˆ̀· ∗ Proof. For a fixed run of A, we calculate the probability that for all times t and ` ∈ [ d 1 ∗ e ] , it is the case that pt,`1 > p t,` 2 only if f ∗(xt) > ` · ∗ and also pt,`1 < p t,` 2 only if f\n∗(xt) < ` · ∗. In this run, A is queried on T ∗ histories and contexts: prefixes of h along with (x\nt, x(`)) for each t ∈ [T ], ` ∈ [ d 1 ∗ e ] . The fairness of A implies for any fixed ht and fixed (xt, x(`)), with probability 1− δ∗, pt,`1 > p t,` 2 only if f ∗(xt) > ` ∗ and pt,`1 < p t,` 2 only if f\n∗(xt) < ` ∗. Then, by a union bound over t ∈ [T ], ` ∈ [ d 1 ∗ e ] , with probability at least 1 − δ∗ T ∗ = 1 − δ, A(h\nt, xt, x(`)) will satisfy this property for all t ∈ [T ], ` ∈ [ d 1 ∗ e ] . We condition on this holding in the remainder of the proof.\nWe now argue that the numeric predictions of B are correct within an additive . Let:\nEt = {` : pt,`1 = p t,` 2 }.\nWhen B(xt) = yt ∈ [0, 1], note that |Et| ≤ 1, else B would have output ⊥.\nIf pt,`1 ≤ p t,` 2 for all `, since |Et| ≤ 1, either p t,0 1 < p t,0 2 or p t,1 1 < p t,1 2 , which we have conditioned on implying that either f∗(xt) < f(x(0)) = 0 or f∗(xt) < f(x(1)) = ∗. Since f∗(xt) ≥ 0, this implies f∗(xt) ∈ [0, ∗) = [ˆ̀ ∗, ∗) = [ŷt, ŷt + ∗).\nOtherwise, we have that pt, ˆ̀ 1 > p t,ˆ̀ 2 , and p t,` 1 ≤ p t,` 2 for all ` > ˆ̀. If (a) ˆ̀= d 1 ∗ e, then f ∗(xt) > 1,\na contradiction, so ˆ̀ < d 1 ∗ e. If (b) ˆ̀ = d 1 ∗ e − 1, then f ∗(xt) > f(x(ˆ̀)) = (d 1 ∗ e − 1) ∗ and so f∗(xt) ∈ ((d 1 ∗ e − 1) ∗, 1] = (ŷt, ŷt + ∗], so ŷt is ∗-accurate. If neither (a) nor (b), then (c) it must be ˆ̀< d 1 ∗ e − 1. Since |E t| ≤ 1, for some ` ∈ {ˆ̀+ 1, ˆ̀+ 2}, we know that pt,`1 < p t,` 2 ; thus, f∗(xt) < f(x(`)) ≤ (ˆ̀+ 2) ∗ and therefore f∗(xt) ∈ (ˆ̀ ∗, (ˆ̀+ 2) ∗) = (ŷt, ŷt + 2 ∗). Finally, we upper-bound m( , δ), the number of rounds t : B(xt) = ⊥. For each such t, A runs on a random draw of one of two contexts, one of whose arms’ payoffs differ by at least ∗. Thus, for one of those contexts, either f∗(xt) ≥ f(x)− ∗ or f∗(xt) ≤ f(x)− ∗. In either case, since pt,`1 = p t,` 2 = 1 2 for x(`) = x, A suffers expected regret at least ∗2 for that context, and at least ∗ 4 when faced with one chosen at random. Thus, m( , δ) · ∗4 = m( , δ) · 8 < R(m( , δ), δ\n∗) = R(m( , δ), δ2T ), since A’s regret is upper bounded by this quantity over m( , δ) rounds (which is an upper bound on the number of rounds for which A is actually run and updated)."
    }, {
      "heading" : "6.1 An Exponential Separation Between Fair and Unfair Learning",
      "text" : "In this section, we exploit the other direction of the equivalence we have proven between fair contextual bandit algorithms and KWIK learning algorithms to give a simple contextual bandit problem for which fairness imposes an exponential cost in its regret bound. This is in contrast to the case in which the underlying class of functions is linear, for which we gave fair contextual bandit algorithms with regret bounds within a polynomial factor of their unconstrained counterparts. In this problem, the context domain is the d-dimensional boolean hypercube: X = {0, 1}d – i.e. the context each round for each individual consists of d boolean attributes. Our class of functions C is the class of boolean conjunctions:\nC = {f | f(x) = xi1 ∧ xi2 ∧ . . . ∧ xik where 0 ≤ k ≤ d and i1, . . . , ik ∈ [d]}.\nWe first give a simple but unfair algorithm, ConjunctionBandit, for this problem which obtains a regret bound which is linear in d. It maintains a set of candidate variables C∗j for each conjunction fj ; this set shrinks across rounds, while always containing the true set of variables over which fj is defined. We denote the boolean value of variable m in the context for arm j in round t by xtj,m.\nThe formal claim and proof that ConjunctionBandit achieves regret R(T ) = O(k2d), as well as ConjunctionBandit’s formal description, can be found in Appendix C. ConjunctionBandit violates the fairness in every round t in which it predicts 0 for arm i but 1 for arm j even though fi(x t) = fj(x t) = 1, as πti = 0 < 1 k < π t j .\nWe now show that fair algorithms cannot guarantee subexponential regret in d. This relies upon a known lower bound for KWIK learning conjunctions [Li, 2009]:\nLemma 11. There exists a sequence of examples (x1, . . . , x2 d−1) such that for , δ ≤ 1/2, every ( , δ)-KWIK learning algorithm B for the class C of conjunctions on d variables must output ⊥ for xt for each t ∈ [2d − 1]. Thus, B has a KWIK bound of at least m( , δ) = Ω(2d).\nWe then use the equivalence between fair algorithms and KWIK learning to translate this lower bound on m( , δ) into a minimum worst case regret bound for fair algorithms on conjunctions. We modify Theorem 6 to yield the following lemma, proven in Appendix C.\nLemma 12. Suppose A is a δ-fair algorithm for the contextual bandit problem over the class C of conjunctions on d variables. If A has regret bound R(T, δ) then for δ′ = 2Tδ, FairToKWIK is an (0, δ′)-KWIK algorithm for C with KWIK bound m(0, δ′) = 4R(m(0, δ′), δ).\nLemma 11 then lets us lower-bound the worst case regret of fair learning algorithms on conjunctions.\nCorollary 2. For δ < 12T , any δ-fair algorithm for the contextual bandit problem over the class C of conjunctions on d boolean variables has a worst case regret bound of R(T ) = Ω(2d).\nProof. Let T ≤ 2d−1. We know then that if δ′ < 1, Lemma 11 guarantees the existence of a sequence of contexts x1, . . . , xT for which any (0, δ′)-KWIK algorithm has KWIK bound m(T, 0, δ′) = T .\nLemma 12 implies 4R(m(T, 0, δ′), δ) gives a KWIK bound of m(T, 0, δ′) when δ′ = 2Tδ. Thus,\nif δ < 12T , then δ ′ < 1 and so R(m(T, 0, δ′), δ) = m(T,0,δ ′) 4 = T 4 .\nTogether with the analysis of ConjunctionBandit, this demonstrates a strong separation between fair and unfair contextual bandit algorithms: when the underlying functions mapping contexts to payoffs are conjunctions on d variables, there exist a sequence of contexts on which fair algorithms must incur regret exponential in d while unfair algorithms can achieve regret linear in d."
    }, {
      "heading" : "A Missing Proofs for the Classic Stochastic Bandits Upper Bound",
      "text" : "We begin by proving Lemma 1, used in Section 3 to prove the fairness of the FairBandits algorithm.\nProof of Lemma 1. Choose an arbitrary arm i and round t and define indicator variablesX1, . . . , Xni(t) where Xn takes on the reward of pull n of arm i. By a Chernoff bound, for any a ≥ 0,\nP [ |µ̂ti − µi| ≤ a\nnti\n] ≤ 2 exp(−2a2/nti).\nIn particular for a = √ nti ln((πt) 2/3δ)/2, it is the case that\nP [ µi 6∈ ( µ̂ti − √ ln((πt)2/3δ)\n2nti , µ̂ti +\n√ ln((πt)2/3δ)\n2nti )] ≤2 exp(−2nti ln((πt)2/3δ)/2nti) = 2 exp(ln(3δ/(πt)2)) = 6δ/(πt)2.\nBy a union bound over all rounds t, the probability of any true mean ever falling outside of its confidence interval is at most δ( 6 π2 ∑∞ t=1 1 t2 ) = δ.\nNext, we prove Lemma 2, which we used in Section 3 to bound the regret of FairBandits in Theorem 2.\nProof of Lemma 2. Let X1, ..., Xt be indicator variables of whether i was pulled at each time t ′ ∈ [t]. Let Mt = ∑\nt′≤tXt′ , with E [Mt] = pt. For any ∈ [0, 1], a standard additive Chernoff bound states that\nP [Mt ≤ pt − t] ≤ e−2t 2 .\nSince i ∈ St, it must be that i ∈ St′ for all t′ ≤ t and all i ∈ St, by the definition of FairBandits. Thus, P [Xi = 1] ≥ 1k for any i ∈ S t, and therefore pt ≥ tk . so this also implies that\nP [ Mt ≤ t\nk − t\n] ≤ e−2t 2 .\nSetting t =\n√ t ln( 2t\n2k δ )\n2 , this bound becomes\nP Mt ≤ t k − √ t ln(2t 2k δ ) 2  ≤ δ 2kt2\nas desired. Then, taking a union bound over all active arms of which there are at most k, the claim follows.\nProof of Lemma 3. This follows from the definition of `ti, u t i and the lower bound on n t i provided by the assumption of the lemma.\nA.1 Missing Derivation of R(T ) for Theorem 2\nR(T ) ≤ T∑ t:0 min(1, k · η(t)) + ( 1 + π 2 ) δT\n≤ T∑ t:0 k ·min(1, η(t)) + ( 1 + π 2 ) δT\n≤ k  T∑ t: t k >2 √ t ln tk δ √√√√ ln tδ t k − √ t ln tkδ + T∑ t: t k ≤2 √ t ln tk δ 1 +O(δ)T\n≤ k  T∑ t: t k >2 √ t ln tk δ √ ln tδ t 2k + T∑ t: t k ≤2 √ t ln tk δ 1 +O(δT ) ≤ k\n(∫ T t=0 √ ln tδ t 2k + ∫ Õ(k2 ln k δ ) t=1 1 ) +O(δT )\n≤ k 3 2 ∫ T t=1 √ ln tδ t + Õ(k3 ln k δ ) +O(δT )\n= k 3 2 √ 2T √ ln kT\nδ + Õ(k3 ln\nk δ ) +O(δT )\n= Õ(k 3 2 √ T ln kT\nδ + k3) +O(δT )\n= Õ(k 3 2 √ T ln kT\nδ + k3)\nwhere the final step follows from δ ≤ 1√ T ."
    }, {
      "heading" : "B Missing Proofs for the Classic Stochastic Bandits Lower Bound",
      "text" : "All lemmas in this section are used in Section 4 to prove the fair lower bound in Theorem 3. The first, Lemma 4, lets us analyze distributions over payoffs.\nProof of Lemma 4. Let Ri represent the joint distribution on rewards for either experiment: in both cases, the joint distribution on rewards is identical, since the process which generates them is the same.\nWe will use the notation m, d1, . . . , dt to represent some fixed realization of the random variables µi, r 1 i , . . . , r t i and µ ′ i, r 1 i , . . . , r t i . In particular, it suffices to show that\nP(µi,r1i ,...,rti)∼W [ (µi, r 1 i , . . . , r t i) = (m, d 1, . . . , dt) ] = P(µ′i,r1i ,...,rti)∼W ′ [ (µ′i, r 1 i , . . . , r t i) = (m, d 1, . . . , dt) ] .\nThe first experiment which generates (µi, r 1 i , . . . , r t i) according to W has probability mass on this particular value of its random variables:\nP(µi,r1i ,...,rti)∼W [ (µi, r 1 i , . . . , r t i) = (m, d 1, . . . , dt) ] = Pµi∼Pi [µi = m] · Pr1i ,...,rti∼B(µi) [ (r1i , . . . , r t i) = (d 1, . . . , dt) ]\nThe second experiment has joint probability: P(µ′i,r1i ,...,rti)∼W ′ [ (µ′i, r 1 i , . . . , r t i) = (m, d 1, . . . , dt) ]\n= Pµ′i∼Pi(r1i ,...,rti) [ µ′i = m ] · P(r1i ,...,r1t )∼Ri [ (r1i , . . . , r 1 t ) = (d 1, . . . , dt) ]\n= P(µ′i,r1i ,...,rti)∼W [ (µ′i, r 1 i , . . . , r t i) = (m, d 1, . . . , dt) ]\nwhere equality follows from Bayes’ Rule.\nNext, we prove Lemma 5, used to reason about distinguishing between arms.\nProof of Lemma 5. Since neither i nor i + 1 is √ δ-distinguished by ht, for any αi ∈ {13 + i 3k , 1 3 + i+1 3k }, αi+1 ∈ { 1 3 + i+1 3k , 1 3 + i+2 3k }, the posterior probability of µi = αi is less than 1 − √ δ, and in particular for α = 13 + i+1 3k , it must be the case that\nPµi∼Pi(ht) [µi = α] > √ δ and also Pµi+1∼Pi(ht) [µi+1 = α] > √ δ.\nSince P = ∏ i Pi, we know that\nPµi,µi+1∼P (ht) [µi = α = µi+1] = Pµi∼Pi(ht) [µi = α] · Pµi+1∼Pi+1(ht) [µi+1 = α] > δ\nwhich completes the proof.\nFinally, we prove Lemma 6, which lets us reason about how fair algorithm choices depend on histories.\nProof of Lemma 6. We will define a set of histories which cause A to play some pair of arms i and i + 1 with different probabilities when µi = µi+1. Define the set unfair(A, µ) such that ht ∈ unfair(A, µ) if there exist i ∈ [k − 1], t′ ∈ [t] such that µi = µi+1 but πt ′\ni|ht′ 6= π t′ i+1|ht′ .\nConsider some ht which has not √\n2δ-distinguished any arm, such that there exists some i, t′ for which πt ′\ni|ht′ 6= 1 k . Then, in particular, there exists some i ∈ [k − 1] such that π\nt′ i|ht′ 6= π t′ i+1|ht′ . By Lemma 5, for all i and in particular this i, it is the case that 2δ < Pµ′∼P |ht [ µ′i = µ ′ i+1 ] = X and so\n2δ < X = Pµ′∼P |ht [ µ′i = µ ′ i+1 ∩ πt ′ i|ht 6= π t′ i+1|ht ] ≤ Pµ′∼P |ht [ ht ∈ unfair(A, µ′) ] (4)\nwhere the first equality comes from the fact that ht is a history for which πt ′ i|ht 6= π t′\ni+1|ht and the second equality from the definition of the set unfair.\nWe will show that Equation 4 cannot hold with probability more than 12 over the draw of µ, h t from the underlying distribution, or else A would not satisfy δ-fairness. Since A is δ-fair, for any fixed µ\nδ ≥ Pht∼A|µ [ ht ∈ unfair(A, µ) ] and therefore for any distribution P over µ that\nδ ≥ Pµ∼P,ht∼A|µ [ ht ∈ unfair(A, µ) ] .\nLemma 4 implies also δ ≥ Pµ∼P,ht∼A|µ,µ′∼P |ht [ ht ∈ unfair(A, µ′) ] , so by Markov’s inequality\n1 2 ≥ Pµ∼P,ht∼A|µ\n[ Pµ′∼P |ht [ ht ∈ unfair(A, µ′) ] ≥ 2δ ] .\nThus, with probability at least 12 over the distribution over histories and means,\nPµ′∼P |ht [ ht ∈ unfair(A, µ′) ] ≤ 2δ.\nHowever, Equation 4 shows this does not hold for any ht which does not √\n2δ-distinguish any arm but for which πt ′\ni|ht′ 6= 1 k for some i ∈ [k], t ′ ≤ t. Thus, for at least 12 of all probability mass over histories, either πt ′\ni|ht′ = 1 k for all i, t\n′ ≤ t, or ht must √ 2δ-distinguish some arm."
    }, {
      "heading" : "C Missing Proofs for the Contextual Bandit Setting",
      "text" : "We begin by proving two results related to KWIKToFair. The first, Lemma 8, was used in Section 5 to prove that KWIKToFair is δ-fair in Theorem 5.\nProof of Lemma 8. We will refer to a violation of either (a) or (b) as a failure of learner Li. For each Li, the set of queries asked of it are pairs (hi, x t i), histories along with new contexts. There are at most T contexts queried, and at most T histories on which Li is queried for a fixed run of our algorithm (namely, prefixes of Li’s final history). Thus, there are at most T 2 queries for Li. Thus, by a union bound over these T 2 queries for learner Li, by the KWIK guarantee, P [Li fails in some round] ≤ T 2δ∗ = min(δ, 1T )/k, and by a union bound over k arms, P [A learner fails in a round] ≤ min(δ, 1T ).\nWe proceed to Theorem 4, used in Section 5 to construct a δ-fair algorithm with quantified regret from KWIK learners.\nProof of Theorem 4. We use repeated calls to KWIKToFair (δ, T ) to run for an indefinite number of rounds. Specifically, we will make calls E = 1, 2, . . . to KWIKToFair (6δ/π(log(T )2, 2E). We will refer to each such call to KWIKToFair by its epoch E. By Lemma 8, each epoch E is 6δ/πE2k-fair, i.e. has a 6δ/πE2k probability of violating fairness. Therefore by a union bound across epochs, the probability of ever violating fairness through repeated calls to KWIKToFair is bounded above by ∑∞ E=1 6δ (πE)2 = 6δ π2 ∑∞ E=1 1 E2 = δ, so the overall algorithm is δ-fair.\nNext, by Lemma 9 each epoch E contributes at most regret 3 · 2Ek ∗E where ∗E denotes the value of ∗ used in epoch E, i.e. ∗E satisfying ∗ E = k ·m( ∗E , 6δ/πE2, 2E). Then since each epoch E covers 2E rounds, through round T the algorithm has used fewer than log(T ) epochs, and by the\ndoubling trick achieves regret R(T ) < ∑log(T )\nE=1 3 · 2Ek ∗E = O(Tk ∗) = O(k2 ·m( ∗, δ∗)).\nNext, we address the special subcase of KWIKToFair for linear functions outlined in Corollary 1.\nProof of Corollary 1. By Lemma 10, for each arm j the associated learner Lj has mistake bound m( , δ) = Õ(d3/ 4). Since ∗ satisfies ∗ = k ·m( ∗, δ)/T we get ∗ = ( kd3\nT\n)1/5 Substituting this\ninto Theorem 4, the overall regret guarantee satisfies regret R(T ) = O(k2 ·m( ∗, δ∗)) = O(Tk ∗) = O(T 4/5k6/5d3/5).\nThis brings us to the formal algorithm description of ConjunctionBandit and its corresponding regret bound, used in Section 6.1 as an example of an unfair learning algorithm for conjunctions.\n1: procedure ConjunctionBandit 2: Let C∗j ← {1, 2, . . . , d} for all j ∈ [k] . Initialize set of candidate variables for fj\n3: for t = 1, 2, . . . do 4: St ← ∅ . Initialize active set of arms 5: for j = 1, 2, . . . , k do 6: if ∧m∈C∗j x t j,m = 1 then 7: St ← St ∪ {j} . Add arm j to active set 8: if St = ∅ then 9: Pull arm j∗ ← (x ∈R [k]) . Pull arm at random 10: if rtj∗ = 1 then 11: C∗j∗ ← C∗j∗ \\ {m | xtj,m = 0} 12: else 13: Pull arm j∗ ← (x ∈R St) . Pull arm from active set at random\nWe can now upper bound the regret achieved by ConjunctionBandit.\nLemma 13. ConjunctionBandit achieves regret R(T ) = O(k2d).\nProof of Lemma 13. First, we claim that for every j, for the duration of the algorithm, that Cj ⊆ C∗j , where Cj is the true set of variables corresponding to fj . This holds at initialization: Cj ⊆ [d] = C∗j . Suppose the claim holds prior to round t: if C ∗ j is updated in this round, then fj(x t j) = 1 ⇒ ∀m ∈ [d] : xtj,m = 0,m /∈ Cj . Thus, C∗j = C∗j \\{m : xtj,m = 0} = C∗j \\{m : xtj,m = 0∩m /∈ Cj} ⊃ Cj . Therefore, the algorithm never makes false positive mistakes: in any round t, j ∈ St ⇒ fj(xtj) = 1. Therefore ConjunctionBandit only accumulates regret in rounds where it makes false negative mistakes by predicting that all arms have reward 0 when some arm has reward 1.\nThen, we have Regret(x1, . . . , xT ) = ∑\nt maxj ( fj(x t j) ) − E [∑ t fit(x t it) ] . We then rewrite the\nfirst term as ∑\nt maxj ( fj(x t j) ) = ∑ t I{fj(xtj) = 1 for some j ∈ [k]} and the second term as\nE [∑ t fit(x t it) ] = T − E [∑ t I{St = ∅ ∧ fj∗(xtj∗) = 0 ∧ fj(xtj) = 1 for some j ∈ [k]} ]\n≥ T − E [∑ t I{fj∗(xtj∗) = 0 | St = ∅ ∧ fj(xtj) = 1 for some j ∈ [k]} ]\n≥ T − ∑ j∈[k] E [∑ t I{fj∗(xtj∗) = 0 | St = ∅ ∧ fj(xtj) = 1} ]\n≥ T − ∑ j∈[k] kd = T − k2d\nwhere the last inequality follows from P [ j∗ = j | St = ∅ ∧ fj(xtj) = 1 ] = 1k and the fact that if St = ∅ and fj∗(xtj∗) = 1 then Cj∗ loses at least one of d variables, and this loss can therefore occur at most d times for each arm j. Substituting this into the original regret expression then yields\nRegret(x1, . . . , xT ) ≤ ∑ t I{fj(xtj) = 1 for some j ∈ [k]} − (T − k2d)\n≤ T − (T − k2d) ≤ k2d.\nFinally, we prove Lemma 12, which we used in Section 6.1 to translate between fair and KWIK learning on conjunctions.\nProof of Lemma 12. We mimic the structure of the proof of Theorem 6, once again using FairToKWIK to construct a KWIK learner B by running the given fair algorithm A on a constructed bandit instance for each context xt.\nThere are two primary modifications for the specific case of conjunctions: as conjunctions output either 0 or 1 we set = 0, ∗ = 1, and δ∗ = δ2T . A therefore runs on 2T histories and contexts, either of form (xt, x(0) = 0) or (xt, x(1) = 1). Since we initialize A to be δ∗-fair, if we fix history ht along with context and arm assignment (xt, x(`)) then, with probability at least 1− δ∗, pt,`1 > p t,` 2 implies f∗(xt) > `/2 and similarly pt,`2 > p t,` 1 implies f\n∗(xt) < `/2. Union bounding over all such t and ` yields that A satisfies this fairness over all t and ` with probability at least 1 − δ, and we condition on this event for the rest of the proof.\nWe proceed to prove that the resulting KWIK learner B is -accurate. Here, as = 0, this requires showing that all of B’s numerical predictions are correct. Assume instead that B outputs an incorrect prediction on (xt, x(`)). By the construction of FairToKWIK, a prediction from B implies that at least one of pt,01 , p t,1 1 , p t,0 2 and p t,1 2 is distinct from the others. We condition on this distinctness to get two cases. In the first case, pt,`1 ≤ p t,` 2 for both ` = 0 and 1. By distinctness, this means that either p t,0 1 < p t,0 2 or pt,11 < p t,1 2 . By the fairness assumption, this respectively implies that f ∗(xt) < f(x(0)) = 0 or f∗(xt) < f(x(1)) = 1. In either event, f∗x(t) = 0 = ŷt. In the second case, pt,`1 > p t,` 2 for at least one of ` = 0 or 1. pt,11 > p t,1 2 violates the fairness assumption on A as f(x(1)) = 1, so it must be that pt,01 > p t,0 2 . Fairness then implies that f\n∗(xt) = 1 = ŷt. Therefore B is -accurate. It remains to upper bound m( , δ). Any round where B outputs ⊥ means a choice between two contexts, one of which has a difference of 1 between arms. It follows that choosing randomly between both arms and contexts incurs expected regret 1/4. Therefore m( ,δ)4 < R(m( , δ), δ ∗, d) = R(m( , δ), δ2T , d)."
    } ],
    "references" : [ {
      "title" : "Auditing black-box models by obscuring features",
      "author" : [ "Philip Adler", "Casey Falk", "Sorelle A. Friedler", "Gabriel Rybeck", "Carlos Scheidegger", "Brandon Smith", "Suresh Venkatasubramanian" ],
      "venue" : "CoRR, abs/1602.07043,",
      "citeRegEx" : "Adler et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Adler et al\\.",
      "year" : 2016
    }, {
      "title" : "Taming the monster: A fast and simple algorithm for contextual bandits",
      "author" : [ "Alekh Agarwal", "Daniel J. Hsu", "Satyen Kale", "John Langford", "Lihong Li", "Robert E. Schapire" ],
      "venue" : "In Proceedings of the 31th International Conference on Machine Learning,",
      "citeRegEx" : "Agarwal et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Agarwal et al\\.",
      "year" : 2014
    }, {
      "title" : "Graphical models for bandit problems",
      "author" : [ "Kareem Amin", "Michael Kearns", "Umar Syed" ],
      "venue" : "arXiv preprint arXiv:1202.3782,",
      "citeRegEx" : "Amin et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Amin et al\\.",
      "year" : 2012
    }, {
      "title" : "Large-scale bandit problems and kwik learning",
      "author" : [ "Kareem Amin", "Michael Kearns", "Moez Draief", "Jacob D Abernethy" ],
      "venue" : "In Proceedings of the 30th International Conference on Machine Learning",
      "citeRegEx" : "Amin et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Amin et al\\.",
      "year" : 2013
    }, {
      "title" : "Finite-time analysis of the multiarmed bandit problem",
      "author" : [ "Peter Auer", "Nicolo Cesa-Bianchi", "Paul Fischer" ],
      "venue" : "Machine learning,",
      "citeRegEx" : "Auer et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Auer et al\\.",
      "year" : 2002
    }, {
      "title" : "Big data’s disparate impact",
      "author" : [ "Solon Barocas", "Andrew D. Selbst" ],
      "venue" : "California Law Review,",
      "citeRegEx" : "Barocas and Selbst.,? \\Q2016\\E",
      "shortCiteRegEx" : "Barocas and Selbst.",
      "year" : 2016
    }, {
      "title" : "The new science of sentencing",
      "author" : [ "Anna Maria Barry-Jester", "Ben Casselman", "Dana Goldstein" ],
      "venue" : "The Marshall Project, August",
      "citeRegEx" : "Barry.Jester et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Barry.Jester et al\\.",
      "year" : 2015
    }, {
      "title" : "Contextual bandit algorithms with supervised learning guarantees",
      "author" : [ "Alina Beygelzimer", "John Langford", "Lihong Li", "Lev Reyzin", "Robert E. Schapire" ],
      "venue" : "In Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics,",
      "citeRegEx" : "Beygelzimer et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Beygelzimer et al\\.",
      "year" : 2011
    }, {
      "title" : "Regret analysis of stochastic and nonstochastic multi-armed bandit problems",
      "author" : [ "Sébastien Bubeck", "Nicolo Cesa-Bianchi" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Bubeck and Cesa.Bianchi.,? \\Q2012\\E",
      "shortCiteRegEx" : "Bubeck and Cesa.Bianchi.",
      "year" : 2012
    }, {
      "title" : "Artificial intolerance. MIT Technology Review, March 28 2016",
      "author" : [ "Nanette Byrnes" ],
      "venue" : "URL https://www. technologyreview.com/s/600996/artificial-intolerance/. Retrieved 4/28/2016",
      "citeRegEx" : "Byrnes.,? \\Q2016\\E",
      "shortCiteRegEx" : "Byrnes.",
      "year" : 2016
    }, {
      "title" : "Three naive bayes approaches for discrimination-free classification",
      "author" : [ "Toon Calders", "Sicco Verwer" ],
      "venue" : "Data Mining and Knowledge Discovery,",
      "citeRegEx" : "Calders and Verwer.,? \\Q2010\\E",
      "shortCiteRegEx" : "Calders and Verwer.",
      "year" : 2010
    }, {
      "title" : "Contextual bandits with linear payoff functions",
      "author" : [ "Wei Chu", "Lihong Li", "Lev Reyzin", "Robert E. Schapire" ],
      "venue" : "In Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics,",
      "citeRegEx" : "Chu et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Chu et al\\.",
      "year" : 2011
    }, {
      "title" : "Regulating by robot: Administrative decision-making in the machine-learning era",
      "author" : [ "Cary Coglianese", "David Lehr" ],
      "venue" : "Georgetown Law Journal,",
      "citeRegEx" : "Coglianese and Lehr.,? \\Q2016\\E",
      "shortCiteRegEx" : "Coglianese and Lehr.",
      "year" : 2016
    }, {
      "title" : "Fairness through awareness",
      "author" : [ "Cynthia Dwork", "Moritz Hardt", "Toniann Pitassi", "Omer Reingold", "Richard Zemel" ],
      "venue" : "In Proceedings of the 3rd Innovations in Theoretical Computer Science Conference,",
      "citeRegEx" : "Dwork et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Dwork et al\\.",
      "year" : 2012
    }, {
      "title" : "Certifying and removing disparate impact",
      "author" : [ "Michael Feldman", "Sorelle A. Friedler", "John Moeller", "Carlos Scheidegger", "Suresh Venkatasubramanian" ],
      "venue" : "In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data",
      "citeRegEx" : "Feldman et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Feldman et al\\.",
      "year" : 2015
    }, {
      "title" : "A confidence-based approach for balancing fairness and accuracy",
      "author" : [ "Benjamin Fish", "Jeremy Kun", "Ádám D Lelkes" ],
      "venue" : "SIAM International Symposium on Data Mining,",
      "citeRegEx" : "Fish et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Fish et al\\.",
      "year" : 2016
    }, {
      "title" : "Navigating the “trackless ocean”: Fairness in big data research and decision making",
      "author" : [ "FTC Commisioner Julie Brill" ],
      "venue" : "Keynote Address at the Columbia",
      "citeRegEx" : "Brill.,? \\Q2015\\E",
      "shortCiteRegEx" : "Brill.",
      "year" : 2015
    }, {
      "title" : "Fairness-aware learning through regularization approach",
      "author" : [ "Toshihiro Kamishima", "Shotaro Akaho", "Jun Sakuma" ],
      "venue" : "In Data Mining Workshops (ICDMW),",
      "citeRegEx" : "Kamishima et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Kamishima et al\\.",
      "year" : 2011
    }, {
      "title" : "Sequential choice from several populations",
      "author" : [ "Michael N Katehakis", "Herbert Robbins" ],
      "venue" : "PROCEEDINGS-NATIONAL ACADEMY OF SCIENCES USA,",
      "citeRegEx" : "Katehakis and Robbins.,? \\Q1995\\E",
      "shortCiteRegEx" : "Katehakis and Robbins.",
      "year" : 1995
    }, {
      "title" : "Asymptotically efficient adaptive allocation rules",
      "author" : [ "Tze Leung Lai", "Herbert Robbins" ],
      "venue" : "Advances in applied mathematics,",
      "citeRegEx" : "Lai and Robbins.,? \\Q1985\\E",
      "shortCiteRegEx" : "Lai and Robbins.",
      "year" : 1985
    }, {
      "title" : "A unifying framework for computational reinforcement learning theory",
      "author" : [ "Lihong Li" ],
      "venue" : "PhD thesis, Rutgers, The State University of New Jersey,",
      "citeRegEx" : "Li.,? \\Q2009\\E",
      "shortCiteRegEx" : "Li.",
      "year" : 2009
    }, {
      "title" : "Knows what it knows: a framework for self-aware learning",
      "author" : [ "Lihong Li", "Michael L Littman", "Thomas J Walsh", "Alexander L Strehl" ],
      "venue" : "Machine learning,",
      "citeRegEx" : "Li et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2011
    }, {
      "title" : "k-nn as an implementation of situation testing for discrimination discovery and prevention",
      "author" : [ "Binh Thanh Luong", "Salvatore Ruggieri", "Franco Turini" ],
      "venue" : "In Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining,",
      "citeRegEx" : "Luong et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Luong et al\\.",
      "year" : 2011
    }, {
      "title" : "Can an algorithm hire better than a human",
      "author" : [ "Clair C Miller" ],
      "venue" : "The New York Times, June",
      "citeRegEx" : "Miller.,? \\Q2015\\E",
      "shortCiteRegEx" : "Miller.",
      "year" : 2015
    }, {
      "title" : "Big data: A report on algorithmic systems, opportunity, and civil rights",
      "author" : [ "Cecilia Munoz", "Megan Smith", "DJ Patil" ],
      "venue" : "Technical report, Executive Office of the President,",
      "citeRegEx" : "Munoz et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Munoz et al\\.",
      "year" : 2016
    }, {
      "title" : "Big data: Seizing opportunities, protecting values. Technical report, Executive Office of the President",
      "author" : [ "John Podesta", "Penny Pritzker", "Ernest J. Moniz", "John Holdern", "Jeffrey Zients" ],
      "venue" : null,
      "citeRegEx" : "Podesta et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Podesta et al\\.",
      "year" : 2014
    }, {
      "title" : "Online linear regression and its application to model-based reinforcement",
      "author" : [ "Michael L Littman" ],
      "venue" : null,
      "citeRegEx" : "Strehl and Littman.,? \\Q2016\\E",
      "shortCiteRegEx" : "Strehl and Littman.",
      "year" : 2016
    }, {
      "title" : "Discrimination in online ad delivery",
      "author" : [ "Rich Zemel", "Yu Wu", "Kevin Swersky", "Toni Pitassi", "Cynthia Dwork" ],
      "venue" : "Neural Information Processing Systems, pages 1417–1424,",
      "citeRegEx" : "Zemel et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Zemel et al\\.",
      "year" : 2008
    } ],
    "referenceMentions" : [ {
      "referenceID" : 23,
      "context" : "Automated techniques from statistics and machine learning are increasingly being used to make decisions that have important consequences on people’s lives, including hiring [Miller, 2015], lending [Byrnes, 2016], policing [Rudin, 2013], and even criminal sentencing [Barry-Jester et al.",
      "startOffset" : 173,
      "endOffset" : 187
    }, {
      "referenceID" : 9,
      "context" : "Automated techniques from statistics and machine learning are increasingly being used to make decisions that have important consequences on people’s lives, including hiring [Miller, 2015], lending [Byrnes, 2016], policing [Rudin, 2013], and even criminal sentencing [Barry-Jester et al.",
      "startOffset" : 197,
      "endOffset" : 211
    }, {
      "referenceID" : 6,
      "context" : "Automated techniques from statistics and machine learning are increasingly being used to make decisions that have important consequences on people’s lives, including hiring [Miller, 2015], lending [Byrnes, 2016], policing [Rudin, 2013], and even criminal sentencing [Barry-Jester et al., 2015].",
      "startOffset" : 266,
      "endOffset" : 293
    }, {
      "referenceID" : 5,
      "context" : "These high stakes uses of machine learning have led to increasing concern in law and policy circles about the potential for (often opaque) machine learning techniques to be discriminatory or unfair [Coglianese and Lehr, 2016, Barocas and Selbst, 2016]. Moreover, these concerns are not merely hypothetical: Sweeney [2013] observed that contextual ads for public record services shown in response to Google searches for stereotypically African American names were more likely to contain text referring to arrest records, compared to comparable ads shown in response to searches for stereotypically Caucasian names, which showed more neutral text.",
      "startOffset" : 226,
      "endOffset" : 322
    }, {
      "referenceID" : 25,
      "context" : "1 For example, a 2014 White House report [Podesta et al., 2014] notes that “[t]he increasing use of algorithms to make eligibility decisions must be carefully monitored for potential discriminatory outcomes for disadvantaged groups, even absent discriminatory intent.",
      "startOffset" : 41,
      "endOffset" : 63
    }, {
      "referenceID" : 24,
      "context" : "” Along the same lines, a 2016 White House report [Munoz et al., 2016] observes that “[a]s improvements in the uses of big data and machine learning continue, it will remain important not to place too much reliance on these new systems without questioning and continuously testing the inputs and mechanics behind them and the results they produce.",
      "startOffset" : 50,
      "endOffset" : 70
    }, {
      "referenceID" : 4,
      "context" : "Without a fairness constraint, it is known that it is possible to guarantee non-trivial regret to the optimal policy after only T = O(k) many rounds [Auer et al., 2002].",
      "startOffset" : 149,
      "endOffset" : 168
    }, {
      "referenceID" : 4,
      "context" : "Without a fairness constraint, it is known that it is possible to guarantee non-trivial regret to the optimal policy after only T = O(k) many rounds [Auer et al., 2002]. In Section 3, we give an algorithm that satisfies our fairness constraint and is able to guarantee non-trivial regret after T = O(k3) rounds. We then show in Section 4 that it is not possible to do better – any It is natural that different populations should have different underlying functions – for example, in a college admissions setting, the function mapping applications to college success probability might weight SAT scores less in a wealthy population that employs SAT tutors, and more in a working-class population that does not – see Dwork et al. [2012] for more discussion of this issue and Munoz et al.",
      "startOffset" : 150,
      "endOffset" : 735
    }, {
      "referenceID" : 4,
      "context" : "Without a fairness constraint, it is known that it is possible to guarantee non-trivial regret to the optimal policy after only T = O(k) many rounds [Auer et al., 2002]. In Section 3, we give an algorithm that satisfies our fairness constraint and is able to guarantee non-trivial regret after T = O(k3) rounds. We then show in Section 4 that it is not possible to do better – any It is natural that different populations should have different underlying functions – for example, in a college admissions setting, the function mapping applications to college success probability might weight SAT scores less in a wealthy population that employs SAT tutors, and more in a working-class population that does not – see Dwork et al. [2012] for more discussion of this issue and Munoz et al. [2016] for examples.",
      "startOffset" : 150,
      "endOffset" : 793
    }, {
      "referenceID" : 21,
      "context" : "We then move on to the general contextual bandit setting and prove a broad characterization result, relating fair contextual bandit learning to KWIK learning [Li et al., 2011].",
      "startOffset" : 158,
      "endOffset" : 175
    }, {
      "referenceID" : 21,
      "context" : "This general connection has immediate implications, because it allows us to import known results for KWIK learning [Li et al., 2011].",
      "startOffset" : 115,
      "endOffset" : 132
    }, {
      "referenceID" : 7,
      "context" : "Calders and Verwer [2010], Luong et al.",
      "startOffset" : 0,
      "endOffset" : 26
    }, {
      "referenceID" : 7,
      "context" : "Calders and Verwer [2010], Luong et al. [2011], Kamishima et al.",
      "startOffset" : 0,
      "endOffset" : 47
    }, {
      "referenceID" : 7,
      "context" : "Calders and Verwer [2010], Luong et al. [2011], Kamishima et al. [2011], Feldman et al.",
      "startOffset" : 0,
      "endOffset" : 72
    }, {
      "referenceID" : 7,
      "context" : "Calders and Verwer [2010], Luong et al. [2011], Kamishima et al. [2011], Feldman et al. [2015], Fish et al.",
      "startOffset" : 0,
      "endOffset" : 95
    }, {
      "referenceID" : 7,
      "context" : "Calders and Verwer [2010], Luong et al. [2011], Kamishima et al. [2011], Feldman et al. [2015], Fish et al. [2016] and Adler et al.",
      "startOffset" : 0,
      "endOffset" : 115
    }, {
      "referenceID" : 0,
      "context" : "[2016] and Adler et al. [2016] for a study of auditing existing algorithms for disparate impact).",
      "startOffset" : 11,
      "endOffset" : 31
    }, {
      "referenceID" : 0,
      "context" : "[2016] and Adler et al. [2016] for a study of auditing existing algorithms for disparate impact). While statistical parity is sometimes a desirable goal – indeed, it is sometimes required by law – as observed by Dwork et al. [2012] and others, it suffers from two problems.",
      "startOffset" : 11,
      "endOffset" : 232
    }, {
      "referenceID" : 0,
      "context" : "[2016] and Adler et al. [2016] for a study of auditing existing algorithms for disparate impact). While statistical parity is sometimes a desirable goal – indeed, it is sometimes required by law – as observed by Dwork et al. [2012] and others, it suffers from two problems. First, if different populations indeed have different statistical properties, then it can be at odds with accurate classification. Second, even in cases when statistical parity is attainable with an optimal classifier, it does not prevent discrimination at an individual level – see Dwork et al. [2012] for a catalog of ways in which statistical parity can be insufficient from the perspective of fairness.",
      "startOffset" : 11,
      "endOffset" : 577
    }, {
      "referenceID" : 0,
      "context" : "[2016] and Adler et al. [2016] for a study of auditing existing algorithms for disparate impact). While statistical parity is sometimes a desirable goal – indeed, it is sometimes required by law – as observed by Dwork et al. [2012] and others, it suffers from two problems. First, if different populations indeed have different statistical properties, then it can be at odds with accurate classification. Second, even in cases when statistical parity is attainable with an optimal classifier, it does not prevent discrimination at an individual level – see Dwork et al. [2012] for a catalog of ways in which statistical parity can be insufficient from the perspective of fairness. In contrast, we study a notion aimed at guaranteeing fairness at the individual level. Our definition of fairness is most closely related to that of Dwork et al. [2012], who proposed and explored the basic properties of a technical definition of individual fairness formalizing the idea that “similar individuals should be treated similarly”.",
      "startOffset" : 11,
      "endOffset" : 850
    }, {
      "referenceID" : 0,
      "context" : "[2016] and Adler et al. [2016] for a study of auditing existing algorithms for disparate impact). While statistical parity is sometimes a desirable goal – indeed, it is sometimes required by law – as observed by Dwork et al. [2012] and others, it suffers from two problems. First, if different populations indeed have different statistical properties, then it can be at odds with accurate classification. Second, even in cases when statistical parity is attainable with an optimal classifier, it does not prevent discrimination at an individual level – see Dwork et al. [2012] for a catalog of ways in which statistical parity can be insufficient from the perspective of fairness. In contrast, we study a notion aimed at guaranteeing fairness at the individual level. Our definition of fairness is most closely related to that of Dwork et al. [2012], who proposed and explored the basic properties of a technical definition of individual fairness formalizing the idea that “similar individuals should be treated similarly”. Specifically, their work presupposes the existence of a task-specific metric on individuals, and proposes that fair algorithms should satisfy a Lipschitz condition with respect to this metric. Our definition of fairness is similar, in that the expected reward of each arm is a natural metric through which we define fairness. The main conceptual distinction between our work and Dwork et al. [2012] is that their work operates under the assumption that the metric is known to the algorithm designer, and hence in their setting, the fairness constraint binds only insofar as it is in conflict with the desired outcome of the algorithm designer.",
      "startOffset" : 11,
      "endOffset" : 1423
    }, {
      "referenceID" : 0,
      "context" : "[2016] and Adler et al. [2016] for a study of auditing existing algorithms for disparate impact). While statistical parity is sometimes a desirable goal – indeed, it is sometimes required by law – as observed by Dwork et al. [2012] and others, it suffers from two problems. First, if different populations indeed have different statistical properties, then it can be at odds with accurate classification. Second, even in cases when statistical parity is attainable with an optimal classifier, it does not prevent discrimination at an individual level – see Dwork et al. [2012] for a catalog of ways in which statistical parity can be insufficient from the perspective of fairness. In contrast, we study a notion aimed at guaranteeing fairness at the individual level. Our definition of fairness is most closely related to that of Dwork et al. [2012], who proposed and explored the basic properties of a technical definition of individual fairness formalizing the idea that “similar individuals should be treated similarly”. Specifically, their work presupposes the existence of a task-specific metric on individuals, and proposes that fair algorithms should satisfy a Lipschitz condition with respect to this metric. Our definition of fairness is similar, in that the expected reward of each arm is a natural metric through which we define fairness. The main conceptual distinction between our work and Dwork et al. [2012] is that their work operates under the assumption that the metric is known to the algorithm designer, and hence in their setting, the fairness constraint binds only insofar as it is in conflict with the desired outcome of the algorithm designer. The most challenging aspect of this approach (as they acknowledge) is that it requires that some third party design a “fair” metric on individuals, which in a sense encodes much of the relevant challenge. The question of how to design such a metric was considered by Zemel et al. [2013], who study methods to learn representations that encode the data, while obscuring protected attributes.",
      "startOffset" : 11,
      "endOffset" : 1955
    }, {
      "referenceID" : 0,
      "context" : "[2016] and Adler et al. [2016] for a study of auditing existing algorithms for disparate impact). While statistical parity is sometimes a desirable goal – indeed, it is sometimes required by law – as observed by Dwork et al. [2012] and others, it suffers from two problems. First, if different populations indeed have different statistical properties, then it can be at odds with accurate classification. Second, even in cases when statistical parity is attainable with an optimal classifier, it does not prevent discrimination at an individual level – see Dwork et al. [2012] for a catalog of ways in which statistical parity can be insufficient from the perspective of fairness. In contrast, we study a notion aimed at guaranteeing fairness at the individual level. Our definition of fairness is most closely related to that of Dwork et al. [2012], who proposed and explored the basic properties of a technical definition of individual fairness formalizing the idea that “similar individuals should be treated similarly”. Specifically, their work presupposes the existence of a task-specific metric on individuals, and proposes that fair algorithms should satisfy a Lipschitz condition with respect to this metric. Our definition of fairness is similar, in that the expected reward of each arm is a natural metric through which we define fairness. The main conceptual distinction between our work and Dwork et al. [2012] is that their work operates under the assumption that the metric is known to the algorithm designer, and hence in their setting, the fairness constraint binds only insofar as it is in conflict with the desired outcome of the algorithm designer. The most challenging aspect of this approach (as they acknowledge) is that it requires that some third party design a “fair” metric on individuals, which in a sense encodes much of the relevant challenge. The question of how to design such a metric was considered by Zemel et al. [2013], who study methods to learn representations that encode the data, while obscuring protected attributes. Our fairness constraint, conversely, is entirely aligned with the goal of the algorithm designer in that it is satisfied by the optimal policy; nevertheless, it affects the space of feasible learning algorithms, because it interferes with learning an optimal policy, which depends on the unknown reward functions. At a technical level, our work is related to Amin et al. [2012] and Amin et al.",
      "startOffset" : 11,
      "endOffset" : 2437
    }, {
      "referenceID" : 0,
      "context" : "[2016] and Adler et al. [2016] for a study of auditing existing algorithms for disparate impact). While statistical parity is sometimes a desirable goal – indeed, it is sometimes required by law – as observed by Dwork et al. [2012] and others, it suffers from two problems. First, if different populations indeed have different statistical properties, then it can be at odds with accurate classification. Second, even in cases when statistical parity is attainable with an optimal classifier, it does not prevent discrimination at an individual level – see Dwork et al. [2012] for a catalog of ways in which statistical parity can be insufficient from the perspective of fairness. In contrast, we study a notion aimed at guaranteeing fairness at the individual level. Our definition of fairness is most closely related to that of Dwork et al. [2012], who proposed and explored the basic properties of a technical definition of individual fairness formalizing the idea that “similar individuals should be treated similarly”. Specifically, their work presupposes the existence of a task-specific metric on individuals, and proposes that fair algorithms should satisfy a Lipschitz condition with respect to this metric. Our definition of fairness is similar, in that the expected reward of each arm is a natural metric through which we define fairness. The main conceptual distinction between our work and Dwork et al. [2012] is that their work operates under the assumption that the metric is known to the algorithm designer, and hence in their setting, the fairness constraint binds only insofar as it is in conflict with the desired outcome of the algorithm designer. The most challenging aspect of this approach (as they acknowledge) is that it requires that some third party design a “fair” metric on individuals, which in a sense encodes much of the relevant challenge. The question of how to design such a metric was considered by Zemel et al. [2013], who study methods to learn representations that encode the data, while obscuring protected attributes. Our fairness constraint, conversely, is entirely aligned with the goal of the algorithm designer in that it is satisfied by the optimal policy; nevertheless, it affects the space of feasible learning algorithms, because it interferes with learning an optimal policy, which depends on the unknown reward functions. At a technical level, our work is related to Amin et al. [2012] and Amin et al. [2013], which also relate KWIK learning to bandit learning in a different context, unrelated to fairness (when the arm space is very large).",
      "startOffset" : 11,
      "endOffset" : 2460
    }, {
      "referenceID" : 13,
      "context" : "This relaxation is a special case of Dwork et al. [2012]’s proposed family of definitions, which require that “similar individuals be treated similarly”.",
      "startOffset" : 37,
      "endOffset" : 57
    }, {
      "referenceID" : 4,
      "context" : "In this section, we describe a simple and intuitive modification of the standard UCB algorithm [Auer et al., 2002], called FairBandits, prove that it is fair, and analyze its regret bound.",
      "startOffset" : 95,
      "endOffset" : 114
    }, {
      "referenceID" : 4,
      "context" : "This is reflected in its regret bound, which is only non-trivial after T k3, whereas the UCB algorithm [Auer et al., 2002] achieves non-trivial regret after T = O(k) rounds.",
      "startOffset" : 103,
      "endOffset" : 122
    }, {
      "referenceID" : 8,
      "context" : "However, it is known that Ω (√ kT ) regret is necessary even in the unrestricted setting (without fairness) if one does not make data-specific assumptions on an instance [Bubeck and Cesa-Bianchi, 2012] (e.",
      "startOffset" : 170,
      "endOffset" : 201
    }, {
      "referenceID" : 20,
      "context" : "This relies upon a known lower bound for KWIK learning conjunctions [Li, 2009]:",
      "startOffset" : 68,
      "endOffset" : 78
    } ],
    "year" : 2017,
    "abstractText" : "We introduce the study of fairness in multi-armed bandit problems. Our fairness definition can be interpreted as demanding that given a pool of applicants (say, for college admission or mortgages), a worse applicant is never favored over a better one, despite a learning algorithm’s uncertainty over the true payoffs. We prove results of two types: First, in the important special case of the classic stochastic bandits problem (i.e. in which there are no contexts), we provide a provably fair algorithm based on chained confidence intervals, and prove a cumulative regret bound with a cubic dependence on the number of arms. We further show that any fair algorithm must have such a dependence. When combined with regret bounds for standard non-fair algorithms such as UCB, this proves a strong separation between fair and unfair learning, which extends to the general contextual case. In the general contextual case, we prove a tight connection between fairness and the KWIK (Knows What It Knows) learning model: a KWIK algorithm for a class of functions can be transformed into a provably fair contextual bandit algorithm, and conversely any fair contextual bandit algorithm can be transformed into a KWIK learning algorithm. This tight connection allows us to provide a provably fair algorithm for the linear contextual bandit problem with a polynomial dependence on the dimension, and to show (for a different class of functions) a worst-case exponential gap in regret between fair and non-fair learning algorithms. ∗Department of Computer and Information Sciences, University of Pennsylvania. {majos,mkearns,jamiemor,aaroth}@cis.upenn.edu. AR is supported in part by an NSF CAREER award, a Sloan Foundation Fellowship, and a Google Faculty Research Award. 1 ar X iv :1 60 5. 07 13 9v 1 [ cs .L G ] 2 3 M ay 2 01 6",
    "creator" : "LaTeX with hyperref package"
  }
}