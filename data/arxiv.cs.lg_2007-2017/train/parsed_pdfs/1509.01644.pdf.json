{
  "name" : "1509.01644.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Reinforcement Learning with Parameterized Actions",
    "authors" : [ "Warwick Masson", "George Konidaris" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "In reinforcement learning, we typically consider either a discrete or a continuous action space (Sutton and Barto 1998). With a discrete action space, the agent makes decisions about which distinct action to perform from a finite action set. With a continuous action space, the selected action is expressed as a real-valued vector. If we use a continuous action space, we lose the ability to consider differences in kind: all actions must be expressible as a single vector. However, if we only use discrete actions, we are restricted to discrete actions or suffer a blow-up in the number of actions to represent a wide range of actions.\nA parameterized action is a discrete action parameterized by a real-valued vector. Modeling actions this way introduces structure into the action space by treating different kinds of continuous actions as distinct. At each step an agent must choose both which action to use and what parameters to execute it with. For example, consider a soccer playing robot which can kick, pass, or run. We can associate a continuous parameter vector to each of these actions: we can kick the ball to a given target position with a given force, pass to a specific position, and run with a given velocity. Each of these actions is parameterized in its own way, so expressing an action as a single vector containing all the parameters for each of these movements would be redundant, as only a small subset of the parameters are applicable.\nWe formally define parameterized action Markov decision processes (PAMDPs) to model the situations where we have distinct actions that require parameters to adjust the action\nCopyright c© 2015, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\nto different situations, or where there are multiple incompatible continuous actions. In this paper, we are concerned with how to learn a policy in domain with pre-defined parameterized actions. A naive approach would treat this as a direct policy search problem, where we optimize the weights for a parameterized policy. We compare this approach against two methods: Q-PAMDP(1) and Q-PAMDP(∞). We show that with appropriate update rules both methods converge to a local optimum. The three methods are compared in a robot soccer goal domain and a platformer domain."
    }, {
      "heading" : "2 Background",
      "text" : "A Markov decision process (MDP) is a tuple 〈S,A, P,R, γ〉, where S is a set of states, A is a set of actions, P (s, a, s′) is the probability of transitioning to state s′ from state s after taking action a, R(s, a, r) is the probability of receiving reward r for taking action a in state s, and γ is a discount factor (Sutton and Barto 1998). We wish to find a policy, π, which selects an action for each state so as to maximize the expected sum of discounted rewards (the return).\nThe value function V π(s) is defined as the expected discounted return and is given by\nV π(s) = Eπ [ ∞∑ t=0 γtrt ] ,\nfor the policy π starting at state s (Sutton and Barto 1998). Similarly, we can define the action-value function\nQπ(s, a) = Eπ [r0 + γV π(s)] , as the expected discounted return for taking action a in state s following policy π. While using the value function in control requires a model, we would prefer to do so without needing such a model. We can approach this problem by learning Q, as knowing the action-value function allows us to select the action which maximizesQπ(s, a). We can learnQ for an optimal policy using a method such as Q-learning (Watkins and Dayan 1992). In domains with a continuous state space, we can represent Q(s, a) using parametric function approximation with a set of parameters ω. Algorithms such as gradient descent SARSA(λ) (Sutton and Barto 1998) can learn such an approximation of Q(s, a).\nFor problems with a continuous action space (A ⊆ Rm), selecting the optimal action with respect to Q(s, a) is nontrivial, as it requires finding a global maximum for a function\nar X\niv :1\n50 9.\n01 64\n4v 1\n[ cs\n.A I]\n5 S\nep 2\n01 5\nin a continuous space. We can avoid this problem using a policy search algorithm, where a class of policies parameterized by a set of parameters θ is given, which transforms the problem into one of direct optimization over θ for an objective function J(θ). Several approaches to the policy search problem exist, including policy gradient methods, entropybased approaches, path integral approaches, and samplebased approaches (Deisenroth, Neumann, and Peters 2013).\nParameterized Tasks A parameterized task is a problem that is determined by task parameters τ . These parameters are fixed throughout an episode, and the goal is to optimize a task dependent policy. Kober et al. (2012) developed algorithms to adjust motor primitives to different task parameters. They apply this to learn table-tennis and darts with different starting positions and targets. Da Silva et al. (2012) introduced the idea of a parameterized skill as a task dependent parameterized policy. They sample a set of tasks, learn their associated parameters, and determine a mapping from tasks to parameters (da Silva, Konidaris, and Barto 2012). Deisenroth et al. (2014) applied a model-based method to learn a task dependent parameterized policy. This is used to learn task dependent policies for ball-hitting task, and for solving a block manipulation problem."
    }, {
      "heading" : "3 Parameterized Actions Markov Decision Processes",
      "text" : "We consider MDPs where the state space is continuous (S ⊆ Rn) and the actions are parameterized: there is a finite set of actions Ad = {a1, a2, . . . , ak}, and each action a ∈ Ad has a set of continuous parametersXa ⊆ Rma . The action space is then given by\nA = ⋃ a∈Ad {(a, x) | x ∈ Xa}.\nWe refer to such MDPs as parameterized action MDPs (PAMDPs). Figure 1 depicts the different action spaces.\nFor example, a quad-rotor delivery system might have continuous actions for moving around and for dropping its payload on a target. We would not want to combine these actions into a single continuous action lest we experiment with dropping payloads while moving. One solution would be to use discretization where we consider a fixed parameterization for the different actions. This approach has the problem of both selecting an appropriate granularity for the policy, and in the blow-up in the number of actions considered.\nWe apply a two-tiered approach for domains with parameterized actions: first, selecting the parameterized action, then providing the parameters for that action. The discrete-action policy is denoted πd(a|s). To select the parameters for the action, we define the action-parameter policy for action a as πa(x|s). The policy is then given by\nπ(a, x|s) = πd(a|s)πa(x|s). In other words, to select a complete action (a, x), we sample an action a from πd(a|s) and then sample a parameter x from πa(x|s).\nThe first approach we consider is one of direct policy search. The action policy is defined by the parameters ω and is denoted by πdω(a|s). The action-parameter policy for action a is determined by a set of parameters θa, and is denoted πaθ (x|s). We use a direct policy search method to optimize the objective function.\nJ(θ, ω) = Es0∼D[V πΘ(s0)].\nwith respect to (θ, ω), where s0 is an initial state sampled according to the initial state distributionD. J is the expected return for a given policy starting at an initial state.\nOur second approach is to alternate updating the parameter-policy and learning an action-value function. For any PAMDP M = 〈S,A, P,R, γ〉 with a fixed parameterpolicy πaθ , there exists a corresponding discrete action MDP, Mθ. We define Mθ = 〈S,Ad, Pθ, Rθ, γ〉 where Ad is the discrete action set and\nPθ(s, a, s ′) = ∫ x∈Xa πaθ (x|s)P (s, (a, x), s′)dx\nRθ(s, a, r) = ∫ x∈Xa πaθ (x|s)R(s, (a, x), r)dr.\nWe represent the action-value function forMθ using function approximation with parameters ω. For Mθ, there exists\nAlgorithm 1 Q-PAMDP(k) Input: Initial parameters θ Parameter update method P-UPDATE Q-learning algorithm Q-LEARN Algorithm: ω ← Q-LEARN(∞)(Mθ,−) repeat θ ← P-UPDATE(k)(Jω, θ) ω ← Q-LEARN(∞)(Mθ, ω)\nuntil θ converges\nan optimal set of representation weights ωθ∗ which maximizes J(θ, ω) with respect to ω. Let\nW (θ) = arg max ω J(θ, ω) = ωθ∗ .\nWe can computeW (θ) using a Q-learning algorithm such as Greedy-GQ (Maei et al. 2010) or gradient-descent Sarsa(λ) (Sutton and Barto 1998). Finally, we let\nJω(θ) = J(θ, ω),\nH(θ) = J(θ,W (θ)).\nIntuitively, H(θ) is the performance of the best discrete policy for Mθ.\nAlgorithm 1 describes a method for alternating updating θ and ω. The algorithm takes two input methods: PUPDATE and Q-LEARN. It takes a positive integer parameter k, which determines the number of P-UPDATE(f, θ) should be a policy search method that optimizes θ with respect to f . Q-LEARN can be any algorithm for Q-learning with function approximation, and is left as a design choice. We consider two main cases of the Q-PAMDP algorithm: Q-PAMDP(1) and Q-PAMDP(∞).\nFor Q-PAMDP(1), we perform a single update of θ, then relearn ω to W (θ). The idea is that to optimize J , we can define a function\nH(θ) = J(θ,W (θ)),\nwhich represents the best possible performance for parameters θ. In the next section we show that if we can find a local optima θ with respect to H , then we have found a local optima with respect to J . If at each step we only update θ once, then update ω until convergence, then we can optimize θ with respect to H .\nWith Q-PAMDP(∞) each step performs a full optimization on θ and then a full optimization of ω. The θ step would optimize J(θ, ω), not H(θ), as we do update ω while we update θ. This approach can be problematic due to premature optimization, and the requirement of full convergence (Thomas and Barto 2011). Full convergence is a problem because it requires a convergence test for every single update, as well as a test for the sequence as a whole."
    }, {
      "heading" : "4 Theoretical Results",
      "text" : "Now we show that Q-PAMDP(1) converges to a local or global optimum with mild assumptions. We assume that if\nwe iterate P-UPDATE, it will converge to some θ∗ with respect to a given objective function. As the P-UPDATE step is a design choice, it can be selected with the appropriate convergence property. We consider the sequence\nωt+1 = W (θt)\nθt+1 = P-UPDATE(Jωt , θt),\nfor some initial parameters θ. Q-PAMDP(1) is equivalent to this sequence if iteration of Q-LEARN converges to W (θ) for each given θ. Next we show that if P-UPDATE can locally optimizes θ with respect to some objective function, then Q-PAMDP(1) converges to a local optimum. Theorem 4.1 (Convergence to a Local Optimum). If the sequence ϑt+1 = P-UPDATE(H, θ0), converges to a local optima with respect to H , then QPAMDP(1) converges to a local optima with respect to J .\nProof. By definition of the sequence above ωt = W (θt), so it follows that\nJωt = J(θ,W (θ)) = H(θ).\nIn other words, the objective function J is equivalent to the objective functionH if the condition on ω is satisfied. Therefore, we can replace J with H in our update for θ, to obtain the update rule\nθt+1 = P-UPDATE(H, θt).\nTherefore by conditions of this theorem the sequence θt converges to a local optima θ∗ with respect to H . Let ω∗ = W (θ∗). As θ∗ is a local optimum with respect to H , by definition there exists > 0, s.t.\n||θ∗ − θ||2 < =⇒ H(θ) ≤ H(θ ∗).\nTherefore for any ω,∣∣∣∣∣∣∣∣(θ∗ω∗ ) − ( θ ω )∣∣∣∣∣∣∣∣ 2 < =⇒ ||θ∗ − θ||2 <\n=⇒ H(θ) ≤ H(θ∗) =⇒ J(θ, ω) ≤ J(θ∗, ω∗).\nTherefore by definition (θ∗, ω∗) is a local optima with respect to J .\nIn summary, if we can locally optimize θ, and ω = W (θ) at each step, then this will find a local optima for a function H(θ) and consequently for J(θ, ω). We can make a similar argument that if the sequence θt converges to a global optimum with respect to H , then Q-PAMDP(1) converges to a global optimum (θ∗, ω∗). The conditions for the previous theorem can be met by assuming that P-UPDATE is a local optimization method such as gradient ascent.\nOne problem is that at each step we must re-learn W (θ) for the updated value of θ. Now we show that if updates to θ are bounded, and W (θ) is continuous function, then the required updates to ω will also be bounded. Intuitively, we are supposing that a small update to θ results in a small change\nin the weight specifying which discrete action to choose. The assumption thatW (θ) is continuous is a strong one, and we do not claim that this will be satisfied by all PAMDPs. It is not necessary for the operation of Q-PAMDP(1), but it is convenient to assume that we do not have to completely re-learn ω after each update to θ. Theorem 4.2 (Bounded Updates to ω). If W is continuous with respect to θ, and\nθt+1 = θt + αtP-UPDATE(θt, ωt),\nwith 0 < ||P-UPDATE(θt, ωt)||2 < δ for some δ > 0, then ∀ > 0, ∃α0 > 0 such that\nαt < α0 =⇒ ||ωt+1 − ωt||2 < . Proof. Let > 0. Let α0 = δ/ ||P-UPDATE(θt, ωt)||2. So if αt < α0,\nδ > αt ||P-UPDATE(θt, ωt)||2 = ||αtP-UPDATE(θt, ωt)||2 = ||θt+1 − θt||2 .\nAs W is continuous, this means that\n||W (θt+1)−W (θt)||2 = ||ωt+1 − ωt||2 < .\nIn other words, if our update to θ is bounded and W is continuous, we can always adjust the learning rate α so that the difference ωt and ωt+1 is bounded. If this is the case then we don’t have to completely re-learn ω at each step, but only adjust slightly. This simplifies the operation of the Q-PAMDP(1) algorithm significantly.\nWith Q-PAMDP(1) we want P-UPDATE to optimize H(θ). One logical choice would be to use a gradient update. The next theorem shows that gradient of H is equal to the gradient of J if ω = W (θ). This is useful as we can apply existing gradient-based methods to compute the gradient of J with respect to θ. The proof follows from the fact that we are at a global optima of J with respect to ω, and so the gradient ∇ωJ is zero. This theorem requires that W is differentiable (and therefore also continuous). Theorem 4.3 (Gradient ofH(θ)). If J(θ, ω) is differentiable with respect to θ and ω and W (θ) is differentiable with respect to θ, then the gradient of H is given by ∇θH(θ) = ∇θJ(θ, ω∗), where ω∗ = W (θ).\nProof. If θ ∈ Rn and ω ∈ Rm, then we can compute the gradient of H by the chain rule:\n∂H(θ) ∂θi = ∂J(θ,W (θ)) ∂θi\n= n∑ j=1 ∂J(θ, ω∗) ∂θj ∂θj ∂θi + m∑ k=1 ∂J(θ, ω∗) ∂ω∗k ∂ω∗k ∂θi\n= ∂J(θ, ω∗)\n∂θi + m∑ k=1 ∂J(θ, ω∗) ∂ω∗k ∂ω∗k ∂θi ,\nwhere ω∗ = W (θ). Note that as by definitions of W ,\nω∗ = W (θ) = arg max ω J(θ, ω),\nwe have that the gradient of J with respect to ω is zero ∂J(θ, ω∗)/∂ω∗k = 0, as ω is a global maximum with respect to J for fixed θ. Therefore, we have that ∇θH(θ) = ∇θJ(θ, ω∗).\nWith this result, if we perform a gradient update on θ with respect to J , and after updating ω = W (θ), the gradient update will be the same as it would be for H . Therefore QPAMDP(1) with P-UPDATE being a single gradient update is equivalent to performance gradient ascent on H .\nTo conclude, if W (θ) is continuous and P-UPDATE converges to a global or local optimum, then Q-PAMDP(1) will converge to a global or local optimum, respectively, and the Q-LEARN step will be bounded if the update rate of the PUPDATE step is bounded. As such, if P-UPDATE is a policy gradient update step then Q-PAMDP by Theorem 4.3 will converge to a local optimum and by Theorem 4.4 the Q-LEARN step will require a bounded number of updates. This policy gradient step can use the gradient of J with respect to θ. Theorem 4.4 (Local Convergence of Q-PAMDP(∞)). If at each step of Q-PAMDP(∞) for some bounded set Θ:\nθt+1 = arg max θ∈Θ J(θ, ωt)\nand ωt+1 = W (θt+1),\nthen Q-PAMDP(∞) converges to a local optimum.\nProof. By definition of W ,\nωt+1 = arg max ω J(θt+1, ω).\nTherefore this algorithm takes the form of direct alternating optimization. As such, it converges to a local optimum (Bezdek and Hathaway 2002).\nQ-PAMDP(∞) has weaker convergence properties than Q-PAMDP(1), as it requires a globally converging P-STEP method. However, it has the potential to bypass nearby local optima (Bezdek and Hathaway 2002)."
    }, {
      "heading" : "5 Experiments",
      "text" : "First, we consider a simplified robo-cup problem (Kitano et al. 1997) where a single striker attempts to score a goal. Each episode starts with the player at a random position along the left bound of the field. The player starts with the ball in possession, and the keeper is positioned between the ball and the goal. The game takes place in a 2D environment where the player and the keeper have a position, velocity and orientation and the ball has a position and velocity resulting in 14 continuous state variables. An episode ends when the keeper possesses the ball, the player scores a goal, or the ball leaves the field. The reward for an action is 0 for non-terminal state, 50 for a terminal goal state, and −d for a terminal non-goal state, where d is the distance of the ball to the goal. The player has two parameterized actions: kick-to(x, y), which kicks to ball towards position (x, y); and shoot-goal(h), which shoots the ball towards a position h along the goal line. If the player is not in possession of\nthe ball, it moves towards it. The keeper has a fixed policy: it moves towards the ball, and if the player shoots at the goal, the keeper moves to intercept the ball. To score a goal, the player must shoot around the keeper. This means that at some positions we must shoot left past the keeper, and at others shoot to the right past the keeper. However at no point do we shoot at the keeper, so an optimal policy is discontinuous. This policy would be difficulty to represent in a purely continuous action space, but is simpler in a parameterized action domain. We split the action into two parameterized actions: shoot-goal-left, shoot-goal-right. This allows us to use a simple action selection policy instead of complex continuous action policy.\nWe represent the action-value function for the discrete action a using linear function approximation: Qω(s, a) = ωTa φa(s), where ωa is a vector of weights, and φa(s) gives the features for state s. For this domain, we use Fourier basis features (Konidaris, Osentoski, and Thomas 2011). As we have 14 state variables, we must be selective in which basis functions to use. We only use basis functions with two nonzero elements and exclude all velocity state variables. We use the soft-max discrete action policy (Sutton and Barto 1998) πdω(a|s) = exp(Qω(s, a)/τ)/ ∑ b exp(Qω(s, b)/τ), where τ is the action selection temperature. We represent the action-parameter policy πaθ as a normal distribution around a weighted sum of features πaθ (x|s) = N (θTa ψa(s),Σ), where θa is a matrix of weights, and ψa(s) gives the features for state s, and Σ is a fixed covariance matrix. We use specialized features for each action. For the shoot-goal actions we use using a simple linear basis (1, g), where g is the projection of the keeper onto the goal line. For kick-to we use linear features (1, bx, by, bx2, by2, (bx−kx)/ ||b− k||2 , (by− ky)/ ||b− k||2), where (bx, by) is the position of the ball and (kx, ky) is the position of the keeper. These linear features allow for a policy that directs the ball around the keeper and towards the goal.\nThe policy is differentiable, allowing us to use a policy gradient method. For the direct policy search approach, we use the episodic natural actor critic (eNAC) algorithm (Peters and Schaal 2008), optimizing J(ω, θ) with respect to (ω, θ). This approach shows the performance of currently available methods. For the Q-PAMDP(1) approach we use the gradient-descent Sarsa(λ) algorithm for Q-learning, and the eNAC algorithm for policy search (Peters and Schaal 2008). At each step we perform one eNAC update based on 50 episodes and then refit Qω using 50 gradient descent Sarsa(λ) episodes. We also consider the same algorithms using Q-PAMDP(∞).\nReturn is directly correlated with goal scoring probability, so their graphs are close to indentical. We plot goal scoring probability in figure 2. We can see that direct eNAC is outperformed by Q-PAMDP(1) and Q-PAMDP(∞). This is likely due to the difficulty of optimizing the action selection parameters directly, rather than with Q-learning.\nFor both methods, the goal probability is greatly increased: while the initial policy rarely scores a goal, both QPAMDP(1) and Q-PAMDP(∞) increase the probability of a goal to roughly 35%. Direct eNAC increases the probability of scoring a goal to 15%. This suggests that a direct opti-\nmization method is insufficient to handle a parameterized action domain. Finally, we depict the performance of SARSA where the parameters are fixed at θ0. This achieves roughly 20% goal scoring probability. Both Q-PAMDP(1) and QPAMDP(∞) strongly out-perform fixed parameter SARSA, but eNAC does not. Figure 3 depicts a single episode using a converged Q-PAMDP(1) policy — the player draws the keeper out and strikes when the goal is open.\nNext we consider a platformer domain. In this domain, the agent starts on a platform and must reach a goal platform while avoiding enemies. If the player reaches the goal platform, touches an enemy, or falls into a gap between platforms, the episode is ended. The reward for a step is the change in x value for that step, divided by the total length of all the platforms and gaps. The player has two primitive actions: run or jump, which continue for a fixed period or until the player lands again respectively. There are two different kinds of jumps: a high jump to get over enemies, and a long jump to get over gaps between platforms. The domain therefore has three parameterized actions: run(dx), hop(dx), and leap(dx).\nThe player only takes actions while on the ground, and enemies only move when the player is on their platform, the state space consists of only four variables (x, ẋ, ex, ėx). These provide the player position, player speed, enemy position, and enemy speed respectively. For learning Qω , as in the previous domain, we use linear function approximation with the Fourier basis. We apply a softmax discrete action policy based on Qω , and a Gaussian parameter policy based on parameter features ψa(s)."
    }, {
      "heading" : "6 Related Work",
      "text" : "Hauskrecht et al. (2004) introduced an algorithm for solving factored MDPs with a hybrid discrete-continuous action\nspace. However, their formalism has an action space with a mixed set of discrete and continuous components, whereas our domain has distinct actions with a different number of continuous components for each action. Furthermore, they assume the domain has a compact factored representation, and only consider planning.\nRachelson (2009) encountered parameterized actions in the form of an action to wait for a given period of time in his research on time dependent, continuous time MDPs (TMDPs). He developed XMDPs, which are TMDPs with a parameterized action space (Rachelson 2009). He developed a Bellman operator for this domain, and in a later paper mentions that the TiMDPpoly algorithm can work with parameterized actions, although this specifically refers to the parameterized wait action (Rachelson, Fabiani, and Garcia 2009). This research also takes a planning perspective, and only considers a time dependent domain. Additionally, the size of the parameter space for the parameterized actions is the same for all actions.\nHoey et al. (2013) considered mixed discrete-continuous actions in their work on Bayesian affect control theory. They model affect control theory, a formalization of interpersonal interaction, as a POMDP with hybrid actions. To approach this problem they use a form of POMCP, a Monte Carlo sampling algorithm, using domain specific adjustments to compute the continuous action components (Silver and Veness 2010). They note that the discrete and continuous components of the action space reflect different control aspects: the discrete control provides the “what”, while the continuous control describes the “how” (Hoey, Schroder, and Alhothali 2013).\nIn their research on symbolic dynamic programming (SDP) algorithms, Zamani et al. (2012) considered domains with a set of discrete parameterized actions. Each of these actions has a different parameter space. Symbolic dynamic programming is a form of planning for relational or firstorder MDPs, where the MDP has a set of logical relationships defining its dynamics and reward function. Their algorithms represent the value function as an extended algebraic\ndecision diagram (XADD). As such, this work is limited to MDPs with predefined logical relations."
    }, {
      "heading" : "7 Conclusion",
      "text" : "The PAMDP formalism models reinforcement learning domains with parameterized actions. PAMDPs allow for new kinds of domains, and for new approaches for old domains. Parameterized actions give us the adaptibility of continuous domains and the use of distinct actions. They also allow for simple representation of discontinuous policies without complex parameterizations. Three approaches for modelfree learning in PAMDPs have been presented: direct optimization, and two variants of the Q-PAMDP algorithm. We have shown that Q-PAMDP(1), with an appropriate PUPDATE method, converges to a local or global optima. QPAMDP(∞) with a global optimization step converges to a local optima.\nWe have examined the performance of the three approaches in the robot soccer goal domain. The robot soccer goal domain models the situation where a striker must out-maneuver a keeper to score a goal. Of these, QPAMDP(1) and Q-PAMDP(∞) outperformed eNAC. We can conclude that direct optimization is an ineffective approach for PAMDPs. Q-PAMDP(1) and Q-PAMDP(∞) performed similarly well in terms of goal scoring, learning policies that score goals roughly 35% of the time."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2017,
    "abstractText" : "We introduce a model-free algorithm for learning in Markov decision processes with parameterized actions—discrete actions with continuous parameters. At each step the agent must select both which action to use and which parameters to use with this action. This models domains where there are distinct actions which can be adjusted to a particular state. We introduce the Q-PAMDP algorithm for learning in these domains. We show that Q-PAMDP converges to a local optima, and compare different approaches in a robot soccer goal-scoring domain and a platformer domain.",
    "creator" : "LaTeX with hyperref package"
  }
}