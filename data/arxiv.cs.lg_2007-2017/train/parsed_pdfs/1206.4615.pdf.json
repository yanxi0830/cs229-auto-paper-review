{
  "name" : "1206.4615.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Lévy Measure Decompositions for the Beta and Gamma Processes",
    "authors" : [ "Yingjian Wang", "Lawrence Carin" ],
    "emails" : [ "yw65@duke.edu", "lcarin@duke.edu" ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "A prominent distinction of nonparametric methods relative to parametric approaches is the utilization of stochastic processes rather than probability distributions. For example, a Gaussian process (Rasmussen & Williams, 2006) may be employed to nonparametrically represent general smooth functions on a continuous space of covariates (e.g., time). Recently the idea of nonparametric methods has extended to feature learning and data clustering, with interest respectively in the beta-Bernoulli process (Thibaux & Jordan, 2007) and the Dirichlet process (Ferguson, 1973). In such processes the nonparametric aspect concerns the number of features/clusters, which are allowed to be unbounded (“infinite”), permitting the model to adapt the number of these entities as the given and fu-\nAppearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s).\nture data indicate. The increasing importance of these models in machine learning warrants a detailed theoretical analysis of their properties, as well as simple constructions for their implementation. In this paper we focus on Lévy processes (Sato, 1999), which are of increasing interest in machine learning.\nA family of Lévy processes, the pure-jump nondecreasing Lévy processes, also fit into the category of the completely random measure proposed by Kingman (Kingman, 1967). The beta process (Hjort, 1990) is an example of such a process, which is applied in nonparametric feature learning. The gamma process falls in this family as well, with its normalization the well-known Dirichlet process. Hierarchical forms of such models have become increasingly popular in machine learning (Teh et al., 2006; Teh, 2006; Thibaux & Jordan, 2007), as have nested models (Blei et al., 2010), and models that introduce covariate dependence (MacEachern, 1999; Williamson et al., 2010; Lin et al., 2010).\nAs a consequence of the important role these models are playing in machine learning, there is a need for the study of the properties of pure-jump nondecreasing Lévy processes. As examples of such work, (Thibaux & Jordan, 2007) and (Paisley et al., 2010) present explicit constructions for generating the beta process, (Teh et al., 2007) derives a construction for the Indian buffet process parallel to the stick-breaking construction of the Dirichlet process (Sethuraman, 1994), and (Thibaux, 2008) obtains a construction for the gamma process under the gamma-Poisson context. Apart from these specialized construction methods, in (Kingman, 1967) a general construction method for completely random measures is proposed, by first decomposing it into a sum of a countable number of σ-finite measures, and then superposing the Poisson processes according to these sub-measures. By regarding the completely random measure as a Lévy process, this method corresponds to decomposing the Lévy measure, which provides clarity of theoretical properties and simplicity in practical implementation. However this Lévy measure\ndecomposition method has not yet come into wide use in machine learning and statistics, probably due to the nonexistence of a universal construction of the measure decomposition.\nIn this paper we develop explicit and simple decompositions by following the conjugacy principle for two widely used Lévy processes, the beta and gamma processes. The conjugacy means that the decompositions are manifested by leveraging the forms of conjugate likelihoods to the Lévy measures. The decompositions bring new perspectives on the beta and gamma processes, with associated properties analyzed here in detail. The decompositions are constituted in terms of an infinite set of sub-processes of form convenient for computation. Since the number of sub-processes is infinite, a truncation analysis is also presented, of interest for practical use. We show some posterior properties of such decompositions, with the beta process as an example. We also extend the decomposition to the symmetric gamma process (positive and negative jumps), suggesting that the Lévy measure decomposition is applicable for other pure-jump Lévy processes represented by their Lévy measures. Summarizing the main contributions of the paper:\n• We constitute Lévy measure decompositions for the beta, stable-beta, gamma, generalized gamma and symmetric gamma processes via the principle of conjugacy, providing new perspectives on these processes.\n• The decomposition of the beta process unifies the constructions in (Thibaux & Jordan, 2007), (Teh & Görür, 2009), and (with a different decomposing method) (Paisley et al., 2010), and a new generative construction for the gamma process and its variations is derived.\n• Truncation analyses and posterior properties for such decompositions are presented for practical use."
    }, {
      "heading" : "2. Background",
      "text" : "Lévy processes (Sato, 1999) and completely random measures (Kingman, 1967) are two closely related concepts. Specifically, some Lévy processes can be regarded as completely random measures. In this section brief reviews and connections are presented for these two important concepts."
    }, {
      "heading" : "2.1. Lévy process",
      "text" : "A Lévy process X(ω) is a stochastic process with independent increments on a measure space (Ω,F). Ω\nis usually taken to be one-dimensional, such as the real line, to represent a stochastic process with variation over time. By the Lévy-Itô decomposition (Sato, 1999), a Lévy process can be decomposed into a continuous Brownian motion with drift, and a discrete part of a pure-jump process. When a Lévy process X(ω) only has the discrete part and its jumps are positive, then for ∀A ∈ F the characteristic function of the random variable X(A) is given by:\nE{ejuX(A)} = exp{ ∫\nR+×A (ejup − 1)ν(dp, dω)} (1)\nwith ν satisfying the integrability condition (Sato, 1999). The expression in (1) defines a category of pure-jump nondecreasing Lévy processes, including most of the Lévy processes currently used in nonparametric Bayesian methods, such as the beta, gamma, Bernoulli, and negative binomial processes. With (1), such a Lévy process can be regarded as a Poisson point process on the product space R+ × Ω with the mean measure ν, called the Lévy measure. On the other hand, if the increments of X(ω) on any measurable set A ∈ F are regarded as a random measure assigned on the set, then X(ω) is also a completely random measure. Due to this equivalence, in the following discussion we will not discriminate the pure-jump nondecreasing Lévy process X with its corresponding completely random measure Φ."
    }, {
      "heading" : "2.2. Completely random measure",
      "text" : "A random measure Φ on a measure space (Ω,F) is termed “completely random” if for any disjoint sets A1,A2 ∈ F the random variables Φ(A1) and Φ(A2) are independent. A completely random measure Φ can be split into three independent components:\nΦ = Φf +Φd +Φo (2) where Φf = ∑\nω∈I φ(ω)δω is the fixed component, with the atoms in I fixed and the jump φ(ω) random; I is a countable set in F . The deterministic component Φd is a deterministic measure on (Ω,F). Φf and Φd are relatively less interesting compared to the third component Φo, which is called the ordinary component of Φ. According to (Kingman, 1967), Φo is discrete with both random atoms and jumps.\nIn (Kingman, 1967), it is noted that Φo can be further split into a countable number of independent parts:\nΦo = ∑ k Φk, Φk = ∑\n(φ(ω),ω)∈Πk\nφ(ω)δω (3)\nDenote ν as the Lévy measure of (the Lévy process corresponding to) Φo, νk as the Lévy measure of Φk,\nΠ a Poisson process with ν its mean measure, and Πk a Poisson process with νk its mean measure; (3) further yields:\nν = ∑ k νk, Π = ⋃ k Πk (4)\nwhich provides a constructive method for Φo: first construct the Poisson process Πk underlying Φk, and then with the superposition theorem (Kingman, 1993) the union of Πk will be a realization of Φo. In the following sections we show how this general construction method of (4) can be applied on pure-jump nondecreasing Lévy processes of increasing interest in machine learning, with an emphasis on the beta and gamma processes, and their generalizations."
    }, {
      "heading" : "3. Beta process",
      "text" : "A beta process (Hjort, 1990) is a Lévy process with beta-distributed increments; B ∼ BP(c(ω), µ) is a beta process if\nB(dω) ∼ Beta(c(ω)µ(dω), c(ω)(1− µ(dω))) (5)\nwhere µ is the base measure on measure space (Ω,F) and a positive function c(ω) the concentration function. Expression (5) indicates that the increments of the beta process are independent, which makes it a special case of the Lévy process family. The Lévy measure of the beta process is\nν(dπ, dω) = c(ω)π−1(1− π)c(ω)−1dπµ(dω) (6)\nwhere Beta(0, c(ω)) = c(ω)π−1(1 − π)c(ω)−1 is an improper beta distribution since its integral over (0, 1) is infinite. As a result, its underlying Poisson process, i.e., the Poisson process with ν as its mean measure on the product space Ω × (0, 1), denoted Π, has an infinite number of points drawn from ν, yielding\nB = ∞∑ i=1 πiδωi (7)\nwhere πi is the jump (increment) which happens at the atom ωi. Real variable γ = µ(Ω) is termed the mass parameter of B, and we assume γ <∞."
    }, {
      "heading" : "3.1. Beta process Lévy measure decomposition",
      "text" : "The infinite integral of the improper beta distribution inspires a decomposition of the improper distribution with an infinite number of proper distributions. The singularity in the improper beta distribution is manifested from π−1. Since π ∈ (0, 1), the geometric series expansion yields\nπ−1 = ∞∑ k=0 (1− π)k, π ∈ (0, 1) (8)\nand substituting (8) in (6), with manipulation detailed in the Supplementary Material, we have the Lévy measure decomposition theorem of the beta process:\nTheorem 1 For a beta process B ∼ BP(c(ω), µ) with base measure µ and concentration c(ω), denote Π as its underlying Poisson process and ν the Lévy measure, then B and Π can be expressed as\nΠ = ∞⋃ k=0 Πk , B = ∞∑ k=0 Bk (9)\nwhere Bk is a Lévy process with Πk its underlying Poisson process. The Lévy measure νk of Bk is a decomposition of ν:\nν = ∞∑ k=0 νk\nνk(dπ, dω) = Beta(1, c(ω) + k)dπµk(dω)\nµk(dω) = c(ω)\nc(ω) + k µ(dω)\n(10)\nwhere Beta(1, c(ω)+k) is the PDF of beta distribution with parameters 1 and c(ω) + k.\nTheorem 1 is the beta process instantiation of the completely random measure decomposing in (4), which indicates that the underlying Poisson process Π of the beta process B is the superposition of an infinite number of independent Poisson processes {Πk}∞k=0, with νk the mean measure of Πk and µk the mean measure of the restriction of Πk on Ω. As a result, the beta process B can be expressed as a sum of an infinite number of independent Lévy processes {Bk}∞k=0 with {Πk}∞k=0 the underlying Poisson process. The independence of {Πk}∞k=0 and {Bk}∞k=0 w.r.t. index k is justified by the fact that both µ and c(ω) are fixed parameters."
    }, {
      "heading" : "3.2. The Lévy process Bk",
      "text" : "It is interesting to study the properties of Bk, such as the expectation and variance. Denoting Bk(dω) =\n1 c(ω)+k+1µk(dω) as the base measure of Bk, for ∀A ∈ F :\nE(Bk(A)) = ∫ A Bk(dω) = Bk(A)\nVar(Bk(A)) = ∫ A 2 c(ω) + k + 2 Bk(dω) (11)\nIt is noteworthy that the Lévy process Bk is no longer a beta process, since (5) is not satisfied. By Theorem 1, the jumps of Bk follow a proper beta distribution parameterized by the concentration function c(ω) and the index k, and µk determines the locations where the jumps happen. Since {Bk}∞k=0 are independent w.r.t.\nthe index k, with Theorem 1: ∞∑ k=0 E(Bk(A)) = E(B(A))\n∞∑ k=0 Var(Bk(A)) = Var(B(A)) (12)\nThe detailed procedure to derive (11) and (12) is given in the Supplementary Material."
    }, {
      "heading" : "3.3. Simulating the beta process",
      "text" : ""
    }, {
      "heading" : "3.3.1. Poisson superposition simulation",
      "text" : "Theorem 1 reveals that the underlying Poisson process of a beta process is a superposition of an infinite number of Poisson processes, each of which has a finite set of atoms. This perspective also provides a simulation procedure for the beta process: first, the Poisson process Πk is sampled for all k = 0, 1, 2, · · · , (here we term the index k as the “round” of the simulation); then take the union of the samples of each Πk as a realization of the Poisson process Π. With the marking theorem (Kingman, 1993) implicitly applied, the simulation procedure of the beta process is as follows:\nSimulation procedure: For round k:\n1: Sample the number of points for Πk: nk ∼ Poisson( ∫ Ω µk(dω)); 2: Sample nk points from µk: ωki i.i.d.∼ µk∫\nΩ µk(dω) , for\ni = 1, 2, · · · , nk; 3: Sample Bk(ωki)\ni.i.d.∼ Beta(1, c(ωki) + k), for i = 1, 2, · · · , nk;\nThen the union ⋃∞ k=0{(ωki, Bk(ωki)} nk i=1 is a realization of Π (and equivalently of B).\nWe refer to the above simulation procedure as the Poisson superposition simulation, for the central role of the Poisson superposition. The especially convenient case is when the beta process is homogeneous, i.e., c(ω) = c is a constant. In this case {ωki}nki=1 for all rounds k are drawn from the same distribution µ/γ; and nk is drawn from Poisson( cγc+k ). For round k, both the number of points and the jumps statistically diminish as k increases, suggesting that the infinite sum in (9) may be truncated as B = ∑K k=0Bk for large K, with minimal impact. Such truncation effects are investigated in detail in Section 3.4."
    }, {
      "heading" : "3.3.2. Related work",
      "text" : "In (Thibaux & Jordan, 2007) the authors derived the above simulation procedure for the homogeneous case\nwithin the beta-Bernoulli process context, which is shown here a necessary result of the Lévy measure decomposition. The same decomposing manipulation of Theorem 1 can be also applied to the stable beta process (Teh & Görür, 2009) which yields:\nνk =Beta(1− σ, c(ω) + σ + k)dπ\n· Γ(c(ω) + σ + k)Γ(c(ω) + 1) Γ(c(ω) + k + 1)Γ(c(ω) + σ) µ(dω) (13)\nIt is noteworthy that the decomposition procedure described in Theorem 1 is not the only Lévy measure decomposing method for the beta process. The work of (Paisley & Jordan, 2012) and (Broderick et al., 2011) show that the stick-breaking construction of the beta process in (Paisley et al., 2010) is indeed a result of another way of decomposing the Lévy measure of the beta process. We next analyze the truncation property of the construction described in Section 3.3.1 and make comparison with the construction of beta process in (Paisley et al., 2010)."
    }, {
      "heading" : "3.4. Truncation analysis",
      "text" : "Since the Poisson superposition simulation operates in rounds, it is natural to analyze the distance between the true beta process B and its truncation ∑K k=0Bk, with truncation at round K. A metric for such distance is the L1 norm:\n||B− K∑ k=0 Bk||1 = E|B− K∑ k=0 Bk| = ∫ Ω µK+1(dω) γ (14)\nThe expectation in (14) is w.r.t. the normalized measure ν/γ, which yields ‖B‖1 = 1. When B is homogeneous, (14) reduces to cc+K+1 , which indicates that the L1 distance decreases at a rate of O( 1K ). For the stick-breaking construction of beta process described in (Paisley et al., 2010), the L1 distance is: ( cc+1 ) K+1.\nAnother metric is the L1 distance between the marginal likelihood of a set of data b = b1:M , with m∞(b) denotes the marginal likelihood (here the likelihood is a Bernoulli process) with prior B, andmK(b) for ∑K\nk=0Bk. This metric was applied on the truncated Indian buffet process (Doshi et al., 2009) and truncated stick-breaking construction of the beta process (Paisley & Jordan, 2012), which indicates\n1 4\n∫ |m∞(b)−mK(b)|db ≤\nPr(∃k > K, 1 ≤ i ≤ nk, 1 ≤ m ≤M, s.t. bmki = 1) (15)\nwhere b1:M i.i.d.∼ BeP(B) are drawn from a Bernoulli process with base measure B; bmki = bm(ωki) is the\nmth realization of the Bernoulli process at atom ωki. For the truncation ∑K k=0Bk it can be shown that the RHS of (15) is bounded by:\nRHS of (15) ≤ 1− exp(−M ∫ Ω µK+1(dω)) (16)\nFor the homogeneous case, the bound of (16) is 1 − exp(−Mγ cc+K+1 ). For the stick-breaking construction of beta process, the bound is given by: 1− exp(−Mγ( cc+1 ) K+1) (Paisley & Jordan, 2012).\nIn order to analyze the bound w.r.t. the truncation level by number of atoms, denote IK = ∑K k=0 nk\nas the total number of atoms in ∑K\nk=0Bk. Since\nK ∼ O(e E(IK ) cγ ), it is proved that (14) and the bound in (16) decreases at a faster rate w.r.t. I than the stick-breaking construction of beta process. This indicates that the simulation procedure described in Section 3.3.1 follows a steeper statistically-decreasing order. The proof is presented in the Supplementary Material."
    }, {
      "heading" : "3.5. Posterior estimation",
      "text" : "The goal of the inference is to estimate the beta process B from a set of observed data b with prior BP(c, µ). The data b = b1:M is the same as in Section 3.4, which can be expressed as:\nbm = ∞∑ i=1 bi,mδωi , m = 1, 2, · · · ,M (17)\nwhere each bi,m ∈ {0, 1}."
    }, {
      "heading" : "3.5.1. Posterior of Bk",
      "text" : "Since B|b ∼ BP(c+M, cµc+M + ∑M m=1 bm c+M ) (Thibaux & Jordan, 2007), the base measure of B|b is a measure with positive masses assigned on single atoms. Theorem 1 is still applicable to this beta process with mixed type of base measure, which yields\nB′ = ∞∑ k=0 B′k ν′k = Beta(1, c+M + k)µ ′ k\nµ′k = cµ c+M + k + ∑M m=1 bm c+M + k\n(18)\nwhere the B′, B′k, ν ′ k, and µ ′ k are the posterior counterparts of B, Bk, νk, and µk."
    }, {
      "heading" : "3.5.2. Posterior estimation of πi:",
      "text" : "Since each µk has a mass ∑M m=1 bi,m c+M+k at the atom ωi,\neach Bk will contribute Poisson( ∑M m=1 bi,m c+M+k ) draws with\nthe jumps following the distribution Beta(1, c +M + k) at the atom ωi, whose sum is the πi. Thus the posterior estimation of πi is given by\nπi|b = ∞∑ k=0 Hk∑ h=1 bkh\nHk ∼ Poisson( ∑M\nm=1 bi,m c+M + k )\nbkh ∼ Beta(1, c+M + k)\n(19)\nfrom which it can be verified that E(πi|b) = ∑M m=1 bi,m c+M , the same as the posterior of πi without decomposition: Beta( ∑M m=1 bi,m, c+M − ∑M m=1 bi,m).\nFor the πi with no observations, i.e., ∑M\nm=1 bi,m = 0, only a particular Bk will contribute to πi. In this case, first the round k to which πi belongs is drawn, then πi is drawn from the beta distribution of that round:\nπi ∼ Beta(1, c+M + k)\nk ∼ MP(α), α ∝ ∞∑ k=0 1 c+M + k δk (20)\nwhere MP(α) is a multinomial process with probability vector α, and α is proportional to the average number of points in each round. Since in practical processing α is always to be truncated with a truncation level K, by the analysis in Section 3.4, (20) provides a way to estimate the πi within the first K rounds. And πi in each round are of statistically different importance, contrasted to the evenly assigned mass in the Indian buffet process."
    }, {
      "heading" : "3.6. Relating the IBP and beta process",
      "text" : "The study of the beta process through its Lévy measure, as discussed in this paper, also uncovers a connection between the Indian buffet process (IBP) (Griffiths & Ghahramani, 2005) and the beta process, by their Lévy measures. The IBP with prior πi ∼ Beta(c γN , c) can be regarded as a Lévy process with the Lévy measure given as:\nνIBP = N\nγ Beta(c\nγ N , c)dπµ(dω) (21)\nhere N is the same as the K in (Griffiths & Ghahramani, 2005). It can be proved that:\nνIBP N→∞= ν (22)\nwhich indicates that the beta process is the limit of the IBP with N → ∞. The detailed proof of (22) is presented in the Supplementary Material. Thus the IBP is like a “mosaic” approximation of beta process, which becomes finer with N increases."
    }, {
      "heading" : "4. Gamma process",
      "text" : "A gamma process (Applebaum, 2009) is a Lévy process with independent gamma increments. The gamma process is traditionally parameterized with a shape measure and a scale function: G ∼ ΓP(α, θ(ω)) where α is the shape measure on a measure space (Ω,F), and the scale θ(ω) a positive function. A gamma process can be intuitively defined by its increments on infinitesimal sets:\nG(dω) ∼ Gamma(α(dω), θ(ω)) (23)\nWhen θ(ω) = θ is a scalar, the gamma process is called homogeneous. The gamma process can also be expressed in the form with a base measure G0 and a concentration c(ω), with c = 1/θ and G0 = θα (Jordan., 2009), to conform with other stochastic processes widely used in machine learning, such as the Dirichlet process. However, the discussion in this paper will stick to the traditional form given by (23).\nAs a pure-jump Lévy process, the gamma process can be regarded as a Poisson process on the product space Ω× R+ with mean measure ν:\nν(dp, dω) = p−1e− p θ(ω) dpα(dω) (24)\nwhere Gamma(0, θ(ω)) = p−1e− p\nθ(ω) is an improper gamma distribution with an infinite integral on R+, which yields the expression of G:\nG = ∞∑ i=1 piδωi (25)"
    }, {
      "heading" : "4.1. Lévy measure decomposition",
      "text" : "Like the beta process, the Lévy measure of the gamma process is characterized by an improper distribution. However, unlike the beta process, the decomposition of the Lévy measure of the gamma process comes from the exponential part. With the details shown in the Supplementary Material, the gamma process G can be decomposed into two parts:\nG = Γ1 + ΓP(α, θ(ω)/2) (26)\nThe second term in (26) is a gamma process with the same shape measure, and half the scale of the gamma process G; the first term Γ1 is a Lévy process with the Lévy measure ∑∞ h=1Gamma(h, θ(ω) 2 )dp α(dω) 2hh\n. Here Gamma(h, θ(ω)2 ) is the PDF of the gamma distribution, with shape parameter h and scale parameter θ(ω) 2 .\nFurther decomposing the exponential part of the gamma process ΓP(α, θ(ω)/2) in (26) yields G =\nΓ1+Γ2+ΓP(α, θ(ω)/3), bearing a gamma process with the same shape and with the scale parameter further decreased. Repeating this manipulation, we obtain the Theorem 2:\nTheorem 2 A gamma process G ∼ ΓP(α, θ(ω)) with shape measure α and scale θ(ω) can be decomposed as:\nG = ∞∑ k=1 Γk, Γk = ∞∑ h=1 Γkh, νk = ∞∑ h=1 νkh νkh = Gamma(h, θ(ω) k + 1 )dp α(dω) (k + 1)hh\n(27)\nwith Γk, Γkh Lévy processes with νk, νkh their Lévy measures.\nTheorem 2 is the gamma process instantiation of (4), which indicates that G can be expressed as the sum of an infinite number of Lévy processes Γk, k = 1, 2, · · · , where Γk is also the sum of an infinite number of Lévy processes Γkh, h = 1, 2, · · · ."
    }, {
      "heading" : "4.2. Lévy processes Γk and Γkh",
      "text" : "In order to obtain further insights into the gamma process G in Theorem 2, the expectations and variances of Γk and Γkh on any measurable set A ∈ F are given:\nE(Γkh(A)) = ∫ A θ(ω)α(dω) (k + 1)h+1\nE(Γk(A)) = ∫ A θ(ω)α(dω) k(k + 1)\n(28)\nFor the variances of Γk and Γkh:\nVar(Γkh(A)) = (h+ 1)\n(k + 1)h+2 ∫ A θ2(ω)α(dω)\nVar(Γk(A)) = [ 1 k2 − 1 (k + 1)2 ] ∫ A θ2(ω)α(dω) (29)\nSince the Lévy processes Γk are independent w.r.t. k, with analogy to (12) it can be verified that the expectation and variance of Γk sum to the expectation of variance of G. The derivations in this section are presented in the Supplementary Material."
    }, {
      "heading" : "4.3. Simulation of gamma process",
      "text" : "Parallel to the simulation of beta process in Section 3.3.1, a simulation procedure of the gamma process is presented:\nSimulation procedure: Sample the Lévy process Γkh:\n1: Sample the number of points for Γkh: nkh ∼ Poisson(γ/(k + 1)hh); 2: Sample nkh points from α: ωkhi i.i.d.∼ αγ , for i =\n1, 2, · · · , nkh; 3: Sample Γkh(ωkhi)\ni.i.d.∼ Gamma(h, θ(ωkhi)k+1 ), for i = 1, 2, · · · , nkh;\nwhere γ = ∫ Ω α(dω) is the mass of the shape mea-\nsure. Then the union ⋃∞ k=1 ⋃∞ h=1(ωkhi,Γkh(ωkhi)) nkh i=1 is a realization of the gamma process G. An advantage of the above simulation procedure compared to the simulation procedure of the beta process in Section 3.3.1 is that independent of whether the gamma process is homogeneous or inhomogeneous, ωkhi is always drawn from a fixed distribution α/γ. Like with the beta process construction in Section 3.3.1, for the gamma process simulation procedure, as k increases the expected number of new points and the expected jumps decrease, again suggesting accurate truncation."
    }, {
      "heading" : "4.4. Truncation analysis",
      "text" : "Since in the simulation procedure in Section 4.3 the index k and h both go to infinity, it is practical to analyze the distance between the true gamma process and the truncated one. To measure such a distance, we apply the L1 norm described in Section 3.4:\n||G− K∑ k=1 H∑ h=1 Γkh||1 = E|G− K∑ k=1 H∑ h=1 Γkh| (30)\nwhere the expectation in (30) is w.r.t. the normalized measure ν/ ∫ Ω θ(ω)α(dω) with ||G||1 = 1; and K and H are the truncation level of k and h. Then for the situation with H =∞:\n‖G− K∑ k=1 ∞∑ h=1 Γkh‖1 = 1 K + 1 (31)\nwhich indicates a O( 1K ) decreasing rate as same as the truncated beta process shown in (14). It is noteworthy that Γ1 alone accounts for on average half the mass of G. When H is finite, a remaining distance∑K\nk=1 1 k(k+1)H+1 is added."
    }, {
      "heading" : "4.5. Generalized gamma process and symmetric gamma process",
      "text" : "Theorem 2 can be easily extended to some variations of the gamma process. Here we give the examples of the generalized gamma process (Brix, 1999) and symmetric gamma process (Çinlar, 2010).\nThe generalized gamma process extends the ordinary gamma process by adding a parameter 0 < σ < 1,\nwhose Lévy measure is 1Γ(1−σ)p −σ−1e− p θ(ω) dpα(dω). Then with the same decomposition procedure, it is straightforward that the Lévy measure for Γkh of the generalized gamma process will change to νkh = Gamma(h− σ, θ(ω)k+1 )dp α(dω) Γ(1−σ)(k+1)hh .\nThe symmetric gamma process is a Lévy process whose increments are the differences of two gammadistributed variables with the same law, whose Lévy measure is |p|−1e− |p| θ(ω) dpα(dω). Since there can be negative increments, the symmetric gamma process is not a completely random measure. However, the same decomposition procedure is still applicable, yielding νkh = Gamma(|p|\n∣∣h, θ(ω)k+1 )dp 2α(dω)(k+1)hh , where the distribution Gamma(|p|\n∣∣h, θ(ω)k+1 ) is to first draw |p| from Gamma(h, θ(ω)k+1 ), then decide the sign of p through a symmetric Bernoulli distribution."
    }, {
      "heading" : "5. Conclusions",
      "text" : "The Lévy measure decomposition of the beta and gamma processes provides new perspectives on the two widely used stochastic processes, by casting insights on the sub-processes constituting them, here the Bk and Γk. And the decomposition prescriptions described here are far from the only ways of such decomposition. Theoretically elegant construction methods are derived from the proposed decompositions, which are directly implementable in practice.\nWe have applied the proposed beta and gamma representations in numerical experiments, the details of which are omitted, as this paper focuses on foundational properties. However, to briefly summarize experience with such representations, consider for example the image inpainting problem considered in (Zhou et al., 2009), based upon a beta process factor analysis model (Paisley & Carin, 2009). In experiments we performed with such a model, using a Gibbs sampler, the beta process prior was implemented using the procedure discussed in Section 3.3.1, with the posterior estimation in Section 3.5 applied for inference. The proposed representation infers a dictionary with the “important” dictionary elements captured by the lowindex members (see the discussion in Section 3.3.1). The model prioritized the first three dictionary elements as being pure colors, specifically red, green, and blue, with the important structured dictionary elements following (and no other pure-color dictionary elements, while in (Zhou et al., 2009) many – seemingly redundant – pure-color dictionary elements are inferred). This “clean” inference of prioritized dictionary elements may be responsible for our also higher observed PSNR in signal recovery, compared to the re-\nsult given in (Zhou et al., 2009). The new gamma process construction in Section 4.3 may be implemented in a similar manner, and may be employed within recent models in machine learning in which the gamma process has been utilized (e.g., (Paisley et al., 2011))."
    }, {
      "heading" : "Acknowledgements",
      "text" : "The research reported here was supported by ARO, NGA, ONR and DARPA (MSEE program)."
    } ],
    "references" : [ {
      "title" : "Processes and Stochastic Calculus",
      "author" : [ "Applebaum", "D. Levy" ],
      "venue" : null,
      "citeRegEx" : "Applebaum and Levy,? \\Q2009\\E",
      "shortCiteRegEx" : "Applebaum and Levy",
      "year" : 2009
    }, {
      "title" : "The nested chinese restaurant process and bayesian nonparametric inference of topic hierarchies",
      "author" : [ "D.M. Blei", "T.L. Griffiths", "M.I. Jordan" ],
      "venue" : "J. ACM,",
      "citeRegEx" : "Blei et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Blei et al\\.",
      "year" : 2010
    }, {
      "title" : "Generalized gamma measures and shot-noise Cox processes",
      "author" : [ "A. Brix" ],
      "venue" : "Advances in Applied Probability,",
      "citeRegEx" : "Brix,? \\Q1999\\E",
      "shortCiteRegEx" : "Brix",
      "year" : 1999
    }, {
      "title" : "Beta processes, stick-breaking, and power laws",
      "author" : [ "T. Broderick", "M. Jordan", "J. Pitman" ],
      "venue" : "Bayesian analysis,",
      "citeRegEx" : "Broderick et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Broderick et al\\.",
      "year" : 2011
    }, {
      "title" : "Probability and Stochastics",
      "author" : [ "E. Çinlar" ],
      "venue" : "Graduate Texts in Mathematics. Springer,",
      "citeRegEx" : "Çinlar,? \\Q2010\\E",
      "shortCiteRegEx" : "Çinlar",
      "year" : 2010
    }, {
      "title" : "Variational inference for the Indian buffet process",
      "author" : [ "F. Doshi", "K.T. Miller", "J. Van Gael", "Y.W. Teh" ],
      "venue" : "In AISTATS,",
      "citeRegEx" : "Doshi et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Doshi et al\\.",
      "year" : 2009
    }, {
      "title" : "A Bayesian analysis of some nonparametric problems",
      "author" : [ "T. Ferguson" ],
      "venue" : "The Annals of Statistics,",
      "citeRegEx" : "Ferguson,? \\Q1973\\E",
      "shortCiteRegEx" : "Ferguson",
      "year" : 1973
    }, {
      "title" : "Infinite latent feature models and the Indian buffet process",
      "author" : [ "T. Griffiths", "Z. Ghahramani" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Griffiths and Ghahramani,? \\Q2005\\E",
      "shortCiteRegEx" : "Griffiths and Ghahramani",
      "year" : 2005
    }, {
      "title" : "Nonparametric Bayes estimators based on beta processes in models for life history data",
      "author" : [ "N.L. Hjort" ],
      "venue" : "Annals of Statistics,",
      "citeRegEx" : "Hjort,? \\Q1990\\E",
      "shortCiteRegEx" : "Hjort",
      "year" : 1990
    }, {
      "title" : "Hierarchical models, nested models and completely random measures. In Frontiers of Statistical Decision Making and Bayesian Analysis: In Honor of James",
      "author" : [ "M.I. Jordan" ],
      "venue" : null,
      "citeRegEx" : "Jordan.,? \\Q2009\\E",
      "shortCiteRegEx" : "Jordan.",
      "year" : 2009
    }, {
      "title" : "Completely random measure",
      "author" : [ "J.F.C. Kingman" ],
      "venue" : "In Pacific Journal of Mathematics,",
      "citeRegEx" : "Kingman,? \\Q1967\\E",
      "shortCiteRegEx" : "Kingman",
      "year" : 1967
    }, {
      "title" : "Construction of dependent dirichlet processes based on poisson processes",
      "author" : [ "D. Lin", "E. Grimson", "J. Fisher" ],
      "venue" : "In NIPS, pp",
      "citeRegEx" : "Lin et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Lin et al\\.",
      "year" : 2010
    }, {
      "title" : "Dependent Nonparametric Processes",
      "author" : [ "S.N. MacEachern" ],
      "venue" : "Proceedings of the Section on Bayesian Statistical Science,",
      "citeRegEx" : "MacEachern,? \\Q1999\\E",
      "shortCiteRegEx" : "MacEachern",
      "year" : 1999
    }, {
      "title" : "Stick-breaking beta processes and the poisson process",
      "author" : [ "J. Paisley", "Blei D.M", "M.I. Jordan" ],
      "venue" : null,
      "citeRegEx" : "Paisley et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Paisley et al\\.",
      "year" : 2012
    }, {
      "title" : "Nonparametric factor analysis with beta process priors",
      "author" : [ "J. Paisley", "L. Carin" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Paisley and Carin,? \\Q2009\\E",
      "shortCiteRegEx" : "Paisley and Carin",
      "year" : 2009
    }, {
      "title" : "A stick-breaking construction of the beta process",
      "author" : [ "J. Paisley", "K. Zaas", "C. Woods", "G. Ginsburg", "L. Carin" ],
      "venue" : "In ICML, pp",
      "citeRegEx" : "Paisley et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Paisley et al\\.",
      "year" : 2010
    }, {
      "title" : "The discrete infinite logistic normal distribution for mixed-membership modeling",
      "author" : [ "J. Paisley", "C. Wang", "D. Blei" ],
      "venue" : "In AISTATS,",
      "citeRegEx" : "Paisley et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Paisley et al\\.",
      "year" : 2011
    }, {
      "title" : "Gaussian Processes for Machine Learning",
      "author" : [ "C. Rasmussen", "C. Williams" ],
      "venue" : null,
      "citeRegEx" : "Rasmussen and Williams,? \\Q2006\\E",
      "shortCiteRegEx" : "Rasmussen and Williams",
      "year" : 2006
    }, {
      "title" : "Lévy processes and infinitely divisible distributions",
      "author" : [ "K. Sato" ],
      "venue" : null,
      "citeRegEx" : "Sato,? \\Q1999\\E",
      "shortCiteRegEx" : "Sato",
      "year" : 1999
    }, {
      "title" : "A constructive definition of Dirichlet priors",
      "author" : [ "J. Sethuraman" ],
      "venue" : "Statistica Sinica,",
      "citeRegEx" : "Sethuraman,? \\Q1994\\E",
      "shortCiteRegEx" : "Sethuraman",
      "year" : 1994
    }, {
      "title" : "A hierarchical Bayesian language model based on Pitman-Yor processes",
      "author" : [ "Y.W. Teh" ],
      "venue" : "In Coling/ACL,",
      "citeRegEx" : "Teh,? \\Q2006\\E",
      "shortCiteRegEx" : "Teh",
      "year" : 2006
    }, {
      "title" : "Indian buffet processes with power-law behavior",
      "author" : [ "Y.W. Teh", "D. Görür" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Teh and Görür,? \\Q2009\\E",
      "shortCiteRegEx" : "Teh and Görür",
      "year" : 2009
    }, {
      "title" : "Hierarchical dirichlet processes",
      "author" : [ "Y.W. Teh", "M.I. Jordan", "M.J. Beal", "D.M. Blei" ],
      "venue" : "JASA, pp. 101:1566–1581,",
      "citeRegEx" : "Teh et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Teh et al\\.",
      "year" : 2006
    }, {
      "title" : "Stickbreaking construction for the Indian buffet process",
      "author" : [ "Y.W. Teh", "D. Görür", "Z. Ghahramani" ],
      "venue" : "In AISTATS,",
      "citeRegEx" : "Teh et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Teh et al\\.",
      "year" : 2007
    }, {
      "title" : "Nonparametric Bayesian Models for Machine Learning",
      "author" : [ "R. Thibaux" ],
      "venue" : "PhD thesis, EECS Dept.,",
      "citeRegEx" : "Thibaux,? \\Q2008\\E",
      "shortCiteRegEx" : "Thibaux",
      "year" : 2008
    }, {
      "title" : "Hierarchical beta processes and the Indian buffet process",
      "author" : [ "R. Thibaux", "M.I. Jordan" ],
      "venue" : "In AISTATS,",
      "citeRegEx" : "Thibaux and Jordan,? \\Q2007\\E",
      "shortCiteRegEx" : "Thibaux and Jordan",
      "year" : 2007
    }, {
      "title" : "Dependent Indian buffet processes",
      "author" : [ "S. Williamson", "P. Orbanz", "Z. Ghahramani" ],
      "venue" : "In AISTATS,",
      "citeRegEx" : "Williamson et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Williamson et al\\.",
      "year" : 2010
    }, {
      "title" : "Non-parametric Bayesian dictionary learning for sparse image representations",
      "author" : [ "M. Zhou", "H. Chen", "J. Paisley", "L. Ren", "G. Sapiro", "L. Carin" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Zhou et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Zhou et al\\.",
      "year" : 2009
    } ],
    "referenceMentions" : [ {
      "referenceID" : 6,
      "context" : "Recently the idea of nonparametric methods has extended to feature learning and data clustering, with interest respectively in the beta-Bernoulli process (Thibaux & Jordan, 2007) and the Dirichlet process (Ferguson, 1973).",
      "startOffset" : 205,
      "endOffset" : 221
    }, {
      "referenceID" : 18,
      "context" : "In this paper we focus on Lévy processes (Sato, 1999), which are of increasing interest in machine learning.",
      "startOffset" : 41,
      "endOffset" : 53
    }, {
      "referenceID" : 10,
      "context" : "A family of Lévy processes, the pure-jump nondecreasing Lévy processes, also fit into the category of the completely random measure proposed by Kingman (Kingman, 1967).",
      "startOffset" : 152,
      "endOffset" : 167
    }, {
      "referenceID" : 8,
      "context" : "The beta process (Hjort, 1990) is an example of such a process, which is applied in nonparametric feature learning.",
      "startOffset" : 17,
      "endOffset" : 30
    }, {
      "referenceID" : 22,
      "context" : "Hierarchical forms of such models have become increasingly popular in machine learning (Teh et al., 2006; Teh, 2006; Thibaux & Jordan, 2007), as have nested models (Blei et al.",
      "startOffset" : 87,
      "endOffset" : 140
    }, {
      "referenceID" : 20,
      "context" : "Hierarchical forms of such models have become increasingly popular in machine learning (Teh et al., 2006; Teh, 2006; Thibaux & Jordan, 2007), as have nested models (Blei et al.",
      "startOffset" : 87,
      "endOffset" : 140
    }, {
      "referenceID" : 1,
      "context" : ", 2006; Teh, 2006; Thibaux & Jordan, 2007), as have nested models (Blei et al., 2010), and models that introduce covariate dependence (MacEachern, 1999; Williamson et al.",
      "startOffset" : 66,
      "endOffset" : 85
    }, {
      "referenceID" : 12,
      "context" : ", 2010), and models that introduce covariate dependence (MacEachern, 1999; Williamson et al., 2010; Lin et al., 2010).",
      "startOffset" : 56,
      "endOffset" : 117
    }, {
      "referenceID" : 26,
      "context" : ", 2010), and models that introduce covariate dependence (MacEachern, 1999; Williamson et al., 2010; Lin et al., 2010).",
      "startOffset" : 56,
      "endOffset" : 117
    }, {
      "referenceID" : 11,
      "context" : ", 2010), and models that introduce covariate dependence (MacEachern, 1999; Williamson et al., 2010; Lin et al., 2010).",
      "startOffset" : 56,
      "endOffset" : 117
    }, {
      "referenceID" : 15,
      "context" : "As examples of such work, (Thibaux & Jordan, 2007) and (Paisley et al., 2010) present explicit constructions for generating the beta process, (Teh et al.",
      "startOffset" : 55,
      "endOffset" : 77
    }, {
      "referenceID" : 23,
      "context" : ", 2010) present explicit constructions for generating the beta process, (Teh et al., 2007) derives a construction for the Indian buffet process parallel to the stick-breaking construction of the Dirichlet process (Sethuraman, 1994), and (Thibaux, 2008) obtains a construction for the gamma process under the gamma-Poisson context.",
      "startOffset" : 72,
      "endOffset" : 90
    }, {
      "referenceID" : 19,
      "context" : ", 2007) derives a construction for the Indian buffet process parallel to the stick-breaking construction of the Dirichlet process (Sethuraman, 1994), and (Thibaux, 2008) obtains a construction for the gamma process under the gamma-Poisson context.",
      "startOffset" : 130,
      "endOffset" : 148
    }, {
      "referenceID" : 24,
      "context" : ", 2007) derives a construction for the Indian buffet process parallel to the stick-breaking construction of the Dirichlet process (Sethuraman, 1994), and (Thibaux, 2008) obtains a construction for the gamma process under the gamma-Poisson context.",
      "startOffset" : 154,
      "endOffset" : 169
    }, {
      "referenceID" : 10,
      "context" : "Apart from these specialized construction methods, in (Kingman, 1967) a general construction method for completely random measures is proposed, by first decomposing it into a sum of a countable number of σ-finite measures, and then superposing the Poisson processes according to these sub-measures.",
      "startOffset" : 54,
      "endOffset" : 69
    }, {
      "referenceID" : 15,
      "context" : "• The decomposition of the beta process unifies the constructions in (Thibaux & Jordan, 2007), (Teh & Görür, 2009), and (with a different decomposing method) (Paisley et al., 2010), and a new generative construction for the gamma process and its variations is derived.",
      "startOffset" : 158,
      "endOffset" : 180
    }, {
      "referenceID" : 18,
      "context" : "Lévy processes (Sato, 1999) and completely random measures (Kingman, 1967) are two closely related concepts.",
      "startOffset" : 15,
      "endOffset" : 27
    }, {
      "referenceID" : 10,
      "context" : "Lévy processes (Sato, 1999) and completely random measures (Kingman, 1967) are two closely related concepts.",
      "startOffset" : 59,
      "endOffset" : 74
    }, {
      "referenceID" : 18,
      "context" : "By the Lévy-Itô decomposition (Sato, 1999), a Lévy process can be decomposed into a continuous Brownian motion with drift, and a discrete part of a pure-jump process.",
      "startOffset" : 30,
      "endOffset" : 42
    }, {
      "referenceID" : 18,
      "context" : "with ν satisfying the integrability condition (Sato, 1999).",
      "startOffset" : 46,
      "endOffset" : 58
    }, {
      "referenceID" : 10,
      "context" : "According to (Kingman, 1967), Φo is discrete with both random atoms and jumps.",
      "startOffset" : 13,
      "endOffset" : 28
    }, {
      "referenceID" : 10,
      "context" : "In (Kingman, 1967), it is noted that Φo can be further split into a countable number of independent parts:",
      "startOffset" : 3,
      "endOffset" : 18
    }, {
      "referenceID" : 8,
      "context" : "A beta process (Hjort, 1990) is a Lévy process with beta-distributed increments; B ∼ BP(c(ω), μ) is a beta process if B(dω) ∼ Beta(c(ω)μ(dω), c(ω)(1− μ(dω))) (5) where μ is the base measure on measure space (Ω,F) and a positive function c(ω) the concentration function.",
      "startOffset" : 15,
      "endOffset" : 28
    }, {
      "referenceID" : 3,
      "context" : "The work of (Paisley & Jordan, 2012) and (Broderick et al., 2011) show that the stick-breaking construction of the beta process in (Paisley et al.",
      "startOffset" : 41,
      "endOffset" : 65
    }, {
      "referenceID" : 15,
      "context" : ", 2011) show that the stick-breaking construction of the beta process in (Paisley et al., 2010) is indeed a result of another way of decomposing the Lévy measure of the beta process.",
      "startOffset" : 73,
      "endOffset" : 95
    }, {
      "referenceID" : 15,
      "context" : "1 and make comparison with the construction of beta process in (Paisley et al., 2010).",
      "startOffset" : 63,
      "endOffset" : 85
    }, {
      "referenceID" : 15,
      "context" : "For the stick-breaking construction of beta process described in (Paisley et al., 2010), the L1 distance is: ( c c+1 ) .",
      "startOffset" : 65,
      "endOffset" : 87
    }, {
      "referenceID" : 5,
      "context" : "This metric was applied on the truncated Indian buffet process (Doshi et al., 2009) and truncated stick-breaking construction of the beta process (Paisley & Jordan, 2012), which indicates 1 4 ∫ |m∞(b)−mK(b)|db ≤ Pr(∃k > K, 1 ≤ i ≤ nk, 1 ≤ m ≤M, s.",
      "startOffset" : 63,
      "endOffset" : 83
    }, {
      "referenceID" : 9,
      "context" : "The gamma process can also be expressed in the form with a base measure G0 and a concentration c(ω), with c = 1/θ and G0 = θα (Jordan., 2009), to conform with other stochastic processes widely used in machine learning, such as the Dirichlet process.",
      "startOffset" : 126,
      "endOffset" : 141
    }, {
      "referenceID" : 2,
      "context" : "Here we give the examples of the generalized gamma process (Brix, 1999) and symmetric gamma process (Çinlar, 2010).",
      "startOffset" : 59,
      "endOffset" : 71
    }, {
      "referenceID" : 4,
      "context" : "Here we give the examples of the generalized gamma process (Brix, 1999) and symmetric gamma process (Çinlar, 2010).",
      "startOffset" : 100,
      "endOffset" : 114
    }, {
      "referenceID" : 27,
      "context" : "However, to briefly summarize experience with such representations, consider for example the image inpainting problem considered in (Zhou et al., 2009), based upon a beta process factor analysis model (Paisley & Carin, 2009).",
      "startOffset" : 132,
      "endOffset" : 151
    }, {
      "referenceID" : 27,
      "context" : "The model prioritized the first three dictionary elements as being pure colors, specifically red, green, and blue, with the important structured dictionary elements following (and no other pure-color dictionary elements, while in (Zhou et al., 2009) many – seemingly redundant – pure-color dictionary elements are inferred).",
      "startOffset" : 230,
      "endOffset" : 249
    }, {
      "referenceID" : 27,
      "context" : "sult given in (Zhou et al., 2009).",
      "startOffset" : 14,
      "endOffset" : 33
    }, {
      "referenceID" : 16,
      "context" : ", (Paisley et al., 2011)).",
      "startOffset" : 2,
      "endOffset" : 24
    } ],
    "year" : 2012,
    "abstractText" : "We develop new representations for the Lévy measures of the beta and gamma processes. These representations are manifested in terms of an infinite sum of well-behaved (proper) beta and gamma distributions. Further, we demonstrate how these infinite sums may be truncated in practice, and explicitly characterize truncation errors. We also perform an analysis of the characteristics of posterior distributions, based on the proposed decompositions. The decompositions provide new insights into the beta and gamma processes (and their generalizations), and we demonstrate how the proposed representation unifies some properties of the two. This paper is meant to provide a rigorous foundation for and new perspectives on Lévy processes, as these are of increasing importance in machine learning.",
    "creator" : "LaTeX with hyperref package"
  }
}