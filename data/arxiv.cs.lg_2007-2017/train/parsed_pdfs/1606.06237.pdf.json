{
  "name" : "1606.06237.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Online and Differentially-Private Tensor Decomposition",
    "authors" : [ "Yining Wang", "Animashree Anandkumar" ],
    "emails" : [ "yiningwa@cs.cmu.edu", "a.anandkumar@uci.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Keywords: Tensor decomposition, tensor power method, online methods, streaming, differential privacy, perturbation analysis."
    }, {
      "heading" : "1 Introduction",
      "text" : "In recent years, tensor decomposition has emerged as a powerful tool to solve many challenging problems in unsupervised [1], supervised [18] and reinforcement learning [4]. Tensors are higher order extensions of matrices which can reveal far greater information compared to matrices, while retaining most of the efficiencies of matrix operations [1].\nA central task in tensor analysis is the process of decomposing the tensor into its rank-1 components, which is usually referred to as CP (Candecomp/Parafac) decomposition in the literature. While decomposition of arbitrary tensors is NP-hard [13], it becomes tractable for the class of tensors with linearly independent components. Through a simple whitening procedure, such tensors can be converted to orthogonally decomposable tensors. Tensor power method is a popular method for computing the decomposition of an orthogonal tensor. It is simple and efficient to implement, and a natural extension of the matrix power method.\nIn the absence of noise, the tensor power method correctly recovers the components under a random initialization followed by deflation. On the other hand, perturbation analysis of tensor power method is much more delicate compared to the matrix case. This is because the problem of tensor decomposition is NP-hard, and if a large amount of arbitrary noise is added to an orthogonal tensor, the decomposition can again become intractable. In [1], guaranteed recovery of components was proven under bounded noise, and the bound was improved in [2]. In this paper, we significantly improve upon the noise requirements, i.e. the extent of noise that can be withstood by the tensor power method.\nIn order for tensor methods to be deployed in large-scale systems, we require fast, parallelizable and scalable algorithms. To achieve this, we need to avoid the exponential increase in computation and memory requirements with the order of the tensor; i.e. a naive implementation on a 3rd-order d-dimensional tensor would require O(d3) computation and memory. Instead, we analyze the online tensor power method that requires only linear (in d) memory and does not form the entire tensor. This is achieved in settings, where the tensor is an empirical higher order moment, computed from the stream of data samples. We can avoid explicit construction of the tensor by running online tensor\n29th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain.\nar X\niv :1\n60 6.\n06 23\n7v 4\n[ st\nat .M\nL ]\n1 5\nD ec\n2 01\npower method directly on i.i.d. data samples. We show that this algorithm correctly recovers tensor components in time1 Õ(nk2d) and Õ(dk) memory for a rank-k tensor and n number of data samples. Additionally, we provide efficient sample complexity analysis.\nAs spectral methods become increasingly popular with recommendation system and health analytics applications [29, 17], data privacy is particularly relevant in the context of preserving sensitive private information. Differential privacy could still be useful even if data privacy is not the prime concern [30]. We propose the first differentially private tensor decomposition algorithm with both privacy and utility guarantees via noise calibrated power iterations. We show that under the natural assumption of tensor incoherence, privacy parameters have no (polynomial) dependence on tensor dimension d. On the other hand, straightforward input perturbation type methods lead to far worse bounds and do not yield guaranteed recovery for all values of privacy parameters."
    }, {
      "heading" : "1.1 Related work",
      "text" : "Online tensor SGD Stochastic gradient descent (SGD) is an intuitive approach for online tensor decomposition and has been successful in practical large-scale tensor decomposition problems [16]. Despite its simplicity, theoretical properties are particularly hard to establish. [11] considered a variant of the SGD objective and proved its correctness. However, the approach in [11] only works for even-order tensors and its sample complexity dependency upon tensor dimension d is poor.\nTensor PCA In the statistical tensor PCA [24] model a d×d×d tensor T = v⊗3+E is observed and one wishes to recover component v under the presence of Gaussian random noise E. [24] shows that ‖E‖op = O(d−1/2) is sufficient to guarantee approximate recovery of v and [14] further improves the noise condition to ‖E‖op = O(d−1/4) via a 4th-order sum-of-squares relaxation. Techniques in both [24, 14] are rather complicated and could be difficult to adapt to memory or privacy constraints. Furthermore, in [24, 14] only one component is considered. On the other hand, [25] shows that ‖E‖op = O(d−1/2) is sufficient for recovering multiple components from noisy tensors. However, [25] assumes exact computation of rank-1 tensor approximation, which is NP-hard in general.\nNoisy matrix power methods Our relaxed noise condition analysis for tensor power method is inspired by recent analysis of noisy matrix power methods [12, 6]. Unlike the matrix case, tensor decomposition no longer requires spectral gap among eigenvalues and eigenvectors are usually recovered one at a time [1, 2]. This poses new challenges and requires non-trivial extensions of matrix power method analysis to the tensor case."
    }, {
      "heading" : "1.2 Notation and Preliminaries",
      "text" : "We use [n] to denote the set {1, 2, · · · , n}. We use bold characters A,T,v for matrices, tensors, vectors and normal characters λ, µ for scalars. A pth order tensor T of dimensions d1, · · · , dp has d1 × · · · × dp elements, each indexed by a p-tuple (i1, · · · , ip) ∈ [d1]× · · · × [dp]. A tensor T of dimensions d× · · · × d is super-symmetric or simply symmetric if Ti1,··· ,ip = Tσ(i1),··· ,σ(ip) for all permutations σ : [p] → [p]. For a tensor T ∈ Rd1×···×dp and matrices A1 ∈ Rm1×d1 , · · · ,Ap ∈ Rmp×dp , the multi-linear form T(A1, · · · ,Ap) is a m1 × · · · ×mp tensor defined as\n[T(A1, · · · ,Ap)]i1,··· ,ip = ∑\nj1∈[d1]\n· · · ∑\njp∈[dp]\nTj1,··· ,jp [A1]j1,i1 · · · [Ap]jp,ip .\nWe use ‖v‖2 = √∑ i v 2 i for vector 2-norm and ‖v‖∞ = maxi |vi| for vector infinity norm. We use ‖T‖op to denote the operator norm or spectral norm of a tensor T, which is defined as ‖T‖op = sup‖u1‖2=···‖up‖2=1 T(u1, · · · ,up). An event A is said to occur with overwhelming probability if Pr[A] ≥ 1− d−10. We limit ourselves to symmetric 3rd-order tensors (p = 3) in this paper. The results can be directly extended to asymmetric tensors since they can first be symmetrized using simple matrix operations (see [1]). Extension to higher-order tensors is also straightforward. A symmetric 3rd-order tensor T is rank-1 if it can be written in the form of\nT = λ · v ⊗ v ⊗ v = λv⊗3 ⇐⇒ Ti,j,` = λ · v(i) · v(j) · v(`), (1) 1Õ hides poly-logarithmic factors.\nAlgorithm 1 Robust tensor power method [1]\n1: Input: symmetric d× d× d tensor T̃, number of components k ≤ d, number of iterations L, R. 2: for i = 1 to k do 3: Initialization: Draw u0 uniformly at random from the unit sphere in Rd. 4: Power iteration: Compute ut = T̃(I,ut−1,ut−1)/‖T̃(I,ut−1,ut−1)‖2 for t = 1, · · · , R. 5: Boosting: Repeat Steps 3 and 4 for L times and obtain u(1)R , · · · ,u (L) R . Let τ ∗ =\nargmaxLτ=1T̃(u (τ) R ,u (τ) R ,u (τ) R ). Set v̂i = u (τ) R and λ̂i = T̃(u (τ) R ,u (τ) R ,u (τ) R ).\n6: Deflation: T̃← T̃− λ̂iv̂⊗3i . 7: end for 8: Output: Estimated eigenvalue/Eigenvector pairs {λ̂i, v̂i}ki=1.\nwhere ⊗ represents the outer product, and v ∈ Rd is a unit vector (i.e., ‖v‖2 = 1) and λ ∈ R+. 2 A tensor T ∈ Rd×d×d is said to have a CP (Candecomp/Parafac) rank k if it can be (minimally) written as the sum of k rank-1 tensors:\nT = ∑ i∈[k] λivi ⊗ vi ⊗ vi, λi ∈ R+, vi ∈ Rd. (2)\nA tensor is said to be orthogonally decomposable if in the above decomposition 〈vi,vj〉 = 0 for i 6= j. Any tensor can be converted to an orthogonal tensor through an invertible whitening transform, provided that v1,v2, . . . ,vk are linearly independent [1]. We thus limit our analysis to orthogonal tensors in this paper since it can be extended to this more general class in a straightforward manner.\nTensor Power Method: A popular algorithm for finding the tensor decomposition in (2) is through the tensor power method. The full algorithm is given in Algorithm 1. We first provide an improved noise analysis for the robust power method, improving error tolerance bounds previously established in [1]. We next propose memory-efficient and/or differentially private variants of the robust power method and give performance guarantee based on our improved noise analysis."
    }, {
      "heading" : "2 Improved Noise Analysis for Tensor Power Method",
      "text" : "When the tensor T has an exact orthogonal decomposition, the power method provably recovers all the components with random initialization and deflation. However, the analysis is more subtle under noise. While matrix perturbation bounds are well understood, it is an open problem in the case of tensors. This is because the problem of tensor decomposition is NP-hard, and becomes tractable only under special conditions such as orthogonality (and more generally linear independence). If a large amount of arbitrary noise is added, the decomposition can again become intractable. In [1], guaranteed recovery of components was proven under bounded noise and we recap the result below.\nTheorem 2.1 ([1] Theorem 5.1, simplified version). Suppose T̃ = T+∆T , where T = ∑k i=1 λiv ⊗3 i with λi > 0 and orthonormal basis vectors{v1, · · · ,vk} ⊆ Rd, d ≥ k, and noise ∆T satisfies ‖∆T ‖op ≤ . Let λmax, λmin be the largest and smallest values in {λi}ki=1 and {λ̂i, v̂i}ki=1 be outputs of Algorithm 1. There exist absolute constants K0, C1, C2, C3 > 0 such that if\n≤ C1 ·λmin/d, R = Ω(log d+log log(λmax/ )), L = Ω(max{K0, k} log(max{K0, k})), (3)\nthen with probability at least 0.9, there exists a permutation π : [k]→ [k] such that\n|λi − λ̂π(i)| ≤ C2 , ‖vi − v̂π(i)‖2 ≤ C3 /λi, ∀i = 1, · · · , k.\nTheorem 2.1 is the first provably correct result on robust tensor decomposition under general noise conditions. In particular, the noise term ∆T can be deterministic or even adversarial. However, one important drawback of Theorem 2.1 is that ‖∆T ‖op must be upper bounded by O(λmin/d), which is a strong assumption for many practical applications [28]. On the other hand, [2, 24] show that by using smart initializations the robust tensor power method is capable of tolerating O(λmin/ √ d)\n2One can always assume without loss of generality that λ ≥ 0 by replacing v with −v instead.\nmagnitude of noise, and [25] suggests that such noise magnitude cannot be improved if deflation (i.e., successive rank-one approximation) is to be performed. In this paper, we show that the relaxed noise bound O(λmin/ √ d) holds even if the initialization of robust TPM is as simple as a vector uniformly sampled from the d-dimensional sphere (Algorithm 1). Our claim is formalized below:\nTheorem 2.2 (Improved noise tolerance analysis for robust TPM). Assume the same notation as in Theorem 2.1. Let ∈ (0, 1/2) be an error tolerance parameter. There exist absolute constants K0, C0, C1, C2, C3 > 0 such that if ∆T satisfies\n‖∆T (I,u(τ)t ,u (τ) t )‖2 ≤ , |∆T (vi,u (τ) t ,u (τ) t )| ≤ min{ / √ k,C0λmin/d} (4)\nfor all i ∈ [k], t ∈ [T ], τ ∈ [L] and furthermore\n≤ C1 · λmin/ √ k, R = Ω(log(λmaxd/ )), L = Ω(max{K0, k} log(max{K0, k})), (5)\nthen with probability at least 0.9, there exists a permutation π : [k]→ [k] such that\n|λi − λ̂π(i)| ≤ C2 , ‖vi − v̂π(i)‖2 ≤ C3 /λi, ∀i = 1, · · · , k.\nDue to space constraints, proof of Theorem 2.2 is placed in Appendix C. We next make several remarks on our results. In particular, we consider three scenarios with increasing assumptions imposed on the noise tensor ∆T and compare the noise conditions in Theorem 2.2 with existing results on orthogonal tensor decomposition:\n1. ∆T does not have any special structure: in this case, we only have |∆T (vi,ut,ut)| ≤ ‖∆T ‖op and our noise conditions reduces to the classical one: ‖∆T ‖op = O(λmin/d). 2. ∆T is “round” in the sense that |∆T (vi,ut,ut)| ≤ O(1/ √ d) · ‖∆T (I,ut,ut)‖2: this is\nthe typical setting when the noise ∆T follows Gaussian or sub-Gaussian distributions, as we explain in Sec. 3 and 4. Our noise condition in this case is ‖∆T ‖op = O(λmin/ √ d), strictly improving Theorem 2.1 on robust tensor power method with random initializations and matching the bound for more advanced SVD initialization techniques in [2].\n3. ∆T is weakly correlated with signal in the sense that ‖∆T (vi, I, I)‖2 = O(λmin/d) for all i ≤ k: in this case our noise condition reduces to ‖∆T ‖op = O(λmin/ √ k), strictly\nimproving over SVD initialization [2] in the “undercomplete” regime k = o(d). Note that the whitening trick [3, 1] does not attain our bound, as we explain in Appendix B.\nFinally, we remark that the log log(1/ ) quadratic convergence rate in Eq. (3) is worsened to log(1/ ) linear rate in Eq. (5). We are not sure whether this is an artifact of our analysis, because similar analysis for the matrix noisy power method [12] also reveals a linear convergence rate.\nImplications Our bounds in Theorem 2.2 results in sharper analysis of both memory-efficient and differentially private power methods which we propose in Sec. 3, 4. Using the original analysis (Theorem 2.1) for the two applications, the memory-efficient tensor power method would have sample complexity cubic in the dimension d and for differentially private tensor decomposition the privacy level ε needs to scale as Ω̃( √ d) as d increases, which is particularly bad as the quality of privacy protection eε degrades exponentially with tensor dimension d. On the other hand, our improved noise condition in Theorem 2.2 greatly sharpens the bounds in both applications: for memory efficient decomposition, we now require only quadratic sample complexity and for differentially private decomposition, the privacy level ε has no polynomial dependence on d. This makes our results far more practical for high-dimensional tensor decomposition applications.\nNumerical verification of noise conditions and comparison with whitening techniques We verify our improved noise conditions for robust tensor power method on simulation tensor data. In particular, we consider three noise models and demonstrate varied asymptotic noise magnitudes at which tensor power method succeeds. The simulation results nicely match our theoretical findings and also suggest, in an empirical way, tightness of noise bounds in Theorem 2.2. Due to space constraints, simulation results are placed in Appendix A.\nWe also compare our improved noise bound with those obtained by whitening, a popular technique that reduces tensor decomposition to matrix decomposition problems [1, 21, 28]. We show in Appendix B that, without side information the standard analysis of whitening based tensor decomposition leads to worse noise tolerance bounds than what we obtained in Theorem 2.2."
    }, {
      "heading" : "3 Memory-Efficient Streaming Tensor Decomposition",
      "text" : "Tensor power method in Algorithm 1 requires significant storage to be deployed: Ω(d3) memory is required to store a dense d × d × d tensor, which is prohibitively large in many real-world applications as tensor dimension d could be really high. We show in this section how to compute tensor decomposition in a memory efficient manner, with storage scaling linearly in d. In particular, we consider the case when tensor T to be decomposed is a population moment Ex∼D[x⊗3] with respect to some unknown underlying data distribution D, and data points x1,x2, · · · i.i.d. sampled fromD are fed into a tensor decomposition algorithm in a streaming fashion. One classical example is topic modeling, where each xi represents documents that come in streams and consistent estimation of topics can be achieved by decomposing variants of the population moment [1, 3].\nAlgorithm 2 displays memory-efficient tensor decomposition procedure on streaming data points. The main idea is to replace the power iteration step T(I,u,u) in Algorithm 1 with a “data association” step that exploits the empirical-moment structure of the tensor T to be decomposed and evaluates approximate power iterations from stochastic data samples. This procedure is highly efficient, in that both time and space complexity scale linearly with tensor dimension d: Proposition 3.1. Algorithm 2 runs in O(nkdLR) time and O(d(k + L)) memory, with O(nkR) sample complexity (number of data point gone through).\nIn the remainder of this section we show Algorithm 2 recovers eigenvectors of the population moment Ex∼D[x⊗3] with high probability and we derive corresponding sample complexity bounds. To facilitate our theoretical analysis we need several assumptions on the data distribution D. The first natural assumption is the low-rankness of the population moment Ex∼D[x⊗3] to be decomposed: Assumption 3.1 (Low-rank moment). The mean tensor T = Ex∼D[x⊗3] admits a low-rank representation T = ∑k i=1 λiv ⊗3 i for λ1, · · · , λk > 0 and orthonormal {v1, · · · ,vk} ⊆ Rd.\nWe also place restrictions on the “noise model”, which imply that the population moment Ex∼D[x⊗3] can be well approximated by a reasonable number of samples with high probability. In particular, we consider sub-Gaussian noise as formulated in Definition 3.1 and Assumption 3.2: Definition 3.1 (Multivariate sub-Gaussian distribution, [15]). A D-dimensional random variable x belongs to the sub-Gaussian distribution family SGD(σ) with parameter σ > 0 if it has zero mean and E [ exp(a>x) ] ≤ exp { ‖a‖22σ2/2 } for all a ∈ RD. Assumption 3.2 (Sub-Gaussian noise). There exists σ > 0 such that the mean-centered vectorized random variable vec(x⊗3 − E[x⊗3]) belongs to SGd3(σ) as defined in Definition 3.1.\nWe remark that Assumption 3.2 includes a wide family of distributions that are of practical importance, for example noise that have compact support. Assumption 3.2 also resembles (B, p)-round noise considered in [12] that imposes spherical symmetry constraints onto the noise distribution.\nWe are now ready to present the main theorem that bounds the recovery (approximation) error of eigenvalues and eigenvectors of the streaming robust tensor power method in Algorithm 2: Theorem 3.1 (Analysis of streaming robust tensor power method). Let Assumptions 3.1, 3.2 hold true and suppose < C1λmin/ √ k for some sufficiently small absolute constant C1 > 0. If\nn = Ω̃ ( min { σ2d\n2 , σ2d2\nλ2min\n}) , R = Ω(log(λmaxd/ )), L = Ω(k log k),\nthen with probability at least 0.9 there exists permutation π : [k]→ [k] such that\n|λi − λ̂π(i)| ≤ C2 , ‖vi − v̂π(i)‖2 ≤ C3 /λi, ∀i = 1, · · · , k for some universal constants C2, C3 > 0.\nCorollary 3.1 is then an immediate consequence of Theorem 3.1, which simplifies the bounds and highlights asymptotic dependencies over important model parameters d, k and σ:\nAlgorithm 2 Online robust tensor power method 1: Input: data stream x1,x2, · · · ∈ Rd, no. of components k, parameters L,R, n. 2: for i = 1 to k do 3: Draw u(1)0 , · · · ,u (L) 0 i.i.d. uniformly at random from the unit sphere Sd−1.\n4: for t = 0 to R− 1 do 5: Initialization: Set accumulators ũ(1)t+1, · · · , ũ (L) t+1 and λ̃\n(1), · · · , λ̃(L) to 0. 6: Data association: Read the next n data points; update ũ(τ)t+1 ← ũ (τ) t+1 + 1 n (x > ` u (τ) t ) 2xi\nand λ̃(τ) ← λ̃(τ) + 1n (x > ` u (τ) t ) 3 for each ` ∈ [n] and τ ∈ [L]. 7: Deflation: For each τ ∈ [L], update ũ(τ)t+1 ← ũ (τ) t+1 − ∑i−1 j=1 λ̂jξ 2 j,τ v̂j\nand λ̃(τ) ← λ̃(τ) − ∑i−1 j=1 λ̂jξ 3 j,τ , where ξj,τ = v̂ > j ũ (τ) t .\n8: Normalization: u(τ)t+1 = ũ (τ) t+1/‖ũ (τ) t+1‖2, for each τ ∈ [L].\n9: end for 10: Find τ∗ = argmaxτ∈[L]λ̃ (τ) and store λ̂i = λ̃(τ ∗), v̂i = u (τ∗) R . 11: end for 12: Output: approximate eigenvalue and eigenvector pairs {λ̂i, v̂i}ki=1 of Êx∼D[x⊗3].\nCorollary 3.1. Under Assumptions 3.1, 3.2, Algorithm 2 correctly learns {λi,vi}ki=1 up toO(1/ √ d) additive error with Õ(σ2kd2) samples and Õ(dk) memory.\nProofs of Theorem 3.1 and Corollary 3.1 are both deferred to Appendix D. Compared to streaming noisy matrix PCA considered in [12], the bound is weaker with an additional 1/k factor in the term involving and 1/d factor in the term that does not involve . We conjecture this to be a fundamental difficulty of the tensor decomposition problem. On the other hand, our bounds resulting from the analysis in Sec. 2 have a O(1/d) improvement compared to applying existing analysis in [1] directly.\nRemark on comparison with SGD: Our proposed streaming tensor power method is nothing but the projected stochastic gradient descent (SGD) procedure on the objective of maximizing the tensor norm on the sphere. The optimal solution of this coincides with the objective of finding the best rank-1 approximation of the tensor. Here, we can estimate all the components of the tensor through deflation. An alternative method is to run SGD based a combined objective function to obtain all the components of the tensor simultaneously, as considered in [16, 11]. However, the analysis in [11] only works for even-order tensors and has worse dependency (at least d9) on tensor dimension d."
    }, {
      "heading" : "4 Differentially private tensor decomposition",
      "text" : "The objective of private data processing is to release data summaries such that any particular entry of the original data cannot be reliably inferred from the released results. Formally speaking, we adopt the popular (ε, δ)-differential privacy criterion proposed in [9]: Definition 4.1 ((ε, δ)-differential privacy [9]). Let M denote all symmetric d-dimensional real third order tensors and O be an arbitrary output set. A randomized algorithm A : M → O is (ε, δ)-differentially private if for all neighboring tensors T,T′ and measurable set O ⊆ O we have\nPr [A(T) ∈ O] ≤ eε Pr [A(T′) ∈ O] + δ,\nwhere ε > 0, δ ∈ [0, 1) are privacy parameters and probabilities are taken over randomness in A.\nSince our tensor decomposition analysis concerns symmetric tensors primarily, we adopt a “symmetric” definition of neighboring tensors in Definition 4.1, as shown below: Definition 4.2 (Neighboring tensors). Two d×d×d symmetric tensors T,T′ are neighboring tensors if there exists i, j, k ∈ [d] such that\nT′−T = ±symmetrize(ei⊗ej⊗ek) = ± (ei ⊗ ej ⊗ ek + ei ⊗ ek ⊗ ej + · · ·+ ek ⊗ ej ⊗ ei) .\nAs noted earlier, the above notions can be similarly extended to asymmetric tensors as well as the guarantees for tensor power method on asymmetric tensors. We also remark that the difference of\nAlgorithm 3 Differentially private robust tensor power method 1: Input: tensor T, no. of components k, number of iterations L,R, privacy parameters ε, δ.\n2: Initialization: D = 0, ν = 6 √ 2 ln(1.25/δ′)\nε′ , δ ′ = δ2K , ε ′ = ε√ K(4+ln(2/δ)) , K = kL(R+ 1).\n3: for i = 1 to k do 4: Initialization: Draw u(1)0 , · · · ,u (τ) 0 uniformly at random from the unit sphere in Rd. 5: for t = 0 to R− 1 do 6: Power iteration: compute ũ(τ)t+1 = (T−D)(I,u (τ) t ,u (τ) t ). 7: Noise calibration: release ū(τ)t+1 = ũ (τ) t+1 + ν‖u (τ) t ‖2∞ · z (τ) t , where z (τ) t\ni.i.d.∼ N (0, Id). 8: Normalization: u(τ)t+1 = ū (τ) t+1/‖ū (τ) t+1‖2.\n9: end for 10: Compute λ̃(τ) = (T−D)(u(τ)R ,u (τ) R ,u (τ) R ) + ν‖u (τ) R ‖3∞ · zτ and let τ∗ = argmaxτ λ̃(τ)."
    }, {
      "heading" : "11: Deflation: λ̂i = λ̃(τ",
      "text" : "∗), v̂i = u (τ∗) R , D← D + λ̂iv̂ ⊗3 i . 12: end for 13: Output: eigenvalue/eigenvector pairs {λ̂i, v̂i}ki=1.\n“neighboring tensors” as defined above has Frobenious norm bounded by O(1). This is necessary because an arbitrary perturbation of a tensor, even if restricted to only one entry, is capable of destroying any utility guarantee possible.\nIn a nutshell, Definitions 4.1, 4.2 state that an algorithm A is differentially private if, conditioned on any set of possible outputs of A, one cannot distinguish with high probability between two “neighboring” tensors T,T′ that differ only in a single entry (up to symmetrization), thus protecting the privacy of any particular element in the original tensor T. Here ε, δ are parameters controlling the level of privacy, with smaller ε, δ values implying stronger privacy guarantee as Pr[A(T) ∈ O] and Pr[A(T′) ∈ O] are closer to each other. Algorithm 3 describes the procedure of privately releasing eigenvectors of a low-rank input tensor T. The main idea for privacy preservation is the following noise calibration step\nūt+1 = ũt+1 + ν‖ut‖2∞ · zt, where zt is a d-dimensional standard Normal random variable and ν‖ut‖2∞ is a carefully designed noise magnitude in order to achieved desired privacy level (ε, δ). One key aspect is that the noise calibration step occurs at every power iteration, which adds to the robustness of the algorithm and achieves sharper bounds. We discuss at the end of this section. Theorem 4.1 (Privacy guarantee). Algorithm 3 satisfies (ε, δ)-differential privacy.\nProof. The only power iteration step of Algorithm 3 can be thought of as K = kL(R+ 1) queries directed to a private data sanitizer which produces f1(T;u) = T(I,u,u) or f2(T;u) = T(u,u,u) each time. The `2-sensitivity of both queries can be separately bounded as\n∆2f1 = sup T′ ‖T(I,u,u)−T′(I,u,u)‖2 ≤ sup i,j,k 2(|uiuj |+ |uiuk|+ |ujuk|) ≤ 6‖u‖2∞;\n∆2f2 = sup T′ ∣∣T(u,u,u)−T′(u,u,u)∣∣ = sup i,j,k 6 ∣∣uiujuk∣∣ ≤ 6‖u‖3∞,\nwhere T′ = T + symmetrize(ei ⊗ ej ⊗ ek) is some neighboring tensor of T. Thus, applying the Gaussian mechanism [9] we can (ε, δ)-privately release one output of either f1(u) or f2(u) by\nf`(u) + ∆2f` ·\n√ 2 ln(1.25/δ)\nε ·w,\nwhere ` = 1, 2 and w ∼ N (0, I) are i.i.d. standard Normal random variables. Finally, applying advanced composition [9] across all K = kL(R+ 1) private releases we complete the proof of this proposition. Note that both normalization and deflation steps do not affect the differential privacy of Algorithm 3 due to the closeness under post-processing property of DP.\nThe rest of the section is devoted to discussing the “utility” of Algorithm 3; i.e., to show that the algorithm is still capable of producing approximate eigenvectors, despite the privacy constraints. Similar to [12], we adopt the following incoherence assumptions on the eigenspace of T:\nAssumption 4.1 (Incoherent basis). Suppose V ∈ Rd×k is the stacked matrix of orthonormal component vectors {vi}ki=1. There exists constant µ0 > 0 such that\nd k max 1≤i≤d ‖V>ei‖22 ≤ µ0. (6)\nNote that by definition, µ0 is always in the range of [1, d/k]. Intuitively, Assumption 4.1 with small constant µ0 implies a relatively “flat” distribution of element magnitudes in T. The incoherence level µ0 plays an important role in the utility guarantee of Algorithm 3, as we show below: Theorem 4.2 (Guaranteed recovery of eigenvector under privacy requirements). Suppose T =∑k\ni=1 λiv ⊗3 i for λ1 > λ2 ≥ λ3 ≥ · · · ≥ λk > 0 with orthonormal v1, · · · ,vk ∈ Rd, and suppose\nAssumption 4.1 holds with µ0. Assume λ1−λ2 ≥ c/ √ d for some sufficiently small universal constant c > 0. If R = Θ(log(λmaxd)), L = Θ(k log k) and ε, δ satisfy\nε = Ω\n( µ0k 2 log(λmaxd/δ)\nλmin\n) , (7)\nthen with probability at least 0.9 the first eigen pair (λ̂1, v̂1) returned by Algorithm 3 satisfies∣∣λ1 − λ̂1∣∣ = O(1/√d), ‖v1 − v̂1‖2 = O(1/(λ1√d)). At a high level, Theorem 4.2 states that when the privacy parameter ε is not too small (i.e., privacy requirements are not too stringent), Algorithm 3 approximately recovers the largest eigenvalue and eigenvector with high probability. Furthermore, when µ0 is a constant, the lower bound condition on the privacy parameter ε does not depend polynomially upon tensor dimension d, which is a much desired property for high-dimensional data analysis. On the other hand, similar results cannot be achieved via simpler methods like input perturbation, as we discuss below:\nComparison with input perturbation Input perturbation is perhaps the simplest method for differentially private data analysis and has been successful in numerous scenarios, e.g. private matrix PCA [10]. In our context, this would entail appending a random Gaussian tensor E directly onto the input tensor T before tensor power iterations. By Gaussian mechanism, the standard deviation σ of each element in E scales as σ = Ω(ε−1 √ log(1/δ)). On the other hand, noise analysis for tensor decomposition derived in [24, 2] and in the subsequent section of this paper requires σ = O(1/d) or ‖E‖op = O(1/ √ d), which implies ε = Ω̃(d) (cf. Lemma F.9). That is, the privacy parameter ε must scale linearly with tensor dimension d to successfully recover even the first principle eigenvector, which renders the privacy guarantee of the input perturbation procedure useless for high-dimensional tensors. Thus, we require a non-trivial new approach for differentially private tensor decomposition.\nFinally, we remark that a more desired utility analysis would bound the approximation error ‖vi−v̂i‖2 for every component v1, · · · ,vk, and not just the top eigenvector. Unfortunately, our current analysis cannot handle deflation effectively as the deflated vector v̂i − vi may not be incoherent. Extension to deflated tensor decomposition remains an interesting open question."
    }, {
      "heading" : "5 Conclusion",
      "text" : "We consider memory-efficient and differentially private tensor decomposition problems in this paper and derive efficient algorithms for both online and private tensor decomposition based on the popular tensor power method framework. Through an improved noise condition analysis of robust tensor power method, we obtain sharper dimension-dependent sample complexity bounds for online tensor decomposition and wider range of privacy parameters values for private tensor decomposition while still retaining utility. Simulation results verify the tightness of our noise conditions in principle.\nOne important direction of future research is to extend our online and/or private tensor decomposition algorithms and analysis to practical applications such as topic modeling and community detection, where tensor decomposition acts as one critical step for data analysis. An end-to-end analysis of online/private methods for these applications would be theoretically interesting and could also greatly benefit practical machine learning of important models.\nAcknowledgement A. Anandkumar is supported in part by Microsoft Faculty Fellowship, NSF Career award CCF-1254106, ONR Award N00014- 14-1-0665, ARO YIP Award W911NF-13-1-0084 and AFOSR YIP FA9550-15-1-0221."
    }, {
      "heading" : "A Simulation results",
      "text" : "We verify our main theoretical results in Theorem 2.2 on synthetic tensors. T is taken to be a rank-3 tensor T = v⊗31 + 0.75v ⊗3 2 + 0.5v ⊗3 3 , where v1 = (1, 0, 0, 0, · · · , ), v2 = (0, 1, 0, 0, · · · ) and v3 = (0, 0, 1, 0, · · · ). The noise tensor E is synthesized according to the following three regimes:\n1. Random Gaussian noise: First generate Eijk i.i.d.∼ N (0, 1) and then super-symmetrize E. 2. Adversarial Gaussian noise: E = ∑d i=1 v2 ⊗ ei ⊗ ei + ei ⊗ v2 ⊗ ei + ei ⊗ ei ⊗ v2,\nwhere ei = (0, · · · , 0, 1, 0, · · · , 0) has all zero entries except for the ith one. 3. Weakly correlated noise: Let {v4, · · · ,vd} be an orthonormal basis of the orthogonal\ncomplement of span{v1,v2,v3}. Set E = ∑d i=4 vi ⊗ vi ⊗ vi.\nIn Fig. 1 we plot the “failure probability” (measured via 20 independent trials per setting) of the robust tensor power method with random initialization against controlled noise magnitude ‖E‖op = σ. A trial is “successful” if for all i ∈ {1, 2, 3} the recovered eigenvector v̂i satisfies v̂>i vi ≥ 1/4. To control ‖E‖op, we first compute the operator norm of the generated raw noise tensor by invoking the eig_sshopm routine in Matlab tensor toolbox [5] (algorithm based on [20]) and then re-scale the entries. By inspecting the noise levels at which phase transition of failure probabilities occurs for different tensor dimensions d, ranging from 25 to 200. It is quite clear from Fig. 1 that the phase transitions occur at σ = O(1/ √ d) for random Gaussian noise, σ = O(1/d) for adversarial noise and σ = O(1/ log d) for weakly correlated noises, which matches our theoretical findings in Sec. 2 up to logarithmic terms. Our simulation results and explicit construction of an “adversarial” noise matrix also suggests that our analysis for robust tensor power method with random initializations under random Gaussian noise and existing analysis for worst-case noise in [1] are tight."
    }, {
      "heading" : "B Comparison with whitening and matrix SVD decompositions",
      "text" : "Another popular thread of tensor decomposition techniques involve whitening and reducing the problem to a matrix SVD decomposition, which is very effective at reducing the dimensionality of\nthe problem in the k = o(d) undercomplete settings [1, 21, 28]. We show in this section that without additional side information, a standard application and analysis of tensor decomposition of whitening and matrix SVD techniques leads to worse error bounds than we established in Theorem 2.2.\nWhen only the 3rd-order tensor T is available, one common whitening approach is to randomly “marginalized out” one view of T̃:\nM(θ) := T̃(I, I,θ), θ randomly drawn on the unit d-dimensional sphere;\nand then evaluate top-k eigen-decomposition of M(θ). LetW = span(v1, · · · ,vk) be the span of the true components of T and Ŵ be the top-k eigenspace of matrix M(θ) obtained by collapsing one view of T̃. We then have the following proposition that bounds the perturbation betweenW and Ŵ: Proposition B.1. Suppose T̃ = T + ∆T as in Theorems 2.1, 2.2 and let ΠW ,ΠŴ be the projection operators ofW and Ŵ , respectively. Then with probability at least 0.9 over the random draw of θ,∥∥ΠW −ΠŴ∥∥2 ≤ Õ (√ d‖∆T ‖op λmin ) , if ‖∆T ‖op = Õ ( λmin√ d ) ,\nProof. First, we decompose M(θ) into two terms:\nM(θ) = k∑ i=1 λi(v > i θ) · viv>i + ∆T (I, I,θ).\nDefine λ̄i = λi(v>i θ) and Ē = ∆T (I, I,θ). We then have that\nM(θ) = M0 + Ē,\nwhere M0 is a d× d rank-k matrix with eigenvalues (λ̄1, · · · , λ̄k) and eigenvectors (v1, · · · ,vk), and Ē satisfies ‖Ē‖2 ≤ ‖∆T ‖op. Since θ is uniformly sampled from the d-dimensional unit sphere, by standard concentration arguments we have that |v>j θ| = Ω̃(1/ √ d) with overwhelming probability for all j = 1, · · · , k and hence σk(M0) = Ω̃(λmin/ √ d),\nwhere σk(·) denotes the kth largest singular value of a matrix. Applying Weyl’s theorem (Lemma F.7) we have that\nσk(M(θ)) ≥ σk(M0)− ‖Ē‖2 = Ω̃(λmin/ √ d),\nwhere the last inequality is due to the condition imposed on noise magnitude ‖∆T ‖op and the fact that ‖Ē‖2 ≤ ‖∆T ‖op. Applying Wedin’s theorem (Lemma F.8) with α = 0 and δ = σk(M(θ)) = Ω̃(λmin/\n√ d) we arrive at∥∥ΠW −ΠŴ∥∥2 ≤ ‖Ē‖2σk(M(θ)) ≤ Õ (√ d‖∆T ‖op λmin ) .\nThis simple result shows that the whitening trick does not trivially lead to matching noise conditions in Theorem 2.2 under k = o(d) settings."
    }, {
      "heading" : "C Proof of Theorem 2.2",
      "text" : "C.1 Proof sketch of Theorem 2.2\nIn this section we sketch the proof of Theorem 2.2. Our proof is mostly built upon the analysis in [1] for robust tensor power method. However, we borrow new ideas from [12] to substantially revise the per-iteration analysis (Lemma C.2), which subsequently results in desired relaxation of noise conditions. Some results and arguments in [1], especially those involved with absolute constants, are simplified for accessibility purposes.\nWe start with Lemma C.1 that analyzes random initializations against eigenvectors.\nLemma C.1. Fix j∗ ∈ {1, · · · , k} and η ∈ (0, 1/2). Suppose L satisfies L = Ω(k/η). Then with probability at least 1− η there exists a initialization u0 such that\nmax 1≤j≤k,j 6=j∗\n|v>j u0| ≤ 0.5|v>j∗u0| and |v>j∗u0| ≥ 1/ √ d. (8)\nRoughly speaking, Lemma C.1 shows that with L = Ω(d log d) initializations the initial vector u0 will slightly bias towards one of the directions j∗ with overwhelming probability. The lemma is a slight generalization of Lemma B.1 in [1] to the k ≤ d case and their proofs are similar. For completeness purposes we include its proof in Appendix C.2. Applying a standard boosting argument we have the following corollary, which guarantees exponentially decaying failure probabilities: Corollary C.1. For any η̃ ∈ (0, 1/2), with L = Ω(k log(1/η̃)) initializations Eq. (8) holds for at least one initialization with probability at least 1− η̃.\nThe following lemma is the key lemma that characterizes the recovery of single eigenvectors of the robust tensor power method. Lemma C.2. Suppose λ1 ≥ λ2 ≥ · · · ≥ λk ≥ 0 and assume without loss of generality that the conditions in Lemma C.1 hold with respect to j∗ = 1. Assume in addition that\n‖∆T (I,ut,ut)‖2 ≤ min { ̃t, λ1\n40 √ d\n} , |∆T (vj ,ut,ut)| ≤ min { ̃t√ k , λ1 8d } , ̃t ≤ λ1 200\nfor all t ∈ [T ] and j such that λj > 0. We then have that 3\nmax j 6=1\nλj |v>j ut| ≤ 0.5λ1|v>1 ut|, tan θ(v1,ut) ≤ 0.8 tan θ(v1,ut−1) + 8̃t/λ1. (9)\nIn addition, if θ(v1,ut) ≤ π/3 we have further that\n|v>j ut+1| |v>1 ut+1| ≤ 0.8 |v>j ut| |v>1 ut| + 8̃t λ1 √ k , ∀j > 1 and λj > 0. (10)\nCompared to existing analysis in (Propositions B.1, B.2, Lemmas B.2, B.3, B.4 in [1]), our proof in Appendix C.2 analyzes the two-phase behavior of robust tensor power method in a unified framework and is thus much cleaner. Furthermore, we borrow ideas from [12] to prove shrinkage of the tangent angle between v1 and ut, which subsequently leads to relaxed noise conditions. We also prove additional bounds regarding |v>j ut| for j > 1 to facilitate later deflation analysis. This result is used for relaxing noise conditions only and is hence not proved in previous work [1].\nFinally, we present the following lemma that analyzes the deflation step in the robust noisy power method, in which both “element-wise” and “full-vector” conditions on the deflated tensor are proved.\nLemma C.3. Let {λ̂i, v̂i}ki=1 be eigenvalue and (orthonormal) eigenvector pairs that approximates {λi,vi}ki=1 with λ1 ≥ · · · ≥ λk > 0 such that for all i ∈ [k],\n|λ̂i − λi| ≤ C , tan θ(vi,vi) ≤ min{ √ 2, C /λi} |v̂>i vj | ≤ C /(λi √ k), ∀j > i (11)\nfor some absolute constantC > 0 and error tolerance parameter > 0. Denote Ei = λ̂iv̂ ⊗3 i −λiv⊗3i as the ith reconstruction error tensor. Let δ ∈ (0, 1) be an arbitrary small constant. There exist universal constants C > 0 such that if ≤ C ′λmin/ √ k then the following holds for all t ∈ [k] and ‖u‖2 = 1:∥∥∥∥∥ t∑ i=1 Ei(I,u,u) ∥∥∥∥∥ 2 ≤ κt(u) and ∣∣∣∣ t∑ i=1 Ei(vj ,u,u)\n∣∣∣∣ ≤ κt(u)2 √k , ∀j > t, (12) where κt(u) = √ δ + C ′′ ∑t i=1 |v>i u|2 and C ′′ > 0 is a universal constant.\nWe are now ready to prove the main theorem. 3For notational simplicity, let tan θ(v1,u−1) =∞.\nProof of Theorem 2.2. We use induction to prove the theorem. For i = 1 all conditions in Lemma C.2 are satisfied with ̃t = 2 when ≤ C1λmin/ √ k for some sufficiently small constant C1 > 0. Lemma C.2 then asserts that, with L = Ω(d log d) initializations and R = Ω(log(λ1k/ )) iterations, ‖v̂1 − v1‖2 ≤ tan θ(v̂1,v1) ≤ C2 /λ1 for some universal constant C2 > 0. Furthermore,\n|λ̂1 − λ1| = ∣∣T̃(v̂1, v̂1, v̂1)− λ1∣∣ ≤ ∣∣∆T (v̂1, v̂1, v̂1)∣∣+ ∣∣T(v̂1, v̂1, v̂1)− λ1∣∣\n≤ O (\n√ k\n) + ∣∣∣∣λ1|v>1 v̂1|3 − λ1 +∑ j>1 λj |v>j v̂1|3 ∣∣∣∣\n≤ O (\n√ k\n) + ∣∣∣∣λ1 [1 +O( λ1 )] − λ1 + ∑ j>1 λjO ( 3 λ3jk 1.5 )∣∣∣∣ ≤ O( ), if ≤ C1λmin/ √ k for some sufficiently small constant C1.\nWe next prove the theorem for the case of i+ 1 assuming by induction that the theorem holds for all {λj ,vj}ij=1. In this case, the “new” noise tensor ∆̃T comes from both the original noise and also noise introduced by deflations; that is, ∆̃T = T̃ + ∑i j=1 Ei. Invoking Lemma C.3 we have that ∆̃T satisfies conditions in Lemma C.2 with\ñt = ( 1 + max{κi(ut), κi(ut)2} ) ,\nwhere κi(u) = √ δ + C ′′ ∑i j=1 |u>vj |2 as defined in Lemma C.3, provided that ≤ C1λmin/ √ k for some sufficiently small constant C1. Furthermore, note that for arbitrary δ ∈ (0, 1), we can again pick C ′1 > 0 to be a sufficiently small constant (possibly depending on δ) such that ≤ C ′1λmin/ √ k\nwould imply ̃t ≤ min{λ1/200, 0.01λmin √ δ/(C ′′k)}. Subsequently, by Eq. (9) we know that after\nΩ(log(λmaxk/ )) iterations we have that tan θ(ut,vi+1) ≤ 0.1 √ δ/(C ′′k) and hence for any j ≤ i,\n|u>t vj | = cos θ(ut,vj) = sin θ(ut,vi+1) ≤ tan θ(ut,vi+1) ≤ 0.1 √ δ/(C ′′k). Consequently,\nC ′′ ∑i j=1 |u>t vj |2 ≤ 0.01δ and therefore κi(ut) ≤ √ 1.01δ ≤ 1. We then have that ̃t ≤ 2 and hence the resuling bounds on |λ̂i+1 − λi+1| and tan θ(ut,vi+1) hold with the same constant C as all previous iterations before i. Finally, applying Lemma C.1 and taking a union bound over all k iterations we complete the proof.\nC.2 Proof of technical lemmas\nProof of Lemma C.1. Let ũ(τ)0 i.i.d.∼ Nd(0, Id×d) for τ ∈ [L] and define Zj,τ = v>j ũ (τ) 0 for j ∈ [d] and τ ∈ [L]. Without loss of generality, assume j∗ = 1. Consider the following sets of events:\nE1 := { Z : max\nτ∈[L] |Z1,τ | ≥ 0.5\n√ lnL− √ 2 ln(6/η) } , (13)\nE2,τ := { Z·,τ : max\n1<j≤k |Zj,τ | ≤\n√ 2 ln k + √ 2 ln(3/η) } , (14)\nE3,τ := Z·,τ : d∑\nj=k+1\n|Zj,τ |2 ≤ 3 ln(3/η) · d+ 2 ln(3/η)  . (15) Suppose E1 holds with τ∗ = argmaxτ |Z1,τ | and suppose in addition that E2,τ∗ and E3,τ∗ hold. To derive Eq. (8) we need to show the following inequalities:\n0.5 √ lnL− √ 2 ln(6/η) ≥ 0.5 (√ 2 ln k + √ 2 ln(3/η) ) ;\n(0.6 √ lnL− √ 2 ln(6/η))2\nk · (0.6 √ lnL− √ 2 ln(6/η))2 + 3 ln(3/η)d+ 2 ln(3/η) ≥ 1 d .\nIt can be easily verified that L = Ω(k/η) satisfies the above inequalities and hence imply Eq. (8) under E1 ∩ E2,τ∗ ∩ E3,τ∗ .\nThe rest of the proof is to lower bound the probabilities of events E1, E2,τ∗ and E3,τ∗ . We first consider E1. Because Z1,1, · · · , Z1,L\ni.i.d.∼ N (0, 1) and f(Z1,1, · · · , Z1,L) = maxτ |Z1,τ | is a 1-Lipschitz function, applying Lemma F.1 we have that\nPr [ max τ |Z1,τ | < µ− t ] ≤ 2e−t 2/2, (16)\nwhere µ = E[maxτ |Z1,τ |]. By Lemma F.2, µ ≥ E[maxτ Z1,τ ] ≥ √ lnL/ √ π ln 2 ≥ 0.5 √ lnL. Setting t = √\n2 ln(6/η) in Eq. (16) we have that Pr[E1] ≥ 1− η/3. Next, suppose E1 holds with τ∗ = argmaxτ |Z1,τ |. Note that E2,τ∗ and E3,τ∗ are independent regardless of the choice of τ∗, because Z1,τ∗ , · · · , Zd,τ∗ are independent Gaussian random variables. We can then lower bound the probabilities of E2,τ∗ and E3,τ∗ separately. We consider E2,τ∗ first. Because Z2,τ∗ , · · · , Zk,τ∗ are i.i.d. standard Normal random variables, applying Lemma F.3 we obtain\nPr [ max 2≤j≤k |Zj,τ∗ | > √ 2 ln k + √ 2t ] ≤ e−t. (17)\nPutting t = ln(3/η) in Eq. (17) we have that Pr[E2,τ∗ |E1] ≥ 1 − η/3. For E3,τ∗ , it is obvious by definition that ∑d j=k+1 |Zj,τ∗ |2 is a χ2d−k-distributed random variable and is independent of E1 and E2,τ∗ . Applying Lemma F.4 the following holds:\nPr  d∑ j=k+1 |Zj,τ∗ |2 > d+ 2 √ dt+ 2t  ≤ e−t. (18) Putting t = ln(3/η) in Eq. (18) and noting that √ d ≤ d, t ≥ 1, we conclude that Pr[E3,τ∗ |E1] ≥ 1− η/3. Finally, applying union bound we have that Pr[E1 ∩ E2,τ∗ ∩ E3,τ∗ ] ≥ 1− η.\nProof of Lemma C.2. First, as a consequence of Corollary C.1, we know that |v>1 u0| ≥ 1/ √ d. The conditions in Lemma C.2 then imply |∆T (vj ,ut,ut)| ≤ λ1|v>1 u0|2/8. We now use induction to prove Eq. (9). When t = 0 Eq. (9) trivially holds due to Lemma C.1 and the condition that j∗ = 1 corresponds to the largest eigenvalue λ1. The objective is then to prove Eq. (9) for the case of t+ 1, assuming it holds for all iterations up to t.\nWe first consider the second part of Eq. (9) concerning tan θ(v1,ut). Let V ∈ Rd×(k−1) be an orthonormal basis of the complement subspace V⊥ = span{v2, · · · ,vk}. Further let εt = ∆T (I,ut,ut). Because T(I,ut,ut) lies in the span of columns of V, we have that\ntan θ(v1,ut+1) ≤ ‖V>T(I,ut,ut)‖2 + ‖εt‖2 |v>1 [T(I,ut,ut) + εt]| ≤ ‖V >T(I,ut,ut)‖2 + ‖εt‖2 |v>1 T(I,ut,ut)| − |v>1 εt| .\nIn addition, note that\n‖V>T(I,ut,ut)‖2 = √√√√ k∑ j=2 λ2j |v>j ut|4 ≤ max j 6=1 λj |v>j ut| · √√√√ k∑ j=2 |v>j ut|2,\nwhere the first equality is due to the orthogonality of {v2, · · · ,vk} and in the last inequality we apply H’́older’s inequality. Because √∑k j=2 |v>j ut|2 = ‖V>ut‖2, we have that\ntan θ(v1,ut+1) ≤ ‖V>ut‖2 ·maxj 6=1 λj |v>j ut|+ ‖εt‖2\n|v>1 ut| · λ1|v>1 ut| − |v>1 εt|\n= tan θ(v1,ut)\n[ maxj 6=1 λj |v>j ut|+ ‖εt‖2/‖V>ut‖2\nλ1|v>1 ut| − |v>1 εt|/|v>1 ut|\n]\n≤ tan θ(v1,ut) [\n0.5λ1|v>1 ut|+ ‖εt‖2/‖V>ut‖2 λ1|v>1 ut| − |v>1 εt|/|v>1 ut|\n] (19)\n= tan θ(v1,ut)\n[ 1\n2\n1\n1− |v>1 εt|/(λ1|v>1 ut|2) ] ︸ ︷︷ ︸\nα\n+ 1\n1− |v>1 εt|/(λ1|v>1 ut|2)︸ ︷︷ ︸ 2α\n· ‖εt‖2 λ1|v>1 ut|2︸ ︷︷ ︸\nβ\n.\nHere in Line 19 we apply the induction hypothesis that maxj 6=1 λj |v>j ut| ≤ 0.5λ1|v>1 ut|. Before proceeding the analysis we first show that |v>1 u0| ≤ |v>1 ut|. Applying the induction hypothesis, we have that\ntan θ(v1,ut) ≤ 0.8t tan θ(v1,u0) + 40̃t/λ1 ≤ 0.8 tan θ(v1,u0) + 40̃t/λ1 ≤ tan θ(v1,u0), where the last inequality is due to ̃t ≤ λ1/200. Subsequently, θ(v1,ut) ≤ θ(v1,u0) and hence |v>1 ut| = cos θ(v1,ut) ≥ cos θ(v1,u0) = |v>1 u0|. Now applying |v>1 εt| ≤ |v>1 u0|2/4 we obtain\n|v>1 εt| ≤ λ1|v>1 u0|2 4 ≤ λ1|v > 1 ut|2 4 =⇒ 1 1− |v>1 εt|/(λ1|v>1 ut|2) ≤ 3 2 =⇒ α ≤ 3 4 . (20)\nNext we bound β by considering two cases. In the first case of |v>1 ut| ≤ 0.5, we have that\nβ = tan θ(v1,ut) · ‖V>εt‖2 λ1|v>1 ut| √ 1− |v>1 ut|2 ≤ 2‖εt‖2 λ1|v>1 ut| · tan θ(v1,ut) ≤ 0.05 tan θ(v1,ut).\n(21) where the last inequality is due to the condition that ‖εt‖2 ≤ λ1|v > 1 u0| 40 ≤ λ1|v>1 ut| 40 . On the other hand, if |v>1 ut| > 0.5 the following holds:\nβ = ‖εt‖2 λ1|v>1 ut|2 ≤ 4‖εt‖2 λ1 ≤ 4̃t λ1 . (22)\nCombining Eq. (20,21,22) we obtain tan θ(v1,ut+1) ≤ 0.8 tan θ(v1,ut) + 8̃t/λ1.\nWe next prove the first part of Eq. (9), namely that maxj 6=1 λj |v>j ut+1| ≤ 0.5λ1|v>1 ut+1|. For those j with λj = 0 the bound trivially holds. So we consider only j > 1 with λj > 0. We then have that\nλ1|v>1 ut+1| λj |v>j ut+1| = λ1|v>1 [T(I,ut,ut) + εt]| λj |v>j [T(I,ut,ut) + εt]| ≥ ( λ1|v>1 ut| λj |v>j ut| )2 ︸ ︷︷ ︸\nα′\n· 1− |v > 1 εt|/(λ1|v>1 u2t |)\n1 + |v>j εt|/(λj |v>j εt|2)︸ ︷︷ ︸ β′\n.\nBy induction hypothesis α′ ≥ 4. Applying conditions on |v>1 εt| we get |v>1 εt| ≤ λ1|v>1 u0| 2\n4 ≤ λ1|v>1 ut| 2\n4 and hence |v > 1 εt|/(λ1|v>1 ut|2) ≤ 1/4. On the other hand,(\nλ1|v>1 ut| λj |v>j ut|\n)2 [ 1 +\n|v>j ε| λj |v>j ut|2\n]−1 = (λj |v>j ut| λ1|v>1 ut| )2 + λj |v>j εt| λ21|v>1 ut|2 −1 ≥ [1 4 + |v>j εt| λ1|v>1 ut|2 ]−1 .\nBecause |v>j εt| ≤ λ1|v>1 u0| 2 8 ≤ λ1|v>1 ut| 2\n8 , the right-hand side of the above equation is lower bounded by 8/3. Therefore, α′β′ ≥ 83 (1− 1 4 ) ≥ 2.\nThe last part of this proof is devoted to showing Eq. (10). Under the condition that θ(v1,ut) ≤ π/3 we have that cos θ(v1,ut) = |v>1 ut| ≥ 1/2. Subsequently, for arbitrary j > 1 with λj > 0 the following holds:\n|v>j ut+1| |v>1 ut+1| ≤ λj |v>j ut|2 + |v>j εt| λ1|v>1 ut|2 − |v>1 εt| ≤ |v>j ut| |v>1 ut| ·1 2\n1\n1− |v>1 εt|/(λ1|v>1 ut|2)︸ ︷︷ ︸ α\n+ |v>j εt|\nλ1|v>1 ut|2 − |v>1 εt|︸ ︷︷ ︸ γ .\nBecause |v>1 ut| ≥ 1/2 and |v>1 εt| ≤ 12λ1|v > 1 u0|2 ≤ 12λ1|v > 1 ut|2, we have γ ≤ 8|v>j εt|/λ1 and hence\n|v>j ut+1| ≤ 0.8|v>j ut|+ 8|v>j εt| λ1 ≤ 0.8|v>j ut|+ 8̃t λ1 √ k .\nProof of Lemma C.3. The first part of Eq. (12) is a simplified result of Lemma B.5 4 in [1] because ‖v̂i − vi‖2 ≤ tan θ(v̂i,vi) when ‖v̂i‖2 = ‖vi‖2 = 1 and θ < π/2. The proofs are almost identical.\n4Except that we operate under a k < d regime, which adds no difficulty to the proof.\nSo we focus on proving the second part of Eq. (12) here. Recall that v>j vi = 0 for all j > i. Subsequently, ∣∣∣∣ t∑\ni=1\nEi(vj ,u,u) ∣∣∣∣ ≤ t∑ i=1 λ̂i|u>v̂i|2|v>j v̂i| ≤ C √ k t∑ i=1 λ̂i λi |u>v̂i|2.\nDefine v̂⊥i = v̂i − (v̂ > i vi)vi as the difference between v̂i and its projection on vi. It is then by definition that ‖v̂⊥i ‖2 = ‖v̂i‖2 sin θ(v̂i,vi) ≤ tan θ(v̂i,vi). Subsequently,\nt∑ i=1 λ̂i λi |u>v̂i|2 ≤ t∑ i=1\n( 1 + |λ̂i − λi|\nλi\n) |u>v̂i|2 ≤ ( C\nλmin + 1 )[ t∑ i=1 ( |u>vi|2 + |u>v̂⊥i |2 )]\n≤ ( C\nλmin + 1\n)[ k‖v̂⊥i ‖22 + t∑ i=1 |u>vi|2 ] ≤ ( C λmin + 1 ) C2k 2\nλ2min︸ ︷︷ ︸ a +\nt∑ i=1 |u>vi|2.\nHere the second step is due to Hölder inequality and the fact that max1≤i≤k |λ̂i−λi| λi ≤ C λmin . For arbitrary δ ∈ (0, 1), set ≤ min{λminC2 , √ δ 2C3 λmin√ k } ≤ C ′λmin/ √ k we have that a ≤ δ/C, and hence the second part of Eq. (12) holds with C ′′ = C."
    }, {
      "heading" : "D Proof of results for streaming robust tensor power method",
      "text" : "Proof of Theorem 3.1. First, note that if x1, · · · ,xn i.i.d.∼ P , P ∈ SGD(σ) then the distribution of the sample mean x̄ = 1n ∑n i=1 xi belongs to SGD(σ/ √ n). To see this, fix any a ∈ RD and one can show that\nE [ exp(a>x̄) ] = n∏ i=1 E [ exp(a>xi/n) ] ≤ n∏ i=1 exp(‖a‖22σ2/n2) = exp(‖a‖22σ2/n),\nwhere the second inequality is due to the fact that xi ∈ SGD(σ) and ‖a/n‖22 = ‖a‖22/n2. Under Assumptions 3.1, 3.2 and using the the above arguments, we know that\nvec(∆T ) = vec\n[ 1\nn n∑ i=1 x⊗3i −T\n] ∈ SGd3(σ/n)\nNow fix vi,ut ∈ Rd with unit L2 norms. Applying Lemma F.6 with respect to Σ = vec(vi ⊗ ut ⊗ ut)vec(vi ⊗ ut ⊗ ut)> we obtain that\nPr [∣∣∆T (vi,ut,ut)∣∣2 > (1 + 2√t+ t)σ2 n ] ≤ e−t, ∀t > 0. (23)\nSubsequently, with overwhelming probability (e.g., ≥ 1− n−10) we have that\n‖∆T (I,ut,ut)‖2 = Õ ( σ √ d\nn\n) ,\n∣∣∆T (vi,ut,ut)∣∣ = Õ(σ√ 1 n ) .\nFinally, with\nn = Ω̃ ( min { σ2d\n2 , σ2d2\nλ2min }) the conditions in Eq. (4) are satisfied with overwhelming probability and hence the error bounds on |λi − λ̂π(i)| and ‖vi − v̂π(i)‖2."
    }, {
      "heading" : "E Proofs of utility results for differentially private tensor decomposition",
      "text" : "Before proving Theorem 4.2, we first present a lemma that upper bounds ‖ut‖∞ when the components V ∈ Rd×k is incoherent (Assumption 4.1) and Gaussian noise across power updates is added.\nLemma E.1. Suppose T = ∑k i=1 λiv ⊗ i 3 and V = (v1, · · · ,vk) satisfies Assumption 4.1 with coherence level µ0. Fixu ∈ Rd with ‖u‖2 = 1 and let ū = T(I,u,u)+σ·z, where z ∼ N (0, Id×d) are zero-mean independently distributed Gaussian random variables. We then have that\n‖ū‖∞ ‖ū‖2 = O\n(√ µ0k log d\nd\n) .\nwith overwhelming probability.\nProof. We prove this lemma by showing an upper bound for ‖ū‖∞ and a lower bound on ‖ū‖2, both with overwhelming probabilities. For the infinity-norm upper bound, we consider the following decomposition via triangle inequality:\n‖ū‖∞ ≤ ‖ũ‖∞ + σ‖z‖∞,\nwhere ũ = T(I,u,u) and z ∼ N (0, Id×d). By definition,\n‖ũ‖∞ = ∥∥∥∥∥ k∑ i=1 λi|u>vi|2vi ∥∥∥∥∥ ∞ = max 1≤j≤d ∣∣∣λ>(V>ej)∣∣∣, where λ is a k-dimensional vector defined as λ = (λ1|u>v1|2, · · · , λk|u>vk|2). By CauchySchwarts inequality, we have that\n‖ũ‖∞ = max 1≤j≤d ∣∣∣λ>(V>ej)∣∣∣ ≤ ‖λ‖2 · max 1≤j≤d ‖V>ej‖2 ≤ √√√√µ0k d ( k∑ i=1 λ2i |u>vi|4 ) ,\nwhere the last inequality is due to the condition that V is incoherent with coherence level µ0. In addition, ‖z‖∞ = O( √ log d) with overwhelming probability, by applying Lemma F.3. As a result,\n‖ū‖∞ ≤ √√√√2kµ0 d ( k∑ i=1 λ2i |u>vi|4 ) +O(σ √ log d). (24)\nWe next lower bound the denominator term ‖ū‖2. By definition, ū follows a multi-variate Gaussian distribution with mean ũ and co-variance σ2Id×d. Applying Lemma F.5 with µ = ‖ũ‖22/σ2 and t = O(log d) we have that ‖ū‖22 = Ω(‖ũ‖22 + σ2d) with overwhelming probability. Note also that\n‖ũ‖22 = ∥∥∥∥∥ k∑ i=1 λi|u>vi|2vi ∥∥∥∥∥ 2\n2\n= k∑ i=1 λ2i |u>vi|4\nbecause {vi}ki=1 are orthonormal vectors. Consequently,\n‖ū‖22 = Ω\n √√√√σ2d+ k∑\ni=1\nλ2i |u>vi|4  . (25) Combining Eqs. (24,25) we obtain\n‖ū‖∞ ‖ū‖2 ≤\n√ 2µ0k d ∑k i=1 λ 2 i |u>vi|4 +O(σ √ log d)\nΩ (√ σ2d+ ∑k i=1 λ 2 i |u>vi|4\n) ≤ O(√µ0k d ) +O (√ log d d ) = O (√ µ0k log d d ) .\nWe are now ready to prove Theorem 4.2.\nProof of Theorem 4.2. Applying Lemma E.1 we can with overwhelming probability upper bound the per-coordinate standard deviation of Gaussian noise calibrated in Algorithm 3:\nmax 0≤t≤T 1≤τ≤L\n{ ν‖u(τ)t ‖2∞, ν‖u (τ) t ‖3∞ } ≤ O\n(√ K · log(1/δ)\nε · µ0k log d d\n) ,\nwhere K = kL(T + 1) = Õ(k2 log(λmaxd)). Let (τ) t = E(I,u (τ) t ,u (τ) t ) = σ (τ) t · z be the noise vector calibrated, where σ(τ)t = ν‖u (τ) t ‖2∞. We then have that with overwhelming probability,\n‖ (τ)t ‖2 = O ( µ0k 2 log(λmaxd/δ)\nε √ d\n) and\n∣∣v>1 (τ)t ∣∣ = O(µ0k2 log(λmaxd/δ)εd ) .\nEquating the upper bound for |v>1 (τ) t | with O(λmin/d) we obtain the desired privacy level condition:\nε = Ω\n( µ0k 2 log(λmaxd/δ)\nλmin\n) .\nIt can also be easily verified that all noise conditions in Theorem 2.2 are satisfied with above lower bound condition on ε."
    }, {
      "heading" : "F Technical lemmas",
      "text" : "F.1 Tail inequalities\nLemma F.1 (Tail bound of Lipschitz function of Gaussian random variables, [8]). Suppose x ∼ Nd(0, σ2Id×d) are d-dimensional independent Gaussian random variables and let f : Rd → R be an L-Lipschitz function; that is, |f(x)−f(y)| ≤ L‖x−y‖2 for all x,y ∈ Rd. Suppose µ = Ex[f(x)]. Then for all t > 0, we have that\nPr [∣∣f(x)− µ∣∣ ≥ t] ≤ 2e−t2/(2σ2L2).\nLemma F.2 (Bounds on maximum of Gaussian random variables, [19]). Suppose X1, · · · , Xn i.i.d.∼ N (0, σ2) and let Y = max1≤i≤nXi. We then have that σ√ π ln 2 √ lnn ≤ E[Y ] ≤ σ √ 2 √ lnn.\nLemma F.3 (Bounds on maximum absolute values of Gaussian random variables; Theorem 3.12, [23]). Suppose X1, · · · , Xn i.i.d.∼ N (0, σ2) and let Y = max1≤i≤n |Xi|. We then have that\nPr [ Y ≥ σ √ 2 lnn+ σ √ 2t ] ≤ e−t, ∀t > 0.\nLemma F.4 (Bounds on Chi-square random variables, [22]). Suppose X ∼ χ2k; that is, X =∑k j=1 Y 2 j for i.i.d. standard Normal random variables Y1, · · · , Yk. We then have that ∀t > 0,\nPr [ X ≥ k + 2 √ kt+ 2t ] ≤ e−t, Pr [ X ≤ k − 2 √ kt ] ≤ e−t.\nLemma F.5 (Bounds on non-central Chi-square random variables, [7]). Suppose X ∼ χ2k(µ); that is, X = ∑k j=1 Y 2 k for independent Normal random variables Y1, · · · , Yk distributed as Yj ∼ N (µj , 1),∑\nj µj = µ. We then have that Pr [ X ≥ (k + µ) + 2 √ (k + 2µ)t+ 2t ] ≤ e−t,\nPr [ X ≤ (k + µ)− 2 √ (k + 2µ)t ] ≤ e−t.\nLemma F.6 (Bounds on quadratic forms of sub-Gaussian random variables, [15]). Suppose X ∼ SGD(σ) and let Σ ∈ RD×D be a positive semidefinite matrix. Then for all t > 0 we have that\nPr [ X>ΣX > σ2 ( tr(Σ) + 2 √ tr(Σ2)t+ 2‖Σ‖t )] ≤ e−t.\nF.2 Matrix perturbation lemmas\nLemma F.7 (Weyl’s theorem; Theorem 4.11, p. 204 in [26]). Let A,E be given m×n matrices with m ≥ n. Then\nmax i∈[n] ∣∣∣σi(A + E)− σi(A)∣∣∣ ≤ ‖E‖2. Lemma F.8 (Wedin’s theorem; Theorem 4.4, pp. 262 in [26]). Let A,E ∈ Rm×n be given matrices with m ≥ n. Let A have the following singular value decomposition U>1U>2\nU>3\nA [ V1 V2 ] = [ Σ1 00 Σ2 0 0 ] ,\nwhere U1,U2,U3,V1,V2 have orthonormal columns and Σ1 and Σ2 are diagonal matrices. Let"
    }, {
      "heading" : "Ã = A + E be a perturbed version of A and (Ũ1, Ũ2, Ũ3, Ṽ1, Ṽ2, Σ̃1, Σ̃2) be analogous singular",
      "text" : "value decomposition of Ã. Let Φ be the matrix of canonical angles between Range(U1) and Range(Ũ1) and Θ be the matrix of canonical angles between Range(V1) and Range(Ṽ1). If there exists α, δ > 0 such that\nmin i σi(Σ̃1) ≥ α+ δ and max i σi(Σ2) ≤ α,\nthen\nmax{‖U1U>1 − Ũ1Ũ>1 ‖2, ‖U1U>1 − Ṽ1Ṽ>1 ‖2} = max{‖ sin Φ‖2, ‖ sin Θ‖2} ≤ ‖E‖2 δ .\nF.3 Lemmas on random tensors\nLemma F.9 (Spectral norm bound of random tensors, [27]). Suppose X is a pth order tensor with dimensions d1, · · · , dp and each element of X is sampled i.i.d. from Gaussian distribution N (0, σ2). Then the following upper bound on ‖X‖op holds with probability at least (1− δ):\n‖X‖op ≤ √√√√8σ2(( p∑ k=1 dp ) ln(2K/K0) + ln(2/δ) ) ,\nwhere K0 = ln(3/2)."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2016,
    "abstractText" : "Tensor decomposition is an important tool for big data analysis. In this paper,<lb>we resolve many of the key algorithmic questions regarding robustness, memory<lb>efficiency, and differential privacy of tensor decomposition. We propose simple<lb>variants of the tensor power method which enjoy these strong properties. We present<lb>the first guarantees for online tensor power method which has a linear memory<lb>requirement. Moreover, we present a noise calibrated tensor power method with<lb>efficient privacy guarantees. At the heart of all these guarantees lies a careful<lb>perturbation analysis derived in this paper which improves up on the existing<lb>results significantly.<lb>",
    "creator" : "LaTeX with hyperref package"
  }
}