{
  "name" : "1708.09630.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Optimal Distributed Control of Multi-agent Systems in Contested Environments via Reinforcement Learning",
    "authors" : [ "Rohollah Moghadam" ],
    "emails" : [ "moghadamr@mst.edu,", "modaresh@mst.edu)." ],
    "sections" : [ {
      "heading" : null,
      "text" : "Index Terms—Reinforcement Learning, Optimal Control, Multi-agent System, Distributed Control, H∞ control.\nI. Introduction\nDistributed learning in multi-agent systems provides scalable, autonomous, flexible and efficient decision making in numerous civilian and military applications such as smart transportation, border and road patrol, space exploration, formation of aircrafts and satellites, and more [1]–[4]. Due to their networked nature, however, adversarial inputs such as disturbances or attacks on sensors and actuators can significantly degrade their performance. In a contested environment with adversarial inputs, corrupted data communicated by a single compromised agent and used by neighbors for learning can mislead the entire network to a wrong understanding of the environment and consequently cause no emergent behavior or an emergent misbehavior.\nDesign of optimal control protocols that possess an ability to learn the uncertainties online has attracted considerable attention for both single-agent and multi-agent systems. Reinforcement learning (RL) [5]–[10], inspired by learning\nRohollah Moghadam and Hamidreza Modares are with the Department of Electrical and Computer Engineering, Missouri University of Science and Technology, Emerson Hall, 301 W 16th St, Rolla, MO 65409 USA (e-mails: moghadamr@mst.edu, modaresh@mst.edu).\nManuscript received April 11, 2017\nmechanisms observed in mammals, has been successfully used to learn optimal solutions online in single agents for both regulation [11]–[13] and tracking [14]–[17] control problems and recently multi-agent systems [18]–[20]. All of the aforementioned approaches ignore the effects of the adversarial inputs on the learning and performance of the overall system. RL-based H∞ control is considered to attenuate the effect of disturbances in [21]–[27], and to mitigate attacks in [28] for single-agent systems. However, in a multi-agent system, the adverse effects of adversarial inputs are considerably more serious, because one compromised agent can degrade the performance of the entire network. Attacks on multi-agent systems have been investigated by several researchers [29]–[39]. Besides, the H∞ control of multi-agent systems is considered in [40]–[46] to attenuate the effects of disturbances on agents. However, no learning is used in these approaches. Moreover, these methods use the full knowledge of the agent’s dynamics and/or the network topology to identify and mitigate attacks or disturbances, which may not be available in a fully distributed system. Developing learning-based control protocols that can find an optimal control strategy in adversarial environments using only measured data is of utmost importance to increase autonomy for multi-agent systems.\nIn this paper, a unified resilient model-free RL-based distributed control protocol for leader-follower multi-agent systems under adversarial inputs is designed. The contributions are as follows:\n• A comprehensive analysis of the adverse effects of the adversarial inputs on the performance of the multi-agent systems is provided. It is shown that corrupted data communicated by a single compromised agent can mislead the entire network and cause no emergent behavior. A more sophisticated adversarial input can also mislead existing disturbance attenuation techniques. • A unified distributed RL-based control framework is presented for both homogeneous and heterogeneous multiagent systems so that only compromised agents will be affected by an adversarial input and their corrupted sensory data will not be used for learning by intact agents. • To attenuate the effect of adversarial inputs on compromised agents themselves, a distributed H∞ control protocol is designed. Decoupled game algebraic Riccati equations are derived for solving the proposed distributed H∞ control problem. This is in contrast to existing RLbased H∞ solutions for multi-agent systems that end up ar X\niv :1\n70 8.\n09 63\n0v 1\n[ cs\n.M A\n] 3\n1 A\nug 2\n01 7\n2 with solving coupled Riccati equations [47], [48], which are extremely difficult to solve.\n• A novel off-policy RL algorithm is developed to solve the disturbance H∞ control problem without requiring an admissible policy. This is in contrast to existing RL algorithms that require partial knowledge of the system dynamics to find an admissible policy.\nThe simulation results show the performance of the proposed control protocol.\nII. Preliminaries\nIn this section, a background of the graph theory is provided.\nA. Graph Theory\nA directed graph G consists of a pair (V,E) in which V = {v1, · · · , vN} is a set of nodes and E ⊆ V × V is a set of edges. The adjacency matrix is defined as A = [ ai j ] , with ai j > 0 if (v j, vi) ∈ E, and ai j = 0 otherwise. The set of nodes vi with edges incoming to node v j is called the neighbors of node vi, namely Ni = {v j : (v j, vi) ∈ E}. The graph Laplacian matrix is defined as L = D−A, where D = diag(di) is the indegree matrix, with di = ∑ j∈Ni ai j as the weighted in-degree of node vi. A (directed) tree is a connected digraph where every node, except the root node, has the in-degree of one. A graph is said to have a spanning tree if a subset of edges forms a directed tree. A leader can be pinned to multiple nodes, resulting in a diagonal pinning matrix G = diag (gi) ∈ RN×N with the pinning gain gi > 0 if the node has access to the leader node and gi = 0 otherwise. Finally, 1N is the N-vector of ones and Im(R) denotes the range space of R.\nAssumption 1. The communication graph has a spanning tree, and the leader is pinned to at least one root node.\nThe following theorems are used in this paper.\n1) Cayley-Hamilton Theorem [49]: The matrix exponential eAt with A ∈ Rn×n can be written by\neAt = n−1∑ k=0 µk Ak (1)\n2) Binomial Theorem [50]: For a positive integer n, one has\n(x + y)n = n∑\nk=0\nCnk x n−kyk, Cnk = n! (n − k)! k! (2)\nIII. Synchronization ofMulti-Agent Systems and Their Vulnerability to Adversarial Inputs\nIn this section, the adversarial inputs are first defined. Then, the standard synchronization control protocol is reviewed for homogeneous multi-agent systems. It is shown that the adversarial inputs launched on a single agent can snowball into a much larger and more catastrophic one.\nA. Adversarial Inputs\nThree different main sources of adversarial inputs that can affect the performance of the multi-agent systems are attacks, disturbances/noises and faults. Disturbance, noise and fault are mainly bounded signals that are caused unintentionally. Conversely, an attack can be intentionally designed to maximize the damage to the network. Attacks can be launched on agents (for example on sensors and actuators) or on the communication network. We only considered attacks on agents in this paper and, therefore, the following assumption is considered.\nAssumption 2. The communication network is secure.\nRemark 1. Although adversarial inputs on communication links are not considered here, as will be shown in the subsequent sections, our method provides resiliency to adversarial inputs on agents without removing the disrupted agents or compromising the network performance and with minimal extra cost for security. This is in contrast to existing attack mitigation methods [34], [51]–[53] in which agents discard their neighbor information based on the discrepancy between their values. This can harm the network connectivity and prevent the network synchronization.\nRemark 2. Attacks on agents are more common than attacks on the communication links. They can be launched without tampering with the physical system, i.e., spoofing a global positioning system (GPS) of a vehicle or jamming the communication channel from the controller to the actuator. Moreover, an attack on agents can be more serious than those on links. For example, if an agent is fully compromised, it can act as an illegitimate leader and all its neighbors receive corrupted information. However, if an outgoing link is under attack, then only the neighbor connected by the infected link receives disrupted information. Remark 3. Note that, in general, the attacker signal has a limited energy, and therefore, it is reasonable to assume that it is an L2-gain signal. This is a commonplace assumption in the literature [32], [54].\nB. Vulnerability of the Standard Output Synchronization approach to Adversarial Inputs\nConsider N agents with identical dynamics given by\nẋi = Axi + Bui + Dωi yi = Cxi i = 1, . . . , n\n(3)\nwhere xi(t) ∈ Rn, ui(t) ∈ Rm, ωi(t) ∈ Rd and yi(t) ∈ Rq are the state, control input, adversarial input and output of agent i, respectively. A, B, C and D are the drift, input, output and adversarial dynamics, respectively.\nAssumption 3. The system dynamic is marginally stable.\nAssumption 4. (A, B) is stabilizable.\nAssumption 5. The full state information of agents is available for feedback.\n3 Let the leader dynamics be defined as\nζ̇0 = S ζ0 y0 = Fζ0\n(4)\nwhere ζ0(t) ∈ Rn, y0(t) ∈ Rq, S ∈ Rn×n and F ∈ Rq×n. Define the local output tracking error for agent i as\nδi = yi − y0 (5)\nDefine the local neighborhood state tracking error ei ∈ Rn for agent i as [55]\nei = ∑ j∈Ni ai j(x j − xi) + gi(ζ0 − xi) (6)\nwhere gi > 0 is the pinning gain, and gi > 0 for at least one root node i. The standard distributed control protocol is then given by\nui = cKei (7)\nwhere c is a scalar coupling gain, and K is a design matrix gain. [55] shows how c and K can be designed by solving an Algebraic Riccati Equation (ARE) to assure synchronization of all agents to the leader. That is, to assure\nδi → 0⇔ yi → y0 (8)\nRemark 4. For the homogeneous leader follower multi-agent system (3) and (4), state synchronization results in output synchronization. On the other hand, the state synchronization does not make sense for heterogeneous multi-agent systems (see (67)), as the dimension of the agents states might not be the same. In this paper, a unified approach is proposed for both homogeneous and heterogeneous multi-agent systems, and therefore, the output synchronization is considered.\nRemark 5. It was shown in [55] that in the absence of the adversarial input ωi(t), if the controller ui(t) in (7) is designed to make the local neighborhood tracking error (6) zero, it guarantees that (8) is satisfied and, therefore, the synchronization problem is solved. However, in the following, we will show that if in the presence of adversarial inputs (6) goes to zero, it does not guarantee that (8) is satisfied.\nDefinition 1. In a graph, agent i is reachable from agent j if there is a directed path of any length from agent j to agent i.\nDefinition 2: An agent is called a disrupted agent, if it is directly under adversarial input. Otherwise, it is called an intact agent.\nFor simplicity, it is assumed S = A for the following Theorems 1 and 2.\nTheorem 1. (Susceptibility of Nodes to Adversarial Inputs) Consider the multi-agent system (3) under the adversarial input ωi(t). Let the control input be designed as (7) such that\nAc = IN ⊗ A − c(L + G) ⊗ (BK) (9)\nis Hurwitz, which guarantees (8) when ωi(t) = 0, ∀i = 1, . . . ,N. Then, if ω j(t) , 0 for agent j, the synchronization error is nonzero for all intact agents reachable from the disrupted agent j.\nProof. Let the synchronization error be εi = xi − ζ0. Then, the global synchronization error dynamics becomes\nε̇ = Acε + (IN ⊗ D)ω (10)\nwhere Ac is defined in (9). The solution to (10) is ε(t) = eActε(0) + ∫ t\n0 eAc(t−τ)(Dcω)dτ, (11)\nwhere Dc = (IN ⊗ D). Using (1) and the fact that eAct → 0 as t → ∞, one has\nε(t)→ ∫ t\n0 ( N−1∑ k=0 µmAkc)(Dcω)dτ (12)\nThe first term of Akc in (9) does not deal with the agent interactions as (IN ⊗ A)k = ( IN ⊗ Ak ) . For the second term of Akc, one has\n((L + G) ⊗ BK)k = (L + G)k ⊗ (BK)k (13)\nUsing (2) and (13), (12) can be written as\nε (t)→ N−1∑ k=0 k∑ m=0 ∫ t 0 µkh (k,m) (Dcω) dτ (14)\nwhere\nh(k,m) = (−1)mcmCkm(IN ⊗ Ak−m)((L + G) ⊗ BK)m\n= (−1)mcmCkm((L + G)m ⊗ Ak−m(BK)m) (15)\nThe last equality in (15) is obtained using Kronecker product property (A ⊗ B) (C ⊗ D) = (AC ⊗ BD) and (13). If m > 0 is the first integer, such that\nlmi j , [ (L + G)m ] i j , 0 (16)\nwhere []i j denotes the element (i, j) of a matrix, then agent i is reachable from agent j, and 0 < m < N − 1 is the length of the shortest directed path from j to i.\nExploiting (14) and (15), the synchronization error for the agent i can be expressed as\nεi (t)→ N∑\nj=1 N−1∑ k=0 k∑ m=1 (−1)mlmi jcmCkm ∫ t 0 µkAk−m(BK)mv jdτ\n(17)\nwhere v j = Dω j (18)\nSince agent j is under adversarial input, v j , 0. Moreover, lmi j , 0 if agent i is reachable from agent j. Therefore, it can be seen from (17) that εi(t) , 0. This completes the proof.\nLemma 1. [56] Let Σ be a diagonal matrix with at least one nonzero positive element, and L be the Laplacian matrix. Then, (L + Σ) is nonsingular.\nTheorem 2. Consider the multi-agent system (3) and (4) with the control protocol (7). Assume that agent i is under an adversarial input ωi that is generated by ω̇i = Γωi where eigenvalues of Γ are a subset of eigenvalues of A. Then, the local neighborhood tracking error (6) is nonzero for disrupted agent i, i.e., ei , 0 and is zero for all other intact agents, i.e., e j = 0∀ j, j , i.\n4 Proof. Let L̄ be the graph Laplacian matrix of the entire network including the leader as the only root node and followers as non-root nodes. Then, it can be partitioned as\nL̄ = 0 0 . . . 0−∆ L f  (19) where ∆ ∈ RN is a vector whose ith element is nonzero and indicates that the follower i is connected to the leader. L f is a matrix which indicates the interaction among leader and followers.\nThe global dynamic of the multi-agent system (3) in terms of the Laplacian matrix (19) can be written as\nẋ = (IN+1 ⊗ A)x − c(L̄ ⊗ BK)x + (IN+1 ⊗ D)ω̄ (20)\nwhere ω̄ = [0, ω1, . . . , ωN]T which 0 in ω̄ indicates that the leader is a trusted node and is not under adversarial input. Since adversarial effects are considered on agents, for simplicity, assume Di = Bi. Then, (20) turns into\nẋ = (IN+1 ⊗ A)x + (IN+1 ⊗ B) ( −c(L̄ ⊗ K)x + ω̄ ) (21)\nIt can be seen that if the second term of (21) tend to zero, i.e., ω̄ ∈ Im(c(L̄ ⊗ K)), then, the agent’s dynamics becomes ẋi → Axi, which indicates their stability. Note that, ω̄ ∈ Im(c(L̄ ⊗K)) if there exists a nonzero vector xs such that\nc(L̄ ⊗ K)xs = ω̄ (22)\nDefine xs = [xls, x f s]T , where xls and x f s are the steady states of the leader and followers, respectively.\nUsing (19), (22) becomes 0 0 . . . 0−c∆ ⊗ K cL f ⊗ K  [ xlsx f s ] = [ 0 ω ] (23)\nSince the leader is not under adversarial input, ẋls = Ax0. From (22), for the followers one has\n− c(∆ ⊗ K)xls + c(L f ⊗ K)x f s = ω (24)\nBased on Assumption 1, followers have at least one incoming link from the leader. On the other hand, L f captures the interaction between all followers as well as the incoming link from the leader. The former is a positive semi-definite Laplacian matrix and the latter can be considered as a diagonal matrix Σ with at least one nonzero positive element added to it. Therefore, as stated in Lemma 1, L f is nonsingular and consequently the solution to (24) becomes\nx f s = (c(L f ⊗ K))−1(ω + c(∆ ⊗ K)xrs) (25)\nSince dynamics of the adversarial input are subset of the system dynamics A, therefore, for every ω̄ there exists a nonzero vector xs such that (22) holds.\nNow, using (19), the global form of the state neighborhood tracking error (6) can be written as\ne = −(L̄ ⊗ In)x (26)\nSince (22) is satisfied, one has\nc(L̄ ⊗ K)xs = ω̄⇒ c(IN+1 ⊗ K)e = −ω̄ (27)\nor, equivalently cKei = −ωi (28)\nThe intact agents are not directly under adversarial inputs, i.e., ω j = 0 ∀ j , i, and, therefore, (28) implies that the local neighborhood tracking error is zero for them, i.e., e j = 0. Moreover, ωi , 0 for the disrupted agent i and (28) indicates that it’s local neighborhood tracking error is nonzero. The proof is completed.\nRemark 6. Existing H∞ disturbance attenuation techniques for multi-agent systems [40] aim to minimize the effect of the disturbance on the local neighborhood tracking error. However, as shown in Theorem 2, in the presence of a stealthy attack, the attacker can deceive agents by assuring that their local neighborhood tracking error is zero, even in the presence of the attack. Therefore, these approach do not work in this case. To overcome this, we propose a distributed control framework to prevent propagating the effects of the disrupted agent throughout the network. In this framework, the local H∞ controller is then designed to further attenuate the effect of the adversarial input on the disrupted agent.\nThe following example verifies the results of Theorem 1 and Theorem 2 and necessitates designing resilient control protocols against adversarial inputs. The adversarial input is assumed as a stealthy attack that has the knowledge of the agent dynamics.\nExample 1. Consider 5 agents communicating with each other according to the communication graph depicted in Fig. 1.\nAgents are assumed scalar by dynamics as\nẋi = ui, i = 1, . . . , 5 (29)\nwhere ui is defined in (7). In this example, the designing parameters are considered as k = 3.16 and c = 2. Let the leader dynamics be generated by\nẋ0 = 0, x0(0) = 2 (30)\nand the control protocol given by (7) be used. Assume that a constant attack signal ω2 = 5 is injected into the actuator of Agent 2 at t = 10 sec. Then, the dynamic of Agent 2 becomes\nẋ2 = u2 + ω2 (31)\nFigure 2 shows the performance of the multi-agent system (29). It can be seen that using the control protocol (7) agents\n5 0 5 10 15 20\nTime(s)\n-2\n-1\n0\n1\n2\n3\nLeader Agent 1 Compromised Agent 2 Agent 3 Agent 4 Agent 5\n(a)\n0 5 10 15 20\nTime(s)\n-10\n-5\n0\n5 e1 e2 e3 e4 e5\n(b)\n0 5 10 15 20\nTime(s)\n-1\n0\n1\n2\n3\n4 δ1 δ2 δ3 δ4 δ5\n(c)\nFig. 2: The multi-agent system (29) response when Agent 2 is under an adversarial input. (a) The agent’s output. (b) The local neighborhood tracking error (6). (c) The tracking error (5).\nsynchronize to the leader before that attacker affects Agent 2. After injecting the attack signal ω2 into Agent 2, as shown in Fig. 2a, Agents 4 and 5 that have a path to Agent 2 do not synchronize to the leader. This complies with the result of Theorem 1. The final values of agents before and after the attack are shown in red in Fig. 1. From Fig. 2b, one can see that the local neighborhood tracking error (6) is zero for all agents except the disrupted Agent 2. This is consistent with the result of Theorem 2. It can be seen from Fig. 2c that although the local neighborhood error (6) goes to zero for all intact agents in the presence of the adversarial input, it does not guarantee that the synchronization error (5) converges to zero. This is stated in Remark 5. One can assume that the attack signal is removed after a finite time, which is greater than 20 sec in Fig. 2, to satisfy this condition.\nIV. Overall Structure of the Proposed Control Protocol\nIn this section, a control protocol is proposed that prevents the adversarial inputs effects on a disrupted agent from propagating throughout the network, and attenuates its effects on the disrupted agent itself.\nAccording to Theorem 1, as shown in Fig. 3, when the standard framework is used for synchronization in distributed multi-agent systems, the corrupted data caused by an adversarial input on the physical system of one agent is directly sent to the communication network. Therefore, its effect propagates across the network and affects intact agents.\nThe leader is the only agent that cannot be affected by an adversarial input on other agents. Any adversarial input on the leader, on the other hand, affects all other agents, based on Theorem 1. In this paper, we assume that the leader is not under any adversarial input.\nBased on this observation, we develop a distributed control framework that does not allow a compromised agent to\nFig. 3: Standard synchronization control protocol under adversarial input.\npropagate its corrupted data across the network. That is, in the event of an adversarial input on a portion of agents, the intact agents still operate normally and remain unaffected. Figure 4 shows the structure of the control framework to prevent adversarial input on one agent from propagating across the network. In this framework, only the leader communicates its actual sensory information and agents do not exchange their actual state information. They estimate the leader state using a distributed observer and communicate this estimation to their neighbors to achieve consensus on the leader state. The observer cannot be physically affected by any adversarial input.\nIt can be seen from Fig. 4 that if an agent is under the adversarial input, it only affects its own dynamics, and cannot affect the distributed observer state. The corrupted state of the disrupted agent is not transmitted to other intact agents. To further increase resiliency in the local level, agents design a local H∞ controller to attenuate the effects of the adversarial input on their own dynamics.\nRemark 7. Using the proposed approach, attacks on sensors and actuators can be recovered without removing the com-\n6 promised agents, under a safe communication. However, the observer output for each agent can indeed be modified by attacks on communication links. Attacks on the communication links can be mitigated by embedding the approaches presented in [36], [54], [57] in the proposed observer. These methods, however, require restrictive connectivity assumptions on the communication network. Separating attacks on nodes from attacks on edges helps recovering from the former without removing any number of compromised nodes (which harm the network connectivity) or making any restrictive assumption on network connectivity and the number of agents under attack.\nThe H∞ controller in Fig. 4 is detailed in Fig. 5. In this figure, ri is the observer state that estimates the leader state for agent i and zi is the controlled or performance output defined such that\n‖zi‖2 = (Cxi − Fri)T Qi(Cxi − Fri) + uTi Riui (32)\nwhere the weight matrices Qi and Ri are symmetric positive definite.\nThe bounded L2-gain condition for the agent i for any ωi ∈ L2 [0,∞) can be defined as∫ ∞\n0 e −αit‖zi(t)‖2dt∫ ∞\n0 e −αit‖ωi(t)‖2dt\n6 γ2i (33)\nwhere αi is the discount factor and γi represents the attenuation level of the adversarial input ωi.\nCondition (33) is satisfied, if the H∞ norm of Tωi,zi , i.e., the transfer function from the adversarial input ωi to the performance output zi, is less than or equal to γi. The goal is now to design the control protocol (7) to satisfy (8) and (33).\nV. The Proposed Control Protocol Approach\nIn this section, a distributed control protocol is presented in a unified framework for both homogeneous and heterogeneous multi-agent systems. Resiliency of this approach to the adversarial input is shown.\nThe proposed dynamic control approach is ṙi = S ri + c  N∑ j=1 ai j(r j − ri) + gi(ζ0 − ri) \nui = Kxixi + Kriri\n(34a)\n(34b)\nwhere ri is the observer state shown in Fig. 5, c is a coupling gain, Kxi and Kri are design control gains.\nDefine the error between the observer state for agent i and the leader state as an observation error\nσi = ri − ζ0 (35)\nLemma 2. [56] Consider the dynamic observer defined in (34a). Then, σi → 0 for all agents, if c > 1/2λmin(L + G).\nProblem 1. (H∞ Output Synchronization Problem) Consider N identical systems in (3). Design a distributed dynamic controller (34) for agent i, i = 1, . . .N, such that\n1) The bounded L2-gain condition (33) is satisfied when ωi , 0. 2) The output synchronization problem is solved, i.e., ‖yi(t) − Fri(t)‖ → 0, i = 1, . . . ,N when ωi = 0.\nConsider the system dynamic (3) and the dynamic observer defined in (34a). Define the augmented system state as\nXi(t) = [ xi(t)T ri(t)T ]T ∈ R2n (36)\nUsing (3),(32) and (34) together yields the augmented system\nẊi = T Xi + B1ui + D1ωi + E1ηi Yi = C1Xi\n(37)\nwhere\nT = [\nA 0 0 S\n] , B1 = [ B 0 ] ,D1 = [ D 0 ] E1 = [ 0\ncIn\n] , C1 = [ C −F ] (38) and\nYi = Cxi(t) − Fri(t) (39)\nand\nηi = N∑ j=1 ai j(r j − ri) + gi(ζ0 − ri) (40)\nis the local neighborhood observer error for agent i. The performance output (32) in terms of the augmented state (36) becomes\n‖Zi‖2 = XTi CT1 QiC1Xi + uTi Riui = YiT QiYi + uTi Riui (41)\nUsing (34) and (37), the control protocol for augmented system can be written as\nui = [Kxi Kri] Xi , KiXi (42)\nIn the following, it is shown that Problem 1 can be solved if the control protocol (42) is designed assuming that ηi = 0 in (37). That is, the separation principle holds.\nTheorem 3. Consider the multi-agent system (3). Then, Problem 1 is solved, if the control protocol (34) is designed to guarantee that Yi in (37) approaches zero assuming ηi = 0.\nProof. We first show that Part 2 of Problem 1 is satisfied, if the condition in the statement of the theorem holds.\nConsider the augmented system (37). Let ηi = 0,∀i. Then, based on (40) one has η = ((L + G) ⊗ In)σ = 0, where σ is the global observation error vector. Since (L+G) is a positive definite, this concludes that σ = 0. To complete the proof, one\n7 needs to show that Problem 1 is solved if the control protocol ui in (34) guarantees that Yi goes to zero when σi = 0.\nUsing (4), (34) and (35), the global observer error dynamic becomes\nσ̇ = (IN ⊗ S − c(L + G) ⊗ In)σ = Aσσ (43)\nLet Yi = 0, ∀i. Then, there exists a zero-error invariant and attractive set Ω = { (x, ζ 0 ) ∣∣∣∣ x = π(ζ0)}.\nAs stated in Lemma 2, σ → 0 and therefore, based on converse Lyapunov theorem [58], there exists a smooth positive definite function V(σ) such that\nV̇σ(σ) 6 0 (44)\nDefine ê = [(y1 − Fr1)T , . . . , (yN − FrN)T ]T = [ YT1 , . . . ,Y T N ]T as the global synchronization error. That is\nê = C̄X (45)\nwhere X = [XT1 , . . . , X T N] T and C̄ = diag(C1, . . . ,C1) with C1 defined in (38). Now, consider the Lyapunov-like function V(σ, ê) = Vσ(σ) for the augmented system (37) and the global synchronization error defined in (45). Then, based on (44), one has\nV̇(σ, ê) 6 0 (46)\nBased on LaSalles invariance principle [58], as t → ∞ all trajectories of (37) and (43) converge to the largest invariant subset of points where V̇(σ, ê) = V̇σ(σ) = 0. Using (44), V̇σ(σ) = 0 if and only if σ = 0. On the other hand, it was shown that the system (37) has the invariant set Ω = { (x, ζ 0 ) ∣∣∣∣ x = π(ζ0)} when σ = 0. Since the tracking error is zero in the invariant set, the point (σ, ê) = (0, 0) is the largest invariant set for V̇(σ, ê) = 0. Since σ → 0, using a similar procedure, one can also show that the L2-gain condition (Part 1 of Problem 1) also holds, if the controller is designed as stated in the theorem. The proof is completed.\nIn the following, we show how to design the control protocol (42) to solve Problem 1 based on results of Theorem 3. We assume for the following analysis that ηi = 0. Based on (33), define the discounted performance function in terms of the augmented system (37) as\nJ(Xi, ui, ωi) = ∫ ∞\n0 e−αi(τ−t)\n[ XTi C T 1 QiC1Xi\n+ uTi Riui − γi2ωiTωi ] dτ (47)\nThe value function for linear systems is quadratic and therefore\nVi(Xi(t)) = J(Xi, ui, ωi) = Xi(t)T PiXi(t) (48)\nThe corresponding Hamiltonian function is\nH (Xi, ui, ωi) , YTi QiYi + u T i Riui − γ2i ωTi ωi − αiVi\n+ ( dVi dXi )T (T Xi + B1ui + D1ωi)\n(49)\nUsing (48) for the left-hand side of (47) and differentiating (47) along with the augmented system (38) gives the following Bellman equation\nH (Xi, ui, ωi) = (T Xi + B1ui + D1ωi)T PiXi+\nXTi Pi(T Xi + B1ui + D1ωi) − αiXTi PiXi + YTi QiYi + uTi Riui − γ2i ωTi ωi = 0\n(50)\nBy applying the stationary conditions [59] as ∂Hi/∂ui = 0, ∂Hi/∂ωi = 0, the optimal control and the worst case adversarial input are\nu∗i = −RiBT1 PiXi ω∗i = 1 γ2i DT1 PiXi (51)\nSubstituting (51) into (50), results the following tracking game ARE\nH ( Xi, u∗i , ω ∗ i ) = XTi [T T Pi + T Pi − αiPi+\nCT1 QiC1 − PiB1R−1i BT1 Pi + 1 γ2i PiD1DT1 Pi]Xi = 0 (52)\nThe following theorem shows that the control protocol (51) along with (52) solves the H∞ output synchronization problem.\nTheorem 4. Consider the multi-agent system (3) under the adversarial input ωi. Let the control protocol be defined as (34). Then, Problem 1 is solved if K∗i is designed as\nK∗i = −R−1i BT1 Pi (53)\nwhere Pi > 0, Pi = PTi is a solution of\nT T Pi + PiT − αiPi + CT1 QiC1−\nPiB1R−1i B T 1 Pi + 1 γ2i PiD1DT1 Pi = 0 (54)\nand\nαi 6 α ∗ i = 2 ∥∥∥∥∥∥∥ (B1R−1i BT1 + 1γ2i D1DT1 )(CT1 QiC1) 1/2 ∥∥∥∥∥∥∥ (55)\nwhere α∗i is the upper bound for αi.\nProof. The proof has two parts. Part1. For any Xi(t), ui(t) and ωi(t), Hamiltonian function (49) can be expressed by\nH (Xi, ui, ωi) = H(Xi, u∗i , ω ∗ i ) + (ui − u∗i )T Ri(ui − u∗i ) − γ2i ∥∥∥ωi − ω∗i ∥∥∥2 (56)\nBased on (54) and (52), H(Xi, u∗i , ω ∗ i ) = 0. Lemma 2 shows that σi in (35) and consequently ηi in (40) converges to zero. Therefore, based on Theorem 3, one needs to show that the proposed control protocol solves Problem 1 assuming that ηi = 0 in (37). Using this fact and (56), (54) and\ndVi(Xi) dt = ( ∂Vi dXi )T Ẋi ⇒\ndVi(Xi) dt = ( ∂Vi dXi )T (T Xi + B1ui + D1ωi)\n(57)\n8 (56) can be written as\ndVi (Xi) dt\n− αiVi (Xi) + XTi ( CT1 QiC1 ) X + uTi Riui\n−γ2i ωTi ωi = (ui − u∗i )T R(ui − u∗i ) − γ2i ∥∥∥ωi − ω∗i ∥∥∥2 (58)\nSelecting ui = u∗i = K ∗ i Xi, with K ∗ i defined in (53), gives\ndVi (Xi) dt − αiVi (Xi) − γ2i ωTi ωi+\nXTi ( CT1 QiC1 + (K ∗ i ) T RiK∗i ) Xi = −γ2i ∥∥∥ωi − ω∗i ∥∥∥2 6 0 (59)\nMultiplying (59) by e−αit yields\nd dt (e−αitXTi PiXi)+\ne−αit(XTi (C T 1 QiC1 + (K ∗ i ) T RiK∗i )X − γ2i ωTi ωi) 6 0 (60)\nIntegrating (60) yields\ne−αiT V[Xi(T )] − V[Xi(0)]\n+ ∫ T 0 e−αit(XTi (C T 1 QiC1 + (K ∗ i )\nT RiK∗i )X − γ2i ωTi ωi)dt 6 0 (61)\nSelecting Xi(0) = 0 and noting that nonegativity of Lyapunov function imply that e−αiT V[Xi(T )] > 0 ∀T , (61) can be written as ∫ T\n0 e−αitXTi (C T 1 QiC1 + (K ∗ i ) T RiK∗i )Xidt 6\nγ2i ∫ T 0 e−αitωTi ωidt (62)\nfor all T . This gives the value function (47) and therefore L2-gain condition (33) and completes the proof of Part 1.\nPart 2. For the poof of Part 2, we first show that A + BKxi is Hurwitz. Define\nPi = [ Pi1 P i 2\nPi2 P i 3\n] (63)\nUsing (38) for the upper left hand side of the discounted game ARE (54), one has\nAT Pi1 + P i 1A − αiPi1 + CT QiC−\nPi1BR −1 i B T Pi + 1 γ2i Pi1DD T Pi1 = 0\n(64)\nand the control gain Kxi becomes\nKxi = −R−1i BT Pi1 (65)\nIt is shown in [21] that if (55) is satisfied, then A + BKxi is Hurwitz. Multiplying left and right hand side of (54) by XTi and Xi, respectively, one has\n2XTi (PiXi) − αiXTi (PiXi) + XTi CT1 QiC1Xi− (PiXi)T B1R−1i BT1 + 1γ2i D1DT1  (PiXi) = 0 (66) The rest of the proof is similar to [60], and therefore, is\nomitted.\nRemark 8. The control protocol in Theorem 4 can be also extended to solving Problem 1 for heterogeneous multi-agent systems. Consider the heterogeneous multi-agent system as\nẋi = Aixi + Biui + Diωi yi = Cixi i = 1, . . . , n\n(67)\nwhere xi(t) ∈ Rni , ui(t) ∈ Rmi , ωi(t) ∈ Rdi and yi(t) ∈ Rq are the state, control input, adversarial input and output of agent i, respectively. In the case of heterogeneous systems, the ARE in (54) and, consequently, the control gain (53) are different from one agent to another. That is, (54) and (53) in Theorem 4 become\nK∗i = −R−1i BT1iPi (68)\nT Ti Pi + PiTi − αiPi + CT1iQiC1i − PiB1iR−1i BT1iPi\n+ 1 γ2i PiD1iDT1iPi = 0 (69)\nwhere Ti = [\nAi 0 0 S\n] , B1i = [ Bi 0 ] ,D1i = [ Di 0 ] ,C1i = [ Ci −F ] and\nαi 6 α ∗ i = 2 ∥∥∥∥∥∥∥ (B1iR−1i BT1i + 1γ2i D1iDT1i)(CT1iQiC1i) 1/2 ∥∥∥∥∥∥∥ (70)\nRemark 9. The advantage of the standard control protocol (7) is that it does not require the absolute state of each agent and only uses the relative state information of agents. However, as shown in this paper, it is vulnerable to adversarial inputs. Moreover, designing the control protocol (7) in an optimal manner by minimizing the performance function [48] as\nJ = ∫ ∞\n0\n N∑\ni=1 i−1∑ j qi j[xi(t) − x j(t)]2 + N∑ i=1 riu2i (t)  (71) ends up with coupled AREs which are extremely difficult to solve, even if the complete knowledge of agents is known. By contrast, we show that the proposed control protocol is resilient to adversarial inputs and decouples the AREs for each agent. Moreover, we will show that the proposed control method does not need any knowledge of the agents dynamics.\nVI. Model Free Off-Policy RL for Solving Optimal Output Synchronization\nIn this section, an RL algorithm is proposed to solve Problem 1 online and without requiring any knowledge of the agents dynamics.\nThe off-policy RL allows to separate the behavior policy from the target policy for both control input and adversarial input. This brings the opportunity to design an independent adaptive controller as a behavior policy to generate data for learning of the target policy while assuring stability during learning without requiring any initial admissible policy. We leverage this and use an adaptive controller as a behavior policy as shown in Fig. 6. This controller does not need\n9 to have any knowledge of the agents dynamics and can generate data used for target policy to learn the optimal solution. The learning has two stages. In the first stage, the adaptive controller is applied to the system to generate data for learning. In the second stage, the target policy with a controller approximated from the adaptive controller on its convergence and reuse of the data obtained in the first stage to update as many policies as required until the optimal solution is found.\nA. Adaptive Controller Structure (Behavior Policy)\nIn this subsection, an adaptive controller for RL is presented. Existing RL methods require an initial admissible policy. However, to obtain an admissible policy, one needs to know partial knowledge of the system dynamics. By integrating the adaptive controller approach and off-policy RL here, no initial control policy is required.\nThe initial admissible policy for RL should satisfy the stability condition when ωi = 0. Using this fact, consider the agent dynamic (3) becomes\nẋi = Axi + Bui (72)\nDefine the tracking error as\nei = xi − ri (73)\nwhere ri is the observer state. The error dynamics is\nėi = ẋi − ṙi = Axi + Bui − ṙi. (74)\nDefine Lyapunov function as\nVi (ei) = 1 2 eTi Piei (75)\nThe defined Lyapunov function is radially unbounded such that\n∂Vi (ei) ∂t = ∂Vi (ei) ∂ei (Axi + Bui − ṙi) 6 0\n⇒ eTi Pi (Axi + Bui − ṙi) 6 0 (76)\nNow, without loss of generality, the unknown terms in (76) are estimated using a linear weight neural networks as f (ei, xi) = eTi PiAxi = W ∗T ai S a(ei, xi) g (ei) = eTi PiB = W ∗T bi S b(ei)\nc (ei) = eTi Pi = W ∗T ci S c(ei)\n(77)\nwhere f (ei, xi), g(ei), and c(ei) are the outputs of the neural network, xi and ei are its inputs. Wai, Wbi, and Wci are synaptic\nweights vectors and S a, S b, and S c are regressor matrices. The regressors are considered as sigmoid activated functions.\nTo determine the adaptive laws for the neural network weights, consider Lyapunov function candidate for the system (72) as\nLi = kiVi (ei) + 1 2 ∣∣∣W̃ai∣∣∣2 + 12 ∣∣∣W̃bi∣∣∣2 + 12 ∣∣∣W̃ci∣∣∣2 (78) where Vi (ei) is defined in (75) and W̃ai, W̃bi and W̃ci are the neural network weights errors as W̃ai = Wai−W∗ai, W̃bi = Wbi− W∗bi, and W̃ci = Wci − W∗ci where W∗ai, W∗bi, and W∗ci are the optimal values of the neural network weights.\nDifferentiating (78) with respect to time, one has\nL̇i = kiV̇i(ei) + W̃TaiẆai + W̃ T biẆbi + W̃ T ciẆci = kW∗Tai S a(ei, xi) + kiW ∗T bi S b(ei)ui − kiW∗Tci S c(ei)ṙi +W̃TaiẆai + W̃ T biẆbi + W̃ T ciẆci\n(79)\nSimilar to [61], one can show that L̇i 6 0 which indicates that the tracking error (73) converges to zero, if we design the following adaptation laws to update the neural network weights  Ẇai = −kaiWai + kiS a(ei, xi) Ẇbi = −kbiWbi + kiS b(ei)ui Ẇci = −kciWci − kiS c(ei)ṙi (80)\nand the control law as ui = − WTai S a(ei, xi) −WTci S c(ei)ṙi + τi |ei|∣∣∣WTbi S b(ei)∣∣∣2  (WTbi S b(ei))T (81) where τi is positive and bounded and is given by\nτi |ei| ki kai ∣∣∣W∗ai∣∣∣2 + kikbi ∣∣∣W∗bi∣∣∣2 + kikci ∣∣∣W∗ci∣∣∣2 It is shown in [61] that the control law (81) with updating laws (80) guarantee the uniform ultimate boundedness of the error ei. Moreover, [61] suggested a resetting method to keep away the value of\n∣∣∣WTbi S b(ei)∣∣∣ from zero. Not that one can use either off-policy policy iteration or offpolicy value iteration to solve the optimal control problem in hand. In both cases, during learning, the system needs to be stable to collect meaningful data for learning. That is, although value iteration does not require an admissible target policy, if the behavior policy is not admissible, the system can grow unbounded in a short time and learning will not take place. If off-policy policy iteration is used, one can approximate an appropriate initial admissible control gain from (81) and using least square to start the target policy with.\nB. Model-free off-policy RL for solving optimal output regulation\nIn order to find the optimal gain (54) without the requirement of the knowledge of the system dynamics, offpolicy RL algorithm [21] is used in this subsection. Offpolicy algorithm has two separate stages. In the first stage, the adaptive controller in previous section is applied to the system and the system information is recorded over the time interval T . Then, in the second stage, without requiring any\n10\nknowledge of the system dynamics, the information gathered in stage 1 is repeatedly used to find a sequence of updated policies uki and d k i converging to u ∗ i and d ∗ i . To this end, the system dynamics (50) is first written as\nẊi = T kXi + B1 ( ui − uki ) + D1 ( di − dki ) (82)\nwhere T k = T + B1Kki −DiKkwi. The uki = Kki Xi and dki = KkwiXi are the control and disturbance target policies, which evaluated and updated. The Bellman equation becomes\ne−αiδtXi(t + δt)T Pki Xi (t + δt) − Xi(t)T Pki Xi (t)\n= ∫ t+δt t d dτ ( e−αi(τ−t)XiT Pki Xi ) dτ\n= − ∫ t+δt\nt e−αi(τ−t)XTi\n( Q̃ki ) Xidτ\n−2 ∫ t+δt\nt e−αi(τ−t)\n( ui − Kki Xi )T RiKk+1i Xidτ\n+2 ∫ t+δt\nt e−α(τ−t)\n( di − KkwiXi )T γ2i K k+1 wi Xidτ\n(83)\nwhere Q̃ki = Q̄i + K T i RiKi−γi2KwiT Kwi. Note that uki and dki are not applied to the system. The behavior control policy ui is the control input which is applied to the system and the actual adversarial input di comes from an external source such as an attacker and is not under our control. The initial target policy can be approximated from (81) after its convergence. Note that (83) is a scalar equation, and can be solved using least-square method after collecting enough number of data samples from the system.\nThe model-free Algorithm 1 uses the Bellman equation (83) to solve the ARE equation (55) simultaneously and find the gain (54). In Algorithm 1, the control policy which is applied to the systems, i.e., ui, can be a fixed stabilizing policy. The data which is gathered by applying this fixed policy to the system is then used in (84) to find the matrix Pki and the improved policy uk+1i = K k+1 i Xi and the disturbance policy dk+1i = K k+1 wi Xi. This corresponds to an updated new policy ui = KiXi.\nVII. Simulation Results\nIn this section, two examples are provided to verify the effectiveness of the proposed control protocol. The communication graph shown in Fig. 1 is used.\nA. Example1.(Homogeneous multi-agent system)\nConsider 5 agents with dynamics given by ẋi = [\n0 −4 1 0\n] xi + [ 1 0 ] ui + [ 1 0 ] ωi,\nyi = [1 0] xi , i = 1, ..., 5 (85)\nThe leader dynamics is ẋ0 = [\n0 −4 1 0\n] x0, y0 = [1 0] x0 (86)\nAlgorithm 1. Off-Policy RL Solution for H∞ Optimal Output Synchronization Problem\nStage 1 (Data Gathering using Adaptive Controller): The adaptive controller is applied to the system to generate data for learning and collect required system information as state, control input and disturbance at N different sampling interval T .\nStage 2 (Reuse the gathered data sequentially to find an optimal policy iteratively): Given uki and d k i , use the collected information in Stage1 to solve the following Bellman equation for Pki and K k+1 i and K k+1 wi simultaneously.\ne−αiδtXi(t + δt)T Pki Xi (t + δt) − Xi(t)T Pki Xi (t) = − ∫ t+δt\nt e−αi(τ−t)XTi\n( Q̃ki ) Xidτ\n−2 ∫ t+δt\nt e−αi(τ−t)\n( ui − Kki Xi )T RiKk+1i Xidτ\n+2 ∫ t+δt\nt e−α(τ−t)\n( di − KkwiXi )T γ2i K k+1 wi Xidτ\n(84)\nStop if convergence is achieved, otherwise set k = k + 1 and go to Stage 2.\nUsing (85) and (34), the augmented system dynamics (37) becomes\nẊi =  0 −4 0 0 1 0 0 0 0 0 0 −4 0 0 1 0  Xi +  1 0 0 0  ui +  1 0 0 0 ωi +  0 0 0 0 c 0 0 c  ηi Yi = [ 1 0 −1 0 ] Xi\n(87) The design parameters are Qi = 100, Ri = 1, αi = 0.1 and, γi = 10 for all agents. The offline solution to the game ARE (54) and consequently the optimal control policy (51) are\nP∗i =  9.95 −0.47 −9.95 0.47 −0.47 35.17 0.47 −35.17 −9.95 0.47 9.95 −0.47 0.47 −35.17 −0.47 35.17  u∗i = [ 9.95 −0.47 −9.95 0.47 ] Xi\n(88)\nNote that the drift dynamics of the leader is considered the same as that of the followers for the standard approach to be considered as a benchmark. The proposed method, however, handles both heterogeneous and homogeneous systems and does not need the drift dynamics of the agents be equal to that of the leader. Figure 7 shows that without any adversarial inputs, agents converge to the leader, if the standard control protocol (7) is applied.\nNow, assume that Agent 2 is affected by an adversarial input defined as\nω2 = { t cos(5t) t 30 0 otherwise\n(89)\nThe agents’ outputs is shown in Fig. 8, when the standard distributed controller is used. It is observed that the intact agents which have a path to the compromised agent do not synchronize to the leader. These results are consistent with Theorem 1.\nThe initial admissible policy required for the behavior policy stage is calculated using the control law (81) and the adaptive\n11\nlaws (80). The design parameters are considered as ki = 10, Ka = kb = 0.2, τi = 0.4.\nWe now implement the off-policy RL Algorithm 1 using the initial control policy provided by the adaptive controller (81). The reinforcement interval is selected as T = 0.1. As can be seen in Fig. 9, the control gain Ki and matrix Pi converge to their optimal values.\nFig. 10 shows the results for the case that the proposed controller in (34) is applied to the system, in the presence of the same adversarial input. One can see that the compromised agent is the only one that does not follow the leader. Moreover, the H∞ controller attenuates the effect of the adversarial input on the disrupted agent which can be seen by comparing the deviation level of the compromised agent state from its desired value in Figs. 8 and 10.\nNow, consider the adversarial input (89) has a limited energy and it’s effect is over on t = 70 sec. In Fig. 11 the\nvulnerability of the standard controller and the performance of the proposed approach are shown. Although after removing the adversarial input agents converge to the leader, the effects of the adversarial input for a short period of time on the network performance cannot be neglected.\nB. Example2.(Heterogeneous multi-agent system)\nConsider a group of the heterogeneous followers with dynamics as A1,3,5 = [ −0.3 −2 0.1 −0.2 ] , B1,3,5 = [ 1.8 0.9 ] C1,3,5 = [ −0.1 1.2\n]  A2,4 =  0 1 0 0 0 1 1 0 −2  , B2,4 =  6 0 1  C2,4 = [ 1 0 0 ] (90)\n12\nAssume the leader dynamic as S = [\n0 1 −1 0\n] , F = [ 1 0 ] (91)\nThe standard control protocol proposed in [62] is used to solve the synchronization problem for heterogeneous multiagent systems as  ui = Kiziżi = Ḡ1izi + Ḡ2ieiv (92) where\neiv =  yi − y0, gi , 0∑ j∈Ni 1 |Ni| (yi − y j)\nḠ1i = [\nAi + BiKxi − L̄iCi BiKzi 0 G1\n] , Ḡ2i = [ L̄i G2 ] (93)\nwhere |Ni| is the cardinality of the set Ni, L̄i is designed to make Ai − L̄iCi Hurwitz and Ki = [Kxi Kzi] is designed to stabilize [\nAi 0 G2Ci G1\n] + [ Bi 0 ] [ Kxi Kzi ] .\nDefine 1-copy internal model for the leader dynamic (91) as G1 = [\n0 −2 0.5 0\n] , G2 = [ 0 1 ] (94)\nThe design parameters for followers, i = 1, 3, 5 are considered as\nKi = [ 0.72 −9.21 2.61 −15.07 ]\nḠi1 =  1.8385 −28.7771 4.6907 −27.1298 1.0818 −12.538 2.3453 −13.5649\n0 0 0 −2 0 0 0.5 0  Ḡi2 =  8.5\n3.375 0 1\n , L̄i =  1.5 0.5 1  and for i = 2, 4 as\nKi = [ −0.78 −0.42 −0.13 −0.0021 −0.98 ]\nḠi1 =  −6.1664 −1.5449 −0.8018 −0.0126 −5.8713 −0.5 0 1.0 0 0 −0.7777 −0.4241 −2.1336 −0.0021 −0.9786\n0 0 0 0 −2.0 0 0 0 −0.5 0\n\nḠi2 =  1.5 0.5 1.0 0\n1.0\n , L̄i =  1.5 0.5 1  The initial conditions are selected randomly. Figure 12 shows the synchronization of all agents to the leader when there is no adversarial input on agents.\nNow, assume that Agent 2 is under an adversarial input as (89). Figure 13 shows the network performance when Agent 2 is under an adversarial input. It is shown that Agents 4 and 6 that have a path to the compromised agent 2 can not converge to the leader. This complies with the result of Theorem 1.\nNow, let the control protocol (34) be used. The control gain Ki can be obtained using (64) and (65). The design parameters are considered as γi = 10, αi = 0.1 and Qi = 100 for all agents. The control gain Ki and the ARE solution Pi for i = 1, 3, 5 are\nP1,3,5 =  0.097 −1.128 1.02 0.0093 −1.128 15.44 −12.99 −1.20\n1.02 −12.99 13.966 0.737 0.0093 −1.20 0.737 3.386  K1,3,5 = [ 0.8406 −11.8661 9.8559 1.0660 ] (95)\nand for i = 2, 4 one has\nP2,4 =  1.666 0.020 −0.0032 −1.667 −0.32 0.020 0.1098 0.5030 0.0086 0.0455 −0.0032 0.5030 0.0245 0.0148 0.0291 −1.667 0.0086 0.0148 1.955 0.0293 −0.32 0.0455 0.0291 0.0293 0.3184  K2,4 = [ −9.993 −0.1716 −0.0052 9.9887 0.1637\n] (96)\n13\nFigure 14 shows the system performance without any adversarial inputs using the control gains in (95) and (96) which complies with the result of Theorem 4.\nConsider Agent 2 is under the adversarial input (89). It can be seen from Fig. 15 that only the disrupted agent does not converge to the leader. Moreover, the proposed control protocol has attenuated the effects of the adversarial input (89) on the disrupted agent.\nNow, the RL Algorithm 1 is utilized to calculate the control gain. The process of finding the initial admissible policy is as same as Example 1. In Fig. 16 and Fig. 17 the convergence of the RL parameters for both two group of agents are shown.\nVIII. Conclusion It is first shown that existing standard synchronization control protocols are prone to adversarial inputs. A unified\nresilient model-free reinforcement learning based distributed H∞ controller for leader-follower multi-agent systems is then presented for both homogeneous and heterogeneous multiagent systems under adversarial inputs. The effect of adversarial inputs on compromised agents is attenuated using a local H∞ controller. An off-policy RL algorithm is developed to learn the solutions of the game algebraic Riccati equations arising from solving the H∞ control protocol. No knowledge of the agents dynamics are required and it is shown that the proposed RL-based H∞ control protocol is resilient against adversarial inputs. In this paper, we assume that the full state information of agents is available and thus the state-feedback control protocol is used. The future work is to develop a resilient output-feedback learning solutions to distributed control problems, which requires taking into account the qsparse observability of agents [63]–[65]. Moreover, novel even-triggering [66], [67] based control protocols will be designed to mitigate attacks on the communication networks.\nReferences\n[1] R. Olfati-Saber, J. A. Fax, and R. M. Murray, “Consensus and cooperation in networked multi-agent systems,” Proceedings of the IEEE, vol. 95, pp. 215–233, Jan 2007. [2] A. Jadbabaie, J. Lin, and A. S. Morse, “Coordination of groups of mobile autonomous agents using nearest neighbor rules,” IEEE Transactions on Automatic Control, vol. 48, pp. 988–1001, June 2003. [3] J. Fax and R. Murray, “Information flow and cooperative control of vehicle formations,” IEEE Transactions on Automatic Control, vol. 49, no. 9, pp. 1465–1476, 2004. cited By 2455. [4] W. Ren, R. W. Beard, and E. M. Atkins, “Information consensus in multivehicle cooperative control,” IEEE Control Systems, vol. 27, pp. 71–82, April 2007. [5] R. S. Sutton and A. G. Barto, Reinforcement learning: An introduction, vol. 1(1). MIT press Cambridge, 1998. [6] W. B. Powell, Approximate Dynamic Programming: Solving the curses of dimensionality, vol. 703. John Wiley & Sons, 2007. [7] W. T. Miller, R. S. Sutton, and P. J. Werbos, A Menu of Designs for Reinforcement Learning Over Time, pp. 67–95. MIT Press, 1995. [8] F. L. Lewis, D. Vrabie, and K. G. Vamvoudakis, “Reinforcement learning and feedback control: Using natural decision methods to design optimal adaptive controllers,” IEEE Control Systems, vol. 32, no. 6, pp. 76–105, 2012. [9] F. Y. Wang, H. Zhang, and D. Liu, “Adaptive dynamic programming: An introduction,” IEEE Computational Intelligence Magazine, vol. 4, pp. 39–47, May 2009. [10] K. Doya, “Reinforcement learning in continuous time and space,” Neural computation, vol. 12, no. 1, pp. 219–245, 2000. [11] D. Liu and Q. Wei, “Policy iteration adaptive dynamic programming algorithm for discrete-time nonlinear systems,” IEEE Transactions on Neural Networks and Learning Systems, vol. 25, pp. 621–634, March 2014.\n14\n[12] D. Liu, D. Wang, and H. Li, “Decentralized stabilization for a class of continuous-time nonlinear interconnected systems using online learning optimal control approach,” IEEE Transactions on Neural Networks and Learning Systems, vol. 25, pp. 418–428, Feb 2014. [13] Y. Zhu, D. Zhao, and X. Li, “Iterative adaptive dynamic programming for solving unknown nonlinear zero-sum game based on online data,” IEEE Transactions on Neural Networks and Learning Systems, vol. 28, pp. 714–725, March 2017. [14] B. Kiumarsi, F. L. Lewis, H. Modares, A. Karimpour, and M.-b. NaghibiSistani, “Reinforcement q-learning for optimal tracking control of linear discrete-time systems with unknown dynamics,” Automatica, vol. 50, no. 4, pp. 1167–1175, 2014. [15] Z. Ni, H. He, and J. Wen, “Adaptive learning in tracking control based on the dual critic network design,” IEEE Transactions on Neural Networks and Learning Systems, vol. 24, pp. 913–928, June 2013. [16] Y. Zhu, D. Zhao, and X. Li, “Using reinforcement learning techniques to solve continuous-time non-linear optimal tracking problem without system dynamics,” IET Control Theory Applications, vol. 10, no. 12, pp. 1339–1347, 2016. [17] B. Kiumarsi and F. L. Lewis, “Actor-critic-based optimal tracking for partially unknown nonlinear discrete-time systems,” IEEE Transactions on Neural Networks and Learning Systems, vol. 26, no. 1, pp. 140–151, 2015. [18] K. G. Vamvoudakis and F. L. Lewis, “Online solution of nonlinear two-player zero-sum games using synchronous policy iteration,” International Journal of Robust and Nonlinear Control, vol. 22, no. 13, pp. 1460–1483, 2012. [19] M. I. Abouheaf and F. L. Lewis, “Multi-agent differential graphical games: Nash online adaptive learning solutions,” in 52nd IEEE Conference on Decision and Control, pp. 5803–5809, Dec 2013. [20] L. Buoniu, R. B. hatska, and B. D. Schutter, “A comprehensive survey of multiagent reinforcement learning,” IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews), vol. 38, pp. 156–172, March 2008. [21] H. Modares, F. L. Lewis, and Z. P. Jiang, “H∞ tracking control of completely unknown continuous-time systems via off-policy reinforcement learning,” IEEE Transactions on Neural Networks and Learning Systems, vol. 26, pp. 2550–2562, Oct 2015. [22] B. Luo, H. N. Wu, and T. Huang, “Off-policy reinforcement learning for h∞ control design,” IEEE Transactions on Cybernetics, vol. 45, pp. 65– 76, Jan 2015. [23] H. Zhang, Q. Wei, and D. Liu, “An iterative adaptive dynamic programming method for solving a class of nonlinear zero-sum differential games,” Automatica, vol. 47, no. 1, pp. 207–214, 2011. [24] H. Modares, F. L. Lewis, and M.-B. N. Sistani, “Online solution of nonquadratic two-player zero-sum games arising in the h control of constrained input systems,” International Journal of Adaptive Control and Signal Processing, vol. 28, no. 3-5, pp. 232–254, 2014. [25] H. N. Wu and B. Luo, “Neural network based online simultaneous policy update algorithm for solving the hji equation in nonlinear control,” IEEE Transactions on Neural Networks and Learning Systems, vol. 23, pp. 1884–1895, Dec 2012. [26] D. Vrabie and F. Lewis, “Adaptive dynamic programming for online solution of a zero-sum differential game,” Journal of Control Theory and Applications, vol. 9, no. 3, pp. 353–360, 2011. [27] H. Li, D. Liu, and D. Wang, “Integral reinforcement learning for linear continuous-time zero-sum games with completely unknown dynamics,” IEEE Transactions on Automation Science and Engineering, vol. 11, pp. 706–714, July 2014. [28] K. G. Vamvoudakis, J. P. Hespanha, B. Sinopoli, and Y. Mo, “Detection in adversarial environments,” IEEE Transactions on Automatic Control, vol. 59, pp. 3209–3223, Dec 2014. [29] K. G. Vamvoudakis, L. R. G. Carrillo, and J. P. Hespanha, “Learning consensus in adversarial environments,” in SPIE Defense, Security, and Sensing, pp. 87410K–87410K, International Society for Optics and Photonics, 2013. [30] S. Amin, A. A. Cárdenas, and S. S. Sastry, “Safe and secure networked control systems under denial-of-service attacks,” in International Workshop on Hybrid Systems: Computation and Control, pp. 31–45, Springer, 2009. [31] A. Teixeira, I. Shames, H. Sandberg, and K. H. Johansson, “A secure control framework for resource-limited adversaries,” Automatica, vol. 51, pp. 135–148, 2015. [32] Z. Feng, G. Hu, and G. Wen, “Distributed consensus tracking for multiagent systems under two types of attacks,” International Journal of Robust and Nonlinear Control, vol. 26, no. 5, pp. 896–918, 2016. [33] S. Amin, G. A. Schwartz, and S. S. Sastry, “Security of interdependent and identical networked control systems,” Automatica, vol. 49, no. 1, pp. 186–192, 2013. [34] F. Pasqualetti, A. Bicchi, and F. Bullo, “Consensus computation in unreliable networks: A system theoretic approach,” IEEE Transactions on Automatic Control, vol. 57, pp. 90–104, Jan 2012. [35] F. Pasqualetti, F. Drfler, and F. Bullo, “Attack detection and identification in cyber-physical systems,” IEEE Transactions on Automatic Control, vol. 58, pp. 2715–2729, Nov 2013. [36] S. Sundaram and C. N. Hadjicostis, “Distributed function calculation via linear iterative strategies in the presence of malicious agents,” IEEE Transactions on Automatic Control, vol. 56, no. 7, pp. 1495–1508, 2011. [37] Y. Mo, R. Chabukswar, and B. Sinopoli, “Detecting integrity attacks on scada systems,” IEEE Transactions on Control Systems Technology, vol. 22, pp. 1396–1407, July 2014. [38] H. He and J. Yan, “Cyber-physical attacks and defences in the smart grid: a survey,” IET Cyber-Physical Systems: Theory & Applications, vol. 1, no. 1, pp. 13–27, 2016. [39] Y. Wu and X. He, “Secure consensus control for multiagent systems with attacks and communication delays,” IEEE/CAA Journal of Automatica Sinica, vol. 4, pp. 136–142, Jan 2017. [40] Q. Jiao, H. Modares, F. L. Lewis, S. Xu, and L. Xie, “Distributed l2-gain output-feedback control of homogeneous and heterogeneous systems,” Automatica, vol. 71, pp. 361–368, 2016. [41] J. Qin, Q. Ma, W. X. Zheng, H. Gao, and Y. Kang, “Robust h-infinity group consensus for interacting clusters of integrator agents,” IEEE Transactions on Automatic Control, vol. PP, no. 99, pp. 1–1, 2017. [42] H. Hong, W. Yu, G. Wen, and X. Yu, “Distributed robust fixedtime consensus for nonlinear and disturbed multiagent systems,” IEEE Transactions on Systems, Man, and Cybernetics: Systems, vol. PP, no. 99, pp. 1–10, 2016. [43] I. Saboori and K. Khorasani, “Consensus achievement of multi-agent systems with directed and switching topology networks,” IEEE Transactions on Automatic Control, vol. 59, pp. 3104–3109, Nov 2014. [44] J. Wang, Z. Duan, Z. Li, and G. Wen, “Distributed h and h 2 consensus control in directed networks,” IET Control Theory & Applications, vol. 8, no. 3, pp. 193–201, 2013. [45] Z. Li, Z. Duan, and G. Chen, “On h∞ and h2 performance regions of multi-agent systems,” Automatica, vol. 47, no. 4, pp. 797–803, 2011. [46] P. Lin, Y. Jia, and L. Li, “Distributed robust h consensus control in directed networks of agents with time-delay,” Systems & Control Letters, vol. 57, no. 8, pp. 643–653, 2008. [47] Q. Jiao, H. Modares, S. Xu, F. L. Lewis, and K. G. Vamvoudakis, “Disturbance rejection of multi-agent systems: A reinforcement learning differential game approach,” in 2015 American Control Conference (ACC), pp. 737–742, July 2015. [48] Y. Cao and W. Ren, “Optimal linear-consensus algorithms: An lqr perspective,” IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics), vol. 40, pp. 819–830, June 2010. [49] G. Birkhoff and S. Lane, A Survey of Modern Algebra. AKP classics, Taylor & Francis, 1977. [50] M. Abramowitz and I. A. Stegun, Handbook of mathematical functions: with formulas, graphs, and mathematical tables, vol. 55. Courier Corporation, 1964. [51] J. R. Klotz, A. Parikh, T. H. Cheng, and W. E. Dixon, “Decentralized synchronization of uncertain nonlinear systems with a reputation algorithm,” IEEE Transactions on Control of Network Systems, vol. PP, no. 99, pp. 1–1, 2016. [52] H. J. LeBlanc, H. Zhang, X. Koutsoukos, and S. Sundaram, “Resilient asymptotic consensus in robust networks,” IEEE Journal on Selected Areas in Communications, vol. 31, pp. 766–781, April 2013. [53] Z. Feng and G. Hu, “Distributed secure average consensus for linear multi-agent systems under dos attacks,” in 2017 American Control Conference (ACC), pp. 2261–2266, May 2017. [54] H. J. LeBlanc and X. D. Koutsoukos, “Resilient synchronization in robust networked multi-agent systems,” in Proceedings of the 16th International Conference on Hybrid Systems: Computation and Control, HSCC ’13, (New York, NY, USA), pp. 21–30, ACM, 2013. [55] H. Zhang, F. L. Lewis, and A. Das, “Optimal design for synchronization of cooperative systems: State feedback, observer and output feedback,” IEEE Transactions on Automatic Control, vol. 56, pp. 1948–1952, Aug 2011. [56] F. L. Lewis, H. Zhang, K. Hengster-Movric, and A. Das, Cooperative control of multi-agent systems: optimal and adaptive design approaches. Springer Science & Business Media, 2013.\n15\n[57] W. Zeng and M. Y. Chow, “Resilient distributed control in the presence of misbehaving agents in networked control systems,” IEEE Transactions on Cybernetics, vol. 44, pp. 2038–2049, Nov 2014. [58] A. Isidori, Nonlinear control systems. Springer Science & Business Media, 2013. [59] F. L. Lewis, D. Veabie, and V. L. Syrmos, Optimal control. John Wiley & Sons, 2012. [60] H. Modares, S. P. Nageshrao, G. A. D. Lopes, R. Babuška, and F. L. Lewis, “Optimal model-free output synchronization of heterogeneous systems using off-policy reinforcement learning,” Automatica, vol. 71, pp. 334–341, 2016. [61] G. A. Rovithakis, “Stable adaptive neuro-control design via lyapunov function derivative estimation,” Automatica, vol. 37, no. 8, pp. 1213– 1221, 2001. [62] X. Wang, Y. Hong, J. Huang, and Z. P. Jiang, “A distributed control approach to a robust output regulation problem for multi-agent linear systems,” IEEE Transactions on Automatic Control, vol. 55, pp. 2891– 2895, Dec 2010. [63] Y. Shoukry and P. Tabuada, “Event-triggered projected luenberger observer for linear systems under sparse sensor attacks,” in 53rd IEEE Conference on Decision and Control, pp. 3548–3553, Dec 2014. [64] C. Lee, H. Shim, and Y. Eun, “Secure and robust state estimation under sensor attacks, measurement noises, and process disturbances: Observerbased combinatorial approach,” in 2015 European Control Conference (ECC), pp. 1872–1877, July 2015. [65] Y. Shoukry, P. Nuzzo, A. Puggelli, A. L. Sangiovanni-Vincentelli, S. A. Seshia, and P. Tabuada, “Secure state estimation for cyber physical systems under sensor attacks: A satisfiability modulo theory approach,” IEEE Transactions on Automatic Control, vol. PP, no. 99, pp. 1–1, 2017. [66] Y. Shoukry and P. Tabuada, “Event-triggered state observers for sparse sensor noise/attacks,” IEEE Transactions on Automatic Control, vol. 61, pp. 2079–2091, Aug 2016. [67] W. P. M. H. Heemels, K. H. Johansson, and P. Tabuada, “An introduction to event-triggered and self-triggered control,” in 2012 IEEE 51st IEEE Conference on Decision and Control (CDC), pp. 3270–3285, Dec 2012."
    } ],
    "references" : [ {
      "title" : "Consensus and cooperation in networked multi-agent systems",
      "author" : [ "R. Olfati-Saber", "J.A. Fax", "R.M. Murray" ],
      "venue" : "Proceedings of the IEEE, vol. 95, pp. 215–233, Jan 2007.",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Coordination of groups of mobile autonomous agents using nearest neighbor rules",
      "author" : [ "A. Jadbabaie", "J. Lin", "A.S. Morse" ],
      "venue" : "IEEE Transactions on Automatic Control, vol. 48, pp. 988–1001, June 2003.",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Information flow and cooperative control of vehicle formations",
      "author" : [ "J. Fax", "R. Murray" ],
      "venue" : "IEEE Transactions on Automatic Control, vol. 49, no. 9, pp. 1465–1476, 2004. cited By 2455.",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Information consensus in multivehicle cooperative control",
      "author" : [ "W. Ren", "R.W. Beard", "E.M. Atkins" ],
      "venue" : "IEEE Control Systems, vol. 27, pp. 71–82, April 2007.",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Approximate Dynamic Programming: Solving the curses of dimensionality, vol. 703",
      "author" : [ "W.B. Powell" ],
      "venue" : null,
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2007
    }, {
      "title" : "A Menu of Designs for Reinforcement Learning Over Time, pp. 67–95",
      "author" : [ "W.T. Miller", "R.S. Sutton", "P.J. Werbos" ],
      "venue" : null,
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 1995
    }, {
      "title" : "Reinforcement learning and feedback control: Using natural decision methods to design optimal adaptive controllers",
      "author" : [ "F.L. Lewis", "D. Vrabie", "K.G. Vamvoudakis" ],
      "venue" : "IEEE Control Systems, vol. 32, no. 6, pp. 76–105, 2012.",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Adaptive dynamic programming: An introduction",
      "author" : [ "F.Y. Wang", "H. Zhang", "D. Liu" ],
      "venue" : "IEEE Computational Intelligence Magazine, vol. 4, pp. 39–47, May 2009.",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Reinforcement learning in continuous time and space",
      "author" : [ "K. Doya" ],
      "venue" : "Neural computation, vol. 12, no. 1, pp. 219–245, 2000.",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "Policy iteration adaptive dynamic programming algorithm for discrete-time nonlinear systems",
      "author" : [ "D. Liu", "Q. Wei" ],
      "venue" : "IEEE Transactions on Neural Networks and Learning Systems, vol. 25, pp. 621–634, March 2014.  14",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Decentralized stabilization for a class of continuous-time nonlinear interconnected systems using online learning optimal control approach",
      "author" : [ "D. Liu", "D. Wang", "H. Li" ],
      "venue" : "IEEE Transactions on Neural Networks and Learning Systems, vol. 25, pp. 418–428, Feb 2014.",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Iterative adaptive dynamic programming for solving unknown nonlinear zero-sum game based on online data",
      "author" : [ "Y. Zhu", "D. Zhao", "X. Li" ],
      "venue" : "IEEE Transactions on Neural Networks and Learning Systems, vol. 28, pp. 714–725, March 2017.",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2017
    }, {
      "title" : "Reinforcement q-learning for optimal tracking control of linear discrete-time systems with unknown dynamics",
      "author" : [ "B. Kiumarsi", "F.L. Lewis", "H. Modares", "A. Karimpour", "M.-b. Naghibi- Sistani" ],
      "venue" : "Automatica, vol. 50, no. 4, pp. 1167–1175, 2014.",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Adaptive learning in tracking control based on the dual critic network design",
      "author" : [ "Z. Ni", "H. He", "J. Wen" ],
      "venue" : "IEEE Transactions on Neural Networks and Learning Systems, vol. 24, pp. 913–928, June 2013.",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Using reinforcement learning techniques to solve continuous-time non-linear optimal tracking problem without system dynamics",
      "author" : [ "Y. Zhu", "D. Zhao", "X. Li" ],
      "venue" : "IET Control Theory Applications, vol. 10, no. 12, pp. 1339–1347, 2016.",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Actor-critic-based optimal tracking for partially unknown nonlinear discrete-time systems",
      "author" : [ "B. Kiumarsi", "F.L. Lewis" ],
      "venue" : "IEEE Transactions on Neural Networks and Learning Systems, vol. 26, no. 1, pp. 140–151, 2015.",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Online solution of nonlinear two-player zero-sum games using synchronous policy iteration",
      "author" : [ "K.G. Vamvoudakis", "F.L. Lewis" ],
      "venue" : "International Journal of Robust and Nonlinear Control, vol. 22, no. 13, pp. 1460–1483, 2012.",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Multi-agent differential graphical games: Nash online adaptive learning solutions",
      "author" : [ "M.I. Abouheaf", "F.L. Lewis" ],
      "venue" : "52nd IEEE Conference on Decision and Control, pp. 5803–5809, Dec 2013.",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "A comprehensive survey of multiagent reinforcement learning",
      "author" : [ "L. Buoniu", "R.B. hatska", "B.D. Schutter" ],
      "venue" : "IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews), vol. 38, pp. 156–172, March 2008.",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "H∞ tracking control of completely unknown continuous-time systems via off-policy reinforcement learning",
      "author" : [ "H. Modares", "F.L. Lewis", "Z.P. Jiang" ],
      "venue" : "IEEE Transactions on Neural Networks and Learning Systems, vol. 26, pp. 2550–2562, Oct 2015.",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Off-policy reinforcement learning for h∞ control design",
      "author" : [ "B. Luo", "H.N. Wu", "T. Huang" ],
      "venue" : "IEEE Transactions on Cybernetics, vol. 45, pp. 65– 76, Jan 2015.",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "An iterative adaptive dynamic programming method for solving a class of nonlinear zero-sum differential games",
      "author" : [ "H. Zhang", "Q. Wei", "D. Liu" ],
      "venue" : "Automatica, vol. 47, no. 1, pp. 207–214, 2011.",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Online solution of nonquadratic two-player zero-sum games arising in the h control of constrained input systems",
      "author" : [ "H. Modares", "F.L. Lewis", "M.-B.N. Sistani" ],
      "venue" : "International Journal of Adaptive Control and Signal Processing, vol. 28, no. 3-5, pp. 232–254, 2014.",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Neural network based online simultaneous policy update algorithm for solving the hji equation in nonlinear control",
      "author" : [ "H.N. Wu", "B. Luo" ],
      "venue" : "IEEE Transactions on Neural Networks and Learning Systems, vol. 23, pp. 1884–1895, Dec 2012.",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 1884
    }, {
      "title" : "Adaptive dynamic programming for online solution of a zero-sum differential game",
      "author" : [ "D. Vrabie", "F. Lewis" ],
      "venue" : "Journal of Control Theory and Applications, vol. 9, no. 3, pp. 353–360, 2011.",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Integral reinforcement learning for linear continuous-time zero-sum games with completely unknown dynamics",
      "author" : [ "H. Li", "D. Liu", "D. Wang" ],
      "venue" : "IEEE Transactions on Automation Science and Engineering, vol. 11, pp. 706–714, July 2014.",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Detection in adversarial environments",
      "author" : [ "K.G. Vamvoudakis", "J.P. Hespanha", "B. Sinopoli", "Y. Mo" ],
      "venue" : "IEEE Transactions on Automatic Control, vol. 59, pp. 3209–3223, Dec 2014.",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Learning consensus in adversarial environments",
      "author" : [ "K.G. Vamvoudakis", "L.R.G. Carrillo", "J.P. Hespanha" ],
      "venue" : "SPIE Defense, Security, and Sensing, pp. 87410K–87410K, International Society for Optics and Photonics, 2013.",
      "citeRegEx" : "29",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Safe and secure networked control systems under denial-of-service attacks",
      "author" : [ "S. Amin", "A.A. Cárdenas", "S.S. Sastry" ],
      "venue" : "International Workshop on Hybrid Systems: Computation and Control, pp. 31–45, Springer, 2009.",
      "citeRegEx" : "30",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "A secure control framework for resource-limited adversaries",
      "author" : [ "A. Teixeira", "I. Shames", "H. Sandberg", "K.H. Johansson" ],
      "venue" : "Automatica, vol. 51, pp. 135–148, 2015.",
      "citeRegEx" : "31",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Distributed consensus tracking for multiagent systems under two types of attacks",
      "author" : [ "Z. Feng", "G. Hu", "G. Wen" ],
      "venue" : "International Journal of Robust and Nonlinear Control, vol. 26, no. 5, pp. 896–918, 2016.",
      "citeRegEx" : "32",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Security of interdependent and identical networked control systems",
      "author" : [ "S. Amin", "G.A. Schwartz", "S.S. Sastry" ],
      "venue" : "Automatica, vol. 49, no. 1, pp. 186–192, 2013.",
      "citeRegEx" : "33",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Consensus computation in unreliable networks: A system theoretic approach",
      "author" : [ "F. Pasqualetti", "A. Bicchi", "F. Bullo" ],
      "venue" : "IEEE Transactions on Automatic Control, vol. 57, pp. 90–104, Jan 2012.",
      "citeRegEx" : "34",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Attack detection and identification in cyber-physical systems",
      "author" : [ "F. Pasqualetti", "F. Drfler", "F. Bullo" ],
      "venue" : "IEEE Transactions on Automatic Control, vol. 58, pp. 2715–2729, Nov 2013.",
      "citeRegEx" : "35",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Distributed function calculation via linear iterative strategies in the presence of malicious agents",
      "author" : [ "S. Sundaram", "C.N. Hadjicostis" ],
      "venue" : "IEEE Transactions on Automatic Control, vol. 56, no. 7, pp. 1495–1508, 2011.",
      "citeRegEx" : "36",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Detecting integrity attacks on scada systems",
      "author" : [ "Y. Mo", "R. Chabukswar", "B. Sinopoli" ],
      "venue" : "IEEE Transactions on Control Systems Technology, vol. 22, pp. 1396–1407, July 2014.",
      "citeRegEx" : "37",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Cyber-physical attacks and defences in the smart grid: a survey",
      "author" : [ "H. He", "J. Yan" ],
      "venue" : "IET Cyber-Physical Systems: Theory & Applications, vol. 1, no. 1, pp. 13–27, 2016.",
      "citeRegEx" : "38",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Secure consensus control for multiagent systems with attacks and communication delays",
      "author" : [ "Y. Wu", "X. He" ],
      "venue" : "IEEE/CAA Journal of Automatica Sinica, vol. 4, pp. 136–142, Jan 2017.",
      "citeRegEx" : "39",
      "shortCiteRegEx" : null,
      "year" : 2017
    }, {
      "title" : "Distributed l2-gain output-feedback control of homogeneous and heterogeneous systems",
      "author" : [ "Q. Jiao", "H. Modares", "F.L. Lewis", "S. Xu", "L. Xie" ],
      "venue" : "Automatica, vol. 71, pp. 361–368, 2016.",
      "citeRegEx" : "40",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Robust h-infinity group consensus for interacting clusters of integrator agents",
      "author" : [ "J. Qin", "Q. Ma", "W.X. Zheng", "H. Gao", "Y. Kang" ],
      "venue" : "IEEE Transactions on Automatic Control, vol. PP, no. 99, pp. 1–1, 2017.",
      "citeRegEx" : "41",
      "shortCiteRegEx" : null,
      "year" : 2017
    }, {
      "title" : "Distributed robust fixedtime consensus for nonlinear and disturbed multiagent systems",
      "author" : [ "H. Hong", "W. Yu", "G. Wen", "X. Yu" ],
      "venue" : "IEEE Transactions on Systems, Man, and Cybernetics: Systems, vol. PP, no. 99, pp. 1–10, 2016.",
      "citeRegEx" : "42",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Consensus achievement of multi-agent systems with directed and switching topology networks",
      "author" : [ "I. Saboori", "K. Khorasani" ],
      "venue" : "IEEE Transactions on Automatic Control, vol. 59, pp. 3104–3109, Nov 2014.",
      "citeRegEx" : "43",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Distributed h and h 2 consensus control in directed networks",
      "author" : [ "J. Wang", "Z. Duan", "Z. Li", "G. Wen" ],
      "venue" : "IET Control Theory & Applications, vol. 8, no. 3, pp. 193–201, 2013.",
      "citeRegEx" : "44",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "On h∞ and h2 performance regions of multi-agent systems",
      "author" : [ "Z. Li", "Z. Duan", "G. Chen" ],
      "venue" : "Automatica, vol. 47, no. 4, pp. 797–803, 2011.",
      "citeRegEx" : "45",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Distributed robust h consensus control in directed networks of agents with time-delay",
      "author" : [ "P. Lin", "Y. Jia", "L. Li" ],
      "venue" : "Systems & Control Letters, vol. 57, no. 8, pp. 643–653, 2008.",
      "citeRegEx" : "46",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Disturbance rejection of multi-agent systems: A reinforcement learning differential game approach",
      "author" : [ "Q. Jiao", "H. Modares", "S. Xu", "F.L. Lewis", "K.G. Vamvoudakis" ],
      "venue" : "2015 American Control Conference (ACC), pp. 737–742, July 2015.",
      "citeRegEx" : "47",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Optimal linear-consensus algorithms: An lqr perspective",
      "author" : [ "Y. Cao", "W. Ren" ],
      "venue" : "IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics), vol. 40, pp. 819–830, June 2010.",
      "citeRegEx" : "48",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "A Survey of Modern Algebra",
      "author" : [ "G. Birkhoff", "S. Lane" ],
      "venue" : "AKP classics, Taylor & Francis,",
      "citeRegEx" : "49",
      "shortCiteRegEx" : "49",
      "year" : 1977
    }, {
      "title" : "Handbook of mathematical functions: with formulas, graphs, and mathematical tables, vol. 55",
      "author" : [ "M. Abramowitz", "I.A. Stegun" ],
      "venue" : "Courier Corporation,",
      "citeRegEx" : "50",
      "shortCiteRegEx" : "50",
      "year" : 1964
    }, {
      "title" : "Decentralized synchronization of uncertain nonlinear systems with a reputation algorithm",
      "author" : [ "J.R. Klotz", "A. Parikh", "T.H. Cheng", "W.E. Dixon" ],
      "venue" : "IEEE Transactions on Control of Network Systems, vol. PP, no. 99, pp. 1–1, 2016.",
      "citeRegEx" : "51",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Resilient asymptotic consensus in robust networks",
      "author" : [ "H.J. LeBlanc", "H. Zhang", "X. Koutsoukos", "S. Sundaram" ],
      "venue" : "IEEE Journal on Selected Areas in Communications, vol. 31, pp. 766–781, April 2013.",
      "citeRegEx" : "52",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Distributed secure average consensus for linear multi-agent systems under dos attacks",
      "author" : [ "Z. Feng", "G. Hu" ],
      "venue" : "2017 American Control Conference (ACC), pp. 2261–2266, May 2017.",
      "citeRegEx" : "53",
      "shortCiteRegEx" : null,
      "year" : 2017
    }, {
      "title" : "Resilient synchronization in robust networked multi-agent systems",
      "author" : [ "H.J. LeBlanc", "X.D. Koutsoukos" ],
      "venue" : "Proceedings of the 16th International Conference on Hybrid Systems: Computation and Control, HSCC ’13, (New York, NY, USA), pp. 21–30, ACM, 2013.",
      "citeRegEx" : "54",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Optimal design for synchronization of cooperative systems: State feedback, observer and output feedback",
      "author" : [ "H. Zhang", "F.L. Lewis", "A. Das" ],
      "venue" : "IEEE Transactions on Automatic Control, vol. 56, pp. 1948–1952, Aug 2011.",
      "citeRegEx" : "55",
      "shortCiteRegEx" : null,
      "year" : 1948
    }, {
      "title" : "Cooperative control of multi-agent systems: optimal and adaptive design approaches",
      "author" : [ "F.L. Lewis", "H. Zhang", "K. Hengster-Movric", "A. Das" ],
      "venue" : "Springer Science & Business Media,",
      "citeRegEx" : "56",
      "shortCiteRegEx" : "56",
      "year" : 2013
    }, {
      "title" : "Resilient distributed control in the presence of misbehaving agents in networked control systems",
      "author" : [ "W. Zeng", "M.Y. Chow" ],
      "venue" : "IEEE Transactions on Cybernetics, vol. 44, pp. 2038–2049, Nov 2014.",
      "citeRegEx" : "57",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Nonlinear control systems",
      "author" : [ "A. Isidori" ],
      "venue" : "Springer Science & Business Media,",
      "citeRegEx" : "58",
      "shortCiteRegEx" : "58",
      "year" : 2013
    }, {
      "title" : "Optimal model-free output synchronization of heterogeneous systems using off-policy reinforcement learning",
      "author" : [ "H. Modares", "S.P. Nageshrao", "G.A.D. Lopes", "R. Babuška", "F.L. Lewis" ],
      "venue" : "Automatica, vol. 71, pp. 334–341, 2016.",
      "citeRegEx" : "60",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Stable adaptive neuro-control design via lyapunov function derivative estimation",
      "author" : [ "G.A. Rovithakis" ],
      "venue" : "Automatica, vol. 37, no. 8, pp. 1213– 1221, 2001.",
      "citeRegEx" : "61",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "A distributed control approach to a robust output regulation problem for multi-agent linear systems",
      "author" : [ "X. Wang", "Y. Hong", "J. Huang", "Z.P. Jiang" ],
      "venue" : "IEEE Transactions on Automatic Control, vol. 55, pp. 2891– 2895, Dec 2010.",
      "citeRegEx" : "62",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Event-triggered projected luenberger observer for linear systems under sparse sensor attacks",
      "author" : [ "Y. Shoukry", "P. Tabuada" ],
      "venue" : "53rd IEEE Conference on Decision and Control, pp. 3548–3553, Dec 2014.",
      "citeRegEx" : "63",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Secure and robust state estimation under sensor attacks, measurement noises, and process disturbances: Observerbased combinatorial approach",
      "author" : [ "C. Lee", "H. Shim", "Y. Eun" ],
      "venue" : "2015 European Control Conference (ECC), pp. 1872–1877, July 2015.",
      "citeRegEx" : "64",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Secure state estimation for cyber physical systems under sensor attacks: A satisfiability modulo theory approach",
      "author" : [ "Y. Shoukry", "P. Nuzzo", "A. Puggelli", "A.L. Sangiovanni-Vincentelli", "S.A. Seshia", "P. Tabuada" ],
      "venue" : "IEEE Transactions on Automatic Control, vol. PP, no. 99, pp. 1–1, 2017.",
      "citeRegEx" : "65",
      "shortCiteRegEx" : null,
      "year" : 2017
    }, {
      "title" : "Event-triggered state observers for sparse sensor noise/attacks",
      "author" : [ "Y. Shoukry", "P. Tabuada" ],
      "venue" : "IEEE Transactions on Automatic Control, vol. 61, pp. 2079–2091, Aug 2016.",
      "citeRegEx" : "66",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "An introduction to event-triggered and self-triggered control",
      "author" : [ "W.P.M.H. Heemels", "K.H. Johansson", "P. Tabuada" ],
      "venue" : "2012 IEEE 51st IEEE Conference on Decision and Control (CDC), pp. 3270–3285, Dec 2012.",
      "citeRegEx" : "67",
      "shortCiteRegEx" : null,
      "year" : 2012
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Introduction Distributed learning in multi-agent systems provides scalable, autonomous, flexible and efficient decision making in numerous civilian and military applications such as smart transportation, border and road patrol, space exploration, formation of aircrafts and satellites, and more [1]–[4].",
      "startOffset" : 295,
      "endOffset" : 298
    }, {
      "referenceID" : 3,
      "context" : "Introduction Distributed learning in multi-agent systems provides scalable, autonomous, flexible and efficient decision making in numerous civilian and military applications such as smart transportation, border and road patrol, space exploration, formation of aircrafts and satellites, and more [1]–[4].",
      "startOffset" : 299,
      "endOffset" : 302
    }, {
      "referenceID" : 8,
      "context" : "Reinforcement learning (RL) [5]–[10], inspired by learning",
      "startOffset" : 32,
      "endOffset" : 36
    }, {
      "referenceID" : 9,
      "context" : "Manuscript received April 11, 2017 mechanisms observed in mammals, has been successfully used to learn optimal solutions online in single agents for both regulation [11]–[13] and tracking [14]–[17] control problems and recently multi-agent systems [18]–[20].",
      "startOffset" : 165,
      "endOffset" : 169
    }, {
      "referenceID" : 11,
      "context" : "Manuscript received April 11, 2017 mechanisms observed in mammals, has been successfully used to learn optimal solutions online in single agents for both regulation [11]–[13] and tracking [14]–[17] control problems and recently multi-agent systems [18]–[20].",
      "startOffset" : 170,
      "endOffset" : 174
    }, {
      "referenceID" : 12,
      "context" : "Manuscript received April 11, 2017 mechanisms observed in mammals, has been successfully used to learn optimal solutions online in single agents for both regulation [11]–[13] and tracking [14]–[17] control problems and recently multi-agent systems [18]–[20].",
      "startOffset" : 188,
      "endOffset" : 192
    }, {
      "referenceID" : 15,
      "context" : "Manuscript received April 11, 2017 mechanisms observed in mammals, has been successfully used to learn optimal solutions online in single agents for both regulation [11]–[13] and tracking [14]–[17] control problems and recently multi-agent systems [18]–[20].",
      "startOffset" : 193,
      "endOffset" : 197
    }, {
      "referenceID" : 16,
      "context" : "Manuscript received April 11, 2017 mechanisms observed in mammals, has been successfully used to learn optimal solutions online in single agents for both regulation [11]–[13] and tracking [14]–[17] control problems and recently multi-agent systems [18]–[20].",
      "startOffset" : 248,
      "endOffset" : 252
    }, {
      "referenceID" : 18,
      "context" : "Manuscript received April 11, 2017 mechanisms observed in mammals, has been successfully used to learn optimal solutions online in single agents for both regulation [11]–[13] and tracking [14]–[17] control problems and recently multi-agent systems [18]–[20].",
      "startOffset" : 253,
      "endOffset" : 257
    }, {
      "referenceID" : 19,
      "context" : "RL-based H∞ control is considered to attenuate the effect of disturbances in [21]–[27], and to mitigate attacks in [28] for single-agent systems.",
      "startOffset" : 77,
      "endOffset" : 81
    }, {
      "referenceID" : 25,
      "context" : "RL-based H∞ control is considered to attenuate the effect of disturbances in [21]–[27], and to mitigate attacks in [28] for single-agent systems.",
      "startOffset" : 82,
      "endOffset" : 86
    }, {
      "referenceID" : 26,
      "context" : "RL-based H∞ control is considered to attenuate the effect of disturbances in [21]–[27], and to mitigate attacks in [28] for single-agent systems.",
      "startOffset" : 115,
      "endOffset" : 119
    }, {
      "referenceID" : 27,
      "context" : "Attacks on multi-agent systems have been investigated by several researchers [29]–[39].",
      "startOffset" : 77,
      "endOffset" : 81
    }, {
      "referenceID" : 37,
      "context" : "Attacks on multi-agent systems have been investigated by several researchers [29]–[39].",
      "startOffset" : 82,
      "endOffset" : 86
    }, {
      "referenceID" : 38,
      "context" : "Besides, the H∞ control of multi-agent systems is considered in [40]–[46] to attenuate the effects of disturbances on agents.",
      "startOffset" : 64,
      "endOffset" : 68
    }, {
      "referenceID" : 44,
      "context" : "Besides, the H∞ control of multi-agent systems is considered in [40]–[46] to attenuate the effects of disturbances on agents.",
      "startOffset" : 69,
      "endOffset" : 73
    }, {
      "referenceID" : 45,
      "context" : "with solving coupled Riccati equations [47], [48], which are extremely difficult to solve.",
      "startOffset" : 39,
      "endOffset" : 43
    }, {
      "referenceID" : 46,
      "context" : "with solving coupled Riccati equations [47], [48], which are extremely difficult to solve.",
      "startOffset" : 45,
      "endOffset" : 49
    }, {
      "referenceID" : 47,
      "context" : "1) Cayley-Hamilton Theorem [49]: The matrix exponential eAt with A ∈ Rn×n can be written by",
      "startOffset" : 27,
      "endOffset" : 31
    }, {
      "referenceID" : 48,
      "context" : "2) Binomial Theorem [50]: For a positive integer n, one has",
      "startOffset" : 20,
      "endOffset" : 24
    }, {
      "referenceID" : 32,
      "context" : "This is in contrast to existing attack mitigation methods [34], [51]–[53] in which agents discard their neighbor information based on the discrepancy between their values.",
      "startOffset" : 58,
      "endOffset" : 62
    }, {
      "referenceID" : 49,
      "context" : "This is in contrast to existing attack mitigation methods [34], [51]–[53] in which agents discard their neighbor information based on the discrepancy between their values.",
      "startOffset" : 64,
      "endOffset" : 68
    }, {
      "referenceID" : 51,
      "context" : "This is in contrast to existing attack mitigation methods [34], [51]–[53] in which agents discard their neighbor information based on the discrepancy between their values.",
      "startOffset" : 69,
      "endOffset" : 73
    }, {
      "referenceID" : 30,
      "context" : "This is a commonplace assumption in the literature [32], [54].",
      "startOffset" : 51,
      "endOffset" : 55
    }, {
      "referenceID" : 52,
      "context" : "This is a commonplace assumption in the literature [32], [54].",
      "startOffset" : 57,
      "endOffset" : 61
    }, {
      "referenceID" : 53,
      "context" : "Define the local neighborhood state tracking error ei ∈ Rn for agent i as [55]",
      "startOffset" : 74,
      "endOffset" : 78
    }, {
      "referenceID" : 53,
      "context" : "[55] shows how c and K can be designed by solving an Algebraic Riccati Equation (ARE) to assure synchronization of all agents to the leader.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 53,
      "context" : "It was shown in [55] that in the absence of the adversarial input ωi(t), if the controller ui(t) in (7) is designed to make the local neighborhood tracking error (6) zero, it guarantees that (8) is satisfied and, therefore, the synchronization problem is solved.",
      "startOffset" : 16,
      "endOffset" : 20
    }, {
      "referenceID" : 54,
      "context" : "[56] Let Σ be a diagonal matrix with at least one nonzero positive element, and L be the Laplacian matrix.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 38,
      "context" : "Existing H∞ disturbance attenuation techniques for multi-agent systems [40] aim to minimize the effect of the disturbance on the local neighborhood tracking error.",
      "startOffset" : 71,
      "endOffset" : 75
    }, {
      "referenceID" : 34,
      "context" : "Attacks on the communication links can be mitigated by embedding the approaches presented in [36], [54], [57] in the proposed observer.",
      "startOffset" : 93,
      "endOffset" : 97
    }, {
      "referenceID" : 52,
      "context" : "Attacks on the communication links can be mitigated by embedding the approaches presented in [36], [54], [57] in the proposed observer.",
      "startOffset" : 99,
      "endOffset" : 103
    }, {
      "referenceID" : 55,
      "context" : "Attacks on the communication links can be mitigated by embedding the approaches presented in [36], [54], [57] in the proposed observer.",
      "startOffset" : 105,
      "endOffset" : 109
    }, {
      "referenceID" : 54,
      "context" : "[56] Consider the dynamic observer defined in (34a).",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 56,
      "context" : "As stated in Lemma 2, σ → 0 and therefore, based on converse Lyapunov theorem [58], there exists a smooth positive definite function V(σ) such that",
      "startOffset" : 78,
      "endOffset" : 82
    }, {
      "referenceID" : 56,
      "context" : "Based on LaSalles invariance principle [58], as t → ∞ all trajectories of (37) and (43) converge to the largest invariant subset of points where V̇(σ, ê) = V̇σ(σ) = 0.",
      "startOffset" : 39,
      "endOffset" : 43
    }, {
      "referenceID" : 19,
      "context" : "It is shown in [21] that if (55) is satisfied, then A + BKxi is Hurwitz.",
      "startOffset" : 15,
      "endOffset" : 19
    }, {
      "referenceID" : 57,
      "context" : "The rest of the proof is similar to [60], and therefore, is omitted.",
      "startOffset" : 36,
      "endOffset" : 40
    }, {
      "referenceID" : 46,
      "context" : "Moreover, designing the control protocol (7) in an optimal manner by minimizing the performance function [48] as",
      "startOffset" : 105,
      "endOffset" : 109
    }, {
      "referenceID" : 58,
      "context" : "Similar to [61], one can show that L̇i 6 0 which indicates that the tracking error (73) converges to zero, if we design the following adaptation laws to update the neural network weights  Ẇai = −kaiWai + kiS a(ei, xi) Ẇbi = −kbiWbi + kiS b(ei)ui Ẇci = −kciWci − kiS c(ei)ṙi (80)",
      "startOffset" : 11,
      "endOffset" : 15
    }, {
      "referenceID" : 58,
      "context" : "It is shown in [61] that the control law (81) with updating laws (80) guarantee the uniform ultimate boundedness of the error ei.",
      "startOffset" : 15,
      "endOffset" : 19
    }, {
      "referenceID" : 58,
      "context" : "Moreover, [61] suggested a resetting method to keep away the value of ∣∣∣WT bi S b(ei)∣∣∣ from zero.",
      "startOffset" : 10,
      "endOffset" : 14
    }, {
      "referenceID" : 19,
      "context" : "Model-free off-policy RL for solving optimal output regulation In order to find the optimal gain (54) without the requirement of the knowledge of the system dynamics, offpolicy RL algorithm [21] is used in this subsection.",
      "startOffset" : 190,
      "endOffset" : 194
    }, {
      "referenceID" : 0,
      "context" : "ẋi = [ 0 −4 1 0 ] xi + [ 1 0 ] ui + [ 1 0 ] ωi,",
      "startOffset" : 23,
      "endOffset" : 30
    }, {
      "referenceID" : 0,
      "context" : "ẋi = [ 0 −4 1 0 ] xi + [ 1 0 ] ui + [ 1 0 ] ωi,",
      "startOffset" : 36,
      "endOffset" : 43
    }, {
      "referenceID" : 0,
      "context" : "yi = [1 0] xi , i = 1, .",
      "startOffset" : 5,
      "endOffset" : 10
    }, {
      "referenceID" : 0,
      "context" : "ẋ0 = [ 0 −4 1 0 ] x0, y0 = [1 0] x0 (86) Algorithm 1.",
      "startOffset" : 27,
      "endOffset" : 32
    }, {
      "referenceID" : 0,
      "context" : "C2,4 = [ 1 0 0 ] (90)",
      "startOffset" : 7,
      "endOffset" : 16
    }, {
      "referenceID" : 0,
      "context" : "S = [ 0 1 −1 0 ] , F = [ 1 0 ] (91)",
      "startOffset" : 23,
      "endOffset" : 30
    }, {
      "referenceID" : 59,
      "context" : "The standard control protocol proposed in [62] is used to solve the synchronization problem for heterogeneous multiagent systems as  ui = Kizi żi = Ḡ1izi + Ḡ2ieiv (92)",
      "startOffset" : 42,
      "endOffset" : 46
    }, {
      "referenceID" : 0,
      "context" : "5 0 ] , G2 = [ 0 1 ] (94)",
      "startOffset" : 13,
      "endOffset" : 20
    }, {
      "referenceID" : 60,
      "context" : "The future work is to develop a resilient output-feedback learning solutions to distributed control problems, which requires taking into account the qsparse observability of agents [63]–[65].",
      "startOffset" : 181,
      "endOffset" : 185
    }, {
      "referenceID" : 62,
      "context" : "The future work is to develop a resilient output-feedback learning solutions to distributed control problems, which requires taking into account the qsparse observability of agents [63]–[65].",
      "startOffset" : 186,
      "endOffset" : 190
    }, {
      "referenceID" : 63,
      "context" : "Moreover, novel even-triggering [66], [67] based control protocols will be designed to mitigate attacks on the communication networks.",
      "startOffset" : 32,
      "endOffset" : 36
    }, {
      "referenceID" : 64,
      "context" : "Moreover, novel even-triggering [66], [67] based control protocols will be designed to mitigate attacks on the communication networks.",
      "startOffset" : 38,
      "endOffset" : 42
    } ],
    "year" : 2017,
    "abstractText" : "This paper presents a model-free reinforcement learning (RL) based distributed control protocol for leaderfollower multi-agent systems. Although RL has been successfully used to learn optimal control protocols for multi-agent systems, the effects of adversarial inputs are ignored. It is shown in this paper, however, that their adverse effects can propagate across the network and impact the learning outcome of other intact agents. To alleviate this problem, a unified RL-based distributed control frameworks is developed for both homogeneous and heterogeneous multi-agent systems to prevent corrupted sensory data from propagating across the network. To this end, only the leader communicates its actual sensory information and other agents estimate the leader state using a distributed observer and communicate this estimation to their neighbors to achieve consensus on the leader state. The observer cannot be physically affected by any adversarial input. To further improve resiliency, distributed H∞ control protocols are designed to attenuate the effect of the adversarial inputs on the compromised agent itself. An off-policy RL algorithm is developed to learn the solutions of the game algebraic Riccati equations arising from solving the H∞ control problem. No knowledge of the agent’s dynamics is required and it is shown that the proposed RL-based H∞ control protocol is resilient against adversarial inputs.",
    "creator" : "LaTeX with hyperref package"
  }
}