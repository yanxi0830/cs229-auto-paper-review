{
  "name" : "1507.05952.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Optimal Testing for Properties of Distributions",
    "authors" : [ "Jayadev Acharya", "Constantinos Daskalakis" ],
    "emails" : [ "jayadev@csail.mit.edu", "costis@mit.edu", "g@csail.mit.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n50 7.\n05 95\n2v 3\n[ cs\n.D S]\n8 D\nec 2\nWe provide a general approach via which we obtain sample-optimal and computationally efficient testers for all these distribution families. At the core of our approach is an algorithm which solves the following problem: Given samples from an unknown distribution p, and a known distribution q, are p and q close in χ2-distance, or far in total variation distance? With this tool in place, we develop a general testing framework which leads to the following results:\n• Testing identity to any distribution over [n] requires Θ(√n/ε2) samples. This is optimal for the uniform distribution. This gives an alternate argument for the minimax sample complexity of testing identity (proved in [VV14]). • For all d ≥ 1 and n sufficiently large, testing whether a discrete distribution over [n]d is monotone requires an optimal Θ(nd/2/ε2) samples. The single-dimensional version of our theorem improves a long line of research starting with [BKR04], where the previous best tester required Ω( √ n log(n)/ε4) samples, while the high-dimensional version\nimproves [BFRV11], which requires Ω̃(nd− 1 2poly(1ε )) samples.\n• For all d ≥ 1, testing whether a collection of random variables over [n1]×· · ·×[nd] are independent requiresO (( ( ∏\nl nl) 1/2 +\n∑ l nl ) /ε2 ) samples. A lower bound of Ω ( ( ∏ l nl) 1/2/ε2 )\nsamples is also proved. This result extends the known results for testing independence to more than two random variables. For the special case of d = 2, when n1 = n2 = n, this improves the results of [BFF+01] to the optimal Θ(n/ε2) sample complexity. • Testing whether a discrete distribution over [n] is log-concave requires an optimal Θ(√n/ε2) samples. The same is true for testing whether a distribution has a monotone hazard rate, and testing whether it is unimodal.\nThe optimality of our testers is established by providing matching lower bounds with respect to both n and ε. Finally, a necessary building block for our testers and an important byproduct of our work are the first known computationally efficient proper learners for discrete log-concave and monotone hazard rate distributions.\n∗Supported by a grant from MITEI-Shell program. †Supported by a Sloan Foundation Fellowship, a Microsoft Research Faculty Fellowship and NSF Award CCF0953960 (CAREER) and CCF-1101491. ‡Supported by NSF Award CCF-0953960 (CAREER). Work done in part while the author was at Microsoft Research Cambridge."
    }, {
      "heading" : "1 Introduction",
      "text" : "The quintessential scientific question is whether an unknown object has some property, i.e. whether a model from a specific class fits the object’s observed behavior. If the unknown object is a probability distribution, p, to which we have sample access, we are typically asked to distinguish whether p belongs to some class C or whether it is sufficiently far from it.\nThis question has received tremendous attention in the field of statistics (see, e.g., [Fis25, LR06]), where test statistics for important properties such as the ones we consider here have been proposed. Nevertheless, the emphasis has been on asymptotic analysis, characterizing the rates of convergence of test statistics under null hypotheses, as the number of samples tends to infinity. In contrast, we wish to study the following problem in the small sample regime:\nΠ(C, ε): Given a family of distributions C, some ε > 0, and sample access to an unknown distribution p over a discrete support, how many samples are required to distinguish between p ∈ C versus dTV(p, C) > ε?\nThe problem has been studied intensely in the literature on property testing and sublinear algorithms [Gol98, Fis01, Rub06, Ron08, Can15], where the emphasis has been on characterizing the optimal tradeoff between p’s support size and the accuracy ε in the number of samples. Several results have been obtained, roughly clustering into three groups, where (i) C is the class of monotone distributions over [n], or more generally a poset [BKR04, BFRV11]; (ii) C is the class of independent, or k-wise independent distributions over a hypergrid [BFF+01, AAK+07]; and (iii) C contains a single-distribution q, and the problem becomes that of testing whether p equals q or is far from it [BFF+01, Pan08, VV14].\nWith respect to (iii), [VV14] exactly characterizes the number of samples required to test identity to each distribution q, providing a single tester matching this bound simultaneously for all q. Nevertheless, this tester and its precursors are not applicable to the composite identity testing problem that we consider. If our class C were finite, we could test against each element in the class, albeit this would not necessarily be sample optimal. If our class C were a continuum, we would need tolerant identity testers, which tend to be more expensive in terms of sample complexity [VV11], and result in substantially suboptimal testers for the classes we consider. Or we could use approaches related to generalized likelihood ratio test, but their behavior is not well-understood in our regime, and optimizing likelihood over our classes becomes computationally intense.\nOur Contributions In this paper, we obtain sample-optimal and computationally efficient testers for Π(C, ε) for the most fundamental shape restrictions to a distribution. Our contributions are the following:\n1. For a known distribution q over [n], and given samples from an unknown distribution p, we show that distinguishing the cases: (a) whether the χ2-distance between p and q is at most ε2/2, versus (b) the ℓ1 distance between p and q is at least ε, requires Θ( √ n/ε2) samples. As\na corollary, we provide a simpler argument to show that identity testing requires Θ( √ n/ε2) samples (previously shown in [VV14]).\n2. For the class C = Mdn of monotone distributions over [n]d we require an optimal Θ ( nd/2\nε2\n)\nnum-\nber of samples, where prior work requires Ω (√\nn logn ε4\n) samples for d = 1 and Ω̃ ( nd− 1 2poly (\n1 ε\n)\n)\nfor d > 1 [BKR04, BFRV11]. Our results improve the exponent of n with respect to d, shave all logarithmic factors in n, and improve the exponent of ε by at least a factor of 2.\n(a) A useful building block and interesting byproduct of our analysis is extending Birgé’s oblivious decomposition for single-dimensional monotone distributions [Bir87] to monotone distributions in d ≥ 1, and to the stronger notion of χ2-distance. See Section C.1.\n(b) Moreover, we show that O(logd n) samples suffice to learn a monotone distribution over [n]d in χ2-distance. See Lemma 5 for the precise statement.\n3. For the class C = Πd of product distributions over [n1] × · · · × [nd], our algorithm requires O (( ( ∏\nℓ nℓ) 1/2 +\n∑ ℓ nℓ ) /ε2 )\nsamples. We note that a product distribution is one where all marginals are independent, so this is equivalent to testing if a collection of random variables are all independent. In the case where nℓ’s are large, then the first term dominates, and the sample complexity is O(( ∏\nℓ nℓ) 1/2 /ε2). In particular, when d is a constant and all nℓ’s\nare equal to n, we achieve the optimal sample complexity of Θ(nd/2/ε2). To the best of our knowledge, this is the first result for d ≥ 3, and when d = 2, this improves the previously known complexity from O (\nn ε6polylog(n/ε)\n)\n[BFF+01, LRR13], significantly improving the dependence on ε and shaving all logarithmic factors.\n4. For the classes C = LCDn, C = MHRn and C = Un of log-concave, monotone-hazard-rate and unimodal distributions over [n], we require an optimal Θ (√ n\nε2\n)\nnumber of samples. Our\ntesters for LCDn and C = MHRn are to our knowledge the first for these classes for the low sample regime we are studying—see [HVK05] and its references for statistics literature on the asymptotic regime. Our tester for Un improves the dependence of the sample complexity on ε by at least a factor of 2 in the exponent, and shaves all logarithmic factors in n, compared to testers based on testing monotonicity.\n(a) A useful building block and important byproduct of our analysis are the first computationally efficient algorithms for properly learning log-concave and monotone-hazard-rate distributions, to within ε in total variation distance, from poly(1/ε) samples, independent of the domain size n. See Corollaries 5 and 7. Again, these are the first computationally efficient algorithms to our knowledge in the low sample regime. [ADLS15, CDSS14] provide algorithms for density estimation, which are non-proper, i.e. will approximate an unknown distribution from these classes with a distribution that does not belong to these classes. On the other hand, the statistics literature focuses on maximum-likelihood estimation in the asymptotic regime—see e.g. [CS10] and its references.\n5. For all the above classes we obtain matching lower bounds, showing that the sample complexity of our testers is optimal with respect to n, ε and when applicable d. See Section 10. Our lower bounds are based on extending Paninski’s lower bound for testing uniformity [Pan08].\nAt the heart of our tester lies a novel use of the χ2 statistic. Naturally, the χ2 and its related ℓ2 statistic have been used in several of the afore-cited results. We propose a new use of the χ 2 statistic enabling our optimal sample complexity. The essence of our approach is to first draw a small number of samples (independent of n for log-concave and monotone-hazard-rate distributions and only logarithmic in n for monotone and unimodal distributions) to approximate the unknown distribution p in χ2 distance. If p ∈ C, our learner is required to output a distribution q that is O(ε)-close to C in total variation and O(ε2)-close to p in χ2 distance. Then some analysis reduces our testing problem to distinguishing the following cases:\n• p and q are O(ε2)-close in χ2 distance; this case corresponds to p ∈ C.\n• p and q are Ω(ε)-far in total variation distance; this case corresponds to dTV(p, C) > ε. We draw a comparison with robust identity testing, in which one must distinguish whether p and q are c1ε-close or c2ε-far in total variation distance, for constants c2 > c1 > 0. In [VV11], Valiant and Valiant show that Ω(n/ log n) samples are required for this problem – a nearly-linear sample complexity, which may be prohibitively large in many settings. In comparison, the problem we study tests for χ2 closeness rather than total variation closeness: a relaxation of the previous problem. However, our tester demonstrates that this relaxation allows us to achieve a substantially sublinear complexity of O( √ n/ε2). On the other hand, this relaxation is still tight enough to be useful, demonstrated by our application in obtaining sample-optimal testers. We note that while the χ2 statistic for testing hypothesis is prevalent in statistics providing optimal error exponents in the large-sample regime, to the best of our knowledge, in the smallsample regime, modified-versions of the χ2 statistic have only been recently used for closenesstesting in [ADJ+12, CDVV14] and for testing uniformity of monotone distributions in [AJOT13]. In particular, [ADJ+12] design an unbiased statistic for estimating the χ2 distance between two unknown distributions.\nIn Section 4, we show that a version of the χ2 statistic, appropriately excluding certain elements of the support, is sufficiently well-concentrated to distinguish between the above cases. Moreover, the sample complexity of our algorithm is optimal for most classes. Our base tester is combined with the afore-mentioned extension of Birgé’s decomposition theorem to test monotone distributions in Section 5 (see Theorem 3 and Corollary 1), and is also used to test independence of distributions in Section 7 (see Theorem 6).\nNaturally, there are several bells and whistles that we need to add to the above skeleton to accommodate all classes of distributions that we are considering. For log-concave and monotonehazard distributions, we are unable to obtain a cheap (in terms of samples) learner that χ2approximates the unknown distribution p throughout its support. Still, we can identify a subset of the support where the χ2-approximation is tight and which captures almost all the probability mass of p. We extend our tester to accommodate excluding subsets of the support from the χ2-approximation. See Theorems 7 and 8 in Sections 8 and 9.\nFor unimodal distributions, we are even unable to identify a large enough subset of the support where the χ2 approximation is guaranteed to be tight. But we can show that there exists a light enough piece of the support (in terms of probability mass under p) that we can exclude to make the χ2 approximation tight. Given that we only use Chebyshev’s inequality to prove the concentration of the test statistic, it would seem that our lack of knowledge of the piece to exclude would involve a union bound and a corresponding increase in the required number of samples. We avoid this through a careful application of Kolmogorov’s max inequality in our setting. See Theorem 5 of Section 6.\nRelated Work. For the problems that we study in thie paper, we have provided the related works in the previous section along with our contributions. We cannot do justice to the role of shape restrictions of probability distributions in probabilistic modeling and testing. It suffices to say that the classes of distributions that we study are fundamental, motivating extensive literature on their learning and testing [BBBB72]. In the recent times, there has been work on shape restricted statistics, pioneered by Jon Wellner, and others. [JW09, BW10] study estimation of monotone and k− monotone densities, and [BJR11, SW14] study estimation of log-concave distributions.\nAs we have mentioned, statistics has focused on the asymptotic regime as the number of samples tends to infinity. Instead we are considering the low sample regime and are more stringent about the\nbehavior of our testers, requiring 2-sided guarantees. We want to accept if the unknown distribution is in our class of interest, and also reject if it is far from the class. For this problem, as discussed above, there are few results when C is a whole class of distributions. Closer related to our paper is the line of papers [BKR04, ACS10, BFRV11] for monotonicity testing, albeit these papers have sub-optimal sample complexity as discussed above. Testing independence of random variables has a long history in statisics [RS81, AK11]. The theoretical computer science community has also considered the problem of testing independence of two random variables [BFF+01, LRR13]. While our results sharpen the case where the variables are over domains of equal size, they demonstrate an interesting asymmetric upper bound when this is not the case. More recently, Acharya and Daskalakis provide optimal testers for the family of Poisson Binomial Distributions [AD15].\nFinally, contemporaneous work of Canonne et al [CDGR15a, CDGR15b] provides a generic algorithm and lower bounds for the single-dimensional families of distributions considered here. We note that their algorithm has a sample complexity which is suboptimal in both n and ε, while our algorithms are optimal. Their algorithm also extends to mixtures of these classes, though some of these extensions are not computationally efficient. They also provide a framework for proving lower bounds, giving the optimal bounds for many classes when ε is sufficiently large with respect to 1/n. In comparison, we provide these lower bounds unconditionally by modifying Paninski’s construction [Pan08] to suit the classes we consider."
    }, {
      "heading" : "2 Preliminaries",
      "text" : "We use the following probability distances in our paper.\nDefinition 1. The total variation distance between distributions p and q is defined as\ndTV(p, q) def = sup A |p(A)− q(A)| = 1 2 ‖p− q‖1.\nFor a subset of the domain, the total variation distance is defined as half of the ℓ1 distance restricted to the subset.\nDefinition 2. The χ2-distance between p and q over [n] is defined by\nχ2(p, q) def =\n∑\ni∈[n]\n(pi − qi)2 qi =\n\n\n∑\ni∈[n]\np2i qi\n\n− 1.\nDefinition 3. The Kolmogorov distance between two probability measures p and q over an ordered set (e.g., R) with cumulative density functions (CDF) Fp and Fq is defined as\ndK(p, q) def = sup x∈R |Fp(x)− Fq(x)|.\nOur paper is primarily concerned with testing against classes of distributions, defined formally as follows:\nDefinition 4. Given ε ∈ (0, 1] and sample access to a distribution p, an algorithm is said to test a class C if it has the following guarantees:\n• If p ∈ C, the algorithm outputs Accept with probability at least 2/3;\n• If dTV(p, C) ≥ ε, the algorithm outputs Reject with probability at least 2/3.\nThe Dvoretzky-Kiefer-Wolfowitz (DKW) inequality gives a generic algorithm for learning any distribution with respect to the Kolmogorov distance [DKW56].\nLemma 1. (See [DKW56],[Mas90]) Suppose we have n i.i.d. samples X1, . . . Xn from a distribution with CDF F . Let Fn(x) def = 1n ∑n i=1 1{Xi≤x} be the empirical CDF. Then Pr[dK(F,Fn) ≥ ε] ≤ 2e−2nε 2 . In particular, if n = Ω((1/ε2) · log(1/δ)), then Pr[dK(F,Fn) ≥ ε] ≤ δ.\nWe note the following useful relationships between these distances [GS02]:\nProposition 1. dK(p, q) 2 ≤ dTV(p, q)2 ≤ 14χ2(p, q).\nIn this paper, we will consider the following classes of distributions:\n• Monotone distributions over [n]d (denoted by Mdn), for which i . j implies fi ≥ fj1;\n• Unimodal distributions over [n] (denoted by Un), for which there exists an i∗ such that fi is non-decreasing for i ≤ i∗ and non-increasing for i ≥ i∗;\n• Log-concave distributions over [n] (denoted by LCDn), the sub-class of unimodal distributions for which fi−1fi+1 ≤ f2i ;\n• Monotone hazard rate (MHR) distributions over [n] (denoted by MHRn), for which i < j implies fi1−Fi ≤ fj 1−Fj .\nDefinition 5. An η-effective support of a distribution p is any set S such that p(S) ≥ 1− η.\nThe flattening of a function f over a subset S is the function f̄ such that f̄i = p(S)/|S|.\nDefinition 6. Let p be a distribution, and support I1, . . . is a partition of the domain. The flattening of p with respect to I1, . . . is the distribution p̄ which is the flattening of p over the intervals I1, . . ..\nPoisson Sampling Throughout this paper, we use the standard Poissonization approach. Instead of drawing exactly m samples from a distribution p, we first draw m′ ∼ Poisson(m), and then draw m′ samples from p. As a result, the number of times different elements in the support of p occur in the sample become independent, giving much simpler analyses. In particular, the number of times we will observe domain element i will be distributed as Poisson(mpi), independently for each i. Since Poisson(m) is tightly concentrated around m, this additional flexibility comes only at a sub-constant cost in the sample complexity with an inversely exponential in m, additive increase in the error probability."
    }, {
      "heading" : "3 Overview",
      "text" : "Our algorithm for testing a distribution p can be decomposed into three steps.\n1This definition describes monotone non-increasing distributions. By symmetry, identical results hold for monotone non-decreasing distributions.\nNear-proper learning in χ2-distance. Our first step requires a learning algorithm with very specific guarantees. In proper learning, we are given sample access to a distribution p ∈ C, where C is some class of distributions, and we wish to output q ∈ C such that p and q are close in total variation distance. In our setting, given sample access to p ∈ C, we wish to output q such that q is close to C in total variation distance, and p and q are close in χ2-distance on an effective support2 of p. From an information theoretic standpoint, this problem is harder than proper learning, since χ2distance is more restrictive than total variation distance. Nonetheless, this problem can be shown to have comparable sample complexity to proper learning for the structured classes we consider in this paper.\nComputation of distance to class. The next step is to see if the hypothesis q is close to the class C or not. Since we have an explicit description of q, this step requires no further samples from p, i.e. it is purely computational. If we find that q is far from the class C, then it must be that p 6∈ C, as otherwise the guarantees from the previous step would imply that q is close to C. Thus, if it is not, we can terminate the algorithm at this point.\nχ2-testing. At this point, the previous two steps guarantee that our distribution q is such that:\n• If p ∈ C, then p and q are close in χ2 distance on a (known) effective support of p;\n• If dTV(p, C) ≥ ε, then p and q are far in total variation distance.\nWe can distinguish between these two cases using O( √ n/ε2) samples with a simple statistical χ2-test, that we describe in Section 4.\nUsing the above three-step approach, our tester, as described in the next section, can directly test monotonicity, log-concavity, and monotone hazard rate. With an extra trick, using Kolmogorov’s max inequality, it can also test unimodality."
    }, {
      "heading" : "4 A Robust χ2-ℓ1 Identity Test",
      "text" : "Our main result in the Section is Theorem 2. As an immediate corollary, we obtain the following result on testing whether an unknown distribution is close in χ2 or far in ℓ1 distance to a known distribution. In particular, we show the following:\nTheorem 1. For a known distribution q, there exists an algorithm with sample complexity\nO( √ n/ε2)\ndistinguishes between the cases\n• χ2(p, q) < ε2/10 versus\n• ‖p − q‖ > ε2.\nwith probability at least 5/6.\nThis theorem follows from our main result of this section, stated next, slightly more generally for classes of distributions.\n2We also require the algorithm to output a description of an effective support for which this property holds. This requirement can be slightly relaxed, as we show in our results for testing unimodality.\nTheorem 2. Suppose we are given ε ∈ (0, 1], a class of probability distributions C, sample access to a distribution p over [n], and an explicit description of a distribution q with the following properties:\nProperty 1. dTV(q, C) ≤ ε2 .\nProperty 2. If p ∈ C, then χ2(p, q) ≤ ε2500 .\nThen there exists an algorithm with the following guarantees:\n• If p ∈ C, the algorithm outputs Accept with probability at least 2/3;\n• If dTV(p, C) ≥ ε, the algorithm outputs Reject with probability at least 2/3.\nThe time and sample complexity of this algorithm are O (√\nn ε2\n)\n.\nRemark 1. As stated in Theorem 2, Property 2 requires that q is O(ε2)-close in χ2-distance to p over its entire domain. For the class of monotone distributions, we are able to efficiently obtain such a q, which immediately implies sample-optimal learning algorithms for this class. However, for some classes, we cannot learn a q with such strong guarantees, and we must consider modifications to our base testing algorithm.\nFor example, for log-concave and monotone hazard rate distributions, we can obtain a distribution q and a set S with the following guarantees:\n• If p ∈ C, then χ2(pS, qS) ≤ O(ε2) and p(S) ≥ 1−O(ε);\n• If dTV(p, C) ≥ ε, then dTV(p, q) ≥ ε/2.\nIn this scenario, the tester will simply pretend the support of p and q is S, ignoring any samples and support elements in [n] \\ S. Analysis of this tester is extremely similar to what we present below. In particular, we can still show that the statistic Z will be separated in the two cases. When p ∈ C, excluding [n]\\S will only reduce Z. On the other hand, when dTV(p, C) ≥ ε, since p(S) ≥ 1−O(ε), p and q must still be far on the remaining support, and we can show that Z is still sufficiently large. Therefore, a small modification allows us to handle this case with the same sample complexity of O(\n√ n/ε2). A further modification can handle even weaker learning guarantees. We could handle the previous case because the tester “knows what we don’t know” – it can explicitly ignore the support over which we do not have a χ2-closeness guarantee. A more difficult case is when there may be a low measure interval hidden in our effective support, over which p and q have a large χ2-distance. While we may have insufficient samples to reliably identify this interval, it may still have a large effect on our statistic. A naive solution would be to consider a tester which tries all possible “guesses” for this “bad” interval, but a union bound would incur an extra logarithmic factor in the sample complexity. We manage to avoid this cost through a careful analysis involving Kolmogorov’s max inequality, maintaining the O( √ n/ε2) sample complexity even in this more difficult case.\nBeing more precise, we can handle cases where we can obtain a distribution q and a set of intervals S = {I1, . . . , Ib} with the following guarantees:\n• If p ∈ C, then p(S) ≥ 1−O(ε), p(Ij) = Θ(p(S)/b) for all j ∈ [b], and there exists a set T ⊆ [b] such that |T | ≥ b− t (for t = O(1)) and χ2(pR, qR) ≤ O(ε2), where R = ∪T Ij ;\n• If dTV(p, C) ≥ ε, then dTV(p, q) ≥ ε/2.\nThis allows us to additionally test against the class of unimodal distributions. The tester requires that an effective support is divided into several intervals of roughly equal measure. It computes our statistic over each of these intervals, and we let our statistic Z be the sum of all but the largest t of these values. In the case when p ∈ C, Z will only become smaller by performing this operation. We use Kolmogorov’s maximal inequality to show that Z remains large when dTV(p, C) ≥ ε. More details on this tester are provided in Section D.\nAlgorithm 1 Chi-squared testing algorithm\n1: Input: ε; an explicit distribution q; (Poisson) m samples from a distribution p, where Ni denotes the number of occurrences of the ith domain element. 2: A ← {i : qi ≥ ε/50n} 3: Z ← ∑i∈A (Ni−mqi)2−Ni mqi 4: if Z ≤ mε2/10 then 5: return Accept\n6: else\n7: return Reject\n8: end if\nProof of Theorem 2: Theorem 2 is proven by analyzing Algorithm 1. As shown in Section A, Z has the following mean and variance:\nE [Z] = m · ∑\ni∈A\n(pi − qi)2 qi = m · χ2(pA, qA) (1)\nVar [Z] = ∑\ni∈A\n[\n2 p2i q2i + 4m · pi · (pi − qi) 2 q2i ]\n(2)\nwhere by pA and qA we denote respectively the vectors p and q restricted to the coordinates in A, and we slightly abuse notation when we write χ2(pA, qA), as these do not then correspond to probability distributions.\nLemma 2 demonstrates the separation in the means of the statistic Z in the two cases of interest, i.e., p ∈ C versus dTV(p, C) ≥ ε, and Lemma 3 shows the separation in the variances in the two cases. These two results are proved in Section B.\nLemma 2. If p ∈ C, then E [Z] ≤ 1500mε2. If dTV(p, C) ≥ ε, then E [Z] ≥ 15mε2.\nLemma 3. If p ∈ C, then Var [Z] ≤ 1500000m2ε4. If dTV(p, C) ≥ ε, then Var [Z] ≤ 1100E[Z]2.\nAssuming Lemmas 2 and 3, Theorem 2 is now a simple application of Chebyshev’s inequality. When p ∈ C, we have that\nE [Z] + √ 3Var [Z]1/2 ≤\n(\n1\n500 +\n√ 3 ( 1\n500000\n)1/2 )\nmε2 ≤ 1 200 mε2.\nThus, Chebyshev’s inequality gives\nPr [ Z ≥ mε2/10 ] ≤ Pr [ Z ≥ mε2/200 ] ≤ Pr [ Z − E [Z] ≥ √ 3Var [Z]1/2 ]\n≤ 1 3 .\nThe case for dTV(p, C) ≥ ε is similar. Here,\nE [Z]− √ 3Var [Z]1/2 ≥\n(\n1− √ 3 ( 1\n100\n)1/2 )\nE[Z] ≥ 3mε2/20.\nTherefore,\nPr [ Z ≤ mε2/10 ] ≤ Pr [ Z ≤ 3mε2/20 ] ≤ Pr [ Z − E [Z] ≤ − √ 3Var [Z]1/2 ]\n≤ 1 3 ."
    }, {
      "heading" : "5 Testing Monotonicity",
      "text" : "As an application of our testing framework, we will demonstrate how to test for monotonicity. Let d ≥ 1, and i = (i1, . . . , id), j = (j1, . . . , jd) ∈ [n]d. We say i < j if il > jl for l = 1, . . . , d.\nDefinition 7. A distribution p over [n]d is monotone (decreasing) if for all i < j, pi ≤ pj.\nOur main result of this section is as follows:\nTheorem 3. For any d ≥ 1, there exists an algorithm for testing monotonicity over [n]d with sample complexity\nO\n(\nnd/2\nε2 +\n(\nd log n\nε2\n)d\n· 1 ε2\n)\nand time complexity O ( nd/2\nε2 + poly(log n, 1/ε)d\n)\n.\nIn particular, this implies the following optimal algorithms for monotonicity testing for all d ≥ 1:\nCorollary 1. Fix any d ≥ 1, and suppose ε > √ d logn n1/4 . Then there exists an algorithm for testing monotonicity over [n]d with sample complexity O ( nd/2/ε2 ) .\nOur analysis starts with a structural lemma about monotone distributions. In [Bir87], Birgé showed that any monotone distribution p over [n] can be obliviously decomposed into O(log(n)/ε) intervals, such that the flattening p̄ (recall Definition 6) of p over these intervals is ε-close to p in total variation distance. [AJOS14] extend this result, giving a bound between the χ2-distance of p and p̄. We strengthen these results by extending them to monotone distributions over [n]d. In particular, we partition the domain [n]d of p into O((d log(n)/ε2)d) rectangles, and compare it with p̄, the flattening over these rectangles.\nLemma 4. Let d ≥ 1. There is an oblivious decomposition of [n]d into O((d log(n)/ε2)d) rectangles such that for any monotone distribution p over [n]d, its flattening p̄ over these rectangles satisfy χ2(p, p̄) ≤ ε2.\nThis effectively reduces the support size to logarithmic in n. At this point, we can apply the Laplace estimator (along the lines of [KOPS15]) and learn a q such that if p was monotone, then q will be O(ε2)-close in χ2-distance.\nLemma 5. Let d ≥ 1, and p be a monotone distribution over [n]d. There is an algorithm which outputs a distribution q such that E [ χ2(p, q) ]\n≤ ε2500 . The time and sample complexity are both O((d log(n)/ε2)d/ε2).\nThe final step before we apply our χ2-tester is to compute the distance between q and Mdn. This subroutine is similar to the one introduced by [BKR04]. The key idea is to write a linear program, which searches for any distribution f which is close to q in total variation distance. We note that the desired properties of f (i.e., monotonicity, normalization, and ε-closeness to q) are easy to enforce as linear constraints. If we find that such an f exists, we will apply our χ2-test to q. If not, we output Reject, as this is sufficient evidence to conclude that p 6∈ Mdn. Note that the linear program operates over the oblivious decomposition used in our structural result, so the complexity is polynomial in (d log(n)/ε)d, rather than the naive nd.\nAt this point, we have precisely the guarantees needed to apply Theorem 2, directly implying Theorem 3. Proof of the lemmas in this section are provided in Section C. We note that the class of monotone distributions is the simplest of the classes we consider. We now consider testing for log-concavity, monotone hazard rate, and unimodality, all of which are much more challenging to test. In particular, these classes require a more sophisticated structural understanding, more complex proper χ2-learning algorithms, and non-trivial modifications to our χ2-tester. We have already given some details on the required adaptations to the tester in Remark 1.\nOur algorithms for learning these classes use convex programming. One of the main challenges is to enforce log-concavity of the PDF when learning LCDn (respectively, of the CDF when learning MHRn), while simultaneously enforcing closeness in total variation distance. This involves a careful choice of our variables, and we exploit structural properties of the classes to ensure the soundness of particular Taylor approximations. We encourage the reader to refer to the proofs of Theorems 5, 7,and 8 for more details."
    }, {
      "heading" : "6 Testing Unimodality",
      "text" : "One striking feature of Birgé’s result is that the decomposition of the domain is oblivious to the samples, and therefore to the unknown distribution. However, such an oblivious decomposition will not work for the unimodal distribution, since the mode is unknown. Suppose we know where the mode of the unknown distribution might be, then the problem can be decomposed into monotone functions over two intervals. Therefore, in theory, one can modify the monotonicity testing algorithm by iterating over all the possible n modes. Indeed, by applying a union bound, it then follows that\nTheorem 4. (Follows from Monotone) For ε > 1/n1/4, there exists an algorithm for testing unimodality over [n] with sample complexity O (√\nn ε2\nlog n ) .\nHowever, this is unsatisfactory, since our lower bound (and as we will demonstrate, the true complexity of this problem) is √ n/ε2. We overcome the logarithmic barrier introduced by the union bound, by employing a non-oblivious decomposition of the domain, and using Kolmogorov’s max-inequality.\nOur main result for testing unimodality is the following theorem, which is proved in Section D.\nTheorem 5. Suppose ε > n−1/4. Then there exists an algorithm for testing unimodality over [n] with sample complexity O( √ n/ε2)."
    }, {
      "heading" : "7 Testing Independence of Random Variables",
      "text" : "Let X def= [n1] × . . . × [nd], and let Πd be the class of all product distributions over X . We first bound the χ2-distance between product distributions in terms of the individual coordinates.\nLemma 6. Let p = p1 × p2 . . .× pd, and q = q1 × q2 . . .× qd be two distributions in Πd. Then\nχ2(p, q) = d ∏\nℓ=1\n(1 + χ2(pℓ, qℓ))− 1.\nProof. By the definition of χ2-distance\nχ2(p, q) = ∑\ni∈X\np2i qi − 1 (3)\n=\nd ∏\nℓ=1\n\n\n∑\ni∈[nℓ]\n( pℓi )2\nqℓi\n\n− 1 (4)\n=\nd ∏\nℓ=1\n( 1 + χ2(pℓ, qℓ) ) − 1. (5)\nAlong the lines of learning monotone distributions in χ2 distance we obtain the following result, proved in Section E.\nLemma 7. There is an algorithm that takes\nO\n(\nd ∑\nℓ=1\nnℓ ε2\n)\nsamples from a distribution p in Πd and outputs a distribution q ∈ Πd such that with probability at least 5/6,\nχ2(p, q) ≤ O(ε2).\nThis fits precisely in our framework of robust χ2-ℓ1 testing. In particular, applying Theorem 2, we obtain the following result.\nTheorem 6. For any d ≥ 1, there exists an algorithm for testing independence of random variables over [n1]× . . . [nd] with sample and time complexity\nO\n(\n( ∏d ℓ=1 nℓ) 1/2 + ∑d ℓ=1 nℓ\nε2\n)\n.\nThe following corollaries are immediate.\nCorollary 2. Suppose ∏d ℓ=1 n 1/2 ℓ ≥ ∑d ℓ=1 nℓ. Then there exists an algorithm for testing independence over [n1]× · · · × [nd] with sample complexity Θ(( ∏d ℓ=1 nℓ) 1/2/ε2).\nIn particular,\nCorollary 3. There exists an algorithm for testing if two distributions over [n] are independent with sample complexity Θ(n/ε2)."
    }, {
      "heading" : "8 Testing Log-Concavity",
      "text" : "In this section we describe our results for testing log-concavity of distributions. Our main result is as follows:\nTheorem 7. There exists an algorithm for testing log-concavity over [n] with sample complexity\nO\n(√ n\nε2 +\n1\nε5\n)\nand time complexity poly(n, 1/ε).\nIn particular, this implies the following optimal tester for this class:\nCorollary 4. Suppose ε > 1/n1/5. Then there exists an algorithm for testing log-concavity over [n] with sample complexity O (√ n/ε2 ) .\nOur algorithm will fit into the structure of our general framework. We first perform a very particular type of learning algorithm, whose guarantees are summarized in the following lemma:\nLemma 8. Given ε > 0 and sample access to a distribution p, there exists an algorithm with the following guarantees:\n• If p ∈ LCDn, the algorithm outputs a distribution q ∈ LCDn and an O(ε)-effective support S of p such that χ2(pS , qS) ≤ ε 2\n500 with probability at least 5/6;\n• If dTV(p,LCDn) ≥ ε, the algorithm either outputs a distribution q ∈ LCDn or Reject.\nThe sample complexity is O(1/ε5) and the time complexity is poly(n, 1/ε).\nWe note that as a corollary, one immediately obtains aO(1/ε5) proper learning algorithm for logconcave distributions. The result is immediate from the first item of Lemma 8 and Proposition 1. We can actually do a bit better – in the proof of Lemma 8, we partition [n] into intervals of probability mass Θ(ε3/2). If one instead partitions into intervals of probability mass Θ(ε/ log(1/ε)) and works directly with total variation distance instead of χ2 distance, one can show that Õ(1/ε4) samples suffice.\nCorollary 5. Given ε > 0 and sample access to a distribution p ∈ LCDn, there exists an algorithm which outputs a distribution q ∈ LCDn such that dTV(p, q) ≤ ε. The sample complexity is Õ(1/ε4) and the time complexity is poly(n, 1/ε).\nThen, given the guarantees of Lemma 8, Theorem 7 follows from Theorem 23. The details of these results are presented in Section F."
    }, {
      "heading" : "9 Testing for Monotone Hazard Rate",
      "text" : "In this section, we obtain our main result for testing for monotone hazard rate:\n3To be more precise, we require the modification of Theorem 7 which is described in Section 4, in order to handle the case where the χ2-distance guarantees only hold for a known effective support.\nTheorem 8. There exists an algorithm for testing monotone hazard rate over [n] with sample complexity\nO\n(√ n\nε2 +\nlog(n/ε)\nε4\n)\nand time complexity poly(n, 1/ε).\nThis implies the following optimal tester for the class:\nCorollary 6. Suppose ε > √ log(n/ε)/n1/4. Then there exists an algorithm for testing monotone hazard rate over [n] with sample complexity O (√ n/ε2 ) .\nWe obey the same framework as before, first applying a χ2-learner with the following guarantees:\nLemma 9. Given ε > 0 and sample access to a distribution p, there exists an algorithm with the following guarantees:\n• If p ∈ MHRn, the algorithm outputs a distribution q ∈ MHRn and an O(ε)-effective support S of p such that χ2(pS , qS) ≤ ε 2\n500 with probability at least 5/6;\n• If dTV(p,MHRn) ≥ ε, the algorithm either outputs a distribution q ∈ MHRn and a set S ⊆ [n] or Reject.\nThe sample complexity is O(log(n/ε)/ε4) and the time complexity is poly(n, 1/ε).\nAs with log-concave distributions, this implies the following proper learning result:\nCorollary 7. Given ε > 0 and sample access to a distribution p ∈ MHRn, there exists an algorithm which outputs a distribution q ∈ MHRn such that dTV(p, q) ≤ ε. The sample complexity is O(log(n/ε)/ε4) and the time complexity is poly(n, 1/ε).\nAgain, combining the learning guarantees of Lemma 9 with the appropriate variant of Theorem 2, we obtain Theorem 8. The details of the argument and proofs are presented in Section G."
    }, {
      "heading" : "10 Lower Bounds",
      "text" : "We now prove sharp lower bounds for the classes of distributions we consider. We show that the example studied by Paninski [Pan08] to prove lower bounds on testing uniformity can be used to prove lower bounds for the classes we consider. They consider a class Q consisting of 2n/2 distributions defined as follows. Without loss of generality assume that n is even. For each of the 2n/2 vectors z0z1 . . . zn/2−1 ∈ {−1, 1}n/2, define a distribution q ∈ Q over [n] as follows.\nqi =\n{\n(1+zℓcε) n for i = 2ℓ+ 1 (1−zℓcε) n for i = 2ℓ.\n(6)\nEach distribution in Q has a total variation distance cε/2 from Un, the uniform distribution over [n]. By choosing c to be an appropriate constant, Paninski [Pan08] showed that a distribution picked uniformly at random fromQ cannot be distinguished from Un with fewer than √ n/ε2 samples with probability at least 2/3. Suppose C is a class of distributions such that\n• The uniform distribution Un is in C,\n• For appropriately chosen c, dTV(C,Q) ≥ ε, then testing C is not easier than distinguishing Un from Q. Invoking [Pan08] immediately implies that testing the class C requires Ω(√n/ε2) samples.\nThe lower bounds for all the one dimensional distributions will follow directly from this construction, and for testing monotonicity in higher dimensions, we extend this construction to d ≥ 1, appropriately. These arguments are proved in Section H, leading to the following lower bounds for testing these classes:\nTheorem 9.\n• For any d ≥ 1, any algorithm for testing monotonicity over [n]d requires Ω(nd/2/ε2) samples.\n• For d ≥ 1, any algorithm for testing independence over [n1]×· · ·×[nd] requires Ω ( (n1·n2...·nd)1/2 ε2 )\nsamples.\n• Any algorithm for testing unimodality, log-concavity, or monotone hazard rate over [n] requires Ω( √ n/ε2) samples."
    }, {
      "heading" : "Acknowledgements",
      "text" : "The authors thank Clément Canonne and Jerry Li; the former for several useful comments and suggestions on previous drafts of this work, and both for helpful discussions and thoughts regarding independence testing."
    }, {
      "heading" : "A Moments of the Chi-Squared Statistic",
      "text" : "We analyze the mean and variance of the statistic\nZ = ∑\ni∈A\n(Xi −mqi)2 −Xi mqi ,\nwhere each Xi is independently distributed according to Poisson(mpi). We start with the mean:\nE [Z] = ∑\ni∈A E\n[ (Xi −mqi)2 −Xi mqi ]\n= ∑\ni∈A\nE [ X2i ] − 2mqiE [Xi] +m2q2i − E [Xi] mqi\n= ∑\ni∈A\nm2p2i +mpi − 2m2qipi +m2q2i −mpi mqi\n= m ∑\ni∈A\n(pi − qi)2 qi\n= m · χ2(pA, qA)\nNext, we analyze the variance. Let λi = E [Xi] = mpi and λ ′ i = mqi.\nVar [Z] = ∑\ni∈A\n1\nλ′2i Var\n[ (Xi − λi)2 + 2(Xi − λi)(λi − λ′i)− (Xi − λi) ]\n= ∑\ni∈A\n1\nλ′2i Var\n[ (Xi − λi)2 + (Xi − λi)(2λi − 2λ′i − 1) ]\n= ∑\ni∈A\n1\nλ′2i E [ (Xi − λi)4 + 2(Xi − λi)3(2λi − 2λ′i − 1) + (Xi − λi)2(2λi − 2λ′i − 1)2 − λ2i ]\n= ∑\ni∈A\n1\nλ′2i [3λ2i + λi + 2λi(2λi − 2λ′i − 1) + λi(2λi − 2λ′i − 1)2 − λ2i ]\n= ∑\ni∈A\n1\nλ′2i [2λ2i + λi + 4λi(λi − λ′i)− 2λi + λi(4(λi − λ′i)2 − 4(λi − λ′i) + 1)]\n= ∑\ni∈A\n1\nλ′2i [2λ2i + 4λi(λi − λ′i)2]\n= ∑\ni∈A\n[\n2 p2i q2i + 4m · pi · (pi − qi) 2 q2i ]\n(7)\nThe third equality is by noting the random variable has expectation λi and the fourth equality substitutes the values of centralized moments of the Poisson distribution."
    }, {
      "heading" : "B Analysis of our χ2-Test Statistic",
      "text" : "We first prove the key lemmas in the analysis of our χ2-test. Proof of Lemma 2: The former case is straightforward from (1) and Property 2 of q.\nWe turn to the latter case. Recall that A = {i : qi ≥ ε/50n}, and thus q(Ā) ≤ ε/50. We first show that dTV(pA, qA) ≥ 6ε25 , where pA, qA are defined as above and in our slight abuse of notation we use dTV(pA, qA) for non-probability vectors to denote 12‖pA − qA‖1.\nPartitioning the support into A and Ā, we have dTV(p, q) = dTV(pA, qA) + dTV(pĀ, qĀ). (8)\nWe consider the following cases separately:\n• p(Ā) ≤ ε/2: In this case,\ndTV(pĀ, qĀ) = 1\n2\n∑ i∈Ā |pi − qi| ≤\n1 2 (p(Ā) + q(Ā)) ≤ 1 2 (ε 2 + ε 50 ) = 13ε 50 .\nPlugging this in (8), and using the fact that dTV(p, q) ≥ ε shows that dTV(pA, qA) ≥ 6ε25 . • p(Ā) > ε/2: In this case, by the reverse triangle inequality,\ndTV(pA, qA) ≥ 1 2 (q(A)− p(A)) ≥ 1 2 ((1− ε/50) − (1− ε/2)) = 6ε 25 .\nBy the Cauchy-Schwarz inequality,\nχ2(pA, qA) ≥ 4 dTV(pA, qA)2\nq(A)\n≥ ε 2\n5 .\nWe conclude by recalling (1).\nProof of Lemma 3: We bound the terms of (2) separately, starting with the first.\n2 ∑\ni∈A\np2i q2i = 2 ∑\ni∈A\n( (pi − qi)2 q2i + 2piqi − q2i q2i )\n= 2 ∑\ni∈A\n( (pi − qi)2 q2i + 2qi(pi − qi) + q2i q2i )\n≤ 2n+ 2 ∑\ni∈A\n( (pi − qi)2 q2i + 2 (pi − qi) qi )\n≤ 4n+ 4 ∑\ni∈A\n(pi − qi)2 q2i\n≤ 4n+ 200n ε ∑\ni∈A\n(pi − qi)2 qi\n= 4n+ 200n\nε\nE[Z]\nm\n≤ 4n+ 1 100\n√ nE[Z] (9)\nThe second inequality is the AM-GM inequality, the third inequality uses that qi ≥ ε50n for all i ∈ A, the last equality uses (1), and the final inequality substitutes a value m ≥ 20000 √ n\nε2 . The second term can be similarly bounded:\n4m ∑\ni∈A\npi(pi − qi)2 q2i ≤ 4m ( ∑\ni∈A\np2i q2i\n)1/2 ( ∑\ni∈A\n(pi − qi)4 q2i\n)1/2\n≤ 4m ( 4n+ 1\n100\n√ nE[Z]\n)1/2 (\n∑\ni∈A\n(pi − qi)4 q2i\n)1/2\n≤ 4m ( 2 √ n+ 1\n10 n1/4E[Z]1/2\n)\n(\n∑\ni∈A\n(pi − qi)2 qi\n)\n=\n( 8 √ n+ 2\n5 n1/4E[Z]1/2\n)\nE[Z]\nThe first inequality is Cauchy-Schwarz, the second inequality uses (9), the third inequality uses the monotonicity of the ℓp norms, and the equality uses (1).\nCombining the two terms, we get\nVar [Z] ≤ 4n+ 9√nE [Z] + 2 5 n1/4E [Z]3/2 .\nWe now consider the two cases in the statement of our lemma.\n• When p ∈ C, we know from Lemma 2 that E [Z] ≤ 1500mε2. Combined with a choice of m ≥ 20000 √ n\nε2 and the above expression for the variance, this gives:\nVar [Z] ≤ 4 200002 m2ε4 + 9 20000 · 500m 2ε4 +\n√ 10\n12500000 m2ε4 ≤ 1 500000 m2ε4.\n• When dTV(p, C) ≥ ε, Lemma 2 and m ≥ 20000 √ n\nε2 give:\nE [Z] ≥ 1 5 mε2 ≥ 4000√n.\nCombining this with our expression for variance we get:\nVar [Z] ≤ 4 40002 E [Z]2 + 9 4000 E [Z]2 +\n2\n5 √ 4000 E [Z]2 ≤ 1 100 E [Z]2 ."
    }, {
      "heading" : "C Details on Testing Monotonicity",
      "text" : "In this section, we prove the lemmas necessary for our monotonicity testing result.\nC.1 A Structural Result for Monotone Distributions on the Hypergrid\nBirgé [Bir87] showed that any monotone distribution is estimated to a total variation ε with a O(log(n)/ε)-piecewise constant distribution. Moreover, the intervals over which the output is constant is independent of the distribution p. This result, was strengthened to the Kullback-Leibler divergence by [AJOS14] to study the compression of monotone distributions. They upper bound the KL divergence by χ2 distance and then bound the χ2 distance. We extend this result to [n]d. We divide [n]d into bd rectangles as follows. Let {I1, . . . , Ib} be a partition of [n] into consecutive intervals defined as:\n|Ij| = { 1 for 1 ≤ j ≤ b2 , ⌊2(1 + γ)j−b/2⌋ for b2 < j ≤ b.\nFor j = (j1, . . . , jd) ∈ [b]d, let Ij def= Ij1 × Ij2 × . . . × Ijd . The χ2 distance between p and p̄ can be bounded as\nχ2(p, p̄) =\n\n\n∑\nj∈[b]d\n∑\ni∈Ij\np2i p̄i\n\n− 1\n≤\n\n\n∑\nj∈[b]d p+ j |Ij|\n\n− 1\nFor j = (j1, . . . , jd) ∈ Slarge, let j∗ = (j∗1 , . . . , j∗b ) be\nj∗i =\n{\nji if ji ≤ b/2 + 1 ji − 1 otherwise.\nWe bound the expression above as follows. Let T ⊆ [d] be any subset of d. Suppose the size of T is ℓ. Let T̄ be the set of all j that satisfy ji = b/2+1 for i ∈ T . In other words, over the dimensions determined by T , the value of the index is equal to d/2 + 1. The map j → j∗ restricted to T is one-to-one, and since at most d − ℓ of the coordinates drop,\n|Ij| ≤ |Ij∗ | · (1 + γ)d−ℓ. Since there are ℓ coordinates that do not change, and each of them have 2(1 + γ) coordinates, we obtain\n∑ j∈T̄ pj ≤ ∑ j∈T̄ p− j∗ · |Ij| · (2(1 + γ))ℓ · (1 + γ)d−ℓ\n= ∑ j∈T̄ p−j∗ · |Ij∗ | · 2ℓ(1 + γ)d.\nSince the mapping is one-to-one, the probability of observing as element in T̄ is the probability of observing b/2+1 in ℓ coordinates, which is at most (2/(b+2))ℓ under any monotone distribution. Therefore,\n∑ j∈T̄ pj ≤\n(\n2\nb+ 2\n)ℓ\n· 2ℓ(1 + γ)d.\nFor any ℓ there are (d ℓ ) choices for T . Therefore,\nχ2(p, p̄) ≤ d ∑\nℓ=0\n(\nd\nℓ\n)(\n4\nb+ 2\n)ℓ\n(1 + γ)d − 1\n= (1 + γ)d ( 1 + 4\nb+ 2\n)d\n− 1\n=\n(\n1 + γ + 4\nb+ 2 +\n4γ\nb+ 2\n)d\n− 1\nRecall that γ = 2 log(n)/b > 1/b, implies that the expression above is at most (1 + 2γ)d − 1. This implies Lemma 4.\nC.2 Monotone Learning\nOur algorithm requires a distribution q satisfying the properties discussed earlier. We learn a monotone distribution from samples as follows.\nBefore proving this result, we prove a general result for χ2 learning of arbitrary discrete distributions, adapting the result from [KOPS15]. For a distribution p, and a partition of the domain into b intervals I1, . . . , Ib, let p̄i = p(Ii)/|Ii| be the flattening of p over these intervals. We saw that for monotone distributions there exists a partition of the domain such that p̄ is close to the underlying distribution in χ2 distance.\nSuppose we are given m samples from a distribution p and a partition I1, . . . , Ib. Let mj be the number of samples that fall in Ij . For i ∈ Ij, let\nqi def =\n1 |Ij | mj + 1 m+ b .\nLet Sj = ∑ i∈Ij p 2 i . The expected χ 2 distance between p and q can be bounded as follows.\nE [ χ2(p, q) ] =\n\n\nb ∑\nj=1\n∑\ni∈Ij\nm ∑\nℓ=0\n(\nm\nℓ\n)\n(p(Ij)) ℓ(1− p(Ij))m−ℓ p2i (ℓ+ 1)/(|Ij |(m+ b))\n\n− 1\n=\n\n\nm+ b\nm+ 1\nb ∑\nj=1\nSj p̄(Ij)/|Ij |\n(\nm ∑\nℓ=0\n(\nm+ 1\nℓ+ 1\n)\n(p(Ij)) ℓ+1(1− p(Ij))m+1−ℓ+1\n)\n\n− 1\n=\n\n\nm+ b\nm+ 1\nb ∑\nj=1\nSj p̄(Ij)/|Ij | ( 1− (1− p(Ij)m+1 )\n\n− 1\n≤\n\n\nm+ b\nm+ 1\nb ∑\nj=1\nSj p̄(Ij)/|Ij |\n\n− 1\n=\n[\nm+ b\nm+ 1\n( χ2(p, p̄) + 1 )\n]\n− 1\n= m+ b m+ 1 · χ2(p, p̄) + b m+ 1 . (10)\nSuppose γ = O(log(n)/b), and b = O(d · log(n)/ε2). Then, by Lemma 4, χ2(p, p̄) ≤ ε2. (11)\nCombining this with (10) gives Lemma 5."
    }, {
      "heading" : "D Details on testing Unimodality",
      "text" : "Recall that to circumvent Birgé’s decomposition, we want to decompose the interval into disjoint intervals such that the probability of each interval is about O(1/b), where b is a parameter, specified later. In particular we consider a decomposition of [n] with the following properties:\n1. For each element i with probability at least 1/b, there is an Iℓ = {i}.\n2. There are at most two intervals with p(I) ≤ 1/2b.\n3. Every other interval I satisfies p(I) ∈ [ 1 2b , 2 b ] .\nLet I1, . . . , IL denote the partition of [n] corresponding to these intervals. Note that L = O(b).\nClaim 1. There is an algorithm that takes O(b log b) samples and outputs I1, . . . , IL satisfying the properties above.\nThe first step in our algorithm is to estimate the total probability within each of these intervals. In particular,\nLemma 10. There is an algorithm that takes m′ = O(b log b/ε2) samples from a distribution p, and with probability at least 9/10 outputs a distribution q̄ that is constant on each IL. Moreover, for any j such that p(Ij) > 1/2b, q̄(Ij) ∈ (1± ε)p(Ij).\nProof. Consider any interval Ij with p(Ij) ≥ 1/2b. The number of samples NIj that fall in that interval is distributed Binomial(m′, p(Ij). Then by Chernoff bounds for m′ > 12b log b/ε2,\nPr ( |NIj −m′p(Ij)| > εm′p(Ij) ) ≤2 exp ( ε2m′p(Ij)/2 )\n(12)\n≤ 1 b2 , (13)\nwhere the last inequality uses the fact that p(Ij) ≥ 1/2b.\nThe next step is estimate the distance of q from Un. This is possible by a simple dynamic program, similar to the one used for monotonicity. If the estimated distance is more than ε/2, we output Reject.\nOur next step is to remove certain intervals. This will be to ensure that when the underlying distribution is unimodal, we are able to estimate the distribution multiplicatively over the remaining intervals. In particular, we do the following preprocessing step:\n• A = ∅.\n• For interval Ij,\n– If\nq(Ij) /∈ ((1− ε) · q(Ij+1), (1 + ε) · q(Ij+1)) OR (14) q(Ij) /∈ ((1− ε) · q(Ij−1), (1 + ε) · q(Ij−1)) , (15)\nadd Ij to A.\n• Add the (at most 2) intervals with mass at most 1/2b to A.\n• Add all intervals j with q(Ij)/|Ij | < ε/50n to A\nIf the distribution is unimodal, we can prove the following about the set of intervals Ac.\nLemma 11. If p is unimodal then,\n• p(IAc) ≥ 1− ε/25 − 1/b−O (log n/(εb)) .\n• Except at most one interval in Ac every other interval Ij satisfies,\np+j p−j ≤ (1 + ε).\nIf this holds, then the χ2 distance between p and q constrained to Ac, is at most ε2. This lemma follows from the following result.\nLemma 12. Let C > 2. For a unimodal distribution over [n], there are at most 4 log(50n/ε)Cε intervals Ij that satisfy p+j p−j < (1 + ε/C).\nProof. To the contrary, if there are more than 4 log(50n/ε)Cε intervals, then at least half of them are on one side of the mode, however this implies that the ratio of the largest probability and smallest probability is at least (1 + ε/C)j , and if j > 2 log(50n/ε)Cε , is at least 50n/ε, contradicting that we have removed all such elements.\nWe have one additional pre-processing step here. We compute q(Ac) and if it is smaller than 1− ε/25, we output Reject.\nSuppose there are L′ intervals in Ac. Then, except at most one interval in L′ we know that the χ2 distance between p and q is at most ε2 when p is unimodal, and the TV distance between p and q is at least ε/2 over Ac. We propose the following simple modification to take into account, the one interval that might introduce a high χ2 distance in spite of having a small total variation. If we knew the interval, we can simply remove it and proceed. Since we do not know where the interval lies, we do the following.\n1. Let Zj be the χ 2 statistic over the ith interval in Ac, computed with O( √ n/ε2) samples.\n2. Let Zl be the largest among all Zj’s.\n3. If ∑ j,j 6=l Zj > mε 2/10, output Reject.\n4. Output Accept.\nThe objective of removing the largest χ2 statistic is our substitute for not knowing the largest interval. We now prove the correctness of this algorithm.\nCase 1 p ∈ UMn: We only concentrate on the final step. The χ2 statistic over all but one interval are at most c ·mε2, and the variance is bounded as before. Since we remove the largest statistic, the expected value of the new statistic is strictly dominated by that of these intervals. Therefore, the algorithm outputs Accept with at least the same probability as if we removed the spurious interval.\nCase 2 p /∈ UMn: This is the hard case to prove for unimodal distributions. We know that the χ2 statistic is large in this case, and we therefore have to prove that it remains large even after removing the largest test statistic Zl.\nWe invoke Kolmogorov’s Maximal Inequality to this end.\nLemma 13 (Kolmogorov’s Maximal Inequality). For independent zero mean random variables X1, . . . ,XL with finite variance, let Sℓ = X1 + . . . Xℓ. Then for any λ > 0,\nPr\n(\nmax 1≤ℓ≤L\n|Sℓ| ≥ λ ) ≤ 1 λ2 · V ar (SL) . (16)\nAs a corollary, it follows that Pr (maxℓ |Xℓ| > 2λ) ≤ 1λ2 · V ar (SL). In the case we are interested in, we let Xi = Zℓ − E [Zℓ]. Then, similar to the computations before, and the fact that each interval has a small mass, it follows that that the variance of the summation is at most E [Zℓ] 2 /100. Taking λ = E [ SL −mε2/3 ]2\n/100, it follows that the statistic does not fall below to √ n. This completes the proof of Theorem 5."
    }, {
      "heading" : "E Learning product distributions in χ2 distance",
      "text" : "In this section we prove Lemma 7. The proof is analogous to the proof for learning monotone distributions, and hinges on the following result of [KOPS15]. Given m samples from a distribution q over n elements, the add-1 estimator (Laplace estimator) q satisfies:\nE [ χ2(p, q) ] ≤ n m+ 1 .\nNow, suppose p is a product distribution over X = [n1] × · · · × [nd]. We simply perform the add-1 estimation over each coordinate independently, giving a distribution q1 × · · · × qd. Since p is a product distribution the estimates in each coordinate is independent. Therefore, a simple application of the previous result and independence of the coordinates implies\nE [ χ2(p, q) ] =\nd ∏\nl=1\n( 1 + E [ χ2(pl, ql) ]) − 1\n≤ d ∏\nl=1\n(\n1 + nl\nm+ 1\n)\n− 1\n≤ exp (∑ l nl m+ 1 ) − 1, (17)\nwhere (17) follows from ex ≥ 1 + x. Using ex ≤ 1 + 2x for 0 ≤ x ≤ 1, we have\nE [ χ2(p, q) ] ≤ 2 ∑ l nl m+ 1 , (18)\nwhenm ≥ ∑l nl. Therefore, following an application of Markov’s inequality, whenm = Ω(( ∑ l nl)/ε 2), Lemma 7 is proved."
    }, {
      "heading" : "F Details on Testing Log-Concavity",
      "text" : "It will suffice to prove Lemma 8. Proof of Lemma 8: We first draw samples from p and obtain a O(1/ε3/2)-piecewise constant distribution f by appropriately flattening the empirical distribution. The proof is now in two parts. In the first part, we show that if p ∈ LCDn then f will be close to p in χ2 distance over its effective support. The second part involves proper learning of p. We will use a linear program on f to find a distribution q ∈ LCDn. This distribution is such that if p ∈ LCDn, then χ2(p, q) is small, and otherwise the algorithm will either output some q ∈ LCDn (with no other relevant guarantees) or Reject.\nWe first construct f . Let p̂ be the empirical distribution obtained by sampling O(1/ε5) samples from p. By Lemma 1, with probability at least 5/6, dK(p, p̂) ≤ ε5/2/10. In particular, note that |pi − p̂i| ≤ ε5/2/10. Condition on this event in the remainder of the proof.\nLet a be the minimum i such that pi ≥ ε3/2/5, and let b be the maximum i satisfying the same condition. Let M = {a, . . . , b} or ∅ if a and b are undefined. By the guarantee provided by the DKW inequality, pi ≥ ε3/2/10 for all i ∈ M . Furthermore, p̂i ∈ pi ± ε3/2/10 ∈ (1± ε) · pi. For each i ∈ M , let fi = p̂i. We note that |M | = O(1/ε), so this contributes O(1/ε) constant pieces to f .\nWe now divide the rest of the domain into t intervals, all but constantly many of measure Θ(ε3/2) (under p). This is done via the following iterative procedure. As a base case, set r0 = 0. Define Ij as [lj , rj ], where lj = rj−1+1 and rj is the largest j ∈ [n] such that p̂(Ij) ≤ 9ε3/2/10. The exception is if Ij would intersect M – in this case, we “skip” M : set rj = a− 1 and lj+1 = b+ 1. If such a j exists, denote it by j∗. We note that p(Ij) ≤ p̂(Ij) + ε5/2/10 ≤ ε3/2. Furthermore, for all j except j∗ and t, rj + 1 6∈ M , so p(Ij) ≥ 9ε3/2/10 − ε3/2/5 − ε5/2/10 ≥ 3ε3/2/5. Observe that this lower bound implies that t ≤ 2\nε3/2 for ε sufficiently small.\nPart 1. For this part of the algorithm, we only care about the guarantees when p ∈ LCDn, so we assume this is the case.\nFor the domain [n] \\M , we let f be the flattening of p̂ over the intervals I1, . . . It. To analyze f , we need a structural property of log-concave distributions due to Chan, Diakonikolas, Servedio, and Sun [CDSS13]. This essentially states that a log-concave distribution cannot have a sudden increase in probability.\nLemma 14 (Lemma 4.1 in [CDSS13]). Let p be a distribution over [n] that is non-decreasing and log-concave on [1, x] ⊆ [n]. Let I = [x, y] be an interval of mass P (I) = τ , and suppose that the interval J = [1, x− 1] has mass p(J) = σ > 0. Then\np(y)/p(x) ≤ 1 + τ/σ.\nRecall that any log-concave distribution is unimodal, and suppose the mode of p is at i0. We will first focus on the intervals I1, . . . , ItL which lie entirely to the left of i0 and M . We will refer to Ij as Lj for all j ≤ tL. Note that p is non-decreasing over these intervals.\nThe next steps to the analysis are as follows. First we show that the flattening of p over Lj is a multiplicative (1 +O(1/j)) estimate for each pi ∈ Lj. Then, we show that flattening the empirical distribution p̂ over Lj is a multiplicative (1 +O(1/j)) estimate of p(i) for each i ∈ Lj. Finally, we exclude a small number of intervals (those corresponding to O(ε) mass at the left and right side of the domain, as well as j∗) in order to get the χ2 approximation we desire on an effective support.\n• First, recall that p(Lj) ≤ ε3/2 for all j. Also, letting Jj = [1, rj−1], we have that p(Jj) ≥ (j − 1) · 3ε3/2/5. Thus by Lemma 14, p(rj) ≤ p(lj)(1 + 2/(j − 1)). Since the distribution is non-decreasing in Lj , the flattening p̄ of p is such that p̄(i) ∈ p(i)(1 ± 2j−1) for all i ∈ Lj .\n• We have that p(Lj) ≥ 3ε3/2/5, and p̂(Lj) ∈ p(Lj) ± ε5/2/10, so p̂(Lj) ∈ p(Lj) · (1 ± ε6 ), and hence p̂(i) ∈ p̄(i) · (1± ε6) for all i ∈ Lj. Combining with the previous point, we have that\np̂(i) ∈ p(i) · ( 1± ( 2ε 3(j − 1) + ε 6 + 2 j − 1 )) ∈ p(i) · ( 1± 11 3(j − 1) ) .\nA symmetric statement holds for the intervals that lie entirely to the right of i0 and M . We will refer to Ij as Rt−j for all j > tL.\nTo summarize, we have the following guarantees for the distribution f :\n• For all i ∈ M , f(i) ∈ p(i) · (1± ε);\n• For all i ∈ Lj (except L1 and Lj∗), f(i) ∈ p(i) · ( 1± 223j ) ;\n• For all i ∈ Rj (except R1), f(i) ∈ p(i) · ( 1± 223j ) ;\nNote that, in particular, we have multiplicative estimates for all intervals, except those in L1, Lj∗ , R1 and the interval containing i0. Let S be the set of all intervals except Lj∗, Lj and Rj for j ≤ 1/√ε, and the one containing i0 Then, since each interval has probability mass at most O(ε3/2) and we are excluding O(1/ √ ε) intervals, p(S) > 1−O(ε).\nWe now compute the χ2-distance induced by this approximation for elements in S. For an element i ∈ Lj ∩ S, we have\n(f(i)− p(i))2 p(i) ≤ 60p(i) j2 .\nSumming over all i ∈ Lj ∩ S gives 60ε3/2\nj2\nsince the probability mass of Lj is at most ε 3/2. Summing this over all Lj for j ≥ 1/ √ ε and j 6= j∗ gives\n60ε3/2 2/ε3/2 ∑\nj=1/ √ ε\n1\nj2 ≤ 60ε3/2\n∫ ∞\n1/ √ ε\n1\nx2 dx\n= 60ε3/2( √ ε)\n= O(ε2)\nas desired.\nPart 2. To obtain a distribution q ∈ LCDn, we write a linear program. We will work in the log domain, so our variables will be Qi, representing log q(i) for i ∈ [n]. We will use Fi = log f(i) as parameters in our LP. There will be no objective function, we simply search for a feasible point. Our constraints will be\nQi−1 +Qi+1 ≤ 2Qi ∀i ∈ [n− 1] Qi ≤ 0 ∀i ∈ [n]\nlog(1 + ε) ≤ |Qi − Fi| ≤ log(1 + ε) for i ∈ M\nlog\n(\n1− 22 3j\n) ≤ |Qi − Fi| ≤ log ( 1 + 22\n3j\n) for i ∈ Lj , j ≥ 1/ √ ε and j 6= j∗\nlog\n(\n1− 22 3j\n) ≤ |Qi − Fi| ≤ log ( 1 + 22\n3j\n) for i ∈ Rj , j ≥ 1/ √ ε\nIf we run the linear program, then after a rescaling and summing the error over all the intervals in the LP gives us that the distance between p and q to be O(ε2) χ2-distance in a set S which has measure p(S) ≥ 1− 4ε, as desired.\nIf the linear program finds a feasible point, then we obtain a q ∈ LCDn. Furthermore, if p ∈ LCDn, this also tells us that (after a rescaling of ε), summing the error over all intervals implies that χ2(pS , qS) ≤ ε 2\n500 for a known set S with p(S) ≥ 1−O(ε), as desired. If M 6= ∅, this algorithm works as described. The issue is if M = ∅, then we don’t know when the L intervals end and the R intervals begin. In this case, we run O(1/ε) LPs, using each interval as the one containing i0, and thus acting as the barrier between the L intervals (to its left) and the R intervals (to its right). If p truly was log-concave, then one of these guesses will be correct and the corresponding LP will find a feasible point."
    }, {
      "heading" : "G Details on MHR testing",
      "text" : "Proof of Lemma 9: As with log-concave distributions, our method for MHR distributions can be split into two parts. In the first step, if p ∈ MHRn, we obtain a distribution q which is O(ε2)-close to p in χ2 distance on a set A of intervals such that p(A) ≥ 1−O(ε). q will achieve this by being a multiplicative (1 + O(ε)) approximation for each element within these intervals. This step is very similar to the decomposition used for unimodal distributions (described in Section D), so we sketch the argument and highlight the key differences.\nThe second step will be to find a feasible point in a linear program. If p ∈ MHRn, there should always be a feasible point, indicating that q is close to a distribution in MHRn (leveraging the particular guarantees for our algorithm for generating q). If dTV(p,MHRn) ≥ ε, there may or may not be a feasible point, but when there is, it should imply the existence of a distribution p∗ ∈ MHRn such that dTV(q, p∗) ≤ ε/2.\nThe analysis will rely on the following lemma from [CDSS13], which roughly states that an MHR distribution is “almost” non-decreasing.\nLemma 15 (Lemma 5.1 in [CDSS13]). Let p be an MHR distribution over [n]. Let I = [a, b] ⊂ [n] be an interval, and R = [b + 1, n] be the elements to the right of I. Let η = p(I)/p(R). Then p(b+ 1) ≥ 11+ηp(a).\nPart 1. As before, with unimodal distributions, we start by taking O( b log b ε2\n) samples, with the goal of partitioning the domain into intervals of mass approximately Θ(1/b). First, we will ignore the left and rightmost intervals of mass Θ(ε). For all “heavy” elements with mass ≥ Θ(1/b), we consider them as singletons. We note that Lemma 15 implies that there will be at most O(1/ε) contiguous intervals of such elements. The rest of the domain is greedily divided (from left to right) into intervals of mass Θ(1/b), cutting an interval short if we reach one of the heavy elements. This will result in the guarantee that all but potentially O(1/ε) intervals have Θ(1/b) mass.\nNext, similar to unimodal distributions, considering the flattened distribution, we discard all intervals for which the per-element probability is not within a (1 ± O(ε)) multiplicative factor of the same value for both neighboring intervals. The claim is that all remaining intervals will have the property that the per-element probability is within a (1±O(ε)) multiplicative factor of the true probability. This is implied by Lemma 15. If there were a point in an interval which was above this range, the distribution must decrease slowly, and the next interval would have a much larger\nper-element weight, thus leading to the removal of this interval. A similar argument forbids us from missing an interval which contains a point that lies outside this range. Relying on the fact that truncating the left and rightmost intervals eliminates elements with low probability mass, similar to the unimodal case, one can show that we will remove at most log(n/ε)/ε intervals, and thus a log(n/ε)/bε probability mass. Choosing b = Ω(ε2/ log(n/ε)) limits this to be O(ε), as desired. At this point, if p is indeed MHR, the multiplicative estimates guarantee that the result is O(ε2)-close in χ2-distance among the remaining intervals.\nPart 2. We note that an equivalent condition for distribution f being MHR is log-concavity of log(1 − F ), where F is the CDF of f . Therefore, our approach for this part will be similar to the approach used for log-concave distributions.\nGiven the output distribution q from the previous part of this algorithm, our goal will be check if there exists an MHR distribution f which is O(ε)-close to q. We will run a linear program with variables fi = log(1 − Fi). First, we ensure that f is a distribution. This can be done with the following constraints:\nfi ≤ 0 ∀i ∈ [n] fi ≥ fi+1 ∀i ∈ [n− 1] fn = −∞\nTo ensure that f is MHR, we use the following constraint:\nfi−1 + fi+1 ≤ 2fi ∀i ∈ [2, n− 1]\nNow, ideally, we would like to ensure f and q are ε-close in total variation distance by ensuring they are pointwise within a multiplicative (1± ε) factor of each other:\n(1− ε) ≤ fi/qi ≤ (1 + ε)\nWe note that this is a stronger condition than f and q being ε-close, but if p ∈ MHRn, the guarantees of the previous step would imply the existence of such an f .\nWe have a separate treatment for the identified singletons (i.e., those with probability ≥ 1/b) and the remainder of the support. For each element qi identified to have ≥ 1/b mass, we add two constraints:\nlog((1 − ε/2b)(1 −Qi)) ≤ fi ≤ log((1 + ε/2b)(1 −Qi)) log((1− ε/2b)(1 −Qi−1)) ≤ fi−1 ≤ log((1 + ε/2b)(1 −Qi−1))\nIf we satisfy these constraints, it implies that\nqi − ε/b ≤ fi ≤ qi + ε/b.\nSince qi ≥ 1/b, this implies (1− ε)qi ≤ fi ≤ (1 + ε)qi\nas desired. Now, the remaining elements each have ≤ 1/b mass. For each such element qi, we create a constraint (1−O(ε)) qi\n1 −Qi−1 ≤ fi−1 − fi ≤ (1 +O(ε)) qi 1 −Qi−1\nNote that the middle term is\n− log ( 1− Fi 1− Fi−1 ) = − log ( 1− fi 1− Fi−1 ) ∈ fi 1− Fi−1 (1± 2ε) ,\nwhere the second equality uses the Taylor expansion and the facts that fi ≤ 1/b and 1− Fi−1 ≥ ε (since during the previous part, we ignored the rightmost O(ε) probability mass). If we satisfy the desired constraints, it implies that\nfi ∈ 1 (1± 2ε) 1− Fi−1 1−Qi−1 (1∓O(ε))qi.\nSince we are taking Ω(1/ε4) samples and 1 − Fi−1 ≥ Ω(ε), Lemma 1 implies that fi is indeed a multiplicative (1± ε) approximation for these points as well.\nWe note that all points which do not fall into these two cases make up a total of O(ε) probability mass. Therefore, f may be arbitrary at these points and only incur O(ε) cost in total variation distance.\nIf we find a feasible point for this linear program, it implies the existence of an MHR distribution withinO(ε) total variation distance. In this case, we continue to the testing portion of the algorithm. Furthermore, if p ∈ MHRn, our method for generating q certifies that such a distribution exists, and we continue on to the testing portion of the algorithm."
    }, {
      "heading" : "H Details of the Lower Bounds",
      "text" : "In this section, for the class of distributions Q described in discussion on lower bounds and a class of interest C, we show that dTV(C,Q) ≥ ε, thus implying a lower bound of Ω( √ n/ε2) for testing C.\nH.1 Monotone distributions\nWe first consider d = 1 and prove that for appropriately chosen c, any monotone distribution over [n] is ε-far from all distributions in Q. Consider any q ∈ Q. For this distribution, we say that i ∈ [n] is a raise-point if qi < qi+1. Let Rq be the set of raise points of q. For q ∈ Q, (6) implies at least one in every four consecutive integers in [n] is a raise point, and therefore, |Rq| ≥ n/4. Moreover, note that if i is a raise-point, then i+1 is not a raise point. For any monotone (decreasing) distribution p, pi ≥ pi+1. For any raise-point i ∈ Rq, by the triangle inequality,\n|pi − qi|+ |pi+1 − qi+1| ≥ |pi − pi+1 + qi+1 − qi| ≥ qi+1 − qi = 2cε\nn . (19)\nSumming over the set Rq, we obtain dTV(p, q) ≥ 12 |Rq| · 2cεn ≥ cε/4. Therefore, if c ≥ 4, then dTV(Mn, q) ≥ ε. This proves the lower bound for d = 1.\nThis argument can be extended to [n]d. Consider the following class of distributions on [n]d. For each point i = (i1, . . . , id) ∈ [n]d, where i1 is even, generate a random z ∈ {−1, 1}, and assign to i a probability of (1 + zcε)/nd. Let e1 def = (1, 0, . . . , 0). Similar to d = 1, assign a probability (1 − zcε)/nd to the point i + e1 = (i1 + 1, i2, . . . , id). This class consists of 2 nd/2\n2 distributions, and Paninski’s arguments extend to give a lower bound of Ω(nd/2/ε2) samples to distinguish this class from the uniform distribution over [n]d. It remains to show that all these distributions are ε far from Mdn. Call a point i as a raise point if pi < pi+e1 . For any i, one of the points i, i + e1, i+2e1, i+3e1 is a raise point, and the number of raise points is at least n\nd/4. Invoking the triangle inequality (identical to (19)) over the raise-points, in the first dimension shows that any monotone distribution over [n]d is at a distance cε4 from any distribution in this class. Choosing c = 4 yields a bound of ε.\nH.2 Testing Product Distributions\nOur idea for testing independence is similar to the previous section. We sketch the construction of a class of distributions on X = [n1]× · · · × [nd]. Then |X | = n1 · n2 . . . · nd. For each element in X assign a value (1±cε) and then for each such assignment, normalize the values so that they add to 1, giving rise to a distribution. This gives us a class of 2|X | distributions. The key argument is to show that a large fraction of these distributions are far from being a product distribution. This follows since the degrees of freedom of a product distribution is exponentially smaller than the number of possible distributions. The second step is to simply apply Paninski’s argument, now over the larger set of distributions, where we show that distinguishing the collection of distributions we constructed from the uniform distribution over X (which is a product distribution) requires √ |X |/ε2 samples.\nH.3 Log-concave and Unimodal distributions\nWe will show that any log-concave or unimodal distribution is ε-far from all distributions in Q. Since LCDn ⊂ Un, it will suffice to show this for every unimodal distribution. Consider any unimodal distribution p, with mode ℓ. Then, p is monotone non-decreasing over the interval [ℓ] and non-increasing over {ℓ+1, . . . , n}. By the argument for monotone distributions, the total variation distance between p and any distribution q over elements greater than ℓ is at least n−ℓ−1n cε 4 , and over elements less than ℓ is at least ℓ−1n cε 4 . Summing these two gives the desired bound.\nH.4 Monotone Hazard distributions\nWe will show that any monotone hazard rate distribution is ε-far from all distributions in Q. Let p be any monotone-hazard distribution. Any distribution q ∈ Q has mass at least 1/2 over the interval I = [n/4, 3n/4]. Therefore, by Lemma 15, for any i ∈ I, pi+1 ( 1 + pi1/4 )\n≥ pi. As noted before, at least n/8 of the raise-points are in I.\nFor any i ∈ I ∩Rq, qi = (1 + cε)/n, qi+1 = (1− cε)/n\ndi = |pi − qi|+ |pi+1 − qi+1|. (20)\nIf pi ≥ (1 + 2cε)/n or pi ≤ 1/n, then the first term, and therefore di is at least cε/n. If pi ∈ (1/n, (1 + 2cε)/n), then for n > 5/(cε)\npi+1 ≥ 1 n · 1 1 + 4n ≥ 1− cε/2 n .\nTherefore the second term of di is at cε/2n. Since there are at least n/8 raise points in I,\ndTV(p, q) ≥ 1\n2\nn 8 · cε 2n ≥ cε 16 . (21)\nThus any MHR distribution is ε-far from Q for c ≥ 16."
    } ],
    "references" : [ {
      "title" : "Testing k-wise and almost k-wise independence",
      "author" : [ "Noga Alon", "Alexandr Andoni", "Tali Kaufman", "Kevin Matulef", "Ronitt Rubinfeld", "Ning Xie" ],
      "venue" : "In Proceedings of STOC,",
      "citeRegEx" : "Alon et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Alon et al\\.",
      "year" : 2007
    }, {
      "title" : "Testing monotone continuous distributions on high-dimensional real cubes",
      "author" : [ "Michal Adamaszek", "Artur Czumaj", "Christian Sohler" ],
      "venue" : "In SODA,",
      "citeRegEx" : "Adamaszek et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Adamaszek et al\\.",
      "year" : 2010
    }, {
      "title" : "Testing Poisson Binomial Distributions",
      "author" : [ "Jayadev Acharya", "Constantinos Daskalakis" ],
      "venue" : "In Proceedings of SODA,",
      "citeRegEx" : "Acharya and Daskalakis.,? \\Q2015\\E",
      "shortCiteRegEx" : "Acharya and Daskalakis.",
      "year" : 2015
    }, {
      "title" : "Competitive classification and closeness testing",
      "author" : [ "Jayadev Acharya", "Hirakendu Das", "Ashkan Jafarpour", "Alon Orlitsky", "Shengjun Pan", "Ananda Theertha Suresh" ],
      "venue" : "In COLT,",
      "citeRegEx" : "Acharya et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Acharya et al\\.",
      "year" : 2012
    }, {
      "title" : "Sample-optimal density estimation in nearly-linear time",
      "author" : [ "Jayadev Acharya", "Ilias Diakonikolas", "Jerry Li", "Ludwig Schmidt" ],
      "venue" : "arXiv preprint arXiv:1506.00671,",
      "citeRegEx" : "Acharya et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Acharya et al\\.",
      "year" : 2015
    }, {
      "title" : "Efficient compression of monotone and m-modal distributions",
      "author" : [ "Jayadev Acharya", "Ashkan Jafarpour", "Alon Orlitsky", "Ananda Theertha Suresh" ],
      "venue" : "In Proceedings of the 2014 IEEE International Symposium on Information Theory, ISIT",
      "citeRegEx" : "Acharya et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Acharya et al\\.",
      "year" : 2014
    }, {
      "title" : "A competitive test for uniformity of monotone distributions",
      "author" : [ "Jayadev Acharya", "Ashkan Jafarpour", "Alon Orlitsky", "Ananda Theertha Suresh" ],
      "venue" : "In Proceedings of AISTATS,",
      "citeRegEx" : "Acharya et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Acharya et al\\.",
      "year" : 2013
    }, {
      "title" : "Categorical data analysis",
      "author" : [ "Alan Agresti", "Maria Kateri" ],
      "venue" : null,
      "citeRegEx" : "Agresti and Kateri.,? \\Q2011\\E",
      "shortCiteRegEx" : "Agresti and Kateri.",
      "year" : 2011
    }, {
      "title" : "Statistical Inference under Order Restrictions",
      "author" : [ "R.E. Barlow", "D.J. Bartholomew", "J.M. Bremner", "H.D. Brunk" ],
      "venue" : null,
      "citeRegEx" : "Barlow et al\\.,? \\Q1972\\E",
      "shortCiteRegEx" : "Barlow et al\\.",
      "year" : 1972
    }, {
      "title" : "Testing random variables for independence and identity",
      "author" : [ "Tugkan Batu", "Eldar Fischer", "Lance Fortnow", "Ravi Kumar", "Ronitt Rubinfeld", "Patrick White" ],
      "venue" : "In Proceedings of FOCS,",
      "citeRegEx" : "Batu et al\\.,? \\Q2001\\E",
      "shortCiteRegEx" : "Batu et al\\.",
      "year" : 2001
    }, {
      "title" : "Testing monotonicity of distributions over general partial orders",
      "author" : [ "Arnab Bhattacharyya", "Eldar Fischer", "Ronitt Rubinfeld", "Paul Valiant" ],
      "venue" : "In ICS,",
      "citeRegEx" : "Bhattacharyya et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Bhattacharyya et al\\.",
      "year" : 2011
    }, {
      "title" : "Estimating a density under order restrictions: Nonasymptotic minimax risk",
      "author" : [ "Lucien Birgé" ],
      "venue" : "The Annals of Statistics,",
      "citeRegEx" : "Birgé.,? \\Q1987\\E",
      "shortCiteRegEx" : "Birgé.",
      "year" : 1987
    }, {
      "title" : "Maximum likelihood estimation and confidence bands for a discrete log-concave",
      "author" : [ "Fadoua Balabdaoui", "Hanna Jankowski", "Kaspar Rufibach" ],
      "venue" : null,
      "citeRegEx" : "Balabdaoui et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Balabdaoui et al\\.",
      "year" : 2011
    }, {
      "title" : "Sublinear algorithms for testing monotone and unimodal distributions",
      "author" : [ "Tuğkan Batu", "Ravi Kumar", "Ronitt Rubinfeld" ],
      "venue" : "In Proceedings of the 36th Annual ACM Symposium on the Theory of Computing,",
      "citeRegEx" : "Batu et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Batu et al\\.",
      "year" : 2004
    }, {
      "title" : "Estimation of a k-monotone density: characterizations, consistency and minimax lower bounds",
      "author" : [ "Fadoua Balabdaoui", "Jon A. Wellner" ],
      "venue" : "Statistica Neerlandica,",
      "citeRegEx" : "Balabdaoui and Wellner.,? \\Q2010\\E",
      "shortCiteRegEx" : "Balabdaoui and Wellner.",
      "year" : 2010
    }, {
      "title" : "A survey on distribution testing: your data is big, but is it blue",
      "author" : [ "Clément L Canonne" ],
      "venue" : "In Electronic Colloquium on Computational Complexity (ECCC),",
      "citeRegEx" : "Canonne.,? \\Q2015\\E",
      "shortCiteRegEx" : "Canonne.",
      "year" : 2015
    }, {
      "title" : "Testing shape restrictions of discrete distributions",
      "author" : [ "Clement Canonne", "Ilias Diakonikolas", "Themis Gouleakis", "Ronitt Rubinfeld" ],
      "venue" : "In STACS,",
      "citeRegEx" : "Canonne et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Canonne et al\\.",
      "year" : 2015
    }, {
      "title" : "Learning mixtures of structured distributions over discrete domains",
      "author" : [ "Siu On Chan", "Ilias Diakonikolas", "Rocco A. Servedio", "Xiaorui Sun" ],
      "venue" : "In Proceedings of the 24th Annual ACM-SIAM Symposium on Discrete Algorithms,",
      "citeRegEx" : "Chan et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Chan et al\\.",
      "year" : 2013
    }, {
      "title" : "Efficient density estimation via piecewise polynomial approximation",
      "author" : [ "Siu On Chan", "Ilias Diakonikolas", "Rocco A. Servedio", "Xiaorui Sun" ],
      "venue" : "In Proceedings of the 46th Annual ACM Symposium on the Theory of Computing,",
      "citeRegEx" : "Chan et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Chan et al\\.",
      "year" : 2014
    }, {
      "title" : "Optimal algorithms for testing closeness of discrete distributions",
      "author" : [ "Siu-On Chan", "Ilias Diakonikolas", "Gregory Valiant", "Paul Valiant" ],
      "venue" : "In Proceedings of SODA,",
      "citeRegEx" : "Chan et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Chan et al\\.",
      "year" : 2014
    }, {
      "title" : "Theoretical properties of the log-concave maximum likelihood estimator of a multidimensional density",
      "author" : [ "Madeleine Cule", "Richard Samworth" ],
      "venue" : "Electronic Journal of Statistics,",
      "citeRegEx" : "Cule and Samworth.,? \\Q2010\\E",
      "shortCiteRegEx" : "Cule and Samworth.",
      "year" : 2010
    }, {
      "title" : "Asymptotic minimax character of the sample distribution function and of the classical multinomial estimator",
      "author" : [ "A. Dvoretzky", "J. Kiefer", "J. Wolfowitz" ],
      "venue" : "The Annals of Mathematical Statistics, 27(3):642–669,",
      "citeRegEx" : "Dvoretzky et al\\.,? \\Q1956\\E",
      "shortCiteRegEx" : "Dvoretzky et al\\.",
      "year" : 1956
    }, {
      "title" : "Statistical Methods for Research Workers",
      "author" : [ "Ronald Aylmer Fisher" ],
      "venue" : "Oliver and Boyd, Edinburgh,",
      "citeRegEx" : "Fisher.,? \\Q1925\\E",
      "shortCiteRegEx" : "Fisher.",
      "year" : 1925
    }, {
      "title" : "The art of uninformed decisions: A primer to property testing",
      "author" : [ "Eldar Fischer" ],
      "venue" : "Science, 75:97–126,",
      "citeRegEx" : "Fischer.,? \\Q2001\\E",
      "shortCiteRegEx" : "Fischer.",
      "year" : 2001
    }, {
      "title" : "Combinatorial property testing (a survey)",
      "author" : [ "Oded Goldreich" ],
      "venue" : "American Mathematical Society,",
      "citeRegEx" : "Goldreich.,? \\Q1998\\E",
      "shortCiteRegEx" : "Goldreich.",
      "year" : 1998
    }, {
      "title" : "On choosing and bounding probability metrics",
      "author" : [ "Alison L. Gibbs", "Francis E. Su" ],
      "venue" : "International Statistical Review,",
      "citeRegEx" : "Gibbs and Su.,? \\Q2002\\E",
      "shortCiteRegEx" : "Gibbs and Su.",
      "year" : 2002
    }, {
      "title" : "Testing for monotone increasing hazard rate",
      "author" : [ "Peter Hall", "Ingrid Van Keilegom" ],
      "venue" : "Annals of Statistics,",
      "citeRegEx" : "Hall and Keilegom.,? \\Q2005\\E",
      "shortCiteRegEx" : "Hall and Keilegom.",
      "year" : 2005
    }, {
      "title" : "Estimation of a discrete monotone density",
      "author" : [ "Hanna K. Jankowski", "Jon A. Wellner" ],
      "venue" : "Electronic Journal of Statistics,",
      "citeRegEx" : "Jankowski and Wellner.,? \\Q2009\\E",
      "shortCiteRegEx" : "Jankowski and Wellner.",
      "year" : 2009
    }, {
      "title" : "On learning distributions from their samples",
      "author" : [ "Sudeep Kamath", "Alon Orlitsky", "Dheeraj Pichapati", "Ananda T. Suresh" ],
      "venue" : "In COLT,",
      "citeRegEx" : "Kamath et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Kamath et al\\.",
      "year" : 2015
    }, {
      "title" : "Testing statistical hypotheses",
      "author" : [ "Erich L Lehmann", "Joseph P Romano" ],
      "venue" : "Springer Science & Business Media,",
      "citeRegEx" : "Lehmann and Romano.,? \\Q2006\\E",
      "shortCiteRegEx" : "Lehmann and Romano.",
      "year" : 2006
    }, {
      "title" : "Testing properties of collections of distributions",
      "author" : [ "Reut Levi", "Dana Ron", "Ronitt Rubinfeld" ],
      "venue" : "Theory of Computing,",
      "citeRegEx" : "Levi et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Levi et al\\.",
      "year" : 2013
    }, {
      "title" : "The tight constant in the Dvoretzky-Kiefer-Wolfowitz inequality",
      "author" : [ "Pascal Massart" ],
      "venue" : "The Annals of Probability, 18(3):1269–1283,",
      "citeRegEx" : "Massart.,? \\Q1990\\E",
      "shortCiteRegEx" : "Massart.",
      "year" : 1990
    }, {
      "title" : "A coincidence-based test for uniformity given very sparsely sampled discrete data",
      "author" : [ "Liam Paninski" ],
      "venue" : "IEEE Transactions on Information Theory,",
      "citeRegEx" : "Paninski.,? \\Q2008\\E",
      "shortCiteRegEx" : "Paninski.",
      "year" : 2008
    }, {
      "title" : "Property testing: A learning theory perspective",
      "author" : [ "Dana Ron" ],
      "venue" : "Foundations and Trends in Machine Learning,",
      "citeRegEx" : "Ron.,? \\Q2008\\E",
      "shortCiteRegEx" : "Ron.",
      "year" : 2008
    }, {
      "title" : "The analysis of categorical data from complex sample surveys: chi-squared tests for goodness of fit and independence in two-way tables",
      "author" : [ "Jon NK Rao", "Alastair J Scott" ],
      "venue" : "Journal of the American Statistical Association,",
      "citeRegEx" : "Rao and Scott.,? \\Q1981\\E",
      "shortCiteRegEx" : "Rao and Scott.",
      "year" : 1981
    }, {
      "title" : "Log-concavity and strong log-concavity: a review",
      "author" : [ "Adrien Saumard", "Jon AWellner" ],
      "venue" : "Statistics Surveys,",
      "citeRegEx" : "Saumard and AWellner.,? \\Q2014\\E",
      "shortCiteRegEx" : "Saumard and AWellner.",
      "year" : 2014
    }, {
      "title" : "Estimating the unseen: An n/ log n-sample estimator for entropy and support size, shown optimal via new CLTs",
      "author" : [ "Gregory Valiant", "Paul Valiant" ],
      "venue" : "In Proceedings of the 43rd Annual ACM Symposium on the Theory of Computing,",
      "citeRegEx" : "Valiant and Valiant.,? \\Q2011\\E",
      "shortCiteRegEx" : "Valiant and Valiant.",
      "year" : 2011
    }, {
      "title" : "An automatic inequality prover and instance optimal identity testing",
      "author" : [ "Gregory Valiant", "Paul Valiant" ],
      "venue" : "In FOCS,",
      "citeRegEx" : "Valiant and Valiant.,? \\Q2014\\E",
      "shortCiteRegEx" : "Valiant and Valiant.",
      "year" : 2014
    }, {
      "title" : "A, the last equality uses (1), and the final inequality substitutes a value m",
      "author" : [ ],
      "venue" : null,
      "citeRegEx" : "∈,? \\Q2000\\E",
      "shortCiteRegEx" : "∈",
      "year" : 2000
    }, {
      "title" : "When dTV(p, C) ≥ ε, Lemma 2 and m",
      "author" : [ "mε" ],
      "venue" : null,
      "citeRegEx" : "mε.,? \\Q2000\\E",
      "shortCiteRegEx" : "mε.",
      "year" : 2000
    } ],
    "referenceMentions" : [ ],
    "year" : 2015,
    "abstractText" : "Given samples from an unknown distribution p, is it possible to distinguish whether p belongs to some class of distributions C versus p being far from every distribution in C? This fundamental question has received tremendous attention in statistics, focusing primarily on asymptotic analysis, and more recently in information theory and theoretical computer science, where the emphasis has been on small sample size and computational complexity. Nevertheless, even for basic properties of distributions such as monotonicity, log-concavity, unimodality, independence, and monotone-hazard rate, the optimal sample complexity is unknown. We provide a general approach via which we obtain sample-optimal and computationally efficient testers for all these distribution families. At the core of our approach is an algorithm which solves the following problem: Given samples from an unknown distribution p, and a known distribution q, are p and q close in χ-distance, or far in total variation distance? With this tool in place, we develop a general testing framework which leads to the following results: • Testing identity to any distribution over [n] requires Θ(√n/ε2) samples. This is optimal for the uniform distribution. This gives an alternate argument for the minimax sample complexity of testing identity (proved in [VV14]). • For all d ≥ 1 and n sufficiently large, testing whether a discrete distribution over [n] is monotone requires an optimal Θ(n/ε) samples. The single-dimensional version of our theorem improves a long line of research starting with [BKR04], where the previous best tester required Ω( √ n log(n)/ε) samples, while the high-dimensional version improves [BFRV11], which requires Ω̃(n 1 2poly(1ε )) samples. • For all d ≥ 1, testing whether a collection of random variables over [n1]×· · ·×[nd] are independent requiresO ((",
    "creator" : "LaTeX with hyperref package"
  }
}