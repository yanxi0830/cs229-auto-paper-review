{
  "name" : "1511.00043.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Learning Adversary Behavior in Security Games: A PAC Model Perspective",
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Stackelberg Security Games (SSGs) is arguably the best example of application of the Stackelberg game model in the real world. Indeed, a slew of successful deployed applications (Tambe 2011) (LAX airport, US air marshal) and extensive research on related topics (Basilico et al.(2009), Korzhyk et al. (2010), Gan et al. (2015)) provide evidence about the generality of the SSG framework. More recently, new application domains of SSGs, from green crime to urban crime, are accompanied by significant amounts of past data of recorded defender strategies and adversary reactions; this has enabled learning adversary behavior from such data: Zhang et al.(2015), Yang et al. (2014). Also, analysis of these datasets and human subject experiment studies (Nguyen et al. 2013) have revealed that modeling the bounded rationality of the adversary enables the defender to further optimize her allocation of limited security resources.\nWhile standard learning techniques have been used to provide prediction results on learned adversary models, a formal study about the learning task at hand is lacking. Such a study could answer several important questions that arise in practice, for example, (1) How much more data would be required if we want to improve prediction accuracy, say from 0.7 to 0.9? (2) What worst utility bound could be provided when the optimal defender strategy is computed using the learned adversary model?\nIn this paper, we initiate a theoretical study of the learning guarantees in an SSG1 setting. We adopt the framework in which the defender first learns the response function of the adversary and then optimizes against the learned response. Thus, unlike past work on learning in SSGs (see related work) where the reasoning about the response happens through payoff and rationality, we treat the response of the bounded rational adversary as the object that can be learned.\nOur first contribution is using the Probably Approximately Correct (PAC) model (Kearns and Vazirani 1994; Anthony and Bartlett 2009) to formulate the learning problem at hand. While PAC learning is fairly standard for classifiers and real valued functions (i.e., regression), we appeal to the framework of (Haussler 1992) to study the PAC learnability of the real vector valued response function in SSGs.\nOur second contribution is an analysis of the Subjective Utility Quantal Response (SUQR) model (Nguyen et al. 2013): the most popular bounded rationality model used in SSGs, which posits a parametrized response function. For this model, we address both questions raised earlier by deriving the sample complexity for PAC learnability and providing worst case utility bounds under certain assumptions.\nOur third contribution includes specifying the first nonparametric Lipschitz (NPL) class of response functions for SSGs, where the only restriction on NPL functions is Lipschitzness. We provide a novel learning technique for NPL, and the sample complexity and utility bounds. This approach makes very little assumptions about the response function, enabling the learning of a multitude of behavior albeit at the cost of higher sample complexity.\nWe also conduct experiments with real-world poaching data from the Queen Elizabeth National Park (QENP) in Uganda (obtained from (Nguyen et al. 2015)) and data collected from human subject experiments. The experimental results support the theoretical rate of growth of the number of samples for different techniques. The NPL approach, while computationally slow, outperforms SUQR on realworld poaching data. Finally, our work opens up a number of exciting research directions, such as studying bounded rationality in active learning setting and real-world application of non-parametric models. (All proofs are in the Appendix. In text, we provide intuitive steps leading to the proofs.)\n1Our theory results apply to Stackelberg games also. See Appendix.\nar X\niv :1\n51 1.\n00 04\n3v 1\n[ cs\n.A I]\n3 0\nO ct\n2 01\n5\nRelated Work: Learning in SSGs with rational adversaries has been studied in two recent papers (Blum, Haghtalab, and Procaccia 2014; Balcan et al. 2015), and in Stackelberg games by Letchford et al. (2009) and Marecki et al. (2012). Our work differs as we study bounded rational adversaries in a passive learning scenario. Also, our focus on SSGs differentiates us from recent work on PAC learnability in co-operative games (Balcan, Procaccia, and Zick 2015). Also, our work is orthogonal to adversarial learning (Vorobeychik and Li 2014), which studies game theoretic models of an adversary attacking a learning algorithm.\nPAC learning has a very rich and extensive body of work (Anthony and Bartlett 2009). We use the framework of (Haussler 1992). For the parametric case, we derive sharp sample complexity bounds based on covering numbers using our techniques rather than using the standard technique of pseudo-dimension (Pollard 1984) or fat shattering dimension (Anthony and Bartlett 2009). For the NPL case we use results from (Tikhomirov and Kolmogorov 1993); this has also been used in (Luxburg and Bousquet 2004)."
    }, {
      "heading" : "2 SSG Preliminaries",
      "text" : "This section introduces the background and preliminary notations for SSGs. A summary of notations used in this paper is presented in Table 1. A SSG is a two player Stackelberg game between a defender (leader) and an adversary (follower) (Paruchuri et al. 2008). The defender wishes to protect T targets with a limited number of security resources K (K << T ). For ease of presentation, we restrict ourselves to the scenario with no scheduling constraints (see Korzhyk et al. (2010)). The defender’s pure strategy is to allocate each resource to a target. A defender’s mixed-strategy x̃ (∀j ∈ P. x̃j ∈ [0, 1], ∑P j=1 x̃j = 1) is then defined as a probability distribution over the set of all possible pure strategies P . An equivalent description (see Korzhyk et al. (2010)) of these mixed strategies are coverage probabilities over the set of targets: x (∀i ∈ T. xi ∈ [0, 1], ∑T i=1 xi ≤ K). We refer to this latter description as the mixed strategy of the defender. A pure strategy of the adversary is defined as attacking a single target. The adversary’s mixed strategy is then a categorical distribution over the set of targets. Thus, it can be expressed as parameters qi (i ∈ T ) of a categorical distribution such that 0 ≤ qi ≤ 1 and ∑ i qi = 1. The adversary’s response to the defender’s mixed strategy is given by a function q : X → Q, where Q is the space of all mixed strategies of the adversary. The matrix U specifies the payoffs of the defender, and her expected utility is xTUq(x) when she plays a mixed strategy x ∈ X .\nBounded Rationality Models: We discuss the SUQR model and its representation for the analysis in this paper below. Building on prior work on quantal response (McFadden 1976), SUQR (Nguyen et al. 2013) states that given n actions, a human player plays action i with probability qi ∝ ew·v , where v denote a vector of feature values for choice i and w denotes the weight parameters for these features. The model is equivalent to conditional logistic regression (McFadden 1973). The features are specific to the domain, e.g., in case of SSG applications, the set of features include the\ncoverage probability xi, the reward Ri and penalty Pi of target i. Since, other than the coverage x, remaining features are fixed in real world data, we assume a target-specific feature ci (which may be a linear combination of rewards and penalties) and analyze the following generalized form of SUQR with parameters w1 and ci’s: qi(x) ∝ ew1xi+ci .2\nAs ∑T i=1 qi(x) = 1, we have qi(x) = ew1xi+ci∑T j=1 e w1xj+cj . Equivalent Alternate Representation: For ease of mathematical proofs, using standard techniques in logistic regression, we take qT ∝ e0, and hence, qi ∝ ew1(xi−xT )+(ci−cT ). To shorten notation, let ciT = ci − cT , xiT = xi − xT . By multiplying the numerator and denominator by ew1xT+cT , it can be verified that e w1xiT+ciT\ne0+ ∑T−1 j=1 e w1xjT+cjT = e w1xi+ci∑T j=1 e w1xj+cj ."
    }, {
      "heading" : "3 Learning Framework for SSG",
      "text" : "First, we introduce some notations: given two n-dimensional points o and o′, the lp distance dlp between the two points is: dlp(o, o ′) = ||o−o′||p = ( ∑n i=1 |oi−o′i|p)1/p. In particular, dl∞(o, o ′) = ||o − o′||∞ = maxi |oi − o′i|. Also, dl̄p = dlp/n. KL denotes the Kullback-Leibler divergence. We use the learning framework of Haussler (1992), which includes an instance space X and outcome space Y . In our context, X is same as the space of defender mixed strategies x ∈ X . Outcome space Y is defined as the space of all possible categorical choices over a set of T targets (i.e., choice of target to attack) for the adversary: ti = 〈t1i , . . . , tTi 〉 ∈ Y (tji = 1 for j = i, otherwise 0). Thus, Y has T elements. To give an example, given three targets T1, T2 and T3, Y = {t1, t2, t3} = {〈1, 0, 0〉, 〈0, 1, 0〉, 〈0, 0, 1〉}, where 〈1, 0, 0〉 denotes that T1 was attacked while T2 and T3 were not attacked, and so on. The training data are samples drawn from Z = X × Y using an unknown distribution, say given by density p(x, y), i.e., each training data point (x, y) denotes the adversary’s response y ∈ Y (attack on a target) to a particular defender mixed strategy x ∈ X . The density p also determines the true attacker behavior qp(x); qp(x) denotes the vector of parameters 〈qp1(x), . . . , q p T (x)〉, where\n2This general form is harder to analyze than the standard SUQR form in which the exponent function (function of xi, Ri, Pi) for all qi is same: w1xi+w2Ri+w3Pi. For completeness, we derive the results for the standard SUQR form in the Appendix.\nqpi (x) = p(ti/x), of the conditional (categorical) distribution over Y given x, i.e., the parameters of the true distribution from which attacks are sampled for a given x.\nHaussler (1992) also defines a decision space A, a space of hypothesis (functions) H with elements h : X → A and a loss function l : Y × A → R. The hypothesis h outputs values in A that enables computing (probabilistic) predictions of the actual outcome. The loss function l captures the loss when the real outcome is y ∈ Y and the prediction of possible outcomes happens using a ∈ A.\nAs a concrete example, for the parametric representation of generalized SUQR in the previous section and considering our 3-target example above, H contains vector valued functions with (T − 1) = 2 components that form the exponents of the numerator of prediction probabilities qi, i.e., H contains functions of the form: 〈w1(x1−x3)+c13, w1(x2− x3) + c23〉; ∀x ∈ X . Also, A is the range of the functions in H, i.e., A ⊂ R2. Then, given h(x) = 〈a1, a2〉, the prediction probabilities qh1 (x), q h 2 (x), q h 3 (x) is given by qhi (x) = eai\n1+ea1+ea2 (assume a3 = 0). PAC learnability: The learning algorithm aims to learn a h ∈ H that minimizes the true risk of using the hypothesis h. The true risk rh(p) of a particular hypothesis (predictor) h, given density function p(x, y) over Z = X × Y , is the expected loss of predicting h(x) when the true outcome is y:\nrh(p) = ∫ p(x, y)l(y, h(x)) dx dy\nOf course, as p is unknown the true risk cannot be computed. However, given (enough) samples from p, the true risk can be estimated by the empirical risk. The empirical risk r̂h(~z), where ~z is a sequence ofm training samples from Z, is defined as: r̂h(~z) = 1/m ∑m i=1 l(yi, h(xi)). Let h\n∗ be the hypothesis that minimizes the true risk, i.e., rh∗(p) = inf{rh(p) | h ∈ H} and let ĥ∗ be the hypothesis that minimizes the empirical risk, i.e., r̂ĥ∗(~z) = inf{r̂h(~z) | h ∈ H}. The following is the well-known PAC learning result (Anthony and Bartlett 2009) for any empirical risk minimizing (ERM) algorithm A yielding hypothesis A(~z):\nIf Pr(∀h ∈ H.|r̂h(~z)− rh(p)| < α/3) > 1− δ/2 and Pr(|r̂A(~z)(~z)− r̂ĥ∗(~z)| < α/3) > 1− δ/2\nthen Pr(|rA(~z)(p)− rh∗(p)| < α) > 1− δ\nThe final result states that output A(~z) has true risk α-close to the lowest true risk in H attained by h∗ with high probability 1−δ over the choice of training samples. The first precondition states that it must be the case that for all h ∈ H the difference between empirical risk and true risk is α3 - close with high probability 1− δ2 . The second pre-condition states that the output A(~z) of the ERM algorithm A should have empirical risk α3 -close to the lowest empirical risk of ĥ∗ with high probability 1 − δ2 . In this work, our empirical risk minimizing algorithms find ĥ∗ exactly (upto precision of convex solvers, see Section 5), thus, satisfying the second pre-condition; hence, we will focus more on the first pre-condition. As the empirical risk estimate gets better with increasing samples, a minimum number of samples are required to ensure that the first pre-condition holds (see Theorem 1). A hypothesis class H is called (α, δ)-PAC learnable\nif there exists an ERM algorithm A such that H and A satisfy the two pre-conditions.\nModeling security games: Having given an example for generalized SUQR, we systematically model learning of adversary behavior in SSGs using the PAC framework for any hypothesis class H. We assume certain properties of the functions h ∈ H that we present below. We assume that the vector valued function h ∈ H takes the form\nh(x) = 〈h1(x), . . . , hT−1(x)〉.\nThus, A is the product space A1× . . . , AT−1. Each hi(x) is assumed to take values between [−M2 , M 2 ], where M >> 1, which implies Ai = [−M2 , M 2 ]. The prediction probabilities induced by any h is qh(x) = 〈qh1 (x), . . . , qhT (x)〉, where qhi (x) = ehi(x) 1+ ∑ i e hi(x)\n(assume hT (x) = 0). Parametric H: In this approach we model generalized SUQR. Generalizing from the three target concrete example, the parametric functions h ∈ H take the form where each component function is hi(x) = w1(xi − xT ) + ciT .\nNon-parametric Lipschitz (NPL)H: For this case the only restriction we impose on functions h ∈ H is that each component function hi is L-Lipschitz where L ≤ K̂, for given and fixed constant K̂. We show later (Lemma 7) that this implies that qh is Lipschitz also.\nNext, given the stochastic nature of the adversary’s attacks, we use a loss function (same for parametric and NPL) such that minimizing the empirical risk is equivalent to maximizing the likelihood of seeing the attack data. The loss function l : Y ×A→ R for actual outcome ti is defined as\nl(ti, a) = − log ( eai/1 + ∑T−1 j=1 e aj ) . (1)\nIt can be readily inferred that minimizing the empirical risk (recall r̂h(~z) = 1/m ∑m i=1 l(yi, h(xi))) is equivalent to maximizing the log likelihood of the training data."
    }, {
      "heading" : "4 Sample Complexity",
      "text" : "In this section we derive the sample complexity for the parametric and NPL case. First, we present a general result about sample complexity bounds for anyH, given our loss l. This result relies on sample complexity results in (Haussler 1992). The bound depends on the capacity C ofH, which we define after the theorem. The bound also assumes an ERM algorithm which we present for our models in Section 5.\nTheorem 1. Assume that the hypothesis spaceH is permissible3. Let the data be generated by m independent draws from X × Y according to p. Then, assuming existence of an ERM algorithm and given our loss l defined in Eq. 1, the least m required to ensure (α, δ)-PAC learnability is (recall dl̄1 is average l1 distance)\n(576M2/α2) ( log(1/δ) + log(8C(α/96T,H, dl̄1) )\nAll terms in the above sample complexity are known, except for the capacity C(α/96T,H, dl̄1). The rest of this\n3As noted in Haussler: “This is a measurability condition defined in Pollard (1984) which need not concern us in practice.”\nsection will focus on computing capacity for both the parametric and NPL hypothesis space. Next, we define covering number N of function spaces, which leads to a definition of capacity C. Let d be a pseudo metric for the set H. For any > 0, an -cover for H is a finite set F ⊆ H such that for any h ∈ H there is a f ∈ F with d(f, h) ≤ , i.e., any element in H is at least -close to some element of the cover F . The covering number N ( ,H, d) denotes the size of the smallest -cover for set H (for the pseudo metric d). We now proceed to define a pseudo metric dL1(P,d) on H with respect to any probability measure P on X and any given pseudo-metric d on A.\ndL1(P,d)(f, g) = ∫ X d(f(x), g(x)) dP (x) ∀f, g ∈ H\nThen, N ( ,H, dL1(P,d)) is the covering number for H for the pseudo metric dL1(P,d). However, to be more general, the capacity of function spaces provides a “distribution-free” notion of covering number. The capacity C( ,H, d) is:\nC( ,H, d) = supP {N ( ,H, dL1(P,d))}\nBound on capacity: We need to bound C( ,H, dl̄1) in order to use the result of Theorem 1 to obtain sample complexity. Haussler(1992) provides an useful technique to bound the capacity for vector valued function space H. Given k functions spaces H1, . . . ,Hk with functions from X to Ai, they define the free product function space ×iHi with functions from X to A = A1 × . . . Ak as ×iHi = {〈h1, . . . , hk〉 | hi ∈ Hi}, where 〈h1, . . . , hk〉(x) = 〈h1(x), . . . , hk(x)〉. They show that:\nC( ,×iHi, dl̄1) < ∏k i=1 C( ,Hi, dl1) (2)\nParametric case: Recall that the hypothesis function h has T − 1 component functions w1xiT + ciT . However, the same weight w1 in all component functions implies that H is not a free product of component function spaces, hence we cannot use Eq. 2. Thus, we decompose H into a direct sum of two functions spaces, each of which capture w1xiT and ciT respectively. Thus, we define the direct-sum semifree product of functions spaces G ⊂ ×iGi and ×iFi as G⊕×iFi = {〈g1+f1, . . . , gT−1+fT−1〉 | 〈g1, . . . , gT−1〉 ∈ G, 〈f1, . . . , fT−1〉 ∈ ×iFi}. We prove the following: Lemma 1. IfH is a direct-sum semi-free product G ⊕×iFi C( ,H, dl̄1) < C( /2,G, dl̄1) ∏T−1 i=1 C( /2,Fi, dl1)\nFor our case, Gi contains functions of the form wxiT (w taking different values for different gi ∈ Gi). A function 〈g1, . . . , gT−1〉 ∈ ×iGi can have different weights for each component gi, and thus we consider the subset G = {〈g1, . . . , gT−1〉 | 〈g1, . . . , gT−1〉 ∈ ×iGi, same coefficient w for all gi}. Fi contains constant valued functions of the form ciT (ciT different for different functions fi ∈ Fi). Then, H = G ⊕ ×iFi. We assume the range of all these functions (gi, fi) to be [−M4 , M 4 ] (so that their sum hi lies in [−M2 , M 2 ]). First, we prove that Lemma 2. C( ,G, dl̄1) ≤M/4 and C( ,Fi, dl1) ≤M/4 . Then, plugging the result of Lemma 2 (substituting /2 for ) into Lemma 1 we obtain C( ,H, dl̄1) < (M/2 ) T . Having bounded C( ,H, dl̄1), we use Theorem 1 to obtain\nTheorem 2. The generalized SUQR parametric hypothesis classH is (α, δ)-PAC learnable with sample complexity4\nO ( (1/α2)(log(1/δ) + T log(T/α)) ) The above result shows a modest T log T growth of sample complexity with increasing targets, suggesting the parametric approach is suited for low amount of data; however, the simplicity of the functions captured by this approach (compared to NPL) results in lower accuracy with increasing data, as shown later in our experiments on real-world data.\nNPL case: Consider the functions spaces Hi consisting of real valued L-Lipschitz functions where L ≤ K̂. Recall that H for the NPL case is defined such that each hi is LLipschitz where L ≤ K̂. Thus, H = ×iHi. Then, using Equation 2: C( ,H, dl̄1) ≤ ∏T−1 i=1 C( ,Hi, dl1).\nNext, our task is to bound C( ,Hi, dl1). Consider the sup-distance metric between real valued functions: dl∞(hi, h ′ i) = supX |hi(x) − h′i(x)| for hi, h′i ∈ Hi. Note that dl∞ is independent of P , and for all functions hi, h ′ i and any P , dL1(P,dl1 )(hi, h ′ i) ≤ dl∞(hi, h′i). Thus, we infer (Haussler 1992) that for all P , N ( ,Hi, dL1(P,dl1 )) ≤ N ( ,Hi, dl∞) and then taking sup over P (recall C( ,Hi, dl1) = supP {N ( ,Hi, dL1(P,dl1 ))}) we get\nC( ,Hi, dl1) ≤ N ( ,Hi, dl∞) (3)\nWe bound N ( ,Hi, dl∞) in terms of the covering number for X (recall X = {x | x ∈ [0, 1]T , ∑ i xi ≤ K}) using results from (Tikhomirov and Kolmogorov 1993).\nLemma 3. N ( ,Hi, dl∞) ≤ ( 2 ⌈ M ⌉ + 1 ) · 2N ( 2K̂ ,X,dl∞ )\nTo use the above result, we still need to bound N ( ,X, dl∞). We do so by combining two remarkable results about Eulerian number 〈 T k 〉 (Knuth 1998) (k integral). • Laplace (1886) (Stanley 1977) discovered that the volume of Xk = {x|x ∈ [0, 1]T , k − 1 ≤ ∑ i xi ≤ k}\nis 〈 T k 〉 /T !. Thus, if XK = ∪Kk=1Xk, then vol(XK) =∑K\nk=1 vol(Xk) = ∑K k=1 〈 T k 〉 /T !.\n• Also, it is known (Tanny 1973) that 〈 T k 〉 /T ! = FT (k) −\nFT (k − 1), where FT (x) is the CDF of the probability distribution of ST = U1 + . . . + UT and each Ui is a uniform random variable on [0, 1).\nCombining these results, vol(XK+1) = FT (K + 1). The volume of a l∞ ball of radius (l∞ ball is a hypercube) is (2 )T (Wang 2005). The number of balls that fit tightly (aligned with the axes) and completely inside XK+1 is bounded by FT (K + 1)/(2 )T . Since << 1, these balls cover XK = X completely and the tight packing ensures that the center of the balls forms an -cover for X . Then, bounding FT (K + 1) using Bernstein’s inequality we get:\nLemma 4. For K + 1 ≤ 0.5T (recall K << T )\nN ( ,X, dl∞) ≤ e −3T (0.5−(K+1)/T )2 1−(K+1)/T /(2 )T\n4In the Appendix, we show that for standard SUQR (simpler than our generalized SUQR) the sample size is O ( 1 α2 (log 1 δ + log T α ) )\nPlugging the above result into Lemma 3 and then using that in Eq. 3, we bound C( ,Hi, dl1). Finally, Eq. 2 gives a bound on C( ,H, dl̄1) that we use in Theorem 1 to obtain Theorem 3. The non-parametric hypothesis class H is a (α, δ)-PAC learnable with sample complexity\nO ( (1/α2)(log(1/δ) + (TT+1/αT )) ) The above result shows that the sample complexity for NPL grows fast with T suggesting that NPL may not be the right approach to use when the number of targets is large."
    }, {
      "heading" : "5 Empirical Risk Minimizer",
      "text" : "As stated earlier, our loss function was designed so that empirical risk minimizing was same as maximizing log likelihood of data. Indeed, in the parametric case, the standard MLE approach can be used as is and has been used in literature. However, given no parameters, maximizing likelihood will provide values h(x) for mixed strategies x in the training data only. So we present a two step ERM algorithm for the NPL case. In the first step, we estimate the most likely value for hi(x) (for each i) for each x in the training data, ensuring that for any pair x, x′ in the training data, |hi(x)−hi(x′)| ≤ K̂||x−x′||1. In the second step, we construct hi with the least Lipschitz constant such that hi takes the values for the training data output by the first step.\nMore formally, assume the training data has s unique values for x in the training set and let these values be x1, . . . , xs. Further, let there be nj distinct data points against xj , i.e., nj attacks against mixed strategy xj . Denote by nj,i the number of attacks at each target i when xj was used. Let hij be the variable that stands for the estimate of value hi(xj); i ∈ {1, . . . , T}, j ∈ {1, . . . , s}. Fix hTj = 0 for all j. Then, probability of attack on target i against mixed strategy xj is given by qij = e\nhij∑ i e hij . Thus, the log\nlikelihood of the training data is ∑s j=1 ∑T i=1 nj,i log qij .\nLet Lip(K̂) denote the set of L-Lipschitz functions with L ≤ K̂. Using our assumption that hi ∈ Lip(K̂), the following optimization problem provides the most likely hij :\nmaxhij ∑s j=1 ∑T i=1 nj,i log\nehij∑ i e hij\nsubject to ∀i, j, j′, |hij − hij′ | ≤ 4K̂||xj − xj ′ ||1\n∀i, j, −M/2 ≤ hij ≤M/2\nGiven solution h∗ij to the above problem, we wish to construct the solution hi such that its Lipschitz constant (given by Khi ) is the lowest possible subject to hi taking the value h∗ij for x\nj . Such a construction provides the most smoothly varying solution given the training data, i.e., we do not assume any more sharp changes in the adversary response than what the training data provides.\nminhi∈Lip(K̂)Khi subject to ∀i, j. hi(x j) = h∗ij (MinLip)\nThe above optimization is impractical to solve computationally as uncountably many constraints are required to relate Khi to hi, Fortunately, we obtain an analytical solution:\nLemma 5. The following is a solution for problem MinLip\nhi(x) = minj{h∗ij +K∗i ||x− xj ||1}\nwhere K∗i = maxj,j′:j 6=j′ |h∗ij − h∗ij′ |/||xj − xj ′ ||1\nNote that hi(xj) = h∗ij . Thus, the value of hi(x) for a x not in the training set and close to xj is quite likely be the hi(x j) plus the scaled distance K∗i ||x− xj ||1."
    }, {
      "heading" : "6 Utility Bounds",
      "text" : "Next, we bound the difference between the optimal utility and the utility derived from planning using the learned h. The utility bound is same for the parametric and NPL case. First, we transform the PAC learning guarantee about the risk of output h to a bound on ||qp(x) − qh(x)||1: the difference between the true distribution qp and predicted distribution qh of attacks. As the PAC guarantee only bounds the risk between h and h∗, in order to relate qp and qh, the lemma below assumes a bounded KL divergence between qh ∗\nand qp. Lemma 6. Assume E[KL(qp(x) || qh∗(x))] ≤ ∗. Given an ERM A with output h = A(~z) and guarantee Pr(|rh(p) − rh∗(p)| < α) > 1 − δ, with prob. ≥ 1 − δ over training samples ~z we have\nPr(||qp(x)− qh(x)||1 ≤ √ 2∆) ≥ 1−∆\nwhere ∆ = (α+ ∗)1/3 and x is sampled using density p. Utility bound: Next, we provide an utility bound, given the above guarantee about learned h. Let the optimal strategy computed using h be x̃, i.e., x̃TUqh(x̃) ≥ x′TUqh(x′) for all x′. Let the true optimal mixed strategy of defender be x∗ (optimal with respect to qp(x)), i.e., the maximum defender’s utility is x∗TUqp(x∗). LetB(x, ) denote the l1 ball of radius around x. We make the following assumptions: (1) hi is K̂-Lipschitz ∀i and qp is K-Lipschitz in l1 norm, (2) ∃ small such that Pr(x ∈ B(x∗, )) > ∆ over choice of x using p and (3) ∃ small such that Pr(x ∈ B(x̃, )) > ∆ over choice of x using p. While the first assumption is mild, the last two assumptions for small means that the points x∗ and x̃ must not lie in low density regions of the distribution used to sample the training points. The following lemma relates assumption (1) to Lipschitzness of qh.\nLemma 7. If hi is K̂-Lipschitz then ∀x, x′ ∈ X. ||qh(x) − qh(x′)||1 ≤ 3K̂||x− x′||1, i.e., qh(x) is 3K̂-Lipschitz.\nThen, we can prove the following: Theorem 4. Given above assumptions and the results of Lemma 6 and 7, with prob.≥ 1−δ over the training samples the expected utility x̃TUqh(x̃) for the learned h is at least\nx∗TUqp(x∗)− (K + 1) − 2 √ 2∆− 6K̂"
    }, {
      "heading" : "7 Experimental Results",
      "text" : "We show experimental results on two datasets: (i) real-world poaching data from QENP (obtained from (Nguyen et al. 2015)); (ii) data from human subjects experiments on AMT\n(obtained from (Kar et al. 2015)), to estimate prediction errors and the amount of data required to reduce the error for both the parametric and NPL learning settings. Also, we compare the NPL approach with the standard SUQR approach and show that the NPL approach, while computationally slow, outperforms SUQR for Uganda data. We compare with SUQR as it is widely used in literature; for completeness we also have experiments for the generalized SUQR model in the Appendix, for which the performance is in between the NPL and standard SUQR.\nFor each dataset, we conduct four experiments with 25%, 50%, 75% and 100% of the original data. We create 100 train-test splits in each of the four experiments per dataset. For each train-test split we compute the average prediction error α (average difference between the log-likelihoods of predicted and actual attack probabilities). We report the 1−δ percentile of these 100 α values, e.g., reported α = 2.09 for δ = 0.1 means that 90 of the 100 test splits have α < 2.09.\nReal-World Poaching data: We first present results of our experiments with real-world poaching data. This dataset contained information about features such as ranger patrols and animal densities and the poachers’ attacks (655 attacks) in response for 2012 at QENP. It is important to note that this data set is extremely noisy because of: (i) Missing observations: all the poaching events are not recorded because the limited number of rangers cannot patrol all the areas in this park all the time; (ii) Uncertain feature values: the animal density feature is also based on incomplete observations of animals; (iii) Uncertain defender strategy: the actual defender mixed strategy is unknown, and hence, we estimate the mixed strategies based on the provided patrol data.\nFirst, to provide a baseline for our error measures, we use the same coarse-grained prediction approach as reported by Nguyen (2015), in which the authors predict whether a target will be attacked or not. Their approach provides area under curve (of a ROC curve) value of 0.73, which is another view\nof our α, δ metric for the coarse grained approach. The results for coarse-grained predictions are shown in Figs. 1(a) and 1(b). Next, in the fine-grained prediction approach we predict the actual number of attacks on each target in our test set; these results are shown in Figs. 1(c) and 1(d).\nWe observe that: (i) α decreases with increasing sample size at a rate proportional to 1√\nm , where m is the number of\nsamples (for non-parametric case and fine-grained prediction, goodness-of-fit, i.e., r2=0.9) — this observation supports the relationship between α andm shown in Theorem 3 and can be used to approximately infer the number of samples required to reduce the prediction error to a certain value, e.g., assuming we collect same number of samples (=655) per year, to reduce α (α is in log-scale) from 2.1 to 1.85, we would require two more years of data; (ii) α values for finegrained predictions are understandably higher than those for coarse-grained predictions because in the fine-grained case we predict the exact number of attacks; (iii) Our NPL model performs better than its parametric counterpart in predicting future poaching attacks for the fine-grained case, indicating that the true adversary behavior model may indeed be more complicated than what can be captured by SUQR.\nAMT data: Here we show fine-grained prediction results on real-world AMT data to demonstrate the performance of both our approaches on somewhat cleaner data. This dataset is cleaner than the Uganda data because: (i) all attacks are observed, and (ii) animal densities and deployed defender strategies are known. The dataset consisted of 16 unique mixed strategies. We used attack data corresponding to 11 randomly chosen mixed strategies for training and data for the remaining mixed strategies for testing. Results are shown in Figs. 1(e) and 1(f). We observe that: (i) α values in this case are lower as compared to the Uganda data as the AMT data is cleaner; and (ii) the NPL model’s performance on this dataset is poor as compared to SUQR due to, (a) low number of samples in AMT data, and (b) real-world poacher behavior may be more complicated than that of AMT participants and hence SUQR in this case was able to better capture AMT participants’ behavior with limited number of samples.5\nRuntime: While running on Matlab R2015a on an Intel Core i7-5500 CPU@2.40Ghz, 8GB RAM machine with a 64-bit Windows 10, on average, the NPL computation takes longer than the parametric setting, as shown in Table 2."
    }, {
      "heading" : "8 Conclusion",
      "text" : "We presented a PAC analysis of the SUQR model. We also presented the first non-parametric learning model for SSGs with a PAC analysis of the same. Our experiments with poaching data and simulated data show that NPL outperforms SUQR with increasing data, but is computationally\n5In the appendix we show that, on simulated data, α does indeed approach zero and NPL outperforms SUQR with enough samples.\ncostly. Finally, we hope that we have laid fertile ground for interesting future research."
    }, {
      "heading" : "A Proofs",
      "text" : ""
    }, {
      "heading" : "Proof of Theorem 1",
      "text" : "Proof. First, Haussler uses the following pseudo metric ρ on A that is defined using the loss function l:\nρ(a, b) = maxy∈Y |l(y, a)− l(y, b)|.\nTo start with, relying heavily on Haussler’s result, we show\nPr(∀h ∈ H.|r̂h(~z)−rh(p)| < α\n3 ) ≥ 1−4C ( α 48 ,H, ρ ) e− α2m 576M2\nChoose α = α′/4M and ν = 2M in Theorem 9 of (Haussler 1992). Using property (3) (Section 2.2, (Haussler 1992)) of dv we obtain |r − s| ≤ whenever dv(r, s) ≤ α′. Using this directly in Theorem 9 of Haussler (1992) we obtain the desired result above.\nNote the dependence of the above probability on m (the number of samples), and compare it to the first precondition in the PAC learning result. By equating δ/2 to 4C(α/48,H, ρ)e− α2m\n576M2 , we derive the sample complexity as\nm ≥ 576M 2\nα2 log 8C(α/48,H, ρ) δ\nWe wish to compute a bound on C( ,H, ρ) in order to use the above result to obtain sample complexity. First, we prove that ρ ≤ 2Tdl̄1 for the loss function we use. This result is used to bound C( ,H, ρ), since, it is readily verified from definition that C( ,H, ρ) ≤ C( /2T,H, dl̄1). Such a bounding directly gives\nm ≥ 576M 2\nα2 log 8C(α/96T,H, ρ) δ\nBelow we prove that ρ ≤ 2Tdl̄1 .\nLemma 8. Given the loss function defined above, we have ρ(a, b) ≤ 2 maxi |ai − bi| ≤ 2 ∑ i |ai − bi| ≤ 2Tdl̄1(a, b)\nProof. By definition, ρ(a, b) = maxi ∣∣∣ − ai + bi +\nlog 1+\n∑T−1 i=1 e ai\n1+ ∑T−1 i=1 e bi ∣∣∣ ≤ maxi |ai−bi|+∣∣ log 1+∑T−1i=1 eai1+∑T−1i=1 ebi ∣∣∣. There is j and k such that maxr = e aj\nebj ≥ e\nai\nebi for all i and\nminr = eak ebk ≤ e ai ebi for all i. Thus,\nlog 1 +minrt\n1 + t ≤ log\n1 + ∑T−1 i=1 e ai\n1 + ∑T−1 i=1 e bi ≤ log 1 +maxrt 1 + t\nwhere t = ∑T−1 i=1 e\nbi . The greatest positive value of the RHS is logmaxr ≤ |aj−bj | and least negative value possible for LHS is logminr ≥ −|ak − bk|. Thus,∣∣ log 1 +∑T−1i=1 eai\n1 + ∑T−1 i=1 e bi ∣∣ ≤ max i ∣∣ai − bi∣∣ Hence, we obtain ρ(a, b) = maxi |l(yi, a) − l(yi, b)| ≤ 2 maxi |ai − bi|, and the last inequality is trivial.\nThus, using the above result we get\nm ≥ 576M 2\nα2 log 8C(α/96T,H, dl̄1) δ"
    }, {
      "heading" : "Proof of Lemma 1",
      "text" : "Proof. Fix any probability distribution over X , say P . For brevity, we write k instead of T − 1. Consider an /2-cover Ui for each Fi; also let V be an /2-cover for G. We claim that U1×, . . . Uk × V is an -cover for G ⊕ ×iFi. Thus, the size of the -cover for G ⊕ ×iFi is bounded by |V | ∏ i |Ui|. With this claim we get\nN ( ,G ⊕ ×iFi, dL1(P,dl̄1 )) < N ( /2,G, dL1(P,dl̄1 ))∏k i=1N ( /2,Fi, dL1(P,dl1 ))\nTaking sup over probability distributions we get our desired result. Now, we prove our claim about -cover. Take any function h = 〈g1 + f1, . . . gk + fk〉. Find functions f ′i ∈ Ui such that dL1(P,dl1 )(fi, f ′ i) < /2. Similarly, find function g′ = 〈g′1, . . . g′k〉 ∈ V such that dL1(P,dl̄1 )(g, g ′) < /2 where g = 〈g1, . . . gk〉. Let h′ = 〈g′1 + f ′1, . . . g′k + f ′k〉. Then,\ndL1(P,dl̄1 ) (h, h′)\n= ∫ X 1 k k∑ i=1 dl1(gi(x) + fi(x), g ′ i(x) + f ′ i(x)) dP (x)\n≤ ∫ X 1 k k∑ i=1 dl1(gi(x), g ′ i(x)) + dl1(fi(x), f ′ i(x)) dP (x)\n= dL1(P,dl̄1 ) (g, g′) +\n1\nk k∑ i=1 dL1(P,dl̄1 ) (fi, f ′ i)\n< /2 + /2 ="
    }, {
      "heading" : "Proof of Lemma 2",
      "text" : "Proof. First, note that xiT = xi − xT lies between [−1, 1] due to the constraints on xi, xT . Then, for any two functions g, g′ ∈ G we have the following result:\ndL1(P,dl̄1 ) (g, g′) =\n= ∫ X 1 T − 1 T−1∑ i=1 dl1(w(xi − xT ), w′(xi − xT )) dP (x)\n= ∫ X 1 T − 1 T−1∑ i=1 |(w − w′)(xi − xT )| dP (x)\n≤ ∫ X 1 T − 1 T−1∑ i=1 |(w − w′)| dP (x) = |(w − w′)|\nAlso, note that since the range of any g = w(xi − xT ) is [−M4 , M 4 ] and given xi − xT lies between [−1, 1], we can claim that w lies between [−M4 , M 4 ]. Thus, given the distance between functions is bounded by the difference in\nweights, it enough to divide the M/2 range of the weights into intervals of size 2 and consider functions at the boundaries. Hence the -cover has at most M/4 .\nThe proof for constant valued functions Fi is similar, since its straightforward to see the distance between two functions in this space is the difference in the constant output. Also, the constants lie in the range [−M4 , M 4 ], Then, the argument is same as the G case."
    }, {
      "heading" : "Proof of Lemma 3",
      "text" : "Proof. First, the space of functionsH = {h/K̂ | h ∈ Hi} is Lipschitz with constant ≤ 1 and |hi(x)| ≤ M/2K̂. Clearly N ( ,Hi, dl∞) ≤ N ( /K̂,H, dl∞). Using the following result from (Tikhomirov and Kolmogorov 1993): for any Lipschitz real valued function spaceH with constant 1, any positive integer s and any distance d N ( ,H, dl∞) ≤ ( 2 ⌈M(s+ 1)\n2K̂\n⌉ + 1 ) · (s+ 1)N ( s s+1 ,X,d)\nThen, we get the bound on N ( /K̂,H, dl∞) by choosing s = 1 and d = dl∞ , and hence obtain the desired bound on N ( ,Hi, dl∞)."
    }, {
      "heading" : "Proof of Lemma 4",
      "text" : "Proof. For ease of notation, we do the proof with k standing for K + 1. Let Yi = Ui − 0.5, then |Y | ≤ 1/2 and ST − 0.5T = ∑ i Yi. Using Bernstein’s inequality with the fact that E[Y 2i ] = 1/12\nP ( ∑ i Yi = ST − 0.5T ≤ −t) ≤ e −0.5t2 T/12+t/6\nThus, P (ST ≤ 0.5T − t) ≤ e 0.5t2\nT/12+t/6 . Take k = 0.5T − t, and hence t = 0.5T − k = T (0.5− k/T ). Hence,\nP (ST ≤ k) ≤ e −3T (0.5−k/T )2 1−k/T"
    }, {
      "heading" : "Proof of Theorem 3",
      "text" : "Proof. Given the results of Lemma 3, we get the sample complexity is of order\n1\nα2\n( log 1\nδ + T\n( N (α\nT ,X, dl1) )) Now, suing result of Lemma 4, we get the required order in the Theorem. We wish to note that if K/T is a constant then theO(e−T ) in Lemma 4 gets swamped by the TT term. However, in practice for fixed T , this term does provide lower actual complexity bound than what is indicated by the order.\nProof of Lemma 5\nProof. Observe that any solution to MinLip will have Lipschitz constant ≥ K∗. Thus, it suffices to show that the Lipschitz constant of hi is K∗, to prove that hi is a solution of MinLip. Take any two x, x′. If the min in the expression for hi occurs for the same j for both x, x′ then |hi(x)− hi(x′)| is given by K∗|||x− xj ||1 − ||x′ − xj ||1|. By application of triangle inequality\n−||x− x′||1 ≤ ||x− xj ||1 − ||x′ − xj ||1 ≤ ||x− x′||1 Thus, |hi(x)− hi(x′)| ≤ K∗||x− x′||1.\nFor the other case when the min for x occurs at some j and min for x′ at some j′ we have the following: hi(x′) = hij′ + K ∗||x′ − xj′ ||1 and hi(x) = hij + K∗||x − xj ′ ||1 . Also, due to the min, hi(x′) ≤ hij + K∗||x′ − xj ||1 = hi(x) +K ∗||x′ − xj ||1 −K∗||x− xj ′ ||1. Thus, we get\nhi(x ′)−hi(x) ≤ K∗(||x′−xj ||1−||x−xj ||1) ≤ K∗||x′−x||1\nUsing the symmetric case inequality for x we get\nhi(x)−hi(x′) ≤ K∗(||x−xj ||1−||x′−xj ||1) ≤ K∗||x−x′||1 Combining both these we can claim that |hi(x)− hi(x′)| ≤ K∗||x′−x||1. Thus, we have proved the sufficient condition that hi is K∗ Lipschitz."
    }, {
      "heading" : "Proof of Lemma 6",
      "text" : "Proof. Let pX be the marginal of p(x, y) for space X . Define the expected entropy E[H(x)] = ∫ pX(x) ∑T i=1 Iy=tiq p i (x) log q p i (x) dx.\nGiven the loss function, we know that rh(p) = − ∫ p(x, y) ∑T i=1 Iy=ti log q h i (x) dx dy. This is same as\n− ∫ pX(x) ∑T i=1 Iy=tiq p i (x) ∑T i=1 Iy=ti log q h i (x) dx dy.\nThis reduces to− ∫ pX(x) ∑T i=1 Iy=tiq p i (x) log q h i (x) dx dy. Thus, we have\nE[H(x)]+rh(p) = ∫ pX(x) T∑ i=1 Iy=tiq p i (x) log qpi (x) qhi (x) dx dy\nHence, we obtain\nE[H(x)] + rh(p) = E[KL(qp(x) || qh(x))]\nHence, |rh(p)− rh∗(p)| is equal to\n|E[KL(qp(x) || qh(x))]− E[KL(qp(x) || q∗(x))]|\nThus, from the assumptions, we get E[KL(qp(x) || qh(x))] ≤ α + ∗ with probability ≥ 1 − δ. Next, using Markov inequality, with probability ≥ 1− δ\nPr(KL(qp(x) || qh(x)) ≥ (α+ ∗)2/3) ≤ (α+ ∗)1/3\nthat is using the notation ∆ = (α+ ∗)1/3, with probability ≥ 1− δ\nPr(KL(qp(x) || qh(x)) ≤ ∆2/3) ≥ 1−∆1/3\nUsing Pinkser’s inequality we get (1/2)||qp(x) − qh(x)||21 ≤ KL(qp(x)||qh(x)). That is, the event KL(qp(x) || qh(x)) ≤ ∆2/3 implies the event ||qp(x) −\nqh(x)||1 ≤ √ 2∆. Thus, Pr(||qp(x)− qh(x)||1 ≤ √\n2∆) ≥ Pr(KL(qp(x) || qh(x)) ≤ ∆2/3). Thus, we obtain: with probability ≥ 1 − δ, Pr(||qp(x) − qh(x)||1 ≤ √ 2∆) ≥ 1−∆."
    }, {
      "heading" : "Proof of Lemma 7",
      "text" : "Proof. We know that qhi (x) = ehi(x)∑ j e hj(x) (assume hT (x) = 0). Thus,\n|qhi (x)− qhi (x′)| = qhi (x′)|ehi(x)−hi(x ′)\n∑ j e hj(x\n′)∑ j e hj(x) − 1|\nLet r denote ∑ j e hj(x\n′)∑ j e hj(x) . There is l and k such that\nmaxr = ehl(x\n′)\nehl(x) ≥ e\nhj(x ′)\nehj(x) for all j and minr = e\nhk(x ′)\nehk(x) ≤\nehj(x ′) ehj(x) for all j. Then, minr ≤ r ≤ maxr First, note that due to our assumption that for each i |hi(x′) − hi(x)| ≤ K̂||x′ − x||1, we have\ne−K̂||x ′−x||1 ≤ minr ≤ r ≤ maxr ≤ eK̂||x ′−x||1\nUsing the Lipschitzness we can also claim that e−K̂||x ′−x||1 ≤ ehi(x)−hi(x′) ≤ eK̂||x′−x||1 . Thus,\ne−2K̂||x ′−x||1 ≤ ehi(x)−hi(x ′) · r ≤ e2K̂||x ′−x||1\nSince, e−2K̂||x ′−x||1 < 1 and e2K̂||x ′−x||1 > 1 we have\n|ehi(x)−hi(x ′)r−1| ≤ max(|e−2K̂||x ′−x||1−1|, |e2K̂||x ′−x||1−1|)\nAlso, it is a fact that |ey−1| ≤ 1.5|y| for |y| ≤ 3/4. Thus, we obtain\n|ehi(x)−hi(x ′)r−1| ≤ 3K̂||x′−x||1 for 2K̂||x′−x||1 ≤ 3/4 Thus, ||qh(x′) − qh(x)||1 = ∑ i |qhi (x) −\nqhi (x ′)| = ∑ i q h i (x ′)|ehi(x)−hi(x′) ∑ j e hj(x ′)∑ j e hj(x) − 1| ≤\n( ∑ i q h i (x ′))3K̂||x′ − x||1 for K̂||x′ − x||1 ≤ 3/8. Since∑\ni q h i (x ′) = 1, we have\n||qh(x′)− qh(x)||1 ≤ 3K̂||x′−x||1 for ||x′−x||1 ≤ 3/8K̂\nIn other words qh is locally 3K̂-Lipschitz for every l1 norm ball of size 3/8K̂. The following allows us to prove global Lipschitzness.\nLemma 9. Any locally L-Lipschitz function f for every lp ball of size δ0 on a compact convex set X ⊂ Rn is Lipschitz on the set X . The Lipschitz constant is also L.\nProof. Take any two points x, y ∈ X , the straight line joining x, y lies in X (as X is convex). Also, a finite number of balls of size δ0 cover X (due to compactness). Thus, there are finitely many points x = z1, . . . , zµ = y on the line from x, y such that dlp(zi, zi+1) ≤ δ0. Further, since these points lie on a straight line we have\ndlp(x, y) = ∑µ−1 1 dlp(zi, zi+1)\nThen, let any metric d be used to measure distance in the range space of f , thus, we get\nd(f(x), f(y)) ≤ ∑µ−1\n1 d(f(zi), f(zi+1)) ≤ ∑µ−1\n1 Ldlp(zi, zi+1) = Ldlp(x, y)\nSince in our case the defender mixed strategy space is compact and convex and qh(x) satisfies the above lemma with L = 3K̂ and δ0 = 3/8K̂, qh(x) is 3K̂-Lipschitz."
    }, {
      "heading" : "Proof of Theorem 4",
      "text" : "Proof. Coupled with the guarantee that with prob. ≥ 1− δ, Pr(||qp(x) − qh(x)||1 ≤ √ 2∆) ≥ 1 − ∆, the assumptions guarantee that with prob. ≥ 1 − δ for the learned hypothesis h there must exist a x′ ∈ B(x∗, ) such that ||qp(x′) − qh(x′)||1 ≤ √ 2∆ and there must exist x′′ ∈\nB(x̃, ) such that ||qp(x′′)− qh(x′′)||1 ≤ √\n2∆. First, for notational ease let γ denote √ 2∆. The following are immediate using triangle inequality, with the results ||qp(x′)− qh(x′)||1 ≤ γ and ||qp(x′′)− qh(x′′)||1 ≤ γ and the Lipschitzness assumptions\n||qp(x∗)− qh(x′)||1 ≤ K + γ (optx∗) ||qp(x̃)− qh(x′′)||1 ≤ 3K̂ + γ (optx̃)\nWe call x̃TUqh(x̃) ≥ x′TUqh(x′) as equation opth. Thus, we bound the utility loss as following\nx∗TUqp(x∗)− x̃TUqp(x̃) = x∗TUqp(x∗)− x̃TUqh(x̃) + x̃TUqh(x̃)− x̃TUp(y/x̃) ≤ x∗TUqp(x∗)− x′TUqh(x′) + x̃TUqh(x̃)− x̃TUp(y/x̃) using opth = (x∗ − x′)TUqp(x∗) + x′TU(qp(x∗)− qh(x′))+ x̃TUqh(x̃)− x̃TUqp(x̃) ≤ + (K + γ) + x̃TUqh(x̃)− x̃TUqp(x̃) using x′ ∈ B(x∗, ), optx∗ = ((K + 1) + γ) + x̃TU(qh(x̃)− qh(x′′))+ x̃TU(qh(x′′)− qp(x̃))\n≤ (K + 1) + γ + 6K̂ + γ using x′′ ∈ B(x̃, ) with Lipschitz qh, optx̃"
    }, {
      "heading" : "B Extension to Stackelberg Games",
      "text" : "Our technique extends to Stackelberg games by noting that the single resource case K = 1 with T − 1 targets gives∑T−1 i=1 xi ≤ 1. This directly maps to a probability distribu-\ntion over T actions. The xi’s with xT = 1− ∑T−1 i=1 xi is the probability of playing an action. With this set-up now the security game is a standard Stackelberg game, but where the leader has T actions and follower has T − 1 actions.\nThus, in order to capture the general Stakelberg game, for the adversary, we assume N actions for the adversary (instead of T − 1 above). Then, similar to security games\nq1, . . . , qN denotes the adversary’s probability of playing an action. Thus, the function h now outputs vectors of size N − 1 (instead of O(T )), i.e., A is a subset of N − 1 dimensional Euclidean space. The model of security game in the PAC framework extends as is to this Stackelberg setup, just with h(x) and A being N − 1 dimensional. The rest of the analysis proceeds exactly as for security games for both parametric and non-parametric case, by replacing the T corresponding to the adversary’s action space by N . Since, the proof technique is exactly same, we just state the final results. Thus, for a Stackelberg game with T leader actions and N follower actions, the bound for Theorem 1 becomes\n576M2\nα2 log 8C(α/96N,H, dl̄1) δ\nIt can be seen from the proof for the parametric part that the sample complexity does not depend on the dimensionality of X , but only on the dimensionality of A. Hence, the sample complexity results from generalized SUQR parametric case is\nO ( 1 α2 (log 1 δ +N log N α ) )\nand for the non-parametric case, which depends on both dimensionality of X and T , the sample complexity is\nO ( 1 α2 (log 1 δ + NT+1 αT ) )"
    }, {
      "heading" : "C Analysis of Standard SUQR form",
      "text" : "For SUQR the rewards and penalties are given and fixed. Let the rewards be given and fixed r = 〈r1, . . . , rT 〉 (each ri ∈ [0, rmax], rmax > 0), and the penalty values are p = 〈p1, . . . , pT 〉 (each pi ∈ [0, pmin], pmin < 0). Thus, the output of h is\nh(x) = 〈w1x1T + w2r1T + w3p1T , . . . , w1xT−1T + w2rT−1T + w3pT−1T 〉\nwhere riT = ri − rT and same for piT . Note that in the above formulation all the component functions hi(x) have same weights. We can consider the function space H as the following direct-sum semi-free product G ⊕ F ⊕ E = {〈g1 + f1 + e1, . . . , gT−1 + fT−1 + eT−1〉 | 〈g1, . . . , gT−1〉 ∈ G, 〈f1, . . . , fT−1〉 ∈ F , 〈e1, . . . , eT−1〉 ∈ E}, where each of G,F , E is defined below. G = {〈g1, . . . , gT−1〉 | 〈g1, . . . , gT−1〉 ∈ ×iGi, all gi have same weight} where Gi has functions of the form wxiT . F = {〈f1, . . . , fT−1〉 | 〈f1, . . . , fT−1〉 ∈ ×iFi, all fi have same weight} where Fi has constant valued functions of the form wriT . E = {〈e1, . . . , eT−1〉 | 〈e1, . . . , eT−1〉 ∈ ×iEi, all ei have same weight} where Ei has constant valued functions of the form wpiT .\nConsider an /3-cover Ue for E , an /3-cover Uf for F and /3-cover Ug for G. We claim that Ue × Uf × Ug is an -cover for E ⊕ F ⊕ G. Thus, the size of the -cover for E ⊕ F ⊕ G is bounded by |Ue||Uf ||Ug|. Thus,\nN ( ,H, dl̄1) ≤ N ( /3,G, dl̄1)N ( /3,F , dl̄1)N ( /3, E , dl̄1)\nTaking sup over P we get\nC( ,H, dl̄1) ≤ C( /3,G, dl̄1)C( /3,F , dl̄1)C( /3, E , dl̄1)\nNow, we show that Ue × Uf × Ug is an -cover for H = E⊕F⊕G Fix any h ∈ H = E⊕F⊕G. Then, h = e+f+g for some e ∈ E , f ∈ F , g ∈ G. Let e′ ∈ Ue be /3 close to e, f ′ ∈ Uf be /3 close to f and g′ ∈ Ug be /3 close to g.\nThen,\ndL1(P,dl̄1 ) (h, h′)\n= ∫ X 1 k k∑ i=1 dl1(hi(x), h ′ i(x)) dP (x)\n≤ ∫ X 1 k k∑ i=1 dl1(gi(x), g ′ i(x))\n+dl1(fi(x), f ′ i(x)) + dl1(ei(x), e ′ i(x)) dP (x)\n= dL1(P,dl̄1 ) (g, g′) + dL1(P,dl̄1 ) (f, f ′) + dL1(P,dl̄1 ) (e, e′) ≤\nSimilar to Lemma 2, it is possible to show that for any probability distribution P , for any function g, g′ dl̄1(g, g\n′) ≤ |w − w′| and f, f ′ dl̄1(f, f\n′) ≤ |w − w′|rmax and e, e′ dl̄1(e, e\n′) ≤ |w − w′||pmin|. Assume each of the functions have a range [−M/6,M/6] (this does not affect the order in terms of M ). Given, these ranges w for g can take values in [−M/6,M/6], w for g can take values in [−M/6rmax,M/6rmax] and w for g can take values in [−M/6|pmin|,M/6|pmin|]. To get a capacity of /3 it is enough to divide the respective w range into intervals of 2 /3, and consider the boundaries. This yields an /3- capacity ofM/2 ,M/2 rmax andM/2 |pmin| for G,F and E respectively.\nThus,\nC( ,H, dl̄1) ≤ (M/2 ) 3 1\nrmax|pmin| Plugging this in sample complexity from Theorem 1 we get the results that the sample complexity is\nO ( 1 α2 (log 1 δ + log T α ) )"
    }, {
      "heading" : "D Experimental Results",
      "text" : "Here we provide additional experimental results on the Uganda, AMT and simulated datasets. First, in Figs. 2(a) and 2(b), we provide results on the Uganda dataset for the generalized parametric case we analyze in our paper (i.e., generalized SUQR) for both the fine-grained and coarsegrained settings. The AMT dataset consisted of 32 unique mixed strategies, 16 of which were deployed for one payoff structure and the remaining 16 for another. In the main paper, we provided results on AMT data for payoff structure 1. Here, in Figs. 2(c) and 2(d), we show results on the AMT data for both the parametric (SUQR) and NPL learning settings on payoff structure 2.\nFor running experiments on simulated data, we used the same mixed strategies and features as for the AMT data, but simulated attacks, first using the actual SUQR model and then using a modified form of the SUQR model. Figs. 2(e)\nand 2(f) show results on simulated data on payoff structures 1 and 2 for the parametric cases, when the data is generated by an adversary with an SUQR model with true weight vector reported in Nguyen et. al (2013) ((w1, w2, w3) = (−9.85, 0.37, 0.15) (ci = w2Ri + w3Pi)). Similar results for the NPL model are shown in Figs. 2(g) and 2(h) respectively. We can see that the NPL approach performs poorly with only one or five samples as expectied but improves significantly as more samples are added. To further show its potential, we modified the true adversary model of generating attacks from SUQR to the following: qi ∝ ew1x 2 i+ci , i.e., instead of xi, the adversary reasons based on x2i . We considered the same true weight vector to simulate attacks. Then, we observe in Figs. 2(i) (for payoff structure 1) and 2(j) (for payoff structure 2 data), that α approaches a value closer to zero for 500 or more sample. Also, the NPL model performs better than the parametric model with 500 or more samples. This shows that the NPL approach is more accurate when the true adversary does not satisfy the simple parametric logistic form, indicating that when we don’t know the true function of the adversary’s decision making process, adopting a nonparametric method to learn the adversary’s behavior is more effective."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2017,
    "abstractText" : "Recent applications of Stackelberg Security Games (SSG), from green crime to urban crime, have employed machine learning tools to learn and predict adversary behavior using available data about defender-adversary interaction. We commit to an approach of directly learning the response function of the adversary and initiate a formal study of the learnability guarantees of the approach. We make three main contributions: (1) we formulate our approach theoretically in the PAC learning framework, (2) we analyze the PAC learnability of the known parametric SUQR model of bounded rationality and (3) we propose the first non-parametric class of response functions for SSGs and analyze its PAC learnability. Finally, we conduct experiments and report the real world performance of the learning methods.",
    "creator" : "LaTeX with hyperref package"
  }
}