{
  "name" : "1703.01030.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Deeply AggreVaTeD: Differentiable Imitation Learning for Sequential Prediction",
    "authors" : [ "Wen Sun", "Arun Venkatraman", "Geoffrey J. Gordon", "Byron Boots", "J. Andrew Bagnell" ],
    "emails" : [ "WENSUN@CS.CMU.EDU", "ARUNVENK@CS.CMU.EDU", "GGORDON@CS.CMU.EDU", "BBOOTS@CC.GATECH.EDU", "DBAGNELL@RI.CMU.EDU" ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "A fundamental challenge in artificial intelligence, robotics, and language processing is to reason, plan, and make a se-\nquence of decisions to minimize accumulated cost, achieve a long-term goal, or optimize for a loss acquired only after many predictions. Reinforcement Learning (RL), especially deep RL, has dramatically advanced the state of the art in sequential decision making in high-dimensional robotics control tasks as well as in playing video and board games (Schulman et al., 2015; Silver et al., 2016). Though conventional supervised learning of deep models has been pivotal in advancing performance in sequential prediction problems, researchers are beginning to utilize deep RL methods to achieve high performance (Ranzato et al., 2015; Bahdanau et al., 2016; Li et al., 2016). Often in sequential prediction tasks, future predictions from the learner are dependent on the history of previous predictions; thus, a poor prediction early on can yield high accumulated loss (cost) for future predictions. Viewing the predictor as a policy π, deep RL algorithms are able to reason about the future accumulated cost in a sequential decision making process whether it is a traditional robotics control problem or a sequential structured prediction task.\nIn contrast with reinforcement learning methods, wellknown imitation learning (IL) and sequential prediction algorithms such as SEARN (Daumé III et al., 2009), DaD (Venkatraman et al., 2015), AggreVaTe (Ross & Bagnell, 2014), and LOLS (Chang et al., 2015b) reduce the sequence prediction problem to supervised learning by leveraging one special property of the sequence prediction problem: at training time we usually have a (near) optimal costto-go oracle. At any point along the sequential prediction process (i.e. state or partially completed sequential prediction), the oracle is able to select the next (near)-best action.\nConcretely, the above methods assume access to an oracle1 that provides an optimal or near-optimal action and the future accumulated loss Q∗, also called the cost-to-\n1Expert, demonstrator, and oracle are used interchangeably.\nar X\niv :1\n70 3.\n01 03\n0v 1\n[ cs\n.L G\n] 3\nM ar\ngo. For robotics control problems, this oracle may come from a human expert guiding the robot during the training phase (Abbeel & Ng, 2004) or from an optimal MDP solver (Ross et al., 2011; Kahn et al., 2016) that either may be too slow to use at test time or leverages information unavailable at test time (e.g., ground truth). Similarly, for sequential prediction problems, an oracle can be constructed by optimization (e.g., beam search) or by a clairvoyant greedy algorithm (Daumé III et al., 2009; Ross et al., 2013; Rhinehart et al., 2015; Chang et al., 2015a) that is near-optimal on the task specific performance metric (e.g., cumulative reward, IoU, Unlabeled Attachment Score, BLEU) given the training data’s ground truth.\nSuch an oracle, however, is only available during training time (e.g., when there is access to ground truth). Thus, the goal of IL is to learn a policy π̂, with the help of the oracle (π∗, Q∗) during the training session, such that π̂ achieves similar quality performance at test time when the oracle is unavailable. In contrast to IL, reinforcement learning methods often initialize with an random policy π0 or cost-to-go (accumulated loss)Q0 predictor which may be far from the optimal. The optimal policy (or cost-to-go) must be found through a tradeoff of dithering, directed exploration, and exploitation.\nThe existence of oracle can be exploited to alleviate blind learning by trial and error: one can imitate the oracle to speed up learning process by significantly reducing exploration. A classic IL method is to collect data from running the demonstrator or oracle and train a regressor or classifier via supervised learning. These methods (Abbeel & Ng, 2004; Syed et al., 2008; Ratliff et al., 2006; Ziebart et al., 2008; Finn et al., 2016; Ho & Ermon, 2016) learn either a policy π̂∗ or Q̂∗ from a fixed-size dataset pre-collected from the oracle. A pernicious problem with these methods is that they require the training and test data to be sampled from the same distribution. This is very difficult to enforce in practice, and, as a result, policies learned by these methods can fail spectacularly in theory and in practice (Ross & Bagnell, 2010). Interactive approaches to IL such as SEARN (Daumé III et al., 2009), DAgger (Ross et al., 2011), and AggreVaTe (Ross & Bagnell, 2014) interleave learning and testing procedures to overcome the data mismatch issue and, as a result, work well in practical applications. Furthermore, these interactive approaches can provide strong theoretical guarantees between training time loss and test time performance through a reduction to no-regret online learning.\nIn this work, we introduce AggreVaTeD, a differentiable version of AggreVaTe (Aggregate Values to Imitate (Ross & Bagnell, 2014)) which extends interactive IL for use in sequential prediction and challenging continuous robot control tasks. We provide two gradient update procedures:\na regular gradient update developed from Online Gradient Descent (OGD) (Zinkevich, 2003) and a natural gradient update (Kakade, 2002; Bagnell & Schneider, 2003) which we show is closely related to Exponential Gradient Descent (EG), another popular no-regret algorithm that enjoys an almost dimension-free property (Bubeck et al., 2015).2\nAggreVaTeD leverages the oracle to learn rich polices that can be represented by complicated non-linear function approximators. Our experiments with deep neural networks on various robotics control simulators and on a dependency parsing sequential prediction task show that AggreVaTeD can achieve expert-level performance and even super-expert performance when the oracle is sub-optimal, a result rarely achieved by non-interactive IL approaches. The differentiable nature of AggreVaTeD additionally allows us to employ LSTM-based policies to handle partially observable settings (e.g., observe only partial robot state). Empirical results demonstrate that by leveraging an oracle, IL can learn much faster than RL in practice.\nIn addition to providing a set of practical algorithms, we develop a comprehensive theoretical study of IL on discrete MDPs. We construct an MDP that demonstrates exponentially better sample efficiency for IL than any RL algorithm. For general discrete MDPs, we provide a regret upper bound for AggreVaTeD with EG, which shows IL can learn dramatically faster than RL. We provide a regret lower bound for any IL algorithm, which demonstrates that AggreVaTeD with EG is near-optimal. Our experimental and the theoretical results support the proposition:\nImitation Learning is a more effective strategy than Reinforcement Learning for sequential prediction with near optimal cost-to-go oracles."
    }, {
      "heading" : "2. Preliminaries",
      "text" : "In order to develop algorithms that reason about longterm decision making, it is convenient to cast the problem into the Markov Decision Process (MDP) framework. The MDP framework consists of a set of states, actions (that come from a policy), cost (loss), and a model that transitions states given actions. For robotics control problems, the robot’s configuration is the state, the controls (e.g., joint torques) are the actions, and the cost is related to achieving a task (e.g., distance walked). Though more nuanced, most sequential predictions can be cast into the same framework (Daumé III et al., 2009). The actions are the learner’s (e.g., RNN’s) predictions. The state is then the result of all the predictions made so far (e.g., the dependency tree constructed so far or the words translated so far). The cumulative cost is the performance metric such as (negative) UAS,\n2i.e., the regret bound depends on poly-log of the dimension of parameter space.\nreceived at the end (horizon) after the final prediction.\nFormally, a finite-horizon Markov Decision Process (MDP) is defined as (S,A, P, C, ρ0, H). Here, S is a set of S many states and A is a set of A actions; given time step t, Pt is the transition dynamics such that for any st ∈ S, st+1 ∈ S, at ∈ A, Pt(st+1|st, at) is the probability of transiting to state st+1 from state st by taking action at at step t; C is the cost distribution such that a cost ct at step t is sampled from Ct(·|st, at). Finally, we denote c̄t as the expected cost, ρ0 as the initial distribution of states, and H ∈ N+ as the finite horizon (max length) of the MDP.\nWe define a stochastic policy π such that for any state s ∈ S, π(·|s) ∈ ∆(A), where ∆(A) is a A-dimension simplex, conditioned on state s. π(a|s) ∈ [0, 1] outputs the probability of taking action a at state s. The distribution of trajectories τ = (s1, a1, . . . , aH−1, sH) is deterministically dependent on π and the MDP, and is defined as\nρπ(τ) = ρ0(s1) H∏ t=2 π(at−1|st−1)Pt−1(st|st−1, at−1).\nThe distribution of the states at time step t, induced by running the policy π until t, is defined ∀st:\ndπt (st) = ∑\n{si,ai}i≤t−1\nρ0(s1) t−1∏ i=1 π(ai|si)Pi(si+1|si, ai).\nNote that the summation above can be replaced by an integral if the state or action space is continuous. The average state distribution d̄π(s) = ∑H t=1 d π t (s)/H .\nThe expected average cost of a policy π can be defined with respect to ρπ or {dπt }:\nµ(π) = E τ∼ρπ [ H∑ t=1 c̄t(st, at)] = H∑ t=1 E s∼dπt (s),a∼π(a|s) [c̄t(s, a)].\nWe define the state-action value Qπt (s, a) (i.e., cost-to-go) for policy π at time step t as:\nQπt (st, at) = c̄t(st, at) + E s∼Pt(·|st,at),a∼π(·|s) Qπt+1(s, a).\nwhere the expectation is taken over the randomness of the policy π and the MDP.\nWe define π∗ as the expert policy (e.g., human demonstrators, search algorithms equipped with ground-truth) and Q∗t (s, a) as the expert’s cost-to-go oracle (note π\n∗ may not be optimal, i.e., π∗ 6∈ arg minπ µ(π)). Throughout the paper, we assume Q∗t (s, a) is known or can be estimated without bias (e.g., by rolling out π∗: starting from state s, applying action a, and then following π∗ for H − t steps).\nWhen π is represented by a function approximator, we use the notation πθ to represent the policy parametrized by\nθ ∈ Rd: π(·|s; θ). In this work we specifically consider optimizing policies in which the parameter dimension d may be large. We also consider the partially observable setting in our experiments, where the policy π(·|o1, a1, ..., ot; θ) is defined over the whole history of partial observations and actions (ot is generated from the hidden state st). We use an LSTM-based policy (Duan et al., 2016) where the LSTM’s hidden states provide a compressed feature of the history."
    }, {
      "heading" : "3. Differentiable Imitation Learning",
      "text" : "Policy based imitation learning aims to learn a policy π̂ that approaches the performance of the expert π∗ in testing time when π∗ is not available anymore. In order to learn rich policies such as with LSTMs or deep networks (Schulman et al., 2015), we derive a policy gradient method for imitation learning and sequential prediction. To do this, we leverage the reduction of IL and sequential prediction to online learning as shown in (Ross & Bagnell, 2014) to learn policies represented by expressive differentiable function approximators.\nThe fundamental idea in Ross & Bagnell (2014) is to use a no-regret online learner to update policies using the following loss function at each episode n:\n`n(π) = 1\nH H∑ t=1 E st∼dπnt [ E a∼π(·|st) [Q∗t (st, a)] ] . (1)\nThe loss function intuitively encourages the learner to find a policy that minimize the expert’s cost-to-go under the state distribution resulting from the current learned policy πn. Specifically, Ross & Bagnell (2014) suggest an algorithm named AggreVaTe (Aggregate Values to Imitate) that uses Follow-the-Leader (FTL) (Shalev-Shwartz et al., 2012) to update policies:πn+1 = arg minπ∈Π ∑n i=1 `n(π), where Π is a pre-defined convex policy set. When `n(π) is strongly convex with respect to π and π∗ ∈ Π, after N iterations AggreVaTe with FTL can find a policy π̂:\nµ(π̂) ≤ µ(π∗)− N +O(ln(N)/N), (2)\nwhere N = [ ∑N n=1 `n(π ∗)−minπ ∑N n=1 `n(π)]/N . Note that N ≥ 0 and the above inequality indicates that π̂ can outperform π∗ when π∗ is not (locally) optimal (i.e., n > 0). Our experimental results support this observation.\nA simple implementation of AggreVaTe that aggregates the values (as the name suggests) will require an exact solution to a batch optimization procedure in each episode. When π is represented by large, non-linear function approximators, the arg min procedure generally takes more and more computation time as n increases.\nOnline Mirror Descent (OMD) (Shalev-Shwartz et al., 2012) are popular for online learning due to its efficiency.\nTherefore we consider two special cases of OMD for optimizing sequence of losses {`n(π)}n: Online Gradient Descent (OGD) (Zinkevich, 2003) and Exponential Gradient Descent (EG) (Shalev-Shwartz et al., 2012), which lead to a regular stochastic policy gradient descent algorithm and a natural policy gradient algorithm, respectively. Also, when applying OGD and EG to {`n(π)}n, one can show that Eq. 2 will hold (with O(1/ √ N)), as long as `n(π) is convex with respect to π."
    }, {
      "heading" : "3.1. Online Gradient Descent",
      "text" : "For discrete actions, the gradient of `n(πθ) (Eq. 1) with respect to the parameters θ of the policy can be computed as\n∇θ`n(θ) = 1\nH H∑ t=1 E st∼d πθn t ∑ a ∇θπ(a|st; θ)Q∗t (st, a).\n(3)\nFor continuous action spaces, we cannot simply replace the summation by integration since in practice it is impossible to evaluate Q∗t (s, a) for infinitely many a, so, instead, we use importance weighting to re-formulate `n (Eq. 1) as\n`n(πθ) = 1\nH H∑ t=1 E s∼d πθn t ,a∼π(·|s;θn) π(a|s; θ) π(a|s; θn) Q∗t (s, a)\n= 1\nH E\nτ∼ρπθn H∑ t=1 π(at|st; θ) π(at|st; θn) Q∗t (st, at). (4)\nSee Appendix A for the derivation of the above equation. With this reformulation, the gradient with respect to θ is\n∇θ`n(θ) = 1\nH E\nτ∼ρπθn H∑ t=1 ∇θπ(at|st; θ) π(at|st; θn) Q∗t (st, at). (5)\nThe above gradient computation enables a very efficient update procedure with online gradient descent: θn+1 = θn − ηn∇θ`n(θ)|θ=θn , where ηn is the learning rate."
    }, {
      "heading" : "3.2. Policy Updates with Natural Gradient Descent",
      "text" : "We derive a natural gradient update procedure for imitation learning inspired by the success of natural gradient descent in RL (Kakade, 2002; Bagnell & Schneider, 2003; Schulman et al., 2015). First, we show that Exponential Gradient Descent (EG) can be leveraged to speed up imitation learning in discrete MDPs. Then we extend EG to continuous MDPs, where we show that, with three steps of approximation, EG leads to a natural gradient update procedure."
    }, {
      "heading" : "3.2.1. EXPONENTIAL GRADIENT IN DISCRETE MDPS",
      "text" : "For notational simplicity, for each state s ∈ S, we represent the policy π(·|s) as a discrete probability vector\nπs ∈ ∆(A). We also represent dπt as a S-dimension probability vector from S-d simplex, consisting of dπt (s),∀s ∈ S. For each s, we use Q∗t (s) to denote the A-dimension vector consisting of the state-action cost-to-go Q∗t (s, a) for all a ∈ A. With this notation, the loss function `n(π) from Eq. 1 can now be written as: `n(π) = 1 H ∑H t=1 ∑ s∈S d πn t (s)(π\ns ·Q∗t (s)), where a · b represents the inner product between vectors a and b. Exponential Gradient updates π as follows: ∀s ∈ S,\nπn+1 = arg min πs∈∆(A),∀s∈S\n1\nH H∑ t=1 ∑ s∈S dπnt (s) ( πs ·Q∗t (s) ) + ∑ s∈S d̄πn(s) ηn,s KL(πs‖πsn), (6)\nwhere KL(q‖p) is the KL-divergence between two probability distributions q and p. This leads to the following closed-form update:\nπsn+1[i] = πsn[i] exp\n( − ηn,sQ̃es[i] )∑|A| j=1 π s n[j] exp ( − ηn,sQ̃es[j]\n) , i ∈ [|A|], (7)\nwhere Q̃es = ∑H t=1 d πn t (s)Q ∗ t (s)/(Hd̄\nπn(s)). We refer readers to (Shalev-Shwartz et al., 2012) or Appendix B for the derivations of the above closed-form updates."
    }, {
      "heading" : "3.2.2. CONTINUOUS MDPS",
      "text" : "We now consider how to update the parametrized policy πθ for continuous MDPs. Replacing summations by integrals, Eq. 6 can be written as:\nθ = arg min θ\n1\nH H∑ t=1 E s∼d πθn t E a∼π(·|s;θ) [Q∗t (s, a)]\n+ E s∼d̄πθn KL(πθ||πθn)/ηn. (8)\nIn order to solve for θ from Eq. 8, we apply several approximations. We first approximate `n(θ) (the first part of the RHS of the above equation) by its first-order Taylor expansion: `n(θ) ≈ `n(θn) +∇θn`n(θn) · (θ− θn). When θ and θn are close, this is a valid local approximation.\nSecond, we replace KL(πθ||πθn) by KL(πθn ||πθ), which is a local approximation since KL(q||p) and KL(p||q) are equal up to the second order (Kakade & Langford, 2002; Schulman et al., 2015).\nThird, we approximate KL(πθn ||πθ) by a second-order Taylor expansion around θn, such that we can approximate the penalization using the Fisher information matrix:\nE s∼d̄πθn KL(πθn ||πθ) ≈ (1/2)(θ − θn)T I(θn)(θ − θn),\nwhere the Fisher information matrix I(θn) = Es,a∼d̄πθn πθn (a|s) ( ∇θn log(πθn(a|s)) )( ∇θn log(πθn(a|s) )T .\nInserting these three approximations into Eq. 8, and solving for θ, we reach the following update rule θn+1 = θn − ηnI(θn)−1∇θ`n(θ)|θ=θn , which is similar to the natural gradient update rule developed in (Kakade, 2002) for the RL setting. Bagnell & Schneider (2003) provided an equivalent representation for Fisher information matrix:\nI(θn) = 1\nH2 E τ∼ρπθn ∇θn log(ρπθn (τ))∇θn log(ρπθn (τ)) T ,\n(9)\nwhere ∇θ log(ρπτ (τ)) is the gradient of the log likelihood of the trajectory τ which can be computed as∑H t=1∇θ log(πθ(at|st)). In the remainder of the paper, we use this Fisher information matrix representation, which yields much faster computation of the descent direction δθ, as we will explain in the next section."
    }, {
      "heading" : "4. Sample-Based Practical Algorithms",
      "text" : "In the previous section, we derived a regular gradient update procedure and a natural gradient update procedure for IL. Note that all of the computations of gradients and Fisher information matrices assumed it was possible to exactly compute expectations including Es∼dπ and Ea∼π(a|s). In this section, we provide practical algorithms where we approximate the gradients and Fisher information matrices using finite samples collected during policy execution."
    }, {
      "heading" : "4.1. Gradient Estimation and Variance Reduction",
      "text" : "We consider an episodic framework where given a policy πn at episode n, we roll out πn K times to collect K trajectories {τni }, for i ∈ [K], τni = {s i,n 1 , a i,n 1 , ...}. For gradient ∇θ`n(θ)|θ=θn we can compute an unbiased estimate using {τni }i∈[K]:\n∇̃θn = 1\nHK K∑ i=1 H∑ t=1 ∑ a ∇θnπθn(a|s i,n t )Q ∗ t (s i,n t , a),\n(10)\n∇̃θn = 1\nHK K∑ i=1 H∑ t=1 ∇θnπθn(a i,n t |s i,n t ) πθn(a i,n t |s i,n t ) Q∗t (s i,n t , a i,n t ).\n(11)\nfor discrete and continuous setting respectively.\nWhen we can compute V ∗t (s) (e.g., minaQ ∗ t (s, a)), we can replace Q∗t (s i,n t , a) in Eq. 10 and Eq. 11 by the state-action advantage function A∗t (s i,n t , a) = Q ∗ t (s i,n t , a) − V ∗t (s i,n t ), which leads to the following two unbiased and variance-\nreduced gradient estimations (Greensmith et al., 2004):\n∇̃θn = 1\nHK K∑ i=1 H∑ t=1 ∑ a ∇θnπθn(a|s i,n t )A ∗ t (s i,n t , a),\n(12)\n∇̃θn = 1\nHK K∑ i=1 H∑ t=1 ∇θnπθn(a i,n t |s i,n t ) πθn(a i,n t |s i,n t ) A∗t (s i,n t , a i,n t ),\n(13)\nwhere Eq. 12 is for discrete action and Eq. 13 for continuous action .\nThe Fisher information matrix (Eq. 9) is approximated as:\nĨ(θn) = 1\nH2K K∑ i=1 ∇θn log(ρπθn (τi))∇θn log(ρπθn (τi)) T = SnS T n , (14)\nwhere, for notation simplicity, we denote Sn as a d×K matrix where the i’s th column is∇θn log(ρπθn (τi))/(H √ K). Namely the Fisher information matrix is represented by a sum of K rank-one matrices. For large policies represented by neural networks, K d, and hence Ĩ(θn) a low rank matrix. One can find the descent direction δθn by solving the linear system SnS T n δθn = ∇̃θn for δθn using Conjugate Gradient (CG) with a fixed number of iterations, which is equivalent to solving the above linear systems using the Partial Least Square (Phatak & de Hoog, 2002). This approach is used in TRPO (Schulman et al., 2015). The difference is that our representation of the Fisher matrix is in the form of SnSTn and in CG we never need to explicitly compute or store SnSTn which requires d2 space and time. Instead, we only compute and store Sn (O(Kd)) and the total computational time is still O(K2d). The learning-rate for natural gradi-\nent descent can be chosen as ηn = √ δKL/(∇̃Tθnδθn), such that KL(ρπθn+1 (τ)‖ρπθn (τ)) ≈ δKL ∈ R +"
    }, {
      "heading" : "4.2. Differentiable Imitation Learning: AggreVaTeD",
      "text" : "We present the differentiable imitation learning framework AggreVaTeD, in Alg. 1. At every iteration n, the roll-in policy π̂n is a mix of the expert policy π∗ and the current policy πθn , with mixing rate α (αn → 0, n → ∞): at every step, with probability α, π̂n picks π∗ and else πθn . This mixing strategy with decay rate was first introduced in (Ross et al., 2011) for IL, and later on was used in sequence prediction (Bengio et al., 2015). In Line 6 one can choose Eq. 10 or the corresponding variance reduced estimation Eq. 12 (Eq. 11 and Eq. 13 for continuous actions) to perform regular gradient descent, and choose CG to perform natural gradient descent. Compared with previous well-known IL and sequential prediction algorithms (Ross\nAlgorithm 1 AggreVaTeD (Differentiable AggreVaTe) 1: Input: The given MDP and expert π∗. Learning rate {ηn}. Schedule rate {αi}, αn → 0, n→∞.\n2: Initialize policy πθ1 (either random or supervised learning). 3: for n = 1 to N do 4: Mixing policies: π̂n = αnπ∗ + (1− αn)πθn . 5: Starting from ρ0, roll in by executing π̂n on the given MDP to generate K trajectories {τni }. 6: Using Q∗ and {τni }i, compute the descent direction δθn (Eq. 10, Eq. 11, Eq. 12, Eq. 13, or CG). 7: Update: θn+1 = θn − ηnδθn . 8: end for 9: Return: the best hypothesis π̂ ∈ {πn}n on validation.\net al., 2011; Ross & Bagnell, 2014; Chang et al., 2015b), AggreVaTeD is extremely simple: we do not need to perform any Data Aggregation (i.e., we do not need to store all {τi}i from all previous iterations); the computational complexity of each episode scales as O(d).\nWhen we use non-linear function approximators to represent the polices, the analysis of AggreVaTe from (Ross & Bagnell, 2014) will not hold, since the loss function `n(θ) is not convex with respect to parameters θ. Nevertheless, as we will show in experiments, in practice AggreVaTeD is still able to learn a policy that is competitive with, and sometimes superior to the oracle’s performance."
    }, {
      "heading" : "5. Quantify the Gap: An Analysis of IL vs RL",
      "text" : "How much faster can IL learn a good policy than RL? In this section we quantify the gap on discrete MDPs when IL can (1) query for an optimalQ∗ or (2) query for a noisy but unbiased estimate ofQ∗. To measure the speed of learning, we look at the cumulative regret of the entire learning process, defined as RN = ∑N n=1(µ(πn)− µ(π∗)). A smaller regret rate indicates faster learning. Throughout this section, we assume the expert π∗ is optimal. We consider finite-horizon, episodic IL and RL algorithms."
    }, {
      "heading" : "5.1. Exponential Gap",
      "text" : "We consider an MDPM shown in Fig. 1 which is a depthK binary tree-structure with S = 2K − 1 states and two\nactions al, ar: go-left and go-right. The transition is deterministic and the initial state s0 (root) is fixed. The cost for each non-leaf state is zero; the cost for each leaf is i.i.d sampled from a given distribution (possibly different distributions per leaf). Below we show that forM, IL can be exponentially more sample efficient than RL.\nTheorem 5.1. ForM, the regret RN of any finite-horizon, episodic RL algorithm is at least:\nE[RN ] ≥ Ω( √ SN). (15)\nThe expectation is with respect to random generation of cost and internal randomness of the algorithm. However, for the same MDPM, with the access to Q∗, we show IL can learn exponentially faster:\nTheorem 5.2. For the MDPM, there exists a policy class such that AggreVaTe with FTL that can achieve the following regret bound:\nRN ≤ O(ln (S)). (16)\nFig. 1 illustrates the intuition behind the theorem. Assume during the first episode, the initial policy π1 picks the rightmost trajectory (bold black) to explore and the algorithm queries from oracle that for s0 we have Q∗(s0, al) < Q∗(s0, ar), it immediately learns that the optimal policy will go left (black arrow) at s0. Hence the algorithm does not have to explore the right sub-tree (dotted circle).\nNext we consider a more difficult setting where one can only query for a noisy but unbiased estimate ofQ∗ (e.g., by rolling out π∗ finite number of times). The above halving argument will not apply since deterministically eliminating nodes based on noisy estimates might permanently remove good trajectories. However, IL can still achieve a poly-log regret with respect to S, even in the noisy setting:\nTheorem 5.3. With only access to unbiased estimate ofQ∗, for the MDPM, AggreVaTeD with EG that can achieve the following regret with probability at least 1− δ:\nRN ≤ O ( ln(S)( √ ln(S)N + √ ln(2/δ)N) ) . (17)\nThe detailed proofs of the above three theorems can be found in Appendix D,E,F respectively. In summary, for MDPM, IL is is exponentially faster than RL."
    }, {
      "heading" : "5.2. Polynomial Gap and Near-Optimality",
      "text" : "We next quantify the gap in general discrete MDPs and also show that AggreVaTeD is near-optimal. We consider the harder case where we can only access an unbiased estimate of Q∗t , for any t and state-action pair. The policy π is represented as a set of probability vectors πs,t ∈ ∆(A), for all s ∈ S and t ∈ [H]: π = {πs,t}s∈S,t∈[H].\nTheorem 5.4. With access to unbiased estimates of Q∗t , AggreVaTeD with EG achieves the regret upper bound:\nRN ≤ O ( HQemax √ S ln(A)N ) . (18)\nHere Qemax is the maximum cost-to-go of the expert. 3 The total regret shown in Eq. 18 allows us to compare IL algorithms to RL algorithms. For example, the Upper Confidence Bound (UCB) based, near-optimal optimistic RL algorithms from (Jaksch et al., 2010), specifically designed for efficient exploration, admit regret Õ(HS √ HAN), leading to a gap of approximately √ HAS compared to the regret bound of imitation learning shown in Eq. 18.\nWe also provide a lower bound on RN for H = 1 case which shows the dependencies on N,A, S are tight:\nTheorem 5.5. There exists an MDP (H=1), with only access to unbiased estimate ofQ∗, any finite-horizon episodic imitation learning algorithm must have:\nE[RN ] ≥ Ω( √ S ln(A)N). (19)\nThe proofs of the above two theorems regarding general MDPs can be found at Appendix G,H. In summary for discrete MDPs, one can expect at least a polynomial gap and a possible exponential gap between IL and RL."
    }, {
      "heading" : "6. Experiments",
      "text" : "We evaluate our algorithms on robotics simulations from OpenAI Gym (Brockman et al., 2016) and on Handwritten Algebra Dependency Parsing (Duyck & Gordon, 2015). We report reward instead of cost, since OpenAI Gym by default uses reward and dependency parsing aims to maximize UAS score. As our approach only promises there exists a policy among all of the learned polices that can perform as well as the expert, we report the performance of the best policy so far: max{µ(π1), ..., µ(πi)}. For regular gradient descent, we use ADAM (Kingma & Ba, 2014) which is a first-order no-regret algorithm, and for natural gradient, we use CG to compute the descent direction. For RL we use REINFORCE (Williams, 1992) and Truncated Natural Policy Gradient (TNPG) (Duan et al., 2016)."
    }, {
      "heading" : "6.1. Robotics Simulations",
      "text" : "We consider CartPole Balancing, Acrobot Swing-up, Hopper and Walker. For generating an expert, similar to previous work (Ho & Ermon, 2016), we used a Deep Q-Network (DQN) to generate Q∗ for CartPole and Acrobot (e.g., to simulate the settings where Q∗ is available), while using the publicly available TRPO implementation to generate\n3Here we assume Qemax is a constant compared to H . If Qemax = Θ(H), then the expert is no better than a random policy of which the cost-to-go is around Θ(H).\nπ∗ for Hopper and Walker to simulate the settings where one has to estimate Q∗ by Monte-Carlo roll outs π∗.\nDiscrete Action Setting We use a one-layer (16 hidden units) neural network with ReLu activation functions to represent the policy π for the Cart-pole and Acrobot benchmarks. The value function Q∗ is obtained from the DQN (Mnih et al., 2015) and represented by a multi-layer fully connected neural network. The policy πθ1 is initialized with common ReLu neural network initialization techniques. For the scheduling rate {αi}, we set all αi = 0: namely we did not roll-in using the expert’s actions during training. We set the number of roll outs K = 50 and horizon H = 500 for CartPole and H = 200 for Acrobot.\nFig. 4a and 4b shows the performance averaged over 10 random trials of AggreVaTeD with regular gradient descent and natural gradient descent. Note that AggreVaTeD outperforms the experts’ performance significantly: Natural gradient surpasses the expert by 5.8% in Acrobot and 25% in Cart-pole. Also, for Acrobot swing-up, at horizon H = 200, with high probability a randomly initialized neural network policy won’t be able to collect any reward signals. Hence the improvement rates of REINFORCE and TNPG are slow. In fact, we observed that for a short horizon such asH = 200, REINFORCE and Truncated Natural Gradient often even fail to improve the policy at all (failed 6 times among 10 trials). On the contrary, AggreVaTeD does not suffer from the delayed reward signal issue, since the expert will collect reward signals much faster than a randomly initialized policy.\nFig. 2c shows the performance of AggreVaTeD with an LSTM policy (32 hidden states) in a partially observed setting where the expert has access to full states but the learner has access to partial observations (link positions). RL algorithms did not achieve any improvement while AggreVaTeD still achieved 92% of expert’s performance.\nContinuous Action Setting We test our approaches on two robotics simulators with continuous actions: (1) the 2-d Walker and (2) the Hopper from the MuJoCo physics simulator. Following the neural network settings described in Schulman et al. (2015), the expert policy π∗ is obtained from TRPO with one hidden layer (64 hidden states), which is the same structure that we use to represent our policies πθ. We set K = 50 and H = 100. We initialize πθ1 by collecting K expert demonstrations and then maximize the likelihood of these demonstrations (i.e., supervised learning). We use Eq. 11 instead of the variance reduced equation here since we need to use MC roll-outs to estimate V ∗ (we simply use one roll-out to estimate Q∗).\nFig. 2d and 2e show the performance averaged over 5 random trials. Note that AggreVaTeD outperforms the expert in the Walker by 5.4% while achieving 97% of the expert’s\nperformance in the Hopper problem. After 100 iterations, we see that by leveraging the help from experts, AggreVaTeD can achieve much faster improvement rate than the corresponding RL algorithms."
    }, {
      "heading" : "6.2. Dependency Parsing on Handwritten Algebra",
      "text" : "We consider a sequential prediction problem: transitionbased dependency parsing for handwritten algebra with raw image data (Duyck & Gordon, 2015). The parsing task for algebra is similar to the classic dependency parsing for natural language (Chang et al., 2015a) where the problem is modelled in the IL setting and the state-of-the-art is achieved by AggreVaTe with FTRL (using Data Aggregation). The additional challenge here is that the inputs are handwritten algebra symbols in raw images. We directly learn to predict parse trees from low level image features (Histogram of Gradient features (HoG)). During training, the expert is constructed using the ground-truth dependencies in training data. The full state s during parsing consists of three data structures: Stack, Buffer and Arcs, which store raw images of the algebraic symbols. Since the sizes of stack, buffer and arcs change during parsing, a common approach is to featurize the state s by taking the features of the latest three symbols from stack, buffer and arcs (e.g., (Chang et al., 2015a)). Hence the problem falls into the partially observable setting, where the feature o is extracted from state s and only contains partial information about s. The dataset consists of 400 sets of handwritten algebra equations. We use 80% for training, 10% for validation, and 10% for testing. We include an example of handwritten algebra equations and its dependency tree in Appendix I. Note that different from robotics simulators where at every episode one can get fresh data from the simulators, the dataset is fixed and sample efficiency is critical.\nThe RNN policy follows the design from (Sutskever et al., 2014). It consists of two LSTMs. Given a sequence of algebra symbols τ , the first LSTM processes one symbol at a time and at the end outputs its hidden states and memory (i.e., a summary of τ ). The second LSTM initializes its own hidden states and memory using the outputs of the first LSTM. At every parsing step t, the second LSTM takes the current partial observation ot (ot consists of features of the\nmost recent item from stack, buffer and arcs) as input, and uses its internal hidden state and memory to compute the action distribution π(·|o1, ..., ot, τ) conditioned on history. We also tested reactive policies constructed as fully connected ReLu neural networks (NN) (one-layer with 1000 hidden states) that directly maps from observation ot to action a, where ot uses the most three recent items. We use variance reduced gradient estimations, which give better performance in practice. The performance is summarised in Table 1. Due to the partial observability of the problem, AggreVaTeD with a LSTM policy achieves significantly better UAS scores compared to the NN reactive policy and DAgger with a Kernelized SVM (Duyck & Gordon, 2015). Also AggreVaTeD with a LSTM policy achieves 97% of optimal expert’s performance. Fig. 3 shows the improvement rate of regular gradient and natural gradient on both validation set and test set. Overall we observe that both methods have similar performance. Natural gradient achieves a better UAS score in validation and converges slightly faster on the test set but also achieves a lower UAS score on test set."
    }, {
      "heading" : "7. Conclusion",
      "text" : "We introduced AggreVaTeD, a differentiable imitation learning algorithm which trains neural network policies for sequential prediction tasks such as continuous robot control and dependency parsing on raw image data. We showed that in theory and in practice IL can learn much faster than RL with access to optimal cost-to-go oracles. The IL learned policies were able to achieve expert and sometimes super-expert levels of performance in both fully observable\nand partially observable settings. The theoretical and experimental results suggest that IL is significantly more effective than RL for sequential prediction with near optimal cost-to-go oracles."
    }, {
      "heading" : "Appendix: Proofs and Detailed Bounds",
      "text" : ""
    }, {
      "heading" : "A. Derivation of Eq. 4",
      "text" : "Starting from Eq. 1 with parametrized policy πθ, we have:\n`n(θ) = 1\nH H∑ t=1 E st∼d πθn t [ E at∼π(·|st;θ) [Q∗t (st, at)] ] = 1\nH H∑ t=1 E st∼d πθn t [ ∫ a π(a|st; θ)Q∗t (st, a)da ]\n= 1\nH H∑ t=1 E st∼d πθn t [ ∫ a π(a|st; θn) π(a|st; θ) π(a|st; θn) Q∗t (st, a)da ]\n= 1\nH H∑ t=1 E st∼d πθn t [ E a∼π(·|st;θn) π(a|st; θ) π(a|st; θn) Q∗t (st, a) ]\n= 1\nH H∑ t=1 E st∼d πθn t ,at∼π(a|st;θn) [ π(at|st; θ) π(at|st; θn) Q∗t (st, at) ] . (20)"
    }, {
      "heading" : "B. Derivation of Exponential Gradient Update in Discrete MDP",
      "text" : "We show the detailed derivation of Eq. 7 for AggreVaTeD with EG in discrete MDP. Recall that with KL-divergence as the penalization, one update the policy in each episode as:\n{πsn+1}s∈S = arg min{πs∈∆(A),∀s} 1 H H∑ t=1 ∑ s∈S dπnt (s) ( πs ·Q∗t (s) ) + ∑ s∼S d̄πn(s) ηn,s KL(πs‖πsn)\nNote that in the above equation, for a particular state s, optimizing πs is in fact independent of πs ′ ,∀s′ 6= s. Hence the optimal sequence {πs}s∈S can be achieved by optimizing πs independently for each s ∈ S. For πs, we have the following update rule:\nπsn+1 = arg min πs∈∆(A)\n1\nH H∑ t=1 dπnt (s)(π s ·Q∗t (s)) + d̄πn(s) ηn,s KL(πs‖πsn)\n= arg min πs∈∆(A) πs · ( H∑ t=1 dπnt (s)Q ∗ t (s)/H) + d̄πn(s) ηn,s KL(πs‖πsn)\n= arg min πs∈∆(A) πs · ( H∑ t=1 dπnt (s)Q ∗ t (s)/(Hd̄ πn(s))) + 1 ηn,s KL(πs‖πsn)\n= arg min πs∈∆(A) πs · Q̃e(s) + 1 ηn,s A∑ j=1 πs[j](log(πs[j])− log(πsn[j])) (21)\nTake the derivative with respect to πs[j], and set it to zero, we get:\nQ̃e(s)[j] + 1\nηn,s (log(πs[j]/πsn[j]) + 1) = 0, (22)\nthis gives us:\nπs[j] = πsn[j] exp(−ηn,sQ̃e(s)[j]− 1). (23)\nSince πs ∈ ∆(A), after normalization, we get:\nπs[j] = πsn[j] exp(−ηn,sQ̃e(s)[j])∑A i=1 π s n[i] exp(−ηn,sQ̃e(s)[i])\n(24)"
    }, {
      "heading" : "C. Lemmas",
      "text" : "Before proving the theorems, we first present the Performance Difference Lemma (Kakade & Langford, 2002; Ross & Bagnell, 2014) which will be used later:\nLemma C.1. For any two policies π1 and π2, we have:\nµ(π1)− µ(π2) = H H∑ t=1 Est∼dπ1t [ Eat∼π1(·|st)[Q π2 t (st, at)− V π2 t (st)] ] . (25)\nWe refer readers to (Ross & Bagnell, 2014) for the detailed proof of the above lemma.\nThe second known result we will use is the analysis of Weighted Majority Algorithm. Let us define the linear loss function as `n(w) = w · yn, for any yn ∈ Rd, and w ∈ ∆(d) from a probability simplex. Running Exponential Gradient Algorithm on the sequence of losses {w · yn} to compute a sequence of decisions {wn}, we have: Lemma C.2. The sequence of decisions {wn} computed by running Exponential Gradient descent with step size µ on the loss functions {w · yn} has the following regret bound:\nN∑ n=1 wn · yn − min w∗∈∆(d) N∑ n=1 w∗ · yn ≤ ln(d) µ + µ 2 N∑ n=1 d∑ i=1 wn[i]yn[i] 2. (26)\nWe refer readers to (Shalev-Shwartz et al., 2012) for detailed proof."
    }, {
      "heading" : "D. Proof of Theorem 5.1",
      "text" : "Proof. We construct a reduction from stochastic Multi-Arm Bandits (MAB) to the MDP M̃. A stochastic MAB is defined by S arms denoted as I1, ..., IS . Each arm It’s cost ci at any time step t is sampled from a fixed but unknown distribution. A bandit algorithm picks an arm It at iteration t and then receives an unbiased sample of the picked arm’s cost cIt . For any bandit algorithm that picks arms I1, I2, ..., IN in N rounds, the expected regret is defined as:\nE[RN ] = E[ N∑ n=1 cIn ]− min i∈[S] N∑ n=1 c̄i, (27)\nwhere the expectation is taken with respect to the randomness of the cost sampling process and possibly the randomness of the bandit algorithm. It has been shown that there exists a set of distributions from which the arms’ costs sampled from, the expected regret E[RN ] is at least Ω( √ SN) (Bubeck et al., 2012).\nConsider a MAB with 2K arms. To construct a MDP from a MAP, we construct a K + 1-depth binary-tree structure MDP with 2K+1− 1 nodes. We set each node in the binary tree as a state in the MDP. The number of actions of the MDP is two, which corresponds to go left or right at a node in the binary tree. We associate each leaf nodes with arms in the original MAB: the cost of the i’th leaf node is sampled from the cost distribution for the i’th arm, while the non-leaf nodes have cost always equal to zero. The initial distribution ρ0 concentrates on the root of the binary tree. Note that there are total 2K trajectories from the root to leafs, and we denote them as τ1, ...τ2K . We consider finite horizon (H = K + 1) episodic RL algorithms that outputs π1, π2, ..., πN at N episodes, where πn is any deterministic policy that maps a node to actions left or right. Any RL algorithm must have the following regret lower bound:\nE[ N∑ n=1 µ(πn)]−min π∗ N∑ n=1 µ(π∗) ≥ Ω( √ SN), (28)\nwhere the expectation is taken with respect to the possible randomness of the RL algorithms. Note that any deterministic policy π identifies a trajectory in the binary tree when rolling in from the root. The optimal policy π∗ simply corresponds to the trajectory that leads to the leaf with the mininum expected cost. Note that each trajectory is associated with an arm from the original MAB, and the expected total cost of a trajectory corresponds to the expected cost of the associated arm. Hence if there exists an RL algorithm that achieves regret O( √ SN), then we can solve the original MAB problem by simply running the RL algorithm on the constructed MDP. Since the lower bound for MAB is Ω( √ SN), this concludes that Eq. 28 holds."
    }, {
      "heading" : "E. Proof of Theorem 5.2",
      "text" : "Proof. For notation simplicity we denote al as the go-left action while ar is the go-right action. Without loss of generality, we assume that the leftmost trajectory has the lowest total cost (e.g., s3 in Fig. 1 has the lowest average cost). We consider the deterministic policy class Π that contains all policy π : S → {al, ar}. Since there are S states and 2 actions, the total number of policies in the policy class is 2S . To prove the upper bound RN ≤ O(log(S)), we claim that for any e ≤ K, at the end of episode e, AggreVaTe with FTL identifies the e’th state on the best trajectory, i,e, the leftmost trajectory s0, s1, s3, ..., s(2K−1−1). We can prove the claim by induction.\nAt episode e = 1, based on the initial policy, AggreVaTe picks a trajectory τ1 to explore. AggreVaTe with FTL collects the states s at τ1 and their associated cost-to-go vectors [Q∗(s, al), Q∗(s, ar)]. Let us denote D1 as the dataset that contains the state,cost-to-go pairs: D1 = {(s, [Q∗(s, al), Q∗(s, al)])}, for s ∈ τ1. Since s0 is visited, the state-cost pair (s0, [Q ∗(s0, al), Q ∗(s0, ar)]) must be in D1. To update policy from π1 to π2, AggreVaTe with FTL runs cost-sensitive classification D1 as:\nπ2 = arg min π |D1|∑ k=1 Q∗(sk, π(sk)), (29)\nwhere sk stands for the k’th data point collected at dataset D1. Due to the construction of policy class Π, we see that π2 must picks action al at state s0 since Q(s0, al) < Q(s0, ar). Hence at the end of the episode e = 1, π2 identifies s1 (i.e., running π2 from root s0 leads to s1), which is on the optimal trajectory.\nNow assume that at the end of episode n − 1, the newly updated policy πn identifies the state s(2n−1−1): namely at the beginning of episode n, if we roll-in πn, the algorithm will keep traverse along the leftmost trajectory till at least state s(2n−1−1). At episode n, let Dn as the dataset contains all data points from Dn−1 and the new collected state, cost-togo pairs from τn: Dn = Dn−1 ∪ {(s, [Q∗(s, al), Q∗(s, ar)])}, for all s ∈ τn. Now if we compute policy πn+1 using cost-sensitive classification (Eq. 29) over Dn, we must learn a policy πn+1 that identifies action al at state s(2j−1), since Qe(s(2j−1), al) < Q\n∗(s(2j−1), ar), and s(2j−1) is included in Dn, for j = 1, ..., n− 1. Hence at the end of episode n, we identify a policy πn+1 such that if we roll in policy πn+1 from s0, we will traverse along the left most trajectory till we reach s(2n−1).\nHence by the induction hypothesis, at the end of episodeK−1, πK will reach state s(2K−1−1), the end of the best trajectory.\nSince AggreVaTe with FTL with policy class Π identifies the best trajectory with at most K − 1 episodes, the cumulative regret is then at most O(K), which is O(log(S)) (assuming the average cost at each leaf is a bounded constant), as S is the number of nodes in the binary-tree structure MDP M̃."
    }, {
      "heading" : "F. Proof of Theorem 5.3",
      "text" : "Since in Theorem 5.3 we assume that we only have access to the noisy, but unbiased estimate of Q∗, the problem becomes more difficult since unlike in the proof of Theorem 5.2, we cannot simply eliminate states completely since the cost-to-go of the states queried from expert is noisy and completely eliminate nodes will potentially result elimination of low cost trajectories. Hence here we consider a different policy representation. We define 2K deterministic base policies π1, ..., π2 K\n, such that rolling in policy πi at state s0 will traverse along the trajectory ending at the i’th leaf. We define the policy class Π as the convex hull of the base policies Π = {π : ∑2K i=1 wiπ i, ∑2K i wi = 1, wi ≥ 0,∀i}. Namely each π ∈ Π is a stochastic policy: when rolling in, with probability wi, π execute the i’th base policy πi from s0. Below we prove that AggreVaTeD with Exponential Gradient Descent achieves the regret bound O( √ ln(S)N).\nProof. We consider finite horizon, episodic imitation learning setting where at each episode n, the algorithm can roll in the current policy πn once and only once and traverses through trajectory τn . Let us define ˜̀n(w) =\n1 K+1 ∑ s∈τn ∑2K j=1 wjQ̃ e(s, πj(s)), where τn is the trajectory traversed by rolling in policy πn starting at s0, and Q̃e is a noisy but unbiased estimate of Q∗. We simply consider the setting where Q̃e is bounded |Q̃e| ≤ lmax (note that we can easily extend our analysis to a more general case where Q̃e is from a sub-Gaussian distribution). Note that ˜̀n(w) is simply a linear loss with respect to w:\n˜̀ n(w) = w · qn, (30)\nwhere qn[j] = ∑ s∈τn Q̃\ne(s, πj(s))/(K+ 1). AggreVaTeD with EG updates w using Exponential gradient descent. Using the result from lemma C.2, we get:\nN∑ n=1 (˜̀n(wn)− ˜̀n(w∗)) = N∑ n=1 (wn · qn − w∗ · qn) ≤ ln(2K) µ + µ 2 N∑ n=1 2K∑ j=1 wn[j]qn[j] 2 ≤ ln(2 K) µ + µ 2 N∑ n=1 l2max\n= ln(2K) µ + µNl2max 2 ≤ lmax\n√ ln(S)N. (31)\nNote that S = 2K+1 − 1. The above inequality holds for any w∗ ∈ ∆(2K), including the we that corresponds to the expert (i.e., we[1] = 1, we[i] = 0, i 6= 1 as we assumed without loss of generality the left most trajectory is the optimal trajectory).\nNow let us define `n(w) as follows:\n`n(w) = 1\nK + 1 K+1∑ t=1 ∑ s∼S dπnt (s) 2K∑ j=1 wjQ ∗(s, πj(s)). (32)\nNote `n(w) can be understood as first rolling in πn infinitely many times and then querying for the exact cost-to-go Q∗ on all the visited states. Clearly ˜̀n(w) is an unbiased estimate of `n(w): E[˜̀n(w)]−`n(w) = 0, where the expectation is over the randomness of the roll-in and sampling procedure of Q̃e at iteration n, conditioned on all events among the previous n− 1 iterations. Also note that |˜̀n(w)− `n(w)| ≤ 2lmax, since `n(w) ≤ lmax. Hence {˜̀n(wn)− `n(wn)} is a bounded martingale difference sequence. Hence by Azuma-Heoffding inequality, we get with probability at least 1− δ/2:\nN∑ n=1 `n(wn)− ˜̀n(wn) ≤ 2lmax √ log(2/δ)N, (33)\nand with probability at least 1− δ/2:\nN∑ n=1 ˜̀ n(w e)− `n(we) ≤ 2lmax √ log(2/δ)N. (34)\nCombine the above inequality using union bound, we get with probability at least 1− δ:\nN∑ n=1 (`n(wn)− `n(we)) ≤ N∑ n=1 (˜̀n(wn)− ˜̀n(we)) + 4lmax √ log(2/δ)N. (35)\nNow let us apply the Performance Difference Lemma (Lemma C.1), we get with probability at least 1− δ:\nN∑ n=1 µ(πn)− N∑ n=1 µ(π∗) = N∑ n=1 (K + 1) ( `n(wn)− `n(we) ) ≤ (K + 1)(lmax √ ln(S)N + 4lmax √ log(2/δ)N), (36)\nrearrange terms we get:\nN∑ n=1 µ(πn)− N∑ n=1 µ(π∗) ≤ log(S)lmax( √ ln(S)N + √ log(2/δ)N) ≤ O(ln(S) √ ln(S)N), (37)\nwith probability at least 1− δ."
    }, {
      "heading" : "G. Proof of Theorem 5.4",
      "text" : "The proof of theorem 5.4 is similar to the one for theorem 5.3. Hence we simply consider the infinitely many roll-ins and exact query of Q∗ case. The finite number roll-in and noisy query of Q∗ case can be handled by using the martingale difference sequence argument as shown in the proof of theorem 5.3.\nProof. Recall that in general setting, the policy π consists of probability vectors πs,t ∈ ∆(A), for all s ∈ S and t ∈ [H]: π = {πs,t}∀s∈S,t∈[H]. Also recall that the loss functions EG is optimizing are {`n(π)} where:\n`n(π) = 1\nH H∑ t=1 ∑ s∈S dπnt (s)(π s,t ·Q∗t (s)) = H∑ t=1 ∑ s∈S πs,t · qs,tn (38)\nwhere as we defined before Q∗t (s) stands for the cost-to-go vector Q ∗ t (s)[j] = Q ∗ t (s, aj), for the j’th action in A, and qs,tn = dπnt (s) H Q ∗ t (s).\nNow if we run Exponential Gradient descent on `n to optimize πs,t for each pair of state and time step independently, we can get the following regret upper bound by using Lemma C.2:\nN∑ n=1 `n(π)−min π N∑ n=1 `n(πn) ≤ H∑ t=1 ∑ s∈S ( ln(A) µ + µ 2 N∑ n=1 A∑ j=1 πs,t[j]qs,tn [j] 2 ) . (39)\nNote that we can upper bound (qs,tn [j]) 2 as:\n(qs,tn [j]) 2 ≤ d\nπn t (s) 2\nH2 (Q∗max)\n2 ≤ d πn t (s)\nH2 (Q2max) 2 (40)\nSubstitute it back, we get:\nN∑ n=1 (`n(πn)− `n(π∗)) ≤ H∑ t=1 ∑ s∈S ( ln(A) µ + µ 2 N∑ n=1 A∑ j=1 πs,t[j]dπnt (s) (Q∗max) 2 H2 )\n= H∑ t=1 (S ln(A) µ + µ(Q∗max) 2 2H2 N∑ n=1 ∑ s∈S dπnt (s) A∑ j=1 πs,t[j] ) = H∑ t=1 (S ln(A) µ + µ(Q∗max) 2 2H2 N ) ≤ Q ∗ max\nH\n√ 2S ln(A)N, (41)\nif we set µ = √\n(Q∗max) 2NS ln(A)/(2H2).\nNow let us apply the performance difference lemma (Lemma C.1), we get:\nRN = N∑ n=1 µ(πn)− N∑ n=1 µ(π∗) = H N∑ n=1 (`n(wn)− `n(we)) ≤ HQemax √ S ln(A)N. (42)"
    }, {
      "heading" : "H. Proof of Theorem 5.5",
      "text" : "Let us use Q̃e(s) to represent the noisy but unbiased estimate of Q∗(s).\nProof. For notation simplicity, we denote S = {s1, s2, ..., sS}. We consider a finite MDP with time horizon H = 1. The initial distribution ρ0 = {1/S, ..., 1/S} puts 1/S weight on each state. We consider the algorithm setting where at every episode n, a state sn ∈ S is sampled from ρ0 and the algorithm uses its current policy πsnn ∈ ∆(A) to pick an action a ∈ A for sn and then receives a noisy but unbiased estimate Q̃e(sn) of Q∗(sn) ∈ R|A|. The algorithm then updates its policy from πs n\nn to π sn n+1 for s n while keep the other polices for other s unchanged (since the algorithm did not receive any\nfeedback regarding Q∗(s) for s 6= sn and the sample distribution ρ0 is fixed and uniform). For expected regret E[RN ] we\nhave the following fact:\nE sn∼ρ0,∀n\n[ E\nQ̃e(sn)∼Psn ,∀n [ N∑ n=1 (πs n n · Q̃e(sn)− π∗sn · Q̃e(sn)) ]]\n= E sn∼ρ0,∀n [ N∑ n=1 E Q̃ei (si)∼Psi ,i≤n−1 [ (πs n n ·Q∗(sn)− πesn ·Q∗(sn)) ]]\n= N∑ n=1 E si∼ρ0,i≤n−1 [ E Q̃ei (si)∼Psi ,i≤n−1 [ E s∼ρ0 (πsn ·Q∗(s)− π∗s ·Q∗(s)) ]] = E\n[ N∑ n=1 E s∼ρ0 πsn ·Q∗(s)− E s∼ρ0 π∗s ·Q∗(s) ]\n= E N∑ n=1 [µ(πn)− µ(π∗)], (43)\nwhere the expectation in the final equation is taken with respect to random variables πi, i ∈ [N ] since each πi is depend on Q̃ej , for j < i and s j , for j < i.\nWe first consider EQ̃e(sn)∼Psn ,∀n [∑N n=1(π sn n · Q̃e(sn) − π∗sn · Q̃e(sn)) ]\nconditioned on a given sequence of s1, ..., sN . Let us define that among N episodes, the set of the index of the episodes that state si is sampled as Ni and its cardinality as Ni, and we then have ∑S i=1Ni = N and Ni ∩Nj = ∅,for i 6= j.\nE Q̃e(sn)∼Psn ,∀n [ N∑ n=1 (πs n n · Q̃e(sn)− π∗sn · Q̃e(sn)) ]\n= S∑ i=1 ∑ j∈Ni E Q̃ej (si)∼Psi (πsij · Q̃ e j(si)− πesiQ̃ e j(si)) (44)\nNote that for each state si, at the rounds from Ni, we can think of the algorithm running any possible online linear regression algorithm to compute the sequence of policies πsij ,∀j ∈ Ni for state si. Note that from classic online linear regression analysis, we can show that for state si there exists a distribution Psi such that for any online algorithm:\nE Q̃ej (si)∼Psi ,∀j∈Ni [ ∑ j∈Ni (πsij · Q̃ e j(si)− πesi · Q̃ e j(si)) ] ≥ c √ ln(A)Ni, (45)\nfor some non-zero positive constant c. Substitute the above inequality into Eq. 44, we have:\nE Q̃e(sn)∼Psn ,∀n [ N∑ n=1 (πs n n · Q̃e(sn)− π∗sn · Q̃e(sn)) ] ≥ S∑ i=1 c √ ln(A)Ni = c √ ln(A) S∑ i=1 √ Ni. (46)\nNow let us put the expectation Esi∼ρ0,∀i back, we have:\nE sn∼ρ0,∀n\n[ E\nQ̃e(sn)∼Psn [ N∑ n=1 (πs n n · Q̃e(sn)− π∗sn · Q̃e(sn))|s1, ..., sn ]] ≥ c √ ln(A) N∑ i=1 E[ √ Ni]. (47)\nNote that each Ni is sampled from a Binomial distribution B(N, 1/S). To lower bound En∼B(N,1/S) √ n, we use Hoeffding’s Inequality here. Note that Ni = ∑N n=1 an, where an = 1 if si is picked at iteration n and zero otherwise. Hence ai is from a Bernoulli distribution with parameter 1/S. Using Hoeffding bound, for Ni/N , we get:\nP (|Ni/N − 1/S| <= ) ≥ 1− exp(−2N 2). (48)\nLet = 1/(2S), and substitute it back to the above inequality, we get:\nP (0.5(N/S) ≤ Ni ≤ 1.5(N/S)) = P ( √ 0.5(N/S) ≤ √ Ni ≤ √ 1.5(N/S)) ≥ 1− exp(−2N/S2). (49)\nHence, we can lower bound E[ √ Ni] as follows:\nE[ √ Ni] ≥ √ 0.5N/S(1− exp(−2N/S2)). (50)\nTake N to infinity, we get:\nlim N→∞\nE[ √ Ni] ≥ √ 0.5N/S. (51)\nSubstitute this result back to Eq. 47 and use the fact from Eq. 43, we get:\nlim N→∞ E[RN ] = lim N→∞ E sn∼ρ0,∀n\n[ E\nQ̃e(sn)∼Psn ,∀n [ N∑ n=1 (πs n n · Q̃e(sn)− π∗sn · Q̃e(sn)) ]] ≥ c √ ln(A) S∑ i=1 E[ √ Ni]\n≥ c √ ln(A)S √ 0.5N/S = Ω( √ S ln(A)N).\nHence we prove the theorem."
    }, {
      "heading" : "I. Details of Dependency Parsing for Handwritten Algebra",
      "text" : "In Fig. 4, we show an example of set of handwritten algebra equations and its dependency tree from a arc-hybird sequence slssslssrrllslsslssrrslssrlssrrslssrr. The preprocess step cropped individual symbols one by one from left to right and from the top equation to the bottom one, centered them, scaled symbols to 40 by 40 images, and finally formed them as a sequence of images.\nSince in the most common dependency parsing setting, there is no immediate reward at every parsing step, the reward-togo Q∗(s, a) is computed by using UAS as follows: start from s and apply action a, then use expert π∗ to roll out til the end of the parsing process; Q∗(s, a) is the UAS score of the final configuration. Hence AggreVaTeD can be considered as directly maximizing the UAS score, while previous approaches such as DAgger or SMILe (Ross et al., 2011) tries to mimic expert’s actions and hence are not directly optimizing the final objective."
    } ],
    "references" : [ {
      "title" : "Apprenticeship learning via inverse reinforcement learning",
      "author" : [ "Abbeel", "Pieter", "Ng", "Andrew Y" ],
      "venue" : "In ICML, pp",
      "citeRegEx" : "Abbeel et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Abbeel et al\\.",
      "year" : 2004
    }, {
      "title" : "An actor-critic algorithm for sequence prediction",
      "author" : [ "Bahdanau", "Dzmitry", "Brakel", "Philemon", "Xu", "Kelvin", "Goyal", "Anirudh", "Lowe", "Ryan", "Pineau", "Joelle", "Courville", "Aaron", "Bengio", "Yoshua" ],
      "venue" : "arXiv preprint arXiv:1607.07086,",
      "citeRegEx" : "Bahdanau et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Bahdanau et al\\.",
      "year" : 2016
    }, {
      "title" : "Scheduled sampling for sequence prediction with recurrent neural networks",
      "author" : [ "Bengio", "Samy", "Vinyals", "Oriol", "Jaitly", "Navdeep", "Shazeer", "Noam" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Bengio et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Bengio et al\\.",
      "year" : 2015
    }, {
      "title" : "Learning to search for dependencies",
      "author" : [ "Chang", "Kai-Wei", "He", "Daumé III", "Hal", "Langford", "John" ],
      "venue" : "arXiv preprint arXiv:1503.05615,",
      "citeRegEx" : "Chang et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Chang et al\\.",
      "year" : 2015
    }, {
      "title" : "Learning to search better than your teacher",
      "author" : [ "Chang", "Kai-wei", "Krishnamurthy", "Akshay", "Agarwal", "Alekh", "Daume", "Hal", "Langford", "John" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Chang et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Chang et al\\.",
      "year" : 2015
    }, {
      "title" : "Searchbased structured prediction",
      "author" : [ "Daumé III", "Hal", "Langford", "John", "Marcu", "Daniel" ],
      "venue" : "Machine learning,",
      "citeRegEx" : "III et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "III et al\\.",
      "year" : 2009
    }, {
      "title" : "Benchmarking deep reinforcement learning for continuous control",
      "author" : [ "Duan", "Yan", "Chen", "Xi", "Houthooft", "Rein", "Schulman", "John", "Abbeel", "Pieter" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Duan et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Duan et al\\.",
      "year" : 2016
    }, {
      "title" : "Predicting structure in handwritten algebra data from low level features",
      "author" : [ "Duyck", "James A", "Gordon", "Geoffrey J" ],
      "venue" : "Data Analysis Project Report, MLD,",
      "citeRegEx" : "Duyck et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Duyck et al\\.",
      "year" : 2015
    }, {
      "title" : "Guided cost learning: Deep inverse optimal control via policy optimization",
      "author" : [ "Finn", "Chelsea", "Levine", "Sergey", "Abbeel", "Pieter" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Finn et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Finn et al\\.",
      "year" : 2016
    }, {
      "title" : "Variance reduction techniques for gradient estimates in reinforcement learning",
      "author" : [ "Greensmith", "Evan", "Bartlett", "Peter L", "Baxter", "Jonathan" ],
      "venue" : null,
      "citeRegEx" : "Greensmith et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Greensmith et al\\.",
      "year" : 2004
    }, {
      "title" : "Generative adversarial imitation learning",
      "author" : [ "Ho", "Jonathan", "Ermon", "Stefano" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Ho et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Ho et al\\.",
      "year" : 2016
    }, {
      "title" : "Near-optimal regret bounds for reinforcement learning",
      "author" : [ "Jaksch", "Thomas", "Ortner", "Ronald", "Auer", "Peter" ],
      "venue" : null,
      "citeRegEx" : "Jaksch et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Jaksch et al\\.",
      "year" : 2010
    }, {
      "title" : "Plato: Policy learning using adaptive trajectory optimization",
      "author" : [ "Kahn", "Gregory", "Zhang", "Tianhao", "Levine", "Sergey", "Abbeel", "Pieter" ],
      "venue" : "arXiv preprint arXiv:1603.00622,",
      "citeRegEx" : "Kahn et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Kahn et al\\.",
      "year" : 2016
    }, {
      "title" : "A natural policy gradient",
      "author" : [ "Kakade", "Sham" ],
      "venue" : null,
      "citeRegEx" : "Kakade and Sham.,? \\Q2002\\E",
      "shortCiteRegEx" : "Kakade and Sham.",
      "year" : 2002
    }, {
      "title" : "Approximately optimal approximate reinforcement learning",
      "author" : [ "Kakade", "Sham", "Langford", "John" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Kakade et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Kakade et al\\.",
      "year" : 2002
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Kingma", "Diederik", "Ba", "Jimmy" ],
      "venue" : "arXiv preprint arXiv:1412.6980,",
      "citeRegEx" : "Kingma et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Kingma et al\\.",
      "year" : 2014
    }, {
      "title" : "Deep reinforcement learning for dialogue generation",
      "author" : [ "Li", "Jiwei", "Monroe", "Will", "Ritter", "Alan", "Galley", "Michel", "Gao", "Jianfeng", "Jurafsky", "Dan" ],
      "venue" : "arXiv preprint arXiv:1606.01541,",
      "citeRegEx" : "Li et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2016
    }, {
      "title" : "Human-level control through deep reinforcement learning",
      "author" : [ "Mnih", "Volodymyr" ],
      "venue" : "Nature,",
      "citeRegEx" : "Mnih and Volodymyr,? \\Q2015\\E",
      "shortCiteRegEx" : "Mnih and Volodymyr",
      "year" : 2015
    }, {
      "title" : "Exploiting the connection between pls, lanczos methods and conjugate gradients: alternative proofs of some properties of pls",
      "author" : [ "Phatak", "Aloke", "de Hoog", "Frank" ],
      "venue" : "Journal of Chemometrics,",
      "citeRegEx" : "Phatak et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Phatak et al\\.",
      "year" : 2002
    }, {
      "title" : "Sequence level training with recurrent neural networks",
      "author" : [ "Ranzato", "Marc’Aurelio", "Chopra", "Sumit", "Auli", "Michael", "Zaremba", "Wojciech" ],
      "venue" : "ICLR 2016,",
      "citeRegEx" : "Ranzato et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Ranzato et al\\.",
      "year" : 2015
    }, {
      "title" : "Maximum margin planning",
      "author" : [ "Ratliff", "Nathan D", "Bagnell", "J Andrew", "Zinkevich", "Martin A" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Ratliff et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Ratliff et al\\.",
      "year" : 2006
    }, {
      "title" : "Visual chunking: A list prediction framework for region-based object detection",
      "author" : [ "Rhinehart", "Nicholas", "Zhou", "Jiaji", "Hebert", "Martial", "Bagnell", "J Andrew" ],
      "venue" : "In ICRA. IEEE,",
      "citeRegEx" : "Rhinehart et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Rhinehart et al\\.",
      "year" : 2015
    }, {
      "title" : "Efficient reductions for imitation learning",
      "author" : [ "Ross", "Stéphane", "Bagnell", "J. Andrew" ],
      "venue" : "In AISTATS, pp",
      "citeRegEx" : "Ross et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Ross et al\\.",
      "year" : 2010
    }, {
      "title" : "Reinforcement and imitation learning via interactive no-regret learning",
      "author" : [ "Ross", "Stephane", "Bagnell", "J Andrew" ],
      "venue" : "arXiv preprint arXiv:1406.5979,",
      "citeRegEx" : "Ross et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Ross et al\\.",
      "year" : 2014
    }, {
      "title" : "A reduction of imitation learning and structured prediction to noregret online learning",
      "author" : [ "Ross", "Stéphane", "Gordon", "Geoffrey J", "Bagnell", "J.Andrew" ],
      "venue" : "In AISTATS,",
      "citeRegEx" : "Ross et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Ross et al\\.",
      "year" : 2011
    }, {
      "title" : "Learning policies for contextual submodular prediction",
      "author" : [ "Ross", "Stephane", "Zhou", "Jiaji", "Yue", "Yisong", "Dey", "Debadeepta", "Bagnell", "Drew" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Ross et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Ross et al\\.",
      "year" : 2013
    }, {
      "title" : "Trust region policy optimization",
      "author" : [ "Schulman", "John", "Levine", "Sergey", "Abbeel", "Pieter", "Jordan", "Michael I", "Moritz", "Philipp" ],
      "venue" : "In ICML, pp",
      "citeRegEx" : "Schulman et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Schulman et al\\.",
      "year" : 2015
    }, {
      "title" : "Mastering the game of go with deep neural networks and tree search",
      "author" : [ "Silver", "David" ],
      "venue" : null,
      "citeRegEx" : "Silver and David,? \\Q2016\\E",
      "shortCiteRegEx" : "Silver and David",
      "year" : 2016
    }, {
      "title" : "Sequence to sequence learning with neural networks",
      "author" : [ "Sutskever", "Ilya", "Vinyals", "Oriol", "Le", "Quoc V" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Sutskever et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Sutskever et al\\.",
      "year" : 2014
    }, {
      "title" : "Apprenticeship learning using linear programming",
      "author" : [ "Syed", "Umar", "Bowling", "Michael", "Schapire", "Robert E" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Syed et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Syed et al\\.",
      "year" : 2008
    }, {
      "title" : "Improving multi-step prediction of learned time series models",
      "author" : [ "Venkatraman", "Arun", "Hebert", "Martial", "Bagnell", "J Andrew" ],
      "venue" : null,
      "citeRegEx" : "Venkatraman et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Venkatraman et al\\.",
      "year" : 2015
    }, {
      "title" : "Simple statistical gradient-following algorithms for connectionist reinforcement learning",
      "author" : [ "Williams", "Ronald J" ],
      "venue" : "Machine learning,",
      "citeRegEx" : "Williams and J.,? \\Q1992\\E",
      "shortCiteRegEx" : "Williams and J.",
      "year" : 1992
    }, {
      "title" : "Maximum entropy inverse reinforcement learning",
      "author" : [ "Ziebart", "Brian D", "Maas", "Andrew L", "Bagnell", "J Andrew", "Dey", "Anind K" ],
      "venue" : "In AAAI,",
      "citeRegEx" : "Ziebart et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Ziebart et al\\.",
      "year" : 2008
    }, {
      "title" : "Online Convex Programming and Generalized Infinitesimal Gradient Ascent",
      "author" : [ "Zinkevich", "Martin" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Zinkevich and Martin.,? \\Q2003\\E",
      "shortCiteRegEx" : "Zinkevich and Martin.",
      "year" : 2003
    }, {
      "title" : "Proof of Theorem 5.1 Proof. We construct a reduction from stochastic Multi-Arm Bandits (MAB) to the MDP M̃. A stochastic MAB is defined by S arms denoted as I, ..., I . Each arm I’s cost ci at any time step t is sampled from a fixed but unknown distribution",
      "author" : [ "D. proof" ],
      "venue" : null,
      "citeRegEx" : "proof.,? \\Q2012\\E",
      "shortCiteRegEx" : "proof.",
      "year" : 2012
    } ],
    "referenceMentions" : [ {
      "referenceID" : 26,
      "context" : "Reinforcement Learning (RL), especially deep RL, has dramatically advanced the state of the art in sequential decision making in high-dimensional robotics control tasks as well as in playing video and board games (Schulman et al., 2015; Silver et al., 2016).",
      "startOffset" : 213,
      "endOffset" : 257
    }, {
      "referenceID" : 19,
      "context" : "Though conventional supervised learning of deep models has been pivotal in advancing performance in sequential prediction problems, researchers are beginning to utilize deep RL methods to achieve high performance (Ranzato et al., 2015; Bahdanau et al., 2016; Li et al., 2016).",
      "startOffset" : 213,
      "endOffset" : 275
    }, {
      "referenceID" : 1,
      "context" : "Though conventional supervised learning of deep models has been pivotal in advancing performance in sequential prediction problems, researchers are beginning to utilize deep RL methods to achieve high performance (Ranzato et al., 2015; Bahdanau et al., 2016; Li et al., 2016).",
      "startOffset" : 213,
      "endOffset" : 275
    }, {
      "referenceID" : 16,
      "context" : "Though conventional supervised learning of deep models has been pivotal in advancing performance in sequential prediction problems, researchers are beginning to utilize deep RL methods to achieve high performance (Ranzato et al., 2015; Bahdanau et al., 2016; Li et al., 2016).",
      "startOffset" : 213,
      "endOffset" : 275
    }, {
      "referenceID" : 30,
      "context" : ", 2009), DaD (Venkatraman et al., 2015), AggreVaTe (Ross & Bagnell, 2014), and LOLS (Chang et al.",
      "startOffset" : 13,
      "endOffset" : 39
    }, {
      "referenceID" : 24,
      "context" : "For robotics control problems, this oracle may come from a human expert guiding the robot during the training phase (Abbeel & Ng, 2004) or from an optimal MDP solver (Ross et al., 2011; Kahn et al., 2016) that either may be too slow to use at test time or leverages information unavailable at test time (e.",
      "startOffset" : 166,
      "endOffset" : 204
    }, {
      "referenceID" : 12,
      "context" : "For robotics control problems, this oracle may come from a human expert guiding the robot during the training phase (Abbeel & Ng, 2004) or from an optimal MDP solver (Ross et al., 2011; Kahn et al., 2016) that either may be too slow to use at test time or leverages information unavailable at test time (e.",
      "startOffset" : 166,
      "endOffset" : 204
    }, {
      "referenceID" : 25,
      "context" : ", beam search) or by a clairvoyant greedy algorithm (Daumé III et al., 2009; Ross et al., 2013; Rhinehart et al., 2015; Chang et al., 2015a) that is near-optimal on the task specific performance metric (e.",
      "startOffset" : 52,
      "endOffset" : 140
    }, {
      "referenceID" : 21,
      "context" : ", beam search) or by a clairvoyant greedy algorithm (Daumé III et al., 2009; Ross et al., 2013; Rhinehart et al., 2015; Chang et al., 2015a) that is near-optimal on the task specific performance metric (e.",
      "startOffset" : 52,
      "endOffset" : 140
    }, {
      "referenceID" : 29,
      "context" : "These methods (Abbeel & Ng, 2004; Syed et al., 2008; Ratliff et al., 2006; Ziebart et al., 2008; Finn et al., 2016; Ho & Ermon, 2016) learn either a policy π̂∗ or Q̂∗ from a fixed-size dataset pre-collected from the oracle.",
      "startOffset" : 14,
      "endOffset" : 133
    }, {
      "referenceID" : 20,
      "context" : "These methods (Abbeel & Ng, 2004; Syed et al., 2008; Ratliff et al., 2006; Ziebart et al., 2008; Finn et al., 2016; Ho & Ermon, 2016) learn either a policy π̂∗ or Q̂∗ from a fixed-size dataset pre-collected from the oracle.",
      "startOffset" : 14,
      "endOffset" : 133
    }, {
      "referenceID" : 32,
      "context" : "These methods (Abbeel & Ng, 2004; Syed et al., 2008; Ratliff et al., 2006; Ziebart et al., 2008; Finn et al., 2016; Ho & Ermon, 2016) learn either a policy π̂∗ or Q̂∗ from a fixed-size dataset pre-collected from the oracle.",
      "startOffset" : 14,
      "endOffset" : 133
    }, {
      "referenceID" : 8,
      "context" : "These methods (Abbeel & Ng, 2004; Syed et al., 2008; Ratliff et al., 2006; Ziebart et al., 2008; Finn et al., 2016; Ho & Ermon, 2016) learn either a policy π̂∗ or Q̂∗ from a fixed-size dataset pre-collected from the oracle.",
      "startOffset" : 14,
      "endOffset" : 133
    }, {
      "referenceID" : 24,
      "context" : ", 2009), DAgger (Ross et al., 2011), and AggreVaTe (Ross & Bagnell, 2014) interleave learning and testing procedures to overcome the data mismatch issue and, as a result, work well in practical applications.",
      "startOffset" : 16,
      "endOffset" : 35
    }, {
      "referenceID" : 6,
      "context" : "We use an LSTM-based policy (Duan et al., 2016) where the LSTM’s hidden states provide a compressed feature of the history.",
      "startOffset" : 28,
      "endOffset" : 47
    }, {
      "referenceID" : 26,
      "context" : "In order to learn rich policies such as with LSTMs or deep networks (Schulman et al., 2015), we derive a policy gradient method for imitation learning and sequential prediction.",
      "startOffset" : 68,
      "endOffset" : 91
    }, {
      "referenceID" : 26,
      "context" : "In order to learn rich policies such as with LSTMs or deep networks (Schulman et al., 2015), we derive a policy gradient method for imitation learning and sequential prediction. To do this, we leverage the reduction of IL and sequential prediction to online learning as shown in (Ross & Bagnell, 2014) to learn policies represented by expressive differentiable function approximators. The fundamental idea in Ross & Bagnell (2014) is to use a no-regret online learner to update policies using the following loss function at each episode n:",
      "startOffset" : 69,
      "endOffset" : 431
    }, {
      "referenceID" : 26,
      "context" : "Policy Updates with Natural Gradient Descent We derive a natural gradient update procedure for imitation learning inspired by the success of natural gradient descent in RL (Kakade, 2002; Bagnell & Schneider, 2003; Schulman et al., 2015).",
      "startOffset" : 172,
      "endOffset" : 236
    }, {
      "referenceID" : 26,
      "context" : "Second, we replace KL(πθ||πθn) by KL(πθn ||πθ), which is a local approximation since KL(q||p) and KL(p||q) are equal up to the second order (Kakade & Langford, 2002; Schulman et al., 2015).",
      "startOffset" : 140,
      "endOffset" : 188
    }, {
      "referenceID" : 9,
      "context" : "11 by the state-action advantage function At (s i,n t , a) = Q ∗ t (s i,n t , a) − V ∗ t (s i,n t ), which leads to the following two unbiased and variancereduced gradient estimations (Greensmith et al., 2004):",
      "startOffset" : 184,
      "endOffset" : 209
    }, {
      "referenceID" : 26,
      "context" : "This approach is used in TRPO (Schulman et al., 2015).",
      "startOffset" : 30,
      "endOffset" : 53
    }, {
      "referenceID" : 24,
      "context" : "This mixing strategy with decay rate was first introduced in (Ross et al., 2011) for IL, and later on was used in sequence prediction (Bengio et al.",
      "startOffset" : 61,
      "endOffset" : 80
    }, {
      "referenceID" : 2,
      "context" : ", 2011) for IL, and later on was used in sequence prediction (Bengio et al., 2015).",
      "startOffset" : 61,
      "endOffset" : 82
    }, {
      "referenceID" : 11,
      "context" : "For example, the Upper Confidence Bound (UCB) based, near-optimal optimistic RL algorithms from (Jaksch et al., 2010), specifically designed for efficient exploration, admit regret Õ(HS √ HAN), leading to a gap of approximately √ HAS compared to the regret bound of imitation learning shown in Eq.",
      "startOffset" : 96,
      "endOffset" : 117
    }, {
      "referenceID" : 6,
      "context" : "For RL we use REINFORCE (Williams, 1992) and Truncated Natural Policy Gradient (TNPG) (Duan et al., 2016).",
      "startOffset" : 86,
      "endOffset" : 105
    }, {
      "referenceID" : 26,
      "context" : "Following the neural network settings described in Schulman et al. (2015), the expert policy π∗ is obtained from TRPO with one hidden layer (64 hidden states), which is the same structure that we use to represent our policies πθ.",
      "startOffset" : 51,
      "endOffset" : 74
    }, {
      "referenceID" : 28,
      "context" : "The RNN policy follows the design from (Sutskever et al., 2014).",
      "startOffset" : 39,
      "endOffset" : 63
    } ],
    "year" : 2017,
    "abstractText" : "Researchers have demonstrated state-of-the-art performance in sequential decision making problems (e.g., robotics control, sequential prediction) with deep neural network models. One often has access to near-optimal oracles that achieve good performance on the task during training. We demonstrate that AggreVaTeD — a policy gradient extension of the Imitation Learning (IL) approach of (Ross & Bagnell, 2014) — can leverage such an oracle to achieve faster and better solutions with less training data than a less-informed Reinforcement Learning (RL) technique. Using both feedforward and recurrent neural predictors, we present stochastic gradient procedures on a sequential prediction task, dependency-parsing from raw image data, as well as on various high dimensional robotics control problems. We also provide a comprehensive theoretical study of IL that demonstrates we can expect up to exponentially lower sample complexity for learning with AggreVaTeD than with RL algorithms, which backs our empirical findings. Our results and theory indicate that the proposed approach can achieve superior performance with respect to the oracle when the demonstrator is sub-optimal.",
    "creator" : "LaTeX with hyperref package"
  }
}