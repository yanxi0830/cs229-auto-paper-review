{
  "name" : "1611.04535.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ "Maria-Florina Balcan", "Vaishnavh Nagarajan", "Ellen Vitercik", "Colin White" ],
    "emails" : [ "ninamf@cs.cmu.edu.", "vaishnavh@cs.cmu.edu.", "vitercik@cs.cmu.edu.", "crwhite@cs.cmu.edu." ],
    "sections" : [ {
      "heading" : null,
      "text" : "In this work, we address this problem for integer quadratic programming and clustering within a learning-theoretic framework where the learning algorithm is given samples from an application-specific distribution over problem instances and learns an algorithm which performs well over the distribution. We provide strong sample complexity guarantees and computationally efficient algorithms which optimize an algorithm family’s parameters to best suit typical inputs from a particular application. We analyze these algorithm families using the learning-theoretic notion of pseudo-dimension. Along with upper bounds, we provide the first pseudo-dimension lower bounds in this line of work, which require an involved analysis of each algorithm family’s overall performance on carefully constructed problem instances. Our bounds are tight and therefore nail down the intrinsic complexity of the algorithm classes we analyze, which consist of multi-stage optimization procedures and which are significantly richer and more complex than classes commonly used in learning theory. For example, even for classes of algorithms parameterized by a single parameter, we prove tight superconstant pseudo-dimension bounds. In this way, not only does our work contribute to the study of algorithm design and analysis by providing a theoretical grounding for application-specific algorithm selection, but it also pushes the boundaries of learning theory.\n∗Authors’ addresses: {ninamf,vaishnavh,vitercik,crwhite}@cs.cmu.edu.\nar X\niv :1\n61 1.\n04 53\n5v 1\n[ cs\n.D S]\n1 4\nN ov"
    }, {
      "heading" : "1 Introduction",
      "text" : "Many NP-hard problems arise in a variety of diverse and oftentimes unrelated application domains. For example, clustering is a widely-studied NP-hard problem in unsupervised machine learning, used to group protein sequences by function, organize documents in databases by subject, and choose the best locations for fire stations or cell-phone towers throughout a city. Although the underlying objective is the same, a “typical problem instance” in one setting may be significantly different from that in another, causing approximation algorithms to have inconsistent performance across the different application domains.\nWe study the problem of characterizing which algorithms are best for which contexts, a task often referred to as application-specific algorithm selection. This line of work allows researchers to compare algorithms according to an application-specific metric, such as expected performance over their problem domain, rather than a more standard, application-agnostic metric, such as worst-case analysis. We conduct a learning-theoretic analysis through a framework introduced by Gupta and Roughgarden [14], wherein an application domain is modeled as a distribution over problem instances. We then fix an infinite class of approximation algorithms for that problem, and design computationally efficient algorithms which learn the approximation algorithm with the best performance with respect to the distribution, and therefore an algorithm which will perform well in the specific application domain. Gupta and Roughgarden focused on greedy and local search heuristics, and left as an open question what other classes of algorithms can be learned over.\nIn this light, we study infinite classes of semidefinite programming (SDP) rounding algorithms for approximating integer quadratic programs, as well as agglomerative clustering algorithms with dynamic programming. The SDP rounding algorithm classes we focus on are known to perform particularly well on the max-cut problem; they contain well-known algorithms such as the GoemansWilliamson (GW) max-cut approximation algorithm, which performs well in the worst-case [12], and more generally all s-linear rounding functions, which were introduced by Feige and Langberg [11]. We note that these approximation algorithms are randomized, whereas Gupta and Roughgarden’s work focused on deterministic algorithms.\nWe also study infinite classes of clustering algorithms with a linkage-based step and a pruning step. We parameterize both of these modules separately, providing an infinite number of linkagebased procedures and an infinite number of pruning steps, and our clustering algorithm classes consist of all possible pairings of a linkage step and pruning step. The linkage-based modules include the classic single-, complete-, and average-linkage algorithms, known to return nearlyoptimal clusterings in a variety of settings [3, 13, 5, 4]. The dynamic programming modules include the k-median, k-means, and k-center objectives, as well as objectives that minimize the distance to a ground truth clustering."
    }, {
      "heading" : "1.1 Problem descriptions and our results",
      "text" : "Before describing our results, we provide a more formal overview of application-specific algorithm selection through a learning-theoretic lens, as introduced by Gupta and Roughgarden [14]. We fix a computational problem, such as max-cut or k-means clustering, and assume that there exists an unknown, application-dependent distribution D over the set of all problem instances Π. For example, the support of D might be a set of social networks, and the researcher’s goal is to choose an algorithm with which to perform a series of clustering analyses. Next, we fix a class of algorithms A. Given a cost function cost : A × Π → [0, H], the learner is tasked with finding an algorithm h ∈ A that approximately optimizes the expected cost with respect to the distribution D, i.e., the one that approximately minimizes Ex∼D[cost(h, x)]. We now formalize this notion.\nDefinition 1 ([14]). A learning algorithm L ( , δ)-learns the algorithm class A with respect to the cost function cost if, for every distribution D over Π, with probability at least 1 − δ over the choice of a sample S ∼ Dm, L outputs an algorithm ĥ ∈ A such that Ex∼D [ cost ( ĥ, x )] − minh∈A {Ex∼D[cost(h, x)]} < . We require that the number of samples be polynomial in n, 1 , and 1 δ , where n is an upper bound on the size of the problem instances in the support of D.1 Further, we say that L is computationally efficient if its running time is also polynomial in n, 1 , and\n1 δ .\nWe present algorithms which learn an approximately optimal algorithm for a given application domain, and we analyze the sample complexity of our algorithms. We use the learning-theoretic concept of pseudo-dimension, which we define formally in Section 2, in both our algorithm design and sample complexity analysis. Pseudo-dimension measures the complexity, or richness, of a function class, and extends the well-known VC dimension to real-valued functions.\nSDP-based methods for integer quadratic programming. Many NP-hard problems, such as max-cut, max-2SAT, and correlation clustering, can be represented as an integer quadratic program of the following form. The input is an n× n matrix A with nonnegative diagonal entries and the output is a binary assignment to each variable in the set X = {x1, . . . , xn} which maximizes∑\ni,j∈[n] aijxixj . 2 In this formulation, xi ∈ {−1, 1} for all i ∈ [n]. Integer quadratic programs have diverse applications, including capital budgeting and financial analysis [19], traffic message management problems [29], machine scheduling [1], and molecular conformation [22]. Max-cut is a special case of integer quadratic programming which has garnered significant attention in both theoretical and empirical communities in computer science. The seminal Goemans-Williamson algorithm is now a textbook example of semidefinite programming [12, 28, 26] and max-cut continues to arise in applications such as machine learning [27], circuit design [31], and computational biology [24]. In fact, libraries of max-cut algorithms have been the subject of empirical work in application-specific algorithm selection [10] and max-cut and integer quadratic programming in general have been used as a test bed for new ideas in heuristic design [18].\nThe best approximation algorithms for integer quadratic programming relax the problem to a semidefinite program, where the input is the same matrix A, but the output is a set of unit vectors maximizing ∑ i,j aij〈~ui, ~uj〉. The final step is to transform, or “round,” the set of vectors into an assignment of the binary variables in X. This assignment corresponds to a feasible solution to the original integer quadratic program. There are infinitely many rounding techniques to choose from, many of which are randomized. These algorithms make up the class of Random Projection, Randomized Rounding algorithms (RPR2), a general framework introduced by Feige and Langberg [11].\nRPR2 algorithms are known to perform well in theory and practice. When the integer quadratic program is a formulation of the max-cut problem, the class of RPR2 algorithms contain the groundbreaking Goemans-Williamson algorithm, which achieves a 0.878 approximation ratio [12]. Assuming the unique games conjecture and P 6= NP , this approximation is optimal to within any additive constant [17]. More generally, if A is any real-valued n × n matrix with nonnegative diagonal entries, then there exists an RPR2 algorithm that achieves an approximation ratio of Ω(1/ log n) [8], and in the worst case, this ratio is tight [2]. Finally, if A is positive semi-definite, then there exists an RPR2 algorithm that achieves a 2/π approximation ratio [6].\n1For example, in clustering, n is an upper bound on the number of points in a problem instance, and in integer quadratic programming, n is an upper bound on the number of variables in a problem instance.\n2When the diagonal entries are allowed to be negative, the ratio between the semidefinite relaxation and the integral optimum can become arbitrarily large, so we restrict the domain to matrices with nonnegative diagonal entries.\nWe analyze several classes of RPR2 rounding function classes, including s-linear [11], outward rotation [32], and ̃-discretized rounding functions [21]. For each class, we derive bounds on the number of samples needed to learn an approximately optimal rounding function with respect to an underlying distribution over problem instances. In particular, we prove that the pseudo-dimension of the class of s-linear RPR2 algorithms is Θ(log n) and the pseudo-dimension of the class of outward rotation RPR2 algorithms is O(log n). Therefore, O ( 1 2 ( log n log 1 + log 1 δ )) samples are sufficient to ( , δ)-learn the optimal algorithm in either of these classes. Finally, we show that O ( 1 2 ( 1 ̃2 log 1 + log 1 δ )) samples are required to ( , δ)-learn the optimal algorithm in the class of ̃-discretized rounding functions. We also provide a computationally efficient learning algorithm for learning an approximately optimal s-linear or outward rotation rounding function with respect to the distribution.\nFinally, our analyses are not specific to RPR2 algorithms that first find an SDP embedding, though this is the most common technique. Rather, our results also apply more generally to any class of RPR2 algorithms where the first step is to find some set of vectors on the unit sphere and then round those vectors to a binary solution using one of the rounding functions we study. This generalization has led to faster approximation algorithms with strong empirical performance. For example, in order to quickly find approximate solutions to the max-cut problem, Johansson et al. replace the SDP embedding with an embedding that that is faster to compute, and then run the standard GW algorithm in order to transform the vector solution into a graph cut [16].\nClustering by agglomerative algorithms with dynamic programming. Given a set of n datapoints and the pairwise distances between them, at a high level, the goal of clustering is to partition the points into groups such that distances within each group are minimized and distances between each group are maximized. A classic way to accomplish this task is to use an objective function. Common clustering objective functions include k-means, k-median, and k-center, which we define later on. We focus on a very general problem where the learner’s main goal is to minimize an abstract cost function such as the cluster purity or the clustering objective function. We study infinite classes of two-step clustering algorithms consisting of a linkage-based step and a dynamic programming step. First, the algorithm runs one of an infinite number of linkage-based routines to construct a hierarchical tree of clusters. Next, the algorithm runs a dynamic programming procedure to find the pruning of this tree that minimizes one of an infinite number of clustering objectives. For example, if the clustering objective is the k-means objective, then the dynamic programming step will return the optimal k-means pruning of the cluster tree.\nFor the linkage-based procedure, we consider several natural parameterizations of agglomerative procedures which give rise to a spectrum of algorithms interpolating between the popular single-, average-, and complete-linkage procedures, known to perform nearly optimally in many settings [3, 4, 5, 13]. For the dynamic programming step, we study an infinite class of objectives which include the standard k-means, k-median, and k-center objectives. We show how to learn the best agglomerative algorithm and pruning objective function pair, thus extending our work to multiparameter algorithms. We provide tight pseudo-dimension bounds, ranging from Θ(logn) for simpler algorithm classes to Θ(n) for more complex algorithm classes. This implies that few samples are required to ( , δ)-learn the optimal algorithm in any of these classes. Finally, we present algorithms for learning over these classes.\nKey Challenges. One of the key challenges in analyzing the pseudo-dimension of the algorithm classes we study is that to do so, we must develop deep insights into how changes to an algorithm’s parameters effect the solution the algorithm returns on an arbitrary input. This obstacle is best exemplified by our clustering analysis, where the cost function could be, for example, the k-means or k-median objective function, or even the distance to some ground-truth clustering. As we range\nover algorithm parameters, the cost of the returned clustering may vary unpredictably. In this way, our algorithm analyses require more care than standard pseudo-dimension deriva-\ntions commonly found in machine learning contexts. Typically, for well-understood function classes used in machine learning, such as linear separators or other smooth curves in Euclidean spaces, there is a simple mapping from the parameters of a specific hypothesis to its prediction on a given example and a close connection between the distance in the parameter space between two parameter vectors and the distance in function space between their associated hypotheses. Roughly speaking, it is necessary to understand this connection in order to determine how many significantly different hypotheses there are over the full range of parameters. Due to the inherent complexity of the classes we consider, connecting the parameter space to the space of approximation algorithms and their associated costs requires a much more delicate analysis. Indeed, the key technical part of our work involves understanding this connection from a learning theoretic perspective."
    }, {
      "heading" : "1.2 Additional related work",
      "text" : "Application-specific algorithm selection has been studied in the artificial intelligence community for decades and is closely related to automated algorithm configuration and algorithm portfolio selection. The algorithm selection problem was first proposed by Rice in 1979 [23], and has since led to breakthroughs in diverse fields including combinatorial auctions [20], scientific computing [9], vehicle routing [7], and SAT [30], among many others. In this line of work, as in ours, the authors assume a distribution over problem instances, and they aim to design algorithms or portfolios of algorithms that will perform well in expectation over the distribution. They implement algorithm selection techniques which are often based on machine learning in order to satisfy their design goals. Arguably the most prominent algorithm selection system is Satzilla, a SAT solver developed by Xu et al. [30]. Satzilla dominated the SAT competition in 2007, thus demonstrating the potential impact of algorithm selection research. However, results in this line of work have come with few theoretical guarantees.\nGupta and Roughgarden leveraged tools from learning theory to conduct a theoretical analysis of several fundamental problems in application-specific algorithm selection [14]. In their work, they study algorithm families such as greedy heuristic algorithms for problems like knapsack, bucketbased sorting algorithms, variants on gradient descent, and they also study application-specific algorithm selection in the online setting. We bolster this line of work by studying algorithms for problems which are ubiquitous in machine learning and optimization: integer quadratic programming and clustering. In this paper, we develop techniques for analyzing randomized algorithms, whereas the algorithms analyzed in the previous work were deterministic. We also provide the first pseudo-dimension lower bounds in this line of work, which require an involved analysis of the each algorithm family’s performance on carefully constructed problem instances. We believe that our lower bounds are of particular interest because for several of the single-parameter algorithm classes we study, we prove Ω(log n) lower bounds. This contradicts intuition because one would naturally expect the pseudo-dimension of a function class to be directly proportional to the number of parameters defining that class."
    }, {
      "heading" : "2 Preliminaries and definitions",
      "text" : "In this section, we provide the notation and definitions we will need from learning theory. We begin with the definition of pseudo-dimension, a natural extension of the well-known VC dimension to learning over classes of real-valued functions. We state the definition generally in terms of an arbitrary class of real-valued functions F , which we then describe how to instantiate for a class of\nalgorithms and an arbitrary cost function. We say that a finite subset S = {x1, . . . , xm} of X is shattered by F if there exist real-valued witnesses r1, . . . , rm such that for all subsets T ⊆ S, there exists a function fT ∈ F such that fT (xi) ≤ ri if and only if i ∈ T . The pseudo-dimension of F , denoted by Pdim(F), is the cardinality of the largest subset of X shattered by F . By bounding Pdim(F), we may derive strong sample complexity guarantees, as follows.\nTheorem 1. [e.g. [15]] Let F be a class of functions with domain X and range in [0, H], and suppose F has pseudo-dimension d. For every distribution D over X, every > 0, and every δ ∈ (0, 1], if m ≥ c ( H )2 ( d log ( H ) + log ( 1 δ )) for a suitable constant c (independent of all other parameters),\nthen with probability at least 1−δ over m samples x1, . . . , xm ∼ D, ∣∣( 1 m ∑m i=1 f(xi) ) − Ex∼D[f(x)] ∣∣ < for every f ∈ F .\nWe may similarly derive the pseudo-dimension of a class of algorithms A together with a cost function cost(h, x) for h ∈ A. Specifically, we say that a set of input problem instances S is shattered by A if we can similarly define |S| witnesses and 2|S| algorithms in A which in turn induce all 2|S| binary labelings of S: whether or not cost(h, x) is less than its witness. Theorem 1 then characterizes the sample complexity required to ensure that for all algorithms h ∈ A, the empirical cost of h on the sample is close to the expected cost of h with respect to the distribution. It is easy to show that if h∗ is the algorithm that minimizes the expected cost and ĥ is the algorithm that minimizes empirical cost, then with high probability, the expected cost of running ĥ is close to\noptimal. Specifically, with probability at least 1− δ, Ex∼D [ cost ( ĥ, x )] −Ex∼D [cost (h∗, x)] < .\nTherefore, if a learning algorithm receives as input a sufficiently large set of samples and returns the algorithm which performs best on that sample, we can be guaranteed that this algorithm is close to optimal with respect to the underlying distribution."
    }, {
      "heading" : "3 SDP-based methods for integer quadratic programming",
      "text" : "In this section, we study several infinite classes of integer-quadratic programming (IQP) approximation algorithms. These classes consist of SDP rounding algorithms and are a generalization of the seminal Goemans-Williamson (GW) max-cut algorithm [12]. We prove that it is possible to learn the optimal algorithm from a fixed class over a specific application domain, and for many of the classes we study, this learning procedure is computationally efficient.\nWe focus on integer-quadratic programs of the form ∑\ni,j∈[n] aijxixj , where the input is a matrix A with nonnegative diagonal entries and the output is an assignment of the binary variables X = {x1, . . . , xn} maximizing this sum. Specifically, each variable in X is set to either −1 or 1. This problem is also known as MaxQP [8]. Most algorithms with the best approximation guarantees use an SDP relaxation. The SDP relaxation has the form\nmaximize ∑\ni,j∈[n]\naij〈~ui, ~uj〉 (1)\nsubject to ~ui ∈ Sn−1. Once we have the resulting set of vectors {~u1, . . . , ~un}, we must decide how they represent an assignment of the binary variables in X. In the GW algorithm, the vectors are projected onto a random vector ~Z drawn from an n-dimensional Gaussian. Next, there is a rounding step; if the directed distance of the resulting projection is greater than 0, then the corresponding binary variable is set to 1, and otherwise it is set to −1.\nThe GW algorithm can be generalized, and sometimes improved upon, by probabilistically assigning each binary variable to 1 or −1. Specifically, in the final rounding step, any rounding function r : R → [−1, 1] can be used to specify that a variable xi is set to 1 with probability 1 2 + 1 2 · r ( 〈~Z, ~ui〉 ) and −1 with probability 12 − 12 · r ( 〈~Z, ~ui〉 ) . See Algorithm 1 for the pseudocode of this generalization. This type of algorithm is known as a Random Projection, Randomized\nAlgorithm 1 SDP rounding algorithm with rounding function r\nInput: Matrix A ∈ Rn×n. 1: Solve the SDP (1) for the optimal embedding U = (~u1, . . . , ~un). 2: Choose a random vector ~Z ∈ Rn according to the n-dimensional Gaussian distribution. 3: Define the fractional assignment h : X → [−1, 1] such that h(xi) = r ( 〈~Z, ~ui〉 ) ."
    }, {
      "heading" : "Output: h.",
      "text" : "Rounding (RPR2) algorithm, so named by Feige and Langberg [11]. The randomized assignment h produced by an RPR2 algorithm is called a fractional assignment. Based on the output h, we can derive a proper assignment of the variables x1, . . . , xn where xi is set to 1 with probability 1 2 + 1 2h(xi) and −1 with probability 12 − 12h(xi).\nIn this work, we study several rich function classes from which the rounding function r may be chosen. In the following section, we analyze the class of s-linear functions. For the max-cut problem, Feige and Langberg proved that when the maximium cut in the graph is not very large, an approximation ratio above the GW ratio is possible using an s-linear rounding function [11]. For example, they proved that if the optimal cut contains at most a 0.6 fraction of the edges, then the ratio is at least 0.9128. The optimal choice of s depends on the graph, but we give a polynomial time algorithm to learn a nearly optimal value for s in expectation over a distribution of problem instances.\nAs we describe in Section 1.1, our techniques are not specific to RPR2 algorithms that first find the optimal embedding of the SDP relaxation. They also apply to any class of RPR2 algorithms where the first step is to find some fixed type of embedding on the unit sphere and then round the vector solution to a binary solution using one of the classes of rounding functions we study. For example, Johansson et al. quickly find strong approximate solutions to the max-cut problem by first approximating an embedding based on the Lovász theta function and then running the standard GW algorithm to round this embedding into a graph cut [16].\nWe also analyze rounding functions beyond s-linear rounding functions. In Appendix B, we study the class of ̃-discretized rounding functions as defined by O’Donnell and Wu [21] and the class of outward rotation algorithms proposed by Zwick [32]. We then extend these results to general classes of “sigmoid-like” rounding functions, which include the classes of s-linear and outward rotation functions.\n3.1 s-linear rounding functions\nAn s-linear rounding function φs : R→ [−1, 1] is parameterized by a constant s > 0, where\nφs(y) =    −1 if y < −s y/s if − s ≤ y ≤ s 1 if y > s.\nSee Figure 1 for a graph of φ2.\nRecall that our goal is to devise an algorithm Lslin that ( , δ)-learns the best s-linear rounding function with respect to a distribution D over MaxQPs problem instances, given m samples from D. Specifically, let slin∗s(A) be the expected value of the solution returned by RPR\n2 using the rounding function φs when evaluated on the MaxQP problem instance defined by the matrix A. We instantiate the cost function cost to be cost(A, s) = −slin∗s(A). Here, we take the negative of slin∗s(A) since our goal is to find the s parameter that will maximize this value in expectation, and minimizing cost(A, s) now amounts to maximizing slin∗s(A). In accordance with our goal, we require that Lslin returns a value ŝ such that with probability at least 1 − δ, if s∗ maximizes E\nA∼D [slin∗s(A)],\nthen E A∼D [slin∗s∗(A)]− E A∼D [slin∗ŝ(A)] < . Naturally, one might expect that the first step would be to bound the pseudo-dimension of the function class Hslin∗ = {slin∗s : A→ [0, 1] | s > 0}, where A is the set of all real-valued n×n matrices with nonnegative diagonal entries. However, we pursue an alternative route that provides a simpler sample complexity and algorithmic analysis.\nIn particular, we instead analyze the class Hslin = {slins : A× Rn → [0, 1] | s > 0}, where slins ( A, ~Z ) is the value of the fractional assignment produced by projecting the SDP embedding of\nA onto ~Z and rounding the directed distances of the resulting projections (multiplied by ||~Z||) using the rounding function φs. Explicitly, slins ( A, ~Z ) = ∑ i,j aijφs ( 〈~ui, ~Z〉 ) · φs ( 〈~ui, ~Z〉 ) . Notice that slin∗s(A) = EZ∼Z [ slins ( A, ~Z )] , where Z denotes the standard n-dimensional Gaussian distribution.3\nWe provide tight bounds on the pseudo-dimension of Hslin, rather than Hslin∗ . Nonetheless, we show that we can use our bound on the pseudo-dimension of Hslin to derive strong generalization guarantees for optimizing over the class of algorithms we ultimately care about, Hslin∗ .\nSpecifically, we then show that empirical maximization over Hslin amounts to empirical maximization overHslin∗ . In particular, suppose that a learning algorithm Lslin takes as inputm samples( A(i), ~Z(i) ) ∼ D×Z and finds a value ŝ such that slinŝ ∈ Hslin maximizes 1m ∑m i=1 slins ( A(i), ~Z(i) ) . We prove that RPR2 using the ŝ-linear rounding function is nearly optimal with respect to D as well. Formally, we prove the following theorem, the proof of which can be found in Appendix A.\nTheorem 2. Let d be the pseudo-dimension of Hslin. Suppose that Lslin is an algorithm that takes as input m samples ( A(i), ~Z(i) ) ∼ D × Z, where m = O ( 1 2 ( d log 1 + log 1 δ )) , and returns the parameter ŝ which maximizes 1m ∑m i=1 slins ( A(i), ~Z(i) ) . Then Lslin ( , δ)-learns the class of\n3There are, in fact, two sources of randomness that contribute to the value of slin∗s(A). First, there is the draw of the random hyperplane Z ∼ Z. Stripping away this layer of randomness, we obtain the function slins ( A, ~Z ) = ∑ i,j aijφs ( 〈~ui, ~Z〉 ) · φs ( 〈~ui, ~Z〉 ) . This sum hides a second layer of randomness, since\nE[xi] = 1 · (\n1 2 + 1 2 · φs\n( 〈~ui, ~Z〉 )) + (−1) · ( 1 2 − 1 2 · φs ( 〈~ui, ~Z〉 )) = φs ( 〈~ui, ~Z〉 ) , which means that slins ( A, ~Z ) =∑\ni,j aijE[xi]E[xj ].\ns-linear rounding functions with respect to the cost function −slin∗s and it is computationally efficient.\nAgain, we set cost(A, s) = −slin∗s(A) since minimizing cost(A, s) then amounts to maximizing slin∗s(A). In the remainder of this section, we will prove that the pseudo-dimension of Hslin is Θ(log n) and then we will present a computationally efficient algorithm Lslin.\nTheorem 3. Let Hslin = {slins : A× Rn → [0, 1] | s > 0}. Then Pdim(Hslin) = Θ(log n). Proof. Theorem 3 follows from Lemma 2 and Lemma 3, wherein we provide matching upper and lower bounds on Pdim(Hslin) of O(log n) and Ω(log n).\nOur lower bound is particularly strong because it holds for a family of positive semidefinite matrices, rather than a more general family of real-valued matrices. In our analysis, we will often\nfix a tuple ( A, ~Z ) and consider slins ( A, ~Z ) as a function of s. We denote this function as\nslinA,~Z(s). We begin with a helpful lemma.\nLemma 1. The function slinA,~Z : R>0 → R is made up of n+ 1 piecewise quadratic components. Moreover, if the border between two components falls at some s ∈ R>0, then it must be that s =∣∣∣〈~ui, ~Z〉 ∣∣∣ for some ~ui in the optimal SDP embedding of A.\nProof. First, let X = {~u1, . . . , ~un} ⊂ Sn−1 be the optimal embedding of A. Then we may write slinA,~Z(s) = ∑ i,j aijφs ( 〈~ui, ~Z〉 ) · φs ( 〈~ui, ~Z〉 ) . For any ~ui ∈ X, the specific form of φs ( 〈~Z, ~ui〉 ) depends solely on whether 〈~Z, ~ui〉 < −s, −s ≤ 〈~Z, ~ui〉 ≤ s, or s < 〈~Z, ~ui〉. Of course, if 〈~Z, ~ui〉 ≥ 0 or 〈~Z, ~ui〉 < 0, we can disregard the possibility that 〈~Z, ~ui〉 < −s or 〈~Z, ~ui〉 > s, respectively. Then so long as ∣∣∣〈~Z, ~ui〉 ∣∣∣ > s, we have that φs ( 〈~Z, ~ui〉 ) = ±1, where the sign depends on the sign of 〈~Z, ~ui〉. Further, when s grows to the point where s > ∣∣∣〈~Z, ~uj〉 ∣∣∣, we have that φs ( 〈~Z, ~ui〉 ) = 〈~Z, ~uj〉/s. Therefore, if we order the set of real values {∣∣∣〈~Z, ~u1〉 ∣∣∣ , . . . , ∣∣∣〈~Z, ~un〉 ∣∣∣ } , then so long as s falls between\ntwo consecutive elements of this ordering, the form of slinA,~Z(s) is fixed. In particular, each summand is either a constant, a constant multiplied by 1s , or a constant multiplied by 1 s2\n, perhaps accompanied by an additive constant term. This means that we may partition the positive real line into n+ 1 intervals where the form of slin(A,~Z)(s) is a fixed quadratic function, as claimed.\nLemma 2. Pdim(Hslin) = O(log n). Proof. We prove this upper bound by showing that if a set S of size m is shatterable, then m = O(log n). This means that the largest shatterable set must be of size O(log n), so the pseudo-\ndimension of Hslin is O(log n). We arrive at this bound by fixing a tuple ( A(i), ~Z(i) ) ∈ S and ana-\nlyzing slinA,~Z(s). In particular, we make use of Lemma 1, from which we know that slin(A,~Z)(s) is composed of n+ 1 piecewise quadratic components. Therefore, if ri is the witness corresponding\nto the element ( A(i), ~Z(i) ) , we can partition the positive real line into at most 3(n + 1) intervals\nwhere slinA,~Z(s) is always either less than its witness ri or greater than ri as s varies over one fixed interval. The constant 3 term comes from the fact that for a single, continuous quadratic component of slin(A,~Z)(s), the function may equal ri at most twice, so there are at most three subintervals where the function is less than or greater than ri\nNow, S consists of m tuples ( A(i), ~Z(i) ) , each of which corresponds to its own partition of the\npositive real line. If we merge these partitions (as shown in Figure 2), simple algebra shows that\nwe are left with at most (3n+ 2)m+ 1 intervals such that for all i ∈ [m], slinA(i), ~Z(i)(s) is always either less than its witness ri or greater than ri as s varies over one fixed interval. In other words, in one interval, the binary labeling of S, defined by whether each sample is less than or greater than its witness, is fixed. This means that if S is shatterable, the 2m values of s which induce all 2m binary labelings of S must come from distinct intervals. Therefore 2m ≤ (3n + 2)m + 1, so m = O(log n).\nLemma 3. Pdim(Hslin) = Ω(log n).\nProof sketch. In order to prove that the pseudo dimension of Hslin is at least c log n for some c, we present a set S = {( G(1), ~Z(1) ) , . . . , ( G(m), ~Z(m) )} of m = c log n graphs and projection vectors that can be shattered by Hslin. In other words, there exist m witnesses r1, . . . , rm and 2m = nc s values H = {s1, . . . , snc} such that for all T ⊆ [m], there exists sT ∈ H such that if j ∈ T , then slinST ( G(j), ~Z(j) ) > rj and if j 6∈ T , then slinST ( G(j), ~Z(j) ) ≤ rj .\nTo build S, we use the same graph G for all G(j) and we vary ~Z(j). We set G to be the graph composed of bn/4c disjoint copies of K4. Via a careful choice of the vectors ~Z(j) and witnesses rj , we pick out 2m critical values of s, which we call C, such that slinG,~Z(1)(s) switches from above its witness to below its witness for every other value of critical values in C. Meanwhile, slinG,~Z(2)(s) switches from below its witness to above its witness half as often as slinG,~Z(1)(s). Similarly, slinG,~Z(3)(s) switches from below its witness to above its witness half as often as slinG,~Z(2)(s), and so on. This is illustrated by Figure 3. Therefore, we achieve every binary labeling of S (whether slins ( G, ~Z(i) ) is less than its witness or greater than its witness) using the functions {slins | s ∈ C}, so S is shattered by Hslin.\nWe now claim that Algorithm 2 learns the best s-linear function over a sample in polynomial time. Lemma 4. Algorithm 2 produces the value ŝ which maximizes 1m ∑m i=1 slins ( A(i), ~Z(i) ) over the input sample S. Algorithm 2 has running time polynomial in m and n.\nProof of Lemma 4. First, define hS, ~Z(s) = 1 m ∑m i=1 slinA(i), ~Z(i)(s), which we claim that Algorithm 2 maximizes. In Lemma 1, we proved that each function slinA(i), ~Z(i)(s) is made up of\nAlgorithm 2 An algorithm for finding an empirical value maximizing s-linear rounding function Input: Sample S = {( A(1), ~Z(1) ) , . . . , ( A(m), ~Z(m) )}\n1: For all i, solve for the SDP embedding U (i) of A(i), where U (i) = ( ~u\n(i) 1 , . . . , ~u (i) n\n) .\n2: Let T = { s1, . . . , s|T | } be the set of all values s > 0 such that there exists a pair of indices\nj ∈ [n], i ∈ [m] with ∣∣∣〈~Z(i), ~u(i)j 〉 ∣∣∣ = s. 3: For i ∈ [|T | − 1], let ŝi be the value in [si, si+1] which maximizes 1m ∑m i=1 slinA(i), ~Z(i)(s).\n4: Let ŝ be the value in { ŝ1, . . . , ŝ|T |−1 } that maximizes 1m ∑m i=1 slinA(i), ~Z(i)(s).\nOutput: ŝ\nat most n + 1 piecewise quadratic components. Therefore, hS, ~Z(s) is made up of at most mn + 1 piecewise quadratic components. Moreover, by Lemma 1, if the border between two components\nfalls at some s ∈ R>0, then it must be that ∣∣∣〈~Z(i), ~x(i)j 〉 ∣∣∣ = s for some ~x(i)j in the optimal max-cut SDP embedding of A(i). These are the thresholds which are computed in Step 2 of Algorithm 2. Therefore, as we increase s starting at 0, s will be a fixed quadratic function between the thresholds, so it is simple to find the optimal value of s between any pair of consecutive thresholds (Step 3),\nand then the value maximizing hS, ~Z(s) = 1 m ∑m i=1 slins ( A(i), ~Z(i) ) (Step 4), which is the global optimum.\nLemma 4 together with Theorem 2 and Theorem 3 imply our main result.\nTheorem 4. Given an input sample of size m = O ( 1 2 ( log n log 1 + 1 δ )) drawn from (D ×Z)m, Algorithm 2 ( , δ)-learns the class of s-linear rounding functions with respect to the cost function −slin∗s and it is computationally efficient. Proof. Let S = {( A(1), ~Z(1) ) , . . . , ( A(m), ~Z(m) )} be a sample of size m. In Lemma 4, we prove that Algorithm 2 on input S returns the value ŝ which maximizes 1m ∑m i=1 slins ( A(i), ~Z(i) ) in polynomial time. Theorem 2 together with Theorem 3 imply that so long as m = O ( 1 2 ( log n log 1 + 1 δ )) , then Algorithm 2 ( , δ)-learns the best s-linear function with respect to D."
    }, {
      "heading" : "4 Agglomerative algorithms with dynamic programming",
      "text" : "In this section, we start with an overview of agglomerative algorithms with dynamic programming, which include several widely-studied clustering algorithms, and then we define several parameterized classes of such algorithms. As in the previous section, we prove it is possible to learn the optimal algorithm from a fixed class for a specific application, and for many of the classes we analyze, this procedure is computationally efficient.\nWe focus on agglomerative algorithms with dynamic programming for clustering problems. A clustering instance V = (V, d) consists of a set V of n points and a distance metric d : V ×V → R≥0 specifying all pairwise distances between these points. At a high level, the goal of clustering is to partition the points into groups such that distances within each group are minimized and distances between each group are maximized. A classic way to accomplish this task is to use an objective function Φ. For example, Φ can be set to k-means, k-median, k-center, or the distance to the ground truth clustering.4 We define the following rich class of clustering objectives. For p ∈ [1,∞)∪ {∞}, set Φ(p)(C, c) = ∑ki=1( ∑ q∈Ci d(q, ci) p)1/p. The k-means, k-median, and k-center objective functions are Φ(2), Φ(1), and Φ(∞), respectively. Next, we define agglomerative clustering algorithms with dynamic programming, which are\nprevalent in practice and enjoy strong theoretical guarantees in a variety of settings [3, 4, 5, 13]. For example, the complete-linkage algorithm yields a constant-factor approximation to the k-center objective for any metric that is induced by a norm, so long as the dimension is constant [13]. The\n4If Φ is the distance to ground truth clustering, then Φ cannot be directly measured when the clustering algorithm is used on new data. However, we assume that the learning algorithm has access to training data which consists of clustering instances labeled by the ground truth clustering. The learning algorithm uses this data to optimize the parameters defining the clustering algorithm family. With high probability, on a new input drawn from the same distribution as the training data, the clustering algorithm will return a clustering that is close to the unknown ground truth clustering. We discuss this in more detail in Section 4.3.\nsingle-linkage algorithm with dynamic programming returns the optimal solution for any centerbased objective when the optimal solution is resilient to small perturbations on the input distances [3]. The average-linkage algorithm also returns the optimal solution when the solution is resilient to perturbations, for the min-sum objective [5].\nAn agglomerative clustering algorithm with dynamic programming is defined by two functions: a merge function and a pruning function. A merge function ξ(A,B) → R≥0 defines the distance between two sets of points A,B ⊆ V . Based on ξ, the algorithm iteratively merges points in the input instance V = (V, d) to build a cluster tree T whose nodes are sets of points in V such that all leaf nodes are singletons and the root node is V itself. The children of any node T in this tree correspond to the two sets of points that were merged to form T during the sequence of merges.\nA pruning function Ψ takes as input a pruning of any subtree of T and returns a score R≥0 for that pruning. More formally, a k′-pruning (for any k′ ≤ n) for any node T in T , is a partition of T into a set of k′ clusters C = {C1, . . . , Ck′} and k′ centers c = {c1, . . . ck′} such that each cluster in C is a descendant node of T in T . Thus, Ψ (C, c) ∈ R≥0 is the score of the pruning (C, c). The algorithm returns the k-pruning of the tree T that is optimal according to Ψ.\nAlgorithm 3 details how the merge function and pruning function work together to form an agglomerative clustering algorithm with dynamic programming. In the dynamic programming step, to find the 1-pruning of any node T , we only need to find the best center c ∈ T . When k′ > 1, we recursively find the best k′-pruning of T by considering different combinations of the best i′pruning of the left child TL and the best k\n′− i′-pruning of the right child TR for i′ ∈ {1, . . . , k− 1} and choosing the best combination.\nAlgorithm 3 Agglomerative algorithm with dynamic programming\nInput: Clustering instance V = (V, d), merge function ξ, pruning function Ψ. 1: Agglomerative merge step to build a cluster tree T according to ξ:\n• Start with n singleton sets {v} for each v ∈ V . • Iteratively merge the two sets A and B which minimize ξ(A,B) until a single set remains. • Let T be the cluster tree corresponding to the sequence of merges.\n2: Dynamic programming to find the k-pruning of T minimizing Ψ : • For each node T , find the best k′-pruning of the subtree rooted at T in T , denoted by( CT,k′ , cT,k′ ) according to following dynamic programming recursion:\nΨ ( CT,k′ , cT,k′ ) = { minc∈T Ψ ({T}, c) if k′ = 1, mini′∈[k′−1] Ψ ( CTL,i′ ∪ CTR,k′−i′ , cTL,i′ ∪ cTR,k′−i′ ) otherwise.\nwhere TL and TR denote the left and right children of T , respectively. Output: The best k-pruning of the root node Troot of T .\nPictorially, Figure 4 depicts an array of available choices when designing an agglomerative clustering algorithm with dynamic programming. Each path in the chart corresponds to an alternative choice of a merge function ξ and pruning function Ψ. The algorithm designer’s goal is to determine the path that is optimal for her specific application domain. Some examples of merge functions ξ used in common agglomerative algorithms include mina∈A,b∈B d(a, b) (single linkage),\n1 |A|·|B| ∑ a∈A,b∈B d(a, b) (average linkage) and maxa∈A,b∈B d(a, b) (complete linkage). Commonly used pruning functions Ψ include the k-means, k-median, and k-center objectives. In Sections 4.1 and 4.2, we analyze several classes of algorithms where the merge function\ncomes from an infinite family of functions while the pruning function is an arbitrary, fixed function. Finally, in Section 4.3, we expand our analysis to include algorithms defined over an infinite family\nof pruning functions in conjunction with any family of merge functions. We note that our results hold even when there is a preprocessing step that precedes the agglomerative merge step (Step 1 in Algorithm 3) such as in [5] which merges some points to begin with. This is because such a pre-processing merge step is independent of ξ or Ψ, so we may learn the optimal choice of ξ and Ψ following the preprocessing stage."
    }, {
      "heading" : "4.1 General classes of linkage-based merge functions",
      "text" : "We now define two infinite families of merge functions for which we will provide tight pseudodimension bounds and computationally efficient learning algorithms, for any fixed but arbitrary pruning function. Both of the classes we propose are defined by merge functions ξ(A,B) that depend only on the maximum and minimum of all pairwise distances between A and B. While both classes are parametrized by a single value α, they differ in that one is defined by a linear combination of these two distances while the other is defined by a non-linear combination. We use A1 and A2 to denote the classes of merge functions\nA1 = { α min u∈A,v∈B d(u, v) + (1− α) max u∈A,v∈B d(u, v) ∣∣∣∣ α ∈ [0, 1] } , A2 = {(\nmin u∈A,v∈B (d(u, v))α + max u∈A,v∈B (d(u, v))α )1/α ∣∣∣∣∣ α ∈ R ∪ {∞,−∞} } .\nGiven a set of pruning functions F , let Ai×F denote the class of algorithms defined by the set of merge function and pruning function pairs (ξ,Ψ) ∈ Ai×F . As noted, in Sections 4.1 and 4.2, we analyze classes of algorithms where the set of pruning functions F is a singleton set consisting of a fixed but arbitrary function Ψ. Given a pruning function Ψ ∈ F and a parameter α, let (Ai(α),Ψ) be the algorithm which first builds a cluster tree according to merge function in Ai defined by α and then prunes the tree according to Ψ. For any pruning function Ψ, both A1 × {Ψ} and A2 × {Ψ} define a spectrum of agglomerative clustering algorithms with dynamic programming, ranging from complete-linkage with dynamic programming (i.e., (A1(0),Ψ) and (A2(∞),Ψ)) to single-linkage with dynamic programming (i.e., (A1(1),Ψ) and (A2(−∞),Ψ)). To reduce notation in Sections 4.1 and 4.2 where F = {Ψ}, we often refer to the algorithm (Ai(α),Ψ) as Ai(α) and the\nclass of algorithms Ai×{Ψ} as Ai when Ψ is clear from context. In particular, if the cost function is Φ(p), then we set Ψ to minimize the Φ(p) objective. In Section 4.3, we expand our analysis to richer classes of algorithms Ai ×F where F consists of infinitely many pruning functions.\nRecall that for a given class of merge functions and a cost function (a generic clustering objective Φ), our goal is to learn a near-optimal value of α in expectation over an unknown distribution of clustering instances. Naturally, one might wonder if there is some α which is optimal across all instances, which would preclude the need for a learning algorithm. We prove this is not the case in the following theorem, proven in Appendix C. To formally describe this result, we set up notation similar to Section 3.1. Let V denote the set of all clustering instances over at most n points. With a slight abuse of notation, we will use Φ(Ai(α),Ψ)(V) to denote the abstract cost of the clustering produced by (Ai(α),Ψ) on the instance V.\nTheorem 5. For b ∈ {1, 2} and a permissible value of α for Ab, there exists a distribution D over clustering instances V such that EV∼D [ Φ\n(p) Ab(α)(V)\n] < EV∼D [ Φ\n(p) Ab(α′)(V)\n] for all permissible values\nof α′ 6= α for Ab.\nNow for an arbitrary objective function Φ and arbitrary pruning function Ψ, we bound the pseudo-dimension of both classes\nHA1×{Ψ},Φ = { Φ(A1(α),Ψ) : V→ R≥0 ∣∣ α ∈ [0, 1] } and HA2×{Ψ},Φ = { Φ(A2(α),Ψ) : V→ R≥0 ∣∣ α ∈ R ∪ {∞,−∞} } .\nWe drop the subscript Φ from HA1×{Ψ},Φ and HA2×{Ψ},Φ when the objective function is clear from the context. Theorem 6. For all objective functions Φ(p), Pdim ( HA1,Φ(p) ) = Θ(log n) and Pdim ( HA2,Φ(p) ) =\nΘ(log n). For all other objective functions Φ and all pruning functions Ψ, Pdim ( HA1×{Ψ},Φ ) =\nO(log n) and Pdim ( HA2×{Ψ},Φ ) = O(log n).\nThis theorem follows from Lemma 5, Lemma 16, and Lemma 6.\nLemma 5. For any objective function Φ and any pruning function Ψ, Pdim ( HA1×{Ψ},Φ ) = O(log n). Proof. Suppose S = { V(1), . . . ,V(m) } is a set of clustering instances that can be shattered by HA1 using the witnesses r1, . . . , rm. We must show that m = O(log n). For each value of α ∈ [0, 1], the algorithm A1(α) induces a binary labeling on each V(i), based on whether or not ΦA1(α) ( V(i) ) ≤ ri.\nFor a fixed V = (V, d), we use the notation ΦA1,V(α) to analyze how ΦA1(α)(V) changes as a function of α. We first claim that ΦA1,V(α) is a piecewise constant function with O(n\n8) discontinuities. To see why this is, first note that for α 6= α′, the clustering returned by A1(α) and the associated cost are both identical to that of A1(α′) if both the algorithms construct the same merge tree.\nNow, as we increase α from 0 to 1 and observe the run of the algorithm for each α, at what values of α do we expect A1(α) to produce different merge trees? To answer this, suppose that at some point in the run of algorithm A1(α), there are two pairs of subsets of V , (A,B) and (X,Y ), that could potentially merge. There exist eight points p, p′ ∈ A, q, q′ ∈ B, x, x′ ∈ X, and y, y′ ∈ Y such that the decision of which pair to merge depends on whether αd(p, q) + (1 − α)d(p′, q′) or αd(x, y) + (1 − α)d(x′, y′) is larger. Clearly, there is at most one value of α for which these expressions are equal, unless both expressions are zero for all α. Assuming that ties are broken arbitrarily but consistently, this implies that there is at most one α ∈ [0, 1] such that the choice of whether to merge (A,B) before (X,Y ) is identical for all α < α′, and similarly identical for α ≥ α′.\nSince each merge decision is defined by eight points, iterating over all pairs (A,B) and (X,Y ) it follows that we can identify all O(n8) unique 8-tuples of points which correspond to a value of α at which some decision flips. This means we can divide [0, 1] into O(n8) intervals over each of which the merge tree, and therefore the output of ΦA1,V(α), is fixed. Every sample V(i) partitions [0, 1] into O(n8) intervals in this way. Merging all m partitions, we can divide [0, 1] into O(mn8) intervals over each of which ΦA1,V(i)(α), and therefore the labeling induced by the witnesses, is fixed for all i ∈ [m] (similar to Figure 2). This means that HA1 can achieve only O(mn8) binary labelings, which is at least 2m since S is shatterable, so m = O(log n).\nA similar proof technique allows us to show that the pseudo-dimension of A2 is O(log n). We defer this proof to the appendix (Lemma 16). Now, we provide a sketch of the lower bounds, with the full details in the appendix. Lemma 6. For any objective function Φ(p), Pdim ( HA1,Φ(p) ) = Ω(log n) and Pdim ( HA2,Φ(p) ) = Ω(log n).\nProof sketch. We present a general proof outline that applies to both classes. Let b ∈ {1, 2}. We construct a set S = { V(1), . . . ,V(m) } of m = log n− 3 clustering instances that can be shattered by Ab. There are 2m = n/8 possible labelings for this set, so we need to show there are n/8 choices of α such that each of these labelings is achievable by some Ab(α) for some α. The crux of the proof lies in showing that given a sequence α0 < α1 < · · · < αn′ < αn′+1 (where n′ = Ω(n)), it is possible to design an instance V = (V, d) over n points and choose a witness r such that ΦAb(α)(V) alternates n′/2 times above and below r as α traverses the sequence of intervals (αi, αi+1).\nHere is a high level description of our construction. There will be two “main” points, a and a′ in V . The rest of the points are defined in groups of 6: (xi, yi, zi, x ′ i, y ′ i, z ′ i), for 1 ≤ i ≤ (n − 2)/6. We will define the distances between all points such that initially for all Ab(α), xi merges to yi to form the set Ai, and x ′ i merges to y ′ i to form the set A ′ i. As for (zi, z ′ i), depending on whether α < αi or not, Ab(α) merges the points zi and z′i with the sets Ai and A′i respectively or vice versa. This means that there are (n−2)/6 values of α such that Ab(α) has a unique behavior in the merge step. Finally, for all α, sets Ai merge to {a}, and sets A′i merge to {a′}. Let A = {a} ∪ ⋃ iAi and\nA′ = {a′} ∪ ⋃iA′i. There will be (n − 2)/6 intervals (αi, αi+1) for which Ab(α) returns a unique partition {A,A′}. By carefully setting the distances, we cause the cost Φ({A,A′}) to oscillate above and below a specified value r along these intervals.\nThe upper bound on the pseudo-dimension implies a computationally efficient learning algorithm. First, from Theorem 1, we know that m = Õ (( H )2 (log n log(H/ ) + log(1/δ)) ) samples\nare sufficient to ( , δ)-learn the optimal algorithm in Ab for b ∈ {1, 2}. Next, as we described in the proofs of Lemmas 5 and 16, the range of feasible values of α can be partitioned into O(mn8) intervals, such that the output of Ab(α) is fixed over the entire set of samples on a given interval. Moreover, these intervals are easy to compute. Therefore, a learning algorithm can iterate over the set of intervals, and for each interval I, choose an arbitrary α ∈ I and compute the average cost of Ab(α) evaluated on the samples. The algorithm then outputs the α that minimizes the average cost.\nTheorem 7. Let Φ be a clustering objective and let Ψ be a pruning function computable in polynomial time. Given an input sample of size m = O (( H )2 ( log n log H + log\n1 δ\n)) and a value\nb ∈ {1, 2}, Algorithm 4 ( , δ)-learns the class Ab×{Ψ} with respect to the cost function Φ and it is computationally efficient.\nAlgorithm 4 An algorithm for finding an empirical cost minimizing algorithm in A1 and A2 Input: Sample S = { V(1), . . . ,V(m) } , algorithm class b ∈ {1, 2}.\n1: Let T = ∅. For each sample V(i) = ( V (i), d(i) ) ∈ S, and for each ordered set of 8 points\n{v1, . . . , v8} ⊆ V (i), solve for α (if a solution exists) in the following equations and add the solutions to T :\nIf b = 1 : αd(v1, v2) + (1− α)d(v3, v4) = αd(v5, v6) + (1− α)d(v7, v8). If b = 2 : d(v1, v2) α + d(v3, v4) α = d(v5, v6) α + d(v7, v8) α.\n2: Order the elements of set T ∪ {−∞,+∞} as α1 < . . . < α|T |. For each 0 ≤ i ≤ |T |, pick an arbitrary α in the interval (αi, αi+1) and run Ab(α) on all clustering instances in S to compute∑ V∈S ΦAb(α)(V). Let α̂ be the value which minimizes ∑ V∈S ΦAb(α)(V)."
    }, {
      "heading" : "Output: α̂",
      "text" : "Proof. If Algorithm 4 outputs the value of α that minimizes the average cost of clustering for a set of m samples, then it follows from Theorem 1, Lemma 5 and Lemma 16 that\nm = O\n(( H )2( log n log H + log 1\nδ\n))\nsamples are sufficient for Algorithm 4 to ( , δ)-learn the optimal algorithm in Ai for i ∈ {1, 2}. To prove that Algorithm 4 indeed finds the empirically best α, recall from the pseudo-dimension analyses that the cost as a function of α for any instance is a piecewise constant function with O(n8) discontinuities. In Step 1 of Algorithm 4, we solve for the values of α at which the discontinuities occur and add them to the set T . T therefore partitions α’s range into O(mn8) subintervals. Within each of these intervals, ∑ V∈S ΦAb(α)(V) is a constant function. Therefore, we pick any arbitrary α within each interval to evaluate the empirical cost over all samples, and find the empirically best α.\nAlgorithm 4 is computationally efficient because there are only O(mn8) many subintervals and the runtime of Ab(α) on a given instance is polytime."
    }, {
      "heading" : "4.2 A richer class of linkage-based merge functions",
      "text" : "In this section, we will analyze the following linkage criterion, which is more algebraically complex than the criteria analyzed in Section 4.1.\nA3 =      1|A||B| ∑ u∈A,v∈B d(u, v)α   1/α ∣∣∣∣∣∣∣ α ∈ R ∪ {∞,−∞}    .\nIntuitively, compared to A1 or A2, this class is defined by a more natural and richer notion of the similarity between two clusters because it considers the distances between all pairs of points in each set, not just the two extreme distances. Consequently, it includes all three common linkagebased algorithms described earlier; setting α = ∞, 1, or −∞ correspond to complete-, average-, and single-linkage, respectively. As before, for an arbitrary objective function Φ let HA3,Φ ={\nΦA3(α) : V→ R≥0 ∣∣ α ∈ R ∪ {∞,−∞} } .\nTheorem 8. For objective functions Φ(p), Pdim ( HA3,Φ(p) ) = Θ(n). For all other objective func-\ntions Φ and all pruning functions Ψ, Pdim ( HA3×{Ψ},Φ ) = O(n).\nThe proof follows from Lemma 7 and Lemma 8.\nLemma 7. For all objective functions Φ and all pruning functions Ψ, Pdim ( HA3×{Ψ},Φ ) = O(n).\nProof. Recall the proofs of Lemma 5 and Lemma 16. We are interested in studying how the merge trees constructed by A3(α) changes over m instances as we increase α over R. To do this, as in the proofs of Lemma 5 and Lemma 16, we fix an instance and consider two pairs of sets A,B and X,Y that could be potentially merged. Now, the decision to merge one pair before the other is determined by the sign of the expression 1|A||B| ∑ p∈A,q∈B(d(p, q)) α − 1|X||Y | ∑ x∈X,y∈Y (d(x, y)) α. First note that this expression has O(n2) terms, and by a consequence of Rolle’s Theorem from e.g. [25] (which we state in Appendix C as Theorem 19), it has O(n2) roots. Therefore, as we iterate\nover the O ( (3n)2 ) possible pairs (A,B) and (X,Y ), we can determine O ( 32n ) unique expressions\neach with O(n2) values of α at which the corresponding decision flips. Thus we can divide R into O ( n232n ) intervals over each of which the output of ΦA3,V(α) is fixed. In fact, suppose\nS = { V(1), . . . ,V(m) } is a shatterable set of size m with witnesses r1, . . . , rm. We can divide R\ninto O ( mn232n ) intervals over each of which ΦA3,V(i)(α) is fixed for all i ∈ [m] and therefore the\ncorresponding labeling of S according to whether or not ΦA3(α) ( V(i) ) ≤ ri is fixed as well for all\ni ∈ [m]. This means that HA3 can achieve only O ( mn232n ) labelings, which is at least 2m for a shatterable set S, so m = O(n). Lemma 8. For all objective functions Φ(p), Pdim ( HA3,Φ(p) ) = Ω(n).\nProof sketch. The crux of the proof is to show that there exists a clustering instance V over n points, a witness r, and a set of α’s 1 = α0 < α1 < · · · < α2N < α2N+1 = 3, where N = b(n−8)/4c, such that ΦA3,V(α) oscillates above and below r along the sequence of intervals (αi, αi+1). We finish the proof in a manner similar to Lemma 6 by constructing instances with fewer oscillations.\nTo construct V, first we define two pairs of points which merge together regardless of the value of α. Call these merged pairs A and B. Next, we define a sequence of points pi and qi for 1 ≤ i ≤ N with distances set such that merges involving points in this sequence occur one after the other. In particular, each pi merges with one of A or B while qi merges with the other. Therefore, there are potentially 2N distinct merge trees which can be created. Using induction to precisely set the distances, we show there are 2N distinct values of α, each corresponding to a unique merge tree, thus enabling A3 to achieve all possible merge tree behaviors. Finally, we carefully add more points to the instance to control the oscillation of the cost function over these intervals as desired.\nThis pseudo-dimension allows us to prove the following theorem.\nTheorem 9. Let Φ be a clustering objective and let Ψ be a pruning function. Given an input sample of size m = O (( H )2 ( n log H + log\n1 δ )) , Algorithm 5 ( , δ)-learns the class A3 × {Ψ} with\nrespect to the cost function Φ.\nProof. The sample complexity analysis follows the same logic as the proof of Theorem 7. To prove that Algorithm 5 indeed finds the empirically best α, recall from the pseudo-dimension analysis that the cost as a function of α for any instance is a piecewise constant function with O(n232n) discontinuities. In Step 1 of Algorithm 5, we solve for the values of α at which the discontinuities occur and add them to the set T . T therefore partitions α’s range into O(mn232n)\nAlgorithm 5 An algorithm for finding an empirical cost minimizing algorithm in A3 Input: Sample S = { V(1), . . . ,V(m) } .\n1: Let T = ∅. For each sample V(i) = ( V (i), d(i) ) ∈ S, and for all A,B,X, Y ⊆ V (i), solve for α (if\na solution exists) in the following equation and add the solutions to T :\n1 |A||B| ∑\np∈A,q∈B (d(p, q))α =\n1 |X||Y | ∑\nx∈X,y∈Y (d(x, y))α.\n2: Order the elements of set T ∪ {−∞,+∞} as α1 < . . . < α|T |. For each 0 ≤ i ≤ |T |, pick an arbitrary α in the interval (αi, αi+1) and run A3(α) on all clustering instances in S to compute∑ V∈S ΦA3(α)(V). Let α̂ be the value which minimizes ∑ V∈S ΦA3(α)(V)."
    }, {
      "heading" : "Output: α̂",
      "text" : "subintervals. Within each of these intervals, ∑ V∈S ΦA3(α)(V) is a constant function. Therefore, we pick any arbitrary α within each interval to evaluate the empirical cost over all samples, and find the empirically best α.\nIn Appendix C.1, we prove more results about A3 assuming a natural restriction on the instance space V. In particular, we show the pseudo-dimension can be drastically reduced if the number of unique distances in each problem instance is not too large. In Appendix C.2, we analyze classes of algorithms that interpolate between A2 and A3. A summary of results for all of our algorithm classes can be found in Table 2."
    }, {
      "heading" : "4.3 Dynamic programming pruning functions",
      "text" : "In the previous section, we analyzed several classes of linkage-based merge functions assuming a fixed pruning function in the dynamic programming step of the standard linkage-based clustering algorithm, i.e. Step 2 of Algorithm 3. In this section, we analyze an infinite class of dynamic programming pruning functions and derive comprehensive sample complexity guarantees for learning the best merge function and pruning function in conjunction.\nBy allowing an application-specific choice of a pruning function, we significantly generalize the standard linkage-based clustering algorithm framework. Recall that in the algorithm selection model, we instantiated the cost function to be a generic clustering objective Φ. In the standard clustering algorithm framework, where Φ is defined to be any general Φ(p) (which include objectives like k-means), the best choice of the pruning function for the algorithm selector is Φ(p) itself as it would return the optimal pruning of the cluster tree for that instantiation of cost. However, when the goal of the algorithm selector is, for example, to provide solutions that are close to a ground truth clustering for each problem instance, the best choice for the pruning function is not obvious. In this case, we assume that the learning algorithm’s training data consists of clustering instances that have been labeled by an expert according to the ground truth clustering. For example, this ground truth clustering might be a partition of a set of images based on their subject, or a partition of a set of proteins by function. On a fresh input data, we no longer have access to the expert or the ground truth, so we cannot hope to prune a cluster tree based on distance to the ground truth.\nInstead, the algorithm selector must empirically evaluate how well pruning according to alternative objective functions, such as k-means or k-median, approximate the ground truth clustering on the labeled training data. In this way, we instantiate cost to be the distance of a clustering from the ground truth clustering. We guarantee that the empirically best pruning function from\na class of computable objectives is near-optimal in expectation over new problem instances drawn from the same distribution as the training data. Crucially, we are able to make this guarantee even though it is not possible to compute the cost of the algorithm’s output on these fresh instances because the ground truth clustering is unknown.\nAlong these lines, we can also handle the case where the training data consists of clustering instances, each of which has been clustered according to an objective function that is NP-hard to compute. In this scenario, our learning algorithm returns a pruning objective function that is efficiently computable and which best approximates the NP-hard objective on the training data, and therefore will best approximate the NP-hard objective on future data. Hence, in this section, we analyze a richer class of algorithms defined by a class of merge functions and a class of pruning functions. The learner now has to learn the best combination of merge and pruning functions from this class.\nTo define this more general class of agglomerative clustering algorithms, let A denote a generic class of linkage-based merge functions (such as any of the classes Ai defined in Sections 4.1 and 4.2) parameterized by α. We also define a rich class of center-based clustering objectives for the dynamic programming step: F = { Ψ(p) | p > 0 } where Ψ(p) takes as input a partition C = {C1, C2, . . . , Ck′} of n′ points and a set of centers c = {c1, c2, . . . , ck′} such that ci ∈ Ci. The function Ψ(p) is defined such that\nΨ(p)(C, c) = p √∑\nCi∈C\n∑\nq∈Ci\n(d(q, ci))p. (2)\nNote that the definition of Ψ(p) is identical to Φ(p), but we use this different notation so as not to confuse the dynamic programming function with the clustering objective function. Let A(α) denote the α-linkage merge function from A and F(p) denote the pruning function Ψ(p). Earlier, for an abstract objective Φ, we bounded the pseudodimension of HA×{Ψ},Φ = { Φ(A(α),Ψ) : V→ R≥0 } , where Φ(A(α),Ψ)(V) denoted the cost of the clustering produced by building the cluster tree on V using the merge function A(α) and then pruning the tree using a fixed pruning function Ψ. Now, we are interested in doubly-parameterized algorithms of the form (A(α),F(p)) which uses the merge function A(α) to build a cluster tree and then use the pruning function F(p) to prune it. To analyze the resulting class of algorithms, which we will denote by A × F , we have to bound the pseudodimension of HA×F ,Φ = { Φ(A(α),F(p)) : V→ R≥0 ∣∣ p > 0 }\n. Recall that in order to show that pseudodimension of HA×{Ψ},Φ is upper bounded by dHA , we proved that, given a sample of m clustering instances over n nodes, we can split the real line into at\nmost O ( m2dHA ) intervals such that as α ranges over a single interval, the m cluster trees returned by the α-linkage merge function are fixed. To extend this analysis to HA×F ,Φ, we first prove a similar fact in Lemma 9. Namely, given a single cluster tree, we can split the real line into a fixed number of intervals such that as p ranges over a single interval, the pruning returned by using the function Ψ(p) is fixed. We then show in Theorem 10 how to combine this analysis of the rich class of dynamic programming algorithms with our previous analysis of the possible merge functions to obtain a comprehensive analysis of agglomerative algorithms with dynamic programming.\nWe visualize the dynamic programming step of Algorithm 3 with pruning function Ψ(p) using a table such as Table 1, which corresponds to the cluster tree in Figure 5. Each row of the table corresponds to a sub-clustering value k′ ≤ k, and each column corresponds to a node of the corresponding cluster tree. In the column corresponding to node T and the row corresponding to the value k′, we fill in the cell with the partition of T into k′ clusters that corresponds to the best k′-pruning of the subtree rooted at T , ( CT,k′ , cT,k′ ) as defined in Step 2 of Algorithm 3.\nLemma 9. Given a cluster tree T for a clustering instance V = (V, d) of n points, the positive real line can be partitioned into a set I of O ( n2(k+1)k2k ) intervals such that for any I ∈ I, the cluster tree pruning according to Ψ(p) is identical for all p ∈ I.\nProof. To prove this claim, we will examine the dynamic programming (DP) table corresponding to the given cluster tree and the pruning function Ψ(p) as p ranges over the positive real line. As the theorem implies, we will show that we can split the positive real line into a set of intervals so that on a fixed interval I, as p ranges over I, the DP table under Ψ(p) corresponding to the cluster tree is invariant. No matter which p ∈ I we choose, the DP table under Ψ(p) will be identical, and therefore the resulting clustering will be identical. After all, the output clustering is the bottomright-most cell of the DP table since that corresponds to the best k-pruning of the node containing all points (see Table 1 for an example). We will prove that the total number of intervals is bounded by O(n2(k+1)k2k).\nWe will prove this lemma using induction on the row number k′ of the DP table. Our inductive hypothesis will be the following. The positive real line can be partitioned into a set I(k′) of O ( n2 ∏k′ j=1 n 2j ) intervals such that for any I(k ′) ∈ I(k′), as p ranges over I(k′), the first k′ rows of the DP table corresponding to Ψ(p) are invariant. Notice that this means that the positive real line\ncan be partitioned into a set I of O ( n2 ∏k j=1 n 2j2 ) = O ( n2(k+1)k2k ) intervals such that for any I ∈ I, as p ranges over I, the DP table corresponding to Ψ(p) is invariant. Therefore, the resulting output clustering is invariant as well.\nBase case (k′ = 1). Let p be a positive real number. Consider the first row of the DP table corresponding to Ψ(p). Recall that each column in the DP table corresponds to a node T in the clustering tree where T ⊆ V . In the first row of the DP table and the column corresponding to node T , we fill in the cell with the single node T and the point c ∈ T which minimizes Ψ(p)({T}, {c}) =∑\nq∈T (d(q, c)) p. The only thing that might change as we vary p is the center minimizing this objective. Let v1 and v2 be two points in T . The point v1 is a better candidate for the center of T than v2 if and only if Ψ (p)({T}, {v1}) ≤ Ψ(p)({T}, {v2}) which means that Ψ(p)({T}, {v1}) −\nΨ(p)({T}, {v2}) ≤ 0, or in other words, ∑ q∈T (d(q, v1)) p −∑q∈T (d(q, v2))p ≤ 0. The equation∑\nq∈T (d(q, v1)) p −∑q∈T (d(q, v2))p has at most 2|T | zeros, so there are at most 2|T | + 1 intervals I1, . . . , It which partition the positive real line such that for any Ii, as p ranges over Ii, whether or not Ψ(p)({T}, {v1}) ≤ Ψ(p)({T}, {v2}) is fixed. For example, see Figure 6. Every pair of points in T similarly partitions the positive real line into 2|T |+ 1 intervals. If we merge all |T |2/2 partitions — one partition for each pair of points in T — then we are left with at most |T | 2\n2 · 2|T |+ 1 = |T |3 + 1 intervals I1, . . . , Iw partitioning the positive real line such that for any Ii, as p ranges over Ii, the point v ∈ T which minimizes Ψ(p)({T}, {v}) is fixed.\nSince T is arbitrary, we can thus partition the real line for each node T ′ in the cluster tree. Again, this partition defines the center of the cluster T ′ as p ranges over the positive real line. If we merge the partition for every node T ∈ T , then we are left with (∑ T∈T |T |3 ) + 1 = O(n4) intervals I1, . . . , I` such that as p ranges over any one interval Ii, the centers of all nodes in the cluster tree are fixed. In other words, for each T , the point vi ∈ T which minimizes Ψ(p)({T}, {vi}) is fixed. Of course, this means that the first row of the DP table is fixed as well. Therefore, the inductive hypothesis holds for the base case.\nInductive step. Consider the k′th row of the DP table. We know from the inductive hypothesis\nthat the positive real line can be partitioned into a set I(k′−1) of O ( n2 ∏k′−1 j=1 n 2j2 ) intervals such that for any I(k ′−1) ∈ I(k′−1), as p ranges over I(k′−1), the first k′ − 1 rows of the DP table corresponding to Ψ(p) are invariant. Fix some interval I(k\n′−1) ∈ I(k′−1). Let T be a node in the cluster tree T and let TL and TR be the left and right children of T in T respectively. Notice that the pruning which belongs in the cell in the ith row and the column corresponding to T does not depend on the other cells in the ith row, but only on the cells in rows 1 through i − 1. In particular, the pruning which belongs in this cell depends on the inequalities defining which i′ ∈ {1, . . . , k′ − 1} minimizes Ψ(p) ( CTL,i′ ∪ CTR,k′−i′ , cTL,i′ ∪ cTR,k′−i′ ) . We will now examine this objective function and show that the minimizing i′, and therefore the optimal pruning, only changes a small number of times as p ranges over I(k\n′−1). For an arbitrary i′ ∈ {1, . . . , k′ − 1}, since i′ and k′ − i′ are both strictly less than k′, the best i′-pruning of TL (CTR,i′ , cTR,i′) is exactly the entry in the i′th row of the DP table and the column corresponding to TL. Similarly, the best k\n′ − i′-pruning of TR, (CTR,k′−i′ , cTR,k′−i′) is exactly the entry in the k′ − i′th row of the DP table and the column corresponding to TR. Crucially, these entries do not change as we vary p ∈ I(k′−1), thanks to the inductive hypothesis.\nTherefore, for any i′, i′′ ∈ {1, . . . , k′ − 1}, we know that for all p ∈ I(k′−1), the k′-pruning of T corresponding to the combination of the best i′-pruning of TL and the best k\n′ − i′ pruning of TR is fixed and can be denoted as (C′, c′). Similarly, the k′-pruning of T corresponding to the combination of the best i′′-pruning of TL and the best k ′ − i′′ pruning of TR is fixed and can\nbe denoted as (C′′, c′′). Then, for any p ∈ I(k′−1), (C′, c′) is a better pruning than (C′′, c′′) if and only if Ψ(p) (C′, c′) ≤ Ψ(p) (C′′, c′′). In order to analyze this inequality, let us consider the equivalent inequality ( Ψ(p) (C′, c′) )p ≤ ( Ψ(p) (C′′, c′′) )p i.e., ( Ψ(p) (C′, c′) )p − ( Ψ(p) (C′′, c′′)\n)p ≤ 0. Now, to expand this expression let C′ = {C ′1, C ′2, . . . , C ′k′} and c′ = {c′1, c′2 . . . , c′k′} and similarly C′′ = {C ′′1 , C ′′2 , . . . , C ′k′} and c′′ = {c′′1, c′′2 . . . , c′′k′}. Then, this inequality can then be written as,\nk′∑\ni=1\n∑\nq∈C′i\n( d(q, c′i) )p − k′∑\ni=1\n∑\nq∈C′′i\n( d(q, c′′i ) )p ≤ 0.\nThe equation ∑k′\ni=1 ∑ q∈C′i (d(q, c′i)) p −∑k′i=1 ∑ q∈C′′i (d(q, c′′i )) p has has at most 2n zeros as p\nranges over I(k ′−1). Therefore, there are at most 2n+ 1 subintervals partitioning I(k ′−1) such that as p ranges over one subinterval, the smaller of Ψ(p) (C′, c′) and Ψ(p) (C′′, c′′) is fixed. In other words, as p ranges over one subinterval, either the combination of the best i′-pruning of T ’s left child and the best (k′ − i′)-pruning of T ’s right child is better than the combination of the best i′′-pruning of T ’s left child with the best (i − i′′)-pruning of T ’s right child, or vice versa. For all pairs i′, i′′ ∈ {1, . . . , k′ − 1}, we can similarly partition I into at most 2n + 1 subintervals defining the better of the two prunings. If we merge all (k′ − 1)2/2 partitions of I(k′−1), we have (k′−1)2\n2 · 2n + 1 = (k′ − 1)2n + 1 total subintervals of I(k ′−1) such that as p ranges over a single\nsubinterval, argmini′∈{1,...,k′−1}Ψ (p) ( CTL,i′ ∪ CTR,k′−i′ , cTL,i′ ∪ cTR,k′−i′ )\nis fixed. Since these equations determine the entry in the ith row of the DP table and the column corresponding to the node T , we have that this entry is also fixed as p ranges over a single subinterval in I(k\n′−1). The above partition of I(k\n′−1) corresponds to only a single cell in the k′th row of the DP table. Considering the k′th row of the DP table as a whole, we must fill in at most 2n entries, since there are at most 2n columns of the DP table. For each column, there is a corresponding partition of I(k ′−1) such that as p ranges over a single subinterval in the partition, the entry in the k′th row and that column is fixed. If we merge all such partitions, we are left with a partition of I(k ′−1) consisting of at most 2n2(i− 1)2 + 1 intervals such that as p ranges over a single interval, the entry in every column of the k′th row is fixed. As these intervals are subsets of I(k\n′−1), by assumption, the first k′ − 1 rows of the DP table are also fixed. Therefore, the first k′ rows are fixed.\nTo recap, we fixed an interval I(k ′−1) such that as p ranges over I(k ′−1), the first k′ − 1 rows of the DP table are fixed. By the inductive hypothesis, there are O ( n2 ∏k′−1 j=1 n 2j2 ) such intervals. Then, we showed that I(k ′−1) can be partitioned into 2n2(k′ − 1)2 + 1 intervals such that for any one subinterval I(k ′), as p ranges over I(k ′), the first k′ rows of the DP table are fixed. Therefore,\nthere are O ( n2 ∏k′ j=1 n 2j2 ) total intervals such that as p ranges over a single interval, the first k′ rows of the DP table are fixed. Aggregating this analysis over all k rows of the DP table, we have that there are\nO ( n2 k∏\nk=1\nn2k′ 2 ) = O ( n2(k+1)k2k )\nintervals such that the entire DP table is fixed so long as p ranges over a single interval.\nTheorem 10. Let A be a class of merge functions parameterized by a real value α and let Φ be an arbitrary clustering objective. Given m clustering instances, suppose we can partition the\ndomain of α into O ( m · 2O(dHA) ) intervals such that as α ranges over a single interval, the m\ncluster trees returned by the α-linkage merge function from A are fixed. Then Pdim(HA×F ,Φ) = O (dHA + k log n).\nProof. Let S be a set of clustering instances. Fix a single interval of α (as shown along the horizontal axis in Figure 7) where the set of cluster trees returned by the α-linkage merge function from A is fixed across all samples. We know from Lemma 9 that we can split the real line into a fixed number of intervals such that as p ranges over a single interval (as shown along the vertical axis in Figure 7), the dynamic programming (DP) table is fixed for all the samples, and therefore the resulting set of clusterings is fixed. In particular, for a fixed α interval, each of the m samples has its own O ( n2(k+1)k2k ) intervals of p, and when we merge them, we are left with O ( mn2(k+1)k2k )\nintervals such that as p ranges over a single interval, each DP table for each sample is fixed, and therefore the resulting clustering for each sample is fixed. Since there are O ( m2dHA ) such α intervals, each inducing O ( mn2(k+1)k2k ) such p intervals in total, we have O ( 2dHAm2n2(k+1)k2k )\ncells in R2 such that if (α, p) is in one fixed cell, the resulting clustering across all samples is fixed. If HA×F shatters S, then it must be that 2m = O ( 2dHAm2n2(k+1)k2k ) , which means that m = O ( log ( 2dHAn2(k+1)k2k ))\n= O (dHA + k log n).\nGiven a set of samples, a technique for finding the empirically best algorithm from A × F follows naturally from the above analysis. We first partition the range of feasible values of α into\nO ( 2O(dHA) ) intervals (as discussed in detail in Section 4.1). For each α interval, we can find the\nfixed set of cluster trees on the samples by running A(α) on the samples for a single arbitrary value of α in that interval. Now, based on these cluster trees, we can partition the values of p into O ( m · n2(k+1)k2k ) intervals as discussed in the proof for Lemma 9. For each interval of p, we only need to pick an arbitrary value of p and run F(p) to prune the cluster trees and determine the fixed empirical cost corresponding to that interval of p and α. Iterating over all partitions of the parameter space in this manner, we can find parameters that result in the best empirical cost. We summarize this result in the following theorem.\nTheorem 11. Let Φ be a clustering objective. Given an input sample of size\nm = O\n(( H )2( (dHA + log n) log H + log 1\nδ\n)) ,\nit is possible to ( , δ)-learn the class of algorithms A × F with respect to the cost function Φ. Moreover, this procedure is efficient if the following conditions hold:\n1. k is constant, which ensures that the partition of p values is polynomial in n. 2. 2dHA is polynomial in n, which ensures that the partition of α values is polynomial in n. 3. It is possible to efficiently compute the partition of α into intervals so that on a single interval\nI, for all α ∈ I, the m cluster trees returned by α-linkage performed on S are fixed."
    }, {
      "heading" : "5 Discussion and open questions",
      "text" : "In this work, we show how to learn near-optimal algorithms over several infinite, rich classes of SDP rounding algorithms and agglomerative clustering algorithms with dynamic programming. We provide computationally efficient learning algorithms for many of these problems and we push the boundaries of learning theory by developing techniques to compute the pseudo-dimension of intricate, multi-stage classes of integer-quadratic programming approximation algorithms and clustering algorithms. We derive tight pseudo-dimension bounds for the classes we study, which lead to strong sample complexity guarantees. We hope that our techniques will lead to theoretical guarantees in other areas where empirical methods for application-specific algorithm selection and portfolio selection have been developed.\nThere are many open avenues for future research in this area. In this work, we focused on algorithm families containing only computationally efficient algorithms. However, oftentimes in empirical AI research, the algorithm families in question contain procedures that are too slow to run to completion on many training instances. In this situation, we would not be able to determine the exact empirical cost of an algorithm on the training set. Could we still make strong, provable guarantees for application-specific algorithm selection in this scenario?\nAcknowledgments. This work was supported in part by grants NSF-CCF 1535967, NSF CCF1422910, NSF IIS-1618714, a Sloan Fellowship, a Microsoft Research Fellowship, a NSF Graduate Research Fellowship, a Microsoft Research Women’s Fellowship, and a National Defense Science and Engineering Graduate (NDSEG) fellowship."
    }, {
      "heading" : "A Proofs from Section 3",
      "text" : "Proof of Theorem 2. Theorem 2 follows directly from the following lemma.\nLemma 10. Suppose that m is sufficiently large to ensure that with probability at least 1− δ over a draw of m samples ( A(i), ~Z(i) ) ∼ D ×Z, for all s-linear functions φs,\n∣∣∣∣∣ 1 m m∑\ni=1\nslins\n( A(i), ~Z(i) ) − E\n(A,~Z)∼D×Z\n[ slins ( A, ~Z )]∣∣∣∣∣ < 2 .\nThen with probability at least 1 − δ, if ŝ maximizes 1m ∑m i=1 slins ( A(i), ~Z(i) ) and s∗ maximizes\nE A∼D [slins (A)], then E A∼D [slins∗ (A)]− E A∼D [slinŝ (A)] < .\nProof. Notice that since D ×Z is a product distribution, we have that\nE (A,~Z)∼D×Z\n[ slins ( A, ~Z )] = E\nA∼D [ E ~Z∼Z [ slins ( A, ~Z )]] = E A∼D [slins (A)] ,\nso we know that with probability at least 1− δ, for all s-linear functions φs, ∣∣∣∣∣ 1 m m∑\ni=1\nslins\n( A(i), ~Z(i) ) − E A∼D [slins (A)] ∣∣∣∣∣ < 2 .\nBy assumption, we know that with probability at least 1− δ, ∣∣∣∣∣ 1 m m∑\ni=1\nslinŝ\n( A(i), ~Z(i) ) − E A∼D [slinŝ (A)] ∣∣∣∣∣ < 2\nand ∣∣∣∣∣ 1 m m∑\ni=1\nslins∗ ( A(i), ~Z(i) ) − E A∼D [slins∗ (A)] ∣∣∣∣∣ < 2 ,\nwhich means that E A∼D [slins∗ (A)]− E A∼D [slinŝ (A)] < .\nProof of Lemma 3. In order to prove that the pseudo dimension ofHslin is at least c log n for some c, we must present a set S = {( G(1), ~Z(1) ) , . . . , ( G(m), ~Z(m) )} of m = c log n graphs and projection vectors that can be shattered by Hslin. In other words, there exist m witnesses r1, . . . , rm and 2m = nc s values H = {s1, . . . , snc} such that for all T ⊆ [m], there exists sT ∈ H such that if j ∈ T , then slinST ( G(j), ~Z(j) ) > rj and if j 6∈ T , then slinST ( G(j), ~Z(j) ) ≤ rj .\nTo build S, we will use the same graph G for all G(j) and we will vary ~Z(j). We set G to be the graph composed of bn/4c disjoint copies of K4. If n = 4, then a simple calculation confirms that an optimal max-cut SDP embedding of G is\n     1 0 0 0   ,   −1/3 2 √ 2/3 0 0   ,   −1/3 − √ 2/3√ 2/3 0   ,   −1/3 − √ 2/3 − √ 2/3 0      .\nTherefore, for n > 4, an optimal embedding is the set of n vectors SDP (G) such that for all i ∈ {0, . . . , bn/4c − 1},\n~e4i+1,− 1\n3 ~e4i+1 +\n2 √ 2\n3 ~e4i+2,−\n1 3 ~e4i+1 −\n√ 2\n3 ~e4i+2 +\n√ 2\n3 ~e4i+3,−\n1 3 ~e4i+1 −\n√ 2\n3 ~e4i+2 −\n√ 2\n3 ~e4i+3\nare elements SDP (G). We now define the set of m vectors ~Z(j). First, we set ~Z(1) to be the vector\n~Z(1) = ( 70, 5 · 70, 5 · 70, 70, 71, 5 · 71, 5 · 71, 71, 72, 5 · 72, 5 · 72, 72, 73, 5 · 73, 5 · 73, 73, . . . ) .\nIn other words, it is the concatenation the vector 7i(1, 5, 5, 1) for all i > 0. Next, ~Z(2) is defined as\n~Z(2) = ( 70, 5 · 70, 5 · 70, 70, 0, 0, 0, 0, 72, 5 · 72, 5 · 72, 72, 0, 0, 0, 0, . . . ) ,\nso ~Z(2) is the same as ~Z(1) for all even powers of 7, and otherwise its entries are 0. In a similar vein,\n~Z(3) = ( 70, 5 · 70, 5 · 70, 70, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 74, 5 · 74, 5 · 74, 74, . . . ) .\nTo pin down this pattern, we set ~Z(j) to be the same as ~Z(1) for all entries of the form 7i2 j−1\n(1, 5, 5, 1) for i ≥ 0, and otherwise its entries are 0.\nWe set the following positive, increasing constants which will appear throughout the remaining analysis:\na = (1, 0, 0, 0) · (1, 5, 5, 1) = 1 b = (−1/3,− √ 2/3, √ 2/3, 0) · (1, 5, 5, 1) = 5 √ 2/3− 5 √ 2+1 3\nc = (−1/3, 2 √ 2/3, 0, 0) · (1, 5, 5, 1) = 10 √\n2−1 3\nd = ∣∣∣(−1/3,− √ 2/3, √ 2/3, 0) · (1, 5, 5, 1) ∣∣∣ = 5 √ 2/3 + 5 √\n2+1 3 .\nWe also set c̃ = b+ c+ bc− d− bd− cd and we claim that the witnesses\nr1 = 1 2 − 1 3n\n( b c2 − 1 )\nrj = 1 2 − c̃ 3n72j−1−2d2 j > 1\nare sufficient to prove that this set is shatterable, and we will spend the remainder of the proof showing that this is true.\nNow, the domain of slinG,~Z(j)(s) can be split into intervals on which it has a simple, fixed form.\nThese intervals begin at 1 and have the form [ 7i2 j−1 , 7(i+1)2 j−1 )\n, for i ≥ 0. It is straightforward matter of calculations to check that for s ∈ [ 7i2 j−1 , 7(i+1)2 j−1 ) ,\nslinG,~Z(j)(s)\n=    1 2 − 13n ( 1 s2 [ c̃ ∑i−1 k=0 7 2k2j−1 ] + 7 2i2j−1 s − 1 )\nif s ∈ [ 7i2 j−1 , 7i2 j−1 b )\n1 2 − 13n ( 1 s2 [ c̃ ∑i−1 k=0 7 2k2j−1 + 72i2 j−1 b ] − 1 )\nif s ∈ [ 7i2 j−1 b, 7i2 j−1 c )\n1 2 − 13n ( 1 s2 [ c̃ ∑i−1 k=0 7 2k2j−1 + 72i2 j−1 (b+ c+ bc) ] − 72i2 j−1 (1+b+c) s ) if s ∈ [ 7i2 j−1 c, 7i2 j−1 d )\n1 2 − 13n ( 1 s2 [ c̃ ∑i k=0 7 2k2j−1 ]) if s ∈ [ 7i2 j−1 d, 7(i+1)2 j−1 )\n(We note here that the power of 7 pattern was chosen so that these intervals are well defined, since 7id < 7i+1.)\nWe call the following increasing sequence of numbers points of interest, which we use to prove that this set is shattered: { 70c, 70d, 71c, 71d, 72c, 72d, . . . , 7ic, 7id, . . . }\nWe make two claims about these points of interest: 1. slinG,~Z(1)(s) is above its witness whenever s = 7 ic and it is below its witness whenever\ns = 7id for i ≥ 0. 2. Let j > 1 and consider slinG,~Z(j)(s). There are 2 j points of interest per interval\n[ 7i2 j−1 , 7(i+1)2 j−1 ) .\nOn the first half of these points of interest, slinG,~Z(j)(s) is greater than its witness and on\nthe second half, slinG,~Z(j)(s) is less than its witness. These claims are illustrated by the dots in Figure 8. Together, these claims imply that S can be shattered because for any vector ~b ∈ {0, 1}m, there exists a point of interest s such that slins(S) induces the binary labeling ~b on S.\nThe first claim is true because\nslinG,~Z(1) ( 7ic ) = 1 2 − 1 3n\n( 1\n72ic2\n[ c̃ i−1∑\nk=0\n72k + 72ib ] − 1 )\n= 1 2 − 1 3n\n( 1\n72ic2\n[ c̃ · 7\n2i − 1 72 − 1 + 7 2ib\n] − 1 ) ,\nwhich is an increasing function of i, so it is minimized when i = 0, where\nslinG,~Z(1) ( 70c ) = 1 2 − 1 3n\n( b c2 − 1 ) = r1\nso slinG,~Z(1) ( 7ic ) is always at least its witness. Further,\nslinG,~Z(1) ( 7id ) = 1 2 − 1 3n\n( 1\n72id2\n[ c̃ i∑\nk=0\n72k\n])\n= 1 2 − 1 3n\n( 1\n72id2\n[ c̃ · 7\n2(i+1) − 1 48\n]) ),\nwhich is again an increasing function in i, with a limit of\n1 2 − 49c̃ 144nd2 < r1.\nTherefore, slinG,~Z(1) ( 7id )\nis always less than its witness, and we may conclude that the first claim is always true.\nFor the second claim, notice that\n7i2 j−1 c < 7i2 j−1 d < 7i2 j−1+1c < 7i2 j−1+1d < 7i2 j−1+2c · · · < 7i2j−1+2j−1c = 7(i+1)2j−1c,\nso there are 2j points of interest per interval [ 7i2 j−1 c, 7(i+1)2 j−1 c ) , as claimed. The first two points of interest, 7i2 j−1 c and 7i2\nj−1 d, fall in an interval where slinG,~Z(j) is decreasing in s. Therefore, it\nis minimized when s = 7i2 j−1 d, where\nslinG,~Z(j)\n( 7i2 j−1 d ) = 1\n2 − 1 3n\n( 1\n72i2j−1d2\n[ c̃ i∑\nk=0\n72k2 j−1 ])\n= 1 2 − 1 3n\n( 1\n72i2j−1d2\n[ c̃ · 7\n(i+1)2j − 1 72j − 1\n]) .\nSimple calculations show that slinG,~Z(j)\n( 7i2 j−1 d ) is an increasing function in i, so it is minimized\nwhen i = 0, where slinG,~Z(j) (d) = 1 2 − c̃3nd2 > rj , as desired.\nThe remaining points of interest fall in the interval [ 7i2 j−1 d, 7(i+1)2 j−1 )\n, so slinG,~Z(j)(s) has\nthe form 12 − 13n ( 1 s2 [ c̃ ∑i k=0 7 2k2j−1 ]) . This segment of the function has a negative derivative, so it is decreasing. If j = 2, then the points of interest we already considered, 7i2 j−1 c and 7i2 j−1 d, make up half\nof the 2j points of interest in the interval [ 7i2 j−1 c, 7(i+1)2 j−1 c ) . Therefore, we only need to show that when s equals 7i2 j−1+1c and 7i2\nj−1+1d, then slinG,~Z(j)(s) is less than its witness. As we saw,\nslinG,~Z(j) is decreasing on this segment, so it is enough to show that slinG,~Z(j)\n( 7i2 j−1+1c ) is less\nthan its witness. To this end,\nslinG,~Z(j)\n( 7i2 j−1+1c ) = 1\n2 − 1 3n\n( 1\n72i2j−1+2c2\n[ c̃ i∑\nk=0\n72k2 j−1 ])\n= 1 2 − 1 3n\n( 1\n72i2j−1+2c2\n[ c̃ · 7\n(i+1)2j − 1 72j − 1\n]) .\nThis is an increasing function of i with a limit of 12 − 13n ( 1 72c2 [ c̃ · 72 j\n72 j−1\n]) < rj when j = 2.\nTherefore, when s equals 7i2 j−1+1b and 7i2 j−1+1c, then slinG,~Z(j)(s) is less than its witness.\nFinally, if j > 2, since slinG,~Z(j)(s) is decreasing on the interval [ 7i2 j−1 c, 7(i+1)2 j−1 ) , we must\nonly check that at the ( 2j−1 − 1 )th point of interest ( 7i2 j−1+2j−2−1d )\n, slinG,~Z(j)(s) is greater than\nits witness and at the ( 2j−1 + 1 )th point of interest ( 7i2 j−1+2j−2c )\n, slinG,~Z(j)(s) is less than its\nwitness. To this end,\nslinG,~Z(j)\n( 7i2 j−1+2j−2−1d ) = 1\n2 − 1 3n\n( 1\n7i2j+2j−1−2d2\n[ c̃ i∑\nk=0\n72k2 j−1 ])\n= 1 2 − 1 3n\n( 1\n7i2j+2j−1−2d2\n[ c̃ · 7\n(i+1)2j − 1 72j − 1\n]) .\nThis function is increasing in i, so it is minimized when i = 0, where\nslinG,~Z(j)\n( 72 j−2−1d ) = 1\n2 − 1 3n\n( 1\n72j−1−2d2\n[ c̃ · 7\n2j − 1 72j − 1\n])\n= 1 2 − 1 3n\n( c̃\n72j−1−2d2\n) = rj .\nTherefore, slinG,~Z(j)\n( 7i2 j−1+2j−2−1d ) ≥ rj for all i.\nNext,\nslinG,~Z(j)\n( 7i2 j−1+2j−2c ) = 1\n2 − 1 3n\n( 1\n7i2j+2j−1c2\n[ c̃ i∑\nk=0\n72k2 j−1 ])\n= 1 2 − 1 3n\n( 1\n7i2j+2j−1c2\n[ c̃ · 7\n(i+1)2j − 1 72j − 1\n]) .\nwhich is an increasing function in i, with a limit of\n1 2 − c̃7\n2j−1\n3nc2 ( 72j − 1 )\nas i tends toward infinity. Therefore,\nslinG,~Z(j)\n( 7i2 j−1+2j−2 ) ≤ 1\n2 − c̃7\n2j−1\n3nc2 ( 72j − 1 ) < rj\nfor all i, so the second claim holds.\nB More algorithm classes for MaxQP"
    }, {
      "heading" : "B.1 ̃-discretized functions for max-cut",
      "text" : "The class of ̃-discretized rounding functions are a finite yet rich class of functions for the RPR2\nparadigm. They were introduced by O’Donnell and Wu as a tool for characterizing the SDP gap curve for the max-cut problem, which we define shortly in order to describe O’Donnell and Wu’s guarantees for RPR2 using ̃-discretized rounding functions. However, we first define the max-cut SDP value of a graph G to be\nSdp(G) = max g:V→Bn    ∑\n(vi,vj)∈E\nwij\n( 1\n2 − 1 2 g(vi) · g(vj) )  ,\nwhere n = |V | and Bn denotes {x ∈ Rn | ||x|| ≤ 1}. Now, suppose that a graph G has an SDP value Sdp(G) ≥ c. The SDP gap curve GapSDP (c) is a function that measures the smallest optimal max-cut value among all graphs such that Sdp(G) ≥ c. In other words, given that Sdp(G) ≥ c, we are guaranteed that the optimal max-cut value of G is at least GapSDP (c). Formally,\nDefinition 2. For 12 ≤ s ≤ c ≤ 1, we call the pair (c, s) an SDP gap if there exists a graph G with Sdp(G) ≥ c and Opt(G) ≤ s. We define the SDP gap curve by\nGapSDP (c) = inf{s | (c, s) is an SDP gap}.\nO’Donnell and Wu prove that if G is a graph such that Sdp(G) ≥ c, if one runs RPR2 iteratively with all ̃-discretized rounding functions, then with high probability, at least one will result in a cut with value GapSDP (c) − ̃. We now formally state the definition of an ̃-discretized rounding function as well as O’Donnell and Wu’s algorithm guarantees.\nDefinition 3. Given ̃ > 0, let Ĩ denote the partition of R \\ {0} into intervals,\nĨ = { ±(−∞,−B],±(−B,−B + ̃2],±(−B + ̃2,−B + 2̃2], . . . ,±(−2̃2,−̃2],±(−̃2, ̃2) } ,\nwhere B = B(̃) is the smallest integer multiple of ̃2 exceeding √\n2 ln(1/̃). We say that a function r : R→ [−1, 1] is ̃-discretized if the following hold:\n1. r is identically −1 on (−∞,−B], 0 at 0, and identically 1 on [B,∞). 2. r’s values on the finite intervals in Ĩ are from the set ̃Z ∩ (−1, 1).\nNote that there are 2O(1/̃ 2) ̃-discretized functions.\nTheorem 12 (Corollary 5.4 in [21]). There is an algorithm which, given any graph G with Sdp(G) ≥ c and any ̃ > 0, runs in time poly(|V |)2O(1/̃2) and with high probability outputs a proper cut in G with value at least GapSDP (c)− ̃.\nNamely, the algorithm alluded to in Theorem 12 takes as input a graph, runs RPR2 using all ̃-discretized rounding functions, and returns the cut with the maximum value. We define cut̃(G) to be the value of the resulting cut.\nIt is well-known that the pseudo-dimension of a finite function class F has pseudo-dimension log |F|. This immediately implies the following theorem. Theorem 13. Given an input sample of size m = O ( 1 2 ( 1 ̃2 log 1 + log 1 δ )) there exists an algorithm that ( , δ)-learns the class of ̃-discretized rounding functions with respect to the cost function −cut̃."
    }, {
      "heading" : "B.2 Outward rotations",
      "text" : "Next we study a class of “outward rotation” based algorithms proposed by Zwick [32]. For the maxcut problem, outward rotations are proven to work better than the random hyperplane technique of Goemans and Williamson [12] on graphs with “light” max-cuts where the max-cut does not constitute a large proportion of the edges. As stated earlier, though Feige and Langberg later showed that there exists a class of rounding functions for which RPR2 becomes equivalent to outward rotations [11], we will analyze this class as it was originally presented by Zwick [32].\nThe class of outward rotation algorithms is characterized by an angle γ ∈ [0, π/2] varying which results in a range of algorithms between the random hyperplane technique of Goemans and Williamson and the naive approach of outputting a random binary assignment [12]. Unlike RPR2, the output here is a binary assignment. An outward rotation algorithm in essence extends the optimal SDP embedding in Rn to R2n. The way this is done can be understood as follows. The original embedding is first carried over to the first n co-ordinates of a 2n-dimensional space while the remaining co-oordinates are set to zero. Suppose un+1, un+2, . . . , u2n are the orthonormal vectors along each of the last n co-ordinates. Each embedding ~ui is rotated “out” of the original space, towards un+i by an angle of γ. After performing these outward rotations, the new embedding is\nprojected onto a random hyperplane in R2n. The binary assignment is then defined deterministically based on the sign of the projections like in the GW algorithm [12]. Intuitively, the parameter γ determines how far the SDP embedding is used to determine the final projection of vi as against an arbitrary value drawn from the normal distribution, which is contributed by ui. We formally define the class below.\nAlgorithm 6 SDP rounding algorithm using γ-outward rotation\nInput: Matrix A ∈ Rn×n 1: Solve the SDP (1) for the optimal embedding U = (~u1, . . . , ~un) of A. 2: Define a new embedding ~u′i in R2n such that the first n co-ordinates correspond to ~ui cos γ and\nthe following n co-ordinates are set to 0 except the (n+ i)th co-ordinate which is set to sin γ. 3: Choose a random vector ~Z ∈ R2n according to the 2n-dimensional Gaussian distribution. 4: For each decision variable xi, assign xi = sgn ( 〈~u′i, ~Z〉 ) .\nOutput: x1, . . . , xn.\nWe will now set up notations similar to Section 3.1. Let owrγ\n( A, ~Z ) be the value of the binary\nassignment produced by projecting the SDP embedding of A onto ~Z after rotating it outwardly by γ. That is,\nowrγ\n( A, ~Z ) = ∑\ni,j\naij sgn ( 〈~u′i, ~Z〉 ) sgn ( 〈~u′j , ~Z〉 ) .\nWe will use owrγ (A) to denote the expected value of owrγ\n( A, ~Z ) when ~Z is sampled from Z,\nthe 2n-dimensional normal distribution. It can be easily seen that a fact very similar to Lemma 10 in Appendix A will apply here. Therefore, we will again use samples of the form ( A(i), ~Z(i) ) for\nthe pseudo-dimension analysis. Let Howr = { owrγ : A× R2n → [0, 1] | γ ∈ [0, π/2] } . We first prove in Section B.2.1 that the pseudo-dimension of Howr is O(log n). Next, in Section B.2.2 we present an efficient learning algorithm.\nB.2.1 The pseudo-dimension of the class of outward rotation based algorithms\nWe show an upper bound on the pseudo-dimension of the class of outward rotation based algorithms. In the following discussion, we will use the notation owrA,~Z(γ) in order to examine how the value changes as a function of γ for a fixed (A, ~Z).\nTheorem 14. Pdim(Howr) = O(log n). Proof. Suppose S = {( A(1), ~Z(1) ) , . . . , ( A(m), ~Z(m) )} is shatterable. This means that there exist\nm thresholds {r1, . . . , rm} ⊂ R such that for each T ⊆ [m], there exists a parameter γT such that owrA(i), ~Z(i) (γT ) > ri if and only if i ∈ T .\nWe claim that for each sample ( A(i), ~Z(i) ) , owrA(i), ~Z(i)(γ) is a piecewise constant function in γ with at most n values of γ at which it is discontinuous. If this were to be true, it means that there exists n values a1, a2, . . . an such that for γ within a given interval in [0, a1], (a1, a2], . . . [an, 1] owrA(i), ~Z(i)(γ) is identical and so is the label given by the witness ri. Therefore, there are at most mn values of γ which define mn+ 1 intervals such that the labels given by the witnesses for the set of m samples is identical within each interval i.e., only at most mn+ 1 distinct labelings of S are\nachievable for any choice of the witnesses. However, since S is shatterable, we need 2m < mn+ 1. Thus, s = O(log n)\nNow, we only need prove our claim about owrA,~Z(γ) given A and γ = ~Z. Observe that as\nγ increases, owrA,~Z(γ) will change only when sgn ( 〈~u′i, ~Z〉 ) changes for some vi. Now, note that 〈~u′i, ~Z〉 = 〈~ui, ~Z[1,...,n]〉 cos γ + zn+i sin γ where ~Z[1,...,n] is the projection of ~Z over the first n coordindates. Clearly, 〈~u′i, ~Z〉 is a monotone function in γ ∈ [0, π/2] and attains zero at\nγ = tan−1 ( − 〈~ui, ~Z[1,...,n]〉\nzn+i\n) .\nThis implies that for each i ∈ [n], sgn ( 〈~u′i, ~Z〉 ) changes at most once within [0, π/2]. Therefore,\nowrA,~Z(γ) is a piecewise constant function with at most n discontinuities."
    }, {
      "heading" : "B.2.2 A learning algorithm",
      "text" : "We now present Algorithm 7 that efficiently learns the best value of γ for outward rotation with respect to samples drawn from D ×Z.\nAlgorithm 7 An algorithm for finding the empirical value maximizing γ Input: Sample S = {( A(1), ~Z(1) ) , . . . , ( A(m), ~Z(m) )}\n1: Solve for { U (1), . . . , U (m) } the optimal SDP embeddings for A(1), . . . , A(m), where U (i) =(\n~u (i) 1 , . . . , ~u (i) n\n) .\n2: Let T = { γ1, . . . , γ|T | } be the set of all values γ ∈ [0, π/2] such that there exists a pair of indices\ni ∈ [n], j ∈ [m] with tan−1 ( − 〈~u (j), ~Z (j) [1,...,n] 〉\nz (j) n+i\n) = γ.\n3: Let γ̂ = argmax γ∈T∪{π/2}\n{ 1 m ∑m i=1 owrγ ( A(i), ~Z(i) )} ."
    }, {
      "heading" : "Output: γ̂",
      "text" : "Lemma 11. Algorithm 7 produces the value γ̂ which maximizes 1m ∑m i=1 owrA(i), ~Z(i)(γ) given the\nsample S = {( A(1), ~Z(1) ) , . . . , ( A(m), ~Z(m) )} . Algorithm 7 has running time polynomial in m and\nn.\nProof. Recall from the proof of Theorem 14 that T defines intervals over [0, π/2] within each of which the behavior of any γ is constant across all samples in S. Therefore, we only need to examine the performance of a single value of γ within each interval to exhaustively evaluate all possibilities, and single out the best one.\nAlso observe that since there are only O(mn) values in T (in Step 2) and since computing the binary assignment on a set of m instances for a particular value of γ takes polynomial time in m and n, Step 3 should also take only polynomial time in m and n.\nTogether with Theorem 2 and Theorem 14, Lemma 11 implies the following theorem.\nTheorem 15. Given an input sample of size m = O ( 1 2 ( log n log 1 + log 1 δ )) drawn from (D×Z)m, Algorithm 7 ( , δ)-learns the class of outward rotation algorithm with respect to the cost function −owrγ.\nB.3 A general analysis of RPR2 algorithms\nIn Sections 3.1 and B.2, we investigated two specific classes of RPR2 algorithms. We will now present an analysis that can be applied to a wide range of classes of RPR2 algorithms including that of s-linear functions and outward rotations. In particular, we show that for most classes of “sigmoid-like” rounding functions, the pseudo-dimension of RPR2 is Θ(log n).\nAs a first step towards this goal, we define what it means for a class of rounding functions to be “sigmoid-like” (Section B.3.1). Next, in order to both generalize and simplify the sample complexity analysis, we provide an alternative randomized procedure to RPR2, which we call Randomized Projection Randomized Thresholding (RPRT), which produces a binary assignment rather than a fractional assignment (Section B.3.2). We prove that RPRT is equivalent to RPR2 by showing that in expectation, the assignment produced by RPRT has the same value as the assignment produced by RPR2 on an arbitrary problem instance. We work in the RPRT framework to bound the sample complexity required to ( , δ)-learn the best rounding function in a fixed class for either RPR2 or RPRT. We prove this by showing that the pseudo-dimension of RPRT with a sigmoid-like rounding function is Θ(log n) (Section B.3.3). Finally, in Section B.3.4, we present an algorithm which ( , δ)-learns the best rounding function in a fixed class.\nB.3.1 A generic class of sigmoid-like rounding functions\nWe say that a class of functions F is “sigmoid-like” if the functions in the class are parameterized by a single constant s ∈ R+ and there exists a baseline function f1 ∈ F such that for every function fs ∈ F , fs(x) := f1(sx). Clearly, such a representation is rich enough to encompass classes of most common sigmoid-like functions including s-linear functions or any piece-wise linear function. In order for a class of sigmoid-like functions to qualify as SDP rounding functions, we additionally require that each function is non-decreasing and has a limit of −1 as x approaches −∞, and has a limit of 1 as x approaches ∞. In particular,\nDefinition 4. A class of real-valued functions F = {fs | s ∈ R+} consists of sigmoid-like rounding functions if there exists a baseline function f1(x) : R→ [−1, 1] such that\n1. f1(x) is non-decreasing, 2. lim\nx→∞ f1(x) = 1 and lim x→−∞ f1(x) = −1,\nand for any function fs ∈ F , we can write fs(x) = f1(sx).\nBefore we define RPRT, we make the following observation about sigmoid-like functions, as it will be useful for our algorithm design and sample complexity analysis. In particular, observe that for each s, Fs(x) := fs(x)+1 2 is a cumulative density function associated with some probability distribution. This is because Fs is non-decreasing and its limits are 0 and 1 as x tends to −∞ and +∞, respectively. We denote the probability density function associated with Fs by ps."
    }, {
      "heading" : "B.3.2 Randomized Projection Randomized Thresholding",
      "text" : "RPRT differs from RPR2 primarily in that it produces a binary assignment rather than a fractional assignment. In essence, RPRT simply samples a binary assignment from the distribution over binary assignments that RPR2 effectively outputs. Like RPR2, RPRT first projects the embedding of an IQP instance A onto a random vector ~Z. However, RPRT then draws a random threshold for each variable and assigns the variable to either 1 or −1 depending on whether the directed distance of the projection (multiplied by ||~Z||) is less than or greater than its threshold. The distribution from which these thresholds are picked are designed to mimic RPR2 in expectation.\nAlgorithm 8 RPRT-based SDP rounding algorithm with rounding function fs ∈ F Input: Matrix A ∈ Rn×n.\n1: Solve the SDP (1) for the optimal embedding U = (~u1, . . . , ~un) of A. 2: Choose a random vector ~Z ∈ Rn according to the n-dimensional Gaussian distribution. 3: Draw ~Q = (q1, q2, . . . , qn) ∼ (p1)n where p1 is the probability density function corresponding to f1(x).\n4: For each decision variable xi, output the assignment xi = sgn ( qi − s〈~ui, ~Z〉 ) .\nOutput: x1, . . . , xn.\nNow we show that the expected value of the binary assignment produced by RPRT is equal to\nthat of RPR2 for a given rounding function fs. In the following discussion, we define rprts\n( A, ~Z, ~Q )\nto be the deterministic value of the binary assignment produced by RPRT using the rounding function fs, given the values of both ~Z and ~Q. Similarly, we define rprts ( A, ~Z ) to be the expected value of the binary assignment produced by RPRT given the value of ~Z. In other words,\nrprts\n( A, ~Z ) = E ~Q∼(p1)n [ rprts ( A, ~Z, ~Q )] . Finally, we define rprts (A) to be the expected value\nof the binary assignment produced by RPRT and by rpr2s (A) the value of the binary assignment produced by RPR2. In other words,\nrprts(A) = E ~Q∼(p1)n, ~Z∼Z [ rprts ( A, ~Z, ~Q )] and rpr2s(A) = E~Z∼Z [ rpr2s ( A, ~Z )]\nTheorem 16. Given a MaxQP problem with input matrix A and a rounding function fs, the expected value of the binary assignment produced by RPRT equals the value of the fractional assignment produced by RPR2, i.e. rprts (A) = rpr2s (A) .\nProof. Let xi and x ′ i denote the binary and fractional assignments produced by RPR 2 and RPRT for a given ~Z and ~Q respectively. We claim that for all i, E ~Q∼(p1)n [x ′ i] = xi. If we can prove this, then it implies that rprts(G,Z) = rpr2s(G,Z). To prove this, we will make use of the fact that given ~Z, and hence given the projection 〈~ui, ~Z〉, x′i and x′j are independent random variables. Therefore, the expectation of x′ix ′ j over all random draws of\n~Q can be written as the product of their individual expected values.\nrprts(A, ~Z) = E\n~Q∼(p1)n\n ∑\ni,j\naijx ′ ix ′ j\n \n= ∑\ni,j\naijE ~Q∼(p1)n [x ′ i]E ~Q∼(p1)n [x ′ j ]\n= ∑\ni,j\naijxixj = rpr2s(A, ~Z).\nOnce we have rprts(A, ~Z) = rpr2s(A, ~Z), it is clear that rprts(A) = rpr2s(A) as ~Z is sampled\nfrom the same distribution in both the algorithms. Now we only need to show that for a given ~Z, E ~Q∼(p1)n [x ′ i] = xi i.e., for a given\n~Z, for any variable, the expected binary assignment of the randomized thresholding step is equal to the fractional assignment of RPR2 for that variable. Let us examine the expected binary assignment of RPRT for a particular variable xi. The probability that xi is assigned +1 is Prqi∼p1 [qi < s〈~ui, ~Z〉].\nSince f1 is the cumulative density function of p1, this is equal to f1(s〈~ui, ~Z〉). However, due to the way we defined fs, this value is in fact equal to fs(〈~ui, ~Z〉). We complete the proof by noting that fs(〈~ui, ~Z〉) is precisely the fractional assignment of RPR2 for the choice of the rounding function fs.\nNow, taking advantage of this equivalence between RPR2 and RPRT for a given class of sigmoidlike functions, we will show that any learning algorithm that ( , δ)-learns the best rounding functions with respect to RPRT also ( , δ)-learns the best rounding functions with respect to RPR2. Recall from Section 3.1 that in the analysis for pseudo-dimension, we incorporated ~Z in the sample set S. In the same vein, we will now also incorporate ~Q in our sample set. In other words, we will pro-\nvide to our learning algorithm the set of samples, {( A(1), ~Z(1), ~Q(1) ) , . . . , ( A(m), ~Z(m), ~Q(m) )} ∼ (D ×Z × (p1)n)m. In order to prove that this indeed works, we now state Theorem 17 and Lemma 12 which parallel Theorem 2 and Lemma 10 respectively.\nTheorem 17. Suppose F = {fs|s > 0} is a class of sigmoid-like rounding functions. Let Hrprt = {rprts : A× Rn × Rn → [0, 1]|s > 0} and let dHrprt be the pseudo-dimension of Hrprt. Suppose that Lrprt is an algorithm that takes as input m samples ( A(i), ~Z(i), ~Q(i) ) ∼ D ×Z × (p1)n, where m = O ( 1 2 (dHrprt log 1 + log 1 δ ) ) , and returns the parameter ŝ that maximizes\n1\nm\nm∑\ni=1\nrprts\n( A(i), ~Z(i), ~Q(i) ) .\nThen Lrprt( , δ)-learns the class of rounding functions F with respect to the cost function −rpr2s and is computationally efficient.\nThe proof for Theorem 17 follows from the following Lemma that is similar to Lemma 10.\nLemma 12. Suppose that m is sufficiently large to ensure that with probability at least 1− δ over a draw of m samples ( A(i), ~Z(i), ~Q(i) ) ∼ D ×Z × (p1)n, for all functions fs ∈ F ,\n∣∣∣∣∣ 1 m m∑\ni=1\nrprts\n( A(i), ~Z(i), ~Q(i) ) − E\n(A,~Z, ~Q(i))∼D×Z×(p1)n\n[ rprts ( A, ~Z, ~Q )]∣∣∣∣∣ < 2 .\nThen with probability at least 1− δ, if ŝ maximizes {\n1 m ∑m i=1 rprts ( A(i), ~Z(i), ~Q(i) )} , and s∗ max-\nimizes { E\nA∼D [rpr2s (A)]\n} , then E\nA∼D [rpr2s∗ (A)]− E A∼D [rpr2ŝ (A)] < .\nProof of Lemma 12. Since D ×Z × (p1)n is a product distribution, we have that\nE (A,~Z, ~Q)∼D×Z×(p1)n\n[ rprts ( A, ~Z, ~Q )] = E\nA∼D [rprts (A)] .\nBut from Theorem 16 we know that rprts (A) = rpr2s (A), which implies that\nE A∼D [rprts (A)] = E A∼D [rpr2s (A)] .\nHence, we can restate the assumption in the theorem statement as follows. With probability at least 1− δ, for all functions fs,\n∣∣∣∣∣ 1 m m∑\ni=1\nrprts\n( A(i), ~Z(i), ~Q(i) ) − E A∼D [rpr2s (A)] ∣∣∣∣∣ < 2 .\nSince this true for ŝ and s∗, we can say that with probability 1− δ, ∣∣∣∣∣ 1 m m∑\ni=1\nrprtŝ\n( A(i), ~Z(i), ~Q(i) ) − E A∼D [rpr2ŝ (A)] ∣∣∣∣∣ < 2\nand ∣∣∣∣∣ 1 m m∑\ni=1\nrprts∗\n( A(i), ~Z(i), ~Q(i) ) − E A∼D [rpr2s∗ (A)] ∣∣∣∣∣ < 2 .\nNow, by definition of ŝ we have that,\n1\nm\nm∑\ni=1\nrprtŝ\n( A(i), ~Z(i), ~Q(i) ) ≤ 1 m m∑\ni=1\nrprts∗\n( A(i), ~Z(i), ~Q(i) ) .\nCombining the previous three inequalities, we get that E A∼D [rpr2s∗ (A)]− E A∼D [rpr2ŝ (A)] < .\nB.3.3 The pseudo-dimension of a general class of RPRT algorithms\nThe main result of this section is a tight bound on the pseudo-dimension of any general class of RPRT algorithms.\nTheorem 18. Let Hrprt = {rprts : A× Rn × Rn → [0, 1] | s > 0}. Pdim(Hrprt) = Θ(log n).\nThis follows from Lemma 13 and Lemma 14 where we provide matching upper and lower bounds on Pdim(Hrprt).\nLemma 13. Pdim(Hrprt) = O(log n). Proof. Suppose S = {( A(1), ~Z(1), ~Q(1) ) , . . . , ( A(m), ~Z(m), ~Q(m) )} is shatterable. Then there exist\nm thresholds {r1, . . . , rm} ⊂ R such that for all T ⊆ [m], there exists a parameters sT such that val′sT ( A(i), ~Z(i) ) > ri if and only if i ∈ T .\nWe first claim that for a given sample ( A(i), ~Z(i), ~Q(i) ) the domain of s i.e., (0,∞) can be par-\ntitioned into at most n+ 1 intervals (0, a (i) 1 ], (a (i) 1 , a (i) 2 ], . . . (a (i) n ,∞) such that the binary assignment produced by RPRT on the sample is identical across all parameter settings of s within a given interval. Once this is proved, we can then consider a partition of the domain of s into at most mn+1 intervals based on the points m⋃ i=1 {a(i)1 , . . . , a (i) n }. Now, we know that for any value of s within a given interval of these mn+ 1 intervals, the labeling induced by the witnesses on S is the same. Therefore, there are at most mn+ 1 possible labelings that can be produced by any choice of the witnesses over any set of m samples. However, since we picked a shatterable instance, it must be that 2m ≤ mn+ 1 i.e., m = O(log n).\nNow to complete the proof, we need to examine the behavior of RPRT on a single sample( A(i), ~Z(i), ~Q(i) ) for different configurations of s. Observe that as we increase s from 0 to ∞\nkeeping all else constant, we expect RPRT to produce a different binary assignment only when the assigned value changes for some vertex vi. However, this happens only when qi = s〈g(vi), ~Z〉 i.e., when s = qi/〈g(vi), ~Z〉. Since there are only n vertices, and at most one value of s at which the algorithm changes behavior, we expect the algorithm to change its behavior at most n times as s increases. This proves our claim.\nWe now prove a lower bound on the pseudo-dimension of Hrprt .\nLemma 14. Pdim(Hrprt) = Ω(log n).\nProof. In order to prove that the pseudo-dimension of Hrprt is at least c log n for some constant c, we must devise a set of samples S = {( G(1), ~Z(1), ~Q(i) ) , . . . , ( G(m), ~Z(m), ~Q(i) )} of size m = c log n that can be shattered by Hrprt. This means that we should be able to find m witnesses r1, . . . , rm and 2m = nc s values H = {s1, . . . , snc} such that for all T ⊆ [m], there exists sT ∈ H such that if i ∈ T , then rprtsT ( G(i), ~Z(i), ~Q(i) ) > ri and if i 6∈ T , then rprtsT ( G(i), ~Z(i), ~Q(i) ) ≤ ri.\nIn our solution, we will use the same S as designed in the proof for Lemma 3. That is, all G(i) are identical and each consists of n/4 disjointK4 graphs over vertex sets Vk = {v4k+1, v4k+2, v4k+3, v4k+4} for k = 0, 1, . . . bn/4c − 4. We will later pick ~Z(i) and ~Q(i) such that as the value of s increases, we change the cuts on V1, V2, . . . in that order and the change in the cuts is alternatingly better and worse. By appropriately choosing different values of ~Z(i), we can carefully place the intervals in which these oscillations occur across all samples so as to be able to shatter a sample of size Ω(log n).\nIn order to define ~Z(i) and ~Q(i), we make use of the following increasing sequence defined recursively: c1 = 8 and ci = c 2 i−1. Note that we can also write ci = 8 2i−1 but we will use the notation ci for the sake of convenience. Let aik = 1 cki . We will now define ~Z(i) in terms of aik as follows. For even k, we define\n( ~Z\n(i) 4k+1, ~Z (i) 4k+2, ~Z (i) 4k+3, ~Z (i) 4k+4\n) = ( 1\n2 · aik, aik,\n3 2 · aik, aik\n)\nand for odd k we define,\n( ~Z\n(i) 4k+1, ~Z (i) 4k+2, ~Z (i) 4k+3, ~Z (i) 4k+4 ) = (aik,−aik,−aik, aik)\nThe rationale behind choosing the above value for ~Z(i) will become evident as we proceed with the proof. First, by a simple calculation we can confirm that the directed distance of the projections of the vertices on ~Z(i) lie in certain intervals as stated below.\nFor even k,\n〈g(i)(vj), ~Z(i)〉 ∈    [ 1 2aik, 2aik ] j = 4k + 1,[ 1 2aik, 2aik ] j = 4k + 2,[ 1 2aik, 2aik ] j = 4k + 3,[\n−2aik,−12aik ] j = 4k + 4.\nFor odd k,\n〈g(i)(vj), ~Z(i)〉 ∈    [ 1 2aik, 2aik ] j = 4k + 1,[ −2aik,−12aik ] j = 4k + 2,[ −2aik,−12aik ] j = 4k + 3,[\n1 2aik, 2aik\n] j = 4k + 4.\nAs for the choice of ~Q(i), we pick the same value for all i. In particular, we choose ~Q(i) as follows. For even k,\n( ~Q\n(i) 4k+1, ~Q (i) 4k+2, ~Q (i) 4k+3, ~Q (i) 4k+4\n) = (−1,+1,+1,−1) ,\nand for odd k,\n( ~Q\n(i) 4k+1, ~Q (i) 4k+2, ~Q (i) 4k+3, ~Q (i) 4k+4\n) = (−1,−1,−1,+1) ,\nAgain, our choice of ~Q(i) will make sense as we proceed with the proof. We are now ready to ana-\nlyze how rprts\n( G(i), ~Z(i), ~Q(i) ) varies as a function of s. We will show that rprts ( G(i), ~Z(i), ~Q(i) )\noscillates above and below a threshold ri, bn/4c/2 times and these oscillations are spaced in such a manner that we can pick 2m values of s that can shatter S.\nFirst, let us examine the values of s at which we expect the behavior of rprts\n( G(i), ~Z(i), ~Q(i) )\nto change. We know that this can only consist of values of s equal to 1/〈g(i)(vj), ~Z(i)〉 for some j. Based on this fact, we observe that for the vertices in Vk the values of s at which the cut changes lie in [ 1 2c k i , 2 · cki ] . Thus, note that the intervals of s in which the cut changes for V1, V2, . . . are spaced far apart in increasing order because 2cki < 1 2c k+1 i given ci > 4.\nOur analysis will henceforth focus on the values of s outside these intervals. We now claim that there exists values bmax and bmin such that the value of the cut is only one of these values in between the above intervals. In particular, the value is equal to bmax between [ 0.5cki , 2 · cki ] and[\n0.5ck+1i , 2 · ck+1i ] for odd k and is equal to bmin for even k. By simple calculation, it can be verified\nthat the cut (S, Sc) assigned to Vk varies as follows. For even k,\nS = { {v4k+1, v4k+4} s < 0.5cki , {v4k+1, v4k+2, v4k+3} s > 2 · cki ,\nand for odd k,\nS = { {v4k+1, v4k+2, v4k+3} s < 0.5cki , {v4k+1, v4k+4} s > 2 · cki .\nNow, when the cut is defined by S = {v4k+1, v4k+4} is 2/3. On the other hand when S = {v4k+1, v4k+2, v4k+3}, the value of the cut is 1/2. Thus we can make a crucial observation here: as s crosses the interval [ 1 2c k i , 2 · cki ] for even k, the net value of the cut over the whole graph decreases, and for odd k it increases. More precisely, we claim that rprts ( G(i), ~Z(i), ~Q(i) ) as a function of s outside these intervals takes one of two values bmin and bmax. To state these values explicitly, let us define a set of variables bk such that for even k, bk = 2/3 and for odd k, bk = 1/2. Observe that when s < 1/2, the value of the cut is 1bn/4c ∑bn/4c−1 k=0 bk as every odd graph contributes a value of 1/2 and every even graph contributes a value of 2/3 to the total cut. We will denote this value by bmax. Now when s ∈ (2, 12ci), the value then decreases to 1bn/4c (∑bn/4c−1 k=0 bk − 2/3 + 1/2 ) which we will call bmin. We can extend this observation as follows to any interval Ii,k = (2 · cki , 12ck+1i )\nrprts\n( G(i), ~Z(i), ~Q(i) ) = { bmin s ∈ Ii,k for even k ≤ bn/4c − 1, bmax s ∈ Ii,k for odd k ≤ bn/4c − 1,\nNow we only need to choose ri to be (bmin + bmax)/2. To complete the proof, we need to show that the oscillations with respect to these witnesses for all the samples are spaced in a manner that we can pick 2m different s covering all possible oscillations for m = Ω(log n).\nThis becomes clear from two observations. First, Ii,2k ∪ Ii,2k+1 ⊂ Ii+1,k because ci = 2c2i−1. Secondly, values of s in Ii,2k and Ii,2k+1 induce different labelings on the ith sample.\nNow consider m = ⌊\nlog(bn/4c) 3 ⌋ samples. It turns out that c2m/2 ≤ cbn/4c1 . That is, Im,0 ∪ Im,1\ncontains I1,0 ∪ . . . I1,bn/4c−1. We then claim that the intervals I1,k each induce a different labeling on S with respect to the witnesses. To see why this is true, observe that since Ii,k ⊂ Ii+1,bk/2c, the labeling induced by s ∈ I1,k is defined by which terms of the sequence k1 = k, ki+1 = bki/2c are odd. This however is in fact the binary equivalent of k. Since the binary equivalent for a given k is unique, the labeling induced by I1,k is unique. Thus, we have Ω(log n) samples that can be shattered."
    }, {
      "heading" : "B.3.4 A learning algorithm",
      "text" : "We now present a learning algorithm(Algorithm 9) which ( , δ)-learns the best rounding function with respect to D from a class of sigmoid-like rounding functions.\nAlgorithm 9 An algorithm for finding an empirical value maximizing rounding function fs Input: Sample S = {( A(1), ~Z(1), ~Q(1) ) , . . . , ( A(m), ~Z(m), ~Q(m) )}\n1: Solve for { X(1), . . . , X(m) } the optimal SDP embeddings of A(1), . . . , A(m), where X(i) =(\n~x (i) 1 , . . . , ~x (i) n\n) .\n2: Let T = { s1, . . . , s|T | } be the set of all values s > 0 such that there exists a pair of indices\ni ∈ [n], j ∈ [m] with ∣∣∣〈~Z(j), ~x(j)i 〉 ∣∣∣ = sq(j)i . 3: Let ŝ = argmax\ns∈T∪{s|T |+1}\n{ 1 m ∑m i=1 rprts ( A(i), ~Z(i), ~Q(i) )} ."
    }, {
      "heading" : "Output: ŝ",
      "text" : "In particular, we prove the following guarantee regarding Algorithm 9’s performance.\nLemma 15. Algorithm 2 produces the value ŝ which maximizes 1m ∑m i=1 rpr2s ( A(i), ~Z(i), ~Q(i) ) given the sample S = {( A(1), ~Z(1), ~Q(1) ) , . . . , ( A(m), ~Z(m), ~Q(m) )} . Algorithm 9 has running time\npolynomial in m and n.\nProof. The correctness of the algorithm is evident from the proof for Lemma 13. The algorithm identifies the n values of s at which the behavior of RPRT changes and proceeds to exhaustively evaluate all the possible binary assignments which are only polynomially many. Therefore the algorithm successfully finds the best value of s in polynomial time. Corollary 1. Given an input sample of size m = O ( 1 2 ( log n log 1 + log 1 δ )) drawn from (D×Z × (p1) n)m, Algorithm 9 ( , δ)-learns the class of rounding functions F with respect to the cost function −rpr2s and is computationally efficient."
    }, {
      "heading" : "C Proofs from Section 4",
      "text" : "Proof of Theorem 5. We give a general proof for all three values of b. We will point out a few places in the proof where the details for b = 1, 2, 3 are different, but the general structure of the argument is the same. For each value of b, we construct a single clustering instance V = (V, d) that has the desired property; the distribution D is merely the single clustering instance with probability 1.\nConsider some permissible value of α, denoted α∗. Set k = 4 and n = 210. The clustering instance consists of two well-separated ‘gadgets’ of two clusters each. The class Ab results in different 2-clusterings of the first gadget depending on whether α ≤ α∗ or not. Similarly, Ab results in different 2-clusterings of the second gadget depending on whether α ≥ α∗ or not. By ensuring that for the first gadget α ≤ α∗ results in the lowest cost 2-clustering, and for the second gadget α ≥ α∗ results in the lowest cost 2-clustering, we ensure that α = α∗ is the optimal parameter overall.\nThe first gadget is as follows. We define five points a1, b1, c1, x1 and y1. For the sake of convenience, we will group the remaining points into four sets A1, B1, X1, and Y1 each containing 25 points. We set the distances as follows: d(a1, b1) = d(x1, y1) = 1, d(a1, c1) = 1.1, and d(b1, c1) = 1.2. For a ∈ A1∪B1, d(c1, a) = 1.51 and d(a1, a) = d(b1, a) = 1.6. For x ∈ X1∪Y1, d(x1, x) = d(y1, x) = 1.6. For a ∈ A1, b ∈ B1, x ∈ X1, and y ∈ Y1, d(a, b) = d(x, y) = 1.6. We also define special points x∗1 ∈ X1 and y∗1 ∈ Y1, which have the same distances as the rest of the points in X1 and Y1 respectively, except that d(x1, x ∗ 1) = 1.51 and d(y1, y ∗ 1) = 1.51. If two points p and q belong to the same set (A1, B1, X1, or Y1), then d(p, q) = 1.5. The distances d(x1, c1) and d(y1, c1) are defined in terms of b and α\n∗, but they will always be between 1.1 and 1.2. For b = 1, we set d(x1, c1) = d(y1, c1) = 1.2 − .1 · α∗. For b = 2 and b = 3, d(x1, c1) = d(y1, c1) = ((1.1 α∗ + 1.2α ∗ )/2) 1 α∗ .\nSo far, all of the distances we have defined are in [1, 2], therefore they trivially satisfy the triangle inequality. We set all of the rest of the distances to be the maximum distances allowed under the triangle inequality. Therefore, the triangle inequality holds over the entire metric.\nNow, let us analyze the merges caused by Ab(α) for various values of α. Regardless of the values of α and b, since the distances between the first five points are the smallest, merges will occur over these initially. In particular, regardless of α and b, a1 is merged with b1, and x1 with y1. Next, by a simple calculation, if α ≤ α∗, then c1 merges with a1∪ b1. If α > α∗, then c1 merges with x1∪ y1. Denote the set containing a1 and b1 by A ′ 1, and denote the set containing x1 and y1 by X ′ 1 (one of these sets will also contain c1). Between A ′ 1 and X ′ 1, the minimum distance is ≥ 1.1 + 1.1 ≥ 2.2. All other subsequent merges (except for the very last merge) will involve all distances smaller than 2.2, so we never need to consider A′1 merging to X ′ 1.\nThe next smallest distances are all 1.5, so all points in A1 will merge together, and similarly for B1, X1, and Y1. At this point, the algorithm has created six sets: A ′ 1, X ′ 1, A1, B1, X1, and Y1. We claim that if α ≤ α∗, A′1 will merge to A1 and B1, and X ′1 will merge to X1 and Y1. This is because the maximum distance between sets in each of these merges is 1.6, whereas the minimum distance between {A′1, A1, B1} and {X ′1, X1, Y1} is ≥ 2.2. Therefore, for all three values of b, the claim holds true.\nNext we claim that the 2-clustering cost of gadget 1 will be lowest for clusters A′1 ∪ A1 ∪ B1} and X ′1 ∪ X1 ∪ Y1 and when c1 ∈ A′1, i.e., when α ≤ α∗. Clearly, since the distances within A′1 ∪ A1 ∪ B1 and X ′1 ∪ X1 ∪ Y1 are much less than the distances across these sets, the best 2- clustering is A′1 ∪ A1 ∪ B1 and X ′1 ∪ X1 ∪ Y1 (with all points at distance ≤ 1.6 to their center). We proved this will be a pruning of the tree when α ≤ α∗. Therefore, we must argue the cost of this 2-clustering is lowest when c1 ∈ A′1. The idea is that c1 can act as a very good center for A′1 ∪ A1 ∪ B1. But if c1 ∈ X ′1, then the best center for A′1 ∪ A1 ∪ B1 will be an arbitrary point in A1 ∪ B1. The cost in the first case is 1.51p · 50 + 1.1p + 1.2p. The cost in the second case is 1.5p · 24 + 1.6p · 27.\nFor X ′1∪X1∪Y1, the center does not change depending on α (x∗1 and y∗1 tie for the best center), so the only difference in the cost is whether or not to include c1. If α ≤ α∗, then the cost is 1.5p · 24 + 1.51p + 1.6p · 26, otherwise the cost is 1.5p · 24 + 1.51p + 1.6p · 26 + (1.6 + 1.2− 0.1α∗)p.\nPutting it all together, if α ≤ α∗, the cost is 1.51p · 50 + 1.1p + 1.2p + 1.5p · 24 + 1.51p + 1.6p · 26. Otherwise the cost is 1.5p · 48 + 1.51p + 1.6p · 53 + (1.6 + 1.2− 0.1α∗)p. Subtracting off like terms, we conclude that the first case is always smaller because 1.51p · 49 + 1.1p + 1.2p < 1.5p · 24 + 1.6p · 26 + (1.6 + 1.2− 0.1α∗)p for all p ≥ 1.\nNext, we will construct the second gadget arbitrarily far away from the first gadget. The second gadget is very similar to the first. There are points a2, b2, c2, x2, y2, x ∗ 2, y ∗ 2 and sets to A2, B2, X2, Y2. d(a2, b2) = d(x2, y2) = 1, d(x2, c2) = 1.1, d(y2, c2) = 1.2, and for b = 1, d(a2, c2) = d(b2, c2) = 1.2 − .1 · α∗. For b = 2 or b = 3, d(a2, c2) = d(b2, c2) = ((1.1α∗ + 1.2α∗)/2) 1 α∗ . The rest of the distances are the same as in gadget 1. Then c2 joins {a2, b2} if α ≥ α∗, not α ≤ α∗. The rest of the argument is identical. So the conclusion we reach, is that the cost for the second gadget is much lower if α ≥ α∗.\nTherefore, the final cost of the 4-clustering is minimized when α = α∗, and the proof is complete.\nNow, we will analyze the pseudo-dimension of HA2,Φ. Recall the argument in Lemma 5 for the upper bound of the pseudo-dimension of HA1,Φ. Here, we relied on the linearity of A1’s merge equation to prove that for any eight points, there is exactly one value of α such that αd(p, q) + (1 − α)d(p′, q′) = αd(x, y) + (1 − α)d(x′, y′). Now we will use a consequence of Rolle’s Theorem (Theorem 19) that tells us that there is at most one value of α such that ((d(p, q))α + d(p′, q′)α)1/α = ((d(x, y))α + d(x′, y′)α)1/α. Theorem 19 (ex. [25]). Let f be a polynomial-exponential sum of the form f(x) = ∑N\ni=1 aib x i ,\nwhere bi > 0, ai ∈ R, and at least one ai is non-zero. The number of roots of f is upper bounded by N .\nLemma 16. For any Φ, Pdim(HA2 ,Φ) = O(log n).\nProof. Suppose S = {V(1), . . . ,V(m)} is a set of clustering instances that can be shattered by HA2 using the witnesses r1, . . . , rs. We must show that m = O(log n). For each value of α ∈ [0, 1], the algorithm A2(α) induces a binary labeling on each V(i), based on whether ΦA2(α)(V(i)) ≤ ri or not.\nOur argument will now parallel that of Lemma 5. Recall that our objective is to understand the behavior of A2(α) over m instances. In particular, as α varies over R we want to count the number of times the algorithm outputs a different merge tree on one of these instances. As we did in Lemma 5, for some instance V we will consider two pairs of sets A,B and X,Y that can be potentially merged. The decision to merge one pair before the other is determined by the sign of dα(p, q) + dα(p′, q′) − dα(x, y) + dα(x′, y′). This expression, as before, is determined by a set of 8 points p, p′ ∈ A, q, q′ ∈ B, x, x′ ∈ X and y, y′ ∈ Y chosen independent of α.\nNow, from Theorem 19, we have that the sign of the above expression as a function of α flips 4 times across R. Since the expression is defined by exactly 8 points, iterating over all pairs (A,B) and (X,Y ) we can list only O(n8) such unique expressions, each of which correspond to O(1) values of α at which the corresponding decision flips. Thus, we can divide R into O(n8) intervals over each of which the output of ΦA2,V(α) is fixed. In fact, we can divide [0, 1] into O(mn\n8) intervals over each of which ΦA2,V(i)(α), and therefore the labeling induced by the witnesses, is fixed for all i ∈ [m]. This means that HA2 can achieve only O(mn8) binary labelings, which is at least 2m since S is shatterable, so m = O(log n).\nNow we proceed to proving Lemma 6 which states that the pseudo-dimension of HA1 and HA1 is Ω(log n). We first prove this lemma for the center-based objective cost denoted by Φ(p) for p ∈ [1,∞) ∪ {∞}. We later note how this can be extended cluster purity based cost.\nWe first prove the following useful statement which helps us construct general examples with desirable properties. In particular, the following lemma guarantees that given a sequence of values of α of size O(n), it is possible to construct an instance V such that the cost of the output of A1(α) on V as a function of α, that is Φ(p)A1,V(α), oscillates above and below some threshold as α moves along the sequence of intervals (αi, αi+1). Given this powerful guarantee, we can then pick appropriate sequences of α and generate a sample set of Ω(log n) instances that correspond to cost functions that oscillate in a manner that helps us pick Ω(n) values of s that shatters the samples.\nLemma 17. Given n ∈ N and given a sequence of n′ ≤ bn/7c α’s such that 0 = α0 < α1 < · · · < αn′ < αn′+1 = .7, there exists a real valued witness r > 0 and a clustering instance V = (V, d), |V | = n, such that for 0 ≤ i ≤ n′/2− 1, Φ(p)A1(α)(V) < r for α ∈ (α2i, α2i+1), and Φ (p) A1(α)(V) > r for α ∈ (α2i+1, α2i+2), for k = 2.\nProof. Here is a high level description of our construction V = (V, d). There will be two “main” points, a and a′ in V . The rest of the points are defined in groups of 6: (xi, yi, zi, x ′ i, y ′ i, z ′ i), for 1 ≤ i ≤ (n− 2)/6. We will define the distances d such that initially for all Ab(α), xi merges to yi to form the set Ai, and x ′ i merges to y ′ i to form the set A ′ i. As for (zi, z ′ i), depending on whether α < αi or not, Ab(α) merges the points zi and z′i with the sets Ai and A′i respectively or vice versa. Thus, there are (n − 2)/6 values of α such that Ab(α) has a unique behavior in the merge step. Finally, for all α, sets Ai merge to {a}, and sets A′i merge to {a′}. Let A = {a} ∪ ⋃ iAi\nand A′ = {a′} ∪⋃iA′i. Thus, there will be (n − 2)/6 intervals (αi, αi+1) for which Ab(α) returns a unique partition {A,A′}. By carefully setting the distances, we cause the cost Φ(p)({A,A′}) to oscillate above and below a specified value r along these intervals.\nFirst of all, in order for d to be a metric, we set all distances in [1, 2] so that the triangle inequality is trivially satisfied. In particular, the following are the distances of the pairs of points within each group for 1 ≤ i ≤ (n− 2)/6.\nd(xi, yi) = d(x ′ i, y ′ i) = 1,\nd(xi, zi) = 1.3, d(yi, zi) = 1.4, d(x′i, zi) = d(y ′ i, zi) = 1.4− .1 · αi, d(xi, x ′ i) = d(yi, y ′ i) = 2.\nWe set the distances to z′i as follows (see Figure 9).\nd(xi, z ′ i) = d(yi, z ′ i) = d(x ′ i, z ′ i) = d(y ′ i, z ′ i) = 1.41, d(zi, z ′ i) = 2.\nThen the first merges will be xi to yi and x ′ i to y ′ i, no matter what α is set to be (when each point is a singleton set, each pair of points with the minimum distance in the metric will merge). Next, zi will either merge to Ai or A ′ i based on the following equation:\nα · 1.3 + (1− α) · 1.4 ≶ α · (1.4− .1 · αi) + (1− α)(1.4− .1 · αi) =⇒ 1.4− .1 · α ≶ 1.4− .1 · αi =⇒ αi ≶ α\nIf α < αi, then zi merges to A ′ i, otherwise it will merge to Ai. Notice that the merge expression for Ai to A ′ i could be as small as α · 1.3 + (1 − α) · 2 = 2 − .7 · α, but we do not want this\nmerge to occur. If we ensure all subsequent merges have maximum distance less than 1.5, then Ai will not merge to A ′ i (until A and A\n′ merge in the very final step) as long as α < .7, because α · 1.5 + (1− α) · 1.5 = 1.5 < 2− .7 · .7.\nThese distances ensure z′i merges after zi regardless of the value of α, since zi is closer than z ′ i\nto xi, x ′ i, yi, and y ′ i. Furthermore, z ′ i will merge to the opposite set of zi, since we set d(zi, z ′ i) = 2. The merge expression for z′i to merge to the opposite set is α · 1.41 + (1−α) · 1.41, while the merge expression to the same set is ≥ α · 1.41 + (1− α) · 2.\nNow we set the distances to a and a′ as follows.\nd(a, xi) = d(a, yi) = d(a ′, x′i) = d(a ′, y′i) = 1.42, d(a, x′i) = d(a, y ′ i) = d(a ′, xi) = d(a ′, y′i) = 2.\nWe also set all distances between Ai and A ′ j to be 2, for all i and j, and all distances between Ai and Aj to be 1.5, for all i 6= j. We will set the distances from a and a′ to zi and z′i later, but they will all fall between 1.45 and 1.5. By construction, every set Ai will merge to the current superset containing {a}, because the merge expression is α · 1.42 + (1− α)1.5, and any other possible merge will have value ≥ α · 1.3 + (1− α) · 2, which is larger for α < .7. Similarly, all A′i sets will merge to {a′}.\nTherefore, the final two sets in the linkage tree are A and A′. Given 1 ≤ i ≤ (n− 2)/6, by construction, for α ∈ (αi, αi+1), {z1, . . . , zi, z′i+1, . . . z′(n−2)/6} ⊆ A and {z′1, . . . , z′i, zi+1, . . . z(n−2)/6} ⊆ A′.\nFinally, we set the distances between a, a′, zi, and z ′ i to ensure the cost function oscillates.\n∀i, d(a, z′i) = d(a′, zi) = 1.46 ∀1 ≤ j ≤ (n− 2)/12, d(a, z2j−1) = d(a′, z′2j) = 1.47,\nand d(a, z2j) = d(a ′, z′2j+1) = (2 · 1.46p − 1.47p)1/p.\nNow we calculate the 2-clustering cost of (A,A′) for α’s in different ranges. Regardless of α, all partitions will pay ∑ i(d(a, xi) p + d(a, yi) p + d(a′, x′i) p + d(a′, y′i) p) = (n− 2)/6 · (4 · 1.42p),but the distances for zi and z ′ i differ. For α ∈ (α0, α1), all of the z’s pay 1.46p, so the cost is (n− 2)/6 · (4 · 1.42p + 2 · 1.46p). Denote this value by rlow . When α ∈ (α1, α2), the only values that change are z1 and z′1, which adds d(a, z1) + d(a′, z′1)− d(a, z′1)− d(a′, z1) = 2 · (1.47p− 1.46p) > 0 to the cost (the inequality is always true for p ∈ [1,∞]). Denote rlow + 2 · (1.47p − 1.46p) by rhigh . When α ∈ (α2, α3), the values of z2 and z′2 change, and the cost changes by d(a, z2) + d(a\n′, z′2) − d(a, z′2) − d(a′, z2) = 2 · ((2 · 1.46p − 1.47p) − 1.46p) = −2 · (1.47p − 1.46p), decreasing it back to rlow .\nIn general, the cost for α ∈ (αi, αi+1) is rlow+ ∑\n1≤j≤i(−1)i+1 ·2(1.47p−1.46p) = rlow +(1.47p− 1.46p)+(−1)i+1 ·(1.47p−1.46p). If α ∈ (α2j , α2j+1), then the cost is rlow , and if α ∈ (α2j+1, α2j+2), the cost is rhigh . We set r = (rlow + rhigh)/2, and conclude that the cost function oscillates above and below r as specified in the lemma statement.\nThe pruning step will clearly pick (A,A′) as the optimal clustering, since the only centers with more than 3 points at distance < 1.5 are a and a′, and (A,A′) are the clusters in which the most points can have a and a′ as centers. This argument proved the case where n′ = (n − 2)/6. If n′ < (n− 2)/6, then we set d(a, zi) = d(a′, z′i) = 1.46 for all i > n′, which ensures the cost function oscillates exactly n′ times. This completes the proof.\nNow we can prove Lemma 6.\nProof of Lemma 6. First, we prove the claim for HA1,Φ(p) by constructing a set of samples S = {V(1), . . . ,V(m)} where m = log n − 3 that can be shattered by HA1,Φ(p) . That is, we should be able to choose 2m = n/8 different values of α such that there exists some witnesses r1, . . . , rm with respect to which Φ (p) A1(α)(·) induces all possible labelings on S.\nChoose a sequence of 2m distinct α’s arbitrarily in the range (0, .7). We will index the terms of this sequence using the notation αx for all x ∈ {0, 1}m, such that αx < αy iff x1x2 . . .xm < y1y2ym. Then the α’s satisfy\n0 < α[0 ... 0 0] < α[0 ... 0 1] < α[0 ... 1 0] < · · · < α[1 ... 1 1] < .7.\nGiven x, denote by n(x) the vector corresponding to x1x2 . . .xs+ 1, therefore, αn(x) is the smallest α greater than αx.\nNow, the crucial step is that we will use Lemma 17 to define our examples V(1), . . . V (m) and witnesses r1, . . . rm so that when α ∈ (αx, αn(x)) the labeling induced by the witnesses on S corresponds to the vector x. This means that for α ∈ (αx, αn(x)) the cost function Φ(p)A1(α)(V (i)) must be greater than ri if the ith term in x is 1, and less than ri otherwise. Since there are only 2 m = n8 x’s, it implies that for any sample V(i) there at most n/8 values of α at which we want its cost to flip above/below ri. We can we can accomplish this using Lemma 17 by choosing αx’s for which V(i) is supposed to switch labels. In this manner, we pick each V(i)i and ri thus creating a sample of size Ω(log n) that is shattered by HA1,Φ(p) .\nIt is straightforward to modify this proof to work for A2. by applying Lemma 17 adapted for A2 holds. In particular, given n ∈ N, p ∈ [1,∞], and given a set of n′ ≤ bn/7c α’s such that 0 = α0 < α1 < · · · < αn′ < αn′+1 = .7, then there exists a real valued witness r > 0 and a clustering instance (V, d), |V | = n, which oscillates up to n/7 times as α increases in (1, 3). The only major change is to set\nd(x′i, zi) = d(y ′ i, zi) = ((1.3 α i + 1.4 α i )/2)\n1 αi .\nThen we may use the same proof as A1 to show that Pdim(HA2 ,Φ(p)) = Ω(logn).\nNote C.1. Lemma 6 assumes that the pruning step fixes a partition, and then the optimal centers can be chosen for each cluster in the partition, but points may not switch clusters even if they are closer to the center in another cluster. This is desirable, for instance, in applications which much have a balanced partition."
    }, {
      "heading" : "If it is desired that the pruning step only outputs the optimal centers, and then the clusters are",
      "text" : "determined by the Voronoi partition of the centers, we modify the proof as follows. We introduce 2n′ more points into the clustering instance: c1, . . . , cn′, and c ′ 1, . . . , c ′ n′. Each ci will merge to cluster A, and each c′i will merge to cluster A ′. We set the distances so that ci and c ′ i will be the best centers for A and A′ when α ∈ (αi, αi+1). The distances are also set up so that the cost of the Voronoi tiling induced by c2i and c ′ 2i is rlow , and the cost for c2i+1 and c ′ 2i+1 is rhigh . This is sufficient for the argument to go through. Furthermore, the lower bound holds even if the cost function is the symmetric distance to the ground truth clustering. For this proof, let A ∪⋃i{z2i, z′2i+1} and A′ ∪ ⋃ i{z2i+1, z′2i} be the ground truth clustering. Then in each interval as α increases, the cost function switches between having (n− 2)/3 errors and having (n− 2)/3− 2 errors.\nIn this section, we give a proof of Lemma 8. We start with a helper lemma.\nLemma 18. Given n, and setting N = b(n−8)/2c, then there exists a clustering instance V = (V, d) of size |V | = n and a set of 2N + 2 α’s for which α-linkage creates a unique merge tree.\nProof. Here is the outline of our construction. At the start, two specific pairs of points will always merge first: pa merges with qa, and pb merges with qb. The sets {pa, qa} and {pb, qb} will stay separated until the last few merge operations. Throughout the analysis, at any point in the merging procedure, we denote the current superset containing {pa, qa} by A, and we similarly denote the superset of {pb, qb} by B. The next points to merge will come in pairs, (pi, qi) for 1 ≤ i ≤ N . We construct the distances so that pi and qi will always merge before pj and qj , for i < j. Furthermore, for all i, {pi} will first merge to A or B, and then {qi} will merge to the opposite set as pi. Let these two merges be called ‘round i’, for 1 ≤ i ≤ N . Finally, there will be a set CA of size N + 2 which merges together and then merges to A, and similarly a set CB which merges to B.\nThus, in our construction, the only freedom is whether pi merges to A or to B, for all i. This is 2N combinations total. The crux of the proof is to show there exists an α for each of these behaviors.\nWe attack the problem as follows. In round 1, the following equation specifies whether p1 merges to A or B:\n1 2 (d(pa, p1) α + d(qa, p1) α) ≶ 1 2 (d(pb, p1) α + d(qb, p1) α)\nIf the LHS is smaller, then p1 merges to A, otherwise B. By carefully setting the distances, we will ensure there exists a value α′ which is the only solution to the equation in the range (1, 3). Then p1 merges to A for all α ∈ (1, α′), and B for all α ∈ (α′, 3). For now, assume it is easy to force q1 to merge to the opposite set.\nIn round 2, there are two equations:\n1 3 (d(pa, p2) α + d(qa, p2) α + d(p1, p2) α) ≶ 1 3 (d(pb, p2) α + d(qb, p2) α + d(q1, p2) α), 1 3 (d(pa, p2) α + d(qa, p2) α + d(q1, p2) α) ≶ 1 3 (d(pb, p2) α + d(qb, p2) α + d(p1, p2) α).\nThe first equation specifies where p2 merges in the case when p1 ∈ A, and the second equation is the case when p1 ∈ B. So we must ensure there exists a specific α[−1] ∈ (1, α′) which solves equation 1, and α[1] ∈ (α′, 3) which solves equation 2, and these are the only solutions in the corresponding intervals.\nIn general, round i has 2i−1 equations corresponding to the 2i−1 possible states for the partially constructed tree. For each state, there is a specific α interval which will cause the algorithm to reach that state. We must ensure that the equation has exactly one solution in that interval. If we achieve this simultaneously for every equation, then the next round will have 2 · 2i−1 states. See Figure 10 for a schematic of the clustering instance.\nFor 1 ≤ i ≤ N , given x ∈ {−1, 1}i−1, let Ex denote the equation in round i which determines where pi merges, in the case where for all 1 ≤ j < i, pj merged to A if xj = −1, or B if xj = 1 (and let E′ denote the single equation for round 1). Let αx ∈ (1, 3) denote the solution to Ex = 0. Then we need to show the α’s are well-defined and follow a specific ordering, shown in Figure 11. This ordering is completely specified by two conditions: (1) α[x −1] < α[x] < α[x 1] and (2) α[x −1 y] < α[x 1 z] for all x,y, z ∈ ⋃ i<N{−1, 1}i and |y| = |z|.\nNow we show how to set up the distances to achieve all of these properties. To enhance readability, we start with an example for n = 10, and then move to the general construction.\nWe give the construction round by round. All distances are in [1, 2] which ensures the triangle inequality is always satisfied. Set d(pa, qa) = d(pb, qb) = 1, and set all pairwise distances between {pa, qa} and {pb, qb} to 2. We also set d(pi, qi) = 2 for all i 6= j.\nHere are the distances for the first round.\nd(pa, p1) = d(pa, q1) = 1.1, d(qa, p1) = d(qa, q1) = 1.2, d(pb, p1) = d(pb, q1) = d(qb, p1) = d(qb, q1) = √ (1.12 + 1.22)/2 ≈ 1.151.\nSay we break ties by lexicographic order. Then p1 will merge first, and E ′ is the following.\n1 2 (1.1α + 1.2α) ≶ 1 2 (1.151α + 1.151α).\nThe unique solution in (1, 3) is α′ = 2. p1 merges to A for α ∈ (1, 2), and B for α ∈ (2, 3). The merge equations for q1 are\n1 3 (1.1α + 1.2α + 2α) ≶ 1 2 (1.151α + 1.151α) if p1 ∈ A, or\n1 2 (1.1α + 1.2α) ≶ 1 3 (1.151α + 1.151α + 2α) otherwise.\nAs long as α ∈ (1, 3), q1 will merge to the opposite cluster as p1. This is because we set d(p1, q1) to be significantly larger than the other relevant distances.\nWe set the round two distances as follows.\nd(pa, p2) = d(pa, q2) = 1.1, d(qa, p2) = d(qa, q2) = 1.2, d(pb, p2) = d(pb, q2) = d(qb, p2) = d(qb, q2) = √\n(1.12 + 1.22)/2 ≈ 1.151, d(p1, p2) = d(p1, q2) = 1.5 + 10 −4, d(q1, p2) = d(q1, q2) = 1.5− 10−4.\nNote the distances from {pa, qa, pb, qb} are the same to p1 as to p2. Since we break ties lexicographically, this ensures p2 and q2 merge in round 2. Alternatively, we can add tiny perturbations to the distances such that they do not affect our analysis, but ensure the correct merge orders regardless of the tiebreaking rule. The values were picked so that the pa, qa, pb, qb distances will have the most influence in the merge equations. If it were just these four distances, then the value of α at equality would be 2 again. But in this round, the p1 and q1 distances show up in the equation. We set d(p1, p2), d(q1, p2) ≈ 1.5 but with small ‘offsets’ so that the values of α are on either side of α′ = 2. Equations E[−1] and E[1] are the following.\n1 3 (1.1α + 1.2α + 1.5001α = 1 3 (2 · 1.151α + 1.4999α), 1 3 (1.1α + 1.2α + 1.4999α) = 1 3 (2 · 1.151α + 1.5001α).\nThen α[−1] ≈ 1.884 and α[1] ≈ 2.124. As in the previous round, it is straightforward to check all four merge equations for q2 send q2 to the opposite cluster as p2, as long as α < 3. So far, our four intervals of α which lead to distinct behavior are (1, 1.884), (1.884, 2), (2, 2.124), and (2.124, 3).\nNow we specify the distances for the third round, the final round in our example. Again, we set the distances from {pa, qa, pb, qb} the same way as in previous rounds. The new distances are as follows.\nd(p1, p3) = d(p1, q3) = 1.5 + 10 −4, d(q1, p3) = d(q1, q3) = 1.5 + 10 −4, d(p2, p3) = d(p2, q3) = 1.5 + 10 −6, d(q2, p3) = d(q2, q3) = 1.5− 10−6.\nAgain, if it were just the first four points, the value of α would be 2. The distances for {p1, q1} differ by a small offset, and the distances for {p2, q2} differ by an even smaller offset, which causes the latter distances to have less influence in the merge equations. This forces the α’s into the correct intervals. In general, the offset value will decrease in higher and higher rounds.\nThe equations E[−1 −1], E[−1 1], E[1 −1], and E[1 1] are as follows:\n1 4 (1.1α + 1.2α + (1.5 + 10−4)α + (1.5 + 10−6)α) = 1 4 (2 · 1.151α + (1.5− 10−4)α + (1.5− 10−6)α), 1\n4 (1.1α + 1.2α + (1.5 + 10−4)α + (1.5− 10−6)α) = 1 4 (2 · 1.151α + (1.5− 10−4)α + (1.5 + 10−6)α), 1\n4 (1.1α + 1.2α + (1.5− 10−4)α + (1.5 + 10−6)α) = 1 4 (2 · 1.151α + (1.5 + 10−4)α + (1.5− 10−6)α), 1\n4 (1.1α + 1.2α + (1.5− 10−4)α + (1.5− 10−6)α) = 1 4 (2 · 1.151α + (1.5 + 10−4)α + (1.5 + 10−6)α).\nSolving for the α’s, we obtain α[−1 −1] ≈ 1.882, α[−1 1] ≈ 1.885, α[1 −1] ≈ 2.123, α[1 1] ≈ 2.125. Furthermore, solving the equations for q3, we find that it will merge to the opposite cluster for α < 3. Therefore, we have 8 different ranges corresponding to the 8 equations.\nThis example suggests a general argument by induction which follows the same intuition. In each round, the new distances have less and less influence in the merge equations, ensuring the α’s stay in the correct ranges to double the number of behaviors.\nIn our argument, we will utilize the following fact (true by elementary calculus).\nFact 1. For all 0 ≤ z ≤ .01 and α ∈ (1, 3), the following are true about g(z, α) = (1.5 − z)α − (1.5 + z)α and h(z, α) = (1.1− z)α + (1.1 + z)α − 2 · (((1.1− z)α + (1.1 + z)α)/2)α2 .\n1. For z > 0, g(z, α) < 0, 2. for a fixed z, g is nonincreasing in α, 3. for a fixed α, g is nonincreasing in z, 4. h(0, α) = 0 and h is nondecreasing in z.\nHere are the details for the general construction. All distances will be between 1 and 2 so that the triangle inequality is satisfied. Given N , for all i,\nd(pa, qa) = d(pb, qb) = 1,\nd(pa, qa) = d(pa, qb) = d(pb, qa) = d(pb, qb) = 2,\n∀i ≤ N, d(pa, pi) = d(pa, qi) = 1.1− q, d(qa, pi) = d(qa, qi) = 1.1 + q,\nd(pb, pi) = d(pb, qi) = d(qb, pi) = d(qb, qi) =\n√ 1\n2 ((1.1− q)2 + (1.1 + q)2),\nd(pi, qi) = 2,\n∀1 ≤ j < i ≤ N, d(pi, pj) = d(pi, qj) = 1.5 + oj d(qi, pj) = d(qi, qj) = 1.5− oj .\nwhere q and oj are offset values in (0, .01) which we will specify later. Then for α ∈ (1, 3), the following are true. • The first two merges are pa to qa and pb to qb, • {pi} and {qi} will always prefer merging to A or B instead of merging to another singleton {pj} or {qj}.\nAfter the first two merges occur, all pi and qi are tied to first merge to A or B. For convenience, we specify the tiebreaking order as {p1, q1, . . . , pN , qN}. Alternatively, at the end we can make tiny perturbations to the distances so that tiebreaking does not occur.\nNext, we choose the value for q, which must be small enough to ensure that qi always merges to the opposite cluster as pi. Consider\nh(α, q, o1, . . . , oN ,x) = N + 2\nN + 3\n( (1.1 + q)α + (1.1− q)α + ∑\ni<N\nxi(1.5 + oi) α + 1.5α\n)\n− 2 · (((1.1 + q)2 + (1.1− q)2)/2)α2 − ∑\ni<N\nxi(1.5 + oi) α.\nIf this equation is positive for all x ∈ {−1, 1}N−1, then qN will always merge to the opposite cluster as pN (and qi will always merge to the opposite cluster as pi, which we can similarly show by setting oj = 0 in h for all j > i).\nNote\nh(α, 0, 0, . . . , 0,x) = N + 2\nN + 3 (2 · 1.1α + (N + 1) · 1.5α)− 2 · 1.1α −N · 1.5α > 0\nfor all x and all α ∈ (1, 3). Fact 1 implies there exists a 0 < q∗ < .01 such that h(α, q, 0, . . . , 0,x) stays positive. Similarly, there exists a cutoff value δ > 0 such that for all 0 < o1, . . . , oN < δ, α ∈ (1, 3), and x ∈ {−1, 1}N−1, h(α, q∗, o1, . . . , ok,x) > 0. Therefore, as long as we set all the offsets oi less than δ, the merges will be as follows:\n1. pa merges to qa and pb merges to qb. 2. For 1 . . . , N , pi merges to A or B, and qi merges to the opposite cluster. Then qN will always\nmerge to the opposite cluster as pN . Now we show that there are 2N intervals for α ∈ (1, 3) which give unique behavior. Recall for\nx ∈ ⋃i<N{−1, 1}i, Ex is defined as\n(1.1− q∗)α + (1.1 + q∗)α − 2 · (1 2\n((1.1− q∗)2 + (1.1 + q∗)2))α2 + ∑\ni<N\nxi((1.5− oi)α − (1.5 + oi)α).\nFor brevity, we denote\nd = ( 1 2 ((1.1− q∗)2 + (1.1 + q∗)2)) 12 .\nWe show the αs are correctly ordered by proving the following three statements with induction. The first statement is sufficient to order the αs, and the second two will help to prove the first.\n1. There exist 0 < o1, . . . , oN < δ such that if we solve Ex = 0 for αx for all x ∈ ⋃ i<N{−1, 1}i,\nthen the α’s satisfy α[x −1] < α[x] < α[x 1] and for all i < N , α[x 1] < α[y −1] for x,y ∈ {−1, 1}i and x1 . . .xi < y1 . . .yi.\n2. For all k′ ≤ N and α ∈ (1, 3),\n(1.5 + ok′) α − (1.5− ok′)α +\n∑\nk′<i<N\n((1.5− oi)α − (1.5 + oi)α) > 0.\n3.\n(1.1− q∗)3 + (1.1 + q∗)3 − 2 · d3 + ∑\ni<N\n((1.5− oi)3 − (1.5 + oi)3) > 0, and\n(1.1− q∗) + (1.1 + q∗)− 2 · d+ ∑\ni<N\n((1.5 + oi)− (1.5− oi)) < 0.\nWe proved the base case in our earlier example for n = 10. Assume for k ≤ N , there exist 0 < o1, . . . , ok < δ which satisfy the three properties. We first prove the inductive step for the second and third statements.\nBy inductive hypothesis, we know for all k′ ≤ k and α ∈ (1, 3),\n(1.5 + ok′) α − (1.5− ok′)α +\n∑\nk′<i≤k ((1.5− oi)α − (1.5 + oi)α) > 0,\nSince there are finite integral values of k′ ≤ k, and the expression is > 0 for all values of k′, then there exists an > 0 such that the expression is ≥ for all values of k′. Then we define za such that (1.5 + za) α− (1.5− za)α < 2 for α ∈ (1, 3). Then for all 0 < z < za, k′ ≤ k+ 1, and α ∈ (1, 3),\n(1.5 + ok′) α − (1.5− ok′)α +\n∑\nk′<i≤k+1 ((1.5− oi)α − (1.5 + oi)α) > 0.\nSo as long as we set 0 < ok+1 < za, the inductive step of the second property will be fulfilled. Now we move to the third property. We have the following from the inductive hypothesis:\n(1.1− q∗)3 + (1.1 + q∗)3 − 2 · d3 + ∑\ni≤k′ ((1.5− oi)3 − (1.5 + oi)3) > 0,\n(1.1− q∗) + (1.1 + q∗)− 2 · d+ ∑\ni≤k′ ((1.5 + oi)− (1.5− oi)) < 0.\nWe may similarly find zb such that for all 0 < ok+1 < zb,\n(1.1− q∗)3 + (1.1 + q∗)3 − 2 · d3 + ∑\ni≤k+1 ((1.5− oi)3 − (1.5 + oi)3) > 0,\n(1.1− q∗) + (1.1 + q∗)− 2 · d+ ∑\ni≤k+1 ((1.5 + oi)− (1.5− oi)) < 0.\nNow we move to proving the inductive step of the first property. Given x ∈ {−1, 1}k, let p(x), n(x) ∈ {−1, 1}k denote the vectors which sit on either side of αx in the ordering, i.e., αx is the only αy in the range (αp(x), αn(x)) such that |y| = k. If x = [1 . . . 1], then set αn(x) = 3, and if x = [0 . . . 0], set αp(x) = 1. Define\nf(α,x, z) = Ex + (1.5− z)α − (1.5 + z)α.\nBy inductive hypothesis, we have that f(αx,x, 0) = 0. We must show there exists zx such that for all 0 ≤ z ≤ zx, f(αx,x, z) < 0 and f(αn(x),x, z) > 0. This will imply that if we choose 0 < ok+1 < zx, then α[x 1] ∈ (αx, αn(x)).\nCase 1: x 6= [1 . . . 1]. Since f(αx,x, 0) = 0, and by Fact 1, then for all 0 < z < .01, f(αx,x, z) < 0. Now denote i∗ as the greatest index such that xi∗ = −1. Then n(x) = [x1 . . .xi∗−1 1 −1 · · ·−1]. By statement 1 of the inductive hypothesis (αn(x) is a root of En(x) = 0),\n(1.1− q∗)αn(x) + (1.1 + q∗)αn(x) − 2 · dαn(x) + ∑\ni≤k (n(x)i(1.5− oi)αn(x) − n(x)i(1.5 + oi)αn(x)) = 0\nFrom statement 2 of the inductive hypothesis, we know that\n(1.5− oi∗)αn(x) − (1.5 + oi∗)αn(x) + ∑\ni∗<i≤k ((1.5 + oi)\nαn(x) − (1.5− oi)αn(x)) < 0.\nIt follows that\n(1.1− q∗)αn(x) + (1.1 + q∗)αn(x) − 2 · dαn(x) + ∑\ni<i∗ (n(x)i(1.5− oi)αn(x) − n(x)i(1.5 + oi)αn(x)) > 0,\nand furthermore,\n(1.1− q∗)αn(x) + (1.1 + q∗)αn(x) − 2 · dαn(x) + ∑\ni<i∗ (xi(1.5− oi)αn(x) − xi(1.5 + oi)αn(x)) > 0.\nTherefore, f(αn(x), 0) > 0, so denote f(αn(x), 0) = > 0. Then because of Fact 1, there exists zx such that ∀0 < z < zx, f(αn(x), z) > 0.\nCase 2: x = [1 . . . 1]. Since f(αx, 0) = 0, and by Fact 1, then for all 0 < z < .01, f(αx, z) < 0. By property 3 of the inductive hypothesis, we have\n(1.1− q∗)3 + (1.1 + q∗)3 − 2 · d3 + ∑\ni≤k ((1.5− oi)3 − (1.5 + oi)3) > 0,\nso say this expression is equal to some > 0. Then from Fact 1, there exists zx such that for all 0 < z < zx, 0 < (1.5+z)\n3−(1.5−z)3 < 2 . Combining these, we have f(3, z) > 0 for all 0 < z < zx. To recap, in both cases we showed there exists zx such that for all 0 < z < min(.01, zx), f(αx, z) < 0 and f(αn(x), z) > 0. We may perform a similar analysis on a related function f ′, defined as f ′(α,x, z) = Ex + (1.5 + z) α − (1.5 − z)α to show there exists z′x such that for all 0 < z < z′x, f ′(αp(x), z) < 0 and f\n′(αx, z) > 0. We perform this analysis over all x ∈ {−1, 1}k. Finally, we set ok+1 = minx(zx, z ′ x, za, zb, .01). Given x ∈ {−1, 1}k, since f(αx, ok+1) < 0 and f(αn(x), ok+1) > 0, there must exist a root α[x 1] ∈ (αx, αn(x)) (and by Fact 1, the function is monotone in α in the short interval (αx, αn(x)), so there is exactly one root). Similarly, there must exist a root α[x −1] ∈ (αp(x), αx). Then we have shown α[x −1] and α[x 1] are roots of E[x −1] and E[x 1], respectively. By construction, α[x −1] < αx < α[x 1], so condition 1 is satisfied. Now we need to show condition 2 is satisfied. Given x,y ∈ {−1, 1}k, let k′ be the largest number for which xi = yi, ∀i ≤ k′. Let z = x[1...k′] = y[1...k′]. Then by the inductive hypothesis,\nαx < αn(x) ≤ αz ≤ αp(y) < αy.\nIt follows that α[x −1] < α[x 1] < αz < α[y −1] < α[y 1],\nproving condition 2. This completes the induction.\nNow we are ready to prove Lemma 8.\nProof of Lemma 8. Given n, and setting N = b(n − 8)/4c, we will show there exists a clustering instance (V, d) of size |V | = n, a witness r, and a set of 2N + 2 α’s 1 = α0 < α1 < · · · < α2N < α2N+1 = 3, such that Φ (p) A3(α)(V) oscillates above and below r between each interval (αi, αi+1).\nWe start by using the construction from Lemma 8, which gives a clustering instance with 2N+8 points and 2N + 2 values of α for which α-linkage creates a unique merge tree. The next part is to add 2N more points and define a witness r so that the cost function alternates above and below r along each neighboring α interval, for a total of 2N oscillations. Finally, we will finish off the proof in a manner similar to Lemma 6.\nStarting with the clustering instance (V, d) from Lemma 8, we add two sets of points, CA and CB, which do not interfere with the previous merges, and ensure the cost functions alternates. Let CA = {ca, c′a, a1, a2, . . . , aN} and CB = {cb, c′b, b1, b2, . . . , bN}. All distances between two points in CA are 1, and similarly for CB. All distances between a point in CA and a point in CB are 2. The distances between CA ∪ CB and A ∪B are as follows (we defined the sets A and B in Lemma 8).\nd(pa, ca) = d(pa, c ′ a) = d(qa, ca) = d(qa, c ′ a) = 1.51, d(pb, cb) = d(pb, c ′ b) = d(qb, cb) = d(qb, c ′ b) = 1.51, d(pa, cb) = d(pa, c ′ b) = d(qa, cb) = d(qa, c ′ b) = 2, d(pb, ca) = d(pb, c ′ a) = d(qb, ca) = d(qb, c ′ a) = 2, d(pa, c) = d(qa, c) = d(pb, c) = d(qb, c) = 2 ∀c ∈ CA ∪ CB \\ {ca, c′a, cb, c′b}, d(c, pi) = d(c, qi) = 1.51 ∀1 ≤ i ≤ N − 1 and c ∈ CA ∪ CB.\nWe will specify the distances between {ca, c′a, cb, c′b} and {pN , qN} soon, but they will be in [1.6, 2]. So at the start of the merge procedure, all points in CA merge together, and all points in CB merge together. Then all merges from Lemma 8 take place, because all relevant distances are smaller than 1.51. We end up with four sets: A, B, CA, and CB. The pairs (A,B) and (CA, CB) are dominated by distances of length 2, so the merges (CA, A) and (CB, B) will occur, which dominate (CA, B) and (CB, A) because of the distances between {pa, qa, pb, qb} and {ca, c′a, cb, c′b}. The final merge to occur will be (CA ∪A,CB ∪B), however, the 2-median pruning step will clearly pick the 2-clustering CA ∪A, CB ∪B, since no other clustering in the tree has almost all distances ≤ 1.51. Then by construction, ca or c ′ a will be the best center for CA ∪ A, which beat pa and qa because 1.51 · (2N) < 1.1 ·N + 2 ·N = 1.55 · (2N). Similarly, cb or c′b will be the best center for CB ∪ B. Note that centers {ca, c′a} and {cb, c′b} currently give equivalent 2-median costs. Denote this cost by r′ (i.e., the cost before we set the distances to pN and qN ).\nNow we set the final distances as follows.\nd(ca, pN ) = d(cb, qN ) = 1.6, d(c′a, pN ) = d(c ′ b, qN ) = 1.7, d(c′a, qN ) = d(c ′ b, pN ) = 1.8,\nd(ca, qN ) = d(cb, pN ) = 1.9.\nIf pN ∈ A and qN ∈ B, then ca and cb will be the best centers, achieving cost r′ + 3.2 for (CA ∪ A,CB ∪ B). If pN ∈ B and qN ∈ A, then c′a and c′b will be the best centers, achieving cost r′ + 3.6 for (CA ∪A,CB ∪B).\nThe distances are also constructed so that in the variant where the pruning outputs the optimal centers, and then all points are allowed to move to their closest center, the cost still oscillates. First note that no points other than pN and qN are affected, since d(ca, pi) = d(ca, qi) for i < N , and similarly for cb. Then pN will move to the cluster with ca or c ′ a, and qN will move to the cluster with cb or c ′ b. If pN was originally in A, then the cost is r\n′ + 3.2, otherwise the cost is r′ + 3.4. In either scenario, we set r = r′ + 3.3. Then we have ensured for all x ∈ {−1, 1}N−1, the cost for α ∈ (αp(x), αx) is < r, and the cost for α ∈ (αx, αn(x)) is > r. We have finished our construction of a clustering instance whose cost function alternates 2N times as α increases. To finish the proof, we will show there exists a set S = {V1, . . . , Vs} of size s = N = b(n − 8)/4c ∈ Ω(n) that is shattered by A. Such a set has 2N orderings total. For V1, we use the construction which alternates 2N times. For V2, we use the same construction, but we eliminate (pN , qN ) so that there are only N − 1 rounds (the extra two points can be added to CA and CB to preserve |V2| = n). Then V2’s cost will alternate 12 · 2N times, between the intervals (αp(x), αx) and (αx, αn(x)), for x ∈ {−1, 1}N−2. So V2 oscillates every other time V1 oscillates, as α increases. In general, Vi will be the construction with only N− i+1 rounds, oscillating 2 N\n2i−1 times, and each oscillation occurs every other time Vi−1 oscillates. This ensures for every x ∈ {−1, 1}N−1, (αp(x), αx) and (αx, αn(x)) will have unique labelings, for a total of 2\nN labelings. This completes the proof.\nNote C.2. As in Lemma 6, this lower bound holds even if the cost function is the symmetric distance to the ground truth clustering. Merely let pN and qN belong to different ground truth clusters, but for all i < N , pi and qi belong to the same ground truth cluster. Since in each adjacent α interval, pN and qN switch clusters, this shows the symmetric distance to the ground truth clustering oscillates between every interval.\nC.1 Analyzing A3 over restricted classes of clustering instances Below we consider restricted classes of clustering instances and improve on the pseudo-dimension bounds of HA3 as compared to Lemma 7. In particular, we consider the class Vβ that consists of clustering instances in which the distances take one of at most β (β ∈ N) real values. A natural example would be one in which all distances are integers and are less than some value H. Here, β = H. For this case, we show a tight bound.\nTheorem 20. For any objective function Φ, let\nHA3,Φ = { ΦA3(α) : Vβ → R≥0 | α ∈ R ∪ {∞,−∞} } .\nThen Pdim(HA3 ,Φ) = O (min(β log n, n)).\nProof. The proof for the upper bound follows a similar line of reasoning as that of Lemma 7. For a particular instance V, let {d1, d2, . . . , dβ} denote the set of β values that its distances can take. The linkage criterion for merging A,B can be expressed as 1|A||B| ∑β i=1 aid α i where, each ai can take one of at most O(n2) values corresponding to the number of pairs of points at a distance di. Therefore, iterating over all pairs of subsets (A,B) like we did in the proof of Lemma 7, we can only list at most O ( (n2)β+1 ) potential linkage criteria. Therefore, the set of all pairs of subsets (A,B) and (X,Y ) will induce at most O ( (n2)2(β+1) ) unique comparisons between two linkage criteria. By the same argument as in the proof of Lemma 7, since each such comparison has O(n2) roots, if S ∈ Vm is a shatterable set, then 2m ≤ n2((n2)2(β+1)), which means that m = O(β log n).\nTheorem 21. For any objective function Φ(p), Pdim(HA3,Φ(p)) = Ω (min(β, n)).\nProof. For the lower bound, we use a similar line of reasoning as in Lemma 8. In each round of the construction in Lemma 8, only a constant number of distinct edge lengths are added. I.e. the offsets oi define new distances 1.5+oi and 1.5−oi per round, but the set of the rest of the distances has constant size. Therefore, we can easily modify the proof to construct a clustering instance with Ω(m) rounds using m distinct distances. This instance will have 2Ω(β) distinct behaviors depending on α, however this reasoning is only consistent for β = o(n). For β = ω(n), we may inherit the lower bound from Lemma 8, so the final pseudo-dimension lower bound is Ω (min(β, n))."
    }, {
      "heading" : "C.2 An Interpolation",
      "text" : "So far, the linkage criteria was based on the distances between either two pairs of points, or every single pair between two sets considered for merging. Now we provide an interpolation between these two extremes. In particular, we define a linkage criterion which uses σ different distances between the sets for comparison. In particular, for any two sets A and B, we define an abstract rule to pick σ pairs of points (pi, qi) from A × B. For example, a natural choice would be to pick the (σ−1)-th quantiles of the set of distances between points in A and B along with the maximum and minimum distances. On picking these points, we define the criterion as a function of these σ distances as follows.\nA1,σ = { σ∑\ni=1\nαid(pi, qi) | ~α = (α1, . . . , ασ) ∈ Rσ } , (3)\nA2,σ =    ( σ∑\ni=1\nd(p, q))α )1/α | α ∈ R ∪ {∞,−∞}    . (4)\nObserve that A1,σ has multiple parameters unlike any of the classes of algorithms we have discussed. Therefore, the analysis for A1,σ is considerably different from the rest as shown below. We use notations similar to the previous sections.\nTheorem 22. For any Φ, let\nHA1,σ ,Φ = { ΦA1,σ(~α) : V→ R≥0 | ~α ∈ Rσ } .\nThe pseudo-dimension of Pdim(HA1,σ ,Φ) = O(σ2 log n).\nProof. The proof parallels that of Lemma 5. Consider two pairs of sets A,B and X,Y that can be potentially merged. Regardless of the parameters chosen, we know that the linkage criterion first chooses σ pairs of points (pi, qi) ∈ A × B and (xi, yi) ∈ X × Y . Now, the decision to merge A,B before X,Y or vice versa is determined by the sign of ∑σ i=1 αid(pi, qi) − ∑σ i=1 αid(xi, yi). If this expression evaluates to zero, we will break ties arbitrarily but consistently. Observe that for a given set of values for d(pi, qi) and d(xi, yi), the above expression is either 0 for all α, or is equal to zero for the hyperplane passing through the origin and normal to (d(p1, q1)− d(x1, y1), . . . , d(pσ, qσ)− d(xσ, yσ)) in the parameter space Rσ. This hyperplane divides the parameter space into two half-spaces each of which correspond to merging one pair of sets before the other.\nNext we note that, for a given problem instance, as we iterate over all pairs of sets (A,B) and (X,Y ), we can list only O ( n4σ ) possible choices for the hyperplane as there are only so many\n4σ-tuples of points pi, qi, xi, yi. Thus, for m problem instances, we can list at most O ( mn4σ ) different hyperplanes in Rσ. These hyperplanes, can partition the parameter space into at most O (( mn4σ )σ) regions such that all the parameter settings in a given region correspond to identical\nmerge trees and hence identical costs/labeling induced by the witnesses. By an argument similar to the proof for Theorem 6, we can concluce that m = O(σ2 log n).\nTheorem 23. For any Φ, let\nHA2,σ ,Φ = { ΦA2,σ(α) : V→ R | α ∈ R≥0 ∪ {∞,−∞ } .\nThen Pdim(HA2,σ) = O (min(σ log n, n)).\nProof. This proof follows the same reasoning as in Lemma 7. The decision of whether to merge any two pairs of sets (A,B) and (X,Y ), is determined by the sign of the difference in their linkage criterion which is ∑σ i=1 d α(pi, qi)− ∑σ i=1 d α(xi, yi) where the points pi, qi, xi, yi are fixed for a given pairs of set. We know from Theorem 19 that this expression has only O(n2) roots. Furthermore, as we iterate over all pairs of sets (A,B) and (X,Y ), we can only generate as many such expressions as there are 4σ-tuples of points pi, qi, xi, yi. In particular, we will list only O(n\n4σ) such expression each with O(n2) roots. In summary, similar to the proof of Lemma 7, we can argue that for a set of samples in Vm to be shattered, we need that 2m ≤ n4σ · n2 i.e., m = O(σ log n).\nTheorem 24. For any Φ(p) Pdim(HA2,σ ,Φ(p)) = Ω(σ).\nProof. We use a similar line of reasoning as in Lemma 8. In round i of the construction in Lemma 8, the merges were between a set of size 1 and a set of size i+ 2. Therefore, we can easily modify the proof to construct a clustering instance with Ω(σ) rounds, and the merge equations will be the same as in Lemma 8. This instance will have 2Ω(σ) distinct behaviors depending on α, and so the pseudo-dimension is Ω(σ)."
    }, {
      "heading" : "Algorithm",
      "text" : ""
    } ],
    "references" : [ {
      "title" : "0-1 quadratic programming approach for optimum solutions of two scheduling problems",
      "author" : [ "Bahram Alidaee", "Gary Kochenberger", "Ahmad Ahmadian" ],
      "venue" : "International Journal of Systems Science,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 1994
    }, {
      "title" : "Center-based clustering under perturbation stability",
      "author" : [ "Pranjal Awasthi", "Avrim Blum", "Or Sheffet" ],
      "venue" : "Information Processing Letters,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2012
    }, {
      "title" : "k-center clustering under perturbation resilience",
      "author" : [ "Maria-Florina Balcan", "Nika Haghtalab", "Colin White" ],
      "venue" : "In Proceedings of the 43rd annual International Colloquium on Automata, Languages, and Programming (ICALP),",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2016
    }, {
      "title" : "Clustering under perturbation resilience",
      "author" : [ "Maria-Florina Balcan", "Yingyu Liang" ],
      "venue" : "SIAM Journal on Computing,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2016
    }, {
      "title" : "Lectures on modern convex optimization: analysis, algorithms, and engineering applications, volume",
      "author" : [ "Ahron Ben-Tal", "Arkadi Nemirovski" ],
      "venue" : null,
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2001
    }, {
      "title" : "A meta-heuristic factory for vehicle routing problems",
      "author" : [ "Yves Caseau", "François Laburthe", "Glenn Silverstein" ],
      "venue" : "In International Conference on Principles and Practice of Constraint Programming (CP),",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 1999
    }, {
      "title" : "Maximizing quadratic programs: extending Grothendieck’s inequality",
      "author" : [ "Moses Charikar", "Anthony Wirth" ],
      "venue" : "In Proceedings of the Annual Symposium on Foundations of Computer Science (FOCS),",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2004
    }, {
      "title" : "Self-adapting linear algebra algorithms and software",
      "author" : [ "Jim Demmel", "Jack Dongarra", "Victor Eijkhout", "Erika Fuentes", "Antoine Petitet", "Rich Vuduc", "R Clint Whaley", "Katherine Yelick" ],
      "venue" : "Proceedings of the IEEE,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2005
    }, {
      "title" : "What works best when? a systematic evaluation of heuristics for max-cut and qubo",
      "author" : [ "Iain Dunning", "Swati Gupta", "John Silberholz" ],
      "venue" : null,
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2015
    }, {
      "title" : "The RPR2 rounding technique for semidefinite programs",
      "author" : [ "Uriel Feige", "Michael Langberg" ],
      "venue" : "Journal of Algorithms,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2006
    }, {
      "title" : "Improved approximation algorithms for maximum cut and satisfiability problems using semidefinite programming",
      "author" : [ "Michel X Goemans", "David P Williamson" ],
      "venue" : "Journal of the ACM (JACM),",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 1995
    }, {
      "title" : "Improved analysis of complete linkage clustering",
      "author" : [ "Anna Grosswendt", "Heiko Roeglin" ],
      "venue" : "In European Symposium of Algorithms,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2015
    }, {
      "title" : "A PAC approach to application-specific algorithm selection",
      "author" : [ "Rishi Gupta", "Tim Roughgarden" ],
      "venue" : "In Proceedings of the 2016 ACM Conference on Innovations in Theoretical Computer Science (ITCS),",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2016
    }, {
      "title" : "Decision theoretic generalizations of the PAC model for neural net and other learning applications",
      "author" : [ "David Haussler" ],
      "venue" : "Information and computation,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 1992
    }, {
      "title" : "Weighted theta functions and embeddings with applications to max-cut, clustering and summarization",
      "author" : [ "Fredrik D Johansson", "Ankani Chattoraj", "Chiranjib Bhattacharyya", "Devdatt Dubhashi" ],
      "venue" : "In Advances in Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2015
    }, {
      "title" : "Optimal inapproximability results for max-cut and other 2-variable csps",
      "author" : [ "Subhash Khot", "Guy Kindler", "Elchanan Mossel", "Ryan O’Donnell" ],
      "venue" : "SIAM Journal on Computing,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2007
    }, {
      "title" : "Hybridizing the cross-entropy method: An application to the max-cut problem",
      "author" : [ "Manuel Laguna", "Abraham Duarte", "Rafael Mart́ı" ],
      "venue" : "Computers & Operations Research,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2009
    }, {
      "title" : "Quadratic binary programming with application to capital-budgeting problems",
      "author" : [ "DJ Laughhunn" ],
      "venue" : "Operations Research,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 1970
    }, {
      "title" : "Empirical hardness models: Methodology and a case study on combinatorial auctions",
      "author" : [ "Kevin Leyton-Brown", "Eugene Nudelman", "Yoav Shoham" ],
      "venue" : "Journal of the ACM (JACM),",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2009
    }, {
      "title" : "An optimal SDP algorithm for max-cut, and equally optimal long code tests",
      "author" : [ "Ryan O’Donnell", "Yi Wu" ],
      "venue" : "In Proceedings of the Annual Symposium on Theory of Computing (STOC),",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2008
    }, {
      "title" : "A quadratic assignment formulation of the molecular conformation problem",
      "author" : [ "AT Phillips", "JB Rosen" ],
      "venue" : "Journal of Global Optimization,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 1994
    }, {
      "title" : "The algorithm selection problem",
      "author" : [ "John R Rice" ],
      "venue" : "Advances in computers,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 1976
    }, {
      "title" : "Using max cut to enhance rooted trees consistency",
      "author" : [ "Sagi Snir", "Satish Rao" ],
      "venue" : "IEEE/ACM Transactions on Computational Biology and Bioinformatics (TCBB),",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2006
    }, {
      "title" : "On the zeros of finite sums of exponential functions",
      "author" : [ "Timo Tossavainen" ],
      "venue" : "Australian Mathematical Society Gazette,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2006
    }, {
      "title" : "Semi-supervised learning using greedy max-cut",
      "author" : [ "Jun Wang", "Tony Jebara", "Shih-Fu Chang" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2013
    }, {
      "title" : "The design of approximation algorithms",
      "author" : [ "David P Williamson", "David B Shmoys" ],
      "venue" : null,
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 2011
    }, {
      "title" : "Mathematical methods of site selection for electronic message systems (EMS)",
      "author" : [ "Christoph Witzgall" ],
      "venue" : "NASA STI/Recon Technical Report N,",
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 1975
    }, {
      "title" : "Satzilla: portfolio-based algorithm selection for SAT",
      "author" : [ "Lin Xu", "Frank Hutter", "Holger H Hoos", "Kevin Leyton-Brown" ],
      "venue" : "Journal of Artificial Intelligence Research,",
      "citeRegEx" : "30",
      "shortCiteRegEx" : "30",
      "year" : 2008
    }, {
      "title" : "Uncertain behaviours of integrated circuits improve computational performance",
      "author" : [ "Chihiro Yoshimura", "Masanao Yamaoka", "Masato Hayashi", "Takuya Okuyama", "Hidetaka Aoki", "Ken-ichi Kawarabayashi", "Hiroyuki Mizuno" ],
      "venue" : "Scientific reports,",
      "citeRegEx" : "31",
      "shortCiteRegEx" : "31",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 12,
      "context" : "We conduct a learning-theoretic analysis through a framework introduced by Gupta and Roughgarden [14], wherein an application domain is modeled as a distribution over problem instances.",
      "startOffset" : 97,
      "endOffset" : 101
    }, {
      "referenceID" : 10,
      "context" : "The SDP rounding algorithm classes we focus on are known to perform particularly well on the max-cut problem; they contain well-known algorithms such as the GoemansWilliamson (GW) max-cut approximation algorithm, which performs well in the worst-case [12], and more generally all s-linear rounding functions, which were introduced by Feige and Langberg [11].",
      "startOffset" : 251,
      "endOffset" : 255
    }, {
      "referenceID" : 9,
      "context" : "The SDP rounding algorithm classes we focus on are known to perform particularly well on the max-cut problem; they contain well-known algorithms such as the GoemansWilliamson (GW) max-cut approximation algorithm, which performs well in the worst-case [12], and more generally all s-linear rounding functions, which were introduced by Feige and Langberg [11].",
      "startOffset" : 353,
      "endOffset" : 357
    }, {
      "referenceID" : 1,
      "context" : "The linkage-based modules include the classic single-, complete-, and average-linkage algorithms, known to return nearlyoptimal clusterings in a variety of settings [3, 13, 5, 4].",
      "startOffset" : 165,
      "endOffset" : 178
    }, {
      "referenceID" : 11,
      "context" : "The linkage-based modules include the classic single-, complete-, and average-linkage algorithms, known to return nearlyoptimal clusterings in a variety of settings [3, 13, 5, 4].",
      "startOffset" : 165,
      "endOffset" : 178
    }, {
      "referenceID" : 3,
      "context" : "The linkage-based modules include the classic single-, complete-, and average-linkage algorithms, known to return nearlyoptimal clusterings in a variety of settings [3, 13, 5, 4].",
      "startOffset" : 165,
      "endOffset" : 178
    }, {
      "referenceID" : 2,
      "context" : "The linkage-based modules include the classic single-, complete-, and average-linkage algorithms, known to return nearlyoptimal clusterings in a variety of settings [3, 13, 5, 4].",
      "startOffset" : 165,
      "endOffset" : 178
    }, {
      "referenceID" : 12,
      "context" : "Before describing our results, we provide a more formal overview of application-specific algorithm selection through a learning-theoretic lens, as introduced by Gupta and Roughgarden [14].",
      "startOffset" : 183,
      "endOffset" : 187
    }, {
      "referenceID" : 12,
      "context" : "Definition 1 ([14]).",
      "startOffset" : 14,
      "endOffset" : 18
    }, {
      "referenceID" : 17,
      "context" : "Integer quadratic programs have diverse applications, including capital budgeting and financial analysis [19], traffic message management problems [29], machine scheduling [1], and molecular conformation [22].",
      "startOffset" : 105,
      "endOffset" : 109
    }, {
      "referenceID" : 26,
      "context" : "Integer quadratic programs have diverse applications, including capital budgeting and financial analysis [19], traffic message management problems [29], machine scheduling [1], and molecular conformation [22].",
      "startOffset" : 147,
      "endOffset" : 151
    }, {
      "referenceID" : 0,
      "context" : "Integer quadratic programs have diverse applications, including capital budgeting and financial analysis [19], traffic message management problems [29], machine scheduling [1], and molecular conformation [22].",
      "startOffset" : 172,
      "endOffset" : 175
    }, {
      "referenceID" : 20,
      "context" : "Integer quadratic programs have diverse applications, including capital budgeting and financial analysis [19], traffic message management problems [29], machine scheduling [1], and molecular conformation [22].",
      "startOffset" : 204,
      "endOffset" : 208
    }, {
      "referenceID" : 10,
      "context" : "The seminal Goemans-Williamson algorithm is now a textbook example of semidefinite programming [12, 28, 26] and max-cut continues to arise in applications such as machine learning [27], circuit design [31], and computational biology [24].",
      "startOffset" : 95,
      "endOffset" : 107
    }, {
      "referenceID" : 25,
      "context" : "The seminal Goemans-Williamson algorithm is now a textbook example of semidefinite programming [12, 28, 26] and max-cut continues to arise in applications such as machine learning [27], circuit design [31], and computational biology [24].",
      "startOffset" : 95,
      "endOffset" : 107
    }, {
      "referenceID" : 24,
      "context" : "The seminal Goemans-Williamson algorithm is now a textbook example of semidefinite programming [12, 28, 26] and max-cut continues to arise in applications such as machine learning [27], circuit design [31], and computational biology [24].",
      "startOffset" : 180,
      "endOffset" : 184
    }, {
      "referenceID" : 28,
      "context" : "The seminal Goemans-Williamson algorithm is now a textbook example of semidefinite programming [12, 28, 26] and max-cut continues to arise in applications such as machine learning [27], circuit design [31], and computational biology [24].",
      "startOffset" : 201,
      "endOffset" : 205
    }, {
      "referenceID" : 22,
      "context" : "The seminal Goemans-Williamson algorithm is now a textbook example of semidefinite programming [12, 28, 26] and max-cut continues to arise in applications such as machine learning [27], circuit design [31], and computational biology [24].",
      "startOffset" : 233,
      "endOffset" : 237
    }, {
      "referenceID" : 8,
      "context" : "In fact, libraries of max-cut algorithms have been the subject of empirical work in application-specific algorithm selection [10] and max-cut and integer quadratic programming in general have been used as a test bed for new ideas in heuristic design [18].",
      "startOffset" : 125,
      "endOffset" : 129
    }, {
      "referenceID" : 16,
      "context" : "In fact, libraries of max-cut algorithms have been the subject of empirical work in application-specific algorithm selection [10] and max-cut and integer quadratic programming in general have been used as a test bed for new ideas in heuristic design [18].",
      "startOffset" : 250,
      "endOffset" : 254
    }, {
      "referenceID" : 9,
      "context" : "These algorithms make up the class of Random Projection, Randomized Rounding algorithms (RPR2), a general framework introduced by Feige and Langberg [11].",
      "startOffset" : 149,
      "endOffset" : 153
    }, {
      "referenceID" : 10,
      "context" : "878 approximation ratio [12].",
      "startOffset" : 24,
      "endOffset" : 28
    }, {
      "referenceID" : 15,
      "context" : "Assuming the unique games conjecture and P 6= NP , this approximation is optimal to within any additive constant [17].",
      "startOffset" : 113,
      "endOffset" : 117
    }, {
      "referenceID" : 6,
      "context" : "More generally, if A is any real-valued n × n matrix with nonnegative diagonal entries, then there exists an RPR2 algorithm that achieves an approximation ratio of Ω(1/ log n) [8], and in the worst case, this ratio is tight [2].",
      "startOffset" : 176,
      "endOffset" : 179
    }, {
      "referenceID" : 4,
      "context" : "Finally, if A is positive semi-definite, then there exists an RPR2 algorithm that achieves a 2/π approximation ratio [6].",
      "startOffset" : 117,
      "endOffset" : 120
    }, {
      "referenceID" : 9,
      "context" : "We analyze several classes of RPR2 rounding function classes, including s-linear [11], outward rotation [32], and \u000F̃-discretized rounding functions [21].",
      "startOffset" : 81,
      "endOffset" : 85
    }, {
      "referenceID" : 19,
      "context" : "We analyze several classes of RPR2 rounding function classes, including s-linear [11], outward rotation [32], and \u000F̃-discretized rounding functions [21].",
      "startOffset" : 148,
      "endOffset" : 152
    }, {
      "referenceID" : 14,
      "context" : "replace the SDP embedding with an embedding that that is faster to compute, and then run the standard GW algorithm in order to transform the vector solution into a graph cut [16].",
      "startOffset" : 174,
      "endOffset" : 178
    }, {
      "referenceID" : 1,
      "context" : "For the linkage-based procedure, we consider several natural parameterizations of agglomerative procedures which give rise to a spectrum of algorithms interpolating between the popular single-, average-, and complete-linkage procedures, known to perform nearly optimally in many settings [3, 4, 5, 13].",
      "startOffset" : 288,
      "endOffset" : 301
    }, {
      "referenceID" : 2,
      "context" : "For the linkage-based procedure, we consider several natural parameterizations of agglomerative procedures which give rise to a spectrum of algorithms interpolating between the popular single-, average-, and complete-linkage procedures, known to perform nearly optimally in many settings [3, 4, 5, 13].",
      "startOffset" : 288,
      "endOffset" : 301
    }, {
      "referenceID" : 3,
      "context" : "For the linkage-based procedure, we consider several natural parameterizations of agglomerative procedures which give rise to a spectrum of algorithms interpolating between the popular single-, average-, and complete-linkage procedures, known to perform nearly optimally in many settings [3, 4, 5, 13].",
      "startOffset" : 288,
      "endOffset" : 301
    }, {
      "referenceID" : 11,
      "context" : "For the linkage-based procedure, we consider several natural parameterizations of agglomerative procedures which give rise to a spectrum of algorithms interpolating between the popular single-, average-, and complete-linkage procedures, known to perform nearly optimally in many settings [3, 4, 5, 13].",
      "startOffset" : 288,
      "endOffset" : 301
    }, {
      "referenceID" : 21,
      "context" : "The algorithm selection problem was first proposed by Rice in 1979 [23], and has since led to breakthroughs in diverse fields including combinatorial auctions [20], scientific computing [9], vehicle routing [7], and SAT [30], among many others.",
      "startOffset" : 67,
      "endOffset" : 71
    }, {
      "referenceID" : 18,
      "context" : "The algorithm selection problem was first proposed by Rice in 1979 [23], and has since led to breakthroughs in diverse fields including combinatorial auctions [20], scientific computing [9], vehicle routing [7], and SAT [30], among many others.",
      "startOffset" : 159,
      "endOffset" : 163
    }, {
      "referenceID" : 7,
      "context" : "The algorithm selection problem was first proposed by Rice in 1979 [23], and has since led to breakthroughs in diverse fields including combinatorial auctions [20], scientific computing [9], vehicle routing [7], and SAT [30], among many others.",
      "startOffset" : 186,
      "endOffset" : 189
    }, {
      "referenceID" : 5,
      "context" : "The algorithm selection problem was first proposed by Rice in 1979 [23], and has since led to breakthroughs in diverse fields including combinatorial auctions [20], scientific computing [9], vehicle routing [7], and SAT [30], among many others.",
      "startOffset" : 207,
      "endOffset" : 210
    }, {
      "referenceID" : 27,
      "context" : "The algorithm selection problem was first proposed by Rice in 1979 [23], and has since led to breakthroughs in diverse fields including combinatorial auctions [20], scientific computing [9], vehicle routing [7], and SAT [30], among many others.",
      "startOffset" : 220,
      "endOffset" : 224
    }, {
      "referenceID" : 27,
      "context" : "[30].",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 12,
      "context" : "Gupta and Roughgarden leveraged tools from learning theory to conduct a theoretical analysis of several fundamental problems in application-specific algorithm selection [14].",
      "startOffset" : 169,
      "endOffset" : 173
    }, {
      "referenceID" : 13,
      "context" : "[15]] Let F be a class of functions with domain X and range in [0, H], and suppose F has pseudo-dimension d.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 10,
      "context" : "These classes consist of SDP rounding algorithms and are a generalization of the seminal Goemans-Williamson (GW) max-cut algorithm [12].",
      "startOffset" : 131,
      "endOffset" : 135
    }, {
      "referenceID" : 6,
      "context" : "This problem is also known as MaxQP [8].",
      "startOffset" : 36,
      "endOffset" : 39
    }, {
      "referenceID" : 9,
      "context" : "Rounding (RPR2) algorithm, so named by Feige and Langberg [11].",
      "startOffset" : 58,
      "endOffset" : 62
    }, {
      "referenceID" : 9,
      "context" : "For the max-cut problem, Feige and Langberg proved that when the maximium cut in the graph is not very large, an approximation ratio above the GW ratio is possible using an s-linear rounding function [11].",
      "startOffset" : 200,
      "endOffset" : 204
    }, {
      "referenceID" : 14,
      "context" : "quickly find strong approximate solutions to the max-cut problem by first approximating an embedding based on the Lovász theta function and then running the standard GW algorithm to round this embedding into a graph cut [16].",
      "startOffset" : 220,
      "endOffset" : 224
    }, {
      "referenceID" : 19,
      "context" : "In Appendix B, we study the class of \u000F̃-discretized rounding functions as defined by O’Donnell and Wu [21] and the class of outward rotation algorithms proposed by Zwick [32].",
      "startOffset" : 102,
      "endOffset" : 106
    }, {
      "referenceID" : 0,
      "context" : "Naturally, one might expect that the first step would be to bound the pseudo-dimension of the function class Hslin∗ = {slins : A→ [0, 1] | s > 0}, where A is the set of all real-valued n×n matrices with nonnegative diagonal entries.",
      "startOffset" : 130,
      "endOffset" : 136
    }, {
      "referenceID" : 0,
      "context" : "In particular, we instead analyze the class Hslin = {slins : A× Rn → [0, 1] | s > 0}, where slins ( A, ~ Z ) is the value of the fractional assignment produced by projecting the SDP embedding of",
      "startOffset" : 69,
      "endOffset" : 75
    }, {
      "referenceID" : 0,
      "context" : "Let Hslin = {slins : A× Rn → [0, 1] | s > 0}.",
      "startOffset" : 29,
      "endOffset" : 35
    }, {
      "referenceID" : 1,
      "context" : "Next, we define agglomerative clustering algorithms with dynamic programming, which are prevalent in practice and enjoy strong theoretical guarantees in a variety of settings [3, 4, 5, 13].",
      "startOffset" : 175,
      "endOffset" : 188
    }, {
      "referenceID" : 2,
      "context" : "Next, we define agglomerative clustering algorithms with dynamic programming, which are prevalent in practice and enjoy strong theoretical guarantees in a variety of settings [3, 4, 5, 13].",
      "startOffset" : 175,
      "endOffset" : 188
    }, {
      "referenceID" : 3,
      "context" : "Next, we define agglomerative clustering algorithms with dynamic programming, which are prevalent in practice and enjoy strong theoretical guarantees in a variety of settings [3, 4, 5, 13].",
      "startOffset" : 175,
      "endOffset" : 188
    }, {
      "referenceID" : 11,
      "context" : "Next, we define agglomerative clustering algorithms with dynamic programming, which are prevalent in practice and enjoy strong theoretical guarantees in a variety of settings [3, 4, 5, 13].",
      "startOffset" : 175,
      "endOffset" : 188
    }, {
      "referenceID" : 11,
      "context" : "For example, the complete-linkage algorithm yields a constant-factor approximation to the k-center objective for any metric that is induced by a norm, so long as the dimension is constant [13].",
      "startOffset" : 188,
      "endOffset" : 192
    }, {
      "referenceID" : 1,
      "context" : "single-linkage algorithm with dynamic programming returns the optimal solution for any centerbased objective when the optimal solution is resilient to small perturbations on the input distances [3].",
      "startOffset" : 194,
      "endOffset" : 197
    }, {
      "referenceID" : 3,
      "context" : "The average-linkage algorithm also returns the optimal solution when the solution is resilient to perturbations, for the min-sum objective [5].",
      "startOffset" : 139,
      "endOffset" : 142
    }, {
      "referenceID" : 3,
      "context" : "We note that our results hold even when there is a preprocessing step that precedes the agglomerative merge step (Step 1 in Algorithm 3) such as in [5] which merges some points to begin with.",
      "startOffset" : 148,
      "endOffset" : 151
    }, {
      "referenceID" : 0,
      "context" : "α min u∈A,v∈B d(u, v) + (1− α) max u∈A,v∈B d(u, v) ∣∣∣∣ α ∈ [0, 1] }",
      "startOffset" : 60,
      "endOffset" : 66
    }, {
      "referenceID" : 0,
      "context" : "HA1×{Ψ},Φ = { Φ(A1(α),Ψ) : V→ R≥0 ∣∣ α ∈ [0, 1] } and HA2×{Ψ},Φ = { Φ(A2(α),Ψ) : V→ R≥0 ∣∣ α ∈ R ∪ {∞,−∞} } .",
      "startOffset" : 41,
      "endOffset" : 47
    }, {
      "referenceID" : 0,
      "context" : "For each value of α ∈ [0, 1], the algorithm A1(α) induces a binary labeling on each V(i), based on whether or not ΦA1(α) ( V(i) ) ≤ ri.",
      "startOffset" : 22,
      "endOffset" : 28
    }, {
      "referenceID" : 0,
      "context" : "Assuming that ties are broken arbitrarily but consistently, this implies that there is at most one α ∈ [0, 1] such that the choice of whether to merge (A,B) before (X,Y ) is identical for all α < α′, and similarly identical for α ≥ α′.",
      "startOffset" : 103,
      "endOffset" : 109
    }, {
      "referenceID" : 0,
      "context" : "This means we can divide [0, 1] into O(n8) intervals over each of which the merge tree, and therefore the output of ΦA1,V(α), is fixed.",
      "startOffset" : 25,
      "endOffset" : 31
    }, {
      "referenceID" : 0,
      "context" : "Every sample V(i) partitions [0, 1] into O(n8) intervals in this way.",
      "startOffset" : 29,
      "endOffset" : 35
    }, {
      "referenceID" : 0,
      "context" : "Merging all m partitions, we can divide [0, 1] into O(mn8) intervals over each of which ΦA1,V(i)(α), and therefore the labeling induced by the witnesses, is fixed for all i ∈ [m] (similar to Figure 2).",
      "startOffset" : 40,
      "endOffset" : 46
    }, {
      "referenceID" : 23,
      "context" : "[25] (which we state in Appendix C as Theorem 19), it has O(n2) roots.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 0,
      "context" : "[1] Bahram Alidaee, Gary Kochenberger, and Ahmad Ahmadian.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 1,
      "context" : "[3] Pranjal Awasthi, Avrim Blum, and Or Sheffet.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 2,
      "context" : "[4] Maria-Florina Balcan, Nika Haghtalab, and Colin White.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 3,
      "context" : "[5] Maria-Florina Balcan and Yingyu Liang.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 4,
      "context" : "[6] Ahron Ben-Tal and Arkadi Nemirovski.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 5,
      "context" : "[7] Yves Caseau, François Laburthe, and Glenn Silverstein.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 6,
      "context" : "[8] Moses Charikar and Anthony Wirth.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 7,
      "context" : "[9] Jim Demmel, Jack Dongarra, Victor Eijkhout, Erika Fuentes, Antoine Petitet, Rich Vuduc, R Clint Whaley, and Katherine Yelick.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 8,
      "context" : "[10] Iain Dunning, Swati Gupta, and John Silberholz.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 9,
      "context" : "[11] Uriel Feige and Michael Langberg.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 10,
      "context" : "[12] Michel X Goemans and David P Williamson.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 11,
      "context" : "[13] Anna Grosswendt and Heiko Roeglin.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 12,
      "context" : "[14] Rishi Gupta and Tim Roughgarden.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 13,
      "context" : "[15] David Haussler.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 14,
      "context" : "[16] Fredrik D Johansson, Ankani Chattoraj, Chiranjib Bhattacharyya, and Devdatt Dubhashi.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 15,
      "context" : "[17] Subhash Khot, Guy Kindler, Elchanan Mossel, and Ryan O’Donnell.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 16,
      "context" : "[18] Manuel Laguna, Abraham Duarte, and Rafael Mart́ı.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 17,
      "context" : "[19] DJ Laughhunn.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 18,
      "context" : "[20] Kevin Leyton-Brown, Eugene Nudelman, and Yoav Shoham.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 19,
      "context" : "[21] Ryan O’Donnell and Yi Wu.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 20,
      "context" : "[22] AT Phillips and JB Rosen.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 21,
      "context" : "[23] John R Rice.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 22,
      "context" : "[24] Sagi Snir and Satish Rao.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 23,
      "context" : "[25] Timo Tossavainen.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 24,
      "context" : "[27] Jun Wang, Tony Jebara, and Shih-Fu Chang.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 25,
      "context" : "[28] David P Williamson and David B Shmoys.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 26,
      "context" : "[29] Christoph Witzgall.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 27,
      "context" : "[30] Lin Xu, Frank Hutter, Holger H Hoos, and Kevin Leyton-Brown.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 28,
      "context" : "[31] Chihiro Yoshimura, Masanao Yamaoka, Masato Hayashi, Takuya Okuyama, Hidetaka Aoki, Ken-ichi Kawarabayashi, and Hiroyuki Mizuno.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 19,
      "context" : "4 in [21]).",
      "startOffset" : 5,
      "endOffset" : 9
    }, {
      "referenceID" : 10,
      "context" : "For the maxcut problem, outward rotations are proven to work better than the random hyperplane technique of Goemans and Williamson [12] on graphs with “light” max-cuts where the max-cut does not constitute a large proportion of the edges.",
      "startOffset" : 131,
      "endOffset" : 135
    }, {
      "referenceID" : 9,
      "context" : "As stated earlier, though Feige and Langberg later showed that there exists a class of rounding functions for which RPR2 becomes equivalent to outward rotations [11], we will analyze this class as it was originally presented by Zwick [32].",
      "startOffset" : 161,
      "endOffset" : 165
    }, {
      "referenceID" : 10,
      "context" : "The class of outward rotation algorithms is characterized by an angle γ ∈ [0, π/2] varying which results in a range of algorithms between the random hyperplane technique of Goemans and Williamson and the naive approach of outputting a random binary assignment [12].",
      "startOffset" : 260,
      "endOffset" : 264
    }, {
      "referenceID" : 10,
      "context" : "The binary assignment is then defined deterministically based on the sign of the projections like in the GW algorithm [12].",
      "startOffset" : 118,
      "endOffset" : 122
    }, {
      "referenceID" : 0,
      "context" : "Let Howr = { owrγ : A× R2n → [0, 1] | γ ∈ [0, π/2] } .",
      "startOffset" : 29,
      "endOffset" : 35
    }, {
      "referenceID" : 0,
      "context" : "Let Hrprt = {rprts : A× Rn × Rn → [0, 1]|s > 0} and let dHrprt be the pseudo-dimension of Hrprt.",
      "startOffset" : 34,
      "endOffset" : 40
    }, {
      "referenceID" : 0,
      "context" : "Let Hrprt = {rprts : A× Rn × Rn → [0, 1] | s > 0}.",
      "startOffset" : 34,
      "endOffset" : 40
    }, {
      "referenceID" : 0,
      "context" : "So far, all of the distances we have defined are in [1, 2], therefore they trivially satisfy the triangle inequality.",
      "startOffset" : 52,
      "endOffset" : 58
    }, {
      "referenceID" : 23,
      "context" : "[25]).",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 0,
      "context" : "For each value of α ∈ [0, 1], the algorithm A2(α) induces a binary labeling on each V(i), based on whether ΦA2(α)(V) ≤ ri or not.",
      "startOffset" : 22,
      "endOffset" : 28
    }, {
      "referenceID" : 0,
      "context" : "In fact, we can divide [0, 1] into O(mn 8) intervals over each of which ΦA2,V(i)(α), and therefore the labeling induced by the witnesses, is fixed for all i ∈ [m].",
      "startOffset" : 23,
      "endOffset" : 29
    }, {
      "referenceID" : 0,
      "context" : "First of all, in order for d to be a metric, we set all distances in [1, 2] so that the triangle inequality is trivially satisfied.",
      "startOffset" : 69,
      "endOffset" : 75
    }, {
      "referenceID" : 0,
      "context" : "So we must ensure there exists a specific α[−1] ∈ (1, α′) which solves equation 1, and α[1] ∈ (α′, 3) which solves equation 2, and these are the only solutions in the corresponding intervals.",
      "startOffset" : 88,
      "endOffset" : 91
    }, {
      "referenceID" : 0,
      "context" : "All distances are in [1, 2] which ensures the triangle inequality is always satisfied.",
      "startOffset" : 21,
      "endOffset" : 27
    }, {
      "referenceID" : 0,
      "context" : "α[1 1] α[1 −1] α[−1]",
      "startOffset" : 1,
      "endOffset" : 6
    }, {
      "referenceID" : 0,
      "context" : "α[1 1] α[1 −1] α[−1]",
      "startOffset" : 1,
      "endOffset" : 6
    }, {
      "referenceID" : 0,
      "context" : "α[1]",
      "startOffset" : 1,
      "endOffset" : 4
    }, {
      "referenceID" : 0,
      "context" : "α[−1 −1] α[−1 1] α[−1] α[1 −1] α[1] α[1 1] α′ p1 q1",
      "startOffset" : 32,
      "endOffset" : 35
    }, {
      "referenceID" : 0,
      "context" : "α[−1 −1] α[−1 1] α[−1] α[1 −1] α[1] α[1 1] α′ p1 q1",
      "startOffset" : 37,
      "endOffset" : 42
    }, {
      "referenceID" : 0,
      "context" : "α[−1 −1] α[−1 1] α[−1] α[1 −1] α[1] α[1 1] α′ p1 q1",
      "startOffset" : 37,
      "endOffset" : 42
    }, {
      "referenceID" : 0,
      "context" : "Equations E[−1] and E[1] are the following.",
      "startOffset" : 21,
      "endOffset" : 24
    }, {
      "referenceID" : 0,
      "context" : "884 and α[1] ≈ 2.",
      "startOffset" : 9,
      "endOffset" : 12
    }, {
      "referenceID" : 0,
      "context" : "The equations E[−1 −1], E[−1 1], E[1 −1], and E[1 1] are as follows:",
      "startOffset" : 47,
      "endOffset" : 52
    }, {
      "referenceID" : 0,
      "context" : "The equations E[−1 −1], E[−1 1], E[1 −1], and E[1 1] are as follows:",
      "startOffset" : 47,
      "endOffset" : 52
    }, {
      "referenceID" : 0,
      "context" : "123, α[1 1] ≈ 2.",
      "startOffset" : 6,
      "endOffset" : 11
    }, {
      "referenceID" : 0,
      "context" : "123, α[1 1] ≈ 2.",
      "startOffset" : 6,
      "endOffset" : 11
    } ],
    "year" : 2017,
    "abstractText" : "Many data analysis problems are NP-hard, a reality that has motivated researchers to develop a wealth of approximation algorithms and heuristics over the past few decades. Max-cut, clustering, and many other partitioning problems fall into this camp, with widespread applications ranging from statistical physics to computational biology. In this work, we focus on two widely studied classes of algorithms that can be used for clustering and partitioning problems, including rich parametrized classes of SDP rounding algorithms and hierarchical clustering algorithms. The best algorithm to use often depends on the specific application or distribution over instances. A worst-case analysis is often used to compare algorithms, but this single number may be derived from atypical inputs which are not present in the particular problem domain, and thus this analysis may be misleading when determining which algorithm to apply. Therefore, it is necessary to develop optimization methods which return the algorithms and parameters best suited for typical inputs from the application at hand. In this work, we address this problem for integer quadratic programming and clustering within a learning-theoretic framework where the learning algorithm is given samples from an application-specific distribution over problem instances and learns an algorithm which performs well over the distribution. We provide strong sample complexity guarantees and computationally efficient algorithms which optimize an algorithm family’s parameters to best suit typical inputs from a particular application. We analyze these algorithm families using the learning-theoretic notion of pseudo-dimension. Along with upper bounds, we provide the first pseudo-dimension lower bounds in this line of work, which require an involved analysis of each algorithm family’s overall performance on carefully constructed problem instances. Our bounds are tight and therefore nail down the intrinsic complexity of the algorithm classes we analyze, which consist of multi-stage optimization procedures and which are significantly richer and more complex than classes commonly used in learning theory. For example, even for classes of algorithms parameterized by a single parameter, we prove tight superconstant pseudo-dimension bounds. In this way, not only does our work contribute to the study of algorithm design and analysis by providing a theoretical grounding for application-specific algorithm selection, but it also pushes the boundaries of learning theory. ∗Authors’ addresses: {ninamf,vaishnavh,vitercik,crwhite}@cs.cmu.edu. ar X iv :1 61 1. 04 53 5v 1 [ cs .D S] 1 4 N ov 2 01 6",
    "creator" : "LaTeX with hyperref package"
  }
}