{
  "name" : "1206.4641.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Total Variation and Euler’s Elastica for Supervised Learning",
    "authors" : [ "Tong Lin", "Hanlin Xue", "Ling Wang", "Hongbin Zha" ],
    "emails" : [ "tonglin123@gmail.com", "xuehl@cis.pku.edu.cn", "ling.wang.nj@gmail.com", "zha@cis.pku.edu.cn" ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "Supervised learning (Bishop, 2006; Hastie T., 2009) infers a function that maps inputs to desired outputs under the guidance of training data. Two main tasks in supervised learning are classification and regression. A huge number of supervised learning methods have been developed in several decades (see a com-\nAppearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s).\nprehensive empirical comparison of these methods in (Caruana & Niculescu-Mizil, 2006)). Existing methods can be roughly divided into statistics based and function learning based (Kotsiantis et al., 2006). One advantage of function learning methods is that powerful mathematical theories in functional analysis can be utilized rather than doing optimizations on discrete data points.\nMost function learning methods can be derived from Tikhonov regularization, which minimizes a loss term plus a smoothing regularizer. The most successful classification and regression method is SVM (Bishop, 2006; Hastie T., 2009; Shawe-Taylor & Cristianini, 2000), whose cost function is composed of a hinge loss and a RKHS norm determined by a kernel. Replacing the hinge loss by a squared loss, the modified algorithm is called Regularized Least Squares (RLS) method (Rifkin, 2002). In addition, manifold regularization (Belkin et al., 2006) introduced a regularizer of squared gradient magnitude on manifolds. Its discrete version amounts to graph Laplacian regularization (Nadler et al., 2009; Zhou & Schölkopf, 2005), which approximates the original energy functional. A most recent work is the geometric level set (GLS) classifier (Varshney & Willsky, 2010), with an energy functional composed of a margin-based loss and a geometric regularization term based on the surface area\nof the decision boundary. Experiments showed that GLS is competitive with SVM and other state-of-theart classifiers.\nIn this paper, the supervised learning problem is formulated as an energy functional minimization under Tikhonov regularization scheme, with the energy composed of a squared loss and a total variation (TV) penalty or an Euler’s elastica (EE) penalty. Since the TV and EE models have achieved great success in image denoising and image inpainting (Aubert & Kornprobst, 2006; Barbero & Sra, 2011; Chan & Shen, 2005), a natural question is whether the success of TV and EE models on image processing applications can be transferred to high dimensional data analysis such as supervised learning. This paper investigates the question by extending TV and EE models to supervised learning settings, and evaluating their performance on benchmark data sets against state-ofthe-art methods. Figure 1 shows the classification result on the popular two moon data by the EE classifier, and the learned target function. Interestingly, the GLS classifier (Varshney & Willsky, 2010) is also motivated by image processing techniques, and its gradient descent time marching leads to a mean curvature flow.\nThe paper is organized as follows. We begin with a brief review of TV and EE in Section 2. In Section 3 the proposed models are described, and numerical solutions are developed in Section 4. Section 5 presents the experimental results, and Section 6 concludes this paper."
    }, {
      "heading" : "2. Preliminaries",
      "text" : "We briefly introduce total variation and Euler’s elastica from an image processing perspective, and point out connections with prior work in the machine learning literature."
    }, {
      "heading" : "2.1. Total Variation (TV)",
      "text" : "The total variation of a 1D real-valued function f is defined as\nV ab (f) = sup np−1∑ i=0 |f(xi+1)− f(xi)|,\nwhere the supremum runs over all partitions of given interval [a, b]. If f is differentiable, the total variation can be written as\nV ab (f) = ∫ b a |f ′(x)|dx.\nSimply, it is a measure of the total quantity of the change of a function. Notice that if f ′(x) > 0, x ∈ [a, b], it is exactly f(b)− f(a) by the basic theorem of calculus. Total variation has been widely used for image processing tasks such as denoising and inpainting. The pioneering work is Rudin, Osher, and Fatemi’s image denoising model (Rudin et al., 1992):\nJ = ∫ Ω ((I − I0)2 + λ|∇I|)dx,\nwhere I0 is the input image with noise, I the desired output image, λ a regulation parameter that balances two terms, and Ω a 2D image domain. The first fitting term measures the fidelity to the input, while the second is a p-Sobolev regularization term (p = 1) where ∇I is understood in the distributional sense. The main merit is to preserve significant image edges during denoising (Aubert & Kornprobst, 2006; Chan & Shen, 2005). Note that TV may have different definitions (Barbero & Sra, 2011).\nIn the machine learning literature, p-Sobolev regularizer can be found in nonparametric smoothing splines, generalized additive models, and projection pursuit regression models (Hastie T., 2009). Specifically, Belkin et al. proposed the manifold regularization term∫\nx∈M |∇Mf |2dx,\non a manifold M (Belkin et al., 2006). On the other hand, discrete graph Laplacian regularization was discussed in (Zhou & Schölkopf, 2005) as∑\nv∈V |∇vf |p,\nwhere v is a vertex from V , and p is an arbitrary number. This penalty measures the roughness of f over a graph."
    }, {
      "heading" : "2.2. Euler’s Elastica (EE)",
      "text" : "Euler (1744) first introduced the elastica energy for a curve on modeling torsion-free elastic rods. Then Mumford (Mumford, 1991) reintroduced elastica into computer vision. Later, elastica based image inpainting methods were developed in (Chan et al., 2002; Masnou & Morel, 1998).\nA curve γ is said to be Euler’s elastica if it is the equilibrium curve of the elasticity energy:\nE[γ] = ∫ γ (a+ bκ2)ds, (1)\nwhere a and b stand for two positive constant weights, κ denotes the scalar curvature, and ds is the arc length\nelement. Euler obtained the energy in studying the steady shape of a thin and torsion-free rod under external forces. The curve implies the lowest elastica energy, thus getting its name. According to (Mumford, 1991), the key link between the elastica and image inpainting relies on the the interpolation capability of elastica. That is, elastica can comply to the connectivity principle better than total variation. Such kinds of ”nonlinear splines”, like classical polynomial splines, are natural tools for completing the missing or occluded edges.\nThe Euler’s elastica based inpainting model was proposed as (Chan & Shen, 2005)\nJ = ∫ Ω\\D (I − I0)2dx+ λ ∫ Ω (a+ bκ2)|∇I|dx, (2)\nwhere D is the region to be inpainted, Ω the whole image domain, and κ the curvature of the associated level set curve with\nκ = ∇ · ( ∇I |∇I| ). (3)\nBy using calculus of variation, its minimization is reduced to an nonlinear Euler-Lagrange equation. The finite difference scheme can be used to give numerical implementation, and experimental results show that the EE based inpainting performs better than its TV version.\nElastica can be regarded as an extension of total variation, since elastica degenerates to total variation if set a = 1 and b = 0. In fact, elastica is a combination of total variation suppressing oscillations in the gradient direction, and a curvature regularization term that penalizes non-smooth level set curves (see Figure 1)."
    }, {
      "heading" : "3. The Proposed Framework",
      "text" : ""
    }, {
      "heading" : "3.1. Problem Setup",
      "text" : "The general supervised learning problem can be described as follows: given a training data {(x1, y1), ...(xn, yn)} with data points xi ∈ Ω ⊂ Rd and corresponding target varibles yi, the goal is to estimate an unknown function u(x) for a new point x. The difference between classification and regression lies only in the corresponding target values, with one discrete and the other continuous. The widely used Tikhonov regularization framework for supervised learning can be formulated as:\nmin n∑\ni=1\nL(u(xi), yi) + λS(u), (4)\nwhere L denotes a loss function and S(u) is a smoothing term. A variety of loss functions L have been pro-\nposed in the literature: hinge loss for SVM, squared loss for RLS, logistic loss for logistic regression, Huber loss, exponential loss, and among others. Throughout the paper, squared loss is used in all models due to its rather simpler differential form."
    }, {
      "heading" : "3.2. Laplacian Regularization (LR)",
      "text" : "A commonly used model using squared loss can be written as\nmin n∑\ni=1\n(u(xi)− yi)2 + λS(u). (5)\nIf the RKHS norm is used for the smoothing term, the model is called regularized least squares (RLS) (Rifkin, 2002). Another natural choice is the squared L2-norm of the gradient: S(u) = |∇u|2, as proposed in (Belkin et al., 2006). Under a continuous setting, we get the following Laplacian regularization (LR) model:\nJLR[u] = ∫ Ω ((u− y)2 + λ|∇u|2)dx. (6)\nThis LR model has been widely used in the image processing literatures. Using calculus of variations, the minimization can be reduced to the following EulerLagrange partial differential equation (PDE) with a natural boundary condition along ∂Ω:{\n−λ△u+ (u− y) = 0 ∂u ∂n |∂Ω = 0 , (7)\nwhere △u is the Laplacian of u, and n denotes the normal vector along the boundary ∂Ω. This PDE is relatively simple and can be solved using common methods in two and three dimensions. The next Section provides a function approximation method for solving the PDE in high dimensions."
    }, {
      "heading" : "3.3. Total Variation (TV) based Smoothing",
      "text" : "Our goal is to explore how TV and EE can be applied to classification and regression problems on high dimensional data sets. A typical procedure has three steps: (a) Set the function learning problem under a continuous setting and design a proper energy functional; (b) Derive the Euler-Lagrange PDE via the calculus of variations; (c) Solve the PDE on discrete data points.\nSimilar to image denoising, total variation (TV) based supervised learning can be formulated as\nJTV [u] = ∫ Ω (u− y)2dx+ λ ∫ Ω |∇u|dx. (8)\nNote that for binary classification the zero level set of u serves as the final decision boundary. The only\ndifference between LR and TV is just the p-Sobolev regularizer with p = 2 for LR and p = 1 for TV. Intuitively, LR penalizes too much gradients on edges, while TV can permit sharper edges near the decision boundaries between two classes."
    }, {
      "heading" : "3.4. Euler’s Elastica (EE) based Smoothing",
      "text" : "Elastica based supervised learning can be formulated as\nJEE [u] = ∫ Ω (u− y)2dx+ λ ∫ Ω (a+ bκ2)|∇u|dx, (9)\nwhere\nκ = ∇ · ∇u |∇u| . (10)\nDue to the elastica regularizer, the resulting decision boundary of this model can have the lowest elastica energy. If set a = 1 and b = 0, this model degenerates to be the TV model. Therefore, an unified solution can be implemented for both TV model and EE model, as described in the next Section.\nHere we have remarks on the curvature in high dimensional spaces. For a 1-D curve such as in image inpainting tasks, u(x, y) = 0 determines a level set curve according to the implicit function theorem. For a 2-D surface, the curvature given by (3) amounts to the mean curvature of this surface. From (Spivak & Spivak, 1979), the mean curvature can be defined as the average of principal curvatures. Abstractly, it can be expressed as the trace of the second fundamental form divided by the intrinsic dimension d. Table 1 summarizes curvature expressions in 1- D, 2-D, and high dimensional spaces. Hence the same expression(10) can be used for high dimensional situations since the constant 1d−1 can be transferred to b or λ."
    }, {
      "heading" : "4. Algorithms",
      "text" : "In contrast to discrete methods such as SVM and graph Laplacian, the proposed framework operates in a\ncontinuous fashion where powerful mathematical analysis tools can make a sense. Specifically, the calculus of variations can be exploited to minimize the energy functional, leading to the Euler-Langrange PDE.\nAs we have mentioned in section 3, the LR functional minimization can be transformed to solving the PDE (7). Similarly, we get the following PDE for the TV model\nλ∇ · ( ∇u |∇u| ) − (u− y) = 0, (11)\nand the PDE for the EE model\nλ∇ ·V − (u− y) = 0, (12)\nwhere\nV = ϕ(κ)n− 1 |∇u| ∇(ϕ′(κ)|∇u|)\n+ 1\n|∇u|3 ∇u(∇uT∇(ϕ′(κ)|∇u|)), (13)\nand ϕ(κ) := 1+bκ2 by fixing a = 1 for simplicity. One can refer to (Chan et al., 2002) for details about the calculus of variations.\nDue to the nonlinearity of the regularizer in TV and EE model, the corresponding PDE is too complicated to be efficiently solved. Even though the PDE in (7) associated with the LR model can be solved by Finite Difference Method (FDM) or Finite Element Method (FEM) in 2-D or 3-D spaces, currently we have no PDE tools to deal with high dimensional data. Therefore we take a function approximation idea by using radial basis functions (RBF), similar to the treatment in GLS (Varshney & Willsky, 2010)."
    }, {
      "heading" : "4.1. Radial Basis Function Approximation",
      "text" : "The function approximation idea relies on the fact that a function u(x) can be expressed as a sum of weighted basis function {φi(x)}. For example, Taylor expansion represents a function by using polynomials. The most widely used is Radial Basis Function(RBF), which is simple in expressions but has powerful fitting ability. The target function u can be expressed by\nu(x) = n∑ i=1 wiφi(x) (14)\nwith a set of Gaussian RBF\nφi(x) = exp(−c|x− xi|2),\nwhere {xi} are the training samples in supervised learning, and c is a parameter. By using RBF approximation, the problem can be reduced to finding the coefficient {wi}.\nHere are some analytical expressions that will be used later:\n∇u = ∑ i wi∇φi = −c ∑ i wi(x− xi)φi,\nn = ∇u |∇u| = − g |g| , g := ∑ i wi(x− xi)φi,\n△u = ∑ i wj△φi = −c ∑ i wi(d− c|x− xi|2)φi,\nκ = ∇ · ∇u |∇u| = − 1 |∇u|3 ∇uTH(u)∇u+ △u |∇u|\n= 1 |g| ∑ i wiφif,\nwhere H(u) is the Hessian matrix of u, and\nf := 1− d+ c|x− xi|2 − c gT (x− xi)(x− xi)Tg\ngTg ."
    }, {
      "heading" : "4.2. Algorithm for LR",
      "text" : "First of all, let’s consider how to deal with the LR model by solving the linear elliptic PDE (7): −λ△u+ (u − y) = 0. By replacing (14) into the PDE and exploiting the linearity of the Laplacian operator, the goal is to find a set of weights {wi}:∑\ni\n[wi(φi − λ△φi)] = y.\nLet w := (w1, w2, ..., wm) T and y := (y1, y2, ...yn) T , where m is the number of the basis functions and n is the number of the training samples. Then we have the following linear equation system:\nΨw = y, Ψij = φj(xi)− λ△φj(xi).\nNumerically, the following regularized least squares solution is considered in practise to avoid ill-posed problems:\nmin w\n|Ψw − y|2 + η|w|2.\nThe solution is simply given byw = (ΨTΨ+ηI)−1ΨTy with a very fast speed."
    }, {
      "heading" : "4.3. Algorithm for TV and EE models",
      "text" : "As the TV model is one special case of the EE model, we describe solutions for the more complicated EE model in this section. Here two algorithms are developed to tackle the nonlinearity: (1) gradient Descent time marching, and (2) Lag-Linear Equation iteration."
    }, {
      "heading" : "4.3.1. Gradient Descent Time Marching",
      "text" : "Using the calculus of variations, we can get the descent gradient for the desired function. With a matrix notation u(x) = Φw where Φij = φj(xi), the gradient of u is given as follows when Φ is fixed:\nΦ ∂w\n∂t =\n ∂u ∂t |x=x1\n... ∂u ∂t |x=xn  . Then each iteration can be written as\nw(k+1) = w(k)−τ ∂w ∂t = w(k)−τΦ−1  ∂u(k) ∂t |x=x1 ...\n∂u(k)\n∂t |x=xn  , where τ is a small time step, and u(k) renews from w(k). The coefficients w is initialized as w(0) = (ΦTΦ + ηI)−1ΦTy. Then the gradient ∂u∂t must be figured out first.\nFrom the PDE (11), the gradient of u for the TV model is simply given by:\n∂u ∂t = λ∇ · ( ∇u |∇u| ) − (u− y).\nFrom (12) and (13), the gradient of u for the EE model can be written as:\n∂u ∂t = λ∇ ·V − (u− y).\nThrough several steps of calculations by leaving out three and higher order terms, ∇ ·V can be expanded into the following expression:\n∇ ·V = κ− 4bκ△u|∇u|4 ∇u TH(u)∇u+ bκ3 + 2b ( 2△u |∇u|3+\nκ |∇u|4 ) (∇uTH(u))(∇uTH(u))T + 2b { △u |∇u|3−\n3\n|∇u|5∇u TH(u)∇u }( − 2△u|∇u|2 + κ |∇u| ) ∇uTH(u)∇u,\nwhere H(u) is the Hessian matrix of u, and\nκ = ∇ · ∇u |∇u| = △u |∇u| − 1 |∇u|3 ∇uTH(u)∇u.\nWe can see that if setting b = 0 the expression is degraded to ∇ ·V = κ = ∇ · ∇u|∇u| , which is exactly the same expression for the TV model. The time complexity in each iteration is O(n2d), where n is the number of data points and d is the dimension. We set the maximal number of iterations as 40. There are 3 parameters in the algorithm: the RBF parameter c, the regularization parameter λ, and the elastica weight parameter b. Note that we set a = 1 since a can be absorbed into λ."
    }, {
      "heading" : "4.3.2. Lagged Linear Equation Iteration",
      "text" : "Following the spirit of the lagged diffusivity fixedpoint iteration method in (Chan & Shen, 2005), we develop the following lagged linear equation iteration method. Empirically, the original lagged diffusivity fixed-point iteration often yields poor performance due to its brute-force linearization on the nonlinear PDE.\nFor the simpler TV model, by expanding the nonlinear term ∇ · (∇u/|∇u|) we have\n− λ |∇u|\n( △u− ∇u\nTH(u)∇u ∇uT∇u\n) + (u− y) = 0,\nwhere H(u) is the Hessian of function u. Then plugging the RBF approximation into above PDE, we get the following system∑\ni\nwi( |g| λ − f)φi = |g|y, (15)\nwhere g := ∑ i wi(x− xi)φi,\nf := 1− d+ c|x− xi|2 − c gT (x− xi)(x− xi)Tg\ngTg ,\nand d is the data dimension. Using the lagged idea, we obtain the lagged linear equation iteration algorithm: 1) By fixing g, solve the system of linear equations with respect to w to get a new w; 2) Compute g with updated w; 3) Iterate until convergence or maximal iteration number.\nFor the more complicated EE model, we have∑ i wi( |g| λK − f)φi = |g|y, (16)\nwhere K := a+ bκ2 = a+ b ( 1 |g| ∑ i wiφif )2 .\nSimilarly, a two-step lagged iteration procedure can be developed for the EE model: 1) By fixing g and K, solve the linear system with respect to w; 2) Compute g and K with updated w; 3) Iterate until convergence or maximal iteration number. There are three parameters: c, λ, and regularization parameter η (empirically chosen in experiments) in the least squares problems."
    }, {
      "heading" : "5. Experimental Results",
      "text" : "The proposed two approaches (TV and EE) are compared with LR, SVM (with RBF kernels), and BackPropagation Neural Networks (BPNN) for binary classification, multi-class classification, and regression on benchmark data sets."
    }, {
      "heading" : "5.1. Binary Classification",
      "text" : "The test data sets for binary classification are from the libsvm website. Originally, these data sets are scaled to [0,1] and serve as benchmark to test the libsvm implementation. Here we downloaded seven data sets to evaluate performance of our methods (TV and EE) with two kinds of implementations of Gradient Descent method (GD) and Lagged Linear Equation method (lagLE).\nThe optimal parameters for each algorithm are selected by grid search using 5-fold cross-validation. To make the grid search more practical, only the two common parameters (c and λ) are searched for SVM, LR, TV, and EE except BPNN. Empirically, the parameter η is set as 1 for LR, and the parameter b is fixed as 0.01 for EE. Then excluding BPNN, the two common parameters are searched from logarithm from −10 : 10 with step 2. For each data set, we randomly run the 5-fold cross validation ten times to reduce the influence of data partition. Table 2 shows the average classification accuracies for the five methods.\nFrom the table we can see that BPNN performs worst, while the LagLE solution of EE outperforms others on 5 data sets. The GD implementation of TV and EE is competitive with SVM. And similar accuracies are achieved by TV and EE partially because of their close connections."
    }, {
      "heading" : "5.2. Multi-class Classification",
      "text" : "For multi-class tests, we collected data sets from libsvm website and UCI Machine Learning Repository, including frequently used small data sets and the USPS handwritten digital set. For USPS data, PCA is used to reduce the dimension to 30 and we randomly select 1000 samples for experiments.\nExcept for BPNN that has a built-in ability for multiclass tasks, almost all function learning approaches are originally designed for binary classification. In order to handle multi-class situations, usually one versus all or one versus one strategies can be adopted. If using one vs all, one needs to learn M functions to fulfill the multi-class task, where M is the number of classes. Recently in (Varshney & Willsky, 2010), an efficient binary encoding strategy was proposed to represent the decision boundary by only log2M functions. In our experiments the one vs all strategy is used. Same as the binary problems, we use the 5-fold cross-validation to choose the optimal parameters for each method. Except BPNN, all other methods have 2 common parameters which are searched from logarithm from−10 : 10 with step 1. The results of average\naccuracies are shown in table 3.\nThe accuracy results demonstrate that BPNN performs the worst, while TV and EE are comparable to SVM on the 11 test data sets. Compared with TV/EE, SVM achieves higher accuracies on 4 data sets, performs worse on 4 data sets, and offers the same best accuracies on 2 data sets. One reason might be that very complex and even wiggly decision boundaries are preferred for some multi-class data sets. Due to the strong regularization on the geometric shapes, TV/EE can not adapt to yield complex decision hypersurfaces for these data sets."
    }, {
      "heading" : "5.3. Regression",
      "text" : "We use seven regression data sets from UCI Machine Learning Repository to validate the proposed methods compared with SVM, BPNN, and LR. All data sets are scaled to [0,1]. Note that here the Gradient Descent (GD) method is used for TV and EE. We take the same experimental settings by running ten times of 5- fold cross-validation for each data set. Table 4 shows the regression results using mean square errors (MSE). Clearly, we can see that both TV and EE achieve lower\nMSE than SVM and LR on 6 data sets. Also TV and EE outperform BPNN on 5 data sets. The results demonstrated superb regression ability of our proposed methods."
    }, {
      "heading" : "6. Conclusion",
      "text" : "Regularization framework and function learning approaches have become very popular in the recent machine learning literature. Due to the great success of total variation and Euler’s elastica models in image processing area, we extend these two models for supervised classification and regression on high dimensional data sets. The TV regularizer permits steeper edges near the decision boundaries, while the elastica smoothing term penalizes non-smooth level set hypersurfaces of the target function. Compared with SVM and BPNN, our proposed methods have demonstrated the competitive performance on commonly used benchmark data sets. Specifically, TV and EE models achieve better performance on most data sets for binary classification and regression. Currently one main disadvantage is the slow convergence speed of iteration procedures. The future work is to explore other\nfitting loss term such as hinge loss, to study other possibilities of basis functions, to investigate the existence and uniqueness of the PDE solutions, and to reduce the running time."
    }, {
      "heading" : "Acknowledgments",
      "text" : "The authors would like to thank the anonymous reviewers for their helpful suggestions. This work was supported by the National Basic Research Program of China (973 Program number 2011CB302202) and the National Science Foundation of China (NSFC grant 61075119)."
    } ],
    "references" : [ {
      "title" : "Mathematical problems in image processing: partial differential equations and the calculus of variations",
      "author" : [ "G. Aubert", "P. Kornprobst" ],
      "venue" : null,
      "citeRegEx" : "Aubert and Kornprobst,? \\Q2006\\E",
      "shortCiteRegEx" : "Aubert and Kornprobst",
      "year" : 2006
    }, {
      "title" : "Fast newton-type methods for total variation regularization",
      "author" : [ "A. Barbero", "S. Sra" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Barbero and Sra,? \\Q2011\\E",
      "shortCiteRegEx" : "Barbero and Sra",
      "year" : 2011
    }, {
      "title" : "Manifold regularization: A geometric framework for learning from labeled and unlabeled examples",
      "author" : [ "M. Belkin", "P. Niyogi", "V. Sindhwani" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Belkin et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Belkin et al\\.",
      "year" : 2006
    }, {
      "title" : "Pattern recognition and machine learning",
      "author" : [ "C.M. Bishop" ],
      "venue" : null,
      "citeRegEx" : "Bishop,? \\Q2006\\E",
      "shortCiteRegEx" : "Bishop",
      "year" : 2006
    }, {
      "title" : "An empirical comparison of supervised learning algorithms",
      "author" : [ "R. Caruana", "A. Niculescu-Mizil" ],
      "venue" : "In ICML, pp",
      "citeRegEx" : "Caruana and Niculescu.Mizil,? \\Q2006\\E",
      "shortCiteRegEx" : "Caruana and Niculescu.Mizil",
      "year" : 2006
    }, {
      "title" : "Image processing and analysis: variational, PDE, wavelet, and stochastic methods",
      "author" : [ "T.F. Chan", "J. Shen" ],
      "venue" : "Society for Industrial Mathematics,",
      "citeRegEx" : "Chan and Shen,? \\Q2005\\E",
      "shortCiteRegEx" : "Chan and Shen",
      "year" : 2005
    }, {
      "title" : "Euler’s elastica and curvature-based inpainting",
      "author" : [ "T.F. Chan", "S.H. Kang", "J. Shen" ],
      "venue" : "SIAM Journal on Applied Mathematics, pp",
      "citeRegEx" : "Chan et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Chan et al\\.",
      "year" : 2002
    }, {
      "title" : "The Elements of Statistical Learning: Data Mining, Inference, and Prediction",
      "author" : [ "T. Hastie", "R. Tibshirani", "J. Friedman" ],
      "venue" : null,
      "citeRegEx" : "Hastie et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Hastie et al\\.",
      "year" : 2009
    }, {
      "title" : "Machine learning:a review of classification and combining techniques",
      "author" : [ "S. Kotsiantis", "I. Zaharakis", "P. Pintelas" ],
      "venue" : "In Artificial Intelligence Review,",
      "citeRegEx" : "Kotsiantis et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Kotsiantis et al\\.",
      "year" : 2006
    }, {
      "title" : "Level lines based disocclusion",
      "author" : [ "S. Masnou", "J.M. Morel" ],
      "venue" : "In ICIP, pp. 259–263",
      "citeRegEx" : "Masnou and Morel,? \\Q1998\\E",
      "shortCiteRegEx" : "Masnou and Morel",
      "year" : 1998
    }, {
      "title" : "Elastica and computer vision",
      "author" : [ "D. Mumford" ],
      "venue" : "Center for Intelligent Control Systems,",
      "citeRegEx" : "Mumford,? \\Q1991\\E",
      "shortCiteRegEx" : "Mumford",
      "year" : 1991
    }, {
      "title" : "Semi-supervised learning with the graph laplacian: The limit of infinite unlabelled data",
      "author" : [ "B. Nadler", "N. Srebro", "X. Zhou" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Nadler et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Nadler et al\\.",
      "year" : 2009
    }, {
      "title" : "Everything old is new again : a fresh look at historical approaches in machine learning",
      "author" : [ "R.M. Rifkin" ],
      "venue" : "PhD thesis,",
      "citeRegEx" : "Rifkin,? \\Q2002\\E",
      "shortCiteRegEx" : "Rifkin",
      "year" : 2002
    }, {
      "title" : "Nonlinear total variation based noise removal algorithms",
      "author" : [ "L.I. Rudin", "S. Osher", "E. Fatemi" ],
      "venue" : "Physica D: Nonlinear Phenomena,",
      "citeRegEx" : "Rudin et al\\.,? \\Q1992\\E",
      "shortCiteRegEx" : "Rudin et al\\.",
      "year" : 1992
    }, {
      "title" : "An introduction to support vector machines and other kernel-based learning methods",
      "author" : [ "J. Shawe-Taylor", "N. Cristianini" ],
      "venue" : null,
      "citeRegEx" : "Shawe.Taylor and Cristianini,? \\Q2000\\E",
      "shortCiteRegEx" : "Shawe.Taylor and Cristianini",
      "year" : 2000
    }, {
      "title" : "A comprehensive introduction to differential geometry, volume 4",
      "author" : [ "M. Spivak" ],
      "venue" : "Publish or perish Berkeley,",
      "citeRegEx" : "Spivak and Spivak,? \\Q1979\\E",
      "shortCiteRegEx" : "Spivak and Spivak",
      "year" : 1979
    }, {
      "title" : "Classification using geometric level sets",
      "author" : [ "K.R. Varshney", "A.S. Willsky" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Varshney and Willsky,? \\Q2010\\E",
      "shortCiteRegEx" : "Varshney and Willsky",
      "year" : 2010
    }, {
      "title" : "Regularization on discrete spaces",
      "author" : [ "D. Zhou", "B. Schölkopf" ],
      "venue" : "In Pattern Recognition,",
      "citeRegEx" : "Zhou and Schölkopf,? \\Q2005\\E",
      "shortCiteRegEx" : "Zhou and Schölkopf",
      "year" : 2005
    } ],
    "referenceMentions" : [ {
      "referenceID" : 3,
      "context" : "Supervised learning (Bishop, 2006; Hastie T., 2009) infers a function that maps inputs to desired outputs under the guidance of training data.",
      "startOffset" : 20,
      "endOffset" : 51
    }, {
      "referenceID" : 8,
      "context" : "Existing methods can be roughly divided into statistics based and function learning based (Kotsiantis et al., 2006).",
      "startOffset" : 90,
      "endOffset" : 115
    }, {
      "referenceID" : 3,
      "context" : "The most successful classification and regression method is SVM (Bishop, 2006; Hastie T., 2009; Shawe-Taylor & Cristianini, 2000), whose cost function is composed of a hinge loss and a RKHS norm determined by a kernel.",
      "startOffset" : 64,
      "endOffset" : 129
    }, {
      "referenceID" : 12,
      "context" : "Replacing the hinge loss by a squared loss, the modified algorithm is called Regularized Least Squares (RLS) method (Rifkin, 2002).",
      "startOffset" : 116,
      "endOffset" : 130
    }, {
      "referenceID" : 2,
      "context" : "In addition, manifold regularization (Belkin et al., 2006) introduced a regularizer of squared gradient magnitude on manifolds.",
      "startOffset" : 37,
      "endOffset" : 58
    }, {
      "referenceID" : 11,
      "context" : "Its discrete version amounts to graph Laplacian regularization (Nadler et al., 2009; Zhou & Schölkopf, 2005), which approximates the original energy functional.",
      "startOffset" : 63,
      "endOffset" : 108
    }, {
      "referenceID" : 13,
      "context" : "The pioneering work is Rudin, Osher, and Fatemi’s image denoising model (Rudin et al., 1992):",
      "startOffset" : 72,
      "endOffset" : 92
    }, {
      "referenceID" : 2,
      "context" : "on a manifold M (Belkin et al., 2006).",
      "startOffset" : 16,
      "endOffset" : 37
    }, {
      "referenceID" : 10,
      "context" : "Then Mumford (Mumford, 1991) reintroduced elastica into computer vision.",
      "startOffset" : 13,
      "endOffset" : 28
    }, {
      "referenceID" : 6,
      "context" : "Later, elastica based image inpainting methods were developed in (Chan et al., 2002; Masnou & Morel, 1998).",
      "startOffset" : 65,
      "endOffset" : 106
    }, {
      "referenceID" : 10,
      "context" : "According to (Mumford, 1991), the key link between the elastica and image inpainting relies on the the interpolation capability of elastica.",
      "startOffset" : 13,
      "endOffset" : 28
    }, {
      "referenceID" : 12,
      "context" : "If the RKHS norm is used for the smoothing term, the model is called regularized least squares (RLS) (Rifkin, 2002).",
      "startOffset" : 101,
      "endOffset" : 115
    }, {
      "referenceID" : 2,
      "context" : "Another natural choice is the squared L2-norm of the gradient: S(u) = |∇u|, as proposed in (Belkin et al., 2006).",
      "startOffset" : 91,
      "endOffset" : 112
    }, {
      "referenceID" : 6,
      "context" : "One can refer to (Chan et al., 2002) for details about the calculus of variations.",
      "startOffset" : 17,
      "endOffset" : 36
    } ],
    "year" : 2012,
    "abstractText" : "In recent years, total variation (TV) and Euler’s elastica (EE) have been successfully applied to image processing tasks such as denoising and inpainting. This paper investigates how to extend TV and EE to the supervised learning settings on high dimensional data. The supervised learning problem can be formulated as an energy functional minimization under Tikhonov regularization scheme, where the energy is composed of a squared loss and a total variation smoothing (or Euler’s elastica smoothing). Its solution via variational principles leads to an Euler-Lagrange PDE. However, the PDE is always high-dimensional and cannot be directly solved by common methods. Instead, radial basis functions are utilized to approximate the target function, reducing the problem to finding the linear coefficients of basis functions. We apply the proposed methods to supervised learning tasks (including binary classification, multi-class classification, and regression) on benchmark data sets. Extensive experiments have demonstrated promising results of the proposed methods.",
    "creator" : " TeX output 2012.05.16:1551"
  }
}