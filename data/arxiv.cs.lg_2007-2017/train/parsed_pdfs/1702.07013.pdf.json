{
  "name" : "1702.07013.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Learning Hawkes Processes from Short Doubly-Censored Event Sequences",
    "authors" : [ "Hongteng Xu", "Dixin Luo", "Hongyuan Zha" ],
    "emails" : [ "HXU42@GATECH.EDU", "DIXIN.LUO@UTORONTO.CA", "ZHA@CC.GATECH.EDU" ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "Real-world interactions among multiple entities are often recorded as asynchronous event sequences, such as user behaviors in social networks, job hunting and hopping among companies, and diseases and their complications. The entities or event types in the sequences often exhibit selftriggering and mutually-triggering patterns. For example, a tweet of a twitter user may trigger further responses from her friends (Zhao et al., 2015). A disease of a patient may trigger other complications (Choi et al., 2015). Hawkes\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, 2017. JMLR: W&CP. Copyright 2017 by the author(s).\nprocesses, an important kind of temporal point process model (Hawkes & Oakes, 1974), have capability to describe the triggering patterns quantitatively and capture the infectivity network of the entities.\nDespite the usefulness of Hawkes processes, robust learning of Hawkes processes often needs many event sequences with events occurring over a long observation window. Unfortunately, the observation window is likely to be very short and sequence-specific in many important practical applications, i.e., within an imagined universal window, each sequence is only observed with a corresponding short subinterval of it, and the events outside this sub-interval are not observed — we call them short doubly-censored (SDC) event sequences. Existing learning algorithms of Hawkes processes directly applied to SDCs may suffer from overfitting, and what is worse, the triggering patterns between historical events and current ones are lost, so that the triggering patterns learned from SDC event sequences are often unreliable. This problem is a thorny issue in several practical applications, especially in those having timevarying triggering patterns. For example, the disease networks of patients should evolve with the increase of age. However, it is very hard to track and record people’s diseases on a life-time scale. Instead, we can only obtain their several admissions (even only one admission) in a hospital during one or two years, which are just SDC event sequences. Therefore, it is highly desirable to propose a method to learn Hawkes processes having a longtime support from a collection of SDC event sequences\nIn this paper, we propose a novel and simple data synthesis method to enhance the robustness of learning algorithms for Hawkes processes. Fig. 1 illustrates the principle of our method. Given a set of SDC event sequences, we sample predecessor for each event sequence from potential candidates and stitch them together as new training data. In the sampling step, the distribution of predecessor (and successor) is estimated according to the similarities between\nar X\niv :1\n70 2.\n07 01\n3v 2\n[ cs\n.L G\n] 7\nJ un\ncurrent sequence and its candidates, and the similarity is defined based on the information of time stamps and (optional) features of event sequences. We analyze the rationality and the feasibility of our data synthesis method and discuss the necessary condition for using the method. Experimental results show that our data synthesis method indeed helps to improve the robustness of various learning algorithms for Hawkes processes. Especially in the case of time-varying Hawkes processes, applying our method in the learning phase achieves much better results than learning directly from SDC event sequences, which is meaningful for many practical applications, e.g., constructing dynamical disease network, and learning long-term infectivity among different IT companies."
    }, {
      "heading" : "2. Related Work",
      "text" : "An event sequence can be represented as s = {(ti, ci)}Mi=1, where time stamps ti’s are in an observation window [Tb, Te] and events ci’s are in a set of event types C = {1, ..., C}. A point process {Nc}c∈C is a random process model taking event sequences as instances, where Nc = {Nc(t)|t ∈ [Tb, Te]} and Nc(t) is the number of type-c events occurring at or before time t. A point process can be characterized via its conditional intensity function λc(t) = E[dNc(t)|HCt ]/dt, where c ∈ C and HCt = {(ti, ci)|ti < t, ci ∈ C} is the set of history. It represents the expected instantaneous happening rate of events given historical record (Daley & Vere-Jones, 2007). The intensity is often modeled with certain parameters Θ to capture the phenomena of interests, i.e., self-triggering (Hawkes & Oakes, 1974) or self-correcting (Xu et al., 2015). Based on {λc(t)}c∈C , the likelihood of an event sequence s is\nL(s; Θ) = ∏\ni λci(ti) exp\n( − ∑\nc ∫ Te Tb λc(s)ds ) . (1)\nHawkes Processes. Hawkes processes (Hawkes & Oakes, 1974) have a particular form of intensity:\nλc(t) = µc + ∑C\nc′=1 ∫ t 0 φcc′(t, s)dNc′(s), (2)\nwhere µc is the exogenous base intensity independent of the\nhistory while ∫ t 0 φcc′(t, s)dNc′(s) is the endogenous intensity capturing the influence of historical events on type-c ones at time t (Xu et al., 2016a). Here, φcc′(t, s) ≥ 0 is called impact function. It quantifies the influence of the type-c′ event at time s to the type-c event at time t. Hawkes processes provide us with a physically-meaningful model to capture the infectivity among various events, which are used in social network analysis (Zhou et al., 2013b; Zhao et al., 2015), behavior analysis (Yang & Zha, 2013; Luo et al., 2015) and financial analysis (Bacry et al., 2013). However, the methods in these references assume that the impact function is shift-invariant (i.e., φcc′(t, s) = φcc′(t− s), t ≥ s), which limits their applications on longtime scale. Recently, the time-dependent Hawkes process (TiDeH) in (Kobayashi & Lambiotte, 2016) and the neural network-based Hawkes process in (Mei & Eisner, 2016) learn very flexible Hawkes processes with complicated intensity functions. Because they highly depend on the size and the quality of data, they may fail in the case of SDC event sequences.\nLearning from Imperfect Observations. In practice, we need to learn sequential models from imperfect observations (e.g., interleaved (Xu et al., 2016b), aggregated (Luo et al., 2016) and extremely-short sequences (Xu et al., 2016c)). Multiple imputation (MI) (Rubin, 2009) is a general framework to build surrogate observations from the current model. For time series, bootstrap method (Efron, 1982; Politis & Romano, 1994; Gonçalves & Kilian, 2004) and its variants (Paparoditis & Politis, 2001; Guan & Loh, 2007) have been used to improve learning results when observations are insufficient. In survival analysis, many techniques have been made to deal with truncated and censored data (Turnbull, 1974; De Gruttola & Lagakos, 1989; Klein & Moeschberger, 2005; Van den Berg & Drepper, 2016). For point processes, the global (Streit, 2010) or local (Fan, 2009) likelihood maximization estimators (MLE) are used to learn Poisson processes. Nonparametric approaches for non-homogeneous Poisson processes use the pseudo MLE (Sun & Kalbfleisch, 1995) or full MLE (Wellner & Zhang, 2000). The bootstrap methods above are also used to learn point processes (Cowling et al., 1996; Guan & Loh, 2007; Kirk & Stumpf, 2009). To learn Hawkes processes\nrobustly, structural constraints, e.g., low-rank (Luo et al., 2015) and group-sparse regularizers (Xu et al., 2016a), are introduced. However, all of these methods do not consider the case of SDC event sequences for Hawkes processes."
    }, {
      "heading" : "3. Learning from SDC Event Sequences",
      "text" : "Suppose that the original complete event sequences are in a long observation window. However, the observation window in practice might be segmented into several intervals {Tnb , Tne }Nn=1, and we can only observe Kn SDC sequences {snk} Kn k=1 in the n-th interval, n = 1, ..., N . Although we can still apply maximum likelihood estimator to learn Hawkes processes, i.e.,\nminΘ− ∑\nn,k logL(snk ; Θ), (3)\nthe SDC event sequences would lead to over-fitting problem and the loss of triggering patterns. Can we do better in such a situation? In this work, we propose a data synthesis method based on a sampling-stitching mechanism, which extends SDC event sequences to longer ones and enhances the robustness of learning algorithms."
    }, {
      "heading" : "3.1. Data Synthesis via Sampling-Stitching",
      "text" : "Denote the k-th SDC event sequence in the n-th interval as snk . Because its predecessor is unavailable, if we learn the parameters of our model via (3) directly, we actually impose a strong assumption on our data that there is no event happening before snk (or previous events are too far away from snk to have influences on s n k ). Obviously, this assumption is questionable — it is likely that there are influential events happening before snk . A more reasonable strategy is enumerating potential predecessors and maximizing the expected log-likelihood over the whole observation window:\nminΘ− ∑\nn,k Es∼HC\nTn b\n[logL([s, snk ]; Θ)]. (4)\nHere Ex∼D[f(x)] represents the expectation of function f(x) with random variable x yielding to a distribution D. s ∼ HCTnb means all possible history before T n b , and L([s, snk ]; Θ) is the likelihood of stitched sequence [s, snk ].\nThe stitched sequence [s, snk ] can be generated via sampling SDC sequence s from previous 1st, ..., (k− 1)-th intervals and stitching s to snk . The sampling process yields to the probabilistic distribution of the stitched sequences. Given snk , we can compute its similarity between its potential predecessor sn ′\nk′ in [T n′ b , T n′ e ] as\nw(sn ′\nk′ , s n k ) =  S(Tnb , T n′ e )S(f n k , f n′ k′ )︸ ︷︷ ︸ optional , Tn ′ e ≤ Tnb ,\n0, otherwise. (5)\nHere, S(a, b) = exp(−‖b − a‖22/σs) is a predefined similarity function with parameter σs. fnk is the feature of snk , which is available in some applications. Note that the availability of feature is optional — even if the feature of sequence is unavailable, we can still define the similarity measurement purely based on time stamps. The normalized {w(sn′k′ , snk )} provides us with the probability that sn ′ k′ appears before snk , i.e., p(s n′ k′ |snk ) ∝ w(sn ′ k′ , s n k ). Then, we can sample sn ′\nk′ according to the categorical distribution, i.e., sn ′\nk′ ∼ Category(w(·, snk )).\nWe can apply such a sampling-stitching mechanism L times iteratively to the SDC sequences in both backward and forward directions and get long stitched event sequences. Specifically, we represent a stitched event sequence as sstitch = [s1, ..., s2L+1], sl ∈ {snk}, l = 1, ..., 2L+ 1, whose probability is\np(sstitch) ∝ ∏2L\nl=1 w(sl, sl+1). (6)\nNote that our data synthesis method naturally contains two variants. When the starting (the ending) point of time window is unavailable, we use the time stamp of the first (the last) event of SDC sequence instead. Additionally, we can relax the constraint Tn ′\ne ≤ Tnb in (5) and allow a SDC sequence to have an overlap with its predecessor/successor. In this case, we preserve the overlap part randomly either from itself or its predecessor/successor before applying our sampling-stitching method. These two variants ensure that our data synthesis method is doable in practice, which are used in the following experiments on real-world data."
    }, {
      "heading" : "3.2. Justification",
      "text" : "After applying our data synthesis method, we obtain many stitched event sequences, which can be used as instances for estimating Es∼HC\nTn b [logL([s, snk ]; Θ)]. Specifically, taking advantage of stitched sequences, we can rewrite the learning problem in (4) approximately as\nminΘ− ∑\nsstitch∈S p(sstitch) logL(sstitch; Θ), (7)\nwhich is actually the minimum cross-entropy estimation. p(sstitch) represents the “true” probability that the stitched sequence happens, which is estimated via the predefined similarity measurement and the sampling mechanism. The likelihood L(sstitch; Θ) represents the “unnatural” probability that the stitched sequence happens, which is estimated based on the definition in (1). Our data synthesis method takes advantage of the information of time stamps and (optional) features and makes p(sstitch) suitable for practical situations. For example, the likelihood of a sequence generally reduces with the increase of observation time window. The proposed probability p(sstitch) yields to\nthe same pattern — according to (6), the longer a stitched sequence is, the smaller its probability becomes.\nThe set of all possible stitched sequences, i.e., the S in (7), is very large, whose cardinality is |S| = O( ∏N n=1Kn). In practice, we cannot and do not need to enumerate all possible combinations. An empirical setting is making the number of stitched sequences comparable to that of original SDC event sequences, i.e., generating O( ∑N n=1Kn) stitched sequences. In the following experiments, we just apply 5(= U ) trials and generate 5 stitched sequences for each original SDC event sequence, which achieves a tradeoff between computational complexity and performance."
    }, {
      "heading" : "3.3. Feasibility",
      "text" : "It should be noted that our data synthesis method is only suitable for those complicated point processes whose historical events have influences on current and future ones. Specifically, we analyze the feasibility of our method for several typical point processes.\nPoisson Processes. Our data synthesis method cannot improve learning results if the event sequences are generated via Poisson processes. For Poisson processes, the happening rate of future events is independent of historical events. In other words, the intensity function of each interval can be learned independently based on the SDC event sequences. The stitched sequences do not provide us with any additional information.\nHawkes Processes. For Hawkes processes, whose intensity function is defined as (2), our data synthesis method can enhance the robustness of learning algorithm generally. In particular, consider a “long” event sequence generated via a Hawkes process in the time window [Tb, Te]. If we divide the time window into 2 intervals, i.e., [Tb, T ] and (T, Te], the intensity function corresponding to the second interval can be written as\nλc(t) = µc + ∑ ti≤T φcci(t, ti) + ∑ T<ti≤Te φcci(t, ti). (8)\nIf the events in the first interval are unobserved, we just have a SDC event sequence, and the second term in (8) is unavailable. Learning Hawkes processes directly from the SDC event sequence ignores the information of the second term, which has a negative influence on learning results. Our data synthesis method leverages the information from other potential predecessors and generates multiple candidate long sequences. As a result, we obtain multiple intensity functions sharing the second interval and maximize the weighted sum of their log-likelihood functions (i.e., an estimated expectation of the log-likelihood of the real long sequence), as (7) does.\nCompared with learning from SDC event sequences directly, applying our data synthesis method can im-\nprove learning results in general, unless the term∑ ti≤T φcci(t, ti) is ignorable. Specifically, we can model the impact functions {φcc′(t, s)}c,c′∈C of Hawkes processes based on basis representation:\nφcc′(t, s) = ψcc′(t)︸ ︷︷ ︸ Infectivity × g(t− s)︸ ︷︷ ︸ Triggering kernel\n= ∑M\nm=1 acc′mκm(t)g(t− s).\n(9)\nHere, we decompose impact functions into two parts: 1) Infectivity ψcc′(t) = ∑M m=1 acc′mκm(t) represents the infectivity of event type c′ to c at time t.1 2) Triggering kernel g(t) = exp(−βt) measures the time decay of infectivity. It means that the infectivity of a historical event to current one reduces exponentially with the increase of temporal distance between them. When β is very large, φcc′(t, s) decays rapidly with the increase of t − s, and the events happening long ago can be ignored. In such a situation, our data synthesis method is unable to improve learning results."
    }, {
      "heading" : "4. Implementation for Hawkes Processes",
      "text" : "Hawkes process is a kind of physically-interpretable model for many natural and social phenomena. The proposed model in (9) reflects many common properties of realworld event sequences. First, the infectivity among various event types often changes smoothly in practice: in social networks, the interaction between two users changes smoothly, which is not established or blocked suddenly; in disease networks, the infectivity among diseases should change smoothly with the increase of patient’s age. Applying Gaussian basis representation guarantees the smoothness of infectivity function. Second, the triggering kernel measures the decay of infectivity over time. According to existing work, the decay of infectivity is exponential approximately, which has been verified in many real-world data (Zhou et al., 2013a; Kobayashi & Lambiotte, 2016; Choi et al., 2015). For learning Hawkes processes from SDC event sequences, we combine our data synthesis method with an EM-based learning algorithm of Hawkes processes. Applying our data synthesis method, we obtain a set of stitched event sequences S = {sn} and their appearance probabilities {pn}, where sn = {(tni , cni ) In i=1|tni ∈ [Tnb , Tne ], cni ∈ C} and pn is calculated based on (5). According to (7, 9), we can learn target Hawkes process via\nmin µ≥0, A≥0\n− ∑|S|\nn=1 pn logL(sn; Θ) + γR(A). (10)\n1When M = 1 and κm(t) ≡ 1, we obtain the simplest timeinvariant Hawkes process. Relaxing the shift-invariant assumption, i.e., M > 1 and κm(t) is Gaussian, we obtain a flexible time-varying Hawkes process model.\nHere Θ = {µ, A} represents the parameters of our model. The vector µ = [µc] and the tensor A = [acc′m] are nonnegative. Based on (1, 9), the log-likelihood function is\nlogL(sn; Θ)\n= ∑In\ni=1 log λci(ti)− ∑C c=1 ∫ Tne Tnb λc(s)ds\n= ∑In\ni=1 log\n[ µcni + ∑ j<i g(τnij) ∑M m=1 acni cnjmκm(t n i ) ] − ∑C\nc=1\n( ∆nµc − ∑M m=1 ∑In i=1 ∑ j≤i accnjmGij ) ,\nwhere τnij = t n i − tnj , Gij = ∫ tni+1 tni\nκm(s)g(s − tnj )ds, and ∆n = Tne − Tnb . R(A) represents the regularizer of parameters, whose weight is γ. Following existing work in (Luo et al., 2015; Zhou et al., 2013a; Xu et al., 2016a), we assume the infectivity connections among different event types to be sparse and impose a `1-norm regularizer on the coefficient tensor A, i.e., R(A) = ‖A‖1 =∑ c,c′,m |acc′m|.\nWe can solve the problem via an EM algorithm. Specifically, when sparse regularizer is applied, we take advantage of ADMM method, introducing auxiliary variable Z = [zcc′m] and dual variable U = [ucc′m] for A and rewriting the objective function in (10) as\n− ∑\nn pn logL(sn; Θ) + 0.5ρ‖A−Z‖2F\n+ ρtr(U>(A−Z)) + γ‖Z‖1. Here ρ controls the weights of regularization terms, which increases with the number of EM iterations. tr(·) computes the trace of matrix. Then, we can update {µ,A}, Z, and U alternatively.\nUpdate µ and A: Given the parameters in the k-th iteration, we apply Jensen’s inequality to − ∑ n logL(sn; Θ) and obtain a surrogate objective function for µ andA:\nQ(µ,A; µk,Ak,Zk,Uk)\n=− N∑ n=1 pn { In∑ i=1 [∑ j<i M∑ m=1 qijm log g(τnij)acni cnjmκm(t n i ) qijm\n+ qi log µcni qi − C∑ c=1 M∑ m=1 ∑ j≤i accnjmGij ] −∆n C∑ c=1 µc } + 0.5ρ‖A−Zk +Uk‖2F ,\nwhere qi = µkcn i\nλk cn i (tni )\nand qijm = g(τnij)a k cn i cn j mκm(t n i )\nλk cn i (tni )\n, and\nλkcni (t n i ) is calculated based on µ k and Ak. Then, we can update µ andA via solving ∂Q∂µ = 0 and ∂Q ∂A = 0. Both of these two equations have closed-form solution:\nµk+1c =\n∑ n pn ∑ cni =c\nqi∑ n pn∆n , ak+1cc′m =\n√ B2 − 4ρC −B\n2ρ , (11)\nAlgorithm 1 Learning Algorithm of Hawkes Processes 1: Input: Event sequences S. The threshold V . Prede-\nfined parameters β, σκ, and γ. 2: Output: ParametersA and µ. 3: Initialize Ak and µk randomly. Zk = Ak, Uk = 0. k = 0, ρ = 1. 4: repeat 5: ObtainAk+1 and µk+1 via (11). 6: Obtain Zk+1 via (12). 7: Obtain Uk+1 via (13). 8: k = k + 1, ρ = 1.5ρ. 9: until ‖Ak −Ak−1‖F < V\n10: A = Ak, µ = µk.\nwhere B = ρ(ukcc′m − zkcc′m) + ∑ n pn ∑\ncni =c, c n j =c\n′, j≤i Gij , C = − ∑ n pn ∑\ncni =c, c n j =c\n′, j≤i qijm.\nUpdate Z: Given Ak+1 and Uk, we can update Z via solving the following optimization problem:\nminZ γ‖Z‖1 + 0.5ρ‖Ak+1 −Z +Uk‖2F .\nApplying soft-thresholding method, we have\nZk+1 = F γ ρ (Ak+1 +Uk), (12)\nwhere Fη(x) = sign(x) min{|x| − η, 0} is the softthresholding function.\nUpdate U : Given Ak+1 and Zk+1, we can further update dual variable as\nUk+1 = Uk + (Ak+1 −Zk+1). (13)\nIn summary, Algorithm 1 shows the scheme of our learning method. Note that the algorithm can be applied to SDC event sequences directly via ignoring pn’s."
    }, {
      "heading" : "5. Experiments",
      "text" : ""
    }, {
      "heading" : "5.1. Implementation Details",
      "text" : "To demonstrate the usefulness of our data synthesis method, we combine it with various learning algorithms of Hawkes processes and learn different models accordingly from SDC event sequences. For time-invariant Hawkes processes, we consider two learning algorithms — our EMbased learning algorithm and the least squares (LS) algorithm in (Eichler et al., 2016). For time-varying Hawkes processes, we apply our EM-based learning algorithm. In the following experiments, we use Gaussian basis functions: κm(t) = exp((t − tm)2/σκ) with center tm and\nbandwidth σκ. The number and the bandwidth of basis can be set according to the basis selection method proposed in (Xu et al., 2016a). Additionally, we set V = 10−4, γ = 1, and σs = 1 in our algorithm. Given SDC event sequences, we learn Hawkes processes in three ways: 1) learning directly from SDC event sequences; 2) applying the stationary bootstrap method in (Politis & Romano, 1994) to generate more synthetic SDC event sequences and learning from these sequences accordingly; 3) learning from stitched sequences generated via our data synthesis method. For real-world data, whose SDC sequences do not have predefined starting and ending time stamps, we applied the variants of our method mentioned in the end of Section 3.1."
    }, {
      "heading" : "5.2. Synthetic Data",
      "text" : "The synthetic SDC event sequences are generated via the following method: 2000 complete event sequences are simulated in the time window [0, 50] based on a 2-dimensional Hawkes process. The base intensity {µc}2c=1 are randomly generated in the range [0.1, 0.2]. The parameter of triggering kernel, β, is set to be 0.2. For time-invariant Hawkes processes, we set the infectivity {ψcc′(t)} to be 4 constants randomly generated in the range [0, 0.2]. For time-varying Hawkes processes, we set ψcc′(t) = 0.2 cos(2π ωcc′ 50 t), where {ωcc′} are randomly generated in the range [1, 4]. Given these complete event sequences, we select 1000 se-\nquences as testing set while the remaining 1000 sequences as training set. To generate SDC event sequences, we segment time window into 10 intervals, and just randomly preserve the data in one interval for each training sequences. We test all methods in 10 trials and compare them on the relative error between real parameters Θ and their estimation results Θ̂, i.e., ‖Θ−Θ̂‖2‖Θ‖2 , and the log-likelihood of testing sequences.\nTime-invariant Hawkes Processes. Fig. 2 shows the comparisons on log-likelihood and relative error for various methods. In Fig. 2(a) we can find that compared with the learning results based on complete event sequences, the results based on SDC event sequences degrade a lot (lower log-likelihood and higher relative error) because of the loss of information. Our data synthesis method improves the learning results consistently with the increase of training sequences, which outperforms its bootstrap-based competitor (Politis & Romano, 1994) as well. To demonstrate the universality of our method, besides our EM-based algorithm, we apply our method to the Least Squares (LS) algorithm (Eichler et al., 2016). Fig. 2(b) shows that our method also improves the learning results of the LS algorithm in the case of SDC event sequences. Both the log-likelihood and the relative error obtained from the stitched sequences approach to the results learned from complete sequences.\nTime-varying Hawkes Processes. Fig. 3 shows the comparisons on log-likelihood and relative error for various methods. Similarly, the learning results are improved because of applying our method — higher log-likelihood and lower relative error are obtained and their standard deviation (the error bars associated with curves) is shrunk. In this case, applying our method twice achieves better results than applying once, which verifies the usefulness of the iterative framework in our sampling-stitching algorithm. Besides objective measurements, in Fig. 4 we visualize the infectivity functions {ψcc′(t)}. It is easy to find that the infectivity functions learned from stitched sequences (red curves) are comparable to those learned from complete event sequences (yellow curves), which have small estimation errors of the ground truth (black curves).\nNote that our iterative framework is useful, especially for time-varying Hawkes processes, when the number of stitches is not very large. In our experiments, we fixed the maximum number of synthetic sequences. As a result, Figs. 2 and 3 show that the likelihoods first increase (i.e., stitching once or twice) and then decrease (i.e., stitching more than three times) while the relative errors have opponent changes w.r.t. the number of stitches. These phenomena imply that too many stitches introduce too much unreliable interdependency among events. Therefore, we fix the number of stitches to 2 in the following experiments."
    }, {
      "heading" : "5.3. Real-World Data",
      "text" : "Besides synthetic data, we also test our method on realworld data, including the LinkedIn data collected by ourselves and the MIMIC III data set (Johnson et al., 2016).\nLinkedIn Data. The LinkedIn data we collected online contain job hopping records of 3, 000 LinkedIn users in 82 IT companies. For each user, her/his check-in time stamps corresponding to different companies are recorded as an event sequence, and her/his profile (e.g., education background, skill list, etc.) is treated as the feature associated with the sequence. For each person, the attractiveness of a company is always time-varying. For example, a young man may be willing to join in startup companies and in-\ncrease his income via jumping between different companies. With the increase of age, he would more like to stay in the same company and achieve internal promotions. In other words, the infectivity network among different companies should be dynamical w.r.t. the age of employees. Unfortunately, most of the records in LinkedIn are short and doubly-censored — only the job hopping events in recent years are recorded. How to construct the dynamical infectivity network among different companies from the SDC event sequences is still an open problem.\nApplying our data synthesis method, we can stitch different users’ job hopping sequences based on their ages (time stamps) and their profile (feature) and learn the dynamical network of company over time. In particular, we select 100 users with relatively complete job hopping history (i.e., the range of their working experience is over 25 years) as testing set. The remaining 2, 900 users are randomly selected as training set. The log-likelihood of testing set in 10 trials is shown in Fig. 5(a). We can find that the log-likelihood obtained from stitched sequences is higher than that obtained from original SDC sequences or that obtained from the sequences generated via the bootstrap method (Politis & Romano, 1994), and its standard deviation is bounded stably. Fig. 6(a) visualizes the adjacent matrix of infectivity network. The properties of the network verifies the rationality of our results: 1) the diagonal elements of the adjacent matrix are larger than other elements in general, which reflects the fact that most employees would like to stay in the same company and achieve a series of internal promotions; 2) with the increase of age, the infectivity network becomes sparse, which reflects the fact that users are more likely to try different companies in the early stages of their careers.\nMIMIC III Data. The MIMIC III data contain admission records of over 40, 000 patients in the Beth Israel Deaconess Medical Center between 2001 and 2012. For each patient, her/his admission time stamps and diseases (represented via the ICD-9 codes (Deyo et al., 1992)) are recorded as an event sequence, and her/his profile (includ-\ning gender, race and chronic history) is represented as binary feature of the sequence. As aforementioned, some work (Choi et al., 2015) has been done to extract timeinvariant disease network from admission records. However, the real disease network should be time-varying w.r.t. the age of patient. Similar to the LinkedIn data, here we only obtain SDC event sequences — the ranges of most admission records are just 1 or 2 years.\nApplying our data synthesis method, we can leverage the information from different patients and stitch their sequences based on their ages and their profile. Focusing on 600 common diseases in 12 categories, we select 15, 000 patients’ admission records randomly as training set and 1, 000 patients with relatively complete records as testing set. Fig. 5(b) shows that applying our data synthesis method indeed helps to improve log-likelihood of testing data. Compared with our bootstrap-based competitor, our data synthesis method gets more obvious improvements. Furthermore, we visualize the adjacent matrix of dynamical network of disease categories in Fig. 6(b). We can find that: 1) with the increase of age the disease network becomes dense, which reflects the fact that the complications of diseases are more and more common when people be-\ncome old; 2) the networks show that neoplasms and the diseases of circulatory, respiratory, and digestive systems have strong self-triggering patterns because the treatments of these diseases often include several phases and require patients to make admission multiple times; 3) for kids and teenagers, their disease networks (i.e., “Age 0” and “Age 10” networks) are very sparse, and their common diseases mainly include neoplasms and the diseases of circulatory, respiratory, and digestive systems; 4) for middle-aged people, the reasons for their admissions are diverse and complicated so that their disease networks are dense and include many mutually-triggering patterns; 5) for longevity people, their disease networks (i.e., “Age 80” and “Age 90” networks) are relatively sparser than those of middle-aged people, because their admissions are generally caused by elderly chronic diseases.\nAdditionally, we visualize the dynamical networks of the diseases of circulatory systems in Fig. 7, and find some interesting triggering patterns. For example, for kids (“Age 0” network), the typical circulatory diseases are “diseases of mitral and aortic valves” (ICD-9 396) and “cardiac dysrhythmias” (ICD-9 427), which are common for premature babies and the kids having congenital heart disease. For\nthe old (“Age 80” network), the network becomes dense. We can find that 1) as a main cause of death, “heart failure” (ICD-9 428) is triggered via multiple other diseases, especially “secondary hypertension” (ICD-9 405); 2) “secondary hypertension” is also likely to cause “other and illdefined cerebrovascular disease” (ICD-9 437); 3) “Hemorrhoids” (ICD-9 455), as a common disease with strong self-triggering pattern, will cause frequent admissions of patients. In summary, the analysis above verifies the rationality of our result — the dynamical disease networks we learned indeed reflect the properties of human’s health trajectory."
    }, {
      "heading" : "6. Conclusion",
      "text" : "In this paper, we propose a novel data synthesis method to learn Hawkes processes from SDC event sequences. With the help of temporal information and optional features, we measure the similarities among different SDC event sequences and estimate the distribution of potential long event sequences. Applying a sampling-stitching mechanism, we successfully synthesize a large amount of long event sequences and learn point processes robustly. We test our method for both time-invariant and time-varying Hawkes processes. Experiments show that our data synthesis method improves the robustness of learning algorithms for various models. In the future, we plan to provide more theoretical and quantitative analysis to our data synthesis method and apply it to more applications.\nAcknowledgements. This work is supported in part via NSF DMS-1317424, NIH R01 GM108341, NSFC 61628203, U1609220 and the Key Program of Shanghai Science and Technology Commission 15JC1401700."
    } ],
    "references" : [ {
      "title" : "Some limit theorems for hawkes processes and application to financial statistics",
      "author" : [ "Bacry", "Emmanuel", "Delattre", "Sylvain", "Hoffmann", "Marc", "Muzy", "Jean-Francois" ],
      "venue" : "Stochastic Processes and their Applications,",
      "citeRegEx" : "Bacry et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Bacry et al\\.",
      "year" : 2013
    }, {
      "title" : "Constructing disease network and temporal progression model via context-sensitive hawkes process",
      "author" : [ "Choi", "Edward", "Du", "Nan", "Chen", "Robert", "Song", "Le", "Sun", "Jimeng" ],
      "venue" : "In ICDM,",
      "citeRegEx" : "Choi et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Choi et al\\.",
      "year" : 2015
    }, {
      "title" : "Bootstrap confidence regions for the intensity of a poisson point process",
      "author" : [ "Cowling", "Ann", "Hall", "Peter", "Phillips", "Michael J" ],
      "venue" : "Journal of the American Statistical Association,",
      "citeRegEx" : "Cowling et al\\.,? \\Q1996\\E",
      "shortCiteRegEx" : "Cowling et al\\.",
      "year" : 1996
    }, {
      "title" : "Analysis of doubly-censored survival data, with application to aids",
      "author" : [ "De Gruttola", "Victor", "Lagakos", "Stephen W" ],
      "venue" : "Biometrics, pp",
      "citeRegEx" : "Gruttola et al\\.,? \\Q1989\\E",
      "shortCiteRegEx" : "Gruttola et al\\.",
      "year" : 1989
    }, {
      "title" : "Adapting a clinical comorbidity index for use with icd9-cm administrative databases",
      "author" : [ "Deyo", "Richard A", "Cherkin", "Daniel C", "Ciol", "Marcia A" ],
      "venue" : "Journal of clinical epidemiology,",
      "citeRegEx" : "Deyo et al\\.,? \\Q1992\\E",
      "shortCiteRegEx" : "Deyo et al\\.",
      "year" : 1992
    }, {
      "title" : "The jackknife, the bootstrap and other resampling plans, volume 38",
      "author" : [ "Efron", "Bradley" ],
      "venue" : null,
      "citeRegEx" : "Efron and Bradley.,? \\Q1982\\E",
      "shortCiteRegEx" : "Efron and Bradley.",
      "year" : 1982
    }, {
      "title" : "Graphical modeling for multivariate hawkes processes with nonparametric link functions",
      "author" : [ "Eichler", "Michael", "Dahlhaus", "Rainer", "Dueck", "Johannes" ],
      "venue" : "Journal of Time Series Analysis,",
      "citeRegEx" : "Eichler et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Eichler et al\\.",
      "year" : 2016
    }, {
      "title" : "Local Likelihood for Intervalcensored and Aggregated Point Process Data",
      "author" : [ "Fan", "Chun-Po Steve" ],
      "venue" : "PhD thesis, University of Toronto,",
      "citeRegEx" : "Fan and Steve.,? \\Q2009\\E",
      "shortCiteRegEx" : "Fan and Steve.",
      "year" : 2009
    }, {
      "title" : "Bootstrapping autoregressions with conditional heteroskedasticity of unknown form",
      "author" : [ "Gonçalves", "Sılvia", "Kilian", "Lutz" ],
      "venue" : "Journal of Econometrics,",
      "citeRegEx" : "Gonçalves et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Gonçalves et al\\.",
      "year" : 2004
    }, {
      "title" : "A thinned block bootstrap variance estimation procedure for inhomogeneous spatial point patterns",
      "author" : [ "Guan", "Yongtao", "Loh", "Ji Meng" ],
      "venue" : "Journal of the American Statistical Association,",
      "citeRegEx" : "Guan et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Guan et al\\.",
      "year" : 2007
    }, {
      "title" : "A cluster process representation of a self-exciting process",
      "author" : [ "Hawkes", "Alan G", "Oakes", "David" ],
      "venue" : "Journal of Applied Probability,",
      "citeRegEx" : "Hawkes et al\\.,? \\Q1974\\E",
      "shortCiteRegEx" : "Hawkes et al\\.",
      "year" : 1974
    }, {
      "title" : "Mimic-iii, a freely accessible critical care database",
      "author" : [ "Johnson", "Alistair EW", "Pollard", "Tom J", "Shen", "Lu", "Lehman", "Li-wei H", "Feng", "Mengling", "Ghassemi", "Mohammad", "Moody", "Benjamin", "Szolovits", "Peter", "Celi", "Leo Anthony", "Mark", "Roger G" ],
      "venue" : "Scientific data,",
      "citeRegEx" : "Johnson et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Johnson et al\\.",
      "year" : 2016
    }, {
      "title" : "Gaussian process regression bootstrapping: exploring the effects of uncertainty in time course data",
      "author" : [ "Kirk", "Paul DW", "Stumpf", "Michael PH" ],
      "venue" : null,
      "citeRegEx" : "Kirk et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Kirk et al\\.",
      "year" : 2009
    }, {
      "title" : "Survival analysis: techniques for censored and truncated data",
      "author" : [ "Klein", "John P", "Moeschberger", "Melvin L" ],
      "venue" : "Springer Science & Business Media,",
      "citeRegEx" : "Klein et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Klein et al\\.",
      "year" : 2005
    }, {
      "title" : "Tideh: Timedependent hawkes process for predicting retweet dynamics",
      "author" : [ "Kobayashi", "Ryota", "Lambiotte", "Renaud" ],
      "venue" : "arXiv preprint arXiv:1603.09449,",
      "citeRegEx" : "Kobayashi et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Kobayashi et al\\.",
      "year" : 2016
    }, {
      "title" : "Multitask multi-dimensional hawkes processes for modeling event sequences",
      "author" : [ "Luo", "Dixin", "Xu", "Hongteng", "Zhen", "Yi", "Ning", "Xia", "Zha", "Hongyuan", "Yang", "Xiaokang", "Zhang", "Wenjun" ],
      "venue" : "In IJCAI,",
      "citeRegEx" : "Luo et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Luo et al\\.",
      "year" : 2015
    }, {
      "title" : "Learning mixtures of markov chains from aggregate data with structural constraints",
      "author" : [ "Luo", "Dixin", "Xu", "Hongteng", "Zhen", "Yi", "Dilkina", "Bistra", "Zha", "Hongyuan", "Yang", "Xiaokang", "Zhang", "Wenjun" ],
      "venue" : "Transactions on Knowledge and Data Engineering,",
      "citeRegEx" : "Luo et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Luo et al\\.",
      "year" : 2016
    }, {
      "title" : "The neural hawkes process: A neurally self-modulating multivariate point process",
      "author" : [ "Mei", "Hongyuan", "Eisner", "Jason" ],
      "venue" : "arXiv preprint arXiv:1612.09328,",
      "citeRegEx" : "Mei et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Mei et al\\.",
      "year" : 2016
    }, {
      "title" : "The stationary bootstrap",
      "author" : [ "Politis", "Dimitris N", "Romano", "Joseph P" ],
      "venue" : "Journal of the American Statistical association,",
      "citeRegEx" : "Politis et al\\.,? \\Q1994\\E",
      "shortCiteRegEx" : "Politis et al\\.",
      "year" : 1994
    }, {
      "title" : "Multiple Imputation for Nonresponse in Surveys, volume 307",
      "author" : [ "Rubin", "Donald B" ],
      "venue" : null,
      "citeRegEx" : "Rubin and B.,? \\Q2009\\E",
      "shortCiteRegEx" : "Rubin and B.",
      "year" : 2009
    }, {
      "title" : "Poisson point processes: imaging, tracking, and sensing",
      "author" : [ "Streit", "Roy L" ],
      "venue" : "Springer Science & Business Media,",
      "citeRegEx" : "Streit and L.,? \\Q2010\\E",
      "shortCiteRegEx" : "Streit and L.",
      "year" : 2010
    }, {
      "title" : "Estimation of the mean function of point processes based on panel count data",
      "author" : [ "J Sun", "Kalbfleisch", "JD" ],
      "venue" : "Statistica Sinica,",
      "citeRegEx" : "Sun et al\\.,? \\Q1995\\E",
      "shortCiteRegEx" : "Sun et al\\.",
      "year" : 1995
    }, {
      "title" : "Nonparametric estimation of a survivorship function with doubly censored data",
      "author" : [ "Turnbull", "Bruce W" ],
      "venue" : "Journal of the American Statistical Association,",
      "citeRegEx" : "Turnbull and W.,? \\Q1974\\E",
      "shortCiteRegEx" : "Turnbull and W.",
      "year" : 1974
    }, {
      "title" : "Inference for shared-frailty survival models with left-truncated data",
      "author" : [ "Van den Berg", "Gerard J", "Drepper", "Bettina" ],
      "venue" : "Econometric Reviews,",
      "citeRegEx" : "Berg et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Berg et al\\.",
      "year" : 2016
    }, {
      "title" : "Two estimators of the mean of a counting process with panel count data",
      "author" : [ "Wellner", "Jon A", "Zhang", "Ying" ],
      "venue" : "Annals of Statistics,",
      "citeRegEx" : "Wellner et al\\.,? \\Q2000\\E",
      "shortCiteRegEx" : "Wellner et al\\.",
      "year" : 2000
    }, {
      "title" : "Trailer generation via a point process-based visual attractiveness model",
      "author" : [ "Xu", "Hongteng", "Zhen", "Yi", "Zha", "Hongyuan" ],
      "venue" : "In IJCAI,",
      "citeRegEx" : "Xu et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2015
    }, {
      "title" : "Learning granger causality for hawkes processes",
      "author" : [ "Xu", "Hongteng", "Farajtabar", "Mehrdad", "Zha", "Hongyuan" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Xu et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2016
    }, {
      "title" : "Pinfer: Learning to infer concurrent request paths from system kernel events",
      "author" : [ "Xu", "Hongteng", "Ning", "Xia", "Zhang", "Hui", "Rhee", "Junghwan", "Jiang", "Guofei" ],
      "venue" : "In ICAC,",
      "citeRegEx" : "Xu et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2016
    }, {
      "title" : "Icu patient flow prediction via discriminative learning of mutually-correcting processes",
      "author" : [ "Xu", "Hongteng", "Wu", "Weichang", "Nemati", "Shamim", "Zha", "Hongyuan" ],
      "venue" : "arXiv preprint arXiv:1602.05112,",
      "citeRegEx" : "Xu et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2016
    }, {
      "title" : "Mixture of mutually exciting processes for viral diffusion",
      "author" : [ "Yang", "Shuang-Hong", "Zha", "Hongyuan" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Yang et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Yang et al\\.",
      "year" : 2013
    }, {
      "title" : "Seismic: A selfexciting point process model for predicting tweet popularity",
      "author" : [ "Zhao", "Qingyuan", "Erdogdu", "Murat A", "He", "Hera Y", "Rajaraman", "Anand", "Leskovec", "Jure" ],
      "venue" : "In KDD,",
      "citeRegEx" : "Zhao et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Zhao et al\\.",
      "year" : 2015
    }, {
      "title" : "Learning social infectivity in sparse low-rank networks using multidimensional hawkes processes",
      "author" : [ "Zhou", "Ke", "Zha", "Hongyuan", "Song", "Le" ],
      "venue" : "In AISTATS,",
      "citeRegEx" : "Zhou et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Zhou et al\\.",
      "year" : 2013
    }, {
      "title" : "Learning triggering kernels for multi-dimensional hawkes processes",
      "author" : [ "Zhou", "Ke", "Zha", "Hongyuan", "Song", "Le" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Zhou et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Zhou et al\\.",
      "year" : 2013
    } ],
    "referenceMentions" : [ {
      "referenceID" : 30,
      "context" : "For example, a tweet of a twitter user may trigger further responses from her friends (Zhao et al., 2015).",
      "startOffset" : 86,
      "endOffset" : 105
    }, {
      "referenceID" : 1,
      "context" : "A disease of a patient may trigger other complications (Choi et al., 2015).",
      "startOffset" : 55,
      "endOffset" : 74
    }, {
      "referenceID" : 25,
      "context" : ", self-triggering (Hawkes & Oakes, 1974) or self-correcting (Xu et al., 2015).",
      "startOffset" : 60,
      "endOffset" : 77
    }, {
      "referenceID" : 30,
      "context" : "Hawkes processes provide us with a physically-meaningful model to capture the infectivity among various events, which are used in social network analysis (Zhou et al., 2013b; Zhao et al., 2015), behavior analysis (Yang & Zha, 2013; Luo et al.",
      "startOffset" : 154,
      "endOffset" : 193
    }, {
      "referenceID" : 15,
      "context" : ", 2015), behavior analysis (Yang & Zha, 2013; Luo et al., 2015) and financial analysis (Bacry et al.",
      "startOffset" : 27,
      "endOffset" : 63
    }, {
      "referenceID" : 0,
      "context" : ", 2015) and financial analysis (Bacry et al., 2013).",
      "startOffset" : 31,
      "endOffset" : 51
    }, {
      "referenceID" : 16,
      "context" : ", 2016b), aggregated (Luo et al., 2016) and extremely-short sequences (Xu et al.",
      "startOffset" : 21,
      "endOffset" : 39
    }, {
      "referenceID" : 2,
      "context" : "The bootstrap methods above are also used to learn point processes (Cowling et al., 1996; Guan & Loh, 2007; Kirk & Stumpf, 2009).",
      "startOffset" : 67,
      "endOffset" : 128
    }, {
      "referenceID" : 15,
      "context" : ", low-rank (Luo et al., 2015) and group-sparse regularizers (Xu et al.",
      "startOffset" : 11,
      "endOffset" : 29
    }, {
      "referenceID" : 1,
      "context" : "According to existing work, the decay of infectivity is exponential approximately, which has been verified in many real-world data (Zhou et al., 2013a; Kobayashi & Lambiotte, 2016; Choi et al., 2015).",
      "startOffset" : 131,
      "endOffset" : 199
    }, {
      "referenceID" : 15,
      "context" : "Following existing work in (Luo et al., 2015; Zhou et al., 2013a; Xu et al., 2016a), we assume the infectivity connections among different event types to be sparse and impose a `1-norm regularizer on the coefficient tensor A, i.",
      "startOffset" : 27,
      "endOffset" : 83
    }, {
      "referenceID" : 6,
      "context" : "For time-invariant Hawkes processes, we consider two learning algorithms — our EMbased learning algorithm and the least squares (LS) algorithm in (Eichler et al., 2016).",
      "startOffset" : 146,
      "endOffset" : 168
    }, {
      "referenceID" : 6,
      "context" : "To demonstrate the universality of our method, besides our EM-based algorithm, we apply our method to the Least Squares (LS) algorithm (Eichler et al., 2016).",
      "startOffset" : 135,
      "endOffset" : 157
    }, {
      "referenceID" : 11,
      "context" : "Besides synthetic data, we also test our method on realworld data, including the LinkedIn data collected by ourselves and the MIMIC III data set (Johnson et al., 2016).",
      "startOffset" : 145,
      "endOffset" : 167
    }, {
      "referenceID" : 4,
      "context" : "For each patient, her/his admission time stamps and diseases (represented via the ICD-9 codes (Deyo et al., 1992)) are recorded as an event sequence, and her/his profile (includ-",
      "startOffset" : 94,
      "endOffset" : 113
    }, {
      "referenceID" : 1,
      "context" : "As aforementioned, some work (Choi et al., 2015) has been done to extract timeinvariant disease network from admission records.",
      "startOffset" : 29,
      "endOffset" : 48
    } ],
    "year" : 2017,
    "abstractText" : "Many real-world applications require robust algorithms to learn point processes based on a type of incomplete data — the so-called short doublycensored (SDC) event sequences. We study this critical problem of quantitative asynchronous event sequence analysis under the framework of Hawkes processes by leveraging the idea of data synthesis. Given SDC event sequences observed in a variety of time intervals, we propose a sampling-stitching data synthesis method — sampling predecessors and successors for each SDC event sequence from potential candidates and stitching them together to synthesize long training sequences. The rationality and the feasibility of our method are discussed in terms of arguments based on likelihood. Experiments on both synthetic and real-world data demonstrate that the proposed data synthesis method improves learning results indeed for both timeinvariant and time-varying Hawkes processes.",
    "creator" : "LaTeX with hyperref package"
  }
}