{
  "name" : "1602.01580.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Long-term Planning by Short-term Prediction",
    "authors" : [ "Shai Shalev-Shwartz", "Nir Ben-Zrihem", "Aviad Cohen", "Amnon Shashua" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "decide on immediate actions so as to optimize a long term objective. For example, when a car tries to merge in a roundabout it should decide on an immediate acceleration/braking command, while the long term effect of the command is the success/failure of the merge. Such problems are characterized by continuous state and action spaces, and by interaction with multiple agents, whose behavior can be adversarial. We argue that dual versions of the MDP framework (that depend on the value function and the Q function) are problematic for autonomous driving applications due to the non Markovian of the natural state space representation, and due to the continuous state and action spaces. We propose to tackle the planning task by decomposing the problem into two phases: First, we apply supervised learning for predicting the near future based on the present. We require that the predictor will be differentiable with respect to the representation of the present. Second, we model a full trajectory of the agent using a recurrent neural network, where unexplained factors are modeled as (additive) input nodes. This allows us to solve the long-term planning problem using supervised learning techniques and direct optimization over the recurrent neural network. Our approach enables us to learn robust policies by incorporating adversarial elements to the environment."
    }, {
      "heading" : "1 Introduction",
      "text" : "Two of the most crucial elements of autonomous driving systems are sensing and planning. Sensing deals with finding a compact representation of the present state of the environment, while planning deals with deciding on what actions to take so as to optimize future objectives. Supervised machine learning techniques are very useful for solving sensing problems. In this paper we describe a machine learning algorithmic framework for the planning part. Traditionally, machine learning approaches for planning are studied under the framework of Reinforcement Learning (RL) — see [4, 10, 22, 23] for a general overview and [12] for a comprehensive review of reinforcement learning in robotics.\nTypically, RL is performed in a sequence of consecutive rounds. At round t, the planner (a.k.a. the agent) observes a state, st ∈ S, which represents the agent as well as the environment. It then should decide on an action at ∈ A. After performing the action, the agent receives an immediate reward, rt ∈ R, and is moved to a new state, st+1. As a simple example, consider an adaptive cruise control (ACC) system, in which a self driving vehicle should implement acceleration/braking so as to keep an adequate distance to a preceding vehicle while maintaining smooth driving. We can model the state as a pair, st = (xt, vt) ∈ R2, where xt is the distance to the preceding vehicle and vt is the velocity of the car relative to the velocity of the preceding vehicle. The action at ∈ R will be the acceleration command (where the car slows down if at < 0). The reward can be some function that depends on |at| (reflecting the smoothness of driving) and on st (reflecting that we keep a safe distance from the preceding vehicle). The goal of the planner is to maximize the cumulative reward (maybe up to a time horizon or a discounted sum of future rewards). To do so, the planner relies on a policy, π : S → A, which maps a state into an action.\nSupervised Learning (SL) can be viewed as a special case of RL, in which st is sampled i.i.d. from some distribution over S and the reward function has the form rt = −`(at, yt), where ` is some loss function, and the learner observes the value of yt which is the (possibly noisy) value of the optimal action to take when viewing the state st.\nThere are several key differences between the fully general RL model and the specific case of SL. These differences makes the general RL problem much harder.\n1. In SL, the actions (or predictions) taken by the learner have no effect on the environment. In particular, st+1 and at are independent. This has two important implications:\nar X\niv :1\n60 2.\n01 58\n0v 1\n[ cs\n.L G\n] 4\nF eb\n2 01\n6\n• In SL we can collect a sample (s1, y1), . . . , (sm, ym) in advance, and only then search for a policy (or predictor) that will have good accuracy on the sample. In contrast, in RL, the state st+1 usually depends on the action (and also on the previous state), which in turn depends on the policy used to generate the action. This ties the data generation process to the policy learning process.\n• Because actions do not effect the environment in SL, the contribution of the choice of at to the performance of π is local, namely, at only affects the value of the immediate reward. In contrast, in RL, actions that are taken at round t might have a long-term effect on the reward values in future rounds.\n2. In SL, the knowledge of the “correct” answer, yt, together with the shape of the reward, rt = −`(at, yt), gives us a full knowledge of the reward for all possible choices of at. Furthermore, this often enables us to calculate the derivative of the reward with respect to at. In contrast, in RL, we only observe a “one-shot” value of the reward for the specific choice of action we took. This is often called a “bandit” feedback. It is one of the main reasons for the need of “exploration”, because if we only get to see a “bandit” feedback, we do not always know if the action we took is the best one.\nBefore explaining our approach for tackling these difficulties, we briefly describe the key idea behind most common reinforcement learning algorithms. Most of the algorithms rely in some way or another on the mathematically elegant model of a Markov Decision Process (MDP), pioneered by the work of Bellman [2, 3]. The Markovian assumption is that the distribution of st+1 is fully determined given st and at. This yields a closed form expression for the cumulative reward of a given policy in terms of the stationary distribution over states of the MDP. The stationary distribution of a policy can be expressed as a solution to a linear programming problem. This yields two families of algorithms: optimizing with respect to the primal problem, which is called policy search, and optimizing with respect to the dual problem, whose variables are called the value function, V π . The value function determines the expected cumulative reward if we start the MDP from the initial state s, and from there on pick actions according to π. A related quantity is the state-action value function, Qπ(s, a), which determines the cumulative reward if we start from state s, immediately pick action a, and from there on pick actions according to π. The Q function gives rise to a crisp characterization of the optimal policy (using the so called Bellman’s equation), and in particular it shows that the optimal policy is a deterministic function from S to A (in fact, it is the greedy policy with respect to the optimal Q function).\nIn a sense, the key advantage of the MDP model is that it allows us to couple all the future into the present using the Q function. That is, given that we are now in state s, the value of Qπ(s, a) tells us the effect of performing action a at the moment on the entire future. Therefore, the Q function gives us a local measure of the quality of an action a, thus making the RL problem more similar to SL.\nMost reinforcement learning algorithms approximate the V function or theQ function in one way or another. Value iteration algorithms, e.g. the Q learning algorithm [26], relies on the fact that the V and Q functions of the optimal policy are fixed points of some operators derived from Bellman’s equation. Actor-critic policy iteration algorithms aim to learn a policy in an iterative way, where at iteration t, the “critic” estimates Qπt and based on this, the “actor” improves the policy.\nDespite the mathematical elegancy of MDPs and the conveniency of switching to the Q function representation, there are several limitations of this approach. First, as noted in [12], usually in robotics, we may only be able to find some approximate notion of a Markovian behaving state. Furthermore, the transition of states depends not only on the agent’s action, but also on actions of other players in the environment. For example, in the ACC example mentioned previously, while the dynamic of the autonomous vehicle is clearly Markovian, the next state depends on the behavior of the other driver, which is not necessarily Markovian. One possible solution to this problem is to use partially observed MDPs [27], in which we still assume that there is a Markovian state, but we only get to see an observation that is distributed according to the hidden state. A more direct approach considers game theoretical generalizations of MDPs, for example the Stochastic Games framework. Indeed, some of the algorithms for MDPs were generalized to multi-agents games. For example, the minimax-Q learning [14] or the Nash-Q learning [9]. Other approaches to Stochastic Games are explicit modeling of the other players, that goes back to Brown’s fictitious play [6], and vanishing regret learning algorithms [8, 7]. See also [25, 24, 11, 5]. As noted in [20], learning in multi-agent setting is inherently more complex than in the single agent setting.\nA second limitation of the Q function representation arises when we depart from a tabular setting. The tabular\nsetting is when the number of states and actions is small, and therefore we can express Q as a table with |S| rows and |A| columns. However, if the natural representation of S andA is as Euclidean spaces, and we try to discretize the state and action spaces, we obtain that the number of states/actions is exponential in the dimension. In such cases, it is not practical to employ the tabular setting. Instead, the Q function is approximated by some function from a parametric hypothesis class (e.g. neural networks of a certain architecture). For example, the deep-Q-networks (DQN) learning algorithm of [16] has been successful at playing Atari games. In DQN, the state space can be continuous but the action space is still a small discrete set. There are approaches for dealing with continuous action spaces (e.g. [21]), but they again rely on approximating the Q function. In any case, the Q function is usually very complicated and sensitive to noise, and it is therefore quite hard to learn it. Indeed, it was observed that value based methods rarely work out-ofthe-box in robotic applications [12], and that the best performing methods rely on a lot of prior knowledge and reward shaping [13, 17]. Intuitively, the difficulty in learning Q is that we need to implicitly understand the dynamics of the underlying Markov process.\nIn the autonomous driving domain we tackle in this paper, the multi-agent adversarial environment leads to nonMarkovianity of the natural state representation. Moreover, the natural state and action representations are continuous in nature. Taken together, we found out thatQ-based learning approaches rarely work out-of-the-box, and require long training time and advanced reward shaping.\nA radically different approach has been introduced by Schmidhuber [19], who tackled the RL problem using a recurrent neural network (RNN). Following [19], there have been several additional algorithms that rely on RNNs for RL problems. For example, Backer [1] proposed to tackle the RL problem using recurrent networks with the LSTM architecture. His approach still relies on the value function. Schäfer [18] used RNN to model the dynamics of partially observed MDPs. Again, he still relies on explicitly modeling the Markovian structure. There have been few other approaches to tackle the RL problem without relying on value functions. Most notably is the REINFORCE framework of Williams [28]. It has been recently successful for visual attention [15, 29]. As already noted by [19], the ability of REINFORCE to estimate the derivative of stochastic units can be straightforwardly combined within the RNN framework.\nIn this paper we combine Schmidhuber’s approach, of tackling the policy learning problem directly using a RNN, with the notions of multi-agents games and robustness to adversarial environments from the game theory literature. Furthermore, we do not explicitly rely on any Markovian assumption. Our approach is described in the next section."
    }, {
      "heading" : "2 Planning by Prediction",
      "text" : "Throughout, we assume that the state space, S, is some subset of Rd, and the action space, A, is some subset of Rk. This is the most natural representation in many applications, and in particular, the ones we describe in Section 3.\nOur goal is to learn a policy π : S → A. As is standard in machine learning, we bias ourselves to pick a policy function π from a hypothesis classH. Namely,H is a predefined set of policy functions from which we should pick the best performing one. In order to learn π using the SL framework, one would need a training set of pairs (state,optimalaction). We of course do not have such a training set. Instead, we only have an access to a “simulator”, that can be used to assess the quality of π. Formally, fixing a horizon T , any policy π induces a distribution over RT , such that the probability of (r1, . . . , rT ) ∈ RT is the probability to apply our simulator for T steps, while on step t we observe st, feed the simulator with the action at = π(st), and observe the reward rt. Denote by B the random bits used by the simulator, we note that we can write the vector r = (r1, . . . , rT ) of rewards as a deterministic function R(B, π). We use Rt(B, π) to denote the t’th element of R(B, π). We can now formulate the problem of learning the policy π as the following optimization problem:\nmax π∈H E B [ T∑ t=1 Rt(B, π) ] , (1)\nwhere the expectation is over the distribution over B. We assume that the hypothesis class,H, is the set of deep neural networks (DNN) of a certain predefined architecture, and therefore every π ∈ H is parametrized by a vector of weights, θ. We use πθ to denote the policy associated with the vector θ.\nIf we could expressR(B, πθ) as a differential function of θ, we could have utilized the Stochastic Gradient Descent (SGD) approach for maximizing (1). That is, starting with an initial θ, at each iteration of SGD we first sample B, then we calculate the gradient of ∑T t=1Rt(B, πθ) with respect to θ, and finally we update θ based on this gradient.\nOur key observation is that by solving two SL problems, described below, we can approximate R(B, πθ) by a differential function of θ. Hence, we can implement SGD for learning πθ directly.\nThe goal of the first SL problem is to learn a deep neural network (DNN), that represents the mapping from a (state,action) pair into the immediate reward value. We denote this DNN by DNNr and it is formally described as a function DNNr : S × A → R. We shall later explain how to learn DNNr using SL, but for now lets just assume that we can do it and have the network DNNr such that DNNr(st, at) ≈ rt. The goal of the second SL problem is to learn a DNN that represents the mapping from (state,action) into the next state. Formally, this DNN is the function DNNN : S × A → S, and for now lets assume we managed to learn DNNN in a supervised manner such that DNNN (st, at) ≈ st+1.\nEquipped with DNNr and DNNN we can describe the process of generating a randomB and calculatingR(B, πθ) as follows. Initially, the simulator picks a seed for its pseudo random number generator and then it determines the initial state s1. At round t, the agent receives st from the simulator and applies πθ to generate the action at = πθ(st). The simulator receives at and generates rt and st+1. At the same time, the agent applies DNNr to generate r̂t = DNNr(st) and applies DNNN to generate ŝt+1 = DNNN (st). Let us denote νt+1 = st+1 − ŝt+1. Therefore, if the simulator receives ŝt+1 it can generate νt+1 and send it to the agent.\nA single round of this process is depicted below. The entire process is obtained by repeating the shaded part of the picture T times. Solid arrows represent differentiable propagation of information while dashed arrows represent non-differentiable propagation of information.\nst\nπθ\nat\nDNNN\nDNNr\nr̂t\nŝt+1 +\nνt+1\nst+1\nSimulatort+1Simulatort Simulatort+2\nRecall that we assume that r̂t ≈ rt and ŝt+1 ≈ st+1. If these approximations are exact, then the dashed arrows can be eliminated from the figure above and the entire process of generating the rewards becomes a differentiable recurrent neural network. In such case, we can solve (1) using the SGD framework, and at each iteration we calculate the gradient by backpropagation in time over the recurrent neural network.\nIn most situations, we expect r̂t and ŝt+1 to slightly deviate from rt and st+1. The deviation of r̂t from rt is less of an issue in practice, because it is often the case that there is nothing special about the exact reward rt, and maximizing an approximation of it leads to similar performance. Therefore, for the sake of simplicity, we assume that maximizing the sum of r̂t is sufficiently good.\nThe more problematic part is the deviation of ŝt+1 from st+1. There are several possible sources for this deviation.\n1. Non-determinism: in the traditional MDP model, st+1 is a random variable whose distribution is a function of (st, at). But, the actual value of st+1 is not necessarily a deterministic function of (st, at).\n2. Non-Markovianity: it may be the case that the process is not Markovian in the state representation. It will always be Markovian in another representation, that is known to the simulator, but we do not know the Markovian representation or we do not want to model it. For example, in the ACC problem given in the next section, st+1 depends on the acceleration commands of the driver in front of us. While the simulator models this behavior in some complicated way, we do not want to model it and prefer to stick with a simple state representation that does not allow us to predict the acceleration of the other driver, but only allows us to react to the other driver’s behavior.\n3. Failures of the learning process: as we will show, we are going to learn DNNN from examples, and we may suffer from the usual inaccuracies of learning algorithms (approximation error, estimation error, and optimization error). As this part is standard we ignore this issue and assume that we have managed to learn DNNN sufficiently good.\nIn any case, despite the fact that ŝt+1 can deviate from st+1, we can still apply backpropagation in time in order to calculate an approximate gradient of the cumulative reward w.r.t. π. In particular, the forward part of the backpropagation is correct, due to the correction made by defining st+1 as a sum of the prediction ŝt+1 and the correction term νt+1 (supplied by the simulator during the forward pass). In the backward part, we propagate the error through the solid arrows given in the figure, but we kill the messages that go through dashed arrows, because we refer to the simulator as a black box. As mentioned previously, we do not impose explicit probabilistic assumptions on νt. In particular, we do not require Markovian relation. Instead, we rely on the recurrent network to propagate “enough” information between past and future through the solid arrows. Intuitively, DNNN (st, at) describes the predictable part of the near future, while νt expresses the unpredictable aspects, mainly due to the behavior of other players in the environment. The learner should learn a policy that will be robust to the behavior of other players. Naturally, if ‖νt‖ is large, the connection between our past actions and future reward values will be too noisy for learning a meaningful policy.\nAs noted in [19], explicitly expressing the dynamic of the system in a transparent way enables to incorporate prior knowledge more easily. For example, in Section 3 we demonstrate how prior knowledge greatly simplifies the problem of defining DNNN .\nFinally, it is left to explain how we can learn DNNr and DNNN within the SL framework. For this, we observe that by relying on the access to the simulator, we have the ability to generate tuples (s, a, r, s′) as training examples, where s is the current state, a is the action, r is the reward, and s′ is the next state. We note that it is customary to use some elements of exploration in generating the training set. Since this is a standard technique, we omit the details. Equipped with such training examples, we can learn DNNr in the SL framework by extracting examples of the form ((s, a), r) from each tuple (s, a, r, s′). The key point here is that even though the action a is not necessarily optimal for s, it does not pose any problem for the task of learning the mapping from state-action into the correct reward. Furthermore, even though the reward is given in a “bandit” manner for the policy learning problem, it forms a “full information” feedback for the problem of learning a network DNNr, such that DNNr(st, at) ≈ rt. Likewise, we can learn DNNN in the SL framework by extracting examples of the form ((s, a), s′) from each tuple (s, a, r, s′). Again, the fact that a is not the optimal action for s does not pose any problem for the task of learning the near future, s′, from the current state and action, (s, a).\nIt is also possible to simultaneously learn DNNr,DNNN , and πθ, by defining an objective that combines the cumulative reward with supervised losses of the form ‖ŝt+1 − st+1‖2 and (r̂t+1 − rt)2. In the experimental section we report results for both separate and join training."
    }, {
      "heading" : "2.1 Robustness to Adversarial Environment",
      "text" : "Since our model does not impose probabilistic assumptions on νt, we can consider environments in which νt is being chosen in an adversarial manner. Of course, we must make some restrictions on νt, otherwise the adversary can make the planning problem impossible. A natural restriction is to require that ‖νt‖ is bounded by a constant. Robustness against adversarial environment is quite useful in autonomous driving applications. We describe a real world aspect of adversarial environment in Section 3.\nHere, we show that choosing νt in an adversarial way might even speed up the learning process, as it can focus the learner toward the robust optimal policy. We consider the following simple game. The state is st ∈ R, the action is at ∈ R, and the immediate loss function is 0.1|at|+ [|st| − 2]+, where [x]+ = max{x, 0} is the ReLU function. The next state is st+1 = st + at + νt, where νt ∈ [−0.5, 0.5] is chosen by the environment in an adversarial manner.\nIt is possible to see that the optimal policy can be written as a two layer network with ReLU: at = −[st− 1.5]+ + [−st − 1.5]+. Observe that when |st| ∈ (1.5, 2], the optimal action has a larger immediate loss than the action a = 0. Therefore, the learner must plan for the future and cannot rely solely on the immediate loss.\nObserve that the derivative of the loss w.r.t. at is 0.1 sign(at) and the derivative w.r.t. st is 1[|st| > 2] sign(st). Suppose we are in a situation in which st ∈ (1.5, 2]. The adversarial choice of νt would be to set νt = 0.5, and therefore, we will have a non-zero loss on round t + 1, whenever at > 1.5 − st. In all such cases, the derivative of the loss will back-propagate directly to at. We therefore see that the adversarial choice of νt helps the learner to get a non-zero back-propagation message in all cases for which the choice of at is sub-optimal."
    }, {
      "heading" : "3 Example Applications",
      "text" : "The goal of this section is to demonstrate some aspects of our approach on two toy examples: adaptive cruise control (ACC) and merging into a roundabout."
    }, {
      "heading" : "3.1 The ACC Problem",
      "text" : "In the ACC problem, a host vehicle is trying to keep an adequate distance of 1.5 seconds to a target car, while driving as smooth as possible. We provide a simple model for this problem as follows. The state space is R3 and the action space is R. The first coordinate of the state is the speed of the target car, the second coordinate is the speed of the host car, and the last coordinate is the distance between host and target (namely, location of the host minus location of the target on the road curve). The action to be taken by the host is the acceleration, and is denoted by at. We denote by τ the difference in time between consecutive rounds (in the experiment we set τ to be 0.1 seconds).\nDenote st = (v target t , v host t , xt) and denote by a target t the (unknown) acceleration of the target. The full dynamics of\nthe system can be described by:\nvtargett = [v target t−1 + τ a target t−1 ]+\nvhostt = [v host t−1 + τ at−1]+\nxt = [xt−1 + τ (v target t−1 − vhostt−1)]+\nThis can be described as a sum of two vectors:\nst = ([st−1[0] + τa target t−1 ]+, [st−1[1] + τat−1]+, [st−1[2] + τ(st−1[0]− st−1[1])]+)\n= (st−1[0], [st−1[1] + τat−1]+, [st−1[2] + τ(st−1[0]− st−1[1])]+)︸ ︷︷ ︸ DNNN (st−1,at) +([st−1[0] + τa target t−1 ]+ − st−1[0], 0, 0)︸ ︷︷ ︸ νt\nThe first vector is the predictable part and the second vector is the unpredictable part.\nThe reward on round t is defined as follows:\n−rt = 0.1 |at| + [|xt/x∗t − 1| − 0.3]+ where x∗t = max{1, 1.5 vhostt }\nThe first term above penalizes for any non-zero acceleration, thus encourages smooth driving. The second term depends on the ratio between the distance to the target car, xt, and the desired distance, x∗t , which is defined as the maximum between a distance of 1 meter and brake distance of 1.5 seconds. Ideally, we would like this ratio to be exactly 1, but as long as this ratio is in [0.7, 1.3] we do not penalize the policy, thus allowing the car some slack (which is important for maintaining a smooth drive)."
    }, {
      "heading" : "3.2 Merging into a Roundabout",
      "text" : "In this experiment, the goal of the agent is to pass a roundabout. An episode starts when the agent is approaching the bottom entrance of the roundabout. The episode ends when the agent reaches the second exit of the roundabout, or after a fixed number of steps. A successful episode is measured first by keeping a safety distance from all other vehicles in the roundabout at all times. Second, the agent should finish the route as quickly as possible. And third, it should adhere a smooth acceleration policy. At the beginning of the episode, we randomly place NT target vehicles on the roundabout.\nTo model a blend of adversarial and typical behavior, with probability p, a target vehicle is modeled by an “aggressive” driving policy, that accelerates when the host tries to merge in front of it. With probability 1 − p, the target vehicle is modeled by a “defensive” driving policy that deaccelerate and let the host merge in. In our experiments we set p = 0.5. The agent has no information about the type of the other drivers. These are chosen at random at the beginning of the episode.\nWe represent the state as the velocity and location of the host (the agent), and the locations, velocities, and accelerations of the target vehicles. Maintaining target accelerations is vital in order to differentiate between aggressive and defensive drivers based on the current state. All target vehicles move on a one-dimensional curve that outlines the roundabout path. The host vehicle moves on its own one-dimensional curve, which intersects the targets’ curve at the merging point, and this point is the origin of both curves. To model reasonable driving, the absolute value of all vehicles’ accelerations are upper bounded by a constant. Velocities are also passed through a ReLU because driving backward is not allowed. Note that by not allowing driving backward we make long-term planning a necessity (the agent cannot regret on its past action).\nRecall that we decompose the next state, st+1, into a sum of a predictable part, DNNN (st, at), and a nonpredictable part, νt+1. In our first experiment, we let DNNN (st, at) be the dynamics of locations and velocities of all vehicles (which are well defined in a differentiable manner), while νt+1 is the targets’ acceleration. It is easy to\nverify that DNNN (st, at) can be expressed as a combination of ReLU functions over an affine transformation, hence it is differentiable with respect to st and at. The vector νt+1 is defined by a simulator in a non-differentiable manner, and in particular implement aggressive behavior for some targets and defensive behavior for other targets. Two frames from the simulator are show in Figure 1. As can be seen in the supplementary videos1, the agent learns to slowdown as it approaches the entrance of the roundabout. It also perfectly learned to give way to aggressive drivers, and to safely continue when merging in front of defensive ones.\nOur second experiment is more ambitious: we do not tell the network the function DNNN (st, at). Instead, we express DNNN as another learnable part of our recurrent network. Besides the rewards for the policy part, we add a loss term of the form ‖DNNN (st, at) − st+1‖2, where st+1 is the actual next state as obtained by the simulator. That is, we learn the prediction of the near future, DNNN , and the policy that plan for the long term, πθ, concurrently. While this learning task is more challenging, as can be seen in the supplementary videos, the learning process still succeeds."
    }, {
      "heading" : "4 Discussion",
      "text" : "We have presented an approach for learning driving polices in the presence of other adversarial cars using recurrent neural networks. Our approach relies on partitioning of the near future into a predictable part and an un-predictable part. We demonstrated the effectiveness of the learning procedure for two simple tasks: adaptive cruise control and roundabout merging. The described technique can be adapted to learning driving policies in other scenarios, such as lane change decisions, highway exit and merge, negotiation of the right of way in junctions, yielding for pedestrians, as well as complicated planning in urban scenarios."
    } ],
    "references" : [ {
      "title" : "Reinforcement learning with long short-term memory",
      "author" : [ "Bram Bakker" ],
      "venue" : "In NIPS, pages 1475–1482,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2001
    }, {
      "title" : "Dynamic programming and lagrange multipliers",
      "author" : [ "Richard Bellman" ],
      "venue" : "Proceedings of the National Academy of Sciences of the United States of America,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 1956
    }, {
      "title" : "Introduction to the mathematical theory of control processes, volume",
      "author" : [ "Richard Bellman" ],
      "venue" : "IMA,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 1971
    }, {
      "title" : "Dynamic programming and optimal control, volume 1",
      "author" : [ "Dimitri P Bertsekas" ],
      "venue" : "Athena Scientific Belmont, MA,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 1995
    }, {
      "title" : "R-max–a general polynomial time algorithm for near-optimal reinforcement learning",
      "author" : [ "Ronen I Brafman", "Moshe Tennenholtz" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2003
    }, {
      "title" : "Iterative solution of games by fictitious play",
      "author" : [ "George W Brown" ],
      "venue" : "Activity analysis of production and allocation,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 1951
    }, {
      "title" : "Prediction, learning, and games",
      "author" : [ "N. Cesa-Bianchi", "G. Lugosi" ],
      "venue" : null,
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2006
    }, {
      "title" : "A simple adaptive procedure leading to correlated",
      "author" : [ "S. HART", "A. MAS-COLELL" ],
      "venue" : "equilibrium. Econometrica,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2000
    }, {
      "title" : "Nash q-learning for general-sum stochastic games",
      "author" : [ "Junling Hu", "Michael P Wellman" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2003
    }, {
      "title" : "Reinforcement learning: A survey",
      "author" : [ "Leslie Pack Kaelbling", "Michael L Littman", "Andrew W Moore" ],
      "venue" : "Journal of artificial intelligence research,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 1996
    }, {
      "title" : "Near-optimal reinforcement learning in polynomial time",
      "author" : [ "Michael Kearns", "Satinder Singh" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2002
    }, {
      "title" : "Reinforcement learning in robotics: A survey",
      "author" : [ "Jens Kober", "J Andrew Bagnell", "Jan Peters" ],
      "venue" : "The International Journal of Robotics Research,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2013
    }, {
      "title" : "Theory and application of reward shaping in reinforcement learning",
      "author" : [ "Adam Daniel Laud" ],
      "venue" : "PhD thesis, University of Illinois at Urbana-Champaign,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2004
    }, {
      "title" : "Markov games as a framework for multi-agent reinforcement learning",
      "author" : [ "Michael L Littman" ],
      "venue" : "In Proceedings of the eleventh international conference on machine learning,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 1994
    }, {
      "title" : "Recurrent models of visual attention",
      "author" : [ "Volodymyr Mnih", "Nicolas Heess", "Alex Graves" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2014
    }, {
      "title" : "Human-level control through deep reinforcement learning",
      "author" : [ "Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Andrei A Rusu", "Joel Veness", "Marc G Bellemare", "Alex Graves", "Martin Riedmiller", "Andreas K Fidjeland", "Georg Ostrovski" ],
      "venue" : "Nature, 518(7540):529–533,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2015
    }, {
      "title" : "Policy invariance under reward transformations: Theory and application to reward shaping",
      "author" : [ "Andrew Y Ng", "Daishi Harada", "Stuart Russell" ],
      "venue" : "In ICML,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 1999
    }, {
      "title" : "Reinforcement Learning with Recurrent Neural Network",
      "author" : [ "Anton Maximilian Schäfer" ],
      "venue" : "PhD thesis, Universitat Osnabruck,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2008
    }, {
      "title" : "Reinforcement learning in markovian and non-markovian environments",
      "author" : [ "Jürgen Schmidhuber" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 1991
    }, {
      "title" : "If multi-agent learning is the answer, what is the question",
      "author" : [ "Yoav Shoham", "Rob Powers", "Trond Grenager" ],
      "venue" : "Artificial Intelligence,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2007
    }, {
      "title" : "Deterministic policy gradient algorithms",
      "author" : [ "David Silver", "Guy Lever", "Nicolas Heess", "Thomas Degris", "Daan Wierstra", "Martin Riedmiller" ],
      "venue" : "In ICML,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2014
    }, {
      "title" : "Reinforcement learning: An introduction, volume 1",
      "author" : [ "Richard S Sutton", "Andrew G Barto" ],
      "venue" : "MIT press Cambridge,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 1998
    }, {
      "title" : "Algorithms for reinforcement learning",
      "author" : [ "Csaba Szepesvári" ],
      "venue" : "Synthesis Lectures on Artificial Intelligence and Machine Learning,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2010
    }, {
      "title" : "Learning to play the game of chess",
      "author" : [ "S. Thrun" ],
      "venue" : "Advances in Neural Information Processing Systems",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 1995
    }, {
      "title" : "Adversarial reinforcement learning",
      "author" : [ "William Uther", "Manuela Veloso" ],
      "venue" : "Technical report,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 1997
    }, {
      "title" : "A survey of solution techniques for the partially observed markov decision process",
      "author" : [ "Chelsea C White III" ],
      "venue" : "Annals of Operations Research,",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 1991
    }, {
      "title" : "Simple statistical gradient-following algorithms for connectionist reinforcement learning",
      "author" : [ "Ronald J Williams" ],
      "venue" : "Machine learning,",
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 1992
    }, {
      "title" : "Show, attend and tell: Neural image caption generation with visual attention",
      "author" : [ "Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Aaron Courville", "Ruslan Salakhutdinov", "Richard Zemel", "Yoshua Bengio" ],
      "venue" : "arXiv preprint arXiv:1502.03044,",
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 3,
      "context" : "Traditionally, machine learning approaches for planning are studied under the framework of Reinforcement Learning (RL) — see [4, 10, 22, 23] for a general overview and [12] for a comprehensive review of reinforcement learning in robotics.",
      "startOffset" : 125,
      "endOffset" : 140
    }, {
      "referenceID" : 9,
      "context" : "Traditionally, machine learning approaches for planning are studied under the framework of Reinforcement Learning (RL) — see [4, 10, 22, 23] for a general overview and [12] for a comprehensive review of reinforcement learning in robotics.",
      "startOffset" : 125,
      "endOffset" : 140
    }, {
      "referenceID" : 21,
      "context" : "Traditionally, machine learning approaches for planning are studied under the framework of Reinforcement Learning (RL) — see [4, 10, 22, 23] for a general overview and [12] for a comprehensive review of reinforcement learning in robotics.",
      "startOffset" : 125,
      "endOffset" : 140
    }, {
      "referenceID" : 22,
      "context" : "Traditionally, machine learning approaches for planning are studied under the framework of Reinforcement Learning (RL) — see [4, 10, 22, 23] for a general overview and [12] for a comprehensive review of reinforcement learning in robotics.",
      "startOffset" : 125,
      "endOffset" : 140
    }, {
      "referenceID" : 11,
      "context" : "Traditionally, machine learning approaches for planning are studied under the framework of Reinforcement Learning (RL) — see [4, 10, 22, 23] for a general overview and [12] for a comprehensive review of reinforcement learning in robotics.",
      "startOffset" : 168,
      "endOffset" : 172
    }, {
      "referenceID" : 1,
      "context" : "Most of the algorithms rely in some way or another on the mathematically elegant model of a Markov Decision Process (MDP), pioneered by the work of Bellman [2, 3].",
      "startOffset" : 156,
      "endOffset" : 162
    }, {
      "referenceID" : 2,
      "context" : "Most of the algorithms rely in some way or another on the mathematically elegant model of a Markov Decision Process (MDP), pioneered by the work of Bellman [2, 3].",
      "startOffset" : 156,
      "endOffset" : 162
    }, {
      "referenceID" : 11,
      "context" : "First, as noted in [12], usually in robotics, we may only be able to find some approximate notion of a Markovian behaving state.",
      "startOffset" : 19,
      "endOffset" : 23
    }, {
      "referenceID" : 25,
      "context" : "One possible solution to this problem is to use partially observed MDPs [27], in which we still assume that there is a Markovian state, but we only get to see an observation that is distributed according to the hidden state.",
      "startOffset" : 72,
      "endOffset" : 76
    }, {
      "referenceID" : 13,
      "context" : "For example, the minimax-Q learning [14] or the Nash-Q learning [9].",
      "startOffset" : 36,
      "endOffset" : 40
    }, {
      "referenceID" : 8,
      "context" : "For example, the minimax-Q learning [14] or the Nash-Q learning [9].",
      "startOffset" : 64,
      "endOffset" : 67
    }, {
      "referenceID" : 5,
      "context" : "Other approaches to Stochastic Games are explicit modeling of the other players, that goes back to Brown’s fictitious play [6], and vanishing regret learning algorithms [8, 7].",
      "startOffset" : 123,
      "endOffset" : 126
    }, {
      "referenceID" : 7,
      "context" : "Other approaches to Stochastic Games are explicit modeling of the other players, that goes back to Brown’s fictitious play [6], and vanishing regret learning algorithms [8, 7].",
      "startOffset" : 169,
      "endOffset" : 175
    }, {
      "referenceID" : 6,
      "context" : "Other approaches to Stochastic Games are explicit modeling of the other players, that goes back to Brown’s fictitious play [6], and vanishing regret learning algorithms [8, 7].",
      "startOffset" : 169,
      "endOffset" : 175
    }, {
      "referenceID" : 24,
      "context" : "See also [25, 24, 11, 5].",
      "startOffset" : 9,
      "endOffset" : 24
    }, {
      "referenceID" : 23,
      "context" : "See also [25, 24, 11, 5].",
      "startOffset" : 9,
      "endOffset" : 24
    }, {
      "referenceID" : 10,
      "context" : "See also [25, 24, 11, 5].",
      "startOffset" : 9,
      "endOffset" : 24
    }, {
      "referenceID" : 4,
      "context" : "See also [25, 24, 11, 5].",
      "startOffset" : 9,
      "endOffset" : 24
    }, {
      "referenceID" : 19,
      "context" : "As noted in [20], learning in multi-agent setting is inherently more complex than in the single agent setting.",
      "startOffset" : 12,
      "endOffset" : 16
    }, {
      "referenceID" : 15,
      "context" : "For example, the deep-Q-networks (DQN) learning algorithm of [16] has been successful at playing Atari games.",
      "startOffset" : 61,
      "endOffset" : 65
    }, {
      "referenceID" : 20,
      "context" : "[21]), but they again rely on approximating the Q function.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 11,
      "context" : "Indeed, it was observed that value based methods rarely work out-ofthe-box in robotic applications [12], and that the best performing methods rely on a lot of prior knowledge and reward shaping [13, 17].",
      "startOffset" : 99,
      "endOffset" : 103
    }, {
      "referenceID" : 12,
      "context" : "Indeed, it was observed that value based methods rarely work out-ofthe-box in robotic applications [12], and that the best performing methods rely on a lot of prior knowledge and reward shaping [13, 17].",
      "startOffset" : 194,
      "endOffset" : 202
    }, {
      "referenceID" : 16,
      "context" : "Indeed, it was observed that value based methods rarely work out-ofthe-box in robotic applications [12], and that the best performing methods rely on a lot of prior knowledge and reward shaping [13, 17].",
      "startOffset" : 194,
      "endOffset" : 202
    }, {
      "referenceID" : 18,
      "context" : "A radically different approach has been introduced by Schmidhuber [19], who tackled the RL problem using a recurrent neural network (RNN).",
      "startOffset" : 66,
      "endOffset" : 70
    }, {
      "referenceID" : 18,
      "context" : "Following [19], there have been several additional algorithms that rely on RNNs for RL problems.",
      "startOffset" : 10,
      "endOffset" : 14
    }, {
      "referenceID" : 0,
      "context" : "For example, Backer [1] proposed to tackle the RL problem using recurrent networks with the LSTM architecture.",
      "startOffset" : 20,
      "endOffset" : 23
    }, {
      "referenceID" : 17,
      "context" : "Schäfer [18] used RNN to model the dynamics of partially observed MDPs.",
      "startOffset" : 8,
      "endOffset" : 12
    }, {
      "referenceID" : 26,
      "context" : "Most notably is the REINFORCE framework of Williams [28].",
      "startOffset" : 52,
      "endOffset" : 56
    }, {
      "referenceID" : 14,
      "context" : "It has been recently successful for visual attention [15, 29].",
      "startOffset" : 53,
      "endOffset" : 61
    }, {
      "referenceID" : 27,
      "context" : "It has been recently successful for visual attention [15, 29].",
      "startOffset" : 53,
      "endOffset" : 61
    }, {
      "referenceID" : 18,
      "context" : "As already noted by [19], the ability of REINFORCE to estimate the derivative of stochastic units can be straightforwardly combined within the RNN framework.",
      "startOffset" : 20,
      "endOffset" : 24
    }, {
      "referenceID" : 18,
      "context" : "As noted in [19], explicitly expressing the dynamic of the system in a transparent way enables to incorporate prior knowledge more easily.",
      "startOffset" : 12,
      "endOffset" : 16
    }, {
      "referenceID" : 0,
      "context" : "st = ([st−1[0] + τa target t−1 ]+, [st−1[1] + τat−1]+, [st−1[2] + τ(st−1[0]− st−1[1])]+)",
      "startOffset" : 40,
      "endOffset" : 43
    }, {
      "referenceID" : 1,
      "context" : "st = ([st−1[0] + τa target t−1 ]+, [st−1[1] + τat−1]+, [st−1[2] + τ(st−1[0]− st−1[1])]+)",
      "startOffset" : 60,
      "endOffset" : 63
    }, {
      "referenceID" : 0,
      "context" : "st = ([st−1[0] + τa target t−1 ]+, [st−1[1] + τat−1]+, [st−1[2] + τ(st−1[0]− st−1[1])]+)",
      "startOffset" : 81,
      "endOffset" : 84
    }, {
      "referenceID" : 0,
      "context" : "= (st−1[0], [st−1[1] + τat−1]+, [st−1[2] + τ(st−1[0]− st−1[1])]+) } {{ } DNNN (st−1,at) +([st−1[0] + τa target t−1 ]+ − st−1[0], 0, 0) } {{ } νt",
      "startOffset" : 17,
      "endOffset" : 20
    }, {
      "referenceID" : 1,
      "context" : "= (st−1[0], [st−1[1] + τat−1]+, [st−1[2] + τ(st−1[0]− st−1[1])]+) } {{ } DNNN (st−1,at) +([st−1[0] + τa target t−1 ]+ − st−1[0], 0, 0) } {{ } νt",
      "startOffset" : 37,
      "endOffset" : 40
    }, {
      "referenceID" : 0,
      "context" : "= (st−1[0], [st−1[1] + τat−1]+, [st−1[2] + τ(st−1[0]− st−1[1])]+) } {{ } DNNN (st−1,at) +([st−1[0] + τa target t−1 ]+ − st−1[0], 0, 0) } {{ } νt",
      "startOffset" : 58,
      "endOffset" : 61
    } ],
    "year" : 2016,
    "abstractText" : "We consider planning problems, that often arise in autonomous driving applications, in which an agent should decide on immediate actions so as to optimize a long term objective. For example, when a car tries to merge in a roundabout it should decide on an immediate acceleration/braking command, while the long term effect of the command is the success/failure of the merge. Such problems are characterized by continuous state and action spaces, and by interaction with multiple agents, whose behavior can be adversarial. We argue that dual versions of the MDP framework (that depend on the value function and the Q function) are problematic for autonomous driving applications due to the non Markovian of the natural state space representation, and due to the continuous state and action spaces. We propose to tackle the planning task by decomposing the problem into two phases: First, we apply supervised learning for predicting the near future based on the present. We require that the predictor will be differentiable with respect to the representation of the present. Second, we model a full trajectory of the agent using a recurrent neural network, where unexplained factors are modeled as (additive) input nodes. This allows us to solve the long-term planning problem using supervised learning techniques and direct optimization over the recurrent neural network. Our approach enables us to learn robust policies by incorporating adversarial elements to the environment.",
    "creator" : "LaTeX with hyperref package"
  }
}