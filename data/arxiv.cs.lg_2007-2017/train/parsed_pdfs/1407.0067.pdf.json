{
  "name" : "1407.0067.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Rates of Convergence for Nearest Neighbor Classification",
    "authors" : [ "Kamalika Chaudhuri", "Sanjoy Dasgupta" ],
    "emails" : [ "kamalika@cs.ucsd.edu", "dasgupta@cs.ucsd.edu" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "In this paper, we deal with binary prediction in metric spaces. A classification problem is defined by a metric space (X , ρ) from which instances are drawn, a space of possible labels Y = {0, 1}, and a distribution P overX ×Y . The goal is to find a function h : X → Y that minimizes the probability of error on pairs (X,Y ) drawn from P; this error rate is the risk R(h) = P(h(X) 6= Y ). The best such function is easy to specify: if we let µ denote the marginal distribution of X and η the conditional probability η(x) = P(Y = 1|X = x), then the predictor 1(η(x) ≥ 1/2) achieves the minimum possible risk, R∗ = EX [min(η(X), 1 − η(X))]. The trouble is that P is unknown and thus a prediction rule must instead be based only on a finite sample of points (X1, Y1), . . . , (Xn, Yn) drawn independently at random from P.\nNearest neighbor (NN) classifiers are among the simplest prediction rules. The 1-NN classifier assigns each point x ∈ X the label Yi of the closest point in X1, . . . , Xn (breaking ties arbitrarily, say). For a positive integer k, the k-NN classifier assigns x the majority label of the k closest points in X1, . . . , Xn. In the latter case, it is common to let k grow with n, in which case the sequence (kn : n ≥ 1) defines a kn-NN classifier. The asymptotic consistency of nearest neighbor classification has been studied in detail, starting with the work of Fix and Hodges [7]. The risk Rn is a random variable that depends on the training sample (X1, Y1), . . . , (Xn, Yn); the usual order of business is to first determine the limiting behavior of the expected value ERn and to then study stronger modes of convergence of Rn. Cover and Hart [2] studied the asymptotics of ERn in general metric spaces, under the assumption that every x in the support of µ is either a continuity point of η or has µ({x}) > 0. For the 1-NN classifier, they found that ERn → EX [2η(X)(1 − η(X))] ≤ 2R∗(1 − R∗); for kn-NN with kn ↑ ∞ and kn/n ↓ 0, they found\nar X\niv :1\n40 7.\n00 67\nv2 [\ncs .L\nG ]\n2 J\nul 2\n01 4\nERn → R∗. For points in Euclidean space, a series of results starting with Stone [15] established consistency without any distributional assumptions. For kn-NN in particular, Rn → R∗ almost surely [5]. These consistency results place nearest neighbor methods in a favored category of nonparametric estimators. But for a fuller understanding it is important to also have rates of convergence. For instance, part of the beauty of nearest neighbor is that it appears to adapt automatically to different distance scales in different regions of space. It would be helpful to have bounds that encapsulate this property.\nRates of convergence are also important in extending nearest neighbor classification to settings such as active learning, semisupervised learning, and domain adaptation, in which the training data is not a fullylabeled data set obtained by i.i.d. sampling from the future test distribution. For instance, in active learning, the starting point is a set of unlabeled points X1, . . . , Xn, and the learner requests the labels of just a few of these, chosen adaptively to be as informative as possible about η. There are many natural schemes for deciding which points to label: for instance, one could repeatedly pick the point furthest away from the labeled points so far, or one could pick the point whose k nearest labeled neighbors have the largest disagreement among their labels. The asymptotics of such selective sampling schemes has been considered in earlier work [4], but ultimately the choice of scheme must depend upon finite-sample behavior. The starting point for understanding this behavior is to first obtain a characterization in the non-active setting."
    }, {
      "heading" : "1.1 Previous work on rates of convergence",
      "text" : "The earliest rates of convergence for nearest neighbor were distribution-free. Cover [3] studied the 1-NN classifier in the case X = R, under the assumption of class-conditional densities with uniformly-bounded third derivatives. He showed that ERn converges at a rate of O(1/n2). Wagner [18] and later Fritz [8] also looked at 1-NN, but in higher dimension X = Rd. The latter obtained an asymptotic rate of convergence for Rn under the milder assumption of non-atomic µ and lower semi-continuous class-conditional densities.\nDistribution-free results are of some value, but fail to precisely characterize which properties of a distribution most influence the performance of nearest neighbor classification. More recent work has investigated several different approaches to obtaining distribution-dependent bounds. Kulkarni and Posner [12] obtained finitesample rates of convergence for 1-NN and kn-NN in terms of the smoothness of η. They assumed that for some constants K and α, and for all x1, x2 ∈ X , |η(x1)− η(x2)| ≤ Kρ(x1, x2)2α. They then gave bounds in terms of the Holder parameter α. Gyorfi [9] looked at the case X = Rd, under the weaker assumption that for some function K : Rd → R and some α, and for all z ∈ Rd and all r > 0,∣∣∣∣∣η(z)− 1µ(B(z, r)) ∫ B(z,r) η(x)µ(dx)\n∣∣∣∣∣ ≤ K(z)rα. This α is similar in spirit to the earlier Holder parameter, but does not require η to be continuous. Gyorfi obtained asymptotic rates in terms of α. Another generalization of standard smoothness conditions was proposed recently [17] in a “probabilistic Lipschitz” assumption, and in this setting rates were obtained for NN classification in bounded spaces X ⊂ Rd. The convergence rates obtained so far have been unsatisfactory in several regards. The finite-sample rates require continuity and thus, for instance, do not apply to discrete distributions. The use of a single Holder parameter is convenient but does not capture cases where different regions of the data space have different distance-scales: a common situation in which NN methods might be expected to shine. Most importantly, what is crucial for nearest neighbor is not how |η(x) − η(x′)| scales with ρ(x, x′)—which is what any Lipschitz or Holder constant captures—but rather how it scales with µ(B(x, ρ(x, x′))). In other words, a suitable smoothness parameter for NN is one that measures the change in η(x) with respect to probability mass rather than distance. We will try to make this point clearer in the next section."
    }, {
      "heading" : "1.2 Some illustrative examples",
      "text" : "We now look at a few examples to get a sense of what properties of a distribution most critically affect the convergence rate of nearest neighbor. In each case, we study the k-NN classifier.\nAs a first example, consider a finite instance space X . For large enough n, the k nearest neighbors of a query x will all be x itself, leading immediately to an error bound. However, this kind of reasoning yields an asymptotic rate of convergence. To get a finite-sample rate, we proceed more generally and observe that for any number of points n, the k nearest neighbors of x will lie within a ball B = B(x, r) whose probability mass under µ is roughly k/n. The quality of the prediction can be assessed by how much η varies within this ball. To be slightly more precise, let η(B) = (1/µ(B)) ∫ B η(x)µ(dx) denote the average η value within the ball. For the k-NN prediction at x to be good, we require that if η(x) is significantly more than 1/2 then so is η(B); and likewise if η(x) is significantly less than 1/2.\nAs a second example, consider a distribution over X = R in which the two classes (Y = 0 and Y = 1) have class-conditional densities µ1 and µ2, respectively. Assume that these two distributions are supported on disjoint intervals, as shown on the left side of Figure 1. Now let’s determine the probability that the k-NN classifier makes a mistake on a specific query x. Clearly, this will happen only if x is near the boundary between the two classes. To be precise, consider an interval around x of probability mass k/n, that is, an interval B = [x− r, x+ r] with µ(B) = k/n. Then the k nearest neighbors will lie roughly in this interval, and there will likely be an error only if the interval contains a substantial portion of the wrong class. Whether or not η is smooth, or the µi are smooth, is irrelevant.\nIt should already be clear that the central objects in analyzing k-NN are balls of probability mass ≈ k/n, specifically those near the decision boundary. Now let’s see a variant of the previous example (Figure 1, right) in which it is no longer the case that η ∈ {0, 1}. Although one of the class-conditional densities in the figure is highly non-smooth, this erratic behavior occurs far from the decision boundary and thus does not affect nearest neighbor performance. And in the vicinity of the boundary, what matters is not how much η varies within intervals of any given radius r, but rather within intervals of probability mass k/n.\nThese examples hopefully clarify that rates of convergence based only on Holder-continuity of η—or similar notions—are inadequate for properly characterizing the statistical behavior of nearest neighbor classifiers."
    }, {
      "heading" : "1.3 Results of this paper",
      "text" : "Let us return to our earlier setting of pairs (X,Y ), where X takes values in a metric space (X , ρ) and has distribution µ, while Y ∈ {0, 1} has conditional probability function η(x) = Pr(Y = 1|X = x). We obtain rates of convergence for k-NN by attempting to make precise the intuitions discussed above. This leads to a somewhat different style of analysis than has been used in earlier work.\nFor any positive integers k ≤ n, we define a notion of effective boundary for k-NN under sample size n. For the moment, denote this set by An,k ⊂ X .\n• We show that with high probability over the training data, the misclassification rate of the k-NN classifier (with respect to the Bayes-optimal classifer) is bounded above by µ(An,k) plus a small additional term that can be made arbitrarily small.\n• We identify a general condition under which, as n and k grow, An,k approaches the actual decision boundary {x | η(x) = 1/2}. This yields universal consistency in a broader range of metric spaces than just Rd.\n• We give a lower bound on the error probability using a different notion of effective boundary. • We introduce a Holder-like smoothness condition that is tailored to nearest neighbor. We compare\nour upper and lower bounds under this kind of smoothness. • We obtain risk bounds under the margin condition of Tsybakov that match the best known results\nfor nonparametric classification. • We look at additional specific cases of interest: when η is bounded away from 1/2, and the even\nmore extreme scenario where η ∈ {0, 1} (zero Bayes risk)."
    }, {
      "heading" : "2 Definitions and results",
      "text" : "Let (X , ρ) denote a separable metric space, and µ a Borel regular probability measure on this space (that is, open sets are measurable, and every set is contained in a Borel set of the same measure) from which instances X are drawn. The label of an instance X = x is Y ∈ {0, 1} and is distributed according to the conditional probability function η : X → [0, 1] as follows: Pr(Y = 1|X = x) = η(x).\nGiven a training set S = ((X1, Y1), . . . , (Xn, Yn)) and a query point x ∈ X , we use the notation X(i)(x) to denote the i-th nearest neighbor of x in the training set, and Y (i)(x) to denote its label. Distances are calculated with respect to the given metric ρ, and ties are broken by preferring points earlier in the sequence. The k-NN classifier is defined by\ngn,k(x) = { 1 if Y (1)(x) + · · ·+ Y (k)(x) ≥ k/2 0 otherwise\nWe analyze the performance of gn,k by comparing it with g(x) = 1(η(x) ≥ 1/2), the omniscent Bayesoptimal classifier. Specifically, we obtain bounds on PrX(gn,k(X) 6= g(X)) that hold with high probability over the choice of training data S."
    }, {
      "heading" : "2.1 Definitions",
      "text" : "We begin with some definitions and notation.\nThe radius and probability-radius of a ball. For any x ∈ X , let\nBo(x, r) = {x′ ∈ X | ρ(x, x′) < r} and B(x, r) = {x′ ∈ X | ρ(x, x′) ≤ r}\ndenote the open and closed balls, respectively, of radius r centered at x. We will mostly be dealing with balls that contain a prescribed probability mass. To this end, for any x ∈ X and any 0 ≤ p ≤ 1, define\nrp(x) = inf{r | µ(B(x, r)) ≥ p}.\nThus µ(B(x, rp(x))) ≥ p (Lemma 22), and rp(x) is the smallest radius for which this holds.\nThe support of µ. The support of distribution µ is defined as\nsupp(µ) = {x ∈ X | µ(B(x, r)) > 0 for all r > 0}.\nIt was shown by [2] that in separable metric spaces, µ(supp(µ)) = 1. For the interested reader, we reproduce their brief proof in the appendix (Lemma 23).\nThe conditional probability function for a set. The conditional probability function η is defined for points x ∈ X , and can be extended to measurable sets A ⊂ X with µ(A) > 0 as follows:\nη(A) = 1\nµ(A) ∫ A η dµ. (1)\nThis is the probability that Y = 1 for a point X chosen at random from the distribution µ restricted to set A. We exclusively consider sets A of the form B(x, r), in which case η is defined whenever x ∈ supp(µ).\nThe effective interiors of the two classes, and the effective boundary. When asked to make a prediction at point x, the k-NN classifier finds the k nearest neighbors, which can be expected to lie in B(x, rp(x)) for p ≈ k/n. It then takes an average over these k labels, which has a standard deviation of ∆ ≈ 1/ √ k. With this in mind, there is a natural definition for the effective interior of the Y = 1 region: the points x with η(x) > 1/2 on which the k-NN classifier is likely to be correct:\nX+p,∆ = {x ∈ supp(µ) | η(x) > 1 2 , η(B(x, r)) ≥ 1 2 + ∆ for all r ≤ rp(x)}.\nThe corresponding definition for the Y = 0 region is\nX−p,∆ = {x ∈ supp(µ) | η(x) < 1 2 , η(B(x, r)) ≤ 1 2 −∆ for all r ≤ rp(x)}.\nThe remainder of X is the effective boundary,\n∂p,∆ = X \\ (X+p,∆ ∪ X − p,∆).\nObserve that ∂p′,∆′ ⊂ ∂p,∆ whenever p′ ≤ p and ∆′ ≤ ∆. Under mild conditions, as p and ∆ tend to zero, the effective boundary tends to the actual decision boundary {x | η(x) = 1/2} (Lemma 12), which we shall denote ∂o."
    }, {
      "heading" : "2.2 A general bound on the misclassification error",
      "text" : "We begin with a general upper bound on the misclassification rate of the k-NN classifier. We will later specialize it to various situations of interest. All proofs appear in the appendix.\nTheorem 1. Pick any 0 < δ < 1 and positive integers k < n. Let gn,k denote the k-NN classifier based on n training points, and g(x) the Bayes-optimal classifier. With probability at least 1 − δ over the choice of training data,\nPrX(gn,k(X) 6= g(X)) ≤ δ + µ ( ∂p,∆ ) ,\nwhere\np = k n · 1 1− √ (4/k) ln(2/δ) , and ∆ = min\n( 1\n2 ,\n√ 1\nk ln\n2\nδ\n) .\nConvergence results for nearest neighbor have traditionally studied the excess riskRn,k−R∗, whereRn,k = Pr(Y 6= gn,k(X)). If we define the pointwise quantities\nRn,k(x) = Pr(Y 6= gn,k(x)|X = x) R∗(x) = min(η(x), 1− η(x)),\nfor all x ∈ X , we see that Rn,k(x)−R∗(x) = |1− 2η(x)|1(gn,k(x) 6= g(x)). (2) Taking expectation over X , we then have Rn,k − R∗ ≤ PrX(gn,k(X) 6= g(X)), and so Theorem 1 is also an upper bound on the excess risk.\nTo obtain an asymptotic result, we can take a sequence of integers (kn) and reals (δn) for which the corresponding pn,∆n ↓ 0. As we will see, this implies that ∂pn,∆n converges to the decision boundary, ∂o."
    }, {
      "heading" : "2.3 Universal consistency",
      "text" : "A series of results, starting with [15], has shown that kn-NN is strongly consistent (Rn = Rn,kn → R∗ almost surely) when X is a finite-dimensional Euclidean space and µ is a Borel measure. A consequence of Theorem 1 is that this phenomenon holds quite a bit more generally. In fact, strong consistency holds in any metric measure space (X , ρ, µ) for which the Lebesgue differentiation theorem is true: that is, spaces in which, for any bounded measurable f ,\nlim r↓0\n1\nµ(B(x, r)) ∫ B(x,r) f dµ = f(x) (3)\nfor almost all (µ-a.e.) x ∈ X . For more details on this differentiation property, see [6, 2.9.8] and [10, 1.13]. It holds, for instance:\n• When (X , ρ) is a finite-dimensional normed space [10, 1.15(a)]. • When (X , ρ, µ) is doubling [10, 1.8], that is, when there exists a constant C(µ) such that µ(B(x, 2r)) ≤ C(µ)µ(B(x, r)) for every ball B(x, r).\n• When µ is an atomic measure on X . Theorem 2. Suppose metric measure space (X , ρ, µ) satisfies differentiation condition (3). Pick a sequence of positive integers (kn), and for each n, let Rn = Rn,kn be the risk of the kn-NN classifier gn,kn .\n1. If kn →∞ and kn/n→ 0, then for all > 0, lim n→∞ Prn(Rn −R∗ > ) = 0.\nHere Prn denotes probability over the training set (X1, Y1), . . . , (Xn, Yn).\n2. If in addition kn/(log n)→∞, then Rn → R∗ almost surely."
    }, {
      "heading" : "2.4 A lower bound",
      "text" : "Next, we give a counterpart to Theorem 1 that lower-bounds the expected probability of error of gn,k. For any positive integers k < n, define the high-error set En,k = E+n,k ∪ E − n,k, where\nE+n,k = { x ∈ supp(µ) | η(x) > 1\n2 , η(B(x, r)) ≤ 1 2 + 1√ k\nfor all rk/n(x) ≤ r ≤ r(k+√k+1)/n(x) }\nE−n,k = { x ∈ supp(µ) | η(x) < 1\n2 , η(B(x, r)) ≥ 1 2 − 1√ k for all rk/n(x) ≤ r ≤ r(k+√k+1)/n(x)\n} .\n(Recall the definition (1) of η(A) for sets A.) We will see that for smooth η this region is comparable to the effective decision boundary ∂k/n,1/√k. Meanwhile, here is a lower bound that applies to any (X , ρ, µ). Theorem 3. For any positive integers k < n, let gn,k denote the k-NN classifier based on n training points. There is an absolute constant co such that the expected misclassification rate satisfies EnPrX(gn,k(X) 6= g(X)) ≥ co µ(En,k), where En is expectation over the choice of training set."
    }, {
      "heading" : "2.5 Smooth measures",
      "text" : "For the purposes of nearest neighbor, it makes sense to define a notion of smoothness with respect to the marginal distribution on instances. We use a variant of Holder-continuity: for α,L > 0, we say the conditional probability function η is (α,L)-smooth in metric measure space (X , ρ, µ) if for all x, x′ ∈ X , |η(x)− η(x′)| ≤ Lµ(Bo(x, ρ(x, x′)))α. This is stated to resemble standard smoothness conditions, but what we will really need is the weaker assertion that for all x ∈ supp(µ) and all r > 0,\n|η(B(x, r))− η(x)| ≤ Lµ(Bo(x, r))α.\nIn such circumstances, the earlier upper and lower bounds on generalization error take on a more easily interpretable form. Recall that the key term in the upper bound (Theorem 1) is µ(∂p,∆), for p ≈ k/n and ∆ ≈ 1/ √ k. Lemma 4. If η is (α,L)-smooth in (X , ρ, µ), then for any p,∆ ≥ 0,\n∂p,∆ ∩ supp(µ) ⊂ { x ∈ X ∣∣∣∣ ∣∣η(x)− 12 ∣∣ ≤ ∆ + Lpα } .\nThis yields a bound on PrX(gn,k(X) 6= g(X)) that is roughly of the form µ({x | |η(x) − 1/2| ≤ k−1/2 + L(k/n)α). The optimal setting of k is then ∼ n2α/(2α+1). The key term in the lower bound of Theorem 3 is µ(En,k). Under the smoothness condition, it becomes directly comparable to the upper bound. Lemma 5. If η is (α,L)-smooth in (X , ρ, µ), then for any k, n,\nEn,k ⊃ { x ∈ supp(µ) ∣∣∣∣ η(x) 6= 12 , |η(x)− 12 | ≤ 1√k − L ( k + √ k + 1 n )α} .\nIt is common to analyze nonparametric classifiers under the assumption that X = Rd and that η is αH - Holder continuous for some α > 0, that is,\n|η(x)− η(x′)| ≤ L‖x− x′‖αH\nfor some constant L. These bounds typically also require µ to have a density that is uniformly bounded (above and/or below). We now relate these assumptions to our notion of smoothness. Lemma 6. Suppose that X ⊂ Rd, and η is αH -Holder continuous, and µ has a density with respect to Lebesgue measure that is≥ µmin on X . Then there is a constant L such that for any x ∈ supp(µ) and r > 0 with B(x, r) ⊂ X ,\n|η(x)− η(B(x, r))| ≤ Lµ(Bo(x, r))αH/d.\n(To remove the requirement that B(x, r) ⊂ X , we would need the boundary of X to be well-behaved, for instance by requiring that X contains a constant fraction of every ball centered in it. This is a familiar assumption in results on nonparametric classification, including the seminal work of [1] that we discuss in the next section.)\nOur smoothness condition for nearest neighbor problems can thus be seen as a generalization of the usual Holder conditions. It applies in broader range of settings, for example for discrete µ."
    }, {
      "heading" : "2.6 Margin bounds",
      "text" : "An achievement of statistical theory in the past two decades has been margin bounds, which give fast rates of convergence for many classifiers when the underlying data distribution P (given by µ and η) satisfies a large margin condition stipulating, roughly, that η moves gracefully away from 1/2 near the decision boundary.\nFollowing [13, 16, 1], for any β ≥ 0, we say P satisfies the β-margin condition if there exists a constant C > 0 such that\nµ ({ x ∣∣∣ ∣∣η(x)− 1\n2 ∣∣ ≤ t}) ≤ Ctβ . Larger β implies a larger margin. We now obtain bounds for the misclassification rate and the excess risk of k-NN under smoothness and margin conditions. Theorem 7. Suppose η is (α,L)-smooth in (X , ρ, µ) and satisfies the β-margin condition (with constant C), for some α, β, L,C ≥ 0. In each of the two following statements, ko and Co are constants depending on α, β, L,C.\n(a) For any 0 < δ < 1, set k = kon2α/(2α+1)(log(1/δ))1/(2α+1). With probability at least 1− δ over the choice of training data,\nPrX(gn,k(X) 6= g(X)) ≤ δ + Co ( log(1/δ)\nn\n)αβ/(2α+1) .\n(b) Set k = kon2α/(2α+1). Then EnRn,k −R∗ ≤ Con−α(β+1)/(2α+1).\nIt is instructive to compare these bounds with the best known rates for nonparametric classification under the margin assumption. The work of [1] (Theorems 3.3 and 3.5) shows that when (X , ρ) = (Rd, ‖ · ‖), and η is αH -Holder continuous, and µ lies in the range [µmin, µmax] for some µmax > µmin > 0, and the β-margin condition holds (along with some other assumptions), an excess risk of n−αH(β+1)/(2αH+d) is achievable and is also the best possible. This is exactly the rate achieved by nearest neighbor classification, once we translate between the different notions of smoothness as per Lemma 6.\nAnother interesting scenario is when η is bounded away from 1/2, that is, there exists some ∆∗ for which\nµ ({ x ∣∣∣ ∣∣η(x)− 1\n2 ∣∣ ≤ ∆∗}) = 0 It follows from Lemma 4 that if η is (α,L)-smooth in (X , ρ, µ), then µ(∂p,∆) = 0 whenever ∆+Lpα ≤ ∆∗. Invoking Theorem 1 with\nk = n\n2\n( ∆∗\n2L\n)1/α , δ = 2e−k(∆ ∗)2/4,\nyields an exponentially fast rate of convergence: Pr(gn(X) 6= g(X)) ≤ 2e−Con, where Co = (∆∗)2+1/α/(8(2L)1/α).\nA final case of interest is when η ∈ {0, 1}, so that the Bayes risk R∗ is zero. We treat this in Section 2.14 in the appendix."
    }, {
      "heading" : "Appendix: Analysis",
      "text" : ""
    }, {
      "heading" : "2.7 A tie-breaking mechanism",
      "text" : "In some situations, such as discrete instance spaces, there is a non-zero probability that two or more of the training points will be equidistant from the query point. In practice, we break ties by a simple rule such as preferring points that appear earlier in the sequence. To accurately reflect this in the analysis, we adopt the following mechanism: for each training point X , we also draw a value Z independently and uniformly at random from [0, 1]. When breaking ties, points with lower Z value are preferred. We use the notation X ′ = (X,Z) ∈ X × [0, 1] to refer to the augmented training instances, drawn from the product measure µ× ν, where ν is Lebesgue measure on [0, 1]. Given a query point x ∈ X and training points X ′1, . . . , X ′n ∈ X × [0, 1], let X ′(1)(x), . . . , X ′ (n)(x) denote a reordering of these points by increasing distance from x, where each X ′(i) is of the form (X(i), Z(i)). With probability 1, this ordering is unambiguous. Also, let Y(1)(x), . . . , Y(n)(x) be the corresponding labels.\nWe will need to consider balls in the augmented space. For xo ∈ X , ro ≥ 0, and zo ∈ [0, 1], define B′(xo, ro, zo) = {(x, z) ∈ X × [0, 1] | either ρ(xo, x) < ro or (ρ(xo, x) = ro and z < zo)}\n= ( Bo(xo, ro)× [0, 1] )⋃( (B(xo, ro) \\Bo(xo, ro))× [0, zo) ) .\nGiven a set of training points (X ′i, Yi) and an augmented ball B ′ = B′(xo, ro, zo), let Ŷ (B′) denote the mean of the Yi for points X ′i ∈ B′; if there is no X ′i ∈ B′, then this is undefined. Let η(B′) denote the mean probability that Y = 1 for points (x, z) ∈ B′; formally, it is given by\nη(B′) = 1\n(µ× ν)(B′) ∫ B′ η d(µ× ν)\nwhenever (µ× ν)(B′) > 0. Here η(x, z) is defined to be η(x). The ball B′ = B(xo, ro, zo) in the augmented space can be thought of as lying between the open ball Bo = Bo(xo, ro) and the closed ball B = B(xo, ro) in the original space; and indeed η(B′) is a convex combination of η(B) and η(Bo) (Lemma 24)."
    }, {
      "heading" : "2.8 Proof of Theorem 1",
      "text" : "Theorem 1 rests on the following basic observation. Lemma 8. Let gn,k denote the k-NN classifier based on training data (X ′1, Y1), . . . , (X ′n, Yn). Pick any xo ∈ X and any 0 ≤ p ≤ 1, 0 ≤ ∆ ≤ 1/2. Let B′ = B′(xo, ρ(xo, X(k+1)(xo)), Z(k+1)). Then\n1(gn,k(xo) 6= g(xo)) ≤ 1(xo ∈ ∂p,∆) + 1(ρ(xo, X(k+1)(xo)) > rp(xo)) +\n1(|Ŷ (B′)− η(B′)| ≥ ∆).\nProof. Suppose xo 6∈ ∂p,∆. Then, without loss of generality, xo lies in X+p,∆, whereupon η(B(xo, r)) ≥ 1/2 + ∆ for all r ≤ rp(xo). Next, suppose r = ρ(xo, X(k+1)(xo)) ≤ rp(xo). Then η(B(xo, r)) and η(Bo(xo, r)) are both ≥ 1/2 + ∆ (Lemma 25). By Lemma 24, η(B′) is a convex combination of these and is thus also ≥ 1/2 + ∆.\nThe prediction gn,k(xo) is based on the average of the Yi values of the k points closest to xo, in other words, Ŷ (B′). If this average differs from η(B′) by less than ∆, then it is > 1/2, whereupon the prediction is correct.\nWhen we take expectations in the inequality of Lemma 8, we see that there are three probabilities to be bounded. The second of these, the probability that ρ(xo, X(k+1)(xo)) > rp(xo), can easily be controlled when p is sufficiently large.\nLemma 9. Fix any xo ∈ X and 0 ≤ p, γ ≤ 1. Pick any positive integer k ≤ (1− γ)np. Let X1, . . . , Xn be chosen uniformly at random from µ. Then\nPrn(ρ(xo, X(k+1)(xo)) > rp(xo)) ≤ e−npγ 2/2 ≤ e−kγ 2/2.\nProof. The probability that any givenXi falls inB(xo, rp(xo)) is at least p (Lemma 22). The probability that ≤ k ≤ (1− γ)np of them land in this ball is, by the multiplicative Chernoff bound, at most e−npγ2/2.\nTo bound the probability that Ŷ (B′) differs substantially from η(B′), a slightly more careful argument is needed.\nLemma 10. Fix any xo ∈ X and any 0 ≤ ∆ ≤ 1/2. Draw (X1, Z1, Y1), . . . , (Xn, Zn, Yn) independently at random and let B′ = B′(xo, ρ(xo, X(k+1)(xo)), Z(k+1)) ⊂ X × [0, 1]. Then\nPrn(|Ŷ (B′)− η(B′)| ≥ ∆) ≤ 2e−2k∆ 2 .\nMoreover, if η(B′) ∈ {0, 1} then Ŷ (B′) = η(B′) with probability one.\nProof. We will pick the points X ′i = (Xi, Zi) and their labels Yi in the following manner:\n1. First pick a point (X1, Z1) ∈ X × [0, 1] according to the marginal distribution of the (k + 1)st nearest neighbor of xo.\n2. Pick k points uniformly at random from the distribution µ × ν restricted to B′ = B′(xo, ρ(xo, X1), Z1).\n3. Pick n−k−1 points uniformly at random from the distribution µ×ν restricted to (X × [0, 1])\\B′.\n4. Randomly permute the n points obtained in this way.\n5. For each (Xi, Zi) in the permuted order, pick a label Yi from the conditional distribution η(Xi).\nThe k nearest neighbors of xo are the points picked in step 2. Their Y values are independent and identically distributed with expectation η(B′). The main bound in the lemma now follows from a direct application of Hoeffding’s inequality.\nThe final statement of the lemma is trivial and is needed to cover situations in which ∆ = 1/2.\nWe now complete the proof of Theorem 1. Adopt the settings of p and ∆ from the theorem statement, and define the central bad event to be\nBAD(Xo, X ′ 1, . . . , X ′ n, Y1, . . . , Yn) = 1(ρ(Xo, X(k+1)(Xo)) > rp(Xo)) + 1(|Ŷ (B′)− η(B′)| ≥ ∆),\nwhere B′ is a shorthand for B′(Xo, ρ(Xo, X(k+1)(Xo)), Z(k+1)), as before. Fix any xo ∈ X . If ∆ < 1/2, then by Lemmas 9 and 10,\nEnBAD(xo, X ′1, . . . , X ′n, Y1, . . . , Yn) ≤ exp(−kγ2/2) + 2 exp(−2k∆2) ≤ δ2, where γ = 1−(k/np) = √\n(4/k) ln(2/δ) and En is expectation over the choice of training data. If ∆ = 1/2 then η(B′) ∈ {0, 1} and we have\nEnBAD(xo, X ′1, . . . , X ′n, Y1, . . . , Yn) ≤ exp(−kγ2/2) ≤ δ2. Taking expectation over Xo, EXoEnBAD(Xo, X ′1, . . . , X ′n, Y1, . . . , Yn) ≤ δ2, from which, by switching expectations and applying Markov’s inequality, we have\nPrn(EXo BAD(Xo, X ′1, . . . , X ′n, Y1, . . . , Yn) ≥ δ) ≤ δ. The theorem then follows by writing the result of Lemma 8 as\nPrXo(gn,k(Xo) 6= g(Xo)) ≤ µ(∂p,∆) + EXo BAD(Xo, X ′1, . . . , X ′n, Y1, . . . , Yn)."
    }, {
      "heading" : "2.9 Proof of Theorem 2",
      "text" : "Recall that we define Rn = PrX(gn,kn(X) 6= Y ). From equation (2), we have: Rn −R∗ ≤ PrX(η(X) 6= 1/2 and gn,kn(X) 6= g(X)). Defining ∂o = {x ∈ X | η(x) = 1/2} to be the decision boundary, we then have the following corollary of Theorem 1. Corollary 11. Let (δn) be any sequence of positive reals, and (kn) any sequence of positive integers. For each n, define (pn) and (∆n) as in Theorem 1. Then\nPrn ( Rn −R∗ > δn + µ ( ∂pn,∆n \\ ∂o )) ≤ δn,\nwhere Prn is probability over the choice of training data.\nFor the rest of the proof, assume that (X , ρ, µ) satisfies Lebesgue’s differentiation theorem: that is, for any bounded measurable f : X → R,\nlim r↓0\n1\nµ(B(x, r)) ∫ B(x,r) f dµ = f(x)\nfor almost all (µ-a.e.) x ∈ X . We’ll see that, as a result, µ(∂pn,∆n \\ ∂o)→ 0. Lemma 12. There exists Xo ⊂ X with µ(Xo) = 0, such that any x ∈ X \\ Xo with η(x) 6= 1/2 lies in X+p,∆ ∪ X − p,∆ for some p,∆ > 0.\nProof. As a result of the differentiation condition,\nlim r↓0 η(B(x, r)) = lim r↓0\n1\nµ(B(x, r)) ∫ B(x,r) η dµ = η(x) (4)\nfor almost all (µ-a.e.) x ∈ X . Let Xo denote the set of x’s for which (4) fails to hold or that are outside supp(µ). Then, µ(Xo) = 0. Now pick any x 6∈ Xo such that η(x) 6= 1/2. Without loss of generality, η(x) > 1/2. Set ∆ = (η(x) − 1/2)/2 > 0. By (4), there is some ro > 0 such that η(B(x, r)) ≥ 1/2 + ∆ whenever 0 ≤ r ≤ ro. Define p = µ(B(x, ro)) > 0. Then rp(x) ≤ ro and x ∈ X+p,∆.\nLemma 13. If pn,∆n ↓ 0, then lim n→∞ µ ( ∂pn,∆n \\ ∂o ) = 0.\nProof. LetAn = ∂pn,∆n \\∂o. ThenA1 ⊃ A2 ⊃ A3 ⊃ · · · . We’ve seen earlier that for any x ∈ X \\(Xo∪∂o) (where Xo is defined in Lemma 12), there exist p,∆ > 0 such that x 6∈ ∂p,∆. Therefore,⋂\nn≥1\nAn ⊂ Xo,\nwhereupon, by continuity from above, µ(An)→ 0.\nConvergence in probability follows immediately.\nLemma 14. If kn →∞ and kn/n→ 0, then for any > 0,\nlim n→∞\nPrn(Rn −R∗ > ) = 0.\nProof. First define δn = exp(−k1/2n ), and define the corresponding pn,∆n as in Theorem 1. It is easily checked that the three sequences δn, pn,∆n all go to zero.\nPick any > 0. By Lemma 13, we can choose a positive integer N so that δn ≤ /2 and µ(∂pn,∆n \\ ∂o) ≤ /2 whenever n ≥ N . Then by Corollary 11, for n ≥ N ,\nPrn(Rn −R∗ > ) ≤ δn.\nNow take n→∞.\nWe finish with almost sure convergence.\nLemma 15. Suppose that in addition to the conditions of Lemma 14, we have kn/(log n) → ∞. Then Rn → R∗ almost surely.\nProof. Choose δn = 1/n2, and for each n set pn,∆n as in Theorem 1. It can be checked that the resulting sequences (pn) and (∆n) both go to zero.\nPick any > 0. Choose N so that ∑ n≥N δn ≤ . Letting ω denote a realization of an infinite training sequence (X1, Y1), (X2, Y2), . . ., we have from Corollary 11 that\nPr { ω ∣∣ ∃n ≥ N : Rn(ω)−R∗ > δn + µ(∂pn,∆n \\ ∂o)} ≤ ∑\nn≥N\nδn ≤ .\nTherefore, with probability at least 1− over ω, we have Rn(ω)−R∗ ≤ δn + µ ( ∂pn,∆n \\ ∂o ) for all n ≥ N , whereupon, by Lemma 13, Rn(ω) → R∗. The result follows since this is true for any > 0."
    }, {
      "heading" : "2.10 Proof of Theorem 3",
      "text" : "For positive integer n and 0 ≤ p ≤ 1, let bin(n, p) denote the (binomial) distribution of the sum of n independent Bernoulli(p) random variables. We will use bin(n, p;≥ k) to denote the probability that this sum is ≥ k; and likewise bin(n, p;≤ k). It is well-known that the binomial distribution can be approximated by a normal distribution, suitably scaled. Slud [14] has finite-sample results of this form that will be useful to us.\nLemma 16. Pick any 0 < p ≤ 1/2 and any nonnegative integer `.\n(a) [14, p. 404, item (v)] If ` ≤ np, then bin(n, p;≥ `) ≥ 1− Φ((`− np)/√np). (b) [14, Thm 2.1] If np ≤ ` ≤ n(1− p), then bin(n, p;≥ `) ≥ 1− Φ((`− np)/ √ np(1− p)).\nHere Φ(a) = (2π)−1/2 ∫ a −∞ exp(−t 2/2)dt is the cumulative distribution function of the standard normal.\nNow we begin the proof of Theorem 3. Fix any integers k < n, and any xo ∈ En,k. Without loss of generality, η(xo) < 1/2.\nPick X1, . . . , Xn and Z1, . . . , Zn (recall the discussion on tie-breaking in Section 2.7) in the following manner:\n1. First pick a point (X1, Z1) ∈ X × [0, 1] according to the marginal distribution of the (k + 1)st nearest neighbor of xo.\n2. Pick k points uniformly at random from the distribution µ × ν restricted to B′ = B′(xo, ρ(xo, X1), Z1); recall the earlier definition of the augmented space X × [0, 1] and augmented balls within this space.\n3. Pick n−k−1 points uniformly at random from the distribution µ×ν restricted to (X × [0, 1])\\B′.\n4. Randomly permute the n points obtained in this way.\nThe (k+1)st nearest neighbor of xo, denotedX(k+1)(xo), is the point chosen in the first step. With constant probability, it lies within a ball of probability mass (k + √ k + 1)/n centered at xo, but not within a ball of probability mass k/n. Call this event G1:\nG1 : rk/n(xo) ≤ ρ(xo, X(k+1)(xo)) ≤ r(k+√k+1)/n(xo)\nLemma 17. There is an absolute constant c1 > 0 such that Pr(G1) ≥ c1.\nProof. The expected number of points Xi that fall in B(xo, r(k+√k+1)/n(xo)) is ≥ k+ √ k+ 1; the probability that the actual number is ≤ k is at most bin(n, (k+ √ k+ 1)/n;≤ k). Likewise, the expected number of points that fall in Bo(xo, rk/n(xo)) is ≤ k, and the probability that the actual number is ≥ k + 1 is at most bin(n, k/n;≥ k + 1). If neither of these bad events occurs, then G1 holds. Therefore,\nPr(G1) ≥ 1− bin ( n, k + √ k + 1\nn ; ≤ k\n) − bin ( n, k\nn ; ≥ k + 1\n) .\nThe last term is easy to bound: it is≤ 1/2 since k is the median of bin(n, k/n) [11]. To bound the first term, we use Lemma 16(a):\nbin ( n, k + √ k + 1\nn ; ≤ k\n) = 1− bin ( n, k + √ k + 1\nn ; ≥ k + 1 ) ≤ Φ ( (k + 1)− (k + √ k + 1)√\nk + √ k + 1\n) ≤ Φ(−1/ √ 3),\nwhich is 1/2− c1 for some constant c1 > 0. Thus Pr(G1) ≥ c1.\nNext, we lower-bound the probability that (conditional on event G1), the k nearest neighbors of xo have an average Y value with the wrong sign. Recalling that η(xo) < 1/2, define the event\nG2 : Ŷ (B ′) > 1/2\nwhere as before, B′ denotes the ball B′(xo, X(k+1)(xo), Zk+1) in the augmented space. Lemma 18. There is an absolute constant c2 > 0 such that Pr(G2|G1) ≥ c2.\nProof. EventG1 depends only on step 1 of the sampling process. Assuming this event occurs, step 2 consists in drawing k points from the distribution µ×ν restricted to B′. Since xo ∈ En,k, we have (by an application of Lemmas 24 and 25) that η(B′) ≥ 1/2 − 1/ √ k. Now, Ŷ (B′) follows a bin(k, η(B′)) distribution, and hence, by Lemma 16(b),\nPr ( Ŷ (B′) > k\n2\n) = Pr ( Ŷ (B′) ≥ ⌈ k + 1\n2\n⌉) ≥ Pr ( Z ≥ 2\n√ k + 2√ k ) where Z is a standard normal. The last tail probability is at least some constant c2.\nIn summary, for xo ∈ En,k, Prn(gn,k(xo) 6= g(xo)) ≥ Pr(G1 ∧G2) ≥ c1c2. Taking expectation over xo, we then get EnPrX(gn,k(x) 6= g(x)) ≥ c1c2µ(En,k), as claimed."
    }, {
      "heading" : "2.11 Proofs of Lemmas 4 and 5",
      "text" : "It is immediate that if η is (α,L)-smooth in (X , ρ, µ), then for all x ∈ supp(µ) and all r > 0, |η(B(x, r))− η(x)| ≤ Lµ(Bo(x, r))α (5)\nPick any x ∈ supp(µ) and any p ≥ 0. For r ≤ rp(x), we have µ(Bo(x, r)) ≤ p and thus, by (5), |η(B(x, r))− η(x)| ≤ Lpα.\nAs a result, if η(x) > 1/2 + ∆ +Lpα then η(B(x, r)) > 1/2 + ∆ whenever r ≤ rp(x). Therefore, such an x lies in the effective interior X+p,∆. A similar result applies to x with η(x) < 1/2 −∆ − Lpα. Therefore, the boundary region ∂p,∆ can only contain points x for which |η(x) − 1/2| ≤ ∆ + Lpα, as claimed by Lemma 4.\nA similar argument yields Lemma 5. Any point x ∈ supp(µ) with 1\n2 < η(x) ≤ 1 2 + 1√ k − L\n( k + √ k + 1\nn )α has η(B(x, r)) ≤ 1/2 + 1/ √ k for all r ≤ r(k+√k+1)/n(x), and therefore lies in E + n,k. Likewise for E − n,k."
    }, {
      "heading" : "2.12 Proof of Lemma 6",
      "text" : "Suppose that η satisfies the α-Holder condition so that for some constant C > 0,\n|η(x)− η(x′)| ≤ C‖x− x′‖αH\nwhenever x, x′ ∈ X . For any x ∈ supp(µ) and r > 0, we then have |η(x)− η(B(x, r))| ≤ CrαH .\nIf µ has a density that is lower-bounded by µmin, and B(x, r) ⊂ X , we also have\nµ(Bo(x, r)) ≥ µminvdrd, where vd is the volume of the unit ball in Rd. The lemma follows by combining these two inequalities."
    }, {
      "heading" : "2.13 Proof of Theorem 7",
      "text" : "Assume that η is (α,L)-smooth in (X , ρ, µ), that is, |η(B(x, r))− η(x)| ≤ Lµ(Bo(x, r))α (6)\nfor all x ∈ supp(µ) and all r > 0, and also that it satisfies the β-margin condition (with constant C), under which, for any t ≥ 0,\nµ ({ x ∣∣∣ ∣∣η(x)− 1\n2 ∣∣ ≤ t}) ≤ Ctβ . (7) Proof of Theorem 7(a)\nSet p,∆ as specified in Theorem 1. It follows from that theorem and from Lemma 4 that under (6) and (7), for any δ > 0, with probability at least 1− δ over the choice of training data,\nPrX(gn,k(X) 6= g(X)) ≤ δ + µ(∂p,∆) ≤ δ + C(∆ + Lpα)β . Expanding p,∆ in terms of k and n, this becomes\nPrX(gn,k(X) 6= g(X)) ≤ δ + C\n(( ln(2/δ)\nk\n)1/2 + L ( 2k\nn\n)α)β ,\nprovided k ≥ 16 ln(2/δ). The result follows by setting k ∝ n2α/(2α+1)(log(1/δ))1/(2α+1).\nProof of Theorem 7(b)\nTheorem 7(b) is an immediate consequence of Lemma 20 below. We begin, however, with an intermediate result about the pointwise expected risk.\nFix any n and any k < n, and set p = 2k/n. Define\n∆(x) = |η(x)− 1/2| ∆o = Lp α\nRecall that the Bayes classifier g(x) has risk R∗(x) = min(η(x), 1 − η(x)) at x. The pointwise risk of the k-NN classifier gn,k is denoted Rn,k(x). Lemma 19. Pick any x ∈ supp(µ) with ∆(x) > ∆o. Under (6),\nEnRn,k(x)−R∗(x) ≤ exp(−k/8) + 4∆(x) exp(−2k(∆(x)−∆o)2).\nProof. Assume without loss of generality that η(x) > 1/2. By (6), for any 0 ≤ r ≤ rp(x), we have\nη(B(x, r)) ≥ η(x)− Lpα = η(x)−∆o = 1\n2 + (∆(x)−∆o),\nwhereby x ∈ X+p,∆(x)−∆o (and thus x 6∈ ∂p,∆(x)−∆o ).\nNext, recalling (2), and then applying Lemma 8,\nRn,k(x)−R∗(x) = 2∆(x)1(gn,k(x) 6= g(x)) ≤ 2∆(x) ( 1(ρ(x,X(k+1)(x)) > rp(x)) + 1(|Ŷ (B′)− η(B′)| ≥ ∆(x)−∆o) ) ,\nwhere B′ is as defined in that lemma statement. We can now take expectation over the training data and invoke Lemmas 9 and 10 to conclude\nEnRn,k(x)−R∗(x) ≤ 2∆(x) ( Prn(ρ(x,X(k+1)(x)) > rp(x)) + Prn(|Ŷ (B′)− η(B′)| ≥ ∆(x)−∆o) )\n≤ 2∆(x) ( exp ( −k\n2\n( 1− k\nnp\n)2) + 2 exp ( −2k(∆(x)−∆o)2 )) ,\nfrom which the lemma follows by substituting p = 2k/n and observing ∆(x) ≤ 1/2.\nLemma 20. Under (6) and (7),\nEnRn,k −R∗ ≤ exp(−k/8) + 6C max ( 2L ( 2k\nn\n)α , √ 8(β + 2)\nk\n)β+1 .\nProof. Recall the definitions of p(= 2k/n) and ∆o,∆(x) above. Further, for each integer i ≥ 1, define ∆i = ∆o · 2i. Fix any io ≥ 1. Lemma 19 bounds the expected pointwise risk for any x with ∆(x) > ∆o. We will apply it to points with ∆(x) > ∆io . For all remaining x, we have EnRn,k(x)−R∗(x) ≤ 2∆io . Taking expectation over X ,\nEnRn −R∗ ≤ EX [ 2∆io1(∆(X) ≤ ∆io) + exp(−k/8) + 4∆(X) exp(−2k(∆(X)−∆o)2)1(∆(X) > ∆io) ] ≤ 2∆ioPrX(∆(X) ≤ ∆io) + exp(−k/8) + 4EX [ ∆(X) exp(−2k(∆(X)−∆o)2)1(∆(X) > ∆io) ] .\nBy the margin condition (7), we have PrX(∆(X) ≤ t) ≤ Ctβ . Thus only the last expectation remains to be bounded. We do so by considering each interval ∆i < ∆(X) ≤ ∆i+1 separately:\nEX [ ∆(X) exp(−2k(∆(X)−∆o)2)1(∆i < ∆(X) ≤ ∆i+1) ] ≤ ∆i+1 exp(−2k(∆i −∆o)2)PrX(∆(X) ≤ ∆i+1) ≤ C∆β+1i+1 exp(−2k(∆i −∆o) 2). (8)\nIf we set\nio = max\n( 1, ⌈ log2 √ 2(β + 2)\nk∆2o\n⌉) ,\nthen for i ≥ io, the terms (8) are upper-bounded by a geometric series with ratio 1/2. This is because the ratio of two successive terms can be bounded as\nC∆β+1i+1 exp(−2k(∆i −∆o)2) C∆β+1i exp(−2k(∆i−1 −∆o)2) = 2β+1 exp(−2k((2i∆o −∆o)2 − (2i−1∆o −∆o)2))\n= 2β+1 exp(−2k∆2o((2i − 1)2 − (2i−1 − 1)2)) ≤ 2β+1 exp(−22i−1k∆2o) ≤ 2β+1 exp(−(β + 2)) ≤ 1/2.\nTherefore\nEX [ ∆(X) exp(−2k(∆(X)−∆o)2)1(∆(X) > ∆io) ] = ∑ i≥io EX [ ∆(X) exp(−2k(∆(X)−∆o)2)1(∆i < ∆(X) ≤ ∆i+1)\n] ≤ ∑ i≥io C∆β+1i+1 exp(−2k(∆i −∆o) 2) ≤ C∆β+1io exp(−2k(∆io−1 −∆o) 2) ≤ C∆β+1io .\nPutting these together, we have EnRn,k − R∗ ≤ 6C∆β+1io + e −k/8. We finish by substituting ∆io = 2io∆o."
    }, {
      "heading" : "2.14 Zero Bayes Risk",
      "text" : "An interesting case is when there is no inherent uncertainty in the conditional probability distribution p(y|x). Formally, for all x in the sample space X , except those in a subset X0 of measure zero, η(x) is either 0 or 1. In this case, the omniscient Bayes classifier will incur risk R∗ = 0; however, a classifier based on a finite sample that is unaware of the true η will incur some non-zero classification error.\nAn interesting quantity to consider in this case is the effective interiors of the classes as a whole:\nX+p = {x ∈ supp(µ) | η(x) = 1, η(B(x, r)) = 1 for all r ≤ rp(x)}. X−p = {x ∈ supp(µ) | η(x) = 0, η(B(x, r)) = 0 for all r ≤ rp(x)}.\nThus, X+p = X+p,1/2, and X − p = X−p,1/2. The rest of X is the effective boundary between the two classes:\n∂p = X \\ (X+p ∪ X−p ).\nIncorporating these two quantities into Theorem 1 yields a bound of the following form.\nLemma 21. Let δ be any positive real and let k < n be positive integers. With probability ≥ 1− δ over the choice of the training data, the error of the k-nearest neighbor classifier gn,k is bounded as:\nPr X\n(gn,k(X) 6= g(X)) ≤ δ + µ(∂p),\nwhere\np = k\nn +\n2 log(2/δ)\nn\n( 1 + √ 1 +\nk\nlog(2/δ)\n)\nProof. The proof is the same as that of Theorem 1, except that the probability of the central bad event is different. We will therefore bound the probabilities of these events.\nObserve that under the conditions of the lemma, for any p,\n∂p, 12 = ∂p\nMoreover, for any x0 /∈ ∂p, η(B′(x0, rp(x0))) is either 0 or 1; this implies that for all x ∈ B′(x0, rp(x0)) except those in a measure zero subset, η(x) is either 0 or 1. Therefore, the probability Pr(Ŷ (B′) 6= η(B′)) is zero.\nThe rest of the lemma follows from plugging this fact in to the proof of Theorem 1 and some simple algebra.\nIn particular, observe that since p increases with increasing k, and the dependence on ∆ is removed, the best bounds are achieved at k = 1 for:\np = 1\nn +\n2(1 + √ 2) log(2/δ)\nn\nThis corroborates the admissibility results of [2], which essentially state that there is no k > 1 such that the k-nearest neighbor algorithm has equal or better error than the 1-nearest neighbor algorithm against all distributions."
    }, {
      "heading" : "2.15 Additional technical lemmas",
      "text" : "Lemma 22. For any x ∈ X and 0 ≤ p ≤ 1, we have µ(B(x, rp(x))) ≥ p.\nProof. Let r∗ = rp(x) = inf{r | µ(B(x, r)) ≥ p}. For any n ≥ 1, let Bn = B(x, r∗ + 1/n). Thus B1 ⊃ B2 ⊃ B3 ⊃ · · · , with µ(Bn) ≥ p. Since B(x, r∗) = ⋂ nBn, it follows by continuity from above of probability measures that µ(Bn)→ µ(B(x, r∗)), so this latter quantity is ≥ p.\nLemma 23 (Cover-Hart). µ(supp(µ)) = 1.\nProof. Let Xo denote a countable dense subset of X . Now, pick any point x 6∈ supp(µ); then there is some r > 0 such that µ(B(x, r)) = 0. It is therefore possible to choose a ball Bx centered in Xo, with rational radius, such that x ∈ Bx and µ(Bx) = 0. Since there are only countably many balls of this sort,\nµ(X \\ supp(µ)) ≤ µ ( ⋃ x 6∈supp(µ) Bx ) = 0.\nLemma 24. Pick any xo ∈ supp(µ), ro > 0, and any Borel set I ⊂ [0, 1]. Define Bo = Bo(xo, ro) and B = B(xo, ro) to be open and closed balls centered at xo, and let A ⊂ X × [0, 1] be given by A = (Bo × [0, 1]) ⋃ ((B \\Bo)× I). Then\nη(A) = µ(B)ν(I)\nµ(B)ν(I) + µ(Bo)(1− ν(I)) η(B) + µ(Bo)(1− ν(I)) µ(B)ν(I) + µ(Bo)(1− ν(I)) η(Bo).\nProof. Since xo lies in the support of µ, we have (µ× ν)(A) ≥ µ(Bo) > 0; hence η(A) is well-defined.\nη(A) = 1\n(µ× ν)(A) ∫ A η d(µ× ν)\n= 1\nµ(Bo) + µ(B \\Bo)ν(I) (∫ Bo η dµ+ ∫ B\\Bo ν(I)η dµ )\n= 1\nµ(Bo) + (µ(B)− µ(Bo))ν(I) (∫ Bo η dµ+ ν(I) (∫ B η dµ− ∫ Bo η dµ )) = µ(Bo)η(Bo) + ν(I)(µ(B)η(B)− µ(Bo)η(Bo))\nµ(Bo)(1− ν(I)) + µ(B)ν(I) ,\nas claimed.\nLemma 25. Suppose that for some xo ∈ supp(µ) and ro > 0 and q > 0, it is the case that η(B(xo, r)) ≥ q whenever r ≤ ro. Then η(Bo(xo, ro)) ≥ q as well.\nProof. Let Bo = Bo(xo, ro). Since Bo = ⋃ r<ro B(xo, r), it follows from the continuity from below of probability measures that\nlim r↑ro\nµ(B(xo, r)) = µ(B o)\nand by dominated convergence that\nlim r↑ro ∫ B(xo,r) η dµ = lim r↑ro ∫ Bo 1(x ∈ B(xo, r))η(x)µ(dx) = ∫ Bo η dµ.\nFor any r ≤ ro, we have η(B(xo, r)) ≥ q, which can be rewritten as∫ B(xo,r) η dµ− q µ(B(xo, r)) ≥ 0.\nTaking the limit r ↑ ro, we then get the desired statement."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2014,
    "abstractText" : "Nearest neighbor methods are a popular class of nonparametric estimators with several<lb>desirable properties, such as adaptivity to different distance scales in different regions of<lb>space. Prior work on convergence rates for nearest neighbor classification has not fully<lb>reflected these subtle properties. We analyze the behavior of these estimators in metric<lb>spaces and provide finite-sample, distribution-dependent rates of convergence under min-<lb>imal assumptions. As a by-product, we are able to establish the universal consistency of<lb>nearest neighbor in a broader range of data spaces than was previously known. We illus-<lb>trate our upper and lower bounds by introducing smoothness classes that are customized<lb>for nearest neighbor classification.",
    "creator" : "LaTeX with hyperref package"
  }
}