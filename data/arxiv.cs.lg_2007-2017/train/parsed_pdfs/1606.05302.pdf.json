{
  "name" : "1606.05302.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Generalized Direct Change Estimation in Ising Model Structure",
    "authors" : [ "Farideh Fazayeli", "Arindam Banerjee" ],
    "emails" : [ "farideh@cs.umn.edu}", "banerjee@cs.umn.edu}" ],
    "sections" : [ {
      "heading" : null,
      "text" : "min(n1,n2) ,\nwhere c depends on the Gaussian width of the unit norm ball. For example, for `1 norm applied to s-sparse change, the change can be accurately estimated with min(n1, n2) = O(s log p) which is sharper than an existing result n1 = O(s\n2 log p) and n2 = O(n21). Experimental results illustrating the effectiveness of the proposed estimator are presented."
    }, {
      "heading" : "1 Introduction",
      "text" : "Over the past decade, considerable progress has been made on estimating the statistical dependency structure of graphical models based on samples drawn from the model. In particular, such advances have been made for Gaussian graphical models, Ising models, Gaussian copulas, as well as certain multi-variate extensions of general exponential family distributions including multivariate Poisson models [2, 10, 16, 22, 23, 34].\nIn this paper, we consider Ising models and focus on the problem of estimating changes in Ising model structure: given two sets of samples Xn11 = {x1i } n1 i=1 and X n2 2 = {x2i } n2 i=1 respectively drawn from two p-dimensional Ising models with true parameters θ∗1 and θ ∗ 2 , where θ ∗ 1 , θ ∗ 2 ∈ Rp×p, the goal is to estimate the change δθ∗ = (θ∗1−θ∗2). In particular, we focus on the situation when the change δθ∗ has structure, such as sparsity, block sparsity, or node-perturbed sparsity, which can be characterized by a suitable (atomic) norm [6, 18]. However, the individual model parameters θ∗1 , θ ∗ 2 need not have any specific structure, and they may both correspond to dense matrices. The goal is to get an estimate δθ̂ of the change δθ∗ such that the estimation error ∆ = (δθ̂ − δθ∗) is small. Such change estimation has potentially wide range of applications including identifying the changes in the neural connectivity networks, the difference between plant trait interactions at different climate conditions, and the changes in the stock market dependency structures.\nOne can consider two broad approaches for solving such change estimation problems: (i) indirect change estimation, where we estimate θ̂1 and θ̂2 from two sets of samples separately and obtain δθ̂ = (θ̂1 − θ̂2), or (ii) direct change estimation, where we directly estimate δθ̂ using the two sets of samples, without estimating θ1 and θ2 individually. In a high dimensional setting, recent advances [5, 22, 23] illustrate that accurate estimation of the parameter θ∗ of an Ising model depends on how sparse or otherwise structured the true parameter θ∗ is. For example, if both θ∗1 and θ ∗ 2\nar X\niv :1\n60 6.\n05 30\n2v 1\n[ m\nat h.\nST ]\n1 6\nJu n\nare sparse and the samples n1, n2 are sufficient to estimate them accurately [22], indirect estimation of δθ̂ should be accurate. However, if the individual parameters θ∗1 and θ ∗ 2 are somewhat dense, and the change δθ\n∗ has considerably more structure, such as block sparsity (only a small block has changed) or node perturbation sparsity (only edges from a few nodes have changed) [18], direct estimation may be considerably more efficient both in terms of the number of samples required as well as the computation time.\nRelated Work: In recent work, Liu et al. [13] proposed a direct change estimator for graphical models based on the ratio of the probability density of the two models [9, 10, 25, 26, 31]. They focused on the special case of L1 norm, i.e., δθ∗ ∈ Rp2 is sparse, and provided non-asymptotic error bounds for the estimator along with a sample complexity of n1 = O(s\n2 log p) and n2 = O(n21) for an unbounded density ratio model, where s is the number of the changed edges with p being the number of variables. Liu et al. [14] improved the sample complexity to min(n1, n2) = O(s2 log p) when a bounded density ratio model is assumed. Zhao et al. [36] considered estimating direct sparse changes in Gaussian graphical models (GGMs). Their estimator is specific to GGMs and can not be applied to Ising models.\nOur Contributions: We consider general structured direct change estimation, while allowing the change to have any structure which can be captured by a suitable (atomic) norm R(·). Our work is a considerable generalization of the existing literature which can only handle sparse changes, captured by the L1 norm. In particular, our work now enables estimators for more general structures such as group/block sparsity, hierarchical group/block sparsity, node perturbation based sparsity, and so on [1, 6, 18, 19]. Interestingly, for the unbounded density ratio model, our analysis yields sharper bounds for the special case of `1 norm, considered by Liu et al. [13]. In particular, when δθ∗ is sparse and our estimator is run with L1 norm, we get a sample complexity of n1 = n2 = O(s log p) which is sharper than n1 = O(s 2 log p) and n2 = O(n21) in [13].\nThe regularized estimator we analyze is broadly a Lasso-type estimator, with key important differences: the objective does not decompose additively over the samples, and the objective depends on samples from two distributions. The estimator builds on the density ratio estimator in [13], but works with general norm regularization [1, 6, 19] where the regularization parameter λn1,n2 depends on the sample size for both Ising models. Our analysis is quite different from the existing literature in change estimation. Liu et. [13] build on the primal-dual witness approach of Wainwright [33], which is effective for the special case of L1 norm. Our analysis is largely geometric, where generic chaining [30] plays a key role, and our results are in terms of Gaussian widths of suitable sets associated with the norm [1, 6].\nThe rest of the paper is organized as follows. In Section 2, we introduce the direct change estimator based on the ratio of the probability density of the Ising models. In Section 3, we establish statistical consistency of the direct change estimator, and conclude in Section 5."
    }, {
      "heading" : "2 Generalized Direct Change Estimation",
      "text" : "We consider the following optimization problem\nargmin δθ\nL(δθ;Xn11 ,X n2 2 ) + λn1,n2R(δθ), (1)\nwhere Xn11 = {x1i } n1 i=1 and X n2 2 = {x2i } n2 i=1 are two sets of i.i.d binary samples drawn from from Ising graphical models with parameter θ∗1 and θ ∗ 2 , respectively, each x 1 i and x 2 i are p−dimensional vectors, and n1, n2 are the respective sample sizes.\nIn this Section, we first give a brief background on Ising model selection. Then, we explain how to develop the loss function L(δθ;Xn11 ,X n2 2 ) based on the density ratio [9, 10, 25, 31] to directly estimate δθ = θ1 − θ2, and finally we describe how to solve the optimization problem (1) for any norm R(δθ)."
    }, {
      "heading" : "2.1 Ising Model",
      "text" : "Let X = (X1, X2, · · · , Xp) denote a random vector in which each variable Xs ∈ {−1, 1}. Let G = (V,E) be an undirected graph with vertex set V = {1, · · · , p} and edge set E whose elements are unordered pairs of distinct vertices. The pairwise Ising Markov random field associated with the graph G over the random vector X is\nP (X = x|θ∗) = 1 Z(θ∗) exp{ ∑ s,t∈E θ∗s,txsxt} (2)\n= 1\nZ(θ∗) exp{〈θ∗, T (x)〉} (3)\n= 1\nZ(Θ∗) exp{xTΘ∗x} (4)\nwhere T (x) = {xsxt}ps,t=1 is a vector of size m = p2, θ∗ = {θ∗s,t} p s,t=1 ∈ Rm and 〈., .〉 is the inner product operator, and Θ∗ ∈ Rp×p where Θ∗s,t = θ∗s,t. Note that basic Ising models also have non-interacting terms like αsxs and we are assuming these terms are zero, and they do not affect the dependency structure.\nThe parameter θ∗ associated with the structure of the graphG reveals the statistical conditional independence structure among the variables i.e., if θ∗s,t = 0, then feature Xs is conditionally independent of Xt given all other variables and there is no edge in the graph G.\nThe partition function, Z(θ∗), plays the role of a normalizing constant, ensuring that the probabilities add up to one which is defined as\nZ(θ∗) = ∑ x∈X exp{〈θ∗, T (x)〉} = exp{Ψ(θ∗)}, (5)\nwhere X be the set of all possible configurations of X ."
    }, {
      "heading" : "2.2 Loss Function",
      "text" : "Here, we build the loss function based on equation (3). Similarly, one can rewrite the loss function based on (4) if the regularization function is over matrices. Consider two Ising models with parameters θ∗1 ∈ Rp 2 and θ∗2 ∈ Rp 2\n. Following Liu et. al [12, 13], a direct estimate for the changes detection problem based on density ratio can be posed as follows\nr(X = x|δθ) = p(X = x|θ1) p(X = x|θ2) = exp{〈T (x), θ1〉} exp{〈T (x), θ2〉}︸ ︷︷ ︸\nr∗(x|δθ)\nZ(θ2) Z(θ1)︸ ︷︷ ︸ 1/Z(δθ) = exp{〈T (x), δθ〉)} Z(δθ) , (6)\nwhere the parameter δθ = θ1 − θ2 encodes the change between two graphical models θ1 and θ2.\nFirst, we show that Z(δθ) = EX∼q[e〈T (X),δθ〉]:\nZ(δθ) = Z(θ1)\nZ(θ2) =\n1\nZ(θ2) ∑ x∈X e〈T (x),θ1〉 = 1 Z(θ2) ∑ x∈X e〈T (x),θ2〉 e〈T (x),θ1〉 e〈T (x),θ2〉\n= ∑ x∈X e〈T (x),θ2〉\nZ(θ2)︸ ︷︷ ︸ p(x|θ2)\ne〈T (x),δθ〉 = EX∼p(X|θ2)[e 〈T (X),δθ〉]. (7)\nNext, using the samples Xn22 from p(X|θ2), we estimate Z(δθ) empirically as\nẐ(δθ) = 1\nn2 n2∑ i=1 exp{〈T (x2i ), δθ〉}, (8)\nand the sample approximation of r(X|δθ) is given as\nr̂(X = x|δθ) = r ∗(X = x|θ1) Ẑ(δθ) = exp{〈T (x), δθ〉}\n1 n2 ∑n2 i=1 exp{〈T (x2i ), δθ〉} . (9)\nUsing the fact that r(X|δθ∗)q(X|θ∗2) = p(X|θ∗1), we approximate r̂(X|δθ), by minimizing the KL divergence,\nKL (p(X|θ∗1)‖r̂(X|δθ)p(X|θ∗2)) = ∑ x∈X p(x|θ∗1) log p(x|θ∗1) p(x|θ∗2)r̂(x|δθ)\n= KL (p(X|θ∗1)‖p(X|θ∗2))︸ ︷︷ ︸ Constant −EX∼p(X|θ∗1 ) [log r̂(X|δθ)] (10)\nThus, using the samples Xn11 and X n2 2 , we define the empirical loss function\nL(δθ;Xn11 ,X n2 2 ) = −1 n1 n1∑ i=1 log r̂(x1i |δθ) = −1 n1 n1∑ i=1 〈T (x1i ), δθ〉+ log 1 n2 n2∑ i=1\nexp{〈T (x2i ), δθ〉}︸ ︷︷ ︸ Ψ̂(δθ)\n(11)\nRemark 1 Note that the loss function (11) does not additively decompose over the samples. The second term in (11) is the logarithm over sum of a function of samples."
    }, {
      "heading" : "2.3 Optimization",
      "text" : "The optimization problem (1) has a composite objective with a smooth convex term corresponding to the loss function (11) and a a potentially non-smooth convex term corresponding to the regularizer. In this section, we present an algorithm in the class of Fast Iterative Shrinkage-Thresholding Algorithms (FISTA) for efficiently solving the problem (1) [3]. For convenience, we refer the loss function L(δθ;Xn11 ,X n2 2 ) as L(δθ) and we drop the subscript {n1, n2} of λn1,n2 .\nOne of the most popular methods for composite objective functions is in the class of FISTA where at each iteration we linearize the smooth term and minimize the quadratic approximation of the form\nQL(δθ, δθt) := L(δθ) + 〈δθ − δθt,∇L(δθt)〉+ L\n2 ‖δθ − δθt‖22 + λR(δθ), (12)\nwhere L denotes the Lipschitz constant of the loss function L(δθ). Ignoring constant terms in δθt, the unique minimizer of the above expression (12) can be written as\npL(δθt) = argmin δθ QL(δθ, δθt) = argmin δθ\nλR(δθ) + L\n2 ∥∥∥∥δθ − (δθt − 1L∇L(δθt) )∥∥∥∥2\n2\n= argmin δθ\nλ L R(δθ) + 1 2 ∥∥∥∥δθ − (δθt − 1L∇L(δθt) )∥∥∥∥2\n2\n. (13)\nIn fact, the updates of δθ is to compute certain proximal operators of the non-smooth term R(.). In general, the proximal operator proxh(x) of a closed proper convex function h : Rd 7→ R ∪ {+∞} [21] is defined as\nproxh(x) = argmin u\n( h(u) + 1\n2 ‖u− x‖22\n) . (14)\nThus, the unique minimizer (13) correspond to prox λ LR ( δθt − 1L∇L(δθt) ) which has rate of convergence ofO(1/t) [20, 21].\nTo improve the rate of convergence, we adapt the idea of FISTA algorithm [3]. The main idea is to iteratively consider the proximal operator prox(.) at a specific linear combination of the previous two iterates {δθt, δθt−1}\nξt+1 = δθt + αt+1 (δθt − δθt−1) , (15)\nAlgorithm 1 Generalized Direct Change Estimator Input: L0 > 0, Xn11 ,X n2 2\nStep 0. Set ξ1 = δθ0, t = 1 Step t. (t ≥ 1) Find the smallest non-negative integers it such that with L̃ = 2itLt−1\nL (pL̃(ξt)) +R (pL̃(ξt)) ≤ QL̃ (pL̃(ξt), ξt) . (17)\nSet Lt = 2itLt−1 and Compute\nδθt = prox λ LR\n( ξt − 1\nL ∇L(ξt)\n) (18)\nβt+1 = 1 + √ 1 + 4β2t 2\n(19)\nξt+1 = δθt + ( βt − 1 βt+1 ) (δθt − δθt−1) (20)\ninstead of just the previous iterate δθt. The choice of αt+1 follows Nesterov’s accelerated gradient descent [20, 21] and is detailed in Algorithm 1. The iterative algorithm simply updates\nδθt+1 = prox λ LR\n( ξt+1 − 1\nL ∇L(ξt+1)\n) . (16)\nThe algorithm has a rate of convergence of O(1/t2) [3]."
    }, {
      "heading" : "2.4 Regularization Function",
      "text" : "We assume that the optimal δθ∗ is sparse or suitably ‘structured’ where such structure can be characterized by having a low value according to a suitable norm R(δθ∗). In below, we provide a few examples of such a norm.\nL1 norm: One example for R(.) we will consider throughout the paper is the L1 norm regularization. We use L1 norm if only a few edges has changed (1st row in Figure 1). In particular, we consider R(δθ) = ‖δθ‖1 if number of non-zeros entries in δθ∗ is s < p2. The prox λ\nL‖.‖1 (.) is given by the elementwise soft-thresholding operation [24] as[\nprox λ L‖.‖1 ] i (z) = sign(zi).max(0, zi − λ L ). (21)\nGroup-sparse norm: Another popular example we consider is the group-sparse norm. We use group lasso norm if a group of edges has changed (2nd row in Figure 1). For some kinds of data, it is reasonable to assume that the variables can be clustered (or grouped) into types, which share similar connectivity or correlation patterns. Let G = {G1,G2, · · · ,GNG} denote a collection of groups, which are subsets of variables. We assume that δΘ∗(s, t) = 0 for any variable s ∈ Gg and for any variable t ∈ Gh. In the group sparse setting for any subset SG ⊆ {1, 2, · · ·NG} with cardinality |SG | = sG , we assume that the parameter δΘ∗ satisfies {δΘ∗s,t = 0 : s, t ∈ Gg & g 6∈ SG}. We will focus on the case when R(δΘ) = ∑NG g=1 ‖δΘ(s, t) : s, t ∈ Gg‖F [15]. Let δΘGg bd the sub-matrix of δΘ covering nodes in Gg . Proximal operator is given by the group specific soft-thresholding operation.[ prox λ\nLR ] g (δΘ) = max(‖δΘGg‖F − λL , 0) ‖δΘGg‖F . (22)\nNode perturbation: Another example is the row-column overlap norm (RCON) [18] to capture perturbed nodes i.e., nodes that have a completely different connectivity pattern to other nodes among two networks (3rd row in Figure 1). A special case of RCON we are interested is ∑p i=1 ‖Vi‖q where δΘ = V + V T , and Vi is the i−th column of matrix V . This norm can be viewed as overlapping group lasso [18] and thus can be solved by applying Algorithm 1 with proximal operator for overlapping group lasso [35]. Also, we can write problem (1) as a constrained optimization\nargmin δΘ,V\nL(δΘ;Xn11 ,X n2 2 ) + λ1δΘ‖1 + λn1,n2 p∑ i=1 ‖Vi‖q\ns.t. δΘ = V + V T , (23)\nand solve it by applying in-exact ADMM techniques [18]."
    }, {
      "heading" : "3 Statistical Recovery for Generalized Direct Change Estimation",
      "text" : "Our goal is to provide non-asymptotic bounds on ‖∆‖2 = ‖δθ∗ − δθ̂‖2 between the true parameter δθ∗ and the minimizer δθ̂ of (1). In this section, we describe various aspects of the problem, introducing notations along the way, and highlight our main result."
    }, {
      "heading" : "3.1 Background and Assumption",
      "text" : "Gaussian Width: In several of our proofs, we use the concept of Gaussian width [6, 8], which is defined as follows.\nDefinition 1 For any set A ∈ Rp, the Gaussian width of the set A is defined as:\nw(A) = Eg [ sup u∈A 〈g, u〉 ] . (24)\nwhere the expectation is over g ∼ N(0, Ip×p), a vector of independent zero-mean unit-variance Gaussian random variable.\nThe Gaussian widthw(A) provides a geometric characterization of the size of the setA. Consider the Gaussian process {Zu} where the constituent Gaussian random variables Zu = 〈u, g〉 are indexed by u ∈ A, and g ∼ N(0, Ip×p). Then the Gaussian width w(A) can be viewed as the expectation of the supremum of the Gaussian process {Zu}. Bounds on the expectations of Gaussian and other empirical processes have been widely studied in the literature, and we will make use of generic chaining for some of our analysis [4, 11, 29, 30].\nThe Error Set: Consider solving the problem (1), under assumption λn1,n2 > βR∗ (∇L(δθ∗;X n1 1 ,X n2 2 )), where β > 1 and R∗(.) is the dual norm of R(.). Banerjee et al. [1] show that for any convex loss function the error vector ∆ = (δθ∗ − δθ̂) lies in a restricted set that is characterized as\nEr = Er(δθ ∗, β) = { ∆ ∈ Rp ∣∣∣∣ R(δθ∗ + ∆) ≤ R(δθ∗) + 1βR(∆) } . (25)\nRestricted Strong Convexity (RSC) Condition: The sample complexity of the problem (1) depends on the RSC condition [19], which ensures that the estimation problem is strongly convex in the neighborhood of the optimal parameter [1, 19]. A convex loss function satisfies the RSC condition in Cr = cone(Er), i.e., ∀∆ ∈ Cr, if there exists a suitable constant κ such that\nδL(δθ∗, u) := L(δθ∗ + u)− L(δθ∗)− 〈∇L(δθ∗), u〉 ≥ κ‖u‖22 (26)\nDeterministic Recovery Bounds: If the RSC condition is satisfied on the error set Cr and λn1,n2 satisfies the assumptions stated earlier, for any norm R(.), Banerjee et al. [1] show a deterministic upper bound for ‖∆‖2 in terms of λn1,n2 , κ, and the norm compatibility constant Ψ(Cr) = supu∈Cr R(u) ‖u‖2 , as\n‖∆‖2 ≤ 1 + β\nβ λn1,n2 κ Ψ(Cr) . (27)\nSmooth Density Ratio Model Assumption: For any vector u such that ‖u‖2 ≤ ‖δθ∗‖2 and every ∈ R, the following inequality holds:\nEX∼p(X|θ2)[exp{ r(X|δθ ∗ + u)− 1}] ≤ exp{ 2}.\nA similar assumption is used in the analysis of Liu et al. [13].\nRemark 2 Bounded density ratio is a special case satisfying the smooth density ratio assumption. Lemma 1 shows a sufficient condition under which the density ratio is bounded.\nLemma 1 Consider two Ising Model with true parameters θ∗1 and θ∗2 . Let d1, d2 s where ‖θ∗1‖0 = d1, ‖θ∗2‖0 = d2, and ‖δθ∗‖0 = s. Assume\nmin i,j=1···p\n(|θ∗1(i, j)|) ≥ 1 d1 − 1 − c1 (d1 − 1)s (28)\nmin i,j=1···p\n(|θ∗2(i, j)|) ≥ 1 d2 − 1 − c2 (d2 − 1)s , (29)\nwhere c1 and c2 are positive constants. Then the density ratio r(X = x|δθ∗) is bounded.\nNote that if individual graphs are dense, then the conditions (28) and (29) are satisfied and as a result the smooth density ratio is satisfied.\nRemark 3 In this paper, we focus on the Ising graphical model. But, our statistical analysis holds for any graphical models that satisfy the above mentioned assumption. Through our analysis, no assumption is required on the individual graphical models."
    }, {
      "heading" : "3.2 Bounds on the regularization parameter",
      "text" : "To get the recovery bound (27) above, one needs to have λn1,n2 ≥ βR∗ (∇L(δθ∗;X n1 1 ,X n2 2 )). However, the bound on λn1,n2 depends on unknown quantity δθ ∗ and the samples Xn11 ,X n2 2 and is hence random. To overcome the above challenges, one can bound the expectation E[R∗ (∇L(δθ∗;Xn11 ,X n2 2 ))] over all samples of size n1 and n2, and obtain high-probability deviation bounds. The goal is to provide a sharp bound on λn1,n2 since the error bound in (27) is directly proportional to λn1,n2 .\nIn theorem 1, we characterize the expectation E[R∗ (∇L(δθ∗;Xn11 ,X n2 2 ))] in terms of the Gaussian width of the unit norm-ball of R(.), which leads to a sharp bound. The upper bound on Gaussian width of the unit norm-ball of R for atomic norms which covers a wide range of norms is provided in [6, 7].\nTheorem 1 Define ΩR = {u : R(u) ≤ 1}. Let φ(R) = supu ‖u‖2 R(u) . Assume that for any u that ‖u‖ ≤ ‖θ ∗‖\n1 2 λmax\n( ∇2L(δθ∗ + u) ) ≤ η0, (30)\nwhere λmax(.) is the maximum eigenvalue. Then under the smooth density ratio assumption, we have\nE [R∗(∇L(δθ∗;Xn11 ,X n2 2 ))] ≤\n2 √ η0(c1w (ΩR) + φ(R))√\nmin(n1, n2) .\nand with probability at least 1− c2e− 2\nR∗ (∇L(δθ∗;Xn11 ,X n2 2 )) ≤ c2(1 + )w(ΩR) + τ1√ min(n1, n2) .\nwhere c1 and c2 are positive constants, τ1 = 2 √ η0φ(R), and w(ΩR) is the Gaussian width of set ΩR.\nNote, that our analysis hold for any norm and it is expressed in terms of the Gaussian width. In the following, we give the bound on the regularization parameter for two examples of the regularization function R(.).\nCorollary 1 If R(δθ) is the L1 norm, and δθ ∈ Rp 2 then with high probability we have the bound\nR∗ (∇L(δθ∗;Xn11 ,X n2 2 )) ≤\nη2 √ log p√ min(n1, n2) . (31)\nCorollary 2 If R(δθ) is the group-sparse norm, and δθ ∈ Rp2 then with high probability we have the bound\nR∗ (∇L(δθ∗;Xn11 ,X n2 2 )) ≤\nη2 √ m+ logNG√\nmin(n1, n2) , (32)\nwhere G = {G1, · · · ,GNG} is a collection of groups, m = maxi |Gi| is the maximum size of any group."
    }, {
      "heading" : "3.3 RSC Condition",
      "text" : "In this Section, we establish the RSC condition for direct change detection estimator (1). Simplifying the expression and applying mean value theorem twice on the left side of RSC condition (26), for ∀γi ∈ [0, 1], we have\nδL(δθ∗, u) := L(δθ∗ + u)− L(δθ∗)− 〈∇L(δθ∗), u〉 ≥ uT∇2L(δθ∗ + γiu)u. (33)\nThus, the RSC condition depends on the non-linear terms of loss function. Recall that the nonlinear term, second term, in Loss function (1) which is the approximation of the log-partition functions only depends on n2. As a results, only samples of Xn22 affect the RSC conditions. Our analysis is an extension of the results on [1] using the generic chaining. We show that, with high probability the RSC condition is satisfied once samples n2 crosses w2(Cr ∩ Sd−1) the Gaussian width of restricted error set. The bound on Gaussian width of the error set for atomic norms has been provided in [7].\nLet ri = r(X = x2i |δθ∗) and ε̄ denote the probability that ri exceeds some constant η0: ε̄ = p(ri > η0) ≥ 1− e− η20 2 .\nTheorem 2 Let X ∈ Rn×p be a design matrix with independent isotropic sub-Gaussian rows with |||Xi|||Ψ2 ≤ κ. Then, for any set A ⊆ Sp−1, for suitable constants η, c1, c2 > 0 with probability at least 1−exp(−ηw2(A)), we have\ninf u∈A\n∂L(θ∗;u,X) ≥ c1ρ2 (\n1− c2κ21 w(A) √ n2\n) − τ (34)\nwhere κ1 = κε̄ , ρ 2 = infu∈A ρ 2 u with ρ 2 u = E [〈 u, T (X2i ) 〉2 I(ri > η0)], and τ is smaller than the first term in right hand side. Thus, for n2 ≥ c2w2(A), with probability at least 1− exp(−ηw2(A)), we have infu∈A ∂L(θ∗;u,X) > 0."
    }, {
      "heading" : "3.4 Statistical Recovery",
      "text" : "With the above results in place, from (27), Theorem 3 provides the main recovery bound for generalized direct change estimator (1).\nTheorem 3 Consider two set of i.i.d samples Xn11 = {x1i } n1 i=1 and X n2 2 = {x2i } n2 i=1. Define ΩR = {u : R(u) ≤ 1}. Assume that δθ̂ is the minimizer of the problem (1). Then, with probability at least 1− η0e− 2 the followings hold\nλn1,n2 ≥ η1√\nmin(n1, n2) (w(ΩR) + ) (35)\nand for n2 ≥ cw2(Cr ∩ Sd−1), with high probability, the estimate δθ̂ satisfies\n‖∆‖2 ≤ O\n( w(ΩR)√\nmin(n1, n2)\n) Ψ(Cr) , (36)\nwhere w(.) is the Gaussian width of a set,and c2, η0, and η1 are positive constants.\nProof: Proof of the Theorem can be directly obtain as the results of (27) and Theorem 1 and Theorem 2.\nIn the following, we provide the recovery bound for two special cases as an example.\nCorollary 3 IfR(δθ) is the L1 norm, δθ∗ ∈ Rp 2 s s-sparse., Ψ(Cr) ≤ 4 √ s, and for n2 > cs log p, the recovery error is bounded by\n‖∆‖2 ≤ c3 Ψ(Cr)λn1,n2\nκ = O\n(√ s log p\nmin(n1, n2)\n) .\nCorollary 4 If R(δθ) is the group-sparse norm, δθ ∈ Rp2 , Ψ(Cr) ≤ 4 √ sG and for n2 ≥ c(msG + sG logNG), the recovery error is bounded by\n‖∆‖2 ≤ c3 Ψ(Cr)λn1,n2\nκ = O\n(√ sGm+ logNG\nmin(n1, n2)\n) ."
    }, {
      "heading" : "4 Experiments",
      "text" : "In this Section, we evaluate generalized direct change estimator (direct) with three different norms. and we compare our direct approach with indirect approach. For indirect approach, we first estimate Ising model structures θ̂1 and θ̂2 with L1 norm regularizer, separately [22]. Then, we obtain δθ̂ = θ̂1 − θ̂2. In all experiments, we draw n1 and n2 i.i.d samples from each Ising model by running Gibbs sampling. Here we set n = n1 = n2 = {20, 50, 100}.\nL1 norm: Here we first generate θ∗1 with three disconnected star sub-graphs (Figure 4-a) with p = 50. We generate the weights uniformly random between {0.3 − 0.5}. We then generate θ∗2 by removing 10 random edges from θ∗1 (Figure 4-b). It is interesting that although individual graphs are sparse, but direct approach has a better ROC curve for all values of n (Figure 4-d). Similar results obtained by with random graph structure of θ∗1 and θ ∗ 2 .\nGroup-sparse norm: In this set of experiments, we evaluate direct method with three different structure for θ∗1 : (i) a random graph structure (Figure 4-e), (ii) scale free graph structure (Figure 4-i), and (iii) a cluster graph structure (Figure 4-m). In all settings, we set p = 60 and generate θ∗2 by removing a block of edges from θ ∗ 1 (Figure 4-(f,j,n)).\nFor random graph structure and block structure, direct method has a better ROC curve (Figure 4-h,p). But, for scalefree structure, since the individual graphs are sparse, indirect method can estimate θ̂1 and θ̂2 correctly, and thus have a better ROC curve (Figure 4-l).\nNode perturbation: Here, we first generate a random graph structure θ∗1 , and then generate θ∗2 by perturbing two nodes in θ∗1 . Here we set p = 60 and generate θ ∗ 2 by setting rows and columns 3, 51 to zero in θ ∗ 1 (Figure 4-s). Although, the individual graphs are dense but direct approach can estimate edges in δθ with only n = 20 samples (Figure 4-t)."
    }, {
      "heading" : "5 Conclusion",
      "text" : "This paper presents the statistical analysis of direct change problem in Ising graphical models where any norm can be plugged in for characterizing the parameter structure. An optimization algorithm based on FISTA-style algorithms is proposed with the convergence rate of O(1/T 2). We provide the statistical analysis for any norm such as L1 norm, group sparse norm, node perturbation, etc. Our analysis is based on generic chaining and illustrates the important role of Gaussian widths (a geometric measure of size of suitable sets) in such results. For the special case of sparsity, we obtain a sharper result than previous results [13] under the same smooth density ratio assumption. Liu et al. [13] obtained the same result with a bounded density ratio assumption which is a more restrictive assumption. Although, we presented the results for Ising model, our analysis can be applied to any graphical model which satisfies the smooth density ratio assumption. Further, we extensively compared our generalized direct change estimator with an indirect approach over a wide range of graph structures and norms. We show that our direct approach has a better ROC curve than indirect approach without any assumption on the structure of individual graphs. We implemented indirect approach by estimating individual Ising model structures with L1 norm regularizer. However, if individual graphs has a suitable structure such as group sparsity, one may apply a regularization that can characterize the graph structure and may improve performance of the indirect approach. We will investigate this possibility in our future research.\nAppendix"
    }, {
      "heading" : "A Background and Preliminaries",
      "text" : "Definition 2 Sub-Gaussian random variable: We say that a random variable x is sub-Gaussian if the moments satisfies\n[E|x|p] 1 p ≤ K2 √ p (37)\nfor any p ≥ 1 with a constant K2. The minimum value of K2 is called sub-Gaussian norm of x, denoted by |||x|||ψ2 . If E[x] = 0, then\nE[exp{tX}] ≤ exp{Ct2|||X|||2Ψ2}, (38)\nwhere C and c are positive constant.\nDefinition 3 Sub-Gaussian random vector: We say that a random vector X in Rn is sub-Gaussian if the onedimensional marginals 〈X,x〉 are sub-Gaussian random variables for all x ∈ Rn. The sub-Gaussian norm of X is defined as\n|||X|||ψ2 = sup x∈Sn−1 ‖〈X,x〉‖ψ2 (39)\nLemma 2 Consider a sub-Gaussian vector X ∈ Rn with |||X|||Ψ2 < K, then for any vector u, 〈X,u〉 is a subGaussian variable with |||〈X,u〉||| < K‖u‖2.\nProof: The argument is based on Definition 3 as follows,\n|||〈X,u〉|||Ψ2 = ‖u‖2 ∣∣∣∣∣∣∣∣∣∣∣∣〈X, u‖u‖2 〉∣∣∣∣∣∣∣∣∣∣∣∣ Ψ2 ≤ ‖u‖2 sup x∈Sn−1 〈X,x〉 = ‖u‖2|||X|||Ψ2 ≤ K‖u‖2. (40)\nLemma 3 Let X1 and X2 be centered sub-Gaussian random variables with |||X1|||Ψ2 = b1 and |||X2|||Ψ1 = b2. Then X1 +X2 is centered sub-Gaussian with |||X1 +X2|||Ψ2 = b1 + b2.\nProof: The argument is based on the definition of moment generating function of sub-Gaussian random variable:\nUsing Holder inequality for any p, q > 0 where 1p + 1 q = 1, we have\nE[exp{t(X1 +X2)}] ≤ (E[exp{tX1}p])1/p(E[exp{tX1}q])1/q\n≤ exp{Ct2(pb21 + qb22)} = exp{Ct2(pb21 + p\n1− p b22)}. (41)\nThe minimum of (41) occurs with p = b2b1 . As a result, we have\nE[exp{t(X1 +X2)}] ≤ exp{Ct2(b1 + b2)2}. (42)\nThe proof is complete.\nA.1 Generic Chaining\nDefinition 4 (Majorizing measure [27]) Given α > 0, and a metric space (T, d) (that need not be finite), we define\nγα(T, d) = inf sup t ∑ n≥0 2n/α∆(An(T )). (43)\nwhere the infimum is taken over all admissible sequences and ∆(An(T )) is the diameter of An(t).\nNote that γ2(T, ‖.‖2) coincides with the Gaussian width of T : w(T ).\nLemma 4 Given a metric space (T, d), we have\nγ1(T, ‖.‖∞) ≤ γ22(T, ‖.‖2). (44)\nProof: Define d2(s, t) = ‖s− t‖2 and d1(s, t) = ‖s− t‖∞. We use the traditional definition of majorizing measure γα,1(T, d) from [27]\nγα,1(T, d) = inf sup t (∫ ∞ 0 ( log\n1\nµ(Bd(t, ε))\n)1/α dε ) . (45)\nwhere Bd(t, ε) is the closed ball of center t and radius ε based on the distance d and the infimum is taken over all the probability measure µ on T .\nNote that γα,1(T, d) coincides with the functional γα(T, d) [28] as\nK(α)−1γα(T, d) ≤ γα,1(T, d) ≤ K(α)γα(T, d), (46)\nwhere K(α) is a constant depending on α only.\nAs a result, it is enough to show that γ1,1(T, d1) ≤ γ22,1(T, d2).\nNote that since for any vector x, we have ‖x‖∞ ≤ ‖x‖2, therefore, for any probability measure µ and t, we have µ(Bd1(t, ε)) ≥ µ(Bd2(t, ε)). As a result,(∫ ∞\n0\n( log\n1\nµ(Bd1(t, ε))\n) dε ) ≤ (∫ ∞\n0\n( log\n1\nµ(Bd2(t, ε))\n) dε ) ≤ (∫ ∞ 0 ( log\n1\nµ(Bd2(t, ε))\n)1/2 dε )2 . (47)\nSince (47) holds for any µ and t, we have\nγ1,1(T, d1) = inf sup t (∫ ∞ 0 ( log\n1\nµ(Bd1(t, ε))\n) dε ) ≤ inf sup\nt (∫ ∞ 0 ( log\n1\nµ(Bd2(t, ε))\n)1/2 dε )2 = γ22,1(T, d2). (48)\nThis completes the proof.\nTheorem 4 [Theorem 1.2.7] in [29] Consider a set T provided with two distances d1 and d2. Consider a process (Xt)t∈T that satisfies E[Xt] = 0 and\nP (|Xs −Xt| ≥ u) ≤ 2 exp ( −min ( u2\nd2(s, t)2 ,\nu\nd1(s, t)\n)) . (49)\nThen\nE[ sup t,s∈T\n|Xs −Xt| ≤ L(γ1(T, d1) + γ2(T, d2)), (50)\nwhere L is a constant.\nTheorem 5 [Theorem 1.2.9] in [29] Under the conditions of Theorem 4, for all values of u1, u2 > 0 we have\nP (|Xs −Xt0| ≥ L(γ1(T, d1) + γ2(T, d2)) + u1D1 + u2D2) ≤ L exp(−min(u22, u1)), (51)\nwhere Dj = 2 ∑ n≥0 en(T, dj). Note that Dj ≤ Lγj(T, dj).\nTheorem 6 [Theorem 8.2 (Fernique-Talagrand’s comparison theorem)] in [32] Let T be an arbitrary set. Consider a Gaussian random process (G(t))t∈T and a sub-Gaussian random process (H(t))t∈T . Assume that E[G(t)] = E[H(t)] = 0 for all t ∈ T . Assume also that for some M > 0, the following increment comparison holds:\n|||H(s)−H(t)|||ψ2 ≤M(E[‖G(s)−G(t)‖ 2 2) 1/2 ∀s, t ∈ T. (52)\nThen\nE[supt∈TH(t)] ≤ CME[supt∈TG(t)]. (53)\nTheorem 7 (Mendelson, Pajor, Tomczak-Jaegermann [17]) There exist absolute constants c1, c2, c3 for which the following holds. Let (Ω, µ) be a probability space, set F be a subset of the unit sphere of L2(µ), i.e., F ⊆ SL2 = {f : |||f |||L2 = 1}, and assume that supf∈F |||f |||ψ2 ≤ κ. Then, for any θ > 0 and n ≥ 1 satisfying\nc1κγ2(F, |||·|||ψ2) ≤ θ √ n , (54)\nwith probability at least 1− exp(−c2θ2n/κ4),\nsup f∈F ∣∣∣∣∣ 1n n∑ i=1 f2(Xi)− E [ f2 ]∣∣∣∣∣ ≤ θ . (55)\nFurther, if F is symmetric, then\nE [ sup f∈F ∣∣∣∣∣ 1n n∑ i=1 f2(Xi)− E [ f2 ]∣∣∣∣∣ ] ≤ c3 max { 2κ γ2(F, |||·|||ψ2)√ n , γ22(F, |||·|||ψ2) n } (56)"
    }, {
      "heading" : "B Regularization Parameter",
      "text" : "Lemma 5 Consider two Ising Model with true parameters θ∗1 and θ∗2 . Let d1, d2 s where ‖θ∗1‖0 = d1, ‖θ∗2‖0 = d2, and ‖δθ∗‖0 = s. Assume\nmin i,j=1···n1\n(|θ∗1(i, j)|) ≥ 1 d1 − 1 − c1 (d1 − 1)s (57)\nmin i,j=1···n2\n(|θ∗2(i, j)|) ≥ 1 d2 − 1 − c2 (d2 − 1)s , (58)\nwhere c1 and c2 are positive constants. Then the density ratio r(X = x|δθ∗) is bounded.\nProof: Let α1 ≤ |θ∗1 | ≤ β1 and α2 ≤ |θ∗2 | ≤ β2. Without loss of generality, assume that ‖θ∗1‖2 = 1 and ‖θ∗2‖2 = 1.\nSo,\nβ1 ≤ 1− (d1 − 1)α1 (59) β2 ≤ 1− (d2 − 1)α2. (60)\nBased on triangle inequality of norms, we have\n‖δθ∗‖∞ = ‖θ∗1 − θ∗2‖∞ ≤ ‖θ∗1‖∞ + ‖θ∗2‖∞ ≤ β1 + β2 ≤ 2− (d1 − 1)α1 − (d2 − 1)α2. (61)\nLet z = T (x), then,\n|〈z, δθ∗〉| ≤ ‖z‖∞‖δθ∗‖1 (62) ≤ s‖δθ∗‖∞ (63) ≤ 2s− [(d1 − 1)α1 − (d2 − 1)α2]s (64)\nwhere the second inequality is the result of ‖z‖∞ ≤ 1 since z comes from an Ising model.\nNote that if\nα1 ≥ s− c1\n(d1 − 1)s =\n1 d1 − 1 − c1 (d1 − 1)s , (65)\nthen\ns− (d1 − 1)α1s ≤ c1. (66)\nSimilarly, if\nα2 ≥ s− c2\n(d2 − 1)s =\n1 d2 − 1 − c2 (d2 − 1)s , (67)\nthen\ns− (d2 − 1)α2s ≤ c2. (68)\nAs a result, we have\n|〈z, δθ∗〉| ≤ c1 + c2. (69) ⇒ exp{〈z, δθ∗〉} ≤ exp c1 + c2 ≤ c0. (70)\nFor example, if c1 = c2 = 1, then c0 = 10.\nTherefore,\nr(X = x|δθ) = exp{〈z, δθ〉)} Z(δθ∗) ≤ c0 Z(δθ∗) . (71)\nThis completes the proof.\nAssumption 1(Smooth Density Ratio Model Assumption) For any vector u such that ‖u‖2 ≤ ‖δθ∗‖2 and every t ∈ R, the following inequality holds:\nEX∼p(X|θ2)[exp{tr(X|δθ ∗ + u)− 1}] ≤ exp{t2}. (72)\nLemma 6 For any constant τ ≤ 1, define random event Mτ as follows, Mτ = {Ψ(δθ∗ + u)−Ψ(δθ∗)− [ Ψ̂(δθ∗ + u)− Ψ̂(δθ∗) ] ≤ τ}. (73)\nThen, for any vector u such that ‖u‖2 ≤ ‖δθ∗‖2, under Assumption 1, we have\nP (M cτ ) = p ( Ψ(δθ∗ + u)−Ψ(δθ∗)− [ Ψ̂(δθ∗ + u)− Ψ̂(δθ∗) ] > τ ) ≤ 4e− n2 5 τ 2 . (74)\nProof: Recall that\nr(X = x|δθ∗) = exp{〈T (x), δθ ∗〉}\nZ(δθ∗)\n⇒ Ẑ(δθ∗) = 1 n2 n2∑ i=1 exp{〈T (x2i ), δθ∗〉} = 1 n2 n2∑ i=1 r(X = x2i |δθ)Z(δθ∗) ⇒ Ẑ(δθ ∗)\nZ(δθ∗) =\n1\nn2 n2∑ i=1 r(X = x2i |δθ∗) (75)\nNote that Z(δθ) = EX∼p(X|θ2)[exp{〈T (x), δθ∗〉}], therefore,\nEX∼p(X|θ2)[r(X|δθ ∗)] = 1. (76)\nUnder the Assumption 1, we have\np (∣∣r(X = x2i |δθ∗)− 1∣∣ > ) ≤ c1e− 2 . (77)\nApplying Hoeffding inequality, we have\np (∣∣∣∣∣ 1n2 n2∑ i=1 r(X = x2i |δθ∗)− 1 ∣∣∣∣∣ ≥ ) ≤ 2e− 2\n(78)\n⇒ p (∣∣∣∣∣ Ẑ(δθ∗)Z(δθ∗) − 1 ∣∣∣∣∣ ≥ ) ≤ 2e−n2 2 . (79)\nTaking the logarithm from both side, and using one side bound, we have\np ( log Ẑ(δθ∗)\nZ(δθ∗) ≥ log( + 1)\n) ≤ e−n2 2\n(80)\n⇒p ( Ψ̂(δθ∗)−Ψ(δθ∗) ≥ log( + 1) ) ≤ e−n2 2 . (81)\nSimilarly, we have\np ( Ψ(δθ∗ + u)− Ψ̂(δθ∗ + u) ≥ − log(1− ) ) ≤ e−n2 2 . (82)\nApplying the union bound, we have\np ( Ψ(δθ∗ + u)−Ψ(δθ∗)− [ Ψ̂(δθ∗ + u)− Ψ̂(δθ∗) ] ≥ log 1 +\n1−\n) ≤ 4e−n2 2 . (83)\nSetting τ = log 1+ 1− , we have p ( Ψ(δθ∗ + u)−Ψ(δθ∗)− [ Ψ̂(δθ∗ + u)− Ψ̂(δθ∗) ] ≥ τ ) ≤ 4e−n2( eτ+1 eτ−1 ) 2 ≤ 4e− n2 5 τ 2 , (84)\nwhere the last inequality is obtained by using the fact that for any τ ≤ 1( eτ + 1\neτ − 1\n)2 > τ2\n5 . (85)\nThis completes the proof.\nLemma 7 Define random event Mt̃ as follows, Mt̃ = {Ψ(δθ∗ + tu)−Ψ(δθ∗)− [ Ψ̂(δθ∗ + tu)− Ψ̂(δθ∗) ] ≤ t̃}, (86) where t̃ = √ 5η1t+ √ 5 2 . Let Z = T (X 1) and z = T (x1). Then,\nP (∣∣∣〈z−∇Ψ̂(θ∗),u〉∣∣∣ ≥ ∣∣Mt̃) ≤ c exp{ − 24η0‖u‖22 }, (87)\nwhere 12λmax ( ∇2Ψ̂(θ∗ + ũ) ) ≤ η0 and c is a positive constant.\nProof: First, note that p(X = x|θ∗1) = p(X = x|θ∗2)r(X = x|δθ∗). Therefore,\nEX∼p(X|θ∗1 )\n[ e〈Z,tu〉 ] = ∑ x∈X e〈z,tu〉p(x|θ∗1)\n= ∑ x∈X e〈z,tu〉p(x|θ∗2)r(x|δθ∗)\n= e−Ψ(δθ ∗) ∑ x∈X e〈z,tu+δθ ∗〉p(x|θ∗2) = eΨ(δθ ∗+tu)−Ψ(δθ∗), (88)\nsince r(x|δθ∗) = exp{x, δθ∗〉 −Ψ(δθ∗)}, and Ψ(δθ∗) = log ∑\nx∈X e 〈x,δθ∗〉p(x|θ∗2).\nAlso, using the Taylor expansion, we have Ψ̂(δθ∗ + tu)− Ψ̂(δθ∗)− 〈 ∇Ψ̂(δθ∗), tu 〉 = 1\n2 tuT∇2Ψ̂(δθ∗ + ũ)tu\n≤ 1 2 t2‖u‖22λmax\n( ∇2Ψ̂(δθ∗ + ũ) ) ≤ t2η0‖u‖22 = t2η1, (89)\nwhere 12λmax ( ∇2Ψ̂(δθ∗ + ũ ) ≤ η0 and η1 = η0‖u‖22.\nThen, given the event Mt̃, the moment generating function of 〈 Z −∇Ψ̂(δθ∗),u 〉 can be bounded as,\nEX∼p(X|θ∗2 ) [ EX∼p(X|θ∗1 ) [ e〈Z−∇Ψ̂(δθ ∗),tu〉 ] ∣∣Mt̃] = EX∼p(X|θ∗2 ) [e〈−∇Ψ̂(δθ∗),tu〉 EX∼p(X|θ∗1 ) [e〈Z,tu〉] ∣∣Mt̃]\n(88) = EX∼p(X|θ∗2 )\n[ eΨ(δθ ∗+tu)−Ψ(δθ∗)−〈∇Ψ̂(δθ∗),tu〉∣∣Mt̃] (86) ≤ EX∼p(X|θ∗2 ) [ eΨ̂(δθ\n∗+tu)−Ψ̂(δθ∗)−〈∇Ψ̂(δθ∗),tu〉+√η1t∣∣Mt̃] (89) ≤ EX∼p(X|θ∗2 ) [ et 2η1+ √ 5η1t+ 1 2\n∣∣Mt̃] = et2η1+√5η1t+ 12 . (90) As a result, using the Chernoff bound, for any t > 0, we have\nP (〈 Z −∇Ψ̂(δθ∗),u 〉 ≥ ∣∣Mt̃) ≤ e−t EX∼p(X|θ∗2 ) [EX∼p(X|θ∗1 ) [e〈Z−∇Ψ̂(δθ∗),tu〉] ∣∣Mt̃]\n(90) ≤ exp{t2η1 + √ 5η1t+ 1\n2 − t }\n(a) ≤ exp{− ( − √ 5η1) 2\n4η1 +\n1 2 }\n≤ c exp{− 2\n4η0‖u‖22 }, (91)\nwhere the inequality (a) is obtained by setting t = − √ η1\n2η1 to minimize it with respect to t, and the last inequality\nobtained by setting c = exp{ 5 √\n5 2 √ η1 − 34} and using the fact that\nexp{− ( − √ 5η1) 2\n4η1 +\n1 2 } ≤ c exp{−\n2\n4η1 }. (92)\nSimilarly, we have P (〈 Z −∇Ψ̂(δθ∗),u 〉 ≤ − ∣∣Mt̃) ≤ e−t EX∼p(X|θ∗2 ) [EX∼p(X|θ∗1 ) [e〈Z−∇Ψ̂(δθ∗),−tu〉] ∣∣Mt̃] ≤ c exp{− 2\n4η0‖u‖22 }. (93)\nThis completes the proof.\nLemma 8 Under the smooth density ratio assumption, we have\nP (∣∣ 〈∇L(δθ∗;Xn11 ,Xn22 ),u〉 ∣∣ ≥ ) ≤ c1 exp{−min(n1, n2) 24η0‖u‖22 }, (94)\nwhere c1 is a positive constant.\nProof: Let t̃ = √ 5η1t+ √ 5 2 and t = −√η1 2η1 . Using the result of lemma 7 we have\nP (〈 T (x1i )−∇Ψ̂(δθ∗),u 〉 ≥ ∣∣Mt̃) = P (〈T (x1i )−∇Ψ̂(δθ∗),u〉 ≥ ∣∣Mt̃)\n≤ c exp{− 2\n4η0‖u‖22 }. (95)\nApplying Hoeffding inequality, we have\nP (∣∣ 〈∇L(δθ∗;Xn11 ,Xn22 ),u〉 ∣∣ ≥ ∣∣Mt̃) = P\n(〈 1\nn1 n1∑ i=1 T (x1i )−∇Ψ̂(δθ∗),u 〉 ≥ ∣∣Mt̃ )\n≤ c exp{− n1 2\n4η0‖u‖22 }. (96)\nMoreover, we can obtain,\nP (〈∇L(δθ∗;Xn11 ,X n2 2 ),u〉 ≤ − ) = P\n(〈 1\nn1 n1∑ i=1 T (x1i )−∇Ψ̂(δθ∗),u\n〉 ≥ )\n≤ P\n(〈 1\nn1 n1∑ i=1 T (x1i )−∇Ψ̂(δθ∗),u 〉 ≥ ∣∣Mt̃ ) P (Mt̃)\n+ P\n(〈 1\nn1 n1∑ i=1 T (x1i )−∇Ψ̂(δθ∗),u 〉 ≥ ∣∣M ct̃ ) P (M ct̃ )\n≤ c exp{ −n1 2 4η0‖u‖22 }+ 4 exp{ −n2 2 4η0‖u‖22 }\n≤ c1 exp{− min(n1, n2)\n2\n4η0‖u‖22 }, (97)\nwhere the last inequality is obtained by using Lemma 6 as follows\nP (M ct̃ ) ≤ 4 exp{− n2 5 t̃2} = 4 exp{−n2 5 ( √ 5η1t+ √ 5 2 )2}\n= 4 exp{−n2 5\n( √ 5η1 ′ −√η1\n2η1 +\n√ 5\n2 )2}\n= 4 exp{−n2 2\n4η1 },\nwhere η1 = η0‖u‖22 and setting c1 = max(4, c). Similarly,\nP (〈∇L(δθ∗;Xn11 ,X n2 2 ),u〉 ≥ ) = P\n(〈 1\nnp np∑ i=1 Zpi −∇Ψ̂(δθ ∗),u\n〉 ≤ − )\n≤ P\n(〈 1\nnp np∑ i=1 Zpi −∇Ψ̂(δθ ∗),u\n〉 ≤ − ∣∣Mt̃ ) P (Mt2)\n+ P\n(〈 1\nnp np∑ i=1 Zpi −∇Ψ̂(δθ ∗),u\n〉 ≤ − ∣∣M ct̃ ) P (M ct̃ )\n≤ c1 exp{− min(n1, n2)\n2\n4η0‖u‖22 } (98)\nThis completes the proof.\nTheorem 1 Define ΩR = {u : R(u) ≤ 1}. Let φ(R) = supu ‖u‖2 R(u) . Assume that for any u that ‖u‖ ≤ ‖θ ∗‖\n1 2 λmax\n( ∇2L(δθ∗ + u) ) ≤ η0, (99)\nwhere λmax(.) is the maximum eigenvalue. Then under the smooth density ratio assumption, we have\nE [R∗(∇L(δθ∗;Xn11 ,X n2 2 ))] ≤\n2 √ η0√\nmin(n1, n2) (c1w (ΩR) + φ(R)) . (100)\nand with probability at least 1− c2e− 2\nR∗ (∇L(δθ∗;Xn11 ,X n2 2 )) ≤ 1√ min(n1, n2) (c2(1 + )w(ΩR) + τ1) . (101)\nwhere c1 and c2 are positive constants, τ1 = 2 √ η0φ(R), and w(ΩR) is the Gaussian width of set ΩR.\nProof: Define µ = E[∇L(δθ∗;Xn11 ,X n2 2 )]. Using the triangle inequality, we have\nR∗ (∇L(δθ∗;Xn11 ,X n2 2 )) ≤ R∗ (∇L(δθ∗;X n1 1 ,X n2 2 )− µ) +R∗ (µ) . (102)\nWe upper bound two terms as follows. First, consider the first term.\nUsing the definition of dual norm, we have\nR∗ (∇L(δθ∗;Xn11 ,X n2 2 )− µ) = sup R(u)≤1 〈∇L(δθ∗;Xn11 ,X n2 2 )− µ,u〉 . (103)\nDefine stochastic process H(s) = 〈∇L(δθ∗;Xn11 ,X n2 2 )− µ, s〉 where E[H(s)] = 0. Then, from Lemma 8, we have\nP (H(s)−H(t) ≥ ) = P (〈∇L(δθ∗;Xn11 ,X n2 2 )− µ, s− t〉 ≥ ) (104)\n≤ c1 exp{− min(n1, n2)\n2\n4η0‖s− t‖22 }. (105)\nConsider the Gaussian process G(u) = 〈u, g〉, indexed by the same set, i.e., u ∈ ΩR, where g ∼ N(0, Id×d) is standard Gaussian vector. Now from definition sub-Gaussian random variables, we have\n|||H(s)−H(t)|||ψ2 ≤ 2 √ η0‖s− t‖2√\nmin(n1, n2) = KEg[‖G(s)−G(t)‖22]1/2, (106)\nwhere Eg[‖G(s)−G(t)‖22]1/2 = Eg[‖〈s− t, g〉‖22]1/2 = ‖s− t‖2, and K = 2 √ η0√\nmin(n1,n2) .\nNext, by applying the Fernique-Talagrand’s comparison theorem 6, we have\nE[ sup u∈ΩR H(u)] = E [ sup u∈ΩR 〈∇L(δθ∗;Xn11 ,X n2 2 ),u〉 ] ≤ c1KE[ sup\nu∈ΩR G(u)] = 2c1\n√ η0 w(ΩR)√ min(n1, n2) , (107)\nwhere c1 is a constant. Thus,\nE [R∗ (∇L(δθ∗;Xn11 ,X n2 2 )− µ)] ≤ c1 w(ΩR)√ min(n1, n2) . (108)\nTo get the concentration bound, we use the direct application of Theorem 2.2.27 in [30] and we have\nP ( sup\ns,t∈ΩR |H(s)−H(t)| ≤ c2(1 + ) w(ΩR)√ min(n1, n2)\n) ≥ 1− c2 exp ( − 2 ) . (109)\nThus, with probability at least 1− c2 exp ( − 2 ) ,\nR∗ (∇L(δθ∗;Xn11 ,X n2 2 )− µ) ≤ c2(1 + ) w(ΩR)√ min(n1, n2) . (110)\nNext, we consider the second term. First note that |||∇L(δθ∗;Xn11 ,X n2 2 )|||Ψ2 ≤\n2 √ η0‖u‖2√\nmin(n1,n2) . Using sub-Gaussian\nvariables property, we have\nE[L(δθ∗;Xn11 ,X n2 2 )] ≤ |||∇L(δθ∗;X n1 1 ,X n2 2 )|||Ψ2 ≤\n2 √ η0‖u‖2√\nmin(n1, n2) (111)\nUsing the definition of the dual norm, we have\nR∗(µ) = R∗ (E [∇L(δθ∗;Xn11 ,X n2 2 )]) = sup u∈ΩR E [〈∇L(δθ∗;Xn11 ,X n2 2 ),u〉] (112)\n≤ 2 √ η0√\nmin(n1, n2) sup u ‖u‖2 R(u)\n= 2 √ η0√\nmin(n1, n2) Φ(R), (113)\nwhere Φ(R) = supu ‖u‖2 R(u) .\nAlso, we have\nE [R∗(µ)] =≤ 2 √ η0√\nmin(n1, n2) Φ(R), (114)\nwhere Φ(R) = supu ‖u‖2 R(u) .\nThis completes the proof."
    }, {
      "heading" : "C RSC condition",
      "text" : "Let ri = r(X = x2i |δθ∗) and ε̄ denote the probability that ri exceeds some constant η0: ε̄ = p(ri > η0) ≥ 1− e− η20 2 .\nTheorem 2 Let X ∈ Rn×p be a design matrix with independent isotropic sub-Gaussian rows with |||Xi|||Ψ2 ≤ κ. Then, for any set A ⊆ Sp−1, for suitable constants η, c1, c2 > 0 with probability at least 1−exp(−ηw2(A)), we have\ninf u∈A\n∂L(θ∗;u,X) ≥ c1ρ2 (\n1− c2κ21 w(A) √ n2\n) − τ (115)\nwhere κ1 = κε̄ , ρ 2 = infu∈A ρ 2 u with ρ 2 u = E [〈 u, T (X2i ) 〉2 I(ri > η0)], and τ is smaller than the first term in right hand side. Thus, for n2 ≥ c2w2(A), with probability at least 1− exp(−ηw2(A)), we have infu∈A ∂L(θ∗;u,X) > 0.\nProof: Define Z = T (X) and zi = T (x2i ). Then,\nL(δθ;Xn11 ,X n2 2 ) = −1 n1 n1∑ i=1 〈T (x1i ), δθ〉+ log 1 n2 n2∑ i=1 exp{〈T (x2i ), δθ〉} (116)\n= −1 n1 n1∑ i=1 〈zi, δθ〉+ log 1 n2 n2∑ i=1 exp{〈zi, δθ〉}. (117)\nThrough the analysis, we consider that Z is centered random variable without loss of generality, since if it is not, the E[Z] will show up as a constant.\nRecall, RSC condition definition as\nδL(δθ∗,u) := L(δθ∗ + u;Xn11 ,X n2 2 )− L(δθ∗;X n1 1 ,X n2 2 )− 〈∇L(δθ∗;X n1 1 ,X n2 2 ),u〉 ≥ κ‖u‖22 (118)\nSimplifying the expression and applying mean value theorem twice on the left side of RSC condition (26), for ∀γi ∈ [0, 1], we have\nδL(δθ∗,u) := L(δθ∗ + u;Xn11 ,X n2 2 )− L(δθ∗;X n1 1 ,X n2 2 )− 〈∇L(δθ∗;X n1 1 ,X n2 2 ),u〉\n≥ uT∇2L(δθ̃;Xn11 ,X n2 2 )u, (119)\nwhere δθ̃ = δθ∗ + γiu. As a result, to show when the RSC condition is satisfied, it is enough to find a lower bound for the right side of the above equation.\nNote that\n∇2L(δθ̃;Xn11 ,X n2 2 ) = ∇2Ψ̂(δθ̃), (120)\nwhere\n∇2Ψ̂(δθ̃) = n2∑ i=1 σiz T i zi −  n2∑ j=1 σjzj T  n2∑ j=1 σjzj  , (121) and\nσi = exp{〈zi, δθ̃〉 − Ψ̂(δθ̃)} = exp 〈zi, δθ̃〉∑n2 j=1 exp 〈zj , δθ̃〉 . (122)\nPutting (121) back in (119), we have\nδL(δθ∗,u) ≥ n2∑ i=1\nσi〈u, zi〉2︸ ︷︷ ︸ A −\n〈 u,\nn2∑ j=1 σjzj 〉2 ︸ ︷︷ ︸\nB\n. (123)\nTo show the RSC condition, we need to show that (123) is strictly positive. First, we obtain the sample complexity so that A is far away from zero, then we show that A is strictly greater than B. This is enough to obtain the sample complexity so that the RSC condition is satisfied.\ni. Lower bound on A: Here, we explain how to get a lower bound on infu∈A ∑n2 i=1 σi〈u, zi〉2.\nLet ri = r(X = x2i |δθ∗), and sr = ∑n2 j=1 rj , then σi = ri sr . Then, we have\nn2∑ i=1 σi〈u, zi〉2 = 1 sr n2∑ i=1 ri〈u, zi〉2 . (124)\nThen, we have\np ( inf u∈A 1\nsr n2∑ i=1 ri〈u, zi〉2 < η0 η1 ρ2 ( 1− cκ21 w(A) √ n2 )) ≤ p( 1 sr < 1 η1n2 )\n+ p ( inf u∈A n2∑ i=1 ri〈u, zi〉2 < η0n2ρ2 ( 1− cκ21 w(A) √ n2 )) .\n(125)\nFirst, we give a bound for the first term. Note that EX∼p(X|θ2)[r(X|δθ∗)] = 1. From the smooth density ratio model assumption, we have\np(|ri − 1| > t) ≤ 2e− t2 2 . (126)\nApplying Hoeffding inequality in (130), we have\np(| 1 n2 sr − 1| ≥ t) = p(| 1 n2 n2∑ j=1 rj − 1| ≥ t) ≤ 2e− n2t 2 2 , (127)\n⇒ p(sr ≥ η1n2) ≤ e− n2(η1−1)\n2\n2 , (128)\n⇒ p( 1 sr ≤ 1 η1n2 ) = p(sr ≥ η1n2) ≤ e− n2(η1−1)\n2\n2 , (129)\nwhere η1 = t+ 1.\nNext, we focus on bounding the second term in (125). Recall that,\np(|ri − 1| > t) ≤ 2e− t2 2 , (130)\n⇒ ε̄1 = p(ri ≥ η0) ≥ 1− e− (1−η0)\n2\n2 , (131)\nwhere the last inequality holds for any η0 = 1− t.\nFor any fixed η0, let W̄i = W̄ui = 〈u, zi〉I(ri > η0). Then, the probability distribution over W̄i can be written as:1\np(W̄i = w) = p(〈u, zi〉 = w)I(ri > η0) p(ri > η0) ≤ 1 ε̄1 p(〈u, zi〉 = w) . (132)\nAs a result, ∣∣∣∣∣∣W̄i∣∣∣∣∣∣ψ2 ≤ κε̄1 = κ1. Thus, W̄i = W̄ui is a sub-Gaussian random variable for any u ∈ A. Let ρ2u = E[(W̄i u )2] > 0. For convenience of notation, let Z0 be i.i.d. as the rows zi, i = 1, . . . , n. Let A ⊆ Sp−1. Consider the following class of functions:\nF = {fu,u ∈ A : fu(.) = 1\nρu 〈·,u〉I(r(.|δθ∗) ≥ η0) : u ∈ A}. (133)\nThen for any fu ∈ F , fu(Z0) = 1ρu 〈Z0,u〉I(ri ≥ η0) and, by construction, F is a subset of the unit sphere, since for fu ∈ F\n‖fu‖2L2 = 1\nρ2u E[〈Z0,u〉2I(ri ≥ η0)] = 1. (134)\nFurther, supfu∈F |||fu|||ψ2 ≤ κ1/2.\nNext, we show that for the current setting, the γ2-functional can be upper bounded by w(A), the Gaussian width of A. Since the process is sub-Gaussian with ϕ2-norm bounded by κ1, we have\nγ2(F ∩ SL2 , |||·|||ψ2) ≤ κ1γ2(F ∩ SL2 , |||·|||L2) ≤ κ1c4w(A) , (135)\nwhere the last inequality follows from generic chaining, in particular [29, Theorem 2.1.1], for an absolute constant c4 > 0.\nIn the context of Theorem 7, we choose\nθ = c1c4κ 2 1 w(A)√ n ≥ c1κ1 γ2(F ∩ SL2 , |||·|||ϕ2)√ n , (136)\nso that the condition on θ is satisfied. With this choice of θ, we have\nθ2n/κ41 = c 2 1c 2 4w 2(A) . (137)\nThen, from Theorem 7, it follows that with probability at least 1− exp(−ηw2(A)), we have\nsup u∈A ∣∣∣∣∣ 1ρun2 n2∑ i=1 1 ρu 〈zi,u〉2I(ri ≥ η0)− 1 ∣∣∣∣∣ ≤ cκ21w(A)√n2 , (138) 1With abuse of notation, we treat the distribution over W̄i as discrete for ease of notation. A similar argument applies for the true continuous\ndistribution, but more notation is needed.\nwhere η = c2c21c 2 4 and c = c1c2 are absolute constants. Thus, with probability at least 1− exp(−ηw2(A)),\ninf u∈A\n1\nn2 n2∑ i=1 〈zi,u〉2I(ri ≥ η0) ≥ inf u∈A ρ2u ( 1− cκ21 w(A) √ n2 ) , (139)\n⇒ inf u∈A n2∑ i=1 〈zi,u〉2I(ri ≥ η0) ≥ n2ρ2 ( 1− cκ21 w(A) √ n2 ) , (140)\nwhere ρ2 = infu∈A ρ2u. Then, with probability at least 1− exp(−ηw2(A)), we have\ninf u∈A n2∑ i=1 ri〈zi,u〉2 ≥ inf u∈A n2∑ i=1 ri〈zi,u〉2I(ri ≥ η0) (141)\n≥ inf u∈A η0 n2∑ i=1 〈zi,u〉2I(ri ≥ η0) (142)\n≥η0n2ρ2 (\n1− cκ21 w(A) √ n2\n) . (143)\nThus,\np ( inf u∈A n2∑ i=1 ri〈zi,u〉2 ≥ η0n2ρ2 ( 1− cκ21 w(A) √ n2 )) ≥ 1− exp(−ηw2(A)), (144)\n⇒ p ( inf u∈A n2∑ i=1 ri〈zi,u〉2 < η0n2ρ2 ( 1− cκ21 w(A) √ n2 )) ≤ exp(−ηw2(A)). (145)\nPutting (129) and (145) into (125), for any n2 ≥ 2ηw 2(A)\n(η1−1)2 we have\np ( inf u∈A n2∑ i=1 σi〈zi,u〉2 < η0 η1 ρ2 ( 1− cκ21 w(A) √ n2 )) ≤ exp(−n2(η1 − 1) 2 2 ) + exp(−ηw2(A)) (146)\n≤ 2 exp(−ηw2(A)), (147)\nii. A is strictly greater than B: Note that, 0 ≤ σi ≤ 1 for all i and ∑n2 i=1 σi = 1. Define f(z) = 〈u, z〉2, which is a convex function of z. Using Jensen’s inequality, we have\nf (∑n2 i=1 σizi∑n2 i=1 σi ) ≤ ∑n2 i=1 σif(zi)∑n2\ni=1 σi (148)〈\nu, ∑n2 i=1 σizi∑n2 i=1 σi 〉2 ≤ ∑n2 i=1 σi〈u, zi〉2∑n2 i=1 σi (149)\n〈u, n2∑ i=1 σjzi〉2 ≤ n2∑ i=1 σi〈u, zi〉2. (150)\nThe equality in (150) holds if z1 = z2 = · · · = zn2 , or if both sides are zero i.e., u is in the null space of zi for all i. Since zi are different with probability 1, then if we show that u is not in the null space of zi for all i, then the inequality (150) is strict inequality.\nThis completes the proof."
    }, {
      "heading" : "Acknowledgment",
      "text" : "The research was supported by NSF grants IIS-1447566, IIS-1447574, IIS-1422557, CCF-1451986, CNS- 1314560, IIS-0953274, IIS-1029711, NASA grant NNX12AQ39A, and gifts from Adobe, IBM, and Yahoo. F. F. acknowledges the support of IDF (2014-2015) and DDF (2015-2016) from the University of Minnesota."
    } ],
    "references" : [ {
      "title" : "Estimation with Norm Regularization",
      "author" : [ "A. Banerjee", "S. Chen", "F. Fazayeli", "V. Sivakumar" ],
      "venue" : "Neural Information Processing Systems",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "and A",
      "author" : [ "O. Banerjee", "L. El Ghaoui" ],
      "venue" : "d’Aspremont. Model selection through sparse maximum likelihood estimation for multivariate gaussian or binary data. The Journal of Machine Learning Research, 9:485–516",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "A fast iterative shrinkage-thresholding algorithm for linear inverse problems",
      "author" : [ "A. Beck", "M. Teboulle" ],
      "venue" : "SIAM Journal on Imaging Sciences, 2(1):183–202",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Concentration Inequalities: A Nonasymptotic Theory of Independence",
      "author" : [ "S. Boucheron", "G. Lugosi", "P. Massart" ],
      "venue" : "Oxford University Press",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "A constrained l1 minimization approach to sparse precision matrix estimation",
      "author" : [ "T. Cai", "W. Liu", "X. Luo" ],
      "venue" : "Journal of the American Statistical Association, 106(494):594–607",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "The Convex Geometry of Linear Inverse Problems",
      "author" : [ "V. Chandrasekaran", "B. Recht", "P.A. Parrilo", "A.S. Willsky" ],
      "venue" : "Foundations of Computational Mathematics, 12(6):805–849",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Structured estimation with atomic norms: General bounds and applications",
      "author" : [ "S. Chen", "A. Banerjee" ],
      "venue" : "Neural Information Processing Systems",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "On Milman’s Inequality and Random Subspaces Which Escape Through a Mesh inR",
      "author" : [ "Y. Gordon" ],
      "venue" : "Geometric Aspects of Functional Analysis, volume 1317 of Lecture Notes in Mathematics, pages 84–106. Springer Berlin",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 1988
    }, {
      "title" : "Covariate shift by kernel mean matching",
      "author" : [ "A. Gretton", "A. Smola", "J. Huang", "M. Schmittfull", "K. Borgwardt", "B. Schölkopf" ],
      "venue" : "Dataset shift in machine learning, 3(4):5",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "A least-squares approach to direct importance estimation",
      "author" : [ "T. Kanamori", "S. Hido", "M. Sugiyama" ],
      "venue" : "The Journal of Machine Learning Research, 10:1391–1445",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Probability in Banach Spaces: Isoperimetry and Processes",
      "author" : [ "M. Ledoux", "M. Talagrand" ],
      "venue" : "Springer",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Direct learning of sparse changes in markov networks by density ratio estimation",
      "author" : [ "S. Liu", "J.A. Quinn", "M.U. Gutmann", "T. Suzuki", "M. Sugiyama" ],
      "venue" : "Neural computation, 26(6):1169–1197",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Support consistency of direct sparse-change learning in markov networks",
      "author" : [ "S. Liu", "T. Suzuki", "M. Sugiyama" ],
      "venue" : "Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Support consistency of direct sparse-change learning in markov networks",
      "author" : [ "S. Liu", "T. Suzuki", "M. Sugiyama" ],
      "venue" : "Joutnal of Annals of Statistics",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Group sparse priors for covariance estimation",
      "author" : [ "B.M. Marlin", "M. Schmidt", "K.P. Murphy" ],
      "venue" : "Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence, pages 383–392. AUAI Press",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "High-dimensional graphs and variable selection with the lasso",
      "author" : [ "N. Meinshausen", "P. Bühlmann" ],
      "venue" : "The Annals of Statistics, pages 1436–1462",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Reconstruction and subGaussian operators in asymptotic geometric analysis",
      "author" : [ "S. Mendelson", "A. Pajor", "N. Tomczak-Jaegermann" ],
      "venue" : "Geometric and Functional Analysis, 17:1248–1282",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Node-based learning of multiple gaussian graphical models",
      "author" : [ "K. Mohan", "P. London", "M. Fazel", "D. Witten", "S.-I. Lee" ],
      "venue" : "The Journal of Machine Learning Research, 15(1):445–488",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "A unified framework for high-dimensional analysis of m-estimators with decomposable regularizers",
      "author" : [ "S.N. Negahban", "P. Ravikumar", "M.J. Wainwright", "B. Yu" ],
      "venue" : "Statistical Science, 27(4):538–557",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Smooth minimization of non-smooth functions",
      "author" : [ "Y. Nesterov" ],
      "venue" : "Mathematical programming, 103(1):127–152",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Proximal algorithms",
      "author" : [ "N. Parikh", "S.P. Boyd" ],
      "venue" : "Foundations and Trends in Optimization, 1(3):127–239",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "High-dimensional ising model selection using 1-regularized logistic regression",
      "author" : [ "P. Ravikumar", "M.J. Wainwright", "J.D. Lafferty" ],
      "venue" : "The Annals of Statistics, 38(3):1287–1319",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "High-dimensional covariance estimation by minimizing 1-penalized log-determinant divergence",
      "author" : [ "P. Ravikumar", "M.J. Wainwright", "G. Raskutti", "B. Yu" ],
      "venue" : "Electronic Journal of Statistics, 5:935–980",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Efficient learning using forward-backward splitting",
      "author" : [ "Y. Singer", "J.C. Duchi" ],
      "venue" : "Neural Information Processing Systems, pages 495–503",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Direct importance estimation with model selection and its application to covariate shift adaptation",
      "author" : [ "M. Sugiyama", "S. Nakajima", "H. Kashima", "P.V. Buenau", "M. Kawanabe" ],
      "venue" : "Advances in neural information processing systems, pages 1433–1440",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Density ratio estimation in machine learning",
      "author" : [ "M. Sugiyama", "T. Suzuki", "T. Kanamori" ],
      "venue" : "Cambridge University Press",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Majorizing measures: the generic chaining",
      "author" : [ "M. Talagrand" ],
      "venue" : "The Annals of Probability, pages 1049–1103",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 1996
    }, {
      "title" : "Majorizing measures without measures",
      "author" : [ "M. Talagrand" ],
      "venue" : "Annals of probability, pages 411–417",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "The Generic Chaining",
      "author" : [ "M. Talagrand" ],
      "venue" : "Springer Monographs in Mathematics. Springer Berlin",
      "citeRegEx" : "29",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Upper and Lower Bounds for Stochastic Processes",
      "author" : [ "M. Talagrand" ],
      "venue" : "Springer",
      "citeRegEx" : "30",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Statistical inference problems and their rigorous solutions",
      "author" : [ "V. Vapnik", "R. Izmailov" ],
      "venue" : "Statistical Learning and Data Sciences, pages 33–71. Springer International Publishing",
      "citeRegEx" : "31",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Estimation in high dimensions: a geometric perspective",
      "author" : [ "R. Vershynin" ],
      "venue" : "Sampling Theory, a Renaissance",
      "citeRegEx" : "32",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Sharp thresholds for high-dimensional and noisy sparsity recovery using `1-constrained quadratic programmming ( Lasso )",
      "author" : [ "M.J. Wainwright" ],
      "venue" : "IEEE Transactions on Information Theory, 55(5):2183–2201",
      "citeRegEx" : "33",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Graphical models via generalized linear models",
      "author" : [ "E. Yang", "A. Genevera", "Z. Liu", "P.K. Ravikumar" ],
      "venue" : "Advances in Neural Information Processing Systems, pages 1358–1366",
      "citeRegEx" : "34",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Efficient methods for overlapping group lasso",
      "author" : [ "L. Yuan", "J. Liu", "J. Ye" ],
      "venue" : "Advances in Neural Information Processing Systems, pages 352–360",
      "citeRegEx" : "35",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Direct estimation of differential networks",
      "author" : [ "S. Zhao", "T. Cai", "H. Li" ],
      "venue" : "Biometrika, 101(2):253–268",
      "citeRegEx" : "36",
      "shortCiteRegEx" : null,
      "year" : 2014
    } ],
    "referenceMentions" : [ {
      "referenceID" : 1,
      "context" : "In particular, such advances have been made for Gaussian graphical models, Ising models, Gaussian copulas, as well as certain multi-variate extensions of general exponential family distributions including multivariate Poisson models [2, 10, 16, 22, 23, 34].",
      "startOffset" : 233,
      "endOffset" : 256
    }, {
      "referenceID" : 9,
      "context" : "In particular, such advances have been made for Gaussian graphical models, Ising models, Gaussian copulas, as well as certain multi-variate extensions of general exponential family distributions including multivariate Poisson models [2, 10, 16, 22, 23, 34].",
      "startOffset" : 233,
      "endOffset" : 256
    }, {
      "referenceID" : 15,
      "context" : "In particular, such advances have been made for Gaussian graphical models, Ising models, Gaussian copulas, as well as certain multi-variate extensions of general exponential family distributions including multivariate Poisson models [2, 10, 16, 22, 23, 34].",
      "startOffset" : 233,
      "endOffset" : 256
    }, {
      "referenceID" : 21,
      "context" : "In particular, such advances have been made for Gaussian graphical models, Ising models, Gaussian copulas, as well as certain multi-variate extensions of general exponential family distributions including multivariate Poisson models [2, 10, 16, 22, 23, 34].",
      "startOffset" : 233,
      "endOffset" : 256
    }, {
      "referenceID" : 22,
      "context" : "In particular, such advances have been made for Gaussian graphical models, Ising models, Gaussian copulas, as well as certain multi-variate extensions of general exponential family distributions including multivariate Poisson models [2, 10, 16, 22, 23, 34].",
      "startOffset" : 233,
      "endOffset" : 256
    }, {
      "referenceID" : 33,
      "context" : "In particular, such advances have been made for Gaussian graphical models, Ising models, Gaussian copulas, as well as certain multi-variate extensions of general exponential family distributions including multivariate Poisson models [2, 10, 16, 22, 23, 34].",
      "startOffset" : 233,
      "endOffset" : 256
    }, {
      "referenceID" : 5,
      "context" : "In particular, we focus on the situation when the change δθ∗ has structure, such as sparsity, block sparsity, or node-perturbed sparsity, which can be characterized by a suitable (atomic) norm [6, 18].",
      "startOffset" : 193,
      "endOffset" : 200
    }, {
      "referenceID" : 17,
      "context" : "In particular, we focus on the situation when the change δθ∗ has structure, such as sparsity, block sparsity, or node-perturbed sparsity, which can be characterized by a suitable (atomic) norm [6, 18].",
      "startOffset" : 193,
      "endOffset" : 200
    }, {
      "referenceID" : 4,
      "context" : "In a high dimensional setting, recent advances [5, 22, 23] illustrate that accurate estimation of the parameter θ∗ of an Ising model depends on how sparse or otherwise structured the true parameter θ∗ is.",
      "startOffset" : 47,
      "endOffset" : 58
    }, {
      "referenceID" : 21,
      "context" : "In a high dimensional setting, recent advances [5, 22, 23] illustrate that accurate estimation of the parameter θ∗ of an Ising model depends on how sparse or otherwise structured the true parameter θ∗ is.",
      "startOffset" : 47,
      "endOffset" : 58
    }, {
      "referenceID" : 22,
      "context" : "In a high dimensional setting, recent advances [5, 22, 23] illustrate that accurate estimation of the parameter θ∗ of an Ising model depends on how sparse or otherwise structured the true parameter θ∗ is.",
      "startOffset" : 47,
      "endOffset" : 58
    }, {
      "referenceID" : 21,
      "context" : "are sparse and the samples n1, n2 are sufficient to estimate them accurately [22], indirect estimation of δθ̂ should be accurate.",
      "startOffset" : 77,
      "endOffset" : 81
    }, {
      "referenceID" : 17,
      "context" : "However, if the individual parameters θ∗ 1 and θ ∗ 2 are somewhat dense, and the change δθ ∗ has considerably more structure, such as block sparsity (only a small block has changed) or node perturbation sparsity (only edges from a few nodes have changed) [18], direct estimation may be considerably more efficient both in terms of the number of samples required as well as the computation time.",
      "startOffset" : 255,
      "endOffset" : 259
    }, {
      "referenceID" : 12,
      "context" : "[13] proposed a direct change estimator for graphical models based on the ratio of the probability density of the two models [9, 10, 25, 26, 31].",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 8,
      "context" : "[13] proposed a direct change estimator for graphical models based on the ratio of the probability density of the two models [9, 10, 25, 26, 31].",
      "startOffset" : 125,
      "endOffset" : 144
    }, {
      "referenceID" : 9,
      "context" : "[13] proposed a direct change estimator for graphical models based on the ratio of the probability density of the two models [9, 10, 25, 26, 31].",
      "startOffset" : 125,
      "endOffset" : 144
    }, {
      "referenceID" : 24,
      "context" : "[13] proposed a direct change estimator for graphical models based on the ratio of the probability density of the two models [9, 10, 25, 26, 31].",
      "startOffset" : 125,
      "endOffset" : 144
    }, {
      "referenceID" : 25,
      "context" : "[13] proposed a direct change estimator for graphical models based on the ratio of the probability density of the two models [9, 10, 25, 26, 31].",
      "startOffset" : 125,
      "endOffset" : 144
    }, {
      "referenceID" : 30,
      "context" : "[13] proposed a direct change estimator for graphical models based on the ratio of the probability density of the two models [9, 10, 25, 26, 31].",
      "startOffset" : 125,
      "endOffset" : 144
    }, {
      "referenceID" : 13,
      "context" : "[14] improved the sample complexity to min(n1, n2) = O(s log p) when a bounded density ratio model is assumed.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 35,
      "context" : "[36] considered estimating direct sparse changes in Gaussian graphical models (GGMs).",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 0,
      "context" : "In particular, our work now enables estimators for more general structures such as group/block sparsity, hierarchical group/block sparsity, node perturbation based sparsity, and so on [1, 6, 18, 19].",
      "startOffset" : 184,
      "endOffset" : 198
    }, {
      "referenceID" : 5,
      "context" : "In particular, our work now enables estimators for more general structures such as group/block sparsity, hierarchical group/block sparsity, node perturbation based sparsity, and so on [1, 6, 18, 19].",
      "startOffset" : 184,
      "endOffset" : 198
    }, {
      "referenceID" : 17,
      "context" : "In particular, our work now enables estimators for more general structures such as group/block sparsity, hierarchical group/block sparsity, node perturbation based sparsity, and so on [1, 6, 18, 19].",
      "startOffset" : 184,
      "endOffset" : 198
    }, {
      "referenceID" : 18,
      "context" : "In particular, our work now enables estimators for more general structures such as group/block sparsity, hierarchical group/block sparsity, node perturbation based sparsity, and so on [1, 6, 18, 19].",
      "startOffset" : 184,
      "endOffset" : 198
    }, {
      "referenceID" : 12,
      "context" : "[13].",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 12,
      "context" : "In particular, when δθ∗ is sparse and our estimator is run with L1 norm, we get a sample complexity of n1 = n2 = O(s log p) which is sharper than n1 = O(s 2 log p) and n2 = O(n1) in [13].",
      "startOffset" : 182,
      "endOffset" : 186
    }, {
      "referenceID" : 12,
      "context" : "The estimator builds on the density ratio estimator in [13], but works with general norm regularization [1, 6, 19] where the regularization parameter λn1,n2 depends on the sample size for both Ising models.",
      "startOffset" : 55,
      "endOffset" : 59
    }, {
      "referenceID" : 0,
      "context" : "The estimator builds on the density ratio estimator in [13], but works with general norm regularization [1, 6, 19] where the regularization parameter λn1,n2 depends on the sample size for both Ising models.",
      "startOffset" : 104,
      "endOffset" : 114
    }, {
      "referenceID" : 5,
      "context" : "The estimator builds on the density ratio estimator in [13], but works with general norm regularization [1, 6, 19] where the regularization parameter λn1,n2 depends on the sample size for both Ising models.",
      "startOffset" : 104,
      "endOffset" : 114
    }, {
      "referenceID" : 18,
      "context" : "The estimator builds on the density ratio estimator in [13], but works with general norm regularization [1, 6, 19] where the regularization parameter λn1,n2 depends on the sample size for both Ising models.",
      "startOffset" : 104,
      "endOffset" : 114
    }, {
      "referenceID" : 12,
      "context" : "[13] build on the primal-dual witness approach of Wainwright [33], which is effective for the special case of L1 norm.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 32,
      "context" : "[13] build on the primal-dual witness approach of Wainwright [33], which is effective for the special case of L1 norm.",
      "startOffset" : 61,
      "endOffset" : 65
    }, {
      "referenceID" : 29,
      "context" : "Our analysis is largely geometric, where generic chaining [30] plays a key role, and our results are in terms of Gaussian widths of suitable sets associated with the norm [1, 6].",
      "startOffset" : 58,
      "endOffset" : 62
    }, {
      "referenceID" : 0,
      "context" : "Our analysis is largely geometric, where generic chaining [30] plays a key role, and our results are in terms of Gaussian widths of suitable sets associated with the norm [1, 6].",
      "startOffset" : 171,
      "endOffset" : 177
    }, {
      "referenceID" : 5,
      "context" : "Our analysis is largely geometric, where generic chaining [30] plays a key role, and our results are in terms of Gaussian widths of suitable sets associated with the norm [1, 6].",
      "startOffset" : 171,
      "endOffset" : 177
    }, {
      "referenceID" : 8,
      "context" : "Then, we explain how to develop the loss function L(δθ;X1 1 ,X n2 2 ) based on the density ratio [9, 10, 25, 31] to directly estimate δθ = θ1 − θ2, and finally we describe how to solve the optimization problem (1) for any norm R(δθ).",
      "startOffset" : 97,
      "endOffset" : 112
    }, {
      "referenceID" : 9,
      "context" : "Then, we explain how to develop the loss function L(δθ;X1 1 ,X n2 2 ) based on the density ratio [9, 10, 25, 31] to directly estimate δθ = θ1 − θ2, and finally we describe how to solve the optimization problem (1) for any norm R(δθ).",
      "startOffset" : 97,
      "endOffset" : 112
    }, {
      "referenceID" : 24,
      "context" : "Then, we explain how to develop the loss function L(δθ;X1 1 ,X n2 2 ) based on the density ratio [9, 10, 25, 31] to directly estimate δθ = θ1 − θ2, and finally we describe how to solve the optimization problem (1) for any norm R(δθ).",
      "startOffset" : 97,
      "endOffset" : 112
    }, {
      "referenceID" : 30,
      "context" : "Then, we explain how to develop the loss function L(δθ;X1 1 ,X n2 2 ) based on the density ratio [9, 10, 25, 31] to directly estimate δθ = θ1 − θ2, and finally we describe how to solve the optimization problem (1) for any norm R(δθ).",
      "startOffset" : 97,
      "endOffset" : 112
    }, {
      "referenceID" : 11,
      "context" : "al [12, 13], a direct estimate for the changes detection problem based on density ratio can be posed as follows r(X = x|δθ) = p(X = x|θ1) p(X = x|θ2) = exp{〈T (x), θ1〉} exp{〈T (x), θ2〉} } {{ } r∗(x|δθ) Z(θ2) Z(θ1) } {{ } 1/Z(δθ) = exp{〈T (x), δθ〉)} Z(δθ) , (6)",
      "startOffset" : 3,
      "endOffset" : 11
    }, {
      "referenceID" : 12,
      "context" : "al [12, 13], a direct estimate for the changes detection problem based on density ratio can be posed as follows r(X = x|δθ) = p(X = x|θ1) p(X = x|θ2) = exp{〈T (x), θ1〉} exp{〈T (x), θ2〉} } {{ } r∗(x|δθ) Z(θ2) Z(θ1) } {{ } 1/Z(δθ) = exp{〈T (x), δθ〉)} Z(δθ) , (6)",
      "startOffset" : 3,
      "endOffset" : 11
    }, {
      "referenceID" : 2,
      "context" : "In this section, we present an algorithm in the class of Fast Iterative Shrinkage-Thresholding Algorithms (FISTA) for efficiently solving the problem (1) [3].",
      "startOffset" : 154,
      "endOffset" : 157
    }, {
      "referenceID" : 20,
      "context" : "In general, the proximal operator proxh(x) of a closed proper convex function h : R 7→ R ∪ {+∞} [21] is defined as proxh(x) = argmin u ( h(u) + 1 2 ‖u− x‖2 ) .",
      "startOffset" : 96,
      "endOffset" : 100
    }, {
      "referenceID" : 19,
      "context" : "(14) Thus, the unique minimizer (13) correspond to prox λ LR ( δθt − 1 L∇L(δθt) ) which has rate of convergence ofO(1/t) [20, 21].",
      "startOffset" : 121,
      "endOffset" : 129
    }, {
      "referenceID" : 20,
      "context" : "(14) Thus, the unique minimizer (13) correspond to prox λ LR ( δθt − 1 L∇L(δθt) ) which has rate of convergence ofO(1/t) [20, 21].",
      "startOffset" : 121,
      "endOffset" : 129
    }, {
      "referenceID" : 2,
      "context" : "To improve the rate of convergence, we adapt the idea of FISTA algorithm [3].",
      "startOffset" : 73,
      "endOffset" : 76
    }, {
      "referenceID" : 19,
      "context" : "The choice of αt+1 follows Nesterov’s accelerated gradient descent [20, 21] and is detailed in Algorithm 1.",
      "startOffset" : 67,
      "endOffset" : 75
    }, {
      "referenceID" : 20,
      "context" : "The choice of αt+1 follows Nesterov’s accelerated gradient descent [20, 21] and is detailed in Algorithm 1.",
      "startOffset" : 67,
      "endOffset" : 75
    }, {
      "referenceID" : 2,
      "context" : "(16) The algorithm has a rate of convergence of O(1/t) [3].",
      "startOffset" : 55,
      "endOffset" : 58
    }, {
      "referenceID" : 23,
      "context" : ") is given by the elementwise soft-thresholding operation [24] as [ prox λ L‖.",
      "startOffset" : 58,
      "endOffset" : 62
    }, {
      "referenceID" : 14,
      "context" : "We will focus on the case when R(δΘ) = ∑NG g=1 ‖δΘ(s, t) : s, t ∈ Gg‖F [15].",
      "startOffset" : 71,
      "endOffset" : 75
    }, {
      "referenceID" : 17,
      "context" : "Node perturbation: Another example is the row-column overlap norm (RCON) [18] to capture perturbed nodes i.",
      "startOffset" : 73,
      "endOffset" : 77
    }, {
      "referenceID" : 17,
      "context" : "This norm can be viewed as overlapping group lasso [18] and thus can be solved by applying Algorithm 1 with proximal operator for overlapping group lasso [35].",
      "startOffset" : 51,
      "endOffset" : 55
    }, {
      "referenceID" : 34,
      "context" : "This norm can be viewed as overlapping group lasso [18] and thus can be solved by applying Algorithm 1 with proximal operator for overlapping group lasso [35].",
      "startOffset" : 154,
      "endOffset" : 158
    }, {
      "referenceID" : 17,
      "context" : "δΘ = V + V T , (23) and solve it by applying in-exact ADMM techniques [18].",
      "startOffset" : 70,
      "endOffset" : 74
    }, {
      "referenceID" : 5,
      "context" : "1 Background and Assumption Gaussian Width: In several of our proofs, we use the concept of Gaussian width [6, 8], which is defined as follows.",
      "startOffset" : 107,
      "endOffset" : 113
    }, {
      "referenceID" : 7,
      "context" : "1 Background and Assumption Gaussian Width: In several of our proofs, we use the concept of Gaussian width [6, 8], which is defined as follows.",
      "startOffset" : 107,
      "endOffset" : 113
    }, {
      "referenceID" : 3,
      "context" : "Bounds on the expectations of Gaussian and other empirical processes have been widely studied in the literature, and we will make use of generic chaining for some of our analysis [4, 11, 29, 30].",
      "startOffset" : 179,
      "endOffset" : 194
    }, {
      "referenceID" : 10,
      "context" : "Bounds on the expectations of Gaussian and other empirical processes have been widely studied in the literature, and we will make use of generic chaining for some of our analysis [4, 11, 29, 30].",
      "startOffset" : 179,
      "endOffset" : 194
    }, {
      "referenceID" : 28,
      "context" : "Bounds on the expectations of Gaussian and other empirical processes have been widely studied in the literature, and we will make use of generic chaining for some of our analysis [4, 11, 29, 30].",
      "startOffset" : 179,
      "endOffset" : 194
    }, {
      "referenceID" : 29,
      "context" : "Bounds on the expectations of Gaussian and other empirical processes have been widely studied in the literature, and we will make use of generic chaining for some of our analysis [4, 11, 29, 30].",
      "startOffset" : 179,
      "endOffset" : 194
    }, {
      "referenceID" : 0,
      "context" : "[1] show that for any convex loss function the error vector ∆ = (δθ∗ − δθ̂) lies in a restricted set that is characterized as Er = Er(δθ ∗, β) = { ∆ ∈ R ∣∣∣∣ R(δθ∗ + ∆) ≤ R(δθ∗) + 1 β } .",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 18,
      "context" : "Restricted Strong Convexity (RSC) Condition: The sample complexity of the problem (1) depends on the RSC condition [19], which ensures that the estimation problem is strongly convex in the neighborhood of the optimal parameter [1, 19].",
      "startOffset" : 115,
      "endOffset" : 119
    }, {
      "referenceID" : 0,
      "context" : "Restricted Strong Convexity (RSC) Condition: The sample complexity of the problem (1) depends on the RSC condition [19], which ensures that the estimation problem is strongly convex in the neighborhood of the optimal parameter [1, 19].",
      "startOffset" : 227,
      "endOffset" : 234
    }, {
      "referenceID" : 18,
      "context" : "Restricted Strong Convexity (RSC) Condition: The sample complexity of the problem (1) depends on the RSC condition [19], which ensures that the estimation problem is strongly convex in the neighborhood of the optimal parameter [1, 19].",
      "startOffset" : 227,
      "endOffset" : 234
    }, {
      "referenceID" : 0,
      "context" : "[1] show a deterministic upper bound for ‖∆‖2 in terms of λn1,n2 , κ, and the norm compatibility constant Ψ(Cr) = supu∈Cr R(u) ‖u‖2 , as ‖∆‖2 ≤ 1 + β β λn1,n2 κ Ψ(Cr) .",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 12,
      "context" : "[13].",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 5,
      "context" : "The upper bound on Gaussian width of the unit norm-ball of R for atomic norms which covers a wide range of norms is provided in [6, 7].",
      "startOffset" : 128,
      "endOffset" : 134
    }, {
      "referenceID" : 6,
      "context" : "The upper bound on Gaussian width of the unit norm-ball of R for atomic norms which covers a wide range of norms is provided in [6, 7].",
      "startOffset" : 128,
      "endOffset" : 134
    }, {
      "referenceID" : 0,
      "context" : "Simplifying the expression and applying mean value theorem twice on the left side of RSC condition (26), for ∀γi ∈ [0, 1], we have δL(δθ∗, u) := L(δθ∗ + u)− L(δθ∗)− 〈∇L(δθ∗), u〉 ≥ uT∇2L(δθ∗ + γiu)u.",
      "startOffset" : 115,
      "endOffset" : 121
    }, {
      "referenceID" : 0,
      "context" : "Our analysis is an extension of the results on [1] using the generic chaining.",
      "startOffset" : 47,
      "endOffset" : 50
    }, {
      "referenceID" : 6,
      "context" : "The bound on Gaussian width of the error set for atomic norms has been provided in [7].",
      "startOffset" : 83,
      "endOffset" : 86
    }, {
      "referenceID" : 21,
      "context" : "For indirect approach, we first estimate Ising model structures θ̂1 and θ̂2 with L1 norm regularizer, separately [22].",
      "startOffset" : 113,
      "endOffset" : 117
    }, {
      "referenceID" : 12,
      "context" : "For the special case of sparsity, we obtain a sharper result than previous results [13] under the same smooth density ratio assumption.",
      "startOffset" : 83,
      "endOffset" : 87
    }, {
      "referenceID" : 12,
      "context" : "[13] obtained the same result with a bounded density ratio assumption which is a more restrictive assumption.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 26,
      "context" : "1 Generic Chaining Definition 4 (Majorizing measure [27]) Given α > 0, and a metric space (T, d) (that need not be finite), we define γα(T, d) = inf sup t ∑ n≥0 2∆(An(T )).",
      "startOffset" : 52,
      "endOffset" : 56
    }, {
      "referenceID" : 26,
      "context" : "We use the traditional definition of majorizing measure γα,1(T, d) from [27] γα,1(T, d) = inf sup t (∫ ∞",
      "startOffset" : 72,
      "endOffset" : 76
    }, {
      "referenceID" : 27,
      "context" : "Note that γα,1(T, d) coincides with the functional γα(T, d) [28] as K(α)γα(T, d) ≤ γα,1(T, d) ≤ K(α)γα(T, d), (46) where K(α) is a constant depending on α only.",
      "startOffset" : 60,
      "endOffset" : 64
    }, {
      "referenceID" : 28,
      "context" : "7] in [29] Consider a set T provided with two distances d1 and d2.",
      "startOffset" : 6,
      "endOffset" : 10
    }, {
      "referenceID" : 28,
      "context" : "9] in [29] Under the conditions of Theorem 4, for all values of u1, u2 > 0 we have P (|Xs −Xt0| ≥ L(γ1(T, d1) + γ2(T, d2)) + u1D1 + u2D2) ≤ L exp(−min(u2, u1)), (51) where Dj = 2 ∑ n≥0 en(T, dj).",
      "startOffset" : 6,
      "endOffset" : 10
    }, {
      "referenceID" : 31,
      "context" : "2 (Fernique-Talagrand’s comparison theorem)] in [32] Let T be an arbitrary set.",
      "startOffset" : 48,
      "endOffset" : 52
    }, {
      "referenceID" : 16,
      "context" : "Theorem 7 (Mendelson, Pajor, Tomczak-Jaegermann [17]) There exist absolute constants c1, c2, c3 for which the following holds.",
      "startOffset" : 48,
      "endOffset" : 52
    }, {
      "referenceID" : 29,
      "context" : "27 in [30] and we have",
      "startOffset" : 6,
      "endOffset" : 10
    }, {
      "referenceID" : 0,
      "context" : "Recall, RSC condition definition as δL(δθ∗,u) := L(δθ∗ + u;X1 1 ,X n2 2 )− L(δθ∗;X n1 1 ,X n2 2 )− 〈∇L(δθ∗;X n1 1 ,X n2 2 ),u〉 ≥ κ‖u‖2 (118) Simplifying the expression and applying mean value theorem twice on the left side of RSC condition (26), for ∀γi ∈ [0, 1], we have δL(δθ∗,u) := L(δθ∗ + u;X1 1 ,X n2 2 )− L(δθ∗;X n1 1 ,X n2 2 )− 〈∇L(δθ∗;X n1 1 ,X n2 2 ),u〉 ≥ u∇L(δθ̃;X1 1 ,X n2 2 )u, (119) where δθ̃ = δθ∗ + γiu.",
      "startOffset" : 256,
      "endOffset" : 262
    } ],
    "year" : 2016,
    "abstractText" : "We consider the problem of estimating change in the dependency structure between two p-dimensional Ising models, based on respectively n1 and n2 samples drawn from the models. The change is assumed to be structured, e.g., sparse, block sparse, node-perturbed sparse, etc., such that it can be characterized by a suitable (atomic) norm. We present and analyze a norm-regularized estimator for directly estimating the change in structure, without having to estimate the structures of the individual Ising models. The estimator can work with any norm, and can be generalized to other graphical models under mild assumptions. We show that only one set of samples, say n2, needs to satisfy the sample complexity requirement for the estimator to work, and the estimation error decreases as c √ min(n1,n2) , where c depends on the Gaussian width of the unit norm ball. For example, for `1 norm applied to s-sparse change, the change can be accurately estimated with min(n1, n2) = O(s log p) which is sharper than an existing result n1 = O(s 2 log p) and n2 = O(n1). Experimental results illustrating the effectiveness of the proposed estimator are presented.",
    "creator" : "LaTeX with hyperref package"
  }
}