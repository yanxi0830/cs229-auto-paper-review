{
  "name" : "1206.6430.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Variational Bayesian Inference with Stochastic Search",
    "authors" : [ "John Paisley", "David M. Blei", "Michael I. Jordan" ],
    "emails" : [ "jpaisley@berkeley.edu", "blei@cs.princeton.edu", "jordan@eecs.berkeley.edu" ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "Mean-field variational Bayesian (MFVB) inference is an optimization-based approach to approximating the full posterior of the latent variables of a Bayesian model (Jordan et al., 1999). It has been applied to many problem domains, for example mixture modeling (Blei & Jordan, 2006), sequential modeling (Beal, 2003) and factor analysis (Paisley & Carin, 2009). In addition, recent development of the theory has extended the method to online inference and stochastic optimization settings, making variational Bayes a viable approach for Bayesian learning with massive data sets (Hoffman et al., 2010; Wang et al., 2011).\nAppearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s).\nVariational Bayes approximates the full posterior by attempting to minimize the Kullback-Leibler divergence between the true posterior and a predefined factorized distribution on the same variables. Minimizing this divergence is equivalent to maximizing the familiar variational objective function. To review, let Θ = {θi} represent the set of latent variables (random effects and parameters) in the model and X represent the data. The joint likelihood of X and Θ is P (X,Θ|Υ), with Υ the set of hyperparameters. Variational inference approximates the posterior P (Θ|X,Υ) with a Q distribution that takes a set of variational parameters Ψ = {ψi}. This distribution is factorized, Q(Θ|Ψ) = ∏ i qi(θi|ψi), and the values of Ψ are optimized to maximize the objective function,\nL(X,Ψ) = EQ[lnP (X,Θ|Υ)] + H[Q(Θ|Ψ)]. (1)\nThe solution is only locally optimal when L is not convex, which is usually the case. Most variational inference algorithms optimize L by coordinate ascent, which repeatedly cycles through and optimizes with respect to each variational parameter ψi. Often the locally optimal value of ψi has a closed-form solution, for example in conjugate exponential models.\nThe log of the joint likelihood results in a sum of terms; a major issue that often arises in MFVB is that not all expectations in this sum are in closed form. A typical solution in this case is to replace the problematic function with another function of the same variables (plus auxiliary variables) that is a point-wise lower bound. This new function is selected such that the expectation is tractable. While inference can now proceed, a drawback of introducing bounds is that the true variational objective function is no longer being optimized, which may lead to a significantly worse posterior approximation. Therefore, much attention has been paid to developing tight bounds of commonly occurring functions (e.g., Jaakkola & Jordan (2000), Marlin et al. (2011), Leisink & Kappen (2001)).\nWe present a method for directly optimizing Eq. (1) for models in which not all expectations are tractable; we show how a stochastic approximation of ∇ψiL can allow for optimization of L when the expectation Eqi [lnP (X,Θ|Υ)] is not in closed form. The approximation is unbiased, and so by using the proposed stochastic method we are directly optimizing L.\nOur stochastic approximation is based on Monte Carlo integration, for which the number of samples heavily depends on the variance of this approximation. We introduce a control variate (Ross, 2006) to significantly reduce the variance of this stochastic approximation. A control variate is a tractable function g that is highly correlated with the intractable function f . The function g replaces f in Eq. (1), and the gradient is then stochastically corrected for bias.\nExisting lower bounds have properties that make them ideal as control variates, and thus can improve the speed of the algorithm. However, a major advantage of the control variate methodology is that it does not require the tractable function g to bound f , but only to correlate well with it (i.e., to approximate it well modulo a scaling). This opens the door to many more functions that may give better approximations than a lower bound. One of these possible functions is the secondorder Taylor expansion, which often gives a very good approximation, while also allowing for closed-form expectations. We show the potential performance gain using this function as a control variate, which we denote the control variate delta method for MFVB.\nRelated work. Recent work by Knowles & Minka (2011) has also addressed the problem of intractable expectations in MFVB inference in the context of developing a more general variational message passing algorithm. Our solution arises from a different perspective and results in a new algorithm based on stochastic optimization. Graves (2011) considers a similar problem for neural networks, but a lack of control variates limits the algorithm to significantly simpler variational approximations. Stochastic search algorithms have also been developed for models of Evolution Strategies (see, e.g., Yi et al. (2009))."
    }, {
      "heading" : "2. Mean-field variational inference",
      "text" : "Mean-field variational Bayesian (MFVB) inference approximates the full posterior of the latent variables of a Bayesian model with a factorized distribution. As motivated in the introduction, let Θ = {θi} be these variables, X the data and Υ all hyperparameters of the prior distributions on Θ. We define the factorized distribution on Θ to be Q(Θ|Ψ) = ∏ i qi(θi|ψi),\nwhere ψi are the parameters of the qi distributions. The variational objective function arises by bounding the marginal likelihood using the Q distribution,\nlnP (X|Υ) = ln ∫\nΘ\nP (X,Θ|Υ)dΘ (2)\n≥ ∫\nΘ\nQ(Θ|Ψ) ln P (X,Θ|Υ) Q(Θ|Ψ) dΘ.\nMaximizing this lower bound (denoted L) with respect to Ψ is equivalent to minimizing the KullbackLeibler divergence between Q(Θ) and P (Θ|X,Υ), which makes up the difference in Eq. (2).\nTo facilitate our discussion, we write the functions appearing in the log joint likelihood as lnP (X,Θ|Υ) =∑ j fj(XAj ,ΘBj ), where Aj indexes the data appearing in function j and Bj indexes the latent variables appearing in function j. We note that the index j does not correspond to variables or distributions, but to the terms of the log joint likelihood. Using this notation, the variational lower bound in Eq. (1) becomes\nL = ∑ j EQ[fj(XAj ,ΘBj )] + ∑ iH[qi(θi|ψi)]. (3)\nFor each function fj , those θi /∈ ΘBj will have their corresponding qi removed from the expectation. For those θi ∈ ΘBj , the expectation of fj results in a new function of variational parameters ψi ∈ ΨBj . Ideally, all expectations will be in closed form, allowing for the optimization of Ψ to proceed.\nIn the case where an expectation in Eq. (3) is not tractable, a nicer functional lower bound can replace the problematic function. That is, let Eqi [fj(θi)] be intractable.1 A common approach to dealing with this issue is to introduce a function g(θi, ξ) that replaces fj and is a point-wise lower bound: fj(θi) ≥ g(θi, ξ) for all θi. The function g usually takes auxiliary variables ξ, which determines how tightly g approximates fj and is tuned along with the other parameters during inference. The expectation Eqi [g(θi, ξ)] has a closed-form solution, and gives a lower bound on the variational objective that can be optimized.\nTo illustrate, consider the case where fj is convex in θi. Then a bound g could be a first-order Taylor expansion of fj about the point ξ, which has a closed-form expectation. Significantly tighter tractable bounds have also been developed for various frequently occurring functions (e.g., Marlin et al. (2011), Knowles & Minka (2011)). In general, the looser the bound the further one is from optimizing the variational objective, and learning of ψi can suffer as a result.\n1We have simplified the notation for clarity."
    }, {
      "heading" : "3. Stochastic search variational Bayes",
      "text" : "We next present a method based on stochastic search for directly optimizing the variational objective function L in cases where some expectations cannot be computed in the log joint likelihood. This method uses a stochastic approximation of the gradient with respect to the variational parameters of the associated q distribution. To further simplify notation, we drop all indices; f is the intractable function of θ (plus other variational parameters), and θ has a variational distribution q taking parameters ψ.\nWe separate the lower bound L into two functions, Ef and h, where h(X,Ψ) contains everything in L except for Ef . Notably, h contains all other functions of ψ resulting from expectations calculated with respect to q. In coordinate ascent variational inference, the first step in optimizing q with respect to its parameters ψ is to take the gradient of the variational objective,\n∇ψL = ∇ψEq[f(θ)] +∇ψh(X,Ψ). (4)\nThis gradient contains a tractable term resulting from ∇ψh, and an intractable term ∇ψEqf . Our goal is to make a stochastic approximation of this gradient. To this end, assuming the necessary regularity conditions, we rewrite this function as\n∇ψEq[f(θ)] = ∇ψ ∫ θ f(θ)q(θ|ψ)dθ (5)\n= ∫ θ f(θ)∇ψq(θ|ψ)dθ\n= ∫ θ f(θ)q(θ|ψ)∇ψ ln q(θ|ψ)dθ.\nWe use the identity ∇ψq(θ|ψ) = q(θ|ψ)∇ψ ln q(θ|ψ).\nIt follows that ∇ψEq[f(θ)] = Eq[f(θ)∇ψ ln q(θ|ψ)]. We can stochastically approximate this expectation using Monte Carlo integration,\n∇ψEq[f(θ)] ≈ 1\nS S∑ s=1 f(θ(s))∇ψ ln q(θ(s)|ψ), (6)\nwhere θ(s) iid∼ q(θ|ψ) for s = 1, . . . , S. We can therefore replace ∇ψEq[f(θ)] with the unbiased stochastic approximation of this gradient in Eq. (6). Denote this approximation as ζ. At iteration t, we update the variational parameter ψ by taking a gradient step,\nψ(t+1) = ψ(t) + ρt∇ψh(X,Ψ(t)) + ρtζt. (7)\nBy decreasing the step size ρt such that ∑∞ t=1 ρt =∞\nand ∑∞ t=1 ρ 2 t < ∞, convergence to a local optimal solution of L is guaranteed. For example, ρt = (w+ t)−η with η ∈ (0.5, 1] and w ≥ 0 satisfies this requirement."
    }, {
      "heading" : "4. Searching with control variates",
      "text" : "A practical issue with the stochastic approximation proposed in Sec. 3 is that the variance of the gradient approximation may be very large. Given S samples of a random vector X, the covariance of its unbiased sample mean X̄ is known to be Cov(X̄) = Cov(X)/S. When the diagonal values of Cov(X) are large, many samples will be required to bring this variance below a desired level for approximating the expectation. As our experiments will show in Sec. 6, the value of S can be very large in practice and lead to a slow algorithm. We therefore seek a variance reduction method to reduce the number of samples needed to construct the stochastic search direction.\nWe introduce a control variate (Ross, 2006) to reduce the variance of the stochastic gradient constructed in Eq. (6). A control variate is a random variable that is highly correlated with an intractable variable, but for which the expectation is tractable. In this case the random variable is f(θ), for which we introduce a control variate g(θ). Control variates are ideal for MFVB because they can leverage the existing bounds, though they also admit a larger class of functions. We next review this variance reduction technique for Ef , and discuss the modifications needed to account for the stochastic vector f(θ)∇ψ ln q(θ|ψ)."
    }, {
      "heading" : "4.1. A control variate for f(θ)",
      "text" : "Generally speaking, variance reduction works by modifying a function of a random variable such that its expectation remains the same, but its variance decreases. Toward this end, we introduce a control variate g(θ), which approximates f(θ) well in the highly probable regions as defined by q(θ), but also has a closed-form expectation under q. Using g and a scalar a ∈ R, we first form the new function f̂ ,\nf̂(θ) = f(θ)− a(g(θ)− Eq[g(θ)]). (8)\nThis function has the same expectation as f and therefore can replace it in L in Eq. (3).\nThe next step is to set the value of a to minimize the variance of f̂ . A simple calculation shows that\nVar(f̂) = Var(f)− 2aCov(f, g) + a2Var(g). (9)\nTaking the derivative with respect to a and setting to zero gives the optimal value,\na = Cov(f, g)\nVar(g) . (10)\nAs is usual, this covariance and variance is unknown in the functions we encounter. We can approximate\nAlgorithm 1 Variational Bayes with stochastic search Goal To calculate ∇ψL = ∇ψEq[f(θ)] +∇ψh(X,Ψ). Approximate ∇ψL using stochastic search. input Variance reduction parameter . 1: Introduce the function g(θ) as a control variate\nthat highly correlates with f(θ). 2: Sample an initial (small) collection θ(s) ∼ q(θ|ψ). 3: Sum the sample variances and covariances β = ∑K k=1 Var(g ∂ ln q ∂ψk ), γ = ∑K k=1 Var(f ∂ ln q ∂ψk ),\nα = ∑K k=1 Cov(f ∂ ln q ∂ψk , g ∂ ln q∂ψk ).\n4: Set â = α/β and S = (γ − α2/β)/ K. 5: Sample θ(s) ∼ q(θ|ψ) i.i.d. for s = 1, . . . , dSe. 6: Construct the stochastic search vector ζ = 1S ∑S s=1{f(θ(s))− âg(θ(s))}∇ψ ln q(θ(s)|ψ). 7: Step in the direction of the stochastic gradient ψ′ = ψ + ρζ + ρ∇ψ(h(X,Ψ) + âEq[g(θ)]).\na with â, found by plugging the sample variance and covariance into Eq. (10) using samples from the algorithm.\nThe potential reduction in variance is seen by plugging Eq. (10) into Eq. (9) and taking the ratio of the two variances,\nVar(f̂)/Var(f) = 1− Corr(f, g)2. (11)\nTherefore, the greater the correlation between f and g, the greater the variance reduction. Tight lower bounds of f by construction have this high correlation, but we note that tight upper bounds work as well, as do wellapproximating functions that do not bound f .\nUsing the control variate g, we now write the stochastic approximation to the gradient as\n∇ψEq[f̂(θ)] ≈ â∇ψEq[g(θ)] (12)\n+ 1\nS S∑ s=1 {f(θ(s))− âg(θ(s))}∇ψ ln q(θ(s)|ψ),\nwhere θ(s) iid∼ q(θ|ψ) for s = 1, . . . , S.\nWriting the stochastic approximation this way allows for a more intuitive understanding of the algorithm. By separating the tractable and stochastic parts as done in Eq. (12), we first replace the intractable function f with a tractable approximation g. (This resembles the standard method when g lower bounds f .) The gradient of Eg is then corrected by a stochastic vector. The variance of the correction is smaller than that of the original stochastic approximation in Sec. 3, since the function f(θ) is close to âg(θ). The gradient of Eg can be thought of as an initial guess, followed by a stochastic correction which ensures that we are optimizing the variational objective function."
    }, {
      "heading" : "4.2. The stochastic search case",
      "text" : "We have introduced a control variate for f(θ), but in fact we would like to minimize the variance of the vector f(θ)∇ψ ln q(θ|ψ) in Eq. (6). In this case, the control variate becomes g(θ)∇ψ ln q(θ|ψ) and we have the following modification.\nLet ψk be the kth dimension of ψ. Then for each dimension the discussion in Sec. 4.1 carries through, but for f ∂ ln q∂ψk and g ∂ ln q ∂ψk instead of f and g. The variance of each dimension again follows Eq. (9), and we seek an a to minimize the sum of these equations. This results in the optimal value\na = ∑ k Cov(f ∂ ln q ∂ψk , g ∂ ln q∂ψk )/ ∑ k Var(g ∂ ln q ∂ψk ),\nwhich we approximate using samples. We summarize stochastic search variational Bayes in Algorithm 1."
    }, {
      "heading" : "5. Stochastic search VB for two models",
      "text" : "We next illustrate stochastic search variational inference on logistic regression and a finite approximation to the hierarchical Dirichlet process (Teh et al., 2007). For logistic regression, we will consider two control variates, one of which is a lower bound and the other of which is not a bound. For the finite HDP, we will consider a piecewise control variate, one part being an upper bound on the original function."
    }, {
      "heading" : "5.1. Logistic regression",
      "text" : "Binary logistic regression takes in d-dimensional data vectors xn and predicts the class yn ∈ {−1, 1} to which each belongs. The parameter is θ ∈ Rd and the prediction law is Pr(yn|xn, θ) = σ(ynxTnθ) where σ(·) is the sigmoid function, σ(b) = (1+e−b)−1. Bayesian logistic regression places a prior distribution on the coefficient vector, θ ∼ Normal(0, cI). For inference we define a Gaussian variational q distribution\nq(θ) = Normal(µ,Σ). (13)\nThe variational lower bound for this model is L = ∑N n=1 Eq[lnσ(ynxTnθ)]+Eq[ln p(θ)−ln q(θ)]. (14)\nThe expectations of fn(yn, xn; θ) := lnσ(ynx T nθ) are intractable. One approach to avoiding this issue is to forgo variational inference and use Laplace’s method to approximate q. This method sets µ to the MAP solution, and Σ−1 to the negative Hessian of the log joint likelihood evaluated at µ. Another is to lower bound fn with the bound in, e.g., Jaakkola & Jordan (2000), which allows for closed-form variational inference. We consider this bound as a control variate.\nA lower bound control variate. The lower bound for fn developed by Jaakkola & Jordan (2000) is a useful control variate for variational logistic regression. For each pair (xn, yn), this bound takes an auxiliary parameter ξn > 0 and has the form\ngn(yn, xn; θ, ξn) = lnσ(ξn) + 1\n2 (ynx\nT nθ − ξn)\n− λ(ξn)((xTnθ)2 − ξ2n). (15)\nWe have λ(ξn) = (2σ(ξn) − 1)/(4ξn). We select this bound for illustrative purposes, but any lower bound will work in principle. For a multivariate Gaussian q distribution, having a quadratic term in g is essential for stochastically learning a full covariance matrix. In general, tighter bounds will require fewer samples, but for some functions finding tight bounds may require much effort. We next consider a general purpose control variate that can help in this case.\nControl variate delta method. We also consider the second-order Taylor expansion of f as a control variate. The second-order Taylor expansion often accurately approximates a function of interest, and when used alone is known as the delta method. In addition to accuracy, the quadratic approximation of the delta method results in a function for which the expectation with respect to q is very likely to be analytic.\nThe delta method arguably should not be used for mean-field variational inference because the secondorder Taylor expansion is not a lower bound. On the other hand, the first-order Taylor expansion often is a lower bound. Therefore, though their bounds are typically loose, first-order approximations are commonly employed for MFVB. An advantage of the proposed stochastic search algorithm is that second-order methods can now be used as a control variate to (i) more\naccurately approximate the function of interest, and (ii) significantly reduce the variance of the stochastic gradient. We call this approach of using Taylor expansion control variates the control variate delta method.\nWe consider a second-order Taylor expansion at µ̂, the current mean of q, for approximating lnσ(ynx T nθ). Letting σn := σ(ynx T n µ̂), this control variate is\ngn(yn, xn; θ, µ̂) = lnσn + yn(1− σn)(θ − µ̂)Txn (16)\n− 1 2 σn(1− σn)(θ − µ̂)TxnxTn (θ − µ̂).\nAs with the Jaakkola & Jordan (2000) bound, this control variate contains a quadratic term that helps in learning the covariance matrix of q.\nWe compare these control variates in Figure 1. In these plots we show the difference fn − gn for two specific q distributions, and with x = 1. We also show 100 samples from q, which indicates the regions where these functions would be evaluated (for a = 1). The plots show that the second-order Taylor expansion approximates fn significantly better where it matters; we support this conclusion with the experiments in Sec. 6."
    }, {
      "heading" : "5.2. Hierarchical Dirichlet processes",
      "text" : "We also investigate a stochastic search VB algorithm for an approximation to the hierarchical Dirichlet process (Teh et al., 2007). We focus on the two-level generative structure using finite dimensional Dirichlet priors as an approximation to the infinite dimensional process—in the limit the HDP is recovered. In this finite process, a top-level Dirichlet-distributed probability vector θ parameterizes the Dirichlet distribution for d = 1, . . . , D second-level probability vectors,\n(πd1, . . . , πdK) iid∼ Dirichlet(βθ1, . . . , βθK),\n(θ1, . . . , θK) ∼ Dirichlet( αK , . . . , α K ). (17)\nIn topic models, these πd vectors are often used as distributions on word distributions. In this section, we focus solely on the generic hierarchical structure in Eq. (17). We define the approximate posterior of θ as\nq(θ) = Dirichlet(c1, . . . , cK). (18)\nThe part of the lower bound associated with θ is Lθ = ∑ k βEq[θk] ∑ d Eq[lnπdk]− ∑ kDEq[ln Γ(βθk)]\n+Eq[ln p(θ)− ln q(θ)]. (19)\nThe expectation Eq[ln Γ(βθk)] is intractable for each k. We use a stochastic approximation, and introduce two control variates for this function, depending on the current expected value of βθk.\nAs βθk approaches zero, the function fk(βθk) = − ln Γ(βθk) diverges to −∞. By construction of the Dirichlet prior, many values of θk will be very small. (In the infinite limit, there are an infinite number of such values smaller than any δ > 0.) The variance in this region is massive—when computer precision becomes an issue it can be infinite (see Figure 2).\nWe propose the control variate gk(βθk) = lnβθk, with Eq[ln θk] = φ(ck)−φ( ∑ j cj) where φ(·) is the digamma function. This control variate not only correlates well with fk, but if a = 1, limβθk→0 fk − agk = 0, as shown in Figure 2. This results from the equality\n− ln Γ(βθk)− lnβθk = − ln Γ(βθk + 1). (20)\nFor all other values of a, this equality does not hold, and the difference fk − agk diverges as βθk → 0. For this model, we can thus give the optimal value of a in advance, and we set a = 1.\nFrom Figure 2, we also see that the approximation gets worse when βθk gets large, which can occur for a few highly probable dimensions when β is large. Since fk is approximately linear in this regime, we use a firstorder Taylor expansion of fk about the mean θ̄k = Eq[θk] as a control variate. This gives the following two control variates,\ngk = lnβθk, 0 < βθ̄k < κ1, (21) gk = − ln Γ(βθ̄k)− β(θk − θ̄k)φ(βθ̄k), βθ̄k > κ2.\nSince fk is concave, this second control variate is an upper bound on Lθ without the stochastic correction. We discuss the boundaries κ1 and κ2 in Sec. 6.\nThus far, we’ve focused mainly on reducing the variance induced by fk, but in Sec. 4.2 we noted that ∇ ln q introduces variance to the Monte Carlo integral as well. This suggests that we should look at other\nparts of the integral for potential variance reduction. We briefly show how this can be done for the HDP.\nThe lower bound in Eq. (19) contains a sum of K intractable integrals over the probability simplex ∆K . We perform separate stochastic approximations of each gradient. Using the fact that each gamma function is over a single dimension of the simplex, for a function of θk the variables θi 6=k will integrate out. In this case, marginalizing a Dirichlet distribution to a single dimension yields a beta distribution. That is,∫ θ∈∆K ln Γ(βθk)q(θ|c)dθ = ∫ 1 0 ln Γ(βθk)q ′ k(θk|c)dθk,\nwhere q′k(θk|c) = Beta(θk|ck, ∑ i6=k ci).\nWe can choose which of these integrals to stochastically approximate for gradient ascent. However, the stochastic gradient using q′k results in significantly less variance than for q since θ (s) k will be near zero; the vector ∇c ln q′k has K − 1 entries containing ln(1− θ(s)k )−Eq[ln(1− θk)], while these values will be ln θ\n(s) i − Eq[ln θi] for i = 1, . . . ,K when using ∇c ln q."
    }, {
      "heading" : "6. Experiments",
      "text" : "We perform experiments using stochastic search VB for binary classification with logistic regression and for topic modeling with the approximate HDP. We next give the details of the experiments we perform and the data sets and algorithms used for comparison.\nData and set-up. For logistic regression, we use five data sets from the UCI repository: Iris, Pima, SPECTF, Voting and WDBC. These data sets range from 150 to 768 labeled examples living in 5 to 45 dimensions, including a dimension of all ones to account for offset. We perform experiments with stochastic search variational inference using the two control variates discussed in Sec. 5.1. We compare with two additional methods for posterior approximation: variational inference with the Jaakkola & Jordan (2000) bound and Laplace’s method. We evaluate performance on the true variational objective function in Eq. (14) using each posterior approximation.\nFor the HDP topic model, we use 8,000 documents with 3,012 vocabulary size from The New York Times. We compare with (i) a point estimate of the top-level probability vector using a delta q distribution, and (ii) fixing the top-level distribution to the uniform vector, which is equivalent to LDA (Blei et al., 2003). We perform experiments for different corpus sizes, different values of β, and we set K = 200.\nLogistic regression results. In Table 1 we show the variational lower bound for each model on each data set. Since all algorithms return an approximation of the posterior distribution on the vector θ, this comparison is meaningful and gives a measure of how close each posterior is to the true posterior. We see a considerable improvement for the stochastic algorithms (denoted by their control variate). Since both stochastic algorithms optimize the same objective, the performance should be the same.\nWe show performance details of the stochastic search VB algorithm in Figure 3 and Table 2. In Figure 3, we show the number of samples, the variance reduction factor and the scaling â as a function of iteration. We see that the control variates provide a major reduction in variance. Also, the Taylor expansion control variate (i.e., control variate delta method) requires significantly fewer samples than the bound control variate, which benefits the running time (see Table 2). While the non-sampling methods are faster, control variates make stochastic search VB a viable inference method when compared to the base algorithm of Sec. 3.\nHierarchical Dirichlet process results. We fit topic models to The New York Times using different numbers of documents (D = 1000 to 8000) and concentration parameter values β ∈ {1, 5, 10, 15}. As switch points for the two control variates, we set κ1 = 1 and κ2 = 2. We summarize our results in Table 3. In general, fitting a variational posterior on the top-level Dirichlet vector yielded a better posterior approximation than a point estimate and a θ fixed as uniform. However, this improvement was not as dramatic as for logistic regression.\nIn Figure 4, we show the number of samples required from the Dirichlet q distribution to approximate the stochastic integral. We compare the two methods discussed in Sec. 5.2 for reducing the variance of the stochastic vector ∇c ln q by instead using ∇c ln q′k. We see a significant reduction in the number of samples. Experiments without control variates were not possible due to computer precision issues and the massive variance of ln Γ(βθ) near zero."
    }, {
      "heading" : "7. Conclusion",
      "text" : "We have presented stochastic search variational Bayes, a method for optimizing intractable variational objective functions such as those arising from nonconjugacy. The algorithm relies on a stochastic approximation of the gradient; we showed how control variates can significantly reduce the variance of this Monte Carlo integral. Since existing lower bounds can be recast as control variates, our approach is relevant to many existing MFVB algorithms. However, a lack\nof restrictions on control variates allows for other types of function approximations when a good bound is not readily available. We introduced the control variate delta method toward this end.\nAcknowledgements J.P. and M.J. are supported by ONR grant number N00014-11-1-0688 under the MURI program. D.B. is supported by ONR N00014-11-1-0651, NSF CAREER 0745520, AFOSR FA9550-09-1-0668, the Alfred P. Sloan foundation, and a grant from Google."
    } ],
    "references" : [ {
      "title" : "Variational Algorithms for Approximate Bayesian Inference",
      "author" : [ "M.J. Beal" ],
      "venue" : "PhD thesis, Gatsby Computational Neuroscience Unit,",
      "citeRegEx" : "Beal,? \\Q2003\\E",
      "shortCiteRegEx" : "Beal",
      "year" : 2003
    }, {
      "title" : "Variational inference for Dirichlet process mixtures",
      "author" : [ "D. Blei", "M. Jordan" ],
      "venue" : "Bayesian Analysis,",
      "citeRegEx" : "Blei and Jordan,? \\Q2006\\E",
      "shortCiteRegEx" : "Blei and Jordan",
      "year" : 2006
    }, {
      "title" : "Latent Dirichlet allocation",
      "author" : [ "D. Blei", "A. Ng", "M. Jordan" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Blei et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Blei et al\\.",
      "year" : 2003
    }, {
      "title" : "Practical variational inference for neural networks",
      "author" : [ "A. Graves" ],
      "venue" : "In Neural Information Processing Systems,",
      "citeRegEx" : "Graves,? \\Q2011\\E",
      "shortCiteRegEx" : "Graves",
      "year" : 2011
    }, {
      "title" : "Online learning for latent Dirichlet allocation",
      "author" : [ "M. Hoffman", "D. Blei", "F. Bach" ],
      "venue" : "In Neural Information Processing Systems,",
      "citeRegEx" : "Hoffman et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Hoffman et al\\.",
      "year" : 2010
    }, {
      "title" : "Bayesian parameter estimation via variational methods",
      "author" : [ "T. Jaakkola", "M.I. Jordan" ],
      "venue" : "Statistics and Computing,",
      "citeRegEx" : "Jaakkola and Jordan,? \\Q2000\\E",
      "shortCiteRegEx" : "Jaakkola and Jordan",
      "year" : 2000
    }, {
      "title" : "An introduction to variational methods for graphical models",
      "author" : [ "M.I. Jordan", "Z. Ghahramani", "T. Jaakkola", "L.K. Saul" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Jordan et al\\.,? \\Q1999\\E",
      "shortCiteRegEx" : "Jordan et al\\.",
      "year" : 1999
    }, {
      "title" : "Non-conjugate variational message passing for multinomial and binary regression",
      "author" : [ "D.A. Knowles", "T.P. Minka" ],
      "venue" : "In Neural Information Processing Systems,",
      "citeRegEx" : "Knowles and Minka,? \\Q2011\\E",
      "shortCiteRegEx" : "Knowles and Minka",
      "year" : 2011
    }, {
      "title" : "A tighter bound for graphical models",
      "author" : [ "M.A.R. Leisink", "H.J. Kappen" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "Leisink and Kappen,? \\Q2001\\E",
      "shortCiteRegEx" : "Leisink and Kappen",
      "year" : 2001
    }, {
      "title" : "Piecewise bounds for estimating Bernoulli-logistic latent Gaussian models",
      "author" : [ "B. Marlin", "E. Khan", "K. Murphy" ],
      "venue" : "In International Conference on Machine Learning,",
      "citeRegEx" : "Marlin et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Marlin et al\\.",
      "year" : 2011
    }, {
      "title" : "Nonparametric factor analysis with beta process priors",
      "author" : [ "J. Paisley", "L. Carin" ],
      "venue" : "In International Conference on Machine Learning,",
      "citeRegEx" : "Paisley and Carin,? \\Q2009\\E",
      "shortCiteRegEx" : "Paisley and Carin",
      "year" : 2009
    }, {
      "title" : "Hierarchical Dirichlet processes",
      "author" : [ "Y. Teh", "M. Jordan", "M. Beal", "D. Blei" ],
      "venue" : "Journal of the American Statistical Association,",
      "citeRegEx" : "Teh et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Teh et al\\.",
      "year" : 2007
    }, {
      "title" : "Online variational inference for the hierarchical Dirichlet process",
      "author" : [ "C. Wang", "J. Paisley", "D. Blei" ],
      "venue" : "In Artificial Intelligence and Statistics,",
      "citeRegEx" : "Wang et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2011
    }, {
      "title" : "Stochastic search using the natural gradient",
      "author" : [ "S. Yi", "D. Wierstra", "T. Schaul", "J. Schmidhuber" ],
      "venue" : "In International Conference on Machine Learning,",
      "citeRegEx" : "Yi et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Yi et al\\.",
      "year" : 2009
    } ],
    "referenceMentions" : [ {
      "referenceID" : 6,
      "context" : "Mean-field variational Bayesian (MFVB) inference is an optimization-based approach to approximating the full posterior of the latent variables of a Bayesian model (Jordan et al., 1999).",
      "startOffset" : 163,
      "endOffset" : 184
    }, {
      "referenceID" : 0,
      "context" : "It has been applied to many problem domains, for example mixture modeling (Blei & Jordan, 2006), sequential modeling (Beal, 2003) and factor analysis (Paisley & Carin, 2009).",
      "startOffset" : 117,
      "endOffset" : 129
    }, {
      "referenceID" : 4,
      "context" : "In addition, recent development of the theory has extended the method to online inference and stochastic optimization settings, making variational Bayes a viable approach for Bayesian learning with massive data sets (Hoffman et al., 2010; Wang et al., 2011).",
      "startOffset" : 216,
      "endOffset" : 257
    }, {
      "referenceID" : 12,
      "context" : "In addition, recent development of the theory has extended the method to online inference and stochastic optimization settings, making variational Bayes a viable approach for Bayesian learning with massive data sets (Hoffman et al., 2010; Wang et al., 2011).",
      "startOffset" : 216,
      "endOffset" : 257
    }, {
      "referenceID" : 9,
      "context" : ", Jaakkola & Jordan (2000), Marlin et al. (2011), Leisink & Kappen (2001)).",
      "startOffset" : 28,
      "endOffset" : 49
    }, {
      "referenceID" : 9,
      "context" : ", Jaakkola & Jordan (2000), Marlin et al. (2011), Leisink & Kappen (2001)).",
      "startOffset" : 28,
      "endOffset" : 74
    }, {
      "referenceID" : 3,
      "context" : "Graves (2011) considers a similar problem for neural networks, but a lack of control variates limits the algorithm to significantly simpler variational approximations.",
      "startOffset" : 0,
      "endOffset" : 14
    }, {
      "referenceID" : 3,
      "context" : "Graves (2011) considers a similar problem for neural networks, but a lack of control variates limits the algorithm to significantly simpler variational approximations. Stochastic search algorithms have also been developed for models of Evolution Strategies (see, e.g., Yi et al. (2009)).",
      "startOffset" : 0,
      "endOffset" : 286
    }, {
      "referenceID" : 9,
      "context" : ", Marlin et al. (2011), Knowles & Minka (2011)).",
      "startOffset" : 2,
      "endOffset" : 23
    }, {
      "referenceID" : 9,
      "context" : ", Marlin et al. (2011), Knowles & Minka (2011)).",
      "startOffset" : 2,
      "endOffset" : 47
    }, {
      "referenceID" : 11,
      "context" : "We next illustrate stochastic search variational inference on logistic regression and a finite approximation to the hierarchical Dirichlet process (Teh et al., 2007).",
      "startOffset" : 147,
      "endOffset" : 165
    }, {
      "referenceID" : 11,
      "context" : "We also investigate a stochastic search VB algorithm for an approximation to the hierarchical Dirichlet process (Teh et al., 2007).",
      "startOffset" : 112,
      "endOffset" : 130
    }, {
      "referenceID" : 2,
      "context" : "We compare with (i) a point estimate of the top-level probability vector using a delta q distribution, and (ii) fixing the top-level distribution to the uniform vector, which is equivalent to LDA (Blei et al., 2003).",
      "startOffset" : 196,
      "endOffset" : 215
    } ],
    "year" : 2012,
    "abstractText" : "Mean-field variational inference is a method for approximate Bayesian posterior inference. It approximates a full posterior distribution with a factorized set of distributions by maximizing a lower bound on the marginal likelihood. This requires the ability to integrate a sum of terms in the log joint likelihood using this factorized distribution. Often not all integrals are in closed form, which is typically handled by using a lower bound. We present an alternative algorithm based on stochastic optimization that allows for direct optimization of the variational lower bound. This method uses control variates to reduce the variance of the stochastic search gradient, in which existing lower bounds can play an important role. We demonstrate the approach on two non-conjugate models: logistic regression and an approximation to the HDP.",
    "creator" : "LaTeX with hyperref package"
  }
}