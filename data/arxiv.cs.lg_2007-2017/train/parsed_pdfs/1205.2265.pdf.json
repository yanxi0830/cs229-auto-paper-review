{
  "name" : "1205.2265.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Efficient Constrained Regret Minimization",
    "authors" : [ "Mehrdad Mahdavi", "Tianbao Yang", "Rong Jin" ],
    "emails" : [ "mahdavim@msu.edu", "yangtia1@msu.edu", "rongjin@msu.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n20 5.\n22 65\nv2 [\ncs .L\nG ]\n4 O\nct 2\n01 2\nKeywords: online learning, bandit, regret-minimization, repeated game playing, constrained decision making"
    }, {
      "heading" : "1 Introduction",
      "text" : "Many practical problems such as online portfolio management [1], prediction from expert advice [2,3], and online shortest path problem [4], involve making repeated decisions in an unknown and unpredictable environment (see, e.g. [5] for a comprehensive review). These situations can be formulated as a repeated game between the decision maker (i.e., the learner) and the adversary (i.e., the environment). At each round of the game, the learner selects an action from a fixed set of actions and then receives feedback (i.e., reward) for the selected action. In the adversarial or non-stochastic feedback model, we make no statistical assumption on the sequence of rewards except that the rewards are bounded. The player would like to learn from the past and hopefully make better decisions as time goes by, so that the total accumulated reward is large.\nThe analysis of online learning algorithms focuses on establishing bounds on the regret that is the difference between the reward of the best fixed action with\nthe hindsight knowledge of the observed sequence and the cumulative reward of the online learner. If the online algorithm attains sublinear bound on the regret, is said to be Hannan consistent [5], which indicates that in the long run, the learner’s average reward per round approaches the average reward per round of the best action. A point worthy of notice is that the performance bound must hold for any sequence of rewards, and in particular if the sequence is chosen adversarially. We also note that this setting differs from the framework of competitive analysis where the decision maker is allowed to first observe the reward vector, and then make the decision and get the reward accordingly [6].\nIn many current literature, the application of online learning is mostly limited to problems without constraints on the decisions. However, in most scenarios, beyond maximizing the cumulative reward, there are some restrictions on the sequence of decisions made by the learner that need to be satisfied on the average. Moreover, in some applications it seems beneficial to sacrifice some reward to get along with other goals simultaneously. Therefore, one might desire algorithms for a much more ambitious framework, where we need to maximize total reward under the constraints defined on the sequence of decisions. Attempts for such extension were made in [7], where the online learning with path constraints has been addressed and algorithms with asymptotically vanishing bound have been proposed.\nAs an illustrative example, let us consider a wireless communication system where the agent chooses an appropriate transmission power in order to transmit a message successfully. If one considers the amount of power required to transmit a packet through a path as its cost, the goal of the agent may be to maximize average throughput, while keeping the average power consumption under some required threshold. As another motivating example, consider the online ads placement with budgeted advertisers. This problem can be cast as a multi armed bandit (MAB) problem, with the set of arms being the set of ads. Since each advertiser has a limited budget to represent his adds, the online learner must consider the budget restriction of each advertiser in making decisions.\nTo model abovementioned situations, we consider modifying the online learning problem to achieve both goals simultaneously where the additional goal is called constraint throughout the paper to distinguish it from the regret. Roughly speaking, we try to devise online algorithms in order to maximize the revenue and to some degree guarantee vanishing bound on the additional constraint. The constraint defined over the actions necessitates a compromise: if the algorithm be too aggressive to satisfy the constraint, then there would be less hope to attain satisfactory cumulative reward at the end of the game and on the other hand, just trying to maximize the cumulative reward will end up in a situation in which the constraint vanishes linearly in terms of the number of rounds.\nAn algorithm addressing this problem has to balance between maximizing the adversary rewards and satisfying the constraint. To affirmatively address the problem, we provide a general framework for repeated games with constraint, and propose a simple randomized algorithm called Lagrangian exponentially weighted average (LEWA) algorithm for a particular class of these games.\nThe proposed formulation is inspired by the theory of Lagrangian method in constrained optimization and is based on primal-dual formulation of the exponentially weighted average (EWA) algorithm [3] [8]. To the best of our knowledge, this is the first time a Lagrangian style relaxation has been proposed for this type of problem.\nThe contribution of the present work is to 1) introduce a general primal-dual framework for solving online learning with constraints problem; 2) propose a Lagrangian based exponentially weighted average algorithm for solving repeated games with constraints; 3) establish expected and high probability bounds on the regret and the violation of the constraints on average; 4) extend the results to the bandit setting where only partial feedback about the rewards and constraints are available.\nNotations. Before proceeding, we define the notations used in this paper. Vectors are indicated in lower case bold letters such as x where x⊤ denotes it transpose. By default, all vectors are column vectors. For a vector x, xi denotes its ith coordinate. We use superscripts to index rounds of the game. Componentwise multiplication between vectors is denoted by ◦. We use [K] as a shorthand for the set of integers {1, 2, . . . ,K}. Throughout the paper we denote by [·]+ the projection onto the positive orthant. We shall use 1 to denote the vector of all ones. Finally, for a K-dimensional vector x, (x)2 represents (x21, . . . , x 2 K)."
    }, {
      "heading" : "2 Statement of the Problem",
      "text" : "We consider the general decision-theoretic framework for online learning and extend it to capture the constraint. In original online decision making, the learner is given access to a pool of K actions. In each round t ∈ [T ], the learner chooses a probability distribution pt = (p t 1, ..., p t K) over the actions [K] and chooses an action i randomly based on pt. In the scenario of full information, at each iteration, the adversary reveals a reward vector rt = (r t 1, · · · , rtK). Choosing an action i results in receiving a reward rti , which we shall assume without loss of generality to be bounded in [0, 1]. In the partial information or bandit setting, only the cost of selected action is revealed by the adversary. The learner competes with the best fixed action in hindsight and his/her goal is to minimize the regret defined as\nRegretT = max p\nT∑\nt=1\np⊤rt − T∑\nt=1\np⊤t rt.\nThis problem is a well studied problem and there are algorithms which attain an optimal regret bound of O( √ T lnK) after T rounds of the game. In this paper we focus on exponentially weighted average (EWA), which will be used later as the baseline of the proposed algorithm. The EWA algorithm maintains a weight vector wt = (w t 1, · · · , wtK) which is used to define the probabilities over actions. After receiving the reward vector rt at round t, the EWA algorithm updates the weight vector according to wt+1i = w t i exp(ηr t i) where η is learning rate.\nIn the new setting addressed in this paper, which we refer to as constrained regret minimization, in addition to the rewards, there exist some constraints on the decisions that need to be satisfied. In particular, for the decision p made by the learner, there is an additional constraint p⊤c ≥ c0 where c is a constraint vector for specifying the constraint (e.g. the cost vector for the arms in MAB problem). We note that, in general, the reward vector rt and the constraint vector c are different and can not be combined as a single objective. The learner’s goal is to maximize the total reward with respect to the optimal decision in hindsight under the constraint p⊤c ≥ c0, i.e.,\nmin p1,...,pT max p⊤c≥c0\nT∑\nt=1\np⊤rt − T∑\nt=1\np⊤t rt,\nand simultaneously satisfy the constraint. Note that the comparator class includes fixed decision p that attains maximal cumulative reward had he known the rewards beforehand, while satisfying the additional constraint.\nWithin our setting, we consider repeated games with adversarial rewards and stochastic constraint. More precisely, let c = (c1, · · · , cK) be the constraint vector defined over actions. In stochastic setting the vector c is unknown to the learner and in each round t ∈ [T ], beyond the reward feedback, the learner receives a random realization ct = (c t 1, · · · , ctK) of c where E[cti] = ci. The learner’s goal is to choose a sequence of decisions pt, t ∈ [T ] to minimize the regret with respect to the optimal decision in hindsight under the constraint p⊤c ≥ c0. Without loss of generality we assume ct ∈ [0, 1]K and c0 ∈ [0, 1]. Formally, the goal of the learner is to attain a gradually vanishing constrained regret as\nRegretT = max p⊤c≥c0\n∑\nt\np⊤rt − ∑\nt\np⊤t rt ≤ O(T 1−β1). (1)\nFurthermore, the decisions pt, t = 1, · · · , T made by the learner are required to attain sub-linear bound on the violation of the constraint in long run, i.e.,\nViolationT =\n[ T∑\nt=1\n( c0 − p⊤t c\n) ]\n+\n≤ O(T 1−β2). (2)\nWe refer to the above bound as the violation of the constraint. We distinguish two different types of constraint satisfaction algorithms: one shot and long term satisfaction. In one shot constraint satisfaction, the learner is required to satisfy the constraint at each round, i.e., p⊤t c ≥ c0. In contrast, in the long term version, the learner is allowed to violate the constraint for some rounds in a controlled way; but the constraint must hold on average for all rounds, i.e., ( ∑T\nt=1 p ⊤ t c)/T ≥ c0.\nThe main questions addressed in this paper are how to modify EWA algorithm to take the constraints under consideration and what would be the bounds on the regret as well as the violation of the constraints attainable by the modified algorithm."
    }, {
      "heading" : "3 Related Works",
      "text" : "As is well known, a wide range of literature deals with the online decision making problem without constraints and there exist a number of regret-minimizing algorithms that have the optimal regret bound. The most well-known and successful work is probably the Hedge algorithm [8], which was a direct generalization of Littlestone and Warmuth’s Weighted Majority (WM) algorithm [3]. Other recent studies include the improved theoretical bounds and the parameter-free hedging algorithm [9] and adaptive Hedge [10] for decision-theoretic online learning. We refer readers to the [5] for an in-depth discussion of this subject.\nAs the first seminal paper in adversarial setting, Mannor et al. [7] introduced the online learning with simple path constraints. They considered the infinitely repeated two player games with stochastic rewards where for every joint action of the players, there is an additional stochastic constraint vector that is accumulated by the decision maker. The learner is asked to keep the cumulative constraint vector in a predefined set in the space of constraint vectors. They showed that if the convex set is affected by both decisions and rewards, the optimal reward is generally unattainable online. The positive result is that a relaxed goal, which is defined in terms of the convex hull of the constrained reward in hindsight is attainable. For the relaxed setting, they suggested two inefficient algorithms: one relies on Blackwell’s approachability theory and the other is based on calibrated forecast of the adversary’s actions. Given the implementation difficulties associated with these two methods, they suggested two efficient heuristic methods to attain the reward with meeting the constraint in the long run. We note that the analysis in [7] is asymptotic while the bounds to be established in this work are applicable to finite repeated games.\nIn [11] the budget limited MAB was introduced where polling an arm is costly where the cost of each arm is fixed in advance. In this setting both the exploration and exploitation phases are limited by a global budget. This setting matches the stochastic rewards with deterministic constraints without violation game discussed before. It has been shown that existing MAB algorithms are not suitable to efficiently deal with costly arms. They proposed the ǫ − first algorithm that dedicates the first ǫ fraction of the total budget exclusively for exploration and the remaining (1 − ǫ) fraction for exploitation. [12] improves the bound obtained in [11] by proposing a knapsack based UCB [12] algorithm which extends the UCB algorithm by solving a knapsack problem at each round to cope with the constraints. We note that knapsack based UCB does not make explicit distinction between exploration and exploitation steps as done in ǫ−first algorithm. In both [12] and [11] the algorithm proceeds as long as sufficient budget existing to play the arms.\nFinally, we remark that our setting differs from the setting considered in [13] which puts restrictions on the actions taken by the adversary and not the learner as in our case."
    }, {
      "heading" : "4 Full Information Constrained Regret Minimization",
      "text" : "In this section, we present the basic algorithm for the online learning with constraint problem and analyze its performance via the primal-dual method in adversarial setting.\nA straightforward approach to tackle the problem is to modify the reward functions of the learner to include constraint term with a penalty coefficient that adjust the probability of the actions when the constraint is violated. This approach circumvents the problem of a constrained online learning by turning it into an unconstrained problem. But a simple analysis shows that, in the adversarial setting, this simple penalty based approach fails to attain gradually vanishing bounds for regret and the violation of constraint. The main difficulty arises from the fact that an adaptive adversary can play with the penalty coefficient associated with the constraint in order to weaken the influence of the penalty parameter which results in linear bound on at least one of the measures, i.e. either regret bound or violation of the constraint.\nAlternatively, since the constraint vector in our setting is stochastic, one possible solution is to take an exploration and exploitation scheme, i.e., to burn a small portion ǫ of the rounds to estimate the constraint vector c by c̃ and then in the remaining (1 − ǫ)T rounds follow the existing algorithms with restricted decisions, i.e., p ∈ ∆K ∩ p⊤c̃ ≥ c0, where ∆K is the simplex over [K]. The parameter ǫ balances the accuracy of estimating c and the number of rounds for exploitation to increase the total reward. One may hope that by careful adjustment of ǫ, it would be possible to get satisfactory bounds on regret and the violation of the constraint. But unfortunately this naive approach suffers from two main drawbacks. First, the number of rounds T is not known in advance. Second, the decisions are made by projecting into an estimated domain p⊤c̃ ≥ c0 instead of the true domain p⊤c ≥ c0 which is problematic as follows. In order to show the regret bound, we need to relate the best cumulative reward in the estimated domain to that in the true domain, which however requires imposing a regularity condition on reward and constrain vectors to be solvable [14]. Basically, we can make the algorithm adaptive to T by using a similar idea to epoch greedy [15] algorithm that runs exploration/exploitation in epochs, but it still suffers from the second drawback. Additionally, projection to the inaccurate estimated constraint c̃ does not exclude the possibility that the solution will be infeasible.\nHere we take a different path to solve the problem. The proposed algorithm is inspired by the theory of Lagrangian method in constrained optimization. The intuition behind the proposed algorithm is to optimize one criterion (i.e., minimizing regret or maximizing the reward) subject to explicit constraint on the restrictions that the learner needs to satisfy in average for the sequence of the decisions. A challenging ingredient in this formulation is that of establishing bounds on the regret and the violation of the constraint. In particular, our\nalgorithms will exhibit a bound in the following structure,\nRegretT + Violation2T O(T 1−α) ≤ O(T 1−β), (3)\nwhere ViolationT is a term related to the violation of the constraint in long term. From (3) we can derive a bound on regret and the violation of the constraint as\nRegretT ≤ O(T 1−β) (4) ViolationT ≤ √ O ([T + T 1−β]T 1−α), (5)\nwhere the last bound follows the fact −RegretT ≤ O(T ). The detailed steps of the proposed algorithm are shown in LEWA. The algorithm keeps two set of variables: the weight vector wt and the Lagrangian multiplier λt. The high level interpretation of the algorithm is as follows: if the constraint is being violated a lot, the decision maker places more weight on the constraint controlled by λt; but it tunes down the weight on the constraint when the constraint is satisfied reasonably. We note the LEWA is equivalent to the original EWA when the constraint is satisfied at each iteration, i.e., p⊤t ct ≥ c0, which gives λ1 = · · · = λt = . . . = 0. It should be emphasized that in some previous works such as [11], the learner is not allowed to exceed the pre-specified threshold for the violation of the constraint and the game stops as soon as the learner violates the constraint. In contrast, within our setting, the learner’s goal is to obtain sub-linear bound on the long term violation of the constraint.\nWe now state the main theorem about the performance of LEWA algorithm.\nTheorem 1. Let p1,p2, · · · ,pT be the sequence of randomized decisions over the set of actions [K] := {1, 2, · · · ,K} produced by LEWA algorithm under the sequence of adversarial rewards r1, r2, · · · , rT ∈ [0, 1]K observed for these decisions. Let λ1, λ2, · · · , λT be the corresponding dual sequence. By setting η =\n√ 4 lnK/(9T ) and δ = η/2 we have:\nmax p⊤c≥c0\nT∑\nt=1\np⊤rt − E [ T∑\nt=1\np⊤t rt\n] ≤ 3 √ T lnK and\nE\n[ T∑\nt=1\n(c0 − p⊤t c) ]\n+\n≤ O(T 3/4),\nwhere expectation is taken over randomness in c1, · · · , cT . From Theorem 1 we see that the LEWA algorithm attains the optimal bound for the regret and an O(T 3/4) bound on the violation of the constraint. Before proving the Theorem 1, we state two lemmas that pave the way to the proof of theorem.\nLemma 1. [Primal Inequality] Let Rt = R 1 t + λtR 2 t , where R 1 t ,R 2 t ∈ RK+ , wt+1 = wt◦exp(ηRt), and pt = wt/w⊤t 1. Assuming max(‖R1t‖∞, ‖R2t‖∞) ≤ s, we have the following primal equality\nT∑\nt=1\n(p− pt)⊤Rt ≤ lnK\nη + s2\n( ηT\n4 +\nη\n4\nT∑\nt=1\nλ2t\n) . (6)\nProof. Let Wt = ∑K i=1 w t i . We first show an upper bound and a lower bound on lnWT+1/W1, followed by combining the bounds together. We have\nT∑\nt=1\nln Wt+1 Wt = ln WT+1 W1\n= ln\nK∑\ni=1\nwT+1i − lnK ≥ ln K∑\ni=1\npiw T+1 i − lnK ≥ ηp⊤\nT∑\nt=1\nRt − lnK,\nwhere the last inequality follows from the concavity of the log function. By following Lemma 2.2 in [5], we obtain\nT∑\nt=1\nln Wt+1 Wt =\nT∑\nt=1\nK∑\ni=1\nwti exp(ηR t i)∑K\nj=1 w t j\n≤ η T∑\nt=1\nK∑\ni=1 wti∑K j=1 w t j Rti + η2 8 s2(1 + λt) 2 ≤ η T∑ t=1 p⊤t Rt + η2 8 T∑ t=1 s2(1 + λt) 2.\nCombining the lower and upper bounds and using the inequality (a + b)2 ≤ 2(a2 + b2), we obtain the desired inequality in (6).\nLemma 2. [Dual Inequality] Let gt(λ) = δ 2 λ2 + λ(βt − c0), λt+1 = [(λt − η∇gt(λt)]+, and λ1 = 0. Assuming η > 0, 0 ≤ βt ≤ β0, we have T∑\nt=1\n(λt − λ)(βt − c0) + δ\n2\nT∑\nt=1\n(λ2t − λ2) ≤ λ2\n2η + (c20 + β 2 0)ηT. (7)\nProof. First we note that\nλt+1 = [λt − η∇gt(λt)]+ = [(1− δη)λt − η(βt − c0)]+ ≤ [(1− δη)λt + ηc0]+.\nBy induction on λt, we can obtain λt ≤ c0 δ . Applying the standard analysis of online gradient descent [16] yields\n|λt+1 − λ|2 = |Π+[λt − η(δλt + βt − c0)]− λ|2\n≤ |λt − λ|2 + |η(δλt − c0) + ηβt|2 − 2(λt − λ)(η∇gt(λt)) ≤ |λt − λ|2 + 2η2c20 + 2η2β20 + 2η(gt(λ) − gt(λt)).\nThen, by rearranging the terms we get\ngt(λt)− gt(λ) ≤ 1\n2η\n( |λt+1 − λ|2 − |λt − λ|2 ) + η(c20 + β 2 0).\nExpanding the terms on l.h.s and taking the sum over t, we obtain the inequality as desired.\nProof. [of Theorem 1] Applying Rt = rt + λtct to the primal inequality in Lemma 1, where max(‖rt‖∞, ‖ct‖∞) ≤ 1, we have\nT∑\nt=1\n(p− pt)⊤(rt + λtct) ≤ lnK\nη +\nηT\n4 +\nη\n4\nT∑\nt=1\nλ2t .\nApplying βt = p ⊤ t ct to the dual inequality in Lemma 2, where βt ≤ 1, c0 ≤ 1, we have\nT∑\nt=1\n(λt − λ)(p⊤t ct − c0) + δ\n2\nT∑\nt=1\n(λ2t − λ2) ≤ λ2\n2η + 2ηT.\nCombining the above two inequalities gives\nT∑\nt=1\n(p⊤rt − p⊤t rt) + T∑\nt=1\nλ(c0 − p⊤t ct)− ( δT\n2 +\n1\n2η\n) λ2\n≤ lnK η + 9ηT 4 +\n( η\n4 − δ 2\n) T∑\nt=1\nλ2t + ∑\nt=1\nλt(c0 − p⊤ct).\nTaking expectation over ct, t = 1, · · · , T , by using E[ct] = c and noting that pt and λt are independent of ct, we have\nE\n[ T∑\nt=1\n( p⊤rt − p⊤t rt ) + T∑\nt=1\nλ(c0 − p⊤t c)− ( δT\n2 +\n1\n2η\n) λ2\n]\n≤ lnK η + 9 4 ηT + E\n[( η\n4 − δ 2\n) T∑\nt=1\nλ2t\n] + E [ T∑\nt=1\nλt(c0 − p⊤c) ] .\nLet p be the solution satisfying p⊤c ≥ c0. Noting that η4 − δ2 ≤ 0 and taking maximization over λ > 0 in l.h.s, we get\nE [ max\np⊤c≥c0\nT∑\nt=1\np⊤rt − p⊤t rt ] + E   [∑T t=1(c0 − p⊤t c) ]2 +\n2(δT + 1/η)\n  ≤ lnK\nη +\n9 4 ηT.\nBy plugging the values of η and δ, and noting the similar structure of above inequality as in (3) and writing in (4) and (5) formats, we obtain the desired bound for regret and the violation of the constraints in long term.\nRemark 1. We note that when deriving the bound for ViolationT , we simply use a weak lower bound on regret as RegretT ≥ −T . It is possible to obtain an improved bound by considering tighter bound for the RegretT . One way to do this is to bound the regret by the variation of the reward vectors as VariationT = ∑T t=1 ‖rt − r̂T ‖∞, where r̂T = (1/T ) ∑T t=1 rt denotes the mean of rt, t ∈ [T ]. The analysis in A bounds the violation of the constraint in terms of VariationT as\n[ T∑\nt=1\n(c0 − x⊤t c) ]\n+\n≤ O( √ T ) +O(T 1/4 √ VariationT ).\nThis bound is significantly better when the variation of the reward vectors is small and in worst case it attains an O(T 3/4) bound similar to Theorem 1."
    }, {
      "heading" : "4.1 A High Probability Bound",
      "text" : "The performance bounds proved in the previous section for the regret and the violation of the constraint only holds in expectation which may have enormous fluctuations around its mean. Here, with a simple trick, we present a modified\nversion of the LEWA algorithm which attains similar bounds with overwhelming probability. To this end, we slightly change the original LEWA algorithm. More specifically, instead of using ct in updating λt+1, we use the average estimate and add a confidence bound to achieve a more accurate estimation of the constraint vector c. The following theorem bounds the regret and the violation of the constrain in high probability for the modified algorithm.\nTheorem 2. Let αt = 1√ t\n√ (1/2) ln (2/ǫ), η = O(T−1/2), and δ = η/2. By\nrunning Algorithm 2 we have with probability 1− ǫ\nmax p⊤c≥c0\nT∑\nt=1\np⊤rt − T∑\nt=1 p⊤t rt ≤ Õ(T 1/2) and [\nT∑\nt=1\n(c0 − p⊤t c) ]\n+\n≤ O(T 3/4),\nwhere Õ(·) omits the log term in T . Proof. Applying Rt = rt + λtct to the primal inequality in Lemma 1, where max(‖rt‖∞, ‖ct‖∞) ≤ 1, we have\nT∑\nt=1\n(p− pt)⊤(rt + λtct) ≤ lnK\nη +\nηT\n4 +\nη\n4\nT∑\nt=1\nλ2t .\nApplying βt = p ⊤ t ct +αt to the dual inequality in Lemma 2, where βt ≤ 1+α1, and c0 ≤ 1, we have T∑\nt=1\n(λt − λ)(p⊤t ct + αt − c0) + δ\n2\nT∑\nt=1\n(λ2t − λ2) ≤ λ2\n2η + [1 + (1 + α1)\n2]ηT.\nCombining the above two inequalities results in\nT∑\nt=1\n( p⊤rt − p⊤t rt ) + T∑\nt=1\nλ(c0 − p⊤t ct − αt)− ( δT\n2 +\n1\n2η\n) λ2\n≤ lnK η +\n( 13\n4 + 2α21\n) ηT + [( η\n4 − δ 2\n) T∑\nt=1\nλ2t\n] + [ T∑\nt=1\nλt(c0 − p⊤ct − αt) ] .\nLet p be the solution satisfying p⊤c ≥ c0. Noting that η4 − δ2 ≤ 0, and with a probability 1− ǫ, |p⊤c− p⊤ct| ≤ αt, which is due to the Hoeffding’s inequality [17], by taking maximization over λ > 0 on the l.h.s, we have with a probability 1− ǫT ,\nmax p⊤c≥c0\nT∑\nt=1\np⊤rt − p⊤t rt +   [∑T t=1(c0 − p⊤t ct − αt) ]2 +\n2(δT + 1/η)\n  ≤ lnK\nη +\n( 13\n4 + 2α21\n) ηT\nPluging the stated values of η and δ, we have, with a probability 1− ǫT ,\nmax p⊤c≥c0\nT∑\nt=1\np⊤rt − p⊤t rt ≤ O ( T 1/2 ln(1/ǫ) )\n[ T∑\nt=1\n(c0 − p⊤t c) ]\n+\n≤ √ (T + T 1/2 ln(1/ǫ))T 1/2 + ∑\nt\n(p⊤t ct + αt − p⊤t c)\n≤ O(T 3/4) + 2 T∑\nt=1\nαt ≤ O(T 3/4) +O ( T 1/2 ln(1/ǫ) ) .\nBy replacing ǫ with ǫ/T and noting that O(T 1/2 lnT ) ≤ O(T 3/4), we obtain the results stated in the theorem."
    }, {
      "heading" : "5 Bandit Constrained Regret Minimization",
      "text" : "In this section, we generalize our results to the bandit setting for both rewards and constraints. In the bandit setting, at each iteration, we are required to choose an action it from the pool of the actions [K]. Then only the reward and the constraint feedback for action it are revealed to the learner, i.e. r t it , c t it . In this\ncase, we are interested in the regret bound as maxp⊤c≥c0 ∑T t=1 p ⊤rt − ∑T t=1 r t it . In the classical setting, i.e., without constraint, this problem can be solved in stochastic and adversarial settings by UCB and Exp3 algorithms proposed in [18] and [19], respectively. The algorithm is shown in BanditLEWA algorithm which uses the similar idea to Exp3 for exploration and exploitation.\nBefore presenting the performance bounds of the algorithm, let us introduce two vectors: r̂t is all zero vector except in itth component which is set to be r̂tit = r t it /ptit and similarly ĉt is all zero vector except in itth component which is set to be ĉtit = c t it /ptit . It is easy to verify that Eit [r̂t] = rt and Eit [ĉt] = ct. The following theorem shows that BanditLEWA algorithm achieves O(T 3/4) regret bound and O(T 3/4) bound on the violation of the constraint in expectation.\nTheorem 3. Let γ = O(T−1/2), η = γ\nK\nδ\nδ + 1 , by running BanditLEWA algo-\nrithm, we have\nmax p⊤c≥c0\nT∑\nt=1\np⊤rt − E [ T∑\nt=1\nrtit\n] ≤ O(T 3/4) and\nE\n[ T∑\nt=1\n(c0 − p⊤t c) ]\n+\n≤ O(T 3/4).\nProof. In order to have an improved analysis, we first derive an improved primal inequality and an improved dual inequality. Let Rt = r̂t+λtĉt. By following the\nanalysis for Exp3 algorithm [19], we have\nT∑\nt=1\nηq⊤t Rt + η 2q⊤t R 2 t ≥ ln WT+1 W1\n≥ ηp⊤ T∑\nt=1\nRt − lnK. (8)\nDividing both sides by η, and taking expectation we get\nE [ p⊤ T∑\nt=1\nRt − T∑\nt=1\nq⊤t Rt\n] ≤ lnK\nη + ηE\n[ T∑\nt=1\nq⊤t (Rt) 2\n]\n≤ lnK η + ηE\n[ T∑\nt=1\n2q⊤t (r̂t) 2 + 2λ2tq ⊤ t (ĉt) 2\n] ≤ lnK\nη +\n2ηKT 1− γ + 2ηK 1− γ\nT∑\nt=1\nλ2t ,\n(9)\nwhere the third inequality follows from the following inequality\nE[q⊤t (ĉt) 2] = E [ qtit ( btit ptit )2] ≤ 1 1− γE [ ptit ( ctit ptit )2]\n= 1 1− γE [ (ctit) 2\nptit\n] = 1 1− γE [ K∑\ni=1\n(cti) 2\n] ≤ K\n1− γ , (10)\nand the same inequality holds for E[q⊤t (r̂t) 2]. Next, we let gt(λ) = δ 2 λ2+λ(q⊤t ĉt− c0). By following the similar analysis in the proof of Lemma 2, we have\ngt(λt)− gt(λ) ≤ 1\n2η\n( |λ− λt|2 − |λ− λt+1|2 ) + η\n2 |∇gt(λt)|2\n≤ 1 2η\n( |λ− λt|2 − |λ− λt+1|2 ) + η(q⊤t ĉt) 2 + η.\nTaking summation and expectation, we have\nE\n[ T∑\nt=1\ngt(λt)− gt(λ) ] ≤ λ 2\n2η + ηE\n[ ∑\nt\nq⊤t (ĉt) 2\n] + ηT. ≤ λ 2\n2η +\nηKT 1− γ + ηT.\n(11)\nCombining equations (11) and (9) gives\nE\n[ T∑\nt=1\np⊤rt − q⊤t rt ] + E [ T∑\nt=1\nλ(c0 − q⊤t c)− ( δT\n2 +\n1\n2η\n) λ2\n]\n≤ lnK η + 4ηKT 1− γ + ( 2ηK 1− γ − γ 2 ) T∑\nt=1\nλ2t + E\n[ T∑\nt=1\nλt(c0 − p⊤c) ] .\nNoting that (1 − γ)qt ≤ pt, so we get\nE\n[ T∑\nt=1\n(1− γ)p⊤rt − p⊤t rt ] + E [ T∑\nt=1\nλ((1 − γ)c0 − p⊤t c)− ( δT\n2 +\n1\n2η\n) λ2\n]\n≤ lnK η + 4ηKT +\n( 2ηK − (1− γ)δ\n2\n) T∑\nt=1\nλ2t + E\n[ T∑\nt=1\nλt(c0 − p⊤c) ] .\nLet c0 ≥ p⊤c, 2ηK ≤ (1− γ) δ2 . By taking maximization over λ, we have\nE [ max\np⊤c≥c0\nT∑\nt=1\np⊤rt − p⊤t rt ] + E   [∑T t=1((1− γ)c0 − p⊤t c) ]2 +\n2(δT + 1/η)\n \n≤ lnK η + 4ηKT + γT = K(δ + 1) lnK δγ + 4 γδ δ + 1 T + γT\n≤ K(δ + 1) lnK δγ + 5δ + 1 δ + 1 γT ≤\n√ (5δ + 1)K lnK\nδ T .\nThen we obtain\nmax p⊤c≥c0\nT∑\nt=1\np⊤rt − E [ T∑\nt=1\np⊤t rt\n] ≤ √ (5δ + 1)K lnK\nδ T\nE\n[ T∑\nt=1\n(c0 − p⊤t c) ]\n+\n≤ √√√√ ( T + √ (5δ + 1)K lnK\nδ T\n) 2(δT + 1/η) + γT .\nLet γ = O(T−1/2), δ = O(T−1/2), then we get O(T 3/4) regret and O(T 3/4) constraint bounds as claimed.\nAs our previous results, we present an algorithm with a high probability bound on the regret and the violation of the constraint. For ease of exposition, we introduce ct = 1\nt ∑t s=1 cs and c̃t = 1 t ∑t s=1 ĉs. We modify BanditLEWA algorithm\nHigh Probability BanditLEWA(η, γ, δ, and ǫ) initialize: w1 = exp ( ηα √ KT ) 1, and λ1 = 0 , where α = 2 √ ln(4KT/ǫ)\niterate t = 1, 2, . . . , T Set qt = wt/ ∑ j wtj\nSet pt = (1− γ)qt + γ/K Draw action it randomly accordingly to the probabilities pt Receive reward rtit and a realization of constraint c t it\nfor action it Update wt+1 by\nwt+1i = exp\n( η [( r̂ti +\nα\npti √ KT\n) + λt ( c̃ti + 2K\nγ α1 √ t\n)])\nUpdate λt+1 = [(1− δη)λt − η(x ⊤\nt ĉt + αt − c0)]+ end iterate\nso that it uses more accurate estimations rather than using correct expectation in updating the primal and dual variables. To this end, we use upper confidence bound for rewards as Exp3.P algorithm [18] and for constraint vector c. The following theorem states the regret bound and the violation of constraints in long term for the high probability BanditLEWA. Theorem 4. Let αt = √ (1/2) ln(6KT/ǫ)/ √ t, γ = O(T−1/2), η = γ\nβK\nδ\nδ + 1 ,\nand α = 2 √ ln(4KT/ǫ), where β = max{3, 1+2α1}, by running High Probability BanditLEWA, we have with probability 1− ǫ\nmax p⊤c≥c0\np⊤ T∑\nt=1\nrt − T∑\nt=1\nrtit ≤ O(T 3/4/\n√ δ) and\n[ T∑\nt=1\n(c0 − p⊤t c) ]\n+\n≤ O( √ δT ).\nThe proof is deferred to B. From this theorem, when δ = O(T−1/4), the regret and the violation bounds are O(T 7/8) and O(T 7/8), respectively."
    }, {
      "heading" : "6 Conclusions and Future Works",
      "text" : "In this paper we proposed an efficient algorithm for regret minimization under stochastic constraints. The proposed algorithm, namely LEWA, is a primal-dual variant of the exponentially weighted average algorithm and relies on the theory of Lagrangian theory in constrained optimization. We establish expected and high probability bounds on the regret and the long term violation of the constraint in full information and bandit settings using novel theoretical analysis. In particular, in full information setting, LEWA algorithms attains optimal Õ( √ T ) regret bound and O(T 3/4) bound on the violation of the constraints in expectation, and with a simple trick in high probability. The present work leaves open a number of interesting directions for future work. In particular, extending the framework to handle multi-criteria online\ndecision making is left to future work. Turning the proposed algorithm to the one which exactly satisfies the constraint in the long run is also an interesting problem. Finally, it would be interesting to see if it is possible to improve the bound obtained for the violation of the constraint."
    }, {
      "heading" : "7 References",
      "text" : ""
    }, {
      "heading" : "B Proof of Theorem 4",
      "text" : "Similar to the analysis for Exp3.P algorithm in [18], we have have the following two upper confidence bounds,\nT∑\nt=1\nr̂ti + ασ t i ≥\nT∑\nt=1\nrti , ∀i (15)\nT∑\nt=1\n( c̃ti + 2K\nγ α1√ t\n) ≥ T∑\nt=1\ncti, ∀i (16)\nwhere σti = √ KT + 1KT ∑t s=1 1/p s i . Following the same line of proof as in [18], we have\nT∑\nt=1\nln Wt+1 Wt\n≤ η ∑\nt=1\nq⊤t (r̂t + λtc̃t) + αη 1− γ √ kT + 4α1ηK γδ √ T\n+ 4η2\n1− γ\nT∑\nt=1\n∑\ni\nr̂ti + 4η2\n1− γ\nT∑\nt=1\nλ2t c̃ ⊤ t 1+\n4α2η2 γ(1− γ) + 16α21η 2K2 γ2δ2 (1 + ln(T ))\nand\nT∑\nt=1\nln Wt+1 Wt\n≥ ηp⊤ T∑\nt=1\n(rt + λtct)− lnK\n+ η\n( T∑\nt=1\n(r̂ti + ασ t i + λtc̃ t i + λt\n2K\nγ αt)− p⊤\nT∑\nt=1\n(rt + λtct)\n) .\nThen we have\np⊤ T∑\nt=1\n(rt + λtct) +\n( T∑\nt=1\n(r̂ti + ασ t i + λtc̃ t i + λt\n2k\nγ αt)− p⊤\nT∑\nt=1\n(rt + λtct) ) − q⊤t (r̂t + λtc̃t)\n≤ α 1− γ\n√ KT + 4α1K\nγδ\n√ T + 4η\n1− γ\nT∑\nt=1\n∑\ni\nr̂ti + 4η\n1− γ\nT∑\nt=1\nλ2t c̃ ⊤ t 1\n+ 4α2η γ(1− γ) + 16α21ηK γ2δ2 (1 + lnT ) + lnK η\nOn the other side, let gt(λ) = δ 2 λ2 +λ(q⊤t ĉt +αt − c0), with probability 1− ǫ/4, we have\ngt(λt)− gt(λ) ≤ 1\n2η\n( |λ− λt|2 − |λ− λt+1|2 ) + η\n2 |∇λgt(λt)|2\n≤ 1 2η\n( |λ− λt|2 − |λ− λt+1|2 ) + η/2(x⊤t ĉt − c0 + αt + δλt)2\n≤ 1 2η\n( |λ− λt|2 − |λ− λt+1|2 ) + η(x⊤t ĉt) 2 + ηC\n≤ 1 2η\n( |λ− λt|2 − |λ− λt+1|2 ) + ηx⊤t (ĉt) 2 + ηC\n≤ 1 2η\n( |λ− λt|2 − |λ− λt+1|2 ) + η\n1− γ 1 ⊤ĉt + ηC\n≤ 1 2η\n( |λ− λt|2 − |λ− λt+1|2 ) + η\n1− γ (1 ⊤c+\nK γ αt) + ηC\nwhere C = (1+α1) 2, αt = α1/ √ t. Taking summation over t = 1, · · · , T of above inequalities, we have\nT∑\nt=1\nδ 2 λ2t − λt(c0 − αt − q⊤t ĉt) + λ(c0 − αt − q⊤t ĉt)− δ 2 λ2\n≤ λ 2\n2η +\nT∑\nt=1\nη\n1− γ (1 ⊤c+\nK γ αt) + ηCT\nCombing the primal inequality and the dual inequality, we have\n( T∑\nt=1\n(r̂ti + ασ t i + λtc̃ t i + λt\n2K\nγ αt)− p⊤\nT∑\nt=1\n(rt + λtct)\n)\n+\nT∑\nt=1\np⊤(rt + λtct)− q⊤t r̂t − λt(c0 − αt) + δ 2 λ2t + λ(c0 − αt − q⊤t c̃t)− δ 2 λ2\n≤ λ 2\n2η +\nT∑\nt=1\nη\n1− γ (1 ⊤c+\nK γ αt) + ηCT + α 1− γ √ KT + 4α1K γδ √ T + 4η 1− γ T∑\nt=1\n∑\ni\nr̂ti\n+ 4η\n1− γ\nT∑\nt=1\nλ2t c̃ ⊤ t 1+\n4α2η γ(1− γ) + 16α21ηK γ2δ2 (1 + lnT ) + lnK η .\nThen with probability 1− ǫ, we have the following inequality:\n( T∑\nt=1\n(r̂ti + ασ t i + λtc̃ t i + λt\n2K\nγ αt)− p⊤\nT∑\nt=1\n(rt + λtct)\n)\n+ T∑\nt=1\np⊤(rt + λtct)− q⊤t r̂t − λt(c0 − αt) + δ 2 λ2t + λ(c0 − αt − q⊤t c̃t)− δ 2 λ2\n≤ λ 2\n2η +\nT∑\nt=1\nη\n1− γ (1 ⊤c+\nK γ αt) + ηCT + α 1− γ √ KT + 4α1k γδ √ T + 4η 1− γ T∑\nt=1\n∑\ni\nr̂ti\n+ 4η\n1− γ\nT∑\nt=1\nλ2t c̃ ⊤ t 1+\n4α2η γ(1− γ) + 16α21ηK γ2δ2 (1 + lnT ) + lnK η .\nLet ÛT = maxi ∑T t=1(r̂ t i + ασ t i + λt(c̃ t i + 2K γ αt)), η = γ βK δ δ+1 , γ ≤ (β)/(4 + β), then we have\n( 1− 4γ\nβ(1 − γ)\n) ÛT − p⊤ T∑\nt=1\n(rt + λtct)\n+ T∑\nt=1\np⊤(rt + λtct)− q⊤t r̂t − λt(c0 − αt) + T∑\nt=1\nλ(c0 − αt − q⊤t c̃t)− ( δT\n2 +\n1\n2η\n) λ2\n≤ T∑\nt=1\nη\n1− γ (1 ⊤c+\nK γ αt) + ηCT + α 1− γ √ KT\n+ 4α1K\nγδ\n√ T + 4α2η\nγ(1− γ) + 16α21ηK γ2δ2 (1 + lnT ) + lnK η .\nSince ÛT ≥ maxi ∑T t=1 r t i +λtc t i, and p ⊤ ∑T t=1(rt+λtct) ≤ maxi ∑T t=1 r t i +λtc t i, then we have with probability 1− ǫ,\nT∑\nt=1\np⊤rt − q⊤t r̂t − λt(c0 − αt − p⊤ct) + T∑\nt=1\nλ(c0 − αt − q⊤t c̃t)− ( δT\n2 +\n1\n2η\n) λ2\n≤ T∑\nt=1\nη\n1− γ (1 ⊤c+\nK γ αt) + ηCT + α 1− γ √ KT + 4α1K γδ √ T + 4α2η γ(1− γ)\n+ 16α21ηK\nγ2δ2 (1 + lnT ) +\nlnK\nη +\n4γ\nβ(1− γ) maxi\n( T∑\nt=1\nrti + λtc t i\n)\n≤ α1 √ T\n1− γ + γT β(1− γ) + CγT β + α 1− γ √ KT + 4α1K γδ √ T\n+ 4α2 β(1 − γ)K + 16α21 βγδ2 (1 + lnT ) + β(K lnK) γ δ + 1 δ + 4γT β(1 − γ) δ + 1 δ .\nThen\nT∑\nt=1\np⊤rt − p⊤t r̂t +\n[∑T t=1((1− γ)(c0 − αt)− p⊤t c̃t) ]2 +\n2(δT + 1/η)\n≤ α1 √ T + C1γT\nβ + α\n√ KT + 4α1k\nγδ\n√ T + 4α2\nβK + 16α21 βγδ2 (1 + lnT )\n+ β(K lnK)\nγ\nδ + 1\nδ + 4γT\nδ + 1\nβδ .\nLet γ = O(T−1/4), η = O(T−1/4), then we obtain\nmax p⊤c≥c0\np⊤ T∑\nt=1\nrt − T∑\nt=1\nrtit ≤ O(T 3/4/\n√ δ) and\n[ T∑\nt=1\n(c0 − p⊤t c) ]\n+\n≤ O( √ δT ),\nwhen δ = O(T−1/4), the regret bound is O(T 7/8), the worse case constraint bound is O(T 7/8)."
    } ],
    "references" : [ {
      "title" : "On stochastic and worst-case models for investing",
      "author" : [ "E. Hazan", "S. Kale" ],
      "venue" : "in: NIPS,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2009
    }, {
      "title" : "The weighted majority algorithm, Inf. Comput",
      "author" : [ "N. Littlestone", "M.K. Warmuth" ],
      "venue" : null,
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 1994
    }, {
      "title" : "Path kernels and multiplicative updates",
      "author" : [ "E. Takimoto", "M.K. Warmuth" ],
      "venue" : "Journal Machine Learnning Research",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2003
    }, {
      "title" : "Prediction, Learning, and Games",
      "author" : [ "N. Cesa-Bianchi", "G. Lugosi" ],
      "venue" : null,
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2006
    }, {
      "title" : "Unified algorithms for online learning and competitive analysis, COLT",
      "author" : [ "J.S.N. Niv Buchbinder", "Shahar Chen", "O. Shamir" ],
      "venue" : null,
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2012
    }, {
      "title" : "Online learning with sample path constraints",
      "author" : [ "S. Mannor", "J.N. Tsitsiklis", "J.Y. Yu" ],
      "venue" : "Journal of Machine Learning Research",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2009
    }, {
      "title" : "A decision-theoretic generalization of on-line learning and an application to boosting",
      "author" : [ "Y. Freund", "R.E. Schapire" ],
      "venue" : "J. Comput. Syst. Sci",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 1997
    }, {
      "title" : "A parameter-free hedging algorithm",
      "author" : [ "K. Chaudhuri", "Y. Freund", "D. Hsu" ],
      "venue" : "in: NIPS,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2009
    }, {
      "title" : "Epsilonfirst policies for budget-limited multi-armed bandits",
      "author" : [ "L. Tran-Thanh", "A.C. Chapman", "E.M. de Cote", "A. Rogers", "N.R. Jennings" ],
      "venue" : "in: AAAI,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2010
    }, {
      "title" : "Knapsack based optimal policies for budget-limited multi-armed bandits, CoRR abs/1204.1909",
      "author" : [ "L. Tran-Thanh", "A.C. Chapman", "A. Rogers", "N.R. Jennings" ],
      "venue" : null,
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 1909
    }, {
      "title" : "Repeated games against budgeted adversaries",
      "author" : [ "J. Abernethy", "M.K. Warmuth" ],
      "venue" : "in: NIPS,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2010
    }, {
      "title" : "A characterization of stability in linear programming, Operations Research",
      "author" : [ "S.M. Robinson" ],
      "venue" : null,
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 1977
    }, {
      "title" : "The epoch-greedy algorithm for multi-armed bandits with side information",
      "author" : [ "J. Langford", "T. Zhang" ],
      "venue" : "in: NIPS,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2007
    }, {
      "title" : "Online convex programming and generalized infinitesimal gradient ascent",
      "author" : [ "M. Zinkevich" ],
      "venue" : "in: Proceedings of the 20th International Conference on Machine Learning,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2003
    }, {
      "title" : "Finite-time analysis of the multiarmed bandit problem, Machine Learning",
      "author" : [ "P. Auer", "N. Cesa-Bianchi", "P. Fischer" ],
      "venue" : null,
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2002
    }, {
      "title" : "The nonstochastic multiarmed bandit problem",
      "author" : [ "P. Auer", "N. Cesa-Bianchi", "Y. Freund", "R.E. Schapire" ],
      "venue" : "SIAM J. Comput",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2002
    }, {
      "title" : "Extracting certainty from uncertainty: Regret bounded by variation in costs",
      "author" : [ "E. Hazan", "S. Kale" ],
      "venue" : "in: COLT,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2008
    } ],
    "referenceMentions" : [ ],
    "year" : 2012,
    "abstractText" : "Online learning constitutes a mathematical and compelling framework to analyze sequential decision making problems in adversarial environments. The learner repeatedly chooses an action, the environment responds with an outcome, and then the learner receives a reward for the played action. The goal of the learner is to maximize his total reward. However, there are situations in which, in addition to maximizing the cumulative reward, there are some additional constraints on the sequence of decisions that must be satisfied on average by the learner. In this paper we study an extension to the online learning where the learner aims to maximize the total reward given that some additional constraints need to be satisfied. By leveraging on the theory of Lagrangian method in constrained optimization, we propose Lagrangian exponentially weighted average (LEWA) algorithm, which is a primal-dual variant of the well known exponentially weighted average algorithm, to efficiently solve constrained online decision making problems. Using novel theoretical analysis, we establish the regret and the violation of the constraint bounds in full information and bandit feedback models.",
    "creator" : "LaTeX with hyperref package"
  }
}