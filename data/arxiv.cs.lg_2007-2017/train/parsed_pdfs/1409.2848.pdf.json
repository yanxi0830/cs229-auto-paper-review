{
  "name" : "1409.2848.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "A Stochastic PCA Algorithm with an Exponential Convergence Rate",
    "authors" : [ "Ohad Shamir" ],
    "emails" : [ "ohad.shamir@weizmann.ac.il" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Principal Component Analysis (PCA) is one of the most common tools for unsupervised data analysis and preprocessing. Given a dataset of n instances x1, . . . ,xn in Rd, we are interested in finding a a d×k matrix U with orthonormal columns, which minimizes\n− 1 n n∑ i=1 ‖U>x‖2. (1)\nU can be interpreted as defining the k-dimensional subspace, on which the projection of the data has largest possible variance. Finding this subspace has numerous uses, from dimensionality reduction and data compression to data visualization, and the problem is extremely well-studied.\nIn this paper, we will focus on the simplest possible form of this problem, where k = 1, and we are interested in finding a single direction along which the variance of the data is maximized (however, as we discuss later, the algorithm to be presented can be readily extended to solve Eq. (1) for k > 1). This direction is specified by a unit vector v1, which optimizes\nmin v:‖v‖=1 − 1 n n∑ i=1 〈v1,x〉2 = − v>1\n( 1\nn n∑ i=1 xix > i\n) v1. (2)\nIn other words, we seek to find the largest eigenvector v1 of the covariance matrix 1n ∑n i=1 xix > i .\nWhen the data size n and the dimension d are modest, this problem can be solved exactly by computing the d × d covariance matrix, and performing an eigendecomposition. However, the required runtime is O(nd2 + d3), which is prohibitive in large-scale applications. Moreover, even storing a d × d matrix in memory can be impossible when d is very large. One possible approach is using iterative methods such as power iteration or the Lanczos method [4]. If the covariance matrix has an eigengap λ between its first and\nar X\niv :1\n40 9.\n28 48\nv1 [\ncs .L\nG ]\n9 S\nep 2\n01 4\nsecond eigenvalues, then these algorithms can be shown to produce a unit vector which is -far from v1 (or −v1) afterO ( log(1/ ) λp ) iterations (where p = 1 for power iterations, and p = 1/2 for the Lanczos method). However, each iteration requires multiplying one or more vectors by the covariance matrix, which requires O(nd) time (by passing through the entire data). Thus, the total runtime is O ( dn log(1/ )λp ) . When λ is small, this runtime is equivalent to many passes over the data, which can be prohibitive for large datasets. An alternative to these deterministic algorithms are stochastic and incremental algorithms (e.g. [8, 11, 12] and more recently, [1, 10, 2]), which utilize the structure of the covariance method. In contrast to the algorithms above, these algorithms perform much cheaper iterations by choosing some xi (uniformly at random or otherwise), and updating wt using only xit . In general, the runtime of each iteration is only O(d). On the flip side, due to their stochastic and incremental nature, the convergence rate (when known) is quite slow, with the number of required iterations scaling at least as 1/ . Thus, the runtime of these methods is at least on the order of O ( d1 ) : Useful for getting a low to medium-accuracy solution, but prohibitive when a high-accuracy solution is required. In this paper, we propose a new stochastic PCA algorithm, denoted as VR-PCA 1, which under suitable assumptions, has provable runtime of\nO ( d ( n+ 1\nλ2\n) log ( 1 )) ,\nwhere λ is the eigengap parameter. This algorithm combines the advantages of the previously discussed approaches, while avoiding their main pitfalls: On one hand, the runtime depends only logarithmically on the accuracy , so it is suitable to get high-accuracy solutions; While on the other hand, the runtime scales as the sum of the data size n and a factor involving the eigengap parameter λ, rather than their product. This means that the algorithm is still applicable when λ is relatively small. In fact, as long as λ ≥ Ω(1/ √ n), this runtime bound is better than those mentioned earlier, and equals dn up to logarithmic factors: Proportional to the time required to perform a single scan of the data.\nVR-PCA builds on a recently-introduced technique for stochastic gradient variance reduction, which has been previously studied (see [6] as well as [9, 7]). However, the setting in which we apply this technique is quite different from previous works, which crucially relied on the strong convexity of the optimization problem (at least locally), and often assume an unconstrained domain. In contrast, our algorithm attempts to minimize the function in Eq. (2), which is nowhere convex, let alone strongly convex (in fact, it is concave everywhere), and over a non-convex domain. As a result, the analysis in previous papers is inapplicable, and we require a new and different analysis to understand the performance of the algorithm."
    }, {
      "heading" : "2 Algorithm and Analysis",
      "text" : "The pseudo-code of our algorithm appears as Algorithm 1 below. We refer to a single execution of the inner loop as an iteration, and each execution of the outer loop as an epoch. Thus, the algorithm consists of several epochs, each of which consists of running m iterations. We note that the runtime of each iteration isO(dn), and the runtime of each epoch, besides the iterations, is dominated by computing ũ in O(dn) time.\n1VR stands for “variance-reduced”.\nAlgorithm 1 VR-PCA Parameters: Step size η, epoch length m Input: Data set {xi}ni=1, Initial unit vector w̃0 for s = 1, 2, . . . do ũ = 1n ∑n i=1 xix > i w̃s−1\nw0 = w̃s−1 for t = 1, 2, . . . ,m do\nPick it ∈ {1, . . . , n} uniformly at random w′t = wt−1 + η ( xitx > it (wt−1 − w̃s−1) + ũ ) wt = 1 ‖w′t‖\nw′t end for w̃s = wm\nend for\nTo understand the structure of the algorithm, it is helpful to consider first the well-known Oja’s algorithm for stochastic PCA optimization [11], on which our algorithm is based. In our setting, this rule is reduced to repeatedly sampling xit uniformly at random, and performing the update\nw′t = wt−1 + ηtxitx > itwt−1 , wt =\n1\n‖w′t‖ wt.\nLetting A = 1n ∑n i=1 xix > i , this can be equivalently rewritten as\nw′t = (I + ηtA)wt−1 + ηt ( xitx > it −A ) wt−1 , wt = 1\n‖w′t‖ wt. (3)\nThus, at each iteration, the algorithm performs a power iteration (using a shifted and scaled version of the matrix A), adds a stochastic zero-mean term ηt ( xitx > it −A ) wt−1, and projects back to the unit sphere. Recently, [3] gave a rigorous finite-time analysis of this algorithm, showing that if ηt = O(1/t), then under suitable conditions, we get a convergence rate of O(1/T ).\nThe reason for the relatively slow convergence rate of this algorithm is the constant variance of the stochastic term added in each step. Inspired by recent variance-reduced stochastic methods for convex optimization [6], we change the algorithm in a way which encourages the variance of the stochastic term to decay over time. Specifically, we can rewrite the update of our VR-PCA algorithm as\nw′t = (I + ηA)wt−1 + η ( xitx > it −A ) (wt−1 − ũ) , wt = 1\n‖w′t‖ wt. (4)\nComparing Eq. (4) to Eq. (3), we see that our algorithm also performs a type of power iteration, followed by adding a stochastic term zero-mean term. However, our algorithm picks a fixed step size η, which is more aggressive that a decaying step size ηt. Moreover, the variance of the stochastic term is no longer constant, but rather controlled by ‖wt−1 − ũ‖. As we get closer to the optimal solution, we expect that both ũ and wt−1 will be closer and closer to each other, leading to decaying variance, and a much faster convergence rate, compared to Oja’s algorithm.\nTo generalize the algorithm to the case where more than one variance direction is sought (i.e. solve Eq. (1) for k > 1), all that is needed is to replace the vectors wt, w̃, ũ etc. by d× k matrices Wt, W̃ , Ũ , and replace the normalization step 1‖w′t‖w ′ t by an orthogonalization step. This is completely analogous to how\niterative algorithms such as power iterations and Oja’s algorithm are generalized to the k > 1 case, and the same intuition discussed above still holds.\nA formal analysis of the algorithm appears as Thm. 1. We note that the parameter settings are designed to get the final bound, and may differ from the optimal choice in practice. This issue is further discussed in Sec. 3.\nTheorem 1. Let v1 be an eigenvector of A = 1n ∑n i=1 xix > i corresponding to the largest singular value. Suppose that\n• A has singular values s1, . . . , sd, which satisfy\ns1 ≥ 1 + λ > 1 ≥ s2 ≥ s3 ≥ . . . ≥ sd ≥ 0\nfor some λ > 0.\n• maxi ‖xi‖ and λ are upper-bounded by a constant.\n• 〈w̃0,v1〉 ≥ 1√2 .\nLet δ, ∈ (0, 1) be fixed. If we run the algorithm with any epoch length parameter m and step size η, such that\nη ≤ c1δ2λ , m ≥ c2 log(2/δ)\nηλ , mη2 +\n√ mη2 log(2/δ) ≤ c3, (5)\n(where c1, c2, c3 designates certain positive numerical constants), and for T = ⌈ log(1/ ) log(2/δ) ⌉ epochs, then with probability at least 1− 2 log(1/ )δ, it holds that\n〈w̃T ,v1〉2 ≥ 1− .\nThe proof of the theorem is provided in Sec. 4. It is easy to verify that for any fixed δ, Eq. (5) holds for any sufficiently large m on the order of 1ηλ , as long as η is chosen to be sufficiently smaller than λ. Therefore, by running the algorithm form = Θ ( 1 λ2 ) iterations per epoch, and T = Θ(log(1/ )) epochs, we get accuracy with high probability2 1−2 log(1/ )δ. Since each iteration requiresO(d) time to implement, and each epoch requires an additional O(dn) time to compute ũ, we get a total runtime of\nO ( d ( n+ 1\nλ2\n) log ( 1 )) , (6)\nestablishing an exponential convergence rate. If λ ≥ Ω(1/ √ n), then the runtime is O(dn log(1/ )) – up to log-factors, proportional to the time required just to scan the data once. The assumptions of the theorem require some discussion. First, the assumption that maxi ‖xi‖ is bounded by some constant is without loss of generality, since we can always rescale the data to make it hold, and the scaling factor is absorbed into the eigengap parameter λ. Moreover, based on the experiments in Sec. 3, we suspect it can be relaxed to suitable moment conditions (e.g. the algorithm appears to work\n2Strictly speaking, this statement is non-trivial only in the regime of where log ( 1 ) 1\nδ , but if δ is a reasonably small ( 1),\nthen this is the practically relevant regime. Moreover, as long as the success probability is positive, we can get an algorithm which succeeds with exponentially high probability by an amplification argument: Simply run several independent instantiations of the algorithm, and pick the solution w for which w> ( 1 n ∑n i=1 xix > i ) is largest.\nwell for data from a high-dimensional Gaussian distribution, whose norm scales with √ d). Second, the theorem assumes that we initialize the algorithm with w̃0 for which 〈w̃0,v1〉 ≥ 1√2 . This is not trivial, since if we have no prior knowledge on v1, and we choose w̃0 uniformly at random from the unit sphere, then it is well-known that |〈w̃0,v1〉| ≤ O(1/ √ d) with high probability. Thus, the theorem should be interpreted as analyzing the algorithm’s convergence after an initial “burn-in” period, which results in some w̃0 with a certain constant distance from v1. This period requires a separate analysis, which we leave to future work. However, since we only need to get to a constant distance from v1, the runtime of that period is independent of the desired accuracy . Alternatively, one can use some different stochastic algorithm with finite-time analysis (e.g. [3]) to get to this constant accuracy, from which point our algorithm and analysis takes over. In any case, we note that some assumption on 〈w̃0,v1〉 being bounded away from 0 must hold: If we initialize the algorithm with w̃0 such that 〈w̃0,v1〉 = 0, then the algorithm may fail to converge (a similar property holds for power iterations, and follows from the non-convex nature of the optimization problem).\nFinally, we note that in the context of strongly convex optimization problems, the variance-reduced technique we use leads to algorithms with runtime\nO ( d ( n+ 1\nλ\n) log ( 1 )) ,\nwhere λ is the strong convexity parameter of the problem [6]. Comparing this with our algorithm’s runtime, and drawing a parallel between strong convexity and the eigengap in PCA problems, it is tempting to conjecture that the 1/λ2 in our runtime analysis can be improved to 1/λ. However, we don’t know if this is true, or whether the 1/λ2 factor is necessary in our setting."
    }, {
      "heading" : "3 Experiments",
      "text" : "We now turn to present some preliminary experimental results, which demonstrates the performance of the VR-PCA algorithm.\nFirst, we performed experiments on several synthetic datasets, where 50,000 examples are drawn i.i.d. from a Gaussian distribution in R1000, with zero mean and covariance matrix I + λeie>i . The spectrum of this matrix equals (1 + λ, 1, 1, . . . , 1), corresponding to an eigengap of roughly λ (in practice, due to finite sample effects, the eigengap of the data covariance matrix is slightly different). Each dataset corresponds to a different value of λ. We note that these datasets do not satisfy the boundedness assumption in our analysis (here the norm of each instance scales as √ d), but nonetheless the algorithm appears to work well in practice. We used a fixed choice of parameters, where m = n and η = 0.05/ √ n. This choice of m ensures that at each epoch, the runtime is about equally divided between the stochastic updates and the computation of ũ. The choice of η is motivated by our theoretical analysis, which requires η on the order of 1/ √ n in the regime where m should be on the order of n. For comparison, we also implemented Oja’s algorithm, using several different step sizes. All algorithms were initialized from the same same random vector, chosen uniformly at random from the unit ball. Again, compared to our analysis, this makes things harder for our algorithm, since we require it to perform well also in the ‘burn-in’ phase. The results are displayed in figure 1, and we see that for all values of λ considered, VR-PCA converges much faster than all versions of Oja’s algorithm, on which it is based, even though we did not tune its parameters. Moreover, since the y-axis is in logarithmic scale, we see that the convergence rate is indeed exponential in general.\nNext, we performed a similar experiment using the well-known MNIST dataset, consisting of 70, 000 binary images of handwritten digits, each represented by a 784-dimensional vector. We pre-processed the data by centering it and dividing each coordinate by its standard deviation times the squared root of the\nVR-PCA, to perform n iterations plus computing ũ), and the y-axis equals log10\n(\n1− w>Aw\nv>1 Av1\n)\n, where w is\nthe vector obtained so far.\ndimension. The results appear in figure 2. As before, the VR-PCA algorithm converges at an exponential rate, and much faster than its competitors. However, on this dataset, the initial convergence is relatively slow. The reason for this is that initially we are still very far from the optimum, and the variance-reduction technique is yet to kick in. To mitigate this, we consider a simple hybrid method, which initializes the VRPCA algorithm with the result of running n iterations of Oja’s algorithm. The decaying step size of Oja’s algorithm is more suitable for this ‘burn-in’ phase, and the resulting hybrid algorithm performs uniformly better than each algorithm alone."
    }, {
      "heading" : "4 Proof of Thm. 1",
      "text" : "We use c to designate positive numerical constants, whose value can vary at different places (even in the same line or expression).\nLet\nA = 1\nn n∑ i=1 xix > i = d∑ i=1 siviv > i ,\nbe an eigendecomposition of A, where v1, . . . ,vd are orthonormal vectors, and recall that we assume\ns1 ≥ 1 + λ > 1 ≥ s2 ≥ s3 ≥ . . . ≥ sd ≥ 0.\nfor some λ > 0.\ndata size (assuming 2n accesses per epoch for VR-PCA), and the y-axis equals log10\n(\n1− w>Aw\nv>1 Av1\n)\n, where\nw is the vector obtained so far.\nPart I: Establishing a Stochastic Recurrence Relation\nWe begin by focusing on a single epoch of the algorithm, and a single iteration t, and analyze how 1 − 〈wt,v1〉2 evolves during that iteration. The key result we need is the following lemma:\nLemma 1. Suppose that 〈wt,v1〉 ≥ 12 , and that 〈w̃s−1,v1〉 ≥ 0. If η ≤ min{c, 1 1+λ , cλ}, then\nE [( 1− 〈wt+1,v1〉2 )∣∣wt] ≤ (1− ηλ\n16\n)( 1− 〈wt,v1〉2 ) + cη2 ( 1− 〈w̃s−1,v1〉2 ) .\nProof. Since we focus on a particular epoch s, let us drop the subscript from w̃s−1, and denote it simply at w̃. Rewriting the update equations from the algorithm, we have that\nwt+1 = w′t+1 ‖w′t+1‖ , where w′t+1 = (I + ηA)wt + η(xx > −A)(wt − w̃),\nwhere x is the random instance chosen at iteration t. It is easy to verify that\n〈w′t+1,vi〉 = ai + zi, (7)\nwhere ai = (1 + ηsi)〈wt,vi〉 , zi = ηv>i (xx> −A)(wt − w̃).\nMoreover, since v1, . . . ,vd form an orthonormal basis in Rd, we have\n‖w′t+1‖2 = d∑ i=1 〈vi,w′t+1〉2 = d∑ i=1 (ai + zi) 2. (8)\nLet E denote expectation with respect to x, conditioned on wt. Combining Eq. (7) and Eq. (8), we have\nE [ 〈wt+1,v1〉2 ] = E [ 〈 w′t+1 ‖w′t+1‖ ,v1〉2 ] = E [〈w′t+1,v1〉2 ‖w′t+1‖2 ] = E [ (a1 + z1) 2∑d i=1(ai + zi) 2 ] . (9)\nBy definition, z1, . . . , zd are zero-mean random variables (as a function of x, conditioned on wt) whereas a1, . . . , ad are fixed. Therefore, we can write the above as\nEz1Ez2,...,zd\n[ (a1 + z1) 2\n(a1 + z1)2 + ∑d i=2(ai + zi) 2\n] .\nBy Jensen’s inequality and the fact that z2, . . . , zd are zero-mean, this is at least\nEz1\n[ (a1 + z1) 2\n(a1 + z1)2 + Ez2,...,zd ∑d i=2(ai + zi) 2\n]\n= Ez1\n[ (a1 + z1) 2\n(a1 + z1)2 + Ez2,...,zd ∑d i=2(a 2 i + 2azi + z 2 i )\n]\n= Ez1\n[ (a1 + z1) 2\n(a1 + z1)2 + ∑d i=2 a 2 i + Ez2,...,zd ∑d i=2 z 2 i\n] (10)\nBy definition of zi and the fact that v1, . . . ,vd are orthonormal (hence ∑ i viv > i is the identity matrix), we have\nd∑ i=2 z2i ≤ d∑ i=1 z2i = η 2(wt − w̃)>(xx> −A) ( d∑ i=1 viv > i ) (xx> −A)(wt − w̃)\n= η2(wt − w̃)>(xx> −A)(xx> −A)(wt − w̃) = η2‖(xx> −A)(wt − w̃)‖2.\nSince the spectral norm of xx> −A is assumed to be bounded by a constant, this is at most cη2‖wt − w̃‖2 for some constant c. Plugging this back to Eq. (10), we get the lower bound\nEz1\n[ (a1 + z1) 2\n(a1 + z1) + ∑d i=2 a 2 i + cη 2‖wt − w̃‖2\n] . (11)\nWe now use a Taylor expansion to lower bound the expression in a more convenient form. To simplify the notation, let\nq = d∑ i=2 a2i + cη 2‖wt − w̃‖2,\nso we can write Eq. (11) as\nEz1 [ (a1 + z1) 2\n(a1 + z1)2 + q\n] , (12)\nwhere q is a non-negative quantity. By a Taylor expansion around z1 = 0 (using a Lagrange remainder term), and using the fact that z1 is zero-mean, we have\nEz1 [ (a1 + z1) 2\n(a1 + z1)2 + q\n] ≥ Ez1 [ a21 a21 + q − ( 2a31 (a21 + q) 2 − 2a1 a21 + q ) z1 − ( max z1 (3(a1 + z1) 2 − q)q ((a1 + z1)2 + q)3 ) z21 ] ≥ a 2 1\na21 + q − ( max z1\n(a1 + z1) 2\n((a1 + z1)2 + q)3\n) 3qE[z21 ]. (13)\nTo continue, we note that\n• By definition of a1 and the assumption 〈wt,v1〉 ≥ 12 , we have |ai| ≥ 1 2 .\n• By definition of zi and the assumption that the spectral norm of xx>−A is bounded by some constant, we have that |zi| ≤ cη‖wt − w̃‖ ≤ cη. Moreover, since we assume 〈wt,v1〉 ≥ 12 , it follows that\n|zi| ≤ cη = 2cη 1\n2 ≤ 2cη〈wt,v1〉 ≤ 2cη|ai| ≤\n1 2 |ai|\nif η is small enough.\nCombining these two observations, we have\nmax z1\n(a1 + z1) 2\n((a1 + z1)2 + q)3 ≤\n( 3 2a1 )2((\n1 2a1 )2 + q )3 ≤ 94a21(1 4a 2 1 + 1 4q ) ( 1 4a 2 1 )2 = 9/4(1/4)3 a21a21 + q . Plugging this back into Eq. (13), and using the fact that |z1| ≤ cη‖wt − w̃‖, we get that\nEz1 [ (a1 + z1) 2\n(a1 + z1)2 + q\n] ≥ a 2 1\na21 + q\n( 1− cqE[z21 ] ) ≥ a 2 1\na21 + q\n( 1− cqη2‖wt − w̃‖2 ) . (14)\nConsidering the definition of q, and the fact that v1, . . . ,vd is an orthonormal basis for Rd, we have\nq ≤ c d∑ i=2 〈vi,wt〉2 + cη2‖wt − w̃‖2 ≤ c+ cη2,\nwhich is at most a constant assuming η is small enough. This means that 1− cqη2‖wt− w̃‖2 from Eq. (14) can be lower bounded by 1− cη2‖wt − w̃‖2, so we get\nEz1 [ (a1 + z1) 2\n(a1 + z1)2 + q\n] ≥ a 2 1\na21 + q\n( 1− cη2‖wt − w̃‖2 ) . (15)\nWe now continue by analyzing the a21/(a 2 1 + q) term. Recalling the definition of a1, q, and the fact that\nv1, . . . ,vd is an orthonormal basis, we have\na21 a21 + q = (1 + ηs1) 2〈wt,v1〉2 (1 + ηs1)2〈wt,v1〉2 + ∑d i=2(1 + ηsi) 2〈vi,wt〉2 + cη2‖wt − w̃‖2\n≥ 〈wt,v1〉 2 〈wt,v1〉2 + ( 1+ηs2 1+ηs1 )2∑d i=2〈vi,wt〉2 + cη2‖wt − w̃‖2 = 〈wt,v1〉2\n〈wt,v1〉2 + ( 1+ηs2 1+ηs1 )2 (1− 〈wt,v1〉2) + cη2‖wt − w̃‖2\n= 〈wt,v1〉2 1− ( 1− ( 1+ηs2 1+ηs1 )2) (1− 〈wt,v1〉2) + cη2‖wt − w̃‖2\n≥ 〈wt,v1〉2 ( 1 + ( 1− ( 1 + ηs2 1 + ηs1 )2)( 1− 〈wt,v1〉2 ) − cη2‖wt − w̃‖2 ) ,\nwhere in the last step we used the elementary inequality 11−x ≥ 1 + x for all x ≤ 1 (and this is indeed justified since 〈wt,v1〉 ≤ 1 and 1+ηs21+ηs1 ≤ 1). We can simplify this expression by noting that since s1 ≥ 1 + λ > 1 ≥ s2, and the assumption in the lemma statement that η(1 + λ) ≤ 1, we have\n1 + ηs2 1 + ηs1 ≤ 1 + η 1 + η(1 + λ) = 1− ηλ 1 + η(1 + λ) ≤ 1− ηλ 2 ,\nso we actually have\na21 a21 + q\n≥ 〈wt,v1〉2 ( 1 + ηλ\n2\n( 1− 〈wt,v1〉2 ) − cη2‖wt − w̃‖2 ) .\nPlugging this back into Eq. (15), we get that Ez1 [ (a1 + z1) 2\n(a1 + z1)2 + q\n] ≥ 〈wt,v1〉2 ( 1 + ηλ\n2\n( 1− 〈wt,v1〉2 ) − cη2‖wt − w̃‖2 )( 1− cη2‖wt − w̃‖2 ) ≥ 〈wt,v1〉2 ( 1 + ηλ\n2\n( 1− 〈wt,v1〉2 ) − cη2‖wt − w̃‖2 ) ,\nwhere we used the elementary inequality (1 + x)(1 − y) ≥ 1 + x − 2y if y ≥ 0 and x ≤ 1. Plugging this lower bound back into Eq. (12), and recalling that this constitutes a lower bound on E[〈wt+1,v1〉2], we get\nE[〈wt+1,v1〉2] ≥ 〈wt,v1〉2 ( 1 + ηλ\n2\n( 1− 〈wt,v1〉2 ) − cη2‖wt − w̃‖2 ) . (16)\nWe now get rid of the ‖wt − w̃‖2 term, by noting that\n‖wt − w̃‖ ≤ ‖wt − v1‖+ ‖w̃ − v1‖ = 2− 2〈wt,v1〉+ 2− 2〈wt,v1〉.\nSince we assume that 〈wt,v1〉, 〈w̃,v1〉 are both positive, and they are also at most 1, this is at most\n2− 2〈wt,v1〉2 + 2− 2〈wt,v1〉2 = 2 ( 1− 〈wt,v1〉2 ) + 2 ( 1− 〈w̃,v1〉2 ) .\nPlugging this back into Eq. (16), we get that E[〈wt+1,v1〉2] ≥ 〈wt,v1〉2 ( 1 + ( ηλ\n2 − cη2\n)( 1− 〈wt,v1〉2 ) − cη2 ( 1− 〈w̃,v1〉2 )) ,\nand since we can assume η < λ8c by picking η small enough, this can be simplified to\nE[〈wt+1,v1〉2] ≥ 〈wt,v1〉2 ( 1 + ηλ\n4\n( 1− 〈wt,v1〉2 ) − cη2 ( 1− 〈w̃,v1〉2 )) .\nThe final stage of the proof consists of converting the bound above to a bound on E[1− 〈wt+1,v1〉2] in terms of 1−〈wt+1,v1〉2. To simplify the notation, let b = ( 1− 〈wt,v1〉2 ) and b̃ = cη2 ( 1− 〈w̃,v1〉2 ) , so the bound above implies\nE[1− 〈wt+1,v1〉2] ≤ 1− (1− b) ( 1 + ηλ\n4 b− b̃ ) = 1− (1− b)− ηλ\n4 b(1− b) + (1− b)b̃\n= b− ηλ 4 b(1− b)− bb̃+ b̃\n= b ( 1− ηλ\n4 (1− b)− b̃\n) + b̃.\nPlugging back the definitions of b̃, b, we get that E[1−〈wt+1,v1〉2] ≤ ( 1− 〈wt,v1〉2 )( 1− ηλ\n4 〈wt,v1〉2 − cη2\n( 1− 〈w̃,v1〉2 )) + cη2 ( 1− 〈w̃,v1〉2 ) .\nSince we assume 〈wt,v1〉 ≥ 12 , we can upper bound this by( 1− 〈wt,v1〉2 )( 1− ηλ\n16\n) + cη2 ( 1− 〈w̃,v1〉2 ) as required.\nPart II: Solving the Recurrence Relation for a Single Epoch\nAs before, since we focus on a single epoch, we drop the subscript from w̃s−1 and denote it simply as w̃. Suppose that η = αλ, where α is a small parameter to be chosen later. Also, let\nbt = 1− 〈wt,v1〉2 and b̃ = 1− 〈w̃,v1〉2.\nThen Lemma 1 tells us that if α is sufficiently small, bt ≤ 34 , and 〈w̃,v1〉 ≥ 0, then\nE [bt+1|wt] ≤ ( 1− α 16 λ2 ) bt + cα 2λ2b̃. (17)\nLemma 2. Let B be the event that bt ≤ 34 for all t = 0, 1, 2, . . . ,m. If α ≤ c, and 〈w̃,v1〉 ≥ 0, then\nE[bm|B,w0] ≤ (( 1− α 16 λ2 )m + cα ) b̃.\nProof. Recall that bt is a deterministic function of the random variable wt, which depends in turn on wt−1 and the random instance chosen at round m. We assume that w0 (and hence b̃) are fixed, and consider how bt evolves as a function of t. Using Eq. (17), we have\nE[bt+1|wt, B] = E [ bt+1|wt, bt+1 ≤ 3\n4\n] ≤ E[bt+1|wt] ≤ ( 1− α 16 λ2 ) bt + cα 2λ2b̃.\nTaking expectation over wt (conditioned on B), we get that E[bt+1|B] ≤ E [( 1− α 16 λ2 ) bt + cα 2λ2b̃ ∣∣∣B]\n= ( 1− α 16 λ2 ) E [bt|B] + cα2λ2b̃.\nUnwinding the recursion, and using that b0 = b̃, we therefore get that\nE[bm|B] ≤ ( 1− α 16 λ2 )m b̃+ cα2λ2b̃ m−1∑ i=0 ( 1− α 16 λ2 )i\n≤ ( 1− α 16 λ2 )m b̃+ cα2λ2b̃ ∞∑ i=0 ( 1− α 16 λ2 )i\n= ( 1− α 16 λ2 )m b̃+ cα2λ2b̃ 1 (α/16)λ2\n= (( 1− α 16 λ2 )m + cα ) b̃.\nas required.\nWe now turn to prove that the event B assumed in Lemma 2 indeed holds with high probability:\nLemma 3. Suppose that α ≤ c, and 〈w̃,v1〉 ≥ 0. Then for any β ∈ (0, 1) and m, if\nb̃+ cmα2λ2 + c √ mα2λ2 log(1/β) ≤ 3\n4 , (18)\nthen it holds with probability at least 1− β that\nbt ≤ b̃+ cmα2λ2 + c √ mα2λ2 log(1/β) ≤ 3\n4\nfor all t = 0, 1, 2, . . . ,m, as well as 〈wm,v1〉 ≥ 0.\nProof. To prove the lemma, we analyze the stochastic process b̃ = b0, b1, b2, . . . , bm, and use a concentration of measure argument. First, we collect the following facts:\n• b̃ = b0 ≤ 34 : This directly follows from the assumption stated in the lemma.\n• The conditional expectation of bt+1 is close to bt, as long as bt ≤ 34 : Supposing that bt ≤ 3 4 for some\nt, and α is sufficiently small. Then by Eq. (17), E [bt+1|wt] ≤ ( 1− α 16 λ2 ) bt + cα 2λ2b̃ = bt − αλ2 ( 1 16 bt − cαb̃ ) ≤ bt + cα2λ2b̃.\n• |bt+1 − bt| is bounded by cαλ: We have |bt+1 − bt| ≤ ∣∣〈wt+1,v1〉2 − 〈wt,v1〉2∣∣ = |〈wt+1,v1〉+ 〈wt,v1〉| ∗ |〈wt+1,v1〉 − 〈wt,v1〉|\n≤ 2 |〈wt+1,v1〉 − 〈wt,v1〉| ≤ 2‖wt+1 −wt‖.\nRecalling the definition of wt+1 in our algorithm, and the fact that the instances x and the matrix A are assumed to have bounded norm, it is easy to verify that ‖wt+1 − wt‖ ≤ cη = cαλ for some appropriate constant c, assuming α is sufficiently small.\nUsing the maximal version of the Hoeffding-Azuma inequality [5], it follows that with probability at least 1− β, it holds simultaneously for all t = 1, . . . ,m (and for t = 0 by assumption) that\nbt ≤ b̃+mcα2λ2b̃+ c √ mα2λ2 log(1/β)\nfor some constant c, as long as the expression above is less than 34 . If the expression is indeed less than 3 4 , then we get that bt ≤ 34 for all t. Upper bounding b̃ by 1 and λ by a constant, and slightly simplifying, we get the statement in the lemma.\nIt remains to prove that if bt ≤ 34 for all t, then 〈wm,v1〉 ≥ 0. Suppose on the contrary that 〈wm,v1〉 < 0. Since |〈wt+1,v1〉 − 〈wt,v1〉| ≤ ‖wt+1 −wt‖ ≤ cαλ as we’ve seen earlier, and 〈w0,v1〉 ≥ 0, it means there must have been some wt such that 〈wt,v1〉 ≤ cαλ. But this means that bt = (1 − 〈wt,v1〉2) ≥ 1− c2α2λ2 > 34 (as long as α is sufficiently small, since we assume λ is bounded), invalidating the assumption that bt ≤ 34 for all t. Therefore, 〈wm,v1〉 ≥ 0 as required.\nCombining Lemma 2 and Lemma 3, and using Markov’s inequality, we get the following corollary:\nLemma 4. Let confidence parameters β, γ ∈ (0, 1) be fixed. Suppose that 〈w̃,v1〉 ≥ 0, and that m,α are chosen such that\nb̃+ cmα2λ2 + c √ mα2λ2 log(1/β) ≤ 3\n4 .\nThen with probability at least 1− (β + γ), it holds that 〈wm,v1〉 ≥ 0, and\nbm ≤ 1\nγ\n(( 1− α 16 λ2 )m + cα ) b̃.\nPart III: Analyzing the Entire Algorithm’s Run\nGiven the analysis in Lemma 4 for a single epoch, we are now ready to prove our theorem. Let\nb̃s = 1− 〈w̃s,v1〉2.\nBy assumption, at the beginning of the first epoch, we have b̃0 = 1 − 〈w̃0,v1〉2 ≤ 1 − 12 = 1 2 . Therefore, by Lemma 4, for any β, γ ∈ ( 0, 12 ) , if we pick\nα ≤ cγ2 and m ≥ c log(1/γ) αλ2 such that 1 2 + cmα2λ2 + c\n√ mα2λ2 log(1/β) ≤ 3\n4 (19)\n(where the constant c in α ≤ cγ2 is sufficiently small, and the constant c in m ≥ c log(1/γ) αλ2\nis sufficiently large), then we get with probability at least 1− (β + γ) that\nb̃1 ≤ 1\nγ (1− αλ2 16 ) c log(1/γ) αλ2 + 1 2 γ2  b̃0 = 1 γ ( exp ( −3 log ( 1 γ )) + 1 2 γ2 ) b̃0\n= 1\nγ\n( γ3 + 1 2 γ2 ) b̃0 ≤ γb̃0,\nas well as 〈w̃1,v1〉 ≥ 0. Since b̃1 is only smaller than b̃0, the conditions of Lemma 4 are fulfilled for b̃ = b̃1, so again with probability at least 1− (β + γ), by the same calculation, we have\nb̃2 ≤ γb̃1 ≤ γ2b̃0.\nRepeatedly applying Lemma 4 and using a union bound, we get that after T epochs, with probability at least 1− T (β + γ),\n1− 〈w̃T ,v1〉2 = b̃T ≤ γT b̃0 < γT . Therefore, for any desired accuracy parameter , we simply need to use T = ⌈ log(1/ ) log(1/γ) ⌉ epochs, and get\n1− 〈w̃T ,v1〉2 ≤ with probability at least 1− T (β + γ) = 1− ⌈ log(1/ ) log(1/γ) ⌉ (β + γ).\nTo get the theorem statement, using a confidence parameter δ, we pick β = γ = δ2 , which ensures that the accuracy bound above holds with probability at least\n1− ⌈ log(1/ )\nlog(2/δ)\n⌉ δ ≥ 1− log(1/ )\nlog(2/δ) δ ≥ 1− 2 log\n( 1 ) δ.\nSubstituting this choice of β, γ into Eq. (19), and recalling that the step size η equals αλ, the theorem follows."
    } ],
    "references" : [ {
      "title" : "Stochastic optimization for PCA and PLS",
      "author" : [ "R. Arora", "A. Cotter", "K. Livescu", "N. Srebro" ],
      "venue" : "In 2012 50th Annual Allerton Conference on Communication, Control, and Computing,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2012
    }, {
      "title" : "Stochastic optimization of pca with capped msg",
      "author" : [ "R. Arora", "A. Cotter", "N. Srebro" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2013
    }, {
      "title" : "The fast convergence of incremental pca",
      "author" : [ "A. Balsubramani", "S. Dasgupta", "Y. Freund" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2013
    }, {
      "title" : "Matrix computations (4",
      "author" : [ "G. Golub", "C. van Loan" ],
      "venue" : null,
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2013
    }, {
      "title" : "Probability inequalities for sums of bounded random variables",
      "author" : [ "W. Hoeffding" ],
      "venue" : "Journal of the American statistical association,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 1963
    }, {
      "title" : "Accelerating stochastic gradient descent using predictive variance reduction",
      "author" : [ "R. Johnson", "T. Zhang" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2013
    }, {
      "title" : "Semi-stochastic gradient descent methods",
      "author" : [ "J. Konecný", "P. Richtárik" ],
      "venue" : "CoRR, abs/1312.1666,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2013
    }, {
      "title" : "The method of stochastic approximation for the determination of the least eigenvalue of a symmetrical matrix",
      "author" : [ "T.P. Krasulina" ],
      "venue" : "USSR Computational Mathematics and Mathematical Physics,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 1969
    }, {
      "title" : "Mixed optimization for smooth functions",
      "author" : [ "M. Mahdavi", "L. Zhang", "R. Jin" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2013
    }, {
      "title" : "Simplified neuron model as a principal component analyzer",
      "author" : [ "E. Oja" ],
      "venue" : "Journal of mathematical biology,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 1982
    }, {
      "title" : "On stochastic approximation of the eigenvectors and eigenvalues of the expectation of a random matrix",
      "author" : [ "E. Oja", "J. Karhunen" ],
      "venue" : "Journal of mathematical analysis and applications,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 1985
    } ],
    "referenceMentions" : [ {
      "referenceID" : 3,
      "context" : "One possible approach is using iterative methods such as power iteration or the Lanczos method [4].",
      "startOffset" : 95,
      "endOffset" : 98
    }, {
      "referenceID" : 7,
      "context" : "[8, 11, 12] and more recently, [1, 10, 2]), which utilize the structure of the covariance method.",
      "startOffset" : 0,
      "endOffset" : 11
    }, {
      "referenceID" : 9,
      "context" : "[8, 11, 12] and more recently, [1, 10, 2]), which utilize the structure of the covariance method.",
      "startOffset" : 0,
      "endOffset" : 11
    }, {
      "referenceID" : 10,
      "context" : "[8, 11, 12] and more recently, [1, 10, 2]), which utilize the structure of the covariance method.",
      "startOffset" : 0,
      "endOffset" : 11
    }, {
      "referenceID" : 0,
      "context" : "[8, 11, 12] and more recently, [1, 10, 2]), which utilize the structure of the covariance method.",
      "startOffset" : 31,
      "endOffset" : 41
    }, {
      "referenceID" : 1,
      "context" : "[8, 11, 12] and more recently, [1, 10, 2]), which utilize the structure of the covariance method.",
      "startOffset" : 31,
      "endOffset" : 41
    }, {
      "referenceID" : 5,
      "context" : "VR-PCA builds on a recently-introduced technique for stochastic gradient variance reduction, which has been previously studied (see [6] as well as [9, 7]).",
      "startOffset" : 132,
      "endOffset" : 135
    }, {
      "referenceID" : 8,
      "context" : "VR-PCA builds on a recently-introduced technique for stochastic gradient variance reduction, which has been previously studied (see [6] as well as [9, 7]).",
      "startOffset" : 147,
      "endOffset" : 153
    }, {
      "referenceID" : 6,
      "context" : "VR-PCA builds on a recently-introduced technique for stochastic gradient variance reduction, which has been previously studied (see [6] as well as [9, 7]).",
      "startOffset" : 147,
      "endOffset" : 153
    }, {
      "referenceID" : 9,
      "context" : "To understand the structure of the algorithm, it is helpful to consider first the well-known Oja’s algorithm for stochastic PCA optimization [11], on which our algorithm is based.",
      "startOffset" : 141,
      "endOffset" : 145
    }, {
      "referenceID" : 2,
      "context" : "Recently, [3] gave a rigorous finite-time analysis of this algorithm, showing that if ηt = O(1/t), then under suitable conditions, we get a convergence rate of O(1/T ).",
      "startOffset" : 10,
      "endOffset" : 13
    }, {
      "referenceID" : 5,
      "context" : "Inspired by recent variance-reduced stochastic methods for convex optimization [6], we change the algorithm in a way which encourages the variance of the stochastic term to decay over time.",
      "startOffset" : 79,
      "endOffset" : 82
    }, {
      "referenceID" : 2,
      "context" : "[3]) to get to this constant accuracy, from which point our algorithm and analysis takes over.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 5,
      "context" : "where λ is the strong convexity parameter of the problem [6].",
      "startOffset" : 57,
      "endOffset" : 60
    }, {
      "referenceID" : 4,
      "context" : "Using the maximal version of the Hoeffding-Azuma inequality [5], it follows that with probability at least 1− β, it holds simultaneously for all t = 1, .",
      "startOffset" : 60,
      "endOffset" : 63
    } ],
    "year" : 2017,
    "abstractText" : "We describe and analyze a simple algorithm for principal component analysis, VR-PCA, which uses computationally cheap stochastic iterations, yet converges exponentially fast to the optimal solution. In contrast, existing algorithms suffer either from slow convergence, or computationally intensive iterations whose runtime scales with the data size. The algorithm builds on a recent variance-reduced stochastic gradient technique, which was previously analyzed for strongly convex optimization, whereas here we apply it to the non-convex PCA problem, using a very different analysis.",
    "creator" : "LaTeX with hyperref package"
  }
}