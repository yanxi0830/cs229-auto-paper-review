{
  "name" : "1610.01476.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "`1 Regularized Gradient Temporal-Difference Learning",
    "authors" : [ "Dominik Meyer", "Hao Shen", "Klaus Diepold", "Meyer Shen Diepold" ],
    "emails" : [ "dominik.meyer@tum.de", "hao.shen@tum.de", "kldi@tum.de" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Keywords: Reinforcement Learning (RL), linear function approximation, Gradient TemporalDifference (GTD) learning, Iterative Soft Thresholding (IST)."
    }, {
      "heading" : "1. Introduction",
      "text" : "One fundamental problem in Reinforcement Learning (RL) is to learn the long-term expected reward, i.e. the value function, which can consequently be used for determining a good control policy, cf. Sutton and Barto (1998). In the general setting with large or infinite state space, exact representation of the actual value function is often inhibitively computationally expensive or hardly possible. To overcome this difficulty, function approximation techniques are employed for estimating the value function from sampled trajectories. The quality of the learned policy depends significantly on the chosen function approximation technique.\nIn this paper, we consider the technique of linear value function approximation. The value function is represented or approximated as a linear combination of a set of features, or basis functions. These features are generated from the sampled states via either some heuristic constructions, e.g. Bradtke and Barto (1996); Keller et al. (2006), or kernel-based approaches, e.g. Taylor and Parr (2009). A common approach generates firstly a vast number of features, which is often much larger than the number of available samples, and then chooses automatically relevant features to approximate the actual value function. Unfortunately, such approaches may fail completely due to overfitting. To cope with this situation, regularization techniques are necessarily to be employed. Other than the simple `2 regu-\nar X\niv :1\n61 0.\n01 47\n6v 1\n[ cs\n.A I]\n5 O\nct 2\n01 6\nMeyer Shen Diepold\nlarization, which penalizes the smoothness of the learned value function, e.g. Farahmand et al. (2008), in this work we focus on `1 regularization. The `1 regularization often produces sparse solutions, thus can serve as a method of automatic feature selection for linear value function approximation.\nThis work focuses on the development of Temporal Difference (TD) learning algorithms, cf. Bradtke and Barto (1996). Recent active researches on applying `1 regularization to TD learning have led to a various number of effective algorithms, e.g. Loth et al. (2007); Kolter and Ng (2009); Johns et al. (2010); Geist and Scherrer (2012); Hoffman et al. (2012). It is important to notice that `1 minimization has been extensively studied in the areas of compressed sensing and image processing, and many efficient `1 minimization algorithms have been developed, cf. Candés and Romberg (2007); Zibulevsky and Elad (2010). Very recently, two advanced `1 minimization algorithms have been adapted to the TD learning, i.e. the Dantzig selector based TD algorithm from Geist et al. (2012) and the orthogonal matching pursuit based TD algorithm developed in Painter-Wakefield and Parr (2012a).\nOn the other hand, most TD learning algorithms are known to be unstable with linear value function approximation and off-policy learning. By observing the fact that most original forms of TD algorithms are not true gradient descent methods, a new class of intrinsic gradient TD (GTD) learning algorithms with linear value function approximation are developed and proven to be stable, cf. Sutton et al. (2008, 2009). However, it is important to know that success of GTD algorithms might be limited due to the fact that the GTD family requires a set of well chosen features. In other words, the GTD algorithms are in potential danger of overfitting. The key contribution of the present work is the development of a family of `1 regularized GTD algorithms, referred to as GTD-IST algorithms. Convergence properties of the proposed algorithms are investigated from the perspective of stochastic optimization.\nThe paper is outlined as follows. In Section 2, we briefly introduce a general setting of TD learning and provide some preliminaries of TD objective functions. Section 3 presents a framework of `1 regularized GTD learning algorithms, and investigates their convergence properties. In Section 4, several numerical experiments depict the practical performance of the proposed algorithms, compared with several existing `1 regularized TD algorithms. Finally, a conclusion is drawn in Section 5."
    }, {
      "heading" : "2. Notations and Preliminaries",
      "text" : "In this work, we consider a RL process as a Markov Decision Process (MDP), defined as a tuple (S,A, P, r, γ), where S is a set of possible states of the environment, A is a set of actions of the agent, P : S×A×S → [0, 1] the conditional transition probabilities P (s, a, s′) over state transitions from state s to state s′ given an action a, r : S → R is a reward function assigning immediate reward r to a state s, and γ ∈ [0, 1] is a discount factor."
    }, {
      "heading" : "2.1. TD Learning with Linear Function Approximation",
      "text" : "The goal of a RL agent is to learn a mapping from states to actions, i.e. a policy π : S → A, which maximizes the value function V π : S → R of a state s taking a policy π, defined as\nV π(s) := E [∑∞\nt=0γ tr(st)|s0 = s, π\n] . (1)\n`1 Regularized GTD Learning\nIt is well known that, for a given policy π, the value function V π fulfills the Bellman equation, i.e.\nV π(s) = r(s) + γ ∑ s′ P (s, π(s), s′)V π(s′). (2)\nThe right hand side of (2) is often referred to as the Bellman operator for policy π, denoted by T V π(s). In other words, the value function V π(s) is the fixed point of the Bellman operator T V π(s), i.e. V π(s) = T V π(s).\nWhen the state space is too large or infinite, exact representation of the value function is often practically unfeasible. Function approximation is thus of great demand for estimating the actual value function. A popular approach is to construct a set of features by the map φ : S → Rk, which are called the features or basis functions, and then to approximate the value function by a linear function. Concretely, for a given state s, the value function is approximated by\nV (s) ≈ (φ(s))>θ =: Vθ, (3)\nwhere θ ∈ Rk is a parameter vector. In the setting of TD learning, the parameter θ is updated at each time step t, i.e. for each state transition and the associated reward (st, rt, s ′ t). Here, we consider the simple one-step TD learning with linear function approximation, i.e. λ = 0 in the framework of TD(λ) learning. The parameter θ is updated as follows\nθt+1 = θt + αtδtφt, (4)\nwhere αt > 0 is a sequence of step-size parameters, and δt is the simple TD error\nδt = rt + θ > t ( γφ′t − φt ) . (5)\nNote, that the TD error δt can be considered as a function of the parameter θt. By abuse of notation, in the rest of the paper we also denote δθ = δ(θ) := r + θ > (γφ′ − φ)."
    }, {
      "heading" : "2.2. Three Objective Functions for TD Learning",
      "text" : "In order to find an optimal parameter θ∗ via an optimization process, one has to define an appropriate objective function, which accurately measures the correctness of the current value function approximation, i.e. how far the current approximation is away from the actual TD solution. In this subsection, we recall three popular objective functions for TD learning.\nMotivated by the fact that the value function is the fixed point of the Bellman operator for a given policy, correctness of an approximation Vθ can be simply measured by the TD error itself, i.e.\nJ1 : Rk → R, J1(θ) := 12 ‖Vθ − T Vθ‖ 2 D = 1 2 (E[δθ]) 2 , (6)\nwhere D ∈ R|S|×|S| is a diagonal matrix, whose components are some state distribution. This cost function is often referred to as the Mean Squared Bellman Error (MSBE). Ideally, the minimum of the MSBE function admits a good value function approximation. Unfortunately, it is well known that, in practice, the performance of an approximation Vθ depends\nMeyer Shen Diepold\non the pre-selected feature space H := { Φθ|θ ∈ Rk }\n, i.e. the span of the features Φ := φ(S). By introducing the projector as\nΠ = Φ ( Φ>DΦ )−1 Φ>D, (7)\nthe so-called Mean Squared Projected Bellman Error (MSPBE) is ofter preferred\nJ2 : Rk → R, J2(θ) := 12 ‖Vθ −ΠT Vθ‖ 2 D\n= 12E[δθφ] >E[φφ>]−1E[δθφ].\n(8)\nMinimizing the MSPBE function finds a fixed point of the projected Bellman operator in the feature space H, i.e. Vθ = ΠT Vθ.\nFinally, we present a less popular objective function for TD learning. Recall the TD parameter update as defined in (4). The vector E[δθφ] ∈ Rk in the second summand can be considered as an error for a given θ. It is expected to be equal to zero at the TD solution. Hence, one can use the `2 norm of this vector, defined as\nJ3 : Rk → R, J3(θ) = 12E[δθφ] >E[δθφ], (9)\nas an objective function for TD learning. The function J3 is referred to as the Norm of Expected TD Update (NEU), which is used to derive the original GTD algorithm in Sutton et al. (2008)."
    }, {
      "heading" : "3. Stochastic Gradient Algorithms for `1 Regularized TD Learning",
      "text" : "In the first part of this section, we present a general framework of gradient algorithms for minimizing the `1 regularized TD objective functions. The second subsection develops two `1 regularized stochastic gradient TD algorithms in the online setting, and investigates their convergence properties from the perspective of stochastic optimization."
    }, {
      "heading" : "3.1. `1 Regularized TD Learning",
      "text" : "Applying an `1 regularizer to the parameter θ leads to the following objective function\nFi(θ) := Ji(θ) + η‖θ‖1, (10)\nwhere i ∈ {1, 2, 3} and ‖θ‖1 = ∑\ni |θi| denotes the `1 norm of a vector θ = [θ1, . . . , θk]> ∈ Rk. Here, the scalar η > 0 weighs the regularization term ‖θ‖1, and balances the sparsity of θ against the TD objective function Ji. The iterative soft thresholding (IST) algorithm is nowadays one classic algorithm for minimizing the cost function (10). It can be interpreted as an extension of the classical gradient algorithm. Due to its high popularity, we skip the derivation of the IST algorithm, and refer to Zibulevsky and Elad (2010) and the references therein for further reading.\nGiven x ∈ Rm and ν > 0, the soft thresholding operator applied to x is defined as\nΨν(x) := sgn(x) max{|x| − ν, 0}\n= { x− sgn(x)ν, if |x| > ν, 0, otherwise,\n(11)\n`1 Regularized GTD Learning\nwhere sgn(·) and max(·) are entry-wise, and is the entry-wise multiplication. Then, minimization of the objective function (10) can be achieved via applying the soft thresholding operator iteratively. Straightforwardly, we define the IST based TD update as follows\nθt+1 = Ψαtη (θt − αt∇Ji(θt)) , (12)\nwhere αt > 0, and ∇Ji(θt) denotes the gradient update of Ji(θt). Specifically, the gradient updates of the three objective functions are given as ∇J1(θt) = E [ δt ] E [ (γφ′t − φt) ] , ∇J2(θt) = E [ (γφ′t − φt)φ>t ] ( E [ φtφ > t ])−1 E[δtφt], ∇J3(θt) = E [ (γφ′t − φt)φ>t ] E [ δtφt ] . (13)\nWe refer to this family of algorithms as TD-IST algorithms. Note that IST has been employed in developing fixed point TD algorithms in Painter-Wakefield and Parr (2012b), whereas in this work we focus on developing intrinsic gradient TD algorithm."
    }, {
      "heading" : "3.2. Stochastic GTD-IST Algorithms",
      "text" : "The TD-IST algorithms presented in the previous subsection are only applicable in the batch setting. In some real applications, it is certainly favorable to have them working online. Stochastic gradient descent algorithms can be developed straightforwardly to minimize the `1 regularized TD objective functions.\nNow let us consider the online setting, i.e. given a sequence of data samples φ1, φ2, . . .. In the form of stochastic gradient descent, we propose a general form of parameter update as\nθt+1 = Ψαtη ( θt − αt∇̃Ji(θt) ) , (14)\nwhere ∇̃Ji(θt) denotes the stochastic gradient updates of Ji(θt), or their appropriate stochastic approximations, cf. Sutton et al. (2008, 2009). To investigate convergence properties of the proposed algorithms requires results from Duchi and Singer (2009), which develops a general framework for analyzing empirical loss minimization with regularizations. We adapt the result in corollary 10 from Duchi and Singer (2009) to our current setting as follows.\nTheorem 1 Let the function J : Rk → R be smooth and strictly convex and θ∗ ∈ Rk be the global minimum of the function F (θ) := J(θ) + η‖θ‖1 with η > 0. If the following three conditions hold: (1) θ∗ fulfills ‖θt − θ∗‖2 ≤ d for some constant d > 0; (2) ‖∇J(θt)‖2 ≤ g for some constant g > 0; and (3) a stochastic estimate of the gradient ∇̃J(θt) fulfills E[∇̃J(θt)] = ∇J(θt), then IST based stochastic algorithms converge with probability one to θ∗.\nLet us look at the `1 regularized NEU function F3 first. Recall the approximate stochastic gradient update, developed in Sutton et al. (2008), as\n∇̃J3(θt) = (φ>t ut)(γφ′t − φt), (15)\nwith ut+1 = ut + βt(δtφt − ut), (16)\nMeyer Shen Diepold\nwhere βt > 0 is a step size parameter. We refer to the corresponding algorithm as the GTD-IST algorithm. Convergence properties of the GTD-IST algorithm are characterized in the following corollary.\nCorollary 2 If (φt, rt, φ ′ t) is an i.i.d sequence with uniformly bounded second moments, and the matrix E[φ(γφ′ − φ)>] ∈ Rk×k is invertible, then the GTD-IST algorithm, whose update is specified in (15), converges with probability one to the TD solution.\nProof Recall the TD error δθ = r + θ >(γφ′ − φ). The `1 regularized NEU cost function F3 can be written as\nF3(θ) = E[δθφ]>E[δθφ] + η‖θ‖1 = E [ rφ+ θ>(γφ′ − φ)φ ]>E[rφ+ θ>(γφ′ − φ)φ]+ η‖θ‖1. (17) It is easily seen that the regularized function F3 is strictly convex if the matrix E[φ(γφ′ − φ)>] is invertible. The TD solution is then the global minimum of F3. The condition of (φt, rt, φ ′ t) being an i.i.d sequence with uniformly bounded second moments ensures that ‖∇Ji(θt)‖2 ≤ g holds true for some constant g > 0. Finally, applying the fact that the stochastic approximation ut is a quasi-stationary estimate of the term E[δφ], cf. Sutton et al. (2008), we have\nE [ ∇̃J3(θt) ] =E [ (γφ′t − φt)φ>t ut ] =E [ (γφ′t − φt)φ>t ] E [ δtφt\n] =∇J3(θt).\n(18)\nThen the result follows from Theorem 1.\nIn order to minimize the MSPBE function J2, two efficient GTD algorithms are developed in Sutton et al. (2009). Their approximate stochastic updates are defined as\n∇̃J (1)2 (θt) = (φ > t wt)(γφ ′ t − φt), (19a) ∇̃J (2)2 (θt) = γ(φ > t wt)φ ′ t − δtφt, (19b)\nwhere wt+1 = wt + βt(δt − φ>t wt)φt. (20)\nWe refer to the corresponding `1 regularized GTD algorithms, which employ the updates (19a) and (19b), as GTD2-IST and TDC-IST algorithms, respectively. With no surprises, they share similar convergence properties as the GTD-IST algorithm.\nCorollary 3 If (φt, rt, φ ′ t) is an i.i.d sequence with uniformly bounded second moments, and both E[φ(φ− γφ′)>] and E[φφ>] are invertible, then both the GTD2-IST and the TDC-IST algorithms, whose updates are specified in (19), converge with probability one to the TD solution.\n`1 Regularized GTD Learning\nProof The `1 regularized MSPBE cost function F2 can be written as\nF2(θ) = E[δθφ]>E[φφ>]−1E[δθφ] + η‖θ‖1 = E [ rφ+ θ>(γφ′ − φ)φ ]>E[φφ>]−1E[rφ+ θ>(γφ′ − φ)φ]+ η‖θ‖1. (21) The function F2 is strictly convex if the matrix\nE [ φ(γφ′ − φ)> ] E[φφ>]−1E [ (γφ′ − φ)φ> ] (22)\nis positive definite, i.e. both E[φ(φ− γφ′)>] and E[φφ>] are invertible. By the fact that the stochastic approximation wt is a quasi-stationary estimate of the term E[φφ>]−1E [ (γφ′ −\nφ)φ> ] , cf. Sutton et al. (2009), we get\nE [ ∇̃J (1)2 (θt) ] = E [ ∇̃J (2)2 (θt) ] = ∇J2(θt). (23)\nThen, the result follows straightforwardly from the same arguments as in Corollary 2.\nMeyer Shen Diepold"
    }, {
      "heading" : "4. Numerical Experiments",
      "text" : "In this section, we investigate the performance of our proposed `1 regularized GTD algorithms, compared with two existing `1 regularized TD algorithms, in both the on-policy and off-policy settings."
    }, {
      "heading" : "4.1. Experiment One: On-Policy Learning",
      "text" : "In this experiment, we apply our proposed algorithms to a random walk problem in the chain environment consisting of seven states. There exists only one action and the transition probability of going right or left is equal. A reward of one is only assigned in the rightmost state, which is the terminal state, whereas the rewards are zero everywhere else. The features consist of a binary encoding of the states and ten additional “noisy” features, which are simply Gaussian noise. In this setting, we run three different experiments."
    }, {
      "heading" : "4.1.1. Regularized vs. Un-regularized",
      "text" : "This experiment compares the performance of the proposed `1 regularized GTD algorithms with their un-regularized counterparts. Figure 1 shows the learning curves of three GTD learning algorithms, namely, GTD, GTD2, and TDC, together with their regularized versions. It is evident that IST based GTD algorithms outperform all their original unregularized versions respectively. The experimental results demonstrate the effectiveness of IST based GTD learning algorithms."
    }, {
      "heading" : "4.1.2. Unfavorable Initializations",
      "text" : "The second experiment investigates the recovery behavior and convergence speed of our proposed algorithms with unfavorable initializations. Here, we only consider the simple GTD-IST algorithm. The parameter vector θ is initialized to have ones for all the noisy features and zeros for all the “good” features. In other words, our experiment starts with the initialization of selecting all the “bad” features. The results in Figure 2 show that the\n`1 Regularized GTD Learning\n`1 regularized GTD algorithms, i.e. the GTD-IST algorithm with different parameter value η, converge faster to the correct selection of features than the original GTD algorithm."
    }, {
      "heading" : "4.1.3. GTD-IST Algorithms vs. Others",
      "text" : "In the third experiment, we compare the GTD-IST algorithms with the L1TD algorithm from Painter-Wakefield and Parr (2012b) and the LARS-TD algorithm from Kolter and Ng (2009). Results in both Figure 3(a) and 3(b) imply that, with or without noise, all three GTD-IST algorithms outperforms the L1TD algorithm consistently. A closer look at the result in the zoomed-in window in Figure 3(c) shows that the LARS-TD algorithm performs the best with the presence of noise. This might be due to the fact that the LARSTD algorithm updates, after every 20 episodes, using all the samples available. Nevertheless, without any surprise, a timing experiment shows in Table 1 that the LARS-TD algorithm performs much slower than the other online algorithms.\nMeyer Shen Diepold"
    }, {
      "heading" : "4.2. Experiment Two: Off-Policy Learning",
      "text" : "To test the performance of the GTD-IST algorithms on the off-policy learning, we employ the well-known star example, proposed in Baird (1995). It consists of seven states with one state being considered as the “center”. In each of the outer states, the agent can choose between two actions: either the “solid” action, which takes it to the center state with probability one, or the “dotted” action, which takes it to any of the other states with equal probability. Reward on all state transitions is equal to zero and the states are represented by tabular features as described in the original setting. We add 20 noisy “Gaussian” features to the state representation. The behavior policy chooses the “solid” action with the probability 1/7 and the “dotted” otherwise, while the estimation policy chooses always the “dotted” action. The learning curves in Figure 4 shows that both GTDIST and GTD2-IST algorithms outperform their original counterparts consistently."
    }, {
      "heading" : "5. Conclusions",
      "text" : "This work combines the recently developed GTD methods with `1 regularization, and proposes a family of GTD-IST algorithms. We investigate the convergence properties of the proposed algorithms from the perspective of stochastic optimization. Preliminary experiments demonstrate that the proposed family of GTD-IST algorithms outperform all their original counterparts and two existing `1 regularized TD algorithms. Being aware of advanced developments in the community of sparse representation, we project to employ further state-of-the-art algorithms of sparse representation to RL. For example, the IST algorithms are usually known to be slow compared to other advanced `1 minimization algorithms. Applying more efficient `1 minimization algorithms, such as Beck and Teboulle (2009), to TD learning are of great interests as the future work.\n`1 Regularized GTD Learning"
    }, {
      "heading" : "Acknowledgements",
      "text" : "This work has been partially supported by the International Graduate School of Science and Engineering (IGSSE), Technische Universität München, Germany. The authors would like to thank Christopher Painter-Wakefield for providing us with the Matlab implementation of the L1TD algorithm."
    } ],
    "references" : [ {
      "title" : "Residual algorithms: Reinforcement learning with function approximation",
      "author" : [ "L. Baird" ],
      "venue" : "In Proceeding of the 12th International Conference on Machine Learning,",
      "citeRegEx" : "Baird.,? \\Q1995\\E",
      "shortCiteRegEx" : "Baird.",
      "year" : 1995
    }, {
      "title" : "A fast iterative shrinkage-thresholding algorithm for linear inverse problems",
      "author" : [ "A. Beck", "M. Teboulle" ],
      "venue" : "SIAM Journal on Imaging Sciences,",
      "citeRegEx" : "Beck and Teboulle.,? \\Q2009\\E",
      "shortCiteRegEx" : "Beck and Teboulle.",
      "year" : 2009
    }, {
      "title" : "Linear least-squares algorithms for temporal difference learning",
      "author" : [ "S.J. Bradtke", "A.G. Barto" ],
      "venue" : "Maching Learning,",
      "citeRegEx" : "Bradtke and Barto.,? \\Q1996\\E",
      "shortCiteRegEx" : "Bradtke and Barto.",
      "year" : 1996
    }, {
      "title" : "Sparsity and incoherence in compressive sampling",
      "author" : [ "E.J. Candés", "J. Romberg" ],
      "venue" : "Inverse Problems,",
      "citeRegEx" : "Candés and Romberg.,? \\Q2007\\E",
      "shortCiteRegEx" : "Candés and Romberg.",
      "year" : 2007
    }, {
      "title" : "Efficient online and batch learning using forward backward splitting",
      "author" : [ "J. Duchi", "Y. Singer" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Duchi and Singer.,? \\Q2009\\E",
      "shortCiteRegEx" : "Duchi and Singer.",
      "year" : 2009
    }, {
      "title" : "Regularized policy iteration",
      "author" : [ "A.M. Farahmand", "M. Ghavamzadeh", "C. Szepesvári", "S. Mannor" ],
      "venue" : "Advances in Neural Information Processing Systems 21,",
      "citeRegEx" : "Farahmand et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Farahmand et al\\.",
      "year" : 2008
    }, {
      "title" : "`1-penalized projected bellman residual",
      "author" : [ "M. Geist", "B. Scherrer" ],
      "venue" : "Recent Advances in Reinforcement Learning,",
      "citeRegEx" : "Geist and Scherrer.,? \\Q2012\\E",
      "shortCiteRegEx" : "Geist and Scherrer.",
      "year" : 2012
    }, {
      "title" : "A Dantzig selector approach to temporal difference learning",
      "author" : [ "M. Geist", "B. Scherrer", "A. Lazaric", "M. Ghavamzadeh" ],
      "venue" : "In Proceedings of the 29th International Conference on Machine Learning,",
      "citeRegEx" : "Geist et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Geist et al\\.",
      "year" : 2012
    }, {
      "title" : "Regularized least squares temporal difference learning with nested `2 and `1 penalization",
      "author" : [ "M.W. Hoffman", "A. Lazaric", "M. Ghavamzadeh", "R. Munos" ],
      "venue" : "Recent Advances in Reinforcement Learning,",
      "citeRegEx" : "Hoffman et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Hoffman et al\\.",
      "year" : 2012
    }, {
      "title" : "Linear complementarity for regularized policy evaluation and improvement",
      "author" : [ "J. Johns", "C. Painter-Wakefield", "R. Parr" ],
      "venue" : "Advances in Neural Information Processing Systems",
      "citeRegEx" : "Johns et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Johns et al\\.",
      "year" : 2010
    }, {
      "title" : "Automatic basis function construction for approximate dynamic programming and reinforcement learning",
      "author" : [ "P.W. Keller", "S. Mannor", "D. Precup" ],
      "venue" : "In Proceedings of the 23rd International Conference on Machine Learning",
      "citeRegEx" : "Keller et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Keller et al\\.",
      "year" : 2006
    }, {
      "title" : "Regularization and feature selection in least-squares temporal difference learning",
      "author" : [ "J.Z. Kolter", "A.Y. Ng" ],
      "venue" : "In Proceedings of the 26th International Conference on Machine Learning (ICML",
      "citeRegEx" : "Kolter and Ng.,? \\Q2009\\E",
      "shortCiteRegEx" : "Kolter and Ng.",
      "year" : 2009
    }, {
      "title" : "Sparse temporal difference learning using lasso",
      "author" : [ "M. Loth", "M. Davy", "P. Preux" ],
      "venue" : "In Proceedings of the 2007 IEEE Symposium on Approximate Dynamic Programming and Reinforcement Learning,",
      "citeRegEx" : "Loth et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Loth et al\\.",
      "year" : 2007
    }, {
      "title" : "Greedy algorithms for sparse reinforcement learning",
      "author" : [ "C. Painter-Wakefield", "R. Parr" ],
      "venue" : "In Proceedings of the 29th International Conference on Machine Learning,",
      "citeRegEx" : "Painter.Wakefield and Parr.,? \\Q2012\\E",
      "shortCiteRegEx" : "Painter.Wakefield and Parr.",
      "year" : 2012
    }, {
      "title" : "L1 regularized linear temporal difference learning",
      "author" : [ "C. Painter-Wakefield", "R. Parr" ],
      "venue" : "Technical report,",
      "citeRegEx" : "Painter.Wakefield and Parr.,? \\Q2012\\E",
      "shortCiteRegEx" : "Painter.Wakefield and Parr.",
      "year" : 2012
    }, {
      "title" : "Reinforcement Learning: An Introduction",
      "author" : [ "R.S. Sutton", "A.G. Barto" ],
      "venue" : null,
      "citeRegEx" : "Sutton and Barto.,? \\Q1998\\E",
      "shortCiteRegEx" : "Sutton and Barto.",
      "year" : 1998
    }, {
      "title" : "A convergent O(n) algorithm for offpolicy temporal-difference learning with linear function approximations",
      "author" : [ "R.S. Sutton", "Csaba Szepesvári", "H.R. Maei" ],
      "venue" : "Advances in Neural Information Processing Systems 21,",
      "citeRegEx" : "Sutton et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Sutton et al\\.",
      "year" : 2008
    }, {
      "title" : "Fast gradient-descent methods for temporal-difference learning with linear function approximation",
      "author" : [ "R.S. Sutton", "H.R. Maei", "D. Precup", "S. Bhatnagar", "D. Silver", "C. Szepesvári", "E. Wiewiora" ],
      "venue" : "In Proceedings of the 26th International Conference on Machine Learning",
      "citeRegEx" : "Sutton et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Sutton et al\\.",
      "year" : 2009
    }, {
      "title" : "Kernelized value function approximation for reinforcement learning",
      "author" : [ "G. Taylor", "R. Parr" ],
      "venue" : "In Proceedings of the 26th International Conference on Machine Learning",
      "citeRegEx" : "Taylor and Parr.,? \\Q2009\\E",
      "shortCiteRegEx" : "Taylor and Parr.",
      "year" : 2009
    }, {
      "title" : "L1-L2 optimization in signal and image processing",
      "author" : [ "M. Zibulevsky", "M. Elad" ],
      "venue" : "IEEE Signal Processing Magazine,",
      "citeRegEx" : "Zibulevsky and Elad.,? \\Q2010\\E",
      "shortCiteRegEx" : "Zibulevsky and Elad.",
      "year" : 2010
    } ],
    "referenceMentions" : [ {
      "referenceID" : 13,
      "context" : "Sutton and Barto (1998). In the general setting with large or infinite state space, exact representation of the actual value function is often inhibitively computationally expensive or hardly possible.",
      "startOffset" : 0,
      "endOffset" : 24
    }, {
      "referenceID" : 2,
      "context" : "Bradtke and Barto (1996); Keller et al.",
      "startOffset" : 0,
      "endOffset" : 25
    }, {
      "referenceID" : 2,
      "context" : "Bradtke and Barto (1996); Keller et al. (2006), or kernel-based approaches, e.",
      "startOffset" : 0,
      "endOffset" : 47
    }, {
      "referenceID" : 2,
      "context" : "Bradtke and Barto (1996); Keller et al. (2006), or kernel-based approaches, e.g. Taylor and Parr (2009). A common approach generates firstly a vast number of features, which is often much larger than the number of available samples, and then chooses automatically relevant features to approximate the actual value function.",
      "startOffset" : 0,
      "endOffset" : 104
    }, {
      "referenceID" : 3,
      "context" : "Farahmand et al. (2008), in this work we focus on `1 regularization.",
      "startOffset" : 0,
      "endOffset" : 24
    }, {
      "referenceID" : 2,
      "context" : "Bradtke and Barto (1996). Recent active researches on applying `1 regularization to TD learning have led to a various number of effective algorithms, e.",
      "startOffset" : 0,
      "endOffset" : 25
    }, {
      "referenceID" : 2,
      "context" : "Bradtke and Barto (1996). Recent active researches on applying `1 regularization to TD learning have led to a various number of effective algorithms, e.g. Loth et al. (2007); Kolter and Ng (2009); Johns et al.",
      "startOffset" : 0,
      "endOffset" : 174
    }, {
      "referenceID" : 2,
      "context" : "Bradtke and Barto (1996). Recent active researches on applying `1 regularization to TD learning have led to a various number of effective algorithms, e.g. Loth et al. (2007); Kolter and Ng (2009); Johns et al.",
      "startOffset" : 0,
      "endOffset" : 196
    }, {
      "referenceID" : 2,
      "context" : "Bradtke and Barto (1996). Recent active researches on applying `1 regularization to TD learning have led to a various number of effective algorithms, e.g. Loth et al. (2007); Kolter and Ng (2009); Johns et al. (2010); Geist and Scherrer (2012); Hoffman et al.",
      "startOffset" : 0,
      "endOffset" : 217
    }, {
      "referenceID" : 2,
      "context" : "Bradtke and Barto (1996). Recent active researches on applying `1 regularization to TD learning have led to a various number of effective algorithms, e.g. Loth et al. (2007); Kolter and Ng (2009); Johns et al. (2010); Geist and Scherrer (2012); Hoffman et al.",
      "startOffset" : 0,
      "endOffset" : 244
    }, {
      "referenceID" : 2,
      "context" : "Bradtke and Barto (1996). Recent active researches on applying `1 regularization to TD learning have led to a various number of effective algorithms, e.g. Loth et al. (2007); Kolter and Ng (2009); Johns et al. (2010); Geist and Scherrer (2012); Hoffman et al. (2012). It is important to notice that `1 minimization has been extensively studied in the areas of compressed sensing and image processing, and many efficient `1 minimization algorithms have been developed, cf.",
      "startOffset" : 0,
      "endOffset" : 267
    }, {
      "referenceID" : 2,
      "context" : "Bradtke and Barto (1996). Recent active researches on applying `1 regularization to TD learning have led to a various number of effective algorithms, e.g. Loth et al. (2007); Kolter and Ng (2009); Johns et al. (2010); Geist and Scherrer (2012); Hoffman et al. (2012). It is important to notice that `1 minimization has been extensively studied in the areas of compressed sensing and image processing, and many efficient `1 minimization algorithms have been developed, cf. Candés and Romberg (2007); Zibulevsky and Elad (2010).",
      "startOffset" : 0,
      "endOffset" : 498
    }, {
      "referenceID" : 2,
      "context" : "Bradtke and Barto (1996). Recent active researches on applying `1 regularization to TD learning have led to a various number of effective algorithms, e.g. Loth et al. (2007); Kolter and Ng (2009); Johns et al. (2010); Geist and Scherrer (2012); Hoffman et al. (2012). It is important to notice that `1 minimization has been extensively studied in the areas of compressed sensing and image processing, and many efficient `1 minimization algorithms have been developed, cf. Candés and Romberg (2007); Zibulevsky and Elad (2010). Very recently, two advanced `1 minimization algorithms have been adapted to the TD learning, i.",
      "startOffset" : 0,
      "endOffset" : 526
    }, {
      "referenceID" : 2,
      "context" : "Bradtke and Barto (1996). Recent active researches on applying `1 regularization to TD learning have led to a various number of effective algorithms, e.g. Loth et al. (2007); Kolter and Ng (2009); Johns et al. (2010); Geist and Scherrer (2012); Hoffman et al. (2012). It is important to notice that `1 minimization has been extensively studied in the areas of compressed sensing and image processing, and many efficient `1 minimization algorithms have been developed, cf. Candés and Romberg (2007); Zibulevsky and Elad (2010). Very recently, two advanced `1 minimization algorithms have been adapted to the TD learning, i.e. the Dantzig selector based TD algorithm from Geist et al. (2012) and the orthogonal matching pursuit based TD algorithm developed in Painter-Wakefield and Parr (2012a).",
      "startOffset" : 0,
      "endOffset" : 690
    }, {
      "referenceID" : 2,
      "context" : "Bradtke and Barto (1996). Recent active researches on applying `1 regularization to TD learning have led to a various number of effective algorithms, e.g. Loth et al. (2007); Kolter and Ng (2009); Johns et al. (2010); Geist and Scherrer (2012); Hoffman et al. (2012). It is important to notice that `1 minimization has been extensively studied in the areas of compressed sensing and image processing, and many efficient `1 minimization algorithms have been developed, cf. Candés and Romberg (2007); Zibulevsky and Elad (2010). Very recently, two advanced `1 minimization algorithms have been adapted to the TD learning, i.e. the Dantzig selector based TD algorithm from Geist et al. (2012) and the orthogonal matching pursuit based TD algorithm developed in Painter-Wakefield and Parr (2012a). On the other hand, most TD learning algorithms are known to be unstable with linear value function approximation and off-policy learning.",
      "startOffset" : 0,
      "endOffset" : 793
    }, {
      "referenceID" : 16,
      "context" : "The function J3 is referred to as the Norm of Expected TD Update (NEU), which is used to derive the original GTD algorithm in Sutton et al. (2008).",
      "startOffset" : 126,
      "endOffset" : 147
    }, {
      "referenceID" : 19,
      "context" : "Due to its high popularity, we skip the derivation of the IST algorithm, and refer to Zibulevsky and Elad (2010) and the references therein for further reading.",
      "startOffset" : 86,
      "endOffset" : 113
    }, {
      "referenceID" : 13,
      "context" : "Note that IST has been employed in developing fixed point TD algorithms in Painter-Wakefield and Parr (2012b), whereas in this work we focus on developing intrinsic gradient TD algorithm.",
      "startOffset" : 75,
      "endOffset" : 110
    }, {
      "referenceID" : 4,
      "context" : "To investigate convergence properties of the proposed algorithms requires results from Duchi and Singer (2009), which develops a general framework for analyzing empirical loss minimization with regularizations.",
      "startOffset" : 87,
      "endOffset" : 111
    }, {
      "referenceID" : 4,
      "context" : "To investigate convergence properties of the proposed algorithms requires results from Duchi and Singer (2009), which develops a general framework for analyzing empirical loss minimization with regularizations. We adapt the result in corollary 10 from Duchi and Singer (2009) to our current setting as follows.",
      "startOffset" : 87,
      "endOffset" : 276
    }, {
      "referenceID" : 4,
      "context" : "To investigate convergence properties of the proposed algorithms requires results from Duchi and Singer (2009), which develops a general framework for analyzing empirical loss minimization with regularizations. We adapt the result in corollary 10 from Duchi and Singer (2009) to our current setting as follows. Theorem 1 Let the function J : Rk → R be smooth and strictly convex and θ∗ ∈ Rk be the global minimum of the function F (θ) := J(θ) + η‖θ‖1 with η > 0. If the following three conditions hold: (1) θ∗ fulfills ‖θt − θ‖2 ≤ d for some constant d > 0; (2) ‖∇J(θt)‖2 ≤ g for some constant g > 0; and (3) a stochastic estimate of the gradient ∇̃J(θt) fulfills E[∇̃J(θt)] = ∇J(θt), then IST based stochastic algorithms converge with probability one to θ∗. Let us look at the `1 regularized NEU function F3 first. Recall the approximate stochastic gradient update, developed in Sutton et al. (2008), as ∇̃J3(θt) = (φt ut)(γφt − φt), (15)",
      "startOffset" : 87,
      "endOffset" : 901
    }, {
      "referenceID" : 16,
      "context" : "Sutton et al. (2008), we have",
      "startOffset" : 0,
      "endOffset" : 21
    }, {
      "referenceID" : 16,
      "context" : "In order to minimize the MSPBE function J2, two efficient GTD algorithms are developed in Sutton et al. (2009). Their approximate stochastic updates are defined as",
      "startOffset" : 90,
      "endOffset" : 111
    }, {
      "referenceID" : 16,
      "context" : "Sutton et al. (2009), we get",
      "startOffset" : 0,
      "endOffset" : 21
    }, {
      "referenceID" : 12,
      "context" : "Others In the third experiment, we compare the GTD-IST algorithms with the L1TD algorithm from Painter-Wakefield and Parr (2012b) and the LARS-TD algorithm from Kolter and Ng (2009).",
      "startOffset" : 95,
      "endOffset" : 130
    }, {
      "referenceID" : 11,
      "context" : "Others In the third experiment, we compare the GTD-IST algorithms with the L1TD algorithm from Painter-Wakefield and Parr (2012b) and the LARS-TD algorithm from Kolter and Ng (2009). Results in both Figure 3(a) and 3(b) imply that, with or without noise, all three GTD-IST algorithms outperforms the L1TD algorithm consistently.",
      "startOffset" : 161,
      "endOffset" : 182
    }, {
      "referenceID" : 0,
      "context" : "Experiment Two: Off-Policy Learning To test the performance of the GTD-IST algorithms on the off-policy learning, we employ the well-known star example, proposed in Baird (1995). It consists of seven states with one state being considered as the “center”.",
      "startOffset" : 165,
      "endOffset" : 178
    }, {
      "referenceID" : 1,
      "context" : "Applying more efficient `1 minimization algorithms, such as Beck and Teboulle (2009), to TD learning are of great interests as the future work.",
      "startOffset" : 60,
      "endOffset" : 85
    } ],
    "year" : 2016,
    "abstractText" : "In this paper, we study the Temporal Difference (TD) learning with linear value function approximation. It is well known that most TD learning algorithms are unstable with linear function approximation and off-policy learning. Recent development of Gradient TD (GTD) algorithms has addressed this problem successfully. However, the success of GTD algorithms requires a set of well chosen features, which are not always available. When the number of features is huge, the GTD algorithms might face the problem of overfitting and being computationally expensive. To cope with this difficulty, regularization techniques, in particular `1 regularization, have attracted significant attentions in developing TD learning algorithms. The present work combines the GTD algorithms with `1 regularization. We propose a family of `1 regularized GTD algorithms, which employ the well known soft thresholding operator. We investigate convergence properties of the proposed algorithms, and depict their performance with several numerical experiments.",
    "creator" : "LaTeX with hyperref package"
  }
}