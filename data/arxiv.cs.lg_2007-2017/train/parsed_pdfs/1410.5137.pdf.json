{
  "name" : "1410.5137.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "On Iterative Hard Thresholding Methods for High-dimensional M-Estimation",
    "authors" : [ "Prateek Jain", "Ambuj Tewari", "Purushottam Kar" ],
    "emails" : [ "prajain@microsoft.com,", "t-purkar@microsoft.com,", "tewaria@umich.edu" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Modern statistical estimation is routinely faced with real world problems where the number of parameters p handily outnumbers the number of observations n. In general, consistent estimation of parameters is not possible in such a situation. Consequently, a rich line of work has focused on models that satisfy special structural assumptions such as sparsity or low-rank structure. Under these assumptions, several works (for example, see [1, 2, 3, 4, 5]) have established that consistent estimation is information theoretically possible in the “n p” regime as well.\nThe question of efficient estimation, however, is faced with feasibility issues since consistent estimation routines often end-up solving NP-hard problems. Examples include sparse regression which requires loss minimization with sparsity constraints and low-rank regression which requires dealing with rank constraints which are not efficiently solvable in general [6].\nInterestingly, recent works have demonstrated that these hardness results can be avoided by assuming certain natural conditions over the loss function being minimized such as restricted strong convexity (RSC) and restricted strong smoothness (RSS). The estimation routines proposed in these works typically make use of convex relaxations [5] or greedy methods [7, 8, 9] which do not suffer from infeasibility issues.\nar X\niv :1\n41 0.\n51 37\nv2 [\ncs .L\nG ]\n2 1\nO ct\nDespite this, certain limitations have precluded widespread use of these techniques. Convex relaxation-based methods typically suffer from slow rates as they solve non-smooth optimization problems apart from being hard to analyze in terms of global guarantees. Greedy methods, on the other hand, are slow in situations with non-negligible sparsity or relatively high rank, owing to their incremental approach of adding/removing individual support elements.\nInstead, the methods of choice for practical applications are actually projected gradient (PGD) methods, also referred to as iterative hard thresholding (IHT) methods. These methods directly project the gradient descent update onto the underlying (non-convex) feasible set. This projection can be performed efficiently for several interesting structures such as sparsity and low rank. However, traditional PGD analyses for convex problems viz. [10] do not apply to these techniques due to the non-convex structure of the problem.\nAn exception to this is the recent work [11] that demonstrates that PGD with non-convex regularization can offer consistent estimates for certain high-dimensional problems. However, the work in [11] is only able to analyze penalties such as SCAD, MCP and capped L1. Moreover, their framework cannot handle commonly used penalties such as L0 or low-rank constraints."
    }, {
      "heading" : "1.1 Insufficiency of RIP based Guarantees for M-estimation",
      "text" : "As noted above, PGD/IHT-style methods have been very popular in literature for sparse recovery and several algorithms including Iterative Hard Thresholding (IHT) [12] or GraDeS [13], Hard Thresholding Pursuit (HTP) [14], CoSaMP [15], Subspace Pursuit (SP) [16], and OMPR(`) [17] have been proposed. However, the analysis of these algorithms has traditionally been restricted to settings that satisfy the Restricted Isometry property (RIP) or incoherence property. As the discussion below demonstrates, this renders these analyses inaccessible to high-dimensional statistical estimation problems.\nAll existing results analyzing these methods require the condition number of the loss function, restricted to sparse vectors, to be smaller than a universal constant. The best known such constant is due to the work of [17] that requires a bound on the RIP constant δ2k ≤ 0.5 (or equivalently a bound 1+δ2k 1−δ2k ≤ 3 on the condition number). In contrast, real-life high dimensional statistical settings, wherein pairs of variables can be arbitrarily correlated, routinely require estimation methods to perform under arbitrarily large condition numbers. In particular if two variates have a covariance\nmatrix like\n[ 1 1−\n1− 1\n] , then the restricted condition number (on a support set of size just 2)\nof the sample matrix cannot be brought down below 1/ even with infinitely many samples. In particular when < 1/6, none of the existing results for hard thresholding methods offer any guarantees. Moreover, most of these analyses consider only the least squares objective. Although recent attempts have been made to extend this to general differentiable objectives [18, 19], the results continue to require that the restricted condition number be less than a universal constant and remain unsatisfactory in a statistical setting."
    }, {
      "heading" : "1.2 Overview of Results",
      "text" : "Our main contribution in this work is an analysis of PGD/IHT-style methods in statistical settings. Our bounds are tight, achieve known minmax lower bounds [20], and hold for arbitrary differentiable, possibly even non-convex functions. Our results hold even when the underlying condition number is arbitrarily large and only require the function to satisfy RSC/RSS conditions. In partic-\nular, this reveals that these iterative methods are indeed applicable to statistical settings, a result that escaped all previous works.\nOur first result shows that the PGD/IHT methods achieve global convergence if used with a relaxed projection step. More formally, if the optimal parameter is s∗-sparse and the problem satisfies RSC and RSS constraints α and L respectively (see Section 2), then PGD methods offer global convergence so long as they employ projection to an s-sparse set where s ≥ 4(L/α)2s∗. This gives convergence rates that are identical to those of convex relaxation and greedy methods for the Gaussian sparse linear model. We then move to a family of efficient “fully corrective” methods and show as before, that for arbitrary functions satisfying the RSC/RSS properties, these methods offer global convergence.\nNext, we show that these results allow PGD-style methods to offer global convergence in a variety of statistical estimation problems such as sparse linear regression and low rank matrix regression. Our results effortlessly extend to the noisy setting as a corollary and give bounds similar to those of [21] that relies on solving an L1 regularized problem.\nOur proofs are able to exploit that even though hard-thresholding is not the prox-operator for any convex prox function, it still provides strong contraction when projection is performed onto sets of sparsity s s∗. This crucial observation allows us to provide the first unified analysis for hard thresholding based gradient descent algorithms. Our empirical results confirm our predictions with respect to the recovery properties of IHT-style algorithms on badly-conditioned sparse recovery problems, as well as demonstrate that these methods can be orders of magnitudes faster than their L1 and greedy counterparts."
    }, {
      "heading" : "1.3 Organization of the Paper",
      "text" : "Section 2 sets the notation and the problem statement. Section 3 introduces the PGD/IHT algorithm that we study and proves that the method guarantees recovery assuming the RSC/RSS property. We also generalize our guarantees to the problem of low-rank matrix regression. Section 4 then provides crisp sample complexity bounds and statistical guarantees for the PGD/IHT estimators. Section 5 extends our analysis to a broad family of “fully-corrective” hard thresholding methods compressive sensing algorithms that include the so-called two-stage hard thresholding and partial hard thresholding algorithms and provide similar results for them as well. We present some empirical results in Section 6 and conclude in Section 7."
    }, {
      "heading" : "2 Problem Setup and Notations",
      "text" : "High-dimensional Sparse Estimation. Given data points X = [X1, . . . , Xn] T , where Xi ∈ Rp, and the target Y = [Y1, . . . , Yn] T , where Yi ∈ R, the goal is to compute an s∗-sparse θ∗ s.t.,\nθ∗ = arg min θ,‖θ‖0≤s∗ f(θ). (1)\nTypically, f can be thought of as an empirical risk function i.e. f(θ) = 1n ∑\ni `(〈Xi,θ〉, Yi) for some loss function ` (see examples in Section 4). However, for our analysis of PGD and other algorithms, we need not assume any other property of f other than differentiability and the following two RSC and RSS properties.\nDefinition 1 (RSC Property). A differentiable function f : Rp → R is said to satisfy restricted strong convexity (RSC) at sparsity level s = s1 + s2 with strong convexity constraint αs if the following holds for all θ1,θ2 s.t. ‖θ1‖0 ≤ s1 and ‖θ2‖0 ≤ s2:\nf(θ1)− f(θ2) ≥ 〈θ1 − θ2,∇θf(θ2)〉+ αs 2 ‖θ1 − θ2‖22.\nDefinition 2 (RSS Property). A differentiable function f : Rp → R is said to satisfy restricted strong smoothness (RSS) at sparsity level s = s1 + s2 with strong convexity constraint Ls if the following holds for all θ1,θ2 s.t. ‖θ1‖0 ≤ s1 and ‖θ2‖0 ≤ s2:\nf(θ1)− f(θ2) ≤ 〈θ1 − θ2,∇θf(θ2)〉+ Ls 2 ‖θ1 − θ2‖22.\nLow-rank Matrix Regression. Low-rank matrix regression is similar to sparse estimation as presented above except that each data point is now a matrix i.e. Xi ∈ Rp1×p2 , the goal being to estimate a low-rank matrix W ∈ Rp1×p2 that minimizes the empirical loss function on the given data.\nW ∗ = arg min W,rank(W )≤r f(W ). (2)\nFor this problem the RSC and RSS properties for f are defined similarly as in Definition 1, 2 except that the L0 norm is replaced by the rank function."
    }, {
      "heading" : "3 Iterative Hard-thresholding Method",
      "text" : "In this section we study the popular projected gradient descent (a.k.a iterative hard thresholding) method for the case of the feasible set being the set of sparse vectors (see Algorithm 1 for pseudocode). The projection operator Ps(z), can be implemented efficiently in this case by projecting z onto the set of s-sparse vectors by selecting the s largest elements (in magnitude) of z. The standard projection property implies that ‖Ps(z) − z‖22 ≤ ‖θ′ − z‖22 for all ‖θ′‖0 ≤ s. However, it turns out that we can prove a significantly stronger property of hard thresholding for the case when ‖θ′‖0 ≤ s∗ and s∗ s. This property is key to analysing IHT and is formalized below.\nLemma 1. For any index set I, any z ∈ RI , let θ = Ps(z). Then for any θ∗ ∈ RI such that ‖θ∗‖0 ≤ s∗, we have\n‖θ − z‖22 ≤ |I| − s |I| − s∗ ‖θ∗ − z‖22.\nSee Appendix A for a detailed proof. Our analysis combines the above observation with the RSC/RSS properties of f to provide\ngeometric convergence rates for the IHT procedure below.\nTheorem 1. Let f have RSC and RSS parameters given by L2s+s∗(f) = L and α2s+s∗(f) = α respectively. Let Algorithm 1 be invoked with f , s ≥ 32 ( L α )2 s∗ and η = 23L . Also let θ ∗ = arg minθ,‖θ‖0≤s∗ f(θ). Then, the τ -th iterate of Algorithm 1, for τ = O( L α · log( f(θ0) )) satisfies:\nf(θτ )− f(θ∗) ≤ .\nAlgorithm 1 Iterative Hard-thresholding\n1: Input: Function f with gradient oracle, sparsity level s, step-size η 2: θ1 = 0, t = 1 3: while not converged do 4: θt+1 = Ps(θ\nt − η∇θf(θt)) 5: t = t+ 1 6: end while 7: Output: θt\nProof. (Sketch) Let St = supp(θt), S∗ = supp(θ∗), St+1 = supp(θt+1) and It = S∗ ∪ St ∪ St+1. Using the RSS property and the fact that supp(θt) ⊆ It and supp(θt+1) ⊆ It, we have:\nf(θt+1)− f(θt) ≤ 〈θt+1 − θt, gt〉+ L 2 ‖θt+1 − θt‖22,\n= L\n2 ‖θt+1It − θ t It +\n2\n3L · gtIt‖ 2 2 −\n1\n2L ‖gtIt‖ 2 2,\nζ1 ≤ L 2 · |I t| − s |It| − s∗ · ‖θ∗It − θ t It + 1 L · gtIt‖ 2 2 − 1 2L (‖gtIt\\(St∪S∗)‖ 2 2 + ‖gtSt∪S∗‖ 2 2), (3)\nwhere ζ1 follows from an application of Lemma 1 with I = I t and the Pythagoras theorem. The above equation has three critical terms. The first term can be bounded using the RSS condition. Using f(θt) − f(θ∗) ≤ 〈gtSt∪S∗ ,θ t − θ∗〉 − α2 ‖θ t − θ∗‖22 ≤ 12α‖g t St∪S∗‖ 2 2 bounds the third term in (3). The second term is more interesting as in general elements of gt S∗ can be arbitrarily small. However, elements of gtIt\\(St∪S∗) should be at least as large as g t S∗\\St+1 as they are selected by hard-thresholding. Combining this insight with bounds for gtS∗\\St+1 and with (3), we obtain the theorem. See Appendix A for a detailed proof."
    }, {
      "heading" : "3.1 Low-rank Matrix Regression",
      "text" : "We now generalize our previous analysis to a projected gradient descent (PGD) method for low-rank matrix regression. Formally, we study the following problem:\nmin W\nf(W ), s.t., rank(W ) ≤ s. (4)\nThe hard-thresholding projection step for low-rank matrices can be solved using SVD i.e.\nPMs(W ) = UsΣsV T s ,\nwhere W = UΣV T is the singular value decomposition of W . Us, Vs are the top-s singular vectors (left and right, respectively) of W and Σs is the diagonal matrix of the top-s singular values of W . To proceed, we first note a property of the above projection similar to Lemma 1.\nLemma 2. Let W ∈ Rp1×p2 be a rank-|It| matrix and let p1 ≥ p2. Then for any rank-s∗ matrix W ∗ ∈ Rp1×p2 we have\n‖PMs(W )−W‖2F ≤ |It| − s |It| − s∗ ‖W ∗ −W‖2F . (5)\nProof. Let W = UΣV T be the singular value decomposition of W . Now, ‖PMs(W ) − W‖2F =∑|It| i=s+1 σ 2 i = ‖Ps(diag(Σ)) − diag(Σ)‖22, where σ1 ≥ · · · ≥ σ|It| ≥ 0 are the singular values of W . Using Lemma 1, we get:\n‖PMs(W )−W‖2F ≤ |It| − s |It| − s∗ ‖Σ∗ − diag(Σ)‖22 ≤ |It| − s |It| − s∗ ‖W ∗ −W‖2F , (6)\nwhere the last step uses the von Neumann’s trace inequality (Tr(A ·B) ≤ ∑\ni σi(A)σi(B)).\nThe following result for low-rank matrix regression immediately follows from Lemma 4.\nTheorem 2. Let f have RSC and RSS parameters given by L2s+s∗(f) = L and α2s+s∗(f) = α. Replace the projection operator Ps in Algorithm 1 with its matrix counterpart PMs as defined in (5).\nSuppose we invoke it with f, s ≥ 32 ( L α )2 s∗, η = 23L . Also let W ∗ = arg minW,rank(W )≤s∗ f(W ). Then the τ -th iterate of Algorithm 1, for τ = O(Lα · log( f(W 0) ) satisfies:\nf(W τ )− f(W ∗) ≤ .\nProof. A proof progression similar to that of Theorem 1 suffices. The only changes that need to be made are: firstly Lemma 2 has to be invoked in place of Lemma 1. Secondly, in place of considering vectors restricted to a subset of coordinates viz. θS , g t I , we would need to consider matrices restricted to subspaces i.e. WS = USU T SW where US is a set of singular vectors spanning the range-space of S."
    }, {
      "heading" : "4 High Dimensional Statistical Estimation",
      "text" : "This section elaborates on how the results of the previous section can be used to give guarantees for IHT-style techniques in a variety of statistical estimation problems. We will first present a generic convergence result and then specialize it to various settings. Suppose we have a sample of data points Z1:n and a loss function L(θ;Z1:n) that depends on a parameter θ and the sample. Then we can show the following result. (See Appendix B for a proof.)\nTheorem 3. Let θ̄ be any s∗-sparse vector. Suppose L(θ;Z1:n) is differentiable and satisfies RSC and RSS at sparsity level s + s∗ with parameters αs+s∗ and Ls+s∗ respectively, for s ≥ 32 ( L2s+s∗ α2s+s∗ )2 s∗. Let θτ be the τ -th iterate of Algorithm 1 for τ chosen as in Theorem 1 and ε be the function value error incurred by Algorithm 1. Then we have\n‖θ̄ − θτ‖2 ≤ 2 √ s+ s∗‖∇L(θ̄;Z1:n)‖∞\nαs+s∗ +\n√ 2\nαs+s∗ .\nNote that the result does not require the loss function to be convex. This fact will be crucially used later. We now apply the above result to several statistical estimation scenarios.\nSparse Linear Regression. Here Zi = (Xi, Yi) ∈ Rp×R and Yi = 〈θ̄, Xi〉+ξi where ξi ∼ N (0, σ2) is label noise. The empirical loss is the usual least squares loss i.e. L(θ;Z1:n) = 1n‖Y − Xθ‖ 2 2. Suppose X1:n are drawn i.i.d. from a sub-Gaussian distribution with covariance Σ with Σjj ≤ 1 for\nall j. Then [22, Lemma 6] immediately implies that RSC and RSS at sparsity level k hold, with probability at least 1 − e−c0n, with αk = 12σmin(Σ) − c1 k log p n and Lk = 2σmax(Σ) + c1 k log p n (c0, c1 are universal constants). So we can set k = 2s + s∗ and if n > 4c1k log p/σmin(Σ) then we have αk ≥ 14σmin(Σ) and Lk ≤ 2.25σmax(Σ) which means that Lk/9αk ≤ κ(Σ) := σmax(Σ)/σmin(Σ). Thus it is enough to choose s = 2592κ(Σ)2s∗ and apply Theorem 3. Note that ‖∇L(θ̄;Z1:n)‖∞ = ‖XT ξ/n‖∞ ≤ 2σ √ log p n with probability at least 1−c2p −c3 (c2, c3 are universal constants). Putting everything together, we have the following bound with high probability:\n‖θ̄ − θτ‖2 ≤ 145 κ(Σ)\nσmin(Σ) σ\n√ s∗ log p\nn + 2\n√\nσmin(Σ) ,\nwhere is the function value error incurred by Algorithm 1.\nNoisy and Missing Data. We now look at cases with feature noise as well. More specifically, assume that we only have access to X̃i’s that are corrupted versions of Xi’s. Two models of noise are popular in literature [21]: a) (additive noise) X̃i = Xi+Wi where Wi ∼ N (0,ΣW ), and b) (missing data) X̃ is an R∪{?}-valued matrix obtained by independently, with probability ν ∈ [0, 1), replacing each entry in X with ?. For the case of additive noise (missing data can be handled similarly), Zi = (X̃i, Yi) and L(θ;Z1:n) = 12θ\nT Γ̂θ−γ̂Tθ where Γ̂ = X̃T X̃/n−ΣW and γ̂ = X̃TY/n are unbiased estimators of Σ and ΣT θ̄ respectively. [21, Appendix A, Lemma 1] implies that RSC, RSS at sparsity level k hold, with failure probability exponentially small in n, with αk = 1 2σmin(Σ)− kτ(p)/n and Lk = 1.5σmax(Σ)+kτ(p)/n for τ(p) = c0σmin(Σ) max( (‖Σ‖2op+‖ΣW ‖2op)2\nσ2min(Σ) , 1) log p. Thus for k = 2s+s∗\nand n ≥ 4kτ(p)/σmin(Σ) we have Lk/αk ≤ 7κ(Σ). Note that L(·;Z1:n) is non-convex but we can still apply Theorem 3 with s = 1568κ(Σ)2s∗ because RSC, RSS hold. Using the high probability upper bound (see [21, Appendix A, Lemma 2]) ‖∇L(θ̄;Z1:n)‖∞ ≤ c1σ̃‖θ̄‖2 √ log p/n gives us the following\n‖θ̄ − θτ‖2 ≤ c2 κ(Σ)\nσmin(Σ) σ̃‖θ̄‖2\n√ s∗ log p\nn + 2\n√\nσmin(Σ) where σ̃ = √ ‖ΣW ‖2op + ‖Σ‖2op(‖ΣW ‖op + σ) and is the function value error in Algorithm 1."
    }, {
      "heading" : "5 Fully-corrective Methods",
      "text" : "In this section, we study a variety of “fully-corrective” methods. These methods keep the optimization objective fully minimized over the support of the current iterate. To this end, we first prove a fundamental theorem for fully-corrective methods that formalizes the intuition that for such methods, a large function value should imply a large gradient at any sparse θ as well. This result is similar to Lemma 1 of [17] but holds under RSC/RSS conditions (rather than the RIP condition as in [17]), as well as for the general loss functions.\nLemma 3. Consider a function f with RSC parameter given by L2s+s∗(f) = L and RSS parameter given by α2s+s∗(f) = α. Let θ ∗ = arg minθ,‖θ‖0≤s∗ f(θ) with S ∗ = supp(θ∗). Let St ⊆ [p] be any subset of co-ordinates s.t. |St| ≤ s. Let θt = arg minθ,supp(θ)⊆St f(θ). Then, we have:\n2α(f(θt)− f(θ∗)) ≤ ‖gtSt∪S∗‖ 2 2 − α2‖θtSt\\S∗‖ 2 2\nSee Appendix C for a detailed proof.\nAlgorithm 2 Two-stage Hard-thresholding\n1: Input: function f with gradient oracle, sparsity level s, sparsity expansion level ` 2: θ1 = 0, t = 1 3: while not converged do 4: gt = ∇θf(θt), St = supp(θt) 5: Zt = St ∪ (largest ` elements of |gt\nSt |)\n6: βt = arg minβ,supp(β)⊆Zt f(β) // fully corrective step 7: θ̃t = Ps(β t) 8: θt+1 = arg min θ,supp(θ)⊆supp(θ̃t) f(θ) // fully corrective step\n9: t = t+ 1 10: end while 11: Output: θt"
    }, {
      "heading" : "5.1 Two-stage Hard Thresholding Methods",
      "text" : "Here we will concentrate on a family of two-stage fully corrective methods that contains popular compressive sensing algorithms like CoSaMP and Subspace Pursuit (see Algorithm 2 for pseudocode). These algorithms have thus far been analyzed only under RIP conditions for the least squares objective. Using our analysis framework developed in the previous sections, we present a generic RSC/RSS-based analysis for general two-stage methods for arbitrary loss functions. Our analysis shall use the following key observation that the the hard thresholding step in two stage methods does not increase the objective function a lot.\nLemma 4. Let Zt ⊆ [n] and |Zt| ≤ q. Let βt = arg minβ,supp(β)⊆Zt f(β) and θ̂t = Pq(βt). Then, the following holds:\nf(θ̂t)− f(βt) ≤ L α · ` s+ `− s∗ · (f(βt)− f(θ∗)).\nProof. Let vt = ∇θf(βt). Then, using the RSS property we get:\nf(θ̂t)− f(βt) ≤ 〈θ̂t − βt,vt〉+ L 2 ‖θ̂t − βt‖22 ζ1 = L 2 ‖θ̂t − βt‖22 ζ2 ≤ L 2 |`| |s+ `− s∗| ‖w − βt‖22, (7)\nwhere w is any vector such that wZt = 0 and ‖w‖0 ≤ s ∗. ζ1 follows by observing v t Zt = 0 and by noting that supp(θ̂t) ⊆ Zt. ζ2 follows by Lemma 1 and the fact that ‖w‖0 ≤ s∗. Now, using RSS property and the fact that ∇θf(βt) = 0, we have:\nα 2 ‖w − βt‖22 ≤ f(βt)− f(w) ≤ f(βt)− f(θ∗). (8)\nThe result now follows by combining (7) and (8).\nTheorem 4. Let f has RSC and RSS parameters given by α2s+s∗(f) = α and L2s+`(f) = L resp. Call Algorithm 2 with f , ` ≥ s∗ and s ≥ 4L2 α2 `+s∗−` ≥ 4L2 α2 s∗. Also let θ∗ = arg minθ,‖θ‖0≤s∗ f(θ). Then, the τ -th iterate of Algorithm 2, for τ = O(Lα · log( f(θ0) ) satisfies:\nf(θτ )− f(θ∗) ≤ .\nSee Appendix C for a detailed proof."
    }, {
      "heading" : "5.2 Partial Hard Thresholding Methods",
      "text" : "Algorithm 3 Iterative Partial Hard-thresholding\n1: Input: function f with gradient oracle, sparsity level s, step size η, partial thresholding level ` 2: θ1 = 0, t = 1 3: while not converged do 4: zt = θt − η∇θtf(θt), St = supp(θt) 5: vt = PHTs(z\nt;St, `) 6: θt+1 = arg minθ,supp(θ)⊆supp(vt) f(θ) // fully corrective step 7: t = t+ 1 8: end while 9: Output: θt\nWe now study Partial Hard Thresholding methods (PHT), a family of fully-corrective iterative methods introduced by [17]. This family is known to provide the best known RIP guarantees in the compressive sensing setting, but the proof is restricted to the RIP setting, and for the leastsquares objective. An interesting member of this family is Orthogonal Matching Pursuit with Replacement (OMPR), which is also a Forward-Backward Greedy Selection method but performs one forward-backward step per iteration.\nThe pseudo code of the general IPHT(`) algorithm is given in Algorithm 3. The algorithm is similar to the fully-corrective projected gradient descent (PGD) method, in fact, PHT(0) is indeed exactly same as the fully-corrected PGD method. But, the partial hard-thresholding projection is used instead of hard-thresholding projection.\nThe partial hard thresholding operator vt = PHTs(z t;St, `) projects zt onto the non-convex set of s-sparse vectors s.t. |supp(vt)\\St| ≤ `. Although, the operator projects onto a non-convex set, still the projection can be performed efficiently by performing hard thresholding only over zt\nSt\nand the ` smallest elements of ztSt . That is, let bot t = smallest ` elements of |ztSt |. Then,\nvtSt\\bott = z t St\\bott , and, v t St∪bott = H`(z t St∪bott).\nWe first show that at least a new element is added during each iteration of IPHT(`).\nLemma 5. Let f , s be supplied to Algorithm 3 and let the RSC and RSS parameters of f be given by L2s+s∗(f) = L and α2s+s∗(f) = α respectively. Let s ≥ 4 ( L α )2 s∗. Then, either f(St) = f(S∗) or |St+1\\St| ≥ 1. That is, at least one new element is added at each iteration of Algorithm 3.\nProof. Suppose no new element is added i.e. |St+1\\St| = 0. Since ` elements of St+1 are selected by hard thresholding zt\nSt∪bott , hence each element of ztSt should be larger (in magnitude) than\nmaxi∈St |z t i |. Note that, ztSt = −ηg t St . Hence,\n‖ztSt\\S∗‖ 2 2 |St\\S∗| = ‖θtSt\\S∗ − ηg t St\\S∗‖ 2 2 |St\\S∗| = ‖θtSt\\S∗‖ 2 2 |St\\S∗| ≥ ‖ztS∗\\St‖ 2 2 |S∗\\St| = ‖θtS∗\\St − ηg t S∗\\St‖ 2 2 |S∗\\St|\n= η2 ‖gtS∗\\St‖ 2 2\n|S∗\\St| = η2\n‖gtS∗∪St‖ 2 2\n|S∗\\St| ,\nwhere we have used the fact that gtSt = 0 and θ t St = 0. Using Lemma 3 and the above equation, we have:\n0 ≤ 2γ ( f(θt)− f(θ∗)− α 2 · ( 1 αγ − 1 ) ‖θt − θ∗‖22 ) ≤ ( γ2 − η2 |S t\\S∗| |S∗\\St| ) ‖gtSt∪S∗‖ 2. (9)\nThe lemma now follows by observing that |S t\\S∗| |S∗\\St| ≥ s−s∗ s∗ ≥ γ2 η2 ≥ 1 η2α2 , by the choice of s.\nWe now provide the proof of convergence for IPHT(`) method in the general RSC-RSS setting:\nTheorem 5. Let f, s be supplied to Algorithm 3. Also, let the RSC and RSS parameters of f be given by α2s+s∗(f) = α and L2s+s∗(f) = L respectively. Let s ≥ 4 ( L α )2 s∗ and let η = 12L . Then, the τ -th iterate of Algorithm 3 satisfies:\nf(θτ )− f(θ∗) ≤ (\n1− α 4L · 1 `+ 1\n)τ · ( f(θ0)− f(θ∗) ) ,\nwhere θ∗ = arg minθ,‖θ‖0≤s∗ f(θ).\nThis implies that for τ = O ( L` α · log( f(θ0) ) ) , we have f(θτ ) − f(θ∗) ≤ . See Appendix C for\na detailed proof."
    }, {
      "heading" : "6 Experiments",
      "text" : "We conducted simulations on high dimensional sparse linear regression problems to verify our predictions. Our experiments demonstrate that hard thresholding and projected gradient techniques can not only offer recovery in stochastic setting, but offer much more scalable routines for the same.\nData: Our problem setting is identical to the one described in the previous section. We fixed a parameter vector θ̄ by choosing s∗ random coordinates and setting them randomly to ±1 values.\nData samples were generated as Zi = (Xi, Yi) where Xi ∼ N (0, Ip) and Yi = 〈θ̄, Xi〉 + ξi where ξi ∼ N (0, σ2). We studied the effect of varying dimensionality p, sparsity s∗, sample size n and label noise level σ on the recovery properties of the various algorithms as well as their run times. We chose baseline values of p = 20000, s∗ = 100, σ = 0.1, n = fo · s∗ log p where fo is the oversampling factor with default value fo = 2. Keeping all other quantities fixed, we varied one of the quantities and generated independent data samples for the experiments.\nAlgorithms: We studied a variety of hard-thresholding style algorithms including HTP [14], GraDeS [13] (or IHT [12]), CoSaMP [15], OMPR [17] and SP [16]. We compared them with a standard implementation of the L1 projected scaled sub-gradient technique [23] for the lasso problem and a greedy method FoBa [24] for the same.\nEvaluation Metrics: For the baseline noise level σ = 0.1, we found that all the algorithms were able to recover the support set within an error of 2%. Consequently, our focus shifted to running times for these experiments. In the experiments where noise levels were varied, we recorded, for each method, the number of undiscovered support set elements.\nResults: Figure1 describes the results of our experiments in graphical form. For sake of clarity we included only HTP, GraDeS, L1 and FoBa results in these graphs. Graphs for the other algorithms CoSaMP, SP and OMPR can be seen in the supplementary material. The graphs indicate that whereas hard thresholding techniques are equally effective as L1 and greedy techniques for recovery in noisy settings, as indicated by Figure1(a), the former can be much more efficient and scalable than the latter. For instance, as Figure1(b), for the base level of p = 20000, HTP was 150× faster than the L1 method. For higher values of p, the runtime gap widened to more than 350×. We also note that in both these cases, HTP actually offered exact support recovery whereas L1 was unable to recover 2 and 4 support elements respectively.\nAlthough FoBa was faster than L1 on Figure1(b) experiments, it was still slower than HTP by 50× and 90× for p = 20000 and 25000 respectively. Moreover, due to its greedy and incremental nature, FoBa was found to suffer badly in settings with larger true sparsity levels. As Figure 1(c) indicates, for even moderate sparsity levels of s∗ = 300 and 500, FoBa is 60 − 75× slower than HTP. As mentioned before, the reason for this slowdown is the greedy approach followed by FoBa: whereas HTP took less than 5 iterations to converge for these two problems, FoBa spend 300 and 500 iterations respectively. GraDeS was found to offer much lesser run times in comparison being slower than HTP by 30− 40× for larger values of p and 2− 5× slower for larger values of s∗.\nExperiments on badly conditioned problems. We also ran experiments to verify the performance of IHT algorithms in high condition number setting. Values of p, s∗ and σ were kept at baseline levels. After selecting the optimal parameter vector θ̄, we selected s∗/2 random coordinates from its support and s∗/2 random coordinates outside its support and constructed a covariance matrix with heavy correlations between these chosen coordinates. The condition number of the resulting matrix was close to 50. Samples were drawn from this distribution and the recovery properties of the different IHT-style algorithms was observed as the projected sparsity levels s were increased. Our results (see Figure 1(d)) corroborate our theoretical observation that these algorithms show a remarkable improvement in recovery properties for ill-conditioned problems with an enlarged projection size."
    }, {
      "heading" : "7 Discussion and Conclusions",
      "text" : "In our work we studied iterative hard thresholding algorithms and showed that these techniques can offer global convergence guarantees for arbitrary, possibly non-convex, differentiable objective functions, which nevertheless satisfy Restricted Strong Convexity/Smoothness (RSC/RSM) conditions. Our results apply to a large family of algorithms that includes existing algorithms such as IHT, GraDeS, CoSaMP, SP and OMPR. Previously the analyses of these algorithms required stringent RIP conditions that did not allow the (restricted) condition number to be larger than universal constants specific to these algorithms.\nOur basic insight was to relax this stringent requirement by running these iterative algorithms with an enlarged support size. We showed that guarantees for high-dimensional M-estimation follow seamlessly from our results by invoking results on RSC/RSM conditions that have already been established in the literature for a variety of statistical settings. Our theoretical results put hard thresholding methods on par with those based on convex relaxation or greedy algorithms. Our experimental results demonstrate that hard thresholding methods outperform convex relaxation and greedy methods in terms of running time, sometime by orders of magnitude, all the while offering competitive or better recovery properties.\nOur results apply to sparsity and low rank structure, arguably two of the most commonly used structures in high dimensional statistical learning problems. In future work, it would be interesting to generalize our algorithms and their analyses to more general structures. A unified analysis for general structures will probably create interesting connections with existing unified frameworks such as those based on decomposability [5] and atomic norms [25]."
    }, {
      "heading" : "A Proofs for Section 3",
      "text" : "Proof of Lemma 1. Without loss of generality, assume that we have reordered coordinates such that |z1| ≥ |z2| ≥ . . . ≥ |zI |. Since the projection operator Ps(·) operates by selecting the largest elements by magnitude, we have θ1 = z1, . . . ,θs = zs and θs+1 = θs+2 = . . . = θ|I| = 0.\nAlso define θz = Ps∗(z). By the above argument, we have θ z 1 = z1, . . . ,θ z s∗ = zs∗ and θ z s∗+1 =\nθzs∗+2 = . . . = θ z |I| = 0. Now we have\n‖θz − z‖ |I| − s∗ − ‖θ − z‖ |I| − s = 1 |I| − s∗ s∑\ni=s∗+1\nz2i +\n( 1\n|I| − s∗ − 1 |I| − s ) |I|∑ i=s+1 z2i\n≥ s− s ∗\n|I| − s∗ z2s + s∗ − s (|I| − s∗)(|I| − s) (|I| − s)z2s+1 ≥ 0, (10)\nsince the coordinates of z are arranged in decreasing order of magnitude. Combining the above with the observation that, due to the projection property ‖θ∗−z‖ ≥ ‖θz−z‖, proves the result.\nProof of Theorem 1. Recall that θt+1 = Ps(θ t − η\n′\nL g t) where η′ = 23 < 1. Let S t = supp(θt), S∗ = supp(θ∗), and St+1 = supp(θt+1). Also, let It = S∗ ∪ St ∪ St+1.\nNow, using the RSS property and the fact that supp(θt) ⊆ It and supp(θt+1) ⊆ It, we have:\nf(θt+1)− f(θt) ≤ 〈θt+1 − θt, gt〉+ L 2 ‖θt+1 − θt‖22,\n= L\n2 ‖θt+1It − θ t It +\nη′ L · gtIt‖ 2 2 − (η′)2 2L ‖gtIt‖ 2 2 + (1− η′)〈θt+1 − θt, gt〉. (11)\nAs supp(θt) = St, supp(θt+1) = St+1 and St\\St+1, St+1 are disjoint, we have:\n〈θt+1 − θt, gt〉 = −〈θtSt\\St+1 , g t St\\St+1〉+ 〈θ t+1 St+1 − θtSt+1 , g t St+1〉,\nζ1 = −〈θtSt\\St+1 , g t St\\St+1〉 −\nη′ L ‖gtSt+1‖ 2 2,\nζ2 ≤ η\n′\n2L ‖gtSt+1\\St‖ 2 2 −\nη′\n2L ‖gtSt\\St+1‖ 2 2 −\nη′ L ‖gtSt+1‖ 2 2,\nζ3 = − η\n′\n2L ‖gtSt+1\\St‖ 2 2 −\nη′\n2L ‖gtSt\\St+1‖ 2 2 −\nη′ L ‖gtSt∩St+1‖ 2 2\n≤ − η ′\n2L ‖gtSt∪St+1‖ 2 2, (12)\nwhere the equality ζ1 follows from the gradient step, i.e., θ t+1 St+1 = θtSt+1− η′ L g t St+1 . The inequality ζ2 follows using the fact that θt+1 is obtained using hard thresholding and the fact that |St\\St+1| =\n|St+1\\St|, as follows:\n‖θtSt\\St+1 − η′\nL gtSt\\St+1‖ 2 2 ≤ ‖θt+1St+1\\St‖ 2 2 =\n(η′)2\nL2 ‖gtSt+1\\St‖ 2 2. (13)\nThe equality ζ3 follows from ‖gtSt+1‖ 2 2 = ‖gtSt+1\\St‖ 2 2 + ‖gtSt∩St+1‖ 2 2.\nHence, using (11) and (12), we have:\nf(θt+1)− f(θt) ≤ L 2 ‖θt+1It − θ t It +\nη′ L · gtIt‖ 2 2 − (η′)2 2L ‖gtIt‖ 2 2 − η′(1− η′) 2L ‖gtSt∪St+1‖ 2 2,\n= L\n2 ‖θt+1It − θ t It +\nη′ L · gtIt‖ 2 2 − (η′)2 2L ‖gtIt\\(St∪S∗)‖ 2 2 − (η′)2 2L ‖gtSt∪S∗‖ 2 2\n− η ′(1− η′)\n2L ‖gtSt∪St+1‖ 2 2. (14)\nNext, let us try to upper bound the first two terms on the right hand side above. Since It\\(St ∪ S∗) = St+1\\(St ∪ S∗) ⊆ St+1, we have θt+1It\\(St∪S∗) = θ t It\\(St∪S∗) − η′ L g t It\\(St∪S∗). However, as θtIt\\St = 0, we actually have θ t+1 It\\(St∪S∗) = − η′ L g t It\\(St∪S∗). Now let us choose a set R ⊆ S t\\St+1 such that |R| = |St+1\\(St ∪ S∗)|. Such a choice is possible since |St+1\\(St ∪ S∗)| = |St\\St+1| − |(St+1 ∩ S∗)\\St| (which itself is a consequence of the fact that |St+1| = |St|). Moreover, since θt+1\nis obtained by hard-thresholding ( θt − η ′ L g t ) , for any choice of R made above, we have:\n(η′)2\nL2 ‖gtIt\\(St∪S∗)‖ 2 2 = ‖θt+1It\\(St∪S∗)‖ 2 2 ≥ ‖θtR −\nη′ L gtR‖22. (15)\nUsing above equation, and the fact that θt+1R = 0 (since R ⊆ St+1), we have:\nL 2 ‖θt+1It − θ t It +\nη′ L · gtIt‖ 2 2 − (η′)2 2L ‖gtIt\\(St∪S∗)‖ 2 2\n≤ L 2 ‖θt+1It − θ t It +\nη′ L · gtIt‖ 2 2 − L 2 ‖θt+1R − θ t R + η′ L gtR‖22\n= L\n2 ‖θt+1It\\R − θ t It\\R +\nη′ L · gtIt\\R‖ 2 2. (16)\nWe can bound the size of It\\R as |It\\R| ≤ |St+1|+|(St\\St+1)\\R|+|S∗| ≤ s+|(St+1∩S∗)\\St|+s∗ ≤ s+ 2s∗. Also, since St+1 ⊆ (It\\R), we have θt+1It\\R = Ps(θ t It\\R − η′ L g t It\\R).\nUsing the above observation with (16) and Lemma 1, we get:\nL 2 ‖θt+1It − θ t It +\nη′ L · gtIt‖ 2 2 − (η′)2 2L ‖gtIt\\(St∪S∗)‖ 2 2\n≤ L 2 · |I t\\R| − s |It\\R| − s∗ ‖θ∗It\\R − θ t It\\R + η′ L · gtIt\\R‖ 2 2,\nζ1 ≤ L 2 · 2s\n∗\ns+ s∗ ‖θ∗It − θ t It +\nη′ L · gtIt‖ 2 2,\n= 2s∗ s+ s∗ · ( η′〈θ∗ − θt, gt〉+ L 2 ‖θ∗ − θt‖22 + (η′)2 2L ‖gtIt‖ 2 2 ) ,\nζ2 ≤ 2s\n∗ s+ s∗ · ( η′f(θ∗)− η′f(θt) + L− η ′α 2 ‖θ∗ − θt‖22 + (η′)2 2L ‖gtIt‖ 2 2 ) , (17)\nwhere the inequality ζ1 follows by |It\\R| ≤ s+ 2s∗ as shown earlier and the observation that x−ax−b is a positive and increasing function on the interval x ≥ a if a ≥ b ≥ 0. Note that since we have St+1 ⊆ (It\\R), we get |It\\R| ≥ s. The inequality ζ2 follows by using RSC.\nUsing (14), (17), and using St+1\\(St ∪ S∗) ⊆ (St+1 ∪ St), we get:\nf(θt+1)− f(θt) ≤ 2s ∗ s+ s∗ · ( η′f(θ∗)− η′f(θt) + L− η ′α 2 ‖θ∗ − θt‖22 + (η′)2 2L ‖gtIt‖ 2 2 ) − (η ′)2\n2L ‖gtSt∪S∗‖ 2 2 − η′(1− η′) 2L ‖gtSt+1\\(St∪S∗)‖ 2 2. (18)\nWe now set η′ = 2/3 as per our earlier choice and set s = 32 ( L α )2 s∗, so that we have 2s ∗\ns+s∗ ≤ α2\n16L(L−η′α) . Since L ≥ α, we also have α2 16L(L−η′α) ≤ 3 16 . Using these inequalities, we now rearrange the terms in (18) above.\nf(θt+1)− f(θt) ≤ 2s ∗\ns+ s∗ · η′ ·\n( f(θ∗)− f(θt) ) + α2\n32L ‖θ∗ − θt‖22 +\n1\n24L ‖gtIt‖ 2 2\n− 2 9L ‖gtSt∪S∗‖ 2 2 − 1 9L ‖gtSt+1\\(St∪S∗)‖ 2 2. (19)\nSplitting ‖gtIt‖ 2 2 = ‖gtSt∪S∗‖ 2 2 + ‖gtSt+1\\(St∪S∗)‖ 2 2 gives us\nf(θt+1)− f(θt) ≤ 2s ∗\ns+ s∗ · η′ ·\n( f(θ∗)− f(θt) ) − 1\n2L\n( 13\n36 ‖gtSt∪S∗‖ 2 2 −\nα2 16 ‖θ∗ − θt‖22 ) − 1 2L · ( 4 9 − 1 12 ) ‖gtSt+1\\(St∪S∗)‖ 2 2,\n≤ 2s ∗\ns+ s∗ · η′ ·\n( f(θ∗)− f(θt) ) − 13\n72L\n( ‖gtSt∪S∗‖ 2 2 − α2\n4 ‖θ∗ − θt‖22 ) ≤ 2s ∗\ns+ s∗ · η′ ·\n( f(θ∗)− f(θt) ) − α\n12L\n( f(θt)− f(θ∗) ) , (20)\nwhere the last inequality above follows using Lemma 6. The result now follows by observing that 2s∗ s+s∗ ≥ 0. Lemma 6. ( ‖gtSt∪S∗‖ 2 2 − α2\n4 ‖θ∗ − θt‖22\n) ≥ α 2 · ( f(θt)− f(θ∗) ) .\nProof. Using the RSC property, we have:\nf(θt)− f(θ∗) ≤ 〈gt,θt − θ∗〉 − α 2 ‖θ∗ − θt‖22\n= 〈gtSt∪S∗ ,θ t St∪S∗ − θ ∗ St∪S∗〉 −\nα 2 ‖θ∗ − θt‖22,\n≤ ‖gtSt∪S∗‖2‖θ t − θ∗‖2 −\nα 2 ‖θ∗ − θt‖22. (21)\nNow,\n‖gtSt∪S∗‖ 2 2 −\nα2\n4 ‖θ∗ − θt‖22 =\n( ‖gtSt∪S∗‖2 − α\n2 ‖θ∗ − θt‖2\n)( ‖gtSt∪S∗‖2 + α\n2 ‖θ∗ − θt‖2\n) ,\n≥ ( f(θt)− f(θ∗) ) ‖θt − θ∗‖2 · ( ‖gtSt∪S∗‖2 + α 2 ‖θ∗ − θt‖2 ) ≥ α\n2 · ( f(θt)− f(θ∗) ) , (22)\nwhere the first inequality above follows from (21)."
    }, {
      "heading" : "B Proofs for Section 4",
      "text" : "Proof of Theorem 3. Let θ∗ be the empirical loss minimizer over the set of s-sparse vectors. Then invoking Theorem 1 with f = L(·;Z1:n), we get\nL(θτ , Z1:n)− ≤ L(θ∗, Z1:n) ≤ L(θ̄, Z1:n)\n≤ L(θτ ;Z1:n) + 〈∇L(θ̄;Z1:n), (θ̄ − θτ )〉 − αs+s∗\n2 ‖θ̄ − θτ‖22\nwhere the 2nd inequality is by definition of θ∗ and 3rd is by RSC (since θ∗,θτ are s∗, s sparse). Duality gives us the upper bound\n〈∇L(θ̄;Z1:n), (θ̄ − θτ )〉 ≤ ‖∇L(θ̄;Z1:n)‖∞‖θ̄ − θτ‖1 ≤ √ s+ s∗‖∇L(θ̄;Z1:n)‖∞‖θ̄ − θτ‖2\nCombining the last two inequalities and rearranging gives a quadratic inequality in ‖θ̄ − θτ‖2:\nαs+s∗\n2 ‖θ̄ − θτ‖22 −\n√ s+ s∗‖∇L(θ̄;Z1:n)‖∞‖θ̄ − θτ‖2 − ≤ 0\nthat immediately yields the result."
    }, {
      "heading" : "C Proofs for Section 5",
      "text" : "Proof of Lemma 3. We will start by proving a more general result of which the claimed result will be a corollary. More specifically, we shall prove that for any γ ≥ 1α , we have\n2γ(f(θt)− f(θ∗)) ≤ 2γ ( f(θt)− f(θ∗) + α 2 · ( 1− 1 αγ ) ‖θt − θ∗‖22 ) ≤ γ2‖gtSt∪S∗‖ 2 2 − ‖θtSt\\S∗‖ 2 2,\nSetting γ = 1α will yield the claimed result. It is easy to see that the following inequality holds trivially since γ ≥ 1α\n2γ(f(θt)− f(θ∗)) ≤ 2γ ( f(θt)− f(θ∗) + α 2 · ( 1− 1 αγ ) ‖θt − θ∗‖22 ) .\nFor the second inequality, we first use the RSC condition to obtain:\nf(θ∗)− f(θt) ≥ 〈θ∗ − θt, gt〉+ α 2 ‖θt − θ∗‖22.\nNow let MDt = S ∗\\St be the set of true support elements missing from θt and FAt = St\\S∗ be the set of incorrect elements included in the support of θt. Since θt is obtained by a “fully corrective” process (recall θt = arg minθ,supp(θ)⊆St f(θ)), we have g t St = 0. Thus 〈θ ∗−θt, gt〉 = 〈θ∗MDt , g t MDt 〉.\nPutting this into the above expansion gives\n〈θ∗MDt , g t MDt〉 ≤ f(θ ∗)− f(θt)− α 2 ‖θt − θ∗‖22 (23)\nWe now present some simple inequalities that will help us get our desired bounds. Firstly, we have\n‖θ∗MDt + γg t MDt‖ 2 2 = ‖θ∗MDt‖ 2 2 + γ 2‖gtMDt‖ 2 2 + 2γ〈θ∗MDt , g t MDt〉 ≥ 0, (24)\nsince the first expression is a norm. Next, since MDt ∩ FAt = ∅, we have\n‖θ∗ − θt‖22 ≥ ‖θ∗MDt‖ 2 2 + ‖θtFAt‖ 2 2. (25)\nPutting equations 23 and 24, we have: 2γ ( f(θt)− f(θ∗) + α\n2 ‖θt − θ∗‖22\n) ≤ ‖θ∗MDt‖ 2 2 + γ 2‖gtMDt‖ 2 2. (26)\nNow, using (25), we get:\n2γ ( f(θt)− f(θ∗) + α\n2\n( 1− 1\nαγ\n) ‖θt − θ∗‖22 ) ≤ γ2‖gtMDt‖ 2 2 − ‖θtFAt‖ 2 2\nWe finish off the proof by noticing that since gtSt = 0, we have ‖g t MDt ‖22 = ‖gtSt∪S∗‖ 2 2\nProof of Theorem 4. Let ztSt = θ t St , z t Zt\\St = − 1 Lg t Zt\\St , and z t Zt\n= 0. Then, using the RSS property, we have:\nf(zt)− f(θt) ≤ 〈zt − θt, gt〉+ L 2 ‖zt − θt‖22,\nζ1 ≤ − 1\nL ‖gtZt\\St‖ 2 2 +\nL 2 ‖ztZt\\St‖ 2 2,\nζ2 = − 1\n2L · ‖gtZt\\St‖ 2 2,\nζ3 ≤ − 1\n2L · ‖gtS∗\\St‖ 2 2,\nζ4 ≤ −α L · ( f(θt)− f(θ∗) ) , (27)\nwhere ζ1 follows by observing g t St = 0, and S t ⊆ Zt. ζ2 follows by ztZt\\St = − 1 Lg t Zt\\St . ζ3 follows by ` ≥ s∗, and Zt\\St are the ` largest elements of |gtZt\\St |. Now, using Lemma 4 and (27) along with f(θt+1) ≤ f(θ̃t) and f(βt) ≤ f(zt), we have:\nf(θt+1)− f(θ∗) ≤ (\n1− α L\n) · ( 1 + L\nα · ` s+ `− s∗\n) · ( f(θt)− f(θ∗) ) . (28)\nTheorem now follows by using the above equation with the assumption that s+ `− s∗ ≥ 4L2·` α2 .\nProof of Theorem 5. Using RSS property:\nf(vt)− f(θt) ≤ 〈vt − θt, gt〉+ L 2 ‖vt − θt‖22,\nζ1 ≤ −η‖gtSt+1\\St‖ 2 2 + L\n2 (‖vtSt+1\\St‖ 2 2 + ‖θtSt\\St+1‖ 2),\nζ2 ≤ −η‖gtSt+1\\St‖ 2 2 + L‖vtSt+1\\St‖ 2 2,\nζ3 = − (1− η · L) · η · ‖gtSt+1\\St‖ 2 2, (29)\nwhere ζ1 follows by observing that g t St = 0 and v t St+1\\St = −ηg t St+1\\St . ζ2 follows by the property of PHT operator which ensures that each element of vtSt+1\\St is bigger than θ t St\\St+1 and by using |St+1\\St| = |St\\St+1|. ζ3 follows by using vtSt+1\\St = −ηg t St+1\\St .\nSimilar to the analysis given in [17], we divide the analysis in three mutually exclusive cases. The lemma then follows by combining (29) with the case-by-case analyses below and observing that f(θt+1) ≤ f(vt) because of the fully corrective step.\nCase 1: |St+1\\St| = ` < |S∗\\St|. In this case, As vtSt+1\\St is obtained by selecting |S t+1\\St|\nlargest elements of zt St∪bott . Hence,\nη2‖gtSt+1\\St‖ 2 2 ≥ min ( 1, |St+1\\St| |S∗\\St| ) ‖ztS∗\\St‖ 2 2\n= η2 min ( 1, |St+1\\St| |S∗\\St| ) ‖gtS∗\\St‖ 2 2\n= η2 min ( 1, |St+1\\St| |S∗\\St| ) ‖gtS∗∪St‖ 2 2, (30)\nsince gtSt = 0. Now, using the fact that |S t+1\\St| = `, |S∗\\St| ≤ s∗, and using Lemma 3, we have:\n‖gtSt+1\\St‖ 2 2 ≥ 2α ·min\n( 1, `\ns∗\n)( f(θt)− f(θ∗) ) . (31)\nCase 2: |St+1\\St| < `, |St+1\\St| ≤ |S∗\\St|. In this case, each element of ztSt+1∩St is larger than each element of ztS∗\\(St+1∪St), else that element of S\n∗\\(St+1 ∪ St) would have been selected. That is,\n‖ztS∗\\(St+1∪St)‖ 2 2 |S∗\\(St+1 ∪ St)| ≤ ‖zt(St+1∩St)\\S∗‖ 2 2 |(St+1 ∩ St)\\S∗| .\nUsing ztS∗\\St = −ηg t S∗\\St , z t St = θ t St and (S t+1 ∩ St)\\S∗ ⊆ St\\S∗, we have:\nη2‖gtS∗\\(St∪St+1)‖ 2 2 ≤\ns∗\ns− `− s∗ ‖θtSt\\S∗‖ 2 2, (32)\nwhere the bound on |S ∗\\(St+1∪St)| |(St+1∩St)\\S∗| follows by observing |S ∗\\(St+1 ∪ St)| ≤ s∗ and |(St+1 ∩ St)\\S∗| ≥ s − ` − s∗. Using (32) and the fact that each element of vt(St+1\\St)∩S∗ is selected from the largest |(St+1\\St) ∩ S∗| elements of −η · gt(St+1\\St)∩S∗ we have:\nη2‖gtS∗\\St‖ 2 2 ≤ η2 ( ‖gt(St+1\\St)∩S∗‖ 2 2 + ‖gtS∗\\(St+1∪St)‖ 2 2 ) ≤ ( η2‖gt(St+1\\St)∩S∗‖ 2 2 +\ns∗\ns− `− s∗ ‖θtSt\\S∗‖ 2 2\n) .\n(33)\nUsing the above equation and Lemma 3, we have:\n2\nα\n( f(θt)− f(θ∗) ) ≤ 1 α2 ‖gt(St+1\\St)∩S∗‖ 2 2 +\n( 1\nα2η2 · s\n∗ s− `− s∗ − 1 ) ‖θtSt\\S∗‖ 2 2, (34)\nUsing s ≥ 4 ( L α )2 s∗, we have:\n‖gtSt+1\\St‖ 2 2 ≥ 2α\n( f(θt)− f(θ∗) ) . (35)\nCase 3: |St+1\\St| ≥ |S∗\\St|. Now, as vtSt+1\\St is obtained by selecting |S t+1\\St| largest\nelements of zt St∪bott . Hence, using Lemma 3, we have:\n‖vtSt+1\\St‖ 2 2 ≥ ‖ztS∗\\St‖ 2 2 = η 2‖gtS∗\\St‖ 2 2 ≥ 2η2 · α ·\n( f(θt)− f(θ∗) ) . (36)\nThe lemma now follows by combining (29), (31), (35), and (36)"
    }, {
      "heading" : "D Supplementary Experimental Results",
      "text" : "Below we present plots that were not included in the main text.\n0 0.1 0.2 0.3 0.4 0\n20\n40\n60\n80\nNoise level (sigma)\nS up\npo rt\nR ec\nov er\ny E\nrr or\nOMPR CoSaMP L1 SP\n(a)\n0.5 1 1.5 2 2.5\nx 104\n0\n50\n100\n150\n200\nDimensionality (p)\nR un\ntim e\n(s ec\n)\nOMPR CoSaMP L1 SP\n(b)\n0 100 200 300 400 500 10−3 10−2\n100\n102\n104\nSparsity (s*)\nR un\ntim e\n(s ec\n)\nOMPR CoSaMP L1 SP\n(c)"
    } ],
    "references" : [ {
      "title" : "Statistics for high-dimensional data: methods, theory and applications",
      "author" : [ "Peter Bühlmann", "Sara Van De Geer" ],
      "venue" : null,
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2011
    }, {
      "title" : "Estimation of (near) low-rank matrices with noise and high-dimensional scaling",
      "author" : [ "Sahand Negahban", "Martin J Wainwright" ],
      "venue" : "The Annals of Statistics,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2011
    }, {
      "title" : "Minimax rates of estimation for high-dimensional linear regression over `q-balls",
      "author" : [ "Garvesh Raskutti", "Martin J Wainwright", "Bin Yu" ],
      "venue" : "Information Theory, IEEE Transactions on,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2011
    }, {
      "title" : "Estimation of high-dimensional low-rank matrices",
      "author" : [ "Angelika Rohde", "Alexandre B Tsybakov" ],
      "venue" : "The Annals of Statistics,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2011
    }, {
      "title" : "A unified framework for high-dimensional analysis of M-estimators with decomposable regularizers",
      "author" : [ "Sahand N Negahban", "Pradeep Ravikumar", "Martin J Wainwright", "Bin Yu" ],
      "venue" : "Statistical Science,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2012
    }, {
      "title" : "Sparse approximate solutions to linear systems",
      "author" : [ "Balas Kausik Natarajan" ],
      "venue" : "SIAM Journal on Computing,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 1995
    }, {
      "title" : "Forward-backward greedy algorithms for general convex smooth functions over a cardinality constraint",
      "author" : [ "Ji Liu", "Ryohei Fujimaki", "Jieping Ye" ],
      "venue" : "In Proceedings of The 31st International Conference on Machine Learning,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2014
    }, {
      "title" : "On learning discrete graphical models using greedy methods",
      "author" : [ "Ali Jalali", "Christopher C Johnson", "Pradeep D Ravikumar" ],
      "venue" : null,
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 1935
    }, {
      "title" : "Trading accuracy for sparsity in optimization problems with sparsity constraints",
      "author" : [ "Shai Shalev-Shwartz", "Nathan Srebro", "Tong Zhang" ],
      "venue" : "SIAM Journal on Optimization,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2010
    }, {
      "title" : "Introductory lectures on convex optimization: A basic course, volume 87 of Applied Optimization",
      "author" : [ "Yurii Nesterov" ],
      "venue" : null,
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2004
    }, {
      "title" : "Regularized M-estimators with nonconvexity: Statistical and algorithmic theory for local optima",
      "author" : [ "P. Loh", "M.J. Wainwright" ],
      "venue" : null,
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2013
    }, {
      "title" : "Iterative hard thresholding for compressed sensing",
      "author" : [ "Thomas Blumensath", "Mike E. Davies" ],
      "venue" : "Applied and Computational Harmonic Analysis,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2009
    }, {
      "title" : "Gradient descent with sparsification: an iterative algorithm for sparse recovery with restricted isometry property",
      "author" : [ "Rahul Garg", "Rohit Khandekar" ],
      "venue" : "In ICML,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2009
    }, {
      "title" : "Hard thresholding pursuit: an algorithm for compressive sensing",
      "author" : [ "Simon Foucart" ],
      "venue" : "SIAM J. on Num. Anal.,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2011
    }, {
      "title" : "CoSaMP: Iterative Signal Recovery from Incomplete and Inaccurate Samples",
      "author" : [ "Deanna Needell", "Joel A. Tropp" ],
      "venue" : "Appl. Comput. Harmon. Anal.,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2008
    }, {
      "title" : "Subspace pursuit for compressive sensing signal reconstruction",
      "author" : [ "Wei Dai", "Olgica Milenkovic" ],
      "venue" : "IEEE Trans. Inf. Theory,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2009
    }, {
      "title" : "Orthogonal matching pursuit with replacement",
      "author" : [ "Prateek Jain", "Ambuj Tewari", "Inderjit S. Dhillon" ],
      "venue" : "In Annual Conference on Neural Information Processing Systems,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2011
    }, {
      "title" : "Greedy sparsity-constrained optimization",
      "author" : [ "Sohail Bahmani", "Bhiksha Raj", "Petros T Boufounos" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2013
    }, {
      "title" : "Gradient hard thresholding pursuit for sparsityconstrained optimization",
      "author" : [ "Xiaotong Yuan", "Ping Li", "Tong Zhang" ],
      "venue" : "In Proceedings of The 31st International Conference on Machine Learning,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2014
    }, {
      "title" : "Lower bounds on the performance of polynomial-time algorithms for sparse linear regression",
      "author" : [ "Yuchen Zhang", "Martin J. Wainwright", "Michael I. Jordan" ],
      "venue" : null,
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 1918
    }, {
      "title" : "High-dimension regression with noisy and missing data: Provable guarantees with non-convexity",
      "author" : [ "P. Loh", "M.J. Wainwright" ],
      "venue" : "Annals of Statistics,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2012
    }, {
      "title" : "Fast global convergence of gradient methods for high-dimensional statistical recovery",
      "author" : [ "Alekh Agarwal", "Sahand N. Negahban", "Martin J. Wainwright" ],
      "venue" : "Annals of Statistics,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2012
    }, {
      "title" : "Graphical Model Structure Learning with L1-Regularization",
      "author" : [ "Mark Schmidt" ],
      "venue" : "PhD thesis, University of British Columbia,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2010
    }, {
      "title" : "Adaptive Forward-Backward Greedy Algorithm for Learning Sparse Representations",
      "author" : [ "Tong Zhang" ],
      "venue" : "IEEE Trans. Inf. Theory,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2011
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Under these assumptions, several works (for example, see [1, 2, 3, 4, 5]) have established that consistent estimation is information theoretically possible in the “n p” regime as well.",
      "startOffset" : 57,
      "endOffset" : 72
    }, {
      "referenceID" : 1,
      "context" : "Under these assumptions, several works (for example, see [1, 2, 3, 4, 5]) have established that consistent estimation is information theoretically possible in the “n p” regime as well.",
      "startOffset" : 57,
      "endOffset" : 72
    }, {
      "referenceID" : 2,
      "context" : "Under these assumptions, several works (for example, see [1, 2, 3, 4, 5]) have established that consistent estimation is information theoretically possible in the “n p” regime as well.",
      "startOffset" : 57,
      "endOffset" : 72
    }, {
      "referenceID" : 3,
      "context" : "Under these assumptions, several works (for example, see [1, 2, 3, 4, 5]) have established that consistent estimation is information theoretically possible in the “n p” regime as well.",
      "startOffset" : 57,
      "endOffset" : 72
    }, {
      "referenceID" : 4,
      "context" : "Under these assumptions, several works (for example, see [1, 2, 3, 4, 5]) have established that consistent estimation is information theoretically possible in the “n p” regime as well.",
      "startOffset" : 57,
      "endOffset" : 72
    }, {
      "referenceID" : 5,
      "context" : "Examples include sparse regression which requires loss minimization with sparsity constraints and low-rank regression which requires dealing with rank constraints which are not efficiently solvable in general [6].",
      "startOffset" : 209,
      "endOffset" : 212
    }, {
      "referenceID" : 4,
      "context" : "The estimation routines proposed in these works typically make use of convex relaxations [5] or greedy methods [7, 8, 9] which do not suffer from infeasibility issues.",
      "startOffset" : 89,
      "endOffset" : 92
    }, {
      "referenceID" : 6,
      "context" : "The estimation routines proposed in these works typically make use of convex relaxations [5] or greedy methods [7, 8, 9] which do not suffer from infeasibility issues.",
      "startOffset" : 111,
      "endOffset" : 120
    }, {
      "referenceID" : 7,
      "context" : "The estimation routines proposed in these works typically make use of convex relaxations [5] or greedy methods [7, 8, 9] which do not suffer from infeasibility issues.",
      "startOffset" : 111,
      "endOffset" : 120
    }, {
      "referenceID" : 8,
      "context" : "The estimation routines proposed in these works typically make use of convex relaxations [5] or greedy methods [7, 8, 9] which do not suffer from infeasibility issues.",
      "startOffset" : 111,
      "endOffset" : 120
    }, {
      "referenceID" : 9,
      "context" : "[10] do not apply to these techniques due to the non-convex structure of the problem.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 10,
      "context" : "An exception to this is the recent work [11] that demonstrates that PGD with non-convex regularization can offer consistent estimates for certain high-dimensional problems.",
      "startOffset" : 40,
      "endOffset" : 44
    }, {
      "referenceID" : 10,
      "context" : "However, the work in [11] is only able to analyze penalties such as SCAD, MCP and capped L1.",
      "startOffset" : 21,
      "endOffset" : 25
    }, {
      "referenceID" : 11,
      "context" : "As noted above, PGD/IHT-style methods have been very popular in literature for sparse recovery and several algorithms including Iterative Hard Thresholding (IHT) [12] or GraDeS [13], Hard Thresholding Pursuit (HTP) [14], CoSaMP [15], Subspace Pursuit (SP) [16], and OMPR(`) [17] have been proposed.",
      "startOffset" : 162,
      "endOffset" : 166
    }, {
      "referenceID" : 12,
      "context" : "As noted above, PGD/IHT-style methods have been very popular in literature for sparse recovery and several algorithms including Iterative Hard Thresholding (IHT) [12] or GraDeS [13], Hard Thresholding Pursuit (HTP) [14], CoSaMP [15], Subspace Pursuit (SP) [16], and OMPR(`) [17] have been proposed.",
      "startOffset" : 177,
      "endOffset" : 181
    }, {
      "referenceID" : 13,
      "context" : "As noted above, PGD/IHT-style methods have been very popular in literature for sparse recovery and several algorithms including Iterative Hard Thresholding (IHT) [12] or GraDeS [13], Hard Thresholding Pursuit (HTP) [14], CoSaMP [15], Subspace Pursuit (SP) [16], and OMPR(`) [17] have been proposed.",
      "startOffset" : 215,
      "endOffset" : 219
    }, {
      "referenceID" : 14,
      "context" : "As noted above, PGD/IHT-style methods have been very popular in literature for sparse recovery and several algorithms including Iterative Hard Thresholding (IHT) [12] or GraDeS [13], Hard Thresholding Pursuit (HTP) [14], CoSaMP [15], Subspace Pursuit (SP) [16], and OMPR(`) [17] have been proposed.",
      "startOffset" : 228,
      "endOffset" : 232
    }, {
      "referenceID" : 15,
      "context" : "As noted above, PGD/IHT-style methods have been very popular in literature for sparse recovery and several algorithms including Iterative Hard Thresholding (IHT) [12] or GraDeS [13], Hard Thresholding Pursuit (HTP) [14], CoSaMP [15], Subspace Pursuit (SP) [16], and OMPR(`) [17] have been proposed.",
      "startOffset" : 256,
      "endOffset" : 260
    }, {
      "referenceID" : 16,
      "context" : "As noted above, PGD/IHT-style methods have been very popular in literature for sparse recovery and several algorithms including Iterative Hard Thresholding (IHT) [12] or GraDeS [13], Hard Thresholding Pursuit (HTP) [14], CoSaMP [15], Subspace Pursuit (SP) [16], and OMPR(`) [17] have been proposed.",
      "startOffset" : 274,
      "endOffset" : 278
    }, {
      "referenceID" : 16,
      "context" : "The best known such constant is due to the work of [17] that requires a bound on the RIP constant δ2k ≤ 0.",
      "startOffset" : 51,
      "endOffset" : 55
    }, {
      "referenceID" : 17,
      "context" : "Although recent attempts have been made to extend this to general differentiable objectives [18, 19], the results continue to require that the restricted condition number be less than a universal constant and remain unsatisfactory in a statistical setting.",
      "startOffset" : 92,
      "endOffset" : 100
    }, {
      "referenceID" : 18,
      "context" : "Although recent attempts have been made to extend this to general differentiable objectives [18, 19], the results continue to require that the restricted condition number be less than a universal constant and remain unsatisfactory in a statistical setting.",
      "startOffset" : 92,
      "endOffset" : 100
    }, {
      "referenceID" : 19,
      "context" : "Our bounds are tight, achieve known minmax lower bounds [20], and hold for arbitrary differentiable, possibly even non-convex functions.",
      "startOffset" : 56,
      "endOffset" : 60
    }, {
      "referenceID" : 20,
      "context" : "Our results effortlessly extend to the noisy setting as a corollary and give bounds similar to those of [21] that relies on solving an L1 regularized problem.",
      "startOffset" : 104,
      "endOffset" : 108
    }, {
      "referenceID" : 20,
      "context" : "Two models of noise are popular in literature [21]: a) (additive noise) X̃i = Xi+Wi where Wi ∼ N (0,ΣW ), and b) (missing data) X̃ is an R∪{?}-valued matrix obtained by independently, with probability ν ∈ [0, 1), replacing each entry in X with ?.",
      "startOffset" : 46,
      "endOffset" : 50
    }, {
      "referenceID" : 16,
      "context" : "This result is similar to Lemma 1 of [17] but holds under RSC/RSS conditions (rather than the RIP condition as in [17]), as well as for the general loss functions.",
      "startOffset" : 37,
      "endOffset" : 41
    }, {
      "referenceID" : 16,
      "context" : "This result is similar to Lemma 1 of [17] but holds under RSC/RSS conditions (rather than the RIP condition as in [17]), as well as for the general loss functions.",
      "startOffset" : 114,
      "endOffset" : 118
    }, {
      "referenceID" : 16,
      "context" : "We now study Partial Hard Thresholding methods (PHT), a family of fully-corrective iterative methods introduced by [17].",
      "startOffset" : 115,
      "endOffset" : 119
    }, {
      "referenceID" : 13,
      "context" : "Algorithms: We studied a variety of hard-thresholding style algorithms including HTP [14], GraDeS [13] (or IHT [12]), CoSaMP [15], OMPR [17] and SP [16].",
      "startOffset" : 85,
      "endOffset" : 89
    }, {
      "referenceID" : 12,
      "context" : "Algorithms: We studied a variety of hard-thresholding style algorithms including HTP [14], GraDeS [13] (or IHT [12]), CoSaMP [15], OMPR [17] and SP [16].",
      "startOffset" : 98,
      "endOffset" : 102
    }, {
      "referenceID" : 11,
      "context" : "Algorithms: We studied a variety of hard-thresholding style algorithms including HTP [14], GraDeS [13] (or IHT [12]), CoSaMP [15], OMPR [17] and SP [16].",
      "startOffset" : 111,
      "endOffset" : 115
    }, {
      "referenceID" : 14,
      "context" : "Algorithms: We studied a variety of hard-thresholding style algorithms including HTP [14], GraDeS [13] (or IHT [12]), CoSaMP [15], OMPR [17] and SP [16].",
      "startOffset" : 125,
      "endOffset" : 129
    }, {
      "referenceID" : 16,
      "context" : "Algorithms: We studied a variety of hard-thresholding style algorithms including HTP [14], GraDeS [13] (or IHT [12]), CoSaMP [15], OMPR [17] and SP [16].",
      "startOffset" : 136,
      "endOffset" : 140
    }, {
      "referenceID" : 15,
      "context" : "Algorithms: We studied a variety of hard-thresholding style algorithms including HTP [14], GraDeS [13] (or IHT [12]), CoSaMP [15], OMPR [17] and SP [16].",
      "startOffset" : 148,
      "endOffset" : 152
    }, {
      "referenceID" : 22,
      "context" : "We compared them with a standard implementation of the L1 projected scaled sub-gradient technique [23] for the lasso problem and a greedy method FoBa [24] for the same.",
      "startOffset" : 98,
      "endOffset" : 102
    }, {
      "referenceID" : 23,
      "context" : "We compared them with a standard implementation of the L1 projected scaled sub-gradient technique [23] for the lasso problem and a greedy method FoBa [24] for the same.",
      "startOffset" : 150,
      "endOffset" : 154
    }, {
      "referenceID" : 4,
      "context" : "A unified analysis for general structures will probably create interesting connections with existing unified frameworks such as those based on decomposability [5] and atomic norms [25].",
      "startOffset" : 159,
      "endOffset" : 162
    } ],
    "year" : 2014,
    "abstractText" : "The use of M-estimators in generalized linear regression models in high dimensional settings requires risk minimization with hard L0 constraints. Of the known methods, the class of projected gradient descent (also known as iterative hard thresholding (IHT)) methods is known to offer the fastest and most scalable solutions. However, the current state-of-the-art is only able to analyze these methods in extremely restrictive settings which do not hold in high dimensional statistical models. In this work we bridge this gap by providing the first analysis for IHT-style methods in the high dimensional statistical setting. Our bounds are tight and match known minimax lower bounds. Our results rely on a general analysis framework that enables us to analyze several popular hard thresholding style algorithms (such as HTP, CoSaMP, SP) in the high dimensional regression setting. We also extend our analysis to a large family of “fully corrective methods” that includes two-stage and partial hard-thresholding algorithms. We show that our results hold for the problem of sparse regression, as well as low-rank matrix recovery.",
    "creator" : "LaTeX with hyperref package"
  }
}