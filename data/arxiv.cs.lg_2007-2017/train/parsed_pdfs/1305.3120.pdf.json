{
  "name" : "1305.3120.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Optimization with First-Order Surrogate Functions",
    "authors" : [ "Julien Mairal" ],
    "emails" : [ "julien.mairal@inria.fr" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n30 5.\n31 20\nv1 [\nst at\n.M L\n] 1\n4 M\nay 2\n01 3"
    }, {
      "heading" : "1. Introduction",
      "text" : "The principle of iteratively minimizing a majorizing surrogate of an objective function is often called majorization-minimization (Lange et al., 2000). Each iteration drives the objective function downhill, thus giving the hope of finding a local optimum. A large number of existing procedures can be interpreted from this point of view. This is for instance the case of gradient-based or proximal methods (see Nesterov, 2007; Beck & Teboulle, 2009; Wright et al., 2009), EM algorithms (see Neal & Hinton, 1998), DC programming (Horst & Thoai, 1999), boosting (Collins et al., 2002; Della Pietra et al., 2001), and some variational Bayes techniques (Wainwright & Jordan, 2008; Seeger & Wipf, 2010). The concept of “surrogate” has also been used successfully in the signal processing literature about sparse optimization (Daubechies et al., 2004; Gasso et al., 2009) and matrix factorization (Lee & Seung, 2001; Mairal et al., 2010).\nIn this paper, we are interested in generalizing the majorization-minimization principle. Our goal is both to discover new algorithms, and to draw connections\nProceedings of the 30 th International Conference on Machine Learning, Atlanta, Georgia, USA, 2013. JMLR: W&CP volume 28. Copyright 2013 by the author(s).\nwith existing methods. We focus our study on “firstorder surrogate functions”, which consist of approximating a possibly non-smooth objective function up to a smooth error. We present several schemes exploiting such surrogates, and analyze their convergence properties: asymptotic stationary point conditions for non-convex problems, and convergence rates for convex ones. More precisely, we successively study:\n• a generic majorization-minimization approach; • a randomized block coordinate descent algorithm (see Tseng & Yun, 2009; Shalev-Shwartz & Tewari, 2009; Nesterov, 2012; Richtárik & Takáč, 2012);\n• an accelerated variant for convex problems inspired by Nesterov (2004); Beck & Teboulle (2009);\n• a generalization of the “Frank-Wolfe” conditional gradient method (see Zhang, 2003; Harchaoui et al., 2013; Hazan & Kale, 2012; Zhang et al., 2012);\n• a new incremental scheme, which we call MISO.1\nWe present in this work a unified view for analyzing a large family of algorithms with simple convergence proofs and strong guarantees. In particular, all the above optimization methods except Frank-Wolfe have linear convergence rates for minimizing strongly convex objective functions. This is remarkable for MISO, the new incremental scheme derived from our framework; to the best of our knowledge, only two recent incremental algorithms share such a property: the stochastic average gradient method (SAG) of Le Roux et al. (2012), and the stochastic dual coordinate ascent method (SDCA) of Shalev-Schwartz & Zhang (2012). Our scheme MISO is inspired in part by these two works, but yields different update rules than SAG or SDCA.\nAfter we present and analyze the different optimization schemes, we conclude the paper with numerical experiments focusing on the scheme MISO. We show that in most cases MISO matches or outperforms cutting-edge solvers for large-scale ℓ2- and ℓ1-regularized logistic regression (Bradley et al., 2011; Beck & Teboulle, 2009; Le Roux et al., 2012; Fan et al., 2008; Bottou, 2010).\n1Minimization by Incremental Surrogate Optimization."
    }, {
      "heading" : "2. Basic Optimization Scheme",
      "text" : "Given a convex subset Θ of Rp and a continuous function f : Rp → R, we are interested in solving\nmin θ∈Θ f(θ),\nwhere we assume, to simplify, that f is bounded below. Our goal is to study the majorization-minimization scheme presented in Algorithm 1 and its variants. This procedure relies on the concept of surrogate functions, which are minimized instead of f at every iteration.2\nAlgorithm 1 Basic Scheme input θ0 ∈ Θ; N (number of iterations). 1: for n = 1, . . . , N do 2: Compute a surrogate function gn of f near θn−1; 3: Update solution: θn ∈ argminθ∈Θ gn(θ). 4: end for output θN (final estimate);\nFor this approach to be successful, we intuitively need surrogates that approximate well the objective f and that are easy to minimize. In this paper, we focus on “first-order surrogate functions” defined below, which will be shown to have “good” theoretical properties.\nDefinition 2.1 (First-Order Surrogate). A function g : Rp → R is a first-order surrogate of f near κ in Θ when the following conditions are satisfied:\n• Majorization: we have g(θ′) ≥ f(θ′) for all θ′ in argminθ∈Θ g(θ). When the more general condition g ≥ f holds, we say that g is a majorant function; • Smoothness: the approximation error h , g − f is differentiable, and its gradient is L-Lipschitz continuous. Moreover, we have h(κ) = 0 and ∇h(κ) = 0. We denote by SL(f, κ) the set of such surrogates, and by SL,ρ(f, κ) the subset of ρ-strongly convex surrogates.\nFirst-order surrogates have a few simple properties, which form the building block of our analyses:\nLemma 2.1 (Basic Properties - Key Lemma). Let g be in SL(f, κ) for some κ in Θ. Define h , g−f and let θ′ be in argminθ∈Θ g(θ). Then, for all θ in Θ,\n• |h(θ)| ≤ L2 ‖θ − κ‖22; • f(θ′) ≤ f(θ) + L2 ‖θ − κ‖22."
    }, {
      "heading" : "Assume that g is in SL,ρ(f, κ), then, for all θ in Θ,",
      "text" : "• f(θ′) + ρ2‖θ′ − θ‖22 ≤ f(θ) + L2 ‖θ − κ‖22. 2Note that this concept differs from the machine learning terminology, where a “surrogate” often denotes a fixed convex upper bound of the nonconvex (0−1)-loss.\nThe proof of this lemma is relatively simple but for space limitation reasons, all proofs in this paper are provided as supplemental material. With Lemma 2.1 in hand, we now study the properties of Algorithm 1."
    }, {
      "heading" : "2.1. Convergence Analysis",
      "text" : "For general non-convex problems, proving convergence to a global (or local) minimum is out of reach, and classical analyses study instead asymptotic stationary point conditions (see, e.g., Bertsekas, 1999). To do so, we make the mild assumption that for all θ, θ′ in Θ, the directional derivative ∇f(θ, θ′ − θ) of f at θ in the direction θ′− θ exists. A classical necessary first-order condition (see Borwein & Lewis, 2006) for θ to be a local minimum of f is to have ∇f(θ, θ′−θ) non-negative for all θ′ in Θ. This naturally leads us to consider the following asymptotic condition to assess the quality of a sequence (θn)n≥0 for non-convex problems:\nDefinition 2.2 (Asymptotic Stationary Point). A sequence (θn)n≥0 satisfies an asymptotic stationary point condition if\nlim inf n→+∞ inf θ∈Θ ∇f(θn, θ − θn) ‖θ − θn‖2 ≥ 0.\nIn particular, if f is differentiable on Rp and Θ = Rp, this condition implies limn→+∞ ‖∇f(θn)‖2 = 0.\nBuilding upon this definition, we now give a first convergence result about Algorithm 1.\nProposition 2.1 (Non-Convex Analysis). Assume that the surrogates gn from Algorithm 1 are in SL(f, θn−1) and are majorant or strongly convex. Then,(f(θn))n≥0 monotonically decreases and (θn)n≥0 satisfies an asymptotic stationary point condition.\nConvergence results for non-convex problems are by nature weak. This is not the case when f is convex. In the next proposition, we obtain convergence rates by following a proof technique from Nesterov (2007) originally designed for proximal gradient methods. Proposition 2.2 (Convex Analysis for SL(f, κ)). Assume that f is convex and that for some R > 0,\n‖θ− θ⋆‖2 ≤ R for all θ ∈ Θ s.t. f(θ) ≤ f(θ0), (1) where θ⋆ is a minimizer of f on Θ. When the surrogate gn in Algorithm 1 are in SL(f, θn−1), we have\nf(θn)− f⋆ ≤ 2LR2\nn+ 2 for all n ≥ 1,\nwhere f⋆ , f(θ⋆). Assume now that f is µ-strongly convex. Regardless of condition (1), we have\nf(θn)− f⋆ ≤ βn(f(θ0)− f⋆) for all n ≥ 1, where β , Lµ if µ > 2L or β , ( 1− µ4L ) otherwise.\nThe result of Proposition 2.2 is interesting in the sense that it provides sharp theoretical results without making strong assumption on the surrogate functions. The next proposition shows that slightly better rates can be obtained when the surrogates are strongly convex. Proposition 2.3 (Convex Analysis for SL,ρ(f, κ)). Assume that f is convex and let θ⋆ be a minimizer of f on Θ. When the surrogates gn of Algorithm 1 are in SL,ρ(f, θn−1) with ρ ≥ L, we have for all n ≥ 1,\nf(θn)− f⋆ ≤ L‖θ0 − θ⋆‖22\n2n .\nWhen f is µ-strongly convex, we have for all n ≥ 1, \n\n\n‖θn − θ⋆‖22 ≤ ( L ρ+µ\n)n\n‖θ0 − θ⋆‖22 f(θn)− f⋆ ≤ ( L ρ+µ )n−1 L‖θ0−θ⋆‖22 2 .\nNote that the condition ρ ≥ L is relatively strong; it can indeed be shown that f is necessarily (ρ−L)strongly convex if ρ>L, and convex if ρ=L. The fact that making stronger assumptions yields better convergence rates suggests that going beyond first-order surrogates could provide even sharper results. This is confirmed in the next proposition:\nProposition 2.4 (Second-Order Surrogates). Make similar assumptions as in Proposition 2.2, and also assume that the error functions hn , gn−f are twice differentiable, that their Hessians ∇2hn are M - Lipschitz, and that ∇2hn(θn−1) = 0 for all n. Then,\nf(θn)− f⋆ ≤ 9MR3\n2(n+ 3)2 for all n ≥ 1.\nIf f is µ-strongly convex, the convergence rate is superlinear with order 3/2.\nConsistently with this proposition, similar rates were obtained by Nesterov & Polyak (2006) for the Newton method with cubic regularization, which involve second-order surrogates. In the next section, we focus again on first-order surrogates, and present simple mechanisms to build them. The proofs of the different claims are provided in the supplemental material."
    }, {
      "heading" : "2.2. Examples of Surrogate Functions",
      "text" : ""
    }, {
      "heading" : "Lipschitz Gradient Surrogates.",
      "text" : "When f is differentiable and ∇f is L-Lipschitz, f admits the following majorant surrogate in S2L,L(f, κ):\ng : θ 7→ f(κ) +∇f(κ)⊤(θ − κ) + L 2 ‖θ − κ‖22.\nIn addition, when f is convex, g is in SL,L(f, κ), and when f is µ-strongly convex, g is in SL−µ,L(f, κ). Note also that minimizing g amounts to performing a classical classical gradient descent step θ′ ← κ− 1L∇f(κ)."
    }, {
      "heading" : "Proximal Gradient Surrogates.",
      "text" : "Assume that f splits into f = f1 + f2, where f1 is differentiable with a L-Lipschitz gradient. Then, f admits the following majorant surrogate in S2L(f, κ):\ng : θ 7→ f1(κ) +∇f1(κ)⊤(θ − κ) + L\n2 ‖θ − κ‖22 + f2(θ).\nThe approximation error g − f is indeed the same as in the previous paragraph and thus:\n• when f1 is convex, g is in SL(f, κ). If f2 is also convex, g is in SL,L(f, κ).\n• when f1 is µ-strongly convex, g is in SL−µ(f, κ). If f2 is also convex, g is in SL−µ,L(f, κ).\nMinimizing g amounts to performing a proximal gradient step (see Nesterov, 2007; Beck & Teboulle, 2009)."
    }, {
      "heading" : "DC Programming Surrogates.",
      "text" : "Assume that f = f1 + f2, where f2 is concave and differentiable with a L2-Lipschitz gradient. Then, the following function g is a majorant surrogate in SL2(f, κ):\ng : θ 7→ f1(θ) + f2(κ) +∇f2(κ)⊤(θ − κ).\nSuch a surrogate forms the root of DC- (difference of convex functions)-programming (see Horst & Thoai, 1999). It is also indirectly used in reweighted-ℓ1 algorithms (Candès et al., 2008) for minimizing on Rp+ a cost function of the form θ 7→ f1(θ)+λ ∑p i=1 log(θi+ε)."
    }, {
      "heading" : "Variational Surrogates.",
      "text" : "Let f be a real-valued function defined on Rp1 × Rp2 . Let Θ1 ⊆ Rp1 and Θ2 ⊆ Rp2 be two convex sets. Define f̃ as f̃(θ1) , minθ2∈Θ2 f(θ1, θ2) and assume that\n• θ1 7→ f(θ1, θ2) is differentiable for all θ2 in Θ2; • θ2 7→ ∇1f(θ1, θ2) is L-Lipschitz for all θ1 in Rp1 ;3 • θ1 7→ ∇1f(θ1, θ2) is L′-Lipschitz for all θ2 in Θ2; • θ2 7→ f(θ1, θ2) is µ-strongly convex for all θ1 in Rp1 .\nLet us fix κ1 in Θ1. Then, the following function is a majorant surrogate in S2L′′(f̃ , κ) for some L′′ > 0:\ng : θ1 7→ f(θ1, κ⋆2) with κ⋆2 , argmin θ2∈Θ2 f̃(κ1, θ2).\nWhen f is jointly convex in θ1 and θ2, f̃ is itself convex and we can choose L′′ = L′. Algorithm 1 becomes a block-coordinate descent procedure with two blocks."
    }, {
      "heading" : "Saddle Point Surrogates.",
      "text" : "Let us make the same assumptions as in the previous paragraph but with the following differences:\n3The notation ∇1 denotes the gradient w.r.t. θ1.\n• θ2 7→f(θ1, θ2) is µ-strongly concave for all θ1 in Rp1 ; • θ1 7→f(θ1, θ2) is convex for all θ2 in Θ2; • f̃(θ1) , maxθ2∈Θ2 f(θ1, θ2).\nThen, f̃ is convex and the function below is a majorant surrogate in S2L′′(f̃ , κ1):\ng : θ1 7→ f(θ1, κ⋆2) + L′′\n2 ‖θ1 − κ1‖22,\nwhere L′′ , max(2L2/µ, L′). When θ1 7→ f(θ1, θ2) is affine, we can instead choose L′′ , L2/µ."
    }, {
      "heading" : "Jensen Surrogates.",
      "text" : "Jensen’s inequality provides a natural mechanism to obtain surrogates for convex functions. Following the presentation of Lange et al. (2000), we consider a convex function f : R 7→ R, a vector x in Rp, and define f̃ : Rp → R as f̃(θ) , f(x⊤θ) for all θ. Let w be a weight vector in Rp+ such that ‖w‖1 = 1 and wi 6= 0 whenever xi 6=0. Then, we define for any κ in Rp\ng : θ 7→ p ∑\ni=1\nwif\n(\nxi wi (θi − κi) + x⊤κ ) ,\nWhen f is differentiable with an L-Lipschitz gradient, and wi , |xi|ν/‖x‖νν, then g is in SL′(f̃ , κ) with\n• L′ = L‖x‖2∞‖x‖0 for ν = 0; • L′ = L‖x‖∞‖x‖1 for ν = 1; • L′ = L‖x‖22 for ν = 2.\nAs far as we know, the convergence rates we provide when using such surrogates are new. We also note that Jensen surrogates have been successfully used in machine learning. For instance, Della Pietra et al. (2001) interpret boosting procedures under this point of view through the concept of auxiliary functions."
    }, {
      "heading" : "Quadratic Surrogates.",
      "text" : "When f is twice differentiable and admits a matrix H such that H−∇2f is always positive definite, the following function is a first-order majorant surrogate:\ng : θ 7→ f(κ) +∇f(κ)⊤(θ − κ) + 1 2 (θ − κ)⊤H(θ − κ).\nThe Lipschitz constant of ∇(g−f) is the largest eigenvalue of H − ∇2f(θ) over Θ. Such surrogates appear frequently in the statistics and machine learning literature (Böhning & Lindsay, 1988; Khan et al., 2010).\nWe have shown that there are many rules to build first-order surrogates. Choosing one instead of another mainly depends on how easy it is to build the surrogate (do we need to estimate an a priori unknown Lipschitz constant?), and on how cheaply it can be minimized."
    }, {
      "heading" : "3. Block Coordinate Scheme",
      "text" : "In this section, we introduce a block coordinate descent extension of Algorithm 1 under the assumptions that\n• Θ is separable—that is, it can be written as a Cartesian product Θ = Θ1 ×Θ2 × . . .×Θk;\n• the surrogates gn are separable into k components:\ngn(θ) = k ∑\ni=1\ngin(θ i) for θ = (θ1, . . . , θk) ∈ Θ.\nWe present a randomized procedure in Algorithm 2 following Tseng & Yun (2009); Shalev-Shwartz & Tewari (2009); Nesterov (2012); Richtárik & Takáč (2012).\nAlgorithm 2 Block Coordinate Descent Scheme\ninput θ0 = (θ 1 0 , . . . , θ k 0 ) ∈ Θ = (Θ1 × . . .×Θk); N .\n1: for n = 1, . . . , N do 2: Choose a separable surrogate gn of f near θn−1; 3: Randomly pick up one block ı̂n and update θ ı̂n n :\nθı̂nn ∈ argmin θı̂n∈Θı̂n g ı̂nn (θ ı̂n).\n4: end for output θN = (θ 1 N , . . . , θ k N ) (final estimate);\nAs before, we first study the convergence for nonconvex problems. The next proposition shows that similar guarantees as for Algorithm 1 can be obtained.\nProposition 3.1 (Non-Convex Analysis). Assume that the functions gn are majorant surrogates in SL(f, θn−1). Assume also that θ0 is the minimizer of a majorant surrogate function in SL(f, θ−1) for some θ−1 in Θ. Then, the conclusions of Proposition 2.1 hold with probability one.\nUnder convexity assumptions on f , the next two propositions give us expected convergence rates. Proposition 3.2 (Convex Analysis for SL(f, κ)). Make the same assumptions as in Proposition 2.2 and define δ , 1k . When the surrogate functions gn in Algorithm 2 are majorant and in SL(f, θn−1), the sequence (f(θn))n≥0 almost surely converges to f⋆ and\nE[f(θn)− f⋆] ≤ 2LR2\n2 + δ(n− n0) for all n ≥ n0,\nwhere n0 , ⌈ log ( 2(f(θ0)−f⋆) LR2 − 1 ) / log ( 1 1−δ )⌉ if f(θ0)− f⋆ > LR2 and n0 , 0 otherwise. Assume now that f is µ-strongly convex. Then, we have instead an expected linear convergence rate\nE[f(θn)− f⋆] ≤ ((1 − δ) + δβ)n(f(θ0)− f⋆),\nwhere β , Lµ if µ > 2L or β , ( 1− µ4L ) otherwise.\nProposition 3.3 (Convex Analysis for SL,ρ(f, κ)). Assume that f is convex. Define δ , 1k . Choose majorant surrogates gn in SL,ρ(f, θn−1) with ρ ≥ L, then (f(θn))n≥0 almost surely converges to f⋆ and we have\nE[f(θn)− f⋆] ≤ C0\n(1− δ) + δn for all n ≥ 1,\nwith C0 , (1−δ)(f(θ0)−f⋆)+ (1−δ)ρ+δL2 ‖θ0−θ⋆‖22. Assume now that f is µ-strongly convex, then we have an expected linear convergence rate \n\n\nL 2 E[‖θ⋆ − θn‖22] ≤ C0\n( (1 − δ) + δ Lρ+µ )n\nE[f(θn)− f⋆] ≤ C0δ ( (1 − δ) + δ Lρ+µ )n−1 .\nThe quantity δ= 1/k represents the probability for a block to be updated during an iteration. Note that updating all blocks (δ=1) gives the same results as in Section 2. Linear convergence for strongly convex objectives with block coordinate descent is classical since the works of Tseng & Yun (2009); Nesterov (2012). Results of the same nature have also been obtained by Richtárik & Takáč (2012) for composite functions."
    }, {
      "heading" : "4. Frank-Wolfe Scheme",
      "text" : "In this section, we show how to use surrogates to generalize the Frank-Wolfe method, an old convex optimization technique that has regained some popularity in machine learning (Zhang, 2003; Harchaoui et al., 2013; Hazan & Kale, 2012; Zhang et al., 2012). We present this approach in Algorithm 3.\nAlgorithm 3 Frank-Wolfe Scheme input θ0 ∈ Θ; N (number of iterations). 1: for n = 1, . . . , N do 2: Let gn be a majorant surrogate in SL,L(f, θn−1). 3: Compute a search direction:\nνn ∈ argmin θ∈Θ\n[ gn(θ)− L\n2 ‖θ − θn−1‖22\n]\n.\n4: Line search: α⋆,argmin α∈[0,1] gn(ανn+(1−α)θn−1). 5: Update solution: θn , α ⋆νn + (1− α⋆)θn−1.\n6: end for output θN (final estimate);\nWhen f is smooth and the “gradient Lipschitz based surrogates” from Section 2.2 are used, Algorithm 3 becomes the classical Frank-Wolfe method.4 Our point of view is however more general since it allows for example to use “proximal gradient surrogates”. The next proposition gives a convergence rate.\n4Note that the classical Frank-Wolfe algorithm performs in fact the line search over the function f and not gn.\nProposition 4.1 (Convex Analysis). Assume that f is convex and that Θ is bounded. Call R , maxθ1,θ2∈Θ ‖θ1 − θ2‖2 the diameter of Θ. Then, the sequence (f(θn))n≥0 provided by Algorithm 3 converges to the minimum f⋆ of f over Θ and\nf(θn)− f⋆ ≤ 2LR2\nn+ 2 for all n ≥ 1.\nOther extensions of Algorithm 3 can also easily be designed by using our framework. We present for instance in the supplemental material a randomized block Frank-Wolfe algorithm, revisiting the recent work of Lacoste-Julien et al. (2013)."
    }, {
      "heading" : "5. Accelerated Scheme",
      "text" : "A popular scheme for convex optimization is the accelerated proximal gradient method (Nesterov, 2007; Beck & Teboulle, 2009). By using surrogate functions, we exploit similar ideas in Algorithm 4. When using the “Lipschitz gradient surrogates” of Section 2.2, Algorithm 4 is exactly the scheme 2.2.19 of Nesterov (2004). When using the “proximal gradient surrogate” and when µ = 0, it is equivalent to the FISTA method of Beck & Teboulle (2009). Algorithm 4 consists of iteratively minimizing a surrogate computed at a point κn−1 extrapolated from θn−1 and θn−2. It results in better convergence rates, as shown in the next proposition by adapting a proof technique of Nesterov (2004).\nAlgorithm 4 Accelerated Scheme input θ0 ∈ Θ; N ; µ (strong convexity parameter); 1: Initialization: κ0 , θ0; a0 = 1; 2: for n = 1, . . . , N do 3: Choose a surrogate gn in SL,L+µ(f, κn−1); 4: Update solution: θn , argminθ∈Θ gn(θ); 5: Compute an ≥ 0 such that:\na2n = (1 − an)a2n−1 + µL+µan;\n6: Set βn , an−1(1−an−1)\na2 n−1 +an and update κ:\nκn , θn + βn(θn − θn−1);\n7: end for output θN (final estimate);\nProposition 5.1 (Convex Analysis). Assume that f is convex. When µ = 0, the sequence (θn)n≥0 provided by Algorithm 4 satisfies for all n ≥ 1,\nf(θn)− f⋆ ≤ 2L‖θ0 − θ⋆‖22\n(n+ 2)2 .\nWhen f is µ-strongly convex, we have instead a linear\nconvergence rate: for n ≥ 1,\nf(θn)− f⋆ ≤ ( 1− √ µ\nL+ µ\n)n−1 L‖θ0 − θ⋆‖22\n2 ."
    }, {
      "heading" : "6. Incremental Scheme",
      "text" : "This section is devoted to objective functions f that split into many components:\nf(θ) = 1\nT\nT ∑\nt=1\nf t(θ). (2)\nThe most classical method exploiting such a structure when f is smooth is probably the stochastic gradient descent (SGD) and its variants (see Bottou, 2010). It consists of drawing at iteration n an index t̂n and updating the solution as θn←θn−1−ηn∇f t̂n(θn−1) with a scalar ηn. Another popular algorithm is the stochastic mirror descent (see Juditsky & Nemirovski, 2011) for general non-smooth convex problems, a setting we do not consider in this paper since non-smooth functions do not always admit first-order surrogates.\nRecently, it was shown by Shalev-Schwartz & Zhang (2012) and Le Roux et al. (2012) that linear convergence rates could be obtained for strongly convex functions f t. The SAG algorithm of Le Roux et al. (2012) for smooth unconstrained optimization is an approximate gradient descent strategy, where an estimate of ∇f is incrementally updated at each iteration. The work of Shalev-Schwartz & Zhang (2012) for composite optimization is a dual coordinate ascent method called SDCA which performs incremental updates in the primal (2). Unlike SGD, both SAG and SDCA require storing information about past iterates.\nIn a different context, incremental EM algorithms have been proposed by Neal & Hinton (1998), where surrogates of a log-likelihood are incrementally updated. By using similar ideas, we present in Algorithm 5 a scheme for solving (2), which we call MISO. In the next propositions, we study its convergence properties.\nAlgorithm 5 Incremental Scheme MISO input θ0 ∈ Θ; N (number of iterations). 1: Choose surrogates gt0 of f\nt near θ0 for all t; 2: for n = 1, . . . , N do 3: Randomly pick up one index t̂n and choose a\nsurrogate gt̂nn of f t̂n near θn−1. Set gtn , g t n−1\nfor t 6= t̂n; 4: Update solution: θn ∈ argmin\nθ∈Θ 1 T ∑T t=1 g t n(θ).\n5: end for output θN (final estimate);\nProposition 6.1 (Non-Convex Analysis). Assume that the surrogates gt̂nn from Algorithm 5 are majorant and are in SL(f t̂n , θn−1). Then, the conclusions of Proposition 2.1 hold with probability one.\nProposition 6.2 (Convex Analysis). Assume that f is convex. Define f⋆ , minθ∈Θ f(θ) and δ, 1T . When the surrogates g t n in Algorithm 5 are majorant and in SL,ρ(f t, θn−1) with ρ≥L, we have\nE[f(θn)− f⋆] ≤ L‖θ⋆ − θ0‖22\n2δn for all n ≥ 1.\nAssume now that f is µ-strongly convex. For all n≥1, \n\n\nE[‖θ⋆−θn‖22] ≤ ( (1−δ)+δ Lρ+µ )n\n‖θ⋆ − θ0‖22 E[f(θn)−f⋆] ≤ ( (1−δ)+δ Lρ+µ )n−1 L‖θ⋆−θ0‖22 2 .\nInterestingly, the proof and the convergence rates of Proposition 6.2 are similar to those of the block coordinate scheme. For both schemes, the current iterate θn can be shown to be the minimizer of an approximate surrogate function which splits into different parts. Each iteration randomly picks up one part, and updates it. Like SAG or SDCA, we obtain linear convergence for strongly convex functions f , even though the upper bounds obtained for SAG and SDCA are better than ours.\nIt is also worth noticing that for smooth unconstrained problems, MISO and SAG yield different, but related, update rules. Assume for instance that “Lipschitz gradient surrogates” are used. At iteration n of MISO, each function gtn is a surrogate of f\nt near some κtn−1. The update rule of MISO can be shown to be θn ← 1 T ∑T t=1κ t n−1− 1TL ∑T t=1∇f t(κtn−1); in comparison, the update rule of SAG is θn←θn−1− 1TL ∑T\nt=1∇f t(κtn−1). The next section complements the theoretical analysis of the scheme MISO by numerical experiments and practical implementation heuristics."
    }, {
      "heading" : "7. Experiments",
      "text" : "In this section, we show that MISO is efficient for solving large-scale machine learning problems."
    }, {
      "heading" : "7.1. Experimental Setting",
      "text" : "We consider ℓ2- and ℓ1- logistic regression without intercept, and denote by m the number of samples and by p the number of features. The corresponding optimization problem can be written\nmin θ∈Rp\n1\nm\nm ∑\nt=1\nlog(1 + e−ytx t⊤θ) + λψ(θ), (3)\nwhere the regularizer ψ is either the ℓ1- or squared ℓ2-norm. The yt’s are in {−1,+1} and the xt’s are vectors in Rp with unit ℓ2-norm. We use four classical datasets described in the following table:\nname m p storage size (GB) alpha 250 000 500 dense 1 rcv1 781 265 47 152 sparse 0.95 covtype 581 012 54 dense 0.11 ocr 2 500 000 1 155 dense 23.1\nThree datasets, alpha, rcv1 and ocr were obtained from the 2008 Pascal large scale learning challenge.5 The dataset covtype is available from the LIBSVM website.6 We have chosen to test several software packages including LIBLINEAR 1.93 (Fan et al., 2008), the ASGD and SGD implementations of L. Bottou (version 2)7, an implementation of SAG kindly provided to us by the authors of Le Roux et al. (2012), the FISTA method of Beck & Teboulle (2009) implemented in the SPAMS toolbox8, and SHOTGUN (Bradley et al., 2011). All these softwares are coded in C++ and were compiled using gcc. Experiments were run on a single core of a 2.00GHz Intel Xeon CPU E5-2650 using 64GB of RAM, and all computations were done in double precision. All the timings reported do not include data loading into memory. Note that we could not run the softwares SPAMS, LIBLINEAR and SHOTGUN on the dataset ocr because of index overflow issues."
    }, {
      "heading" : "7.2. On Implementing MISO",
      "text" : "The objective function (3) splits into m components f t : θ 7→ log(1 + e−ytxt⊤θ) + λψ(θ). It is thus natural to consider the incremental scheme of Section 6 together with the proximal gradient surrogates of Section 2.2. Concretely, we build at iteration n of MISO a surrogate gt̂nn of f\nt̂n as follows: gt̂nn : θ 7→ lt̂n(θn−1)+ ∇lt̂n(θn−1)⊤(θ−θn−1)+L2 ‖θ−θn−1‖22+λψ(θ), where lt is the logistic function θ 7→ log(1 + e−ytxt⊤θ). After removing the dependency over n to simplify the notation, all the surrogates can be rewritten as gt : θ 7→ at + zt⊤θ+ L2 ‖θ‖22 +λψ(θ), where at is a constant and zt is a vector in Rp. Therefore, all surrogates can be “summarized” by the pair (at, z\nt), quantities which we keep into memory during the optimization. Then, finding the estimate θn amounts to minimizing a function of the form θ 7→ z̄⊤n θ + L2 ‖θ‖22 + λψ(θ), where z̄n is the average value of the quantities z\nt at iteration n. It is then easy to see that obtaining z̄n+1\n5http://largescale.ml.tu-berlin.de. 6http://www.csie.ntu.edu.tw/~cjlin/libsvm/. 7http://leon.bottou.org/projects/sgd. 8http://spams-devel.gforge.inria.fr/.\nfrom z̄n can be done in O(p) operations with the following update: z̄n+1 ← z̄n + (zt̂nnew − zt̂nold)/m. One issue is that building the surrogates gt requires choosing some constant L. An upper bound on the Lipschitz constants of the gradients ∇lt could be used here. However, we have observed that significantly faster convergence could be achieved by using a smaller value, probably because a local Lipschitz constant may be better adapted than a global one. By studying the proof of Proposition 6.2, we notice indeed that our convergence rates can be obtained without majorant surrogates, when we simply have: E[f t(θn)] ≤ E[gtn(θn)] for all t and n. This motivates the following heuristics:\n• MISO1: start by performing one pass over η=5% of the data to select a constant L′ yielding the smallest decrease of the objective, and set L = L′η;\n• MISO2: in addition to MISO1, check the inequalities f t̂n(θn−1)≤ gt̂nn−1(θn−1) during the optimization. After each pass over the data, if the rate of satisfied inequalities drops below 50%, double the value of L.\nFollowing these strategies, we have implemented the scheme MISO in C++. The resulting software package will be publicly released with an open source license."
    }, {
      "heading" : "7.3. ℓ2-Regularized Logistic Regression",
      "text" : "We compare LIBLINEAR, FISTA, SAG, ASGD, SGD, MISO1, MISO2 and MISO2 with T = 1000 blocks (grouping some observations into minibatches). LIBLINEAR was run using the option -s 0 -e 0.000001. The implementation of SAG includes a heuristic line search in the same spirit as MISO2, introduced by Le Roux et al. (2012). Every method was stopped after 50 passes over the data. We considered three regularization regimes, high (λ= 10−3), medium (λ= 10−5) and low (λ=10−7). We present in Figure 1 the values of the objective function during the optimization for the regime medium, both in terms of passes over the data and training time. The regimes low and high are provided as supplemental material only. Note that to reduce the memory load, we used a minibatch strategy for the dataset rcv1 with T = 10 000 blocks.\nOverall, there is no clear winner from this experiment, and the preference for an algorithm depends on the dataset, the required precision, or the regularization level. The best methods seem to be consistently MISO, ASGD and SAG and the slowest one FISTA. Note that this apparently mixed result is a significant achievement. We have indeed focused on state-ofthe-art solvers, which already significantly outperform a large number of other baselines (see Bottou, 2010; Fan et al., 2008; Le Roux et al., 2012)."
    }, {
      "heading" : "7.4. ℓ1-Regularized Logistic Regression",
      "text" : "Since SAG, SGD and ASGD cannot deal with ℓ1regularization, we compare here LIBLINEAR, FISTA, SHOTGUN and MISO. We use for LIBLINEAR the option -s 6 -e 0.000001. We proceed as in Section 7.3, considering three regularization regimes yielding different sparsity levels. We report the results for one of them in Figure 2 and provide the rest as supplemental material. In this experiment, our method outperforms other competitors, except LIBLINEAR on the dataset rcv1 when a high precision is required (and the regularization is low). We also remark that a low precision solution is often achieved quickly using the minibatch scheme (MISO2 b1000), but this strategy is outperformed by MISO1 and MISO2 for high precisions."
    }, {
      "heading" : "8. Conclusion",
      "text" : "In this paper, we have introduced a flexible optimization framework based on the computation of “surrogate functions”. We have revisited numerous schemes and discovered new ones. For each of them, we have studied convergence guarantees for non-convex problems and convergence rates for convex ones. Our methodology led us in particular to the design of an in-\nEffective passes over data / Dataset alpha\nD is\nta nc\ne to\no pt\nim um\n0 5 10 15 20 25 30 35 40 45 50 10−5\n10−4\n10−3\n10−2\n10−1\n100\nTraining time (sec) / Dataset alpha\nD is\nta nc\ne to\no pt\nim um\n100 101 102 10−5\n10−4\n10−3\n10−2\n10−1\n100\nFISTA LIBLINEAR SHOTGUN MISO1 MISO2 MISO2 b1000\nEffective passes over data / Dataset rcv1\nD is\nta nc\ne to\no pt\nim um\n0 5 10 15 20 25 30 35 40 45 50 10−5\n10−4\n10−3\n10−2\n10−1\n100\nTraining time (sec) / Dataset rcv1\nD is\nta nc\ne to\no pt\nim um\n100 101 102 103 10−5\n10−4\n10−3\n10−2\n10−1\n100\nFISTA LIBLINEAR SHOTGUN MISO1 b10000 MISO2 b10000 MISO2 b1000\nEffective passes over data / Dataset covtype\nD is\nta nc\ne to\no pt\nim um\n0 5 10 15 20 25 30 35 40 45 50 10−5\n10−4\n10−3\n10−2\n10−1\n100\nTraining time (sec) / Dataset covtype\nD is\nta nc\ne to\no pt\nim um\n10−1 100 101 102 10−5\n10−4\n10−3\n10−2\n10−1\n100\nFISTA LIBLINEAR SHOTGUN MISO1 MISO2 MISO2 b1000\nEffective passes over data / Dataset ocr\nD is\nta nc\ne to\no pt\nim um\n0 5 10 15 20 25 30 35 40 45 50 10−5\n10−4\n10−3\n10−2\n10−1\n100\nTraining time (sec) / Dataset ocr\nD is\nta nc\ne to\no pt\nim um\n101 102 103 10−5\n10−4\n10−3\n10−2\n10−1\n100\nMISO1 MISO2 MISO2 b1000"
    }, {
      "heading" : "Acknowledgments",
      "text" : ""
    }, {
      "heading" : "Bradley, J.K., Kyrola, A., Bickson, D., and Guestrin, C.",
      "text" : "Parallel coordinate descent for l1-regularized loss minimization. In Proc. ICML, 2011.\nCandès, E.J., Wakin, M., and Boyd, S.P. Enhancing sparsity by reweighted ℓ1 minimization. J. Fourier Anal. Appl., 14(5):877–905, 2008.\nCollins, M., Schapire, R.E., and Singer, Y. Logistic regression, AdaBoost and Bregman distances. Mach. Learn., 48(1):253–285, 2002.\nDaubechies, I., Defrise, M., and De Mol, C. An iterative thresholding algorithm for linear inverse problems with a sparsity constraint. Commun. Pur. Appl. Math., 57 (11):1413–1457, 2004."
    }, {
      "heading" : "Della Pietra, S., Della Pietra, V., and Lafferty, J. Duality",
      "text" : "and auxiliary functions for Bregman distances. Technical report, CMU-CS-01-109, 2001.\nFan, R.-E., Chang, K.-W., Hsieh, C.-J., Wang, X.-R., and Lin, C.-J. LIBLINEAR: A library for large linear classification. J. Mach. Learn. Res., 9:1871–1874, 2008."
    }, {
      "heading" : "Gasso, G., Rakotomamonjy, A., and Canu, S. Recovering",
      "text" : "sparse signals with non-convex penalties and DC programming. IEEE T. Signal Process., 57(12):4686–4698, 2009.\nHarchaoui, Z., Juditsky, A., and Nemirovski, A. Conditional gradient algorithms for norm-regularized smooth convex optimization. preprint arXiv:1302.2325v4, 2013.\nHazan, E. and Kale, S. Projection-free online learning. In Proc. ICML, 2012.\nHorst, R. and Thoai, N.V. DC programming: overview. J. Optim. Theory App., 103(1):1–43, 1999.\nJuditsky, A. and Nemirovski, A. First order methods for nonsmooth convex large-scale optimization, I: General purpose methods. In Optimization for Machine Learning. MIT Press, 2011.\nKhan, E., Marlin, B., Bouchard, G., and Murphy, K. Variational bounds for mixed-data factor analysis. In Adv. NIPS, 2010."
    }, {
      "heading" : "Lacoste-Julien, S., Jaggi, M., Schmidt, M., and Pletscher,",
      "text" : "P. Block-coordinate Frank-Wolfe optimization for structural SVMs. In Proc. ICML, 2013."
    }, {
      "heading" : "Lange, K., Hunter, D.R., and Yang, I. Optimization",
      "text" : "transfer using surrogate objective functions. J. Comput. Graph. Stat., 9(1):1–20, 2000."
    }, {
      "heading" : "Le Roux, N., Schmidt, M., and Bach, F. A stochastic",
      "text" : "gradient method with an exponential convergence rate for finite training sets. In Adv. NIPS, 2012."
    }, {
      "heading" : "Lee, D.D. and Seung, H.S. Algorithms for non-negative",
      "text" : "matrix factorization. In Adv. NIPS, 2001."
    }, {
      "heading" : "Mairal, J., Bach, F., Ponce, J., and Sapiro, G. Online",
      "text" : "learning for matrix factorization and sparse coding. J. Mach. Learn. Res., 11:19–60, 2010.\nNeal, R.M. and Hinton, G.E. A view of the EM algorithm that justifies incremental, sparse, and other variants. Learning in graphical models, 89:355–368, 1998.\nNesterov, Y. Introductory lectures on convex optimization. Kluwer Academic Publishers, 2004.\nNesterov, Y. Gradient methods for minimizing composite objective functions. Technical report, CORE Discussion Paper, 2007.\nNesterov, Y. Efficiency of coordinate descent methods on huge-scale optimization problems. SIAM J. Optimiz., 22 (2):341–362, 2012."
    }, {
      "heading" : "Nesterov, Y. and Polyak, B.T. Cubic regularization of",
      "text" : "Newton method and its global performance. Math. Program., 108(1):177–205, 2006.\nRichtárik, P. and Takáč, M. Iteration complexity of randomized block coordinate descent methods for minimizing a composite function. Math. Program., 2012.\nSeeger, M.W. and Wipf, D.P. Variational Bayesian inference techniques. IEEE Signal Proc. Mag., 27(6):81–91, 2010."
    }, {
      "heading" : "Shalev-Schwartz, S. and Zhang, T. Proximal stochastic",
      "text" : "dual coordinate ascent. preprint arXiv 1211.2717v1, 2012."
    }, {
      "heading" : "Shalev-Shwartz, S. and Tewari, A. Stochastic methods for",
      "text" : "ℓ1 regularized loss minimization. In Proc. ICML, 2009.\nTseng, P. and Yun, S. A coordinate gradient descent method for nonsmooth separable minimization. Math. Program., 117:387–423, 2009."
    }, {
      "heading" : "Wainwright, M.J. and Jordan, M.I. Graphical models,",
      "text" : "exponential families, and variational inference. Found. Trends Mach. Learn., 1(1-2):1–305, 2008.\nWright, S., Nowak, R., and Figueiredo, M. Sparse reconstruction by separable approximation. IEEE T. Signal Process., 57(7):2479–2493, 2009.\nZhang, T. Sequential greedy approximation for certain convex optimization problems. IEEE T. Inform. Theory, 49 (3):682–691, 2003.\nZhang, X., Yu, Y., and Schuurmans, D. Accelerated training for matrix-norm regularization: a boosting approach. In Adv. NIPS, 2012.\nSupplementary Material\nOptimization with First-Order Surrogate Functions\nOutline. In Appendix A, we present simple mathematical definitions. Appendix B contains useful mathematical results, which are used in the paper. In Appendix C, we present various mechanisms to build first-order surrogate functions; it is in fact a more rigorous version of Section 2.2, where all claims are proved. In Appendix D, we present the block Frank-Wolfe optimization scheme. Finally, all proofs of propositions are given in Appendix E, and Appendix F contains additional experimental results."
    }, {
      "heading" : "A. Mathematical Background",
      "text" : "For self-containedness purposes, we introduce in this section some mathematical definitions. Most of them can be found in classical textbooks on optimization (e.g., Bertsekas, 1999; Boyd & Vandenberghe, 2004; Borwein & Lewis, 2006; Nocedal & Wright, 2006; Nesterov, 2004).\nDefinition A.1 (Directional Derivative). Let us consider a function f : Θ ⊆ Rp → R, where Θ is a convex set, and θ, θ′ be in Θ. When it exists, the following limit is called the directional derivative of f at θ in the direction θ′ − θ:\n∇f(θ, θ′ − θ) , lim t→0+ f(θ + t(θ′ − θ))− f(θ) t .\nWhen f is differentiable at θ, directional derivatives always exist and we have ∇f(θ, θ′ − θ) = ∇f(θ)⊤(θ′ − θ). Definition A.2 (Feasible Direction). Let Θ ⊆ Rp be a convex set and θ be a point in Θ. A vector z in Rp is a feasible direction if θ + z is in Θ. In other words, z can be written as θ′ − θ, where θ′ is in Θ. Definition A.3 (Stationary Point). Let us consider a function f : Θ ⊆ Rp → R, where Θ is a convex set, such that f admits directional derivatives everywhere in Θ for every feasible direction. Let θ be a point in Θ. We say that θ is a stationary point if for all θ′ 6= θ in Θ,\n∇f(θ, θ′ − θ) ≥ 0. (4)\nWhen f is differentiable and θ is in the interior of Θ, this condition reduces to ∇f(θ) = 0. When f is convex and θ is also in the interior of Θ, this condition reduces to 0 ∈ ∂f(θ), where ∂f is the subdifferential of f .\nProof. Let us assume that θ is a stationary point and f is differentiable at θ. Then, for all θ′ in Θ, ∇f(θ, θ′−θ) = ∇f(θ)⊤(θ′ − θ) ≥ 0. In particular, since θ is in the interior of Θ, we can find θ′ such that θ′ − θ = −δ∇f(θ) for some δ > 0 small enough. Thus, we necessarily have ∇f(θ) = 0. The converse is trivial. The equivalence between (4) and 0 ∈ ∂f(θ) when f is convex but non-differentiable can be found in Borwein & Lewis (2006, Proposition 3.1.6).\nDefinition A.4 (Lipschitz Continuity). A function f : Θ ⊆ Rp → R is called Lipschitz if there exists a constant L > 0 such that for all θ, θ′ in Θ, we have\n|f(θ′)− f(θ)| ≤ L‖θ− θ′‖2.\nIn that case, we say that the function is L-Lipschitz.\nDefinition A.5 (Strong Convexity). Let Θ be a convex set. A function f : Θ ⊆ Rp → R is called µ-strongly convex when there exists a constant µ > 0 such that for all θ′ in Θ, the function θ 7→ f(θ) − µ2 ‖θ − θ′‖22 is convex. This definition is equivalent to having for all α in [0, 1] and θ, θ′ in Θ,\nf(αθ + (1− α)θ′) ≤ αf(θ) + (1 − α)f(θ′)− µ 2 α(1 − α)‖θ − θ′‖22. (5)\nNote that the value µ = 0 leads to the classical definition of convex functions.\nProof. Let us consider θ′ in Θ and define the function g : θ 7→ f(θ)− µ2 ‖θ − θ′‖22. This function is convex if and only if for all α in [0, 1], we have\ng(αθ + (1− α)θ′) ≤ αg(θ) + (1− α)g(θ′).\nIn other words, if and only if\nf(αθ + (1 − α)θ′)− µ 2 α2‖θ − θ′‖22 ≤ α ( f(θ)− µ 2 ‖θ − θ′‖22 ) + (1− α)f(θ′),\nwhich is equivalent to (5)."
    }, {
      "heading" : "B. Useful Mathematical Results",
      "text" : "We provide in this section a few propositions and lemmas which are used in this paper.\nLemma B.1 (Convex Surrogate for Functions with Lipschitz Gradient). Let f : Rp → R be differentiable and ∇f be L-Lipschitz continuous. Then, for all θ, θ′ in Rp,\n|f(θ′)− f(θ)−∇f(θ)⊤(θ′ − θ)| ≤ L 2 ‖θ − θ′‖22. (6)\nProof. This lemma is classical (see Nesterov, 2004, Lemma 1.2.3 and its proof).\nNote that Eq. (6) does not imply the gradient of a differentiable function f to be L-Lipschitz continuous. The equivalence is only true in some cases, as shown in the following lemma.\nLemma B.2 (Relation between Quadratic Surrogates and Lipschitz Constants). Let f : Rp → R be a differentiable function. Assume that for all θ, θ′ in Rp, inequality (6) holds. Then, ∇f is L-Lipschitz continuous when one of the following conditions is true:\n1. f is convex;\n2. f is twice differentiable;\n3. ∇f is Lipschitz continuous (the lemma then provides the Lipschitz constant L)."
    }, {
      "heading" : "Proof.",
      "text" : ""
    }, {
      "heading" : "First point:",
      "text" : "a proof of the first point can be found in Nesterov (2004, Theorem 2.1.5)."
    }, {
      "heading" : "Second point:",
      "text" : "To prove the second point, we upper-bound the extremal eigenvalues of the Hessian matrix. Let us fix θ in Rp. Since f is twice differentiable at θ, we have for all θ′ in Rp\n∇f(θ′)−∇f(θ) = ∇2f(θ)(θ′ − θ) + o(‖θ′ − θ‖2),\nand thus (θ′ − θ)⊤(∇f(θ′)−∇f(θ)) = (θ′ − θ)⊤∇2f(θ)(θ′ − θ) + o(‖θ′ − θ‖22). (7)\nSumming twice Eq. (6) without the absolute values when exchanging the roles of θ and θ′ gives\n(θ′ − θ)⊤(∇f(θ′)−∇f(θ)) ≤ L‖θ − θ′‖22.\nPlugging Eq. (7) into this inequality yields ‖∇2f(θ)‖2 ≤ L. To conclude, we use a mean value theorem, as done for example in Nesterov (2004, Lemma 1.2.2)\n‖∇f(θ′)−∇f(θ)‖2 = ∥ ∥ ∥\n∥\n∫ 1\nt=0\n∇2f(θ + t(θ′ − θ))(θ′ − θ)dt ∥ ∥ ∥\n∥\n2\n≤ ∫ 1\nt=0\n‖∇2f(θ + t(θ′ − θ))‖2dt‖θ′ − θ‖2\n≤ L‖θ− θ′‖2."
    }, {
      "heading" : "Third point:",
      "text" : "Proving the third point is more difficult due to the lack of smoothness assumptions on f . However, when making the explicit assumption that ∇f is Lipschitz continuous, we can show that Eq. (6) provides us a Lipschitz constant. The proof exploits some results from nonsmooth analysis developed by Clarke (1983). We essentially use a mean value theorem for multi-dimensional Lipschitz functions (Clarke, 1983, Proposition 2.6.5), exploiting the fact that a Lipschitz function is differentiable almost everywhere (Rademacher theorem). This allows us to follow a similar proof as for the twice differentiable case.\nMore precisely, we have that ∇f is Lipschitz continuous and thus differentiable almost everywhere on Θ. Let us call the Hessian matrix ∇2f(θ) at a point θ in Rp, when it exists. Then, we have at such a point ‖∇2f(θ)‖2 ≤ L, following the beginning of the second point’s proof. Then, it turns out that for all θ in Rp, the following mean value theorem holds for almost all θ′ (see Clarke, 1983, proof of Proposition 2.6.5):\n∇f(θ′)−∇f(θ) = ∫ 1\nt=0\n∇2f(θ + t(θ′ − θ))(θ′ − θ)dt.\nThis comes from the fact that for almost all θ′, the intersection of the line segment [θ, θ′] and the set where ∇2f is not defined has 0 one-dimensional measure (see again Clarke, 1983, Proposition 2.6.5). We therefore have for almost all θ′ (and a fixed θ), ‖∇f(θ) − ∇f(θ′)‖2 ≤ L‖θ − θ′‖2 and the general result comes from a continuity argument.\nLemma B.3 (Surrogate for Functions with Lipschitz Hessian). Let f : Rp → R be a twice differentiable function with M -Lipschitz continuous Hessian. Then for all θ, θ′ in Rp,\n∣ ∣ ∣ ∣ f(θ′)− f(θ)−∇f(θ)⊤(θ′ − θ)− 1 2 (θ′ − θ)⊤∇2f(θ)(θ′ − θ) ∣ ∣ ∣ ∣ ≤ M 6 ‖θ − θ′‖32.\nProof. This is again a classical lemma (see Nesterov, 2004, Lemma 1.2.4).\nLemma B.4 (Lower Surrogate for Strongly Convex Functions). Let f : Rp → R be a µ-strongly convex function. Suppose that f is differentiable, then the following inequality holds for all θ, θ′ in Rp:\nf(θ′) ≥ f(θ) +∇f(θ)⊤(θ′ − θ) + µ 2 ‖θ − θ′‖22.\nProof. θ′ 7→ f(θ′)− µ2 ‖θ − θ′‖22 is convex and differentiable and is therefore above its tangent at θ, immediately leading to the desired inequality.\nLemma B.5 (Second-Order Growth Property). Let f : Rp → R be a µ-strongly convex function and Θ ⊆ Rp be a convex set. Let θ⋆ be the minimizer of f on Θ. Then, the following condition holds for all θ in Θ:\nf(θ) ≥ f(θ⋆) + µ 2 ‖θ − θ⋆‖22.\nProof. Let us define the function g : θ 7→ f(θ) − µ2 ‖θ − θ⋆‖22. We show that θ⋆ is a minimizer of the convex function g by looking at first-order optimality conditions based on directional derivatives. For all θ in Θ, we have\n∇g(θ⋆, θ − θ⋆) = lim t→0+ f(θ⋆ + t(θ − θ⋆))− f(θ⋆)− µt22 ‖θ − θ⋆‖22 t\n= lim t→0+ f(θ⋆ + t(θ − θ⋆))− f(θ⋆) t = ∇f(θ⋆, θ − θ⋆) ≥ 0,\nwhere ∇f(θ⋆, θ − θ⋆) is non-negative because θ⋆ is a stationary point of f on Θ. Thus, θ⋆ is also a stationary point of the function g on Θ, and is a minimizer of g on Θ since g is convex (Borwein & Lewis, 2006, Proposition 2.1.2). This is sufficient to conclude.\nLemma B.6 (Lipschitz Continuity of Minimizers for Parameterized Functions). Let f : Rp1 ×Θ2 → R be a function of two variables where Θ2 ⊆ Rp2 is a convex set. Assume that\n• θ1 7→ f(θ1, θ2) is differentiable for all θ2 in Θ2; • θ2 7→ ∇1f(θ1, θ2) is L-Lipschitz continuous for all θ1 in Rp1 ; • θ2 7→ f(θ1, θ2) is µ-strongly convex for all θ1 in Rp1 .\nThen, the function θ1 7→ argminθ2∈Θ2 f(θ1, θ2) is well defined and Lµ -Lipschitz.\nProof. Let us consider θ1, θ ′ 1 in R p1 and the corresponding (unique by strong convexity) solutions θ⋆2 , argminθ2∈Θ2 f(θ1, θ2) and θ ′⋆ 2 , argminθ2∈Θ2 f(θ ′ 1, θ2). From the second-order growth condition of Lemma B.5, we have µ\n2 ‖θ⋆2 − θ′⋆2 ‖22 ≤ f(θ1, θ′⋆2 )− f(θ1, θ⋆2),\nand µ\n2 ‖θ⋆2 − θ′⋆2 ‖22 ≤ f(θ′1, θ⋆2)− f(θ′1, θ′⋆2 ),\nDefine the function g : κ 7→ f(κ, θ′⋆2 )− f(κ, θ⋆2) and sum the above inequalities. We obtain\nµ‖θ⋆2 − θ′⋆2 ‖22 ≤ g(θ1)− g(θ′1).\nWe notice that the gradient of g is bounded: for all κ in Rp1 , ‖∇g(κ)‖2 = ‖∇1f(κ, θ′⋆2 ) − ∇1f(κ, θ⋆2)‖ ≤ L‖θ′⋆2 − θ⋆2‖2. We use here the fact that ∇1f is L-Lipschitz with respect to its second argument. Thus, g is Lipschitz with constant L‖θ′⋆2 − θ⋆2‖2 and\nµ‖θ⋆2 − θ′⋆2 ‖22 ≤ L‖θ⋆2 − θ′⋆2 ‖2‖θ1 − θ′1‖2.\nThis is sufficient to conclude."
    }, {
      "heading" : "Lemma B.7 (Differentiability of Optimal Value Functions).",
      "text" : "Let us consider a function f defined as in Lemma B.6 and with the same properties. Define the optimal value function f̃(θ1) , minθ2∈Θ2 f(θ1, θ2). Then, f̃ is differentiable and ∇f̃(θ1) = ∇1f(θ1, θ⋆2), where θ⋆2 , argminθ2∈Θ2 f(θ1, θ2). Moreover,\n1. when f is convex and θ1 7→ ∇1f(θ1, θ2) is L′-Lipschitz continuous for all θ2 in Θ2, the function f̃ is convex and ∇f̃ is Lipschitz continuous with constant L′;\n2. when θ1 7→ f(θ1, θ2) is concave for all θ2 in Θ2, the function f̃ is concave and ∇f̃ is Lipschitz continuous with constant 2L 2\nµ ;\n3. when θ1 7→ f(θ1, θ2) is affine for all θ2 in Θ2, the function f̃ is concave and ∇f̃ is Lipschitz continuous with constant L 2\nµ .\nProof. Note that this lemma is a variant of a theorem introduced by Danskin (1967). We first prove the differentiability of f before detailing how to obtain the Lipschitz constants.\nDifferentiability of f : Let us consider θ1 and θ ′ 1 in R\np1 , and let us use the same notation and definitions as in the proof of Lemma B.6. Then, we have\nf̃(θ′1)− f̃(θ1) = f(θ′1, θ′⋆2 )− f(θ1, θ⋆2) = f(θ′1, θ ′⋆ 2 )− f(θ′1, θ⋆2) + f(θ′1, θ⋆2)− f(θ1, θ⋆2)\n= g(θ′1) + f(θ ′ 1, θ ⋆ 2)− f(θ1, θ⋆2) = g(θ′1) +∇1f(θ1, θ⋆2)⊤(θ′1 − θ1) + o(‖θ′1 − θ1‖2),\n(8)\nwhere g is defined in the proof of Lemma B.6. Recall that the function g is Lipschitz with constant L‖θ′⋆2 − θ⋆2‖2 (see the proof of Lemma B.6). Thus,\n|g(θ′1)| ≤ |g(θ1)− g(θ′1)| ≤ L‖θ′⋆2 − θ⋆2‖2‖θ′1 − θ1‖2 ≤ L2\nµ ‖θ′1 − θ1‖22, (9)\nwhere the first inequality uses the fact that g(θ′1) ≤ 0 and g(θ1) ≥ 0. The last inequality uses Lemma B.6. We can now show that f̃(θ′1) = f̃(θ1) +∇1f(θ1, θ⋆2)⊤(θ′1 − θ1) + o(‖θ′1 − θ1‖2). The function f̃ thus admits a first-order Taylor expansion and is differentiable. Moreover, we have ∇f̃(θ1) = ∇1f(θ1, θ⋆2)."
    }, {
      "heading" : "Proof of the first point:",
      "text" : "When f is jointly convex in θ1 and θ2, it is easy to show that f̃ is also convex (Boyd & Vandenberghe, 2004, Section 3.2.5).\nBy explicitly upper-bounding the quantity o(‖θ′1 − θ1‖2) in Eq. (8) using the L′-Lipschitz continuity of ∇1f in its first argument and the inequality g(θ′1) ≤ 0, we have\n0 ≤ f̃(θ′1)− f̃(θ1)−∇f̃(θ1)⊤(θ′1 − θ1) ≤ L′\n2 ‖θ′1 − θ1‖22,\nwe can apply Lemma B.2 to ensure that ∇f̃ is L′-Lipschitz continuous."
    }, {
      "heading" : "Proof of the second point:",
      "text" : "−f̃ is a pointwise supremum of convex functions and is therefore convex (see Boyd & Vandenberghe, 2004, Section 3.2.3). Then, we have from Eq. (8) and using the concavity of θ1 7→ f(θ1, θ⋆2):\nf̃(θ′1)− f̃(θ1) ≥ g(θ′1) +∇f̃(θ1)⊤(θ′1 − θ1).\nThus,\n0 ≤ −f̃(θ′1) + f̃(θ1) +∇f̃(θ1)⊤(θ′1 − θ1) ≤ |g(θ′1)| ≤ L2\nµ ‖θ1 − θ′1‖22,\nwhere the last inequality was shown in Eq. (9). We can then apply Lemma B.2 to the convex function −f̃ and we obtain the desired Lipschitz constant 2L 2\nµ ."
    }, {
      "heading" : "Proof of the third point:",
      "text" : "When θ1 7→ f(θ1, θ2) is affine, ∇1f(θ1, θ2) is independent of θ1.\n‖∇f̃(θ′1)−∇f̃(θ1)‖2 = ‖∇1f(θ′1, θ′⋆2 )−∇1f̃(θ1, θ⋆2)‖2\n= ‖∇g(θ1)‖2 ≤ L‖θ2 − θ⋆2‖2 ≤ L2\nµ ‖θ1 − θ′1‖2,\nwhere the upper-bound on the gradient of g was shown in the proof of Lemma B.6.\nLemma B.8 (Pythagoras Relation). Let θ, κ, ν in Rp. Then ‖κ− θ‖22 + 2(κ− θ)⊤(θ − ν) = ‖κ− ν‖22 − ‖θ − ν‖22. Lemma B.9 (Regularity of Residual Functions). Let f, g : Rp → R be two functions. Define the difference function h , g − f . Then,\n1. if g is ρ-strongly convex and f differentiable with L-Lipschitz continuous gradient, with ρ ≥ L, the function h is (ρ− L)-strongly convex;\n2. if g and f are convex and differentiable with L-Lipschitz continuous gradient, ∇h is L-Lipschitz continuous.\n3. if g and f are µ-strongly convex and differentiable with L-Lipschitz continuous gradient, ∇h is (L − µ)Lipschitz continuous."
    }, {
      "heading" : "Proof.",
      "text" : ""
    }, {
      "heading" : "Proof of the first point:",
      "text" : "Let θ′ be in Rp, and define l : θ 7→ g(θ)− f(θ)− (ρ−L)2 ‖θ − θ′‖22. Then,\nl(θ) = ( g(θ)− ρ 2 ‖θ − θ′‖22 ) +\n(\nL 2 ‖θ − θ′‖22 − f(θ)\n)\n,\nThe left term inside parentheses is convex by definition of strong convexity. Let us call the right term l′ : θ 7→ L 2 ‖θ−θ′‖22−f(θ). The function l′ is differentiable and we can show that it is above its tangent, therefore convex. Let us indeed fix κ in Rp:\nl′(θ) = L\n2 ‖θ − θ′‖22 − f(θ) ≥ −f(κ)−∇f(κ)⊤(θ − κ)−\nL 2 ‖θ − κ‖22 + L 2 ‖θ − θ′‖22\n=\n(\nL 2 ‖κ− θ′‖22 − f(κ)\n)\n+ L(κ− θ′)⊤(θ − κ)−∇f(κ)⊤(θ − κ)\n= l′(κ) +∇l′(κ)⊤(θ − κ).\nThe first inequality comes from Lemma B.1 applied to the function f at κ. The second equality is simply due to the trivial relation described in Lemma B.8.\nProof of the second and third points: We simply prove the third point, and then obtain the second point by choosing µ = 0. We have for all θ and θ′ in Rp, according to Lemma B.1 and B.4\nµ 2 ‖θ − θ′‖22 ≤ f(θ′)− f(θ)−∇f(θ)⊤(θ′ − θ) ≤ L 2 ‖θ − θ′‖22,\nand\n−L 2 ‖θ − θ′‖22 ≤ −g(θ′) + g(θ) +∇g(θ)⊤(θ′ − θ) ≤ − µ 2 ‖θ − θ′‖22.\nSumming the two inequalities we have that\n|h(θ′)− h(θ)−∇h(θ)⊤(θ′ − θ)| ≤ L− µ 2 ‖θ − θ′‖22.\nwhere h , g− f . Since h is differentiable with a Lipschitz gradient, the result follows from Lemma B.2 (whether h is convex or not)."
    }, {
      "heading" : "C. Mechanisms to Construct First-Order Surrogate Functions",
      "text" : "We provide here some details and justifications to Section 2.2. We start with a basic lemma, which gives us elementary techniques to combine surrogate functions.\nLemma C.1 (Combination Rules for Majorant First-Order Surrogates). Let us consider two functions f : Rp → R and f ′ : Rp → R, and majorant surrogate functions g in SL(f, κ) and g′ in SL(f ′, κ) for some κ in Θ. Then, the following combination rules hold:\n• Linear combination: for all α, β > 0, αg+βg′ is a majorant surrogate function in SαL+βL′(αf+βf ′, κ); • Transitivity: consider g′′ a majorant surrogate in SL′′(g, κ). Then, g′′ is a majorant surrogate in\nSL+L′′(f, κ); • Negation: the function g′′ : θ 7→ −g(θ) + L2 ‖θ − κ‖22 is a majorant surrogate in S2L(−f, κ).\nProof. The first two points are easy to check. For the last one, we have for all θ in Θ, g(θ) − f(θ) ≤ L2 ‖θ − κ‖22 according to Lemma 2.1. The proposed surrogate is therefore majorant for −f . We can now define the approximation error function h′′ : θ 7→ f(θ) − g(θ) + L2 ‖θ − κ‖22, which is differentiable with 2L-Lipschitz continuous gradient and g′′ is in S2L(−f, κ) (we have used the fact that θ 7→ L2 ‖θ− κ‖22 and h , g − f are both differentiable and their gradients are L-Lipschitz).\nIn the next paragraphs, we justify the different surrogates we have introduced in Section 2.2."
    }, {
      "heading" : "Lipschitz Gradient Surrogates.",
      "text" : "When f is differentiable and ∇f is L-Lipschitz, we consider the following surrogate:\ng : θ 7→ f(κ) +∇f(κ)⊤(θ − κ) + L 2 ‖θ − κ‖22.\nBy applying Lemma B.1 and studying the approximation error h , g − f , we immediately obtain that g is a majorant surrogate in S2L,L(f, κ). When f is convex, we can use Lemma B.9 to prove that g is in SL,L(f, κ) and SL−µ,L(f, κ) when f is µ-strongly convex."
    }, {
      "heading" : "Proximal Gradient Surrogates.",
      "text" : "Assume that f splits into f = f1 + f2, where f1 is differentiable with a L-Lipschitz gradient. Then, we have presented the following surrogate\ng : θ 7→ f1(κ) +∇f1(κ)⊤(θ − κ) + L\n2 ‖θ − κ‖22 + f2(θ).\nFollowing the same arguments as in the previous paragraph, we have that g is in S2L(f, κ). Moreover, when f1 is convex, g is in SL(f, κ). If f2 is also convex, g is in SL,L(f, κ). When f1 is µ-strongly convex, g is in SL−µ(f, κ). If f2 is also convex, g is in SL−µ,L(f, κ)."
    }, {
      "heading" : "DC Programming Surrogates.",
      "text" : "Assume that f = f1 + f2, where f2 is concave and differentiable with a L2-Lipschitz gradient. Then, we have presented the following surrogate\ng : θ 7→ f1(θ) + f2(κ) +∇f2(κ)⊤(θ − κ).\nIt is easy to see that g is a majorant surrogate since f2 is concave and below its tangents. It is also easy to see that the approximation error g − f has a L2-Lipschitz continuous gradient."
    }, {
      "heading" : "Variational Surrogates.",
      "text" : "Let f be a function defined on Rp1 × Rp2 . Let Θ1 ⊆ Rp1 and Θ2 ⊆ Rp2 be two convex sets. Define f̃ as f̃(θ1) , minθ2∈Θ2 f(θ1, θ2) and assume that\n• θ1 7→ f(θ1, θ2) is differentiable for all θ2 in Θ2; • θ2 7→ ∇1f(θ1, θ2) is L-Lipschitz for all θ1 in Rp1 ; • θ1 7→ ∇1f(θ1, θ2) is L′-Lipschitz for all θ2 in Θ2;\n• θ2 7→ f(θ1, θ2) is µ-strongly convex for all θ1 in Rp1 .\nLet us fix κ1 in Θ1. Then, we can show that the following function is a majorant surrogate in SL′′(f̃ , κ) for some L′′ > 0:\ng : θ1 7→ f(θ1, κ⋆2) with κ⋆2 , argmin θ2∈Θ2 f̃(κ1, θ2).\nWe can indeed apply Lemma B.7 which ensures that f̃ is differentiable with ∇f̃(θ1) = ∇1f(θ1, θ⋆2) and θ⋆2 , argmin f(θ1, θ2). Considering the approximation error function h , g − f̃ , we indeed have that h(κ1) = 0, ∇h(κ1) = 0 and since θ⋆2 as a function of θ1 is Lipschitz according to Lemma B.6, we also have that ∇h is Lipschitz continuous.\nWhen f is jointly convex in θ1 and θ2, f̃ is itself convex and ∇f̃ is L′-Lipschitz continuous according to Lemma B.7. We can then apply Lemma B.9 to obtain that ∇h is L′-Lipschitz continuous such that we can choose L′′ = L′."
    }, {
      "heading" : "Saddle Point Surrogates.",
      "text" : "Let us make the same assumptions as in the previous paragraph with the following exceptions\n• θ2 7→f(θ1, θ2) is µ-strongly concave for all θ1 in Rp1 ; • θ1 7→f(θ1, θ2) is convex for all θ2 in Θ2; • f̃(θ1) , maxθ2∈Θ2 f(θ1, θ2).\nThen, f̃ is convex as the pointwise supremum of convex functions (see Boyd & Vandenberghe, 2004) and we can show that the function below is a majorant surrogate in S2L′′(f̃ , κ1):\ng : θ1 7→ f(θ1, κ⋆2) + L′′\n2 ‖θ1 − κ1‖22,\nwhere L′′ , max(2L2/µ, L′). When θ1 7→ f(θ1, θ2) is affine, we can instead choose L′′ , L2/µ. We indeed apply the same methodology as in the previous paragraph. Lemma B.7 tells us that the function −f̃ is differentiable with 2L2/µ-Lipschitz continuous gradient (only L2/µ in the affine case). Then, we have that the function θ1 7→ −f(θ1, κ⋆2) is in SL′′(−f̃ , κ1) by using Lemma B.9. We then apply the negation rule of Lemma C.1 to conclude."
    }, {
      "heading" : "Jensen Surrogates.",
      "text" : "Let us recall the definition of Jensen surrogates. Following Lange et al. (2000), we consider a convex function f : R 7→ R, a vector x in Rp and define f̃ : Rp → R as f̃(θ) , f(x⊤θ) for all θ. Let w in Rp+ be a weight vector such that w ≥ 0, ‖w‖1 = 1 and wi 6= 0 whenever xi 6=0. Then, we consider the following function g for any κ in Rp\ng : θ 7→ p ∑\ni=1\nwif\n(\nxi wi (θi − κi) + x⊤κ ) ,\nAssume that f is differentiable with a L-Lipschitz gradient and wi , |xi|ν/‖x‖νν for some ν ≥ 0.9 ∇f is obviously Lipschitz with constant L‖x‖22. g is also convex, differentiable with Lipschitz continuous gradient with constant L′ obtained below with simple calculations:\n• if ν = 0, L′ = L‖x‖2∞‖x‖0; • if ν = 1, L′ = L‖x‖∞‖x‖1; • if ν = 2, L′ = L‖x‖22.\nThe fact that g is majorant is a simple application of Jensen inequality. It is also obvious that g(κ) = f(κ) and that ∇g(κ) = ∇f(κ). We now apply Lemma B.9, noticing that we always have L′ greater than L‖x‖22, and we have that g is in SL′(f̃ , κ).\n9With an abuse of notation, ‖x‖00 denotes the ℓ0-pseudo norm, also denoted by ‖x‖0."
    }, {
      "heading" : "Quadratic Surrogates.",
      "text" : "When f is twice differentiable and admits a matrix H in such that ∇2f − H is always positive definite, the following function is a first-order majorant surrogate:\ng : θ 7→ f(κ) +∇f(κ)⊤(θ − κ) + 1 2 (θ − κ)⊤H(θ − κ).\nThe fact that it is majorant is simply an application of the mean-value theorem."
    }, {
      "heading" : "D. Additional Optimization Scheme: Block Frank-Wolfe",
      "text" : "We provide in this section an additional optimization scheme, combining the ideas of Sections 4 and 3 with separability assumptions on the surrogates gn and Θ. It results in a block coordinate version of the Frank-Wolfe optimization scheme presented in Algorithm 6 generalizing a procedure recently introduced by Lacoste-Julien et al. (2013). More precisely, the algorithm of Lacoste-Julien et al. (2013) corresponds to using a quadratic surrogate as provided by Lemma B.1 when f is smooth with L-Lipschitz gradient, and performing a line search on the function f instead of gn. Our approach on the other hand can afford to have a non-smooth component in f and in that sense is more general. Note that Lacoste-Julien et al. (2013) also presents duality gap guarantees and various extensions and applications, which we do not consider in our paper.\nAlgorithm 6 Block Frank-Wolfe Scheme\ninput θ0 = (θ 1 0 , . . . , θ k 0 ) ∈ Θ = Θ1 × . . .×Θk (initial point); N (number of iterations).\n1: for n = 1, . . . , N do 2: Compute a separable majorant surrogate function gn = ∑k i=1 g i n in SL,L(f, θn−1); 3: Randomly pick one block ı̂n in {1, . . . , k} and compute a search direction:\nν ı̂nn ∈ argmin θı̂n∈Θı̂n\n[\ng ı̂nn (θ ı̂n)− L\n2 ‖θı̂n − θı̂nn−1‖22\n]\n.\n4: Line search: α⋆ , argmin\nα∈[0,1] g ı̂nn ((1 − α)θı̂nn−1 + αν ı̂nn ).\n5: Update θı̂nn : θı̂nn , (1− α⋆)θı̂nn−1 + α⋆ν ı̂nn .\n6: end for output θN = (θ 1 N , . . . , θ k N ) (final estimate);\nProposition D.1 (Convergence Rate for Algorithm 6). Let f be convex, bounded below and f⋆ be the minimum of f on Θ = Θ1 × . . .× Θk. Assume that Θ is bounded and call R , maxθ1,θ2∈Θ ‖θ1 − θ2‖2 its diameter. The sequence (f(θn))n≥0 provided by Algorithm 6 converges almost surely to f⋆ and we have for all n ≥ 1,\nE[f(θn)− f⋆] ≤ 2LR2\n2 + δ(n− n0) ,\nwhere δ , 1/k and n0 , ⌈ log ( 2(f(θ0)−f⋆) LR2 − 1 ) / log ( 1 1−δ )⌉ if f(θ0)− f⋆ > LR2 and n0 , 0 otherwise.\nThe proof is given in Appendix E."
    }, {
      "heading" : "E. Proofs of Lemmas and Propositions",
      "text" : "We present in this section the proofs of the different lemmas and propositions in the paper."
    }, {
      "heading" : "E.1. Proof of Lemma 2.1",
      "text" : "Proof. The first inequality is a direct applications of Lemma B.1 applied to the function h at the point κ when noticing that h(κ) = 0 and ∇h(κ) = 0. Then, for all θ in Θ, we have\nf(θ′) ≤ g(θ′) ≤ g(θ) = f(θ) + h(θ),\nand we obtain the second inequality from the first one. When g is ρ-strongly convex, we can in addition exploit the second-order growth property of g presented in Lemma B.5, and obtain\nf(θ′) + ρ\n2 ‖θ′ − θ‖22 ≤ g(θ′) +\nρ 2 ‖θ − θ′‖22 ≤ g(θ) = f(θ) + h(θ),\nand the third inequality follows from the second one."
    }, {
      "heading" : "E.2. Proof of Proposition 2.1",
      "text" : "Proof. The fact that (f(θn))n≥0 is non-increasing and convergent because bounded below is clear:\nf(θn) ≤ gn(θn) ≤ gn(θn−1) = f(θn−1),\nwhere the first inequality and the last equality come from Definition 2.1. The second inequality comes from the definition of θn. Denote by f\n⋆ the limit of the sequence (f(θn))n≥0 and by hn , gn − f the approximation error functions. The latter are differentiable and their gradient are L-Lipschitz continuous according to the definitions of the surrogate functions. Then,\nf(θn) + hn(θn) = gn(θn) ≤ f(θn−1),\nand thus, by summing over n, ∞ ∑\nn=1\nhn(θn) ≤ f(θ0)− f⋆,\nand the non-negative sequence (hn(θn))n≥0 necessarily converges to zero.\nWe have then two possibilities (according to the assumptions made in the proposition):\n• if the gn’s are majorant surrogates, plugging θ′ = θn − 1L∇hn(θn) in Lemma B.1 yields\nhn(θ ′) ≤ hn(θn)−\n1\n2L ‖∇hn(θn)‖22,\nand therefore,\n‖∇hn(θn)‖22 ≤ 2L(hn(θn)− hn(θ′)) ≤ 2Lhn(θn) −→ n→+∞ 0,\nwhere we use the fact that hn(θ ′) ≥ 0 because gn is majorant.\n• otherwise, the functions gn are ρ-strongly convex and we can exploit some inequalities of Lemma 2.1. Notably,\nρ 2 ‖θn − θn−1‖22 ≤ f(θn−1)− f(θn).\nSumming this inequality over n yields that ‖θn − θn−1‖22 necessarily converges to zero, and\n‖∇hn(θn)‖2 = ‖∇hn(θn)−∇hn(θn−1)‖2 ≤ L‖θn − θn−1‖2 −→ n→+∞ 0,\nsince ∇hn(θn−1) = 0 according to Definition 2.1.\nWe can now compute directional derivatives of f at a point θn and a direction θ − θn, where θ is in Θ:\n∇f(θn, θ − θn) = ∇gn(θn, θ − θn)−∇hn(θn)⊤(θ − θn).\nNote that θn minimizes gn on Θ and therefore ∇gn(θn, θ − θn) ≥ 0. Therefore,\n∇f(θn, θ − θn) ≥ −‖∇hn(θn)‖2‖θ − θn‖2,\nusing Cauchy-Schwarz inequality. Then,\nlim inf n→+∞ inf θ∈Θ ∇f(θn, θ − θn) ‖θ − θn‖2 ≥ − lim n→+∞ ‖∇hn(θn)‖2 = 0."
    }, {
      "heading" : "E.3. Proof of Proposition 2.2",
      "text" : "Proof. We separately prove the two parts of the proposition. Non-strongly convex case: Let us define hn , gn − f the approximation error function at iteration n. From Lemma 2.1 (with g= gn, κ= θn−1, θ′=θn), we have\nf(θn) ≤ min θ∈Θ\n[\nf(θ) + L\n2 ‖θ − θn−1‖22\n]\n.\nThen, following a similar proof technique as Nesterov (2007, Theorem 4), we have\nf(θn) ≤ min α∈[0,1]\nf(αθ⋆ + (1− α)θn−1) + Lα2\n2 ‖θ⋆ − θn−1‖22,\n≤ min α∈[0,1]\nαf(θ⋆) + (1 − α)f(θn−1) + Lα2\n2 ‖θ⋆ − θn−1‖22,\n(10)\nwhere the minimization over Θ in the previous equation is replaced by a minimization on the line segment αθ⋆ + (1 − α)θn−1 : α ∈ [0, 1]. Then, because the sequence (f(θn))n≥0 is monotonically decreasing we can use the bounded level set assumption, which yields\nf(θn)− f⋆ ≤ min α∈[0,1]\n(1− α)(f(θn−1)− f⋆) + LR2α2\n2 .\n• if f(θn−1)− f⋆ ≥ LR2, then we have the optimal value α⋆ = 1 and f(θn)− f⋆ ≤ LR 2\n2 ;\n• otherwise α⋆ = f(θn−1)−f ⋆\nLR2 . Denoting by rn , f(θn)− f⋆, we have\nrn ≤ rn−1 ( 1− rn−1 2LR2 ) .\nThus, r−1n ≥ r−1n−1 ( 1− rn−12LR2 )−1 ≥ r−1n−1+ 12LR2 , where the second inequality comes from the convexity inequality (1− x)−1 ≥ 1 + x for x ∈ (0, 1).\nThen, we have seen that if r0 ≥ LR2, then r1 ≤ LR 2 2 and thus r −1 n ≥ r−11 + n−12LR2 ≥ n+32LR2 . Otherwise, we have r−1n ≥ r−10 + n2LR2 ≥ n+22LR2 , which is sufficient to conclude. µ-strongly convex case: Let us now assume that f is µ-strongly convex, and drop the bounded level sets assumption. The proof again follows Nesterov (2007) for computing the convergence rate of proximal gradient methods. We start from (10). We can then use the second-order growth property of f (Lemma B.5) which states that f(θn−1) ≥ f⋆+ µ2 ‖θn−1−θ⋆‖22 and obtain\nf(θn)− f⋆ ≤ (\nmin α∈[0,1]\n1− α+ Lα 2\nµ\n)\n(f(θn−1)− f⋆).\nAt this point, it is easy to show that\n• if µ ≥ 2L, then the previous binomial is minimized for α⋆ = 1 and\nf(θn)− f⋆ ≤ L\nµ (f(θn−1)− f⋆);\n• if µ ≤ 2L, then we have α⋆ = µ2L and\nf(θn)− f⋆ ≤ ( 1− µ 4L ) (f(θn−1)− f⋆),\nwhich is sufficient to conclude."
    }, {
      "heading" : "E.4. Proof of Proposition 2.3",
      "text" : "Proof. We separately prove the two parts of the proposition."
    }, {
      "heading" : "Non-strongly convex case:",
      "text" : "From Lemma 2.1 (with g=gn, κ=θn−1, θ′=θn, θ=θ⋆), we have\nf(θn)− f(θ⋆) ≤ L\n2 ‖θn−1 − θ⋆‖22 −\nρ 2 ‖θn − θ⋆‖22 ≤ L 2 ‖θn−1 − θ⋆‖22 − L 2 ‖θn − θ⋆‖22. (11)\nBy summing this inequality, we have\nn(f(θn)− f(θ⋆)) ≤ n ∑\nk=1\n(f(θk)− f(θ⋆)) ≤ L\n2 (‖θ0 − θ⋆‖22 − ‖θn − θ⋆‖22) ≤ L‖θ0 − θ⋆‖22 2 ,\nwhere the first inequality comes from the fact that f(θk) ≥ f(θn) for all k ≤ n. This is sufficient to prove (2.3). Note that finding telescopic sums to prove convergence rates is a classical technique (see Beck & Teboulle, 2009).\nµ-strongly convex case: Let us now prove the second part of the proposition and assume that f is µ-strongly convex. The strong convexity implies the second-order growth property of Lemma B.5: f(θn)−f⋆ ≥ µ2 ‖θn−θ⋆‖22 for all n. Combined with (11), this yields\nµ+ ρ\n2 ‖θn − θ⋆‖22 ≤\nL 2 ‖θn−1 − θ⋆‖22,\nand thus\nf(θn)− f(θ⋆) ≤ L\n2 ‖θn−1 − θ⋆‖22 ≤\n(\nL\nρ+ µ\n)n−1 L‖θ0 − θ⋆‖22\n2 ."
    }, {
      "heading" : "E.5. Proof of Proposition 2.4",
      "text" : "Proof. We separately prove the two parts of the proposition."
    }, {
      "heading" : "Non-strongly convex case:",
      "text" : "Following a similar scheme as in Proposition 2.2 and using Lemma B.3 on the approximation error functions hn instead of Lemma 2.1, we have\nf(θn) ≤ min α∈[0,1]\nf(αθ⋆ + (1− α)θn−1) + Mα3\n6 ‖θ⋆ − θn−1‖32, (12)\nThen, again following the proof of Proposition 2.2,\nf(θn)− f⋆ ≤ min α∈[0,1]\n(1− α)(f(θn−1)− f⋆) + α3MR3\n6 .\nDenoting by rn , f(θn)− f⋆ and by α⋆ the solution of this optimization problem, we have\n• if rn−1 ≥ MR3/2, then α⋆ = 1 and rn ≤ MR3/6; • otherwise, α⋆ = √ 2rn−1/(MR3), and\nrn ≤ rn−1 ( 1− √\n8rn−1 9MR3\n)\n.\nIt follows that r −1/2 n ≥ r−1/2n−1\n(\n1− √\n8rn−1 9MR3 )−1/2 ≥ r−1/2n−1 + √ 2 9MR3 , where the last inequality comes from the\nconvexity inequality (1− x)−1/2 ≥ 1 + x/2.\nThen, we have seen that if r0 ≥ MR3/2, then r1 ≤ MR3/6 and thus r−1/2n ≥ r−1/21 + (n − 1) √ 2 9MR3 ≥ √\n2 9MR3 (n− 1 + 3\n√ 3) ≥ √\n2 9MR3 (n+ 4). Otherwise, we have r −1/2 n ≥ r−1/20 + n\n√\n2 9MR3 ≥\n√\n2 9MR3 (n+ 3). This\nis sufficient to obtain the first part of the proposition.\nµ-strongly convex case: Let us now assume that f is µ-strongly convex, and drop the bounded level sets assumption. Starting again from (12),\nf(θn) ≤ min α∈[0,1]\nf(αθ⋆ + (1− α)θn−1) + Mα3\n6 ‖θ⋆ − θn−1‖32,\nand using the second-order growth property of f , we have\nrn ≤ min α∈[0,1]\n(1− α)rn−1 + γα3\n3 r 3/2 n−1.\n• when γ√rn−1 ≥ 1, we have α⋆ = 1√ γ √ rn−1 and the desired inequality follows;\n• otherwise, α⋆ = 1 and rn ≤ 32r 3/2 n−1 ≤ rn−1 3 ."
    }, {
      "heading" : "E.6. Proof of Proposition 3.1",
      "text" : "Proof. We proceed in several steps and adapt the convergence proof of Proposition 2.1 to our new setting."
    }, {
      "heading" : "Definition of an approximate surrogate ḡn:",
      "text" : "We define recursively the sequence of functions (ḡn)n≥0 as follows:\nḡn , ḡn−1 + g ı̂n n − ḡ ı̂nn−1,\nwhere the surrogate gn and the index ı̂n are chosen in the algorithm. We also define ḡ−1 as a majorant separable surrogate function such that θ0 ∈ argminθ∈Θ ḡ−1(θ) (we have assumed in the proposition that such a surrogate function exists). Then, it is easy to see that ḡn is constructed in such a way that θn is a minimizer of ḡn over Θ for all n ≥ 0 and that ḡn ≥ f . Almost sure convergence of (f(θn))n≥0 and consequences: We have f(θn) ≤ gn(θn) = ∑k i=1 g k n(θ k n) ≤ ∑k i=1 g k n(θ k n−1) = gn(θn−1) = f(θn−1) since we have g ı̂n n (θ ı̂n n ) ≤ g ı̂nn (θ ı̂n n−1) and g i n(θ i n) = g i n(θ i n−1) for i 6= ı̂n. Thus, (f(θn))n≥0 is monotonically decreasing and converges almost\nsurely. We also have\nE[ḡn(θn)− ḡn−1(θn−1)] = E[ḡn(θn)− ḡn(θn−1)] + E[ḡn(θn−1)− ḡn−1(θn−1)] = E[ḡn(θn)− ḡn(θn−1)] + E[g ı̂nn (θı̂nn−1)− ḡ ı̂nn−1(θı̂nn−1)] = E[ḡn(θn)− ḡn(θn−1)] + E[E[g ı̂nn (θı̂nn−1)− ḡ ı̂nn−1(θı̂nn−1)|θn−1]]\n= E[ḡn(θn)− ḡn(θn−1)] + 1\nk E[gn(θn−1)− ḡn−1(θn−1)]\n= E[ḡn(θn)− ḡn(θn−1)] + 1\nk E[f(θn−1)− ḡn−1(θn−1)].\nNote that both terms ḡn(θn)− ḡn(θn−1) and f(θn−1)− ḡn−1(θn−1) are non-positive with probability one and thus the sequence (E[ḡn(θn)])n≥0 is non-increasing, bounded below and convergent. The term E[ḡn(θn)− ḡn−1(θn−1)] is therefore the summand of a converging sum, and so are E[ḡn(θn) − ḡn(θn−1)] and E[f(θn−1) − ḡn−1(θn−1)]. Then, we have by using Beppo-Levi theorem\n+∞ ∑\nn=0\nE[ḡn(θn)− f(θn)] = E [ +∞ ∑\nn=0\nḡn(θn)− f(θn) ] < +∞.\nThus, the term ḡn(θn)− f(θn) converges almost surely to 0. Asymptotic stationary point conditions: Let us denote by h̄n , ḡn − f which is differentiable with L-Lipschitz continuous gradient. Then, for all θ in Θ,\n∇f(θn, θ − θn) = ∇ḡn(θn, θ − θn)−∇h̄n(θn)⊤(θ − θn).\nWe have ∇ḡn(θn, θ − θn) ≥ 0 since θn is a minimizer of ḡn, and ‖∇h̄n(θn)‖22 ≤ 2Lh̄n(θn), following a similar argument as in the proof of Proposition 2.1. Since we have shown that h̄n(θn) almost surely converges to zero, we conclude using the Cauchy-Schwarz inequality as in the proof of Proposition 2.1."
    }, {
      "heading" : "E.7. Proof of Proposition 3.2",
      "text" : "Proof. The fact that (f(θn))n≥0 almost surely converges follows the beginning of Proposition 3.1. To show the convergence rates of (E[f(θn)])n≥0, we adapt the proof of Proposition 2.2 to out stochastic block setting. Let us denote by θ⋆n a minimizer of the surrogate function gn over Θ. Since the indices ı̂n are picked up uniformly at random, we have the following conditional probabilities\nP(θin = θ ⋆i n |θn−1) = δ and P(θin = θin−1|θn−1) = 1− δ.\nWe can then obtain the following inequalities for all θ in Θ\nE[f(θn)|θn−1] ≤ E[gn(θn)|θn−1] = k ∑\ni=1\nE[gin(θ i n)|θn−1] =\nk ∑\ni=1\n(1− δ)gin(θin−1) + δgin(θ⋆in )\n= (1− δ)gn(θn−1) + δgn(θ⋆n) ≤ (1− δ)f(θn−1) + δgn(θ) ≤ (1− δ)f(θn−1) + δ ( f(θ) + L\n2 ‖θ − θn−1‖22\n)\n,\n(13)\nwhere we have used the conditional probabilities computed above and the fact that |gn(θ)−f(θ)| ≤ L2 ‖θ−θn−1‖22 according to Lemma 2.1. Let us now follow the proof of Proposition 2.2:\nE[f(θn)|θn−1] ≤ (1 − δ)f(θn−1) + δ (\nmin α∈[0,1]\nf(αθ⋆ + (1− α)θn−1) + Lα2\n2 ‖θ⋆ − θn−1‖22\n)\n.\nWe can now proceed by considering two different cases.\nCase 1: without strong convexity: To simplify the notation, we now introduce the quantities rn , f(θn) − f⋆ and following again the proof of Proposition 2.2, we have\nE[rn|θn−1] ≤ (1− δ)rn−1 + δ (\nmin α∈[0,1]\n(1− α)rn−1 + LR2α2\n2\n)\n.\nThe term in parenthesis on the right is a concave function of rn−1 as a pointwise infimum of concave functions (in fact, pointwise infimum of linear functions). By taking the expectation and using Jensen inequality, we thus have\nE[rn] ≤ (1 − δ)E[rn−1] + δ (\nmin α∈[0,1]\n(1− α)E[rn−1] + LR2α2\n2\n)\n.\nBy following again the proof of Proposition 2.2, we have\nE[rn] ≤ (1 − δ)E[rn−1] + δ { LR2 2 if E[rn−1] > LR 2\nE[rn−1] ( 1− E[rn−1]2LR2 ) otherwise.\nWe also notice that the inequality E[rn] ≤ (1 − δ)E[rn−1] + δLR 2\n2 is always true. This yields after simple\ncalculations E[rn] ≤ (1− δ)nr0 + (1− (1− δ)n)LR 2 2 for all n ≥ 1. We also remark that the definition of n0 in the proposition implies that (1− δ)n0r0 +(1− (1− δ)n0)LR 2\n2 ≤ LR2 after some short calculations. Thus, we have for all n > n0 E[rn] ≤ E[rn−1] ( 1− δE[rn−1]2LR2 ) and thus E[rn] −1 ≥ E[rn0 ]−1 + (n−n0)δ2LR2 ≥ 2+(n−n0)δ 2LR2 , following similar derivations as in Proposition 2.2. This is sufficient to conclude.\nCase 2: under strong convexity assumptions: We proceed similarly as in case 1, but upper-bound instead ‖θ⋆ − θn−1‖22 by 2rn−1/µ. This leads us to a similar relation as in the proof of Proposition 2.2:\nE[rn] ≤ (1− δ)E[rn−1] + δ (\nmin α∈[0,1]\n1− α+ Lα 2\nµ\n)\nE[rn−1],\nand following again the proof of Proposition 2.2, we have\nE[rn] ≤ ((1− δ) + δβ)E[rn−1],\nyielding the desired convergence rate."
    }, {
      "heading" : "E.8. Proof of Proposition 3.3",
      "text" : "Proof. The fact that (f(θn))n≥0 almost surely converges follows the beginning of Proposition 3.1. We then separately prove the two remaining parts of the proposition.\nWithout strong convexity assumptions: Using the same notation as in the proof of Proposition 3.2, we can replace the inequality gn(θ ⋆ n) ≤ gn(θ) in Eq. (13) by gn(θ ⋆ n) ≤ gn(θ)− ρ2‖θ⋆n − θ‖22 (using Lemma B.5), and we obtain\nE[f(θn)|θn−1] ≤ (1− δ)f(θn−1) + δ ( f⋆ + L\n2 ‖θ⋆ − θn−1‖22 −\nρ 2 ‖θ⋆ − θ⋆n‖22\n)\n,\nWe also remark that\nE [ ‖θ⋆ − θn‖22|θn−1 ] =\nk ∑\ni=1\nE [ ‖θ⋆i − θin‖22|θn−1 ] =\nk ∑\ni=1\n(1− δ)‖θ⋆i − θin−1‖22 + δ‖θ⋆i − θ⋆in ‖22\n= (1− δ)‖θ⋆ − θn−1‖22 + δ‖θ⋆ − θ⋆n‖22.\nCombining the two previous inequalities yields\nE\n[ f(θn) + ρ\n2 ‖θ⋆ − θn‖22\n∣ ∣ ∣ θn−1 ] ≤ (1 − δ)f(θn−1) + δf⋆ + (1− δ)ρ+ δL\n2 ‖θ⋆ − θn−1‖22.\nLet us now define rn , E[f(θn)− f⋆]. Taking the expectation in the previous inequality gives\nrn − (1 − δ)rn−1 ≤ δL+ (1 − δ)ρ\n2 E[‖θ⋆ − θn−1‖22]−\nρ 2 E[‖θ⋆ − θ⋆n‖22]\n≤ δL+ (1 − δ)ρ 2 ( E[‖θ⋆ − θn−1‖22]− E[‖θ⋆ − θn‖22] ) .\n(14)\nSumming these inequalities and using the fact that rn ≤ rn−1 yields\nnδrn + (1− δ)(rn − r0) ≤ n ∑\nk=1\nrk − (1− δ)rk−1 ≤ δL+ (1 − δ)ρ\n2 ‖θ⋆ − θ0‖22,\nwhich gives the desired convergence rate.\nWith strong convexity assumptions: Assume now that f is µ-strongly convex. To simplify the notation, we introduce the quantity ξn , 1 2E[‖θ⋆−θn‖22]. We can now rewrite the first inequality in (14) as\nrn + ρξn ≤ (1− δ)rn−1 + ((1− δ)ρ+ δL)ξn−1. We are going to exploit two inequalities. Since we have the second-order growth property rn ≥ µξn, for all β in [0, 1], βrn + (ρ+ (1− β)µ)ξn ≤ (1− δ)rn−1 + ((1− δ)ρ+ δL)ξn−1. By choosing β , (1−δ)(ρ+µ)(1−δ)(ρ+µ)+δL , it is easy to show that\n(1 − δ)rn + ((1 − δ)ρ+ δL)ξn ≤ (1 − δ)(ρ+ µ) + δL\nρ+ µ ((1− δ)rn−1 + ((1− δ)ρ+ δL)ξn−1) .\nThus, we have by induction\n(1− δ)rn + ((1 − δ)ρ+ δL)ξn ≤ ( (1− δ)(ρ+ µ) + δL ρ+ µ\n)n\n((1− δ)r0 + ((1− δ)ρ+ δL)ξ0) ,\nand again, since µξn ≤ rn, we obtain the convergence rate of (ξn)n≥0\nξn ≤ C ( (1− δ)(ρ+ µ) + δL ρ+ µ\n)n\n≤ αn ( (1− δ)r0 L + ξ0 ) ,\nwhere we have defined the quantities α , (1−δ)(ρ+µ)+δLρ+µ and C , (1−δ)r0+((1−δ)ρ+δL)ξ0 (1−δ)(ρ+µ)+δL . We now compute the convergence rate of (rn)n≥0 by induction. Suppose that rn−1 ≤ C′αn−2 for some constant C′ and some n ≥ 2. We have shown in (14) that rn ≤ (1 − δ)rn−1 + Lξn−1. By using the induction hypothesis, we have rn ≤ ((1−δ)C′/α+LC)αn−1. We therefore study under which conditions we have both ((1−δ)C′/α+LC) ≤ C′ and r1 ≤ C′, which are sufficient conditions to have by induction rn ≤ C′αn−1 for all n. It is easy to show that the quantity C′ , (1−δ)r0+((1−δ)ρ+δL)ξ0δ satisfies such conditions."
    }, {
      "heading" : "E.9. Proof of Proposition 4.1",
      "text" : "Proof. We have from the strong convexity of gn:\nf(θn) ≤ gn(θn) ≤ min α∈[0,1]\n(1− α)gn(θn−1) + αgn(νn)− L\n2 α(1 − α)‖θn−1 − νn‖22,\nwhere νn is defined in Algorithm 3. Let us now consider θ ⋆ such that f(θ⋆) = f⋆. Then, we have\nf(θn) ≤ min α∈[0,1]\n(1 − α)f(θn−1) + α ( gn(νn)− L\n2 ‖νn − θn−1‖22\n)\n+ L\n2 (α− α(1 − α))‖νn − θn−1‖22.\n≤ min α∈[0,1]\n(1 − α)f(θn−1) + α ( gn(θ ⋆)− L\n2 ‖θ⋆ − θn−1‖22\n)\n+ α2LR2\n2\n≤ min α∈[0,1]\n(1 − α)f(θn−1) + αf⋆ + α2LR2\n2 ,\n(15)\nwhere we have first used the equality gn(θn−1) = f(θn−1), then the second inequality exploits gn(νn)− L2 ‖νn − θn−1‖22 ≤ gn(θ⋆) − L2 ‖θ⋆ − θn−1‖22 from the definition of νn. Finally, we use the fact that gn(θ⋆) = f⋆ + hn(θ⋆) where hn is the approximation error function gn − f with |hn(θ⋆)| ≤ L2 ‖θ⋆ − θn−1‖22 is ensured by Lemma 2.1. Minimizing (15) with respect to α and denoting by rn , f(θn)− f⋆ yields\nrn ≤ { LR2 2 if rn−1 ≤ LR2 rn−1 ( 1− rn−12LR2 ) otherwise .\nThese are the same relations used in the proof of Proposition 2.2, leading therefore to the same convergence rate."
    }, {
      "heading" : "E.10. Proof of Proposition 5.1",
      "text" : "Proof. We follow the proof techniques introduced by Nesterov (2004) using the so called “estimate sequences”, and more precisely we adapt the proof of Nesterov (2004, Theorem 2.2.8) to deal with our surrogate functions."
    }, {
      "heading" : "Preliminaries:",
      "text" : "We rely heavily on Lemma 2.1, which we recall and expand here. Let us define ρ , L+ µ. Then, for all θ in Θ,\nf(θn) ≤ f(θ) + L\n2 ‖θ − κn−1‖22 −\nρ 2 ‖θ − θn‖22\n= f(θ) + L\n2 ‖θ − κn−1‖22 −\nρ 2 ‖θ − κn−1 + κn−1 − θn‖22\n= f(θ)− µ 2 ‖θ − κn−1‖22 − ρ 2 ‖θn − κn−1‖22 + ρ(θ − κn−1)⊤(θn − κn−1).\n(16)\nTo simplify the notation in the sequel, we introduce the quantity ξn , θn − κn−1, which Nesterov (2004) calls “gradient mapping”, up to a multiplicative constant. Then, (16) can be rewritten\nf(θn) ≤ f(θ)− µ\n2 ‖θ − κn−1‖22 −\nρ 2 ‖ξn‖22 + ρ(θ − κn−1)⊤ξn. (17)\nDefinition of the estimate sequence by induction: Keeping in mind this key quantity, let us now proceed by induction to prove the main result. The recursion hypothesis Hn for n ≥ 1 is the existence of a function ḡn : Rp → R such that\n\n  \n  \nḡn(θ) = ḡ ⋆ n + γn 2 ‖θ − vn‖22 ḡn(θ) ≤ f(θ) + An2 ‖θ − θ0‖22 ∀θ ∈ Θ f(θn) ≤ ḡ⋆n (ρan + γn)κn = ρanθn + γnvn , (Hn)\nfor some vn and some values An, γn recursively defined as follows: Ak = Ak−1(1 − ak−1) and γk = (1 − ak−1)γk−1 + µak−1 for all k ≥ 2, and A1 = L, γ1 = ρ. We recall that the scalars ak are also defined in the algorithm. The functions ḡn which we are going to recursively define are related to the “estimate sequences” introduced by Nesterov (2004). Along with the quantity An, they indeed reflect the convergence rate of the algorithm, since Hn implies that f(θn)− f⋆ ≤ An2 ‖θ⋆ − θ0‖22."
    }, {
      "heading" : "Initialization of the induction for n = 1:",
      "text" : "Let us first initialize the induction, by showing that H1 is true. We remark that A1 = L and γ1 = ρ are chosen such that We can thus define\nḡ1(θ) = f(θ1) + γ1 2 ‖θ − θ1‖22.\nIn other words, we define v1 , θ1 and ḡ ⋆ 1 , f(θ1), and we obviously have the first and third conditions of H1. The second one is simply an application of Lemma 2.1, when noticing that κ0 = θ0. The last condition is also satisfied because κ1 = θ1 = v1 (since β1 = 0 in the algorithm)."
    }, {
      "heading" : "Induction argument:",
      "text" : "Since we have shown that H1 is true, we now assume Hn−1 for n ≥ 2 and show Hn. We define ḡn : Rp → R such that for all θ in Rp\nḡn(θ) = (1 − an−1)ḡn−1(θ) + an−1 ( f(θn) + µ\n2 ‖θ − κn−1‖22 +\nρ 2 ‖ξn‖22 − ρ(θ − κn−1)⊤ξn ) . (18)\nBecause of (17), the term between parenthesis on the right is smaller than f(θ) and thus, we have ḡn(θ) ≤ f(θ) + (1 − an−1)An−12 ‖θ − θ0‖22 = f(θ) + An2 ‖θ − θ0‖22, by definition of An. Thus, the second condition of Hn is true. The function ḡn is moreover quadratic and the first condition is easy to check (using appropriate values for ḡ⋆n and vn). Let us now check the third condition, namely that f(θn) ≤ minθ∈Θ ḡn(θ). We first remark that\nḡn−1(θ) = ḡ ⋆ n−1 + γn−1 2 ‖θ − vn−1‖22\n≥ f(θn−1) + γn−1 2 ‖θ − vn−1‖22 ≥ f(θn) + ρ\n2 ‖ξn‖22 − ρ(θn−1 − κn−1)⊤ξn + γn−1 2 ‖θ − vn−1‖22,\nThe first inequality comes from the induction hypothesis Hn−1 and the second inequality comes from (17). Then, we can combine this inequality with (18).\nḡn(θ) ≥ f(θn) + ρ 2 ‖ξn‖22 − (1− an)ρ(θn−1 − κn−1)⊤ξn +B(θ), (19)\nwhere\nB(θ) , (1 − an−1)γn−1\n2 ‖θ − vn−1‖22 +\nan−1µ\n2 ‖θ − κn−1‖22 − ρan−1(θ − κn−1)⊤ξn.\nNote that B(θ) is also the part of ḡn dependent on θ, and such that vn = argminθ∈Rp B(θ). Minimizing B(θ) yields\nvn = 1\nγn ((1− an−1)γn−1vn−1 + an−1µκn−1) + ρan−1 γn ξn, (20)\nwhere we recall that γn = (1− an−1)γn−1 + µan−1. Moreover, we have the convexity inequality\nB(θ) = γn 2\n(\n(1− an−1)γn−1 γn ‖θ − vn−1‖22 + an−1µ γn ‖θ − κn−1‖22\n)\n− ρan−1(θ − κn−1)⊤ξn\n≥ γn 2\n∥ ∥ ∥ ∥ θ − ( (1− an−1)γn−1 γn vn−1 + an−1µ γn κn−1 )∥ ∥ ∥ ∥ 2\n2\n− ρan−1(θ − κn−1)⊤ξn.\nand thus, using the closed form of vn computed in (20), we have\nB(vn) ≥ γn 2\n∥ ∥ ∥ ∥ ρan−1 γn ξn ∥ ∥ ∥ ∥ 2\n2\n− ρan−1(1− an−1)γn−1 γn (vn−1 − κn−1)⊤ ξn − ρ2a2n−1 γn ‖ξn‖22\n= −ρ 2a2n−1 2γn ‖ξn‖22 − ρan−1(1− an−1)γn−1 γn (vn−1 − κn−1)⊤ ξn.\nWe can now obtain the following lower-bound on ḡ⋆n , minθ∈Rp ḡn(θ), plugging the value of B(vn) into (19),\nḡ⋆n ≥ f(θn) + ( ρ 2 − ρ 2a2n−1 2γn ) ‖ξn‖22 − (1− an−1)ρ ( θn−1 − κn−1 + an−1γn−1 γn (vn−1 − κn−1) )⊤ ξn.\nGiven the definitions of γn and an, and the fact that ρa 2 0 = γ1, we also obviously have the relation ρa 2 n−1 = γn for all n ≥ 0. This cancels the factor in front of ‖ξn‖22. It is also easy to show that the fourth condition of Hn−1 implies θn−1 − κn−1 + an−1γn−1γn (vn−1 − κn−1) = 0. Since we have shown the three first conditions of Hn, it remains to show the last one, namely that (ρan+γn)κn = ρanθn + γnvn. We first remark that (20) can be rewritten\nγnvn = (1 − an−1)γn−1vn−1 + an−1(µ− ρ)κn−1 + ρan−1θn.\nCombining with the fourth condition of Hn−1, we have\nγnvn = (1− an−1) ((ρan−1 + γn−1)κn−1 − ρan−1θn−1) + an−1(µ− ρ)κn−1 + ρan−1θn = −(1− an−1)an−1ρθn−1 + ρan−1θn\n= γn\n(\nθn−1 + 1\nan−1 (θn − θn−1)\n)\n,\nwhere we use the relation ρa2n−1 = γn and the recursive relation between γn and γn−1 to remove the terms depending on κn−1 in the first equation. Now that we have a simple form describing vn, we can finally show,\nρanθn + γnvn ρan + γn = θn + γn(1/an−1 − 1) ρan + γn (θn − θn−1).\nAnd some simple computation shows that the right part of this equation is equal to κn. In other words, the factor in front of (θn − θn−1) is equal to βn, and the last condition of Hn is satisfied."
    }, {
      "heading" : "Obtaining the convergence rate:",
      "text" : "Since Hn is true for all n ≥ 1, we have f(θn)−f⋆ ≤ An2 ‖θ⋆−θ0‖22 and thus it remains to compute the convergence rate of the sequence An to prove the main result. We follow here the proof of Nesterov (2004, Lemma 2.2.4). Let us first look at the case µ = 0. It is easy to show by induction that for all n ≥ 1, we have An = La2n−1. Moreover\n1 an − 1 an−1 = an−1 − an an−1an = a2n−1 − a2n an−1an(an−1 + an) = a2n−1an an−1an(an−1 + an) = an−1 an−1 + an ≥ 1 2\nwhere we use the relation a2n = (1− an)a2n−1 and and the fact that an ≤ an−1 for all n ≥ 1. Thus, we have\n1 an − 1 a0 = 1 an − 1 ≥ n 2 ,\nand an ≤ 2/(n+ 2). Since An = La2n−1, this gives us the desired convergence rate. When µ > 0, we have the relation a2n = (1 − an)a2n−1 − µρan. It is then easy to show by induction that for all n ≥ 0, we have an ≥ √ µ ρ . Thus, An ≤ ( 1− √ µ ρ )n−1 A1. Since A1 = L, we have obtain the second convergence rate."
    }, {
      "heading" : "E.11. Proof of Proposition 6.1",
      "text" : "Proof. The proof is very similar to the one of Proposition 3.1. We proceed in several steps."
    }, {
      "heading" : "Almost sure convergence of f(θn):",
      "text" : "Let us denote by ḡn , 1 T ∑T t=1 g t n. We have the following recursion relation\nḡn = ḡn−1 + g t̂n n − gt̂nn−1,\nwhere the surrogates and the index t̂n are chosen in the algorithm. This allows us to obtain the following inequalities, which hold with probability one\nḡn(θn) ≤ ḡn(θn−1) = ḡn−1(θn−1) + gt̂nn (θn−1)− gt̂nn−1(θn−1) = ḡn−1(θn−1) + f t̂n(θn−1)− gt̂nn−1(θn−1) ≤ ḡn−1(θn−1).\nThe first inequality is true by definition of θn and the second one because ḡ t̂n n−1 is a majorant surrogate of f t̂n . The sequence (ḡn(θn))n≥0 is thus monotonically decreasing, bounded below with probability one and thus converges almost surely. Note now that the previous inequalities imply\nE[ḡn(θn)]− E[ḡn−1(θn−1)] ≤ E[f t̂n(θn−1)− gt̂nn−1(θn−1)]. (21)\nThe non-positive term E[ḡn(θn)] − E[ḡn−1(θn−1)] is the summand of a converging sum. Thus, the non-positive\nterms E[f t̂n(θn−1)− gt̂nn−1(θn−1)] is also the summand of a converging sum and we have\nE\n[ +∞ ∑\nn=0\ngt̂n+1n (θn)− f t̂n+1(θn) ] = +∞ ∑\nn=0\nE[gt̂n+1n (θn)− f t̂n+1(θn)]\n=\n+∞ ∑\nn=0\nE[E[gt̂n+1n (θn)− f t̂n+1(θn)|θn]]\n=\n+∞ ∑\nn=0\nE[ḡn(θn)− f(θn)]\n= E\n[ +∞ ∑\nn=0\nḡn(θn)− f(θn) ] < +∞,\nwhere we use two times Beppo-Lévy theorem to exchange the expectation and the sum signs in front of nonnegative quantities. As a result, the term ḡn(θn)− f(θn) converges almost surely to 0, implying the almost sure convergence of f(θn).\nAsymptotic stationary point conditions: Let us denote by h̄n , ḡn − f which is differentiable with L-Lipschitz continuous gradient. Then, for all θ in Θ,\n∇f(θn, θ − θn) = ∇ḡn(θn, θ − θn)−∇h̄n(θn)⊤(θ − θn).\nWe have ∇ḡn(θn, θ − θn) ≥ 0 by definition of θn, and ‖∇h̄n(θn)‖22 ≤ 2Lh̄n(θn), following a similar argument as in the proof of Proposition 2.1. Since we have shown that h̄n(θn) almost surely converges to zero, we conclude as in the proof of Proposition 3.1."
    }, {
      "heading" : "E.12. Proof of Proposition 6.2",
      "text" : "Proof. The almost sure convergence of f(θn) was shown in Proposition 6.1. We now prove the proposition in several steps and start with some preliminaries."
    }, {
      "heading" : "Preliminaries:",
      "text" : "Let us denote by κtn−1 the point in Θ such that g t n is in SL,ρ(f t, κtn−1). We remark that such points are drawn according to the following conditional probability distribution:\nP(κtn−1 = θn−1|θn−1) = δ and P(κtn−1 = κtn−2|θn−1) = 1− δ,\nwhere δ , 1/T . Thus we have for all t in {1, . . . , T } and all n ≥ 1,\nE[‖θ⋆ − κtn−1‖22] = E[E[‖θ⋆ − κtn−1‖22|θn−1]] = δE[‖θ⋆ − θn−1‖22] + (1− δ)E[‖θ⋆ − κtn−2‖22]. (22)\nThe other relation we need is an extension of Lemma 2.1 to the incremental setting. For all θ in Θ, we have\nf(θn) ≤ f(θ) + 1\nT\nT ∑\nt=1\n(\nL 2 ‖θ − κtn−1‖22 − ρ 2 ‖θ − θn‖22\n)\n. (23)\nThe proof of this relation is similar to that of Lemma 2.1, exploiting to ρ-strong convexity of ḡn. We can now study the first part of the proposition."
    }, {
      "heading" : "Monotonic decrease of E[f(θn)]:",
      "text" : "Note that E[gt̂nn−1(θn−1)] = E[E[g t̂n n−1(θn−1)|θn−1]] = E[ḡn−1(θn−1)]. Applying this relation to Eq. (21), we have\nE[f(θn)] ≤ E[ḡn(θn)] ≤ E[f t̂n(θn−1)] = E[E[f t̂n(θn−1)|θn−1]] = E[f(θn−1)],\nwhere the first inequality comes from the fact that f ≤ ḡn (see proof of Proposition 6.1).\nNon-strongly convex case (ρ = L); convergence rate: Denote by An , E[ 1 2T ∑T t=1 ‖θ⋆ − κtn‖22] and by ξn , 12E[‖θ⋆ − θn‖22]. Then, we have from (23) and by taking the expectation\nE[f(θn)− f⋆] ≤ LAn−1 − Lξn.\nIt follows from (22) that An = δξn + (1− δ)An−1 and thus\nE[f(θn)− f⋆] ≤ L\nδ (An−1 −An).\nBy summing the above inequalities, and using the fact that E[f(θn)− f⋆] is monotonically decreasing, we obtain that\nnE[f(θn)− f⋆] ≤ LA0 δ ,\nleading to the convergence rate of Eq. (6.2), since A0 = 1 2‖θ⋆ − θ0‖22.\nµ-strongly convex case: Suppose now that f is µ-strongly convex. We will prove the proposition by induction. Assume that for some n ≥ 1, we have An−1 ≤ βn−1ξ0 with β , (1−δ)(ρ+µ)+δLρ+µ . We have from (23) and the second-order growth condition of Lemma B.5\nµξn ≤ E[f(θn)− f⋆] ≤ LAn−1 − ρξn,\nwhich is true for all n ≥ 1. Combining the previous inequality, Eq. (22), and the induction hypothesis, we have\nAn = δξn + (1− δ)An−1 ≤ ( δL\nµ+ ρ + (1− δ)\n)\nβn−1ξ0 = β nξ0.\nSince we have A0 = ξ0, the induction hypothesis is true for all n ≥ 0. Since we have from (23) ξn ≤ Lρ+µAn−1, and E[f(θn)− f⋆] ≤ LAn−1, we finally have shown the desired convergence rate (6.2)."
    }, {
      "heading" : "E.13. Proof of Proposition D.1",
      "text" : "Proof. Let us denote by ν⋆n , argminθ∈Θ [ gn(θ) − L2 ‖θ − θn−1‖22 ]\n. Because of the separability of the surrogate function gn, we have after a few calculations\nE[f(θn)|θn−1] ≤ E[gn(θn)|θn−1] ≤ (1− δ)f(θn−1) + δ min α∈[0,1] gn((1 − α)θn−1 + αν⋆n).\nFollowing the proof of Proposition 4.1, we have\nE[f(θn)|θn−1] ≤ (1− δ)f(θn−1) + δ min α∈[0,1]\n[\n(1 − α)f(θn−1) + αf⋆ + α2LR2\n2\n]\n.\nTaking the expectation and defining rn , E[f(θn)− f⋆], we have\nrn ≤ (1− δ)rn−1 + δ min α∈[0,1]\n(1− α)rn−1 + α2LR2\n2 ,\nwhere we have used Jensen inequality similarly as in the proof of Proposition 3.2.\nMinimizing with respect to α yields\nrn ≤ (1− δ)rn−1 + δ { LR2 2 if rn−1 > LR 2\nrn−1 ( 1− rn−12LR2 ) otherwise.\nThis is the same recursive relations as in the proof of Proposition 3.2, and we therefore obtain the same convergence rate."
    }, {
      "heading" : "F. Additional Experimental Results",
      "text" : "Figures 3 and 4 presents benchmarks for ℓ2-logistic regressions with a different regularization parameter than Figure 1. Similarly, we present ℓ1-logistic regressions benchmarks in Figures 5 and 6 with a different sparsity level than Figure 2."
    }, {
      "heading" : "Supplementary References",
      "text" : "Boyd, S.P. and Vandenberghe, L. Convex Optimization. Cambridge University Press, 2004.\nClarke, F.H. Optimization and Nonsmooth Analysis. John Wiley, 1983.\nDanskin, J. M. The theory of max-min, and its application to weapons allocation problems. Ökonometrie und Unternehmensforschung, 1967.\nNocedal, J. and Wright, S.J. Numerical optimization. Springer Verlag, 2006. 2nd edition."
    } ],
    "references" : [ {
      "title" : "Optimization and Nonsmooth Analysis",
      "author" : [ "F.H. Clarke" ],
      "venue" : "John Wiley,",
      "citeRegEx" : "Clarke,? \\Q1983\\E",
      "shortCiteRegEx" : "Clarke",
      "year" : 1983
    }, {
      "title" : "The theory of max-min, and its application to weapons allocation problems",
      "author" : [ "J.M. Danskin" ],
      "venue" : "Ökonometrie und Unternehmensforschung,",
      "citeRegEx" : "Danskin,? \\Q1967\\E",
      "shortCiteRegEx" : "Danskin",
      "year" : 1967
    }, {
      "title" : "Numerical optimization",
      "author" : [ "J. Nocedal", "S.J. Wright" ],
      "venue" : null,
      "citeRegEx" : "Nocedal and Wright,? \\Q2006\\E",
      "shortCiteRegEx" : "Nocedal and Wright",
      "year" : 2006
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "The proof exploits some results from nonsmooth analysis developed by Clarke (1983). We essentially use a mean value theorem for multi-dimensional Lipschitz functions (Clarke, 1983, Proposition 2.",
      "startOffset" : 69,
      "endOffset" : 83
    }, {
      "referenceID" : 1,
      "context" : "Note that this lemma is a variant of a theorem introduced by Danskin (1967). We first prove the differentiability of f before detailing how to obtain the Lipschitz constants.",
      "startOffset" : 61,
      "endOffset" : 76
    } ],
    "year" : 2013,
    "abstractText" : "In this paper, we study optimization methods consisting of iteratively minimizing surrogates of an objective function. By proposing several algorithmic variants and simple convergence analyses, we make two main contributions. First, we provide a unified viewpoint for several first-order optimization techniques such as accelerated proximal gradient, block coordinate descent, or FrankWolfe algorithms. Second, we introduce a new incremental scheme that experimentally matches or outperforms state-of-the-art solvers for large-scale optimization problems typically arising in machine learning.",
    "creator" : "LaTeX with hyperref package"
  }
}