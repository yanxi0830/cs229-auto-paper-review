{
  "name" : "1503.00491.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ ],
    "emails" : [ "name.lastname@isti.cnr.it", "fsebastiani@qf.org.qa", "permissions@acm.org." ],
    "sections" : [ {
      "heading" : null,
      "text" : "AA:1\nThis paper is a revised and extended version of [Berardi et al. 2012]. The order in which the authors are listed is purely alphabetical; each author has given an equal contribution to this work. Authors’ address: Giacomo Berardi and Andrea Esuli, Istituto di Scienza e Tecnologie dell’Informazione, Consiglio Nazionale delle Ricerche, Via Giuseppe Moruzzi 1, 56124 Pisa, Italy. E-mail: firstname.lastname@isti.cnr.it . Fabrizio Sebastiani, Qatar Computing Research Institute, PO Box 5825, Doha, Qatar. E-mail: fsebastiani@qf.org.qa . Fabrizio Sebastiani is on leave from the Italian National Council of Research. Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies show this notice on the first page or initial screen of a display along with the full citation. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, to republish, to post on servers, to redistribute to lists, or to use any component of this work in other works requires prior specific permission and/or a fee. Permissions may be requested from Publications Dept., ACM, Inc., 2 Penn Plaza, Suite 701, New York, NY 10121-0701 USA, fax +1 (212) 869-0481, or permissions@acm.org. c© YYYY ACM 1556-4681/YYYY/01-ARTA $15.00 DOI:http://dx.doi.org/10.1145/0000000.0000000\nACM Transactions on Knowledge Discovery from Data, Vol. V, No. N, Article A, Publication date: January YYYY.\nar X\niv :1\n50 3.\n00 49\n1v 1\n[ cs\n.L G\n] 2\nM ar\n2 01\n5\nUtility-Theoretic Ranking for Semi-Automated Text Classification\nGIACOMO BERARDI, ANDREA ESULI, Italian National Council of Research FABRIZIO SEBASTIANI, Qatar Computing Research Institute\nSemi-Automated Text Classification (SATC) may be defined as the task of ranking a set D of automatically labelled textual documents in such a way that, if a human annotator validates (i.e., inspects and corrects where appropriate) the documents in a top-ranked portion of D with the goal of increasing the overall labelling accuracy of D, the expected increase is maximized. An obvious SATC strategy is to rank D so that the documents that the classifier has labelled with the lowest confidence are top-ranked. In this work we show that this strategy is suboptimal. We develop new utility-theoretic ranking methods based on the notion of validation gain, defined as the improvement in classification effectiveness that would derive by validating a given automatically labelled document. We also propose a new effectiveness measure for SATCoriented ranking methods, based on the expected reduction in classification error brought about by partially validating a list generated by a given ranking method. We report the results of experiments showing that, with respect to the baseline method above, and according to the proposed measure, our utility-theoretic ranking methods can achieve substantially higher expected reductions in classification error.\nCategories and Subject Descriptors: Information systems [Information retrieval]: Retrieval tasks and goals—Clustering and Classification; Computing methodologies [Machine learning]: Learning paradigms—Supervised learning\nGeneral Terms: Algorithm, Design, Experimentation, Measurements\nAdditional Key Words and Phrases: Text classification, supervised learning, semi-automated text classification, cost-sensitive learning, ranking"
    }, {
      "heading" : "1. INTRODUCTION",
      "text" : "Suppose an organization needs to classify a set D of textual documents under classification scheme C, and suppose that D is too large to be classified manually, so that resorting to some form of automated text classification (TC) is the only viable option. Suppose also that the organization has strict accuracy standards, so that the level of effectiveness obtainable via state-of-the-art TC technology (including any possible improvements obtained via active learning) is not sufficient. In this case, the most plausible strategy is to train an automatic classifier Φ̂ on the available training data Tr, improve it as much as possible (e.g., via active learning), classify D by means of Φ̂, and then have a human editor validate (i.e., inspect and correct where appropriate) the results of the automatic classification. The human annotator will validate only a subset D′ ⊂ D, e.g., until she is confident that the overall level of accuracy of D is sufficient, or until she runs out of time. We call this scenario semi-automated text classification (SATC).\nAn automatic TC system may support this task by ranking, after the classification phase has ended and before validation begins, the classified documents in such a way that, if the human annotator validates the documents starting from the top of the ranking, the expected increase in classification effectiveness that derives from this validation is maximized. This paper is concerned with devising good ranking strategies for this task.\nOne obvious strategy (also used in [Martinez-Alvarez et al. 2012]) is to rank the documents in ascending order of the confidence scores generated by Φ̂, so that the topranked documents are the ones that Φ̂ has classified with the lowest confidence. The rationale is that an increase in effectiveness can derive only by validating misclassi-\nACM Transactions on Knowledge Discovery from Data, Vol. V, No. N, Article A, Publication date: January YYYY.\nfied documents, and that a good ranking method is simply the one that top-ranks the documents with the highest probability of misclassification, which (in the absence of other information) we may take to be the documents which Φ̂ has classified with the lowest confidence.\nIn this work we show that this strategy is, in general, suboptimal. Simply stated, the reason is that the improvements in effectiveness that derive from correcting a false positive or a false negative, respectively, may not be the same, depending on which evaluation function we take to represent our notion of “effectiveness”. Additionally, the ratio between these improvements may vary during the validation process. In other words, an optimal ranking strategy must take into account the above improvements and how these impact on the evaluation function; we will thus look at ranking methods based on explicit loss minimization, i.e., optimized for the specific effectiveness measures used.\nThe contributions of this paper are the following. First, we develop new utilitytheoretic ranking methods for SATC based on the notion of validation gain, defined as the improvement in effectiveness that would derive by correcting a given type of mistake (i.e., false positive or false negative). Second, we propose a new evaluation measure for SATC based on a probabilistic user model, and use it to evaluate our experiments on standard text classification datasets. The results of these experiments show that, with respect to the confidence-based baseline method discussed above, our ranking methods are substantially more effective.\nThe rest of the paper is organized as follows. Section 2 reviews related work, while Section 3 sets the stage by introducing preliminary definitions and notation. Section 4 describes our base utility-theoretic strategy for ranking the automatically labelled documents, while in Section 5 we propose a novel effectiveness measure for this task based on a probabilistic user model. Section 6 reports the results of our experiments in which we test the effectiveness of ranking strategies by simulating the work of a human annotator that validates variable-sized portions of the labelled test set. In Section 7 we address a potential problem deriving from the “static” nature of our strategy, by describing a “dynamic” (albeit computationally more expensive) version of the same strategy, and draw an experimental comparison between the two. In Section 8 we acknowledge the existence of two different ways (“micro” and “macro”) of averaging effectiveness results across classes, and show that the methods we have developed so far are optimized for macro-averaging; we thus develop and test methods optimized for micro-averaged effectiveness. Section 9 concludes by charting avenues for future research."
    }, {
      "heading" : "2. RELATED WORK",
      "text" : "Many researchers have tackled the problem of how to improve on the accuracy delivered by an automatic text classifier when this accuracy is not up to the standards required by the application (as, e.g., stipulated in a Service Level Agreement).\nA standard response to this problem is to ask human annotators to label additional data that can then be used in retraining a (hopefully) more accurate classifier. This can be done via the use of active learning techniques (AL – see e.g., [Hoi et al. 2006; Tong and Koller 2001]), i.e., via algorithms that rank unlabelled documents in such a way that the top-ranked ones bring about, once manually labelled and used for retraining, the highest expected improvement in classification accuracy. Still, the improvement in accuracy that can be obtained via active learning is limited: even by using the best active learning algorithm, accuracy tends to plateau after a certain number of unlabelled documents have been manually annotated. When this plateau is reached, annotating more documents will not improve accuracy any further [Settles 2012]. Similar consid-\nACM Transactions on Knowledge Discovery from Data, Vol. V, No. N, Article A, Publication date: January YYYY.\nerations apply when active learning is carried out at the term level, rather than at the document level [Godbole et al. 2004; Raghavan et al. 2006].\nA related response to the same problem is to use training data cleaning techniques (TDC – see e.g., [Brodley and Friedl 1999; Esuli and Sebastiani 2013; Fukumoto and Suzuki 2004]), i.e., use algorithms that optimize the human annotator’s efforts at correcting possible labelling mistakes in the training set. TDC algorithms rank the training documents in such a way that the top-ranked ones bring about, once their labels are manually checked and then used for retraining, the highest expected improvement in classification accuracy. In other words, TDC is to labelled training documents what AL is to unlabelled ones. Similarly to what happens in active learning, in many applicative contexts high enough accuracy levels cannot be attained even at the price of carefully validating the entire training set for labelling mistakes.\nYet another response may be the use of some form of weakly supervised learning / semi-supervised learning, i.e., of techniques that allow training a classifier when training data are few, often leveraging unlabelled data along with the labelled training data [Chapelle et al. 2006; Zhu and Goldberg 2009]. This solution relies on the fact that unlabelled data is often available in large quantities, sometimes even from the same source where the training and test data originate. Similarly to the cases of AL and TDC, improvements with respect to the results of the purely supervised setting may be obtained, but these improvements are going to be limited anyway.\nIn conclusion, when the required accuracy standards are high, neither training data cleaning, nor active learning, nor weakly supervised / semi-supervised learning, nor a combination of them, may suffice to reach up to these standards. In this case, after either or all such techniques have been applied, we can only resort to manual validation of part of the automatically classified documents by a human annotator. Supporting this last phase is the goal of semi-automated text classification.\nAll the techniques discussed above are different from SATC, since in SATC we are not concerned with improving the quality of the trained classifier. We are instead concerned with improving the quality of the automatically classified test set, typically after all attempts at injecting additional quality in the automatic classifier (and in the training set) have proved insufficient; in particular, no retraining / reclassification phase is involved in SATC.\nActive learning. As remarked above, SATC certainly bears relations to active learning. In both SATC and in the selective sampling approach to AL ([Lewis and Catlett 1994]; also known as pool-based approach [McCallum and Nigam 1998]), the automatically classified objects are ranked and the human annotator is encouraged to correct possible misclassifications by working down from the top of the ranked list. However, as remarked above, the goals of the two tasks are different. In active learning we are interested in top-ranking the unlabelled documents that, once manually labelled, would maximize the information fed back to the learning process, while in SATC we are interested in top-ranking the unlabelled documents that, once manually validated, maximize the expected accuracy of the automatically classified document set. As a result, the optimal ranking strategies for the two tasks may be different too.\nSome approaches to AL take into account the costs of misclassification, thus attributing different levels of importance to different types of error. In [Kapoor et al. 2007] these costs are embedded into a decision-theoretic framework, which is reminiscent of our utility-theoretic framework. A value-of-information criterion is used in order to select samples which maximize profit, determined by the total risk of classification and the total cost of labelling. The total risk is formulated as a utility function in which the probability of each classification and the risk associated with it are taken into account. The concept of risk is reminiscent of the notion of “gain” defined in our utility\nACM Transactions on Knowledge Discovery from Data, Vol. V, No. N, Article A, Publication date: January YYYY.\nfunction (see Section 4.2), but its purpose is to consider the human effort needed in correcting a misclassified sample [Vijayanarasimhan and Grauman 2009]. Therefore this decision-theoretic strategy is not aimed to directly improve classification accuracy, but to minimise the manual work of the annotator, which is quantified by the risk and the cost of labelling. Semi-automated TC. While AL (and, to a much lesser degree, TDC) have been investigated extensively in a TC context, semi-automated TC has been fairly neglected by the research community. While a number of papers (e.g., [Larkey and Croft 1996; Sebastiani 2002; Yang and Liu 1999]) have evoked the existence of this scenario, we are not aware of many published papers that either discuss ranking policies for supporting the human annotator’s effort, or that attempt to quantify the effort needed for reaching a desired level of accuracy. For instance, while discussing a system for the automatic assignment of ICD9 classes to patients’ discharge summaries, Larkey and Croft [1996] say “We envision these classifiers being used in an interactive system which would display the 20 or so top ranking [classes] and their scores to an expert user. The user could choose among these candidates (...)”, but do not present experiments that quantify the accuracy that the validation activity brings about, or methods aimed at optimizing the cost-effectiveness of this activity.\nThe recent [Martinez-Alvarez et al. 2012] tackles the related problem of deciding when a document is too difficult for automated classification, and should thus be routed to a human annotator. However, the method presented in the paper is not applicable to our case, since (a) it is undefined for documents with no predicted labels (a fairly frequent case in multi-label TC), and (b) it is undefined when the classification threshold is zero (again, a fairly frequent case in modern learning algorithms).\nIn a subsequent paper [Martinez-Alvarez et al. 2013], the same authors study a family of SATC methods that exploit “document difficulty”, taking into account the confidence scores computed by the base classifiers. They also present a comparison between the techniques they propose and that presented in an earlier version of the present paper [Berardi et al. 2012]; in this comparison, the former are claimed to outperform the latter on the Reuters-21578 dataset discussed in Section 6.4. However, this comparison is incorrect since the authors compare the results of their ranking methods as applied to confidence scores generated by SVMs, with those of the [Berardi et al. 2012] ranking method as applied to confidences scores generated by a different learner. A correct comparison among ranking methods must instead be carried out by providing to all methods the same input, i.e., the same confidence scores (whose generation is not part of the method itself). The comparison reported in [Martinez-Alvarez et al. 2013] is incorrect also because it is carried out in terms of the ENERµρ measure (see Section 5.3); instead, as stated in [Berardi et al. 2012], the measure according to which the method of [Berardi et al. 2012] should be evaluated is ENERMρ , and not ENERµρ , since it is ENERMρ that that method was optimized for. In Section 8 we will indeed present SATC methods optimized for ENERµρ .\nAn application of the method discussed in Section 7 to performing SATC in a market research context is presented in [Berardi et al. 2014]."
    }, {
      "heading" : "3. PRELIMINARIES",
      "text" : "Given a set of textual documents D and a predefined set of classes C = {c1, . . . , cm}, (multi-class multi-label) TC is usually defined as the task of estimating an unknown target function Φ : D × C → {−1,+1}, that describes how documents ought to be classified, by means of a function Φ̂ : D × C → {−1,+1} called the classifier1; +1 and −1\n1Consistently with most mathematical literature we use the caret symbol (ˆ) to indicate estimation.\nACM Transactions on Knowledge Discovery from Data, Vol. V, No. N, Article A, Publication date: January YYYY.\nrepresent membership and non-membership of the document in the class. Here, “multiclass” means that there are m ≥ 2 classes, while “multi-label” refers to the fact that each document may belong to zero, one, or several classes at the same time. Multi-class multi-label TC is usually accomplished by generating m independent binary classifiers Φ̂j , one for each cj ∈ C, each entrusted with deciding whether a document belongs or not to a class cj . In this paper we will actually restrict our attention to classifiers Φ̂j that, aside from taking a binary decision Dij ∈ {−1,+1} on a given document di, also return a confidence estimate Cij , i.e., a numerical value representing the strength of their belief in the fact that Dij is correct (the higher the value, the higher the confidence). We formalize this by taking a binary classifier to be a function Φ̂j : D → R in which the sign of the returned value Dij ≡ sgn(Φ̂j(di)) ∈ {−1,+1} indicates the binary decision of the classifier, and the absolute value Cij ≡ |Φ̂j(di)| represents its confidence in the decision.\nFor the time being we also assume that\nF1(Φ̂j(Te)) = 2TPj\n2TPj + FPj + FNj (1)\n(the well-known harmonic mean of precision and recall) is the chosen evaluation measure for binary classification, where Φ̂j(Te) indicates the result of applying Φ̂j to the test set Te and TPj , FPj , FNj , TNj indicate the numbers of true positives, false positives, false negatives, true negatives in Te for class cj . Note that F1 is undefined when TPj = FPj = FNj = 0; in this case we take F1(Φ̂j(Te)) = 1, since Φ̂j has correctly classified all documents as negative examples. The assumption that F1 is our evaluation measure is not restrictive; as will be evident later on in the paper, our methods can be customized to any evaluation function that can be computed from a contingency table.\nAs a measure of effectiveness for multi-class multi-label TC, for the moment being we use macro-averaged F1 (noted FM1 ), which is obtained by computing the classspecific F1 values and averaging them across all the cj ∈ C. An alternative way of averaging across the classes (micro-averaged F1) will be discussed in Section 8.\nIn this paper the set of unlabelled documents that the classifier must automatically label (and rank) in the “operational” phase will be represented by the test set Te."
    }, {
      "heading" : "4. A RANKING METHOD FOR SATC BASED ON UTILITY THEORY",
      "text" : ""
    }, {
      "heading" : "4.1. Ranking by utility",
      "text" : "For the time being let us concentrate on the binary case, i.e., let us assume there is a single class cj that needs to be separated from its complement cj . The policy we propose for ranking the automatically labelled documents in Φ̂j(Te) makes use of utility theory, an extension of probability theory that incorporates the notion of gain (or loss) that derives from a given course of action [Anand 1993; von Neumann and Morgenstern 1944]. Utility theory is a general theory of rational action under uncertainty, and as such is used in many fields of human activity; for instance, one such field is betting, since in placing a certain bet we take into account (a) the probabilities of occurrence that we subjectively attribute to a set of outcomes (say, to the possible outcomes of a given football game), and (b) the gains or losses that we obtain, having bet on one of them, if the various outcomes materialise.\nIn order to explain our method let us introduce some basics of utility theory. Given a set A = {α1, α2, . . .} of possible courses of action and a set Ω = {ω1, ω2, . . .} of mutually disjoint events, the expected utility U(αi,Ω) that derives from choosing course of action\nACM Transactions on Knowledge Discovery from Data, Vol. V, No. N, Article A, Publication date: January YYYY.\nαi given that any of the events in Ω may occur, is defined as U(αi,Ω) = ∑ ωk∈Ω P (ωk)G(αi, ωk) (2)\nwhere P (ωk) is the probability of occurrence of event ωk and G(αi, ωk) is the gain obtained if αi is chosen and event ωk occurs. For instance, αi may be the course of action “betting on Arsenal FC’s win” and Ω may be the set of mutually disjoint events Ω = {ω1, ω2, ω3}, where ω1=“Arsenal FC wins”, ω2=“Arsenal FC and Chelsea FC tie”, and ω3=“Chelsea FC wins”; in this case,\n— P (ω1), P (ω2), P (ω3) are the probabilities of occurrence that we subjectively attribute to the three events ω1, ω2, ω3; —G(αi, ω1), G(αi, ω2), G(αi, ω3) are the economic rewards we obtain if we choose course of action αi (i.e., we bet on the win of Arsenal FC) and the respective event occurs. Of course, this economic reward will be positive if ω1 occurs and negative if either ω2 or ω3 occur.\nWhen we face alternative courses of action, acting rationally means choosing the course of action that maximises our expected utility. For instance, given the alternative courses of action α1=“betting on Arsenal FC’s win”, α2=“betting on Arsenal FC’s and Chelsea FC’s tie”, α3=“betting on Chelsea FC’s win”, we should pick among {α1, α2, α3} the course of action that maximises U(αi,Ω).\nHow does this translate into a method for ranking automatically labelled documents? Assume we have a set D = {d1, ..., dn} of such documents that we want to rank, and that cj is the class we deal with. For instantiating Equation ?? concretely we need\n(1) to decide what our set A = {α1, α2, . . .} of alternative courses of action is; (2) to decide what the set Ω = {ω1, ω2, . . .} of mutually disjoint events is; (3) to define the gains G(αi, ωk); (4) to specify how we compute the probabilities of occurrence P (ωk).\nLet us discuss each of these steps in turn. Concerning Step 1, we will take the action of validating document di as course of action αi. In this way we will evaluate the expected utility Uj(di,Ω) (i.e., the expected increase in the overall classification accuracy of Te) that derives to the classification accuracy of class cj from validating each document di, and we will be able to rank the documents by their Uj(di,Ω) value, so as to top-rank the ones with the highest expected utility.\nConcerning Step 2, we have argued in the introduction that the increase in accuracy that derives from validating a document depends on whether the document is a true positive, a false positive, a false negative, or a true negative; as a consequence, we will take Ω = {tpj , fpj , fnj , tnj}, where each of these events implicitly refers to the document di under scrutiny (e.g., tpj denotes the event “document di is a true positive for class cj”). Our utility function has thus the form\nUj(di,Ω) = ∑\nωk∈{tpj ,fpj ,fnj ,tnj}\nP (ωk)G(di, ωk) (3)\nHow to address Step 3 (defining the gains) will be the subject of Sections 4.2 and 4.3, while Step 4 (computing the probabilities of occurrence) will be discussed in Section 4.4.\nACM Transactions on Knowledge Discovery from Data, Vol. V, No. N, Article A, Publication date: January YYYY."
    }, {
      "heading" : "4.2. Validation gains",
      "text" : "We equateG(di, fpj) in Equation ?? with the average increase in F1(Φ̂j(Te)) that would derive by manually validating the label attributed by Φ̂j to a document di in FPj . We call this the validation gain of a document in FPj . Note that validation gains are independent of a particular document, i.e., G(d′, fpj) = G(d′′, fpj) for all d′, d′′ ∈ Te. Analogous arguments apply to G(di, tpj), G(di, fnj), and G(di, tnj).\nQuite evidently, G(di, tpj) = G(di, tnj) = 0, since when the human annotator validates the label attributed to di by Φ̂j and finds out it is correct, she will not modify it, and the value of F1(Φ̂j(Te)) will thus remain unchanged.\nConcerning misclassified documents, it is easy to see that, in general, G(di, fpj) 6= G(di, fnj). In fact, if a false positive is corrected, the increase in F1 is the one deriving from removing a false positive and adding a true negative, i.e.,\nG(di, fpj) = 1\nFPj (FFP1 (Φ̂j(Te))− F1(Φ̂j(Te)))\n= 1\nFPj ( 2TPj 2TPj + FNj − 2TPj 2TPj + FPj + FNj )\n(4)\nwhere by FFP1 (Φ̂j) we indicate the value of F1 that would derive by correcting all false positives of Φ̂j(Te), i.e., turning all of them into true negatives. Conversely, if a false negative is corrected, the increase in F1 is the one deriving from removing a false negative and adding a true positive, i.e.,\nG(di, fnj) = 1\nFNj (FFN1 (Φ̂j(Te))− F1(Φ̂j(Te)))\n= 1\nFNj (\n2(TPj + FNj) 2(TPj + FNj) + FPj − 2TPj 2TPj + FPj + FNj )\n(5)\nwhere by FFN1 (Φ̂j) we indicate the value of F1 that would derive by turning all the false negatives of Φ̂j(Te) into true positives.\nEquation ?? defines the gain deriving from the correction of a false positive as the average across the gains deriving from the correction of each false positive in the contingency table (and analogously for Equation ??). The advantage of such a definition is that such average gain can be computed once for all during the entire process. We will see a different definition, leading to a different SATC method, in Section 7."
    }, {
      "heading" : "4.3. Smoothing contingency cell estimates",
      "text" : "One problem that needs to be tackled in order to compute G(di, fpj) and G(di, fnj) is that the contingency cell counts TPj , FPj , FNj are not known (since in operational settings we do not know which test documents have been classified correctly and which have been instead misclassified), and thus need to be estimated2. In order to estimate them we make the assumption that the training set and the test set are independent and identically distributed. We then perform a k-fold cross-validation (k-FCV) on the training set: if by TPTrj we denote the number of true positives for class cj resulting from the k-fold cross-validation on Tr, the maximum-likelihood estimate of TPj is ˆTP ML\nj = TP Tr j · |Te|/|Tr|; same for F̂P\nML j and ˆFN ML j 3.\n2We will disregard the estimation of TNj since it is unnecessary for our purposes, given that F1(Φ̂j(Te)) does not depend on TNj . 3As in many other contexts, the assumption that the training set and the test set are independent and identically distributed may not be verified in practice; if it is not, in our case this leads to imprecise estimates of\nACM Transactions on Knowledge Discovery from Data, Vol. V, No. N, Article A, Publication date: January YYYY.\nHowever, these maximum-likelihood cell count estimates need to be smoothed, so as to avoid zero counts. In fact, if ˆTP ML\nj = 0 it would derive from Equation ?? that there is nothing to be gained by correcting a false positive, which is counterintuitive. Similarly, if F̂P ML\nj = 0 the very notion of FFP1 (Φ̂j) would be meaningless, since it does not make sense to speak of “removing a false positive” when there are no false positives; and the same goes for ˆFN ML\nj .\nA second reason why ˆTP ML j , F̂P ML j , ˆFN ML\nj need to be smoothed is that, when |Te|/|Tr| < 1, they may give rise to negative values for G(di, fpj) and G(di, fnj), which is counterintuitive. To see this, note that ˆTP ML j , F̂P ML j , ˆFN ML\nj may not be integers (which is not bad per se, since the notions of precision, recall, and their harmonic mean intuitively make sense also when we allow the contingency cell counts to be nonnegative reals instead of the usual integers), and may be smaller than 1 (this happens when |Te|/|Tr| < 1). This latter fact is problematic, both in theory (since it is meaningless to speak of, say, removing a false positive from Te when “there are less than 1 false positives in Te”) and in practice (since it is easy to verify that negative values for G(di, fpj) and G(di, fnj) may derive).\nSmoothing has extensively been studied in language modelling for speech processing [Chen and Goodman 1996] and for ad hoc search in IR [Zhai and Lafferty 2004]. However, the present context is slightly different, in that we need to smooth contingency tables, and not (as in the cases above) language models. In particular, while the ˆTP ML j , F̂P ML j , and ˆFN ML\nj are the obvious counterparts of the document model resulting from maximum-likelihood estimation, there is no obvious counterpart to the “collection model”, thus making the use of, e.g., Jelinek-Mercer smoothing problematic. A further difference is that we here require the smoothed counts not only to be nonzero, but also to be ≥ 1 (a requirement not to be found in language modelling).\nSmoothing has also been studied specifically for the purpose of smoothing contingency cell estimates [Burman 1987; Simonoff 1983]. However, these methods are inapplicable to our case, since they were originally conceived for contingency tables characterized by a small (i.e., ≤ 1) ratio between the number of observations (which in our case is |Te|) and the number of cells (which in our case is 4); our case is quite the opposite. Additionally, these smoothing methods do not operate under the constraint that the smoothed counts should all be ≥ 1, which is a hard constraint for us.\nFor all these reasons, rather than adopting more sophisticated forms of smoothing, we adopt simple additive smoothing (also known as Laplace smoothing), a special case of Bayesian smoothing using Dirichlet priors [Zhai and Lafferty 2004] which is obtained by adding a fixed quantity to each of ˆTP ML j , F̂P ML j , ˆFN ML\nj . As a fixed quantity we add 1, since it is the quantity that all our cell counts need to be greater than or equal to for Equations ?? and ?? to make sense. We denote the resulting estimates by ˆTP La j , F̂P La j , ˆFN La\nj . As it will be clear in Section 6 and following, this simple form of smoothing proves almost optimal, which seems to indicate that there is not much to be gained by applying more sophisticated smoothing methods to our problem context.\nthe contingency cell counts. While this may be suboptimal, there is practically nothing that we can do about it, since we do not know the real values of these counts; in other words, k-FCV is our “best possible shot” at estimating them in the absence of foreknowledge. As discussed in Section 6.5, we will exactly measure how suboptimal using k-FCV is, by running experiments in which an oracle feeds our utility-theoretic method with the true values of the contingency cells.\nACM Transactions on Knowledge Discovery from Data, Vol. V, No. N, Article A, Publication date: January YYYY.\nNote that we apply smoothing in an “on demand” fashion, i.e., we check if the contingency table needs smoothing at all (i.e., if any of ˆTP ML j , F̂P ML j , ˆFN ML\nj is < 1) and we smooth it only if this is the case. The reason why we adopt this “on-demand” policy will be especially apparent in Section 7."
    }, {
      "heading" : "4.4. Turning confidence scores into probabilities",
      "text" : "We derive the probabilities P (ωk) in Equation ?? by assuming that the confidence scores Cij generated by Φ̂j can be trusted (i.e., that the higher Cij , the higher the probability that Dij is correct), and by applying to Cij a generalized logistic function f(z) = eσz/(eσz + 1). This results in\nP (fpj |Dij = +1) = 1− eσCij\neσCij + 1\nP (fnj |Dij = −1) = 1− eσCij\neσCij + 1\n(6)\nThe generalized logistic function (see Figure 1) has the effect of monotonically converting scores ranging on (−∞,+∞) into real values in the [0.0,1.0] range (hence the probabilities of Equation ?? range on [0.0,0.5]). When Cij = 0 (this happens when Φ̂j has no confidence at all in its own decision Dij), then\nP (tpj |Dij = +1) = P (fpj |Dij = +1) = 0.5 P (fnj |Dij = −1) = P (tnj |Dij = −1) = 0.5\n(7)\ni.e., the probability of correct classification and the probability of misclassification are identical. Conversely, we have\nlim Cij→+∞\nP (fpj |Dij = +1) = 0\nlim Cij→+∞\nP (fnj |Dij = −1) = 0 (8)\ni.e., when Φ̂j has a very high confidence in its own decision Dij , the probability that Dij is wrong is taken to be close to 0.\nThe reason why we use a generalized version of the logistic function instead of its non-parametric version (which corresponds to the case σ = 1) is that using this latter within Equation ?? would give rise to a very high number of zero probabilities of misclassification, since the non-parametric logistic function converts every positive number above a certain threshold (≈ 36) to a number that standard implementations round up to 1 even by working in double precision. By tuning the σ parameter (the growth rate) we can tune the speed at which the right-hand side of the sigmoid asymptotically approaches 1, and we can thus tune how evenly Equation ?? distributes the confidence values across the [0.0,0.5] interval.\nThe process of optimizing σ within Equation ?? is usually called probability calibration. How we actually optimize σ is discussed in Section 6.1."
    }, {
      "heading" : "4.5. Ranking by total utility",
      "text" : "Our function Uj(di,Ω) of Section 4.1 is thus obtained by plugging Equations ?? and ?? into Equation ??. Therefore, we are now in a position to compute, given an automatically classified document di and a class cj , the utility, for the aims of increasing F1(Φ̂j(Te)), of manually validating the label Dij attributed to di by Φ̂j .\nNow, let us recall from Section 3 that our goal is addressing not just the binary, but the multi-class multi-label TC case, in which binary classification must be accom-\nACM Transactions on Knowledge Discovery from Data, Vol. V, No. N, Article A, Publication date: January YYYY.\nplished simultaneously for |C| ≥ 2 different classes. It might seem sensible to propose ranking, for each cj ∈ C, all the automatically labelled documents in Te in decreasing order of their Uj(di,Ω) value. Unfortunately, this would generate |C| different rankings, and in an operational context it seems implausible to ask a human annotator to scan |C| different rankings of the same document set (this would mean reading the same document |C| times in order to validate its labels). As suggested in [Esuli and Sebastiani 2009] for active learning, it seems instead more plausible to generate a single ranking, according to a score U(di,Ω) that is a function of the |C| different Uj(di,Ω) scores. In such a way, the human annotator will scan this single ranking from the top, validating all the |C| different labels for di before moving on to another document. As the criterion for generating the overall utility score U(di,Ω) we use total utility, corresponding to the simple sum\nU(di,Ω) = ∑ cj∈C Uj(di,Ω) (9)\nOur final ranking is thus generated by sorting the test documents in descending order of their U(di,Ω) score.\nFrom the standpoint of computational cost, this technique is O(|Te| · (|C|+ log |Te|)), since the cost of sorting the test documents by their U(·,Ω) score is O(|Te| log |Te|), and the cost of computing the U(·,Ω) score for |Te| documents and |C| classes is O(|Te| · |C|)."
    }, {
      "heading" : "5. EXPECTED NORMALIZED ERROR REDUCTION",
      "text" : "No measures are known from literature for evaluating the effectiveness of a SATCoriented ranking method ρ. We here propose such a measure, which we call expected normalized error reduction (denoted ENERρ). In this section we will introduce ENERρ in a stepwise fashion."
    }, {
      "heading" : "5.1. Error reduction at rank",
      "text" : "Let us first introduce the notion of residual error at rank n (noted Eρ(n)), defined as the error that is still present in the document set Te after the human annotator has validated the documents at the first n rank positions in the ranking generated by ρ.\nACM Transactions on Knowledge Discovery from Data, Vol. V, No. N, Article A, Publication date: January YYYY.\nThe value of Eρ(0) is the initial error generated by the automated classifier, and the value of Eρ(|Te|) is 0. We assume our measure of error to range on [0,1]; if so, Eρ(n) ranges on [0,1] too. We will hereafter call n the validation depth (or inspection depth).\nWe next define error reduction at rank n to be\nERρ(n) = Eρ(0)− Eρ(n)\nEρ(0) (10)\ni.e., a value in [0,1] that indicates the error reduction obtained by a human annotator who has validated the documents at the first n rank positions in the ranking generated by ρ; 0 stands for no reduction, 1 stands for total elimination of error.\nExample plots of the ERρ(n) measure are displayed in Figure 2, where different curves represent different ranking methods ρ′, ρ′′, ..., and where, for better convenience, the x axis indicates the fraction n/|Te| of the test set that has been validated rather than the number n of validated documents. By definition all curves start at the origin of the axes (i.e, if the annotator validates 0 test documents, no error reduction is obtained) and end at the upper right corner of the graph (i.e., if the annotator validates all the |Te| test documents, a complete elimination of error is obtained). More convex (i.e., higher) curves represent better strategies, since they indicate that a higher error reduction is achieved for the same amount of manual validation effort.\nThe reason why we focus on error reduction, instead of the complementary concept of “increase in accuracy”, is that error reduction has always the same upper bound (i.e., 100% reduction), independently of the initial error. In contrast, the increase in accuracy that derives from validating the documents does not always have the same upper bound. For instance, if the initial accuracy is 0.5, if we assume that accuracy values range on [0,1] then an increase in accuracy of 100% is indeed possible, while this increase is not possible if the initial accuracy is 0.9. This makes the notion of “increase in accuracy” less immediately interpretable, since different datasets and/or different classifiers give rise to different initial levels of accuracy. So, using “error reduction”\nACM Transactions on Knowledge Discovery from Data, Vol. V, No. N, Article A, Publication date: January YYYY.\ninstead of “increase in accuracy” makes our curves more immediately interpretable, since error reduction has the same range (i.e., [0,1]) irrespectively of dataset used and/or initial classifier used.\nSince (as stated in Section 3) we use F1 for measuring effectiveness, as a measure of classification error we use E1 ≡ (1−F1), which indeed (as assumed at the beginning of this section) ranges on [0,1]. In order to measure the overall effectiveness of a ranking method across the entire set C of classes, we compute macro-averaged E1 (noted EM1 ), obtained by computing the class-specific E1 values and averaging them across the cj ’s; from this it derives that EM1 = 1 − FM1 . By ERMρ (n) we will indicate macro-averaged ERρ(n), also obtained by computing the class-specific ERρ(n) values and averaging them across the cj ’s."
    }, {
      "heading" : "5.2. Normalized error reduction at rank ...",
      "text" : "One problem with ERρ(n), though, is that the expected ERρ(n) value of the random ranker is fairly high4, since it amounts to n|Te| . The difference between the ERρ(n) value of a genuinely engineered ranking method ρ and the expected ERρ(n) value of the random ranker is particularly small for high values of n, and is null for n = |Te|. This means that it makes sense to factor out the random factor from ERρ(n). This leads us to define the normalized error reduction of ranking method ρ as NERρ(n) = ERρ(n)− n|Te| , with macro-averaged NERρ(n) obtained as usual and denoted, as usual, by NERMρ (n)."
    }, {
      "heading" : "5.3. ... and its expected value",
      "text" : "However, NERρ(n) is still unsatisfactory as a measure, since it depends on a specific value of n (which is undesirable, since our human annotator may decide to work down the ranked list as far as she deems suitable). Following [Robertson 2008] we assume that the human annotator stops validating the ranked list at exactly rank n with probability Ps(n) (the index s stands for “stoppage”). We can then define the expected normalized error reduction of ranking method ρ on a given document set Te as the expected value of NERρ(n) according to probability distribution Ps(n), i.e.,\nENERρ = |Te|∑ n=1 Ps(n)NERρ(n) (11)\nwith macro-averaged ENERρ indicated, as usual, as ENERMρ . Different probability distributions Ps(n) can be assumed. In order to base the definition of such a distribution on a plausible model of user behaviour, we here make the assumption (along with [Moffat and Zobel 2008]) that a human annotator, after validating a document, goes on to validate the next document with probability (or persistence [Moffat and Zobel 2008]) p or stops validating with probability (1− p), so that\nPs(n) = { pn−1(1− p) if n ∈ {1, . . . , |Te| − 1} pn−1 if n = |Te| (12)\nIt can be shown that, for a sufficiently large value of |Te|, ∑|Te| n=1 n · Ps(n) (the expected number of documents that the human annotator will validate as a function of p) asymp-\n4That the expected ERρ(n) value of the random ranker is n|Te| is something that we have not tried to formally prove. However, that this holds is supported by intuition and is unequivocally shown by Monte Carlo experiments we have run on our datasets; see Figures 2 to 4 for a graphical representation.\nACM Transactions on Knowledge Discovery from Data, Vol. V, No. N, Article A, Publication date: January YYYY.\ntotically tends to 11−p . The value ξ = 1 |Te|(1−p) thus denotes the expected fraction of the test set that the human annotator will validate as a function of p.\nUsing this distribution in practice entails the need of determining a realistic value for p. A value p = 0 corresponds to a situation in which the human annotator only validates the top-ranked document, while p = 1 indicates a human annotator who validates each document in the ranked list. Unlike in ad hoc search, we think that in a SATC context it would be unrealistic to take a value for p as given irrespective of the size of Te. In fact, given a desired level of error reduction, when |Te| is large the human annotators need to be more persistent (i.e., characterized by higher p) than when |Te| is small. Therefore, instead of assuming a predetermined value of p we assume a predetermined value of ξ, and derive the value of p from the equation ξ = 1|Te|(1−p) . For example, in a certain application we might assume ξ = .20 (i.e., assume that the average human annotator validates 20% of the test set). In this case, if |Te| = 1000, then p = 1 − 1.20·1000 = .9950, while if |Te| = 10, 000, then p = 1 − 1 .20·10000 = .9995. In the experiments of Section 6 we will test all values of p corresponding to values of ξ in {.05, .10, .20}.\nNote that the values of ENERρ are bound above by 1, but a value of 1 is not attainable. In fact, even the “perfect ranker” (i.e., the ranking method that top-ranks all misclassified documents, noted Perf ) cannot attain an ENERρ value of 1, since in order to achieve total error elimination all the misclassified documents need to be validated anyway, one by one, which means that the only condition in which ENERPerf might equal 1 is when there is just 1 misclassified document. We do not try to normalize ENERρ by the value of ENERPerf since ENERPerf cannot be characterized analytically, and depends on the actual labels in the test set."
    }, {
      "heading" : "6. EXPERIMENTS",
      "text" : "We have now fully specified (Section 4) a method for performing SATC-oriented ranking and (Section 5) a measure for evaluating the quality of the produced rankings, so we are now in a position to test the effectiveness of our proposed method. In Sections 6.1 to 6.5 we will describe our experimental setting, while in Section 6.6 we will report and discuss the actual results of these experiments."
    }, {
      "heading" : "6.1. Experimental protocol",
      "text" : "Let Ω be a dataset partitioned into a training set Tr and a test set Te. In each experiment reported in this paper we adopt the following experimental protocol:\n(1) For each cj ∈ C (a) Train classifier Φ̂j on Tr and classify Te by means of Φ̂j ; (b) Run k-fold cross-validation on Tr, thereby\ni. computing TPTrj , FPTrj , and FNTrj ; ii. optimizing the σ parameter of Equation ?? (see Section 6.2 below for the\nactual optimization method used); (2) For every ranking policy ρ tested\n(a) Rank Te according to ρ; (b) Scan the ranked list from the top, correcting possible misclassifications and\ncomputing the resulting values of ENERMρ for different values of ξ.\nFor Step 1b we have used k = 10; we think this value guarantees a good tradeoff between the accuracy of the parameter estimates (which tends to increase with k) and the cost of computing these estimates (which also increases with k).\nACM Transactions on Knowledge Discovery from Data, Vol. V, No. N, Article A, Publication date: January YYYY."
    }, {
      "heading" : "6.2. Probability calibration",
      "text" : "We optimize the σ parameter by picking the value of σ that minimizes the average (across the cj ∈ C) absolute value of the difference between PosTrj , the number of positive training examples of class cj , and E[PosTrj ], the expected number of such examples as resulting from the probabilities of membership in cj computed in the k-fold crossvalidation. That is, we pool together all the training documents classified in the k-fold cross-validation phase, and then we pick\narg min σ\n1 |C| ∑ cj∈C |PosTrj − E[PosTrj ]| =\narg min σ\n1 |C| ∑ cj∈C |PosTrj − ∑ di∈Tr P (cj |di)| =\narg min σ\n1 |C| ∑ cj∈C |PosTrj − ∑ di∈Tr eσΦ̂j(di) eσΦ̂j(di) + 1 |\n(13)\nThis method is a much faster calibration method than the traditional method of picking the value of σ that has performed best in k-fold cross-validation5. In fact, unlike the latter, it does not depend on the ranking method ρ. Therefore, this method spares us from the need of ranking the training set several times, i.e., once for each combination of a tested value of σ and a ranking method ρ."
    }, {
      "heading" : "6.3. Learning algorithms",
      "text" : "As our first learning algorithm for generating our classifiers Φ̂j we use a boostingbased learner called MP-BOOST [Esuli et al. 2006]. Boosting-based methods have shown very good performance across many learning tasks and, at the same time, have strong justifications from computational learning theory. MP-BOOST is a variant of ADABOOST.MH [Schapire and Singer 2000] optimized for multi-label settings, which has been shown in [Esuli et al. 2006] to obtain considerable effectiveness improvements with respect to ADABOOST.MH. In all our experiments we set the S parameter of MP-BOOST (representing the number of boosting iterations) to 1000.\nAs the second learning algorithm we use support vector machines (SVMs). We use the implementation from the freely available LibSvm library6, with a linear kernel and parameters at their default values.\nIn all the experiments discussed in this paper stop words have been removed, punctuation has been removed, all letters have been converted to lowercase, numbers have been removed, and stemming has been performed by means of Porter’s stemmer. Word stems are thus our indexing units. Since MP-BOOST requires binary input, only their presence/ absence in the document is recorded, and no weighting is performed. Documents are instead weighted (by standard cosine-normalized tfidf ) for the SVMs experiments."
    }, {
      "heading" : "6.4. Datasets",
      "text" : "Our first dataset is the REUTERS-21578 corpus. It consists of a set of 12,902 news stories, partitioned (according to the standard “ModApté” split we have adopted) into a training set of 9603 documents and a test set of 3299 documents. The documents\n5This method is sometimes called Platt calibration (see e.g., [Niculescu-Mizil and Caruana 2005]), due its use in [Platt 2000]. However, the method was in use well before Platt’s article (see e.g., [Ittner et al. 1995, Section 2.3]). 6http://www.csie.ntu.edu.tw/∼cjlin/libsvm/\nACM Transactions on Knowledge Discovery from Data, Vol. V, No. N, Article A, Publication date: January YYYY.\nare labelled by 118 categories; the average number of categories per document is 1.08, ranging from a minimum of 0 to a maximum of 16; the number of positive examples per class ranges from a minimum of 1 to a maximum of 3964. In our experiments we have restricted our attention to the 115 categories with at least one positive training example. This dataset is publicly available7 and is probably the most widely used benchmark in text classification research; this fact allows other researchers to easily replicate the results of our experiments.\nAnother dataset we have used is OHSUMED [Hersh et al. 1994], a test collection consisting of a set of 348,566 MEDLINE references spanning the years from 1987 to 1991. Each entry consists of summary information relative to a paper published on one of 270 medical journals. The available fields are title, abstract, MeSH indexing terms, author, source, and publication type. Not all the entries contain abstract and MeSH indexing terms. In our experiments we have scrupulously followed the experimental setup presented in [Lewis et al. 1996]. In particular, (i) we have used for our experiments only the 233,445 entries with both abstract and MeSH indexing terms; (ii) we have used the entries relative to years 1987 to 1990 (183,229 documents) as the training set and those relative to year 1991 (50,216 documents) as the test set; (iii) as the categories on which to perform our experiments we have used the main heading MeSH index terms assigned to the entries. Concerning this latter point, we have restricted our experiments to the 97 MeSH index terms that belong to the Heart Disease (HD) subtree of the MeSH tree, and that have at least one positive training example. This is the only point in which we deviate from [Lewis et al. 1996], which experiments only on the 77 most frequent MeSH index terms of the HD subtree.\nThe main characteristics of our datasets, and of three variants (called REUTERS21578/10, REUTERS-21578/100, and OHSUMED-S) that will be discussed in Section 6.6, are conveniently summarized in Table I."
    }, {
      "heading" : "6.5. Lower bounds and upper bounds",
      "text" : "As the baseline for our experiments we use the confidence-based strategy discussed in Section 1, which corresponds to using our utility-theoretic method with both G(fp) and G(fn) set to 1. As discussed in Footnote ??, while this strategy has not (to the best of our knowledge) explicitly been proposed before, it seems a reasonable, common-sense strategy anyway.\n7http://www.daviddlewis.com/resources/testcollections/∼reuters21578/\nACM Transactions on Knowledge Discovery from Data, Vol. V, No. N, Article A, Publication date: January YYYY.\nWhile the confidence-based method will act as our lower bound, we have also run “oracle-based” methods aimed at identifying upper bounds for the effectiveness of our utility-theoretic method, i.e., at assessing the effectiveness of “idealized” (albeit nonrealistic) systems at our task.\nThe first such method (dubbed Oracle1) works by “peeking” at the actual values of TPj , FPj , FNj in Te, using them in the computation of G(di, fpj) and G(di, fnj), and applying our utility-theoretic method as usual. Oracle1 thus indicates how our method would behave were it able to “perfectly” estimate TPj , FPj , and FNj . The difference in effectiveness between Oracle1 and our method will thus be due to (i) the performance of the method adopted for smoothing contingency tables, and (ii) possible differences between the distribution of the documents across the contingency table cells in the training and in the test set.\nIn the second such method (Oracle2) we instead peek at the true labels of the documents in Te, which means that we will be able to (a) use the actual values of TPj , FPj , FNj in the computation of G(di, fpj) and G(di, fnj) (as in Oracle1), and (b) replace the probabilities in Equation ?? with the true binary values (i.e., replacing P (x) with 1 if x is true and 0 if x is false), after which we apply our utility-based ranking method as usual. The difference in effectiveness between Oracle2 and our method will be due to factors (i) and (ii) already mentioned for Oracle1 and to our method’s (obvious) inability to perfectly predict whether a document was classified correctly or not."
    }, {
      "heading" : "6.6. Results and discussion",
      "text" : "The results of our experiments are given in Table II, where we present the results of running, for each of two learners (MP-BOOST and SVMs) and five datasets (REUTERS-21578, OHSUMED, and three variants of them – called REUTERS21578/10, REUTERS-21578/100, OHSUMED-S – that we will introduce in Sections 6.6.2, 6.6.3, 6.6.4), our utility-theoretic method against the three methods discussed in Section 6.5. In Table II our method, Oracle1 and Oracle2 are actually indicated as U-Theoretic(s), Oracle1(s) and Oracle2(s), to distinguish them from variants (indicated as U-Theoretic(d), Oracle1(d) and Oracle2(d)) that will be described in Section 7. Table II presents ENERMρ (ξ) values for three representative values of ξ, i.e., 0.05, 0.10, and 0.20.\nFor each of two learners and five datasets, and for each pairwise combination of all the methods discussed (including those we will discuss in Section 7), we have run a paired t-test with ENERMρ (0.10) as the evaluation measure and 0.05 as the significance level, in order to determine whether the difference in performance between the two methods is statistically significant. The results of such tests are reported in Table III.\n6.6.1. Mid-sized test sets. Figure 2 plots the results, in terms of ERMρ (n), of our experiments with the MP-BOOST and SVM learners on the REUTERS-21578 dataset. The results of these experiments in terms of ENERMρ as a function of the chosen value of ξ are instead reported in Table II. The optimal value of σ returned by the k-fold crossvalidation phase is .554 for MP-BOOST and 7.096 for SVMs; these values, sharply different from 1 and from each other, clearly show the advantage of converting confidence scores into probabilities via a generalized logistic function.\nThe first insight we can draw from these results is that our U-Theoretic(s) method outperforms Baseline in a very substantial way (the paired t-test – see Table III – indicates that this difference is statistically significant). This can be appreciated both from the plots of Figures 2, in which the red curve (corresponding to U-Theoretic(s)) is markedly higher than the green curve (corresponding to Baseline), and from Table\nACM Transactions on Knowledge Discovery from Data, Vol. V, No. N, Article A, Publication date: January YYYY.\nII. In this latter, for ξ = .10 (corresponding to p = .996) our method obtains relative improvements over Baseline of +109% (MP-BOOST) and +51% (SVMs); for ξ = .20 the improvements, while not as high as for ξ = .10, are still sizeable (+84% for MP-BOOST and +34% for SVMs), while for ξ = .05 the improvements are even higher than for ξ = .10 (+128% for MP-BOOST and +69% for SVMs).\nACM Transactions on Knowledge Discovery from Data, Vol. V, No. N, Article A, Publication date: January YYYY.\nA second insight is that, surprisingly, our method hardly differs in terms of performance from Oracle1(s). The two curves can be barely distinguished in Figure 2, and in terms of ENERMρ Oracle1(s) is even slightly outperformed, in the MP-BOOST experiments, by U-Theoretic(s) (e.g., .226 vs. .222 for ξ = .10); the paired t-test (see Table III) indicates that the difference between the two methods is not statistically significant. This shows that (at least judging from these experiments) Laplace smoothing is nearly optimal, and there is likely not much we can gain from applying alternative, more sophisticated smoothing methods. This is sharply different from what happens in language modelling, where Laplace smoothing has been shown to be an underperformer [Gale and Church 1994]. The fact that with MP-BOOST our method slightly (and strangely) outperforms Oracle1(s) is probably due to accidental, “serendipitous” interactions between the probability estimation component (Equation ??) and the contingency cell estimation component of Section 4.3; in fact, the paired t-test indicates (see Table III) that this difference is not statistically significant.\nA third interesting fact is that error reduction is markedly better in the SVM experiments than in the MP-BOOST experiments. This is evident from the fact that the Figure 2 curves for SVMs are much more convex (i.e., are higher) and are closer to the optimum (i.e., closer to the Oracle2(s) curve) than the corresponding Figure 2 curves for MP-BOOST. This fact is also evident from the numerical results reported in Table II where, with U-Theoretic(s), SVMs obtain ENERMρ (.10) = .531, which is +134% better than the ENERMρ (.10) = .226 result obtained by MP-BOOST (similar\nACM Transactions on Knowledge Discovery from Data, Vol. V, No. N, Article A, Publication date: January YYYY.\nimprovements can be observed for the other methods and for the other values of ξ). This provides a striking contrast with the classification accuracy results reported in\nFigure I where, on the same dataset, MP-BOOST (EM1 = .392) substantially outperformed SVMs (EM1 = .473). It is easy to conjecture that, even if MP-BOOST yields higher classification accuracy, it generates less reliable (calibrated) confidence scores, i.e., it generates confidence scores that correlate with the ground truth worse than the SVM-generated scores.\nThe rates of improvement of U-Theoretic(s) over the baseline are instead much higher for MP-BOOST than for SVMs (e.g., for ξ = .10 these are +109% and +51%, respectively). (The same goes for the improvements of Oracle1(s) over the baseline.) This is likely due to the fact that, as observed above, the absolute values of ENERMρ (ξ) obtained by the baseline are much higher for SVMs than for MP-BOOST for all methods, so the margins of improvement with respect to the baseline are smaller for SVMs than for MP-BOOST.\n6.6.2. Small test sets. We have also run a batch of experiments aimed at assessing how the methods fare when ranking test sets much smaller than REUTERS-21578. This may be more challenging than ranking larger sets since, when the test set is small, Laplace smoothing (i) can seriously perturb the relative proportions among the cell counts, which can generate poor estimates of G(di, fpj) and G(di, fnj), and (ii) is performed for more classes, since (as discussed at the end of Section 4.3) we smooth “on demand” only, and since the likelihood that ˆTP ML j , F̂P ML j , ˆFN ML\nj are smaller than 1 is higher with small test sets. This is also a realistic setting since, if a set of unlabelled documents is small, it is likely that validating a portion of it that can lead to sizeable enough effectiveness improvements is feasible from an economic point of view.\nRather than choosing a completely different dataset, we generate 10 new test sets by randomly splitting the REUTERS-21578 test set in 10 equally-sized parts (about 330 documents each). In our experiments we run each ranking method on each such part individually and average the results across the 10 parts. We call this experimental scenario REUTERS-21578/10. This allows us to study the effects of test set size on our methods in a more controlled way than if we had picked a completely different dataset, since test set size is the only difference with respect to the previous REUTERS-21578 experiments.\nThe results displayed in Figure 3 allow us to visually appreciate that U-Theoretic(s) substantially outperforms Baseline also in this context. This can be seen also from Table II: for ξ = .10 the relative improvement over Baseline is +110% for MP-BOOST and +30% for SVMs, and similarly substantial improvements are obtained for the two other values of ξ tested.\nIncidentally, note that the REUTERS-21578/10 experiments model an application scenario in which a set of automatically labelled documents is split (e.g., to achieve faster throughput) among 10 human annotators, each one entrusted with validating a part of the set. In this case, each annotator is presented with a ranking of her own document subset, and works exclusively on it8.\n6.6.3. Tiny test sets. In further experiments that we have run, we have split the REUTERS-21578 test set even further, i.e., into 100 equally-sized parts of about 33 documents each, so as to test the performance of Laplace smoothing methods in even\n8Actually, if we did have k annotators available, the best strategy would be to generate the k rankings in a “round robin” fashion, i.e., by allotting to annotator i the documents ranked (in the global ranking) at the positions r such that (r mod k) = i. This splitting method would guarantee that only the most promising documents are validated by the annotators.\nACM Transactions on Knowledge Discovery from Data, Vol. V, No. N, Article A, Publication date: January YYYY.\nmore challenging conditions. We call this experimental scenario REUTERS-21578/100. From an application point of view this is a less interesting scenario than the two previously discussed ones, since applying a ranking method to a set of 33 documents only is of debatable utility, given that a human annotator confronted with the task of validating just 33 documents can arguably check them all without any need for ranking. The goal of these experiments is thus checking whether our method can perform well even in extreme, albeit scarcely realistic, conditions.\nThe detailed ERMρ (n) plots for this REUTERS-21578/100 scenario are presented in Figure 4, while the ENERMρ results are reported in Table II9. U-Theoretic(s) still outperforms Baseline, with a relative improvement of +42% with MP-BOOST and +21% with SVMs with ξ = .10, corresponding to p = .696; qualitatively similar improvements are obtained with the other tested values of ξ.\nNote that in these experiments, unlike in those performed on the full REUTERS21578, the Oracle1(s) method proves to be markedly superior to U-Theoretic(s) (e.g., .247 vs. .172 in terms of ENERMρ (.10) with MP-BOOST, and similarly for other values of ξ and for the SVM learner); unlike in the previous two datasets, the difference between the two methods turns out to be statistically significant. The reason is that, for a smaller test set, (a) distribution drift is higher, (b) “smoothing on demand” is invoked more frequently (because the likelihood that contingency table cells have a value ≤ 1 is higher), and (c) when smoothing is indeed applied the distribution across the cells of the contingency table is perturbed more strongly.\nNote also that the ERMρ (n) curves are smoother than the analogous curves for the full REUTERS-21578 and, although to a lesser extent, those for REUTERS-21578/10. This is due to the fact that the curves in Figure 4 result from averages across 100 different experiments, and the increase brought about at rank n is actually the average of the increases brought about at rank n in the 100 experiments.\n9From the next experiments onwards, for reasons of space we will not include the full plots in the style of Figures 2 to 4, and will only report ENERMρ results.\nACM Transactions on Knowledge Discovery from Data, Vol. V, No. N, Article A, Publication date: January YYYY.\n6.6.4. Large test sets. While in the previous sections we have discussed experiments on mid-sized to small (or very small) datasets, we now look at larger datasets such as OHSUMED. The OHSUMED results in Table II confirm the quality of U-Theoretic(s), which outperforms the purely confidence-based baseline by +10% (MP-BOOST) and +9% (SVMs) in terms of ENERMρ (.10); qualitatively similar improvements are obtained for the other two values of ξ studied.\nThe OHSUMED collection is characterized by the presence of an unusually large number (93.1% of the entire lot) of unlabelled documents (i.e., documents, that are negative examples for all cj ∈ C) that originally belonged to other subtrees of the MeSH tree. Since such a large percentage is unnatural, we have generated (and also used in our experiments) a variant of OHSUMED (called OHSUMED-S) by removing all the unlabelled documents from both the training set and the test set.\nAs illustrated in Table II, on OHSUMED-S U-Theoretic(s) outperforms the confidence-based baseline by a very large margin (+374% with MP-BOOST and +127% with SVMs for ξ = .10, with qualitatively similar results for the other two tested values of ξ).\n6.6.5. Discussion. In sum, the results discussed from Section 6.6.1 to the present one have unequivocally shown that U-Theoretic(s) outperforms the confidence-based baseline, usually by a large or very large margin, for all the five tested datasets and for both tested learners.\nNote that, for all five datasets and for both learners, the improvements of the utilitytheoretic methods over Baseline are larger for smaller values of ξ. This indicates that the difference between the two methods is larger for smaller validation depths, i.e., where using the utility-theoretic method pays off the most is at the very top of the ranking. This is an important feature of this method, since it means that all human annotators, be they persistent or not (i.e., independently of the depth at which they validate), are going to benefit from this approach.\nACM Transactions on Knowledge Discovery from Data, Vol. V, No. N, Article A, Publication date: January YYYY."
    }, {
      "heading" : "7. AN IMPROVED, “DYNAMIC” RANKING FUNCTION FOR SATC",
      "text" : "The utility-theoretic method discussed in Section 4 is reasonable but, in principle, suboptimal, and its suboptimality derives from its “static” nature. To see this, assume that the system has ranked the test documents according to the strategy above, that the human annotator has started from the top of the list and validated the labels of document di, that she has found out that its label assignment for class cj is a false negative, and that she has corrected it, thus bringing about an increase in F1 equivalent to\n2(TPj + 1) 2(TPj + 1) + FPj + (FNj − 1) − 2TPj 2TPj + FPj + FNj (14)\nFollowing this correction, the value of FNj is decreased by 1 and the value of TPj is increased by 1. This means that, when another false negative for cj is found and corrected, the value of (??) has changed. In other words, the improvement in F1 due to the validation of a false negative is not constant through the validation process. Of course, similar considerations apply for false positives.\nThis suggests redefining the validation gains defined in Equations ?? and ?? as\nG(di, fpj) = 2TPj 2TPj + (FPj − 1) + FNj − 2TPj 2TPj + FPj + FNj\nG(di, fnj) = 2(TPj + 1) 2(TPj + 1) + FPj + (FNj − 1) − 2TPj 2TPj + FPj + FNj\n(15)\nTo see the novelty introduced with respect to Equation ??, in the following we will discuss the case of false negatives; the case of false positives is completely analogous. The difference between Equation ?? and Equation ?? is that the former equates G(di, fnj) with the increase in F1(Φ̂j(Te)) that would derive by correcting all of the documents in FNj divided by their number, while the latter equates G(di, fnj) with the increase in F1(Φ̂j(Te)) that would derive by correcting the next document in FNj . In other words, we might say that Equation ?? enforces the notion of average gain, while Equation ?? enforces the notion of pointwise gain10. The two versions return different values of G(di, fnj): as the following example shows, it is immediate to verify that if FNj contains more than one document, the validation gainsG(di, fnj) that derive by correcting different documents are the same (by definition) if we use Equation ?? but are not the same if we use Equation ??.\nExample 7.1. Suppose we have classified a set of 100 documents according to class cj , and that the classification is such that TPj = 10, FNj = 20, FPj = 30, and TNj = 40. According to Equation ??, G(di, fnj) evaluates to ≈ 0.0190 for each false negative corrected. Instead, according to Equation ??, G(di, fnj) evaluates to ≈ 0.0241 for the 1st false negative corrected, ≈ 0.0235 for the 2nd, ≈ 0.0228 for the 3rd, ..., down to ≈ 0.0147 for the 20th.\nGiven this new definition we may implement a dynamic strategy in which, instead of plainly sorting the test documents in descending order of their U(di,Ω) score, after each correction is made we update ˆTP La j , F̂P La j , ˆFN La j by adding and subtracting 1 where\n10Equations ?? might have also been formulated in a continuous way, i.e., as partial derivatives of F1 in the two variables TPj and TNj (in other words, Equations ?? would thus represent the gradient of F1). We have preferred to stick to a discrete formulation, since (a) Equations ?? and ?? are instead not naturally formulated as derivatives (exactly because they represent average – rather than pointwise – gains), and since (b) having Equations ??, ?? and ?? all formulated in a common notation allows an easier comparison among them.\nACM Transactions on Knowledge Discovery from Data, Vol. V, No. N, Article A, Publication date: January YYYY.\nappropriate, we recompute G(di, fpj), G(di, fnj) and U(di,Ω), and we use the newly computed U(di,Ω) values when selecting the document that should be presented next to the human annotator. In detail, the following steps are iteratively performed:\n(1) For all classes cj ∈ C, compute G(di, fpj) and/or G(di, fnj) using Equations ??; (2) If the human annotator does not want to stop validating documents, then identify\nthe document dmax ≡ arg max di∈Te U(di,Ω) for which total utility is maximised;\n(3) Remove dmax from Te; (4) For all cj ∈ C, have the human annotator check the label attached by Φ̂j to dmax;\nif all these labels are correct go to Step 2; else, for all classes cj ∈ C for which the label attached by Φ̂j to dmax is incorrect: (a) Have the human annotator correct the label; (b) If dmax was a false positive for cj , decrease F̂P La j by 1; if it was a false negative\nfor cj , increase ˆTP La j by 1 and decrease ˆFN La j by 1;\n(c) Re-smooth ˆTP La j , F̂P La j , ˆFN La\nj if needed; (d) Recompute G(di, fpj) and/or G(di, fnj) and go back to Step 2.\nThis might also be dubbed an incremental ranking strategy, in the sense pioneered in [Aalbersberg 1992] for relevance feedback in ad-hoc search, in the sense that the values of G(di, fpj) and G(di, fnj) are incrementally updated so that the U(di,Ω) function reflects the fact that part of Te has indeed been corrected. In keeping with [Brandt et al. 2011] we prefer to call it a dynamic strategy, and to call the one of Section 4 a static one.\nNote that in Step 2 we simply compute the maximum element (according to U(di,Ω)) of Te instead of sorting the entire set, since we can perform this step in O(|Te|) instead of O(|Te| log |Te|)11. Furthermore, note that in this algorithm the re-computation of Uj(di,Ω) does not entail the recomputation of the probabilities P (fpj) and/or P (fnj) of Equation ??, since these probabilities are computed (i.e., calibrated) once for all, immediately after the training phase.\nNote also that computing validation gains via Equations ?? and ?? is the only possibility within the static method (since the values of G(di, fpj) and G(di, fnj) produced must be used unchanged throughout the process), but is clearly inadequate in a dynamic context, in which validation gains are always supposed to be up-to-date reflections of the current situation.\nThe dynamic nature of this method makes it clear why, as specified at the end of Section 4.3, we smooth the cell count estimates only “on demand” (see also Step 4c of the above algorithm), i.e., only if any of ˆTP ML j , F̂P ML j , ˆFN ML j is < 1. To see this, suppose that we smooth ˆTP ML j , F̂P ML j , ˆFN ML\nj at each iteration, even when not strictly needed. Adding a count of one to each of them at each iteration means that, after k iterations, k counts have been added to each of them; this means that, after many iterations, the counts added to the cells have completely disrupted the relative proportions among the cells that result from the maximum-likelihood estimation. This would likely make the dynamic method underperform the static method, which does not suffer from this problem since the maximum-likelihood estimates are smoothed only once. As a result,\n11When computing this maximum element returns repeatedly a document whose labels are all correct, the lack of a sorting step entails the need of computing the maximum element several times in a row with the values of G(di, fpj) and G(di, fnj) unchanged. In these cases, the presence of a sorting step would thus have been advantageous. However, the likelihood that this situation occurs tends to be small, especially when |C| is large, thus making the computation of the maximum element preferable to sorting.\nACM Transactions on Knowledge Discovery from Data, Vol. V, No. N, Article A, Publication date: January YYYY.\nwe smooth a contingency table only when strictly needed, i.e., when one of ˆTP ML\nj ,\nF̂P ML j , ˆFN ML\nj is < 1. By solving the inequality G(di, fnj) > G(di, fpj) we may find out under which conditions correcting a false negative yields a higher gain than correcting a false positive. It turns out that, when validation gains are defined according to Equation ??, G(di, fnj) > G(di, fpj) whenever FN + FP > 1, i.e., practically always. Of course, this need not be the case for evaluation functions different from F1, and in particular for instances of Fβ with β 6= 1.\nFrom the standpoint of total computational cost, our dynamic technique is O(|Te| · (|C| + |Te|)), since (i) computing the U(di,Ω) score for |Te| documents and computing their maximum according to the computed U(di,Ω) score can be done in O(|Te| · |C|) steps, and (ii) this step must be repeated O(|Te|) times. This policy is thus, as expected, computationally more expensive than the previous one."
    }, {
      "heading" : "7.1. Experiments",
      "text" : "The results of the experiments with the dynamic version of our utility-theoretic method and of our two oracle-based methods are reported in Figures 2 to 4 and in Table II, where they are indicated as U-Theoretic(d), Oracle1(d) and Oracle2(d). Of course there exists no dynamic version of the baseline method, since this latter does not involve validation gains.\nThe first observation that can be drawn from these results is the fact that UTheoretic(d) is not superior to U-Theoretic(s), as could instead have been expected. In fact, in Figures 2 to 4 the curves corresponding to the former are barely distinguishable from those corresponding to the latter, and the numeric results reported in Table II show no substantial difference either; as reported in Table III, in 7 out of 10 cases (2 learners × 5 datasets) the difference is not statistically significant. Note that there are extremely small differences also between Oracle1(s) and Oracle1(d); again, in 7 out of 10 cases no statistically significant difference can be detected. This shows that the lack of any substantial difference between static and dynamic is not due to a possible suboptimality of the method for estimating contingency table cells (including the method adopted for smoothing the estimates). Analogously, note also the extremely small differences between Oracle2(s) and Oracle2(d) (again, no statistically significant difference in 7 out of 10 cases), which indicates that the culprit is not the method for estimating the probabilities of misclassification.\nThis substantial equivalence between the static and the dynamic methods is somehow surprising, since on a purely intuitive basis the dynamic method seems definitely superior to the static one. We think that the reason for this apparently counterintuitive results is that, when validation gains are recomputed in Step 4d of the algorithm, the magnitude of the update (i.e., the difference between validation gains before and after the update) is too small to make an impact. This is especially true for large test sets, where incrementing or decrementing by 1 the value of a contingency cell makes too tiny a difference, since that value is very large.\nActually, the part of Figure 2 relative to MP-BOOST displays an apparently strange phenomenon, i.e., the fact that for some values of ξ the Oracle2(s) method outperforms Oracle2(d). A similar phenomenon can be noticed in some of the cells of Table II, where the static version of either Oracle1 or Oracle2 outperforms, even if by a small margin, the dynamic version. This seems especially strange for Oracle2(d), which is the theoretically optimal method (since it is a method that operates with perfect foreknowledge), and as such should be impossible to beat. The reason for this apparently counterintuitive behaviour lies not in the ranking methods, but in a counterintuitive property of F1, i.e., the fact that, when TP = FN = 0 (i.e., there are no positives in the gold\nACM Transactions on Knowledge Discovery from Data, Vol. V, No. N, Article A, Publication date: January YYYY.\nstandard – and 25 out of 115 classes in the dataset used in Figure 2 have this property), its value is 0 when FP > 0 but 1 when FP = 0 (so, TP = FP = FN = 0 is a “point of discontinuity” for F1). This essentially means that, when TP = FN = 0 and FP > 0, G(di, fnj) is 1/|FP | for the static method and 0 for the dynamic method; i.e., in this case the dynamic method does not provide any incentive for correcting a false positive, while the static method does. As a result, the static method can speed up the correction of false positives more than the dynamic method does. As mentioned above, this phenomenon exposes a suboptimality not of the dynamic method, but of the F1 function.\nIn Table IV we report the actual computation times incurred by both U-Theoretic(s) and U-Theoretic(d) on our five datasets12. These figures confirm that the dynamic method is (as already discussed above) substantially more expensive to run than the static method; in particular, the magnitude of this difference, together with the marginal (if any) accuracy improvements brought about by the dynamic method over the static one, shows that the static method is much more cost-effective than the dynamic one. In other words, the bad news is that the dynamic method brings about no improvement; the good news is that the computationally cheaper static method is hard to beat."
    }, {
      "heading" : "8. A “MICRO-ORIENTED” RANKING FUNCTION FOR SATC",
      "text" : "In Section 3 we have assumed that the evaluation of classification algorithms across the |C| classes of interest is performed by macro-averaging the F1 results obtained for the individual classes cj ∈ C. Consistently with this view, in Section 5 we have introduced macro-averaged versions of E1, ERρ, NERρ, and ENERρ. macro-averaging across the classes in |C| essentially means paying equal attention to all of them, irrespective of their frequency or other such characteristics.\n12The times reported are relative to an experiment in which the entire test set is validated; this is because, in a simulated experiment, the entire test set must be validated in order to compute the ERMρ (n) values reported in Figures 2 to 4. In a realistic setting in which only a portion of the ranked list is validated, the difference between U-Theoretic(s) and U-Theoretic(d) is smaller, since the cost of recomputing validation gains is roughly proportional to the validation depth, and since this cost affects U-Theoretic(d) but not UTheoretic(s).\nACM Transactions on Knowledge Discovery from Data, Vol. V, No. N, Article A, Publication date: January YYYY.\nHowever, there is an alternative, equally important way to evaluate effectiveness when a set of |C| classes is involved, namely, micro-averaged effectiveness. While macro-averaged measures are computed by first computing the measure of interest individually on each class-specific contingency table and then averaging the results, micro-averaged measures are computed by merging the |C| contingency tables into a single one (via summing the values of the corresponding cells) and then computing the measure of interest on the resulting table. For instance, micro-averaged F1 (noted Fµ1 ) is obtained by (i) computing the category-specific values TPj , FPj and FNj for all cj ∈ C, (ii) obtaining TP as the sum of the TPj ’s (same for FP and FN ), and then (iii) applying Equation ??. Measures such as Eµ1 , ERµρ , NERµρ , and ENERµρ are defined in the obvious way. The net effect of using a single, global contingency table is that micro-averaged measures pay more attention to more frequent classes, i.e., the more the members of a class cj in the test set, the more the measure is influenced by cj .\nNeither macro- nor micro-averaging are the “right” way to average in evaluating multi-label multi-class classification; it is instead the case that in some applications we may want to pay equal attention to all the classes (in which case macro-averaging would be our evaluation method of choice), while in some other applications we may want to pay more attention to the most frequent classes (in which case we should opt for micro-averaging).\nWhile we have not explicitly discussed this, the method of Section 4 was devised with macro-averaged effectiveness in mind. To see this, note that the U(di,Ω) function of Equation ?? is based on an unweighted sum of the class-specific Uj(di,Ω) scores, i.e., it pays equal importance to all classes in C. This means that Equation ?? is optimized for metrics that also pay equal attention to all classes, as all macro-averaged measures do. We now describe a way to modify the method of Section 4 in such a way that it is instead optimal when our effectiveness measure of choice (e.g., ENERρ) is micro-averaged. To do this, we do away with Equation ?? and (similarly to what happens for Fµ1 and E µ 1 ) compute instead U(di,Ω) directly on a single, global contingency table obtained by the cell-wise sum of the class-specific contingency tables. That is, we redefine U(di,Ω) as\nU(di,Ω) = ∑ cj∈C ∑ ωk∈{tpj ,fpj ,fnj ,tnj} P (ωk)G(di, ωk) (16)\nwhere\nG(di, fpj) = 1\nFP (FFP1 (Φ̂(Te))− F1(Φ̂(Te)))\n= 1\nFP (\n2TP 2TP + FN − 2TP 2TP + FP + FN )\nG(di, fnj) = 1\nFN (FFN1 (Φ̂(Te))− F1(Φ̂(Te)))\n= 1\nFN (\n2(TP + FN) 2(TP + FN) + FP − 2TP 2TP + FP + FN )\n(17)\nEquations ?? are the same as Equation ?? and ??, but for the fact that the latter are class-specific (as indicated by the index j) while the former are global. This is due to the fact that, when using micro-averaging, there is a single contingency table, and the gain obtained by correcting, say, a false positive for cx is equal to the gain obtained by correcting a false positive for cy, for any cx, cy ∈ C. Of course, Equations ?? are to be applied when the static method of Section 4 needs to be optimized for micro-averaging; when we instead want to do the same optimization for the dynamic method of Section 7, we need instead to apply, in the obvious way, “global” versions of Equations ??.\nACM Transactions on Knowledge Discovery from Data, Vol. V, No. N, Article A, Publication date: January YYYY.\nActually, a second aspect in the method of Section 4 that we need to change in order for it to be optimized for micro-averaging is the probability calibration method discussed in Section 6.2. In fact, Equation ?? is clearly devised with macro-averaging in mind, since it minimizes the average across the cj ∈ C of the difference between the number PosTrj and the expected number E[PosTrj ] of positive training examples of class cj . Again, all classes are given equal attention. For our micro-averaging-oriented method we thus replace Equation ?? with\narg min σ |PosTr − E[PosTr]| = arg min σ | ∑ cj∈C PosTrj − ∑ cj∈C E[PosTrj ]| =\narg min σ | ∑ cj∈C PosTrj − ∑ cj∈C ∑ di∈Tr P (cj |di)| =\narg min σ | ∑ cj∈C PosTrj − ∑ cj∈C ∑ di∈Tr eσΦ̂j(di) eσΦ̂j(di) + 1 |\n(18)\nwhere the difference between the number and the expected number of training examples in the global contingency table is minimized. It is easy to verify that the two methods may return different values of σ, as the following example shows.\nExample 8.1. Suppose that C = {c1, c2}, that PosTr1 = 20 and that PosTr2 = 10. Suppose that when σ = a then E[PosTr1 ] = 18 and E[PosTr2 ] = 8, while when σ = b then E[PosTr1 ] = 17 and E[PosTr2 ] = 13. According to Equation ?? value a is better than b (since 1|C| ∑ cj∈C |Pos Tr j − E[PosTrj ]| is equal to 2 for σ = a and to 3 for σ = b), but according to Equation ?? value b is better than a (since |PosTr −E[PosTr]| is equal to 4 for σ = a and to 0 for σ = b).\nThe same smoothing methods as discussed in Section 4.3 can instead be used; however, note that smoothing is likely to be needed much less frequently (if at all) here since, given that we now have a single global contingency table, it is much less likely that any of its cells have values < 1."
    }, {
      "heading" : "8.1. Experiments",
      "text" : "The experiments with our “micro-oriented” methods are reported in Table V. Note that, since the method we use as baseline corresponds (as noted in Section 6.5) to using UTheoretic(s) with all validation gains set to 1, the baseline we use here is different from the baseline we had used in Section 6.6, since the latter was optimized for macroaveraging while the one we use here is optimized for micro-averaging. This guarantees that, in both cases, our baselines are strong ones.\nThe results show that utility-theoretic methods bring about a much slighter improvement with respect to the baseline, compared to what we have seen for the macro-oriented methods. For instance, for the SVM learner, REUTERS-21578 dataset, and validation depth ξ = .10, the improvement of our (static) micro-oriented utilitytheoretic method with respect to the baseline is just +2%, while the improvement was +51% for the equivalent macro-oriented method. Across the two ranking methods (static and dynamic), five datasets, two learners, and three values of inspection depth studied, improvements range from -1% (i.e., in a few peculiar cases we even have a small deterioration) to +14%, much smaller than in the macro-oriented case in which the improvements ranged between +2% and +402%.\nACM Transactions on Knowledge Discovery from Data, Vol. V, No. N, Article A, Publication date: January YYYY.\nThe main reason for these much smaller improvements lies in the combined action of two factors. The first factor is that the validation gains of Equations ?? are computed on the global contingency table, whose cells contain very large numbers, |C| times larger than the values in the local contingency tables of the macro-oriented method. This means that, since the values of the validation gains are very small (given\nACM Transactions on Knowledge Discovery from Data, Vol. V, No. N, Article A, Publication date: January YYYY.\nthat an increase or a decrease by 1 of very large values brings about little difference), the difference between G(di, fpj) and G(di, fnj) is even smaller. This makes the difference between the utility-theoretic methods and the baseline smaller. The second factor is that the utility function of Equation ??, by collapsing all the class-specific utility values for a document into a single value, tends to dwarf the differences between the documents.\nIt should also be noted that, in the micro-oriented method, improvements are small also because the margins of improvement are small. To witness, the improvements brought about by Oracle2(d) (our theoretical upper bound) with respect to the baseline are smaller than for the macro-oriented method. For instance, for the MP-BOOST learner, REUTERS-21578 dataset, and validation depth ξ = .10, this improvement is +168%, while it was +571% for the macro-oriented method. So, improving over the baseline is more difficult for the micro-oriented method than for the macro-oriented one. The reason why the margins of improvement are smaller is that, when accuracy is evaluated at the macro level, the infrequent classes play a bigger role than when evaluating at the micro level. Infrequent classes are such that a large reduction in error can be achieved even by validating a few documents of the right type (i.e., false negatives). As a consequence, for the infrequent classes a ranking method that pays attention to validation gains has the potential to obtain sizeable improvements in accuracy right from the beginning; and a method that favours the infrequent classes tends to shine when evaluated at the macro level."
    }, {
      "heading" : "9. CONCLUSIONS",
      "text" : "We have presented a range of methods, all based on utility theory, for ranking the documents labelled by an automatic classifier. The documents are ranked in such a way as to maximize the expected reduction in classification error brought about by a human annotator who validates a top-ranked subset of the ranked list. We have also proposed an evaluation measure for such ranking methods, based on the expectation of the (normalized) reduction in error brought about by the human annotator’s validation activity. This “semi-automated document classification” task is different from “soft (document-ranking) classification”, since in the latter case it is the documents with the highest probability of being members of the class (and not the ones which bring about the highest expected utility if validated) that are top-ranked.\nExperiments carried out on standard datasets and variants thereof show that the intuition of using utility theory is correct. In particular, of four methods studied, we have found that two methods optimized for micro-averaged effectiveness bring about only limited improvements, while the two methods optimized for macro-averaged effectiveness deliver drastically improved performance with respect to the baseline. We have also found that the two “static” methods, while seemingly inferior to the “dynamic” ones on a purely intuitive basis, perform as well as the dynamic ones at a fraction of the computational cost.\nIt should be remarked that the very fact of using a utility function, i.e., a function in which different events are characterized by different gains, makes sense here since we have adopted an evaluation function, such as F1, in which correcting a false positive or a false negative brings about different benefits to the final effectiveness score. If we instead adopted standard accuracy (i.e., the percentage of binary classification decisions that are correct) as the evaluation measure, utility would default to the probability of misclassification, and our method would coincide with the baseline, since correcting a false positive or a false negative would bring about the same benefit. The methods we have presented are justified by the fact that, in text classification and in other classification contexts in which imbalance is the rule, F1 is the standard evaluation function,\nACM Transactions on Knowledge Discovery from Data, Vol. V, No. N, Article A, Publication date: January YYYY.\nwhile standard accuracy is a deprecated measure because of its lack of robustness to class imbalance (see e.g., [Sebastiani 2002, Section 7.1.2] for a discussion of this point).\nThe methods we have proposed are valid also when a different instantiation of the Fβ function (i.e., with β 6= 1) is used as the evaluation function. This may be the case, e.g., when classification is to be applied to a recall-oriented task (such as e-discovery [Oard et al. 2010; Oard and Webber 2013]), in which case values β > 1 are appropriate. In these cases our utility-theoretic method can be used once the appropriate instance of Fβ is plugged, in place of F1, into the equations defining the validation gains (and into the equations that lead to the definition of ENERρ(ξ)). The same trivially holds for any other evaluation function, even different from Fβ and even multivariate and nonlinear, provided it can be computed from a contingency table. It is easy to foresee that, the higher the difference between the roles that false positives and false negatives play into the chosen function, the bigger the improvements brought about by the utilitytheoretic methods with respect to the baseline are going to be. (For instance, it is easy to foresee that these improvements would be higher for F2 than for F1.)\nWe also remark that this technique is not limited to text classification, but can be useful in any classification context in which class imbalance [He and Garcia 2009], or cost-sensitivity in general [Elkan 2001], suggest using a measure (such as Fβ) that caters for these characteristics.\nNote that, by using our methods, it is also easy to provide the human annotator with an estimate of how accurate the labels of the test set are as a result of her validation activity. In fact, if the contingency cell maximum-likelihood estimates ˆTP ML j , F̂P ML j , and ˆFN ML\nj (see Section 4.3) are updated (adding and subtracting 1 where appropriate) after each correction by the human annotator, at any point in the validation activity these are up-to-date estimates of how well the test set is now classified, and from these estimates F1 (or other) can be computed as usual.\nIn the future, we would like to try applying a SATC method after a transductive learner (e.g., Transductive SVMs [Joachims 1999]) has been used to generate the base classifier in place of the standard inductive learners we have used in this work. A transductive method, rather than attempting to generate a model that minimizes the expected risk on any test set, attempts to minimize misclassifications on a specific test set. When the focus of one’s application is squeezing the highest possible accuracy from a specific test set, as is the case when using SATC, it would thus make sense to use a transductive instead of an inductive learning method."
    }, {
      "heading" : "10. ACKNOWLEDGMENTS",
      "text" : "We would like to thank David Lewis and Diego Marcheggiani for many interesting discussions on the topics of this paper."
    } ],
    "references" : [ {
      "title" : "Incremental Relevance Feedback",
      "author" : [ "IJsbrand J. Aalbersberg" ],
      "venue" : "In Proceedings of the 15th ACM International Conference on Research and Development in Information Retrieval (SIGIR",
      "citeRegEx" : "Aalbersberg.,? \\Q1992\\E",
      "shortCiteRegEx" : "Aalbersberg.",
      "year" : 1992
    }, {
      "title" : "Foundations of Rational Choice under Risk",
      "author" : [ "Paul Anand" ],
      "venue" : null,
      "citeRegEx" : "Anand.,? \\Q1993\\E",
      "shortCiteRegEx" : "Anand.",
      "year" : 1993
    }, {
      "title" : "A Utility-Theoretic Ranking Method for Semi-Automated Text Classification",
      "author" : [ "Giacomo Berardi", "Andrea Esuli", "Fabrizio Sebastiani" ],
      "venue" : "In Proceedings of the 35th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2012). Portland,",
      "citeRegEx" : "Berardi et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Berardi et al\\.",
      "year" : 2012
    }, {
      "title" : "Optimising human inspection work in automated verbatim coding",
      "author" : [ "Giacomo Berardi", "Andrea Esuli", "Fabrizio Sebastiani" ],
      "venue" : "International Journal of Market Research 56,",
      "citeRegEx" : "Berardi et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Berardi et al\\.",
      "year" : 2014
    }, {
      "title" : "Dynamic Ranked Retrieval",
      "author" : [ "Christina Brandt", "Thorsten Joachims", "Yisong Yue", "Jacob Bank" ],
      "venue" : "In Proceedings of the 4th International Conference on Web Search and Web Data Mining (WSDM",
      "citeRegEx" : "Brandt et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Brandt et al\\.",
      "year" : 2011
    }, {
      "title" : "Identifying mislabeled training data",
      "author" : [ "Carla E. Brodley", "Mark A. Friedl" ],
      "venue" : "Journal of Artificial Intelligence Research",
      "citeRegEx" : "Brodley and Friedl.,? \\Q1999\\E",
      "shortCiteRegEx" : "Brodley and Friedl.",
      "year" : 1999
    }, {
      "title" : "Smoothing Sparse Contingency Tables",
      "author" : [ "Prabir Burman" ],
      "venue" : "The Indian Journal of Statistics 49,",
      "citeRegEx" : "Burman.,? \\Q1987\\E",
      "shortCiteRegEx" : "Burman.",
      "year" : 1987
    }, {
      "title" : "An Empirical Study of Smoothing Techniques for Language Modeling",
      "author" : [ "Stanley F. Chen", "Joshua Goodman" ],
      "venue" : "In Proceedings of the 34th Annual Meeting on Association for Computational Linguistics (ACL",
      "citeRegEx" : "Chen and Goodman.,? \\Q1996\\E",
      "shortCiteRegEx" : "Chen and Goodman.",
      "year" : 1996
    }, {
      "title" : "The foundations of cost-sensitive learning",
      "author" : [ "Charles Elkan" ],
      "venue" : "In Proceedings of the 17th International Joint Conference on Artificial Intelligence (IJCAI 2001). Seattle,",
      "citeRegEx" : "Elkan.,? \\Q2001\\E",
      "shortCiteRegEx" : "Elkan.",
      "year" : 2001
    }, {
      "title" : "MP-Boost: A Multiple-Pivot Boosting Algorithm and its Application to Text Categorization",
      "author" : [ "Andrea Esuli", "Tiziano Fagni", "Fabrizio Sebastiani" ],
      "venue" : "In Proceedings of the 13th International Symposium on String Processing and Information Retrieval (SPIRE",
      "citeRegEx" : "Esuli et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Esuli et al\\.",
      "year" : 2006
    }, {
      "title" : "Active Learning Strategies for Multi-Label Text Classification",
      "author" : [ "Andrea Esuli", "Fabrizio Sebastiani" ],
      "venue" : "In Proceedings of the 31st European Conference on Information Retrieval (ECIR",
      "citeRegEx" : "Esuli and Sebastiani.,? \\Q2009\\E",
      "shortCiteRegEx" : "Esuli and Sebastiani.",
      "year" : 2009
    }, {
      "title" : "Training Data Cleaning for Text Classification",
      "author" : [ "Andrea Esuli", "Fabrizio Sebastiani" ],
      "venue" : "ACM Transactions on Information Systems 31,",
      "citeRegEx" : "Esuli and Sebastiani.,? \\Q2013\\E",
      "shortCiteRegEx" : "Esuli and Sebastiani.",
      "year" : 2013
    }, {
      "title" : "Correcting category errors in text classification",
      "author" : [ "Fumiyo Fukumoto", "Yoshimi Suzuki" ],
      "venue" : "In Proceedings of the 20th International Conference on Computational Linguistics (COLING 2004). Geneva,",
      "citeRegEx" : "Fukumoto and Suzuki.,? \\Q2004\\E",
      "shortCiteRegEx" : "Fukumoto and Suzuki.",
      "year" : 2004
    }, {
      "title" : "What’s Wrong with Adding One? In Corpus-Based Research into Language",
      "author" : [ "William A. Gale", "Kenneth W. Church" ],
      "venue" : "In honour of Jan Aarts, N. Oostdijk and P. de Haan (Eds.). Rodopi,",
      "citeRegEx" : "Gale and Church.,? \\Q1994\\E",
      "shortCiteRegEx" : "Gale and Church.",
      "year" : 1994
    }, {
      "title" : "Document Classification Through Interactive Supervision of Document and Term Labels",
      "author" : [ "Shantanu Godbole", "Abhay Harpale", "Sunita Sarawagi", "Soumen Chakrabarti" ],
      "venue" : "In Proceedings of the 8th European Conference on Principles and Practice of Knowledge Discovery in Databases (PKDD",
      "citeRegEx" : "Godbole et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Godbole et al\\.",
      "year" : 2004
    }, {
      "title" : "Learning from imbalanced data",
      "author" : [ "Haibo He", "Edwardo A. Garcia" ],
      "venue" : "IEEE Transactions on Knowledge and Data Engineering 21,",
      "citeRegEx" : "He and Garcia.,? \\Q2009\\E",
      "shortCiteRegEx" : "He and Garcia.",
      "year" : 2009
    }, {
      "title" : "OHSUMED: An interactive retrieval evaluation and new large text collection for research",
      "author" : [ "William Hersh", "Christopher Buckley", "T.J. Leone", "David Hickman" ],
      "venue" : "In Proceedings of the 17th ACM International Conference on Research and Development in Information Retrieval (SIGIR",
      "citeRegEx" : "Hersh et al\\.,? \\Q1994\\E",
      "shortCiteRegEx" : "Hersh et al\\.",
      "year" : 1994
    }, {
      "title" : "Large-scale text categorization by batch mode active learning",
      "author" : [ "Steven C. Hoi", "Rong Jin", "Michael R. Lyu" ],
      "venue" : "In Proceedings of the 15th International Conference on World Wide Web (WWW",
      "citeRegEx" : "Hoi et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Hoi et al\\.",
      "year" : 2006
    }, {
      "title" : "Text categorization of low quality images",
      "author" : [ "David J. Ittner", "David D. Lewis", "David D. Ahn" ],
      "venue" : "In Proceedings of the 4th Annual Symposium on Document Analysis and Information Retrieval (SDAIR",
      "citeRegEx" : "Ittner et al\\.,? \\Q1995\\E",
      "shortCiteRegEx" : "Ittner et al\\.",
      "year" : 1995
    }, {
      "title" : "Transductive Inference for Text Classification using Support Vector Machines",
      "author" : [ "Thorsten Joachims" ],
      "venue" : "In Proceedings of the 16th International Conference on Machine Learning (ICML",
      "citeRegEx" : "Joachims.,? \\Q1999\\E",
      "shortCiteRegEx" : "Joachims.",
      "year" : 1999
    }, {
      "title" : "Selective Supervision: Guiding Supervised Learning with Decision-Theoretic Active Learning",
      "author" : [ "Ashish Kapoor", "Eric Horvitz", "Sumit Basu" ],
      "venue" : "In Proceedings of the 20th International Joint Conference on Artifical Intelligence (IJCAI",
      "citeRegEx" : "Kapoor et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Kapoor et al\\.",
      "year" : 2007
    }, {
      "title" : "Combining classifiers in text categorization",
      "author" : [ "Leah S. Larkey", "W. Bruce Croft" ],
      "venue" : "In Proceedings of the 19th ACM International Conference on Research and Development in Information Retrieval (SIGIR",
      "citeRegEx" : "Larkey and Croft.,? \\Q1996\\E",
      "shortCiteRegEx" : "Larkey and Croft.",
      "year" : 1996
    }, {
      "title" : "Heterogeneous uncertainty sampling for supervised learning",
      "author" : [ "David D. Lewis", "Jason Catlett" ],
      "venue" : "In Proceedings of 11th International Conference on Machine Learning (ICML",
      "citeRegEx" : "Lewis and Catlett.,? \\Q1994\\E",
      "shortCiteRegEx" : "Lewis and Catlett.",
      "year" : 1994
    }, {
      "title" : "Training algorithms for linear text classifiers",
      "author" : [ "David D. Lewis", "Robert E. Schapire", "James P. Callan", "Ron Papka" ],
      "venue" : "In Proceedings of the 19th ACM International Conference on Research and Development in Information Retrieval (SIGIR",
      "citeRegEx" : "Lewis et al\\.,? \\Q1996\\E",
      "shortCiteRegEx" : "Lewis et al\\.",
      "year" : 1996
    }, {
      "title" : "Document Difficulty Framework for Semi-Automatic Text Classification",
      "author" : [ "Miguel Martinez-Alvarez", "Alejandro Bellogin", "Thomas Roelleke" ],
      "venue" : "In Proceedings of the 15th International Conference on Data Warehousing and Knowledge Discovery (DaWaK",
      "citeRegEx" : "Martinez.Alvarez et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Martinez.Alvarez et al\\.",
      "year" : 2013
    }, {
      "title" : "Semi-automatic Document classification: Exploiting Document Difficulty",
      "author" : [ "Miguel Martinez-Alvarez", "Sirvan Yahyaei", "Thomas Roelleke" ],
      "venue" : "In Proceedings of the 34th European Conference on Information Retrieval (ECIR",
      "citeRegEx" : "Martinez.Alvarez et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Martinez.Alvarez et al\\.",
      "year" : 2012
    }, {
      "title" : "Employing EM in pool-based active learning for text classification",
      "author" : [ "Andrew K. McCallum", "Kamal Nigam" ],
      "venue" : "In Proceedings of the 15th International Conference on Machine Learning (ICML 1998). Madison,",
      "citeRegEx" : "McCallum and Nigam.,? \\Q1998\\E",
      "shortCiteRegEx" : "McCallum and Nigam.",
      "year" : 1998
    }, {
      "title" : "Rank-Biased Precision for Measurement of Retrieval Effectiveness",
      "author" : [ "Alistair Moffat", "Justin Zobel" ],
      "venue" : "ACM Transactions on Information Systems 27,",
      "citeRegEx" : "Moffat and Zobel.,? \\Q2008\\E",
      "shortCiteRegEx" : "Moffat and Zobel.",
      "year" : 2008
    }, {
      "title" : "Obtaining Calibrated Probabilities from Boosting",
      "author" : [ "Alexandru Niculescu-Mizil", "Rich Caruana" ],
      "venue" : "In Proceedings of the 21st Conference Annual Conference on Uncertainty in Artificial Intelligence (UAI 2005). Arlington,",
      "citeRegEx" : "Niculescu.Mizil and Caruana.,? \\Q2005\\E",
      "shortCiteRegEx" : "Niculescu.Mizil and Caruana.",
      "year" : 2005
    }, {
      "title" : "Evaluation of information retrieval for E-discovery",
      "author" : [ "Douglas W. Oard", "Jason R. Baron", "Bruce Hedin", "David D. Lewis", "Stephen Tomlinson" ],
      "venue" : "Artificial Intelligence and Law 18,",
      "citeRegEx" : "Oard et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Oard et al\\.",
      "year" : 2010
    }, {
      "title" : "Information Retrieval for E-Discovery",
      "author" : [ "Douglas W. Oard", "William Webber" ],
      "venue" : "Foundations and Trends in Information Retrieval 7,",
      "citeRegEx" : "Oard and Webber.,? \\Q2013\\E",
      "shortCiteRegEx" : "Oard and Webber.",
      "year" : 2013
    }, {
      "title" : "Probabilistic outputs for support vector machines and comparison to regularized likelihood methods",
      "author" : [ "John C. Platt" ],
      "venue" : "In Advances in Large Margin Classifiers,",
      "citeRegEx" : "Platt.,? \\Q2000\\E",
      "shortCiteRegEx" : "Platt.",
      "year" : 2000
    }, {
      "title" : "Active Learning with Feedback on Features and Instances",
      "author" : [ "Hema Raghavan", "Omid Madani", "Rosie Jones" ],
      "venue" : "Journal of Machine Learning Research",
      "citeRegEx" : "Raghavan et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Raghavan et al\\.",
      "year" : 2006
    }, {
      "title" : "A new interpretation of average precision",
      "author" : [ "Stephen E. Robertson" ],
      "venue" : "In Proceedings of the 31st ACM International Conference on Research and Development in Information Retrieval (SIGIR 2008). Singapore,",
      "citeRegEx" : "Robertson.,? \\Q2008\\E",
      "shortCiteRegEx" : "Robertson.",
      "year" : 2008
    }, {
      "title" : "BoosTexter: A boosting-based system for text categorization",
      "author" : [ "Robert E. Schapire", "Yoram Singer" ],
      "venue" : "Machine Learning 39,",
      "citeRegEx" : "Schapire and Singer.,? \\Q2000\\E",
      "shortCiteRegEx" : "Schapire and Singer.",
      "year" : 2000
    }, {
      "title" : "Machine learning in automated text categorization",
      "author" : [ "Fabrizio Sebastiani" ],
      "venue" : "Comput. Surveys 34,",
      "citeRegEx" : "Sebastiani.,? \\Q2002\\E",
      "shortCiteRegEx" : "Sebastiani.",
      "year" : 2002
    }, {
      "title" : "Active learning",
      "author" : [ "Burr Settles" ],
      "venue" : null,
      "citeRegEx" : "Settles.,? \\Q2012\\E",
      "shortCiteRegEx" : "Settles.",
      "year" : 2012
    }, {
      "title" : "A penalty function approach to smoothing large sparse contingency tables",
      "author" : [ "Jeffrey S. Simonoff" ],
      "venue" : "The Annals of Statistics 11,",
      "citeRegEx" : "Simonoff.,? \\Q1983\\E",
      "shortCiteRegEx" : "Simonoff.",
      "year" : 1983
    }, {
      "title" : "Support Vector Machine Active Learning with Applications to Text Classification",
      "author" : [ "Simon Tong", "Daphne Koller" ],
      "venue" : "Journal of Machine Learning Research",
      "citeRegEx" : "Tong and Koller.,? \\Q2001\\E",
      "shortCiteRegEx" : "Tong and Koller.",
      "year" : 2001
    }, {
      "title" : "What’s it going to cost you?: Predicting effort vs. informativeness for multi-label image annotations",
      "author" : [ "Sudheendra Vijayanarasimhan", "Kristen Grauman" ],
      "venue" : "In Proceedings of the 15th IEEE Conference on Computer Vision and Pattern Recognition (CVPR 2009). Miami,",
      "citeRegEx" : "Vijayanarasimhan and Grauman.,? \\Q2009\\E",
      "shortCiteRegEx" : "Vijayanarasimhan and Grauman.",
      "year" : 2009
    }, {
      "title" : "Theory of Games and Economic Behavior",
      "author" : [ "John von Neumann", "Oskar Morgenstern" ],
      "venue" : null,
      "citeRegEx" : "Neumann and Morgenstern.,? \\Q1944\\E",
      "shortCiteRegEx" : "Neumann and Morgenstern.",
      "year" : 1944
    }, {
      "title" : "A re-examination of text categorization methods",
      "author" : [ "Yiming Yang", "Xin Liu" ],
      "venue" : "In Proceedings of the 22nd ACM International Conference on Research and Development in Information Retrieval (SIGIR",
      "citeRegEx" : "Yang and Liu.,? \\Q1999\\E",
      "shortCiteRegEx" : "Yang and Liu.",
      "year" : 1999
    }, {
      "title" : "A Study of Smoothing Methods for Language Models Applied to Information Retrieval",
      "author" : [ "ChengXiang Zhai", "John Lafferty" ],
      "venue" : "ACM Transactions on Information Systems 22,",
      "citeRegEx" : "Zhai and Lafferty.,? \\Q2004\\E",
      "shortCiteRegEx" : "Zhai and Lafferty.",
      "year" : 2004
    }, {
      "title" : "Introduction to Semi-Supervised Learning",
      "author" : [ "Xiaojin Zhu", "Andrew B. Goldberg" ],
      "venue" : null,
      "citeRegEx" : "Zhu and Goldberg.,? \\Q2009\\E",
      "shortCiteRegEx" : "Zhu and Goldberg.",
      "year" : 2009
    } ],
    "referenceMentions" : [ {
      "referenceID" : 2,
      "context" : "This paper is a revised and extended version of [Berardi et al. 2012].",
      "startOffset" : 48,
      "endOffset" : 69
    }, {
      "referenceID" : 25,
      "context" : "One obvious strategy (also used in [Martinez-Alvarez et al. 2012]) is to rank the documents in ascending order of the confidence scores generated by Φ̂, so that the topranked documents are the ones that Φ̂ has classified with the lowest confidence.",
      "startOffset" : 35,
      "endOffset" : 65
    }, {
      "referenceID" : 17,
      "context" : ", [Hoi et al. 2006; Tong and Koller 2001]), i.",
      "startOffset" : 2,
      "endOffset" : 41
    }, {
      "referenceID" : 14,
      "context" : "erations apply when active learning is carried out at the term level, rather than at the document level [Godbole et al. 2004; Raghavan et al. 2006].",
      "startOffset" : 104,
      "endOffset" : 147
    }, {
      "referenceID" : 32,
      "context" : "erations apply when active learning is carried out at the term level, rather than at the document level [Godbole et al. 2004; Raghavan et al. 2006].",
      "startOffset" : 104,
      "endOffset" : 147
    }, {
      "referenceID" : 20,
      "context" : "In [Kapoor et al. 2007] these costs are embedded into a decision-theoretic framework, which is reminiscent of our utility-theoretic framework.",
      "startOffset" : 3,
      "endOffset" : 23
    }, {
      "referenceID" : 25,
      "context" : "The recent [Martinez-Alvarez et al. 2012] tackles the related problem of deciding when a document is too difficult for automated classification, and should thus be routed to a human annotator.",
      "startOffset" : 11,
      "endOffset" : 41
    }, {
      "referenceID" : 24,
      "context" : "In a subsequent paper [Martinez-Alvarez et al. 2013], the same authors study a family of SATC methods that exploit “document difficulty”, taking into account the confidence scores computed by the base classifiers.",
      "startOffset" : 22,
      "endOffset" : 52
    }, {
      "referenceID" : 2,
      "context" : "They also present a comparison between the techniques they propose and that presented in an earlier version of the present paper [Berardi et al. 2012]; in this comparison, the former are claimed to outperform the latter on the Reuters-21578 dataset discussed in Section 6.",
      "startOffset" : 129,
      "endOffset" : 150
    }, {
      "referenceID" : 2,
      "context" : "However, this comparison is incorrect since the authors compare the results of their ranking methods as applied to confidence scores generated by SVMs, with those of the [Berardi et al. 2012] ranking method as applied to confidences scores generated by a different learner.",
      "startOffset" : 170,
      "endOffset" : 191
    }, {
      "referenceID" : 24,
      "context" : "The comparison reported in [Martinez-Alvarez et al. 2013] is incorrect also because it is carried out in terms of the ENER ρ measure (see Section 5.",
      "startOffset" : 27,
      "endOffset" : 57
    }, {
      "referenceID" : 2,
      "context" : "3); instead, as stated in [Berardi et al. 2012], the measure according to which the method of [Berardi et al.",
      "startOffset" : 26,
      "endOffset" : 47
    }, {
      "referenceID" : 2,
      "context" : "2012], the measure according to which the method of [Berardi et al. 2012] should be evaluated is ENER ρ , and not ENER ρ , since it is ENER ρ that that method was optimized for.",
      "startOffset" : 52,
      "endOffset" : 73
    }, {
      "referenceID" : 3,
      "context" : "An application of the method discussed in Section 7 to performing SATC in a market research context is presented in [Berardi et al. 2014].",
      "startOffset" : 116,
      "endOffset" : 137
    }, {
      "referenceID" : 19,
      "context" : ", [Larkey and Croft 1996; Sebastiani 2002; Yang and Liu 1999]) have evoked the existence of this scenario, we are not aware of many published papers that either discuss ranking policies for supporting the human annotator’s effort, or that attempt to quantify the effort needed for reaching a desired level of accuracy. For instance, while discussing a system for the automatic assignment of ICD9 classes to patients’ discharge summaries, Larkey and Croft [1996] say “We envision these classifiers being used in an interactive system which would display the 20 or so top ranking [classes] and their scores to an expert user.",
      "startOffset" : 3,
      "endOffset" : 462
    }, {
      "referenceID" : 9,
      "context" : "As our first learning algorithm for generating our classifiers Φ̂j we use a boostingbased learner called MP-BOOST [Esuli et al. 2006].",
      "startOffset" : 114,
      "endOffset" : 133
    }, {
      "referenceID" : 9,
      "context" : "MH [Schapire and Singer 2000] optimized for multi-label settings, which has been shown in [Esuli et al. 2006] to obtain considerable effectiveness improvements with respect to ADABOOST.",
      "startOffset" : 90,
      "endOffset" : 109
    }, {
      "referenceID" : 16,
      "context" : "Another dataset we have used is OHSUMED [Hersh et al. 1994], a test collection consisting of a set of 348,566 MEDLINE references spanning the years from 1987 to 1991.",
      "startOffset" : 40,
      "endOffset" : 59
    }, {
      "referenceID" : 23,
      "context" : "In our experiments we have scrupulously followed the experimental setup presented in [Lewis et al. 1996].",
      "startOffset" : 85,
      "endOffset" : 104
    }, {
      "referenceID" : 23,
      "context" : "This is the only point in which we deviate from [Lewis et al. 1996], which experiments only on the 77 most frequent MeSH index terms of the HD subtree.",
      "startOffset" : 48,
      "endOffset" : 67
    }, {
      "referenceID" : 4,
      "context" : "In keeping with [Brandt et al. 2011] we prefer to call it a dynamic strategy, and to call the one of Section 4 a static one.",
      "startOffset" : 16,
      "endOffset" : 36
    }, {
      "referenceID" : 29,
      "context" : ", when classification is to be applied to a recall-oriented task (such as e-discovery [Oard et al. 2010; Oard and Webber 2013]), in which case values β > 1 are appropriate.",
      "startOffset" : 86,
      "endOffset" : 126
    } ],
    "year" : 2015,
    "abstractText" : "This paper is a revised and extended version of [Berardi et al. 2012]. The order in which the authors are listed is purely alphabetical; each author has given an equal contribution to this work. Authors’ address: Giacomo Berardi and Andrea Esuli, Istituto di Scienza e Tecnologie dell’Informazione, Consiglio Nazionale delle Ricerche, Via Giuseppe Moruzzi 1, 56124 Pisa, Italy. E-mail: firstname.lastname@isti.cnr.it . Fabrizio Sebastiani, Qatar Computing Research Institute, PO Box 5825, Doha, Qatar. E-mail: fsebastiani@qf.org.qa . Fabrizio Sebastiani is on leave from the Italian National Council of Research. Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies show this notice on the first page or initial screen of a display along with the full citation. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, to republish, to post on servers, to redistribute to lists, or to use any component of this work in other works requires prior specific permission and/or a fee. Permissions may be requested from Publications Dept., ACM, Inc., 2 Penn Plaza, Suite 701, New York, NY 10121-0701 USA, fax +1 (212) 869-0481, or permissions@acm.org. c © YYYY ACM 1556-4681/YYYY/01-ARTA $15.00 DOI:http://dx.doi.org/10.1145/0000000.0000000",
    "creator" : "LaTeX with hyperref package"
  }
}