{
  "name" : "1603.02839.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Starting Small – Learning with Adaptive Sample Sizes",
    "authors" : [ "Hadi Daneshmand", "Aurelien Lucchi" ],
    "emails" : [ "HADI.DANESHMAND@INF.ETHZ.CH", "AURELIEN.LUCCHI@INF.ETHZ.CH", "THOMAS.HOFMANN@INF.ETHZ.CH" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n60 3.\n02 83\n9v 1\n[ cs\n.L G\n] 9\nM ar"
    }, {
      "heading" : "1. Introduction",
      "text" : "In empirical risk minimization (ERM) (Vapnik, 1998) the training set S is used to define a sample risk RS , which is then minimized with regard to a pre-defined function class. One effectively equates learning algorithms with optimization algorithms. However, for all practical purposes an approximate solution of RS will be sufficient, as long as the optimization error is small relative to the statistical accuracy at sample size n := |S|. This is important for massive data sets, where optimization to numerical precision is infeasible. Instead of performing early stopping on black-box optimization, one ought to understand the trade-offs between statistical and computational accuracy, cf. (Chandrasekaran & Jordan, 2013). In this paper, we investigate a much neglected facet of this topic, namely how to dynamically control the effective sample size in optimization.\nMany large-scale optimization algorithms are iterative: they use sampled or aggregated data to perform a sequence\nof update steps. This includes the popular family of gradient descent methods. Often, the computational complexity increases with the size of the training sample, e.g. in steepest-descent, where the cost of a gradient computation scales with n. Does one really need a highly accurate gradient though, in particular in the early phase of optimization? Why not use subsets Tt ⊆ S which are increased in size with the iteration count t, matching-up statistical accuracy with optimization accuracy in a dynamic manner? This is the general program we pursue in this paper. In order to make this idea concrete and to reach competitive results, we focus on a recent variant of stochastic gradient descent (SGD), which is known as SAGA (Defazio et al., 2014). As we will show, this algorithm has a particularly interesting property in how its convergence rate depends on n."
    }, {
      "heading" : "1.1. Empirical Risk Minimization",
      "text" : "Formally, we assume that training examples x ∈ S ⊆ X have been drawn i.i.d. from some underlying, but unknown probability distribution P . We fix a function class F parametrized by weight vectors w ∈ Rd and define the expected risk as R(w) := Efx(w), where f is an x-indexed family of loss functions, often convex. We denote the minimum and the minimizer of R(w) over F by R∗ and w∗, respectively. Given that P is unknown, ERM suggests to rely on the empirical (or sample) risk with regard to S\nRS(w) := 1\nn\n∑ x∈S fx(w), w ∗ S := argmin w∈F RS(w) . (1)\nNote that one may absorb a regularizer in the definition of the loss fx."
    }, {
      "heading" : "1.2. Generalization bounds",
      "text" : "The relation between w∗ and w∗S has been widely studied in the literature on learning theory. It is usually analysed with the help of uniform convergence bounds that take the\ngeneric form (Boucheron et al., 2005)\nES\n[\nsup w∈F\n|R(w) −RS(w)| ] ≤ H(n) , (2)\nwhere the expectation is over a random n-sample S. Here H is a bound that depends on n, usually through a ratio n/d, where d is the capacity of F (e.g. VC dimension). In the realizable case, we may be able to observe a favorable H(n) ∝ d/n, whereas in the pessimistic case, we may only be able to establish weaker bounds such as H(n) ∝ √\nd/n (e.g. for linear function classes); see also (Bousquet & Bottou, 2008). We ignore additional log factors that can be eliminated using the ”chaining” technique (Bousquet, 2002; Bousquet & Bottou, 2008)."
    }, {
      "heading" : "1.3. Statistical efficiency",
      "text" : "Assume now that we have some approximate optimization algorithm, which given S produces solutions wS that are on average ǫ(n) optimal, i.e. ES [RS(wS)−R∗S ] ≤ ǫ(n). One can then provide the following quality guarantee in expectation over sample sets S (Bousquet & Bottou, 2008)\nESR(wS)−R∗ ≤ H(n) + ǫ(n) , (3)\nwhich is an additive decomposition of the expected solution suboptimality into an estimation (or statistical) error H(n) and an optimization (or computational) error ǫ(n). For a given computational budget, one typically finds that ǫ(n) is increasing with n, whereas H(n) is always decreasing. This hints at a trade-off, which may suggest to chose a sample size m < n. Intuitively speaking, concentrating the computational budget on fewer data may be better than spreading computations too thinly."
    }, {
      "heading" : "1.4. Stochastic Gradient Optimization",
      "text" : "For large scale problems, stochastic gradient descent is a method of choice in order to optimize problems of the form given in Eq. (1). Yet, while SGD update directions equal the true (negative) gradient direction in expectation, high variance typically leads to sub-linear convergence. This is where variance-reducing methods for ERM such as SAG (Roux et al., 2012), SVRG (Johnson & Zhang, 2013), and SAGA (Defazio et al., 2014) come into play. We focus on the latter here, where one can establish the following result on the convergence rate (see appendix).\nLemma 1. Let all fx be convex with L-Lipschitz continuous gradients and assume that RS is µ-strongly convex. Then the suboptimality of the SAGA iterate wt after t steps is w.h.p. over a randomly sampled S bounded by\nEA [ RS(wt)−R∗S ] ≤ ρtnCS , ρn = 1−min ( 1 n , µ L ) ,\nwhere the expectation is over the algorithmic randomness.\nThis highlights two different regimes: For small n, the condition number κ := Lµ dictates how fast the optimization algorithm converges. On the other hand, for large n, the convergence rate of SAGA becomes ρn = 1− 1n ."
    }, {
      "heading" : "1.5. Contributions",
      "text" : "Our main question is: can we obtain faster convergence to a statistically accurate solution by running SAGA on an initially smaller sample, whose size is then gradually increased? Motivated by a simple, yet succinct analysis, we present a novel algorithm, called DYNASAGA that implements this idea and achieves ǫ(n) ≤ H(n) after only 2n iterations."
    }, {
      "heading" : "2. Related Work",
      "text" : "Stochastic approximation is a powerful tool for minimizing objective Eq. (1) for convex loss functions. The pioneering work of (Robbins & Monro, 1951) is essentially a streaming SGD method where each observation is used only once. Another major milestones has been the idea of iterate averaging (Polyak & Juditsky, 1992). A thorough theoretical analysis of asymptotic convergence of SGD can be found in (Kushner & Yin, 2003), whereas some non-asymptotic results have been presented in (Moulines & Bach, 2011).\nA line of recent work known as variance-reduced SGD, e.g. (Roux et al., 2012; Shalev-Shwartz & Zhang, 2013; Johnson & Zhang, 2013; Defazio et al., 2014), has exploited the finite sum structure of the empirical risk to establish linear convergence for strongly convex objectives. There is also evidence of slightly improved statistical efficiency (Babanezhad et al., 2015). (Frostig et al., 2015) provides a non-asymptotic analysis of a streaming SVRG algorithm (SSVRG), for which a a convergence rate approaching that of the ERM is established.\nThere have also been approaches of non-uniform sampling of data points, e.g. by (Schmidt et al., 2013; He & Takác, 2015), with the goal of sampling more important data points more often. This direction is largely orthogonal to our dynamic sizing of the sample, which is purely based on random subsampling."
    }, {
      "heading" : "3. Methodology",
      "text" : ""
    }, {
      "heading" : "3.1. Setting and Assumptions",
      "text" : "We work under the assumptions made in Lemma 1 and focus on the large data regime, where n ≥ κ and the geometric rate of convergence of SAGA depends on n through ρn = 1− 1/n. This is an interesting regime as the guaranteed progress per update is larger for smaller samples.\nThis form of ρn implies for the case of performing t = n\niterations, i.e. performing one pass1:\nE [RS(wn)−R∗S ] ≤ ( 1− 1 n\n)n\nCS ≤ CS e . (4)\nSo we are guaranteed to improve the solution suboptimality on average by a factor 1/e per pass. This in turn implies that in order to get to a guaranteed accuracy O(n−α), we need O(αn log n) update steps."
    }, {
      "heading" : "3.2. Sample Size Optimization",
      "text" : "For illustrative purposes, let us use the above result to select a sample size for SAGA, which yields the best guarantees.\nProposition 2. Assume H(m) = D/m and n is given. Define C to be an upper-bound on CS , ∀S (from Lemma 1), then for m ≥ κ, V (m) := Dm + Ce− n m provides a bound on the expected suboptimality of SAGA. It is minimized for the choice\nm∗ = max\n{\nκ, n\nlogn+ log CD\n}\n.\nProof. The first claim follows directly from the assumptions and Lemma 1. Moreover the tightest bound is obtained by differentiating V with regard to 1/m and solving for m (see Lemma 9 in appendix).\nThe result implies that we will perform roughly logn + log CD epochs on the optimally sized sample. Also the value of the bound is (for simplicity, assuming C = D)\nV (m∗) = logn\nn +\n1 n ≤ V (n) = 1 n + 1 e , (5)\n1The SAGA analysis holds for i.i.d. sampling, so strictly speaking this is not a pass, but corresponds to n update steps.\nshowing that the single pass approximation error on the full sample is too large (constant), relative to the statistical accuracy."
    }, {
      "heading" : "3.3. Dynamic Sample Growth",
      "text" : "As we have seen, optimizing over a smaller sample can be beneficial (if we believe the significance of the bounds). But why chose a single sample size once and for all? A smaller sample set seems advantageous early on, but as an optimization algorithm approaches the empirical minimizer, it is hit by the statistical accuracy limit. This suggests that we should dynamically increment the size of the sample set. We illustrate this idea in Figure 2. In order to analyze such a dynamic sampling scheme, we need to relate the suboptimality on a sub-sample T to a suboptimality bound on S. We establish a basic result in the following theorem.\nTheorem 3. Let w be an (ǫ, T )-optimal solution, i.e. RT (w)−R∗T ≤ ǫ, where T ⊆ S, m := |T |, n := |S|. Then the suboptimality of w for RS is bounded w.h.p. in the choice of T as:\nES [RS(w)−R∗S ] ≤ ǫ+ n−m\nn H(m) . (6)\nProof. Consider the following equality\nRS(w) −R∗S = RS(w) (1) ∓ RT (w) (2) ∓ R∗T (3) − R∗S We bound the three involved differences (in expectation) as follows: (2): RT (w) − R∗T ≤ ǫ by assumption. (3): ES [RT (w∗T )−RS(w∗S)] ≤ 0 as T ⊆ S. For (1) we apply the bound (see Lemma 10 in the appendix)\nES|T [RS(w)−RT (w)] ≤ n−m\nn |R(w) −RT (w)| .\nMoreover\nET [R(w)−RT (w)] ≤ sup w′ |R(w′)−RT (w′)| ≤ H(m)\nby Eq. (2), which concludes the proof.\nIn plain English, this result suggests the following: If we have optimized w to (ǫ, T ) accuracy on a sub-sample T and we want to continue optimizing on a larger sample S ⊇ T , then we can bound the suboptimality onRS by the same ǫ plus an additional ”switching cost” of (n−m)/n ·H(m)."
    }, {
      "heading" : "4. Algorithms & Analysis",
      "text" : ""
    }, {
      "heading" : "4.1. Computational Limited Learning",
      "text" : "The work of (Bottou, 2010) emphasized that for massive data sets the limiting factor of any learning algorithm will\nbe its computational complexity T , rather than the number of samples n. For SGD this computational limit typically translates into the number of stochastic gradients evaluated by the algorithm, i.e. T becomes the number of update steps. One obvious strategy with abundant data is to sample a new data point in every iteration. There are asymptotic results establishing bounds for various SGD variants in (Bousquet & Bottou, 2008). However, SAGA and related algorithms rely on memorizing past stochastic gradients, cf. (Hofmann et al., 2015), which makes it beneficial to revisit data points, and which is at the root of results such as Lemma 1. This leads to a qualitatively different behavior and our findings indicate that indeed, the trade-offs for large scale learning need to be re-visited, cf. Table 1."
    }, {
      "heading" : "4.2. SAGA with Dynamic Sample Sizes",
      "text" : "We suggest to modify SAGA to work with a dynamic sample size schedule. Let us define a schedule as a monotonic function M : Z+ → Z+, where t is the iteration number and M(t) the effective sample size used at t. We assume that a sequence of data points X = (x1, . . . ,xn) drawn from P is given such that M induces a nested sequence of samples Tt := {xi : 1 ≤ i ≤ M(t)}.\nAlgorithm 2 DYNASAGA 1: Input:\ntraining examples X = (x1,x2, . . . ,xn), xi ∼ P total number of iterations T (e.g. T = 2n) starting point w0 ∈ Rd (e.g w0 = 0) learning rate η > 0 (e.g. η = 14L ) sample schedule M : [1 : T ] → [1 : n]\n2: w ← w0 3: for i = 1, . . . , n do 4: αi ← ∇fxi(w0) {can also be done on the fly} 5: end for 6: for t = 1, . . . , T do 7: sample xi ∼ Uniform(x1, . . . ,xM(t)) 8: g ← ∇fxi(wt−1) 9: A ← ∑M(t)j=1 αj/M(t) {can be done incrementally}\n10: wt ← wt−1 − η (g − αi +A) 11: αi ← g 12: end for\nDYNASAGA generalizes SAGA (Defazio et al., 2014) in that it samples data points non-uniformly at each iteration. Specifically, for a given schedule M and iteration t, it samples uniformly from Tt, but ignores X − Tt. The pseudocode for DYNASAGA is shown in Algorithm 1."
    }, {
      "heading" : "4.3. Upper Bound Recurrence",
      "text" : "We pursue the strategy of using the basic inequalities obtained so far and to stitch them together in the form of a recurrence. At any iteration t we allow ourselves the choice to augment the current sample of size m by some increment △m ≥ 0. We define an upper bound function U as follows\nU(t, n) = min\n\n\n ρnU(t − 1, n) min m<n [ U(t,m) + n−mn H(m) ] , (7)\nsuch that U(0,m) = ξ, where the initial error ξ is defined as:\nξ := 4L\nµ\n[ R(w0)−R(w∗) ] . (8)\nWe refer the reader to Lemma 8 in the Appendix for further details on how to derive the expression for ξ.\nThe construction of Eq. (7) is motivated by the following result:\nProposition 4. W.h.p. over the random n-sample X , the iterate sequence wt generated by DYNASAGA fulfils\nEX [ RTn(wt)−R∗Tn ] ≤ U(t, n) .\nProof. By induction over t. The result for t = 0 follows directly from Lemma 8. The first case in Eq. (7) for the\ninduction step (fixed sample size) follows from Lemma 1. The second case holds by virtue of Theorem 3 for any m, hence also for the minimum.\nAlthough the U-recursion can be solved for small n using dynamic programming (assuming knowledge of all constants), we analyse a much simpler heuristics and its n → ∞ behavior. This leads to interesting insights, while being very practical. In particular, our algorithm is an anytime algorithm, which does not require knowledge of the total number of iterations T ahead of time."
    }, {
      "heading" : "4.4. Sample Schedules",
      "text" : "In this section, we present and analyse two adaptive sample-size schemes for DYNASAGA.\nLINEAR We start with sample size κ and perform 2κ steps. From then on, we add a new sample every other iteration. The effective sample size is thus\nMLIN(t) = max { 2κ, ⌈ t 2 ⌉}\n(9)\nNote that this strategy defines an upper bound on U(2t, t) and U(2t+ 1, t).\nALTERNATING We have also implemented a variant where we perform updates in alternation: every other iteration we sample a new data point, which is added to the set. However, we also force an update on this fresh sample. In alternation, we simply re-sample an existing data point uniformly at random. We do not provide a theoretical analysis for this scheme but show experimentally that it slightly outperforms the LINEAR strategy (see results in the appendix). We thus report results for the ALTERNATING strategy in the experimental section."
    }, {
      "heading" : "4.5. Analysis",
      "text" : "We now provide an analysis that establishes the convergence rate of the LINEAR strategy.\nLemma 5. For H(n) = Dn−α, 0 < α ≤ 1, the LINEAR strategy obtains the following suboptimality\nU(2n, n) ≤ H (n) + ξ 2 (κ n\n)2\n(10)\nProof. By induction over n. The base case follows from Cm ≤ ξ. Using Eq. (7) and (10) for the inductive case, we get\nU (2(n+ 1), n+ 1) (7) ≤ ρ2n+1\n[\nU (2n, n) + 1\nn+ 1 H(n)\n]\n(10) ≤ ξ\n2\n(\nκ\nn+ 1\n)2\n+ n2 (n+ 2)\n(n+ 1) 3 H(n)\nNote that by definition of the logarithmic function, log [n(n+ 2)] < 2 log(n+ 1), and moreover\nn\nn+ 1 H(n) H(n+ 1) =\nn1−α\n(n+ 1)1−α ≤ 1 ,\nwhich completes the proof.\nThis means that for large enough n the LINEAR strategy is able to approach the statistical accuracy with 2n iterations, i.e. two ”passes” over the data. Note the very significant improvement relative to the log n factor inherent to the optimal fixed sample size choice (see Table 1 for a comparison of these two bounds).\nWhat does that imply for the T = n case that we have been emphasizing? It is simple to state an answer as a corollary.\nCorollary 6. Under the same assumptions as Lemma 5, it holds for even n\nU(n, n) ≤ ( 3 · 2α−1 ) H (n) + 2ξ (κ\nn\n)2\nProof. Note that with Eq. (7) (a) and Lemma 5 (b) we get\nU(2n, 2n) (a) ≤ U(2n, n) + 1\n2 H(n)\n(b) ≤ 3\n2 H(n) + 2ξ\n( κ\n2n\n)2\nThe fact that H(n) = 2αH(2n) completes the proof.\nThe proof of the above corollary suggests to only use n = T/2 samples, when performing T steps and to simply ignore the other half (that potentially could have been sampled). One might wonder if a better strategy than the LINEAR one could be defined, e.g. by iterating more than twice on each newly added sample or by increasing the sample size by more than one. The next lemma answers this question and proves that the LINEAR strategy is optimal for large-scale datasets as long as H(n) ∝ 1/n. Lemma 7. Assume that H(n) ∝ D/n, then the LINEAR strategy is optimal for all sample size n > κ.\nProof. Here, we briefly state a sketch of the proof . The details are presented in Appendix A.2. First, we reformulate the problem of the optimal sample size schedule in terms of number of iterations on each samples size. Given that this problem is convex, we can use the KKT conditions to prove the optimality of incrementing by one sample (see Lemma 12) and iterating twice on each sample size (see Lemma 13)."
    }, {
      "heading" : "5. Experimental Results",
      "text" : "We present experimental results on synthetic as well as real-world data, which largely confirms the above analysis."
    }, {
      "heading" : "5.1. Baselines",
      "text" : "We compare DYNASAGA (both the LINEAR and ALTERNATING strategy) to various optimization methods presented in Section 2. This includes SGD (with constant and decreasing step-size), SAGA, streaming SVRG (SSVRG) as well as the mixed SGD/SVRG approach presented in (Babanezhad et al., 2015)."
    }, {
      "heading" : "5.2. Experiment on synthetic data",
      "text" : "We consider linear regression, where inputs a ∈ Rd are drawn from a Gaussian distribution N (0,Σd×d) and outputs are corrupted by additive noise y = 〈x,w∗〉 + ǫ, ǫ ∼ N ( 0, σ2 ) . We are given n i.i.d observations of this\nmodel, S = {(ai, yi)}ni=1, from which we compute the least squares risk RS(w) = 1n ∑n i=1 (〈ai,w〉 − yi) 2.\nBy considering the matrix An to be a row-wise arrangement of the input vectors ai, we can write the Hessian matrix of Rn(w) as Σn = 1nATnAn. When n ≫ d, the matrix Σn converges to Σ and we can therefore assume that Rn(w) is µ-strongly convex and L-Lipschitz where the constants µ and L are the smallest and largest eigenvalues of Σ. We experiment with two different values for the condition number κ.\nCase κ = √ n: We use a diagonal Σ with elements decreasing from 1 to 1√ n , hence κ = √ n. In this particular case the analysis derived in Lemma 5 predicts an upper\nbound U(n, n) < O( 1n ) which is confirmed by the results shown in Figure 3.\nCase κ = n 3 4 : When κ = n 3 4 , the term ( κ n )2 is the dominating term in the proposed upper-bound. In this case, U(n, n) is thus upper-bounded by O (\n1√ n\n)\n, which is once\nagain verified experimentally in Figure 3."
    }, {
      "heading" : "5.3. Experiments on Real Datasets",
      "text" : "We also ran experiments on several real-world datasets in order to compare the performance of DYNASAGA to stateof-the-art methods. The details of the datasets are shown in Table 2. Throughout all the experiments we used the lo-\ngistic loss with a regularizer λ = 1√ n 2. Figures 4, and 5 show the suboptimality on the empirical risk and expected risk after a single pass over the datasets. The various parameters used for the baseline methods are described in Table 3. A critical factor in the performance of most baselines, especially SGD, is the selection of the step-size. We picked the best-performing step-size within the common range guided by existing theoretical analyses, specifically η = 1/L and η = CC+µt for various values of C. Overall, we can see that DYNASAGA performs very well, both as an optimization as well as a learning algorithm. SGD is also very competitive and typically achieves faster convergence than the other baselines, however, its behaviour is not stable throughout all the datasets. The SGD variant with decreasing step-size is typically very fast in the early stages but then slows down after a certain number of steps. The results on the RCV dataset are somehow surprising as SGD with constant step-size clearly outperforms all methods but we show in the appendix that its behaviour gets worse as we increase the condition number. As can be seen very clearly, DYNASAGA yields excellent solutions in terms of expected risk after one pass (see suboptimality values that intersect with the vertical red dashed lines)."
    }, {
      "heading" : "6. Conclusion",
      "text" : "We have presented a new methodology to exploit the tradeoff between computational and statistical complexity, in order to achieve fast convergence to a statistically efficient solution. Specifically, we have focussed on a modification of SAGA and suggested a simple dynamic sampling schedule that adds one new data point every other update step. Our analysis shows competitive convergence rates both in term of suboptimality on the empirical risk as well as (more importantly) the expected risk in a one pass or a two pass setting. These results have been validated experimentally.\nOur approach depends on the underlying optimization\n2We also present some additional results for various regularizers of the form λ = 1\nnp , p < 1 in the appendix\nmethod only through its convergence rate for minimizing an empirical risk. We thus suspect that a similar sample size adaption is applicable to a much wider range of algorithms, including to non-convex optimization methods for deep learning."
    }, {
      "heading" : "A. Appendix",
      "text" : "A.1. Proofs\nProof of Lemma 1.\nProof. We start with the convergence rate of SAGA established in (Defazio et al., 2014) as\nEA [ ‖wt −w∗S‖2 ] ≤ ρt|S| [ ‖w0 −w∗S‖2 + |S|\nµ|S|+ L ( RS(w0)− 〈∇RS(w∗S),w0 −w∗S〉 − R∗S )\n]\n. (11)\nWe then use the L-smoothness assumption of fx(w) to relate the suboptimality on the function values to the bound in Eq. (11).\nEA [ |RS(wt)−RS(w∗S)| ] = EA [ |Ex∈S [ fx(w t) ] −Ex∈S [fx(w∗S)] | ]\nL−smoothness ≤ LEA [ ‖wt −w∗S‖2 ]\nEq. 11 ≤ ρt|S|CS ,\nwhere CS is the initial suboptimality on the empirical risk defined as:\nCS = L\n[ ‖w0 −w∗S‖2 + |S|\nµ|S|+ L ( RS(w0)− 〈∇RS(w∗S),w0 −w∗S〉 − R∗S )\n]\n(12)\nNote that this initial error depends on the set S and its size |S|. In the following Lemma, we propose an upper bound on this initial error that is independent of S\nLemma 8. W.h.p, the initial suboptimality error of sample S is bounded by:\nCS ≤ ξ := 4L\nµ\n[ R(w0)−R(w∗) ]\nProof. We first use the fact that RS(w) is µ-strongly convex as well as the optimality of w∗S to bound CS as\nCS := L\n( ‖w0 −w∗S‖2 + |S|\nµ|S|+ L [ RS(w0)− 〈∇RS(w∗S),w0 −w∗S〉 − RS(w∗S) ]\n)\n≤ L µ [ RS(w0)−RS(w∗S) ] + |S|L µ|S| + L [ RS(w0)− 〈∇RS(w∗S),w0 −w∗S〉 − RS(w∗S) ]\n≤ L µ [ RS(w0)−RS(w∗S) ] + |S|L µ|S| + L [ RS(w0)−RS(w∗S) ]\n(L>0)\n≤ 2L µ [ RS(w0)−RS(w∗S) ]\n≤ 2L µ\n[\nRS(w0) [1] ∓ R(w0) [2] ∓ R(w∗) [3] ∓ R(w∗S)−RS(w∗S)\n]\nWe use the generalization bounds in (Vapnik, 1998) to upper bound [1] and [2]. For [3], we used the uniform convergence rate of the ERM that implies (Vapnik, 1998):\nR(w∗S)−R(w∗) ≤ c sup w |RS(w)−R(w)|,\nwhere c is a constant. We then get\nCS w.h.p ≤ 2L\nµ\n[ H(|S|) +R(w0)−R(w∗) + cH(|S|) +H(|S|) ] . (13)\nWe also make the further assumption that with high probability the initial suboptimality is greater than a constant factor of the statistical accuracy, i.e. R(w0)−R(w∗) > (2 + c)H(|S|). We can then further upper bound CS as\nCS ≤ 4L\nµ\n[ R(w0)−R(w∗) ] . (14)\nLemma 9 (for Proposition 2).\nV (m) := D\nm + Ce− n m , then argmin 0<m≤n V (m) =\nn\nlog nCD\nProof.\ndV\ndm−1 = D − nCe− nm != 0\n⇐⇒ e− nm = D nC ⇐⇒ n m = log nC D\nSolving for m, this indeed corresponds to a minimum which can be verified by checking the boundary values m = n and m → 0. Lemma 10 (for Theorem 3).\nES|T [RS(w)−RT (w)] ≤ n−m\nn |R(w) −RT (w)| .\nProof.\nES|T [RS(w)−RT (w)] = ES−T |T [RS(w)−RT (w)]\n= ES−T\n\n\n1\nn\n\n\n∑ x∈T fx(w) + ∑ y∈S−T fy(w)\n\n− 1 m ∑\nx∈T fx(w)\n\n\n= n−m\nn ES−T\n\n\n1\nn−m ∑ y∈S−T fy(w) −RT (w)\n\n\n= n−m\nn ES−T\n\n\n1\nn−m ∑ y∈S−T fy(w) −RT (w)\n\n\n= n−m\nn [ES−T [RS−T (w)] −RT (w)]\n= n−m\nn [R(w) −RT (w)]\nA.2. Optimality of the LINEAR Strategy\nWe here introduce a new notation and chose to represent a sample size schedule by a vector tn = 〈tm〉,m < n where tm denotes the number of iterations on sample size m. Note that the total number of iterations up to the sample size n is T = ∑\nm<n tm. We define n − as the sample size that we iterate on immediately before sample size n, i.e.\nn− = max{k < n : tk > 0}. (15)\nWe now rewrite the suboptimality bound in terms of the sample size schedule tn as\nA(tn) = ES [RS(w(tn))−RS(w∗)]\n= ρtnn\n(\nA(tn − ) + n− n−\nn H(n−)\n)\n, (16)\nwhere the second equality is derived using Lemma 1 and Theorem 3.\nOne can relate the upper bound U(n, n) to A(tn) using the following constrained program:\nU(n, n) = min tn\nA(tn) (17)\nSubject to ∀m ≤ n : −tm ≤ 0 ∑\nm≤n tm = n\nIn the following we aim at showing that the LINEAR Strategy is the optimal solution of Equation 16. We first prove a Lemma that will be used in the rest of our analysis.\nLemma 11 (Expansion of A(tn)). if H(n) = D/n, then\nA(tn) := C(tn) +\nn ∑\nm=m0+1\nBm(t n), where (18)\nC(tn) := ξ\nn ∏\ni=m0\n( i− 1 i\n)ti\n, Bm(t n) :=\nD\n(m− 1)m\nn ∏\ni=m\n( i− 1 i\n)ti\n. (19)\nProof. Although one could painstakingly unroll the recursivity in Equation 16, we here provide a simple induction proof. First, one can easily verify that the equation holds for n = m0. For the inductive step, we assume it holds for n− and prove it holds for all {k : n− < k ≤ n}. According to the definition of n−, we have tk = 0 for all n− < k < n, and therefore\nρtkk =\nk ∏\nm=n−+1\nρtmm . (20)\nWe will also make use of the following equality in our analysis:\nk − n− k H(n−) = H(n−)−H(k) (H(n)=D/n)= k ∑\nm=n−+1\nH(m− 1)−H(m). (21)\nWe are now ready to prove the inductive step.\nA(tk) EQ 16 = ρtkk\n(\nA(tn − ) + k − n−\nk H(n−)\n)\n(22)\n= ρtkk\n\nC(tn − ) +\nn− ∑\nm=m0+1\nBm(t n−) + k − n− k H(n−)\n\n (23)\nEQ 19, 20 = C(tk) +\nn− ∑\nm=m0+1\nBm(t k) + ρtkk\n( k − n− k H(n−) )\n(24)\nEQ 21 = C(tk) +\nn− ∑\nm=m0+1\nBm(t k) + ρtkk\nk ∑\nm=n−+1\nD\n(m− 1)m (25)\nEQ 20 = C(tk) +\nn− ∑\nm=m0+1\nBm(t k) +\nk ∑\nm=n−+1\nBm(t k) (26)\n= C(tk) +\nk ∑\nm=m0+1\nBm(t k) (27)\nUsing the definitions provided in Lemma 11, we investigate the optimality conditions of the optimal sample size strategy. In the following, we simplify our notations and write Bm and C instead of Bm(tn) and C(tn).\nAs a first step in our analysis, we introduce the following equations based on the definitions of Bm and C.\nBm = 1 m(m− 1) ∏\ni≥m\n( i− 1 i )ti = m+ 1 m− 1 ( m− 1 m )tm Bm+1 . (28)\nn ∏\ni=m\n( i − 1 i )ti = n ∏\ni=m\nexp\n(\nlog\n(\n( i− 1 i\n)ti ))\n= exp\n[\nn ∑\ni=m\nti log\n(\n1− 1 i\n)\n]\n. (29)\nWe now compute the derivative of A(tn∗ ) as\n∂A(tn∗ )\n∂tm = log(1− 1 m )\n(\nC(tn) +\nm ∑\nk=m0+1\nBk(t n)\n)\n≃ − 1 m\n(\nC +\nm ∑\nk=m0+1\nBk\n)\n. (30)\nC(tn) and Bm(tn) are log-convex (hence convex) functions with respect to tn. Since the sum operator preserves convexity (Boyd & Vandenberghe, 2004), A(tn) is convex as well. Let λi, ν denote the Lagrangian coefficients associated with the inequality and equality constraints respectively. According the KKT conditions (Boyd & Vandenberghe, 2004) for the the optimal solution, the following inequalities hold:\nλm ≥ 0 (31) −λmt∗m = 0 (32)\n∂A(tn∗ )\n∂tm − λm + ν = 0 (33)\nAccording the above condition there are two possible cases for the partial derivative ∂A(t n ∗ )\n∂tm :\n• For the case of t∗m > 0, the slackness condition 32 implies that λm = 0. Then, according to the condition 33:\n∂A(tn∗ )\n∂tm = −ν\nEQ. 30 =⇒ 1\nm\n(\nC + m ∑\nk=m0+1\nBk\n)\n= ν (34)\n• For the case of t∗m = 0, λi > 0(a.) holds based on the complementary slackness condition 32.\n∂A(tn∗ )\n∂tm = λi − ν\n(a.) > −ν\nEQ. 30 =⇒ 1\nm\n(\nC +\nm ∑\nk=m0+1\nBk\n)\n< ν (35)\nIn the following two lemmas we use the conditions of optimality derived in Equations 34 and 35 to prove optimality of the LINEAR Strategy. Specifically, we first prove that for the optimal strategy, tm > 0 for m0 < m ≤ n− and tm = 0 for m > n−. We also prove the optimality of incrementing the sample size by one. In the second lemma, we show that t∗m ≃ 2. Lemma 12 (Optimality of sample size increment). For large enough m, a schedule with tm = 0 and tm+1 > 0 cannot be optimal.\nProof. Note that by repeated application of Equation (28) we obtain\nBm+1 < Bm < · · · < Bm−+1 EQ. 34 & 35 < ν (36)\nwhere optimality conditions a. tm− > 0 (EQ.34) and b. tm−+1 = 0 (EQ.35) yeild the last inequality:\nBm−+1 =\nm−+1 ∑\nk=m0+1\nBk − m− ∑\nk=m0+1\nBk ∓ C (37)\na. =\nm−+1 ∑\nk=m0+1\nBk + C −mν (38)\nb. < (m+ 1)ν −mν = ν (39)\nOn the other hand, optimality of a. tm+1 > 0 (EQ.34) and b. tm = 0 (EQ.35) also imply Bm+1 > ν which is in contradiction with the previously established Bm+1 < ν. Indeed, we have\nBm+1 = m+1 ∑\nk=m0+1\nBk − m ∑\nk=m0+1\nBk ∓ C (40)\na. = (m+ 1)ν −\nm ∑\nk=m0+1\nBk − C (41)\nb. > (m+ 1)ν −mν = ν (42)\nLemma 13 (Optimality of two iterations). Consider tn∗ as the minimizer of the optimization problem 17. For sufficiently large m : m0 < m ≤ n−, t∗m ≃ 2.\nProof. Using Lemma 12, t∗m > 0 holds for m0 < m ≤ n−. We proceed with optimality conditions a. t∗m > 0 and b. t∗m−1 > 0 in equation 34.\nBm =\nm ∑\nk=m0+1\nBk − m−1 ∑\nk=m0+1\nBk ∓ C (43)\na. = mν −\nm−1 ∑\nk=m0+1\nBk − C (44)\nb. = mν − (m− 1)ν = ν (45)\nConsequently, Bm = Bm+1 = ν. Using Equation 28, one conclude that t∗m ≃ 2:\nm− 1 m+ 1 = ( m− 1 m\n)t∗m ⇐⇒ t∗m = log\n( 1− 2m+1 )\nlog ( 1− 1m ) ≃ 2m m+ 1 ≃ 2 . (46)\nA.3. Additional Experimental results\nA.3.1. COMPARISON OF THE TWO ADAPTIVE SAMPLE SIZE SCHEMES FOR DYNASAGA\nWe here compare the LINEAR and ALTERNATING schemes on the collection of real datasets presented in Table 2 for a regularizer λ = n− 1\n2 . The results for the empirical and expected risk shown in Figure 6 and Figure 7 show that the ALTERNATING scheme slightly outperforms the LINEAR strategy.\nA.3.2. EFFECT OF THE REGULARIZER\nWe here present additional results for various regularizers of the form λ = 1np , p < 1. In the interest of clarity we only show results on four datasets. We can see a similar trend to the main results presented in the paper for λ = 1√\nn where\nDYNASAGAshows very fast convergence in terms of both empirical and expected risk. SGD is also very competitive and typically achieves faster convergence than the other baselines, however, its behaviour is not stable throughout all the datasets.\nA.4. Details of Experiments\nThe various parameters of all baselines and DYNASAGA are represented in Table 3."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2016,
    "abstractText" : "For many machine learning problems, data is abundant and it may be prohibitive to make multiple passes through the full training set. In this context, we investigate strategies for dynamically increasing the effective sample size, when using iterative methods such as stochastic gradient descent. Our interest is motivated by the rise of variance-reduced methods, which achieve linear convergence rates that scale favorably for smaller sample sizes. Exploiting this feature, we show – theoretically and empirically – how to obtain significant speed-ups with a novel algorithm that reaches statistical accuracy on an n-sample in 2n, instead of n logn steps.",
    "creator" : "LaTeX with hyperref package"
  }
}