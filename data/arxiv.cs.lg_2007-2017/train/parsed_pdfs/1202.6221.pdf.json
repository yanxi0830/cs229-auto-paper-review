{
  "name" : "1202.6221.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Confusion Matrix Stability Bounds for Multiclass Classification",
    "authors" : [ "Pierre Machart", "Liva Ralaivola" ],
    "emails" : [ "firstname.name@lif.univ-mrs.fr" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n20 2.\n62 21\nv1 [\ncs .L\nG ]\n2 8\nFe b\n20 12\nIn this paper, we provide new theoretical results on the generalization properties of learning algorithms for multiclass classification problems. The originality of our work is that we propose to use the confusion matrix of a classifier as a measure of its quality; our contribution is in the line of work which attempts to set up and study the statistical properties of new evaluation measures such as, e.g. ROC curves. In the confusion-based learning framework we propose, we claim that a targetted objective is to minimize the size of the confusion matrix C, measured through its operator norm ‖C‖. We derive generalization bounds on the (size of the) confusion matrix in an extended framework of uniform stability, adapted to the case of matrix valued loss. Pivotal to our study is a very recent matrix concentration inequality that generalizes McDiarmid’s inequality. As an illustration of the relevance of our theoretical results, we show how two SVM learning procedures can be proved to be confusion-friendly. To the best of our knowledge, the present paper is the first that focuses on the confusion matrix from a theoretical point of view.\nKeywords: Machine Learning, Stability Generalization Bounds, Confusion Matrix, Non-Commutative Concentration Inequality, Multi-Class"
    }, {
      "heading" : "1 Introduction",
      "text" : "Multiclass classification is an important problem of machine learning. The issue of having at hand statistically relevant procedures to learn reliable predictors is of particular interest nowadays, given the widespread need of such predictors in information retrieval, web mining, bioinformatics or neuroscience.\nYet, the literature on multiclass learning is not as voluminous than that of binary classification, whereas this problem raises questions from the algorithmic, theoretical and practical points of view. One of the prominent questions is that of the measure to use in order to assess the quality of a multiclass predictor. Here, we develop our results with the idea that the confusion matrix is a performance measure that deserves to be studied as it provides a finer information on the properties of a classifier than the mere misclassification rate. More precisely, building on very recent matrix-based concentration inequalities provided by Tropp (2011) —sometimes referred to as noncommutative concentration inequalities— we establish a stability based framework for confusion-aware learning algorithm. In particular, we prove a generalization bound for confusion stable learning algorithms and show that there exist such algorithms in the literature. In a sense, our framework and our results extend those of Bousquet and Elisseeff (2002), which are designed for scalar loss functions. To the best of our knowledge, this is the first work that establishes generalization bounds for confusion matrices.\nThe paper is organized as follows. Section 2 describes the setting we are interested in and motivates the use of the confusion matrix as a performance measure. Section 3 introduces the new notion of stability that will prove essential to our study; the main theorem of this paper, together with its proof, are provided. Section 4 is devoted to the analysis of two SVM procdures in the light of our new framework. A discussion on the merits and possible extensions of our approach concludes the paper (Section 5).\n1"
    }, {
      "heading" : "2 Confusion Matrix Awareness",
      "text" : ""
    }, {
      "heading" : "2.1 Notation",
      "text" : "As said earlier, we focus on the problem of multiclass classification The input space is denoted by X and the target space is Y = {1, . . . , Q}. The training sequence Z = {Zi = (Xi, Yi)}mi=1 is made of m identically and independently random pairs Zi = (Xi, Yi) distributed according to some unknown (but fixed) distribution D over X × Y. The sequence of input data will be referred to as X = {Xi}mi=1 and the sequence of corresponding labels Y = {Yi}mi=1, we may write Z = {X,Y }. The realization of Zi = (Xi, Yi) is zi = (xi, yi) and z, x and y refer to the realizations of the corresponding sequences of random variables. For a sequence y = {y1, · · · , ym} of m labels, mq(y), or simply mq when clear from context, denotes the number of labels from y that are equal to q; s(y) it the binary sequence {s1(y), . . . , sQ(y)} of size Q such that sq(y) = 1 if q ∈ y and sq(y) = 0 otherwise.\nWe will use DX|y for the conditional distribution onX given that Y = y; therefore, for a given sequence y = {y1, . . . , ym} ∈ Ym, DX|y = ⊗mi=1DX|yi is the distribution of the random sample X = {X1, . . . , Xm} over X Tm such that Xi is distributed according to DX|yi ; for q ∈ Y, andX distributed according to DX|y, Xq = {Xi1 , . . . , Ximq } denotes the random sequence of variables such that Xik is distributed according to DX|q. E[·] and EX|y[·] denote the expectations with respect to D and DX|y, respectively.\nFor a training sequence Z, Zi denotes the sequence {Z1, . . . Zi−1, Z ′i, Zi+1, . . . , Zm} where Z ′i is distributed as Zi; Z\n\\i is the sequence {Z1, . . . Zi−1, Zi+1, . . . , Zm} — these definitions carry directly over when conditioned on a sequence of labels y (with, henceforth, y′i = yi).\nWe will consider a family H of predictors such that H ⊆ {h : h(x) ∈ RQ, ∀x ∈ X}. For h ∈ H, hq ∈ RX denotes its qth coordinate. Also, ℓ = (ℓq)1≤q≤Q is a set of loss functions such that: ℓq : H×X ×Y → R+."
    }, {
      "heading" : "2.2 Confusion Matrix",
      "text" : "We here provide a discussion as to why minding the confusion matrix or confusion loss (terms that we will use interchangeably) is crucial in multiclass classification. We also introduce the reason why we may see the confusion matrix as an operator and, therefore, motivate the recourse to the operator norm to measure the ‘size’ of the confusion matrix. As the definition of the confusion loss in the online learning framework is less usual and a bit more intricate than its definition in the batch scenario, we develop the discussion here within the batch setting —obviously, the argument carries over to the online learning framework.\nIn many situations, e.g. class-imbalanced datasets, it is important not to measure the quality of a predictor H on its classification error PXY (h(X) 6= Y ) only; this may lead to erroneous conclusions regarding the quality of h. Indeed, if, for instance, some class q is predominantly present in the data at hand, say P(Y = q) = 1 − ε, for some small ε > 0, then the predictor hmaj that always outputs hmaj(x) = q regardless of x has a classification error lower thant ε. Yet, it might be important not to classify an instance of some class p in class q: in the context of classifying mushrooms according to the categories {hallucinogen, poisonous, innocuous}, the consequence of predicting innocuous (the majority class) instead of hallucinogen or poisonous might be disastrous.\nAs a consequence, we claim that a more relevant object to consider is the confusion matrix which, given a binary sequence s = {s1 · · · sQ} ∈ {0, 1}Q, is defined as\nCs(h) := ∑\nq:sq=1\nEX|qL(h,X, q),\nwhere, given an hypothesis h ∈ H, x ∈ X , y ∈ Y, L(h, x, y) = (lij)1≤i,j≤Q ∈ RQ×Q is the loss matrix such that:\nlij := { ℓj(h, x, y) if i = yand i 6= j 0 otherwise.\nNote that this matrix has at most one nonzero row, namely its ith row.\nTechnical Report V 1.0 2\nFor a sequence y ∈ Ym of m labels and a random sequence X distributed according to DX|y, the conditional empirical confusion matrix Ĉy(h,X) is given by\nĈy(h,X) := m∑\ni=1\n1\nmyi L(h,Xi, yi) =\n∑ q∈y Lq(h,X,y),\nwhere\nLq(h,X,y) := 1\nmq\n∑\ni:yi=q\nL(h,Xi, q).\nFor a random sequence Z = {X,Y } distributed according to Dm, the (unconditional) empirical confusion matrix is given by EX|Y ĈY (h,X) = Cs(Y )(h), which is a random variable, as it depends on the random sequence Y . For exposition purposes it will often be more convenient to consider a fixed sequence y of labels and state results on Ĉy(h,X), noting that\nEX|yĈy(h,X) = Cs(y)(h). The slight differences between our definitions of (conditional) confusion matrices and the usual definition of a confusion matrix is that the diagonal elements are all zero and that they can accomodate any family of loss functions (and not just the 0− 1 loss).\nA natural objective that may be pursued in multiclass classification is to learn a classifier h with ‘small’ confusion matrix, where ‘small’ might be defined with respect to (some) matrix norm of Cs(h). The norm that we retain is the operator norm that we denote ‖ · ‖ from now on: for a matrix M , ‖M‖ is defined as\n‖M‖ = max v 6=0 ‖Mv‖2 ‖v‖2 ,\nwhere ‖·‖2 is the Euclidean norm; ‖M‖ is merely the largest singular value ofM —note that ‖M⊤‖ = ‖M‖. A nice reason for focusing on the operator norm is that Cs(h) is often precisely used as an operator that acts on the vector of prior distributions\nπ = [P(Y = 1) · · ·P(Y = Q)].\nIndeed, a quantity of interest is for instance the risk Rℓ(h) of h, with\nRℓ(h) := ‖π⊤C1(h)‖1 = Q∑\np,q=1\nEX|pℓq(h,X, p)πp\n= EY\n{ Q∑\nq=1\nEX|Y ℓq(h,X, Y )\n}\n= EXY\n{ Q∑\nq=1\nℓq(h,X, Y )\n} .\nIt is interesting to observe that, ∀h, ∀π:\n0 ≤ Rℓ(h) = ‖πC1(h)‖1 = π⊤C1(h)1 ≤ √ Q ∥∥π⊤C1(h) ∥∥ 2 = √ Q ∥∥C⊤1 (h)π ∥∥ 2\n≤ √ Q ∥∥C⊤\n1 (h) ∥∥ ‖π‖2 ≤ √ Q ∥∥C⊤1 (h) ∥∥ = √ Q ‖C1(h)‖ ,\nwhere we have used Cauchy-Schwarz inequalty in the second line, the definition of the operator norm on the third line and the fact that ‖π‖2 ≤ 1 for any π in {λ ∈ RQ : λq ≥ 0, ∑ q λq = 1}.\nIn addition to support the use of the operator norm, this also says that bounding the norm of the confusion loss is a good way to control the risk of h as well (independently of the prior distribution, see discussion below).\nTechnical Report V 1.0 3"
    }, {
      "heading" : "3 Deriving Stability Bounds on the Confusion Matrix",
      "text" : "One of the most prominent issues in learning theory is to estimate the real performance of a learning system. The usual approach consists in studying how empirical measures converge to their expectation. In the traditional settings, it often boils down to providing bounds describing how the empirical risk relates to the expected one. In this work, we show that one can use similar techniques to provide bounds on the operator norm of the confusion matrix."
    }, {
      "heading" : "3.1 Stability",
      "text" : "Following the early work of Vapnik (1982), the risk has traditionally been estimated through its empirical measure and a measure of the complexity of the hypothesis class such as Vapnik-Chervonenkis (VC-dim), fat-shattering dimension or Rademacher complexity. During the last decade, a new and successful approach based on algorithmic stability to provide some new bounds has emerged. One of the highlights of this approach is the focus on some properties of the learning algorithm at hand, instead of the characterization of the hypothesis class. Roughly, this makes it possible to take advantage from the knowledge on how a given algorithm actually explores the hypothesis space, often leading to tighter bounds.\nThe main results in Bousquet and Elisseeff (2002) were obtained using the following definition of uniform stability.\nDefinition 1 (Uniform stability Bousquet and Elisseeff (2002)). An algorithm A has uniform stability β with respect to the loss function l if the following holds:\n∀S ∈ Zm, ∀i ∈ {1, . . . ,m}, ‖l(AS , .)− l(AS\\i , .)‖∞ ≤ β.\nFollowing the same approach, we now focus on the generalization of such results for confusion matrices. We introduce a new definition of confusion stability.\nDefinition 2 (Confusion stability). An algorithm A is confusion stable with respect to the loss matrix L if ∀q ∈ Y, there exist a βq decreasing as 1mq such that A has βq uniform stability with respect to the loss ℓq"
    }, {
      "heading" : "3.2 Non-Commutative McDiarmid",
      "text" : "In Bousquet and Elisseeff (2002), the authors make use of some well-known concentration inequalities to derive bounds. More specifically, they use a variation of Azuma’s inequality, due to McDiarmid McDiarmid (1989). It describes how a scalar function of independent random variables (the elements of our training set) normally concentrates around its mean, with variance depending on how changing one of the random variables impacts the value of the function.\nSome more recent work Tropp (2011) extends McDiarmid’s inequality to the matrix setting. For the sake of self-containedness, we recall this non-commutative bound.\nTheorem 1 (Matrix bounded difference (Tropp (2011), corollary 7.5)). Let S and Si be defined as above, and let H be a function that maps m variables to a self-adjoint matrix of dimension Q. Consider a sequence {Ai} of fixed self-adjoint matrices that satisfy\n( H(S)−H(Si) )2 4 A2k, (1)\nwhere zi and zi range over all possible values of the space Zi it belongs to, for each index i. Then, for all t ≥ 0,\nP{λmax ( H(z)− EH(z) ) ≥ t} ≤ Qe−t2/8σ2 ,\nwhere z = (Z1, . . . , Zm) and σ 2 := ‖∑i A2i ‖.\nOne may notice thatH has to be applied on a function mapping to self-adjoint matrices. Unfortunately, our confusion matrices are real-valued but are not symmetric. However, we can make use of a dilation technique to overcome this.\nNamely, instead of working directly on a non-self-adjoint matrix A, we will build a self-adjoint dilated matrix D(A) :\nD(A) = ( 0 A A∗ 0 ) ,\nTechnical Report V 1.0 4\nwhere A∗ denotes the adjoint of A. The choice of such a dilation is directly motivated by the following property, recalled in Tropp (2011).\nLemma 1. If λ is the largest eigenvalue of D(A), then λ is the largest singular value of A.\nAs a direct consequence, any result on the largest eigenvalue of a dilated matrix D(A) directly applies to the operator norm ‖A‖ of A."
    }, {
      "heading" : "3.3 Stability Bound",
      "text" : "Theorem 2 (Confusion bound). Let A be a learning algorithm. Assume that all the loss functions under consideration take values in the range [0;M ]. Let y ∈ Ym be a fixed sequence of labels.\nIf A has confusion stability β with respect to all loss matrices Lq, for q ∈ Y, then, ∀m ≥ 1, ∀δ ∈ (0, 1), the following holds, with probability 1− δ over the random draw of X ∼ DX|y,\n∥∥∥Ĉy(A,X)− Cs(y)(A) ∥∥∥ ≤ 2 ∑\nq\nβq +Q\n√ 8 ln ( Q2\nδ\n)( 4 √ mminβmin +M √ Q\nmmin\n) .\nAs a consequence, with probability 1− δ over the random draw of Z ∼ Dm,\n∥∥∥ĈY (A,X)− Cs(Y )(A) ∥∥∥ ≤ 2 ∑\nq\nβq +Q\n√ 8 ln ( Q2\nδ\n)( 4 √ mminβmin +M √ Q\nmmin\n) .\nSketch of proof. The complete proof can be found in the following subsection. We proceed in three steps to proof the first bound. To start with, we know that by the triangle inequality\n‖Ĉ(A,X)− Cs(y)(A)‖ = ∥∥∥∥∥ ∑\nq∈y (Lq(AZ ,Z)− EXLq(AZ ,Z))\n∥∥∥∥∥\n≤ ∑\nq∈y ‖Lq(AZ ,Z)− EXLq(AZ ,Z)‖ . (2)\nUsing standard uniform stability techniques, we bound each summand with probability 1− δ/Q. Then, using the union bound we have a bound on ‖Ĉ(A,X) − Cs(y)(A)‖ that holds with probability at least 1− δ. Finally, recoursing to a simple argument, we express the obtained bound solely with respect to mmin. In order to get the bound with the uncondional confusion matrix Cs(Y )(A) it suffices to observe that for any event E(X,Y ) that depends on X and Y , such that for all sequences y, PX|y{E(X,y)} ≤ δ, the following holds:\nPXY (E(X ,Y )) = EXY { I{E(X,Y )} }\n= EY { EX|Y I{E(X,Y )} } = ∑\ny\nEX|Y I{E(X,Y )}PY (Y = y)\n= ∑\ny\nPX|y{E(X,y)}PY (Y = y)\n≤ ∑\ny\nδPY (Y = y) = δ,\nwhich gives the desired result.\nRemark 1. It is straightforward to directly obtain a bound on ‖Cs(y)(A)‖ and ‖Cs(Y )(A)‖ by using the triangle inequality |‖A‖ − ‖B‖| ≤ ‖A−B‖ on the bounds given in the theorem.\nTechnical Report V 1.0 5"
    }, {
      "heading" : "3.4 Proof of Theorem 2",
      "text" : "Proof. To ease the readability, we introduce some additional notation: Z,Zi,Z\\i will refer respectively to {X,y}, {Xi,yi}, {X\\i,y\\i}, and\nLq = EX|qL(AZ, X, q), L̂q = Lq(AZ,X,y), Liq = EX|qL(AZi , X, q), L̂iq = Lq(AZi ,Xi,yi), L\\iq = EX|qL(AZ\\i , X, q), L̂\\iq = Lq(AZ\\i ,X\\i,y\\i).\nAfter using the triangle inequality in (2), we need to provide a bound on each summand. To get the result, we will, for each q, fix the Xk such that yk 6= q and work with functions of mq variables. Then, we will apply Theorem 1 for each Hq(Xq,yq) := D(Lq)−D(L̂q). To do so, we will bound the differences\n‖Hq(Xq,yq)−Hq(Xiq,yiq)‖.\nNote that\n‖Hq(Xq,yq)−Hq(X iq,yiq)‖ = ‖D(Lq)−D(L̂q)−D(Liq) +D(L̂iq)‖ = ‖Lq − L̂q − Liq + L̂iq‖ ≤ ‖Lq − Liq‖+ ‖L̂q − L̂iq‖\nStep 1: bounding ‖Lq − Liq‖. We can trivially write:\n‖Lq − Liq‖ ≤ ‖Lq − L\\iq ‖+ ‖Liq − L\\iq ‖\nUsing the βq-stability of A:\n‖Lq − L\\iq ‖ = ∥∥EX|q [L(AZ , X, q)− L(AZ\\i , X, q)] ∥∥\n≤ EX|q ‖L(AZ , X, q)− L(AZ\\i , X, q)‖ ≤ βq,\nand the same holds for ‖Liq − L \\i q ‖, i.e. ‖Liq − L \\i q ‖ ≤ βq. Thus, we have:\n‖Lq − Liq‖ ≤ 2βq. (3)\nStep 2: bounding ‖L̂q − L̂iq‖. This is a little trickier than the first step.\n‖L̂q − L̂iq‖ = ∥∥Lq(AZ,Z)− Lq(AZi ,Zi) ∥∥\n= 1\nmq\n∥∥∥ ∑\nk:k 6=i,yk=q\n( L(AZ, Xk, q)− L(AZi , Xk, q) ) + L(AZ, Xi, q)− L(AZi , X ′i, q) ∥∥∥\n≤ 1 mq\n∥∥∥ ∑\nk:k 6=i,yk=q\n( L(AZi , Xk, q)− L(AZi , Xk, q) )∥∥∥+ 1 mq ∥∥∥L(AZ, Xi, q)− L(AZi , X ′i, q) ∥∥∥\nUsing the βq-stability argument as before, we have:\n∥∥∥ ∑\nk:k 6=i,yk=q\n( L(AZ, Xk, q)− L(AZi , Xk, q) )∥∥∥ ≤ ∑\nk:k 6=i,yk=q ‖L(AZ, Xk, q)− L(AZi , Xk, q)‖\n≤ ∑\nk:k 6=i,yk=q 2βq ≤ 2mqβq.\nOn the other hand, we observe that\n∥∥∥L(AZ, Xi, q)− L(AZi , X ′i, q) ∥∥∥ ≤ √ QM.\nTechnical Report V 1.0 6\nIndeed, the matrix ∆ := L(AZ, Xi, q)−L(AZi , X ′i, q) is a matrix that is zero except for (possibly) its qth row, that we may call δq. Thus:\n‖∆‖ = sup v:‖v‖2≤1 ‖∆v‖2 = sup v:‖v‖2≤1 ‖δq · v‖ = ‖δq‖2,\nwhere v is a vector of dimension Q. Since each of the Q elements of δq is in the range [−M ;M ], we get that ‖δq‖2 ≤ √ QM.\nThis allows us to conclude that\n‖L̂q − L̂iq‖ ≤ 2βq + √ QM\nmq (4)\nStep 3: Applying Matrix McDiarmid Combining (3) and (4) that we just proved, we have that, for all i such that yi = q\n(Hq(Zq)−Hq(Ziq))2 4 ( 4βq + √ QM\nmq\n)2 I.\nTherefore, Theorem 1 may be applied on Hq(Xq, yq) = D(Lq − L̂q) with\nσ2q = mqβq = ( 4 √ mqβq + √ QM\n√ mq\n)2\nto give, for t > 0:\nPX|y { ‖Lq − L̂q − E[Lq − L̂q]‖ ≥ t } ≤ 2Q exp    − t 2\n8 ( 4 √ mqβq + √ QM√ mq )2\n   ,\nwhich, using the triangle inequality |‖A‖ − ‖B‖| ≤ ‖A−B‖,\ngives\nPX|y { ‖Lq − L̂q‖ ≥ t+ ‖EX|y[Lq − L̂q]‖ } ≤ 2Q exp    − t 2\n8 ( 4 √ mqβq + √ QM√ mq )2\n   .\nWe may want to bound ‖EX|y[Lq − L̂q]‖. To do so, we note that for any i such that yi = q, and for X ′i distributed according to DX|q:\nEX|yL̂q = EX|yLq(AZ,X,y)\n= 1\nmq\n∑\nj:yj=q\nEX|yL(AZ, Xj, q)\n= 1\nmq\n∑\nj:yj=q\nEX,X′ i |yL(AZi , X ′i, q)\n= EX,X′ i |yL(AZi , X ′i, q).\nHence, using the βq stability\n‖E[Lq − L̂q]‖ = ∥∥EX,X′ i |y [L(AZ, X ′i, q)− L(AZi , X ′i, q)] ∥∥ , ≤ EX,X′ i |y ‖L(AZ, X ′i, q)− L(AZi , X ′i, q)‖\n≤ EX,X′ i |y ‖L(AZ, X ′i, q)− L(AZ\\i , X ′i, q)‖+ EX,X′i|y ‖L(AZi , X ′ i, q)− L(AZ\\i , X ′i, q)‖ ≤ 2βq.\nThis leads to\nPX|y { ‖Lq − L̂q‖ ≥ t+ 2βq } ≤ 2Q exp    − t 2\n8 ( 4 √ mqβq + √ QM√ mq )2\n   .\nTechnical Report V 1.0 7\nStep 4. Union Bound Now, using the union bound, we have\nP { ∃q : ‖Lq − L̂q‖ ≥ t+ 2βq } ≤ ∑\nq∈Y P\n{ ∃q : ‖Lq − L̂q‖ ≥ t+ 2βq }\n≤ 2Q ∑\nq\nexp    − t 2\n8 ( 4 √ mqβq + √ QM√ mq )2\n  \n≤ 2Q2max q exp    − t 2\n8 ( 4 √ mqβq + √ QM√ mq )2\n  \nAccording to our definition of confusion stability (cf. Definition 2), βq decreases as 1\nmq . Therefore\nP { ∃q : ‖Lq − L̂q‖ ≥ t+ 2βq } ≤ 2Q2 max\nq exp    − t 2\n8 ( 4 √ mminβmin + √ QM√ mmin )2\n  \nwhere mmin = minq mq and βmin is the associated βq. Setting the right hand side to δ, we can get, with probability 1− δ,\n∑\nq\n‖Cq − Cqemp‖ ≤ 2 ∑\nq\nβq +Q\n√ 8 ln ( 2Q2\nδ\n)( 4 √ mminβmin +M √ Q\nmmin\n)\nHence the result."
    }, {
      "heading" : "4 Analysis of existing algorithms",
      "text" : "Now that the main result on stability bound has been established, we will investigate how some already existing multi-class algorithms display some stability properties and thus fall in the scope of our analysis. More precisely, we will analyse two well-known models for multiclass support vector machines and we will show that they to promote small confusion error. But first, we will study the more general stability of multi-class algorithms using regularization in Reproducing Kernel Hilbert Spaces (RKHS)."
    }, {
      "heading" : "4.1 Hilbert Space Regularized Algorithms",
      "text" : "Many well-known and widely-used algorithms feature a minimization of a regularized objective functionTikhonov and Arsenin (1977). In the context of kernel machinesCristianini and Shawe-Taylor (2000), this regularizer Ω(h) may take the following form:\nΩ(h) = ∑\nq\n‖hq‖2k.\nwhere k : X × X → R denotes the kernel associated to the RKHS H. In order to study the stability properties of algorithms, minimizing a data-fitting term, penalized by such regularizers, in our multi-class setting, we need to introduce a minor definition that is an addition to definition 19 of Bousquet and Elisseeff (2002).\nDefinition 3. A loss function ℓ defined on HQ ×Y is σ-multi-admissible if ℓ is σ-admissible with respect to any of his Q first arguments.\nThis allows us to come up with the following theorem.\nTheorem 3. Let H be a reproducing kernel Hilbert space (with kernel k) such that ∀X ∈ X , k(X,X) ≤ κ2 < +∞. Let L be a loss matrix, such that ∀q ∈ Y, ℓq is σq-multi-admissible. And let A be an algorithm such that\nAS = argmin h∈HQ\n∑\nq\n∑\nn:yn=q\n1\nmq ℓq(h, xn, q) + λ\n∑\nq\n‖hq‖2k. := argmin h∈HQ J(h).\nTechnical Report V 1.0 8\nThen A is confusion stable with respect to the loss matrix L. Moreover, ∀q ∈ Y, we have\nβq ≤ σ2qQκ 2\n2λmq\nSketch of proof. Roughly, the idea is to exploit definition 3 in order to apply the theorem 22 of Bousquet and Elisseeff (2002) for each loss ℓq. Moreover our regularizer is a sum (over q) of RKHS norms, hence the additional Q in the the bound on βq.\nFrom now on, we will always suppose that we are working with kernels such that k(X,X) ≤ κ2 < +∞."
    }, {
      "heading" : "4.2 Lee, Lin and Wahba model",
      "text" : "One of the most well-known and well-studied model for multi-class classification, in the context of SVM, was proposed by Lee et al. (2004). In this work, the authors suggest the use of the following loss function.\nℓ(h, z) = ∑\nq 6=y\n( hq(x) + 1\nQ− 1\n)\n+\nTheir algorithm, denoted ALLW, then consists in minimizing the following (penalized) functional,\nJ(h) = 1\nm\nm∑\nk=1\n∑\nq 6=yk\n( hq(xk) + 1\nQ− 1\n)\n+\n+ λ\nQ∑\nq=1\n‖hq‖2,\nwith the constraint ∑\nq hq = 0. We can trivially rewrite J(h) as\nJ(h) = ∑\nq\n∑\nn:yn=q\n1\nmq ℓq(h, xn, q) + λ\nQ∑\nq=1\n‖hq‖2,\nwith\nℓq(h, xn, q) = ∑\np6=q\n( hp(xk) + 1\nQ− 1\n)\n+\n.\nIt is straightforward that for any q, ℓq is 1-multi-admissible. We thus can apply theorem 3 and get\nβq ≤ Qκ 2\n2λmq .\nLemma 2. Let h∗ denote the solution found by ALLW. ∀x ∈ X , ∀y ∈ Y, ∀q, we have ℓq(h∗, x, y) ≤ Qκ√λ +1. Proof. As h∗ is a minimizer of J , we have\nJ(h∗) ≤ J(0) = ∑\nq\n∑\nn:yn=q\n1\nmq ℓq(0, xn, q)\n= ∑\nq\n∑\nn:yn=q\n1\n(Q− 1)mq = 1.\nAs the data fitting term is non-negative, we also have\nJ(h∗) ≥ λ ∑\nq\n‖h∗q‖2k.\nGiven that h∗ ∈ H, Cauchy-Schwarz inequality gives\n∀x ∈ X , ‖h∗q‖k ≥ |h∗q(x)|\nκ .\nCollecting things, we have\n∀x ∈ X , |h∗q(x)| ≤ κ√ λ .\nGoing back to the definition of lq, we get the result.\nTechnical Report V 1.0 9\nUsing theorem 2, it follows that, with probability 1− δ,\n∥∥∥ĈY (ALLW,X)− Cs(Y )(ALLW) ∥∥∥ ≤ ∑\nq\nQκ2 λmq +\n√ 8 ln ( Q2\nδ\n)( 2Q2κ2 λ + ( Qκ√ λ + 1 ) Q √ Q )\n√ mmin\n.\nWith regards to the βq we obtained, one can conclude that ALLW is confusion stable."
    }, {
      "heading" : "4.3 Weston and Watkins model",
      "text" : "One of the oldest models, when it comes to multi-class SVM is due to Weston and Watkins (1998). They consider the following loss functions.\nℓ(h, x, y) = ∑\nq 6=y (1− hy(x) + hq(x))+\nThe algorithm AWW minimizes the following functional\nJ(h) = 1\nm\nm∑\nk=1\n∑\nq 6=yk\n(1− hy(x) + hq(x))+ + λ Q∑\nq<p=1\n‖hq − hp‖2,\nThis time, for 1 ≤ p, q ≤ Q, we will introduce the functions hpq = hp − hq. We can then rewrite J(h) as\nJ(h) = ∑\nq\n∑\nn:yn=q\n1\nmq ℓq(h, xn, q) + λ\nQ∑\np=1\np−1∑\nq=1\n‖hpq‖2,\nwith\nℓq(h, xn, q) = ∑\np6=q (1− hpq(xn))+ .\nIt still is straightforward that for any q, ℓq is 1-multi-admissible. However, this time, our regularizer\nconsists in the sum of Q(Q−1)2 < Q2 2 norms. Applying theorem 3 therefore gives us βq ≤ Q2κ2\n4λmq\nLemma 3. Let h∗ denote the solution found by AWW. ∀x ∈ X , ∀y ∈ Y, ∀q, we have ℓq(h∗, x, y) ≤ Q ( 1 + κ √ Q λ ) .\nThis lemma can be proven following exactly the same techniques and reasoning as Lemma 2. Using theorem 2, it follows that, with probability 1− δ,\n∥∥∥ĈY (AWW,X)− Cs(Y )(AWW) ∥∥∥ ≤ ∑\nq\nQ2κ2 2λmq +\n√ 8 ln ( Q2\nδ\n)( Q3κ2 λ +Q 2 (√ Q+ κ Q√ λ ))\n√ mmin\n.\nWith regards to the βq we obtained, one can conclude that AWW is confusion stable."
    }, {
      "heading" : "5 Discussion and Conclusion",
      "text" : "In this paper, we have proposed a new framework, namely the algorithmic confusion stability, together with new bounds to characterize the generalization properties of multiclass learning algorithms. The crux of our study is to envision the confusion matrix as a performance measure, which differs from commonly encountered approaches that investigate generalization properties of scalar-valued performances (such as, e.g., the accuracy).\nA few questions that are raised by the present work are the following. Is it possible to derive confusion stable algorithms that precisely aim at controlling the norm of their confusion matrix? Are there other algorithms than those analyzed here that may be studied in our new framework? In particular, is it the case for k-nearest-neighbors, the generalization analysis of which is amenable thanks to classical stability arguments? On a broader perspective: how can noncommutative concentration inequalities be of some use in machine learning?\nTechnical Report V 1.0 10"
    } ],
    "references" : [ {
      "title" : "Stability and generalization",
      "author" : [ "O. Bousquet", "A. Elisseeff" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Bousquet and Elisseeff,? \\Q2002\\E",
      "shortCiteRegEx" : "Bousquet and Elisseeff",
      "year" : 2002
    }, {
      "title" : "An Introduction to Support Vector Machines: and Other Kernel-Based Learning Methods",
      "author" : [ "N. Cristianini", "J. Shawe-Taylor" ],
      "venue" : null,
      "citeRegEx" : "Cristianini and Shawe.Taylor,? \\Q2000\\E",
      "shortCiteRegEx" : "Cristianini and Shawe.Taylor",
      "year" : 2000
    }, {
      "title" : "Multicategory support vector machines",
      "author" : [ "Y. Lee", "Y. Lin", "G. Wahba" ],
      "venue" : "Journal of the American Statistical Association,",
      "citeRegEx" : "Lee et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Lee et al\\.",
      "year" : 2004
    }, {
      "title" : "On the method of bounded differences",
      "author" : [ "C. McDiarmid" ],
      "venue" : "In Surveys in Combinatorics,",
      "citeRegEx" : "McDiarmid,? \\Q1989\\E",
      "shortCiteRegEx" : "McDiarmid",
      "year" : 1989
    }, {
      "title" : "Solutions of Ill-Posed Problems. Winston",
      "author" : [ "A.N. Tikhonov", "V.Y. Arsenin" ],
      "venue" : null,
      "citeRegEx" : "Tikhonov and Arsenin,? \\Q1977\\E",
      "shortCiteRegEx" : "Tikhonov and Arsenin",
      "year" : 1977
    }, {
      "title" : "User-friendly tail bounds for sums of random matrices. Foundations of Computational Mathematics",
      "author" : [ "J.A. Tropp" ],
      "venue" : null,
      "citeRegEx" : "Tropp,? \\Q2011\\E",
      "shortCiteRegEx" : "Tropp",
      "year" : 2011
    }, {
      "title" : "Estimation of Dependences Based on Empirical Data",
      "author" : [ "V.N. Vapnik" ],
      "venue" : null,
      "citeRegEx" : "Vapnik,? \\Q1982\\E",
      "shortCiteRegEx" : "Vapnik",
      "year" : 1982
    }, {
      "title" : "Multi-class support vector machines",
      "author" : [ "J. Weston", "C. Watkins" ],
      "venue" : null,
      "citeRegEx" : "Weston and Watkins,? \\Q1998\\E",
      "shortCiteRegEx" : "Weston and Watkins",
      "year" : 1998
    } ],
    "referenceMentions" : [ {
      "referenceID" : 4,
      "context" : "More precisely, building on very recent matrix-based concentration inequalities provided by Tropp (2011) —sometimes referred to as noncommutative concentration inequalities— we establish a stability based framework for confusion-aware learning algorithm.",
      "startOffset" : 92,
      "endOffset" : 105
    }, {
      "referenceID" : 0,
      "context" : "In a sense, our framework and our results extend those of Bousquet and Elisseeff (2002), which are designed for scalar loss functions.",
      "startOffset" : 58,
      "endOffset" : 88
    }, {
      "referenceID" : 5,
      "context" : "1 Stability Following the early work of Vapnik (1982), the risk has traditionally been estimated through its empirical measure and a measure of the complexity of the hypothesis class such as Vapnik-Chervonenkis (VC-dim), fat-shattering dimension or Rademacher complexity.",
      "startOffset" : 40,
      "endOffset" : 54
    }, {
      "referenceID" : 0,
      "context" : "The main results in Bousquet and Elisseeff (2002) were obtained using the following definition of uniform stability.",
      "startOffset" : 20,
      "endOffset" : 50
    }, {
      "referenceID" : 0,
      "context" : "The main results in Bousquet and Elisseeff (2002) were obtained using the following definition of uniform stability. Definition 1 (Uniform stability Bousquet and Elisseeff (2002)).",
      "startOffset" : 20,
      "endOffset" : 179
    }, {
      "referenceID" : 0,
      "context" : "2 Non-Commutative McDiarmid In Bousquet and Elisseeff (2002), the authors make use of some well-known concentration inequalities to derive bounds.",
      "startOffset" : 31,
      "endOffset" : 61
    }, {
      "referenceID" : 0,
      "context" : "2 Non-Commutative McDiarmid In Bousquet and Elisseeff (2002), the authors make use of some well-known concentration inequalities to derive bounds. More specifically, they use a variation of Azuma’s inequality, due to McDiarmid McDiarmid (1989). It describes how a scalar function of independent random variables (the elements of our training set) normally concentrates around its mean, with variance depending on how changing one of the random variables impacts the value of the function.",
      "startOffset" : 31,
      "endOffset" : 244
    }, {
      "referenceID" : 0,
      "context" : "2 Non-Commutative McDiarmid In Bousquet and Elisseeff (2002), the authors make use of some well-known concentration inequalities to derive bounds. More specifically, they use a variation of Azuma’s inequality, due to McDiarmid McDiarmid (1989). It describes how a scalar function of independent random variables (the elements of our training set) normally concentrates around its mean, with variance depending on how changing one of the random variables impacts the value of the function. Some more recent work Tropp (2011) extends McDiarmid’s inequality to the matrix setting.",
      "startOffset" : 31,
      "endOffset" : 524
    }, {
      "referenceID" : 0,
      "context" : "2 Non-Commutative McDiarmid In Bousquet and Elisseeff (2002), the authors make use of some well-known concentration inequalities to derive bounds. More specifically, they use a variation of Azuma’s inequality, due to McDiarmid McDiarmid (1989). It describes how a scalar function of independent random variables (the elements of our training set) normally concentrates around its mean, with variance depending on how changing one of the random variables impacts the value of the function. Some more recent work Tropp (2011) extends McDiarmid’s inequality to the matrix setting. For the sake of self-containedness, we recall this non-commutative bound. Theorem 1 (Matrix bounded difference (Tropp (2011), corollary 7.",
      "startOffset" : 31,
      "endOffset" : 703
    }, {
      "referenceID" : 5,
      "context" : "The choice of such a dilation is directly motivated by the following property, recalled in Tropp (2011). Lemma 1.",
      "startOffset" : 91,
      "endOffset" : 104
    }, {
      "referenceID" : 3,
      "context" : "1 Hilbert Space Regularized Algorithms Many well-known and widely-used algorithms feature a minimization of a regularized objective functionTikhonov and Arsenin (1977). In the context of kernel machinesCristianini and Shawe-Taylor (2000), this regularizer Ω(h) may take the following form:",
      "startOffset" : 140,
      "endOffset" : 168
    }, {
      "referenceID" : 1,
      "context" : "In the context of kernel machinesCristianini and Shawe-Taylor (2000), this regularizer Ω(h) may take the following form:",
      "startOffset" : 33,
      "endOffset" : 69
    }, {
      "referenceID" : 0,
      "context" : "In order to study the stability properties of algorithms, minimizing a data-fitting term, penalized by such regularizers, in our multi-class setting, we need to introduce a minor definition that is an addition to definition 19 of Bousquet and Elisseeff (2002). Definition 3.",
      "startOffset" : 230,
      "endOffset" : 260
    }, {
      "referenceID" : 0,
      "context" : "Roughly, the idea is to exploit definition 3 in order to apply the theorem 22 of Bousquet and Elisseeff (2002) for each loss lq.",
      "startOffset" : 81,
      "endOffset" : 111
    }, {
      "referenceID" : 2,
      "context" : "2 Lee, Lin and Wahba model One of the most well-known and well-studied model for multi-class classification, in the context of SVM, was proposed by Lee et al. (2004). In this work, the authors suggest the use of the following loss function.",
      "startOffset" : 148,
      "endOffset" : 166
    }, {
      "referenceID" : 7,
      "context" : "3 Weston and Watkins model One of the oldest models, when it comes to multi-class SVM is due to Weston and Watkins (1998). They consider the following loss functions.",
      "startOffset" : 2,
      "endOffset" : 122
    } ],
    "year" : 2017,
    "abstractText" : "In this paper, we provide new theoretical results on the generalization properties of learning algorithms for multiclass classification problems. The originality of our work is that we propose to use the confusion matrix of a classifier as a measure of its quality; our contribution is in the line of work which attempts to set up and study the statistical properties of new evaluation measures such as, e.g. ROC curves. In the confusion-based learning framework we propose, we claim that a targetted objective is to minimize the size of the confusion matrix C, measured through its operator norm ‖C‖. We derive generalization bounds on the (size of the) confusion matrix in an extended framework of uniform stability, adapted to the case of matrix valued loss. Pivotal to our study is a very recent matrix concentration inequality that generalizes McDiarmid’s inequality. As an illustration of the relevance of our theoretical results, we show how two SVM learning procedures can be proved to be confusion-friendly. To the best of our knowledge, the present paper is the first that focuses on the confusion matrix from a theoretical point of view.",
    "creator" : "LaTeX with hyperref package"
  }
}