{
  "name" : "1212.1527.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Learning Mixtures of Arbitrary Distributions over Large Discrete Domains",
    "authors" : [ "Yuval Rabani", "Leonard J. Schulman", "Chaitanya Swamy" ],
    "emails" : [ "yrabani@cs.huji.ac.il.", "schulman@caltech.edu." ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n21 2.\n15 27\nv3 [\ncs .L\nG ]\n1 8\nSe p\n20 13\nThis task is information-theoretically impossible for k > 1 under the usual sampling process from a mixture distribution. However, there are situations (such as the above-mentioned topic model case) in which each sample point consists of several observations from the same mixture constituent. This number of observations, which we call the “sampling aperture”, is a crucial parameter of the problem.\nWe obtain the first bounds for this mixture-learning problem without imposing any assumptions on the mixture constituents. We show that efficient learning is possible exactly at the information-theoretically least-possible aperture of 2k − 1. Thus, we achieve near-optimal dependence on n and optimal aperture. While the sample-size required by our algorithm depends exponentially on k, we prove that such a dependence is unavoidable when one considers general mixtures.\nA sequence of tools contribute to the algorithm, such as concentration results for random matrices, dimension reduction, moment estimations, and sensitivity analysis."
    }, {
      "heading" : "1 Introduction",
      "text" : "We give an algorithm for learning a mixture of unstructured distributions. More specifically, we consider the problem of learning a mixture of k arbitrary distributions over a large finite domain [n] = {1, 2, . . . , n}. This finds applications in various unsupervised learning scenarios including collaborative filtering [29], and learning topic models from a corpus of documents spanning several topics [39, 11], which is often used as the prototypical motivating example for this problem. Our goal is to learn the probabilistic model that is hypothesized to generate the observed data. In particular, we learn the constituents of the mixture and their weights in the mixture. (In the topic models application, the mixture constituents are the topic distributions.)\nIt is information-theoretically impossible to reconstruct the mixture model from single-snapshot samples. Thus, our work relies on multi-snapshot samples. To illustrate, in the (pure documents)\n∗The Rachel and Selim Benin School of Computer Science and Engineering and the Center of Excellence on Algorithms, The Hebrew University of Jerusalem, Jerusalem 91904, Israel. Email: yrabani@cs.huji.ac.il.\n†Caltech, Pasadena, CA 91125, USA. Supported in part by NSF CCF-1038578, NSF CCF-0515342, NSA H9823006-1-0074, and NSF ITR CCR-0326554. Email: schulman@caltech.edu.\n‡Dept. of Combinatorics and Optimization, Univ. Waterloo, Waterloo, ON N2L 3G1, Canada. Supported in part by NSERC grant 32760-06, an NSERC Discovery Accelerator Supplement Award, and an Ontario Early Researcher Award. Email: cswamy@math.uwaterloo.ca.\ntopic model introduced in [39], each document consists of a bag of words generated by selecting a topic with probability proportional to its mixture weight and then taking independent samples from this topic’s distribution (over words); so n is the size of the vocabulary and k is the number of topics. Notice that typically n will be quite large, and substantially larger than k. Also, clearly, if very long documents are available, the problem becomes easy, as each document already provides a very good sample for the distribution of its topic. Thus, it is desirable to keep the dependence of the sample size on n as low as possible, while at the same time minimize what we call the aperture, which is the number of snapshots per sample point (i.e., words per document). These parameters govern both the applicability of an algorithm and its computational complexity.\nOur results. We provide the first bounds for the mixture-learning problem without making any limiting assumptions on the mixture constituents. Let probability distributions p1, . . . , pk ∈ ∆n−1 denote the k-mixture constituents, where ∆n−1 is the (n − 1)-simplex, and w1, . . . , wk denote the mixture weights. Our algorithm uses\nO\n(\nk3n lnn\nǫ6\n)\n+O\n(\nk2n ln6 n ln ( k ǫ )\nǫ4\n)\n+O\n(\nk\nǫ\n)O(k2)\n(1)\ndocuments (i.e., samples) and reconstructs with high probability (see Theorem 4.1) each mixture constituent up to ℓ1-error ǫ, and each mixture weight up to additive error ǫ. We make no assumptions on the constituents. The asymptotic notation hides factors that are polynomial in wmin := mint wt and the “width” of the mixture (which intuitively measures the minimum variation distance between any two constituents). The three terms in (1) correspond to the requirements for the number of 1-, 2-, and (2k − 1)-snapshots respectively. So we need aperture 2k − 1 only for a small part of the sample (and this is necessary).\nNotably, we achieve near-optimal dependence on n and optimal aperture. To see this, and put our bounds in perspective, notice importantly that we recover the mixture constituents within ℓ1distance ǫ. One needs Ω ( n/ǫ2 )\nsamples to learn even a single arbitrary distribution over [n] (i.e., k = 1) within ℓ1-error ǫ; for larger k but fixed aperture (independent of n), a sample size of Ω(n) is necessary to recover even the expectation of the mixture distribution with constant ℓ1-error. On the other hand, aperture Ω ( (n + k2) log nk )\nis sufficient for algorithmically trivial recovery of the model with constant ℓ∞ error using few samples. Restricting the aperture to 2k − 2 makes recovery impossible to arbitrary accuracy (without additional assumptions): we show that there are two far-apart k-mixtures that generate exactly the same aperture-(2k− 2) sample distribution; moreover, we prove that with O(k) aperture, an exponential in k sample size is necessary for arbitrary-accuracy reconstruction. These lower bounds hold even for n = 2, and hence apply to arbitrary mixtures even if we allow O(k log n) aperture. Also, they apply even if we only want to construct a k-mixture source that is close in transportation distance to the true k-mixture source (as opposed to recovering the parameters of the true mixture). Section 6 presents these lower bounds. (Interestingly, an exponential in k sample-size lower bound is also known for the problem of learning a mixture of k Gaussians [36], but this lower bound applies for the parameter-recovery problem and not for reconstructing a mixture that is close to the true Gaussian mixture.)\nOur work yields new insights into the mixture-learning problem that nicely complements the recent interesting work of [4, 3, 2]. These papers posit certain assumptions on the mixture constituents, use constant aperture, and obtain incomparable sample-size bounds: they recover the constituents up to ℓ2 or ℓ∞ error using sample size that is poly(k) and sublinear in (or independent of) n. An important new insight revealed by our work is that such bounds of constant aperture and poly(k) sample size are impossible to achieve for arbitrary mixtures. Moreover, if we seek to\nachieve ℓ1-error ǫ, there are inputs for which their sample size is Ω(n 3) (or worse, again ignoring dependence on wmin and “width”; see Appendix B). This is a significantly poorer dependence on n compared to our near-linear dependence (so our bounds are better when n is large but k is small). To appreciate a key distinction between our work and [4, 3, 2], observe that with Ω(n3) samples, the entire distribution on 3-snapshots can be estimated fairly accurately; the challenge in [4, 3, 2] is therefore to recover the model from this relatively noiseless data. In contrast, a major challenge for achieving ℓ1-reconstruction with O(n polylog n) samples is to ensure that the error remains bounded despite the presence of very noisy data due to the small sample size, and we develop suitable machinery to achieve this.\nWe now give a rough sketch of our algorithm (see Section 3) and the ideas behind its analysis (Section 4). Let P = (p1, . . . , pk), r = ∑\nt wtp t be the expectation of the mixture, and k′ =\nrank(p1 − r, . . . , pk − r). We first argue that it suffices to focus on isotropic mixtures (Lemma 3.3). Our algorithm reduces the problem to the problem of learning one-dimensional mixtures. Note that this is a special case of the general learning problem that we need to be able to solve (since we do not make any assumptions about the rank of P ). We choose k′ random lines that are close to the affine hull, aff(P ), of P and “project” the mixture on to these k′ lines. We learn each projected mixture, which is a one-dimensional mixture-learning problem, and combine the inferred projections on these k′ lines to obtain k points that are close to aff(P ). Finally, we project these k′ points on to ∆n−1 to obtain k distributions over [n], which we argue are close (in ℓ1-distance) to p1, . . . , pk.\nVarious difficulties arise in implementing this plan. We first learn a good approximation to aff(P ) using spectral techniques and 2-snapshots. We use ideas similar to [35, 6, 34], but our challenge is to show that the covariance matrix A = ∑\ntwt(p t−r)(pt−r)† can be well-approximated\nby the empirical covariance matrix with only O(n ln6 n) 2-snapshots. A random orthonormal basis of the learned affine space supplies the k′ lines on which we project our mixture. Of course, we do not know P , so “projecting” on to a basis vector b actually means that we project snapshots from P on to b by mapping item i to bi. For this to be meaningful, we need to ensure that if the mixture constituents are far apart in variation distance then their projections (b†pt)t∈[k] are also well separated relative to the spread of the support {b1, . . . bn} of the one-dimensional distribution. In general, we can only claim a relative separation of Θ (\n1√ n\n) (since mint6=t′ ‖pt − pt ′‖2 may be\nΘ ( 1√ n ) ). We avoid this via a careful balancing act: we prove (Lemma 4.3) that the ℓ∞ norm of unit vectors in aff(P ) is O (\n1√ n\n)\n, and argue that this isotropy property suffices since b is close to\naff(P ). Finally, a key ingredient of our algorithm (see Section 5) is to show how to solve the onedimensional mixture-learning problem and learn the real projections (b†pt)t∈[k] from the projected snapshots. This is technically the most difficult step and the one that requires aperture 2k − 1 (the smallest aperture at which this is possible). We show that the projected snapshots on b yield empirical moments of a related distribution and use this to learn the projections and the mixture weights via a method of moments (see, e.g., [25, 24, 31, 10, 36, 3]). One technical difficulty is that variation distance in ∆n−1 translates to transportation distance [42] in the one-dimensional projection. We use a combination of convex programming and numerical-analysis techniques to learn the projections from the empirical “directional” moments. In the process, we establish some novel properties about the moment curve—an object that plays a central role in convex and polyhedral geometry [8]—that may be of independent interest.\nRelated work. The past decade has witnessed tremendous progress in the theory of learning statistical mixture models. The most striking example is that of learning mixtures of high dimen-\nsional Gaussians. Starting with Dasgupta’s groundbreaking paper [20], a long sequence of improvements [21, 5, 41, 32, 1, 24, 13] culminated in the recent results [31, 10, 36] that essentially resolve the problem in its general form. In this vein, other highly structured mixture models, such as mixtures of discrete product distributions [33, 26, 18, 25, 14, 16] and similar models [18, 9, 37, 32, 19, 15, 22], have been studied intensively. One important difference between this line of work and ours is that the structure of those mixtures enables learning using single-snapshot samples, whereas this is impossible in our case. Another interesting difference between our setting and the work on structured models (and this is typical of most results on PAC-style learning) is that the amount of information in each sample point is roughly in the same ballpark as the information needed to describe the model. In our setting, the amount of information in each sample point is exponentially sparser than the information needed to describe the model to good accuracy. Thus, the topic-model learning problem motivates the natural question of inference from sparse samples. This issue is also encountered in collaborative filtering; see [34] for some related theoretical problems.\nRecently and independently, [4, 3, 2] have considered much the same question as ours.1 They make certain assumptions about the mixture constituents which makes it possible to learn the mixture with constant aperture and poly(n, k) sample size (for ℓ1-error). In comparison with our work, their sample bounds are attractive in terms of k but come at the expense of added assumptions (which are necessary), and have a worse dependence on n.\nThe assumptions in [4, 3, 2] impose some limitations on the applicability of their algorithms. To understand this, it is illuminating to consider the case where all the pts lie on a line-segment in ∆n−1 as an illustration. This poses no problems for our algorithm: we recover the pts along with their mixture weights. However, as we show below, the algorithms in [4, 3, 2] all fail to reconstruct this mixture. Anandkumar et al. [3] solve the same problem that we consider, under the assumption that P (viewed as an n × k matrix) has rank k. This is clearly violated here, rendering their algorithm inapplicable. The other two papers [4, 2] consider the setting where each multi-snapshot is generated from a combination of mixture constituents [39, 28]: first a convex combination λ ∈ ∆k−1 is sampled from a mixture distribution T on ∆k−1, then the snapshot is generated by sampling from the distribution\n∑k t=1 λtp t. The goal is to learn the mixture constituents and the mixture distribution. (The problem we consider is the special case where T places weight wt on the t-th vertex of ∆k−1.) [4] posits a ρ-separability assumption on the mixture constituents, wherein each pt has a unique “anchor word” i such that pti ≥ ρ and pt ′ i = 0 for every t ′ 6= t, whereas [2] weakens this to the requirement that P has rank k. Both papers handle the case where T is the Dirichlet distribution (which gives the latent Dirichlet model [12]); [4] obtains results for other mixture distributions as well.\nIn order to apply these algorithms, we can view the input as being specified by two constituents, x and y, which are the end points of the line segment; T then places weight wt on the convex combination (λt, 1− λt)†, where pt = λtx+(1−λt)y. This T is far from the Dirichlet distribution, so [3] does not apply here. Suppose that x and y satisfy the ρ-separability condition. (Note that ρ may only be O (\n1 n\n)\n, even if x and y have disjoint supports.) We can then apply the algorithm of Arora et al. [4]. But this does not recover T ; it returns the “topic correlation” matrix ET [λλ†], which does not reconstruct the mixture (w,P ).\nThis limitation should not be surprising since [4] uses constant aperture. Indeed, [4] notes that it is impossible to reconstruct T with arbitrary accuracy (with any constant aperture) even if one knows the constituents x and y. In this context, we remark that our earlier work [40] uses the approach presented in this paper and solves the problem for arbitrary mixtures of two distributions,\n1An earlier stage of this work, including the case k = 2 as well as some other results that are not subsumed by this paper, dates to 2007. The last version of that phase has been posted since May 2008 at [40]. The extension to arbitrary k is from last year.\nyielding a crisp statement about the tradeoff between the sampling aperture and the accuracy with which T can be learnt.\nOur methods bear some resemblance with the recent independent work of Gravin et al. [27] who consider the problem of recovering the vertices of a polytope from its directional moments. [27] solves this problem for a polynomial density function assuming that exact directional moments are available; they do not perform any sensitivity analysis for measuring the error in their output if one has noisy information. In contrast, we solve this problem given only noisy empirical moment statistics and using much smaller aperture, albeit when the polytope is a subset of the (n − 1)- simplex and the distribution is concentrated on its vertices.\nFinally, it is also pertinent to compare our mixture-learning problem with the problem of learning a mixture of product distributions (e.g., [25]). Multi-snapshot samples can be thought of as single-snapshot samples from the power distribution on [n]K , where K is the aperture. The product distribution literature typically deals with samples spaces that are the product of many small cardinality components, whereas our problem deals with samples spaces that are the product of few large cardinality components."
    }, {
      "heading" : "2 Preliminaries",
      "text" : ""
    }, {
      "heading" : "2.1 Mixture sources, snapshots, and projections",
      "text" : "Let [n] denote {1, 2, . . . , n}, and ∆n−1 denote the (n − 1)-simplex {x ∈ Rn≥0 : ∑ i xi = 1}. A k-mixture source (w,P ) on [n] consists of k mixture constituents P = (p1, . . . , pk), where pt has support [n] for all t ∈ [k], along with the corresponding mixture weights w = (w1, . . . , wk) ∈ ∆k−1. An m-snapshot from (w,P ) is obtained by choosing t ∈ [k] according to the distribution w, and then choosing i ∈ [n] m times independently according to the distribution pt. The probability distribution on m-snapshots is thus a mixture of k power distributions on the product space [n]m. We also consider mixture sources whose constituents are distributions on R. A k-mixture source (w,P ) on R consists of k mixture constituents P = (p1, p2, . . . , pk), where each pt is a probability distribution on R, along with corresponding mixture weights w = (w1, . . . , wk) ∈ ∆k−1.\nGiven a distribution p on [n] and a vector x ∈ Rn, we define the projection of p on x, denoted πx(p), to be the discrete distribution on R that assigns probability mass ∑\ni:xi=β pi to β ∈ R. (Thus,\nπx(p) has support {x1, . . . , xn} and E[πx(p)] = x†p.) Given a k-mixture source (w,P ) on [n], we define the projected k-mixture source (w, πx(P )) on R to be the k-mixture source on R given by (\nw, (πx(p 1), . . . , πx(p k)) )\n. We also denote by (w,E[πx(P )]) the distribution that assigns probability mass wt to E[πx(p\nt)] = x†pt for all t ∈ [k]. This is an example of what we call a k-spike distribution, which is a distribution on R that assigns positive probability mass to k points in R."
    }, {
      "heading" : "2.2 Transportation distance for mixtures",
      "text" : "Let ( w, (p1, . . . , pk) ) and ( w̃, (p̃1, . . . , p̃ℓ) )\nbe k- and ℓ- mixture sources on [n] respectively. The transportation distance (with respect to the total variation distance 12‖x−y‖1 on measures on ∆n−1) between these two sources, denoted by Tran(w,P ; w̃, P̃ ), is the optimum value of the following linear program (LP).\nmin k ∑\ni=1\nℓ ∑\nj=1\nxij · 1\n2 ‖pi − p̃j‖1 s.t.\nℓ ∑\nj=1\nxij = wi ∀i ∈ [k], k ∑\ni=1\nxij = w̃j ∀j ∈ [ℓ], x ≥ 0.\nThe transportation distance Tran(w,α; w̃, α̃) between a k-spike distribution ( w,α = (α1, . . . , αk) ) and an ℓ-spike distribution ( w̃, α̃ = (α̃1, . . . , α̃ℓ) )\nis defined as the optimum value of the above LP with the objective function replaced by ∑\ni∈[k],j∈[ℓ] xij |αi − α̃j |."
    }, {
      "heading" : "2.3 Perturbation results and operator norm of random matrices",
      "text" : "Definition 2.1. The operator norm of A (induced by the ℓ2 norm) is defined by ‖A‖op = maxx 6=0 ‖Ax‖2 ‖x‖2 . The Frobenius norm of A = (Ai,j) is defined by ‖A‖F = √ ∑ i,j A 2 i,j .\nLemma 2.2 (Weyl; see Theorem 4.3.1 in [30]). Let A and B be n×n matrices such that ‖A−B‖op ≤ ρ. Let λ1(A) ≥ . . . ≥ λn(A), and λ1(B) ≥ . . . ≥ λn(B) be the sorted list of eigenvalues of A and B respectively. Then |λi(A)− λi(B)| ≤ ρ for all i = 1, . . . , n.\nLemma 2.3. Let A,B be n × n positive semi-definite (PSD) matrices whose nonzero eigenvalues are at least ε > 0. Let ΠA and ΠB be the projection operators onto the column spaces of A and B respectively. Let ‖A−B‖op ≤ ρ. Then ‖ΠA −ΠB‖op ≤ √ 4ρ/ε.\nProof. Note that AΠA = A, Π 2 A = ΠA, BΠB = B, and Π 2 B = ΠB . Let x be a unit vector. Since ‖(A − B)‖op ≤ ρ and ΠB is a contraction, ‖(A − B)ΠBx‖ ≤ ρ‖ΠBx‖ ≤ ρ. Now note that (A − B)ΠBx = AΠBx − Bx so by the triangle inequality, we have ‖AΠBx − Ax‖ ≤ 2ρ. Now we can also write AΠBx− Ax = A(ΠB − I)x = A(ΠAΠB − ΠA)x. Since A here is acting on a vector that has already been projected down by ΠA, we can conclude\n2ρ ≥ ‖AΠBx−Ax‖ = ‖A(ΠAΠB −ΠA)x‖ ≥ ε‖(ΠAΠB −ΠA)x‖.\nThus, 2ρ/ε ≥ ‖(ΠA − ΠAΠB)x‖. By the symmetric argument we also can write 2ρ/ε ≥ ‖(ΠB − ΠBΠA)x‖. Adding these and applying the triangle inequality we have\n4ρ/ε ≥ ‖(ΠA −ΠAΠB +ΠB −ΠBΠA)x‖ = ‖(Π2A −ΠAΠB −ΠBΠA +Π2B)x‖ = ‖(ΠA −ΠB)2x‖.\nTheorem 2.4 ([43]). For every µ > 0, there is a constant κ = κ(µ) = O(µ) > 0 such that the following holds. Let Xi,j , 1 ≤ i ≤ j ≤ n be independent random variables with |Xij | ≤ K, E[Xi,j ] = 0, and Var(Xi,j) ≤ σ2 for all i, j ∈ [n], where σ ≥ κ2n−1/2K ln2 n. Let A be the symmetric matrix with entries Ai,j = Xmin(i,j),max(i,j) for all i, j ∈ [n]. Then, Pr [ ‖A‖op ≤ 2σ √ n+ κ(Kσ)1/2n1/4 lnn ] ≥ 1− n−µ."
    }, {
      "heading" : "3 Our algorithm",
      "text" : "We now describe our algorithm that uses 1-, 2-, and (2k − 1)-snapshots from the mixture source (w,P ). Given a matrix Z, we use Span(Z) to denote the column space of Z. Let r =\n∑k t=1 wtp t\ndenote the 1-snapshot distribution of (w,P ). Let M be the n×n symmetric matrix representing the 2-snapshot distribution of (w,P ); so Mi,j is the probability of obtaining the 2-snapshot (i, j) ∈ [n]2. Let R = rr†.\nProposition 3.1. M = ∑k t=1 wtp tpt† = R+A, where A = ∑k t=1 wt(p t − r)(pt − r)†.\nNote that M and A are both PSD. We say that (w,P ) is ζ-wide if (i) ‖p − q‖2 ≥ ζ√n for any two distinct p, q ∈ P ; and (ii) the smallest non-zero eigenvalue of A is at least ζ2‖r‖∞ ≥ ζ 2\nn . We assume that wmin := mint wt > 0. Let k\n′ = rank(A) ≤ k−1. It is easy to estimate r using Chernoff bounds (Lemma A.1).\nLemma 3.2. For every µ ∈ N and every σ > 0, if we use N ≥ 8(µ+2) σ3 ·n lnn independent 1-snapshots and set r̃i to be the frequency of i in these 1-snapshots for all i ∈ [n], then with probability at least 1− n−µ the following hold.\n(1− σ)ri ≤ r̃i ≤ (1 + σ)ri ∀i with ri ≥ σ\n2n , r̃i ≤ (1 + σ)σ/2n ∀i with ri <\nσ\n2n . (2)\nIt will be convenient in the sequel to assume that our mixture source (w,P ) is isotropic, by which we mean that 12n ≤ ri ≤ 2n for all i ∈ [n]; notice that this implies that pti ≤ 2wminn for all i ∈ [n]. We show below that this can be assumed at the expense of a small additive error.\nLemma 3.3. Suppose that we can learn, with probability 1 − 1ω , the constituents of an isotropic ζ-wide k-mixture source on [n] to within transportation distance ǫ using N1(n; ζ, ω, ǫ), N2(n; ζ, ω, ǫ), and N2k−1(n; ζ, ω, ǫ) 1-, 2-, and (2k−1)-snapshots respectively. Then, we can learn, with probability 1−O (\n1 ω\n)\n, the constituents of an arbitrary ζ-wide k-mixture source (w,P ) on [n] to within transporta-\ntion distance 2ǫ using O ( lnω σ3 · n lnn ) + 6ωN1 ( n σ , ζ 2 , ω, ǫ ) , 6ωN2 ( n σ , ζ 2 , ω, ǫ ) , and 6ωN2k−1 ( n σ , ζ 2 , ω, ǫ ) 1-, 2-, and (2k − 1)-snapshots respectively, where σ = ǫζ232kwmin .\nProof. Given (w,P ), we first compute an estimate r̃ satisfying (2), where µ = 2 + lnω, using O (\nlnω σ3 · n lnn\n)\n1-snapshots. We assume in the sequel that (2) holds. Consider the following modification of the mixture constituents. We eliminate items i such that r̃i < 2σ n . Each remaining item i is “split” into ni = ⌊nr̃i/σ⌋ items, and the probability of i is split equally among its copies. The mixture weights are unchanged. From (2), we have that ri < 4σ n if i is eliminated. So the total weight of eliminated items is at most 4σ. Let n′ = ∑\ni:r̃i≥2σ/n ni ≤ n σ be the number of new items.\nLet P̂ = (p̂1, . . . , p̂k) denote the modified mixture constituents, and r̂ denote the distribution of the modified 1-snapshots. We prove below that the modified mixture (w, P̂ ) is isotropic and ζ/2-wide.\nWe use the algorithm for isotropic mixture sources to learn (w, P̂ ) within transportation distance ǫ, using the following procedure to sample m-snapshots from (w, P̂ ). We obtain an m-snapshot from (w,P ). We eliminate this snapshot if it includes an eliminated item; otherwise, each item i in the snapshot is replaced by one of its ni copies, chosen uniformly at random (and independently of previous such choices). From the inferred modified mixture source, we can obtain an estimate of the original mixture source by aggregating, for each inferred mixture constituent, the probabilities of the items that we split, and setting the probability of each eliminated item to 0. This degrades the quality of the solution by the weight of the eliminated items, which is at most an additive 4σ ≤ ǫ term in the transportation distance.\nThe probability that anm-snapshot from (w,P ) survives is at least (1−4σ)m ≥ 12 form ≤ 2k−1. Therefore, with probability at least 1− 13ω , we need at most 6ωN m-snapshots from (w,P ) to obtain N m-snapshots from (w, P̂ ). (If we violate this bound, we declare failure.) Thus, we use at most the stated number of 1-, 2-, and (2k−1)-snapshots from (w,P ) and succeed with probability 1−O (\n1 ω\n)\n.\nWe conclude by showing that (w, P̂ ) is isotropic and ζ/2-wide. Let S = {i ∈ [n] : r̃i < 2σ/n} denote the set of eliminated items. Recall that r̃ satisfies (2). So we have 3132 ≤ r̃iri ≤ 33 32 for every non-eliminated item. We use iℓ, where ℓ = 1, . . . , ni, to denote a new item obtained by splitting item i. Define ni = 0 if i is eliminated.\nThe number n′ of new items is at most nσ and at least ∑ i/∈S 2 3 · nr̃iσ ≥ 23 · nσ · (1 − 2σ) ≥ 5n8σ .\nLet K = ∑ i/∈S ri ≥ 1 − 4σ ≥ 7/8. For every new item iℓ, we have r̂iℓ ≥ rinr̃i/σ ≥ 32σ 33n ≥ 12n′ and r̂iℓ ≤ 1K · 32 · rinr̃i/σ ≤ 384σ 217n ≤ 2n′ . Thus, (w, P̂ ) is isotropic.\nNow consider the width of (w, P̂ ). For t = 1, . . . , k, define p′t ∈ Rn to be the vector where p′ti = 0 if i ∈ S, and p′ti = pti otherwise. For any distinct t, t′ ∈ [k], we have ‖p̂t − p̂t ′‖2 ≥ ‖p̂ t−p̂t′‖1√\nn′\nand\n‖p̂t − p̂t′‖1 = ‖p′t − p′t′‖1\nK ≥ ‖pt − pt′‖1 −\n∑ i∈S max{pti, pt ′ i } ≥ ζ − n · 4σ wminn ≥ ζ/2.\nLet Â = ∑k t=1 wt(p̂ t − r̂)(p̂t − r̂)†, which is an n′ × n′ matrix. We need to prove that the smallest non-zero eigenvalue of Â is at least ζ 2\n4 · ‖r̂‖∞. It will be convenient to define the following matrices. Let B ∈ R([n]\\S)×([n]\\S) be the matrix defined by setting Bi,j = Ai,j for all i, j /∈ S. Define A′ to be the n × n matrix obtained by padding B with 0s: set A′i,j = Ai,j = Bi,j if i, j /∈ S, and equal to 0 otherwise. It is easy to see that the non-zero eigenvalues of A′ coincide with the non-zero eigenvalues of B. Define X ∈ Rn′×([n]\\S) as follows. Letting {iℓ}i/∈S,ℓ=1,...,ni index the rows of X, we set Xiℓ,j = 1 Kni if j = i, and 0 otherwise. Notice that Â = XBX†. To see this, it is convenient to define a padded version Y ∈ Rn′×[n] of X by setting Yiℓ,j = Xiℓ,j if j /∈ S and 0 otherwise. Then, we have p̂t = Y pt for all t ∈ [k], and hence, Â = Y AY † = XBX†.\nNote that rank(A′) ≤ rank(A) = k′. Consider A − A′. Suppose i ∈ S, so pti ≤ 4σwminn for all t ∈ [k]. Then,\n|(A−A′)i,j | = |Ai,j | = |Mi,j −Ri,j| ≤ max {\nk ∑\nt=1\nwtp t ip t j, rirj\n} ≤ 4σ wminn · rj ≤ 4σ wminn · ‖r‖∞.\nHence, ‖A−A′‖op ≤ ‖A−A′‖F ≤ 8σwmin · ‖r‖∞. By Lemma 2.2, this implies that\nλk′(B) = λk′(A ′) ≥ λk′(A)− ‖A−A′‖op ≥\n( ζ2 − 8σ wmin ) ‖r‖∞ ≥ ζ2 2 · ‖r‖∞.\nWe now argue that λk′(XBX †) ≥ λk′(B)/(maxi ni). By the Courant-Fischer theorem (see, e.g., Theorem 4.2.11 in [30]), this is equivalent to showing that there exist vectors y1, . . . , yk ′ ∈ Rn′ , such that for every unit vector v ∈ Span(y1, . . . , yk′), we have v†(XBX†)v ≥ λk′ (B)maxi ni . We know that there are vectors u1, . . . , uk ′ ∈ R[n]\\S such that zBz† ≥ λk′(B)‖z‖2 for every z ∈ Span(u1, . . . , uk ′ ). Set ytiℓ = u t i for every copy iℓ of item i ∈ [n] \\S, and every t ∈ [k′]. Consider any v ∈ Span(y1, . . . , yk ′ ). We have that z = X†v ∈ Span(u1, . . . , uk′), and since viℓ = zi for every copy iℓ of item i ∈ [n] \\ S we have that ‖v‖22 ≤ (maxi ni)‖z‖22. Therefore, if v is a unit vector, we have v†XBX†v = z†Bz ≥ λk′(B)‖z‖22 ≥ λk′ (B) maxi ni .\nPutting everything together, we have that λk′(Â) ≥ ζ 2‖r‖∞\n2maxi ni . Note that ‖r‖∞ ≥ 3233‖r̃‖∞ and\n‖r̃‖∞ maxi ni ≥ σn ≥ 217384‖r̂‖∞. So the smallest non-zero eigenvalue of Â is λk′(Â) ≥ ζ2 4 ‖r̂‖∞.\nAlgorithm overview. Our algorithm for learning an isotropic k-mixture source on [n] takes three parameters: ζ ≤ 1 such that (w,P ) is ζ-wide, ω ∈ N, which controls the success probability of the algorithm, and δ ∈ (0, 1), which controls the statistical distance between the constituents of the learnt model and the constituents of the correct model. For convenience, we assume that δ is sufficiently small. The output of the algorithm is a k-mixture source (w̃, P̃ ) such that with probability 1−O (\n1 ω\n)\n, ‖w− w̃‖∞ and ‖pt− p̃t‖1 for all t ∈ [k] tend to 0 as δ → 0 (see Theorem 4.1). The algorithm (see Algorithm 1) consists of three stages. First, we reduce the dimensionality of the problem from n to k′ using only 1- and 2-snapshots. By Lemma 3.2, we have an estimate r̃ that is component-wise close to r. Thus, R̃ = r̃r̃† is close in operator norm to R. So we focus on learning the column space of A for which we employ spectral techniques. Leveraging Theorem 2.4, we argue (Lemma 4.2) that by using O(n ln6 n) 2-snapshots, one can compute (with high probability) a good enough estimate M̃ of M , and hence obtain a PSD matrix Ã such that ‖A− Ã‖op is small.\nThe remaining task is to learn the projection of P on the affine space r̃ + Span(Ã), and the mixture weights, which then yields the desired k-mixture source (w̃, P̃ ). We divide this into two steps. We choose a random orthonormal basis {b1, . . . , bk′} of Span(Ã). For each bj , we consider the projected k-mixture source (w, πbj (P )) on R. In Section 5, we devise a procedure to learn the corresponding k-spike distribution (w,E[πbj (P )]) using (2k− 1)-snapshots from (w, πbj (P )) (which we can obtain using (2k − 1)-snapshots from (w,P )). Applying this procedure (see Lemma 4.7), we obtain weights w̃j1, . . . , w̃ j k and k (distinct) values α j 1, . . . , α j k such that each true spike (wt, b † jp t) maps to a distinct inferred spike (w̃j σj(t) , αj σj (t) ).\nFinally, we match up σj and σk′ for all j ∈ [k′ − 1] to obtain k points in r̃ + Span(Ã) that are close to the projection of P on r̃ + Span(Ã). For every j ∈ [k′ − 1], we generate a random unit “test vector” zj in Span(bj , bk′) and learn the projections {z†jpt}t∈[k]. Since (w,P ) is ζ-wide, results about random projections and the guarantees obtained from our k-spike learning procedure imply that z†j (α j t1bj + α k′ t2bk′) is close to some value in {z † jp t}t∈[k] iff there is some t such that αjt1 and αk ′ t2 are close respectively to b†jpt and b † k′p t (Lemma 4.8). Thus, we can use the learned projections of {z†jpt}t∈[k] to match up {α j t}t∈[k] and {αk ′ t }t∈[k].\nAlgorithm 1. Input: an isotropic ζ-wide k-mixture source (w,P ) on [n], and parameters ω > 1 and δ > 0. Output: a k-mixture source (w̃, P̃ ) on [n] that is “close” to (w,P ).\nDefine T = 3ωk4, H = 4 w2 min ζ √ n and L = ζ 64ω1.5k4 √ n . We assume that δ ≤ w\n3 min ζ4\n229ω5k16 . Let κ = κ(2 + lnω)\nbe given by Theorem 2.4; we assume κ ≥ 1 for convenience. Define c = 6400κ2 w2\nmin δ2\n· ln ( 1 δ ) . We assume that\nw2min ≥ 240κ ln 2.5 n√ n .\nA1. Dimension reduction.\nA1.1 Use Lemma 3.2 with µ = 2 + lnω and σ = δ48 to compute an estimate r̃ of r. Set R̃ = r̃r̃ †. A1.2 Independent of all other random variables, choose a Poisson random variable N2 with expectation E[N2] = cn ln\n6 n. Choose N2 independent 2-snapshots and construct a symmetric n × n matrix M̃ as follows: set M̃i,i = frequency of the 2-snapshot (i, i) in the sample for all i ∈ [n], and M̃i,j = M̃j,i = half the combined frequency of 2-snapshots (i, j) and (j, i) in the sample, for all i, j ∈ [n], i 6= j.\nA1.3 Compute the spectral decomposition M̃ − R̃ = ∑ni=1 λiviv † i , where λ1 ≥ . . . ≥ λn. A1.4 Set Ã = ∑\ni:λi≥ζ2/2n λiviv † i . Note that Ã is PSD.\nA2. Learning projections of (w, P ) on random vectors in Span(Ã).\nA2.1 Pick an orthonormal basis B = {b1, . . . , bk′} for Span(Ã) uniformly at random. A2.2 Set (w̃j , αj) ← Learn ( bj, δ, 1\n6ωk\n) for all j = 1, . . . , k′.\nA3. Combining the projections to obtain (w̃, P̃ ).\nA3.1 Pick θ ∈ [0, 2π] uniformly at random. A3.2 For each j = 1, . . . , k′ − 1, we do the following.\n– Let zj = bj cos θ + bk′ sin θ. – Set (ŵj , α̂j) ← Learn ( zj , δ, 1\n6ωk\n)\n.\n– For each t1, t2 ∈ [k], if there exists t ∈ [k] such that ∣ ∣(αjt1bj+α k′ t2bk′) †zj− α̂jt ∣\n∣ ≤ ( √ 2+1)L/(2+\n5T ) then set ̺j(t2) = t1.\nA3.3 Define ̺k ′ (t) = t for all t ∈ [k]. A3.4 For every t ∈ [k]: set w̃t = ( ∑k′ j=1 w̃ j ̺j(t) ) /k′, p̂t = r̃ + ∑k′ j=1 ( αj̺j(t) − b † j r̃ ) bj , and p̃ t =\nargminx∈∆n−1 ‖x− p̂t‖1, which can be computed by solving an LP. Return ( w̃, P̃ = (p̃1, . . . , p̃k) ) .\nAlgorithm Learn(v, ς, ε) Input: a unit vector v ∈ Span(Ã), and parameters ς > 0, ε > 0. We assume that (a) |v†(p− q)| ≥ L for all distinct p, q ∈ P ; and (b) 1024kς < wminL16H . Output: a k-spike distribution ( w̄, (γ1, . . . , γk) ) close to (w,E[πv(P )]).\nL1. Solve the following convex program:\nmin ‖x‖∞ s.t. v†x ≥ 1− 4δ\nζ2 , ‖x‖22 ≤ 1 (Qv)\nto obtain x∗; set a = x ∗ ‖x∗‖2 . We prove in Lemma 4.4 that ‖a‖∞ ≤ H and |a †(p − q)| ≥ L2 for all\np, q ∈ P, p 6= q. L2. Let s = ς4k. Apply the procedure in Section 5 leading to Theorem 5.1 for ( w, πa/2H(P ) )\nto infer a k-spike distribution (w̄, β) that, with probability at least 1 − ε, is within transportation distance O ( sΩ(1/k) ) from ( w,E[πa/2H(P )] )\n. This uses a sample of (2k− 1)-snapshots of size 3k24ks−4k ln(4k/ε). L3. For every t ∈ [k], set γt = (2Hβt)(a†v). Return (w̄, γ). Remark 3.4. We cannot compute the spectral decomposition in step A1.3 exactly, or solve (Qv) exactly in step L1, since the output may be irrational. However, one can obtain a decomposition such that ‖M̃ − R̃− ∑n\ni=1 λiviv † i ‖op = O ( δ n )\nand compute a 2-approximate solution to (Qv) in polytime, and this suffices: slightly modifying the constants H and c makes the entire analysis go through. We have chosen the presentation above to keep exposition simple."
    }, {
      "heading" : "4 Analysis",
      "text" : "Theorem 4.1. Algorithm 1 uses O ( lnω δ3 · n lnn ) 1-snapshots, O ( ln2 ω ln(1/δ)\nδ2w2 min\n· n ln6 n ) 2-snapshots,\nand O ( k24k δ16k2 · ln(24ωk2) ) (2k − 1)-snapshots, and computes a k-mixture source (w̃, P̃ ) on [n] such that with probability 1−O (\n1 ω\n)\n, there is a permutation σ : [k] 7→ [k] such that for all t = 1, . . . , k,\n|wt − w̃σ(t)| = O (δω1.5k5\nw2minζ 2\n)\nand ‖pt − p̃σ(t)‖1 = O (\n√ kδ\nw1.5minζ\n)\n.\nHence, Tran(w,P ; w̃, P̃ ) = O (\n√ kδ\nw1.5minζ\n)\n. The running time is polynomial in the sample size.\nThe roadmap of the proof is as follows. By Lemma 3.2, with probability at least 1 − 1 ωn2 , (\n1− δ48 ) ri ≤ r̃i ≤ ( 1+ δ48 ) ri for all i ∈ [n]. We assume that this holds in the sequel. In Lemma 4.2, we prove that the matrix Ã computed after step A1 is a good estimate of A. In Lemma 4.3, we derive some properties of the column space of A. Lemma 4.4 then uses these properties to show that algorithm Learn returns a good approximation to (w,E[πv(P )]). Claim 4.5 and Lemma 4.6 prove that the projections of the mixture constituents on the bjs and the zjs are well-separated. Combining this with Lemma 4.4, we prove in Lemma 4.7 that with suitably large probability, every true spike (wt, b † jp\nt) maps to a distinct nearby inferred spike on every bj, j ∈ [k′], and similarly every true spike (wt, z † jp\nt) maps to a distinct nearby inferred spike on every zj , j ∈ [k′ − 1]. Lemma 4.8 shows that one can then match up the spikes on the different bjs. This yields k points in Span(Ã) that are close to the projection of P on Span(Ã). Finally, we argue that this can be mapped to a k-mixture source (w̃, P̃ ) that is close to (w,P ).\nLemma 4.2. With probability at least 1 − 1nω , the matrix Ã computed after step A1 satisfies rank(Ã) = k′ = rank(A) and ‖A− Ã‖op ≤ δn .\nProof. Recall that k′ = rank(A). Let B = M̃ − R̃ = ∑ni=1 λiviv † i , where λ1 ≥ . . . ≥ λn. We prove below that with probability at least 1− 1nω , we have ‖M − M̃‖op ≤ δ4n and ‖R− R̃‖op ≤ δ4n . This implies that ‖A−B‖op ≤ ‖M−M̃‖op+‖R−R̃‖op ≤ δ2n . Hence, by Lemma 2.2, it follows that by the ζ-wide assumption, λk′ ≥ ζ 2 n − δ2n ≥ 3ζ2 4n , and |λi| ≤ δ2n ≤ ζ2 4n for all i > k ′. Thus, we include exactly k′ eigenvectors when defining Ã, so rank(Ã) = k′. Since Ã is the closest rank-k′ approximation in operator norm to B, we have ‖A− Ã‖op ≤ ‖A−B‖op + ‖B − Ã‖op ≤ 2‖A−B‖op ≤ δn .\nIt is easy to see that |R̃i,j − Ri,j | ≤ 3σri,j, where σ = δ/48, and so ‖R − R̃‖op ≤ ‖R − R̃‖F ≤ δ 4n . Bounding ‖M − M̃‖op is more challenging. We carefully define a matrix whose entries are independent random variables with bounded variance, and then apply Theorem 2.4.\nNote that Mi,j ≤ min { 2 n , 4 wminn2 } due to isotropy. Let K = 4 ln(1/δ)δ and K ′ = 5 ln(1/δ)δ . Let\nD = N2 · ( M̃ −M ) . Let Xℓi,i = 1 if the ℓ-th snapshot is (i, i), for i ∈ [n], and for i, j ∈ [n], i 6= j, let Xℓi,j = X ℓ j,i = 1 2 if the ℓ-th 2-snapshot is (i, j) or (j, i), and 0 otherwise. Let Y ℓ i,j = X ℓ i,j −Mi,j = Xℓi,j − E[Xℓi,j ]; so Di,j = ∑N2 ℓ=1 Y ℓ i,j for all i, j ∈ [n]. We have σ2(n2) := Var[Di,j |N2 = n2] = n2Var[X 1 i,j ] ≤ n2 E[(X1i,j)2] ≤ n2Mi,j. For n2 ≤ 2cn ln6 n, we have σ2(n2) ≤ 8c ln 6 n w2\nmin n\n≤ lnn ln(1/δ) δ2\n(since w4min ≥ 57600κ 2 ln5 n n ). So by Bernstein’s inequality (Lemma A.2),\nPr[|Di,j | > K lnn|N2 = n2] ≤ 2 exp ( − K 2 ln2 n\n2 ( σ2(n2) +K lnn/3 )\n)\n≤ 2max { exp (\n−K2 ln2 n 4σ2(n2) ) , exp ( −3K lnn4 )\n} ≤ 2δ n3 .\nSince Pr[N2 > 2c ln 6 n] ≤ n−3, we can say that with probability at least 1 − 2n−2, we have |Di,j | ≤ K lnn for every i, j ∈ [n] and N2 ≤ 2c ln6 n. Define a matrix D′ by putting, for every i, j ∈ [n], D′i,j = sign(Di,j) ·min { |Di,j |,K lnn } . Put D′′ = D′−E[D′]. Clearly, E[D′′i,j ] = 0 for every i, j ∈ [n]. We prove below that ∣ ∣E[D′i,j ] ∣ ∣ ≤ 3δc ln6 n n2\n≤ lnn ln(1/δ)\nδ ; therefore, |D′′i,j | ≤ K ′ lnn. The entries of D are independent random variables as N2 is a Poisson random variable; hence, the entries of D′′ are also independent random variables. Also Var[D′′i,j] ≤ Var[Di,j ] since censoring a random variable to an interval can only reduce the variance. Note that Di,j = ∑N2 ℓ=1 Y ℓ i,j follows the compound Poisson distribution. So we have\nVar[Di,j ] = E[N2] · E[(Y 1i,j)2] = E[N2] ·Var[X1i,j] ≤ E[N2]Mi,j ≤ 4c ln6 n w2min · n ≤ ĉ 2K ′2 ln6 n n\nwhere ĉ = max { 2\n√ c\nwminK ′ , κ2\n}\n. Thus, by Theorem 2.4, the constant κ = κ(2 + lnω) > 0 is such that\nwith probability at least 1− 1 n2ω\n‖D′′‖op ≤ 2 · ĉK ′ ln3 n√ n · √n+ κ\n√\nK ′ lnn · ĉK ′ ln3 n√ n · 4√n · lnn ≤ ( 2K ′ĉ+ κK ′ √ ĉ ) ln3 n. (3)\nWe have Pr [ N2 ≥ 12 E[N2] ] ≥ 1− n−2, Thus, with probability at least 1 − 1nω , we have that N2 ≥ 1 2 E[N2], D\n′ = D, and ‖D′′‖op is bounded by (3). We show below that 2‖E[D′]‖op/E[N2] ≤ 6δn−2 ≤ δ/20n. One can verify that 4K ′ĉ/c ≤ δ/10 and 2κK ′ √ ĉ/c ≤ δ/10. Therefore, with probability at least 1− 1nω , we have that ‖M − M̃‖op = 1N2 · ‖D‖op ≤ 2 E[N2] · (‖D′′‖op + ‖E[D′]‖op) ≤ δ4n .\nFinally, we bound ‖E[D′]‖op. We have ‖E[D′]‖op ≤ ‖E[D′]‖F ≤ n · maxi,j ∣ ∣E[D′i,j ] ∣ ∣. Let\nµ = cn ln6 n = E[N2]. Fix any i, j. We have ∣ ∣E[D′i,j ] ∣ ∣ = ∣ ∣E[D′i,j − Di,j ] ∣ ∣ ≤ E [ |D′i,j − Di,j| ] . For\nany n2 ≤ 2 ln(1/δ)µ, we have Var[Di,j |N2 = n2] ≤ n2Mi,j ≤ 8c ln(1/δ) ln 6 n\nw2 min\nn . So by Bernstein’s\ninequality, we have that Pr[|Di,j | > K lnn|N2 ≤ 2 ln(1/δ)µ] < 2δn−3. Also, |D′i,j − Di,j | ≤ N2 always. Therefore,\nE [ |D′i,j −Di,j| ∣ ∣N2 = n2 ]\n≤ { 2δn−3n2 if n2 ≤ 2 ln ( 1 δ ) µ;\nn2 otherwise\nand E [ |D′i,j − Di,j | ] ≤ µ − Pr[N2 ≤ 2 ln(1/δ)µ] E[N2|N2 ≤ 2 ln(1/δ)µ](1 − 2δn−3). Since N2 is Poisson distributed, we have\nPr[N2 ≤ 2 ln(1/δ)µ] E[N2|N2 ≤ 2 ln(1/δ)µ] = ⌊2 ln(1/δ)µ⌋ ∑\nℓ=0\nℓ · µ ℓe−µ\nℓ! = µ\n⌊2 ln(1/δ)µ⌋−1 ∑\nℓ=0\nµℓe−µ\nℓ!\n≥ µPr[N2 ≤ ln(1/δ)µ] ≥ µ(1− δn−3).\nThus, E [ |D′i,j−Di,j| ] ≤ µ−µ(1−δn−3)(1−2δn−3) ≤ 3δn−3µ, and 2‖E[D′]‖op/E[N2] ≤ 6δn−2.\nWe assume in the sequel that the high-probability event stated in Lemma 4.2 happens. Thus,\nLemma 2.3 implies that ‖ΠA −ΠÃ‖op ≤ 2 √ δ ζ .\nLemma 4.3. For every unit vector b ∈ Span(A), ‖b‖∞ ≤ 2w2 min ζ √ n .\nProof. Recall that A = ∑k t=1 wt(p t − r)(pt − r)†, and the smallest non-zero eigenvalue of A is at least ζ2/n. Note that Span(A) = Span{p1 − r, . . . , pk − r}. Let Z = conv(P ). If r + b ∈ Z, then ‖r + b‖∞ ≤ 2wminn , r + b ≥ 0, and ‖r‖∞ ≤ 2 n imply that ‖b‖∞ ≤ 2wminn . Otherwise, let the line segment [r, r + b] intersect the boundary of Z at some point b′. We show that ‖r − b′‖22 ≥ ζ2w2 min n . The lemma then follows since b = (b′ − r)/‖b′ − r‖2 and so ‖b‖∞ = ‖b ′−r‖∞\n‖b′−r‖2 ≤ 2\nw2 min\nζ √ n .\nLet S be a facet of Z such that b′ ∈ S, r /∈ S (note that r is in the strict interior of P ). Since Z ⊆ Span(A), one can find a unit vector v ∈ Span(A) such that S is exactly the set of points that minimize v†x over x ∈ Z. Let dL = v†r −minx∈Z v†x = v†(r − b′). We lower bound ‖r − b′‖2 by dL. Note that dL > 0. Clearly, v†(pt − r) ≥ −dL for all t ∈ [k]. Projecting P onto v, we have that (a)\n∑k t=1 wtv †(pt − r) = 0; and (b) vTAv = ∑kt=1 wt ( v†(pt − r) )2 ≥ ζ2n since\nv ∈ Span(A) and (w,P ) is ζ-wide. Let WL = ∑ t:v†(pt−r)≤0 wt, let WR = 1 −WL ≥ wmin, and let dR = maxt{v†(pt − r)}. Then, 0 = ∑k t=1wtv\n†(pt − r) ≥ WL(−dL) + wmindR, so dR ≤ dL · WLwmin , Also ζ 2\nn ≤ ∑k t=1 wt ( v†(pt − r) )2 ≤ WL · d2L + WR · d2R ≤ WL · d2L + WR · d2L · W 2L w2\nmin\n≤ d 2 L\nw2 min\n. So,\nd2L ≥ ζ2w2 min n .\nLemma 4.4. If the assumptions stated in Algorithm Learn are satisfied, then: (i) the vector a computed in Learn satisfies ‖a‖∞ ≤ H, and |a†(p−q)| ≥ L/2 for every two mixture constituents p, q ∈ P ; (ii) with probability at least 1− ε, the output (w̄, γ) of Learn satisfies the following: there is a permutation σ : [k] 7→ [k] such that for all t = 1, . . . , k,\n|wt − w̄σ(t)| = O (ςω1.5k5\nw2minζ 2\n) , |v†pt − γσ(t)| ≤ 2048kHς\nwmin +\n8 √ 2δ\nwminζ √ n ≤ 2048kHς wmin + L 8T .\nProof. We have v†ΠA(v) = 1 − ‖v − ΠA(v)‖22 = 1 − ‖(ΠÃ − ΠA)v‖22 ≥ 1 − 4δζ2 . Thus, ΠA(v) is feasible to (Qv), and since ‖ΠA(v)‖2 ≤ 1, by Lemma 4.3, the optimal solution x∗ to (Qv) satisfies\n‖x∗‖∞ ≤ 2‖ΠA(v)‖∞ ≤ H/2. Also ‖x∗‖22 ≥ v†x∗ ≥ 1 − 4δζ2 ≥ 14 , so ‖a‖∞ ≤ H. Note that ‖v − a‖22 = 2(1− v†a) ≤ 2(1− v†x∗) ≤ 8δζ2 . It follows that for any two mixture constituents p, q, we have\n|a†(p − q)| ≥ |v†(p − q)| − |(v − a)†(p− q)| ≥ |v†(p− q)| − 2 √ 2δ\nζ ‖p − q‖2\n≥ |v†(p− q)| − 8 √ 2δ\nwminζ √ n ≥ |v†(p− q)| − L 2 ≥ L 2 .\nThis proves part (i). For part (ii), we note that any two spikes in the k-spike mixture (\nw,E[πa/2H (P )] ) are separated by a distance of at least L/4H. Since s < L/4H, Theorem 5.1 guarantees that with a sample of (2k − 1)-snapshots of size 3k24ks−4k log(4k/ε), with probability at least 1 − ε, the learned k-spike distribution (w̄, β) satisfies Tran ( w,E[πa/2H (P )]; w̄, β )\n≤ 1024ks1/(4k) = 1024kς < Lwmin8H . Notice that this implies that there is a permutation σ : [k] 7→ [k] such that ∀t = 1, . . . , k:\n|(a/2H)†pt − βσ(t)| ≤ 1024kς\nwmin <\nL\n8H , |wt − w̄σ(t)| = O\n( kς\nL/8H\n) = O (ςω1.5k5\nw2minζ 2\n)\n. (4)\nFix some t ∈ [k]. Let t′ = σ(t). From (4), we know that |a†pt − 2H · βt′ | = 2048kHςwmin . We bound |v†pt−a†pt| and |2Hβt′ − γt′ |, which together with the above will complete the proof of the lemma. We have |(v − a)†pt| ≤ ‖v − a‖2‖pt‖2 ≤ 4 √ 2δ\nwminζ √ n . Since γt′ = (2Hβt′)a †v and |βt′ | ≤ 12 , we have |2Hβt′ −γt′ | ≤ H·4δζ2 ≤ 16δw2\nmin ζ3\n√ n . It follows that |v†pt−γt′ | ≤ 2048kHςwmin +\n8 √ 2δ\nwminζ √ n ≤ 2048kHςwmin + L 8T .\nClaim 4.5. Let Z be a random unit vector in Span(Ã) and v ∈ Span(Ã). Pr [ |Z†v| < ‖v‖232ω1.5k4 ]\n< 1\n3ωk′k2 .\nProof. One way of choosing the random unit vector Z is as follows. Fix an orthonormal basis {u1, . . . , uk′} for Span(Ã). We choose independent N(0, 1) random variables Xi for i ∈ [k′]. Define C = ∑k′\ni=1Xiui and set Z = C/‖C‖2. Set a1 = π32ω2k′2k4 and a2 = 2 + 4 ln(12ωk′k2) k′ ≤ 96ωk. Note that C†v/‖v‖2 is distributed as N(0, 1). Therefore, Pr [ |C†v| ≤ ‖v‖2 √ a1 ] ≤ √ 2a1 π ≤\n1 4ωk′k2 . Also, ‖C‖22 = ∑k′ i=1 X 2 i follows the χ 2 k′ distribution. So\nPr[‖C‖22 > a2k′] < ( a2e 1−a2 )k′/2 < exp ( (1− a2/2)k′/2 ) < 1\n12ωk′k2 .\nObserve that √\na1 a2k′ ≥ 1 32ω1.5k4 . So if the “bad” event stated in the lemma happens, then |C†v| ≤ ‖v‖2 √ a1 or ‖C‖22 ≥ a2k′ happens; the probability of this is at most 13ωk′k2 .\nLemma 4.6. With probability at least 1− 13ω , for every pair p, q ∈ P , we have (i) |b † j(p − q)| ≥ L for every j ∈ [k′] and (ii) |z†j (p− q)| ≥ L for every j ∈ [k′ − 1].\nProof. Define p̃ = ΠÃ(p) for a mixture constituent p. Clearly, for any v ∈ Span(Ã), v†p̃ = v†p. Recall that ‖ΠA −ΠÃ| ≤ 2 √ δ\nζ . So for every p, q ∈ P , ‖p̃− q̃‖22 ≥ ‖p− q‖22 −‖(ΠA −ΠÃ)(p− q)‖22 ≥ ‖p − q‖2/4; hence, ‖p̃ − q̃‖2 ≥ ζ2√n . Notice that the zj vectors are also random unit vectors in Span(Ã). Applying Claim 4.5 to each event involving one of the {bj}j∈[k′], {zj}j∈[k′−1] random unit vectors, and one of the\n(k 2 ) vectors ‖p̃ − q̃‖ for p̃, q̃ ∈ ΠÃ(P ), and taking the union bound over the at most k′k2 such events completes the proof.\nLemma 4.7. With probability at least 1 − 23ω , the k-spike distributions obtained in steps A2 and A3 satisfy: (i) For every j ∈ [k′], there is a permutation σj : [k] 7→ [k] such that for all t ∈ [k],\n|wt − w̃jσj(t)| = O (δω1.5k5\nw2minζ 2\n)\n, |b†jpt − α j σj(t) | = O (\n√ δ\nw1.5minζ √ n\n) and is at most L\n2 + 5T .\nHence, |αjt1 − α j t2 | ≥ L− 2L2+5T = L1+0.4/T for all distinct t1, t2 ∈ [k]. (ii) For every j ∈ [k′ − 1], for every t ∈ [k], there is a distinct t′ such that\n|wt − ŵjt′ | = O (δω1.5k5\nw2minζ 2\n)\n, |z†jpt − α̂ j t′ | = O\n(\n√ δ\nw1.5minζ √ n\n) and is at most L\n2 + 5T .\nProof. Assume that the event stated in Lemma 4.6 happens. Then the inputs to Learn in steps A2 and A3 are “valid”, i.e., satisfy the assumptions stated in Algorithm Learn. Plug in ς = δ and ε = 16ωk in Lemma 4.4. Taking the union bound over all the bjs and the zjs, we obtain that the probability that Learn fails on some input, when all the bjs and zjs are valid is at most 1 3ω . The lemma follows from Lemma 4.4 by noting that 2048kHδwmin = O ( √ δ w1.5minζ √ n ) and is at most L24T , and L/24T + L/8T ≤ L/(2 + 5T ).\nLemma 4.8. With probability at least 1− 1ω , for every j = 1, . . . , k′−1 ̺j is a well-defined function and ̺j(σk ′ (t)) = σj(t) for every t ∈ [k].\nProof. Lemma 4.8 Assume that the events in Lemmas 4.6 and 4.7 occur. Fix j ∈ [k′ − 1]. We call a point αjt1bj + α k′ t2bk′ a grid-j point. Call this grid point “genuine” if there exists t ∈ [k] such that σj(t) = t1 and σ k′(t) = t2, and “fake” otherwise. The distance between any two grid-j points is at least L/(1 + 0.4/T ) (by Lemma 4.7). So the probability there is a pair of genuine and fake grid-j points whose projections on zj are less than L/(T + 0.4) away is at most k 3 · 2π arcsin ( 1 T )\n≤ k3 · 2π · 65T ≤ 13ωk . Therefore, with probability at least 1−ω, the events in Lemma 4.6 and Lemma 4.7 happen, and for all j ∈ [k′ − 1], every pair of genuine and fake grid-j points project to points on zj that are at least L/(T + 0.4) apart. We condition on this in the sequel.\nNow fix j ∈ [k′ − 1] and consider any pair t1, t2 ∈ [k]2. Let g be the grid-j point bjαjt1 + bk′αk ′\nt2 We show that ̺j(t2) = t1 iff g is a genuine grid-j point. If g is genuine, let t be such that σj(t) = t1, σ k′(t) = t2. Let p ′ be the projection of pt on Span(bj , bk′). By Lemma 4.7, we have that ‖p′ − g‖2 ≤ √ 2L 2+5T . Also, there exists t ′ ∈ [k] such that |α̂jt′ − z † jp t| ≤ L2+5T . Since z † jp ′ = z†jp t, this implies that |z†jg − α̂ j t′ | ≤ |α̂ j t′ − z † jp t|+ |z†j (p′ − g)| ≤ ( √ 2+1)L 2+5T and so ̺ j(t2) = t1.\nNow suppose g is fake but |z†jg − α̂ j t′ | ≤ (\n√ 2 + 1)L/(2 + 5T ) for some t′ ∈ [k]. Let t ∈ [k] be\nsuch that |α̂jt′ − z † jp t| ≤ L2+5T . Let g′ be the genuine grid point bjα j σj(t) + bk′α k′ σk′ (t) . So |z†jg′− α̂ j t′ | ≤ ( √ 2 + 1)L/(2 + 5T ), and hence |z†j (g − g′)| ≤ 2( √ 2+1)L 2+5T < L 0.4+T which is a contradiction.\nProof of Theorem 4.1. We condition on the fact that all the “good” events stated in Lemmas 3.2, 4.2, 4.6, 4.7, and 4.8 happen. The probability of success is thus 1−O (\n1 ω\n)\n. The sample-size bounds\nfollow from the description of the algorithm. For notational simplicity, let σk ′ be the identity permutation, i.e., σk ′ (t) = t for all t ∈ [k]. So by Lemma 4.8, we have ̺j(t) = σj(t) for every j ∈ [k′ − 1] and t ∈ k.\nFor t = 1, 2, . . . , k, define p̄t = r̃ + ∑k′ j=1 b † j(p t − r̃)bj = r̃ +ΠÃ(pt − r̃). Fix t ∈ [k]. Then\n‖pt − p̃t‖1 ≤ ‖pt − p̂t‖1 + ‖p̂t − p̃t‖1 ≤ 2‖pt − p̂t‖1 ≤ 2 ( ‖pt − p̄t‖1 + ‖p̄t − p̂t‖1 ) .\nWe have ‖pt − p̄t‖2 = ∥ ∥ ∥r − r̃ + (pt − r)− k′ ∑\nj=1\nb†j(p t − r̃)bj\n∥ ∥ ∥\n2\n= ∥ ∥r − r̃ +ΠÃ(r̃ − r) + (pt − r)−ΠÃ(pt − r) ∥ ∥ 2\n≤ 2 · ‖r − r̃‖2 + ·‖ΠA −ΠÃ‖op · ‖pt − r‖2 ≤ δ\n12 √ n +\n8 √ 2δ\nwminζ √ n .\nAlso ‖p̄t − p̂t‖2 ≤ ∥ ∥ ∥\nk′ ∑\nj=1\n( b†jp t − αj σj(t) ) bj\n∥ ∥ ∥\n2 = O\n( √ kδ\nw1.5minζ √ n\n)\nwhere the last equality follows from Lemma 4.7. Thus, ‖pt − p̃t‖1 = O (\n√ kδ w1.5 min ζ ) . Also, we have\n|wt − w̃t| = O ( δω1.5k5\nw2 min ζ2\n)\nby Lemma 4.7. Finally, note that\nTran(w,P ; w̃, P̃ ) ≤ 1 2 (\nk ∑\nt=1\nmin{wt, w̃t}max t ‖pt − p̃t‖1 + ‖w − w̃‖1 max t6=t′\n‖pt − p̃t′‖1 )\n≤ 1 2 (\nk ∑\nt=1\nmin{wt, w̃t}max t ‖pt − p̃t‖1 + ‖w − w̃‖1 ( max t6=t′ ‖pt − pt′‖1 +max t ‖pt − p̃t‖1 )\n)\n≤ max t\n‖pt − p̃t‖1 + ‖w − w̃‖1 · 2\nwmin = O\n( √ kδ\nw1.5minζ\n)\n.\nThe running time is dominated by the time required to compute the spectral decomposition in step A1.3, the calls to Learn in steps A2.2 and A3.2, and the time to compute p̃t in step A3.4. The other steps are clearly polytime. As noted in Remark 3.4, it suffices to compute a decomposition such that ‖M̃ − R̃−∑ni=1 λiviv † i ‖ = O ( δ n ) ; this takes time poly ( n, ln(n/δ) )\n. The LP used in step A3.4 is of polynomial size, and hence can be solved in polytime. Procedure Learn requires solving (Qv); again, an approximate solution suffices and can be computed in polytime. Theorem 5.1 proves that the one-dimensional learning problem can be solved in polytime; hence, Learn takes polytime."
    }, {
      "heading" : "5 The one-dimensional problem: learning mixture sources on [0,1]",
      "text" : "In this section, we supply the key subroutine called upon in step L2 of Algorithm Learn, which will complete the description of Algorithm 1. We are given a k-mixture source ( w, πx(P ) ) on [ −12 , 12 ]\n. (Recall that Learn invokes the procedure for the mixture ( w, πa/2H (P ) )\nwhere ‖a‖∞ ≤ H.) It is clear that we cannot in general reconstruct this mixture source with an aperture size that is independent of n, let alone aperture 2k−1. However, our goal is somewhat different and more modest. We seek to reconstruct the k-spike distribution ( w,E[πx(P )] )\n, and we show that this can be achieved with aperture 2k− 1 (which is the smallest aperture at which this is information-theoretically possible).\nIt is easy to obtain a (2k − 1)-snapshot from (w, πx(P )) given a (2k − 1)-snapshot from (w,P ) by simply replacing each item i ∈ [n] that appears in the snapshot by xi. We will assume in the sequel that every constituent πx(p\nt) is supported on [0, 1], which is simply a translation by 12 . To simplify notation, we use θ = ( ϑ, (q1, . . . , qk) )\nto denote the k-mixture source on [0, 1], and (\nϑ, α = (α1, . . . , αk) ) to denote the corresponding k-spike distribution, where αi ∈ [0, 1] is the\nexpectation of qi for all i ∈ [k]. We equivalently view (ϑ, α) as a k-mixture source ( ϑ, (f1, . . . , fk) ) on {0, 1}: each f i is a “coin” whose bias is f i1 = αi. In Section 5.1, we describe how to learn such a binary mixture source from its (2k − 1)-snapshots (see Algorithm 2 and Theorem 5.3). Thus, if we can obtain (2k − 1)-snapshots from the binary source ( ϑ, (f1, . . . , fk) )\n(although our input is θ) then Theorem 5.3 would yield the the desired result. We show that this is indeed possible, and hence, obtain the following result (whose proof appears at the end of Section 5.1).\nTheorem 5.1. Let θ = ( ϑ, (q1, . . . , qk) )\nbe a k-mixture source on [0, 1], and (ϑ, α) be the corresponding k-spike distribution. Let τ = minj 6=j′ |αj − αj′ |. For any s < τ and ψ > 0, using 3k24ks−4k ln(4k/ψ) (2k − 1)-snapshots from source θ, one can compute in polytime a k-spike distribution (ϑ̃, α̃) on [0, 1] such that Tran(ϑ, α; ϑ̃, α̃) ≤ 1024ks1/(4k) with probability at least 1− ψ.\n5.1 Learning a binary k-mixture source\nRecall that ( ϑ, (f1, . . . , fk) ) denotes the binary k-mixture source, and αi = f i 1 is the bias of the i-th “coin”. We can collect from each (2k−1)-snapshot a random variable 0 ≤ X ≤ 2k−1 denoting the number of times the outcome “1” occurs in the snapshot. Thus, Pr[X = i] =\n(2k−1 i ) ∑k j=1 ϑjα i j(1−\nαj) 2k−1−i. Our objective is to use these statistics to reconstruct, in transportation distance (see Section 2.2), the binary source (i.e., the mixture weights and the k biases). Now consider the equivalent k-spike distribution (ϑ, α). The i-th moment, and (what we call) the i-th normalized binomial moment (NBM) of this distribution are\ngi(ϑ, α) = k ∑\nj=1\nϑjα i j , νi(ϑ, α) =\nk ∑\nj=1\nϑjα i j(1− αj)2k−1−i (5)\nUp to the factors (2k−1\ni\n)\nthe NBMs are precisely the statistics of the random variable X and so our objective in this section can be restated as: use the empirical NBMs to reconstruct the k-spike distribution (ϑ, α).\nLet g(ϑ, α) = ( gi(ϑ, α) )2k−1 i=0 and ν(ϑ, α) = ( νi(ϑ, α) )2k−1 i=0\ndenote the row-vectors of the first 2k−1 moments and NBMs respectively of (ϑ, α). For an integer b > 0 and a vector β = (β1, . . . , βℓ), let Ab(β) be the ℓ × b matrix (Ab(β))ij = (1 − βi)b−1−jβji (with 1 ≤ i ≤ ℓ and 0 ≤ j ≤ b − 1). Analogously, let Vb(β) be the ℓ × b “Vandermonde” matrix (Vb(β))ij = βji (with 1 ≤ i ≤ ℓ and 0 ≤ j ≤ b − 1). Let Pas be the 2k × 2k lower-triangular “Pascal triangle” matrix with non-zero entries Pasij = ( 2k−1−j i−j )\nfor 0 ≤ j ≤ 2k − 1 and j ≤ i ≤ 2k − 1. Then V2k(α) = A2k(α)Pas, ν(ϑ, α) = ϑA2k(α), and g(ϑ, α) = ϑV2k(α) = ν(ϑ, α)Pas.\nIn our algorithm it is convenient to use the empirical ordinary moments, but what we obtain are actually the empirical NBMs, so we need the following lemma. Lemma 5.2. ‖Pas‖op ≤ 4k/ √ 3.\nProof. The non-zero entries in column j of Pas are (m ℓ ) for ℓ = 0, . . . ,m = 2k − 1 − j. Therefore, ‖Pas‖op ≤ ‖Pas‖F = √ ∑2k−1 m=0 ∑m ℓ=0 (m ℓ )2 = √ ∑2k−1 m=0 (2m m ) ≤ √ ∑2k−1 m=0 2 2m.\nOur algorithm uses two input parameters τ and ξ as input, and the empirical NBM vector ν̃, which we convert to an empirical moment vector g̃ by multiplying by Pas. Since we infer (in the sampling limit) the locations of the k spikes exactly, there is a singularity in the process when spikes coincide. So we assume a minimum separation between spikes: τ = minj 6=j′ |αj − αj′ |. (It is of course possible to simply run a doubling search for sufficiently small τ , but the required accuracy in the moments, and hence sample size, does increase as τ decreases.) We also assume a bound ξ\non the accuracy of our empirical statistics. (When we utilize Theorem 5.3 to obtain Theorem 5.1, ξ is a consequence, and not an input parameter). We require that\n‖ν̃ − ν(ϑ, α)‖2 ≤ ξ4−k √ 3, ξ ≤ τ2k (6)\nTheorem 5.3. There is a polytime algorithm that receives as input τ, ξ, an empirical NBM vector ν̃ ∈ R2k satisfying (6), and outputs a k-spike distribution (ϑ̃, α̃) on [0, 1] such that Tran(ϑ, α; ϑ̃, α̃) ≤ O(ξΩ(1/k 2)).\nWe first show the information-theoretic feasibility of Theorem 5.3: the transportation distance between two probability measures on [0, 1] is upper bounded by (a moderately-growing function of) the Euclidean distance between their moment maps. (To use Lemma 5.4 to prove Theorem 5.3, we have to also show how to compute ϑ̃ and α̃ from g̃ such that ‖g̃ − g(ϑ̃, α̃)‖2, and hence, ‖g(ϑ, α)− g(ϑ̃, α̃)‖2 is small.)\nLemma 5.4. For any two (at most) k-spike distributions (ϑ, α) (ϑ̃, α̃) on [0, 1],\n‖g(ϑ, α) − g(ϑ̃, α̃)‖2 ≥ 1 (2k − 1)4k28k−5 · ( Tran(ϑ, α; ϑ̃, α̃) )4k−2 .\nLemma 5.4 can be geometrically interpreted as follows. The point g(ϑ, α) is in the convex hull of the moment curve and is therefore, by Caratheodory’s theorem, expressible as a convex combination of 2k points on the curve. However, this point is special in that it belongs to the collection of points expressible as a convex combination of merely k points of the curve. Lemma 5.4 shows that g(ϑ, α) is in fact uniquely expressible in this way, and that moreover this combination is stable: any nearby point in this collection can only be expressed as a very similar convex combination. We utilize the following lemma, which can be understood as a global curvature property of the moment curve; we defer its proof to Section 5.2. We prove a partial converse of Lemma 5.4 in Section 6, and hence obtain a sample-size lower bound that is exponential in k. The moment curve plays a central role in convex and polyhedral geometry [8], but as far as we know Lemmas 5.4 and 5.5 are new, and may be of independent interest.\nLemma 5.5. Let 0 ≤ β1 < . . . < βκ+1 ≤ 1, ℓ ∈ [κ], and s = βℓ+1 − βℓ. Let γ(x) = ∑κ i=0 γix i be a real polynomial of degree κ evaluating to 1 at the points β1, . . . , βℓ and evaluating to 0 at the points βℓ+1, . . . , βκ+1. Then ∑κ i=0 γ 2 i ≤ κ224κ−1s−2κ.\nProof of Lemma 5.4. Denote {α1, . . . , αk} ∪ {α̃1, . . . , α̃k} by α = {α1, . . . , αK} where α1 < . . . < αK . Define ϑi = ∑\nj:αj=αi ϑj−\n∑\nj:α̃j=αi ϑ̃j for i ∈ [K]. Let ϑ ∈ RK be the row vector (ϑ1, . . . , ϑK).\nLet η = Tran(ϑ, α; ϑ̃, α̃). So we need to show that ‖ϑV2k(α)‖2 ≥ 1(2k−1)4k28k−5 · η4k−2. It suffices to show that ‖ϑVK(α)‖2 ≥ 1(K−1)2K24K−5 · η2K−2. There is an 1 ≤ ℓ < K such that ∣ ∣ ∑ℓ i=1 ϑi ∣\n∣ · (αℓ+1 − αℓ) ≥ η/(K − 1). Let δ = ∑ℓ i=1 ϑi; without loss of generality δ ≥ 0, and note that δ ≤ 1. Let s = αℓ+1 − αℓ, so (K − 1)δs ≥ η. Denote row i of a matrix Z by Zi∗ and column j by Z∗j . We lower bound ‖ϑVK(α)‖2, by considering its minimum value under the constraints ∑ℓ i=1 ϑi = δ and ∑K i=1 ϑi = 0. A vector y† = ϑVK(α) minimizing ‖y‖2 must be orthogonal to VK(α)i∗ − VK(α)i′∗ if 1 ≤ i < i′ ≤ ℓ or if ℓ + 1 ≤ i < i′ ≤ K. This means that there are scalars c and d such that VK(α)y = c( ∑ℓ\nj=1 ej) + d( ∑K j=ℓ+1 ej), where vector ej ∈ RK has a 1 in the j-th position and 0 everywhere else. Therefore, y = cγ + dγ′, where γ =\n∑ℓ j=1(VK(α) −1)∗j and γ′ = ∑K j=ℓ+1(VK(α) −1)∗j . At the\nsame time\nδ =\nℓ ∑\ni=1\nϑi = ϑVK(α)γ = y †γ = c‖γ‖22+dγ′†γ −δ =\nK ∑\ni=ℓ+1\nϑi = ϑVK(α)γ ′ = y†γ′ = cγ†γ′+d‖γ′‖22\nand hence, ‖y‖22 = y · (cγ + dγ′) = (c− d)δ. Solving for c, d gives c− d = δ‖γ+γ′‖22\n‖γ‖2 2 ·‖γ′‖2 2 −(γ†·γ′)2 .\nFirst we examine the numerator of c − d. Like any combination of the columns of VK(α)−1, γ + γ′ is the list of coefficients of a polynomial of degree K − 1, in the basis 1, x, . . . , xK−1. By definition, γ + γ′ = ∑\nj(VK(α) −1)∗j , which is to say that for every i, VK(α)i∗ · (γ + γ′) = 1. So the\npolynomial γ + γ′ evaluates to 1 at every αi. It can therefore only be the constant polynomial 1; this means that (γ + γ′)i = 1 if i = 0, and (γ + γ′)i = 0 otherwise. Thus ‖γ + γ′‖22 = 1.\nNext we examine the denominator, which we upper bound by ‖γ‖22 · ‖γ′‖22. When interpreted as a polynomial, γ takes the value 1 on a nonempty set of points α1, . . . , αℓ separated by the positive distance s = αℓ+1 − αℓ from another nonempty set of points αℓ+1, . . . , αK upon which it takes the value 0. Observe that if the polynomial was required to change value by a large amount within a short interval, it would have to have large coefficients. A converse to this is the inequality stated in Lemma 5.5. Using this to bound ‖γ‖22 and ‖γ′‖22, and since δs ≥ η/(K − 1), we obtain that\n‖y‖22 = (c− d)δ ≥ δ2 ‖γ‖22 · ‖γ′‖22 ≥ δ 2 ((K − 1)224K−5s−2K+2)2 ≥ η4K−4 (K − 1)4K28K−10 .\nWe now define the algorithm promised by Theorem 5.3. To give some intuition, suppose first that we are given the true moment vector g(ϑ, α) = ϑV2k(α). Observe that there is a common vector λ = (λ0, . . . , λk)\n† of length k + 1 that is a dependency among every k + 1 adjacent columns of V2k(α). In other words, letting Λ = Λ(λ) denote the 2k × k matrix with Λij = λi−j (for 0 ≤ i < 2k, 0 ≤ j < k and with the understanding λℓ = 0 for ℓ /∈ {0, . . . , k}), V2k(α)Λ = 0. Thus g(ϑ, α)Λ = ϑV2k(α)Λ = 0. Overtly this is a system of 2k equations to determine λ. But we eliminate the redundancy in Λ by forming the k × (k + 1) matrix G = G(g(ϑ, α)) defined by Gij = g(ϑ, α)i+j for i = 0, . . . , k− 1 and j = 0, . . . , k, and then solve the system of linear equations Gλ = 0 to obtain λ. This system does not have a unique solution, so in the sequel λ will denote a solution with λk = 1. For each i = 1, . . . , k, we have ( V2k(α)Λ )\ni,1 = ∑k ℓ=0 λℓαi ℓ = 0. This implies\nthat we can obtain the αi values by computing the roots of the polynomial Pλ(x) := ∑k ℓ=0 λℓx ℓ. Once we have the αi’s, we can compute ϑ by solving the linear system yV2k(α) = g(ϑ, α) for y. Of course, we are actually given g̃ rather than the true vector g(ϑ, α). So we need to control\nthe error in estimating first α and then ϑ. The learning algorithm is as follows.\nAlgorithm 2. Input: parameters ξ, τ and empirical moments g̃ such that ‖g̃ − g(ϑ, α)‖2 ≤ ξ. Output: a k-spike distribution (ϑ̃, α̃)\nB1. Solve the minimization problem:\nminimize ‖x‖1 s.t. ‖G(g̃)x‖1 ≤ 2kkξ, xk = 1 (P)\nwhich can be encoded as a linear program and hence solved in polytime, to obtain a solution λ̃. Observe that since G(g̃) has k + 1 columns and k rows, there is always a feasible solution.\nB2. Let ᾱ1, . . . , ᾱk be the (possibly complex) roots of the polynomial Pλ̃. Thus, we have V2k(ᾱ)Λ(λ̃) = 0. We map the roots to values in [0, 1] as follows. Let ǫ = 4τ (2kξ)\n1/k. First we compute α̂1, . . . , α̂k such that |α̂i − ᾱi| ≤ ǫ for all i, in time poly ( log(1ǫ ) )\n, using Pan’s algorithm [38, Theorem 1.1]2. We now set α̃i = max{0,min{Re(α̂i), 1}}.\n2The theorem requires that the complex roots lie within the unit circle and that the coefficient of the highest-degree term is 1; but the discussion following it in [38] shows that this is essentially without loss of generality.\nB3. Finally, we set ϑ̃ to be the row-vector y that minimizes ‖yV2k(α̃)− g̃‖22 subject to ‖y‖1 = 1, y ≥ 0. Note that this is a convex quadratic program that can be solved exactly in polytime [17].\nWe now analyze Algorithm 2 and justify Theorem 5.3. Recall that τ = minj 6=j′ |αj − αj′ |. We need the following lemma, whose proof appears in Section 5.2.\nLemma 5.6. The weights ϑ̃ satisfy ‖ϑ̃V2k(α̃)− g̃‖2 ≤ ‖g(ϑ, α) − g̃‖2 + (8k) 5/2 τ · (2kξ)1/k.\nProof of Theorem 5.3. We call Algorithm 2 with g̃ = ν̃Pas. By Lemma 5.2, we obtain that ‖g̃ − g(ϑ, α)‖2 ≤ ξ, and by Lemma 5.6, we have that ‖g(ϑ, α) − ϑ̃V2k(α̃)‖2 ≤ 2‖g(ϑ, α) − g̃‖2 + 8τ · (8k)3/2 ( 2kξ )1/k . Coupled with Lemma 5.4 and since ξ ≤ τ2k, we obtain that\nTran(ϑ, α; ϑ̃, α̃) ≤ [ (2k − 1)4k28k−5‖g(ϑ, α) − g(ϑ̃, α̃)‖2 ] 1 4k−2\n≤ [ (2k − 1)4k28k−5 ( 2ξ + (8k) 5 2\nτ\n( 2kξ )\n1 k\n)\n] 1\n4k−2\n≤ [ (2k − 1)4k28k−5 ( 2ξ + (8k)5/2 ( 2k √ ξ )1/2k\n)] 1\n4k−2 ≤ 1024 · kξ 1 8k2 .\nProof of Theorem 5.1. We convert θ to the corresponding binary source ( ϑ, (f1, . . . , fk) )\nby randomized rounding. Given a (2k − 1)-snapshot z = (z1, . . . , z2k−1) ∈ [0, 1]2k−1 from θ, we obtain a (2k − 1)-snapshot from the binary source as follows. We choose 2k − 1 independent values a1, . . . , a2k−1 uniformly at random from [0, 1] and set Xi = 1 if zi ≥ ai and 0 otherwise for all i ∈ [2k − 1]. Note that if qj is the constituent generating the (2k − 1)-snapshot z, then Pr[Xi = 1|qj ] = E[Xi|qj] = αj, and so X1, . . . ,X2k−1 is a random (2k−1)-snapshot from the above binary source.\nNow we apply Theorem 5.3, setting ξ = s2k. Let ν̃ be the empirical NBM-vector obtained from\nthe (2k − 1)-snapshots of the above binary source (i.e., ν̃i = (2k−1\ni )−1· (frequency with which the (2k − 1)-snapshot has exactly i 1s)). The stated sample size ensures, via a Chernoff bound, that Pr [ |ν̃i − ν(ϑ, α)i| ≥ ξ4 −k\n√ 6k\n]\n< ψ2k for all i = 0, . . . , 2k − 1. Hence, with probability at least 1− ψ, we have ‖ν̃ − ν(ϑ, α)‖2 ≤ √ 2k · ‖ν̃ − ν(ϑ, α)‖∞ ≤ ξ4−k/ √ 3."
    }, {
      "heading" : "5.2 Proofs of Lemma 5.5 and Lemma 5.6",
      "text" : "Proof of Lemma 5.5. There are two easy cases to dismiss before we reach the more subtle part of this lemma. The first easy case is ℓ = 1. In this case γ is a single Lagrange interpolant: γ(x) =\n∏κ+1 j=2 x−βj β1−βj . For 0 ≤ i ≤ κ let e κ i (β2, . . . , βκ+1) be the i’th elementary symmetric mean,\neκi (β2, . . . , βκ+1) = 1 (\nκ i\n)\n∑\nS⊆{2,...,κ+1}:|S|=i\n∏ j∈S βj\nand observe that for all i, 0 ≤ eκi (β2, . . . , βκ+1) ≤ 1. Now\nγ(x) = (\nκ+1 ∏\nj=2\n1\nβ1 − βj\n) κ ∑\ni=0\n(−1)κ−i ( κ\ni\n)\neκκ−i(β2, . . . , βκ+1)x i\nSo ∑κ i=0 γ 2 i =\n(\n∏κ+1 j=2 1 β1−βj\n)2 ∑κ\ni=0 ((κ i ) eκi (β2, . . . , βκ+1) )2 ≤ s−2κ∑κi=0 (κ i )2 = (2κ κ ) s−2κ.\nThe second easy case is ℓ = κ; this is almost as simple. Merely note that the above argument applies to the polynomial 1 − γ, so that we have only to allow for the possible increase of |γ0| by 1. Hence\n∑κ i=0 γ 2 i ≤ 4\n(\n2κ κ\n) s−2κ. We now consider the less trivial case of 1 < ℓ < κ. The difficulty here is that the La-\ngrange interpolants of γ may have very large coefficients, particularly if among β1, . . . , βℓ or among βℓ+1, . . . , βκ+1 there are closely spaced roots, as well there may be. We must show that these large coefficients cancel out in γ.\nThe trick is to examine not γ but ∂γ/∂x. The roots of the derivative interlace the two sets on which γ is constant, which is to say, with β′1 ≤ . . . ≤ β′κ−1 denoting the roots of ∂γ/∂x, that for j < ℓ, βj ≤ β′j ≤ βj+1, and for j ≥ ℓ, βj+1 ≤ β′j ≤ βj+2. In particular, none of the roots fall in the interval (βℓ, βℓ+1). For some constant C we can write ∂γ/∂x = C ∏κ−1 j=0 (x−β′j) (with sign(C) = (−1)1+κ−ℓ). Observe that ∫ βℓ+1 βℓ ∂γ ∂x(x) dx = −1. So (−1)1+κ−ℓ/C = ∫ βℓ+1 βℓ\n(−1)κ−ℓ∏κ−1j=0 (x − β′j) dx. Observe that if for any j < ℓ, β′j is increased, or if for any j ≥ ℓ, β′j is decreased, then the integral decreases. So (−1)1+κ−ℓ/C ≥\n∫ βℓ+1 βℓ (−1)κ−ℓ(x − βℓ)ℓ−1(x − βℓ+1)κ−ℓ dx. This is a definite integral that can be evaluated in closed form:\n∫ βℓ+1\nβℓ\n(−1)κ−ℓ(x− βℓ)ℓ−1(x− βℓ+1)κ−ℓ dx = (βℓ+1 − βℓ)κ(ℓ− 1)!(κ − ℓ)!/κ! .\nHence, (−1)1+κ−ℓC ≤ κ!sκ(ℓ−1)!(κ−ℓ)! . The sum of squares of coefficients of ∂γ ∂x is C2 ∑κ−1\ni=0\n( κ−1 i )2 (eκ−1i (β ′ 1, . . . , β ′ κ−1)) 2 ≤ C2 ( 2κ−2 κ−1 )\n. Integration only decreases the magnitude of the coefficients, so the same bound applies to γ, with the exception of the constant coefficient. The constant coefficient can be bounded by the fact that γ has a root in (0, 1), and that in that interval the derivative is bounded in magnitude by C\n∑κ−1 i=0 (κ−1 i ) = C · 2κ. So |γ0| ≤ C · 2κ. Consequently, ∑κ\ni=0 γ 2 i is at most\nC2 [( 2κ− 2 κ− 1 ) + 22κ ] ≤ (2κ−2 κ−1 ) + 22κ s2κ · (\nκ!\n(ℓ− 1)!(κ − ℓ)!\n)2 ≤ 5κ 222κ−2 s2κ · ( κ− 1 ℓ− 1 )2 ≤ 5κ 224κ−4 s2κ ,\nwhich completes the proof of the lemma.\nProof of Lemma 5.6. Recall that G = G(g(ϑ, α)) is the k×(k+1) matrix defined byGij = g(ϑ, α)i+j for i = 0, . . . , k − 1 and j = 0, . . . , k; λ is such that Gλ = 0 and λk = 1; Λ = Λ(λ) is the 2k × k matrix with Λij = λi−j (for 0 ≤ i < 2k, 0 ≤ j < k with the understanding λℓ = 0 for ℓ /∈ {0, . . . , k}; and Pλ(x) is the polynomial ∑k ℓ=0 λℓx\nℓ. We use Vk, V2k, to denote Vk(α), V2k(α) respectively, and Ṽk, Ṽ2k, G̃, Λ̃ to denote Vk(α̃), V2k(α̃), G(g̃),Λ(λ̃) respectively. We abbreviate g(ϑ, α) to g.\nLemma 5.7. If ‖g̃ − g‖2 ≤ ξ, then ‖Gλ̃‖1 ≤ 2k+1kξ.\nProof. First, observe that G̃λ = Gλ+(G̃−G)λ = (G̃−G)λ. Also ‖λ‖2 ≤ ‖λ‖1 = ∏k i=1(1+αi) ≤ 2k. The last two inequalities follows since Pλ(x) = ∏k i=1(x−αi), and Pλ(−1) = (−1)k‖λ‖1. So for any i = 1, . . . , k, ∣ ∣(G − G̃)i · λ ∣\n∣ ≤ ‖λ‖2‖Gi − G̃i‖2 ≤ 2kξ. Thus, λ is a feasible solution to (P), which implies that ‖λ̃‖1 ≤ 2k. We have ‖Gλ̃‖1 ≤ ‖G̃λ̃‖1 + ‖(G − G̃)λ̃‖1 ≤ 2kkξ + ‖(G − G̃)λ̃‖1. For any i = 1, . . . , k, ∣ ∣(G− G̃)i · λ̃ ∣ ∣ ≤ ‖Gi − G̃i‖2‖λ̃‖2 ≤ 2kξ, so ‖Gλ̃‖1 ≤ 2k+1kξ.\nLemma 5.8. For every αi, i = 1, . . . , k, there exists a σ(i) ∈ {1, . . . , k} such that ϑi|αi − α̃σ(i)| ≤ 8 τ (2kξ) 1/k.\nProof. Since ‖Gλ̃‖2 ≤ 2k+1kξ (by Lemma 5.7), we have equivalently that the ‖.‖2 norm of gΛ̃ = ϑV2kΛ̃ is at most 2 k+1kξ. We may write ϑV2kΛ̃ as\nϑV2kΛ̃ = ϑ\n\n   \nPλ̃(α1) α1Pλ̃(α1) · · · αk−11 Pλ̃(α1) Pλ̃(α2) α2Pλ̃(α2) · · · αk−12 Pλ̃(α2)\n... ...\n. . . ...\nPλ̃(αk) αkPλ̃(αk) · · · αk−1k Pλ̃(αk)\n\n   \nwhich is equal to ϑ′Vk(α) where ϑ′ = ( ϑ1Pλ̃(α1), · · · , ϑkPλ̃(αk) ) . Thus, we are given that ‖ϑ′Vk‖2 ≤ 2k+1kξ.\nLet (γi)† = ( argminy∈Rk:yi=1 ‖yVk‖2 ) Vk. Then, we also have ‖ϑ′Vk‖2 ≥ maxi |ϑ′i|‖γi‖2. Note that γi must be orthogonal to (Vk)j∗ for all j 6= i, and (Vk)i∗γi = ‖γi‖22. (Recall that Zi∗ denotes row i of a matrix Z.) Let Qi(x) = ∑k−1 ℓ=0 γ i ℓx ℓ. Then, Qi(x) = ‖γi‖22 ∏ j 6=i x−αj αi−αj . Also, since the coefficients of Qi(x) have alternating signs, we have\n|Qi(−1)| = ‖γi‖1 = ‖γi‖22 ∏\nj 6=i\n1 + αj |αi − αj| .\nHence, ‖γi‖2 ≥ ∏ j 6=i |αi−αj | 1+αj . So we obtain the lower bound\n‖ϑ′Vk‖2 ≥ max i\n(\n|ϑ′i|· ∏\nj 6=i\n|αi − αj | 1 + αj ) ≥ max i ( ϑi (τ 2\n)k−1 k∏\nj=1\n|αi−ᾱj| )\n≥ max i\n(\nϑi\n(τ\n2\n)k−1 k∏\nj=1\n|αi−Re(ᾱj)| ) .\nThe last inequality follows since complex roots occur in conjugate pairs, so if ᾱℓ = a+bi is complex, then there must be some ℓ′ such that ᾱℓ′ = a− bi and therefore,\n∏\nj\n|αi − ᾱj| = ( (αi − a)2 + b2 ) · ∏ j 6=ℓ,ℓ′ |αi − ᾱj | ≥ (αi − a)2 · ∏ j 6=ℓ,ℓ′ |αi − ᾱj |.\nNow, we claim that |αi−Re(ᾱj)| ≥ ∣ ∣|αi− α̃j|−ǫ ∣\n∣ for every j. If both Re(ᾱj) and Re(α̂j) lie in [0, 1], or both of them are less than 0, or both are greater than 1, then this follows since |ᾱj − α̂j| ≤ ǫ and αi ∈ [0, 1]. If Re(ᾱj) /∈ [0, 1] but Re(α̂j) ∈ [0, 1], or if Re(ᾱj) ∈ [0, 1] but Re(α̂j) /∈ [0, 1], then this again follows since |ᾱj − α̂j | ≤ ǫ. Combining everything, we get that\n2k(2kξ) ≥ ‖ϑ′Vk‖2 ≥ max i\n(\nϑi\n(τ\n2\n)k−1 k∏\nj=1\n∣ ∣|αi − α̃j | − ǫ ∣ ∣\n)\n.\nThis implies that for every i = 1, . . . , k, there exists σ(i) ∈ {1, . . . , k} such that ϑi|αi − α̃σ(i)| ≤ 4 τ · ( 2kξ )1/k + ǫ.\nWe can now wrap up the proof of Lemma 5.6. Let η = 8τ · ( 2kξ )1/k . We will bound ‖ϑ̃Ṽ2k − g̃‖2 by exhibiting a solution y ∈ [0, 1]k , ‖y‖1 = 1 such that ‖yṼ2k − g̃‖2 ≤ ‖g − g̃‖ + k(8k)3/2η. Let σ be the function whose existence is proved in Lemma 5.8. For j = 1, . . . , k, set yj = ∑ i:σ(i)=j ϑi (if σ−1(j) = ∅, then yj = 0). We have ‖yṼ2k − g̃‖2 ≤ ‖g − g̃‖2 + ‖g − yṼ2k‖2. We expand g − yṼ2k = ϑV2k − yṼ2k = ∑k i=1 ϑi ( (V2k)i∗ − (Ṽ2k)σ(i)∗ ) For every i,\nϑ2i ‖(V2k)i∗ − (Ṽ2k)σ(i)∗‖22 = ϑ2i 2k−1 ∑\nℓ=0\n(αℓi − α̃ℓσ(i))2 ≤ ϑ2i · 8k3 · η2.\nTherefore, ‖g − yṼ2k‖2 ≤ k(8k)3/2η."
    }, {
      "heading" : "6 Lower bounds",
      "text" : "In this section, we prove sample-size and aperture lower bounds that apply even to the setting where we have k-mixture sources on {0, 1} (so n = 2). Recall that a k-mixture source on {0, 1} may be equivalently viewed as a k-spike distribution supported on [0, 1]; in the sequel, we therefore focus on k-spike distributions. The separation of a k-spike distribution (or the equivalent k-mixture source) is the minimum separation between its spikes. Theorem 6.2 proves that 2k−1 is the smallest aperture at which it becomes possible to reconstruct a k-spike distribution. We emphasize that this is an information-theoretic lower bound. We show (Theorem 6.2) that there are two k-spike distributions supported on [0, 1] having separation Ω (\n1 k\n) and transportation distance Ω ( 1 k )\nthat yield exactly the same first 2k − 2 moments. Moreover, for any b ≥ 2k − 1, by adjusting the constant in the Ω(.)s, one can ensure that the (2k − 1)-th, . . . , b-th moments of these two k-spike distributions are exponentially close.\nIt follows immediately that even with infinite sample size it is impossible to reconstruct a kmixture source (with arbitrarily small error) if we limit the aperture to 2k − 2. Furthermore, we leverage the exponential closeness of the moments to show that for any aperture b ≥ 2k − 1, there exists τ = Ω (\n1 k\n)\nsuch that reconstructing a k-mixture source on {0, 1} having separation τ to within transportation distance τ4 requires exponential in k sample size (Theorem 6.1). In fact, since n = 2, this means that with arbitrary mixtures, the exponential dependence of the sample size on k remains even with aperture O(k log n), and more generally, even with aperture O ( k ·κ(n) ) for any function κ(.). (To place this in perspective, observe that with separation τ = Ω (\n1 k\n)\n, if we have Ω(k2 log k) aperture, then O(k3) samples suffice to reconstruct the given mixture source to within transportation distance τ4 . This is because with, with high probability, we will see every {0, 1} source or “coin” with weight ϑi ≥ 1τ2 , and we can estimate its bias within additive error, say τ8 , with probability 1 − 1poly(k) since we have Ω ( log k τ2 ) coin tosses available. The unseen coins contribute O(τ) to the transportation distance, so we infer k-mixture source within transportation distance τ4 .)\nTheorem 6.1. (i) With aperture 2k − 2, it is impossible to reconstruct a k-mixture source having separation at least 12k to within transportation distance 1 8k even with infinite sample size. (ii) For any ψ ∈ (0, 1), and any constants cA ≥ 1, cE ≥ 0, there exists τ = Ω ( 1 k )\nsuch that reconstructing a k-mixture source having separation τ to within transportation distance τ4 with probability at least 1− ψ using aperture cA(2k − 1) requires Ω ( 3cEk ln( 1ψ ) ) samples.\nOur approach for proving Theorem 6.1 is as follows. To prove the existence of two suitable k-spike distributions (Theorem 6.2), we fix some spike locations ensuring the required separation and transportation-distance bounds, and search for suitable probability weights to place on these locations so as to obtain the desired closeness of moments for the two k-spike distributions. Since moments are linear functions of the weights (and the spike locations are fixed), this search problem can be encoded as a minimization LP (P1). To upper bound the optimum, we move to the dual LP (D1), which can be interpreted as finding a polynomial satisfying certain conditions on its coefficients and roots, to maximize the variation between its values at certain spike locations. We upper bound the variation possible by such a polynomial using elementary properties of polynomials. Finally, the closeness of moments of the two k-spike distributions obtained this way implies that the distributions of b-snapshots of these two distributions have exponentially small variation distance (Lemma 6.3), and this yields the sample-size lower bound in Theorem 6.1.\nTheorem 6.2. Let b ≥ 2k − 1, and ρ ≥ 2. There are two k-spike distributions (y, α) and (z, β) on [0, 1] with separation 2(2k−1)ρ and Tran(y, α; z, β) ≥ 1(2k−1)ρ such that gℓ(y, α) = gℓ(z, β) for all\nℓ = 0, . . . , 2k − 2, and ∑bℓ=2k−1 ( b ℓ ) 2ℓ|gℓ(y, α) − gℓ(z, β)| ≤ 4 · 3 b ρ2k−1 .\nProof. Let ǫ = 1ρ . We set αi = ǫ · 2(i−1) 2k−1 , and βi = ǫ · 2i−12k−1 = αi+ ǫ2k−1 for all i = 1, . . . , k. Note that for any mixture weights y1, . . . , yk, and z1, . . . , zk, the separation of (y, α) and (z, β) is 2\n(2k−1)ρ and\nTran(y, α; z, β) ≥ 1(2k−1)ρ . We obtain y and z by solving the following linear program (P1), whose optimal value we show is at most 4 · 3b\nρ2k−1 .\nmin b ∑\nℓ=2k−1\n(\nb\nℓ\n)\n2ℓλℓ (P1)\ns.t.\nk ∑\ni=1\n(\nziβ ℓ i − yiαℓi) = 0 ∀ℓ = 0, . . . , 2k − 2 (7)\nk ∑\ni=1\n(\nziβ ℓ i − yiαℓi) ≤ λℓ ∀ℓ = 2k − 1, . . . , b (8)\nk ∑\ni=1\n(\nyiα ℓ i − ziβℓi ) ≤ λℓ ∀ℓ = 2k − 1, . . . , b (9)\nk ∑\ni=1\nyi = 1, yi, zi ≥ 0 ∀i = 1, . . . , k.\nmax c (D1)\ns.t. c− 2k−2 ∑\nℓ=0\nγℓα ℓ i −\nb ∑\nℓ=2k−1 (γℓ − θℓ)αℓi ≤ 0\n∀i = 1, . . . , k 2k−2 ∑\nℓ=0\nγℓβ ℓ i +\nb ∑\nℓ=2k−1 (γℓ − θℓ)βℓi ≤ 0\n∀i = 1, . . . , k\nγℓ + θℓ ≤ ( b\nℓ\n)\n2ℓ ∀ℓ = 2k − 1, . . . , b\nγℓ, θℓ ≥ 0 ∀ℓ.\n(D1) is the dual of (P1). The dual variable c corresponds to ∑\ni yi = 1, variables γℓ for ℓ = 0, . . . , b correspond to (7) and (9), and variables θℓ for ℓ = 2k − 1, . . . , b correspond to (8). Given a feasible solution to (D1), if we set γ′ℓ = γℓ − min{γℓ, θℓ}, θ′ℓ = θℓ − min{γℓ, θℓ} for all ℓ = 2k−1, . . . , b, then we obtain another feasible solution to (D1), where γ′ℓ+θ′ℓ = |γ′ℓ−θ′ℓ| = |γℓ−θℓ|. Thus, an optimal solution to (D1) can be interpreted as a polynomial f(x) =\n∑b ℓ=0 fℓx ℓ satisfying\n|fℓ| ≤ (b ℓ ) 2ℓ for all ℓ = 2k − 1, . . . , b, and f(αi) ≥ c > 0 ≥ f(βi) for all i = 1, . . . , k (where c > 0 follows from Lemma 5.4).\nLet c′ = 3b · ρ/(ρ−1) ρ2k−1 ≤ 2 · 3b ρ2k−1 . Suppose that c > 2c′. Observe that for any x ∈ [0, ǫ], by the Cauchy-Schwarz inequality and since the ℓ2-norm is at most the ℓ1 norm, we have\n∣ ∣ ∣\nb ∑\nℓ=2k−1 fℓx\nℓ ∣ ∣ ∣ ≤ (\nb ∑\nℓ=2k−1 |fℓ|\n)( b ∑\nℓ=2k−1 xℓ )\n≤ b ∑\nℓ=2k−1\n(\nb\nℓ\n) 2ℓ · ρ/(ρ− 1) ρ2k−1 ≤ c′. (10)\nLet h(x) = ∑2k−2 ℓ=0 fℓx ℓ − c/2. Then, due to (10), we have f(x)− c/2− c′ ≤ h(x) ≤ f(x)− c/2 + c′ for all x ∈ [0, ǫ], so h(αi) > 0 > h(βi) for all i = 1, . . . , k. But then h(x) has 2k − 1 roots—one in every (αi, βi) and (βi, αi+1) interval—which is impossible since h(x) is a polynomial of degree 2k − 2.\nGiven a k-spike distribution ( ϑ, α = (α1, . . . , αk) )\non [0, 1], we abuse notation and denote the equivalent k-mixture source on {0, 1} also by (ϑ, α); that is, θ = (ϑ, α) represents a mixture of k “coins”, where coin i has bias αi and is chosen with weight ϑi. Let g(ϑ, α) = ( gi(ϑ, α) )2k−1 i=0 . We use Dθ (viewed as a vector in R {0,1}2k−1 ≥0 ) to denote the distribution of (2k − 1)-snapshots induced by θ on {0, 1}2k−1. The total variation distance dTV(Dy,Dz) between two such distributions is defined to be 12‖Dy −Dz‖1.\nLemma 6.3. Let b ≥ 2k−1. Given two k-mixture sources y = (y, α) and z = (z, β) on {0, 1} with identical first 2k − 2 moments, we have dTV(Dy,b,Dz,b) = 12 ∑b ℓ=2k−1 (b ℓ ) 2ℓ|gℓ(y, α) − gℓ(z, β)|.\nProof. For any s ∈ {0, 1}b with i 1s, we have Dy,bs = νi(y, α) and Dz,bs = νi(z, β). Therefore, dTV(D y,b,Dz,b) = 12 ∑b i=0 (b i )\n|νi(y, α)−νi(z, β)|. Let B be the (b+1)× (b+1) diagonal matrix with Bii = ( b i ) for 0 ≤ i ≤ b. Then, ( b i ) νi(y, α) = ( gb(y, α)Pas−1b+1B ) i . Let ∆gb := gb(y, α) − gb(z, β). So dTV(D y,b,Dz,b) = 12‖(∆gb)(Pas−1b+1B)‖1. We prove below that Pas−1b+1 is the lower triangular matrix with entries (Pas−1b+1)ij = (−1)i−j (b−j i−j )\nfor 0 ≤ j ≤ i ≤ b (and 0 otherwise). Let Zi∗ denote row i of matrix Z. Since (∆gb)i = 0 for i = 0, . . . , 2k − 2, we have that dTV(Dy,b,Dz,v) is at most\n1\n2\nb ∑\nℓ=2k−1 |(∆gb)ℓ| · ‖(Pas−1b+1B)ℓ,∗‖1 =\n1\n2\nb ∑\nℓ=2k−1 |(∆gb)ℓ|\nℓ ∑\nj=0\n(\nb− j ℓ− j\n)(\nb\nj\n)\n= 1\n2\nb ∑\nℓ=2k−1 |(∆gb)ℓ|\n(\nb\nℓ\n) ℓ ∑\nj=0\n(\nℓ\nj\n)\n= 1\n2\nb ∑\nℓ=2k−1 |(∆gb)ℓ|\n(\nb\nℓ\n)\n2ℓ.\nTo see the claim about Pas−1b+1, let Q be the claimed inverse matrix; so Qij = (−1)i−j (b−j i−j ) for 0 ≤ j ≤ i ≤ b. Then (Pasb+1Q)ij = 0 for j > i, and is equal to ∑i\nℓ=j\n( b−ℓ i−ℓ ) (−1)ℓ−j ( b−j ℓ−j ) otherwise.\nThe latter expression evaluates to ( b−j i−j ) ∑i ℓ=j(−1)ℓ−j ( i−j ℓ−j ) = ( b−j i−j ) (1 − 1)i−j , which is 0 if i > j, and 1 if i = j.\nProof of Theorem 6.1. For part (i), take ρ = 2 and b = 2k− 1. Consider the two k-mixture sources y = (y, α) and z = (z, β) given by Theorem 6.2, which have separation 12k−1 and transportation distance 12(2k−1) . For any b ′ ≤ 2k − 2, the distributions Dy,b′ and Dz,b′ are identical and hence indistinguishable even with infinitely many samples, but the stated reconstruction task would allow us to do precisely this.\nFor part (ii), set ρ = 3cE+cA , b = cA(2k − 1), and τ = 2(2k−1)ρ . Let y = (y, α) and z = (z, β) be as given by Theorem 6.2 (for this b, ρ), which satisfy the required separation property. Suppose that we can perform the stated reconstruction task using N b-snapshots. Then, we can distinguish between y and z with probability at least 1 − ψ. But this probability is also upper bounded by [\n1 + dTV ( (Dy,b)N , (Dz,b)N )] /2, where (Dy,b)N and (Dz,b)N are the N -fold products of Dy,b and Dz,b respectively. Thus, dTV ( (Dy,b)N , (Dz,b)N ) ≥ 1− 2ψ. By Proposition 11 and Lemma 12 in [7]\nN ≥ 1 4dTV(Dy,b,Dz,b) ln\n(\n1 1− (1− 2ψ)2 ) ≥ ρ 2k−1 8 · 3b ln ( 1 4ψ ) = Ω ( 3cE(2k−1) ln ( 1 ψ ) )\nwhere the second inequality follows from Theorem 6.2 and Lemma 6.3."
    }, {
      "heading" : "A Probability background",
      "text" : "We use the following large-deviation bounds in our analysis.\nLemma A.1 (Chernoff bound; see Theorem 1.1 in [23]). Let X1, . . . ,XN be independent random variables with Xi ∈ [0, 1] for all i, and µ = ( ∑ i EXi ) /N . Then, Pr [∣ ∣ 1 N ∑ iXi−µ ∣ ∣ > ǫ ] ≤ 2e−2ǫ2N .\nLemma A.2 (Bernstein’s inequality; see Theorem 1.2 in [23]). Let X1, . . . ,XN be independent random variables with |Xi| ≤ b, E[Xi] = 0 for all i, and let σ2 = ∑ i Var[Xi]. Then, Pr [\n|∑iXi| > t ] ≤ 2 exp (\n− t2 2(σ2+bt/3) ) ."
    }, {
      "heading" : "B Sample-size dependence of [2, 3, 4] on n for ℓ1-reconstruction",
      "text" : "We view P = (p1, . . . , pk) as an n × k matrix. Recall that r = ∑kt=1 wtptp † t , A = ∑k t=1 wt(p t − r)(pt − r)†, and M = rr† + A. Let wmax := maxt wt. We consider isotropic k-mixture sources, which is justified by Lemma 3.3. So 12n ≤ ri ≤ 2n for all i ∈ [n]. Note that ‖r‖1, ‖r‖22, and ‖r‖∞ are all Θ (\n1 n\n)\n. It will be convenient to split the width parameter ζ into two parameters. Let (i) ζ1√ n = minp,q∈P,p 6=q ‖p − q‖2; and (ii) ζ22‖r‖∞ be the smallest non-zero eigenvalue of A. Then, the width of (w,P ) is ζ = min{ζ1, ζ2}. We use σi(Z) to denote the i-th largest singular value of a matrix Z. If Z has rank ℓ, its condition number is given by κ(Z) := σ1(Z)/σℓ(Z). For a square matrix Z with real eigenvalues, we use λi(Z) to denote the i-th largest eigenvalue of Z. Note that if Z is an n × k matrix, then σi(Z)2 = λi(ZZ†) = λi(Z†Z) for all i = 1, . . . , k. Also the singular values of ZZ† coincide with its eigenvalues, and the same holds for Z†Z.\nWe now proceed to evaluate the sample-size dependence of [2, 3, 4] on n for reconstructing the mixture constituents within ℓ1-distance ǫ. Since these papers use different parameters than we do, in order to obtain a meaningful comparison, we relate their bounds to our parameters ζ1, ζ2; we keep track of the resulting dependence on n but ignore the (polynomial) dependence on other quantities. We show that the sample size needed is at least Ω ( n4\nǫ2\n)\n, with the exception of Algorithm\nB in [3], which needs Ω ( n3\nǫ2\n)\nsamples. As required by [2, 3, 4], we assume that P has full column rank. It follows that M has rank k and A has rank k − 1. The following inequality will be useful. Proposition B.1. Let D = diag(d1, . . . , dk) where d1 ≥ d2 ≥ . . . ≥ dk > 0. Then λk(PDP †) ≥ dkλk(PP †) = dkσk(P )2.\nComparison with [4]. The algorithm in [4] requires also that P be ρ-separable. This means that for every t ∈ [k], there is some i ∈ [n] such that pti ≥ ρ and pt ′ i = 0 for all t ′ 6= t. This has the\nfollowing implications. For any t, t′ ∈ [k], t 6= t′, we have ‖pt − pt′‖2 ≥ √ 2ρ, so ζ1√\nn ≥\n√ 2ρ. We can\nwrite P †P = Y + Z, where Y is a PSD matrix, and Z is a diagonal matrix whose diagonal entries are at least ρ2. So λk(P †P ) = λk(PP †) ≥ ρ2. Therefore,\nζ22‖r‖∞ + ‖r‖22 = λk(A) + ‖r‖22 ≥ λk(M) ≥ wmin · ρ2\nwhere the first inequality follows from Lemma 2.2, and the second from Proposition B.1. It follows that ρ = O (\n1√ n\n)\n. The bound in [4] to obtain ℓ∞ error ε is (ignoring dependence on other quantities)\nΩ ( 1 ε2ρ6 ) . So setting ε = ǫn to guarantee ℓ1-error at most ǫ and plugging in the above upper bounds on ρ, we obtain that the sample size is Ω ( n5\nǫ2\n)\n.\nComparison with [2]. The sample size required by [2] for the latent Dirichlet model for obtaining ℓ2 error ε is Ω (\n1 ε2σk(P )6\n)\n. Proposition B.1 yields λk(M) ≥ wmin · σk(P )2 and as argued above, λk(M) ≤ λk(A) + ‖r‖22 = O ( 1 n ) . So σk(P ) 6 = O ( 1 n3 ) . Setting ε = ǫ√ n for ℓ1 error ǫ, this yields a bound of Ω ( n4\nǫ2\n)\n.\nComparison with [3]. Algorithm A in [3] requires sample size Ω ( 1 σk(P )8σk(M)4ε2 ) to recover each pt to within ℓ2-distance εmaxp∈P ‖p‖2. Since maxp∈P ‖p‖2 ≤ 2wmin√n due to isotropy, we can set ε = ǫwmin2 to obtain ℓ1-error ǫ. Since σk(P ) 2 and σk(M) = λk(M) are both O ( 1 n ) , we obtain a bound of Ω ( n8\nǫ2\n)\n.\nAlgorithm B in [3] uses sample size Ω ( κ(P )8/ ( ζ2 1\nn · σk(M)2ε2 )\n)\nto recover each pt to within ℓ2-\ndistance εmaxp∈P ‖p‖2. Clearly κ(P ) ≥ 1. Again, setting ε = ǫwmin2 , this yields a sample size of Ω ( n3\nǫ2\n)\nfor ℓ1 error ǫ."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2013,
    "abstractText" : "<lb>We give an algorithm for learning a mixture of unstructured distributions. This problem<lb>arises in various unsupervised learning scenarios, for example in learning topic models from<lb>a corpus of documents spanning several topics. We show how to learn the constituents of<lb>a mixture of k arbitrary distributions over a large discrete domain [n] = {1, 2, . . . , n} and the<lb>mixture weights, using O(n polylogn) samples. (In the topic-model learning setting, the mixture<lb>constituents correspond to the topic distributions.)<lb>This task is information-theoretically impossible for k > 1 under the usual sampling process<lb>from a mixture distribution. However, there are situations (such as the above-mentioned topic<lb>model case) in which each sample point consists of several observations from the same mixture<lb>constituent. This number of observations, which we call the “sampling aperture”, is a crucial<lb>parameter of the problem.<lb>We obtain the first bounds for this mixture-learning problem without imposing any assump-<lb>tions on the mixture constituents. We show that efficient learning is possible exactly at the<lb>information-theoretically least-possible aperture of 2k − 1. Thus, we achieve near-optimal de-<lb>pendence on n and optimal aperture. While the sample-size required by our algorithm depends<lb>exponentially on k, we prove that such a dependence is unavoidable when one considers general<lb>mixtures.<lb>A sequence of tools contribute to the algorithm, such as concentration results for random<lb>matrices, dimension reduction, moment estimations, and sensitivity analysis.",
    "creator" : "LaTeX with hyperref package"
  }
}