{
  "name" : "1601.04011.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Average Stability is Invariant to Data Preconditioning. Implications to Exp-concave Empirical Risk Minimization",
    "authors" : [ "Alon Gonen", "Shai Shalev-Shwartz" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n60 1.\n04 01\n1v 4\n[ cs\n.L G\n] 1\n6 A"
    }, {
      "heading" : "1 Introduction",
      "text" : "Central to statistical learning theory is the notion of (algorithmic) stability. Since being introduced by [4], deep connections between the generalization ability and the algorithmic stability of a learning algorithm have been established. It was shown by [22, 18] that stability characterizes learnability. Furthermore, in expectation, some notion of stability is exactly equal to the generalization error of an algorithm (namely, to the gap between true loss and train loss).\nFor generalized linear learning problems, a prominent geometric property which upper bounds the stability rate is the condition number of the loss function. While uniform convergence bounds ([21][Chapter 4]) mostly yield bounds that scale with 1{?n, where n is the size of the sample, well-conditioned problems admit faster (stability) rates that scale linearly with 1{n. The caveat is that typical (large-scale) machine learning problems are ill-conditioned. While we defer the precise definition of the condition number to the next part, let us mention that the condition number is controlled by two related quantities corresponding to both the choice of the loss function and the choice of the coordinate system. In a nutshell, our paper establishes the following result:\n∗School of Computer Science, The Hebrew University, Jerusalem, Israel †School of Computer Science, The Hebrew University, Jerusalem, Israel\nThe average stability of ERM is invariant to the choice of the coordinate system.\nWhile this observation admits a one-line proof, it has far-reaching implications. In particular, in this paper we use this observation to establish fast rates for empirical risk minimization.\nThe rest of the paper is organized as follows. In Section 2 we define the setting and proceed to provide basic definitions and results in stability analysis. In Section 3 we state and prove our main result. Section 4 discusses the implications to linear regression as well as improved bounds on the stability of SGD. Related work is discussed in Section 5."
    }, {
      "heading" : "2 Preliminaries",
      "text" : ""
    }, {
      "heading" : "2.1 Setup",
      "text" : "We consider the problem of minimizing the risk associated with generalized linear model :\nmin wPW\nLpwq :“ Epx,yq„DrφypwJxqs . (1)\nHere, both the domain W and the instance space X are assumed to be compact and convex subsets of Rd. We denote by D an arbitrary probability distribution defined over X ˆ Y. Each element y in the label set Y induces a twice differentiable1 loss function of the form φy : twJx : w P W, x P X u Ñ R`. We make the following assumptions on the loss function:\n(A1) For each y P Y, φy is ρ-Lipschitz, i.e., |φ1ypzq| ď ρ for all z. (A2) For each y P Y, φy is α-strongly convex, i.e., φ2ypzq ě α for all z.\nOur main example is the following formulation of linear regression ([19]).\nExample 1 (Linear Regression:) Let X be any compact and convex subset of Rd and Y be an interval of the form r´Y, Y s. The domain W is given by\nW “ tw P Rd : p@x P X q |wJx| ď Y u .\nFor all y P Y, let φy be the square loss, φypzq “ 12pz ´ yq2. Note that for any y P Y and z P twJx : w P W, x P X u,\n|φ1ypzq| “ 1 2 |2pz ´ yq| ď |z| ` |y| ď 2Y, }φ2ypzq} “ 1 .\nHence, the assumptions (A1-2) are satisfied with ρ “ 2Y and α “ 1.\nMore generally, our setting captures all known exp-concave functions ([13]). A twicecontinuously differentiable function f : W Ñ R is said to be ᾱ-exp-concave if ∇2fpwq ľ ᾱ∇fpwq∇fpwqJ for all w P W.\nLemma 1 Consider a risk of the form (1) that satisfies the assumptions (A1-2). Then, for any px, yq P X ˆ Y, the function w P W ÞÑ φypwJxq is α{ρ2-exp concave.\n1As we do not require smoothness of the loss function, our results can easily be extended to continuous but non-differentiable functions.\nProof Fix a pair px, yq P X ˆY. The gradient and the Hessian of the map ℓpwq “ φypwJxq are given by ∇ℓpwq “ φ1pwJxqx, ∇2ℓpwq “ φ2pwJxqxxJ . (2) By assumption |φ1pwJxq| ď ρ and φ2pwJxq ě α, hence ℓ is α{ρ2-exp concave.\nA learning algorithm A receives as an input a training sequence (a.k.a. sample) of n i.i.d. pairs, S “ ppxi, yiqqni“1 „ Dn, and outputs a predictor, ApSq P W. The empirical risk function, L̂ : W Ñ R, is defined as\nL̂Spwq “ L̂pwq “ 1\nn\nn ÿ i“1 φyipwJxiq loooomoooon\n:“ℓ̂ipwq\n. (3)\nIn this paper we focus on the ERM algorithm, whose output is a minimizer of the empirical risk.2 We denote the output of the ERM by ŵpSq, or simply ŵ when S is understood from the context. The generalization error and the excess risk of ŵ are defined by Lpŵq ´ L̂pwq and Lpŵq ´ Lpw‹q, respectively. For ERM, it is immediate that any upper bound on the generalization error translates into the same bound on the excess risk.\nRemark 1 While we mostly focus on exact ERM, it should be emphasized that our results are easily extended to any algorithm that approximately minimizes the empirical risk. The formulation of Lemma 2 below highlights this idea."
    }, {
      "heading" : "2.2 Stability",
      "text" : "In this section we review basic definitions and results on stability. For completeness, we also provide proofs of the stated results.\nLet S “ ppxi, yiqqni“1 be a training sequence. For every i P rns, let ŵi be a minimizer of the risk w.r.t. Sztpxi, yiqu, namely,\nŵi P argmin wPW\n1\nn´ 1 ÿ j‰i ℓ̂jpwq .\nThe average stability of ERM is defined as\n∆pS,Wq “ 1 n\nn ÿ i“1 pℓ̂ipŵiq ´ ℓ̂ipŵqq . (4)\nWe omit the dependency on W when it is clear from the context. The next lemma shows that the expected generalization error of the ERM is equal to the expected average stability.\nLemma 2 Let A be a possibly randomized algorithm and denote by ŵ its output. The generalization error of A satisfies\nES„Dn´1rLpŵq ´ L̂pŵqs “ ES„Dnr∆pSqs . (5) 2The compactness of W implies that both the true and the empirical risks admit minimizers.\nFurthermore, if A satisfies, for every sample S, ErL̂pŵqs ď minwPW L̂pwq ` ǫ, where the expectation is with respect to A’s own randomization, then the excess risk of A is bounded by\nES„Dn´1rLpŵq ´ Lpw‹qs ď ES„Dnr∆pSqs ` ǫ . (6)\nProof Since ŵi does not depend on the i.i.d. pair pxi, yiq,\nES„Dnrℓ̂ipŵiqs “ ES„Dn´1rLpŵpSqqs , i “ 1, . . . , n .\nBy linearity of expectation, we obtain\nES„Dn\n«\n1\nn\nn ÿ i“1 ℓ̂ipŵiq\nff\n“ ES„Dn´1rLpŵpSqqs .\nTherefore,\nEr∆pSqs “ ES„Dn « 1\nn\nn ÿ i“1 ℓ̂ipŵiq\nff ´ES„Dn « 1\nn\nn ÿ i“1 ℓ̂ipŵq\nff\n“ ES„Dn´1rLpŵqs´ES„DnrL̂pŵqs .\nThis establishes the first claim. Next, by assumption, for every S, ErL̂pŵqs ď L̂pw‹q ` ǫ. Hence,\nES„DnrL̂pŵqs ď ES„DnrL̂pw‹qs ` ǫ “ Lpw‹q ` ǫ .\nCombining this inequality with the first claim, concludes the proof."
    }, {
      "heading" : "2.2.1 Stability of Well-conditioned Objectives",
      "text" : "Lemma 2 motivates us to derive an upper bound on the average stability. A key quantity that governs ∆pSq is the condition number of the objective. We next provide exact definitions and discuss this relation.\nFix a training sequence S. We denote the empirical covariance matrix by\nĈ :“ ĈpSq “ 1 n\nn ÿ i“1 xix J i .\nThe (average) empirical condition number of Ĉ is defined as\nκpĈq “ trpĈq λminpĈq ,\nwhere trpĈq is the trace of Ĉ and λminpĈq is the smallest nonzero eigenvalue of Ĉ. We define the functional condition number as the ratio between the squared Lipschitz parameter and the strong convexity parameter:\nκpφq “ ρ 2\nα .\nFinally, we define the condition number of the objective as the product between the empirical and the functional condition number:\nκ “ κpĈqκpφq .\nLemma 3 For every training sequence S,\n∆pSq ď 2κ n “ 2κpĈqκpφq n “ 2ρ 2 αn κpĈq . (7)\nTo the best of our knowledge, this result has only been proved in the context of regularized loss minimization (e.g., the bound on the uniform stability in [21][Corollary 13.6]). Inspecting the proofs, one can notice that the role of regularization is merely to ensure the strong convexity of the objective. This simple observation is crucial for our development.\nProof (of Lemma 3) We first assume that Ĉ is of full rank. Note that for all w, the Hessian of L̂ at w is given by\n∇2L̂pwq “ 1 n\nn ÿ i“1 φ2pwJxiqxixJi ľ 1 n n ÿ i“1 αxix J i “ α Ĉ . (8)\nIn particular, L̂ is strongly convex and ŵ is uniquely defined. Denote the strong convexity parameter of L̂ by µ̂. We also denote the Lipschitz parameter of each ℓ̂i by ρ̂i and define ρ̂2 “ 1\nn řn i“1 ρ̂ 2\ni . We will shortly derive upper and lower bounds on these parameters, but first let us relate them to the average stability.\nFix some i P rns and let ∆̂i “ ℓ̂ipŵiq ´ ℓ̂ipŵq (we do not assume that ŵi is uniquely defined). The ρ̂i-Lipschitzness of ℓ̂i yields the bound\n∆̂i ď ρ̂i}ŵi ´ ŵ} .\nThe µ̂-strong convexity of L̂ implies (e.g. using [20][Lemma 2.8]) that\nµ̂ 2 }ŵi ´ ŵ}2 ď L̂pŵiq ´ L̂pŵq .\nOn the other hand, since ŵi minimizes the risk over Sztpxi, yiqu, we have that\nL̂pŵiq ´ L̂pŵq “ ř j‰ipℓ̂jpŵiq ´ ℓ̂jpŵqq n ` ℓ̂ipŵiq ´ ℓ̂ipŵq n ď 0` ∆̂i n .\nCombining the bounds, we conclude the following bound for every i P rns:\n∆̂2i ď ρ̂2i }ŵi ´ ŵ}2 ď 2ρ̂2i µ̂ pL̂pŵiq ´ L̂pŵqq ď 2ρ̂2i nµ̂ ∆̂i .\nDividing by ∆̂i (we may assume w.l.o.g. that ∆̂i ą 0), we obtain\n∆̂i ď 2ρ̂2i nµ̂ . (9)\nLet us remark that at this point, we can deduce a bound of maxiPrns 2ρ̂2i nµ̂\non the uniform stability. This matches the bound in [21][Corollary 13.6]. We next proceed to establish the claimed bound on the average stability.\nBy averaging (9) over i “ 1, . . . , n , we obtain\n∆̂ “ 1 n\nn ÿ i“1 ∆̂i ď\n˜\n1\nn\nn ÿ i“1 ρ̂2i\n¸\n2 nµ̂ “ 2ρ̂\n2\nnµ̂ . (10)\nIt remains to derive bounds on ρ̂ and µ̂. Note that\n}∇ℓ̂ipwq}2 “ }φ1pwJxiqxi}2 ď ρ2}xi}2 “ ρ2trpxixJi q .\nHence, ρ̂2i ď ρ2trpxixJi q. By averaging, we obtain that ρ̂2 ď ρ2trpĈq. Next, using (8) we obtain that µ̂ ě αλdpĈq. By substituting the bounds on ρ̂2 and µ̂ in (10), we conclude the desired bound.\nNote that if Ĉ is not of full rank, we can replace each vector x P Rd with UJx, where the columns of U form an orthonormal basis for spanptx1, . . . , xnuq, without affecting ∆̂, ∆̂1, . . . , ∆̂n (this modification is only for the sake of the analysis). As a result, the new covariance matrix is of full rank and its eigenvalues are λ1pĈq, . . . , λminpĈq. Repeating the above arguments, we conclude the proof.\nLet us specify the bound to linear regression as formulated in Example (1). As α “ 1 and ρ “ 2Y , the functional condition number is 4Y 2. Hence, the average stability is bounded by\n∆pSq ď 4Y 2\nn κ̂pĈq . (11)\nUsing Lemma 2 we deduce the same bound on the excess risk. The weakness of this bound stems from the fact that empirically, the empirical condition number tends to be huge (e.g., see the empirical study in [8]).\nIn the next section we show that the (dependence on the) empirical condition number associated with our arbitrary choice of coordinate system can be replaced by the empirical condition number obtained by an optimal preconditioning."
    }, {
      "heading" : "3 Preconditioned Stability",
      "text" : "We are now in position to describe our main result. Let P be a (symmetric) positive definite matrix, SP be the training set obtained by replacing every xj with x̃j “ P´1{2xj , and WP “ P 1{2W. We call P´1{2 a preconditioner. Recall the definition of average stability from Equation (4). Our main theorem is:\nTheorem 1 For any training sequence S and positive definite matrix P ,\n∆pSP ,WP q “ ∆pS,Wq .\nIn words, the average stability is invariant to the choice of the coordinate system.\nProof The crucial observation is that the empirical risk minimization with respect to SP over the domain WP is equivalent to the ERM w.r.t. S over the domain W in the following sense. For any pair pw, w̃ “ P 1{2wq P W ˆWP and any j P rns, the prediction pw̃qJx̃j is equal to the prediction wJxj . Therefore, the empirical risks L̂Sppw̃q and L̂Spwq are equal. By associating the corresponding minimizers of the empirical risk (i.e., ŵ is associated with P 1{2ŵ and for any i P rns, ŵi is associated with P 1{2ŵi), we conclude our proof. Theorem 1 tells us that we can analyze the stability of SP instead of the stability of S. Crucially, this is true for every P , even one that is chosen based on S. Therefore, the expected suboptimality is upper bounded by the expected value of the quantity, infPą0∆pSP ,WP q, which we refer to as the preconditioned average stability. Equipped with this observation, we next choose P that leads to a minimal condition number, and consequently obtain a tighter bound on the excess risk.\nNote that for every P ą 0, the empirical covariance matrix that corresponds to the preconditioned training sequence, SP , is\n1\nn\nn ÿ i“1 pP´1{2xiqpP´1{2xiqJ “ P´1{2\n˜\n1\nn\nn ÿ i“1 xix J i\n¸\nP´1{2 “ P´1{2ĈP´1{2 .\nWhen Ĉ is of full rank, by choosing P “ Ĉ, we obtain that\nκpP´1{2ĈP´1{2 looooooomooooooon\nI\nq “ trpIq λminpIq “ d .\nIf Ĉ is not of full rank, we can add arbitrary “noise” in directions that do not lie in the column space of Ĉ. For example, by choosing P “ Ĉ ` δpI ´ ĈĈ:q, (where δ can be any positive scalar), we obtain that κpP´1{2ĈP´1{2q “ rankpĈq ď d. It is easy to see that in both cases, we obtain the minimal value of κpP´1{2ĈP´1{2q over all matrices P ą 0. Combining this bound with Lemma 2 and Lemma 3, we arrive at the following conclusion.\nCorollary 1 Consider the optimization problem (1), where for all y P Y, φy is ρ-Lipschitz and α-strongly convex. The expected excess risk of empirical risk minimization is bounded by\nES„Dn´1rLpŵq ´ Lpw‹qs ď ES„Dnr∆pSqs “ ES„Dnr inf Pą0\n∆pSP qs ď 2ρ2 d\nαn .\nRemark 2 Note that the theorem holds for any algorithm; We only use the fact that the prediction itself is invariant to preconditioning (for any algorithm).\nUsing Lemma 2, we can deduce similar bound holds for approximate ERM."
    }, {
      "heading" : "4 Some Implications",
      "text" : ""
    }, {
      "heading" : "4.1 Linear Regression",
      "text" : "We start by specifying our bounds to linear regression (Example (1)).\nCorollary 2 (Linear Regression) Consider linear regression as formulated in Example (1). The expected excess risk of ERM is bounded by\nES„Dn´1rLpŵq ´ Lpw‹qs ď ∆pSq ď 4Y 2d\nn .\nComparing the bounds in (11) and Corollary 2, we see that the dependence on κ̂pĈq is replaced by the optimal empirical condition number, κ̂pIq “ d. As we mentioned above, this gap tends to be huge in practice.\nAs we discuss in Section 5, standard bounds for this setting depend on the geometry of X and W. On the contrary, it follows from the generalized Cauchy-Schwarz inequality that for any choice of a norm } ¨ } on Rd, our bound applies to the sets3\nX “ B}¨} “ tx P Rd : }x} ď 1u, W “ Y B}¨}‹ :“ tw P Rd : }w}‹ ď Y u (12)"
    }, {
      "heading" : "4.2 The Average Stability of Stochastic Gradient Descent",
      "text" : "One of the most widely used algorithms in machine learning is Stochastic Gradient Descent (SGD). Besides its computational simplicity, its popularity stems also from its generalization abilities ([21][Section 14.5]). Recently, [9] studied the (uniform) stability of SGD in various settings. Following our notation, theorem 3.9 of their paper implies a bound of maxi 2ρ̂2i γn on the uniform stability, where γ is the strong convexity of the entire objective, and for any i P rns, ρ̂i is the Lipschitz parameter of ℓ̂i. As the proof of Lemma 3 reveals, γ can be bounded by ακ̂pĈq and ρ̂i is at most ρ̂2}xi}2. In particular, the bound depends on the choice of the coordinate system.\nAs implied by [5][Theorem 3.2], SGD can be viewed in our context as an (approximate) ERM. Hence, the average stability of SGD is invariant to the choice of the coordinate system and the stability rate of SGD is bounded as in Corollary 1."
    }, {
      "heading" : "5 Related Work",
      "text" : ""
    }, {
      "heading" : "5.1 Slower rates",
      "text" : "One of the most direct techniques for establishing bounds on the excess risk is via analyzing the Rademacher complexity ([3]) of the associated class of predictors. In our setting, these techniques have been employed by [11] to establish bounds of order 1{?n on the generalization error of ERM. We refer to these rates as slower due to the inferior dependence on the sample size n.\n3In fact, under mild additional assumptions on X , any two sets X and W that satisfy our assumptions can be presented in this way. Assume that X is symmetric (i.e., x P X iff ´x P X ) and 0 P intpX q. Then it is known ([7]) that X induces a norm on Rd through the Minkowsky functional\n}x} :“ ppxq “ inf tt P R : x P tBu .\nIt is immediate that the closed unit ball tx : }x} ď 1u is X itself. Therefore, the dual norm is simply the support function of X\n}w}‹ “ max xPX w J x .\nIt follows that X and W can be described as in (12)."
    }, {
      "heading" : "5.2 Dependence on Norm",
      "text" : "Note that since both the uniform and the average stability of ERM are bounded above by its generalization error ([22]), the bounds of [11] translate into bounds on the average stability.\nUnlike our fast rates, the exact bounds depend on the geometry of the set X and W. For example: a) If both X and W are the Euclidean unit ball in Rd, then the obtained bound scales with 1{?n. b) If X is the unit ℓ8-ball and W is the ℓ1-ball, then the obtained bound scales with a logpdq{n."
    }, {
      "heading" : "5.3 Lower bounds on the excess risk",
      "text" : "Lower bounds for stochastic minimization of exp-concave functions have been studied in [16]. For our setting, theorem 2 in this paper implies a bound of Ωpd{nq on the excess risk of any algorithm.\nFor the special case of linear regression with\nX “ tx P Rd : }x}2 ď 1u, W “ tw P Rd : }w}2 ď Bu, Y “ r´Y, Y s (13)\n[23] proved the lower bound Ω ´\nmintY 2, B2`dY 2 n , BY? n u ¯ on the generalization error of ERM.\nThe left-most term is trivially attained by the predictor w “ 0. The middle term is attained by combining the Vovk-Azoury-Warmuth forecaster ([1, 24]) with standard online-to-batch conversions ([6]). Last, the right term is attained by ERM, as implied by the aforementioned upper bound of [11].\nIt is left open whether the middle term in the lower bound is attained by ERM. Note that if B “ ωp ? dY q, then the middle term in the above lower bound is asymptotically larger than our upper bound. However, since in the setting of [23] (Equation (13)) the magnitude of the predictions is not uniformly upper bounded by Y , no contradiction arises."
    }, {
      "heading" : "5.4 Stability and Regularization",
      "text" : "Previous work ([4, 22]) studied the rate of uniform stability in various settings. For our setting, their bounds on the expected risk are identical to the bound in Lemma 3. As we explained above, these fast rates are often worse than the so-called slower rates due to the dependence on the empirical condition number. The standard approach for tackling this problem is add a regularization term. By adding the regularization term λ}w}2 to the objective, one effectively increases the eigenvalues of Ĉ by λ, and consequently, the overall condition number is decreased. However, as explained in [21][Section 13.4], this modification usually does not preserve the fast rates.4"
    }, {
      "heading" : "5.5 Stability and Exp-concavity",
      "text" : "Informally, exp-concavity can be seen as a local and weaker form of strong convexity. Indeed, the Online Newton Step (ONS) of [10], which has been designed for online minimization\n4Namely, when tuning λ, we need to ensure that any ǫ{2-approximate minimizer with respect to the regularized objective is also an ǫ-approximate minimizer with respect to the unregularized objective. As explained in [21][Section 13.3], by optimally controlling this tradeoff, we no longer obtain fast rates (i.e., the stability rate scales with 1{?n rather than 1{n).\nof exp-concave functions, achieves improved (logarithmic) regret bounds that resemble the regret bounds for strongly convex functions ([10]). The online-to-batch analysis of [16] yields a bound on the excess risk that coincides with our bounds up to logarithmic factors. The main shortcoming of the ONS algorithm is that it employs expensive iterations (the runtime per iteration scales at least quadratically with d). Hence, it is natural to ask whether there exist simpler algorithms that achieve fast rates.\nThis question was answered affirmatively by [15]. This work, which is most closely related to our work, considers the minimization of a risk of the form F pwq “ Erfpw,Zqs, where for any z, fp¨, zq is β̄-smooth5 and ᾱ-exp-concave function. They established fast rates for any algorithm that minimizes the regularized risk L̂pwq ` 1\nn Rpwq, where Rpwq\nis assumed to be a 1-strongly convex function (e.g., one can set Rpwq “ 1 2 }w}2). While exp-concavity is weaker than strong convexity, [15][section 4.2] interprets exp-concavity as strong convexity in the (local) norm induced by the outer products of the gradients and the regularization term. In other words, the problem is well-conditioned with respect to this local norm. Note that their formulation is more general in the sense that they do not assume a GLM structure. However, it should be emphasized that all the known exp-concave functions in machine learning are of the form (1)).\nThe above interpretation of [15] inspired us to make one step forward and directly show that regularization is not required as long as a related (preconditioned) problem is well conditioned. Besides the obvious importance of showing the insignificance of regularization in this context, we believe that the notion of preconditioned stability and its relation to the excess risk make these ideas more transparent and simplify the proofs.\nThe upper bound of [15] on the excess risk scales with 24βd ᾱn “ 24βdρ2 αn\n(recall that the exp-concavity parameter ᾱ is equal to α{ρ2). Note that our analysis does not assume smoothness of the loss. This resolves the question raised by [15] regarding the necessity of the smoothness assumption. Note that for linear regression, the smoothness is 1, making our bounds identical to the bounds of [15] for this special case.\nAs discussed in [15], it is difficult to translate bounds on the average stability into high-probability bounds (while preserving the fast rate and introducing only logarithmic dependence on 1{δ)."
    }, {
      "heading" : "5.6 Other Techniques and High-Probability Bounds",
      "text" : "The bound on the expected excess risk in Corollary 1 can be obtained by using two additional techniques. Both of these techniques also yield high probability bounds. We next survey the corresponding results.\nA recent follow-up work by [17] established a bound of Õpd logpnq ` logp1{δq{nq on the excess risk of ERM, where δ is the confidence parameter.6. He also showed how to get rid of the logpnq factor by boosting the confidence of ERM. The proof is centered around a Bernstein condition which holds due to the exp-concavity assumption.\nAnother alternative, is to bound the excess risk by the local Rademacher complexity (LRC) of the associated class of predictors (e.g., using Corollary 5.3 in [2]). In our setting, one can derive bounds on the LRC (e.g., using [14]) which coincide with our bounds.\n5That is, the maximal eigenvalue of the Hessian of f at any point w is at most β. 6The dependence on the exp-concavity parameter as well as the diameter of the loss function is hidden.\nAll of these techniques employ arguably heavy machinery and lack the geometric interpretation, which is nicely captured by our notion of preconditioned stability."
    }, {
      "heading" : "Acknowledgments",
      "text" : "We thank Iliya Tolstikhin for pointing out the alternative proof of Corollary 1 using local Rademacher complexities."
    } ],
    "references" : [ {
      "title" : "Relative loss bounds for on-line density estimation with the exponential family of distributions",
      "author" : [ "Katy S Azoury", "Manfred K Warmuth" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2001
    }, {
      "title" : "Local rademacher complexities",
      "author" : [ "Peter L Bartlett", "Olivier Bousquet", "Shahar Mendelson" ],
      "venue" : "Annals of Statistics,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2005
    }, {
      "title" : "Rademacher and gaussian complexities: Risk bounds and structural results",
      "author" : [ "Peter L Bartlett", "Shahar Mendelson" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2003
    }, {
      "title" : "Stability and generalization",
      "author" : [ "Olivier Bousquet", "André Elisseeff" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2002
    }, {
      "title" : "Convex optimization: Algorithms and complexity",
      "author" : [ "Sébastien Bubeck" ],
      "venue" : "Foundations and Trends R  © in Machine Learning,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2015
    }, {
      "title" : "On the generalization ability of on-line learning algorithms",
      "author" : [ "Nicolo Cesa-Bianchi", "Alex Conconi", "Claudio Gentile" ],
      "venue" : "Information Theory, IEEE Transactions on,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2004
    }, {
      "title" : "A course in functional analysis, volume 96",
      "author" : [ "John B Conway" ],
      "venue" : "Springer Science & Business Media,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2013
    }, {
      "title" : "Solving ridge regression using sketched preconditioned svrg",
      "author" : [ "Alon Gonen", "Francesco Orabona", "Shai Shalev-Shwartz" ],
      "venue" : "arXiv preprint arXiv:1602.02350,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2016
    }, {
      "title" : "Train faster, generalize better: Stability of stochastic gradient descent",
      "author" : [ "Moritz Hardt", "Benjamin Recht", "Yoram Singer" ],
      "venue" : "arXiv preprint arXiv:1509.01240,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2015
    }, {
      "title" : "Logarithmic regret algorithms for online convex optimization",
      "author" : [ "Elad Hazan", "Amit Agarwal", "Satyen Kale" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2007
    }, {
      "title" : "On the complexity of linear prediction: Risk bounds, margin bounds, and regularization",
      "author" : [ "Sham M Kakade", "Karthik Sridharan", "Ambuj Tewari" ],
      "venue" : "In Advances in neural information processing systems,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2009
    }, {
      "title" : "Algorithmic stability and sanity-check bounds for leave-one-out cross-validation",
      "author" : [ "Michael Kearns", "Dana Ron" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 1999
    }, {
      "title" : "Averaging expert predictions",
      "author" : [ "Jyrki Kivinen", "Manfred K Warmuth" ],
      "venue" : "In Computational Learning Theory,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 1999
    }, {
      "title" : "Oracle inequalities in empirical risk minimization and sparse recovery problems: Lecture notes",
      "author" : [ "V Koltchinskii" ],
      "venue" : "Technical report,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2008
    }, {
      "title" : "Fast rates for exp-concave empirical risk minimization",
      "author" : [ "Tomer Koren", "Kfir Levy" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2015
    }, {
      "title" : "Lower and upper bounds on the generalization of stochastic exponentially concave optimization",
      "author" : [ "Mehrdad Mahdavi", "Lijun Zhang", "Rong Jin" ],
      "venue" : "In Proceedings of The 28th Conference on Learning Theory,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2015
    }, {
      "title" : "From exp-concavity to variance control: O (1/n) rates and onlineto-batch conversion with high probability",
      "author" : [ "Nishant A Mehta" ],
      "venue" : null,
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2016
    }, {
      "title" : "Learning theory: stability is sufficient for generalization and necessary and sufficient for consistency of empirical risk minimization",
      "author" : [ "Sayan Mukherjee", "Partha Niyogi", "Tomaso Poggio", "Ryan Rifkin" ],
      "venue" : "Advances in Computational Mathematics,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2006
    }, {
      "title" : "Beyond logarithmic bounds in online learning",
      "author" : [ "Francesco Orabona", "Nicolo Cesa-Bianchi", "Claudio Gentile" ],
      "venue" : "In AISTATS,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2012
    }, {
      "title" : "Online learning and online convex optimization",
      "author" : [ "Shai Shalev-Shwartz" ],
      "venue" : "Foundations and Trends in Machine Learning,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2011
    }, {
      "title" : "Understanding Machine Learning: From Theory to Algorithms",
      "author" : [ "Shai Shalev-Shwartz", "Shai Ben-David" ],
      "venue" : null,
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2014
    }, {
      "title" : "Learnability, stability and uniform convergence",
      "author" : [ "Shai Shalev-Shwartz", "Ohad Shamir", "Nathan Srebro", "Karthik Sridharan" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2010
    }, {
      "title" : "The sample complexity of learning linear predictors with the squared loss",
      "author" : [ "Ohad Shamir" ],
      "venue" : "arXiv preprint arXiv:1406.5143,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2014
    }, {
      "title" : "Competitive on-line statistics",
      "author" : [ "Volodya Vovk" ],
      "venue" : "International Statistical Review,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2001
    } ],
    "referenceMentions" : [ {
      "referenceID" : 11,
      "context" : "Abstract We show that the average stability notion introduced by [12, 4] is invariant to data preconditioning, for a wide class of generalized linear models that includes most of the known exp-concave losses.",
      "startOffset" : 65,
      "endOffset" : 72
    }, {
      "referenceID" : 3,
      "context" : "Abstract We show that the average stability notion introduced by [12, 4] is invariant to data preconditioning, for a wide class of generalized linear models that includes most of the known exp-concave losses.",
      "startOffset" : 65,
      "endOffset" : 72
    }, {
      "referenceID" : 8,
      "context" : "b) We strengthen the recent bounds of [9] on the stability rate of the Stochastic Gradient Descent algorithm.",
      "startOffset" : 38,
      "endOffset" : 41
    }, {
      "referenceID" : 3,
      "context" : "Since being introduced by [4], deep connections between the generalization ability and the algorithmic stability of a learning algorithm have been established.",
      "startOffset" : 26,
      "endOffset" : 29
    }, {
      "referenceID" : 21,
      "context" : "It was shown by [22, 18] that stability characterizes learnability.",
      "startOffset" : 16,
      "endOffset" : 24
    }, {
      "referenceID" : 17,
      "context" : "It was shown by [22, 18] that stability characterizes learnability.",
      "startOffset" : 16,
      "endOffset" : 24
    }, {
      "referenceID" : 20,
      "context" : "While uniform convergence bounds ([21][Chapter 4]) mostly yield bounds that scale with 1{?n, where n is the size of the sample, well-conditioned problems admit faster (stability) rates that scale linearly with 1{n.",
      "startOffset" : 34,
      "endOffset" : 38
    }, {
      "referenceID" : 18,
      "context" : "Our main example is the following formulation of linear regression ([19]).",
      "startOffset" : 68,
      "endOffset" : 72
    }, {
      "referenceID" : 12,
      "context" : "More generally, our setting captures all known exp-concave functions ([13]).",
      "startOffset" : 70,
      "endOffset" : 74
    }, {
      "referenceID" : 20,
      "context" : ", the bound on the uniform stability in [21][Corollary 13.",
      "startOffset" : 40,
      "endOffset" : 44
    }, {
      "referenceID" : 19,
      "context" : "using [20][Lemma 2.",
      "startOffset" : 6,
      "endOffset" : 10
    }, {
      "referenceID" : 20,
      "context" : "This matches the bound in [21][Corollary 13.",
      "startOffset" : 26,
      "endOffset" : 30
    }, {
      "referenceID" : 7,
      "context" : ", see the empirical study in [8]).",
      "startOffset" : 29,
      "endOffset" : 32
    }, {
      "referenceID" : 20,
      "context" : "Besides its computational simplicity, its popularity stems also from its generalization abilities ([21][Section 14.",
      "startOffset" : 99,
      "endOffset" : 103
    }, {
      "referenceID" : 8,
      "context" : "Recently, [9] studied the (uniform) stability of SGD in various settings.",
      "startOffset" : 10,
      "endOffset" : 13
    }, {
      "referenceID" : 4,
      "context" : "As implied by [5][Theorem 3.",
      "startOffset" : 14,
      "endOffset" : 17
    }, {
      "referenceID" : 2,
      "context" : "1 Slower rates One of the most direct techniques for establishing bounds on the excess risk is via analyzing the Rademacher complexity ([3]) of the associated class of predictors.",
      "startOffset" : 136,
      "endOffset" : 139
    }, {
      "referenceID" : 10,
      "context" : "In our setting, these techniques have been employed by [11] to establish bounds of order 1{?n on the generalization error of ERM.",
      "startOffset" : 55,
      "endOffset" : 59
    }, {
      "referenceID" : 6,
      "context" : "Then it is known ([7]) that X induces a norm on R through the Minkowsky functional }x} :“ ppxq “ inf tt P R : x P tBu .",
      "startOffset" : 18,
      "endOffset" : 21
    }, {
      "referenceID" : 21,
      "context" : "2 Dependence on Norm Note that since both the uniform and the average stability of ERM are bounded above by its generalization error ([22]), the bounds of [11] translate into bounds on the average stability.",
      "startOffset" : 134,
      "endOffset" : 138
    }, {
      "referenceID" : 10,
      "context" : "2 Dependence on Norm Note that since both the uniform and the average stability of ERM are bounded above by its generalization error ([22]), the bounds of [11] translate into bounds on the average stability.",
      "startOffset" : 155,
      "endOffset" : 159
    }, {
      "referenceID" : 15,
      "context" : "3 Lower bounds on the excess risk Lower bounds for stochastic minimization of exp-concave functions have been studied in [16].",
      "startOffset" : 121,
      "endOffset" : 125
    }, {
      "referenceID" : 22,
      "context" : "For the special case of linear regression with X “ tx P R : }x}2 ď 1u, W “ tw P R : }w}2 ď Bu, Y “ r ́Y, Y s (13) [23] proved the lower bound Ω ́",
      "startOffset" : 114,
      "endOffset" : 118
    }, {
      "referenceID" : 0,
      "context" : "The middle term is attained by combining the Vovk-Azoury-Warmuth forecaster ([1, 24]) with standard online-to-batch conversions ([6]).",
      "startOffset" : 77,
      "endOffset" : 84
    }, {
      "referenceID" : 23,
      "context" : "The middle term is attained by combining the Vovk-Azoury-Warmuth forecaster ([1, 24]) with standard online-to-batch conversions ([6]).",
      "startOffset" : 77,
      "endOffset" : 84
    }, {
      "referenceID" : 5,
      "context" : "The middle term is attained by combining the Vovk-Azoury-Warmuth forecaster ([1, 24]) with standard online-to-batch conversions ([6]).",
      "startOffset" : 129,
      "endOffset" : 132
    }, {
      "referenceID" : 10,
      "context" : "Last, the right term is attained by ERM, as implied by the aforementioned upper bound of [11].",
      "startOffset" : 89,
      "endOffset" : 93
    }, {
      "referenceID" : 22,
      "context" : "However, since in the setting of [23] (Equation (13)) the magnitude of the predictions is not uniformly upper bounded by Y , no contradiction arises.",
      "startOffset" : 33,
      "endOffset" : 37
    }, {
      "referenceID" : 3,
      "context" : "4 Stability and Regularization Previous work ([4, 22]) studied the rate of uniform stability in various settings.",
      "startOffset" : 46,
      "endOffset" : 53
    }, {
      "referenceID" : 21,
      "context" : "4 Stability and Regularization Previous work ([4, 22]) studied the rate of uniform stability in various settings.",
      "startOffset" : 46,
      "endOffset" : 53
    }, {
      "referenceID" : 20,
      "context" : "However, as explained in [21][Section 13.",
      "startOffset" : 25,
      "endOffset" : 29
    }, {
      "referenceID" : 9,
      "context" : "Indeed, the Online Newton Step (ONS) of [10], which has been designed for online minimization Namely, when tuning λ, we need to ensure that any ǫ{2-approximate minimizer with respect to the regularized objective is also an ǫ-approximate minimizer with respect to the unregularized objective.",
      "startOffset" : 40,
      "endOffset" : 44
    }, {
      "referenceID" : 20,
      "context" : "As explained in [21][Section 13.",
      "startOffset" : 16,
      "endOffset" : 20
    }, {
      "referenceID" : 9,
      "context" : "of exp-concave functions, achieves improved (logarithmic) regret bounds that resemble the regret bounds for strongly convex functions ([10]).",
      "startOffset" : 135,
      "endOffset" : 139
    }, {
      "referenceID" : 15,
      "context" : "The online-to-batch analysis of [16] yields a bound on the excess risk that coincides with our bounds up to logarithmic factors.",
      "startOffset" : 32,
      "endOffset" : 36
    }, {
      "referenceID" : 14,
      "context" : "This question was answered affirmatively by [15].",
      "startOffset" : 44,
      "endOffset" : 48
    }, {
      "referenceID" : 14,
      "context" : "While exp-concavity is weaker than strong convexity, [15][section 4.",
      "startOffset" : 53,
      "endOffset" : 57
    }, {
      "referenceID" : 14,
      "context" : "The above interpretation of [15] inspired us to make one step forward and directly show that regularization is not required as long as a related (preconditioned) problem is well conditioned.",
      "startOffset" : 28,
      "endOffset" : 32
    }, {
      "referenceID" : 14,
      "context" : "The upper bound of [15] on the excess risk scales with 24βd ᾱn “ 24βdρ αn (recall that the exp-concavity parameter ᾱ is equal to α{ρ2).",
      "startOffset" : 19,
      "endOffset" : 23
    }, {
      "referenceID" : 14,
      "context" : "This resolves the question raised by [15] regarding the necessity of the smoothness assumption.",
      "startOffset" : 37,
      "endOffset" : 41
    }, {
      "referenceID" : 14,
      "context" : "Note that for linear regression, the smoothness is 1, making our bounds identical to the bounds of [15] for this special case.",
      "startOffset" : 99,
      "endOffset" : 103
    }, {
      "referenceID" : 14,
      "context" : "As discussed in [15], it is difficult to translate bounds on the average stability into high-probability bounds (while preserving the fast rate and introducing only logarithmic dependence on 1{δ).",
      "startOffset" : 16,
      "endOffset" : 20
    }, {
      "referenceID" : 16,
      "context" : "A recent follow-up work by [17] established a bound of Õpd logpnq ` logp1{δq{nq on the excess risk of ERM, where δ is the confidence parameter.",
      "startOffset" : 27,
      "endOffset" : 31
    }, {
      "referenceID" : 1,
      "context" : "3 in [2]).",
      "startOffset" : 5,
      "endOffset" : 8
    }, {
      "referenceID" : 13,
      "context" : ", using [14]) which coincide with our bounds.",
      "startOffset" : 8,
      "endOffset" : 12
    } ],
    "year" : 2017,
    "abstractText" : "We show that the average stability notion introduced by [12, 4] is invariant to data preconditioning, for a wide class of generalized linear models that includes most of the known exp-concave losses. In other words, when analyzing the stability rate of a given algorithm, we may assume the optimal preconditioning of the data. This implies that, at least from a statistical perspective, explicit regularization is not required in order to compensate for ill-conditioned data, which stands in contrast to a widely common approach that includes a regularization for analyzing the sample complexity of generalized linear models. Several important implications of our findings include: a) We demonstrate that the excess risk of empirical risk minimization (ERM) is controlled by the preconditioned stability rate. This immediately yields a relatively short and elegant proof for the fast rates attained by ERM in our context. b) We strengthen the recent bounds of [9] on the stability rate of the Stochastic Gradient Descent algorithm.",
    "creator" : "LaTeX with hyperref package"
  }
}