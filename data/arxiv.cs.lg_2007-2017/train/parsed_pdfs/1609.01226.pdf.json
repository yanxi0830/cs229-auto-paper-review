{
  "name" : "1609.01226.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "The Robustness of Estimator Composition",
    "authors" : [ "Pingfan Tang", "Jeff M. Phillips" ],
    "emails" : [ "tang1984@cs.utah.edu", "jeffp@cs.utah.edu" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Robust statistical estimators [5, 7] (in particular, resistant estimators), such as the median, are an essential tool in data analysis since they are provably immune to outliers. Given data with a large fraction of extreme outliers, a robust estimator guarantees the returned value is still within the nonoutlier part of the data. In particular, the roll of these estimators is quickly growing in importance as the scale and automation associated with data collection and data processing becomes more commonplace. Artisanal data (hand crafted and carefully curated), where potential outliers can be removed, is becoming proportionally less common. Instead, important decisions are being made blindly based on the output of analysis functions, often without looking at individual data points and their effect on the outcome. Thus using estimators as part of this pipeline that are not robust are susceptible to erroneous and dangerous decisions as the result of a few extreme and rogue data points.\nAlthough other approaches like regularization and pruning a constant number of obvious outliers are common as well, they do not come with the important guarantees that ensure these unwanted outcomes absolutely cannot occur.\nIn this paper we initiate the formal study of the robustness of composition of estimators through the notion of breakdown points. These are especially important with the growth of data analysis pipelines where the final result or prediction is the result of several layers of data processing. When each layer in this pipeline is modeled as an estimator, then our analysis provides the first general robustness analysis of these processes.\nThe breakdown point [4, 3] is a basic measure of robustness of an estimator. Intuitively, it describes how many outliers can be in the data without the estimator becoming unreliable. However, the literature is full of slightly inconsistent and informal definitions of this concept. For example:\n29th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain.\nar X\niv :1\n60 9.\n01 22\n6v 1\n[ cs\n.L G\n] 5\nS ep\n2 01\n• Aloupis [1] write “the breakdown point is the proportion of data which must be moved to infinity so that the estimator will do the same.”\n• Huber and Ronchetti [8] write “the breakdown point is the smallest fraction of bad observations that may cause an estimator to take on arbitrarily large aberrant values.\"\n• Dasgupta, Kumar, and Srikumar [14] write “the breakdown point of an estimator is the largest fraction of the data that can be moved arbitrarily without perturbing the estimator to the boundary of the parameter space.”\nAll of these definitions have similar meanings, and they are typically sufficient for the purpose of understanding a single estimator. However, they are not mathematically rigorous, and it is difficult to use them to discuss the breakdown point of composite estimators.\nComposition of Estimators. In a bit more detail (we give formal definitions in Section 2.1), an estimator E maps a data set to single value in another space, sometimes the same as a single data point. For instance the mean or the median are simple estimators on one-dimensional data. A composite E1-E2 estimator applies two estimators E1 and E2 on data stored in a hierarchy. Let P = {P1, P2, . . . , Pn} be a set of subdata sets, where each subdata set Pi = {pi,1, pi,2, . . . , pi,k} has individual data readings. Then the E1-E2 estimator reports E2(E1(P1), E1(P2), . . . , E1(Pn)), that is the estimator E2 applied to the output of estimator E1 on each subdata set."
    }, {
      "heading" : "1.1 Examples of Estimator Composition",
      "text" : "Composite estimators arise in many scenarios in data analysis.\nUncertain Data. For instance, in the last decade there has been increased focus on the study of uncertainty data [11, 9, 2] where instead of analyzing a data set, we are given a model of the uncertainty of each data point. Consider tracking the summarization of a group of n people based on noisy GPS measurements. For each person i we might get k readings of their location Pi, and use these k readings as a discrete probability distribution of where that person might be. Then in order to represent the center of this set of people a natural thing to do would be to estimate the location of each person as xi ← E1(Pi), and then use these estimates to summarize the entire group E2(x1, x2, . . . , xn). Using the mean as E1 and E2 would be easy, but would be susceptible to even a single outrageous outlier (all people are in Manhattan, but a spurious reading was at (0, 0) lat-long, off the coast of Africa). An alternative is to use the L1-median for E1 and E2, that is known to have an optimal breakdown point of 0.5. But what is the breakdown point of the E1-E2 estimator?\nRobust Analysis of Bursty Behavior. Understanding the robustness of estimators can also be critical towards how much one can “game” a system. For instance, consider a start-up media website that gets bursts of traffic from memes they curate. They publish a statistic showing the median of the top half of traffic days each month, and aggregate these by taking the median of such values over the top half of all months. This is a composite estimator, and they proudly claim, even through they have bursty traffic, it is robust (each estimator has a breakdown point of 0.25). If this composite estimator shows large traffic, should a potential buyer of this website by impressed? Is there a better, more robust estimator the potential buyer could request? If the media website can stagger the release of its content, how should they distribute it to maximize this composite estimator?\nPart of the Data Analysis Pipeline. This process of estimator composition is very common in broad data analysis literature. This arises from the idea of an “analysis pipeline” where at several stages estimators or analysis is performed on data, and then further estimators and analysis are performed downstream. In many cases a robust estimator like the median is used, specifically for its robustness properties, but there is no analysis of how robust the composition of these estimators is."
    }, {
      "heading" : "1.2 Main Results",
      "text" : "This paper initiates the formal and general study of the robustness of composite estimators.\n• In Subsection 2.1, we give two formal definitions of breakdown points which are both required to prove composition theorem. One variant of the definition closely aligns with other formalizations [4, 3], while another is fundamentally different.\n• The main result provides general conditions under which an E1-E2 estimator with breakdown points β1 and β2, has a breakdown point of β1β2 (Theorem 2 in Subsection 2.2).\n• Moreover, by showing examples where our conditions do not strictly apply, we gain an understanding of how to circumvent the above result. An example is in composite percentile estimators (e.g., E1 returns the 25th percentile, and E2 the 75th percentile of a ranked set). These composite estimators have larger breakdown point than β1 · β2. • The main result can extended to multiple compositions, under suitable conditions, so for\ninstance anE1-E2-E3 estimator has a breakdown point of β1β2β3 (Theorem 3 in Subsection 2.3). This implies that long analysis chains can be very suspect to a few carefully places outliers since the breakdown point decays exponentially in the length of the analysis chain.\n• In Section 3, we highlight several applications of this theory, including robust regression, robustness of p-values, a depth-3 composition, and how to advantageously manipulate the observation about percentile estimator composition. We demonstrate a few more applications with simulations in Section 4."
    }, {
      "heading" : "2 Robustness of Estimator Composition",
      "text" : ""
    }, {
      "heading" : "2.1 Formal Definitions of Breakdown Points",
      "text" : "In this paper, we give two definitions for the breakdown point: Asymptotic Breakdown Point and Asymptotic Onto-Breakdown Point. The first definition, Asymptotic Breakdown Point, is similar to the classic formal definitions in [4] and [3] (including their highly technical nature), although their definitions of the estimator are slightly different leading to some minor differences in special cases. However our second definition, Asymptotic Onto-Breakdown Point, is a structurally new definition, and we illustrate how it can result in significantly different values on some common and useful estimators. Our main theorem will require both definitions, and the differences in performance will lead to several new applications and insights.\nWe define an estimator E as a function from the collection of some finite subsets of a metric space (X , d) to another metric space (X ′, d′):\nE : A ⊂ {X ⊂X | 0 < |X| <∞} 7→X ′, (1)\nwhere X is a multiset. This means if x ∈ X then x can appear more than once in X , and the multiplicity of elements will be considered when we compute |X|.\nFinite Sample Breakdown Point. For estimator E defined in (1) and positive integer n we define its finite sample breakdown point gE(n) over a set M as\ngE(n) = { max(M) if M 6= ∅ 0 if M = ∅ (2)\nwhere for ρ(x′, X) = maxx∈X d(x′, x) is the distance from x′ to the furthest point in X ,\nM = {m ∈ [0, n] | ∀X ∈ A , |X| = n, ∀ G1 > 0,∃ G2 = G2(X,G1) s.t. ∀X ′ ∈ A , if |X ′| = n and |{x′ ∈ X ′ | ρ(x′, X) > G1}| ≤ m then d′(E(X), E(X ′)) ≤ G2}.\n(3)\nFor an estimator E in (1) and X ∈ A , the finite sample breakdown point gE(n) means if the number of unbounded points in X ′ is at most gE(n), then E(X ′) will be bounded. Lets break this definition down a bit more. The definition holds over all data sets X ∈ A of size n, and for all values G1 > 0 and some value G2 defined as a function G2(X,G1) of the data set X and value G1. Then gE(n) is the maximum value m (over all X , G1, and G2 above) such that for all X ′ ∈ A with |X ′| = n then |{x′ ∈ X ′ | ρ(x′, X) > G1}| ≤ m (that is at most m points are further than G1 from X) where the estimators are close, d′(E(X), E(X ′)) ≤ G2. For example, consider a point set X = {0, 0.15, 0.2, 0.25, 0.4, 0.55, 0.6, 0.65, 0.72, 0.8, 1.0} with n = 11 and median 0.55. If we set G1 = 3, then we can consider sets X ′ of size 11 with fewer than m points that are either greater than 3 or less than −2. This means in X ′ there are at most m points which are greater than 3 or less than −2, and all other n−m points are in [−2, 3]. Under these conditions, we can (conservatively) set G2 = 4, and know that for values of m as 1, 2, 3, 4, or 5, then the median of X ′ must be between −3.45 and 4.55; and this holds no matter where we set those m points (e.g., at 20 or at 1000). This does not hold for m ≥ 6, so gE(11) = 5.\nAsymptotic Breakdown Point. If the limit limn→∞ gE(n)n exists, then we define this limit\nβ = lim n→∞\ngE(n)\nn (4)\nas the asymptotic breakdown point, or breakdown point for short, of the estimator E. Remark 1. It is not hard to see that many common estimators satisfy the conditions. For example, the median, L1-median [1], and Siegel estimators [12] all have asymptotic breakdown points of 0.5.\nAsymptotic Onto-Breakdown Point. For an estimator E given in (1) and positive integer n, if\nM̃ = {0 ≤ m ≤ n | ∀ X ∈ A , |X| = n, ∀ y ∈X ′, ∃ X ′ ∈ A s.t. |X ′| = n, |X ∩X ′| = n−m,E(X ′) = y}\nis not empty, we define fE(n) = min(M̃). (5)\nThe definition of fE(n) implies, if we change fE(n) elements in X , we can make E become any value in X ′: it is onto. In contrast gE(n) only requires E(X ′) to become far from E(X), perhaps only in one direction. Then the asymptotic onto-breakdown point is defined as the following limit if it exists\nlim n→∞\nfE(n)\nn . (6)\nRemark 2. For a quantile estimator E that returns a percentile other than the 50th, then limn→∞ gE(n) n 6= limn→∞ fE(n) n . For instance, if E returns the 25th percentile of a ranked set, setting only 25% of the data points to −∞ causes E to return −∞; hence limn→∞ gE(n)n = 0.25. And while any value less than the original 25th percentile can also be obtained; to return a value larger than the largest element in the original set, at least 75% of the data must be modified, thus limn→∞ fE(n) n = 0.75.\nAs we will observe in Section 3, this nuance in definition regarding percentile estimators will allow for some interesting composite estimator design.\n2.2 Definition of E1-E2 Estimators, and their Robustness We consider the following two estimators:\nE1 : A1 ⊂ {X ⊂X1 | 0 < |X| <∞} 7→X2, (7) E2 : A2 ⊂ {X ⊂X2 | 0 < |X| <∞} 7→X ′2 , (8)\nwhere any finite subset of E1(A1), the range of E1, belongs to A2. Suppose Pi ∈ A1, |Pi| = k for i = 1, 2, · · · , n and Pflat = ]ni=1Pi, where ] means if x appears n1 times in X1 and n2 times in X2 then x appears n1 + n2 times in X1 ]X2. We define\nE(Pflat) = E2 (E1(P1), E1(P2), · · · , E1(Pn)) . (9)\nTheorem 1. Suppose gE1(k) and gE2(n) are the finite sample breakdown points of estimators E1 and E2 which are given by (7) and (8) respectively. If gE(nk) is the finite sample breakdown point of E given by (9), then we have gE2(n)gE1(k) ≤ gE(nk). (10) and if\nβ1 = lim k→∞\ngE1(k)\nk , β2 = lim n→∞\ngE2(n)\nn , β = lim n,k→∞\ngE(nk)\nnk\nand all exist, then β1β2 ≤ β. (11)\nProof. For any fixed G1 > 0, and any subsets P ′1, P ′ 2, · · · , P ′n ∈ A1 satisfying |P ′1| = |P ′2| = · · · = |P ′n| = k, and |{p′ ∈ P ′flat| ρ(p′, Pflat) > G1}| ≤ gE2(n)gE1(k) (12) where P ′flat = ]ni=1P ′i , we introduce the notation\nX = {E1(P1), E1(P2), · · · , E1(Pn)}, X ′ = {E1(P ′1), E1(P ′2), · · · , E1(P ′n)}.\nSo, in order to prove (10), we only need to bound E(P ′flat).\nWe define I1 = {1 ≤ i ≤ n| |{p′ ∈ P ′i | ρ(p′, Pi) > G1}| > gE1(k)} (13)\nand then have |I1| ≤ gE2(n). (14)\nOtherwise, since ρ(p′, Pi) > G1 implies ρ(p′, Pflat) > G1, from |I1| > gE2(n) and (13) we can obtain |{p′ ∈ P ′flat| ρ(p′, Pflat) > G1}| > gE2(n)gE1(k) which is contradictory to (12).\nFor any i /∈ I1, we have |{p′ ∈ P ′i | ρ(p′, Pi) > G1}| ≤ gE1(k), so, from the definition of gE1(k) we know ∃ Gi2 = Gi2(Pi, G1), s.t. d2(E1(P ′i ), E1(Pi)) ≤ Gi2 ∀ i /∈ I1. where d2 is the metric of space X2. Let\nG2 = max i/∈I1 Gi2 + max 1≤i,j≤n d2(E1(Pi), E1(Pj))\nthen we have ρ(E1(P ′ i ), X) ≤ G2,∀ i /∈ I1. (15)\nDefining I2 = {1 ≤ i ≤ n | ρ(E1(P ′i ), X) > G2} from (15) we have I2 ⊂ I1, which implies |I2| ≤ |I1| ≤ gE2(n) by (14). Therefore, from the definition of gE2(n), we have\n∃ G3 = G3(X,G2) s.t. ‖E(P ′flat)− E(Pflat)‖ = ‖E2(X ′)− E2(X)‖ ≤ G3,\nwhich implies (10), and (11) can be obtained from (10) directly. Thus, the proof is completed.\nRemark 3. Under the condition of Theorem 1, we cannot guarantee β = β1β2. For example, suppose E1 andE2 take the 25th percentile and the 75th percentile of a ranked set of real numbers respectively. So, we have β1 = β2 = 14 . However, β = 1 4 · 3 4 = 3 16 .\nIn fact, the limit of gE(nk)nk as n, k →∞ may even not exist. For example, suppose E1 takes the 25th percentile of a ranked set of real numbers. When n is odd E2 takes the the 25th percentile of a ranked set of n real numbers, and when n is even E2 takes the the 75th percentile of a ranked set of n real numbers. Thus, β1 = β2 = 14 , but gE(nk) ≈ 1 4nk if n is odd, and gE(nk) ≈ 1 4 · 3 4nk if n is even, which implies limn,k→∞ gE(nk)\nnk does not exist.\nTherefore, to guarantee β exist and β = β1β2, we introduce the definition of asymptotic ontobreakdown point in (6). As shown in Remark 2, the values of (4) and (6) may be not equal. However, with the condition of the asymptotic breakdown point and asymptotic onto-breakdown point of E1 being the same, we can finally state our desired clean result.\nTheorem 2. For estimators E1, E2 and E given by (7), (8) and (9) respectively, suppose gE1(k), gE2(n) and gE(nk) are defined by (2), and fE1(k) is defined by (5). Moreover, E1 is an onto function and for any fixed positive integer n we have\n∃ X ∈ A2, |X| = n,G1 > 0, s.t. ∀ G2 > 0,∃ X ′ ∈ A2 satisfying |X ′| = n, |X ′ \\X| = gE2(n) + 1, and d′2(E2(X), E2(X ′)) > G2.\n(16)\nwhere d′2 is the metric of space X ′ 2 .\nIf\nβ1 = lim k→∞\ngE1(k)\nk = lim k→∞\nfE1(k)\nk , and β2 = lim n→∞\ngE2(n)\nn (17)\nboth exist, then\nβ = lim n,k→∞\ngE(nk)\nnk exists and β = β1β2. (18)\nProof. For any fixed positive integer n, we can find X = {x1, x2, · · · , xn} ∈ A2, and G1 > 0 satisfying (16). Since E1 is an onto function, we can find Pflat = ]ni=1Pi such that Pi ∈ A1 and E1(Pi) = xi for all 1 ≤ i ≤ n. From (16), we know for anyG2 > 0, we can findX ′ ∈ A2 such that |X ′| = n, |X ′\\X| = gE2(n)+1 and\nd′(E2(X), E2(X ′)) > G2.\nThis implies the number of different elements betweenX andX ′ is gE2(n)+1. For any x ′ i ∈ X ′ \\X , we can find P ′i ∈ A1 such that |P ′i | = k, E1(P ′i ) = x′i and |P ′i \\ Pi| = fE1(k). So, we only need to change fE1(k)(gE2(n) + 1) points of Pflat, and then we can obtain P ′ flat such that |P ′flat \\ Pflat| = fE1(k)(gE2(n) + 1) and d ′(E(Pflat), E(P ′ flat)) > G2. This implies\ngE(nk) ≤ fE1(k)(gE2(n) + 1). (19)\nTherefore, from Theorem 1 and (19) we have\ngE1(k)\nk\ngE2(n) n ≤ gE(nk) nk ≤ fE1(k) k (gE2(n) + 1) n . (20)\nLetting n and k go to infinity in (20), we obtain (18) from (17). Thus, the proof of this theorem is completed.\nRemark 4. Without the introduction of fE(n), we cannot even guarantee β ≤ β1 or β ≤ β2 only under the condition of Theorem 1, even if E1 and E2 are both onto functions. For example, for any P = {p1, p2, · · · , pk} ⊂ R and X = {x1, x2, · · · , xn} ⊂ R, we define E1(P ) = 1/median(P ) (if median(P ) 6= 0, otherwise define E1(P ) = 0) and E2(X) = median(y1, y2, · · · , yn), where yi (1 ≤ y ≤ n) is given by yi = 1/xi (if xi 6= 0, otherwise define yi = 0). Since gE1(k) = gE2(n) = 0 for all n, k, we have β1 = β2 = 0. However, in order to make E2(E1(P1), E1(P2), · · · , E1(Pn))→ +∞, we need to make about n2 elements in {E(P1), E(P2), · · · , E(Pn)} go to 0+. To make E1(Pi)→ 0+, we need to make about k2 points in Pi go to +∞. Therefore, we have gE(nk) ≈ n 2 · k 2 and β = 14 ."
    }, {
      "heading" : "2.3 Multi-level Composition of Estimators",
      "text" : "To study the breakdown point of composite estimators with more than two levels, we introduce the following estimator:\nE3 : A3 ⊂ {X ⊂X ′2 | 0 < |X| <∞} 7→X ′3 , (21)\nwhere any finite subset of E2(A2), the range of E2, belongs to A3. Suppose Pi,j ∈ A1, |Pi,j | = k for i = 1, 2, · · · , n, j = 1, 2, · · · ,m and P jflat = ] n i=1Pi,j , Pflat = ]mj=1P j flat. We define\nE(Pflat) = E3 ( E2(P̃ 1 flat), E2(P̃ 2 flat), · · · , E2(P̃mflat) ) , (22)\nwhere P̃ jflat = {E1(P1,j), E1(P2,j), · · · , E1(Pn,j)}, for j = 1, 2, · · · ,m. From Theorem 2, we can obtain the following theorem about the breakdown point of E in (22). Theorem 3. For estimators E1, E2, E3 and E given by (7), (8), (21) and (22) respectively, suppose gE1(k), gE2(n), gE3(m) and gE(mnk) are defined by (2), and fE1(k), fE2(n) are defined by (5). Moreover, E1 and E2 are both onto functions, and for any fixed positive integer m we have\n∃ X ∈ A3, |X| = m,G1 > 0, s.t. ∀ G2 > 0,∃ X ′ ∈ A3 satisfying |X ′| = m, |X ′ \\X| = gE3(m) + 1, and d′3(E3(X), E3(X ′)) > G2.\nwhere d′3 is the metric of space X ′ 3 . If\nβ1 = lim k→∞\ngE1(k)\nk = lim k→∞\nfE1(k)\nk , β2 = lim n→∞\ngE2(n)\nn = lim n→∞\nfE2(n)\nn , (23)\nand β3 = limm→∞ gE3 (m)\nm all exist, then\nβ = lim m,n,k→∞\ngE(mnk)\nmnk exist and β = β1β2β3. (24)\nProof. We define an estimator Ẽ:\nẼ(P̃ jflat) = E2(E1(P1,j), E1(P2,j), · · · , E1(Pn,j))\nfor j = 1, 2, · · · ,m, and first prove the breakdown point of Ẽ is β̃ = β1β2. For any fixed y ∈ X ′2 and X = {E1(P1), E1(P2), · · · , E1(Pn)}, we can find X ′ ∈ A2 such that |X ′| = n, |X∩X ′| = n−fE2(n) andE2(X ′) = y. For any element y′ ∈ X ′\\(X∩X ′), we can find E1(Pi) ∈ X \\ (X ∩X ′) and P ′i ∈ A1 such that |P ′i | = k, |Pi ∩P ′i | = k− gE1(k) and E1(P ′i ) = y′. This implies we can find a set P ′flat ⊂X1 such that |P ′flat| = nk, |Pflat ∩ P ′flat| = nk − fE2(n)fE1(k) and Ẽ(P ′flat) = y, i.e. we only need to change fE2(n)fE1(k) points in Pflat, and Ẽ can become any value. So, we have\nfẼ(nk) ≤ fE2(n)fE1(k). (25)\nApplying Theorem 1 to E1 and E2, we obtain\ngE2(n)gE1(k) ≤ gẼ(nk). (26)\nSince gẼ(nk) < fẼ(nk), from (25) and (26), we have\ngE2(n)\nn\ngE1(k) k ≤ gẼ(nk) nk < fẼ(nk) nk ≤ fE2(n) n fE1(k) k . (27)\nLetting n, k go to infinity in (27), from (23) we obtain the breakdown point of Ẽ is\nβ̃ = lim n,k→∞\ngẼ(nk)\nnk = lim n,k→∞\nfẼ(nk)\nnk = β1β2.\nSince E(Pflat) = E3(Ẽ(P̃ 1flat), Ẽ(P̃ 2 flat), · · · , Ẽ(P̃mflat)), we apply Theorem 2 to Ẽ and E3, and then obtain (24)."
    }, {
      "heading" : "3 Applications",
      "text" : "We next discuss several applications of our main theorems and observations. Applications 2 and 4 are direct applications of the easy to use theorems. Applications 1 and 3 take advantage of some of the nuances in definition, in particular the unexpected robustness of composing quantile estimators."
    }, {
      "heading" : "3.1 Application 1 : Balancing Percentiles",
      "text" : "For n companies, for simplicity, assume each company has k employees. We are interested in the income of the regular employees of all companies, not the executives who may have exorbitant pay. Let pi,j represents the income of the jth employee in the ith company. Set Pflat = ]ni=1Pi where the ith company has a set Pi = {pi,1, pi,2, · · · , pi,k} ⊂ R and for notational convenience pi,1 ≤ pi,2 ≤ · · · ≤ pi,k for i ∈ {1, 2, · · · , n}. Suppose the income data Pi of each company is preprocessed by a 45-percentile estimator E1 (median of lowest 90% of incomes), with breakdown point β1 = 0.45. In theory E1(Pi) can better reflect the income of regular employees in a company, since there may be about 10% of employees in the management of a company and their incomes are usually much higher than that of common employees. So, the preprocessed data is X = {E1(P1), E1(P2), · · · , E1(Pn)}. If we define E2(X) = median(X) and E(Pflat) = E2(X), then the breakdown point of E2 is β2 = 0.5, and the breakdown points of E is β = β1β2 = 0.225.\nHowever, if we use another E2, then E can be more robust. For example, for X = {x1, x2, · · · , xn} where x1 ≤ x2 ≤ · · · ≤ xn, we can define E2 as the 55-percentile estimator (median of largest 90% of incomes). In order to make E(Pflat) = E2(X) = E2(E1(P1), E1(P2), · · · , E1(Pn)) go to infinity, we need to either move 55% points of X to −∞ or move 45% points of X to +∞. In either case, we need to move about 0.45 · 0.55nk points of Pflat to infinity. This means the breakdown point of E is β = 0.45 · 0.55 = 0.2475 which is greater than 0.225. This example implies if we know how the raw data is preprocessed by estimator E1, we can choose a proper estimator E2 to make the E1-E2 estimator more robust."
    }, {
      "heading" : "3.2 Application 2 : Regression of L1 Medians",
      "text" : "Suppose we want to use linear regression to robustly predict the weight of a person from his or her height, and we have multiple readings of each person’s height and weight. The raw data is Pflat = ]ni=1Pi where for the ith person we have a set Pi = {pi,1, pi,2, · · · , pi,k} ⊂ R2 and pi,j = (xi,j , yi,j) for i ∈ {1, 2, · · · , n}, j ∈ {1, 2, · · · , k}. Here, xi,j and yi,j are the height and weight respectively of the ith person in their jth measurement.\nOne “robust” way to process this data, is to first pre-process each Pi with its L1-median [1]: (x̄i, ȳi)← E1(Pi), where E1(Pi) = L1-median(Pi) has breakdown point β1 = 0.5. Then we could generate a linear model to predict weight ŷi = ax+b from the Siegel Estimator [12]: E2(Z) = (a, b), with breakdown point β2 = 0.5. From Theorem 2 we immediately know the breakdown point of E(Pflat) = E2(E1(P1), E1(P2), · · · , E1(Pn)) is β = β1β2 = 0.5 · 0.5 = 0.25. Alternatively, taking the Siegel estimator of Pflat (i.e., returning E2(Pflat)) would have a much larger breakdown point of 0.5. So a seemingly harmless operation of normalizing the data with a robust estimator (with optimal 0.5 breakdown point) drastically decreases the robustness of the process."
    }, {
      "heading" : "3.3 Application 3 : Significance Thresholds",
      "text" : "Suppose we are studying the distribution of the wingspread of fruit flies. There are n = 500 flies, and the variance of the true wingspread among these flies is on the order of 0.1 units. Our goal is to estimate the 0.05 significance level of this distribution of wingspread among normal flies.\nTo obtain a measured value of the wingspread of the ith fly, denoted Fi, we measure the wingspread of ith fly k = 100 times independently, and obtain the measurement set Pi = {pi,1, pi,2, · · · , pi,k}. The measurement is carried out by a machine automatically and quickly, which implies the variance of each Pi is typically very small, perhaps only 0.0001 units, but there are outliers in Pi with small chance due to possible machine malfunction. This malfunction may be correlated to individual flies because of anatomical issues, or it may have autocorrelation (the machine jams for a series of consecutive measurements).\nTo perform hypothesis testing we desire the 0.05 significance level, so we are interested in the 95th percentile of the set F = {F1, F2, · · · , Fn}. So a post processing estimator E2 returns the 95th percentile of F and has a breakdown point of β2 = 0.05 [6]. Now, we need to design an estimator E1 to process the raw data Pflat = ]ni=1Pi to obtain F = {F1, F2, · · · , Fn}. For example, we can defineE1 as Fi = E1(Pi) = median(Pi) and estimator E as E(Pflat) = E2(E1(P1), E1(P2), · · · , E1(Pn)). Then, the breakdown point ofE1 is 0.5. Since the breakdown point ofE2 is 0.05, the breakdown point of the composite estimator E is β = β1β2 = 0.5 · 0.05 = 0.025. This means if the measurement machine malfunctioned only 2.5% of the time, we could have an anomalous significant level, leading to false discovery. Can we make this process more robust by adjusting E1?\nActually, yes!, we can use another pre-processing estimator to get a more robust E. Since the variance of each Pi is only 0.0001, we can let E1 return the 5th percentile of a ranked set of real numbers, then there is not much difference between E1(Pi) and the median of Pi. (Note: this introduces a small amount of bias that can likely be accounted for in other ways.) In order to make E(Pflat) = E2(F ) go to infinity we need to move 5% points of X to −∞ (causing E2 to give an anomalous value) or 95% points of X to +∞ (causing many, 95%, of the E1 values, to give anomalous values). In either case, we need to move about 5% · 95% points of Pflat to infinity. So, the breakdown points of E is β = 0.05 · 0.95 = 0.0475 which is greater than 0.025. That is, we can now sustain up to 4.75% of the measurement machine’s reading to be anomalous, almost double than before, without leading to an anomalous significance threshold value.\nThis example implies if we know the post-processing estimator E2, we can choose a proper method to preprocess the raw data to make the E1-E2 estimator more robust.\nRemark 5. A further study would be required to use such a composite estimator in practice due some bias it introduces. To replicate the normalization process on new experimental data (e.g., on a new species with hypothesized long wingspread), we would probably need to make one of the following adjustments to the standard process of measuring the wingspread of the new species and directly comparing it to the significance threshold. (a) Also consider the 5th percentile of the experimental measurements (with breakdown point 0.05 instead of 0.5). (b) Adjust the significance level by\nroughly 0.0001 units (the variance over Pi) making it conservative with respect to the 5th percentile versus the 50th percentile decision of each fly’s measurements, so the 50th percentile could be used on the new experimental data. Or, (c) use a different percentile (say the (95 + ε)th percentile instead of 95th) to balance the bias in using the 5th percentile of measurements. In the specific scenario we describe, we believe option (b) may be a very acceptable option with little lack in precision (due to difference in variance 0.1 and 0.0001) but with large gain in robustness."
    }, {
      "heading" : "3.4 Application 4 : 3-Level Composition",
      "text" : "Suppose we want to use a single value to represent the temperature of the US in a certain day. There are m = 50 states in the country. Suppose each state has n = 100 meteorological stations, and the station i in state j measures the local temperature k = 24 times to get the data Pi,j = {ti,j,1, ti,j,2, · · · , ti,j,k}. We define P jflat = ] n i=1Pi,j , Pflat = ]mj=1P j flat and\nE1(Pi,j) = median(Pi,j), E2(P j flat) = median (E1(P1,j), E1(P1,j), · · · , E1(Pn,j))\nE(Pflat) = E3(E2(P 1 flat), E2(P 2 flat), · · · , E2(Pmflat)) = median(E2(P 1flat), E2(P 2flat), · · · , E2(Pmflat)).\nSo, the break down points of E1, E2 and E3 are β1 = β2 = β3 = 0.5. From Theorem 3, we know the break down point of E is β = β1β2β3 = 0.125. Therefore, we know the estimator E is not very robust, and it may be not a good choice to use E(Pflat) to represent the temperature of the US in a certain day.\nThis example illustrates how the more times the raw data is aggregated, the more unreliable the final result can become."
    }, {
      "heading" : "4 Simulations",
      "text" : "We next describe a few more scenarios where our new theory on estimator composition is relevant. For these we simulate a couple of data sets to demonstrate how one might construct interesting algorithms from these ideas."
    }, {
      "heading" : "4.1 Simulation 1 : Estimator Manipulation",
      "text" : "In this simulation we actually construct a method to relocate an estimator by modifying the smallest number of points possible. We specifically target the L1-median of L1-medians since its somewhat non-trivial to solve for the new location of data points.\nIn particular, given a target point p0 ∈ R2 and a set of nk points Pflat = ]ni=1Pi, where Pi = {pi,1, pi,2, · · · , pi,k} ⊂ R2, we use simulation to show that we only need to change ñk̃ points of Pflat, then we can get a new set P̃flat = ]ni=1P̃i such that median(median(P̃1),median(P̃2), · · · ,median(P̃n)) = p0. Here, the \"median\" means L1-median, and\nñ = { 1 2n if n is even 1 2 (n+ 1) if n is odd , k̃ = { 1 2k if k is even 1 2 (k + 1) if k is odd .\nTo do this, we first show that, given k points S = {(xi, yi) | 1 ≤ i ≤ k} in R2, and a target point (x0, y0), we can change k̃ points of S to make (x0, y0) as the L1-median of the new set. As n and k grow, then ñk̃/(nk) = 0.25 is the asymptotic breakdown point of this estimator, as a consequence of Theorem 2, and thus we may need to move this many points to get the result.\nIf (x0, y0) is the L1-median of the set {(xi, yi) | 1 ≤ i ≤ k}, then we have [13]: k∑\ni=1\nxi − x0√ (xi − x0)2 + (yi − y0)2 = 0, k∑ i=1 yi − y0√ (xi − x0)2 + (yi − y0)2 = 0. (28)\nWe define ~x = (x1, x2, · · · , xk̃), ~y = (y1, y2, · · · , yk̃) and\nh(~x, ~y) =\n( k∑\ni=1\nxi − x0√ (xi − x0)2 + (yi − y0)2\n)2 + ( k∑\ni=1\nyi − y0√ (xi − x0)2 + (yi − y0)2\n)2 ."
    }, {
      "heading" : "5 8 3 4 (10.7631, 11.0663) (10.7025 11.0623)",
      "text" : ""
    }, {
      "heading" : "10 5 5 3 (-13.8252, -4.7462) (-13.8330, -4.7482)",
      "text" : ""
    }, {
      "heading" : "100 50 50 25 ( -14.0778, 18.3665) ( -14.0773, 18.3658)",
      "text" : "Since (28) is the sufficient and necessary condition for L1-median, if we can find ~x and ~y such that h(~x, ~y) = 0, then (x0, y0) is the L1-median of the new set.\nSince\n∂xih(~x, ~y) =2 ( k∑\nj=1\nxj − x0√ (xj − x0)2 + (yj − y0)2 ) (yi − y0)2( (xi − x0)2 + (yi − y0)2 ) 3 2\n− 2 ( k∑\nj=1\nyj − y0√ (xj − x0)2 + (yj − y0)2 ) (xi − x0)(yi − y0)( (xi − x0)2 + (yi − y0)2 ) 3 2 ,\n∂yih(~x, ~y) =− 2 ( k∑\nj=1\nxj − x0√ (xj − x0)2 + (yj − y0)2 ) (xi − x0)(yi − y0)( (xi − x0)2 + (yi − y0)2 ) 3 2\n+ 2 ( k∑\nj=1\nyj − y0√ (xj − x0)2 + (yj − y0)2 ) (xi − x0)2( (xi − x0)2 + (yi − y0)2 ) 3 2 ,\nwe can use gradient descent to compute ~x, ~y to minimize h. For the input S = {(xi, yi)|1 ≤ i ≤ k}, we choose the initial value ~x0 = {x1, x2, · · · , xk̃}, ~y0 = {y1, y2, · · · , yk̃}, and then update ~x and ~y along the negative gradient direction of h, until the Euclidean norm of gradient is less than 0.00001.\nThe algorithm framework is then as follows, using the above gradient descent formulation at each step. We first compute the L1-median mi for each Pi, and then change ñ points in {m1,m2, · · · ,mn} to obtain {m′1,m′2, · · · ,m′ñ,mñ+1, · · · ,mn} such that median(m′1,m ′ 2, · · · ,m′ñ,mñ+1, · · · ,mn) = p0. For each m′i, we change k̃ points in Pi to obtain P̃i = {p′i,1, p′i,2, · · · , p′i,k̃, pi,k̃+1, · · · , pi,k}\nsuch that median(P̃i) = m′i. Thus, we have median ( median(P̃1), · · · ,median(P̃ñ),median(Pñ+1), · · · ,median(Pn) ) = p0. (29)\nTo show a simulation of this process, we use a uniform distribution to randomly generate nk points in the region [−10, 10] × [−10, 10], and generate a target point p0 = (x0, y0) in the region [−20, 20]× [−20, 20], and then use our algorithm to change ñk̃ points in the given set, to make the new set satisfy (29). Table 1 shows the result of running this experiment for different n and k, where (x′0, y ′ 0) is the median of medians for the new set obtained by our algorithm. It lists the various values n and k, the corresponding values ñ and k̃ of points modified, and the target point and result of our algorithm. If we reduce the terminating condition, which means increasing the number of iteration, we can obtain a more accurate result, but only requiring the Euclidean norm of gradient to be less than 0.00001, we get very accurate results, within about 0.01 in each coordinate.\nWe illustrate the results of this process graphically for a couple of examples in Table 1; for the cases n = 5, k = 8, (x0, y0) = (0.9961, 1.0126) and n = 5, k = 8, (x0, y0) = (10.7631, 11.0663) These are shown in Figure 1 and Figure 2, respectively. In these two figures, the green star is the target point. Since n = 5, we use five different markers (circle, square, upward-pointing triangle,\ndownward-pointing triangle, and diamond) to represent five kinds of points. The given data Pflat are shown by black points and unfilled points. Our algorithm changes those unfilled points to the blue ones, and the green points are the medians of the new subsets. The red star is the median of medians for Pflat, and other red points are the median of old subsets. So, we only changed 12 points out of 40, and the median of medians for the new data set is very close to the target point."
    }, {
      "heading" : "4.2 Simulation 2 : Router Monitoring",
      "text" : "Suppose there are n = 100 routers in a network, and each router monitors a stream of length k = 1000. A router can use streaming algorithm to monitor a single percentile, for instance the frugal algorithm here [10] only needs a few bites per percentile maintained – it does not need to monitor all. We will consider monitoring the approximate median (50% percentile), 10% percentile, and 90% percentile of the stream, and sending these to a single command center. The command center will analyze these data to determine whether an attack occurs. In practice, command centers monitor much larger streams (values of k) and many more routers (values of n).\nWe use standard normal distribution to generate an array Si with 1000 entries to simulate the ith stream, and assume the routers use the estimator E1 to process streams, i.e. E1 returns the approximate 10% percentile, or 90% percentile, or the median of a stream. The command center uses the estimator E2 to process the gathered data S = (E1(S1), E1(S2), · · · , E1(Sn)), and E2 can return the 10% percentile, or 90% percentile, or the median of S. In our simulation, we compute each of these quantities exactly. We use outliers in interval [100, 110] or [−110,−100] to simulate attacks. These values may represent some statistic deemed worth monitoring, say the packet length or header size after it has been appropriately normalized.\nWe choose n1 streams, and put k1 outliers from the same interval (all positive, or all negative) to each chosen stream. Table 2 shows the final output from command center for different combinations of estimators and outliers. The first column in Table 2 shows the proportion of outliers, which is equal to n1k1nk . For example, in the third row of the table, we choose 11 streams randomly and put 110 outliers drawn from [100,110] into each chosen stream, so the proportion of outliers is (11 × 110)/(100 × 1000) = 1.21%. When a value being monitored as a composite of various percentiles becomes very large (above 100, so not from the normal distribution) we mark it bold.\nIt is shown in Table 2 that for the case E1 : 10%, E2 : 10% and E1 : 90%, E2 : 90%, we can use 1.21% of outliers to change the output of E1-E2 estimator, since in this situation the breakdown point of E1-E2 estimator is 0.01. For the case E1 : 10%, E2 : 90% and E1 : 10%, E2 : 90%, we can use 10.01% of outliers to change the output of E1-E2 estimator, since in this situation the breakdown point of E1-E2 estimator is 0.09. When E1 and E2 both return the median of a data set, we can use 26.01% of outliers to change the output of E1-E2 estimator, since in this situation the breakdown point of E1-E2 estimator is 0.25.\nThis experiment illustrates how using various composite estimators with different percentiles can highlight various levels of potential distributed denial of service attacks. For instance, if only the E1 : 10%, E2 : 10% estimator is flagged, then we see a few routers have a few anomalous packets, and even though it is distributed to only about 10% of routers and 10% of data, we can observe it; but for the most part would be at most a warning. If E1 : 10%, E2 : 90% estimator or E1 : 50%, E2 : 50% estimator is flagged, it means at least 9% or 25% of the packets across all routers much be anomalous, and we may see a real DDS or an early sign of one. These are all conservative estimates. On the other hand, if at least 10% of the packets are modified on 10% of routers (not too much, perhaps as little as 1%), then the E1 : 10%, E2 : 10% estimator will definitely observe it. And if at least 10% of the packets are modified on 50% of the routers (over 5% of all packets), then an E1 : 10%, E2 : 50% estimator will definitely observe it. Further work is required to discover the best combination of percentiles to monitor, but using our observations about composite estimators suggests this approach which can monitor against various distributions of DDS attacks without only a few simple estimators, requiring a few bites each, at each router."
    }, {
      "heading" : "5 Discussion",
      "text" : "In this paper, we define the breakdown point of the composition of two or more estimators. These definitions are technical but necessary to understand the robustness of composite estimators; and they do not stray too far from prior formal definitions [4, 3]. Generally, the composition of two of\nmore estimators is less robust than each individual estimator. We highlight a few applications and believe many more exist. These results already provide important insights for complex data analysis pipelines common to large-scale automated data analysis. Moreover, these approaches provides worst case guarantees that are concrete about when outliers can or cannot create a problem, as opposed to some regularization-based approaches that just tend to work on most data.\nNext we will highlight a few more insights from this work, or discuss challenges for follow-on work.\nOn the dangers of composition. The common case of composing two estimators, each with breakdown point of 0.5 yields a composite estimator of 0.25. This means if the result is anomalous, at least 25% of the data must change, down from 50%. In other cases, the resulting composite estimator might yield an even smaller breakdown point of say 0.05. This seems like very bad news! But for large data sets, adversarially changing 5% of data is still a lot. For instance with 1 million data points, then 5% is 50, 000, which would still be an ominously difficult task to modify. So even a 0.05 or 0.01 breakdown point on large data is a useful barrier to manipulation (of the sort in our Simulation 1 below). On the other hand, repeated composition can quickly (exponentially) decrease the breakdown point until it is dangerously low; hence we believe this new theory will play an import role in understanding the robustness and security of long data analysis pipelines.\nRobustness and unbiasedness. In this paper, we focus exclusively on the robustness of estimators, but it is also important to aim for low-MSE or unbiasedness estimators. An interesting future direction is to design estimators that are both robust (including have large onto-breakdown points) as well as other properties. We lead this direction with a few points:\n• Composing two unbiased estimators will typically be unbiased (some care may be needed in weighting). • Robustness is a worst-case analysis (protecting against adversarial data) and its claims are often orthogonal to those about low-MSE. • Our analysis bounds the robustness of composition of any two (or more) estimators. So if other work independently shows low-MSE or low-bias properties, then we can immediately combine these works to show both.\nRemoving all subsets size k constraint. The restriction |Pi| = k (all subsets at the first level are the same size) is mainly for expositional convenience. Otherwise, there are some technical issues with reweighing points in Pflat and defining the limits. In fact, suppose |Pi| = ki for i = 1, 2, · · · , n, Pflat = ]ni=1Pi, gE1(k1) ≤ gE1(k2) ≤ · · · ≤ gE1(kn), and\nE(Pflat) = E2 (E1(P1), E1(P2), · · · , E1(Pn)) .\nThen using the method in the proof of Theorem 1, we can obtain a result similar :\ngE2 (n)∑ i=1 gE1(ki) ≤ gE( n∑ i=1 ki) (30)\nwhich is a generalization of (10).\nFinite sampling breakdown point for composite estimators. Theorem 2 provides an asymptotic breakdown point for composite estimators. But for smaller data sets, a finite sample version is also useful and important. Equation (10) already gives a lower bound of the finite sample breakdown point of composite estimators. To get an upper bound on the finite sample vesion, we can modify Theorem 2, by adding a condition fE1(k) = gE1(k) + C where C is a positive constant. Then there is also an annoying off-by-one error on gE2 (see eq (20)), so the result would be something like\ngE1(k)gE2(n) ≤ gE(nk) ≤ (gE1(k) + C)(gE2(n) + 1),\nand it is not completely tight. We leave providing a tight bound (up to these constants) as an open question."
    } ],
    "references" : [ {
      "title" : "Geometric measures of data depth",
      "author" : [ "G. Aloupis" ],
      "venue" : "Data Depth: Robust Multivariate Analysis, Computational Geometry and Applications. AMS,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Approximation algorithms for clustering uncertain data",
      "author" : [ "G. Cormode", "A. McGregor" ],
      "venue" : "PODS,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "The breakdown point: Examples and counterexamples",
      "author" : [ "P. Davies", "U. Gather" ],
      "venue" : "REVSTAT – Statitical Journal, 5:1–17,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "A general qualitative definition mof robustness",
      "author" : [ "F.R. Hampel" ],
      "venue" : "Annals of Mathematical Statistics, 42:1887– 1896,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 1971
    }, {
      "title" : "Robust Statistics: The Approach Based on Influence Functions",
      "author" : [ "F.R. Hampel", "E.M. Ronchetti", "P.J. Rousseeuw", "W.A. Stahel" ],
      "venue" : "Wiley,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 1986
    }, {
      "title" : "Breakdown robustness of tests",
      "author" : [ "X. He", "D.G. Simplson", "S.L. Portnoy" ],
      "venue" : "Journal of the Maerican Statistical Association, 85:446–452,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 1990
    }, {
      "title" : "Robust Statistics",
      "author" : [ "P.J. Huber" ],
      "venue" : "Wiley,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 1981
    }, {
      "title" : "Breakdown point",
      "author" : [ "P.J. Huber", "E.M. Ronchetti" ],
      "venue" : "Robust Statistics, page 8. John Wiley & Sons, Inc.,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Geometric computation on indecisive points",
      "author" : [ "A.G. Jørgensen", "M. Löffler", "J.M. Phillips" ],
      "venue" : "WADS,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Frugal streaming for estimating quantiles: One (or two) memory suffices",
      "author" : [ "S.M. Ma", "Qiang", "M. Sandler" ],
      "venue" : "arXiv preprint arXiv: 1407.1121,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2014
    }, {
      "title" : "Representing uncertain data: models, properties, and algorithms",
      "author" : [ "A.D. Sarma", "O. Benjelloun", "A. Halevy", "S. Nabar", "J. Widom" ],
      "venue" : "VLDBJ, 18:989–1019,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Robust regression using repeated medians",
      "author" : [ "A.F. Siegel" ],
      "venue" : "Biometrika, 82:242–244,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 1982
    }, {
      "title" : "On the point for which the sum of the distances to n given points is minimum",
      "author" : [ "E. Weiszfeld", "F. Plastria" ],
      "venue" : "Annals of Operations Research, 167:7–41,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "The standard deviation",
      "author" : [ "A.H. Welsh" ],
      "venue" : "Aspects of Statistical Inference, page 245. Wiley-Interscience;,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 1996
    } ],
    "referenceMentions" : [ {
      "referenceID" : 4,
      "context" : "Robust statistical estimators [5, 7] (in particular, resistant estimators), such as the median, are an essential tool in data analysis since they are provably immune to outliers.",
      "startOffset" : 30,
      "endOffset" : 36
    }, {
      "referenceID" : 6,
      "context" : "Robust statistical estimators [5, 7] (in particular, resistant estimators), such as the median, are an essential tool in data analysis since they are provably immune to outliers.",
      "startOffset" : 30,
      "endOffset" : 36
    }, {
      "referenceID" : 3,
      "context" : "The breakdown point [4, 3] is a basic measure of robustness of an estimator.",
      "startOffset" : 20,
      "endOffset" : 26
    }, {
      "referenceID" : 2,
      "context" : "The breakdown point [4, 3] is a basic measure of robustness of an estimator.",
      "startOffset" : 20,
      "endOffset" : 26
    }, {
      "referenceID" : 0,
      "context" : "• Aloupis [1] write “the breakdown point is the proportion of data which must be moved to infinity so that the estimator will do the same.",
      "startOffset" : 10,
      "endOffset" : 13
    }, {
      "referenceID" : 7,
      "context" : "” • Huber and Ronchetti [8] write “the breakdown point is the smallest fraction of bad observations that may cause an estimator to take on arbitrarily large aberrant values.",
      "startOffset" : 24,
      "endOffset" : 27
    }, {
      "referenceID" : 13,
      "context" : "\" • Dasgupta, Kumar, and Srikumar [14] write “the breakdown point of an estimator is the largest fraction of the data that can be moved arbitrarily without perturbing the estimator to the boundary of the parameter space.",
      "startOffset" : 34,
      "endOffset" : 38
    }, {
      "referenceID" : 10,
      "context" : "For instance, in the last decade there has been increased focus on the study of uncertainty data [11, 9, 2] where instead of analyzing a data set, we are given a model of the uncertainty of each data point.",
      "startOffset" : 97,
      "endOffset" : 107
    }, {
      "referenceID" : 8,
      "context" : "For instance, in the last decade there has been increased focus on the study of uncertainty data [11, 9, 2] where instead of analyzing a data set, we are given a model of the uncertainty of each data point.",
      "startOffset" : 97,
      "endOffset" : 107
    }, {
      "referenceID" : 1,
      "context" : "For instance, in the last decade there has been increased focus on the study of uncertainty data [11, 9, 2] where instead of analyzing a data set, we are given a model of the uncertainty of each data point.",
      "startOffset" : 97,
      "endOffset" : 107
    }, {
      "referenceID" : 3,
      "context" : "One variant of the definition closely aligns with other formalizations [4, 3], while another is fundamentally different.",
      "startOffset" : 71,
      "endOffset" : 77
    }, {
      "referenceID" : 2,
      "context" : "One variant of the definition closely aligns with other formalizations [4, 3], while another is fundamentally different.",
      "startOffset" : 71,
      "endOffset" : 77
    }, {
      "referenceID" : 3,
      "context" : "The first definition, Asymptotic Breakdown Point, is similar to the classic formal definitions in [4] and [3] (including their highly technical nature), although their definitions of the estimator are slightly different leading to some minor differences in special cases.",
      "startOffset" : 98,
      "endOffset" : 101
    }, {
      "referenceID" : 2,
      "context" : "The first definition, Asymptotic Breakdown Point, is similar to the classic formal definitions in [4] and [3] (including their highly technical nature), although their definitions of the estimator are slightly different leading to some minor differences in special cases.",
      "startOffset" : 106,
      "endOffset" : 109
    }, {
      "referenceID" : 0,
      "context" : "For example, the median, L1-median [1], and Siegel estimators [12] all have asymptotic breakdown points of 0.",
      "startOffset" : 35,
      "endOffset" : 38
    }, {
      "referenceID" : 11,
      "context" : "For example, the median, L1-median [1], and Siegel estimators [12] all have asymptotic breakdown points of 0.",
      "startOffset" : 62,
      "endOffset" : 66
    }, {
      "referenceID" : 0,
      "context" : "One “robust” way to process this data, is to first pre-process each Pi with its L1-median [1]: (x̄i, ȳi)← E1(Pi), where E1(Pi) = L1-median(Pi) has breakdown point β1 = 0.",
      "startOffset" : 90,
      "endOffset" : 93
    }, {
      "referenceID" : 11,
      "context" : "Then we could generate a linear model to predict weight ŷi = ax+b from the Siegel Estimator [12]: E2(Z) = (a, b), with breakdown point β2 = 0.",
      "startOffset" : 92,
      "endOffset" : 96
    }, {
      "referenceID" : 5,
      "context" : "05 [6].",
      "startOffset" : 3,
      "endOffset" : 6
    }, {
      "referenceID" : 12,
      "context" : "If (x0, y0) is the L1-median of the set {(xi, yi) | 1 ≤ i ≤ k}, then we have [13]:",
      "startOffset" : 77,
      "endOffset" : 81
    }, {
      "referenceID" : 9,
      "context" : "A router can use streaming algorithm to monitor a single percentile, for instance the frugal algorithm here [10] only needs a few bites per percentile maintained – it does not need to monitor all.",
      "startOffset" : 108,
      "endOffset" : 112
    }, {
      "referenceID" : 3,
      "context" : "These definitions are technical but necessary to understand the robustness of composite estimators; and they do not stray too far from prior formal definitions [4, 3].",
      "startOffset" : 160,
      "endOffset" : 166
    }, {
      "referenceID" : 2,
      "context" : "These definitions are technical but necessary to understand the robustness of composite estimators; and they do not stray too far from prior formal definitions [4, 3].",
      "startOffset" : 160,
      "endOffset" : 166
    } ],
    "year" : 2016,
    "abstractText" : "We formalize notions of robustness for composite estimators via the notion of a breakdown point. A composite estimator successively applies two (or more) estimators: on data decomposed into disjoint parts, it applies the first estimator on each part, then the second estimator on the outputs of the first estimator. And so on, if the composition is of more than two estimators. Informally, the breakdown point is the minimum fraction of data points which if significantly modified will also significantly modify the output of the estimator, so it is typically desirable to have a large breakdown point. Our main result shows that, under mild conditions on the individual estimators, the breakdown point of the composite estimator is the product of the breakdown points of the individual estimators. We also demonstrate several scenarios, ranging from regression to statistical testing, where this analysis is easy to apply, useful in understanding worst case robustness, and sheds powerful insights onto the associated data analysis.",
    "creator" : "LaTeX with hyperref package"
  }
}