{
  "name" : "1511.06961.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "On the Linear Algebraic Structure of Distributed Word Representations",
    "authors" : [ "Lisa Seung-Yeon Lee", "Lisa Lee" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "On the Linear Algebraic Structure of Distributed Word Representations\nLisa Seung-Yeon Lee\nAdvised by Professor Sanjeev Arora\nMay \nA thesis submitted to the Princeton University Department of Mathematics in partial fulfillment of the requirements for the degree of Bachelor of Arts.\nar X\niv :1\n51 1.\n06 96\n1v 1\n[ cs\n.C L\n] 2\n2 N\nov 2\n01 5"
    }, {
      "heading" : "Abstract",
      "text" : "In this work, we leverage the linear algebraic structure of distributed word representations to automatically extend knowledge bases and allow a machine to learn new facts about the world. Our goal is to extract structured facts from corpora in a simpler manner, without applying classifiers or patterns, and using only the co-occurrence statistics of words. We demonstrate that the linear algebraic structure of word embeddings can be used to reduce data requirements for methods of learning facts. In particular, we demonstrate that words belonging to a common category, or pairs of words satisfying a certain relation, form a low-rank subspace in the projected space. We compute a basis for this low-rank subspace using singular value decomposition (SVD), then use this basis to discover new facts and to fit vectors for less frequent words which we do not yet have vectors for.\nThis thesis represents my own work in accordance with university regulations.\nLisa Lee"
    }, {
      "heading" : "Contents",
      "text" : " Introduction  . Distributed word representations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  . Extending existing knowledge bases . . . . . . . . . . . . . . . . . . . . . . . . . . . .  . Overview of the paper . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . \n Word embeddings  . Notation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  . Methods for learning word vectors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . \n.. Skip-gram . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  .. GloVe . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  .. Squared Norm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . \n. Justification for why the word embedding methods work . . . . . . . . . . . . . . . . .  .. Justification for explicit, high-dimensional word embeddings . . . . . . . . . .  .. Justification for low-dimensional embeddings . . . . . . . . . . . . . . . . . . .  .. Motivation behind the Squared Norm objective . . . . . . . . . . . . . . . . . . \n Methods  . Training the word vectors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  . Preprocessing the corpus . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . \n Categories and relations  . Notation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  . Obtaining training examples of categories and relations . . . . . . . . . . . . . . . . .  . Experiments in the subsequent chapters . . . . . . . . . . . . . . . . . . . . . . . . . . \n Low-rank subspaces of categories and relations  . Computing a low-rank basis using SVD . . . . . . . . . . . . . . . . . . . . . . . . . . \n.. Experiment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  . Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  . Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . \n Extending a knowledge base  . Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  . Learning new words in a category . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . \n.. Performance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . \n. Learning new word pairs in a relation . . . . . . . . . . . . . . . . . . . . . . . . . . . .  .. Experiment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  .. Performance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  .. Varying levels of difficulty for different relations . . . . . . . . . . . . . . . . .  . Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . \n Learning vectors for less frequent words  . Learning a new vector . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . \n.. Motivation behind the optimization objective . . . . . . . . . . . . . . . . . . .  . Experiment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  . Performance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . \n.. Order and cosine score of the learned vector . . . . . . . . . . . . . . . . . . . .  .. Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . \n. Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . \n Using an external knowledge source to reduce false-positive rate  . Analogy queries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  . Wordnet . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  . Experiment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  . Performance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  . Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . \n Conclusion "
    }, {
      "heading" : "Acknowledgements",
      "text" : "First and foremost, I would like to thank Professor Sanjeev Arora for his patient guidance, advice, and support during the planning and development of this research work. I also would like to express my very great appreciation to Dr. Yingyu Liang, without whom I could not have completed this research project; thank you so much for the time and effort you took to help me run experiments and understand concepts, and for suggesting new ideas and directions to take when I felt stuck. I would also like to deeply thank Tengyu Ma for his valuable ideas and suggestions during this research project.\nI am also greatly indebted to Ming-Yee Tsang, who helped me format my thesis, and debug the complicated regular expressions code for preprocessing the huge Wikipedia corpus (see Section .). Many thanks to Victor Luu, Irene Lo, Eliott Joo, Christina Funk, and Ante Qu for proofreading my thesis and providing invaluable feedback.\nChanning, thank you so much for keeping me company while I was writing this thesis, for impromptu coffee runs in the middle of the night so that I wouldn’t fall asleep, for always motivating and encouraging me, and a myriad other things.\nI would also like to thank my wonderful friends and famiLee for their support. Mimi, thank you for being the silliest, kindest, coolest, most patient sister and best friend that you are to me. Joonhee, you will always be my cute little brother no matter how old you are. Thank you for all the fun Naruto missions we went on when you were little, for all your hilarious jokes, and for playing LoL/Pokemon/Minecraft with me. Happy, I love you too. Too bad you can’t read this. Woof woof. Thank you Laon for always being by my side since I was five. Thank you umma and abba for raising me and my siblings (and Happy), and for always encouraging me to try my best.\nBilly, thanks for always challenging me to think more rigorously, and for all the fun musicmaking we did (yay Brahms, Rachmaninoff, Saint-Saens, Chopin, Piazzolla, Hisaishi, Pokemon). I am also extremely grateful to have found such a loving community in Manna Christian Fellowship during my freshman year.\nLast but not least, thank you God for giving me these four precious years at Princeton University to meet all these wonderful people and to study math."
    }, {
      "heading" : "Chapter ",
      "text" : ""
    }, {
      "heading" : "Introduction",
      "text" : ". Distributed word representations\nDistributed representations of words in a vector space represent each word with a real-valued vector, called a word vector. They are also known as word embeddings because they embed an entire vocabulary into a relatively low-dimensional linear space whose dimensions are latent continuous features. One of the earliest ideas of distributed representations dates back to  [], and has since been applied to statistical language modeling with considerable success. These word vectors have shown to improve performance in a variety of natural language processing tasks including automatic speech recognition [], information retrieval [], document classification [], and parsing [].\nThe word vectors are trained over large corpora typically in a totally unsupervised manner, using the co-occurrence statistics of words. Past methods to obtain word embeddings include matrix factorization methods [], variants of neural networks [, , , , , , ], and energybased models [, ]. The learned word vectors explicitly capture many linguistic regularities and patterns, such as semantic and syntactic attributes of words. Therefore, words that appear in similar contexts, or belong to a common “category” (e.g., country names, composer names, or university names), tend to form a cluster in the projected space.\nRecently, Mikolov et al. [] demonstrated that word embeddings created by a recurrent neural net (RNN) and by a related energy-based model called wordvec exhibit an additional linear structure which captures the relation between pairs of words, and allows one to solve analogy queries such as “man:woman::king:??” using simple vector arithmetics. More specifically, “queen” happens to be the word whose vector vqueen is the closest approximation to the vector vwoman − vman + vking. Other subsequent works [, , ] produced word vectors that can be used to solve analogy queries in the same way. It remains a mystery as to why these radically different embedding methods, including highly non-linear ones, produce vectors exhibiting similar linear structure. A summary of current justifications for this phenomenon is provided in Section .. A corpus (plural corpora) is a large and structured set of unlabeled texts. We say two words co-occur in a corpus if they appear together within a certain (fixed) distance in the text.\nOn the Linear Structure of Word Embeddings Chapter . Introduction | \n. Extending existing knowledge bases\nIn this work, we aim to leverage the linear algebraic structure of word embeddings to extend knowledge bases and learn new facts. Knowledge bases such as Wordnet [] or Freebase [] are a key source for providing structured information about general human knowledge. Building such knowledge bases, however, is an extremely slow and labor-intensive process. Consequently, there has been much interest in finding methods for automatically learning new facts and extending knowledge bases, e.g., by applying patterns or classifiers on large corpora [, , ]. Carlson et al.’s NELL (Never-Ending Language Learning) system [], for instance, extracts structured facts from the web to build a knowledge base, using over  different classifiers and extraction methods in combination with a large-scale semi-supervised multi-task learning algorithm.\nOur goal is to extract structured facts from corpora in a simpler manner, without applying classifiers or patterns, and using only the co-occurrence statistics of words. More specifically, we use the word co-occurrence statistics to produce word vectors, and then leverage their linear structure to learn new facts, such as new words belonging to a known category, or new pairs of words satisfying a known relation (see Chapter ). Our methods can supplement other methods for extending knowledge bases to reduce false positive rate, or narrow down the search space for discovering new facts.\n. Overview of the paper\nIn this paper, we will demonstrate that the linear algebraic structure of word embeddings can be used to reduce data requirements for methods of learning facts. In Chapter , we present a few methods for learning word vectors, and provide intuition as to why the embedding methods work. Chapter  describes how the word vectors used in our experiments were trained. Chapter  introduces the notion of categories and relations, which can be used to represent facts about the world in a knowledge base. In Chapter , we explore the linear algebraic structure of word embeddings. In particular, we demonstrate that words belonging to a common category, or pairs of words satisfying a certain relation, form a low-rank subspace in the projected space. We compute a basis for this low-rank subspace using singular value decomposition (SVD), then use this basis to discover new facts (Chapter ) and to fit vectors for less frequent words which we do not yet have vectors for (Chapter ). We also demonstrate that, using an external knowledge source such as Wordnet [], one can improve accuracy on analogy queries of the form “a:b::c:??” (Chapter ).\nA knowledge base is a collection of information that represents facts about the world."
    }, {
      "heading" : "Chapter ",
      "text" : ""
    }, {
      "heading" : "Word embeddings",
      "text" : "In this chapter, we introduce the reader to recent word embedding methods, and provide justifications for why these methods work.\nIn Section ., we present three different methods for producing word vectors that exhibit the desired linear properties: Mikolov et al.’s skip-gram with negative sampling (SGNS) method [], Pennington et al.’s GloVe method [], and Arora et al.’s Squared Norm (SN) objective []. In Section ., we provide a summary of current justifications for why these methods work. For further details and evaluations of these methods, see [, , ].\nThe three methods presented in Section . achieve similar, state-of-the-art performance on analogy query tasks. In our experiments, we use the SN objective (.) to train the word vectors because it is perhaps the simplest method thus far for fitting word embeddings, and it is also the only method out of the three which provably finds the near-optimum fit (see Arora et al. []).\n. Notation\nWe first introduce some notation. Let D be the set of distinct words that appear in a corpus C; then we say D is a set of vocabulary words that appear in C. We can enumerate the sequence of words (or tokens) in C as w1,w2, . . . ,w|C|, where |C| is the total number of tokens in C. Let k ∈N be fixed; then the context window of size k around a word wi ∈ C is the multiset consisting of the k tokens appearing before and after wi in the corpus,\nwindowk(wi) := {wi−k ,wi−k+1, . . . ,wi−1} ∪ {wi+1,wi+2, . . . ,wi+k}.\nTypically, the context window size k is chosen to be a fixed number between 5 and 10. For a vocabulary word w ∈ D, let κ(w) := ⋃\ni∈{1,...,|C|}: wi=w\nwindowk(wi)\nbe the set of all tokens appearing in some context window around w. Each distinct word χ ∈ κ(w) is called a context word for w. Let Dcontext be the set of all context words; that is, Dcontext is the That is, allowing one to solve analogy queries using linear algebraic vector arithmetics. (according to their generative model for text corpora described in their paper []) This is just one way of defining context, and other types of contexts can be considered; see [].\nOn the Linear Structure of Word Embeddings Chapter . Word embeddings | \nset of all distinct words in ∪w∈Dκ(w). Note that, because of the way we define context, we have Dcontext = D and the distinction between a word and a context word is arbitrary, i.e., we are free to interchange the two roles.\nFor two words w,w′ ∈ D, let\nXww′ := |{wi ∈ κ(w) : wi = w′}|\ni.e., Xww′ is the number of times word w′ appears in any context window around w. Then Xw :=∑ w′ Xww′ = |κ(w)|, and p(w′ | w) := Xww′ Xw is the empirical probability that word w′ appears in some\ncontext window around w (i.e., w′ is a context word for w). Also, p(w) := Xw∑ w′ Xw′\nis the empirical probability that a randomly selected word of the corpus is w. The matrix X whose rows and columns are indexed by the words inD, and whose entries are Xww′ , is called the word co-occurrence matrix of C.\nIn this paper, let d ∈N be the dimension of the word vectors vw ∈Rd . The word co-occurrence statistics are used to train the word vectors vw ∈Rd for words w ∈ D.\n. Methods for learning word vectors\nBelow, we present a few methods for obtaining word embeddings which allow one to solve analogy queries using linear algebraic vector arithmetics. Other methods include large-dimensional embeddings that explicitly encode co-occurrence statistics [] (see Section .) and noise-contrastive estimation [].\n.. Skip-gram\nFor a word w ∈ D and a context χ ∈ Dcontext, we say the pair (w,χ) is observed in the corpus and write (w,χ) ∈ C, if χ appears in some context window around w (i.e., χ ∈ κ(w)). In Mikolov et al.’s skip-gram with negative sampling (SGNG) model [, ], the probability that a word-context pair (w,χ) is observed in the corpus is parametrized by\np(w,χ) = 1 1 + exp ( −vw · vχ ) , where vw,vχ ∈ Rd are the vectors for w and χ respectively. SGNS tries to maximize p(w,χ) for observed (w,χ) pairs in the corpus C, while minimizing p(w,χ) for randomly sampled “negative” samples (w,χ). Their optimization objective is the log-likelihood,\nargmax {vw :w∈D}  ∏ (w,χ)∈C p(w,χ)   ∏ (w,χ)<C (1− p(w,χ))  =argmax {vw :w∈D} ∑ (w,χ)∈C logp(w,χ) + ∑ (w,χ)<C log(1− p(w,χ)) ,\nThe dimension d of the word vectors is a parameter that can be chosen, and is typically much smaller than the number of vocabulary words |D| or the size of the corpus |C|. In the three methods presented in Section ., a dimension between 50 and 300 is used. We assume that the randomly generated negative samples (w,χ) are not observed in the corpus.\nOn the Linear Structure of Word Embeddings Chapter . Word embeddings | \nwhere argmaxx f (x) := {x : f (x) ≥ f (y) ∀y} is the set of arguments for which the given function f attains its maximum value.\n.. GloVe\nPennington et al.’s GloVe (“Global Vectors for Word Representation”) method [] fits, for each word w ∈ D, two low-dimensional vectors vw, ṽw ∈ Rd and scalars bw, b̃w ∈ R so as to minimize the cost function ∑\nw,w′ f (Xww′ )\n( vw · ṽw′ + bw + b̃w′ − logXww′ )2 , (.)\nwhere f (x) = min {(\nx xmax\n)α ,1 } for some constants xmax and α. In their experiments, they use α = 0.75\nand xmax = 100. The purpose of the weighing function f is twofold:\n• f (x) is non-decreasing, so that rare co-occurrences (which tend to have greater noise) are not overweighted.\n• f (x) is relatively small for large values of x, so that frequent co-occurrences are not overweighted.\nFor the motivation behind the GloVe objective (.), we refer the reader to their paper [].\n.. Squared Norm\nArora et al.’s Squared Norm (SN) method [] fits a scalar Z ∈ R and, for each word w ∈ D, a vector vw ∈Rd so as to minimize the objective∑\nw,w′ f (Xww′ )\n( log(Xw,w′ )− ‖vw + vw′‖2 −Z )2 , (.)\nwhere f is the same weighting function as in the GloVe objective (.). For the motivation behind the SN objective (.), see Section ...\n. Justification for why the word embedding methods work\nOnce the word vectors have been produced, one can answer analogy queries of the form “a:b::c:??” by finding the word d̃ whose vector is closest to vb − va + vc according to cosine similarity, i.e.,\nd̃ = argmax d vd · (vb − va + vc)\n= argmin d (va − vb − vc) · vd\n= argmin d\n2(va − vb − vc) · vd + ‖va − vb − vc‖2 + ‖vd‖2\n= argmin d ‖va − vb − vc + vd‖2 (.)\nIn this paper, ‖·‖ refers to Euclidean norm, or ‖·‖2.\nOn the Linear Structure of Word Embeddings Chapter . Word embeddings | \nwhere the vectors vw for each word w have been normalized so that ‖vw‖ = 1. It remains a mystery as to why such vastly different embedding methods, including highly nonlinear ones, produce vectors exhibiting similar linear structure, and achieve fairly similar accuracy on analogy queries. In the rest of this chapter, we provide a summary of the current justifications for this phenomenon.\n.. Justification for explicit, high-dimensional word embeddings\nLevy and Goldberg [] and Pennington et al. [] provide a statistical intuition as to why the answer to the analogy “man:woman::king:??” must be “queen”. The reason is that most contexts χ ∈ Dcontext satisfy\np(χ |man) p(χ |woman) ≈ p(χ | king) p(χ | queen)\nwhere p(χ | w) is the conditional probability that χ appears in some context window around word w. For example, both ratios are around  for most contexts (e.g., “sleep”, “the”, “food”), but the ratio deviates from when χ is not gender-neutral (e.g., “dress”, “he”, “she”, “Elizabeth”, “Henry”). Therefore, a reasonable answer to the analogy query “a:b::c:??” is the word d that minimizes∑\nχ∈Dcontext\n( log\np(χ | a) p(χ | b) − log p(χ | c) p(χ | d)\n)2 . (.)\nHence, Levy and Goldberg [] proposed a very high-dimensional embedding that explicitly encodes correlation statistics between words and contexts: The vector for word w ∈ D is given by vw ∈ R|Dcontext |, where vw is indexed by all possible contexts χ ∈ Dcontext, and the entry (vw)χ in coordinate χ is equal to\nPMI(w,χ) := log p(w,χ) p(w)p(χ) = log p(χ | w) p(χ) . (.)\nNote that with this word embedding, the objective (.) is equivalent to (.):\nmin d ‖va − vb − vc + vd‖22 = min d ∑ χ ( log p(χ | a) p(χ) − log p(χ | b) p(χ) − log p(χ | c) p(χ) + log p(χ | d) p(χ) )2 = min\nd ∑ χ ( log p(χ | a) p(χ | b) − log p(χ | c) p(χ | d) )2 .\nAnd indeed, Levy and Goldberg [] show that the explicit word embeddings solve analogies via linear algebraic queries empirically.\n.. Justification for low-dimensional embeddings\nHowever, the above justification only applies to large-dimensional embeddings that explicitly encode correlation statistics between words and contexts. Recently, Arora et al. [] gave a justification for why low-dimensional embeddings are successful in solving analogy queries. More specifically, they postulate that the PMI matrix and the word vectors vw ∈Rd satisfy the following properties: The second to last equality follows because ‖vd‖2 = 1 is a constant. The PMI (pointwise mutual information) matrix is a |D| × |Dcontext|matrix whose rows are indexed by words w ∈ D and columns are indexed by contexts χ ∈ Dcontext, and whose entries are PMI(w,χ) as defined in (.).\nOn the Linear Structure of Word Embeddings Chapter . Word embeddings | \nProperty A. The PMI matrix can be approximated by a positive semidefinite matrix of fairly low rank, which is closer to logn than to n. This yields natural word embeddings vw that are implicit in the co-occurrence data itself: PMI(w,w′) ≈ vw · vw′ .\nProperty B. The word vectors vw are approximately isotropic meaning the expectation Ew[vwvTw ] is approximately like the identity matrix I , in that every one of its eigenvalues lies in [1,1 + δ] for some small δ > 0.\nThey provide a plausible generative model for text corpora using log-linear distributions, under which both properties hold. Using this generative model, they prove that, up to a constant logZ and some small error,\nlogp(w,w′) ∝ ‖vw + vw′‖2 − 2logZ (.) logp(w) ∝ ‖vw‖2 − logZ (.)\nwhere p(w,w′) is the probability that the words w and w′ appear together in the corpus, and p(w) is the prior of seeing word w in the corpus. Therefore,\nPMI(w,w′) := logp(w,χ)− logp(w)− logp(χ) ∝ ‖vw + vw′‖2 − ‖vw‖2 − ‖vw′‖2 by (.) and (.) = 2vw · vw′ ∝ vw · vw′ . (.)\nProperty B implies that, for every vector v ∈Rd ,\n‖v‖2 ≈ vT Ew[vwvTw ]v (up to error 1 + δ) = Ew[(v · vw)2] ,\nand so the query (.) for solving “a:b::c:??” becomes\nargmin d ‖va − vb − vc + vd‖2 ≈ argmin d Ew(va · vw − vb · vw − vc · vw + vd · vw)2\n= argmin d\nEw(PMI(a,w)−PMI(b,w)−PMI(c,w) + PMI(d,w))2 by (.)\n= argmin d Ew\n( log\np(χ | a) p(χ) − log p(χ | b) p(χ) − log p(χ | c) p(χ) + log p(χ | d) p(χ) )2 = argmin\nd Ew\n( log\np(w | a) p(w | b) − log p(w | c) p(w | d)\n)2 ,\nwhich is close to (.), with word w acting as context χ. The generative model directly models how words are produced as the corpus is being generated, and captures latent semantic structure in the text. For details about their generative model, see []. For an overview of log-linear models, which are very widely used in natural language processing, see [].\nOn the Linear Structure of Word Embeddings Chapter . Word embeddings | \n.. Motivation behind the Squared Norm objective\nNote that by (.), we have logp(w,w′) ∝ ‖vw+vw′‖22+C where C := −2logZ is an unknown constant. Then since the empirical probability p(w,w′) is given by\np(w,w′) = p(w′ | w)p(w)\n= Xww′\nXw Xw∑ ŵXŵ\n∝ Xww′ ,\nthis gives us the Squared Norm (SN) objective (.)."
    }, {
      "heading" : "Chapter ",
      "text" : ""
    }, {
      "heading" : "Methods",
      "text" : ". Training the word vectors\nOur training corpus C is an english Wikipedia corpus consisting of roughly . billion tokens, which was preprocessed before training so that multi-word named-entities (e.g., “princeton university”) are treated as a single word (e.g., “princeton_university”); see Section ..\nWe use a fixed context window of size 10 to compute the co-occurrence data. Since it is too memory-expensive to store the co-occurrence count between all distinct words that appear in C, and because the co-occurrence data for infrequent words are too noisy to generate good word vectors with, we fix a minimum threshold m0 ∈N, and only consider the set D of vocabulary words that appear at least m0 times in the training corpus. In our experiments, we set m0 = 1000, and the resulting vocabulary size is |D| = 60,265. Words which appear fewer than  times in the corpus are not included in the vocabulary D, and hence, we do not learn vectors for these words. In Chapter , we provide a way of learning vectors for less frequent words with only a small amount of co-occurrence data.\nTo train the word vectors VD := {vw : w ∈ D}, we optimize the Squared Norm (SN) objective (.) using Adagrad []. We use d = 300 as the dimension of the word vectors vw ∈ Rd , which is also the dimension used in [, , ] for their experiments on analogy query tasks. After training, we normalize the learned word vectors vw ∈ VD so that ‖vw‖ = 1.\n. Preprocessing the corpus\nWord embeddings are inherently limited by their inability to represent multi-word, idiomatic phrases whose meanings are not simple compositions of the individual words. For instance, “kevin_bacon” is the name of an individual, and so it is not a natural combination of the meanings of “kevin” and “bacon”. Many facts about the world are concerned with multi-word entities, and hence, it is important to learn vectors for these entities.\nhttp://dumps.wikimedia.org/enwiki/latest/enwiki-latest-pages-articles.xml.bz Words which appear less than m0 = 1000 times are ignored in the sense that () co-occurrence counts for these words are not computed, and () these words are ignored when computing co-occurrence counts for other words. They do not affect or change the context window around any word (i.e., they are not deleted from the corpus).\nOn the Linear Structure of Word Embeddings Chapter . Methods | \nTo allow training of vectors for multi-word named-entities, we preprocessed the corpus C before training in the following manner: We used the named-entity recognition library in the Natural Language Toolkit (NLTK) [] to identify strings ξ which correspond to multi-word named-entities in the corpus, and replaced each space in ξ with an underscore to make ξ a single word. (For example, we replaced every instance of the string “princeton university” in the corpus with “princeton_university”.) After preprocessing, we train vectors for multi-word named-entities in the same way as with other words.\nThe preprocessing decreased the size of D (i.e., the number of vocabulary words that appear at leastm0 = 1000 times in the corpus) from 68,430 to 60,265, but increased the number of words that appear at least 100 times in the corpus from 296,376 to 344,112.\nNamed-entity recognition (NER) is the task of labeling sequences of words in a text which are the names of things, such as the names of persons, organizations, and locations. NER is beyond the scope of this paper, and we refer the reader to []."
    }, {
      "heading" : "Chapter ",
      "text" : ""
    }, {
      "heading" : "Categories and relations",
      "text" : "In this chapter, we introduce the notion of categories and relations, which can be used to represent facts about the world in a knowledge base. In Figures . and ., we list the categories and relations that are used for experiments in the subsequent chapters.\n. Notation\nLet D be the set of vocabulary words that appear in a corpus C. Words w ∈ D belong to certain categories: for example, the word princeton belongs to the categories university and municipality, while the word christianity belongs to the category religion. Moreover, a pair of words sometimes satisfy a certain relation: e.g., the word pair (united_states, dollar) satisfies the relation currency_used.\nGiven a category c, let Dc ⊆ D be the set of words that belong to c, and let\nVc := {vw : w ∈ Dc}.\nSimilarly, given a relation r, let Dr ⊆ D ×D be the set of word pairs that satisfy the relation r, and let\nVr := {va − vb : (a,b) ∈ Dr }.\nThe vector subspace spanned by Vc is called the subspace of category c, and similarly, the vector subspace spanned by Vr is called the subspace of relation r. Moreover, we say a relation r is welldefined if there exists categories cA and cB such that, for all (a,b) ∈ Dr , a belongs to cA and b belongs to cB.\nFor example, if c = country, thenDc contains words such as germany, japan, and united_states. If r = capital_city, then Dr contains pairs such as (germany, berlin), (japan, tokyo), and (united_states, washington_dc). Also, the relation r = capital_city is well-defined, with cA = country and cB = city. See Figure .(a)-(h) for other examples of well-defined relations. Freebase [] is an example of a knowledge base whose data items are organized using categories and relations.\nOn the Linear Structure of Word Embeddings Chapter . Categories and relations | \n. Obtaining training examples of categories and relations\nWe obtained training examples of different categories and relations from various sources including Freebase [] and the GloVe demo code []. Figures . and . list the category and relation files that are used for experiments in the subsequent chapters. Note that some words in the category and relation files, such as maremma_sheepdog in the category animal, are not in the set D of vocabulary words because they appear fewer than m0 = 1000 times in the corpus C. Since we do not have learned vectors for these words, we remove them from the category and relation files in the experiments.\n. Experiments in the subsequent chapters\nIn Chapter , we will show that the subspace of a category c, or the subspace of a well-defined relation r, can be approximated well by a relatively low-dimensional subspace, whose basis vectors can be computed using SVD. In Chapter , we use this basis to discover new words belonging to a category, or new word pairs satisfying a well-defined relation. In Chapter , we also use this basis to learn vectors for words with substantially less co-occurrence data.\nChapter  explores the idea of using external knowledge sources such as Wordnet [] to improve accuracy on analogy queries, and uses the relation files in Figure . as a benchmark.\nOn the Linear Structure of Word Embeddings Chapter . Categories and relations | \nmaremma_sheepdog pheasant rabbit english_springer_spaniel laika\n(a) animal ( words)\nbeijing shanghai tokyo seoul pyeongyang\n(b) asian_city ( words)\nshaun_livingston baron_davis larry_bird lebron_james joakim_noah\n(c) basketball_player ( words)\nraclette castelrosso beal_organic lincolnshire_poacher mascarpone\n(d) cheese ( words)\nkiev budapest speyer zagreb da_lat\n(e) city ( words)\ndebussy beethoven mahler dvorak ravel\n(f) classical_composer ( words)\nsake_screwdriver la_cucaracha manhattan beton wine_cooler\n(g) cocktail ( words)\nkievan_rus hungary bishopric_of_speyer kingdom_of_croatiaslavonia french_indochina\n(h) country ( words)\npeso rial dollar cedi euro\n(i) currency ( words)\npi_day lag_baomer lao_new_year canada_day child_health_day\n(j) holiday ( words)\nhiligaynon pangasinan moldovan saami_ume judeotunisian_arabic\n(k) languages ( words)\njanuary february march april may\n(l) month ( words)\nmellotron shvi xiqin friction_drum soprano_cornet\n(m) musical_instrument ( words)\ntswana_music rap_opera african_reggae glam_punk barcarolle\n(n) music_genre ( words)\nroanoke_logperch amanita_parvipantherina platyzoa troides_haliphron solo_man\n(o) organism_group ( words)\narnold_schwarzenegger joe_pickett alan_greenspan john_maynard_keynes karl_marx\n(p) politician ( words)\nbuddhism_in_japan anglican kirant_mundhum saura congregational_church\n(q) religion ( words)\nextreme_ironing cammag skeleton speed_skating water_aerobics\n(r) sport ( words)\njohn_rider_house brevard_zoo butaritari time_and_tide_museum museo_liverino\n(s) tourist_attraction ( words)\nprinceton harvard yale oxford cambridge\n(t) university ( words)\nFigure .: For each category file, we list the first words and the total number of words it contains.\nOn the Linear Structure of Word Embeddings Chapter . Categories and relations | \nalgeria dinar angola kwanza argentina peso armenia dram brazil real bulgaria lev cambodia riel canada dollar croatia kuna\n(a) country-currency ( pairs)\nvalentine february halloween october thanksgiving november christmas december\n(b) holiday-month ( pairs)\nhanukkah judaism passover judaism passover samaritanism sukkot judaism shavuot judaism purim judaism diwali sikhism diwali jainism diwali hinduism\n(c) holiday-religion ( pairs)\nspanish spain welsh wales french france polish poland dutch netherlands japanese japan chinese china german germany korean korea\n(d) country-language ( pairs)\nathens greece baghdad iraq bangkok thailand beijing china berlin germany bern switzerland cairo egypt canberra australia hanoi vietnam\n(e) country-capital ( pairs)\nchicago illinois houston texas fort_worth texas kansas_city missouri philadelphia pennsylvania phoenix arizona knoxville tennessee san_jose california newark california\n(f) city-in-state ( pairs)\nkievan_rus kiev hungary budapest bishopric_of_speyer speyer kingdom_of_croatiaslavonia zagreb french_indochina da_lat republic_of_afghanistan kabul moravian_serbia krusevac somali_democratic_republic mogadishu guatemala guatemala_city\n(g) capital_country ( pairs)\nchile peso iran rial persia rial ghana cedi san_marino euro tonga paanga gambia dalasi grand_duchy_of_tuscany florin indonesia rupiah\n(h) currency_used ( pairs)\nFigure .: A list of facts-based relation files. For each relation file, we list the first  word pairs and the total number of word pairs it contains. Note that (a) is a smaller subset of (h), and (e) is a smaller subset of (g), the former containing only the more “well-known” pairs.\nOn the Linear Structure of Word Embeddings Chapter . Categories and relations | \namazing amazingly apparent apparently calm calmly cheerful cheerfully complete completely efficient efficiently fortunate fortunately free freely furious furiously\n(i) gram-adj-adv ( pairs)\nacceptable unacceptable aware unaware certain uncertain clear unclear comfortable uncomfortable competitive uncompetitive consistent inconsistent convincing unconvincing convenient inconvenient\n(j) gram-opposite ( pairs)\nbad worse big bigger bright brighter cheap cheaper cold colder cool cooler deep deeper easy easier fast faster\n(k) gram-comparative ( pairs)\nbad worst big biggest bright brightest cold coldest cool coolest dark darkest easy easiest fast fastest good best\n(l) gram-superlative ( pairs)\ncode coding dance dancing debug debugging decrease decreasing describe describing discover discovering enhance enhancing fly flying generate generating\n(m) gram-present-participle ( pairs)\nalbania albanian argentina argentinean australia australian austria austrian belarus belorussian brazil brazilian bulgaria bulgarian cambodia cambodian chile chilean\n(n) gram-nationality-adj ( pairs)\ndancing danced decreasing decreased describing described enhancing enhanced falling fell feeding fed flying flew generating generated going went\n(o) gram-past-tense ( pairs)\nbanana bananas bird birds bottle bottles building buildings car cars cat cats child children cloud clouds color colors\n(p) gram-plural ( pairs)\ndecrease decreases describe describes eat eats enhance enhances estimate estimates find finds generate generates go goes implement implements\n(q) gram-plural-verbs ( pairs)\nFigure .: A list of grammar-based relation files. For each relation file, we list the first  word pairs and the total number of word pairs it contains.\nOn the Linear Structure of Word Embeddings Chapter . Categories and relations | \ntriangle three quadrangle four pentagon five hexagonal six heptagon seven octagon eight january one february two march three\n(r) associated-number ( pairs)\ntub bathroom stove kitchen nurse hospital waiter restaurant bed bedroom desk classroom priest church aircraft sky submarine water\n(s) associated-position ( pairs)\nwarm hot cool cold cold freezing good great bad terrible pretty beautiful interested obsessed scared terrified tasty delicious\n(t) common-very ( pairs)\ndead alive black white sick healthy rich poor fat skinny young old old new bright dim smooth rough\n(u) graded-antonym ( pairs)\nfire hot ice cold genius smart idiot stupid\n(v) has-characteristic ( pairs)\near hear eye see tongue taste nose smell mouth eat feet walk broom clean stove cook hammer hit\n(w) has-function ( pairs)\nbird feather dog fur cat fur hamster fur alligator leather snake skin fish scale human skin\n(x) has-skin ( pairs)\ncat kitten dog puppy cow calf rooster chick human baby bear cub fish minow stone pebble mountain hill\n(y) noun-baby ( pairs)\neyes see ears hear nose smell tongue taste skin feel\n(z) senses ( pairs)\nFigure .: A list of semantics-based relation files. For each relation file, we list the first  word pairs and the total number of word pairs it contains."
    }, {
      "heading" : "Chapter ",
      "text" : ""
    }, {
      "heading" : "Low-rank subspaces of categories and",
      "text" : "relations\nWe demonstrate that the subspace of a category c, or the subspace of a well-defined relation r, can be approximated well by a relatively low-dimensional subspace in the projected space. More specifically, we use singular value decomposition (SVD) to approximate a low-rank basis Uk = {u1, . . . ,uk} for the subspace of category c or relation r (see Algorithm .). Section .. outlines an experiment where we use cross-validation to show that Uk captures most of the vectors in Vc = {vw : w ∈ Dc} or Vr = {va − vb : (a,b) ∈ Dr } (see Figure .). Moreover, we show that the first basis vector u1 captures the most information about a category c or a relation r (see Figures . and .).\n. Computing a low-rank basis using SVD\nThe function GET_BASIS (Algorithm .) uses SVD to compute a rank-k basis for the subspace spanned by a set of vectors V . So GET_BASIS(Vc, k) and GET_BASIS(Vr , k) return a rank-k basis for the subspace of category c and for the subspace of relation r, respectively.\nIn the following experiment, we use cross-validation to assess how much the low-rank basis Uk = {u1, . . . ,uk} returned by GET_BASIS captures the subspace of a category c or a relation r. We demonstrate that the first basis vector u1 (corresponding to the largest singular value σ1) captures the most information. In particular, we show that the first basis vector u1 captures significantly more mass of the subspace than any other basis vector (see Figure .), and that the only i ∈ {1, . . . , k} satisfying\nv ·ui > 0 ∀v ∈ Vc (or ∀v ∈ Vr )\nis i = 1 (see Figures . and .).\n.. Experiment\nLet c be a category where we have a set Sc ⊆ Dc of words that belong to the category c, with size |Sc | > 50. For each rank k ∈ {1,2, . . . ,25}, we perform the following experiment over T = 50 repeated trials: The same experiment is done for a relation r, by replacing Vc with Vr , and Sc ⊆ Dc with Sr ⊆ Dr .\nOn the Linear Structure of Word Embeddings Chapter . Low-rank subspaces | \nfunction GET_BASIS(V ,k): Returns a rank-k basis for the subspace spanned by V .\nInputs:\n• V = {v1,v2, . . . , vn}, a set of vectors in Rd . Let |V | := n denote the number of vectors in V .\n• k ∈N, the rank of the basis (where k d)\nStep . Let X be a d × |V |matrix whose column vectors are v ∈ V . Using SVD, factorize X as\nX =UΣW T ,\nwhere U ∈ Rd×d and W ∈ R|V |×|V | are orthogonal matrices, and Σ ∈ Rd×|V | is the diagonal matrix of the singular values of X in descending order.\nStep . Let Uk be the d × k submatrix obtained by taking the first k columns u1, . . . ,uk of U (which correspond to the k largest singular values of X). Since U is orthogonal, the columns of Uk form a rank-k orthonormal basis.\nStep . Scale u1 by ±1 so that ∑ v∈V v ·u1 > 0.\nStep . Return Uk .\nend function\nAlgorithm .: GET_BASIS(V ,k) returns a rank-k basis, Uk ∈ Rd×k , for the subspace spanned by a set of vectors V .\nOn the Linear Structure of Word Embeddings Chapter . Low-rank subspaces | \nStep . Randomly partition Sc into a training set S1 and a testing set S2, where the training set size is |S1| = 0.7|Sc |. For each i ∈ {1,2}, let Vi := {vw : w ∈ Si}.\nStep . Compute a rank-k basis Uk for the subspace spanned by V1, using Algorithm .:\nUk ←GET_BASIS(V1, k).\nStep . To measure how much a vector v ∈Rd is captured by the span of Uk ∈Rd×k , define\nφ(Uk ,v) := ‖UTk v‖ ‖v‖ .\nNow, test how much Uk captures the vectors V2 of the testing set, by computing the capture rate\nφ(Uk ,V2) := 1 |V2| ∑ v∈V2 φ(Uk ,v). (.)\nIf φ(Uk ,V2) is large, i.e., the vectors in V2 have a large projection onto Uk , then it would indicate that Uk is a good low-rank approximation for the subspace of category c.\nStep . Look at the distribution of the values in {ui · v}v∈V2 for each i ∈ {1, . . . , k}.\n. Results For each rank k ∈ {1,2, . . .25}, we plot the average capture rate φ̄k := 1T ∑T t=1φ(U (t) k ,V (t) 2 ) over T = 50 repeated trials in Figure .. Notice that when k = 1, φ̄k is already between 0.420 and 0.673. For k ≥ 2 on the other hand, the increase from φ̄k−1 to φ̄k is relatively small.\nWe found that the values {v · u1}v∈V2 all have the same sign , whereas for i ≥ 2, the values {v · ui}v∈V2 are more evenly distributed around . Figure . shows the distribution {v · ui}v∈V2 for i = 1 and i = 2; the distribution {v ·ui}v∈V2 for i ≥ 2 is similar to the distribution for i = 2.\n. Conclusion\nOur results show that the subspace of many categories and relations are low-dimensional. Moreover, we demonstrated that the first basis u1 is the “defining” vector that encodes the most general information about a category c (or a relation r): Letting v = vw for any w ∈ Dc (or v = va −vb for any (a,b) ∈ Dr ), the first coordinate v ·u1 has the largest magnitude, and the sign of v ·u1 is always positive. All other subsequent basis vectors ui for i ≥ 2 encode more “specific” information pertaining to individual words in Dc (or word pairs in Dr ).\nIt remains to be discovered what specific features are captured by these basis vectors for various categories and relations. For example, if Uk is a basis for the subspace of category c = country,\nFor each trial t ∈ {1, . . . ,T }, φ(U (t)k ,V (t) 2 ) is the capture rate attained in trial t, where V (t) 2 = {vw : w ∈ S (t) 2 } is the set of\nvectors for words in the testing set S (t) 2 (which is randomly generated in Step ), and U (t) k is the rank-k basis computed in Step . Recall that, in Step  of Algorithm ., we scale u1 by ±1 so that ∑ v∈V1 v ·u1 > 0.\nOn the Linear Structure of Word Embeddings Chapter . Low-rank subspaces | \nthen perhaps having a positive second coordinate vw ·u2 in the basis indicates that w is a developed country, and having a negative fourth coordinate vw · u4 indicates that country w is located in Europe. We leave this to future work.\nOn the Linear Structure of Word Embeddings Chapter . Low-rank subspaces | \n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n0 5 10 15 20 25 k\nφk\ncategory\nanimal\nmusic_genre\nmusical_instrument\norganism_classification\npolitician\nreligion\nsport\ntourist_attraction\ncountry\ncapital_city\ncurrency\nlanguage\n0.5\n0.6\n0.7\n0.8\n0 5 10 15 20 25 k\nφk\nrelation\ncapital_country\ncurrency_used\ncountry_language\ncountry−capital2\ncity−in−state\nFigure .: Results from the experiment in Section .., where we use cross-validation over T = 50 repeated trials to assess how much the low-rank basis Uk returned by GET_BASIS (Algorithm .) captures the subspace of a category c (or a relation r). In each trial t ∈ {1, . . . ,T }, we randomly split Vc (or Vr ) into a training set V (t) 1 and a testing set V (t) 2 , then compute a rank-k basis U (t) k for the subspace spanned by V (t)1 . For each rank k ∈ {1,2, . . .25}, we plot the average capture rate φ̄k := 1 T ∑T t=1φ(U (t) k ,V (t) 2 ) (.), where a higher capture rate means that the vectors in V2 have a large projection onto Uk . When rank is k = 1, φ̄k is already between 0.420 and 0.673. For ranks k ≥ 2 on the other hand, the increase from φ̄k−1 to φ̄k is relatively small. This shows that the first basis vector u1 is the “defining” vector that encodes the most information about a category c (or a relation r).\nOn the Linear Structure of Word Embeddings Chapter . Low-rank subspaces | \n● u2\nu1\n−1.0 −0.5 0.0 0.5 1.0\nanimal\n●u2 u1\n−1.0 −0.5 0.0 0.5 1.0\ncheese\n● u2 u1\n−1.0 −0.5 0.0 0.5 1.0\ncocktail\n● u2 u1\n−1.0 −0.5 0.0 0.5 1.0\ncountry\n●● u2 u1\n−1.0 −0.5 0.0 0.5 1.0\ncurrency\nu2 u1\n−1.0 −0.5 0.0 0.5 1.0\nlanguages\nu2 u1\n−1.0 −0.5 0.0 0.5 1.0\nmusical_instrument\nu2 u1\n−1.0 −0.5 0.0 0.5 1.0\nmusic_genre\nu2 u1\n−1.0 −0.5 0.0 0.5 1.0\norganism_group\nu2 u1\n−1.0 −0.5 0.0 0.5 1.0\npolitician\n●● ●u2 u1\n−1.0 −0.5 0.0 0.5 1.0\nreligion\nu2 u1\n−1.0 −0.5 0.0 0.5 1.0\nsport\n● ●●● ●u2 u1\n−1.0 −0.5 0.0 0.5 1.0\ntourist_attraction\nFigure .: For various categories c from Figure ., we randomly split Vc into a training set V1 and a testing set V2, then compute a rank-k basis Uk = {u1,u2, . . . ,uk} for the subspace spanned by V1. Here, we provide a boxplot of the distribution of {v ·u1}v∈V2 (denoted by u) and the distribution of {v · u2}v∈V2 (denoted by u). Note that for each category c, we have v · u1 > 0 for all v ∈ V2, whereas {v ·u2}v∈V2 is more evenly distributed around . The distribution of {v ·ui}v∈V2 for i ≥ 2 is similar to the distribution for i = 2, so we omit them here.\nOn the Linear Structure of Word Embeddings Chapter . Low-rank subspaces | \n●● ● ● ●● ●● ● u2 u1\n−1.0 −0.5 0.0 0.5 1.0\ncapital_country\n● ●● u2 u1\n−1.0 −0.5 0.0 0.5 1.0\ncity−in−state\n● u2 u1\n−1.0 −0.5 0.0 0.5 1.0\ncountry−capital2\nu2 u1\n−1.0 −0.5 0.0 0.5 1.0\ncountry_language\n●● ●● ● ● u2 u1\n−1.0 −0.5 0.0 0.5 1.0\ncurrency_used\nFigure .: For various relations r from Figure ., we randomly split Vr into a training set V1 and a testing set V2, then compute a rank-k basis Uk = {u1,u2, . . . ,uk} for the subspace spanned by V1. Here, we provide a boxplot of the distribution of {v · u1}v∈V2 (denoted by u) and the distribution of {v · u2}v∈V2 (denoted by u). Note that for eachrelation r, we have v · u1 > 0 for all v ∈ V2 (except for one outlier in r = capital_country, and one outlier in r = city-in-state), whereas {v ·u2}v∈V2 is more evenly distributed around . The distribution of {v · ui}v∈V2 for i ≥ 2 is similar to the distribution for i = 2, so we omit them here."
    }, {
      "heading" : "Chapter ",
      "text" : ""
    }, {
      "heading" : "Extending a knowledge base",
      "text" : "Let KB denote the current knowledge base, which consists of facts about the world that the machine currently knows. We assume that KB is incomplete, and that there are new facts to be learned. In other words, there exists categories c such that KB only knows a subset Sc ⊂ Dc of words that belong to c, and similarly, there exists relations r such that KB only knows a subset Sr ⊂ Dr of word pairs that satisfy r. Our goal is to discover new facts outside KB, such as words in D\\Sc that also belong to the category c, or word pairs in (D×D)\\Sr that also satisfy the relation r.\nIn Section ., we provide an algorithm called EXTEND_CATEGORY for discovering words in D\\Sc that also belong to c (see Algorithm .). Table . lists the top  words returned by EXTEND_CATEGORY for various categories, and shows that the algorithm performs very well. Then in Section ., we present an algorithm called EXTEND_RELATION for discovering new word pairs in (D ×D)\\Sr that also satisfy r (see Algorithm .). Its accuracy for returning correct word pairs is provided in Figure ..\nWe demonstrate that the low-dimensional subspace of categories and relations can be used to discover new facts with fairly low false-positive rates. The performance of EXTEND_RELATION (Algorithm .) is especially surprising, given the simplicity of the algorithm and the fact that it returns plausible word pairs out of all |D|(|D| − 1) ≈ 3.6e+09 possible word pairs in D×D.\n. Motivation\nIn Socher et. al’s paper [], a neural tensor network (NTN) model is used to learn semantic word vectors that can capture relationships between two words. More specifically, their NTN model computes a score of how plausible a word pair (a,b) satisfies a certain relationship r by the following function:\ng(va, r,vb) = b · f ( vTa W [1:m] r vb +θr [ va vb ] + cr ) (.)\nwhere va,vb ∈Rd are the vector representations of the words a,b respectively, f = tanh is a sigmoid function, andW [1:m]r ∈Rd×d×m is a tensor. The remaining parameters for relation r are the standard form of a neural network: θr ∈Rm×2d and b,cr ∈Rm.\nWith this model, however, we see a potential danger of overfitting the data because the number of parameters in the term vTa W [1:m] r vb in (.) is quadratic in d. Hence, our motivation is to employ\nOn the Linear Structure of Word Embeddings Chapter . Extending a knowledge base | \nfunction EXTEND_CATEGORY(Sc, k, δ): Returns a set of words in D\\Sc.\nInputs:\n• A subset Sc ⊂ Dc of words that belong to some category c\n• k ∈N, the rank of the basis for the subspace of category c\n• δ ∈ [0,1], a threshold value\nStep . Compute a rank-k basis Uk ∈Rd×k for the subspace of category c using Algorithm .:\nUk ← GET_BASIS({vw : w ∈ Sc}, k).\nLet u1 be the first (column) basis vector of Uk .\nStep . Return the set of words w ∈ D\\Sc whose vectors have (i) a positive first coordinate vw ·u1 in the basis Uk , and (ii) a large enough projection ‖vwUk‖ > δ in the subspace of category c:\n{w ∈ D\\Sc : vw ·u1 > 0, ‖vwUk‖ > δ} .\nend function\nAlgorithm .: EXTEND_CATEGORY(Sc, k,δ) returns a set of words in D\\Sc which are likely to belong to category c.\nthe linear structure of the word embeddings to fit a more general model. The low-dimensional subspace of categories, as demonstrated in Chapter , gives rise to a simple algorithm that allows one to discover new facts that are not in KB.\n. Learning new words in a category\nLet c be a category such that KB only knows a subset Sc ⊂ Dc of words that belong to c. We provide a method called EXTEND_CATEGORY (Algorithm .) for discovering new words in D\\Sc that also belong to c.\n.. Performance\nWe tested EXTEND_CATEGORY(Sc, k,δ) on various categories c from Figure ., using rank k = 10 and threshold δ = 0.6. Table . lists the top  words returned by the algorithm, where the words w ∈ D\\Sc are ordered in descending magnitude of the projection ‖vwUk‖ onto the subspace Uk of category c. The algorithm makes a few mistakes, e.g., it returns london as a tourist_attraction, and aren as a basketball_player. But overall, the algorithm seems to work very well and returns correct words that belong to the category.\nOn the Linear Structure of Word Embeddings Chapter . Extending a knowledge base | \n(a) c = classical_composer\nword projection schumann . beethoven . stravinsky . liszt . schubert .\n(b) c = sport\nword projection biking . volleyball . skiing . softball . soccer .\n(c) c = university\nword projection cambridge_university . university_of_california . new_york_university . stanford_university . yale_university .\n(d) c = basketball_player\nword projection dwyane_wade . aren . kobe_bryant . chris_bosh . tim_duncan .\n(e) c = religion\nword projection christianity . hinduism . taoism . buddhist . judaism .\n(f) c = tourist_attraction\nword projection metropolitan_museum_of_art . museum_of_modern_art . london . national_gallery . tate_gallery .\n(g) c = holiday\nword projection diwali . christmas . passover . new_year . rosh_hashanah .\n(h) c = month\nword projection august . april . october . february . november .\n(i) c = animal\nword projection horses . moose . elk . raccoon . goats .\n(j) c = asian_city\nword projection taipei . taichung . kaohsiung . osaka . tianjin .\nTable .: Given a set Sc ⊂ Dc of words belonging to a category c, EXTEND_CATEGORY (Algorithm .) returns new words in D\\Sc which are also likely to belong to c. Here, we list the top  words returned by EXTEND_CATEGORY(Sc, k,δ) for various categories c in Figure ., using rank k = 10 and threshold δ = 0.6. The words w ∈ D\\Sc are ordered in descending magnitude of the projection ‖vwUk‖ onto the category subspace. The algorithm makes a few mistakes, e.g., it returns london as a tourist_attraction, and aren as a basketball_player. But overall, the algorithm seems to work very well, and returns correct words that belong to the category.\nOn the Linear Structure of Word Embeddings Chapter . Extending a knowledge base | \n. Learning new word pairs in a relation\nLet r be a well-defined relation such that KB only knows a subset Sr ⊂ Dr of word pairs that satisfy r. We now present a method called EXTEND_RELATION (Algorithm .) for discovering new word pairs in (D×D)\\Sr that also satisfy r.\n.. Experiment\nWe tested EXTEND_RELATION on three well-defined relations from Figure .: capital_city, city_in_state, and currency_used. For each of the relations r, let Sr ⊆ Dr be the set of word pairs contained in the corresponding relation file (see Figure .(f)-(h)). We used cross-validation to assess the accuracy rate of Algorithm ., as follows. For each rank k ∈ {1,2, . . . ,9} and threshold δ ∈ {0.4,0.45, . . . ,0.75}, we repeated the following experiment over T = 50 trials:\nStep . Randomly partition Sr into a training set S1 and a testing set S2, where the training set size is |S1| = 0.3|Sr |. Let A := {a : (a,b) ∈ S2} and B := {b : (a,b) ∈ S2}.\nStep . Use Algorithm . to find a set S of word pairs in (D ×D)\\S1 which are likely to satisfy relation r:\nS ← EXTEND_RELATION(S1, {k,k,k}, {δ,δ,δ}).\nStep . Let S ′ := {(a,b) ∈ S : a ∈ A or b ∈ B}. For each answer (a,b) ∈ S ′ , we count it as correct if (a,b) ∈ S2, and incorrect otherwise. So the resulting accuracy of EXTEND_RELATION using parameters k and δ is\nacc(k,δ) := # correct answers # total answers = |S ′ ∩ S2| |S ′ | .\n.. Performance\nFor each rank k ∈ {1,2, . . . ,9} and threshold δ ∈ {0.4,0.45,0.5, . . . ,0.75}, we plot the average accuracy 1 T ∑T t=1 acc\n(t)(k,δ) over T = 50 trials in Figure .a. The table in Figure .b lists the parameters k and δ that attained the highest average accuracy. The algorithm achieved significantly higher accuracy for r = capital_city than for the other two relations; we discuss why in Section ... However, the performance of EXTEND_RELATION is still quite remarkable, considering the fact that it returns reasonable word pairs out of the (|D| 2 ) ≈ 1.8e+09 possible word pairs in D×D.\nAs the threshold δ is increased, the algorithm filters out more words in Step  and word pairs in Step  of the algorithm, resulting in a smaller number of word pairs returned by the algorithm. Figure . illustrates this effect for r = capital_city.\nNote that the algorithm’s accuracy can be further improved by fine-tuning the parameters. In our experiment (Section ..), we used the same rank k for kA, kB, and kr , and also used the same threshold δ for δA, δB, and δr ; but one can vary each of these parameters separately to achieve better performance.\nWe ignore the answers (a,b) ∈ S such that a < A and b < B, because we do not have an automated way of determining whether it is correct or incorrect. One could check each of these answers manually using an external knowledge source (e.g., Google search), but doing so would be very time-consuming. where acc(t)(k,δ) is the accuracy obtained in trial t.\nOn the Linear Structure of Word Embeddings Chapter . Extending a knowledge base | \nfunction EXTEND_RELATION(Sr , {kA, kB, kr }, {δA,δB,δr }): Returns a set of word pairs in (D×D)\\Sr .\nInputs:\n• A subset Sr ⊂ Dr of word pairs that satisfy some well-defined relation r. Let A := {a : (a,b) ∈ Sr } and B := {b : (a,b) ∈ Sr }. Let cA be the category such that, for all a ∈ A, a belongs to category cA. Similarly, let cB be the category such that, for all b ∈ B, b belongs to category cB.\n• kA, kB, kr ∈N, the rank of the basis for the subspaces of cA, cB, and r respectively\n• δA,δB,δr ∈ [0,1], threshold values\nStep . Use Algorithm . to get a set of words SA ⊆ D\\A whose vectors have a large enough projection (≥ δA) in the rank-kA subspace of category cA, and a set of words SB ⊆ D\\B whose vectors have a large enough projection (≥ δB) in the rank-kB subspace of category cB:\nSA← EXTEND_CATEGORY(A,kA,δA) SB← EXTEND_CATEGORY(B,kB,δB).\nStep . Compute a rank-kr basis for the subspace of relation r using Algorithm .:\nUkr ← GET_BASIS({va − vb : (a,b) ∈ Sr }, kr ).\nLet u1 be the first (column) basis vector of Ukr .\nStep . Return the set of word pairs (a,b) ∈ SA × SB whose difference vectors have (i) a positive first coordinate (va − vb) · u1 in the basis Ukr , and (ii) a large enough projection ‖(va − vb)Ukr ‖ > δr in the subspace of relation r:\n{(a,b) ∈ SA × SB : (va − vb) ·u1 > 0, ‖(va − vb)Uk‖ > δr } .\nend function\nAlgorithm .: EXTEND_RELATION(Sr , {kA, kB, kr }, {δA,δB,δr }) returns a set of word pairs (a,b) ∈ (D×D)\\Sr which are likely to satisfy relation r.\nOn the Linear Structure of Word Embeddings Chapter . Extending a knowledge base | \n.. Varying levels of difficulty for different relations\nWe provide two explanations as to why EXTEND_RELATION underperforms on relations such as city_in_state and currency_used:\n. For r = capital_city, there is a one-to-one mapping between the sets Ar := {a : (a,b) ∈ Sr } and Br := {b : (a,b) ∈ Sr }, whereas the same does not hold for r = city_in_state or r = currency_used (see Figure .). This causes the algorithm to return many false-positive answers for the relations city_in_state and currency_used, as we illustrate with an example below.\nConsider the relation r = currency_used. In the set Sr of word pairs contained in the relation file for r (see Figure .(h)), there are  country-currency pairs (a,b) ∈ Sr where b = franc. For  of these pairs {(a,franc) ∈ Sr }, the country word a belongs to the category c = african_country. Because the low-rank basis Ukr for relation r (computed in Step  of Algorithm .) tries to capture the vectors in the set {va − vfranc : (a, franc) ∈ Sr }, and the vectors of words belonging to the category c = african_country are clustered together, the algorithm returns many false-positive pairs consisting of an African country and the currency franc. For example, in many trial runs, the algorithm returns incorrect pairs such as (kenya, franc), (uganda, franc), and (sudan, franc). False-positive answers such as these cause the algorithm’s accuracy to drop.\n. Some relations are just inherently more difficult than others to represent using word vectors. For example, Figure . shows that solving analogy queries of the form “a:b::c:??” for pairs (a,b), (c,d) in the relation file for country-currency is more difficult than for pairs (a,b), (c,d) satisfying the relation country-capital. This may explain why EXTEND_RELATION performs worse on currency_used than on capital_city.\n. Conclusion\nWe have demonstrated that the low-dimensional subspace of categories and relations can be used to discover new facts with fairly low false-positive rates. The performance of EXTEND_RELATION (Algorithm .) is especially surprising, given the simplicity of the algorithm and the fact that it returns plausible word pairs out of all possible word pairs inD×D. The algorithms EXTEND_CATEGORY (Algorithm .) and EXTEND_RELATION (Algorithm .) are computationally efficient, are shown to drastically narrow down the search space for discovering new facts, and can be used to supplement other methods for extending knowledge bases.\nIn other words, given a word a ∈ Ar , there exists a unique word b ∈ Br such that (a,b) satisfies the relation r; and conversely, given a word b ∈ Br , there exists a unique word a ∈ Ar such that (a,b) satisfies r. country-currency is a smaller subset of the relation file for currency_used, and country-capital is a smaller subset of capital_city; see Figure ..\nOn the Linear Structure of Word Embeddings Chapter . Extending a knowledge base | \n0.2\n0.4\n0.6\n0.8\n0.4 0.5 0.6 0.7 δ\nac cu\nra cy\ncapital_country\n0.2\n0.4\n0.6\n0.8\n0.4 0.5 0.6 0.7 δ\nac cu\nra cy\ncity−in−state\n0.2\n0.4\n0.6\n0.8\n0.4 0.5 0.6 0.7 δ\nac cu\nra cy\nrank−1\nrank−2\nrank−3\nrank−4\nrank−5\nrank−6\nrank−7\nrank−8\nrank−9\ncurrency_used\n(a) For each rank k ∈ {1,2, . . . ,9} and threshold δ ∈ {0.4,0.45, . . . ,0.75}, we plot the average accuracy 1 T ∑T t=1 acc (t)(k,δ) over T = 50 trials, where for each trial t, acc(t)(k,δ) = # correct answers# total answers is the accuracy of the answers returned by EXTEND_RELATION(S (t) 1 , {k,k,k}, {δ,δ,δ}). (Here, each S (t) 1 ⊂ Dr is a random subset of the word pairs contained in the relation file of r (see Figure .(f)-(h)). S(t)1 is generated randomly, at each trial t, in Step  of the experiment in Section ... ).\nr maxk,δ acc(k,δ) rank k threshold δ capital_city .  . city_in_state .  . currency_used .  .\n(b) For each relation r, we list the parameters k and δ that achieved the highest average accuracy in (a).\nFigure .: Given a set Sr ⊂ Dr of word pairs satisfying a relation r, EXTEND_RELATION (Algorithm .) returns new word pairs in (D×D)\\Sr which are also likely to satisfy relation r. We use crossvalidation to assess the accuracy rate of EXTEND_RELATION on three well-defined relations from Figure .(f)-(h). Note that EXTEND_RELATION performs very well on r = capital_city, achieving accuracy as high as 0.909. We discuss why the accuracy is lower for the other two relations in Section ... For details about the experiment, see Section ...\nOn the Linear Structure of Word Embeddings Chapter . Extending a knowledge base | \n(a) r = capital_city\ncA = country cB = city\njapan\ngermany\ncanada\ntokyo\nberlin\nottawa\n(b) r = city_in_state\ncA = city\ncB = us_state trenton\nnewark\nsan_francisco\nlos_angeles\nmountain_view\nnew_jersey\ncalifornia\n(c) r = currency_used\ncA = country cB = currency\nalgeria\nserbia_montenegro\ngermany\nfrance\ndinar\neuro\nfranc\nFigure .: For r = capital_city, there is a one-to-one mapping between Ar := {a : (a,b) ∈ Sr } and Br := {b : (a,b) ∈ Sr }, since a country has exactly one capital city. The same does not hold for (b) or (c), however: (b) Each U.S. state contains multiple cities, and some cities in different U.S. states have identical names (e.g., both California and New Jersey have a city named Newark). (c) A country can have multiple currencies (either concurrently or over history), and the same currency can be used in multiple countries. This causes EXTEND_RELATION to return a higher number of false-positive (incorrect) answers for (b) and (c); see Section .. for a detailed explanation.\nOn the Linear Structure of Word Embeddings Chapter . Extending a knowledge base | \n0\n25\n50\n75\n0.4 0.5 0.6 0.7 δ\nrank−2\n0\n25\n50\n75\n0.4 0.5 0.6 0.7 δ\nrank−7\n0\n25\n50\n75\n0.4 0.5 0.6 0.7 δ\nrank−8\nFigure .: Let r = capital_city, and let S1 ⊂ Dr be a random subset of the word pairs contained in the relation file of r (see Figure .(g)). We plot the number of correct (blue) and incorrect (red) word pairs returned, respectively, by calling EXTEND_RELATION(S1, {k,k,k}, {δ, δ, δ}) for varying threshold values δ (see Algorithm .). We only provide plots for the ranks k ∈ {2,7,8}; the plots for other ranks are similar. As the threshold δ is increased, the algorithm filters out more word pairs in Steps  and  of the algorithm, resulting in a smaller number of word pairs returned by the algorithm."
    }, {
      "heading" : "Chapter ",
      "text" : "Learning vectors for less frequent words\nSince the size of the co-occurrence data is quadratic in the size of the vocabulary D, and since the co-occurrence data for infrequent words are too noisy to generate good word vectors with, we restrict the vocabulary D to only the words that appear at least m0 = 1000 times in the corpus. Hence, we do not have vectors for words that appear fewer than 1000 times in the corpus. These words include the famous composer claude_debussy ( times), Malaysian currency ringgit ( times), the famous actor adam_sandler ( times), and the historical event boston_massacre ( times). In order to continually extend the knowledge base KB, it becomes necessary to learn vectors for these less frequent words.\nWe demonstrate that, using the low-dimensional subspace of categories, one can substantially reduce the amount of co-occurrence data needed to learn vectors for words. In particular, we present an algorithm called LEARN_VECTOR (Algorithm .) for learning vectors of words with only a small amount of co-occurrence data. We test the algorithm on seven words ŵ (listed in Table .) which we already have vectors for, and compare the “true” vector vŵ ∈ VD to the vector v̂ returned by LEARN_VECTOR. The algorithm’s performance is given in Figure .. In general, the algorithm achieves very good performance while using only a small fraction of the total amount of co-occurrence data in the Wikipedia corpus C.\nOne can extend this method to learn vectors for any words – even words that do not appear at all in the Wikipedia corpus – using web-scraping tools, such as Google search, to obtain additional co-occurrence data.\n. Learning a new vector\nLet ŵ be a word such that (i) we know the category c that ŵ belongs in, and (ii) we have a small corpus Γ (where |Γ | |C|) containing co-occurrence data for ŵ. Then we provide a method LEARN_VECTOR for learning its word vector v̂ ∈Rd (see Algorithm .). There are a total of 283,847 words that occur between  and  times in the corpus, which are not included in D.\nOn the Linear Structure of Word Embeddings Chapter . Learning vectors | \nfunction LEARN_VECTOR(ŵ, c,Γ , k,η,λ): Returns a learned vector v̂ ∈Rd for word ŵ.\nInputs:\n• ŵ, a word\n• c, the category that ŵ belongs in. Let Vc := {vw : w ∈ Dc} be the vectors of words belonging to c.\n• Γ , a small corpus containing co-occurrence data for ŵ\n• k ∈N, the rank of the basis for the category subspace\n• η ∈ (0,1], the learning rate for Adagrad\n• λ > 0, the weight of the regularization term in the objective (.)\nStep . Compute a rank-k basis Uk ∈Rd×k for the subspace of category c using Algorithm .:\nUk ← GET_BASIS(Vc, k).\nStep . Consider only the set D ′ of vocabulary words that appear at least m′0 = 10 times in Γ . For each word w ∈ D ′ , compute Yŵw, the number of times word w appears in any context window around ŵ in Γ .\nStep . Use Adagrad with learning rate η to train parameters v̂ ∈ Rd , b ∈ Rk , and Z ∈ R so as to minimize the objective∑\nw∈D ′∩D g(Yŵw)\n( ||v̂ + vw ||2 − log(Yŵw) +Z )2 +λ‖v̂ −Ukb‖2, (.)\nwhere {vw}w∈D are the already-learned word vectors, and g(x) := min {( x 10 )0.75 ,1 } . Note\nthat v̂, b, and Z are initialized randomly.\nStep . Normalize v̂ so that ‖v̂‖ = 1.\nStep . Return v̂, the learned word vector for ŵ.\nend function\nAlgorithm .: LEARN_VECTOR(ŵ, c,Γ , k,η,λ) returns a vector v̂ ∈ Rd for word ŵ learned using the objective (.).\nOn the Linear Structure of Word Embeddings Chapter . Learning vectors | \nŵ c k Xŵ california us_state  .e+ christianity religion  .e+ germany country  .e+ hinduism religion  .e+ japan country  .e+ massachusetts us_state  .e+ princeton university  .e+\nTable .: In the experiment, we train vectors for the words ŵ by minimizing the objective (.). c is the category that ŵ belongs in. For the words california, germany, and japan, we used k = 10 as the rank of the basis of the category subspace; for the remaining four words, we used rank k = 3. Xŵ := ∑ w∈CXŵw measures the amount of co-occurrence data for ŵ in the original corpus C.\n.. Motivation behind the optimization objective\nThe objective (.) is nearly identical to the Squared Norm (SN) objective (.), except for a few differences: (i) we have an additional regularization term λ‖v̂−Ukb‖2, (ii) the co-occurrence counts Yŵw are taken from the smaller corpus Γ , and (iii) the summation is taken over words w ∈ D ′ ∩D. Note that b has a closed-form solution, since to minimize ‖v̂ −Ukb‖2, one can just take b to be the projection of v̂ onto Uk . The regularization term λ‖v̂ −Ukb‖2 serves as a prior knowledge, forcing the new vector v̂ to be trained near the subspace Uk of category c, but also allowing v̂ to lie outside the subspace. The hope is that the regularization term reduces the amount of co-occurrence data needed to fit v̂. Note that in general, the regularization weight λ should be decreasing in the size of Γ , since less prior knowledge is needed with more data.\n. Experiment\nFor our experiment, we chose seven words ŵ (listed in Table .) which we already have vectors for, fitted a vector v̂ using LEARN_VECTOR (Algorithm .), and compared v̂ to the true vector vŵ ∈ VD (see Figure .). We withheld the true vector vŵ ∈ VD from training, by taking ŵ out of the summation over D ′ ∩D in the objective (.). For the words california, germany, and japan, we used k = 10 as the rank of the basis of the category subspace; for the remaining four words, we used rank k = 3.\nFor each word ŵ, we use Yŵ(Γ ) := ∑ w∈D ′ Yŵw to quantify the amount of co-occurrence data for word ŵ in a corpus Γ with vocabulary set D ′ , and evaluate the algorithm in Section . for varying values of Yŵ(Γ ). More specifically, we extracted six subcorpora Γ1, . . . ,Γ6 from the original corpus C, where Γ1 ⊂ Γ2 ⊂ · · · ⊂ Γ6 ⊂ C and Yŵ(Γ1) < Yŵ(Γ2) < · · · < Yŵ(Γ6) Yŵ(C) = Xŵ. For each i ∈ {1, . . . ,6}, we ran the algorithm with various learning rates ηi and regularization weights λi ; Table . lists the parameter values that resulted in the best performance for each corpus size and each word.\n. Performance\nWe evaluate the performance of LEARN_VECTOR by considering the order and the cosine score of the learned vector v̂ returned by the algorithm, defined below.\nOn the Linear Structure of Word Embeddings Chapter . Learning vectors | \n.. Order and cosine score of the learned vector\nLet ŵ be one of the seven words we trained a vector for, v̂ the learned vector returned by the algorithm, and vŵ ∈ VD the “true” vector for word ŵ. Number the vectors v1,v2, . . . , v|D| ∈ VD in order of decreasing cosine similarity from v̂, so that v1 · v̂ > v2 · v̂ > · · · > v|D| · v̂. Then the order of v̂ is the number k ∈N such that vŵ = vk , i.e., the true vector vŵ has the kth largest cosine similarity from v̂ out of all the words in D. The cosine score of v̂ is the cosine similarity between the true vector vŵ and the vector v̂ returned by the algorithm, i.e., vŵ · v̂.\n.. Evaluation\nIn Figure ., we provide a plot of the order and cosine score of v̂ for varying values of ln(Yŵ), where Yŵ := ∑ w∈D ′ Yŵw is the amount of co-occurrence data for ŵ in the training corpus Γ with vocabulary set D ′ . Note that in general, the order of v̂ is decreasing, and the cosine score of v̂ is increasing, in the amount of co-occurrence data Yŵ. Moreover, the algorithm seems to achieve a higher cosine score by using a smaller rank k for the basis of the category subspace: The words for which rank k = 10 was used (california, germany, and japan) have lower cosine scores than the words for which rank k = 3 was used.\nWe provide an explanation as to why using a smaller rank k improves the algorithm’s performance. For a category c, let Uk be a rank-k basis for the subspace of c. The regularization term λ‖v −Ukb‖ in the objective (.) serves to train v̂ near the subspace Uk , which by definition only captures the general notion of category c. Recall from Chapter  that the first basis vector u1 of Uk is the defining vector that encodes the most information about c, while the subsequent basis vectors ui for i ≥ 2 capture more specific information about individual words in c. By using a smaller rank k, we throw away the more “noisy” vectors ui for i ≥ k ≥ 1, allowing Uk to capture the generation notion of category c better. This allows the regularization term to train the “category” component of v̂ more accurately. Note that the other component, which is specific to word ŵ and lies outside\nthe category subspace, is trained by the term g(Yww′ ) ( ||v + vw′ ||2 − log(Yww′ ) +Z )2 in the objective (.). To illustrate our point, we trained two vectors for the same word ŵ, one using a low rank k and the other using a high rank k, and compared their order and cosine score (see Figure .). For both ŵ = massachusetts and ŵ = hinduism, the vector learned using the lower rank resulted in a lower order and a much higher cosine score. This demonstrates that using a smaller rank k results in better performance for LEARN_VECTOR.\nTo compare the amount of co-occurrence data for ŵ in a subcorpus Γ to the amount of cooccurrence data for ŵ in the Wikipedia corpus C, we look at the fraction Yŵ(Γ )/Xŵ, which is listed in Table .. Note that the algorithm achieves very good performance while using only a small fraction of the total amount of co-occurrence data in the Wikipedia corpus C. For example, by using only Yŵ(Γ )/Xŵ = 1.846e−02 of the total amount of co-occurrence data for the word w = christianity, the algorithm is able to learn a vector whose order is  and cosine score is 0.84.\nLastly, note that the performance depends heavily on the parameter values chosen, and can be further improved by fine-tuning the parameters.\nOn the Linear Structure of Word Embeddings Chapter . Learning vectors | \n0\n10\n20\n30\n6 7 8 9 ln(Yŵ)\nor de\nr\n0.6\n0.7\n0.8\n6 7 8 9 ln(Yŵ)\nco si\nne s\nco re\nword\ncalifornia\nchristianity\ngermany\nhinduism\njapan\nmassachusetts\nprinceton\nFigure .: The order and cosine score of the learned vector v̂ returned by LEARN_VECTOR (Algorithm .) for word ŵ, using varying amounts of co-occurrence data Yŵ. (See Section .. for the definitions of order and cosine score.) Note that in general, the order of v̂ is decreasing, and the cosine score of v̂ is increasing, in the amount of co-occurrence data Yŵ. (The occasional decrease in the cosine score is due to random initialization of the vector v̂.) Also, observe that the words for which rank k = 10 was used (california, germany, and japan) have lower cosine scores than the words for which rank k = 3 was used. The algorithm achieves very good performance while using only a small fraction of the total amount of co-occurrence data in the Wikipedia corpus C: For example, by using only Yŵ(Γ )/Xŵ = 1.846e−02 of the total amount of co-occurrence data for the word w = christianity, the algorithm is able to learn a vector whose order is  and cosine score is 0.84.\n. Conclusion\nWe have demonstrated that, in principle, one can learn vectors with substantially less data by using the low-dimensional subspace of categories. An interesting experiment to try is the following: Use Algorithm . to learn vectors for rare words, and see if new facts can be discovered using these vectors. We leave this to future work.\nMoreover, one can extend this method to learn vectors for any words – even words that do not appear at all in the Wikipedia corpus – using web-scraping tools, such as Google search, to obtain additional co-occurrence data. However, the corpora obtained from Google search may be drawn from a different distribution than the wikipedia corpus, and hence skew the data in a certain way. We leave this to future work.\nOne weakness of LEARN_VECTOR is that it requires having prior knowledge of what category a word ŵ belongs in. If our prior knowledge is wrong, then the fitted vector for ŵ may be very bad. One could come up with an automatic method classifying which category w belongs to.\nOn the Linear Structure of Word Embeddings Chapter . Learning vectors | \n(a) w = california\ni lnYw(Γi ) Yw(Γi )/Xw ηi λi 1 . .e- .e- . 2 . .e- .e- . 3 . .e- .e- . 4 . .e- .e- . 5 . .e- .e- . 6 . .e- .e- .\n(b) w = christianity\ni lnYw(Γi ) Yw(Γi )/Xw ηi λi 1 . .e- .e- . 2 . .e- .e- . 3 . .e- .e- . 4 . .e- .e- . 5 . .e- .e- . 6 . .e- .e- .\n(c) w = germany\ni lnYw(Γi ) Yw(Γi )/Xw ηi λi 1 . .e- .e- . 2 . .e- .e- . 3 . .e- .e- . 4 . .e- .e- . 5 . .e- .e- . 6 . .e- .e- .\n(d) w = hinduism\ni lnYw(Γi ) Yw(Γi )/Xw ηi λi 1 . .e- .e- . 2 . .e- .e- . 3 . .e- .e- . 4 . .e- .e- . 5 . .e- .e- . 6 . .e- .e- .\n(e) w = japan\ni lnYw(Γi ) Yw(Γi )/Xw ηi λi 1 . .e- .e- . 2 . .e- .e- . 3 . .e- .e- . 4 . .e- .e- . 5 . .e- .e- . 6 . .e- .e- .\n(f) w = massachusetts\ni lnYw(Γi ) Yw(Γi )/Xw ηi λi 1 . .e- .e- . 2 . .e- .e- . 3 . .e- .e- . 4 . .e- .e- . 5 . .e- .e- . 6 . .e- .e- .\n(g) w = princeton\ni lnYw(Γi ) Yw(Γi )/Xw ηi λi 1 . .e- .e- . 2 . .e- .e- . 3 . .e- .e- . 4 . .e- .e- . 5 . .e- .e- . 6 . .e- .e- .\nTable .: For each word ŵ, we extracted six subcorpora Γ1, . . . ,Γ6 from the original corpus C, where Γ1 ⊂ Γ2 ⊂ · · · ⊂ Γ6. We use Yŵ(Γ ) := ∑ w∈D ′ Yŵw to quantify the amount of co-occurrence data for word ŵ in a corpus Γ with vocabulary set D ′ . For each i ∈ {1, . . . ,6}, we trained the algorithm on the subcorpus Γi for various learning rates ηi and regularization weights λi . In the tables above, we list the parameter values ηi , λi that resulted in the best performance (shown in Figure .). To compare the amount of co-occurrence data for ŵ in Γ to the amount of co-occurrence data for ŵ in C, we look at the fraction Yŵ(Γ )/Xŵ.\nOn the Linear Structure of Word Embeddings Chapter . Learning vectors | \n0\n10\n20\n30\n6 7 8 9 ln(Yŵ)\nor de\nr\n0.5\n0.6\n0.7\n0.8\n6 7 8 9 ln(Yŵ)\nco si\nne s\nco re\nrank−10\nrank−3\n(a) ŵ = massachusetts\n0\n50\n100\n150\n6 7 8 9 ln(Yŵ)\nor de\nr\n0.4\n0.5\n0.6\n0.7\n0.8\n6 7 8 9 ln(Yŵ)\nco si\nne s\nco re\nrank−15\nrank−3\n(b) ŵ = hinduism\nFigure .: Using LEARN_VECTOR (Algorithm .), we trained two vectors for the same word ŵ, one using a low rank k (blue) and the other using a high rank k (red), and compared their order and cosine score. (For each rank k, we tried various learning rates η and regularization weights λ to try to optimize performance; here, we provide the best performance found for each k.) For both ŵ = massachusetts and ŵ = hinduism, the vector learned using the lower rank resulted in a lower order and a much higher cosine score. This demonstrates that using a smaller rank k results in better performance for LEARN_VECTOR."
    }, {
      "heading" : "Chapter ",
      "text" : "Using an external knowledge source to reduce false-positive rate\nOne can use external knowledge sources such as a dictionary or Wordnet [] to filter false-positive answers and improve accuracy on analogy queries. In this section, we focus on analogy queries “a:b::c:??” where there exists categories cA, cB such that both a and c belong in cA, and both b and the correct answer d belong in cB. In other words, (a,b) and (c,d) both satisfy a common relation r that is well-defined.\nSOLVE_QUERY (Algorithm .) is a generic method for returning the top N answers to an analogy query “a:b::c:??”. In Section ., we present two ways for filtering the candidate list of answers to reduce false-positive rate: POS filter and LEX filter. Figures ., ., and . compare the accuracy of SOLVE_QUERY with and without these filters on  different relations r. We show that the POS filter always increases the accuracy (either slightly or significantly, depending on the relation r), unless the accuracy is already 100% without filter. On the other hand, the performance for the LEX filter varies widely, depending on the nature of the relation r. When used on appropriate relations r, such as facts-based relations (see Figure .(a)-(h)), the LEX filter can improve the accuracy by as much as +19.2% than without filter, and +17.7% than with POS filter (see Figure .(a)).\n. Analogy queries\nRecall that word embeddings allow one to solve analogy queries of the form “a:b::c:??” using simple vector arithmetics. More specifically, for two word pairs (a,b), (c,d) satisfying a common relation r, their word vectors satisfy\nva − vb ≈ vc − vd .\nHence, a method to solve the analogy query “a:b::c:??” is to find the word d ∈ D whose vector vd is closest to vb − va + vc.\nGiven a set of words ∆ ⊆ D and a numberN ∈ {1, . . . , |∆|}, SOLVE_QUERY (Algorithm .) returns the top N answers in ∆ for the analogy query “a:b::c:??”. More specifically, it returns N words in ∆ corresponding to the top N vectors in V∆ := {vw : w ∈ ∆} that are closest to the vector vb − va + vc. It is also used in other works (e.g. [, , ]) to evaluate a method’s performance on analogy tasks.\nOn the Linear Structure of Word Embeddings Chapter . Using external knowledge | \nfunction SOLVE_QUERY({a,b,c},∆,N ): Returns a list of N words in ∆.\nInputs:\n• Words a,b,c for which we want to solve the analogy query “a:b::c:??”\n• ∆ ⊆ D, a set of candidate answers\n• N ∈ {1,2, . . . , |∆|}, the number of answers to return.\nStep . Let V∆ := {vw : w ∈ ∆}. Number the vectors v1,v2, . . . , v|∆| ∈ V∆ in order of decreasing cosine similarity from the vector vb − va + vc. Let S := {v1,v2, . . . ,vN }.\nStep . Return the list {w ∈ ∆ : vw ∈ S}.\nend function\nAlgorithm .: SOLVE_QUERY({a,b,c},∆,N ) returns top N answers from ∆ for the analogy query “a:b::c:??”.\nWe say SOLVE_QUERY returns the correct answer if the correct answer d is in the set returned by the algorithm.\n. Wordnet\nIn Wordnet, each word is labeled with POS (“part-of-speech”) and LEX (“lexicographic”) tags. The POS tag indicates the syntactic category of a word, such as noun, verb, adjective, and adverb. The LEX tag is more specific: See Table . for a complete list of the LEX tags in Wordnet. For any word w ∈ D, let pos(w) and lex(w) be the set of POS and LEX tags for w, respectively. For example, for the currency word w = euro,\npos(euro) = {noun}, lex(euro) = {noun.quantity}.\nDefine the following sets:\nDpos(w) := {w′ ∈ D : pos(w′)∩pos(w) , ∅}, Dlex(w) := {w′ ∈ D : lex(w′)∩ lex(w) , ∅}.\nIn other words, Dpos(w) is the set of words that share a common POS tag with w, and similarly, Dlex(w) is the set of words that share a common LEX tag with w. For example, Dpos(euro) contains all the noun words in D, and Dlex(euro) contains words such as dollar and kilometer which have the LEX tag noun.quantity.\nConsider the analogy query “a:b::c:??” where there exists categories cA and cB such that both a and c belong in cA, and both b and the correct answer d belong in cB. If we assume that every\nOn the Linear Structure of Word Embeddings Chapter . Using external knowledge | \nword in category cB share a common POS (or LEX) tag, then we can use Wordnet to filter out words in D which cannot belong in cB. More specifically, we only search among the words in Dpos(b) (or Dlex(b)) for the correct answer d. Note that LEX is a stronger filter than POS, in the sense that Dlex(b) ⊂ Dpos(b).\n. Experiment\nWe tested SOLVE_QUERY (Algorithm .) on  relations from Figure . in the following manner. For each relation r, let Sr ⊂ Dr be the set of word pairs contained in the correponding relation file (see Figure .). For each N ∈ {1,5,10,25,50}, we performed the following experiment:\nStep . Initialize n = npos = nlex = 0.\nStep . For each (a,b) ∈ Sr , and for each (c,d) ∈ Sr such that (c,d) , (a,b), solve the analogy query “a:b::c:??” using Algorithm .:\nS← SOLVE_QUERY({a,b,c},D,N ) Spos← SOLVE_QUERY({a,b,c},Dpos(b),N ) Slex← SOLVE_QUERY({a,b,c},Dlex(b),N ).\nWe say S, Spos, and Slex are the answers returned by the algorithm without filter, with POS filter, and with LEX filter, respectively.\n• If d ∈ S, then increment n by . • If d ∈ Spos, then increment npos by . • If d ∈ Slex, then increment nlex by .\nIn other words, we test the algorithm on the analogy queries “a:b::c:??” and “c:d::a:??” for every possible pairs (a,b), (c,d) ∈ Sr . The total number of times the algorithm returns the correct answer without filter, with POS filter, and with LEX filter are n, npos, and nlex, respectively. Since the total number of analogy queries tested on is |Sr |(|Sr |−1), the accuracy of the algorithm without filter, with POS filter, and with LEX filter are given by n|Sr |(|Sr |−1) , npos |Sr |(|Sr |−1) , and nlex|Sr |(|Sr |−1) , respectively.\n. Performance\nFigures ., ., and . compare the accuracy of SOLVE_QUERY with and without filters on  different relations r, which are taken from Figure .. We ignore the performance on relation (n) gram-nationality-adj in Figure ., due to the fact that Wordnet does not have an entry for the word “argentinean” which is included in the test bed.\nDepending on the relation r, the POS filter always increases the accuracy, either slightly (see (a), (c), (j), (l), (m), (o)-(z)) or significantly (see (i)), unless the accuracy is already 100% without filter (see (b), (d), (e), (f), (k)).\nSo for all analogy queries “a:b::c:??” where either b or the correct answer d is the word argentinean, both POS and LEX filter out the correct answer from D, causing the algorithm to get the analogy query wrong and therefore lower its accuracy slightly.\nOn the Linear Structure of Word Embeddings Chapter . Using external knowledge | \n# LEX tag Description  adj.all all adjective clusters  adj.pert relational adjectives (pertainyms)  adv.all all adverbs  noun.Tops unique beginner for nouns  noun.act nouns denoting acts or actions  noun.animal nouns denoting animals  noun.artifact nouns denoting man-made objects  noun.attribute nouns denoting attributes of people and objects  noun.body nouns denoting body parts  noun.cognition nouns denoting cognitive processes and contents  noun.communication nouns denoting communicative processes and contents  noun.event nouns denoting natural events  noun.feeling nouns denoting feelings and emotions  noun.food nouns denoting foods and drinks  noun.group nouns denoting groupings of people or objects  noun.location nouns denoting spatial position  noun.motive nouns denoting goals  noun.object nouns denoting natural objects (not man-made)  noun.person nouns denoting people  noun.phenomenon nouns denoting natural phenomena  noun.plant nouns denoting plants  noun.possession nouns denoting possession and transfer of possession  noun.process nouns denoting natural processes  noun.quantity nouns denoting quantities and units of measure  noun.relation nouns denoting relations between people or things or ideas  noun.shape nouns denoting two and three dimensional shapes  noun.state nouns denoting stable states of affairs  noun.substance nouns denoting substances  noun.time nouns denoting time and temporal relations  verb.body verbs of grooming, dressing and bodily care  verb.change verbs of size, temperature change, intensifying, etc.  verb.cognition verbs of thinking, judging, analyzing, doubting  verb.communication verbs of telling, asking, ordering, singing  verb.competition verbs of fighting, athletic activities  verb.consumption verbs of eating and drinking  verb.contact verbs of touching, hitting, tying, digging  verb.creation verbs of sewing, baking, painting, performing  verb.emotion verbs of feeling  verb.motion verbs of walking, flying, swimming  verb.perception verbs of seeing, hearing, feeling  verb.possession verbs of buying, selling, owning  verb.social verbs of political and social activities and events  verb.stative verbs of being, having, spatial relations  verb.weather verbs of raining, snowing, thawing, thundering  adj.ppl participial adjectives\nTable .: List of all LEX tags in Wordnet.\nOn the Linear Structure of Word Embeddings Chapter . Using external knowledge | \nOn the other hand, the performance for the LEX filter varies widely depending on the nature of the relation r, due to the fact that LEX is a stronger filter than POS. For relations where words belonging to cB share a common LEX tag (e.g., (a), (c), (i), (j), (l), (r)-(v), (z)), LEX improves the accuracy significantly by filtering out false-positive answers. On the contrary, for relations where words belonging to cB have different LEX tags (e.g., (m), (o)-(q), (w), (y)), LEX filters out the correct answer from D, and hence worsens the accuracy significantly. When used on appropriate relations r, such as facts-based relations (see Figure .(a)-(h)), the LEX filter can improve the accuracy by as much as +19.2% than without filter, and +17.7% than with POS filter (see the plot in Figure .(a) for N = 50).\n. Conclusion\nWe have shown that external knowledge sources such as Wordnet can be used to improve accuracy on analogy queries, sometimes significantly, by filtering out false-positive answers. As an extension of the idea, we can apply the Wordnet filter to EXTEND_CATEGORY (Algorithm .) for learning new words in a category, or EXTEND_RELATION (Algorithm .) for learning new word pairs in a relation, to decrease the false-positive rate and improve its performance. We leave this to future work.\nOn the Linear Structure of Word Embeddings Chapter . Using external knowledge | \n0.00\n0.25\n0.50\n0.75\n1.00\n0 10 20 30 40 50 N\nac cu\nra cy\n(a) country−currency\n0.00\n0.25\n0.50\n0.75\n1.00\n0 10 20 30 40 50 N\nac cu\nra cy\n(b) holiday−month\n0.00\n0.25\n0.50\n0.75\n1.00\n0 10 20 30 40 50 N\nac cu\nra cy\n(c) holiday−religion\n0.00\n0.25\n0.50\n0.75\n1.00\n0 10 20 30 40 50 N\nac cu\nra cy\n(d) country−language\n0.00\n0.25\n0.50\n0.75\n1.00\n0 10 20 30 40 50 N\nac cu\nra cy\n(e) country−capital2\n0.00\n0.25\n0.50\n0.75\n1.00\n0 10 20 30 40 50 N\nac cu\nra cy\n(f) city−in−state\nFigure .: Accuracy of the algorithm without filter (green), with POS filter (blue), and with LEX filter (red) on the facts-based relation files from Figure .(a)-(f). For (a) and (c), the LEX filter improves the accuracy significantly, while the POS filter improves the accuracy only slightly. For all other relation files, the algorithm already achieves an accuracy of  without filter.\nOn the Linear Structure of Word Embeddings Chapter . Using external knowledge | \n0.00\n0.25\n0.50\n0.75\n1.00\n0 10 20 30 40 50 N\nac cu\nra cy\n(i) gram1−adj−adv\n0.00\n0.25\n0.50\n0.75\n1.00\n0 10 20 30 40 50 N\nac cu\nra cy\n(j) gram2−opposite\n0.00\n0.25\n0.50\n0.75\n1.00\n0 10 20 30 40 50 N\nac cu\nra cy\n(k) gram3−comparative\n0.00\n0.25\n0.50\n0.75\n1.00\n0 10 20 30 40 50 N\nac cu\nra cy\n(l) gram4−superlative\n0.00\n0.25\n0.50\n0.75\n1.00\n0 10 20 30 40 50 N\nac cu\nra cy\n(m) gram5−present−particle\n0.00\n0.25\n0.50\n0.75\n1.00\n0 10 20 30 40 50 N\nac cu\nra cy\n(n) gram6−nationality−adj\n0.00\n0.25\n0.50\n0.75\n1.00\n0 10 20 30 40 50 N\nac cu\nra cy\n(o) gram7−past−tense\n0.00\n0.25\n0.50\n0.75\n1.00\n0 10 20 30 40 50 N\nac cu\nra cy\n(p) gram8−plural\n0.00\n0.25\n0.50\n0.75\n1.00\n0 10 20 30 40 50 N\nac cu\nra cy\n(q) gram9−plural−verbs\nFigure .: Accuracy of the algorithm without filter (green), with POS filter (blue), and with LEX filter (red) on the grammar-based relation files from Figure .(i)-(q). For (n), both the LEX filter and the POS filter worsen the accuracy slightly, due to the fact that Wordnet does not have an entry for the word “argentinean”. For all other relation files, POS improves the accuracy by a modest amount. LEX improves the accuracy for (i), (j), and (l), but performs very poorly for (m), (o)-(q), due to the fact that words belonging to cB have different LEX tags, causing LEX to filter out the correct answer.\nOn the Linear Structure of Word Embeddings Chapter . Using external knowledge | \n0.00\n0.25\n0.50\n0.75\n1.00\n0 10 20 30 40 50 N\nac cu\nra cy\n(r) associated−number\n0.00\n0.25\n0.50\n0.75\n1.00\n0 10 20 30 40 50 N\nac cu\nra cy\n(s) associated−position\n0.00\n0.25\n0.50\n0.75\n1.00\n0 10 20 30 40 50 N\nac cu\nra cy\n(t) common−very\n0.00\n0.25\n0.50\n0.75\n1.00\n0 10 20 30 40 50 N\nac cu\nra cy\n(u) graded−antonym\n0.00\n0.25\n0.50\n0.75\n1.00\n0 10 20 30 40 50 N\nac cu\nra cy\n(v) has−characteristic\n0.00\n0.25\n0.50\n0.75\n1.00\n0 10 20 30 40 50 N\nac cu\nra cy\n(w) has−function\n0.00\n0.25\n0.50\n0.75\n1.00\n0 10 20 30 40 50 N\nac cu\nra cy\n(x) has−skin\n0.00\n0.25\n0.50\n0.75\n1.00\n0 10 20 30 40 50 N\nac cu\nra cy\n(y) noun−baby\n0.00\n0.25\n0.50\n0.75\n1.00\n0 10 20 30 40 50 N\nac cu\nra cy\n(z) senses\nFigure .: Accuracy of the algorithm without filter (green), with POS filter (blue), and with LEX filter (red) on the semantics-based relation files from Figure .(r)-(z). LEX filter worsens the accuracy for (w) and (y), but improves the accuracy significantly on all other relation files. POS filter consistently improves the accuracy on all relation files."
    }, {
      "heading" : "Chapter ",
      "text" : ""
    }, {
      "heading" : "Conclusion",
      "text" : "We have demonstrated that the linear algebraic structure of word embeddings can be used to reduce data requirements for methods of learning facts. In particular, we demonstrated that categories and relations form a low-rank subspace Uk = {u1, . . . ,uk} in the projected space (Chapter ), and this subspace can be used to discover new facts with fairly low false-positive rates () and learn new vectors for words with substantially less co-occurrence data (Chapter ).\nIn Chapter , we demonstrated that the first basis vector u1 of a low-rank subspace encodes the most general information about a category c (or a relation r), whereas the subsequent basis vectors ui for i ≥ 2 encode more “specific” information pertaining to individual words in Dc (or word pairs in Dr ). It remains to be discovered what specific features are captured by these basis vectors for various categories and relations. For example, if Uk is a basis for the subspace of category c = country, then perhaps having a positive second coordinate vw · u2 in the basis indicates that w is a developed country, and having a negative fourth coordinate vw · u4 indicates that country w is located in Europe. We leave this to future work.\nIn Chapter , we used the low-dimensional subspace of categories and relations to discover new facts with fairly low false-positive rates. The performance of EXTEND_RELATION (Algorithm .) is fairly surprising, given the simplicity of the algorithm and the fact that it returns plausible word pairs out of all possible word pairs in D×D. At the very least, EXTEND_RELATION has shown to drastically narrow down the search space for discovering new word pairs satisfying a given relation, and allows for sharper classification than simple clustering. It can supplement other methods for extending knowledge bases to improve efficiency and attain even higher accuracy rates. One could also combine EXTEND_RELATION with POS and LEX filters described in Chapter , or explore other ways of utilizing external knowledge sources (e.g., a dictionary) to filter falsepositive answers.\nIn Chapter  we demonstrated that, in principle, one can learn vectors with substantially less data by using the low-dimensional subspace of categories. An interesting experiment to try is the following: Use LEARN_VECTOR to learn vectors for rare words in the Wikipedia corpus, and see if new facts can be discovered using these vectors. It would also be interesting to try variants of the objective (.), perhaps by adding the regularization term λ‖v̂ −Ukb‖2 to other existing objectives such as GloVe [].\nMoreover, one can extend this method to learn vectors for any words – even words that do not appear at all in the Wikipedia corpus – using web-scraping tools, such as Google search, to obtain\nOn the Linear Structure of Word Embeddings Chapter . Conclusion | \nadditional co-occurrence data. However, the corpora obtained from Google search may be drawn from a different distribution than the Wikipedia corpus, and hence could skew the data in a certain way. We leave this to future work."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2015,
    "abstractText" : "In this work, we leverage the linear algebraic structure of distributed word representations to automatically extend knowledge bases and allow a machine to learn new facts about the world. Our goal is to extract structured facts from corpora in a simpler manner, without applying classifiers or patterns, and using only the co-occurrence statistics of words. We demonstrate that the linear algebraic structure of word embeddings can be used to reduce data requirements for methods of learning facts. In particular, we demonstrate that words belonging to a common category, or pairs of words satisfying a certain relation, form a low-rank subspace in the projected space. We compute a basis for this low-rank subspace using singular value decomposition (SVD), then use this basis to discover new facts and to fit vectors for less frequent words which we do not yet have vectors for. This thesis represents my own work in accordance with university regulations.",
    "creator" : "LaTeX with hyperref package"
  }
}