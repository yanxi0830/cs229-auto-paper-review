{
  "name" : "1307.4564.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "From Bandits to Experts: A Tale of Domination and Independence",
    "authors" : [ "Noga Alon" ],
    "emails" : [ "nogaa@tau.ac.il", "nicolo.cesa-bianchi@unimi.it", "claudio.gentile@uninsubria.it", "mansour@tau.ac.il" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n30 7.\n45 64\nv1 [\ncs .L\nG ]"
    }, {
      "heading" : "1 Introduction",
      "text" : "Prediction with expert advice —see, e.g., [10, 13, 5, 8, 6]— is a general abstract framework for studying sequential prediction problems, formulated as repeated games between a player and an adversary. A well studied example of prediction game is the following: In each round, the adversary privately assigns a loss value to each action in a fixed set. Then the player chooses an action (possibly using randomization) and incurs the corresponding loss. The goal of the player is to control regret, which is defined as the excess loss incurred by the player as compared to the best fixed action over a sequence of rounds. Two important variants of this game have been studied in the past: the expert setting, where at the end of each round the player observes the loss assigned to each action for that round, and the bandit setting, where the player only observes the loss of the chosen action, but not that of other actions.\nLet K be the number of available actions, and T be the number of prediction rounds. The best possible regret for the expert setting is of order √ T logK. This optimal rate is achieved by the Hedge algorithm [8] or the Follow the Perturbed Leader algorithm [9]. In the bandit setting, the optimal regret is of order √ TK, achieved by the INF algorithm [2]. A bandit variant of Hedge, called Exp3 [3], achieves a regret with a slightly worse bound of order √ TK logK.\nRecently, Mannor and Shamir [11] introduced an elegant way for defining intermediate observability models between the expert setting (full observability) and the bandit setting (single observability). An intuitive way of representing an observability model is through a directed graph\nover actions: an arc from action i to action j implies that when playing action i we get information also about the loss of action j. Thus, the expert setting is obtained by choosing a complete graph over actions (playing any action reveals all losses), and the bandit setting is obtained by choosing an empty edge set (playing an action only reveals the loss of that action).\nThe main result of [11] concerns undirected observability graphs. The regret is characterized in terms of the independence number α of the undirected observability graph. Specifically, they prove that √ Tα logK is the optimal regret (up to logarithmic factors) and show that a variant of Exp3, called ELP, achieves this bound when the graph is known ahead of time, where α ∈ {1, . . . ,K} interpolates between full observability (α = 1 for the clique) and single observability (α = K for the graph with no edges). Given the observability graph, ELP runs a linear program to compute the desired distribution over actions. In the case when the graph changes over time, and at each time\nstep ELP observes the current observability graph before prediction, a bound of √∑T\nt=1 αt logK is shown, where αt is the independence number of the graph at time t. A major problem left open in [11] was the characterization of regret for directed observability graphs, a setting for which they only proved partial results.\nOur main result is a full characterization (to within logarithmic factors) of regret in the case of directed and dynamic observability graphs. Our upper bounds are proven using a new algorithm, called Exp3-DOM. This algorithm is efficient to run even in the dynamic case: it just needs to compute a small dominating set of the current observability graph (which must be given as side information) before prediction.1 As in the undirected case, the regret for the directed case is characterized in terms of the independence numbers of the observability graphs (computed ignoring edge directions). We arrive at this result by showing that a key quantity emerging in the analysis of Exp3-DOM can be bounded in terms of the independence numbers of the graphs. This bound (Lemma 13 in the appendix) is based on a combinatorial construction which might be of independent interest.\nWe also explore the possibility of the learning algorithm receiving the observability graph only after prediction, and not before. For this setting, we introduce a new variant of Exp3, called Exp3-SET, which achieves the same regret as ELP for undirected graphs, but without the need of accessing the current observability graph before each prediction. We show that in some random directed graph models Exp3-SET has also a good performance. In general, we can upper bound the regret of Exp3-SET as a function of the maximum acyclic subgraph of the observability graph, but this upper bound may not be tight. Yet, Exp3-SET is much simpler and computationally less demanding than ELP, which needs to solve a linear program in each round.\nThere are a variety of real-world settings where partial observability models corresponding to directed and undirected graphs are applicable. One of them is route selection. We are given a graph of possible routes connecting cities: when we select a route r connecting two cities, we observe the cost (say, driving time or fuel consumption) of the “edges” along that route and, in addition, we have complete information on any sub-route r′ of r, but not vice versa. We abstract this in our model by having an observability graph over routes r, and an arc from r to any of its sub-routes r′.\nSequential prediction problems with partial observability models also arise in the context of recommendation systems. For example, an online retailer, which advertises products to users, knows that users buying certain products are often interested in a set of related products. This knowledge can be represented as a graph over the set of products, where two products are joined\n1 Computing an approximately minimum dominating set can be done by running a standard greedy set cover algorithm, see Section 2.\nby an edge if and only if users who buy any one of the two are likely to buy the other as well. In certain cases, however, edges have a preferred orientation. For instance, a person buying a video game console might also buy a high-def cable to connect it to the TV set. Vice versa, interest in high-def cables need not indicate an interest in game consoles.\nSuch observability models may also arise in the case when a recommendation system operates in a network of users. For example, consider the problem of recommending a sequence of products, or contents, to users in a group. Suppose the recommendation system is hosted on an online social network, on which users can befriend each other. In this case, it has been observed that social relationships reveal similarities in tastes and interests [12]. However, social links can also be asymmetric (e.g., followers of celebrities). In such cases, followers might be more likely to shape their preferences after the person they follow, than the other way around. Hence, a product liked by a celebrity is probably also liked by his/her followers, whereas a preference expressed by a follower is more often specific to that person."
    }, {
      "heading" : "2 Learning protocol, notation, and preliminaries",
      "text" : "As stated in the introduction, we consider an adversarial multi-armed bandit setting with a finite action set V = {1, . . . ,K}. At each time t = 1, 2, . . . , a player (the “learning algorithm”) picks some action It ∈ V and incurs a bounded loss ℓIt,t ∈ [0, 1]. Unlike the standard adversarial bandit problem [3, 6], where only the played action It reveals its loss ℓIt,t, here we assume all the losses in a subset SIt,t ⊆ V of actions are revealed after It is played. More formally, the player observes the pairs (i, ℓi,t) for each i ∈ SIt,t. We also assume i ∈ Si,t for any i and t, that is, any action reveals its own loss when played. Note that the bandit setting (Si,t = {i}) and the expert setting (Si,t = V ) are both special cases of this framework. We call Si,t the observation set of action i at time t, and write i t−→ j when at time t playing action i also reveals the loss of action j. Hence, Si,t = {j ∈ V : i t−→ j}. The family of observation sets {Si,t}i∈V we collectively call the observation system at time t.\nThe adversaries we consider are nonoblivious. Namely, each loss ℓi,t at time t can be an arbitrary function of the past player’s actions I1, . . . , It−1. The performance of a player A is measured through the regret\nmax k∈V\nE [ LA,T − Lk,T ] ,\nwhere LA,T = ℓI1,1 + · · ·+ ℓIT ,T and Lk,T = ℓk,1 + · · ·+ ℓk,T are the cumulative losses of the player and of action k, respectively. The expectation is taken with respect to the player’s internal randomization (since losses are allowed to depend on the player’s past random actions, also Lk,t may be random).2 The observation system {Si,t}i∈V is either adversarially generated (in which case, each Si,t can be an arbitrary function of past player’s actions, just like losses are), or randomly generated —see Section 3. In this respect, we distinguish between adversarial and random observation systems.\nMoreover, whereas some algorithms need to know the observation system at the beginning of each step t, others need not. From this viewpoint, we shall consider two online learning settings. In the first setting, called the informed setting, the whole observation system {Si,t}i∈V selected by the adversary is made available to the learner before making its choice It. This is essentially the\n2 Although we defined the problem in terms of losses, our analysis can be applied to the case when actions return rewards gi,t ∈ [0, 1] via the transformation ℓi,t = 1− gi,t.\n“side-information” framework first considered in [11] In the second setting, called the uninformed setting, no information whatsoever regarding the time-t observation system is given to the learner prior to prediction.\nWe find it convenient to adopt the same graph-theoretic interpretation of observation systems as in [11]. At each time step t = 1, 2, . . . , the observation system {Si,t}i∈V defines a directed graph Gt = (V,Dt), where V is the set of actions, and Dt is the set of arcs, i.e., ordered pairs of nodes. For j 6= i, arc (i, j) ∈ Dt if and only if i t−→ j (the self-loops created by i t−→ i are intentionally ignored). Hence, we can equivalently define {Si,t}i∈V in terms of Gt. Observe that the outdegree d+i of any i ∈ V equals |Si,t| − 1. Similarly, the indegree d−i of i is the number of action j 6= i such that i ∈ Sj,t (i.e., such that j t−→ i). A notable special case of the above is when the observation system is symmetric over time: j ∈ Si,t if and only if i ∈ Sj,t for all i, j and t. In words, playing i at time t reveals the loss of j if and only if playing j at time t reveals the loss of i. A symmetric observation system is equivalent to Gt being an undirected graph or, more precisely, to a directed graph having, for every pair of nodes i, j ∈ V , either no arcs or length-two directed cycles. Thus, from the point of view of the symmetry of the observation system, we also distinguish between the directed case (Gt is a general directed graph) and the symmetric case (Gt is an undirected graph for all t). For instance, combining the terminology introduced so far, the adversarial, informed, and directed setting is when Gt is an adversarially-generated directed graph disclosed to the algorithm in round t before prediction, while the random, uninformed, and directed setting is when Gt is a randomly generated directed graph which is not given to the algorithm before prediction.\nThe analysis of our algorithms depends on certain properties of the sequence of graphs Gt. Two graph-theoretic notions playing an important role here are those of independent sets and dominating sets. Given an undirected graph G = (V,E), an independent set of G is any subset T ⊆ V such that no two i, j ∈ T are connected by an edge in E. An independent set is maximal if no proper superset thereof is itself an independent set. The size of a largest (maximal) independent set is the independence number of G, denoted by α(G). If G is directed, we can still associate with it an independence number: we simply view G as undirected by ignoring arc orientation. If G = (V,D) is a directed graph, then a subset R ⊆ V is a dominating set for G if for all j 6∈ R there exists some i ∈ R such that arc (i, j) ∈ D. In our bandit setting, a time-t dominating set Rt is a subset of actions with the property that the loss of any remaining action in round t can be observed by playing some action in Rt. A dominating set is minimal if no proper subset thereof is itself a dominating set. The domination number of directed graph G, denoted by γ(G), is the size of a smallest (minimal) dominating set of G.\nComputing a minimum dominating set for an arbitrary directed graph Gt is equivalent to solving a minimum set cover problem on the associated observation system {Si,t}i∈V . Although minimum set cover is NP-hard, the well-known Greedy Set Cover algorithm [7], which repeatedly selects from {Si,t}i∈V the set containing the largest number of uncovered elements so far, computes a dominating set Rt such that |Rt| ≤ γ(Gt) (1 + lnK).\nFinally, we can also lift the independence number of an undirected graph to directed graphs through the notion of maximum acyclic subgraphs: Given a directed graph G = (V,D), an acyclic subgraph of G is any graph G′ = (V ′,D′) such that V ′ ⊆ V , and D′ = D ∩ ( V ′ × V ′ ) , with no (directed) cycles. We denote by mas(G) = |V ′| the maximum size of such V ′. Note that when G is undirected (more precisely, as above, when G is a directed graph having for every pair of nodes i, j ∈ V either no arcs or length-two cycles), then mas(G) = α(G), otherwise mas(G) ≥ α(G). In particular, when G is itself a directed acyclic graph, then mas(G) = |V |.\nAlgorithm 1: Exp3-SET: Algorithm for the uninformed setting\nParameter: η ∈ [0, 1]; Initialize: wi,1 = 1 for all i ∈ V = {1, . . . ,K}; For t = 1, 2, . . . :\n1. Observation system {Si,t}i∈V is generated (but not disclosed) ;\n2. Set pi,t = wi,t Wi,t\nfor each i ∈ V , where Wt = ∑\nj∈V\nwj,t ;\n3. Play action It drawn according to distribution pt = (p1,t, . . . , pK,t) ;\n4. Observe pairs (i, ℓi,t) for all i ∈ SIt,t;\n5. Observation system {Si,t}i∈V is disclosed ; 6. For any i ∈ V set wi,t+1 = wi,t exp ( −η ℓ̂i,t ) , where\nℓ̂i,t = ℓi,t qi,t\nI{i ∈ SIt,t} and qi,t = ∑\nj : j t−→i\npj,t ."
    }, {
      "heading" : "3 Algorithms without Explicit Exploration: The Uninformed Set-",
      "text" : "ting\nIn this section, we show that a simple variant of the Exp3 algorithm [3] obtains optimal regret (to within logarithmic factors) in two variants of the uninformed setting: (1) adversarial and symmetric, (2) random and directed. We then show that even the harder adversarial and directed setting lends itself to an analysis, though with a weaker regret bound.\nExp3-SET (Algorithm 1) runs Exp3 without mixing with the uniform distribution. Similar to Exp3, Exp3-SET uses loss estimates ℓ̂i,t that divide each observed loss ℓi,t by the probability qi,t of observing it. This probability qi,t is simply the sum of all pj,t such that j t−→ i (the sum includes pi,t). Next, we bound the regret of Exp3-SET in terms of the key quantity\nQt = ∑\ni∈V\npi,t qi,t\n= ∑\ni∈V pi,t∑ j : j t−→i pj,t . (1)\nEach term pi,t/qi,t can be viewed as the probability of drawing i from pt conditioned on the event that i was observed. Similar to [11], a key aspect to our analysis is the ability to deterministically (and nonvacuously)3 upper bound Qt in terms of certain quantities defined on {Si,t}i∈V . We shall do so in two ways, either irrespective of how small each pi,t may be (this section) or depending on suitable lower bounds on the probabilities pi,t (Section 4). In fact, forcing lower bounds on pi,t is equivalent to adding exploration terms to the algorithm, which can be done only when knowing {Si,t}i∈V before each prediction —an information available only in the informed setting.\n3 An obvious upper bound on Qt is K.\nThe following simple result is the building block for all subsequent results in the uninformed setting.4\nTheorem 1 In the adversarial case, the regret of Exp3-SET satisfies\nmax k∈V\nE [ LA,T − Lk,T ] ≤ lnK\nη +\nη\n2\nT∑\nt=1\nE[Qt] .\nAs we said, in the adversarial and symmetric case the observation system at time t can be described by an undirected graph Gt = (V,Et). This is essentially the problem of [11], which they studied in the easier informed setting, where the same quantity Qt above arises in the analysis of their ELP algorithm. In their Lemma 3, they show that Qt ≤ α(Gt), irrespective of the choice of the probabilities pt. When applied to Exp3-SET, this immediately gives the following result.\nCorollary 2 In the adversarial and symmetric case, the regret of Exp3-SET satisfies\nmax k∈V\nE [ LA,T − Lk,T ] ≤ lnK\nη +\nη\n2\nT∑\nt=1\nE[α(Gt)] .\nIn particular, if for constants α1, . . . , αT we have α(Gt) ≤ αt, t = 1, . . . , T , then setting η =√ (2 lnK) /∑T t=1 αt, gives\nmax k∈V\nE [ LA,T − Lk,T ] ≤ √√√√2(lnK) T∑\nt=1\nαt .\nAs shown in [11], the knowledge of ∑T\nt=1 α(Gt) for tuning η can be dispensed with (at the cost of extra log factors in the bound) by binning the values of η and running Exp3 on top of a pool of instances of Exp-SET, one for each bin. The bounds proven in Corollary 2 are equivalent to those proven in [11] (Theorem 2 therein) for the ELP algorithm. Yet, our analysis is much simpler and, more importantly, our algorithm is simpler and more efficient than ELP, which requires solving a linear program at each step. Moreover, unlike ELP, Exp-SET does not require prior knowledge of the observation system {Si,t}i∈V at the beginning of each step.\nWe now turn to the directed setting. We first treat the random case, and then the harder adversarial case.\nThe Erdős-Renyi model is a standard model for random directed graphs G = (V,D), where we are given a density parameter r ∈ [0, 1] and, for any pair i, j ∈ V , arc (i, j) ∈ D with independent probability r.5 We have the following result.\nCorollary 3 Let Gt be generated according to the Erdős-Renyi model with parameter r ∈ [0, 1]. Then the regret of Exp3-SET satisfies\nmax k∈V\nE [ LA,T − Lk,T ] ≤ lnK\nη +\nη T\n2r\n( 1− (1− r)K ) .\n4 All proofs are given in the appendix. 5 Self loops, i.e., arcs (i, i) are included by default here."
    }, {
      "heading" : "In the above, the expectations E[·] are w.r.t. both the algorithm’s randomization and the random",
      "text" : "generation of Gt occurring at each round. In particular, setting η = √ 2r lnK\nT (1−(1−r)K ) , gives\nmax k∈V\nE [ LA,T − Lk,T ] ≤\n√ 2(lnK)T (1− (1− r)K)\nr .\nNote that as r ranges in [0, 1] we interpolate between the bandit (r = 0)6 and the expert (r = 1) regret bounds.\nIn the adversarial setting, we have the following result.\nCorollary 4 In the adversarial and directed case, the regret of Exp3-SET satisfies\nmax k∈V\nE [ LA,T − Lk,T ] ≤ lnK\nη +\nη\n2\nT∑\nt=1\nE[mas(Gt)] .\nIn particular, if for constants m1, . . . ,mT we have mas(Gt) ≤ mt, t = 1, . . . , T , then setting η =√ (2 lnK) /∑T t=1 mt, gives\nmax k∈V\nE [ LA,T − Lk,T ] ≤ √√√√2(lnK) T∑\nt=1\nmt .\nObserve that Corollary 4 is a strict generalization of Corollary 2 because, as we pointed out in Section 2, mas(Gt) ≥ α(Gt), with equality holding when Gt is an undirected graph.\nAs far as lower bounds are concerned, in the symmetric setting, the authors of [11] derive a lower bound of Ω (√ α(G)T ) in the case when Gt = G for all t. We remark that similar to the symmetric\nsetting, we can derive a lower bound of Ω (√ α(G)T ) . The simple observation is that given a directed graph G, we can define a new graph G′ which is made undirected just by reciprocating arcs; namely, if there is an arc (i, j) in G we add arcs (i, j) and (j, i) in G′. Note that α(G) = α(G′). Since in G′ the learner can only receive more information than in G, any lower bound on G also applies to G′. Therefore we derive the following corollary to the lower bound of [11] (Theorem 4 therein).\nCorollary 5 Fix a directed graph G, and suppose Gt = G for all t. Then there exists a (randomized) adversarial strategy such that for any T = Ω ( α(G)3 ) and for any learning strategy, the expected regret of the learner is Ω (√ α(G)T ) .\nOne may wonder whether a sharper lower bound argument exists which applies to the general directed setting and involves the larger quantity mas(G). Unfortunately, the above measure does not seem to be related to the optimal regret: Using Claim 1 in the appendix (see proof of Theorem 3) one can exhibit a sequence of graphs each having a large acyclic subgraph, on which the regret of Exp3-SET is still small.\nThe lack of a lower boundmatching the upper bound provided by Corollary 4 is a good indication that something more sophisticated has to be done in order to upper bound Qt in (1). This leads us to consider more refined ways of allocating probabilities pi,t to nodes. However, this allocation will require prior knowledge of the graphs Gt.\n6 Observe that limr→0+ 1−(1−r)K r = K."
    }, {
      "heading" : "4 Algorithms with Explicit Exploration: The Informed Setting",
      "text" : "We are still in the general scenario where graphs Gt are arbitrary and directed, but now Gt is made available before prediction. We start by showing a simple example where our analysis of Exp3-SET inherently fails. This is due to the fact that, when the graph induced by the observation system is directed, the key quantity Qt defined in (1) cannot be nonvacuously upper bounded independent of the choice of probabilities pi,t. A way round it is to introduce a new algorithm, called Exp3DOM, which controls probabilities pi,t by adding an exploration term to the distribution pt. This exploration term is supported on a dominating set of the current graph Gt. For this reason, Exp3DOM requires prior access to a dominating set Rt at each time step t which, in turn, requires prior knowledge of the entire observation system {Si,t}i∈V .\nAs announced, the next result shows that, even for simple directed graphs, there exist distributions pt on the vertices such that Qt is linear in the number of nodes while the independence number is 1.7 Hence, nontrivial bounds on Qt can be found only by imposing conditions on distribution pt.\nFact 6 Let G = (V,D) be a total order on V = {1, . . . ,K}, i.e., such that for all i ∈ V , arc (j, i) ∈ D for all j = i+ 1, . . . ,K. Let p = (p1, . . . , pK) be a distribution on V such that pi = 2−i, for i < K and pk = 2 −K+1. Then\nQ = K∑\ni=1\npi pi + ∑ j : j−→i pj\n= K∑\ni=1 pi∑K j=i pj = K + 1 2 .\nWe are now ready to introduce and analyze the new algorithm Exp3-DOM for the adversarial, informed and directed setting. Exp3-DOM (see Algorithm 2) runs O(logK) variants of Exp3 indexed by b = 0, 1, . . . , ⌊log2K⌋. At time t the algorithm is given observation system {Si,t}i∈V , and computes a dominating set Rt of the directed graph Gt induced by {Si,t}i∈V . Based on the size |Rt| of Rt, the algorithm uses instance bt = ⌊log2 |Rt|⌋ to pick action It. We use a superscript b to denote the quantities relevant to the variant of Exp3 indexed by b. Similarly to the analysis of Exp3-SET, the key quantities are\nq (b) i,t =\n∑\nj : i∈Sj,t\np (b) j,t =\n∑\nj : j t−→i\np (b) j,t and Q (b) t =\n∑\ni∈V\np (b) i,t q (b) i,t , b = 0, 1, . . . , ⌊log2K⌋ .\nLet T (b) = { t = 1, . . . , T : |Rt| ∈ [2b, 2b+1 − 1] } . Clearly, the sets T (b) are a partition of the time steps {1, . . . , T}, so that ∑b |T (b)| = T . Since the adversary adaptively chooses the dominating sets Rt, the sets T\n(b) are random. This causes a problem in tuning the parameters γ(b). For this reason, we do not prove a regret bound for Exp3-DOM, where each instance uses a fixed γ(b), but for a slight variant (described in the proof of Theorem 7 —see the appendix) where each γ(b) is set through a doubling trick.\nTheorem 7 In the adversarial and directed case, the regret of Exp3-DOM satisfies\nmax k∈V\nE [ LA,T − Lk,T ] ≤\n⌊log2 K⌋∑\nb=0\n 2 b lnK\nγ(b) + γ(b)E\n  ∑\nt∈T (b)\n( 1 + Q (b) t\n2b+1\n)    . (2)\n7 In this specific example, the maximum acyclic subgraph has size K, which confirms the looseness of Corollary 4.\nAlgorithm 2: Exp3-DOM\nInput: Exploration parameters γ(b) ∈ (0, 1] for b ∈ { 0, 1, . . . , ⌊log2 K⌋ } ; Initialization: w (b) i,1 = 1 for all i ∈ V and b ∈ { 0, 1, . . . , ⌊log2 K⌋ } ; For t = 1, 2, . . . :\n1. Observation system {Si,t}i∈V is generated and disclosed ;\n2. Compute a dominating set Rt ⊆ V for Gt associated with {Si,t}i∈V ; 3. Let bt be such that |Rt| ∈ [ 2bt , 2bt+1 − 1 ] ;\n4. Set W (bt) t = ∑ i∈V w (bt) i,t ;\n5. Set p (bt) i,t =\n( 1− γ(bt) ) w(bt)i,t W\n(bt) t\n+ γ(bt)\n|Rt| I{i ∈ Rt};\n6. Play action It drawn according to distribution p (bt) t = ( p (bt) 1,t , . . . , p (bt) V,t ) ;\n7. Observe pairs (i, ℓi,t) for all i ∈ SIt,t;\n8. For any i ∈ V set w(bt)i,t+1 = w (bt) i,t exp ( −γ(bt) ℓ̂(bt)i,t /2bt ) , where\nℓ̂ (bt) i,t =\nℓi,t\nq (bt) i,t\nI{i ∈ SIt,t} and q(bt)i,t = ∑\nj : j t−→i\np (bt) j,t .\nMoreover, if we use a doubling trick to choose γ(b) for each b = 0, . . . , ⌊log2 K⌋, then\nmax k∈V\nE [ LA,T − Lk,T ] = O  (lnK)E   √√√√ T∑\nt=1\n( 4|Rt|+Q(bt)t )  + (lnK) ln(KT )   . (3)\nImportantly, the next result shows how bound (3) of Theorem 7 can be expressed in terms of the sequence α(Gt) of independence numbers of graphs Gt whenever the Greedy Set Cover algorithm [7] (see Section 2) is used to compute the dominating set Rt of the observation system at time t.\nCorollary 8 If Step 2 of Exp3-DOM uses the Greedy Set Cover algorithm to compute the dominating sets Rt, then the regret of Exp-DOM with doubling trick satisfies\nmax k∈V\nE [ LA,T − Lk,T ] = O  ln(K) √√√√ln(KT ) T∑\nt=1\nα(Gt) + ln(K) ln(KT )\n  ,\nwhere, for each t, α(Gt) is the independence number of the graph Gt induced by observation system {Si,t}i∈V ."
    }, {
      "heading" : "5 Conclusions and work in progress",
      "text" : "We have investigated online prediction problems in partial information regimes that interpolate between the classical bandit and expert settings. We have shown a number of results characterizing prediction performance in terms of: the structure of the observation system, the amount of information available before prediction, the nature (adversarial or fully random) of the process generating the observation system. Our results are substantial improvements over the paper [11] that initiated this interesting line of research. Our improvements are diverse, and range from considering both informed and uninformed settings to delivering more refined graph-theoretic characterizations, from providing more efficient algorithmic solutions to relying on simpler (and often more general) analytical tools.\nSome research directions we are currently pursuing are the following.\n1. We are currently investigating the extent to which our results could be applied to the case when the observation system {Si,t}i∈V may depend on the loss ℓIt,t of player’s action It. Notice that this would prevent a direct construction of an unbiased estimator for unobserved losses, which many worst-case bandit algorithms (including ours —see the appendix) hinge upon.\n2. The upper bound contained in Corollary 4 and expressed in terms of mas(·) is almost certainly suboptimal, even in the uninformed setting, and we are trying to see if more adequate graph complexity measures can be used instead.\n3. Our lower bound (Corollary 5) heavily relies on the corresponding lower bound in [11] which, in turn, refers to a constant graph sequence. We would like to provide a more complete charecterization applying to sequences of adversarially-generated graphs G1, G2, . . . , GT in terms of sequences of their corresponding independence numbers α(G1), α(G2), . . . , α(GT ) (or variants thereof), in both the uninformed and the informed settings."
    }, {
      "heading" : "Acknowledgments",
      "text" : "The first author was supported in part by an ERC advanced grant, by a USA-Israeli BSF grant, and by the Israeli I-CORE program. The second author acknowledges partial support by MIUR (project ARS TechnoMedia, PRIN 2010-2011, grant no. 2010N5K7EB 003). The fourth author was supported in part by a grant from the Israel Science Foundation, a grant from the United StatesIsrael Binational Science Foundation (BSF), a grant by Israel Ministry of Science and Technology and the Israeli Centers of Research Excellence (I-CORE) program (Center No. 4/11)."
    }, {
      "heading" : "A Technical lemmas and proofs",
      "text" : "This section contains the proofs of all technical results occurring in the main text, along with ancillary graph-theoretic lemmas. Throughout this appendix, Et[·] is a shorthand for E [ · | I1, . . . , It−1 ] .\nProof of Theorem 1. Following the proof of Exp3 [3], we have\nWt+1 Wt\n= ∑\ni∈V\nwi,t+1 Wt\n= ∑\ni∈V\nwi,t exp(−η ℓ̂i,t) Wt\n= ∑\ni∈V\npi,t exp(−η ℓ̂i,t)\n≤ ∑\ni∈V\npi,t ( 1− ηℓ̂i,t + 1\n2 η2(ℓ̂i,t) 2\n) using e−x ≤ 1− x+ x2/2 for all x ≥ 0\n≤ 1− η ∑\ni∈V\npi,tℓ̂i,t + η2\n2\n∑\ni∈V\npi,t(ℓ̂i,t) 2 .\nTaking logs, using ln(1− x) ≤ −x for all x ≥ 0, and summing over t = 1, . . . , T yields\nln WT+1 W1\n≤ −η T∑\nt=1\n∑\ni∈V\npi,tℓ̂i,t + η2\n2\nT∑\nt=1\n∑\ni∈V\npi,t(ℓ̂i,t) 2 .\nMoreover, for any fixed comparison action k, we also have\nln WT+1 W1 ≥ ln wk,T+1 W1\n= −η T∑\nt=1\nℓ̂k,t − lnK .\nPutting together and rearranging gives\nT∑\nt=1\n∑\ni∈V\npi,tℓ̂i,t ≤ T∑\nt=1\nℓ̂k,t + lnK\nη +\nη\n2\nT∑\nt=1\n∑\ni∈V\npi,t(ℓ̂i,t) 2 . (4)\nNote that, for all i ∈ V ,\nEt[ℓ̂i,t] = ∑\nj : i∈Sj,t\npj,t ℓi,t qi,t\n= ∑\nj : j t−→i\npj,t ℓi,t qi,t = ℓi,t qi,t\n∑\nj : j t−→i\npj,t = ℓi,t .\nMoreover,\nEt [ (ℓ̂i,t) 2 ] = ∑\nj : i∈Sj,t\npj,t ℓ2i,t q2i,t = ℓ2i,t q2i,t\n∑\nj : j t−→i\npj,t ≤ 1\nq2i,t\n∑\nj : j t−→i\npj,t = 1\nqi,t .\nHence, taking expectations Et on both sides of (4), and recalling the definition of Qt, we can write\nT∑\nt=1\n∑\ni∈V\npi,t ℓi,t ≤ T∑\nt=1\nℓk,t + lnK\nη +\nη\n2\nT∑\nt=1\nQt . (5)\nFinally, taking expectations to remove conditioning gives\nE [ LA,T − Lk,T ] ≤ lnK\nη +\nη\n2\nT∑\nt=1\nE[Qt] ,\nas claimed.\nProof of Corollary 3. Fix round t, and let G = (V,D) be the Erdős-Renyi random graph generated at time t, N−i be the in-neighborhood of node i, i.e., the set of nodes j such that (j, i) ∈ D, and denote by d−i the indegree of i.\nClaim 1 Let p1, . . . , pK be an arbitrary probability distribution defined over V , f : V → V be an arbitrary permutation of V , and Ef denote the expectation w.r.t. permutation f when f is drawn uniformly at random. Then, for any i ∈ V , we have\nEf   pf(i) pf(i) + ∑ j : f(j)∈N−\nf(i) pf(j)\n  = 1\n1 + d−i .\nProof of Claim 1. Consider selecting a subset S ⊂ V of 1 + d−i nodes. We shall consider the contribution to the expectation when S = N−f(i)∪{f(i)}. Since there are K(K−1) · · · (K−d − i +1) terms (out of K!) contributing to the expectation, we can write\nEf   pf(i) pf(i) + ∑ j : f(j)∈N−\nf(i) pf(j)\n  = 1(K\nd−i\n) ∑\nS⊂V,|S|=d−i\n1\n1 + d−i\n∑\ni∈S\npi pi + ∑ j∈S,j 6=i pj\n= 1(K d−i\n) ∑\nS⊂V,|S|=d−i\n1\n1 + d−i\n= 1\n1 + d−i .\nClaim 2 Let p1, . . . , pK be an arbitrary probability distribution defined over V , and E denote the expectation w.r.t. the Erdős-Renyi random draw of arcs at time t. Then, for any fixed i ∈ V , we have\nE   pi pi +\n∑ j : j t−→i pj\n  = 1\nrK\n( 1− (1− r)K ) .\nProof of Claim 2. For the given i ∈ V and time t, consider the Bernoulli random variables Xj , j ∈\nV \\ {i}, and denote by Ej : j 6=i the expectation w.r.t. all of them. We symmetrize E [\npi pi+ ∑\nj : j t−→i pj\n]\nby means of a random permutation f , as in Claim 1. We can write\nE   pi pi +\n∑ j : j t−→i pj\n  = Ej : j 6=i [ pi\npi + ∑ j : j 6=iXjpj\n]\n= Ej : j 6=iEf\n[ pf(i)\npf(i) + ∑ j : j 6=iXf(j)pf(j)\n] (by symmetry)\n= Ej : j 6=i\n[ 1\n1 + ∑\nj : j 6=iXj\n] (from Claim 1)\n=\nK−1∑\ni=0\n( K − 1\ni\n) ri(1− r)K−1−i 1\ni+ 1\n= 1\nrK\nK−1∑\ni=0\n( K\ni+ 1\n) ri+1(1− r)K−1−i\n= 1\nrK\n( 1− (1− r)K ) .\nAt this point, we follow the proof of Theorem 1 up until (5). We take an expectation EG1,...,GT w.r.t. the randomness in generating the sequence of graphs G1, . . . , GT . This yields\nT∑\nt=1\nEG1,...,GT\n[ ∑\ni∈V\npi,t ℓi,t\n] ≤ T∑\nt=1\nℓk,t + lnK\nη +\nη\n2\nT∑\nt=1\nEG1,...,GT [Qt] .\nWe use Claim 2 to upper bound EG1,...,GT [Qt] by 1 r\n( 1− (1− r)K ) , and take the outer expectation\nto remove conditioning, as in the proof of Theorem 1. This concludes the proof.\nThe following lemma can be seen as a generalization of Lemma 3 in [11].\nLemma 9 Let G = (V,D) be a directed graph with vertex set V = {1, . . . ,K}, and arc set D. Let N−i be the in-neighborhood of node i, i.e., the set of nodes j such that (j, i) ∈ D. Then\nK∑\ni=1\npi pi + ∑ j∈N−i pj ≤ mas(G) .\nProof. We will show that there is a subset of vertices V ′ such that the induced graph is acyclic and |V ′| ≥ ∑Ki=1 pipi+∑\nj∈N −\ni\npj .\nWe prove the lemma by growing set V ′ starting off from V ′ = ∅. Let\nΦ0 =\nK∑\ni=1\npi pi + ∑ j∈N−i pj ,\nand i1 be the vertex which minimizes pi + ∑\nj∈N−i pj over i ∈ V . We are going to delete i1 from\nthe graph, along with all its incoming neighbors (set N−i1 ), and all edges which are incident (both\ndeparting and incoming) to these nodes, and then iterating on the remaining graph. Let us denote the in-neighborhoods of the shrunken graph from the first step by N−i,1.\nThe contribution of all the deleted vertices to Φ0 is\n∑\nr∈N−i1 ∪{i1}\npr pr + ∑ j∈N−r pj ≤\n∑\nr∈N−i1 ∪{i1}\npr pi1 + ∑ j∈N−i1 pj = 1 ,\nwhere the inequality follows from the minimality of i1. Let V ′ ← V ′ ∪ {i1}, and V1 = V − (N−i1 ∪ {i1}). Then from the first step we have\nΦ1 = ∑\ni∈V1\npi pi + ∑ j∈N−i,1 pj ≥\n∑\ni∈V1\npi pi + ∑ j∈N−i pj ≥ Φ0 − 1 .\nWe apply the very same argument to Φ1 with node i2 (minimizing pi + ∑\nj∈N−i,1 pj over i ∈ V1),\nto Φ2 with node i3, . . . , to Φs−1 with node is, up until Φs = 0, i.e., up until no nodes are left in the shrunken graph. This gives Φ0 ≤ s = |V ′|, where V ′ = {i1, i2, . . . , is}. Moreover, since in each step r = 1, . . . , s we remore all remaining arcs incoming to ir, the graph induced by set V\n′ cannot contain cycles.\nProof of Corollary 4. The claim follows from a direct combination of Theorem 1 with Lemma 9.\nProof of Fact 6. Using standard properties of geometric sums, one can immediately see that\nK∑\ni=1 pi∑K j=i pj =\nK−1∑\ni=1\n2−i\n2−i+1 +\n2−K+1 2−K+1 = K − 1 2 + 1 = K + 1 2 ,\nhence the claimed result.\nThe following graph-theoretic lemma turns out to be fairly useful for analyzing directed settings. It is a directed-graph counterpart to a well-known result [4, 14] holding for undirected graphs.\nLemma 10 Let G = (V,D) be a directed graph, with V = {1, . . . ,K}. Let d−i be the indegree of node i, and α = α(G) be the independence number of G. Then\nK∑\ni=1\n1\n1 + d−i ≤ 2α ln\n( 1 + K\nα\n) .\nProof. We will proceed by induction, starting off from the original K-node graph G = GK with indegrees {d−i }Ki=1 = {d−i,K}Ki=1, and independence number α = αK , and then progressively shrink G by eliminating nodes and incident (both departing and incoming) arcs, thereby obtaining a sequence of smaller and smaller graphs GK , GK−1, GK−2, . . ., and associated indegrees {d−i,K}Ki=1, {d−i,K−1}K−1i=1 , {d−i,K−2}K−2i=1 , . . . , and independence numbers αK , αK−1, αK−2, . . .. Specifically, in step s we sort nodes i = 1, . . . , s of Gs in nonincreasing value of d − i,s, and obtain Gs−1 from Gs by eliminating node 1 (i.e., one having the largest indegree among the nodes of Gs), along with its incident arcs. On all such graphs, we will use the classical Turan’s theorem (e.g., [1]) stating that\nany undirected graph with ns nodes and ms edges has an independent set of size at least ns\n2ms ns\n+1 .\nThis implies that if Gs = (Vs,Ds), then αs satisfies 8\n|Ds| |Vs| ≥ |Vs| 2αs − 1 2 . (6)\nWe then start from GK . We can write\nd−1,K = maxi=1...K d−i,K ≥\n1\nK\nK∑\ni=1\nd−i,K = |DK | |VK | ≥ |VK | 2αK − 1 2 .\nHence,\nK∑\ni=1\n1\n1 + d−i,K =\n1\n1 + d−1,K +\nK∑\ni=2\n1\n1 + d−i,K\n≤ 2αK αK +K +\nK∑\ni=2\n1\n1 + d−i,K\n≤ 2αK αK +K +\nK−1∑\ni=1\n1\n1 + d−i,K−1 ,\nwhere the last inequality follows from d−i+1,K ≥ d−i,K−1, i = 1, . . . K − 1, due to the arc elimination turning GK into GK−1. Recursively applying the very same argument to GK−1 (i.e., to the sum∑K−1\ni=1 1\n1+d− i,K−1\n), and then iterating all the way to G1 yields the upper bound\nK∑\ni=1\n1\n1 + d−i,K ≤\nK∑\ni=1\n2αi αi + i .\nCombining with αi ≤ αK = α, and ∑K i=1 1 α+i ≤ ln ( 1 + Kα ) concludes the proof.\nThe next lemma relates the size |Rt| of the dominating set Rt computed by the Greedy Set Cover algorithm of [7] operating on the time-t observation system {Si,t}i∈V to the independence number α(Gt) and the domination number γ(Gt) of Gt.\nLemma 11 Let {Si}i∈V be an observation system, and G = (V,D) be the induced directed graph, with vertex set V = {1, . . . ,K}, independence number α = α(G), and domination number γ = γ(G). Then the dominating set R constructed by the Greedy Set Cover algorithm (see Section 2) satisfies\n|R| ≤ min { γ(1 + lnK), ⌈2α lnK⌉+ 1 } .\nProof. As recalled in Section 2, the Greedy Set Cover algorithm of [7] achieves |R| ≤ γ(1 + lnK). In order to prove the other bound, consider the sequence of graphs G = G1, G2, . . . , where each Gs+1 = (Vs+1,Ds+1) is obtained by removing from Gs the vertex is selected by the Greedy Set\n8 Notice that |Ds| is at least as large as the number of edges of the undirected version of Gs which the independence number αs actually refers to.\nCover algorithm, together with all the vertices in Gs that are dominated by is, and all arcs incident to these vertices. By definition of the algorithm, the outdegree d+s of is in Gs is largest in Gs. Hence,\nd+s ≥ |Ds| |Vs| ≥ |Vs| 2αs − 1 2 ≥ |Vs| 2α − 1 2\nby Turan’s theorem (e.g., [1]), where αs is the independence number of Gs and α ≥ αs. This shows that\n|Vs+1| = |Vs| − d+s − 1 ≤ |Vs| ( 1− 1\n2α\n) ≤ |Vs|e−1/(2α) .\nIterating, we obtain |Vs| ≤ K e−s/(2α). Choosing s = ⌈2α lnK⌉+ 1 gives |Vs| < 1, thereby covering all nodes. Hence the dominating set R = {i1, . . . , is} so constructed satisfies |R| ≤ ⌈2α lnK⌉ + 1.\nLemma 12 If a, b ≥ 0, and a+ b ≥ B > A > 0, then\na a+ b−A ≤ a a+ b + A B −A .\nProof. a\na+ b−A − a a+ b =\naA (a+ b)(a+ b−A) ≤ A a+ b−A ≤ A B −A .\nWe now lift Lemma 10 to a more general statement.\nLemma 13 Let G = (V,D) be a directed graph, with vertex set V = {1, . . . ,K}, and arc set D. Let N−i be the in-neighborhood of node i, i.e., the set of nodes j such that (j, i) ∈ D. Let α be the independence number of G, R ⊆ V be a dominating set for G of size r = |R|, and p1, . . . , pK be a probability distribution defined over V , such that pi ≥ β > 0, for i ∈ R. Then\nK∑\ni=1\npi pi + ∑ j∈N−i pj ≤ 2α ln\n( 1 +\n⌈K2rβ ⌉+K α\n) + 2r .\nProof. The idea is to appropriately discretize the probability values pi, and then upper bound the discretized counterpart of ∑K i=1 pi pi+ ∑ j∈N −\ni\npj by reducing to an expression that can be handled\nby Lemma 10. In order to make this discretization effective, we need to single out the terms pi pi+ ∑\nj∈N −\ni\npj corresponding to nodes i ∈ R. We first write\nK∑\ni=1\npi pi + ∑ j∈N−i pj =\n∑\ni∈R\npi pi + ∑ j∈N−i pj + ∑ i/∈R pi pi + ∑ j∈N−i pj\n≤ r + ∑\ni/∈R\npi pi + ∑ j∈N−i pj , (7)\nand then focus on (7).\nLet us discretize the unit interval9 (0, 1] into subintervals ( j−1M , j M ], j = 1, . . . ,M , where M =\n⌈K2rβ ⌉. Let p̂i = j/M be the discretized version of pi, being j the unique integer such that\np̂i − 1/M < pi ≤ p̂i .\nLet us focus on a single node i /∈ R with indegree d−i = |N−i |, and introduce the shorthand notation Pi =\n∑ j∈N−i pj, and P̂i = ∑ j∈N−i p̂j. We have that P̂i ≥ Pi ≥ β, since i is dominated by some\nnode j ∈ R∩N−i such that pj ≥ β. Moreover, Pi > P̂i− d−i M ≥ β− d−i M > 0, and p̂i+ P̂i ≥ β. Hence, for any fixed node i /∈ R, we can write pi\npi + Pi ≤ p̂i p̂i + Pi\n< p̂i\np̂i + P̂i − d − i\nM\n≤ p̂i p̂i + P̂i\n+ d−i /M\nβ − d−i /M\n= p̂i\np̂i + P̂i + d−i βM − d−i\n< p̂i\np̂i + P̂i +\nr\nK − r ,\nwhere in the second-last inequality we used Lemma 12 with a = p̂i, b = P̂i, A = d − i /M , and B = β > d−i /M . Recalling (7), and summing over i then gives\nK∑\ni=1\npi pi + Pi\n≤ r + ∑\ni/∈R\np̂i\np̂i + P̂i + r =\n∑\ni/∈R\np̂i\np̂i + P̂i + 2r . (8)\nTherefore, we continue by bounding from above the right-hand side of (8). We first observe that\n∑\ni/∈R\np̂i\np̂i + P̂i =\n∑\ni/∈R\nŝi\nŝi + Ŝi , Ŝi =\n∑\nj∈N−i\nŝj , (9)\nwhere ŝi = Mp̂i, i = 1, . . . ,K, are integers. Based on the original graph G, we construct a new graph Ĝ made up of connected cliques. In particular:\n• Each node i of G is replaced in Ĝ by a clique Ci of size ŝi; nodes within Ci are connected by length-two cycles.\n• If arc (i, j) is in G, then for each node of Ci draw an arc towards each node of Cj.\nWe would like to apply Lemma 10 to Ĝ. Notice that, by the above construction:\n• The independence number of Ĝ is the same as that of G;\n• The indegree d̂−k of each node k in clique Ci satisfies d̂−k = ŝi − 1 + Ŝi. 9 The zero value won’t be of our concern here, because if pi = 0, the corresponding term in (7) can be disregarded.\n• The total number of nodes of Ĝ is K∑\ni=1\nŝi = M\nK∑\ni=1\np̂i < M\nK∑\ni=1\n( pi + 1\nM\n) = M +K .\nHence, we are in a position to apply Lemma 10 to Ĝ with indegrees d̂−k , revealing that\n∑\ni/∈R\nŝi\nŝi + Ŝi =\n∑\ni/∈R\n∑\nk∈Ci\n1\n1 + d̂−k ≤\nK∑\ni=1\n∑\nk∈Ci\n1\n1 + d̂−k ≤ 2α ln\n( 1 + M +K\nα\n) .\nPutting together as in (8) and (9), and recalling the value of M gives the claimed result.\nProof of Theorem 7. We start to bound the contribution to the overall regret of an instance indexed by b. When clear from the context, we remove the superscript b from γ(b), w (b) i,t , p (b) i,t , and other related quantities. For any t ∈ T (b) we have\nWt+1 Wt\n= ∑\ni∈V\nwi,t+1 Wt\n= ∑\ni∈V\nwi,t Wt\nexp ( −(γ/2b) ℓ̂i,t )\n= ∑\ni∈Rt\npi,t − γ/|Rt| 1− γ exp ( −(γ/2b) ℓ̂i,t ) + ∑\ni 6∈Rt\npi,t 1− γ exp\n( −(γ/2b) ℓ̂i,t )\n≤ ∑\ni∈Rt\npi,t − γ/|Rt| 1− γ\n( 1− γ\n2b ℓ̂i,t +\n1\n2 ( γ 2b ℓ̂i,t )2) + ∑\ni 6∈Rt\npi,t 1− γ\n( 1− γ\n2b ℓ̂i,t +\n1\n2 ( γ 2b ℓ̂i,t\n)2)\n(using e−x ≤ 1− x+ x2/2 for all x ≥ 0)\n≤ 1− γ/2 b 1− γ ∑\ni∈V\npi,tℓ̂i,t + γ2/2b 1− γ ∑\ni∈Rt\nℓ̂i,t |Rt| + 1 2\n(γ/2b)2 1− γ ∑\ni∈V\npi,t ( ℓ̂i,t )2 .\nTaking logs, upper bounding, and summing over t ∈ T (b) yields\nln W|T (b)|+1 W1 ≤ − γ/2 b 1− γ ∑\nt∈T (b)\n∑\ni∈V\npi,tℓ̂i,t + γ2/2b 1− γ ∑\nt∈T (b)\n∑\ni∈Rt\nℓ̂i,t |Rt| + 1 2\n(γ/2b)2 1− γ ∑\nt∈T (b)\n∑\ni∈V\npi,t ( ℓ̂i,t )2 .\nMoreover, for any fixed comparison action k, we also have\nln W|T (b)|+1\nW1 ≥ ln\nwk,|T (b)|+1\nW1 = − γ 2b\n∑\nt∈T (b)\nℓ̂k,t − lnK .\nPutting together, rearranging, and using 1− γ ≤ 1 gives\n∑\nt∈T (b)\n∑\ni∈V\npi,tℓ̂i,t ≤ ∑\nt∈T (b)\nℓ̂k,t + 2b lnK\nγ + γ\n∑\nt∈T (b)\n∑\ni∈Rt\nℓ̂i,t |Rt| + γ 2b+1\n∑\nt∈T (b)\n∑\ni∈V\npi,t ( ℓ̂i,t )2 .\nReintroducing the notation γ(b) and summing over b = 0, 1, . . . , ⌊log2K⌋ gives\nT∑\nt=1\n( ∑\ni∈V\np (bt) i,t ℓ̂ (bt) i,t − ℓ̂k,t\n) ≤ ⌊log2 K⌋∑\nb=0\n2b lnK\nγ(b) +\nT∑\nt=1\n∑\ni∈Rt\nγ(bt)ℓ̂ (bt) i,t\n|Rt| +\nT∑\nt=1\nγ(bt) 2bt+1\n∑\ni∈V\np (bt) i,t ( ℓ̂ (bt) i,t )2 . (10)\nNow, similarly to the proof of Theorem 1, we have that, for any i and t, Et [ ℓ̂ (bt) i,t ] = ℓi,t and\nEt\n[ (ℓ̂\n(bt) i,t )\n2 ] ≤ 1\nq (bt) i,t\n. Hence, taking expectations Et on both sides of (10) and recalling the definition\nof Q (b) t gives\nT∑\nt=1\n( ∑\ni∈V\np (bt) i,t ℓi,t − ℓk,t\n) ≤ ⌊log2 K⌋∑\nb=0\n2b lnK\nγ(b) +\nT∑\nt=1\n∑\ni∈Rt\nγ(bt)ℓi,t |Rt| +\nT∑\nt=1\nγ(bt) 2bt+1 Q (bt) t . (11)\nMoreover, T∑\nt=1\n∑\ni∈Rt\nγ(bt)ℓi,t |Rt| ≤ T∑\nt=1\n∑\ni∈Rt\nγ(bt) |Rt| =\nT∑\nt=1\nγ(bt) =\n⌊log2 K⌋∑\nb=0\nγ(b)|T (b)|\nand T∑\nt=1\nγ(bt) 2bt+1 Q (bt) t =\n⌊log2 K⌋∑\nb=0\nγ(b) 2b+1\n∑\nt∈T (b)\nQ (b) t .\nHence, plugging back into (11), taking outer expectations on both sides and recalling that T (b) is random (since the adversary adaptively decides which steps t fall into T (b)), we get\nE [ LA,T − Lk,T ] ≤\n⌊log2 K⌋∑\nb=0\nE\n 2 b lnK\nγ(b) + γ(b)|T (b)|+ γ\n(b)\n2b+1\n∑\nt∈T (b)\nQ (b) t\n \n=\n⌊log2 K⌋∑\nb=0\n 2 b lnK\nγ(b) + γ(b)E\n  ∑\nt∈T (b)\n( 1 + Q (b) t\n2b+1\n)    . (12)\nThis establishes (2). In order to prove inequality (3), we need to tune each γ(b) separately. However, a good choice of γ(b) depends on the unknown random quantity\nQ (b) = ∑\nt∈T (b)\n( 1 + Q (b) t\n2b+1\n) .\nTo overcome this problem, we slightly modify Exp3-DOM by applying a doubling trick10 to guess Q (b) for each b. Specifically, for each b = 0, 1, . . . , ⌊log2K⌋, we use a sequence γ(b)r = √ (2b lnK)/2r, for r = 0, 1, . . . . We initially run the algorithm with γ (b) 0 . Whenever the algorithm is running with\n10 The pseudo-code for the variant of Exp3-DOM using such a doubling trick is not displayed in this extended abstract.\nγ (b) r and observes that ∑ sQ (b) s > 2 r, where the sum is over all s so far in T (b),11 then we restart the algorithm with γ (b) r+1. Because the contribution of instance b to (12) is\n2b lnK\nγ(b) + γ(b)\n∑\nt∈T (b)\n( 1 + Q (b) t\n2b+1\n) ,\nthe regret we pay when using any γ (b) r is at most 2 √ (2b lnK)2r. The largest r we need is ⌈ log2 Q (b)⌉ and ⌈log2 Q (b) ⌉∑\nr=0\n2r/2 < 5 √ Q (b) .\nSince we pay regret at most 1 for each restart, we get\nE [ LA,T − Lk,T ] ≤ c\n⌊log2 K⌋∑\nb=0\nE   √√√√√(lnK)  2b|T (b)|+ 1\n2\n∑\nt∈T (b)\nQ (b) t\n + ⌈ log2Q (b)⌉   .\nfor some positive constant c. Taking into account that\n⌊log2 K⌋∑\nb=0\n2b|T (b)| ≤ 2 T∑\nt=1\n|Rt|\n⌊log2 K⌋∑\nb=0\n∑\nt∈T (b)\nQ (b) t =\nT∑\nt=1\nQ (bt) t\n⌊log2 K⌋∑\nb=0\n⌈ log2 Q (b)⌉ = O ( (lnK) ln(KT ) ) ,\nwe obtain\nE [ LA,T − Lk,T ] ≤ c\n⌊log2 K⌋∑\nb=0\nE   √√√√√(lnK)  2b|T (b)|+ 1\n2\n∑\nt∈T (b)\nQ (b) t\n   +O ( (lnK) ln(KT ) )\n≤ c ⌊log2 K⌋E\n  √√√√ lnK ⌊log2 K⌋ T∑\nt=1\n( 2|Rt|+ 1\n2 Q\n(bt) t\n) +O ( (lnK) ln(KT ) )\n= O  (lnK)E   √√√√ T∑\nt=1\n( 4|Rt|+Q(bt)t )  + (lnK) ln(KT )  \nas desired.\n11 Notice that ∑\ns Q\n(b) s is an observable quantity.\nProof of Corollary 8. We start off from the upper bound (3) in the statement of Theorem 7. We want to bound the quantities |Rt| and Q(bt)t occurring therein at any step t in which a restart does not occur —the regret for the time steps when a restart occurs is already accounted for by the term O ( (lnK) ln(KT ) ) in (3). Now, Lemma 11 gives\n|Rt| = O ( α(Gt) lnK ) .\nIf γt = γ (bt) t for any time t when a restart does not occur, it is not hard to see that γt =\nΩ (√ (lnK)/(KT ) ) . Moreover, Lemma 13 states that\nQt = O ( α(Gt) ln(K 2/γt) + |Rt| ) = O ( α(Gt) ln(K/γt) ) .\nHence, Qt = O ( α(Gt) ln(KT ) ) .\nPutting together as in (3) gives the desired result."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2013,
    "abstractText" : "<lb>We consider the partial observability model for multi-armed bandits, introduced by Mannor<lb>and Shamir [11]. Our main result is a characterization of regret in the directed observability<lb>model in terms of the dominating and independence numbers of the observability graph. We also<lb>show that in the undirected case, the learner can achieve optimal regret without even accessing<lb>the observability graph before selecting an action. Both results are shown using variants of the<lb>Exp3 algorithm operating on the observability graph in a time-efficient manner.",
    "creator" : "LaTeX with hyperref package"
  }
}