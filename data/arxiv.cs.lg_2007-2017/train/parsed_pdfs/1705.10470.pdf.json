{
  "name" : "1705.10470.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Iterative Machine Teaching",
    "authors" : [ "Weiyang Liu", "Bo Dai", "James M. Rehg", "Le Song" ],
    "emails" : [ "<wyliu@gatech.edu,", "lsong@cc.gatech.edu>." ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "Machine teaching is the problem of constructing an optimal (usually minimal) dataset according to a target concept such that a student model can learn the target concept based on this dataset. Recently, there is a surge of interests in machine teaching which has found diverse applications in model compression (Bucila et al., 2006; Han et al., 2015; Ba & Caruana, 2014; Romero et al., 2014), transfer learning (Pan & Yang, 2010) and cyber-security problems (Alfeld et al., 2016; 2017; Mei & Zhu, 2015). Furthermore, machine teaching is also closely related to other subjects of interests, such as curriculum learning (Bengio et al., 2009) and knowledge distilation (Hinton et al., 2015).\nIn the traditional machine learning paradigm, a teacher will\n*The first two authors contributed equally. 1College of Computing, Georgia Institute of Technology, USA. Correspondence to: <wyliu@gatech.edu, lsong@cc.gatech.edu>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\nSample query\nProvide label for sample\nOracle\nDataset\nProvide training set\nTeacher\nProv ide i\nnfor mati\non\nCon struc\nt min imal\ntrain ing s\net\nInter act o\nnly o nce\nLearner\nTeacher Iterative Learner\nProvide information\nProvide samples for this iteration\nInteract iteratively Iterative Machine Teaching\nActive Learning\nPassive Learning\nMachine Teaching\nFigure 1. Comparison between iterative machine teaching and the other learning paradigms.\ntypically construct a batch set of examples, and provide them to a learning algorithm in one shot; then the learning algorithm will work on this batch dataset trying to learn the target concept. Thus, many research work under this topic try to construct the smallest such dataset, or characterize the size of of such dataset, called the teaching dimension of the student model (Zhu, 2013; 2015). There are also many seminal theory work on analyzing the teaching dimension of different models (Shinohara & Miyano, 1991; Goldman & Kearns, 1995; Doliwa et al., 2014; Liu et al., 2016). However, in many real world applications, the student model is typically updated via an iterative algorithm, and we get the opportunity to observe the performance of the student model as we feed examples to it. For instance, • In model compression where we want to transfer a tar-\nget “teacher model” to a destination “student model”, we can constantly observe student model’s prediction on current training points. Intuitively, such observations will allow us to get a better estimate where the student model is and pick examples more intelligently to better guide the student model to convergence. • In cyber-security setting where an attack wants to mislead a recommendation system that learns online, the attacker can constantly generate fake clicks and observe the system’s response. Intuitively, such feedback will allow the attacker to figure out the state of the learning system, and design better strategy to mislead the system.\nFrom the aspects of both faster model compression and better avoiding hacker attack, we seek to understand some fun-\nar X\niv :1\n70 5.\n10 47\n0v 1\n[ st\nat .M\nL ]\n3 0\nM ay\n2 01\n7\ndamental questions, such as, what is the sequence of examples that teacher should feed to the student in each iteration in order to achieve fast convergence? And how many such examples or such sequential steps are needed?\nIn this paper, we will focus on this new paradigm, called iterative machine teaching, which extends traditional machine teaching from batch setting to iterative setting. In this new setting, the teacher model can communicate with and influence the student model in multiple rounds, but the student model remains passive. More specifically, in each round, the teacher model can observe (potentially different levels of) information about the students to intelligently choose one example, and the student model runs a fixed iterative algorithm using this chosen example.\nFurthermore, the smallest number of examples (or rounds) the teacher needs to construct in order for the student to efficiently learn a target model is called the iterative teaching dimension of the student algorithm. Notice that in this new paradigm, we shift from describing the complexity of a model to the complexity of an algorithm. Therefore, for the same student model, such as logistic regression, the iterative teaching dimension for a teacher model can be different depending on the student’s learning algorithms, such as gradient descent versus conjugate gradient descent. In some sense, the teacher in this new setting is becoming active, but not the student. In Fig. 1, we summarize the differences of iterative machine teaching from traditional machine teaching, active learning and passive learning.\nBesides introducing the new paradigm, we also propose three iterative teaching algorithms, called omniscient teacher, surrogate teacher and imitation teacher, based on the level of information about the student that the teacher has access to. Furthermore we provide partial theoretical analysis for these algorithms under different example construction schemes. Our analysis shows that under suitable conditions, iterative teachers can always perform better than passive teacher, and achieve exponential improvements. Our analysis also identifies two crucial properties, namely teaching monotonicity and teacher capability, which play critical roles in achieving fast iterative teaching.\nTo corroborate our theoretical findings, we also conduct extensive experiments on both synthetic data and real image data. In both cases, the experimental results verify our theoretical findings and the effectiveness of our proposed iterative teaching algorithms."
    }, {
      "heading" : "2. Related Work",
      "text" : "Machine teaching. Machine teaching problem is to find an optimal training set given a student model and a target. (Zhu, 2015) proposes a general teaching framework. (Zhu, 2013) considers Bayesian learner in exponential family and expresses the machine teaching as an optimization prob-\nlem over teaching examples that balance the future loss of the learner and the effort of the teacher. (Liu et al., 2016) provides the teaching dimension of several linear learners. The framework has been applied to security (Mei & Zhu, 2015), human computer interaction (Meek et al., 2016) and education (Khan et al., 2011). (Johns et al., 2015) further extends machine teaching to interactive settings. However, these work ignores the fact that a student model is typically learned by an iterative algorithm, and we usually care more about how fast the student can learn from the teacher.\nInteractive Machine Learning. (Cakmak & Thomaz, 2014) consider the scenario of a human training an agent to perform a classification task by showing examples. They study how to improve human teacher by giving teaching guidance. (Singla et al., 2014) consider the crowdsourcing problem and propose a sequential teaching algorithm that can teach crowd worker to better classify the query. Both work consider a very different setting where the learner (i.e. human learner) is not iterative and does not have a particular optimization algorithm.\nActive learning. Active learning enables a learner to interactively query the oracle to obtain the desired outputs at new samples. Machine teaching is different from active learning in the sense that active learners explore the optimal parameters by itself rather than being guided by the teacher. Therefore they have different sample complexity (Balcan et al., 2010; Zhu, 2013).\nCurriculum learning. Curriculum learning (Bengio et al., 2009) is a general training strategy that encourages to input training examples from easy ones to difficult ones. Very interestingly, our iterative teacher model suggests similar training strategy in our experiments."
    }, {
      "heading" : "3. Iterative Machine Teaching",
      "text" : "The proposed iterative machine teaching is a general concept, and the paper considers the following settings:\nStudent’s Asset. In general, the asset of a student (learner) includes the initial parameter w0, loss function, optimization algorithm, representation (feature), model, learning rate ηt over time (and initial η0) and the trackability of the parameter wt. The ideal case is that a teacher has access to all of them and can track the parameters and learning rate, while the worst case is that a teacher knows nothing about them. How practical the teaching is depends on the prior knowledge and trackability of a teacher.\nRepresentation. The teacher represents an example as (x, y) while the student represents the same example as (x̃, ỹ) (typically y= ỹ). The representation x∈X and x̃∈ X̃ can be different but deterministically related. We assume there exists x̃=G(x) for an unknown invertible mapping G. Model. The teacher uses a linear model y=〈v, x〉 with parameter v∗ (w∗ for student’s space) that is taught to the stu-\ndent. The student also uses a linear model ỹ=〈w, x̃〉 with parameter w, i.e., ỹ=〈w,G(x)〉=f(x) in general. w and v do not necessarily lie in the same space, but for omniscient teacher, they are equivalent and interchangeably used.\nTeaching protocol. In general, the teacher can only communicate with the student via examples. In this paper, the teacher provides one example xt in one iteration, where t denotes the t-th iteration. The goal of the teacher is to provide examples in each iteration such that the student parameter w converge to its optimum w∗ as fast as possible.\nLoss function. The teacher and student share the same loss function. We assume this is a convex loss function `(f(x), y), and the best model is usually found by minimizing the expected loss below:\nw∗ = argmin w E(x,y) [`(〈w, x〉 , y)] . (1)\nwhere the sampling distribution (x, y)∼P(x, y). Without loss of generality, we only consider typical loss functions, such as square loss 12 (〈w, x〉−y)\n2, logistic loss log(1+ exp(−y 〈w, x〉)) and hinge loss max(1−y 〈w, x〉 , 0).\nAlgorithm. The student uses the stochastic gradient descent to optimize the model. The iterative update is\nwt+1 = wt − ηt ∂`(〈w, x〉 , y)\n∂w . (2)\nWithout teacher’s guiding, the student can be viewed as being guided by a random teacher who randomly feed an example to the student in each iteration."
    }, {
      "heading" : "4. Teaching by an Omniscient Teacher",
      "text" : "An omniscient teacher has access to the student’s feature space, model, loss function and optimization algorithm. In specific, omniscient teacher’s (x, y) and student’s (x̃, ỹ) share the same representation space, and teacher’s optimal model v∗ is also the same as student’s optimal model w∗."
    }, {
      "heading" : "4.1. Intuition and teaching algorithm",
      "text" : "In order to gain intuition on how to make the student model converge faster, we will start with looking into the difference between the current student parameter and the teacher parameter w∗ during each iteration:∥∥wt+1 − w∗∥∥2\n2 = ∥∥∥∥wt − ηt ∂`(〈w, x〉 , y)∂w − w∗ ∥∥∥∥2 2\n= ∥∥wt − w∗∥∥2\n2 + η2t\n∥∥∥∥∥∂`( 〈 wt, x 〉 , y) ∂wt ∥∥∥∥∥ 2\n2︸ ︷︷ ︸ T1(x,y|wt):Difficulty of an example (x, y)\n− 2ηt 〈 wt − w∗, ∂`( 〈 wt, x 〉 , y)\n∂wt 〉 ︸ ︷︷ ︸ T2(x,y|wt):Usefulness of an example (x, y)\n(3)\nBased on the decomposition of the parameter error, the teacher aims to choose a particular example (x, y) such that ‖wt+1−w∗‖22 is most reduced compared to ‖wt−w∗‖22\nfrom the last iteration. Thus the general strategy for the teacher is to choose an example (x, y), such that η2t T1− 2ηtT2 is minimized in the t-th iteration:\nargmin x∈X ,y∈Y\nη2t T1(x, y|wt)− 2ηtT2(x, y|wt). (4)\nThe teaching algorithm of omniscient teacher is summarized in Alg.1. The smallest value of η2t T1−2ηtT2 is −‖wt − w∗‖22. If the teacher achieves this, it means that we have reached the teaching goal after this iteration. However, it usually cannot be done in just one iteration, because of the limitation of teacher’s capability to provide examples. T1 and T2 have some nice intuitive interpretations:\nDifficulty of an example. T1 quantifies the difficulty level of an example. This interpretation for different loss functions becomes especially clear when the data lives on the surface of a sphere, i.e., ‖x‖=1. For instance, • For linear regression, T1 =(〈w, x〉−y)2. The larger the\nnorm of gradient is, the more difficult the example is. • For logistic regression, we have T1 =‖ 11+exp(y〈w,x〉)‖ 2 2.\nWe know that 11+exp(y〈w,x〉) is the probability of predicting the wrong label. The larger the number is, the more difficult the example is. • For support vector machines, we have T1 = 12 (sign(1− y 〈w, x〉)+1). Different from above losses, the hinge loss has a threshold to identify the difficulty of examples. While the example is difficult enough, it will produce 1. Otherwise it is 0.\nInterestingly, the difficulty level is not related to the teacher w∗, but is based on the current parameters of the learner wt. From another perspective, the difficulty level can also be interpreted as the information that an example carries. Essentially, a difficult example is usually more informative. In such sense, our difficulty level has similar interpretation to curriculum learning, but with different expression.\nUsefulness of an example. T2 quantifies the usefulness of an example. Concretely, T2 is the correlation between discrepancy wt − w∗ and the information (difficulty) of an example. If the information of the example has large correlation with the discrepancy, it means that this example is very useful in this teaching iteration.\nTrade-off. Eq.(4) aims to minimize the difficulty level T1 and maximize the usefulness T2. In other word, the teacher always prefers easy but useful examples. When the learning rate is large, T1 term plays a more important role. When learning rate is small, T2 term plays a more important role. This suggests that initially the teacher should choose easier examples to feed into the student model, and later on the teacher should choose examples to focus more on reducing the discrepancy between wt − w∗. Such examples are very likely the difficult ones. Even if the learning rate is fixed, the gradient ∇w` is usually large for a convex loss function at the beginning, so reducing the difficulty level\n(choosing easy examples) is more important. While near the optimum, the gradient ∇w` is usually small, so T2 becomes more important. It is also likely to choose difficult examples. It has nice connection with curriculum learning (easy example first and difficult later) and boosting (gradually focus on difficult examples)."
    }, {
      "heading" : "4.2. Teaching monotonicity and universal speedup",
      "text" : "Can the omniscient teacher always do better than a teacher who feed random examples to the student (in terms of convergence)? In this section, we identify generic conditions under which we can guarantee that the iterative teaching algorithm always perform better than random teacher.\nDefinition 1 (Teaching Volume) For a specific loss function `, we first define a teaching volume function TV (w) with model parameter w as\nTV (w) = max x∈X ,y∈Y\n{−η2t T1(x, y|w) + 2ηtT2(x, y|w)} (5)\nTheorem 2 (Teaching Monotonicity) Given a training set X and a loss function `, if the inequality ‖w1 − w∗‖2 − TV (w1) ≤ ‖w2 − w∗‖2 − TV (w2) (6) holds for any w1, w2 that satisfy ‖w1−w∗‖2≤‖w2− w∗‖2, then with the same parameter initialization and learning rate, the omniscient teacher can always converge not slower than random teacher.\nThe teaching volume represents the teacher’s teaching effort in this iteration, so ‖wt−w∗‖2−TV (wt) characterizes the remaining teaching effort needed to achieve the teaching goal after iteration t. Theorem 2 says that for a loss function and a training set, if the remaining teaching effort is monotonically decreasing while the model parameter gets closer to the optimum, we can guarantee that the omniscient teacher can always converge not slower than random teacher. It is a sufficient condition for loss functions to achieve faster convergence than SGD. For example, the square loss satisfies the condition with certain training set:\nProposition 3 The square loss satisfies the teaching monotonicity condition given the training set {x|‖x‖ ≤ R}."
    }, {
      "heading" : "4.3. Teaching capability and exponential speedup",
      "text" : "The theorem in previous subsection insures that under certain conditions the omniscient teacher can always lead to faster convergence for the student model, but can there be exponential speedup? To this end, we introduce further assumptions of the “richness” of teaching examples, which we call teaching capability. We start from the ideal case, i.e., the synthesis-based omniscient teacher with hyperspherical feature space, and then, extend to real cases with the restrictions on teacher’s knowledge domain, sampling scheme, and student information. We present specific teaching strategies in terms of teaching capability (strong to weak): synthesis, combination and (rescalable) pool.\nSynthesis-based teaching. In synthesis-based teaching, the teacher can provide any samples from\nX = {x ∈ Rd, ‖x‖ ≤ R} Y = R (Regression) or {−1, 1} (Classification).\nTheorem 4 (Exponential Synthesis-based Teaching) For a synthesis-based omniscient teacher and a student with fixed learning rate η 6=0, if the loss function `(·, ·) satisfies that for any w ∈ Rd, there exists γ 6=0, |γ|≤ R‖w−w∗‖ such that while x̂=γ (w−w∗) and ŷ∈Y , we have\n0 < γ∇〈w,x̂〉` (〈w, x̂〉 , ŷ) ≤ 1\nη ,\nthen the student can learn an -approximation of w∗ with O(Cγ,η1 log 1 ) samples. We call such loss function `(·, ·) exponentially teachable in synthesis-based teaching.\nThe constant is Cγ,η1 =(log 1 1−ην(γ) ) −1 in which ν(γ) := minw,y γ∇〈w,x̂〉` (〈w, x̂〉 , y)>0. ν(γ) is related to the convergence speed. Note that the sample complexity serves as the iterative teaching dimension corresponding to this particular teacher, student, algorithm and training data.\nThe sample complexity in iterative teaching is deterministic, different from the high probability bounds of traditional sample complexity with random i.i.d.samples or actively required samples. This is because the teacher provides the samples deterministically without noise in every iteration.\nThe radiusR for X , which can be interpreted as the knowledge domain of the teacher, will affect the sample complexity by constraining the valid values of γ, and thus Cγ,η1 . For example, for absolute loss, if R is large, such that 1 η ≤ R ‖w0−w∗‖ , γ can be set to 1 η and the ν(γ) will be 1 η in this case. Therefore, we have Cγ,η1 =0, which means the student can learn with only one example (one iteration). However, if 1η > R ‖w0−w∗‖ , we have C γ,η 1 >0, and the student can converge exponentially. The similar phenomenon appears in the square loss, hinge loss, and logistic loss. Refer to Appendix A for details.\nThe exponential synthesis-based teaching is closely related to Lipschitz continuity and strong convexity of loss functions in the sense that the two regularities provide positive lower and upper bound for γ∇〈w,x〉` (〈w, x〉 , y).\nProposition 5 The Lipschitz continuous and strongly convex loss functions are exponentially teachable in synthesisbased teaching.\nThe exponential synthesis-based teachability is a weaker condition compared to the strong convexity and Lipschitz continuity. We can show that besides the Lipschitz continuous and strongly convex loss, there are some other loss functions, which are not strongly convex, but still are exponentially teachable in synthesis-based scenario, e.g., the hinge loss and logistic loss. Proofs are in Appendix A.\nCombination-based teaching. In this scenario, the teacher\nAlgorithm 1 The omniscient teacher 1: Randomly initialize the student and teacher parameter w0; 2: Set t = 1 and the maximal iteration number T ; 3: while wt has not converged or t < T do 4: Solve the optimization (e.g., pool-based teaching):\n(x t , y t ) = argmin x∈X ,y∈Y η 2 t ∥∥∥∥∥∥ ∂` (〈 wt−1, x 〉 , y ) ∂wt−1 ∥∥∥∥∥∥ 2\n− 2ηt 〈 w t−1 − w∗, ∂` (〈 wt−1, x 〉 , y )\n∂wt−1\n〉\n5: Use the selected example (xt, yt) to perform the update:\nw t = w t−1 − ηt ∂` (〈 wt−1, xt 〉 , yt )\n∂wt−1 .\n6: t← t+ 1 7: end while\ncan provide examples from (αi ∈ R)\nX = { x|‖x‖ ≤ R, x = m∑ i=1 αixi, xi ∈ D } ,D = {x1, . . . , xm} Y = R (Regression) or {−1, 1} (Classification)\nCorollary 6 For a combination-based omniscient teacher and a student with fixed learning rate η 6= 0 and initializationw0, if the loss function is exponentially synthesis-based teachable and w0−w∗∈span (D), the student can learn an -approximation of w∗ with O ( Cγ,η1 log 1 ) samples.\nAlthough the knowledge pool of teacher is more restricted compared to the synthesis-based scenario, with teacher’s extra work to combine samples, the teacher can behave the same as the most knowledgable synthesis-based teacher.\nRescalable pool-based teaching. This scenario is further restricted in both knowledge pool and the effort to prepare samples. The teacher can provide examples from X ×Y: X = {x|‖x‖ ≤ R, x = γxi, xi ∈ D, γ ∈ R},D = {x1, . . .} Y = R (Regression) or {−1, 1} (Classification)\nIn such scenario, we cannot get arbitrary direction rather than the samples from the candidate pool. Therefore, to achieve the exponential improvement, the candidate pool should contain rich enough directions. To characterize the richness in finite case, we define the pool volume as\nDefinition 7 (Pool Volume) Given the training example pool X ∈ Rd, the volume of X is defined as\nV(X ) := min w∈span(D) max x∈X 〈w, x〉 ‖w‖2 .\nObviously, for the candidate pool of the synthesis-based teacher, we have V(X )=1. In general, for finite candidate pool, the pool volume is 0<V(X )<1.\nTheorem 8 For a rescalable pool-based omniscient teacher and a student with fixed learning rate η 6=0 and initialization w0, if for any w∈Rd, w 6⊥w∗ and w0−w∗∈ span (D), there exists {x, y}∈X ×Y and γ such that while\nx̂= γ‖w−w ∗‖\n‖x‖ x, ŷ=y, we have\n0 < γ∇〈w,x̂〉` (〈w, x̂〉 , ŷ) < 2V(X ) η ,\nthen the student can learn an -approximation of w∗ with O(Cη,γ,V(X )2 log 1 ) samples. We say such loss function is exponentially teachable in rescalable pool-based teaching.\nThe pool volume plays a vital role in pool-based teaching. It not only affects the existence of γ and {x̂, ŷ} to satisfy the conditions, but also changes the convergence rate. While V(X ) increases, Cη,γ,V(X )2 will decrease, yielding smaller sample complexity. With V(X )< 1, the rescalable pool-based teaching requires more samples than the synthesis-based teaching. As V(X ) increases to 1, the candidate pool becomes { x∈Rd, ‖x‖≤R } and C η,γ,V(X ) 2 approaches to C γ,η 1 . Then the convergence speed of rescalable pool-based teaching approaches to the synthesis/combination-based teaching."
    }, {
      "heading" : "5. Teaching by a less informative teacher",
      "text" : "To make the teacher model useful in practice, we further design two less informative teacher model that requires less and less information from the student."
    }, {
      "heading" : "5.1. The surrogate teacher",
      "text" : "Suppose we can only query the function output from the learned 〈wt, x〉, but we can not directly access wt. How can we choose the example? In this case we propose to make use of the the convexity of the loss function. That is〈 wt − w∗, ∂`( 〈 wt, x 〉 , y)\n∂wt x\n〉 > `( 〈 wt, x 〉 , y)− `(〈w∗, x〉 , y).\n(7) Taking the pool-based teaching as an example, we can instead optimize the following surrogate loss function:\n(xt, yt) = argmin {x,y}∈X η2t ∥∥∥∥∥∂`( 〈 wt, x 〉 , y) ∂wt ∥∥∥∥∥ 2\n2 − 2ηt ( `( 〈 wt, x 〉 , y)− `(〈w∗, x〉 , y) ) (8) by replacing 〈 wt − w∗, ∂`(〈w\nt,x〉,y) ∂wt\n〉 with its lower\nbound. The advantage of this approach is that the teacher only need to query the learner for the function output 〈wt, x〉 to choose the example, without the need to access the learner parameter wt directly. Furthermore, after noticing that in this formulation, the teacher makes prediction via inner products, we find that the surrogate teacher can also be applied to the scenario where the teacher and the student use different feature spaces by further replacing (`(〈wt, x〉 , y)−`(〈w∗, x〉 , y)) with (`(〈wt, x〉 , y)−`(〈v∗, x̃〉 , y)). With this modification, we can provide examples without using information about w∗. The performance of the surrogate teacher largely depends on the tightness of such convexity lower bound.\nAlgorithm 2 The imitation teacher 1: Randomly initialize the student parameter w0 and the teacher\nparameter v0; Randomly select a training sample (x0, y0); 2: Set t = 1 and the maximal iteration number T ; 3: while wt has not converged or t < T do 4: Perform the update:\nv t = v t−1 − ηv (〈 v t−1 , x t−1 〉 − 〈 w t , x t−1 〉) x t−1.\n5: Solve the optimization (e.g., pool-based teaching):\n(x t , y t ) = argmin x∈X ,y∈Y η 2 t ∥∥∥∥∥∥ ∂` (〈 wt, x 〉 , y ) ∂vt ∥∥∥∥∥∥ 2\n− 2ηt 〈 v t − v∗, ∂` (〈 vt, x 〉 , y )\n∂vt\n〉.\n6: Provide the selected example (xt, yt) for the student to perform the update ;\nw t+1 = w t − ηt\n∂` (〈 wt, x 〉 , y )\n∂w .\n7: t← t+ 1 8: end while"
    }, {
      "heading" : "5.2. The imitation teacher",
      "text" : "When the teacher and the student have different feature spaces, this teaching setting will be much closer to practice than all the previous settings and also more challenging. To this end, we present an imitation teacher who learns to imitate the inner product output 〈wt, x〉 of the student model and simultaneously choose examples in teacher’s own feature space. The teacher can possibly use active learning to imitate the student’s 〈wt, x〉. In this imitation, the student model stays unchanged and the teacher model could update itself via multiple queries to the student (input an example and see the inner product output of the student). We present a more simple and straightforward imitation teacher (Alg. 2) which works in a way similar to stochastic mirror descent (Nemirovski et al., 2009; Hall & Willett, 2013). In specific, the teacher first learns to approximate the student’s 〈wt, x〉 with the following iterative update:\nvt+1=vt−ηv (〈 vt, x 〉 − 〈 wt, x 〉) x (9)\nwhere ηv is the learning rate for the update. Then we use vt+1 to perform the example synthesis or selection in teacher’s own feature space. We summarize this simple yet effective imitation teacher model in Alg. 2."
    }, {
      "heading" : "6. Discussion",
      "text" : "Optimality of the teacher model. For arbitary loss functions, the optimal teacher model for a student model should find the training example sequence to achieve the fastest possible convergence. Exhaustively finding such example sequence is computational impossible. For example, there are nT possible training sequences (T is the iteration number) for n-size pool-based teaching. As a results, we need to make use of the properties of loss function to design the teacher model. The proposed teacher models are not necessarily optimal, but they are good enough under some conditions for loss function, student model and training data.\nTheoretical aspects of the teacher model. The theoretical study of the teacher model includes finding the conditions for the loss function and training data such that the teacher model is optimal, or achieves provable faster convergence rate, or provably converges faster than the random teacher. We desire these conditions to be sufficient and necessary, but sometimes sufficient conditions suffice in practice. For different student models, the theoretical analysis may be different and we merely consider stochastic gradient learner here. There are still lots of optimization algorithms that can be considered. Besides, our teacher models are not necessarily the best, so it is also important to come up with better teacher models with provable guarantees. Although our paper mainly focuses on the fixed learning rate, our results are still applicable for the dynamic learning rate. However, the teacher should be more powerful in synthesizing or choosing examples (R should be larger than fixed learning rate case). In human teaching, it actually makes sense because while teaching a student who learns knowledge with dynamic speed, the teacher should be more powerful so that the student consistently learn fast.\nPractical aspects of the teacher model. In practice, we usually want the teacher model to be less and less informative to the student model, scalable to large datasets, efficient to compute. How to make the teacher model scalable, efficient and least informative remains open challenges."
    }, {
      "heading" : "7. Experiments",
      "text" : ""
    }, {
      "heading" : "7.1. Experimental details",
      "text" : "Performance metric. We use three metric to evaluate the convergence performance: objective value w.r.t. the training set, difference between wt and w∗ (‖wt − w∗‖2), and the classification accuracy on testing set.\nParameters and setup. We give detailed experimental setup in Appendix B. In the first section, we compare different teaching strategies, while in the remaining sections, we evaluate the practical pool-based teaching. For fairness, learning rates for all compared methods are the same."
    }, {
      "heading" : "7.2. Comparison of different teaching strategies",
      "text" : "We first compare four different teaching strategies for the omniscient teacher. We consider two scenarios. One is that\nthe dimension of feature space is smaller than the number of samples (the given features are sufficient to represent the entire feature), and the other is that the feature dimension is greater than the number of samples (the given features are not sufficient to represent the entire feature). In these two scenarios, we find that synthesis-based teaching usually works the best and always achieves exponential convergence. The combination-based teaching is exactly the same as the synthesis-based teaching in the first scenario, but it is much worse than synthesis in the second scenario. Rescalable pool-based teaching is also better than poolbased teaching. Empirically, the experiment verifies our theoretical findings: the more flexible the teaching strategy is, the more convergence gain we may obtain."
    }, {
      "heading" : "7.3. Teaching linear models on Gaussian data",
      "text" : "This experiment explores the convergence of three typical linear models: ridge regression (RR), logistic regression (LR) and support vector machine (SVM) on Gaussian data. Note that SGD on selected set is to run SGD on the union of all samples selected by the omniscient teacher. For the scenario of different feature spaces, we use a random orthogonal projection matrix to generate the teacher’s feature space from student’s. All teachers use pool-based teaching strategy. For fair comparisons, we use the same random initialization and the same learning rate.\nTeaching in the same feature space. The results in Fig. 3 show that the learner can converge much faster using the example provided by the teacher, showing the effectiveness of our teaching models. As expected, we find that the omniscient teacher consistently achieves faster convergence than the surrogate teacher who has no access to w. It is because the omniscient teacher always has more information about the learner. More interestingly, our guiding algorithms also consistently outperform SGD on the selected set, showing that the order of inputting training samples matters.\nTeaching in different feature spaces. It is a more practical scenario that teacher and student use different feature spaces. While the omniscient teacher model is no longer\napplicable here, we teach the student model using the surrogate teacher and the imitation teacher. While the feature spaces are totally different, it can be expected that there will be a mismatch gap between the teacher model parameter and the student model parameter. Even in such a challenging scenario, the experimental results show that our teacher model still outperforms the conventional SGD and batch GD in most cases. One can observe that the surrogate teacher performs poorly in the SVM, which may be caused by the tightness of the approximated lower bound of the T2 term. Compared to the surrogate teacher, the imitation teacher is more stable and consistently improves the convergence in all three linear models."
    }, {
      "heading" : "7.4. Teaching linear models on uniform spherical data",
      "text" : "In this experiment, we use a different data distribution to further evaluate the teacher models. We will examine LR and SVM by classifying uniform spherical data.\nTeaching in the same feature space. From Fig. 5, one can observe that the convergence is consistently improved while using omniscient teacher to provide examples to learners. We find that the significance of improvement is related to the training data distribution and loss function, as indicated by our theoretical results. The surrogate teacher produces less convergence gain in SVM, because the convexity lower bound becomes very loose in this case. Overall, omniscient teacher still presents strong teaching capability. More interestingly, we use simple SGD run on the sample set selected by the omniscient teacher and also get faster convergence, showing that the selected example set is better than the entire set in terms of convergence.\nTeaching in different feature spaces. While the teacher and student use different feature spaces, one can observe\nfrom Fig. 5 that the surrogate teacher performs very poorly, even worse than the original SGD and BGD. The imitation teacher works much better and achieves consistent and significant convergence speedup, showing its superiority while the teacher and the student use different features."
    }, {
      "heading" : "7.5. Teaching Linear Classifiers on MNIST Dataset",
      "text" : "We further evaluate our teacher models on MNIST dataset. We use 24D random features to classify the digits (0/1, 3/5 as examples). We generate the teacher’s features using a random projection matrix from the original 24D student’s features. Note that, omniscient teacher and surrogate teacher (same space) assume the teacher uses the student’s feature space, while surrogate teacher (different space) and imitation teacher assume the teacher uses its own space. From Fig. 6, one can observe that all these teacher model produces significant convergence speedup. We can see that the omniscient teacher converges fastest as expected. Interestingly, our imitation teacher achieves very similar convergence speedup to the omniscient teacher under the condition that the teacher does not know the student’s feature space. In Fig.7, we also show some examples of teacher’s selected digit images (0/1, 7/9 as examples) and find that the teacher tends to select easy example at the beginning and gradually shift the focus to difficult examples. This also has the intrinsic connections with the curriculum learning. To some extent, we theoretically show the correctness of the philosophy of curriculum learning"
    }, {
      "heading" : "7.6. Teaching Fully Connected Layers in Convolutional Neural Networks on CIFAR-10 Dataset",
      "text" : "We further extend our teacher models from binary classification to multi-class classification. We use the teacher model to teach the final fully connected layers in convo-\nlutional neural network on CIFAR-10 dataset. We first train three baseline CNNs (6/9/12 convolution layers, detailed network configuration is given in the appendix) on CIFAR-10 without data augmentation and obtain the 83.5%, 86.1%, 87.2% accuracy. First, we applied the omniscient teacher and the surrogate teacher to the CNN-6 student using the optimal fully connected layer from the joint backprop training. This is essentially to teach the fully connected layer in the same feature space. Second, we applied the surrogate teacher and the imitation teacher to the CNN-6 student using the parameters of optimal fully connected layers from CNN-9 and CNN-12. This is essentially to teach the fully connected layer in different feature spaces. More interestingly, this different feature space may not necessarily have an invertible one-to-one mapping, but we could still observe convergence speedup using our teacher models. From Fig. 8, we can see that all the teacher models produces very fast convergence in terms of testing accuracy. We also show the classification accuracy of using the original back-prop FC layer in Fig. 8. While the iteration goes, our teacher models can even produces better testing accuracy than the original learned FC layer. In terms of the objective value, the omniscient teacher shows the greatest convergence speedup, and the imitation teacher performs slightly worse but still much better than the SGD.\nTe st\nin g\nA cc\nur ac\ny\nIteration Number Iteration Number\nO bj\nec tiv\ne Va\nlu e\n0 500 1000 1500 2000 2500 3000\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\nJoint learned FC (Backprop) SGD learned FC Omniscient teacher Surrogate teacher (same) Surrogate teacher (CNN-9) Imitation teacher (CNN-9) Imitation teacher (CNN-12)\n0 500 1000 1500 2000 2500 3000\n1.2\n1.4\n1.6\n1.8\n2\n2.2 2.4 SGD Omniscient teacher Surrogate teacher (same) Surrogate teacher (CNN-9) Imitation teacher (CNN-9) Imitation teacher (CNN-12)\nFigure 8. Fully Connected Layers in CNNs on CIFAR-10. Left: accuracy on testing set. Right: training objective value."
    }, {
      "heading" : "8. Concluding Remarks",
      "text" : "This paper extends machine teaching to a novel iterative framework. We first elaborate the settings of iterative machine teaching. Then we study two important properties of the iterative machine teaching: teaching monotonicity and teaching capability. Based on the framework, we propose three teacher models for gradient learners, and give theoretical analysis for these models to provably achieve fast convergence. Extensive experiments on both synthetic data and real image datasets verify our theoretical findings and validate the effectiveness of these teacher models."
    }, {
      "heading" : "Acknowledgement",
      "text" : "This project was supported in part by NSF IIS-1218749, NIH BIGDATA 1R01GM108341, NSF CAREER IIS1350983, NSF IIS-1639792 EAGER, ONR N00014-15-12340, Nvidia and Intel."
    }, {
      "heading" : "A. Details of the Proof",
      "text" : "Proof of Theorem 2 We assume the optimization starts with an initialized weights w0. t is denoted as the iteration index. Let wtg and w t s be the model parameter updated by our omniscient teacher and SGD, respectively. We first consider the case where t = 1. For SGD, the first gradient update w1s is\nw1s = w 0 − ηt\n∂`( 〈 w0, xs 〉 , ys)\n∂w0 . (10)\nThen we compute the difference between w1s and w ∗:∥∥w1s − w∗∥∥22 = ∥∥∥∥∥w0 − ηt ∂`( 〈 w0, x 〉 , y) ∂w0 − w∗ ∥∥∥∥∥ 2\n2 = ∥∥w0 − w∗∥∥2\n2 + η2t ∥∥∥∥∥∂`( 〈 w0, x 〉 , y) ∂w0 ∥∥∥∥∥ 2\n2\n− 2ηt 〈 w0 − w∗, ∂`( 〈 w0, x 〉 , y)\n∂w0 〉 (11) Because the omniscient teacher is to minimize last two term, so we are guaranteed to have∥∥w1g − w∗∥∥22 ≤ ∥∥w1s − w∗∥∥22 . (12) So with the same initialization w0g = w 0 s , ∥∥w1g − w∗∥∥22 ≤ ∥∥w1s − w∗∥∥22 is always true. Then we consider the case where t = k, k ≥ 1. We first compute the difference between wk+1g and w∗:∥∥wk+1g − w∗∥∥22 = ∥∥∥∥∥wkg − ηt ∂`( 〈 wkg , x 〉 , y) ∂wk+1 − w∗ ∥∥∥∥∥ 2\n2 = ∥∥wkg − w∗∥∥22 + min{x,y} { η2t ∥∥∥∥∥∂`( 〈 wkg , x 〉 , y)\n∂wkg\n∥∥∥∥∥ 2\n2\n− 2ηt 〈 wkg − w∗, ∂`( 〈 wkg , x 〉 , y)\n∂wkg\n〉}\n= ∥∥wkg − w∗∥∥22 + η2t ∥∥∥∥∥∂`( 〈 wkg , x k ∗ 〉 , yk∗ )\n∂wkg\n∥∥∥∥∥ 2\n2\n− 2ηt 〈 wkg − w∗, ∂`( 〈 wkg , x k ∗ 〉 , yk∗ )\n∂wkg 〉 = ∥∥wkg − w∗∥∥22 − TV (wkg )\n(13)\nwhere xk∗, y k ∗ is the sample selected by the omniscient teacher in the k-th iteration. Using the given conditions, we can bound the difference between wk+1s and w ∗ from below:∥∥wk+1s − w∗∥∥22 = ∥∥∥∥∥wks − ηt ∂`( 〈 wks , x s 〉 , ys) ∂wks − w∗ ∥∥∥∥∥ 2\n2 = ∥∥wks − w∗∥∥22 + η2t ∥∥∥∥∥∂`( 〈 wks , x k s 〉 , yks )\n∂wks\n∥∥∥∥∥ 2\n2\n− 2ηt 〈 wks − w∗, ∂`( 〈 wks , x k s 〉 , yks )\n∂wks 〉 ≥ ∥∥wks − w∗∥∥22 − TV (wks )\n(14)\nwhere xks , y k s is the sample selected by the random teacher in the k-th iteration. Comparing Eq. 13 and Eq. 14 and using the condition in the theorem, the following inequality always holds under the condition ∥∥wkg − w∗∥∥22 ≤ ∥∥wks − w∗∥∥22:∥∥wk+1s − w∗∥∥22 = ∥∥wks − w∗∥∥22 − TV (wks ) ≥ ∥∥wkg − w∗∥∥22 − TV (wkg ) = ∥∥wk+1g − w∗∥∥22 . (15)\nFurther because we already know that ∥∥w1g − w∗∥∥22 ≤ ∥∥w1s − w∗∥∥22, using induction we can conclude that ∥∥wtg − w∗∥∥22 will be always not larger than ‖wts − w∗‖ 2 2 (t can be any iteration). Therefore, in each iteration the omniscient teacher can always converge not slower than random teacher (SGD).\n11\nProof of Proposition 3 Consider the square loss `(〈w, x〉 , y) = (〈w, x〉 − y)2, we have ∂`(〈w,x〉,y)∂w = 2(〈w, x〉 − y)x. Suppose we are given two initializations w1, w2 satisfying ‖w1 − w∗‖22 ≤ ‖w2 − w∗‖ 2 2. For square loss, we first write out\n‖w1 − w∗‖2 − TV (w1) = ‖w1 − w∗‖2 + min x∈X ,y∈Y {η2t T1(x, y|w1)− 2ηtT2(x, y|w1)}\n= ‖w1 − w∗‖2 + min {x,y}\n{ η2t ∥∥∥∥∂`(〈w1, x〉 , y)∂w1 ∥∥∥∥2 2 − 2ηt 〈 w1 − w∗, ∂`(〈w1, x∗〉 , y∗) ∂w1 〉} = ‖w1 − w∗‖2 + { 2( R‖w1−w∗‖ ) 2 ‖w1 − w∗‖2 (w1 − w∗), if R‖w1−w∗‖ < 1 ηt\n−‖w1 − w∗‖2 , if R‖w1−w∗‖ ≥ 1 ηt\n(16)\nSimilarly for w2, we have\n‖w2 − w∗‖2 − TV (w2)\n= ‖w2 − w∗‖2 +\n{ 2( R‖w2−w∗‖ ) 2 ‖w2 − w∗‖2 (w2 − w∗), if R‖w2−w∗‖ < 1 ηt\n−‖w2 − w∗‖2 , if R‖w2−w∗‖ ≥ 1 ηt\n(17)\nThere will be three scenarios to consider: (1) Rηt ≤ ‖w1 − w∗‖ ≤ ‖w2 − w∗‖; (2) ‖w1 − w∗‖ ≤ Rηt ≤ ‖w2 − w∗‖; (3) ‖w1 − w∗‖ ≤ ‖w2 − w∗‖ ≤ Rηt. It is easy to verify that under all three scenarios, we have\n‖w1 − w∗‖2 − TV (w1) ≤ ‖w2 − w∗‖2 − TV (w2) (18)\nTo simplify notations, we denote β(〈w,x〉,y) = ∇〈w,x〉` (〈w, x〉 , y) for a loss function `(·, ·) in the following proof. For omniscient teacher, (x̂, ŷ) denotes a specific construction of (x, y). Notice that (x̃, ỹ) will not be used in omniscient teacher case to avoid ambiguity, since the student and the teacher use the same representation space.\nProof of Theorem 4 At t-step, the omniscient teacher selects the samples via optimization min\nx∈X ,y∈Y η2‖∇wt`\n(〈 wt, x 〉 , y ) ‖2 − 2η 〈 wt − w∗,∇wt` (〈 wt, x 〉 , y )〉 .\nWe denote x̂ = γ (wt − w∗) and ŷ ∈ Y , since γ (w − w∗) ∈ X , we have min\nx∈X ,y∈Y η2‖∇wt`\n(〈 wt, x 〉 , y ) ‖2 − 2η 〈 wt − w∗,∇wt` (〈 wt, x 〉 , y )〉\n(19) ≤ ( η2β2(〈wt,x̂〉,ŷ)γ 2 − 2ηβ(〈wt,x̂〉,ŷ)γ ) ‖wt − w∗‖22. (20)\nPlug Eq. (19) into the recursion Eq. (3), we have∥∥wt+1 − w∗∥∥2 2\n= min x∈X ,y∈Y ∥∥∥∥wt − η ∂`(〈w, x〉 , y)∂w − w∗ ∥∥∥∥2 2\n= ∥∥wt − w∗∥∥2\n2 + min x∈X ,y∈Y η2 ∥∥∥∥∂`(〈wt, x〉 , y)∂wt ∥∥∥∥2 2 − 2η 〈 wt − w∗, ∂`(〈w t, x〉 , y) ∂wt 〉 ≤ (\n1 + η2β2(〈wt,x̂〉,ŷ)γ 2 − 2ηβ(〈wt,x̂〉,ŷ)γ ) ‖wt − w∗‖22 = ( 1− ηβ(〈wt,γ(wt−w∗)〉,ŷ)γ )2 ‖wt − w∗‖22. (21)\nFirst we let ν(γ) = minw,y γ∇〈w,γ(w−w∗)〉` (〈w, γ (w − w∗)〉 , y). Then we have the condition 0 < ν(γ) ≤ γβ(〈w,γ(w−w∗)〉,ŷ) ≤ 1η <∞ for any w, y, so we can obtain\n0 ≤ 1− γηβ(〈w,γ(w−w∗)〉,ŷ) ≤ 1− ην(γ), after simplifying ν(γ) to ν, we therefore have the following inequality from Eq. (21):∥∥wt+1 − w∗∥∥2\n2 ≤ (1− ην)2 ∥∥wt − w∗∥∥2 2 ,\nThus we can have the exponential convergence:∥∥wt − w∗∥∥ 2 ≤ (1− ην)t ∥∥w0 − w∗∥∥ 2 ,\nin other words, the student needs (\nlog 11−ην\n)−1 log ‖w 0−w∗‖ samples to achieve an -approximation of w ∗.\nProof of Proposition 5 Because ` (〈w, x〉 , y) is ζ1-strongly convex w.r.t. w, we have\nζ1\n( ` (〈w, x〉 , y)−min\nw ` (〈w, x〉 , y)\n) ≤ ‖∇w` (〈w, x〉 , y)‖2 = β2(〈w,x〉,y) ‖x‖ 2 , ∀ {x, y} ∈ X × Y,\nwhere X = { x ∈ Rd, ‖x‖ ≤ R } . Using x̂ = γ(w − w∗), γ ≥ 0, we have√\nζ1\n( ` (〈w, γ(w − w∗)〉 , y)−min\nw ` (〈w, γ(w − w∗)〉 , y)\n) ≤ β(〈w,γ(w−w∗)〉,y)γ‖w − w∗‖.\nWe assume the loss function is always non-negative, i.e., ` (〈w, x〉 , y) ≥ 0. Therefore we have√ ζ1 (` (〈w, γ(w − w∗)〉 , y)) ≤ β(〈w,γ(w−w∗)〉,y)γ‖w − w∗‖.\nBecause ` (〈w, x〉 , y) is ζ-strongly convex w.r.t. w, it is also ζ2-strongly convex w.r.t. 〈w, x〉. Then we perform Taylor expansion to ` (〈w, γ(w − w∗)〉 , y) w.r.t. 〈w, x〉 at the point 〈w∗, x〉 and obtain\n` (〈w, γ(w − w∗)〉 , y) ≥ ` (〈w, γ(w∗ − w∗)〉 , y) +∇〈w,x〉` (〈w, γ(w∗ − w∗)〉 , y) (w − w∗)Tx+ ζ2 2 ‖(w − w∗)Tx‖2\nwhich leads to\n` (〈w, γ(w − w∗)〉 , y) ≥ ζ2 2 γ2‖w − w∗‖4\nCombining pieces, we have √ ζ1ζ2\n2 γ‖w − w∗‖ ≤ β(〈w,γ(w−w∗)〉,y)γ. Then if we set γ = min {√\n2 ζ1ζ2 1 ‖w−w∗‖η , R ‖w−w∗‖ } , we can have 1η ≤ β(〈w,γ(w−w∗)〉,y)γ. Because ` (〈w, x〉 , y) is\nLipschitz continuous w.r.t. 〈w, x〉 with parameter L, we have∥∥β(〈w,x〉,y) − β(〈w∗,x〉,y)∥∥ ≤ LR ‖w − w∗‖ Because β(〈w∗,x〉,y) = 0, we have the following inequality:∥∥β(〈w,x〉,y)∥∥ ≤ LR ‖w − w∗‖ If we multiply both side with γ, we can have\nβ(〈w,x〉,y)γ ≤ LR ‖w − w∗‖ γ By setting γ as 1LRη‖w−w∗‖ , we arrive at β(〈w,x〉,y)γ < 1 η . Combining pieces, as long as we set\nγ = min\n{√ 2\nζ1ζ2\n1\nη‖w − w∗‖ ,\nR\n‖w − w∗‖ ,\n1\nLRη ‖w − w∗‖\n} ,\nthen we can have\n0 < c ≤ β(〈w,γx̂〉,ŷ)γ ≤ 1\nη .\nwhere c is a non-zero positive constant. Therefore, we achieve the condition for the exponential synthesis-based teaching.\nBy the Proposition 5, the absolute loss and sqaure loss are exponentially teachable in synthesis-based case, and we can obtain γ by plugging into the general form. We will tighten the γ up by analyzing absolute loss and square loss separately. Besides that, we also show the commonly used loss functions for classification, e.g., hinge loss and logistic loss, are also exponentially teachable in synthesis-based teaching if ‖w∗‖ can be bounded.\nProposition 9 Absolute loss is exponentially teachable in synthesis-based teaching.\nProof To show one loss function is exponentially teachable in synthesis-based case, we just need to find the appropriate γ such that the learning intensity is bounded below and above, according to Theorem 4. For the absolute loss, i.e.,\n` (〈w, x〉 , y) = |〈w, x〉 − y| , its sub-gradient is ∇w`(〈w, x〉 , y) = sign(〈w, x〉 − y)x, and thus, the learning intensity β(〈w,x〉,y) = sign (〈w, x〉 − y). For w 6= w∗, plugging x̂ = γ (w − w∗) and\nŷ = 〈w∗, γ (w − w∗)〉 into the learning intensity, we have βγ〈w,x̂〉,ŷγ = sign ( γ2 〈w − w∗, w − w∗〉 ) γ = γ.\nRecall that γ 6= 0, |γ| ≤ R‖wt−w∗‖ , ∀t ∈ N, we have\nγ ≤ min t∈N\nR\n‖wt − w∗‖ := C.\nSet γ = min{C, 1η}, we have ν = min{C, 1 η}. Therefore, we obtain the exponential decay. In fact, since the ‖w t − w∗‖ decreases in every step, we have C = R‖w0−w∗‖ . In following proof, we will follow the same argument to use this fact.\nProposition 10 Square loss is exponentially teachable in synthesis-based teaching.\nProof For square loss, i.e., ` (〈w, x〉 , y) = (〈w, x〉 − y)2 ,\nits gradient is ∇w` (〈w, x〉 , y) = 2 (〈w, x〉 − y)x, and thus, the learning intensity β〈w,x〉,y = 2 (〈w, x〉 − y). For w 6= w∗, plugging x̂ = γ (w − w∗) and ŷ = 〈w∗, γ (w − w∗)〉 into the learning intensity, we have\nβ(〈w,x̂〉,ŷ)γ = 2γ 2 ‖w − w∗‖2 . Set γ = min {\n1√ 2η‖wt−w∗‖ , R ‖wt−w∗‖\n} , we achieve the exponential teachable condition.\nProposition 11 Hinge loss is exponentially teachable in synthesis-based teaching if ‖w∗‖ ≤ 1.\nProof For hinge loss, i.e., ` (〈w, x〉 , y) = max (1− y 〈w, x〉 , 0) , as long as 1− y 〈w, x〉 > 0, its subgradient will be ∇w` (〈w, x〉 , y) = −yx. Denote x̂ = γ (w − w∗), we have β〈w,x̂〉,ŷ = −ŷ where ŷ ∈ {−1, 1}. To satisfy the exponential teachable condition, we need to select ŷ and γ such that\n1− ŷ 〈w, x̂〉 > 0 0 < −ŷγ ≤ 1η |γ| ≤ R‖w−w∗‖ ⇒  ŷγ 〈w,w − w∗〉 < 1 − 1η ≤ ŷγ < 0 |γ| ≤ R‖w−w∗‖ ⇒  〈w,w − w∗〉 > −1 − 1η ≤ ŷγ < 0 |γ| ≤ R‖w−w∗‖ .\nIf ‖w∗‖ ≤ 1, we can show 〈w,w∗〉 ≤ ‖w‖ ‖w∗‖ ≤ ‖w‖ < 1 + ‖w‖2 ,\nwhere the last inequality comes from the fact 1 + a2 − a > 0, and thus, we have 〈w,w − w∗〉 > −1. Therefore, we select any configuration of ŷ and γ satisfying\n−1 η ≤ ŷγ < 0, and |γ| ≤ R ‖w − w∗‖ .\nParticularly, we set ŷ = −1 and γ = min {\n1 η , R ‖w0−w∗‖\n} .\nProposition 12 Logistic loss is exponentially teachable in synthesis-based teaching if ‖w∗‖ ≤ 1.\nProof For the logistic loss, i.e., ` (〈w, x〉 , y) = log (1 + exp(−y 〈w, x〉)) ,\nits gradient is\n∇w` (〈w, x〉 , y) = − yx\n1 + exp(y 〈w, x〉) .\nDenote x̂ = γ (w − w∗), we have β〈w,x̂〉,ŷ = − ŷ1+exp(ŷ〈w,x̂〉) where ŷ ∈ {−1, 1}. To satisfy the exponential teachable condition, we need to select ŷ and γ such that{\n0 < − ŷγ1+exp(ŷ〈w,x̂〉) ≤ 1 η |γ| ≤ R‖w−w∗‖ .\nParticularly, we set ŷ = −1, we can fix the γ by\n0 < γ\n1 + exp(γ) <\nγ 1 + exp(ŷ 〈w, x̂〉) ≤ γ ≤ 1 η , and |γ| ≤ R ‖w − w∗‖ .\nThe γ1+exp(γ) < γ 1+exp(ŷ〈w,x̂〉) is obtained by the monotonicity of exp(·) and 〈w,w − w ∗〉 > −1 when ‖w∗‖. Therefore, we can choose γ = min {\n1 η , R ‖w0−w∗‖ } , and thus, the lower bound ν = γ1+exp(γ) .\nProof of Corollary 6 In each update, given the training sample x ∈ span (X ), we have wt+1 = wt − ηβ〈w,x〉,yx, therefore, the ∆t+1w := wt+1 −w0 ∈ span (X ). If w0 −w∗ ∈ span (X ), wt+1 −w∗ ∈ span (X ), which means by linear combination, we can construct γ̂ ∑n i=1 α t ixi = γ (w\nt − w∗). With the condition that the loss function is exponentially synthesis-based teachable, we achieve the conclusion that the combination-based omniscient teacher will converge at least exponentially with the same rate to the synthesis-based teaching.\nProof of Theorem 8 The proof is similar to the synthesis-based case. However, we introduce the consideration of the effect of pool-based teaching. Specifically, we first obtain a virtual training sample in full space, and then, we generate the sample from the candidate pool to mimic the virtual sample.\nWith the condition w0−v∗ ∈ span (D), as we discussed in the proof of Corollary 6, in every iteration, wt−v∗ ∈ span (D). Therefore, we only need to consider in the space of span (D). Meanwhile, since the teacher can rescale the sample, without loss of generality, we assume if x ∈ X , then −x ∈ X to make the rescaling is always positive.\nAt t-step, as the loss is exponentially synthesis-based teachable with γ, therefore, we have the virtually constructed sample {xv, yv} where xv = γ (wt − w∗) with γ satisfying the condition of exponentially teachable in synthesis-based settings, we first rescale the candidate pool X such that\n∀x ∈ X , γx ‖x‖ = ‖xv‖ = γ ∥∥wt − w∗∥∥ .\nWe denote the rescaled candidate pool as Xt, under the condition of rescalable pool-based teachability, there is a sample {x̂, ŷ} ∈ X × Y with scale factor γ̂ such that\nmin (x,y)∈Xt×Y\nη2‖∇wt` (〈 wt, x 〉 , y ) ‖2 − 2η 〈 wt − w∗,∇wt` (〈 wt, x 〉 , y )〉\n≤ η2β2〈wt,γ̂x̂〉,ŷ ‖x̂‖ 2 − 2ηβ〈wt,γ̂x̂〉,ŷ〈wt − w∗, γ̂x̂〉.\nWe decompose the γ̂x̂ = axv + xv⊥ with a = 〈γ̂x̂,xv〉 ‖xv‖2 . and xv⊥ = γ̂x̂− axv . Then, we have\nmin (x,y)∈Xt×Y\nη2‖∇wt` (〈 wt, x 〉 , y ) ‖2 − 2η 〈 wt − w∗,∇wt` (〈 wt, x 〉 , y )〉\n≤ η2β2〈wt,γ̂x̂〉,ŷ ‖x̂‖ 2 − 2ηβ〈wt,γ̂x̂〉,ŷ〈wt − w∗, γ̂x̂〉 = η2β2〈wt,γ̂x̂〉,ŷγ 2 ‖w − w∗‖2 − 2ηβ〈wt,γ̂x̂〉,ŷ〈wt − w∗, axv + xv⊥〉\n= η2β2〈wt,γ̂x̂〉,ŷγ 2 ‖w − w∗‖2 − 2ηβ〈wt,γ̂x̂〉,ŷγa ∥∥wt − w∗∥∥2 . Under the condition\n0 < γβ〈w,γ w−w∗x̂ 〉,ŷ < 2V(X ) η ,\nwe denote ν (γ) = minw,x̂∈X ,ŷ∈Y γβ〈w,γ w−w∗x̂ 〉,ŷ > 0 and µ (γ) = maxw,x̂∈X ,ŷ∈Y γβ〈w,γ w−w ∗ x̂ 〉,ŷ < 2V(X ) η .\nwe have the recursion ∥∥wt+1 − w∗∥∥2 2 ≤ r(η, γ) ∥∥wt − w∗∥∥2 2 ,\nwith r(η, γ,V(X )) := max { 1 + η2µ (γ) 2 − 2ηµ (γ)V(X ), 1 + η2ν (γ)2 − 2ην (γ)V(X ) } and 0 ≤ r(η, γ) < 1. Therefore, the algorithm converges exponentially∥∥wt − w∗∥∥ 2 ≤ r (η, γ)t/2 ∥∥w0 − w∗∥∥ 2 ,\nin other words, the student needs 2 (\nlog 1r(η,γ,V(X ))\n)−1 log ‖w 0−w∗‖ samples to achieve an -approximation of w ∗. For\nclearity, we define the constant term as Cη,γ,V(X )2 = 2 ( log 1r(η,γ,V(X )) )−1 ."
    }, {
      "heading" : "B. Detailed Experimental Setting",
      "text" : "General Settings We have used three linear models in the experiments. In specific, the formulation of ridge regression (RR) is\nmin w∈Rd,b∈R\n1\nn n∑ i=1 1 2 (wTxi + b− yi)2 + λ 2 ‖w‖2\nThe formulation of logistic regression (LR) is\nmin w∈Rd,b∈R\n1\nn n∑ i=1 log(1 + exp{−yi(wTxi + b)}) + λ 2 ‖w‖2\nThe formulation of support vector machine (SVM) is\nmin w∈Rd,b∈R\n1\nn n∑ i=1 max(1− yi(wTxi + b), 0) + λ 2 ‖w‖2\nComparison of different teaching strategies We use a linear regression model (ridge regression with λ = 0) for this experiment. We set R as 1 and uniformly generate 30 data points as our knowledge pool for the teacher. In this first case, we set the feature dimension as 2, while in the second case, feature dimension is 70. The learning rate is set as 0.0001 for pool-based teaching, same as BGD and SGD.\nExperiments on Gaussian data Specifically, RR is run on training data (xi, y) where each entry in xi is Gaussian distributed and y = 〈w∗,xi〉+ . LR and SVM are run on {X1,+1} and {X2,−1} where xi ∈ X1 is Gaussian distributed in each entry and +1,−1 are the labels. Specifically, we use the 10-dimension data that is Gaussian distributed with (0.5, · · · , 0.5) (label +1) and (−0.5, · · · ,−0.5) (label−1) as mean and identity matrix as covariance matrix. We generate 1000 training data points for each class. Learning rate for the same feature space is 0.0001, while learning rate for different feature spaces are 0.00001. λ is set as 0.00005.\nExperiments on uniform spherical data We first generate the training data that are uniformly distributed on a unit sphere ‖xi‖2 =1. Then we set the data points on half of the sphere ((0, π]) as label +1 and the other half ((π, 2π]) as label −1. All the generated data points are 2D. For the scenario of different features, we use a random orthogonal projection\nmatrix to generate the teacher’s feature space from student’s. Learning rate for the same feature space is 0.001, while learning rate for different feature spaces are 0.0001. λ is set as 0.00005.\nExperiments on MNIST dataset We use 24D random features (projected by a random matrix R784×24) for the MNIST dataset. The learning rate for all the compared methods are 0.001. Note that, we generate the teacher’s features using a random projection matrix (R24×24) from the original 24D student’s features. λ is set as 0.00005.\nExperiments on CIFAR-10 dataset The learning rate for all the compared methods are 0.001. λ is set as 0.00005. The goal is to learn the R32×10 fully connected layer, which is also the classifiers for 10 classes. The three network we use in the experiments are shown as follows:"
    } ],
    "references" : [ {
      "title" : "Data poisoning attacks against autoregressive models",
      "author" : [ "Alfeld", "Scott", "Zhu", "Xiaojin", "Barford", "Paul" ],
      "venue" : "In AAAI,",
      "citeRegEx" : "Alfeld et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Alfeld et al\\.",
      "year" : 2016
    }, {
      "title" : "Explicit defense actions against test-set attacks",
      "author" : [ "Alfeld", "Scott", "Zhu", "Xiaojin", "Barford", "Paul" ],
      "venue" : "In AAAI,",
      "citeRegEx" : "Alfeld et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Alfeld et al\\.",
      "year" : 2017
    }, {
      "title" : "Do deep nets really need to be deep? In Advances in neural information processing",
      "author" : [ "Ba", "Jimmy", "Caruana", "Rich" ],
      "venue" : null,
      "citeRegEx" : "Ba et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Ba et al\\.",
      "year" : 2014
    }, {
      "title" : "The true sample complexity of active learning",
      "author" : [ "Balcan", "Maria-Florina", "Hanneke", "Steve", "Vaughan", "Jennifer Wortman" ],
      "venue" : "Machine learning,",
      "citeRegEx" : "Balcan et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Balcan et al\\.",
      "year" : 2010
    }, {
      "title" : "Curriculum learning",
      "author" : [ "Bengio", "Yoshua", "Louradour", "Jérôme", "Collobert", "Ronan", "Weston", "Jason" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Bengio et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Bengio et al\\.",
      "year" : 2009
    }, {
      "title" : "Eliciting good teaching from humans for machine learners",
      "author" : [ "Cakmak", "Maya", "Thomaz", "Andrea L" ],
      "venue" : "Artificial Intelligence,",
      "citeRegEx" : "Cakmak et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Cakmak et al\\.",
      "year" : 2014
    }, {
      "title" : "Recursive teaching dimension, vcdimension and sample compression",
      "author" : [ "Doliwa", "Thorsten", "Fan", "Gaojian", "Simon", "Hans Ulrich", "Zilles", "Sandra" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Doliwa et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Doliwa et al\\.",
      "year" : 2014
    }, {
      "title" : "On the complexity of teaching",
      "author" : [ "Goldman", "Sally A", "Kearns", "Michael J" ],
      "venue" : "Journal of Computer and System Sciences,",
      "citeRegEx" : "Goldman et al\\.,? \\Q1995\\E",
      "shortCiteRegEx" : "Goldman et al\\.",
      "year" : 1995
    }, {
      "title" : "Online optimization in dynamic environments",
      "author" : [ "Hall", "Eric C", "Willett", "Rebecca M" ],
      "venue" : "arXiv preprint arXiv:1307.5944,",
      "citeRegEx" : "Hall et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Hall et al\\.",
      "year" : 2013
    }, {
      "title" : "Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding",
      "author" : [ "Han", "Song", "Mao", "Huizi", "Dally", "William J" ],
      "venue" : "arXiv preprint arXiv:1510.00149,",
      "citeRegEx" : "Han et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Han et al\\.",
      "year" : 2015
    }, {
      "title" : "Distilling the knowledge in a neural network",
      "author" : [ "Hinton", "Geoffrey", "Vinyals", "Oriol", "Dean", "Jeff" ],
      "venue" : "arXiv preprint arXiv:1503.02531,",
      "citeRegEx" : "Hinton et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Hinton et al\\.",
      "year" : 2015
    }, {
      "title" : "Becoming the expert - interactive multi-class machine teaching",
      "author" : [ "Johns", "Edward", "Mac Aodha", "Oisin", "Brostow", "Gabriel J" ],
      "venue" : "In CVPR,",
      "citeRegEx" : "Johns et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Johns et al\\.",
      "year" : 2015
    }, {
      "title" : "How do humans teach: On curriculum learning and teaching dimension",
      "author" : [ "Khan", "Faisal", "Mutlu", "Bilge", "Zhu", "Xiaojin" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Khan et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Khan et al\\.",
      "year" : 2011
    }, {
      "title" : "The teaching dimension of linear learners",
      "author" : [ "Liu", "Ji", "Zhu", "Xiaojin", "Ohannessian", "H Gorune" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Liu et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2016
    }, {
      "title" : "Analysis of a design pattern for teaching with features and labels",
      "author" : [ "Meek", "Christopher", "Simard", "Patrice", "Zhu", "Xiaojin" ],
      "venue" : "arXiv preprint arXiv:1611.05950,",
      "citeRegEx" : "Meek et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Meek et al\\.",
      "year" : 2016
    }, {
      "title" : "Using machine teaching to identify optimal training-set attacks on machine learners",
      "author" : [ "Mei", "Shike", "Zhu", "Xiaojin" ],
      "venue" : "In AAAI,",
      "citeRegEx" : "Mei et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Mei et al\\.",
      "year" : 2015
    }, {
      "title" : "Robust stochastic approximation approach to stochastic programming",
      "author" : [ "Nemirovski", "Arkadi", "Juditsky", "Anatoli", "Lan", "Guanghui", "Shapiro", "Alexander" ],
      "venue" : "SIAM Journal on optimization,",
      "citeRegEx" : "Nemirovski et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Nemirovski et al\\.",
      "year" : 2009
    }, {
      "title" : "A survey on transfer learning",
      "author" : [ "Pan", "Sinno Jialin", "Yang", "Qiang" ],
      "venue" : "IEEE Transactions on knowledge and data engineering,",
      "citeRegEx" : "Pan et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Pan et al\\.",
      "year" : 2010
    }, {
      "title" : "Fitnets: Hints for thin deep nets",
      "author" : [ "Romero", "Adriana", "Ballas", "Nicolas", "Kahou", "Samira Ebrahimi", "Chassang", "Antoine", "Gatta", "Carlo", "Bengio", "Yoshua" ],
      "venue" : "arXiv preprint arXiv:1412.6550,",
      "citeRegEx" : "Romero et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Romero et al\\.",
      "year" : 2014
    }, {
      "title" : "Teachability in computational learning",
      "author" : [ "Shinohara", "Ayumi", "Miyano", "Satoru" ],
      "venue" : "New Generation Computing,",
      "citeRegEx" : "Shinohara et al\\.,? \\Q1991\\E",
      "shortCiteRegEx" : "Shinohara et al\\.",
      "year" : 1991
    }, {
      "title" : "Near-optimally teaching the crowd to classify",
      "author" : [ "Singla", "Adish", "Bogunovic", "Ilija", "Bartok", "Gabor", "Karbasi", "Amin", "Krause", "Andreas" ],
      "venue" : "In ICML, pp",
      "citeRegEx" : "Singla et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Singla et al\\.",
      "year" : 2014
    }, {
      "title" : "Machine teaching for bayesian learners in the exponential family",
      "author" : [ "Zhu", "Xiaojin" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Zhu and Xiaojin.,? \\Q2013\\E",
      "shortCiteRegEx" : "Zhu and Xiaojin.",
      "year" : 2013
    }, {
      "title" : "Machine teaching: An inverse problem to machine learning and an approach toward optimal education",
      "author" : [ "Zhu", "Xiaojin" ],
      "venue" : "In AAAI,",
      "citeRegEx" : "Zhu and Xiaojin.,? \\Q2015\\E",
      "shortCiteRegEx" : "Zhu and Xiaojin.",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 9,
      "context" : "Recently, there is a surge of interests in machine teaching which has found diverse applications in model compression (Bucila et al., 2006; Han et al., 2015; Ba & Caruana, 2014; Romero et al., 2014), transfer learning (Pan & Yang, 2010) and cyber-security problems (Alfeld et al.",
      "startOffset" : 118,
      "endOffset" : 198
    }, {
      "referenceID" : 18,
      "context" : "Recently, there is a surge of interests in machine teaching which has found diverse applications in model compression (Bucila et al., 2006; Han et al., 2015; Ba & Caruana, 2014; Romero et al., 2014), transfer learning (Pan & Yang, 2010) and cyber-security problems (Alfeld et al.",
      "startOffset" : 118,
      "endOffset" : 198
    }, {
      "referenceID" : 0,
      "context" : ", 2014), transfer learning (Pan & Yang, 2010) and cyber-security problems (Alfeld et al., 2016; 2017; Mei & Zhu, 2015).",
      "startOffset" : 74,
      "endOffset" : 118
    }, {
      "referenceID" : 4,
      "context" : "Furthermore, machine teaching is also closely related to other subjects of interests, such as curriculum learning (Bengio et al., 2009) and knowledge distilation (Hinton et al.",
      "startOffset" : 114,
      "endOffset" : 135
    }, {
      "referenceID" : 10,
      "context" : ", 2009) and knowledge distilation (Hinton et al., 2015).",
      "startOffset" : 34,
      "endOffset" : 55
    }, {
      "referenceID" : 6,
      "context" : "There are also many seminal theory work on analyzing the teaching dimension of different models (Shinohara & Miyano, 1991; Goldman & Kearns, 1995; Doliwa et al., 2014; Liu et al., 2016).",
      "startOffset" : 96,
      "endOffset" : 185
    }, {
      "referenceID" : 13,
      "context" : "There are also many seminal theory work on analyzing the teaching dimension of different models (Shinohara & Miyano, 1991; Goldman & Kearns, 1995; Doliwa et al., 2014; Liu et al., 2016).",
      "startOffset" : 96,
      "endOffset" : 185
    }, {
      "referenceID" : 13,
      "context" : "(Liu et al., 2016) provides the teaching dimension of several linear learners.",
      "startOffset" : 0,
      "endOffset" : 18
    }, {
      "referenceID" : 14,
      "context" : "The framework has been applied to security (Mei & Zhu, 2015), human computer interaction (Meek et al., 2016) and education (Khan et al.",
      "startOffset" : 89,
      "endOffset" : 108
    }, {
      "referenceID" : 12,
      "context" : ", 2016) and education (Khan et al., 2011).",
      "startOffset" : 22,
      "endOffset" : 41
    }, {
      "referenceID" : 11,
      "context" : "(Johns et al., 2015) further extends machine teaching to interactive settings.",
      "startOffset" : 0,
      "endOffset" : 20
    }, {
      "referenceID" : 20,
      "context" : "(Singla et al., 2014) consider the crowdsourcing problem and propose a sequential teaching algorithm that can teach crowd worker to better classify the query.",
      "startOffset" : 0,
      "endOffset" : 21
    }, {
      "referenceID" : 3,
      "context" : "Therefore they have different sample complexity (Balcan et al., 2010; Zhu, 2013).",
      "startOffset" : 48,
      "endOffset" : 80
    }, {
      "referenceID" : 4,
      "context" : "Curriculum learning (Bengio et al., 2009) is a general training strategy that encourages to input training examples from easy ones to difficult ones.",
      "startOffset" : 20,
      "endOffset" : 41
    }, {
      "referenceID" : 16,
      "context" : "2) which works in a way similar to stochastic mirror descent (Nemirovski et al., 2009; Hall & Willett, 2013).",
      "startOffset" : 61,
      "endOffset" : 108
    } ],
    "year" : 2017,
    "abstractText" : "In this paper, we consider the problem of machine teaching, the inverse problem of machine learning. Different from traditional machine teaching which views the learners as batch algorithms, we study a new paradigm where the learner uses an iterative algorithm and a teacher can feed examples sequentially and intelligently based on the current performance of the learner. We show that the teaching complexity in the iterative case is very different from that in the batch case. Instead of constructing a minimal training set for learners, our iterative machine teaching focuses on achieving fast convergence in the learner model. Depending on the level of information the teacher has from the learner model, we design teaching algorithms which can provably reduce the number of teaching examples and achieve faster convergence than learning without teachers. We also validate our theoretical findings with extensive experiments on different data distribution and real image datasets.",
    "creator" : "LaTeX with hyperref package"
  }
}