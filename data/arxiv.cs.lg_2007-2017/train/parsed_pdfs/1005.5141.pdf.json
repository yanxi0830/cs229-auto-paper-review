{
  "name" : "1005.5141.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Constructing Positive Definite Elastic Kernels with Application to Time Series Classification",
    "authors" : [ "Pierre-François Marteau" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n00 5.\n51 41\nv1 [\ncs .L\nG ]\n2 7\nM ay\n2 01\n0 DRAFT PAPER IN SUBMISSION 1\nIndex Terms—Elastic distance, Time warp kernel, Time warp inner product, Definiteness, Time series classification, SVM.\n✦"
    }, {
      "heading" : "1 INTRODUCTION",
      "text" : "E LASTIC similarity measures such as Dynamic TimeWarping (DTW) or Edit Distances have proved to be quite efficient compared to non elastic similarity measures such as euclidean measures or LP norms, when addressing tasks that require the matching of times series data, in particular time series clustering and classification. A very wide scope of application ranging from physics, chemistry, finance, bio-informatics, network monitoring, etc, have demonstrated the benefits of using elastic measures. A natural follow up to the elaboration of elastic measures is to examine whether or not it is possible to construct Reproducing Time Warp Hilbert Spaces (RTWHS) from a given elastic measure, basically vector spaces characterized with inner products having time warp capabilities. Another intriguing question is to determine whether it is possible or not to define an inner product structure from which a given elastic measure is induced ? This question, apart from its theoretical implication, has a great impact when regarding the potential application fields, since, if the answer is positive, it gives a direct access to the Linear Algebra results and tools. Unfortunately it seems that common elastic measures that derive from DTW or Edit Distance are not directly induced by an inner product of any sort, even when such measures are metrics. One can conjecture that it is not possible to embed time series in an Hilbert space having a time warp capability using these classical elastic measures.\n• P.F. Marteau and Sylvie Gibet are with the VALORIA Lab., Université Européenne de Bretagne, Université de Bretagne Sud, 56000 Vannes, France. E-mail: {Pierre-Francois.Marteau, Sylvie.Gibet}(AT)univ-ubs.fr\nThis paper is aiming at exploring this issue and proposes the construction of Time Warp Kernels (TWK) that try to preserve the properties of elastic measures from which they are derived, while offering the possibility to embed time series in TWHS. The main contributions of the paper are the following\n1) we establish the indefiniteness of main time warp measures used in the literature 2) we propose some mechanisms to construct positive definite kernels from classical time warp measures 3) we define simple Time Warp Inner Product (TWIP) as an extension to the Euclidean Inner Product. 4) we experiment and compare the proposed kernels on time series classification tasks using a large variety of time series datasets.\nThe paper is organized as follows: the second section of the paper synthesized the related works; the third section introduces the notation and mathematical backgrounds that are used throughout the paper; the fourth section addresses the non definiteness of classical elastic measures that prevents the direct construction of an inner product from these measures. The fifth section develops the construction of some TWK and TWIP from classical elastic measures and discusses their potential benefits. The sixth section gather clustering and classification experimentations on a wide range of time series data and compares TWK and TWIP accuracies with classical elastic and non elastic measures. The seventh section proposes a conclusion and some further research perspectives. An annex gives the proof of our main results."
    }, {
      "heading" : "2 RELATED WORKS",
      "text" : "During the last decades, the use of kernel based methods in pattern analysis has provided numerous results and\nfruitful applications in various domains such as biology, statistics, networking, signal processing, etc. Some of these domains, such as bioinformatics, or more generally domains that rely on sequence or time series models, require the analysis and processing of variable lengths vectors, sequences or time stamped data. A lot of methods and algorithms have been developed to quantify the similarity of such objects. From the original dynamic programming [2] implementation of the symbolic edit distance [11] by Wagner and Fisher [24], the Smith and Waterman (SW) algorithm [20] has been designed to evaluate the similarity between two symbolic sequences by means of a local gap alignment. More efficient local heuristics have been since proposed to meet the massive symbolic data challenge, such as BLAST [1] or FASTA [14]. Similarly, dynamic time warping measures have been developed to evaluate similarity between numeric time series or time stamped data [23], [17], and more recently [5], [13] propose elastic metrics dedicated to such numeric data. Our capability to construct from such ”elastic distance”, kernels with elastic or time warp properties, has attracted large attention since significant benefits are expected from potential applications of kernel-based machine learning algorithms to variable length data, or more generally data for which some elastic matching has a meaning. Among the kernel machine algorithms applicable to discrimination or regression tasks, Support Vector Machines (SVM) are reported to yield state of the art performances. SVM or vast margin classifiers [21], [4], [19] are a set of supervised algorithms that learn from positive and negative examples how to solve discrimination or regression problems. They generalized linear classification algorithms by integrating two concepts: the maximal margin principle and a kernel function that defines the similarity or dissimilarity of any pairs of examples, typically such as an inner product between the vector representation of two examples. The definition of ’good’ kernels from known elastic or time warp distances applicable to data objects of variable lengths has thus been a major challenge since the 90s. The notion of ’goodness’ has rapidly been associated to the concept of definiteness. Basically SVM algorithms involve an optimization process whose solution is proved to be uniquely defined if and only if the kernel is positive definite: in that case the objective function to optimize is quadratic and the optimization problem convex. Nevertheless, if the definiteness of kernels is an issue, in practice, many situations exist where definite kernels are not applicable. This seems to be the case for the main elastic measures traditionally used to estimate the similarity of objects of variable lengths. A pragmatic approach consists in using indefinite kernels although contradictory results have been reported about the impact of definiteness or indefiniteness of kernels on the empirical performances of SVMs. The sub-optimality of the non convex optimization process is possibly one of the causes leading to these un-guaranteed performances [25], [7]. Regulation procedures have been proposed to locally approximate indefinite kernels functions by definite ones with some benefits. Among others, some approaches apply direct spectral transformations to indefinite kernels: the methods [26] consist in flipping the negative eigenvalues or shifting the eigenvalues using the minimal shift value required to make the spectrum of eigenvalues positive and reconstructing the kernel with the original eigenvectors in order to produce a positive semidefinite kernel. Yet, in general, ’convexification’ procedures are difficult to interpret geometrically and the expected effect of the original indefinite kernel may be lost. Some theoretical highlights have been provided through approaches that consist in embedding the data into a pseudo-Euclidean (pE) space and formulating the classification problem with an indefinite kernel as that of minimizing the distance between convex hulls formed from the two categories of data embedded in the pE space [8]. The geometric interpretation results in a constructive method allowing to apprehend, and in some cases predict, the classification behavior of an indefinite kernel SVM in the corresponding pE space. Our approach is founded on the work of Haussler (199) on convolution kernels [9] defined on set of discrete structures such as strings, trees or graphs. The iterative method that is develops is generative as it allows to build complex kernels from the convolution of simple local kernels. Following the work of Haussler (1999), Saigo and al [16] define, from the smith and waterman algorithm [20], a kernel to detect local alignment between strings by convolving simpler kernels. These authors show that the Smith and Waterman distance measure dedicated to determining similar regions between two nucleotide or protein sequences, that is not definite, is nevertheless connected to the logarithm of a point wise limit of a series of definite convolution kernels. In fact these previous works have very general implications, the first being that classical elastic measures can also be understood as the limit of a series of definite convolution kernels. We generalize in some way the results presented by Siago and al. on the Smith and Waterman algorithm and propose extensions to construct time warp inner products."
    }, {
      "heading" : "3 NOTATIONS AND MATHEMATICAL BACKGROUNDS",
      "text" : "In order to ensure that this paper is self-content, we give, without much details, commonly used definitions for metric or quasi metric, inner product, kernel and definiteness, sequence set, and classical elastic measures."
    }, {
      "heading" : "3.1 Premetric, pseudometric and metric",
      "text" : "Definition 3.1: A premetric on a set U is a function δ : U × U → R which satisfies the following axioms: For all (x, y) ∈ U × U ,\n1) δ(x, y) ≥ 0 (non negativity) 2) δ(x, x) = 0 (null if identical)\nDefinition 3.2: A pseudometric on a set U is a function δ : U × U → R which satisfies the following axioms: For all (x, y) ∈ U × U , 1) δ(x, y) ≥ 0 (non negativity) 2) δ(x, x) = 0 (null if identical) 3) δ(x, y) = δ(y, x) (symmetry) 4) δ(x, z) ≤ d(x, y) + d(y, z). (subadditivity/triangle\ninequality)\nDefinition 3.3: A metric, called also a distance, on a set U is a pseudometric δ : U × U → R for which the second axiom rewrites : For all (x, y) ∈ U × U , δ(x, y) = 0 if and only if x=y. (null iff identical)"
    }, {
      "heading" : "3.2 Inner Product",
      "text" : "In the following, the field of scalars denoted F is either the field of real numbers R or the field of complex numbers C. Definition 3.4: An inner product space is a vector space V over the field F together with an inner product, i.e., with a map 〈·, ·〉 : V × V → F that satisfies the following three axioms for all vectors x, y, z ∈ V and all scalars a ∈ F: 1) Conjugate symmetry:\n〈x, y〉 = 〈y, x〉. 2) Linearity in the first argument:\n〈ax, y〉 = a〈x, y〉. 〈x+ y, z〉 = 〈x, z〉+ 〈y, z〉.\n3) Positive-definiteness: 〈x, x〉 ≥ 0 with equality only for x = 0."
    }, {
      "heading" : "3.3 Kernel and definiteness",
      "text" : "Definition 3.5: A kernel on a non empty set U refers to a complex (or real) valued symmetric function ϕ(x, y) : U × U → C (or R). Definition 3.6: Let U be a non empty set. A function ϕ : U × U → C is called a positive (resp. negative) definite kernel if and only if it is Hermitian (i.e. ϕ(x, y) = ϕ(y, x) where the overline stands for the conjugate number) for all x and y in U and ∑n\ni,j=1 cic̄jϕ(xi, xj) ≥ 0 (resp. ∑n i,j=1 cic̄jϕ(xi, xj) ≤ 0), for all n in N, {x1, x2, ..., xn} ⊆ U and {c1, c2, ..., cn} ⊆ C.\nDefinition 3.7: Let U be a non empty set. A function ϕ : U × U → C is called a conditionally positive (resp. conditionally negative) definite kernel if and only if it is Hermitian (i.e. ϕ(x, y) = ϕ(y, x) for all x and y in U ) and\n∑n i,j=1 cic̄jϕ(xi, xj) ≥ 0\n(resp. ∑n i,j=1 cic̄jϕ(xi, xj) ≤ 0), for all n ≥ 2 in N, {x1, x2, ..., xn} ∈ Un and {c1, c2, ..., cn} ∈ Cn with\n∑n i=1 ci = 0.\nIn the last two above definitions, it is easy to show that it is sufficient to consider mutually different elements in U , i.e. collections of distinct elements x1, x2, ..., xn. This is what we consider for the remaining of the paper\nDefinition 3.8: A positive (resp. negative) definite kernel defined on a finite set U is also called a positive (resp. negative) semidefinite matrix. Similarly, a positive (resp. negative) conditionally definite kernel defined on a finite set is also called a positive (resp. negative) conditionally semidefinite matrix. For short, we will use PD, ND, CPD and CND for positive definite, negative definite, conditionally positive definite and conditionally negative definite to characterize either kernel or matrix through out the paper.\nIt is quite straightforward to construct PD kernels from CND kernels. For instance, if ϕ is a CND kernel on a set U and Ω ∈ U then [**ref**] ϕ′(x, y) = ϕ(x,Ω) + ϕ(y,Ω) − ϕ(x, y) − ϕ(Ω,Ω) is a PD kernel, so are e(ϕ\n′(x,y)) and e−ϕ(x,y). The converse is also true."
    }, {
      "heading" : "3.4 Sequence set",
      "text" : "Definition 3.9: Let U be the set of finite sequences (symbolic sequences or time series): U = {Ap1|p ∈ N}. Ap1 is a sequence with discrete index varying between 1 and p. We note Ω the empty sequence (with null length) and by convention A01 = Ω so that Ω is member of set U. |A| denotes the length of the sequence A. Let Up = {A ∈ U | |A| ≤ p} be the set of sequences whose length is lower or equal to p.\nDefinition 3.10: Let A be a finite sequence. Let a′i be the ith element (symbol or sample) of sequence A. We will consider that a′i ∈ S × T where S embeds the multidimensional space variables (either symbolic or numeric) and T ⊂ R embeds the time stamp variable, so that we can write a′i = (ai, tai) where ai ∈ S and tai ∈ T , with the condition that tai > taj whenever i > j (time stamps strictly increase in the sequence of samples). Aji with i ≤ j is the subsequence consisting of the ith through the jth element (inclusive) of A. So Aji = a ′ ia ′ i+1...a ′ j . Λ denotes the null element. A j i with i > j is the null time series, e.g. Ω."
    }, {
      "heading" : "3.5 General Edit/Elastic distance on a sequence set",
      "text" : "Definition 3.11: An edit operation is a pair (a′, b′) 6= (Λ,Λ) of sequence elements, written a′ → b′. Sequence B results from the application of the edit operation a → b into sequence A, written A ⇒ B via a′ → b′, if A = σa′τ and B = σb′τ for some subsequences\nσ and τ . We call a′ → b′ a match operation if a′ 6= Λ and b′ 6= Λ, a delete operation if b′ = Λ , an insert operation if a′ = Λ.\nFor any pair of sequences Ap1, B q 1 , for which we consider the extensions Ap0, B q 0 whose first element is the null symbol Λ, and for each elementary edit operation related to position 0 ≤ i ≤ p in sequence A and to position 0 ≤ j ≤ q in sequence B is associate a cost value Γa′\ni →b′ j (Ap1, B q 1), or Γa′i→Λ,j(A p 1, B q 1) or\nΓΛ,i→b′j (A p 1, B q 1) ∈ R. To simplify this writing we will simply write Γ(a′i → b′j), Γ(a′i → Λ) or Γ(Λ → b′j) although this will not be fully appropriate in general.\nDefinition 3.12: A function δ : U × U → R is called an edit distance defined on U if, for any pairs of sequences Ap1, B q 1 , the following recursive equation is satisfied\nδ(Ap1, B q 1) =\nMin\n\n\n\nδ(Ap−11 , B q 1) + Γ(a ′ p → Λ) delete δ(Ap−11 , B q−1 1 ) + Γ(a ′ p → b′q) match δ(Ap1, B q−1 1 ) + Γ(Λ → b′q) insert\n(1)\nNote that not all edit/elastic distances are metric. In particular, the dynamic time warping distance does not satisfy the triangle inequality."
    }, {
      "heading" : "3.5.1 Levenshtein distance",
      "text" : "The Levenshtein distance δlev(x, y) has been defined for string matching. For this edit distance, the delete and insert operations induce unitary costs, i.e. Γ(a′p → Λ) = Γ(Λ → b′q) = 1 while the match cost is null if a′p = b′p or 1 otherwise."
    }, {
      "heading" : "3.5.2 Dynamic time warping",
      "text" : "The DTW similarity measure δdtw [23][17] is defined according to the previous notations as:\nδdtw(A p 1, B q 1) = dLP (ap, bq)\n+ Min\n\n\n\nδdtw(A p−1 1 , B q 1) δdtw(A p−1 1 , B q−1 1 ) δdtw(A p 1, B q−1 1 )\n(2)\nwhere dLP (ap, bq) is the Lp norm in R k, and so for DTW, Γ(a′p → Λ) = Γ(a′p → b′q) = Γ(Λ → b′q) = dLP (ap, bq). One may note that the time stamp values are not used, therefore the costs of each edit operation involve vectors a and b in S instead of vectors a′ and b′ in S × T . One of the main restrictions of δdtw is that it does not comply with the triangle inequality as shown in [5].\n3.5.3 Edit Distance with real penalty\nδerp(A p 1, B q 1) = Min\n\n\n\nδerp(A p−1 1 , B q 1) + Γ(a ′ p → Λ) δerp(A p−1 1 , B q−1 1 ) + Γ(a ′ p → b′q) δerp(A p 1, B q−1 1 ) + Γ(Λ → b′q) (3)\nwith\nΓ(a′p → Λ) = dLP (ap, g) Γ(a′p → b′q) = dLP (ap, bq) Γ(Λ → b′q) = dLP (g, bq)\nwhere g is a constant in S and dLP (x, y) is the Lp norm of vector (x− y) in S. Note that the time stamp coordinate is not taken into account, therefore δerp is a distance on S but not on S×T . Thus the cost of each edit operation involves vectors a and b in Rk instead of vectors a′ and b′ in Rk+1. According to the authors of ERP [5], the constant g should be set to 0 for some intuitive geometric interpretation and in order to preserve the mean value of the transformed time series when adding gap samples."
    }, {
      "heading" : "3.5.4 Time warp edit distance",
      "text" : "Time Warp Edit Distance (TWED) [12], [13] is defined similarly to the edit distance defined for string [11][24]. The similarity between any two time series A and B of finite length, respectively p and q is defined as: δtwed(A p 1, B q 1) =\nMin\n\n\n\nδtwed(A p−1 1 , B q 1) + Γ(a ′ p → Λ) deleteA δtwed(A p−1 1 , B q−1 1 ) + Γ(a ′ p → b′q) match δtwed(A p 1, B q−1 1 ) + Γ(Λ → b′q) deleteB\n(4)\nwith\nΓ(a′p → Λ) = d(a′p, a′p−1) + λ Γ(a′p → b′q) = d(a′p, b′q) + d(a′p−1, b′q−1) Γ(Λ → b′q) = d(b′q, b′q−1) + λ\nThe time stamps are exploited to evaluate d(a′, b′). In practice, d(a′, b′) = dLP (a, b)+ ν · dLP (ta, tb) where λ is a positive constant that represent a gap penalty and ν is a non negative constant which characterizes the stiffness of the δtwed elastic measure."
    }, {
      "heading" : "4 UNDEFINITENESS OF ELASTIC DISTANCE KERNELS",
      "text" : "The Levenshtein distance kernel ϕ(x, y) = δlev(x, y) is known to be undefinite. We report below the first known counter-example produced by [6]. Let consider the subset of sequences V = {abc, bad, dab, adc, bcd} that leads to the following distance matrix\nMVlev =\n\n     0 3 2 1 2 3 0 2 2 1 2 2 0 3 3 1 2 3 0 3 2 1 3 3 0\n\n    \n(5)\nand consider coefficient vectors C and D in R5 such that C = [1, 1,−2/3,−2/3,−2/3] with ∑5i=1 ci = 0 and D = [1/3, 2/3, 1/3,−2/3,−2/3] with\n∑5 i=1 di = 0.\nClearly C · MVlev · CT = 2/3 > 0 and D · MVlev · DT = −4/3 < 0, showing that MVlev has\nno definiteness.\nThe DTW kernel ϕ(x, y) = δdtw(x, y) is also known to be neither CND nor CPD. The following example demonstrate this known result. Let consider the subset of sequences V = {01, 012, 0123, 01234}. Then the DTW empiric gram matrix evaluated on V is\nMVdtw =\n\n   0 1 2 3 1 0 0 0 2 0 0 0 3 0 0 0\n\n  \n(6)\nand consider coefficient vectors C and D in R4 such that C = [1/4,−3/8,−1/8, 1/4] with ∑4i=1 ci = 0 and D = [−1/4,−1/4, 1/4, 1/4] with\n∑4 i=1 di = 0. Clearly\nC ·MVdtw ·CT = 2/32 > 0 and D ·MVdtw ·DT = −1/2 < 0, showing that MVdtw has no definiteness.\nSimilarly, it is easy to find simple counter examples that show that neither ERP nor TWED kernels are definite. Let consider the subset of sequences V = {010, 012, 103, 301, 032, 123, 023, 003, 302, 321}. For the TWED metric, with ν = 1.0 and λ = 0.0 we get the following matrix:\nMVtwed = \n               0 2 7 9 6 7 5 5 10 9 2 0 5 9 4 5 3 3 8 9 7 5 0 6 7 4 6 2 5 10 9 9 6 0 13 10 12 8 1 4 6 4 7 13 0 5 3 5 12 9 7 5 4 10 5 0 2 6 9 6 5 3 6 12 3 2 0 4 11 8 5 3 2 8 5 6 4 0 7 10 10 8 5 1 12 9 11 7 0 5 9 9 10 4 9 6 8 10 5 0\n\n              \n(7)\nThe eigenvalue spectrum for this matrix is the following: {4.62, 0.04, −2.14, −0.98, −0.72, −0.37, −0.19, −0.17, −0.06, −0.03 }. This spectrum contains 2 strictly positive eigenvalues, showing that MVtwed has no definiteness.\nFor the ERP metric, with g = 0.0 we get the following matrix:\nMVerp =\n\n               0 2 3 3 4 5 4 2 4 5 2 0 3 5 2 3 2 2 4 5 3 3 0 4 3 2 3 1 3 4 3 5 4 0 7 6 7 5 1 2 4 2 3 7 0 3 2 2 6 5 5 3 2 6 3 0 1 3 5 4 4 2 3 7 2 1 0 2 6 5 2 2 1 5 2 3 2 0 4 5 4 4 3 1 6 5 6 4 0 1 5 5 4 2 5 4 5 5 1 0\n\n               (8)\nThe eigenvalue spectrum for this matrix is the following: {4.63, 0.02, 1.39e − 17, −2.21, −0.97, −0.56, −0.41, −0.26, −0.17, −0.08 }. This spectrum contains 3 strictly positive eigenvalues (although the third positive eigenvalue which is very small could be the result of the imprecision of the used diagonalization algorithm), showing that MVerp has no definiteness.\nThis shows that the metric properties of a distance defined on U, in particular the triangle inequality, are not sufficient conditions to establish definiteness (conditionally or not) of the associated distance kernel. One could conjecture that elastic distances cannot be definite (conditionally or not), possibly because of the presence of the max or min operators into the recursive equation. We will see in the following sections that replacing these min or max operators by a sum operator allows under some conditions to construct series of positive definite kernels whose limit is quite directly connected to the elastic distance kernels previously addressed."
    }, {
      "heading" : "5 CONSTRUCTING POSITIVE DEFINITE KERNELS FROM ELASTIC DISTANCE",
      "text" : "The simple idea leading to the construction of positive definite kernels from a given elastic distance defined on U is to replace the min or max operator into the recursive equation defining the elastic distance by a ∑\noperator. Instead of keeping one of the best alignment paths, the new kernel will sum up all the subsequence alignments with some weighting factor that could be optimized. This has been done successfully for the Smith and Waterman symbolic distance that is also known to be indefinite [16] and we propose in the following sub sections some generalizations and extensions of this result."
    }, {
      "heading" : "5.1 Summative Time Warped Kernels",
      "text" : "Definition 5.1: A function < .; . >: U×U → R is called a Summative Time Warp Kernel (STWK) if, for any pairs of sequences Ap1, B q 1 , there exists a function f : R → R such that the following recursive equation is satisfied\n< Ap1;B q 1 >=\n∑\n\n\n\n< Ap−11 , B q 1 > ⋆f(Γ(a ′ p → Λ)) delete < Ap−11 , B q−1 1 > ⋆f(Γ(a ′ p → b′q)) match < Ap1, B q−1 1 > ⋆f(Γ(Λ → b′q)) insert\n(9)\nWhere ⋆ is either the addition or the multiplication. Summative refers to the ∑\noperator replacing the min or max usually used. The recursion is initialized using < A01, B 0 1 >=< Ω,Ω >= ξ ∈ R. This type of kernel sum up for all the possible alignment paths between the two times series, the multiplication or the addition of the local quantities f(Γ(a′ → b′)).\nDefinition 5.2: If ⋆ is the addition, the STWK is called additive, otherwise it will be called multiplicative.\nThe following theorem states necessary and sufficient conditions on f(Γ(a′ → b′)) for an STWK to be definite and thus is a basis for the construction of definite STWK.\nTheorem 5.3: Definiteness of STWK:\ni) A STWK is positive definite on U if the local kernel k(a′, b′) = f(Γ(a′ → b′)) is positive definite on ((S × T ) ∪ {Λ})2 and ξ > 0.\nii) An additive STWK is negative definite on U if the local kernel k(a′, b′) = f(Γ(a′ → b′)) is negative definite on ((S × T ) ∪ {Λ})2 and ξ ≤ 0.\niii) An additive STWK is conditionally positive definite if the local kernel k(a′, b′) = f(Γ(a′ → b′)) is conditionally positive definite on ((S × T ) ∪ {Λ})2.\nA sketch of proof for theorem 5.3 is given in the appendix.\nAs in general the cost function Γ is conditionally negative definite, choosing for f(h) the exponential ensures that f(Γ(a′ → b′)) is a positive definite kernel [18]. Other functions can be used such as the Inverse Multi Quadric kernel k(a′, b′) = 1√\n(Γ(a′→b′))2+θ2 . As with\nthe exponential (Gaussian or Laplace) kernel, Multi Quadric kernel results in a positive definite matrix with full rank (Micchelli, 1986) and thus forms a infinite dimension feature space."
    }, {
      "heading" : "5.2 Some instances of additive and multiplicative STWK",
      "text" : ""
    }, {
      "heading" : "5.2.1 Additive STWK",
      "text" : "Definition 5.4:\n< Ap1, B q 1 >twip= 1 3 ·\n∑\n\n\n\n< Ap−11 , B q 1 >twip delete < Ap−11 , B q−1 1 >twip +e\n−ν.d(tap ,tbq )(ap · bq) match < Ap1, B q−1 1 >twip insert\nwhere d is a distance, and ν a stiffness parameter. We suggest to take ξ = 0 for this additive SWTK.\nProposition 5.5: Definiteness of the additive STWK < ., . >twip that has the property of an inner product:\ni) < ., . >twip is positive definite.\nii) Furthermore, < ., . >twip is an inner product on (U,⊕,⊗), that we call a Time Warp Inner Product (TWIP), where ⊕ and ⊗ are defined in definition 5.2.1 and Algorithm 5.6 respectively,\niii) The euclidean inner product < ., . >ED on a set of time series of constant lengths and uniformly\nsampled is the limit when ν → ∞ of < ., . >twip on this same set.\nDefinition 5.6: For all A ∈ U and all λ ∈ R, C = λ⊗ A ∈ U is such that for all 0 ≤ i ≤ |A|, c′i = (λ.ai, tai) and thus |C| = |A|.\nAlgorithm 1 A⊕B For all A, B in U, C = A⊕B ∈ U is given by i ← 0, j ← 0, k ← 0 WHILE i < |A| OR j < |B| DO IF i < |A| AND j < |B|\nIF tai = tbj c′k = (ai + bj, tai) i ← i+ 1, j ← j + 1, k ← k + 1 ESLE IF tai < tbj c′k = (ai, tai) i ← i+ 1, k ← k + 1 IF tai > tbj c′k = (ai, tbj) j ← j + 1, k ← k + 1\nEND IF ENDIF IF i < |A|\nc′k = (ai, tai) i ← i+ 1, k ← k + 1\nEND IF IF j < |B|\nc′k = (bj, tbj) j ← j + 1, k ← k + 1\nEND IF END WHILE\nNote that any discrete time series spaces of variable lengths and non uniformly sampled, when provided with the metric (norm) induced by a TWIP, is a Hilbert space. The proof of proposition 5.5 is straightforward and is omitted."
    }, {
      "heading" : "5.2.2 Multiplicative exponentiated STWK",
      "text" : "Definition 5.7:\n< Ap1, B q 1 >me= 1 3 ·\n∑\n\n \n \n< Ap−11 , B q 1 >me .e −ν′·Γ(a′p→Λ) delete < Ap−11 , B q−1 1 >me .e −ν′·Γ(a′p→b ′ q) match < Ap1, B q−1 1 >me .e −ν′·Γ(Λ→b′q) insert (10)\nwhere ν′ is a stiffness parameter that weights the contribution of the local elementary costs. The larger ν′ is, the more the kernel is selective around the optimal paths. At the limit, when ν′ → ∞, only the optimal paths costs are sum up by the kernel. Note that, as in general several optimal paths leading to the same global cost exist, limν′→+∞ −1/ν′ · log(< A,B >me) does\nnot coincide with the elastic distance δ that involves the same corresponding elementary costs.\nWe suggest to set ξ = 1 for this kind of multiplicative ATWK.\nProposition 5.8: Definiteness of the multiplicative exponentiated STWK < ., . >me < ., . >me is positive definite for the cost functions Γ(a′p → Λ), Γ(a′p → b′q) and Γ(Λ → b′q) involved in the computation of the δlev , δdtw, δerp and δtwed distances. The multiplicative SWTKs constructed from these distances are referred respectively to STWKlev, STWKerp, STWKdtw, STWKtwed in the remaining of the paper. The proof of proposition 5.8 is straightforward and is omitted."
    }, {
      "heading" : "6 CLASSIFICATION EXPERIMENTS",
      "text" : "We empirically evaluate the effectiveness of some STWK comparatively to Gaussian Radial Basis Function (RBF) Kernels or elastic distance substituting kernels [7] using some classification tasks on a set of times series coming from quite different application fields. The classification task we have considered consists of assigning one of the possible categories to an unknown time series for the 20 data sets available at UCR repository [10]. As time is not explicitly given for these datasets, we used the index value of the samples as the time stamps for the whole experiment.\nFor each dataset, a training subset (TRAIN) is defined as well as an independent testing subset (TEST). We use the training sets to train two kind of classifiers:\n• the first one is a first near neighbor (1-NN) classifier: first we select a training data set containing time series for which the correct category is known. To assign a category to an unknown time series selected from a testing data set (different from the train set), we select its nearest neighbor (in the sense of a distance or similarity measure) within the training data set, then, assign the associated category to its nearest neighbor. For that experiment a leave one out procedure is performed on the training dataset to optimized the meta parameters of the considered comparability measure. • the second one is a SVM classifier [4], [22] configured with a Gaussian RBF kernel whose parameters are C > 0, a trade off between regularization and constraint violation and σ that determines the width of the Gaussian function. To determine the C and σ hyper parameter values we adopt a 5- folded cross-validation method on each training subset. According to this procedure, given a predefined training set TRAIN and a test set TEST, we adapt the meta parameters based on the training set TRAIN: we first divide T into 5 stratified subsets\nTRAIN1, TRAIN2, , TRAIN5; then for each subset TRAINi we use it as a new test set, and regard (TRAIN − TRAINi) as a new training set; Based on the average error rate across the ten subsets, the optimal values of meta parameters are selected as the ones leading to the minimal average error rate."
    }, {
      "heading" : "6.1 Additive STWK",
      "text" : "We tested the additive STWK based on the Time Warp Inner Product < Ap1, B q 1 >twip (Eq.10). Precisely, we used the time warp distance induced by < Ap1, B q 1 >twip, basically δtwip(A p 1, B q 1) = (< A p 1 −Bq1 , Ap1 −Bq1 >twip) 1/2 ."
    }, {
      "heading" : "6.1.1 Meta parameters",
      "text" : "δtwip is characterized by the meta parameter ν (the stiffness parameter) that is optimized for each dataset on the train data by minimizing the classification error rate of a first near neighbor classifier. For this kernel, ν is selected in {100, 10, 1, .1, .01, ..., 1e− 5}.\nTo explore the potential benefits of TWIP against the Euclidean inner product, we tested also the Euclidean Distance δed that is the limit when ν → ∞ of δtwip.\nThe kernels exploited by the SVM classifiers are the Gaussian kernels STWKtwid(A,B) = eδtwid(A,B) 2/(2·σ2) and Ked(A,B) = e δed(A,B)\n2/(2·σ2). The meta parameters C is selected into the discrete set {2−5, 2−4, ..., 1, 2, ..., 210}, and σ2 into {2−5, 2−4, ..., 1, 2, ..., 210}. Table 1 gives for each data set and each tested kernels (Ked and STWKtwip) the corresponding optimized values of the meta parameters."
    }, {
      "heading" : "6.2 Multiplicative STWK",
      "text" : "We tested the multiplicative exponentiated STWK based on the δerp, δdtw, δtwed distance costs. We consider respectively the positive definite STWKerp, STWKdtw, STWKtwed kernels. Our experiment compares classification errors on the test data for\n• the first near neighbor classifiers based on the δerp, δdtw, δtwed distance measures (1-NN δerp, 1-NN δdtw and 1-NN δtwed), • the SVM classifiers using Gaussian distance substituting kernels based on the same distances and their corresponding STWK, e.g. SVM δerp, SVM STWKerp, SVM δdtw, SVM STWKdtw, SVM δtwed, SVM STWKtwed.\nFor δerp, δtwed, STWKerp and STWKtwed we used the L1-norm, while the L2-norm has been implemented for δdtw and STWKdtw, a classical choice for DTW [15]."
    }, {
      "heading" : "6.2.1 Meta parameters",
      "text" : "For δerp kernel, the meta parameter g is optimized for each dataset on the train data by minimizing the classification error rate of a first near neighbor classifier using a Leave One Out (LOO) procedure. For this kernel, g is selected in {−3,−2.99,−2.98, · · · , 2.98, 2.99, 3}. This optimized value is also used for comparison into the STWKme(ERP ) kernel.\nFor δtwed kernel, the meta parameters λ and ν are optimized for each dataset on the train data by minimizing the classification error rate of a first near neighbor classifier. For our experiment, the stiffness value (ν) is selected from {10−5, 10−4, 10−3, 10−2, 10−1, 1} and λ is selected from {0, .25, .5, .75, 1.0}. If different (ν, λ) values lead to the minimal error rate estimated for the training data then the pairs containing the highest ν value are selected first, then the pair with the highest λ value is finally selected. These optimized (λ, ν)values are also used for comparability purposes into the STWKtwed kernel.\nThe kernels exploited by the SVM classifiers are the Gaussian Radial Basis Function (RBF) kernels K(A,B) = eδ(A,B)\n2/(2·σ2) where δ stands for δerp, δdtw, δtwed, STWKerp(ERP , STWKdtw, STWKtwed. The meta parameters C is selected into {2−5, 2−4, ..., 1, 2, ..., 210}, and σ2 into {2−5, 2−4, ..., 1, 2, ..., 210}. The best values are obtained using a cross validation procedure. For the STWKerp, STWKdtw and STWKtwed kernels, the meta parameter 1/ν′ is selected into the discrete set S = {10−5, 10−4, ..., 1, 10, 100}. The optimization procedure is as follows:\n• for each value in S, we train a SVM STWK∗ classifier on the training dataset using the previously described 5-folded cross validation procedure to select the SVM meta parameters cost and σ and the average of the classification error is recorded. • the best σ,C and ν′ values are the one that lead to the minimal average error.\nTable 2 gives for each data set and each tested kernels (δerp, δdtw, δtwed, STWKerp, STWKdtw and STWKtwed) the corresponding optimized values of the meta parameters."
    }, {
      "heading" : "6.3 Discussion",
      "text" : ""
    }, {
      "heading" : "6.3.1 Additive STWK experiment analysis",
      "text" : "Table 3 shows the classification error rates obtained for the tested methods, e.g. the first near neighbor classifier based on the Euclidean Distance and the distance induced by the time warp inner product (1-NN ED and 1-NN δtwip), the Gaussian RBF kernel SVM based on the euclidean distance and the distance induced by the time warp inner product (SVM Ked and SVM STWKtwip).\nThis experiment shows that the time warp inner product is significantly more effective for the considered tasks comparatively to the edit distance measure, since it exhibits, on average, the lowest error rates for the testing data for both the 1-NN and SVM classifiers, as shown in Table 3 and Figures 1 and 2. The stiffness parameter in δtwip seems to play a significant role in these classification tasks, and this for a quite large majority of data sets."
    }, {
      "heading" : "6.3.2 Multiplicative STWK experiment analysis",
      "text" : "Tables 4 and 5 show the classification error rates obtained for the tested methods, e.g. the first near neighbor classifier based on the δerp, δdtw and δtwed distances (1- NN δerp, 1-NN δdtw and 1-NN δtwed), the Gaussian RBF kernel SVM based on the same distances (SVM δerp, SVM δdtw and SVM δtwed) and euclidean distance and the Gaussian RBF kernel SVM based on the STWK kernels (SVM STWKerp, SVM STWKdtw and SVM STWKtwed). In this experiment, we show that the SVM classifiers clearly outperforms the 1-NN classifiers. But the interesting results reported in tables 4 and 5 and figures 3, 4 and 5 is that SVM STWKerp and SVM STWKerp performs slightly better than SVM δerp and SVM δtwed respectively, and the SVM STWKdtw is clearly much better than the SVM δdtw. This could come from the fact that δerp and δtwed are metrics but not δdtw. SVM δdtw behaves poorly compared to the other tested classifiers probably because the SVM optimization process is not performing well. Nevertheless, the STWKdtw kernel based on δdtw seems to correct greatly its drawbacks."
    }, {
      "heading" : "7 CONCLUSION",
      "text" : "Following the works on convolution kernels [9] and local alignment kernels defined for strings processing around the Smith and Waterman algorithm [20] [16], we summative time warp kernels (STWK) applicable for string and time series processing. We give some simple sufficient conditions to build positive definite STWK. Our generalization leads us to propose additive and multiplicative STWK. For multiplicative STWK, we show that, for the exponentiated version we have experimented, the sufficient conditions are basically satisfied by very classical elastic distances defined by a recursive equation, in particular this is the case for the edit distance, the well known Dynamic Time Warping measure and for some variant such as the Edit Distance With Real penalty and the Time Warp Edit Distance, these two last being metrics as well as the symbolic edit distance. From the general additive STWK definition we have been able to propose a time warp inner product (TWIP) from which a metric (or norm) that generalizes the euclidean distance (or euclidean norm) is induced. The experiments conducted on a large variety of time series datasets show that both the multiplicative positive definite STWKs outperform\nthe indefinite elastic distances they are derived from, when considering 1-NN and SVM classification tasks. Our experiments also show that the additive STWK we constructed from the proposed instance of TWIP outperforms similarly and significantly the kernels derived from the euclidean inner product.\nThis time warp inner product opens some interesting perspectives since it leads to reconsider the notion of orthogonality in discrete time series spaces. In particular, in such spaces spaces provided with a TWIP the discrete sine and cosine waveforms are not any more orthogonal. If so, what may look like a discrete elastic Fourier transform ?"
    }, {
      "heading" : "APPENDIX A",
      "text" : "A.1 Proof of theorem 5.3\ni) Let show that if the function f(Γ(a′ → b′)) : (S×T )∪{Λ} → R is positive definite and if ξ > 0 , then an additive or multiplicative STWK is definite positive.\nTo that end, we consider the set Vr = {V ⊂ U such that Max(Ak,Al)∈V 2([Ak| + |Al|) ≤ r}, and show by induction on r that, P1: if the previous conditions are satisfied, for all r ∈ R+ ∪ {0}, all V = {A1, A2, · · · , A|V |} ∈ Vr, all (α1, α2, · · · , α|V |) ∈ R|V |, ∑ k,l αkαl (< Ak, Al >) ≥ 0.\nBase case (BC): The proposition P1 is obviously true for r = 0 since we restrict the sequence set to U0 = {Ω} since ξ > 0.\nInductive Hypothesis (IH): Let suppose that the proposition P1 is true for a value r ≥ 0 and let show that it is verified for r + 1.\nLet first note < Ak, Al >i,j=< A i k,1, A j l,1 > the restriction of the STWK up to index 0 ≤ i ≤ |Ak| in Ak and index 0 ≤ j ≤ |Al| in Al.\nFor all finite subset V = {A1, A2, · · · , A|V |} ∈ Vr+1 and all (α1, α2, · · · , α|V |) ∈ R|V | we have\n∑\nk,l\nαkαl < Ak, Al >=\n∑\nm,n/|Am|,|An|≥1\nαmαn < Am, An >\n+ ∑\np,q/|Ap|=0,|Aq|≥1\nαpαq < Ap, Aq >\n+ ∑\np,q/|Ap|=0,|Aq|≥1\nαpαq < Aq, Ap >\n+ ∑\nr,s/|Ar|=|As|=0\nαrαs < Ar , As >\nthus, by definition of the STWK\nΓa′ i →b′ j (Ap1, B q 1)\n∑\nk,l\nαkαl < Ak, Al >=\n∑\nm,n/|Am|,|An|≥1\nαmαn(< Am, An >|Am|,|An|−1 ⋆\nf(ΓΛ→a′ n,|An| (Am, An))\n+ ∑\nm,n/|Am|,|An|≥1\nαmαn(< Am, An >|Am|−1,|An|−1 ⋆\nf(Γa′ m,|Am| →a′ n,|An| (Am, An))\n+ ∑\nm,n/|Am|,|An|≥1\nαmαn(< Am, An >|Am|−1,|An| ⋆\nf(Γa′ m,|Am| →Λ(Am, An))\n+ ∑\np,q/|Ap|=0,|Aq|≥1\nαpαq(< Ω, Aq >0,|Aq|−1 ⋆\nf(ΓΛ→a′ l,|Aq | (Ap, Aq))\n+ ∑\np,q/|Ap|=0,|Aq|≥1\nαpαq(< Aq,Ω >|Aq|−1,0 ⋆\nf(Γa′ l,|Aq| →Λ(Aq, Ap))\n+ ∑\nr,s/|Ar|=|As|=0\nαrαs.ξ\nWithin each term (except the last one) of the sum constituting the right hand side of the previous equality, the ⋆ operator associates two kernels whose arguments are identical. The restricted STWK present within these terms are positive definite since they all apply on subsets that belong to some Vs with s ≤ r and which are by IH positive definite. As ⋆ is either the addition or the multiplication and as positive definite kernels are closed under summation or multiplication [3], all this terms are positive. The last term being obviously positive, we establish that ∑\nk,l\nαkαl (< Ak, Al >) ≥ 0. Thus the proposition P1 is\ntrue at r + 1. By induction, the proposition P1 is true for all r ∈ R+ ∪ {0}. Finally we have established that For all finite subset V = {A1, A2, · · · , A|V |} ∈ Vr+1 and all (α1, α2, · · · , α|V |) ∈ R|V | we have ∑\nk,l\nαkαl (< Ak, Al >) ≥ 0, e.g. the STWK < ., . >\nis positive definite\nii) and iii) are proved in a very similar way.\nA.2 Proof of proposition 5.5\nThe proofs of i) and ii) are obtained using a similar recursion as the one used to prove theorem 5.3."
    }, {
      "heading" : "ACKNOWLEDGMENTS",
      "text" : ""
    } ],
    "references" : [ {
      "title" : "A basic local alignment serach tool",
      "author" : [ "S. Altschul", "W. Gish", "W. Miller", "E. Myers", "D. Lipman" ],
      "venue" : "Journal of Molecular Biology, 215:403– 410",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 1990
    }, {
      "title" : "Dynamic Programming",
      "author" : [ "R. Bellman" ],
      "venue" : "Princeton Univ Press",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 1957
    }, {
      "title" : "Harmonic Analysis on Semigroups: Theory of Positive Definite and Related Functions, volume 100 of Graduate Texts in Mathematics",
      "author" : [ "Christian Berg", "Jens Peter Reus Christensen", "Paul Ressel" ],
      "venue" : null,
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 1984
    }, {
      "title" : "A training algorithm for optimal margin classifiers",
      "author" : [ "Bernhard E. Boser", "Isabelle Guyon", "Vladimir Vapnik" ],
      "venue" : "In COLT,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 1992
    }, {
      "title" : "On the marriage of lp-norm and edit distance",
      "author" : [ "L. Chen", "R. Ng" ],
      "venue" : "Proceedings of the 30th International Conference on Very Large Data Bases, pages 792–801",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Positive Definite Rational Kernels",
      "author" : [ "Corinna Cortes", "Patrick Haffner", "Mehryar Mohri" ],
      "venue" : "In Proceedings of COLT’03,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2003
    }, {
      "title" : "Learning with distance substitution kernels",
      "author" : [ "B. Haasdonk", "C. Bahlmann" ],
      "venue" : "DAGM-Symposium, pages 220–227",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Feature space interpretation of svms with indefinite kernels",
      "author" : [ "Bernard Haasdonk" ],
      "venue" : "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2005
    }, {
      "title" : "Convolution kernels on discrete structures",
      "author" : [ "D. Haussler" ],
      "venue" : "Technical report, University of California, Santa Cruz",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "L",
      "author" : [ "E.J. Keogh", "X. Xi" ],
      "venue" : "Wei, and C.A. Ratanamahatana. The ucr time series classification-clustering datasets",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Binary codes capable of correcting deletions",
      "author" : [ "V.I. Levenshtein" ],
      "venue" : "insertions, and reversals. Doklady Akademii Nauk SSSR, 163(4):845- 848, 1965 ",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 1966
    }, {
      "title" : "Time warp edit distance",
      "author" : [ "P.F. Marteau" ],
      "venue" : "Technical report, VALORIA, Universite de Bretagne Sud",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Time warp edit distance with stiffness adjustment for time series matching",
      "author" : [ "P.F. Marteau" ],
      "venue" : "IEEE Trans. Pattern Anal. Mach. Intell., 31(2):306–318",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Rapid and sensitive sequence comparisons with fasp and fasta",
      "author" : [ "W. Pearson" ],
      "venue" : "Methods Enzymol, 183:63–98",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 1990
    }, {
      "title" : "Making time-series classification more accurate using learned constraints",
      "author" : [ "C.A. Ratanamahatana", "E.J. Keogh" ],
      "venue" : "Proceedings of the Fourth SIAM International Conference on Data Mining (SDM’04), pages 11–22",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Protein homology detection using string alignment kernels",
      "author" : [ "H. Saigo", "J.P. Vert", "N. Ueda", "T. Akutsu" ],
      "venue" : "Bioinformatics, 20:1682– 1689",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "A dynamic programming approach to continuous speech recognition",
      "author" : [ "H. Sakoe", "S. Chiba" ],
      "venue" : "Proceedings of the 7th International Congress of Acoustic, pages 65–68",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 1971
    }, {
      "title" : "Metric spaces and positive definite functions",
      "author" : [ "I.J. Schoenberg" ],
      "venue" : "Transactions of the American Mathematical Society,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 1938
    }, {
      "title" : "Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond",
      "author" : [ "Bernhard Scholkopf", "Alexander J. Smola" ],
      "venue" : null,
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2001
    }, {
      "title" : "Identification of common molecular subsequences",
      "author" : [ "T. Smith", "Waterman M" ],
      "venue" : "Journal of Molecular Biology,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 1981
    }, {
      "title" : "Statistical Learning Theory",
      "author" : [ "Vladimir Vapnik" ],
      "venue" : "Wiley-Interscience, ISBN 0-471-03003-1,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 1989
    }, {
      "title" : "Automatic recognition of 200 words",
      "author" : [ "V.M. Velichko", "N.G. Zagoruyko" ],
      "venue" : "International Journal of Man-Machine Studies, 2:223–234",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 1970
    }, {
      "title" : "The string-to-string correction problem",
      "author" : [ "R.A. Wagner", "M.J. Fischer" ],
      "venue" : "Journal of the ACM (JACM), 21:168–173",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 1973
    }, {
      "title" : "Distances and (indefinite) kernels for sets of objects",
      "author" : [ "Adam Woznica", "Alexandros Kalousis", "Melanie Hilario" ],
      "venue" : "Data Mining, IEEE International Conference on,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2006
    }, {
      "title" : "Learning with non-metric proximity matrices",
      "author" : [ "Gang Wu", "Edward Y. Chang", "Zhihua Zhang" ],
      "venue" : "Proceedings of the 13th annual ACM international conference on Multimedia,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2005
    } ],
    "referenceMentions" : [ {
      "referenceID" : 1,
      "context" : "From the original dynamic programming [2] implementation of the symbolic edit distance [11] by Wagner and Fisher [24], the Smith and Waterman (SW) algorithm [20] has been designed to evaluate the similarity between two symbolic sequences by means of a local gap alignment.",
      "startOffset" : 38,
      "endOffset" : 41
    }, {
      "referenceID" : 10,
      "context" : "From the original dynamic programming [2] implementation of the symbolic edit distance [11] by Wagner and Fisher [24], the Smith and Waterman (SW) algorithm [20] has been designed to evaluate the similarity between two symbolic sequences by means of a local gap alignment.",
      "startOffset" : 87,
      "endOffset" : 91
    }, {
      "referenceID" : 22,
      "context" : "From the original dynamic programming [2] implementation of the symbolic edit distance [11] by Wagner and Fisher [24], the Smith and Waterman (SW) algorithm [20] has been designed to evaluate the similarity between two symbolic sequences by means of a local gap alignment.",
      "startOffset" : 113,
      "endOffset" : 117
    }, {
      "referenceID" : 19,
      "context" : "From the original dynamic programming [2] implementation of the symbolic edit distance [11] by Wagner and Fisher [24], the Smith and Waterman (SW) algorithm [20] has been designed to evaluate the similarity between two symbolic sequences by means of a local gap alignment.",
      "startOffset" : 157,
      "endOffset" : 161
    }, {
      "referenceID" : 0,
      "context" : "More efficient local heuristics have been since proposed to meet the massive symbolic data challenge, such as BLAST [1] or FASTA [14].",
      "startOffset" : 116,
      "endOffset" : 119
    }, {
      "referenceID" : 13,
      "context" : "More efficient local heuristics have been since proposed to meet the massive symbolic data challenge, such as BLAST [1] or FASTA [14].",
      "startOffset" : 129,
      "endOffset" : 133
    }, {
      "referenceID" : 21,
      "context" : "Similarly, dynamic time warping measures have been developed to evaluate similarity between numeric time series or time stamped data [23], [17], and more recently [5], [13] propose elastic metrics dedicated to such numeric data.",
      "startOffset" : 133,
      "endOffset" : 137
    }, {
      "referenceID" : 16,
      "context" : "Similarly, dynamic time warping measures have been developed to evaluate similarity between numeric time series or time stamped data [23], [17], and more recently [5], [13] propose elastic metrics dedicated to such numeric data.",
      "startOffset" : 139,
      "endOffset" : 143
    }, {
      "referenceID" : 4,
      "context" : "Similarly, dynamic time warping measures have been developed to evaluate similarity between numeric time series or time stamped data [23], [17], and more recently [5], [13] propose elastic metrics dedicated to such numeric data.",
      "startOffset" : 163,
      "endOffset" : 166
    }, {
      "referenceID" : 12,
      "context" : "Similarly, dynamic time warping measures have been developed to evaluate similarity between numeric time series or time stamped data [23], [17], and more recently [5], [13] propose elastic metrics dedicated to such numeric data.",
      "startOffset" : 168,
      "endOffset" : 172
    }, {
      "referenceID" : 20,
      "context" : "SVM or vast margin classifiers [21], [4], [19] are a set of supervised algorithms that learn from positive and negative examples how to solve discrimination or regression problems.",
      "startOffset" : 31,
      "endOffset" : 35
    }, {
      "referenceID" : 3,
      "context" : "SVM or vast margin classifiers [21], [4], [19] are a set of supervised algorithms that learn from positive and negative examples how to solve discrimination or regression problems.",
      "startOffset" : 37,
      "endOffset" : 40
    }, {
      "referenceID" : 18,
      "context" : "SVM or vast margin classifiers [21], [4], [19] are a set of supervised algorithms that learn from positive and negative examples how to solve discrimination or regression problems.",
      "startOffset" : 42,
      "endOffset" : 46
    }, {
      "referenceID" : 23,
      "context" : "The sub-optimality of the non convex optimization process is possibly one of the causes leading to these un-guaranteed performances [25], [7].",
      "startOffset" : 132,
      "endOffset" : 136
    }, {
      "referenceID" : 6,
      "context" : "The sub-optimality of the non convex optimization process is possibly one of the causes leading to these un-guaranteed performances [25], [7].",
      "startOffset" : 138,
      "endOffset" : 141
    }, {
      "referenceID" : 24,
      "context" : "Among others, some approaches apply direct spectral transformations to indefinite kernels: the methods [26] consist in flipping the negative eigenvalues or shifting the eigenvalues using the minimal shift value required to make the spectrum of eigenvalues positive and reconstructing the kernel with the original eigenvectors in order to produce a positive semidefinite kernel.",
      "startOffset" : 103,
      "endOffset" : 107
    }, {
      "referenceID" : 7,
      "context" : "Some theoretical highlights have been provided through approaches that consist in embedding the data into a pseudo-Euclidean (pE) space and formulating the classification problem with an indefinite kernel as that of minimizing the distance between convex hulls formed from the two categories of data embedded in the pE space [8].",
      "startOffset" : 325,
      "endOffset" : 328
    }, {
      "referenceID" : 8,
      "context" : "Our approach is founded on the work of Haussler (199) on convolution kernels [9] defined on set of discrete structures such as strings, trees or graphs.",
      "startOffset" : 77,
      "endOffset" : 80
    }, {
      "referenceID" : 15,
      "context" : "Following the work of Haussler (1999), Saigo and al [16] define, from the smith and waterman algorithm [20], a kernel to detect local alignment between strings by convolving simpler kernels.",
      "startOffset" : 52,
      "endOffset" : 56
    }, {
      "referenceID" : 19,
      "context" : "Following the work of Haussler (1999), Saigo and al [16] define, from the smith and waterman algorithm [20], a kernel to detect local alignment between strings by convolving simpler kernels.",
      "startOffset" : 103,
      "endOffset" : 107
    }, {
      "referenceID" : 21,
      "context" : "2 Dynamic time warping The DTW similarity measure δdtw [23][17] is defined according to the previous notations as:",
      "startOffset" : 55,
      "endOffset" : 59
    }, {
      "referenceID" : 16,
      "context" : "2 Dynamic time warping The DTW similarity measure δdtw [23][17] is defined according to the previous notations as:",
      "startOffset" : 59,
      "endOffset" : 63
    }, {
      "referenceID" : 4,
      "context" : "One of the main restrictions of δdtw is that it does not comply with the triangle inequality as shown in [5].",
      "startOffset" : 105,
      "endOffset" : 108
    }, {
      "referenceID" : 4,
      "context" : "According to the authors of ERP [5], the constant g should be set to 0 for some intuitive geometric interpretation and in order to preserve the mean value of the transformed time series when adding gap samples.",
      "startOffset" : 32,
      "endOffset" : 35
    }, {
      "referenceID" : 11,
      "context" : "4 Time warp edit distance Time Warp Edit Distance (TWED) [12], [13] is defined similarly to the edit distance defined for string [11][24].",
      "startOffset" : 57,
      "endOffset" : 61
    }, {
      "referenceID" : 12,
      "context" : "4 Time warp edit distance Time Warp Edit Distance (TWED) [12], [13] is defined similarly to the edit distance defined for string [11][24].",
      "startOffset" : 63,
      "endOffset" : 67
    }, {
      "referenceID" : 10,
      "context" : "4 Time warp edit distance Time Warp Edit Distance (TWED) [12], [13] is defined similarly to the edit distance defined for string [11][24].",
      "startOffset" : 129,
      "endOffset" : 133
    }, {
      "referenceID" : 22,
      "context" : "4 Time warp edit distance Time Warp Edit Distance (TWED) [12], [13] is defined similarly to the edit distance defined for string [11][24].",
      "startOffset" : 133,
      "endOffset" : 137
    }, {
      "referenceID" : 5,
      "context" : "We report below the first known counter-example produced by [6].",
      "startOffset" : 60,
      "endOffset" : 63
    }, {
      "referenceID" : 15,
      "context" : "This has been done successfully for the Smith and Waterman symbolic distance that is also known to be indefinite [16] and we propose in the following sub sections some generalizations and extensions of this result.",
      "startOffset" : 113,
      "endOffset" : 117
    }, {
      "referenceID" : 17,
      "context" : "As in general the cost function Γ is conditionally negative definite, choosing for f(h) the exponential ensures that f(Γ(a → b)) is a positive definite kernel [18].",
      "startOffset" : 159,
      "endOffset" : 163
    }, {
      "referenceID" : 6,
      "context" : "We empirically evaluate the effectiveness of some STWK comparatively to Gaussian Radial Basis Function (RBF) Kernels or elastic distance substituting kernels [7] using some classification tasks on a set of times series coming from quite different application fields.",
      "startOffset" : 158,
      "endOffset" : 161
    }, {
      "referenceID" : 9,
      "context" : "The classification task we have considered consists of assigning one of the possible categories to an unknown time series for the 20 data sets available at UCR repository [10].",
      "startOffset" : 171,
      "endOffset" : 175
    }, {
      "referenceID" : 3,
      "context" : "• the second one is a SVM classifier [4], [22] configured with a Gaussian RBF kernel whose parameters are C > 0, a trade off between regularization and constraint violation and σ that determines the width of the Gaussian function.",
      "startOffset" : 37,
      "endOffset" : 40
    }, {
      "referenceID" : 14,
      "context" : "For δerp, δtwed, STWKerp and STWKtwed we used the L1-norm, while the L2-norm has been implemented for δdtw and STWKdtw, a classical choice for DTW [15].",
      "startOffset" : 147,
      "endOffset" : 151
    }, {
      "referenceID" : 8,
      "context" : "Following the works on convolution kernels [9] and local alignment kernels defined for strings processing around the Smith and Waterman algorithm [20] [16], we summative time warp kernels (STWK) applicable for string and time series processing.",
      "startOffset" : 43,
      "endOffset" : 46
    }, {
      "referenceID" : 19,
      "context" : "Following the works on convolution kernels [9] and local alignment kernels defined for strings processing around the Smith and Waterman algorithm [20] [16], we summative time warp kernels (STWK) applicable for string and time series processing.",
      "startOffset" : 146,
      "endOffset" : 150
    }, {
      "referenceID" : 15,
      "context" : "Following the works on convolution kernels [9] and local alignment kernels defined for strings processing around the Smith and Waterman algorithm [20] [16], we summative time warp kernels (STWK) applicable for string and time series processing.",
      "startOffset" : 151,
      "endOffset" : 155
    }, {
      "referenceID" : 2,
      "context" : "As ⋆ is either the addition or the multiplication and as positive definite kernels are closed under summation or multiplication [3], all this terms are positive.",
      "startOffset" : 128,
      "endOffset" : 131
    } ],
    "year" : 2017,
    "abstractText" : "This paper proposes some extensions to the work on kernels dedicated to string alignment (biological sequence alignment) based on the summing up of scores obtained by local alignments with gaps. The extensions we propose allow to construct, from classical time warp distances, what we called summative time warp kernels that are positive definite if some simple sufficient conditions are satisfied. Furthermore, from the same formalism, we derive a time warp inner product that extend the usual euclidean inner product, providing the capability to handle discrete sequences or time series of variable lengths in an Hilbert space. The classification experiment we conducted, using either first near neighbor classifier or Support Vector Machine classifier leads to conclude that the positive definite elastic kernels we propose outperform the distance substituting kernels for the classical elastic distances we tested. In a similar way, the kernel based on the distance induced by the time warp inner product outperforms significantly on the considered task the kernel based on the euclidean distance.",
    "creator" : "LaTeX with hyperref package"
  }
}