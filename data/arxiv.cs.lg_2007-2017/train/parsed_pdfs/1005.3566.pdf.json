{
  "name" : "1005.3566.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Evolution with Drifting Targets",
    "authors" : [ "Varun Kanade", "Leslie G. Valiant", "Jennifer Wortman Vaughan" ],
    "emails" : [ "vkanade@fas.harvard.edu", "valiant@seas.harvard.edu", "jenn@seas.harvard.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n00 5.\n35 66\nv1 [\ncs .L\nG ]\n1 9\nM ay\n2 01\nThe above translation result can be also interpreted as one on the robustness of the notion of evolvability itself under changes of definition. As a second result in that direction we show that every evolution algorithm can be converted to a quasi-monotonic one that can evolve from any starting point without the performance ever dipping significantly below that of the starting point. This permits the somewhat unnatural feature of arbitrary performance degradations to be removed from several known robustness translations."
    }, {
      "heading" : "1 Overview",
      "text" : "The evolvability model introduced by Valiant [19] was designed to provide a quantitative theory for studying mechanisms that can evolve in populations of realistic size, in a reasonable number of generations through the Darwinian process of variation and selection. It models evolving mechanisms as functions of many arguments, where the value of a function represents the outcome of the mechanism, and the arguments the controlling factors. For example, the function might determine the expression level of a particular protein given the expression levels of related proteins. Evolution is then modeled as a restricted form of learning from examples, in which the learner observes only the empirical performance of a set of functions that are feasible variants of the current function. The performance of a function is defined as its correlation with the ideal function, which specifies for every possible circumstance the behavior that is most beneficial in the current environment for the evolving entity.\nThe evolution process consists of repeated applications of a random variation step followed by a selection step. In the variation step of round i, a polynomial number of variants of the algorithm’s current hypothesis ri are generated, and their performance empirically tested. In the selection step, one of the variants with high performance is chosen as ri+1. An algorithm therefore consists of both a procedure for describing possible variants and as well as a selection mechanism for choosing among the variants. The algorithm succeeds if it produces a hypothesis with performance close to the ideal function using only a polynomial amount of resources (in terms of number of generations and population size).\n∗This work was supported in part by NSF-CCF-04-27129. †Vaughan is supported by NSF under grant CNS-0937060 to the CRA for the CIFellows Project. Any\nopinions, findings, conclusions, or recommendations expressed in this material are those of the authors alone.\nThe basic model as defined in Valiant [19] is concerned with the evolution of Boolean functions using representations that are randomized Boolean functions. This has been shown by Feldman [10] to be a highly robust class under variations in definition, as is necessary for any computational model that aims to capture the capabilities and limitations of a natural phenomenon. This model has also been extended to allow for representations with real number values, in which case a range of models arise that differ according to whether the quadratic loss or some other metric is used in evaluating performance [17, 10]. Our interest here remains with the original Boolean model, which is invariant under changes of this metric.\nIn this paper we consider the issue of stability of an evolution algorithm to gradual changes, or drift, in the target or ideal function. Such stability is a desirable property of evolution algorithms that is not explicitly captured in the original definition. We present two main results in this paper. First, for specific evolution algorithms we quantify how resistant they are to drift. Second, we show that evolutionary algorithms can be transformed to stable ones, showing that the evolutionary model is robust also under modifications that require resistance to drift.\nThe issue of resistance to drift has been discussed informally before in the context of evolution algorithms that are monotone in the sense that their performance is increasing, or at least nondecreasing, at every stage [17, 10]. We shall therefore start by distinguishing among three notions of monotonicity in terms of properties that need to hold with high probability: (i) quasi-monotonic, where for any ǫ the performance never goes more than ǫ below that of the starting hypothesis r0, (ii) monotonic, where the performance never goes below that of r0, and (iii) strictly monotonic, where performance increases by at least an inverse polynomial amount at each step. Definition (ii) is essentially Feldman’s [10] and definition (iii) is implicit in Michael [17].\nWe define a notion of an evolution algorithm being stable to drift in the sense that for some inverse polynomial amount of drift, using only polynomial resources, the algorithm will converge to performance 1− ǫ, and will stay with such high performance in perpetuity in the sense that at every subsequent time, except with probability ǫ, its performance will be at least 1− ǫ.\nAs our main result demonstrating the robustness of the evolutionary model itself, we show, through the simulation of query learning algorithms [9], that for every distribution D, every function class that is evolvable in the original definition, is also evolvable by an algorithm that is both (i) quasi-monotonic, and (ii) stable to some inverse polynomial amount of drift. While the definitions allow any small enough inverse polynomial drift rate, they require good performance in perpetuity, and with the same representation class for all ǫ. Some technical complications arise as a result of the latter two requirements.\nAs a vehicle for studying the stability of specific algorithms, we show that there are natural evolutionary algorithms for linear separators over symmetric distributions and over the more general product normal distributions. Further we formulate a general result that states that for any strictly monotonic evolution algorithm, where the increase in performance at every step is defined by an inverse polynomial b, one can determine upper bounds on the polynomial parameters of the evolution algorithm, namely those that bound the generation numbers, population sizes, and sample sizes, and also a lower bound on the drift that can be resisted. We illustrate the usefulness of this formulation by applying it to show that our algorithms for linear separators can resist a significant amount of drift. We also apply it to existing algorithms for evolving conjunctions over the uniform distribution, with or without negations. We note that the advantages of evolution algorithms that use natural representations, over those obtained through simulations of query learning algorithms, may be quantified in terms of how moderate the degrees are of the polynomials that bound the generation number, population size, sample size and (inverse) drift rate of these algorithms. These results appear in Sections 6 and 7 and may be read independently of Section 5.\nAll omitted details and proofs appear in the appendix."
    }, {
      "heading" : "2 The Computational Model of Evolution",
      "text" : "In this section, we provide an overview of the original computational model of evolution (Valiant [19], where further details can be found). Many of these notions will be familiar to readers who are acquainted with the PAC model of learning [18]."
    }, {
      "heading" : "2.1 Basic Definitions",
      "text" : "Let X be a space of examples. A concept class C over X is a set of functions mapping elements in X to {−1, 1}. A representation class R over X consists of a set of (possibly randomized) functions from X to {−1, 1} described in a particular language. Throughout this paper, we think of C as the class of functions from which the ideal target f is selected, and R as a class of representations from which the evolutionary algorithm chooses an r to approximate f . We consider only classes of\nrepresentations that can be evaluated efficiently, that is, classes R such that for any r ∈ R and any x ∈ X , r(x) can be evaluated in time polynomial in the size of x.\nWe associate a complexity parameter n with X , C, andR. This parameter indicates the number of dimensions of each element in the domain. For example, we might define Xn to be {−1, 1}n, Cn to be the class of monotone conjunctions over n variables, and Rn to be the class of monotone conjunctions over n variables with each conjunction represented as a list of variables. Then C = {Cn}∞n=1 and R = {Rn}∞n=1 are really ensembles of classes.1 Many of our results depend on this complexity parameter n. However, we drop the subscripts when the meaning is clear from context.\nThe performance of a representation r with respect to the ideal target f is measured with respect to a distribution D over examples. This distribution represents the relative frequency with which the organism faces each set of conditions in X . Formally, for any pair of functions f : X → {−1, 1}, r : X → {−1, 1}, and distribution D over X , we define the performance of r with respect to f as\nPerff (r,D) = Ex∼D[f(x)r(x)] = 1− 2errD(f, r) , where errD(f, r) = Prx∼D(f(x) 6= r(x)) is the 0/1 error between f and r. The performance thus measures the correlation between f and r and is always between −1 and 1.\nA new mutation is selected after each round of variation based in part on the observed fitness of the variants, i.e., their empirical correlations with the target on a polynomial number of examples. Formally, the empirical performance of r with respect to f on a set of examples x1, · · · , xs chosen independently according to D is a random variable defined as (1/s)∑si=1 f(xi)r(xi).\nWe denote by ǫ an accuracy parameter specifying how close to the ideal target a representation must be to be considered good. A representation r is a good approximation of f if Perff (r,D) ≥ 1−ǫ (or equivalently, if errD(f, r) ≤ ǫ/2). We allow the evolution algorithm to use resources that are polynomial in both 1/ǫ and the dimension n."
    }, {
      "heading" : "2.2 Model of Variation and Selection",
      "text" : "An evolutionary algorithm E determines at each round i which set of mutations of the algorithm’s current hypothesis ri−1 should be evaluated as candidates for ri, and how the selection will be made. The algorithm E = (R, Neigh, µ, t, s) is specified by the following set of components:\n• The representation class R = {Rn}∞n=1 specifies the space of representations over X from which the algorithm may choose functions r to approximate the target f .\n• The (possibly randomized) function Neigh(r, ǫ) specifies for each r ∈ Rn the set of representations r′ ∈ Rn into which r can randomly mutate. This set of representations is referred to as the neighborhood of r. For all r and ǫ, it is required that r ∈ Neigh(r, ǫ) and that the size of the neighborhood is upper bounded by a polynomial.\n• The function µ(r, r′, ǫ) specifies for each r ∈ Rn and each r′ ∈ Neigh(r, ǫ) the probability that r mutates into r′. It is required that for all r and ǫ, for all r′ ∈ Neigh(r, ǫ), µ(r, r′, ǫ) ≥ 1/p(n, 1/ǫ) for a polynomial p.\n• The function t(r, ǫ), referred to as the tolerance of E , determines the difference in performance that a mutation in the neighborhood of r must exhibit in order to be considered a “beneficial”, “neutral”, or “deleterious” mutation. The tolerance is required to be bounded from above and below, for all representations r, by a pair of inverse polynomials in n and 1/ǫ.\n• Finally, the function s(r, ǫ), referred to as the sample size, determines the number of examples used to evaluate the empirical performance of each r′ ∈ Neigh(r, ǫ). The sample size must also be polynomial in n and 1/ǫ.\nThe functions Neigh, µ, t, and s must all be computable in time polynomial in n and 1/ǫ. We are now ready to describe a single round of the evolution process. For any ideal target f ∈ C, distribution D, evolutionary algorithm E = (R, Neigh, µ, t, s), accuracy parameter ǫ, and representation ri−1, the mutator M(f,D, E , ǫ, ri−1) returns a random mutation ri ∈ Neigh(ri−1, ǫ) using the following selection procedure. First, for each r ∈ Neigh(ri−1, ǫ), the mutator computes the empirical performance of r with respect to f on a sample of size s.2 Call this v(r). Let\nBene = { r ∣ ∣ r ∈ Neigh(ri−1, ǫ), v(r) ≥ v(ri−1) + t(ri−1, ǫ) }\n1As in the PAC model, n should additionally upper bound the size of representation of the function to be learned, but for brevity we shall omit this aspect here.\n2We assume a single sample is used to evaluate the performance of all neighbors and ri−1, but one could interpret the model as using independent samples for each representation. This would not change our results.\nbe the set of “beneficial” mutations and\nNeut = { r ∣ ∣ r ∈ Neigh(ri−1, ǫ), |v(r) − v(ri−1)| < t(ri−1, ǫ) }\nbe the set of “neutral” mutations. If at least one beneficial mutation exists, then a mutation r is chosen from Bene as the survivor ri with relative probability µ(ri−1, r, ǫ). If no beneficial mutation exists, then a mutation r is chosen from Neut as the survivor ri, again with probability proportional to µ(ri−1, r, ǫ). Notice that, by definition, ri−1 is always a member of Neut, and hence a neutral mutation is guaranteed to exist."
    }, {
      "heading" : "2.3 Putting It All Together",
      "text" : "A concept class C is said to be evolvable by algorithm E over distribution D if for every target f ∈ C, starting at any r0 ∈ R, the sequence of mutations defined by E converges in polynomial time to a representation r whose performance with respect to f is close to 1. This is formalized as follows.\nDefinition 1 (Evolvability [19]) For a concept class C, distribution D, and evolutionary algorithm E = (R, Neigh, µ, t, s), we say that C is evolvable over D by E if there exists a polynomial g(n, 1/ǫ) such that for every n ∈ N, f ∈ Cn, r0 ∈ Rn, and ǫ > 0, with probability at least 1 − ǫ, a sequence r0, r1, r2, · · · generated by setting ri = M(f,D, E , ǫ, ri−1) for all i satisfies Perff (rg(n,1/ǫ),D) ≥ 1− ǫ.\nWe say that the class C is evolvable over D if there exists a valid evolution algorithm E = (R, Neigh, µ, t, s) such that C is evolvable over D by E . The polynomial g(n, 1/ǫ), referred to as the generation polynomial, is an upper bound on the number of generations required for the evolution process to converge. If the above definition holds only for a particular value (or set of values) for r0, then we say that C is evolvable with initialization."
    }, {
      "heading" : "2.4 Alternative Models",
      "text" : "Various alternative formulations of the basic computational model of evolution described here have been studied. Many have been proved equivalent to the basic model in the sense that any concept class C evolvable in the basic model is evolvable in the alternative model and vice versa. Here we briefly discuss some of the variations that have been considered.\nThe performance measure Perff (r,D) is defined in terms of the 0/1 loss. Alternative performance measures based on squared loss or other loss functions have been studied in the context of evolution [10, 11, 17]. However, these alternative measures are identical to the original when f and r are (possibly randomized) binary functions, as we have assumed. (When the model is extended to allow real-valued function output, evolvability with a performance measure based on any nonlinear loss function is strictly more powerful than evolvability with the standard correlation-based performance measure [10]. We do not consider that extension in this work.)\nAlternate rules for determining how a mutation is selected have also been considered. In particular, Feldman [10] showed that evolvability using a selection rule that always chooses among the mutations with the highest or near highest empirical performance in the neighborhood is equivalent to evolvability with the original selection rule based on the classes Bene and Neut. He also discussed the performance of “smooth” selection rules, in which the probability of a given mutation surviving is a smooth function of its original frequency and the performance of mutations in the neighborhood.\nFinally, Feldman [9, 10] showed that fixed-tolerance evolvability, in which the tolerance t is a function of only n and 1/ǫ but not the representation ri−1, is equivalent to the basic model."
    }, {
      "heading" : "3 Notions of Monotonicity",
      "text" : "Feldman [10, 11] introduced the notion of monotonic evolution in the computational model described above. His notion of monotonicity, restated here in Definition 2, requires that with high probability, the performance of the current representation ri never drops below the performance of the initial representation r0 during the evolution process.\nDefinition 2 (Monotonic Evolution) An evolution algorithm E monotonically evolves a class C over a distribution D if E evolves C over D and with probability at least 1− ǫ, for all i ≤ g(n, 1/ǫ), Perff (ri,D) ≥ Perff (r0,D), where g(n, 1/ǫ) and r0, r1, · · · are defined as in Definition 1.\nWhen explicit initialization of the starting representation r0 is prohibited, this is equivalent to requiring that Perff (ri,D) ≥ Perff (ri−1,D) for all i ≤ g(n, 1/ǫ). In other words, it is equivalent to requiring that with high probability, performance never decreases during the evolution process.\n(Feldman showed that if representations may produce real-valued output and an alternate performance measure based on squared loss in considered, then any class C that is efficiently SQ learnable over a known, efficiently samplable distribution D is monotonically evolvable over D.)\nA stronger notion of monotonicity was used by Michael [17], who, in the context of real-valued representations and quadratic loss functions, developed an evolution algorithm for learning 1-decision lists in which only beneficial mutations are allowed. In this spirit, we define the notion of strict monotonic evolution, which requires a significant (inverse polynomial) performance increase at every round of evolution until a representation with sufficiently high performance is found.\nDefinition 3 (Strict Monotonic Evolution) An evolution algorithm E strictly monotonically evolves a class C over a distribution D if E evolves C over D and, for a polynomial m, with probability at least 1 − ǫ, for all i ≤ g(n, 1/ǫ), either Perff (ri−1,D) ≥ 1 − ǫ or Perff (ri,D) ≥ Perff (ri−1,D) + 1/m(n, 1/ǫ), where g(n, 1/ǫ) and r0, r1, · · · are defined as in Definition 1.\nBelow we show that a class C is strictly monotonically evolvable over a distribution D using representation class R if and only if it is possible to define a neighborhood function satisfying the property that for any r ∈ R and f ∈ C, if Perff (r,D) is not already near optimal, there exists a neighbor r′ of r such that r′ has a noticeable (again, inverse polynomial) performance improvement over r. We call such a neighborhood function strictly beneficial. The idea of strictly beneficial neighborhood functions plays an important role in developing our results in Sections 6 and 7. Feldman [11] uses a similar notion to show monotonic evolution under square loss.\nDefinition 4 (Strictly Beneficial Neighborhood Function) For a concept class C, distribution D, and representation class R, we say that a (possibly randomized) function Neigh is a strictly beneficial neighborhood function if the size of Neigh(r, ǫ) is upper bounded by a polynomial p(n, 1/ǫ), and there exists a polynomial b(n, 1/ǫ) such that for every n ∈ N, f ∈ Cn, r ∈ Rn, and ǫ > 0, if Perff (r,D) < 1 − ǫ/2, then there exists a r′ ∈ Neigh(r, ǫ) such that Perff (r ′,D) ≥ Perff (r,D) + 1/b(n, 1/ǫ). We refer to b(n, 1/ǫ) as the benefit polynomial.\nLemma 5 For any concept class C, distribution D, and representation class R, if Neigh is a strictly beneficial neighborhood function for C, D, and R, then there exist valid functions µ, t, and s such that C is strictly monotonically evolvable over D by E = (R, Neigh, µ, t, s). If a concept class C is strictly monotonically evolvable over D by E = (R, Neigh, µ, t, s), then Neigh is a strictly beneficial neighborhood function for C, D, and R.\nThe proof of the second half of the lemma is immediate; the definition of strictly monotonic evolvability requires that for any initial representation r0 ∈ R, with high probability either Perff (r0,D) ≥ 1 − ǫ/2 or Perff (r1,D) ≥ Perff (r0,D) + 1/m(n, 2/ǫ) for a polynomial m. Thus if Perff (r0,D) < 1 − ǫ/2 there must exist an r1 in the neighborhood of r0 such that Perff (r1,D) ≥ Perff (r0,D) + 1/m(n, 2/ǫ). The key idea behind the proof of the first half is to show that it is possible to set the tolerance t(r, ǫ) in such a way that with high probability, Bene is never empty and there is never a representation in Bene with performance too much worse than that of the beneficial mutation guaranteed by the definition of the strictly beneficial neighborhood function. This implies that the mutation algorithm is guaranteed to choose a new representation with a significant increase in performance at each round.\nFinally, we define quasi-monotonic evolution. This is similar to the monotonic evolution, except that the performance is allowed to go slightly below that of r0. In Section 5.7, we show that this notion can be made universal, in the sense that every evolvable class is also evolvable quasimonotonically.\nDefinition 6 (Quasi-Monotonic Evolution) An evolution algorithm quasi-monotonically evolves a class C over D if E evolves C over D and with probability at least 1− ǫ, for all i ≤ g(n, 1/ǫ), Perff (ri,D) ≥ Perff (r0,D)− ǫ, where g(n, 1/ǫ) and r0, r1, · · · are defined as in Definition 1."
    }, {
      "heading" : "4 Resistance to Drift",
      "text" : "There are many ways one could choose to formalize the notion of drift resistance. Our formalization is closely related to ideas from the work on tracking drifting concepts in the computational learning literature. The first models of concept drift were proposed around the same time by Helmbold and Long [12] and Kuh et al. [16]. In both of these models, at each time i, an input point xi is drawn from a fixed but unknown distribution D and labeled by a target function fi ∈ C. It is assumed\nthat the error of fi with respect to fi−1 on D is less than a fixed value ∆. Helmbold and Long [12] showed that a simple algorithm that chooses a concept to (approximately) minimize error over recent time steps achieves an average error of Õ( √ ∆d) where d is the VC dimension of C.3 More\ngeneral models of drift have also been proposed [2, 3]. Let fi ∈ C denote the ideal function on round i of the evolution process. Following Helmbold and Long [12], we make the assumption that for all i, errD(fi−1, fi) ≤ ∆ for some value ∆. This is equivalent to assuming that Perffi−1(fi,D) ≥ 1 − 2∆. Call a sequence of functions satisfying this condition a ∆-drifting sequence. We make no other assumptions on the sequence of ideal functions.\nDefinition 7 (Evolvability with Drifting Targets) For a concept class C, distribution D, and evolution algorithm E = (R, Neigh, µ, t, s), we say that C is evolvable with drifting targets over D by E if there exist polynomials g(n, 1/ǫ) and d(n, 1/ǫ) such that for every n ∈ N, r0 ∈ Rn, and ǫ > 0, for any ∆ ≤ 1/d(n, 1/ǫ), and every ∆-drifting sequence f1, f2, . . . (with fi ∈ Cn for all i), if r0, r1, . . . is generated by E such that ri = M(fi−1,D, E , ǫ, ri−1), then for all ℓ ≥ g(n, 1/ǫ), with probability at least 1− ǫ, Perffℓ(rℓ,D) ≥ 1− ǫ. We refer to d(n, 1/ǫ) as the drift polynomial.\nAs in the basic definition, we say that the class C is evolvable with drifting targets over D if there exists a valid evolution algorithm E = (R, Neigh, µ, t, s) such that C is evolvable with drifting targets over D by E . The drift polynomial specifies how much drift the algorithm can tolerate.\nOur first main technical result, Theorem 8, relates the idea of monotonicity described above to drift resistance by showing that given a strictly beneficial neighborhood function for a class C, distribution D, and representation class R, one can construct a mutation algorithm E such that C is evolvable with drifting targets over D by E . The tolerance t and sample size s of E and the resulting generation polynomial g and drift polynomial d directly depend only on the benefit polynomial b as described below. The proof is very similar to the proof of the first half of Lemma 5. Once again the key idea is to show that it is possible to set the tolerance such that with high probability, Bene is never empty and there is never a representation in Bene with performance too much worse than the guaranteed beneficial mutation. This implies that the mutation algorithm is guaranteed to choose a new representation with a significant increase in performance with respect to the previous target fi−1 at each round i with high probability. As long as fi−1 and fi are sufficiently close, the chosen representation is also guaranteed to have good performance with respect to fi.\nTheorem 8 For any concept class C, distribution D, and representation class R, if Neigh is a strictly beneficial neighborhood function for C, D, and R, then there exist valid functions µ, t, and s such that C is evolvable with drifting targets over D by E = (R, Neigh, µ, t, s). In particular, if Neigh is strictly beneficial with benefit polynomial b(n, 1/ǫ), and p(n, 1/ǫ) is an arbitrary polynomial upper bound on the size of Neigh(r, ǫ), then C is evolvable with drifting targets over D with • any distributions µ that satisfy µ(r, r′, ǫ) ≥ 1/p(n, 1/ǫ) for all r ∈ Rn, ǫ, and r′ ∈ Neigh(r, ǫ), • tolerance function t(r, ǫ) = 1/(2b(n, 1/ǫ)) for all r ∈ Rn, • any generation polynomial g(n, 1/ǫ) ≥ 16b(n, 1/ǫ), • any sample size s(n, 1/ǫ) ≥ 128(b(n, 1/ǫ))2 ln ( 2p(n, 1/ǫ)g(n, 1/ǫ)/ǫ ) , and\n• any drift polynomial d(n, 1/ǫ) ≥ 16b(n, 1/ǫ), which allows drift ∆ ≤ 1/(16b(n, 1/ǫ)).\nIn Sections 6 and 7, which can be read independent of Section 5, we appeal to this theorem in order to prove that some common concept classes are evolvable with drifting targets with relatively large values of ∆. Using Lemma 5, we also obtain the following corollary.\nCorollary 9 If a concept class C is strictly monotonically evolvable over D, then C is evolvable with drifting targets over D."
    }, {
      "heading" : "5 Robustness Results",
      "text" : "Feldman [9] proved that the original model of evolvability is equivalent to a restriction of the statistical query model of learning [15] known as learning by correlational statistical queries (CSQ) [5]. We extend Feldman’s analysis to show that CSQ learning is also equivalent to both evolvability with drifting targets and quasi-monotonic evolvability, and so the notion of evolvability is robust to these changes in definition. We begin by briefly reviewing the CSQ model.\n3Throughout the paper, we use the notation Õ to suppress logarithmic factors."
    }, {
      "heading" : "5.1 Learning from Correlational Statistical Queries",
      "text" : "The statistical query (SQ) model was introduced by Kearns [15] and has been widely studied due to its connections to learning with noise [1, 4]. Like the PAC model, the goal of an SQ learner is to produce a hypothesis h that approximates the behavior of a target function f with respect to a fixed but unknown distribution D. Unlike the PAC model, the learner is not given direct access to labeled examples 〈x, f(x)〉, but is instead given access to a statistical query oracle. The learner submits queries of the form (ψ, τ) to the oracle, where ψ : X ×{−1, 1} → [−1, 1] is a query function and τ ∈ [0, 1] is a tolerance parameter. The oracle responds to each query with any value v such that |Ex∼D[ψ(x, f(x))] − v| ≤ τ . An algorithm is said to efficiently learn a class C in the SQ model if for all n ∈ N, ǫ > 0, and f ∈ Cn, and every distribution Dn over Xn, the algorithm, given access to ǫ and the SQ oracle for f and Dn, outputs a polynomially computable hypothesis h in polynomial time such that err(f, h) ≤ ǫ. Furthermore it is required that each query (ψ, τ) made by the algorithm can be evaluated in polynomial time given access to f and Dn. It is known that any class efficiently learnable in the SQ model is efficiently learnable in the PAC model with label noise [15].\nA query (ψ, τ) is called a correlational statistical query (CSQ) [5] if ψ(x, f(x)) = φ(x)f(x) for some function φ : X → [−1, 1]. An algorithm A is said to efficiently learn a class C in the CSQ model if A efficiently learns C in the SQ model using only correlational statistical queries.\nIt is useful to consider one additional type of query, the CSQ> query [9]. A CSQ> query is specified by a triple (φ, θ, τ), where φ : X → [−1, 1] is a query function, θ is a threshold, and τ ∈ [0, 1] is a tolerance parameter. When presented with such a query, a CSQ> oracle for target f and distribution D returns 1 if Ex∼D[φ(x)f(x)] ≥ θ+ τ , 0 if Ex∼D[φ(x)f(x)] ≤ θ− τ , and arbitrary value of either 1 or 0 otherwise. Feldman [9] showed that if there exists an algorithm for learning C over D that makes CSQs, then there exists an algorithm for learning C over D using CSQ>s of the form (φ, θ, τ) where θ ≥ τ for all queries. Furthermore the number of queries made by this algorithm is at most O(log(1/τ)) times the number of queries made by the original CSQ algorithm."
    }, {
      "heading" : "5.2 Overview of the Reduction",
      "text" : "The construction we present uses Feldman’s simulation [9] repeatedly. Fix a concept class C and a distribution D such that C is learnable over D in the CSQ model. As mentioned above, this implies that there exists a CSQ> algorithm A for learning C over D. Let H be the class of hypotheses from which the output of A is chosen. In the analysis that follows, we restrict our attention to the case in which A is deterministic. However, the extension of our analysis to randomized algorithms is straightforward using Feldman’s ideas (see Lemma 4.7 in his paper [9]).\nFirst, we present a high level outline of our reduction. Throughout this section we will use randomized Boolean functions. If ψ : X → [−1, 1] is a real valued function, let Ψ denote the randomized Boolean function such that for every x, E[Ψ(x)] = ψ(x). It can be easily verified that for any function φ(x), Ex,Ψ[φ(x)Ψ(x)] = Ex[φ(x)ψ(x)]. For the rest of this section, we will abuse notation and simply write real-valued functions in place of the corresponding randomized Boolean functions.\nOur representation is of the form r = (1−ǫ/2)h+(ǫ/2)Φ. Here h is a hypothesis from H and Φ is function that encodes the state of the CSQ> algorithm that is being simulated. Feldman’s simulation only uses the second part. Our simulation runs in perpetuity, restarting Feldman’s simulation each time it has completed. Since the target functions are drifting over time, if h has high performance with respect to the current target function, it will retain the performance for some time steps in the future, but not forever. During this time, Feldman’s simulation on the Φ part produces a new hypothesis h′ which has high performance at the time this simulation is completed. At this time, we will transition to a representation r′ = (1− ǫ/2)h′ + (ǫ/2)Φ, where Φ is reset to start Feldman’s simulation anew. Thus, although the target drifts, our simulation will continuously run Feldman’s simulation to find a hypothesis that has a high performance with respect to the current target.\nThe rest of section 5 details the reduction. First, we show how a single run of A is simulated, which is essentially Feldman’s reduction with minor modifications. Then we discuss how to restart this simulation once it has completed. This requires the addition of certain intermediate states to keep the reduction feasible in the evolution model. We also show that our reduction can be made quasi-monotonic. Finally, we show how all this can be done using a representation class that is independent of ǫ, as is required. This last step is shown in the appendix."
    }, {
      "heading" : "5.3 Construction of the Evolutionary Algorithm",
      "text" : "We describe the construction of our evolutionary algorithm E . Let τ = τ(n, 1/ǫ) be a polynomial lower bound on the tolerance of the queries made by A when run with accuracy parameter ǫ/4. Without loss of generality, we may assume all queries are made with this tolerance. Let q = q(n, 1/ǫ)\nbe a polynomial upper bound on the number of queries made by A, and assume that Amakes exactly q queries (if not, redundant queries can be added). Here, we allow our representation class to be dependent on ǫ. However, this restriction may be removed (cf. Appendix A.7.1) In the remainder of this section we drop the subscripts n and ǫ, except where there is a possibility of confusion.\nFollowing Feldman’s notation, let z denote a bit string of length q which records the oracle responses to the queries made by A; that is, the ith bit of z is 1 if and only if the answer to the ith query is 1. Let |z| denote the length of z, zi the prefix of z of length i, and zi the ith bit of z. Since A is deterministic, the ith query made by A depends only on responses to the previous i−1 queries. We denote this query by (φzi−1 , θzi−1 , τ), with θzi−1 ≥ τ , as discussed in Section 5.1. Let hz denote the final hypothesis output by A given query responses z. Since we have chosen to simulate A with accuracy parameter ǫ/4, hz is guaranteed to satisfy Perff (hz ,D) ≥ 1 − ǫ/4 for any function f for which the query responses in z are valid. Finally, let σ denote the empty string.\nFor every i ∈ {1, · · · , q} and z ∈ {0, 1}i, we define Φz = (1/q) ∑i\nj=1 I(zj = 1)φzj−1 (x), where I is an indicator function that is 1 if its input is true and 0 otherwise. For any h ∈ H, define rǫ[h, z] = (1− ǫ/2)h(x) + (ǫ/2)Φz(x). Recall that each of these real-valued functions can be treated as a randomized Boolean function as required by the evolution model. The performance of this function, which we use as our basic representation, is mainly determined by the performance of h, but by setting the tolerance parameter low enough, the Φz part can learn useful information about the (drifting) targets by simulating A.\nLet R̃ǫ = {rǫ[h, z] | h ∈ H, 0 ≤ |z| ≤ q − 1}. The representations in R̃ǫ will be used for simulating one round of A. To reach a state where we can restart the simulation, we will need to add intermediate representations. These are defined below.\nLet tu(n, 1/ǫ) be an upper bound on ǫθzi/(8q) for all i and z i. (This will be a polynomial upper bound on all tolerances t that we define below.) Assume for simplicity that K = 2/tu(n, 1/ǫ) is an integer. Let w0 = rǫ[h, z], for some h ∈ H and |z| = q (w0 depends on h and z, but to keep notation simple we will avoid subscripts). For k = 1, . . . ,K, define wk = (1− k(tu(n, 1/ǫ)/2))w0. Notice that wK = 0, where 0 is a function that can be realized by a randomized function that ignores its input and predicts +1 or −1 randomly. Let Wǫ = {wi | w0 = rǫ[h, z], h ∈ H, |z| = q, i ∈ {0, . . . ,K}}. Finally define Rǫ = R̃ǫ ∪Wǫ. For every representation rǫ[h, z] ∈ R̃ǫ, we set • Neigh(rǫ[h, z], ǫ) = {rǫ[h, z], rǫ[h, z0], rǫ[h, z1]}, • µ(rǫ[h, z], rǫ[h, z], ǫ) = η and µ(rǫ[h, z], rǫ[h, z0], ǫ)=µ(r, rǫ[h, z1], ǫ)=(1− η)/2, • t(rǫ[h, z], ǫ) = ǫθzi/(8q). For the remaining representations wk ∈ Wǫ, with w0 = rǫ[h, z], we set • Neigh(wK , ǫ) = {wK , rǫ[0, σ]} and Neigh(wk, ǫ) = {wk, wk+1, rǫ[hz,ǫ, σ]} for all k < K, • µ(wK , wK) = η and µ(wK , rǫ[0, σ]) = 1− η, and µ(wk, wk, ǫ) = η2, µ(wk, wk+1, ǫ) = η− η2, and µ(wk, rǫ[hz,ǫ, σ]) = 1− η for all k < K,\n• t(wk, ǫ) = tu(n, 1/ǫ). Finally, let η = ǫ/(4q+2K), τ ′ = min{(ǫτ)/(2q), tu(n, 1/ǫ)/8}, and s = 1/(2(τ ′)2) log((6q+3K)/ǫ). Let E = (Rǫ, Neigh, µ, t, s) with components defined as above. We show that E evolves C over D tolerating drift of ∆ = (ǫτ)/(4q+2K +2). This value of drift, while small, is an inverse polynomial in n and 1/ǫ as required. The point to note is that the evolutionary algorithm runs perpetually, while still maintaining high performance on any given round with high probability.\nFor any representation r, we denote by LPE the union of the low probability events that some estimates of performance are not within τ ′ of their true value, or that a mutation with relative probability less than 2η (either in Bene or Neut) is selected over other mutations.\n5.4 Simulating the CSQ> Algorithm for Drifting Targets\nWe now show that it is possible to simulate a CSQ> algorithm using an evolution algorithm E even when the target is drifting. However, if we simulate a query (φ, θ, τ) on round i, there is no guarantee that the answer to this query will remain valid in future rounds. The following lemma shows that by lowering the tolerance of the simulated query below the tolerance that is actually required by the CSQ> algorithm, we are able to generate a sequence of query answers that remain valid over many rounds. Specifically, it shows that if v is a valid response for the query (φ, θ, τ/2) with respect to fi, then v is also a valid response for the query (φ, θ, τ) with respect to fj for any j ∈ [i − τ/(2∆), i+ τ/(2∆)]. Lemma 10 Let f1, f2, · · · be a ∆-drifting sequence with respect to the distribution D over X . For any tolerance τ , any threshold θ, any indices i and j such that |i − j| ≤ τ/(2∆), and any function φ : X → [−1, 1], if Ex∼D[φ(x)fj(x)] ≥ θ + τ , then Ex∼D[φ(x)fi(x)] ≥ θ + τ/2. Similarly, if Ex∼D[φ(x)fj(x)] ≤ θ − τ , then Ex∼D[φ(x)fi(x)] ≤ θ − τ/2.\nWe say that a string z is consistent with a target function f , if for all 1 ≤ i ≤ |z|, zi is a valid response to the query (φzi−1 , θzi−1 , τ), with respect to f . Suppose that the algorithm E starts with representation r0 = rǫ[h, σ]. (Recall that σ denotes the empty string.) The following lemma shows that after q time steps, with high probability it will reach a representation rǫ[h, z] where |z| = q and z is consistent with the target function fq, implying that z is a proper simulation of A on fq.\nLemma 11 If ∆ ≤ τ/(2q), then for any ∆-drifting sequence f0, f1, . . . , fq, if r0, r1, . . . , rq is the sequence of representations of E starting at r0 = rǫ[h, σ], and if the LPE does not occur for q rounds, then rq = rǫ[h, z] where |z| = q and z is consistent with fq.\nThe proof uses the following ideas: If the LPE does not occur, there are no mutations of the form r → r, so the length of z increases by 1 every round, and also all estimates of performance are within τ ′ of their true value. When this is the case, and after observing that rǫ[h, zi0] is always neutral, it is possible to show that for any round i, (i) if rǫ[h, z\ni1] is beneficial, then 1 is a valid answer to the ith query with respect to fi, (ii) if rǫ[h, z\ni1] is deleterious then 0 is a valid answer for the ith query with respect to fi, and (c) if rǫ[h, z\ni1] is neutral, then both 0 and 1 are valid answers to the ith query. This implies that zi+1 is always a valid answer to the ith query with respect to fi, and by Lemma 10, with respect to fq."
    }, {
      "heading" : "5.5 Restarting the Simulation",
      "text" : "We now discuss how to restart Feldman’s simulation once it completes. Suppose we are in a representation of the form rǫ[h, z], where |z| = q, and z is consistent with the current target function f . Then if hz is the hypothesis output by A using query responses in z, we are guaranteed that (with high probability) Perff (hz ,D) ≥ 1−ǫ/4. At this point, we would like the algorithm to choose a new representation rǫ[hz, σ], where σ is the empty string. The intuition behind this move is as follows. The performance of rǫ[hz, σ] is guaranteed to be high (and to remain high for many generations) because much of the weight is on the hz term. Thus we can use the second term (Φσ) to restart the learning process. After q more time steps have passed, it may be the case that the performance of hz is no longer as high with respect to the new target, but the simulated algorithm will have already found a different hypothesis that does have high performance with respect to this new target.\nThere is one tricky aspect of this approach. In some circumstances, we may need to restart the simulation by moving from rǫ[h, z] to rǫ[hz, σ] even though z is not consistent with f . This situation can arise for two reasons. First, we might be near the beginning of the evolution process when E has not had enough generations to correctly determine the query responses (starting state may be rǫ[h, z0] where z0 has wrong answers). Second, there is some small probability of failure on any given round and we would like the evolutionary algorithm to recover from such failures smoothly. In either case, to handle the situation in which hz may have performance below zero (or very close), we will also allow rǫ[h, z] to mutate to rǫ[0, σ].\nThe required changes from rǫ[h, z] to either rǫ[hz, σ] or rǫ[0, σ] described above may be deleterious. To handle this, we employ a technique of Feldman [9], where we first decrease the performance gradually (through neutral mutations) until these mutations are no longer deleterious. The representations defined in Wǫ achieve this. The claim is that starting from any representation of the form wk, we reach either rǫ[hz, σ] or rǫ[0, σ] in at most K − k + 1 steps, with high probability. Furthermore, since the probability of moving to rǫ[hz, σ] is very high, this representation will be reached if it is ever a neutral mutation (i.e., the LPE does not happen). Thus, the performance always stays above the performance of rǫ[hz, σ]. Lemma 12 formalizes this claim.\nLemma 12 If ∆ ≤ tu(n, 1/ǫ)/4, then for any ∆-drifting sequence f0, f1, . . . , fq, if r0, r1, . . . , rq is the sequence of mutations of E starting at r0 = wk, then if the LPE does not happen at any time-step, there exists a j ≤ K−k+1 such that rj = rǫ[hz,ǫ, σ] or rj = rǫ[0, σ]. Furthermore, for all 1 ≤ i < j, Perffi(ri,D) ≥ Perffi(rǫ[hz,ǫ, σ],D)."
    }, {
      "heading" : "5.6 Equivalence to Evolvability with Drifting Targets",
      "text" : "Combining these results, we prove the equivalence between evolvability and evolvability with drifting targets starting from any representation in Rǫ. The proof we give here uses the representation class Rǫ and therefore assumes that the value of ǫ is known. For the needed generalization to the case where R = ∪ǫRǫ, Feldman’s backsliding trick [9] can be used to first reach a representation with zero performance, and then move to a representation in Rǫ. Theorem 13 shows that every concept class that is learnable using CSQs (and thus every class that is evolvable) is evolvable with drifting targets.\nTheorem 13 If C is evolvable over distribution D, then C is evolvable with drifting targets over D. Proof: Let A be a CSQ> algorithm for learning C over D with accuracy ǫ/4. A makes q = q(n, 1/ǫ) queries of tolerance τ and outputs h satisfying Perff (h,D) ≥ 1 − ǫ/4. Let E be the evolutionary algorithm derived fromA as described in Section 5.3. Recall thatK = 2/tu(n, 1/ǫ), let g = 2q+K+1. We show that starting from an arbitrary representation r0 ∈ Rǫ, with probability at least 1 − ǫ, Perffg (rg,D) ≥ 1 − ǫ. This is sufficient to show that for all ℓ ≥ g, with probability at least 1 − ǫ, Perffℓ(rℓ,D) ≥ 1− ǫ, since we can consider the run of E starting from rℓ−g.\nWith the setting of parameters as described in Section 5.3, with probability at least 1 − ǫ, the LPE does not occur for g time steps, i.e., all estimates are within τ ′ = min{(τǫ)/(2q), tu(n, 1/ǫ)/8} of their true value and unlikely mutations (those with relative probabilities less that 2η) are not chosen. Thus, we can apply the results of Lemmas 11 and 12. We assume that this is the case for the rest of the proof. When ∆ = (ǫτ)/(4q+2K+2), the assumption of Lemmas 11 and 12 hold and we can apply them.\nFirst, we argue that starting from an arbitrary representation, in at most q +K steps, we will have reached a representation of the form rǫ[h, σ], for some h ∈ H. If the start representation is rǫ[h, z] for |z| ≤ q − 1, then in at most q − 1 steps we reach a representation of the form rǫ[h, z′] with |z′| = q, in which case by Lemma 12, the algorithm will transition to representation rǫ[h, σ] in at most K + 1 additional steps. Alternately, if the start representation is wk for k ∈ {0, . . . ,K} as defined in Section 5.3, then by Lemma 12, we reach a representation of the form rǫ[h, σ] in at most K + 1 steps.\nLet m be the time step when E first reaches the representation of the form rǫ[h, σ]. Then using Lemma 11, rm+q = rǫ[h, z\n∗], where z∗ is consistent with fm+q. Let h∗ = hz∗,ǫ be the hypothesis output by the simulated run of A. Then Perffm+q (h∗,D) ≥ 1− ǫ/4, and hence Perffm+q (rǫ[h∗, σ],D) ≥ 1− 3ǫ/4. For the value of ∆ we are using, for all i ≤ g, Perffi(rǫ[h∗, σ],D) ≥ 1− ǫ.\nFrom such a representation, when all estimates of performance are within τ ′ of their true value and unlikely mutations (those with relative probability ≤ 2η) do not occur, the performance will remain above 1 − ǫ. By Lemma 12, the algorithm will move from rm+q = rǫ[h, z∗] to rǫ[h∗, σ] in at most K + 1 steps, and during these time steps for any time step i it holds that Perffi(ri,D) ≥ Perffi(rǫ[h\n∗, σ],D). Once rǫ[h∗, σ] is reached, for q steps the representations will be of the form rǫ[h\n∗, z]. For any such time step i, Perffi(ri,D) ≥ Perffi(rǫ[h∗, σ],D). This is because if the answers in z are correct (and they will be since the LPE does not happen at any time step), the term Φz is made up of only those functions φzj−1 for which z\nj = 1, which are those for which φzj−1 has a correlation greater than θzj−1 − τ ≥ 0 with the target fi (using Lemma 10). Since as observed above the performance of rǫ[h\n∗, σ] does not degrade below 1− ǫ in the time horizon we are interested in Perffi(ri,D) ≥ Perffi [rǫ[h∗, σ]) ≥ 1− ǫ."
    }, {
      "heading" : "5.7 Equivalence to Quasi-Monotonic Evolution",
      "text" : "Finally, we show that all evolvable classes are also evolvable quasi-monotonically. In the proof of Theorem 13, we showed that for all ℓ ≥ g = 2q+K+1, with high probability Perffℓ(rℓ,D) ≥ 1−ǫ, so quasi-monotonicity is satisfied trivially. Thus we only need to show quasi-monotonicity for the first g steps. We will use the same construction as defined in Section 5.3, with modifications. However, this assumes that the representation knows ǫ, since now the trick of having the performance slide back to zero would violate quasi-monotonicity. To make the representation class independent of ǫ a more complex construction is needed. Details can be found in the appendix.\nTheorem 14 If C is evolvable over distribution D, then C is quasi-monotonically evolvable over D with drifting targets."
    }, {
      "heading" : "6 Evolving Hyperplanes with Drifting Targets",
      "text" : "In this section, we present two alternative algorithms for evolving n-dimensional hyperplanes with drifting targets. The first algorithm, which generates the neighbors of a hyperplane by rotating it a small amount in one of 2(n − 1) directions, tolerates drift on the order of ǫ/n, but only over spherically symmetric distributions. The second algorithm, which generates the neighbors of a hyperplane by shifting single components of its normal vector, tolerates a smaller drift, but works when the distribution is an unknown product normal distribution. To our knowledge, these are the first positive results on evolving hyperplanes in the computational model of evolution.\nFormally, let Cn be the class of all n-dimensional homogeneous linear separators.4 For notational convenience, we reference each linear separator in Cn by the hyperplane’s n-dimensional unit length\n4A homogeneous linear separator is one that passes through the origin. [6]\nnormal vector f ∈ Rn. For every f ∈ Cn and x ∈ Rn, we then have that f(x) = 1 if f · x ≥ 0, and f(x) = −1 otherwise. The evolution algorithms we consider in this section use a representation class Rn also consisting of n-dimensional unit vectors, where r ∈ Rn is the normal vector of the hyperplane it represents.5 Then R = {r | ‖r‖2 = 1}. We describe the two algorithms in turn."
    }, {
      "heading" : "6.1 An Evolution Algorithm Based on Rotations",
      "text" : "For the rotation-based algorithm, we define the neighborhood function of r ∈ Rn as follows. Let {u1 = r,u2, · · · ,un} be an orthonormal basis for Rn. This orthonormal basis can be chosen arbitrarily (and potentially randomly) as long as u1 = r. Then\nNeigh(r, ǫ) = r ∪ { r′ ∣ ∣ r′ = cos ( ǫ/(π √ n) ) r± sin ( ǫ/(π √ n) ) ui , i ∈ {2, · · · , n} } . (1)\nIn other words, each r′ ∈ Neigh(r, ǫ) is obtained by rotating r by an angle of ǫ/(π√n) in some direction. The size of this neighbor set is clearly 2n− 1. We obtain the following theorem.\nTheorem 15 Let C be the class of homogeneous linear separators, R be the class of homogeneous linear separators represented by unit length normal vectors, and D be an arbitrary spherically symmetric distribution. Define Neigh as in Equation 1 and let p be any polynomial satisfying p(n, 1/ǫ) ≥ 2n−1. Then C is evolvable with drifting targets over D by algorithm A = (R, Neigh, µ, t, s) with • any distributions µ that satisfy µ(r, r′, ǫ) ≥ 1/p(n, 1/ǫ) for all r ∈ Rn, ǫ, and r′ ∈ Neigh(r, ǫ), • tolerance function t(r, ǫ) = ǫ/(π3n) for all r ∈ Rn, • any generation polynomial g(n, 1/ǫ) ≥ 8π3n/ǫ, • a sample size s(n, 1/ǫ) = Õ(n2/ǫ2), and • any drift polynomial d(n, 1/ǫ) ≥ 8π3n/ǫ, which allows drift ∆ ≤ ǫ/(8π3n).\nTo prove this, we need only to show that Neigh is a strictly beneficial neighborhood function for C, D, and R with b(n, 1/ǫ) = π3n/(2ǫ). The theorem then follows from Theorem 8. The analysis relies on the fact that under any spherically symmetric distribution D (for example, the uniform distribution over a sphere), errD(u,v) = arccos(u ·v)/π, where arccos(u ·v) is the angle between u and v [6]. This allows us to reason about the performance of one function with respect to another by analyzing the dot product between their normal vectors."
    }, {
      "heading" : "6.2 A Component-Wise Evolution Algorithm",
      "text" : "We now describe the alternate algorithm for evolving homogeneous linear separators. The guarantees we achieve are inferior to those described in the previous section. However, this algorithm applies when D is any unknown product normal distribution (with polynomial variance) over Rn.\nLet ri and fi denote the ith components of r and f respectively (not the values of the representation and ideal function at round i as in previous sections). The alternate algorithm is based on the following observations. First, whenever there exists some i for which ri and fi have different signs and aren’t too close to 0, we can obtain a new representation with a non-trivial increase in performance by flipping the sign of ri. Second, if there are no beneficial sign flips, if there is some i for which ri is not too close to fi, we can obtain a new representation with a significant increase in performance by adjusting ri a little and renormalizing. The amount we must adjust ri depends on the standard deviation of D in the ith dimension, so we must try many values when D is unknown. Finally, if the above conditions do not hold, then the performance of r is already good enough.\nDenote by {ei}ni=1 the basis of Rn. Let σ1, . . . , σn be the standard deviation of the distribution D in the n dimensions. We assume that 1 ≥ σi ≥ (1/n)k for some constant k for all i, and that the algorithm is given access to the value of k, but not the particular values σi. We define the neighborhood function as Neigh(r, ǫ) = Nfl ∪Nsl, where Nfl = {r− 2riei | i = 1, . . . , d} is the set of representations obtained by flipping the sign of one component of r, and\nNsl =\n\n\n\nr ± jǫ2 12nk √ n ei\n‖r ± jǫ2 12nk √ n ei‖2\n∣\n∣\n∣\n∣\n∣\n∣\ni ∈ {1, · · · , d}, j ∈ {1, · · · , 4nk}\n\n\n\nis the set obtained by shifting each component by various amounts. We obtain the following.\n5Technically we must assume that the representations r ∈ Rn and input points x ∈ R n are expressed to a fixed finite precision so that r ·x is guaranteed to be computable in polynomial time, but for simplicity, in the analysis that follows, we treat both as simply vectors of real numbers.\nTheorem 16 Let C be the class of homogeneous linear separators, and R be the class of homogeneous linear separators represented by unit length normal vectors, and D be a product normal distribution with (unknown) standard deviations σ1, · · · , σn such that 1 ≥ σi ≥ (1/n)k for all i for a constant k. Define Neigh as above and let p be any polynomial such that p(n, 1/ǫ) ≥ 8n2k+1 + 2n. Then C is evolvable with drifting targets over D by algorithm A = (R, Neigh, µ, t, s) with • any distribution µ satisfying µ(r, r′, ǫ) ≥ 1/p(n, 1/ǫ) for all r ∈ Rn and r′ ∈ Neigh(r, ǫ), • tolerance function t(r, ǫ) = ǫ6/(288n), • any generation polynomial g(n, 1/ǫ) ≥ 2304n/ǫ6, • a sample size s(n, 1/ǫ) = Õ(n2/ǫ12), and • any drift polynomial d(n, 1/ǫ) ≥ 2304n/ǫ6, which allows drift ∆ ≤ ǫ6/(2304n).\nThe proof formalizes the set of observations described above, using them to show that Neigh is a strictly beneficial neighborhood function for C, D, and R with b(n, 1/ǫ) = 144n/ǫ6. The theorem is then an immediate consequence of Theorem 8."
    }, {
      "heading" : "7 Evolving Conjunctions with Drifting Targets",
      "text" : "We now show that conjunctions are evolvable with drifting targets over the uniform distribution with a drift of O(ǫ2), independent of n. We begin by examining monotone conjunctions and prove that the neighborhood function defined by Valiant [19] is a strictly beneficial neighborhood function with b(n, 1/ǫ) = ǫ2/9. Our proof uses techniques similar to those used in the simplified analysis of Valiant’s algorithm presented by Diochnos and Turán [8]. By building on ideas from Jacobson [14], we extend this result to show that general conjunctions are evolvable with the same rate of drift."
    }, {
      "heading" : "7.1 Monotone Conjunctions",
      "text" : "We represent monotone conjunctions using a representation class R where each r ∈ R is a subset of {1, · · · , n} such that |r| ≤ log2(3/ǫ), representing the conjunction of the variables xj for all j ∈ r. We therefore allow the representation class to depend on ǫ in our analysis. This dependence is easy to remove (e.g., using Valiant’s technique of allowing an initial phase in which the length of the representation decreases until it is below log2(3/ǫ) [19]), but simplifies presentation.\nThe neighborhood of a representation r consists of the set of conjunctions that are formed by adding a variable to r, removing a variable from r, and swapping a variable in r with a variable not in r, plus the representation r itself. Formally, define the following three sets of conjunctions: N+(r) = {r ∪ {j}|j 6∈ r}, N−(r) = {r \\ {j}|j ∈ r}, and N±(r) = {r \\ {j} ∪ {k}|j ∈ S, k 6∈ S}. The neighborhood Neigh(r, ǫ) is then defined as follows. Let q = ⌈log2(3/ǫ)⌉. If r is the empty set, then Neigh(r, ǫ) = N+(r) ∪ r. If 0 < |r| < q, then Neigh(r, ǫ) = N+(r) ∪ N−(r) ∪N±(r) ∪ r. Finally, if |r| = q, then Neigh(r, ǫ) = N−(r) ∪ N±(r) ∪ r. Note that the size of the neighborhood is bounded by 1+n+n2/4 in the worst case; the combined size of the sets N+(r) and N−(r) is at most n, and the size of N±(r) is at most n2/4. We obtain the following theorem.\nTheorem 17 Let C be the class of monotone conjunctions, R be the class of monotone conjunctions of size at most q = ⌈log2(3/ǫ)⌉ represented as subsets of indices, and D be the uniform distribution. Define Neigh as above and let p be any polynomial satisfying p(n, 1/ǫ) ≥ 1 + n + n2/4. Then C is evolvable with drifting targets over D by algorithm A = (R, Neigh, µ, t, s) with • any distributions µ that satisfy µ(r, r′, ǫ) ≥ 1/p(n, 1/ǫ) for all r ∈ Rn, ǫ, and r′ ∈ Neigh(r, ǫ), • tolerance function t(r, ǫ) = ǫ2/18 for all r ∈ Rn, • any generation polynomial g(n, 1/ǫ) ≥ 144/ǫ2, • a sample size s(n, 1/ǫ) = Õ(1/ǫ2), and • any drift polynomial d(n, 1/ǫ) ≥ 144/ǫ2, which allows drift ∆ ≤ ǫ2/144.\nTo prove the theorem, we show that Neigh is a strictly beneficial target function with benefit polynomial b(n, 1/ǫ) = 9/ǫ2 and once again appeal to Theorem 8. The proof is then essentially just a case-by-case analysis of the performance of the best r′ ∈ Neigh(r, ǫ) for an exhaustive set of conditions on r and f ."
    }, {
      "heading" : "7.2 General Conjunctions",
      "text" : "Jacobson [14] proposed an extension to the algorithm above that applies to general conjunctions. The key innovation in his algorithm is the addition of a fourth set N ′(r) to the neighborhood or r,\nwhere each r′ ∈ N ′(r) is obtained by negating a subset of the literals in r. We show here that the drift rate of his construction can be analyzed in a similar way to the monotone case.\nWe represent general conjunctions using a representation class R where each r ∈ R is a subset of {1, · · · , n} ∪ {−1, · · · ,−n} such that |r| ≤ log2(3/ǫ). Here each r represents the conjunction of literals xj for all positive j ∈ r and negated literals x−j for all negative j ∈ r, and we restrict R so that it is never the case that both j ∈ r and −j ∈ r. The dependence of this representation class on ǫ can be removed as before.\nAs before, the neighborhood of a representation r includes the set of conjunctions that are formed by adding a variable to r, removing a variable from r, and swapping a variable in r with a variable not in r, plus the representation r itself. However, it now also includes a fourth set N ′(r) of all conjunctions that can be obtained by negating a subset of the literals of r. The size of the set N ′(r) is at most 2q ≤ 6/ǫ, so by a similar argument to the one above, the size of the neighborhood is bounded by 1 + 2n+ n2 + 6/ǫ. We obtain the following theorem.\nTheorem 18 Let C be the class of conjunctions, R be the class of conjunctions of at most q = ⌈log2(3/ǫ)⌉ literals represented as above, and D be the uniform distribution. Define Neigh as above and let p be any polynomial satisfying p(n, 1/ǫ) ≥ 1+2n+n2+6/ǫ. Then C is evolvable with drifting targets over D by A = (R, Neigh, µ, t, s) with µ, t, g, s, and d as specified in Theorem 17.\nThe proof uses many of the same ideas as the proof of Theorem 17. However, there are a few extra cases that need to be considered. First, if f is a “long” conjunction, and r contains at least one literal that is the negation of a literal in f , then we show that adding another literal to r leads to a significant increase in performance. (If r is already of maximum size, then the performance is already good enough.) Second, we show that if f is “short” and r contains at least one literal that is the negation of a literal in f , then there exists an r′ ∈ N ′(r) with significantly better performance. All other cases are identical to the monotone case."
    }, {
      "heading" : "A Additional Proofs",
      "text" : ""
    }, {
      "heading" : "A.1 Accuracy of the Empirical Performance",
      "text" : "In order to prove Lemma 5 and Theorem 8, it is necessary to examine how close the empirical performance of a representation r is to the representation’s true performance. The following simple lemma shows that as long as the sample size s(n, 1/ǫ) is sufficiently large, the empirical performance of each representation will be close to the true performance with high probability.\nLemma 19 Consider any r ∈ R and f ∈ C and fix any Z > 0 and δ > 0. Let N be an upper bound on the size of the neighborhood Neigh(r, 1/ǫ). For each r′ ∈ Neigh(r, ǫ), let v(r′) be the empirical performance of r′ with respect to f on a sample of size s ≥ 2 ln(2N/δ)/Z2. With probability 1 − δ, for all r′ ∈ Neigh(r, ǫ), |v(r′)− Perff (r′,D)| ≤ Z. Proof: Consider a particular r′ ∈ Neigh(r, ǫ). By Hoeffding’s inequality [13], for any Z, Pr (|v(r′)− Perff (r′,D)| ≥ Z) ≤ 2 exp ( −sZ2/2 ) . The right hand side of this inequality is upper bounded by δ/N as long as s ≥ 2 ln(2N/δ)/Z2, as we have assumed. The lemma then follows from a standard application of the union bound."
    }, {
      "heading" : "A.2 Proof of Lemma 5",
      "text" : "Suppose that Neigh is a strictly beneficial neighborhood function for C, D, and R with benefit polynomial b(n, 1/ǫ). To prove the first half of the lemma, we will construct an algorithm for strictly monotonically evolving C over D. First, for any r ∈ Rn and ǫ > 0, we set the tolerance at t(r, ǫ) = 1/(2b(n, 1/ǫ)). We then set s(n, 1/ǫ) = 128(b(n, 1/ǫ))2 ln(2p(n, 1/ǫ)/δ) for a choice of δ that will be specified below. By Lemma 19, this guarantees that on a particular round i, with probability at least 1− δ, for all r ∈ Neigh(ri−1, ǫ), |v(r) − Perff (r,D)| ≤ 1/(8b(n, 1/ǫ)). For the remainder of this proof, we refer to this high probability event as the HPE.\nFor any fixed round i, consider first the case that Perff (ri−1,D) ≥ 1 − ǫ/2. Since ri−1 ∈ Neigh(ri−1, ǫ), there is always at least one neutral mutation available and there could be a beneficial mutation, so ri will always be chosen from either Bene or Neut. Consider an arbitrary ri chosen from Bene ∪ Neut. If the HPE occurs, then\n(Perff (ri−1,D)− Perff (ri,D)) ≤ ( v(ri−1) + 1\n8b(n, 1/ǫ)\n) − ( v(ri)− 1\n8b(n, 1/ǫ)\n)\n= (v(ri−1)− v(ri)) + 1\n4b(n, 1/ǫ)\n≤ t(r, ǫ) + 1 4b(n, 1/ǫ) = 3 4b(n, 1/ǫ) ≤ 3ǫ 8 .\nThe last inequality uses the fact that 1/b(n, 1/ǫ) ≤ ǫ/2. This must be the case to guarantee that an improvement of 1/b(n, 1/ǫ) is possible when the performance is arbitrarily close to (but still less than) 1 − ǫ/2; otherwise, the definition of strictly beneficial neighborhood would not be satisfied. We then have\nPerff (ri,D) ≥ 1− ǫ 2 − 3ǫ 8 > 1− ǫ . (2)\nNow consider the case in which Perff (ri−1,D) < 1 − ǫ/2. Since Neigh is a strictly beneficial neighborhood function, it must be the case that there exists a representation r ∈ Neigh(ri−1, ǫ) such that Perff (r,D) ≥ Perff (ri−1,D) + 1/b(n, 1/ǫ). Call this representation r∗. If the HPE occurs, then\nv(r∗)− v(ri−1) ≥ ( Perff (r ∗,D)− 1\n8b(n, 1/ǫ)\n) − ( Perff (ri−1,D) + 1\n8b(n, 1/ǫ)\n)\n= (Perff (r ∗,D)− Perff (ri−1,D))−\n1 4b(n, 1/ǫ) ≥ 3 4b(n, 1/ǫ) > t(r, ǫ) ,\nand so r∗ ∈ Bene. Since the set Bene is non-empty, a representation in this set will be chosen for ri. Consider an arbitrary ri chosen from Bene. If the HPE occurs, then\n(Perff (ri,D)− Perff (ri−1,D)) ≥ ( v(r) − 1 8b(n, 1/ǫ) ) − ( v(ri−1) + 1 8b(n, 1/ǫ) )\n= (v(r) − v(ri−1))− 1\n4b(n, 1/ǫ)\n≥ t(r, ǫ)− 1 4b(n, 1/ǫ) = 1 4b(n, 1/ǫ) . (3)\nNow, let g(n, 1/ǫ) = 8b(n, 1/ǫ). Setting the parameter δ = ǫ/g(n, 1/ǫ) above and applying the union bound again, we have that with probability at least 1 − ǫ, the HPE occurs at all round i ∈ {1, 2, · · · , g(n, 1/ǫ)}. Suppose this is the case. From the argument leading up to Equation 3, we know that the performance of the current representation is monotonically increasing as long as the performance is less than 1 − ǫ/2, and furthermore increases by at least 1/(4b(n, 1/ǫ)) on each around. It remains to show that the algorithm evolves C, that is, that Perff (rg(n,1/ǫ),D) ≥ 1− ǫ.\nFrom the monotonic improvement when the performance is less than 1− ǫ/2 and the argument leading up to Equation 2, it is clear that if the performance ever reaches 1 − ǫ/2, it will not fall below 1 − ǫ again before round g(n, 1/ǫ). It is easy to see that the performance reaches 1 − ǫ/2 at some point during these g(n, 1/ǫ) rounds. In the worst case, the performance starts at −1. It is guaranteed to increase by at least 1/(4b(n, 1/ǫ)) on each round. Thus it must reach 1 − ǫ/2 in no more than g(n, 1/ǫ) = 8b(n, 1/ǫ) rounds.\nTo prove the second half of the lemma, note that the definition of strictly monotonic evolvability requires that for any initial representation r0 ∈ R, with high probability either Perff (r0,D) ≥ 1−ǫ/2 or Perff (r1,D) ≥ Perff (r0,D) + 1/m(n, 2/ǫ). This implies that if Perff (r0,D) < 1 − ǫ/2 there must exist an r1 ∈ Neigh(r1, ǫ) such that Perff (r1,D) ≥ Perff (r0,D) + 1/m(n, 2/ǫ)."
    }, {
      "heading" : "A.3 Proof of Theorem 8",
      "text" : "Suppose that Neigh is a strictly beneficial neighborhood function for C, D, and R with benefit polynomial b(n, 1/ǫ). For any r ∈ Rn and ǫ > 0, we set the tolerance at t(r, ǫ) = 1/(2b(n, 1/ǫ)). We then set s(n, 1/ǫ) = 128(b(n, 1/ǫ))2 ln(2p(n, 1/ǫ)/δ) for a choice of δ that will be specified below. This guarantees that on a particular round i, with probability at least 1 − δ, for all r ∈ Neigh(ri−1, ǫ), ∣ ∣v(r) − Perffi−1(r,D) ∣\n∣ ≤ 1/(8b(n, 1/ǫ)). (See Lemma 19 in Appendix A.1 for details.) For the remainder of this proof, we refer to this high probability event as the HPE.\nFix an i. Suppose ∆ ≤ 1/(16b(n, 1/ǫ)). If f1, f2, · · · is a ∆-drifting sequence, then for any r ∈ R, ∣\n∣Perffi−1(r,D)− Perffi(r,D) ∣ ∣ ≤ Ex∼D [ |fi−1(x) − fi(x)| · |r(x)| ] ≤ 2errD(fi−1, fi) ≤ 2∆ ≤ 1/(8b(n, 1/ǫ)) . (4)\nConsider the case that Perffi−1(ri−1,D) ≥ 1− ǫ/2. Since ri−1 ∈ Neigh(ri−1, ǫ), there is at least one neutral mutation available and there could be a beneficial mutation, so ri will be chosen from either Bene or Neut. Consider an arbitrary ri chosen from Bene∪ Neut. If the HPE occurs, then\nPerffi−1(ri−1,D)− Perffi−1(ri,D) ≤ ( v(ri−1) + 1\n8b(n, 1/ǫ)\n) − ( v(ri)− 1\n8b(n, 1/ǫ)\n)\n≤ t(r, ǫ) + 1 4b(n, 1/ǫ) = 3 4b(n, 1/ǫ) .\nThen from Equation 4 and the assumption that ∆ ≤ 1/(16b(n, 1/ǫ)),\nPerffi(ri,D) ≥ Perffi−1 (ri,D)− 1\n8b(n, 1/ǫ)\n≥ ( Perffi−1(ri−1,D)− 3\n4b(n, 1/ǫ)\n)\n− 1 8b(n, 1/ǫ) ≥ ( 1− ǫ 2 ) − 7ǫ 16 > 1− ǫ . (5)\nThe last line uses the fact that 1/b(n, 1/ǫ) ≤ ǫ/2. This must be the case to guarantee that an improvement of 1/b(n, 1/ǫ) is possible when the performance is arbitrarily close to (but still less than) 1− ǫ/2; otherwise, the definition of strictly beneficial neighborhood would not be satisfied.\nNow consider the case in which Perffi−1(ri−1,D) < 1− ǫ/2. Since Neigh is a strictly beneficial neighborhood function, it must be the case that there exists a representation r ∈ Neigh(ri−1, ǫ) such that Perffi−1(r,D) ≥ Perffi−1(ri−1,D) + 1/b(n, 1/ǫ). Call this r∗. If the HPE occurs, then\nv(r∗)− v(ri−1) ≥ ( Perffi−1(r ∗,D)− 1\n8b(n, 1/ǫ)\n) − ( Perffi−1(ri−1,D) + 1\n8b(n, 1/ǫ)\n)\n= ( Perffi−1(r ∗,D)− Perffi−1(ri−1,D) ) − 1 4b(n, 1/ǫ) ≥ 3 4b(n, 1/ǫ) > t(r, ǫ) ,\nand so r∗ ∈ Bene. Since the set Bene is non-empty, a representation in this set will be chosen for ri. Consider an arbitrary ri chosen from Bene. If the HPE occurs, then\nPerffi−1(ri,D)− Perffi−1(ri−1,D) ≥ (v(r) − 1/(8b(n, 1/ǫ)))− (v(ri−1) + 1/(8b(n, 1/ǫ))) ≥ t(r, ǫ)− 1/(4b(n, 1/ǫ)) = 1/(4b(n, 1/ǫ)) .\nThen from Equation 4,\nPerffi(r,D) − Perffi−1(ri−1,D) ≥ Perffi−1(r,D)− Perffi−1(ri−1,D)− 2∆ ≥ 1/(4b(n, 1/ǫ))− 1/(8b(n, 1/ǫ)) = 1/(8b(n, 1/ǫ)) . (6)\nNow, let g(n, 1/ǫ) = 16b(n, 1/ǫ) and consider any round ℓ ≥ g(n, 1/ǫ). Setting the parameter δ = ǫ/g(n, 1/ǫ) above and applying the union bound again, we have that with probability at least 1− ǫ, the HPE occurs at all rounds i ∈ {ℓ− g(n, 1/ǫ), · · · , ℓ− 1}. Suppose this is the case.\nFrom the argument leading up to Equation 6, we know that the performance of the current representation with respect to the current target is monotonically increasing as long as the performance is less than 1− ǫ/2, and increases by at least 1/(8b(n, 1/ǫ)) on each round. Combining this with the argument leading up to Equation 5, it is clear that if the performance ever reaches 1 − ǫ/2 during this period of time, it will never again fall below 1− ǫ before round ℓ. It remains to show that the performance reaches 1− ǫ/2 at some point during these g(n, 1/ǫ) rounds. This is also easy to see. In the worst case, the performance starts at −1. It is guaranteed to increase by at least 1/(8b(n, 1/ǫ)) on each round, so it must reach 1− ǫ/2 in no more than g(n, 1/ǫ) = 16b(n, 1/ǫ) rounds.\nThis shows that for any ℓ ≥ g(n, 1/ǫ), with probability at least 1 − ǫ, Perffℓ(rℓ,D) ≥ 1 − ǫ and so C is evolvable with drifting targets."
    }, {
      "heading" : "A.4 Proof of Lemma 10",
      "text" : "Assume that i < j. The proof for the case in which i > j is nearly identical, and the result is trivial if i = j. For any τ and any function φ : X → [−1, 1],\n|E[φ(x)fi(x)] − E[φ(x)fj(x)]| = |E [φ(x)(fi(x)− fj(x))]| = ∣ ∣ ∣ ∣\n∣\nE\n[\nφ(x)\nj−i ∑\nk=1\n(fi+k−1(x)− fi+k(x)) ]∣ ∣ ∣ ∣\n∣\n≤ E [ |φ(x)| j−i ∑\nk=1\n|fi+k−1(x)− fi+k(x)| ]\n≤ j−i ∑\nk=1\nE [|fi+k−1(x)− fi+k(x)|] ≤ (j − i)∆ ≤ τ\n2 ,\nwhere all expectations are taken with respect to x ∼ D. Therefore if Ex∼D[φ(x)fj(x)] ≥ θ+ τ , then Ex∼D[φ(x)fi(x)] ≥ Ex∼D[φ(x)fj(x)]− τ\n2 ≥ θ + τ − τ 2 = θ + τ 2 .\nSimilarly, if Ex∼D[φ(x)fj(x)] ≤ θ − τ , then Ex∼D[φ(x)fi(x)] ≤ Ex∼D[φ(x)fj(x)] + τ\n2 ≤ θ − τ + τ 2 = θ − τ 2 ."
    }, {
      "heading" : "A.5 Proof of Lemma 11",
      "text" : "Under the assumption that the LPE does not occur at any time step, after q time steps if rq = rǫ[h, z], then |z| = q, since we add one bit at each step. Let ri = rǫ[h, zi]. We consider the possible mutations of ri. Observe that rǫ[h, z i0] is always neutral for all i. The cases we need to consider are (a) rǫ[h, z i1] is beneficial, and therefore chosen as the next representation implying that zi+1 = 1, (b) rǫ[h, z i1] is deleterious, and therefore rǫ[h, z i0] is chosen as the next representation, implying that zi+1 = 0, and (c) rǫ[h, z i1] is neutral, which implies that either rǫ[h, z i1] or rǫ[h, z i0] can be chosen as the next representation, implying that zi+1 = 0 or 1. Suppose we are in case (a), then we show that 1 is a valid answer to the query (φzi,ǫ, θzi,ǫ, τ/2) with respect to fi. Consider,\nt\n(\nri, 1\nǫ\n)\n≤ v(rǫ[h, zi1])− v(ri) ≤ Perffi(rǫ[h, zi1],D)− Perffi(ri,D) + 2τ ′ = ǫ 2q E[φzi,ǫ · fi] + 2τ ′ .\nRe-arranging the terms, we get:\nE[φzi,ǫ · fi] ≥ 2q\nǫ\n(\nt\n(\nri, 1\nǫ\n) − 2τ ′ ) ≥ θzi,ǫ − τ\n2 .\nSimilarly one can show in case (b), that E[φzi,ǫ · f ] ≤ θzi,ǫ − τ/2 and hence 0 is a valid answer to the query (φzi,ǫ, θzi,ǫ, τ/2) and in case (c), that θzi,ǫ − τ/2 ≤ E[φzi,ǫ · f ] ≤ θzi,ǫ + τ/2, and hence both 0 and 1 are valid answers.\nBy Lemma 10, if zi+1 is a valid answer to the query (φzi,ǫ, θzi,ǫ, τ/2), with respect to fi it is a valid answer to the (φzi,ǫ, θzi,ǫ, τ) with respect to fq (since ∆ ≤ τ/(2q)). Thus z is consistent with fq."
    }, {
      "heading" : "A.6 Proof of Lemma 12",
      "text" : "Let τ ′ = tu(n, 1/ǫ)/8. Assuming that the LPE does not occur at any time step, wj+1 is always a neutral mutation for wj , and mutations of the form wj → wj will not occur. Also rǫ[hz,ǫ, σ] will always be chosen if it is a neutral mutation. Then in K − k rounds we will reach wK (if we had not already gone to rǫ[hz,ǫ, σ]) and hence on the next round we will move to rǫ[0, σ]. This implies that the number of steps is at most K − k + 1.\nNow, suppose that if at some stage Perffi(ri,D) < Perffi(rǫ[hz,ǫ, σ],D). Then Perffi−1(rǫ[hz,ǫ, σ],D)− Perffi−1(ri−1,D)\n≥ Perffi(rǫ[hz,ǫ, σ],D) − Perffi−1(ri,D)− tu(n, 1ǫ ) 2 −∆ ≥ − tu(n, 1 ǫ ) 2 −∆,\nand so rǫ[hz,ǫ, σ] is a neutral mutation for ri−1. By the assumption above, ri = rǫ[hz,ǫ, σ], proving the lemma."
    }, {
      "heading" : "A.7 Proof Sketch for Theorem 14",
      "text" : "We apply pieces of analysis of Theorem 13 here. We omit some details since the arguments are very similar; in fact, the argument that this algorithm is resistant to drift is nearly identical. To start, we let the representation class be Rǫ which depends on ǫ. Here, backsliding is not allowed since it degrades performance arbitrarily. We discuss how to encode all values of ǫ in the same representation class in the Section A.7.1 below.\nWe will use the same construction as defined in Section 5.3, with only a small modification. For the representations in Wǫ, say w0 = rǫ[h, z] with |z| = q, and in the neighborhood of wk we will also add rǫ[h, σ] in addition to the existing rǫ[hz, σ], rǫ[0, σ] and wk+1. Thus, even if hz∗ has poor performance, we can ensure that the performance goes more than ǫ lower than the starting state. Formally, • Neigh′(wK , ǫ) = {wK , rǫ[0, σ]} and Neigh′(wk, ǫ) = {wk, wk+1, rǫ[hz, σ], rǫ[h, σ]} for k < K, • µ′(wK , wK) = η and µ′(wK , rǫ[0, σ]) = 1 − η, and µ′(wk, wk) = η2, µ′(wk, wk+1) = µ′(wk, rǫ[hz, σ] = (η − η2)/2, and µ′(wk, rǫ[h, σ]) = 1− η for k < K, and\n• t′(wk, ǫ) = tu(n, 1/ǫ) for all k. Let η = ǫ/(4q + 2K), τ ′ = min{(ǫτ)/(2q), tu(n, 1/ǫ)/8}, and s = 1/(2(τ ′)2) log((6q + 3K)/ǫ), as defined earlier in section 5.3. Let Neigh′ = Neigh, µ′ = µ and t′ = t for the representations in R̃ǫ. We will show that E = (Rǫ, Neigh′, µ′, t′, s) evolves C quasi-monotonically.\nThe intuition of the proof is as follows. Any two representations rǫ[h, z] and rǫ[h, z ′] are within performance ǫ of each other (by definition). Using a similar argument as that for Lemma 12, one can show that while we start decreasing performance from w0 = rǫ[h, z], the performance never dips below the performance of rǫ[h, σ] (and since this has the highest probability, this will be chosen whenever it is neutral). If rǫ[hz, σ] is chosen earlier, it will be because its performance was higher than that of rǫ[h, σ] and quasi-monotonicity is maintained. Just as in Theorem 13, one can show that in at most 2q +K + 1 steps, a representation with performance 1− ǫ is reached.\nFor our setting of parameters, for g time steps LPE does not occur. And we will assume that this is the case. Recall that ∆ = (ǫτ)/(4q + 2K + 2).\nThere are two distinct types of starting representations: (i) rǫ[h, z] with |z| < q, or (ii) wk for some k where w0 = rǫ[h, z] with |z| = q. Suppose first that the starting representation is rǫ[h, z]. Since LPE events don’t occur, we will reach rǫ[h, z\n∗] with |z∗| = q in q − |z| steps. Note that for all z′ for any f , |Perff (rǫ[h, z′],D) − Perff (rǫ[h, z],D)| ≤ ǫ. So during this phase quasi-monotonicity is maintained.\nConsider the case in which the starting representation is instead wk for some k, with w0 = rǫ[h, z ∗] and |z∗| = q, or the case in which we reach such a representation wk after starting at rǫ[h, z]. The algorithm then transitions to either representation rǫ[h, σ] or representation rǫ[hz∗ , σ]. Furthermore, the transition to rǫ[hz∗ , σ] happens only if Perff (hz∗ ,D) ≥ Perff (h,D). This happens in at most K steps (using argument similar to that of Lemma 12) and during this time the performance never goes below that of rǫ[h, σ], and so never goes more than ǫ/2 below that of the starting representation.\nLet h′ be either h or hz∗ depending on which was chosen as described in the above paragraph. Since ∆ is low, the performance of h′ will never go significantly below that of h (even if h′ = hz∗) for the next g steps, hence it is sufficient to prove then that the performance will not drop significantly below that of rǫ[h\n′, σ]. From rǫ[h′, σ] in at most q steps we reach a representation rǫ[h′, z′] where z′ is consistent with f . During this time the performance never goes more than ǫ/2 below that of Perf(rǫ[h\n′, σ],D). From rǫ[h′, z′] we reach a representation with performance greater than 1 − ǫ in one step, or rǫ[h ′, z′] already has performance at least 1− ǫ. Thus, the evolution is quasi-monotonic."
    }, {
      "heading" : "A.7.1 Removing the Need to Know ǫ",
      "text" : "In Section A.7, we showed that any CSQ algorithm can be converted into an evolutionary algorithm that is drift-resistant and quasi-monotonic, provided we are allowed to fix ǫ and encode its value in the representation. Here, we describe in some detail how a representation class that simultaneously encodes all values of ǫ can be constructed. Note that the definition of evolvability allows the neighborhood to depend on ǫ, but not the starting representation.\nWe assume that the parameter ǫ provided to the algorithm is a power of 2. If this were not the case we could simply run the algorithm with ǫ′, setting ǫ′ = 2⌊log ǫ⌋. The performance guarantees would only be better since ǫ′ ≤ ǫ. Furthermore, since ǫ′ ≥ ǫ/2, the running time would not be affected, except up to a constant factor. The representations will encode values of ǫ ranging over the set Sǫ = {1/2, 1/4, . . . , 2−n}. It is not necessary to consider values of ǫ smaller than this, since this would allow the algorithm to take time exponential in n, and hence an exhaustive search over all functions of polynomial-sized representations would be feasible in just one round of evolution. For the rest of this section, assume that ǫ can only take values from this set.\nRecall the notation used in Section 5. In particular, A is a CSQ> algorithm that takes parameter ǫ, makes q = q(n, 1/ǫ) queries of tolerance τ(n, 1/ǫ), returns a hypothesis h with Perff (h,D) ≥ 1−ǫ. Similar to the definitions in 5.3, let Φz,ǫ = (1/q) ∑q i=1 I(zj = 1)φzj−1,ǫ(x).\nDefine a term as follows:\n• Every h ∈ H is a term, and h is said to encode no ǫ. • For any ǫ1, let T1 be a term that either encodes no ǫ or encodes only ǫ′ > ǫ1. Then T = (1− ǫ1/2)T1 + (ǫ1/2)Φz,ǫ1 is a term if |z| ≤ q(n, 1/ǫ1). Furthermore, T is said to encode all of the values of ǫ that T1 encodes plus ǫ1.\nThus any term T may encode up to n values of ǫ, and the values of ǫ will increase as we get deeper in the term. This ensures that all terms have polynomial-sized (in n) representations, and the number of terms is finite.\nLet list(T ) denote the list of all ǫ ∈ Sǫ that are encoded in T . Observe that the definition of term implies that the smallest ǫ ∈ list(T ) is encoded at the outermost level, and the values increase as we move to the interior. In particular if ǫ1 is the smallest value in list(T ), then T = (1−ǫ1/2)T1+ (ǫ1/2)Φz,ǫ1 for some z and T1 and list(T1) = list(T ) \\ {ǫ1}. Denote by out(T ) the smallest value of ǫ in list(T ) and let next(T ) be T1 such that T = (1− out(T )/2)T1 + (out(T )/2)Φz,out(T ).\nWe consider all terms except those of the form h ∈ H to be valid representations. The representation class will also contain more representations, that we shall define shortly.\nConsider the following three cases.\n(a) The evolutionary algorithm is in a state T , such that out(T ) = ǫ, where ǫ is the true parameter of the algorithm. Then (pretending as if T is in H) the results from Section 5 will apply directly. In particular, let T = rǫ[T1, z] = (1−ǫ/2)T1+(ǫ/2)Φz,ǫ. Then Neigh(T, ǫ) = {T, rǫ[T, z0], rǫ[T, z1]} if |z| ≤ q(n, 1/ǫ). When |z| = q(n, 1/ǫ), we again define states similar to those in Wǫ in 5.3, which allow the algorithm to gradually slide to move to rǫ[hz, σ], rǫ[T, σ] or rǫ[0, σ] (but the performance will never go more than ǫ lower than T with high probability).\n(b) The case when out(T ) ≤ ǫ. Let T0 = T , and define Ti = next(Ti−1) for all i, let k be the smallest such that out(Tk) ≥ ǫ. (It may happen that Tk = h for some h ∈ H. Note that,\nT1 = (1−ǫ1/2) ( (1−ǫ2/2) ( · · · (1− ǫk−1/2)Tk + (ǫk−1/2)Φzk−1,ǫk−1 · · · ) +(ǫ2/2)Φz2,ǫ2 ) +(ǫ1/2)Φz1,ǫ1 .\nThen since ǫ1 < ǫ2 < · · · < ǫk−1 ≤ ǫ/2, and every ǫi is a power of 2, Perff (Tk,D) ≥ Perf(T1,D)− 2(ǫ1 + · · · ǫk−1) ≥ 4(ǫ/2) .\nThen Perff (Tk,D) ≥ Perff (T,D) − 2ǫ (because ǫk−1 ≤ ǫ/2 ). If out(Tk) = ǫ, let rb = Tk. Otherwise, let rb = (1− ǫ/2)Tk + (ǫ/2)Φσ,ǫ. Note that rb is always a valid term (by the above definition) and hence it is in the representation class. Also Perff (rb,D) ≥ Perf(T,D)− 3ǫ.\n(c) The case, when out(T ) > ǫ. Let rc = (1 − ǫ/2)T + (ǫ/2)Φσ,ǫ. Again, rc is a valid term and hence in the representation class. Also in this case Perff (rc,D) ≥ Perff (T,D)− ǫ. In cases (b) and (c), if we can transition to the representations rb and rc respectively, we will\nhave reduced to case (a). However since the moves themselves may be deleterious, we need to add intermediate representations similar to those defined in Wǫ in Section 5.3. In particular let w0 = T (where T may be that of case (b) or (c)). Define wk = (1 − k(tu(n, 1/ǫ)/2))w0, where tu(n, 1/ǫ) is the polynomial upper bound on the tolerances. Define Neigh(wk, ǫ) = {wk, wk+1, rb}\n(or Neigh(wk, ǫ) = {wk, wk+1, rc}). The idea is the same that the performance reduces gradually until the jump to rb (or rc) is no longer deleterious. During this time the performance never goes below that of rb (respectively rc) and hence quasi-monotonicity is maintained. (Although in some cases degradation may be 3ǫ, we could just run with higher accuracy (say ǫ/4) to begin with.)\nSo far we have ignored the drift. However, notice that the number of time steps to get to a representation which encodes the correct value of ǫ is, with high probability, polynomial (in fact just 2/tu(n, 1/ǫ)). Thus by making the drift small enough (though still an inverse polynomial), the function can be made to look essentially unchanging to the evolution algorithm."
    }, {
      "heading" : "A.8 Proof of Theorem 15",
      "text" : "We show that Neigh is a strictly beneficial neighborhood function for C, D, and R with b(n, 1/ǫ) = π3n/(2ǫ). The theorem is then an immediate consequence of Theorem 8.\nThe analysis relies heavily on a couple of useful trigonometric facts. First, it is well known (see, for example, Dasgupta [6]) that under any spherically symmetric distribution D (for example, the uniform distribution over a sphere), errD(u,v) = arccos(u · v)/π, where arccos(u · v) is the angle between u and v. We will use this fact repeatedly. We also make use of the following inequalities from Dasgupta et al. [7]. For any θ ∈ [0, π/2], 2θ/π ≤ sin(θ) ≤ θ, and 4θ2/π2 ≤ 1− cos(θ) ≤ θ2/2.\nConsider an arbitrary r ∈ Rn and f ∈ Cn. To simplify presentation, assume that r1 = 1 and ri = 0 for i ∈ {2, · · · , n}. (Here and for the remainder of this proof, we use the notation ri and fi to denote the ith components of r and f , not the values of the representation and ideal function at round i as in previous sections.) This assumption is without loss of generality since we are considering only spherically symmetric distributions. Furthermore, assume that the axes are oriented such that for any r′ ∈ Neigh(r, ǫ) (except for r itself), r′1 = cos(ǫ/(π √ n)), r′i = ± sin(ǫ/(π √ n)) for some i ∈ {2, · · · , n}, and r′j = 0 for all other j. This change in basis is also without loss of generality. Suppose that Perff (r,D) < 1− ǫ/2 since otherwise there is nothing to prove. The condition that we need to prove can be stated as\nmax r′∈Neigh(r,ǫ)\nPerff (r ′,D) ≥ Perff (r,D) +\n1\nb(n, 1/ǫ) = Perff (r,D) +\n2ǫ\nπ3n .\nUsing the facts that for any unit vectors u and v, Perfv(u,D) = 1− 2err(u,v) and errD(u,v) = arccos(u ·v)/π, this condition is equivalent to arccos(maxr′∈Neigh(r,ǫ) r′ · f) ≤ arccos (r · f)− ǫ/(π2n). By definition of the neighborhood function, there exists a r′ ∈ Neigh(r, ǫ) such that\nr′ · f ≥ f1 cos ( ǫ\nπ √ n\n)\n+ max i∈{2,··· ,n}\n|fi| sin ( ǫ\nπ √ n\n) ≥ f1 cos ( ǫ\nπ √ n\n)\n+\n√\n1− f21 n sin ( ǫ π √ n ) .\nUsing the standard trigonometric equality that for any θ and φ, arccos(θ)− arccos(φ) = arccos(θφ+ √\n(1− θ2)(1− φ2)), we have\narccos(r · f)− ǫ π2n = arccos(f1)− arccos ( cos ( ǫ π2n ))\n= arccos\n(\nf1 cos ( ǫ\nπ2n\n) + √ 1− f21 sin ( ǫ\nπ2n\n)\n)\n.\nThen since arccos is decreasing in [0, π], to prove the result, it is sufficient to show that\narccos\n(\nf1 cos\n(\nǫ\nπ √ n\n)\n+\n√\n1− f21 n sin ( ǫ π √ n )\n)\n≤ arccos ( f1 cos ( ǫ\nπ2n\n) + √ 1− f21 sin ( ǫ\nπ2n\n)\n)\nor taking the cosine of both sides and rearranging terms,\nf1\n(\ncos ( ǫ\nπ2n\n) − cos ( ǫ\nπ √ n\n))\n≤ √ 1− f21 ( 1√ n sin ( ǫ π √ n ) − sin ( ǫ π2n ) ) . (7)\nFirst consider the case in which f1 < 0. In this case, it is sufficient to show that the difference of cosines on the left hand side of the equation and the difference in sines on the right hand side are both positive. This can be verified easily using the inequalities for sines and cosines given above.\nFor the rest of this proof, assume that f1 > 0. Since we have assumed that Perff (r,D) < 1−ǫ/2, it follows that err(r, f) = arccos(f1)/π > ǫ/4, or equivalently, f1 < cos(ǫπ/4). Then √\n1− f21 > √\n1− (cos(ǫπ/4))2 = sin(ǫπ/4), and for Equation 7 to hold, it is sufficient to show that\ncos ( ǫπ\n4\n)\n(\ncos ( ǫ\nπ2n\n) − cos ( ǫ\nπ √ n\n))\n≤ sin ( ǫπ\n4\n)\n(\n1√ n sin\n(\nǫ\nπ √ n\n)\n− sin ( ǫ\nπ2n\n)\n)\n. (8)\nUsing the inequalities for sines and cosines given above, we have that\ncos ( ǫπ\n4\n)\n(\ncos ( ǫ\nπ2n\n) − cos ( ǫ\nπ √ n\n)) ≤ 1 ( 1− cos ( ǫ\nπ √ n\n))\n≤ ǫ 2\n2π2n ,\nand\nsin ( ǫπ\n4\n)\n(\n1√ n sin\n(\nǫ\nπ √ n\n)\n− sin ( ǫ\nπ2n\n)\n)\n≥ ǫ 2\n(\n2ǫ π2n − ǫ π2n\n)\n= ǫ2\n2π2n .\nTherefore Equation 8 holds, Neigh is a strictly beneficial neighbor function, and C is evolvable with drifting targets."
    }, {
      "heading" : "A.9 Proof of Theorem 16",
      "text" : "We start by analyzing the simpler case in which D is known to be a spherical Gaussian distribution. In this case, a simpler neighborhood function can be used in which set of “shift” neighbors Nsl is greatly reduced. Below, we explain how to extend this analysis to the case in which D is an unknown product normal distributions over Rn and more complex neighborhood function defined in Section 6.2 is used.\nThroughout this proof, we use the notation ri and fi to denote the ith components of r and f respectively, and denote by ei the basis of R\nn. We define the simplified neighborhood function as Neigh′(r, ǫ) = Nfl ∪ N ′sl, where Nfl = {r − 2riei | i = 1, . . . , d} is still the set of representations obtained by flipping the sign of one component of r, and N ′\nsl = {r′i/‖r′i‖2 | r′i = r±βei, i = 1, . . . , d}\nis the set obtained by shifting each component a small amount and renormalizing, with β satisfying ǫ2/(6 √ n) ≤ β ≤ ǫ2/(3√n).\nWe first show that for any target function f , increasing Perff (r,D) is the equivalent to increasing f · r. The following two lemmas establish this. These lemmas rely on the same trigonometric facts used in the proof of Theorem 15. In particular, under any spherically symmetric distribution D, errD(u,v) = arccos(u · v)/π, where arccos(u · v) is the angle between u and v, and for any θ ∈ [0, π/2], 2θ/π ≤ sin(θ) ≤ θ, and 4θ2/π2 ≤ 1− cos(θ) ≤ θ2/2.\nLemma 20 Let D be a spherical Gaussian distribution. For unit vectors v, f and α ∈ (0, 1), if f · v ≥ 1− α, then Perf f (v,D) ≥ 1−√α\nProof: Since f and v are unit vectors and α ∈ (0, 1), i.e., f · v > 0, we may write f · v = cos(θ) for θ ∈ [0, π/2]. Thus we have that 1 − 4θ2/π2 ≥ cos(θ) ≥ 1 − α, and hence θ/π ≤ √α/2. But θ/π = err(f ,v) and Perff (v,D) = 1− 2err(f ,v) ≥ 1− √ α.\nLemma 21 Let D be a spherical Gaussian distribution. For unit vectors u, v, f , if f ·u−f ·v ≥ ω > 0, then Perf\nf (u,D) − Perf f (v,D) ≥ ω/2.\nProof: Since u, v and f are unit vectors, we may assume that there exit angles φ, θ ∈ [0, π] such that f · u = cos(φ) and f · v = cos(θ), and that φ < θ. Since the derivative of the cosine function is lower bounded by −1, cos(φ) − cos(θ) ≤ θ − φ. Finally observe that Perff (u,D) − Perff (v,D) = 2(err(f ,v)− err(f ,u)) = 2π (θ − φ) ≥ ω/2.\nThe next lemma shows that Neigh′ is a strictly beneficial neighborhood function. More than the lemma statement itself, it is the analysis of this lemma that is important to us. Below we will show how to extend this analysis to the case in which D is a product normal distribution.\nLemma 22 Let C be the class of homogeneous linear separators, R be the class of homogeneous linear separators represented by unit length normal vectors, and D be a spherical Gaussian distribution. Define Neigh′ as in the previous paragraph and let p be any polynomial such that p(n, 1/ǫ) ≥ 3n. Then Neigh′ is a strictly beneficial neighborhood function for C, D, and R, with b(n, 1/ǫ) = 144n/ǫ6.\nProof: Let ρ = ǫ3/(12 √ n) and η = ǫ2/(3 √ n). By assumption, β then satisfies η/2 ≤ β ≤ η.\nConsider an arbitrary r ∈ Rn and f ∈ Cn. If there exists r′ ∈ Nfl such that Perff (r′,D) − Perff (r,D) ≥ 1/b(n, 1/ǫ), then we are done, so assume that there is no element in Nfl with this property. In this case, we then claim that one of the following must hold for all i = 1, . . . , n: (i) ri and fi have the same sign, (ii) |ri| ≤ ρ, or (iii) |fi| ≤ ρ. If none of these properties hold, then f · (r − 2riei) − f · r = −2rifi ≥ 2ρ2 and by Lemma 21, the change in performance is at least ρ2 = 1/b(n, 1/ǫ).\nIn the rest of the analysis we assume that if no flip is a beneficial mutation (by at least 1/b(n, 1/ǫ)), all ri and fi are in the interval [−ρ, 1]. The reason this does not affect generality is this: Suppose one of them is smaller than −ρ, we know that the other one then lies in the interval [−1, ρ]. We can now assume that the basis we were working with actually contained −ei rather than ei. (This is useful for analysis, so that we can only consider mutations which increase the value of any component.) However, the neighborhood contains both mutations. Thus, we may assume that all fi and ri are in [−ρ, 1], and hence f · r ≥ −ρ(‖f‖1 + ‖r‖1) ≥ −2ρ √ n.\nIn this situation if there is no i for which ri ≤ fi− η, then f · r ≥ 1− ǫ2, and we are already close to optimal. (as shown below)\nf · r = ∑\ni\nfiri = ∑\ni fi∈[−ρ,0)\nfiri + ∑\ni fi∈[0,1]\nfiri ≥ −ρ‖r‖1 + ∑\ni fi∈[0,1]\nfi(fi − η)\n≥ ∑\ni fi∈[0,1]\nf2i − ρ‖r‖1 − η‖f‖1 ≥ 1− nρ2 − √ nρ−√nη\nSuppose there exists an i for which ri ≤ fi− η, then fi ≥ η−ρ ≥ η/2 > 0. Let r′ = r+βei, with η/2 ≤ β ≤ η. Then ‖r′‖2 = √ 1 + 2βri + β2. From elementary algebra, we get the inequality that 1 + βri ≤ √\n1 + 2βri + β2 ≤ 1 + βri + β2/2 (assuming βri ∈ (−1, 1), which is true). Then consider the following quantity of interest:\nf · r ′ ‖r′‖2 − f · r = f · r ′ − √ 1 + βri + β2(f · r) ‖r′‖2\nSince 1/2 ≤ ‖r′‖2 ≤ 2, if the quantity in the numerator is positive (as we will show) we have\n2\n( f · r ′\n‖r′‖2 − f · r\n)\n≥ f · (r+ βei)− √ 1 + 2βri + β2(f · r)\n= βfi + f · r ( 1− √ 1 + 2βri + β2 ) . (9)\nNotice that by our setting of parameters, ri ≥ −ρ ≥ −β/2, thus the quantity under the square root sign is greater than 1. When f · r < 0, the second term in the above expression is actually positive, and hence the total quantity is at least as much as the first term which is at least βη/2 = η2/4 ≥ 2/b(n, 1/ǫ). Thus we will consider the case when f · r ≥ 0. In that case continuing from equation (9) and using the fact that √ 1 + 2βri + β2 ≤ 1 + βri + β2/2, we get\nf · r ′\n‖r′‖2 − f · r ≥ 1 2 ( βfi − (1− ǫ2)(βri + β2/2) ) .\nSince ri + β/2 ≤ ri + η ≤ fi, this is greater than βfiǫ2/2 ≥ 2/b(n, 1/ǫ). Hence Neigh′ is a strictly beneficial neighborhood function."
    }, {
      "heading" : "Product Normal Distributions",
      "text" : "We now describe how the analysis above can be adjusted to handle product normal distributions with polynomial variances. Recall that σ1, . . . , σn are the standard deviations of the distribution D in each of the n dimensions, and that 1 ≥ σi ≥ (1/n)k for all i for some constant k (which is known by the algorithm). Assume without loss of generality that 1 = σ1 ≥ σ2 ≥ · · · ≥ σn ≥ (1/n)k.\nDefine τ(x1, . . . , xn) = (x1/σ1, · · · , xn/σn) and λ(x1, . . . , xn) = (σ1x1, · · · , σnxn). Note that the transformations τ and λ are inverses. Let N [0,Σ] denote the distribution with covariance matrix Σ = diag(σ21 , . . . , σ 2 n), and let N [0, I] denote the spherical normal distribution with variance 1. Note that if x is distributed according to N [0,Σ], τ(x) is distributed according to N [0, I]. For any vector f and r, we have\nerrN [0,Σ](f , r) = Prx∼N [0,Σ][sign(λ(f) · τ(x)) 6= sign(λ(r) · τ(x)] = Prz∼N [0,I][sign(λ(f) · z) 6= sign(λ(r) · z)] = errN [0,I](λ(f), λ(r)) .\nWe assume that ‖f‖2 = 1 and that our representations consist of vectors also of unit norm. Then, for all r we have, (1/n)k ≤ ‖λ(r)‖2 ≤ 1. Observe that the “flips” are invariant with respect to λ, i.e., λ(r − 2riei) = λ(r) + 2λ(r)iei. Because of this, we can consider the same set Nfl of flips as in the spherical distribution case.\nUnfortunately, it does not suffice to use the same set of “shift” mutationsN ′ sl . Let r be our current representation and let r′ = r + γei. Consider the two vectors λ(r)/‖λ(r)‖2 and λ(r′)/‖λ(r)‖2, and consider their ith components, which are σiri/‖λ(r)‖2 and σi(ri + γ)/‖λ(r)‖2 respectively. The difference between the two is σiγ/‖λ(r)‖2. As in the proof of Lemma 22, let η = ǫ2/(3 √ n). If γ took a certain value such that η/2 ≤ σiγ/‖λ(r)‖2 ≤ η, then by the same analysis in the proof of Lemma 22, this would be a beneficial mutation.\nSince we don’t know the values of σi and ‖λ(r)‖2, we use the following trick: Let\nNi =\n{ r± ( jη\n4nk\n) ei | 1 ≤ j ≤ 4nk } .\nNow consider the quantity\nγj = σi ‖λ(r)‖2 ηj 4nk .\nObserve that (1/n)k ≤ σi/‖λ(r)‖2 ≤ nk. Thus γ1 ≤ η/4, γ4n2k ≥ η, and finally γj − γj−1 ≤ η/4; at least one j satisfies η/2 ≤ γj ≤ η. Let Nsl = {r′/‖r′‖2|r′ ∈ Ni, i ∈ {1, . . . , n}}. (This is the same set Nsl defined in Section 6.2, only in slightly different notation.) With Neigh(r, ǫ) = Nfl ∪Nsl, Neigh is then a strictly beneficial neighborhood function with respect to any product normal distribution with variance lower bounded by (1/n)k as desired. The benefit polynomial remains the same as in Lemma 22, b(n, 1/ǫ) = 144n/ǫ6, though the neighborhood size is larger."
    }, {
      "heading" : "A.10 Proof of Theorem 17",
      "text" : "We show that Neigh is a strictly beneficial neighborhood function for C, D, and R with benefit polynomial b(n, 1/ǫ) = 9/ǫ2. The theorem is then an immediate consequence of Theorem 8.\nSince q = ⌈log2(3/ǫ)⌉, it follows that ǫ/6 ≤ 2−q ≤ ǫ/3. We make use of this repeatedly. Consider an arbitrary r ∈ Rn and f ∈ Cn. As in Diochnos and Turán [8], we define m to be the number of “mutual” variables shared between f and r, u to be the number of “undiscovered” variables that appear in f but not r, and w to be the number of “wrong” variables that appear in r but not f . Thus |f | = m+ u and |r| = m+w. The functions r and f disagree if and only if all m mutual variables are true and either all u undiscovered variables are true while some wrong variable is false, or all w wrong variables are true while some undiscovered variable is false. Therefore if D is uniform, errD(f, r) = 2−m (2−u(1− 2−w) + 2−w(1− 2−u)) = 2−m−u + 2−m−w − 21−m−u−w, so\nPerff (r,D) = 1− 21−m−u − 21−m−w + 22−m−u−w = 1− 21−|f | − 21−|r| + 22−m−u−w . (10) We start by considering the case in which the target is “long”, that is, |f | = m + u ≥ q + 1. If |r| = m+w = q, then Perff (r,D) > 1−21−|f |−21−|r| ≥ 1−2−q−21−q = 1−3 ·2−q ≥ 1− ǫ, and the performance of r with respect to f is already good enough. This is because both f and r are almost always false under the uniform distribution. On the other hand, if |r| < q, then there must exist a neighbor r′ in the set N+(r) such that the variable contained in r′ but not r is an undiscovered variable of f . Then Perff (r\n′,D)− Perff (r,D) = 2−|r| > 2−q = ǫ/6. Now consider the case in which the target is “short”, that is, f = m + u ≤ q. Suppose that\nu = 0 (so the variables in f are a strict subset of the variables in r). If w = 0, then f and r must be identical, so assume w > 0. Then there must exist a neighbor r′ in the set N− such that the variable contained in r but not r′ is a wrong variable. From Equation 10, for this neighbor r′, Perff (r′,D)− Perff (r,D) = 21−|r| ≥ 21−q ≥ ǫ/3. On the other hand, suppose that u > 0. If |r| = m+w < q, then there must exist a neighbor r′ in the set N+(r) such that the variable contained in r′ but not r is an undiscovered variable of f . As above, Perff (r\n′,D)− Perff (r,D) = 2−|r| > 2−q = ǫ/6. Finally, if |r| = m+w = q, then there must exist a neighbor r′ in the set N± such that the variable contained in r but not r′ is wrong and the variable contained in r′ but not r is an undiscovered variable of f . In this case, from Equation 10, Perff (r\n′,D)− Perff (r,D) = 22−m−u−w ≥ 22−2q ≥ ǫ2/9. We have shown that whenever Perff (r,D) < 1 − ǫ, there exists an r′ ∈ Neigh(r, ǫ) such that\nPerff (r ′,D)− Perff (r,D) ≥ ǫ2/9, so Neigh is a strictly beneficial neighborhood function."
    }, {
      "heading" : "A.11 Proof of Theorem 18",
      "text" : "We show that Neigh is a strictly beneficial neighborhood function for C, D, and R with benefit polynomial b(n, 1/ǫ) = 9/ǫ2. The theorem is then an immediate consequence of Theorem 8.\nConsider an arbitrary r ∈ Rn and f ∈ Cn. As in the proof of Theorem 17, we start by considering the case in which the target is “long”, that is, |f | ≥ q+ 1, where q = ⌈log2(3/ǫ)⌉. If |r| = q, then as before, Perff (r,D) > 1−21−|f |−21−|r| ≥ 1−ǫ, and the performance of r with respect to f is already good enough. If r does not contain a literal that is a negation of a literal in f , then Equation 10\nholds and just as in the proof of Theorem 17, there must exist a neighbor r′ in the set N+(r) such that Perff (r\n′,D) − Perff (r,D) = 2−|r| > 2−q = ǫ/6. On the other hand, if r does contain at least one literal that is a negation of a literal in f , then f and r are never simultaneously true and so Perff (r,D) = 1−2(2−|f |+2−|r|) = 1−21−|f |−21−|r|. In this case, by a similar argument, a neighbor r′ ∈ N+ has performance 1− 21−|f | − 2−|r|, so Perff (r′,D)− Perff (r,D) = 2−|r| > 2−q ≥ ǫ/6.\nNow consider the case in which f is “short”. If r does not contain a literal that is a negation of a literal in f , then Equation 10 holds and the case-by-case analysis is identical to the analysis in the proof of Theorem 17. Suppose r contains at least one literal that is the negation of a literal in f . In this case, as above, Perff (r,D) = 1 − 21−|f | − 21−|r|. Let r′ ∈ N ′(r) be the conjunction obtained by starting with r and negating all literals in S. From Equation 10, we have that Perff (r\n′,D) ≥ 1− 21−|f | − 21−|r| + 22−|f |−|r|, and so Perff (r′,D)− Perff (r,D) ≥ 22−|f |−|r| ≥ 22−2q ≥ ǫ2/9. The lemma statement follows."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2010,
    "abstractText" : "We consider the question of the stability of evolutionary algorithms to gradual changes,<lb>or drift, in the target concept. We define an algorithm to be resistant to drift if, for<lb>some inverse polynomial drift rate in the target function, it converges to accuracy 1 − ǫ<lb>with polynomial resources, and then stays within that accuracy indefinitely, except with<lb>probability ǫ at any one time. We show that every evolution algorithm, in the sense of<lb>Valiant [19], can be converted using the Correlational Query technique of Feldman [9], into<lb>such a drift resistant algorithm. For certain evolutionary algorithms, such as for Boolean<lb>conjunctions, we give bounds on the rates of drift that they can resist. We develop some<lb>new evolution algorithms that are resistant to significant drift. In particular, we give an<lb>algorithm for evolving linear separators over the spherically symmetric distribution that is<lb>resistant to a drift rate of O(ǫ/n), and another algorithm over the more general product<lb>normal distributions that resists a smaller drift rate. The above translation result can be also interpreted as one on the robustness of the notion of<lb>evolvability itself under changes of definition. As a second result in that direction we show<lb>that every evolution algorithm can be converted to a quasi-monotonic one that can evolve<lb>from any starting point without the performance ever dipping significantly below that of<lb>the starting point. This permits the somewhat unnatural feature of arbitrary performance<lb>degradations to be removed from several known robustness translations.",
    "creator" : "LaTeX with hyperref package"
  }
}