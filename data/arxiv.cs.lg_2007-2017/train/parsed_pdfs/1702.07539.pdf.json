{
  "name" : "1702.07539.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Tight Bounds for Bandit Combinatorial Optimization",
    "authors" : [ "Alon Cohen", "Tamir Hazan", "Tomer Koren" ],
    "emails" : [ "alon.cohen@technion.ac.il", "tamir.hazan@technion.ac.il", "tkoren@google.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n70 2.\n07 53\n9v 1\n[ cs\n.L G\n] 2\n4 Fe\nb 20\n17\n? dT q where d is the dimension of the problem and k is a bound over\nthe maximal instantaneous loss, disproving a conjecture of Audibert, Bubeck, and Lugosi (2013) who argued that the optimal rate should be of the form rΘpk ? dT q. Our bounds apply to several important instances of the framework, and in particular, imply a tight bound for the well-studied bandit shortest path problem. By that, we also resolve an open problem posed by Cesa-Bianchi and Lugosi (2012)."
    }, {
      "heading" : "1 Introduction",
      "text" : "We consider the problem of online combinatorial optimization with bandit feedback, also known as bandit combinatorial optimization, or more succinctly as combinatorial bandits. The problem can be described as the following game between a learner and an environment, that proceeds for T rounds. On each round t “ 1, 2, . . . , T , the learner has to pick, possibly at random, an action xt from a subset S Ď t0, 1ud of the hypercube in d-dimensions, with the property that each element x P S has at exactly k non-zero entries, that is řdi“1 xi “ k. Simultaneously, the environment privately chooses a loss vector ℓt P r0, 1sd. The learner then incurs the loss ℓt ¨ xt P r0, ks and may observe only this loss (but not the vector ℓt) as feedback. The goal of the learner throughout the T rounds of the game is to minimize her regret, defined as\nTÿ\nt“1\nℓt ¨ xt ´ min xPS\nTÿ\nt“1\nℓt ¨ x .\nBandit combinatorial optimization is a fundamental primitive of sequential decision making under uncertainty, and abstracts several major problems in this context (see, e.g., Bubeck et al., 2012b). Perhaps the most important and well-studied problem captured by this framework is online network routing, also known as the online shortest path problem (Takimoto and Warmuth, 2003; Kalai and Vempala, 2005). In this setting, a source station s repeatedly sends communication packets to a target station t through a network represented by a connected directed acyclic graph. On each decision round, the environment associates each edge in the network with a loss, and the learner suffers the loss accumulated over the edges in her chosen path. Each packet can be routed differently and the station has to pick routes so as to minimize the overall amount of time it takes the packets to arrive. In the bandit version of the problem, the only feedback that the source station observes is the roundtrip time of each packet—namely the time it takes the packet to travel to its destination and return to the source.\nThe network routing problem can be cast in the online combinatorial optimization framework as follows: the set of all s-t paths can be represented as a set S Ď t0, 1ud where d is the number of\nedges in the graph, and the non-zero entries in each x P S indicate the edges that are contained in the path x; then, if ℓt P r0, 1sd is the loss vector that associates costs to edges in the network on decision round t, then the cost of path x is given by ℓt ¨ x. The assumption that řd i“1 xi “ k for all x P S means that the length of an s-t path in the network is exactly k (which is also an upper bound on the maximal cost of any s-t path).\nThe study of bandit combinatorial optimization dates back to the work of Awerbuch and Kleinberg (2004), who considered the online shortest path problem in the bandit setting, henceforth called the bandit shortest path problem, in which the learner observes only the loss that she has suffered, and showed an Opkd5{3T 2{3q bound on the expected regret. Dani et al. (2008) and Abernethy et al. (2008) considered the problem in the wider context of bandit linear optimization and established a regret bound with the optimal ? T dependence. Subsequently, Cesa-Bianchi and Lugosi (2012) focused on bandit combinatorial optimization, and showed that a similar bound can be achieved for a large number of problems under this framework, often with computationally efficient algorithms. For the bandit shortest path problem, Cesa-Bianchi and Lugosi (2012) conjectured that the general upper bound is in fact suboptimal and that the correct tight bound is of the form rOpk ? dT q, and could be obtained by a clever adaptation of their algorithm.\nMore recently, Audibert et al. (2013) showed that the aforementioned rOpk3{2 ? dT q upper bound holds for any combinatorial bandit problem using a general online optimization algorithm. Additionally, the authors gave a new lower bound of Ωpk ? dT q on the expected regret in\ncombinatorial bandits, which leaves a gap of ? k between that and their upper bound (ignoring logarithmic factors). They conjectured as well that the lower bound is, in fact, the correct rate and articulated that the upper bound could be improved by non-trivial modifications of the existing algorithmic techniques.\nIn this paper, we revisit the study of optimal regret rates in bandit combinatorial optimization. Our main contribution is in disproving the conjectures of Cesa-Bianchi and Lugosi (2012) and Audibert et al. (2013) and showing that the expected regret of combinatorial bandits in general, and of the bandit shortest path problem in particular, is in fact rΘpk3{2 ? dT q. Namely,\nwe show a new lower bound of rΩpk3{2 ? dT q for combinatorial bandits that matches the best known upper bound up to logarithmic factors, and also holds (via simple adaptations) in the context of bandit shortest path. Furthermore, we show how this lower bound can be adapted to the setting of online ranking (Helmbold and Warmuth, 2009).\nSurprisingly, the construction used in our lower bound is very simple and is based on straightforward adaptations of the one used by Audibert et al. (2013). Furthermore, our analysis is also significantly simpler and shorter than theirs. In a nutshell, the improvement in the bound is obtained via the following observation: when picking its randomized losses for fooling the learner, the environment can choose noisy vectors whose entries are strongly correlated with each other rather than being independent, as is the case in typical lower bound constructions (and, in particular, as suggested by Audibert et al., 2013).1 Since the learner never observes individual entries of the loss vectors and can only see a sum of k of them (for a particular choice of the action set S), she cannot exploit this correlation in any way. On the other hand, with correlated noise terms the observed loss value can have a variance that grows quadratically with k, rather linearly as is the case with i.i.d. noise, which directly deteriorates the learner’s regret by an additional factor of ? k."
    }, {
      "heading" : "1.1 Related work",
      "text" : "Combinatorial bandit optimization is closely related to a somewhat more general online learning scenario known as bandit linear optimization, which was first considered by Dani et al. (2008) and Abernethy et al. (2008). In this setting, the decision set S is not restricted to subsets of\n1Note that the correlation discussed here is between different entries of the same loss vector, rather than between different loss vectors at different rounds. In particular, the loss vectors in our lower bound constructions are still chosen i.i.d. so our bounds also apply to the stochastic i.i.d. case.\nthe hypercube t0, 1ud and may be an arbitrary compact convex set in Rd; instead, the only requirement is that the loss the learner incurs by picking any action in S is bounded (say, by 1 in absolute value) for all possible loss vectors of the environment. State-of-the-art bounds for this problem were obtained by Bubeck et al. (2012a) and Hazan and Karnin (2016), the latter using computationally-efficient algorithms.\nThe general linear optimization setting allows for more general geometries of the sets in which the decisions and the loss vectors reside (e.g., they are typically assumed to be subsets of the Euclidean unit ball), and consequently the bounds obtained in that setting are often not immediately comparable to those in the combinatorial one. In particular, the lower bounds proved by Dani et al. (2008) and more recently by Shamir (2015) hold in the general linear optimization setting (with Euclidean geometry) and do not apply to any natural problem in the combinatorial setting.\nA significant amount of work has been devoted to combinatorial optimization in the closely related semi-bandit feedback model (e.g., György et al., 2007; Kale et al., 2010; Audibert et al., 2013; Neu, 2015; Neu and Bartók, 2016), in which after playing an action xt the learner may observe the individual entries of the loss vector ℓt that correspond to active entries of xt, namely those entries i for which xtpiq “ 1. For example, in the context of the online shortest path problem, instead of observing just the overall cost of the chosen path (as is the case in the bandit setting), the player may observe the individual cost of each edge in that path. In the semi-bandit case, however, the regret of bandit combinatorial optimization is by now well understood, and is known to be of the form Θp ? kdT q; see Audibert et al. (2013) and the references therein. For further and more detailed account on related partial information models and their regret analysis, we refer to the recent survey by Bubeck et al. (2012b)."
    }, {
      "heading" : "2 Main results",
      "text" : "We now state the main results of this paper. As our results are lower bounds on the learner’s regret, we will henceforth focus on oblivious environments, that are required to choose the entire sequence ℓ1, . . . , ℓT before the game begins and thus do not react adaptively to the player’s randomized decisions. (A lower bound for such environments also implies a lower bound for more general adaptive environments.) In this setup, we will give bounds on the expected regret, defined as\nRT “ E « Tÿ\nt“1\nℓt ¨ xt ff\n´ min xPS\nTÿ\nt“1\nℓt ¨ x , (1)\nwhere the expectations are taken over the random choices of the learner. Our first result deals with the general combinatorial bandits setting and shows that if the environment is free to choose any action set S, the regret of the learner can be very large. Our lower bound is attained in the multitask bandit problem, in which a learner is simultaneously trying to solve k instances of the n-armed bandit problem (Auer et al., 2002) with n “ d{k (we assume for simplicity that the latter is an integer). At every round of the game, the learner plays k actions, one in each of the bandit problems, and observes the sum of the losses that correspond with these k actions. Then, the set S of actions is given as follows:\nS “ $ & %x P t0, 1u d : @j P rks jnÿ\ni“pj´1qn`1\nxpiq “ 1 , . - . (2)\nTheorem 1 (multitask MAB). Assume that n ě 2, and let the set of actions S Ď t0, 1ud be as defined in Eq. (2). Any learning algorithm for the multitask bandit problem must incur at least rΩpk3{2 ? dT q expected regret in the worst case.\nThe bound in the theorem hides a factor of log´1{2 T which is an artifact of our construction and is likely to be redundant. Note, however, that up to logarithmic factors the bound is tight and matches the upper bounds of Bubeck et al. (2012a) and Hazan and Karnin (2016).\nThe lower bound of Theorem 1 does not hold for any set S but rather to an instance of the multitask bandit problem. However, as we show in the following results, it still is general enough to imply lower bounds for two important instances of bandit linear optimization. Our next theorem gives a lower bound for the bandit shortest path problem, and shows that even when we limit the action set S to paths in a certain graph, the regret of the learner can still be forced to be large. Formally, given a connected DAG G “ pV,Eq with d edges and two nodes s, t P V , we define the set of actions S Ď t0, 1ud as follows:\nS “ ! x P t0, 1ud : the set te P E : xpeq “ 1u forms an s-t path ) . (3)\nThen, we have the following:\nTheorem 2 (online shortest paths). Assume that k ď d{2. There exists a graph with d edges such that any s-t path has exactly k edges (see Figure 1), for which the action set S is defined as in Eq. (3). Against this graph any online learning algorithm for the bandit shortest path problem must suffer at least rΩpk3{2 ? dT q expected regret in the worst case.\nAgain, the theorem implies that the tight regret rate for bandit shortest path is rΘpk3{2 ? dT q, contrary to what was conjectured in the literature (Cesa-Bianchi and Lugosi, 2012). Our last main result shows a lower bound for the online ranking problem. This problem can be cast as finding a maximum matching in the complete bipartite graph Kk,n, that has d “ kn edges. The set of all of these matchings is represented by the action set S Ď t0, 1ud, and the non-zero entries of every x P S indicate which edges participate in the matching that corresponds with x. Formally,\nS “ $ & %x P t0, 1u d : @j P rks jnÿ\ni“pj´1qn`1\nxpiq “ 1, @l P rns kÿ\ni“1\nxppi´ 1qn ` lq “ 1 , . - . (4)\nTheorem 3 (online ranking). Assume that k ď n{2. Consider the problem of online ranking between k and n elements, whose action set S is defined in Eq. (4). Any bandit learning algorithm for this problem must suffer at least rΩpk3{2 ? dT q expected regret in the worst case."
    }, {
      "heading" : "3 Proofs",
      "text" : ""
    }, {
      "heading" : "3.1 Main result",
      "text" : "In this section we prove Theorem 1. We show a lower bound of rΩpk3{2 ? dT q on the regret of any online learning algorithm applied to an instance of the multitask bandit problem. Surprisingly, the factor ? k improvement is obtained via a simple modification of previous constructions (Audibert et al., 2013). We start by applying Yao’s minimax principle, implying that it suffices to show randomized strategy for the environment that forces any deterministic learning algorithm to suffer rΩpk3{2 ? dT q regret in expectation. We shall construct the environment’s strategy as follows.\nSet ǫ “ σ a\nkd{p4T q. Before the game begins, the environment chooses the best arm in each of the k problems in S uniformly at random, and denote the vector indicating this choice by x‹ P S. At every round t, the environment samples Zt „ N p0, σ2q. Denote the loss generated by environment on round t as L1tpiq “ 1{2´ ǫ ¨ x‹piq ` Zt for i “ 1, 2, . . . , d.\nThe idea behind this construction is as follows. In order to avoid large losses and minimize her regret, the learner has to identify the best arm in each of the k subproblems, namely, to\nrecover x‹. Now, suppose that the losses of each coordinate were sampled independently, and each entry in L1t were to receive an i.i.d. sample of the Gaussian noise. Then the variance of the loss observed by the learner, namely of the random variable Lt ¨ x for any choice of x P S, is of the order of k. On the other hand, because of the correlation between the losses of the different coordinates in the construction above, the variance of the observed loss is of the order of k2. This allows us to gain and additional ? k factor in the lower bound on the regret. Note that crucially, the learner always observes a sum of k random noise terms and can never peek into the individual terms in the sum (this is due to the bandit feedback and the specific structure of the decision set S); hence, the correlation in the noise cannot be exploited by the learner and the increase in the overall variance comes at no price.\nFor the construction above, we have the following lemma.\nLemma 4. Any deterministic player must suffer regret of at least σk3{2 ? dT {8 in expectation against an environment that plays the losses L1 1 , . . . , L1T .\nTo show that Theorem 1 holds we need to show that the learner suffers large regret against an environment that plays losses that are bounded in r0, 1sd. While the losses we have constructed L1 1 , . . . , L1T are unbounded, for the right choice of σ they are bounded with high probability. We now show that this allows us to obtain a lower bound on the regret against an environment that plays losses L1, L2, . . . , LT , such that Ltpiq “ clippL1tpiqq for clippaq “ maxtminta, 1u, 0u.\nTheorem 5. Assume that T ě kd and let σ2 “ 1{p192 ` 96 log T q. Any deterministic player must suffer an expected regret of at least σk3{2 ? dT {16 against an environment that plays the losses L1, . . . , LT .\nThe proof of Theorem 1 is now given by setting the value of σ into the bound in Theorem 5."
    }, {
      "heading" : "3.2 Bandit shortest path",
      "text" : "In this section we show a lower bound for the bandit shortest path problem, proving Theorem 2. Suppose without loss of generality that k and d are even, and that d is a multiple of k. We show a lower bound on the regret by constructing a graph that simulates the multitask bandit problem with k{2 problems of d{k arms each.\nThis graph is shown in Figure 1. The graph consists of d edges and d{2 ` k{2 ` 1 vertices set in k{2 layers. Each layer has an incoming vertex connected to d{k intermediate vertices, all of them connected to the same outgoing vertex. This outgoing vertex is the incoming vertex of the next layer and so forth. Note that to form an s-t path the learner has to pass through exactly one of the d{k vertices in each layer, and therefore every such path has exactly k edges.\nNow, given the losses L1, L2, . . . , LT generated by the environment of Section 3.1, we shall construct an environment for the shortest path problem such that the regret of the learner would be the same as the one in the proof of Theorem 1. Indeed, recall that the loss at coordinates pj´1qd{k`1, . . . , jd{k is associated with the losses of the j’th d{k-armed bandit problem. Then on round t for the j’th layer of the graph, we can set the losses Ltppj ´ 1qd{k` 1q, . . . , Ltpjd{kq to the edges going from the incoming vertex to the intermediate vertices, and a loss of 0 to the edges going from the intermediate vertices to the outgoing vertex.\nTherefore, we have a bijection between any s-t path and a set of k{2 arms in the aforementioned multitask bandit problem, such that the sum of the losses on the edges of the s-t path and the sum of the losses of these k{2 arms are the same. We conclude by invoking Theorem 1 that says that any learner must suffer an expected regret of at least rΩpk3{2 ? dT q, as claimed."
    }, {
      "heading" : "3.3 Online ranking",
      "text" : "In this section we prove Theorem 3 by a similar construction to the one in Section 3.1, for which we present the following random environment.\nSet ǫ “ σ a\nkd{p8T q. Before the game starts, the environment samples a maximum matching in Kk,n unfiromly at random, and denote the vector indicating this choice by x\n‹ P S, for the set S defined in Eq. (4). At every round t, the environment samples Zt „ N p0, σ2q. Denote the loss generated by the environment on round t as L1tpiq “ 1{2´ ǫ ¨x‹piq`Zt for all i “ 1, 2, . . . , d.\nWe have the following lemma.\nLemma 6. Any deterministic player must suffer regret of at least σk3{2 ? dT {8 in expectation against an environment that plays the losses L1 1 , . . . , L1T .\nNow to prove Theorem 3, the result above can be adapted to bounded losses in the same manner as done in Theorem 5."
    }, {
      "heading" : "4 Additional proofs",
      "text" : ""
    }, {
      "heading" : "4.1 Proof of Lemma 4",
      "text" : "Proof. Let us denote by i‹ 1 , . . . , i‹k the locations of the non-zero coordinates of the random variable x‹, arranged in increasing order. We next introduce the random variables T1, . . . , Tk, where each Tj is the number of times the learner played an xt such that xtpi‹j q “ 1. For each x P S, we introduce the notations Px and Ex indicating probability and expectation with respect to the marginal distributions under which x‹ “ x. Then,\nRT “ E « Tÿ\nt“1\nL1t ¨ xt ´min xPS\nTÿ\nt“1\nL1t ¨ x ff\ně E « Tÿ\nt“1\nL1t ¨ xt ´ Tÿ\nt“1\nL1t ¨ x‹ ff\n“ 1 nk\nÿ\nxPS\nEx\n« Tÿ\nt“1\nL1t ¨ xt ´ Tÿ\nt“1\nL1t ¨ x ff\n“ 1 nk\nÿ\nxPS\nǫ ¨ Ex « kÿ\nj“1\npT ´ Tjq ff\n“ ǫ ˜ kT ´ kÿ\nj“1\n1\nnk\nÿ\nxPS\nEx rTjs ¸ , (5)\nand in order to proceed, we need to upper bound ExrTjs for each j. For every x P S and j P rks we introduce a new distribution, which is the same as Px except that the loss of coordinate i‹j is also 1{2 ` Zt. We shall refer to these new laws by Px,´j and Ex,´j. Let λt be the loss observed at time t, and λ\nptq “ pλ1, . . . , λtq be the losses observed up to and including time t. Then, since the sequence λpT q determines the actions of the learner over the entire game, and by Pinsker’s inequality,\nExrTjs ´ Ex,´jrTjs ď T ¨DTV ´ Px,´j ” λpT q ı , Px ” λpT q ı¯\nď T c 1\n2 DKL\n` Px,´j “ λpT q ‰ ››Px “ λpT q ‰˘ . (6)\nMoreover, by the chain rule of KL-divergence, DKL ` Px,´jrλpT qs ››PxrλpT qs ˘ equals\nTÿ\nt“1\nEλpt´1q„Px,´j\n” DKL ´ Px,´j ” λt ˇ̌ ˇλpt´1q ı ›››Px ” λt ˇ̌ ˇλpt´1q ı ı̄ . (7)\nConsider a single term in the sum, and recall that λpt´1q determines the action xt chosen by the learner on round t. If xtpi‹j q “ 0, the loss observed under Px and Px,´j are the same, and the KL divergence is 0. If xtpi‹j q “ 1 then the observed losses under Px and Px,´j are both Gaussian whose means are ǫ apart, and the variance of both of them is σ2k2. Therefore,\nDKL ´ Px,´j ” λt ˇ̌ ˇλpt´1q ı ›››Px ” λt ˇ̌ ˇλpt´1q ı¯ ď ǫ 2\n2k2σ2 .\nPlugging the above back into Eq. (7),\nDKL ´ Px,´j ” λpT q ı ›››Px ” λpT q ı¯ ď Tÿ\nt“1\nPx,´j “ xtpi‹j q “ 1 ‰ ¨ ǫ 2\n2k2σ2 “ ǫ\n2\n2k2σ2 Ex,´jrTjs ,\nand the latter back into Eq. (6), we get ExrTjs ď Ex,´jrTjs ` ǫT {p2kσq ¨ a\nEx,´j rTjs. Next, we need the following lemma that we prove on Section 4.2.\nLemma 7. In the conditions of Lemma 4 and by the construction above, we have\n1\nnk\nÿ\nxPS\nEx,´j rTjs “ T\nn .\nNote that n ě 2 by assumption. Therefore, for all j “ 1, 2, . . . , k,\n1\nnk\nÿ\nxPS\nExrTjs ď 1\nnk\nÿ\nxPS\nEx,´jrTjs ` ǫT 2kσ ¨ 1 nk\nÿ\nxPS\nb Ex,´jrTjs\nď 1 nk\nÿ\nxPS\nEx,´jrTjs ` ǫT\n2kσ\nd 1\nnk\nÿ\nxPS\nEx,´jrTjs\nď T 2 ` ǫT 2σ\nc T\nkd ,\nsince d “ kn. Let us now return to Eq. (5). We can lower bound the regret as\nRT ě ǫ ˜ kT ´ kÿ\nj“1\n˜ T\n2 ` ǫT 2σ\nc T\nkd\n¸¸\n“ ǫkT ˜ 1\n2 ´ ǫ 2σ\nc T\nkd\n¸ .\nFor our choice of ǫ, we get that ǫ{p2σq a T {pkdq is at most 1{4, and so\nRT ě σ c kd\n4T ¨ kT\nˆ 1\n2 ´ 1 4\n˙ “ σ\n8 k3{2\n? dT ,\nas claimed."
    }, {
      "heading" : "4.2 Proof of Lemma 7",
      "text" : "Proof. For any choice i‹ 1 , i‹ 2 , . . . , i‹k, let us denote by xpi‹q the corresponding x‹ P S. Following Audibert et al. (2013), we consider\nÿ\nxPS\nEx,´j rTjs “ ÿ\ni‹ 1 ,...,i‹j´1,i ‹ j`1,...,i ‹ k\nÿ\ni‹j\nExpi‹q,´j rTjs .\nNow, keeping i‹ 1 , . . . , i‹j´1, i ‹ j`1, . . . , i ‹ k fixed the distribution Pxpi‹q,´j is the same for any choice of i‹j and therefore, since at every round of the game the learner must choose exactly one arm in the j’th problem, we must have\nř i‹j Expi‹q,´jrTjs “ T .\nPutting it all together, we obtain\nÿ\ni‹ 1 ,...,i‹j´1,i ‹ j`1,...,i ‹ k\nÿ\ni‹j\nExpi‹q,´j rTjs “ ÿ\ni‹ 1 ,...,i‹j´1,i ‹ j`1,...,i ‹ k\nT “ nk´1T ,\nand thus 1\nnk\nÿ\nxPS\nEx,´j rTjs “ 1 nk nk´1T “ T n ."
    }, {
      "heading" : "4.3 Proof of Theorem 5",
      "text" : "Proof. Let X1,X2, . . . ,XT be the predictions of the learner against an environment that plays L1, L2, . . . , LT , and let R̂T be the regret attained by the learner,\nR̂T “ Tÿ\nt“1\nLt ¨Xt ´min xPS\nTÿ\nt“1\nLt ¨ x .\nAlso define the pretend-regret obtained by playing X1,X2, . . . ,XT against an enivronment that plays L1\n1 , L1 2 , . . . , L1T as\nR̂1T “ Tÿ\nt“1\nL1t ¨Xt ´min xPS\nTÿ\nt“1\nL1t ¨ x .\nNow note that if it happens that at every round t, all coordinates of L1t are between 0 and 1, then R̂T “ R̂1T . Denote this event by E. Then,\nErR̂1T s ď ErR̂T s ` kT ¨ PrEcs “ RT ` kT ¨ PrEcs (8)\nwhere the inequality is true since the regret is at most kT with probability 1. It thus remains to upper bound the probability that E does not occur. We will show that PrEcs ď ǫ{8, which by combining Eq. (8) and Lemma 4 would yield:\nRT ě σk3{2\n? dT\n8 ´ σk\n3{2 ? dT\n16 “ σk\n3{2 ? dT\n16 ,\nas required. Now, for E to occur it suffices that ǫ ď 1{4 and that Zt ď 1{4 for every round t. Since\nǫ “ c σ2kd\n4T ď\nd kd p192` 96 log T qT ď c 1 192 ď 1 4 ,\nby our choice of ǫ and σ and since T ě kd by assumption, we have that the probability PrEcs is upper bounded by the probability that Zt ą 1{4 at some (at least one) round t. Employing the standard tail bound PpZ ą xq ď expp´x2{2σ2q for the normal distribution and the union bound, the latter is bounded by\nT ¨ PrZ1 ą 1{4s ď T exp ˜ ´ 1 2σ2 ˆ 1 4 ˙ 2 ¸\n“ T exp p´p6` 3 log T qq\n“ e´6 1 T 2 .\nTherefore, for the probability that E does not occur to be at most ǫ{8 it suffices to have\n16e´6 1\nT 2 ď\nd 1\np192 ` 96 log T qT .\nRearranging the terms it then suffices to have T 3 ě 0.16 ` 0.08 log T , that holds for any T ě 1."
    }, {
      "heading" : "4.4 Proof of Lemma 6",
      "text" : "Proof. Let us denote by i‹ 1 , . . . , i‹k the locations of the nonzero coordinates of the random variable x‹, arranged in increasing order. We next introduce the random variables T1, . . . , Tk, where each Tj is the number of times the learner played an xt such that xtpi‹j q “ 1. For each x P S, we introduce the notations Px and Ex indicating probability and expectation with respect to the marginal distributions under which x‹ “ x. Then,\nRT “ E « Tÿ\nt“1\nL1t ¨ xt ´min xPS\nTÿ\nt“1\nL1t ¨ x ff\ně ǫ ˜ kT ´ kÿ\nj“1\npn´ kq! n!\nÿ\nxPS\nEx rTjs ¸ , (9)\nand in order to proceed, we need to upper bound ExrTjs for each j. For every x P S and j P rks we introduce a new distribution, which is the same as Px except that the loss of coordinate i‹j is also 1{2`Zt. We shall refer to these new laws by Px,´j and Ex,´j. From now on the proof proceeds similarly to that of Lemma 4, with the exception that Lemma 7 is replaced by the following lemma, whose proof can be found in Section 4.5.\nLemma 8. In the conditions of Lemma 6 and by the construction above, we have\npn ´ kq! n!\nÿ\nxPS\nEx,´j rTjs ď T\nn´ k ` 1 .\nRecall that k ď n{2 by assumption, that in particular implies n ´ k ` 1 ě 2 as well as n´ k ` 1 ě n{2. Therefore, for all j “ 1, 2, . . . , k,\npn´ kq! n!\nÿ\nxPS\nExrTjs ď pn´ kq!\nn!\nÿ\nxPS\nEx,´jrTjs ` ǫT 2kσ ¨ pn´ kq! n!\nÿ\nxPS\nb Ex,´jrTjs\nď pn´ kq! n!\nÿ\nxPS\nEx,´jrTjs ` ǫT\n2kσ\nd pn´ kq!\nn!\nÿ\nxPS\nEx,´jrTjs\nď T 2 ` ǫT 2kσ\nc 2T\nn .\nLet us now return to Eq. (9). Using the fact that n “ d{k, we can lower bound the regret as\nRT ě ǫkT ˜ 1\n2 ´ ǫ σ\nc T\n2kd\n¸ ,\nwhich, by our choice of ǫ, allows us to obtain the desired lower bound."
    }, {
      "heading" : "4.5 Proof of Lemma 8",
      "text" : "Proof. Recall that we sample x‹ uniformly at random from S, the set defined in Eq. (4), and denote by UpSq the uniform distribution over S. Then, recalling the random variables i‹ 1 , i‹ 2 , . . . , i‹k we can compute Ex‹„UpSqEx‹,´j rTjs , by conditioning on i‹\n1 , . . . , i‹j´1, i ‹ j`1, . . . , i ‹ k and taking the outer expectation only over i ‹ j .\nNow, there are exactly n´ k` 1 possible ways to choose i‹j in order to complete a maximal matching. In addition, the distribution Px‹,´j is the same for any possible choice of i ‹ j , and since at every round of the game the learner must choose exactly one position for the j’th element, we must have\nEx‹„UpSq ” Ex‹,´j rTjs ˇ̌ ˇ i‹1 “ i1, . . . , i‹j´1 “ ij´1, i‹j`1 “ ij`1, . . . , i‹k “ ik ı\n“ 1 n´ k ` 1\nÿ\nx‹PS\n1ri‹1“i1,...,i‹j´1“ij´1,i‹j`1“ij`1,...,i‹k“iksEx‹,´jrTjs\nď T n´ k ` 1 ."
    }, {
      "heading" : "5 Conclusion and open problems",
      "text" : "In this paper, we gave a tight characterization of the optimal regret rate in bandit combinatorial optimization and proved that it grows as rΘpk3{2 ? dT q, disproving the conjectures of\nCesa-Bianchi and Lugosi (2012) and Audibert et al. (2013). Our lower bounds apply to important instances of the framework, including the bandit versions of the online shortest path and the online ranking problems.\nAn interesting direction for future work is to explore instance-specific bounds, i.e., bounds that depend on the structure of the specific action set S used by the learner. What are the geometric and combinatorial properties of the set S that dictate the optimal rate of regret in the induces learning problem? In particular, in the specific context of the bandit shortest path problem, what are the graph-theoretic properties of the network that govern the difficulty of the online problem? Even in extremely simple graphs, such as the two-dimensional directed grid over n2 nodes (where the s and t nodes are located in two opposite corners), characterizing the optimal rate of regret remains an open problem. We suspect such problems to be non-trivial already in full-information online combinatorial optimization, but expect the bandit setting to be particularly challenging.\nFor the problem of online ranking, Theorem 3 handles the case of k ˆ n permutations in which k is smaller than n. However, quantifying the rate of regret in the important case of full permutations (i.e., with k “ n) remains an open problem. In particular, is the optimal regret Θpn2 ? T q in this setting?"
    } ],
    "references" : [ {
      "title" : "Competing in the dark: An efficient algorithm for bandit linear optimization",
      "author" : [ "J.D. Abernethy", "E. Hazan", "A. Rakhlin" ],
      "venue" : "In 21st Annual Conference on Learning Theory,",
      "citeRegEx" : "Abernethy et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Abernethy et al\\.",
      "year" : 2008
    }, {
      "title" : "Regret in online combinatorial optimization",
      "author" : [ "J.-Y. Audibert", "S. Bubeck", "G. Lugosi" ],
      "venue" : "Mathematics of Operations Research,",
      "citeRegEx" : "Audibert et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Audibert et al\\.",
      "year" : 2013
    }, {
      "title" : "The nonstochastic multiarmed bandit problem",
      "author" : [ "P. Auer", "N. Cesa-Bianchi", "Y. Freund", "R.E. Schapire" ],
      "venue" : "SIAM journal on computing,",
      "citeRegEx" : "Auer et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Auer et al\\.",
      "year" : 2002
    }, {
      "title" : "Adaptive routing with end-to-end feedback: Distributed learning and geometric approaches",
      "author" : [ "B. Awerbuch", "R.D. Kleinberg" ],
      "venue" : "In Proceedings of the thirty-sixth annual ACM symposium on Theory of computing,",
      "citeRegEx" : "Awerbuch and Kleinberg.,? \\Q2004\\E",
      "shortCiteRegEx" : "Awerbuch and Kleinberg.",
      "year" : 2004
    }, {
      "title" : "Towards minimax policies for online linear optimization with bandit feedback",
      "author" : [ "S. Bubeck", "N. Cesa-Bianchi", "S.M. Kakade", "S. Mannor", "N. Srebro", "R.C. Williamson" ],
      "venue" : "In COLT,",
      "citeRegEx" : "Bubeck et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Bubeck et al\\.",
      "year" : 2012
    }, {
      "title" : "Combinatorial bandits",
      "author" : [ "N. Cesa-Bianchi", "G. Lugosi" ],
      "venue" : "Journal of Computer and System Sciences,",
      "citeRegEx" : "Cesa.Bianchi and Lugosi.,? \\Q2012\\E",
      "shortCiteRegEx" : "Cesa.Bianchi and Lugosi.",
      "year" : 2012
    }, {
      "title" : "The price of bandit information for online optimization",
      "author" : [ "V. Dani", "S.M. Kakade", "T.P. Hayes" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Dani et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Dani et al\\.",
      "year" : 2008
    }, {
      "title" : "The on-line shortest path problem under partial monitoring",
      "author" : [ "A. György", "T. Linder", "G. Lugosi", "G. Ottucsák" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "György et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "György et al\\.",
      "year" : 2007
    }, {
      "title" : "Volumetric spanners: An efficient exploration basis for learning",
      "author" : [ "E. Hazan", "Z. Karnin" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Hazan and Karnin.,? \\Q2016\\E",
      "shortCiteRegEx" : "Hazan and Karnin.",
      "year" : 2016
    }, {
      "title" : "Learning permutations with exponential weights",
      "author" : [ "D.P. Helmbold", "M.K. Warmuth" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Helmbold and Warmuth.,? \\Q2009\\E",
      "shortCiteRegEx" : "Helmbold and Warmuth.",
      "year" : 2009
    }, {
      "title" : "Efficient algorithms for online decision problems",
      "author" : [ "A. Kalai", "S. Vempala" ],
      "venue" : "Journal of Computer and System Sciences,",
      "citeRegEx" : "Kalai and Vempala.,? \\Q2005\\E",
      "shortCiteRegEx" : "Kalai and Vempala.",
      "year" : 2005
    }, {
      "title" : "Non-stochastic bandit slate problems",
      "author" : [ "S. Kale", "L. Reyzin", "R.E. Schapire" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Kale et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Kale et al\\.",
      "year" : 2010
    }, {
      "title" : "First-order regret bounds for combinatorial semi-bandits",
      "author" : [ "G. Neu" ],
      "venue" : "In Proceedings of The 28th Conference on Learning Theory, pages 1360–1375,",
      "citeRegEx" : "Neu.,? \\Q2015\\E",
      "shortCiteRegEx" : "Neu.",
      "year" : 2015
    }, {
      "title" : "Importance weighting without importance weights: An efficient algorithm for combinatorial semi-bandits",
      "author" : [ "G. Neu", "G. Bartók" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Neu and Bartók.,? \\Q2016\\E",
      "shortCiteRegEx" : "Neu and Bartók.",
      "year" : 2016
    }, {
      "title" : "On the complexity of bandit linear optimization",
      "author" : [ "O. Shamir" ],
      "venue" : "In Proceedings of The 28th Conference on Learning Theory, pages 1523–1551,",
      "citeRegEx" : "Shamir.,? \\Q2015\\E",
      "shortCiteRegEx" : "Shamir.",
      "year" : 2015
    }, {
      "title" : "Path kernels and multiplicative updates",
      "author" : [ "E. Takimoto", "M.K. Warmuth" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Takimoto and Warmuth.,? \\Q2003\\E",
      "shortCiteRegEx" : "Takimoto and Warmuth.",
      "year" : 2003
    } ],
    "referenceMentions" : [ {
      "referenceID" : 5,
      "context" : "By that, we also resolve an open problem posed by Cesa-Bianchi and Lugosi (2012).",
      "startOffset" : 50,
      "endOffset" : 81
    }, {
      "referenceID" : 15,
      "context" : "Perhaps the most important and well-studied problem captured by this framework is online network routing, also known as the online shortest path problem (Takimoto and Warmuth, 2003; Kalai and Vempala, 2005).",
      "startOffset" : 153,
      "endOffset" : 206
    }, {
      "referenceID" : 10,
      "context" : "Perhaps the most important and well-studied problem captured by this framework is online network routing, also known as the online shortest path problem (Takimoto and Warmuth, 2003; Kalai and Vempala, 2005).",
      "startOffset" : 153,
      "endOffset" : 206
    }, {
      "referenceID" : 9,
      "context" : "Furthermore, we show how this lower bound can be adapted to the setting of online ranking (Helmbold and Warmuth, 2009).",
      "startOffset" : 90,
      "endOffset" : 118
    }, {
      "referenceID" : 1,
      "context" : "The study of bandit combinatorial optimization dates back to the work of Awerbuch and Kleinberg (2004), who considered the online shortest path problem in the bandit setting, henceforth called the bandit shortest path problem, in which the learner observes only the loss that she has suffered, and showed an Opkd5{3T 2{3q bound on the expected regret.",
      "startOffset" : 73,
      "endOffset" : 103
    }, {
      "referenceID" : 1,
      "context" : "The study of bandit combinatorial optimization dates back to the work of Awerbuch and Kleinberg (2004), who considered the online shortest path problem in the bandit setting, henceforth called the bandit shortest path problem, in which the learner observes only the loss that she has suffered, and showed an Opkd5{3T 2{3q bound on the expected regret. Dani et al. (2008) and Abernethy et al.",
      "startOffset" : 73,
      "endOffset" : 371
    }, {
      "referenceID" : 0,
      "context" : "(2008) and Abernethy et al. (2008) considered the problem in the wider context of bandit linear optimization and established a regret bound with the optimal ? T dependence.",
      "startOffset" : 11,
      "endOffset" : 35
    }, {
      "referenceID" : 0,
      "context" : "(2008) and Abernethy et al. (2008) considered the problem in the wider context of bandit linear optimization and established a regret bound with the optimal ? T dependence. Subsequently, Cesa-Bianchi and Lugosi (2012) focused on bandit combinatorial optimization, and showed that a similar bound can be achieved for a large number of problems under this framework, often with computationally efficient algorithms.",
      "startOffset" : 11,
      "endOffset" : 218
    }, {
      "referenceID" : 0,
      "context" : "(2008) and Abernethy et al. (2008) considered the problem in the wider context of bandit linear optimization and established a regret bound with the optimal ? T dependence. Subsequently, Cesa-Bianchi and Lugosi (2012) focused on bandit combinatorial optimization, and showed that a similar bound can be achieved for a large number of problems under this framework, often with computationally efficient algorithms. For the bandit shortest path problem, Cesa-Bianchi and Lugosi (2012) conjectured that the general upper bound is in fact suboptimal and that the correct tight bound is of the form r Opk ? dT q, and could be obtained by a clever adaptation of their algorithm.",
      "startOffset" : 11,
      "endOffset" : 483
    }, {
      "referenceID" : 0,
      "context" : "(2008) and Abernethy et al. (2008) considered the problem in the wider context of bandit linear optimization and established a regret bound with the optimal ? T dependence. Subsequently, Cesa-Bianchi and Lugosi (2012) focused on bandit combinatorial optimization, and showed that a similar bound can be achieved for a large number of problems under this framework, often with computationally efficient algorithms. For the bandit shortest path problem, Cesa-Bianchi and Lugosi (2012) conjectured that the general upper bound is in fact suboptimal and that the correct tight bound is of the form r Opk ? dT q, and could be obtained by a clever adaptation of their algorithm. More recently, Audibert et al. (2013) showed that the aforementioned r Opk3{2 ? dT q upper bound holds for any combinatorial bandit problem using a general online optimization algorithm.",
      "startOffset" : 11,
      "endOffset" : 711
    }, {
      "referenceID" : 0,
      "context" : "(2008) and Abernethy et al. (2008) considered the problem in the wider context of bandit linear optimization and established a regret bound with the optimal ? T dependence. Subsequently, Cesa-Bianchi and Lugosi (2012) focused on bandit combinatorial optimization, and showed that a similar bound can be achieved for a large number of problems under this framework, often with computationally efficient algorithms. For the bandit shortest path problem, Cesa-Bianchi and Lugosi (2012) conjectured that the general upper bound is in fact suboptimal and that the correct tight bound is of the form r Opk ? dT q, and could be obtained by a clever adaptation of their algorithm. More recently, Audibert et al. (2013) showed that the aforementioned r Opk3{2 ? dT q upper bound holds for any combinatorial bandit problem using a general online optimization algorithm. Additionally, the authors gave a new lower bound of Ωpk ? dT q on the expected regret in combinatorial bandits, which leaves a gap of ? k between that and their upper bound (ignoring logarithmic factors). They conjectured as well that the lower bound is, in fact, the correct rate and articulated that the upper bound could be improved by non-trivial modifications of the existing algorithmic techniques. In this paper, we revisit the study of optimal regret rates in bandit combinatorial optimization. Our main contribution is in disproving the conjectures of Cesa-Bianchi and Lugosi (2012) and Audibert et al.",
      "startOffset" : 11,
      "endOffset" : 1452
    }, {
      "referenceID" : 0,
      "context" : "(2008) and Abernethy et al. (2008) considered the problem in the wider context of bandit linear optimization and established a regret bound with the optimal ? T dependence. Subsequently, Cesa-Bianchi and Lugosi (2012) focused on bandit combinatorial optimization, and showed that a similar bound can be achieved for a large number of problems under this framework, often with computationally efficient algorithms. For the bandit shortest path problem, Cesa-Bianchi and Lugosi (2012) conjectured that the general upper bound is in fact suboptimal and that the correct tight bound is of the form r Opk ? dT q, and could be obtained by a clever adaptation of their algorithm. More recently, Audibert et al. (2013) showed that the aforementioned r Opk3{2 ? dT q upper bound holds for any combinatorial bandit problem using a general online optimization algorithm. Additionally, the authors gave a new lower bound of Ωpk ? dT q on the expected regret in combinatorial bandits, which leaves a gap of ? k between that and their upper bound (ignoring logarithmic factors). They conjectured as well that the lower bound is, in fact, the correct rate and articulated that the upper bound could be improved by non-trivial modifications of the existing algorithmic techniques. In this paper, we revisit the study of optimal regret rates in bandit combinatorial optimization. Our main contribution is in disproving the conjectures of Cesa-Bianchi and Lugosi (2012) and Audibert et al. (2013) and showing that the expected regret of combinatorial bandits in general, and of the bandit shortest path problem in particular, is in fact r Θpk3{2 ? dT q.",
      "startOffset" : 11,
      "endOffset" : 1479
    }, {
      "referenceID" : 0,
      "context" : "(2008) and Abernethy et al. (2008) considered the problem in the wider context of bandit linear optimization and established a regret bound with the optimal ? T dependence. Subsequently, Cesa-Bianchi and Lugosi (2012) focused on bandit combinatorial optimization, and showed that a similar bound can be achieved for a large number of problems under this framework, often with computationally efficient algorithms. For the bandit shortest path problem, Cesa-Bianchi and Lugosi (2012) conjectured that the general upper bound is in fact suboptimal and that the correct tight bound is of the form r Opk ? dT q, and could be obtained by a clever adaptation of their algorithm. More recently, Audibert et al. (2013) showed that the aforementioned r Opk3{2 ? dT q upper bound holds for any combinatorial bandit problem using a general online optimization algorithm. Additionally, the authors gave a new lower bound of Ωpk ? dT q on the expected regret in combinatorial bandits, which leaves a gap of ? k between that and their upper bound (ignoring logarithmic factors). They conjectured as well that the lower bound is, in fact, the correct rate and articulated that the upper bound could be improved by non-trivial modifications of the existing algorithmic techniques. In this paper, we revisit the study of optimal regret rates in bandit combinatorial optimization. Our main contribution is in disproving the conjectures of Cesa-Bianchi and Lugosi (2012) and Audibert et al. (2013) and showing that the expected regret of combinatorial bandits in general, and of the bandit shortest path problem in particular, is in fact r Θpk3{2 ? dT q. Namely, we show a new lower bound of r Ωpk3{2 ? dT q for combinatorial bandits that matches the best known upper bound up to logarithmic factors, and also holds (via simple adaptations) in the context of bandit shortest path. Furthermore, we show how this lower bound can be adapted to the setting of online ranking (Helmbold and Warmuth, 2009). Surprisingly, the construction used in our lower bound is very simple and is based on straightforward adaptations of the one used by Audibert et al. (2013). Furthermore, our analysis is also significantly simpler and shorter than theirs.",
      "startOffset" : 11,
      "endOffset" : 2138
    }, {
      "referenceID" : 5,
      "context" : "1 Related work Combinatorial bandit optimization is closely related to a somewhat more general online learning scenario known as bandit linear optimization, which was first considered by Dani et al. (2008) and Abernethy et al.",
      "startOffset" : 187,
      "endOffset" : 206
    }, {
      "referenceID" : 0,
      "context" : "(2008) and Abernethy et al. (2008). In this setting, the decision set S is not restricted to subsets of Note that the correlation discussed here is between different entries of the same loss vector, rather than between different loss vectors at different rounds.",
      "startOffset" : 11,
      "endOffset" : 35
    }, {
      "referenceID" : 11,
      "context" : "A significant amount of work has been devoted to combinatorial optimization in the closely related semi-bandit feedback model (e.g., György et al., 2007; Kale et al., 2010; Audibert et al., 2013; Neu, 2015; Neu and Bartók, 2016), in which after playing an action xt the learner may observe the individual entries of the loss vector lt that correspond to active entries of xt, namely those entries i for which xtpiq “ 1.",
      "startOffset" : 126,
      "endOffset" : 228
    }, {
      "referenceID" : 1,
      "context" : "A significant amount of work has been devoted to combinatorial optimization in the closely related semi-bandit feedback model (e.g., György et al., 2007; Kale et al., 2010; Audibert et al., 2013; Neu, 2015; Neu and Bartók, 2016), in which after playing an action xt the learner may observe the individual entries of the loss vector lt that correspond to active entries of xt, namely those entries i for which xtpiq “ 1.",
      "startOffset" : 126,
      "endOffset" : 228
    }, {
      "referenceID" : 12,
      "context" : "A significant amount of work has been devoted to combinatorial optimization in the closely related semi-bandit feedback model (e.g., György et al., 2007; Kale et al., 2010; Audibert et al., 2013; Neu, 2015; Neu and Bartók, 2016), in which after playing an action xt the learner may observe the individual entries of the loss vector lt that correspond to active entries of xt, namely those entries i for which xtpiq “ 1.",
      "startOffset" : 126,
      "endOffset" : 228
    }, {
      "referenceID" : 13,
      "context" : "A significant amount of work has been devoted to combinatorial optimization in the closely related semi-bandit feedback model (e.g., György et al., 2007; Kale et al., 2010; Audibert et al., 2013; Neu, 2015; Neu and Bartók, 2016), in which after playing an action xt the learner may observe the individual entries of the loss vector lt that correspond to active entries of xt, namely those entries i for which xtpiq “ 1.",
      "startOffset" : 126,
      "endOffset" : 228
    }, {
      "referenceID" : 3,
      "context" : "State-of-the-art bounds for this problem were obtained by Bubeck et al. (2012a) and Hazan and Karnin (2016), the latter using computationally-efficient algorithms.",
      "startOffset" : 58,
      "endOffset" : 80
    }, {
      "referenceID" : 3,
      "context" : "State-of-the-art bounds for this problem were obtained by Bubeck et al. (2012a) and Hazan and Karnin (2016), the latter using computationally-efficient algorithms.",
      "startOffset" : 58,
      "endOffset" : 108
    }, {
      "referenceID" : 3,
      "context" : "State-of-the-art bounds for this problem were obtained by Bubeck et al. (2012a) and Hazan and Karnin (2016), the latter using computationally-efficient algorithms. The general linear optimization setting allows for more general geometries of the sets in which the decisions and the loss vectors reside (e.g., they are typically assumed to be subsets of the Euclidean unit ball), and consequently the bounds obtained in that setting are often not immediately comparable to those in the combinatorial one. In particular, the lower bounds proved by Dani et al. (2008) and more recently by Shamir (2015) hold in the general linear optimization setting (with Euclidean geometry) and do not apply to any natural problem in the combinatorial setting.",
      "startOffset" : 58,
      "endOffset" : 565
    }, {
      "referenceID" : 3,
      "context" : "State-of-the-art bounds for this problem were obtained by Bubeck et al. (2012a) and Hazan and Karnin (2016), the latter using computationally-efficient algorithms. The general linear optimization setting allows for more general geometries of the sets in which the decisions and the loss vectors reside (e.g., they are typically assumed to be subsets of the Euclidean unit ball), and consequently the bounds obtained in that setting are often not immediately comparable to those in the combinatorial one. In particular, the lower bounds proved by Dani et al. (2008) and more recently by Shamir (2015) hold in the general linear optimization setting (with Euclidean geometry) and do not apply to any natural problem in the combinatorial setting.",
      "startOffset" : 58,
      "endOffset" : 600
    }, {
      "referenceID" : 1,
      "context" : ", 2010; Audibert et al., 2013; Neu, 2015; Neu and Bartók, 2016), in which after playing an action xt the learner may observe the individual entries of the loss vector lt that correspond to active entries of xt, namely those entries i for which xtpiq “ 1. For example, in the context of the online shortest path problem, instead of observing just the overall cost of the chosen path (as is the case in the bandit setting), the player may observe the individual cost of each edge in that path. In the semi-bandit case, however, the regret of bandit combinatorial optimization is by now well understood, and is known to be of the form Θp ? kdT q; see Audibert et al. (2013) and the references therein.",
      "startOffset" : 8,
      "endOffset" : 671
    }, {
      "referenceID" : 1,
      "context" : ", 2010; Audibert et al., 2013; Neu, 2015; Neu and Bartók, 2016), in which after playing an action xt the learner may observe the individual entries of the loss vector lt that correspond to active entries of xt, namely those entries i for which xtpiq “ 1. For example, in the context of the online shortest path problem, instead of observing just the overall cost of the chosen path (as is the case in the bandit setting), the player may observe the individual cost of each edge in that path. In the semi-bandit case, however, the regret of bandit combinatorial optimization is by now well understood, and is known to be of the form Θp ? kdT q; see Audibert et al. (2013) and the references therein. For further and more detailed account on related partial information models and their regret analysis, we refer to the recent survey by Bubeck et al. (2012b).",
      "startOffset" : 8,
      "endOffset" : 857
    }, {
      "referenceID" : 2,
      "context" : "Our lower bound is attained in the multitask bandit problem, in which a learner is simultaneously trying to solve k instances of the n-armed bandit problem (Auer et al., 2002) with n “ d{k (we assume for simplicity that the latter is an integer).",
      "startOffset" : 156,
      "endOffset" : 175
    }, {
      "referenceID" : 4,
      "context" : "Note, however, that up to logarithmic factors the bound is tight and matches the upper bounds of Bubeck et al. (2012a) and Hazan and Karnin (2016).",
      "startOffset" : 97,
      "endOffset" : 119
    }, {
      "referenceID" : 4,
      "context" : "Note, however, that up to logarithmic factors the bound is tight and matches the upper bounds of Bubeck et al. (2012a) and Hazan and Karnin (2016). The lower bound of Theorem 1 does not hold for any set S but rather to an instance of the multitask bandit problem.",
      "startOffset" : 97,
      "endOffset" : 147
    }, {
      "referenceID" : 5,
      "context" : "Again, the theorem implies that the tight regret rate for bandit shortest path is r Θpk3{2 ? dT q, contrary to what was conjectured in the literature (Cesa-Bianchi and Lugosi, 2012).",
      "startOffset" : 150,
      "endOffset" : 181
    }, {
      "referenceID" : 1,
      "context" : "Surprisingly, the factor ? k improvement is obtained via a simple modification of previous constructions (Audibert et al., 2013).",
      "startOffset" : 105,
      "endOffset" : 128
    }, {
      "referenceID" : 1,
      "context" : "Following Audibert et al. (2013), we consider ÿ",
      "startOffset" : 10,
      "endOffset" : 33
    }, {
      "referenceID" : 1,
      "context" : "Cesa-Bianchi and Lugosi (2012) and Audibert et al. (2013). Our lower bounds apply to important instances of the framework, including the bandit versions of the online shortest path and the online ranking problems.",
      "startOffset" : 35,
      "endOffset" : 58
    } ],
    "year" : 2017,
    "abstractText" : "We revisit the study of optimal regret rates in bandit combinatorial optimization—a fundamental framework for sequential decision making under uncertainty that abstracts numerous combinatorial prediction problems. We prove that the attainable regret in this setting grows as r Θpk3{2 ? dT q where d is the dimension of the problem and k is a bound over the maximal instantaneous loss, disproving a conjecture of Audibert, Bubeck, and Lugosi (2013) who argued that the optimal rate should be of the form r Θpk ? dT q. Our bounds apply to several important instances of the framework, and in particular, imply a tight bound for the well-studied bandit shortest path problem. By that, we also resolve an open problem posed by Cesa-Bianchi and Lugosi (2012).",
    "creator" : "LaTeX with hyperref package"
  }
}