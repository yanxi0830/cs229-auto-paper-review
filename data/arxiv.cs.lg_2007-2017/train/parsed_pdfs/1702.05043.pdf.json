{
  "name" : "1702.05043.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Unbiased Online Recurrent Optimization",
    "authors" : [ "Corentin Tallec", "Yann Ollivier" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "On synthetic tasks, UORO is found to overcome several deficiencies of TBPTT. For instance, when a parameter has a positive short-term but negative long-term influence, TBPTT may require truncation lengths substantially larger than the intrinsic temporal range of the interactions, while UORO performs well thanks to the unbiasedness of its gradients."
    }, {
      "heading" : "Introduction",
      "text" : "Current recurrent network training algorithms are ill-suited to online learning via a single pass through long sequences of temporal data. Backpropagation Through Time (BPTT [Jae02]) is well suited to many short training sequences. Its online counterpart, Truncated Backpropagation Through Time, biases learning towards short-time dependencies, 1 and still requires some storage of past inputs and states. Fully online gradient computation methods, such as Real Time Recurrent Learning (RTRL), have been known for quite a while [WZ89] but their computational cost discards them even for moderately-sized networks [Jae02].\nLike our previous NoBackTrack (NBT) algorithm [OTC15], UORO maintains an unbiased approximation of the gradient of the loss with respect to the parameters of the system,\n1 Arguably, TBPTT might still learn some dependencies beyond its truncation range, by a mechanism similar to Echo State Networks (ESN [Jae02]). However, TBPTT’s gradient estimate has a marked bias towards short-term rather than long-term dependencies, as shown in the first experiment of Section 4.\nar X\niv :1\n70 2.\n05 04\n3v 1\n[ cs\n.N E\n] 1\n6 Fe\nb 20\nin a fully streaming fashion. But unlike NBT, UORO can be easily implemented in a blackbox fashion on top of an existing recurrent model in current machine learning software, without delving into the mathematical structure and code of the model.\nPrevious attempts at introducing generic online learning algorithms at reasonable computational cost have resulted in biased gradient estimates. Echo State Networks (ESNs) [JLPS07] simply set to 0 the gradients of recurrent parameters. Others, e.g., [MNM02, Ste04], introduce approaches resembling ESNs, but keep a biased estimate of the recurrent gradients. The original Long Short Term Memory algorithm [HS97] (LSTM now refers to a particular architecture) cuts gradient flows going out of gating units to make gradient computation tractable. Decoupled Neural Interfaces, introduced in [JCO+16], bootstraps truncated gradient estimates using synthetic gradients generated by feedforward neural networks. The algorithm in [MMW02] is based on correlations between one-step gradients and incurred losses, maintained by running a randomized alternative trajectory alongside the standard trajectory; in light of UORO it might be reinterpreted as an approximately unbiased zeroth-order gradient estimate. Generally these approaches lack a strong theoretical backing, except arguably ESNs.\nThe UORO algorithm is presented in Section 2 after a reminder on backpropagation and tangent forward propagation in Section 1. Multi-step UORO is presented in Section 3 and small-scale experimental results are provided in Section 4. An implementation of UORO is available at https://github.com/ctallec/uoro."
    }, {
      "heading" : "1 Notation and tangent forward propagation",
      "text" : "Consider a non-recurrent computational graph (e.g., a feedforward neural network) that computes\nF : Rdata × Rparams → Routput v 7→ F (v)\nTypically, v = (x, θ) with x ∈ Rdata and θ ∈ Rparams . Backpropagation is the right multiplication of a row vector from the output space, δo ∈ Routput, by the Jacobian of F , ∂F/∂v [LBOM96]; we denote it by F.backprop(v, δo) := δo (∂F/∂v). In terms of x and θ, backpropagation produces a pair (δo (∂F/∂x), δo (∂F/∂θ)). It can be efficiently computed via the usual algorithms.\nIn what follows, we will also use tangent forward propagation: the forward propagation of an infinitesimal change of the current value of v, defined as\nF.forwarddiff(v, δv) := lim ε→0 F (v + ε δv)− F (v) ε = (∂F/∂v) δv (1)\nnamely, the left multiplication of a column vector δv from the input space by the Jacobian matrix of F . It can be estimated either numerically with a small ε, or algebraically. It is computationally as costly as ordinary forward propagation (denoted by F.forward(v)).\nIn what follows, the gradient of a scalar loss ` with respect to a column vector θ, denoted by ∂`/∂θ, is a row vector with the same number of elements as θ (this orientation is consistent with the Jacobian matrix ∂`/∂θ, and thus with the chain rule). When v is a column vector and w is a row vector, v ⊗ w denotes the outer product of v and w, that is, the matrix A such that Ai,j = viwj , and w · v = ∑ i viwi denotes the scalar product of w and v."
    }, {
      "heading" : "2 Unbiased Online Recurrent Optimization",
      "text" : "Consider a recurrent model or dynamical system with smooth transition function\nF : Rinput × Rstate × Rparams → Routput × Rstate (x, s, θ) 7→ F (x, s, θ).\ndefining the dynamics (ot+1, st+1) = F (xt+1, st, θ). (2)\nAt each time step, the system incurs a loss\n`t = `(ot, ôt). (3)\nF is decomposed into (Fout, Fstate) which are both smooth functions. Most current recurrent architectures fall into this framework. For instance, LSTMs are easily rephrased in term of F , with st = (ct, ht) and θ the set of all parameters.\nThe UORO algorithm computes an unbiased estimate of the gradient of the loss with respect to θ in a streaming fashion, provided it is possible to backpropagate and tangent forward propagate through one step of the dynamical system, i.e., through the function F .\nThis gradient estimate can then be fed to any stochastic gradient optimizer, such as Adaptative Momentum [KB14] (Adam) or Adaptative Gradient [DHS10]. Vanilla stochastic gradient descent (SGD) and Adam are used hereafter. In Algorithm 1 below, such optimizers are denoted by SGDOpt and the corresponding parameter update given current parameter θ, gradient estimate gt and learning rate ηt is denoted SGDOpt.update(gt, ηt, θ).\nExact online computation of ∂`t/∂θ requires computation of ∂st/∂θ , the derivative of the current state with respect to the parameters. 2 By application of the multivariate chain rule, this quantity evolves as\n∂st+1 ∂θ = ∂Fstate ∂θ (xt+1, st, θ) + ∂Fstate ∂s (xt+1, st, θ) ∂st ∂θ . (4)\nThis is how RTRL [Jae02] updates its gradient estimate. Storing ∂st+1/∂θ requires n × p memory units, where n is the number of recurrent states and p the number of parameters. This is unfeasible for reasonably-sized networks.\n2 ∂st+1/∂θ is not ∂Fstate(xt+1, st, θ)/∂θ. The latter computes the derivative of st+1 for fixed st. The former accounts for changes in the whole state trajectory induced by changes of θ.\nUORO mimics RTRL but drastically reduces its computational and storage complexity. Instead of fully maintaining ∂st/∂θ, UORO only maintains a rank-one unbiased estimate of the form s̃t⊗ θ̃t, with s̃t a column vector of size state and θ̃t a row vector of size params.\nUORO’s update rules, described in Algorithm 1, are\ns̃t+1 ← ρ0 ∂Fstate ∂s (xt+1, st, θ) s̃t + ρ1 ν (5)\nθ̃t+1 ← θ̃t ρ0\n+ ν>\nρ1\n∂Fstate ∂θ (xt+1, st, θ) (6)\nwhere ν is a column vector of random signs of the same dimension as st, and ρ0 and ρ1 are normalizing constants aimed at minimizing variance, specified in Algorithm 1. It is proven in the Appendix that E [ s̃t ⊗ θ̃t ] follows the exact evolution equation (4),\nhence unbiasedness of the gradient estimate ∂st∂θ ≈ s̃t ⊗ θ̃t (Proposition 1). This rank-one approach is based on [OTC15], which performs finer variance reduction. However, [OTC15] involves computing ∂F istate/∂θ for every i ranging over state space indices; these quantities are not easily computed for non-sparse F , or indeed for any complex model F .\nThe gradient of the loss at time t+ 1 with respect to the parameters is estimated as\n∂`t+1 ∂θ = ∂`t+1 ∂ot ∂ot ∂θ = ∂`t+1 ∂ot ( ∂Fout ∂θ (xt+1, st, θ) + ∂Fout ∂s (xt+1, st, θ) ∂st ∂θ ) (7)\nin which we substitute ∂st∂θ ≈ s̃t ⊗ θ̃t. This is unbiased if s̃t ⊗ θ̃t is. This can be computed efficiently by backpropagating ∂`t+1/∂ot once through F (see g̃t in Algorithm 1).\nNote that unbiasedness only holds in the limit of small learning rates; otherwise, computing ∂st+1∂θ online mixes gradients at different values of the learned parameter θ, and the meaning of the statement is more complex [OTC15].\nThe computational complexity of UORO is O(p) per step. The storage complexity (on top of that of the model itself) is O(max(n, p))."
    }, {
      "heading" : "3 Multi-step UORO",
      "text" : "UORO provides an unbiased gradient estimate. However, this estimate comes at the price of injecting noise into the gradient. This requires smaller learning rates.\nTo reduce the noise on short-term gradients, UORO can be used on top of truncated BPTT to correct TBPTT’s gradients and make them unbiased.\nFormally, this just requires applying Algorithm 1 to a new transition function F T which is just T consecutive steps of the original model F . Then, as in TBPTT, the backpropagation operation in Algorithm 1 becomes a backpropagation over the last T steps. The loss\nof one step of F T is the sum of the losses of the last T steps of F , namely `t+Tt+1 := t+T∑ k=t+1 `k.\nAlgorithm 1: One step of UORO (from time t to t+ 1)"
    }, {
      "heading" : "Data:",
      "text" : "- xt+1, ôt+1, st and θ: input, target, recurrent state and parameters - s̃t column vector of size state, θ̃t row vector of size params such that E s̃t ⊗ θ̃t = ∂st/∂θ - SGDOpt and ηt+1: stochastic optimizer and its learning rate"
    }, {
      "heading" : "Result:",
      "text" : "- `t+1, st+1 and θ: loss, recurrent state and updated parameters - s̃t+1 and θ̃t+1 such that E s̃t+1 ⊗ θ̃t+1 = ∂st+1/∂θ - g̃t+1 such that Eg̃t+1 = ∂`t+1/∂θ\nbegin /* compute next state and loss */ (ot+1, st+1) = F.forward(xt+1, st, θ) `t+1 = `(ot+1, ôt+1) /* compute gradient estimate */ (δs, δθ)← Fout.backprop((xt+1, st, θ), ∂`(ot+1, ôt+1)/∂o) g̃t+1 ← (δs · s̃t) θ̃t + δθ /* prepare for reduction */ Draw ν, column vector of random signs ±1 of size state s̃t+1 ← Fstate.forwarddiff((xt+1, st, θ), (0, s̃t, 0)) (_, δθg)← Fstate.backprop((xt+1, st, θ), ν>) /* compute normalizers */\nρ0 ← √\n‖θ̃t‖ ‖s̃t+1‖+ ε + ε , ρ1 ← √ ‖δθg‖ ‖ν‖+ ε + ε with ε = 10 −7\n/* reduce */ s̃t+1 ← ρ0 s̃t+1 + ρ1 ν\nθ̃t+1 ← θ̃t ρ0 + δθg ρ1 /* update θ */ SGDOpt.update(g̃t+1, ηt+1, θ)\nend\nLikewise, the forward tangent propagation is performed through F T . This way, we obtain an unbiased gradient estimate in which the gradients from the last T steps are computed exactly and incur no noise.\nThe resulting algorithm is referred to as UORO-T . Its scaling in T is similar to TBPTTT , both in terms of memory and computation. In the experiments below, UORO-T reduced variance early on, but did not significantly improve later performance.\nThe noise in UORO can also be reduced by using higher-rank gradient estimates (rank-r instead of rank-1), which amounts to maintaining r distinct values of s̃ and θ̃ in Algorithm 1 and averaging the resulting values of g̃. We did not exploit this possibility in the experiments below, although r = 2 visibly reduced variance in preliminary tests."
    }, {
      "heading" : "4 Experiments",
      "text" : "We tested UORO on synthetic cases involving temporal dependencies that TBPTT has difficulty learning, due to short-sightedness or improper balancing of time scales. UORO overcomes those deficiencies and competes with or sometimes largely outperforms TBPTT.\nInfluence balancing. The first test case examplifies learning of a scalar parameter θ which has a positive influence in the short term, but a negative one in the long run. Shortsightedness of truncated algorithms results in abrupt failure, with the parameter exploding in the wrong direction unless the truncation length exceeds the temporal dependency range by a factor of 10 or so.\nConsider the linear dynamics\nst+1 = Ast + (θ, . . . , θ,−θ, . . . ,−θ)> (8)\nwith A a square matrix of size n with that Ai,i = 1/2, Ai,i+1 = 1/2, and 0 elsewhere; θ ∈ R is a scalar parameter. The second term has p positive-θ entries and n−p negative-θ entries. Intuitively, the effect of θ on a unit diffuses to shallower units over time (Fig. 1). Unit i\nonly feels the effect of θ from unit i+ n after n time steps. The loss considered is a target on the shallowest unit s1,\n`t = 1 2(s 1 t − 1)2. (9)\nThe system stabilizes to an equilibrium B (θ, . . . , θ,−θ, . . . ,−θ)> for any starting point, with Bi,j = 2 for j ≥ i and 0 elsewhere. At equilibrium, one checks that ∂s1t /∂θ = 4p− 2n.\nLearning of θ is performed online using vanilla SGD, with the gradient estimate coming either from TBPTT-T with various T , or from UORO. Learning rates are of the form η\n1+ √ t\nwhere t is time and η is a suitable base learning rate. As shown in Fig. 3a, UORO solves the problem while TBPTT-T fails to converge for any learning rate, even for truncations largely above n. Failure is caused by ill balancing of time dependencies: the influence of θ on the loss is estimated with the wrong sign due to truncation. For n = 23 units, with 13 minus signs, TBPTT requires a truncation above 200 to converge.\nNext-character prediction. The next experiment is character-level synthetic text prediction: the goal is to train a recurrent model to predict the t + 1-th character of a text given the first t online, with a single pass on the data sequence.\nA single layer of 64 units, either GRU or LSTM, is used to output a probability vector for the next character. The cross entropy criterion is used to compute the loss.\nTo make plots readable and reduce visual noise, at each time we plot a moving average of approximately √ t previous losses, namely L(t+ 1) := ( 1− 1√\nt\n) L(t) + 1√\nt `t+1.\nOptimization was performed using Adam with β1 = 0.9 and β2 = 0.999 and a decreasing learning rate η = γ\n1+α √ t , with t the number of characters processed. As UORO requires\nsmaller learning rates than TBPTT in order to converge, this favors UORO. UORO often fails to converge with non-decreasing learning rates, due to its stochastic nature.\nDistant brackets dataset (s, k, a). The distant brackets dataset is generated by repeatedly outputting a left bracket, generating s random characters from an alphabet of size a, outputting a right bracket, generating k random characters from the same alphabet,\nrepeating the same first s characters between brackets and finally outputting a line break. A sample is shown in Fig. 2a. The entropy rate of such a sequence is (s+k) log2(a)2s+k+5 bits per character. For this dataset, α = 0.015 and γ = 10−3.\nUORO and TBPTT-4 are compared in this setup, with a TBPTT truncation deliberately shorter than the inherent time range of the data, to illustrate its potential bias if the inherent time range in the data is unknown a priori.\nThe results are given in Fig. 3b. UORO beats TBPTT-4 in the long run, and succeeds in reaching near optimal behaviour with both models. On the other hand, for both LSTM and GRU units, TBPTT-4 displays faster early convergence, as well as lower variance. GRU TBPTT-4 keeps learning more dependencies at a slower rate. LSTM TBPTT-4 remains stuck near the memoryless optimum. LSTMs and GRUs display somewhat different dynamics, both for UORO and TBPTT.\nanbn(k, l) dataset The anbn(k, l) dataset is generated by repeatedly generating a random number n between k and l, outputting a string of n a’s, a line break, n b’s, and a line break. Its entropy rate is log2(k−l+1)k+l+2 bits per character. A sample is given in Fig. 2b.\nPlots for a few particular setups are given in Fig. 4. For this dataset, the learning rates used α = 0.03 and γ = 10−3.\nNumerical results at the end of training are given in Fig. 5. For reference, the true entropy rate is 0.14 bits per character, while the entropy rate of a model that does not understand that the numbers of a’s and b’s coincide would be double, 0.28 bpc.\nTBPTT with truncation 16 converges to the true entropy rate with LSTMs but not with\nGRUs. UORO solves the problem with both LSTMs and GRUs, despite being memoryless. UORO clearly exhibits more noise. Late convergence is somewhat slower with UORO than TBPTT-16 (for LSTMs, since TBPTT-16 does not converge with GRUs): the true entropy rate is reached after about 3 times as many characters, mostly due to UORO noise. Truncating TBPTT to shorter ranges, e.g., TBPTT-2, affects its performance (Fig. 5).\nFor both LSTMs and GRUs, using UORO-T with increased range does not consistently improves the final convergence results, but reduces early noise, as shown in Fig. 4b."
    }, {
      "heading" : "Conclusion",
      "text" : "We introduced UORO, an algorithm for training recurrent neural networks in a streaming, memoryless fashion. UORO is easy to implement, requires as little computation time as TBPTT and copes for TBPTT’s shortsightedness, at the cost of noise injection. UORO\nprovably provides an unbiased estimate of the gradient of the loss, making it theoretically sound for small learning rates. Furthermore, experimental results indicate that the added noise does not unreasonably hurt the learning process, and that UORO is able to solve some problems on which truncated BPTT fails."
    }, {
      "heading" : "A Unbiasedness of gradient estimates: proof",
      "text" : "Proposition 1. Consider the sequence s̃t, θ̃t and g̃t obtained in Algorithm 1 with ηt = 0. Then, for every time t, E [ s̃t ⊗ θ̃t ] = ∂st ∂θ . Consequently, the gradient estimate g̃t+1 satisfies Eg̃t+1 = ∂`t+1 ∂θ .\nProof. By induction, At t = 0, s̃0 = 0, θ̃0 = 0 thus E [ s̃0 ⊗ θ̃0 ] = 0 =\n∂s0 ∂θ . Let t be such that E [ s̃t ⊗ θ̃t ] = ∂st ∂θ . Using the update equations (5), (6) for s̃t+1 and\nθ̃t+1 yields E [ s̃t+1 ⊗ θ̃t+1 ] =E [ ∂Fstate ∂s (xt+1, st, θ) s̃t ⊗ θ̃t ] + E [ ρ1 ρ0 ν ⊗ θ̃t ] +\nE [ ρ0 ρ1 ∂Fstate ∂s (xt+1, st, θ) s̃t ⊗ ν> ∂Fstate ∂θ (xt+1, st, θ) ] + E [ ν ⊗ ν> ∂Fstate ∂θ (xt+1, st, θ) ] = ∂Fstate ∂s (xt+1, st, θ)E [ s̃t ⊗ θ̃t ] + E [ ρ1 ρ0 ν ⊗ θ̃t ] +\n∂Fstate ∂s (xt+1, st, θ)E [ ρ0 ρ1 s̃t ⊗ ν> ] ∂Fstate ∂θ (xt+1, st, θ) + E [ ν ⊗ ν> ] ∂Fstate ∂θ (xt+1, st, θ).\nNow by induction hypothesis, E [ s̃t ⊗ θ̃t ] = ∂st ∂θ . By definition of ν, E [ ν ⊗ ν> ] = I. By\nindependence of ν and ρ1 from θ̃t, s̃t and ρ0, only E [ρ1 ν] and E [ ν>\nρ1\n] are left to evaluate.\nBut the law of ν is symmetric by sign change ν 7→ −ν, and moreover, ρ1(ν) = ρ1(−ν). Therefore by symmetry both E[ρ1 ν] and E[ν>/ρ1] are 0. Therefore,\nE [ s̃t+1 ⊗ θ̃t+1 ] = ∂Fstate ∂s (xt+1, st, θ) ∂st ∂θ + ∂Fstate ∂θ (xt+1, st, θ) = ∂st+1 ∂θ\nhence unbiasedness. The statement for ∂`t+1∂θ follows by inserting ∂st ∂θ = E [ s̃t ⊗ θ̃t ] in (7)."
    } ],
    "references" : [ {
      "title" : "Adaptive subgradient methods for online learning and stochastic optimization",
      "author" : [ "John Duchi", "Elad Hazan", "Yoram Singer" ],
      "venue" : "Technical Report UCB/EECS2010-24, EECS Department,",
      "citeRegEx" : "Duchi et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Duchi et al\\.",
      "year" : 2010
    }, {
      "title" : "Long short-term memory",
      "author" : [ "Sepp Hochreiter", "Jürgen Schmidhuber" ],
      "venue" : "Neural Comput.,",
      "citeRegEx" : "Hochreiter and Schmidhuber.,? \\Q1997\\E",
      "shortCiteRegEx" : "Hochreiter and Schmidhuber.",
      "year" : 1997
    }, {
      "title" : "Tutorial on training recurrent neural networks, covering BPPT, RTRL, EKF and the “echo state network",
      "author" : [ "Herbert Jaeger" ],
      "venue" : null,
      "citeRegEx" : "Jaeger.,? \\Q2002\\E",
      "shortCiteRegEx" : "Jaeger.",
      "year" : 2002
    }, {
      "title" : "Decoupled neural interfaces using synthetic gradients",
      "author" : [ "Max Jaderberg", "Wojciech Marian Czarnecki", "Simon Osindero", "Oriol Vinyals", "Alex Graves", "Koray Kavukcuoglu" ],
      "venue" : null,
      "citeRegEx" : "Jaderberg et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Jaderberg et al\\.",
      "year" : 2016
    }, {
      "title" : "Optimization and Applications of Echo State Networks with Leaky-Integrator Neurons",
      "author" : [ "Herbert Jaeger", "Mantas Lukoševičius", "Dan Popovici", "Udo Siewert" ],
      "venue" : "Neural Networks,",
      "citeRegEx" : "Jaeger et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Jaeger et al\\.",
      "year" : 2007
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik P. Kingma", "Jimmy Ba" ],
      "venue" : "CoRR, abs/1412.6980,",
      "citeRegEx" : "Kingma and Ba.,? \\Q2014\\E",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2014
    }, {
      "title" : "Efficient backprop",
      "author" : [ "Yann LeCun", "Léon Bottou", "Genevieve B. Orr", "Klaus-Robert Müller" ],
      "venue" : "Neural Networks: Tricks of the Trade,",
      "citeRegEx" : "LeCun et al\\.,? \\Q1996\\E",
      "shortCiteRegEx" : "LeCun et al\\.",
      "year" : 1996
    }, {
      "title" : "A Monte Carlo EM approach for partially observable diffusion processes: Theory and applications to neural networks",
      "author" : [ "Javier R. Movellan", "Paul Mineiro", "R.J. Williams" ],
      "venue" : "Neural Comput.,",
      "citeRegEx" : "Movellan et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Movellan et al\\.",
      "year" : 2002
    }, {
      "title" : "Real-time computing without stable states: A new framework for neural computation based on perturbations",
      "author" : [ "Wolfgang Maass", "Thomas Natschläger", "Henry Markram" ],
      "venue" : "Neural Comput.,",
      "citeRegEx" : "Maass et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Maass et al\\.",
      "year" : 2002
    }, {
      "title" : "Training recurrent networks online without backtracking",
      "author" : [ "Yann Ollivier", "Corentin Tallec", "Guillaume Charpiat" ],
      "venue" : "CoRR, abs/1507.07680,",
      "citeRegEx" : "Ollivier et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Ollivier et al\\.",
      "year" : 2015
    }, {
      "title" : "Backpropagation-decorrelation: online recurrent learning with O(N) complexity",
      "author" : [ "Jochen J. Steil" ],
      "venue" : "In Neural Networks,",
      "citeRegEx" : "Steil.,? \\Q2004\\E",
      "shortCiteRegEx" : "Steil.",
      "year" : 2004
    }, {
      "title" : "A learning algorithm for continually running fully recurrent neural networks",
      "author" : [ "Ronald J. Williams", "David Zipser" ],
      "venue" : "Neural Comput.,",
      "citeRegEx" : "Williams and Zipser.,? \\Q1989\\E",
      "shortCiteRegEx" : "Williams and Zipser.",
      "year" : 1989
    } ],
    "referenceMentions" : [ ],
    "year" : 2017,
    "abstractText" : "The novel Unbiased Online Recurrent Optimization (UORO) algorithm allows for online learning of general recurrent computational graphs such as recurrent network models. It works in a streaming fashion and avoids backtracking through past activations and inputs. UORO is a modification of NoBackTrack [OTC15] that bypasses the need for model sparsity and makes implementation easy in current deep learning frameworks, even for complex models. Computationally, UORO is as costly as Truncated Backpropagation Through Time (TBPTT). Contrary to TBPTT, UORO is guaranteed to provide unbiased gradient estimates, and does not favor short-term dependencies. The downside is added noise, requiring smaller learning rates. On synthetic tasks, UORO is found to overcome several deficiencies of TBPTT. For instance, when a parameter has a positive short-term but negative long-term influence, TBPTT may require truncation lengths substantially larger than the intrinsic temporal range of the interactions, while UORO performs well thanks to the unbiasedness of its gradients.",
    "creator" : "LaTeX with hyperref package"
  }
}