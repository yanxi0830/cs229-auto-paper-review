{
  "name" : "1601.07213.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Unifying Adversarial Training Algorithms with Flexible Deep Data Gradient Regularization",
    "authors" : [ "Alexander G. Ororbia II", "Daniel Kifer", "C. Lee Giles" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Deep neural architectures are highly effective at a vast array of tasks, both supervised and unsupervised. However, recently, it has been shown that deep architectures are sensitive to certain kinds of pertubations of the input, which can range from being barely perceptible to quite noticeable (even semi-random noise), as in Nguyen et al. (2014). Samples containing this type of noise are called “adversarial examples” (Szegedy et al., 2013) and can cause a trained network to confidently misclassify its input. While there are a variety of ways to generate adversarial samples, the fastest and most effective approaches in the current literature are based on the idea of using back-propagation to acquire the derivative of the loss with respect to an input image (i.e. the Jacobian) and adding a small multiple of the Jacobian to the image. ar X\niv :1\n60 1.\n07 21\n3v 1\n[ cs\n.L G\n] 2\nEarlier work suggested adding a regularization penalty on the deep Jacobian (Goodfellow et al., 2014; Gu and Rigazio, 2014), but had difficulty in computing the derivative (with respect to the weights) of the Jacobian, which is necessary for gradient-descent based algorithms. Instead, they utilized approximations such as a shallow layerwise Jacobian penalty (Gu and Rigazio, 2014) that is also used for regularizing contractive auto-encoders (Gu and Rigazio, 2014).\nHere we provide an efficient, deterministic back-propagation style algorithm for training with a wide variety of Jacobian penalties. It would seem that the resulting algorithm has the potential for unifying existing approaches for deep adversarial training. In particular, it helps explain some of the newer approaches to adversarial training (Miyato et al., 2015; Huang et al., 2015). These approaches set up an adversarial objective as a constrained optimization problem and then approximate/simplify it using properties that hold for optimal solutions of unconstrained problems. The algorithms then developed approximate optimization (when compared to our algorithms) and can be viewed as regularizations of the deep Jacobian."
    }, {
      "heading" : "2 The DataGrad Framework",
      "text" : "Given a set of loss functions L0,L1, . . . ,Lm and regulariziers R1, . . . ,Rm, consider the following loss function:\nLDG(t,d,Θ) = λ0 L0(t,d,Θ) + λ1R1(J L1(t,d,Θ)) + · · ·+ λmRm(J Lm(t,d,Θ))\nwhere d = (d1, d2, ...dk) is a data sample, t is its corresponding label/target and Θ = {W1,W2, ...,WK} represents the parameters of a K layer neural network. We use J Li to denote the Jacobian of Li (the gradient of Li with respect to d). λ0, λ1, . . . , λm are the weight coefficients of the terms in the DataGrad loss function.\nWe denote the entire dataset as D = {(d(1), t(1)), . . . , (d(n), t(n))}. Following the framework of empirical risk minimization with stochastic gradient descent, the goal is to minimize the objective function:\nn∑ i=1 LDG(t(i),d(i),Θ) = n∑ i=1 λ0 L0(t,d,Θ) + m∑ j=1 λj n∑ i=1 Rj(J Lj(t(i),d(i),Θ))\nby iterating the following parameter updates (here w`ij is the component of Θ representing the weight of the incoming edge to node i of layer ` from node j of layer `− 1):\nw`ij ← w`ij − ηλ0 ∂\n∂w (`) ij [L0(t,d,Θ)]− η m∑ r=1 λr ∂ ∂w (`) ij [Rr(J Lr(t,d,Θ)] (1)\nwhere η is the step-size coefficient."
    }, {
      "heading" : "2.1 The Derivation",
      "text" : "The first update term of Equation 1, ∂\n∂w (`) ij [L0(t,d,Θ)], is provided by standard backpropagation. For the remaining terms, since the Jacobian of the loss also depends on the current weights Θ, we see that\n∂\n∂w (`) ij\n[Rr(J Lr(t,d,Θ)] = ∂Rr(∂ Lr∂d1 , . . . , ∂ Lr ∂dk )\n∂w (`) ij\n= k∑\ns=1\n∂Rr(a1, . . . , ak) ∂as ∂2 Lr(t,d,Θ) ∂w\n(`) ij ∂ds\n(2)\nwhere as is a variable that takes the current value of ∂ Lr∂dk . It turns out that these mixed partial derivatives (with respect to weights and with respect to data) have structural similarities to the Hessian (since derivatives with respect to the data are computed almost exactly the same way as the derivatives with respect to the lowest layer weights). Since exact computation of the Hessian is slow (Bishop, 1992), we would expect that the computation of this matrix of partial derivatives would also be slow. However, it turns out that we do not need to compute the full matrix – we only need this matrix times a vector, and hence we can use ideas reminiscent of fast Hessian multiplication algorithms (Pearlmutter, 1994). At points of continuous differentiability, we have:\nk∑ s=1 ∂Rr(a1, . . . , ak) ∂as ∂2 Lr(t,d,Θ) ∂w (`) ij ∂ds = k∑ s=1 ∂Rr(a1, . . . , ak) ∂as ∂2 Lr(t,d,Θ) ∂ds∂w (`) ij\n= ∂2 L(t,d + φy,Θ)\n∂φ ∂w`ij (3)\nevaluated at the point φ = 0 and direction y = (∂Rr(a1,...,ak) ∂a1 , . . . , ∂Rr(a1,...,ak) ∂ak\n). The outer directional derivative with respect to the scalar φ can be computed using finite differences. Thus, Equations 2 and 3 mean that we can compute the term ∂\n∂w (`) ij [Rr(J Lr(t,d,Θ)] from the stochastic gradient descent update equation (Equation 1) as follows.\n1. Use standard back-propagation to simultaneously compute the vector derivatives ∂ Lr(t,d,Θ)\n∂Θ and ∂ Lr(t,d,Θ) ∂d (note that the latter corresponds to the vector (a1, . . . , as)\nin our derivation).\n2. Analytically determine the gradient of Rr with respect to its immediate inputs. For example, if Rr is the L2 penalty Rr(x1, . . . , xs) = |x1|2 + · · · + |xs|2 then the immediate gradient would be (2x1, . . . , 2xs) and if Rr is the L1 penalty, the immediate gradient would be (sign(x1), . . . , sign(xs)).\n3. Evaluate the immediate gradient of Rr at the vector ∂ Lr(t,d,Θ)∂d . This corresponds to the adversarial direction, as is denoted by y in our derivation.\n4. Form the adversarial example d̂ = d + φy, where y is the result of the previous step and φ is a small constant.\n5. Use a second back-propagation pass (with d̂ as input) to compute ∂ Lr(t,d̂,Θ) ∂Θ and then return the finite difference (\n∂ Lr(t,d̂,Θ) ∂Θ − ∂ Lr(t,d,Θ) ∂Θ\n) /φ"
    }, {
      "heading" : "2.2 The High-level View: Putting it All Together",
      "text" : "At a high level, the loss Lr and regularizer Rr together serve to define an adversarial noise vector y and adversarial example d̂ = d + φy (where φ is a small constant), as explained in the previous section. Different choices of Lr and Rr result in different types of adversarial examples. For example, setting Rr to be the L1 penalty, the resulting adversarial example is the same as the fast gradient sign method of Goodfellow et al. (2014).\nPutting together the components of our finite differences algorithm, the stochastic gradient descent update equation becomes:\nw`ij ← w`ij − ηλ0 ∂ L0(t,d,Θ)\n∂w (`) ij\n− η m∑ r=1 λr φ\n( ∂ Lr(t,x,Θ)\n∂w (`) ij\n∣∣∣ x=xr − ∂ Lr(t,d,Θ) ∂w\n(`) ij\n)\n= w`ij − η ( λ0 − ∑ r λr φ ) ∂ L0(t,d,Θ) ∂w (`) ij − η m∑ r=1 λr φ ∂ Lr(t,x,Θ) ∂w (`) ij ∣∣∣ x=xr\n(4)\nwhere xr is the adversarial example of d resulting from regularizer Rr in conjunction with loss Lr, and the notation ∂ Lr(t,x,Θ)\n∂w (`) ij ∣∣∣ x=xr here specifically means to compute the\nderivative using back-propagation with xr as an input – in other words, xr is not to be treated as a function of Θ (and its components w(`)ij ) when computing this partial derivative."
    }, {
      "heading" : "2.3 Related Work",
      "text" : "Since the recent discovery of adversarial samples (Szegedy et al., 2013), a variety of remedies have been proposed to make neural architectures robust to this problem. A straightforward solution is to simply add adversarial examples during each training round of stochastic gradient descent (Szegedy et al., 2013). This is exactly what Equation 4 specifies, so that post-hoc solution can be justified as a regularization of the data gradient. Subsequent work (Goodfellow et al., 2014) introduced the objective function∑\nd αL(t,d,Θ) + (1 − α)L(t, d̂,Θ), where d̂ is the adversarial version of input d. A gradient-based method would need to compute the derivative with respect tow(`)ij , which is α∂ L(t,d,Θ) ∂w\n(`) ij\n+ (1−α)∂ L(t,d̂,Θ) ∂w\n(`) ij\n+ (1−α)∂ L(t,d̂,Θ) ∂d̂ · d d̂ dw\n(`) ij\n, since the construction of d̂ de-\npends on w(`)ij . Their work approximates the optimization by ignoring the third term, as it is difficult to compute. This approximation then results in an updated equation having the form of Equation 4, and hence actually optimizes the DataGrad objective. Nkland (2015) present a variant where the deep network is trained using back-propagation only on adversarial examples (rather than a mix of adversarial and original examples). Our work shows that this method also optimizes the DataGrad objective where the loss L0 is removed (by setting λ0 = 0).\nBoth Huang et al. (2015) and Miyato et al. (2015) propose to optimize constrained objective functions that can be put in the form minΘ ∑ d maxg(r)≤c f(t,d, r,Θ), where r represents adversarial noise and the constraint g(r) ≤ c puts a bound on the size of the noise. Letting r∗(d,Θ) be the (constrained) optimal value of r for each d and setting of Θ, this is the same as the objective minΘ ∑ d f(t,d, r\n∗(d,Θ),Θ). The derivative of any term in the summation respect to w(`)ij is then equal to\n∂f(t,d, r,Θ)\n∂w (`) ij\n∣∣∣ r=r∗(d,Θ) + ∂f(t,d, r,Θ) ∂r ∣∣∣ r=r∗(d,Θ) · ∂r ∗(d,Θ)\n∂w (`) ij\n(5)\nNow, if r∗(d,Θ) were an unconstrained maximum value of r, then ∂f(t,d,r,Θ) ∂r ∣∣∣ r=r∗(d,Θ) would equal 0 and the second term of Equation 5 would disappear. However, since r∗ is a constrained optimum and the constraint is active, the second term would generally be nonzero. Since the derivative of the constrained optimum is difficult to compute, Huang et al. (2015) and Miyato et al. (2015) opt to approximate/simplify the derivative making the second term disappear (as it would in the unconstrained case). The remaining term in the derivative means that the update equation will be the same as Equation 4 with λ0 = 0 and an appropriate setting of L1 and R1. Thus their approximate optimization algorithm turns out to be optimizing the DataGrad objective.\nFinally, Gu and Rigazio (2014) penalizes the Frobenius norm of the deep Jacobian. However, they do this with a shallow layer-wise approximation. Specifically, they note that shallow contractive auto-encoders optimize the same objective for shallow (1-layer) networks and that the gradient of the Jacobian can be computed analytically in those cases Gu and Rigazio (2014). Thus, Gu and Rigazio (2014) applies this penalty layer by layer (hence it is a penalty on the derivative of each layer with respect to its immediate inputs) and uses this penalty as an approximation to regularizing the deep Jacobian. Since DataGrad does regularize the deep Jacobian, the work of Gu and Rigazio (2014) can also be viewed as an approximation to DataGrad.\nThus, DataGrad provides a unifying view of previously proposed optimizations for training deep architectures that are resilient to adversarial noise."
    }, {
      "heading" : "3 Experimental Results",
      "text" : "By setting λ0 = λ1 = 1 and λj = 0 for j > 1, setting L0 = L1 and either choosing R1 to be either the L1 or L2 penalty, DataGrad would become a regularization algorithm on either the L1 or L2 norm of the Jacobian of the loss L0 1. These two approaches are denoted as DG-L1 and DG-L2, respectively. We are interested in evaluating how this type of regularization compares to traditional L1 and L2 regularization of the network parameters (L1 and L2, respectively). In this setup, DataGrad needs only 2 forward passes and 2 backward passes to perform a weight update (in contrast to 1 forward and 1 backward pass in back-propagation).\nWe implemented several deep sparse rectifier architectures (Glorot et al., 2011) (3 layers), under the various schemes described above, on the permutation-invariant\n1This procedure is explicitly described in Algorithm 1 found in Appendix .1\nMNIST data-set 2, comprised of 60,000 training samples and 10,000 testing samples. 3000 images (randomly sampled without replacement from the training split) were selected as a validation set that was used for tuning architecture meta-parameters via a coarse grid-search. Hyper-parameters and ranges searched included λ = [0.0001, 0.1] and φ = [0.005, 0.1] coefficients for controlling DataGrad, and the L1 = [0.0001, 0.01] and L2 = [0.0001, 0.01] penalty coefficients for controlling the classical regularizers. Mini-batches of size 20 were used to calculate each parameter update in a gradientdescent framework. Image features were gray-scale pixel features of which we normalized to the range of [0, 1]. The learning rate for all models was held fixed at 0.01 (found after some manual experimentation) and we did not use any additional gradient-descent heuristics (i.e., momentum, drop-out, etc.) for simplicity, since we are interested in investigating the effect that the regularizers have on model robustness to adversarial samples.3 We conducted 10 trials for each learning set-up (using a different seeded random initialization in each case) and selected the best model for each using an averaged validation error score before application to the test-sets.\nBeyond simply evaluating original MNIST test-split generalization error, we generate and evaluate performance on a series of adversarial test-sets each comprised of 10,000 samples. Each candidate adversarial sample is generated via the first portion of Algorithm 1 (see Appendix .1), namely the first back-propagation step used to gen-\n2http://yann.lecun.com/exdb/mnist/. 3We note generalization performance could be further improved with additional mechanisms, such as\nan adaptive learning rate, finer-grained meta-parameter tuning, and usage of momentum and drop-out.\nerate the derivative of the loss with respect to model inputs and the application of the appropriate regularizer function to create the adversarial noise. In this procedure, we vary the adversarial noise coefficient φ along the values {0.005, 0.01, 0.05, 0.1}, where noise values corresponds to maximal pixel gains of {∼ 1,∼ 3,∼ 12,∼ 25}. For each coefficient setting, we generate two adversarial sets corresponding to noise generated under bothR1 andR2.\nWe observe that Figures 1 and 2 indicate that a DataGrad-regularized architecture outperforms a non-regularized and classically-regularized one. However, it is more striking and important to note that as the φ factor is raised, the non-regularized model’s performance quickly and dramatically degrades. Classical L1 and L2 regularizers appear to help in mitigating the damage, but seemingly only afford minimal robustness to adversarial perturbation. In contrast, the proposed DG-L1 and DG-L2 regularizers appear to yield a significant reduction in error on the various adversarial test-sets, the improvement clearer as the amount of noise is increased. The visualization of some adversarial samples (Figure 2) demonstrates that while there is clearly more noise applied to generate adversarials with a higher noise factor φ, the sample itself is still quite recognizable to the human eye. However, a neural architecture, such as a deep rectifier network, that is sensitive to adversarial noise incorrectly classifies these images. Interestingly enough, we also observe improved classification error on the original test-set when using the DataGrad procedure, the DG-L1 variant offering the lowest error of all.\nConclusion To create an deep neural architecture more robust to adversarial examples, we propose the DataGrad learning procedure that regularizes a deep Jacobian-based penalty. More importantly, our general formulation unifies previous proposals for adversarial training\nof neural architectures. Empirically, we found that this regularized form of learning not only significantly reduces error in classifying adversarial samples but even yields improvements in generalization ability. We postulate that one reason for this is that adversarial samples generated during the DataGrad learning phase potentially cover more of the underlying data manifold (or minimally yields benefits similar to syntheticdata-based training).\nSince DataGrad is effectively a “deep” data-driven penalty, it may be used in tandem with most training objective functions (whether supervised, unsupervised Bengio et al. (2007), or hybrid Ororbia II et al. (2015)). Future work entails improving the efficiency of the proposed DataGrad back-propagation procedure (i.e., cutting down on the number of passes to gather necessary statistics) and investigating our procedure in wider variety of settings."
    } ],
    "references" : [ {
      "title" : "Greedy Layer-wise Training of Deep Networks",
      "author" : [ "Y. Bengio", "P. Lamblin", "D. Popovici", "H. Larochelle" ],
      "venue" : "Advances in Neural Information Processing Systems.",
      "citeRegEx" : "Bengio et al\\.,? 2007",
      "shortCiteRegEx" : "Bengio et al\\.",
      "year" : 2007
    }, {
      "title" : "Exact computation of the hessian matrix for the multi-layer perceptron",
      "author" : [ "C.M. Bishop" ],
      "venue" : "Neural Computation, 4(4):494–501.",
      "citeRegEx" : "Bishop,? 1992",
      "shortCiteRegEx" : "Bishop",
      "year" : 1992
    }, {
      "title" : "Deep sparse rectifier networks",
      "author" : [ "X. Glorot", "A. Bordes", "Y. Bengio" ],
      "venue" : "Proc. 14th International Conference on Artificial Intelligence and Statistics, volume 15, pages 315–323.",
      "citeRegEx" : "Glorot et al\\.,? 2011",
      "shortCiteRegEx" : "Glorot et al\\.",
      "year" : 2011
    }, {
      "title" : "Explaining and harnessing adversarial examples",
      "author" : [ "I.J. Goodfellow", "J. Shlens", "C. Szegedy" ],
      "venue" : "http://arxiv.org/abs/1412.6572.",
      "citeRegEx" : "Goodfellow et al\\.,? 2014",
      "shortCiteRegEx" : "Goodfellow et al\\.",
      "year" : 2014
    }, {
      "title" : "Towards deep neural network architectures robust to adversarial examples",
      "author" : [ "S. Gu", "L. Rigazio" ],
      "venue" : "http://arxiv.org/abs/1412.5068.",
      "citeRegEx" : "Gu and Rigazio,? 2014",
      "shortCiteRegEx" : "Gu and Rigazio",
      "year" : 2014
    }, {
      "title" : "Learning with a strong adversary",
      "author" : [ "R. Huang", "B. Xu", "D. Schuurmans", "C. Szepesvari" ],
      "venue" : "arXiv:1511.03034 [cs]. http://arxiv.org/abs/1511.03034.",
      "citeRegEx" : "Huang et al\\.,? 2015",
      "shortCiteRegEx" : "Huang et al\\.",
      "year" : 2015
    }, {
      "title" : "Distributional smoothing with virtual adversarial training",
      "author" : [ "T. Miyato", "Maeda", "S.-i.", "M. Koyama", "K. Nakae", "S. Ishii" ],
      "venue" : "http://arxiv.org/abs/1507. 00677.",
      "citeRegEx" : "Miyato et al\\.,? 2015",
      "shortCiteRegEx" : "Miyato et al\\.",
      "year" : 2015
    }, {
      "title" : "Deep neural networks are easily fooled: High confidence predictions for unrecognizable images",
      "author" : [ "A. Nguyen", "J. Yosinski", "J. Clune" ],
      "venue" : "http://arxiv.org/ abs/1412.1897.",
      "citeRegEx" : "Nguyen et al\\.,? 2014",
      "shortCiteRegEx" : "Nguyen et al\\.",
      "year" : 2014
    }, {
      "title" : "Improving back-propagation by adding an adversarial gradient",
      "author" : [ "A. Nkland" ],
      "venue" : "arXiv:1510.04189 [cs, stat]. http://arxiv.org/abs/1510.04189.",
      "citeRegEx" : "Nkland,? 2015",
      "shortCiteRegEx" : "Nkland",
      "year" : 2015
    }, {
      "title" : "Online learning of deep hybrid architectures for semi-supervised categorization",
      "author" : [ "A.G. Ororbia II", "D. Reitter", "J. Wu", "C.L. Giles" ],
      "venue" : "Machine Learning and Knowledge Discovery in Databases (Proceedings, ECML PKDD 2015), volume 9284 of Lecture Notes in Computer Science, pages 516–532. Springer, Porto, Portugal.",
      "citeRegEx" : "II et al\\.,? 2015",
      "shortCiteRegEx" : "II et al\\.",
      "year" : 2015
    }, {
      "title" : "Fast exact multiplication by the hessian",
      "author" : [ "B.A. Pearlmutter" ],
      "venue" : "Neural Computa-",
      "citeRegEx" : "Pearlmutter,? 1994",
      "shortCiteRegEx" : "Pearlmutter",
      "year" : 1994
    }, {
      "title" : "Intriguing properties of neural networks. http://arxiv.org",
      "author" : [ "R. gus" ],
      "venue" : null,
      "citeRegEx" : "gus,? \\Q2013\\E",
      "shortCiteRegEx" : "gus",
      "year" : 2013
    } ],
    "referenceMentions" : [ {
      "referenceID" : 7,
      "context" : "However, recently, it has been shown that deep architectures are sensitive to certain kinds of pertubations of the input, which can range from being barely perceptible to quite noticeable (even semi-random noise), as in Nguyen et al. (2014). Samples containing this type of noise are called “adversarial examples” (Szegedy et al.",
      "startOffset" : 220,
      "endOffset" : 241
    }, {
      "referenceID" : 3,
      "context" : "Earlier work suggested adding a regularization penalty on the deep Jacobian (Goodfellow et al., 2014; Gu and Rigazio, 2014), but had difficulty in computing the derivative (with respect to the weights) of the Jacobian, which is necessary for gradient-descent based algorithms.",
      "startOffset" : 76,
      "endOffset" : 123
    }, {
      "referenceID" : 4,
      "context" : "Earlier work suggested adding a regularization penalty on the deep Jacobian (Goodfellow et al., 2014; Gu and Rigazio, 2014), but had difficulty in computing the derivative (with respect to the weights) of the Jacobian, which is necessary for gradient-descent based algorithms.",
      "startOffset" : 76,
      "endOffset" : 123
    }, {
      "referenceID" : 4,
      "context" : "Instead, they utilized approximations such as a shallow layerwise Jacobian penalty (Gu and Rigazio, 2014) that is also used for regularizing contractive auto-encoders (Gu and Rigazio, 2014).",
      "startOffset" : 83,
      "endOffset" : 105
    }, {
      "referenceID" : 4,
      "context" : "Instead, they utilized approximations such as a shallow layerwise Jacobian penalty (Gu and Rigazio, 2014) that is also used for regularizing contractive auto-encoders (Gu and Rigazio, 2014).",
      "startOffset" : 167,
      "endOffset" : 189
    }, {
      "referenceID" : 6,
      "context" : "In particular, it helps explain some of the newer approaches to adversarial training (Miyato et al., 2015; Huang et al., 2015).",
      "startOffset" : 85,
      "endOffset" : 126
    }, {
      "referenceID" : 5,
      "context" : "In particular, it helps explain some of the newer approaches to adversarial training (Miyato et al., 2015; Huang et al., 2015).",
      "startOffset" : 85,
      "endOffset" : 126
    }, {
      "referenceID" : 1,
      "context" : "Since exact computation of the Hessian is slow (Bishop, 1992), we would expect that the computation of this matrix of partial derivatives would also be slow.",
      "startOffset" : 47,
      "endOffset" : 61
    }, {
      "referenceID" : 10,
      "context" : "However, it turns out that we do not need to compute the full matrix – we only need this matrix times a vector, and hence we can use ideas reminiscent of fast Hessian multiplication algorithms (Pearlmutter, 1994).",
      "startOffset" : 193,
      "endOffset" : 212
    }, {
      "referenceID" : 3,
      "context" : "For example, setting Rr to be the L1 penalty, the resulting adversarial example is the same as the fast gradient sign method of Goodfellow et al. (2014). Putting together the components of our finite differences algorithm, the stochastic gradient descent update equation becomes:",
      "startOffset" : 128,
      "endOffset" : 153
    }, {
      "referenceID" : 3,
      "context" : "Subsequent work (Goodfellow et al., 2014) introduced the objective function ∑ d αL(t,d,Θ) + (1 − α)L(t, d̂,Θ), where d̂ is the adversarial version of input d.",
      "startOffset" : 16,
      "endOffset" : 41
    }, {
      "referenceID" : 3,
      "context" : "Subsequent work (Goodfellow et al., 2014) introduced the objective function ∑ d αL(t,d,Θ) + (1 − α)L(t, d̂,Θ), where d̂ is the adversarial version of input d. A gradient-based method would need to compute the derivative with respect tow ij , which is α L(t,d,Θ) ∂w (`) ij + (1−α) L(t,d̂,Θ) ∂w (`) ij + (1−α) L(t,d̂,Θ) ∂d̂ · d d̂ dw (`) ij , since the construction of d̂ depends on w ij . Their work approximates the optimization by ignoring the third term, as it is difficult to compute. This approximation then results in an updated equation having the form of Equation 4, and hence actually optimizes the DataGrad objective. Nkland (2015) present a variant where the deep network is trained using back-propagation only on adversarial examples (rather than a mix of adversarial and original examples).",
      "startOffset" : 17,
      "endOffset" : 641
    }, {
      "referenceID" : 5,
      "context" : "Both Huang et al. (2015) and Miyato et al.",
      "startOffset" : 5,
      "endOffset" : 25
    }, {
      "referenceID" : 5,
      "context" : "Both Huang et al. (2015) and Miyato et al. (2015) propose to optimize constrained objective functions that can be put in the form minΘ ∑ d maxg(r)≤c f(t,d, r,Θ), where r represents adversarial noise and the constraint g(r) ≤ c puts a bound on the size of the noise.",
      "startOffset" : 5,
      "endOffset" : 50
    }, {
      "referenceID" : 4,
      "context" : "Since the derivative of the constrained optimum is difficult to compute, Huang et al. (2015) and Miyato et al.",
      "startOffset" : 73,
      "endOffset" : 93
    }, {
      "referenceID" : 4,
      "context" : "Since the derivative of the constrained optimum is difficult to compute, Huang et al. (2015) and Miyato et al. (2015) opt to approximate/simplify the derivative making the second term disappear (as it would in the unconstrained case).",
      "startOffset" : 73,
      "endOffset" : 118
    }, {
      "referenceID" : 4,
      "context" : "Finally, Gu and Rigazio (2014) penalizes the Frobenius norm of the deep Jacobian.",
      "startOffset" : 9,
      "endOffset" : 31
    }, {
      "referenceID" : 4,
      "context" : "Finally, Gu and Rigazio (2014) penalizes the Frobenius norm of the deep Jacobian. However, they do this with a shallow layer-wise approximation. Specifically, they note that shallow contractive auto-encoders optimize the same objective for shallow (1-layer) networks and that the gradient of the Jacobian can be computed analytically in those cases Gu and Rigazio (2014). Thus, Gu and Rigazio (2014) applies this penalty layer by layer (hence it is a penalty on the derivative of each layer with respect to its immediate inputs) and uses this penalty as an approximation to regularizing the deep Jacobian.",
      "startOffset" : 9,
      "endOffset" : 371
    }, {
      "referenceID" : 4,
      "context" : "Finally, Gu and Rigazio (2014) penalizes the Frobenius norm of the deep Jacobian. However, they do this with a shallow layer-wise approximation. Specifically, they note that shallow contractive auto-encoders optimize the same objective for shallow (1-layer) networks and that the gradient of the Jacobian can be computed analytically in those cases Gu and Rigazio (2014). Thus, Gu and Rigazio (2014) applies this penalty layer by layer (hence it is a penalty on the derivative of each layer with respect to its immediate inputs) and uses this penalty as an approximation to regularizing the deep Jacobian.",
      "startOffset" : 9,
      "endOffset" : 400
    }, {
      "referenceID" : 4,
      "context" : "Finally, Gu and Rigazio (2014) penalizes the Frobenius norm of the deep Jacobian. However, they do this with a shallow layer-wise approximation. Specifically, they note that shallow contractive auto-encoders optimize the same objective for shallow (1-layer) networks and that the gradient of the Jacobian can be computed analytically in those cases Gu and Rigazio (2014). Thus, Gu and Rigazio (2014) applies this penalty layer by layer (hence it is a penalty on the derivative of each layer with respect to its immediate inputs) and uses this penalty as an approximation to regularizing the deep Jacobian. Since DataGrad does regularize the deep Jacobian, the work of Gu and Rigazio (2014) can also be viewed as an approximation to DataGrad.",
      "startOffset" : 9,
      "endOffset" : 690
    }, {
      "referenceID" : 2,
      "context" : "We implemented several deep sparse rectifier architectures (Glorot et al., 2011) (3 layers), under the various schemes described above, on the permutation-invariant 1This procedure is explicitly described in Algorithm 1 found in Appendix .",
      "startOffset" : 59,
      "endOffset" : 80
    }, {
      "referenceID" : 0,
      "context" : "Since DataGrad is effectively a “deep” data-driven penalty, it may be used in tandem with most training objective functions (whether supervised, unsupervised Bengio et al. (2007), or hybrid Ororbia II et al.",
      "startOffset" : 158,
      "endOffset" : 179
    }, {
      "referenceID" : 0,
      "context" : "Since DataGrad is effectively a “deep” data-driven penalty, it may be used in tandem with most training objective functions (whether supervised, unsupervised Bengio et al. (2007), or hybrid Ororbia II et al. (2015)).",
      "startOffset" : 158,
      "endOffset" : 215
    } ],
    "year" : 2017,
    "abstractText" : "We present DataGrad, a general back-propagation style training procedure for deep neural architectures that uses regularization of a deep Jacobian-based penalty. It can be viewed as a deep extension of the layerwise contractive auto-encoder penalty. More importantly, it unifies previous proposals for adversarial training of deep neural nets – this list includes directly modifying the gradient, training on a mix of original and adversarial examples, using contractive penalties, and approximately optimizing constrained adversarial objective functions. In an experiment using a Deep Sparse Rectifier Network, we find that the deep Jacobian regularization of DataGrad (which also has L1 and L2 flavors of regularization) outperforms traditional L1 and L2 regularization both on the original dataset as well as on adversarial examples.",
    "creator" : "LaTeX with hyperref package"
  }
}