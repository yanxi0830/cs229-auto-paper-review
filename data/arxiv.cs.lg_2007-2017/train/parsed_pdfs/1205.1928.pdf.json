{
  "name" : "1205.1928.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n20 5.\n19 28\nv3 [\nm at\nh. FA\nA family of regularization functionals is said to admit a linear representer theorem if every member of the family admits minimizers that lie in a fixed finite dimensional subspace. A recent characterization states that a general class of regularization functionals with differentiable regularizer admits a linear representer theorem if and only if the regularization term is a non-decreasing function of the norm. In this report, we improve over such result by replacing the differentiability assumption with lower semicontinuity and deriving a proof that is independent of the dimensionality of the space."
    }, {
      "heading" : "1 Introduction",
      "text" : "Tikhonov regularization [13] is a popular and well-studied methodology to address ill-posed estimation problems [15], and learning from examples [4]. In this report, we focus on regularization problems defined over a real Hilbert space H. A Hilbert space is a vector space endowed with a inner product and a norm that is complete1. Such setting is general enough to take into account a broad family of finite-dimensional regularization techniques such as regularized least squares or support vector machines for classification or regression, kernel principal component analysis, as well as a variety of regularization problems defined over infinite-dimensional reproducing kernel Hilbert spaces (RKHS).\nIn general, we study the problem of minimizing an extended real-valued functional J : H → R ∪ {+∞} of the form\nJ(w) = f(L1w, . . . , Lℓw) + Ω(w), (1)\nwhere L1, . . . , Lℓ are bounded (continuous) linear functionals on H. The functional J is the sum of an error term f , which typically depends on empirical\n1Meaning that Cauchy sequences are convergent.\ndata, and a regularizer Ω that enforces certain desirable properties on the solution. By allowing the functional J to take the value +∞, problems with hard constraints on the values Liw are included in the framework.\nIn machine learning, the most common class of regularization problems concerns a situation where a set of data pairs (xi, yi) is available, H is a space of real-valued functions, and the objective functional to be minimized is of the form\nJ(w) = c ((x1, y1, w(x1)), · · · , (xℓ, yℓ, w(xℓ)) + Ω(w).\nIt is easy to see that this setting is a particular case of (1). Indeed, the dependence on the data pairs (xi, yi) can be absorbed into the definition of f , and Li are point-wise evaluation functionals, i.e. such that Liw = w(xi). Several popular techniques can be cast in such regularization framework.\nExample 1 (Regularized least squares). Also known as ridge regression when H is finite-dimensional. Corresponds to the choice\nc ((x1, y1, w(x1)), · · · , (xℓ, yℓ, w(xℓ)) = γ ℓ∑\ni=1\n(yi − w(xi)) 2,\nand Ω(w) = ‖w‖2, where the complexity parameter γ ≥ 0 controls the trade-off between fitting of training data and regularity of the solution.\nExample 2 (Support vector machine). Given binary labels yi = ±1, the SVM classifier can be interpreted as a regularization method corresponding to the choice\nc ((x1, y1, w(x1)), · · · , (xℓ, yℓ, w(xℓ)) = γ\nℓ∑\ni=1\nmax{0, 1− yiw(xi)},\nand Ω(w) = ‖w‖2. The hard-margin SVM can be recovered by letting γ → +∞.\nExample 3 (Kernel principal component analysis). Kernel PCA can be shown to be equivalent to a regularization problem where\nc ((x1, y1, w(x1)), · · · , (xℓ, yℓ, w(xℓ)) =\n{ 0, 1\nℓ ∑ℓ i=1 ( w(xi)− 1 ℓ ∑ℓ j=1 w(xj) )2 = 1\n+∞, otherwise ,\nand Ω is any strictly monotonically increasing function of the norm ‖w‖ [11]. In this problem, there are no labels yi, but the feature extractor function w is constrained to produce vectors with unitary empirical variance.\nWithin the formulation (1), the possibility of using general continuous linear functionals Li allows to consider a much broader class of regularization problems.\nExample 4 (Tikhonov deconvolution). Given a input signal u, assume that the convolution u ∗ w is well-defined for any w ∈ H, and the point-wise evaluated convolution functionals\nLiw = (u ∗ w)(xi) =\n∫\nX\nu(s)w(xi − s)ds,\nare continuous. A possible way to recover w from noisy measurements yi of the “output signal” is to solve regularization problems such as\nmin w∈H\n( γ ℓ∑\ni=1\n(yi − (u ∗ w)(xi)) 2 + ‖w‖2 ) ,\nwhere the objective functional is of the form (1).\nExample 5 (Learning from probability measures). In many classical learning problems, it is appropriate to represent input training data as probability distributions instead of single points. Given a finite set of probability measures Pi on a measurable space (X ,A), where A is a σ-algebra of subsets of X , introduce the expectations\nLiw = EPi(w) =\n∫\nX\nw(x)dPi(x).\nThen, given output labels yi, one can learn a input-output relationship by solving regularization problems of the form\nmin w∈H\n( c ((y1, EP1(w)), · · · , (yℓ, EPℓ(w)) + ‖w‖ 2 ) .\nIf the expectations are bounded linear functionals, such regularization functional is of the form (1).\nExample 6 (Ivanov regularization). By allowing the regularizer Ω to take the value +∞, we can also take into account the whole class of Ivanov-type regularization problems of the form\nmin w∈H f(L1w, . . . , Lℓw), subject to φ(w) ≤ 1,\nby reformulating them as the minimization of a functional of the type (1), where\nΩ(w) = { 0, φ(w) ≤ 1 +∞, otherwise .\nLet’s now go back to the general formulation (1). By the Riesz representation theorem [8, 5], J can be rewritten as\nJ(w) = f(〈w,w1〉, . . . , 〈w,wℓ〉) + Ω(w),\nwhere wi is the representer of the linear functional Li with respect to the inner product. Consider the following definition.\nDefinition 1. A family F of regularization functionals of the form (1) is said to admit a linear representer theorem if, for any J ∈ F , and any choice of bounded linear functionals Li, there exists a minimizer w\n∗ that can be written as a linear combination of the representers:\nw∗ =\nℓ∑\ni=1\nciwi.\nIf a linear representer theorem holds, the regularization problem boils down to a ℓ-dimensional optimization problem on the scalar coefficients ci. This property is important in practice, since it allows to employ numerical optimization techniques to compute a solution, independently of the dimension of H. Sufficient conditions under which a family of functionals admits a representer theorem have been widely studied in the literature of statistics, inverse problems, and machine learning. The theorem also provides the foundations of learning techniques such as regularized kernel methods and support vector machines, see [14, 10, 12] and references therein.\nRepresenter theorems are of particular interest when H is a reproducing kernel Hilbert space (RKHS) [2]. Given a non-empty set X , a RKHS is a space of functions w : X → R such that point-wise evaluation functionals are bounded, namely, for any x ∈ X , there exists a non-negative real number Cx such that\n|w(x)| ≤ Cx‖w‖, ∀w ∈ H.\nIt can be shown that a RKHS can be uniquely associated to a positive-semidefinite kernel function K : X ×X → R (called reproducing kernel), such that so-called reproducing property holds:\nw(x) = 〈w,Kx〉, ∀ (x,w) ∈ X ×H,\nwhere the kernel sections Kx are defined as\nKx(y) = K(x, y), ∀y ∈ X .\nThe reproducing property states that the representers of point-wise evaluation functionals coincide with the kernel sections. Starting from the reproducing property, it is also easy to show that the representer of any bounded linear functional L is given by a function KL ∈ H such that\nKL(x) = LKx, ∀x ∈ X .\nTherefore, in a RKHS, the representer of any bounded linear functional can be obtained explicitly in terms of the reproducing kernel.\nIf the regularization functional (1) admits minimizers, and the regularizer Ω is a nondecreasing function of the norm, i.e.\nΩ(w) = h(‖w‖), with h : R → R ∪ {+∞}, nondecreasing, (2)\nthe linear representer theorem follows easily from the Pythagorean identity. A proof that the condition (2) is sufficient appeared in [9] in the case where H is a RKHS and Li are point-wise evaluation functionals. Earlier instances of representer theorems can be found in [6, 3, 7]. More recently, the question of whether condition (2) is also necessary for the existence of linear representer theorems has been investigated [1]. In particular, [1] shows that, if Ω is differentiable (and certain technical existence conditions hold), then (2) is necessary and sufficient. The proof of [1] heavily exploits differentiability of Ω, but the authors conjecture that the hypothesis can be relaxed. In this report, we show that (2) is necessary and sufficient for the family of regularization functionals of the form (1) to admit a linear representer theorem, by merely assuming that Ω is lower semicontinuous and satisfies basic conditions for the existence of minimizers. The proof is based on a characterization of radial nondecreasing functionals on a Hilbert space."
    }, {
      "heading" : "2 A characterization of radial nondecreasing func-",
      "text" : "tionals\nIn this section, we present a characterization of radial nondecreasing functionals defined over Hilbert spaces. We will make use of the following definition.\nDefinition 2. A subset S of a Hilbert space H is called star-shaped with respect to a point z ∈ H if\n(1− λ)z + λx ∈ S, ∀x ∈ S, ∀λ ∈ [0, 1].\nIt is easy to verify that a convex set is star-shaped with respect to any point of the set, whereas a star-shaped set does not have to be convex.\nThe following Theorem provides a geometric characterization of radial nondecreasing functions defined on a Hilbert space that generalizes the analogous result of [1] for differentiable functions.\nTheorem 1. Let H denote a Hilbert space such that dimH ≥ 2, and let Ω : H → R ∪ {+∞} a lower semicontinuous function. Then, (2) holds if and only if\nΩ(x+ y) ≥ max{Ω(x),Ω(y)}, ∀x, y ∈ H : 〈x, y〉 = 0. (3)\nProof. Assume that (2) holds. Then, for any pair of orthogonal vectors x, y ∈ H, we have\nΩ(x+ y) = h (‖x+ y‖) = h (√ ‖x‖2 + ‖y‖2 ) ≥ max{h (‖x‖) , h (‖y‖)}\n= max{Ω(x),Ω(y)}.\nConversely, assume that condition (3) holds. Since dimH ≥ 2, by fixing a generic vector x ∈ X \\ {0} and a number λ ∈ [0, 1], there exists a vector y such that ‖y‖ = 1 and\nλ = 1− cos2 θ,\nwhere\ncos θ = 〈x, y〉\n‖x‖‖y‖ .\nIn view of (3), we have\nΩ(x) = Ω(x− 〈x, y〉y + 〈x, y〉y)\n≥ Ω(x− 〈x, y〉y) = Ω ( x− cos2 θx + cos2 θx− 〈x, y〉y )\n≥ Ω (λx) .\nSince the last inequality trivially holds also when x = 0, we conclude that\nΩ(x) ≥ Ω(λx), ∀x ∈ H, ∀λ ∈ [0, 1], (4)\nso that Ω is non-decreasing along all the rays passing through the origin. In particular, the minimum of Ω is attained at x = 0.\nNow, for any c ≥ Ω(0), consider the sublevel sets\nSc = {x ∈ H : Ω(x) ≤ c} .\nFrom (4), it follows that Sc is not empty and star-shaped with respect to the origin. In addition, since Ω is lower semi-continuous, Sc is also closed. We now show that Sc is either a closed ball centered at the origin, or the whole space. To this end, we show that, for any x ∈ Sc, the whole ball\nB = {y ∈ H : ‖y‖ ≤ ‖x‖},\nis contained in Sc. First, take any y ∈ int(B) \\ span{x}, where int denotes the interior. Then, y has norm strictly less than ‖x‖, that is\n0 < ‖y‖ < ‖x‖,\nand is not aligned with x, i.e.\ny 6= λx, ∀λ ∈ R.\nLet θ ∈ R denote the angle between x and y. Now, construct a sequence of points xk as follows: {\nx0 = y, xk+1 = xk + akuk,\nwhere\nak = ‖xk‖ tan\n( θ\nn\n) , n ∈ N\nand uk is the unique unitary vector that is orthogonal to xk, belongs to the two-dimensional subspace span{x, y}, and is such that 〈uk, x〉 > 0, that is\nuk ∈ span{x, y}, ‖uk‖ = 1, 〈uk, xk〉 = 0, 〈uk, x〉 > 0.\nBy orthogonality, we have\n‖xk+1‖ 2 = ‖xk‖ 2 + a2k = ‖xk‖ 2\n( 1 + tan2 ( θ\nn\n)) = ‖y‖2 ( 1 + tan2 ( θ\nn\n))k+1 .\n(5) In addition, the angle between xk+1 and xk is given by\nθk = arctan\n( ak\n‖xk‖\n) = θ\nn ,\nso that the total angle between y and xn is given by\nn−1∑\nk=0\nθk = θ.\nSince all the points xk belong to the subspace spanned by x and y, and the angle between x and xn is zero, we have that xn is positively aligned with x, that is\nxn = λx, λ ≥ 0.\nNow, we show that n can be chosen in such a way that λ ≤ 1. Indeed, from (5) we have\nλ2 =\n( ‖xn‖\n‖x‖\n)2 = ( ‖y‖\n‖x‖\n)2( 1 + tan2 ( θ\nn\n))n ,\nand it can be verified that\nlim n→+∞\n( 1 + tan2 ( θ\nn\n))n = 1,\ntherefore λ ≤ 1 for a sufficiently large n. Now, write the difference vector in the form\nλx− y =\nn−1∑\nk=0\n(xk+1 − xk),\nand observe that 〈xk+1 − xk, xk〉 = 0.\nBy using (4) and proceeding by induction, we have\nc ≥ Ω(λx) = Ω (xn − xn−1 + xn−1) ≥ Ω(xn−1) ≥ · · · ≥ Ω(x0) = Ω(y),\nso that y ∈ Sc. Since Sc is closed and the closure of int(B) \\ span{x} is the whole ball B, every point y ∈ B is also included in Sc. This proves that Sc is either a closed ball centered at the origin, or the whole space H.\nFinally, for any pair of points such that ‖x‖ = ‖y‖, we have x ∈ SΩ(y), and y ∈ SΩ(x), so that\nΩ(x) = Ω(y)."
    }, {
      "heading" : "3 Representer theorem: a necessary and suffi-",
      "text" : "cient condition\nIn this section, we prove that condition (2) is necessary and sufficient for suitable families of regularization functionals of the type (1) to admit a linear representer theorem.\nTheorem 2. Let H denote a Hilbert space such that dimH ≥ 2. Let F denote a family of functionals J : H → R∪{+∞} of the form (1) that admit minimizers.\n1. If Ω satisfy (2), then F admits a linear representer theorem.\n2. Conversely, assume that F contains a set of functionals of the form\nJγp (w) = γf (〈w, p〉) + Ω (w) , ∀p ∈ H, ∀γ ∈ R+, (6)\nwhere f(z) is uniquely minimized at z = 1. For any lower-semicontinuous Ω, the family F admits a linear representer theorem only if (2) holds.\nProof. The first part of the theorem (sufficiency) follows from an orthogonality argument. Take any functional J ∈ F . Let R = span{w1, . . . , wℓ} and let R ⊥ denote its orthogonal complement. Any minimizer w∗ of J can be uniquely decomposed as\nw∗ = u+ v, u ∈ R, v ∈ R⊥.\nIf (2) holds, then we have\nJ(w∗)− J(u) = h(‖w∗‖)− h(‖u‖) ≥ 0,\nso that u ∈ R is also a minimizer. Now, let’s prove the second part of the theorem. First of all, observe that the functional Jγ0 (w) = γf(0) + Ω(w), obtained by setting p = 0 in (6), belongs to F . By hypothesis, Jγ0 admits minimizers. In addition, by the representer theorem, the only admissible minimizer of J0 is the origin, that is\nΩ(y) ≥ Ω(0), ∀y ∈ H. (7)\nNow take any x ∈ H \\ {0} and let\np = x\n‖x‖2 .\nBy the representer theorem, the functional Jγp of the form (6) admits a minimizer of the type\nw = λ(γ)x.\nNow, take any y ∈ H such that 〈x, y〉 = 0. By using the fact that f(z) is minimized at z = 1, and the linear representer theorem, we have\nγf(1)+Ω (λ(γ)x) ≤ γf(λ(γ))+Ω (λ(γ)x) = Jγp (λ(γ)x) ≤ J γ p (x+y) = γf(1)+Ω (x+ y) .\nBy combining this last inequality with (7), we conclude that\nΩ (x+ y) ≥ Ω (λ(γ)x) , ∀x, y ∈ H : 〈x, y〉 = 0, ∀γ ∈ R+. (8)\nNow, there are two cases:\n• Ω (x+ y) = +∞\n• Ω (x+ y) = C < +∞.\nIn the first case, we trivially have\nΩ (x+ y) ≥ Ω(x).\nIn the second case, using (7) and (8), we obtain\n0 ≤ γ (f(λ(γ))− f(1)) ≤ Ω (x+ y)−Ω (λ(γ)x) ≤ C−Ω(0) < +∞, ∀γ ∈ R+. (9) Let γk denote a sequence such that limk→+∞ γk = +∞, and consider the sequence\nak = γk (f(λ(γk))− f(1)) .\nFrom (9), it follows that ak is bounded. Since z = 1 is the only minimizer of f(z), the sequence ak can remain bounded only if\nlim k→+∞ λ(γk) = 1.\nBy taking the limit inferior in (8) for γ → +∞, and using the fact that Ω is lower semicontinuous, we obtain condition (3). It follows that Ω satisfies the hypotheses of Theorem 1, therefore (2) holds.\nThe second part of Theorem 2 states that any lower-semicontinuous regularizer Ω has to be of the form (2) in order for the family F to admit a linear representer theorem. Observe that Ω is not required to be differentiable or even continuous. Moreover, it needs not to have bounded lower level sets. For the necessary condition to holds, the family F has to be broad enough to contain at least a set of regularization functionals of the form (6). The following examples show how to apply the necessary condition of Theorem 2 to classes of regularization problems with standard loss functions.\n• Let L : R2 → R ∪ {+∞} denote any loss function of the type\nL(y, z) = L̃(y − z),\nsuch that L̃(t) is uniquely minimized at t = 0. Then, for any lowersemicontinuous regularizer Ω, the family of regularization functionals of the form\nJ(w) = γ\nℓ∑\ni=1\nL (yi, 〈w,wi〉) + Ω(w),\nadmits a linear representer theorem if and only if (2) holds. To see that the hypotheses of Theorem 2 are satisfied, it is sufficient to consider the subset of functionals with ℓ = 1, y1 = 1, and w1 = p ∈ H. These functionals can be written in the form (6) with\nf(z) = L(1, z).\n• The class of regularization problems with the hinge (SVM) loss of the form\nJ(w) = γ\nℓ∑\ni=1\nmax{0, 1− yi〈w,wi〉} +Ω(w),\nwith Ω lower-semicontinuous, admits a linear representer theorem if and only if Ω satisfy (2). For instance, by choosing ℓ = 2, and\n(y1, w1) = (1, p), (y2, w2) = (−1, p/2),\nwe obtain regularization functionals of the form (6) with\nf(z) = max{0, 1− z}+max{0, 1 + z/2},\nand it is easy to verify that f is uniquely minimized at z = 1."
    }, {
      "heading" : "4 Conclusions",
      "text" : "We have shown that some general families of regularization functionals defined over a Hilbert space with lower semicontinuous regularizer admits a linear representer theorem if and only if the regularizer is a radial nondecreasing function. The result extends a previous characterization of [1], by relaxing the assumptions on the regularization term. We provide a unified proof that holds simultaneously for the finite and the infinite dimensional case."
    } ],
    "references" : [ {
      "title" : "When is there a representer theorem? Vector versus matrix regularizers",
      "author" : [ "A. Argyriou", "C.A. Micchelli", "M. Pontil" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2009
    }, {
      "title" : "Theory of reproducing kernels",
      "author" : [ "N. Aronszajn" ],
      "venue" : "Transactions of the American Mathematical Society,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 1950
    }, {
      "title" : "Asymptotic analysis of penalized likelihood and related estimators",
      "author" : [ "D. Cox", "F. O’ Sullivan" ],
      "venue" : "The Annals of Statistics,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 1990
    }, {
      "title" : "On the mathematical foundations of learning",
      "author" : [ "F. Cucker", "S. Smale" ],
      "venue" : "Bulletin of the American mathematical society,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2001
    }, {
      "title" : "Sur les ensembles de fonctions et les opérations linéaires",
      "author" : [ "M. Fréchet" ],
      "venue" : "Comptes rendus de l’Académie des sciences Paris,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 1907
    }, {
      "title" : "Some results on Tchebycheffian spline functions",
      "author" : [ "G. Kimeldorf", "G. Wahba" ],
      "venue" : "Journal of Mathematical Analysis and Applications,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 1971
    }, {
      "title" : "Networks for approximation and learning",
      "author" : [ "T. Poggio", "F. Girosi" ],
      "venue" : "In Proceedings of the IEEE,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 1990
    }, {
      "title" : "Sur une espèce de géométrie analytique des systèmes de fonctions sommables",
      "author" : [ "F. Riesz" ],
      "venue" : "Comptes rendus de l’Académie des sciences Paris,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 1907
    }, {
      "title" : "A generalized representer theorem",
      "author" : [ "B. Schölkopf", "R. Herbrich", "A.J. Smola" ],
      "venue" : "Proceedings of the Annual Conference on Computational Learning Theory,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2001
    }, {
      "title" : "Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond. (Adaptive Computation and Machine Learning)",
      "author" : [ "B. Schölkopf", "A.J. Smola" ],
      "venue" : null,
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2001
    }, {
      "title" : "Nonlinear component analysis as a kernel eigenvalue problem",
      "author" : [ "B. Schölkopf", "A.J. Smola", "K-R Müller" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 1998
    }, {
      "title" : "Kernel Methods for Pattern Analysis",
      "author" : [ "J. Shawe-Taylor", "N. Cristianini" ],
      "venue" : null,
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2004
    }, {
      "title" : "Solutions of Ill Posed Problems",
      "author" : [ "A.N. Tikhonov", "V.Y. Arsenin" ],
      "venue" : null,
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 1977
    }, {
      "title" : "Statistical Learning Theory",
      "author" : [ "V. Vapnik" ],
      "venue" : null,
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 1998
    }, {
      "title" : "Spline Models for Observational Data",
      "author" : [ "G. Wahba" ],
      "venue" : "SIAM, Philadelphia,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 1990
    } ],
    "referenceMentions" : [ {
      "referenceID" : 12,
      "context" : "Tikhonov regularization [13] is a popular and well-studied methodology to address ill-posed estimation problems [15], and learning from examples [4].",
      "startOffset" : 24,
      "endOffset" : 28
    }, {
      "referenceID" : 14,
      "context" : "Tikhonov regularization [13] is a popular and well-studied methodology to address ill-posed estimation problems [15], and learning from examples [4].",
      "startOffset" : 112,
      "endOffset" : 116
    }, {
      "referenceID" : 3,
      "context" : "Tikhonov regularization [13] is a popular and well-studied methodology to address ill-posed estimation problems [15], and learning from examples [4].",
      "startOffset" : 145,
      "endOffset" : 148
    }, {
      "referenceID" : 10,
      "context" : "and Ω is any strictly monotonically increasing function of the norm ‖w‖ [11].",
      "startOffset" : 72,
      "endOffset" : 76
    }, {
      "referenceID" : 7,
      "context" : "By the Riesz representation theorem [8, 5], J can be rewritten as J(w) = f(〈w,w1〉, .",
      "startOffset" : 36,
      "endOffset" : 42
    }, {
      "referenceID" : 4,
      "context" : "By the Riesz representation theorem [8, 5], J can be rewritten as J(w) = f(〈w,w1〉, .",
      "startOffset" : 36,
      "endOffset" : 42
    }, {
      "referenceID" : 13,
      "context" : "The theorem also provides the foundations of learning techniques such as regularized kernel methods and support vector machines, see [14, 10, 12] and references therein.",
      "startOffset" : 133,
      "endOffset" : 145
    }, {
      "referenceID" : 9,
      "context" : "The theorem also provides the foundations of learning techniques such as regularized kernel methods and support vector machines, see [14, 10, 12] and references therein.",
      "startOffset" : 133,
      "endOffset" : 145
    }, {
      "referenceID" : 11,
      "context" : "The theorem also provides the foundations of learning techniques such as regularized kernel methods and support vector machines, see [14, 10, 12] and references therein.",
      "startOffset" : 133,
      "endOffset" : 145
    }, {
      "referenceID" : 1,
      "context" : "Representer theorems are of particular interest when H is a reproducing kernel Hilbert space (RKHS) [2].",
      "startOffset" : 100,
      "endOffset" : 103
    }, {
      "referenceID" : 8,
      "context" : "A proof that the condition (2) is sufficient appeared in [9] in the case where H is a RKHS and Li are point-wise evaluation functionals.",
      "startOffset" : 57,
      "endOffset" : 60
    }, {
      "referenceID" : 5,
      "context" : "Earlier instances of representer theorems can be found in [6, 3, 7].",
      "startOffset" : 58,
      "endOffset" : 67
    }, {
      "referenceID" : 2,
      "context" : "Earlier instances of representer theorems can be found in [6, 3, 7].",
      "startOffset" : 58,
      "endOffset" : 67
    }, {
      "referenceID" : 6,
      "context" : "Earlier instances of representer theorems can be found in [6, 3, 7].",
      "startOffset" : 58,
      "endOffset" : 67
    }, {
      "referenceID" : 0,
      "context" : "More recently, the question of whether condition (2) is also necessary for the existence of linear representer theorems has been investigated [1].",
      "startOffset" : 142,
      "endOffset" : 145
    }, {
      "referenceID" : 0,
      "context" : "In particular, [1] shows that, if Ω is differentiable (and certain technical existence conditions hold), then (2) is necessary and sufficient.",
      "startOffset" : 15,
      "endOffset" : 18
    }, {
      "referenceID" : 0,
      "context" : "The proof of [1] heavily exploits differentiability of Ω, but the authors conjecture that the hypothesis can be relaxed.",
      "startOffset" : 13,
      "endOffset" : 16
    }, {
      "referenceID" : 0,
      "context" : "A subset S of a Hilbert space H is called star-shaped with respect to a point z ∈ H if (1− λ)z + λx ∈ S, ∀x ∈ S, ∀λ ∈ [0, 1].",
      "startOffset" : 118,
      "endOffset" : 124
    }, {
      "referenceID" : 0,
      "context" : "The following Theorem provides a geometric characterization of radial nondecreasing functions defined on a Hilbert space that generalizes the analogous result of [1] for differentiable functions.",
      "startOffset" : 162,
      "endOffset" : 165
    }, {
      "referenceID" : 0,
      "context" : "Since dimH ≥ 2, by fixing a generic vector x ∈ X \\ {0} and a number λ ∈ [0, 1], there exists a vector y such that ‖y‖ = 1 and λ = 1− cos θ,",
      "startOffset" : 72,
      "endOffset" : 78
    }, {
      "referenceID" : 0,
      "context" : "Ω(x) ≥ Ω(λx), ∀x ∈ H, ∀λ ∈ [0, 1], (4)",
      "startOffset" : 27,
      "endOffset" : 33
    }, {
      "referenceID" : 0,
      "context" : "The result extends a previous characterization of [1], by relaxing the assumptions on the regularization term.",
      "startOffset" : 50,
      "endOffset" : 53
    } ],
    "year" : 2012,
    "abstractText" : "A family of regularization functionals is said to admit a linear representer theorem if every member of the family admits minimizers that lie in a fixed finite dimensional subspace. A recent characterization states that a general class of regularization functionals with differentiable regularizer admits a linear representer theorem if and only if the regularization term is a non-decreasing function of the norm. In this report, we improve over such result by replacing the differentiability assumption with lower semicontinuity and deriving a proof that is independent of the dimensionality of the space.",
    "creator" : "LaTeX with hyperref package"
  }
}