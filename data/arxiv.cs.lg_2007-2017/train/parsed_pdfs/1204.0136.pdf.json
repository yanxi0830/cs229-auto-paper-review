{
  "name" : "1204.0136.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Near-Optimal Algorithms for Online Matrix Prediction",
    "authors" : [ "Elad Hazan", "Satyen Kale", "Shai Shalev-Shwartz" ],
    "emails" : [ "ehazan@ie.technion.ac.il.", "sckale@us.ibm.com." ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n20 4.\n01 36\nv1 [\ncs .L\nG ]\n3 1\nM ar\n2 01\n2\n√ β τ T ) for all problems in which the comparison class is composed of (β, τ)-decomposable matrices. By analyzing the decomposability of cut matrices, triangular matrices, and low tracenorm matrices, we derive near optimal regret bounds for online max-cut, online gambling, and online collaborative filtering. In particular, this resolves (in the affirmative) an open problem posed by Abernethy [2010], Kleinberg et al. [2010]. Finally, we derive lower bounds for the three problems and show that our upper bounds are optimal up to logarithmic factors. In particular, our lower bound for the online collaborative filtering problem resolves another open problem posed by Shamir and Srebro [2011]."
    }, {
      "heading" : "1 Introduction",
      "text" : "We consider online learning problems in which on each round the learner receives (it, jt) ∈ [m]× [n] and should return a prediction in [−1, 1]. For example, in the online collaborative filtering problem, m is the number of users, n is the number of items (e.g., movies), and on each online round the learner should predict a number in [−1, 1] indicating how much user it ∈ [m] likes item jt ∈ [n]. Once the learner makes the prediction, the environment responds with a loss function, ℓt : [−1, 1] → R, that assesses the correctness of the learner’s prediction.\nA natural approach for the learner is to maintain a matrix Wt ∈ [−1, 1]m×n, and to predict the corresponding entry, Wt(it, jt). The matrix is updated based on the loss function and the process continues.\nWithout further structure, the above setting is equivalent to mn independent prediction problems - one per user-item pair. However, it is usually assumed that there is a relationship between the different matrix entries - e.g. similar users prefer similar movies. This can be modeled in the online learning setting by assuming that there is some fixed matrix W, in a restricted class of matrices W ⊆ [−1, 1]m×n, such that the strategy which always predicts W (it, jt) has a small cumulative loss. A common choice for W in the collaborative filtering application is to be the set\n∗Technion - Israel Institute of Technology. ehazan@ie.technion.ac.il. †IBM T. J. Watson Research Center. sckale@us.ibm.com. ‡Hebrew University. shais@cs.huji.ac.il.\nof matrices with a trace norm of at most τ (which intuitively requires the prediction matrix to be of low rank). As usual, rather than assuming that some W ∈ W has a small cumulative loss, we require that the regret of the online learner with respect to W will be small. Formally, after T rounds, the regret of the learner is\nRegret :=\nT∑\nt=1\nℓt(Wt(it, jt))− min W∈W\nT∑\nt=1\nℓt(W (it, jt)),\nand we would like the regret to be as small as possible. A natural question is what properties of W enables us to derive an efficient online learning algorithm that enjoys low regret, and how does the regret depend on the properties of W. In this paper we define a property of matrices, called (β, τ)-decomposability, and derive an efficient online learning algorithm that enjoys a regret bound of Õ( √ β τ T ) for any problem in which W ⊂ [−1, 1]m×n and every matrix W ∈ W is (β, τ)-decomposable. Roughly speaking, W is (β, τ)decomposable if a symmetrization of it can be written as P−N where both P and N are positive semidefinite, have sum of traces bounded by τ , and have diagonal elements bounded by β.\nWe apply this technique to three online learning problems.\n1. Online max-cut: On each round, the learner receives a pair of graph nodes (i, j) ∈ [n]× [n], and should decide whether there is an edge connecting i and j. Then, it receives a binary feedback. The comparison class is the set of all cuts of the graph, which can be encoded as the set of matrices {WA : A ⊂ [n]}, where WA(i, j) indicates if (i, j) crosses the cut defined by A or not. It is possible to achieve a regret of O( √ nT ) for this problem by a non-efficient\nalgorithm (simply refer to each A as an expert and apply a prediction with expert advice algorithm). Our algorithm yields a nearly optimal regret bound of O( √\nn log(n)T ) for this problem. This is the first efficient algorithm that achieves near optimal regret.\n2. Online Gambling: On each round, the learner receives a pair of teams (i, j) ∈ [n] × [n], and should predict whether i is going to beat j in an upcoming matchup or vice versa. The comparison class is the set of permutations over the teams, where a permutation will predict that i is going to beat j if i appears before j in the permutation. Permutations can be encoded naturally as matrices, where W (i, j) is either 1 (if i appears before j in the permutation) or 0. Again, it is possible to achieve a regret of O( √\nn log(n)T ) by a non-efficient algorithm (that simply treats each permutation as an expert). Our algorithm yields a nearly optimal regret bound of O( √ n log3(n)T ). This resolves an open problem posed in Abernethy [2010],\nKleinberg et al. [2010]. Achieving this kind of regret bound was widely considered intractable, since computing the best permutation in hindsight is exactly the NP-hard minimum feedback arc set problem. In fact, Kanade and Steinke [2012] tried to show computational hardness for this problem by reducing the problem of online agnostic learning of halfspaces in a restricted setting to it. This paper shows that the problem is in fact tractable.\n3. Online Collaborative Filtering: We already mentioned this problem previously. We consider the comparison class W = {W ∈ [−1, 1]m×n : ‖W‖⋆ ≤ τ}, where ‖ · ‖⋆ is the trace norm. Without loss of generality assume m ≤ n. Our algorithm yields a nearly optimal regret bound of O( √ τ √ n log(n)T ). Since for this problem one typically has τ = Θ(n), we\ncan rewrite the regret bound as O( √ n3/2 log(n)T ). In contrast, a direct application of the\nonline mirror descent framework to this problem yields a regret of O( √ τ2T ) = O( √ n2T ). The latter is a trivial bound since the bound becomes meaningful only after T ≥ n2 rounds (which means that we saw the entire matrix).\nRecently, Cesa-Bianchi and Shamir [2011] proposed a rather different algorithm with regret bounded by O(τ √ n) but under the additional assumption that each entry (i, j) is seen only once. In addition, while both the runtime of our method and the Cesa-Bianchi and Shamir [2011] method is polynomial, the runtime of our method is significantly smaller: for m ≈ n, each iteration of our method can be implemented in Õ(n3) time (see Section 6), whereas the runtime of each iteration in their algorithm is at least Ω(n4) and can be significantly larger depending on the specific implementation.1\nFinally, we derive (nearly) matching lower bounds for the three problems. In particular, our lower bound for the online collaborative filtering problem implies that the sample complexity of learning matrices with bounded entries and trace norm of Θ(n) is Ω(n3/2). This matches an upper bound on the sample complexity derived by Shamir and Shalev-Shwartz [2011] and solves an open problem posed by Shamir and Srebro [2011]."
    }, {
      "heading" : "2 Problem statements and main results",
      "text" : "We start with the definition of (β, τ)-decomposability. For this, we first define a symmetrization operator.\nDefinition 1 (Symmetrization). Given an m× n non-symmetric matrix W its symmetrization is the (m+ n)× (m+ n) matrix:\nsym(W) :=\n[ 0 W\nW⊤ 0\n]\n.\nIf m = n and W is symmetric, then sym(W) := W.\nThe main property of matrices we rely on is (β, τ)-decomposability, which we define below.\nDefinition 2 ((β, τ)-decomposability). An m × n matrix W is (β, τ)-decomposable if there exist symmetric, positive semidefinite matrices P,N ∈ Rp×p, where p is the order of sym(W), such that the following conditions hold:\nsym(W) = P−N, Tr(P) + Tr(N) ≤ τ,\n∀i ∈ [p] : P (i, i), N(i, i) ≤ β.\nWe say that a set of matrices W is (β, τ)-decomposable if every matrix in W is (β, τ)-decomposable. 1Specifically, each iteration in their algorithm requires solving n empirical risk minimization problems over the hypothesis space of m× n matrices with a bounded trace norm (in their notation, to obtain the optimal bound, one should set T = n2 and η ≥ 1/n, and then should solve ηT empirical risk minimization problems per iteration). It is not clear what is the optimal runtime of solving each such empirical risk minimization problem. We believe that it is impossible to obtain a solver which is significantly faster than n4.\nIn the above, the parameter β stands for a bound on the diagonal elements of P and N, while the parameter τ stands for the trace of P andN. It is easy to verify that ifW is (β, τ)-decomposable then so is its convex hull, conv(W). Throughout this paper, we assume for technical convenience that β ≥ 1.2\nThere is an intriguing connection between the (β, τ)-decomposition for a rectangular matrix W and its max-norm and trace norm: the least possible β in any (β, τ)-decomposition exactly equals half the max-norm of W (see Theorem 21), and the least possible τ in any (β, τ)-decomposition exactly equals twice the trace-norm of W (see Theorem 23).\nOur first contribution is a generic low regret algorithm for online matrix prediction with a (β, τ)-decomposable comparison class. We also assume that all the matrices in the comparison class have bounded entries. Formally, we consider the following problem.\nOnline Matrix Prediction\nparameters: β ≥ 1, τ ≥ 0, G ≥ 0 input: A set of matrices, W ⊂ [−1, 1]m×n, which is (β, τ)-decomposable for t = 1, 2, . . . , T adversary supplies a pair of indices (it, jt) ∈ [m]× [n] learner picks Wt ∈ conv(W) and outputs the prediction Wt(it, jt) adversary supplies a convex, G-Lipschitz, loss function ℓt : [−1, 1] → R learner pays ℓt(Wt(it, jt))\nTheorem 1. There exists an efficient algorithm for Online Matrix Prediction which enjoys the regret bound\nRegret ≤ 2G √ τβ log(2p)T ,\nwhere p is the order of sym(W) for any matrix W ∈ W.\nThe Online Matrix Prediction problem captures several specific problems considered in the literature, given in the next few subsections."
    }, {
      "heading" : "2.1 Online Max-Cut",
      "text" : "Recall that on each round of online max-cut, the learner should decide whether two vertices of a graph, (it, jt) are joined by an edge or not. The learner outputs a number ŷt ∈ [−1, 1] which is to be interpreted as a randomized prediction in {−1, 1}: predict 1 with probability 1+ŷt2 and −1 with the remaining probability. The adversary then supplies the true outcome, yt ∈ {−1, 1}, where yt = 1 indicates the outcome “(it, jt) are joined by an edge”, and yt = −1 the opposite outcome. The loss suffered by the learner is the absolute loss,\nℓt(ŷt) = 1\n2 |ŷt − yt|,\nwhich can be also interpreted as the probability that a randomized prediction according to ŷt will not equal the true outcome yt.\n2The condition β ≥ 1 is not a serious restriction since for any (β, τ )-decomposition of W, viz. sym(W) = P−N, we have β ≥ |P (i, j)|, |N(i, j)| for all (i, j) since P,N 0; and so 2β ≥ |P (i, j) − N(i, j)| = |W (i, j)|. Thus, if we make the reasonable assumption that there is some W ∈ W with |W (i, j)| = 1 for some (i, j), then β ≥ 1\n2 is necessary.\nThe comparison class is W = {WA|A ⊆ [n]}, where\nWA(i, j) =\n{\n1 if ((i ∈ A) and (j /∈ A)) or ((j ∈ A) and (i /∈ A)) −1 otherwise.\nThat is, WA(i, j) indicates if (i, j) crosses the cut defined by A or not. The following lemma (proved in Appendix C) formalizes the relationship of this online problem to the max-cut problem:\nLemma 2. Consider an online sequence of loss functions {ℓt} as above. Let\nW∗ = arg min W∈W\n∑\nt\nℓt(W (it, jt)) .\nThen W∗ = WA for the set A that determines the max cut in the weighted graph over [n] nodes whose weights are given by wij = ∑\nt:(it,jt)=(i,j) yt for every (i, j).\nA regret bound of O( √ nT ) is attainable for this problem as follows via an exponential time algorithm: consider the set of all 2n cuts in the graph. For each cut defined by A, consider a decision rule or “expert” that predicts according to the matrix WA. Standard bounds for the experts algorithm imply the O( √ nT ) regret bound.\nA simple way to get an efficient algorithm is to replace W with the class of all matrices in {−1, 1}n×n. This leads to n2 different prediction tasks, each of which corresponds to the decision if there is an edge between two nodes, which is efficiently solvable. However, the regret with respect to this larger comparison class scales like O( √ n2T ).\nAnother popular approach for circumventing the hardness is to replace W with the set of matrices whose trace-norm is bounded by τ = n. However, applying the online mirror descent algorithmic framework with an appropriate squared-Schatten norm regularization, as described in [Kakade et al., 2010], leads to a regret bound that again scales like O( √ n2T ).\nIn contrast, our Online Matrix Prediction algorithm yields an efficient solution for this problem, with a regret that scales like √\nn log(n)T . The regret bound of the algorithm follows from the following:\nLemma 3. W is (1, n)-decomposable.\nCombining the above with Theorem 1 yields:\nCorollary 4. There is an efficient algorithm for the online max-cut problem with regret bounded by 2 √ n log(n)T .\nWe prove (in Appendix 5) that the upper bound is near-optimal:\nTheorem 5. For any algorithm for the online max-cut problem, there is a sequence of entries (it, jt) and loss functions ℓt for t = 1, 2, . . . , T such that the regret of the algorithm is at least√\nnT/16."
    }, {
      "heading" : "2.2 Collaborative Filtering with Bounded Trace Norm",
      "text" : "In this problem, the comparison set W is the following set of m × n matrices with trace norm bounded by some parameter τ :\nW := {W ∈ [−1, 1]m×n : ‖W‖⋆ ≤ τ}. (1)\nWithout loss of generality we assume that m ≤ n. As before, applying the technique of Kakade et al. [2010] leads to a regret bound that scales as√\nτ2T , which leads to trivial results in the most relevant case where τ = Θ( √ mn). In contrast, we\ncan obtain a much better result based on the following lemma. Lemma 6. The class W given in (1) is ( √ m+ n, 2τ)-decomposable.\nCombining the above with Theorem 1 yields:\nCorollary 7. There is an efficient algorithm for the online collaborative filtering problem with regret bounded by 2G √ 2τ √ n+m log(2(m+ n))T ), assuming that for all t the loss function is GLipschitz.\nThis upper bound is near-optimal, as we can also show (in Appendix 5) the following lower bound on the regret:\nTheorem 8. For any algorithm for online collaborative filtering problem with trace norm bounded by τ , there is a sequence of entries (it, jt) and G-Lipschitz loss functions ℓt for t = 1, 2, . . . , T such that the regret of the algorithm is at least G √\n1 2τ\n√ nT .\nIn fact, the technique used to prove the above lower bound also implies a lower bound on the sample complexity of collaborative filtering in the batch setting (proved in Appendix 5). Theorem 9. The sample complexity of learning W in the batch setting, is Ω(τ√n/ε2). In particular, when τ = Θ(n), the sample complexity is Ω(n1.5/ε2).\nThis matches an upper bound given by Shamir and Shalev-Shwartz [2011]. The question of determining the sample complexity of W in the batch setting has been posed as an open problem by Shamir (who conjectured that it scales like n1.5) and Srebro (who conjectured that it scales like n4/3)."
    }, {
      "heading" : "2.3 Online gambling",
      "text" : "In the gambling problem, we define the comparison set W as the following set of n × n matrices. First, for every permutation π : [n] → [n], define the matrix Wπ as:\nWπ(i, j) =\n{\n1 if π(i) ≤ π(j) 0 otherwise.\nThen the set W is defined as\nW := {Wπ : π is a permutation of [n]}. (2)\nOn round t, the adversary supplies a pair (it, jt) with it 6= jt, and the learner outputs as a prediction ŷt = Wt(it, jt) ∈ [0, 1], where we interpret ŷt as the probability that it will beat jt. The adversary then supplies the true outcome, ŷt ∈ {0, 1}, where ŷt = 1 indicates the outcome “it beats jt”, and ŷt = 0 the opposite outcome. The loss suffered by the learner is the absolute loss,\nℓt(yt) = |yt − ŷt|,\nwhich can be also interpreted as the probability that a randomized prediction according to ŷt will not equal to the true outcome yt.\nAs before, we tackle the problem by analyzing the decomposability of W.\nLemma 10. The class W given in (2) is (O(log(n)), O(n log(n)))-decomposable.\nCombining the above with Theorem 1 yields:\nCorollary 11. There is an efficient algorithm for the online gambling problem with regret bounded by O( √ n log3(n)T ).\nThis upper bound is near-optimal, as Kleinberg et al. [2010] essentially prove the following lower bound on the regret:\nTheorem 12. For any algorithm for the online gambling problem, there is a sequence of entries (it, jt) and labels yt, for t = 1, 2, . . . , T , such that the regret of the algorithm is at least Ω( √ n log(n)T )."
    }, {
      "heading" : "3 The Algorithm for Online Matrix Prediction",
      "text" : "In this section we prove Theorem 1 by constructing an efficient algorithm for Online Matrix Prediction and analyze its regret. We start by describing an algorithm for Online Linear Optimization (OLO) over a certain set of matrices and with a certain set of linear loss functions. We show later that the Online Matrix Prediction problem can be reduced to this online convex optimization problem."
    }, {
      "heading" : "3.1 The (β, τ, γ)-OLO problem",
      "text" : "In this section, all matrices are in the space of real symmetric matrices of size N × N , which we denote by SN×N .\nOn each round of online linear optimization, the learner chooses an element from a convex set K and the adversary responds with a linear loss function. In our case, the convex set K is a subset of the set of matrices with bounded trace and diagonal values:\nK ⊆ {X ∈ SN×N : X 0, ∀i ∈ [N ] : Xii ≤ β, Tr(X) ≤ τ}.\nWe assume for convenience that τN I ∈ K. The loss function on round t is the function X 7→ X • Lt def= ∑ i,j X(i, j)Lt(i, j), where Lt is a matrix from the following set of matrices:\nL = {L ∈ SN×N : L2 def= LL is a diagonal matrix s.t. Tr(L2) ≤ γ}.\nWe call the above setting a (β, γ, τ)-OLO problem. As usual, we analyze the regret of the algorithm\nRegret := T∑\nt=1\nXt • Lt − min X∈K\nT∑\nt=1\nX • Lt ,\nwhere X1, . . . ,XT are the predictions of the learner. Below we describe and analyze an algorithm for the (β, γ, τ)-OLO problem. The algorithm, forms of which independently appeared in the work of Tsuda et al. [2006] and Arora and Kale [2007], performs exponentiated gradient steps followed by Bregman projections onto K. The projection operation is defined with respect to the quantum relative entropy divergence:\n∆(X,A) = Tr(X log(X)−X log(A)−X+A).\nAlgorithm 1 Matrix Multiplicative Weights with Quantum Relative Entropy Projections\n1: Input: η 2: Initialize X1 = τ N I. 3: for t = 1, 2, . . . , T : do 4: Play the matrix Xt. 5: Obtain loss matrix Lt. 6: Update Xt+1 = argminX∈K ∆(X, exp(log(Xt)− ηLt)). 7: end for\nAlgorithm 1 has the following regret bound (essentially following Tsuda et al. [2006], Arora and Kale [2007], also proved in Appendix A for completeness):\nTheorem 13. Suppose η is chosen so that η‖Lt‖ ≤ 1 for all t (where ‖Lt‖ is the spectral norm of Lt). Then\nRegret ≤ η T∑\nt=1\nXt • L2t + τ log(N)\nη .\nEquipped with the above we are ready to prove a regret bound for (β, γ, τ)-OLO.\nTheorem 14. Assume T ≥ τ log(N)β . Then, applying Algorithm 1 with η = √ τ log(N) βγT on a (β, γ, τ)OLO problem yields an efficient algorithm whose regret is at most 2 √ βγτ log(N)T .\nProof. Clearly, Algorithm 1) can be implemented in polynomial time since the update of step 6 is a convex optimization problem. To analyze the regret of the algorithm we rely on Theorem 13. By the definition of K and L, we get that Xt • L2t ≤ βγ. Hence, the regret bound becomes\nRegret ≤ ηβγT + τ log(N) η .\nSubstituting the value of η, we get the stated regret bound. One technical condition is that the above regret bound holds as long as η is chosen small enough so that for all t, we have η‖Lt‖ ≤ 1. Now ‖Lt‖ ≤ ‖Lt‖F = √ Tr(L2t ) ≤ √ γ. Thus, for T ≥ τ log(N)β , the technical condition is satisfied for η = √\nτ log(N) βγT ."
    }, {
      "heading" : "3.2 An Algorithm for the Online Matrix Prediction Problem",
      "text" : "In this section we describe a reduction from the Online Matrix Prediction problem (with a (β, τ)decomposable comparison class) to a (β, 4G2, τ)-OCO problem with N = 2p. The regret bound of the derived algorithm will follow directly from Theorem 14.\nWe now describe the reduction. To simplify our notation, let q be m if W contains nonsymmetric matrices and q = 0 otherwise. Note that the definition of sym(W) implies that for a pair of indices (i, j) ∈ [m]× [n], their corresponding indices in sym(W) are (i, j + q).\nGiven any matrix W ∈ W we embed its symmetrization sym(W) (which has size p × p) into the set of 2p×2p positive semidefinite matrices as follows. Since W admits a (β, τ)-decomposition, there exist P,N 0 such that sym(W) = P − N, Tr(P) + Tr(N) ≤ τ , and for all i ∈ [p], P (i, i), N(i, i) ≤ β. The embedding of W in S2p×2p, denoted φ(W), is defined to be the matrix3\nφ(W) =\n[ P 0\n0 N\n]\n.\nIt is easy to verify that φ(W) belongs to the convex set K defined below:\nK := { X ∈ S2p×2p s.t.\nX 0 (3) ∀i ∈ [2p] : X(i, i) ≤ β Tr(X) ≤ τ ∀(i, j) ∈ [m]× [n] : (X(i, j + q)−X(p+ i, p + j + q)) ∈ [−1, 1] }\nWe shall run the OLO algorithm with the set K. On round t, if the adversary gives the pair (it, jt), then we predict ŷt = Xt(it, jt + q)−Xt(p+ it, p+ jt + q) . The last constraint defining K simply ensures that ŷt ∈ [−1, 1]. While this constraint makes the quantum relative entropy projection onto K more complex, in Appendix 6 we show how we can leverage the knowledge of (it, jt) to get a very fast implementation.\nNext we describe how to choose the loss matrices Lt using the subderivative of ℓt. Given the loss function ℓt, let g be a subderivative of ℓt at ŷt. Since ℓt is convex and G-Lipschitz, we have that |g| ≤ G. Define Lt ∈ S2p×2p as follows:\nLt(i, j) =\n \n\ng if (i, j) = (it, jt + q) or (i, j) = (jt + q, it) −g if (i, j) = (p + it, p+ jt + q) or (i, j) = (p + jt + q, p+ it) 0 otherwise.\n(4)\nNote that L2t is a diagonal matrix, whose only non-zero diagonal entries are (it + q, it + q), (jt + q, jt+q), (p+it+q, p+it+q), and (p+jt+q, p+jt+q), all equalling g 2. Hence, Tr(L2t ) = 4g 2 ≤ 4G2.\n3Note that this mapping depends on the choice of P and N for each matrix W ∈ W. We make an arbitrary choice for each W.\nTo summarize, the Online Matrix Prediction algorithm will be as follows:\nAlgorithm 2 Matrix Multiplicative Weights for Online Matrix Prediction\n1: Input: β, τ,G,m, n, p, q (see text for definitions) 2: Set: γ = 4G2, N = 2p, η = √\nτ log(N) βγT\n3: Let K be as defined in (3) 4: Initialize X1 = τ N I. 5: for t = 1, 2, . . . , T : do 6: Adversary supplies a pair of indices (it, jt) ∈ [m]× [n]. 7: Predict ŷt = Xt(it, jt + q)−Xt(p+ it, p+ jt + q). 8: Obtain loss function ℓt : [−1, 1] → R and pay ℓt(ŷt). 9: Let g be a sub-derivative of ℓt at ŷt\n10: Let Lt be as defined in (4) 11: Update Xt+1 = argminX∈K ∆(X, exp(log(Xt)− ηLt)). 12: end for\nTo analyze the algorithm, note that for any W ∈ W,\nφ(W) • Lt = 2g(P (it, jt)−N(it, jt)) = 2gW (it, jt),\nand Xt • Lt = 2g(Xt(it, jt + q)−Xt(p + it, p+ jt + q)) = 2gŷt.\nSo for any W ∈ W, we have\nXt • Lt − φ(W) • Lt = 2g(ŷt −W (it, jt)) ≥ 2(ℓt(ŷt)− ℓt(W (it, jt))),\nby the convexity of ℓt(·). This implies that for any W ∈ W,\nT∑\nt=1\nℓt(ŷt)− ℓt(W (it, jt)) ≤ 1\n2\n[ T∑\nt=1\nXt • Lt − φ(W) • Lt ]\n≤ 1 2 · RegretOLO.\nThus, the regret of the Online Matrix Prediction problem is at most half the regret in the (β, 4G2, τ)OLO problem."
    }, {
      "heading" : "3.2.1 Proof of Theorem 1",
      "text" : "Following our reduction, we can now appeal to Theorem 14. For T ≥ τ log(2p)β , the bound of Theorem 14 applies and gives a regret bound of 2G √\nτβ log(2p)T . For T < τ log(2p)β , note that in any round, the regret can be at most 2G, since the subderivatives of the loss functions are bounded in absolute value by G and the domain is [−1, 1], so the regret is bounded by 2GT < 2G √\nτβ log(2p)T since β ≥ 1. Thus, we have proved the regret bound stated in Theorem 1."
    }, {
      "heading" : "4 Decomposability Proofs",
      "text" : "In this section we prove the decomposability results for the comparison classes corresponds to maxcut, collaborative filtering, and gambling. All the three decompositions we give are optimal up to constant factors."
    }, {
      "heading" : "4.1 Proof of Lemma 3 (max-cut)",
      "text" : "We need to show that every matrix WA ∈ W admits a (1, n)-decomposition. We can rewrite WA = −wAw⊤ where wA ∈ Rn is the vector such that\nWA(i) =\n{\n1 if i ∈ A −1 otherwise.\nSince WA is already symmetric, sym(WA) = WA = −wAw⊤A. Thus we can choose P = 0 and N = wAw ⊤ A. These are positive semidefinite matrices with diagonals bounded by 1 and sum of traces equals to n, which concludes the proof. Since Tr(wAw ⊤ A) = n, this (1, n)-decomposition is optimal."
    }, {
      "heading" : "4.2 Proof of Lemma 6 (collaborative filtering)",
      "text" : "We need to show that every matrixW ∈ W, i.e. anm×nmatrix over [−1, 1] with ‖W‖⋆ ≤ τ , admits a ( √ m+ n, 2τ)-decomposition. The ( √ m+ n, 2τ)-decomposition of W is a direct consequence of the following theorem, setting Y = sym(W), with p = m + n, and the fact that ‖sym(W)‖⋆ = 2‖W‖⋆ (see Lemma 19). Theorem 15. Let Y be a p× p symmetric matrix with entries in [−1, 1]. Then Y can be written as Y = P−N where P and N are both positive semidefinite matrices with diagonal entries bounded by √ p, and Tr(P) + Tr(N) = ‖Y‖⋆.\nProof. Let\nY = ∑\ni\nλiviv ⊤ i\nbe the eigenvalue decomposition of Y. We now show that\nP = ∑\ni: λi≥0 λiviv\n⊤ i and N =\n∑\ni: λi<0\n−λiviv⊤i\nsatisfy the required conditions. Clearly Tr(P)+Tr(N) = ∑\ni |λi| = ‖Y‖⋆. Define abs(Y) = P+N =∑ i |λi|viv⊤i . Note that\nabs(Y)2 = ∑\ni\nλ2iviv ⊤ i = Y 2.\nWe now show that all entries (and in particular, the diagonal entries) of abs(Y) are bounded in magnitude by √ p. Since P and N are both positive semidefinite, their diagonal elements must be non-negative, so we conclude that the diagonal entries of P and N are bounded by √ p as well.\nSince all the entries of Y are bounded in magnitude by 1, it follows that all entries of Y2 are bounded in magnitude by p. In particular, the diagonal entries of Y2 are bounded by p. Since these diagonal entries are equal to the squared lengths of the rows of abs(Y), it follows that each entry of abs(Y) is bounded in magnitude by √ p.\nThis decomposition is optimal up to constant factors. Consider the matrix W formed by taking m = τ√\nn rows of an n × n Hadamard matrix. In Theorem 20 (proved in Appendix D), we prove\nthat any (β, τ̃ )-decomposition of sym(W) must have βτ̃ ≥ 14τ √ n. Since the regret bound depends on the product βτ̃ , we conclude that the decomposition obtained from Theorem 15 is optimal up to a constant factor."
    }, {
      "heading" : "4.3 Proof of Lemma 10 (gambling)",
      "text" : "We need to show that every matrix W ∈ W, i.e. an n × n matrix Wπ for some permutation π : [n] → [n], admits a (O(log(n)), O(n log(n)))-decomposition. One minor change that needs to be made to Algorithm 2 is that the last constraint in (3) needs to be changed to\n∀(i, j) ∈ [n]× [n] : (X(i, j + q)−X(p + i, p + j + q)) ∈ [0, 1],\nto ensure that the prediction lies in [0, 1] rather than [−1, 1]. The analysis remains intact, and so does the regret bound.\nWe now give the decomposition. The following upper triangular matrix T plays a pivotal role:\nT (i, j) =\n{\n1 if i ≤ j 0 otherwise.\nThe reason this matrix is so important is because any matrix Wπ is obtained by permuting the rows and columns of T. In particular, let Pπ be the permutation matrix defined by the permutation π, i.e.\nPπ(i, j) =\n{\n1 if j = π(i)\n0 otherwise.\nThen it is easy to check that Wπ = PπTP ⊤ π .\nUsing this fact, we get [ Pπ 0\n0 Pπ\n]\n︸ ︷︷ ︸\nQπ\nsym(T) [ P⊤π 0 0 P⊤π ] = [ Pπ 0 0 Pπ ] [ 0 T T⊤ 0 ] [ P⊤π 0 0 P⊤π ]\n=\n[ 0 PπTP ⊤ π\nPπT ⊤P⊤π 0\n]\n=\n[ 0 Wπ\nW⊤π 0\n]\n= sym(Wπ).\nNow, note thatQπ is a permutation matrix (viz. the one defined by the permutation π ′ : [2n] → [2n] defined as π′(i) = π(i) for 1 ≤ i ≤ n, and π′(i) = π(i− n) +n for n < i ≤ 2n). Thus, if T admits a (β, τ)-decomposition, sym(T) = P−N, then\nsym(Wπ) = Qπsym(T)Q ⊤ π = QπPQ ⊤ π −QπNQ⊤π\nis a (β, τ)-decomposition for sym(Wπ). This is because the diagonal entries of QπPQ ⊤ π (resp. QπNQ ⊤ π ) are simply a permutation (viz. π ′) of the diagonal entries of P (resp. N). Since\nABA⊤ 0 if B 0 for any matrix A, the matrices QπPQ⊤π and QπPQ⊤π are both positive semidefinite.\nSo now we show that T admits a (O(log(n)), O(n log(n)))-decomposition. For convenience, we assume that n is a power of 2, i.e. n = 2k for some integer k ≥ 0. For n that are not a power of 2, we can readily obtain a decomposition by the following observation: if we take the smallest power of 2 that is larger than n, say 2k, and consider the symmetrized triangular matrix for 2k, then sym(T) can be expressed as a principal submatrix of it. Then taking the corresponding principal submatrices from the decomposition for the triangular matrix for 2k we obtain a decomposition for n. This uses the fact that principal submatrices of positive semidefinite matrices are positive semidefinite as well.\nTheorem 16. Let n = 2k for some integer k ≥ 0. Then T admits a (k+1, 4n(k+1))-decomposition.\nProof. We show that sym(T) can be written as a difference of positive semidefinite matrices with diagonals bounded by k+1. The bound on the sum of traces, 4n(k+1), of the two matrices follows trivially.\nWe use a recursive construction. Let the triangular matrix for n = 2k be denoted by Tk. For k = 0, the following is a decomposition for T0 with diagonals bounded by 1:\nsym(T0) = [ 0 1 1 0 ] = [ 1 1 1 1 ] − [ 1 0 0 1 ] .\nSo now assume that k > 0 and we have a decomposition for Tk−1 with diagonals bounded by k, i.e.\nsym(Tk−1) =\n[ 0 Tk−1\nT⊤k−1 0\n]\n= P−N,\nwhere P,N 0, and for all i ∈ [2k], P (i, i), N(i, i) ≤ k. We need the following block decomposition of P and N into contiguous 2k−1 × 2k−1 blocks as follows:\nP =\n[ PA PB\nPC PD\n]\nand N =\n[ NA NB\nNC ND\n]\n.\nThen we have the following decomposition of sym(Tk). All the blocks in the decomposition below are of size 2k−1 × 2k−1.\nsym(Tk) =\n\n  \n0 0 0 1\n0 0 0 0\n0 0 0 0\n1 0 0 0\n\n   +\n\n  \n0 0 Tk−1 0 0 0 0 Tk−1\nT⊤k−1 0 0 0 0 T⊤k−1 0 0\n\n   .\nNow, consider the following decompositions of the two matrices above as a difference of positive semidefinite matrices. For the first matrix, the diagonals in the decomposition are bounded by 1:\n\n  \n0 0 0 1\n0 0 0 0\n0 0 0 0 1 0 0 0\n\n   =\n\n  \n1 0 0 1\n0 0 0 0\n0 0 0 0 1 0 0 1\n\n   −\n\n  \n1 0 0 0\n0 0 0 0\n0 0 0 0 0 0 0 1\n\n   .\nFor the second matrix, the diagonals in the decomposition are bounded by k.\n\n  \n0 0 Tk−1 0 0 0 0 Tk−1\nT⊤k−1 0 0 0 0 T⊤k−1 0 0\n\n   =\n\n  \nPA 0 PB 0\n0 PA 0 PB\nPC 0 PD 0\n0 PC 0 PD\n\n   −\n\n  \nNA 0 NB 0\n0 NA 0 NB\nNC 0 ND 0\n0 NC 0 ND\n\n   .\nIt is easy to verify that the matrices in the decomposition above are positive semidefinite, since each is a sum of two positive semidefinite matrices. For example:\n\n  \nPA 0 PB 0\n0 PA 0 PB\nPC 0 PD 0\n0 PC 0 PD\n\n   =\n\n  \nPA 0 PB 0\n0 0 0 0 PC 0 PD 0\n0 0 0 0\n\n   +\n\n  \n0 0 0 0 0 PA 0 PB\n0 0 0 0 0 PC 0 PD\n\n   .\nAdding the two decompositions, we get a decomposition for sym(Tk) as a difference of two positive semidefinite matrices. The diagonal entries of these two matrices are bounded by k+1, as required.\nThis decomposition is optimal up to constant factors. This is because the singular values of T are 1\n2 cos( kπ 2n+1\n) for k = 1, 2, . . . , n (see Elkies [2011]). This implies that ‖T‖⋆ = Θ(n log(n)). Thus,\nthe best β one can get is Θ(log(n)), and the best τ is Θ(n log(n))."
    }, {
      "heading" : "5 Lower bounds",
      "text" : "In this section we prove the lower bounds stated in Section 2."
    }, {
      "heading" : "5.1 Online Max Cut",
      "text" : "We prove Theorem 5, which we restate here for convenience:\nTheorem 5 restated: For any algorithm for the online max cut problem, there is a sequence of entries (it, jt) and loss functions ℓt for t = 1, 2, . . . , T such that the regret of the algorithm is at least √ nT/16.\nProof. Consider the following stochastic adversary. Divide up the time period T into n/2 equal size4 intervals Ti, for i ∈ [n/2], corresponding to the n/2 pairs of indices (i, i + n/2) for i ∈ [n/2]. For every i ∈ [n/2] and for each t ∈ Ti, the adversary sets (it, jt) = (i, i + n/2) and yt to be a Rademacher random variable independent of all other such variables. Clearly, the expected regret of any algorithm for the online max cut problem equals T2 .\nNow, define the following subset of vertices A: for every i ∈ [n/2], consider Si = ∑\nt∈Ti yt. If Si < 0, include both i, i + n/2 ∈ A, else only include i ∈ A. By construction, the matrix WA has the following property for all i ∈ [n/2]:\nWA(i, i+ n/2) = sgn(Si).\n4We assume for convenience that n 2 and 2T n are integers.\nUsing the definition of ℓt and the fact that |Ti| = 2T/n, we obtain\nE\n  ∑\nt∈Ti\nℓt(WA(i, i+ n/2))\n\n = E\n  ∑\nt∈Ti\n( 1 2 − sgn(Si) 2 yt )\n\n\n= E\n[ T\nn − |Si| 2\n] ≤ T n − √ T 4n ,\nwhere we used Khintchine’s inequality: if X is a sum of k independent Rademacher random variables, then E[|X|] ≥ √ k/2. Summing up over all i ∈ [n/2], we get that\nE\n[ T∑\nt=1\nℓt(WA(it, jt))\n]\n≤ n 2\n[\nT n − √ T 4n\n]\n= T\n2 −\n√\nnT 16 .\nHence the expected regret of the algorithm is at least √\nnT 16 . In particular, there is a setting of the\nŷt variables so that the regret of the algorithm is at least √ nT 16 ."
    }, {
      "heading" : "5.2 Online Collaborative Filtering with Bounded Trace Norm",
      "text" : "We start with the proof of Theorem 8, which we restate here for convenience:\nTheorem 8 restated: For any algorithm for online collaborative filtering problem with trace norm bounded by τ , there is a sequence of entries (it, jt) and loss functions ℓt for t = 1, 2, . . . , T such that the regret of the algorithm is at least G √\n1 2τ\n√ nT .\nProof. First, we may assume that τ ≤ m√n: this is because for any matrix W ∈ [−1, 1]m×n, we have\n‖W‖⋆ ≤ √ rank(W)‖W‖F ≤ √ m · √ mn = m √ n,\nsince rank(W) ≤ m. So now we focus on the sub-matrix formed by the first τ√ n rows5 and all n\ncolumns. This sub-matrix has τ √ n entries.\nConsider the following stochastic adversary. Divide up the time period T into τ √ n intervals of\nlength T τ √ n , indexed by τ\n√ n pairs (i, j) corresponding to the entries of the sub-matrix. For every\n(i, j), and for every round t in the interval Iij corresponding to (i, j), we set the loss function to be ℓt(W) = σtGWij, where σt ∈ {−1, 1} is a Rademacher random variable chosen independently of all other such variables. Note that the absolute value of derivative of the loss function is G.\nClearly, any algorithm for OCF has expected loss 0. Now consider the matrix W⋆ where\n∀i ∈ [\nτ√ n\n]\n, j ∈ [n] : W ⋆ij = −sgn ( ∑ t∈Iijσt ) ,\nand all entries in rows i > τ√ n are set to 0. Since rank(W⋆) ≤ τ√ n , we have\n‖W⋆‖⋆ ≤ √ rank(W⋆) · ‖W⋆‖F ≤ √ τ√ n · √ τ √ n = τ,\n5For convenience, we assume that τ√ n and T τ √ n are integers.\nso W⋆ ∈ W.6 The expected loss of W⋆ is\n∑\nij\nE\n  ∑\nt∈Iij\nσtGW ⋆ ij\n  = G ∑\nij\nE\n\n− ∣ ∣ ∣ ∣ ∣ ∣ ∑\nt∈Iij\nσt ∣ ∣ ∣ ∣ ∣ ∣  \n≥ −G ∑\nij\n√\n1 2 |Iij |\n= −Gτ √ n ·\n√\nT\n2τ √ n\n= −G √ 1\n2 τ √ nT ,\nwhere the inequality above is again due to Khintchine’s inequality. Hence, the expected regret of the algorithm is at least G √\n1 2τ √ nT . In particular, there is a specific assignment of values to σt\nsuch that the regret of the algorithm is at least G √\n1 2τ\n√ nT .\nThe construction we used for deriving the above lower bound can be easily adapted to derive a lower bound on the sample complexity of learning the class W in the batch setting. This is formalized in Theorem 9, which we restate here for convenience.\nTheorem 9 restated The sample complexity of learningW in the batch setting, is Ω(τ√n/ε2). In particular, when τ = Θ(n), the sample complexity is Ω(n1.5/ε2).\nProof. For simplicity, let us choose m = n. Let k = τ/ √ n and fix some small ε. Define a family of distributions over [n]2 × {−1, 1} as follows. Each distribution is parameterized by a matrix W such that there is some I ⊂ [n], with |I| = k, where W (i, j) ∈ {−1, 1} for i ∈ I and W (i, j) = 0 for i /∈ I. Now, the probability to sample an example (i, j, y) is\n( 1 2 + 2ε ) 1 kn if i ∈ I and y = W (i, j),\nis ( 1 2 − 2ε ) 1 kn if i ∈ I and y = −W (i, j), and the probability is 0 in all other cases.\nAs in the proof of Theorem 8, any matrix defining such distribution is in W. Furthermore, if we consider the absolute loss function: ℓ(W, (i, j, y)) = 12 |W (i, j)− y|, then the expected loss of W with respect to the distribution it defines is\nE [ 1 2 |W (i, j) − y| ] = 12 − 2ε .\nIn contrast, by standard no-free-lunch arguments, no algorithm can know to predict an entry (i, j) with error smaller than 12 − ε without observing Ω(1/ε2) examples from this entry. Therefore, no algorithm can have an error smaller than 12 − ε without receiving Ω(kn/ε2) examples."
    }, {
      "heading" : "6 Implementation Details",
      "text" : "In general, the update rule in Algorithm 1 is a convex optimization problem and can be computed in polynomial time. We now give the following more efficient implementation which takes essentially Õ(p3) time per round. This is based on the following theorem that is essentially proved in Tsuda et al. [2006]:\n6This construction is tight: e.g. if W⋆ is formed by taking t√ n rows of an n× n Hadamard matrix.\nTheorem 17. The optimal solution of argminX∈K ∆(X,Y), where Y is a given symmetric matrix, and K := {X ∈ Sn×n : Aj •X ≤ bj for j = 1, 2, . . . ,m}, is given by\nX⋆ = exp(log(Y)−∑mj=1α⋆jA′j),\nwhere A′j = 1 2 (Aj +A ⊤ j ), and α ⋆ = 〈α⋆1, α⋆2, . . . , α⋆m〉 is given by\nα ⋆ = arg max ∀j∈[m]: αj≥0 −Tr(exp(log(Y)−∑mj=1αjA′j))− ∑m j=1αjbj.\nThe idea is to avoid taking projections on the set K in each round. If the chosen entry in round t is (it, jt), then we compute Xt as\nXt = arg min X∈Kt\n∆(X, exp(log(Xt−1 − ηLt−1)),\nwhere the polytope Kt is defined as\nKt := { X ∈ S2p×2p s.t.\nX(it, it) +X(jt + q, jt + q) +X(p + it, p+ it) +X(p + jt + q, p+ jt + q) ≤ 4β X(it, jt + q)−X(p + it, p+ jt + q)) ≤ 1 X(p+ it, p + jt + q))−X(it, jt + q) ≤ 1 Tr(X) ≤ τ }\nThe observation is that this suffices for the regret bound of Theorem 14 to hold since the optimal point in hindsight X⋆ ∈ Kt for all t (see the proof of Theorem 13).\nNote that Kt is defined using just 4 constraints, and hence the dual problem given in Theorem 17 has only 4 variables αj . Thus, standard convex optimization techniques (say, the ellipsoid method) can be used to solve the dual problem to ε-precision in O(log(1/ε)) iterations, each of which requires computing the gradient and/or the Hessian of the objective, which can be done in O(p3) time via the eigendecomposition, leading to an Õ(p3) time algorithm overall.\nMore precisely, the iteration count for convex optimization methods have logarithmic dependence on the range of the αj variables. Since Tr(Xt−1) ≤ τ , we see (using the Golden-Thompson inequality [Golden, 1965, Thompson, 1965]) that\nTr(exp(log(Xt−1 − ηLt−1))) ≤ Xt−1 • exp(−ηLt−1) ≤ 3τ.\nThus, setting all αj = 0, the dual objective value is at least −3τ . Since bj ≥ 1 for all j, we get that the optimal values of αj are all bounded by 3τ . Thus, the range of all αj can be set to [0, 3τ ], giving a O(log( τε )) bound on the number of iterations."
    }, {
      "heading" : "7 Conclusions",
      "text" : "In recent years the FTRL (Follow The Regularized Leader) paradigm has become the method of choice for proving regret bounds for online learning problems. In several online learning problems a direct application of this paradigm has failed to give tight regret bounds due to suboptimal “convexification” of the problem. This unsatisfying situation occurred in mainstream applications, such as online collaborative filtering, but also in basic prediction settings such as the online max cut or online gambling settings.\nIn this paper we single out a common property of these unresolved problems: they involve structured matrix prediction, in the sense that the matrices involved have certain nice decompositions. We give a unified formulation for three of these structured matrix prediction problems which leads to near-optimal convexification. Applying the standard FTRL algorithm, Matrix Multiplicative Weights, now gives efficient and near optimal regret algorithms for these problems. In the process we resolve two COLT open problems. The main conclusion of this paper is that spectral analysis in matrix predictions tasks can be surprisingly powerful, even when the connection between the spectrum and the problem may not be obvious on first sight (such as in the online gambling problem).\nWe leave open the question of bridging the logarithmic gap between known upper and lower bounds for regret in these structured prediction problems. Note that since all the three decompositions in this paper are optimal up to constant factors, one cannot close the gap by improving the decomposition; some fundamentally different algorithm seems necessary. It would also be interesting to see more applications of the (β, τ)-decomposition for other online matrix prediction problems."
    }, {
      "heading" : "A Matrix Multiplicative Weights Algorithm",
      "text" : "For the sake of completeness, we prove Theorem 13. The setting is as follows. We have an online convex optimization problem where the decision set is a convex subset K of N ×N positive semidefinite matrices of trace bounded by τ , viz. for all X ∈ K, we have X 0 and Tr(X) ≤ τ . We assume for convenience that τN I ∈ K. In each round t, the learner produces a matrix Xt ∈ K, and the adversary supplies a loss matrix Lt ∈ RN×N , which is assumed to be symmetric. The loss of the learner is Xt • Lt. The goal is to minimize regret defined as\nRegret := T∑\nt=1\nXt • Lt − min X∈K\nT∑\nt=1\nX • Lt.\nConsider Algorithm 1. We now prove Theorem 13, which we restate here for convenience:\nTheorem 18. Suppose η is chosen so that η‖Lt‖ ≤ 1 for all t. Then\nRegret ≤ η T∑\nt=1\nXt • L2t + τ log(N)\nη .\nProof. Consider any round t. Let X ∈ K be any matrix. We use the quantum relative entropy, ∆(X,Xt), as a potential function. We have\n∆(X, exp(log(Xt)− ηLt))−∆(X,Xt) = ηX • Lt − Tr(Xt) + Tr(exp(log(Xt)− ηLt)). (5)\nNow quantum relative entropy projection onto the set K is a Bregman projection, and hence the Generalized Pythagorean inequality applies (see Tsuda et al. [2006]):\n∆(X,Xt+1) + ∆(Xt+1, exp(log(Xt)− ηLt))) ≤ ∆(X, exp(log(Xt)− ηLt))),\nand since ∆(Xt+1, exp(log(Xt)− ηLt))) ≥ 0, we get that\n∆(X,Xt+1) ≤ ∆(X, exp(log(Xt)− ηLt))).\nHence from (5) we get\n∆(X,Xt+1)−∆(X,Xt) ≤ ηX • Lt −Tr(Xt) + Tr(exp(log(Xt)− ηLt)). (6)\nNow, using the Golden-Thompson inequality [Golden, 1965, Thompson, 1965], we have\nTr(exp(log(Xt)− ηLt)) ≤ Tr(Xt exp(−ηLt))\nNext, using the fact that exp(A) I+A+A2 for ‖A‖ ≤ 1,7 we obtain\nTr(Xt exp(−ηLt)) ≤ Tr(Xt(I− ηLt + η2L2t ) = Tr(Xt)− ηXt • Lt + η2Xt • L2t .\nCombining the above and plugging into (6) we get\n∆(X,Xt+1)−∆(X,Xt) ≤ ηX • Lt − ηXt • Lt + η2Xt • L2t . (7)\nSumming up from t = 1 to T , and rearranging, we get\nRegret ≤ η T∑\nt=1\nXt • L2t + ∆(X,X1)−∆(X,XT+1)\nη\n≤ η T∑\nt=1\nXt • L2t + τ log(N)\nη ,\nsince ∆(X,XT+1) ≥ 0 and\n∆(X,X1) = X • (log(X)− log( τN I))− Tr(X) + τ = X • log( 1τX) + log(τ)Tr(X)− log( τN )Tr(X)− Tr(X) + τ ≤ Tr(X)(log(N)− 1) + τ ≤ τ log(N).\nThe first inequality above follows because Tr(X) ≤ τ , so log( 1τX) ≺ 0. The second inequality uses Tr(X) ≤ τ ."
    }, {
      "heading" : "B Technical Lemmas and Proofs",
      "text" : "Lemma 19. For m× n non-symmetric matrices W, if W = UΣV⊤ is the singular value decomposition of W, then\nsym(W) =\n[ 1√ 2 U 1√ 2 U\n1√ 2 V − 1√ 2 V\n][ Σ 0\n0 −Σ\n][ 1√ 2 U⊤ 1√ 2 V⊤\n1√ 2 U⊤ − 1√ 2 V⊤\n]\nis the eigenvalue decomposition of sym(W). In particular, ‖sym(W)‖⋆ = 2‖W‖⋆. 7To see this, note that we can write A = VDV⊤ for some orthonormal V and diagonal D. Therefore,\nI+A+A2 − eA = V ( I+D+D2 − eD ) V ⊤ .\nNow, by the inequality 1 + a + a2 − ea ≥ 0, which holds for all a ≤ 1, we obtain that all elements of the diagonal matrix ( I+D+D2 − eD ) are non-negative.\nProof. By the block matrix multiplication rule we have [\n1√ 2 U 1√ 2 U 1√ 2 V − 1√ 2 V\n][ Σ 0\n0 −Σ\n] [ 1√ 2 U⊤ 1√ 2 V⊤\n1√ 2 U⊤ − 1√ 2 V⊤\n]\n=\n[ 1√ 2 UΣ − 1√ 2 UΣ\n1√ 2 VΣ 1√ 2 VΣ\n] [ 1√ 2 U⊤ 1√ 2 V⊤\n1√ 2 U⊤ − 1√ 2 V⊤\n]\n=\n[ 0 UΣV⊤\nVΣU⊤ 0\n]\n=\n[ 0 W\nW⊤ 0\n]\n.\nIn addition, it is easy to check that the columns of\n[ 1√ 2 U 1√ 2 U\n1√ 2 V − 1√ 2 V\n]\nare orthonormal. It follows\nthat the above form is the eigendecomposition of sym(W). Therefore, for any Schatten norm: ‖sym(W)‖ = 2‖Σ‖ = 2‖W‖, which concludes our proof."
    }, {
      "heading" : "C The optimal cut in the Online Max Cut problem",
      "text" : "We prove Lemma 2, which we restate here for convenience.\nLemma 2 restated Consider an online sequence of loss functions {ℓt = 12 |yt − ŷy|}. Let\nW∗ = arg min W∈W\n∑\nt\nℓt(W (it, jt)) .\nThen W∗ = WA for the set A that determines the max cut in the weighted graph over [n] nodes whose weights are given by wij = ∑\nt:(it,jt)=(i,j) yt for every (i, j).\nProof. Consider WA. For each pair (i, j) let c + ij , c − ij be the total number of iterations in which the pair (i, j) appeared in the adversarial sequence with yt = 1 or yt = −1 respectively. Since ŷt ∈ [−1, 1] we can rewrite the total loss as:\n∑\nt\nℓt(WA(it, jt)) = 1\n2\n∑\n(i,j)\n[c+ij · (1−WA(i, j)) + c−ij · (1 +WA(i, j))]\n= 1\n2\n∑\n(i,j)\nWA(i, j) · (c−ij − c+ij) + CT\n= −1 2\n∑\n(i,j)\nWA(i, j) · wij + CT\nWhere CT is a constant which is independent of WA. Hence, minimizing the above expression is equivalent to maximizing the expression:\n∑\n(i,j)\nWA(i, j) · wij = 2 · ∑\n(i,j): WA(i,j)=1\nwij − ∑\n(i,j)\nwij.\nSince ∑\n(i,j)wij is a constant independent of A, the cut which maximizes this expression is the maximum cut in the weighted graph over the weights wij ."
    }, {
      "heading" : "D Optimality of Decomposition for Collaborative Filtering",
      "text" : "In this section, we prove the following theorem:\nTheorem 20. Consider the matrix W formed by taking m = τ√ n rows of an n × n Hadamard matrix. This matrix has ‖W‖⋆ = τ , and any (β, τ̃ )-decomposition for sym(W) has\nβτ̃ ≥ 1 4 τ √ n.\nProof. Since the rows of W are orthogonal to each other, the m singular values of W all equal √ n, and thus ‖W‖⋆ = m √ n = τ . Further, the SVD of W is (here, Im is the m×m identity matrix):\nW = Im( √ nIm)(\n1√ n W).\nUsing Lemma 19 the eigendecomposition of sym(W) can be written as\nsym(W) = U( √ nIm)U ⊤ +V(− √ nIm)V ⊤,\nwhere U = [ 1√\n2 Im, 1√ 2n W]⊤ and V = [ 1√ 2 Im,− 1√2nW] ⊤\nare p×m matrices with orthonormal columns. Let sym(W) = P − N be a (β, τ̃ )-decomposition. Now consider the following matrices: first, define the p× p diagonal matrix\nD :=\n[ 1√ 2m Im 0\n0 √ mn\n2 √ 2τ̃ In\n]\n.\nFinally, define the p× p positive semidefinite matrix\nY := DUU⊤D.\nSince U has orthonormal columns we have UU⊤ Ip, and so\nY DIpD = D2.\nNow, consider\nY • sym(W) = Y • (P−N) ≤ Y •P (∵ Y,N 0, so Y •N ≥ 0) ≤ D2 •P (∵ Y D2)\n=\nm∑\ni=1\n1\n2m P (i, i) +\np ∑\ni=m+1\nmn\n8τ̃ P (i, i)\n≤ 1 2 β + mn 8τ̃ τ,\nsince P (i, i) ≤ β for all i and Tr(P) ≤ τ . We also have\nY • sym(W) = Tr(DUU⊤Dsym(W)) = Tr(UU⊤Dsym(W)D)\n=\n√ n\n4τ̃ Tr(UU⊤sym(W)) (∵ Dsym(W)D = sym(\n√ n 4τ̃ W))\n=\n√ n\n4τ̃ Tr(UU⊤[U(\n√ nIm)U ⊤ +V(− √ nIm)V ⊤])\n= mn\n4τ̃ ,\nsince U⊤V = 0. Putting the above two inequalities together, we have\nmn 4τ̃ ≤ 1 2 β + mn 8τ̃ ,\nwhich implies that\nβτ̃ ≥ 1 4 mn = 1 4 τ √ n\nas required."
    }, {
      "heading" : "E Relation between (β, τ)-decomposition, max-norm and trace-",
      "text" : "norm\nIn this section, we consider m×n non-symmetric matrix W. The max-norm of W is defined to be (see Lee et al. [2010]) the value of the following SDP:\nmin t [\nY1 W W⊤ Y2\n]\n0\n∀i ∈ [m], j ∈ [n] : Y1(i, i), Y2(j, j) ≤ t. (8)\nThe least possible β in any (β, τ)-decomposition for W is given by the following SDP:\nmin β [\n0 W\nW⊤ 0\n]\n= P−N\nP, N 0 ∀i ∈ [m+ n] : P (i, i), N(i, i) ≤ β. (9)\nTheorem 21. The least possible β in any (β, τ)-decomposition exactly equals half the max-norm of W.\nProof. Let t∗ and β∗ be the optima of SDPs (8) and (9) respectively. Let Y1, Y2 be the optimal solution to SDP (8), so that for all i ∈ [m], j ∈ [n] we have Y1(i, i), Y2(j, j) ≤ t∗. Consider the matrices\nP = 1\n2\n[ Y1 W\nW⊤ Y2\n]\nand N = 1\n2\n[ Y1 −W\n−W⊤ Y2\n]\n.\nUsing the feasibility of Y1, Y2 and Lemma 22, we get that P,N 0. Thus this is a feasible solution to SDP (9). Hence, we conclude that t∗ ≥ 2β∗.\nNow let P, N be the optimal solution to SDP (8), so that for all i ∈ [m + n] we have P (i, i), N(i, i) ≤ β∗. Consider the blocks of P and N formed by the first m indices and the last n indices:\nP =\n[ PA PB\nPC PD\n]\nand N =\n[ NA NB\nNC ND\n]\n.\nSince N 0, by Lemma 22 the following matrix is positive semidefinite as well:\nN′ := [ NA −NB −NC ND ]\n0.\nSo P+N′ 0, i.e. P+N′ = [ PA +NA W\nW⊤ PD +ND\n]\n0.\nThus, Y1 = P A +NA and Y2 = P D +ND is a feasible solution to SDP (8). Now for all i ∈ [m] we have Y1(i, i) ≤ PA(i, i) + NA(i, i) ≤ 2β∗, and similarly for all j ∈ [n] we have Y2(j, j) ≤ 2β∗. Thus, we conclude that t∗ ≤ 2β∗.\nLemma 22. Let P be a positive semidefinite matrix of order m+ n and let\nP =\n[ PA PB\nPC PD\n]\n.\nbe the block decomposition of P formed by the first m indices and the last n indices. Then the following matrix is positive semidefinite:\nP′ := [ PA −PB −PC PD ] .\nProof. Since P 0, there are vectors vi, for all i, j ∈ [m + n] such that P (i, j) = vi · vj. Then consider the vectors\nwi :=\n{\nvi if i ∈ [m] −vi otherwise.\nIt is easy to check that for all i, j ∈ [m + n] we have P ′(i, j) = wi · wj. Thus, we conclude that P′ 0.\nFinally, we show the connection between the trace-norm and the least possible τ in any (β, τ)decomposition:\nTheorem 23. The least possible τ in any (β, τ)-decomposition exactly equals twice the trace-norm of W.\nProof. Let τ∗ be the least possible value of τ in any (β, τ)-decomposition, and let P,N be positive semidefinite matrices such that sym(W) = P − N and Tr(P) + Tr(N) = τ∗. Then by triangle inequality, we have\n‖sym(W)‖⋆ ≤ ‖P‖⋆ + ‖N‖⋆.\nSince ‖sym(W)‖⋆ = 2‖W‖⋆, ‖P‖⋆ = Tr(P), and ‖N‖⋆ = Tr(N), we conclude that τ∗ ≥ 2‖W‖⋆. Now, let\nsym(W) = ∑\ni\nλiviv ⊤ i\nbe the eigenvalue decomposition of sym(W). Now consider the positive semidefinite matrices\nP = ∑\ni: λi≥0 λiviv\n⊤ i and N =\n∑\ni: λi<0\n−λiviv⊤i .\nClearly sym(W) = P−N, and\nTr(P) + Tr(N) = ∑\ni\n|λi| = ‖sym(W)‖⋆ = 2‖W‖⋆.\nHence, τ∗ ≤ 2‖W‖⋆, completing the proof."
    } ],
    "references" : [ {
      "title" : "Can we learn to gamble efficiently",
      "author" : [ "J. Abernethy" ],
      "venue" : "In COLT,",
      "citeRegEx" : "Abernethy.,? \\Q2010\\E",
      "shortCiteRegEx" : "Abernethy.",
      "year" : 2010
    }, {
      "title" : "Lower Bounds for the Helmholtz Function",
      "author" : [ "S. Golden" ],
      "venue" : "Physical Review,",
      "citeRegEx" : "Golden.,? \\Q1965\\E",
      "shortCiteRegEx" : "Golden.",
      "year" : 1965
    }, {
      "title" : "Regularization techniques for learning with matrices",
      "author" : [ "S. Kakade", "S. Shalev-Shwartz", "A. Tewari" ],
      "venue" : null,
      "citeRegEx" : "Kakade et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Kakade et al\\.",
      "year" : 2010
    }, {
      "title" : "Learning hurdles for sleeping experts",
      "author" : [ "V. Kanade", "T. Steinke" ],
      "venue" : "In Innovations in Theoretical Computer Science,",
      "citeRegEx" : "Kanade and Steinke.,? \\Q2012\\E",
      "shortCiteRegEx" : "Kanade and Steinke.",
      "year" : 2012
    }, {
      "title" : "Regret bounds for sleeping experts and bandits",
      "author" : [ "R. Kleinberg", "A. Niculescu-Mizil", "Y. Sharma" ],
      "venue" : "Machine learning,",
      "citeRegEx" : "Kleinberg et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Kleinberg et al\\.",
      "year" : 2010
    }, {
      "title" : "Practical large-scale optimization for max-norm regularization",
      "author" : [ "J. Lee", "B. Recht", "R. Salakhutdinov", "N. Srebro", "J.A. Tropp" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Lee et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Lee et al\\.",
      "year" : 2010
    }, {
      "title" : "Collaborative filtering with the trace norm: Learning, bounding, and transducing",
      "author" : [ "O. Shamir", "S. Shalev-Shwartz" ],
      "venue" : "In 24th Annual Conference on Learning Theory (COLT),",
      "citeRegEx" : "Shamir and Shalev.Shwartz.,? \\Q2011\\E",
      "shortCiteRegEx" : "Shamir and Shalev.Shwartz.",
      "year" : 2011
    }, {
      "title" : "Sample complexity of trace-norm",
      "author" : [ "O. Shamir", "N. Srebro" ],
      "venue" : "In COLT,",
      "citeRegEx" : "Shamir and Srebro.,? \\Q2011\\E",
      "shortCiteRegEx" : "Shamir and Srebro.",
      "year" : 2011
    }, {
      "title" : "Inequality with applications in statistical mechanics",
      "author" : [ "C.J. Thompson" ],
      "venue" : "Journal of Mathematical Physics,",
      "citeRegEx" : "Thompson.,? \\Q1965\\E",
      "shortCiteRegEx" : "Thompson.",
      "year" : 1965
    }, {
      "title" : "Matrix exponentiated gradient updates for on-line learning and bregman projection",
      "author" : [ "K. Tsuda", "G. Ratsch", "M.K. Warmuth" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Tsuda et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Tsuda et al\\.",
      "year" : 2006
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "In particular, this resolves (in the affirmative) an open problem posed by Abernethy [2010], Kleinberg et al.",
      "startOffset" : 75,
      "endOffset" : 92
    }, {
      "referenceID" : 0,
      "context" : "In particular, this resolves (in the affirmative) an open problem posed by Abernethy [2010], Kleinberg et al. [2010]. Finally, we derive lower bounds for the three problems and show that our upper bounds are optimal up to logarithmic factors.",
      "startOffset" : 75,
      "endOffset" : 117
    }, {
      "referenceID" : 0,
      "context" : "In particular, this resolves (in the affirmative) an open problem posed by Abernethy [2010], Kleinberg et al. [2010]. Finally, we derive lower bounds for the three problems and show that our upper bounds are optimal up to logarithmic factors. In particular, our lower bound for the online collaborative filtering problem resolves another open problem posed by Shamir and Srebro [2011].",
      "startOffset" : 75,
      "endOffset" : 385
    }, {
      "referenceID" : 0,
      "context" : "This resolves an open problem posed in Abernethy [2010], Kleinberg et al.",
      "startOffset" : 39,
      "endOffset" : 56
    }, {
      "referenceID" : 0,
      "context" : "This resolves an open problem posed in Abernethy [2010], Kleinberg et al. [2010]. Achieving this kind of regret bound was widely considered intractable, since computing the best permutation in hindsight is exactly the NP-hard minimum feedback arc set problem.",
      "startOffset" : 39,
      "endOffset" : 81
    }, {
      "referenceID" : 0,
      "context" : "This resolves an open problem posed in Abernethy [2010], Kleinberg et al. [2010]. Achieving this kind of regret bound was widely considered intractable, since computing the best permutation in hindsight is exactly the NP-hard minimum feedback arc set problem. In fact, Kanade and Steinke [2012] tried to show computational hardness for this problem by reducing the problem of online agnostic learning of halfspaces in a restricted setting to it.",
      "startOffset" : 39,
      "endOffset" : 295
    }, {
      "referenceID" : 6,
      "context" : "This matches an upper bound on the sample complexity derived by Shamir and Shalev-Shwartz [2011] and solves an open problem posed by Shamir and Srebro [2011].",
      "startOffset" : 64,
      "endOffset" : 97
    }, {
      "referenceID" : 6,
      "context" : "This matches an upper bound on the sample complexity derived by Shamir and Shalev-Shwartz [2011] and solves an open problem posed by Shamir and Srebro [2011].",
      "startOffset" : 64,
      "endOffset" : 158
    }, {
      "referenceID" : 2,
      "context" : "However, applying the online mirror descent algorithmic framework with an appropriate squared-Schatten norm regularization, as described in [Kakade et al., 2010], leads to a regret bound that again scales like O( √ n2T ).",
      "startOffset" : 140,
      "endOffset" : 161
    }, {
      "referenceID" : 2,
      "context" : "As before, applying the technique of Kakade et al. [2010] leads to a regret bound that scales as √ τ2T , which leads to trivial results in the most relevant case where τ = Θ( √ mn).",
      "startOffset" : 37,
      "endOffset" : 58
    }, {
      "referenceID" : 6,
      "context" : "This matches an upper bound given by Shamir and Shalev-Shwartz [2011]. The question of determining the sample complexity of W in the batch setting has been posed as an open problem by Shamir (who conjectured that it scales like n1.",
      "startOffset" : 37,
      "endOffset" : 70
    }, {
      "referenceID" : 4,
      "context" : "This upper bound is near-optimal, as Kleinberg et al. [2010] essentially prove the following lower bound on the regret:",
      "startOffset" : 37,
      "endOffset" : 61
    }, {
      "referenceID" : 9,
      "context" : "The algorithm, forms of which independently appeared in the work of Tsuda et al. [2006] and Arora and Kale [2007], performs exponentiated gradient steps followed by Bregman projections onto K.",
      "startOffset" : 68,
      "endOffset" : 88
    }, {
      "referenceID" : 9,
      "context" : "The algorithm, forms of which independently appeared in the work of Tsuda et al. [2006] and Arora and Kale [2007], performs exponentiated gradient steps followed by Bregman projections onto K.",
      "startOffset" : 68,
      "endOffset" : 114
    }, {
      "referenceID" : 9,
      "context" : "Algorithm 1 has the following regret bound (essentially following Tsuda et al. [2006], Arora and Kale [2007], also proved in Appendix A for completeness): Theorem 13.",
      "startOffset" : 66,
      "endOffset" : 86
    }, {
      "referenceID" : 9,
      "context" : "Algorithm 1 has the following regret bound (essentially following Tsuda et al. [2006], Arora and Kale [2007], also proved in Appendix A for completeness): Theorem 13.",
      "startOffset" : 66,
      "endOffset" : 109
    }, {
      "referenceID" : 9,
      "context" : "This is based on the following theorem that is essentially proved in Tsuda et al. [2006]: This construction is tight: e.",
      "startOffset" : 69,
      "endOffset" : 89
    } ],
    "year" : 2012,
    "abstractText" : "In several online prediction problems of recent interest the comparison class is composed of matrices with bounded entries. For example, in the online max-cut problem, the comparison class is matrices which represent cuts of a given graph and in online gambling the comparison class is matrices which represent permutations over n teams. Another important example is online collaborative filtering in which a widely used comparison class is the set of matrices with a small trace norm. In this paper we isolate a property of matrices, which we call (β, τ)decomposability, and derive an efficient online learning algorithm, that enjoys a regret bound of Õ( √ β τ T ) for all problems in which the comparison class is composed of (β, τ)-decomposable matrices. By analyzing the decomposability of cut matrices, triangular matrices, and low tracenorm matrices, we derive near optimal regret bounds for online max-cut, online gambling, and online collaborative filtering. In particular, this resolves (in the affirmative) an open problem posed by Abernethy [2010], Kleinberg et al. [2010]. Finally, we derive lower bounds for the three problems and show that our upper bounds are optimal up to logarithmic factors. In particular, our lower bound for the online collaborative filtering problem resolves another open problem posed by Shamir and Srebro [2011].",
    "creator" : "LaTeX with hyperref package"
  }
}