{
  "name" : "1605.07774.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Generalized Mirror Descents in Congestion Games∗",
    "authors" : [ "Po-An Chen", "Chi-Jen Lu" ],
    "emails" : [ "poanchen@nctu.edu.tw.", "cjlu@iis.sinica.edu.tw." ],
    "sections" : [ {
      "heading" : null,
      "text" : "We answer this question positively in the bulletin-board model by showing that when employing the mirror-descent algorithm, a well-known generic no-regret algorithm, the actual plays converge quickly to equilibria in nonatomic congestion games. This gives rise to a family of algorithms, including the multiplicative updates algorithm and the gradient descent algorithm as well as many others. Furthermore, we show that our dynamics achieves good bounds on the outcome quality in terms of the price-of-anarchy type of measures with two different social costs: the average individual cost and the maximum individual cost.\nFinally, the bandit model considers a probably more realistic and prevalent setting with only partial information, in which at each time step each player only knows the cost of her own currently played strategy, but not any costs of unplayed strategies. For the class of atomic congestion games, we propose a family of bandit algorithms based on the mirror-descent algorithms previously presented, and show that when each player individually adopts such a bandit algorithm, their joint (mixed) strategy profile quickly converges with implications.\n∗Part of the results in this paper have appeared in preliminary form in the proceedings of AAMAS 2014 [13] and as an extended abstract in AAMAS 2015 [14]. †Institute of Information Management, National Chiao Tung University, Taiwan. Email: poanchen@nctu.edu.tw. Supported in part by NSC 102-2221-E-009-061-MY2. ‡Institute of Information Science, Academia Sinica, Taiwan. Email: cjlu@iis.sinica.edu.tw.\nar X\niv :1\n60 5.\n07 77\n4v 2\n[ cs\n.G T\n] 1\n3 O\nct 2\n01 6"
    }, {
      "heading" : "1 Introduction",
      "text" : "Nash equilibrium is a widely-adopted solution concept in game theory, which is used for predicting the outcomes of systems consisting of self-interested players. We are interested in repeated game play, and a Nash equilibrium describes a steady state in which the system would stay once it is reached. However, this raises the issue of how such a state can be reached. In fact, for a general game, computing a Nash equilibrium is believed to be hard (according to the PPAD-hardness results [15]), so an equilibrium may not be reached in a reasonable amount of time in general, and the outcomes that we have observed may all be far out of any equilibrium, which would render the study on equilibria meaningless. To address this issue, a line of research is to consider natural efficient dynamics which players have incentive to follow, and study how the system evolves according to such dynamics.\n• Best or better response dynamics. One natural dynamics is the best or better response dynamics, in which a deviating player at each time makes a best or better change in his/her strategy to improve his/her payoff given the current choice of the other players. This means that, for a player to deviate, there must be enough information regarding the current choice that the others had made. It is well-known that such dynamics leads to pure Nash equilibria in congestion games. However, a player may not have incentive to play this way because making such deviations may not be beneficial if other players also deviate at the same time.\n• Generic no-regret dynamics. One may argue that a plausible incentive for a player is to maximize his/her average payoff through time, and dynamics based on “no-regret” algorithms from the area of online learning (e.g., [34, Chapter 4]) have thus been proposed in the study. The no-regret property is preserved in full or partial information models of feedback by a variety of algorithms. For a nonatomic routing game, it is known that if each infinitesimal player plays any arbitrary no-regret algorithm, the “time-averaged” flow and flows at most time steps would be at some type of approximate Nash equilibrium [10]. For a “socially concave” game, a similar time-averaged convergence result is also known [20].1 Convergence to a Nash or approximate Nash equilibrium is not always the case in general, and playing arbitrary no-regret algorithms can result in a larger set of outcomes than Nash equilibria, namely coarse correlated equilibria.2 Nevertheless, if one only cares about the outcome quality and the quality is measured by the price of anarchy [30] with the average individual cost, it is known that the price of total anarchy achieved by such no-regret algorithms can still match the price of anarchy at Nash equilibrium in special games, such as atomic congestion games [11] or even a wider class of smooth games [36]. On the other hand, there are broad classes of games and natural measures of outcome quality for which large gaps are known between no-regret outcomes and Nash equilibria.\nNotice that the convergence results mentioned above are, instead of the convergence of the actual strategy, about the convergence of the time-averaged strategy [10, 20] or flows at most time steps being close to equilibria [10]. Even in the latter case, those time steps where flows are close to equilibria are arbitrarily distributed over time (not guaranteed to gather toward the end in time), which means that a flow at some very late point in time can still be far away from any equilibria and flows may not stabilize. The guarantees are still not\n1Note that the games that we consider here are not socially concave. 2See, for example, [37, Proposition 3.1] among others that discuss this.\non the convergence of the actual plays. Such results are useful if the goal is to solve the computational problem of computing an approximate Nash equilibrium, but they may not tell us much about how the system actually evolves. In particular, even though the timeaveraged play converges to an equilibrium or most plays are close to equilibria, the actual strategy may not converge and may be far away from an equilibrium. For many applications, for example, that require the system to stabilize, the time-averaged play convergence or most plays being close to equilibria may not be enough.\n• Multiplicative updates dynamics with full information. Although it is nice to be able to have general positive results on what generic no-regret algorithms can achieve, one may wonder if going from generic no-regret algorithms to specific ones could yield stronger results, in terms of convergence or quality of outcomes that the algorithms converge to. One of the best known no-regret algorithms is the Multiplicative Updates (MU) algorithm [31, 25]. Kleinberg et al. [28] studied this for atomic congestion games in the full information setting, in which players have full information about the cost functions so that they can determine the cost of every other strategy they could have used given other players strategies at the current round. It was shown that if each player employs such an MU algorithm, the actual joint mixed strategy profile of players converges to a pure Nash equilibrium with high probability for most games. Note that here it is the actual joint strategy profile, instead of the timeaveraged one, which converges. Furthermore, since the set of pure Nash equilibria can be a very small subset of correlated equilibria, the price of total anarchy achieved this way can be much smaller than that by a generic no-regret algorithm.\n• Multiplicative updates dynamics with bulletin-board posting. In another work [29], Kleinberg et al. studied the smaller class of load balancing games, but in the more stringent partial-information setting of the “bulletin-board” model, in which players only know the actual cost value of each edge according to the actual strategies played at the current round. They showed that if all the players play according to a common distribution (i.e., mixed strategy) and update the distribution using such an MU algorithm, the common distribution converges to some symmetric equilibrium of the nonatomic version of the game. As a result, the price of total anarchy achieved this way is also considerably smaller than that by a generic no-regret one. However, their analysis relies crucially on the assumption that all the players at each round play according to the same distribution. This assumption may not be reasonable in other settings or in other games, which makes the applicability of their analysis somewhat limited. On the other hand, the analysis in [28] can do without the assumption and deal with general asymmetry in players’ probability distributions, but it only works in the full information model.\nNote that there is no equilibrium selection for the best (or better) response dynamics and generic no-regret dynamics, and they could converge to the worst corresponding equilibrium. Nonetheless, the results of multiplicative updates dynamics suggest that the dynamics converge to a subset of mixed outcomes, namely, pure Nash equilibria with high probability in [28] and quite uniformly distributed mixed Nash equilibria in the case of bulletin-board load balancing [29, Lemma 6]; the price-of-anarchy type of efficiency gets better since the worst (mixed) Nash equilibrium in the induced subset could be better than the worst the worst coarse correlated equilibrium, meaning equilibrium is selected by the dynamics.\nThese results of multiplicative updates, which form a good comparison and complement to each other, along with the results on generic no-regret plays motivate our quest for other classes of learning dynamics in suitable classes of games and settings. Are there more cases of natural no-regret dynamics that perform well in suitable classes of games in terms of convergence time and quality of outcomes that the dynamics converge to? We first answer this question positively by providing a family of such dynamics in the bulletin-board model for the class of nonatomic congestion games with cost functions of bounded slopes. More precisely, we show that in such a game, if each infinitesimal player individually plays some type of the mirror-descent algorithm [7], a well-known general no-regret algorithm, then their joint strategy profile quickly converges to an approximate notion of Wardrop equilibrium.3 We also show that our dynamics achieves good bounds on the quality of outcomes in terms of the price-of-anarchy type of measures with two different social costs: the average individual cost and the maximum individual cost.\nAll the previously mentioned results are based on somewhat generous information models. For instance of congestion games, edge cost functions are assumed common knowledge in the full information model of [28] so that players can determine the costs of currently unplayed strategies if they were used. A bit more stringent information model than full information was considered for the load-balancing games in [29] and for the general congestion games in our results previously mentioned, in which the edge cost functions are not common knowledge anymore, but still the cost values of all paths at each step are assumed available through “bulletin-board” posting. With such global information, players can get a grasp of the costs corresponding to played and even unplayed strategies, which allows players to update their strategies better and makes convergence of the whole system potentially easier. However, such an assumption on the information availability may not always be realistic and may limit the applicability of these results.\nThe “bandit” model in online learning on the other hand considers a probably more realistic and prevalent setting, in which at each time step each player only knows the cost of her (or his) own currently played strategy, but not any costs of unplayed strategies. This gives rise to the dilemma between exploration and exploitation which players have to face. In the area of online learning, many bandit algorithms with no-regret guarantee have been developed, including for example those based on the multiplicative updates algorithm for the experts problem [3] and those based on the gradient-descent algorithm for online linear or convex optimization [1, 24]. However, not much is known in the area of game theory for playing repeated games in the bandit model. Although similar convergence results of average plays can be established immediately for bandit algorithms with noregret guarantee, we are not aware of any previous result establishing convergence of actual plays in the bandit setting. In fact, it is not clear how to design bandit algorithms with such convergence guarantee.\nA natural attempt, following the standard approach for designing bandit algorithms such as those in [3, 1, 24], is to come up with estimates of the true cost values and feed these estimates to a full information algorithm, to replace the true cost values that are available in the full information setting. For this approach to work for most problems in online learning, it simply suffices to guarantee these estimates being “unbiased”, in the sense that their expected values equal the true cost values. However, in the setting of repeated game playing, using such unbiased estimates does not seem to ensure convergence of actual plays in general. This is because these estimates, even with guarantee on expected values, can still have high variance and thus have actual values\n3Wardrop equilibrium can be seen as Nash equilibrium specialized for games with infinitely many agents [39] such as nonatomic congestion games.\nvery different from the true cost values. Unfortunately, this is indeed the case for adapting most existing bandit algorithms, as their estimates actually can take very different values from the true cost values with some probability (for example, see the one-point gradient estimate in [24]) although these estimates are enough for their goal of just achieving no regret therein. In particular, when using such estimates in, for example, mirror descents, each update step may go in a very different (possibly in almost opposite) direction from the desired one according to the true cost values, which does not seem likely to result in convergence to equilibrium. This motivates us to ask the question: are there natural classes of bandit algorithms which selfish players individually have incentive to adopt (by the no-regret property) and the whole system will quickly converge to an approximate Nash equilibrium (even just for some classes of instances) with social cost guarantees?\nWe then answer this more challenging question affirmatively as well. For the class of atomic congestion games, we propose a family of bandit algorithms based on the mirror-descent algorithms presented in the first-half part, and we mainly show that when each player individually adopts such a bandit algorithm, their joint strategy profile quickly converges. The reasons why we focus on atomic congestion games instead of nonatomic ones are two-folded: the bandit algorithm for atomic congestion games can be applied for nonatomic congestion games, by treating the joint (mixed) strategy profile of an atomic game directly as the joint (pure) strategy profile (i.e., flow distribution) of a nonatomic game; the bandit informational setting is not well defined for nonatomic congestion game since players can split their flows to literally all the allowed paths and get all the path costs.\nIn the bandit model, each player can only update her own strategy according to very limited and local information about the whole system, but we show that when each player individually adopts any such bandit algorithm, the whole system still quickly converges, and is to an approximate mixed-strategy equilibrium that has a small approximation error in many natural cases (but has a large approximation error in general) and is beneficial to the society as a whole. This is not only reminiscent of the result of convergence to some specific mixed Nash equilibria in load balancing with bulletin posting [29] (so a better price-of-anarchy type of efficiency is possible), but also more generally for atomic congestion games like in [28]. This may appear even less expected than that in the bulletin-board or full-information model, where each player at least has more abundant and more global information available.\nWe provide definitions and some preliminaries in Section 2. First, the generalized mirror-descent algorithm and convergence result in the bulletin-board model are presented in Section 3. The bandit algorithm and convergence result are then presented in Section 4. Approximate equilibria and the outcome quality bounds in terms of the price-of-anarchy type of measures are discussed along with the convergence results. We summarize with conclusions and future work in Section 5."
    }, {
      "heading" : "1.1 Discussion of Our Results and Techniques",
      "text" : "Bulletin-Board Model\nThe mirror-descent algorithm in fact can be seen as a family of algorithms. By instantiating it properly, one can recover the MU algorithm, the gradient-descent algorithm, as well as many others, and our result establishes the fast convergence of all these algorithms at once. Let us stress that as in [28, 29], our notion of convergence is the stronger one: what converges is the actual joint strategy profile. Note that in the congestion game, different players naturally have different sets of strategies, so it is no longer reasonable to assume that all the players use the same distribution to play as in [29]. Therefore, we allow players to use different distributions and moreover, we allow\nplayers to update according to different learning rates. Still, we manage to prove the convergence, just as [28] but in the more difficult bulletin model and with a concrete bound on convergence time.\nFurthermore, we provide bounds on the price-of-anarchy type of measures achieved by our dynamics, in terms of the average individual cost and the maximum individual cost. Using the average individual cost as the social cost, we show that the ratio between the social cost achieved by our dynamics and the optimal one approaches some constant, which depends on the slopes of the cost functions. Using the maximum individual cost as the social cost, we show that the ratio between the social cost achieved by our dynamics and the optimal one also approaches the same constant in symmetric games. In each case, there is a tradeoff between the ratio we can achieve and the time it takes: by letting the system evolve for a longer time, it will get closer to an equilibrium, and the resulting ratio will approach closer to that constant.\nOur main technical contribution is the convergence of our dynamics to an approximate equilibrium. To show this, we consider a smooth convex potential function of the game which has the joint strategy profile of players as its input. The interesting observation is that although each player individually applies the mirror descent algorithm to his/her own strategy using costs related only to him/her, we show that the updates performed by all the players collectively can be seen as following some generalized mirror descent process on the potential function. The generalized mirror descent allows different step sizes in different dimensions,4 and we need this generalization because we allow different learning rates for different players. The standard mirror descent, on the other hand, has the same step size across all the dimensions, so that it moves at each time in exactly the opposite direction of the gradient vector. It is known that doing the standard mirror descent on a smooth convex function leads to a fast convergence to its minimum [9, 33]. However, our generalized mirror descent no longer moves in the opposite direction of the gradient vector as different step sizes have different scaling effects in different dimensions, and therefore it is not clear if the process would still converge. Interestingly, we show that a similar convergence result can also be achieved, which may be of independent interest (since this works for all games with a smooth convex potential function satisfying some properties). Finally, let us remark that the standard mirror descent algorithm, instead of the generalized one, has also been used for different problems in game theory: for finding market equilibria in Fisher markets [9] and convex potential markets [16]. Our convergence result for the generalized mirror descent algorithm is an extension of that for the standard one.\nBandit Model\nTo be able to achieve convergence in the bandit model, the first hurdle we have to overcome is for each player to have good enough estimates for the true costs of all her allowed paths. As discussed before, we would like these estimates to have actual values (rather than expected values) close to the true cost values, but the estimation methods used in [3, 1, 24] do not work. In fact, an apparent difficulty is that a player can only learn the cost of one single path at each time, but the cost of each path actually depends on how other players choose their paths at that time, which may be very different at different times. Then how can a player possibly obtain good estimates for the true costs of those unchosen paths at that time? Inspired by the bandit algorithm in [34, Chapter 4.6], we consider dividing the time steps into episodes and letting each player play the same (mixed) strategy at each step during an episode. The expected cost of each path then becomes the same for\n4This is similar to adaptive optimization methods such as in [19].\neach step in an episode, and this allows a player to obtain a good estimate for the expected cost of each allowed path, simply by choosing each path an enough number of steps and averaging the costs. With such good estimates, each player can then update her strategy for the next episode by feeding these estimates to the bulletin-board mirror-descent algorithm, to replace the true path costs it needs.\nTo prove our convergence results, we would like to follow the approach used in the bulletinboard model by showing that the collective update of all the players together corresponds to doing some generalized mirror descent on a convex function. However, as we consider the atomic version of the congestion game instead of the nonatomic version, there are more hurdles that we need to clear. The first is the choice of the function for analyzing the convergence of our dynamics, as the potential function Φ used in the bulletin-board model is defined over nonatomic flows of players. Actually, the function used to show the convergence of dynamics does not have to be the potential function (for ensuring existence of equilibria) of the game under study as long as the converged outcomes via such used function can be interpreted in the game under study. We find the same function Φ still suitable for us, by seeing each player’s mixed strategy as a nonatomic flow. Note that this has been similarly used in atomic load balancing [29] where their mixed strategies are like converging to equilibrium flows of the nonatomic version of load-balancing games, translated into quite uniformly distributed mixed Nash equilibrium in the atomic version. Yet, this causes a subtle problem. Namely, the gradient of Φ actually corresponds to path costs according to nonatomic flows rather than those according to atomic flows that players in our atomic game have access to.5 This results in a non-negligible amount of error in the estimation of the gradient vector, which seems unavoidable for nonlinear cost functions, and we can at best do an “approximate” mirror descent with such an approximate gradient vector.\nHowever, the convergence analysis in the bulletin-board model relies crucially on being able to move (in the mirror space) precisely in the oppositive direction of the gradient vector, in order to guarantee that the Φ value decreases, while it is not hard to find cases with increased Φ value when moving in a slightly different direction. We bypass this difficulty by showing that as long as the current Φ value compared to the minimum one is still considerably large, relative to the error of the approximate gradient vector, the next Φ value will decrease from the current one by some large amount. This provides us a way to bound the number of steps needed to reach a Φ value within some distance of the minimum one, with the distance dictated by the errors of the approximate gradient vectors.\nThe convergence also implies an approximate equilibrium in mixed strategies6 with a caveat that the approximation can be bad in general to make such equilibrium meaningless. Nevertheless, there are broad classes of instances when such approximate equilibrium is indeed meaningful: the approximation error can be small for some classes of cost functions, for example, linear cost functions and even more generally, when bounds (constant with respect to the amount of flow, not necessarily to the inputs of an instance) on second derivatives are small enough; for some classes of structures of allowed paths, the approximation error can still be small, for example, in the load-balancing setting like in [29].\n5Note that exactly the same problem was faced in studying multiplicative updates in atomic load balancing under the bulletin-board model [29], where they eventually showed the convergence of mixed strategies using the nonatomic version of load-balancing games and its potential function. Even without a bandit problem, an error due to this occurred there, too.\n6There can be alternative definitions of approximate equilibrium."
    }, {
      "heading" : "1.2 Related Work",
      "text" : "There are numerous works regarding reaching various notions of equilibria in congestion/potential games in general: PLS-completeness [21] and inapproximability [38] of computing Pure Nash equilibria, best-response types of dynamics for converging to approximate equilibria [17, 5] or connectedcomponent-based “sink equilibria” [26]. There are studies of no-regret algorithms in zero-sum game play [18], game play by selecting strategy profiles to query the corresponding payoffs [22], etc. Playing arbitrary no-“swap”-regret algorithms converges to correlated equilibria [34, Chapter 4.4.3] while playing arbitrary no-regret algorithms results in coarse correlated equilibria [37, Proposition 3.1]. That is to say that the no-regret property is enough to guarantee (coarse) correlated equilibria in general. On the other hand, our focus is to propose specific classes of no-regret algorithms for players to have incentives to adopt, with actual convergence guarantee even with bandit feedback.\nOur modeling framework in this article is most similar to that in [28]. A vector of the probabilities for the all available actions for each player is maintained. Players sample an action according to this distribution at each time. Initially, the probabilities can be all equal, i.e., uniform distribution. Every time each player updates the weights multiplicatively preferring actions of low cost, which generalizes the weighted majority algorithm introduced by Littlestone and Warmuth [31] and the Hedge algorithm of Freund and Schapire [25]. Kleinberg et al. showed that if players use such dynamics to adjust their strategies in atomic congestion games, then game play converges to a subset of mixed Nash equilibria, so-called weakly stable equilibria. Pure Nash equilibria are weakly stable by definition, and the converse was shown true with probability 1 when congestion costs are selected at random independently on each edge.\nKleinberg et al. [29] also studied the performance of learning algorithms in load-balancing games, i.e., congestion games on parallel links, under the “bulletin board model” in which players assess edge costs according to the actual cost incurred on that edge, and not the hypothetical cost if the player had used it. This algorithm for specifying a mixed strategy at each time step is a version of the Hedge algorithm [25], modified so that players assess edge costs according to the actual cost incurred on that edge, and not the hypothetical cost if the player had used it for players that do not use the edge at this time step. It was shown that the bulletin board variant of Hedge is also a no-regret learning algorithm. Their main result is that the expected makespan of the outcome is bounded by O(log n) where n is the number of links/players, exponentially better than the known lower bounds for arbitrary no-regret algorithms. Many of our assumptions regarding atomic splittable congestion games follow [29], and the analyses for convergence share some highlevel intuitions.\nEven-Dar et al. studied a subclass of concave games called socially concave games in [20]. They showed that if each player follows any no-external regret minimization procedure, then the dynamics will converge in the sense that the average action vector will converge to a Nash equilibrium. Even if we change convexity to concavity and costs to utilities in our paper, potential games that we consider here are not socially concave games. Thus, their results do not directly apply.\nBesides the dynamics based on no-regret learning algorithms, some other works design Markovian rerouting policies in congestion games [23, 2], where an agent’s behavior only depends on the outcome of the immediately previous round and not on all the previous rounds. In nonatomic congestion games, inspired by so-called replicator dynamics [40] the algorithms of Fisher at al. [23], for an agent, adaptively sample paths with a probability proportional to the fraction of agents using this path and reroute with a chosen probability if the latency of the sampled path is smaller than\nthat of the current one. They showed a bicriteria result, an upper bound on the total number of rounds in which it does not hold that almost all agents have a latency close to the average latency. Note that such “equilibria” are transient in the sense that these equilibria can be left again even after they are reached. This is more like in [10], but not the actual convergence in our stronger sense. Nevertheless, with exploration uniformly at random on unused paths, convergence to a Wardrop equilibrium can now be achieved, and for symmetric games they gave a polynomial bound on the number of rounds taken to get close to the optimal potential value.\nAckermann et al. [2] used what they called “concurrent imitation dynamics”, which is very similar to the rerouting policies of [23], in atomic congestion games and mainly gave similar results for symmetric atomic congestion games where the analysis needs to take probabilistic effects into account. They first showed a convergence to a local minimum of the potential in pseudopolynomial time. Their main result is a stronger bound on the expected time to reach an approximate stable state in which at most a very small fraction of the agents deviate by more than a very small fraction from the average latency. Finally, with a suitable combination of imitation with exploration that samples other strategies directly, they guaranteed convergence to Nash equilibria and gave the expected convergence time.\nFor a generic two-player coordination game, Mehta et al. [32] showed that, starting from all but a zero measure of initial probability distributions, a discrete multiplicative weight update algorithm known as discrete replicator dynamics converges to pure Nash equilibria. This is not like the randomized results in [28, 29]. Their results only require that any row/column of the payoff matrix consist of distinct entries, and hold even if the game has uncountably many Nash equilibria.\nThere is still a variety of different dynamics in repeated games. Auletta et al. [4] presented general bounds on the mixing time of “logit” dynamics for classes of strategic games. In the logit dynamics, individual participants act selfishly and keep responding according to some partial noisy knowledge in the complex system. In particular, they proved nearly tight bounds for potential games and games with dominant strategies. Kleinberg et al. [27] analyzed a game with a unique Nash equilibrium, but where natural learning dynamics only cycles, not converging to this equilibrium. They showed that the outcome of this learning process is optimal and has much better social welfare than the unique Nash equilibrium. Balcan et al. [6] showed that convergence may not lead to any meaningful notions of equilibria, but may result in good efficiency in terms of some objectives."
    }, {
      "heading" : "2 Preliminaries",
      "text" : "Nonatomic Congestion Games. In this article, we first consider the nonatomic congestion game described by (N,E, (Si)i∈N , (ce)e∈E), where N is the set of commodities, E is the set of edges (resources), Si ⊆ 2E is the collection of the allowed paths (the allowed subsets of resources) for commodity i, and ce is the cost function of edge e, which is a nondecreasing function of the amount of load on it. A commodity is a “pseudo-player”, which we simply call a “player” throughout the discussion of nonatomic congestion games, meaning that a player herself does not act as a selfish party, but the infinitesimal parties that a player is composed of do. We describe it in more detail in the following.\nLet us assume that N = {1, . . . , n}, |E| = m, and each player has a load of 1/n (so the total load is 1). Each player consists of a huge infinite number of selfish agents (or see each player as a group of infinitesimal players of the same type). The agents of player i split the load of player i so that each agent has a small (infinitesimal) amount ∆ of load, i.e., ∆ → 0. Each agent of\nplayer i must choose one single path s from Si and put that ∆ amount of load all on s. We call the “aggregated” result of such choices of agents of player i the strategy of player i, and it can be represented by a |Si|-dimensional vector xi = (xi,s)s∈Si , where xi,s ∈ [0, 1] is the amount of the load that the agents of player i puts on the path s. Note that ∑ s∈Si xi,s = 1/n and let Ki be the feasible set for all such vectors xi. Then the strategies of all players can be jointly represented by a vector\nx = (x1, ..., xn) = ((x1,s)s∈S1 , ..., (xn,s)s∈Sn) ∈ Rd, where d = ∑\ni∈N |Si|, and let K = K1 × · · · × Kn be the feasible set for all such vectors x. Each allowed path of a player intersects at most k allowed paths (including that path itself) of that player. We call xi the flow of player i and x the flow of the system.\n7 Note that an edge e ∈ E can be shared by different paths, and the aggregated load on e, denoted by `e(x), is ∑ s:e∈s ∑ i∈N xi,s.\nThe cost of a path s is defined as cs(x) = ∑\ne∈s ce(`e(x)), and the individual cost of player i is defined as Ci(x) = ∑ s∈Si xi,scs(x). Each agent of player i chooses a path s from Si that minimizes cs(x) in nonatomic congestion games. 8\nSuch a game admits the following potential function (e.g., see [35, Eq. (1)], and it can be found as early as in [8]):9\nΦ(x) = ∑ e ∫ `e(x) 0 ce(y)dy. (1)\nTo see that this is indeed a potential function, note that if some player deviates an infinitesimal fraction of load from s to s′ (where xi,s > 0) such that cs(x) > cs′(x\n′) (where x is almost the same as x′ except for the small fraction of moved load), then ∂Φ(x)/∂xi,s > ∂Φ(x\n′)/∂xi,s′ , which means that the rate of decrease in Φ is larger than the rate of increase in Φ and thus the resulting Φ decreases. We will need the following, which we prove in Appendix A.\nProposition 1. The function Φ defined in (1) is convex.\nAtomic Congestion Games. We later consider the atomic congestion game, also described by (N,E, (Si)i∈N , (ce)e∈E). Let us assume that there are n players, each player has at most d allowed paths, and each path has length at most m; let k be the maximum number of the allowed paths (including that path itself) of a player that each allowed path of that player intersects, and each player has a flow of amount 1/n to route. The strategy of each player i is to send her entire flow on a single path, chosen randomly according to some distribution over her allowed paths, which can be represented by a |Si|-dimensional vector πi = (πi,s)s∈Si , where πi,s ∈ [0, 1] is the probability of choosing path s. It turns out to be more convenient for us to represent each player’s strategy πi by an equivalent form xi = (1/n)πi, where 1/n is the amount of flow each player has. That is, for every i ∈ N and s ∈ Si, xi,s = (1/n)πi,s ∈ [0, 1/n] and ∑ s∈Si xi,s = 1/n. Let Ki denote the feasible set of all such xi ∈ [0, 1/n]|Si| for player i, and let K = K1 × · · · × Kn, which is the feasible set of all such joint strategy profiles x = (x1, . . . , xn) of the n players.\n7Although we borrow the terms such as edge, path, and flow from routing games, the congestion games are more general as there are no underlying graphs and a path can be just any arbitrary subset of edges.\n8One can show that our dynamics are no-regret algorithms for each agent of each player i, and this provides an incentive for the agents to adopt the dynamics. If, instead of each agent choosing a path s to minimize cs(x), the agents of player i cooperate to minimize the cost of player i, Ci(x), we have an atomic splittable congestion game.\n9Note that our convergence result will be proved more generally for any convex potential function satisfying certain properties.\nWe will still be using the same function as in (1) for convergence analysis. We are aware of the potential function of Rosenthal for atomic congestion games (see [28] for the potential function there), typically used for showing existence of pure Nash equilibria. Actually, different functions could be used for different purposes. For our purpose of doing mirror-gradient descents on some convex function to show actual convergence, the function that we are using guarantees convexity (while the Rosenthal one does not) and thereby other convenience for analysis.\nTo represent which path a player i actually chooses, we use another vector Xi = (Xi,s)s∈Si ∈ {0, 1/n}|Si|, where\nXi,s = { 1/n if player i chooses path s, 0 otherwise.\n(2)\nWe call X = (X1, . . . , Xn) the choice vector of the n players. Then the cost of a path s with respect to X is defined as cs(X) = ∑ e∈si ce(`e(X)),\nwhere `e(X) is the amount of flow passing through edge e, defined as `e(X) = ∑ j∈N ∑ r∈Sj :e∈r Xj,r. (3)\nA useful property of the choice vector X is that as each player i chooses path s with probability πi,s = xi,sn, the expected value of each Xi,s is exactly xi,s, which implies that E [Xi] = xi for each i and E [X] = x.\nProperties and Social Costs. As in [29], we assume that the cost functions satisfy the property that for any y ∈ [0, 1] and any e ∈ E, ce(0) = 0, ce(1) ≤ 1, c′e(y) ≥ A > 0 and 0 ≤ c′′e(y) ≤ B, where A,B are positive constants.10 By Lemma 4 of [29], for constants a = A and b = B + 1 defined accordingly, the cost functions satisfy the condition that\nay ≤ ce(y) ≤ by, for any y ∈ [0, 1]. (4)\nThen the function Φ defined in terms of such cost functions is smooth in the following sense.\nDefinition 1. A function Φ over K is called (α, β, λ)-smooth if for any x ∈ K,\nΦ(x) ≤ α, ‖∇Φ(x)‖∞ ≤ β, and ∇2Φ(x) λI.\nProposition 2. The function Φ defined in (1) with cost functions satisfying condition (4) is (α, β, λ)-smooth, for\nα = bm/2, β = bm, and λ = bmk.\nWe will prove the proposition in Appendix B. We consider two types of social cost functions. The first is the average individual cost function,\ndefined as CA(x) = ∑ e `e(x)ce(`e(x)),\n10They are constants with respect to the amount of flow, not necessarily to m or n.\nand the second is the maximum individual cost function, defined as\nCM (x) = max s∈S ∑ e∈s ce(`e(x)), where S = ⋃ i∈N Si.\nUsing them, we measure the quality of outcome for a flow x ∈ K in the following two ways. The first is the ratio CA(x)/CA(x\n∗), where x∗ = arg minz∈K CA(z), and the second is the ratio CM (x)/CM (x̂), where x̂ = arg minz∈K CM (z)."
    }, {
      "heading" : "3 The Bulletin-Board Model",
      "text" : ""
    }, {
      "heading" : "3.1 Dynamics",
      "text" : "We consider the setting in which the players play the game iteratively in the following way. At step t, each player i plays the strategy xti by sending the amount x t i,s of load on path s for each s ∈ Si. After that, she gets to know the vector ĉti = (cs(x t))s∈Si of cost values, where cs(x t) = ∑ e∈s ce(`e(x t)) is the cost value on the path s at that step. With this, she updates her next strategy xt+1i in some way and then proceeds to the next iteration. In the alternative definition of the game, the corresponding setting is that at step t, each agent of player i sends its load of ∆ all on some path s ∈ Si, which is chosen according to some distribution. We assume that all agents of player i start with the same initial distribution and update their distributions at each step t using the same algorithm according to the same information ĉti. Then we can conclude that their distributions at step t are all the same,11 which basically can be described by the flow xti of player i, due to the law of large number as the number of agents is huge. Thus, the settings for the two definitions of the game also match.\nWe have not specified how the players or agents of players update their next strategies. Different update algorithms may make the whole system evolve in rather different ways, and we would like to understand if there are update algorithms which players or agents of players have incentive to adopt that can lead to desirable outcomes for the whole system. One can argue that a plausible incentive for a player is to minimize her regret. Two well-known no-regret algorithms are the gradient descent algorithm and the multiplicative update algorithm, both of which can be seen as special cases of a more general algorithm called mirror descent algorithm (see e.g. [7] for more detail). Inspired by this, we consider the following update rule for player i or agents of player i:\nxt+1i = arg min zi∈Ki\n{ ηi〈ĉti, zi〉+ BRi(zi, xti) } (5)\n= arg min zi∈Ki\nBRi(zi, xti − ηiĉti). (6)\nHere, ηi > 0 is some learning rate, Ri : Ki → R is some regularization function, and BRi(·, ·) is the Bregman divergence with respect to Ri defined as\nBRi(ui, vi) = Ri(ui)−Ri(vi)− 〈∇Ri(vi), ui − vi〉\nfor ui, vi ∈ Ki. This gives rise to a family of update rules for different choices of the function Ri. For example, it is well-known that by choosing Ri(ui) = ‖ui‖22/2, one recovers the gradient descent algorithm, while by choosing Ri(ui) = ∑ s(ui,s lnui,s−ui,s), one recovers the multiplicative update\n11The distributions of agents from different players are still different in general.\nalgorithm. Using a similar argument as in [29], one can show that this algorithm, with a properly chosen Ri, is indeed a no-regret algorithm for each agent of player i (see Appendix C for a proof sketch), and this provides an incentive for the agents to use the algorithm. We need these Ri’s (and BRi(·, ·)’s) to satisfy the following. The choices of Ri’s that satisfy this assumption will be discussed in Section 3.2.2.\nAssumption 3. For any i ∈ N and any xi, yi ∈ Ki,\n‖xi − yi‖22 ≤ 2 · BRi(xi, yi).\nThen the function Φ is “smooth” with respect to these Ri’s in the following sense by Definition 1 and Assumption 3.\nDefinition 2. We say that Φ is λ-smooth with respect to (R1, . . . , Rn) if for any two inputs x = (x1, . . . , xn) and x ′ = (x′1, . . . , x ′ n) in K,\nΦ(x′) ≤ Φ(x) + 〈∇Φ(x), x′ − x〉+ λ n∑ i=1 BRi(x′i, xi). (7)"
    }, {
      "heading" : "3.2 Convergence Results and Equilibria",
      "text" : "Our main result in this section is the following, which shows that if each player (or agent of a player) uses such an update algorithm, the system quickly converges, in the sense that the value of the potential function Φ(xt) quickly approaches the minimum Φ(q), where q = arg minz∈K Φ(z).\nTheorem 4. Consider any nonatomic congestion game of n players, with a potential function Φ which is λ-smooth with respect to some (R1, . . . , Rn). Let q = (q1, . . . , qn) = arg minz∈K Φ(z). Now suppose that each player i starts from some initial strategy x0i , with BRi(qi, x0i ) ≤ γ, and updates her strategy according to the rule in (5), with ηi ∈ [η, 1/λ] for some η. Then for any ε ∈ (0, 1) there exists some Tε ≤ nγ/(ηε) such that for any t ≥ Tε, Φ(xt) ≤ Φ(q) + ε.\nWe will prove this main result right after presenting its derived result. From Theorem 4, we have the following, which we will prove in Section 3.2.2.\nCorollary 5. Consider any nonatomic congestion game of n players with parameters given in Section 2, and let λ = mbk. Now if each player i plays the gradient descent algorithm by starting from any x0i ∈ Ki and using any ηi ∈ [η, 1/λ], then Tε ≤ 2/(nηε). Furthermore, if each player i plays the multiplicative update algorithm by starting from a uniform x0i (same load on each allowed path) and using any ηi ∈ [η, 1/λ], then Tε ≤ (n ln(dn))/(ηε).\nRemark 1. According to Corollary 5, playing the gradient descent algorithm guarantees a faster convergence time. In particular, if each player i uses ηi = 1/λ, then adopting the gradient descent algorithm leads to a convergence time Tε ≤ 2mbk/(nε), while adopting the multiplicative update algorithm leads to Tε ≤ (mbkn ln(dn))/ε.\nNote that the given bound on Tε is proportional to γ by Theorem 4, which is an upper bound on BRi(qi, x0i ). In Section 3.2.2 of Proof of Corollary 5, we can see that γ is smaller in the gradient descent algorithm than in the multiplicative update algorithm. Thus, the corresponding different bounds on Tε.\nImplications of Φ(xt) being close to Φ(q) include xt being an approximate equilibrium, which will be proved in Section 3.2.3, and achieving social efficiency, which will be given in Section 3.3. We say that a flow x ∈ K is an δ-equilibrium if for any player i ∈ N and any paths s, s′ ∈ Si with xi,s > 0, cs(x) ≤ cs′(x) + δ. Note that with δ = 0, we recover the standard definition of equilibrium for nonatomic games. The following shows that after the convergence time, the system playing our algorithm will stay in an δ-equilibrium for a small δ.\nTheorem 6. Any x ∈ K such that Φ(x) ≤ Φ(q) + ε for any ε must be a δ-equilibrium for some δ ≤ √ 8bmε.\nAnalysis\nTo prove Theorem 4, the key observation is that the updates by all players collectively can be seen as doing a generalized version of the mirror descent, with different step sizes in different dimensions, on the potential function Φ defined in (1). To see this, note that for any i ∈ N and s ∈ Si, the s’th entry of ĉti is\ncs(x t) = ∑ e∈s ce(`e(x t)) = ∂Φ(xt) ∂xi,s ,\nwhich means that the d-dimensional vector (ĉti)i∈N is in fact equal to ∇Φ(xt), the gradient of Φ at xt. That is, if we write ∇Φ(xt) = (∇1Φ(xt), . . . ,∇nΦ(xt)), with ∇iΦ(xt) being the portion of ∇Φ(xt) corresponding to player i, then the update rule of (5) and (6) becomes the following:\nxt+1i = arg min zi∈Ki\n{ ηi〈∇iΦ(xt), zi〉+ BRi(zi, xti) } (8)\n= arg min zi∈Ki\nBRi(zi, xti − ηi∇iΦ(xt)). (9)\nObserve that when all the ηi’s are identical, the collective update of all players moves the whole system exactly in the direction of −∇Φ(xt), and this becomes the standard mirror descent algorithm which has the same step size across all dimensions. It is known that doing such a mirror descent on a smooth convex function leads to a fast convergence to its minimum [9, 33]. On the other hand, we consider the more general case in which different players can have different learning rates, and this corresponds to a more general mirror descent algorithm which allows different step sizes in different dimensions. Because the different step sizes have different scaling effects in different dimensions, the collective update now no longer moves the whole system in the direction of −∇Φ(xt), and it is not clear if a similar convergence result can be obtained. Interestingly, the following theorem shows that doing such a generalized mirror descent algorithm on a general smooth convex function still gives us a fast convergence to its minimum.\nTheorem 7. Suppose K = K1 × · · · × Kn, with each Ki being a convex set. Let Φ : K → R be any convex function which is λ-smooth with respect to some (R1, . . . , Rn) and let q = (q1, . . . , qn) = arg minz∈K Φ(z). Suppose we start from some x 0 = (x01, . . . , x 0 n), with each BRi(qi, x0i ) ≤ γ, and then use the update rule in (8), with each ηi ∈ [η, 1/λ] for some η. Then for any ε ∈ (0, 1), there exists some Tε ≤ nγ/(ηε) such that for any t ≥ Tε, Φ(xt) ≤ Φ(q) + ε.\nWe will prove Theorem 7 in Section 3.2.1. Now note that Theorem 4 follows immediately from Theorem 7 since our potential function Φ is convex by Proposition 1. On the other hand, Theorem 7 works for a general convex function (not restricted to the specific potential function given in (1)), which may have independent interest of its own."
    }, {
      "heading" : "3.2.1 Proof of Theorem 7",
      "text" : "Our proof follows closely that in [9] for the special case in which all the ηi’s are identical. To simplify our notation, let us denote the gradient vector ∇Φ(xt) by gt = (gt1, . . . , gtn), with gti = ∇iΦ(xt).\nUsing the assumption that for each i, ηi ≤ 1/λ and thus λ ≤ 1/ηi, the λ-smoothness condition (7) implies that\nΦ(xt+1) ≤ Φ(xt) + 〈gt, xt+1 − xt〉+ n∑ i=1 1 ηi BRi(xt+1i , x t i), (10)\nbecause each BRi(xit+1, xit) is nonnegative. Then we need the following two lemmas, which we will prove later.\nLemma 8. For any integer t ≥ 0, Φ(xt+1) ≤ Φ(xt).\nLemma 9. For any integer T ≥ 1, T−1∑ t=0 ( Φ(xt+1)− Φ(q) ) ≤ n∑ i=1 1 ηi BRi(qi, x0i ).\nCombining these two lemmas together, we obtain\nT ( Φ(xT )− Φ(q) ) ≤ T−1∑ t=0 ( Φ(xt+1)− Φ(q) ) ≤\nn∑ i=1 1 ηi BRi(qi, x0i )\n≤ nγ η .\nDividing both sides by T gives us\nΦ(xT )− Φ(q) ≤ nγ ηT ≤ ε,\nwhen T ≥ nγ/(ηε), and we have the theorem. It remains to prove the two lemmas, which we do next.\nProof of Lemma 8. We know from (10) that\nΦ(xt+1) ≤ Φ(xt) + n∑ i=1 ( 〈gti , xt+1i − x t i〉+ 1 ηi BRi(xt+1i , x t i) ) .\nTo bound the sum above, note that according to the definition of xt+1i in (8), we have\n〈gti , xt+1i − x t i〉+\n1 ηi BRi(xt+1i , x t i)\n≤ 〈gti , xti − xti〉+ 1\nηi BRi(xti, xti)\n= 0.\nApplying this to the above bound on Φ(xt+1), Lemma 8 follows.\nProof of Lemma 9. We know from (10) that for any t ≥ 0, Φ(xt+1) is at most\nΦ(xt) + 〈gt, xt+1 − xt〉+ n∑ i=1 1 ηi BRi(xt+1i , x t i),\nwhere the second term above can be expressed as\n〈gt, xt+1 − xt〉 = 〈gt, q − xt〉+ 〈gt, xt+1 − q〉\n= 〈gt, q − xt〉+ n∑ i=1 〈gti , xt+1i − qi〉.\nSince Φ(xt) + 〈gt, q − xt〉 ≤ Φ(q) for a convex Φ, we thus know that Φ(xt+1) is at most\nΦ(q) + n∑ i=1 ( 〈gti , xt+1i − qi〉+ 1 ηi BRi(xt+1, xt) ) . (11)\nTo bound the sum above, we rely on the following.\nProposition 10. For each i, 〈gti , x t+1 i − qi〉 is at most\n1\nηi\n( BRi(qi, xti)− BRi(qi, xt+1i )− B Ri(xt+1i , x t i) ) .\nProof. According to the definition of xt+1i in (8), it is also the minimizer of the function\nL(z) = ηi〈gti , z − qi〉+ BRi(z, xti)\nover z ∈ Ki, since 〈gti ,−qi〉 is a constant independent of z. Then from a well-known fact in convex optimization [12, p.139-140], we know that\n〈∇L(xt+1i ), qi − x t+1 i 〉 ≥ 0.\nSince ∇L(xt+1i ) = ηigti +∇Ri(x t+1 i )−∇Ri(xti), we have ηi〈gti , xt+1i − qi〉 ≤ 〈 ∇Ri(xt+1i )−∇Ri(x t i), qi − xt+1i 〉 . (12)\nThen according to the definition of BRi(·), we have\nBRi(qi, xti) = Ri(qi)−Ri(xti)− 〈∇Ri(xti), qi − xti〉, BRi(qi, xt+1i )\n= Ri(qi)−Ri(xt+1i )− 〈∇Ri(x t+1 i ), qi − x t+1 i 〉, and\nBRi(xt+1i , x t i)\n= Ri(x t+1 i )−Ri(x t i)− 〈∇Ri(xti), xt+1i − x t i〉.\nBy subtracting the second and the third equalities from the first, we obtain\nBRi(qi, xti)− BRi(qi, xt+1i )− B Ri(xt+1i , x t i) = 〈 ∇Ri(xt+1i )−∇Ri(x t i), qi − xt+1i 〉 .\nSubstituting this into (12) proves the proposition.\nCombining the bound from this proposition with the upper bound on Φ(xt+1) in (11), we obtain\nΦ(xt+1) ≤ Φ(q) + n∑ i=1 1 ηi ( BRi(qi, xti)− BRi(qi, xt+1i ) ) .\nThis implies that\nT−1∑ t=0 ( Φ(xt+1)− Φ(q) ) ≤\nn∑ i=1 1 ηi T−1∑ t=0 ( BRi(q, xti)− BRi(q, xt+1i ) ) ≤\nn∑ i=1 1 ηi BRi(qi, x0i ),\nwhich proves Lemma 9."
    }, {
      "heading" : "3.2.2 Proof of Corollary 5",
      "text" : "Let us first consider the case that each player plays the gradient descent algorithm. Note that this corresponds to choosing Ri(ui) = ‖ui‖22/2 for each i, and one can show that BRi(ui, vi) = ‖ui − vi‖22/2, for ui, vi ∈ Ki. Then, we have\nBRi(qi, x0i ) = ‖qi − x0i ‖22/2 ≤ ‖qi − x0i ‖21/2\nwhich is at most ( ‖qi‖1 + ‖x0i ‖1 )2 /2 ≤ 2/n2.\nTherefore, we can choose γ = 2/n2 to have BRi(qi, x0i ) ≤ γ. Furthermore, using the Taylor expansion together with Proposition 2, we know that for any x, x′ ∈ K,\nΦ(x′) ≤ Φ(x) + 〈∇Φ(x), x′ − x〉+ λ‖x′ − x‖22/2,\nwith λ = mbk. Since\n‖x′ − x‖22/2 = ∑ i ‖x′i − xi‖22/2 = ∑ i BRi(x′i, xi),\nwe can guarantee that Φ is λ-smooth with this choice of Ri’s. Next, let us consider the case that each player plays the multiplicative update algorithm. Note\nthat this corresponds to choosing Ri(ui) = ∑\ns(ui,s lnui,s − ui,s) for each i, and one can show that BRi(ui, vi) = ∑ s ui,s ln(ui,s/vi,s), for ui, vi ∈ Ki. Then, we have\nBRi(qi, x0i ) ≤ ∑ s qi,s ln(|Si|n) ≤ ln(dn).\nTherefore, we can choose γ = ln(dn) to have BRi(qi, x0i ) ≤ γ. Furthermore, we know that\n‖x′i − xi‖22/2 ≤ ‖x′i − xi‖21/2 ≤ BRi(x′i, xi),\nby Pinsker’s inequality.12 Therefore, we can again guarantee that Φ is λ-smooth with this choice of Ri’s.\nSubstituting these bounds of γ and λ into Theorem 4, Corollary 5 then follows."
    }, {
      "heading" : "3.2.3 Proof of Theorem 6",
      "text" : "Consider any x ∈ K such that Φ(x) ≤ Φ(q) + ε and any i ∈ N . Let s0 be the path in Si which minimizes cs(x) among s ∈ Si, and let s1 be the path which maximizes cs(x) among s ∈ Si with xi,s > 0. Let δ = cs1(x)− cs0(x) and our goal is to show that δ is small by bounding it in terms of ε. The idea is that if δ were large, we could move a significant amount of load from s1 to s0 and decrease the Φ value substantially, which is impossible as Φ(x) is close to the minimum value Φ(q).\nFormally, let us move some ∆ amount, which we will set later, of load from s1 to s0, and let z denote the new flow. Note that the cost increase on s0 and the cost decrease on s1 are both at most mb∆, since c′e(y) ≤ b for any y according to the condition (4). Thus, with ∆ = δ/(4bm) (since δ = 4mb∆, i.e., the path cost difference can be expressed by multiplying the maximum number of edges in a path, the upper bound on an edge cost changing rate, the amount of load moved, and some proper constant scaling), we can have cs1(z) ≥ cs1(x)− δ/4 and cs0(z) ≤ cs0(x) + δ/4, so that\ncs1(z)− cs0(z) ≥ cs1(x)− cs0(x)− δ/2 = δ/2, (13)\nwhich can be used to bound the decrease of the Φ value. Moving the load decreases the Φ value by the amount\nΦ(x)− Φ(z) = ∑\ne∈s1\\s0\n∫ `e(x) `e(x)−∆ ce(y)dy − ∑\ne∈s0\\s1\n∫ `e(x)+∆ `e(x) ce(y)dy\n≥ ∑\ne∈s1\\s0\n∆ce(`e(x)−∆)− ∑\ne∈s0\\s1\n∆ce(`e(x) + ∆)\n= ∆ ∑ e∈s1 ce(`e(z))−∆ ∑ e∈s0 ce(`e(z)) = ∆ (cs1(z)− cs0(z)) ≥ ∆δ/2,\nwhere the first inequality holds as the function ce is nondecreasing and the last inequality holds by (13). Since z is still a feasible flow in K, its Φ value cannot be smaller than that of q and we must have Φ(x) − Φ(z) ≤ Φ(x) − Φ(q) ≤ ε, which implies that ∆δ/2 ≤ ε. With ∆ = δ/(4bm), we have δ ≤ √ 8bmε (and ∆ ≤ √\nε 2bm). Since this holds for any i ∈ N , we have the theorem."
    }, {
      "heading" : "3.3 Social Costs",
      "text" : "According to Theorem 4, the flow xt at step t ≥ Tε enjoys the nice property that Φ(xt) ≤ Φ(q) + ε. In this section, we show the implication of this property on the social costs.\n12Pinsker’s inequality states that the total variance is upper bounded by the KL-divergence between two probability distributions where the total variance can be defined as half the 1-norm between these two distributions. The Bregman divergence is KL-divergence here."
    }, {
      "heading" : "3.3.1 Average Individual Cost",
      "text" : "We show that after the convergence time, the average individual cost achieved by our algorithm is only within a constant factor from the optimum one.\nTheorem 11. Any x ∈ K such that Φ(x) ≤ Φ(q) + ε must have CA(x)CA(x∗) ≤ b a ( 1 + 2mεa ) .\nProof. For any z ∈ K, we can rewrite CA(z) as CA(z) = ∑ e `e(z)ce(`e(z))\n= ∑ e ∫ `e(z) 0 (yce(y)) ′ dy\n= ∑ e ∫ `e(z) 0 ( ce(y) + yc ′ e(y) ) dy.\nUnder the condition (4), we have yc′e(y) ≤ yb = bab0y ≤ b ace(y) and thus\nCA(z) ≤ ∑ e ∫ `e(z) 0 ( 1 + b b0 ) ce(y)dy\n= a+ b\na Φ(z). (14)\nOn the other hand, we also have yc′e(y) ≥ yb0 = ab by ≥ a b ce(y) and thus\nCA(z) ≥ ∑ e ∫ `e(z) 0 ( 1 + a b ) ce(y)dy\n= a+ b\nb Φ(z). (15)\nReplacing z in (14) by x with Φ(x) ≤ Φ(q) + ε, and replacing z in (15) by x∗, we obtain\nCA(x) CA(x∗) ≤ b a Φ(x) Φ(x∗) ≤ b a Φ(q) + ε Φ(q) ,\nas Φ(x∗) ≥ Φ(q), which gives us\nCA(x) CA(x∗) ≤ b a\n( 1 + ε\nΦ(q)\n) . (16)\nFinally, using the condition (4), we have for any z ∈ K that\nΦ(z) ≥ ∑ e ∫ `e(z) 0 aydy = a 2 ∑ e (`e(z)) 2\n≥ a 2m (∑ e `e(z) )2 ≥ a 2m , (17)\nwhere the second inequality is by Cauchy-Schwarz and the last inequality holds as the total load of players is 1. Substituting the bound of (17) into (16) with z = q, we have the theorem.\nRemark 2. We can make CA(x)CA(x∗) ≤ b a (1 + σ) for any σ we want, by choosing ε = aσ/(2m). Then by Remark 1, one can compute the corresponding convergence time Tε, which is proportional to 1/σ."
    }, {
      "heading" : "3.3.2 Maximum Individual Cost in Symmetric Games",
      "text" : "In a symmetric game, Si = S for every i ∈ N . Taking advantage of this property, we show that after the convergence time the maximum individual cost achieved by our algorithm is again within a constant factor from the optimum one.\nTheorem 12. Any x ∈ K such that Φ(x) ≤ Φ(q) + ε must have CM (x)CM (x̂) ≤ b a ( 1 + 2mεa + δm b ) , where\nδ ≤ √ 8bmε.\nProof. Consider any x ∈ K with Φ(x) ≤ Φ(q)+ε. Let s0 = arg mins∈S cs(x) and s1 = arg maxs∈S cs(x). To apply Theorem 6, let us choose a player i with xi,s1 > 0; such a player must exist because otherwise there would be no load on s1 and cs1(x) = 0 could not be the highest path cost. Since Si = S in a symmetric game, s0 is also the path of player i with the lowest path cost. Therefore, we can apply Theorem 6 and have δ = cs1(x)− cs0(x) ≤ √ 8bmε. Note that CM (x) = cs1(x) by definition. Thus, we have CM (x)\nCM (x̂) ≤ cs1(x) CA(x̂) = cs0(x) + δ CA(x̂) ≤ CA(x) + δ CA(x∗) ,\nwhere the first inequality is by the definitions of CM and CA, and the second inequality follows from the fact that cs0(x) ≤ CA(x) and x∗ minimizes CA. Furthermore,\nCA(x) + δ\nCA(x∗) =\nCA(x)\nCA(x∗) +\nδ CA(x∗) ≤ b a\n( 1 + 2εm\na\n) +\nδ\nCA(x∗)\nby Theorem 11. Finally, using a similar analysis as in the proof of Theorem 11, one can show that\nCA(x ∗) ≥ ∑ e ∫ `e(x∗) 0 (ay + ay)dy = a ∑ e (`e(x ∗))2 ≥ a m .\nCombining all the bounds together, we have the theorem.\nRemark 3. We can make CM (x)CM (x̂) ≤ b a(1 + σ) for any σ we want, by choosing ε = aσ 2/(32m). Then according to Remark 1, one can compute the corresponding convergence time Tε, which is now proportional to 1/σ2."
    }, {
      "heading" : "4 The Bandit Model",
      "text" : "We consider the setting in which the players play the congestion game iteratively in the following way. In step t, each player i plays some strategy xti by sampling a path si ∈ Si with probability πti,si = x t i,si n and sending her entire flow of amount 1/n through that path si. After that, she gets to know the cost of that path, which is\ncsi(X t) = ∑ e∈si ce(`e(X t)),\nwhere Xt is the choice vector of the players in step t. With this feedback, she updates her next strategy xt+1i in some way and then proceeds to the next step. Let us stress that the only information a player has access to in a step is the cost of the path she has just chosen, and she has no information about that of any other path she has not chosen. This is known as the bandit model in the area of online learning [34, Chapter 4][3, 24]. It is a more challenging model compared to the bulletin-board model considered in [29] and in Section 3, in which each player i can learn the costs of all her allowed (even unchosen) paths.\nWe consider the same function for the convergence purpose:\nΦ(x) = ∑ e∈E ∫ `e(x) 0 ce(y)dy, (18)\nfor x ∈ K, where `e(x) = ∑ j∈N ∑ r∈Sj :e∈r xtj,r,\nwith `(·) being the same function defined in (3). According to Proposition 1, Φ is a convex function. We still assume that the cost functions satisfy the property in (4).\nNote that this function Φ was used in Section 3 for a nonatomic version of the congestion game, with its input corresponding to splittable flows of players, and its minimizer corresponding to a pure Nash equilibrium with infinitely many agents (i.e., a Wardrop equilibrium). As we consider the atomic version of the congestion game, we now interpret its input as the mixed strategies of players. However, its minimizer corresponds no longer to a pure or mixed Nash equilibrium, but instead to an approximate Nash equilibrium, to be discussed later. We will rely on the following proposition, whose proof we omit here as it follows easily from an analysis in [29, Proof of Lemma 8] using Taylor expansion. The proposition intuitively means that the difference between the expected cost and the cost of expected flow (proportional to the probability distribution of mixed strategies) on s is bounded.\nProposition 13. Let x ∈ K and X the choice vector sampled according to x. Then for any s, 0 ≤ E [cs(X)] − cs(x) ≤ Bm8n where B is the bound on the second derivative of cost functions (Section 2). This implies that classes of cost functions with smaller B give better bounds on the difference. In particular, with linear cost functions where B = 0, then E [cs(X)] = cs(x) for any s."
    }, {
      "heading" : "4.1 Bandit Algorithms",
      "text" : "We would like to follow the approach of Section 3.1 which showed that when each player plays a mirror-descent algorithm with her own learning rate, the joint strategy profile of the players converges quickly. However, there are two key differences in our setting which prevent us from applying their result directly. The first is that here we consider the atomic version of the congestion game, in which each player must send her entire flow on one single path, unlike the non-atomic version considered in Section 3 in which a player can split her flow across multiple paths in each step. The second difference is that the feedback model considered in Section 3 is the easier bulletin model, in which each player, after sending her flow in a step, gets to see all the costs of her allowed paths. That is, in step t, player i is able to learn the cost\ncs(x t) = ∑ e∈s ce(`e(x t)), with `e(x) = ∑ j∈N ∑ r∈Sj :e∈r xtj,r,\nfor every s ∈ Si, where xt is the joint strategy profile of players, describing how each player splits her flow. It was shown in Section 3.1 that for any i ∈ N and s ∈ Si,\ncs(x t) = ∑ e∈s ce(`e(x t)) = ∂Φ(xt) ∂xi,s , (19)\nwhich means that each player i gets to know the part of the gradient vector ∇Φ(xt) related to her, denoted as ∇iΦ(xt), consisting of ∂Φ(x\nt) ∂xi,s\nfor s ∈ Si. This allows each player to update her strategy in a way that the whole system of players can be seen as collectively running a generalized mirrordescent algorithm on the convex potential function Φ. However, in the more challenging bandit model we adopt in this paper, each player i does not know the whole vector ∇iΦ(xt). Instead, the only information player i has after choosing a path si is\ncsi(X t) = ∑ e∈si ce(`e(X t)), with `e(X t) = ∑ j∈N ∑ r∈Sj :e∈r Xtj,r,\nwhere Xt is the choice vector of players at step t. Note that not only does each player i receive less information (one value instead of |Si| values), the information csi(Xt) she receives actually is different from the more useful value csi(x\nt) that corresponds to an entry in ∇Φ(xt). In order to follow the framework in Section 3, each player i needs to have a way to approximate the vector ∇iΦ(xt), her portion of the gradient vector ∇Φ(xt). Our basic idea is for each player to divide the time steps into episodes, each consisting of a consecutive number of steps, and to do the following in each episode. During episode τ , each player i plays some fixed strategy xτi for all the steps (instead of playing different strategies in different steps), collects statistics to obtain an estimate ĝτi for ∇iΦ(xτ ), and at the end of the episode uses ĝτi to update her strategy for the next episode. Two keys are: how to come up with the estimate ĝτi and how to update the next strategy, which we describe next."
    }, {
      "heading" : "4.1.1 Updating the Strategies",
      "text" : "With a “good” estimate ĝτi that we will define and show how to achieve in Section 4.1.2, we can follow Section 3.1 and consider the following update rule for each player i’s strategy of the next episode:\nxτ+1i = arg min zi∈K̄i\n{ ηi〈ĝτi , zi〉+ BRi(zi, xτi ) } (20)\nHere, ηi > 0 is some learning rate, Ri : K̄i → R is some regularization function, and BRi(·, ·) is the Bregman divergence with respect to Ri defined as\nBRi(ui, vi) = Ri(ui)−Ri(vi)− 〈∇Ri(vi), ui − vi〉\nfor ui, vi ∈ K̄i. This gives rise to a family of update rules for different choices of Ri. For example, it is known that by choosing Ri(ui) = ‖ui‖22/2 to have BRi(xi, yi) = ‖xi − yi‖22/2, one recovers the gradient descent algorithm, while by choosing Ri(ui) = ∑ s(ui,s lnui,s − ui,s) to have BRi(xi, yi) =∑\ns xi,s ln(xi,s/yi,s), one recovers the multiplicative updates algorithm. In the bandit atomic model,\n(with a properly chosen Ri) we may need the number of players n to be large to have low regret 13 for each player in order to provide an incentive for the players to use the algorithm. We will need these Ri’s with BRi(·, ·)’s to satisfy the following, which is a bit stricter than Assumption 3.\nAssumption 14. There is some parameter Γ such that for any i ∈ N and any xi, yi ∈ K̄i,\nΓ · BRi(xi, yi) ≤ ‖xi − yi‖22 ≤ 2 · BRi(xi, yi).\nNote that the parameter Γ in the assumption is determined by the choice of Ri as well as the set K̄i, which is a subset (defined in Section 4.1.2) of Ki and in turn depends on the parameter Λ introduced in Section 4.1.2. It is clear that for Ri(ui) = ‖ui‖22/2 and BRi(xi, yi) = ‖xi − yi‖22/2, the assumption holds with Γ = 2. For Ri(xi) = ∑ s(xi,s lnxi,s − xi,s) and BRi(xi, yi) =∑\ns xi,s ln(xi,s/yi,s), the following shows that the assumption holds with Γ = Λ/n, which we prove in Appendix D.\nProposition 15. With BRi(xi, yi) = ∑ s xi,s ln(xi,s/yi,s), it holds that for any xi, yi ∈ K̄i,\nΛ n · BRi(xi, yi) ≤ ‖xi − yi‖22 ≤ 2 · BRi(xi, yi).\nLet us remark that we use the L2 norm ‖ · ‖2 in the assumption, instead of a general norm, for the purpose of simplifying our presentation in the next section; one can check that our analysis there also works for a general norm, but the bounds derived would be more complicated."
    }, {
      "heading" : "4.1.2 Approximating the Gradient",
      "text" : "Now, we define and show how to guarantee a good estimate ĝτi . Consider any episode τ and player i. For each path s ∈ Si, she computes the average cost of that path during the episode as the corresponding entry in ĝτi . That is, if we let ĝ τ i,s denote the s’th entry in ĝ τ i and let T τi,s denote the set of steps in episode τ that player i chose path s, then\nĝτi,s = 1 |T τi,s| ∑ t∈T τi,s cs(X t), (21)\nwhich we would like to be a good approximation of cs(x τ ). This requires |T τi,s| to be large enough which in turns needs πτi,s, or equivalently x τ i,s, to be large enough. To guarantee this, we restrict each player i to play strategies in a smaller feasible set K̄i ⊆ Ki, such that for any xi ∈ K̄i, xi,s ≥ Λ(1/n), for some parameter Λ, which implies that each path s is chosen with probability πi,s ≥ Λ. One way of choosing such K̄i is for each yi ∈ Ki to include a corresponding xi with xi,s = (1−Λ)yi,s+Λ(1/n), for each s. In this way, one can have the guarantee that every yi ∈ Ki has some xi ∈ K̄i such that ‖xi − yi‖1 ≤ dΛ(1/n). We sum up the procedure for approximating the gradient in the following.\nThen by setting the number of steps in each episode τ to\nνn2 log(ndτ)\nΛm2 ,\nfor a large enough constant ν, we have the following.\n13Later in Section 4.2.2, it will be clear that since our goal is to make the convergence close to the minimizer (considering the approximation error of the gradient), the learning rate bound η there is set dependent on n in a way such that η is smaller when n is larger. This in turn affects that n has to be large enough to make the dominant term, which is proportional to η, in the regret small enough.\nAlgorithm 1 Procedure for approximating the gradient\n1: for each yi ∈ Ki include a corresponding xi ∈ K̄i with xi,s = (1− Λ)yi,s + Λ(1/n), for each s. 2: for any episode τ and player i do 3: for each path s ∈ Si do 4: Compute ĝτi,s =\n1 |T τi,s| ∑ t∈T τi,s cs(X t).\n5: end for 6: end for\nLemma 16. With high probability of at least 1−2κ for a small enough constant κ chosen arbitrarily, each player i in each episode τ can have\n‖ĝτi −∇iΦ(xτ )‖∞ ≤ 4bm\nn .\nProof. Fix any player i, path s ∈ Si, and episode τ . Our goal is to show that ∣∣ĝτi,s −∇i,sΦ(xτ )∣∣ = ∣∣∣∣∣∣ 1|T τi,s| ∑ t∈T τi,s ( cs(X t)− cs(xτ ) )∣∣∣∣∣∣ (22)\nis small with high probability. We would like to show that cs(X t) has expected value E[cs(X t)] close to cs(x\nτ ) so that we can apply a Hoeffding bound on (22), but there is a subtlety here that we need to be careful about. Note that for a given t, although the expected value of Xt equals xτ when there is no additional conditioning, this is no longer true under the condition that t ∈ T τi,s, as player i’s choice does not remain random but is fixed to s according to the definition of T τi,s.\nLet us consider a related random variable X̂t, which is similar to Xt but with player i’s choice left random, sampled according to xτi . Then it has the nice property that E[X̂ t] = xτ , so that E[`e(X̂ t)] = `e(x\nτ ) for any e and hence∣∣∣E [cs(X̂t)]− cs(xτ )∣∣∣ ≤ Bm 8n , (23)\nfor any s by Proposition 13. In addition, it has costs close to those associated with Xt, as by Taylor’s expansion on ce(`e(X t)) at `e(X̂ t), |ce(`e(Xt))− ce(`e(X̂t))| is at most\nb ∣∣∣`e(Xt)− `e(X̂t)∣∣∣+ b\n2 ∣∣∣`e(Xt)− `e(X̂t)∣∣∣2 ≤ b n + b 2n2 ≤ 2b n\nfor any e, which implies that ∣∣∣cs(Xt)− cs(X̂t)∣∣∣ ≤∑ e∈s 2b n ≤ 2bm n (24)\nfor any s. From the bounds in (24) and (23), we have∣∣E [cs(Xt)]− cs(xτ )∣∣ ≤\n∣∣∣E [cs(X̂t)]− cs(xτ )∣∣∣+ ∣∣∣E [cs(Xt)]− E [cs(X̂t)]∣∣∣ ≤ Bm\n8n +\n2bm\nn\n≤ 3bm n . (25)\nWith this, we can then apply the Hoeffding bound to get\nPr ∣∣∣∣∣∣ 1|T τi,s| ∑ t∈T τi,s ( cs(X t)− cs(xτ ) )∣∣∣∣∣∣ > 4bmn  ≤ κ ndτ2\nfor some small enough constant κ, whenever |T τi,s| ≥ wτ where wτ = νn2 log(ndτ) 2m2 for a large enough constant ν. Let us define the following good event\n• G: |T τi,s| ≥ wτ for every τ, i, s.\nRecall that each episode τ consists of 2w τ\nΛ steps. Thus, for any τ, i, s,\nE [ |T τi,s| ] ≥ Λ2w τ\nΛ = 2wτ\nand a union bound together with a Hoeffding bound give us Pr [¬G] ≤ ∑ τ,i,s Pr [ |T τi,s| < wτ ] ≤ ∑ τ,i,s κ 2ndτ2 ≤ κ.\nAs a result, we have\nPr [ ∃τ, i, s : ∣∣ĝτi,s −∇i,sΦ(xτ )∣∣ > 4bmn ]\n≤ ∑ τ,i,s Pr [∣∣ĝτi,s −∇i,sΦ(xτ )∣∣ > 4bmn | G ] + Pr [¬G]\n≤ ∑ τ,i,s κ 2ndτ2 + κ ≤ 2κ,\nfor a small enough constant κ, which proves the lemma."
    }, {
      "heading" : "4.2 Convergence Results",
      "text" : "In this section, we analyze the behavior of the system of players adopting the algorithm described in Section 4.1. Our main result is the following, which shows that the system indeed quickly converges, in the sense that the value of the potential function Φ(xτ ) quickly comes near the minimum value Φ(q), where q = arg minz∈K Φ(z), and then stays close afterwards.\nTheorem 17. Consider any atomic congestion game of n players, with a potential function Φ which is (α, β, λ)-smooth, and let q = arg minz∈K Φ(z). Suppose each player i updates her strategy according to the rule in (5), with ηi ≤ 1/λ, using ĝτi described in (21) with the guarantee that\n‖ĝτi −∇iΦ(xτ )‖∞ ≤ . (26)\nConsider any η such that η ≤ ηi for any i and θ = √ ηΓ n ≤ 1, and let δ = 6 + θβdΛ, where Γ is the parameter in Assumption 14 and Λ is the parameter introduced in Section 4.1.2. Then for τ0 = α/δ and 4 = 3δ/θ, it holds that for any τ ≥ τ0,\nΦ(xτ ) ≤ Φ(q) +4\nor equivalently,\nΦ(xτ ) ≤ Φ(q) + 3(6 √\nηΓn + βdΛ).\nAs in Section 3.1, we consider two examples of setting the choices in the following, which will be proved in Section 4.2.2.\nCorollary 18. Consider any atomic congestion game of n players with parameters given in Section 2. If each player i plays the gradient descent algorithm over K̄i with ηi ≤ 1/λ, then with high probability,\n4 = O\n( k1/2m\nn\n) and T0 = O ( n4d log(nd)\nm2k1/2\n) .\nFurthermore, if each player i plays the multiplicative updates algorithm over K̄i with ηi ≤ 1/λ, then with high probability,\n4 = O\n(( kd\nn\n)1/3 m ) and T0 = O ( n10/3d2/3 log(nd)\nm2k1/3\n) .\nFrom the corollary, we see that playing the gradient descent algorithm guarantees a smaller error 4, as k ≤ d. Take for example the load balancing game studied in [29], which has d = n and k = m = 1. The gradient-descent algorithm can achieve 4 = O(1/n), while the multiplicative updates algorithm only has 4 = O(1). Although the convergence time T0 of the gradient-descent algorithm may look higher in the the corollary, one can in fact make it smaller by choosing a smaller Λ (with a slightly increased 4), so that both of its T0 and 4 are smaller than those of the multiplicative updates algorithm. Note that to have a small error, both algorithms need n, the number of players, to be large. This seems unavoidable in the bandit model considered here, because the amount of flow each player has is as large as 1/n, which limits the accuracy each player can have on the estimations of the path costs and gradient vectors.\nImplication of the convergence, including approximate equilibria and guarantees regarding social costs, will be discussed in Section 4.3.\nAnalysis\nThe proof of the theorem is based on the idea of Section 3.1 that if each player i can actually have her portion ∇iΦ(xτ ) of the gradient vector ∇Φ(xτ ) and do the update\nxτ+1i = arg min zi∈Ki\n{ ηi〈∇iΦ(xτ ), zi〉+ BRi(zi, xτi ) } ,\nthen the collective update of all players can be seen as doing some generalized mirror descent on the convex function Φ. Then it was shown in Section 3.1 that doing such a mirror descent on any convex function will lead to a quick convergence. However, in our setting, each player can only obtain an estimate ĝτi of the desired vector ∇iΦ(xτ ), which only allows players collectively to perform an “approximate” mirror descent on the convex function. Unfortunately, the convergence analysis of Section 3.1 relies on showing that Φ(xτ+1) ≤ Φ(xτ ) for every τ , which depends crucially on being able to move (in the mirror space) precisely in the opposite direction of the gradient vector. As we now can no longer move in that precise direction, the next Φ value may increase, and\nit is not clear if convergence can still be guaranteed. Interestingly, we provide a positive answer, with the following theorem. In fact it works for any general convex function Φ, which is smooth in the sense of Definition 1.\nTheorem 19. Let Φ : K → R be any (α, β, λ)-smooth convex function, with q = arg minz∈K Φ(z), where K = K1 × · · · × Kn, such that ‖x − y‖1 ≤ 2 for any x, y ∈ K. Suppose for each i, we have a convex K̄i ⊆ Ki with the property that any x ∈ K has some y ∈ K̄ = K̄1 × · · · × K̄n such that ‖x − y‖1 ≤ dΛ, and suppose moreover that we have Ri’s satisfying Assumption 14 so Γ is the parameter therein. Now consider the update rule in (5), with each ηi ≤ 1/λ, using ĝτi such that\n‖ĝτi −∇Φi(xτ )‖∞ ≤ .\nConsider any η such that η ≤ ηi for any i and θ = √ ηΓ n ≤ 1, and let δ = 6 + θβdΛ. Then for τ0 = α/δ and 4 = 3δ/θ, it holds that for any τ ≥ τ0,\nΦ(xτ ) ≤ Φ(q) +4\nor equivalently,\nΦ(xτ ) ≤ Φ(q) + 3(6 √\nηΓn + βdΛ).\nNote that Theorem 17 follows immediately from Theorem 19. This is because our function Φ is convex and furthermore, any x ∈ K has some y ∈ K̄ with ‖x − y‖1 = ∑ i ‖xi − yi‖1 ≤∑\ni dΛ(1/n) = dΛ. On the other hand, Theorem 7 works for a general convex function, which may have independent interest of its own. To prove Theorem 19, we rely on the following key lemma. Lemma 20. Let θ = √ ηΓ n and δ = 6 + θβdΛ. Then given the assumption of Theorem 19, we have Φ(xτ+1) ≤ Φ(xτ )− θ (Φ(xτ )− Φ(q)) + δ,\nfor any integer τ ≥ 0. This means that\nΦ(xτ+1) ≤ Φ(xτ )− δ\nwhenever Φ(xτ )− Φ(q) ≥ 2δ/θ.\nThis lemma helps us deal with the difficulty we discussed before. That is, although doing mirror descent using the exact gradient vector can guarantee a decreased Φ value, as shown in Section 3.1, this may no longer hold in general when using an approximate gradient vector. Nevertheless, Lemma 20 shows that as long as the current Φ value has a large enough gap from the minimum one, doing mirror descent using an approximate gradient vector can still guarantee a decreased Φ value. We will prove the lemma in Section 4.2.1. Assuming the lemma, now we proceed to prove Theorem 19.\nProof of Theorem 19. From Lemma 20, we know that as long as Φ(xτ ) − Φ(q) ≥ 2δ/θ, the next Φ(xτ+1) will decrease from Φ(xτ ) by at least the amount δ. Since Φ(x) ≤ α for any x ∈ K, according to the smoothness assumption, and Φ(q) ≥ 0, we know that there must be some episode τ1 ≤ τ0 = α/δ such that Φ(xτ1) − Φ(q) < 2δ/θ. After that, the value of Φ may again exceed the threshold Φ(q) + 2δ/θ, but we next argue that it cannot go much higher than that.\nLet τ2 denote the episode after τ1 such that Φ(x τ2) reaches the highest value. Then Φ(xτ2) much have just been increased from its previous episode with a smaller Φ value, which can only happen when\nΦ(xτ2−1)− Φ(q) ≤ 2δ/θ.\nOn the other hand, we know from Lemma 20 that\nΦ(xτ2) ≤ Φ(xτ2−1) + δ.\nConsequently, for any τ ≥ τ0 ≥ τ1, we must have\nΦ(xτ ) ≤ Φ(xτ2) ≤ Φ(q) + 2δ/θ + δ ≤ Φ(q) + 3δ/θ,\nwhich completes the proof of Theorem 19."
    }, {
      "heading" : "4.2.1 Proof of Lemma 20",
      "text" : "To simplify our notation, let us denote the gradient vector ∇Φ(xτ ) by gτ = (gτ1 , . . . , gτn), with gτi = ∇iΦ(xτ ). Then from the assumption that Φ is (α, β, λ)-smooth, we have\nΦ(xτ+1) ≤ Φ(xτ ) + 〈gτ , xτ+1 − xτ 〉+ λ 2 ‖xτ+1 − xτ‖22,\nwith the right-hand side above being\nΦ(xτ ) + ∑ i∈N ( 〈gτi , xτ+1i − x τ i 〉+ λ 2 ‖xτ+1i − x τ i ‖22 )\n≤ Φ(xτ ) + ∑ i∈N ( 〈gτi , xτ+1i − x τ i 〉+ 1 ηi BRi(xτ+1i , x τ i ) ) ,\naccording to Assumption 14 together with the assumption that ηi ≤ 1/λ and hence λ ≤ 1/ηi, as well as the fact that BRi(xτ+1, xτ ) is nonnegative. To bound the sum above, let us first change 〈gτi , x τ+1 i − xτi 〉 into\n〈ĝτi , xτ+1i − x τ i 〉+ 〈gτi − ĝτi , xτ+1i − x τ i 〉,\nso that each\n〈gτi , xτ+1i − x τ i 〉+\n1 ηi BRi(xτ+1i , x τ i )\nnow becomes\n〈ĝτi , xτ+1i − x τ i 〉+\n1 ηi BRi(xτ+1i , x τ i ) + 〈gτi − ĝτi , xτ+1i − x τ i 〉.\nThe sum of the first two terms above according to the definition of xτ+1i in (5) is at most\n〈ĝτi , zi − xτi 〉+ 1\nηi BRi(zi, xτi ),\nfor any zi ∈ K̄i, while the last term above by the Cauchy-Schwarz inequality is at most\n‖gτi − ĝτi ‖∞‖xτ+1i − x τ i ‖1 ≤ ‖xτ+1i − x τ i ‖1,\naccording to the assumption that ‖gτi − ĝτi ‖∞ ≤ . Since∑ i∈N ‖xτ+1i − x τ i ‖1 = ‖xτ+1 − xτ‖1 ≤ 2\nfrom the assumption of the lemma, we have∑ i∈N ( 〈gτi , xτ+1i − x τ i 〉+ 1 ηi BRi(xτ+1i , x τ i ) ) ≤\n∑ i∈N 〈ĝτi , zi − xτi 〉+ 1 η ∑ i∈N BRi(zi, xτi ) + 2 , (27)\nfor any zi ∈ K̄i. Our goal then is to choose these zi’s to make the above small enough. Our idea is to take z = (z1, . . . , zn) as a point that moves from x τ towards\nq̄ = (q̄1, . . . , q̄n) = arg min x∈K̄ Φ(x).\nThat is, we consider zi = x τ i + θ(q̄i − xτi ),\nfor some parameter θ ∈ [0, 1] to be determined later. Note that zi does belong to K̄i because it is a convex combination of xτi and q̄i, both of which are in the convex set K̄i. With zi−xτi = θ(q̄i−xτi ), we are now able to relate the bound in (27) to Φ(xτ )− Φ(q̄) because the first term there is∑\ni∈N 〈ĝτi , zi − xτi 〉\n= ∑ i∈N 〈gτi , zi − xτi 〉+ ∑ i∈N 〈ĝτi − gτi , zi − xτi 〉\n= θ ∑ i∈N 〈gτi , q̄i − xτi 〉+ θ ∑ i∈N 〈ĝτi − gτi , q̄i − xτi 〉, (28)\nwhere the first term in (28) is\nθ〈gτ , q̄ − xτ 〉 ≤ −θ (Φ(xτ )− Φ(q̄))\nby the convexity of Φ, while the second term in (28) by the Cauchy-Schwarz inequality is at most θ ∑ i∈N ‖ĝτi − gτi ‖∞‖zi − xτi ‖1 ≤ θ ‖z − xτ‖1 ≤ 2 ,\nas ‖z − xτ‖1 ≤ 2 and θ ≤ 1. It remains to bound the second term in (27). Note that according to Assumption 14,\nBRi(zi, xτi ) ≤ 1\nΓ ‖zi − xτi ‖22 =\nθ2 Γ ‖q̄i − xτi ‖22,\nwhere\n‖q̄i − xτi ‖22 = ∑ s ( q̄i,s − xτi,s )2 ≤∑ s 1 n ∣∣q̄i,s − xτi,s∣∣ ,\nsince q̄i,s, x τ i,s ∈ [0, 1n ] for every s. This means that\n1\nη ∑ i∈N BRi(zi, xτi ) ≤ θ2 ηΓn ‖xτ − q̄‖1 ≤ 2θ2 ηΓn .\nBy plugging these bounds into (27), we have∑ i∈N ( 〈gτi , xτ+1i − x τ i 〉+ 1 ηi BRi(xτ+1i , x τ i ) ) ≤ −θ (Φ(xτ )− Φ(q̄)) + 2 + 2θ 2\nηΓn + 2 ,\nwhich then implies that\nΦ(xτ+1) ≤ Φ(xτ )− θ (Φ(xτ )− Φ(q̄)) + 2θ 2\nηΓn + 4 , (29)\nfor any θ ∈ [0, 1]. The bound in (29) is still not satisfactory as it involves q̄ instead of q. To relate Φ(q̄) to Φ(q), note that according to the assumption, there exists some q′ ∈ K̄ such that ‖q− q′‖1 ≤ dΛ, while as q̄ minimizes Φ over K̄, we have\nΦ(q̄) ≤ Φ(q′) ≤ Φ(q) + 〈∇Φ(q′), q′ − q〉\nby the convexity of Φ. Then, by the Cauchy-Schwarz inequality, we have\nΦ(q̄) ≤ Φ(q) + ‖∇Φ(q′)‖∞‖q′ − q‖1 ≤ Φ(q) + βdΛ,\nas ‖∇Φ(q′)‖∞ ≤ β by the smoothness assumption on Φ. Finally, by substituting this into (29), we have\nΦ(xτ+1) ≤ Φ(xτ )− θ (Φ(xτ )− Φ(q)) + 2θ 2\nηΓn + 4 + θβdΛ.\nNow let us choose θ = √ ηΓn , and note that θ ≤ 1 by the assumption on η. With this choice\nof θ, we obtain Φ(xτ+1) ≤ Φ(xτ )− θ (Φ(xτ )− Φ(q)) + δ,\nwhere δ = 6 + θβdΛ.\nThis implies that Φ(xτ+1) ≤ Φ(xτ )− δ whenever Φ(xτ )− Φ(q) ≥ 2δ/θ, which completes the proof of Lemma 20."
    }, {
      "heading" : "4.2.2 Proof of Corollary 18",
      "text" : "Note that the bounds in the theorem depend on the parameters Γ and Λ. In fact, we have the freedom to choose the parameter Λ as well as the functions Ri(·)’s, while the parameter Γ is then determined by them.\nThe first is to choose Ri(xi) = ‖xi‖22/2 to have BRi(xi, yi) = ‖xi − yi‖22/2. In this case, each player’s algorithm becomes the gradient-descent algorithm, and we can choose\nΓ = 2 and Λ =\n√\n2ηn\n1\nβd ,\nwhich gives us\n4 = 21 √\n2ηn and τ0 =\nα 7 .\nThe second example is to chooseRi(xi) = ∑ s(xi,s lnxi,s−xi,s) to have BRi(xi, yi) = ∑\ns xi,s ln(xi,s/yi,s). In this case, each player’s algorithm becomes the multiplicative updates algorithm, and according to Proposition 15 we can choose\nΓ = Λ\nn and Λ =\n(\nηβ2d2\n)1/3 ,\nwhich gives us\n4 = 21 ( βd\nη\n)1/3 and τ0 = α\n7 .\nAlthough we stated above the convergence bounds in terms of the number of episodes, by incorporating the sizes of the episodes, it follows that the number of steps needed for convergence is at most\nT0 = νn2 log(ndτ0)\nΛm2 · τ0.\nFinally, let us go back to the congestion game and substitute the parameters α = bm/2, β = bm, λ = bmk from Proposition 2 into the bounds above. Let us set the parameter = 4bm/n according to Lemma 16 so that (26) holds for every player and episode with high probability. Moreover, let us assume for simplicity that ηi ≥ Ω(1/λ) so that we have η ≥ Ω(1/λ). Then we have the corollary."
    }, {
      "heading" : "4.3 Equilibria and Social Costs",
      "text" : "Now we briefly discuss the implication of the guarantee Φ(xτ ) ≤ Φ(q) + 4 given by the results in Section 4.2. The first is that such an πτ is an approximate equilibrium in mixed strategies, where we say that π is a δ-equilibrium if for any player i ∈ N and any paths s, s′ ∈ Si with xi,s > 0, E[cs(X)] ≤ E[cs′(X)] + δ, where X is the choice vector sampled according to x and xi = 1 nπi for any i.\n14 To show this, note that we know from Section 3.2.3 that any xτ satisfying the condition Φ(xτ ) ≤ Φ(q) +4 must have cs(xτ ) ≤ cs′(xτ ) + √ 8bm4. This and Proposition 13\n14There is an alternative way to define an approximate equilibrium. Let (yi(t), X−i) denote a vector where with no randomness yi(t) is a vector with\n1 n\nin dimension t and all 0’s in the rest and still with randomness X−i is the choice vector sampled according to x without dimension i and xi =\n1 n πi for any i. If π is a δ-equilibrium if for any player i ∈ N\nand any paths s, s′ ∈ Si with xi,s > 0, E[cs(yi(s), X−i)] ≤ E[cs′(yi(s′), X−i)] + δ, where (yi(s), X−i), (yi(s′), X−i) are defined above. We then have that E[cs(yi(s), X τ −i)] ≤ E[cs′(yi(s′), Xτ−i)] + √ 8bm4+ 3bm n since any xτ satisfying\nthe condition Φ(xτ ) ≤ Φ(q) + 4 must have cs(xτ ) ≤ cs′(xτ ) + √\n8bm4 along with Inequality (25). Note that bm n\ncan be large in general to make such equilibrium less meaningful due to bad approximation, yet the difference bound is much smaller for some classes of structures of allowed paths. For example, in load-balancing games, E[cs(X τ )] ≤ E[cs′(Xτ )] + √ 8bm4+ 3b\nn since any s consists of only one edge, instead of at most m edges.\ntogether imply that E[cs(X τ )] − Bmn ≤ E[cs′(X\nτ )] + √ 8bm4 ≤ E[cs′(Xτ )] + √\n8bm4, which gives that E[cs(X τ )] ≤ E[cs′(Xτ )] + √ 8bm4+ Bmn , where X τ is the choice vector sampled according to xτ . Bm n can be large in general to make such equilibrium meaningless due to bad approximation. Nevertheless, there are natural cases when such approximate equilibrium is meaningful. As mentioned in Proposition 13, for classes of cost functions with small B, small Bmn can be obtained to give meaningful δ-equilibria. For example, in particular for linear cost functions where B = 0, E[cs(X τ )] ≤ E[cs′(Xτ )] + √ 8bm4 since E[cs(Xτ )] = cs(xτ ) for any s; more generally, even if B is not 0, it could be inherently small enough for some classes of cost functions to make Bmn small, recalling that B (defined in Section 2) is a constant with respect to the load, not necessarily to mn . For some classes of structures of allowed paths, the difference bound is smaller. For example, in load-balancing games of [29], it becomes that E[cs(X τ )] ≤ E[cs′(Xτ )] + √\n8bm4 + Bn since any s consists of only one edge, instead of at most m edges. Using similar analyses to those in Section 3.3, we can also derive bounds on the ratio of a social cost at convergence to that at optimum."
    }, {
      "heading" : "5 Conclusions and Future Work",
      "text" : "We show that the mirror-descent dynamics converges to an approximate equilibrium in nonatomic congestion games. We do this by observing that the dynamics corresponds to a mirror-descent process on a convex potential function of such a game and then proving that the process converges to the minimum of the function. Moreover, we provide bounds on the outcome quality achieved by our dynamics in terms of two social costs: the average individual cost and the maximum individual cost. Finally, we propose a new family of bandit algorithms and show that when each player adopt such an algorithm in an atomic congestion game, their actual joint strategy profile quickly approaches an approximate Nash equilibrium.\nThere may be other no-regret or even other learning algorithms which could guarantee nice convergence properties or simply good quality of outcomes. There are more learning algorithms and dynamics to be explored in repeated games, while classes of games are even more numerous. Beyond learning, there is still a variety of different dynamics to consider in repeated games.\nA different line of future work would be to consider appropriate bandit scenarios for market equilibrium problems and to see if generalized mirror-descents with approximate gradients also work there. Yet another line of work could be extending our framework to other partial-information models by other suitable gradient estimation methods."
    }, {
      "heading" : "A Proof of Proposition 1",
      "text" : "Recall that\nΦ(x) = ∑ e∈E ∫ `e(x) 0 ce(y)dy,\nwhere `e(x) = ∑ i∈N ∑ s:e∈s xi,s. Let\nψe(v) = ∫ v 0 ce(y)dy\nso that Φ(x) = ∑\ne∈E ψe(`e(x)). Observe that `e is a linear function of x ∈ K, while ψe is a convex function of v ∈ R as ce is assumed to be nondecreasing. Then for any δ ∈ [0, 1] and any x, x′ ∈ K,\n(1− δ)Φ(x) + δΦ(x′) = ∑ e∈E ( (1− δ)ψe(`e(x)) + δψe(`e(x′)) ) ≥\n∑ e∈E ψe((1− δ)`e(x) + δ`e(x′))\n= ∑ e∈E ψe(`e((1− δ)x+ δx′)) = Φ((1− δ)x+ δx′).\nThis proves that Φ is convex."
    }, {
      "heading" : "B Proof of Proposition 2",
      "text" : "Consider any x ∈ K. First, to bound Φ(x), recall that ce(y) ≤ by for any y, so that we have\nΦ(x) = ∑ e∈E ∫ `e(x) 0 ce(y)dy\n≤ ∑ e∈E ∫ `e(x) 0 bydy = b\n2 ∑ e∈E (`e(x)) 2.\nSince each player has a flow of amount 1n , we know that `e(x) ≤ 1 and hence∑ e∈E (`e(x)) 2 ≤ ∑ e∈E `e(x)\n= ∑ e∈E ∑ j∈N ∑ r∈Sj :e∈r xj,r\n= ∑ j∈N ∑ r∈Sj ∑ e∈r xj,r = m,\nwhich implies that Φ(x) ≤ bm2 . Next, to bound ‖∇Φ(x)‖∞, note that for any i ∈ N and s ∈ Si, the entry in ∇Φ(x) indexed by (i, s) is\ncs(x) = ∑ e∈s ce(x) ≤ bm.\nTherefore, we have ‖∇Φ(x)‖∞ ≤ bm. Finally, let us bound ∇2Φ(x). Consider any i, j ∈ N , s ∈ Si, and r ∈ Sj . We know that\n∂2Φ(x) ∂xi,s∂xj,r = ∑ e∈s∩r c′e(`e(x)).\nFrom this, we know that each entry of the Hessian matrix ∇2Φ(x) is either zero or at most bm. Consider any z ∈ Rd. For any i ∈ N and s ∈ Si, let Γi(s) denote the set of paths in Si that intersects s. Then\nz>(∇2Φ(x))z ≤ ∑\n(i,s),(j,r)\n∂2Φ(x)\n∂xi,s∂xj,r |zi,s||zi,r|\n≤ bm ∑ i ∑ s |zi,s| ∑\nr∈Γi(s)\n|zi,r|\n≤ bm ∑ i ∑ s |zi,s| √ k ∑\nr∈Γi(s)\n|zi,r|2\n≤ bm ∑ i √∑ s |zi,s|2 √∑ s k ∑\nr∈Γi(s)\n|zi,r|2,\nwhere the last two inequalities both use the Cauchy-Schwarz inequality. Now observe that∑ s ∑ r∈Γi(s) |zi,r|2 = ∑ r ∑ s∈Γi(r) |zi,r|2 ≤ k ∑ r |zi,r|2\nand therefore we have\nz>(∇2Φ(x))z ≤ bm ∑ i k ∑ s |zi,s|2 = bmk‖z‖22\nfor any z. This implies that ∇2Φ(x) λI with λ = bmk, which proves the proposition."
    }, {
      "heading" : "C The No-Regret Property of Our Dynamics",
      "text" : "Mirror descents is known to have the no-regret property even when the cost functions of the edges can vary with time. We consider such a setting, where the actual cost of each edge at time step t is cte(`e(x t)), where `e(x t) is the load of the edge in question at t. We will define a new cost function Cte(x) as follows:\nCte(x) := { cte(x) if x ≤ `e(xt) cte(`e(x t)) otherwise.\nUnder these new cost functions, the cost of any edge observed at time step t, cte(`e(x t)), is actually the worst possible and any further increase on the load of any edge would have no effect on its cost. If this optimistic view of the costs were actually true, then the algorithm using bulletin-board posting would perform exactly as the mirror-descent algorithm and thus preserve the no-regret property. Yet, the actual cost of any strategy under the real cost functions c, when taking into account the effect of the infinitesimal deviating agent (of a player), would be at least as bad as that under the optimistic costs C. Thus, the performance of our algorithm is also of -regret for → 0 in regards to the best strategy in hindsight under the true costs."
    }, {
      "heading" : "D Proof of Proposition 15",
      "text" : "According to the definition,\nBRi(xi, yi) = ∑ s xi,s ln ( 1 + xi,s − yi,s yi,s ) ≤\n∑ s xi,s xi,s − yi,s yi,s\n= ∑ s (xi,s − yi,s)2 yi,s + ∑ s (xi,s − yi,s) ,\nwhich using the fact that ∑\ns xi,s = 1 n = ∑ s yi,s becomes∑\ns\n(xi,s − yi,s)2\nyi,s ≤ n Λ ‖xi − yi‖22,\nas we have yi,s ≥ Λn for any yi ∈ K̄i. This shows that Λ nB Ri(xi, yi) ≤ ‖xi − yi‖22 for any xi, yi ∈ K̄i. On the other hand, for any xi, yi ∈ Ki,\n‖xi − yi‖22 ≤ ‖xi − yi‖21 ≤ 2BRi(xi, yi),\nby Pinsker’s inequality. Thus, we have the proposition."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2016,
    "abstractText" : "<lb>Different types of dynamics have been studied in repeated game play, and one of them<lb>which has received much attention recently consists of those based on “no-regret” algorithms<lb>from the area of machine learning. It is known that dynamics based on generic no-regret<lb>algorithms may not converge to Nash equilibria in general, but to a larger set of outcomes,<lb>namely coarse correlated equilibria. Moreover, convergence results based on generic no-regret<lb>algorithms typically use a weaker notion of convergence: the convergence of the average plays<lb>instead of the actual plays. Some work has been done showing that when using a specific no-<lb>regret algorithm, the well-known multiplicative updates algorithm, convergence of actual plays<lb>to equilibria can be shown and better quality of outcomes in terms of the price of anarchy can be<lb>reached for atomic congestion games and load balancing games. Are there more cases of natural<lb>no-regret dynamics that perform well in suitable classes of games in terms of convergence and<lb>quality of outcomes that the dynamics converge to?<lb>We answer this question positively in the bulletin-board model by showing that when em-<lb>ploying the mirror-descent algorithm, a well-known generic no-regret algorithm, the actual plays<lb>converge quickly to equilibria in nonatomic congestion games. This gives rise to a family of al-<lb>gorithms, including the multiplicative updates algorithm and the gradient descent algorithm as<lb>well as many others. Furthermore, we show that our dynamics achieves good bounds on the<lb>outcome quality in terms of the price-of-anarchy type of measures with two different social costs:<lb>the average individual cost and the maximum individual cost.<lb>Finally, the bandit model considers a probably more realistic and prevalent setting with<lb>only partial information, in which at each time step each player only knows the cost of her<lb>own currently played strategy, but not any costs of unplayed strategies. For the class of atomic<lb>congestion games, we propose a family of bandit algorithms based on the mirror-descent algo-<lb>rithms previously presented, and show that when each player individually adopts such a bandit<lb>algorithm, their joint (mixed) strategy profile quickly converges with implications. ∗Part of the results in this paper have appeared in preliminary form in the proceedings of AAMAS 2014 [13] and<lb>as an extended abstract in AAMAS 2015 [14].<lb>†Institute of Information Management, National Chiao Tung University, Taiwan. Email: poanchen@nctu.edu.tw.<lb>Supported in part by NSC 102-2221-E-009-061-MY2.<lb>‡Institute of Information Science, Academia Sinica, Taiwan. Email: cjlu@iis.sinica.edu.tw. 1<lb>ar<lb>X<lb>iv<lb>:1<lb>60<lb>5.<lb>07<lb>77<lb>4v<lb>2<lb>[<lb>cs<lb>.G<lb>T<lb>]<lb>1<lb>3<lb>O<lb>ct<lb>2<lb>01<lb>6",
    "creator" : "LaTeX with hyperref package"
  }
}