{
  "name" : "1402.0635.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Generalization and Exploration via Randomized Value Functions",
    "authors" : [ "Ian Osband", "Benjamin Van Roy", "Zheng Wen" ],
    "emails" : [ "IOSBAND@STANFORD.EDU", "BVR@STANFORD.EDU", "ZHENGWEN207@GMAIL.COM" ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "The design of reinforcement learning (RL) algorithms that explore intractably large state-action spaces efficiently remains an important challenge. In this paper, we propose randomized least-squares value iteration (RLSVI), which generalizes using a linearly parameterized value function. Prior RL algorithms that generalize in this way require, in the worst case, learning times exponential in the number of model parameters and/or the planning horizon. RLSVI aims to overcome these inefficiencies.\nRLSVI operates in a manner similar to least-squares value iteration (LSVI) and also shares much of the spirit of other closely related approaches such as TD, LSTD, and SARSA (see, e.g., (Sutton & Barto, 1998; Szepesvári, 2010)). What fundamentally distinguishes RLSVI is that the algorithm explores through randomly sampling statistically plausible value functions, whereas the aforementioned alternatives are typically applied in conjunction with action-dithering schemes such as Boltzmann or -greedy exploration, which lead to highly inefficient learning. The concept of explor-\ning by sampling statistically plausible value functions is broader than any specific algorithm, and beyond our proposal and study of RLSVI. We view an important role of this paper is to establish this broad concept as a promising approach to tackling a critical challenge in RL: synthesizing efficient exploration and effective generalization.\nWe will present computational results comparing RLSVI to LSVI with action-dithering schemes. In our case studies, these algorithms generalize using identical linearly parameterized value functions but are distinguished by how they explore. The results demonstrate that RLSVI enjoys dramatic efficiency gains. Further, we establish a bound on the expected regret for an episodic tabula rasa learning context. Our bound is Õ( √ H3SAT ), where S and A denote the cardinalities of the state and action spaces, T denotes time elapsed, and H denotes the episode duration. This matches the worst case lower bound for this problem up to logarithmic factors (Jaksch et al., 2010). It is interesting to contrast this against known Õ( √ H3S2AT ) bounds for other provably efficient tabula rasa RL algorithms (e.g., UCRL2 (Jaksch et al., 2010)) adapted to this context. To our knowledge, our results establish RLSVI as the first RL algorithm that is provably efficient in a tabula rasa context and also demonstrates efficiency when generalizing via linearly parameterized value functions.\nThere is a sizable literature on RL algorithms that are provably efficient in tabula rasa contexts (Brafman & Tennenholtz, 2002; Kakade, 2003; Kearns & Koller, 1999; Lattimore et al., 2013; Ortner & Ryabko, 2012; Osband et al., 2013; Strehl et al., 2006). The literature on RL algorithms that generalize and explore in a provably efficient manner is sparser. There is work on model-based RL algorithms (Abbasi-Yadkori & Szepesvári, 2011; Osband & Van Roy, 2014a;b), which apply to specific model classes and are computationally intractable. Value function generalization approaches have the potential to overcome those computational challenges and offer practical means for synthesizing efficient exploration and effective generalization. A relevant line of work establishes that efficient RL with value function generalization reduces to efficient KWIK online ar X\niv :1\n40 2.\n06 35\nv3 [\nst at\n.M L\n] 1\n5 Fe\nb 20\n16\nregression (Li & Littman, 2010; Li et al., 2008). However, it is not known whether the KWIK online regression problem can be solved efficiently. In terms of concrete algorithms, there is optimistic constraint propagation (OCP) (Wen & Van Roy, 2013), a provably efficient RL algorithm for exploration and value function generalization in deterministic systems, and C-PACE (Pazis & Parr, 2013), a provably efficient RL algorithm that generalizes using interpolative representations. These contributions represent important developments, but OCP is not suitable for stochastic systems and is highly sensitive to model mis-specification, and generalizing effectively in high-dimensional state spaces calls for methods that extrapolate. RLSVI advances this research agenda, leveraging randomized value functions to explore efficiently with linearly parameterized value functions. The only other work we know of involving exploration through random sampling of value functions is (Dearden et al., 1998). That work proposed an algorithm for tabula rasa learning; the algorithm does not generalize over the state-action space."
    }, {
      "heading" : "2. Episodic reinforcement learning",
      "text" : "A finite-horizon MDPM=(S,A,H,P,R,π), where S is a finite state space,A is a finite action space,H is the number of periods, P encodes transition probabilities, R encodes reward distributions, and π is a state distribution. In each episode, the initial state s0 is sampled from π, and, in period h=0,1,··· ,H−1, if the state is sh and an action ah is selected then a next state sh+1 is sampled from Ph(·|sh,ah) and a reward rh is sampled from Rh(·|sh,ah,sh+1). The episode terminates when state sH is reached and a terminal reward is sampled from RH (·|sH).\nTo represent the history of actions and observations over multiple episodes, we will often index variables by both episode and period. For example, slh, alh and rlh respectively denote the state, action, and reward observed during period h in episode l.\nA policy µ = (µ0, µ1, · · · , µH−1) is a sequence of functions, each mapping S to A. For each policy µ, we define a value function for h = 0, ..,H:\nV µh (s):=EM [∑H τ=hrτ ∣∣∣sh=s,aτ=µτ (sτ ) for τ=h,..,H−1] The optimal value function is defined by V ∗h (s) =\nsupµ V µ h (s). A policy µ ∗ is said to be optimal if V µ ∗\n= V ∗. It is also useful to define a state-action optimal value function for h = 0, ..,H − 1:\nQ∗h(s, a) := EM [ rh + V ∗ h+1(sh+1) ∣∣sh = s, ah = a]"
    }, {
      "heading" : "A policy µ∗ is optimal⇐⇒ µ∗h(s)∈argmaxα∈AQ∗h(s,α), ∀s,h.",
      "text" : "A reinforcement learning algorithm generates each action alh based on observations made up to period h of episode l. Over\neach episode, the algorithm realizes reward ∑H h=0rlh. One way to quantify the performance of a reinforcement learning algorithm is in terms of the expected cumulative regret over L episodes, or time T=LH , defined by\nRegret(T,M) = ∑T/H−1 l=0 EM [ V ∗0 (sl0)− ∑H h=0 rlh ] .\nConsider a scenario in which the agent models that, for each h, Q∗h ∈ span [Φh] for some Φh ∈ RSA×K . With some abuse of notation, we use S and A to denote the cardinalities of the state and action spaces. We refer this matrix Φh as a generalization matrix and use Φh(s, a) to denote the row of matrix Φh associated with state-action pair (s, a). For k = 1, 2, · · · ,K, we write the kth column of Φh as φhk and refer to φhk as a basis function. We refer to contexts where the agent’s belief is correct as coherent learning, and refer the alternative as agnostic learning."
    }, {
      "heading" : "3. The problem with dithering for exploration",
      "text" : "LSVI can be applied at each episode to estimate the optimal value function Q∗ from data gathered over previous episodes. To form an RL algorithm based on LSVI, we must specify how the agent selects actions. The most common scheme is to selectively take actions at random, we call this approach dithering. Appendix A presents RL algorithms resulting from combining LSVI with the most common schemes of -greedy or Boltzmann exploration.\nThe literature on efficient RL shows that these dithering schemes can lead to regret that grows exponentially in H and/or S (Kearns & Singh, 2002; Brafman & Tennenholtz, 2002; Kakade, 2003). Provably efficient exploration schemes in RL require that exploration is directed towards potentially informative state-action pairs and consistent over multiple timesteps. This literature provides several more intelligent exploration schemes that are provably efficient, but most only apply to tabula rasa RL, where little prior information is available and learning is considered efficient even if the time required scales with the cardinality of the state-action space. In a sense, RLSVI represents a synthesis of ideas from efficient tabula rasa reinforcement learning and value function generalization methods.\nTo motivate some of the benefits of RLSVI, in Figure 1 we provide a simple example that highlights the failings of dithering methods. In this setting LSVI with Boltzmann or -greedy exploration requires exponentially many episodes to learn an optimal policy, even in a coherent learning context and even with a small number of basis functions.\nThis environment is made up of a long chain of states S = {1, .., N}. Each step the agent can transition left or right. Actions left are deterministic, but actions right only succeed with probability 1 − 1/N , otherwise they go left. All states have zero reward except for the far rightN which\ngives a reward of 1. Each episode is of length H = N − 1 and the agent will begin each episode at state 1. The optimal policy is to go right at every step to receive an expected reward of p∗ = (1− 1N )\nN−1 each episode, all other policies give no reward. Example 1 establishes that, for any choice of basis function, LSVI with any -greedy or Boltzmann exploration will lead to regret that grows exponentially in S. A similar result holds for policy gradient algorithms.\nExample 1. Let l∗ be the first episode during which state N is visited. It is easy to see that θlh = 0 for all h and all l < l∗. Furthermore, with either -greedy or Boltzmann exploration, actions are sampled uniformly at random over episodes l < l∗. Thus, in any episode l < l∗, the red node will be reached with probability p∗2−(S−1) = p∗2−H . It follows that E[l∗] ≥ 2S−1 − 1 and lim infT→∞Regret(T,M) ≥ 2S−1 − 1."
    }, {
      "heading" : "4. Randomized value functions",
      "text" : "We now consider an alternative approach to exploration that involves randomly sampling value functions rather than actions. As a specific scheme of this kind, we propose randomized least-squares value iteration (RLSVI), which we present as Algorithm 1.1 To obtain an RL algorithm, we simply select greedy actions in each episode, as specified in Algorithm 2.\nThe manner in which RLSVI explores is inspired by Thompson sampling (Thompson, 1933), which has been shown to explore efficiently across a very general class of online optimization problems (Russo & Van Roy, 2013; 2014). In Thompson sampling, the agent samples from a posterior distribution over models, and selects the action that optimizes the sampled model. RLSVI similarly samples from a distribution over plausible value functions and selects actions that optimize resulting samples. This distribution can be thought of as an approximation to a posterior distribution over value functions. RLSVI bears a close connection to PSRL (Osband et al., 2013), which maintains and samples from a posterior distribution over MDPs and is a direct application of Thompson sampling to RL. PSRL satisfies regret bounds that scale with the dimensionality, rather than the cardinality, of the underlying MDP (Osband & Van Roy, 2014b;a). However, PSRL does not accommodate value function generalization without MDP planning, a feature that we expect to be of great practical importance.\n1Note that when l = 0, both A and b are empty, hence, we set θ̃l0 = θ̃l1 = · · · = θ̃l,H−1 = 0.\nAlgorithm 1 Randomized Least-Squares Value Iteration Input: Data Φ0(si0,ai0),ri0,..,ΦH−1(siH−1,aiH−1),riH : i<L, Parameters λ>0, σ>0 Output: θ̃l0,..,θ̃l,H−1\n1: for h=H−1,..,1,0 do 2: Generate regression problem A∈Rl×K , b∈Rl:\nA←  Φh(s0h,a0h)... Φh(sl−1,h,al−1,h)  bi← { rih+maxα ( Φh+1θ̃l,h+1 ) (si,h+1,α) if h<H−1\nrih+ri,h+1 if h=H−1\n3: Bayesian linear regression for the value function\nθlh← 1\nσ2\n( 1\nσ2 A>A+λI\n)−1 A>b\nΣlh← ( 1\nσ2 A>A+λI )−1 4: Sample θ̃lh∼N(θlh,Σlh) from Gaussian posterior 5: end for\nAlgorithm 2 RLSVI with greedy action Input: Features Φ0,..,ΦH−1; σ>0, λ>0\n1: for l=0,1,.. do 2: Compute θ̃l0,..,θ̃l,H−1 using Algorithm 1 3: Observe sl0 4: for h=0,..,H−1 do 5: Sample alh∈argmaxα∈A ( Φhθ̃lh ) (slh,α) 6: Observe rlh and sl,h+1 7: end for 8: Observe rlH 9: end for"
    }, {
      "heading" : "5. Provably efficient tabular learning",
      "text" : "RLSVI is an algorithm designed for efficient exploration in large MDPs with linear value function generalization. So far, there are no algorithms with analytical regret bounds in this setting. In fact, most common methods are provably inefficient, as demonstrated in Example 1, regardless of the choice of basis function. In this section we will establish an expected regret bound for RLSVI in a tabular setting without generalization where the basis functions Φh = I .\nThe bound is on an expectation with respect to a probability space (Ω,F ,P). We define the MDP M = (S,A, H, P,R, π) and all other random variables we will consider with respect to this probability space. We assume that S, A, H , and π, are deterministic and that R and P are drawn from a prior distribution. We will assume that\nrewards R(s, a, h) are drawn from independent Dirichlet αR(s, a, h) ∈ R2+ with values on {−1, 0} and transitions Dirichlet αP (s, a, h) ∈ RS+. Analytical techniques exist to extend similar results to general bounded distributions; see, for example (Agrawal & Goyal, 2012).\nTheorem 1. If Algorithm 1 is executed with Φh=I for h= 0,..,H−1, λ≥max(s,a,h) ( 1TαR(s,a,h)+1TαP (s,a,h) ) and σ≥ √ H2+1, then:\nE [Regret(T,M)] ≤ Õ (√ H3SAT )\n(1)\nSurprisingly, these scalings better state of the art optimistic algorithms specifically designed for efficient analysis which would admit Õ( √ H3S2AT ) regret (Jaksch et al., 2010). This is an important result since it demonstrates that RLSVI can be provably-efficient, in contrast to popular dithering approaches such as -greedy which are provably inefficient."
    }, {
      "heading" : "5.1. Preliminaries",
      "text" : "Central to our analysis is the notion of stochastic optimism, which induces a partial ordering among random variables.\nDefinition 1. For any X and Y real-valued random variables we say thatX is stochastically optimistic for Y if and only if for any u:R→R convex and increasing\nE[u(X)] ≥ E[u(Y )].\nWe will use the notation X <so Y to express this relation. It is worth noting that stochastic optimism is closely connected with second-order stochastic dominance: X <so Y if and only if −Y second-order stochastically dominates −X (Hadar & Russell, 1969). We repoduce the following result which establishes such a relation involving Gaussian and Dirichlet random variables in Appendix G.\nLemma 1. For all V ∈ [0, 1]N and α ∈ [0,∞)N with αT1 ≥ 2, if X ∼ N(α>V/α>1, 1/α>1) and Y = PTV for P ∼ Dirichlet(α) then X <so Y ."
    }, {
      "heading" : "5.2. Proof sketch",
      "text" : "Let Q̃lh = Φhθ̃lh and µ̃l denote the value function and policy generated by RLSVI for episode l and let Ṽ lh(s) = maxa Q̃ l h(s, a). We can decompose the per-episode regret\nV ∗0 (sl0)− V µ̃l 0 (sl0) = Ṽ l 0 (sl0)−V µ̃l0 (sl0)︸ ︷︷ ︸\n∆concl\n+ V ∗0 (sl0)−Ṽ l0 (sl0)︸ ︷︷ ︸ ∆optl .\nWe will bound this regret by first showing that RLSVI generates optimistic estimates of V ∗, so that ∆optl has nonpositive expectation for any history Hl available prior to episode l. The remaining term ∆concl vanishes as estimates generated by RLSVI concentrate around V ∗.\nLemma 2. Conditional on any data H, the Q-values generated by RLSVI are stochastically optimistic for the true Q-values Q̃lh(s, a) <so Q ∗ h(s, a) for all s, a, h.\nProof. Fix any data Hl available and use backwards induction on h = H − 1, .., 1. For any (s, a, h) we write n(s, a, h) for the amount of visits to that datapoint in Hl. We will write R̂(s, a, h), P̂ (s, a, h) for the empirical mean reward and mean transitions based upon the data Hl. We can now write the posterior mean rewards and transitions:\nR(s, a, h)|Hl = −1× αR1 (s, a, h) + n(s, a, h)R̂(s, a, h)\n1TαR(s, a, h) + n(s, a, h)\nP (s, a, h)|Hl = αP (s, a, h) + n(s, a, h)P̂ (s, a, h)\n1TαP (s, a, h) + n(s, a, h)\nNow, using Φh = I for all (s, a, h) we can write the RLSVI updates in similar form. Note that, Σlh is diagonal with each diagonal entry equal to σ2/(n(s, a, h) + λσ2). In the case of h = H − 1\nθ l H−1(s, a) = n(s, a,H − 1)R̂(s, a,H − 1)\nn(s, a,H − 1) + λσ2\nUsing the relation that R̂ ≥ R Lemma 1 means that\nN(θ l H−1(s, a), 1\nn(s, a, h) + 1TαR(s, a, h) ) <so RH−1|Hl.\nTherefore, choosing λ > maxs,a,h 1TαR(s, a, h) and σ > 1, we must satisfy the lemma for all s, a and h = H − 1.\nFor the inductive step we assume that the result holds for all s, a and j > h, we now want to prove the result for all (s, a) at timestep h. Once again, we can express θ l\nh(s, a) in closed form.\nθ l h(s, a) = n(s, a, h)\n( R̂(s, a, h) + P̂ (s, a, h)T Ṽ lh+1 ) n(s, a, h) + λσ2\nTo simplify notation we omit the arguments (s, a, h) where they should be obvious from context. The posterior mean estimate for the next step value V ∗h , conditional onHl:\nE[Q∗h(s, a)|Hl] = R+ P T V ∗h+1 ≤\nn(R̂+ P̂TV ∗h+1)\nn+ λσ2 .\nAs long as λ > 1TαR + 1T (αP ) and σ2 > H2. By our induction process Ṽ lh+1 <so V ∗ h+1 so that\nE[Q∗h(s, a)|Hl] ≤ E\n[ n(R̂+ P̂T Ṽ lh+1)\nn+ λσ2 | Hl\n] .\nWe can conclude by Lemma 1 and noting that the noise from rewards is dominated by N(0, 1) and the noise from transitions is dominated by N(0, H2). This requires that σ2 ≥ H2 + 1.\nLemma 2 means RLSVI generates stochastically optimistic Q-values for any history Hl. All that remains is to prove the remaining estimates E[∆concl |Hl] concentrate around the true values with data. Intuitively this should be clear, since the size of the Gaussian perturbations decreases as more data is gathered. In the remainder of this section we will sketch this result.\nThe concentration error ∆concl = Ṽ l 0 (sl0) − V µ̃l 0 (sl0). We decompose the value estimate Ṽ l0 explicitly:\nṼ l0 (sl0) = n(R̂+ P̂T Ṽ lh+1)\nn+ λσ2 + wσ\n= R+ P T Ṽ lh+1 + b R + bP + wσ0\nwhere wσ is the Gaussian noise from RLSVI and bR = bR(sl0, al00), b\nP = bP (sl0, al00) are optimistic bias terms for RLSVI. These terms emerge since RLSVI shrinks estimates towards zero rather than the Dirichlet prior for rewards and transitions.\nNext we note that, conditional on Hl we can rewrite P T Ṽ lh+1 = Ṽ l h+1(s\n′) + dh where s′ ∼ P ∗(s, a, h) and dh is some martingale difference. This allows us to decompose the error in our policy to the estimation error of the states and actions we actually visit. We also note that, conditional on the data Hl the true MDP is independent of the sampling process of RLSVI. This means that:\nE[V µ̃l0 (sl0)|Hl] = R+ P T V µ̃lh+1.\nOnce again, we can replace this transition term with a single sample s′ ∼ P ∗(s, a, h) and a martingale difference. Combining these observations allows us to reduce the concentration error\nE[Ṽ l0 (sl0)− V µ̃l 0 (sl0)|Hl] =\nH−1∑ h=0 { bR(slh, alh, h) + b P (slh, alh, h) + w σ h } .\nWe can even write explicit expressions for bR, bP and wσ .\nbR(s, a, h) = nR̂ n+ λσ2 − nR̂− α R 1 n+ 1TαR\nbP (s, a, h) = nP̂T Ṽ lh+1 n+ λσ2 − (nP̂ + αP )T Ṽ lh+1 n+ 1TαP\nwσh ∼ N ( 0, σ2\nn+ λσ2 ) The final details for this proof are technical but the argument is simple. We let λ=1TαR+1TαP and σ=√ H2+1. Up to Õ notation bR' α R 1\nn+1TαP , bP' H1\nTαP n+1TαP\nand wσh' H√n+H21TαR+1TαP . Summing using a pigeonhole principle for ∑ s,a,hn(s,a,h)=T gives us an\nupper bound on the regret. We write K(s,a,h):=( αR1 (s,a,h)+H1 TαP (s,a,h) ) to bound the effects of the\nprior mistmatch in RLSVI arising from the bias terms bR, bP . The constraint αT1 ≥ 2 can only be violated twice for each s, a, h. Therefore up to O(·) notation:\nE [∑T/H−1\nl=0 E[∆ conc l |Hl] ] ≤ 2SAH+∑\ns,a,hK(s,a,h)log(T+K(s,a,h))+H √ SAHT log(T )\nThis completes the proof of Theorem 1."
    }, {
      "heading" : "6. Experiments",
      "text" : "Our analysis in Section 5 shows that RLSVI with tabular basis functions acts as an effective Gaussian approximation to PSRL. This demonstrates a clear distinction between exploration via randomized value functions and dithering strategies such as Example 1. However, the motivating for RLSVI is not for tabular environments, where several provably efficient RL algorithms already exist, but instead for large systems that require generalization.\nWe believe that, under some conditions, it may be possible to establish polynomial regret bounds for RLSVI with value function generalization. To stimulate thinking on this topic we present a conjecture of result what may be possible in Appendix B. For now, we will present a series of experiments designed to test the applicability and scalability of RLSVI for exploration with generalization.\nOur experiments are divided into three sections. First, we present a series of didactic chain environments similar to Figure 1. We show that RLSVI can effectively synthesize exploration with generalization with both coherent and agnostic value functions that are intractable under any dithering scheme. Next, we apply our Algorithm to learning to play Tetris. We demonstrate that RLSVI leads to faster learning, improved stability and a superior learned policy in a large-scale video game. Finally, we consider a business application with a simple model for a recommendation system. We show that an RL algorithm can improve upon even the optimal myopic bandit strategy. RLSVI learns this optimal strategy when dithering strategies do not."
    }, {
      "heading" : "6.1. Testing for efficient exploration",
      "text" : "We now consider a series of environments modelled on Example 1, where dithering strategies for exploration are provably inefficient. Importantly, and unlike the tabular setting of Section 5, our algorithm will only interact with the MDP but through a set of basis function Φ which generalize across states. We examine the empirical performance of RLSVI and find that it does efficiently balance exploration and generalization in this didactic example."
    }, {
      "heading" : "6.1.1. COHERENT LEARNING",
      "text" : "In our first experiments, we generate a random set ofK basis functions. This basis is coherent but the individual basis functions are not otherwise informative. We form a ran-\ndom linear subspace VhK spanned by (1,Q∗h,w̃1,..,w̃k−2). Here wi and w̃i are IID Gaussian ∼N(0,I)∈RSA. We then form Φh by projecting (1,w1,..,wk−1) onto VhK and renormalize each component to have equal 2-norm2. Figure 2 presents the empirical regret for RLSVI withK=10,N= 50,σ=0.1,λ=1 and an -greedy agent over 5 seeds3.\nFigure 1 shows that RLSVI consistently learns the optimal policy in roughly 500 episodes. Any dithering strategy would take at least 1015 episodes for this result. The state of the art upper bounds for the efficient optimistic algorithm UCRL given by appendix C.5 in (Dann & Brunskill, 2015) for H = 15,S= 6,A= 2, = 1,δ= 1 only kick in after more than 1010 suboptimal episodes. RLSVI is able to effectively exploit the generalization and prior structure from the basis functions to learn much faster.\nWe now examine how learning scales as we change the chain length N and number of basis functions K. We observe that RLSVI essentially maintains the optimal policy once it discovers the rewarding state. We use the number of episodes until 10 rewards as a proxy for learning time. We report the average of five random seeds.\nFigure 3 examines the time to learn as we vary the chain length N with fixed K=10 basis functions. We include the dithering lower bound 2N−1 as a dashed line and a lower bound scaling 110H\n2SA for tabular learning algorithms as a solid line (Dann & Brunskill, 2015). ForN=100, 2N−1> 1028 and H2SA>106. RLSVI demonstrates scalable generalization and exploration to outperform these bounds.\nFigure 4 examines the time to learn as we vary the basis functions K in a fixed N=50 length chain. Learning time\n2For more details on this experiment see Appendix C. 3In this setting any choice of or Boltzmann η is equivalent.\nscales gracefully with K. Further, the marginal effect of K decrease as dim(VhK)=K approaches dim(RSA)=100. We include a local polynomial regression in blue to highlight this trend. Importantly, even for large K the performance is far superior to the dithering and tabular bounds4.\nFigure 5 examines these same scalings on a logarithmic scale. We find the data for these experiments is consistent with polynomial learning as hypothesized in Appendix B. These results are remarkably robust over several orders of magnitude in both σ and λ. We present more detailed analysis of these sensitivies in Appendix C."
    }, {
      "heading" : "6.1.2. AGNOSTIC LEARNING",
      "text" : "Unlike the example above, practical RL problems will typically be agnostic. The true value function Q∗h will not lie within VhK . To examine RLSVI in this setting we generate basis functions by adding Gaussian noise to the true value function φhk ∼ N(Q∗h, ρI). The parameter ρ determines the scale of this noise. For ρ = 0 this problem is coherent but for ρ > 0 this will typically not be the case. We fix N = 20,K = 20, σ = 0.1 and λ = 1.\nFor i=0,..,1000 we run RLSVI for 10,000 episodes with ρ=i/1000 and a random seed. Figure 6 presents the number of episodes until 10 rewards for each value of ρ. For large values of ρ, and an extremely misspecified basis, RLSVI is not effective. However, there is some region 0 < ρ < ρ∗ where learning remains remarkably stable5.\nThis simple example gives us some hope that RLSVI can be\n4For chain N=50, the bounds 2N−1>1014 and H2SA>105. 5Note Q∗h(s,a)∈{0,1} so ρ=0.5 represents significant noise.\nuseful in the agnostic setting. In our remaining experiments we will demonstrate that RLSVI can acheive state of the art results in more practical problems with agnostic features."
    }, {
      "heading" : "6.2. Tetris",
      "text" : "We now turn our attention to learning to play the iconic video game Tetris. In this game, random blocks fall sequentially on a 2D grid with 20 rows and 10 columns. At each step the agent can move and rotate the object subject to the constraints of the grid. The game starts with an empty grid and ends when a square in the top row becomes full. However, when a row becomes full it is removed and all bricks above it move downward. The objective is to maximize the score attained (total number of rows removed) before the end of the game.\nTetris has been something of a benchmark problem for RL and approximate dynamic programming, with several papers on this topic (Gabillon et al., 2013). Our focus is not so much to learn a high-scoring Tetris player, but instead to demonstrate the RLSVI offers benefits over other forms of exploration with LSVI. Tetris is challenging for RL with a huge state space with more than 2200 states. In order to tackle this problem efficiently we use 22 benchmark features. These featurs give the height of each column, the absolute difference in height of each column, the maximum height of a column, the number of “holes” and a constant. It is well known that you can find far superior linear basis functions, but we use these to mirror their approach.\nIn order to apply RLSVI to Tetris, which does not have fixed episode length, we made a few natural modifications to the algorithm. First, we approximate a timehomogeneous value function. We also only the keep most recent N=105 transitions to cap the linear growth in memory and computational requirements, similar to (Mnih, 2015). Details are provided in Appendix D. In Figure 7 we present learning curves for RLSVI λ=1,σ=1 and LSVI with a tuned -greedy exploration schedule6 averaged over 5 seeds. The results are significant in several ways.\nFirst, both RLSVI and LSVI make significant improvements over the previous approach of LSPI with the same\n6We found that we could not acheive good performance for any fixed . We used an annealing exploration schedule that was tuned to give good performance. See Appendix D\nbasis functions (Bertsekas & Ioffe, 1996). Both algorithms reach higher final performance (' 3500 and 4500 respectively) than the best level for LSPI (3183). They also reach this performance after many fewer games and, unlike LSPI do not “collapse” after finding their peak performance. We believe that these improvements are mostly due to the memory replay buffer, which stores a bank of recent past transitions, rather than LSPI which is purely online.\nSecond, both RLSVI and LSVI learn from scratch where LSPI required a scoring initial policy to begin learning. We believe this is due to improved exploration schemes, LSPI is completely greedy so struggles to learn without an initial policy. LSVI with a tuned schedule is much better. However, we do see a significant improvement through exploration via RLSVI even when compared to the tuned scheme. More details are available in Appendix D."
    }, {
      "heading" : "6.3. A recommendation engine",
      "text" : "We will now show that efficient exploration and generalization can be helpful in a simple model of customer interaction. Consider an agent which recommends J ≤ N products from Z = {1, 2, . . . , N} sequentially to a customer. The conditional probability that the customer likes a product depends on the product, some items are better than others. However it also depends on what the user has observed, what she liked and what she disliked. We represent the products the customer has seen by Z̃ ⊆ Z . For each product n ∈ Z̃ we will indicate xn ∈ {−1,+1} for her preferences {dislike, like} respectively. If the customer has not observed the product n /∈ Z̃ we will write xn = 0. We model the probability that the customer will like a new product a /∈ Z̃ by a logistic transformation linear in x:\nP(a|x) = 1/ (1 + exp (− [βa + ∑ n γanxn])) . (2)\nImportantly, this model reflects that the customers’ preferences may evolve as their experiences change. For example, a customer may be much more likely to watch the second season of the TV show “Breaking Bad” if they have watched the first season and liked it.\nThe agent in this setting is the recommendation system,\nwhose goal is to maximize the cumulative amount of items liked through time for each customer. The agent does not know p(a|x) initially, but can learn to estimate the parameters β, γ through interactions across different customers. Each customer is modeled as an episode with horizon length H = J with a “cold start” and no previous observed products Z̃ = ∅. For our simulations we set βa = 0 ∀a and sample a random problem instance by sampling γan ∼ N(0, c2) independently for each a and n.\nAlthough this setting is simple, the number of possible states |S| = |{−1, 0,+1}|H = 3J is exponential in J . To learn in time less than |S| it is crucial that we can exploit generalization between states as per equation (2). For this problem we constuct the following simple basis functions: ∀1 ≤ n,m, a ≤ N , let φm(x, a) = 1{a = m} and φmn(x, a) = xn1{a = m}. In each period h form Φh = ((φn)n, (φm)m). The dimension of our function class K = N2 +N is exponentially smaller than the number of states. However, barring a freak event, this simple basis will lead to an agnostic learning problem.\nFigure 8 and 9 show the performance of RLSVI compared to several benchmark methods. In Figure 8 we plot the cumulative regret of RLSVI when compared against LSVI with Boltzmann exploration and identical basis features. We see that RLSVI explores much more efficiently than Boltzmann exploration over a wide range of temperatures.\nIn Figure 9 we show that, using this efficient exploration method, the reinforcement learning policy is able to outperform not only benchmark bandit algorithms but even\nthe optimal myopic policy7. Bernoulli Thompson sampling does not learn much even after 1200 episodes, since the algorithm does not take context into account. The linear contextual bandit outperforms RLSVI at first. This is not surprising, since learning a myopic policy is simpler than a multi-period policy. However as more data is gathered RLSVI eventually learns a richer policy which outperforms the myopic policy.\nAppendix E provides pseudocode for this computational study. We set N = 10, H = J = 5, c = 2 and L = 1200. Note that such problems have |S| = 4521 states; this allows us to solve each MDP exactly so that we can compute regret. Each result is averaged over 100 problem instances and for each problem instance, we repeat simulations 10 times. The cumulative regret for both RLSVI (with λ = 0.2 and σ2 = 10−3) and LSVI with Boltzmann exploration (with λ = 0.2 and a variety of “temperature” settings η) are plotted in Figure 8. RLSVI clearly outperforms LSVI with Boltzmann exploration.\nOur simulations use an extremely simplified model. Nevertheless, they highlight the potential value of RL over multiarmed bandit approaches in recommendation systems and other customer interactions. An RL algorithm may outperform even even an optimal myopic system, particularly where large amounts of data are available. In some settings, efficient generalization and exploration can be crucial."
    }, {
      "heading" : "7. Closing remarks",
      "text" : "We have established a regret bound that affirms efficiency of RLSVI in a tabula rasa learning context. However the real promise of RLSVI lies in its potential as an efficient method for exploration in large-scale environments with generalization. RLSVI is simple, practical and explores efficiently in several environments where state of the art approaches are ineffective.\nWe believe that this approach to exploration via randomized value functions represents an important concept beyond our specific implementation of RLSVI. RLSVI is designed for generalization with linear value functions, but many of the great successes in RL have come with highly nonlinear “deep” neural networks from Backgammon (Tesauro, 1995) to Atari8 (Mnih, 2015). The insights and approach gained from RLSVI may still be useful in this nonlinear setting. For example, we might adapt RLSVI to instead take approximate posterior samples from a nonlinear value function via a nonparametric bootstrap (Osband & Van Roy, 2015).\n7The optimal myopic policy knows the true model defined in Equation 2, but does not plan over multiple timesteps.\n8Interestingly, recent work has been able to reproduce similar performance using linear value functions (Liang et al., 2015)."
    }, {
      "heading" : "APPENDICES",
      "text" : ""
    }, {
      "heading" : "A. LSVI with Boltzmann exploration/ -greedy exploration",
      "text" : "The LSVI algorithm iterates backwards over time periods in the planning horizon, in each iteration fitting a value function to the sum of immediate rewards and value estimates of the next period. Each value function is fitted via least-squares: note that vectors θlh satisfy\nθlh ∈ arg min ζ∈RK\n( ‖Aζ − b‖2 + λ‖ζ‖2 ) . (3)\nNotice that in Algorithm 3, when l = 0, matrix A and vector b are empty. In this case, we simply set θl0 = θl1 = · · · = θl,H−1 = 0.\nAlgorithm 3 Least-Squares Value Iteration Input: Data Φ(si0,ai0),ri0,..,Φ(siH−1,aiH−1),riH : i<L . Parameter λ>0 Output: θl0,...,θl,H−1\n1: θlH←0, ΦH←0 2: for h=H−1,...,1,0 do 3: Generate regression problem A∈Rl×K , b∈Rl:\nA←  Φh(s0h,a0h)... Φh(sl−1,h,al−1,h)  bi← { rih+maxα ( Φh+1θ̃l,h+1 ) (si,h+1,α) if h<H−1\nrih+ri,h+1 if h=H−1\n4: Linear regression for value function\nθlh←(A>A+λI)−1A>b\n5: end for\nRL algorithms produced by synthesizing Boltzmann exploration or -greedy exploration with LSVI are presented as Algorithms 4 and 5. In these algorithms the “temperature” parameters η in Boltzmann exploration and in -greedy exploration control the degree to which random perturbations distort greedy actions.\nAlgorithm 4 LSVI with Boltzmann exploration Input: Features Φ0,..,ΦH−1; η>0, λ>0\n1: for l=0,1,··· do 2: Compute θl0,...,θl,H−1 based on Algorithm 3 3: Observe xl0 4: for h=0,1,...,H−1 do 5: Sample alh∼E[(Φhθlh)(xlh,a)/η] 6: Observe rlh and xl,h+1 7: end for 8: end for\nAlgorithm 5 LSVI with -greedy exploration Input: Features Φ0,..,ΦH−1; >0, λ>0\n1: for l=0,1,... do 2: Compute θl0,...,θl,H−1 using Algorithm 3 3: Observe xl0 4: for h=0,1,···,H−1 do 5: Sample ξ∼Bernoulli( ) 6: if ξ=1 then 7: Sample alh∼unif(A) 8: else 9: Sample alh∈argmaxα∈A(Φhθlh)(xlh,α)\n10: end if 11: Observe rlh and xl,h+1 12: end for 13: end for"
    }, {
      "heading" : "B. Efficient exploration with generalization",
      "text" : "Our computational results suggest that, when coupled with generalization, RLSVI enjoys levels of efficiency far beyond what can be achieved by Boltzmann or -greedy exploration. We leave as an open problem establishing efficiency guarantees in such contexts. To stimulate thinking on this topic, we put forth a conjecture.\nConjecture 1. For all M = (S,A, H, P,R, π), Φ0, . . . ,ΦH−1, σ, and λ, if reward distributions R have support [−σ, σ], there is a unique (θ0, . . . , θH−1) ∈ RK×H satisfying Q∗h = Φhθh for h = 0, . . . ,H − 1, and∑H−1 h=0 ‖θh‖2 ≤ KH λ , then there exists a polynomial poly such that\nRegret(T,M) ≤ √ T poly ( K,H,max\nh,x,a ‖Φh(x, a)‖, σ, 1/λ\n) .\nAs one would hope for from an RL algorithm that generalizes, this bound does not depend on the number of states or actions. Instead, there is a dependence on the number of basis functions. In Appendix C we present empirical results that are consistent with this conjecture."
    }, {
      "heading" : "C. Chain experiments",
      "text" : "C.1. Generating a random coherent basis\nWe present full details for Algorithm 6, which generates the random coherent basis functions Φh ∈ RSA×K for h = 1, ..,H . In this algorithm we use some standard notation for indexing vector elements. For any A ∈ Rm×n we will write A[i, j] for the element in the ith row and jth column. We will use the placeholder · to repesent the entire axis so that, for example, A[·, 1] ∈ Rn is the first column of A.\nAlgorithm 6 Generating a random coherent basis Input: S,A,H,K ∈ N, Q∗h ∈ RSA for h = 1, ..,H Output: Φh ∈ RSA×K for h = 1, ..,H\n1: Sample Ψ ∼ N(0, I) ∈ RHSA×K 2: Set Ψ[·, 1]← 1 3: Stack Q∗ ← (Q∗1, .., Q∗h) ∈ RHSA 4: Set Ψ[·, 2]← Q∗ 5: Form projection P ← Ψ(ΨTΨ)−1ΨT 6: Sample W ∼ N(0, I) ∈ RHSA×K 7: Set W [·, 1]← 1 8: Project WP ← PW ∈ RHSA×K 9: Scale WP [·, k]← WP [·,k]‖WP [·,k]‖2HSA for k = 1, ..,K\n10: Reshape Φ← reshape(WP ) ∈ RH×SA×K 11: Return Φ[h, ·, ·] ∈ RSA×K for h = 1, ..,H The reason we rescale the value function in step (9) of Algorithm 6 is so that the resulting random basis functions are on a similar scale to Q∗. This is a completely arbitrary choice as any scaling in Φ can be exactly replicated by similar rescalings in λ and σ."
    }, {
      "heading" : "C.2. Robustness to λ, σ",
      "text" : "In Figures 10 and 11 we present the cumulative regret for N = 50,K = 10 over the first 10000 episodes for several orders of magnitude for σ and λ. For most combinations of parameters the learning remains remarkably stable.\nWe find that large values of σ lead to slowers learning, since the Bayesian posterior concentrates only very slowly with new data. However, in stochastic domains we found that choosing a σ which is too small might cause the RLSVI posterior to concentrate too quickly and so fail to sufficiently explore. This is a similar insight to previous analyses of Thompson sampling (Agrawal & Goyal, 2012) and matches the flavour of Theorem 1.\nC.3. Scaling with number of bases K In Figure 4 we demonstrated that RLSVI seems to scale gracefully with the number of basis features on a chain of length N = 50. In Figure 13 we reproduce these reults for chains of several different lengths. To highlight the overall trend we present a local polynomial regression for each chain length.\nRoughly speaking, for low numbers of featuresK the number of episodes required until learning appears to increase linearly with the number of basis features. However, the marginal increase from a new basis features seems to decrease and almost plateau once the number of features reaches the maximum dimension for the problemK ≥ SA.\nC.4. Approximate polynomial learning Our simulation results empirically demonstrate learning which appears to be polynomial in both N and K. Inspired by the results in Figure 5, we present the learning times for different N and K together with a quadratic regression fit separately for each K."
    }, {
      "heading" : "D. Tetris experiments",
      "text" : ""
    }, {
      "heading" : "D.1. Algorithm specification",
      "text" : "In Algorithm 7 we present a natural adaptation to RLSVI without known episode length, but still a regular episodic structure. This is the algorithm we use for our experiments in Tetris. The LSVI algorithms are formed in the same way.\nAlgorithm 7 Stationary RLSVI Input: Data Φ(s1,a1),r1,..,Φ(sT ,aT ) . Previous estimate θ̃−l ≡ θ̃l−1 . Parameters λ>0, σ>0, γ∈ [0,1] Output: θ̃l\n1: Generate regression problem A∈RT×K , b∈RT :\nA←  Φh(s1,a1)... Φh(sT ,aT )  bi← { ri+γmaxα ( Φθ̃−l ) (si+1,α) if si not terminal\nri if si is terminal\n2: Bayesian linear regression for the value function\nθl← 1\nσ2\n( 1\nσ2 A>A+λI\n)−1 A>b\nΣl← ( 1\nσ2 A>A+λI )−1 3: Sample θ̃l∼N(θl,Σl) from Gaussian posterior\nAlgorithm 8 RLSVI with greedy action Input: Features Φ; λ>0, σ>0, γ∈ [0,1]\n1: θ−0 ←0; t←0 2: for Episode l=0,1,.. do 3: Compute θ̃l using Algorithm 7 4: Observe st 5: while TRUE do 6: Update t← t+1 7: Sample at∈argmaxα∈A ( Φθ̃ ) (st,α) 8: Observe rt and st+1 9: if st+1 is terminal then\n10: BREAK 11: end if 12: end while 13: end for\nThis algorithm simply approximates a time-homogenous value function using Bayesian linear regression. We found that a discount rate of γ = 0.99 was helpful for stability in both RLSVI and LSVI.\nIn order to avoid growing computational and memory cost as LSVI collects more data we used a very simple strategy\nto only store the most recent N transitions. For our experiments we set N = 105. Computation for RLSVI and LSVI remained negligible compared to the cost of running the Tetris simulator for our implementations.\nTo see how small this memory requirement is note that, apart from the number of holes, every feature and reward is a positive integer between 0 and 20 inclusive. The number of holes is a positive integer between 0 and 199. We could store the information 105 transitions for every possible action using less than 10mb of memory."
    }, {
      "heading" : "D.2. Effective improvements",
      "text" : "We present the results for RLSVI with fixed σ = 1 and λ = 1. This corresponds to a Bayesian linear regression with a known noise variance in Algorithm 7. We actually found slightly better performance using a Bayesian linear regression with an inverse gamma prior over an unknown variance. This is the conjugate prior for Gaussian regression with known variance. Since the improvements were minor and it slightly complicates the algorithm we omit these results. However, we believe that using a wider prior over the variance will be more robust in application, rather than picking a specific σ and λ."
    }, {
      "heading" : "D.3. Mini-tetris",
      "text" : "In Figure 7 we show that RLSVI outperforms LSVI even with a highly tuned annealing scheme for . However, these results are much more extreme on a didactic version of mini-tetris. We make a tetris board with only 4 rows and only S, Z pieces. This problem is much more difficult and highlights the need for efficient exploration in a more extreme way.\nIn Figure 14 we present the results for this mini-tetris environment. As expected, this example highlights the benefits of RLSVI over LSVI with dithering. RLSVI greatly outperforms LSVI even with a tuned schedule. RLSVI learns faster and reaches a higher convergent policy."
    }, {
      "heading" : "E. Recommendation system experiments",
      "text" : ""
    }, {
      "heading" : "E.1. Experiment Setup",
      "text" : "For the recommendation system experiments, the experiment setup is specified in Algorithm 9. We set N = 10, J = H = 5, c = 2 and L = 1200.\nAlgorithm 9 Recommendation System Experiments: Experiment Setup Input: N ∈ Z++, J = H ∈ Z++, c > 0, L ∈ Z++ Output: ∆̂(0), . . . , ∆̂(L− 1)\nfor i = 1, . . . , 100 do Sample a problem instance γan ∼ N(0, c2) Run the Bernoulli bandit algorithm 100 times Run the linear contextual bandit algorithm 100 times for for each η ∈ {10−4, 10−3, 10−2, 10−1, 1, 10} do\nRun LSVI-Boltzmann with λ = 0.2 and η 10 times end for Run RLSVI with λ = 0.2 and σ2 = 10−3 10 times\nend for Compute the average regret for each algorithm\nThe myopic policy is defined as follows: for all episode l = 0, 1, · · · and for all step h = 0, · · · , H − 1, choose alh ∈ arg maxa P (a|xlh), where alh and xlh are respectively the action and the state at step h of episode l.\nE.2. Bernoulli bandit algorithm\nThe Bernoulli bandit algorithm is described in Algorithm 10, which is a Thompson sampling algorithm with uniform prior. Obviously, this algorithm aims to learn the myopic policy.\nAlgorithm 10 Bernoulli bandit algorithm"
    }, {
      "heading" : "Input: N ∈ N, J ∈ N, L ∈ N",
      "text" : "Initialization: Set αn = βn = 1, ∀n = 1, 2, . . . , N for l = 0, . . . , L− 1 do\nRandomly sample p̂ln ∼ beta (αn, βn), ∀n = 1, . . . , N Sort p̂ln’s in the descending order, and recommend the first J products in order to the customer for n = 1, . . . , N do\nif product n is recommended in episode l then if customer likes product then αn ← αn + 1\nelse βn ← βn + 1\nend if end if\nend for end for\nE.3. Linear contextual bandit algorithm\nIn this subsection, we describe the linear contextual bandit algorithm. The linear contextual bandit algorithm is similar to RLSVI, but without backward value propagation, a key feature of RLSVI. It is straightforward to see that the linear contextual bandit algorithm aims to learn the myopic policy. This algorithm is specified in Algorithm 11 and 12. Notice that this algorithm can be implemented incrementally, hence, it is computationally efficient. In this computational study, we use the same basis functions as RLSVI, and the same algorithm parameters (i.e. λ = 0.2 and σ2 = 10−3).\nAlgorithm 11 Randomized exploration in linear contextual bandits Input: Data Φ(si0,ai0),ri0,..,Φ(siH−1,aiH−1),riH : i<L . Parameters λ>0, σ>0 Output: θ̂l0,...,θ̂l,H−1\n1: θ̂lH←0, ΦH←0 2: for h=H−1,...,1,0 do 3: Generate regression matrix and vector\nA←  Φh(s0h,a0h)... Φh(sl−1,h,al−1,h)  b←\n r0,h... rl−1,h  4: Estimate value function\nθlh← 1\nσ2\n( 1\nσ2 A>A+λσ2I\n)−1 A>b\nΣlh← ( 1\nσ2 A>A+λI )−1 5: Sample θ̂lh∼N(θlh,Σlh) 6: end for\nAlgorithm 12 Linear contextual bandit algorithm Input: Features Φ0,..,ΦH , σ>0,λ>0\n1: for l=0,1,··· do 2: Compute θ̂l0,...,θ̂l,H−1 using Algorithm 11 3: Observe xl0 4: for h=0,···,H−1 do 5: Sample alh∼unif ( argmaxα∈A ( Φhθ̂lh ) (xlh,α)\n) 6: Observe rlh and xl,h+1 7: end for 8: end for"
    }, {
      "heading" : "F. Extensions",
      "text" : "We now briefly discuss a couple possible extensions of the version of RLSVI proposed in Algorithm 1 and 8. One is an incremental version which is computationally more efficient. The other addresses continual learning in an infinite horizon discounted Markov decision process. In the same sense that RLSVI shares much with LSVI but is distinguished by its new approach to exploration, these extensions share much with least-squares Q-learning (Lagoudakis et al., 2002).\nF.1. Incremental learning\nNote that Algorithm 1 is a batch learning algorithm, in the sense that, in each episode l, though Σlh’s can be computed incrementally, it needs all past observations to compute θ̄lh’s. Thus, its per-episode compute time grows with l, which is undesirable if the algorithm is applied over many episodes.\nOne way to fix this problem is to derive an incremental RLSVI that updates θ̄lh’s and Σlh’s using summary statistics of past data and new observations made over the most recent episode. One approach is to do this by computing\nΣ−1l+1,h ← (1− νl)Σ −1 lh +\n1\nσ2 Φh (xlh, alh)\n> Φh (xlh, alh)\nyl+1,h ← (1− νl)ylh + 1\nσ2\n[ rlh + max\nα∈A\n( Φh+1θ̃l,h+1 ) (xl,h+1, α) ] Φh (xlh, alh) > , (4)\nand setting θ̄l+1,h = Σ−1l+1,hyl+1,h. Note that we sample θ̃lh ∼ N(θ̄lh,Σlh), and initialize y0h = 0, Σ −1 0h = λI , ∀h. The step size νl controls the influence of past observations on Σlh and θ̄lh. Once θ̃lh’s are computed, the actions are chosen based on Algorithm 8. Another approach would be simply to approximate the solution for θlh numerically via random sampling and stochastic gradient descent similar to other works with non-linear architectures (Mnih, 2015). The per-episode compute time of these incremental algorithms are episode-independent, which allows for deployment at large scale. On the other hand, we expect the batch version of RLSVI to be more data efficient and thus incur lower regret."
    }, {
      "heading" : "F.2. Continual learning",
      "text" : "Finally, we propose a version of RLSVI for RL in infinite-horizon time-invariant discounted MDPs. A discounted MDP is identified by a sextupleM = (S,A, γ, P,R, π), where γ ∈ (0, 1) is the discount factor. S,A, P,R, π are defined similarly with the finite horizon case. Specifically, in each time t = 0, 1, . . ., if the state is xt and an action at is selected then a subsequent state xt+1 is sampled from P (·|xt, at) and a reward rt is sampled from R (·|xt, at, xt+1). We also use V ∗ to denote the optimal state value function, and Q∗ to denote the optimal action-contingent value function. Note that V ∗ and Q∗ do not depend on t in this case.\nAlgorithm 13 Continual RLSVI Input: θ̃t ∈ RK , wt ∈ RK , Φ ∈ R|S||A|×K , σ > 0, λ > 0, γ ∈ (0, 1), {(xτ , aτ , rτ ) : τ ≤ t}, xt+1 Output: θ̃t+1 ∈ RK , wt+1 ∈ RK\n1: Generate regression matrix and vector\nA←  Φ(x0, a0)... Φ(xt, at)  b←  r0 + γmaxα∈A ( Φθ̃t ) (x1, α) ...\nrt + γmaxα∈A ( Φθ̃t ) (xt+1, α)  2: Estimate value function\nθt+1 ← 1\nσ2\n( 1\nσ2 A>A+ λI\n)−1 A>b Σt+1 ← ( 1\nσ2 A>A+ λI )−1 3: Sample wt+1 ∼ N( √ 1− γ2wt, γ2Σt+1) 4: Set θ̃t+1 = θt+1 + wt+1\nSimilarly with the episodic case, an RL algorithm generates each action at based on observations made up to time t,\nincluding all states, actions, and rewards observed in previous time steps, as well as the state space S, action space A, discount factor γ, and possible prior information. We consider a scenario in which the agent has prior knowledge that Q∗ lies within a linear space spanned by a generalization matrix Φ ∈ R|S||A|×K .\nA version of RLSVI for continual learning is presented in Algorithm 13. Note that θ̃t and wt are values computed by the algorithm in the previous time period. We initialize θ̃0 = 0 and w0 = 0. Similarly to Algorithm 1, Algorithm 13 randomly perturbs value estimates in directions of significant uncertainty to incentivize exploration. Note that the random perturbation vectors wt+1 ∼ N( √ 1− γ2wt, γ2Σt+1) are sampled to ensure autocorrelation and that marginal covariance matrices of consecutive perturbations differ only slightly. In each period t, once θ̃t is computed, a greedy action is selected. Avoiding frequent abrupt changes in the perturbation vector is important as this allows the agent to execute on multi-period plans to reach poorly understood state-action pairs."
    }, {
      "heading" : "G. Gaussian vs Dirichlet optimism",
      "text" : "The goal of this subsection is to prove Lemma 1, reproduced below:\nFor all v ∈ [0, 1]N and α ∈ [1,∞)N with αT1 ≥ 2, if x ∼ N(α>v/α>1, 1/α>1) and y = pT v for p ∼ Dirichlet(α) then x <so y.\nWe begin with a lemma recapping some basic equivalences of stochastic optimism.\nLemma 3 (Optimism equivalence). The following are equivalent to X <so Y :\n1. For any random variable Z independent of X and Y , E[max(X,Z)] ≥ E[max(Y,Z)] 2. For any α ∈ R, ∫∞ α {P(X ≥ s)− P(Y ≥ s)} ds ≥ 0. 3. X =D Y +A+W for A ≥ 0 and E [W |Y +A] = 0 for all values y + a. 4. For any u : R→ R convex and increasing E[u(X)] ≥ E[u(Y )]\nThese properties are well known from the theory of second order stochastic dominance (Levy, 1992; Hadar & Russell, 1969) but can be re-derived using only elementary integration by parts. X <so Y if and only if −Y is second order stochastic dominant for −X ."
    }, {
      "heading" : "G.1. Beta vs. Dirichlet",
      "text" : "In order to prove Lemma 1 we will first prove an intermediate result that shows a particular Beta distribution ỹ is optimistic for y. Before we can prove this result we first state a more basic result that we will use on Gamma distributions.\nLemma 4. For independent random variables γ1 ∼ Gamma(k1, θ) and γ2 ∼ Gamma(k2, θ),\nE[γ1|γ1 + γ2] = k1\nk1 + k2 (γ1 + γ2) and E[γ2|γ1 + γ2] = k2 k1 + k2 (γ1 + γ2).\nWe can now present our optimistic lemma for Beta versus Dirichlet.\nLemma 5. Let y = p>v for some random variable p ∼ Dirichlet(α) and constants v ∈ <d and α ∈ Nd. Without loss of generality, assume v1 ≤ v2 ≤ · · · ≤ vd. Let α̃ = ∑d i=1 αi(vi − v1)/(vd − v1) and β̃ = ∑d i=1 αi(vd − vi)/(vd − v1). Then, there exists a random variable p̃ ∼ Beta(α̃, β̃) such that, for ỹ = p̃vd + (1− p̃)v1, E[ỹ|y] = E[y]. Proof. Let γi = Gamma(α, 1), with γ1, . . . , γd independent, and let γ = ∑d i=1 γi, so that\np ≡D γ/γ.\nLet α0i = αi(vi − v1)/(vd − v1) and α1i = αi(vd − vi)/(vd − v1) so that\nα = α0 + α1.\nDefine independent random variables γ0 ∼ Gamma(α0i , 1) and γ1 ∼ Gamma(α1i , 1) so that\nγ ≡D γ0 + γ1.\nTake γ0 and γ1 to be independent, and couple these variables with γ so that γ = γ0 + γ1. Note that β̃ = ∑d i=1 α 0 i and\nα̃ = ∑d i=1 α 1 i . Let γ 0 = ∑d i=1 γ 0 i and γ 1 = ∑d i=1 γ 1 i , so that\n1− p̃ ≡D γ0/γ and p̃ ≡D γ1/γ.\nCouple these variables so that 1− p̃ = γ0/γ and p̃ = γ1/γ. We then have E[ỹ|y] = E[(1− p̃)v1 + p̃vd|y] = E [ v1γ 0\nγ + vdγ\n1\nγ ∣∣∣y] = E [E [v1γ0 + vdγ1 γ ∣∣∣γ, y] ∣∣∣y] = E [ v1E[γ\n0|γ] + vdE[γ1|γ] γ ∣∣∣y] = E[v1∑di=1E[γ0i |γi] + vd∑di=1E[γ1i |γi] γ ∣∣∣y] (a) = E [ v1 ∑d i=1 γiα 0 i /αi + vd ∑d i=1 γiα 1 i /αi\nγ\n∣∣∣y]\n= E\n[ v1 ∑d i=1 γi(vi − v1) + vd ∑d i=1 γi(vd − vi)\nγ(vd − v1)\n∣∣∣y]\n= E [∑d i=1 γivi γ ∣∣∣y] = E[ d∑ i=1 pivi ∣∣∣y] = y, where (a) follows from Lemma 4."
    }, {
      "heading" : "G.2. Gaussian vs Beta",
      "text" : "In the previous section we showed that a matched Beta distribution ỹ would be optimistic for the Dirichlet y. We will now show that the Normal random variable x is optimistic for ỹ and so complete the proof of Lemma 1, x <so ỹ <so y.\nUnfortunately, unlike the case of Beta vs Dirichlet it is quite difficult to show this optimism relationship between Gaussian x and Beta ỹ directly. Instead we make an appeal to the stronger dominance relationship of single-crossing CDFs.\nDefinition 2 (Single crossing dominance). Let X and Y be real-valued random variables with CDFs FX and FY respectively. We say that X single-crossing dominates Y if E[X] ≥ E[Y ] and there a crossing point a ∈ R such that:\nFX(s) ≥ FY (s) ⇐⇒ s ≤ a. (5)\nNote that single crossing dominance implies stochastic optimism. The remainder of this section is devoted to proving that the following lemma:\nLemma 6. Let ỹ ∼ Beta(α, β) for any α > 0, β > 0 and x ∼ N ( µ = αα+β , σ 2 = 1α+β ) . Then, x single crossing dominates ỹ.\nTrivially, these two distributions will always have equal means so it is enough to show that their CDFs can cross at most once on (0, 1)."
    }, {
      "heading" : "G.3. Double crossing PDFs",
      "text" : "By repeated application of the mean value theorem, if we want to prove that the CDFs cross at most once on (0, 1) then it is sufficient to prove that the PDFs cross at most twice on the same interval. Our strategy will be to show via mechanical calculus that for the known densities of x and ỹ the PDFs cross at most twice on (0, 1). We lament that the proof as it stands is so laborious, but our attempts at a more elegant solution has so far been unsucessful. The remainder of this appendix is devoted to proving this “double-crossing” property via manipulation of the PDFs for different values of α, β.\nWe write fN for the density of the Normal x and fB for the density of the Beta ỹ respectively. We know that at the boundary fN (0−) > fB(0−) and fN (1+) > fB(1+) where the ± represents the left and right limits respectively. Since the densities are postive over the interval, we can consider the log PDFs instead.\nlB(x) = (α− 1) log(x) + (β − 1) log(1− x) +KB\nlN (x) = − 1\n2 (α+ β)\n( x− α\nα+ β\n)2 +KN\nSince log(x) is injective and increasing, if we could show that lN (x)− lB(x) = 0 has at most two solutions on the interval we would be done.\nInstead we will attempt to prove an even stronger condition, that l′N (x)−l′B(x) = 0 has at most one solution in the interval. This is not necessary for what we actually want to show, but it is sufficient and easier to deal with since we can ignore the annoying constants.\nl′B(x) = α− 1 x − β − 1\n1− x l′N (x) = α− (α+ β)x\nFinally we will consider an even stronger condition, if l′′N (x) − l′′B(x) = 0 has no solution then l′B(x) − l′N (x) must be monotone over the region and so it can have at most one root.\nl′′B(x) = − α− 1 x2 − β − 1 (1− x)2\nl′′N (x) = −(α+ β)\nSo now let us define:\nh(x) := l′′N (x)− l′′B(x) = α− 1 x2 + β − 1 (1− x)2 − (α+ β) (6)\nOur goal now is to show that h(x) = 0 does not have any solutions for x ∈ [0, 1].\nOnce again, we will look at the derivatives and analyse them for different values of α, β > 0. h′(x) = −2 ( α− 1 x3 − β − 1 (1− x)3 )\nh′′(x) = 6 ( α− 1 x4 + β − 1 (1− x)4 ) G.3.1. SPECIAL CASE α > 1, β ≤ 1\nIn this region we want to show that actually g(x) = l′N (x) − l′B(x) has no solutions. We follow a very similar line of argument and write A = α− 1 > 0 and B = β − 1 ≤ 0 as before.\ng(x) = α− (α+ β)x+ β − 1 1− x − α− 1 x\ng′(x) = h(x) = A\nx2 +\nB\n(1− x)2 − (α+ β)\ng′′(x) = h′(x) = −2 ( A\nx3 − B (1− x)3 ) Now since B ≤ 0 we note that g′′(x) ≤ 0 and so g(x) is a concave function. If we can show that the maximum of g lies below 0 then we know that there can be no roots.\nWe now attempt to solve g′(x) = 0:\ng′(x) = A\nx2 +\nB\n(1− x)2 = 0 =⇒ −A/B = ( x\n1− x )2 =⇒ x = K\n1 +K ∈ (0, 1)\nWhere here we write K = √ −A/B > 0. We’re ignoring the case of B = 0 as this is even easier to show separately. We now evaluate the function g at its minimum xK = K1+K and write C = −B ≥ 0.\ng(xK) = (A+ 1)− (A+B + 2) K 1 +K +B(1 +K)−A1 +K K\n= −AK2 −AK −A+BK3 +BK2 +BK −K2 +K = −AK2 −AK −A− CK3 − CK2 − CK −K2 +K = −A(A/C)−A(A/C)1/2 −A− C(A/C)3/2 − C(A/C)− C(A/C)1/2 −A/C + (A/C)1/2 = −A2C−1 −A3/2C−1/2 −A−A3/2C−1/2 −A−A1/2C1/2 −AC−1 +A1/2C1/2 = −A2C−1 − 2A3/2C−1/2 − 2A−AC−1 ≤ 0\nTherefore we are done with this sub proof. The case of α ≤ 1, β > 1 can be dealt with similarly.\nG.3.2. CONVEX FUNCTION α > 1, β > 1, (α− 1)(β − 1) ≥ 19 In the case of α, β > 1 we know that h(x) is a convex function on (0, 1). So now if we solve h′(x∗) = 0 and h(x∗) > 0 then we have proved our statement. We will write A = α− 1, B = β − 1 for convenience.\nWe now attempt to solve h′(x) = 0\nh′(x) = A x3 − B (1− x)3 = 0\n=⇒ A/B = ( x\n1− x )3 =⇒ x = K\n1 +K ∈ (0, 1)\nWhere for convenience we have written K = (A/B)1/3 > 0. We now evaluate the function h at its minimum xK = K1+K .\nh(xK) = A (K + 1)2\nK2 +B(K + 1)2 − (A+B + 2)\n= A(2/K + 1/K2) +B(K2 + 2K)− 2 = 3(A2/3B1/3 +A1/3B2/3)− 2\nSo as long as h(xK) > 0 we have shown that the CDFs are single crossing. We note a simpler characterization of A,B that guarantees this condition:\nA,B ≥ 1/3 =⇒ AB ≥ 1/9 =⇒ (A2/3B1/3 +A1/3B2/3) ≥ 2/3\nAnd so we have shown that somehow for α, β large enough away from 1 we are OK. Certianly we have proved the result for α, β ≥ 4/3.\nG.3.3. FINAL REGION {α > 1, β > 1, (α− 1)(β − 1) ≤ 19}\nWe now produce a final argument that even in this remaining region the two PDFs are at most double crossing. The argument is really no different than before, the only difficulty is that it is not enough to only look at the derivatives of the log likelihoods, we need to use some bound on the normalizing constants to get our bounds. By symmetry in the problem, it will suffice to consider only the case α > β, the other result follows similarly.\nIn this region of interest, we know that β ∈ (1, 43 ) and so we will make use of an upper bound to the normalizing constant of the Beta distribution, the Beta function.\nB(α, β) = ∫ 1 x=0 xα−1(1− x)β−1dx\n≤ ∫ 1 x=0 xα−1dx = 1 α (7)\nOur thinking is that, because in B the value of β − 1 is relatively small, this approximation will not be too bad. Therefore, we can explicitly bound the log likelihood of the Beta distribution:\nlB(x) ≥ l̃B(x) := (α− 1) log(x) + (β − 1) log(1− x) + log(α)\nWe will now make use of a calculus argument as in the previous sections of the proof. We want to find two points x1 < x2 for which h(xi) = l′′N (x)− l′′B(x) > 0. Since α, β > 1 we know that h is convex and so for all x /∈ [x1, x2] then h > 0. If we can also show that the gap of the Beta over the maximum of the normal log likelihood\nGap : lB(xi)− lN (xi) ≥ f(xi) := l̃B(xi)−max x lN (x) > 0 (8)\nis positive then it must mean there are no crossings over the region [x1, x2], since l̃B is concave and therefore totally above the maximum of lN over the whole region [x1, x2].\nNow consider the regions x ∈ [0, x1), we know by consideration of the tails that if there is more than one root in this segment then there must be at least three crossings. If there are three crossings, then the second derivative of their difference h must have at least one root on this region. However we know that h is convex, so if we can show that h(xi) > 0 this cannot be possible. We use a similar argument for x ∈ (x2, 1]. We will now complete this proof by lengthy amounts of calculus.\nLet’s remind ourselves of the definition:\nh(x) := l′′N (x)− l′′B(x) = α− 1 x2 + β − 1 (1− x)2 − (α+ β)\nFor ease of notation we will write A = α− 1, B = β − 1. We note that:\nh(x) ≥ h1(x) = A\nx2 − (A+B + 2), h(x) ≥ h2(x) =\nB\n(1− x)2 − (A+B + 2)\nand we solve for h1(x1) = 0, h2(x2) = 0. This means that\nx1 =\n√ A\nA+B + 2 , x2 = 1−\n√ B\nA+B + 2\nand clearly h(x1) > 0, h(x2) > 0. Now, if we can show that, for all possible values of A,B in this region f(xi) = lB(xi)−maxx lN (x) > 0, our proof will be complete.\nWe will now write f(xi) = fi(A,B) to make the dependence on A,B more clear.\nf1(A,B) = log(1 +A) +A log\n(√ A\nA+B + 2\n) +B log ( 1− √ A\nA+B + 2\n) + 1\n2 log(2π)− 1 2 log(A+B + 2)\nf2(A,B) = log(1 +A) +A log\n( 1− √ B\nA+B + 2\n) +B log (√ B\nA+B + 2\n) + 1\n2 log(2π)− 1 2 log(A+B + 2)\nWe will now show that ∂fi∂B ≤ 0 for all of the values in our region A > B > 0.\n∂f1 ∂B = − A 2(A+B + 2) + log\n( 1− √ A\nA+B + 2\n) +\nB √ A\n2(A+B + 2)3/2 ( 1− √\nA A+B+2\n) − 1 2(A+B + 2)\n= 1\n2(A+B + 2)  B√A√ A+B + 2 ( 1− √ A\nA+B+2\n) −A− 1 + log(1−√ A\nA+B + 2\n)\n= 1\n2(A+B + 2)\n( B √ A\n√ A+B + 2− √ A −A− 1\n) + log ( 1− √ A\nA+B + 2\n)\n≤ 1 2(A+B + 2)\n( √ B/3\n√ A+B + 2− √ A −A− 1\n) − √\nA\nA+B + 2\n≤ 1 2(A+B + 2)\n( 1\n3\n√ B\nB + 2 −A− 1\n) − √\nA\nA+B + 2\n≤ − A 2(A+B + 2)\n− √\nA\nA+B + 2\n≤ 0\nand similarly,\n∂f2 ∂B = −A\n √ B A+B+2\n2B +\n1\n2(A+B + 2) + log(√ B A+B + 2 ) +B ( A+ 2 2B(A+B + 2) ) − 1 2(A+B + 2)\n= 1\n2(A+B + 2)\n( A+ 2−A− 1−A √ A+B + 2\nB\n) + log (√ B\nA+B + 2\n)\n= 1\n2(A+B + 2)\n( 1−A √ A+B + 2\nB\n) + 1\n2 log\n( B\nA+B + 2\n)\nNow we can look at each term to observe that ∂ 2f2\n∂A∂B < 0. Therefore this expression ∂f2 ∂B is maximized over A for A = 0.\nWe now examine this expression:\n∂f2 ∂B ∣∣ A=0 = 1 2(B + 2) + 1 2 log\n( B\nB + 2\n) ≤ 1\n2\n( 1\nB + 2 +\nB B + 2 − 1 ) ≤ 0\nTherefore, the expressions fi are minimized at at the largest possible B = 19A for any given A over our region. We will now write gi(A) := fi(A, 19A ) for this evalutation at the extremal boundary. If we can show that gi(A) ≥ 0 for all A ≥ 1 3 and i = 1, 2 we will be done.\nWe will perform a similar argument to show that gi is monotone increasing, g′i(A) ≥ 0 for all A ≥ 13 .\ng1(A) = log(1 +A) +A log\n(√ A\nA+ 19A + 2\n) + 1\n9A log\n( 1− √ A\nA+ 19A + 2\n)\n+ 1 2 log(2π)− 1 2 log(A+ 1 9A + 2)\n= log(1 +A) + A 2 log(A)− 1 2 (1 +A) log(A+ 1 9A + 2)\n+ 1\n9A log\n( 1− √ A\nA+ 19A + 2\n) + 1\n2 log(2π)\nNote that the function p(A) = A + 19A is increasing in A for A ≥ 1 3 . We can conservatively bound g from below noting\n1 9A ≤ 1 in our region.\ng1(A) ≥ = log(1 +A) + A 2 log(A)− 1 2 (1 +A) log(A+ 3) + 1 9A log\n( 1− √ A\nA+ 2\n) + 1\n2 log(2π)\n≥ log(1 +A) + A 2 log(A)− 1 2 (1 +A) log(A+ 3)− 1 9A\n√ A+ 1\n2 log(2π) =: g̃1(A)\nNow we can use calculus to say that:\ng̃′1(A) = 1\nA+ 1 +\n1\nA+ 3 +\nlog(A)\n2 +\n1 18A3/2 − 1 2 log(A+ 3)\n≥ 1 A+ 1 + 1 A+ 3 +\n1\n18A3/2 +\n1 2 log( A A+ 3 )\nThis expression is monotone decreasing in A and with a limit ≥ 0 and so we can say that g̃1(A) is monotone increasing. Therefore g1(A) ≥ g̃1(A) ≥ g̃1(1/3) for all A. We can explicitly evaluate this numerically and g̃1(1/3) > 0.01 so we are done.\nThe final piece of this proof is to do a similar argument for g2(A)\ng2(A) = log(1 +A) +A log\n( 1− √ 1 9A\nA+ 19A + 2\n) + 1\n9A log\n(√ 1\n9A\nA+ 19A + 2\n)\n+ 1 2 log(2π)− 1 2 log(A+ 1 9A + 2)\n= log(1 +A) +A log ( 1− √ 1\n9A2 + 18A+ 1\n) + 1\n2\n( 1\n9A log\n( 1\n9A )) −1\n2\n( 1\n9A + 1\n) log ( A+ 1\n9A + 2\n) + 1\n2 log(2π) ≥ log(1 +A) +A ( − 1√\n9A2\n) + 1\n2\n( 1\n9A log\n( 1\n9A\n)) − 1\n2\n( 1\n3 + 1\n) log ( A+ 1\n3 + 2\n) + 1\n2 log(2π)\n≥ log(1 +A)− 1 3 − 1 2e − 2 3 log(A+ 7 3 ) + 1 2 log(2π) =: g̃2(A)\nNow, once again we can see that g̃2 is monotone increasing:\ng̃′2(A) = 1 1 +A − 2/3 A+ 7/3\n= A+ 5\n(A+ 1)(3A+ 7) ≥ 0\nWe complete the argument by noting g2(A) ≥ g̃2(A) ≥ g̃2(1/3) > 0.01, which concludes our proof of the PDF double crossing in this region."
    }, {
      "heading" : "G.4. Recap",
      "text" : "Using the results of the previous sections we complete the proof of Lemma 6 for Gaussian vs Beta dominance for all possible α, β > 0 such that α + β ≥ 1. Piecing together Lemma 5 with Lemma 6 completes our proof of Lemma 1. We imagine that there is a much more elegant and general proof method available for future work."
    } ],
    "references" : [ {
      "title" : "Regret bounds for the adaptive control of linear quadratic systems",
      "author" : [ "Abbasi-Yadkori", "Yasin", "Szepesvári", "Csaba" ],
      "venue" : "Journal of Machine Learning Research - Proceedings Track,",
      "citeRegEx" : "Abbasi.Yadkori et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Abbasi.Yadkori et al\\.",
      "year" : 2011
    }, {
      "title" : "Further optimal regret bounds for Thompson sampling",
      "author" : [ "Agrawal", "Shipra", "Goyal", "Navin" ],
      "venue" : "arXiv preprint arXiv:1209.3353,",
      "citeRegEx" : "Agrawal et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Agrawal et al\\.",
      "year" : 2012
    }, {
      "title" : "Temporal differences-based policy iteration and applications in neuro-dynamic programming",
      "author" : [ "Bertsekas", "Dimitri P", "Ioffe", "Sergey" ],
      "venue" : "Lab. for Info. and Decision Systems Report LIDS-P-2349,",
      "citeRegEx" : "Bertsekas et al\\.,? \\Q1996\\E",
      "shortCiteRegEx" : "Bertsekas et al\\.",
      "year" : 1996
    }, {
      "title" : "R-max - a general polynomial time algorithm for near-optimal reinforcement learning",
      "author" : [ "Brafman", "Ronen I", "Tennenholtz", "Moshe" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Brafman et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Brafman et al\\.",
      "year" : 2002
    }, {
      "title" : "Sample complexity of episodic fixed-horizon reinforcement learning",
      "author" : [ "Dann", "Christoph", "Brunskill", "Emma" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Dann et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Dann et al\\.",
      "year" : 2015
    }, {
      "title" : "Approximate dynamic programming finally performs well in the game of tetris",
      "author" : [ "Gabillon", "Victor", "Ghavamzadeh", "Mohammad", "Scherrer", "Bruno" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Gabillon et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Gabillon et al\\.",
      "year" : 2013
    }, {
      "title" : "Rules for ordering uncertain prospects",
      "author" : [ "Hadar", "Josef", "Russell", "William R" ],
      "venue" : "The American Economic Review, pp",
      "citeRegEx" : "Hadar et al\\.,? \\Q1969\\E",
      "shortCiteRegEx" : "Hadar et al\\.",
      "year" : 1969
    }, {
      "title" : "Nearoptimal regret bounds for reinforcement learning",
      "author" : [ "Jaksch", "Thomas", "Ortner", "Ronald", "Auer", "Peter" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Jaksch et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Jaksch et al\\.",
      "year" : 2010
    }, {
      "title" : "On the Sample Complexity of Reinforcement Learning",
      "author" : [ "Kakade", "Sham" ],
      "venue" : "PhD thesis,",
      "citeRegEx" : "Kakade and Sham.,? \\Q2003\\E",
      "shortCiteRegEx" : "Kakade and Sham.",
      "year" : 2003
    }, {
      "title" : "Efficient reinforcement learning in factored MDPs",
      "author" : [ "Kearns", "Michael J", "Koller", "Daphne" ],
      "venue" : "In IJCAI, pp",
      "citeRegEx" : "Kearns et al\\.,? \\Q1999\\E",
      "shortCiteRegEx" : "Kearns et al\\.",
      "year" : 1999
    }, {
      "title" : "Near-optimal reinforcement learning in polynomial time",
      "author" : [ "Kearns", "Michael J", "Singh", "Satinder P" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Kearns et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Kearns et al\\.",
      "year" : 2002
    }, {
      "title" : "Least-squares methods in reinforcement learning for control",
      "author" : [ "Lagoudakis", "Michail", "Parr", "Ronald", "Littman", "Michael L" ],
      "venue" : "In Second Hellenic Conference on Artificial Intelligence (SETN-02),",
      "citeRegEx" : "Lagoudakis et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Lagoudakis et al\\.",
      "year" : 2002
    }, {
      "title" : "The sample-complexity of general reinforcement learning",
      "author" : [ "Lattimore", "Tor", "Hutter", "Marcus", "Sunehag", "Peter" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Lattimore et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Lattimore et al\\.",
      "year" : 2013
    }, {
      "title" : "Stochastic dominance and expected utility: survey and analysis",
      "author" : [ "Levy", "Haim" ],
      "venue" : "Management Science,",
      "citeRegEx" : "Levy and Haim.,? \\Q1992\\E",
      "shortCiteRegEx" : "Levy and Haim.",
      "year" : 1992
    }, {
      "title" : "Reducing reinforcement learning to KWIK online regression",
      "author" : [ "Li", "Lihong", "Littman", "Michael" ],
      "venue" : "Annals of Mathematics and Artificial Intelligence,",
      "citeRegEx" : "Li et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2010
    }, {
      "title" : "Knows what it knows: a framework for self-aware learning",
      "author" : [ "Li", "Lihong", "Littman", "Michael L", "Walsh", "Thomas J" ],
      "venue" : "In ICML, pp",
      "citeRegEx" : "Li et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Li et al\\.",
      "year" : 2008
    }, {
      "title" : "State of the art control of atari games using shallow reinforcement learning",
      "author" : [ "Liang", "Yitao", "Machado", "Marlos C", "Talvitie", "Erik", "Bowling", "Michael H" ],
      "venue" : "CoRR, abs/1512.01563,",
      "citeRegEx" : "Liang et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Liang et al\\.",
      "year" : 2015
    }, {
      "title" : "Human-level control through deep reinforcement learning",
      "author" : [ "Mnih", "Volodymyr" ],
      "venue" : "Nature, 518(7540):529–533,",
      "citeRegEx" : "Mnih and Volodymyr,? \\Q2015\\E",
      "shortCiteRegEx" : "Mnih and Volodymyr",
      "year" : 2015
    }, {
      "title" : "Online regret bounds for undiscounted continuous reinforcement learning",
      "author" : [ "Ortner", "Ronald", "Ryabko", "Daniil" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Ortner et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Ortner et al\\.",
      "year" : 2012
    }, {
      "title" : "Model-based reinforcement learning and the eluder dimension",
      "author" : [ "Osband", "Ian", "Van Roy", "Benjamin" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Osband et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Osband et al\\.",
      "year" : 2014
    }, {
      "title" : "Near-optimal reinforcement learning in factored MDPs",
      "author" : [ "Osband", "Ian", "Van Roy", "Benjamin" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Osband et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Osband et al\\.",
      "year" : 2014
    }, {
      "title" : "Bootstrapped thompson sampling and deep exploration",
      "author" : [ "Osband", "Ian", "Van Roy", "Benjamin" ],
      "venue" : "arXiv preprint arXiv:1507.00300,",
      "citeRegEx" : "Osband et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Osband et al\\.",
      "year" : 2015
    }, {
      "title" : "More) efficient reinforcement learning via posterior sampling",
      "author" : [ "Osband", "Ian", "Russo", "Daniel", "Van Roy", "Benjamin" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Osband et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Osband et al\\.",
      "year" : 2013
    }, {
      "title" : "PAC optimal exploration in continuous space Markov decision processes",
      "author" : [ "Pazis", "Jason", "Parr", "Ronald" ],
      "venue" : "In AAAI. Citeseer,",
      "citeRegEx" : "Pazis et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Pazis et al\\.",
      "year" : 2013
    }, {
      "title" : "Eluder dimension and the sample complexity of optimistic exploration",
      "author" : [ "Russo", "Dan", "Van Roy", "Benjamin" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Russo et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Russo et al\\.",
      "year" : 2013
    }, {
      "title" : "Learning to optimize via posterior sampling",
      "author" : [ "Russo", "Daniel", "Van Roy", "Benjamin" ],
      "venue" : "Mathematics of Operations Research,",
      "citeRegEx" : "Russo et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Russo et al\\.",
      "year" : 2014
    }, {
      "title" : "PAC model-free reinforcement learning",
      "author" : [ "Strehl", "Alexander L", "Li", "Lihong", "Wiewiora", "Eric", "Langford", "John", "Littman", "Michael L" ],
      "venue" : "In ICML, pp",
      "citeRegEx" : "Strehl et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Strehl et al\\.",
      "year" : 2006
    }, {
      "title" : "Reinforcement Learning: An Introduction",
      "author" : [ "Sutton", "Richard", "Barto", "Andrew" ],
      "venue" : null,
      "citeRegEx" : "Sutton et al\\.,? \\Q1998\\E",
      "shortCiteRegEx" : "Sutton et al\\.",
      "year" : 1998
    }, {
      "title" : "Algorithms for Reinforcement Learning",
      "author" : [ "Szepesvári", "Csaba" ],
      "venue" : "Synthesis Lectures on Artificial Intelligence and Machine Learning. Morgan & Claypool Publishers,",
      "citeRegEx" : "Szepesvári and Csaba.,? \\Q2010\\E",
      "shortCiteRegEx" : "Szepesvári and Csaba.",
      "year" : 2010
    }, {
      "title" : "Temporal difference learning and tdgammon",
      "author" : [ "Tesauro", "Gerald" ],
      "venue" : "Communications of the ACM,",
      "citeRegEx" : "Tesauro and Gerald.,? \\Q1995\\E",
      "shortCiteRegEx" : "Tesauro and Gerald.",
      "year" : 1995
    }, {
      "title" : "On the likelihood that one unknown probability exceeds another in view of the evidence of two samples",
      "author" : [ "W.R. Thompson" ],
      "venue" : null,
      "citeRegEx" : "Thompson,? \\Q1933\\E",
      "shortCiteRegEx" : "Thompson",
      "year" : 1933
    }, {
      "title" : "Efficient exploration and value function generalization in deterministic systems",
      "author" : [ "Wen", "Zheng", "Van Roy", "Benjamin" ],
      "venue" : "In NIPS, pp",
      "citeRegEx" : "Wen et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Wen et al\\.",
      "year" : 2013
    } ],
    "referenceMentions" : [ {
      "referenceID" : 7,
      "context" : "This matches the worst case lower bound for this problem up to logarithmic factors (Jaksch et al., 2010).",
      "startOffset" : 83,
      "endOffset" : 104
    }, {
      "referenceID" : 7,
      "context" : ", UCRL2 (Jaksch et al., 2010)) adapted to this context.",
      "startOffset" : 8,
      "endOffset" : 29
    }, {
      "referenceID" : 12,
      "context" : "There is a sizable literature on RL algorithms that are provably efficient in tabula rasa contexts (Brafman & Tennenholtz, 2002; Kakade, 2003; Kearns & Koller, 1999; Lattimore et al., 2013; Ortner & Ryabko, 2012; Osband et al., 2013; Strehl et al., 2006).",
      "startOffset" : 99,
      "endOffset" : 254
    }, {
      "referenceID" : 22,
      "context" : "There is a sizable literature on RL algorithms that are provably efficient in tabula rasa contexts (Brafman & Tennenholtz, 2002; Kakade, 2003; Kearns & Koller, 1999; Lattimore et al., 2013; Ortner & Ryabko, 2012; Osband et al., 2013; Strehl et al., 2006).",
      "startOffset" : 99,
      "endOffset" : 254
    }, {
      "referenceID" : 26,
      "context" : "There is a sizable literature on RL algorithms that are provably efficient in tabula rasa contexts (Brafman & Tennenholtz, 2002; Kakade, 2003; Kearns & Koller, 1999; Lattimore et al., 2013; Ortner & Ryabko, 2012; Osband et al., 2013; Strehl et al., 2006).",
      "startOffset" : 99,
      "endOffset" : 254
    }, {
      "referenceID" : 15,
      "context" : "regression (Li & Littman, 2010; Li et al., 2008).",
      "startOffset" : 11,
      "endOffset" : 48
    }, {
      "referenceID" : 30,
      "context" : "The manner in which RLSVI explores is inspired by Thompson sampling (Thompson, 1933), which has been shown to explore efficiently across a very general class of online optimization problems (Russo & Van Roy, 2013; 2014).",
      "startOffset" : 68,
      "endOffset" : 84
    }, {
      "referenceID" : 22,
      "context" : "RLSVI bears a close connection to PSRL (Osband et al., 2013), which maintains and samples from a posterior distribution over MDPs and is a direct application of Thompson sampling to RL.",
      "startOffset" : 39,
      "endOffset" : 60
    }, {
      "referenceID" : 7,
      "context" : "Surprisingly, these scalings better state of the art optimistic algorithms specifically designed for efficient analysis which would admit Õ( √ H3S2AT ) regret (Jaksch et al., 2010).",
      "startOffset" : 159,
      "endOffset" : 180
    }, {
      "referenceID" : 5,
      "context" : "Tetris has been something of a benchmark problem for RL and approximate dynamic programming, with several papers on this topic (Gabillon et al., 2013).",
      "startOffset" : 127,
      "endOffset" : 150
    }, {
      "referenceID" : 16,
      "context" : "Interestingly, recent work has been able to reproduce similar performance using linear value functions (Liang et al., 2015).",
      "startOffset" : 103,
      "endOffset" : 123
    } ],
    "year" : 2016,
    "abstractText" : "We propose randomized least-squares value iteration (RLSVI) – a new reinforcement learning algorithm designed to explore and generalize efficiently via linearly parameterized value functions. We explain why versions of least-squares value iteration that use Boltzmann or -greedy exploration can be highly inefficient, and we present computational results that demonstrate dramatic efficiency gains enjoyed by RLSVI. Further, we establish an upper bound on the expected regret of RLSVI that demonstrates nearoptimality in a tabula rasa learning context. More broadly, our results suggest that randomized value functions offer a promising approach to tackling a critical challenge in reinforcement learning: synthesizing efficient exploration and effective generalization.",
    "creator" : "LaTeX with hyperref package"
  }
}