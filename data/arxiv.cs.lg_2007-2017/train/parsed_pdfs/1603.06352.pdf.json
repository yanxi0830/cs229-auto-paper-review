{
  "name" : "1603.06352.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Online Learning with Low Rank Experts ̊",
    "authors" : [ "Elad Hazan", "Tomer Koren", "Roi Livni", "Yishay Mansour" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n60 3.\n06 35\n2v 2\n[ cs\n.L G\n] 2\n3 M\nay 2\n01 6\n? dT q,\nand extend it to a setting of an approximate d subspace. For the adversarial model we show an upper bound of Opd ? T q and a lower bound of Ωp ? dT q."
    }, {
      "heading" : "1 Introduction",
      "text" : "Arguably the most well known problem in online learning theory is the so called prediction with experts advice problem. In its simplest form, a learner wishes to make an educated decision and at each round chooses to take the advice of one of N experts. The learner then suffers a loss between 0 and 1.\nIt is a standard result in online learning that, without further assumptions, the best strategy for the learner will incur Θp ? T logNq regret (Cesa-Bianchi and Lugosi, 2006). However, it is natural to assume that while experts are abundant, their decisions are based on common paradigms and that their decision making is based on few degrees of freedom – for example, if experts are indeed experts, their political bias, social background or school of thought largely dominates their decision making. Experts can also be assets on which the learner wishes to distribute her wealth. In this setting, weather, market condition and interests are dominant factors.\nIt is also sensible to assume that one can exploit this structure to achieve better regret bounds, potentially independent of the actual number of experts while still maintaining a strategy of picking an expert’s advice at each round. Our main result is of this flavor and we show how a learner can exploit hidden structure in the problem in an online setting.\nWe model the problem as follows: We assume that each expert corresponds to a vector ui in Rd space where d is potentially small. Then at each round the experts loss corresponds to a scalar product with a vector vt chosen arbitrarily, and possibly in an adversarial manner. The learner does not observe the chosen embedding of the experts in Euclidean space nor the vectors vt, and can only observe the loss of each expert.\nTo further motivate our setting, let us consider the low rank expert model in the stochastic case. It is well known that for linear predictors in d-dimensional space the regret will be Op ? dT q, independent of the number of experts. Indeed, we show that a simple follow the leader algorithm will achieve this regret bound. In fact, one novelty of this paper is a regret\n˚Accepted for presentation at Conference on Learning Theory (COLT) 2016. :Princeton University; email: ehazan@cs.princeton.edu. Parts of this work were done while at Microsoft Research, Herzliya. ;Technion—Israel Institute of Technology and Microsoft Research, Herzliya; email: tomerk@technion.ac.il. §The Hebrew University of Jerusalem and Microsoft Research, Herzliya; email: roi.livni@mail.huji.ac.il. ¶Microsoft Research, Herzliya and Tel Aviv University; email: mansour.yishay@gmail.com.\nbound that depends on an approximate rank – formally we show that one can improve on the Op ? T logNq regret bound and derive bounds that depend on the approximate rank rather than the number of experts. The non-stochastic setting is more challenging. It is true that for linear predictors in d-\ndimension one can achieve Op ? dT q regret bound even in the non-stochastic case. But the result assumes that learner has access to the geometric structure of the problem, namely, the embedding of the experts in the Euclidean space. Given the embedding one can apply a Follow the Regularized Leader approach with proper regularization to derive the desired regret bound.\nOur main result is a regret minimization algorithm that achieves an Opd ? T q regret in this low d-rank setting, when the learner does not have access to the experts’ embedding in Euclidean space. Our algorithm does not need to know the value of the rank d, and adaptively adapts to it. Thus we demonstrate a regret bound that is independent of number of experts. We accompany this upper bound with an Ωp ? dT q lower bound.\nOur results are part of a larger agenda in online learning. A working premise in Online Learning is that in most cases the stochastic case is the hardest case. Indeed, the literature is filled with generalization bounds and their analogue regret bounds. However, a striking difference is that the statistical bounds are often achieved using simple ERM algorithms, that are oblivious to any structure in the problem, even if the structure is required for the generalization bounds to be valid. In contrast, to achieve the analogue regret bound, one has to work harder. For finite hypothesis class the logN factor is achieved by a sophisticated algorithm, and for more general convex problems in Euclidean space a problem-specific regularization needs to be invoked in order to achieve optimality. Thus, a key difference is that online algorithms need to be tailored to the structure of the problem. This leads to the disappointing fact that to achieve optimal regret bounds, it is not enough for the problem to be structured but the learner needs to actively understand the structure.\nOur current research is an attempt to better understand this key difference: we wish to understand whether an online linear predictor can somehow exploit the geometry of the problem in an implicit manner, similarly to batch ERM algorithms, and how. For this, we invoke a setting where the learner must choose its predictor without the a-priori ability to devise a regularizer. Our findings so far indeed demonstrate that even without access to the structure the learner can indeed overcome her dependence on the irrelevant parameter N .\nTechnically, one should compare our regret bound of Opd ? T q to the standard regret bound\nof Op ? T logNq. For our bound to be superior one needs that d “ op ? logNq; while this can indeed be the case in various settings, our result can be better seen as a first step in a more general research direction. We aim to understand how online algorithms can take advantage of structural assumptions in the losses, without being given any explicit information about it."
    }, {
      "heading" : "1.1 Related Work",
      "text" : "Low rank assumptions are ubiquitous in the Machine Learning literature. They have been successfully applied to various problems, most notably to matrix completion (Candès and Recht, 2009; Foygel and Srebro, 2011; Srebro et al., 2004) but also in the context of classification with missing data (Goldberg et al., 2010; Hazan et al., 2015) and large scale optimization (Shalev-Shwartz et al., 2011).\nA similar problem that was studied in the literature is the Branching Experts Problem (Gofer et al., 2013). In the branching expert problem N potential experts are effectively only k distinct experts, but the clustering of the experts to the k clusters is unknown a-priori. This case can be considered as a special instance of our setting as indeed we can embed each expert as a kdimensional vector. Gofer et al. (2013) proved a sharp Θp ? kT q regret (the bound is tight only when k ă c logN for some constant c ą 0). It is perhaps worth noting that when effectively only k experts appear, the stochastic bound is Op ? T log kq, thus showing that in this similar problem, it is not true that the stochastic case is the hardest case.\nComplexity measures for online learning. We are not the first to try and understand what is the proper analogue for ERM in the online setting. Notions like the VC-dimension and Rademacher complexities have been extended to notions of Littlestone-Dimension (Littlestone, 1988; Shalev-Shwartz, 2011), and Sequential Rademacher Complexity (Rakhlin et al., 2010) respectively.\nThe SOA algorithm suggested by Ben-David et al. (2009) is a general framework for regret minimization that depends solely on the Littlestone dimension. However, the SOA algorithm is conceptually distinct from an ERM algorithm within our framework: to implement the SOA algorithm, one has to have access to the structure of the class (specifically, one needs to compute the Littlestone dimension of subclasses within the algorithm).\nSequential Rademacher complexity seems like a powerful tool for improving our bounds and answering some of our open problems. There are also advances in constructing effective algorithms within this framework (Rakhlin et al., 2012). However, as the branching expert example shows, there is no general argument that show that structure in the problem leads to stochastic–analogue bounds on the complexity.\nLearning from easy data. In another line of research, which is similar in spirit to ours, several authors attempt to go beyond worst-case analysis in online learning, and provide algorithms and bounds that can exploit deficiencies in the data. Work in this direction includes the study of worst-case robust online algorithms that can also adapt to stochastic i.i.d. data (e.g., Hazan and Kale, 2009; Rakhlin et al., 2013; De Rooij et al., 2014; Sani et al., 2014), as well as the exploration of various structural assumptions that can be leveraged for obtaining improved regret guarantees (e.g., Cesa-Bianchi et al., 2007; Hazan and Kale, 2010, 2011; Chiang et al., 2012; Rakhlin and Sridharan, 2013). However, to the best of our knowledge, low rank assumptions in online learning have not been explored in this context.\nAdaptive online algorithms. Online adaptive learning methods have recently been the topic of extensive study and are effective for large scale stochastic optimization in practice. One of the earliest and most widely used methods in this family is the AdaGrad algorithm (Duchi et al., 2011), a subgradient descent method that dynamically incorporate knowledge of the geometry of the data from earlier iterations. Our problem can be cast into an online linear optimization problem and subgradient descent methods are indeed applicable. It might seem at first sight that adapting the regularization via AdaGrad can lead to desired results. However, the analysis of the AdaGrad algorithm can only yield an Op ? dNT q bound on the regret in our\nlow-rank setting. In fact, a closer inspection reveals that the ? N factor in the latter bound is unavoidable for AdaGrad: as we show in Appendix B, in our setting the regret of AdaGrad is lower bounded by Ωpmint ? N,T uq."
    }, {
      "heading" : "2 Problem Setup and Main Results",
      "text" : "We recall the standard adversarial online experts model for T rounds with N experts. At each round t “ 1, . . . , T , the learner chooses a probability vector xt P ∆N , where ∆N denotes the N -simplex, namely the set of all possible distributions over N experts,\n∆N “ ! x P RN : @i, xpiq ě 0 and ÿN i“1 xpiq “ 1 ) .\nAn adversary replies by choosing a loss vector ℓt P r´1, 1sN ,1 and the learner suffers a loss xtpℓtq “ xt ¨ℓt. The objective of the learner is to minimize her regret, which is defined as follows,\nRegretT “ T ÿ\nt“1 xt ¨ ℓt ´ min iPrNs\nT ÿ t“1 ℓtpiq.\nIn the stochastic online experts model, the adversary selects a distribution D over the loss vectors in r´1, 1sN , and at time t a random ℓt P r´1, 1sN is selected from D. The regret is.\nRegretT “ T ÿ\nt“1 xt ¨ Erℓts ´ min iPrNs\nT ÿ t“1 Erℓtpiqs ,\nwhere the expectations are taken over the random loss vectors selected from D. In our setting, we wish to assume that there is a structure over the experts which implies that the loss vectors are structured, and are derived from a low rank subspace. Therefore we will add the following constraint over the adversary: let L P RNˆT be the loss matrix obtained in hindsight (i.e., the t’th column of L is ℓt). We restrict the feasible strategies for the adversary to only such that satisfy: rankpLq “ d . An equivalent formulation of our model is as follows: An adversary chooses at the beginning of the game a matrix U P RNˆd, where each row corresponds to an expert. At round t the adversary chooses a vector vt, and the learner gets to observe ℓt where ℓt “ Uvt. The objective of the learner remains the same: to choose at each round a probability distribution xt that minimizes the regret. We stress that the learner observes only the loss vectors ℓt, and does not have access to either U or the vectors vt."
    }, {
      "heading" : "2.1 Main Results",
      "text" : "We next state the main results of this paper:\nTheorem 1. The T -round regret of Algorithm 2 (described in Section 4 below) is at most Opd ? T q, where d “ rankpLq.\nWe remark that a regret upper bound of Op ? T mintd, ? logNuq is attainable by combining the standard multiplicative-updates algorithm with our algorithm.2 Our upper bound is accompanied by the following lower bound.\nTheorem 2. For any online learning algorithm, T and d ď log2N , there exists a sequence of loss vectors ℓ1, . . . ℓT P r´1, 1sN such that\nRegretT “ T ÿ\nt“1 xt ¨ ℓt ´ min iPrNs\nT ÿ t“1 ℓtpiq ě\nc\ndT\n8 ,\nand rankpLq “ d. 1As will become apparent later, in our setup it is more natural to consider symmetric r´1, 1s loss values rather than the typical r0, 1s losses. The two variants of the problem are equivalent up to a simple shift and scaling of the losses—a transformation that preserves the rank of the loss matrix.\n2A standard way to accomplish this is by running the two online algorithms in parallel, and choosing between their predictions by treating them as two meta-experts in another multiplicative-weights algorithm."
    }, {
      "heading" : "3 Preliminaries",
      "text" : ""
    }, {
      "heading" : "3.1 Notation",
      "text" : "Let In be the n ˆ n identity matrix. Let 1n be a vector of length n with all 1 entries. The columns of a matrix U are denoted by u1, u2, . . .. The i’th coordinate of a vector x is denoted by xpiq. For a matrix M , we denote by M : the Moore-Penrose pseudo-inverse of M . For a positive definite matrix H ą 0 we will denote its corresponding norm }x}H “ ? xTHx, and its\ndual norm }x}˚H “ ? xTH´1x. Given a positive semi-definite matrix M ľ 0 its corresponding Ellipsoid is defined as: EpMq “ tx : xTM :x ď 1u ."
    }, {
      "heading" : "3.2 Ellipsoidal Approximation of Convex Bodies",
      "text" : "A main tool in our algorithm is an Ellipsoid approximation of convex bodies. Recall John’s theorem for symmetric zero-centered convex bodies.\nTheorem 3 (John’s Theorem; e.g, Ball, 1997). Let K be a convex body in Rd that is symmetric around zero (i.e., K “ ´K). Let E be an ellipsoid with minimum volume enclosing K. Then:\n1? d E Ď K Ď E .\nWhile computing the minimum volume enclosed ellipsoid is computationally hard, for symmetric convex bodies it can be approximated to within 1` ǫ factor in polynomial time. Specifically, given as input a matrix A P RNˆd, consider the polytope PA “ tx : }Ax}8 ď 1u. We have the following.\nTheorem 4 (Grötschel et al., 2012, Theorem 4.6.5). There exists a poly-time procedure MVEEpAq that receives as input a matrix A P RNˆd and returns a matrix M such that\n1? 2d EpMq Ď PA Ď EpMq."
    }, {
      "heading" : "3.3 Online Mirror Descent",
      "text" : "Another main tool in our analysis is the well-known Online Mirror Descent algorithm for online convex optimization. The Online mirror descent is a subgradient descent method for optimization over a convex set in Rd that implies a regularization factor, chosen a-priori. In Algorithm 1 we describe the algorithm for the special case where the convex set is ∆N and the regularization function is chosen to be } ¨ }2H for some input matrix H ą 0:\nAlgorithm 1 OMD: Online Mirror Descent\n1: input: H ą 0, tηtuTt“1, x1 P ∆N . 2: for t “ 1 to T do 3: Play xt 4: Suffer cost xt ¨ ℓt and observe ℓt 5: Update\nxt`1 “ arg min xP∆N ℓt ¨ x` η´1t }x´ xt}2H .\n6: end for\nThe regret bound of the algorithm is dependent on the choice of regularization and is given as follows:\nLemma 5 (e.g., Hazan, 2015). The T -round regret of the OMD algorithm (Algorithm 1) is bounded as follows:\nT ÿ t“1 ℓt ¨ xt ´ T ÿ t“1 ℓt ¨ x˚ ď 1 ηT }x1 ´ x˚}2H ` 1 2 T ÿ t“1 ηtp}ℓt}˚Hq2 ."
    }, {
      "heading" : "3.4 Rademacher Complexity",
      "text" : "Our tool to analyze the stochastic case will be the Rademacher Complexity, specifically we will use it to bound the regret of a “Follow The Leader” algorithm (FTL). Recall that the FTL algorithm selection rule is defined as follows:\nxt “ arg min xP∆N\nt´1 ÿ i“1 ℓi ¨ x.\nOne way to bound the regret of the FTL algorithm in the stochastic case is by bounding the Rademacher complexity of the feasible samples. Recall that the Rademacher Complexity of a class of target function F over a sample St “ tℓ1, . . . , ℓtu is defined as follows\nRpF , Stq “ Eσ «\nsup fPF\n1\nt\nt ÿ i“1 σifpℓiq\nff\n,\nwhere σ P t´1, 1ut are i.i.d. Rademacher distributed random variables. The following bound is standard and well known, and for completeness we provide a proof in Appendix A.1.3\nLemma 6. Let K be a symmetric convex set centered around zero in Rd. Recall that the dual set K˚ is defined as follows:\nK˚ “ tx : sup yPK |y ¨ x| ď 1u.\nLet St “ tℓ1, . . . , ℓtu Ď K and let F Ď αK˚ be a subclass of linear functions, then:\nRpF , Stq ď α c d\nt .\nAnother standard bound applies to the case where F is bounded in the l1-norm.\nLemma 7 (Kakade et al., 2009). Let St “ tℓ̂1, . . . , ℓ̂tu P RN and let F1 be a subclass of linear functions such that supt}f}1 : f P Fu ď 1, then:\nRpF1, Stq ď max i\n}ℓ̂i}8 c 2 logN\nt .\nThe Rademacher complexity is a powerful tool in statistical learning theory and it allows us to bound the generalization error of an FTL algorithm. Namely, for every sample St “ tℓ1, . . . , ℓtu denote:\nfS “ argmin fPF\nt ÿ i“1 fpℓiq.\nThen we have the following bound for every f˚ P F (for i.i.d. loss vectors; see for example Shalev-Shwartz and Ben-David, 2014):\nE St„D E ℓ„D rfStpℓq ´ f˚pℓqs ď 2 E St„D rRpF , Stqs .\n3Surprisingly, we could not find any specific reference that precisely derives it.\nApplying this to FTL in the experts setting we have, in terms of regret, that for any x˚:\nE\n«\nT ÿ t“1 ℓt ¨ xt ´ ℓt ¨ x˚\nff\n“ T ÿ\nt“1 E ℓ1,...,ℓt´1„D E\nℓt„D rℓt ¨ xt ´ ℓt ¨ x˚s\nď 2 T ÿ\nt“1 E St´1„D rRp∆N , St´1qs . (1)"
    }, {
      "heading" : "4 Upper Bound",
      "text" : "In this section we discuss our online algorithm for the adversarial model, which is given in Algorithm 2. The algorithm is a version of Online Mirror Descent with adaptive regularization. It maintains a positive-definite matrix H, which is being updated whenever the newly observed loss vector ℓt is not in the span of previously appeared losses. In all other time steps—i.e., when ℓt remains in the previous span—the algorithm preforms an Online Mirror Descent type update (see Algorithm 1), with the function }x}2H “ xTHx as a regularizer.\nThe algorithm updates the regularization matrix H so as to adapt to the low-dimensional geometry of the set of feasible loss vectors. Indeed, as our analysis below reveals, H is an ellipsoidal approximation of a certain low-dimensional convex set in RN to which the loss vectors ℓt can be localized. This low-dimensional set is the intersection of the unit cube in N dimensions— in which the loss vectors ℓt reside by definition—and the low dimensional subspace spanned by previously observed loss vectors, given by spanpUq. Whenever the latter subspace changes, namely, once a newly observed loss vector leaves the span of previous vectors, the ellipsoidal approximation is recomputed and the matrix H is updated accordingly.\nAlgorithm 2 Online Low Rank Experts\n1: Initialize: x1 “ 1N 1N , τ “ 0, k “ 0, U “ tu 2: for t “ 1 to T do 3: Observe ℓt, suffer cost xt ¨ ℓt. 4: if ℓt R spanpUq then 5: Add ℓt as a new column of U , reset τ “ 0, and set k Ð k ` 1. 6: Compute M “ MVEEpUTq and H “ In ` UTMU . 7: end if 8: let τ Ð τ ` 1 and ηt “ 4 a k{τ , and set:\nxt`1 “ arg min xP∆N ℓt ¨ x` η´1t }x´ xt}2H .\n9: end for\nTo derive Theorem 1, we begin with analyzing a simpler case where the learner is aware of the subspace from which losses are derived. Specifically, assume that at the beginning of the rounds, the learner is equipped with a rank d matrix U such that for all losses ℓ1, ℓ2, . . . P spanpUq where we denote by spanpUq the span of the columns of the matrix U .\nIn this simplified setting, we can obtain a regret bound of Op ? dT q via John’s theorem (Theorem 3).4 As discussed above, the loss vectors ℓ1, . . . , ℓT can be localized to the intersection of the unit cube in N dimensions with the d-dimensional subspace spanned by the columns of U . Then, John’s theorem asserts that the minimal-volume enclosing ellipsoid of the intersection is a ? d-approximation to the set of feasible loss vectors.\n4We remark that for the simplified setting, the Op ? dT q regret bound is in fact tight, as our Ωp ? dT q lower bound (given in Section 5) applies in a setting where the subspace of the loss vectors is known a-priori to the learner.\nTheorem 8. Run Algorithm 1 with Input H, tηtu and x1 defined as follows: (i) H “ In ` UTMU , where M “ MVEEpUTq, (ii) ηt “ 4 a\nd{t, where d “ rankpUq, and (iii) x1 P ∆ is arbitrary. If ℓ1, . . . , ℓT P spanpUq , then the expected T -round regret of the algorithm is at most 8 ? dT .\nProof. Consider the d-dimensional polytope\nP “ tv P Rd : }UTv}8 ď 1u.\nThen by John’s Theorem (Theorem 3), we have,\nEp 1 2d Mq Ď P Ď EpMq . (2)\nIn order to apply Lemma 5, we need to bound both }ℓt}˚H and }x1 ´ x˚}2H . We first bound the norms }ℓt}˚H . Notice that for each loss vector ℓt there exists vt P P such that ℓt “ UTvt (as ℓt P spanpUq and }ℓt}8 ď 1). Thus, we can write,\np}ℓt}˚Hq2 “ ℓTt H´1ℓt “ vTt UpIn ` UTMUq´1UTvt ď vTt UpUTMUq:UTvt “ vTt M´1vt ,\nwhere we have used Lemma 11 (see Appendix A). Now, since vt P P and EpMq is enclosing P , we obtain vTt M ´1vt ď 1. This proves that\np}ℓt}˚Hq2 ď 1.\nNext we bound }x1 ´ x˚}H ď 2 Since }x1 ´ x˚}H ď 2maxxP∆n }x}H , it suffices to bound maxxP∆n }x}H . Hence, our goal is to show that }x}H ď 2 ? d for all x P ∆n. Since }x}2H “ 1` 2d }x}2H 1 with H 1 “ 12dUTMU , it is enough to bound the norm }x}2H 1 . Given a convex set in R d, recall that the dual set is given by\nP ˚ “ tx : sup pPP |x ¨ p| ď 1u.\nThe dual of an ellipsoid EpMq is given by pEpMqq˚ “ EpM´1q and it is standard to show that Eq. (2) implies in the dual:\npEpMqq˚ Ď P ˚ Ď pEp 1 2d Mqq˚.\nTaken together we obtain that P ˚ Ď Ep2dM´1q. Note that by definition the columns of U are in P ˚, hence, for every ui, }ui}2M ď 2d. Since x P ∆N ,\n}x}2H 1 “ 12d}Ux} 2 M ď 12d maxi }ui} 2 M ď 1 .\nEquipped with the bounds }x}H ď ? 1` 2d ď 2 ? d for all x P ∆n and }ℓt}˚H ď 1 for all t, we are now ready to analyze the regret of the algorithm, which via Lemma 5 can be bounded as follows:\nRegretT “ T ÿ\nt“1 ℓt ¨ xt ´\nT ÿ t“1 ℓt ¨ x˚\nď 1 ηT }x1 ´ x˚}2H ` 1 2\nT ÿ t“1 ηtp}ℓt}˚Hq2 7 Lemma 5\nď 4 ηT max xP∆n }x}2H ` 1 2\nT ÿ t“1 ηtp}ℓt}˚Hq2 7 }x1 ´ x˚}H ď 4 max xP∆n }x}H\nď 16d ηT ` 1 2\nT ÿ t“1 ηt 7 max xP∆n }x}2H ď 4d, }ℓt}˚H ď 1.\nA choice of ηt “ 4 a d{t, together with the inequality řTt“1 1{ ? t ď 2 ? T , gives the theorem.\nThe d-low rank setting does not assume that the learner has access to the subspace U , and potentially an adversary may adapt her choice of subspace to the learner’s strategy. However, the learner can still obtain regret bounds that are independent of the number of experts. We are now ready to prove Theorem 1.\nProof of Theorem 1. Let t0 “ 1, td`1 “ T and for all 1 ď k ď d let tk be the round where the k’th column is added to U . Also, let Tk “ tk`1 ´ tk the length of the k’th epoch. Notice that between rounds tk and tk`1 the algorithm’s execution is identical to Algorithm 1 with input depicted in Theorem 8. Therefore its regret in this time period is at most 8 ? kTk. The total regret is then bounded by\n8 d ÿ\nk“0\na\nkTk ď 8\ng f f e d ÿ\nk“0 k ¨\ng f f e d ÿ\nk“0 Tk ď 8d\n? T ,\nand the theorem follows."
    }, {
      "heading" : "4.1 Stochastic Online Experts",
      "text" : "We now turn to analyze the regret in the stochastic model, where the loss vectors ℓt are chosen i.i.d. from some unknown distribution. In this case we can achieve a right regret bound of Op ? dT q using a simple “Follow The Leader” (FTL) algorithm. We will in fact show an even stronger result for the stochastic case, that an approximate rank is enough to bound the complexity. Recall that the approximate rank, rankǫpLq, of a matrix is defined as follows (see Alon et al., 2013): rankǫpLq “ mintrankpL1q : }L1 ´ L}8 ă ǫu. The following statement is the main result for this section:\nTheorem 9. Assume that an adversary chooses her losses tℓtu i.i.d. from some distribution D supported on r´1, 1sN . Then the T -round regret of the FTL algorithm is bounded by:\nRegretT ď 8E ” a T ¨ rankǫpLq ı ` ǫ a T logN,\nfor every 0 ď ǫ ă 1. In particular, if rankpLq ď d almost surely, then RegretT “ Op ? dT q.\nProof. Our proof relies on Eq. (1) and a bound for Rp∆N , Stq. Fix a sequence ST “ tℓ1, . . . , ℓT u and let d “ rankǫpLq and let U be N ˆ d matrix such that\nL “ UV ` L̂ ,\nwhere maxi,j |L̂i,j| ă ǫ. We will denotes the columns of L̂ by ℓ̂1, . . . ℓ̂N . We define a symmetric convex set centered around zero in Rd as follows:\nK “ tv : supi |ui ¨ v| ď 2u .\nNote that for every vt we have that vt P K if ǫ ď 1. By definition of the set we have: ui P 2K˚ for every i. One can verify that K˚ is convex, hence if we let F “ convpu1, . . . , uN q we have that F Ď 2K˚. We can think of F as a linear function space, where fupvq “ u ¨ v. It follows by Lemma 6 that RpF , Stq ď a 2d{t. Finally,\nRp∆N , Stq “ E «\nsup xP∆N\nt ÿ\ni“1\n1 t σix ¨ ℓi\nff ď E «\nsup xP∆N\n1\nt\nt ÿ i“1 σix ¨ Uvi\nff ` E «\nsup xP∆N\n1\nt\nt ÿ i“1 σix ¨ ℓ̂\nff\n.\nNext, we have:\nE\n«\nsup xP∆N\n1\nt\nt ÿ i“1 σix ¨ Uvi\nff “ E «\nsup xP∆N\n1\nt\nt ÿ i“1 σipUTxq ¨ vi\nff\n“ sup fPconvpuiq E\n«\n1\nt\nt ÿ i“1 σifpviq\nff\n“ RpF , Stq ă 2 c d\nt . (3)\nand by Lemma 7,\nE sup xP∆N\n«\n1\nt\nt ÿ i“1 σix ¨ ℓ̂i\nff\nď ǫ c 2 logN\nt . (4)\nTaking Eq. (3) and Eq. (4) together, we have:\nRp∆N , Stq ď 2 c rankǫ L\nt ` ǫ\nc\n2 logN\nt .\nThe statement now follows from Eq. (1)."
    }, {
      "heading" : "5 Lower Bound",
      "text" : "We now prove Theorem 2. For our proof we will rely on lower bounds for online learning of hypotheses classes with respect to the Littlestone dimension (see Shalev-Shwartz, 2011). For a class H of target functions h : X Ñ t0, 1u, the Littlestone dimension LdimpHq measures the complexity, or online learnability, of the class.\nTo define LdimpHq one considers trees whose internal nodes are labeled by instances. Any branch in such a tree can be described as a sequence of examples px1, y1q, . . . , pxd, ydq where xi is the instance associated with the ith node in the path, and yi is 1 if xi`1 is the right child of the i–th node, and yi “ 0 otherwise. LdimpHq is then defined as the depth of the largest binary tree that is shattered by H. An instance-labeled tree is said to be shattered by a class H if for any root-to-leaf path px1, y1q, . . . , pxd, ydq there is some h P H such that hpxiq “ yi.\nTo prove Theorem 2, we need the following result about the Littlestone dimension.\nLemma 10 (Ben-David et al., 2009). Let H be any hypothesis class with finite LdimpHq, where Ldim is the Littlestone-dimension of a class H. For any (possibly randomized) algorithm, there exists a sequence of labeled instances pv1, y1q, . . . , pvT , yT q with yt P t0, 1u such that\nE\n«\nT ÿ t“1 |ŷt ´ yt|\nff\n´min hPH\nT ÿ i“1 |hpxtq ´ yt| ě\nc\nLdimpHqT 8 .\nwhere ŷt is the algorithm’s output at iteration t.\nProof of Theorem 2. We let H be the 2d vertices of the d-dimensional hypercube. We define a function class F over the domain X “ te1, . . . , edu of standard basis vectors. A function fu P F , is labeled by u P H, and defined over the set of basis vector ej , as follows,\nfupejq “ # 0 if upjq “ ´1, 1 if upjq “ 1.\nOne can verify that LdimpFq “ d. For each ui P H and y P t0, 1u, we can write\n|fuipejq ´ y| “ 1´ p2y ´ 1q ¨ ui ¨ ej\n2 .\nBy Lemma 10, we deduce that for any algorithm, there exists a sequence pv1, ȳ1q, . . . , pvT , ȳT q of standard basis vectors v1, . . . , vT and ȳ1, . . . ȳT P t´1, 1u such that:\nT ÿ\nt“1\nÿ\ni\nxtpiqui ¨ pȳtvtq ´min u\nT ÿ i“1 u ¨ pȳtvtq ě 2\nc\ndT\n8 . (5)\nWe now consider an adversary that chooses U as his expert matrix, and at round t the learner observes ℓt “ Upȳtvtq. The lower bound now follows from Eq. (5); the fact that rankpLq “ d follows from the fact that our experts are embedded in Rd."
    }, {
      "heading" : "6 Discussion and Open Problems",
      "text" : "We considered the problem of experts with a hidden low rank structure. Our findings are that in the non-stochastic case, similar to the stochastic case, the regret bounds are independent of the number of experts. The most natural question is then to bridge the gap between the upper and lower bounds: Open Problem 1. Is there an algorithm that can achieve regret Op ? dT q for any sequence\nℓ1, . . . , ℓT such that rankpLq “ d? Alternatively, can one prove a lower bound of Ωpd ? T q?\nAs discussed, our agenda is more general than the low-rank setting. Our aim is to construct new online algorithms that can exploit structure in the data, without explicit information on the structure. Other settings can also be considered within our framework.\nAnother interesting setting, that avoids dependence in dimension, is to assume that experts are embedded in a Hilbert space. By isomorphisms of Hilbert spaces this is equivalent to an adversary that chooses an expert embedding matrix U P RNˆN such that for every ui we have }ui}2 ď 1 and correspondingly at each time point we receive a vector vt such that }vt}2 ď 1 as a result we have a factorization:\nL “ UVT, }U}2,8, }V }2,8 ď 1,\nwhere }X}2,8 “ sup}y}ď1 }Xy}8. Recall the definition of the max-norm, also called the γ2-norm (Srebro and Shraibman, 2005):\n}L}max “ min UVT“L }U}2,8 ¨ }V }2,8.\nThus, similar to the low rank setting we can define this setting as follows: At each round a learner chooses xt P ∆N , an adversary replies by choosing a loss vector ℓt, and the learner incurs the corresponding loss. The adversary is restricted to strategies such that\n}L}max ď 1.\nThe importance of this setting is that the proper generalization bound for this case is dimension independent (e.g., Kakade et al., 2009). Hence, we ask the following question: Open Problem 2. Is there an algorithm that can achieve regret Op ? T q for any sequence ℓ1, . . . , ℓT such that }L}max ď 1? We can also generalize this setting to any pair of norms, } ¨ } and its dual } ¨ }˚, where the description of the game remains the same. The adversary chooses an embedding U of the experts with bounded } ¨ } norm. Then, at each round he chooses a set of vectors vt with } ¨ }˚ bounded norm.\nFinally, a different interesting direction to pursue in future work is to extend the noisy result to the adversarial setting. Namely, Open Problem 3. Is there an algorithm that can achieve regret Op ? dT ` ǫ ? T logNq for any sequence ℓ1, . . . , ℓT such that rankǫpLq ď d?"
    }, {
      "heading" : "A Technical Proofs",
      "text" : "Lemma 11. Let M P Rdˆd, U P Rdˆn such that M ą 0 and U . Then\nUpUTMUq:UT “ M´1.\nProof. Let N “ M1{2U . Then, we have NpNTNq:NT “ Id. To see this, write the SVD decomposition N “ OΣVT with diagonal non-singular Σ P Rdˆd and OOT “ OTO “ VTV “ Id. Then, NpNTNq:NT “ OΣVTpV Σ2VTq:V ΣOT “ OΣVTpV Σ´2VTqV ΣOT “ Id. Expanding the definition of N , we get M1{2UpUTMUq:UTM1{2 “ Id, and since M1{2 is nonsingular, we can multiply by M´1{2 on both sides and obtain the lemma.\nA.1 Proof of Lemma 6\nThe proof relies on the following corollary of John’s Theorem:\nLemma 12. Let K be a symmetric convex set centered around zero in Rd. There exists a positive semi-definite matrix Σ such that for every x P K:\nxTΣx ď sup fPK˚ |fpxq|2 ď dpxTΣxq .\nProof. (of Lemma 6). wlog we assume α “ 1, the general case follows since RpαF , Sq “ αRpF , Sq. We have\nRpF , Sq “ Eσ «\nsup fPF\n1\nt\nt ÿ i“1 σifpℓiq\nff “ Eσ «\nsup fPF fp1 t\nt ÿ i“1 σiℓiq\nff\nď\ng f f eEσ «\nsup fPF f2p1 t\nt ÿ i“1 σipℓiqq\nff\n.\nNext, we take Σ whose existence follows from Lemma 12. Note that Σ defines a scalar product. Specifically let us denote xℓi, ℓjy “ ℓTi Σℓj, and also we let }ℓi}22 “ ℓTi Σℓi. Then we have\ng f f eEσ «\nsup fPK˚ f2\n˜\n1\nt\nt ÿ i“1 σiℓi\n¸ff\nď\ng f f f edEσ » – 1\nt2\n› › › › › t ÿ\ni“1 σiℓi\n› › › › › 2\n2\nfi\nfl\n“\ng f f edEσ « 1\nt2\nt ÿ\ni,j“1 σiσjxℓi, ℓjy\nff\n“\ng f f edEσ « 1\nt2\nt ÿ i“1 σ2}ℓi}22\nff\n“\ng f f e d\nt2\nt ÿ i“1 }ℓi}22\nď c d\nt max i }ℓi}22 ď\nd\nd t max i sup fPK˚\nf2pℓiq ď c d\nt ,\nas claimed."
    }, {
      "heading" : "B Lower Bounds for the AdaGrad Algorithm",
      "text" : "AdaGrad (see Algorithm 3) is an algorithm that adapts the regularization matrix with respect to prior losses. Our aim in this section is to show that this learning scheme of the regularization cannot lead to a regret bound that is independent of the number of experts. Our strategy is as follows: since the AdaGrad algorithm depends on a learning rate parameter η we consider two cases: either η scales with N and becomes smaller, but then we show that for some sequence the algorithm’s update is “too slow”. On the other hand, we show that if η does not scale with N , the algorithm becomes less stable, and we can again inflict damage. Taken together we prove the following statement:\nTheorem 13. Consider Algorithm 3. For concreteness we assume that x1 “ 1N 1. For sufficiently large N , if T ă ? N{6 then there exist a sequence tℓ1, . . . , ℓT u such that RegretT ě T {2 and rankpLq “ 1.\nLemma 14. Consider Algorithm 3 with arbitrary η and δ. For concreteness we assume that x1 “ 1N 1. For sufficiently large N , if T ď max ` 1 36η2 `2 ? δ 6η , η2N´δ ˘ then there exist a sequence ℓ1, . . . , ℓT such that RegretT ě T {2 and rankpLq “ 1. Proof. We prove each bound separately.\nAlgorithm 3 AdaGrad\n1: Input: η, δ, x1 P ∆N . 2: Initialize: S0 “ G0 “ δI 3: for t “ 1 to T do 4: Observe ℓt, suffer cost xt ¨ ℓt. 5: set\nSt “ St´1 ` ℓtℓTt , Gt “ S 1{2 t yt`1 “ xt ´ ηG´1t ℓt xt`1 “ arg min\nxP∆N }yt`1 ´ x}2Gt\n6: end for\nCase 1: T ă 1 36η2\n` 2 ? δ\n6η . We let ℓt “ e “ p´1, 1N´1 , 1N´1 , 1N´1 , . . . 1N´1q for all t. For every t\nwe have that Gt “ a\nteeT ` δI and\nηG´1t ℓt “ η?\nt` δ}e} e.\nNext we use the inequality:\nT ÿ\nt“1\n1? t` δ ď ż T\n0\n1? t` δ\ndt “ 2 ´? T ` δ ´ ? δ ¯ .\nFor T ď ´ 1 6η ` ? δ ¯2 ´ δ “ 1 36η2 ` 2 ? δ 6η , we have that:\n´? T ` δ ´ ? δ ¯\nă 1 6η ;\n1 N ` η}e}\nT ÿ\ni“1\n1? t` δ ď 1 N ` 2η}e} ´? T ` δ ´ ? δ ¯ ď 1 2 ,\nwhere last inequality follows since }e} ą 1 and we assume N ě 6. One can observe that our update rule does not take yt out of the simplex ∆N and we have\nxt “ 1 N 1´ η}e} ÿ 1? t` δ e,\nand further, xtp1q ă 12 . In hindsight xtp1q suffers loss ´T while all other experts suffer positive loss. Hence the algorithm’s regret is at least\nRegretT ě T\n2 .\nCase 2: T ă η2N ´ δ. We now choose e “ p`1,`1,`1,`1,`1,`1 loooooooooooooomoooooooooooooon\nN{2 times\n,´1,´1,´1,´1,´1,´1 loooooooooooooomoooooooooooooon\nN{2 times\nq.\nand let ℓt “ p´1qt`1e.\nAs before note that\nηG´1t ℓt “ p´1qt`1η? t` δ}e} e “ p´1q t`1η a Npt` δq e.\nWe claim that for ? t` δ ă η ? N and t ą 1 we have that:\nxt “ 2\nN\n$\n’ ’ ’ &\n’ ’ ’ %\np1, 1, 1, 1, 1, 1 loooooomoooooon\nN{2 times\n, 0, 0, 0, 0, 0, 0 loooooomoooooon\nN{2 times\nq t is even,\np0, 0, 0, 0, 0, 0 loooooomoooooon\nN{2 times\n, 1, 1, 1, 1, 1, 1 loooooomoooooon\nN{2 times\nq t is odd. (6)\nHence xtℓt “ 1 and since the cumulative loss of each expert is at most 1 we have that:\nRegretT ě T\n2 .\nTo see that Eq. (6) holds, we will show the statement for x2 other cases are easier and follow the same proof: y1 “ 1 1N ` αe, where |α| ą 1?N , hence it has the form\nyt “ pa, a, a, a, a, a loooooomoooooon\nN{2 times\n,´b,´b,´b,´b,´b loooooooooomoooooooooon\nN{2 times\nq\nwhere a´ b “ 2{N and a, b ą 0. The statement now follows from Lemma 15 (see below).\nProof of Theorem 13. By Lemma 14 we need to show that\nmin η,δ max\n˜\n1\n9η2 ` 2\n? δ 3η , η2N ´ δ ¸ ą ? N 6 .\nTo prove this, we note that since both terms in the max are monotone in both variables, the minimum is attained when there is equality, i.e., the minimal η, δ satisfy:\n1\n36η2 ` 2\n? δ 6η “ η2N ´ δ.\nSince 1 36η2\n` 2 ? δ\n6η ` δ “\n´\n1 6η `\n? δ ¯2 , we get:\n? N “ 1\nη\nˆ\n1 6η ` ? δ ˙ ,\nand we have that: ? N\n6 “\n? 2\n36η2 `\n? δ 6η ă ? 2 36η2 ` 2 ? δ 6η .\nIt remains to prove Lemma 15, that was used for the proof of Lemma 14.\nLemma 15. Let y “ pa, a, a, a, a, a loooooomoooooon\nN{2 times\n,´b,´b,´b,´b,´b loooooooooomoooooooooon\nN{2 times\nq where a, b ě 0 and assume that a´ b “\n2{N . Let Gt “ ? δI ` αeeT´1{2 for some α ą 0, where\ne “ p`1,`1,`1,`1,`1,`1 loooooooooooooomoooooooooooooon\nN{2 times\n,´1,´1,´1,´1,´1,´1 loooooooooooooomoooooooooooooon\nN{2 times\nq .\nThen\nmin xP∆N\n1 2 }y ´ x}2Gt “ 2 N p1, 1, 1, 1, 1, 1 loooooomoooooon\nN{2 times\n, 0, 0, 0, 0, 0 loooomoooon\nN{2 times\nq .\nProof. Considering the Lagrangian and KKT conditions, we observe that x minimizes the distance iff the following hold:\n1. x P ∆N (primal feasibility)\n2. λ ą 0 and θp1q “ θp2q “ ¨ ¨ ¨ “ θpNq. (dual feasibility)\n3. x “ y `G´1t pλ` θq (stationarity)\n4. xpiq ‰ 0 ñ λpiq “ 0 and λpiq ‰ 0 ñ xpiq “ 0. (complementary slackness)\nNext note that e is an eigenvector of Gt and we have for some c ă 0 that\nG´1t ce “ p´b,´b,´b,´b,´b,´blooooooooooooomooooooooooooon N{2 times ,`b,`b,`b,`b,`b loooooooooomoooooooooon N{2 times q.\nNow we can write\nce “ p0, 0, . . . , 0, 0 loooooomoooooon\nN{2 times\n,´2c,´2c, . . . ,´2c,´2c loooooooooooooomoooooooooooooon\nN{2 times\nq\nloooooooooooooooooooooooomoooooooooooooooooooooooon\nλ\n`pc, c, c, c, c, c looooomooooon\nN{2 times\n, c, c, c, c, c, c looooomooooon\nN{2 times\nq\nlooooooooooooooomooooooooooooooon\nθ\n,\nthat concludes the proof."
    } ],
    "references" : [ {
      "title" : "The approximate rank of a matrix and its algorithmic applications: approximate rank",
      "author" : [ "N. Alon", "T. Lee", "A. Shraibman", "S. Vempala" ],
      "venue" : "In Proceedings of the forty-fifth annual ACM symposium on Theory of computing,",
      "citeRegEx" : "Alon et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Alon et al\\.",
      "year" : 2013
    }, {
      "title" : "An elementary introduction to modern convex geometry",
      "author" : [ "K. Ball" ],
      "venue" : "Flavors of geometry,",
      "citeRegEx" : "Ball.,? \\Q1997\\E",
      "shortCiteRegEx" : "Ball.",
      "year" : 1997
    }, {
      "title" : "Agnostic online learning",
      "author" : [ "S. Ben-David", "D. Pál", "S. Shalev-Shwartz" ],
      "venue" : "In COLT. Citeseer,",
      "citeRegEx" : "Ben.David et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Ben.David et al\\.",
      "year" : 2009
    }, {
      "title" : "Exact matrix completion via convex optimization",
      "author" : [ "E.J. Candès", "B. Recht" ],
      "venue" : "Foundations of Computational mathematics,",
      "citeRegEx" : "Candès and Recht.,? \\Q2009\\E",
      "shortCiteRegEx" : "Candès and Recht.",
      "year" : 2009
    }, {
      "title" : "Prediction, Learning, and Games",
      "author" : [ "N. Cesa-Bianchi", "G. Lugosi" ],
      "venue" : null,
      "citeRegEx" : "Cesa.Bianchi and Lugosi.,? \\Q2006\\E",
      "shortCiteRegEx" : "Cesa.Bianchi and Lugosi.",
      "year" : 2006
    }, {
      "title" : "Improved second-order bounds for prediction with expert advice",
      "author" : [ "N. Cesa-Bianchi", "Y. Mansour", "G. Stoltz" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Cesa.Bianchi et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Cesa.Bianchi et al\\.",
      "year" : 2007
    }, {
      "title" : "Online optimization with gradual variations",
      "author" : [ "C.-K. Chiang", "T. Yang", "C.-J. Lee", "M. Mahdavi", "C.-J. Lu", "R. Jin", "S. Zhu" ],
      "venue" : "In COLT,",
      "citeRegEx" : "Chiang et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Chiang et al\\.",
      "year" : 2012
    }, {
      "title" : "Follow the leader if you can, hedge if you must",
      "author" : [ "S. De Rooij", "T. Van Erven", "P.D. Grünwald", "W.M. Koolen" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "Rooij et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Rooij et al\\.",
      "year" : 2014
    }, {
      "title" : "Adaptive subgradient methods for online learning and stochastic optimization",
      "author" : [ "J. Duchi", "E. Hazan", "Y. Singer" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "Duchi et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Duchi et al\\.",
      "year" : 2011
    }, {
      "title" : "Concentration-based guarantees for low-rank matrix reconstruction",
      "author" : [ "R. Foygel", "N. Srebro" ],
      "venue" : "arXiv preprint arXiv:1102.3923,",
      "citeRegEx" : "Foygel and Srebro.,? \\Q2011\\E",
      "shortCiteRegEx" : "Foygel and Srebro.",
      "year" : 2011
    }, {
      "title" : "Regret minimization for branching experts",
      "author" : [ "E. Gofer", "N. Cesa-Bianchi", "C. Gentile", "Y. Mansour" ],
      "venue" : "In Conference on Learning Theory,",
      "citeRegEx" : "Gofer et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Gofer et al\\.",
      "year" : 2013
    }, {
      "title" : "Transduction with matrix completion: Three birds with one stone",
      "author" : [ "A. Goldberg", "B. Recht", "J. Xu", "R. Nowak", "X. Zhu" ],
      "venue" : "In Advances in neural information processing systems,",
      "citeRegEx" : "Goldberg et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Goldberg et al\\.",
      "year" : 2010
    }, {
      "title" : "Geometric algorithms and combinatorial optimization, volume 2",
      "author" : [ "M. Grötschel", "L. Lovász", "A. Schrijver" ],
      "venue" : "Springer Science & Business Media,",
      "citeRegEx" : "Grötschel et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Grötschel et al\\.",
      "year" : 2012
    }, {
      "title" : "Introduction to Onlne Convex Optimization, Draft",
      "author" : [ "E. Hazan" ],
      "venue" : "Publishers Inc.,",
      "citeRegEx" : "Hazan.,? \\Q2015\\E",
      "shortCiteRegEx" : "Hazan.",
      "year" : 2015
    }, {
      "title" : "On stochastic and worst-case models for investing",
      "author" : [ "E. Hazan", "S. Kale" ],
      "venue" : "Advances in Neural Information Processing Systems",
      "citeRegEx" : "Hazan and Kale.,? \\Q2009\\E",
      "shortCiteRegEx" : "Hazan and Kale.",
      "year" : 2009
    }, {
      "title" : "Extracting certainty from uncertainty: Regret bounded by variation in costs",
      "author" : [ "E. Hazan", "S. Kale" ],
      "venue" : "Machine learning,",
      "citeRegEx" : "Hazan and Kale.,? \\Q2010\\E",
      "shortCiteRegEx" : "Hazan and Kale.",
      "year" : 2010
    }, {
      "title" : "Better algorithms for benign bandits",
      "author" : [ "E. Hazan", "S. Kale" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "Hazan and Kale.,? \\Q2011\\E",
      "shortCiteRegEx" : "Hazan and Kale.",
      "year" : 2011
    }, {
      "title" : "Classification with low rank and missing data",
      "author" : [ "E. Hazan", "R. Livni", "Y. Mansour" ],
      "venue" : "In Proceedings of The 32nd International Conference on Machine Learning,",
      "citeRegEx" : "Hazan et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Hazan et al\\.",
      "year" : 2015
    }, {
      "title" : "Maximum-margin matrix factorization",
      "author" : [ "N. Srebro", "J. Rennie", "T.S. Jaakkola" ],
      "venue" : null,
      "citeRegEx" : "Srebro et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Srebro et al\\.",
      "year" : 2005
    } ],
    "referenceMentions" : [ {
      "referenceID" : 4,
      "context" : "It is a standard result in online learning that, without further assumptions, the best strategy for the learner will incur Θp ? T logNq regret (Cesa-Bianchi and Lugosi, 2006).",
      "startOffset" : 143,
      "endOffset" : 174
    }, {
      "referenceID" : 3,
      "context" : "They have been successfully applied to various problems, most notably to matrix completion (Candès and Recht, 2009; Foygel and Srebro, 2011; Srebro et al., 2004) but also in the context of classification with missing data (Goldberg et al.",
      "startOffset" : 91,
      "endOffset" : 161
    }, {
      "referenceID" : 9,
      "context" : "They have been successfully applied to various problems, most notably to matrix completion (Candès and Recht, 2009; Foygel and Srebro, 2011; Srebro et al., 2004) but also in the context of classification with missing data (Goldberg et al.",
      "startOffset" : 91,
      "endOffset" : 161
    }, {
      "referenceID" : 11,
      "context" : ", 2004) but also in the context of classification with missing data (Goldberg et al., 2010; Hazan et al., 2015) and large scale optimization (Shalev-Shwartz et al.",
      "startOffset" : 68,
      "endOffset" : 111
    }, {
      "referenceID" : 17,
      "context" : ", 2004) but also in the context of classification with missing data (Goldberg et al., 2010; Hazan et al., 2015) and large scale optimization (Shalev-Shwartz et al.",
      "startOffset" : 68,
      "endOffset" : 111
    }, {
      "referenceID" : 10,
      "context" : "A similar problem that was studied in the literature is the Branching Experts Problem (Gofer et al., 2013).",
      "startOffset" : 86,
      "endOffset" : 106
    }, {
      "referenceID" : 3,
      "context" : "They have been successfully applied to various problems, most notably to matrix completion (Candès and Recht, 2009; Foygel and Srebro, 2011; Srebro et al., 2004) but also in the context of classification with missing data (Goldberg et al., 2010; Hazan et al., 2015) and large scale optimization (Shalev-Shwartz et al., 2011). A similar problem that was studied in the literature is the Branching Experts Problem (Gofer et al., 2013). In the branching expert problem N potential experts are effectively only k distinct experts, but the clustering of the experts to the k clusters is unknown a-priori. This case can be considered as a special instance of our setting as indeed we can embed each expert as a kdimensional vector. Gofer et al. (2013) proved a sharp Θp ? kT q regret (the bound is tight only when k ă c logN for some constant c ą 0).",
      "startOffset" : 92,
      "endOffset" : 746
    }, {
      "referenceID" : 6,
      "context" : ", 2014), as well as the exploration of various structural assumptions that can be leveraged for obtaining improved regret guarantees (e.g., Cesa-Bianchi et al., 2007; Hazan and Kale, 2010, 2011; Chiang et al., 2012; Rakhlin and Sridharan, 2013).",
      "startOffset" : 133,
      "endOffset" : 244
    }, {
      "referenceID" : 8,
      "context" : "One of the earliest and most widely used methods in this family is the AdaGrad algorithm (Duchi et al., 2011), a subgradient descent method that dynamically incorporate knowledge of the geometry of the data from earlier iterations.",
      "startOffset" : 89,
      "endOffset" : 109
    }, {
      "referenceID" : 2,
      "context" : "The SOA algorithm suggested by Ben-David et al. (2009) is a general framework for regret minimization that depends solely on the Littlestone dimension.",
      "startOffset" : 31,
      "endOffset" : 55
    }, {
      "referenceID" : 2,
      "context" : "Lemma 10 (Ben-David et al., 2009).",
      "startOffset" : 9,
      "endOffset" : 33
    } ],
    "year" : 2016,
    "abstractText" : "We consider the problem of prediction with expert advice when the losses of the experts have low-dimensional structure: they are restricted to an unknown d-dimensional subspace. We devise algorithms with regret bounds that are independent of the number of experts and depend only on the rank d. For the stochastic model we show a tight bound of Θp ? dT q, and extend it to a setting of an approximate d subspace. For the adversarial model we show an upper bound of Opd ? T q and a lower bound of Ωp ? dT q.",
    "creator" : "LaTeX with hyperref package"
  }
}