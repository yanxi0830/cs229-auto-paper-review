{
  "name" : "1302.7263.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Online Similarity Prediction of Networked Data from Known and Unknown Graphs",
    "authors" : [ "Claudio Gentile" ],
    "emails" : [ "claudio.gentile@uninsubria.it", "m.herbster@cs.ucl.ac.uk", "s.pasteris@cs.ucl.ac.uk" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "The study of networked data has spurred a large amount of research efforts. Applications like spam detection, product recommendation, link analysis, community detection, are by now well-known tasks in Social Network analysis and E-Commerce. In all these tasks, networked data are typically viewed as graphs, where vertices carry some kind of relevant information (e.g., user features in a social network), and connecting edges reflect a form of semantic similarity between the data associated with the incident vertices. Such a similarity ranges from friendship among people in a social network to common user’s reactions to online ads in a recommender system, from functional relationships among proteins in a protein-protein interaction network to connectivity patterns in\nar X\niv :1\n30 2.\n72 63\nv3 [\na communication network. Coarsely speaking, similarity prediction aims at inferring the existence of new pairwise relationships based on known ones. These pairwise constraints, which specify whether two objects belong to the same class or not, may arise directly from domain knowledge or be available with little human effort.\nThere is a wide range of possible means of capturing the structure of a graph in this learning context: through combinatorial and classical graph-theoretical methods (e.g., [19]); through spectral approachs (e.g. [3, 22]), using convex duality and resistive geometry (e.g., [24]), and even algebraic methods (e.g., [34]). In many of these approaches, the underlying assumption is that the graph structure is largely known in advance (a kind of “transductive” learning setting), and serves as a way to bias the inference process, so as to implement the principle that “connected vertices tend to be similar.” Yet, this setting is oftentimes unrealistic and/or infeasible. For instance, a large online social network with millions of vertices and tens of millions of edges hardly lends itself to be processed as a whole via a Laplacian-regularized optimization approach or, even if it does (thanks to the computationally powerful tools currently available), it need not be known ahead of time. As a striking example, if we are representing a security agency, and at each point in time we receive a “trace” of communicating individuals, we still might want to predict whether a given pair in the trace belong to the same “gang”/community, even if the actual network of relationships is unknown to us. So, in this case, we are incrementally learning similarity patterns among individuals while, at the same time, exploring the network. Another important scenario of an unknown network structure is when the network itself grows with time, hence the prediction algorithms are expected to somehow adapt to its temporal evolution. Our results. We study online similarity prediction over graphs in two models. One in which the graph is known1 a priori to the learner, and one in which it is unknown. In both settings there is an undisclosed labeling of a graph so that each vertex is the member of one of K classes. Two vertices are similar if they are in the same class and dissimilar otherwise. The learner receives an online sequence of vertex pairs and similarity feedback. On the receipt of a pair the learner then predicts if the pair is similar. The true pair label, similar or dissimilar, is then received and the goal of the learner is to minimize mistaken predictions. Our aim in both settings is then to bound the number of prediction mistakes over an arbitrary (and adversarially generated) sequence of pairs.\nIn the model where the graph is known, we first show via reductions to online vertex classification methods on graphs (e.g., [23, 24, 25, 27, 29, 28, 11, 12, 13], and references therein), that a suitable adaptation of the Matrix Winnow algorithm [47] readily provides an almost optimal mistake bound. This adaptation amounts to sparsifying the underlying graph G via a random spanning tree, whose diameter is then shortened by a known rebalancing technique [29, 12]. Unfortunately, due to its computational burden (cubic time per round), the resulting algorithm does not provide a satisfactory answer to actual deployment on large networks. Therefore, we develop an analogous adaptation of a Matrix Perceptron algorithm that delivers a much more attractive answer (thanks to its poly-logarithmic time per round), though with an inferior online prediction performance guarantee.\nThe unknown model is identical to the known one, except that the learner does not initially receive the underlying graph G. Rather, G is incrementally revealed, as now when the learner receives a pair it also receives as side information an adversarially generated path within G con-\n1The reader should keep in mind that while the data at hand may not be natively graphical, it might still be convenient in practice to artificially generate a graph for similarity prediction, since the graph may encode side information that is otherwise unexploitable.\nnecting the vertices of the pair. Here, we observe that the machinery we used for the known graph case is inapplicable. Instead, we design and analyze an algorithm which may be interpreted as a matrix version of an adaptive p-norm Perceptron [20, 17] with the relatively efficient quadratic running time per round. Related work. This paper lies at the intersection between online learning on graphs and matrix/metric learning. Both fields include a substantial amount of work, so we can hardly do it justice here. Below we outline some of the main contributions in matrix/metric learning, with a special emphasis on those we believe are most related to this paper. Relevant papers in online class prediction on graphs will be recalled in Section 3.\nSimilarity prediction on graphs can be seen as a special case of matrix learning. Relevant works on this subject include [46, 47, 10, 31] – see also [21] for recent usage in the context of online cut prediction. In all these papers, special care is put into designing appropriate regularization terms driving the online optimization problem, the focus typically being on spectral sparseness. When operating on graph structures with Laplacian-based regularization, these algorithms achieve mistake bounds depending on functions of the cut-size of the labeled graph – see Section 4. Yet, in the absence of further efforts, their scaling properties make them inappropriate to practical usage in large networks. Metric learning is also relevant to this paper. Metric learning is a special case of matrix learning where the matrix is positive semi-definite. Relevant references include [45, 15, 39, 49, 9]. Some of these papers also contain generalization bound arguments. Yet, no specific concerns are cast on networked data frameworks. Related to our bidirectional reduction from class prediction to similarity prediction is the thread of papers on kernels on pairs (e.g., [2, 39, 35, 6]), where kernels over pairs of objects are constructed as a way to measure the “distance” between the two referenced pairs. The idea is then to combine with any standard kernel algorithm. The so-called matrix completion task (specifically, the recent reference [32]) is also related to our work. In that paper, the authors introduce a matrix recovery method working in noisy environments, which incorporates both a low-rank and a Laplacian-regularization term. The problem of recovery of low-rank matrices has extensively been studied in the recent statistical literature (e.g., [7, 8, 18, 44, 40, 33], and references therein), the main concern being bounding the recovery error rate, but disregarding the computational aspects of the selected estimators. Moreover, the way they typically measure error rate is not easily comparable to online mistake bounds. Finally, the literature on semisupervised clustering/clustering with side information ([4, 16] – see also [43] for a recent reference on spectral approaches to clustering) is related to this paper, since the similarity feedback can be interpreted as a must-link/cannot-link feedback. Nonetheless, their formal statements are fairly different from ours.\nTo summarize, whereas we are motivationally close to [32], from a technical viewpoint, we are perhaps closer to [45, 46, 47, 10, 21, 31], as well as to the literature on online learning on graphs.\nBefore delving into the graph-based similarity problem, we start off by investigating the problem of similarity prediction in abstract terms, showing that similarity prediction reduces to classification, and vice versa. This will pave the way for all later results."
    }, {
      "heading" : "2 Online class and similarity prediction",
      "text" : "In this section we examine the correspondence in predictive performance (mistake bounds) between the classification and similarity prediction frameworks. Preliminaries. The set of all finite sequences from a set X is denoted X *. We use the Iverson bracket notation [predicate] = 1 if the predicate is true and [predicate] = 0 if false. In K-class\nprediction in the online mistake bound model, an example sequence (x1, y1), . . . , (xT , yT ) ∈ (X × Y)* is revealed incrementally, where X is a set of patterns and Y := {1, . . . ,K} is the set of K class labels. The goal on the t-th trial is to predict the class yt given the previous t−1 pattern/label pairs and xt. The overall aim of an algorithm is to minimize the number of its mistaken predictions. In similarity prediction, examples are pairs of patterns with “similarity” labels i.e., ((x′, x′′), y) ∈ X 2 × Ys with Ys = {0, 1}. We interpret y ∈ Ys as similar if y = 0 and dissimilar if y = 1; we also introduce the convenient function sim(y′, y′′) := 1 − [y′ = y′′] which maps a pair of class labels y′, y′′ ∈ Y to a similarity label. A concept is a function f : X → Y that maps patterns to labels. An example sequence S is consistent with a concept f for classification if (x, y) ∈ S implies y = f(x) and for similarity if ((x′, x′′), y) ∈ S implies y = sim(f(x′), f(x′′)). We use MA(S) to denote the number of prediction mistakes of the online algorithm A on example sequence S. Given an algorithm A, we define the mistake bound with respect to a concept f as BA(f) := maxSMA(S), the maximum being over all sequences S consistent with f .\nTheorem 1. Given an online classification algorithm Ac one may construct a similarity algorithm As such that if S is any similarity sequence consistent with any concept f then\nMAs(S) ≤ 5BAc(f) log2K , (1)\nand given an online similarity algorithm As one may construct a classification algorithm Ac such that if S is any classification sequence consistent with any concept f then\nMAc(S) ≤ BAs(f) +K . (2)\nThe direct implementation of the similarity algorithm As from the classification algorithm Ac is infeasible, as its running time is exponential in the mistake bound. In Appendix A.1 we prove a more general result (see Lemma 7) than in equation (1) which applies also to noisy sequences and to “order-dependent” bounds, as in the shifting-expert bounds in [26]. We also argue (Appendix A.1.1) that the “logK” term in (1) is necessary. Observe that equation (2) implies a lower bound for similarity prediction if we have a lower bound for the corresponding class prediction problem with only a weakening by an additive “−K” term."
    }, {
      "heading" : "3 Class and similarity prediction on graphs",
      "text" : "We now introduce notation specific to the graph setting. Let then G = (V,E) be an undirected and connected graph with n = |V | vertices, V = {1, . . . , n}, and m = |E| edges. The assignment of K class labels to the vertices of a graph is denoted by a vector y = (y1, . . . , yn), where yi ∈ {1, . . . ,K} denotes the label of the i-th vertex among the K possible labels. The vertex-labeled graph will often be denoted by the pairing (G,y). Associated with each pair (i, j) ∈ V 2 of (not necessarily adjacent) vertices is a similarity label yi,j ∈ {0, 1}, where yi,j = 1 if and only if yi 6= yj . As is typical of graph-based prediction problems (e.g., [23, 24, 25, 27, 29, 28, 11, 12, 13], and references therein), the graph structure plays the role of an inductive bias, where adjacent vertices tend to belong to the same class. The set of cut-edges in (G,y) is denoted as ΦG(y) := {(i, j) ∈ E : yi,j = 1} (when nonambiguous, we abbreviate it to ΦG), and the associated cut-size as |ΦG(y)|. The set of cut-edges with respect to class label k is denoted as ΦGk (y) := {(i, j) ∈ E : k ∈ {yi, yj}, yi,j = 1} (when nonambiguous, we abbreviate it to ΦGk ). Notice that ∑K k=1 |ΦGk (y)| = 2|ΦG(y)|. We let Ψ be the m×n (oriented and transposed) incidence matrix of G. Specifically, if we let the edges in E be enumerated as (i1, j1), . . . , (im, jm), and fix arbitrarily an orientation for them (e.g., from the left\nendpoint to the right endpoint), then Ψ is the matrix that maps any vector v = (v1, . . . , vn) > ∈ Rn to the vector Ψv ∈ Rm, where [Ψv]` = vi` − vj` , ` = 1, . . . ,m. Moreover, since G is connected, the null space of Ψ is spanned by the constant vector 1 = (1, . . . , 1)>, that is, Ψv = 0 implies that v = c1, for some constant c. We denote by Ψ+ the (n×m-dimensional) pseudoinverse of Ψ. The graph Laplacian matrix may be defined as L := Ψ>Ψ, thus notice that L+ = Ψ+(Ψ+)>. If G is identified with a resistive network such that each edge is a unit resistor, then the effective resistance RGi,j between a pair of vertices (i, j) ∈ V 2 can be defined as RGi,j = (ei − ej)>L+(ei − ej), where ei is the i-th vector in the canonical basis of Rn. When (i, j) ∈ E then RGi,j also equals the probability that a spanning tree of G drawn uniformly at random (from the set of all spanning trees of G) includes (i, j) as one of its n− 1 edges (e.g., [38]). The resistance diameter of G is max(i,j)∈V 2 RGi,j . It is known that the effective resistance defines a metric over the vertices of G. Moreover, when G is actually a tree, then RGi,j corresponds to the number of edges in the (unique) path from i to j. Hence, in this case, the resistance diameter of G coincides with its (geodesic) diameter."
    }, {
      "heading" : "3.1 Class prediction on graphs",
      "text" : "Roughly speaking, algorithms and bounds for sequential class prediction on graphs split between two types: Those which approximate the original graph with a tree or those that maintain the original graph. By approximating the graph with a tree, extremely efficient algorithms are obtained with strong optimality guarantees. By exploiting the full graph, algorithms are obtained which take advantage of the connectivity to achieve sharp bounds when the graph contains, e.g., dense clusters. Relevant literature on this subject includes [27, 25, 23, 28, 29, 11, 24, 12]. Known representatives of the first kind are upper bounds of the form O(|ΦT |(1 + log n|ΦT |)) [29] or of the form O(|ΦT | logDT ) [11], where T is some spanning tree of G, and DT is the (geodesic) diameter of T . In particular, if T is drawn uniformly at random, the above turn to bounds on the expected number of mistakes of the form O(E[|ΦT |] log n), where E[|ΦT |] is the resistance-weighted cut-size of G, E[|ΦT |] = ∑ (i,j)∈ΦG(y)R G i,j , which can be far smaller than |ΦG| when G is well connected. Representatives of the second kind are bounds of the form O(ρ+ |ΦG|Rρ) [23, 24], where ρ is the number of balls in a cover of the vertices of G such that Rρ is the maximum over the resistance diameters of the balls in the cover. Since resistance diameter lower bounds geodesic diameter, this alternative approach leverages a different connectivity structure of the graph than the resistanceweighted cut-size.\nIn all of the above mentioned works, the bounds and algorithms are for the K = 2 class prediction case. In Appendix A.2 we argue for a simple reduction that will raise a variety of cut-based algorithms and bounds from the two-class to the K-class case. Specifically, a two-class mistake bound of the form M ≤ c|ΦG(y)| ∀y ∈ {0, 1}n, for some c ≥ 0 easily turns into a Kclass mistake bound of the form M ≤ 2c|ΦG(y)| ∀y ∈ {1, . . . ,K}n, where K need not be known in advance to the algorithm. Therefore, bounds of the form O(E[|ΦT |] log n) also hold in the multiclass setting.\nOn the lower bound side, [11] contains an argument showing (for the K = 2 class case) that for any φ ≥ 0 a labeling y exists such that any algorithm will make at least φ/2 mistakes while E[|ΦT |] < φ. In short, Ω(E[|ΦT |]) is also a lower bound on the number of mistakes in the class prediction problem on graphs. When combined with Theorem 1 in Section 2, the above results immediately yield upper and lower bounds for the similarity prediction problem over graphs.\nProposition 1. Let (G,y) be a labeled graph, and T be a random spanning tree of G. Then an algorithm exists for the similarity prediction problem on G whose expected number of mistakes\nE[M ] satisfies E[M ] = O(E[|ΦT (y)|] logK log n) . Moreover, for any φ ≥ 0 a K-class labeling y exists such that any similarity prediction algorithm on G will make at least φ/2−K mistakes while E[|ΦT |] < φ.\nThe upper bound above refers to a computationally inefficient algorithm and, clearly enough, more direct version space arguments would lead to similar results. Section 4 contains a more efficient approach to similarity prediction over graphs.\nTo close this section, we observe that the upper bounds on class predictions of the form O(E[|ΦT |] log n) taken from [29, 12] are essentially relying on linearizing the graph G into a path graph, and then predicting optimally on it via an efficient Bayes classifier (aka Halving Algorithm, e.g., [36]). One might wonder whether a similar approach would directly apply to the similarity prediction problem. We now show that exact computation of the probabilities of the Bayes classifier for a path graph is #P-complete under similarity feedback.\nThe Ising distribution over graph labelings (y ∈ {1, 2}n) is defined as p(y) ∝ 2−β|φG(y)|. Given a set of vertices and associated labels, the marginal distribution at each vertex can be computed in linear time when the graph is a path ([42]). In [29] this simple fact was exploited to give an efficient class prediction algorithm by a particular linearization of a graph to a path graph. The equivalent problem in similarity prediction requires us to compute marginals given a set of pairwise constraints. The following theorem shows that computing the partition function (and hence the relevant marginals) of the Ising distribution on a path with pairwise label constraints is #P-complete.\nTheorem 2. Computing the partition function of the (ferromagnetic) Ising model on a path graph with pairwise constraints is #P-complete, where an Instance is an n-vertex path graph P , a set of pairs C ⊂ {1, . . . , n}2, and a natural number, β, presented in unary, and the desired Output is the value of the partition function, ZP (C, β) := ∑ y∈{1,2}n:{(yi=yj)}(i,j)∈C 2 −β|φP (y)| .\nThus computing the exact marginal probabilities on even a path graph will be infeasible (given the hardness of #P). As an alternative, in the following section we discuss the application of the Matrix Perceptron and Matrix Winnow algorithms to similarity prediction."
    }, {
      "heading" : "3.2 Similarity prediction on graphs",
      "text" : "In Algorithm 1 we give a simple application of the Matrix Winnow (superscript “w”) and Perceptron (superscript “p”) algorithms to similarity prediction on graphs. The key aspect of the construction (common to many methods in metric learning) is the creation of rank one matrices which correspond to similarity “instances” (see (3)). We then use the standard analysis of the Perceptron [41] and Matrix Winnow [47] algorithms with appropriate thresholds to obtain Proposition 2. A key observation is that the squared Frobenius norm of the (un-normalized) instance matrices is bounded by the squared resistance diameter of the graph, and the squared Frobenius norm of the (unnormalized) “comparator” matrix is bounded by the cut-size squared |ΦG|2. Proposition 2. Let (G,y) be a labeled graph and let Ψ be the (transposed) incidence matrix associated with the Laplacian of G. Then, if we run the Matrix Winnow and Perceptron algorithms with similarity instances constructed from Ψ, we have the following mistake bounds:\nMw = O ( |ΦG| max\n(i,j)∈V 2 RGi,j log n\n) and Mp = O ( |ΦG|2 max\n(i,j)∈V 2 (RGi,j)\n2 ) .\nAlgorithm 1: Perceptron and Matrix Winnow algorithms on a graph\nInput: Graph G = (V,E), |V | = n, with Laplacian L = Ψ>Ψ, and R := max(i,j)∈V 2 RGi,j ; Parameters: Perceptron threshold θ̂p = R2; Winnow threshold θ̂w = η\neη−e−η 1 R |ΦG| ,\nWinnow learning rate η = 1.28; Initialization: W p0 = 0 ∈ Rm×m; Ww0 = 1m I ∈ R\nm×m; For t = 1, 2, . . . , T :\n• Get pair of vertices (it, jt) ∈ V 2, and construct similarity instances,\nXpt = (Ψ +)>(eit − ejt)(eit − ejt)>Ψ+; Xwt =\n(Ψ+)>(eit − ejt)(eit − ejt)>Ψ+\n(eit − ejt)>L+(eit − ejt) ; (3)\n• Predict: ŷpt = [tr((W p t−1) >Xpt ) > θ̂ p]; ŷwt = [tr((W w t−1) >Xwt ) > θ̂ w]; • Observe yt ∈ {0, 1} and, if mistake (yt 6= ŷt), update\nW pt ←W p t−1 + (yt − ŷ p t )X p t ; logW w t ← logWwt−1 + η (yt − ŷwt )Xwt .\nA severe drawback of both these algorithms is that on a generic graph, initilization requires computing a pseudo-inverse (typically cubic time), and furthermore the update of Matrix Winnow requires a cubic-time computation of an eigendecomposition (to compute matrix exponentials) on each mistaken trial.2 In the following section, we focus on a construction based on a graph approximation for which we develop an efficient implementation of the Perceptron algorithm which will require only poly-logarithmic time per round."
    }, {
      "heading" : "4 Efficient similarity prediction on graphs",
      "text" : "Relying on the notation of Section 3, we turn to efficient similarity prediction on graphs. We present adaptations of Matrix Winnow and Matrix Perceptron to the case when the original graph G is sparsified through a linearized and rebalanced random spanning tree of G. This sparsification technique, called Binary Support Tree (BST) in [29], brings the twofold advantage of yielding improved mistake bounds and faster prediction algorithms. More specifically, the use of a BST replaces the (perhaps very large) resistance diameter term max(i,j)∈V 2 R G i,j in the mistake bounds of Proposition 2 by a logarithmic term, the other term in the mistake bound becoming (when dealing with the expected number of mistakes) only a logarithmic factor larger than the (often far smaller) sum of the resistance-weighted cut-sizes in a spanning tree. Moreover, when combined with the Perceptron algorithm, a BST allows us to develop a very fast implementation whose running time per round is poly-logarithmic in n, rather than cubic, as in Matrix Winnow-like algorithms.\nRecall that a uniformly random spanning tree of an unweighted graph can be sampled in expected time O(n lnn) for “most” graphs [5]. Using the nice algorithm of [48], the expected time reduces to O(n) —see also the work of [1]. However, all known techniques take expected time Θ(n3) in certain pathological cases.\nIn a nutshell, a BST B of G is a full balanced binary tree whose leaves correspond to the vertices\n2 Additionally, there is a tuning issue related to Matrix Winnow, since the threshold θ̂w depends on the (unknown) cut-size.\nin3 V . In order to construct B from G, we first extract a random spanning tree T of G, then we visit T through a depth-first visit, order its vertices according to this visit eliminating duplicates (thereby obtaining a path graph P ), and finally we build B on top of P . Since B has 2n−1 vertices, we extend the class labels from leaves to internal vertices by letting, for each internal vertex i of B, yi be equal to the class label of i’s left child. Figure 1 illustrates the process. A simple adaptation of [29] (Section 6 therein) shows that for any class k = 1, . . . ,K we have |ΦBk | ≤ 2 |ΦTk | log2 n. With the above handy, we can prove the following bounds (see Appendix A.3 for further details).\nTheorem 3. Let (G,y) be a labeled graph, T be a random spanning tree of G, B be the corresponding BST, and ΨB be the (transposed) incidence matrix associated with B.\n1. If we run Matrix Winnow with similarity instances constructed from ΨB (see Algorithm 1) then the expected number of mistakes E[M ] on G satisfies E[M ] = O ( ϕ log3 n ) ,\n2. and if we run the Matrix Perceptron algorithm with similarity instances constructed from ΨB then E[M ] = O ( ϕ2 log4 n ) ,\nwhere we denote the resistance-weighted cut-size as ϕ = E[|ΦT |] = ∑\n(i,j)∈ΦG R G i,j.\nThe bound for Matrix Winnow is optimal up to a log3 n factor — compare to the lower bound in Proposition 1. However, this tight bound is obtained at the cost of having an algorithm which is O(n3) per round, even when run on a tree. This is because matrix exponentials require storing and updating a full SVD of the algorithm’s weight matrix at each round, thereby making this algorithm highly impractical when G is large. On the other hand, the Perceptron bound is significantly suboptimal (due to its dependence on the squared resistance-weighted cut-size), but it has the invaluable advantage of lending itself to a very efficient implementation: Whereas a naive implementation would lead to an O(n2) running time per round, we now show that a more involved implementation exists which takes only O(log2 n), yielding an exponential improvement in the per-round running time."
    }, {
      "heading" : "4.1 Implementing Matrix Perceptron on BST",
      "text" : "The algorithm operates on the BST B by maintaining a (2n − 1) × (2n − 1) symmetric matrix F with integer entries initially set to zero. At time t, when receiving the pair of leaves (it, jt),\n3 We assume w.l.o.g. that n = |V | is a power of 2. Otherwise, we may add dummy “leaves”.\nthe algorithm constructs Pt, the (unique) path in B connecting it to jt. Then the prediction ŷit,jt ∈ {0, 1} is computed as\nŷit,jt =\n{ 1 if ∑ `,`′∈Pt F`,`′ ≥ 4 log 2 n,\n0 otherwise . (4)\nUpon receiving label yit,jt , the algorithm updates F as follows. First of all, the algorithm is mistake driven, so an update takes place only if yit,jt 6= ŷit,jt . Let Nt be the set of neighbors of the vertices in Pt, and define St := Nt \\ (Pt \\{it, jt}). We recursively assign integer tags ft(`) to vertices ` ∈ Nt as follows: 1. For all ` ∈ Pt, if ` is the s-th vertex in Pt then we set ft(`) = s; 2. For all ` ∈ Nt \\Pt, let n` be the (unique) neighbor of ` that is contained in Pt. Then we set ft(`) = ft(n`). We then update F on each pair (`, `′) ∈ S2t as\nF`,`′ ← F`,`′ + (2yit,jt − 1) (ft(`)− ft(`′))2 . (5)\nFigure 2 illustrates the process. The following theorem is the main technical result of this section. Its involved proof is given in Appendix A.3.\nTheorem 4. Let B be a BST of a labeled graph (G,y) with |V | = n. Then the algorithm described by (4) and (5) is equivalent to Matrix Perceptron run with similarity instances constructed from ΨB. Moreover, the algorithm takes O(log2 n) per trial, and there exists an adaptive representation of F with an initialisation time of only O(n) (rather than O(n2))."
    }, {
      "heading" : "5 The Unknown Graph Case",
      "text" : "We now consider the case when the graph G = (V,E) is unknown to the learner beforehand. The graph structure is thus revealed incrementally as more and more pairs (it, jt) get produced by the adversary. A reasonable online protocol that includes progressive graph disclosure is the following. At the beginning of round t = 1 the learner knows nothing about G, but the number of vertices n\n— prior knowledge of n makes presentation easier, but could easily be removed from this setting. In the generic round t, the adversary presents to the learner both pair (it, jt) ∈ V × V and a path within G from it to jt. The learner is then compelled to predict whether or not the two vertices are similar. Notice that, although the presented path may have cut-edges, there might be alternative paths in G connecting the two vertices with no cut-edges. The learner need not see them. The adversary then reveals the similarity label yit,jt in G, and the next round begins. In this setting, the adversary has complete knowledge of G, and can decide to produce paths and place the cutedges in an adaptive fashion. Notice that, because of the incremental disclosure of G, no such constructions as Ψ-based similarity instances and/or BST, as contained in Section 3.2 and Section 4, are immediately applicable.\nAs a simple warm-up, consider the case when G is a tree and the K class label sets of vertices (henceforth called clusters) correspond to connected components of G. Figure 3 (a) gives an example. Since the graph is a tree, the number of cut-edges equals K − 1. We can associate with such a tree a linear-threshold function vector u = (u1, . . . , un−1)\n> ∈ {0, 1}n−1, where ui is 1 if and only if the i-th edge is a cut-edge. The ordering of edges within u can be determined ex-post by first disclosure times. For instance, if in round t = 1 the adversary produces pair (6, 4) and path 6 → 3 → 1 → 4 (Figure 3 (a)), then edge (6, 3) will be the first edge, (3, 1) will be the second, and (1, 4) will be the third. Then, if in round t = 2 the new pair is (3, 5) and the associated path is 3 → 1 → 5, the newly revealed edge (1, 5) will be the fourth edge within u. With this ordering in mind, the algorithm builds at time t the (n − 1)-dimensional vector xt = (x1,t, . . . , xn−1,t)\n> ∈ {0, 1}n−1 corresponding to the path disclosed at time t, where xi,t is 1 if and only if the i-th edge belongs to the path. Now, it is clear that yit,jt = 1 if u\n>xt ≥ 1, and yit,jt = 0 if u\n>xt = 0. Therefore, this turns out to be a sparse linear-threshold function learning problem, and a simple application of the standard Winnow algorithm [36] leads to an O(K log n) = O ( |ΦG| log n ) mistake bound obtained by an efficient (O(n) time per round) algorithm, independent of the structural properties of G, such as its diameter. One might wonder if an adaptation of the above procedure exists which applies to a general graph G by, say, extracting a spanning tree T out of G, and then applying the Winnow algorithm on T . Unfortunately, the answer is negative for at least two reasons. First, the above linearthreshold model heavily relies on the fact that clusters are connected, which need not be the case in our similarity problem. More critically, even if the clusters are connected in G, they need not be connected in T . Figure 3 (b)-(c) shows a typical example where Winnow applied to a spanning tree fails. Given this state of affairs, we are lead to consider a slightly different representation for pairs of vertices and paths. Yet, as before, this representation will suggest a linear separability condition, as well as the deployment of appropriate linear-threshold algorithms."
    }, {
      "heading" : "5.1 Algorithm and analysis",
      "text" : "Algorithm 2 contains the pseudocode of our algorithm. When interpreted as operating on vectors, the algorithm is simply an r-norm perceptron algorithm [20, 17] with nonzero threshold, and norm r = 2 log(n − 1)2 = 4 log(n − 1), being (n − 1)2 the length of the vectors maintained throughout, and s the dual to norm r. At time t, the algorithm observes pair (it, jt) and path p(it→jt), builds the instance vector xt ∈ {−1, 0, 1}n−1 and the long vector vec(Xt) out of the rank-one matrix Xt = xtx > t , where vec(·) is the standard vectorization of a matrix that stacks its columns one underneath the other. In order to construct xt from p(it→jt), the algorithm maintains a forest made up of the union of paths seen so far. If pair (it, jt) is already connected by a path p in the current forest, then xt is the instance vector associated with path p (as for the Winnow algorithm on\n3 4 7\n21\n6\n5\ne4 e6\ne1\ne2\ne5\ne3 3 4\n7\n21\n6\n5\n(a) (c)(b)\n3 4\n7\n21\n6\n5\na tree in the previous section, but taking edge orientations into account – see Figure 7 in Appendix A.4 for details). Otherwise, path p(it→jt) is added to the forest and xt will be an instance vector associated with the new path p(it→jt). In adding the new path to the forest, we need to make sure that no circuits are generated. In particular, as soon as a revealed edge in a path causes two subtrees to join, the algorithm merges the two subtrees and processes all remaining edges in that path in a sequential manner so as to avoid generating circuits. The algorithm will end up using a spanning tree T of G for building its instance vectors xt. This spanning tree is determined on the fly by the adversarial choices of pairs and paths, so it is not known to the algorithm ahead of time.4 But any later change to the spanning forest is designed so as to keep consistency with all previous vectors xt.\nThe decision threshold (r − 1)||xt||4r = (r − 1)||vec(Xt)||2r follows from a standard analysis of the r-norm perceptron algorithm with nonzero threshold (easily adapted from [20, 17]), as well as the update rule. In short, since the graph is initially unknown, the algorithm is pretending to learn vectors rather than (Laplacian-regularized) matrices, and relies on a regularization that takes advantage of the sparsity of such vectors. The analysis of Theorem 5 below rests on ancillary (and\n4 In fact, because the algorithm is deterministic, this spanning tree is fully determined by the adversary. We are currently exploring to what extent randomization is beneficial for an algorithm in this setting.\nclassical) properties of matroids on graphs. These are recalled in Appendix A.4, before the proof of the theorem.\nTheorem 5. With the notation introduced in this section, let Algorithm 2 be run on an arbitrary sequence of pairs (i1, j1), (i2, j2), . . . and associated sequence of paths p(i1→j1), p(i2→j2), . . . . Then we have the mistake bound M = O ( |ΦG|4 log n ) .\nRemark 1. As explained in the proof of Theorem 5, the separability condition (9) allows one to run any vector or matrix mirror descent linear-threshold algorithm. In particular, since matrix U therein is spectrally sparse (rank K << n), one could use unitarily invariant regularization methods, like (squared) trace norm-based online algorithms (e.g., [47, 10, 31]). For instance, Matrix Winnow (more generally, Matrix EG-like algorithms [46]) would get bounds which are linear in the cutsize but also (due to their unitary invariance) linear in ||xt||22. The latter can be as large as the diameter of T , which can easily be O(n) even if the diameter of G is much smaller. This makes these bounds significantly worse than Theorem 5 when the total cutsize |ΦG| is small compared to n (which is our underlying assumption throughout). Group norm regularizers can also be used. Yet, because Xt has rank one, when |ΦG| is small these regularizers do not lead to better bounds5 than Theorem 5. Moreover, it is worth mentioning that, among the standard mirror descent linear-threshold algorithms operating on vectors vec(·), our choice of the r-norm Perceptron is motivated by the fact this algorithm achieves a logarithmic bound in n with no prior knowledge of the actual cutsize |ΦG| (or an upper bound thereof) – see Section 3.2, and the discussion in [17] about tuning of parameters in r-norm Perceptron and Winnow/Weighted Majority-like algorithms.\nAs a final remark, our algorithm has an O(n2) running time per round, trivially due to the update rule operating on O(n2)-long vectors. The construction of instance vector xt out of path p(it→jt) can indeed be implemented faster than Θ(n\n2) by maintaining well-known data structures for disjoint sets (e.g., [14, Ch. 22])."
    }, {
      "heading" : "A Proofs",
      "text" : "This appendix contains all omitted proofs. Notation is as in the main text.\nA.1 Missing proofs from Section 2\nThe set of example sequences consistent with a concept f for class prediction is denoted by Sc(f) := ({(x, f(x))}x∈X )∗, and for similarity prediction by Ss(f) := ({((x′, x′′), sim(f(x′), f(x′′))}x′,x′′∈X )∗. A prediction algorithm is a mapping A : (X × Y)* → YX from example sequences to prediction functions. Thus if A is a prediction algorithm and S = (x1, y1), . . . , (xT , yT ) ∈ (X × Y)* is an example sequence, then the online prediction mistakes are\nMA(S) := T∑ t=1 [A((x1, y1), . . . , (xt−1, yt−1))(xt) 6= yt] .\nWe write S′ ⊆ S to denote that S′ is a subset of S as well to denote that S′ is a subsequence of S. We now introduce a weaker notion of a mistake bound as defined with respect to specific sequences rather than to the alternate notion of a concept. The weakness of this definition allows the construction in the following lemma to apply to “noisy” as well “consistent” example sequences.\nDefinition 6. Given an algorithm A, we define the subsequential mistake bound with respect to an example sequence S as\nB◦A(S) := max S′⊆S MA(S ′) .\nThus, a subsequential mistake bound is simply the “worst-case” mistake bound over all subsequences.\nLemma 7. Given an online classification algorithm A, there exists a similarity algorithm A′ such that for every sequence S = (x1, y1), . . . , (x2T , y2T ) its mistakes on\nS′ = ((x1, x2), sim(y1, y2)), . . . , ((x2T−1, x2T ), sim(y2T−1, y2T ))\nis bounded as MA′(S ′)) ≤ cB◦A(S) log2K , (6)\nwith c < 5.\nProof. The proof of (6) works by a modification of the standard weighed majority algorithm [37] arguments. The key idea is that similarity reduces to classification if we received the actual class labels as feedback rather than just similar/dissimilar as feedback. Since we do not have the classlabels, we instead “hallucinate” all possible feedback histories and then combine these histories on each trial using a weighted majority vote. If we only keep track of histories generated when the weighted majority vote is mistaken, the bound is small. Our master voting algorithm A′ follows.\n1. Initialisation: We initialize the parameter β = 0.294. We create a pool containing example sequences (“hallucinated histories”) S := {s}, with initially the empty history s = 〈〉 with weight ws := 1 .\n2. For t = 1, . . . , T do\n3. Receive: the pattern pair (x2t−1, x2t)\n4. Predict: similar if∑ s∈S ws[A(s)(x2t−1) = A(s)(x2t)] ≥ ∑ s∈S ws[A(s)(x2t−1) 6= A(s)(x2t)]\notherwise predict dissimilar.\n5. Receive: Similarity feedback sim(y2t−1, y2t) if prediction was correct go to 2.\n6. Two cases, first if this algorithm predicted similar when the pair was dissimilar then for each history s ∈ S with a mistaken prediction create K × (K − 1) histories s1,2, . . . , sK,K−1 so that si,j is equal to s but has the two “speculated” examples (x2t−1, i), (x2t, j) appended\nto it. Then set ws1,2 = . . . = wsK,K−1 := β K(K−1)ws and remove s from S. Second (predicted dissimilar) as above but now we need to add only K new histories to the pool.\n7. Go to 2.\nObserve that there exists a history s∗ ∈ S generated by no more B◦A(S) “mistakes” since there is always at least one history in the pool S which is a subsequence of S. Thus\nws∗ ≥ (\nβ\nK(K − 1)\n)B◦A(S) .\nFurthermore, the total weight of the pool of histories W := ∑\ns∈S ws is reduced to a fraction of\nits weight no larger than 1+β2 whenever this master algorithm A ′ makes a mistake. Thus since\nW ≥ ws∗ , we have ( 1 + β\n2\n)MA′ (S′) ≥ (\nβ\nK(K − 1)\n)B◦A(S) .\nSolving for MA′(S ′) we have\nMA′(S ′) ≤ B◦A(S)  log2 K(K−1)β log2 2 1+β  . Substituting in β = .294 allows us to obtain the upper bound of (6) with c ≈ 4.99.\nWe observe, that reduction of similarity to classification holds for a wide variety online mistake bounds. Thus, e.g., we do not require the input sequence to be consistent, i.e., in S we may have examples (x′, y′) and (x′′, y′′) such that x′ = x′′ but y′ 6= y′′. The usual type of mistake bound is permutation invariant i.e., the bound is the “same” for all permutations of the input sequence; typical examples include the the Weighted Majority [37] and p-Norm Perceptron [20, 17] algorithms. Observe that if BpA(S) is a permutation invariant bound, then B ◦ A(S) ≤ B p A(S), since every subsequence of S is the prefix of a permutation of S. However, our reduction also more broadly applies to such “order-dependent” bounds, as the shifting-expert bounds in [26].\nWe now show that classification reduces to similarity. This reduction is efficient and does not introduce a multiplicative constant but requires the stronger assumption of consistency not required by Lemma 7.\nLemma 8. Given an online similarity algorithm As there exists an online classification algorithm Ac such that for any concept f if S ∈ Sc(f) then\nMAc(S) ≤ max S′∈Ss(f)\nMAs(S ′) +K . (7)\nProof. As a warm-up, pretend we know a set P ⊆ X such that |P | = K and for each i ∈ {1, . . . ,K} there exists an x ∈ P such that f(x) = i. Using P we create algorithm Ac as follows. We maintain a history (example sequence) h, which is initially empty. Then on every trial when we receive a pattern xt we predict ŷt ∈ {f(x) : As(h)((x, xt)) = similar, x ∈ P}, and if the set contains multiple elements or is empty then we predict arbitrarily. If Ac incurs a mistake, we add to our history h the K examples ((x, xt), sim(f(x), f(xt)))x∈P . Observe that if A\nc incurs a mistake then at least one example corresponding to a mistaken similarity prediction is added to h and necessarily h ∈ Ss(f). Thus the mistakes of Ac are bounded by maxS′∈Ss(f)MAs(S′). Now, since we do not actually know a set P , we may modify our algorithm Ac so that although P is initially empty we predict as before, and if we make a mistake on xt because there does not exist an x ∈ P such that f(x) = f(xt), we then add xt to P . We can only make K such mistakes, so we have the bound of (7).\nProof of Theorem 1: If S is a classification sequence consistent with a concept f and A is a classification algorithm, then B◦A(S) ≤ BA(f), and hence (6) implies (1). Then, since (7) is equivalent to (2), we are done. 2\nA.1.1 The logK term is necessary in Theorem 1\nIn our study of class prediction on graphs we observed (see Appendix A.2) that certain 2-class bounds may be converted to K-class bounds with no explicit dependence on K. Yet Theorem 1 introduces a factor of logK for similarity prediction. So a question that arises is this simply a byproduct of the above analysis or is the “logK” factor tight. In the following, we demonstrate it is tight by introducing the paired permutation problem, which may be “solved” in the classification setting with no more than O(K) mistakes. Conversely, we show that an adversary can force Ω(K logK) mistakes in the similarity setting.\nWe introduce the following notation. Let z : {1, . . . ,K} → {1, . . . ,K} denote a permutation function, a member of the set ZK of all K! bijective functions from {1, . . . ,K} to {1, . . . ,K}. A paired permutation function is the mapping yz : {1, . . . ,K}2 → {1, . . . ,K}, with yz(x′, x′′) := max(z(x′), z(x′′)). So, for example, consider a 3-element permutation z(1) → 2, z(2) → 3, and z(3) → 1. Then, e.g., yz(1, 2) → 3 and yz(1, 1) → 2. Thus, we define the set of the paired permutation problem example sequences for class prediction as PPc := ∪z∈ZKSc(yz), and for similarity as PPs := ∪z∈ZKSs(yz).\nTheorem 9. There exists a class prediction algorithm A such that for any S ∈ PPc we have MA(S) = O(K). Furthermore, for any similarity prediction algorithm A′, there exists an S′ ∈ PPs such that MA′(S ′) = Ω(K logK).\nProof of Theorem 9: First, we show that there exists a class prediction algorithm A such that for any S ∈ PPc we have MA(S) = O(K). Consider the simpler problem for the concept class of permutations ∪z∈ZKSc(z). By simply predicting consistently with the past examples we cannot incur more than K − 1 mistakes. The algorithm A0(s) (consistent predictor), predicts y on receipt of pattern xt if there exists some example (x, y) in its history s such that xt = x, otherwise it predicts a y ∈ {1, . . . ,K} not in its history. Now, using A0 as a base algorithm, we can use the principle of the master algorithm of Lemma 7 to achieve O(K) mistakes for the paired permutation problem. Thus, when we receive a pair ((x′, x′′), y) either yz(x ′) = y or yz(x ′′) = y, hence on a mistake we may “hallucinate” these two possible continuations. Our class prediction example sequence is S = (x′1, x ′′ 1), y1), . . . , ((x ′ T , x ′′ T ), yT ) ∈ PP c, and the algorithm A follows.\n1. Initialisation: We initialize the parameter β = 0.294. We create a pool containing example sequences (“hallucinated histories”) S := {s}, with initially the empty history s = 〈〉 with weight ws := 1 .\n2. For t = 1, . . . , T do\n3. Receive: the pattern xt = (x ′ t, x ′′ t )\n4. Predict: ŷt = argmax\nk∈{1,...,K} ∑ s∈S ws[max(A0(s)(x ′ t), A0(s)(x ′′ t )) = k] . (8)\n5. Receive: Class feedback yt ∈ {1, . . . ,K}. If prediction was correct go to 2.\n6. For each history s ∈ S with a mistaken prediction, create two histories s′, s′′ so that s′ (s′′) is equal to s but has the example (x′t, y) (the example (x ′′ t , y)) appended to it. Then set\nws′ = ws′′ := β 2ws , and remove s from S.\n7. Go to 2.\nObserve that there exists a history s∗ ∈ S generated by no more K−1 “mistakes”. This is because, by induction, there is always a consistent history (i.e., the empty history is initially consistent, and when the master algorithm A makes a mistake and a consistent history makes a mistake, then either the continuation (x′t, y) or (x ′′ t , y) is consistent). Finally, observe that once a consistent history contains K − 1 examples, it can no longer make mistakes. Thus\nws∗ ≥ ( β\n2\n)K−1 .\nFurthermore, the total weight of the pool of histories W := ∑\ns∈S ws is reduced to a fraction of its\nweight no larger than 1+β2 whenever this master algorithm A makes a mistake. Since W ≥ ws∗ , we have (\n1 + β\n2\n)MA(S) ≥ ( β\n2\n)K−1 .\nSolving for MA(S) we can write\nMA(S) ≤ (K − 1)\n( log2 2 β\nlog2 2\n1+β\n) .\nSubstituting in β = 0.294 allows us to obtain the upper bound of MA(S) ≤ 4.1(K − 1). The argument then follows as in Theorem 1.\nNow consider the similarity problem. If we receive an instance of the form (((x′, x′′), (x′′, x′′)), y) (with x′ 6= x′′) then y = similar implies z(x′) < z(x′′) and y = disimilar implies z(x′) > z(x′′). Thus with each mistaken example we learn precisely a single ‘<’ comparison. It follows from standard lower bounds on comparison-based sorting algorithms (e.g., [14]) that an adversary can force Ω(K logK) comparisons, and thus mistakes, for any “comparison”-algorithm to learn an arbitrary permutation. 2\nAny problem associated with a set of example sequences S may be iterated into a set of r independent problems by a cross-product-like construction, so that if S1, . . . , Sr ∈ S and if Si = (xi1, y i 1), . . . , (x i Ti , yiTi) then an r-iterated example sequence is\n((x11, 1), y 1 1), . . . , ((x 1 T1 , 1), y 1 T1), . . . , ((x i 1, i), y i 1), . . . , ((x r Tr , r), y r Tr) .\nWe have simply conjoined the r example sequences into a single example sequence with each pattern “x” paired with an integer indicating from which sequence it originated. Thus by r-iterating the paired permutation problem we trivially observe mistake bounds of O(rK) and Ω(rK logK) for all r ∈ N in the class and similarity setting, respectively, thereby implying that the multiplicative “logK” gap occurs for an infinite family of classification/similarity problems.\nA.2 Missing proofs from Section 3\nLifting 2-class prediction to K-class prediction on graphs\nSuppose we have an algorithm for the 2-class graph labeling problem with a mistake bound of the form M ≤ c|ΦG(y)| for all y ∈ {1, 2}n, with c ≥ 0, We show that this implies the existence in the K-class setting of an algorithm with a bound of M ≤ 2c|ΦG(y)| for all y ∈ {1, . . . ,K}n, where K need not be known in advance to the algorithm.\nThe algorithm simply works by combining the predictions of “one versus rest” classifiers. We train one classifier per class, and introduce a new classifier as soon as that class first appears. On any given trial, the combination is straightforward: If there is only one classifier predicting with its own class then we go with that class, otherwise we just assume a mistake. Thus, on any given trial, we can only be mistaken if one of the current “one-verse-rest” classifiers makes a mistake. This implies that our mistake bound is the sum of the mistake bounds of all of the “one-verse-rest” classifiers. Because each such binary classifier has a mistake bound of the form M ≤ c|ΦGk (y)|, and∑K\nk=1 |ΦGk (y)| = 2|ΦG(y)|, we have that the K-class classifier has a bound of the form 2c|ΦG(y)|.\nProof of Theorem 2: We show that computing the partition function for the Ising model on a general graph reduces to computing the partition function problem for the Ising Model on a path graph with pairwise constraints hence showing #P-completeness. The partition problem for the Ising Model on a graph is defined by,\nInstance : An n-vertex graphG, and a natural number, β, presented in unary notation.\nOutput: The value of the partition function ZG(β), ZG(β) := ∑\ny∈{1,2}n 2−β|φ G(y)| .\nThis problem was shown #P-complete in [30, Theorem 15]. The reduction to the partition problem on a path graph with constraints is as follows.\nWe are given a graph G = (VG, EG) with n = |VG|, and further assume each vertex is “labeled” uniquely from 1, . . . , n. We construct the following path graph with pairwise constraints (see Figure 4) for an illustration.\n1. Find a spanning tree T = (VT , ET ) of G, and let R = EG − ET .\n2. Perform a depth-first-visit of T . From the 2n− 1 vertex visit sequence, create an isomorphic path graph P0 with 2n−1 vertices such that each vertex in P0 is labeled with the corresponding vertex label from the visit of T . Thus each edge of T is mapped to two edges in P0.\n3. We now proceed to create a path graph P = (VP , EP ) from P0, which also includes each edge in R twice. We initialize P as a “duplicate” of P0 including labels. For each edge (v ′ r, v ′′ r ) ∈ R\nwe then do the following:\n(a) Choose an arbitrary vertex v′ ∈ VP so that v′ and v′r have the same label; (b) Let v′′′′ be a neighbor of v′ in P (i.e, (v′, v′′′′) ∈ EP ); (c) Add vertices v′′ and v′′′ to P with the labels of v′′r and v ′ r, respectively;\n(d) Remove the edge (v′, v′′′′) from P and add the edges (v′, v′′), (v′′, v′′′) and (v′′′, v′′′′) to P .\n4. Finally create pairwise equality constraints between all vertices with the same label.\nThus observe for every edge in G there are two analogous edges in P , and furthermore if edge (v, w) 6∈ G then there is not an analogous edge in P . Hence ZG(2β) = ZP (C, β).\nProof sketch Proposition 2 We start off with the Matrix Perceptron bound. For brevity, we write Xt instead of X p t . Also, let 〈A,B〉 be a shorthand for the inner product tr(ATB). We can write\n〈Xt, Xt〉 = tr((Ψ+)T (eit − ejt)(eit − ejt)TΨ+(Ψ+)T (eit − ejt)(eit − ejt)TΨ+) = tr((eit − ejt)TΨ+(Ψ+)T (eit − ejt)(eit − ejt)TΨ+(Ψ+)T (eit − ejt)) = ((eit − ejt)TL+(eit − ejt))2\n= (RGit,jt) 2 ≤ R2 .\nMoreover, for any k ∈ {1, . . . ,K}, define K vectors u1, . . . ,uK ∈ Rn as follows.\nuk = (uk,1, . . . , uk,n) >, with uk,i = [k = yi] ,\nbeing yi the label of the i-th vertex of G. Now, if we let U := Ψ (∑K k=1 uku > k ) Ψ>, we have\n〈U,Xt〉 = tr(UTXt)\n= K∑ k=1 tr(Ψuku T k Ψ T (Ψ+)T (eit − ejt)(eit − ejt)TΨ+)\n= K∑ k=1 tr((eit − ejt)TΨ+ΨukuTk ΨT (Ψ+)T (eit − ejt))\n= K∑ k=1 ((eit − ejt)TΨ+Ψuk)2 .\nBy definition of pseudoinverse, Ψ(Ψ+Ψuk) = (ΨΨ +Ψ)uk = Ψuk for all k = 1, . . . ,K. Hence (recall Section 3), Ψ+Ψuk = uk + c1 for some c ∈ R. We therefore have that (eit − ejt)TΨ+Ψuk = uk,it − uk,jt , i.e.,\n〈U,Xt〉 = K∑ k=1 (uk,it − uk,jt)2 .\nNow, if yit,jt = 0 (i.e., yit = yjt) then for all k we have uk,it − uk,jt = 0, so that 〈U,Xt〉 = 0. On the other hand, if yit,jt = 1 (i.e., yit 6= yjt) then there exist distinct a, b ∈ {1, . . . ,K} such that |ua,it − ua,jt | = |ub,it − ub,jt | = 1, and for all other k 6= a, b we have uk,it − uk,jt = 0. So, in this case 〈U,Xt〉 = 2.\nThis gives the linear separability condition of sequence (X1, yi1,j1), (X2, yi2,j2), . . . w.r.t. U . Finally, we bound 〈U,U〉. Let ΦGa,b := {(i, j) ∈ E : yi = a, yj = b}. We have:\n〈U,U〉 = tr(UTU)\n= tr (( K∑ a=1 Ψuau T aΨ T )( K∑ b=1 Ψubu T b Ψ T ))\n= K∑ a=1 K∑ b=1 tr(Ψuau T aΨ TΨubu T b Ψ T )\n= K∑ a=1 K∑ b=1 tr(uTb Ψ TΨuau T aΨ TΨua)\n= K∑ a=1 K∑ b=1 (uTb Ψ TΨua) 2\n= K∑ a=1 |ΦGa |2 +∑ b6=a |ΦGa,b|2  . So, noticing that ∑ b : b 6=a |ΦGa,b| = |ΦGa | and hence that ∑ b : b6=a |ΦGa,b|2 ≤ |ΦGa |2, we conclude that\n〈U,U〉 ≤ 2|ΦG|2 .\nWith the above handy, the mistake bound on Mp easily follows from the standard analysis of the Perceptron algorithm with nonzero threshold.\nBy a similar token, the bound on Mw follows from the arguments in [47], after defining U to be a normalized version of the one we defined above for Matrix Perceptron, and noticing that Xwt in Algorithm 1 are positive semidefinite and normalized to trace 1.\nA.3 Missing proofs from Section 4\nThe following lemma relies on the equivalence between effective resistance RGi,j of an edge (i, j) and its probability of being included in a randomly drawn spanning tree.\nLemma 10. Let (G,y) be a labeled graph, and T be a spanning tree of G drawn uniformly at random. Then, for all k = 1, . . . ,K, we have:\n1. E[|ΦTk |] = ∑ (i,j)∈ΦGk RGi,j, and\n2. E[|ΦTk |2] ≤ 2( ∑ (i,j)∈ΦGk RGi,j) 2 .\nProof. Set s = |ΦGk | and ΦGk = {(i1, j1), (i2, j2), . . . , (is, js)}. Also, for ` = 1, . . . , s, let X` be the random variable which is 1 if (i`, j`) is an edge of T , and 0 otherwise. From E[X`] = RGi`,j` we immediately have 1). In order to prove 2), we rely on the negative correlation of variables X`, i.e., that E[X`X`′ ] ≤ E[X`]E[X`′ ] for ` 6= `′ (see, e.g., [38]). Then we can write\nE(|ΦTk |2) = E ( s∑ `=1 X` )2 = E\n[ s∑ `=1 s∑ `′=1 X`X`′ ]\n= s∑ `=1 E[X`] + s∑ `=1 ∑ `′ 6=` E[X`X`′ ]\n≤ s∑ `=1 E[X`] + s∑ `=1 ∑ `′ 6=` E[X`]E[X`′ ] .\nNow, for any spanning tree T of G, if s ≥ 1 then it must be the case that |ΦTk | ≥ 1, and hence∑s `=1 E[X`] = E[|ΦTk |] ≥ 1 . Combined with the above we obtain:\nE[|ΦTk |2] ≤ ( s∑ `=1 E[X`] )2 + s∑ `=1 ∑ `′ 6=` E[X`]E[X`′ ] ≤ 2 ( s∑ `=1 E[X`] )2 = 2 ( s∑ `=1 RGi`,j` )2 ,\nas claimed.\nProof of Theorem 3 From Proposition 2 we have that if we execute Matrix Winnow on B = BT with similarity instances constructed from ΨB, then the number M of mistakes satisfies\nM = O ( |ΦB|DB log n ) ,\nwhere DB is the resistance diameter of B. Since B is a tree, its resistance diameter is equal to its diameter, which is O(log n). Moreover, |ΦBk | = O(|ΦTk | log n), for k = 1, . . . ,K, hence |ΦB| = O(|ΦT | log n). Plugging back, taking expectation over T , and using Lemma 10, 1) proves the Matrix Winnow bound. Similarly, if we run the Matrix Perceptron algorithm on B with similarity instances constructed from ΨB then\nM = O ( |ΦB|2D2B ) .\nProceeding as before, in combination with Lemma 10, 2), proves the Matrix Perceptron bound.\nProof of Theorem 4 First of all, the fact that the algorithm is O(log2 n) per round easily follows from the fact that, since B is a balanced binary tree, the sizes of sets Pt (prediction step in (4)) and St (update step in (5)) are both O(log n).\nAs for initialization time, a naive implementation would require O(n2) (we must build the zero matrix F ). We now outline a method of growing a data structure that stores a representation of F online for which the initialisation time is only O(n), while keeping the per round time to O(log2 n). For every vertex ` in B the algorithm maintains a subtree B` of B, initially set to {ρ}, being ρ the root of B. At every vertex `′ ∈ B` is stored the value F`,`′ . At the start of time t, the algorithm climbs B from it to ρ, in doing so storing the ordered list Lit of vertices in the path from ρ to it. The same is done with jt. The set St is then computed. For all ` ∈ St, the tree B` is then extended to include the vertices in Nt and the path from it (note that for each ` ∈ St this takes only O(log n) time, since we have the list Lit). Whenever a new vertex `′ is added to B`, the value F`,`′ is set to zero. Hence, we initialize F “on demand”, the only initialization step being the allocation of the BST, i.e., O(n) time.\nWe now continue by showing the equivalence of the sequence of predictions issued by (4) to those of the Matrix Perceptron algorithm with similarity instances constructed from ΨB.\nFor every ` ∈ St define Λt(`) as the maximal subtree of B that contains ` and does not contain any nodes in Pt \\ {it, jt}.\nLemma 11. Λt(·) defined above enjoys the following properties (see Figure 5, left, for reference).\n1. For all `, Λt(`) is uniquely defined;\n2. Any subtree T of B that has no vertices from Pt \\ {it, jt} (and hence any of the trees Λt) contains at most one vertex from St;\n3. The subtrees {Λt(`) : ` ∈ St} are pairwise disjoint;\n4. The set {Λt(`) : ` ∈ St}∪ (Pt \\ {it, jt}) covers B (so in particular {Λt(`) : ` ∈ St} covers the set of leaves of B).\nProof. 1. Suppose we have subtrees T and T ′ with T 6= T ′ that both satisfy the conditions of Λt(`). Then w.l.o.g assume there exists a vertex ` ′ in T that is not in T ′. Since T and T ′\nare both connected and both contain `, the subgraph T ∪ T ′ of B is connected and is hence a subtree. Since neither T nor T ′ contains vertices in Pt \\ {it, jt}, T ∪ T ′ does not contain any such either. Hence, because T ′ is a strict subtree of T ∪ T ′, we have contradicted the maximality of T ′.\n2. Suppose T has distinct vertices `, `′ ∈ St. Since T is connected, it must contain the path in B from ` to `′. This path goes from ` to the neighbor of ` that is in Pt \\ {it, jt}, then follows the path Pt \\ {it, jt} (in the right direction) until a neighbor of `′ is reached. The path then terminates at `′. Such a path contains at least one vertex in Pt \\ {it, jt}, contradicting the initial assumption about T .\n3. Assume the converse – that there exist distinct `, `′ in St such that Λt(`) and Λt(`′) share vertices. Then, since Λt(`) and Λt(`\n′) are connected, Λt(`) ∪ Λt(`′) must also be connected (and hence must be a subtree of B). Since Λt(`)∪Λt(`′) shares no vertices with Pt\\{it, jt}, and contains both ` and `′ (which are both in St), the statement in Item 2 above is contradicted.\n4. Assume that we have a ` ∈ B \\ (Pt \\ {it, jt}). Then let P ′ be the path from ` to the (first vertex encountered in) the path Pt \\{it, jt}. Let `′ be the second from last vertex in P ′. Then `′ is a neighbor of a vertex in Pt, but is not in Pt \\ {it, jt}, so it must be in St. This implies that the path P ′′ that goes from ` to `′ contains no vertices in Pt \\ {it, jt} and is therefore (Item 1) a subtree of Λt(` ′). Hence, ` ∈ Λt(`′).\nLemma 12. Let L be the Laplacian matrix of B, and `, `′ ∈ St. Then for any pair of vertices κ and κ′ of B with κ ∈ Λt(`) and κ′ ∈ Λt(`′) we have\n(eκ − eκ′)TL+(eit − ejt) = ft(`′)− ft(`) ,\nwhere ei is the i-th element in the canonical basis of R2n−1.\nProof. We first extend the tagging function ft to all vertices of B via the vector 6 f̃t as follows (note that, by Lemma 11, f̃t is well defined):\n1. For all ` ∈ Pt \\ {it, jt}, set f̃t(`) = ft(`);\n2. For all `′ ∈ St and ` ∈ Λt(`′), set f̃t(`) = ft(`′).\nClaim 1. Lf̃t = ejt − eit.\nProof of claim. For any vertex κ of B \\ {it, jt} one of the following holds:\n1. If κ ∈ Pt, then κ has a neighbor κ1 with f̃t(κ1) = f̃t(κ)− 1, one neighbour κ2 with f̃t(κ2) = f̃t(κ) + 1, and (unless κ is the root of B) one neighbour κ3 with f̃t(κ3) = f̃t(κ). We therefore have that [Lf̃t]κ = 3f̃t(κ)− f̃t(κ1)− f̃t(κ2)− f̃t(κ3) = 0.\n2. If κ ∈ Nt \\ Pt, then κ has one neighbor κ1 in Pt and we have f̃t(κ1) = f̃t(κ). Let Tκ be the subtree of B containing exactly vertex κ and all neighbors of κ bar κ1. Since Pt is connected, it contains κ1 and does not contain κ, none of the other neighbors of κ being in Pt. Hence Tκ is a subtree of B that contains κ and no vertices from Pt \\ {it, jt}, and so by Lemma 11, item 1 it must be a subtree of Λt(κ). Hence, by definition of f̃t, all vertices κ2 in Tκ satisfy f̃t(κ2) = f̃t(κ). This implies that for all neighbors κ3 of κ we have f̃t(κ3) = f̃t(κ), which in turn gives [Lf̃t]k = 0.\n3. If κ /∈ Nt then, by Lemma 11 item 4, let κ be contained in Λt(`) for some ` ∈ St. Let Tκ be the subtree of B containing exactly vertex κ and all neighbors of κ. Note that Tκ is a subtree of B that contains κ and no vertices from Pt \\ {it, jt}. Since Λt(`) also contains κ (hence Λt(`)∪Tκ is connected), we have that Λt(`)∪Tκ is a subtree of B that contains ` and no vertices from P \\ {it, jt}. By Lemma 11 item 1, this implies that Λt(`) ∪ Tκ is a subtree of (and hence equal to) Λt(`). Hence, by definition of f̃t, we have that f̃t is identical on Tκ. Thus all neighbors κ1 of κ satisfy f̃t(κ1) = f̃t(κ), implying again [Lf̃t]κ = 0.\nSo in either case [Lf̃t]κ = 0. Finally, let i′t be the neighbor of it in B. We have [Lf̃t]it = f̃t(it)−f̃t(i′t) = 1−2 = −1. Similarly, we have [Lf̃t]jt = 1. Putting together, we have shown that Lf̃t = ejt − eit , thereby concluding the proof of Claim 1.\nNow, by definition of pseudoinverse,\nLf̃t = LL +Lf̃t = LL +(ejt − eit) .\nThis mplies that L(f̃t−L+(ejt −eit)) = 0. Therefore (see Section 3) there exists a constant c such that f̃t = L +(ejt − eit) + c1. From the definition of f̃ we can write\nft(` ′)− ft(`) = f̃t(κ′)− f̃t(κ)\n= ([L+(ejt − eit)]κ′ − c)− ([L+(ejt − eit)]κ − c) = (eκ − eκ′)TL+(eit − ejt),\nas claimed.\n6 In our notation, we interchangeably view f̃ both as a tagging function from the 2n − 1 vertices of B to the natural numbers and as a (2n− 1)-dimensional vector.\nLemma 13. Let L be the Laplacian matrix of B, and κ, κ′ be two vertices of B. Let P be the path from κ to κ′ in B. Then for any t either |P ∩ St| ≤ 1 or P ∩ St = {`, `′}, for two distinct vertices ` and `′. No other cases are possible. Moreover,\n((eκ − eκ′)TL+(eit − ejt))2 = { 0 if |P ∩ St| ≤ 1 (ft(`)− ft(`′))2 if P ∩ St = {`, `′} .\nProof. By Lemma 11 item 4, we have two possible cases only:\n1. There exists ` ∈ St such that both κ and κ′ are in Λt(`): In this case (since Λt(`) is connected) the path P lies in Λt(`). Since, by Lemma 11 item 2, no `′ ∈ St with `′ 6= ` can be in Λt(`), it is only ever possible that P contains at most one vertex ` (if any) of St.\n2. There exist two distinct nodes `, `′ ∈ St such that κ ∈ Λ(`) and κ′ ∈ Λ(`′). In this case, P corresponds to the following path: First go from κ to ` (by Lemma 11 item 2, since this path lies in Λ(`) the only vertex in St that lies in the section of the path is `); then go to the neighbor of ` that is in Pt \\ {it, jt}; then follow the path Pt \\ {it, jt} until you reach the neighbor of `′ (this section of P contains no vertices in St); then go from `′ to κ (by Lemma 11 item 2, since this path lies in Λ(`′) the only vertex in St that lies in this section of the path is `′). Thus, P ∩ St = {`, `′}.\nThe result then follows by applying Lemma 12 to the two cases above.\nFigure 5 illustrates the above lemmas by means of an example. To conclude the proof, let 〈A,B〉 be a shorthand for tr(A>B). We see that from Algorithm 1,\nLemma 13, and the definition of F in (4) we can write\n〈Wt, Xt〉 = t−1∑\nt′=1,t′∈M (2yit′ ,jt′ − 1)〈Xt′ , Xt〉\n= t−1∑\nt′=1,t′∈M (2yit′ ,jt′ − 1)((eit′ − ejt′ ) TL+(eit − ejt))2\n= ∑\n(`,`′)∈P2t\nF`,`′ ,\nwhereM is the set of mistaken rounds, and the second-last equality follows from a similar argument as the one contained in the proof of Proposition 2. Threshold 2 log n in (4) is an upper bound on the radius squared 〈Xt, Xt〉 of instance matrices (denoted by R2 in Algorithm 1). In fact, from the proof of Proposition 2,\nmax t 〈Xt, Xt〉 ≤ max (i,j)∈V 2 ((ei − ej)>L+(ei − ej))2 = max (i,j)∈V 2 (RGi,j) 2 ,\nwhich is upper bounded by the square of the diameter 2 logn of B.\nA.4 Ancillary results, and missing proofs from Section 5\nThis section contains the proof of Theorem 5, along with preparatory results."
    }, {
      "heading" : "A digression on cuts and directed paths",
      "text" : "Given7 a connected and unweighted graph G = (V,E), with n = |V | vertices and m = |E| edges, any partition of V into two subsets induces a cut over E. A cut is a cutset if it is induced by a two-connected component partition of V . Fix now any spanning tree T of G (this will be the one constructed by the algorithm at the end of the game, based on the paths produced by the adversary – see Section 5.1). In this context, the n − 1 edges of T are often called branches and the remaining m − n + 1 edges are often called chords. Any branch of T cuts the tree into two components (it is therefore a cutset), and induces a two-connected component partition over V . Any such cutset is called a fundamental cutset of G (w.r.t. T ). Cuts are always subsets of E, hence they can naturally be represented as (binary) indicator vectors with m components. For reasons that will be clear momentarily, it is also convenient to assign each edge an orientation (tail vertex to head vertex) and each cut an inward/outward direction. In particular, it is customary to give a fundamental cut the orientation of its branch. As a consequence of orientations/directions, cuts are rather represented as m-dimensional vectors whose components have values in {−1, 0, 1}.\n7 The reader familiar with the theory of matroids will recognize what is recalled here as a well known example of a regular matroid on graphs. One can learn about them in standard textbooks/handbooks, e.g., [19, Ch.6].\nFigure 6 (a) gives an example. In this figure, all edges are directed from the low index vertex to the high index vertex. Branch e1 determines a cutset (more precisely, a fundamental cutset w.r.t. to the depicted spanning tree) separating vertices 2 and 7 from the remaining ones. Edge e6 isolates just vertex 6 from the rest (again, a fundamental cutset). The fundamental cut determined by branch e1 is represented by vector q = (1, 0, 0, 0, 0, 0,−1, 1, 0, 1, 0, 1). This is because if we interpret the orientation of branch e1 as outward to the cut, then e7 is inward, e8 is outward, as well as e10 and e12. The matrix in Figure 6 (b) contains as rows all fundamental cutsets. This is usually called the fundamental cutset matrix, often denoted by Q (recall that this matrix depends on spanning tree T – for readability, we drop this dependence from our notation). Matrix Q has rank n − 1. Moreover, any cut (viewed as an m-dimensional vector) in the graph can be represented as a linear combination of fundamental cutset vectors with linear combination coefficients −1, +1, and 0. In essence, cuts are an (n − 1)-dimensional vector space with fundamental cutsets (rows Qi of Q) as basis. It is important to observe that the vectors Qi involved in this representation are precisely those corresponding to the branches of T that are either moving inward (coefficient −1) or outward (coefficient +1). Hence the fewer are the branches of T cutting inward or outward, the sparser is this representation. Matrix Q has also further properties, like total unimodularity. This implies that any linear combination of their rows with coefficients in {−1, 0,+1} will result in a vector whose coefficients are again in {−1, 0,+1}.\nTo summarize, given a spanning tree T of G, a direction for G’s edges, and the associated matrix Q, any cutset8 q in G can be represented as an m-dimensional vector q = Q> u, where u ∈ {−1, 0,+1}n−1 has as many nonzero components as are the branches of T belonging to q. With this representation (induced by T ) in hand, we are essentially aimed at learning in a sequential fashion u’s components.\nIn order to tie this up with our similarity problem, we view the edges belonging to a given cutset as the cut edges separating a (connected) cluster from the rest of the graph, and then associate with any given K-labeling of the vertices of G a sequence of K weight vectors uk, k = 1, . . . ,K, each one corresponding to one label. Since a given label can spread over multiple clusters (i.e., the vertices belonging to a given class label need not be a connected component of G), we first need to collect connected components belonging to the same cluster by summing the associated coefficient vectors. As an example, suppose in Figure 6 (a) we have 3 vertex labels corresponding to the three colors. The blue cluster contains vertices 4, 5, 6 and 7, the green one vertex 1, and the red one vertices 2 and 3. Now, whereas the blue and the green labels are connected, the red one is not. Hence we have u{4,5,6,7} = (0, 0,−1,−1,−1,−1)>, u{1} = (1, 1, 1, 1, 0, 0)>, and u{2,3} = (−1,−1, 0, 0, 1, 1)> is the sum of the two cutset coefficient vectors u{2} = (−1, 0, 0, 0, 1, 0)>, and u{3} = (0,−1, 0, 0, 0, 1)>. In general, our goal will then be to learn a sparse (and rank-K) matrix U = ∑K k=1 uku > k , where uk corresponds to the k-th (connected or disconnected) class label. Consistent with the above, we represent the pair of vertices (it, jt) as an indicator vector encoding the unique path in T that connects the two vertices. This encoding takes edge orientation into account. For instance, the pair of vertices (6, 7) in Figure 6 (c) is connected in T by path p(6→7) = 6→ 3→ 1→ 2→ 7. According to the direction of traversed edges (edge e1 is traversed according to its orientation, edge e2 in the opposite direction, etc.), path p(6→7) is represented by vector p = (1,−1, 0, 0, 1,−1, 0, 0, 0, 0, 0, 0)> = ((p′t)>|0>m−n+1), hence Qp = p′t = (1,−1, 0, 0, 1,−1)>.9\n8 Though we are only interested in cutsets here, this statement holds more generally for any cut of the graph. 9 Any other path connecting 6 to 7 in G would yield the same representation. For instance, going back to Figure 6 (a), consider path p′(6→7) = 6 → 4 → 7, whose edges are not in T . This gives p′ = (0, 0, 0, 0, 0, 0, 0, 0,−1, 1, 0, 0)>.\nIt is important to observe that computing Qp does not require full knowledge of matrix Q, since Qp only depends on T and the way its edges are traversed. With the above handy, we are ready to prove Theorem 5.\nProof of Theorem 5 For the constructed spanning tree10 T , let uk ∈ {−1, 0, 1}n−1 be the vector of coefficients representing the k-th class label w.r.t. the fundamental cutset matrix Q associated with T , and set U = ∑K k=1 uku > k . Also, let xt be the instance vector computed by the algorithm at time t. Observe that, by the way xt is constructed (see Figure 7 for an illustrative example) we have xt = Qpt = p ′ t for all t, being pt and p ′ t the path vectors alluded at above. For any given class k, we have that u>k xt = u > k p ′ t. Recall that vector uk contains +1 in each component corresponding to an\nYet, Qp′ = Qp = (1,−1, 0, 0, 1,−1)>. This invariance holds in general: Given the pair (i, j), the quantity Qp is independent of p, if we let p vary over all paths in G departing from i and arriving at j. This common value is the (n − 1)-dimensional vector containing the edges in the unique path in T joining i and j (taking traversal directions into account). Said differently, once we are given T , the quantity Qp only depends on i and j, not on the path chosen to connect them. This invariance easily follows from the fact that cuts are orthogonal to circuits, see, e.g., [19, Ch.6].\n10 If less than n − 1 edges end up being revealed, the set of edges maintained by the algorithm cannot form a spanning tree of G. Hence T can be taken to be any spanning tree of G including all the revealed edges.\noutward branch of T , −1 in each component corresponding to an inward branch, and 0 otherwise. We distinguish four cases (see Figure 6 (c), for reference):\n1. it and jt are both in the k-th class. In this case, the path in T that connects it to jt must exit and enter the k-th class the same number of times (possibly zero). Since we only traverse branches, we have in the dot product u>k p ′ t an equal number of +1 terms (corresponding to\ndepartures from the k-th class) and −1 (corresponding to arrivals). Hence u>k p′t = 0. Notice that this applies even when the k-th class is not connected.\n2. it is in the k-th class, but jt is not. In this case, the number of departures from the k-th class should exceed the number of arrivals by exactly one. Hence we must have u>k p ′ t = 1.\n3. it is not in the k-th class, but jt is. By symmetry (swapping it with jt), we have u > k p ′ t = −1.\n4. Neither it nor jt is in the k-th class. Again, we have an equal number of arrival/departures to/from the k-th class (possibly zero), hence u>k p ′ t = 0.\nWe are now in a position to state our linear separability condition. We can write\nvec(U)>vec(Xt) = tr(U >Xt) = tr( K∑ k=1 uku > k xtx > t ) = K∑ k=1 (u>k xt) 2 = K∑ k=1 (u>k p ′ t) 2,\nwhich is 2 if it and jt are in different classes (i.e., it and jt are dissimilar), and 0, otherwise (i.e., it and jt are similar). We have therefore obtained that the label yt associated with (it, jt) is delivered by the following linear-threshold function:\nyt = { 1 if vec(U)>Xt ≥ 1 0 otherwise .\n(9)\nBecause we can interchangeably view vec(·) as vectors or matrices, this opens up the possibility of running any linear-threshold learning algorithm (on either vectors or matrices). For r-norm Perceptrons with the selected norm r and decision threshold, we have a bound on the number M of mistakes of the form [20, 17]\nM = O ( ||vec(U)||21 ||vec(Xt)||2∞ log n ) ,\nwhere\n||vec(U)||1 = ||vec( K∑ k=1 uku > k )||1 = || K∑ k=1 vec(uku > k )||1 ≤ K∑ k=1 ||vec(uku>k )||1 = K∑ k=1 ||uk||21 ,\nand ||vec(Xt)||∞ = ||vec(xtx>t )||∞ = ||xt||2∞ = 1 .\nMoreover, by the way vectors uk are constructed, we have ||uk||1 = |ΦTk |. In turn, |ΦTk | ≤ |ΦGk | holds independent of the connectedness of the k-th cluster. Putting together and upper bounding concludes the proof."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2013,
    "abstractText" : "<lb>We consider online similarity prediction problems over networked data. We begin by relat-<lb>ing this task to the more standard class prediction problem, showing that, given an arbitrary<lb>algorithm for class prediction, we can construct an algorithm for similarity prediction with<lb>“nearly” the same mistake bound, and vice versa. After noticing that this general construction<lb>is computationally infeasible, we target our study to feasible similarity prediction algorithms on<lb>networked data. We initially assume that the network structure is known to the learner. Here<lb>we observe that Matrix Winnow [47] has a near-optimal mistake guarantee, at the price of cubic<lb>prediction time per round. This motivates our effort for an efficient implementation of a Percep-<lb>tron algorithm with a weaker mistake guarantee but with only poly-logarithmic prediction time.<lb>Our focus then turns to the challenging case of networks whose structure is initially unknown<lb>to the learner. In this novel setting, where the network structure is only incrementally revealed,<lb>we obtain a mistake-bounded algorithm with a quadratic prediction time per round.",
    "creator" : "LaTeX with hyperref package"
  }
}