{
  "name" : "1607.07804.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Error-Resilient Machine Learning in Near Threshold Voltage via Classifier Ensemble",
    "authors" : [ "Sai Zhang", "Naresh Shanbhag" ],
    "emails" : [ "szhang12@illinois.edu", "shanbhag@illinois.edu" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Emerging applications such as recognition, mining, synthesis (RMS), rely on computationally intensive machine learning algorithms to extract patterns from complex data. Conventionally, these algorithms are deployed on large scale general-purpose computing platforms such as CPU and GPUbased clusters, leading to significant cost in energy. Machine learning algorithms play an important role in enabling in-situ data analytics employing energy-constrained embedded platforms. This stringent energy constraint precludes the use of general-purpose hardware platforms, resulting in much interest in dedicated integrated circuit implementation of machine learning kernels [4, 11]. These implementations have shown to achieve a 252× energy reduction for convolutional neural networkbased vision system [4] and a 5.2× throughput enhancement for a k-nearest-neighbor (KNN) engine [11] as compared to implementations on general-purpose platforms. Conventionally, energy consumption of machine learning implementations is minimized by reducing the computational complexity, precision, and data movement [16]. Such techniques exploit the algorithms’ intrinsic robustness to numerical errors. This intrinsic robustness can be exploited in another way to further reduce energy - by implementing such kernels on circuit fabrics that operate at the limits of energy efficiency and hence tend to be unreliable. Examples of such circuit fabrics include near threshold voltage (NTV) CMOS [8], and emerging devices such as CNFET [18] and spin [15]. For example, it has been shown that NTV operation achieves up to 10× energy savings, but leads to increased delay variations as large as 14× [8]. Hardware errors are expected to be common place in scaled or beyond CMOS processes [15, 18], and there is great interest in understanding ar X\niv :1\n60 7.\n07 80\n4v 1\n[ cs\n.L G\n] 3\nJ ul\nthe behavior of machine learning architectures in presence of these errors. The most common machine learning architecture is the centralized architecture (see Fig. 1(a)) where a complex block such as the support vector machine (SVM) is employed to process all the input data. However, the computational complexity of centralized architecture increases dramatically as a function of the non-linearity of the decision boundary [17]. The classifier ensemble (CE, see Fig. 1(b)) is a distributed architecture for machine learning which combines several weak (low-complexity) classifiers to form a strong classifier. CE enables on-chip training due to its distributed nature, and exhibits robustness to feature/label noise. Thus, it is of great importance to compare the robustness and energy efficiency of distributed machine learning architectures designed using CE with centralized architectures such as SVM. In this paper, we compare the robustness of distributed and centralized machine learning architectures in presence of timing errors due to NTV operations. Specifically, we compare a CE method - random forest (RF) - with SVM using architectural-level error models [19] in 45 nm CMOS. Employing the breast cancer data set in the UCI machine learning repository [1], we show that RF achieves a detection accuracy (Pdet) that varies by 3.2% while maintaining a median Pdet ≥ 0.9 when operating with a gate level delay variation of 28.9%. This is 5× lower as compared to SVM. We further propose a new error weighted voting to enhance the robustness of RF by employing the timing error statistics of the NTV circuit fabric. Simulation results confirm that the proposed method leads to a Pdet that varies by only 1.4%, which is 12× lower compared to SVM. The rest of the paper is organized as follows. Section 2 provides the background for CE, SVM, and the architectural level error models. Section 3 describes architectures for RF and SVM classifiers. Section 4 presents simulation results validating the error models in a 45 nm CMOS process, and employs these models to compare the detection accuracy of SVM, RF, and proposed RF with error weighted voting scheme. Conclusions are presented in Section 5."
    }, {
      "heading" : "2 Background",
      "text" : ""
    }, {
      "heading" : "2.1 Classifier Ensemble (CE)",
      "text" : "Classifier ensemble (also referred to as Multiple Classifier System) has been employed to enhance the performance of single classifier system [6]. A wide variety of CE methods exist. In bootstrap aggregating (bagging) [2], multiple training sets are generated from the original training set via random sampling with replacement, in order to train multiple classifiers. Adaboost [9] is another popular method for ensemble generation. The training samples are re-weighted after each iteration so that the mis-classified samples get higher weights. Other methods such as randomness injection, random subspace [10] and output coding [6] also exist. RF is a CE method that combines random subspace and bagging, while employing an ensemble of decision trees (DTs) as weak classifiers. It is a popular technique for classification, prediction, variable selection, and has shown superior results compared to other linear and non-linear predictive modeling techniques [3]. Advantages include parallel training, robustness to overfitting, ease of design, the capability of getting out-of-bag (OOB) error estimate, and others. In RF, the training set for each individual DT is generated using bagging. During the training of each DT, a random subset of features are selected, and the best feature is selected to split the DT according to an appropriate criterion. Several variations of RF exists based on the type of DT used as\nbase classifiers. Classification and regression tree (CART) [3] employs the Gini index as a measure of the impurity of nodes. ID3 [14] employs information gain as the criterion. C4.5 [14] improves ID3 by using the information gain ratio."
    }, {
      "heading" : "2.2 Support Vector Machine",
      "text" : "Support vector machine (SVM) [5] is a popular supervised learning method for classification and regression. SVM operates by first training a model (the training phase) followed by test/classification (the test phase). During the training phase, labeled feature vectors are used to train a model. During the test phase, SVM produces a predictive label when provided with a new (test) feature vector. SVM training can be formulated as the solution to the following optimization problem:\nmin 12‖w‖ 2 + C ∑ i ξi s.t. ci(w\nTxi−b) ≥ 1−ξi ξi ≥ 0\nwhere C is the cost factor, ξi is the soft margin, xi is the feature vector, ci is the label corresponding to the feature vector xi, w is the weight vector, and b is the bias. It can be shown that the optimum weights wo are represented as a linear combination of the feature vectors that lie on the margins, i.e., support vectors. Kernel trick [5] can be employed to realize non-linear decision boundaries."
    }, {
      "heading" : "2.3 Computational Error Model",
      "text" : "Timing errors occur in data-path circuits whenever a storage element captures an incorrect logic value at its input. The probability of such an error event increases dramatically when circuits are operating in NTV [8], i.e., supply voltages 0.3 V ≤ Vdd ≤ 0.7 V. The resulting timing errors are a complex function of the circuit state, inputs, architecture, and the process technology. A statistical model of such errors is therefore essential in order to understand the impact of errors on system performance.\nIn this paper, the computed outputs and timing errors are treated as random variables (RVs). We employ capital letters and small letters to denote a RV Y and its realization y, respectively. We employ the following additive error model [19]:\nya = yo ⊕ η (1) where ya = [yba,0, ....y b a,B−1]\nT is the B-bit observed (erroneous) output of a pipeline stage which is also a realization of the RV vector Ya = [Y ba,0, ....Y b a,B−1] T , yo = [ybo,0, ....y b o,B−1]\nT is the ideal (error-free) output which is also a realization of the RV vector Yo = [Y bo,0, ....Y b o,B−1]\nT , and η = [η0b, ....ηB−1b]T is the timing error vector which is also a realization of the RV vector N = [N b0 , ....NB−1\nb]T , ⊕ is the element-wise addition in Galois Field over 2 (GF(2)), and yba,i, y b o,i, η b i ∈ {0, 1} (i = 0, . . . , B − 1) are the B bits of ya, yo, and η, respectively. During the error modeling phase, samples of the timing error η are obtained via HDL simulations. Using these samples, we estimate the parameters of the error probability mass function (PMF) P (η) for use in system simulations. Since the logic errors are bit-level events that are made dependent by the structure of the logic network, the error bits follow a joint Bernoulli distribution. The procedure to obtain an analytical model for P (η) begins by determining a latent Gaussian RV U = [U0, ..., UB−1]T with mean vector µu and covariance matrix Cu which are chosen such that P (η) can be expressed as:\nP (η) = Φ([0, ...0]T ;Dµu,DCuD T )\nwhere\nD =   (−1)η b 0 0 0 0 . . . 0\n0 0 (−1)η b B−1\n \nand Φ(u;µu,Cu) is the cumulative distribution function of a multivariate Gaussian with mean vector µu and covariance matrix Cu. The mean vector µu and covariance matrix Cu are estimated from error samples obtained via HDL simulations.\nDuring system simulations, we employ the following procedure to perform error injection: 1) generate samples u = [u1, . . . , uB−1]T of RV U , 2) employ the dichotomized Gaussian (DG) approximation [13] to generate ηbi from ui as follows:\nηbi = { 1 ui ≥ 0 0 ui < 0 for (i = 0, 1..., B − 1) (2)\nand 3) perform error injection by employing (1). In this paper, the error model P (η) and the error injection procedure above are employed in system simulations in Section 4."
    }, {
      "heading" : "3 System Architecture",
      "text" : "In this section, we present system architectures for RF and SVM classifiers. The RF classifier is chosen as the implemented CE architecture due to its comparison based architecture which results in simple base learner architecture. The polynomial SVM is chosen as the implemented centralized architecture as it offers a good trade-off between decision boundary flexibility and hardware complexity [12]."
    }, {
      "heading" : "3.1 The RF Architecture",
      "text" : "The RF classifier is implemented using an ensemble of L two-stage DT classifiers (weak learners) shown in Fig. 2(a). The lth DT is trained from a bootstrapped training set Sl obtained from the original training set S , and processes the Ml-dimensional data vector xl = [xl,1, xl,2, . . . , xl,Ml ]T obtained from the M -dimensional test data vector x (M Ml). Stage 1 of the lth DT consists of a comparator array that computes sgn(xl,i−Tl,i) (l = 1, 2, . . . , L, and i = 1, 2, . . . ,Ml) where Tl,is are the thresholds obtained via training. Stage 2 consists of a look up table (LUT) which encodes the decision of each root-to-leaf path into a 1-bit output ya,l ∈ {0, 1}. The outputs of the L DTs are combined via a voter block to generate the final decision. Each DT is trained using the Gini index [3] as the training criterion.\nConventionally, a majority voter is employed to combine the outputs from all DTs as follows:\nŷa = maj(ya,1, ya,2, ..., ya,L)\nwhere ŷa ∈ {0, 1} is the majority voter output, and ya,l is the lth DT output given by: ya,l = yo,l ⊕ ηl\nwhere yo,l ∈ {0, 1} is the error-free output and ηl ∈ {0, 1} is the timing error of the lth DT. The RF with majority voter is denoted as RF-M. In case of binary classification, the majority voter can be implemented as shown in Fig. 2(b).\nIn order to enhance the robustness of RF in presence of timing errors, we propose an error weighted voting scheme where the timing error statistics are incorporated during the decision process. In order to do so, we employ the maximum-a-posterior (MAP) criterion, i.e.:\nŷa = arg max ∀c∈C\nP (c|x) (3)\nwhere C is the label set, P (c|x) is the posterior probability of class label c conditioned on the test data x. Thus:\nP (c|x) = L∑\nl=1\nP (c|Rl,x)P (Rl|x) (4)\n= L∑\nl=1\nP (c|Rl,x)P (Rl) (5)\n≈ L∑\nl=1\n1{ya,l = c}pl (6)\nwhere P (c|Rl,x) denotes the posterior probability of the class label, Rl is the event of the lth DT being correct during the training phase, pl = P (Rl) is the probability of the event Rl, and 1{·} denotes the indicator function. Equation (4) implies (5) because the test data x and event Rl are independent, and (5) implies (6) because we assume the DT output has a probability mass of 1 at the selected class label. The final decision ŷa is obtained from (3) by choosing the label c that maximizes (6). Note that pl represents the decision accuracy of the lth DT in presence of timing errors.\nIn the case of binary classification, one can simplify (3) using (6) into:\nŷa =\n{ 1 if ∑L l=1 1{ya,l = 1}p′l > 12\n0 otherwise\nwhere p′l = pl∑L l=1 pl and the voter can be implemented as shown in Fig. 2(c).\nTo incorporate the timing error statistics of each DT, we express pl in (6) as follows:\npl = 1∑\nηl=0\nP (Rl, ηl) = 1∑\nηl=0\nP (Rl|ηl)P (ηl) (7)\nwhere P (Rl|ηl) is the probability of correct decision of the lth DT conditioned on ηl. The probabilities P (Rl|ηl) and P (ηl) can be obtained during the training phase for each DT. For a RF binary classifier, (7) can be simplified from the theorem of total probability as follows:\npl = P (Rl|ηl = 0)P (ηl = 0) + P (Rl|ηl = 1)P (ηl 6= 0) = P (Rl|ηl = 0)(1− pηl) + P (Rl|ηl = 1)pηl (8)\nIn binary classification, the erroneous output of the lth DT can be expressed as:\nya,l = c⊕ ηl ⊕ el (9)\nwhere c, ηl, el denote the true label, timing error, and error due to noise in data, respectively. Thus, the event Rl = {el ⊕ ηl = 0}, and we have:\nP (Rl|ηl = 1) = P (el ⊕ ηl = 0|ηl = 1) = P (el = 1|ηl = 1) (10) = P (el = 1) (11) = P (el = 1|ηl = 0) = P (el ⊕ ηl = 1|ηl = 0) = 1− P (el ⊕ ηl = 0|ηl = 0) = 1− P (Rl|ηl = 0) (12)\nwhere (10) to (11) comes from the independence of el and ηl. Substituting (12) into (8) leads to: pl = P (Rl|ηl = 0)(1− pηl) + (1− P (Rl|ηl = 0))pηl (13) where P (Rl|ηl = 0) can be obtained via performing validation using out-of-bag samples, and pηl = P (ηl 6= 0) is the error rate of the lth DT. As indicated in 13, the error weighted voting decreases the weight of the lth DT when its error rate increases. We denote RF with error weighted voting scheme as RF-EW.\nWhen error rate pηl = 0, the error weighted voting scheme reduces to the conventional weighted voter [6] where pl = P (Rl|ηl = 0). The RF with conventional weighted voter is denoted as RF-W. The performance of RF-EW improves when the DTs exhibit uncorrelated errors, i.e., the DT outputs exhibit diversity in terms of error statistics. It is possible to enhance DT diversity by designing each DT to have different: 1) algorithm (algorithmic diversity), 2) architecture (architectural diversity), and 3) data-path precision (precisional diversity), across the DT ensemble. Precision has a significant impact on the timing error statistics since the hardware errors under investigation are due to timing violations. Therefore, in this paper, the precision of each DT data-path in the RF-EW is randomly assigned uniformly between 4b and 8b, leading to different critical path delays among the DTs, and hence uncorrelated errors."
    }, {
      "heading" : "3.2 The SVM Architecture",
      "text" : "The centralized machine learning algorithm employed in this paper is a second-order polynomial kernel SVM described as:\nŷa = sgn(ya)\nya =\nN∑\ni=1\n(βsTi x + γ) 2 αi + b (14)\nwhere x = [x1, x2, ..., xM ]T is the M dimensional test data vector, si = [s1, s2, ..., sM ]T is the ith support vector, αi is the weight associated with si, b is the bias, β and γ are parameters of the polynomial kernel, andN is the total number of support vectors (typicallyN M ). Direct computation of (14) requiresO(NM) multiply-accumulate (MAC) operations. The following reformulation [12] reduces the number of MAC operations to O(M2):\nya = x̃ TW̃x̃ + b (15)\nW̃ =\nN∑\ni=1\nαis̃is̃i T\nwhere W̃ is a precomputed weight matrix, x̃ = [\n1 x\n] , and s̃i = [ γ βsi ] . Figure 3 shows a folded\nSVM architecture implementing (15) where Stage 1 computes W̃x̃, and Stage 2 computes the dot product between x̃ and Stage 1 output, and adds the bias term b."
    }, {
      "heading" : "3.3 System Analysis",
      "text" : "The potential robustness improvement achieved by RF can be analyzed by inspecting the generalized error E [( C − 1L ∑L l=1 Ŷa,l )2] where C is the label and 1L ∑L l=1 Ŷa,l is the RF output where equal\nweights in the voter is assumed for simplicity of analysis. Here the expectation is taken over the distribution of the label C, the training set S, and the timing error N1, ..., NL. We start by deriving the generalized error for a single DT defined as E[(C − Ŷa)2] where Ŷa is the DT output. It can be shown that (see Appendix A):\nE [( C − Ŷa )2] = σ2C + b\n2 + σ2 Ŷa\n(16)\nwhere σ2C = E [( C−E[C] )2] is the irreducible error (noise), b2 = (E[C]−E[Ŷa])2 is the bias term and σ2 Ŷa = E [( E[Ŷa]−Ŷa )2] is the variance of Ŷa. Such a decomposition identifies the contribution of different error sources and allows one to understand the effect of CE in reducing these errors.\nFor CE, it can further be shown that the noise σ2C,RF and the bias b 2 RF (corresponding to the first two terms in (16)) do not change, i.e., σ2C,RF = σ 2 C and b 2 RF = b\n2, respectively. However, the output variance σ2RF can be expressed as (see Appendix B):\nσ2RF = 1\nL σ2 Ŷa\n(17)\nWe can see from (17) that σ2RF is reduced by a factor of 1 L from σ 2 Ŷa , and that for the RF to achieve a lower variance, σ2\nŶa should be less thanL times the variance of the centralized system. The reduction\nof variance leads to reduced generalized error and mis-classification rate."
    }, {
      "heading" : "4 Simulation Results",
      "text" : "This section begins with the validation of the timing error model of Section 2.3 in a commercial 45 nm CMOS process. These timing error models are derived for the RF and SVM classifiers of Fig. 2(a) and Fig. 3, respectively. Next, in Section 4.2, the detection accuracy of the SVM and RF architectures are compared using the validated error models. We employ the Breast Cancer Wisconsin dataset from UCI machine learning repository [1] which consists of labeled feature vectors (benign vs. malignant) constructed from digitized images of fine needle aspirates (FNA) of patient tissue. The SVM architecture being considered in this study consists of two types of MACs: Stage 1 employs 8 b input, 8 b coefficient, and Stage 2 employs 10 b input, 8 b coefficient MACs. The conventional RF-M and RF-W have Stage 1 consisting of comparator arrays with 8 b input and 8 b thresholds, and LUTs implemented as logic networks during the architecture generation. In the proposed RF-EW, each DT is implemented using a randomly selected precision uniformly distributed between 4 b and 8 b for both the input and the thresholds. These precisions were chosen to obtain less than 0.5% degradation in Pdet compared to a floating point implementation. The RF is trained by choosing randomly 3 features per node, and stopping the tree growth when the current node is pure or contains less than or equal to 2 samples. We do not restrict the depth of the tree. The complexities of the SVM and RF (with ensemble size L = 10) were found to be 1.63K and 1.47K 2-input NAND gate equivalents, respectively. In all cases, the parameters of SVM and RF were trained assuming no timing errors. During simulation, 50% of the data is employed during the training and testing, respectively."
    }, {
      "heading" : "4.1 Model Generation Methodology and Validation",
      "text" : "The error model generation and validation methodology is shown in Fig. 4(a), and described below:\n1. Characterize the gate delay distribution vs. operating voltage Vdd of basic gates using HSPICE in NTV range 0.3 V-0.7 V.\n2. Implement the SVM and RF architectures shown in Fig. 3 and Fig. 2(a), respectively, using structural Verilog HDL using the basic gates characterized in Step 1.\n3. Emulate process variations at NTV by generating multiple (30) architectural instances of each type (SVM and RF) and assigning random gate delays obtained via sampling the gate delay distributions obtained in Step 1. Note that the presence of process variation makes the detection accuracy pdet = P (Ŷa = c) a RV, which we denote as Pdet.\n4. Run HDL (bit and clock accurate) simulations of each instance using a characterization dataset to obtain error samples η and classification accuracy Pdet−h for the two architectures. The characterization dataset is obtained via sampling with replacement from the application level data to emulate the input statistics.\n5. Generate the error PMF P (η) employing the procedure described in Section 2.3 [19]. 6. Run fixed-point MATLAB simulations using P (η) to inject errors for both SVM and RF\nusing the UCI dataset to obtain detection accuracy Pdet−s. Compare Pdet−s with Pdet−h.\nFigure 4(b) plots the SVM detection accuracy Pdet−h obtained in Step 4 (HDL simulations using gate delay distributions) and Pdet−s obtained in Step 6 (MATLAB simulations using P (η)) as a function of gate-level delay variation (σ/µ)d. We find that the median Pdet−h (p̄det−h) and Pdet−s (p̄det−s) differs by no more than 5% when (σ/µ)d varies between 2.8% and 33%. Figure 4(b) also shows that the variation of Pdet−h increases as (σ/µ)d increases from 2.8% to 29%, and then reduces because all the instances fail to perform correct classification for further increases in (σ/µ)d. The variation in Pdet−h is also modeled accurately as the maximum and minimum values of Pdet−h and Pdet−s differ by no more than 3% and 5%, respectively. Similar results were obtained for the RF architecture as well. These results indicate that the timing error is well-modeled by its PMF P (η), and that the system performance can be accurately estimated by employing the methodology in Section 4.1."
    }, {
      "heading" : "4.2 Comparison of SVM and RF",
      "text" : ""
    }, {
      "heading" : "4.2.1 Comparison of timing error rates",
      "text" : "We first compare the timing error rates pη = P (η 6= 0) of SVM and RF obtained via HDL simulations as the voltage decreases in NTV. Figure 5(a) shows that the median timing error rate p̄η\nincreases by 500× from 2.1 × 10−3 to 0.99, and from 1.1 × 10−3 to 0.61 for SVM and RF, respectively, as the voltage Vdd decreases from 0.7 V to 0.3 V, indicating that the RF architecture has up to 4.5× lower timing error rate compared with SVM. The error rate of RF architecture is lower because it has comparator blocks which have a much simpler data path compared with the MAC units in SVM. Figure 5(a) also demonstrates that the gate level delay variation (σ/µ)d increases by 12× from 2.8% to 33% as the voltage Vdd decreases from 0.7 V to 0.3 V. Next, we employ P (η) to inject errors in fixed-point MATLAB simulations of SVM and RF architectures to compare their robustness to timing errors in NTV. All comparisons henceforth are in terms of Pdet−s. Hence, we simplify the subscript and denote the detection accuracy as Pdet. Four architectures are compared: 1) SVM, 2) RF with majority voter [3] (RF-M), 3) RF with weighted majority voter [6] (RF-W), and 4) RF with the proposed error weighted voter (RF-EW). We will compare the four architectures in terms of median (p̄det) and standard deviation (σpdet ) of detection accuracy Pdet."
    }, {
      "heading" : "4.2.2 Comparison of p̄det",
      "text" : "Figure 5(b) shows that RF has higher p̄det than SVM when the ensemble size L is sufficiently large. Specifically, RF-M is able to maintain p̄det ≥ 0.9 for (σ/µ)d ≤ 28.9% with L = 10, whereas SVM can only maintain the same performance for (σ/µ)d ≤ 11.7%. Additionally, RF-EW achieves up to 3% higher p̄det compared with RF-W and RF-EW, and is able to maintain p̄det ≥ 0.9 for (σ/µ)d ≤ 29.6%. Finally, Figure 5(b) further shows that RF with L = 10 is able to maintain p̄det ≥ 0.9 even at (σ/µ)d of 28.9%. This indicates that RF architectures have a higher robustness to timing errors compared with SVM in spite of its complexity being lower by 10% when L = 10."
    }, {
      "heading" : "4.2.3 Comparison of σpdet",
      "text" : "Figure 5(c) shows that σpdet is significantly reduced as L increases. RF-M achieves σpdet ≤ 3.5× 10−4 when L = 10, which is 5X lower compared to SVM or RF-M with L = 1. This further demonstrates that distributed architectures are inherently more robust to timing errors than centralized ones. Figure 5(c) also shows that RF-EW achieves σpdet ≤ 1.4×10−2 when (σ/µ)d ≤ 29.6%, which is 12× and 3.5× lower compared to SVM and RF-W, respectively. This demonstrates that incorporating timing error statistics into the decision making process enhances robustness. When (σ/µ)d ≥ 30%, σpdet of RF-EW is higher than that of RF-M and RF-W because all instances of RF-M and RF-W achieve a low Pdet ≈ 0.6, whereas some instances of RF-EW can still achieve a Pdet ≥ 0.9, leading to increased σpdet . To understand the robustness improvement achi-eved by RF, Fig. 6 shows that the RF output variance σ2RF reduces from 0.16 to 0.02 as L increases from 1 to 25 when no precision diversity is employed. The variance reduction is more significant when the ensemble size L is small, and slows down as L further increases. This is because the independence assumption across the DTs is violated for large L. Figure 6 also shows that σ2RF can be further reduced to 0.01 due to more uncorrelated error statistics when precision diversity is employed as in the RF-EW. As shown in (16), the reduction of variance leads to lower generalized error and higher Pdet."
    }, {
      "heading" : "5 Conclusion",
      "text" : "In this paper, the inherent robustness of CE and centralized machine learning architectures in presence of timing violations is compared. It is shown that distributed architectures employing CE are inherently more robust than centralized ones to timing errors. Furthermore, it is shown that the algorithm itself can be adapted to further enhance the robustness. Such enhancement is achieved by using error weighted voting during the decision combination, and employing precision diversity in the architecture data path. The results demonstrate that in the CE framework, architectural level information can be incorporated at the system level to achieve enhanced robustness. In the future, architectural and algorithmic level diversity techniques can be employed to improve the robustness of CE. In addition, the robustness of CE in presence of defects errors (stuck-at-faults) can also be evaluated."
    }, {
      "heading" : "Acknowledgment",
      "text" : "This work was supported in part by Systems on Nanoscale Information fabriCs (SONIC), one of the six SRC STARnet Centers, sponsored by MARCO and DARPA."
    } ],
    "references" : [ {
      "title" : "Bagging predictors",
      "author" : [ "L. Breiman" ],
      "venue" : "Mach. Learn., 24(2):123–140, Aug.",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 1996
    }, {
      "title" : "Random forests",
      "author" : [ "L. Breiman" ],
      "venue" : "Mach. Learn., 45(1):5–32, Oct.",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "eyeriss: An energy-efficient reconfigurable accelerator for deep convolutional neural networks",
      "author" : [ "Y.H. Chen", "T. Krishna", "J. Emer", "V. Sze" ],
      "venue" : "ISSCC",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2016
    }, {
      "title" : "Support-vector networks",
      "author" : [ "C. Cortes", "V. Vapnik" ],
      "venue" : "Mach. Learn., 20(3):273–297, Sept.",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 1995
    }, {
      "title" : "Ensemble methods in machine learning",
      "author" : [ "T.G. Dietterich" ],
      "venue" : "Proceedings of the First International Workshop on Multiple Classifier Systems, MCS ’00, pages 1–15, London, UK, UK,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "A unified bias-variance decomposition and its applications",
      "author" : [ "P. Domingos" ],
      "venue" : "In proc. 17th International Conf. on Machine Learning, pages 231–238. Morgan Kaufmann,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "Near-threshold computing: Reclaiming moore’s law through energy efficient integrated circuits",
      "author" : [ "R. Dreslinski", "M. Wieckowski", "D. Blaauw", "D. Sylvester", "T. Mudge" ],
      "venue" : "Proceedings of the IEEE, 98(2):253–266, Feb",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Experiments with a new boosting algorithm",
      "author" : [ "Y. Freund", "R.E. Schapire" ],
      "venue" : "L. Saitta, editor, Proceedings of the Thirteenth International Conference on Machine Learning (ICML 1996), pages 148–156. Morgan Kaufmann,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 1996
    }, {
      "title" : "The random subspace method for constructing decision forests",
      "author" : [ "T.K. Ho" ],
      "venue" : "IEEE Trans. Pattern Anal. Mach. Intell., 20(8):832–844, Aug.",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "A 21.5M-query-vectors/s 3.37nJ/vector reconfigurable k-nearestneighbor accelerator with adaptive precision in 14nm tri-gate CMOS",
      "author" : [ "H. Kaul", "M.A. Anders", "S.K. Mathew", "G. Chen", "S.K. Satpathy", "S.K. Hsu", "A. Agarwal", "R.K. Krishnamurthy" ],
      "venue" : null,
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2016
    }, {
      "title" : "Low-energy formulations of support vector machine kernel functions for biomedical sensor applications",
      "author" : [ "K. Lee", "S. Kung", "N. Verma" ],
      "venue" : "Signal Processing Systems, 69(3):339–349,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Generating spike trains with specified correlation coefficients",
      "author" : [ "J.H. Macke", "P. Berens", "A.S. Ecker", "A.S. Tolias", "M. Bethge" ],
      "venue" : "Neural Computation, 21(2):397–423, Feb",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Programs for Machine Learning",
      "author" : [ "J.R. Quinlan" ],
      "venue" : null,
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 1993
    }, {
      "title" : "Beyond charge-based computation: Boolean and non-boolean computing with spin torque devices",
      "author" : [ "K. Roy", "M. Sharad", "D. Fan", "K. Yogendra" ],
      "venue" : "Low Power Electronics and Design (ISLPED), 2013 IEEE International Symposium on, Sept",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Approximate computing and the quest for computing efficiency",
      "author" : [ "S. Venkataramani", "S.T. Chakradhar", "K. Roy", "A. Raghunathan" ],
      "venue" : "2015 52nd ACM/EDAC/IEEE Design Automation Conference (DAC), pages 1–6, June",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Enabling system-level platform resilience through embedded data-driven inference capabilities in electronic devices",
      "author" : [ "N. Verma", "K.H. Lee", "K.J. Jang", "A. Shoeb" ],
      "venue" : "2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 5285–5288, March",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Carbon nanotube circuits: Opportunities and challenges",
      "author" : [ "H. Wei", "M. Shulaker" ],
      "venue" : "DATE",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2013
    }, {
      "title" : "Probabilistic error models for machine learning kernels implemented on stochastic nanoscale fabrics",
      "author" : [ "S. Zhang", "N. Shanbhag" ],
      "venue" : "Design, Automation Test in Europe Conference Exhibition (DATE),",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 2,
      "context" : "This stringent energy constraint precludes the use of general-purpose hardware platforms, resulting in much interest in dedicated integrated circuit implementation of machine learning kernels [4, 11].",
      "startOffset" : 192,
      "endOffset" : 199
    }, {
      "referenceID" : 9,
      "context" : "This stringent energy constraint precludes the use of general-purpose hardware platforms, resulting in much interest in dedicated integrated circuit implementation of machine learning kernels [4, 11].",
      "startOffset" : 192,
      "endOffset" : 199
    }, {
      "referenceID" : 2,
      "context" : "These implementations have shown to achieve a 252× energy reduction for convolutional neural networkbased vision system [4] and a 5.",
      "startOffset" : 120,
      "endOffset" : 123
    }, {
      "referenceID" : 9,
      "context" : "2× throughput enhancement for a k-nearest-neighbor (KNN) engine [11] as compared to implementations on general-purpose platforms.",
      "startOffset" : 64,
      "endOffset" : 68
    }, {
      "referenceID" : 14,
      "context" : "Conventionally, energy consumption of machine learning implementations is minimized by reducing the computational complexity, precision, and data movement [16].",
      "startOffset" : 155,
      "endOffset" : 159
    }, {
      "referenceID" : 6,
      "context" : "Examples of such circuit fabrics include near threshold voltage (NTV) CMOS [8], and emerging devices such as CNFET [18] and spin [15].",
      "startOffset" : 75,
      "endOffset" : 78
    }, {
      "referenceID" : 16,
      "context" : "Examples of such circuit fabrics include near threshold voltage (NTV) CMOS [8], and emerging devices such as CNFET [18] and spin [15].",
      "startOffset" : 115,
      "endOffset" : 119
    }, {
      "referenceID" : 13,
      "context" : "Examples of such circuit fabrics include near threshold voltage (NTV) CMOS [8], and emerging devices such as CNFET [18] and spin [15].",
      "startOffset" : 129,
      "endOffset" : 133
    }, {
      "referenceID" : 6,
      "context" : "For example, it has been shown that NTV operation achieves up to 10× energy savings, but leads to increased delay variations as large as 14× [8].",
      "startOffset" : 141,
      "endOffset" : 144
    }, {
      "referenceID" : 13,
      "context" : "Hardware errors are expected to be common place in scaled or beyond CMOS processes [15, 18], and there is great interest in understanding ar X iv :1 60 7.",
      "startOffset" : 83,
      "endOffset" : 91
    }, {
      "referenceID" : 16,
      "context" : "Hardware errors are expected to be common place in scaled or beyond CMOS processes [15, 18], and there is great interest in understanding ar X iv :1 60 7.",
      "startOffset" : 83,
      "endOffset" : 91
    }, {
      "referenceID" : 15,
      "context" : "However, the computational complexity of centralized architecture increases dramatically as a function of the non-linearity of the decision boundary [17].",
      "startOffset" : 149,
      "endOffset" : 153
    }, {
      "referenceID" : 17,
      "context" : "Specifically, we compare a CE method random forest (RF) - with SVM using architectural-level error models [19] in 45 nm CMOS.",
      "startOffset" : 106,
      "endOffset" : 110
    }, {
      "referenceID" : 4,
      "context" : "Classifier ensemble (also referred to as Multiple Classifier System) has been employed to enhance the performance of single classifier system [6].",
      "startOffset" : 142,
      "endOffset" : 145
    }, {
      "referenceID" : 0,
      "context" : "In bootstrap aggregating (bagging) [2], multiple training sets are generated from the original training set via random sampling with replacement, in order to train multiple classifiers.",
      "startOffset" : 35,
      "endOffset" : 38
    }, {
      "referenceID" : 7,
      "context" : "Adaboost [9] is another popular method for ensemble generation.",
      "startOffset" : 9,
      "endOffset" : 12
    }, {
      "referenceID" : 8,
      "context" : "Other methods such as randomness injection, random subspace [10] and output coding [6] also exist.",
      "startOffset" : 60,
      "endOffset" : 64
    }, {
      "referenceID" : 4,
      "context" : "Other methods such as randomness injection, random subspace [10] and output coding [6] also exist.",
      "startOffset" : 83,
      "endOffset" : 86
    }, {
      "referenceID" : 1,
      "context" : "It is a popular technique for classification, prediction, variable selection, and has shown superior results compared to other linear and non-linear predictive modeling techniques [3].",
      "startOffset" : 180,
      "endOffset" : 183
    }, {
      "referenceID" : 1,
      "context" : "Classification and regression tree (CART) [3] employs the Gini index as a measure of the impurity of nodes.",
      "startOffset" : 42,
      "endOffset" : 45
    }, {
      "referenceID" : 12,
      "context" : "ID3 [14] employs information gain as the criterion.",
      "startOffset" : 4,
      "endOffset" : 8
    }, {
      "referenceID" : 12,
      "context" : "5 [14] improves ID3 by using the information gain ratio.",
      "startOffset" : 2,
      "endOffset" : 6
    }, {
      "referenceID" : 3,
      "context" : "Support vector machine (SVM) [5] is a popular supervised learning method for classification and regression.",
      "startOffset" : 29,
      "endOffset" : 32
    }, {
      "referenceID" : 3,
      "context" : "Kernel trick [5] can be employed to realize non-linear decision boundaries.",
      "startOffset" : 13,
      "endOffset" : 16
    }, {
      "referenceID" : 6,
      "context" : "The probability of such an error event increases dramatically when circuits are operating in NTV [8], i.",
      "startOffset" : 97,
      "endOffset" : 100
    }, {
      "referenceID" : 17,
      "context" : "We employ the following additive error model [19]: ya = yo ⊕ η (1) where ya = [y a,0, .",
      "startOffset" : 45,
      "endOffset" : 49
    }, {
      "referenceID" : 11,
      "context" : ", uB−1] of RV U , 2) employ the dichotomized Gaussian (DG) approximation [13] to generate η i from ui as follows:",
      "startOffset" : 73,
      "endOffset" : 77
    }, {
      "referenceID" : 10,
      "context" : "The polynomial SVM is chosen as the implemented centralized architecture as it offers a good trade-off between decision boundary flexibility and hardware complexity [12].",
      "startOffset" : 165,
      "endOffset" : 169
    }, {
      "referenceID" : 1,
      "context" : "Each DT is trained using the Gini index [3] as the training criterion.",
      "startOffset" : 40,
      "endOffset" : 43
    }, {
      "referenceID" : 4,
      "context" : "When error rate pηl = 0, the error weighted voting scheme reduces to the conventional weighted voter [6] where pl = P (Rl|ηl = 0).",
      "startOffset" : 101,
      "endOffset" : 104
    }, {
      "referenceID" : 10,
      "context" : "The following reformulation [12] reduces the number of MAC operations to O(M): ya = x̃ W̃x̃ + b (15)",
      "startOffset" : 28,
      "endOffset" : 32
    }, {
      "referenceID" : 17,
      "context" : "3 [19].",
      "startOffset" : 2,
      "endOffset" : 6
    }, {
      "referenceID" : 1,
      "context" : "Four architectures are compared: 1) SVM, 2) RF with majority voter [3] (RF-M), 3) RF with weighted majority voter [6] (RF-W), and 4) RF with the proposed error weighted voter (RF-EW).",
      "startOffset" : 67,
      "endOffset" : 70
    }, {
      "referenceID" : 4,
      "context" : "Four architectures are compared: 1) SVM, 2) RF with majority voter [3] (RF-M), 3) RF with weighted majority voter [6] (RF-W), and 4) RF with the proposed error weighted voter (RF-EW).",
      "startOffset" : 114,
      "endOffset" : 117
    } ],
    "year" : 2016,
    "abstractText" : "In this paper, we present the design of error-resilient machine learning architectures by employing a distributed machine learning framework referred to as classifier ensemble (CE). CE combines several simple classifiers to obtain a strong one. In contrast, centralized machine learning employs a single complex block. We compare the random forest (RF) and the support vector machine (SVM), which are representative techniques from the CE and centralized frameworks, respectively. Employing the dataset from UCI machine learning repository and architecturallevel error models in a commercial 45 nm CMOS process, it is demonstrated that RF-based architectures are significantly more robust than SVM architectures in presence of timing errors due to process variations in near-threshold voltage (NTV) regions (0.3 V 0.7 V). In particular, the RF architecture exhibits a detection accuracy (Pdet) that varies by 3.2% while maintaining a median Pdet ≥ 0.9 at a gate level delay variation of 28.9% . In comparison, SVM exhibits a Pdet that varies by 16.8%. Additionally, we propose an error weighted voting technique that incorporates the timing error statistics of the NTV circuit fabric to further enhance robustness. Simulation results confirm that the error weighted voting achieves a Pdet that varies by only 1.4%, which is 12× lower compared to SVM.",
    "creator" : "LaTeX with hyperref package"
  }
}