{
  "name" : "1008.5325.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ ],
    "emails" : [ "bickson@cs.cmu.edu", "guestrin@cs.cmu.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n00 8.\n53 25\nv1 [\ncs .L\nG ]\n3 1\nHeavy-tailed distributions naturally occur in many real life problems. Unfortunately, it is typically not possible to compute inference in closed-form in graphical models which involve such heavy-tailed distributions.\nIn this work, we propose a novel simple linear graphical model for independent latent random variables, called linear characteristic model (LCM), defined in the characteristic function domain. Using stable distributions, a heavy-tailed family of distributions which is a generalization of Cauchy, Lévy and Gaussian distributions, we show for the first time, how to compute both exact and approximate inference in such a linear multivariate graphical model. LCMs are not limited to stable distributions, in fact LCMs are always defined for any random variables (discrete, continuous or a mixture of both).\nWe provide a realistic problem from the field of computer networks to demonstrate the applicability of our construction. Other potential application is iterative decoding of linear channels with non-Gaussian noise."
    }, {
      "heading" : "1 Introduction",
      "text" : "Heavy-tailed distributions naturally occur in many real life phenomena, for example in computer networks [16, 8, 10]. Typically, a small set of machines are responsible for a large fraction of the consumed network bandwidth. Equivalently, a small set of users generate a large fraction of the network traffic. Another common property of communication networks is that network traffic tends to be linear [3, 16]. Linearity is explained by the fact that the total incoming traffic at a node is composed from the sum of distinct incoming flows.\nRecently, several works propose to use linear multivariate statistical methods for monitoring network health, performance analysis or intrusion detection [9, 7, 10, 8]. Some of the aspects of network traffic makes the task of modeling it using a probabilistic graphical models challenging. In many cases, the underlying heavy-tailed distributions are difficult to work with analytically. That is why existing solutions in the area of network monitoring involve various approximations of the joint probability distribution function using a variety of techniques: mixtures of distributions [3], spectral decomposition [7] historgrams [8], sketches [10], entropy [8], sampled moments [16], etc.\nIn the current work, we propose a novel linear probabilistic graphical model called linear characteristic model (LCM) to model linear interactions of independent heavy-tailed random variables (Section 3). Using the stable family of distributions (defined in Section 2), a family of heavy-tailed distributions, we show how to compute both exact and approximate inference (Section 4). Using real data from the domain of computer networks we demonstrate the applicability of our proposed methods for computing inference in LCM (Section 5).\nWe summarize our contributions below:\n• We propose a new linear graphical model called LCM, defined as a product of factors in the cf domain. We show that our model is well defined for any collection of random variables, since any random variable has a matching cf.\n• Computing inference in closed form in linear models involving continuous variables is typically limited to the well understood cases of Gaussians, Gaussian mixtures and simple regression problems in exponential families. In this work, we extend the applicability of belief propagation to the stable family of distributions, a generalization of Gaussian, Cauchy and Lévy distributions. We analyze both exact and approximate inference algorithms, including convergence and accuracy of the solution.\n• We demonstrate the applicability of our proposed method, performing inference in real settings, using tomography data obtained from the PlanetLab network."
    }, {
      "heading" : "1.1 Related work",
      "text" : "There are three main relevant works in the machine learning domain which are related to the current work: Convolutional Factor Graphs (CFG), Copulas and Independent Component Analysis (ICA). Below we shortly review them and motivate why a new graphical model is needed.\nConvolutional Factor Graphs (CFG) [12, 13] are a graphical model for representing linear relation of independent latent random variables. CFG assume that the probability distribution factorizes as a product of potentials, and uses duality to derive a convolution factorization in the characteristic function (cf) domain. Unlike CFG, we assume that probability distribution factorizes as a product of potentials in the cf domain. There are two justifications for our new model: (a) LCMs are always defined, for any probability distribution, while CFG may are not defined when the inverse Fourier transform does not exist. (b) For certain distributions, as in the case of stable distribution, there is no closed-form pdf, thus it is not possible to compute inference in this case using CFG.\nA closely related technique is the Copula method [15, 11]. Similar to our work, Copulas assume a linear underlying model. The main difference is that Copulas transform each marginal variable into a uniform distribution and perform inference in the cumulative distribution function (cdf) domain. In contrast, we perform inference in the cf domain. In our case of interest, when the underlying distributions are stable, Copulas can not be used since stable distributions are not analytically expressible in the cdf domain.\nA third related technique is ICA (independent component analysis) on linear models [21]. Assuming a linear model Y = AX1, where the observations Y are given, the task is to estimate the linear relation matrix A, using only the fact that the latent variables X are statistically mutually independent. Both techniques (LCM and ICA) are complementary, since ICA can be used to learn the linear model, while LCM is used for computing inference in the learned model."
    }, {
      "heading" : "2 Stable distribution",
      "text" : "Stable distribution [23] is a family of heavy-tailed distributions, where Cauchy, Lévy and Gaussian are special instances of this family (see Figure 1). Stable distributions are used in different problem domains, including economics, physics, geology and astronomy [17]. Stable distribution are useful since they can model heavy-tailed distributions that naturally occur in practice. As we will soon show with our networking example, network flows exhibit empirical distribution which can be modeled remarkably well by stable distributions.\nWe denote a stable distribution by a tuple of four parameters: S(α, β, γ, δ). We call α as the characteristic exponent, β is the skew parameter, γ is a scale parameter and δ is a shift parameter. For example (Fig. 1), a Gaussian N (µ, σ2) is a stable distribution with the parameters S(2, 0, σ√\n2 , µ),\na Cauchy distribution Cauchy(γ, δ) is stable with S(1, 0, γ, δ) and a Lévy distribution Lévy(γ, δ) s stable with S(12 , 1, γ, δ). Following we define formally a stable distribution. We begin by defining a unit scale, zero-centered stable random variable.\n1Linear model is formally defined in Section 3.\nDefinition 2.1. [18, Def. 1.6] A random variable X is stable if and only if X ∼ aZ+ b, 0 < α ≤ 2, −1 ≤ β ≤ 1, a, b ∈ R, a 6= 0 and Z is a random variable with characteristic function2\nE[exp(iuZ)] =\n{\nexp ( − |u|α[1− iβ tan(πα 2 ) sign(u)] ) α 6= 1 exp (\n− |u|[1 + iβ 2 π sign(u) log(|u|)]\n) α = 1 . (1)\nNext we define a general stable random variable. Definition 2.2. [18, Def. 1.7] A random variable X is S(α, β, γ, δ) if\nX ∼\n{\nγ(Z − β tan(πα 2 )) + δ α 6= 1 γZ + δ α = 1 ,\nwhere Z is given by (1). X has characteristic function\nE exp(iuZ) =\n{\nexp(−γα|u|α[1− iβ tan(πα 2 ) sign(u)(|γu|1−α − 1)] + iδu) α 6= 1 exp(−γ|u|[1 + iβ 2 π sign(u) log(γ|u|)] + iδu) α = 1 .\nA basic property of stable laws is that weighted sums of α-stable random variables is α-stable (and hence the family is called stable). This property will be useful in the next section where we compute inference in a linear graphical model with underlying stable distribution. The following proposition formulates this linearity.\nProposition 2.1. [18, Prop. 1.16]\na) Multiplication by a scalar. If X ∼ S(α, β, γ, δ) then for any a, b ∈ R, a 6= 0,\naX + b ∼ S(α, sign(a)β, |a|γ, aδ + b) .\nb) Summation of two stable variables. If X1 ∼ S(α, β1, γ1, δ1) and X2 ∼ S(α, β2, γ2, δ2) are independent, then X1 +X2 ∼ S(α, β, γ, δ) where\nβ = β1γ\nα 1 + β2γ α 2\nγα1 + γ α 2\n, γα = γα1 + γ α 2 , δ = δ1 + δ2 + ξ ,\nξ =\n{\ntan(πα2 )[βγ − β1γ1 − β2γ2] α 6= 1 2 π [βγ log γ − β1γ1 log γ1 − β2γ2 log γ2] α = 1 .\nNote that both X1, X2 have to be distributed with the same characteristic exponent α."
    }, {
      "heading" : "3 Linear characteristic models",
      "text" : "One of the drawbacks of general stable distributions, is that they do not have closed-form equation for the pdf or the cdf. This fact makes the handling of stable distributions more difficult. This is probably one of the reasons stable distribution are rarely used in the probabilistic graphical models community.\nWe propose a novel approach for modeling linear interactions between random variables distributed according to stable distributions, using a new linear probabilistic graphical model called LCM. A new graphical model is needed, since previous approaches like CFG or the Copula method can not be used for computing inference in closed-form in linear models involving stable distribution, because they require computation in the pdf or cdf domains respectively.\nWe start by defining a linear model:\nDefinition 3.1. (Linear model) Let X1, · · · , Xn a set of mutually independent random variables.3 Let Y1, · · · , Ym be a set of observations obtained using the linear model:\nYi ∼ ∑\nj\nAijXj ∀i ,\nwhere Aij ∈ R are weighting scalars. We denote the linear model in matrix notation as Y = AX .\n2We formally define characteristic function in the supplementary material. 3We do not limit the type of random variables in this definitions. The variables may be discrete, continuous,\nor a mixture of both.\nLinear models are useful in many domains. For example, in linear channel decoding, X are the transmitted codewords, the matrix A is the linear channel transformation and Y is a vector of observations. When X are distributed using a Gaussian distribution, the channel model is called AWGN (additive white Gaussian noise) channel. Typically, the decoding task is finding the most probable X, given A and the observation Y. Despite the fact that X are assumed statistically mutually independent when transmitting, given an observation Y , X are not independent any more, since they are correlated via the observation. Besides of the network application we focus on, other potential application to our current work is linear channel decoding with stable, non-Gaussian, noise.\nIn the rest of this section we develop the foundations for computing inference in a linear model using underlying stable distributions. Because stable distributions do not have closed-form equations in the pdf domain, we must work in the cf domain. Hence, we define a dual linear model in the cf domain."
    }, {
      "heading" : "3.1 Duality of LCM and CFG",
      "text" : "Given a linear model (Def. 3.1) the probability of the observation p(yi) is given as a convolution of the hidden variables that compose it [13]: p(yi) = p(x1) ∗ · · · ∗ p(xn) , ∗ ∏\nj=1...n\np(xj) ,\nwhere ∗, ∗ ∏ are the binary and multivariate convolution operators respectively. [13] have further\nshown that the joint probability p(X,Y ) of any linear model can be factorized as a convolution:\np(X,Y ) = p(x1, · · · , xn, y1, · · · , ym) = ∗ ∏\ni\np(xi, y1, · · · , ym) . (2)\nInformally, LCM is the dual representation of (2) in the characteristic function domain. Next, we define LCM formally, and establish the duality to the factorization given in (2).\nDefinition 3.2. (LCM) Given the linear model Y=AX, we define the linear characteristic model (LCM)\nϕ(t1, · · · , tn, s1, · · · , sm) , ∏\ni\nϕ(ti, s1, · · · , sm) ,\nwhere ϕ(ti, s1, · · · , sm) is the characteristic function4 of the joint distribution p(xi, y1, · · · , ym).\nThe following two theorems establish duality between the LCM and its dual representation in the pdf domain. This duality is well known (see for example [12, 13]), but important for explaining the derivation of LCM from the linear model.\nTheorem 3.3. Given a LCM, assuming p(X,Y ) as defined in (2) has a closed form and the Fourier transform F [p(X,Y )] exists, then the F [p(X,Y )] = ϕ(t1, · · · , tn, s1, · · · , sm).\nTheorem 3.4. Given a LCM, when the inverse Fourier transform exists, then F−1(ϕ(t1, · · · , tn, s1, · · · , sm)) = p(X,Y ) as defined in (2).\nThe proof of the two above theorems is postponed to the supplementary material. Whenever the inverse Fourier transform exists, LCM model has a dual CFG model. In contrast to the CFG model, LCM are always defined, even the inverse Fourier transform does not exist. The duality is useful, since it allows us to compute inference in either representations, whenever it is more convenient."
    }, {
      "heading" : "4 Main result: exact and approximate inference in LCM",
      "text" : "This section brings our main result. Typically, exact inference in linear models with continuous variables is limited to the well understood cases of Gaussian, Gaussian mixtures and simple regression problem in exponential families. In this section we extend previous results, to show how to compute inference (both exact and approximate) in linear model with underlying stable distributions. Due to lack of space, all proofs are deferred to the supplementary material.\n4Defined in the supplementary material.\nAlgorithm 1: Exact inference in LCM using LCM-Elimination"
    }, {
      "heading" : "4.1 Exact inference in LCM",
      "text" : "The inference task typically involves computation of marginal distribution or a conditional distribution of a probability function. For the rest of the discussion we focus on marginal distribution. Marginal distribution of the node xi is typically computed by integrating out all other nodes:\np(xi) =\n∫\nX\\i\np(X,Y ) dX\\i ,\nwhere X \\ i is the set of all nodes excluding node i. Unfortunately, when working with stable distribution, the above integral is intractable. Instead, we propose to use a dual operation called slicing, computed in the cf domain. Definition 4.1. (slicing/evaluation)[22, p. 110]\n(a) Joint cf. Given random variables X1, X2, the joint cf is ϕX1,X2(t1, t2) = E[e it1x1+it2x2 ]. (b) Marginal cf. The marginal cf is derived from the joint cf by ϕX1 (t1) = ϕX1,X2(t1, 0). This operation is called slicing or evaluation. We denote the slicing operation as ϕX1(t1) =\nϕX1,X2(t1, t2)\n]\nt2=0\n.\nThe following theorem establishes the fact that marginal distribution can be computed in the cf domain, by using the slicing operation. Theorem 4.2. Given a LCM, the marginal cf of the random variable Xi can be computed using\nϕ(ti) = ∏\nj\nϕ(tj , s1, · · · , sm)\n]\nT\\i=0\n, (3)\nIn case the inverse Fourier transform exists, then the marginal probability of the hidden variable Xi is given by p(xi) ∼ F−1{ϕ(ti)} .\nBased on the results of Thm. 4.2 we propose an exact inference algorithm, LCM-Elimination, for computing the marginal cf (shown in Algorithm 1). We use the notation N(k) as the set of graph neighbors of node k, excluding k5. T is the set {t1, · · · , tn}.\nLCM-Elimination is dual to CFG-Elimination algorithm [13]. LCM-Elimination operates in the cf domain, by evaluating one variable at a time, and updating the remaining graphical model accordingly. The order of elimination does not affect correctness (although it may affect efficiency). Once the marginal cf ϕ(ti), is computed, assuming the inverse Fourier transform exists, we can compute the desired marginal probability p(xi)."
    }, {
      "heading" : "4.2 Exact inference in stable distributions",
      "text" : "After defining LCM and showing that inference can be computed in the cf domain, we are finally ready to show how to compute exact inference in a linear model with underlying stable distributions.\n5More detailed explanation of the construction of a graphical model out of the linear relation matrix A is found on [1, Chapter 2.3].\nInitialize: mij(xj) = 1, ∀Aij 6= 0. Iterate until convergence\nmij(tj) = ϕi(ti, s1, · · · , sm) ∏\nk∈N(i)\\j\nmki(ti)\n]\nti=0\nFinally:\nϕ(ti) = ϕi(ti, s1, · · · , sm) ∏\nk∈N(i)\nmki(ti).\nInitialize: mij(xj) = 1, ∀Aij 6= 0. Iterate until convergence\nmij(xj) =\n∫\nxi\np(xi, y1, · · · , ym) ∗ ∗ ∏\nk∈N(i)\\j\nmki(xi)dxi\nFinally:\np(xi) = p(xi, y1, · · · , ym) ∗ ∗ ∏\nk∈N(i)\nmki(xi).\nInitialize: βxi , γxi , δxi = S(α, 0, 0, 0), ∀i. Iterate until convergence:\nγαxi = (γαyi − ∑\nj 6=i\nγαxj |Aij |α) βi = (βyiγ α yi − ∑\nj 6=i\nsign(Aij)|Aij|αβxj γ α xj )\nξxi =\n{\ntan(πα2 )[βyiγyi + βσγσ − ∑ j Aijβxj γxj ] α 6= 1 2 π [βyiγyi log(γyi ) + βσγσ log(γσ) − ∑ j Aij log(Aij)βxj γxj − ∑ j Aijβxj γxj log(γxj )] α = 1 (6)\nOutput: xi ∼ S(α, βxi/γ α xi , γxi , δxi )\n(a) (b)\n(c)\nAlgorithm 2: Approximate inference in LCM using the (a) Characteristic-Sum-Product (CSP) algorithm (b) Integral Convolution (IC) algorithm. Both are exact on tree topologies. (c) Stable-Jacobi algorithm.\nWe assume that all observation nodes Yi are distributed according to a stable distribution. From the linearity property of stable distribution, it is clear that the hidden variables Xi are distributed according to a stable distribution as well. The following theorem is one of the the novel contributions of this work, since as far as we know, no closed-form solution was previously derived.\nTheorem 4.3. Given a LCM with n i.i.d. hidden variables Xi and n observations Yi distributed according to stable distribution Yi ∼ S(α, βyi , γyi , δyi), assuming the matrix An×n is invertible\n6, the result of exact inference for computing the marginals p(xi) ∼ Ŝ(α, βxi , γxi , δxi) is given in vector notation:\nβx = γ −α x ⊙ [(|A| α ⊙ sign(A))−1(βy ⊙ γ α y )] , γ α x = (|A| α)−1γαy , δx = A −1[δy − ξ ], (4)\nξx =\n{\ntan(πα2 )[βy ⊙ γy −A(βx ⊙ γx)] α 6= 1 2 π [βy ⊙ γy ⊙ log(γy)−(A⊙ log(|A|)(βx ⊙ γx)−A(βx ⊙ γx ⊙ log(γx))] α = 1 , (5)\nwhere ⊙ is the entrywise product (of both vectors and matrices),|A| is the absolute value (entrywise) and log(A), Aα, sign(A) are entrywise matrix operations."
    }, {
      "heading" : "4.3 Approximate Inference in LCM",
      "text" : "Typically, the cost of exact inference may be expensive. For example, in the related linear model of a multivariate Gaussian (a special case of stable distribution), LCM-Elimination reduces to Gaussian elimination type algorithm with a cost of O(n3), where n is the number of variables. Approximate methods for inference like belief propagation [19], usually require less work than exact inference, but may not always converge (or convergence to an unsatisfactory solution). The cost of exact inference motivates us to devise a more efficient approximations.\nWe propose two novel algorithms that are variants of belief propagation for computing approximate inference in LCM. The first, Characteristic-Slice-Product (CSP) is defined in LCM (shown in Algorithm 2(a)). The second, Integral-Convolution (IC) algorithm (Algorithm 2(b)) is its dual in CFG. As in belief propagation, our algorithms are exact on tree graphical models. The following theorem establishes this fact.\nTheorem 4.4. Given an LCM with underlying tree topology (the matrix A is an irreducible adjacency matrix of a tree graph), the CSP and IC algorithms, compute exact inference, resulting in the marginal cf and the marginal distribution respectively.\n6To simplify discussion we assume that the length of both the hidden and observation vectors |X| = |Y | = n. However the results can be equivalently extended to the more general case where |X| = n, |Y | = m,m 6= n. See for example [2].\nThe basic property which allows us to devise the CSP algorithm is that LCM is defined as a product of factor in the cf domain. Typically, belief propagation algorithms are applied to a probability distribution which factors as a product of potentials in the pdf domain. The sum-product algorithm uses the distributivity of the integral and product operation to devise efficient recursive evaluation of the marginal probability. Equivalently, the Characteristic-Slice-Product algorithm uses the distributivity of the slicing and product operations to perform efficient inference to compute the marginal cf in the cf domain, as shown in Theorem 4.4. In a similar way, the Integral-Convolution algorithm uses distributivity of the integral and convolution operations to perform efficient inference in CFG. Note that the original CFG work [12, 13] did not consider approximate inference. Hence our proposed IC algorithm further extends the CFG model."
    }, {
      "heading" : "4.4 Approximate inference for stable distributions",
      "text" : "For the case of stable distributions, we derive an approximation algorithm, Stable-Approx (Algorithm 2(c)), out of the CSP update rules. The algorithm is derived by substituting the convolution and multiplication by scalar (Prop. 2.1 b,a) into the update rules of the CSP algorithm given in Algorithm 2(a).\nLike belief propagation, our approximate algorithm Stable-Approx is not guaranteed to converge on general graphs containing cycles. We have analyzed the evolution dynamics of the update equations for Stable-Approx and derived sufficient conditions for convergence. Furthermore, we have analyzed the accuracy of the approximation. Not surprisingly, the sufficient condition for convergence relates to the properties of the scalar relation matrix A. The following theorem is one of the main novel contributions of this work. It provides both sufficient condition for convergence of Stable-Approx as well as closed-form equations for the fixed point.\nTheorem 4.5. Given a LCM with n i.i.d hidden variables Xi, n observations Yi distributed according to stable distribution Yi ∼ S(α, βyi , γyi , δyi), assuming the linear relation matrix An×n is positive definite and normalized to a unit diagonal7, Stable-Jacobi (as given in Algorithm 3) converges to a unique fixed point under both the following sufficient conditions for convergence (both should hold):\n(1) ρ(|R|α) < 1 , (2) ρ(R) < 1 .\nwhere ρ(R) is the spectral radius (the largest absolute value of the eigenvalues of R), andR , I−A. Furthermore, the unique fixed points of convergence are given by equations (4)-(5). The algorithm converges to the exact marginals for the linear-stable channel.8"
    }, {
      "heading" : "5 Application: Network flow monitoring",
      "text" : "In this section we propose a novel application for inference in LCMs to model network traffic flows of a large operational worldwide testbed. This is an important problem in monitoring and anomaly detection of communication networks [9, 10, 3]. We obtained Netflow PlanetLab network data [5] collected on 25 January 2010. The PlanetLab network [20] is a distributed networking testbed with around 1000 server nodes scattered in about 500 sites around the world. We define a network flow as a directed edge between a transmitting and receiving hosts. The number of packets transmitted in this flow is the scalar edge weight.\nWe propose to use LCMs for modeling distribution of network flows. Figure 2(a) plots a distribution of flows, sorted by their bandwidth, on a typical PlanetLab node. Empirically, we found out that network flow distribution in a single PlanetLab node are fitted quite well using Lévy distribution a stable distribution with α = 0.5, β = 1. The empirical means are mean(γ) ≈ 1e−4, mean(δ) ≈ 1. For performing the fitting, we use Mark Veillette’s Matlab stable distribution package [24].\n7When the matrix A is positive definite it is always possible to normalize it to a unit diagonal. The nor-\nmalized matrix is D− 1 2AD− 1\n2 where D = diag(A). Normalizing to a unit diagonal is done to simplify convergence analysis (as done for example in [6]) but does not limit the generality of the proposed method.\n8Note that there is an interesting relation to the walk-summability convergence condition [6] of belief propagation in the Gaussian case: ρ(|R|) < 1. However, our results are more general since they apply for any characteristic exponent 0 < α ≤ 2 and not just for α = 2 as in the Gaussian case.\nUsing previously proposed techniques utilizing histograms [10] for tracking flow distribution in Figure 2(a), we would need to store 40 values (percentage of bandwidth for each source port). In contrast, by approximating network flow distribution with stable distributions, we need only 4 parameters (α, β, γ, δ)! Thus we dramatically reduce storage requirements. Furthermore, using the developed theory in previous sections, we are able to linearly aggregate distribution of flows in clusters of nodes.\nWe extracted a connected component of traffic flows connecting the core network 652 nodes. We fitted a stable distribution characterizing flow behavior for each machine. A partition of 376 machines as the observed flows Yi (where flow distribution is known). The task is to predict the distribution of the unobserved remaining 376 flows Xi, based on the observed traffic flows (entries of Aij ). We run approximate inference using Stable-Approx and compared the results to the exact result computed by LCM-Elimination. We emphasize again, that using related techniques discussed in the related work section (Copula method , CFG, and ICA) it is not possible to compute exact inference for the problem at hand. Figure 2(c) plots convergence of the three parameters β, γ, δ as a function of iteration number of the Stable-approx algorithm. Note that convergence speed is geometric. (ρ(A) = 0.02 << 1).\nRegarding computation overhead, LCM-Exact algorithm requires 4 · 3763 operations, while StableApprox converged to an accuracy of 1e−5 in only 4 · 3762 · 25 operations. Additional benefit of the Stable-Approx is that it is a distributed algorithm, naturally suitable for communication networks."
    }, {
      "heading" : "6 Conclusion and future work",
      "text" : "We have presented a novel linear graphical model called LCM, defined in the cf domain. We have shown for the first time how to perform exact and approximate inference in a linear multivariate graphical model when the underlying distributions are stable. We have discussed an application of our construction for computing inference of network flows.\nWe have proposed to borrow ideas from belief propagation, for computing efficient inference, based on the distributivity property of the slice-product operations and the integral-convolution operations. We believe that other problem domains may benefit from this construction, and plan to pursue this as a future work.\nWe believe there are several exciting directions for extending this work. Other families of distributions like geometric stable distributions or Wishart can be analyzed in our model. The Fourier transform can be replaced with more general kernel transform, creating richer models."
    }, {
      "heading" : "Acknowledgement",
      "text" : "D. Bickson would like to thank Andrea Pagnani (ISI) for inspiring the direction of this research, to John P. Nolan (American University) , for sharing parts of his excellent book about stable distribution online, Mark Veillette (Boston University) for sharing his stable distribution code online, to Jason K. Johnson (LANL) for assisting in the convergence analysis and to Sapan Bathia, Marc E. Fiuczynski (Princeton University) for providing the PlanetFlow data. This research was supported by Army Research Office MURI W911NF0710287."
    }, {
      "heading" : "7 Supplementary material",
      "text" : "Definition 7.1. Characteristic Function. For a scalar random variable X , the characteristic function is defined as the expected value of eitX where i is the imaginary unit, and t ∈ R is the argument of the characteristic function: ϕX(t) = E[eitX ] = ∫∞ −∞ e\nitxdFX(x) where FX(x) is the cumulative distribution function of X . If a random variable X has a probability density function fX , then the characteristic function is its Fourier transform, ϕX(t) = ∞ ∫\n−∞ eitxfX(x)dx."
    }, {
      "heading" : "7.1 Proof of Theorem 3.3",
      "text" : "Proof.\nF(p(X,Y )) = F( ∗ ∏\ni\n(p(xi, y1, · · · , ym)) = ∏\ni\nF(p(xi, y1, · · · , ym)) =\n= ∏\ni\nϕ(ti, s1, · · · , sm) = ϕ(t1, · · · , tn, s1, · · · , sm)."
    }, {
      "heading" : "7.2 Proof of Theorem 3.4",
      "text" : "Proof. F−1(ϕ(t1, · · · , tn, s1, · · · , sm)) = F −1( ∏\ni\nϕ(ti, s1, · · · , sm))\n=\n∗ ∏\ni\nF−1(ϕ(ti, s1, · · · , sm)) =\n∗ ∏\ni\np(xi, y1, · · · , ym) = p(X,Y ) ."
    }, {
      "heading" : "7.3 Proof of Theorem 4.2",
      "text" : "Proof. The proof follows from the Projection-Slice theorem (also known as the Central Slice theorem) [14, p. 349], which is briefly stated here. Let f(x, y) be a multivariate function and F (u, v) be its matching Fourier transform. Then\nF{p(x)} = F{\n∫ ∞\n−∞\nf(x, y)dy} =\n∫ ∞\n−∞\ne iux[\n∫ ∞\n−∞\nf(x, y)dy]dx =\n∫ ∞\n−∞\n∫ ∞\n−∞\ne iux f(x, y)dxdy = F (u, 0) .\nThis theorem is naturally extended to multiple variables. In our case,\nϕ(0, 0, ti, · · · , 0) = ∏\nj\nϕ(tj , y1, · · · , ym)\n]\nT\\i=0\n=\n∫ ∞\n−∞\ne itixi\n[\n∗ ∏\nj\np(xj , y1, · · · , ym) ] dX =\n∫ ∞\n−∞\ne itixi\n[\n∫ ∞\n−∞\n∗ ∏\nj\np(xj , y1, · · · , ym)dX\\i ] dxi = F{\n∫\nX\\i\n[\n∗ ∏\nj\np(xj, y1, · · · , ym) ] dX\\i} = F{p(xi)} ."
    }, {
      "heading" : "7.4 Proof of Thm. 4.3",
      "text" : "Proof. We use the linear relation between distributions to extract X : X = A−1Y . Note that X must distribute according to stable distribution since it is composed from linear combination of stable variables. For the scale parameter we get (using the linearity of A substituted in Prop. 2.1 (a),(b))\nγαyi = ∑\nj\n|Aij | αγxj\nIn vector notation we got γαy = |A| αγx .\nSolving this linear system of equations we get\nγαx = (|A| α)−1[γαy ].\nRegarding the skew parameter βx using Prop. 2.1(a,b) we get that\nβyi =\n∑\nj sign(Aij)βxj |Aij | αγαxj\nγαyi .\nIn vector notation we get\nβy = γ −α y ⊙ [(sign(A) ⊙ |A| α)(βx ⊙ γ α x )] .\nNow assume that γαx is a known constant, we can exact βx and get\nβx = γ −α x ⊙ [((sign(A)⊙ |A| α)−1(βy ⊙ γ α)] .\nRegarding the location parameter δx,\nδyi = ∑\nj\nAijδxj + ξi ,\nξi =\n{\ntan(πα2 )[βyiγyi − ∑\nj sign(Aij)βxj |Aij |γxj ] α 6= 1 2 π [βyiγyi log(γyi)− ∑ j sign(Aij)βxj |Aij |γxj log(|Aij |γxj)] α = 1 .\nIn matrix notation (after some algebra) we get\nδy = Aδx + ξ\nξ =\n{\ntan(πα2 )[βy ⊙ γy −A(βx ⊙ γx)] α 6= 1 2 π [βy ⊙ γy ⊙ log(γy)− (A⊙ log(|A|))(βx ⊙ γx)− A(βx ⊙ γx ⊙ log(γy))] α = 1 .\nIn total we got a linear system that is solved using\nδx = A −1(δy − ξ) ."
    }, {
      "heading" : "7.5 Proof of Theorem 4.4",
      "text" : "Proof. W.l.g we prove for the Slice-product algorithm 2(a). The other algorithms are symmetric because the slice/convolution and integral/convolution operations maintain the distributivity property as well.\nWe are interested in computing the posterior marginal probability\np(xi) =\n∫\nx\\i\np(x, y)dX\\i ∼\n∫\nX\\i\np(x1, · · · , xn, y1, · · · , ym)dX\\i (7)\n= F−1{ ∏\ni\nϕ(ti, s1, · · · , sm) ]\nti=0 } . (8)\nW.l.g assume that Xi is a tree root. Its matching marginal cf ϕ(ti) can be written as a combination of incoming message computed by the neighboring sub trees:\nϕ(ti) ∼ ϕ(ti, s1, · · · , sm) ∏\nj∈N(i) mji(ti) ,\nwhere the messages mji(ti) are defined by the algorithm 2(a). We prove using full induction on the tree diameter. The messages mji(ti) satisfy the recursion:\nmji(ti) = ϕ(ti, s1, · · · , sm) ∏\nk∈N(j)\\i mkj(xj)\n]\nxj=0 .\nThe basis for the induction is a tree with a single node x1. In this case there are no incoming messages, ϕ(t1) = ϕ(ti, s1, · · · , sm) and we are done. Now assume that the induction assumption holds for a tree with diameter d − 1 or less and we want to prove it for a tree with diameter d. We\nmake the following construction. We add a new node xi to the tree to get a tree with diameter d. This node has one or more neighbors j ∈ N(i).\nϕ(ti) ∼ ϕ(ti, s1, · · · , sm) ∏\nj∈N(i) mji(ti) =\n= (ti, s1, · · · , sm) ∏\nj∈N(i) p(tj , s1, · · · , sm)\n∏\nl∈N(j)\\i mlj(tj)]\n]\ntj=0 .\nUsing distributivity of the slice/product (algorithm 2(a)), and the tree assumption (separate trees connected to node k are disjoint), we interchange order of operators to get:\nϕ(ti) ∼ ϕ(ti, s1, · · · , sm)[ ∏\nj 6=i ϕ(tj , s1, · · · , sm))]\n]\ntj 6=i=0 =\n= ∏\ni\nϕ(ti, s1, · · · , sm) ]\ntX\\i=0\nThis completes the proof since we have obtained the formulation (8)."
    }, {
      "heading" : "7.6 Proof of Theorem 4.5",
      "text" : "Proof. We start with the scale parameter calculation since it is decoupled from the other parameters.\nγαxi = γyi − ∑ j 6=i γαxj |Aij | α\nThis iteration is a Jacobi iteration for solving the linear system\n|A|αγαx = γy\nThe linear system solution is given in (4) as desired. It is further known that this iteration converges when ρ(|R|α) < 1.\nRegarding the skew parameter β the Stable-Jacobi update rule is:\nβxi = βyiγ α yi −\n∑ j 6=i sign(Aij)|Aij | αβxjγ α xj .\nThis conforms to the Jacobi equation for solving the linear system\n[|A|α ⊙ sign(A)]β̂x = βy ⊙ γ α y\nAssuming this system converged, we divide by γαy to get (4)\nβx = γ −α x ⊙ [|A| α ⊙ sign(A)]−1[βy ⊙ γ α y ] .\nThe iteration for computing a skew parameter β converges when ρ(|R|α ⊙ sign(R)) < 1. Using [4, Theorem 8.4.5, Section 8.4] we get that ρ(|Rα ⊙ sign(R)|) = ρ(|Rα|) > ρ(|R|α ⊙ sign(R)). In other words, when the sufficient condition for the scale parameter γ holds (ρ(|Rα|) < 1), then the skew parameter β converges as well.\nNow we analyze the shift parameter δ evolution. The parameter is given by\nδxi = δyi − ∑\nj 6=i Aijδxj − ξxj ,\nThis is a Jacobi equation for solving the linear system\nAδx = δy − ξ\nIs given in (4). This iteration converges when ρ(R) < 1, which is the second sufficient condition for convergence."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2017,
    "abstractText" : "<lb>Heavy-tailed distributions naturally occur in many real life problems. Unfortu-<lb>nately, it is typically not possible to compute inference in closed-form in graphical<lb>models which involve such heavy-tailed distributions.<lb>In this work, we propose a novel simple linear graphical model for independent<lb>latent random variables, called linear characteristic model (LCM), defined in the<lb>characteristic function domain. Using stable distributions, a heavy-tailed family<lb>of distributions which is a generalization of Cauchy, Lévy and Gaussian distri-<lb>butions, we show for the first time, how to compute both exact and approximate<lb>inference in such a linear multivariate graphical model. LCMs are not limited to<lb>stable distributions, in fact LCMs are always defined for any random variables<lb>(discrete, continuous or a mixture of both).<lb>We provide a realistic problem from the field of computer networks to demon-<lb>strate the applicability of our construction. Other potential application is iterative<lb>decoding of linear channels with non-Gaussian noise.",
    "creator" : "LaTeX with hyperref package"
  }
}