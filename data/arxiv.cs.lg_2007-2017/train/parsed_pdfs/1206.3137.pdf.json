{
  "name" : "1206.3137.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Identifiability and Unmixing of Latent Parse Trees",
    "authors" : [ "Daniel Hsu", "Sham M. Kakade", "Percy Liang" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Generative parsing models, which define joint distributions over sentences and their parse trees, are one of the core techniques in computational linguistics. We are interested in the unsupervised learning of these models [2–6], where the goal is to estimate the model parameters given only examples of sentences. Unsupervised learning can fail for a number of reasons [7]: model misspecification, non-identifiability, estimation error, and computation error. In this paper, we delve into two of these issues: identifiability and computation. In doing so, we confront a central challenge of parsing models—that the topology of the parse tree is unobserved and varies across sentences. This is in contrast to standard phylogenetic models [8] and other latent tree models for which there is a single fixed global tree across all examples [9].\nA model is identifiable if there is enough information in the data to pinpoint the parameters (up to some trivial equivalence class); establishing the identifiability of a model is often a highly nontrivial task. A classic result of Kruskal [10] has been employed to prove the identifiability of a wide class of latent variable models, including hidden Markov models and certain restricted mixtures of latent tree models [11–13]. However, these techniques cannot be directly applied to parsing models since the tree topology varies over an exponential set of possible topologies. Instead, we turn to techniques from algebraic geometry [14–17]; we show that a simple numerical procedure can be used to check identifiability for a wide class of models in NLP. Using this tool, we discover that probabilistic context-free grammars (PCFGs) are non-identifiable, but that simpler PCFG variants and dependency models are identifiable.\nThe most common way to estimate unsupervised parsing models is by using local techniques such as EM [18] or MCMC sampling [19], but these methods can suffer from local optima and slow mixing. Meanwhile, recent work [1,20–23] has shown that spectral methods can be used to estimate mixture models and HMMs with provable guarantees. These techniques express low-order moments\nar X\niv :1\n20 6.\n31 37\nv1 [\nst at\n.M L\n] 1\n4 Ju\nn 20\n12\nof the observable distribution as a product of matrix parameters and use eigenvalue decomposition to recover these matrices. However, these methods are not directly applicable to parsing models because the tree topology again varies non-trivially. To address this, we propose a new technique, unmixing. The main idea is to express moments of the observable distribution as a mixture over the possible topologies. For restricted parsing models, the moments for a fixed tree structure can be “unmixed”, thereby reducing the problem to one with a fixed topology, which can be tackled using standard techniques [1]. Importantly, our unmixing technique does not require the training sentences be annotated with the tree topologies a priori, in contrast to recent extensions of [21] to learning PCFGs [24] and dependency trees [25,26], which work on a fixed topology."
    }, {
      "heading" : "2 Notation",
      "text" : "For a positive integer n, define [n] def = {1, . . . , n} and 〈n〉 = {e1, . . . , en}, where ei is the vector which is 1 in component i and 0 elsewhere. For integers a, b ∈ [n], let a ⊗n b = (a − 1)n + b ∈ [n2] be the integer encoding of the pair (a, b). For a pair of matrices, A,B ∈ Rm×n, define the columnwise tensor product A ⊗c B ∈ Rm2×n to be such that (A ⊗c B)(i1⊗mi2)j = Ai1jBi2j . For a matrix A ∈ Rm×n, let A† denote the Moore-Penrose pseudoinverse."
    }, {
      "heading" : "3 Parsing models",
      "text" : "A sentence is a sequence of L words, x = (x1, . . . , xL), where each word xi ∈ 〈d〉 is one of d possible word types. A (generative) parsing model defines a joint distribution Pθ(x, z) over a sentence x and its parse tree z (to be made precise later), where θ are the model parameters (a collection of multinomials). Each parse tree z has a topology Topology(z) ∈ Topologies, which is both unobserved and varying across sentences. The learning problem is to recover θ given only samples of x.\nTwo important classes of models of natural language syntax are constituency models, which represent a hierarchical grouping and labeling of the phrases of a sentence (e.g., Figure 1(a)), and dependency models, which represent pairwise relationships between the words of a sentence (e.g., Figure 1(b))."
    }, {
      "heading" : "3.1 Constituency models",
      "text" : "A constituency tree z = (V, s) consists of a set of nodes V and a collection of hidden states s = {sv}v∈V . Each state sv ∈ 〈k〉 represents one of k possible syntactic categories. Each node v has the form [i : j] for 0 ≤ i < j ≤ L corresponding to the phrase between positions i and j of the sentence. These nodes form a binary tree as follows: the root node is [0 : L] ∈ V , and for each node [i : j] ∈ V with j − i > 1, there exists a unique m with i < m < j defining the two children nodes [i : m] ∈ V and [m : j] ∈ V . Let Topology(z) be an integer encoding of V .\nPCFG. Perhaps the most well-known constituency parsing model is the probabilistic context-free grammar (PCFG). The parameters of a PCFG are θ = (π,B,O), where π ∈ Rk specifies the initial state distribution, B ∈ Rk2×k specifies the binary production distributions, and O ∈ Rd×k specifies the emission distributions.\nA PCFG corresponds to the following generative process (see Figure 1(a) for an example): choose a topology Topology(z) uniformly at random;1 generate the state of the root node using π; recursively generate pairs of children states given their parents using B; and finally generate words xi given their parents using O. This generative process defines a joint probability over a sentence x and a parse tree z:\nPθ(x, z) = |Topologies |−1π>s[0:L] ∏\n[i:m],[m:j]∈V (s[i:m] ⊗k s[m:j])>Bs[i:j] L∏ i=1 x>i Os[i−1:i], (1)\nWe will also consider two variants of the PCFG with additional restrictions:\nPCFG-I. The left and right children states are generated independently—that is, we have the following factorization: B = T1 ⊗c T2 for some T1, T2 ∈ Rk×k.\nPCFG-IE. The left and the right productions are independent and equal: B = T ⊗c T ."
    }, {
      "heading" : "3.2 Dependency tree models",
      "text" : "In contrast to constituency trees, which posit internal nodes with latent states, dependency trees connect the words directly. A dependency tree z is a set of directed edges (i, j), where i, j ∈ [L]\n1 Usually a PCFG induces a topology via a state-dependent probability of choosing a binary production versus an emission. Our model is a restriction which corresponds to a state-independent probability.\nare distinct positions in the sentence. Let Root(z) denote the position of the root node of z. We consider only projective dependency trees [27]: z is projective if for every path from i to j to k in z, we have that j and k are on the same side of i (that is, j − i and k− i have the same sign). Let Topology(z) be an integer encoding of z.\nDEP-I. We consider the simple dependency model of [4]. The parameters of this model are θ = (π,A↙, A↘), where π ∈ Rd is the initial word distribution and A↙, A↘ ∈ Rd×d are the left and right argument distributions. The generative process is as follows: choose a topology Topology(z) uniformly at random, generate the root word using π, and recursively generate argument words to the left to the right given the parent word using A↙ and A↘, respectively. The corresponding joint probability distribution is as follows:\nPθ(x, z) = |Topologies |−1π>xRoot(z) ∏\n(i,j)∈z\nx>j Adir(i,j)xi, (2)\nwhere dir(i, j) =↙ if j < i and ↘ if j > i. We also consider the following two variants:\nDEP-IE. The left and right argument distributions are equal: A = A↙ = A↘.\nDEP-IES. A = A↙ = A↘ and π is the stationary distribution of A (that is, π = Aπ)."
    }, {
      "heading" : "4 Identifiability",
      "text" : "Our goal is to estimate model parameters θ0 ∈ Θ given only access to sentences x ∼ Pθ0 . Specifically, suppose we have an observation function φ(x) ∈ Rm, which is the only lens through which an algorithm can view the data. We ask a basic question: in the limit of infinite data, is it informationtheoretically possible to identify θ0 from the observed moments µ(θ0) def = Eθ0 [φ(x)]?\nTo be more precise, define the equivalence class of θ0 to be the set of parameters θ that yield the same observed moments:\nSΘ(θ0) = {θ ∈ Θ : µ(θ) = µ(θ0)}. (3)\nIt is impossible for an algorithm to distinguish among the elements of SΘ(θ0). Therefore, one might want to ensure that |SΘ(θ0)| = 1 for all θ0 ∈ Θ. However, this requirement is too strong for two reasons. First, models often have natural symmetries—e.g., the k states of any PCFG can be permuted without changing µ(θ), so |SΘ(θ0)| ≥ k!. Second, |SΘ(θ0)| = ∞ for some pathological θ0’s—e.g., PCFGs where all states have the same emission distribution O are indistinguishable regardless of the production distributions B. The following definition of identifiability accommodates these two exceptional cases:\nDefinition 1 (Identifiability). A model family with parameter space Θ is (globally) identifiable from φ if there exists a measure zero set E such that |SΘ(θ0)| is finite for every θ0 ∈ Θ\\E. It is locally identifiable from φ if there exists a measure zero set E such that, for every θ0 ∈ Θ\\E, there exists an open neighborhood N(θ0) around θ0 such that SΘ(θ0) ∩N(θ0) = {θ0}.\nExample of non-identifiability. Consider the DEP-IE model with L = 2 with the full observation function φ(x) = x1 ⊗ x2. The corresponding observed moments are µ(θ) = 0.5Adiag(π) + 0.5 diag(π)A>. Note that Adiag(π) is an arbitrary d × d matrix whose entries sum to 1, which has d2 − 1 degrees of freedom, whereas µ(θ) is a symmetric matrix whose entries sum to 1, which has ( d+1\n2\n) − 1 degrees of freedom. Therefore, SΘ(θ) has dimension ( d 2 ) and therefore the model is\nnon-identifiable.\nParameter counting. It is important to compute the degrees of freedom correctly—simple parameter counting is insufficient. For example, consider the PCFG-IE model with L = 2. The observed moments with respect to φ(x) = x1⊗ x2 is a d× d matrix, which places d2 constraints on the k2 + (d− 1)k parameters. When d ≥ 2k, there are more constraints than parameters, but the PCFG-IE model with L = 2 is actually non-identifiable (as we will see later). The issue here is that the number of constraints does not reveal the fact that some of these constraints are redundant."
    }, {
      "heading" : "4.1 Observation functions",
      "text" : "An observation function φ(x) and its associated observed moments µ(θ0) = Eθ0 [φ(x)] reveals aspects of the distribution Pθ0(x). For example, φ(x) = x1 would only reveal the marginal distribution of the first word, whereas φ(x) = x1 ⊗ · · · ⊗ xL reveals the entire distribution of x. There is a tradeoff: Higher-order moments provide more information, but are harder to estimate reliably given finite data, and are also computationally more expensive. In this paper, we consider the following intermediate moments:\nφ12(x) def = x1 ⊗ x2 φ∗∗(x) def = ( xi ⊗ xj : i, j ∈ [L] ) φ123(x) def = x1 ⊗ x2 ⊗ x3 φ∗∗∗(x) def = ( xi ⊗ xj ⊗ xk : i, j, k ∈ [L]\n) φ123η(x) def = (x1 ⊗ x2)(η>x3) φ∗∗∗η(x) def = ( (xi ⊗ xj)(η>xk) : i, j, k ∈ [L]\n) φall(x) def = x1 ⊗ · · · ⊗ xL\nAbove, η ∈ Rd denotes a unit vector in Rd (e.g., e1) which picks out a linear combination of matrix slices from a third-order d× d× d tensor."
    }, {
      "heading" : "4.2 Automatically checking identifiability",
      "text" : "One immediate goal is to determine which models in Section 3 are identifiable from which of the observed moments (Section 4.1). A powerful analytic tool that has been succesfully applied in previous work is Kruskal’s theorem [10,11], but (i) it is does not immediately apply to models with random topologies, and (ii) only gives sufficient conditions for identifiability, and cannot be used to determine non-identifiability. Furthermore, since it is common practice to explore many different models for a given problem in rapid succession, we would like to check identifiability quickly and reliably. In this section, we develop an automatic procedure to do this.\nTo establish identifiability, let us examine the algebraic structure of SΘ(θ0) for θ0 ∈ Θ, where we assume that the parameter space Θ is an open subset of [0, 1]n.2 Recall that SΘ(θ0) is defined\n2While we initially defined θ to be a tuple of conditional probability matrices, we will now use its non-redundant vectorized form θ ∈ Rn.\nby the moment constraints µ(θ) = µ(θ0). We can write these constraints as hθ0(θ) = 0, where\nhθ0(θ) def = µ(θ)− µ(θ0)\nis a vector of m polynomials in θ. Let us now compute the number of degrees of freedom of hθ0 around θ0. The key quantity is J(θ) ∈ Rm×n, the Jacobian of hθ0 at θ (note that the Jacobian of hθ0 does not depend on θ0; it is precisely the Jacobian of µ). This Jacobian criterion is well-established in algebraic geometry, and has been adopted in the statistical literature for testing model identifiability and other related properties [14–17].\nIntuitively, each row of J(θ0) corresponds to a direction of a constraint violation, and thus the row space of J(θ0) corresponds to all directions that would take us outside the equivalence class SΘ(θ0). If J(θ0) has less than rank n, then there is a direction orthogonal to all the rows along which we can move and still satisfy all the constraints—in other words, |SΘ(θ0)| is infinite, and therefore the model is non-identifiable. This intuition leads to the following algorithm:\nCheckIdentifiability: −1. Choose a point θ̃ ∈ Θ uniformly at random. −2. Compute the Jacobian matrix J(θ̃). −3. Return “yes” if the rank of J(θ̃) = n and “no” otherwise.\nThe following theorem asserts the correctness of CheckIdentifiability. It is largely based on techniques in [16], although we have not seen it explicitly stated in this form.\nTheorem 1 (Correctness of CheckIdentifiability). Assume the parameter space Θ is a nonempty open connected subset of [0, 1]n; and the observed moments µ : Rn → Rm, with respect to observation function φ, is a polynomial map. Then with probability 1, CheckIdentifiability returns “yes” iff the model family is locally identifiable from φ. Moreover, if it returns “yes”, then there exists E ⊂ Θ of measure zero such that the model family with parameter space Θ \\ E is identifiable from φ.\nThe proof of Theorem 1 is given in Appendix A.\n4.3 Implementation of CheckIdentifiability\nComputing the Jacobian. The rows of J correspond to ∂Eθ[φj(x)]/∂θ and can be computed efficiently by adapting dynamic programs used in the E-step of an EM algorithm for parsing models. There are two main differences: (i) we must sum over possible values of x in addition to z, and (ii) we are not computing moments, but rather gradients thereof. Specifically, we adapt the CKY algorithm for constituency models and the algorithm of [27] for dependency models. See Appendix C.1 for more details.\nNumerical issues. Because we implemented CheckIdentifiability on a finite precision machine, the results are subject to numerical precision errors. However, we verified that our numerical results are consistent with various analytically-derived identifiability results (e.g., from [11])."
    }, {
      "heading" : "4.4 Identifiability of constituency and dependency tree models",
      "text" : "We checked the identifiability status of various constituency and dependency tree models using our implementation of CheckIdentifiability. We focus on the regime where d ≥ k for PCFGs; additional results for d < k are given in Appendix B.\nThe results are reported in Figure 2. First, we found that the PCFG is not identifiable from φall (and therefore not identifiable from any φ) for L ∈ {3, 4, 5}; we believe that the same holds for all L. This negative result motivates exploring restricted subclasses of PCFGs, such as PCFGI and PCFG-IE, which factorize the binary productions.3 For these classes, we found that the sentence length L and choice of observation function can influence identifiability: Both models are identifiable for large enough L (e.g., L ≥ 3) and with a sufficiently rich observation function (e.g., φ123η).\nThe dependency models, DEP-I and DEP-IE, were all found to be identifiable for L ≥ 3 from second-order moments φ∗∗. The conditions for identifiability are less stringent than their constituency counterparts (PCFG-I and PCFG-IE), which is natural since dependency models are simpler without the latent states. Note that in all identifiable models, second-order moments suffice to determine the distribution—this is good news because low-order moments are easier to estimate."
    }, {
      "heading" : "5 Unmixing algorithms",
      "text" : "Having established which parsing models are identifiable, we now turn to parameter estimation for these models. We will consider algorithms based on moment matching—those that try to find a θ satisfying µ(θ) = u for some u. Typically, u is an empirical estimate of µ(θ0) = Eθ0 [φ(x)] based on samples x ∼ Pθ0 .4\nIn general, solving µ(θ) = u corresponds to finding solutions to systems of multivariate polynomials, which is NP-hard [28]. However, µ(θ) often has additional structure which we can exploit. For instance, for an HMM, the sliced third-order moments µ123η(θ) can be written as a product of parameter matrices in θ, and each matrix can be recovered by decomposing the product [1].\nFor parsing models, the challenge is that the topology is random, so the moments is not a single product, but a mixture over products. To deal with this complication, we propose a new technique,\n3Note that these subclasses occupy measure zero subsets of the PCFG parameter space, which is expected given the non-identifiability of the general PCFG.\n4We will develop our algorithms assuming true moments (u = µ(θ0)). The empirical moments converge to the true\nmoments at Op(n − 1\n2 ), and matrix perturbation arguments (e.g., [1]) can be used derive sample complexity arguments for the parameter error.\nwhich we call unmixing: We “unmix” the products from the mixtures, essentially reducing the problem to one with a fixed topology.\nWe will first present the general idea of unmixing (Section 5.1) and then apply it to the PCFG-IE model (Section 5.2) and the DEP-IES model (Section 5.3)."
    }, {
      "heading" : "5.1 General case",
      "text" : "We assume the observation function φ(x) consists of a collection of observation matrices {φo(x)}o∈O (e.g., for o = (i, j), φo(x) = xi ⊗ xj). Given an observation matrix φo(x) and a topology t ∈ Topologies, consider the mapping that computes the observed moment conditioned on that topology: Ψo,t(θ) = Eθ[φo(x) | Topology = t]. As we range o over O and t over Topologies, we will enounter a finite number of such mappings. We call these mappings compound parameters, denoted {Ψp}p∈P .\nNow write the observed moments as a weighted sum: µo(θ) = ∑ p∈P\nP(Ψo,Topology = Ψp)︸ ︷︷ ︸ def =Mop Ψp for all o ∈ O, (4)\nwhere we have defined Mop to be the probability mass over tree topologies that yield compound parameter Ψp. We let {Mop}o∈O,p∈P be the mixing matrix. Note that (4) defines a system of equations µ = MΨ, where the variables are the compound parameters and the constraints are the observed moments. In a sense, we have replaced the original system of polynomial equations (in θ) with a system of linear equations (in Ψ).\nThe key to the utility of this technique is that the number of compound parameters can be polynomial in L even when the number of possible topologies is exponential in L. Previous analytic techniques [13] based on Kruskal’s theorem [10] cannot be applied here because the possible topologies are too many and too varied.\nNote that the mixing equation µ = MΨ holds for each sentence length L, but many compound parameters p appear in the equations of multiple L. Therefore, we can combine the equations across all observed sentence lengths, yielding a more constrained system than if we considered the equations of each L separately.\nThe following proposition shows how we can recover θ by unmixing the observed moments µ:\nProposition 1 (Unmixing). Suppose that there exists an efficient base algorithm to recover θ from some subset of compound parameters {Ψp(θ) : p ∈ P0}, and that e>p is in the row space of M for each p ∈ P0. Then we can recover θ as follows:\nUnmix(µ): −1. Compute the mixing matrix M (4). −2. Retrieve the compound parameters Ψp(θ) = (M †µ)p for each p ∈ P0. −3. Call the base algorithm on {Ψp(θ) : p ∈ P0} to obtain θ.\nFor all our parsing models, M can be computed efficiently using dynamic programming (Appendix C.2). Note that M is data-independent, so this computation can be done once in advance."
    }, {
      "heading" : "5.2 Application to the PCFG-IE model",
      "text" : "As a concrete example, consider the PCFG-IE model over L = 3 words. Write A = OT . For any η ∈ Rd, we can express the observed moments as a sum over the two possible topologies in Figure 1(a):\nµ123η def = E[x1 ⊗ x2(η>x3)] = 0.5Ψ1;η + 0.5Ψ2;η, Ψ1;η def = Adiag(T diag(π)A>η)A>,\nµ132η def = E[x1 ⊗ x3(η>x2)] = 0.5Ψ3;η + 0.5Ψ2;η, Ψ2;η def = Adiag(π)T> diag(A>η)A>,\nµ231η def = E[x2 ⊗ x3(η>x1)] = 0.5Ψ3;η + 0.5Ψ1;η, Ψ3;η def = Adiag(A>η)T diag(π)A>,\nor compactly in matrix form: µ123ηµ132η µ231η  ︸ ︷︷ ︸\nobserved moments µη\n=  0.5I 0.5I 00 0.5I 0.5I 0.5I 0 0.5I  ︸ ︷︷ ︸\nmixing matrix M\n Ψ1;ηΨ2;η Ψ3;η  ︸ ︷︷ ︸\ncompound parameters Ψη\n.\nLet us observe µη at two different values of η, say at η = 1 and η = τ for some random τ . Since the mixing matrix M is invertible, we can obtain the compound parameters Ψ2;1 = (M\n−1µ1)2 and Ψ2;τ = (M\n−1µτ )2. Now we will recover θ from Ψ2;1 and Ψ2;τ by first extracting A = OT via an eigenvalue decomposition, and then recovering π, T , and O in turn (all up to the same unknown permutation) via elementary matrix operations.\nFor the first step, we will use the following tool (adapted from Algorithm A of [1]), which allow us to decompose two related matrix products:\nLemma 1 (Spectral decomposition). Let M1,M2 ∈ Rd×k have full column rank and D be a diagonal matrix with distinct diagonal entries. Suppose we observe X = M1M > 2 and Y = M1DM > 2 . Then Decompose(X,Y ) recovers M1 up to a permutation and scaling of the columns.\nDecompose(X,Y ): −1. Find U1, U2 ∈ Rd×k such that range(U1) = range(X) and range(U2) = range(X>). −2. Perform an eigenvalue decomposition of (U>1 Y U2)(U>1 XU2)−1 = V SV −1. −3. Return (U>1 )†V .\nFirst, run Decompose(X = Ψ>2;1, Y = Ψ > 2;τ ) (Lemma 1), which corresponds to M1 = A and M2 = Adiag(π)T >. This produces AΠS for some permutation matrix Π and diagonal scaling S. Since we know that the columns of A sum to one, we can identify AΠ. To recover the initial distribution π (up to permutation), take Ψ2;11 = Aπ and left-multiply by (AΠ)† to get Π−1π. For T , put the entries of π in a diagonal matrix: Π−1 diag(π)Π. Take Ψ>2;1 = AT diag(π)A > and multiply by (AΠ)† on the left and ((AΠ)>)†(Π−1 diag(π)Π)−1 on the right, which yields Π−1TΠ. (Note that Π is orthogonal, so Π−1 = Π>.) Finally, multiply AΠ = OTΠ and (Π−1TΠ)−1, which yields OΠ.\nThe above algorithm identifies the PCFG-IE from only length 3 sentences. To exploit sentences of different lengths, we can compute a mixing matrix M which includes constraints from sentences\nof length 1 ≤ L ≤ Lmax up to some upper bound Lmax. For example, Lmax = 10 results in a 990 × 2376 mixing matrix. We can retrieve the same compound parameters (Ψ2;1 and Ψ2;τ ) from the pseudoinverse of M and as proceed as before."
    }, {
      "heading" : "5.3 Application to the DEP-IES model",
      "text" : "We now turn to the DEP-IES model over L = 3 words. Our goal is to recover the parameters θ = (π,A). Let D = diag(π) = diag(Aπ), where the second equality is due to stationarity of π.\nµ1 def = E[x1] = π,\nµ12 def = E[x1 ⊗ x2] = 7−1(DA> +DA> +DA>A> +AD +ADA> +AD +DA>),\nµ13 def = E[x1 ⊗ x3] = 7−1(DA> +DA>A> +DA> +ADA> +AD +AAD +AD),\nµ̃12 def = Ẽ[x1 ⊗ x2] = 2−1(DA> +AD),\nwhere Ẽ[·] is taken with respect to length 2 sentences. Having recovered π from µ1, it remains to recover A. By selectively combining the moments above, we can compute AA + A = [7(µ13 − µ12) + 2µ̃12] diag(µ1)\n−1. Assuming A is generic position, it is diagonalizable: A = QΛQ−1 for some diagonal matrix Λ = diag(λ1, . . . , λd), possibly with complex entries. Therefore, we can recover Λ2 + Λ = Q−1(AA+A)Q. Since Λ is diagonal, we simply have d independent quadratic equations in λi, which can be solved in closed form. After obtaining Λ, we retrieve A = QΛQ −1."
    }, {
      "heading" : "6 Discussion",
      "text" : "In this work, we have shed some light on the identifiability of standard generative parsing models using our numerical identifiability checker. Given the ease with which this checker can be applied, we believe it should be a useful tool for analyzing more sophisticated models [6], as well as developing new ones which are expressive yet identifiable.\nThere is still a large gap between showing identifiability and developing explicit algorithms. We have made some progress on closing it with our unmixing technique, which can deal with models where the tree topology varies non-trivially."
    }, {
      "heading" : "A Proof of Theorem 1",
      "text" : "Theorem 1 (restated). Assume Θ is a non-empty open connected subset of [0, 1]n and µ : Rn → Rm is a polynomial map. With probability 1, the following holds.\n• CheckIdentifiability returns “no” ⇒ for almost all θ0 ∈ Θ and any open neighborhood N(θ0) around θ0, |SΘ(θ0) ∩N(θ0)| is infinite (not locally identifiable).\n• CheckIdentifiability returns “yes” ⇒ (i) for almost all θ0 ∈ Θ, there exists an open neighborhood N(θ0) around θ0 such that |SΘ(θ0) ∩ N(θ0)| = 1 (locally identifiable); and (ii) there exists a set E ⊂ Θ with measure zero such that |SΘ\\E(θ0)| is finite for every θ0 ∈ Θ \\ E (identifiability of Θ\\E).\nThe proof of Theorem 1 crucially relies on the following lemma from [16] which holds even in the case that µ is merely an analytic function (see Lemma 9 of [17] for a simpler proof in the case µ is a polynomial map); it states that the Jacobian achieves its maximal rank almost everywhere in Θ. To state this precisely, first define rmax def = max{rank(J(θ)) : θ ∈ Θ} and Θmax def = {θ ∈ Θ : rank(J(θ)) = rmax}.\nLemma 2. The set Θ \\Θmax has Lebesgue measure zero. That is, Θmax is almost all of Θ.\nProof of Theorem 1. By Lemma 2, CheckIdentifiability chooses a point θ̃ ∈ Θmax with probability 1. We henceforth condition on this event, so rank(J(θ̃)) = rmax.\nCase 1: rank(J(θ̃)) < n (i.e., “no” is returned). In this case, we have rmax < n. We now employ an argument from the proof of Proposition 20 of [16]. Fix any θ0 ∈ Θmax. Since Θ is open, Weyl’s theorem implies that there is an open neighborhood U around θ0 in Θ on which rank(J(θ)) = rmax\nfor all θ ∈ U (i.e., rank(J(·)) is constant on U). Therefore, by the constant rank theorem, there is an open neighborhood N(θ0) around θ0 in Θ such that µ\n−1(µ(θ0)) ∩N(θ0) is homeomorphic with an open set in Rn−rmax . Therefore SΘ(θ0) ∩N(θ0) is uncountably infinite.\nCase 2: rank(J(θ̃)) = n (i.e., “yes” is returned). In this case, we have rmax = n. Therefore for every θ0 ∈ Θmax, the Jacobian J(θ0) has full column rank, and thus by the inverse function theorem, µ is injective on a neighborhood of θ0. This in turn implies that for all θ0 ∈ Θmax, there exists an open neighborhood N(θ0) around θ0 such that SΘ(θ0) ∩N(θ0) = {θ0}. This proves (i).\nTo show (ii), define E def= Θ \\ Θmax, and now claim that for every θ0 ∈ Θmax, the equivalence class SΘmax(θ0) is finite. Observe that by (i), the set SΘmax(θ0) contains only geometrically isolated solutions to the system of polynomial equations given by µ(θ) = µ(θ0). Therefore the claim follows immediately from Bézout’s Theorem, which implies that the number of geometrically isolated solutions is finite.\nRemark. All the models considered in this paper have moments µ which correspond to a polynomial map. However, for some models (e.g., exponential families), µ will not be a polynomial map, but rather, a general analytic function. In this case, Theorem 1 holds with one modification to (ii). If CheckIdentifiability returns “yes”, then we have the following weaker guarantee in place of (ii): SΘmax(θ0) is countable (but not necessarily finite) for all θ0 ∈ Θmax. The above proof does not require the fact that µ is a polynomial map except in the invocation of Bézout’s Theorem. In place of Bézout’s Theorem, we use the following argument. If SΘmax(θ0) is uncountable, then it contains a limit point θ∗ ∈ SΘmax(θ0); thus for any small enough neighborhood N(θ∗) of θ∗, there is some θ ∈ SΘmax(θ0) ∩N(θ∗). This contradicts (i) as applied to θ∗, and thus we conclude that SΘmax(θ0) is countable."
    }, {
      "heading" : "B Additional results from the identifiability checker",
      "text" : "PCFG models with d < k. The PCFG models that we’ve considered so far assume that the number of words d is at least the number of hidden states k, which is a realistic assumption for natural language. However, there are applications, e.g., computational biology, where the vocabulary size d is relatively small. In this regime, identifiability becomes trickier because the data doesn’t reveal as much about the hidden states, and brings us closer to the boundary between identifiability and non-identifiability. In this section, we consider the d < k regime.\nThe following table gives additional identifiability results from CheckIdentifiability for values of d, k, and L where d < k (recall that the results reported in Section 4.4 only considered values where d ≥ k). In each cell, we show the (k, d, L) values for which CheckIdentifiability returned “yes”; the values checked were k ∈ {3, 4, . . . , 8}, d ∈ {2, . . . , k − 1}, L ∈ {3, 4, . . . , 9}.\nφ12 φ∗∗ φ123e1 φ123 φ∗∗∗e1 φ∗∗∗ PCFG None\nPCFG-I None (3, 2,≥ 6) (4, 2,≥ 8) (4, 3,≥ 5) (5, 3,≥ 6) (5, 4,≥ 4) (6, 3,≥ 7) (6, 4,≥ 5) (6, 5,≥ 4) (7, 3,≥ 8) (7, 4,≥ 6) (7, 5,≥ 5) (7, 6,≥ 4)\nNone\n(5, 4,≥ 4) (6, 5,≥ 4) (7, 5,≥ 4) (7, 6,≥ 4)\n(3, 2,≥ 5) (4, 2,≥ 6) (4, 3,≥ 4) (5, 2,≥ 7)\n(5,≥ 3,≥ 4) (6, 2,≥ 8) (6, 3,≥ 5) (6,≥ 4,≥ 4) (7, 2,≥ 9) (7, 3,≥ 5) (7,≥ 4,≥ 4)\nPCFG-IE None (3, 2,≥ 6) (4, 2,≥ 8) (4, 3,≥ 5) (5, 3,≥ 6) (5, 4,≥ 5) (6, 3,≥ 7) (6, 4,≥ 5) (6, 5,≥ 4) (7, 3,≥ 8) (7, 4,≥ 6) (7, 5,≥ 5) (7, 6,≥ 4) (5, 4,≥ 4) (6, 5,≥ 4) (7, 5,≥ 5) (7, 6,≥ 4)\n(4, 3,≥ 4) (5, 4,≥ 4)\n(6,≥ 4,≥ 4) (7,≥ 5,≥ 4)\n(3, 2,≥ 5) (4, 2,≥ 6) (4, 3,≥ 4) (5, 2,≥ 7) (5, 3,≥ 5) (5, 4,≥ 4) (6, 2,≥ 8) (6, 3,≥ 5)\n(6,≥ 4,≥ 4) (7, 2,≥ 9) (7, 3,≥ 5) (7,≥ 4,≥ 4)\n(3, 2,≥ 5) (4, 2,≥ 6) (4, 3,≥ 4) (5, 2,≥ 7)\n(5,≥ 3,≥ 4) (6, 2,≥ 8) (6, 3,≥ 5) (6,≥ 4,≥ 4) (7, 2,≥ 9) (7, 3,≥ 5) (7,≥ 4,≥ 4)\nFixed topology models. We now present some results for latent class models (LCMs) and hidden Markov models (HMMs). While identifiability for these models are more developed than for parsing models, we show that the identifiability checker can refine the results even for the classic models.\nThe parameters of an HMM are θ = (π, T,O), where π ∈ Rk specifies the initial state distribution, T ∈ Rk×k specifies the state transition probabilities, and O ∈ Rd×k specifies the emission distributions. The probability over a sentence x is:\nPθ(x) = 1>T diag(O>xL) · · ·T diag(O>x2)T diag(O>x1)π. (5)\nThe parameters of an LCM are θ = (π,O)—the same as that of an HMM except with T ≡ I. The probability over a sentence x is also given by (5) (with T = I).\nThe following table summarizes some identifiability results obtained by CheckIdentifiability (for d ≥ k); these results have all been proven analytically in previous work (e.g., [8, 10,11,20,21]) except for the identifiability of HMMs from φ∗∗.\nφ12 φ∗∗ φ123e1 φ123 φ∗∗∗e1 φ∗∗∗ LCM No Yes iff L ≥ 3 HMM No Yes iff L ≥ 3\nIt is known that LCMs are not identifiable from φ∗∗ for any value of L [8]. However, LCMs constitute a subfamily of HMMs arising from a measure zero subset of the HMM parameter space. Therefore the identifiability of HMMs from φ∗∗ (for L ≥ 3) does not contradict this result. The result does not appear to be covered by application of Kruskal’s theorem in previous work [11], so we prove the result rigorously below.\nIt can be checked using (5) that\nEθ[φ12(x)] = O diag(π)T>O> Eθ[φ34(x)] = O diag(Tπ)T>O>.\nLet M1 def = O, M2 def = OT diag(π), and D def = diag(Tπ) diag(π)−1. Provided that\n1. π > 0,\n2. O has full column rank,\n3. T is invertible,\n4. the ratios of probabilities (Tπ)i/πi, ranging over i ∈ [k], are distinct\n(all of which are true for all but a measure zero set of parameters in Θ), the matrices M1 and M2 have full column rank and the diagonal matrix D has distinct diagonal entries. Therefore Lemma 1 can be applied with X = Eθ[φ12(x)] = M1M>2 and Y = Eθ[φ34(x)] = M1DM>2 to recover M1 = O. It is easy to see that π and T can also easily be recovered.\nNote that the fourth condition above, that Tπ be entry-wise distinct from π, is violated when a LCM distribution is cast as an HMM distribution (by setting T = I so Tπ = π). However, the set of HMM parameters satisfying this equation is a measure zero set.\nDiscussion. CheckIdentifiability tests for local identifiability. If it finds that a model family is not locally identifiable, then it is not globally identifiable. However the inverse claim is not necessarily true: if it finds that a model family is locally identifiable, it is not necessarily globally identifiable. Theorem 1 provides the somewhat weaker guarantee that a restricted model family is globally identifiable, where the equivalence classes SΘ\\E(θ0) are only taken with respect to a subset Θ \\ E ⊆ Θ of the parameter space. However, there is a gap between this property (which is with respect to Θ \\ E) and true global identifiability (which is with respect to Θ).\nOn the other hand, having explicit estimators guarantees us proper global identifiability with respect to the original model family Θ. In fact, the exceptional set E can typically be characterized explicitly. For instance, in the case of PCFG-IE, the set Θ \\ E contains those θ = (π, T,O) that satisfy full rank conditions:\nΘ \\ E = {(π, T,O) : π 0, T is invertible, O has full column rank}. (6)\nAdditionally, the explicit estimators also provides an explicit characterization of the elements in the equivalence class SΘ(θ0) for each θ0 ∈ Θ \\ E : the set SΘ(θ0) contains exactly k! elements corresponding to permutation of the hidden states. Specifically,\nSΘ((π, T,O)) = {(Π−1π,Π−1TΠ, OΠ) : Π is a permutation matrix. (7)\nNote that this is shaper than Theorem 1, which only says that the equivalence classes have to be finite."
    }, {
      "heading" : "C Dynamic programs",
      "text" : "For a sentence of length L, the number of parse trees is exponential in L. Therefore, dynamic programming is often employed to efficiently compute expectations over the parse trees, the core computation in the E-step of the EM algorithm. In the case of PCFG, this dynamic program is referred to as the CKY algorithm, which runs in O(L3k3) time, where k is the number of hidden states. For simple dependency models, a O(L3) dynamic program was developed by [29]. At a high-level, the states of the dynamic program in both cases are the spans [i : j] of the sentence (and for the PCFG, the these states include the hidden states z[i:j] of the nodes).\nIn this paper, we need to compute (i) the Jacobian matrix for checking identifiability (Section 4.2) and (ii) the mixing matrix for recovering compound parameters (Section 5.1). Both computations can be performed efficiently with a modified version of the classic dynamic programs, which we will describe in this section.\nC.1 Computing the Jacobian matrix\nRecall that the j-th row of the Jacobian matrix J is (the transpose of) the gradient of hj(θ) = µj(θ)− µj(θ0). Specifically, entry Jji is the derivative of the j-th moment with respect to the i-th parameter:\nJji = ∂hj(θ)\n∂θi (8)\n= ∂Eθ[φj(x)]\n∂θi (9)\n= ∑ x,z ∂pθ(x, z) ∂θi φj(x). (10)\nWe can encode the sum over the exponential set of possible sentences x and parse trees z using a directed acyclic hypergraph so that each hyperpath through the hypergraph corresponds to a (x, z) pair. Specifically, a hypergraph consists of the following:\n• a set of nodes V with a designated start node Start ∈ V and an end node End ∈ V, and\n• a set of hyperedges E where each hyperedge e ∈ E has a source node e.a ∈ V and a pair of target nodes (e.b, e.c) ∈ V × V (we say that e connects e.a to e.b and e.c) and an index e.i ∈ [n] corresponding to a component of the parameter vector θ ∈ Rn.\nDefine a hyperpath P to be a subset of the edges E such that:\n• (Start, a, b) ∈ P for some a, b ∈ V;\n• if (a, b, c) ∈ P and b 6= End, then (b, d, e) ∈ P for some d, e ∈ V; and\n• if (a, b, c) ∈ P and c 6= End, then (c, d, e) ∈ P for some d, e ∈ V.\nEach hyperpath P , encoding (x, z), is associated with a probability equal to the product of all of the parameters on that hyperpath:\npθ(x, z) = pθ(P ) = ∏ e∈P θe.i. (11)\nIn this way, the hypergraph compactly defines a distribution over exponentially many hyperpaths. Now, we assume that each moment φj(x) corresponds to a function fj : E 7→ R mapping each hyperedge e to a real number so that the moment is equal to the product over function values:\nφj(x) = ∏ e∈P fj(e), (12)\nwhere P is any hyperpath that encodes the sentence x and some parse tree z (we assume that the product is the same no matter what z is).\nNow, let us write out the Jacobian matrix entries in terms of hyperpaths:\nJji = ∑ P ∑ e0∈P ∂θe0.i ∂θi ∏ e∈P,e6=e0 θe.ifj(e). (13)\nThe sum over hyperpaths P can be computed efficiently as follows. For each hypergraph node a, we compute an inside score α(a), which sums over all possible partial hyperpaths terminating at the target node, and an outside score β(a), which sums over all possible partial hyperpaths from the source node:\nα(a) def = ∑ e∈E:e.a=a θe.iα(e.b)α(e.c), (14) β(a) def =\n∑ e∈E:e.b=a θe.iα(e.c)β(e.a) ∑ e∈E:e.c=a θe.iα(e.b)β(e.a). (15)\nThe Jacobian entry Jji can be computed as follows: Jji = ∑ e∈E β(e.a)α(e.b)α(e.c)I[i = e.i]. (16)\nExample: PCFG. For a PCFG, nodes V have the form (i, j, s) ∈ [L]× [L]× [k], corresponding to a hidden state s over span [i : j]. For each hidden state s, we have a hyperedge e connecting e.a = Start to e.b = (s, 0, L) and e.c = End; this hyperedge has parameter index e.i corresponding to πs. For each span [i : j] with j − i > 1, split point i < m < j, and hidden states s1, s2, s3 ∈ [k], E contains a hyperedge e connecting e.a = (i, j, s1) to e.b = (i,m, s2) and e.c = (m, j, s3); the parameter index e.i corresponds to the binary production B(s2⊗ks3)s1 . For each span [i− 1 : i], hidden state s ∈ [k] and word x ∈ [d], we have a hyperedge e connecting e.a = (i − 1, i, s) to e.b = End and e.c = End with parameter index e.i corresponding to the emission Oxs.\nThe moments can be encoded as follows: For example, if φj(x) = I[xi = t], then we define fj(e) to be 0 if the source node corresponds to position i (e.a = (i− 1, i, s)) and the parameter index e.i does not correspond to Ots for some s ∈ [k], and 1 otherwise. In this way, ∏ e∈P fj(e) is zero if P encodes a sentence with xi 6= t. Higher-order moments simply correspond to hyperedge-wise multiplication of these first-order moments. For example, if φj1(x) = I[xi1 = t1] and φj2(x) = I[xi2 = t2], then the second-order moment φj(x) = I[xi1 = t1, xi2 = t2] corresponds to fj(e) = fj1(e)fj2(e).\nC.2 Computing the mixing matrix\nRecall that the mixing matrix M includes a row for each observation matrix o ∈ O and a column for each compound parameter p ∈ P. Assuming a uniform distribution over topologies, computing each entry of M reduces to counting the number of topologies t consistent with a particular compound parameter Ψp:\nMop = P(Ψo,Topology = Ψp) (17) = |Topologies |−1 ∑ t I[Ψo,t = Ψp]. (18)\nFirst, we will characterize the set of compound parameters graphically in terms of backbone structures. As an example, consider the PCFG-IE model and the observation matrix φ12 (o = 12) corresponding to the marginal distribution over the first two words of the sentence. Given a topology t, consider starting at the root, descending to the lowest common ancestor of x1 and x2, and then following both paths down to x1 and x2, respectively. We refer to this traversal as the backbone structure with respect to topology t and observation matrix φ12. See Figure 3 for an example of the backbone structure, outlined in blue.\nNote that the compound parameter Ψ12,t(θ) = Eθ[φ12(x) | Topology = t] can be written as a product over the parameter matrices, one for each edge of the backbone structure. For Figure 3, this would yield\nΨ12,1(θ) = OT diag(Tπ)T >O>. (19)\nFor general trees, we would have\nΨ12,t(θ) = OT n1 diag(Tn3π)(T>)n2O>. (20)\nfor some positive integers n1, n2, n3 corresponding to the number of edges (in t) from the common node to the preterminal node z01, the preterminal node z12, and the root z0L, respectively.\nNote that the compound parameter does not depend on the structure of t outside the backbone— that part of the topology is effectively marginalized out—so the compound parameter Ψ12,t(θ) will\nbe identical for all topologies sharing that same backbone structure. Therefore, there are only a polynomial number of compound parameters despite an exponential number of topologies t.5\nWe define a dynamic program that recursively computes Mop for the PCFG-IE model under a fixed second-order observation matrix φi0j0 . Specifically, for each span [i : j] define H(i, j) to be the set of pairs 〈t, n〉 where t is a partial backbone structure t and n is the number of partial topologies over span [i : j] which are consistent with t.\nIn the base case H(i−1, i), if i is either of the designated leaf positions defined by the observation matrix (i0 or j0), then we return the single-node backbone structure •; otherwise, we return the null backbone structure ø:\nH(i− 1, i) = { {〈ø, 1〉} if i = i0 or i = j0 {〈•, 1〉} otherwise.\n(21)\nIn the recursive case H(i, j), we consider all split points m, partial backbones t1 and t2 from H(i,m) and H(m, j), respectively, and create a new tree with t1 and/or t2 as the subtrees if they are not null:\nH(i, j) = +⋃\ni<m<j +⋃ 〈t1,n1〉∈H(i,m) +⋃ 〈t2,n2〉∈H(m,j) 〈Combine(t1, t2), n1n2〉 , (22)\nCombine(t1, t2) =  (T : t1, T : t2) if t1 6= ø and t2 6= ø, T : t1 if t1 6= ø, T : t2 if t2 6= ø, ø otherwise.\n(23)\nHere, we use the notation ⋃+ to denote a multi-set union: {〈t, n1〉}∪+{〈t, n2〉} = {〈t, n1 + n2〉}. In this notation, the backbone structure in Figure 3 would be represented as T : (T : O : •, T : O : •), which can be easily converted to the compound parameter OT diag(Tπ)T>O>.\nFor third-order observation matrices (e.g., φi0j0k0η), we add an additional case to H(i− 1, i) to return 〈◦, 1〉 if i = k0; note that k0 is represented by a special node ◦ because that observation is projected using η. The first case of Combine(t1, t2) undergoes one change: if t2 is a chain ending in ◦, then we return (T : t2, T : t1). The reason for this is best demonstrated by an example: consider topology 1 in Figure 3, and the two observation matrices φ132η and φ231η. Without the reordering, we would have the backbone structure: (T : (T : •, T : ◦), T : •) and (T : (T : ◦, T : •), T : •). However, they have the same compound parameter OT diag(T>O>η)T> diag(π)TO. This is because the contribution of a subtree ending in ◦ is simply a diagonal matrix (diag(T>O>η) in this case) which is applied on the hidden state regardless of whether it came from the left or right side.\n5One might also see why the unmixing technique does not directly apply to the PCFG-I model, where T is replaced with T1 for left edges and T2 for right edges. In that case, there are many backbone structures (and thus more compound parameters) due to the different interleavings of left and right edges."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2012,
    "abstractText" : "<lb>This paper explores unsupervised learning of parsing models along two directions. First,<lb>which models are identifiable from infinite data? We use a general technique for numerically<lb>checking identifiability based on the rank of a Jacobian matrix, and apply it to several stan-<lb>dard constituency and dependency parsing models. Second, for identifiable models, how do<lb>we estimate the parameters efficiently? EM suffers from local optima, while recent work using<lb>spectral methods [1] cannot be directly applied since the topology of the parse tree varies across<lb>sentences. We develop a strategy, unmixing, which deals with this additional complexity for<lb>restricted classes of parsing models.",
    "creator" : "LaTeX with hyperref package"
  }
}