{
  "name" : "1705.09886.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Convergence Analysis of Two-layer Neural Networks with ReLU Activation",
    "authors" : [ "Yuanzhi Li", "Yang Yuan" ],
    "emails" : [ "yuanzhil@cs.princeton.edu", "yangyuan@cs.cornell.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "In this paper, we make progress on understanding this mystery by providing a convergence analysis for SGD on a rich subset of two-layer feedforward networks with ReLU activations. This subset is characterized by a special structure called “identity mapping”. We prove that, if input follows from Gaussian distribution, with standard O(1/ √ d) initialization of the weights, SGD converges to the global minimum in polynomial number of steps. Unlike normal vanilla networks, the “identity mapping” makes our network asymmetric and thus the global minimum is unique. To complement our theory, we are also able to show experimentally that multi-layer networks with this mapping have better performance compared with normal vanilla networks.\nOur convergence theorem differs from traditional non-convex optimization techniques. We show that SGD converges to optimal in “two phases”: In phase I, the gradient points to the wrong direction, however, a potential function g gradually decreases. Then in phase II, SGD enters a nice one point convex region and converges. We also show that the identity mapping is necessary for convergence, as it moves the initial point to a better place for optimization. Experiment verifies our claims.\n1 Introduction Deep learning is the mainstream technique for many machine learning tasks, including image recognition, machine translation, speech recognition, etc. [14]. Despite its success, the theoretical understanding on how it works remains poor. It is well known that neural networks have great expressive power [19, 6, 3, 7, 27]. That is, for every function there exists a set of weights on the neural network such that it approximates the function everywhere. However, it is unclear how to obtain the desired weights. In practice, the most commonly used method is stochastic gradient descent based methods (e.g., SGD, Momentum [36], Adagrad [9], Adam [22]), but to the best of our knowledge, there were no theoretical guarantees that such methods will find good weights.\nIn this paper, we give the first convergence analysis of SGD for two-layer feedforward network with ReLU activations. For this basic network, it is known that even in the simplified setting where the weights are initialized symmetrically and the ground truth forms orthonormal basis, gradient descent might get stuck at saddle points [37]. Moreover, the origin is a singular point with noncontinuous gradient, which makes the analysis more difficult.\nInspired by the structure of residual network (ResNet) [18], we add an extra identity mapping for the hidden layer (see Figure 1). Surprisingly, we show that simply by adding this mapping, with the standard initialization scheme and small step size, SGD always converges to the ground truth. In other words, the optimization becomes significantly easier, after adding the identity mapping. See Figure 2, based on our analysis, the region near the identity matrix I contains only one global minimum without any saddle points or local minima, thus is easy for SGD to optimize. The role of the identity mapping here, is to move the initial point to this easier region.\nar X\niv :1\n70 5.\n09 88\n6v 1\n[ cs\n.L G\n] 2\n8 M\nOther than being feedforward and shallow, our network is different from ResNet in the sense that our identity mapping skips one layer instead of two. However, as we will show in Section 5.1, the skip-one-layer identity mapping already brings significant improvement to vanilla networks. Therefore, we believe our model captures one of the most important features of ResNet, and partially explains why ResNets are easier to train in practice.\nFormally, we consider the following function.\nf(x,W) = ‖ReLU((I + W)>x)‖1 (1)\nwhere ReLU(v) = max(v, 0) is the ReLU activation function. x ∈ Rd is the input vector sampled from a Gaussian distribution, and W ∈ Rd×d is the weight matrix, where d is the number of input units. Notice that I adds ei to column i of W, which makes f asymmetric in the sense that by switching any two columns in W, we get different functions. Fixing the second layer to be all one is without loss of generality, because ReLU is non-negative homogeneous, i.e., one can always multiply the input of ReLU and divide the output of ReLU by the same factor and get the same function.\nFollowing the standard setting [30, 37], we assume that there exists a two-layer teacher network with weight W∗. We train the student network using `2 loss:\nL(W) = Ex[(f(x,W)− f(x,W∗))2] (2)\nWe will define a potential function g, and show that if g is small, the gradient points to partially correct direction and we get closer to W∗ after every SGD step. However, g could be large and thus gradient might point to the reverse direction. Fortunately, we also show that if g is large, by doing SGD, it will keep decreasing until it is small enough while maintaining the weight W in a nice region. We call the process of decreasing g as Phase I, and the process of approaching W∗ as Phase II. See Figure 3 and simulations in Section 5.3.\nOur two phases framework is fundamentally different from any type of local convergence, as in Phase I, the gradient is pointing to the wrong direction to W∗, so the path from W to W∗ is non-convex, and SGD takes a long detour to arrive W∗. This framework could be potentially useful for analyzing other non-convex problems.\nTo support our theory, we have done a few other experiments and got interesting observations. For example, as predicted by our theorem, we found that for multilayer feedforward network with identity mappings, zero initialization performs as good as random initialization. At the first glance, it contradicts the common belief “random initialization is necessary to break symmetry”, but actually the identity mapping itself serves as the asymmetric component. See Section 5.4.\nAnother common belief is that neural network has lots of local minima and saddle points [8], so even if there exists a global minimum, we may not be able to arrive there. As a result, even when the teacher network is shallow, the student network usually needs to be deeper, otherwise it will underfit. However, both our theorem and our experiment show that if the shallow teacher network is in a pretty large region near identity (Figure 2), SGD always converges to the global minimum by initializing the weights I + W in this region, with equally shallow student network. By contrast, wrong initialization gets stuck at local minimum and underfit. See Section 5.2.\nRelated Work Expressivity. Even two-layer network has great expressive power. For example, two-layer network with sigmoid activations could approximate any continuous function [19, 6, 3]. ReLU is the state-of-the-art activation function, and performs much better than sigmoid [26, 12], but fewer and weaker results on expressivity of feedforward ReLU networks are known [25, 28, 27].\nLearning. Most previous results on learning neural network are negative [35, 24, 34], or positive but with special algorithms other than SGD [20, 39, 33, 13], or with strong assumptions on the model [1, 2]. [31] proved that with high probability, there exists a continuous decreasing path from random initial point to the global minimum, but SGD may not follow this path.\nLinear network and independent activation. Some previous works simplified the model by ignoring the activation functions and considering deep linear networks [32, 21] or deep linear residual networks [16], which can only learn linear functions. Some previous results are based on independent activation assumption that the activations of ReLU and the input are independent, which is unrealistic [4, 21].\nSaddle points. It is observed that saddle point is not a big problem for neural networks [8, 15]. In general, if the objective is strict-saddle [10], SGD could escape all saddle points.\n2 Preliminaries Denote x as the input vector in Rd. For now, we first consider x sampled from normal distribution N (0, I). Denote W∗ = (w∗1 , · · · , w∗n) ∈ Rd×d as the weights for the teacher network, W = (w1, · · · , wn) ∈ Rd×d as the weights for the student network, where w∗i , wi ∈ Rd are column vectors. f(x,W∗), f(x,W) are defined in (1), representing the teacher and student network.\nWe want to know whether a randomly initialized W will converge to W∗, if we run SGD with l2 loss defined in (2). Alternatively, we can write the loss L(W) as\nEx[(ΣiReLU(〈ei + wi, x〉)− ΣiReLU(〈ei + w∗i , x〉))2]\nTaking derivative with respect to wj , we get\n∇L(W)j = 2Ex [(∑\ni\nReLU(〈ei + wi, x〉)− ∑\ni\nReLU(〈ei + w∗i , x〉) ) x1〈ej+wj ,x〉≥0 ]\nwhere 1e is the indicator function that equals 1 if the event e is true, and 0 otherwise. Here ∇L(W) ∈ Rd×d, and∇L(W)j is its j-th column.\nDenote θi,j as the angle between ei +wi and ej +wj , θi∗,j as the angle between ei +w∗i and ej +wj . Denote v̄ = v‖v‖2 . Denote I + W\n∗ and I + W∗ as the column-normalized version of I + W∗ and I + W such that every column has unit norm. Since the input is from a normal distribution, one can compute the expectation inside the gradient as follows.\nLemma 2.1 (Eqn (13) from [37]). If x ∼ N (0, I), then −∇L(W)j = ∑d i=1 ( π 2 (w ∗ i − wi) + ( π 2 − θi∗,j ) (ei + w∗i )− ( π 2 − θi,j ) (ei + wi) + ( ‖ei + w∗i ‖2 sin θi∗,j − ‖ei + wi‖2 sin θi,j ) ej + wj )\nDenote u ∈ Rd as the all one vector. Denote Diag(W) as the diagonal matrix of matrix W, Diag(v) as a diagonal matrix whose main diagonal equals to the vector v. Denote Off-Diag(W) , W − Diag(W). Denote [d] as the set {1, · · · , d}. Throughout the paper, we abuse the notation of inner product between matrices W,W∗,∇L(W), such that 〈∇L(W),W〉 means the summation of the entrywise products. ‖W‖2 is the spectral norm of W, and ‖W‖F is the Frobenius norm of W. We define the potential function g and variables gj ,Aj ,A below, which will be useful in the proof. Definition 2.2. We define the potential function g , ∑d i=1(‖ei + w∗i ‖2 − ‖ei + wi‖2), and variable gj ,∑\ni6=j(‖ei + w∗i ‖2 − ‖ei + wi‖2).\nDefinition 2.3. Denote Aj , ∑ i 6=j((ei+w ∗ i )ei + w ∗ i >−(ei+wi)ei + wi>),A , ∑d i=1((ei+w ∗ i )ei + w ∗ i >− (ei + wi)ei + wi > ) = (I + W∗)I + W∗ > − (I + W)I + W>.\nIn this paper, we consider the standard SGD with mini batch method for training the neural network. Assume W0 is the initial point, and in step t > 0, we have the following updating rule:\nWt+1 = Wt − ηtGt\nwhere the stochastic gradient Gt = ∇L(Wt) + Et with E[Et] = 0 and ‖Et‖F ≤ ε. Let G2 , 6dγ + ε,GF , 6d1.5γ + ε. As we will see in Lemma B.2, they are the upper bound of ‖Gt‖2 and ‖Gt‖F respectively.\nIt’s clear that L is not convex, In order to get convergence guarantees, we need a weaker condition called one point convexity.\nDefinition 2.4 (One point strongly convexity). A function f(x) is called δ-one point strongly convex in domain D with respect to point x∗, if ∀x ∈ D, 〈−∇f(x), x∗ − x〉 > δ‖x∗ − x‖22.\nBy definition, if a function f is strongly convex, it is also one point strongly convex in the entire space with respect to the global minimum. However, the reverse is not necessarily true, e.g., see Figure 4. If a function is one point strongly convex, then in every step a positive fraction of the negative gradient is pointing to the optimal point. As long as the step size is small enough, we will finally arrive the optimal point, possibly by a winding path. See Figure 3 for illustration, where starting from W6 (Phase II), we get closer to W∗ in every step. Formally, we have the following lemma.\nLemma 2.5. For function f(W), consider the SGD update Wt+1 = Wt − ηGt, where E[Gt] = ∇f(Wt), E[‖Gt‖2F ] ≤ G2. Suppose for all t, Wt is always inside the δ-one point strongly convex region with diameter D, i.e., ‖Wt −W∗‖F ≤ D. Then for any α > 0 and any T such that Tα log T ≥ D 2δ2 (1+α)G2 , if η = (1+α) log T δT , we have ‖WT −W∗‖2F ≤ (1+α) log TG 2 δ2T .\nThe proof can be found in Appendix I. Lemma 2.5 uses constant step size, so it easily fits the standard practical scheme that shrinks η by a factor of 10 after every a few epochs. For example, we may apply Lemma 2.5 every time η gets changed.\n3 Main Theorem Theorem 3.1 (Main Theorem). There exists constants γ > γ0 > 0 such that If x ∼ N (0, I), ‖W0‖2, ‖W∗‖2 ≤ γ0, d ≥ 100, ε ≤ γ2, then SGD for L(W) will find the ground truth W∗ by two phases. In Phase I, by setting\nη ≤ γ2 G22 , the potential function will keep decreasing until it is smaller than 197γ2, which takes at most 116η steps. In Phase II, for any α > 0 and any T such that Tα log T ≥ 36d 1004(1+α)G2F , if we set η = (1+α) log TδT , we have ‖WT −W∗‖2F ≤ 1002(1+α) log TG2F 9T .\nRemarks. Randomly initializing the weights with O(1/ √ d) is standard in deep learning, see [23, 11, 17]. It\nis also well known that if the entries are initialized with O(1/ √ d), the spectral norm of the random matrix is O(1) [29]. So our result matches with the common practice. Moreover, as we will show in Section 5.5, networks with small average spectral norm already have good performance. Thus, our assumption ‖W∗‖2 = O(1) is reasonable. Notice that here we assume the spectral norm of W∗ to be constant, which means the Frobenius norm ‖W∗‖F could be as big as O( √ d).\nThe assumption that the input follows a Gaussian distribution is not necessarily true in practice (Although this is a common assumption appeared in the previous papers [4, 37, 38], and also considered plausible in [5]). We could easily generalize the analysis to rotation invariant distributions, and potentially more general distributions (see Section 6). Moreover, previous analyses either ignore the nonlinear activations and thus consider linear model [32, 21, 16], or directly [4, 21] or indirectly [37]1 assume that the activations are independent. By contrast, in our model the ReLU activations are highly correlated2 as ‖W‖2, ‖W∗‖2 = Ω(1). As pointed out by [5], eliminating the unrealistic assumptions on activation independence is the central problem of analyzing the loss surface of neural network, which was not fully addressed by the previous analyses.\nTo prove the main theorem, we split the process and present the following two theorems, which will be proved in Appendix B and C.\nTheorem 3.2 (Phase I). There exists a constant γ > γ0 > 0 such that If ‖W0‖2, ‖W∗‖2 ≤ γ0, d ≥ 100, η ≤ γ 2\nG22 ,\nε ≤ γ2, then gt will keep decreasing by a factor of 1− 0.5ηd for every step, until gt1 ≤ 197γ2 for step t1 ≤ 116η . After that, Phase II starts. That is, for every T > t1, we have ‖WT ‖2 ≤ 1100 and gT ≤ 0.1.\nTheorem 3.3 (Phase II). There exists a constant γ such that if ‖W‖2, ‖W∗‖2 ≤ γ, and g ≤ 0.1, then 〈−∇L(W),W∗ −W〉 = ∑dj=1〈−∇L(W)j , w∗j − wj〉 > 0.03‖W∗ −W‖2F .\nWith these two theorems, we get the main theorem immediately.\nProof for Theorem 3.1. By Theorem 3.2, we know the statement for Phase I is true, and we will enter phase II in 116η steps. After entering Phase II, based on Theorem 3.3, we simply use Lemma 2.5 by setting δ = 0.03, D = √ d\n50 , G = GF to get the convergence guarantee.\n4 Overview of the Proofs General Picture. In many convergence analyses for non-convex functions, one would like to show that L is one point strongly convex, and directly apply Lemma 2.5 to get the convergence result. However, this is not true for 2-layer neural network, as the gradient may point to the wrong direction, see Section 5.3.\nSo when is our L one point convex? Consider the following thought experiment: First, suppose ‖W‖2, ‖W∗‖2 → 0, we know ‖wi‖2, ‖w∗i ‖2 also go to 0. Thus, ei + wi and ei + w∗i are close to ei. As a result, θi,j , θi∗,j ≈ π2 , and θi∗,i ≈ 0. Based on Lemma 2.1, this gives us a naïve approximation of the negative gradient, i.e., −∇L(W)j ≈ π2 (w∗j − wj) + π2 ∑d i=1(w ∗ i − wi) + ej + wj ∑ i 6=j(‖ei + w∗i ‖2 − ‖ei + wi‖2) .\nWhile the first two terms π2 (w ∗ j − wj) and π2 ∑d i=1(w ∗ i − wi) have positive inner product with W∗ −W,\nthe last term gj = ej + wj ∑ i 6=j(‖ei + w∗i ‖2 − ‖ei + wi‖2) can point to arbitrary direction. If the last term is small, it can be covered by the first two terms, and L becomes one point strongly convex. So we define a potential function closely related to the last term: g = ∑d i=1(‖ei +w∗i ‖2−‖ei +wi‖2). We show that if g is small enough, L is also one point strongly convex (Theorem 3.3).\n1They assume input is Gaussian and the W∗ is orthonormal, which means the activations are independent in teacher network. 2 Let σi be the output of i-th ReLU unit, then in our setting, ∑ i,j Cov[σi, σj ] can be as large as Ω(d), which is far from being independent.\nHowever, from random initialization, g can be as large as of Ω( √ d), which is too big to be covered. Fortunately, we show that if g is big, it will gradually decrease simply by doing SGD on L. More specifically, we introduce a two phases convergence analysis framework:\n1. In Phase I, the potential function g is decreasing to a small value. 2. In Phase II, g remains small, so L is one point convex and thus W starts to converge to W∗. We believe that this framework could be helpful for other non-convex problems. Technical difficulty: Phase I. Our key technical challenge is to show that in Phase I, the potential function actually decreases to O(1) after polynomial number of iterations. However, we cannot show this by merely looking at g itself. Instead, we introduce an auxiliary variable s = (W∗ −W)u. By doing a careful calculation, we get their joint update rules (Lemma B.3 and Lemma B.4):\n{ st+1 ≈ st − πηd2 st + ηO( √ dgt + √ dγ)\ngt+1 ≈ gt − ηdgt + ηO(γ √ d‖st‖2 + dγ2)\nSolving this dynamics, we can show that gt will approach to (and stay around) O(γ), thus we enter Phase II. Technical difficulty: Phase II. Although the overall approximation in the thought experiment looks simple, the argument is based on an over simplified assumption that θi∗,j , θi,j ≈ π2 for i 6= j. However, when W∗ has constant spectral norm, even when W is very close to W∗, θi,j∗ could be constantly far away from π2 , which prevents us from applying this approximation directly. To get a formal proof, we use the standard Taylor expansion and control the higher order terms. Specifically, we write θi∗,j as θi∗,j = arccos〈ei + w∗i , ej + wj〉 and expand arccos at point 0, thus,\nθi∗,j = π\n2 − 〈ei + w∗i , ej + wj〉+O(〈ei + w∗i , ej + wj〉3)\nHowever, even when W ≈W∗, the higher order term O(〈ei + w∗i , ej + wj〉3) still can be as large as a constant, which is too big for us. Our trick here is to consider the “joint Taylor expansion”:\nθi∗,j − θi,j = 〈ei + wi, ej + wj〉 − 〈ei + w∗i , ej + wj〉+O(|〈ei + w∗i , ej + wj〉3 − 〈ei + wi, ej + wj〉3|)\nAs W approaches W∗, |〈ei + w∗i , ej + wj〉3 − 〈ei + wi, ej + wj〉3| also tends to zero, therefore our approximation has bounded error.\nIn the thought experiment, we already know that the constant part in the Taylor expansion of ∇L(W) is π 2 −O(g)-one point convex. We show that after taking inner product with W∗−W, the first order terms are lower bounded by (roughly) −1.3‖W∗−W‖2F and the higher order terms are lower bounded by −0.085‖W∗−W‖2F . Adding them together, we can see that L(W) is one point convex as long as g is small. See Figure 5.\nGeometric Lemma. In order to get through the whole analysis, we need tight bounds on a few common terms that appear everywhere. Instead of using naïve algebraic techniques, we come up with a nice geometric proof to get nearly optimal bounds. See Section D.\nFlowchart of the proofs. Although the proofs of our theorems are intricate, many lemmas have clear intuition behind the statement. Therefore, we add “*” to these lemmas, so that time constrained readers could feel confident to skip the proofs. We also plot a flowchart of the proofs in Figure 6 to help the readers spend time wisely.\nSince the proofs are long and complicated, we choose to present them in a top-down way. That is, we present the main theorems (Theorem 3.1, Theorem 3.2, and Theorem 3.3) in the main paper, and then present the necessary lemmas in order to prove those main theorems in Section A, Section B and Section C. Finally, we present the proofs for those lemma in Section F, Section G and Section H, respectively.\n5 Experiments In this section, we present several simulation results to support our theory. Our code can be found in Github3 and the supplementary materials.\n5.1 Importance of identity mapping In this experiment, we compare the standard ResNet [18] and single skip model where identity mapping skips only one layer. We used the open source code from Facebook4, and made minor modifications to get the single skip model. See Figure 7. We also ran the vanilla network, where the identity mappings are completely removed.\nIn this experiment, we choose Cifar-10 as the dataset, and all the networks have 56-layers. Other than the identity mappings, all other settings are identical and default, including network width, initialization, optimization methods, etc. We run the experiments for 5 times for both single skip model and vanilla network, and report the average test error. See Table 1 for the results.\nAs we can see, compared with vanilla network, by simply using a single skip identity mapping, one can already improve the test error by 3.03%, and is 2.04% close to the ResNet. So we claim that a single skip identity mapping already brings significant improvement on test accuracy.\n5.2 Global minimum convergence In this experiment, we verify our main theorem that for two-layer teacher network and student network with identity mappings, as long as ‖W0‖2, ‖W∗‖2 is small, SGD always converges to the global minimum W∗, thus gives almost 0 training error and test error. We consider three student networks. The first one (ResLink) is defined using (2), the second one (Vanilla) is the same model without the identity mapping. The last one (3-Block) is a three block network with each block containing a linear layer (500 hidden nodes), a batch normalization and a ReLU layer. The teacher network always shares the same structure as the student network.\n3https://github.com/anonymous 4https://github.com/facebook/fb.resnet.torch.git\nThe input dimension is 100. We generated a fixed W∗ for all the trials with ‖W∗‖2 ≈ 0.6, ‖W∗‖F ≈ 5.7. We generated a training set of size 100, 000, and test set of size 10, 000, sampled from a Gaussian distribution. We use batch size 200, step size 0.001. We run ResLink for 5 times with random initialization (‖W‖2 ≈ 0.6 and ‖W‖F ≈ 5), and plot the curves by taking the average.\nFigure 8(a) shows test error and training error of the three networks. Comparing Vanilla with 3-Block, we find that 3-Block is more expressive, so its training error is smaller; but it suffers from overfitting and has bigger test error. This is the standard overfitting vs underfitting tradeoff. Surprisingly, with only one hidden layer, ResLink has both zero test error and training error. If we look at Figure 8(b), we know the distance between W and W∗ converges to 0, meaning ResLink indeed finds the global optimal in all 5 trials. By contrast, Vanilla, which is essentially the same network with different initialization, could not converge to the global optimal5. This is exactly what our theory predicted.\n5.3 Verify the dynamics In this experiment, we verify our claims on the dynamics. Based on the analysis, we construct a 1500× 1500 matrix W s.t. ‖W‖2 ≈ 0.15, ‖W‖F ≈ 5 , and set W∗ = 0. By plugging them into (2), one can see that even in this simple case that W∗ = 0, initially the gradient is pointing to the wrong direction, i.e., not one point convex. We then run SGD on W by using samples x from Gaussian distribution, with batch size 300, step size 0.0001.\nFigure 9(a) shows the first 100 iterations. We can see that initially the inner product defined in Definition 2.4 is negative, then after about 15 iterations, it turns positive, which means W is in the one point strongly convex region. At the same time, the potential g keeps decreasing to a small value, while the distance to optimal (which also equals to ‖W‖F in this experiment) is not affected. They precisely match with our description of Phase I in Theorem 3.2.\n5To make comparison meaningful, we set W − I to be the actual weight for Vanilla as its identity mapping is missing, which is why it has a much bigger initial norm.\nAfter that, we enter Phase II and slowly approach to W∗, see Figure 9(b). Notice that the potential g is always very small, the inner product is always positive, and the distance to optimal is slowly decreasing. Again, they precisely match with our Theorem 3.3.\n5.4 Zero initialization works In this experiment, we used a simple 5-block neural network on MNIST, where every block contains a 784∗784 feedforward layer, an identity mapping, and a ReLU layer. Cross entropy criterion is used. We compare zero initialization with standard O(1/ √ d) random initialization. We found that for zero initialization, we can get 1.28% test error, while for random initialization, we can get 1.27% test error. Both results were obtained by taking average among 5 runs and use step size 0.1, batch size 256. If the identity mapping is removed, zero initialization no longer works.\n5.5 Spectral norm of W∗\nWe have run a few experiments to find feedforward networks with identity mapping, small ‖W‖2 and good expressivity. For 15-block network, where each block contains a 784×784 feedforward layer, an identity mapping and a ReLU layer, we found that, in order to achieve 2.14% test error for MNIST, it suffices to have average ‖W‖2 ≈ 0.34 among all 15 feedforward layers. Actually, when the network becomes deeper, the average norm of the weight matrices will further decrease, otherwise the output of the network will explode. See for example [16] for justification.\nWe also applied the exact model f defined in (1) to distinguish two classes in MNIST. For any input image x, We say it’s in class A if f(x,W) < TA,B , and in class B otherwise. Here TA,B is the optimal threshold for the function f(x,0) to distinguish A and B. If W = 0, we get 7% training error for distinguish class 0 and class 1. However, it can be improved to 1% with ‖W‖2 = 0.6. We tried this experiment for all possible 45 pairs of classes in MNIST, and improve the average training error from 34% (using W = 0) to 14% (using ‖W‖2 = 0.6). Therefore our model with ‖W‖2 = Ω(1) has reasonable expressive power, and is substantially different from just using the identity mapping alone.\n6 Discussions The assumption that the input is Gaussian can be relaxed in several ways. For example, when the distribution is N (0,Σ) where ‖Σ− I‖2 is bounded by a small constant, the same result holds with slightly worse constants. Moreover, since the analysis relies Lemma 2.1, which is proved by converting the original input space into polar space, it is easy to generalize the calculation to rotation invariant distributions. Finally, for more general distributions, as long as we could explicitly compute the expectation, which is in the form of O(W∗ −W) plus certain potential function, our analysis framework may also be applied.\nThere are many exciting open problems. For example, Our paper is the first one that gives solid SGD analysis for neural network with nonlinear activations, without unrealistic assumptions like independent activation assumption. It would be great if one could further extend it to multiple layers, which would be a major breakthrough of understanding optimization for deep learning. Moreover, our two phase framework could be applied to other non-convex problems as well.\nReferences [1] Alexandr Andoni, Rina Panigrahy, Gregory Valiant, and Li Zhang. Learning polynomials with neural\nnetworks. In ICML, pages 1908–1916, 2014.\n[2] Sanjeev Arora, Aditya Bhaskara, Rong Ge, and Tengyu Ma. Provable bounds for learning some deep representations. In Proceedings of the 31th International Conference on Machine Learning, ICML 2014, Beijing, China, 21-26 June 2014, pages 584–592, 2014.\n[3] Andrew R. Barron. Universal approximation bounds for superpositions of a sigmoidal function. IEEE Trans. Information Theory, 39(3):930–945, 1993.\n[4] Anna Choromanska, Mikael Henaff, Michaël Mathieu, Gérard Ben Arous, and Yann LeCun. The loss surfaces of multilayer networks. In AISTATS, 2015.\n[5] Anna Choromanska, Yann LeCun, and Gérard Ben Arous. Open problem: The landscape of the loss surfaces of multilayer networks. In Proceedings of The 28th Conference on Learning Theory, COLT 2015, Paris, France, July 3-6, 2015, pages 1756–1760, 2015.\n[6] George Cybenko. Approximation by superpositions of a sigmoidal function. MCSS, 5(4):455, 1992.\n[7] Amit Daniely, Roy Frostig, and Yoram Singer. Toward deeper understanding of neural networks: The power of initialization and a dual view on expressivity. In NIPS, pages 2253–2261, 2016.\n[8] Yann N Dauphin, Razvan Pascanu, Caglar Gulcehre, Kyunghyun Cho, Surya Ganguli, and Yoshua Bengio. Identifying and attacking the saddle point problem in high-dimensional non-convex optimization. In NIPS 2014, pages 2933–2941, 2014.\n[9] John C. Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and stochastic optimization. Journal of Machine Learning Research, 12:2121–2159, 2011.\n[10] Rong Ge, Furong Huang, Chi Jin, and Yang Yuan. Escaping from saddle points - online stochastic gradient for tensor decomposition. In COLT 2015, volume 40, pages 797–842, 2015.\n[11] Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural networks. In AISTATS, pages 249–256, 2010.\n[12] Xavier Glorot, Antoine Bordes, and Yoshua Bengio. Deep sparse rectifier neural networks. In AISTATS, pages 315–323, 2011.\n[13] Surbhi Goel, Varun Kanade, Adam R. Klivans, and Justin Thaler. Reliably learning the relu in polynomial time. CoRR, abs/1611.10258, 2016.\n[14] Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning. MIT Press, 2016. http://www. deeplearningbook.org.\n[15] Ian J. Goodfellow and Oriol Vinyals. Qualitatively characterizing neural network optimization problems. CoRR, abs/1412.6544, 2014.\n[16] Moritz Hardt and Tengyu Ma. Identity matters in deep learning. CoRR, abs/1611.04231, 2016.\n[17] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In ICCV, pages 1026–1034, 2015.\n[18] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, pages 770–778, 2016.\n[19] Kurt Hornik, Maxwell B. Stinchcombe, and Halbert White. Multilayer feedforward networks are universal approximators. Neural Networks, 2(5):359–366, 1989.\n[20] Majid Janzamin, Hanie Sedghi, and Anima Anandkumar. Beating the perils of non-convexity: Guaranteed training of neural networks using tensor methods. arXiv preprint arXiv:1506.08473, 2015.\n[21] Kenji Kawaguchi. Deep learning without poor local minima. In NIPS, pages 586–594, 2016.\n[22] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. CoRR, abs/1412.6980, 2014.\n[23] Yann LeCun, Leon Bottou, Genevieve B. Orr, and Klaus Robert Müller. Efficient BackProp, pages 9–50. Springer Berlin Heidelberg, Berlin, Heidelberg, 1998.\n[24] Roi Livni, Shai Shalev-Shwartz, and Ohad Shamir. On the computational efficiency of training neural networks. In NIPS, pages 855–863, 2014.\n[25] Guido F. Montúfar, Razvan Pascanu, KyungHyun Cho, and Yoshua Bengio. On the number of linear regions of deep neural networks. In NIPS, pages 2924–2932, 2014.\n[26] Vinod Nair and Geoffrey E. Hinton. Rectified linear units improve restricted boltzmann machines. In ICML, pages 807–814, 2010.\n[27] Xingyuan Pan and Vivek Srikumar. Expressiveness of rectifier networks. In ICML, pages 2427–2435, 2016.\n[28] Razvan Pascanu, Guido Montúfar, and Yoshua Bengio. On the number of inference regions of deep feed forward networks with piece-wise linear activations. CoRR, abs/1312.6098, 2013.\n[29] M. Rudelson and R. Vershynin. Non-asymptotic theory of random matrices: extreme singular values. ArXiv e-prints, 2010.\n[30] David Saad and Sara A. Solla. Dynamics of on-line gradient descent learning for multilayer neural networks. Advances in Neural Information Processing Systems, 8:302–308, 1996.\n[31] Itay Safran and Ohad Shamir. On the quality of the initial basin in overspecified neural networks. In ICML, pages 774–782, 2016.\n[32] Andrew M. Saxe, James L. McClelland, and Surya Ganguli. Exact solutions to the nonlinear dynamics of learning in deep linear neural networks. CoRR, abs/1312.6120, 2013.\n[33] Hanie Sedghi and Anima Anandkumar. Provable methods for training neural networks with sparse connectivity. ICLR, 2015.\n[34] Ohad Shamir. Distribution-specific hardness of learning neural networks. CoRR, abs/1609.01037, 2016.\n[35] Jirí Síma. Training a single sigmoidal neuron is hard. Neural Computation, 14(11):2709–2728, 2002.\n[36] Ilya Sutskever, James Martens, George E. Dahl, and Geoffrey E. Hinton. On the importance of initialization and momentum in deep learning. In ICML, pages 1139–1147, 2013.\n[37] Yuandong Tian. Symmetry-breaking convergence analysis of certain two-layered neural networks with relu nonlinearity. In Submitted to ICLR 2017, 2016.\n[38] Bo Xie, Yingyu Liang, and Le Song. Diversity leads to generalization in neural networks. In AISTATS, 2017.\n[39] Yuchen Zhang, Jason D. Lee, Martin J. Wainwright, and Michael I. Jordan. Learning halfspaces and neural networks with random initialization. CoRR, abs/1511.07948, 2015.\nA Compute Approximation Matrix The exact form of −∇L(W)j in Lemma 2.1 contains variables like θi∗,j , θi,j , sin θi∗,j , sin θi,j , which are hard to deal with. In this section, we compute the approximation of these terms using Taylor series, and show that the approximation loss is minor. While the proofs are technically involved, the claims themselves are not surprising. Hence, we encourage the readers to skip the proofs (Appendix F) for the first reading.\nDefine the j-th column of the approximation matrix P as follows. See Definition 2.2 and Definition 2.3 for gj ,Aj .\nPj , P1,j + P2,j + P3,j , where\nP1,j , d∑\ni=1\nπ 2 (w∗i − wi),\nP2,j , gjej + wj + (\nI− 1 2 ej + wj · ej + wj>\n) Ajej + wj ,\nP3,j , (π\n2 − θj∗,j\n) (ej + w ∗ j )− π\n2 (ej + wj) + ‖ej + w∗j ‖ sin θj∗,jej + wj .\nTreat P1,j ,P2,j ,P3,j as j-th column of matrix P1,P2,P3 respectively, we have P = P1 + P2 + P3. Although P depends on W, we abuse the notation and simply write P.\nClaim A.1. Pj approximates−∇L(W)j by setting (π2−θi,j) ≈ 〈ei + wi, ej + wj〉, (π2−θi∗,j) ≈ 〈ei + w∗i , ej + wj〉, sin θi,j ≈ 1− 12 〈ei + wi, ej + wj〉2 and sin θi∗,j ≈ 1− 12 〈ei + w∗i , ej + wj〉2.\nBelow we show that the approximation loss is negligible in terms of one point convexity and spectral norm.\nLemma* A.2. If ‖W‖2, ‖W∗‖2 ≤ γ ≤ 1100 , |〈P +∇L(W),W∗ −W〉| < 0.085‖W∗ −W‖2F .\nLemma* A.3. If ‖W‖2, ‖W∗‖2 ≤ γ ≤ 1100 , ‖P +∇L(W)‖2 ≤ 3.5γ2.\nB Phase I: The Decreasing Potential Function As we saw in Theorem 3.3, if ‖W‖2, ‖W∗‖2 is bounded by a constant γ = 1100 , and the potential function g ≤ 0.1, L(W) is 0.03-one point convex, which will give us convergence guarantee according to Lemma 2.5. However, g could be larger than 0.1 initially, and as we run SGD, ‖W‖2 might be larger than 1100 as well.\nIn this section, we address both problems by analyzing the dynamics of SGD, thus prove Theorem 3.2. The proofs can be found in Appendix G. Before proceeding to the interesting stuff, we need a simpler form of∇L(W) to work with, see below.\nLemma B.1. If ‖W‖2, ‖W∗‖2 ≤ γ ≤ 1100 , the negative gradient of L(W) is approximately\nQ(W) , π 2\n(W∗ −W) ( I + uu> ) + (W∗ −W)> − 2Diag(W∗ −W) + gI + W\nwhere u is the all 1 vector. The approximation error is ‖Q(W)− [−∇L(W)]‖2 ≤ 61γ2.\nWe immediately get the bound of the gradient norm.\nLemma* B.2. If ‖W‖2, ‖W∗‖2 ≤ γ ≤ 1100 , ‖∇L(W)‖2 ≤ 6dγ.\nNow we are ready to analyze the dynamics. We use subscript t under each variable to denote its value at the step t. For simplicity, let Qt , Q(Wt). Define st , (W∗ −Wt)u. We first compute the updating rule for gt.\nLemma B.3. If ‖Wt‖2, ‖W∗‖2 ≤ γ ≤ 1100 , d ≥ 100, η ≤ γ2 G22 , then |gt+1| ≤ (1 − 0.95ηd)|gt| + 86ηdγ2 +\n1.03η √ dε+ 4.8η‖st‖2γ √ d.\nThe bound contains ‖st‖2 which could be large, so we also need to compute its updating rule:\nLemma B.4. If ‖Wt‖2, ‖W∗‖2 ≤ γ ≤ 1100 , then ‖st+1‖2 ≤ ( 1− η (d+1)π2 ) ‖st‖2+η(6.61γ+1.03|gt|+ε) √ d.\nCombining the two lemmas, we are ready to show that gt will shrink, conditioned on that ‖Wt‖2 is bounded by γ.\nLemma B.5. If for every step t > 0, ‖Wt‖2, ‖W∗‖2 ≤ γ ≤ 1100 ,d ≥ 100, η ≤ γ2 G22 , ε ≤ γ2, then |gt| will keep decreasing by a factor of 1− 0.5ηd for every step, until |gt1 | ≤ 197γ2 for t1 ≤ 116η .\nFortunately, we also know that ‖Wt‖2 is always bounded by γ during the process described in Lemma B.5.\nLemma B.6. There exists a constant γ > γ0 > 0 such that if ‖W0‖2, ‖W∗‖2 ≤ γ0, d ≥ 100, η ≤ γ 2 G22 , ε ≤ γ2, then in the process of Phase I (Lemma B.5), we always have ‖WT ‖2 ≤ γ ≤ 1100 for any T > 0.\nNow, we are at the state where |gt| is small, and ‖WT ‖2 ≤ γ, which means we are in Phase II. The next lemma ensures that we will stay in Phase II forever.\nLemma B.7. There exists a constant γ0 > γ > 0 such that if ‖W0‖2, ‖W∗‖2 ≤ γ0, d ≥ 100, η ≤ γ 2 G22 , ε ≤ γ2, then after |gt1 | ≤ 197γ2, Phase I ends and Phase II starts. That is, for every T > t1, ‖WT ‖2 ≤ γ and |gT | ≤ 0.1.\nProof for Theorem 3.2. We immediately get Theorem 3.2 by combining the above three lemmas. They show that gt will decrease to a small value in Phase I (Lemma B.5), ‖Wt‖2 will keep small during this process (Lemma B.6), and they all keep small afterwards (Lemma B.7).\nC Phase II: One Point Convexity In this section, we prove Theorem 3.3. See detailed proofs in Appendix H. Using Lemma A.2, it suffices to bound\n〈P,W∗ −W〉 = d∑\nj=1\n〈P1,j + P2,j + P3,j , w∗j − wj〉\nHere the first term is easy to calculate.\nd∑\nj=1\n〈P1,j , w∗j − wj〉 = π\n2 ∥∥∥∥∥ d∑\ni=1\n(w∗i − wi) ∥∥∥∥∥ 2\n2\n≥ 0 (3)\nFor notational simplicity, denote\nxj , ( ej + wj · ej + wj> ) (w∗j − wj),\nX , (x1, · · · , xd) (4) zj , (\nI− 1 2 ej + wj · ej + wj>\n) (w∗j − wj) (5)\nBy Definition of P2,j and (5), we have\nd∑\nj=1\n〈P2,j , w∗j − wj〉 = d∑\nj=1\n〈 gjej + wj , w ∗ j − wj 〉 + d∑\nj=1\nz>j Ajej + wj (6)\nWe bound the above two terms separately below.\nO A B\nC D\nE\nFGH\nLemma C.3. If ‖W‖2, ‖W∗‖2 ≤ γ ≤ 1100 , ∑d j=1〈P3,j , w∗j − wj〉 ≥ ( π 2 − 0.021 ) ‖W∗ −W‖2F .\nProof of Theorem 3.3. By (3), (6), Lemma C.1, Lemma C.2 and Lemma C.3, we know\n〈P,W∗ −W〉 ≥ ( π\n2 − 1.321− 8γ − (1 + γ)g 2(1− 2γ)\n) ‖W∗ −W‖2F > ( 0.169− (1 + γ)g\n2(1− 2γ)\n) ‖W∗ −W‖2F\nUsing Lemma A.2, we get\n〈−∇L(W),W∗ −W〉 > (\n0.084− (1 + γ)g 2(1− 2γ)\n) ‖W∗ −W‖2F > 0.03‖W∗ −W‖2F\nThe last inequality holds when g ≤ 0.1.\nD A Geometric Lemma In our proof, we need very tight bounds for a few terms. In order to get such bounds, we present a nice and intuitive geometric lemma as follows.\nLemma D.1. If ‖W‖2, ‖W∗‖2 ≤ γ, then ∀i ∈ [d],\n1. ‖ei + w∗i − ei + wi‖2 ≤ ‖(I−ei+wi·ei+wi>)(w∗i−wi)‖2√ 1−2γ ≤ ‖w∗i−wi‖2√ 1−2γ\n2. −‖w ∗ i−wi‖22\n2(1−2γ) ≤ 〈ei + w∗i − ei + wi, ei + wi〉 ≤ 0\n3. if γ ≤ 1100 ,0 ≤ θi,i∗ ≤ 1.001‖w∗i − wi‖2.\nProof. See Figure 10. Denote ei+w∗i as −−→ OC, ei+wi as −−→ OD, ei + w∗i as −→ OA, ei + wi as −−→ OB. Thus, ‖w∗i−wi‖2 = ‖−−→DC‖2. 1. Since −−→ OD⊥−−→CF , we know ‖−−→CD‖2 ≥ ‖ −−→ CF‖2. Since4CFO ∼ 4AEO, we know\n‖−−→CD‖2 ‖−→AE‖2 ≥ ‖ −−→ CF‖2 ‖−→AE‖2 = ‖−−→OC‖2 ‖−→OA‖2 = ‖ei + w∗i ‖2 ≥ 1− γ (7)\nThe last inequality holds as ‖W∗‖2 ≤ γ. Notice that ‖−→OA‖2 = ‖ −−→ OB‖2 = 1, we know4ABO is a isosceles triangle. Thus, ‖ −→ AG‖2 = ‖ −−→ GB‖2. Notice that4ABE ∼ 4BGO, we have\n‖−→AE‖2 ‖−−→AB‖2 = ‖−−→OG‖2 ‖−−→OB‖2 =\n√ 1− ‖−−→GB‖22\n1 (8)\nWLOG, assume ‖−−→OC‖2 ≥ ‖ −−→ OD‖2, as shown in the figure. We draw −−→ HB ‖ −−→CD, and we know ‖−−→OH‖2 ≥\n‖−−→OB‖2 = ‖ −→ OA‖2. Since4CDO ∼ 4HBO, we have\n‖−−→CD‖2 ‖−−→HB‖2 = ‖−−→OD‖2 ‖−−→OB‖2 = ‖−−→OD‖2 ≥ 1− γ\nSo ‖−−→CD‖2 ≥ (1 − γ)‖ −−→ HB‖2. On the other hand, ∠BAO < π2 , and A is between H and O, so ∠BAH > π2 , which means ‖−−→HB‖2 ≥ ‖ −−→ AB‖2 = 2‖ −−→ GB‖2. Thus, ‖ −−→ GB‖2 ≤ ‖ −−→ HB‖2 2 ≤ ‖−−→CD‖2 2(1−γ) .\nSubstitute it into (8), we get\n‖−→AE‖2 ‖−−→AB‖2 ≥\n√\n1− ‖ −−→ CD‖22 4(1− γ)2 ≥ √ 1− ( γ 1− γ )2\nThe last inequality holds since ‖−−→CD‖2 = ‖w∗i − wi‖2 ≤ 2γ. Substitute this inequality into (7), we get\n‖ei + w∗i − ei + wi‖2 = ‖ −−→ AB‖2\n≤ ‖ −→ AE‖2√\n1− (\nγ 1−γ\n)2 ≤ ‖−−→CF‖2\n(1− γ) √ 1− (\nγ 1−γ\n)2 (9)\n≤ ‖ −−→ CD‖2\n(1− γ) √ 1− (\nγ 1−γ\n)2 = ‖w∗i − wi‖2√ 1− 2γ (10)\nNotice that ei + wi > (w∗i − wi) = −‖ −−→ DF‖2, so ei + wi · ei + wi>(w∗i − wi) = −−→ DF . That means,\n‖(I− ei + wi · ei + wi>)(w∗i − wi)‖2 = ‖ −−→ DC −−−→DF‖2 = ‖ −−→ CF‖2\nThe lemma follows by (9) and (10). 2. By Figure 10, we know |〈ei + w∗i − ei + wi, ei + wi〉| = ‖ −−→ BE‖2. Since4ABE ∼ 4GBO, we have\n‖−−→BE‖2 ‖−−→AB‖2 = ‖−−→GB‖2 ‖−−→BO‖2 = ‖−−→AB‖2 2\nTherefore, using (10) we get\n|〈ei + w∗i − ei + wi, ei + wi〉| = ‖−−→AB‖22 2 ≤ ‖w ∗ i − wi‖22 2(1− 2γ)\nMoreover, 〈ei + w∗i − ei + wi, ei + wi〉 =〈ei + w∗i , ei + wi〉 − 1 ≤ 0. 3. We know that\nθi,i∗ = 2 arcsin ‖ −→ AG‖2 = 2 arcsin ‖ei + w∗i − ei + wi‖2 2\n≤ ‖ei + w∗i − ei + wi‖2 + ‖ei + w∗i − ei + wi‖32\n8\nThe last inequality holds by Taylor’s Series for arcsin, and the fact ‖ei + w∗i − ei + wi‖2 = ‖ −−→ AB‖2 ≤ ‖w∗i − wi‖2 ≤ 2γ ≤ 150 . Thus, we have θi,i∗ ≤ 1.001‖w∗i − wi‖2.\nE More Handy Lemmas Lemma* E.1. If ‖W‖2, ‖W∗‖2 ≤ γ, then\n• (1−γ) 2 (1+γ)2 I I + W > I + W (1+γ) 2 (1−γ)2 I, (1−γ)2 (1+γ)2 I I + W∗ > I + W∗ (1+γ) 2 (1−γ)2 I,\n• (1− γ)2I (I + W)>(I + W) (1 + γ)2I, (1− γ)2I (I + W∗)>(I + W∗) (1 + γ)2I.\nTherefore, the singular value of I + W is at most 1+γ1−γ and at least 1−γ 1+γ . The singular value of I + W is at most 1 + γ and at least 1− γ. The same claims hold for I + W∗, I + W∗ respectively.\nProof. Since ‖W‖2 ≤ γ, we have 1 − γ ≤ ‖I + W‖2 ≤ 1 + γ, and 1 − γ ≤ ‖ei + wi‖2 ≤ 1 + γ. Therefore, I + W = Σ(I + W) where Σ is a diagonal matrix whose entries are within [ 11+γ , 1 1−γ ]. Putting into I + W > I + W, we have\nI + W > I + W = (I + W)>Σ2(I + W) 1 (1− γ)2 (I + W) >(I + W) (1 + γ) 2 (1− γ)2 I\nSimilarly we can show I + W > I + W (1−γ) 2\n(1+γ)2 I. Thus we know the singular value of I + W is at most 1+γ 1−γ and at least 1−γ 1+γ . The same proof works for I + W, I + W ∗ and I + W∗.\nLemma* E.2. If ‖W‖2, ‖W∗‖2 ≤ γ ≤ 1100 , we have\n|〈ei + w∗i , ej + wj〉| ≤ 2.1γ, |〈ei + wi, ej + wj〉| ≤ 2.1γ\nProof. We know\n|〈ei + w∗i , ej + wj〉| = |〈ei + w∗i , ej + wj〉| ‖ei + w∗i ‖2‖ej + wj‖2 ≤ |〈ei + w ∗ i , ej + wj〉| (1− γ)2 = |w∗i,j |+ |wi,j |+ |〈wi, wj〉| (1− γ)2 ≤ (2 + γ)γ (1− γ)2 ≤ 2.1γ\nwhere the last inequality holds since γ ≤ 1100 . The same analysis works for 〈ei + wi, ej + wj〉.\nLemma* E.3 (Triangle inequality between ei+wi, ei+w∗i , w∗i −wi). |‖ei+wi‖2−‖ei+w∗i ‖2| ≤ ‖w∗i −wi‖2.\nLemma* E.4. If ‖W‖2, ‖W∗‖2 ≤ γ, |g| ≤ 2dγ. Proof. By definition and Lemma E.3, we know |g| = ∑di=1(‖ei + w∗i ‖2 − ‖ei + wi‖2) ≤ ∑d i=1 ‖w∗i − wi‖2 ≤ 2dγ.\nLemma* E.5. If ‖W‖2, ‖W∗‖2 ≤ γ, |〈ei + w∗i − ei + wi, ej + wj〉| ≤ ‖w∗i−wi‖2√ 1−2γ .\nProof. By Cauchy Schwartz and Lemma D.1 term 1.\nLemma* E.6. |xk − yk| ≤ k2 |x− y|(|x|k−1 + |y|k−1). Proof. |xk − yk| = ∣∣∣(x− y) ∑k−1 t=1 xtyk−t−1+ytxk−t−1 2\n∣∣∣ ≤ k2 |x− y|(|x|k−1 + |y|k−1), where the last inequality holds since |xtyk−t−1 + ytxk−t−1| ≤ |x|t|y|k−t−1 + |y|t|x|k−t−1 ≤ |x|k−1 + |y|k−1, by rearrangement inequality.\nLemma* E.7. If ‖W‖2, ‖W∗‖2 ≤ γ ≤ 1100 , for k ≥ 3, we have\n‖〈ei + w∗i , ej + wj〉k(ei + w∗i )− 〈ei + wi, ej + wj〉k(ei + wi)‖2 ≤6(2.2γ)k−3 ( 〈ei + w∗i , ej + wj〉2 + 〈ei + wi, ej + wj〉2 ) ‖w∗i − wi‖2\nProof.\n‖〈ei + w∗i , ej + wj〉k(ei + w∗i )− 〈ei + wi, ej + wj〉k(ei + wi)‖2 ≤‖w∗i − wi‖2|〈ei + w∗i , ej + wj〉k|+ ‖(〈ei + w∗i , ej + wj〉k − 〈ei + wi, ej + wj〉k)(ei + wi)‖2 ≤‖w∗i − wi‖2|〈ei + w∗i , ej + wj〉k|+ (1 + γ)|〈ei + w∗i , ej + wj〉k − 〈ei + wi, ej + wj〉k| ¬ ≤‖w∗i − wi‖2(2.1γ)k−2〈ei + w∗i , ej + wj〉2\n+ (1 + γ)k 2 |〈ei + w∗i − ei + wi, ej + wj〉|(|〈ei + w∗i , ej + wj〉|k−1 + |〈ei + wi, ej + wj〉|k−1)\n≤〈ei + w∗i , ej + wj〉2 ( ‖w∗i − wi‖2(2.1γ)k−2 + (1 + γ)k(2.1γ)k−3\n2 |〈ei + w∗i − ei + wi, ej + wj〉|\n)\n+ 〈ei + wi, ej + wj〉2 ( (1 + γ)k(2.1γ)k−3\n2 |〈ei + w∗i − ei + wi, ej + wj〉|\n)\n ≤‖w∗i − wi‖2 [( (2.1γ)k−2 + 0.52k(2.1γ)k−3 ) 〈ei + w∗i , ej + wj〉2 + 0.52k(2.1γ)k−3〈ei + wi, ej + wj〉2 ]\n® ≤‖w∗i − wi‖2 [ 0.55k(2.1γ)k−3〈ei + w∗i , ej + wj〉2 + 0.52k(2.1γ)k−3〈ei + wi, ej + wj〉2 ]\n¯ ≤6(2.2γ)k−3 ( 〈ei + w∗i , ej + wj〉2 + 〈ei + wi, ej + wj〉2 ) ‖w∗i − wi‖2\nwhere ¬ uses Lemma E.2 and Lemma E.6,  uses Lemma E.5, ® holds as γ ≤ 1100 , and ¯ holds since 0.55k(2.1)k−3 ≤ 6(2.2)k−3 for k ≥ 3.\nLemma* E.8. If ‖W‖2, ‖W∗‖2 ≤ γ ≤ 1100 , for k ≥ 2, ∣∣‖ei + wi‖2〈ei + wi, ej + wj〉2k − ‖ei + w∗i ‖2〈ei + w∗i , ej + wj〉2k\n∣∣ ≤8(2.2γ)2k−3 ( 〈ei + wi, ej + wj〉2 + 〈ei + w∗i , ej + wj〉2 ) ‖w∗i − wi‖2\nProof. ∣∣‖ei + wi‖2〈ei + wi, ej + wj〉2k − ‖ei + w∗i ‖2〈ei + w∗i , ej + wj〉2k\n∣∣ ≤‖ei + wi‖2 ∣∣〈ei + wi, ej + wj〉2k − 〈ei + w∗i , ej + wj〉2k ∣∣+ |‖ei + wi‖2 − ‖ei + w∗i ‖2| 〈ei + w∗i , ej + wj〉2k\n¬ ≤‖ei + wi‖2 ∣∣〈ei + wi, ej + wj〉2k − 〈ei + w∗i , ej + wj〉2k ∣∣+ ‖w∗i − wi‖2(2.1γ)2k−2〈ei + w∗i , ej + wj〉2\n ≤(1 + γ)k|〈ei + wi − ei + w∗i , ej + wj〉| ( |〈ei + wi, ej + wj〉|2k−1 + |〈ei + w∗i , ej + wj〉|2k−1 )\n+ ‖w∗i − wi‖2(2.1γ)2k−2〈ei + w∗i , ej + wj〉2 ® ≤ [\n(1 + γ)k(2.1γ)2k−3√ 1− 2γ 〈ei + wi, ej + wj〉 2 +\n( (1 + γ)k(2.1γ)2k−3√\n1− 2γ + (2.1γ) 2k−2\n) 〈ei + w∗i , ej + wj〉2 ] ‖w∗i − wi‖2\n¯ ≤1.05k(2.1γ)2k−3 ( 〈ei + wi, ej + wj〉2 + 〈ei + w∗i , ej + wj〉2 ) ‖w∗i − wi‖2\n° ≤8(2.2γ)2k−3 ( 〈ei + wi, ej + wj〉2 + 〈ei + w∗i , ej + wj〉2 ) ‖w∗i − wi‖2\nwhere ¬ uses Lemma E.2 and Lemma E.3,  uses Lemma E.6, ® uses Lemma E.5, ¯ holds as γ ≤ 1100 , and ± holds as 1.05k(2.1)2k−3 ≤ 8(2.2)2k−3 for k ≥ 2.\nLemma* E.9. If ‖W‖2, ‖W∗‖2 ≤ γ, for fixed j ∈ [d], ∑\ni 6=j 〈ei + wi, ej + wj〉2 ≤\n4γ (1− γ)2 , ∑\ni 6=j 〈ei + w∗i , ej + wj〉2 ≤\n4γ(1 + γ)\n1− 2γ .\nSimilarly, for fixed i ∈ [d], ∑\nj 6=i 〈ei + wi, ej + wj〉2 ≤\n4γ (1− γ)2 , ∑\nj 6=i 〈ei + w∗i , ej + wj〉2 ≤\n4γ(1 + γ)\n1− 2γ .\nProof. By matrix multiplication,\nd∑\ni=1\n〈ei + w∗i , ej + wj〉2 = d∑\ni=1\nej + wj > ei + w∗i · ei + w∗i > ej + wj = ej + wj > I + W∗ · I + W∗>ej + wj\nBy Lemma E.1, we know I + W∗ · I + W∗> (1+γ) 2 (1−γ)2 I. That means, ∑d i=1〈ei + w∗i , ej + wj〉2 ≤ (1+γ)2\n(1−γ)2 . On the other hand, by Lemma D.1 term 2, 〈ej + w∗j , ej + wj〉2 = (1 − 〈ej + w∗j − ej + wj , ej + wj〉)2 ≥ 1− ‖w\n∗ i−wi‖22 1−2γ .\nTherefore, we know\n∑ i6=j 〈ei + w∗i , ej + wj〉2 ≤ (1 + γ)2 (1− γ)2 − 1 + ‖w∗i − wi‖22 1− 2γ = 4γ (1− γ)2 + ‖w∗i − wi‖22 1− 2γ ≤ 4γ(1 + γ) 1− 2γ\nUsing the same analysis, we get ∑ i 6=j〈ei + wi, ej + wj〉2 ≤ (1+γ)2 (1−γ)2 − 1 = 4γ\n(1−γ)2 . The analysis for fixed i is similar.\nLemma* E.10. For any matrix A, we have ‖Diag(A)‖2 ≤ ‖A‖2 and ‖Off-Diag(A)‖2 ≤ 2‖A‖2.\nProof. By definition, we know ‖Diag(A)‖2 = maxi∈[d] e>i Aei ≤ maxv∈Rd v>Av = ‖A‖2, and ‖Off-Diag(A)‖2 ≤ ‖A‖2 + ‖Diag(A)‖2 ≤ 2‖A‖2.\nLemma* E.11. If ‖W‖2, ‖W∗‖2 ≤ γ, ‖A‖2 ≤ 2γ(γ 2+3)\n1−γ2 .\nProof. By Lemma E.1, we have\n‖A‖2 = ‖(I + W∗)I + W∗ > − (I + W)I + W>‖2 ≤\n(1 + γ)2 1− γ − (1− γ)2 1 + γ = 2γ(γ2 + 3) 1− γ2 .\nLemma* E.12. If ‖W‖2, ‖W∗‖2 ≤ γ ≤ 1100 , |ej + wj > Aej + wj − e>j Aej | ≤ 5γ2.\nProof.\n|ej + wj>Aej + wj − e>j Aej | ≤ |ej + wj > A(ej + wj − ej)|+ |(ej + wj − ej)>Aej | ¬ ≤ 4γ\n2(γ2 + 3) 1− γ2  < 5γ2\nwhere ¬ uses Cauchy Schwartz, Lemma E.11 and ‖ej + wj − ej‖2 ≤ γ, and  holds as γ ≤ 1100 .\nLemma* E.13. For any i ∈ [n], |‖[ei + w∗i ‖2 − ‖ei + wi‖2]− [w∗i,i − wi,i]| ≤ 6.07γ2.\nProof.\n‖ei + wi‖2 − ‖ei + w∗i ‖2 = 〈ei + wi, ei + wi〉 − 〈ei + w∗i , ei + w∗i 〉 = 〈ei + wi, ei + wi − ei + w∗i 〉+ 〈wi − w∗i , ei + w∗i 〉 = 〈wi − w∗i , ei〉+ 〈ei + wi, ei + wi − ei + w∗i 〉+ 〈wi − w∗i , ei + w∗i − ei〉 = wi,i − w∗i,i + 〈ei + wi, ei + wi − ei + w∗i 〉+ 〈wi − w∗i , ei + w∗i − ei〉\nAs a result,\n|[‖ei + wi‖2 − ‖ei + w∗i ‖2]− [wi,i − w∗i,i]| ≤ ||〈ei + wi, ei + wi − ei + w∗i 〉|+ |〈wi − w∗i , ei + w∗i − ei〉| ¬ ≤ (1 + γ)2γ 2\n1− 2γ + 4γ 2 ≤ 6.07γ2\nwhere ¬ uses Lemma D.1 term 2 and ‖ei + w∗i − ei‖2 ≤ 2γ, and Cauchy Schwartz. So the claim follows.\nCorollary E.14. |g − Tr(W∗ −W)| ≤ 6.07dγ2.\nLemma* E.15. I + W is close to I on its diagonals, and close to W on its off-diagonals. More specifically, if ‖W‖2, ‖W∗‖2 ≤ γ ≤ 1100 ,\n‖Diag(I + W)− I‖2 ≤ γ2\n2(1− γ)2 , ‖Diag(I + W ∗)− I‖2 ≤\nγ2\n2(1− γ)2\n‖Off-Diag(I + W −W)‖2 ≤ 4γ2\n1− γ , ‖Off-Diag(I + W ∗ −W∗)‖2 ≤\n4γ2\n1− γ ‖I + W − I‖2 ≤ 2.05γ, ‖I + W∗ − I‖2 ≤ 2.05γ\nProof. For the diagonal terms,\n‖Diag(I + W)− I‖2 = max j |I + Wj,j − 1| = max j ∣∣∣∣ 1 + wj,j − ‖ej + wj‖2\n‖ej + wj‖2\n∣∣∣∣\n≤max j ∣∣∣∣ (1 + wj,j) 2 − ‖ej + wj‖22 ‖ej + wj‖2 ∣∣∣∣ ∣∣∣∣\n1\n1 + wj,j + ‖ej + wj‖2\n∣∣∣∣ ≤ maxj ∑ i 6=j w 2 j,i 2(1− γ)2 ≤ γ2 2(1− γ)2\nFor the off-diagonal terms, we know I + W = (I + W)Σ for some diagonal matrix Σ, so\n‖Off-Diag(I + W −W)‖2 = ‖Off-Diag((I + W)Σ−W)‖2 = ‖Off-Diag((Σ− I)W)‖2 ¬ ≤ 2‖(Σ− I)W‖2 ≤\n4γ2\n1− γ\nwhere ¬ uses Lemma E.10. For the difference between I + W and I, we split I + W into diagonal and offdiagonal parts:\n‖I + W − I‖2 = ‖Diag(I + W) + Off-Diag(I + W)− I‖2\n=‖Off-Diag(W)‖2 + γ2 2(1− γ)2 + 4γ2 1− γ ¬ ≤ 2‖W‖2 + γ2(9− 8γ) 2(1− γ)2 ≤ 2.05γ\nwhere ¬ uses Lemma E.10.\nLemma* E.16. If ‖W‖2, ‖W∗‖2 ≤ γ ≤ 1100 ,\n‖A− [W∗ −W + (W∗ −W)> −Diag(W∗ −W)]‖2 ≤ 9.2γ2\nProof. By definition, ∥∥∥ [ (I + W∗)I + W∗ > − (I + W)I + W> ] − [ (W∗ −W) + (I + W∗> − I + W>)\n]∥∥∥ 2\n=‖W∗(I + W∗> − I)−W(I + W> − I)‖2 ≤ ‖W∗(I + W∗ > − I)‖2 + ‖W(I + W)> − I)‖2\n≤2.05γ2 + 2.05γ2 = 4.1γ2\nwhere the last inequality uses Lemma E.15. Below we further approximate I + W∗ > − I + W>.\n∥∥∥ [ I + W∗ > − I + W> ] − [ (W∗ −W)> −Diag(W∗ −W) ]∥∥∥ 2\n= ∥∥∥Diag(I + W∗> − I + W>) + Off-Diag(I + W∗> − I + W>)− [ (W∗ −W)> −Diag(W∗ −W) ]∥∥∥ 2\n¬ ≤‖Off-Diag(I + W∗> − I + W>)−Off-Diag(W∗ −W)>‖2 +\nγ2\n(1− γ)2  ≤ 4γ 2\n1− γ + γ2 (1− γ)2 ≤ 5.1γ 2\nwhere ¬ uses Lemma E.15,  uses Lemma E.15 Combining everything,\n‖A− [W∗ −W + (W∗ −W)> −Diag(W∗ −W)]‖2 ≤ 9.2γ2\nUsing Lemma E.10, we immediately have the following corollary.\nCorollary E.17. ‖Diag(A)−Diag(W∗ −W)‖2 ≤ 9.2γ2.\nLemma* E.18. For η ≤ 1πd , ∥∥∥I− η\n(π 2 uu> + (π 2 + 1 ) I )∥∥∥ 2 ≤ ( 1− η (π 2 + 1 ))\nProof. Consider another basis (e′1, · · · , e′d) where e′1 = u‖u‖2 . For every unit vector v = (v1, · · · , vd) in this new space, we know\nvT ( I− η (π 2 uu> + (π 2 + 1 ) I )) v = ‖v‖22 − η (π 2 + 1 ) ‖v‖22 − πηd 2 v21\nHence we get 0 ≤ vT ( I− η (π 2 uu> + (π 2 + 1 ) I )) v ≤ ( 1− η (π 2 + 1 )) ‖v‖22\nBy definition of matrix norm, the lemma follows.\nF Proofs for Section A\nF.1 Proof for Claim A.1 Comparing with Lemma 2.1, we know that for fixed j, P1,j is already contained in −∇L(W)j as the first term, while P3,j is simply the summand when i = j, ignoring the first term. Below we show how to obtain P2,j from i 6= j cases. We will bound the approximation error in Lemma A.2 and Lemma A.3. ∑\ni 6=j\n((π 2 − θi∗,j ) (ei + w ∗ i )− (π 2 − θi,j ) (ei + wi) + (‖ei + w∗i ‖ sin θi∗,j − ‖ei + wi‖ sin θi,j) ej + wj )\n≈ ∑\ni 6=j\n( 〈ei + w∗i , ej + wj〉(ei + w∗i )− 〈ei + wi, ej + wj〉(ei + wi) )\n+ ∑\ni 6=j\n( ‖ei + w∗i ‖ ( 1− 1\n2 〈ei + w∗i , ej + wj〉2\n) − ‖ei + wi‖ ( 1− 1\n2 〈ei + wi, ej + wj〉2\n)) ej + wj\n= ∑\ni 6=j ((ei + w\n∗ i )ei + w ∗ i > − (ei + wi)ei + wi>)ej + wj\n+ ∑\ni 6=j\n( ‖ei + w∗i ‖ − ‖ei + wi‖ − 1\n2 ej + wj\n> ei + w∗i ‖ei + w∗i ‖ei + w∗i > ej + wj\n+ 1\n2 ej + wj\n> ei + wi‖ei + wi‖ei + wi>ej + wj ) ej + wj\n=Ajej + wj +\n ∑\ni 6=j (‖ei + w∗i ‖ − ‖ei + wi‖)−\n∑\ni 6=j\n1 2 ej + wj > (ei + w ∗ i )ei + w ∗ i > ej + wj\n+ ∑\ni 6=j\n1 2 ej + wj > (ei + wi)ei + wi > ej + wj\n  ej + wj\n=Ajej + wj + ( gj − 1\n2 ej + wj\n> Ajej + wj ) ej + wj = P2,j .\nF.2 Proof for Lemma A.2 In order to prove this lemma, we bound the approximation loss of θi,j , θi∗,j in Lemma F.1, and the approximation loss of sin θi,j , sin θi∗,j in Lemma F.2.\nLemma* F.1 (Approximation loss related to θi,j , θi∗,j). If ‖W‖2, ‖W∗‖2 ≤ γ ≤ 1100 ,\nd∑\nj=1\n∑\ni6=j\n∣∣∣ 〈 ( π\n2 − θi∗,j − 〈ei + w∗i , ej + wj〉)(ei + w∗i )− (\nπ 2 − θi,j − 〈ei + wi, ej + wj〉)(ei + wi), w∗j − wj\n〉∣∣∣\n≤0.083‖W∗ −W‖2F\nProof. By definition, π2 − θi∗,j = arcsin〈ei + w∗i , ej + wj〉, and π2 − θi,j = arcsin〈ei + wi, ej + wj〉. The Taylor series of arcsinx at x = 0 is ∑∞ k=0 (2k)! 4k(k!)2(2k+1) x2k+1, where for k ≥ 1,\n(2k)! 4k(k!)2(2k + 1) ≤ 1 6 (11)\nThus,\nd∑\nj=1\n∑\ni6=j\n∣∣∣ 〈 ( π\n2 − θi∗,j − 〈ei + w∗i , ej + wj〉)(ei + w∗i )− (\nπ 2 − θi,j − 〈ei + wi, ej + wj〉)(ei + wi), w∗j − wj\n〉∣∣∣\n¬ ≤\nd∑\nj=1\n∑\ni6=j\n∞∑\nk=1\n1\n6\n∣∣〈〈ei + w∗i , ej + wj〉2k+1(ei + w∗i )− 〈ei + wi, ej + wj〉2k+1(ei + wi), w∗j − wj 〉∣∣\n ≤\nd∑\nj=1\n∑\ni6=j\n∞∑\nk=1\n1\n6\n∥∥〈ei + w∗i , ej + wj〉2k+1(ei + w∗i )− 〈ei + wi, ej + wj〉2k+1(ei + wi) ∥∥ 2 ‖w∗j − wj‖2\n® ≤\nd∑\nj=1\n∑\ni6=j\n∞∑\nk=1\n(2.2γ)2k−2 ( 〈ei + w∗i , ej + wj〉2 + 〈ei + wi, ej + wj〉2 ) ‖w∗i − wi‖2‖w∗j − wj‖2\n¯ ≤\nd∑\nj=1\n∑ i6=j 1.01 ( 〈ei + w∗i , ej + wj〉2 + 〈ei + wi, ej + wj〉2 ) ‖w∗i − wi‖2‖w∗j − wj‖2\n° ≤1.01   d∑\nj=1\n∑\ni 6=j\n( 〈ei + w∗i , ej + wj〉2 + 〈ei + wi, ej + wj〉2 ) ‖w∗i − wi‖22\n  1 2\n  d∑\nj=1\n∑\ni 6=j\n( 〈ei + w∗i , ej + wj〉2 + 〈ei + wi, ej + wj〉2 ) ‖w∗j − wj‖22\n  1 2\n≤1.01\n  d∑\ni=1\n‖w∗i − wi‖22\n ∑\ni 6=j\n( 〈ei + w∗i , ej + wj〉2 + 〈ei + wi, ej + wj〉2\n)     1 2\n  d∑\nj=1\n‖w∗j − wj‖22\n ∑\ni 6=j\n( 〈ei + w∗i , ej + wj〉2 + 〈ei + wi, ej + wj〉2\n)     1 2\n± ≤1.01\n( 4γ\n(1− γ)2 + 4γ(1 + γ) 1− 2γ\n) ‖W∗ −W‖2F ² ≤ 0.083‖W∗ −W‖2F\nwhere ¬ is by Taylor series,  uses Cauchy Schwartz, ® uses Lemma E.7, ¯ holds as γ ≤ 1100 , ° uses Cauchy Schwartz, ± uses Lemma E.9, ² holds as γ ≤ 1100 .\nLemma* F.2 (Approximation loss related to sin θi,j , sin θi∗,j). If ‖W‖2, ‖W∗‖2 ≤ γ ≤ 1100 ,\nd∑\nj=1\n∑\ni 6=j\n∣∣∣∣ ( ‖ei + w∗i ‖2 ( sin θi∗,j − 1 + 1\n2 〈ei + w∗i , ej + wj〉2\n) −\n‖ei + wi‖2 ( sin θi,j − 1 + 1\n2 〈ei + wi, ej + wj〉2\n)) 〈ej + wj , w∗j − wj〉 ∣∣∣∣ ≤ 0.002‖W∗ −W‖2F\nProof. By definition, we know θi∗,j = arccos〈ei + w∗i , ej + wj〉, and θi,j = arccos〈ei + wi, ej + wj〉. The Taylor series of sin(arccosx) at x = 0 is 1− x22 − x 4 8 − x 6 16 − 5x 8 128 − · · · = ∑∞ k=0 ckx 2k, where ck ≤ 18 for k ≥ 2.\nThus,\nd∑\nj=1\n∑\ni 6=j\n∣∣∣∣ ( ‖ei + w∗i ‖2 ( sin θi∗,j − 1 + 1\n2 〈ei + w∗i , ej + wj〉2\n) −\n‖ei + wi‖2 ( sin θi,j − 1 + 1\n2 〈ei + wi, ej + wj〉2\n)) 〈ej + wj , w∗j − wj〉 ∣∣∣∣\n¬ ≤\nd∑\nj=1\n∑\ni 6=j\n∣∣∣∣∣ ∞∑\nk=2\n1\n8\n( ‖ei + wi‖2〈ei + wi, ej + wj〉2k − ‖ei + w∗i ‖2〈ei + w∗i , ej + wj〉2k ) ∣∣∣∣∣ ‖w ∗ j − wj‖2\n ≤\nd∑\nj=1\n∑\ni 6=j\n∞∑\nk=2\n(2.2γ)2k−3 ( 〈ei + wi, ej + wj〉2 + 〈ei + w∗i , ej + wj〉2 ) ‖w∗i − wi‖2‖w∗j − wj‖2\n® ≤2.3γ   d∑\nj=1\n∑\ni 6=j\n( 〈ei + wi, ej + wj〉2 + 〈ei + w∗i , ej + wj〉2 ) ‖w∗i − wi‖22\n  1 2\n  d∑\nj=1\n∑\ni 6=j\n( 〈ei + wi, ej + wj〉2 + 〈ei + w∗i , ej + wj〉2 ) ‖w∗j − wj‖22\n  1 2\n≤2.3γ\n  d∑\ni=1\n‖w∗i − wi‖22\n ∑\nj 6=i\n( 〈ei + wi, ej + wj〉2 + 〈ei + w∗i , ej + wj〉2\n)     1 2\n  d∑\nj=1\n‖w∗j − wj‖22\n ∑\ni 6=j\n( 〈ei + wi, ej + wj〉2 + 〈ei + w∗i , ej + wj〉2\n)     1 2\n¯ ≤2.3γ\n( 4γ\n(1− γ)2 + 4γ(1 + γ) 1− 2γ\n) ‖W∗ −W‖2F ° < 0.002‖W∗ −W‖2F\nwhere ¬ is by Taylor series,  uses Lemma E.8 and Cauchy Schwartz, ® uses Cauchy Schwartz and γ ≤ 1100 , ¯ uses Lemma E.9, and ° holds as γ ≤ 1100 .\nProof for Lemma A.2. Combining the results from Lemma F.1 and Lemma F.2, the lemma follows.\nF.3 Proof for Lemma A.3 Denote ∆ , P + ∇L(W). This lemma is harder to prove than the previous one since we need to bound the spectral norm of a matrix ∆. First of all, we need to represent ∆. Again, the difference has two parts: approximation for θi,j , θi∗,j , and sin θi,j , sin θi∗,j . Denote the two parts as ∆1,∆2, where ∆ = ∆1 + ∆2. From the proof of Lemma F.1, we know the j-th column of the first part is\n∆1,j , ∑\ni 6=j\n∞∑\nk=1\n(2k)!\n4k(k!)2(2k + 1)\n( 〈ei + w∗i , ej + wj〉2k+1(ei + w∗i )− 〈ei + wi, ej + wj〉2k+1(ei + wi) )\nAnd the j-th column of the second part is\n∆2,j , ∑\ni6=j\n∞∑\nk=2\nck ( ‖ei + wi‖2〈ei + wi, ej + wj〉2k − ‖ei + w∗i ‖2〈ei + w∗i , ej + wj〉2k ) ej + wj\nBelow we bound ‖∆1‖2 in Lemma F.3, and bounds ‖∆2‖2 in Lemma F.4. Lemma* F.3. If ‖W‖2, ‖W∗‖2 ≤ γ ≤ 1100 , ‖∆1‖2 ≤ 3.4γ2.\nProof. Define U,V such that for i = j,Ui,j = Vi,j = 0, and for i 6= j,\nUi,j =\n∞∑\nk=1\n(2k)!\n4k(k!)2(2k + 1) 〈ei + w∗i , ej + wj〉2k+1,Vi,j =\n∞∑\nk=1\n(2k)!\n4k(k!)2(2k + 1) 〈ei + wi, ej + wj〉2k+1\nBy matrix multiplication,\n∆1 =\nd∑\ni=1\n[(I + W∗)∗,iUi,∗ − (I + W)∗,iVi,∗] = (I + W∗)U− (I + W)V (12)\nSo it suffices to bound ‖U‖2, ‖V‖2. For i 6= j,\n|Ui,j | = ∣∣∣∣∣ ∞∑\nk=1\n(2k)!\n4k(k!)2(2k + 1) 〈ei + w∗i , ej + wj〉2k+1 ∣∣∣∣∣ ¬ ≤ ∞∑\nk=1\n(2.1γ)2k−1\n6 〈ei + w∗i , ej + wj〉2 ≤ 0.4γ〈ei + w∗i , ej + wj〉2\nwhere ¬ uses Lemma E.2 and (11). Now, we know\n‖U‖1 ¬= max j\nd∑\ni=1\n|Ui,j | ≤ max j\n∑ i 6=j 0.4γ〈ei + w∗i , ej + wj〉2  ≤ 1.6(1 + γ)γ 2 1− 2γ ≤ 1.65γ 2\nwhere ¬ is by definition,  uses Lemma E.9. Similarly,\n‖U‖∞ = max i\nd∑\nj=1\n|Ui,j | ≤ max i\n∑ j 6=i 0.4γ〈ei + w∗i , ej + wj〉2 ≤ 1.65γ2\nBy Hölder’s inequality, we have ‖U‖2 ≤ √ ‖U‖1‖U‖∞ ≤ 1.65γ2\nNow we do the same analysis for V.\n|Vi,j | = ∣∣∣∣∣ ∞∑\nk=1\n(2k)!\n4k(k!)2(2k + 1) 〈ei + wi, ej + wj〉2k+1 ∣∣∣∣∣\n≤ ∞∑\nk=1\n(2.1γ)2k−1\n6 〈ei + wi, ej + wj〉2 ≤ 0.4γ〈ei + wi, ej + wj〉2\nHence, ‖V‖1 = maxj ∑d i=1 |Vi,j | ≤ maxj ∑ i 6=j 0.4γ〈ei + wi, ej + wj〉2 ≤ 1.65γ2. Similarly, ‖V‖∞ ≤\n1.65γ2, and by Hölder’s inequality, ‖V‖2 ≤ √ ‖V‖1‖V‖∞ ≤ 1.65γ2. Using (12), we get\n‖∆1‖2 ≤ ‖I + W∗‖2‖U‖2 + ‖I + W‖2‖V‖2 ≤ 2(1 + γ)1.65γ2 < 3.4γ2\nLemma* F.4. If ‖W‖2, ‖W∗‖2 ≤ γ ≤ 1100 , ‖∆2‖2 ≤ 6γ3.\nProof. By definition, we can write\n∆2 = I + WDiag    ∑\ni 6=j\n∞∑\nk=2\nck ( ‖ei + wi‖2〈ei + wi, ej + wj〉2k − ‖ei + w∗i ‖2〈ei + w∗i , ej + wj〉2k )    d\nj=1\nSo it suffices to bound the norm of the diagonal matrix, which is the maximum of the diagonal entries. For any j ∈ [d], we have\n∣∣∣∣∣∣ ∑\ni 6=j\n∞∑\nk=2\nck ( ‖ei + wi‖2〈ei + wi, ej + wj〉2k − ‖ei + w∗i ‖2〈ei + w∗i , ej + wj〉2k ) ∣∣∣∣∣∣\n≤ ∑\ni 6=j\n∞∑\nk=2\n1\n8\n( ‖ei + wi‖2〈ei + wi, ej + wj〉2k|+ |‖ei + w∗i ‖2〈ei + w∗i , ej + wj〉2k )\n¬ ≤ ∑\ni 6=j\n∞∑\nk=2\n1 4 (1 + γ)(2.1γ)2k−2\n( 〈ei + wi, ej + wj〉2 + 〈ei + w∗i , ej + wj〉2 )\n ≤0.6γ2\n∑\ni 6=j\n( 〈ei + wi, ej + wj〉2 + 〈ei + w∗i , ej + wj〉2 )\n® ≤0.6γ2\n( 4γ\n(1− γ)2 + 4γ(1 + γ) 1− 2γ\n) < 5γ3\nwhere ¬ uses Lemma E.2,  uses γ ≤ 1100 , ® uses Lemma E.9. So we get ‖∆2‖2 ≤ 1+γ 1−γ 5γ 3 ≤ 6γ3.\nProof for Lemma A.3. Combining the results from Lemma F.3 and Lemma F.4, the lemma follows.\nG Proofs for Section B\nG.1 Proof for Lemma B.1 In Lemma A.3, we use P(W) to approximate −∇L(W) in terms of spectral norm, with approximation loss 3.5γ2. Below we will get Q(W) from P(W) by removing a few more lower order terms. By definition 2.3, we have P2,j =gej + wj − (‖ej + w∗j ‖2 − ‖ej + wj‖2)ej + wj + (\nI− 1 2 ej + wj · ej + wj>\n) Aej + wj\n+ ( I− 1\n2 ej + wj · ej + wj>\n) (ej + wj)− ( I− 1\n2 ej + wj · ej + wj>\n) (ej + w ∗ j )ej + w ∗ j > ej + wj\n=gej + wj − (‖ej + w∗j ‖2 − ‖ej + wj‖2)ej + wj + (\nI− 1 2 ej + wj · ej + wj>\n) Aej + wj\n+ 1 2 (ej + wj)− (ej + w∗j )ej + w∗j > ej + wj + 1 2 ej + wj‖ej + w∗j ‖2(ej + w∗j > ej + wj) 2\n=gej + wj +\n( I− 1\n2 ej + wj · ej + wj>\n) Aej + wj + 3\n2 (ej + wj)− ej + w∗j\n> ej + wj(ej + w ∗ j )\n+\n( 1\n2 ‖ej + w∗j ‖2(ej + w∗j > ej + wj)\n2 − ‖ej + w∗j ‖2 ) ej + wj\n=gej + wj +\n( I− 1\n2 ej + wj · ej + wj>\n) Aej + wj − w∗j + wj + (1− ej + w∗j > ej + wj)(ej + w ∗ j )\n+\n( 1\n2 ‖ej + wj‖2 +\n1 2 ‖ej + w∗j ‖2(ej + w∗j > ej + wj)\n2 − ‖ej + w∗j ‖2 ) ej + wj\nCombining every column together, we get\nP2 = gI + W+AI + W− 1 2 I + WDiag({ej + wj>Aej + wj}dj=1)−(W∗−W)+I + W∗Σ1+I + WΣ2\nwhere\nΣ1 = Diag({(‖ej + w∗j ‖2 − ‖ej + w∗j ‖2ej + w∗j > ej + wj)}dj=1)\nΣ2 = Diag({ 1\n2 ‖ej + wj‖2 +\n1 2 ‖ej + w∗j ‖2(ej + w∗j > ej + wj) 2 − ‖ej + w∗j ‖2}dj=1)\nUsing Lemma E.12, we replace ej + wj > Aej + wj with e>j Aej . By Lemma E.1, ∥∥∥∥P2 − [ gI + W + AI + W − 1\n2 I + WDiag(A)− (W∗ −W) + I + W∗Σ1 + I + WΣ2 ]∥∥∥∥ 2 ≤ 5(1 + γ) 2(1− γ) < 2.6γ 2\nWe then focus on the middle two summands in the sum.\nAI + W − 1 2 I + WDiag(A) = (A− 1 2 Diag(A)) + A(I + W − I)− 1 2 (I + W − I)Diag(A)\nBy Lemma E.10, ‖Diag(A)‖2 ≤ ‖A‖2, so ∥∥∥∥ [ AI + W − 1\n2 I + WDiag(A)\n] − [ A− 1\n2 Diag(A) ]∥∥∥∥ 2 = ∥∥∥∥A(I + W − I)− 1 2 (I + W − I)Diag(A) ∥∥∥∥ 2\n≤‖A‖2‖I + W − I‖2 + 1\n2 ‖I + W − I‖2‖Diag(A)‖2\n¬ ≤ 3γ(γ\n2 + 3)\n1− γ2 2.05γ < 18.5γ 2\nwhere ¬ uses Lemma E.11 and Lemma E.15. Moreover, by Lemma D.1 term 2, we know ‖Σ1‖2 ≤ maxi∈[d](1 + γ)‖w ∗ i−wi‖22\n2(1−2γ) ≤ 2.07γ2, and in Σ2, ∣∣∣∣ 1\n2 ‖ej + w∗j ‖2(ej + w∗j > ej + wj) 2 − 1 2 ‖ej + w∗j ‖2 ∣∣∣∣ ≤ 1 2 (1 + γ) ∣∣∣ej + w∗j > ej + wj − 1 ∣∣∣ ∣∣∣ej + w∗j > ej + wj + 1 ∣∣∣ ≤ 2.07γ2\nso the following terms approximates P2 with approximation loss (2.6 + 18.5 + 2.07 + 2.07)γ2 < 25.3γ2.\nI + W(gI−Σ3) + A− 1 2 Diag(A)− (W∗ −W)\nwhere Σ3 = Diag({ 12‖ej + w∗j ‖2 − 12‖ej + wj‖2}dj=1). By Lemma E.16 and Corollary E.17, we know ‖A− [W∗−W+(W∗−W)>−Diag(W∗−W)]‖2 ≤ 9.2γ2 and ‖Diag(A)−Diag(W∗ −W)‖2 ≤ 9.2γ2. Therefore, with approximation loss of 18.4γ2, we get ∥∥∥∥ [ A− 1\n2 Diag(A)\n] − [ W∗ −W + (W∗ −W)> − 3\n2 Diag(W∗ −W) ]∥∥∥∥ 2 ≤ 18.4γ2\nWe then approximate Σ3:\n‖(I + W)Σ3 − (I + W) 1 2 Diag(W∗ −W)‖2 ≤ 1 + γ 1− γ\n( 1\n2 max j |‖ej + w∗j ‖2 − ‖ej + wj‖2 − w∗j,j + wj,j |\n) < 3.1γ2\nwhere the last inequality is by Lemma E.13. Moreover,\n‖I + W ( 1\n2 Diag(W∗ −W)\n) − 1\n2 Diag(W∗ −W)‖2\n≤‖I + W − I‖2 ∥∥∥∥ 1\n2 Diag(W∗ −W) ∥∥∥∥ 2 < 2.05γ ( 1 2 max i |w∗i,i − wi,i| ) < 2.05γ2\nPutting everything together, with approximation loss of (25.3 + 18.4 + 3.1 + 2.05)γ2 = 49γ2 to P2, we get\n(W∗ −W)> − 2Diag(W∗ −W) + gI + W\nFor P3, using the same idea in the proof of Lemma C.3, we have\nP3 = π 2 (W∗ −W) +\n( I + W − I + W∗ ) Σ4 + I + WΣ5\nwhere Σ4 = Diag({θj,j∗‖ej + w∗j ‖2}dj=1),Σ5 = Diag({‖ej + w∗j ‖2 sin θj,j∗ − θj,j∗‖ej + w∗j ‖2}dj=1). By Taylor’s Theorem, we know ‖Σ5‖2 ≤ ‖Diag({‖ej + w∗j ‖2θ3j,j∗/3}dj=1)‖2.\nNotice that θj,j∗ ≤ 2.002γ by Lemma D.1 term 3, and ‖I + W − I + W∗‖2 ≤ 1+γ1−γ − 1−γ 1+γ ≤ 4.001γ. Consequently, ∥∥∥P3 − π\n2 (W∗ −W) ∥∥∥ 2 ≤ ‖ ( I + W − I + W∗ ) Σ4‖2 + ‖I + WΣ5‖2\n<4.001 ∗ 2.002(1 + γ)γ2 + (1 + γ) 2\n3(1− γ) (2.002γ) 3 < 8.1γ2 + 2.8γ3 < 8.2γ2\nwe only need to keep the term π2 (W ∗ −W) with approximation loss 8.2γ2 to P3.\nNow, combining the approximations to P2 and P3, and Lemma A.3, we have the following matrix with (49 + 8.2 + 3.5)γ2 < 61γ2 approximation loss to −∇L(W):\nπ 2 (W∗ −W)\n( I + uu> ) + (W∗ −W)> − 2Diag(W∗ −W) + gI + W\nwhere u is the all 1 vector.\nG.2 Proof for Lemma B.2 By Lemma E.4, we know |g| ≤ 2dγ. Using Lemma B.1,\n‖∇L(W)‖2 ≤ 61γ2 + ∥∥∥π\n2 (W∗ −W)\n( I + uu> ) + (W∗ −W)> − 2Diag(W∗ −W) + gI + W ∥∥∥ 2\n≤61γ2 + (d+ 1)πγ + 2γ + 4γ + |g|1 + γ 1− γ < 61γ 2 + (d+ 3)πγ + 2.05dγ < 6dγ.\nG.3 Proof for Lemma B.3 In this proof, we use wj to represent the j-th column of Wt, and denote4wj as the j-th column of Gt. By definition we know ‖Gt‖2 = ‖∇L(Wt) + Et‖2 ¬ ≤ ‖∇L(Wt)‖2 + ‖Et‖2  ≤ 6dγ + ε = G2, where ¬ uses triangle inequality,  uses Lemma B.2. We have\nη‖4wj‖2 ≤ η‖Gt‖2 ≤ γ2 G2 ≤ γ 6d , η2‖4wj‖2 ≤ η‖Gt‖22 ≤ γ2 (13)\nBy Definition 2.2, we know\n4gt , gt+1 − gt = d∑\nj=1\n( 〈ej + wj , ej + wj〉 ‖ej + wj‖2 − 〈ej + wj − η4wj , ej + wj − η4wj〉‖ej + wj − η4wj‖2 )\n= d∑\nj=1\n( 〈ej + wj , ej + wj〉‖ej + wj − η4wj‖2 − 〈ej + wj − η4wj , ej + wj − η4wj〉‖ej + wj‖2 ‖ej + wj‖2‖ej + wj − η4wj‖2 )\n= d∑\nj=1\n(‖ej + wj‖2(‖ej + wj − η4wj‖2 − ‖ej + wj‖2) + 2η〈4wj , ej + wj〉 − η2‖4wj‖22 ‖ej + wj − η4wj‖2 )\nIf we project η4wj onto the ej + wj direction, we get\n‖ej + wj − η4wj‖2 = √ (‖ej + wj‖2 − 〈ej + wj , η4wj〉)2 + (‖η4j‖22 − 〈ej + wj , η4wj〉2)2\n≤ √ (‖ej + wj‖2 − 〈ej + wj , η4wj〉)2 + ‖η4wj‖22 ¬ ≤ ‖ej + wj‖2 − 〈ej + wj , η4wj〉+ ‖η4wj‖22\nUsing (13), we have ‖ej + wj‖2 − 〈ej + wj , η4wj〉 ≥ 12 . By taking square on both sides, we know ¬ holds. It is trivial to show that ‖ej + wj − η4wj‖2 ≥ ‖ej + wj‖2 − 〈ej + wj , η4wj〉, so we know\n−〈ej + wj , η4wj〉 ≤ ‖ej + wj − η4wj‖2 − ‖ej + wj‖2 ≤ −〈ej + wj , η4wj〉+ ‖η4wj‖22 (14)\nThus, with approximation loss ∑d j=1 ‖ej+wj‖2‖η4wj‖22 ‖ej+wj−η4wj‖2 , we have :\n4gt ≈ d∑\nj=1\n(−‖ej + wj‖2〈ej + wj , η4wj〉+ 2η〈4wj , ej + wj〉 − η2‖4wj‖22 ‖ej + wj − η4wj‖2 )\n= d∑\nj=1\nη〈4wj , ej + wj〉 − η2‖4wj‖22 ‖ej + wj − η4wj‖2\n=\nd∑\nj=1\n−η2‖4wj‖22 ‖ej + wj − η4wj‖2 +\nd∑\nj=1\n(‖ej + wj‖2 − ‖ej + wj − η4wj‖2)η〈4wj , ej + wj〉 ‖ej + wj − η4wj‖2 + η〈Gt, I + Wt〉\nThus we get the following approximation for4gt.\n|4gt − η〈Gt, I + Wt〉|\n≤ d∑\nj=1\n[ −η2‖4wj‖22 ‖ej + wj − η4wj‖2 + (‖ej + wj‖2 − ‖ej + wj − η4wj‖2)η〈4wj , ej + wj〉 ‖ej + wj − η4wj‖2 + ‖ej + wj‖2‖η4wj‖22 ‖ej + wj − η4wj‖2 ]\n¬ ≤\nd∑\nj=1\n[ η〈4wj , ej + wj〉(η〈4wj , ej + wj〉+ ‖η4wj‖22)\n‖ej + wj − η4wj‖2 + 0.02η2‖4wj‖22\n]\n ≤\nd∑\nj=1\n[ η2‖4wj‖22 + η3‖4wj‖32 ‖ej + wj − η4wj‖2 + 0.02ηγ2 ] ® ≤ 1.04ηdγ2\nwhere ¬ uses (14) again, and  ® uses (13), γ ≤ 1100 and ‖ej + wj − η4wj‖2 ≤ 0.98. Thus |4gt − η〈∇L(Wt), I + Wt〉| ≤ 1.04ηdγ2 + |η〈Et, I + Wt〉| < 1.04ηdγ2 + 1.03η √ dε\nWe want to approximate I + Wt with I. Below is the error bound.\n|〈∇L(Wt), I + Wt − I〉| = |〈∇L(Wt) + Qt −Qt, I + Wt − I〉|\n¬ =d · 61γ2 · 2.05γ +\nd∑\ni=1\n2.05γ ∥∥∥(Qt − π\n2 (W∗ −Wt)uu>)i ∥∥∥ 2 + 〈π 2 (W∗ −Wt)uu>, I + Wt − I 〉\n ≤1.251dγ2 + 2.05dγ ( πγ + 2γ + 4γ + 1 + γ 1− γ |gt| ) + Tr ([π 2 (W∗ −Wt)u ] [ u>I + Wt − I ]>)\n® ≤20dγ2 + 2.1dγ|gt|+ ∥∥∥π 2 (W∗ −Wt)u ∥∥∥ 2 ∥∥(I + Wt − I)u ∥∥ 2 ¯ ≤ 20dγ2 + 2.1dγ|gt|+ 2.05π 2 ‖s‖2γ √ d\nwhere ¬ uses Cauchy Schwartz and Lemma E.15,  uses the definition of Q and Lemma E.1, ® holds as for any vector u, v, Tr(uv>) ≤ ‖u‖2‖v‖2, ¯ uses Lemma E.15.\nHence,\n|4gt − η〈∇L(Wt), I〉| ≤1.04ηdγ2 + 1.03η √ dε+ |η〈∇L(Wt), I + Wt − I〉| <1.04ηdγ2 + 1.03η √ dε+ 20ηdγ2 + 2.1ηdγ|gt|+ 2.05π\n2 η‖s‖2γ\n√ d\n<21.1ηdγ2 + 1.03η √ dε+ 2.1ηdγ|gt|+ 2.05π\n2 η‖s‖2γ\n√ d\nSo with approximation loss of 21.1ηdγ2 + 1.03η √ dε+ 2.1ηdγ|gt|+ 2.05π2 η‖s‖2γ √ d, it suffices to consider ηTr(∇L(Wt)). On the other hand, according to Lemma B.1, with approximation loss of 61γ2, we can use −Qt to approximate∇L(Wt).\nTr(Qt) = π 2 Tr ( (W∗ −Wt) ( I + uu> )) + Tr(W∗ −Wt)> − 2Tr(Diag(W∗ −Wt)) + gTr(I + Wt)\n= (π 2 − 1 ) Tr(W∗ −Wt) + π 2 Tr ( (W∗ −Wt) ( uu> )) + gTr(I + Wt) = (π\n2 − 1 ) (Tr(W∗ −Wt)− gt) + (π 2 − 1 ) gt + π 2 Tr ( (W∗ −Wt) ( uu> )) + gtTr(I + Wt)\nTherefore, ∣∣∣Tr(Qt)− gtTr(I)−\n(π 2 − 1 ) gt ∣∣∣ = ∣∣∣Tr(Qt)− ( d+ π 2 − 1 ) gt ∣∣∣\n≤ ∣∣∣ (π 2 − 1 ) (Tr(W∗ −Wt)− gt) + π 2 Tr ( (W∗ −Wt) ( uu> )) + gt(Tr(I + Wt − I)) ∣∣∣ ¬ ≤6.07\n(π 2 − 1 ) dγ2 + π 2 ‖st‖2 √ d+ 2.05|gt|dγ\nwhere ¬ uses Lemma E.14 and Lemma E.15. Thus, ∣∣∣4gt − [ −η ( d+ π\n2 − 1 ) gt ]∣∣∣\n≤η [ 21.1dγ2 + 1.03 √ dε+ 2.1dγ|gt|+ 2.05π\n2 ‖s‖2γ\n√ d+ 61dγ2 + 2.05|gt|dγ + 6.07 (π 2 − 1 ) dγ2 + π 2 ‖st‖2 √ d\n]\n≤η [ 86dγ2 + 1.03 √ dε+ 4.15dγ|gt|+ 4.8‖st‖2γ √ d ]\nNow we have\n|gt+1| = |gt −4gt| ≤ ( 1− η ( d+ π\n2 − 1− 4.15dγ\n)) |gt|+ 86ηdγ2 + 1.03η √ dε+ 4.8η‖st‖2γ √ d\n≤(1− 0.95ηd)|gt|+ 86ηdγ2 + 1.03η √ dε+ 4.8η‖st‖2γ √ d\nG.4 Proof for Lemma B.4 By definition of st,\n4st , st+1 − st = (Wt −Wt+1)u = η(∇L(Wt) + Et)u = −ηQtu+ η(Qt +∇L(Wt) + Et)u\nBy definition of Qt,\nQtu = (π\n2 (W∗ −Wt)\n( I + uu> ) + (W∗ −Wt)> − 2Diag(W∗ −Wt) + gtI + Wt ) u\n= (d+ 1)π\n2 st +\n( (W∗ −Wt)> − 2Diag(W∗ −Wt) + gtI + Wt ) u\nThus, we know ∥∥∥∥Qtu− (d+ 1)π\n2 st ∥∥∥∥ 2 = ∥∥((W∗ −Wt)> − 2Diag(W∗ −Wt) + gtI + Wt ) u ∥∥ 2\n≤ √ d ( ‖(W∗ −Wt)>‖2 + 2‖Diag(W∗ −Wt)‖2 + ‖gtI + Wt‖2 ) ¬ ≤ √ d ( 2γ + 4γ + |gt| 1 + γ\n1− γ\n) < (6γ + 1.03|gt|) √ d\nwhere ¬ uses Lemma E.1 and Lemma E.10. By Lemma B.1, ‖4st− [−η (d+1)π2 st]‖2 < η(6γ+ 1.03|gt|) √ d+η‖(Qt+∇L(Wt) +Et)u‖2 ≤ η(6.61γ+\n1.03|gt|+ ε) √ d.\nG.5 Proof for Lemma B.5 Combining Lemma B.3 and Lemma B.4, we get\n|gt+1|+ ‖st+1‖2 ≤(1− 0.95ηd)(|gt|+ ‖st‖2) + η(6.6γ + 1.03|gt|+ ε) √ d+ 86ηdγ2 + 1.03η √ dε+ (4.8ηγ √ d− 0.62ηd)‖st‖2\n¬ ≤(1− 0.95ηd)(|gt|+ ‖st‖2) + 6.6ηγ √ d+ 86ηdγ2 + η1.03|gt| √ d+ 2.03η √ dε\n ≤(1− 0.84ηd)(|gt|+ ‖st‖2) + 6.6ηγ √ d+ 87ηdγ2\nwhere ¬ uses γ ≤ 1100 , d ≥ 100,  uses ε ≤ γ2 and d ≥ 100. So if the following inequality holds, |gt|+ ‖st‖2 will always decrease by factor at least 1− 0.5ηd.\n0.34ηd(|gt|+ ‖st‖2) ≥ 6.6ηγ √ d+ 87ηdγ2\nWhich gives\n|gt|+ ‖st‖2 ≥ 6.6ηγ\n√ d+ 87ηdγ2\n0.34ηd =\n6.6γ\n0.34 √ d\n+ 87γ2\n0.34\nwhere the last expression is smaller than 4.5γ. Hence, |gt|+ ‖st‖2 will keep decreasing by 1− 0.5ηd as long as it is larger than 4.5γ. So we have ‖st‖2 ≤ 4.5γ. Now plug it back to the updating rule of |gt|:\n|gt+1| ≤(1− 0.95ηd)|gt|+ 86ηdγ2 + 1.03η √ dε+ 4.8η‖st‖2γ √ d\n≤(1− 0.95ηd)|gt|+ 86ηdγ2 + 1.03η √ dε+ 21.6ηγ2 √ d\nIn order to get factor 1− 0.5ηd, we have 0.45ηd|gt| ≥ 86ηdγ2 + 1.03η √ dε+ 21.6ηγ2 √ d\nSolve this inequality, we get\n86ηdγ2 + 1.03η √ dε+ 21.6ηγ2 √ d\n0.45ηd =\n86γ2 0.45 + 1.03ε+ 21.6γ2 0.45 √ d ≤ 197γ2\nThe last inequality uses d ≥ 100, ε ≤ γ2. So even after |gt|+ ‖st‖2 is below 4.5γ, |gt| will keep decreasing by factor 1− 0.5ηd until it is smaller than 197γ2.\nFinally we bound the number of steps to arrive 197γ2. Let γ = 1400 , γ0 = 1 8000 . Again, the constants here are pretty loose. Since |gt| ≤ (1 − 0.5ηd)t|g0| ≤ (1 − 0.5ηd)t2dγ0, in order to let gt ≤ 197γ2, it suffices to have t ≥ log 197γ2 2dγ0\nlog(1− ηd2 ) . Since ηd is small, by Taylor expansion we know log(1− ηd2 ) ≈ − ηd 2 . Thus, it suffices to let\nt ≥ 2 log(0.203d)ηd . Notice that log(0.203d) d is decreasing for d ≥ 100, we know it suffices to let t ≥ 116η .\nG.6 Proof for Lemma B.6 Let H = W −W∗, by the updating rule of Wt and the definition of Qt, we know\nHt+1 = Ht − ηHt (π\n2 uu> +\nπ\n2\n) − ηH>t + 2ηDiag(Ht) + ηgtI + W − η(Gt + Qt)\nThat gives,\n‖Ht+1 + H>t+1‖2 ≤ ∥∥∥(Ht + H>t ) ( I− η (π 2 uu> + π 2 + 1 ))∥∥∥ 2 + 2η ∥∥Diag(Ht + H>t ) ∥∥ 2\n+ 2η|gt|‖I + W‖2 + 2η ‖Et +∇L(Wt) + Qt‖2 ¬ ≤ ( I− η\n(π 2 + 1 )) ‖Ht + H>t ‖2 + 2η‖Ht + H>t ‖2 + 2(1 + γ)η|gt| 1− γ + 2ηε+ 122ηγ 2\n ≤ ( I− η (π 2 − 1 )) ‖Ht + H>t ‖2 + 2.05η|gt|+ 124ηγ2 (15)\nwhere ¬ uses Lemma E.18, Lemma E.10, ‖Et‖2 ≤ ε and Lemma B.1.  uses ε ≤ γ2 and γ ≤ 1100 . Similarly, we get\n‖Ht+1 −H>t+1‖2 ¬ ≤ ∥∥∥(Ht −H>t ) ( I− η (π 2 uu> + π 2 − 1 ))∥∥∥ 2 + η|gt|‖I + W − I + I− I + W\n>‖2 + 2η ‖Et +∇L(Wt) + Qt‖2  ≤ ( I− η\n(π 2 − 1 )) ‖Ht −H>t ‖2 + 4.10ηγ|gt|+ 124ηγ2 (16)\nwhere ¬ holds as the diagonal terms cancel out,  uses Lemma E.18, Lemma E.15. Adding (15) and (16), we get\n‖Ht+1 + H>t+1‖2 + ‖Ht+1 −H>t+1‖2 ≤ ( I− η\n(π 2 − 1 )) ( ‖Ht + H>t ‖2 + ‖Ht −H>t ‖2 ) + 2.1η|gt|+ 248ηγ2 (17)\nFor any T > 0, by applying (17) recursively, we have\n‖HT + H>T ‖2 + ‖HT −H>T ‖2 ≤ ‖H0 + H>0 ‖2 + ‖H0 −H>0 ‖2 + 2.1η T−1∑\nt=0\n|gt|+ 248ηTγ2\nBy Lemma E.4 we know |g0| ≤ 2dγ0, so 2.1η ∑T−1 t=0 |gt| ≤ 2.1η|g0|(1−(1−0.5ηd)T ) (0.5ηd) ≤ 4.2|g0| d ≤ 8.4γ0. By the proof of Lemma B.5, we know T ≤ 116η , so 248ηTγ2 ≤ 15.5γ2. By triangle inequality, we know ‖H0‖2 ≤ ‖W0‖2 + ‖W∗‖2 ≤ 2γ0, so ‖H0 + H>0 ‖2 + ‖H0 −H>0 ‖2 ≤\n4‖H0‖2 ≤ 8γ0. By triangle inequality again we get\n‖HT ‖2 ≤ ‖HT + H>T ‖2 + ‖HT −H>T ‖2 ≤ ‖H0 + H>0 ‖2 + ‖H0−H>0 ‖2 + 19γ2 + 8.4γ0 ≤ 16.4γ0 + 15.5γ2\nRecall we set γ = 1400 , γ0 = 1 8000 in the proof of Lemma B.5, we know ‖WT ‖2 ≤ ‖W∗‖2 + ‖HT ‖2 ≤ 17.4γ0 + 15.5γ 2 ≤ 1440 ≤ γ.\nG.7 Proof for Lemma B.7 First, by the proof of Lemma B.5, we know |gt| will keep small if ‖Wt‖2 ≤ γ ≤ 1100 .\nAdding (15) and (16), we get\n‖Ht+1 + H>t+1‖2 + ‖Ht+1 −H>t+1‖2 ≤ ( I− η\n(π 2 − 1 )) ( ‖Ht+1 + H>t+1‖2 + ‖Ht+1 −H>t+1‖2 ) + 2.1η|gt|+ 248ηγ2\n¬ ≤ ( I− η (π 2 − 1 )) ( ‖Ht+1 + H>t+1‖2 + ‖Ht+1 −H>t+1‖2 ) + 661ηγ2 (18)\nwhere ¬ holds as |gt| ≤ 197γ2. So either ‖Ht+1 + H>t+1‖2 + ‖Ht+1−H>t+1‖2 keeps decreasing, or it increases, i.e.,\nη (π 2 − 1 ) ( ‖Ht+1 + H>t+1‖2 + ‖Ht+1 −H>t+1‖2 ) ≤ 197ηγ2\nThat gives,\n‖Ht+1 + H>t+1‖2 + ‖Ht+1 −H>t+1‖2 ≤ 197γ2\nπ 2 − 1\n≤ 346γ2\nTherefore, combined with the proof of Lemma B.6, we know ‖Ht+1 + H>t+1‖2 + ‖Ht+1 −H>t+1‖2 will keep decreasing until it is at most 346γ2. Now,\n‖Wt‖2 ≤ ‖Ht‖2 + ‖W∗‖2 ≤ ‖Ht+1 + H>t+1‖2 + ‖Ht+1 −H>t+1‖2 + γ0 ¬ ≤ (346 + 20)γ2 ≤ γ\nwhere ¬ holds as γ0 = 18000 . So ‖Wt‖2 is always bounded by γ.\nH Proofs for Section C For notational simplicity, denote\nxj , ( ej + wj · ej + wj> ) (w∗j − wj),\nX , (x1, · · · , xd) (19) yj , ( I− ej + wj · ej + wj> ) (w∗j − wj),\nY , (y1, · · · , yd) (20) zj , (\nI− 1 2 ej + wj · ej + wj>\n) (w∗j − wj),\nZ , (z1, · · · , zd)\nWe have the following relationship between xj , yj , zj .\nLemma H.1.\n‖zj‖22 = 1 4 ‖xj‖22 + ‖yj‖22, ‖xj‖22 + ‖yj‖22 = ‖w∗j − wj‖22 (21)\nProof for Lemma H.1. By definition,\n‖zj‖22 =‖w∗j − wj‖22 (\nI− 1 2 ej + wj · ej + wj>\n)>( I− 1\n2 ej + wj · ej + wj>\n)\n=‖w∗j − wj‖22 ( I− ej + wj · ej + wj> + 1\n4 ej + wj · ej + wj>ej + wj · ej + wj>\n)\n=‖w∗j − wj‖22 (\nI− 3 4 ej + wj · ej + wj>\n) ,\nand similarly\n‖yj‖22 =‖w∗j − wj‖22 ( I− ej + wj · ej + wj> )> ( I− ej + wj · ej + wj> ) = ‖w∗j − wj‖22 ( I− ej + wj · ej + wj> ) , ‖xj‖22 =‖w∗j − wj‖22 ( ej + wj · ej + wj> )> ( ej + wj · ej + wj> ) = ‖w∗j − wj‖22 ( ej + wj · ej + wj> )\nThe lemma follows.\nH.1 Proof for Lemma C.1 In this proof, we heavily use the following trick between the summation of four vector products, and the trace of four matrix products. We give one example below, and other cases are similar. Lemma H.2. ∑ i,j z > j (ei+w ∗ i )(ei + w ∗ i−ei + wi)>ej + wj = Tr ([ Z>(I + W∗) ] [ (I + W∗ − I + W)>I + W ]) . Proof. By definition, Tr(AB) = ∑d j=1(AB)j,j = ∑ i,j Aj,iBi,j . Thus,\nTr ([ Z>(I + W∗) ] [ (I + W∗ − I + W)>I + W ]) = ∑\ni,j\n[ Z>(I + W∗) ] j,i [ (I + W∗ − I + W)>I + W ] i,j\nBy definition, [ Z>(I + W∗) ] j,i = z>j (ei+w ∗ i ), and [ (I + W∗ − I + W)>I + W ] i,j\n= (ei + w∗i−ei + wi)>ej + wj , so the lemma follows.\nNow we proceed to prove Lemma C.1. We first bound ∑d j=1 z > j Ajej + wj below by splitting Aj into three\nparts, and then improve the lower bound in Lemma H.4.\nLemma H.3. If ‖W‖2, ‖W∗‖2 ≤ γ ≤ 1100 , we have\nd∑\nj=1\nz>j Ajej + wj ≥ −8γ‖W∗ −W‖2F − √ ‖W∗ −W‖2f − 3\n4 ‖X‖2F\n√ ‖W∗ −W‖2F − ‖X‖2F\n.\nProof. We rewrite Aj as\nAj = Bj + 1\n2 Cj + Dj (22)\nwhere\nBj = ∑\ni 6=j (ei+w\n∗ i )(ei + w ∗ i−ei + wi)>, Cj =\n∑ i6=j 〈w∗i−wi, ei + wi〉ei + wi·ei + wi > , Dj =\n ∑\ni 6=j ziei + wi\n>  \nFor notational simplicity, we also write B,C,D as the corresponding terms with sum ∑d i=1 instead of ∑ i 6=j , so they do not depend on index j. We estimate B,C,D first, then estimate Bj ,Cj ,Dj respectively by taking the differences.\n1. From B to Bj :\nd∑\nj=1\nz>j Bej + wj = ∑\ni,j\nz>j (ei + w ∗ i )(ei + w ∗ i − ei + wi)>ej + wj\n¬ =Tr ([ Z>(I + W) ] [ (I + W∗ − I + W)>I + W ])  ≥ − ∥∥(I + W)>Z ∥∥ F ∥∥∥I + W>(I + W∗ − I + W) ∥∥∥ F\n® ≥− ‖I + W‖2‖I + W‖2 ‖Z‖F ∥∥I + W∗ − I + W ∥∥ F ¯ ≥ − (1 + γ) 2 1− γ ‖Z‖F ∥∥I + W∗ − I + W ∥∥ F\n(23)\nwhere ¬ uses Lemma H.2,  uses Tr(AB) ≥ −‖A‖F ‖B‖F , ® uses ‖AB‖F ≤ ‖A‖2‖B‖F , and ¯ uses Lemma E.1. By Lemma D.1 term 1, we have\n∥∥I + W∗ − I + W ∥∥ F ≤ √∑d i=1 ‖yi‖22 1− 2γ = ‖Y‖F√ 1− 2γ (24)\nOn the other hand,\nd∑\nj=1\nz>j (Bj −B)ej + wj = d∑\nj=1\nz>j (ej + w ∗ j )(ej + w ∗ j − ej + wj)>ej + wj\n= d∑\nj=1\n(w∗j − wj)>(I− 1 2 ej + wj · ej + wj>)(ej + w∗j )(ej + w∗j − ej + wj)>ej + wj\nFor any vector x, ej + wj · ej + wj>x is the projection of x onto the direction ej + wj , so 12 ≤ ‖I− 12ej + wj · ej + wj >‖2 ≤ 1, and\n|(w∗j − wj)>(ej + w∗j )(ej + w∗j − ej + wj)>ej + wj | ¬ ≤ |(w∗j − wj)>(ej + w∗j )| ‖w∗j − wj‖22 2(1− 2γ)\n ≤ ‖w∗j − wj‖32(1 + γ) 2(1− 2γ) ≤ ‖w∗j − wj‖22(1 + γ)γ 1− 2γ (25)\nwhere ¬ uses Lemma D.1 term 2, and  uses Cauchy-Schwartz. Combining (23),(24),(25), we get\nd∑\nj=1\nz>j Bjej + wj ≥ − (1 + γ)2 (1− γ)√1− 2γ ‖Z‖F ‖Y‖F − (1 + γ)γ 1− 2γ ‖W ∗ −W‖2F\n2. From C to Cj :\nd∑\nj=1\nz>j Cej + wj = ∑\ni,j\nz>j 〈w∗i − wi, ei + wi〉ei + wi · ei + wi > ej + wj\n¬ =Tr( [ Z>X ] [ I + W > I + W ] ) = Tr(Z>X) + Tr(Z>X(I + W > I + W − I))\n ≥Tr(Z>X)− ‖Z‖F ‖X‖F ‖I + W > I + W − I‖2 ® ≥ Tr(Z>X)− 4γ\n(1− γ)2 ‖Z‖F ‖X‖F\nwhere ¬ uses Lemma H.2 and xj = 〈w∗j − wj , ej + wj〉ej + wj ,  uses Tr(AB) ≥ −‖A‖F ‖B‖F , and ‖AB‖F ≤ ‖A‖2‖B‖F , and ® uses Lemma E.1. On the other hand,\nd∑\nj=1\nz>j (C−Cj)ej + wj = d∑\nj=1\nz>j 〈w∗j − wj , ej + wj〉ej + wj · ej + wj > ej + wj\n=\nd∑\nj=1\nz>j 〈w∗j − wj , ej + wj〉ej + wj = Tr(Z>X)\nThat implies, 12 ∑d j=1 z > j Cjej + wj ≥ − 2γ(1−γ)2 ‖Z‖F ‖X‖F . 3. From D to Dj :\nd∑\nj=1\nz>j Dej + wj = ∑\ni,j\nz>j ziei + wi > ej + wj = Tr ([ Z>Z ] [ I + W > I + W ]) ≥ (1− γ) 2\n(1 + γ)2 ‖Z‖2F\nwhere the last inequality holds by Lemma E.1. On the other hand,\nz>j (D−Dj)ej + wj = ‖zj‖22 That gives, ∑\nj\nz>j Djej + wj ≥ − 4γ\n(1 + γ)2 ‖Z‖2F\nNow, combining Bj ,Cj ,Dj together, using (22), we have\nd∑\nj=1\nz>j Ajej + wj ≥− (1 + γ)2 (1− γ)√1− 2γ ‖Z‖F ‖Y‖F − (1 + γ)γ 1− 2γ ‖W ∗ −W‖2F\n− 2γ (1− γ)2 ‖Z‖F ‖X‖F −\n4γ\n(1 + γ)2 ‖Z‖2F\nBy definition, we know ‖X‖F ≤ ‖W∗ −W‖F , ‖Y‖F ≤ ‖W∗ −W‖F , ‖Z‖F ≤ ‖W∗ −W‖F , and γ ≤ 1100 , so\n− (1 + γ)γ 1− 2γ ‖W ∗ −W‖2F − 2γ (1− γ)2 ‖Z‖F ‖X‖F − 4γ (1 + γ)2 ‖Z‖2F ≥ −7γ‖W∗ −W‖2F (26)\nMoreover,\n− (\n(1 + γ)2 (1− γ)√1− 2γ − 1 ) ‖Z‖F ‖Y‖F ≥ −0.05γ‖W∗ −W‖2F (27)\nThus, those are small order terms. The only term left is ‖Z‖F ‖Y‖F . By (21), we know\n‖Z‖F ‖Y‖F ≤ √ ‖W∗ −W‖2F − 3\n4 ‖X‖2F\n√ ‖W∗ −W‖2F − ‖X‖2F (28)\nCombining (26), (27), (28), we get:\nd∑\nj=1\nz>j Ajej + wj ≥ −8γ‖W∗ −W‖2F − √ ‖W∗ −W‖2f − 3\n4 ‖X‖2F\n√ ‖W∗ −W‖2F − ‖X‖2F\nNow it remains to bound √ ‖W∗ −W‖2f − 34‖X‖2F √ ‖W∗ −W‖2F − ‖X‖2F .\nLemma H.4.\n− √ ‖W∗ −W‖2F − 3\n4 ‖X‖2F\n√ ‖W∗ −W‖2F − ‖X‖2F ≥ −1.3‖W∗ −W‖2F + ‖W∗ −W‖F ‖X‖F\nProof. Consider the function f(x) = √ y2 − 34x2 √ y2 − x2 + xy, where x ∈ [0, y]. It suffices to show that f(x) ≤ 1.3y2. Indeed, we know\nf ′(x) = x(6x2 − 7y2)\n2 √ 4y2 − 3x2 √ y2 − x2 + y\nWhen x = 0, f ′(x) = y > 0, and when x→ y, f ′(x) < 0. We want to find the place where f ′(x) = 0, which gives the maximum value. Assume x = λy, this is equivalent to solve\nλy(6(λy)2 − 7y2) = −2y √ 4y2 − 3(λy)2 √ y2 − (λy)2\nCancel all y, and we get the solution x ≈ 0.566y, where f(x) ≈ 1.2845y2 < 1.3y2.\nProof of Lemma C.1. Combining Lemma H.3 and Lemma H.4, we have proved Lemma C.1.\nH.2 Proof for Lemma C.2 Again, we first consider the full sum, g = ∑d i=1(‖ei + w∗i ‖2 − ‖ei + wi‖2).\nBy Lemma E.3, we have\n|g − gj | = |‖ej + w∗j ‖2 − ‖ej + wj‖2| ≤ ‖w∗j − wj‖2 Thus by Cauchy Schwartz,\n|(g − gj)〈w∗j − wj , ej + wj〉| ≤ ‖w∗j − wj‖2‖xj‖2 Summing over j, we get\nd∑\nj=1\n|(g − gj)〈w∗j − wj , ej + wj〉| ≤ d∑\nj=1\n‖w∗j − wj‖2‖xj‖2 ≤ ‖W∗ −W‖F ‖X‖F (29)\nwhere the last inequality is by Cauchy Schwartz. Now\ng\nd∑\nj=1\n〈w∗j − wj , ej + wj〉 = g d∑\nj=1\n〈ej + w∗j − ej + wj , ej + wj〉\n=g d∑\nj=1\n(‖ej + w∗j ‖2 − ‖ej + wj‖2 + 〈ej + w∗j , ej + wj − ej + w∗j 〉) = g2 + gb ≥ gb (30)\nwhere b is defined to be ∑d j=1〈ej + w∗j , ej + wj − ej + w∗j 〉. By Lemma D.1 term 2 we know\n− (1 + γ)‖W ∗ −W‖2F\n2(1− 2γ) ≤ b ≤ 0\nCombining (29), (30), the lemma follows.\nd∑\nj=1\n〈gjej + wj , w∗j − wj〉 = d∑\nj=1\n〈(gj − g)ej + wj , w∗j − wj〉+ d∑\nj=1\n〈gej + wj , w∗j − wj〉\n≥ − ‖W∗ −W‖F ‖X‖F + g2 + gb ≥ −‖W∗ −W‖F ‖X‖F − (1 + γ)g‖W∗ −W‖2F\n2(1− 2γ)\nH.3 Proof for Lemma C.3\nd∑\nj=1\n〈P3,j , w∗j − wj〉 = d∑\nj=1\n〈π 2 (w∗j − wj)− θj∗,j(ej + w∗j ) + ‖ej + w∗j ‖ sin θj∗,jej + wj , w∗j − wj〉\n¬ =\nd∑\nj=1\n〈π 2\n(w∗j − wj)− θj∗,j‖ej + w∗j ‖2(ej + w∗j − ej + wj) + αj∗,j |θj∗,j |3‖ej + w∗j ‖ej + wj\n3 , w∗j − wj〉\n ≥π 2 ‖W∗ −W‖2F −\nd∑\nj=1\n1.001(1 + γ)‖w∗j − wj‖22‖ej + w∗j − ej + wj‖2 − d∑\nj=1\n0.335(1 + γ)‖w∗j − wj‖42\n® ≥π 2 ‖W∗ −W‖2F −\nd∑\nj=1\n1.001(1 + γ)√ 1− 2γ ‖w ∗ j − wj‖32 −\nd∑\nj=1\n0.335(1 + γ)‖w∗j − wj‖42\n¯ ≥ (π\n2 − 0.021\n) ‖W∗ −W‖2F\nwhere ¬ uses Taylor’s Theorem for sin θj∗,j , so we know |αj∗,j | ≤ 1.  uses Lemma D.1 term 3 and Cauchy Schwartz, ® uses Lemma D.1 term 1, ¯ holds since γ ≤ 1100 , and the two small order terms can be bounded by 0.021‖W∗ −W‖2F .\nI Proofs for Section 2\nI.1 Proof for Lemma 2.5 By the updating rule, we have\n‖Wt+1 −W∗‖2F = ‖Wt −W∗ − ηGt‖2F = ‖Wt −W∗‖2F − 2〈Wt −W∗, η∇f(W)〉+ η2‖Gt‖2F ≤‖Wt −W∗‖2F − 2〈Wt −W∗, η∇f(W)〉+ η2tG2 ≤ (1− 2ηδ)‖Wt −W∗‖2F + η2G2\nNow if ηδ‖Wt−W∗‖2F ≥ η2G2, we know the ‖Wt−W∗‖2F will decrease by a factor of (1− ηδ) for every step. Otherwise, although it could increase, we know\n‖Wt −W∗‖2F ≤ ηG2\nδ\nBy setting η = (1+α) log TδT , we know after T steps, either ‖WT −W∗‖2F is already smaller than ηG2\nδ = (1+α) log TG2\nδ2T , or it is decreasing by factor of (1− ηδ) for every step, which means\n‖WT −W∗‖2F ≤ ‖W0 −W∗‖2F (1− ηδ)T ≤ D2e−ηδT = D2e−(1+α) log T = D2T−α\nT ≤ (1 + α) log TG\n2\nδ2T .\nThe last inequality holds since\nTα log T ≥ D 2δ2\n(1 + α)G2\nThus, ‖WT −W∗‖2F will be smaller than (1+α) log TG 2 δ2T ."
    } ],
    "references" : [ {
      "title" : "Learning polynomials with neural networks",
      "author" : [ "Alexandr Andoni", "Rina Panigrahy", "Gregory Valiant", "Li Zhang" ],
      "venue" : "In ICML, pages 1908–1916,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2014
    }, {
      "title" : "Provable bounds for learning some deep representations",
      "author" : [ "Sanjeev Arora", "Aditya Bhaskara", "Rong Ge", "Tengyu Ma" ],
      "venue" : "In Proceedings of the 31th International Conference on Machine Learning,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2014
    }, {
      "title" : "Universal approximation bounds for superpositions of a sigmoidal function",
      "author" : [ "Andrew R. Barron" ],
      "venue" : "IEEE Trans. Information Theory,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 1993
    }, {
      "title" : "The loss surfaces of multilayer networks",
      "author" : [ "Anna Choromanska", "Mikael Henaff", "Michaël Mathieu", "Gérard Ben Arous", "Yann LeCun" ],
      "venue" : "In AISTATS,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2015
    }, {
      "title" : "Open problem: The landscape of the loss surfaces of multilayer networks",
      "author" : [ "Anna Choromanska", "Yann LeCun", "Gérard Ben Arous" ],
      "venue" : "In Proceedings of The 28th Conference on Learning Theory, COLT",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2015
    }, {
      "title" : "Approximation by superpositions of a sigmoidal function",
      "author" : [ "George Cybenko" ],
      "venue" : "MCSS,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 1992
    }, {
      "title" : "Toward deeper understanding of neural networks: The power of initialization and a dual view on expressivity",
      "author" : [ "Amit Daniely", "Roy Frostig", "Yoram Singer" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2016
    }, {
      "title" : "Identifying and attacking the saddle point problem in high-dimensional non-convex optimization",
      "author" : [ "Yann N Dauphin", "Razvan Pascanu", "Caglar Gulcehre", "Kyunghyun Cho", "Surya Ganguli", "Yoshua Bengio" ],
      "venue" : "NIPS",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2014
    }, {
      "title" : "Adaptive subgradient methods for online learning and stochastic optimization",
      "author" : [ "John C. Duchi", "Elad Hazan", "Yoram Singer" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2011
    }, {
      "title" : "Escaping from saddle points - online stochastic gradient for tensor decomposition",
      "author" : [ "Rong Ge", "Furong Huang", "Chi Jin", "Yang Yuan" ],
      "venue" : "In COLT 2015,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2015
    }, {
      "title" : "Understanding the difficulty of training deep feedforward neural networks",
      "author" : [ "Xavier Glorot", "Yoshua Bengio" ],
      "venue" : "In AISTATS,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2010
    }, {
      "title" : "Deep sparse rectifier neural networks",
      "author" : [ "Xavier Glorot", "Antoine Bordes", "Yoshua Bengio" ],
      "venue" : "In AISTATS,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2011
    }, {
      "title" : "Reliably learning the relu in polynomial time",
      "author" : [ "Surbhi Goel", "Varun Kanade", "Adam R. Klivans", "Justin Thaler" ],
      "venue" : null,
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2016
    }, {
      "title" : "Qualitatively characterizing neural network optimization problems",
      "author" : [ "Ian J. Goodfellow", "Oriol Vinyals" ],
      "venue" : "CoRR, abs/1412.6544,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2014
    }, {
      "title" : "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification",
      "author" : [ "Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun" ],
      "venue" : "In ICCV,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2015
    }, {
      "title" : "Deep residual learning for image recognition",
      "author" : [ "Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun" ],
      "venue" : "In CVPR,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2016
    }, {
      "title" : "Multilayer feedforward networks are universal approximators",
      "author" : [ "Kurt Hornik", "Maxwell B. Stinchcombe", "Halbert White" ],
      "venue" : "Neural Networks,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 1989
    }, {
      "title" : "Beating the perils of non-convexity: Guaranteed training of neural networks using tensor methods",
      "author" : [ "Majid Janzamin", "Hanie Sedghi", "Anima Anandkumar" ],
      "venue" : "arXiv preprint arXiv:1506.08473,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2015
    }, {
      "title" : "Deep learning without poor local minima",
      "author" : [ "Kenji Kawaguchi" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2016
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik P. Kingma", "Jimmy Ba" ],
      "venue" : "CoRR, abs/1412.6980,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2014
    }, {
      "title" : "Efficient BackProp, pages 9–50",
      "author" : [ "Yann LeCun", "Leon Bottou", "Genevieve B. Orr", "Klaus Robert Müller" ],
      "venue" : null,
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 1998
    }, {
      "title" : "On the computational efficiency of training neural networks",
      "author" : [ "Roi Livni", "Shai Shalev-Shwartz", "Ohad Shamir" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2014
    }, {
      "title" : "On the number of linear regions of deep neural networks",
      "author" : [ "Guido F. Montúfar", "Razvan Pascanu", "KyungHyun Cho", "Yoshua Bengio" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2014
    }, {
      "title" : "Rectified linear units improve restricted boltzmann machines",
      "author" : [ "Vinod Nair", "Geoffrey E. Hinton" ],
      "venue" : "In ICML,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2010
    }, {
      "title" : "Expressiveness of rectifier networks",
      "author" : [ "Xingyuan Pan", "Vivek Srikumar" ],
      "venue" : "In ICML,",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2016
    }, {
      "title" : "On the number of inference regions of deep feed forward networks with piece-wise linear activations",
      "author" : [ "Razvan Pascanu", "Guido Montúfar", "Yoshua Bengio" ],
      "venue" : "CoRR, abs/1312.6098,",
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 2013
    }, {
      "title" : "Non-asymptotic theory of random matrices: extreme singular values",
      "author" : [ "M. Rudelson", "R. Vershynin" ],
      "venue" : "ArXiv e-prints,",
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 2010
    }, {
      "title" : "Dynamics of on-line gradient descent learning for multilayer neural networks",
      "author" : [ "David Saad", "Sara A. Solla" ],
      "venue" : "Advances in Neural Information Processing Systems,",
      "citeRegEx" : "30",
      "shortCiteRegEx" : "30",
      "year" : 1996
    }, {
      "title" : "On the quality of the initial basin in overspecified neural networks",
      "author" : [ "Itay Safran", "Ohad Shamir" ],
      "venue" : "In ICML,",
      "citeRegEx" : "31",
      "shortCiteRegEx" : "31",
      "year" : 2016
    }, {
      "title" : "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks",
      "author" : [ "Andrew M. Saxe", "James L. McClelland", "Surya Ganguli" ],
      "venue" : "CoRR, abs/1312.6120,",
      "citeRegEx" : "32",
      "shortCiteRegEx" : "32",
      "year" : 2013
    }, {
      "title" : "Provable methods for training neural networks with sparse connectivity",
      "author" : [ "Hanie Sedghi", "Anima Anandkumar" ],
      "venue" : null,
      "citeRegEx" : "33",
      "shortCiteRegEx" : "33",
      "year" : 2015
    }, {
      "title" : "Distribution-specific hardness of learning",
      "author" : [ "Ohad Shamir" ],
      "venue" : "neural networks. CoRR,",
      "citeRegEx" : "34",
      "shortCiteRegEx" : "34",
      "year" : 2016
    }, {
      "title" : "Training a single sigmoidal neuron is hard",
      "author" : [ "Jirí Síma" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "35",
      "shortCiteRegEx" : "35",
      "year" : 2002
    }, {
      "title" : "On the importance of initialization and momentum in deep learning",
      "author" : [ "Ilya Sutskever", "James Martens", "George E. Dahl", "Geoffrey E. Hinton" ],
      "venue" : "In ICML,",
      "citeRegEx" : "36",
      "shortCiteRegEx" : "36",
      "year" : 2013
    }, {
      "title" : "Symmetry-breaking convergence analysis of certain two-layered neural networks with relu nonlinearity",
      "author" : [ "Yuandong Tian" ],
      "venue" : "In Submitted to ICLR",
      "citeRegEx" : "37",
      "shortCiteRegEx" : "37",
      "year" : 2016
    }, {
      "title" : "Diversity leads to generalization in neural networks",
      "author" : [ "Bo Xie", "Yingyu Liang", "Le Song" ],
      "venue" : "In AISTATS,",
      "citeRegEx" : "38",
      "shortCiteRegEx" : "38",
      "year" : 2017
    } ],
    "referenceMentions" : [ {
      "referenceID" : 16,
      "context" : "It is well known that neural networks have great expressive power [19, 6, 3, 7, 27].",
      "startOffset" : 66,
      "endOffset" : 83
    }, {
      "referenceID" : 5,
      "context" : "It is well known that neural networks have great expressive power [19, 6, 3, 7, 27].",
      "startOffset" : 66,
      "endOffset" : 83
    }, {
      "referenceID" : 2,
      "context" : "It is well known that neural networks have great expressive power [19, 6, 3, 7, 27].",
      "startOffset" : 66,
      "endOffset" : 83
    }, {
      "referenceID" : 6,
      "context" : "It is well known that neural networks have great expressive power [19, 6, 3, 7, 27].",
      "startOffset" : 66,
      "endOffset" : 83
    }, {
      "referenceID" : 24,
      "context" : "It is well known that neural networks have great expressive power [19, 6, 3, 7, 27].",
      "startOffset" : 66,
      "endOffset" : 83
    }, {
      "referenceID" : 33,
      "context" : ", SGD, Momentum [36], Adagrad [9], Adam [22]), but to the best of our knowledge, there were no theoretical guarantees that such methods will find good weights.",
      "startOffset" : 16,
      "endOffset" : 20
    }, {
      "referenceID" : 8,
      "context" : ", SGD, Momentum [36], Adagrad [9], Adam [22]), but to the best of our knowledge, there were no theoretical guarantees that such methods will find good weights.",
      "startOffset" : 30,
      "endOffset" : 33
    }, {
      "referenceID" : 19,
      "context" : ", SGD, Momentum [36], Adagrad [9], Adam [22]), but to the best of our knowledge, there were no theoretical guarantees that such methods will find good weights.",
      "startOffset" : 40,
      "endOffset" : 44
    }, {
      "referenceID" : 34,
      "context" : "For this basic network, it is known that even in the simplified setting where the weights are initialized symmetrically and the ground truth forms orthonormal basis, gradient descent might get stuck at saddle points [37].",
      "startOffset" : 216,
      "endOffset" : 220
    }, {
      "referenceID" : 15,
      "context" : "Inspired by the structure of residual network (ResNet) [18], we add an extra identity mapping for the hidden layer (see Figure 1).",
      "startOffset" : 55,
      "endOffset" : 59
    }, {
      "referenceID" : 27,
      "context" : "Following the standard setting [30, 37], we assume that there exists a two-layer teacher network with weight W∗.",
      "startOffset" : 31,
      "endOffset" : 39
    }, {
      "referenceID" : 34,
      "context" : "Following the standard setting [30, 37], we assume that there exists a two-layer teacher network with weight W∗.",
      "startOffset" : 31,
      "endOffset" : 39
    }, {
      "referenceID" : 7,
      "context" : "Another common belief is that neural network has lots of local minima and saddle points [8], so even if there exists a global minimum, we may not be able to arrive there.",
      "startOffset" : 88,
      "endOffset" : 91
    }, {
      "referenceID" : 16,
      "context" : "For example, two-layer network with sigmoid activations could approximate any continuous function [19, 6, 3].",
      "startOffset" : 98,
      "endOffset" : 108
    }, {
      "referenceID" : 5,
      "context" : "For example, two-layer network with sigmoid activations could approximate any continuous function [19, 6, 3].",
      "startOffset" : 98,
      "endOffset" : 108
    }, {
      "referenceID" : 2,
      "context" : "For example, two-layer network with sigmoid activations could approximate any continuous function [19, 6, 3].",
      "startOffset" : 98,
      "endOffset" : 108
    }, {
      "referenceID" : 23,
      "context" : "ReLU is the state-of-the-art activation function, and performs much better than sigmoid [26, 12], but fewer and weaker results on expressivity of feedforward ReLU networks are known [25, 28, 27].",
      "startOffset" : 88,
      "endOffset" : 96
    }, {
      "referenceID" : 11,
      "context" : "ReLU is the state-of-the-art activation function, and performs much better than sigmoid [26, 12], but fewer and weaker results on expressivity of feedforward ReLU networks are known [25, 28, 27].",
      "startOffset" : 88,
      "endOffset" : 96
    }, {
      "referenceID" : 22,
      "context" : "ReLU is the state-of-the-art activation function, and performs much better than sigmoid [26, 12], but fewer and weaker results on expressivity of feedforward ReLU networks are known [25, 28, 27].",
      "startOffset" : 182,
      "endOffset" : 194
    }, {
      "referenceID" : 25,
      "context" : "ReLU is the state-of-the-art activation function, and performs much better than sigmoid [26, 12], but fewer and weaker results on expressivity of feedforward ReLU networks are known [25, 28, 27].",
      "startOffset" : 182,
      "endOffset" : 194
    }, {
      "referenceID" : 24,
      "context" : "ReLU is the state-of-the-art activation function, and performs much better than sigmoid [26, 12], but fewer and weaker results on expressivity of feedforward ReLU networks are known [25, 28, 27].",
      "startOffset" : 182,
      "endOffset" : 194
    }, {
      "referenceID" : 32,
      "context" : "Most previous results on learning neural network are negative [35, 24, 34], or positive but with special algorithms other than SGD [20, 39, 33, 13], or with strong assumptions on the model [1, 2].",
      "startOffset" : 62,
      "endOffset" : 74
    }, {
      "referenceID" : 21,
      "context" : "Most previous results on learning neural network are negative [35, 24, 34], or positive but with special algorithms other than SGD [20, 39, 33, 13], or with strong assumptions on the model [1, 2].",
      "startOffset" : 62,
      "endOffset" : 74
    }, {
      "referenceID" : 31,
      "context" : "Most previous results on learning neural network are negative [35, 24, 34], or positive but with special algorithms other than SGD [20, 39, 33, 13], or with strong assumptions on the model [1, 2].",
      "startOffset" : 62,
      "endOffset" : 74
    }, {
      "referenceID" : 17,
      "context" : "Most previous results on learning neural network are negative [35, 24, 34], or positive but with special algorithms other than SGD [20, 39, 33, 13], or with strong assumptions on the model [1, 2].",
      "startOffset" : 131,
      "endOffset" : 147
    }, {
      "referenceID" : 30,
      "context" : "Most previous results on learning neural network are negative [35, 24, 34], or positive but with special algorithms other than SGD [20, 39, 33, 13], or with strong assumptions on the model [1, 2].",
      "startOffset" : 131,
      "endOffset" : 147
    }, {
      "referenceID" : 12,
      "context" : "Most previous results on learning neural network are negative [35, 24, 34], or positive but with special algorithms other than SGD [20, 39, 33, 13], or with strong assumptions on the model [1, 2].",
      "startOffset" : 131,
      "endOffset" : 147
    }, {
      "referenceID" : 0,
      "context" : "Most previous results on learning neural network are negative [35, 24, 34], or positive but with special algorithms other than SGD [20, 39, 33, 13], or with strong assumptions on the model [1, 2].",
      "startOffset" : 189,
      "endOffset" : 195
    }, {
      "referenceID" : 1,
      "context" : "Most previous results on learning neural network are negative [35, 24, 34], or positive but with special algorithms other than SGD [20, 39, 33, 13], or with strong assumptions on the model [1, 2].",
      "startOffset" : 189,
      "endOffset" : 195
    }, {
      "referenceID" : 28,
      "context" : "[31] proved that with high probability, there exists a continuous decreasing path from random initial point to the global minimum, but SGD may not follow this path.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 29,
      "context" : "Some previous works simplified the model by ignoring the activation functions and considering deep linear networks [32, 21] or deep linear residual networks [16], which can only learn linear functions.",
      "startOffset" : 115,
      "endOffset" : 123
    }, {
      "referenceID" : 18,
      "context" : "Some previous works simplified the model by ignoring the activation functions and considering deep linear networks [32, 21] or deep linear residual networks [16], which can only learn linear functions.",
      "startOffset" : 115,
      "endOffset" : 123
    }, {
      "referenceID" : 3,
      "context" : "Some previous results are based on independent activation assumption that the activations of ReLU and the input are independent, which is unrealistic [4, 21].",
      "startOffset" : 150,
      "endOffset" : 157
    }, {
      "referenceID" : 18,
      "context" : "Some previous results are based on independent activation assumption that the activations of ReLU and the input are independent, which is unrealistic [4, 21].",
      "startOffset" : 150,
      "endOffset" : 157
    }, {
      "referenceID" : 7,
      "context" : "It is observed that saddle point is not a big problem for neural networks [8, 15].",
      "startOffset" : 74,
      "endOffset" : 81
    }, {
      "referenceID" : 13,
      "context" : "It is observed that saddle point is not a big problem for neural networks [8, 15].",
      "startOffset" : 74,
      "endOffset" : 81
    }, {
      "referenceID" : 9,
      "context" : "In general, if the objective is strict-saddle [10], SGD could escape all saddle points.",
      "startOffset" : 46,
      "endOffset" : 50
    }, {
      "referenceID" : 34,
      "context" : "1 (Eqn (13) from [37]).",
      "startOffset" : 17,
      "endOffset" : 21
    }, {
      "referenceID" : 20,
      "context" : "Randomly initializing the weights with O(1/ √ d) is standard in deep learning, see [23, 11, 17].",
      "startOffset" : 83,
      "endOffset" : 95
    }, {
      "referenceID" : 10,
      "context" : "Randomly initializing the weights with O(1/ √ d) is standard in deep learning, see [23, 11, 17].",
      "startOffset" : 83,
      "endOffset" : 95
    }, {
      "referenceID" : 14,
      "context" : "Randomly initializing the weights with O(1/ √ d) is standard in deep learning, see [23, 11, 17].",
      "startOffset" : 83,
      "endOffset" : 95
    }, {
      "referenceID" : 26,
      "context" : "It is also well known that if the entries are initialized with O(1/ √ d), the spectral norm of the random matrix is O(1) [29].",
      "startOffset" : 121,
      "endOffset" : 125
    }, {
      "referenceID" : 3,
      "context" : "The assumption that the input follows a Gaussian distribution is not necessarily true in practice (Although this is a common assumption appeared in the previous papers [4, 37, 38], and also considered plausible in [5]).",
      "startOffset" : 168,
      "endOffset" : 179
    }, {
      "referenceID" : 34,
      "context" : "The assumption that the input follows a Gaussian distribution is not necessarily true in practice (Although this is a common assumption appeared in the previous papers [4, 37, 38], and also considered plausible in [5]).",
      "startOffset" : 168,
      "endOffset" : 179
    }, {
      "referenceID" : 35,
      "context" : "The assumption that the input follows a Gaussian distribution is not necessarily true in practice (Although this is a common assumption appeared in the previous papers [4, 37, 38], and also considered plausible in [5]).",
      "startOffset" : 168,
      "endOffset" : 179
    }, {
      "referenceID" : 4,
      "context" : "The assumption that the input follows a Gaussian distribution is not necessarily true in practice (Although this is a common assumption appeared in the previous papers [4, 37, 38], and also considered plausible in [5]).",
      "startOffset" : 214,
      "endOffset" : 217
    }, {
      "referenceID" : 29,
      "context" : "Moreover, previous analyses either ignore the nonlinear activations and thus consider linear model [32, 21, 16], or directly [4, 21] or indirectly [37]1 assume that the activations are independent.",
      "startOffset" : 99,
      "endOffset" : 111
    }, {
      "referenceID" : 18,
      "context" : "Moreover, previous analyses either ignore the nonlinear activations and thus consider linear model [32, 21, 16], or directly [4, 21] or indirectly [37]1 assume that the activations are independent.",
      "startOffset" : 99,
      "endOffset" : 111
    }, {
      "referenceID" : 3,
      "context" : "Moreover, previous analyses either ignore the nonlinear activations and thus consider linear model [32, 21, 16], or directly [4, 21] or indirectly [37]1 assume that the activations are independent.",
      "startOffset" : 125,
      "endOffset" : 132
    }, {
      "referenceID" : 18,
      "context" : "Moreover, previous analyses either ignore the nonlinear activations and thus consider linear model [32, 21, 16], or directly [4, 21] or indirectly [37]1 assume that the activations are independent.",
      "startOffset" : 125,
      "endOffset" : 132
    }, {
      "referenceID" : 34,
      "context" : "Moreover, previous analyses either ignore the nonlinear activations and thus consider linear model [32, 21, 16], or directly [4, 21] or indirectly [37]1 assume that the activations are independent.",
      "startOffset" : 147,
      "endOffset" : 151
    }, {
      "referenceID" : 4,
      "context" : "As pointed out by [5], eliminating the unrealistic assumptions on activation independence is the central problem of analyzing the loss surface of neural network, which was not fully addressed by the previous analyses.",
      "startOffset" : 18,
      "endOffset" : 21
    }, {
      "referenceID" : 15,
      "context" : "1 Importance of identity mapping In this experiment, we compare the standard ResNet [18] and single skip model where identity mapping skips only one layer.",
      "startOffset" : 84,
      "endOffset" : 88
    }, {
      "referenceID" : 0,
      "context" : "References [1] Alexandr Andoni, Rina Panigrahy, Gregory Valiant, and Li Zhang.",
      "startOffset" : 11,
      "endOffset" : 14
    }, {
      "referenceID" : 1,
      "context" : "[2] Sanjeev Arora, Aditya Bhaskara, Rong Ge, and Tengyu Ma.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 2,
      "context" : "[3] Andrew R.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 3,
      "context" : "[4] Anna Choromanska, Mikael Henaff, Michaël Mathieu, Gérard Ben Arous, and Yann LeCun.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 4,
      "context" : "[5] Anna Choromanska, Yann LeCun, and Gérard Ben Arous.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 5,
      "context" : "[6] George Cybenko.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 6,
      "context" : "[7] Amit Daniely, Roy Frostig, and Yoram Singer.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 7,
      "context" : "[8] Yann N Dauphin, Razvan Pascanu, Caglar Gulcehre, Kyunghyun Cho, Surya Ganguli, and Yoshua Bengio.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 8,
      "context" : "[9] John C.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 9,
      "context" : "[10] Rong Ge, Furong Huang, Chi Jin, and Yang Yuan.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 10,
      "context" : "[11] Xavier Glorot and Yoshua Bengio.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 11,
      "context" : "[12] Xavier Glorot, Antoine Bordes, and Yoshua Bengio.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 12,
      "context" : "[13] Surbhi Goel, Varun Kanade, Adam R.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 13,
      "context" : "[15] Ian J.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 14,
      "context" : "[17] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 15,
      "context" : "[18] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 16,
      "context" : "[19] Kurt Hornik, Maxwell B.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 17,
      "context" : "[20] Majid Janzamin, Hanie Sedghi, and Anima Anandkumar.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 18,
      "context" : "[21] Kenji Kawaguchi.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 19,
      "context" : "[22] Diederik P.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 20,
      "context" : "[23] Yann LeCun, Leon Bottou, Genevieve B.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 21,
      "context" : "[24] Roi Livni, Shai Shalev-Shwartz, and Ohad Shamir.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 22,
      "context" : "[25] Guido F.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 23,
      "context" : "[26] Vinod Nair and Geoffrey E.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 24,
      "context" : "[27] Xingyuan Pan and Vivek Srikumar.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 25,
      "context" : "[28] Razvan Pascanu, Guido Montúfar, and Yoshua Bengio.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 26,
      "context" : "[29] M.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 27,
      "context" : "[30] David Saad and Sara A.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 28,
      "context" : "[31] Itay Safran and Ohad Shamir.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 29,
      "context" : "[32] Andrew M.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 30,
      "context" : "[33] Hanie Sedghi and Anima Anandkumar.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 31,
      "context" : "[34] Ohad Shamir.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 32,
      "context" : "[35] Jirí Síma.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 33,
      "context" : "[36] Ilya Sutskever, James Martens, George E.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 34,
      "context" : "[37] Yuandong Tian.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 35,
      "context" : "[38] Bo Xie, Yingyu Liang, and Le Song.",
      "startOffset" : 0,
      "endOffset" : 4
    } ],
    "year" : 2017,
    "abstractText" : "In recent years, stochastic gradient descent (SGD) based techniques has become the standard tools for training neural networks. However, formal theoretical understanding of why SGD can train neural networks in practice is largely missing. In this paper, we make progress on understanding this mystery by providing a convergence analysis for SGD on a rich subset of two-layer feedforward networks with ReLU activations. This subset is characterized by a special structure called “identity mapping”. We prove that, if input follows from Gaussian distribution, with standard O(1/ √ d) initialization of the weights, SGD converges to the global minimum in polynomial number of steps. Unlike normal vanilla networks, the “identity mapping” makes our network asymmetric and thus the global minimum is unique. To complement our theory, we are also able to show experimentally that multi-layer networks with this mapping have better performance compared with normal vanilla networks. Our convergence theorem differs from traditional non-convex optimization techniques. We show that SGD converges to optimal in “two phases”: In phase I, the gradient points to the wrong direction, however, a potential function g gradually decreases. Then in phase II, SGD enters a nice one point convex region and converges. We also show that the identity mapping is necessary for convergence, as it moves the initial point to a better place for optimization. Experiment verifies our claims.",
    "creator" : "LaTeX with hyperref package"
  }
}