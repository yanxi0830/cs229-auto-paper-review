{
  "name" : "1205.2664.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "A Bayesian Sampling Approach to Exploration in Reinforcement Learning",
    "authors" : [ "John Asmuth", "Lihong Li", "Michael L. Littman", "Ali Nouri", "David Wingate" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "We present a modular approach to reinforcement learning that uses a Bayesian representation of the uncertainty over models. The approach, BOSS (Best of Sampled Set), drives exploration by sampling multiple models from the posterior and selecting actions optimistically. It extends previous work by providing a rule for deciding when to resample and how to combine the models. We show that our algorithm achieves nearoptimal reward with high probability with a sample complexity that is low relative to the speed at which the posterior distribution converges during learning. We demonstrate that BOSS performs quite favorably compared to state-of-the-art reinforcement-learning approaches and illustrate its flexibility by pairing it with a non-parametric model that generalizes across states."
    }, {
      "heading" : "1 INTRODUCTION",
      "text" : "The exploration-exploitation dilemma is a defining problem in the field of reinforcement learning (RL). To behave in a way that attains high reward, an agent must acquire experience that reveals the structure of its environment, reducing its uncertainty about the dynamics. A broad spectrum of exploration approaches has been studied, which can be coarsely classified as belief-lookahead, myopic, and undirected approaches.\nBelief-lookahead approaches are desirable because they make optimal decisions in the face of their uncertainty. However, they are generally intractable forcing algorithm designers to create approximations that sacrifice optimality. A state-of-the-art belief-lookahead approach is BEETLE (Poupart et al., 2006), which plans in the continuous belief space defined by the agent’s uncertainty.\nMyopic (Wang et al., 2005) approaches make decisions to reduce uncertainty, but they do not explicitly consider how this reduced uncertainty will impact future reward. While myopic approaches can lay no claim to optimality in general, some include guarantees on their total regret or on the number of subtoptimal decisions made during learning. An example of such an algorithm is RMAX (Brafman & Tennenholtz, 2002), which distinguishes “known” and “unknown” states based on how often they have been visited. It explores by acting to maximize reward under the assumption that unknown states deliver maximum reward.\nUndirected (Thrun, 1992) approaches take exploratory actions, but without regard to what parts of their environment models remain uncertain. Classic approaches such as -greedy and Boltzmann exploration that choose random actions occasionally fall into this category. The guarantees possible for this class of algorithms are generally weaker—convergence to optimal behavior in the limit, for example. A sophisticated approach that falls into this category is Bayesian DP (Strens, 2000). It maintains a Bayesian posterior over models and periodically draws a sample from this distribution. It then acts optimally with respect to this sampled model.\nThe algorithm proposed in this paper (Section 2) is a myopic Bayesian approach that maintains its uncertainty in the form of a posterior over models. As new information becomes available, it draws a set of samples from this posterior and acts optimistically with respect to this collection—the best of sampled set (or BOSS). We show that, with high probability, it takes near-optimal actions on all but a small number of trials (Section 3). We have found that its behavior is quite promising, exploring better than undirected approaches and scaling better than belief-lookahead approaches (Section 4). We also demonstrate its compatibility with sophisticated Bayesian models, resulting in an approach that can generalize experience between states (Section 5).\nNote that our analysis assumes a black box algorithm that can sample from a posterior in the appropriate model class. Although great strides have been made recently in representing and sampling from Bayesian posteriors, it remains a challenging and often intractable problem. The correctness of our algorithm also requires that the prior it uses is an accurate description of the space of models—as if the environment is chosen from the algorithm’s prior. Some assumption of this form is necessary for a Bayesian approach to show any benefits over an algorithm that makes a worst-case assumption."
    }, {
      "heading" : "2 BOSS: BEST OF SAMPLED SET",
      "text" : "The idea of sampling from the posterior for decision making has been around for decades (Thompson, 1933). Several recent algorithms have used this technique for Bayesian RL (Strens, 2000; Wilson et al., 2007). In this context, Bayesian posteriors are maintained over the space of Markov decision processes (MDPs) and sampling the posterior requires drawing a complete MDP from this distribution.\nAny sampling approach must address a few key questions: 1) When to sample, 2) How many models to sample, and 3) How to combine models. A natural approach to the first question is to resample after every T timesteps, for some fixed T . There are challenges to selecting the right value of T , however. Small T can lead to “thrashing” behavior in which the agent rapidly switches exploration plans and ends up making little progress. Large T can lead to slow learning as new information in the posterior is not exploited between samples. Strens (2000) advocates a T approximating the depth of exploratory planning required. He suggests several ways to address the third question, leaving their investigation for future work.\nBOSS provides a novel answer to these questions. It samples multiple models (K) from the posterior whenever the number of transitions from a state–action pair reaches a pre-defined threshold (B). It then combines the results into an optimistic MDP for decision making—a process we call merging. Analogously to RMAX, once a state–action pair has been observed B times, we call it known.\nIn what follows, we use S to refer to the size of the state space, A the size of the action space, and γ the discount factor. All sampled MDPs share these quantities, but differ in their transition functions. For simplicity, we assume the reward function is known in advance; otherwise, it can be encoded in the transitions.\nGiven K sampled models from the posterior, m1,m2, · · · ,mK , merging is the process of creating a\nnew MDP, m#, with the same state space, but an augmented action space of KA actions. Each action ai,j in m#, for i ∈ {1, · · · ,K}, j ∈ {1, · · · , A}, corresponds to the jth action in mi. Transition and reward functions are formed straightforwardly—the transition function for ai,j is copied from the one for aj in mi, for example. Finally, for any state s, if a policy in m# is to take an action aij , then the actual action taken in the original MDP is aj . A complete description of BOSS is given in Algorithm 1.\nAlgorithm 1 BOSS Algorithm 0: Inputs: K,B 1: Initialize the current state s1. 2: do sample← TRUE. 3: qs,a ← 0,∀s, a 4: for all timesteps t = 1, 2, 3, . . . do 5: if do sample then 6: Sample K models m1,m2, · · · ,mK from the posterior (initially, the prior) distribution. 7: Merge the models into the mixed MDP m#. 8: Solve m# to obtain πm# . 9: do sample← FALSE. 10: end if 11: Use πm# for action selection: at ← πm#(st), and observe reward rt and next state st+1. 12: qst,at ← qst,at + 1. 13: Update the posterior distribution based the transition (st, at, rt, st+1). 14: if qst,at = B then 15: do sample← TRUE 16: end if 17: end for\nBOSS solves no more than SA merged MDPs, requiring polynomial time for planning. It draws a maximum of KSA samples. Thus, in distributions in which sampling can be done efficiently, the overall computational demands are relatively low."
    }, {
      "heading" : "3 ANALYSIS",
      "text" : "This section provides a formal analysis of BOSS’s efficiency of exploration. We view the algorithm as a non-stationary policy, for which a value function can be defined. As such, the value of state s, when visited by algorithm A at time t, denoted by V At(st), is the expected discounted sum of future rewards the algorithm will collect after visiting s at time t. Our goal is to show that, when parameters K and B are chosen appropriately, with high probability, V At(st) will be -close to optimal except for a polynomial number of steps (Theorem 3.1). Our objective, and some of our techniques, closely follow work in the PAC-MDP framework (Kakade, 2003; Strehl et al., 2006)."
    }, {
      "heading" : "3.1 A GENERAL SAMPLE COMPLEXITY BOUND FOR BOSS",
      "text" : "Let m∗ be the true MDP. When possible, we denote quantities related to this MDP, such as V ∗m∗ , by their shorthand versions, V ∗. By assumption, the true MDP m∗ is drawn from the prior distribution, and so after observing a sequence of transitions, m∗ may be viewed as being drawn from the posterior distribution.\nLemma 3.1 Let s0 be a fixed state, p′ the posterior distribution over MDPs, and δ1 ∈ (0, 1). If the sample size K = Θ( 1δ1 ln 1 δ1 ), then with probability at least 1 − δ1, a model among these K models is optimistic compared to m∗ in s0: maxi V ∗mi(s0) ≥ V ∗(s0).\nProof (sketch). For any fixed, true model m∗, define P as the probability of sampling an optimistic model according to p′:\nP = ∑ m∈M p′(m)I (V πmm (s0) ≥ V πm∗ m∗ (s0)) ,\nwhere I(·) is the set-indicator function andM is the set of MDPs. We consider two mutually exclusive cases. In the first case where P ≥ δ1/2, the probability that none of the K sampled models is optimistic is (1 − P )K , which is at most (1 − δ1/2)K . Let this failure probability (1− δ1/2)K be δ1/2 and solve for K to get\nK = log(δ1/2)\nlog(1− δ1/2) = Θ ( 1 δ1 log 1 δ1 ) .\nThe other case where P < δ1/2 happens with small probability since the chance of drawing any model, including m∗, from that part of the posterior is at most δ1/2. Combining these two cases, the probability that no optimistic model is included in the K samples is at most δ1/2 + δ1/2 = δ1.\nLemma 3.2 The sample size K = Θ(S 2A δ ln SA δ ) suffices to guarantee V ∗m#(s) ≥ V ∗(s) for all s during the entire learning process with probability at least 1− δ.\nProof (sketch). For each model-sampling step, the construction of m# implies V ∗m#(s) ≥ V ∗ mi(s). By a union bound over all state–action pairs and Lemma 3.1, we have V ∗m#(s) ≥ V\n∗(s) for all s with probability at least 1−Sδ1. During the entire learning process, there are at most SA model-sampling steps. Applying a union bound again to these steps, we know V ∗m#(s) ≥ V\n∗(s) for all s in every K-sample set with probability at least 1 − S2Aδ1. Letting δ = S2Aδ1 completes the proof.\nTo simplify analysis, we assume that samples in a state–action pair do not affect the posterior of transition probabilities in other state–actions. However, the\nresult should hold more generally with respect to the posterior induced by the experience in the other states. Define the Bayesian concentration sample complexity, f(s, a, , δ, ρ), as the minimum number c such that, if c IID transitions from (s, a) are observed, then with probability 1 − δ the following holds true: an -ball (measured by `1-distance) centered at the true model m∗ has at least 1− ρ probability mass in the posterior distribution. Formally, with probability at least 1− δ,\nPr m∼posterior (‖Tm(s, a)− Tm∗(s, a)‖1 < ) ≥ 1− ρ.\nWe call ρ the diffusion parameter.\nLemma 3.3 If the knownness parameter B = maxs,a f(s, a, , δSA , ρ S2A2K ), then the transition function of all the sampled models are -close (in the `1 sense) to the true transition function for all the known state–action pairs during the entire learning process with probability at least 1− δ − ρ.\nProof (sketch). The proof consists of several applications of the union bound. The first is applied to all state–action pairs, implying the posterior concentrates around the true model for all state–action pairs with diffusion ρ′ = ρS2A2K with probability at least 1− δ. Now, suppose the posterior concentrates around m∗ with diffusion ρ′. For any known (s, a), the probability that a sampled MDP’s transition function in (s, a) is -accurate is at least 1 − ρ′, according to the definition of f . By the union bound, the sampled MDP’s transition function is -accurate in all known state– action pairs with probability at least 1 − SAρ′. A union bound is applied a second time to the K sampled models, implying all K sampled MDPs’ transition functions are -accurate in all known state–action pairs with probability at least 1−SAKρ′. Finally, using a union bound a third time to all model-sampling steps in BOSS, we know that all sampled models have -accurate transitions in all known (s, a) with probability at least 1 − S2A2Kρ′ = 1 − ρ. Combining this result with the δ failure probability in the previous paragraph completes the proof.\nTheorem 3.1 When the knownness parameter B = maxs,a f ( s, a, (1− γ)2, δSA , δ S2A2K ) , then with probability at least 1− 4δ, V At(st) ≥ V ∗(st)− 4 in all but ζ( , δ) = O ( SAB\n(1−γ)2 ln 1 δ ln 1 (1−γ)\n) steps.\nProof (sketch). The proof relies on a general PACMDP theorem by Strehl et al. (2006) by verifying their three required conditions hold. First, the value function is optimistic, as guaranteed by Lemma 3.2. Second, the accuracy condition is satisfied since the `1-error in the transition probabilities, (1 − γ)2,\ntranslates into an error bound in the value function (Kearns & Singh, 2002). Lastly, the agent visits an unknown state–action at most SAB times, satisfying the learning complexity condition. The probability that any of the three conditions fails is, due to a union bound, at most 3δ: the first δ comes from Lemma 3.2, and the other two from Lemma 3.3."
    }, {
      "heading" : "3.2 THE BAYESIAN CONCENTRATION SAMPLE COMPLEXITY",
      "text" : "Theorem 3.1 depends on the Bayesian concentration sample complexity f . A full analysis of f is beyond the scope of this paper. In general, f depends on certain properties of the model space as well as the prior distribution. While it is likely that a more accurate estimate of f can be obtained in special cases, we make use of a fairly general result by Zhang (2006) to relate our sample complexity of exploration in Theorem 3.1 to certain characteristics of the Bayesian prior. Future work can instantiate this general result to special MDP classes and prior distributions.\nWe will need two key quantities introduced by Zhang (2006; Section 5.2). The first is the critical prior-mass radius, εp,n, which characterizes how dense the prior distribution p is around the true model (smaller values imply denser priors). The second is the critical upper-bracketing radius with coefficient 2/3, denoted εupper,n, whose decay rate (as n becomes large) controls the consistency of the Bayesian posterior distribution. When εupper,n = o(1), the posterior is consistent. Now, define εn = 4εp,n + 32εupper,n. The next lemma states that as long as εn decreases sufficiently fast as n → ∞, we may upper bound the Bayesian concentration sample complexity.\nLemma 3.4 If there exists a constant c > 0 such that εn = O(n−c), then f(s, a, , δ, ρ) = max{O( − 2c δ− 1c ), O( −2δ−1 ln 1ρ )}.\nProof (sketch). We set ρ = 1/2 and γ = 2 as used in Corollary 5.2 of Zhang (2006) to solve for n. Zhang’s corollary is stated using Rényi-entropy (DRE1\n2 ) as the\ndistance metric between distributions. But, the same bound applies straightforwardly to `1-distance because DRE1\n2 (q||p) ≥ ‖p− q‖21/2.\nWe may further simplify the result in Lemma 3.4 by assuming without loss of generality that c ≤ 1, resulting in a potentially looser bound of f(s, a, , δ, ρ) = O( − 2 c δ−\n1 c ln 1ρ ). A direct consequence of this simpli-\nfied result, when combined with Theorem 3.1, is that BOSS behaves -optimally with probability at least\n1− δ in all but at most\nÕ\n( S1+ 1 cA1+ 1 c\n1+ 2 c δ 1 c (1− γ)2+ 4c\n)\nsteps, where Õ(·) suppresses logarithmic dependence. This result formalizes the intuition that, if the problem-specific quantity εn decreases sufficiently fast, BOSS enjoys polynomial sample complexity of exploration with high probability.\nWhen an uninformative Dirichlet prior is used, it can be shown that f is polynomial in all relevant quantities, and thus Theorem 3.1 provides a performance guarantee similar to the PAC-MDP result for RMAX (Kakade, 2003)."
    }, {
      "heading" : "4 EXPERIMENTS",
      "text" : "This section presents computational experiments with BOSS, evaluating its performance on a simple domain from the literature to allow for a comparison with other published approaches.\nConsider the well-studied 5-state chain problem (Chain) (Strens, 2000; Poupart et al., 2006). The agent has two actions: Action 1 advances the agent along the chain, and Action 2 resets the agent to the first node. Action 1, when taken from the last node, leaves the agent where it is and gives a reward of 10— all other rewards are 0. Action 2 always has a reward of 2. With probability 0.2 the outcomes are switched, however. Optimal behavior is to always choose Action 1 to reach the high reward at the end of the chain.\nThe slip probability 0.2 is the same for all state–action pairs. Poupart et al. (2006) consider the impact of encoding this constraint as a strong prior on the transition dynamics. That is, whereas in the Full prior, the agent assumes each state–action pair corresponds to independent multinomial distributions over next states, under the Tied prior, the agent knows the underlying transition dynamics except for the value of a single slip probability that is shared between all state– action pairs. They also introduce a Semi prior in which the two actions have independent slip probabilities. Posteriors for Full can be maintained using a Dirichlet (the conjugate for the multinomial) and Tied/Semi can be represented with a simple Beta distribution.\nIn keeping with published results on this problem, Table 1 reports cumulative rewards in the first 1000 steps, averaged over 500 runs. Standard error is on the order of 20 to 50. The optimal policy for this problem scores 3677. The exploit algorithm is one that always acts optimally with respect to the average model weighted by the posterior. RAM-RMAX (Leffler et al., 2007)\nis a version of RMAX that can exploit the tied parameters of tasks like this one. Results for BEETLE and exploit are due to Poupart et al. (2006). All runs used a discount factor of γ = 0.95 and BOSS used B = 10 and K = 5.\nAll algorithms perform very well in the Tied scenario (although RAM-RMAX is a bit slower as it needs to estimate the slip probability very accurately to avoid finding a suboptimal policy). Poupart et al. (2006) point out that BEETLE (a belief-lookahead approach) is more effective than exploit (an undirected approach) in the Semi scenario, which requires more careful exploration to perform well. In Full, however, BEETLE falls behind because the larger parameter space makes it difficult for it to complete its belief-lookahead analysis.\nBOSS, on the other hand, explores as effectively as BEETLE in Semi, but is also effective in Full. A similarly positive result (3158) in Full is obtained by Bayesian DP (Strens, 2000)."
    }, {
      "heading" : "5 BAYESIAN MODELING OF STATE CLUSTERS",
      "text" : "The idea of state clusters is implicit in the Tied prior. We say that two states are in the same cluster if their probability distributions over relative outcomes are the same given any action. In Chain, for example, the outcomes are advancing along the chain or resetting to the beginning. Both actions produce the same distribution on these two outcomes independent of state, Action 1 is 0.8/0.2 and Action 2 is 0.2/0.8, so Chain can be viewed as a one-cluster environment.\nWe introduce a variant of the chain example, the twocluster Chain2, which includes an additional state cluster. Cluster 1—states 1, 3, and 5—behaves identically to the cluster in Chain. Cluster 2—states 2 and 4— has roughly the reverse distributions (Action 1 0.3/0.7, Action 2 0.7/0.3).\nRAM-RMAX can take advantage of cluster structure, but only if it is known in advance. In this section, we show how BOSS with an appropriate prior can learn an unknown cluster structure and exploit it to speed up learning."
    }, {
      "heading" : "5.1 A NON-PARAMETRIC MODEL OF STATE CLUSTERING",
      "text" : "We derive a non-parametric cluster model that can simultaneously use observed transition outcomes to discover which parameters to tie and estimate their values. We first assume that the observed outcomes for each state in a cluster c are generated independently, but from a shared multinomial parameter vector θc. We then place a Dirichlet prior over each θc and integrate them out. This process has the effect of coupling all of the states in a particular cluster together, implying that we can use all observed outcomes of states in a cluster to improve our estimates of the associated transition probabilities.\nThe generative model is\nκ ∼ CRP(α) θκ(s) ∼ Dirichlet(η) os,a ∼ Multinomial(θκ(s))\nwhere κ is a clustering of states (κ(s) is the id of s’s cluster), θκ(s) is a multinomial over outcomes associated with each cluster, and os,a is the observed outcome counts for state s and action a. Here, CRP is a Chinese Restaurant Process (Aldous, 1985), a flexible distribution that allows us to infer both the number of clusters and the assignment of states to clusters. The parameters of the model are α ∈ R, the concentration parameter of the CRP, and η ∈ NN , a vector of N pseudo-counts parameterizing the Dirichlet.\nThe posterior distribution over clusters κ and multinomial vectors θ given our observations os,a (represented as “data” below) is\np(κ, θ|data) ∝ p(data|θ)p(θ|η)p(κ|α) = ∏ s,a p(os,a|θκ(s))p(θκ(s)|η)p(κ|α)\n= ∏ c∈κ ∏ s∈c ∏ a∈A p(os,a|θc)p(θc|η)p(κ|α)\nwhere c is the set of all states in a particular cluster. We now integrate out the multinomial parameter vector θc in closed form, resulting in a standard Dirichlet compound multinomial distribution (or multivariate Polya distribution):\np(data|κ) = ∏ c∈κ ∫ θc ∏ s∈c ∏ a∈A p(os,a|θc)p(θc|η) = (1)\n∏ c∈κ,a∈A Γ( ∑ i ηi)∏ i Γ(ηi) ∏ s Γ( ∑ i o s,a i + 1)∏ i,s Γ( o s,a i + 1) ∏ i Γ( ∑ s o s,a i + ηi) Γ( ∑ i,s o s,a i + ηi) .\nBecause the θc parameters have been integrated out of the model, the posterior distribution over models\nis simply a distribution over κ. We can also sample transition probabilities for each state by examining the posterior predictive distribution of θc.\nTo sample models from the posterior, we sample cluster assignments and transition probabilities in two stages, using repeated sweeps of Gibbs sampling. For each state s, we fix the cluster assignments of all other states and sample over the possible assignments of s (including a new cluster):\np(κ(s)|κ−s,data) ∝ p(data|κ)p(κ)\nwhere κ(s) is the cluster assignment of state s and κ−s is the cluster assignments of all other states. Here, p(data|κ) is given by Eq. 1 and p(κ) is the CRP prior\np(κ|α) = αr Γ(α) Γ(α+ ∑ i κi) r∏ i=1 Γ(κi)\nwith r the total number of clusters and κi the number of states in each cluster.\nGiven κ, we sample transition probabilities for each action from the posterior predictive distribution over θc, which, due to conjugacy, is a Dirichlet distribution:\nθc|κ, η, α, a ∼ Dirichlet(η + ∑ s∈c os,a)."
    }, {
      "heading" : "5.2 BOSS WITH CLUSTERING PRIOR",
      "text" : "We ranBOSS in a factorial design where we varied the environment (Chain vs. Chain2) and the prior (Tied, Full, vs. Cluster, where Cluster is the model described in the previous subsection). For our experiments, BOSS used a discount factor of γ = 0.95, knownness parameter B = 10, and a sample size of K = 5. The Cluster CRP used α = 0.5 and whenever a sample was required, the Gibbs sampler ran for a burn period of 500 sweeps with 50 sweeps between each sample.\nFigure 1 displays the results of running BOSS with different priors in Chain and Chain2. The top line on the graph corresponds to the results for Chain. Moving from left to right, BOSS is run with weaker priors— Tied, Cluster, and Full. Not surprisingly, performance decreases with weaker priors. Interestingly, however, Cluster is not significantly worse than Tied—it is able to identify the single cluster and learn it quickly.\nThe second line on the plot is the results for Chain2, which has two clusters. Here, Tied’s assumption of the existence of a single cluster is violated and performance suffers as a result. Cluster outperforms Full by a smaller margin, here. Learning two independent clusters is still better than learning all states separately, but the gap is narrowing. On a larger example with more sharing, we’d expect the difference to be more\ndramatic. Nonetheless, the differences here are statistically significant (2× 3 ANOVA p < 0.001)."
    }, {
      "heading" : "5.3 VARYING K",
      "text" : "The experiments reported in the previous section used model samples of size K = 5. Our next experiment was intended to show the effect of varying the sample size. Note that Bayesian DP is very similar to BOSS with K = 1, so it is important to quantify the impact of this parameter to understand the relationship between these algorithms.\nFigure 2 shows the result of running BOSS on Chain2 using the same parameters as in the previous section. Note that performance generally improves with K. The difference between K = 1 and K = 10 is statistically significant (t-test p < 0.001)."
    }, {
      "heading" : "5.4 6x6 MARBLE MAZE",
      "text" : "To demonstrate the exploration behavior of our algorithm, we developed a 6x6 grid-world domain with standard dynamics (Russell & Norvig, 1994). In this environment, the four actions, N, S, E and W, carry the agent through the maze on its way to the goal. Each action has its intended effect with probability .8, and the rest of the time the agent travels in one of the two perpendicular directions with equal likelihood. If there is a wall in the direction the agent tried to go, it will remain where it is. Each step has a cost of 0.001, and terminal rewards of −1 and +1 are received for falling into a pit or reaching the goal, respectively. The map of the domain, along with its optimal policy, is illustrated in Figure 3.\nThe dynamics of this environment are such that each local pattern of walls (at most 16) can be modeled as a separate cluster. In fact, fewer than 16 clusters appear in the grid and fewer still are likely to be encountered along an optimal trajectory. Nonetheless, we expected BOSS to find and use a larger set of clusters than in the previous experiments.\nFor this domain, BOSS used a discount factor of γ = 0.95 and a CRP hyperparameter of α = 10. Whenever an MDP set was needed, the Gibbs sampler ran for a burn period of 100 sweeps with 50 sweeps between each sample. We also ran RMAX in this domain.\nThe cumulative reward achieved by the BOSS variants that learned the cluster structure, in Figure 4, dominated those of RMAX, which did not know the cluster structure. The primary difference visible in the graph is the time needed to obtain the optimal policy. Remarkably, BOSS B = 10 K = 10 latches onto near optimal behavior nearly instantaneously whereas the RMAX variants required 50 to 250 trials before behaving as well. This finding can be partially explained by the choice of the clustering prior and the\noutcomes it drew from, which effectively put a lower bound on the number of steps to the goal from any state. This information made it easy for the agent to ignore longer paths when it had already found something that worked.\nLooking at the clustering performed by the algorithm, a number of interesting features emerge. Although it does not find a one-to-one mapping from states to patterns of walls, it gets very close. In particular, among the states that are visited often in the optimal policy and for the actions chosen in these states, the algorithm groups them perfectly. The first, third, fourth, and fifth states in the top row of the grid are all assigned to the same cluster. These are the states in which there is a wall above and none below or right, impacting the success probability of N and E, the two actions chosen in these states. The first, second, third, and fifth states in the rightmost column are similarly grouped together. These are the states with a wall to the right, but none below or left, impacting the success probability of S and E, the two actions chosen in these states. Other, less commonly visited states, are clustered somewhat more haphazardly, as it was not necessary to visit them often to obtain high reward in this grid. The sampled models used around 10 clusters to capture the dynamics."
    }, {
      "heading" : "5.5 COMPUTATIONAL COMPLEXITY",
      "text" : "The computation time required by BOSS depends on two distinct factors. First, the time required for perstep planning using value iteration scales with the number of sampled MDPs, K. Second, the time required for sampling new MDPs depends linearly on K and on the type of prior used. For a simple prior, such\nas Full, samples can be drawn extremely quickly. For a more complex prior, such as Cluster, samples can take longer. In the 6x6 Marble Maze, samples were drawn at a rate of roughly one every ten seconds. It is worth noting that sampling can be carried out in parallel."
    }, {
      "heading" : "6 CONCLUSIONS",
      "text" : "We presented a modular approach to exploration called BOSS that interfaces a Bayesian model learner to an algorithm that samples models and constructs exploring behavior that converges quickly to near optimality. We compared the algorithm to several stateof-the-art exploration approaches and showed it was as good as the best known algorithm in each scenario tested. We also derived a non-parametric Bayesian clustering model and showed how BOSS could use it to learn more quickly than could non-generalizing comparison algorithms.\nIn future work, we plan to analyze the more general setting in which priors are assumed to be only approximate indicators of the real distribution over environments. We are also interested in hierarchical approaches that can learn, in a transfer-like setting, more accurate priors. Highly related work in this direction was presented by Wilson et al. (2007).\nAn interesting direction for future research is to consider extensions of our clustered state model where the clustering is done in feature space, possibly using non-parametric models such as the Indian Buffet Process (Griffiths & Ghahramani, 2006). Such a model could simultaneously learn how to decompose states into features and also discover which observable features of a state (color, texture, position) are reliable indicators of the dynamics.\nWe feel that decomposing the details of the Bayesian model from the exploration and decision-making components allow for a very general RL approach. Newly developed languages for specifying Bayesian models (Goodman et al., 2008) could be integrated directly with BOSS to produce a flexible learning toolkit."
    }, {
      "heading" : "Acknowledgements",
      "text" : "We thank Josh Tenenbaum, Tong Zhang, and the reviewers. This work was supported by DARPA IPTO FA8750-05-2-0249 and NSF IIS-0713435."
    } ],
    "references" : [ {
      "title" : "Exchangeability and related topics",
      "author" : [ "D. Aldous" ],
      "venue" : null,
      "citeRegEx" : "Aldous,? \\Q1985\\E",
      "shortCiteRegEx" : "Aldous",
      "year" : 1985
    }, {
      "title" : "Infinite latent feature models and the Indian buffet process",
      "author" : [ "T.L. Griffiths", "Z. Ghahramani" ],
      "venue" : "Neural Information Processing Systems (NIPS)",
      "citeRegEx" : "Griffiths and Ghahramani,? \\Q2006\\E",
      "shortCiteRegEx" : "Griffiths and Ghahramani",
      "year" : 2006
    }, {
      "title" : "On the sample complexity of reinforcement learning",
      "author" : [ "S.M. Kakade" ],
      "venue" : "Doctoral dissertation,",
      "citeRegEx" : "Kakade,? \\Q2003\\E",
      "shortCiteRegEx" : "Kakade",
      "year" : 2003
    }, {
      "title" : "Near-optimal reinforcement learning in polynomial time",
      "author" : [ "M.J. Kearns", "S.P. Singh" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Kearns and Singh,? \\Q2002\\E",
      "shortCiteRegEx" : "Kearns and Singh",
      "year" : 2002
    }, {
      "title" : "Efficient reinforcement learning with relocatable action models",
      "author" : [ "B.R. Leffler", "M.L. Littman", "T. Edmunds" ],
      "venue" : "Proceedings of the Twenty-Second Conference on Artificial Intelligence (AAAI-07)",
      "citeRegEx" : "Leffler et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Leffler et al\\.",
      "year" : 2007
    }, {
      "title" : "An analytic solution to discrete Bayesian reinforcement learning",
      "author" : [ "P. Poupart", "N. Vlassis", "J. Hoey", "K. Regan" ],
      "venue" : "Proceedings of the 23rd International Conference on Machine Learning (pp. 697–704)",
      "citeRegEx" : "Poupart et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Poupart et al\\.",
      "year" : 2006
    }, {
      "title" : "Artificial intelligence: A modern approach",
      "author" : [ "S.J. Russell", "P. Norvig" ],
      "venue" : null,
      "citeRegEx" : "Russell and Norvig,? \\Q1994\\E",
      "shortCiteRegEx" : "Russell and Norvig",
      "year" : 1994
    }, {
      "title" : "Incremental model-based learners with formal learning-time guarantees",
      "author" : [ "A.L. Strehl", "L. Li", "M.L. Littman" ],
      "venue" : "Proceedings of the 22nd Conference on Uncertainty in Artificial Intelligence (UAI",
      "citeRegEx" : "Strehl et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Strehl et al\\.",
      "year" : 2006
    }, {
      "title" : "A Bayesian framework for reinforcement learning",
      "author" : [ "M.J.A. Strens" ],
      "venue" : "Proceedings of the Seventeenth International Conference on Machine Learning (ICML",
      "citeRegEx" : "Strens,? \\Q2000\\E",
      "shortCiteRegEx" : "Strens",
      "year" : 2000
    }, {
      "title" : "On the likelihood that one unknown probability exceeds another in view of the evidence of two samples",
      "author" : [ "W.R. Thompson" ],
      "venue" : null,
      "citeRegEx" : "Thompson,? \\Q1933\\E",
      "shortCiteRegEx" : "Thompson",
      "year" : 1933
    }, {
      "title" : "The role of exploration in learning control",
      "author" : [ "S.B. Thrun" ],
      "venue" : null,
      "citeRegEx" : "Thrun,? \\Q1992\\E",
      "shortCiteRegEx" : "Thrun",
      "year" : 1992
    }, {
      "title" : "Bayesian sparse sampling for on-line reward optimization",
      "author" : [ "T. Wang", "D. Lizotte", "M. Bowling", "D. Schuurmans" ],
      "venue" : "ICML ’05: Proceedings of the 22nd international conference on Machine Learning (pp. 956–963)",
      "citeRegEx" : "Wang et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2005
    }, {
      "title" : "Multitask reinforcement learning: A hierarchical Bayesian approach",
      "author" : [ "A. Wilson", "A. Fern", "S. Ray", "P. Tadepalli" ],
      "venue" : "Machine Learning, Proceedings of the TwentyFourth International Conference (ICML",
      "citeRegEx" : "Wilson et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Wilson et al\\.",
      "year" : 2007
    }, {
      "title" : "From ε-entropy to KL-entropy: Analysis of minimum information complexity density estimation",
      "author" : [ "T. Zhang" ],
      "venue" : "The Annals of Statistics,",
      "citeRegEx" : "Zhang,? \\Q2006\\E",
      "shortCiteRegEx" : "Zhang",
      "year" : 2006
    } ],
    "referenceMentions" : [ {
      "referenceID" : 5,
      "context" : "A state-of-the-art belief-lookahead approach is BEETLE (Poupart et al., 2006), which plans in the continuous belief space defined by the agent’s uncertainty.",
      "startOffset" : 55,
      "endOffset" : 77
    }, {
      "referenceID" : 11,
      "context" : "Myopic (Wang et al., 2005) approaches make decisions to reduce uncertainty, but they do not explicitly consider how this reduced uncertainty will impact future reward.",
      "startOffset" : 7,
      "endOffset" : 26
    }, {
      "referenceID" : 10,
      "context" : "Undirected (Thrun, 1992) approaches take exploratory actions, but without regard to what parts of their environment models remain uncertain.",
      "startOffset" : 11,
      "endOffset" : 24
    }, {
      "referenceID" : 8,
      "context" : "A sophisticated approach that falls into this category is Bayesian DP (Strens, 2000).",
      "startOffset" : 70,
      "endOffset" : 84
    }, {
      "referenceID" : 9,
      "context" : "The idea of sampling from the posterior for decision making has been around for decades (Thompson, 1933).",
      "startOffset" : 88,
      "endOffset" : 104
    }, {
      "referenceID" : 8,
      "context" : "Several recent algorithms have used this technique for Bayesian RL (Strens, 2000; Wilson et al., 2007).",
      "startOffset" : 67,
      "endOffset" : 102
    }, {
      "referenceID" : 12,
      "context" : "Several recent algorithms have used this technique for Bayesian RL (Strens, 2000; Wilson et al., 2007).",
      "startOffset" : 67,
      "endOffset" : 102
    }, {
      "referenceID" : 8,
      "context" : "Strens (2000) advocates a T approximating the depth of exploratory planning required.",
      "startOffset" : 0,
      "endOffset" : 14
    }, {
      "referenceID" : 2,
      "context" : "Our objective, and some of our techniques, closely follow work in the PAC-MDP framework (Kakade, 2003; Strehl et al., 2006).",
      "startOffset" : 88,
      "endOffset" : 123
    }, {
      "referenceID" : 7,
      "context" : "Our objective, and some of our techniques, closely follow work in the PAC-MDP framework (Kakade, 2003; Strehl et al., 2006).",
      "startOffset" : 88,
      "endOffset" : 123
    }, {
      "referenceID" : 7,
      "context" : "The proof relies on a general PACMDP theorem by Strehl et al. (2006) by verifying their three required conditions hold.",
      "startOffset" : 48,
      "endOffset" : 69
    }, {
      "referenceID" : 13,
      "context" : "While it is likely that a more accurate estimate of f can be obtained in special cases, we make use of a fairly general result by Zhang (2006) to relate our sample complexity of exploration in Theorem 3.",
      "startOffset" : 130,
      "endOffset" : 143
    }, {
      "referenceID" : 13,
      "context" : "2 of Zhang (2006) to solve for n.",
      "startOffset" : 5,
      "endOffset" : 18
    }, {
      "referenceID" : 2,
      "context" : "1 provides a performance guarantee similar to the PAC-MDP result for RMAX (Kakade, 2003).",
      "startOffset" : 74,
      "endOffset" : 88
    }, {
      "referenceID" : 8,
      "context" : "Consider the well-studied 5-state chain problem (Chain) (Strens, 2000; Poupart et al., 2006).",
      "startOffset" : 56,
      "endOffset" : 92
    }, {
      "referenceID" : 5,
      "context" : "Consider the well-studied 5-state chain problem (Chain) (Strens, 2000; Poupart et al., 2006).",
      "startOffset" : 56,
      "endOffset" : 92
    }, {
      "referenceID" : 5,
      "context" : "Poupart et al. (2006) consider the impact of encoding this constraint as a strong prior on the transition dynamics.",
      "startOffset" : 0,
      "endOffset" : 22
    }, {
      "referenceID" : 4,
      "context" : "RAM-RMAX (Leffler et al., 2007) ASMUTH ET AL.",
      "startOffset" : 9,
      "endOffset" : 31
    }, {
      "referenceID" : 5,
      "context" : "Results for BEETLE and exploit are due to Poupart et al. (2006). All runs used a discount factor of γ = 0.",
      "startOffset" : 42,
      "endOffset" : 64
    }, {
      "referenceID" : 5,
      "context" : "Poupart et al. (2006) point out that BEETLE (a belief-lookahead approach) is more effective than exploit (an undirected approach) in the Semi scenario, which requires more careful exploration to perform well.",
      "startOffset" : 0,
      "endOffset" : 22
    }, {
      "referenceID" : 8,
      "context" : "A similarly positive result (3158) in Full is obtained by Bayesian DP (Strens, 2000).",
      "startOffset" : 70,
      "endOffset" : 84
    }, {
      "referenceID" : 0,
      "context" : "Here, CRP is a Chinese Restaurant Process (Aldous, 1985), a flexible distribution that allows us to infer both the number of clusters and the assignment of states to clusters.",
      "startOffset" : 42,
      "endOffset" : 56
    }, {
      "referenceID" : 12,
      "context" : "Highly related work in this direction was presented by Wilson et al. (2007).",
      "startOffset" : 55,
      "endOffset" : 76
    } ],
    "year" : 2009,
    "abstractText" : "We present a modular approach to reinforcement learning that uses a Bayesian representation of the uncertainty over models. The approach, BOSS (Best of Sampled Set), drives exploration by sampling multiple models from the posterior and selecting actions optimistically. It extends previous work by providing a rule for deciding when to resample and how to combine the models. We show that our algorithm achieves nearoptimal reward with high probability with a sample complexity that is low relative to the speed at which the posterior distribution converges during learning. We demonstrate that BOSS performs quite favorably compared to state-of-the-art reinforcement-learning approaches and illustrate its flexibility by pairing it with a non-parametric model that generalizes across states.",
    "creator" : "TeX"
  }
}