{
  "name" : "1701.06123.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Optimization on Product Submanifolds of Convolution Kernels",
    "authors" : [ "Mete Ozay", "Takayuki Okatani" ],
    "emails" : [ "mozay@vision.is.tohoku.ac.jp", "okatani@vision.is.tohoku.ac.jp" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n70 1.\n06 12\n3v 1\n[ cs\n.C V\n] 2\n2 Ja\nn 20"
    }, {
      "heading" : "1 INTRODUCTION",
      "text" : "Product manifolds have been used to solve various optimization problems in machine learning, pattern recognition and computer vision. In the recent works, stochastic gradient descent (SGD) methods have been employed on embedded kernel submanifolds [5] and Grassmann manifolds [4] of convolution kernels (weights) of convolutional neural networks (CNNs) [1], [2]. Ozay and Okatani [5] proposed a SGD method to train CNNs using embedded kernel submanifolds of convolution kernels with convergence properties. Products of the Stiefel manifolds are used for action recognition by employing CNNs in [3].\nIn this paper, we address a problem of optimization on products of embedded kernel submanifolds (PEMs) using a SGD method to train CNNs considering its convergence properties. Our contribution can be summarized as follows:\n1) We first explore the effect of geometric properties of product manifolds, such as sectional curvature and geodesic distance, on convergence properties of PEMs compared to that of the SGD employed on component manifolds. 2) Next, we generalize the SGD methods employed on kernel submanifolds [2], [3], [5] for optimization on product of different collections of kernel submanifolds using a SGD algorithm, called C-SGD. 3) In the theoretical results, we first explore the effect of geometric properties of component manifolds on the convergence of a SGD employed on PEMs. Then, we employ the results for adaptive step size estimation of the SGD. Moreover, we provide an example for employment of the theoretical results for optimization on PEMs of the sphere."
    }, {
      "heading" : "2 OPTIMIZATION ON PRODUCT MANIFOLDS FOR TRAINING OF CNNS",
      "text" : "We address the problem of optimization on products of kernel submanifolds to train CNNs using an SGD method. Suppose that we are given a set of training samples S = {si = (Ii, yi)}Ni=1 of a random variable s drawn from a distribution P on a measurable space S, where yi is a class label of the ith image Ii. An L-layer CNN consists of a set of tensors W = {Wl}Ll=1, where Wl = {Wd,l ∈ RAl×Bl×Cl}Dld=1, and Wd,l = [Wc,d,l ∈ RAl×Bl]Clc=1 is a tensor1 composed of kernels (weight matrices) Wc,d,l constructed at each layer l = 1,2, . . . , L, for each cth channel c = 1,2, . . . ,Cl and each dth kernel d = 1,2, . . . ,Dl.\nAt each lth convolution layer, we compute a feature representation fl(Xl;Wl) by compositionally employing non-linear functions, and convolving an image I with kernels by fl(Xl;Wl) = fl(⋅;Wl) ○ ⋯ ○ f1(X1;W1), (1) where X1 ∶= I is an image for l = 1, and Xl = [Xc,l]Clc=1. The cth channel of the data matrix Xc,l is convolved with the kernel Wc,d,l to obtain the d\nth feature map Xc,l+1 ∶= X̂d,l by X̂d,l =Wc,d,l ∗Xc,l,∀c, d, l 2. Given a batch of samples s ⊆ S, we denote a value of a classification loss function for a kernel ω ≜Wc,d,l by L(ω, s), and the loss function of kernels W utilized in the CNN by L(W , s). If we assume that s contains a single sample, then, an expected loss or cost function of the CNN is computed by\nL(W) ≜ EP{L(W , s)} = ∫ L(W , s)dP . (2) The expected loss L(ω) for ω is computed by\nL(ω) ≜ EP{L(ω, s)} = ∫ L(ω, s)dP . (3) For a finite set of samples S, L(W) is approximated by an empirical loss 1 ∣S∣ ∑∣S∣i=1 L(W , si), where ∣S∣ is the size of S (similarly, L(ω) is approximated by the empirical loss for ω). Then, feature representations are learned using SGD by solving\nmin W L(W). (4) In the SGD algorithms [5], [2], [3], each kernel is assumed to reside on an embedded kernel submanifold Mc,d,l at the lth layer 1. We use shorthand notation for matrix concatenation such that [Wc,d,l] Cl c=1 ≜ [W1,d,l,⋯,WCl,d,l].\n2. We ignore the bias terms in the notation for the sake of simplicity.\n2 of a CNN, such that ω ∈Mc,d,l,∀c, d. In this work, we propose a SGD algorithm called C-SGD by generalizing the SGD algorithms employed on kernel submanifolds [5], [2], [3] for optimization on product of different collections of the submanifolds, which are defined next.\nDefinition 2.1 (Product of embedded kernel submanifolds of convolution kernels). Suppose that Gl = {Mι ∶ ι ∈ IGl} is a collection of kernel submanifolds Mι of dimension dι, which is identified by a set of indices IGl ,∀l = 1,2, . . . , L. Moreover, suppose that Gl ⊆ Gl is a subset of manifolds identified by IGl ⊆ IGl .\nA Gl product manifold of convolution kernels (Gl-PEM) constructed at the lth layer of an L layer CNN is a product of embedded kernel manifolds belonging to Gl which is computed by\nMGl = ⨉ ι∈IGl Mι, (5)\nwhere ⨉ is the topological Cartesian product. Given any kernel ωι ∈ Mι, the product map\nφGl ∶ UGl → R dGl (6)\nwhere dGl = ∑ ι∈IGl dι, (Uι, φι) is a coordinate chart for Mι with ωι ∈ Uι, φGl = ⨉\nι∈IGl φι, and UGl = ⨉ ι∈IGl Uι. A kernel\nωGl ∈MGl is then obtained by concatenating kernels belonging toMι ∈ Gl, ∀ι ∈ IGl , using ωGl = (ω1, ω2,⋯, ω∣IGl ∣), where ∣IGl ∣ is the cardinality of IGl . The product smooth manifold structure is defined by the smooth map\nφGl ○ψ −1 Gl = ⨉\nι∈IGl\nφι ×ψ −1 ι , (7)\nwhere ψGl = ⨉ ι∈IGl ψι. A Gl-PEM is simply called a product of embedded kernel submanifolds of convolution kernels (PEM). ∎\nWhen we employ a C-SGD on a Gl-PEM MGl , each kernel ω is moved on the Gl-PEM in the descent direction of gradient of loss at each tth step of the C-SGD. More precisely, direction and amount of movement of a kernel ωtl are determined at the t\nth step and the lth layer by\n1) projection of the gradient gradE L(ωtl ), which is obtained using back-propagation from the upper layer, onto the tangent space Tωt l MGl at gradL(ωtl ) (line 6 of Algorithm 1), 2) movement of ωtl on Tωt l MGl according to a function of the\nlearning rate to vt (line 7 of Algorithm 1), and 3) projection of the moved kernel at vt onto the manifold MGl\nto compute ωt+1l (line 8 of Algorithm 1).\nConvergence properties of the C-SGD are affected first by the smooth manifold structure of MGl as defined in Definition 2.1. In addition, convergence rate is affected by metric and curvature properties of MGl which are analyzed in the next theorem.\nLemma 2.2 (Metric and curvature properties of PEMs). If each kernel submanifold Mι is a Riemannian manifold endowed with a metric gι, then a Gl-PEM is endowed with the metric gGl = ⊕\nι∈IGl\ngι defined by\ngGl( ∑ ι∈IGl uι, ∑ ι∈IGl vι) = ∑ ι∈IGl gι(uι, vι), (8)\nwhere uι ∈ TωιMι and vι ∈ TωιMι are tangent vectors belonging to the tangent space TωιMι computed at ωι ∈ Mι. In addition, if\nCι is the Riemannian curvature tensor of Mι and xι, yι ∈ TωιMι, then the Riemannian curvature tensor CGl of MGl is computed by\nCGl( ∑ ι∈IGl uι, ∑ ι∈IGl vι, ∑ ι∈IGl xι, ∑ ι∈IGl yι) = Ĉ, (9)\nwhere Ĉ ≜ ∑ ι∈IGl Cι(uι, vι, xι, yι). It follows that the MGl has never strictly positive sectional curvature cGl in the metric (8). In addition, no compact MGl admits a metric with negative sectional curvature cGl .\nWe compute the metric of a Gl-PEM MGl using metrics identified on the component manifolds Mι employing (8) given in Lemma 2.2. In addition, we use the sectional curvature of the MGl given in Lemma 2.2 to analyze convergence of the C-SGD, and to compute adaptive step size. Note that some sectional curvatures vanish on the MGl by the lemma. For instance, suppose that eachMι is a unit two-sphere S2, ∀ι ∈ IGl . Then, MGl computed by (5) has unit curvature along two-dimensional subspaces of its tangent spaces, called two-planes. On the other hand, MGl has zero curvature along all two-planes spanning exactly two distinct spheres. Therefore, learning rates need to be computed adaptively according to sectional curvatures at each layer of the CNN and at each epoch of the C-SGD for each kernel ω on each manifold MGl . In order to explode this property more precisely, we introduce the following theorem.\nTheorem 2.3. Suppose that there exists a local minimum ω̂Gl ∈ MGl ,∀Gl ⊆ Gl,∀l, and ∃ǫ > 0 such that inf ρt Gl >ǫ 1 2 ⟨φωt Gl\n(ω̂Gl)−1,∇L(ωtGl)⟩ < 0, where φ is an exponential map or a twice continuously differentiable retraction, ⟨⋅, ⋅⟩ is the inner product and ρtGl ≜ ρ(ωtGl , ω̂Gl) is the geodesic distance between ωtGl and ω̂Gl on MGl . In addition, suppose that the following conditions are satisfied for each MGl ;\n(i) Condition on learning rate g(t,Θ): ∞\n∑ t=0 g(t,Θ) = +∞ and ∞∑ t=0 g(t,Θ)2 <∞. (10) (ii) Condition on step size and direction\nh(gradL(ωtGl), g(t,Θ)): h(gradL(ωtGl), g(t,Θ)) = −g(t,Θ)g(ωt\nGl )gradL(ω t Gl), (11)\nwhere g(ωtGl) = max{1,Γt1}, Γt1 = (RtGl)2Γt2, Γt2 = max{(2ρtGl +RtGl)2, (1 + cGl(ρtGl +RtGl))}, cGl is the sectional curvature of MGl , R t Gl\n≜ ∥gradL(ωtGl)∥2, and ∥gradL(ωtGl)∥2 = ( ∑\nωt l,ι ∈Mι,ι∈IGl\ngradL(ωtl,ι)2) 1 2 .\nThen, the loss function and the gradient converges almost surely (a.s.) by L(ωtGl) a.s.ÐÐÐ→t→∞ L(ω̂Gl), and ∇L(ω t Gl ) a.s.ÐÐÐ→ t→∞ 0, for each MGl ,∀l.\nConditions (i) and (ii) given in Theorem 2.3 can be used to compute adaptive learning rates and step sizes to train CNNs by optimization using the C-SGD algorithm (Algorithm 1) assuring convergence to minima. As an example, we use the results obtained from Lemma 2.2 in Theorem 2.3 to compute adaptive step sizes for the sphere in the following theorem.\n3 Algorithm 1 C-SGD on products of kernel submanifolds. 1: Input: T (number of iterations), S (training set),\nΘ (set of hyperparameters), L (a loss function), IGl ,∀l (sets of indices) . 2: Initialization: Construct a collection of products of kernel submanifolds Gl,∀l = 1,2, . . . , L, and initialize kernels ωtGl ∈MGl , ∀Gl ⊆ Gl,∀l. 3: for each iteration t = 1,2, . . . , T do 4: for each layer l = 1,2, . . . , L do 5: Compute the Euclidean gradient gradEL(ωtGl). 6: gradL(ωtGl) ∶= Πωtl (gradE L(ωtGl),Θ). 7: vt ∶= h(gradL(ωtGl), g(t,Θ)). 8: ωt+1Gl ∶= φωtGl (vt),∀ω t Gl\n. 9: t ∶= t + 1.\n10: end for 11: end for 12: Output: A set of estimated kernels {ωTGl}Ll=1.\nCorollary 2.4. Suppose that Mι are identified by dι ≥ 2 dimensional unit sphere Sdι , and ρtGl ≤ ĉ−1, where ĉ is an upper bound on the sectional curvatures of MGl ,∀l at ω t Gl\n∈ MGl ,∀t. If condition (i) given in Theorem 2.3 is satisfied, and step size is computed using (11) with g(ωtGl) =max{1, (RtGl)2(2 +RtGl)2}, then L(ωtGl) a.s.ÐÐÐ→t→∞ L(ω̂Gl), and ∇L(ω t Gl ) a.s.ÐÐÐ→ t→∞ 0, for each MGl ,∀l."
    }, {
      "heading" : "3 CONCLUSION",
      "text" : "We address a problem of optimization on product of embedded submanifolds of convolution kernels (PEMs) in convolutional neural networks (CNNs). In our theoretical results, we first explore metric and curvature properties of PEMs and show that PEMs have strictly positive sectional curvature. Next, we propose a SGD method, called C-SGD, by generalizing the SGD methods employed on kernel submanifolds for optimization on product of different collections of kernel submanifolds. Then, we analyze convergence properties of C-SGD employed on PEMs considering their sectional curvature properties. In the theoretical results, we expound the constraints that can be used to compute adaptive step size of C-SGD in order to assure its asymptotic convergence. Moreover, we employ the proposed methods for computation of adaptive step size of C-SGD for optimization on PEMs of the sphere. Our proposed C-SGD and the theoretical results can be used to train CNNs using products of different kernels at different layers by estimating adaptive step size of C-SGD with assurance of convergence."
    } ],
    "references" : [ {
      "title" : "Generalized backpropagation, étude de cas: Orthogonality",
      "author" : [ "M. Harandi", "B. Fernando" ],
      "venue" : null,
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2016
    }, {
      "title" : "A riemannian network for spd matrix learning",
      "author" : [ "Z. Huang", "L.V. Gool" ],
      "venue" : "In Assoc. for the Adv. of Artificial Intelligence (AAAI),",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2017
    }, {
      "title" : "Deep learning on lie groups for skeleton-based action recognition",
      "author" : [ "Z. Huang", "C. Wan", "T. Probst", "L.V. Gool" ],
      "venue" : null,
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2016
    }, {
      "title" : "Building deep networks on grassmann manifolds",
      "author" : [ "Z. Huang", "J. Wu", "L.V. Gool" ],
      "venue" : null,
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2016
    }, {
      "title" : "Optimization on submanifolds of convolution kernels in cnns",
      "author" : [ "M. Ozay", "T. Okatani" ],
      "venue" : null,
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 4,
      "context" : "In the recent works, stochastic gradient descent (SGD) methods have been employed on embedded kernel submanifolds [5] and Grassmann manifolds [4] of convolution kernels (weights) of convolutional neural networks (CNNs) [1], [2].",
      "startOffset" : 114,
      "endOffset" : 117
    }, {
      "referenceID" : 3,
      "context" : "In the recent works, stochastic gradient descent (SGD) methods have been employed on embedded kernel submanifolds [5] and Grassmann manifolds [4] of convolution kernels (weights) of convolutional neural networks (CNNs) [1], [2].",
      "startOffset" : 142,
      "endOffset" : 145
    }, {
      "referenceID" : 0,
      "context" : "In the recent works, stochastic gradient descent (SGD) methods have been employed on embedded kernel submanifolds [5] and Grassmann manifolds [4] of convolution kernels (weights) of convolutional neural networks (CNNs) [1], [2].",
      "startOffset" : 219,
      "endOffset" : 222
    }, {
      "referenceID" : 1,
      "context" : "In the recent works, stochastic gradient descent (SGD) methods have been employed on embedded kernel submanifolds [5] and Grassmann manifolds [4] of convolution kernels (weights) of convolutional neural networks (CNNs) [1], [2].",
      "startOffset" : 224,
      "endOffset" : 227
    }, {
      "referenceID" : 4,
      "context" : "Ozay and Okatani [5] proposed a SGD method to train CNNs using embedded kernel submanifolds of convolution kernels with convergence properties.",
      "startOffset" : 17,
      "endOffset" : 20
    }, {
      "referenceID" : 2,
      "context" : "Products of the Stiefel manifolds are used for action recognition by employing CNNs in [3].",
      "startOffset" : 87,
      "endOffset" : 90
    }, {
      "referenceID" : 1,
      "context" : "2) Next, we generalize the SGD methods employed on kernel submanifolds [2], [3], [5] for optimization on product of different collections of kernel submanifolds using a SGD algorithm, called C-SGD.",
      "startOffset" : 71,
      "endOffset" : 74
    }, {
      "referenceID" : 2,
      "context" : "2) Next, we generalize the SGD methods employed on kernel submanifolds [2], [3], [5] for optimization on product of different collections of kernel submanifolds using a SGD algorithm, called C-SGD.",
      "startOffset" : 76,
      "endOffset" : 79
    }, {
      "referenceID" : 4,
      "context" : "2) Next, we generalize the SGD methods employed on kernel submanifolds [2], [3], [5] for optimization on product of different collections of kernel submanifolds using a SGD algorithm, called C-SGD.",
      "startOffset" : 81,
      "endOffset" : 84
    }, {
      "referenceID" : 4,
      "context" : "In the SGD algorithms [5], [2], [3], each kernel is assumed to reside on an embedded kernel submanifold Mc,d,l at the l layer",
      "startOffset" : 22,
      "endOffset" : 25
    }, {
      "referenceID" : 1,
      "context" : "In the SGD algorithms [5], [2], [3], each kernel is assumed to reside on an embedded kernel submanifold Mc,d,l at the l layer",
      "startOffset" : 27,
      "endOffset" : 30
    }, {
      "referenceID" : 2,
      "context" : "In the SGD algorithms [5], [2], [3], each kernel is assumed to reside on an embedded kernel submanifold Mc,d,l at the l layer",
      "startOffset" : 32,
      "endOffset" : 35
    }, {
      "referenceID" : 4,
      "context" : "In this work, we propose a SGD algorithm called C-SGD by generalizing the SGD algorithms employed on kernel submanifolds [5], [2], [3] for optimization on product of different collections of the submanifolds, which are defined next.",
      "startOffset" : 121,
      "endOffset" : 124
    }, {
      "referenceID" : 1,
      "context" : "In this work, we propose a SGD algorithm called C-SGD by generalizing the SGD algorithms employed on kernel submanifolds [5], [2], [3] for optimization on product of different collections of the submanifolds, which are defined next.",
      "startOffset" : 126,
      "endOffset" : 129
    }, {
      "referenceID" : 2,
      "context" : "In this work, we propose a SGD algorithm called C-SGD by generalizing the SGD algorithms employed on kernel submanifolds [5], [2], [3] for optimization on product of different collections of the submanifolds, which are defined next.",
      "startOffset" : 131,
      "endOffset" : 134
    } ],
    "year" : 2017,
    "abstractText" : "We address a problem of optimization on product of embedded submanifolds of convolution kernels (PEMs) in convolutional neural networks (CNNs). First, we explore metric and curvature properties of PEMs in terms of component manifolds. Next, we propose a SGD method, called C-SGD, by generalizing the SGD methods employed on kernel submanifolds for optimization on product of different collections of kernel submanifolds. Then, we analyze convergence properties of the C-SGD considering sectional curvature properties of PEMs. In the theoretical results, we expound the constraints that can be used to compute adaptive step sizes of the C-SGD in order to assure the asymptotic convergence.",
    "creator" : "LaTeX with hyperref package"
  }
}