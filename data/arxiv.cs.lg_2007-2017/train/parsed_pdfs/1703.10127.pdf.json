{
  "name" : "1703.10127.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Priv’IT: Private and Sample Efficient Identity Testing",
    "authors" : [ "Bryan Cai", "Constantinos Daskalakis", "Gautam Kamath" ],
    "emails" : [ "bcai@mit.edu", "costis@csail.mit.edu", "g@csail.mit.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "We provide theoretical bounds for the sample size |D| so that our method both satisfies (ε, 0)- differential privacy, and guarantees βI and βII type I and type II errors. We show that differential privacy may come for free in some regimes of parameters, and we always beat the sample complexity resulting from running the χ2-test with noisy counts, or standard approaches such as repetition for endowing non-private χ2-style statistics with differential privacy guarantees. We experimentally compare the sample complexity of our method to that of recently proposed methods for private hypothesis testing [GLRV16, KR17]."
    }, {
      "heading" : "1 Introduction",
      "text" : "Hypothesis testing is the age-old problem of deciding whether observations from an unknown phenomenon p conform to a model q. Often p can be viewed as a distribution over some alphabet Σ, and the goal is to determine, using samples from p, whether it is equal to some model distribution q or not. This type of test is the lifeblood of the scientific method and has received tremendous study in statistics since its very beginnings. Naturally, the focus has been on minimizing the number of observations from the unknown distribution p that are needed to determine, with confidence, whether p = q or p 6= q.\nIn several fields of research and application, however, samples may contain sensitive information about individuals; consider for example, individuals participating in some clinical study of a disease that carries social stigma. It may thus be crucial to guarantee that operating on the samples needed to test a statistical hypothesis protects sensitive information about the samples. This is not at odds with the goal of hypothesis testing itself, since the latter is about verifying a property of the population p from which the samples are drawn, and not of the samples themselves.\nWithout care, however, sensitive information about the sample might actually be divulged by statistical processing that is improperly designed. As recently exhibited, for example, it may be possible to determine whether individuals participated in a study from data that would typically be published in genome-wide association studies [HSR+08]. Motivated in part by this realization, there has been increased recent interest in developing data sharing techniques which are private [JS13, USF13, YFSU14, SSB16].\nProtecting privacy when computing on data has been extensively studied in several fields ranging from statistics to diverse branches of computer science including algorithms, cryptography, database theory, and machine learning; see, e.g., [Dal77, AW89, AA01, DN03, Dwo08, DR14] and their references. A notion of privacy proposed by theoretical computer scientists which has found a lot of traction is that of differential privacy [DMNS06]. Roughly speaking, it requires that the output of an algorithm on two neighboring datasets D and D′ that differ in the value of one element be statistically close. For a formal definition see Section 2.\nOur goal in this paper is to develop tools for privately performing statistical hypothesis testing. In particular, we are interested in studying the tradeoffs between statistical accuracy, power, significance, and\nar X\niv :1\n70 3.\n10 12\n7v 3\n[ cs\n.D S]\n7 J\nun 2\n01 7\nprivacy in the sample size. To be precise, given samples from a categorical distribution p over some domain Σ, an explicitly described distribution q over Σ, some privacy parameter ε, accuracy parameter α, and requirements βI and βII for the type I and type II errors of our test, the goal is to distinguish between p = q and dTV(p, q) ≥ α. We want that the output of our test be (ε, 0)-differentially private, and that the probability we make a type I or type II error be βI and βII respectively. Treating these as hard constraints, we want to minimize the number of samples that we draw from p.\nNotice that the correctness constraint on our test pertains to whether we draw the right conclusion about how p compares to q, while the privacy constraint pertains to whether we respect the privacy of the samples that we draw from p. The pertinent question is how much the privacy constraint increases the number of samples that are needed to guarantee correctness. Our main result is that privacy may come for free in certain regimes of parameters, and has a mild cost for all regimes of parameters.\nTo be precise, without privacy constraints, it is well known that identity testing can be performed\nfrom O( √ n\nα2 · log 1 β ) samples, where n is the size of Σ and β = min{βI, βII}, and that this is tight [BFF +01,\nPan08, VV14, ADK15]. Our main theoretical result is that, with privacy constraints, the number of samples that are needed is\nÕ ( max {√ n\nα2 ,\n√ n\nα3/2ε ,\nn1/3\nα5/3ε2/3\n} · log(1/β) ) . (1)\nOur statistical test is provided in Section 5 where the above upper bound on the number of samples that it requires is proven as Theorem 3. Notice that privacy comes for free when the privacy requirement ε is Ω( √ α) – for example when ε = 10% and the required statistical accuracy is 3%.\nThe precise constants sitting in the O(·) notation of Eq. (1) are given in the proof of Theorem 3. We experimentally verify the sample efficiency of our tests by comparing them to recently proposed private statistical tests [GLRV16, KR17], discussed in more detail shortly. Fixing a differential privacy and type I, type II error constraints, we compare how many samples are required by our and their methods to distinguish between hypotheses that are α = 0.1 apart in total variation distance. We find that different algorithms are more efficient depending on the regime and properties desired by the analyst. Our experiments and further discussion of the tradeoffs are presented in Section 6.\nApproach. A standard approach to turn an algorithm differentially private is to use repetition. As already mentioned above, absent differential privacy constraints, statistical tests have been provided that use an optimal m = O( √ n\nα2 · log 1 β ) number of samples. A trivial way to get (ε, 0)-differential privacy using such a\nnon-private test is to create O(1/ε) datasets, each comprising m samples from p, and run the non-private test on one of these datasets, chosen randomly. It is clear that changing the value of a single element in the combined dataset may only affect the output of the test with probability at most ε. Thus the output is (ε, 0)-differentially private; see Section 3 for a proof. The issue with this approach is that the total number of samples that it draws is m/ε = O( √ n\nεα2 · log 1 β ), which is higher than our target. See Corollary 1.\nA different approach towards private hypothesis testing is to look deeper into the non-private tests and try to “privatize” them. The most sample-efficient tests are variations of the classical χ2-test. They compute the number of times, Ni, that element i ∈ Σ appears in the sample and aggregate those counts using a statistic that equals, or is close to, the χ2-divergence between the empirical distribution defined by these counts and the hypothesis distribution q. They accept q if the statistic is low and reject q if it is high, using some threshold.\nA reasonable approach to privatize such a test is to add noise, e.g. Laplace(1/ε) noise, to each count Ni, before running the test. It is well known that adding Laplace(1/ε) noise to a set of counts makes them differentially private, see Theorem 1. However, it also increases the variance of the statistic. This has been noticed empirically in recent work of [GLRV16] for the χ2-test. We show that the variance of the optimal χ2style test statistic significantly increases if we add Laplace noise to the counts, in Section 4.1, thus increasing the sample complexity from O( √ n) to Ω(n3/4). So this route, too, seems problematic.\nA last approach towards designing differentially private tests is to exploit the distance beween the null and the alternative hypotheses. A correct test should accept the null with probability close to 1, and reject an alternative that is α-far from the null with probability close to 1, but there are no requirements for correctness when the alternative is very close to the null. We could thus try to interpolate smoothly between\ndatasets that we expect to see when sampling the null and datasets that we expect to see when sampling an alternative that is far from the null. Rather than outputting “accept” or “reject” by merely thresholding our statistic, we would like to tune the probability that we output “reject” based on the value of our statistic, and make it so that the “reject” probability is ε-Lipschitz as a function of the dataset. Moreover, the probability should be close to 0 on datasets that we expect to see under the null and close to 1 on datasets that we expect to see under an alternative that is α-far. As we show in Section 4.2, χ2-style statistics have high sensitivity, requiring ω( √ n) samples to be made appropriately Lipschitz.\nWhile both the approach of adding noise to the counts, and that of turning the output of the test Lipschitz fail in isolation, our test actually goes through by intricately combining these two approaches. It has two steps:\n1. A filtering step, whose goal is to “reject” when p is blatantly far from q. This step is performed by comparing the counts Ni with their expectations under q, after having added Laplace(1/ε) noise to these counts. If the noisy counts deviate from their expectation, taking into account the extra variance introduced by the noise, then we can safely “reject.” Moreover, because noise was added, this step is differentially private.\n2. If the filtering step fails to reject, we perform a statistical step. This step just computes the χ2-style statistic from [ADK15], without adding noise to the counts. The crucial observation is that if the filtering step does not reject, then the statistic is actually ε-Lipschitz with respect to the counts, and thus the value of the statistic is still differentially private. We use the value of the statistic to determine the bias of a coin that outputs “reject.”\nDetails of our test are given in Section 5.\nRelated Work. Identity testing is one of the most classical problems in statistics, where it is traditionally called hypothesis or goodness-of-fit testing, see [Pea00, Fis35, RS81, Agr12] for some classical and contemporary references. In this field, the focus is often on asymptotic analysis, where the number of samples goes to infinity, and we wish to get a grasp on their asymptotic distributions and error exponents [Agr12, TAW10]. In the past twenty years, this problem has enjoyed significant interest in the theoretical computer science community (see, i.e., [BFF+01, Pan08, LRR13, VV14, ADK15, CDGR16, DK16, DDK16], and [Can15] for a survey), where the focus has instead been on the finite sample regime, rather than asymptotics. Specifically, the goal is to minimize the number of samples required, while still remaining computationally tractable.\nA number of recent works [WLK15, GLRV16, KR17] (and a simultaneous work, focused on independence testing [KSF17]) investigate differential privacy with the former set of goals. In particular, their algorithms focus on fixing a desired significance (type I error) and privacy requirement, and study the asymptotic distribution of the test statistics. On the other hand, we are the first work to apply differential privacy to the latter line of inquiry, where our goal is to minimize the number of samples required to ensure the desired significance, power and privacy. As a point of comparison between these two worlds, we provide an empirical evaluation of our method versus their methods.\nThe problem of distribution estimation (rather than testing) has also recently been studied under the lens of differential privacy [DHS15]. This is another classical statistics problem which has recently piqued the interest of the theoretical computer science community. We note that the techniques required for this setting are quite different from ours, as we must deal with issues that arise from very sparsely sampled data."
    }, {
      "heading" : "2 Preliminaries",
      "text" : "In this paper, we will focus on discrete probability distributions over [n]. For a distribution p, we will use the notation pi to denote the mass p places on symbol i.\nDefinition 1. The total variation distance between p and q is defined as\ndTV(p, q) = 1\n2 ∑ i∈[n] |pi − qi| .\nDefinition 2. A randomized algorithm M with domain Nn is (ε, δ)-differentially private if for all S ⊆ Range(M) and for all pairs of inputs D,D′ such that ‖D −D′‖1 ≤ 1:\nPr [M(D) ∈ S] ≤ eε Pr [M(D′) ∈ S] + δ.\nIf δ = 0, the guarantee is called pure differential privacy.\nIn the context of distribution testing, the neighboring dataset definition corresponds to two datasets where one dataset is generated from the other by removing one sample. Up to a factor of 2, this is equivalent to the alternative definition where one dataset is generated from the other by arbitrarily changing one sample.\nDefinition 3. An algorithm for the (α, βI, βII)-identity testing problem with respect to a (known) distribution q takes m samples from an (unknown) distribution p and has the following guarantees:\n• If p = q, then with probability at least 1− βI it outputs “p = q;”\n• If dTV(p, q) ≥ α, then with probability at least 1− βII it outputs “p 6= q.”\nIn particular, βI and βII are the type I and type II errors of the test. Parameter α is the radius of distinguishing accuracy. Notice that, when p satisfies neither of cases above, the algorithm’s output may be arbitrary.\nWe note that if an algorithm is to satisfy both these definitions, the latter condition (the correctness property) need only be satisfied when p falls into one of the two cases, while the former condition (the privacy property) must be satisfied for all realizations of the samples from p (and in particular, for p which do not fall into the two cases above).\nWe recall the classical Laplace mechanism, which states that applying independent Laplace noise to a set of counts is differentially private.\nTheorem 1 (Theorem 3.6 of [DR14]). Given a set of counts N1, . . . , Nn, the noised counts (N1+Y1, . . . , Nn+ Yn) are (ε, 0)-differentially private when the Yi’s are i.i.d. random variables drawn from Laplace(1/ε).\nFinally, we recall the definition of zero-concentrated differential privacy from [BS16] and its relationship to differential privacy.\nDefinition 4. A randomized algorithm M with domain Nn is ρ-zero-concentrated differentially private (ρ-zCDP) if for all pairs of inputs D,D′ such that ‖D −D′‖1 ≤ 1 and all α ∈ (1,∞):\nDα(M(D)||M(D′)) ≤ ρα,\nwhere Dα is the α-Rényi divergence between the distribution of M(D) and M(D ′).\nProposition 1 (Propositions 1.3 and 1.4 of [BS16]). If a mechanism M1 satisfies (ε, 0)-differential privacy, then M1 satisfies ε2 2 -zCDP. If a mechanism M2 satisfies ρ-zCDP, then M2 satisfies (ρ + 2 √ ρ log(1/δ), δ)differential privacy for any δ > 0."
    }, {
      "heading" : "3 A Simple Upper Bound",
      "text" : "In this section, we provide an O (√\nn α2ε\n) upper bound for the differentially private identity testing problem.\nMore generally, we show that if an algorithm requires a dataset of size m for a decision problem, then it can be made (ε, 0)-differentially private at a multiplicative cost of 1/ε in the sample size. This is a folklore result, but we include and prove it here for completeness.\nTheorem 2. Suppose there exists an algorithm for a decision problem P which succeeds with probability at least 1 − β and requires a dataset of size m. Then there exists an (ε, 0)-differentially private algorithm for P which succeeds with probability at least 45 (1− β) + 1/10 and requires a dataset of size O(m/ε).\nProof. First, with probability 1/5, we flip a coin and output yes or no with equal probability. This guarantees that we have probability at least 1/10 of either outcome, which will allow us to satisfy the multiplicative guarantee of differential privacy.\nWe then draw 10/ε datasets of size m, and solve the decision problem (non-privately) for each of them. Finally, we select a random one of these computations and output its outcome.\nThe correctness follows, since we randomly choose the right answer with probability 1/10, or with probability 4/5, we solve the problem correctly with probability 1−β. As for privacy, we note that, if we remove a single element of the dataset, we may only change the outcome of one of these computations. Since we pick a random computation, this is selected with probability ε/10, and thus the probability of any outcome is additively shifted by at most ε/10. Since we know the minimum probability of any output is 1/10, this gives the desired multiplicative guarantee required for (ε, 0)-differential privacy.\nWe obtain the following corollary by noting that the tester of [ADK15] (among others) requires O( √ n/α2)\nsamples for identity testing.\nCorollary 1. There exists an (ε, 0)-differentially private testing algorithm for the (α, βI, βII)-identity testing problem for any distribution q which requires\nm = O\n(√ n\nεα2 · log(1/β) ) samples, where β = min (βI, βII)."
    }, {
      "heading" : "4 Roadblocks to Differentially Private Testing",
      "text" : "In this section, we describe roadblocks which prevent two natural approaches to differentially private testing from working.\nIn Section 4.1, we show that if one simply adds Laplace noise to the empirical counts of a dataset (i.e., runs the Laplace mechanism of Theorem 1) and then attempts to run an optimal identity tester, the variance of the statistic increases dramatically, and thus results in a much larger sample complexity, even for the case of uniformity testing. The intuition behind this phenomenon is as follows. When performing uniformity testing in the small sample regime (when the number of samples m is the square root of the domain size n), we will see a (1 − o(1))n elements 0 times, O( √ n) elements 1 time, and O(1) elements 2 times. If we add Laplace(10) noise to guarantee (0.1, 0)-differential privacy, this obliterates the signal provided by these collision statistics, and thus many more samples are required before the signal prevails.\nIn Section 4.2, we demonstrate that χ2 statistics have high sensitivity, and thus are not naturally differentially private. In other words, if we consider a χ2 statistic Z on two datasets D and D′ which differ in one record, |Z(D) − Z(D′)| may be quite large. This implies that methods such as rescaling this statistic and interpreting it as a probability, or applying noise to the statistic, will not be differentially private until we have taken a large number of samples."
    }, {
      "heading" : "4.1 A Laplaced χ2-statistic has large variance",
      "text" : "Proposition 2. Applying the Laplace mechanism to a dataset before applying the identity tester of [ADK15] results in a significant increase in the variance, even when considering the case of uniformity. More precisely, if we consider the statistic\nZ ′(D) = ∑ i∈[n] (Ni + Yi −m/n)2 − (Ni + Yi) m/n\nwhere Ni is the number of occurrences of symbol i in the dataset D (which is of size Poisson(m)) and Yi ∼ Laplace(1/ε), then\n• If p is uniform, then E[Z ′] = 2n 2 ε2m and Var[Z ′] ≥ 20n 3 ε4m2 .\n• If p is a particular distribution which is α-far in total variation distance from uniform, then E[Z ′] = 4mα2 + 2n 2\nε2m .\nThe variance of the statistic can be compared to that of the unnoised statistic, which is upper bounded by m2α4. We can see that the noised statistic has larger variance until m = Ω(n3/4).\nProof. First, we compute the mean of Z ′. Note that since |D| ∼ Poisson(m), the Ni’s will be independently distributed as Poisson(mpi) (see, i.e., [ADK15] for additional discussion).\nE[Z ′] = E [ ∑ i∈[n] (Ni + Yi −m/n)2 − (Ni + Yi) m/n ]\n= E [ ∑ i∈[n] (Ni −m/n)2 −Ni m/n\n+ ∑ i∈[n] Y 2i + 2Yi(Ni −m/n)− Yi m/n\n]\n= m · χ2(p, q) + ∑ i∈[n] 2 ε2 m/n\n= m · χ2(p, q) + 2n 2\nε2m\nIn other words, the mean is a rescaling of the χ2 distance between p and q, shifted by some constant amount. When p = q, the χ2-distance between p and q is 0, and the expectation is just the second term. Focus on the case where n is even, and consider p such that pi = (1 + 2α)/n if i is even, and (1 − 2α)/n otherwise. This is α-far from uniform in total variation distance. Furthermore, by direct calculation, χ2(p, q) = 4α2, and thus the expectation of Z ′ in this case is 4mα2 + 2n 2\nε2m . Next, we examine the variance of Z ′. Let λi = mpi and λ ′ i = mqi = m/n. By a similar computation as\nbefore, we have that\nVar[Z ′] = ∑ i∈[n] 1 λ′2i [ 2λ2i + 4λi(λi − λ′i)2\n+ 1\nε2 (8λi + 2(2λi − 2λ′i − 1)2) +\n20 ε4\n] .\nSince all four summands of this expression are non-negative, we have that\nVar[Z ′] ≥ 20 ε4 ∑ i∈[n] 1 λ′2i = 20n3 ε4m2 .\nIf we wish to use Chebyshev’s inequality to separate these two cases, we require that Var[Z ′] is at most the square of the mean separation. In other words, we require that\n20n3 ε4m2 ≤ m2α4,\nor that\nm = Ω\n( n3/4\nεα\n) ."
    }, {
      "heading" : "4.2 A χ2-statistic has high sensitivity",
      "text" : "Consider the primary statistic which we use in Algorithm 1:\nZ(D) = 1\nmα2 ∑ i∈[n] (Ni −mqi)2 −Ni mqi .\nAs shown in Section 5, E[Z] = 0 if p = q and E[Z] ≥ 1 if dTV(p, q) ≥ α, and the variance of Z is such that these two cases can be separated with constant probability. A natural approach is to truncate this statistic to the range [0, 1], interpret it as a probability and output the result of Bernoulli(Z) – if p = q, the result is likely to be 0, and if dTV(p, q) ≥ α, the result is likely to be 1. One might hope that this statistic is naturally private. More specifically, we would like that the statistic Z has low sensitivity, and does not change much if we remove a single individual. Unfortunately, this is not the case. We consider datasets D,D′, where D′ is identical to D, but with one fewer occurrence of symbol i. It can be shown that the difference in Z is\n|Z(D)− Z(D′)| = 2|Ni −mqi − 1| m2α2qi\nLetting q be the uniform distribution and requiring that this is at most ε (for the sake of privacy), we have a constraint which is roughly of the form\n2Nin m2α2 ≤ ε,\nor that\nm = Ω\n(√ Ni √ n\nε0.5α\n) .\nIn particular, if Ni = n c for any c > 0, this does not achieve the desired O( √ n) sample complexity. One may observe that, if Ni is this large, looking at symbol i alone is sufficient to conclude p is not uniform, even if the count Ni had Laplace noise added. Indeed, our main algorithm of Section 5 works in part due to our formalization and quantification of this intuition."
    }, {
      "heading" : "5 Priv’IT: A Differentially Private Identity Tester",
      "text" : "In this section, we prove our main testing upper bound:\nTheorem 3. There exists an (ε, 0)-differentially private testing algorithm for the (α, βI, βII)-identity testing problem for any distribution q which requires\nm = Õ ( max {√ n\nα2 ,\n√ n\nα3/2ε ,\nn1/3\nα5/3ε2/3\n} · log(1/β) ) samples, where β = min (βI, βII).\nThe pseudocode for this algorithm is provided in Algorithm 1. We fix the constants c1 = 1/4 and c2 = 3/40. For a high-level overview of our algorithm’s approach, we refer the reader to the Approach paragraph in Section 1. Proof of Theorem 3: We will prove the theorem for the case where β = 1/3, the general case follows at the cost of a multiplicative log(1/β) in the sample complexity from a standard amplification argument. To be more precise, we can consider splitting our dataset into O(log(1/β)) sub-datasets and run the β = 1/3 test on each one independently. We return the majority result – since each test is correct with probability ≥ 2/3, correctness of the overall test follows by Chernoff bound. It remains to argue privacy – note that a neighboring dataset will only result in a single sub-dataset being changed. Since we take the majority result, conditioning on the result of the other sub-tests, the result on this sub-dataset will either be irrelvant to or equal to the overall output. In the former case, any test is private, and in the latter case, we know that the individual test is ε-differentially private. Overall privacy follows by applying the law of total probability.\nWe require the following two claims, which give bounds on the random variables Ni and Yi. Note that, due to the fact that we draw Poisson(m) samples, each Ni ∼ Poisson(mpi) independently.\nClaim 1. |Yi| ≤ 2c2ε log ( 1 1−(1−c2)1/|A| ) simultaneously for all i ∈ A with probability exactly 1− c2.\nProof. The survival function of the folded Laplace distribution with parameter 2/c2ε is exp (−c2εx/2), and the probability that a sample from it exceeding the value 2c2ε log\n( 1\n1−(1−c2)1/|A|\n) is equal to 1− (1− c2)1/|A|.\nThe probability that probability that it does not exceed this value is (1 − c2)1/|A|, and since the Yi’s are independent, the probability that none exceeds this value is 1− c2, as desired.\nAlgorithm 1 Priv’IT: A differentially private identity tester\n1: Input: ε; an explicit distribution q; sample access to a distribution p 2: Define A ← {i : qi ≥ c1α/n}, Ā ← [n] \\ A 3: Sample Yi ∼ Laplace(2/c2ε) for all i ∈ A 4: if there exists i ∈ A such that |Yi| ≥ 2c2ε log ( 1 1−(1−c2)1/|A| ) then 5: return either “p 6= q” or “p = q” with equal probability 6: end if 7: Draw a multiset S of Poisson(m) samples from p 8: Let Ni be the number of occurrences of the ith domain element in S 9: for i ∈ A do\n10: if |Ni + Yi −mqi| ≥ 2c2ε log ( 1 1−(1−c2)1/|A| ) + max { 4 √ mqi log n, log n } then 11: return “p 6= q” 12: end if 13: end for 14: Z ← 2mα2 ∑ i∈A (Ni−mqi)2−Ni mqi 15: Let T be the closest value to Z which is contained in the interval [0, 1] 16: Sample b ∼ Bernoulli(T ) 17: if b = 1 then 18: return “p 6= q” 19: else 20: return “p = q” 21: end if Claim 2. |Ni − mpi| ≤ max { 4 √ mpi log n, log n } simultaneously for all i ∈ A with probability at least 1− 2n0.84 − 1.1 n . Proof. We consider this in two cases. Let X be a Poisson(λ) random variable. First, assume that λ ≥ e−3 log n. By Bennett’s inequality, we have the following tail bound [Pol15, Can17]:\nPr [|X − λ| ≥ x] ≤ 2 exp ( −x 2 2λ ψ (x λ )) ,\nwhere\nψ(t) = (1 + t) log(1 + t)− t\nt2/2 .\nConsider x = 4 √ λ log n. At this point, we have\nψ(x/λ) = ψ(4 √ log n/λ) ≥ ψ(4e3/2) ≥ 0.23.\nThus,\nPr [ |X − λ| ≥ 4 √ λ log n ] ≤ 2 exp (−0.23 · 8 log n)\n≤ 2n−1.84.\nNow, we focus on the other case, where λ ≤ e−3 log n. Here, we appeal to Proposition 1 of [Kla00], which implies the following via Stirling’s approximation:\nPr [|X − λ| ≥ kλ] ≤ k k − 1 exp(−λ+ kλ− kλ log k).\nWe set kλ = log n, giving the upper bound\nk\nk − 1 n1−log k ≤ 1.1 · n−2.\nWe conclude by taking a union bound over [n], with the argument for each i ∈ [n] depending on whether λ = mpi is large or small.\nWe proceed with proving the two desiderata of this algorithm, correctness and privacy.\nCorrectness. We use the following two properties of the statistic Z(D), which rely on the condition that m = Ω( √ n/α2). The proofs of these properties are identical to the proofs of Lemma 2 and 3 in [ADK15], and are omitted.\nClaim 3. If p = q, then E[Z] = 0. If dTV(p, q) ≥ α, then E[Z] ≥ 1. Claim 4. If p = q, then Var[Z] ≤ 1/1000. If dTV(p, q) ≥ α, then Var[Z] ≤ 1/1000 ·E[Z]2.\nFirst, we note that, by Claim 1, the probability that we return in line 5 is exactly c2. We now consider the case where p = q. We note that by Claim 2, the probability that we output “p 6= q” in line 10 is o(1), and thus negligible. By Chebyshev’s inequality, we get that Z ≤ 1/10 with probability at least 9/10, and we output “p = q” with probability at least c2/2 + (1− c2) · (9/10− c2)2 ≥ 2/3 (note that we subtract c2 from 9/10 since we are conditioning on an event with probability 1 − c2, and by union bound). Similarly, when dTV(p, q) ≥ α, Chebyshev’s inequality gives that Z ≥ 9/10 with probability at least 9/10, and therefore we output “p 6= q” with probability at least 2/3.\nPrivacy. We will prove (0, c2ε/2)-differential privacy. By Claim 1, the probability that we return in line 5 is exactly c2. Thus the minimum probability of any output of the algorithm is at least c2/2, and therefore (0, c2ε/2)-differential privacy implies (ε, 0)-differential privacy.\nWe first consider the possibility of rejecting in line 11. Consider two neighboring datasets D and D′, which differ by 1 in the frequency of symbol i. Coupling the randomness of the Yj ’s on these two datasets, the only case in which the output differs is when Yi is such that the value of |Ni + Yi−mqi| lies on opposite sides of the threshold for the two datasets. Since Ni differs by 1 in the two datasets, and the probability mass assigned by the PDF of Yi to any interval of length 1 is at most c2ε/4, the probability that the outputs differ is at most c2ε/4. Therefore, this step is (0, c2ε/4)-differentially private.\nWe next consider the value of Z for two neighboring datasets D and D′, where D′ has one fewer occurrence of symbol i. We only consider the case where we have not already returned in line 11, as otherwise the value of Z is irrelevant for determining the output of the algorithm.\nZ(D)− Z(D′)\n= 1\nmα2\n[ (Ni −mqi)2 −Ni\nmqi − (Ni − 1−mqi) 2 − (Ni − 1) mqi ] = 1\nmα2\n[ (Ni −mqi)2 −Ni\nmqi − (Ni −mqi) 2 − 2(Ni −mqi) + 1−Ni + 1 mqi ] =\n2(Ni −mqi − 1) m2α2qi .\nSince we did not return in line 11,\n|Ni −mqi| ≤ 4\nc2ε log\n( 1\n1− (1− c2)1/n\n) + max { 4 √ mqi log n, log n } ≤ 4 log(n/c2)\nc2ε + max\n{ 4 √ mqi log n, log n } This implies that\n|Z(D)− Z(D′)| = 2|Ni −mqi − 1| m2α2qi\n≤ 2 m2α2qi\n( 6 log(n/c2) c2ε + 4 √ mqi log n ) .\nWe will enforce that each of these terms are at most c2ε/8.\n12 log(n/c2) m2α2qic2ε ≤ c2ε 8 ⇒ m ≥\n√ 96\nc22c1\n√ n log(n/c2)\nα1.5ε\n8 √ log n\nm1.5α2 √ qi ≤ c2ε 8 ⇒ m ≥\n( 64\nc2 √ c1\n)2/3 (n log n)1/3\nα5/3ε2/3\nSince both terms are at most c2ε/8, this step is (0, c2ε/4)-differentially private. Combining with the previous step gives the desired (0, c2ε/2)-differential privacy, and thus (as argued at the beginning of the privacy section of this proof) ε-pure differential privacy."
    }, {
      "heading" : "6 Experiments",
      "text" : "We performed an empirical evaluation of our algorithm, Priv’IT, on synthetic datasets. All experiments were performed on a laptop computer with a 2.6 GHz Intel Core i7-6700HQ CPU and 8 GB of RAM. Significant discussion is required to provide a full comparison with prior work in this area, since performance of the algorithms varies depending on the regime.\nWe compared our algorithm with two recent algorithms for differentially private hypothesis testing:\n1. The Monte Carlo Goodness of fit test with Laplace noise from [GLRV16], MCGOF;\n2. The projected Goodness of Fit test from [KR17], zCDP-GOF.\nWe note that we implemented a modified version of Priv’IT, which differs from Algorithm 1 in lines 14 to 21. In particular, we instead consider a statistic\nZ = ∑ i∈A (Ni −mqi)2 −Ni mqi .\nWe add Laplace noise to Z, with scale parameter Θ(∆/ε), where ∆ is the sensitivity of Z, which guarantees (ε/2, 0)-differential privacy. Then, similar to the other algorithms, we choose a threshold for this noised statistic such that we have the desired type I error. This algorithm can be analyzed to provide identical theoretical guarantees as Algorithm 1, but with the practical advantage that there are fewer parameters to tune.\nTo begin our experimental evaluation, we started with uniformity testing. Our experimental setup was as follows. The algorithms were provided q as the uniform distribution over [n]. The algorithms were also provided with samples from some distribution p. This (unknown) p was q for the case p = q, or a distribution which we call the “Paninski construction” for the case dTV(p, q) ≥ α. The Paninski construction is a distribution where half the elements of the support have mass (1 + α)/n and half have mass (1− α)/n. We use this name for the construction as [Pan08] showed that this example is one of the hardest to distinguish from uniform: one requires Ω( √ n/α2) samples to (non-privately) distinguish a random permutation of this construction from the uniform distribution. We fixed parameters ε = 0.1 and α = 0.1. In addition, recall that Proposition 1 implies that pure differential privacy (the privacy guaranteed by Priv’IT) is stronger than zCDP (the privacy guaranteed by zCDP-GOF). In particular, our guarantee of ε-pure differential privacy implies ε2/2-zCDP. As a result, we ran zCDP-GOF with a privacy parameter of 0.005-zCDP, which is equivalent to the amount of zCDP our algorithm provides. Our experiments were conducted on a number of different support sizes n, ranging from 10 to 10600. For each n, we ran the testing algorithms with increasing sample sizes m in order to discover the minimum sample size when the type I and type II errors were both empirically below 1/3. To determine these empirical error rates, we ran all algorithms 1000 times for each n and m, and recorded the fraction of the time each algorithm was correct. As the other algorithms take a parameter βI as a target type I error, we input 1/3 as this parameter.\nThe results of our first test are provided in Figure 1. The x-axis indicates the support size, and the y-axis indicates the minimum number of samples required. We plot three lines, which demonstrate the empirical number of samples required to obtain 1/3 type I and type II error for the different algorithms. We can see that in this case, zCDP-GOF is the most statistically efficient, followed by MCGOF and Priv’IT.\nTo explain this difference in statistical efficiency, we note that the theoretical guarantees of Priv’IT imply that it performs well even when data is sparsely sampled. More precisely, one of the benefits of our tester is that it can reduce the variance induced by elements whose expected number of occurrences is less than 1. Since none of these testers reach this regime (i.e., even zCDP-GOF at n = 10000 expects to see each element 10 times), we do not reap the benefits of Priv’IT. Ideally, we would run these algorithms on the uniform distribution at sufficiently large support sizes. However, since this is prohibitively expensive to do\nUniformity Testing\nwith thousands of repetitions (for any of these methods), we instead demonstrate the advantages of our tester on a different distribution.\nOur second test is conducted with q being a 2-histogram1, where all but a vanishing fraction of the probability mass is concentrated on a small, constant fraction of the support2. This serves as our proxy for a very large support, since now we will have elements which have a sub-constant expected number of occurrences. The algorithms are provided with samples from a distribution p, which is either q or a similar Paninski construction as before, where the total variation distance from q is placed on the support elements containing non-negligible mass. We ran the test on support sizes n ranging from 10 to 6800. All other parameters are the same as in the previous test.\nThe results of our second test are provided in Figure 2. In this case, we compare Priv’IT and zCDP-GOF, and note that our test is slightly better for all support sizes n, though the difference can be pronounced or diminished depending on the construction of the distribution q. We found that MCGOF was incredibly inefficient on this construction – even for n = 400 it required 130000 samples, which is a factor of 10 worse than zCDP-GOF on a support of size n = 6800. To explain this phenomenon, we can inspect the contribution of a single domain element i to their statistic:\n(Ni + Yi −mqi)2\nmqi .\nIn the case where mqi 1 and p = q, this is approximately equal to Y 2 i\nmqi . The standard deviation of this\nterm will be of the order 1mqiε2 , which can be made arbitrarily large as mqi → 0. While zCDP-GOF may naively seem susceptible to this same pitfall, their projection method appears to elegantly avoid it.\nAs a final test, we note that zCDP-GOF guarantees zCDP, while Priv’IT guarantees (vanilla) differen-\ntial privacy. In our previous tests, our guarantee was ε-differential privacy, while theirs was ε 2\n2 -zCDP: by Proposition 1, our guarantees imply theirs. In the third test, we revisit uniformity testing, but when their guarantees imply ours. More specifically, again with ε = 0.1, we ran zCDP-GOF with the guarantee of ε 2\n2 -\nzCDP and Priv’IT with the guarantee of ( ε 2 2 + ε √\n2 log(1/δ), δ) for various δ > 0. We note that δ is often thought in theory to be “cryptographically small” (such as 2−100), but we compare with a wide range of δ, both large and small: δ = 1/et for t ∈ {1, 2, 4, 8, 16}. This test was conducted on support sizes n ranging from 10 to 6000.\nThe results of our third test are provided in Figure 3. We found that, for all δ tested, Priv’IT required fewer samples than zCDP-GOF. This is unsurprising for δ very large and small, since the differential privacy guarantees become very easy to satisfy, but we found it to be true for even “moderate” values of δ. This\n1A k-histogram is a distribution where the domain can be partitioned into k intervals such that the distribution is uniform over each interval.\n2In particular, in Figure 3, n/200 support elements contained 1 − 10/n probability mass, but similar trends hold with modifications of these parameters.\nIdentity Testing on a 2-Histogram\nUniformity Testing, Revisited\nimplies that if an analyst is satisfied with approximate differential privacy, she might be better off using Priv’IT, rather than an algorithm which guarantees zCDP.\nWhile the main focus of our evaluation was statistical in nature, we will note that Priv’IT was more efficient in runtime than our implementation of MCGOF, and more efficient in memory usage than our implementation of zCDP-GOF. The former point was observed by noting that, in the same amount of time, Priv’IT was able to reach a trial corresponding to a support size of 20000, while MCGOF was only able to reach 10000. The latter point was observed by noting that zCDP-GOF ran out of memory at a support size of 11800. This is likely because zCDP-GOF requires matrix computations on a matrix of size O(n2). It is plausible that all of these implementations could be made more time and memory efficient, but we found our implementations to be sufficient for the sake of our comparison."
    }, {
      "heading" : "Acknowledgments",
      "text" : "The authors would like to thank Jon Ullman for helpful discussions in the early stages of this work. The authors were supported by NSF CCF-1551875, CCF-1617730, CCF-1650733, and ONR N00014-12-1-0999."
    } ],
    "references" : [ {
      "title" : "On the design and quantification of privacy preserving data mining algorithms",
      "author" : [ "Dakshi Agrawal", "Charu C. Aggarwal" ],
      "venue" : "In Proceedings of the 20th ACM SIGMOD-SIGACT-SIGART Symposium on Principles of Database Systems,",
      "citeRegEx" : "Agrawal and Aggarwal.,? \\Q2001\\E",
      "shortCiteRegEx" : "Agrawal and Aggarwal.",
      "year" : 2001
    }, {
      "title" : "Optimal testing for properties of distributions",
      "author" : [ "Jayadev Acharya", "Constantinos Daskalakis", "Gautam Kamath" ],
      "venue" : "In Advances in Neural Information Processing Systems 28,",
      "citeRegEx" : "Acharya et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Acharya et al\\.",
      "year" : 2015
    }, {
      "title" : "Security-control methods for statistical databases: A comparative study",
      "author" : [ "Nabil R. Adam", "John C. Worthmann" ],
      "venue" : "ACM Computing Surveys (CSUR),",
      "citeRegEx" : "Adam and Worthmann.,? \\Q1989\\E",
      "shortCiteRegEx" : "Adam and Worthmann.",
      "year" : 1989
    }, {
      "title" : "Testing random variables for independence and identity",
      "author" : [ "Tugkan Batu", "Eldar Fischer", "Lance Fortnow", "Ravi Kumar", "Ronitt Rubinfeld", "Patrick White" ],
      "venue" : "In Proceedings of the 42nd Annual IEEE Symposium on Foundations of Computer Science,",
      "citeRegEx" : "Batu et al\\.,? \\Q2001\\E",
      "shortCiteRegEx" : "Batu et al\\.",
      "year" : 2001
    }, {
      "title" : "Concentrated differential privacy: Simplifications, extensions, and lower bounds",
      "author" : [ "Mark Bun", "Thomas Steinke" ],
      "venue" : "In Proceedings of the 14th Conference on Theory of Cryptography, TCC ’16-B,",
      "citeRegEx" : "Bun and Steinke.,? \\Q2016\\E",
      "shortCiteRegEx" : "Bun and Steinke.",
      "year" : 2016
    }, {
      "title" : "A survey on distribution testing: Your data is big. but is it blue",
      "author" : [ "Clément L. Canonne" ],
      "venue" : "Electronic Colloquium on Computational Complexity (ECCC),",
      "citeRegEx" : "Canonne.,? \\Q2015\\E",
      "shortCiteRegEx" : "Canonne.",
      "year" : 2015
    }, {
      "title" : "A short note on Poisson tail bounds",
      "author" : [ "Clément L. Canonne" ],
      "venue" : "http://www.cs.columbia.edu/ ~ccanonne/files/misc/2017-poissonconcentration.pdf,",
      "citeRegEx" : "Canonne.,? \\Q2017\\E",
      "shortCiteRegEx" : "Canonne.",
      "year" : 2017
    }, {
      "title" : "Testing shape restrictions of discrete distributions",
      "author" : [ "Clément L. Canonne", "Ilias Diakonikolas", "Themis Gouleakis", "Ronitt Rubinfeld" ],
      "venue" : "In Proceedings of the 33rd Symposium on Theoretical Aspects of Computer Science,",
      "citeRegEx" : "Canonne et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Canonne et al\\.",
      "year" : 2016
    }, {
      "title" : "Towards a methodology for statistical disclosure control",
      "author" : [ "Tore Dalenius" ],
      "venue" : "Statistisk Tidskrift,",
      "citeRegEx" : "Dalenius.,? \\Q1977\\E",
      "shortCiteRegEx" : "Dalenius.",
      "year" : 1977
    }, {
      "title" : "Testing Ising models",
      "author" : [ "Constantinos Daskalakis", "Nishanth Dikkala", "Gautam Kamath" ],
      "venue" : "arXiv preprint arXiv:1612.03147,",
      "citeRegEx" : "Daskalakis et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Daskalakis et al\\.",
      "year" : 2016
    }, {
      "title" : "Differentially private learning of structured discrete distributions",
      "author" : [ "Ilias Diakonikolas", "Moritz Hardt", "Ludwig Schmidt" ],
      "venue" : "In Advances in Neural Information Processing Systems 28,",
      "citeRegEx" : "Diakonikolas et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Diakonikolas et al\\.",
      "year" : 2015
    }, {
      "title" : "A new approach for testing properties of discrete distributions",
      "author" : [ "Ilias Diakonikolas", "Daniel M. Kane" ],
      "venue" : "In Proceedings of the 57th Annual IEEE Symposium on Foundations of Computer Science,",
      "citeRegEx" : "Diakonikolas and Kane.,? \\Q2016\\E",
      "shortCiteRegEx" : "Diakonikolas and Kane.",
      "year" : 2016
    }, {
      "title" : "Calibrating noise to sensitivity in private data analysis",
      "author" : [ "Cynthia Dwork", "Frank McSherry", "Kobbi Nissim", "Adam Smith" ],
      "venue" : "In Proceedings of the 3rd Conference on Theory of Cryptography,",
      "citeRegEx" : "Dwork et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Dwork et al\\.",
      "year" : 2006
    }, {
      "title" : "Revealing information while preserving privacy",
      "author" : [ "Irit Dinur", "Kobbi Nissim" ],
      "venue" : "In Proceedings of the 22nd ACM SIGMOD-SIGACT-SIGART Symposium on Principles of Database Systems,",
      "citeRegEx" : "Dinur and Nissim.,? \\Q2003\\E",
      "shortCiteRegEx" : "Dinur and Nissim.",
      "year" : 2003
    }, {
      "title" : "The Algorithmic Foundations of Differential Privacy",
      "author" : [ "Cynthia Dwork", "Aaron Roth" ],
      "venue" : "Now Publishers, Inc.,",
      "citeRegEx" : "Dwork and Roth.,? \\Q2014\\E",
      "shortCiteRegEx" : "Dwork and Roth.",
      "year" : 2014
    }, {
      "title" : "Differential privacy: A survey of results",
      "author" : [ "Cynthia Dwork" ],
      "venue" : "In Proceedings of the 5th International Conference on Theory and Applications of Models of Computation,",
      "citeRegEx" : "Dwork.,? \\Q2008\\E",
      "shortCiteRegEx" : "Dwork.",
      "year" : 2008
    }, {
      "title" : "Differentially private chi-squared hypothesis testing: Goodness of fit and independence testing",
      "author" : [ "Marco Gaboardi", "Hyun-Woo Lim", "Ryan M. Rogers", "Salil P. Vadhan" ],
      "venue" : "In Proceedings of the 33rd International Conference on Machine Learning,",
      "citeRegEx" : "Gaboardi et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Gaboardi et al\\.",
      "year" : 2016
    }, {
      "title" : "Resolving individuals contributing trace amounts of dna to highly complex mixtures using high-density snp genotyping microarrays",
      "author" : [ "Nils Homer", "Szabolcs Szelinger", "Margot Redman", "David Duggan", "Waibhav Tembe", "Jill Muehling", "John V. Pearson", "Dietrich A. Stephan", "Stanley F. Nelson", "David W. Craig" ],
      "venue" : "PLoS Genetics,",
      "citeRegEx" : "Homer et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Homer et al\\.",
      "year" : 2008
    }, {
      "title" : "Privacy-preserving data exploration in genome-wide association studies",
      "author" : [ "Aaron Johnson", "Vitaly Shmatikov" ],
      "venue" : "In Proceedings of the 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD",
      "citeRegEx" : "Johnson and Shmatikov.,? \\Q2013\\E",
      "shortCiteRegEx" : "Johnson and Shmatikov.",
      "year" : 2013
    }, {
      "title" : "Bounds on tail probabilities of discrete distributions",
      "author" : [ "Bernhard Klar" ],
      "venue" : "Probability in the Engineering and Informational Sciences,",
      "citeRegEx" : "Klar.,? \\Q2000\\E",
      "shortCiteRegEx" : "Klar.",
      "year" : 2000
    }, {
      "title" : "A new class of private chi-square tests",
      "author" : [ "Daniel Kifer", "Ryan M. Rogers" ],
      "venue" : "In Proceedings of the 20th International Conference on Artificial Intelligence and Statistics,",
      "citeRegEx" : "Kifer and Rogers.,? \\Q2017\\E",
      "shortCiteRegEx" : "Kifer and Rogers.",
      "year" : 2017
    }, {
      "title" : "Differentially private chi-squared test by unit circle mechanism",
      "author" : [ "Kazuya Kakizaki", "Jun Sakuma", "Kazuto Fukuchi" ],
      "venue" : "In Proceedings of the 34th International Conference on Machine Learning,",
      "citeRegEx" : "Kakizaki et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Kakizaki et al\\.",
      "year" : 2017
    }, {
      "title" : "Testing properties of collections of distributions",
      "author" : [ "Reut Levi", "Dana Ron", "Ronitt Rubinfeld" ],
      "venue" : "Theory of Computing,",
      "citeRegEx" : "Levi et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Levi et al\\.",
      "year" : 2013
    }, {
      "title" : "A coincidence-based test for uniformity given very sparsely sampled discrete data",
      "author" : [ "Liam Paninski" ],
      "venue" : "IEEE Transactions on Information Theory,",
      "citeRegEx" : "Paninski.,? \\Q2008\\E",
      "shortCiteRegEx" : "Paninski.",
      "year" : 2008
    }, {
      "title" : "On the criterion that a given system of deviations from the probable in the case of a correlated system of variables is such that it can be reasonably supposed to have arisen from random sampling",
      "author" : [ "Karl Pearson" ],
      "venue" : "Philosophical Magazine Series",
      "citeRegEx" : "Pearson.,? \\Q1900\\E",
      "shortCiteRegEx" : "Pearson.",
      "year" : 1900
    }, {
      "title" : "A few good inequalities",
      "author" : [ "David Pollard" ],
      "venue" : "http://www.stat.yale.edu/~pollard/Books/Mini/ Basic.pdf,",
      "citeRegEx" : "Pollard.,? \\Q2015\\E",
      "shortCiteRegEx" : "Pollard.",
      "year" : 2015
    }, {
      "title" : "The analysis of categorical data from complex sample surveys: Chi-squared tests for goodness of fit and independence in two-way tables",
      "author" : [ "Jon N.K. Rao", "Alastair J. Scott" ],
      "venue" : "Journal of the Americal Statistical Association,",
      "citeRegEx" : "Rao and Scott.,? \\Q1981\\E",
      "shortCiteRegEx" : "Rao and Scott.",
      "year" : 1981
    }, {
      "title" : "Enabling privacy-preserving gwass in heterogeneous human populations",
      "author" : [ "Sean Simmons", "Cenk Sahinalp", "Bonnie Berger" ],
      "venue" : "Cell Systems,",
      "citeRegEx" : "Simmons et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Simmons et al\\.",
      "year" : 2016
    }, {
      "title" : "Error exponents for composite hypothesis testing of Markov forest distributions",
      "author" : [ "Vincent Y.F. Tan", "Animashree Anandkumar", "Alan S. Willsky" ],
      "venue" : "In Proceedings of the 2010 IEEE International Symposium on Information Theory, ISIT",
      "citeRegEx" : "Tan et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Tan et al\\.",
      "year" : 2010
    }, {
      "title" : "Privacy-preserving data sharing for genome-wide association studies",
      "author" : [ "Caroline Uhler", "Aleksandra Slavković", "Stephen E. Fienberg" ],
      "venue" : "The Journal of Privacy and Confidentiality,",
      "citeRegEx" : "Uhler et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Uhler et al\\.",
      "year" : 2013
    }, {
      "title" : "An automatic inequality prover and instance optimal identity testing",
      "author" : [ "Gregory Valiant", "Paul Valiant" ],
      "venue" : "In Proceedings of the 55th Annual IEEE Symposium on Foundations of Computer Science,",
      "citeRegEx" : "Valiant and Valiant.,? \\Q2014\\E",
      "shortCiteRegEx" : "Valiant and Valiant.",
      "year" : 2014
    }, {
      "title" : "Differentially private hypothesis testing, revisited",
      "author" : [ "Yue Wang", "Jaewoo Lee", "Daniel Kifer" ],
      "venue" : "arXiv preprint arXiv:1511.03376,",
      "citeRegEx" : "Wang et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2015
    }, {
      "title" : "Scalable privacypreserving data sharing methodology for genome-wide association studies",
      "author" : [ "Fei Yu", "Stephen E. Fienberg", "Aleksandra B. Slavković", "Caroline Uhler" ],
      "venue" : "Journal of Biomedical Informatics,",
      "citeRegEx" : "Yu et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Yu et al\\.",
      "year" : 2014
    } ],
    "referenceMentions" : [ ],
    "year" : 2017,
    "abstractText" : "We develop differentially private hypothesis testing methods for the small sample regime. Given a sample D from a categorical distribution p over some domain Σ, an explicitly described distribution q over Σ, some privacy parameter ε, accuracy parameter α, and requirements βI and βII for the type I and type II errors of our test, the goal is to distinguish between p = q and dTV(p, q) ≥ α. We provide theoretical bounds for the sample size |D| so that our method both satisfies (ε, 0)differential privacy, and guarantees βI and βII type I and type II errors. We show that differential privacy may come for free in some regimes of parameters, and we always beat the sample complexity resulting from running the χ-test with noisy counts, or standard approaches such as repetition for endowing non-private χ-style statistics with differential privacy guarantees. We experimentally compare the sample complexity of our method to that of recently proposed methods for private hypothesis testing [GLRV16, KR17].",
    "creator" : "LaTeX with hyperref package"
  }
}