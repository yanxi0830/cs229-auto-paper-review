{
  "name" : "1312.3388.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Online Bayesian Passive-Aggressive Learning",
    "authors" : [ "Tianlin Shi", "Jun Zhu" ],
    "emails" : [ "STL501@GMAIL.COM", "DCSZJ@MAIL.TSINGHUA.EDU.CN" ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "Online learning is an effective way to deal with large-scale applications, especially applications with streaming data. Among the popular algorithms, online Passive-Aggressive (PA) learning (Crammer et al., 2006) provides a generic framework for online large-margin learning, with many applications (McDonald et al., 2005; Chiang et al., 2008). Though enjoying strong discriminative ability suitable for predictive tasks, existing online PA methods are formulated as a point estimate problem by optimizing some deterministic objective function. This may lead to some inconvenience. For example, a single large-margin model is often less than sufficient in describing complex data, such as those with rich underlying structures.\nProceedings of the 31 st International Conference on Machine Learning, Beijing, China, 2014. JMLR: W&CP volume 32. Copyright 2014 by the author(s).\nOn the other hand, Bayesian methods enjoy great flexibility in describing the possible underlying structures of complex data. Moreover, the recent progress on nonparametric Bayesian methods (Hjort, 2010; Teh et al., 2006a) further provides an increasingly important framework that allows the Bayesian models to have an unbounded model complexity, e.g., an infinite number of components in a mixture model (Hjort, 2010) or an infinite number of units in a latent feature model (Ghahramani & Griffiths, 2005), and to adapt when the learning environment changes. For Bayesian models, one challenging problem is posterior inference, for which both variational and Monte Carlo methods can be too expensive to be applied to large-scale applications. To scale up Bayesian inference, much progress has been made on developing online variational Bayes (Hoffman et al., 2010; Mimno et al., 2012) and online Monte Carlo (Ahn et al., 2012) methods. However, due to the generative nature, Bayesian models are lack of the discriminative ability of large-margin methods and usually less than sufficient in performing discriminative tasks.\nSuccessful attempts have been made to bring large-margin learning and Bayesian methods together. For example, maximum entropy discrimination (MED) (Jaakkola et al., 1999) made a significant advance in conjoining maxmargin learning and Bayesian generative models, mainly in the context of supervised learning and structured output prediction (Zhu & Xing, 2009). Recently, much attention has been focused on generalizing MED to incorporate latent variables and perform nonparametric Bayesian inference, in many contexts including topic modeling (Zhu et al., 2012), matrix factorization (Xu et al., 2012), and multi-task learning (Jebara, 2011; Zhu et al., 2011). However, posterior inference in such models remain a big challenge. It is desirable to develop efficient online algorithms for these Bayesian max-margin models.\nTo address the above problems of both the existing online PA algorithms and Bayesian max-margin models, this paper presents online Bayesian Passive-Aggressive (BayesPA) learning, a general framework of performing\nar X\niv :1\n31 2.\n33 88\nv1 [\ncs .L\nG ]\n1 2\nD ec\n2 01\nonline learning for Bayesian max-margin models. We show that online BayesPA subsumes the standard online PA when the underlying model is linear and the parameter prior is Gaussian. We further show that another major significance of BayesPA is its natural generalization to incorporate latent variables and to perform nonparametric Bayesian inference, thus allowing online BayesPA to have the great flexibility of (nonparametric) Bayesian methods for explorative analysis as well as the strong discriminative ability of large-margin learning for predictive tasks. As concrete examples, we apply the theory of online BayesPA to topic modeling and derive efficient online learning algorithms for max-margin supervised topic models (Zhu et al., 2012). We further develop efficient online learning algorithms for the nonparametric max-margin topic models, an extension of the nonparametric topic models (Teh et al., 2006a; Wang et al., 2011) for predictive tasks. Extensive empirical results on real data sets show significant improvements on time efficiency and maintenance of comparable results with the batch counterparts."
    }, {
      "heading" : "2. Bayesian Passive-Aggressive Learning",
      "text" : "In this section, we present a general perspective on online max-margin Bayesian inference."
    }, {
      "heading" : "2.1. Online PA Learning",
      "text" : "The goal of online supervised learning is to minimize the cumulative loss for a certain prediction task from the sequentially arriving training samples. Online PassiveAggressive (PA) algorithms (Crammer et al., 2006) achieve this goal by updating some parameterized model w (e.g., the weights of a linear SVM) in an online manner with the instantaneous losses from arriving data {xt}t≥0 and corresponding responses {yt}t≥0. The losses ` (w;xt, yt), as they consider, could be the hinge loss ( − ytw>xt)+ for binary classification or the -insensitive loss (|yt − w>xt| − )+ for regression, where is a hyper-parameter and (x)+ = max(0, x). The Passive-Aggressive update rule is then derived by defining the new weightwt+1 as the solution to the following optimization problem:\nmin w\n1 2 ||w −wt||2 s.t.: ` (w;xt, yt) = 0. (1)\nIntuitively, if wt suffers no loss from the new data, i.e., ` (wt;xt, yt) = 0, the algorithm passively assignswt+1 = wt; otherwise, it aggressively projects wt to the feasible zone of parameter vectors that attain zero loss. With provable bounds, (Crammer et al., 2006) shows that online PA algorithms could achieve comparable results to the optimal classifier w∗. In practice, in order to account for inseparable training samples, soft margin constraints are often\nadopted and the resulting learning problem is\nmin w\n1 2 ||w −wt||2 + 2c` (w;xt, yt), (2)\nwhere c is a positive regularization parameter. For problems (1) and (2) with samples arriving one at a time, closedform solutions can be derived (Crammer et al., 2006)."
    }, {
      "heading" : "2.2. Online BayesPA Learning",
      "text" : "Instead of updating a point estimate of w, online Bayesian PA (BayesPA) sequentially infers a new posterior distribution qt+1(w), either parametric or nonparametric, on the arrival of new data (xt, yt) by solving the following optimization problem:\nmin q(w)∈Ft\nKL[q(w)||qt(w)]− Eq(w)[log p(xt|w)]\ns.t.: ` [q(w);xt, yt] = 0, (3)\nwhere Ft is some distribution family, e.g., the probability simplex P . In other words, we find a posterior distribution qt+1(w) in the feasible zone that is not only close to qt(w) by the commonly used KL-divergence, but also has a high likelihood of explaining new data. As a result, if Bayes’ rule already gives the posterior distribution qt+1(w) ∝ qt(w)p(xt|w) that suffers no loss (i.e., ` = 0), BayesPA passively updates the posterior following just Bayes’ rule; otherwise, BayesPA aggressively projects the new posterior to the feasible zone of posteriors that attain zero loss. We should note that when no likelihood is defined (e.g., p(xt|w) is independent of w), BayesPA will passively set qt+1(w) = qt(w) if qt(w) suffers no loss. We call it non-likelihood BayesPA.\nIn practical problems, the constraints in (3) could be unrealizable. To deal with such cases, we introduce the softmargin version of BayesPA learning, which is equivalent to minimizing the objective function L(q(w)) in problem (3) with a regularization term (Cortes & Vapnik, 1995):\nqt+1(w) = argmin q(w)∈Ft\nL(q(w)) + 2c` (q(w);xt, yt). (4)\nFor the max-margin classifiers that we focus on in this paper, two loss functions ` (q(w);xt, yt) are common — the hinge loss of an averaging classifier that makes predictions using the rule ŷt = sign Eq(w)[w>xt]:\n`Avg [q(w);xt, yt] = ( − ytEq(w)[w>xt])+\nand the expected hinge loss of a Gibbs classifier that randomly draws a classifier w ∼ q(w) to make predictions using the rule ŷt = sign w>xt:\n`Gibbs [q(w);xt, yt] = Eq(w)[( − ytw>xt)+].\nThey are closely connected via the following lemma due to the convexity of the function (x)+.\nLemma 2.1. The expected hinge loss `Gibbs is an upper bound of the hinge loss `Avg , that is, ` Gibbs ≥ `Avg .\nBefore developing BayesPA learning for practical problems, we make several observations.\nLemma 2.2. If q0(w) = N (0, I), Ft = P and we use `Avg , the non-likelihood BayesPA subsumes the online PA.\nThis can be proved by induction. First, we can show that qt(w) = N (µt, I) is a normal distribution with an identity covariance matrix. Second, we can show that the posterior mean µt is updated in the same way as in the online PA. We defer the detailed proof to Appendix A.\nLemma 2.3. If Ft = P and we use `Gibbs , the update rule of online BayesPA is\nqt+1(w) = qt(w)p(xt|w)e−2c( −ytw\n>xt)+\nΓ(xt, yt) , (5)\nwhere Γ(xt, yt) is the normalization constant.\nTherefore, the posterior qt(w) in the previous round t becomes a prior, while the newly observed data and its loss function provide a likelihood and an unnormalized pseudolikelihood respectively.\nMini-Batches. A useful technique to reduce the noise in data is the use of mini-batches. Suppose that we have a mini-batch of data points at time t with an index set Bt, denoted as Xt = {xd}d∈Bt ,Yt = {yd}d∈Bt . The Bayesian PA update equation for this mini-batch is simply,\nqt+1(w) = argmin q∈Ft\nL(q(w)) + 2c` (q(w);Xt,Yt),\nwhere ` (q(w);Xt,Yt) = ∑ d∈Bt ` (q(w);xd, yd)."
    }, {
      "heading" : "2.3. Learning with Latent Structures",
      "text" : "To expressively explain complex real-word data, Bayesian models with latent structures have been extensively developed. The latent structures could typically be characterized by two kinds of latent variables — local latent variables hd (d ≥ 0) that characterize the hidden structures of each observed data xd and global variables M that capture the common properties shared by all data.\nThe goal of Bayesian PA learning with latent structures is therefore to update the distribution of M as well as weights w based on each incoming mini-batch (Xt,Yt) and their corresponding latent variables Ht = {hd}d∈Bt . Because of the uncertainty in Ht, we extend BayesPA to infer the joint posterior distribution, qt+1(w,M,Ht), as solving\nmin q∈Ft\nL(q(w,M,Ht)) + 2c` (q(w,M,Ht);Xt,Yt),(6)\nwhere L(q)=KL[q||qt(w,M)p0(Ht)]−Eq[log p(Xt|w, M,Ht)] and ` (q;Xt,Yt) is some cumulative marginloss on the min-batch data induced from some classifiers defined on the latent variables Ht and/or global variables M. Both the averaging classifiers and Gibbs classifiers can be used as in the case without latent variables. We will present concrete examples in the next section.\nBefore diving into the details, we should note that in real online setting, only global variables are maintained in the bookkeeping, while the local information in the streaming data is forgotten. However, (6) gives us a distribution of (w,M) that is coupled with the local variables Ht. Although in some cases we can marginalize out the local variables Ht, in general we would not obtain a closed-form posterior distribution qt+1(w,M) for the next optimization round, especially in dealing with some involved models like MedLDA (Zhu et al., 2012). Therefore, we resort to approximation methods, e.g., by posing additional assumptions about q(w,M,Ht) such as the mean-field assumption, q(w,M,Ht) = q(w)q(M)q(Ht). Then, we can solve the problem via an iterative procedure and use the optimal distribution q∗(w)q∗(M) as qt+1(w,M). More details will be provided in next sections."
    }, {
      "heading" : "3. Online Max-Margin Topic Models",
      "text" : "We apply the theory of online BayesPA to topic modeling and develop online learning algorithms for max-margin topic models. We also present a nonparametric generalization to resolve the number of topics in the next section."
    }, {
      "heading" : "3.1. Batch MedLDA",
      "text" : "A max-margin topic model consists of a latent Dirichlet allocation (LDA) (Blei et al., 2003) model for describing the underlying topic representations and a max-margin classifier for predicting responses. Specifically, LDA is a hierarchical Bayesian model that treats each document as an admixture of topics, Φ = {φk}Kk=1, where each topic φk is a multinomial distribution over a W -word vocabulary. Let θ denote the mixing proportions. The generative process of document d is described as\nθd ∼ Dir(α), zdi ∼ Mult(θd), xdi ∼ Mult(φzdi), ∀i ∈ [nd]\nwhere zdi is a topic assignment variable and Mult(·) is a multinomial distribution. For Bayesian LDA, the topics are drawn from a Dirichlet distribution, i.e., φk ∼ Dir(γ).\nGiven a document set X = {xd}Dd=1. Let Z = {zd}Dd=1 and Θ = {θd}Dd=1. LDA infers the posterior distribution p(Φ,Θ,Z|X) ∝ p0(Φ,Θ,Z)p(X|Z,Φ) via Bayes’\nrule. From a variational point of view, the Bayes posterior is equivalent to the solution of the optimization problem:\nmin q∈P\nKL[q(Φ,Θ,Z)||p(Φ,Θ,Z|X)].\nThe advantage of the variational formulation of Bayesian inference lies in the convenience of posing restrictions on the post-data distribution with a regularization term. For supervised topic models (Blei & McAuliffe, 2010; Zhu et al., 2012), such a regularization term could be a loss function of a prediction model w on the data X = {xd}Dd=1 and response signals Y = {yd}Dd=1. As a regularized Bayesian (RegBayes) model (Jiang et al., 2012), MedLDA infers a distribution of the latent variables Z as well as classification weights w by solving the problem:\nmin q∈P L(q(w,Φ,Θ,Z)) + 2c D∑ d=1 ` (q(w, zd);xd, yd),\nwhere L(q(w,Φ,Θ,Z)) = KL[q(w,Φ,Θ,Z)||p(w,Φ,Θ, Z|X)] . To specify the loss function, a linear discriminant function needs to be defined with respect to w and zd\nf(w, zd) = w >z̄d, (7)\nwhere z̄dk = 1nd ∑ i I[zdi = k] is the average topic assignments of the words in document d. Based on the discriminant function, both averaging classifiers with the hinge loss\n`Avg (q(w, zd);xd, yd) = ( − ydEq[f(w, zd)])+, (8)\nand Gibbs classifiers with the expected hinge loss\n`Gibbs (q(w, zd);xd, yd) = Eq[( − ydf(w, zd))+], (9)\nhave been proposed, with extensive comparisons reported in (Zhu et al., 2013a) using batch learning algorithms."
    }, {
      "heading" : "3.2. Online MedLDA",
      "text" : "To apply the online BayesPA, we have the global variables M = Φ and local variables Ht = (Θt,Zt). We consider Gibbs MedLDA because as shown in (Zhu et al., 2013a) it admits efficient inference algorithms by exploring data augmentation. Specifically, let ζd = − ydf(w, zd) and ψ(yd|zd,w) = e−2c(ζd)+ . Then in light of Lemma 2.3, the optimal solution to problem (6), qt+1(w,M,Ht), is\nqt(w,M)p0(Ht)p(Xt|Ht,M)ψ(Yt|Ht,w) Γ(Xt,Yt) ,\nwhere ψ(Yt|Ht,w) = ∏\nd∈Bt ψ(yd|hd,w) and Γ(Xt,Yt) is a normalization constant. To potentially improve the inference accuracy, we first integrate out the local variables Θt by the conjugacy between a Dirichlet prior and a multinomial likelihood (Griffiths & Steyvers, 2004; Teh et al., 2006b). Then we have the local variables Ht = Zt. By the equality (Zhu et al., 2013a):\nψ(yd|zd,w) = ∫ ∞\n0\nψ(yd, λd|zd,w)dλd, (10)\nwhere ψ(yd, λd|zd,w) = (2πλd)−1/2 exp(− (λd+cζd) 2\n2λd ),\nthe collapsed posterior qt+1(w,Φ,Zt) is a marginal distribution of qt+1(w,Φ,Zt,λt), which equals to\np0(Zt)qt(w,Φ)p(Xt|Zt,Φ)ψ(Yt,λt|Zt,w) Γ(Xt,Yt) ,\nwhere ψ(Yt,λt|Zt,w) = ∏\nd∈Bt ψ(yd, λd|zd,w) and λt = {λd}d∈Bt are augmented variables, which are also locally associated with individual documents. In fact, the augmented distribution qt+1(w,Φ,Zt,λt) is the solution to the problem:\nmin q∈P L(q(w,Φ,Zt,λt))− Eq[logψ(Yt,λt|Zt,w)], (11)\nwhere L(q) = KL[q(w,Φ,Zt,λt)‖qt(w,Φ)p0(Zt)]− Eq[log p(Xt|Zt,Φ)]. We can show that this objective is an upper bound of that in the original problem (6). See Appendix B for details.\nWith the mild mean-field assumption that q(w,Φ,Zt,λt) = q(w)q(Φ)q(Zt,λt), we can solve (11) via an iterative procedure that alternately updates each factor distribution (Jordan et al., 1998), as detailed below.\nGlobal Update: By fixing the distribution of local variables, q(Zt,λt), and ignoring irrelevant variables, we have the mean-field update equations:\nq(Φk) ∝ qt(Φk) exp(Eq(Zt)[log p0(Zt)p(X|Zt,Φ)]), ∀ k q(w) ∝ qt(w) exp(Eq(Zt,λt)[log p0(Zt)ψ(Yt,λt|Zt,w)]).\nIf initially q0(Φk) = Dir(∆0k1, ...,∆ 0 kW ) and q0(w) = N (w;µ0,Σ0), by induction we can show that the inferred distributions in each round has a closed form, namely, qt(Φk) = Dir(∆tk1, ...,∆ t kW ) and qt(w) = N (w;µt,Σt). For the above update equations, we have\nq(Φk) = Dir(∆∗k1, ...,∆ ∗ kW ), (12)\nwhere ∆∗kw = ∆ t kw + ∑ d∈Bt ∑ i∈[nd] γ k di · I[xdi = w] for all words w and γkdi = Eq(zd)I[zdi = k] is the probability of assigning word xdi to topic k, and\nq(w) = N (w;µ∗,Σ∗), (13)\nwhere the posterior paramters are computed as (Σ∗)−1 = (Σt)−1 + c2 ∑ d∈Bt Eq(zd,λd)[λ −1 d z̄dz̄ > d ] and µ ∗ =\nΣ∗(Σt)−1µt + Σ∗ · c ∑ d∈Bt Eq(zd,λd)[yd(1 + c λ −1 d )z̄d].\nLocal Update: Given the distribution of global variables, q(Φ,w), the mean-field update equation for (Zt,λt) is\nq(Zt,λt) ∝ p0(Zt) ∏ d∈Bt 1√ 2πλd exp ( ∑ i∈[nd] Λzdi,xdi\n−Eq(Φ,w)[ (λd + cζd)\n2 2λd ] ) , (14)\nAlgorithm 1 Online MedLDA 1: Let q0(w) = N (0; v2I), q0(φk) = Dir(γ), ∀ k. 2: for t = 0→∞ do 3: Set q(Φ,w) = qt(Φ,w). Initialize Zt. 4: for i = 1→ I do 5: Draw samples {Z(j)t ,λ (j) t }Jj=1 from (15, 16).\n6: Discard the first β burn-in samples (β < J ). 7: Use the rest J − β samples to update q(Φ,w) following (12, 13). 8: end for 9: Set qt+1(Φ,w) = q(Φ,w).\n10: end for\nwhere Λzdi,xdi = Eq(Φ)[log(Φzdi,xdi)] = Ψ(∆∗zdi,xdi) − Ψ( ∑ w ∆ ∗ zdi,w\n) and Ψ(·) is the digamma function, due to the distribution in (12). But it is impossible to evaluate the expectation in the global update using (14) because of the huge number of configurations for (Zt,λt). As a result, we turn to Gibbs sampling and estimate the required expectations using multiple empirical samples. This hybrid strategy has shown promising performance for LDA (Mimno et al., 2012). Specifically, the conditional distributions used in the Gibbs sampling are as follows:\nFor Zt: By canceling out common factors, the conditional distribution of one variable zdi given Z¬dit and λt is\nq(zdi=k|Z¬dit ,λt)∝(α+C¬didk )exp ( cyd(c +λd)µ ∗ k\nndλd\n+Λk,xdi − c2(µ∗2k +Σ ∗ kk+2(µ ∗ kµ ∗+Σ∗·,k) >C¬did )\n2n2dλd\n) , (15)\nwhere Σ∗·,k is the k-th column of Σ ∗,C¬did is a vector with the k-th entry being the number of words in document d (except the i-th word) that are assigned to topic k.\nFor λt: Let ζ̄d = −ydz̄>d µ∗. The conditional distribution of each variable λd given Zt is\nq(λd|Zt) ∝ 1√2πλd exp ( − c 2z̄>d Σ ∗z̄d+(λd+cζ̄d) 2 2λd ) =GIG ( λd; 1 2 , 1, c 2(ζ̄2d + z̄ > d Σ ∗z̄d) ) , (16)\na generalized inverse gaussian distribution (Devroye, 1986). Therefore, λ−1d follows an inverse gaussian distribution IG(λ−1d ; 1\nc √ ζ̄2d+z̄ > d Σ ∗z̄d , 1), from which we can draw\na sample in constant time (Michael et al., 1976).\nFor training, we run the global and local updates alternately until convergence at each round of PA optimization, as outlined in Alg. 1. To make predictions on testing data, we then draw one sample of ŵ as the classification weight and apply the prediction rule. The inference of z̄ for testing documents is the same as in (Zhu et al., 2013a)."
    }, {
      "heading" : "4. Online Nonparametric MedLDA",
      "text" : "We present online nonparametric MedLDA for resolving the unknown number of topics, based on the theory of hierarchical Dirichlet process (HDP) (Teh et al., 2006a)."
    }, {
      "heading" : "4.1. Batch MedHDP",
      "text" : "HDP provides an extension to LDA that allows for a nonparametric inference of the unknown topic numbers. The generative process of HDP can be summarized using a stick-breaking construction (Wang & Blei, 2012), where the stick lengths π = {πk}∞k=1 are generated as:\nπk = π̄k ∏ i<k (1− π̄i), π̄k ∼ Beta(1, γ), for k = 1, ...,∞,\nand the topic mixing proportions are generated as θd ∼ Dir(απ), for d = 1, ..., D. Each topic φk is a sample from a Dirichlet base distribution, i.e., φk ∼ Dir(η). After we get the topic mixing proportions θd, the generation of words is the same as in the standard LDA.\nTo augment the HDP topic model for predictive tasks, we introduce a classifier w and define the linear discriminant function in the same form as (7), where we should note that since the number of words in a document is finite, the average topic assignment vector z̄d has only a finite number of non-zero elements. Therefore, the dot product in (7) is in fact finite. Let π̄ = {π̄k}∞k=1. We define MedHDP as solving the following problem to infer the joint posterior q(w, π̄,Φ,Θ,Z)1:\nmin q∈P L(q(w, π̄,Φ,Θ,Z)) + 2c D∑ d=1 ` (q(w, zd);xd,yd),\nwhere L(q(w,Φ, π̄,Θ,Z)) = KL[q(w,Φ, π̄,Θ,Z)||p(w, π̄,Φ,Θ,Z|X)], and the loss function could be either (8) or (9), leading to the MedHDP topic models with either averaging or Gibbs classifiers."
    }, {
      "heading" : "4.2. Online MedHDP",
      "text" : "To apply the online BayesPA, we have the global variables M = (π̄,Φ), and the local variables Ht = (Θt,Zt). We again focus on the expected hinge loss (9) in this paper. As in online MedLDA, we marginalize out Θt and adopt the same data augmentation technique with the augmented variables λt. Furthermore, to simplify the sampling scheme, we introduce auxiliary latent variables St = {sd}d∈Bt , where sdk represents the number of occupied tables serving dish k in a Chinese Restaurant Process (Teh et al., 2006a; Wang & Blei, 2012). By definition, we have p(Zt,St|π̄) = ∏ d∈Bt p(sd, zd|π̄) and\np(sd, zd|π̄) ∝ ∞∏ k=1 S(ndz̄dk, sdk)(απk) sdk , (17)\n1Given π̄, π can be computed via the stick breaking process.\nwhere S(a, b) are unsigned Stirling numbers of the first kind (Antoniak, 1974). It is not hard to verify that p(zd|π̄) = ∑ sd p(sd, zd|π̄). Therefore, we have local variables Ht = (Zt,St,λt), and the target collapsed posterior qt+1(w, π̄,Φ,Zt,λt) is the marginal distribution of qt+1(w, π̄,Φ,Ht), which is the solution of the problem:\nmin q∈Ft\nL(q(w, π̄,Φ,Ht))−Eq[logψ(Yt,λt|Zt,w)], (18)\nwhere L(q) = KL[q||qt(w, π̄,Φ)p(Zt,St|π̄)p(Xt|Zt,Φ)] . As in online MedLDA, we solve (18) via an iterative procedure detailed below.\nGlobal Update: By fixing the distribution of local variables, q(Zt,St,λt), and ignoring the irrelevant terms, we have the mean-field update equations for Φ and w, the same as in (12) and (13), while for π̄, we have\nq(π̄k) ∝ qt(π̄k) ∏ d∈Bt exp(Eq(hd)[log p(sd, zd|π̄)]). (19)\nBy induction, we can show that qt(π̄k) = Beta(utk, v t k), a Beta distribution at each step, and the update equation is\nq(π̄k) = Beta(u∗k, v ∗ k), (20)\nwhere u∗k = u t k + ∑ d∈Bt Eq(sd)[sdk] and v ∗ k = v t k +∑\nd∈Bt Eq(sd)[ ∑ j>k sdj ] for k = {1, 2, ...}. SinceZt contains only finite number of discrete variables, we only need to maintain and update the global distribution for a finite number of topics.\nLocal Update: Fixing the global distribution q(w, π̄,Φ), we get the mean-field update equation for (Zt,St,λt):\nq(Zt,St,λt) ∝ q̃(Zt,St)q̃(Zt,λt) (21)\nwhere q̃(Zt,St) = exp(Eq(Φ)q(π̄)[log p(Xt|Φ,Zt) + log p(Zt,St|π̄)]) and q̃(Zt,λt) = exp(Eq(w)[logψ(Yt, λt|w,Zt)]). To overcome the the potentially unbounded latent space, we take the ideas from (Wang & Blei, 2012) and adopt an approximation for q̃(Zt,St):\nq̃(Zt,St) ≈ Eq(Φ)q(π̄)[p(X|Φ,Zt)p(Zt,St|π̄)]. (22)\nInstead of marginalizing out π̄ in (22), which is analytically difficult, we sample π̄ jointly with (Zt,St,λt). This leads to the following Gibbs sampling scheme:\nFor Zt: Let K be the current inferred number of topics. The conditional distribution of one variable zdi givenZ¬dit , λt and π̄ can be derived from (21) with sd marginalized out for convenience:\nq(zdi = k|Z¬dit ,λt, π̄) ∝ (απk+C\n¬di dk )(C ¬di kxdi +∆∗kxdi )∑\nw (C ¬di kw +∆ ∗ kw) exp ( cyd(c +λd)µ ∗ k\nndλd − c\n2(µ∗2k +Σ ∗ kk+2(µ ∗ kµ ∗+Σ∗·,k) >C¬did )\n2n2dλd\n) .\nBesides, for k > K and symmetric Dirichlet prior η, this becomes q(zdi = k|Z¬dit ,λt, π̄) ∝ απk/W , and therefore the total probability of assigning a new topic is\nq(zdi > K|Z¬dit ,λt, π̄) ∝ α\n( 1−\nK∑ k=1 πk\n) /W.\nFor λt: The conditional distribution q(λd|Zt,St, π̄) is the same as (16).\nFor St: The conditional distribution of sdk given Zt, π̄, λt can be derived from the joint distribution (17):\nq(sdk|Zt,λt, π̄) ∝ S(ndz̄dk, sdk)(απk)sdk (23)\nFor π̄: It can be derived from (21) that given (Zt,St,λt), each π̄k follows the beta distribution, π̄k ∼ Beta(ak, bk), where ak = u∗k + ∑ d∈Bt sdk and bk = v ∗ k +∑\nd∈Bt ∑ j>k sdj .\nSimilar to online MedLDA, we iterate the above steps till convergence for training."
    }, {
      "heading" : "5. Experiments",
      "text" : "We demonstrate the efficiency and prediction accuracy of online MedLDA and MedHDP, denoted as paMedLDA and paMedHDP, on the 20Newsgroup (20NG) and a large Wikipedia dataset. A sensitivity analysis of the key parameters is also provided. Following the same setting in (Zhu et al., 2012), we remove a standard list of stop words. All of the experiments are done on a normal computer with single-core clock rate up to 2.4 GHz."
    }, {
      "heading" : "5.1. Classification on 20Newsgroup",
      "text" : "We perform multi-class classification on the entire 20NG dataset with all the 20 categories. The training set contains 11,269 documents, with the smallest category having 376 documents and the biggest category having 599 documents. The test set contains 7,505 documents, with the smallest and biggest categories having 259 and 399 documents respectively. We adopt the ”one-vs-all” strategy (Rifkin & Klautau, 2004) to combine binary classifiers for multi-class prediction tasks.\nWe compare paMedLDA and paMedHDP with their batch counterparts, denoted as bMedLDA and bMedHDP, which are obtained by letting the batch size |B| be equal to the dataset sizeD, and Gibbs MedLDA, denoted as gMedLDA, (Zhu et al., 2013a), which performs Gibbs sampling in the batch manner. We also consider online unsupervised topic models as baselines, including sparse inference for LDA (spLDA) (Mimno et al., 2012), which has been demonstrated to be superior than online variational LDA (Hoffman et al., 2010) in performance, and truncation-free online variational HDP (tfHDP) (Wang & Blei, 2012), which\nhas been shown to be promising in nonparametric topic modeling. For both of them, we learn a linear SVM with the topic representations using LIBSVM (Chang & Lin, 2011). The performances of other batch supervised topic models, such as sLDA (Blei & McAuliffe, 2010) and DiscLDA (Lacoste-Julien et al., 2008), are reported in (Zhu et al., 2012). For all LDA-based topic models, we use symmetric Dirichlet priors α = 1/K · 1,γ = 0.5 · 1; for all HDP-based topic models, we use α = 5, γ = 1,η = 0.45 · 1; for all MED topic models, we use = 164, c = 1, v = 1, the choice of which is not crucial to the models’ performance as shown in (Zhu et al., 2013a).\nWe first analyze how many processed documents are sufficient for each model to converge. Figure 1 shows the prediction accuracy with the number of passes through the entire 20NG dataset, where K = 80 for parametric models and (I,J , β) = (1, 2, 0) for BayesPA. As we could observe, by solving a series of latent BayesPA learning problems, paMedLDA and paMedHDP fully explore the redundancy of documents and converge in one pass, while their batch counterparts need many passes as burn-in steps. Besides, compared with the online unsupervised learning algorithms, BayesPA topic models utilize supervising-side information from each mini-batch, and therefore exhibit a faster convergence rate in discrimination ability.\nNext, we study each model’s best performance possible and the corresponding training time. To allow for a fair comparison, we train each model until the relative change of its objective is less than 10−4. Figure 2 shows the accuracy and training time of LDA-based models on the whole dataset with varying numbers of topics. Similarly, Figure 3 shows the accuracy and training time of HDP-based models, where the dots stand for the mean inferred numbers of topics, and the lengths of the horizontal bars represent their standard deviations. As we can see, BayesPA topic models, at the power of online learning, are about 1 order of magnitude faster than their batch counterparts in training time. Furthermore, thanks to the merits of Gibbs sampling, which does not pose strict mean-field assumptions about the independence of latent variables, BayesPA topic mod-\nels parallel their batch alternatives in accuracy."
    }, {
      "heading" : "5.2. Further Discussions",
      "text" : "We provide further discussions on BayesPA learning for topic models. First, we analyze the models’ sensitivity to some key parameters. Second, we illustrate an application with a large Wikipedia dataset containing 1.1 million documents, where class labels are not exclusive."
    }, {
      "heading" : "5.2.1. SENSITIVITY ANALYSIS",
      "text" : "Batch Size |B|: Figure 4 presents the test errors of BayesPA topic models as a function of training time on the entire 20NG dataset with various batch sizes, where K = 40. We can see that the convergence speeds of different algorithms vary. First of all, the batch algorithms suffer from multiple passes through the dataset and therefore are much slower than the online alternatives. Second, we could observe that algorithms with medium batch sizes (|B| = 64, 256) converge faster. If we choose a batch size too small, for example, |B| = 1, each iteration would not provide sufficient evidence for the update of global variables; if the batch size is too large, each mini-batch becomes redundant and the convergence rate reduces.\nNumber of iterations I and samples J : Since the time complexity of Algorithm 1 is linear in both I and J , we would like to know how these parameters influence the\nquality of the trained model. First, notice that the first β samples are discarded as burn-in steps. To understand how large β is sufficient, we consider the settings of the pairs (J , β) and check the prediction accuracy of Algorithm 1 for K = 40, |B| = 512, as shown in Table 1.\nWe can see that accuracies closer to the diagonal of the table are relatively lower, while settings with the same number of kept samples, e.g. (J , β) = (3, 0), (5, 2), (9, 6), yield similar results. The number of kept samples exhibits a more significant role in the performance of BayesPA topic models than the burn-in steps.\nNext, we analyze which setting of (I,J ) guarantees good performance. Figure 5 presents the results. As we can see, for J = 1, the algorithms suffer from the noisy approximation and therefore sacrifices prediction accuracy. But for larger J , simply I = 1 is promising, possibly due to the redundancy among mini-batches."
    }, {
      "heading" : "5.2.2. MULTI-TASK CLASSIFICATION",
      "text" : "For multi-task classification, a set of binary classifiers are trained, each of which identifies whether a document xd belongs to a specific task/category yτd ∈ {+1,−1}. These binary classifiers are allowed to share common latent representations and therefore could be attained via a modified BayesPA update equation:\nmin q∈Ft L(q(w,M,Ht)) + 2c T∑ τ=1 ` (q(w,M,Ht);Xt,Y τt )\nwhere T is the total number of tasks. We can then derive the multi-task version of Passive-Aggressive topic models, denoted by paMedLDA-mt and paMedHDP-mt, in a way similar to Section 3.2 and 4.2.\nWe test paMedLDA-mt and paMedHDP-mt as well as comparison models, including bMedLDA-mt, bMedHDP-mt and gMedLDA-mt (Zhu et al., 2013b) on a large Wiki dataset built from the Wikipedia set used in PASCAL LSHC challenge 2012 2. The Wiki dataset is a collection of documents with labels up to 20 different kinds, while the data distribution among the labels is balanced. The training/testing split is 1.1 million / 5 thousand. To measure performance, we use F1 score, the harmonic mean of precision and recall.\nFigure 6 shows the F1 scores of various models as a function of training time. We can see that BayesPA topic models are again about 1 order of magnitude faster than the batch alternatives and yet produce comparable results. Therefore, BayesPA topic models are potentially extendable to large-scale multi-class settings.\n2See http://lshtc.iit.demokritos.gr/."
    }, {
      "heading" : "6. Conclusions and Future Work",
      "text" : "We present online Bayesian Passive-Aggressive (BayesPA) learning as a new framework for max-margin Bayesian inference of online streaming data. We show that BayesPA subsumes the online PA, and more significantly, generalizes naturally to incorporate latent variables and to perform nonparametric Bayesian inference, therefore providing great flexibility for explorative analysis. Based on the ideas of BayesPA, we develop efficient online learning algorithms for max-margin topic models as well as their nonparametric extensions. Empirical experiments on several real datasets demonstrate significant improvements on time efficiency, while maintaining comparable results.\nAs future work, we are interested in showing provable bounds in its convergence and limitations. Furthermore, better understanding its mathematical structure would allow one to design more involved BayesPA algorithms for various models. We are also interested in developing highly scalable, distributed (Broderick et al., 2013) BayesPA learning paradigms, which will better meet the demand of processing massive real data available today."
    }, {
      "heading" : "Appendix A: Proof of Lemma 2.2",
      "text" : "In this section, we prove Lemma 2.2. We should note that our deviations below also provide insights for the developments of online BayesPA algorithms with the averaging classifiers.\nProof. We prove for the more generalized soft-margin version of BayesPA learning, which can be reformulated using a slack variable ξ:\nqt+1(w) = argmin q(w)∈P\nKL[q(w)||qt(w)] + cξ\ns.t. : ytEq[w>xt] ≥ − ξ, ξ ≥ 0. (24)\nSimilar to Corollary 5 in (Zhu et al., 2012), the optimal solution q∗(w) of the above problem can be derived from its functional Lagrangian and has the following form:\nq∗(w) = 1\nΓ(τ) qt(w) exp(τytw\n>xt) (25)\nwhere Γ(τ) is a normalization term and τ is the optimal solution to the dual problem:\nmax τ τ − log Γ(τ) s.t. 0 ≤ τ ≤ c\n(26)\nUsing this primal-dual interpretation, we first prove that for prior p0(w) = N (w0, I), qt(w) = N (µt, I) for some µt in each round t = 0, 1, 2, .... This can be shown by induction. Assume for round t, the distribution qt(w) = N (µt, I). Then for round t+ 1, the distribution by (25) is\nqt+1(w) = 1\nC · Γ(τ) exp\n( − 1\n2 ||w − (µt + τytxt)||2 ) (27)\nwhere C is some constant. Therefore, the distribution qt+1(w) = N (µt + τxt, I). As a by-product, the normalization term Γ(τ) = √ 2π exp(τytx > t µt + 1 2τ 2x>t xt).\nNext, we show that µt+1 = µt + τytxt is the optimal solution of the online Passive-Aggressive update rule (Crammer et al., 2006). To see this, we plug the derived Γ(τ) into (26), and obtain\nmax τ\nτ − 12τ 2x>t xt − τytµ>t xt\ns.t. 0 ≤ τ ≤ c (28)\nwhich is exactly the dual form of the online PassiveAggressive update rule:\nµ∗t+1 = arg min ||µ− µt||2 + cξ s.t. ytµ>xt ≥ − ξ, ξ ≥ 0,\n(29)\nthe optimal solution to which is µ∗t+1 = µt + τytxt. It is then clear that µt+1 = µ∗t+1."
    }, {
      "heading" : "Appendix B:",
      "text" : "We show the objective in (11) is an upper bound of that in (6), that is,\nL(q(w,Φ,Zt,λt))− Eq[log(ψ(Yt,λt|Zt,w))]\n≥ L(q(w,Φ,Zt)) + 2c ∑ d∈Bt Eq[(ξd)+] (30)\nwhere L(q) = KL[q||qt(w,Φ)q0(Zt)].\nProof. We first have\nL(q(w,Φ,Zt,λt)) = Eq[log q(λt | w,Φ,Zt)q(w,Φ,Zt)\nqt(w,Φ,Zt) ],\nand\nL(q(w,Φ,Zt)) = Eq[log q(w,Φ,Zt)\nqt(w,Φ,Zt) ]\nComparing these two equations and canceling out common factors, we know that in order for (30) to make sense, it suffices to prove\nH[q′]− Eq′ [log(ψ(Yt,λt|Zt,w)] ≥ 2c ∑ d∈Bt Eq′ [(ξd)+]\n(31) is uniformly true for any given (w,Φ,Zt), where H(·) is the entropy operator and q′ = q(λt | w,Φ,Zt). The inequality (31) can be reformulated as\nEq′ [log q′\nψ(Yt,λt|Zt,w) ] ≥ 2c ∑ d∈Bt Eq′ [(ξd)+] (32)\nExploiting the convexity of the function log(·), i.e.\n−Eq′ [log ψ(Yt,λt|Zt,w)\nq′ ] ≥ − log ∫ λt ψ(Yt,λt|Zt,w) dλt,\nand utilizing the equality (10), we then have (32) and therefore prove (30)."
    } ],
    "references" : [ {
      "title" : "Bayesian posterior sampling via stochastic gradient Fisher scoring",
      "author" : [ "S. Ahn", "A. Korattikara", "M. Welling" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Ahn et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Ahn et al\\.",
      "year" : 2012
    }, {
      "title" : "Mixtures of Dirichlet processes with applications to Bayesian nonparametric problems",
      "author" : [ "C.E. Antoniak" ],
      "venue" : "The annals of statistics,",
      "citeRegEx" : "Antoniak,? \\Q1974\\E",
      "shortCiteRegEx" : "Antoniak",
      "year" : 1974
    }, {
      "title" : "Supervised topic models",
      "author" : [ "D.M. Blei", "J.D. McAuliffe" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Blei and McAuliffe,? \\Q2010\\E",
      "shortCiteRegEx" : "Blei and McAuliffe",
      "year" : 2010
    }, {
      "title" : "Streaming variational Bayes",
      "author" : [ "T. Broderick", "N. Boyd", "A. Wibisono", "A.C. Wilson", "M.I. Jordan" ],
      "venue" : "arXiv preprint arXiv:1307.6769,",
      "citeRegEx" : "Broderick et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Broderick et al\\.",
      "year" : 2013
    }, {
      "title" : "LIBSVM: a library for support vector machines",
      "author" : [ "C.C. Chang", "Lin", "C.-J" ],
      "venue" : "ACM Transactions on Intelligent Systems and Technology (TIST),",
      "citeRegEx" : "Chang et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Chang et al\\.",
      "year" : 2011
    }, {
      "title" : "Online large-margin training of syntactic and structural translation features",
      "author" : [ "D. Chiang", "Y. Marton", "P. Resnik" ],
      "venue" : "In EMNLP,",
      "citeRegEx" : "Chiang et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Chiang et al\\.",
      "year" : 2008
    }, {
      "title" : "Non-Uniform Random Variate Generation",
      "author" : [ "L. Devroye" ],
      "venue" : null,
      "citeRegEx" : "Devroye,? \\Q1986\\E",
      "shortCiteRegEx" : "Devroye",
      "year" : 1986
    }, {
      "title" : "Infinite latent feature models and the Indian buffet process",
      "author" : [ "Z. Ghahramani", "T.L. Griffiths" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Ghahramani and Griffiths,? \\Q2005\\E",
      "shortCiteRegEx" : "Ghahramani and Griffiths",
      "year" : 2005
    }, {
      "title" : "Bayesian Nonparametrics",
      "author" : [ "N.L. Hjort" ],
      "venue" : null,
      "citeRegEx" : "Hjort,? \\Q2010\\E",
      "shortCiteRegEx" : "Hjort",
      "year" : 2010
    }, {
      "title" : "Online learning for latent Dirichlet allocation",
      "author" : [ "M. Hoffman", "F.R. Bach", "D.M. Blei" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Hoffman et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Hoffman et al\\.",
      "year" : 2010
    }, {
      "title" : "Maximum entropy discrimination",
      "author" : [ "T. Jaakkola", "M. Meila", "T. Jebara" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Jaakkola et al\\.,? \\Q1999\\E",
      "shortCiteRegEx" : "Jaakkola et al\\.",
      "year" : 1999
    }, {
      "title" : "Multitask sparsity via maximum entropy discrimination",
      "author" : [ "T. Jebara" ],
      "venue" : "JMLR, 12:75–110,",
      "citeRegEx" : "Jebara,? \\Q2011\\E",
      "shortCiteRegEx" : "Jebara",
      "year" : 2011
    }, {
      "title" : "Monte Carlo Methods for Maximum Margin Supervised Topic Models",
      "author" : [ "Q. Jiang", "J. Zhu", "M. Sun", "E.P. Xing" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Jiang et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Jiang et al\\.",
      "year" : 2012
    }, {
      "title" : "An Introduction to Variational Methods for Graphical Models",
      "author" : [ "M.I. Jordan", "Z. Ghahramani", "T.S. Jaakkola", "L.K. Saul" ],
      "venue" : null,
      "citeRegEx" : "Jordan et al\\.,? \\Q1998\\E",
      "shortCiteRegEx" : "Jordan et al\\.",
      "year" : 1998
    }, {
      "title" : "DiscLDA: Discriminative learning for dimensionality reduction and classification",
      "author" : [ "S. Lacoste-Julien", "F. Sha", "M.I. Jordan" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Lacoste.Julien et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Lacoste.Julien et al\\.",
      "year" : 2008
    }, {
      "title" : "Online largemargin training of dependency parsers",
      "author" : [ "R. McDonald", "K. Crammer", "F. Pereira" ],
      "venue" : "In ACL,",
      "citeRegEx" : "McDonald et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "McDonald et al\\.",
      "year" : 2005
    }, {
      "title" : "Generating random variates using transformations with multiple roots",
      "author" : [ "J.R. Michael", "W.R. Schucany", "R.W. Haas" ],
      "venue" : "The American Statistician,",
      "citeRegEx" : "Michael et al\\.,? \\Q1976\\E",
      "shortCiteRegEx" : "Michael et al\\.",
      "year" : 1976
    }, {
      "title" : "Sparse stochastic inference for latent dirichlet allocation",
      "author" : [ "D. Mimno", "M. Hoffman", "D.M. Blei" ],
      "venue" : null,
      "citeRegEx" : "Mimno et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Mimno et al\\.",
      "year" : 2012
    }, {
      "title" : "Hierarchical Dirichlet processes",
      "author" : [ "Y.W. Teh", "M.I. Jordan", "M.J. Beal", "D.M. Blei" ],
      "venue" : "Journal of the American Statistical Association,",
      "citeRegEx" : "Teh et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Teh et al\\.",
      "year" : 2006
    }, {
      "title" : "A collapsed variational Bayesian inference algorithm for latent Dirichlet allocation",
      "author" : [ "Y.W. Teh", "D. Newman", "M. Welling" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Teh et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Teh et al\\.",
      "year" : 2006
    }, {
      "title" : "Truncation-free online variational inference for Bayesian nonparametric models",
      "author" : [ "C. Wang", "D.M. Blei" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Wang and Blei,? \\Q2012\\E",
      "shortCiteRegEx" : "Wang and Blei",
      "year" : 2012
    }, {
      "title" : "Online variational inference for the hierarchical Dirichlet process",
      "author" : [ "C. Wang", "J.W. Paisley", "D.M. Blei" ],
      "venue" : "In AISTATS,",
      "citeRegEx" : "Wang et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2011
    }, {
      "title" : "Bayesian nonparametric maximum margin matrix factorization for collaborative prediction",
      "author" : [ "M. Xu", "J. Zhu", "B. Zhang" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Xu et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2012
    }, {
      "title" : "Maximum entropy discrimination",
      "author" : [ "J. Zhu", "E.P. Xing" ],
      "venue" : "Markov networks. JMLR,",
      "citeRegEx" : "Zhu and Xing,? \\Q2009\\E",
      "shortCiteRegEx" : "Zhu and Xing",
      "year" : 2009
    }, {
      "title" : "Infinite SVM: a Dirichlet process mixture of large-margin kernel machines",
      "author" : [ "J. Zhu", "N. Chen", "E.P. Xing" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Zhu et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Zhu et al\\.",
      "year" : 2011
    }, {
      "title" : "MedLDA: maximum margin supervised topic models",
      "author" : [ "J. Zhu", "A. Ahmed", "E.P. Xing" ],
      "venue" : "JMLR, 13:2237–2278,",
      "citeRegEx" : "Zhu et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Zhu et al\\.",
      "year" : 2012
    }, {
      "title" : "Gibbs max-margin topic models with fast sampling algorithms",
      "author" : [ "J. Zhu", "N. Chen", "H. Perkins", "B. Zhang" ],
      "venue" : null,
      "citeRegEx" : "Zhu et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Zhu et al\\.",
      "year" : 2013
    }, {
      "title" : "Scalable inference in max-margin topic models",
      "author" : [ "J. Zhu", "X. Zheng", "L. Zhou", "B. Zhang" ],
      "venue" : "In SIGKDD,",
      "citeRegEx" : "Zhu et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Zhu et al\\.",
      "year" : 2013
    } ],
    "referenceMentions" : [ {
      "referenceID" : 15,
      "context" : ", 2006) provides a generic framework for online large-margin learning, with many applications (McDonald et al., 2005; Chiang et al., 2008).",
      "startOffset" : 94,
      "endOffset" : 138
    }, {
      "referenceID" : 5,
      "context" : ", 2006) provides a generic framework for online large-margin learning, with many applications (McDonald et al., 2005; Chiang et al., 2008).",
      "startOffset" : 94,
      "endOffset" : 138
    }, {
      "referenceID" : 8,
      "context" : "Moreover, the recent progress on nonparametric Bayesian methods (Hjort, 2010; Teh et al., 2006a) further provides an increasingly important framework that allows the Bayesian models to have an unbounded model complexity, e.",
      "startOffset" : 64,
      "endOffset" : 96
    }, {
      "referenceID" : 8,
      "context" : ", an infinite number of components in a mixture model (Hjort, 2010) or an infinite number of units in a latent feature model (Ghahramani & Griffiths, 2005), and to adapt when the learning environment changes.",
      "startOffset" : 54,
      "endOffset" : 67
    }, {
      "referenceID" : 9,
      "context" : "To scale up Bayesian inference, much progress has been made on developing online variational Bayes (Hoffman et al., 2010; Mimno et al., 2012) and online Monte Carlo (Ahn et al.",
      "startOffset" : 99,
      "endOffset" : 141
    }, {
      "referenceID" : 17,
      "context" : "To scale up Bayesian inference, much progress has been made on developing online variational Bayes (Hoffman et al., 2010; Mimno et al., 2012) and online Monte Carlo (Ahn et al.",
      "startOffset" : 99,
      "endOffset" : 141
    }, {
      "referenceID" : 0,
      "context" : ", 2012) and online Monte Carlo (Ahn et al., 2012) methods.",
      "startOffset" : 31,
      "endOffset" : 49
    }, {
      "referenceID" : 10,
      "context" : "For example, maximum entropy discrimination (MED) (Jaakkola et al., 1999) made a significant advance in conjoining maxmargin learning and Bayesian generative models, mainly in the context of supervised learning and structured output prediction (Zhu & Xing, 2009).",
      "startOffset" : 50,
      "endOffset" : 73
    }, {
      "referenceID" : 25,
      "context" : "Recently, much attention has been focused on generalizing MED to incorporate latent variables and perform nonparametric Bayesian inference, in many contexts including topic modeling (Zhu et al., 2012), matrix factorization (Xu et al.",
      "startOffset" : 182,
      "endOffset" : 200
    }, {
      "referenceID" : 22,
      "context" : ", 2012), matrix factorization (Xu et al., 2012), and multi-task learning (Jebara, 2011; Zhu et al.",
      "startOffset" : 30,
      "endOffset" : 47
    }, {
      "referenceID" : 11,
      "context" : ", 2012), and multi-task learning (Jebara, 2011; Zhu et al., 2011).",
      "startOffset" : 33,
      "endOffset" : 65
    }, {
      "referenceID" : 24,
      "context" : ", 2012), and multi-task learning (Jebara, 2011; Zhu et al., 2011).",
      "startOffset" : 33,
      "endOffset" : 65
    }, {
      "referenceID" : 25,
      "context" : "As concrete examples, we apply the theory of online BayesPA to topic modeling and derive efficient online learning algorithms for max-margin supervised topic models (Zhu et al., 2012).",
      "startOffset" : 165,
      "endOffset" : 183
    }, {
      "referenceID" : 21,
      "context" : "We further develop efficient online learning algorithms for the nonparametric max-margin topic models, an extension of the nonparametric topic models (Teh et al., 2006a; Wang et al., 2011) for predictive tasks.",
      "startOffset" : 150,
      "endOffset" : 188
    }, {
      "referenceID" : 25,
      "context" : "Although in some cases we can marginalize out the local variables Ht, in general we would not obtain a closed-form posterior distribution qt+1(w,M) for the next optimization round, especially in dealing with some involved models like MedLDA (Zhu et al., 2012).",
      "startOffset" : 241,
      "endOffset" : 259
    }, {
      "referenceID" : 25,
      "context" : "For supervised topic models (Blei & McAuliffe, 2010; Zhu et al., 2012), such a regularization term could be a loss function of a prediction model w on the data X = {xd}d=1 and response signals Y = {yd}d=1.",
      "startOffset" : 28,
      "endOffset" : 70
    }, {
      "referenceID" : 12,
      "context" : "As a regularized Bayesian (RegBayes) model (Jiang et al., 2012), MedLDA infers a distribution of the latent variables Z as well as classification weights w by solving the problem:",
      "startOffset" : 43,
      "endOffset" : 63
    }, {
      "referenceID" : 13,
      "context" : "With the mild mean-field assumption that q(w,Φ,Zt,λt) = q(w)q(Φ)q(Zt,λt), we can solve (11) via an iterative procedure that alternately updates each factor distribution (Jordan et al., 1998), as detailed below.",
      "startOffset" : 169,
      "endOffset" : 190
    }, {
      "referenceID" : 17,
      "context" : "This hybrid strategy has shown promising performance for LDA (Mimno et al., 2012).",
      "startOffset" : 61,
      "endOffset" : 81
    }, {
      "referenceID" : 6,
      "context" : "a generalized inverse gaussian distribution (Devroye, 1986).",
      "startOffset" : 44,
      "endOffset" : 59
    }, {
      "referenceID" : 16,
      "context" : "Therefore, λ−1 d follows an inverse gaussian distribution IG(λ−1 d ; 1 c √ ζ̄2 d+z̄ > d Σ ∗z̄d , 1), from which we can draw a sample in constant time (Michael et al., 1976).",
      "startOffset" : 150,
      "endOffset" : 172
    }, {
      "referenceID" : 1,
      "context" : "where S(a, b) are unsigned Stirling numbers of the first kind (Antoniak, 1974).",
      "startOffset" : 62,
      "endOffset" : 78
    }, {
      "referenceID" : 25,
      "context" : "Following the same setting in (Zhu et al., 2012), we remove a standard list of stop words.",
      "startOffset" : 30,
      "endOffset" : 48
    }, {
      "referenceID" : 17,
      "context" : "We also consider online unsupervised topic models as baselines, including sparse inference for LDA (spLDA) (Mimno et al., 2012), which has been demonstrated to be superior than online variational LDA (Hoffman et al.",
      "startOffset" : 107,
      "endOffset" : 127
    }, {
      "referenceID" : 9,
      "context" : ", 2012), which has been demonstrated to be superior than online variational LDA (Hoffman et al., 2010) in performance, and truncation-free online variational HDP (tfHDP) (Wang & Blei, 2012), which",
      "startOffset" : 80,
      "endOffset" : 102
    }, {
      "referenceID" : 14,
      "context" : "The performances of other batch supervised topic models, such as sLDA (Blei & McAuliffe, 2010) and DiscLDA (Lacoste-Julien et al., 2008), are reported in (Zhu et al.",
      "startOffset" : 107,
      "endOffset" : 136
    }, {
      "referenceID" : 25,
      "context" : ", 2008), are reported in (Zhu et al., 2012).",
      "startOffset" : 25,
      "endOffset" : 43
    } ],
    "year" : 2013,
    "abstractText" : "Online Passive-Aggressive (PA) learning is an effective framework for performing max-margin online learning. But the deterministic formulation and estimated single large-margin model could limit its capability in discovering descriptive structures underlying complex data. This paper presents online Bayesian Passive-Aggressive (BayesPA) learning, which subsumes the online PA and extends naturally to incorporate latent variables and perform nonparametric Bayesian inference, thus providing great flexibility for explorative analysis. We apply BayesPA to topic modeling and derive efficient online learning algorithms for max-margin topic models. We further develop nonparametric methods to resolve the number of topics. Experimental results on real datasets show that our approaches significantly improve time efficiency while maintaining comparable results with the batch counterparts.",
    "creator" : "LaTeX with hyperref package"
  }
}