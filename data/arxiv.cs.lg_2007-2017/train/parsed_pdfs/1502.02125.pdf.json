{
  "name" : "1502.02125.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Contextual Online Learning for Multimedia Content Aggregation",
    "authors" : [ "Cem Tekin", "Mihaela van der Schaar" ],
    "emails" : [ "pubs-permissions@ieee.org.", "cmtkn@ucla.edu,", "mihaela@ee.ucla.edu." ],
    "sections" : [ {
      "heading" : null,
      "text" : "Index Terms—Social multimedia, distributed online learning, content aggregation, multi-armed bandits.\nI. INTRODUCTION\nA plethora of multimedia applications (web-based TV [2], [3], personalized video retrieval [4], personalized news aggregation [5], etc.) are emerging which require matching multimedia content generated by distributed sources with consumers exhibiting different interests. The matching is often performed by CAs (e.g., Dailymotion, Metacafe [6]) that are responsible for mining the content of numerous multimedia sources in search of finding content which is interesting for the users. Both the characteristics of the content and preference of the consumers are evolving over time. An example of the system with users, CAs and multimedia sources is given in Fig. 1.\nEach user is characterized by its context, which is a realvalued vector, that provides information about the users’\nCopyright (c) 2015 IEEE. Personal use of this material is permitted. However, permission to use this material for any other purposes must be obtained from the IEEE by sending a request to pubs-permissions@ieee.org.\nThis work is partially supported by the grants NSF CNS 1016081 and AFOSR DDDAS.\nC. Tekin and Mihaela van der Schaar are in Department of Electrical Engineering, UCLA, Los Angeles, CA, 90095. Email: cmtkn@ucla.edu, mihaela@ee.ucla.edu.\nThis online technical report is an extended version of the paper that appeared in IEEE Transactions on Multimedia [1].\ncontent preferences. We assume a model where users arrive sequentially to a CA, and based on the type (context) of the user, the CA requests content from either one of the multimedia sources that it is connected to or from another CA that it is connected to. The context can represent information such as age, gender, search query, previously consumed content, etc. It may also represent the type of the device that the user is using [7] (e.g., PDA, PC, mobile phone). The CA’s role is to match its user with the most suitable content, which can be accomplished by requesting content from the most suitable multimedia source.1 Since both the content generated by the multimedia sources and the user’s characteristics change over time, it is unknown to the CA which multimedia source to match with the user. This problem can be formulated as an online learning problem, where the CA learns the best matching by exploring matchings of users with different content providers. After a particular content matching is made, the user “consumes” the content, and provides feedback/rating, such as like or dislike.2 It is this feedback that helps a CA learn the preferences of its users and the characteristics of the content that is provided by the multimedia sources. Since this\n1Although we use the term request to explain how content from a multimedia source is mined, our proposed method works also when a CA extracts the content from the multimedia source, without any decision making performed by the multimedia source.\n2Our framework also works when the feedback is missing for some users.\nar X\niv :1\n50 2.\n02 12\n5v 2\n[ cs\n.M M\n] 2\n4 M\nar 2\n01 5\n2 is a learning problem we equivalently call a CA, a content learner or simply, a learner.\nTwo possible real-world applications of content aggregation are business news aggregation and music aggregation. Business news aggregators can collect information from a variety of multinational and multilingual sources and make personalized recommendations to specific individuals/companies based on their unique needs (see e.g. [8]). Music aggregators enable matching listeners with music content they enjoy both within the content network of the listeners as well as outside this network. For instance, distributed music aggregators can facilitate the sharing of music collections owned by diverse users without the need for centralized content manager/moderator/providers (see e.g. [9]). A discussion of how these applications can be modeled using our framework is given in Section III. Moreover, our proposed methods are tested on real-world datasets related to news aggregation and music aggregation in Section VII.\nFor each CA i, there are two types of users: direct and indirect. Direct users are the users that visit the website of CA i to search for content. Indirect users are the users of another CA that requests content from CA i. A CA’s goal is to maximize the number of likes received from its users (both direct and indirect). This objective can be achieved by all CAs by the following distributed learning method: all CAs learn online which matching action to take for its current user, i.e., obtain content from a multimedia source that is directly connected, or request content from another CA. However, it is not trivial how to use the past information collected by the CAs in an efficient way, due to the vast number of contexts (different user types) and dynamically changing user and content characteristics. For instance, a certain type of content may become popular among users at a certain point in time, which will require the CA to obtain content from the multimedia source that generates that type of content.\nTo jointly optimize the performance of the multimedia content aggregation system, we propose an online learning methodology that builds on contextual bandits [10], [11]. The performance of the proposed methodology is evaluated using the notion of regret: the difference between the expected total reward (number of content likes minus costs of obtaining the content) of the best content matching strategy given complete knowledge about the user preferences and content characteristics and the expected total reward of the algorithm used by the CAs. When the user preferences and content characteristics are static, our proposed algorithms achieve sublinear regret in the number of users that have arrived to the system.3 When the user preferences and content characteristics are slowly changing over time, our proposed algorithms achieve timeaveraged regret, where > 0 depends on the rate of change of the user and content characteristics.\nThe remainder of the paper is organized as follows. In Section II, we describe the related work and highlight the differences from our work. In Section III, we describe the decentralized content aggregation problem, the optimal content\n3We use index t to denote the number of users that have arrived so far. We also call t the time index, and assume that one user arrives at each time step.\nmatching scheme given the complete system model, and the regret of a learning algorithm with respect to the optimal content matching scheme. Then, we consider the model with unknown, static user preferences and content characteristics and propose a distributed online learning algorithm in Section IV. The analysis of the unknown, dynamic user preferences and content characteristics are given in Section VI. Using realworld datasets, we provide numerical results on the performance of our distributed online learning algorithms in Section VII. Finally, the concluding remarks are given in Section VIII."
    }, {
      "heading" : "II. RELATED WORK",
      "text" : "Related work can be categorized into two: related work on recommender systems and related work on online learning methods called multi-armed bandits."
    }, {
      "heading" : "A. Related work on recommender systems and content matching",
      "text" : "A recommender system recommends items to its users based on the characteristics of the users and the items. The goal of a recommender system is to learn which users like which items, and recommend items such that the number of likes is maximized. For instance, in [5], [12] a recommender system that learns the preferences of its users in an online way based on the ratings submitted by the users is provided. It is assumed that the true relevance score of an item for a user is a linear function of the context of the user and the features of the item. Under this assumption, an online learning algorithm is proposed. In contrast, we consider a different model, where the relevance score need not be linear in the context. Moreover, due to the distributed nature of the problem we consider, our online learning algorithms need an additional phase called the training phase, which accounts for the fact that the CAs are uncertain about the information of the other aggregators that they are linked with. We focus on the long run performance and show that the regret per unit time approaches zero when the user and content characteristics are static. An online learning algorithm for a centralized recommender which updates its recommendations as both the preferences of the users and the characteristics of items change over time is proposed in [13].\nThe general framework which exploits the similarities between the past users and the current user to recommend content to the current user is called collaborative filtering [14]–[16]. These methods find the similarities between the current user and the past users by examining their search and feedback patterns, and then based on the interactions with the past similar users, matches the user with the content that has the highest estimated relevance score. For example, the most relevant content can be the content that is liked the highest number of times by similar users. Groups of similar users can be created by various methods such as clustering [15], and then, the matching will be made based on the content matched with the past users that are in the same group.\nThe most striking difference between our content matching system and previously proposed is that in prior works, there is a central CA which knows the entire set of different types of\n3 Our work [5], [12] [15] [14] [19] Distributed Yes No No No No\nReward model Hölder Linear N/A N/A N/A Confidence bounds Yes No No No No\nRegret bound Yes Yes No No No Dynamic user Yes No Yes Yes Yes\n/content distribution\nTABLE I COMPARISON OF OUR WORK WITH OTHER WORK IN RECOMMENDER SYSTEMS\ncontent, and all the users arrive to this central CA. In contrast, we consider a decentralized system consisting of many CAs, many multimedia sources that these CAs are connected to, and heterogeneous user arrivals to these CAs. These CAs are cooperating with each other by only knowing the connections with their own neighbors but not the entire network topology. Hence, a CA does not know which multimedia sources another CA is connected to, but it learns over time whether that CA has access to content that the users like or not. Thus, our model can be viewed as a giant collection of individual CAs that are running in parallel.\nAnother line of work [17], [18] uses social streams mined in one domain, e.g., Twitter, to build a topic space that relates these streams to content in the multimedia domain. For example, in [17], Tweet streams are used to provide video recommendations in a commercial video search engine. A content adaptation method is proposed in [7] which enables the users with different types of contexts and devices to receive content that is in a suitable format to be accessed. Video popularity prediction is studied in [18], where the goal is to predict if a video will become popular in the multimedia domain, by detecting social trends in another social media domain (such as Twitter), and transferring this knowledge to the multimedia domain. Although these methods are very different from our methods, the idea of transferring knowledge from one multimedia domain to another can be carried out by CAs specialized in specific types of cross-domain content matching For instance, one CA may transfer knowledge from tweets to predict the content which will have a high relevance/popularity for a user with a particular context, while another CA may scan through the Facebook posts of the user’s friends to calculate the context of the domain in addition to the context of the user, and provide a matching according to this.\nThe advantages of our proposed approach over prior work in recommender systems are: (i) systematic analysis of recommendations’ performance, including confidence bounds on the accuracy of the recommendations; (ii) no need for a priori knowledge of the users’ preferences (i.e., system learns onthe-fly); (iii) achieve high accuracy even when the users’ characteristics and content characteristics are changing over time; (iv) all these features are enabled in a network of distributed CAs.\nThe differences of our work from the prior work in recommender systems is summarized in Table I."
    }, {
      "heading" : "B. Related Work on Multi-armed Bandits",
      "text" : "Other than distributed content recommendation, our learning framework can be applied to any problem that can be formulated as a decentralized contextual bandit problem. Contextual\nbandits have been studied before in [10], [11], [20]–[22] in a single agent setting, where the agent sequentially chooses from a set of alternatives with unknown rewards, and the rewards depend on the context information provided to the agent at each time step. In [5], a contextual bandit algorithm named LinUCB is proposed for recommending personalized news articles, which is variant of the UCB algorithm [23] designed for linear payoffs. Numerical results on real-world Internet data are provided, but no theoretical results on the resulting regret are derived. The main difference of our work from single agent contextual bandits is that: (i) a three phase learning algorithm with training, exploration and exploitation phases is needed instead of the standard two phase, i.e., exploration and exploitation phases, algorithms used in centralized contextual bandit problems; (ii) the adaptive partitions of the context space should be formed in a way that each learner/aggregator can efficiently utilize what is learned by other learners about the same context; (iii) the algorithm is robust to missing feedback (some users do not rate the content)."
    }, {
      "heading" : "III. PROBLEM FORMULATION",
      "text" : "The system model is shown in Fig. 1. There are M content aggregators (CAs) which are indexed by the set M := {1, 2, . . . ,M}. We also call each CA a learner since it needs to learn which type of content to provide to its users. Let M−i :=M−{i} be the set of CAs that CA i can choose from to request content. Each CA has access to the contents over its content network as shown in Fig. 1. The set of contents in CA i’s content network is denoted by Ci. The set of all contents is denoted by C := ∪i∈MCi. The system works in a discrete time setting t = 1, 2, . . . , T , where the following events happen sequentially, in each time slot: (i) a user with context xi(t) arrives to each CA i ∈ M,4 (ii) based on the context of its user each CA matches its user with a content (either from its own content network or by requesting content from another CA), (iii) the user provides a feedback, denoted by yi(t), which is either like (yi(t) = 1) or dislike (yi(t) = 0).\nThe set of content matching actions of CA i is denoted by Ki := Ci ∪M−i. Let X = [0, 1]d be the context space,5 where d is the dimension of the context space. The context can include many properties of the user such as age, gender, income, previously liked content, etc. We assume that all these quantities are mapped into [0, 1]d. For instance, this mapping can be established by feature extraction methods such as the one given in [5]. Another method is to represent each property of a user by a real number between [0, 1] (e.g., normalize the age by a maximum possible age, represent gender by set {0, 1}, etc.), without feature extraction. The feedback set of a user is denoted by Y := {0, 1}. Let Cmax := maxi∈M |Ci|. We assume that all CAs know Cmax but they do not need to know the content networks of other CAs.\nThe following two examples demonstrate how business news aggregation and music aggregation fits our problem formulation.\n4Although in this model user arrivals are synchronous, our framework will work for asynchronous user arrivals as well.\n5In general, our results will hold for any bounded subspace of Rn.\n4 Example 1: Business news aggregation. Consider a distributed set of news aggregators that operate in different countries (for instance a European news aggregator network as in [8]). Each news aggregator’s content network (as portrayed in Fig. 1 of the manuscript) consists of content producers (multimedia sources) that are located in specific regions/countries. Consider a user with context x (e.g. age, gender, nationality, profession) who subscribes to the CA A, which is located in the country where the user lives. This CA has access to content from local producers in that country but it can also request content from other CAs, located in different countries. Hence, a CA has access to (local) content generated in other countries. In such scenarios, our proposed system is able to recommend to the user subscribing to CA A also content from other CAs, by discovering the content that is most relevant to that user (based on its context x) across the entire network of CAs. For instance, for a user doing business in the transportation industry, our content aggregator system may learn to recommend road construction news, accidents or gas prices from particular regions that are on the route of the transportation network of the user.\nExample 2: Music aggregation. Consider a distributed set of music aggregators that are specialized in specific genres of music: classical, jazz, rock, rap, etc. Our proposed model allows music aggregators to share content to provide personalized recommendation for a specific user. For instance, a user that subscribes (frequents/listens) to the classical music aggregator may also like specific jazz tracks. Our proposed system is able to discover and recommend to that user also other music that it will enjoy in addition to the music available to/owned by in aggregator to which it subscribes."
    }, {
      "heading" : "A. User and Content Characteristics",
      "text" : "In this paper we consider two types of user and content characteristics. First, we consider the case when the user and content characteristics are static, i.e., they do not change over time. For this case, for a user with context x, πc(x) denotes the probability that the user will like content c. We call this the relevance score of content c.\nThe second case we consider corresponds to the scenario when the characteristics of the users and content are dynamic. For online multimedia content, especially for social media, it is known that both the user and the content characteristics are dynamic and noisy [24], hence the problem exhibits concept drift [25]. Formally, a concept is the distribution of the problem, i.e., the joint distribution of the user and content characteristics, at a certain point of time [26]. Concept drift is a change in this distribution. For the case with concept drift, we propose a learning algorithm that takes into account the speed of the drift to decide what window of past observations to use in estimating the relevance score. The proposed learning algorithm has theoretical performance guarantees in contrast to prior work on concept drift which mainly deal with the problem in a ad-hoc manner. Indeed, it is customary to assume that online content is highly dynamic. A certain type of content may become popular for a certain period of time, and then, its popularity may decrease over time and a new content may emerge as popular. In addition, although the type of the content\nremains the same, such as soccer news, its popularity may change over time due to exogenous events such as the World Cup etc. Similarly, a certain type of content may become popular for a certain type of demographics (e.g., users of a particular age, gender, profession, etc.). However, over time the interest of these users may shift to other types of content. In such cases, where the popularity of content changes over time for a user with context x, πc(x, t) denotes the probability that the user at time t will like content c.\nAs we stated earlier, a CA i can either recommend content from multimedia sources that it is directly connected to or can ask another CA for content. By asking for content c from another CA j, CA i will incur cost dij ≥ 0. For the purpose of our paper, the cost is a generic term. For instance, it can be a payment made to CA j to display it to CA i’s user, or it may be associated with the advertising loss CA i incurs by directing its user to CA j’s website. When the cost is payment, it can be money, tokens [27] or Bitcoins [28]. Since this cost is bounded, without loss of generality we assume that dij ∈ [0, 1] for all i, j ∈ M. In order make our model general, we also assume that there is a cost associated with recommending a type of content c ∈ Ci, which is given by dic ∈ [0, 1], for CA i. For instance, this can be a payment made to the multimedia source that owns content c.\nAn intrinsic assumption we make is that the CAs are cooperative. That is, CA j ∈ M−i will return the content that is mostly to be liked by CA i’s user when asked by CA i to recommend a content. This cooperative structure can be justified as follows. Whenever a user likes the content of CA j (either its own user or user of another CA), CA j obtains a benefit. This can be either an additional payment made by CA i when the content recommended by CA j is liked by CA i’s user, or it can simply be the case that whenever a content of CA j is liked by someone its popularity increases. However, we assume that the CAs’ decisions do not change their pool of users. The future user arrivals to the CAs are independent of their past content matching strategies. For instance, users of a CA may have monthly or yearly subscriptions, so they will not shift from one CA to another CA when they like the content of the other CA.\nThe goal of CA i is to explore the matching actions in Ki to learn the best content for each context, while at the same time exploiting the best content for the user with context xi(t) arriving at each time instance t to maximize its total number of likes minus costs. CA i’s problem can be modeled as a contextual bandit problem [10], [21], [22], [29], where likes and costs translate into rewards. In the next subsection, we formally define the benchmark solution which is computed using perfect knowledge about the probability that a content c will be liked by a user with context x (which requires complete knowledge of user and content characteristics). Then, we define the regret which is the performance loss due to uncertainty about the user and content characteristics."
    }, {
      "heading" : "B. Optimal Content Matching with Complete Information",
      "text" : "Our benchmark when evaluating the performance of the learning algorithms is the optimal solution which always recommends the content with the highest relevance score minus\ncost for CA i from the set C given context xi(t) at time t. This corresponds to selecting the best matching action in Ki given xi(t). Next, we define the expected rewards of the matching actions, and the action selection policy of the benchmark. For a matching action k ∈ M−i, its relevance score is given as πk(x) := πc∗k(x)(x), where c ∗ k(x) := arg maxc∈Cj πc(x). For a matching action k ∈ Ci its relevance score is equal to the relevance score of content k. The expected reward of CA i from choosing action k ∈ Ki is given by the quasilinear utility function\nµik(x) := πk(x)− dik (1)\nwhere dik ∈ [0, 1] is the normalized cost of choosing action k for CA i. Our proposed system will also work for more general expected reward functions as long as the expected reward of a learner is a function of the relevance score of the chosen action and the cost (payment, communication cost, etc.) associated with choosing that action. The oracle benchmark is given by\nk∗i (x) := arg max k∈Ki µik(x) ∀x ∈ X . (2)\nThe oracle benchmark depends on relevance scores as well as costs of matching content from its own content network or other CA’s content network. The case dik = 0 for all k ∈ Ki and i ∈ M, corresponds to the scheme in which content matching has zero cost, hence k∗i (x) = arg maxk∈Ki πk(x) = arg maxc∈C πc(x). This corresponds to the best centralized solution, where CAs act as a single entity. On the other hand, when dik ≥ 1 for all k ∈ M−i and i ∈ M, in the oracle benchmark a CA must not cooperate with any other CA and should only use its own content. Hence k∗i (x) = arg maxc∈Ci(πc(x) − d i c). In the following subsections, we will show that independent of the values of relevance scores and costs, our algorithms will achieve sublinear regret (in the number of users or equivalently time) with respect to the oracle benchmark."
    }, {
      "heading" : "C. The Regret of Learning",
      "text" : "In this subsection we define the regret as a performance measure of the learning algorithm used by the CAs. Simply, the regret is the loss incurred due to the unknown system dynamics. Regret of a learning algorithm which selects the matching action/arm ai(t) at time t for CA i is defined with respect to the best matching action k∗i (x) given in (2). Then, the regret of CA i at time T is\nRi(T ) := T∑ t=1 ( πk∗i (xi(t))(xi(t))− d i k∗i (xi(t)) ) − E\n[ T∑ t=1 ( I(yi(t) = L)− diai(t) )] . (3)\nRegret gives the convergence rate of the total expected reward of the learning algorithm to the value of the optimal solution given in (2). Any algorithm whose regret is sublinear, i.e., Ri(T ) = O(T\nγ) such that γ < 1, will converge to the optimal solution in terms of the average reward.\nA summary of notations is given in Table II. In the next section, we propose an online learning algorithm which achieves\nM: Set of all CAs Ci: Contents in the Content Network of CA i Cmax: maxi∈M |Ci| C: Set of all contents X = [0, 1]d: Context space Y: Set of feedbacks a user can give xi(t): d-dimensional context of tth user of CA i yi(t): Feedback of the tth user of CA i Ki: Set of content matching actions of CA i πc(x): Relevance score of content c for context x dik: Cost of choosing matching action k for CA i µik(x): Expected reward (static) of CA i from matching action k for context x k∗i (x): Optimal matching action of CA i given context x (oracle benchmark) Ri(T ): Regret of CA i at time T βa := ∑∞ t=1 1/t a\nsublinear regret when the user and content characteristics are static."
    }, {
      "heading" : "IV. A DISTRIBUTED ONLINE CONTENT MATCHING ALGORITHM",
      "text" : "In this section we propose an online learning algorithm for content matching when the user and content characteristics are static. In contrast to prior online learning algorithms that exploit the context information [10], [11], [20]–[22], [29], which consider a single learner setting, the proposed algorithm helps a CA to learn from the experience of other CAs. With this mechanism, a CA is able to recommend content from multimedia sources that it has no direct connection, without needing to know the IDs of such multimedia sources and their content. It learns about these multimedia sources only through the other CAs that it is connected to.\nIn order to bound the regret of this algorithm analytically we use the following assumption. When the content characteristics are static, we assume that each type of content has similar relevance scores for similar contexts; we formalize this in terms of a Lipschitz condition.\nAssumption 1: There exists L > 0, γ > 0 such that for all x, x′ ∈ X and c ∈ C, we have |πc(x)−πc(x′)| ≤ L||x−x′||γ .\nAssumption 1 indicates that the probability that a type c content is liked by users with similar contexts will be similar to each other. For instance, if two users have similar age, gender, etc., then it is more likely that they like the same content. We call L the similarity constant and γ the similarity exponent. These parameters will depend on the characteristics of the users and the content. We assume that γ is known by the CAs. However, an unknown γ can be estimated online using the history of likes and dislikes by users with different contexts, and our proposed algorithms can be modified to include the estimation of γ.\nIn view of this assumption, the important question becomes how to learn from the past experience which content to match with the current user. We answer this question by proposing an algorithm which partitions the context space of a CA, and learns the relevance scores of different types of content for each set in the partition, based only on the past experience in that set. The algorithm is designed in a way to\n6\nachieve optimal tradeoff between the size of the partition and the past observations that can be used together to learn the relevance scores. It also includes a mechanism to help CAs learn from each other’s users. We call our proposed algorithm the DIStributed COntent Matching algorithm (DISCOM), and its pseudocode is given in Fig. 4, Fig. 5 and Fig. 6.\nEach CA i has two tasks: matching content with its own users and matching content with the users of other CAs when requested by those CAs. We call the first task the maximization task (implemented by DISCOMmax given in Fig. 5), since the goal of CA i is to maximize the number of likes from its own users. The second task is called the cooperation task (implemented by DISCOMcoop given in Fig. 6), since the goal of CA i is to help other CAs obtain content from its own content network in order to maximize the likes they receive from their users. This cooperation is beneficial to CA i because of numerous reasons. Firstly, since every CA cooperates, CA i can reach a much larger set of content including the content from other CA’s content networks, hence will be able to provide content with higher relevance score to its users. Secondly, when CA i helps CA j, it will observe the feedback of CA j’s user for the matched content, hence will be able to update the estimated relevance score of its content, which is beneficial if a user similar to CA j’s user arrives to CA i in the future. Thirdly, payment mechanisms can be incorporated to the system such that CA i gets a payment from CA j when its content is liked by CA j’s user.\nIn summary, there are two types of content matching actions for a user of CA i. In the first type, the content is recommended from a source that is directly connected to CA i, while in the second type, the content is recommended from a source that CA i is connected through another CA. The information exchange between multimedia sources and CAs for these two types of actions is shown in Fig. 2 and Fig. 3.\nLet T be the time horizon of interest (equivalent to the number of users that arrive to each CA). DISCOM creates a partition of X = [0, 1]d based on T . For instance T can be\nthe average number of visits to the CA’s website in one day. Although in reality the average number of visits to different CAs can be different, our analysis of the regret in this section will hold since it is the worst-case analysis (assuming that users arrive only to CA i, while the other CAs only learn through CA i’s users). Moreover, the case of heterogeneous number of visits can be easily addressed if each CA informs other CAs about its average number of visits. Then, CA i can keep M different partitions of the context space; one for itself and M −1 for the other CAs. If called by CA j, it will match a content to CA j’s user based on the partition it keeps for CA j. Hence, we focus on the case when T is common to all CAs.\nWe first define mT as the slicing level used by DISCOM, which is an integer that is used to partition X . DISCOM forms a partition of X consisting of (mT )d sets (hypercubes) where each set is a d-dimensional hypercube with edge length 1/mT . This partition is denoted by PT . The hypercubes in PT are oriented such that one of them has a corner located at the origin of the d-dimensional Euclidian space. It is clear that the number of hypercubes is increasing in mT , while their size is decreasing in mT . When mT is small each hypercube covers a large set of contexts, hence the number of past observations that can be used to estimate relevance scores of matching actions in each set is large. However, the variation of the true relevance scores of the contexts within a hypercube increases with the size of the hypercube. DISCOM should set mT to a value that balances this tradeoff.\nA hypercube in PT is denoted by p. The hypercube in PT that contains context xi(t) is denoted by pi(t). When xi(t) is located at a boundary of multiple hypercubes in PT , it is randomly assigned to one of these hypercubes.\nDISCOMmax operates as follows. CA i matches its user at time t with a content by taking a matching action based on one of the three phases: training phase in which CA i requests content from another CA j for the purpose of helping CA j to learn the relevance score of content in its content network for users with context xi(t) (but CA i does not update the relevance score for CA j because it thinks that CA j may not know much about its own content), exploration phase in which CA i selects a matching action in Ki and updates its relevance score based on the feedback of its user, and exploitation phase in which CA i chooses the matching action with the highest relevance score minus cost.\nSince the CAs are cooperative, when another CA requests content from CA i, CA i will choose content from its content network with the highest estimated relevance score for the user of the requesting CA. To maximize the number of likes minus costs in exploitations, CA i must have an accurate estimate of the relevance scores of other CAs. This task is not trivial since CA i does not know the content network of other CAs. In order to do this, CA i should smartly select which of its users’ feedbacks to use when estimating the relevance score of CA j. The feedbacks should come from previous times at which CA i has a very high confidence that the content of CA j matched with its user is the one with the highest relevance score for the context of CA i’s user. Thus, the training phase of CA i helps other CAs build accurate estimates about the relevance scores\n7 DISCOM for CA i: 1: Input: H1(t), H2(t), H3(t), T , mT 2: Initialize: Partition X into hypercubes denoted by PT 3: Initialize: Set counters N ip = 0, ∀p ∈ PT , N ik,p = 0, ∀k ∈ Ki, p ∈ PT , N tr,ij,p = 0,∀j ∈M−i, p ∈ PT\n4: Initialize: Set relevance score estimates r̄ik,p = 0, ∀k ∈ Ki, p ∈ PT 5: while t ≥ 1 do 6: Run DISCOMmax to find p = pi(t), to obtain a matching action ai, and value of train flag 7: If ai ∈M−i ask CA ai for content and pass xi(t) 8: Receive CAi(t), the set of CAs who requested content from CA i, and their contexts 9: if CAi(t) 6= ∅ then 10: Run DISCOMcoop to obtain the content to be selected bi := {bi,j}j∈CAi(t) and hypercubes that the contexts of the users in CAi(t) lie in pi := {pi,j}j∈CAi(t) 11: end if 12: if ai ∈ Ci then 13: Pay cost diai , obtain content ai 14: Show ai to the user, receive feedback r ∈ {0, 1}\ndrawn from Berai(xi(t)) a\n15: else 16: Pay cost diai , obtain content bai,i from CA ai 17: Show bai,i to the user, receive feedback r ∈ {0, 1} drawn from Berbai,i(xi(t)) 18: end if 19: if train = 1 then 20: N tr,iai,p + + 21: else 22: r̄iai,p = (r̄ i ai,pN i ai,p + r)/(N i ai,p + 1) 23: N ip + +, N iai,p + + 24: end if 25: if CAi(t) 6= ∅ then 26: for j ∈ CAi(t) do 27: Send content bi,j to CA j’s user 28: Observe feedback r drawn from Berbi,j (xj(t)) 29: r̄ibi,j ,pi,j = r̄ibi,j ,pi,j Nibi,j ,pi,j +r\nNi bi,j ,pi,j +1\n30: N ipi,j + +, N i bi,j ,pi,j\n+ + 31: end for 32: end if 33: t = t+ 1 34: end while\naBerai (xi(t)) is the Bernoulli distribution with expected value πai (xi(t))\nFig. 4. Pseudocode for DISCOM algorithm.\nof their content, before CA i uses any feedback for content coming from these CAs to form relevance score estimates about them. In contrast, the exploration phase of CA i helps it to build accurate estimates about the relevance score of its matching actions.\nAt time t, the phase that CA i will be in is determined by the amount of time it had explored, exploited or trained for past users with contexts similar to the context of the current user. For this CA i keeps counters and control functions which are described below. Let N ip(t) be the number of user arrivals to CA i with contexts in p ∈ PT by time t (its own arrivals and arrivals to other CAs who requested content from CA i) except the training phases of CA i. For c ∈ Ci, let N ic,p(t) be the number of times content c is selected in response to a user arriving to CA i with context in hypercube p by time t (including times other CAs request content from CA i for their users with contexts in set p). Other than these, CA i keeps two counters for each other CA in each set in the partition, which\nDISCOMmax (maximization part of DISCOM) for CA i:\n1: train = 0 2: Find the hypercube in PT that xi(t) belongs to, i.e., pi(t) 3: Let p = pi(t) 4: Compute the set of under-explored matching actions Cuei,p(t)\ngiven in (4) 5: if Cuei,p(t) 6= ∅ then 6: Select ai randomly from Cuei,p(t) 7: else 8: Compute the set of training candidates Mcti,p(t) given in (5) 9: //Update the counters of training candidates\n10: for j ∈Muti,p(t) do 11: Obtain N jp from CA j, set N tr,i j,p = N j p −N ij,p 12: end for 13: Compute the set of under-trained CAs Muti,p(t) given in (6) 14: Compute the set of under-explored CAs Muei,p(t) given in (7) 15: if Muti,p(t) 6= ∅ then 16: Select ai randomly from Muti,p(t), train = 1 17: else if Muei,p(t) 6= ∅ then 18: Select ai randomly from Muei,p(t) 19: else 20: Select ai randomly from arg maxk∈Ki r̄ i k,p − dik 21: end if 22: end if\nFig. 5. Pseudocode for the maximization part of DISCOM algorithm.\nDISCOMcoop (cooperation part of DISCOM) for CA i\n1: for j ∈ CAi(t) do 2: Find the set in PT that xj(t) belongs to, i.e., pi,j 3: Compute the set of under-explored matching actions Cuei,pi,j (t) given in (4) 4: if Cuei,pi,j (t) 6= ∅ then 5: Select bi,j randomly from Cuei,pi,j (t) 6: else 7: bi,j = arg maxc∈Ci r̄ i c,pi,j 8: end if 9: end for\nFig. 6. Pseudocode for the cooperation part of DISCOM algorithm.\nit uses to decide the phase it should be in. The first one, i.e., N tr,ij,p (t), is an estimate on the number of user arrivals with contexts in p to CA j from all CAs except the training phases of CA j and exploration, exploitation phases of CA i. This counter is only updated when CA i thinks that CA j should be trained. The second one, i.e., N ij,p(t), counts the number of users of CA i with contexts in p for which content is requested from CA j at exploration and exploitation phases of CA i by time t.\nAt each time slot t, CA i first identifies pi(t). Then, it chooses its phase at time t by giving highest priority to exploration of content in its own content network, second highest priority to training of the other CAs, third highest priority to exploration of the other CAs, and lowest priority to exploitation. The reason that exploration of own content has a higher priority than training of other CAs is that it will minimize the number of times CA i will be trained by other CAs, which we describe below.\nFirst, CA i identifies the set of under-explored content in its content network:\nCuei,p(t) := {c ∈ Ci : N ic,p(t) ≤ H1(t)} (4)\nwhere H1(t) is a deterministic, increasing function of t which is called the control function. The value of this function will\n8 affect the regret of DISCOM. For c ∈ Ci, the accuracy of relevance score estimates increase with H1(t), hence it should be selected to balance the tradeoff between accuracy and the number of explorations. If Cuei,p(t) is non-empty, CA i enters the exploration phase and randomly selects a content in this set to explore. Otherwise, it identifies the set of training candidates:\nMcti,p(t) := {j ∈M−i : N tr,i j,p (t) ≤ H2(t)} (5)\nwhere H2(t) is a control function similar to H1(t). Accuracy of other CA’s relevance score estimates of content in their own networks increases with H2(t), hence it should be selected to balance the possible reward gain of CA i due to this increase with the reward loss of CA i due to the number of trainings. If this set is non-empty, CA i asks the CAs j ∈Mcti,p(t) to report N jp (t). Based in the reported values it recomputes N tr,i j,p (t) as N tr,ij,p (t) = N j p (t) − N ij,p(t). Using the updated values, CA i identifies the set of under-trained CAs:\nMuti,p(t) := {j ∈M−i : N tr,i j,p (t) ≤ H2(t)}. (6)\nIf this set is non-empty, CA i enters the training phase and randomly selects a CA in this set to train it. WhenMcti,p(t) or Muti,p(t) is empty, this implies that there is no under-trained CA, hence CA i checks if there is an under-explored matching action. The set of CAs for which CA i does not have accurate relevance scores is given by\nMuei,p(t) := {j ∈M−i : N ij,p(t) ≤ H3(t)} (7)\nwhere H3(t) is also a control function similar to H1(t). If this set is non-empty, CA i enters the exploration phase and randomly selects a CA in this set to request content from to explore it. Otherwise, CA i enters the exploitation phase in which it selects the matching action with the highest estimated relevance score minus cost for its user with context xi(t) ∈ p = pi(t), i.e.,\nai(t) ∈ arg max k∈Ki r̄ik,p(t)− dik (8)\nwhere r̄ik,p(t) is the sample mean estimate of the relevance score of CA i for matching action k at time t, which is computed as follows. For j ∈ M−i, let E ij,p(t) be the set of feedbacks collected by CA i at times it selected CA j while CA i’s users’ contexts are in set p in its exploration and exploitation phases by time t. For estimating the relevance score of contents in its own content network, CA i can also use the feedback obtained from other CAs’ users at times they requested content from CA i. In order to take this into account, for c ∈ Ci, let E ic,p(t) be the set of feedbacks observed by CA i at times it selected its content c for its own users with contexts in set p union the set of feedbacks observed by CA i when it selected its content c for the users of other CAs with contexts in set p who requests content from CA i by time t.\nTherefore, sample mean relevance score of matching action k ∈ Ki for users with contexts in set p for CA i is defined as r̄ik,p(t) = (∑ r∈Eik,p(t) r ) /|E ik,p(t)|, An important observation is that computation of r̄ik,p(t) does not take into account the matching costs. Let µ̂ik,p(t) := r̄ i k,p(t)−dik be the estimated net\nL: Similarity constant. γ: Similarity exponent T : Time horizon mT : Slicing level of DISCOM PT : DISCOM’s partition of X into (mT )d hypercubes pi(t): Hypercube in PT that contains xi(t) N ip(t): Number of all user arrivals to CA i with contexts in p ∈ PT by time t except the training phases of CA i N ic,p(t): Number of times content c is selected in response to a user arriving to CA i with context in hypercube p by time t N tr,ij,p(t): Estimate of CA i on the number of user arrivals with contexts in p to CA j from all CAs except the training phases of CA j and exploration, exploitation phases of CA i N ij,p(t): Number of users of CA i with contexts in p for which content is requested from CA j at exploration and exploitation phases of CA i by time t H1(t), H2(t), H3(t): Control functions of DISCOM Cuei,p(t): Set of under-explored content in Ci Mcti,p(t): Set of training candidates of CA i Muti,p(t): Set of CAs under-trained by CA i Muei,p(t): Set of CAs under-explored by CA i r̄ik,p(t): Sample man relevance score of action k of CA i at time t µ̂ik,p(t) Estimated net reward of action k of CA i at time t\nTABLE III NOTATIONS USED IN DEFINITION OF DISCOM.\nreward (relevance score minus cost) of matching action k for set p. Of note, when there is more than one maximizer of (8), one of them is randomly selected. In order to run DISCOM, CA i does not need to keep the sets E ik,p(t) in its memory. r̄ik,p(t) can be computed by using only r̄ i k,p(t − 1) and the feedback at time t. The cooperation part of DISCOM, i.e., DISCOMcoop operates as follows. Let CAi(t) be the set CAs who request content from CA i at time t. For each j ∈ CAi(t), CA i first checks if it has any under-explored content c for pj(t), i.e., c such that N ic,pj(t)(t) ≤ H1(t). If so, it randomly selects one of its underexplored content to match it with the user of CA j. Otherwise, it exploits its content in Ci with the highest estimated relevance score for CA j’s current user’s context, i.e.,\nbi,j(t) ∈ arg max c∈Ci r̄ic,pj(t)(t). (9)\nA summary of notations used in the description of DISCOM is given in Table III. The following theorem provides a bound on the regret of DISCOM.\nTheorem 1: When DISCOM is run by all CAs with parameters H1(t) = t2γ/(3γ+d) log t, H2(t) = Cmaxt2γ/(3γ+d) log t, H3(t) = t 2γ/(3γ+d) log t and mT = ⌈ T 1/(3γ+d) ⌉ ,6 we have\nRi(T ) ≤ 4(M + Cmax + 1)β2\n+ T 2γ+d 3γ+d\n( 14Ldγ/2 + 12 + 4(|Ci|+M)MCmaxβ2\n(2γ + d)/(3γ + d) +2d+1Zi log T ) + T d 3γ+d 2d+1(|Ci|+ 2(M − 1)),\ni.e., Ri(T ) = Õ ( MCmaxT 2γ+d 3γ+d ) ,7 where Zi = |Ci|+ (M −\n6For a number r ∈ R, let dre be the smallest integer that is greater than or equal to r.\n7Õ(·) is the Big-O notation in which the terms with logarithmic growth rates are hidden.\n9 1)(Cmax + 1). Proof: The proof is given Appendix B.\nFor any d > 0 and γ > 0, the regret given in Theorem 1 is sublinear in time (or number of user arrivals). This guarantees that the regret per-user, i.e., the time averaged regret, converges to 0 (limT→∞ E[Ri(T )]/T = 0). It is also observed that the regret increases in the dimension d of the context. By Assumption 1, a context is similar to another context if they are similar in each dimension, hence number of hypercubes in the partition PT increases with d.\nIn our analysis of the regret of DISCOM we assumed that T is fixed and given as an input to the algorithm. DISCOM can be made to run independently of the final time T by using a standard method called the doubling trick (see, e.g., [10]). The idea is to divide time into rounds with geometrically increasing lengths and run a new instance of DISCOM at each round. For instance, consider rounds τ ∈ {1, 2, . . .}, where each round has length 2τ . Run a new instance of DISCOM at the beginning of each round with time parameter 2τ . This modified version will also have Õ ( T (2γ+d)/(3γ+d) ) regret.\nMaximizing the satisfaction of an individual user is as important as maximizing the overall satisfaction of all users. The next corollary shows that by using DISCOM, CAs guarantee that their users will almost always be provided with the best content available within the entire content network.\nCorollary 1: Assume that DISCOM is run with the set of parameters given in Theorem 1. When DISCOM is in exploitation phase for CA i, we have\nP(µiai(t)(xi(t)) < µ i k∗i (xi(t)) (xi(t))− δT )\n≤ 2|Ki| t2 + 2|Ki|MCmaxβ2 tγ/(3γ+d) .\nwhere δT = (6Ldγ/2 + 6)T−γ/(3γ+d). Proof: The proof is given Appendix C.\nRemark 1: (Differential Services) Maximizing the performance for an individual user is particularly important for providing differential services based on the types of the users. For instance, a CA may want to provide higher quality recommendations to a subscriber (high type user) who has paid for the subscription compared to a non-subscriber (low type user). To do this, the CA can exploit the best content for the subscribed user, while perform exploration on a different user that is not subscribed."
    }, {
      "heading" : "V. REGRET WHEN FEEDBACK IS MISSING",
      "text" : "When analyzing the performance of DISCOM, we assumed that the users always provide a feedback: like or dislike. However, in most of the online content aggregation platforms user feedback is not always available. In this section we consider the effect of missing feedback on the performance of the proposed algorithm. We assume that each user gives a feedback with probability pr (which is unknown to the CAs). If the user at time t does not give feedback, we assume that DISCOM does not update its counters. This will result in a larger number of trainings and explorations compared to the case when feedback is always available. The following theorem gives an upper bound on the regret of DISCOM for this case.\nTheorem 2: Let the DISCOM algorithm run with parameters H1(t) = t2γ/(3γ+d) log t, H2(t) = Cmaxt2γ/(3γ+d) log t, H3(t) = t 2γ/(3γ+d) log t, and mT = ⌈ T 1/(3γ+d) ⌉ . Then, if a user reveals its feedback with probability pr, we have for CA i,\nRi(T ) ≤ 4(M + Cmax + 1)β2\n+ T 2γ+d 3γ+d\n( 14Ldγ/2 + 12 + 4(|Ci|+M)MCmaxβ2\n(2γ + d)/(3γ + d)\n+ 2d+1Zi pr log T ) + T d 3γ+d 2d+1\n|Ci|+ 2(M − 1) pr ,\ni.e., Ri(T ) = Õ ( MCmaxT 2γ+d 3γ+d /pr ) , where Zi = |Ci| +\n(M − 1)(Cmax + 1), βa := ∑∞ t=1 1/t\na. Proof: The proof is given Appendix D.\nFrom Theorem 2, we see that missing feedback does not change the time order of the regret. However, the regret is scaled with 1/pr, which is the expected number of users required for a single feedback."
    }, {
      "heading" : "VI. LEARNING UNDER DYNAMIC USER AND CONTENT CHARACTERISTICS",
      "text" : "When the user and content characteristics change over time, the relevance score of content c for a user with context x changes over time. In this section, we assume that the following relation holds between the probabilities that a content will be liked with users with similar contexts at two different times t and t′.\nAssumption 2: For each c ∈ C, there exists L > 0, γ > 0 such that for all x, x′ ∈ X , we have\n|πc,t(x)− πc,t′(x′)| ≤ L (||x− x′||) γ + |t− t′|/Ts\nwhere 1/Ts > 0 is the speed of the change in user and content characteristics. We call Ts the stability parameter.\nAssumption 2 captures the temporal dynamics of content matching which is absent in Assumption 1. Such temporal variations are often referred to as concept drift [30], [31]. When there is concept drift, a learner should also consider which past information to take into account when learning, in addition to how to combine the past information to learn the best matching strategy.\nThe following modification of DISCOM will deal with dynamically changing user and content characteristics by using a time window of past observations in estimating the relevance scores. The modified algorithm is called DISCOM with time window (DISCOM-W). This algorithm groups the time slots into rounds ζ = 1, 2, . . . each having a fixed length of 2τh time slots, where τh is an integer called the half window length. Some of the time slots in these rounds overlap with each other as given in Fig. 7. The idea is to keep separate control functions and counters for each round, and calculate the sample mean relevance scores for groups of similar contexts based only on the observations that are made during the time window of that round. We call η = 1 the initialization round. The control functions for the initialization round of DISCOMW is the same as the control functions H1(t), H2(t) and H3(t)\n10\nof DISCOM whose values are given in Theorem 1. For the other rounds ζ > 1, the control functions depend on τh and are given as\nHτh1 (t) = H τh 3 (t) = (t mod τh + 1) z log(t mod τh + 1)\nand\nHτh2 (t) = Cmax(t mod τh + 1) z log(t mod τh + 1)\nfor some 0 < z < 1. Each round η is divided into two subrounds. Except the initialization round, i.e., η = 1, the first sub-round is called the passive sub-round, while the second sub-round is called the active sub-round. For the initialization round both sub-rounds are active sub-rounds. In order to reduce the number of trainings and explorations, DISCOMW has an overlapping round structure as shown in Fig. 7. For each round except the initialization round, passive subrounds of round η, overlaps with the active sub-round of round η − 1. DISCOM-W operates in the same way as DISCOM in each round. DISCOM-W can be viewed as an algorithm which generates a new instance of DISCOM at the beginning of each round, with the modified control functions. DISCOM-W runs two different instances of DISCOM at each round. One of these instances is the active instance based on which content matchings are performed, and the other one is the passive instance which learns through the content matchings made by the active instance.\nLet the instance of DISCOM that is run by DISCOM-W at round η be DISCOMη . The hypercubes of DISCOMη are formed in a way similar to DISCOM’s. The input time horizon is taken as Ts which is the stability parameter given in Assumption 2, and the slicing parameter mTs is set accordingly. DISCOMη uses the partition of X into (mTs)d hypercubes denoted by PTs . When all CAs are using DISCOM-W, the matching action selection of CA i only depends on the history of content matchings and feedback observations at round η. If time t is in the active sub-round of round η, matching action of CA i ∈M is taken according to DISCOMη . As a result of the content matching, sample mean relevance scores and counters of both DISCOMη and DISCOMη+1 are updated. Else if time t is in the passive sub-round of round η, matching action of CA i ∈ M is taken according to DISCOMη−1 (see Fig. 7). As a result of this, sample mean relevance scores and counters of both DISCOMη−1 and DISCOMη are updated.\nAt the start of a round η, the relevance score estimates and counters for DISCOMη are equal to zero. However, due to the two sub-round structure, when the active sub-round of round η starts, CA i already has some observations for the context and actions taken in the passive sub-round of that round, hence depending on the arrivals and actions in the passive sub-round, the CA may even start the active sub-round by exploiting, whereas it should have always spent some time in training and exploration if it starts an active sub-round without any past observations (cold start problem).\nIn this section, due to the concept drift, even though the context of a past user can be similar to the context of the current user, their relevance scores for a content c can be very different. Hence DISCOM-W assumes that a past user\nis similar to the current user only if it arrived in the current round. Since round length is fixed, it is impossible to have sublinear number of similar context observations for every t. Thus, achieving sublinear regret under concept drift is not possible. Therefore, in this section we focus on the average regret which is given by\nRavgi (T ) := 1\nT T∑ t=1 ( πk∗i (xi(t))(xi(t))− d i k∗i (xi(t)) ) − 1 T E [ T∑ t=1 ( I(yi(t) = L)− diai(t) )] .\nThe following theorem bounds the average regret of DISCOM-W.\nTheorem 3: When DISCOM-W is run with parameters\nHτh1 (t) = H τh 3 (t) = (t mod τh + 1) 2γ 3γ+d log(t mod τh + 1) Hτh2 (t) = Cmax(t mod τh + 1) 2γ 3γ+d log(t mod τh + 1)\nmTs = dT 1 3γ+d s e and τh = bT (3γ+d)/(4γ+d)s c,8 where Ts is the stability parameter which is given in Assumption 2, the time averaged regret of CA i by time T is\nRavgi (T ) = Õ\n( T −γ 4γ+d s ) for any T > 0. Hence DISCOM-W is = Õ ( T −γ 4γ+d s\n) approximately optimal in terms of the average reward.\nProof: The proof is given Appendix E. From the result of this theorem we see that the average regret decays as the stability parameter Ts increases. This is because, DISCOM-W will use a longer time window (round) when Ts is large, and thus can get more observations to estimate the sample mean relevance scores of the matching actions in that round, which will result in better estimates hence smaller number of suboptimal matching action selections. Moreover, the average number of trainings and explorations required decrease with the round length.\n8For a number b, bbc denotes the largest integer that is smaller than or equal to b.\n11"
    }, {
      "heading" : "VII. NUMERICAL RESULTS",
      "text" : "In this section we provide numerical results for our proposed algorithms DISCOM and DISCOM-W on real-world datasets."
    }, {
      "heading" : "A. Datasets",
      "text" : "For all the datasets below, for a CA the cost of choosing a content within the content network and the cost of choosing another CA is set to 0. Hence, the only factor that affects the total reward is the users’ ratings for the contents.\nYahoo! Today Module (YTM) [5]: The dataset contains news article webpage recommendations of Yahoo! Front Page. Each instance is composed of (i) ID of the recommended content, (ii) the user’s context (2-dimensional vector), (iii) the user’s click information. The user’s click information for a webpage/content is associated with the relevance score of that content. It is equal to 1 if the user clicked on the recommended webpage and 0 else. The dataset contains T = 70000 instances and 40 different types of content. We generate 4 CAs and assign 10 of the 40 types of content to each CA’s content network. Each CA has direct access to content in its own network, while it can also access to the content in other CAs’ content network by requesting content from these CAs. Users are divided into four groups according to their contexts and each group is randomly assigned to one of the CAs. Hence, the user arrival processes to different CA’s are different. The performance of a CA is evaluated in terms of the average number of clicks, i.e., click through rate (CTR), of the contents that are matched with its users.\nMusic Dataset (MD): The dataset contains contextual information and ratings (like/dislike) of music genres (classical, rock, pop, rap) collected from 413 students at UCLA. We generate 2 CAs each specialized in two of the four music genres. Users among the 413 users randomly arrive to each CA. A CA either recommends a music content that is in its content network or asks another CA, specialized in another music genre, to provide a music item. As a result, the rating of the user for the genre of the provided music content is revealed to the CA. The performance of a CA is evaluated in terms of the average number of likes it gets for the contents that are matched with its users.\nYahoo! Today Module (YTM) with Drift (YTMD): This dataset is generated from YTM to simulate the scenario where the user ratings for a particular content changes over time. After every 10000 instances, 20 contents are randomly selected and user clicks for these contents are set to 0 (no click) for the next 10000 instances. For instance, this can represent a scenario where some news articles lose their popularity a day after they become available while some other news articles related to ongoing events will stay popular for several days."
    }, {
      "heading" : "B. Learning Algorithms",
      "text" : "While DISCOM and DISCOM-W are the first distributed algorithms to perform content aggregation (see Table I), we compare their performance with distributed versions of the centralized algorithms proposed in [5], [10], [19], [22]. In the distributed implementation of these centralized algorithms, we assume that each CA runs an independent instance of these\nalgorithms. For instance, when implementing a centralized algorithm A on the distributed system of CAs, we assume that each CA i runs its own instance of A denoted by Ai. When CA i selects CA j as a matching action in Ki by using its algorithm Ai, CA j will select the content for CA i using its algorithm Aj with CA i’s user’s context on the set of contents Cj . In our numerical results, each algorithm is run for different values of its input parameters. The results are shown for the parameter values for which the corresponding algorithm performs the best.\nDISCOM: Our algorithm given in Fig. 4 with control functions H1(t), H2(t) and H3(t) divided by 10 for MD, and by 20 for YTM and YTMD to reduce the number of trainings and explorations.9\nDISCOM-W: Our algorithm given in Fig. 7 which is the time-windowed version of DISCOM with control functions H1(t), H2(t) and H3(t) divided by 20 to reduce the number of trainings and explorations.\nAs we mentioned in Remark 1, both DISCOM and DISCOM-W can provide differential services to its users. In this case both algorithms always exploit for the users with high type (subscribers) and if necessary can train and explore for the users with low type (non-subscribers). Hence, the performance of DISCOM and DISCOM-W for differential services is equal to their performance for the set of high type users.\nLinUCB [5], [22]: This algorithm computes an index for each matching action by assuming that the relevance score of a matching action for a user is a linear combination of the contexts of the user. Then for each user it selects the matching action with the highest index.\nHybrid- [19]: This algorithm forms context-dependent sample mean rewards for the matching actions by considering the history of observations and decisions for groups of contexts that are similar to each other. For user t it either explores a random matching action with probability t or exploits the best matching action with probability 1− t, where t is decreasing in t.\nContextual zooming (CZ) [10]: This algorithm adaptively creates balls over the joint action and context space, calculates an index for each ball based on the history of selections of that ball, and at each time step selects a matching action according to the ball with the highest index that contains the current context."
    }, {
      "heading" : "C. Yahoo! Today Module Simulations",
      "text" : "In YTM each instance (user) has two contexts (x1, x2) ∈ [0, 1]2. We simulate the algorithms in Section VII-B for three different context sets in which the learning algorithms only decide based on (i) the first context x1, (ii) the second context x2, and (iii) both contexts (x1, x2) of the users. The mT parameter of DISCOM for these simulations is set to the optimal value found in Theorem 1 (for γ = 1) which is dT 1/4e for simulations with a single context and dT 1/5e for simulations with both contexts. DISCOM is run for numerous z values ranging from 1/4 to 1/2. Table IV compares the\n9The number of trainings and explorations required in the regret bounds are the worst-case numbers. In reality, good performance is achieved with a much smaller number of trainings and explorations.\n12\nperformance of DISCOM, LinUCB, Hybrid- and CZ. All of the algorithms are evaluated at the parameter values in which they perform the best. As seen from the table the CTR for DISCOM with differential services is 16%, 5% and 7% higher than the best of LinUCB, Hybrid- and CZ for contexts x1, x2 and (x1, x2), respectively.\nTable V compares the performance of DISCOM, the percentage of training, exploration and exploitation phases for different control functions (different z parameters) for simulations with context x2. As expected, the percentage of trainings and explorations increase with the control function. As z increases matching actions are explored with a higher accuracy, and hence the average exploitation reward (CTR) increases."
    }, {
      "heading" : "D. Music Dataset Simulations",
      "text" : "Table VI compares the performance of DISCOM, LinUCB, Hybrid- and CZ for the music dataset. The parameter values used for DISCOM for the result in Table VI are z = 1/8 and mT = 4. From the results is is observed that DISCOM achieves 10% improvement over LinUCB, 5% improvement over Hybrid- , and 28% improvement over CZ in terms of the average number of likes achieved for the users of CA 1. Moreover, the average number of likes received by DISCOM for the high type users (differential services) is even higher, which is 13%, 8% and 32% higher than LinUCB, HE and CZ, respectively."
    }, {
      "heading" : "E. Yahoo! Today Module with Drift Simulations",
      "text" : "Table VII compares the performance of DISCOM-W with half window length (τh = 2500) and mT = 10, DISCOM (with mT set equal to dT 1/4e simulations with a single context dimension and dT 1/5e for the simulation with two context dimensions), LinUCB, Hybrid- and CZ. For the results in the table, the z parameter value of DISCOM and DISCOMW are set to the z value in which they achieve the highest number of clicks. Similarly, LinUCB, Hybrid- and CZ are\nalso evaluated at their best parameter values. The results show the performance of DISCOM and DISCOM-W for differential services. DISCOM-W performs the best in this dataset in terms of the average number of clicks, with about 23%, 11.3% and 51.6% improvement over the best of LinUCB, Hybrid- and CZ, for types of contexts x1, x2, and (x1, x2), respectively."
    }, {
      "heading" : "VIII. CONCLUSION",
      "text" : "In this paper we considered novel online learning algorithms for content matching by a distributed set of CAs. We have characterized the relation between the user and content characteristics in terms of a relevance score, and proposed online learning algorithms that learns to match each user with the content with the highest relevance score. When the user and content characteristics are static, the best matching between content and each type of user can be learned perfectly, i.e., the average regret due to suboptimal matching goes to zero. When the user and content characteristics are dynamic, depending on the rate of the change, an approximately optimal matching between content and each user type can be learned. In addition to our theoretical results, we have validated the concept of distributed content matching on real-world datasets. An interesting future research direction is to investigate the interaction between different CAs when they compete for the same pool of users. Should a CA send a content that has a high chance of being liked by another CA’s user to increase its immediate reward, or should it send a content that has a high chance of being disliked by the other CA’s user to divert that user from using that CA and switch to it instead."
    }, {
      "heading" : "APPENDIX A",
      "text" : ""
    }, {
      "heading" : "A BOUND ON DIVERGENT SERIES",
      "text" : "For p > 0, p 6= 1, T∑ t=1 t−p ≤ 1 + (T 1−p − 1)/(1− p).\nProof: See [32]."
    }, {
      "heading" : "APPENDIX B PROOF OF THEOREM 1",
      "text" : ""
    }, {
      "heading" : "A. Necessary Definitions and Notations",
      "text" : "Let βa := ∑∞ t=1 1/t\na, and let log(.) denote logarithm in base e. For each hypercube p ∈ PT let πc,p := supx∈p πc(x), πc,p := infx∈p πc(x), for c ∈ C, and µik,p := supx∈p µik(x), µi k,p := infx∈p µ i k(x), for k ∈ Ki. Let x∗p be the context at the center (center of symmetry) of the hypercube p. We define the optimal matching action of CA i for hypercube p as k∗i (p) := arg maxk∈Ki µ i k(x ∗ p). When the hypercube p\n13\nis clear from the context, we will simply denote the optimal matching action for hypercube p with k∗i . Let\nLip(t) := { k ∈ Ki : µik∗i (p),p − µ i k,p > (4Ld γ/2 + 6)t−z/2 }\nbe the set of suboptimal matching actions of CA i at time t in hypercube p. Also related to this let\nCjp(t) := { c ∈ Cj : πc∗j (p),p − πc,p > (4Ld γ/2 + 6)t−z/2 }\nbe the set of suboptimal contents of CA j at time t in hypercube p, where c∗j (p) = arg maxc∈Cj πc(x ∗ p). Also when the hypercube p is clear from the context we will just use c∗j . The contents in Cjp(t) are the ones that CA j should not select when called by another CA. The regret given in (3) can be written as a sum of three components:\nRi(T ) = E[R e i (T )] + E[R s i (T )] + E[R n i (T )]\nwhere Rei (T ) is the regret due to trainings and explorations by time T , Rsi (T ) is the regret due to suboptimal matching action selections in exploitations by time T and Rni (T ) is the regret due to near optimal matching action selections in exploitations by time T , which are all random variables."
    }, {
      "heading" : "B. Bounding the Regret in Training, Exploration and Exploitation phases.",
      "text" : "In the following lemmas we will bound each of these terms separately. The following lemma bounds E[Rei (T )].\nLemma 1: Consider all CAs running DISCOM with parameters H1(t) = tz log t, H2(t) = Cmaxtz log t, H3(t) = tz log t and mT = dTκe, where 0 < z < 1 and 0 < κ < 1/d. Then, we have\nE[Rei (T )] ≤ 2d+1(|Ci|+ (M − 1)(Cmax + 1))T z+κd log T + 2d+1(|Ci|+ 2(M − 1))Tκd.\nProof: Since time slot t is a training or an exploration slot for CA i if and only if\nMuti,pi(t)(t) ∪M ue i,pi(t) (t) ∪ Cuei,pi(t)(t) 6= ∅\nup to time T , there can be at most dT z log T e exploration slots in which a content c ∈ Ci is matched with the user of CA i, dCmaxT z log T e training slots in which CA i selects CA j ∈M−i, dT z log T e exploration slots in which CA i selects CA j ∈M−i. Result follows from summing these terms and the fact that (mT )d ≤ 2dTκd for any T ≥ 1. The additional factor of 2 comes from the fact that the realized regret at any time slot can be at most 2.\nFor any k ∈ Ki and p ∈ PT , the sample mean r̄ik,p(t) of the relevance score of matching action k represents a random variable which is the average of the independent samples in set E ik,p(t). Since these samples are not identically distributed, in order to facilitate our analysis of the regret, we generate two different artificial i.i.d. processes to bound the probabilities related to µ̂ik,p(t) = r̄ i k,p(t)− dik, k ∈ Ki. The first one is the best process for CA i in which the net reward of the matching action k for a user with context in p is sampled from an i.i.d. Bernoulli process with mean µik,p, the other one is the worst process for CA i in which this net reward is sampled from\nan i.i.d. Bernoulli process with mean µi k,p . Let µ̂b,ik,p(z) denote the sample mean of the z samples from the best process and µ̂w,ik,p(z) denote the sample mean of the z samples from the worst process for CA i. We will bound the terms E[Rni (T )] and E[Rsi (T )] by using these artificial processes along with the similarity information given in Assumption 1.\nLet Ξij,p(t) be the event that a suboptimal content c ∈ Cj is selected by CA j ∈ M−i, when it is called by CA i for a context in set p for the tth time in the exploitation phases of CA i. Let Xij,p(t) denote the random variable which is the number of times CA j selects a suboptimal content when called by CA i in exploitation slots of CA i when the context is in set p ∈ PT by time t. Clearly, we have\nXij,p(t) = |Eij,p(t)|∑ t′=1 I(Ξij,p(t ′))\nwhere I(·) is the indicator function which is equal to 1 if the event inside is true and 0 otherwise. The following lemma bounds E[Rsi (T )].\nLemma 2: Consider all CAs running DISCOM with parameters H1(t) = tz log t, H2(t) = Cmaxtz log t, H3(t) = tz log t and mT = dTκe, where 0 < z < 1 and κ = z/(2γ). Then, we have\nE[Rei (T )] ≤ 4(|Ci|+M)β2\n+ 4(|Ci|+M)MCmaxβ2 T 1−z/2\n1− z/2 .\nProof: Consider time t. For simplicity of notation let p = pi(t). Let\nWi(t) := {Muti,pi(t)(t) ∪M ue i,pi(t) (t) ∪ Cuei,pi(t)(t) = ∅}\nbe the event that CA i exploits at time t. First, we will bound the probability that CA i selects a suboptimal matching action in an exploitation slot. Then, using this we will bound the expected number of times a suboptimal matching action is selected by CA i in exploitation slots. Note that every time a suboptimal matching action is selected by CA i, since µik(x) = π i k(x) − dik ∈ [−1, 1] for all k ∈ Ki, the realized (hence expected) loss is bounded above by 2. Therefore 2 times the expected number of times a suboptimal matching action is chosen in an exploitation slot bounds the regret due to suboptimal matching actions in exploitation slots. Let Vik(t) be the event that matching action k ∈ Ki is chosen at time t by CA i. We have\nRsi (T ) ≤ 2 T∑ t=1 ∑ k∈Li\npi(t) (t)\nI(Vik(t),Wi(t)).\nTaking the expectation\nE[Rsi (T )] ≤ 2 T∑ t=1 ∑ k∈Li\npi(t) (t)\nP(Vik(t),Wi(t)). (10)\nLet Bij,p(t) be the event that at most tφ samples in E ij,p(t) are collected from suboptimal content of CA j. Let Bi(t) :=⋂ j∈M−i B i j,pi(t) (t). For a setA, letAc denote the complement\n14\nof that set. For any k ∈ Ki, we have P ( Vik,p(t),Wi(t) ) ≤ P ( µ̂ik,p(t) ≥ µik,p +Ht,Wi(t),Bi(t)\n) + P ( µ̂ik∗i ,p(t) ≤ µ i k∗i ,p −Ht,Wi(t),Bi(t) ) + P(Bi(t)c)\n+ P ( µ̂ik,p(t) ≥ µ̂ik∗i ,p(t), µ̂ i k,p(t) < µ i k,p +Ht,\nµ̂ik∗i ,p(t) > µ i k∗i ,p −Ht,Wi(t),Bi(t)\n) (11)\nfor some Ht > 0. We have for any suboptimal matching action k ∈ Lip(t),\nP ( µ̂ik,p(t) ≥ µ̂ik∗i ,p(t), µ̂ i k,p(t) < µ i k,p +Ht,\nµ̂ik∗i ,p(t) > µ i k∗i ,p −Ht,Wi(t),Bik,p(t) ) ≤ P ( µ̂b,ik,p(|E i k,p(t)|) ≥ µ̂ w,i k∗i ,p (|E ik∗i ,p(t)|)− t φ−z,\nµ̂b,ik,p(|E i k,p(t)|) < µik,p + L (√ d/mT )γ +Ht + t φ−z,\nµ̂w,ik∗i ,p (|E ik∗i ,p(t)|) > µ i k∗i ,p − L\n(√ d/mT )γ −Ht, Wi(t) ) .\nFor k ∈ Lip(t), when 2L (√\nd/mT )γ + 2Ht + 2t φ−z ≤ (4Ldγ/2 + 6)t−z/2 (12)\nthe three inequalities given below\nµ k∗i ,p − µik,p > (4Ldγ/2 + 6)t−z/2 µ̂b,ik,p(|E i k,p(t)|) < µik,p + L (√ d/mT )γ +Ht + t φ−z\nµ̂w,ik∗i ,p (|E ik,p(t)|) > µik∗i ,p − L\n(√ d/mT )γ −Ht\ntogether imply that µ̂b,ik,p(|E ik,p(t)|) < µ̂ w,i k∗i ,p (|E ik,p(t)|)− tφ−z , which implies that\nP ( µ̂ik,p(t) ≥ µ̂ik∗i ,p(t), µ̂ i k,p(t) < µ i k,p +Ht,\nµ̂ik∗i ,p(t) > µ i k∗i ,p −Ht,Wi(t),Bik,p(t)\n) = 0. (13)\nLet Ht = 2tφ−z + Ldγ/2m −γ T . A sufficient condition that implies (12) is\n4Ldγ/2t−κγ + 6tφ−z ≤ (4Ldγ/2 + 6)t−z/2 (14)\nwhich holds for all t ≥ 1 when φ = z/2 and κγ ≥ z/2. Using a Chernoff-Hoeffding bound, for any k ∈ Lipi(t)(t), since on the event Wi(t), |E ik,pi(t)(t)| ≥ t z log t, we have\nP ( µ̂ik,p(t) ≥ µik,p +Ht,Wi(t),Bi(t) ) ≤ t−2 (15)\nand P ( µ̂ik∗i ,p(t) ≤ µ i k∗i ,p −Ht,Wi(t),Bi(t) ) ≤ t−2. (16)\nSince {Bij,p(t)c,Wi(t)} = {Xij,p(t) ≥ tφ}, by applying the Markov inequality, we have\nP(Bij,p(t)c,Wi(t)) ≤ E[Xij,p(t)]t−φ.\nSince\nXij,p(t) = |Eij,p(t)|∑ t′=1 I(Ξij,p(t ′))\nand P ( Ξij,p(t) ) ≤\n∑ m∈Cjp(t) P ( r̄jm,p(t) ≥ r̄ j c∗j ,p (t) )\n≤ ∑\nm∈Cjp(t)\n( P ( r̄jm,p(t) ≥ πm,p +Ht,Wi(t) ) +P ( r̄jc∗j ,p (t) ≤ πc∗j ,p −Ht,W i(t) ) + P ( r̄jm,p(t) ≥ r̄ j c∗j ,p (t),\nr̄jm,p(t) < πm,p +Ht, r̄ j c∗j ,p (t) > πc∗j ,p −Ht,W i(t) )) .\nWhen (14) holds, the last probability in the sum above is equal to zero while the first two probabilities are upper bounded by e−2(Ht) 2tz log t. Thus, we have\nP ( Ξij,p(t) ) ≤ ∑ m∈Cjp(t) 2e−2(Ht) 2tz log t ≤ 2|Cj |t−2.\nThis implies that\nE[Xij,p(t)] ≤ ∞∑ t′=1 P(Ξij,p(t ′)) ≤ 2|Cj | ∞∑ t′=1 (t′)−2.\nTherefore, by the Markov inequality and union bound we get\nP(Bij,pi(t)(t) c,Wi(t)) = P(Xij,pi(t)(t) ≥ t φ)\n≤ 2|Cj |β2t−z/2\nand\nP(Bi(t)c,Wi(t)) ≤ 2MCmaxβ2t−z/2. (17)\nThen, using (13), (15), (16) and (17), we have P ( Vik(t),Wi(t) ) ≤ 2t−2 + 2MCmaxβ2t−z/2,\nfor any k ∈ Lipi(t)(t), and By (10), and by the result of Appendix A, we get the stated bound for E[Rsi (T )].\nThe next lemma bounds E[Rni (T )]. Lemma 3: Consider all CAs running DISCOM with parameters H1(t) = tz log t, H2(t) = Cmaxtz log t, H3(t) = tz log t and mT = dTκe, where 0 < z < 1 and κ = z/(2γ). Then, we have\nE[Rni (T )] ≤ (14Ldγ/2 + 12)\n1− z/2 T 1−z/2 + 4Cmaxβ2.\nProof: At any time t, for any k ∈ Ki−Lip(t) and x ∈ p, we have\nµik∗i (x) (x)− µik(x) ≤ (7Ldγ/2 + 6)t−z/2.\nSimilarly, for any j ∈M, c ∈ Cj −Cjp(t) and x ∈ p, we have\nπc∗j (x)(x)− πc(x) ≤ (7Ld γ/2 + 6)t−z/2.\nDue to the above inequalities, if a near optimal action in Ci∩(Ki−Lip(t)) is chosen by CA i at time t, the contribution to the regret is at most (7Ldγ/2 + 6)t−z/2. If a near optimal\n15\nCA j ∈M−i∩(Ki−Lip(t)) is called by CA i at time t, and if CA j selects one of its near optimal contents in Cj−Cjp(t), then the contribution to the regret is at most 2(7Ldγ/2 + 6)t−z/2. Moreover since we are in an exploitation step, the near-optimal CA j that is chosen can choose one of its suboptimal contents in Cjp(t) with probability at most 2Cmaxt−2, which will result in an expected regret of at most 4Cmaxt−2.\nTherefore, the total regret due to near optimal choices of CA i by time T is upper bounded by\n(14Ldγ/2 + 12) T∑ t=1 t−z/2 + 4Cmax T∑ t=1 t−2\n≤ (14Ld γ/2 + 12)\n1− z/2 T 1−z/2 + 4Cmaxβ2.\nby using the result in Appendix A. Next, we give the proof ot Theorem 1 by combining the results of the above lemmas."
    }, {
      "heading" : "C. Proof of Theorem 1",
      "text" : "The highest orders of regret that come from Lemmas 1, 2 and 3 are Õ(Tκd+z), O(T 1−z/2), O(T 1−z/2). We need to optimize them with respect to the constraint Regret is minimized when κd + z = 1 − z/2, which is attained by z = 2γ/(3γ + d). Result follows from summing the bounds in Lemmas 1, 2 and 3."
    }, {
      "heading" : "APPENDIX C PROOF OF COROLLARY 1",
      "text" : "From the proof of Lemma 2, for z = 2γ/(3γ+d), we have P ( Vik(t),Wi(t) ) ≤ 2t−2 + 2MCmaxβ2t−γ/(3γ+d)\nfor k ∈ Lipi(t)(t). This implies that P ( ai(t) ∈ Lipi(t)(t),W i(t) )\n≤ ∑\nk∈Li pi(t) (t)\nP ( Vik(t),Wi(t) ) ≤ 2|Ki|\nt2 + 2|Ki|MCmaxβ2 tγ/(3γ+d) .\nThe difference between the expected reward of an action within a hypercube from its expected reward at the center of the hypercube is at most Ldγ/2/(mT )γ . Since mT = dT 1/(3γ+d)e, ai(t) ∈ Ki − Lipi(t)(t) implies that\nµiai(t)(xi(t)) ≥ µ i k∗i (xi(t)) (xi(t))− (6Ldγ/2 + 6)T−γ/(3γ+d)."
    }, {
      "heading" : "APPENDIX D PROOF OF THEOREM 2",
      "text" : "In order for time t to be an exploitation slot for CA i it is required that Muti,pi(t)(t)∪M ue i,pi(t)\n(t)∪ Cuei,pi(t)(t) = ∅. Since the counters of DISCOM are updated only when feedback is received, and since the control functions are the same as the ones that are used in the setting where feedback is always available, the regret due to suboptimal and near optimal matching actions by time t with missing feedback\nwill not be any greater than the regret due to suboptimal and near optimal matching actions for the case when the users always provide feedback. Therefore, the bounds given in Lemmas 2 and 3 will also hold for the case with missing feedback. Only the regret due to trainings and explorations increases, since more trainings and explorations are needed before the counters exceed the values of the control functions such that the relevance score estimates are accurate enough to exploit. Consider any p ∈ PT . From the definition of DISCOM, the number of exploration slots in which content c ∈ Ci is matched with CA i’s user and the user’s feedback is observed is at most dT 2γ/(3γ+d)e. The number of training slots in which CA i requested content from CA j ∈ M−i and received the feedback about this content from its user is at most ⌈ CmaxT 2γ/(3γ+d) log T ⌉ . The number of exploration\nslots in which CA i selected CA j ∈ M−i is at most⌈ T 2γ/(3γ+d) log T ⌉ .\nLet τexp(T ) be the random variable which denotes the smallest time step for which for each c ∈ Ci there are dT 2γ/(3γ+d)e feedback observations, for each j ∈M−i there are ⌈ CmaxT 2γ/(3γ+d) log T ⌉\nfeedback observations for the trainings and ⌈ T 2γ/(3γ+d) log T ⌉ feedback observations for the explorations. Then, E[τexp(T )] is the expected number of training plus exploration slots by time T . Let Yexp(t) be the random variable which denotes the number of time slots in which the feedback is not provided by the users of CA i till CA i received t feedbacks from its users. Let Ai(T ) = ZiT 2γ/(3γ+d) log T + (|Ci|+ 2(M − 1)). We have\nE[τexp(T )] = E[Yexp(Ai(T ))] +Ai(T ).\nYexp(Ai(T )) is a negative binomial random variable with probability of observing no feedback at any time t equals to 1− pr. Therefore,\nE[Yexp(Ai(T ))] = (1− pr)Ai(T )/pr.\nUsing this, we get\nE[τexp(T )] = Ai(T )/pr.\nThe regret bound follows from substituting this into the proof of Theorem 1."
    }, {
      "heading" : "APPENDIX E PROOF OF THEOREM 3",
      "text" : "The basic idea is to choose τh in a way that the regret due to variation of relevance scores over time and the regret due to variation of estimated relevance scores due to the limited number of observations during each round is balanced. Majority of the steps of this proof is similar to the proof of Theorem 1 hence some of the steps are omitted.\nConsider a round η of length 2τh. Denote the set of time slots in round η by [η]. For any c ∈ C let\nπ̄c,p,η := sup x∈p,t∈[η] πc,t(x),\nπc,p,η := inf x∈p,t∈[η] πc,t(x).\n16\nFor any k ∈ Ki let\nµ̄ik,p,η := sup x∈p,t∈[η] µik,t(x),\nµi k,p,η\n:= inf x∈p,t∈[η]\nµik,t(x),\nwhere µik,t(x) is defined as the time-varying version of µ i k(x) given in (1) under Assumption 2. For CA i, the set of suboptimal matching actions is given as\nLip,η(t) := { k ∈ Ki : µik∗i (p),p,η − µ̄ i k,p,η\n> (4Ldγ/2 + 6)(t mod τh + 1)−z/2 + 4τh Ts\n} ,\nwhere k∗i (p) is the matching action with the highest net reward for the context at the center of p at the time slot in the middle of round η.\nConsider the regret due to explorations and trainings for DISCOMη incurred over times when it is in the active subphase (over τh time slots). Similar to the proof of Lemma 1 it can be shown that the regret due to trainings and explorations is\nE[Rei (τh)] = Õ ( τz+κdh ) .\nSimilar to the proof of Lemma 2, it can be shown that the regret due to suboptimal matching action selections is\nE[Rsi (τh)] = O ( τ 1−z/2 h ) when κ = z/(2γ). Since the definition of a sub-optimal matching action is different for dynamic user and content characteristics, the regret due to near optimal matching actions in Ki−Lip,η(t) is different from Lemma 3. At time t which is in round η, since a near optimal matching action’s contribution to the one-step regret is at most\n(8Ldγ/2 + 12)(t mod τh + 1) −z/2 + 4τh/Ts\nsumming over all time slots in a round η, we have E[Rni (τh)] = O ( τ 1−z/2 h ) +O ( τ2h Ts ) .\nClearly we have E[Rsi (τh)] ≤ E[Rei (τh)]. Let τh = bTφs c for some φ > 0. Then we have\nE[Rei (τh)]\nτh = Õ\n( Tφz+φκd−φs ) ,\nand E[Rni (τh)]\nτh = O\n( T−φz/2s ) +O ( Tφ−1s ) .\nThe sum (E[Rei (τh)] + E[R s i (τh)] + E[R n i (τh)])/τh is minimized by setting z = 2γ/(3γ + d) and φ = 1/(1 + z/2). Hence, τh = bT 3γ+d 4γ+d s c the order of the time averaged regret is\nequal to Õ ( T −γ 4γ+d s ) ."
    } ],
    "references" : [ {
      "title" : "Contextual online learning for multimedia content aggregation",
      "author" : [ "C. Tekin", "M. van der Schaar" ],
      "venue" : "to appear in IEEE Trans. Multimedia, 2015.",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Pricing and investment for online TV content platforms",
      "author" : [ "S. Ren", "M. van der Schaar" ],
      "venue" : "IEEE Trans. Multimedia, vol. 14, no. 6, pp. 1566– 1578, 2012.",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Advanced IPTV services personalization through context-aware content recommendation",
      "author" : [ "S. Song", "H. Moustafa", "H. Afifi" ],
      "venue" : "IEEE Trans. Multimedia, vol. 14, no. 6, pp. 1528–1537, 2012.",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "A novel framework for semantic annotation and personalized retrieval of sports video",
      "author" : [ "C. Xu", "J. Wang", "H. Lu", "Y. Zhang" ],
      "venue" : "IEEE Trans. Multimedia, vol. 10, no. 3, pp. 421–436, 2008.",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "A contextual-bandit approach to personalized news article recommendation",
      "author" : [ "L. Li", "W. Chu", "J. Langford", "R.E. Schapire" ],
      "venue" : "Proc. 19th Int. Conf. World Wide Web, 2010, pp. 661–670.",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Analyzing video services in web 2.0: a global perspective",
      "author" : [ "M. Saxena", "U. Sharan", "S. Fahmy" ],
      "venue" : "Proc. 18th Int. Workshop Network and Operating Systems Support for Digital Audio and Video, 2008, pp. 39– 44.",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Adapting multimedia internet content for universal access",
      "author" : [ "R. Mohan", "J.R. Smith", "C.-S. Li" ],
      "venue" : "IEEE Trans. Multimedia, vol. 1, no. 1, pp. 104–114, 1999.",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "Building an integrated pan- European news distribution network",
      "author" : [ "M. Schranz", "S. Dustdar", "C. Platzer" ],
      "venue" : "Collaborative Networks and Their Breeding Environments. Springer US, 2005, pp. 587–596.",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "FStream: a decentralized and social music streamer",
      "author" : [ "A. Boutet", "K. Kloudas", "A.-M. Kermarrec" ],
      "venue" : "Proc. Int. Conf. Networked Systems (NETYS), pp. 253–257.",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 0
    }, {
      "title" : "Contextual bandits with similarity information",
      "author" : [ "A. Slivkins" ],
      "venue" : "Proc. 24th Annual Conf. Learning Theory (COLT), vol. 19, June 2011, pp. 679–702.",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Contextual multi-armed bandits",
      "author" : [ "T. Lu", "D. Pál", "M. Pál" ],
      "venue" : "Proc. 13th Int. Conf. Artificial Intelligence and Statistics (AISTATS), vol. 9, May 2010, pp. 485–492.",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Linear bandits in high dimension and recommendation systems",
      "author" : [ "Y. Deshpande", "A. Montanari" ],
      "venue" : "Proc. 50th Annual Allerton Conf. Communication, Control and Computing, 2012, pp. 1750–1754.",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "A fast bandit algorithm for recommendations to users with heterogeneous tastes",
      "author" : [ "P. Kohli", "M. Salek", "G. Stoddard" ],
      "venue" : "Proc. 27th Conf. Artificial Intelligence (AAAI), July 2013, pp. 1135–1141.",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "A hidden Markov model for collaborative filtering",
      "author" : [ "N. Sahoo", "P.V. Singh", "T. Mukhopadhyay" ],
      "venue" : "MIS Quarterly, vol. 36, no. 4, pp. 1329–1356, 2012.",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Amazon.com recommendations: Item-to-item collaborative filtering",
      "author" : [ "G. Linden", "B. Smith", "J. York" ],
      "venue" : "Internet Comput., vol. 7, no. 1, pp. 76–80, 2003.",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Collaborative filtering with the simple Bayesian classifier",
      "author" : [ "K. Miyahara", "M.J. Pazzani" ],
      "venue" : "PRICAI 2000 Topics in Artificial Intelligence. Springer, 2000, pp. 679–689.",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "Empowering cross-domain internet media with real-time topic learning from social streams",
      "author" : [ "S.D. Roy", "T. Mei", "W. Zeng", "S. Li" ],
      "venue" : "Proc. IEEE Int. Conf. Multimedia and Expo (ICME), 2012, pp. 49–54.",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Towards cross-domain learning for social video popularity prediction",
      "author" : [ "S. Roy", "T. Mei", "W. Zeng", "S. Li" ],
      "venue" : "IEEE Trans. Multimedia, vol. 15, no. 6, pp. 1255–1267, Oct 2013.",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Hybrid-εgreedy for mobile context-aware recommender system",
      "author" : [ "D. Bouneffouf", "A. Bouzeghoub", "A.L. Gançarski" ],
      "venue" : "Advances in Knowledge Discovery and Data Mining. Springer, 2012, pp. 468–479.",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Online learning with prior knowledge",
      "author" : [ "E. Hazan", "N. Megiddo" ],
      "venue" : "Learning Theory. Springer, 2007, pp. 499–513.",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Efficient optimal learning for contextual bandits",
      "author" : [ "M. Dudik", "D. Hsu", "S. Kale", "N. Karampatziakis", "J. Langford", "L. Reyzin", "T. Zhang" ],
      "venue" : "arXiv preprint arXiv:1106.2369, 2011.",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Contextual bandits with linear payoff functions",
      "author" : [ "W. Chu", "L. Li", "L. Reyzin", "R.E. Schapire" ],
      "venue" : "Proc. 14th Int. Conf. Artificial Intelligence and Statistics (AISTATS), vol. 15, April 2011, pp. 208–214.",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Finite-time analysis of the multiarmed bandit problem",
      "author" : [ "P. Auer", "N. Cesa-Bianchi", "P. Fischer" ],
      "venue" : "Machine Learning, vol. 47, p. 235256, 2002.",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Social multimedia: highlighting opportunities for search and mining of multimedia data in social media applications",
      "author" : [ "M. Naaman" ],
      "venue" : "Multimedia Tools and Applications, vol. 56, no. 1, pp. 9–34, 2012.",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "The impact of diversity on online ensemble learning in the presence of concept drift",
      "author" : [ "L.L. Minku", "A.P. White", "X. Yao" ],
      "venue" : "IEEE Trans. Knowl. Data Eng., vol. 22, no. 5, pp. 730–742, 2010.",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "A framework for generating data to simulate changing environments",
      "author" : [ "A. Narasimhamurthy", "L.I. Kuncheva" ],
      "venue" : "Proc. 25th IASTED Int. Multi-Conf.: Artificial Intelligence and Applications, February 2007, pp. 384–389.  17",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Efficient online exchange via fiat money",
      "author" : [ "M. van der Schaar", "J. Xu", "W. Zame" ],
      "venue" : "Economic Theory, vol. 54, no. 2, pp. 211–248, 2013.",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Bitcoin: A peer-to-peer electronic cash system",
      "author" : [ "S. Nakamoto" ],
      "venue" : "Consulted, vol. 1, no. 2012, p. 28, 2008.",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "The epoch-greedy algorithm for contextual multi-armed bandits",
      "author" : [ "J. Langford", "T. Zhang" ],
      "venue" : "Proc. 20th Int. Conf. Neural Information Processing Systems (NIPS), vol. 20, 2007, pp. 1096–1103.",
      "citeRegEx" : "29",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "On appropriate assumptions to mine data streams: Analysis and practice",
      "author" : [ "J. Gao", "W. Fan", "J. Han" ],
      "venue" : "Proc. 7th IEEE Int. Conf. Data Mining (ICDM), 2007, pp. 143–152.",
      "citeRegEx" : "30",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Integrating novel class detection with classification for concept-drifting data streams",
      "author" : [ "M.M. Masud", "J. Gao", "L. Khan", "J. Han", "B. Thuraisingham" ],
      "venue" : "Machine Learning and Knowledge Discovery in Databases. Springer, 2009, pp. 79–94.",
      "citeRegEx" : "31",
      "shortCiteRegEx" : null,
      "year" : 2009
    } ],
    "referenceMentions" : [ {
      "referenceID" : 1,
      "context" : "A plethora of multimedia applications (web-based TV [2], [3], personalized video retrieval [4], personalized news aggregation [5], etc.",
      "startOffset" : 52,
      "endOffset" : 55
    }, {
      "referenceID" : 2,
      "context" : "A plethora of multimedia applications (web-based TV [2], [3], personalized video retrieval [4], personalized news aggregation [5], etc.",
      "startOffset" : 57,
      "endOffset" : 60
    }, {
      "referenceID" : 3,
      "context" : "A plethora of multimedia applications (web-based TV [2], [3], personalized video retrieval [4], personalized news aggregation [5], etc.",
      "startOffset" : 91,
      "endOffset" : 94
    }, {
      "referenceID" : 4,
      "context" : "A plethora of multimedia applications (web-based TV [2], [3], personalized video retrieval [4], personalized news aggregation [5], etc.",
      "startOffset" : 126,
      "endOffset" : 129
    }, {
      "referenceID" : 5,
      "context" : ", Dailymotion, Metacafe [6]) that are responsible for mining the content of numerous multimedia sources in search of finding content which is interesting for the users.",
      "startOffset" : 24,
      "endOffset" : 27
    }, {
      "referenceID" : 0,
      "context" : "This online technical report is an extended version of the paper that appeared in IEEE Transactions on Multimedia [1].",
      "startOffset" : 114,
      "endOffset" : 117
    }, {
      "referenceID" : 6,
      "context" : "It may also represent the type of the device that the user is using [7] (e.",
      "startOffset" : 68,
      "endOffset" : 71
    }, {
      "referenceID" : 7,
      "context" : "[8]).",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 8,
      "context" : "[9]).",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 9,
      "context" : "To jointly optimize the performance of the multimedia content aggregation system, we propose an online learning methodology that builds on contextual bandits [10], [11].",
      "startOffset" : 158,
      "endOffset" : 162
    }, {
      "referenceID" : 10,
      "context" : "To jointly optimize the performance of the multimedia content aggregation system, we propose an online learning methodology that builds on contextual bandits [10], [11].",
      "startOffset" : 164,
      "endOffset" : 168
    }, {
      "referenceID" : 4,
      "context" : "For instance, in [5], [12] a recommender system that learns the preferences of its users in an online way based on the ratings submitted by the users is provided.",
      "startOffset" : 17,
      "endOffset" : 20
    }, {
      "referenceID" : 11,
      "context" : "For instance, in [5], [12] a recommender system that learns the preferences of its users in an online way based on the ratings submitted by the users is provided.",
      "startOffset" : 22,
      "endOffset" : 26
    }, {
      "referenceID" : 12,
      "context" : "An online learning algorithm for a centralized recommender which updates its recommendations as both the preferences of the users and the characteristics of items change over time is proposed in [13].",
      "startOffset" : 195,
      "endOffset" : 199
    }, {
      "referenceID" : 13,
      "context" : "The general framework which exploits the similarities between the past users and the current user to recommend content to the current user is called collaborative filtering [14]–[16].",
      "startOffset" : 173,
      "endOffset" : 177
    }, {
      "referenceID" : 15,
      "context" : "The general framework which exploits the similarities between the past users and the current user to recommend content to the current user is called collaborative filtering [14]–[16].",
      "startOffset" : 178,
      "endOffset" : 182
    }, {
      "referenceID" : 14,
      "context" : "Groups of similar users can be created by various methods such as clustering [15], and then, the matching will be made based on the content matched with the past users that are in the same group.",
      "startOffset" : 77,
      "endOffset" : 81
    }, {
      "referenceID" : 4,
      "context" : "Our work [5], [12] [15] [14] [19] Distributed Yes No No No No Reward model Hölder Linear N/A N/A N/A Confidence bounds Yes No No No No Regret bound Yes Yes No No No Dynamic user Yes No Yes Yes Yes /content distribution",
      "startOffset" : 9,
      "endOffset" : 12
    }, {
      "referenceID" : 11,
      "context" : "Our work [5], [12] [15] [14] [19] Distributed Yes No No No No Reward model Hölder Linear N/A N/A N/A Confidence bounds Yes No No No No Regret bound Yes Yes No No No Dynamic user Yes No Yes Yes Yes /content distribution",
      "startOffset" : 14,
      "endOffset" : 18
    }, {
      "referenceID" : 14,
      "context" : "Our work [5], [12] [15] [14] [19] Distributed Yes No No No No Reward model Hölder Linear N/A N/A N/A Confidence bounds Yes No No No No Regret bound Yes Yes No No No Dynamic user Yes No Yes Yes Yes /content distribution",
      "startOffset" : 19,
      "endOffset" : 23
    }, {
      "referenceID" : 13,
      "context" : "Our work [5], [12] [15] [14] [19] Distributed Yes No No No No Reward model Hölder Linear N/A N/A N/A Confidence bounds Yes No No No No Regret bound Yes Yes No No No Dynamic user Yes No Yes Yes Yes /content distribution",
      "startOffset" : 24,
      "endOffset" : 28
    }, {
      "referenceID" : 18,
      "context" : "Our work [5], [12] [15] [14] [19] Distributed Yes No No No No Reward model Hölder Linear N/A N/A N/A Confidence bounds Yes No No No No Regret bound Yes Yes No No No Dynamic user Yes No Yes Yes Yes /content distribution",
      "startOffset" : 29,
      "endOffset" : 33
    }, {
      "referenceID" : 16,
      "context" : "Another line of work [17], [18] uses social streams mined in one domain, e.",
      "startOffset" : 21,
      "endOffset" : 25
    }, {
      "referenceID" : 17,
      "context" : "Another line of work [17], [18] uses social streams mined in one domain, e.",
      "startOffset" : 27,
      "endOffset" : 31
    }, {
      "referenceID" : 16,
      "context" : "For example, in [17], Tweet streams are used to provide video recommendations in a commercial video search engine.",
      "startOffset" : 16,
      "endOffset" : 20
    }, {
      "referenceID" : 6,
      "context" : "A content adaptation method is proposed in [7] which enables the users with different types of contexts and devices to receive content that is in a suitable format to be accessed.",
      "startOffset" : 43,
      "endOffset" : 46
    }, {
      "referenceID" : 17,
      "context" : "Video popularity prediction is studied in [18], where the goal is to predict if a video will become popular in the multimedia domain, by detecting social trends in another social media domain (such as Twitter), and transferring this knowledge to the multimedia domain.",
      "startOffset" : 42,
      "endOffset" : 46
    }, {
      "referenceID" : 9,
      "context" : "Contextual bandits have been studied before in [10], [11], [20]–[22] in a single agent setting, where the agent sequentially chooses from a set of alternatives with unknown rewards, and the rewards depend on the context information provided to the agent at each time step.",
      "startOffset" : 47,
      "endOffset" : 51
    }, {
      "referenceID" : 10,
      "context" : "Contextual bandits have been studied before in [10], [11], [20]–[22] in a single agent setting, where the agent sequentially chooses from a set of alternatives with unknown rewards, and the rewards depend on the context information provided to the agent at each time step.",
      "startOffset" : 53,
      "endOffset" : 57
    }, {
      "referenceID" : 19,
      "context" : "Contextual bandits have been studied before in [10], [11], [20]–[22] in a single agent setting, where the agent sequentially chooses from a set of alternatives with unknown rewards, and the rewards depend on the context information provided to the agent at each time step.",
      "startOffset" : 59,
      "endOffset" : 63
    }, {
      "referenceID" : 21,
      "context" : "Contextual bandits have been studied before in [10], [11], [20]–[22] in a single agent setting, where the agent sequentially chooses from a set of alternatives with unknown rewards, and the rewards depend on the context information provided to the agent at each time step.",
      "startOffset" : 64,
      "endOffset" : 68
    }, {
      "referenceID" : 4,
      "context" : "In [5], a contextual bandit algorithm named LinUCB is proposed for recommending personalized news articles, which is variant of the UCB algorithm [23] designed for linear payoffs.",
      "startOffset" : 3,
      "endOffset" : 6
    }, {
      "referenceID" : 22,
      "context" : "In [5], a contextual bandit algorithm named LinUCB is proposed for recommending personalized news articles, which is variant of the UCB algorithm [23] designed for linear payoffs.",
      "startOffset" : 146,
      "endOffset" : 150
    }, {
      "referenceID" : 0,
      "context" : "Let X = [0, 1] be the context space,5 where d is the dimension of the context space.",
      "startOffset" : 8,
      "endOffset" : 14
    }, {
      "referenceID" : 0,
      "context" : "We assume that all these quantities are mapped into [0, 1].",
      "startOffset" : 52,
      "endOffset" : 58
    }, {
      "referenceID" : 4,
      "context" : "For instance, this mapping can be established by feature extraction methods such as the one given in [5].",
      "startOffset" : 101,
      "endOffset" : 104
    }, {
      "referenceID" : 0,
      "context" : "Another method is to represent each property of a user by a real number between [0, 1] (e.",
      "startOffset" : 80,
      "endOffset" : 86
    }, {
      "referenceID" : 7,
      "context" : "Consider a distributed set of news aggregators that operate in different countries (for instance a European news aggregator network as in [8]).",
      "startOffset" : 138,
      "endOffset" : 141
    }, {
      "referenceID" : 23,
      "context" : "For online multimedia content, especially for social media, it is known that both the user and the content characteristics are dynamic and noisy [24], hence the problem exhibits concept drift [25].",
      "startOffset" : 145,
      "endOffset" : 149
    }, {
      "referenceID" : 24,
      "context" : "For online multimedia content, especially for social media, it is known that both the user and the content characteristics are dynamic and noisy [24], hence the problem exhibits concept drift [25].",
      "startOffset" : 192,
      "endOffset" : 196
    }, {
      "referenceID" : 25,
      "context" : ", the joint distribution of the user and content characteristics, at a certain point of time [26].",
      "startOffset" : 93,
      "endOffset" : 97
    }, {
      "referenceID" : 26,
      "context" : "When the cost is payment, it can be money, tokens [27] or Bitcoins [28].",
      "startOffset" : 50,
      "endOffset" : 54
    }, {
      "referenceID" : 27,
      "context" : "When the cost is payment, it can be money, tokens [27] or Bitcoins [28].",
      "startOffset" : 67,
      "endOffset" : 71
    }, {
      "referenceID" : 0,
      "context" : "Since this cost is bounded, without loss of generality we assume that dj ∈ [0, 1] for all i, j ∈ M.",
      "startOffset" : 75,
      "endOffset" : 81
    }, {
      "referenceID" : 0,
      "context" : "In order make our model general, we also assume that there is a cost associated with recommending a type of content c ∈ Ci, which is given by dc ∈ [0, 1], for CA i.",
      "startOffset" : 147,
      "endOffset" : 153
    }, {
      "referenceID" : 9,
      "context" : "CA i’s problem can be modeled as a contextual bandit problem [10], [21], [22], [29], where likes and costs translate into rewards.",
      "startOffset" : 61,
      "endOffset" : 65
    }, {
      "referenceID" : 20,
      "context" : "CA i’s problem can be modeled as a contextual bandit problem [10], [21], [22], [29], where likes and costs translate into rewards.",
      "startOffset" : 67,
      "endOffset" : 71
    }, {
      "referenceID" : 21,
      "context" : "CA i’s problem can be modeled as a contextual bandit problem [10], [21], [22], [29], where likes and costs translate into rewards.",
      "startOffset" : 73,
      "endOffset" : 77
    }, {
      "referenceID" : 28,
      "context" : "CA i’s problem can be modeled as a contextual bandit problem [10], [21], [22], [29], where likes and costs translate into rewards.",
      "startOffset" : 79,
      "endOffset" : 83
    }, {
      "referenceID" : 0,
      "context" : "where dk ∈ [0, 1] is the normalized cost of choosing action k for CA i.",
      "startOffset" : 11,
      "endOffset" : 17
    }, {
      "referenceID" : 0,
      "context" : "In the next section, we propose an online learning algorithm which achieves M: Set of all CAs Ci: Contents in the Content Network of CA i Cmax: maxi∈M |Ci| C: Set of all contents X = [0, 1]: Context space Y: Set of feedbacks a user can give xi(t): d-dimensional context of tth user of CA i yi(t): Feedback of the tth user of CA i Ki: Set of content matching actions of CA i πc(x): Relevance score of content c for context x dk: Cost of choosing matching action k for CA i μk(x): Expected reward (static) of CA i from matching action k for context x k∗ i (x): Optimal matching action of CA i given context x (oracle benchmark) Ri(T ): Regret of CA i at time T βa := ∑∞ t=1 1/t a",
      "startOffset" : 183,
      "endOffset" : 189
    }, {
      "referenceID" : 9,
      "context" : "In contrast to prior online learning algorithms that exploit the context information [10], [11], [20]–[22], [29], which consider a single learner setting, the proposed algorithm helps a CA to learn from the experience of other CAs.",
      "startOffset" : 85,
      "endOffset" : 89
    }, {
      "referenceID" : 10,
      "context" : "In contrast to prior online learning algorithms that exploit the context information [10], [11], [20]–[22], [29], which consider a single learner setting, the proposed algorithm helps a CA to learn from the experience of other CAs.",
      "startOffset" : 91,
      "endOffset" : 95
    }, {
      "referenceID" : 19,
      "context" : "In contrast to prior online learning algorithms that exploit the context information [10], [11], [20]–[22], [29], which consider a single learner setting, the proposed algorithm helps a CA to learn from the experience of other CAs.",
      "startOffset" : 97,
      "endOffset" : 101
    }, {
      "referenceID" : 21,
      "context" : "In contrast to prior online learning algorithms that exploit the context information [10], [11], [20]–[22], [29], which consider a single learner setting, the proposed algorithm helps a CA to learn from the experience of other CAs.",
      "startOffset" : 102,
      "endOffset" : 106
    }, {
      "referenceID" : 28,
      "context" : "In contrast to prior online learning algorithms that exploit the context information [10], [11], [20]–[22], [29], which consider a single learner setting, the proposed algorithm helps a CA to learn from the experience of other CAs.",
      "startOffset" : 108,
      "endOffset" : 112
    }, {
      "referenceID" : 0,
      "context" : "DISCOM creates a partition of X = [0, 1] based on T .",
      "startOffset" : 34,
      "endOffset" : 40
    }, {
      "referenceID" : 9,
      "context" : ", [10]).",
      "startOffset" : 2,
      "endOffset" : 6
    }, {
      "referenceID" : 29,
      "context" : "Such temporal variations are often referred to as concept drift [30], [31].",
      "startOffset" : 64,
      "endOffset" : 68
    }, {
      "referenceID" : 30,
      "context" : "Such temporal variations are often referred to as concept drift [30], [31].",
      "startOffset" : 70,
      "endOffset" : 74
    }, {
      "referenceID" : 4,
      "context" : "Yahoo! Today Module (YTM) [5]: The dataset contains news article webpage recommendations of Yahoo! Front Page.",
      "startOffset" : 26,
      "endOffset" : 29
    }, {
      "referenceID" : 4,
      "context" : "While DISCOM and DISCOM-W are the first distributed algorithms to perform content aggregation (see Table I), we compare their performance with distributed versions of the centralized algorithms proposed in [5], [10], [19], [22].",
      "startOffset" : 206,
      "endOffset" : 209
    }, {
      "referenceID" : 9,
      "context" : "While DISCOM and DISCOM-W are the first distributed algorithms to perform content aggregation (see Table I), we compare their performance with distributed versions of the centralized algorithms proposed in [5], [10], [19], [22].",
      "startOffset" : 211,
      "endOffset" : 215
    }, {
      "referenceID" : 18,
      "context" : "While DISCOM and DISCOM-W are the first distributed algorithms to perform content aggregation (see Table I), we compare their performance with distributed versions of the centralized algorithms proposed in [5], [10], [19], [22].",
      "startOffset" : 217,
      "endOffset" : 221
    }, {
      "referenceID" : 21,
      "context" : "While DISCOM and DISCOM-W are the first distributed algorithms to perform content aggregation (see Table I), we compare their performance with distributed versions of the centralized algorithms proposed in [5], [10], [19], [22].",
      "startOffset" : 223,
      "endOffset" : 227
    }, {
      "referenceID" : 4,
      "context" : "LinUCB [5], [22]: This algorithm computes an index for each matching action by assuming that the relevance score of a matching action for a user is a linear combination of the contexts of the user.",
      "startOffset" : 7,
      "endOffset" : 10
    }, {
      "referenceID" : 21,
      "context" : "LinUCB [5], [22]: This algorithm computes an index for each matching action by assuming that the relevance score of a matching action for a user is a linear combination of the contexts of the user.",
      "startOffset" : 12,
      "endOffset" : 16
    }, {
      "referenceID" : 18,
      "context" : "Hybrid- [19]: This algorithm forms context-dependent sample mean rewards for the matching actions by considering the history of observations and decisions for groups of contexts that are similar to each other.",
      "startOffset" : 8,
      "endOffset" : 12
    }, {
      "referenceID" : 9,
      "context" : "Contextual zooming (CZ) [10]: This algorithm adaptively creates balls over the joint action and context space, calculates an index for each ball based on the history of selections of that ball, and at each time step selects a matching action according to the ball with the highest index that contains the current",
      "startOffset" : 24,
      "endOffset" : 28
    }, {
      "referenceID" : 0,
      "context" : "In YTM each instance (user) has two contexts (x1, x2) ∈ [0, 1].",
      "startOffset" : 56,
      "endOffset" : 62
    } ],
    "year" : 2015,
    "abstractText" : "The last decade has witnessed a tremendous growth in the volume as well as the diversity of multimedia content generated by a multitude of sources (news agencies, social media, etc.). Faced with a variety of content choices, consumers are exhibiting diverse preferences for content; their preferences often depend on the context in which they consume content as well as various exogenous events. To satisfy the consumers’ demand for such diverse content, multimedia content aggregators (CAs) have emerged which gather content from numerous multimedia sources. A key challenge for such systems is to accurately predict what type of content each of its consumers prefers in a certain context, and adapt these predictions to the evolving consumers’ preferences, contexts and content characteristics. We propose a novel, distributed, online multimedia content aggregation framework, which gathers content generated by multiple heterogeneous producers to fulfill its consumers’ demand for content. Since both the multimedia content characteristics and the consumers’ preferences and contexts are unknown, the optimal content aggregation strategy is unknown a priori. Our proposed content aggregation algorithm is able to learn online what content to gather and how to match content and users by exploiting similarities between consumer types. We prove bounds for our proposed learning algorithms that guarantee both the accuracy of the predictions as well as the learning speed. Importantly, our algorithms operate efficiently even when feedback from consumers is missing or content and preferences evolve over time. Illustrative results highlight the merits of the proposed content aggregation system in a variety of settings.",
    "creator" : "LaTeX with hyperref package"
  }
}