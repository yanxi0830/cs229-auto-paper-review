{
  "name" : "1409.2579.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "A theoretical contribution to the fast implementation of null linear discriminant analysis method using random matrix multiplication with scatter matrices",
    "authors" : [ "Ting-ting Feng", "Gang Wu" ],
    "emails" : [ "tofengtingting@163.com.", "gangwu76@126.com", "wugangzy@gmail.com." ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n40 9.\n25 79\nv1 [\ncs .N\nA ]\n9 S\nep 2\n01 4\nThe null linear discriminant analysis method is a competitive approach for dimensionality reduction. The implementation of this method, however, is computationally expensive. Recently, a fast implementation of null linear discriminant analysis method using random matrix multiplication with scatter matrices was proposed. However, if the random matrix is chosen arbitrarily, the orientation matrix may be rank deficient, and some useful discriminant information will be lost. In this paper, we investigate how to choose the random matrix properly, such that the two criteria of the null LDA method are satisfied theoretically. We give a necessary and sufficient condition to guarantee full column rank of the orientation matrix. Moreover, the geometric characterization of the condition is also described. Keywords:Dimensionality reduction, Linear discriminant analysis (LDA), Null linear discriminant analysis (Null LDA), Small sample size problem."
    }, {
      "heading" : "1 Introduction",
      "text" : "Dimensionality reduction has become an ubiquitous preprocessing step in many applications. In general, its\nobjectives are to remove irrelevant and redundant data to reduce the computational cost and to improve the\nquality of data for efficient data-intensive processing tasks such as face recognition and data mining. Linear\ndiscriminant analysis (LDA) is one of the most popular and powerful dimensionality reduction techniques\nfor classification (Fukunaga, 1990). However, a main disadvantage of LDA is that the so-called total scatter\nmatrix must be nonsingular. Indeed, in many applications, the scatter matrices can be singular since the\ndata points are from a very high-dimensional space, and thus usually the number of the data samples is\nmuch smaller than the data dimension. This is the well-known small sample size (SSS) problem or the\nundersampled problem (Fukunaga, 1990).\nLet X = [x1,x2, . . . ,xn] be a set of training samples in a d-dimensional feature space, and Ω = {ωj : j = 1, 2, . . . , c} be the class labels, with ωj being the j-th class. We denote by nj the number of samples in the j-th class, which satisfies ∑c\nj=1 nj = n. Let µj be the centroid of the j-th class, and µ be the global centroid\nof the training data set. Then we define the within-class scatter matrix\nSW =\nc ∑\nj=1\n∑\nxi∈ωj\n(xi − µj)(xi − µj)T ,\n1School of Mathematics and statistics, Jiangsu Normal University, Xuzhou, 221116, Jiangsu, P.R. China. Email: tofengtingting@163.com. This author is supported by the Postgraduate Innovation Project of Jiangsu Province under grant CXLX13 968. 2Corresponding author (G. Wu). Department of Mathematics, China University of Mining and Technology & School of Mathematics and statistics, Jiangsu Normal University, Xuzhou, 221116, Jiangsu, P.R. China. Email: gangwu76@126.com and wugangzy@gmail.com. This author is supported by the National Science Foundation of China under grant 11371176, the Natural Science Foundation of Jiangsu Province under grant BK20131126, the 333 Project of Jiangsu Province, and the Talent Introduction Program of China University of Mining and Technology.\nand the between-class scatter matrix\nSB =\nc ∑\nj=1\nnj(µj − µ)(µj − µ)T ≡ BBT ,\nwhere B = [ √ n1(µ1 − µ), √ n2(µ2 − µ), . . . , √ nc(µc − µ)] ∈ Rd×c. The total scatter matrix is defined as\nST =\nn ∑\nj=1\n(xj − µ)(xj − µ)T ,\nmoreover, it is known that (Fukunaga, 1990)\nST = SW + SB.\nWithout loss of generality, we assume that the n training vectors are linear independent. Consequently, the ranks of the matrices ST , SB and SW are n− 1, c− 1 and n− c, respectively. The LDA method is realized by maximizing the between-class scatter distance while minimizing the total\nscatter (or the within-class scatter) distance (Fukunaga 1990). However, when the dimension of data is much\nlarger than the number of training samples, the total scatter matrix ST (or the within scatter matrix SW ) will be singular, and we suffer from the small sample size problem (Fukunaga, 1990).\nThe null linear discriminant analysis (null LDA) method (Chen et al., 2000) is a competitive approach\nto overcome this difficulty. It first computes the null space of the within-class scatter matrix SW , and then computes the principal components of the between-class scatter matrix SB within the null space of SW . In essence, the null LDA method is to find the orientation (or the transformation) matrix W = [w1,w2, . . . ,wh] ∈ Rd×h (of rank h with 1 ≤ h ≤ c− 1) that satisfies the following two conditions (Sharma et al., 2012)\nSWW = 0, (1.1)\nand\nSBW 6= 0. (1.2)\nWhen ST is singular, the null LDA method solves\nW = S†TSBW, (1.3)\nfor the orientation matrix W , where S†T stands for the pseudo inverse (or the Moore-Penrose inverse) of ST . In (Sharma et al., 2012), it was shown that the equation (1.3) is a sufficient condition for the null LDA method. However, the null LDA method requires eigenvalue decomposition of S†TSB, and the computational cost will be prohibitive when d is large. In order to release the overhead, Sharma and Paliwal (Sharma et al., 2012) propose to replace W on the right-hand side of (1.3) by any random matrix Y ∈ Rd×(c−1) of rank c− 1, and make use of\nW = S†TSBY (1.4)\nas the orientation matrix, moreover, they present a fast implementation of null LDA method in (Sharma et\nal., 2012). In recent years, this method has gained wide attentions in the area of dimensionality reduction\nand data mining (Alvarez-Ginarte et al., 2013; Lu et al., 2013; Lyons et al., 2014; Sharma et al., 2014).\nThe following theorem is the main theorem of (Sharma et al., 2012). It shows that (1.4) is a sufficient\ncondition for null LDA. Meanwhile, it is also the basis of the fast implementation of null LDA method\n(Sharma et al., 2012); for more details, we refer to (Sharma et al., 2012).\nTheorem 1. [Theorem 3 of (Sharma et al., 2012)] If the orientation matrix W ∈ Rd×(c−1) is obtained by using the relation W = S†TSBY (where Y ∈ Rd×(c−1) is any random matrix of rank c−1), then it satisfies the two criteria on null LDA method ( Eqs. (1.1) and (1.2) ) .\nRemark 1. However, we find that this theorem is incomplete. For example, let X = [x1,x2;x3,x4], where {x1,x2} ∈ ω1 and {x3,x4} ∈ ω2. Suppose that µ1 = x1+x22 = ê and µ2 = x3+x42 = 2ê, where\nê = [1, 0, 1, 1, . . . , 1]T ∈ Rd,\nwith d ≫ n = 4. Therefore, µ = (x1 +x2 +x3 +x4)/4 = 32 ê, B = [ √ 2(µ1 −µ), √ 2(µ2 −µ)] = [− √ 2 2 ê, √ 2 2 ê], and\nSB = BB T = êêT .\nNote that rank(SB) = c− 1 = 1. In terms of Theorem 1, as Y can be chosen as any random vector, we pick\nY = [0, α, 0, . . . , 0]T ∈ Rd,\nwhere α is any positive number that satisfies 0 < α < 1. Then, SBY = 0, W = S † TSBY = 0, and\nSBW = 0,\nwhich does not satisfy the criterion (1.2).\nAs a result, if the random matrix is chosen arbitrarily, the orientation matrix may be rank deficient,\nand some discriminant information is lost. In this paper, we revisit the fast implementation of null linear\ndiscriminant analysis method and consider how to choose the random matrix properly, such that the two\ncriteria (1.1) and (1.2) of the null LDA method are satisfied theoretically. We give a necessary and sufficient\ncondition to guarantee that the orientation matrix W from (1.4) is of full column rank. Moreover, the\ngeometric characterization of this condition is also investigated."
    }, {
      "heading" : "2 The main result",
      "text" : "Since the n training vectors {x}ni=1 are linear independent, and the orientation matrix W is required to be of full column rank in the null LDA method, in this paper, we focus on how to choose Y ∈ Rd×(c−1) (of rank c− 1) in (1.4), such that rank(W ) = c− 1. We follow the notations used in (Sharma et al., 2012).\nLet\nST = UΣ 2UT = [U1, U2]\n[\nΣ21 0\n0 0\n][\nUT1 UT2\n]\nbe the eigenvalue decomposition of ST , where U1 ∈ Rd×(n−1) corresponds to the range of ST , U2 ∈ R\nd×(d−n+1) corresponds to the null space of ST , and Σ1 ∈ R(n−1)×(n−1) is a diagonal matrix with positive diagonal elements. From now on, we denote G = S†TSB for notation simplicity. By Lemma A3 of (Sharma et al., 2012), we have that\nG = S†TSB = U\n[\nΣ−21 0\n0 0\n]\nUTSBUU T = U\n[\nΣ−21 U T 1 SBU1 0\n0 0\n]\nUT ,\nand\nGU = U\n[\nΣ−21 U T 1 SBU1 0\n0 0\n]\n.\nRecall that SB = BB T , thus\nGU = U\n[\nΣ−11 Σ −1 1 U T 1 BB TU1Σ −1 1 Σ1 0\n0 0\n]\n.\nLet Q = Σ−11 U T 1 B, and QQ T = RΛRT be the eigenvalue decomposition, where R ∈ R(n−1)×(n−1) is orthonormal and Λ ∈ R(n−1)×(n−1) is diagonal. So we arrive at\nG[U1, U2] = [U1, U2]\n[\nΣ−11 RΛR TΣ1 0\n0 0\n]\n. (2.5)\nThat is,\nGU1 = U1Σ −1 1 RΛR TΣ1, (2.6)\nand\nGU2 = 0. (2.7)\nMoreover, it was proven in Lemma A2 of (Sharma et al., 2012) that Λ =\n[\nIc−1 0\n0 0\n]\n, where Ic−1 is the\n(c− 1)× (c− 1) identity matrix. It follows from (2.6) that\nGU1Σ −1 1 R = U1Σ −1 1 RΛ = U1Σ −1 1 R\n[\nIc−1 0\n0 0\n]\n.\nNotice that span{U1Σ−11 R} = span{U1}. Decompose U1Σ−11 R = [Û1, Û2], where Û1 ∈ Rd×(c−1) is the matrix composed of the first c− 1 columns of U1Σ−11 R, and Û2 ∈ Rd×(n−c), then\nG[Û1, Û2] = [Û1, Û2]\n[\nIc−1 0\n0 0\n]\n= [Û1, 0],\ni.e.,\nGÛ1 = Û1 and GÛ2 = 0. (2.8)\nRemark 2. Denote U = [U1Σ−11 R, U2] = [Û1, Û2, U2] ∈ Rd×d, it is seen that the columns of U construct a basis in Rd, moreover, we have that\nrank ( [Û2, U2] ) = d− c+ 1 ≫ 1.\nTherefore, if the d× (c− 1) matrix Y ∈ span{Û2, U2}, then it follows from (2.7) and (2.8) that W = GY = S†TSBY = 0, SBW = 0, and Theorem 1 fails to hold.\nNext, we aim to give a necessary and sufficient condition for rank(W ) = c − 1. As the columns of U = [Û1, Û2, U2] construct a basis of Rd, for any matrix Y ∈ Rd×(c−1), there exists a matrix [ẐT1 , ẐT2 , ZT2 ]T ∈ R d×(c−1), such that\nY = [Û1, Û2, U2]\n\n \nẐ1 Ẑ2 Z2\n\n  = Û1Ẑ1 + Û2Ẑ2 + U2Z2. (2.9)\nThus,\n[ẐT1 , Ẑ T 2 , Z T 2 ] T = U−1Y,\nand Ẑ1 = (U−1Y )(1 : c− 1, :) ∈ R(c−1)×(c−1) is the first c− 1 rows of U−1Y . Here (U−1Y )(1 : c− 1, :) stands for the first c− 1 rows of the matrix U−1Y .\nFrom (1.4), (2.7), (2.8) and (2.9), we obtain\nW = S†TSBY = S † TSB[Û1, Û2, U2]\n\n \nẐ1 Ẑ2 Z2\n\n  = S†TSBÛ1Ẑ1\n= GÛ1Ẑ1 = Û1Ẑ1. (2.10)\nSince Û1 is of full column rank, we have from (2.10) that rank(W ) = c− 1 if and only if rank(Ẑ1) = c− 1, i.e., Ẑ1 is nonsingular.\nWe are in a position to consider how to evaluate Ẑ1 in practice. Recall that U = [Û1, Û2, U2] = [U1(Σ −1 1 R), U2]. Let Σ −1 1 R = Q̂R̂ be the QR decomposition, where Q̂ ∈ R(n−1)×(n−1) is an orthogonal matrix and R̂ ∈ R(n−1)×(n−1) is an upper triangular matrix, then\nU = [U1Σ−11 R, U2] = [U1Q̂R̂, U2] = [U1Q̂, U2] [ R̂ 0\n0 Id−n+1\n]\n,\nis the QR decomposition of U , where [U1Q̂, U2] is orthonormal and In−d+1 is the (n− d+ 1)× (n− d+ 1) identity matrix. Thus,\nU−1Y = [ R̂−1 0\n0 In−d+1\n][\nQ̂TUT1 UT2\n]\nY\n=\n[\nR̂−1 0\n0 In−d+1\n][\nQ̂TUT1 Y\nUT2 Y\n]\n=\n[\nR̂−1Q̂TUT1 Y\nUT2 Y\n]\n. (2.11)\nLet R̂−1 =\n[\nR̂T1 R̂T2\n]\n, where R̂T1 ∈ R(c−1)×(n−1) is composed of the first c − 1 rows of R̂−1, and R̂T2 ∈\nR (n−c)×(n−1) is composed of the last n− c rows of R̂−1. So we obtain from (2.11) that\nẐ1 = (U−1Y )(1 : c− 1, :) = (R̂−1Q̂TUT1 Y )(1 : c− 1, :) = ( U1Q̂R̂1 )T Y. (2.12)\nFurthermore, if rank(Ẑ1) = c−1, then we have from (2.10) that W = S†TSBY is of rank c−1. According to Lemma A3 of (Sharma et al., 2012), we have\nS†TSBW = (S † TSB)(S † TSB)Y = S † TSBY = W,\nand it follows from Theorem 1 and Theorem 2 of (Sharma et al., 2012) that W satisfies the null LDA criteria\n(1.1) and (1.2).\nIn summary, we have the main theorem that is a modification to Theorem 1 [Theorem 3 in (Sharma et\nal., 2012)].\nTheorem 2. Let Y ∈ Rd×(c−1) be a random matrix of rank c− 1, and let\nẐ1 = ( U1Q̂R̂1 )T Y (13)\nbe the (c− 1)× (c− 1) matrix composed of the first c− 1 rows of U−1Y . Then W = S†TSBY is of rank c− 1 if and only if Ẑ1 is nonsingular. Moreover, if Ẑ1 is nonsigular, then W = S † TSBY satisfies the criteria of the null LDA method ( Eqs. (1.1) and (1.2) ) .\nNotice that U1Q̂R̂1 is of full rank. Given a random matrix Y ∈ Rd×(c−1), the following theorem describes the geometric characterization of the condition for Ẑ1 being nonsingular.\nTheorem 3. Suppose that Y ∈ Rd×(c−1) is of full column rank, and denote by span{Y } the subspace spanned by the columns of Y . Let K = span{Y } and L = span{U1Q̂R̂1}, then Ẑ1 is nonsingular if and only if any nonzero vector x ∈ K (or y ∈ L), it is not orthogonal to L (or K).\nProof. The proof is by contradiction. On one hand, suppose that there is a nonzero vector x ∈ K and x ⊥ L. Then there exists a nonzero vector z ∈ Rc−1, such that x = Y z. Since x ⊥ L, we obtain\n0 = (U1Q̂R̂1) Tx =\n( U1Q̂R̂1 )T Y z = Ẑ1z,\nand Ẑ1 is singular. This shows that, if Ẑ1 is nonsingular, then for any nonzero vector x ∈ K, it is not orthogonal to L. On the other hand, we assume that Ẑ1 is singular. Then there is a nonzero vector z ∈ Rc−1, such that Ẑ1z = ( U1Q̂R̂1 )T\nY z = 0. Let x ≡ Y z ∈ K, then x 6= 0, and it is orthogonal to L. This implies that, if for any nonzero vector x ∈ K, it is not orthogonal to L, then Ẑ1 is nonsingular.\nRemark 3. Given a random matrix Y , Theorem 2 can be utilized to check whether W is of full rank a prior\nin the fast implementation of the null LDA method (Sharma et al., 2012). Indeed, it indicates that W is of rank c − 1 if and only if Ẑ1 = ( U1Q̂R̂1 )T Y is nonsingular. Equivalently, Theorem 3 shows that this only happens if and only if for any nonzero vector x in K = span{Y }, it is not orthogonal to L = span{U1Q̂R̂1}, moreover, for any nonzero vector y in L = span{U1Q̂R̂1}, it is not orthogonal to K = span{Y }.\nIn practice, however, the case of a “near singular” (i.e., the smallest eigenvalue is not zero but is close\nto zero) Ẑ1 can occur if Y is chosen arbitrarily (Golub et al., 2013). Consequently, W will be near rank deficient and the two criteria of the null LDA method can not be satisfied any more. In this situation, we\nsuggest using another random matrix Y instead."
    }, {
      "heading" : "Acknowledgments",
      "text" : "The first author is supported by the Postgraduate Innovation Project of Jiangsu Province under grant\nCXLX13 968. The second author is supported by the National Science Foundation of China under grant\n11371176, the Natural Science Foundation of Jiangsu Province under grant BK20131126, the 333 Project\nof Jiangsu Province, and the Talent Introduction Program of China University of Mining and Technology.\nMeanwhile, the authors would like to thank Ting-ting Xu for helpful discussions."
    } ],
    "references" : [ {
      "title" : "Integration of ligand and structure-based virtual screening for identification of leading anabolic steroids, The Journal of steroid biochemistry and molecular biology",
      "author" : [ "Y. Alvarez-Ginarte", "L Montero-Cabrera" ],
      "venue" : "Pattern Recognition,",
      "citeRegEx" : "Alvarez.Ginarte and Montero.Cabrera,? \\Q2013\\E",
      "shortCiteRegEx" : "Alvarez.Ginarte and Montero.Cabrera",
      "year" : 2013
    }, {
      "title" : "Complexity-reduced implementations of complete and null-space-based linear discriminant analysis",
      "author" : [ "G.H. Golub", "C.F. Van Loan" ],
      "venue" : "Matrix Computations,",
      "citeRegEx" : "Golub and Loan,? \\Q2013\\E",
      "shortCiteRegEx" : "Golub and Loan",
      "year" : 2013
    }, {
      "title" : "Protein fold recognition by alignment of amino acid residues using kernelized dynamic time warping",
      "author" : [ "J. Lyons", "N. Biswas", "A Sharma" ],
      "venue" : "Journal of theoretical biology,",
      "citeRegEx" : "Lyons et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Lyons et al\\.",
      "year" : 2014
    } ],
    "referenceMentions" : [ {
      "referenceID" : 2,
      "context" : "In recent years, this method has gained wide attentions in the area of dimensionality reduction and data mining (Alvarez-Ginarte et al., 2013; Lu et al., 2013; Lyons et al., 2014; Sharma et al., 2014).",
      "startOffset" : 112,
      "endOffset" : 200
    } ],
    "year" : 2014,
    "abstractText" : "The null linear discriminant analysis method is a competitive approach for dimensionality reduction. The implementation of this method, however, is computationally expensive. Recently, a fast implementation of null linear discriminant analysis method using random matrix multiplication with scatter matrices was proposed. However, if the random matrix is chosen arbitrarily, the orientation matrix may be rank deficient, and some useful discriminant information will be lost. In this paper, we investigate how to choose the random matrix properly, such that the two criteria of the null LDA method are satisfied theoretically. We give a necessary and sufficient condition to guarantee full column rank of the orientation matrix. Moreover, the geometric characterization of the condition is also described.",
    "creator" : "LaTeX with hyperref package"
  }
}