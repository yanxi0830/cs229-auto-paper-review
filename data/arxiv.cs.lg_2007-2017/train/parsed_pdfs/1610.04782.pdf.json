{
  "name" : "1610.04782.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "An Adaptive Test of Independence with Analytic Kernel Embeddings",
    "authors" : [ "Wittawat Jitkrittum", "Zoltán Szabó", "Arthur Gretton" ],
    "emails" : [ "wittawat@gatsby.ucl.ac.uk", "zoltan.szabo@polytechnique.edu", "arthur.gretton@gmail.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "A new computationally efficient dependence measure, and an adaptive statistical test of independence, are proposed. The dependence measure is the difference between analytic embeddings of the joint distribution and the product of the marginals, evaluated at a finite set of locations (features). These features are chosen so as to maximize a lower bound on the test power, resulting in a test that is data-efficient, and that runs in linear time (with respect to the sample size n). The optimized features can be interpreted as evidence to reject the null hypothesis, indicating regions in the joint domain where the joint distribution and the product of the marginals differ most. Consistency of the independence test is established, for an appropriate choice of features. In real-world benchmarks, independence tests using the optimized features perform comparably to the state-of-the-art quadratictime HSIC test, and outperform competing O(n) and O(n log n) tests."
    }, {
      "heading" : "1 Introduction",
      "text" : "We consider the design of adaptive, nonparametric statistical tests of dependence: that is, tests of whether a joint distribution Pxy factorizes into the product of marginals PxPy. While classical tests of dependence, such as Pearson’s correlation and Kendall’s τ , are able to detect monotonic relations between univariate variables, more modern tests can address complex interactions, for instance changes in variance of X with the value of Y . Key to many recent tests is to examine covariance or correlation between data features. These interactions become significantly harder to detect, and the features\n∗Zoltán Szabó’s ORCID ID: http://orcid.org/ 0000-0001-6183-7603.\nare more difficult to design when the data reside in high dimensions.\nA basic nonlinear dependence measure is the HilbertSchmidt Independence Criterion (HSIC), which is the Hilbert-Schmidt norm of the covariance operator between feature mappings of the random variables [Gretton et al., 2005, 2008]. Each random variable X and Y is mapped to a respective reproducing kernel Hilbert space Hk and Hl. For sufficiently rich mappings, the covariance operator norm is zero if and only if the variables are independent. A second basic nonlinear dependence measure is the smoothed difference between the characteristic function of the joint distribution, and that of the product of marginals. When a particular smoothing function is used, the statistic corresponds to the covariance between distances of X and Y variable pairs [Feuerverger, 1993, Székely et al., 2007, Székely and Rizzo, 2009], yielding a simple test statistic. It has been shown by Sejdinovic et al. [2013] that the distance covariance (and its generalization to semi-metrics) is an instance of HSIC for an appropriate choice of kernels. A disadvantage of these feature covariance statistics, however, is that they require quadratic time to compute (besides in the special case of the distance covariance with univariate real-valued variables, where Huo and Székely [2014] achieve an O(n log n) cost). Moreover, the feature covariance statistics have intractable null distributions, and either a permutation approach or the solution of an expensive eigenvalue problem [e.g. Zhang et al., 2011] is required for consistent estimation of the quantiles. Several approaches were proposed by Zhang et al. [2016] to obtain faster tests along the lines of HSIC. These include computing HSIC on finite-dimensional feature mappings chosen as random Fourier features (RFFs) [Rahimi and Recht, 2008], a block-averaged statistic, and a Nyström approximation to the statistic. Key to each of these approaches is a more efficient computation of the statistic and its threshold un-\nar X\niv :1\n61 0.\n04 78\n2v 1\n[ st\nat .M\nL ]\n1 5\nO ct\nder the null distribution: for RFFs, the null distribution is a finite weighted sum of χ2 variables; for the blockaveraged statistic, the null distribution is asymptotically normal; for Nyström, either a permutation approach is employed, or the spectrum of the Nyström approximation to the kernel matrix is used in approximating the null distribution. Each of these methods costs significantly less than the O(n2) cost of the full HSIC (the cost is linear in n, but also depends quadratically on the number of features retained). A potential disadvantage of the Nyström and Fourier approaches is that the features are not optimized to maximize test power, but are chosen randomly. The block statistic performs worse than both, due to the large variance of the statistic under the null (which can be mitigated by observing more data).\nIn addition to feature covariances, correlation measures have also been developed in infinite dimensional feature spaces: in particular, Bach and Jordan [2002], Fukumizu et al. [2008] proposed statistics on the correlation operator in a reproducing kernel Hilbert space. While convergence has been established for certain of these statistics, their computational cost is high at O(n3), and test thresholds have relied on permutation. A number of much faster approaches to testing based on feature correlations have been proposed, however. For instance, Dauxois and Nkiet [1998] compute statistics of the correlation between finite sets of basis functions, chosen for instance to be step functions or low order B-splines. The cost of this approach is O(n). This idea was extended by Lopez-Paz et al. [2013], who computed the canonical correlation between finite sets of basis functions chosen as random Fourier features; in addition, they performed a copula transform on the inputs, with a total cost of O(n log n). Finally, space partitioning approaches have also been proposed, based on statistics such as the KL divergence, however these apply only to univariate variables [Heller et al., 2016], or to multivariate variables of low dimension [Gretton and Györfi, 2010] (that said, these tests have other advantages of theoretical interest, notably distribution-independent test thresholds).\nThe approach we take is most closely related to HSIC on a finite set of features. Our simplest test statistic, the Finite Set Independence Criterion (FSIC), is an average of covariances of analytic functions (i.e., features) defined on each of X and Y . A normalized version of the statistic (NFSIC) yields a distribution-independent asymptotic test threshold. We show that our test is consistent, despite a finite number of analytic features being used, via a generalization of arguments in Chwialkowski et al. [2015]. As in recent work on two-sample testing by Jitkrittum et al. [2016], our test is adaptive in the sense that we choose our features on a held-out validation set to optimize a lower bound on the test power. The design of features for independence testing turns out\nto be quite different to the case of two-sample testing, however: the task is to find correlated feature pairs on the respective marginal domains, rather than attempting to find a single, high-dimensional feature representation for the entire (x, y) (as we would need to do if we were comparing distributions Pxy and Qxy, rather than testing a specific property of Pxy). We demonstrate the performance of our tests on several challenging artificial and real-world datasets, including detection of dependence between music and its year of appearance, and between videos and captions. In these experiments, we outperform competing linear and O(n log n) time tests."
    }, {
      "heading" : "2 Independence Criteria and Statistical Tests",
      "text" : "We introduce two test statistics: first, the Finite Set Independence Criterion (FSIC), which builds on the principle that dependence can be measured in terms of the covariance between data features. Next, we propose a normalized version of this statistic (NFSIC), with a simpler asymptotic distribution when Pxy = PxPy. We show how to select features for the latter statistic to maximize a lower bound on the power of its corresponding statistical test."
    }, {
      "heading" : "2.1 The Finite Set Independence Criterion",
      "text" : "We begin by introducing the Hilbert-Schmidt Independence Criterion (HSIC) as proposed in Gretton et al. [2005], since our unnormalized statistic is built along similar lines. Consider two random variables X ∈ X ⊂ Rdx and Y ∈ Y ⊂ Rdy . Denote by Pxy the joint distribution between X and Y ; Px and Py are the marginal distributions of X and Y . Let ⊗ denote the tensor product, such that (a⊗ b) c = a 〈b, c〉. Assume that k : X × X → R and l : Y ×Y → R are positive definite kernels associated with reproducing kernel Hilbert spaces (RKHS) Hk and Hl, respectively. Let ‖ · ‖HS be the norm on the space of Hl → Hk Hilbert-Schmidt operators. Then, HSIC between X and Y is defined as\nHSIC(X,Y ) = ∥∥µxy − µx ⊗ µy∥∥2HS\n= E(x,y),(x′,y′) [k(x,x′)l(y,y′)] + ExEx′ [k(x,x′)]EyEy′ [l(y,y′)] − 2E(x,y) [Ex′ [k(x,x′)]Ey′ [l(y,y′)]] , (1)\nwhere Ex := Ex∼Px , Ey := Ey∼Py , E(x,y) := E(x,y)∼Pxy , and x′ is an independent copy of x. The mean embedding of Pxy belongs to the space of HilbertSchmidt operators from Hl to Hk, µxy := ∫ X×Y k(x, ·)⊗ l(y, ·) dPxy(x,y) ∈ HS(Hl,Hk), and the marginal mean\nembeddings are µx := ∫ X k(x, ·) dPx(x) ∈ Hk and\nµy := ∫ Y l(y, ·) dPy(y) ∈ Hl [Smola et al., 2007]. Gretton et al. [2005, Theorem 4] show that if the kernels k and l are universal [Steinwart and Christmann, 2008] on compact domains X and Y, then HSIC(X,Y ) = 0 if and only if X and Y are independent. Alternatively, Gretton [2015] shows that it is sufficient for each of k and l to be characteristic to their respective domains (meaning that distribution embeddings are injective in each marginal domain: see Sriperumbudur et al. [2010]). Given a joint sample Zn = {(xi,yi)}ni=1 ∼ Pxy, an empirical estimator of HSIC can be computed in O(n2) time by replacing the population expectations in (1) with their corresponding empirical expectations based on Zn.\nWe now propose our new linear-time dependence measure, the Finite Set Independence Criterion (FSIC). Let X ⊂ Rdx and Y ⊂ Rdy be open sets. Define the empirical measure ν := 1J ∑J i=1 δ(vi,wi) over J test locations VJ := {(vi,wi)}Ji=1 ⊂ X × Y where δt denotes the Dirac measure centered on t, and (vi,wi) are realizations from an absolutely continuous distribution (wrt the Lebesgue measure). Write Exy for E(x,y)∼Pxy . The idea is to see µxy(v,w) = Exy[k(x,v)l(y,w)], µx(v) = Ex[k(x,v)] and µy(w) = Ey[l(y,w)] as smooth functions, and consider an L2(X × Y, ν) distance between µxy and µxµy instead of a Hilbert-Schmidt distance as in HSIC [Gretton et al., 2005]. Let µxµy(x,y) := µx(x)µy(y). FSIC is defined as\nFSIC2(X,Y ) := ‖µxy − µxµy‖2L2(X×Y,ν)\n= ∫ X ∫ Y (µxy(x,y)− µx(x)µy(y))2 dν(x,y)\n:= 1\nJ J∑ i=1 u(vi,wi) 2 = 1 J ‖u‖22, where\nu(v,w) := µxy(v,w)− µx(v)µy(w) = Exy[k(x,v)l(y,w)]− Ex[k(x,v)]Ey[l(y,w)], (2) = covxy[k(x,v), l(y,w)],\nand u := (u(v1,w1), . . . , u(vJ ,wJ))>. Our first result in Proposition 2 states that FSIC(X,Y ) almost surely defines a dependence measure for the random variables X and Y , provided that the product kernel on the joint space X × Y is characteristic and analytic (see Definition 1).\nDefinition 1 (Analytic kernels [Chwialkowski et al., 2015]). Let X be an open set in Rd. A positive definite kernel k : X × X → R is said to be analytic on its domain X × X if for all v ∈ X , f(x) := k(x,v) is an analytic function on X .\nAssumption A. The kernels k : X × X → R and l : Y × Y → R are bounded by Bk and Bl respectively, and\nthe product kernel g((x,y), (x′,y′)) := k(x,x′)l(y,y′) is characteristic [Sriperumbudur et al., 2010, Definition 6], and analytic (Definition 1) on (X × Y)× (X × Y).\nProposition 2 (FSIC is a dependence measure). Assume that 1. Assumption A holds.\n2. The test locations VJ = {(vi,wi)}Ji=1 are drawn from an absolutely continuous distribution.\nThen, almost surely, FSIC(X,Y ) = 1√ J ‖u‖2 = 0 if and only if X and Y are independent.\nProof. Since g is characteristic, the mean embedding map Πg : P 7→ E(x,y)∼P [g((x,y), ·)] is injective [Sriperumbudur et al., 2010, Section 3], where P is a probability distribution on X × Y . Since g is analytic, by Lemma 10 (Appendix), µxy and µxµy are analytic functions. Thus, Lemma11 (Appendix, setting Λ = Πg) guarantees that FSIC(X,Y ) = 0 ⇐⇒ Pxy = PxPy ⇐⇒ X and Y are independent almost surely.\nFSIC uses µxy as a proxy for Pxy, and µxµy as a proxy for PxPy. Proposition 2 suggests that, to detect the dependence betweenX and Y , it is sufficient to evaluate at a finite number of locations (defined by VJ ) the difference of the population joint embedding µxy and the embedding of the product of the marginal distributions µxµy. A brief explanation to justify this property is as follows. If Pxy = PxPy, then ρ(v,w) := µxy(v,w)−µxµy(v,w) is zero, and FSIC(X,Y ) = 0 for any VJ . If Pxy 6= PxPy, then ρ will not be a zero function, since the mean embedding map is injective (require the product kernel to be characteristic). Using the same argument as in Chwialkowski et al. [2015], since k and l are analytic, ρ is also analytic, and the set of roots R := {(v,w) | ρ(v,w) = 0} has Lebesgue measure zero. Thus, it is sufficient to draw (v,w) from an absolutely continuous distribution, as we are guaranteed that (v,w) /∈ R giving FSIC(X,Y ) > 0. For FSIC to be a dependence measure, the product kernel is required to be characteristic and analytic. We next show in Proposition 3 that Gaussian kernels k and l yield such a product kernel.\nProposition 3 (A product of Gaussian kernels is characteristic and analytic). Let k(x,x′) = exp ( −(x− x′)>A(x− x′) ) and\nl(y,y′) = exp ( −(y − y′)>B(y − y′) ) be Gaussian kernels on Rdx × Rdx and Rdy × Rdy respectively, for positive definite matrices A and B. Then, g((x,y), (x′,y′)) = k(x,x′)l(y,y′) is characteristic and analytic on (Rdx × Rdy )× (Rdx × Rdy ). Proof (sketch). The main idea is to use the fact a Gaussian kernel is analytic, and a product of Gaussian kernels is a Gaussian kernel on the pair of variables. See the full proof in Appendix D.\nPlug-in Estimator We now give an empirical estimator of FSIC. Assume that we observe a joint sample Zn := {(xi,yi)}ni=1\ni.i.d.∼ Pxy. Unbiased estimators of µxy(v,w) and µxµy(v,w) are µ̂xy(v,w) := 1n ∑n i=1 k(xi,v)l(yi,w)\nand µ̂xµy(v,w) := 1n(n−1) ∑n i=1 ∑ j 6=i k(xi,v)l(yj ,w), respectively. A straightforward empirical estimator of FSIC2 is then given by\nF̂SIC2(Zn) = 1\nJ J∑ i=1 û(vi,wi) 2,\nû(v,w) := µ̂xy(v,w)− µ̂xµy(v,w) (3)\n= 2 n(n− 1) ∑ i<j h(v,w)((xi,yi), (xj ,yj)), (4)\nwhere h(v,w)((x,y), (x′,y′)) := 12 (k(x,v) − k(x′,v))(l(y,w) − l(y′,w)). For conciseness, we define û := (û1, . . . , ûJ)> ∈ RJ where ûi := û(vi,wi) so that F̂SIC2(Zn) = 1J û\n>û. F̂SIC2 can be efficiently computed in O((dx + dy)Jn) time [see (3)], assuming that the runtime complexity of evaluating k(x,v) is O(dx) and that of l(y,w) is O(dy). The unbiasedness of µ̂xµy is necessary for (4) to be a Ustatistic. This fact and the rewriting of F̂SIC2 in terms of h(v,w)((x,y), (x\n′,y′)) will be exploited when the asymptotic distribution of û is derived (Proposition 4). Since FSIC satisfies FSIC(X,Y ) = 0 ⇐⇒ X ⊥ Y , in principle its empirical estimator can be used as a test statistic for an independence test proposing a null hypothesis H0 : “X and Y are independent” against an alternative H1 : “X and Y are dependent”. The null distribution (i.e., distribution of the test statistic assuming that H0 is true) is challenging to obtain, however and depends on the unknown Pxy. This prompts us to consider a normalized version of FSIC whose asymptotic null distribution of a convenient form. We first derive the asymptotic distribution of û in Proposition 4, which we use to derive the normalized test statistic in Theorem 5. As a shorthand, we write z := (x,y), and t := (v,w).\nProposition 4 (Asymptotic distribution of û). Define k̃(x,v) := k(x,v)−Ex′k(x′,v), and l̃(y,w) := l(y,w)− Ey′ l(y′,w). Then, under both H0 and H1, for any fixed locations t and t′,\ncovz[û(t), û(t ′)] n→∞−−−−→ covz[k̃(x,v)l̃(y,w), k̃(x,v′)l̃(y,w′)] = Exy[ ( k̃(x,v)l̃(y,w)− u(t) )( k̃(x,v′)l̃(y,w′)− u(t′) ) ],\nwhere u(v,w) is given in (2), and û(v,w) is defined in (4). Second, if 0 < covz[û(ti), û(ti)] < ∞ for i = 1, . . . , J , then √ n(û − u) d→ N (0,Σ) as n → ∞, where Σij = cov[û(ti), û(tj)] and u := (u(t1), . . . , u(tJ))>.\nProof. We first note that for a fixed t = (v,w), û(v,w) is a one-sample second-order U-statistic [Serfling, 2009, Section 5.1.3] with a U-statistic kernel ht where ht(a,b) =\nht(b,a). Thus, by Kowalski and Tu [2008, Section 5.1, Theorem 1], it follows directly that cov[û(t), û(t′)] = 4covz[Eaht(z,a),Ebht′(z,b)]. Substituting ht with its definition yields the first claim, where we note that Exy[k̃(x,v)l̃(y,w)] = u(v,w). For the second claim, since û is a multivariate onesample U-statistic, by Lehmann [1999, Theorem 6.1.6] and Kowalski and Tu [2008, Section 5.1, Theorem 1], it follows that √ n(û − u) d→ N (0,Σ) as n → ∞, where Σij = cov[û(ti), û(tj)].\nRecall from Proposition 2 that u = 0 holds almost surely under H0. The asymptotic normality in the second claim of Proposition 4 implies that nF̂SIC2 = nJ û\n>û converges in distribution to a sum of J dependent weighted χ2 random variables. The dependence comes from the fact that the coordinates û1 . . . , ûJ of û all depend on the sample Zn. This null distribution is not analytically tractable, and requires a large number of simulations to compute the rejection threshold Tα for a given significance value α."
    }, {
      "heading" : "2.2 Normalized FSIC and Adaptive Test",
      "text" : "For the purpose of an independence test, we will consider a normalized variant of F̂SIC2, which we call N̂FSIC2, whose tractable asymptotic null distribution is χ2(J), the chi-squared distribution with J degrees of freedom. We then show that the independence test defined by N̂FSIC2 is consistent. These results are given in Theorem 5.\nTheorem 5 (Independence test using N̂FSIC2 is consistent). Let Σ̂ be a consistent estimate of Σ based on the joint sample Zn. The N̂FSIC2 statistic is defined as\nλ̂n := nû > ( Σ̂ + γnI )−1 û where γn ≥ 0 is a regulariza-\ntion parameter. Assume that\n1. Assumption A holds."
    }, {
      "heading" : "2. Σ is invertible almost surely with respect to VJ =",
      "text" : "{(vi,wi)}Ji=1 drawn from an absolutely continuous distribution.\n3. limn→∞ γn = 0."
    }, {
      "heading" : "Then, for any k, l and VJ satisfying the assumptions,",
      "text" : "1. Under H0, λ̂n d→ χ2(J) as n→∞. 2. Under H1, for any r ∈ R, limn→∞ P ( λ̂n ≥ r ) = 1\nalmost surely. That is, the independence test based on N̂FSIC2 is consistent.\nProof (sketch) . Under H0, nû>(Σ̂ + γnI)−1û asymptotically follows χ2(J) because √ nû is asymptotically normally distributed (see Proposition 4). Claim 2 builds on the result in Proposition 2 stating that u 6= 0 under H1;\nit follows using the convergence of û to u. The full proof can be found in Appendix E.\nTheorem 5 states that if H1 holds, the statistic can be arbitrarily large as n increases, allowing H0 to be rejected for any fixed threshold. Asymptotically the test threshold Tα is given by the (1−α)-quantile of χ2(J) and is independent of n. The assumption on the consistency of Σ̂ is required to obtain the asymptotic chi-squared distribution. The regularization parameter γn is to ensure that (Σ̂ + γnI)−1 can be stably computed. In practice, γn requires no tuning, and can be set to be a very small constant. The next proposition states that the computational\ncomplexity of the N̂FSIC2 estimator is linear in both the input dimension and sample size, and that it can be expressed in terms of the K =[Kij ] = [k(vi,xj)] ∈ RJ×n,L = [Lij ] = [l(wi,yj)] ∈ RJ×n matrices.\nProposition 6 (An empirical estimator of N̂FSIC2). Let 1n := (1, . . . , 1)\n> ∈ Rn. Denote by ◦ the element-wise matrix product. Then,\n1. û = (K◦L)1nn−1 − (K1n)◦(L1n) n(n−1) .\n2. A consistent estimator for Σ is Σ̂ = ΓΓ >\nn where\nΓ := (K− n−1K1n1>n ) ◦ (L− n−1L1n1>n )− ûb1>n , ûb = n−1 (K ◦ L) 1n − n−2 (K1n) ◦ (L1n) .\nAssume that the complexity of the kernel evaluation is linear in the input dimension. Then the test statistic\nλ̂n = nû > ( Σ̂ + γnI )−1 û can be computed in O(J3 +\nJ2n+ (dx + dy)Jn) time.\nProof (sketch). Claim 1 for û is straightforward. The expression for Σ̂ in claim 2 follows directly from the asymptotic covariance expression in Proposition 4. The consistency of Σ̂ can be obtained by noting that the finite sample bound for P(‖Σ̂ − Σ‖F > t) decreases as n increases. This is implicitly shown in Appendix F.2.2 and its following sections.\nAlthough the dependency of the estimator on J is cubic, we empirically observe that only a small value of J is required (see Section 3). The number of test locations J relates to the number of regions in X × Y of pxy and pxpy that differ (see Figure 1). In particular, J need not increase with n for test consistency. Our final theoretical result gives a lower bound on\nthe test power of N̂FSIC2 i.e., the probability of correctly rejectingH0. We will use this lower bound as the objective function to determine VJ and the kernel parameters. Let ‖ · ‖F be the Frobenius norm.\nTheorem 7 (A lower bound on the test power). Let NFSIC2(X,Y ) := λn := nu\n>Σ−1u. Let K be a kernel class for k, L be a kernel class for l, and V be a collection with each element being a set of J locations. Assume that"
    }, {
      "heading" : "1. There exist finite Bk and Bl such that",
      "text" : "supk∈K supx,x′∈X |k(x,x′)| ≤ Bk and supl∈L supy,y′∈Y |l(y,y′)| ≤ Bl.\n2. c̃ := supk∈K supl∈L supVJ∈V ‖Σ −1‖F <∞.\nThen, for any k ∈ K, l ∈ L, VJ ∈ V, and λn ≥ r, the test power satisfies P ( λ̂n ≥ r ) ≥ L(λn) where\nL(λn) = 1− 62e−ξ1γ 2 n(λn−r) 2/n − 2e−b0.5nc(λn−r) 2/[ξ2n2]\n− 2e−[(λn−r)γn(n−1)/3−ξ3n−c3γ 2 nn(n−1)] 2 /[ξ4n2(n−1)],\nb·c is the floor function, ξ1 := 132c21J2B∗ , ξ2 := 72c 2 2JB 2, B := BkBl, ξ3 := 8c1B2J , c3 := 4B2Jc̃2, ξ4 := 28B4J2c21, c1 := 4B2J √ Jc̃, c2 := 4B √ Jc̃, and B∗ is a constant depending on only Bk and Bl. Moreover, for sufficiently large fixed n, L(λn) is increasing in λn.\nWe provide the proof in Appendix F. To put Theorem 7 into perspective, let θx and θy be the parameters of the kernels k and l, respectively. We denote by θ = {θx, θy, VJ} the collection of all tuning parameters of the test. Assume that K = { (x,v) 7→ exp ( −‖x−v‖ 2\n2σ2x\n) | σ2x ∈ [σ2x,l, σ2x,u] } =:\nKg for some 0 < σ2x,l < σ2x,u < ∞ and L ={ (y,w) 7→ exp ( −‖y−w‖ 2\n2σ2y\n) | σ2y ∈ [σ2y,l, σ2y,u] } =: Lg for\nsome 0 < σ2y,l < σ 2 y,u < ∞ are Gaussian kernel classes. Then, in Theorem 7, B = Bk = Bl = 1, and B∗ = 2. The assumption c̃ <∞ is a technical condition to guarantee that the test power lower bound is finite for all θ defined by the feasible sets K,L, and V. Let V ,r :={ VJ | ‖vi‖2, ‖wi‖2 ≤ r and ‖vi − vj‖22 + ‖wi −wj‖22 ≥ , for all i 6= j } . If we set K = Kg,L = Lg, and V = V ,r for some , r > 0, then c̃ < ∞ as Kg,Lg, and V ,r are compact. In practice, these conditions do not necessarily create restrictions as they almost always hold implicitly. We show in Appendix C that the objective function used to choose VJ will discourage any two locations to be in the same neighborhood. Parameter Tuning The test power lower bound L(λn) in Theorem 7 is a function of λn = nu>Σ−1u which is the population counterpart of the test statistic λ̂n. As in FSIC, it can be shown that λn = 0 if and only if X are Y are independent (from Proposition 2). If X and Y are dependent, then λn > 0. According to Theorem 7, for a sufficiently large n, the test power lower bound is increasing in λn. One can therefore think of λn (a function of θ) as representing how easily the test rejects H0 given a problem Pxy. The higher the λn, the\ngreater the lower bound on the test power, and thus the more likely it is that the test will reject H0 when it is false.\nIn light of this reasoning, we propose setting θ to θ∗ = arg maxθ λn. That this procedure is also valid under H0 can be seen as follows. Under H0, θ∗ = arg maxθ 0 will be arbitrary. Since Theorem 7 guarantees that λ̂n\nd→ χ2(J) as n→∞ for any θ, the asymptotic null distribution does not change by using θ∗. In practice, λn is a population quantity which is unknown. We propose dividing the sample Zn into two disjoint sets: training and test sets. The training set is used to optimize for θ∗, and the test set is used for the actual independence test with the optimized θ∗. The splitting is to guarantee the independence of θ∗ and the test sample, which is an assumption of Theorem 5.\nTo better under N̂FSIC2, we visualize µ̂xy(v,w), µ̂xµy(v,w) and Σ̂(v,w) as a function of one test location (v,w) on a simple toy problem. In this problem, Y = −X +Z where Z ∼ N (0, 0.32). As we consider only one location (J = 1), Σ̂(v,w) is a scalar. The\nstatistic can be written as λ̂n = n (µ̂xy(v,w)−µ̂xµy(v,w))\n2\nΣ̂(v,w) .\nThese components are shown in Figure 1, where we use Gaussian kernels for both X and Y , and the horizontal and vertical axes correspond to v ∈ R and w ∈ R, respectively.\nIntuitively, û(v,w) = µ̂xy(v,w)− µ̂xµy(v,w) captures the difference of the joint distribution and the product of the marginals as a function of (v,w). Squaring û(v,w) and dividing it by the variance shown in Figure 1c gives the statistic (also the parameter tuning objective) shown in Figure 1d. The latter figure suggests that the parameter tuning objective function can be non-convex. However, we note that the non-convexity arises since there are multiple ways to detect the difference between the joint distribution and the product of the marginals. In this case, the lower left and upper right regions equally indicate the largest difference."
    }, {
      "heading" : "3 Experiments",
      "text" : "In this section, we empirically study the performance of the proposed method on both toy (Section 3.1) and real-life problems (Section 3.2). Our interest is in the performance of linear-time tests on challenging problems which require a large sample size to be able to accurately reveal the dependence. All the code is available at https: //github.com/wittawatj/fsic-test.\nWe compare the proposed NFSIC with optimization (NFSIC-opt) to five multivariate nonparametric tests. The N̂FSIC2 test without optimization (NFSIC-med) acts as a baseline, allowing the effect of parameter optimization to be clearly seen. For pedagogical reason, we consider the original HSIC test of Gretton et al. [2005] denoted by QHSIC, which is a quadratic-time test. Nyström HSIC (NyHSIC) uses a Nyström approximation to the kernel matrices of X and Y when computing the HSIC statistic. FHSIC is another variant of HSIC in which a random Fourier feature approximation [Rahimi and Recht, 2008] to the kernel is used. NyHSIC and FHSIC are studied in Zhang et al. [2016] and can be computed in O(n), with quadratic dependency on the number of inducing points in NyHSIC, and quadratic dependency in the number of random features in FHSIC. Finally, the Randomized Dependence Coefficient (RDC) proposed in Lopez-Paz et al. [2013] is also considered. The RDC can be seen as the primal form (with random Fourier features) of the kernel canonical correlation analysis of Bach and Jordan [2002] on copula-transformed data. We consider RDC as a linear-time test even though preprocessing by an empirical copula transform costs O((dx + dy)n log n). We use Gaussian kernel classes Kg and Lg for both X and Y in all the methods. Except NFSIC-opt, all other tests use full sample to conduct the independence test, where the Gaussian widths σx and σy are set according to the widely used median heuristic i.e., σx = median ({‖xi − xj‖2 | 1 ≤ i < j ≤ n}), and σy is set in the same way using {yi}ni=1. The J locations for NFSIC-med are randomly drawn from the standard multivariate normal distribution in each trial. For a sample of size n, NFSIC-opt uses half the sample for parameter tuning, and the other disjoint half for the test. We permute the sample 300 times in RDC1 and HSIC to simulate from the null distribution and compute the test threshold. The null distributions for FHSIC and NyHSIC are given by a finite sum of weighted χ2(1) random variables given in Eq. 8 of Zhang et al. [2016]. Unless stated otherwise, we set the test threshold of the two NFSIC tests to be the (1− α)-quantile of χ2(J). To provide a fair comparison, we set J = 10, use 10 inducing points in NyHSIC, and 10\n1We use a permutation test for RDC, following the authors’ implementation (https://github.com/lopezpaz/randomized_ dependence_coefficient, referred commit: b0ac6c0).\nrandom Fourier features in FHSIC and RDC. Optimization of NFSIC-opt The parameters of NFSIC-opt are σx, σy, and J locations of size (dx + dy)J . We treat all the parameters as a long vector in R2+(dx+dy)J and use gradient ascent to optimize λ̂n/2. We observe that initializing VJ by randomly picking J points from the training sample yields good performance. The regularization parameter γn in NFSIC is fixed to a small value, and is not optimized. It is worth emphasizing that the complexity of the optimization procedure is still linear in n. Since FSIC, NyHFSIC and RDC rely on a finitedimensional kernel approximation, these tests are consistent only if both the number of features increases with n. By constrast, the proposed NFSIC requires only n to go to infinity to achieve consistency i.e., J can be fixed. We refer the reader to Appendix C for a brief investigation of the test power vs. increasing J . The test power does not necessarily monotonically increase with J ."
    }, {
      "heading" : "3.1 Toy Problems",
      "text" : "We consider three toy problems: Same Gaussian (SG), Sinusoid (Sin), and Gaussian Sign (GSign). 1. Same Gaussian (SG). The two variables are independently drawn from the standard multivariate normal distribution i.e., X ∼ N (0, Idx) and Y ∼ N (0, Idy ) where Id is the d× d identity matrix. This problem represents a case in which H0 holds. 2. Sinusoid (Sin). Let pxy be the probability density of Pxy. In the Sinusoid problem, the dependency of X and Y is characterized by (X,Y ) ∼ pxy(x, y) ∝ 1 + sin(ωx) sin(ωy), where the domains of X ,Y = (−π, π) and ω is the frequency of the sinusoid. As the frequency ω increases, the drawn sample becomes more similar to a sample drawn from Uniform((−π, π)2). That is, the higher ω, the harder to detect the dependency between X and Y . This problem was studied in Sejdinovic et al. [2013]. Plots of the density for a few values of ω are shown in Figures 6 and 7 in the appendix. The main characteristic of interest in this problem is the local change in the density function. 3. Gaussian Sign (GSign). In this problem, Y =\n|Z|\n∏dx\ni=1 sgn(Xi), where X ∼ N (0, Idx), sgn(·) is the sign function, and Z ∼ N (0, 1) serves as a source of noise. The full interaction of X = (X1, . . . , Xdx) is what makes the problem challenging. That is, Y is dependent on X, yet it is independent of any proper subset of {X1, . . . , Xd}. Thus, simultaneous consideration of all the coordinates of X is required to successfully detect the dependency.\nWe fix n = 4000 and vary the problem parameters. Each problem is repeated for 300 trials, and the sample is redrawn each time. The significance level α is set to 0.05. The results are shown in Figure 2. It can be seen that in the SG problem (Figure 2b) where H0 holds, all the tests achieve roughly correct type-I errors at α = 0.05. In the Sin problem, NFSIC-opt achieves the highest test power for all considered ω = 1, . . . , 6, highlighting its strength in detecting local changes in the joint density. The performance of NFSIC-med is significantly lower than that of NFSIC-opt. This phenomenon clearly emphasizes the importance of the optimization to place the locations at the relevant regions in X × Y . RDC has a remarkably high performance in both Sin and GSign (Figure 2c, 2d) despite no parameter tuning. Interestingly, both NFSICopt and RDC outperform the quadratic-time QHSIC in these two problems. The ability to simultaneously consider interacting features of NFSIC-opt is indicated by its superior test power in GSign, especially at the challenging settings of dx = 5, 6. An average trial runtime for each test in the SG problem is shown in Figure 2a. We observe that the runtime does not increase with dimension, as the complexity of all the tests is linear in the dimension of the input. All the tests are implemented in Python using a common SciPy Stack.\nTo investigate the sample efficiency of all the tests, we fix dx = dy = 250 in SG, ω = 4 in Sin, dx = 4 in GSign, and increase n. Figure 3 shows the results. The quadratic dependency on n in QHSIC makes it infeasible both in terms of memory and runtime to consider n larger than 6000 (Figure 3a). In constrast, although not the most time-efficient, NFSIC-opt has the highest sampleefficiency for GSign, and for Sin in the low-sample regime, significantly outperforming QHSIC. Despite the small additional overhead from the optimization, we are yet able to conduct an accurate test with n = 105, dx = dy =\n250 in less than 100 seconds. We observe in Figure 3b that the two NFSIC variants have correct type-I errors across all sample sizes, indicating that the asymptotic null distribution approximately holds by the time n reaches 1000. We recall from Theorem 5 that the NFSIC test with random test locations will asymptotically reject H0 if it is false. A demonstration of this property is given in Figure 3c, where the test power of NFSIC-med eventually reaches 1 with n higher than 105."
    }, {
      "heading" : "3.2 Real Problems",
      "text" : "We now examine the performance of our proposed test on real problems. Million Song Data (MSD) We consider a subset of the Million Song Data2 [Bertin-Mahieux et al., 2011], in which each song (X) out of 515,345 is represented by 90 features, of which 12 features are timbre average (over all segments) of the song, and 78 features are timbre covariance. Most of the songs are western commercial tracks from 1922 to 2011. The goal is to detect the dependency between each song and its year of release (Y ). We set α = 0.01, and repeat for 300 trials where the full sample is randomly subsampled to n points in each trial. Other settings are the same as in the toy problems. To make sure that the type-I error is correct, we use the permutation approach in the NFSIC tests to compute the threshold. Figure 4b shows the test powers as n increases from 500 to 2000. To simulate the case where H0 holds in the problem, we permute the sample to break the dependency of X and Y . The results are shown in Figure 5 in the appendix.\nEvidently, NFSIC-opt has the highest test power among all the linear-time tests for all the sample sizes. Its test power is second to only QHSIC. We recall that NFSIC-opt uses half of the sample for parameter tuning. Thus, at n = 500, the actual sample for testing is 250, which is relatively small. The fact that there is a vast power gain from 0.4 (NFSIC-med) to 0.8 (NFSIC-opt) at n = 500 suggests that the optimization procedure can perform well even at a lower sample sizes.\n2Million Song Data subset: https://archive.ics.uci.edu/ml/ datasets/YearPredictionMSD.\nVideos and Captions Our last problem is based on the VideoStory46K3 dataset [Habibian et al., 2014]. The dataset contains 45,826 Youtube videos (X) of an average length of roughly one minute, and their corresponding text captions (Y ) uploaded by the users. Each video is represented as a dx = 2000 dimensional Fisher vector encoding of motion boundary histograms (MBH) descriptors of Wang and Schmid [2013]. Each caption is represented as a bag of words with each feature being the frequency of one word. After filtering only words which occur in at least six video captions, we obtain dy = 1878 words. We examine the test powers as n increases from 2000 to 8000. The results are given in Figure 4. The problem is sufficiently challenging that all linear-time tests achieve a low power at n = 2000. QHSIC performs exceptionally well on this problem, achieving a maximum power throughout. NFSIC-opt has the highest sample efficiency among the linear-time tests, showing that the optimization procedure is also practical in a high dimensional setting."
    }, {
      "heading" : "Acknowledgement",
      "text" : "We thank the Gatsby Charitable Foundation for the financial support. The major part of this work was carried out while Zoltán Szabó was a research associate at the Gatsby Computational Neuroscience Unit, University College London.\n3VideoStory46K dataset: https://ivi.fnwi.uva.nl/isis/ mediamill/datasets/videostory.php."
    }, {
      "heading" : "R. J. Serfling. Approximation Theorems of Mathematical",
      "text" : "Statistics. John Wiley & Sons, 2009.\nA. Smola, A. Gretton, L. Song, and B. Schölkopf. A hilbert space embedding for distributions. In International Conference on Algorithmic Learning Theory (ALT), pages 13–31, 2007.\nB. K. Sriperumbudur, A. Gretton, K. Fukumizu, B. Schölkopf, and G. R. G. Lanckriet. Hilbert Space Embeddings and Metrics on Probability Measures. Journal of Machine Learning Research, 11:1517–1561, 2010.\nI. Steinwart and A. Christmann. Support vector machines. Springer Science & Business Media, 2008.\nG. J. Székely and M. L. Rizzo. Brownian distance covariance. The Annals of Applied Statistics, 3(4):1236–1265, 2009.\nG. J. Székely, M. L. Rizzo, and N. K. Bakirov. Measuring and testing dependence by correlation of distances. The Annals of Statistics, 35(6):2769–2794, 2007.\nA. W. v. d. Vaart. Asymptotic Statistics. Cambridge University Press, 2000.\nH. Wang and C. Schmid. Action recognition with improved trajectories. In IEEE International Conference on Computer Vision (ICCV), pages 3551–3558, 2013.\nK. Zhang, J. Peters, D. Janzing, B., and B. Schölkopf. Kernel-based conditional independence test and application in causal discovery. In Conference on Uncertainty in Artificial Intelligence (UAI), pages 804–813, 2011.\nQ. Zhang, S. Filippi, A. Gretton, and D. Sejdinovic. LargeScale Kernel Methods for Independence Testing. 2016. URL http://arxiv.org/abs/1606.07892.\nAn Adaptive Test of Independence with Analytic Kernel Embeddings Supplementary Material"
    }, {
      "heading" : "A Type-I Errors",
      "text" : "In this section, we show that all the tests have correct type-I errors (i.e., the probability of reject H0 when it is true) in real problems. We permute the joint sample so that the dependency is broken to simulate cases in which H0 holds. The results are shown in Figure 5."
    }, {
      "heading" : "B Redundant Test Locations",
      "text" : "Here, we provide a simple illustration to show that two locations t1 = (v1,w1) and t2 = (v2,w2) which are too close to each other will reduce the optimization objective. We consider the Sinusoid problem described in Section 3.1 with ω = 1, and use J = 2 test locations. In Figure 6, t1 is fixed at the red star, while t2 is varied along the horizontal line. The objective value λ̂n as a function of (t1, t2) is shown in the bottom figure. It can be seen that λ̂n decreases sharply when t2 is in the neighborhood of t1. This property implies that two locations which are too close will not maximize the objective function (i.e., the second feature contains no additional information when it matches the first). For J > 2, the objective sharply decreases if any two locations are in the same neighborhood."
    }, {
      "heading" : "C Test Power vs. J",
      "text" : "It might seem intuitive that as the number of locations J increases, the test power should also increase. Here, we empirically show that this statement is not always true. Consider the Sinusoid toy example described in Section 3.1 with ω = 2 (also see the left figure of Figure 7). By construction, X and Y are dependent in this problem. We run\nNFSIC test with a sample size of n = 800, varying J from 1 to 600. For each value of J , the test is repeated for 500 times. In each trial, the sample is redrawn and the J test locations are drawn from Uniform((−π, π)2). There is no optimization of the test locations. We use Gaussian kernels for both X and Y , and use the median heuristic to set the Gaussian widths to 1.8. Figure 7 shows the test power as J increases.\nWe observe that the test power does not monotonically increase as J increases. When J = 1, the difference of pxy and pxpy cannot be adequately captured, resulting in a low power. The power increases rapidly to roughly 0.8 at J = 10, and stays at the maximum until about J = 100. Then, the power starts to drop sharply when J is higher than 400 in this problem.\nUnlike random Fourier features, the number of test locations in NFSIC is not the number of Monte Carlo particles used to approximate an expectation. There is a tradeoff: if the test locations are in key regions (i.e., regions in which there is a big difference between pxy and pxpy), then they increase power; yet the statistic gains in variance (thus reducing test power) as J increases. As can be seen in Figure 7, there are eight key regions (in blue) that can reveal the difference of pxy and pxpy. Using an unnecessarily high J not only makes the covariance matrix Σ̂ harder to estimate accurately, it also increases the computation as the complexity on J is O(J3).\nWe note that NFSIC is not intended to be used with a large J . In practice, it should be set to be large enough so as to capture the key regions as stated. As a practical guide, with optimization of the test locations, a good starting point is J = 5 or 10."
    }, {
      "heading" : "D Proof of Proposition 3",
      "text" : "Recall Proposition 3,\nProposition (A product of Gaussian kernels is characteristic and analytic). Let k(x,x′) = exp ( −(x− x′)>A(x− x′) ) and l(y,y′) = exp ( −(y − y′)>B(y − y′) ) be Gaussian kernels on Rdx × Rdx and Rdy × Rdy respectively, for positive definite matrices A and B. Then, g((x,y), (x′,y′)) = k(x,x′)l(y,y′) is characteristic and analytic on (Rdx × Rdy )× (Rdx × Rdy ).\nProof. Let z := (x>,y>)> and z′ := (x′>,y′>)> be vectors in Rdx+dy . We prove by reducing the product kernel to one Gaussian kernel with g(z, z′) = exp ( −(z− z′)>C(z− z′) ) where C := ( A 0 0 B ) . Write g(z, z′) = Ψ(z− z′)\nwhere Ψ(t) := exp ( −t>Ct ) . Since C is positive definite, we see that the finite measure ζ corresponding to Ψ as defined in Lemma 12 has support everywhere in Rdx+dy . Thus, Sriperumbudur et al. [2010, Theorem 9] implies that g is characteristic. To see that g is analytic, we observe that for each z′ ∈ Rdx+dy , z 7→ −(z − z′)>C(z − z′) is a multivariate polynomial in z, which is known to be analytic. Using the fact that t 7→ exp(t) is analytic on R, and that a composition of analytic functions is analytic, we see that z 7→ exp ( −(z− z′)>C(z− z′) ) is analytic on Rdx+dy for each z′."
    }, {
      "heading" : "E Proof of Theorem 5",
      "text" : "Recall Theorem 5,\nTheorem 5 (Independence test using N̂FSIC2 is consistent). Let Σ̂ be a consistent estimate of Σ based on the joint sample Zn. The N̂FSIC2 statistic is defined as λ̂n := nû> ( Σ̂ + γnI )−1 û where γn ≥ 0 is a regularization\nparameter. Assume that\n1. Assumption A holds.\n2. Σ is invertible almost surely with respect to VJ = {(vi,wi)}Ji=1 drawn from an absolutely continuous distribution.\n3. limn→∞ γn = 0.\nThen, for any k, l and VJ satisfying the assumptions,\n1. Under H0, λ̂n d→ χ2(J) as n→∞. 2. Under H1, for any r ∈ R, limn→∞ P ( λ̂n ≥ r ) = 1 almost surely. That is, the independence test based on\nN̂FSIC2 is consistent. Proof. Assume thatH0 holds. The consistency of Σ̂ and the continuous mapping theorem imply that ( Σ̂ + γnI )−1 p→ Σ−1 which is a constant. Let a be a random vector in RJ following N (0,Σ). By Vaart [2000, Theorem 2.7 (v)], it\nfollows that [ √ nû, ( Σ̂ + γnI )−1] d→ [a,Σ−1] where u = 0 almost surely by Proposition 2, and √nû d→ N (0,Σ) by Proposition 4. Since f(x,S) := x>Sx is continuous, f ( √ nû, ( Σ̂ + γnI\n)−1) d→ f(a,Σ−1). Equivalently, nû> ( Σ̂ + γnI )−1 û\nd→ a>Σ−1a ∼ χ2(J) by Anderson [2003, Theorem 3.3.3]. This proves the first claim. The proof of the second claim has a very similar structure to the proof of Proposition 2 of Chwialkowski et al. [2015]. Assume that H1 holds. Then, u 6= 0 almost surely by Proposition 2. Since k and l are bounded, it follows that |ht(z, z′)| ≤ 2BkBl for any z, z′ (see (8)), and we have that û\na.s.→ u by Serfling [2009, Section 5.4, Theorem A]. Thus, û> ( Σ̂ + γnI )−1 û− rn d→ u>Σ−1u by the continuous mapping theorem, and the consistency of Σ̂. Consequently,\nlim n→∞\nP ( λ̂n ≥ r ) = 1− lim n→∞ P ( û> ( Σ̂ + γnI )−1 û− r n < 0\n) (a) = 1− P ( u>Σ−1u < 0 ) (b) = 1,\nwhere at (a) we use the Portmanteau theorem [Vaart, 2000, Lemma 2.2 (i)] guaranteeing that xn d→ x if and only if P(xn < t)→ P(x < t) for all continuity points of t 7→ P(x < t). Step (b) is justified by noting that the covariance matrix Σ is positive definite so that u>Σ−1u > 0, and t 7→ P(u>Σ−1u < t) (a step function) is continuous at 0."
    }, {
      "heading" : "F Proof of Theorem 7",
      "text" : "Recall Theorem 7,\nTheorem 7 (A lower bound on the test power). Let NFSIC2(X,Y ) := λn := nu>Σ−1u. Let K be a kernel class for k, L be a kernel class for l, and V be a collection with each element being a set of J locations. Assume that 1. There exist finite Bk and Bl such that supk∈K supx,x′∈X |k(x,x′)| ≤ Bk and supl∈L supy,y′∈Y |l(y,y′)| ≤ Bl.\n2. c̃ := supk∈K supl∈L supVJ∈V ‖Σ −1‖F <∞. Then, for any k ∈ K, l ∈ L, VJ ∈ V, and λn ≥ r, the test power satisfies P ( λ̂n ≥ r ) ≥ L(λn) where\nL(λn) = 1− 62e−ξ1γ 2 n(λn−r) 2/n − 2e−b0.5nc(λn−r) 2/[ξ2n2]\n− 2e−[(λn−r)γn(n−1)/3−ξ3n−c3γ 2 nn(n−1)] 2 /[ξ4n2(n−1)],\nb·c is the floor function, ξ1 := 132c21J2B∗ , ξ2 := 72c 2 2JB 2, B := BkBl, ξ3 := 8c1B2J , c3 := 4B2Jc̃2, ξ4 := 28B4J2c21,\nc1 := 4B 2J √ Jc̃, c2 := 4B √ Jc̃, and B∗ is a constant depending on only Bk and Bl. Moreover, for sufficiently large fixed n, L(λn) is increasing in λn.\nOverview of the proof We first derive a probabilistic bound for |λ̂n − λn|/n. The bound is in turn upper bounded by an expression involving ‖û− u‖2 and ‖Σ̂−Σ‖F . The difference ‖û− u‖2 can be bounded by applying the bound for U-statistics given in Serfling [2009, Theorem A, p. 201]. For ‖Σ̂−Σ‖F , we decompose it into a sum of smaller components, and bound each term with a product variant of the Hoeffding’s inequality (Lemma 9). L(λn) is obtained by combining all the bounds with the union bound."
    }, {
      "heading" : "F.1 Notations",
      "text" : "Let 〈A,B〉F := tr(A>B) denote the Frobenius inner product, and ‖A‖F := √\ntr(A>A) be the Frobenius norm. Write z := (x,y) to denote a pair of points from X × Y. We write t := (v,w) to denote a pair of test locations from X × Y. For brevity, an expectation over (x,y) (i.e., E(x,y)∼Pxy) will be written as Ez or Exy. Define k̃(x,v) := k(x,v)− Ex′k(x′,v), and l̃(y,w) := l(y,w)− Ey′ l(y′,w). Let B2(r) := {x | ‖x‖2 ≤ r} be a closed ball with radius r centered at the origin. Similarly, define BF (r) := {A | ‖A‖F ≤ r} to be a closed ball with radius r of J × J matrices under the Frobenius norm. Denote the max operation by (x1, . . . , xm)+ = max(x1, . . . , xm). For a product of marginal mean embeddings µx(v)µy(w), we write µ̂xµy(v,w) := 1\nn(n−1) ∑n i=1 ∑ j 6=i k(xi,v)l(yj ,w) to denote the unbiased plug-in estimator, and write µ̂x(v)µ̂y(w) := 1 n ∑n i=1 k(xi,v) 1 n ∑n j=1 l(yj ,w) which is a biased estimator. Define û\nb(v,w) := µ̂xy(v,w) − µ̂x(v)µ̂y(w) so that ûb := ( ûb(t1), . . . , û b(tJ) )> where the superscript b stands for “biased”. To avoid confusing with a positive definite kernel, we will refer to a U-statistic kernel as a core."
    }, {
      "heading" : "F.2 Proof",
      "text" : "We will first derive a bound for P(|λ̂n − λn| ≥ t), which will then be reparametrized to get a bound for the target quantity P(λ̂n ≥ r). We closely follow the proof in Jitkrittum et al. [2016, Section C.1] up to (12), then we diverge. We start by considering |λ̂n − λn|/n.\n|λ̂n − λn|/n = ∣∣∣û>(Σ̂ + γnI)−1û− u>Σ−1u∣∣∣\n= ∣∣∣∣û> (Σ̂ + γnI)−1 û− u> (Σ + γnI)−1 u + u> (Σ + γnI)−1 u− u>Σ−1u∣∣∣∣ ≤ ∣∣∣∣û> (Σ̂ + γnI)−1 û− u> (Σ + γnI)−1 u∣∣∣∣+ ∣∣∣u> (Σ + γnI)−1 u− u>Σ−1u∣∣∣\n:= (F)1 + (F)2 .\nWe next bound (F1) and (F2) separately.\n(F)1 = ∣∣∣∣û> (Σ̂ + γnI)−1 û− u> (Σ + γnI)−1 u∣∣∣∣ =\n∣∣∣∣û> (Σ̂ + γnI)−1 û− û> (Σ + γnI)−1 û + û> (Σ + γnI)−1 û− u> (Σ + γnI)−1 u∣∣∣∣ ≤ ∣∣∣∣û> (Σ̂ + γnI)−1 û− û> (Σ + γnI)−1 û∣∣∣∣+ ∣∣∣û> (Σ + γnI)−1 û− u> (Σ + γnI)−1 u∣∣∣\n= ∣∣∣∣〈ûû>,(Σ̂ + γnI)−1 − (Σ + γnI)−1〉 F ∣∣∣∣+ ∣∣∣〈ûû> − uu>, (Σ + γnI)−1〉 F ∣∣∣ ≤ ‖ûû>‖F ‖(Σ̂ + γnI)−1 − (Σ + γnI)−1‖F + ‖ûû> − uu>‖F ‖(Σ + γnI)−1‖F = ‖ûû>‖F ‖(Σ̂ + γnI)−1[(Σ + γnI)− (Σ̂ + γnI)](Σ + γnI)−1‖F + ‖ûû> − ûu> + ûu> − uu>‖F ‖(Σ + γnI)−1‖F\n(a) ≤ ‖ûû>‖F ‖(Σ̂ + γnI)−1‖F ‖Σ− Σ̂‖F ‖Σ−1‖F + ‖ûû> − ûu> + ûu> − uu>‖F ‖Σ−1‖F (b)\n≤ √ J\nγn ‖û‖22‖Σ− Σ̂‖F ‖Σ−1‖F +\n( ‖û(û− u)>‖F + ‖(û− u)u>‖F ) ‖Σ−1‖F\n≤ √ J\nγn ‖û‖22‖Σ− Σ̂‖F ‖Σ−1‖F + (‖û‖2 + ‖u‖2) ‖û− u‖2‖Σ−1‖F , (5)\nwhere at (a) we used ‖(Σ + γnI)−1‖F ≤ ‖Σ−1‖F , at (b) we used ‖(Σ̂ + γnI)−1‖F ≤ √ J‖(Σ̂ + γnI)−1‖2 ≤ √ J/γn.\nFor (F)2, we have\n(F)2 = ∣∣∣u> (Σ + γnI)−1 u− u>Σ−1u∣∣∣\n= ∣∣〈uu>, (Σ + γnI)−1 −Σ−1〉F ∣∣ ≤ ‖uu>‖F ‖(Σ + γnI)−1 −Σ−1‖F = ‖u‖22‖(Σ + γnI)−1 [Σ− (Σ + γnI)] Σ−1‖F ≤ γn‖u‖22‖(Σ + γnI)−1‖F ‖Σ−1‖F (a) ≤ γn‖u‖22‖Σ−1‖2F , (6)\nwhere at (a) we used ‖(Σ + γnI)−1‖F ≤ ‖Σ−1‖F . Combining (5) and (6), we have∣∣∣û>(Σ̂ + γnI)−1û− u>Σ−1u∣∣∣\n≤ √ J\nγn ‖û‖2‖Σ− Σ̂‖F ‖Σ−1‖F + (‖û‖2 + ‖u‖2) ‖û− u‖2‖Σ−1‖F + γn‖u‖22‖Σ−1‖2F . (7)\nBounding ‖û‖22 and ‖u‖22 Here, we show that by the boundedness of the kernels k and l, it follows that ‖û‖22 is bounded. Recall that supx,x′∈X |k(x,x′)| ≤ Bk, supy,y′ |l(y,y′)| ≤ Bl, our notation t = (v,w) for the test locations, and zi := (xi,yi). We first show that the U-statistic core h is bounded.\n|ht((x,y), (x′,y′))| = ∣∣∣∣12(k(x,v)− k(x′,v))(l(y,w)− l(y′,w)) ∣∣∣∣ ≤ 1\n2 (|k(x,v)|+ |k(x′,v)|) (|l(y,w)|+ |l(y′,w)|)\n≤ 2BkBl := 2B, (8)\nwhere we define B := BkBl. It follows that\n‖û‖22 = J∑\nm=1\n 2 n(n− 1) ∑ i<j htm(zi, zj) 2 ≤ J∑ m=1 [2BkBl] 2 = 4B2J, (9)\n‖u‖22 = J∑\nm=1\n[EzEz′htm(z, z′)] 2 ≤ 4B2J. (10)\nUsing the upper bounds on ‖û‖22, ‖u‖22 ,(7) and the definition of c̃, we have∣∣∣û>(Σ̂ + γnI)−1û− u>Σ−1u∣∣∣ ≤ √ J\nγn 4B2Jc̃‖Σ− Σ̂‖F + 4B\n√ Jc̃‖û− u‖2 + 4B2Jc̃2γn\n=: c1 γn ‖Σ− Σ̂‖F + c2‖û− u‖2 + c3γn, (11)\nwhere we define c1 := 4B2J √ Jc̃, c2 := 4B √ Jc̃, and c3 := 4B2Jc̃2. This upper bound implies that\n|λ̂n − λn| ≤ c1 γn n‖Σ− Σ̂‖F + c2n‖û− u‖2 + c3nγn. (12)\nWe will separately upper bound ‖Σ− Σ̂‖F and ‖û− u‖2, and combine them with a union bound.\nF.2.1 Bounding ‖û− u‖2 Let t∗ = arg maxt∈{t1,...,tJ} |û(t)− u(t)|. Recall that u = (u(t1), . . . , u(tJ))> = (u1, . . . , uJ)>.\n‖û− u‖2 = sup b∈B2(1) 〈b, û− u〉2 ≤ sup b∈B2(1) J∑ j=1 |bj ||û(tj)− u(tj)|\n≤ |û(t∗)− u(t∗)| sup b∈B2(1) J∑ j=1 |bj |\n(a) ≤ √ J |û(t∗)− u(t∗)| sup\nb∈B2(1) ‖b‖2\n= √ J |û(t∗)− u(t∗)|, (13)\nwhere at (a) we used ‖a‖1 ≤ √ J‖a‖2 for any a ∈ RJ . From (13), it can be seen that bounding ‖û−u‖2 amounts to bounding the difference of a U-statistic û(t∗) (see (4)) to its expectation u(t∗). Combining (13) and (12), we have\n|λ̂n − λn| ≤ c1 γn n‖Σ− Σ̂‖F + c2n\n√ J |û(t∗)− u(t∗)|+ c3nγn. (14)\nF.2.2 Bounding ‖Σ̂−Σ‖F\nThe plan is to write Σ̂ = Ŝ− ûbûb>,Σ = S− uu>, so that ‖Σ̂−Σ‖F ≤ ‖Ŝ− S‖F + ‖ûbûb> − uu>‖F and bound separately ‖Ŝ− S‖F and ‖ûbûb> − uu>‖F .\nRecall that Σij = η(ti, tj), η(t, t′) = Exy[ ( k̃(x,v)l̃(y,w)− u(v,w) )( k̃(x,v′)l̃(y,w′)− u(v′,w′) ) ] where k̃(x,v) = k(x,v)−Ex′k(x′,v), and l̃(y,w) = l(y,w)−Ey′ l(y′,w). Its empirical estimator (see Proposition 6) is Σ̂ij = η̂(ti, tj) where\nη̂(t, t′) = 1\nn n∑ i=1 [ ( k(xi,v)l(yi,w)− ûb(v,w) )( k(xi,v ′)l(yi,w ′)− ûb(v′,w′) ) ]\n= 1\nn n∑ i=1 k(xi,v)l(yi,w)k(xi,v ′)l(yi,w ′)− ûb(v,w)ûb(v′,w′),\nk(x,v) := k(x,v) − 1n ∑n i=1 k(xi,v), and l(y,w) := l(y,w) − 1 n ∑n i=1 l(yi,w). We\nnote that 1n ∑n i=1 k(xi,v)l(yi,w) = û\nb(v,w). We define Ŝ ∈ RJ×J such that Ŝij := 1 n ∑n m=1 k(xm,vi)l(ym,wi)k(xm,vj)l(yi,wj), and define similarly its population counterpart S such that Sij := Exy[k̃(x,v)l̃(y,w)k̃(x,v′)l̃(y,w′)]. We have\nΣ̂ = Ŝ− ûbûb>, Σ = S− uu>,\n‖Σ̂−Σ‖F = ‖Ŝ− S− (ûbûb> − uu>)‖F (15)\n≤ ‖Ŝ− S‖F + ‖ûbûb> − uu>‖F . (16)\nWith (16), (14) becomes\n|λ̂n − λn| ≤ c1n\nγn ‖Ŝ− S‖F +\nc1n\nγn ‖ûbûb> − uu>‖F + c2n\n√ J |û(t∗)− u(t∗)|+ c3nγn. (17)\nWe will further separately bound ‖Ŝ− S‖F and ‖ûbûb> − uu>‖F .\nF.2.3 Bounding ‖ûbûb> − uu>‖F\n‖ûbûb> − uu>‖F = ‖ûbûb> − ûbu> + ûbu> − uu>‖F ≤ ‖ûb(ûb − u)>‖F + ‖(ûb − u)u>‖F = ‖ûb‖2‖ûb − u‖2 + ‖ûb − u‖2‖u‖2 ≤ 4B √ J‖ûb − u‖2,\nwhere we used (10) and the fact that ‖ûb‖2 ≤ 2B √ J which can be shown similarly to (9) as\n‖ûb‖22 = J∑\nm=1\n[µ̂xy(vm,wm)− µ̂x(vm)µ̂y(wm)]2 = J∑\nm=1\n 1 n2 n∑ i=1 n∑ j=1 htm(zi, zj) 2 ≤ J∑ m=1 [2BkBl] 2 = 4B2J.\nLet (ṽ, w̃) := t̃ = arg maxt∈{t1,...,tJ} |ûb(t)− u(t)|. We bound ‖ûb − u‖2 by\n‖ûb − u‖2 (a) ≤ √ J |ûb(t̃)− u(t̃)|\n= √ J ∣∣µ̂xy(t̃)− µ̂x(ṽ)µ̂y(w̃)− u(t̃)∣∣\n= √ J ∣∣µ̂xy(t̃)− µ̂xµy(t̃) + µ̂xµy(t̃)− µ̂x(ṽ)µ̂y(w̃)− u(t̃)∣∣\n≤ √ J ∣∣µ̂xy(t̃)− µ̂xµy(t̃)− u(t̃)∣∣+√J ∣∣µ̂xµy(t̃)− µ̂x(ṽ)µ̂y(w̃)∣∣\n= √ J ∣∣û(t̃)− u(t̃)∣∣+√J ∣∣µ̂xµy(t̃)− µ̂x(ṽ)µ̂y(w̃)∣∣ , (18)\nwhere at (a) we used the same reasoning as in (13). The bias ∣∣µ̂xµy(t̃)− µ̂x(ṽ)µ̂y(w̃)∣∣ in the second term can be bounded as∣∣µ̂xµy(t̃)− µ̂x(ṽ)µ̂y(w̃)∣∣ =\n∣∣∣∣∣∣ 1n(n− 1) n∑ i=1 ∑ j 6=i k(xi, ṽ)l(yj , w̃)− 1 n2 n∑ i=1 n∑ j=1 k(xi, ṽ)l(yj , w̃) ∣∣∣∣∣∣ =\n∣∣∣∣∣∣ 1n(n− 1) n∑ i=1 n∑ j=1 k(xi, ṽ)l(yj , w̃)− 1 n(n− 1) n∑ i=1 k(xi, ṽ)l(yi, w̃)− 1 n2 n∑ i=1 n∑ j=1 k(xi, ṽ)l(yj , w̃) ∣∣∣∣∣∣ = ∣∣∣∣∣∣ ( 1− n n− 1 ) 1 n2 n∑ i=1 n∑ j=1 k(xi, ṽ)l(yj , w̃) + 1 n(n− 1) n∑ i=1 k(xi, ṽ)l(yi, w̃)\n∣∣∣∣∣∣ ≤ ∣∣∣∣∣∣ ( 1− n n− 1 ) 1 n2 n∑ i=1 n∑ j=1 k(xi, ṽ)l(yj , w̃) ∣∣∣∣∣∣+ ∣∣∣∣∣ 1n(n− 1) n∑ i=1 k(xi, ṽ)l(yi, w̃)\n∣∣∣∣∣ ≤ B n− 1 + B n− 1 = 2B n− 1 .\nCombining this upper bound with (18), we have\n‖ûbûb> − uu>‖F ≤ 4BJ ∣∣û(t̃)− u(t̃)∣∣+ 8B2J\nn− 1 . (19)\nWith (19), (17) becomes\n|λ̂n − λn| ≤ c1n\nγn ‖Ŝ− S‖F +\n4BJc1n\nγn\n∣∣û(t̃)− u(t̃)∣∣+ c1n γn 8B2J n− 1 + c2n √ J |û(t∗)− u(t∗)|+ c3nγn. (20)\nF.2.4 Bounding ‖Ŝ− S‖F Recall that VJ = {t1, . . . , tJ}, Ŝij = Ŝ(ti, tj) = 1n ∑n m=1 k(xm,vi)l(ym,wi)k(xm,vj)l(ym,wj), and Sij = S(ti, tj) = Exy[k̃(x,vi)l̃(y,wi)k̃(x,vj)l̃(y,wj)]. Let (t(1), t(2)) = arg max(s,t)∈VJ×VJ |Ŝ(s, t)− S(s, t)|.\n‖Ŝ− S‖F = sup B∈BF (1)\n〈 B, Ŝ− S 〉 F\n≤ sup B∈BF (1) J∑ i=1 J∑ j=1 |Bij ||Ŝij − Sij |\n≤ ∣∣∣Ŝ(t(1), t(2))− S(t(1), t(2))∣∣∣ sup\nB∈BF (1) J∑ i=1 J∑ j=1 |Bij |\n(a) ≤ J ∣∣∣Ŝ(t(1), t(2))− S(t(1), t(2))∣∣∣ sup\nB∈BF (1) ‖B‖F\n= J ∣∣∣Ŝ(t(1), t(2))− S(t(1), t(2))∣∣∣ , (21)\nwhere at (a) we used ∑J i=1 ∑J j=1 |Aij | ≤ J‖A‖F for any matrix A ∈ RJ×J . We arrive at\n|λ̂n − λn| ≤ c1Jn\nγn ∣∣∣Ŝ(t(1), t(2))− S(t(1), t(2))∣∣∣+ 4BJc1n γn ∣∣û(t̃)− u(t̃)∣∣ + c1n\nγn\n8B2J n− 1 + c2n √ J |û(t∗)− u(t∗)|+ c3nγn. (22)"
    }, {
      "heading" : "F.2.5 Bounding",
      "text" : "∣∣∣Ŝ(t, t′)− S(t, t′)∣∣∣\nHaving an upper bound for ∣∣∣Ŝ(t, t′)− S(t, t′)∣∣∣ will allow us to bound (22). To keep the notations uncluttered, we\nwill define the following shorthands.\nExpression Shorthand\nk(x,v) a\nk(x,v′) a′\nk(xi,v) ai\nk(xi,v ′) a′i\nEx∼Pxk(x,v) ã Ex∼Pxk(x,v′) ã′\n1 n ∑n i=1 k(xi,v) a 1 n ∑n i=1 k(xi,v ′) a′\nExpression Shorthand\nl(y,w) b\nl(y,w′) b′\nl(yi,w) bi\nl(yi,w ′) b′i\nEy∼Py l(y,w) b̃\nEy∼Py l(y,w′) b̃′\n1 n ∑n i=1 l(yi,w) b\n1 n ∑n i=1 l(yi,w ′) b ′\nWe will also use · to denote a empirical expectation over x, or y, or (x,y). The argument under · will determine the variable over which we take the expectation. For instance, aa′ = 1n ∑n i=1 k(xi,v)k(xi,v\n′) and aba′ = 1n ∑n i=1 k(xi,v)l(yi,w)k(xi,v\n′), and so on. We define in the same way for the population expectation using ·̃ i.e., ãa′ = Ex [k(x,v)k(x,v′)] and ãba′ = Exy [k(x,v)l(y,w)k(x,v′)].\nWith these shorthands, we can rewrite Ŝ(t, t′) and S(t, t′) as\nŜ(t, t′) = 1\nn n∑ i=1 (ai − a)(bi − b)(a′i − a′)(b′i − b ′ ),\nS(t, t′) = Exy [ (a− ã)(b− b̃)(a′ − ã′)(b′ − b̃′) ] .\nBy expanding S(t, t′), we have\nS(t, t′) = Exy [ + aba′b′ − aba′b̃′ − abã′b′ + abã′b̃′\n− ab̃a′b′ + ab̃a′b̃′ + ab̃ã′b′ − ab̃ã′b̃′\n− ãba′b′ + ãba′b̃′ + ãbã′b′ − ãbã′b̃′ + ãb̃a′b′ − ãb̃a′b̃′ − ãb̃ã′b̃′ + ãb̃ã′b̃′ ]\n= +ãba′b′ − ãba′b̃′ − ãbb′ã′ + ãbã′b̃′\n− ãa′b′b̃+ ãa′b̃b̃′ + ãb′ã′b̃− ãb̃ã′b̃′\n− ã′bb′ã+ ã′bãb̃′ + ãã′b̃b′ − ãb̃ã′b̃′\n+ ã′b′ãb̃− ãb̃ã′b̃′ − ãb̃ã′b̃′ + ãb̃ã′b̃′\n= +ãba′b′ − ãba′b̃′ − ãbb′ã′ + ãbã′b̃′\n− ãa′b′b̃+ ãa′b̃b̃′ + ãb′ã′b̃+ ã′b′ãb̃\n− ã′bb′ã+ ã′bãb̃′ + ãã′b̃b′ − 3ãb̃ã′b̃′.\nThe expansion of Ŝ(t, t′) can be done in the same way. By the triangle inequality, we have∣∣∣Ŝ(t, t′)− S(t, t′)∣∣∣ ≤ ∣∣∣aba′b′ − ãba′b′∣∣∣+ ∣∣∣aba′ b′ − ãba′b̃′∣∣∣+ ∣∣∣abb′a′ − ãbb′ã′∣∣∣+ ∣∣∣aba′b′ − ãbã′b̃′∣∣∣∣∣∣aa′b′ b− ãa′b′b̃∣∣∣+ ∣∣∣aa′ b b′ − ãa′b̃b̃′∣∣∣+ ∣∣∣ab′a′b− ãb′ã′b̃∣∣∣+ ∣∣∣a′b′ab− ã′b′ãb̃∣∣∣∣∣∣a′bb′a− ã′bb′ã∣∣∣+ ∣∣∣a′bab′ − ã′bãb̃′∣∣∣+ ∣∣∣a a′bb′ − ãã′b̃b′∣∣∣+ 3 ∣∣∣aba′b′ − ãb̃ã′b̃′∣∣∣ . The first term\n∣∣∣aba′b′ − ãba′b′∣∣∣ can be bounded by applying the Hoeffding’s inequality. Other terms can be bounded by applying Lemma 9. Recall that we write (x1, . . . , xm)+ for max(x1, . . . , xm)."
    }, {
      "heading" : "Bounding",
      "text" : "∣∣∣aba′b′ − ãba′b′∣∣∣ (1st term). Since −B2 ≤ aba′b′ ≤ B2, by the Hoeffding’s inequality (Lemma 14), we\nhave\nP (∣∣∣aba′b′ − ãba′b′∣∣∣ ≤ t) ≥ 1− 2 exp(− nt2\n2B4\n) ."
    }, {
      "heading" : "Bounding",
      "text" : "∣∣∣aba′ b′ − ãba′b̃′∣∣∣ (2nd term). Let f1(x,y) = aba′ = k(x,v)l(y,w)k(x,v′) and f2(y) = b′ = l(y,w′).\nWe note that |f1(x,y)| ≤ (BBk, Bl)+ and |f2(y)| ≤ (BBk, Bl)+. Thus, by Lemma 9 with E = 2, we have\nP (∣∣∣aba′ b′ − ãba′b̃′∣∣∣ ≤ t) ≥ 1− 4 exp(− nt2\n8(BBk, Bl)4+\n) ."
    }, {
      "heading" : "Bounding",
      "text" : "∣∣∣aba′b′ − ãbã′b̃′∣∣∣ (4th term). Let f1(x,y) = ab = k(x,v)l(y,w), f2(x) = a′ = k(x,v′) and f3(y) = b′ = l(y,w′). We can see that |f1(x,y)|, |f2(x)|, |f3(y)| ≤ (B,Bk, Bl)+. Thus, by Lemma 9 with E = 3, we have\nP (∣∣∣aba′b′ − ãbã′b̃′∣∣∣ ≤ t) ≥ 1− 6 exp(− nt2\n18(B,Bk, Bl)6+\n) ."
    }, {
      "heading" : "Bounding",
      "text" : "∣∣∣aba′b′ − ãb̃ã′b̃′∣∣∣ (last term). Let f1(x) = a = k(x,v), f2(y) = b = l(y,w), f3(x) = a′ = k(x,v′) and f4(y) = b ′ = l(y,w′). It can be seen that |f1(x)|, |f2(y)|, |f3(x)|, |f4(y)| ≤ (Bk, Bl)+. Thus, by Lemma 9 with E = 4, we have\nP ( 3 ∣∣∣aba′b′ − ãb̃ã′b̃′∣∣∣ ≤ t) ≥ 1− 8 exp(− nt2\n32 · 32(Bk, Bl)8+\n) .\nBounds for other terms can be derived in a similar way to yield\n(3rd term) P (∣∣∣abb′a′ − ãbb′ã′∣∣∣ ≤ t) ≥ 1− 4 exp(− nt2\n8(BBl, Bk)4+\n) ,\n(5th term) P (∣∣∣aa′b′ b− ãa′b′b̃∣∣∣ ≤ t) ≥ 1− 4 exp(− nt2\n8(BBk, Bl)4+\n) ,\n(6th term) P (∣∣∣aa′ b b′ − ãa′b̃b̃′∣∣∣ ≤ t) ≥ 1− 6 exp(− nt2\n18(B2k, Bl) 6 +\n) ,\n(7th term) P (∣∣∣ab′a′b− ãb′ã′b̃∣∣∣ ≤ t) ≥ 1− 6 exp(− nt2\n18(B,Bk, Bl)6+\n) ,\n(8th term) P (∣∣∣a′b′ab− ã′b′ãb̃∣∣∣ ≤ t) ≥ 1− 6 exp(− nt2\n18(B,Bk, Bl)6+\n) ,\n(9th term) P (∣∣∣a′bb′a− ã′bb′ã∣∣∣ ≤ t) ≥ 1− 4 exp(− nt2\n8(BBl, Bk)4+\n) ,\n(10th term) P (∣∣∣a′bab′ − ã′bãb̃′∣∣∣ ≤ t) ≥ 1− 6 exp(− nt2\n18(B,Bk, Bl)6+\n) ,\n(11th term) P (∣∣∣a a′bb′ − ãã′b̃b′∣∣∣ ≤ t) ≥ 1− 6 exp(− nt2\n18(Bk, B2l ) 6 +\n) .\nBy the union bound, we have\nP (∣∣∣Ŝ(t, t′)− S(t, t′)∣∣∣ ≤ 12t) ≥ 1− [ 2 exp ( − nt 2\n2B4\n) + 4 exp ( − nt 2\n8(BBk, Bl)4+\n) + 4 exp ( − nt 2\n8(BBl, Bk)4+\n) + 6 exp ( − nt 2\n18(B,Bk, Bl)6+ ) 4 exp ( − nt 2\n8(BBk, Bl)4+\n) + 6 exp ( − nt 2\n18(B2k, Bl) 6 +\n) + 6 exp ( − nt 2\n18(B,Bk, Bl)6+\n) + 6 exp ( − nt 2\n18(B,Bk, Bl)6+ ) 4 exp ( − nt 2\n8(BBl, Bk)4+\n) + 6 exp ( − nt 2\n18(B,Bk, Bl)6+\n) + 6 exp ( − nt 2\n18(Bk, B2l ) 6 +\n) + 8 exp ( − nt 2\n32 · 32(Bk, Bl)8+ )] = 1− [ 2 exp ( − nt 2\n2B4\n) + 8 exp ( − nt 2\n8(BBk, Bl)4+\n) + 8 exp ( − nt 2\n8(BBl, Bk)4+\n) + 24 exp ( − nt 2\n18(B,Bk, Bl)6+ ) + 6 exp ( − nt 2\n18(B2k, Bl) 6 +\n) + 6 exp ( − nt 2\n18(Bk, B2l ) 6 +\n) + 8 exp ( − nt 2\n32 · 32(Bk, Bl)8+ )] ≥ 1− [ 2 exp ( −12 2nt2\nB∗\n) + 8 exp ( −12 2nt2\nB∗\n) + 8 exp ( −12 2nt2\nB∗\n) + 24 exp ( −12 2nt2\nB∗ ) + 6 exp ( −12 2nt2\nB∗\n) + 6 exp ( −12 2nt2\nB∗\n) + 8 exp ( −12 2nt2\nB∗ )] = 1− 62 exp ( −12 2nt2\nB∗\n) ,\nwhere\nB∗ := 1\n122 max(2B4, 8(BBk, Bl) 4 +, 8(BBl, Bk) 4 +, 18(B,Bk, Bl) 6 +, 18(B 2 k, Bl) 6 +, 18(Bk, B 2 l ) 6 +, 32 · 32(Bk, Bl)8+).\nBy reparameterization, it follows that P ( c1Jn\nγn ∣∣∣Ŝ(t, t′)− S(t, t′)∣∣∣ ≤ t) ≥ 1− 62 exp(− γ2nt2 c21J 2nB∗ ) . (23)"
    }, {
      "heading" : "F.2.6 Union Bound for",
      "text" : "∣∣∣λ̂n − λn∣∣∣ and Final Lower Bound\nRecall from (22) that\n|λ̂n − λn| ≤ c1Jn\nγn ∣∣∣Ŝ(t(1), t(2))− S(t(1), t(2))∣∣∣+ 4BJc1n γn ∣∣û(t̃)− u(t̃)∣∣ + c1n\nγn\n8B2J n− 1 + c2n √ J |û(t∗)− u(t∗)|+ c3nγn.\nWe will bound terms in (22) separately and combine all the bounds with the union bound. As shown in (8), the U-statistic core h is bounded between −2B and 2B. Thus, by Lemma 13 (with m = 2), we have\nP ( c2n √ J |û(t∗)− u(t∗)| ≤ t ) ≥ 1− 2 exp ( − b0.5nct 2\n8c22n 2JB2\n) . (24)\nBounding c1nγn 8B2J n−1 + c3nγn + 4BJc1n γn ∣∣û(t̃)− u(t̃)∣∣. By Lemma 13 (with m = 2), it follows that P ( c1n\nγn\n8B2J n− 1 + c3nγn + 4BJc1n\nγn\n∣∣û(t̃)− u(t̃)∣∣ ≤ t)\n≥ 1− 2 exp −b0.5ncγ2n [ t− c1nγn 8B2J n−1 − c3nγn ]2 27B4J2c21n 2  = 1− 2 exp ( − b0.5nc [ tγn(n− 1)− 8c1B2nJ − c3n(n− 1)γ2n\n]2 27B4J2c21n 2(n− 1)2 ) (a) ≥ 1− 2 exp ( − [ tγn(n− 1)− 8c1B2nJ − c3n(n− 1)γ2n\n]2 28B4J2c21n 2(n− 1) ) , (25)\nwhere at (a) we used b0.5nc ≥ (n− 1)/2. Combining (23), (24), and (25) with the union bound (set T = 3t), we can bound (22) with\nP (∣∣∣λ̂n − λn∣∣∣ ≤ T) ≥ 1− 62 exp(− γ2nT 2\n32c21J 2nB∗\n) − 2 exp ( − b0.5ncT 2\n72c22n 2JB2 ) − 2 exp ( − [ Tγn(n− 1)/3− 8c1B2nJ − c3γ2nn(n− 1)\n]2 28B4J2c21n 2(n− 1) ) .\nSince ∣∣∣λ̂n − λn∣∣∣ ≤ T implies λ̂n ≥ λn − T , a reparametrization with r = λn − T gives\nP ( λ̂n ≥ r ) ≥ 1− 62 exp ( −γ 2 n(λn − r)2\n32c21J 2nB∗\n) − 2 exp ( −b0.5nc(λn − r) 2\n72c22n 2JB2 ) − 2 exp ( − [ (λn − r)γn(n− 1)/3− 8c1B2nJ − c3γ2nn(n− 1)\n]2 28B4J2c21n 2(n− 1) ) := L(λn).\nGrouping constants into ξ1, . . . ξ5 gives the result.\nThe lower bound L(λn) takes the form\n1− 62 exp ( −C1(λn − Tα)2 ) − 2 exp ( −C2(λn − Tα)2 ) − 2 exp ( − [(λn − Tα)C3 − C4] 2\nC5\n) ,\nwhere C1, . . . , C5 are positive constants. For fixed large enough n such that λn > Tα, and fixed significance level α, increasing λn will increase L(λn). Specifically, since n is fixed, increasing u>Σ−1u in λn = nu>Σ−1u will increase L(λn)."
    }, {
      "heading" : "G Helper Lemmas",
      "text" : "This section contains lemmas used to prove the main results in this work.\nLemma 8 (Product to sum). Assume that |ai| ≤ B, |bi| ≤ B for i = 1, . . . , E. Then ∣∣∣∏Ei=1 ai −∏Ei=1 bi∣∣∣ ≤\nBE−1 ∑E j=1 |aj − bj |.\nProof. ∣∣∣∣∣∣ E∏ i=1 ai − E∏ j=1 bj ∣∣∣∣∣∣ ≤ ∣∣∣∣∣ E∏ i=1 ai − E−1∏ i=1 aibE ∣∣∣∣∣+ ∣∣∣∣∣ E−1∏ i=1 aibE − E−2∏ i=1 aibE−1bE ∣∣∣∣∣+ . . .+ ∣∣∣∣∣∣a1 E∏ j=2 bj − E∏ j=1 bj ∣∣∣∣∣∣ ≤ |aE − bE |\n∣∣∣∣∣ E−1∏ i=1 ai ∣∣∣∣∣+ |aE−1 − bE−1| ∣∣∣∣∣ ( E−2∏ i=1 ai ) bE ∣∣∣∣∣+ . . .+ |a1 − b1| ∣∣∣∣∣∣ E∏ j=2 bj ∣∣∣∣∣∣ ≤ |aE − bE |BE−1 + |aE−1 − bE−1|BE−1 + . . .+ |a1 − b1|BE−1 = BE−1 E∑ j=1 |aj − bj |\napplying triangle inequality, and the boundedness of ai and bi-s.\nLemma 9 (Product variant of the Hoeffding’s inequality). For i = 1, . . . , E, let {x(i)j } ni j=1 ⊂ Xi be an i.i.d. sample from a distribution Pi, and fi : Xi 7→ R be a measurable function. Note that it is possible that P1 = P2 = · · · = PE and {x(1)j } n1 j=1 = · · · = {x (E) j } nE j=1. Assume that |fi(x)| ≤ B <∞ for all x ∈ Xi and i = 1, . . . , E. Write P̂i to denote an empirical distribution based on the sample {x(i)j } ni j=1. Then,\nP (∣∣∣∣∣ [ E∏ i=1 Ex(i)∼P̂ifi(x (i)) ] − [ E∏ i=1 Ex(i)∼Pifi(x (i)) ]∣∣∣∣∣ ≤ T ) ≥ 1− 2 E∑ i=1 exp ( − niT 2 2E2B2E ) .\nProof. By Lemma8, we have∣∣∣∣∣ [ E∏ i=1 Ex(i)∼P̂ifi(x (i)) ] − [ E∏ i=1 Ex(i)∼Pifi(x (i)) ]∣∣∣∣∣ ≤ BE−1 E∑ i=1 ∣∣∣Ex(i)∼P̂ifi(x(i))− Ex(i)∼Pifi(x(i))∣∣∣ . By applying the Hoeffding’s inequality to each term in the sum, we have P (∣∣∣Ex(i)∼P̂ifi(x(i))− Ex(i)∼Pifi(x(i))∣∣∣ ≤ t) ≥ 1− 2 exp(− 2nit24B2 ) . The result is obtained with a union bound."
    }, {
      "heading" : "H External Lemmas",
      "text" : "In this section, we provide known results referred to in this work.\nLemma 10 (Chwialkowski et al. [2015, Lemma 1]). If k is a bounded, analytic kernel (in the sense given in Definition 1) on Rd × Rd, then all functions in the RKHS defined by k are analytic.\nLemma 11 (Chwialkowski et al. [2015, Lemma 3]). Let Λ be an injective mapping from the space of probability measures into a space of analytic functions on Rd. Define\nd2VJ (P,Q) = J∑ j=1 |[ΛP ](vj)− [ΛQ](vj)|2 ,\nwhere VJ = {vi}Ji=1 are vector-valued i.i.d. random variables from a distribution which is absolutely continuous with respect to the Lebesgue measure. Then, dVJ (P,Q) is almost surely (w.r.t. VJ) a metric.\nLemma 12 (Bochner’s theorem [Rudin, 2011]). A continuous function Ψ : Rd → R is positive definite if and only if it is the Fourier transform of a finite nonnegative Borel measure ζ on Rd, that is, Ψ(x) = ∫ Rd e −ix>ω dζ(ω), x ∈ Rd.\nLemma 13 (A bound for U-statistics [Serfling, 2009, Theorem A, p. 201]). Let h(x1, . . . ,xm) be a Ustatistic kernel for an m-order U-statistic such that h(x1, . . . ,xm) ∈ [a, b] where a ≤ b < ∞. Let Un =( n m )−1∑ i1<···<im h(xi1 , . . . ,xim) be a U-statistic computed with a sample of size n, where the summation is over the(\nn m ) combinations of m distinct elements {i1, . . . , im} from {1, . . . , n}. Then, for t > 0 and n ≥ m,\nP(Un − Eh(x1, . . . ,xm) ≥ t) ≤ exp ( −2bn/mct2/(b− a)2 ) ,\nP(|Un − Eh(x1, . . . ,xm)| ≥ t) ≤ 2 exp ( −2bn/mct2/(b− a)2 ) ,\nwhere bxc denotes the greatest integer which is smaller than or equal to x. Hoeffind’s inequality is a special case when m = 1.\nLemma 14 (Hoeffding’s inequality). Let X1, . . . , Xn be i.i.d. random variables such that a ≤ Xi ≤ b almost surely. Define X := 1n ∑n i=1Xi. Then,\nP (∣∣X − E[X]∣∣ ≤ α) ≥ 1− 2 exp(− 2nα2\n(b− a)2\n) ."
    } ],
    "references" : [ {
      "title" : "Fast Two-Sample Testing with Analytic Representations of Probability Measures",
      "author" : [ "K.P. Chwialkowski", "A. Ramdas", "D. Sejdinovic", "A. Gretton" ],
      "venue" : "In Advances in Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "Chwialkowski et al\\.,? \\Q1981\\E",
      "shortCiteRegEx" : "Chwialkowski et al\\.",
      "year" : 1981
    }, {
      "title" : "Interpretable Distribution Features with Maximum Testing Power. 2016",
      "author" : [ "W. Jitkrittum", "Z. Szabó", "K. Chwialkowski", "A. Gretton" ],
      "venue" : "URL http://arxiv.org/abs/1605.06796",
      "citeRegEx" : "Jitkrittum et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Jitkrittum et al\\.",
      "year" : 2016
    }, {
      "title" : "Fourier analysis on groups",
      "author" : [ "W. Rudin" ],
      "venue" : null,
      "citeRegEx" : "Rudin.,? \\Q2011\\E",
      "shortCiteRegEx" : "Rudin.",
      "year" : 2011
    }, {
      "title" : "Approximation Theorems of Mathematical Statistics",
      "author" : [ "R.J. Serfling" ],
      "venue" : null,
      "citeRegEx" : "Serfling.,? \\Q2009\\E",
      "shortCiteRegEx" : "Serfling.",
      "year" : 2009
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "We show that our test is consistent, despite a finite number of analytic features being used, via a generalization of arguments in Chwialkowski et al. [2015]. As in recent work on two-sample testing by Jitkrittum et al.",
      "startOffset" : 131,
      "endOffset" : 158
    }, {
      "referenceID" : 0,
      "context" : "We show that our test is consistent, despite a finite number of analytic features being used, via a generalization of arguments in Chwialkowski et al. [2015]. As in recent work on two-sample testing by Jitkrittum et al. [2016], our test is adaptive in the sense that we choose our features on a held-out validation set to optimize a lower bound on the test power.",
      "startOffset" : 131,
      "endOffset" : 227
    }, {
      "referenceID" : 0,
      "context" : "Using the same argument as in Chwialkowski et al. [2015], since k and l are analytic, ρ is also analytic, and the set of roots R := {(v,w) | ρ(v,w) = 0} has Lebesgue measure zero.",
      "startOffset" : 30,
      "endOffset" : 57
    }, {
      "referenceID" : 0,
      "context" : "The proof of the second claim has a very similar structure to the proof of Proposition 2 of Chwialkowski et al. [2015]. Assume that H1 holds.",
      "startOffset" : 92,
      "endOffset" : 119
    }, {
      "referenceID" : 2,
      "context" : "Lemma 12 (Bochner’s theorem [Rudin, 2011]).",
      "startOffset" : 28,
      "endOffset" : 41
    } ],
    "year" : 2016,
    "abstractText" : "A new computationally efficient dependence measure, and an adaptive statistical test of independence, are proposed. The dependence measure is the difference between analytic embeddings of the joint distribution and the product of the marginals, evaluated at a finite set of locations (features). These features are chosen so as to maximize a lower bound on the test power, resulting in a test that is data-efficient, and that runs in linear time (with respect to the sample size n). The optimized features can be interpreted as evidence to reject the null hypothesis, indicating regions in the joint domain where the joint distribution and the product of the marginals differ most. Consistency of the independence test is established, for an appropriate choice of features. In real-world benchmarks, independence tests using the optimized features perform comparably to the state-of-the-art quadratictime HSIC test, and outperform competing O(n) and O(n log n) tests.",
    "creator" : "LaTeX with hyperref package"
  }
}