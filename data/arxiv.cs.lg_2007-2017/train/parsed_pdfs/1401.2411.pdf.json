{
  "name" : "1401.2411.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1. Introduction.",
      "text" : "Clustering algorithms have been studied for several decades [DH73], and they remain one of the main ingredients in unsupervised learning [DHS01]. Intuitively, a cluster is both a geometric concept (e.g., a lowdimensional region in a high-dimensional space) and a probabilistic concept (e.g., a region of the input space in which the sample data density is high).\nRecently, a variant of the traditional clustering algorithms has attracted some attention, under the rubric of manifold learning : [TSL00] [RS00] [BN03]. In this variant, the learning task is to construct a low-dimensional manifold, embedded in the original high-dimensional space, on which the probability density of the input data is high. For\nDate: October, 2013; December, 2013. c© L. Thorne McCarty. 1\nar X\niv :1\n40 1.\n24 11\nv1 [\ncs .L\nG ]\n1 0\nJa n\nexample, in one recent paper, Rifai, et al. [RDV+11], outline three hypotheses that motivate much of this work:\n1. The semi-supervised learning hypothesis, according to which learning aspects of the input distribution p(x) can improve models of the conditional distribution of the supervised target p(y|x), i.e., p(x) and p(y|x) share something . . . [citations omitted] 2. The (unsupervised) manifold hypothesis, according to which real world data presented in high dimensional spaces is likely to concentrate in the vicinity of non-linear sub-manifolds of much lower dimensionality . . . [citations omitted] 3. The manifold hypothesis for classification, according to which points of different classes are likely to concentrate along different sub-manifolds, separated by low density regions of the input space.\nThe authors then present a “Contractive Auto-Encoder (CAE)” algorithm to exploit these hypotheses, and they combine this with an existing supervised learning algorithm to produce what they call a “Manifold Tangent Classifier (MTC),” which performs very well on several datasets. It is interesting to note that these algorithms are based, explicitly, on concepts from differential geometry, but they draw only implicitly on probability theory. The informal language of probability theory abounds. For example, the authors write that the “data density concentrates near low-dimensional manifolds” and “different classes correspond to disjoint manifolds separated by low density” (see abstract). But there is no explicit probability model in the paper.\nIn this paper, we will develop a theory of clustering and coding which combines a geometric model with a probabilistic model in a principled way. The geometric model is a Riemannian manifold with a Riemannian metric, gij(x), which we interpret as a measure of dissimilarity. The probabilistic model consists of a stochastic process with an invariant probability measure which matches the density of the sample input data. The link between the two models is a potential function, U(x), and its gradient, ∇U(x). We use the gradient to define the dissimilarity metric, which guarantees that our measure of dissimilarity will depend on the probability measure. Roughly speaking, the dissimilarity will be small in a region in which the probability density is high, and vice versa. Finally, we use the dissimilarity metric to define a coordinate system on the embedded Riemannian manifold, which gives us a low-dimensional encoding of our original data.\nSection 2 reviews the “Mathematical Background” of the paper, including several theorems which will play a central role in the subsequent discussion. Section 3 then discusses “Prototype Coding,” our overall model, and explains how the dissimilarity metric and the lowdimensional coordinate system are related to the stochastic process with an invariant probability measure. Section 4 investigates the differential geometry component of the model more carefully, with a focus on the important concept of an “Integral Manifold.” At this point in the paper, we restrict our analysis to R3 rather than Rn, although we will see later (in Section 7) that this is not actually a limitation on the scope of the theory. Instead, the restriction to three dimensions simplifies our calculations, and makes them much easier to visualize. Accordingly, in Section 5, we present the results of a number of experiments using Mathematica, including some full-color three-dimensional graphics of several examples which are intended to aid our intuitions about the main elements of the theory. Section 6 discusses an interesting technical result, which also helps to link the geometric model to the probabilistic model. Finally, Section 7 discusses “Future Work,” including a further analysis of the connections between the present theory and the current literature on manifold learning."
    }, {
      "heading" : "2. Mathematical Background.",
      "text" : "Let’s start with a model that will be familiar to most physicists: the Feynman-Kac formula [Fey48] [Kac49]. We will write this formula as follows:\n(1) u(t,x) = ∫ Ω f(Xt) exp [ − ∫ t 0 V (Xs) ds ] Wx(dX) Here, Xt ≡ X(t, ω) denotes a continuous path in Rn, and Wx denotes Wiener measure over all such paths beginning at X0 = x. If V : R\nn → R is bounded below, then u(t,x) is a solution to the Cauchy initial value problem:\n(2) ∂u\n∂t =\n1 2 ∆u− V (x)u with u(0, ·) = f\nin which ∆ denotes the standard Laplacian in Cartesian coordinates. Conversely, any bounded solution to (2) is equal to the function defined by (1). See [Str93], Section 4.3. Now, following Feynman’s heuristic picture of formula (1), we can write a discrete approximation to Wiener measure as:∫\nexp [ −\nm∑ k=1 sk − sk−1 2 ( |X(sk)−X(sk−1)| sk − sk−1 )2] dX(s1) . . . dX(sm),\nmultiplied by a normalization factor, so that the exponential function in the integrand of (1) could be viewed, in the limit, as:\n(3) exp [ − ∫ t\n0\n1 2 |Ẋ(s)|2 + V (X(s)) ds ] See [Str93], Section 4.2, or [Str11], Section 8.1. The quantity inside the integral sign is, of course, the Hamiltonian of a classical dynamical system with the potential function: V (x).\nThis model obviously possesses some of the properties that we want: Equations (1) and (2) specify a stochastic process that depends on the potential function, V (x), and the exponent in formula (3) can be interpreted as an expression in differential geometry, which also depends on V (x). Furthermore, the paths that minimize the “energy” in (3) will maximize the probability in (1). Now imagine that we can choose the potential function, V (x), in such a way as to generate an invariant probability measure on Rn. In other words, imagine that we can find a steady-state solution to equation (2). We can then project our stochastic process onto a nonlinear subspace of Rn — i.e., onto an embedded Riemannian manifold — and examine the probability density induced on that subspace. Feynman’s heuristic picture of the relationship between (1) and (3) suggests that the subspaces of maximal probability will also be the subspaces of minimal energy, and the hope is that this will lead us to a solution to the clustering and coding problems in Rn.\nHowever, there are several problems with this model:\n• First, it is well known that Feynman’s heuristic interpretation of formula (1) is mathematical nonsense, since there is no analogue of Lebesgue measure in an infinite-dimensional space. The relationship between (1) and (2) holds rigorously, as stated, ifWx is Wiener measure, or Brownian motion, but there is still a gulf between (1) and (3). To interpret the integral in (3) as an expression in differential geometry, the paths Xs ≡ X(s) ≡ X(s, ω) must be continuous and differentiable. But, under Wiener measure, with probability one, the paths X(t, ω) are continuous but nowhere differentiable. Thus there is a fundamental clash between the geometric model and the probabilistic model. Stroock calls this “a fact which . . . haunts every attempt to deal with Brownian paths,” [Str96], p. 140.\n• Second, assuming that we can overcome our first problem, it is not a simple matter to project a stochastic process from Rn\nonto an embedded Riemannian manifold. The mathematical problem itself has only been solved, in general, during the course\nof the past 20 or 30 years, and it is now part of a subject known as stochastic differential geometry. See [EM89] or [Hsu02]. But the calculations are not trivial.\n• Finally, it would be a mistake to assume that the FeynmanKac formula can be used directly to generate a stochastic process, with a proper probabilistic interpretation. Instead, we will need a new potential function, U(x), and we will need a further derivation from equations (1) and (2), in order to construct a stochastic process with an invariant probability measure. This also means that we will not be able to define our dissimilarity metric, directly, by minimizing the energy functional in formula (3).\nIn the remainder of this section, we will address these three problems, in reverse order. Our analysis will eventually lead us to a modification of the naive Feynman-Kac model, and to the definition of a dissimilarity metric which will achieve the goals articulated in Section 1.\n2.1. A Stochastic Process with an Invariant Measure. To see the problem with the basic Feynman-Kac formula, it is helpful to rewrite (1) using an operator:\n(4) [Ptf ](x) = ∫ Ω f(Xt) exp [ − ∫ t 0 V (Xs) ds ] Wx(dX)\nIt turns out that Pt1 6= 1, which means that we cannot use this operator to construct a Markov process with a proper probabilistic interpretation. Another manifestation of the same problem is the fact that V (x) has a natural interpretation as the “killing rate” for the process, i.e., the probability per unit of time that a path starting at X0 = x will “die” by time δt. Thus the process “dissipates” as time goes by.\nTo fix this problem, we need a new potential function. If µ is a probability density that satisfies 1\n2 ∆µ− V (x)µ = 0, then\nV (x) = 1\n2\n( ∆µ\nµ\n) = 1\n2\n( ∆ log µ+ |∇ log µ|2 ) The first equality is trivial, and the second equality follows from a straightforward computation, e.g., by expanding ∆ log µ in Cartesian coordinates. This equation suggests that we should work with a potential function U(x) and define V (x) as follows:\n(5) V (x) = 1\n2\n( ∆U(x) + |∇U(x)|2 )\nNow consider the following initial value problem:\n(6) ∂w\n∂t =\n1 2 ∆w + ∇U(x) ·∇w with w(0, ·) = f\nLemma 1. w(t,x) is a solution to (6) if and only if eU(x)w(t,x) is a solution to (2) with initial value u(0, ·) = eUf . Proof. By a straightforward computation, using the definition in (5) of V (x) in terms of U(x).\nWe now use both U and V to define a new operator:\n[Qtf ](x) =(7) exp [−U(X0)] ∫\nΩ\nf(Xt) exp [ U(Xt)− ∫ t 0 V (Xs) ds ] Wx(dX)\nTheorem 1. If U is bounded above and V is bounded below, and if w(t,x) is a solution to (6) and eU(x)w(t,x) is also bounded, then w(t,x) is equal to [Qtf ](x) as defined in (7). Furthermore, Qt1 = 1 for all t ≥ 0, and (Qt)t≥0 is a semigroup of operators which defines a Markov process on Rn with the invariant measure µ = eU(x).\nProof. See Theorem 4.3.36 in [Str93] or Theorem 10.3.33 in [Str11].\nIn the literature, (6) is known as a diffusion equation with a drift term ∇U . It is a nice feature of our formalism that this drift term is the gradient of a potential U(x), and that the invariant measure turns out to be the exponential of the potential U(x). For a numerical example, if U(x) is a negative quadratic polynomial (which would be bounded above), then V (x) would be a positive quadratic polynomial (which would be bounded below), and the invariant measure would be a Gaussian. See Section 5.1 below.\nSources: These results appear in [Str93], Section 4.3, but the analysis there uses a different definition of V in terms of U . In the second edition of his book, Stroock switches to the more natural definition in (5) above, but with the opposite sign. See [Str11], Section 10.3. Øksendal also uses this example, with the same definition of V and the same sign, in Exercises 8.15 and 8.16 of his text [Øks03].\n2.2. Mapping a Diffusion to an Embedded Manifold. The equations in the previous section were all expressed in Cartesian coordinates, and the results would be different in a different coordinate system. For a simple example, if the standard 2-dimensional Laplacian were transformed into polar coordinates, it would acquire an additional first-order “drift” term. This is a problem if we want to map a diffusion from Rn onto a nonlinear Riemannian manifold.\nOne approach to this problem is to analyze the diffusion by means of a stochastic differential equation, in two versions, one due to Itô, and one due to Stratonovich. We will write a 1-dimensional Itô process as:\nX(t) = X(0) + ∫ t 0 σ(s, ω) dB(s, ω) + ∫ t 0 b(s, ω) ds\nwhere the first integral is an Itô integral defined with respect to the Brownian motion B(t, ω), and the second integral is an ordinary Riemann or Lebesgue integral. In differential notation, this would be:\n(8) dX(t) = σ(t, ω) dB(t, ω) + b(t, ω) dt Extending this notation to n dimensions, let B1(t, ω), . . . ,Bd(t, ω) be d independent Brownian motion processes, assume that b : Rn → Rn and σ : Rn → Rn×d are Lipschitz continuous, and define the n-dimensional Itô process as follows:\n(9) dX(t) = σ11 . . . σ1d... ... σn1 . . . σ n d dB1(t)... dBd(t)  + b1... bn  dt We want to construct a differential operator associated with this process. Setting a = σσT , define L for all f ∈ C2(Rn; R) by:\n(10) [Lf ](x) = 1 2 ∑ i,j aij(x) ∂2f ∂xi∂xj + ∑ i bi(x) ∂f ∂xi\nTheorem 2. The operator L defined in (10) is the infinitesimal generator of the n-dimensional Itô process given by (9).\nProof. See Definition 7.3.1 and Theorem 7.3.3 in [Øks03].\nIntuitively, σ is the “square root” of a. Note also that, if a = σσT is the identity matrix and b = ∇U , then (9) and (10) give us the same stochastic process in Rn as does (6).\nFor our purposes, however, the Itô process has a defect: It is not invariant under coordinate transformations. This can be seen by an examination of Itô’s formula, which functions as a “chain rule” for the stochastic calculus, but with a second-order correction term. Let F : Rn → R be a function with continuous second-order partial derivatives. Then Itô’s formula asserts that:\ndF (X(t)) = ∑ i ∂F (X(t)) ∂xi dXi(t) + 1 2 ∑ i,j ∂2F (X(t)) ∂xi ∂xj dXi(t) dXj(t)\nSee [Øks03], Chapter 4. An alternative is to use the Stratonovich integral, which cancels out the correction term. A common notational\ndevice is to insert the symbol “◦” in (9) to indicate that the stochastic integral is intended to be interpreted in the Stratonovich sense rather than the Itô sense. Using this notation, the equation for dF (X(t)) would be written as:\n(11) dF (X(t)) = ∑ i ∂F (X(t)) ∂xi ◦ dXi(t)\nin accordance with the usual rules of the Newton-Leibniz calculus. Since F could be an arbitrary coordinate transformation, the use of the Stratonovich formula in (11), instead of Itô’s formula, makes it possible to combine the stochastic calculus with the traditional constructs of Riemannian geometry.\nFortunately, the Itô integral and the Stratonovich integral can be developed in parallel, and it is possible to choose whichever version works best in a particular application. In the 1-dimensional case, we will write the Stratonovich version of a stochastic process as follows:\nX(t) = X(0) + ∫ t 0 σ(s, ω) ◦ dB(s, ω) + ∫ t 0 b̃(s, ω) ds\nNotice the notation “◦ dB(s, ω)” here, and the use of the function b̃(s, ω) instead of b(s, ω). Written as a differential, this would be:\n(12) dX(t) = σ(t, ω) ◦ dB(t, ω) + b̃(t, ω) dt Extending this notation to n dimensions, we have:\n(13) dX(t) = σ11 . . . σ1d... ... σn1 . . . σ n d  ◦ dB1(t)... dBd(t)  + b̃1... b̃n  dt Lemma 2. The stochastic process defined by the Itô integral in (9) is identical to the process defined by the Stratonovich integral in (13) if and only if\n(14) b̃i = bi − 1 2 d∑ k=1 n∑ j=1 ∂σik ∂xj σjk\nProof. See [Str66] or [Itô75].\nWe thus have a simple mapping between the two formalisms, with the advantage that the stochastic differential equation in Stratonovich form is invariant under coordinate transformations.\nLemma 2 has an interesting consequence if we start out with the stochastic process given by (6). Recall that a = σσT is the identity and b = ∇U in this case. Suppose we satisfy the condition a = I by\nsetting σ = I. Then the second term in (14) vanishes, and b̃ = b. However, if we subsequently apply a nonlinear coordinate transformation to our process, or project it onto a nonlinear subspace, then the Ito and Stratonovich equations will diverge, and we will want to use the Stratonovich equation from then on.\nLet us now reinterpret the preceding analysis as a general property of vector fields. Define the column vectors\nA0 = b̃1... b̃n  and Ak = σ1k... σnk  for k = 1, . . . , d and rewrite (13) as:\n(15) dX(t) = ( A1| . . . |Ad ) ◦ dB(t) + A0 dt\nWe will think of a vector field as a differential operator, essentially the directional derivative with respect to a given vector V. Let us write this in shorthand notation as V∂. It then makes sense to talk about the “square” of a vector field, which we can define as the composition of the differential operator with itself: (V∂)2 = V∂ ◦V∂. Expanding this formula in a coordinate system, we have:(∑\ni\nV i ∂\n∂xi\n) ◦ (∑ j V j ∂ ∂xj ) =(16)\n∑ i,j V iV j ∂2 ∂xi∂xj + ∑ i,j ∂V i ∂xj V j ∂ ∂xi\nNow apply this equation to each of the vector fields Ak∂.\nTheorem 3. If L is the differential operator associated with the stochastic process defined in (15), then\n(17) L = 1 2 d∑ k=1 (Ak∂) 2 + A0∂\nProof. By a straightforward computation, using (10), (14) and (16).\nWith Theorem 3 as a guide, we can bypass the Itô or Stratonovich stochastic differential equations entirely, and work directly with vector fields. This is our second (but closely related) approach to the problem of mapping diffusions to embedded manifolds. Let V0∂ and Vk∂,\nfor k = 1, . . . , d, be arbitrary vector fields, and define the differential operator\n(18) L = 1 2 d∑ k=1 (Vk∂) 2 + V0∂.\nThis is known as the Hörmander form for the operator L, and it, too, can be shown to be invariant under coordinate transformations. See [Hör67]. Thus L works just as well in an arbitrary manifold M as it does in Rn endowed with Cartesian coordinates. The only condition that we need to impose to guarantee that L, as defined in (18), gives us a nondegenerate diffusion in M is to require that the vector fields {V1(x)∂, . . . ,Vd(x)∂} span the tangent space on M at x. For these reasons, Stroock relies on the Hörmander formalism extensively in his book on the analysis of Brownian paths on Riemannian manifolds [Str00].\nSources: For the basic results on stochastic differential equations, using Itô’s formalism, the reader should consult [Øks03], but Øksendal’s text provides only a cursory treatment of Stratonovich’s formalism. The original paper by Stratonovich [Str66] is still very readable, but his theory was only given a solid mathematical foundation some years later by Itô [Itô75]. Chapter 8 of [Str03] is an excellent contemporary account of Stratonovich’s theory, set in a broader context.\n2.3. Integral Curves and Martingales on Manifolds. There remains the problem that “haunts every attempt to deal with Brownian paths,” [Str96], p. 140. How do you reconcile the “smooth” curves of differential geometry with the “rough” paths that provide the support for Wiener measure? One answer, suggested by Stroock, emerges from a study of the relationship between the integral curves of a vector field and the concept of a martingale.\nLet’s examine this idea, first, in the ordinary Euclidean space Rn. Roughly speaking, a (continuous parameter) martingale Mt is a stochastic process which is “conditionally constant” in the sense that"
    }, {
      "heading" : "E[ Mt | Fs ] = Ms for all 0 ≤ s ≤ t,",
      "text" : "where the conditional expectation E is taken with respect to an nondecreasing family of sub-σ-algebras {Fs}s≥0 with the property that each Mt is Ft-measurable. Since we are only considering probability spaces (Ω,F ,P) in which Ω is the set of continuous paths in Rn and for which the σ-algebras F and {Fs}s≥0 are fixed, we will suppress these references in our notation, and refer simply to a “martingale with respect\nto P,” or a P-martingale. We are interested in the relationship between martingales and differential operators.\nDefinition 1. Let L be a second-order differential operator, and let Px be a probability measure on the space C([0,∞); Rn) of all continuous paths in Rn such that Px(X0 = x) = 1. We say that Px solves the martingale problem for L starting at x if\nMt ≡ f(Xt)− ∫ t\n0\n[Lf ](Xs)ds\nis a Px-martingale for every f ∈ C∞(Rn; R).\nNot surprisingly:\nLemma 3. If L = 1 2 ∆, then the Wiener measure Wx solves the martingale problem for L starting at x.\nProof. See Corollary 7.1.20 and Remark 7.1.23 in [Str93].\nLet us now consider the operator L = b ·∇ and the integral equation:\n(19) Yt = x + ∫ t 0 b(Ys) ds, 0 ≤ t, where Yt ≡ Y (t) is a continuous path in Rn. An equivalent differential equation is:\nY ′(t) = b(Y (t))(20)\nY (0) = x\nBy the existence and uniqueness theorem for ordinary differential equations, (19) and (20) have a unique solution, which would commonly be referred to as the integral curve of the vector field b starting at x. Intuitively, an integral curve is a curve whose tangent is identical to the given vector field at each point. Note, too, that an integral curve is a “smooth” curve if b is a smooth vector field. We have the following result:\nLemma 4. Let L = b·∇, and let Px be the unit point mass concentrated on the solution to (19) or (20) . Then Px solves the martingale problem for L starting at x.\nProof. See Exercise 7.1.32 in [Str93].\nWe now put these two examples together, and consider the differential operator:\n(21) L = 1 2 ∆ + b ·∇\nalong with the stochastic process determined by the integral equation:\n(22) Yt = Xt + ∫ t 0 b(Ys) ds, 0 ≤ t\nIn this equation, we are assuming that Xt is our original stochastic process with the usual Wiener measureWx, and Yt is a derived process with a derived probability measure.\nTheorem 4. Let L be the differential operator given by (21), and let Qx be the probability measure determined by (22) when Xt is a stochastic process whose probability law is given by Wiener measure. Then Qx solves the martingale problem for L starting at x.\nProof. See Theorem 7.3.10 in [Str93].\nIntuitively, these results show that a stochastic process defined by (6), or (9), or (13), has a “pure” diffusion part and a “pure” drift part, and the drift part follows the integral curve of the drift vector.\nThe preceding analysis is not confined to Euclidean Rn, since a similar construction works when L is given in Hörmander form by (18), see [ST94], and this means that all results can be replicated in an arbitrary Riemannian manifold, see [ST96]. The theory is explicated further in [Str00], where it serves as the foundation for Stroock’s construction and analysis of Brownian motion on a Riemannian manifold. Specifically, Section 2.2.1 of [Str00] includes a generalization of Lemma 4 above, and Theorem 2.40 of [Str00] is a generalization of Theorem 4."
    }, {
      "heading" : "3. Prototype Coding.",
      "text" : "In discussing the mathematical background of the paper in the previous section, we were actually developing, implicitly, the main elements of our geometric and probabilistic models. The potential function, U(x), and its gradient, ∇U(x), were introduced in connection with equations (6) and (7) and Theorem 1. Equation (6) is a diffusion equation with a drift term, ∇U(x), and it has an invariant probability density equal to eU(x), modulo a normalization factor. The stochastic process described by equation (6) can also be written as an Itô process, using equations (9) and (10) and Theorem 2, or it can be written in Stratonovich form, using equation (13) and Lemma 2. An alternative view of equation (6) is given by Stroock’s result, Theorem 4, on the relationship between integral curves and martingales on manifolds.\nRecall that the main goal of our theory is to construct a lowerdimensional subspace of the original Euclidean space, Rn, which is “optimal” in some sense. To be specific, let’s say that the subspace\nshould be a k-dimensional Riemannian manifold, embedded in Rn, with a local coordinate system centered at (0, 0, . . . , 0). We will use a form of prototype coding for the coordinate system, measuring the distance from the origin (i.e., the “prototype”) in k−1 specified directions. Extending this coordinate system to all of Rn, we can assume that these k − 1 coordinate directions have been chosen from among n − 1 coordinate directions in the full space. We will now follow the strategy suggested at the beginning of Section 2 for the naive FeynmanKac model. Choose U(x) and ∇U(x) so that the invariant probability density for the stochastic process given by equation (6) matches the density of our sample input data in Rn. We can then project this stochastic process onto the embedded k-dimensional manifold, and examine the probability density induced on that manifold. The hope is that this procedure will lead us to the “best” k-dimensional coordinate system for the purpose of encoding our initial data.\nHow to do this? Our first step was described briefly in the text following Theorem 2 above. We start with (6): a diffusion equation with a drift vector, ∇U(x). We then write the differential operator associated with (6) in the form given by (10):\nL = 1 2 ∑ i,j aij(x) ∂2 ∂xi∂xj + ∑ i bi(x) ∂ ∂xi\nby setting a(x) equal to the identity matrix, and setting b(x) = ∇U(x). By Theorem 2, L is the infinitesimal generator of the n-dimensional Itô process given by (9):\ndX(t) = σik(x) dB1(t)...\ndBn(t)  + b1(x)... bn(x)  dt\nThe choice of σ(x) is arbitrary, as long as a(x) = σ(x)σ(x)T is the identity matrix, which means that σ(x) must be an orthogonal transformation. These equations are expressed in Cartesian coordinates.\nTo implement the idea of prototype coding, suppose we are given a radial coordinate, ρ, and the directional coordinates θ1, θ2, . . . , θn−1. For convenience, we will use the symbol Θ to refer to the entire sequence\nof directional coordinates. Assume the existence of n coordinate transformation functions, with the usual properties:\nx1 = x1(ρ, θ1, θ2, . . . , θn−1)\nx2 = x2(ρ, θ1, θ2, . . . , θn−1)\n. . .\nxn = xn(ρ, θ1, θ2, . . . , θn−1)\nLet J(ρ,Θ) denote the Jacobian matrix of these transformation functions. We want to represent our stochastic process in this new coordinate system, and to do so we need to convert the Itô equation, given by (9), into a Stratonovich equation in the form given by (13). We have two equalities:\ndX(t) = σik(x)  ◦ dB1(t)... dBn(t)  + b̃1(x)... b̃n(x)  dt(23)\ndX(t) = J(ρ,Θ)  ◦  dXρ(t) dXθ1(t)\n... dXθn−1(t) (24) The first equality is justified by Lemma 2. The second equality is justified by the Stratonovich formula for the “chain rule,” given by (11). The notation dXρ(t), dXθ1(t), . . . , dXθn−1(t), in the second equation, expresses the fact that Xρ(t), Xθ1(t), . . . , and Xθn−1(t) are intended to represent the components of a new stochastic process defined on (ρ,Θ).\nWe can now combine and solve equations (23) and (24) to obtain: dXρ(t) dXθ1(t)\n... dXθn−1(t)\n = J(ρ,Θ) −1σik(x(ρ,Θ))  ◦ dB1(t)... dBn(t)  + J(ρ,Θ)\n−1 b̃1(x(ρ,Θ))... b̃n(x(ρ,Θ))  dt We thus have a representation of our original stochastic process, in Stratonovich form, but expressed entirely in the new (ρ,Θ) coordinate\nsystem. Note that the second term in this solution is just the transformation law for a contravariant vector, or a type (1, 0) tensor.\nNow consider the decomposition of a Stratonovich stochastic differential equation as in (15): dXρ(t) dXθ1(t)\n... dXθn−1(t)  = (A1| . . . |An) ◦ dB(t) + A0 dt By matching the components of this equation with the components of the preceding equation, we can determine the vector fields A0∂ and A1∂, . . . ,An∂. Then, applying Theorem 3 and expanding the expression inside (17), we can compute a new infinitesimal generator, L, for our stochastic process, expressed again entirely in the (ρ,Θ) coordinate system. Finally, whatever our result might be, it can be written in the following form:\n(25) L = 1 2 n−1∑ i,j=0 αij(ρ,Θ) ∂2 ∂yi∂yj + n−1∑ i=0 βi(ρ,Θ) ∂ ∂yi\nwhere y0 = ρ and yi = θi, for i = 1, . . . , n − 1. (To distinguish this equation for L from the L we started out with, we have written the coefficients of the differential operators as αij(ρ,Θ) and βi(ρ,Θ) instead of aij(x) and bi(x).) Note that this is the infinitesimal generator of an Itô process, but we derived it by an excursion through Stratonovich!\nBefore proceeding further, we need to analyze the (ρ,Θ) coordinate system. How is it defined? What are its properties? First, we want the radial coordinate, ρ, to follow the drift vector, ∇U(x). We have already seen how to do this. Suppose ρ̂(t) is the solution to the following differential equation, based on (20):\nρ̂′(t) = ∇U(ρ̂(t)) ρ̂(0) = x0\nIn other words, ρ̂(t) is the integral curve of the vector field ∇U(x) starting at x0. This is almost the construction that we want for our radial coordinate, but not quite. We will actually work with a generalization of the concept of an integral curve, known as an integral manifold. A one-dimensional integral manifold is, roughly speaking, just the image of an integral curve without the parametrization, and it always exists, for any vector field. Since we want to be able to alter the parametrization of ρ̂(t), arbitrarily, in order to choose a suitable\ncoordinate, ρ, the one-dimensional integral manifold is the device that we need.\nFor the directional coordinates, θ1, θ2, . . . , θn−1, the obvious generalization would be an integral manifold of dimension n − 1, orthogonal to the integral manifold for ρ. But, for k ≥ 2, a k-dimensional integral manifold exists if and only if certain conditions are satisfied, known as the Frobenius integrability conditions. Fortunately, as we will see, if we are looking for an integral manifold orthogonal to a vector field that is proportional to the gradient of a potential function, such as ∇U(x), then the Theorem of Frobenius gives us the results that we want. Our analysis here is based on the standard literature in differential geometry. See, e.g., [Spi99], Chapter 6; [BG68], Chapter 3; [AM77], Chapter 8. We will discuss these results in Section 4.\nTo summarize: At this point, we have a one-dimensional integral manifold for the ρ coordinate, and an orthogonal n − 1 dimensional integral manifold for the Θ coordinates. But we want to construct a lower -dimensional subspace by projecting our stochastic process onto a k − 1 dimensional subset of the coordinates θ1, θ2, . . . , θn−1. Taken together with the ρ coordinate, we want this operation to give us an “optimal” k dimensional subspace. The mathematical device that we need is a Riemannian metric, gij(x), which we will use to measure dissimilarity on the integral manifolds. And crucially: the dissimilarity metric should depend on the probability measure. Roughly speaking, the dissimilarity should be small in a region in which the probability density is high, and large in a region in which the probability density is low. We can then take the following steps:\n• To find a principal axis for the ρ coordinate, we minimize the Riemannian distance, gij(x), along the drift vector. • To choose the principal directions for the θ1, θ2, . . . , θk−1 coor-\ndinates, we diagonalize the Riemannian matrix, ( gij(x) ), and select the eigenvectors associated with the k− 1 smallest eigenvalues. • To compute the coordinate curves, we follow the geodesics of\nthe Riemannian metric, gij(x), in each of the k − 1 principal directions.\nThus, overall, we are minimizing dissimilarity, and maximizing probability. We will show how to do this, using concrete examples, in Sections 5.1 and 5.2 of this paper.\nIn the following section, we will see how to construct an integral manifold orthogonal to ∇U , and how to define a dissimilarity metric, gij(x), with the desired properties. Because of the prominent role\nplayed by the Riemannian dissimilarity metric in our theory, it is natural to describe it as a theory of differential similarity.\n4. Integral Manifolds in R3.\nFrom this point on, for purposes of exposition, we will restrict our investigations from Rn to R3. We will see later (in Section 7) that this is not a limitation on the scope of the theory, since our results can easily be generalized again to Rn. Instead, the restriction to three dimensions simplifies our calculations, and makes them easier to visualize, as we will see in Section 5.\nSince we are now working in three-dimensional Euclidean space, we are primarily interested in two-dimensional integral manifolds. Is there a two-dimensional integral manifold orthogonal to the drift vector,∇U? Consider, first, a more general case. Suppose G = (P (x), Q(x), R(x)) represents the coordinates of a vector field that is defined but not equal to (0, 0, 0) in some open region D ⊆ R3.\nTheorem 5. There exists a two-dimensional integral manifold in D with tangent plane everywhere orthogonal to G if and only if\nG · (∇×G) = 0\nProof. See [BC64], Problem 29, p. 23; [Car71], pp. 97–98; [LR75], pp. 155–156.\nIntuitively, this theorem states that G must be orthogonal to its own “curl,” a condition that is satisfied if G is proportional to the gradient of a scalar potential. Thus, any G in the form N(x)∇U(x) would work.\nWe still need a method to compute this integral manifold, however, and to define a curvilinear coordinate system on it. One approach is to choose basis vectors for a two-dimensional subspace of the tangent space at x in the following form:\nV∂ = f(x) ∂\n∂x +\n∂\n∂y (26)\nW∂ = g(x) ∂\n∂x +\n∂\n∂z\nNow compute: V×W = (f, 1, 0)× (g, 0, 1) = (1,−f,−g). If V×W is proportional to G, then G is orthogonal to the plane containing both V and W, and conversely. So we can set:\nV ×W = 1 P (x) G = 1 P (x) (P (x), Q(x), R(x))\n= (1,−f(x),−g(x))\nand obtain the results f(x) = −Q(x)/P (x) and g(x) = −R(x)/P (x). If G = ∇U(x), then Theorem 5 applies. In this case, the vector fields given by P (x)V = (−Q(x), P (x), 0) and P (x)W = (−R(x), 0, P (x)) provide what we want, namely, a basis for the tangent plane to the twodimensional integral manifold that is everywhere orthogonal to the drift vector, ∇U .\nThis construction can also be justified directly by the Theorem of Frobenius. Geometrically, we interpret V∂ and W∂ as the basis vectors for a tangent subbundle, E, in R3. (Historically, a tangent subbundle was called a “distribution,” but this term does not have the right connotations today.) We compute the Lie bracket of V∂ and W∂ as follows:\n[V∂,W∂] = V∂ ◦W∂ −W∂ ◦V∂(27)\n=\n[ ∂g\n∂y − ∂f ∂z + f(x, y, z) ∂g ∂x − g(x, y, z)∂f ∂x\n] ∂\n∂x\nNow the geometric version of the Theorem of Frobenius asserts that, if [V∂,W∂] “belongs to” E whenever V∂ and W∂ “belong to” E, then E can be extended to a full integral manifold in R3. But if V∂ and W∂ are defined by the equations in (26) and also “belong to” E, then [V∂,W∂] “belongs to” E if and only if the bracketed expression on the right-hand side of (27) is identically zero. This leads to the following classical statement of the Frobenius integrability conditions as a system of partial differential equations:\n∂g ∂y + f(x, y, z) ∂g ∂x = ∂f ∂z + g(x, y, z) ∂f ∂x\nAs a further check on Theorem 5, we can verify by a direct computation that the preceding equation holds for f(x) and g(x), as defined previously, when G = ∇U(x).\nTo simplify the notation, let us absorb the factor P (x) into the definition of the two tangential vector fields, and write:\n∇U(x) = (P (x), Q(x), R(x)) V(x) = (−Q(x), P (x), 0) W(x) = (−R(x), 0, P (x))\nIn this form, it is easy to see that∇U(x) is orthogonal to both V(x) and W(x). Note also that V(x) and W(x) are not orthogonal to each other, although the vector fields V∂ = V(x)/P (x) and W∂ = W(x)/P (x) commute, as we have seen, when viewed as differential operators. Now one way to use these tangential vector fields is to compute a global (ρ, θ, φ) coordinate system. For example, we can compute the integral\ncurves of the vector field V(x) and use these for a coordinate called θ, and we can compute the integral curves of the vector field W(x) and use these for a coordinate called φ. Note that the θ coordinate curves will all lie in the global xy plane, and the φ coordinate curves will all lie in the global xz plane, if we take this approach.\nBut another approach is to use these vector fields to construct a local coordinate system. Any linear combination of V(x) and W(x) could be taken as one of the basis vectors for the tangent subbundle, and we can vary this linear combination as we move around the integral manifold. To implement this idea, it is useful to define a Riemannian metric on the integral manifold. The most natural way to do this is to define a metric tensor on all of R3, using the inner products of ∇U(x), V(x) and W(x), in that order. We thus define:gij(x)\n =  P 2(x) +Q2(x) +R2(x) 0 00 P 2(x) +Q2(x) Q(x)R(x)\n0 Q(x)R(x) P 2(x) +R2(x)  To remain consistent with the coordinate notation introduced in Section 3, we let i and j range over 0, 1, 2, and we stipulate that y0 = ρ, y1 = θ, y2 = φ. Since P (x), Q(x), R(x), are the components of the drift vector, ∇U(x), and since the diffusion equation in which ∇U(x) appears has an invariant probability density that is determined by the exponential of the potential function, U(x), it should be clear that gij(x) has at least some of the properties that we have been looking for. We thus adopt this formula, provisionally, as the definition of our dissimilarity metric.\nThe matrix (gij(x)) is not diagonal, in general, but it can easily be diagonalized. The eigenvectors are:\nξ1 =  0−R(x) Q(x)  , ξ2 =  0Q(x) R(x)  , ξ3 =  10 0  , and the corresponding eigenvalues are: λ1(x) = P\n2(x) and λ2(x) = λ3(x) = P\n2(x) + Q2(x) + R2(x). This analysis leads to a spectral decomposition of (gij(x)) as:\n20 L. THORNE MCCARTY\nλ1(x)κ(x)  0 0 00 R2(x) −Q(x)R(x) 0 −Q(x)R(x) Q2(x)  +\nλ2(x)κ(x)  0 0 00 Q2(x) Q(x)R(x) 0 Q(x)R(x) R2(x)  + λ2(x)  1 0 00 0 0 0 0 0  where κ(x) = 1/(Q2(x) + R2(x)). Obviously, the third term in this expression corresponds to the ρ coordinate. Since λ1(x) is always the smallest eigenvalue, we will use the eigenvector ξ1 to determine the initial direction of the θ coordinate on the surface of the integral manifold orthogonal to ∇U(x). It is obvious, too, that ξ1 and ξ2 are orthogonal to each other, even though V(x) and W(x) are not.\nThe main application of our dissimilarity metric, however, is to compute geodesics on the surface of the integral manifold orthogonal to ∇U(x). Recall that any linear combination of V(x) and W(x) yields a vector in the tangent subbundle, E, and thus we can construct vector fields in E in the form v(t)V(x) + w(t)W(x) for arbitrary functions v(t) and w(t). For a geodesic, we are looking for a curve γ(t) with values in R3 which minimizes the “energy” functional:\n(28) 1\n2 ∫ T 0 ( v(t) w(t) )( g11(γ(t)) g12(γ(t)) g21(γ(t)) g22(γ(t)) )( v(t) w(t) ) dt\nsubject to the constraint:\n(29) γ ′(t) = v(t)V(γ(t)) + w(t)W(γ(t))\nThis variational problem leads to a system of Euler-Lagrange equations for the curves γ(t) = (x(t), y(t), z(t)) and (v(t), w(t)), plus three Lagrange multipliers. For initial conditions, we specify (x(0), y(0), z(0)) and we use the smallest eigenvalue of gij(x(0), y(0), z(0)) to determine the initial value (v(0), w(0)). This is a complicated system of equations, but it can be solved numerically in Mathematica.\nThe preceding analysis was based on a global coordinate system centered on the x axis, since our initial vector fields were determined by the equations P (x)V = (−Q(x), P (x), 0) and P (x)W = (−R(x), 0, P (x)). But we could also work with a coordinate system centered on the y axis, using the equations Q(x)V = (0,−R(x), Q(x)) and Q(x)W = (Q(x),−P (x), 0), or the z axis, using R(x)V = (R(x), 0,−P (x)) and R(x)W = (0, R(x),−Q(x)). In fact, it is useful to be able to switch from one such coordinate system to another, as we move around the\nintegral manifold. Since g00(x) = P 2(x)+Q2(x)+R2(x) = |∇U(x)|2, it is obvious that the ρ coordinate is independent of the global coordinate system used to define it. But the same is true of gij(x) when i 6= 0 and j 6= 0. To see this, let y1 and y2 denote the Θ coordinates centered on the x axis, and let ȳ1 and ȳ2 denote the Θ coordinates centered on the y axis. The Jacobian matrix of the coordinate transformation from ȳk to yi can be computed as follows: ∂yi/∂ȳk \n=  ∂x/∂ρ −Q(x) −R(x)∂y/∂ρ P (x) 0 ∂z/∂ρ 0 P (x) −1 ∂x/∂ρ 0 Q(x)∂y/∂ρ −R(x) −P (x) ∂z/∂ρ Q(x) 0  =\n 1 0 00 −R(x)/P (x) −1 0 Q(x)/P (x) 0  Now let gij(x) and ḡkl(x) denote the dissimilarity metric based on the yi and ȳk coordinates, respectively. Restricting our attention to the 2× 2 matrix for the Θ coordinates, we compute:(\nḡ11(x) ḡ12(x) ḡ21(x) ḡ22(x) ) = ( Q2(x) +R2(x) P (x)R(x) P (x)R(x) P 2(x) +Q2(x) )\n= ( −R(x)/P (x) −1 Q(x)/P (x) 0 )T ( g11(x) g12(x) g21(x) g22(x) )( −R(x)/P (x) −1 Q(x)/P (x) 0 ) But this is just an instantiation of the transformation law for a type (0,2) tensor:\nḡkl(x) = 2∑\ni,j=1\n∂yi ∂ȳk gij(x) ∂yj ∂ȳl\nThe same calculations obviously lead to the same results for all pairwise transformations among the three global coordinate systems. Thus, on a two-dimensional integral manifold, for a fixed ρ, the dissimilarity metric, gij(x), is independent of the global coordinate system used to define it.\n5. Experiments with Mathematica.\nTo sharpen our intuitions, and before developing the theory of differential similarity any further, let’s look at some experiments in R3 using the computational and graphical facilities of Mathematica. Section 5.1 is a comprehensive study of the Gaussian case, which is the one example that can be solved analytically. Section 5.2 then considers what we will refer to as the “curvilinear Gaussian” case. Here, we apply a quadratic potential function to the output of a cubic polynomial coordinate transformation, producing an example that cannot be solved analytically, but which still retains some degree of tractability. Finally, in Section 5.3, we put two “curvilinear Gaussians” together in a mixture distribution.\nThe source code for these examples is available in three Mathematica notebooks:\nGaussian.nb CurvilinearGaussian.nb BimodalCurvilinearGaussian.nb\n5.1. The Gaussian Case. Consider, first, the case of a quadratic potential, for which most results can be obtained analytically in closed form. Define U(x) as follows:\nU(x, y, z) = −1 2 (ax2 + by2 + cz2)\nThen the gradient is: ∇U(x, y, z) = (−ax,−by,−cz), and the derived potential V (x) is:\nV (x, y, z) = 1 2 (a2x2 + b2y2 + c2z2)− 1 2 (a+ b+ c)\n(We can ignore the constant term.) It is well known that the FeynmanKac formula, given by either (4) or (7), has a closed-form solution whenever U(x) and V (x) are quadratic polynomials. Furthermore, the invariant probability measure, eU(x), given by Theorem 1, is obviously a Gaussian. Adding in the normalization factor, the invariant probability density function is: (30) √ abc (2π)− 3 2 exp [ −1\n2 (ax2 + by2 + cz2) ] Note that the covariance matrix in (30) is already in diagonalized form.\nFor a numerical example, set a = 1, b = 2, c = 4. Figure 1 then shows the surface defined by the equation U(x, y, z) = −2. Figure 2 shows a StreamPlot of the gradient vector field generated by∇U(x, y, z) at z = 0. This picture makes sense, intuitively. Notice that the drift vector is\n“transporting probability mass towards the origin,” to counteract the dissipative effects of the diffusion term in the stochastic process. If the system is in perfect balance, of course, we have an invariant probability measure, which in this case is a Gaussian.\nThe Gaussian case is simple enough that we can solve the differential equations explicitly in Mathematica, using DSolve. First, the integral curve of the vector field ∇U(x) = (P (x), Q(x), R(x)) starting at x0 = (x0, y0, z0) is given by:\nρ̂(t) =  x0 e−aty0 e−bt z0 e −ct  For the tangential vector fields, we will start with a global coordinate system centered on the x axis, so that V(x) = (−Q(x), P (x), 0) = (by,−ax, 0) and W(x) = (−R(x), 0, P (x)) = (cz, 0,−ax). Then the integral curve of the vector field V∂ starting at x1 = (x1, y1, z1) is given by:\nθ̂(t) =  x1 cos√ab t + y1 √b/a sin√ab ty1 cos√ab t − x1 √a/b sin√ab t z1  and the integral curve of W∂ starting at x2 = (x2, y2, z2) is given by:\nφ̂(t) =  x2 cos√ac t + z2 √c/a sin√ac ty2 z2 cos √ ac t − x2 √ a/c sin √ ac t  Figure 3 shows the global coordinate system on a two-dimensional integral manifold that would be generated by these curves. Note that the θ coordinate curves lie in the xy plane, and the φ coordinate curves lie in the xz plane, as expected.\nGiven the curves ρ̂(t), θ̂(t) and φ̂(t), what does it mean to say that a point in R3 has the coordinates (ρ, θ, φ)? We adopt the following conventions: Starting with the x axis as the principal axis, choose a “maximal” point x0 = (x0, 0, 0) and follow the curve ρ̂(t) towards the origin. There are two natural measures of distance along this curve: the Euclidean arc length, which in this case is just the value of the xcoordinate, and the Riemannian arc length, which is determined by our dissimilarity metric, gij(x). Our choice here is to use the Euclidean arc length to specify the ρ coordinate. (We will subsequently see another role for the Riemannian arc length.) In the Gaussian case, therefore, ρ has the value x0 e\n−at, which ranges over the interval (0, x0] as t ranges from ∞ to 0. But by choosing a value for ρ, we are also choosing the\nintegral manifold on which θ̂(t) and φ̂(t) are defined. Therefore, to interpret the coordinates θ and φ, starting at (ρ, 0, 0), we traverse the\ndistance θ along the θ̂(t) curve from the slice θ = 0, and we traverse\nthe distance φ along the φ̂(t) curve from the slice φ = 0, until we arrive\nat the point (ρ, θ, φ). Note, too, that we can traverse the θ̂(t) and\nφ̂(t) curves in either order, and still arrive at the same point, as long as we remain within a neighborhood of (ρ, 0, 0) in which these curves intersect. The black dots in Figure 3 may be helpful in visualizing this procedure.\nOnce again, the Gaussian case is simple enough that we can analyze the coordinate transformation from (ρ, θ, φ) to (x, y, z), and derive an explicit expression for its Jacobian matrix. First, let ~θs(x) = θ̂x(s) denote the flow of the vector field V∂ starting at x, and similarly let ~φt(x) = φ̂x(t) denote the flow of the vector field W∂ starting at x. Applying the composition, ~θs ◦ ~φt, of the flows ~θs and ~φt to the point x = (ρ, 0, 0), we obtain the following equations, for arbitrary s and t:\nx = ~x(ρ, s, t) = ρ cos √ ab s cos √ ac t(31)\ny = ~y(ρ, s, t) = − ρ √ a/b sin √ ab s cos √ ac t\nz = ~z(ρ, s, t) = − ρ √ a/c sin √ ac t\nBy a simple calculation:\n∂\n∂s  ~x(ρ, s, t)~y(ρ, s, t) ~z(ρ, s, t)  =  b ~y(ρ, s, t)−a ~x(ρ, s, t) 0  In other words, ∂/∂s = V∂. By another simple calculation, setting s = 0 in (31), we have:\n∂\n∂t  ~x(ρ, 0, t)~y(ρ, 0, t) ~z(ρ, 0, t)  =  −ρ √ac sin√ac t0 −a ρ cos √ ac t  =  c ~z(ρ, 0, t)0 −a ~x(ρ, 0, t)  In other words, ∂/∂t = W∂ at s = 0. But we can now show that, if we move along the V∂ coordinate curves from the slice s = 0 at (ρ, 0, t) to a new slice, where s 6= 0, the components of W∂ do not change. This result follows from the commutative property of the coordinate vector fields: V∂ ◦W∂ = W∂ ◦V∂. See, e.g., [BG68], Theorem 3.7.1, and [Spi99], Lemma 5.13. We can obtain a similar result if we reverse the composition of the flows ~θs and ~φt, and apply ~φt ◦ ~θs to the point x = (ρ, 0, 0). In this case, we can compute ∂/∂t = W∂ for all s and t, and ∂/∂s = V∂ for t = 0. Again, if we move along the W∂ coordinate\ncurves from the slice t = 0 at (ρ, s, 0) to a new slice, where t 6= 0, the components of V∂ do not change.\nNow consider the coordinate transformation itself. Start anywhere on the slice s = 0 and follow the flow ~θs to s = θ. Separately, start anywhere on the slice t = 0 and follow the flow ~φt to t = φ. If we are within a sufficiently small neighborhood of (ρ, 0, 0), these curves will intersect at some point (x, y, z) on the integral manifold for ρ. Thus, applying the inverse function theorem, we can recover the coordinate values (ρ, θ, φ) = (ρ(x, y, z), θ(x, y, z), φ(x, y, z)). See, e.g., [BG68], Theorem 3.7.1, or [BC64], Theorem 1.5. Finally, even though we may not be able to write down an explicit formula for the coordinate transformation, in general, we can see from the equations above that the Jacobian matrix of (x, y, z) = (x(ρ, θ, φ), y(ρ, θ, φ), z(ρ, θ, φ)) can be written explicitly as:\n(32) J(ρ, θ, φ) =  x(ρ, θ, φ)/ρ b y(ρ, θ, φ) c z(ρ, θ, φ)y(ρ, θ, φ)/ρ −a x(ρ, θ, φ) 0 z(ρ, θ, φ)/ρ 0 −a x(ρ, θ, φ)  This is all we need to carry out the calculations described in Section 3, including the calculation of the coefficients αij(ρ, θ, φ) and βi(ρ, θ, φ) in Equation (25). We will analyze these results further in Section 6.\nWe have referred to the x axis in Figure 3 as the “principal axis” because of its correspondence to the results of Principal Component Analysis (PCA) in traditional linear statistics. For the Gaussian probability density given by (30), with a = 1, b = 2, c = 4, the first component identified by PCA would be the x axis, and the second component would be the y axis. Thus the “principal surface” would be defined by the xy plane, which corresponds to the (ρ, θ) surface in our curvilinear coordinate system. Figure 4 depicts this surface, with the ρ and θ coordinates illustrated. The maximal point on the principal axis is (10, 0, 0), and the θ coordinate curves have been evenly spaced along the ρ coordinate curve from (10, 0, 0) to (0, 0, 0). Similarly, the ρ coordinate curves have been evenly spaced along the maximal θ coordinate curve, which passes through the point (10, 0, 0). Clearly, the (ρ, θ) surface in Figure 4 encodes the same information as the xy plane does in traditional linear statistics.\nWhat if our potential function, U(x), while still a quadratic, was not already in diagonalized form? Figure 5 shows an example in which the potential function depicted in Figure 1 has been rotated through the angle π/3 around the line from (0, 0, 0) to (1, 1, 1). Under this rotation, the maximal point on the principal axis, (10, 0, 0), would be displaced to the position (20/3,−10/3, 20/3). If our basis vectors, V(x)\nand W(x), were also rotated in the same way, we could still compute closed form solutions to the differential equations, using DSolve, and this procedure would still give us explicit expressions for the functions ρ̂(t), θ̂(t) and φ̂(t), although these expressions would be more complex than they were previously. Continuing in this way, as before, we would eventually produce the (ρ, θ) surface shown in Figure 6.\nBut this calculation would only be possible if we knew, a priori, what the rotation was. Suppose we had no information about the rotation. And, to make the task even more challenging, suppose we were not able to rely on symbolic methods in Mathematica, such as DSolve, but had to use numerical methods, such as NDSolve. Could we still compute the (ρ, θ) surface shown in Figure 6?\nWe need to address a preliminary issue: When we were working with DSolve in the simple Gaussian case, we were able to compute an explicit expression for ρ̂(t) and convert it into a formula for the ρ coordinate measured in Euclidean arc length. Basically, we were constructing a new parametrization of ρ̂(t). This is not easy to do in the general case, however, because it would require us to invert the general formula for arc length. Fortunately, there is a simpler approach, which works very well using NDSolve. In place of the differential equation derived from (20), we use the normalized version:\nγ′(t) = ∇U(γ(t)) ‖∇U(γ(t))‖\nγ(0) = x0\nSince our tangent vector now has length 1, the integral curve that solves this equation will be parametrized by Euclidean arc length, but otherwise it will be identical to ρ̂(t). The formula for Riemannian arc length, using our dissimilarity metric, gij(x), is also very simple when γ(t) is defined in this way:\n∫ T 0 √√√√√( 1 0 0 )  gij(γ(t))  10 0  dt = ∫ T 0 √ g00(γ(t)) dt\nNote that a similar normalized differential equation could be used with the vector fields V(x) and W(x), if we wanted to compute integral\ncurves θ̂(t) and φ̂(t) parametrized by Euclidean arc length, although the formula for the Riemannian arc length would be different.\nWe are now ready to compute the (ρ, θ) surface in Figure 6, without knowledge of the rotation, and without the use of analytical methods in\nMathematica. We will start off with the basis vectors V(x) and W(x) centered on the x axis. There are three steps:\n(1) Find a principal axis for the ρ coordinate.\nThe basic idea is to find a point (x0, y0, z0) at a fixed Euclidean distance from the origin, and an integral curve γ(t) which solves the normalized differential equation for ∇U(x) starting at x = (x0, y0, z0), and for which the Riemannian distance, gij(x), measured along γ(t) for a fixed interval, t, is minimal. In short, we are looking for the least Riemannian distance for a fixed Euclidean distance.\nWe use NDSolve to compute γ(t), and we use NIntegrate to compute the Riemannian distance along γ(t). FindMinimum then searches for the minimal point (x0, y0, z0) satisfying these constraints. In our rotated Gaussian example, we can start the search at (10, 0, 0) with the constraint that (x0, y0, z0) must lie on the sphere x2 + y2 + z2 = 100, and FindMinimum will return the value (x0, y0, z0) = (6.66666,−3.33335, 6.66667). This is a reasonably good match with the analytical value, (x0, y0, z0) = (20/3,−10/3, 20/3).\n(2) Determine the initial directions of the θ coordinate.\nThe basic idea is to work with the eigenvector, ξ1(x), associated with the smallest eigenvalue, λ1(x), of the dissimilarity matrix, ( gij(x) ) , at the point x = (x0, y0, z0). For expository purposes, let’s initially use the analytical value (x0, y0, z0) = (20/3,−10/3, 20/3). Then the smallest eigenvalue is 400/9, and its associated eigenvector is (0, 2, 1). However, the inner product of the basis vectors, V(x) and W(x), at (x0, y0, z0) is negative, which means that we need to flip the sign of, say, W(x), to determine the initial directions of the θ coordinate. We thus set (v(0), w(0)) = (2,−1), for one direction, and (v(0), w(0)) = (−2, 1), for the opposite direction.\nThere is only a minor difference if we use the numerical value (x0, y0, z0) = (6.66666,−3.33335, 6.66667). We then set (v(0), w(0)) = (1.99996,−1), for one direction, and, for the opposite direction, (v(0), w(0)) = (−1.99996, 1).\n(3) Compute the geodesic curves of the (ρ, θ) coordinate system.\nIn the final step, we compute the geodesic curves that solve the variational problem given by (28) and (29), with the initial value (x(0), y(0), z(0)) = (6.66666,−3.33335, 6.66667) and\nwith (v(0), w(0)) equal to either (1.99996,−1) or (−1.99996, 1). Mathematica has a VariationalMethods package which computes the Euler-Lagrange equations symbolically from the specification of a variational problem. We use this package, and then solve the resulting equations numerically with NDSolve.\nThere is one complication: NDSolve encounters a singularity when the value of P (x) gets very small and the values of v(t) and w(t) grow very large. We will examine a solution to this problem in Section 5.2, below.\nThe end result of these three steps is a radial coordinate curve, ρ, and an outer transverse coordinate curve, θ, which match perfectly the curves in Figure 6, within the expected tolerance of a numerical approximation and up to the location of the numerical singularities.\nThis match between the numerical results and the analytical results provides some evidence that our techniques are working correctly. We will now apply these techniques to an example for which analytical results are not available.\n5.2. The Curvilinear Gaussian. For this example, we start with a cubic polynomial: C(t) = t3−t2−t. We then define a cubic polynomial coordinate transformation from (x, y, z) to (u, v, w) as follows:\nu = u(x, y, z) = C(1.4 y) + 2x(y2 + z2)\nv = v(x, y, z) = C(1.2 z) + 2y(z2 + x2)\nw = w(x, y, z) = C(1.0 x) + 2z(x2 + y2)\nFinally, we define U(x) as a quadratic potential function in the variables u, v and w:\nU(x, y, z) = −1 2 (a u(x, y, z)2 + b v(x, y, z)2 + cw(x, y, z)2) ∗ 10−6\nThus U(x) is a sixth-degree polynomial in x, y and z, and the gradient, ∇U(x), is a fifth-degree polynomial. There are no known closed-form solutions to the Feynman-Kac formula, given by either (4) or (7), when U(x) and V (x) are higher-order polynomials. However, it is possible to discretize the Feynman-Kac “path integral,” and obtain approximate numerical solutions. See, for example, [Lya04].\nFor a numerical example, set a = 1, b = 2, c = 4. Figure 7 then shows the surface defined by the equation U(x, y, z) = −10. Figure 8 shows a StreamPlot of the gradient vector field generated by ∇U(x, y, z) at z = −10. Figure 9 shows a stack of such stream plots, at the values z = 10, z = 0 and z = −10. Notice how the drift vector twists and\nturns to counteract the dissipative effects of the diffusion term, and maintain an invariant probability measure.\nFigure 10 is analogous to Figure 3 in the Gaussian case, and depicts the integral manifold that passes through the point (20, 0,−10). The coordinate curves in Figure 10 are generated by a global coordinate system centered on the x axis, with P (x)V = (−Q(x), P (x), 0) and P (x)W = (−R(x), 0, P (x)). These curves are thus analogous to the global θ and φ coordinate curves shown in Figure 3.\nFinally, Figure 11 shows the (ρ, θ) surface computed by our numerical techniques, and analogous to the (ρ, θ) surface in Figure 6. As before, we start off with the basis vectors V(x) and W(x) centered on the x axis, and we proceed through three steps:\n(1) Find a principal axis for the ρ coordinate.\nAgain, we use NDSolve, NIntegrate and FindMinimum. We start the search at (20, 0,−10), and impose the constraint that (x0, y0, z0) must lie on the sphere x\n2 +y2 +z2 = 500. We obtain the result: (x0, y0, z0) = (20.4316, 1.27953,−8.99505).\n(2) Determine the initial directions of the θ coordinate.\nWorking with the eigenvector ξ1(x) at the point x = (20.4316, 1.27953,−8.99505), we set (v(0), w(0)) = (7.03133,−1) and, alternatively, (v(0), w(0)) = (−7.03133, 1). The first pair of values will give us the θ coordinate curve in the clockwise direction, as seen from the vantage point of Figure 11; the second pair of values will give us the θ coordinate curve in the counterclockwise direction.\n(3) Compute the geodesic curves of the (ρ, θ) coordinate system.\nUsing the initial values calculated in steps (1) and (2), we construct the Euler-Lagrange equations for the variational problem given by (28) and (29), and we solve them using NDSolve. We encounter singularities in both coordinate curves, however, and we cannot extend the geodesics into the negative x and positive z quadrant.\nThese three steps generate the principal ρ coordinate curve and the outer θ coordinate curves shown in Figure 11, at least in the positive x and the negative z quadrant. The remaining θ coordinate curves in this quadrant are evenly spaced along the ρ coordinate, and the remaining ρ coordinate curves are evenly spaced along the outer θ coordinate, and they have been generated by the same three steps.\nTo complete Figure 11, we need another approach. One solution is to draw a second set of coordinate curves starting from the antipodal point in the opposite quadrant. Again, there are three steps:\n(1) Find a principal axis for the ρ coordinate.\nTo find our initial starting point, (x0, y0, z0), we used NDSolve, NIntegrate and FindMinimum to search for the least Riemannian distance for a fixed Euclidean distance. In fact, the Riemannian distance from (20.4316, 1.27953,−8.99505) to the origin is 6.30863, but this distance is the same from any point on the integral manifold. For example, the Riemannian distance to the origin along the ρ coordinate curves at 315◦, 45◦, and 90◦, is also 6.30863, although the Euclidean distance is different in each case. Thus, to find the antipodal point in the negative x and positive z quadrant, we search along the integral manifold at a constant Riemannian distance to find the greatest Euclidean distance. FindMaximum gives us the result that we want: (x1, y1, z1) = (−19.2034,−1.25676, 9.25639).\n(2) Determine the initial directions of the θ coordinate.\nWorking with the eigenvector ξ1(x) at the point (x1, y1, z1), we set (v(0), w(0)) = (7.36377,−1) for the θ coordinate curve in the clockwise direction, and (v(0), w(0)) = (−7.36377, 1) for the θ coordinate curve in the counterclockwise direction.\n(3) Compute the geodesic curves of the (ρ, θ) coordinate system.\nUsing the initial values calculated in steps (1) and (2), we construct the Euler-Lagrange equations for the variational problem given by (28) and (29), and we solve them using NDSolve. Although we encounter singularities again in both cases, we can see from a close inspection of Figure 11 that there remains only a small gap to the location of the singularities along the θ coordinate curves that were previously constructed from (x0, y0, z0).\nThese three steps generate the principal ρ coordinate curve and the outer θ coordinate curves shown in Figure 11, in the negative x and positive z quadrant. The remaining θ coordinate curves in this quadrant are evenly spaced along the ρ coordinate, and the remaining ρ coordinate curves are evenly spaced along the outer θ coordinate. The reader will note that the coordinate curves in both quadrants in Figure 11 match up reasonably well.\n5.3. The Bimodal Curvilinear Gaussian. Finally, we consider a bimodal case. Figure 12 shows two copies of the curvilinear Gaussian defined in Section 5.2. One copy has been translated from (0, 0, 0) to (20, 20,−10). The other copy has been translated from (0, 0, 0) to (−20,−20, 10) and rotated by π/2 around a line parallel to the y-axis. But the probability density is a mixture. If U1(x) is the potential function for the first copy and U2(x) is the potential function for the second copy, then the invariant probability density is given by:\neU(x) ' eU1(x) + eU2(x),\nmodulo an appropriate normalization factor. Figure 12 is actually showing the surface defined by the equation:\neU1(x,y,z) + eU2(x,y,z) = 0.0001\nThe advantage of this representation lies in the fact that our calculations for each copy will be almost independent of each other. Observe that the effective potential function for the mixture will be:\nU(x) ' log(eU1(x) + eU2(x))\nThus the gradient of U(x) in a neighborhood of (20, 20,−10) will be almost identical to the gradient of U1(x) computed by itself, and the gradient of U(x) in a neighborhood of (−20,−20, 10) will be almost identical to the gradient of U2(x) computed by itself.\nThe mixture distribution thus provides a useful representation of clusters. Analyzing the situation, intuitively, in terms of our dissimilarity metric, the two clusters in Figure 12 will be exponentially far apart."
    }, {
      "heading" : "6. Diffusion Coefficients and Dissimilarity Metrics",
      "text" : "Recall the main results from Section 3: We started with a diffusion process represented by an Ito stochastic differential equation, in Cartesian coordinates; we transformed this into a Stratonovich equation in the coordinates (ρ,Θ); and we then converted this back into an Ito process characterized by a differential operator with coefficients αij(ρ,Θ) and βi(ρ,Θ). The one necessary ingredient was the Jacobian matrix of the coordinate transformation.\nAs an illustration, let’s try a brute force solution of these equations in the simple Gaussian case discussed in Section 5.1. The Jacobian is given by equation (32). For ease of reference, here is equation (23),\nrewritten for the three-dimensional coordinate system (ρ, θ, φ):\ndX(t) = σik(x(ρ, θ, φ))  ◦ dB1(t)dB2(t) dB3(t)  + b̃1(x(ρ, θ, φ))b̃2(x(ρ, θ, φ)) b̃3(x(ρ, θ, φ))  dt For the moment, we will assume that (σik(x(ρ, θ, φ))) is an orthogonal transformation, but otherwise arbitrary. Our procedure is to combine and solve equations (23) and (24), and then expand the result using Theorem 3. When we do so, we discover that the “sum of squares” inside equation (17) yields an expression consisting of 2679 terms! However, by using the fact that (σik(x(ρ, θ, φ))) is an orthogonal transformation, we can eliminate all terms in which the factors σik appear without derivatives. Furthermore, all the terms that include derivatives of σik are cancelled out by similar terms in the expansion of A0∂ inside equation (17). The net result is equation (25), in the following form:\nL = 1 2 2∑ i,j=0 αij(ρ, θ, φ) ∂2 ∂yi∂yj + 2∑ i=0 βi(ρ, θ, φ) ∂ ∂yi\nwhere y0 = ρ, y1 = θ and y2 = φ. Thus, the exact choice we make for the transformation (σik(x(ρ, θ, φ))) turns out to be irrelevant. However, the diffusion coefficients αij(ρ, θ, φ) and the drift coefficients βi(ρ, θ, φ) are still very complex, and they do not provide much insight into the structure of the solution, even in the simple Gaussian case.\nFor more insight, let’s separate the ρ coordinate from the Θ coordinates. The basic idea of the (ρ,Θ) coordinate system was to align the ρ coordinate with the drift vector, ∇U(x), so that the trajectory of our stochastic process in the direction of the Θ coordinates would be orthogonal to the drift. The definition of our dissimilarity metric, ( gij(x) ), also exhibited a strong separation between the ρ coordinate and the Θ coordinates. So there is a natural question here: What is the relationship between the representation of our stochastic process in Θ coordinates and the Θ submatrix of ( gij(x) )?\nThe answer is well known in the case of pure Brownian motion, without drift. The earliest example is in [Str71] and [Itô75]. Stroock discovered that if you project Brownian motion in R3 onto the surface of a sphere of radius r centered at (0, 0, 0), the differential operator of the resulting stochastic process, in spherical coordinates, (r, ϑ, ϕ), is:\nL = 1 2 1 r2\n( ∂2\n∂ϑ2 +\n1\nsin2 ϑ\n∂2\n∂ϕ2 +\n1\ntanϑ\n∂\n∂ϑ\n)\nwhich is the spherical Laplacian divided by 2. This result can be generalized to an arbitrary Riemannian manifold,M, embedded in Rn. For any f ∈ C∞(M; R), the Laplace-Beltrami operator, ∆M, is defined by:\n∆M f = divM (gradM f)\nin which the divergence, divM, and the gradient, gradM, can both be defined on M independently of a coordinate system.\nTheorem 6. Let M be an embedded submanifold of Rn, and let ∆M be the Laplace-Beltrami operator on M. Then\nL = 1 2 ∆M\nis the differential operator of a Brownian motion process in Rn that has been projected orthogonally onto M.\nProof. The proof starts by showing that L can always be written in Hörmander form without the V0∂ term. In particular, we can write:\nL = 1 2 ∆M = 1 2 n∑ k=1 (ΠM(ek∂) ∂) 2\nwhere (e1, e2, . . . , en) is an orthonormal basis for R n and ΠM is the orthogonal projection operator from the tangent bundle in Rn onto the tangent bundle in M. See Section 4.2.1 of [Str00] or Theorem 3.1.4 in [Hsu02]. From this result, it follows that we can construct a diffusion process onM whose increments are precisely the projections, under ΠM, of the increments of a Brownian motion process in R\nn, and whose differential operator is L. For the details, see Theorem 4.37 in [Str00].\nThe diffusion process constructed in Theorem 6 is known as Brownian motion on M.\nLet us now analyze the stochastic process defined by equation (6), or (9), or (13), projected onto the ρ and Θ coordinates separately. To simplify the calculations, we will initially focus our attention on the simple Gaussian case, in which ∇U(x, y, z) = (−ax,−by,−cz), and we will start with a construction borrowed from [Str71] and [Itô75], but\nadapted to match this example. Consider the following matrix:πij(x, y, z)  =  1 0 00 1 0 0 0 1  − 1\n|∇U(x, y, z)|2  −a x−b y −c z ( −a x −b y −c z ) in which the product in the second line should be interpreted as the multiplication of a 3× 1 matrix times a 1× 3 matrix, yielding a 3× 3 matrix. It is easy to check that ( πij(x)\n) is idempotent:πik(x, y, z)  πkj (x, y, z)  = πij(x, y, z) \nand that it maps the vector ∇U(x, y, z) onto the origin:πij(x, y, z)   −ax−by\n−cz\n =  00\n0  Thus ( πij(x) ) is a projection onto the plane tangent to the integral manifold at (x, y, z). We now apply this projection operator to the right-hand side of equation (23), as rewritten above. First, we set σ equal to the identity matrix, so that b̃ = b = ∇U . (See the discussion following Lemma 2 in Section 2.2.) Then the projection operator ( πij(x) ) annihilates the second term in (23), and we are left with:\ndX(t) = πij(x(ρ, θ, φ))  ◦ dB1(t)dB2(t) dB3(t) (33) We now combine equation (33) with equation (24), and solve this system of equations to obtain:dXρ(t)dXθ(t)\ndXφ(t)\n = J(ρ, θ, φ) −1πij(x(ρ, θ, φ))  ◦ dB1(t)dB2(t) dB3(t) (34) As a verification that our calculations are on the right track, we note that the multiplication of the two matrices on the right-hand side of (34) produces a matrix in which the first row is identically zero. This means that dXρ(t) = 0, which is exactly the result that we want.\nWe now continue the procedure outlined in Section 3, applying Theorem 3 to equation (34), and expanding the “sum of squares” inside (17). This allows us to compute the coefficients αij(ρ, θ, φ) and βi(ρ, θ, φ) in (25). It turns out that αij(ρ, θ, φ) = 0 whenever i = 0 or j = 0, which is what we would expect. For the remaining diffusion coefficients, we compute:\nα11(ρ, θ, φ) = a2 x(ρ, θ, φ)2 + c2 z(ρ, θ, φ)2\na2 x(ρ, θ, φ)2 |∇U |2\nα22(ρ, θ, φ) = a2 x(ρ, θ, φ)2 + b2 y(ρ, θ, φ)2\na2 x(ρ, θ, φ)2 |∇U |2\nα12(ρ, θ, φ) = α21(ρ, θ, φ) = − b c y(ρ, θ, φ) z(ρ, θ, φ) a2 x(ρ, θ, φ)2 |∇U |2\nAlternatively, we can write the nonzero diffusion coefficients as a 2× 2 matrix:( αij(ρ, θ, φ) ) = 1\na2 x(ρ, θ, φ)2 ×\n(( 1 0 0 1 ) − 1 |∇U |2 ( −b y(ρ, θ, φ) −c z(ρ, θ, φ) )( −b y(ρ, θ, φ) −c z(ρ, θ, φ) )) It turns out also that the the drift coefficient β0(ρ, θ, φ) = 0, as we would expect, and for the other drift coefficients we compute:\nβ1(ρ, θ, φ) = b y(ρ, θ, φ)\n2 a x(ρ, θ, φ) |∇U |2 ×(b+ c) + 1\n|∇U |2  a2 x(ρ, θ, φ)b2 y(ρ, θ, φ) c2 z(ρ, θ, φ)  · ∇U \nβ2(ρ, θ, φ) = c z(ρ, θ, φ)\n2 a x(ρ, θ, φ) |∇U |2 ×(b+ c) + 1\n|∇U |2  a2 x(ρ, θ, φ)b2 y(ρ, θ, φ) c2 z(ρ, θ, φ)  · ∇U \nKeep in mind that these are the coefficients for the first-order terms ∂/∂θ and ∂/∂φ.\nFor comparison, we will now compute the Laplace-Beltrami operator for the simple Gaussian case, using our Riemannian dissimilarity\nmetric, gij(ρ, θ, φ) = gij(x(ρ, θ, φ)), on the two-dimensional integral manifold given by the Theorem of Frobenius. In a local coordinate system, the Laplace-Beltrami operator is usually written as follows:\n∆M f = 1√ G n∑ j=1 ∂ ∂yj\n( √ G\nn∑ i=1 gij(y) ∂f ∂yi ) where G is the determinant of the matrix ( gij(y) ) and ( g\nij(y) ) is its inverse. Alternatively, we can expand the expression inside the parentheses, and write L in the form of equation (25):\nL = 1 2 ∆M = 1 2 n∑ i,j=1 gij(y) ∂2 ∂yi∂yj + n∑ i=1 hi(y) ∂ ∂yi ,\nwith hi(y) = 1\n2 √ G n∑ j=1\n∂ (√ Ggij(y) )\n∂yj\nWhen we do the calculations in the simple Gaussian case, with n = 2, we discover that the diffusion coefficients are identical:(\nαij(ρ, θ, φ) ) = ( gij(ρ, θ, φ) ) and the drift coefficients are similar, but not identical:\nh1(ρ, θ, φ) = b y(ρ, θ, φ)\n2 a x(ρ, θ, φ) |∇U |2 ×(a+ b+ c) + 1 |∇U |2  a2 x(ρ, θ, φ)b2 y(ρ, θ, φ) c2 z(ρ, θ, φ)  · ∇U \nh2(ρ, θ, φ) = c z(ρ, θ, φ)\n2 a x(ρ, θ, φ) |∇U |2 ×(a+ b+ c) + 1 |∇U |2  a2 x(ρ, θ, φ)b2 y(ρ, θ, φ) c2 z(ρ, θ, φ)  · ∇U \nIn fact, there is a simple relationship between the coefficients βi(ρ, θ, φ) and hi(ρ, θ, φ):\nβ1(ρ, θ, φ)− h1(ρ, θ, φ) = − b y(ρ, θ, φ) 2x(ρ, θ, φ) |∇U |2 (35)\nβ2(ρ, θ, φ)− h2(ρ, θ, φ) = − c z(ρ, θ, φ) 2x(ρ, θ, φ) |∇U |2\nIs there an explanation for these results? The key is to recognize that the stochastic process defined by equation (6), or (9), or (13), is not Brownian motion. Brownian motion in Rn dissipates, and does not generate an invariant probability measure. Thus the projection of Brownian motion onto a Riemannian manifold, M, would dissipate as well. But the stochastic process defined by equation (6), when projected onto the manifold, M, would not dissipate, in general. This difference must be reflected in the drift coefficients for ∂/∂θ and ∂/∂φ, as shown by equation (35).\nFigure 13 shows the “drift correction vector field” generated by equation (35) on one quadrant of the integral manifold through (10, 0, 0), for the simple Gaussian case. The magnitude of the vector field is coded by color, with red indicating that the length of the vector is near zero. Keep in mind that we are looking at the difference between the two vector fields, ( βi(ρ, θ, φ) ) and (hi(ρ, θ, φ) ), as defined by equation (35). The vector fields themselves are oriented (approximately) in the opposite direction, but they have different magnitudes.\nWe have presented detailed calculations for the simple Gaussian case, so that our results would be easy to visualize. But the same calculations work for the general case, ∇U(x) = (P (x), Q(x), R(x)). The projection operator, ( πij(x(ρ, θ, φ)) ) , and the Jacobian matrix, ( J(ρ, θ, φ) ), can be defined in the same way, and the computational procedure from Section 3, applying Theorem 3 and expanding equation (17), still goes through. The expansion of the Laplace-Beltrami operator for the general dissimilarity metric, gij(ρ, θ, φ) = gij(x(ρ, θ, φ)), also goes through. We end up, again, with diffusion coefficients that are identical:\n( αij(ρ, θ, φ) ) = ( gij(ρ, θ, φ) ) =\n1\nP 2( x(ρ, θ, φ) ) ×\n(( 1 0 0 1 ) − 1 |∇U |2 ( Q( x(ρ, θ, φ) ) R( x(ρ, θ, φ) ) )( Q( x(ρ, θ, φ) ) R( x(ρ, θ, φ) ) ))\nand drift coefficients that differ by a single, but more complex, term:\nβ1(ρ, θ, φ) − h1(ρ, θ, φ) =\n− 1 2P 2 |∇U |2\n( P ( P ∂Q\n∂x − Q ∂P ∂x\n) +R ( R ∂Q\n∂x −Q ∂R ∂x\n))\nβ2(ρ, θ, φ) − h2(ρ, θ, φ) =\n− 1 2P 2 |∇U |2\n( P ( P ∂R\n∂x − R ∂P ∂x\n) +Q ( Q ∂R\n∂x −R ∂Q ∂x\n))\nNote that ∂P/∂x is the only partial derivative in these drift correction equations which is nonzero in the case ∇U(x, y, z) = (−ax,−by,−cz). Thus, for the simple Gaussian case, we can easily verify that the coefficients of the drift correction vector field reduce to the two terms:\n− b y(ρ, θ, φ) 2x(ρ, θ, φ) |∇U |2 and − c z(ρ, θ, φ) 2x(ρ, θ, φ) |∇U |2\nin agreement with equation (35)."
    }, {
      "heading" : "7. Future Work",
      "text" : "The theory of differential similarity combines a stochastic model with a geometric model, and it works because there is a common mathematical object in both models: the gradient, ∇U(x), of a potential function, U(x). In the stochastic model, ∇U(x) is the drift vector, which guarantees the existence of an invariant probability measure. In the geometric model, ∇U(x) guarantees the existence of an orthogonal integral manifold. We have seen, in Section 6, that there is a theoretical connection between these two models, in which ∇U(x) plays a crucial role, and we have seen the practical consequences of this connection in the computational examples in Sections 5.1 and 5.2.\nThe main deficiency in the theory, as presented in this paper, is the restriction of the geometric model to the three-dimensional case. We imposed this restriction to simplify the calculations, and to make it easy to visualize the examples in Mathematica. But the theory is not inherently limited to three dimensions. Theorem 5 in Section 4 was written using the vector cross product and the “curl,” which is a threedimensional concept, but it is actually a special case of a general result in Rn which follows from the dual version of the Theorem of Frobenius, expressed in terms of differential forms. It follows that V∂ and W∂, our basis vectors for the tangent subbundle in R3, can be generalized to Rn. In particular, if ∇U(x) = (P0(x), P1(x), . . . , Pn−1(x) ), we can\nwrite:\n∇U(x) = ( P0(x), P1(x), P2(x), . . . , Pn−2(x), Pn−1(x) ) V1(x) = ( −P1(x), P0(x), 0, . . . , 0, 0 ) V2(x) = ( −P2(x), 0, P0(x), . . . , 0, 0 )\n. . .\nVn−2(x) = ( −Pn−2(x), 0, 0, . . . , P0(x), 0 ) Vn−1(x) = ( −Pn−1(x), 0, 0, . . . , 0, P0(x) )\nIt is straightforward to verify that ∇U(x) is orthogonal to each Vi(x), and that the tangent subbundle spanned by {Vi∂ = Vi(x)/P0(x)} satisfies the Frobenius integrability conditions. Thus the remaining results in Section 4 can be generalized as well. We will discuss these generalizations in a forthcoming paper, which has the working title: “Differential Similarity in Higher-Dimensional Spaces: Theory and Applications.”\nOnce the theory is extended to higher dimensions, it will become apparent that there are various connections to recent work in manifold learning, as described in Section 1. Work in this area tends to follow either a geometric approach or a probabilistic approach, but not both. Examples of the geometric approach include: [TSL00] [RS00] [BN03] [DG03]. Belkin and Niyogi [BN03], for example, work with the eigenvectors of the graph Laplacian and the eigenfunctions of the LaplaceBeltrami operator, and show that the solution to these eigenproblems yields an “optimal” embedding of a low-dimensional manifold into a higher-dimensional space, but their arguments are geometric rather than probabilistic. Examples of the probabilistic approach include: [HR02] [TB99] [CSP+10]. Tipping and Bishop [TB99] work with a mixture of low-dimensional Gaussians embedded in a higher-dimensional space, each with its own mean and covariance matrix, and they use the EM algorithm to estimate the parameters of this model. Chen, et al., [CSP+10] adopt a similar model, along with the assumption that the Gaussian mixture covers a low-dimensional manifold, and they estimate both the number of components in the mixture and the dimensionality of the subspaces, using Bayesian techniques. But neither paper makes use of the geometric structure of the embedded manifold.\nOne exception to this dichotomy between geometric and probabilistic approaches is a paper by Lee and Wasserman [LW10], which has some interesting connections to the present work. The paper starts out by defining a Markov chain on Rn with a transition kernel Ω (x, ·) which gives preference to nearby points, y, that have a high probability density, p(y). This kernel is then used to define the one-step diffusion\noperator, A , and its m-step version, A ,m. The authors then construct a continuous time operator: At = lim →0A , t/ . The analogous mathematical object in our theory would be the operator Qt in equation (7). Lee and Wasserman are primarily interested in the eigenfunctions of A ,m and At, which have applications to various spectral clustering problems, following the work of Belkin and Niyogi [BN03] and others. They also use Ω (x, ·) to define a diffusion distance, D2 ,m(x, z), and its continuous time version, D2t (x, z), but there does not seem to be a straightforward relationship between this distance and our dissimilarity metric, gij(x). The paper concludes with several examples that demonstrate the utility of these concepts.\nThe other important contribution of Lee and Wasserman [LW10] is their analysis of the statistical estimators for the population quantities, At and D 2 t . This is essential future work for our theory as well, since we need to apply our model to real data in order to fully validate it. One of the most important applications of manifold learning is to the field known as deep learning [BLPL06] [HOT06] [RPCL06]. In deep learning, a neural network with many layers (hence, “deep”) is trained in two stages: the first stage uses an unsupervised learning algorithm to construct a set of “features” bottom up; the second stage applies a supervised learning algorithm to the feature hierarchy, top down. In fact, the paper by Rifai, et al. [RDV+11], cited and quoted in Section 1, is an example of just such an approach. In their experiment, the authors extract a tangent plane at each training point using an unsupervised technique (CAE), and then train their network in a supervised manner using a (20 year old) technique that was designed to be sensitive to tangent directions (MTC). They write:\nTo the best of our knowledge this is the first time that the implicit relationship between an unsupervised learned mapping and the tangent space of a manifold is rendered explicit and successfully exploited for the training of a classifier.\nTo combine manifold learning and deep learning in this way, it is necessary to make a decision about the network interface: What kind of representation of the data should we use as the output of one layer and the input of the next? There are reasons to think that prototype coding (see Section 3) with our Riemannian dissimilarity metric, gij(x), will turn out to be a good choice, but this is obviously an important question on our agenda for future work."
    } ],
    "references" : [ {
      "title" : "Introduction to Differentiable Manifolds",
      "author" : [ "Louis Auslander", "Robert E. MacKenzie" ],
      "venue" : "Dover Publications,",
      "citeRegEx" : "AM77",
      "shortCiteRegEx" : null,
      "year" : 1977
    }, {
      "title" : "Pure and applied mathematics",
      "author" : [ "Richard L. Bishop", "Richard J. Crittenden. Geometry of Manifolds" ],
      "venue" : "Academic Press,",
      "citeRegEx" : "BC64",
      "shortCiteRegEx" : null,
      "year" : 1964
    }, {
      "title" : "Tensor Analysis on Manifolds",
      "author" : [ "Richard L. Bishop", "Samuel I. Goldberg" ],
      "venue" : "Macmillan,",
      "citeRegEx" : "BG68",
      "shortCiteRegEx" : null,
      "year" : 1968
    }, {
      "title" : "Greedy layerwise training of deep networks",
      "author" : [ "Y. Bengio", "P. Lamblin", "D. Popovici", "H. Larochelle" ],
      "venue" : "Advances in Neural Information Processing Systems, volume 19, pages 153–160",
      "citeRegEx" : "BLPL06",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Laplacian eigenmaps for dimensionality reduction and data representation",
      "author" : [ "M. Belkin", "P. Niyogi" ],
      "venue" : "Neural Computation, 15(6):1373–1396",
      "citeRegEx" : "BN03",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Dover Books on Mathematics Series",
      "author" : [ "Henri Cartan. Differential Forms" ],
      "venue" : "Dover Publications,",
      "citeRegEx" : "Car71",
      "shortCiteRegEx" : null,
      "year" : 1971
    }, {
      "title" : "Compressive sensing on manifolds using a nonparametric mixture of factor analyzers: Algorithm and performance bounds",
      "author" : [ "M. Chen", "J. Silva", "J.W. Paisley", "C. Wang", "D.B. Dunson", "L. Carin" ],
      "venue" : "IEEE Transactions on Signal Processing, 58(12):6140–6155",
      "citeRegEx" : "CSP10",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Hessian eigenmaps: Locally linear embedding techniques for high-dimensional data",
      "author" : [ "D. Donoho", "C. Grimes" ],
      "venue" : "Proceedings of National Academy of Sciences, 100:5591–5596",
      "citeRegEx" : "DG03",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "John Willey & Sons",
      "author" : [ "Richard O. Duda", "Peter E. Hart. Pattern Classification", "Scene Analysis" ],
      "venue" : "New York,",
      "citeRegEx" : "DH73",
      "shortCiteRegEx" : null,
      "year" : 1973
    }, {
      "title" : "chapter 10: Unsupervised Learning and Clustering",
      "author" : [ "Richard O. Duda", "Peter E. Hart", "David G. Stork. Pattern Classification" ],
      "venue" : "Wiley & Sons, Inc., New York, 2nd edition,",
      "citeRegEx" : "DHS01",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Stochastic Calculus in Manifolds",
      "author" : [ "Michel Emery", "Paul A. Meyer" ],
      "venue" : "World Publishing Company,",
      "citeRegEx" : "EM89",
      "shortCiteRegEx" : null,
      "year" : 1989
    }, {
      "title" : "Space-time approach to non-relativistic quantum mechanics",
      "author" : [ "R.P. Feynman" ],
      "venue" : "Rev. Mod. Phys., 20:367–387",
      "citeRegEx" : "Fey48",
      "shortCiteRegEx" : null,
      "year" : 1948
    }, {
      "title" : "Hypoelliptic second order differential equations",
      "author" : [ "L. Hörmander" ],
      "venue" : "Acta Mathematica, 119:147–171",
      "citeRegEx" : "Hör67",
      "shortCiteRegEx" : null,
      "year" : 1967
    }, {
      "title" : "A fast learning algorithm for deep belief nets",
      "author" : [ "G.E. Hinton", "S. Osindero", "Y.-W. Teh" ],
      "venue" : "Neural Computation, 18(7):1527–1554",
      "citeRegEx" : "HOT06",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Stochastic neighbor embedding",
      "author" : [ "G.E. Hinton", "S.T. Roweis" ],
      "venue" : "Advances in Neural Information Processing Systems, volume 15, pages 833– 840",
      "citeRegEx" : "HR02",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Contemporary Mathematics",
      "author" : [ "Elton P. Hsu. Stochastic Analysis on Manifolds" ],
      "venue" : "American Mathematical Society,",
      "citeRegEx" : "Hsu02",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Stochastic differentials",
      "author" : [ "K. Itô" ],
      "venue" : "Applied Mathematics & Optimization, 1(4):374–381",
      "citeRegEx" : "Itô75",
      "shortCiteRegEx" : null,
      "year" : 1975
    }, {
      "title" : "On distributions of certain Wiener functionals",
      "author" : [ "M. Kac" ],
      "venue" : "Trans. Amer. Math. Soc., 65:1–13",
      "citeRegEx" : "Kac49",
      "shortCiteRegEx" : null,
      "year" : 1949
    }, {
      "title" : "Pure and Applied Mathematics",
      "author" : [ "David Lovelock", "Hanno Rund. Tensors", "Differential Forms", "Variational Principles" ],
      "venue" : "Wiley,",
      "citeRegEx" : "LR75",
      "shortCiteRegEx" : null,
      "year" : 1975
    }, {
      "title" : "Spectral connectivity analysis",
      "author" : [ "A.B. Lee", "L. Wasserman" ],
      "venue" : "Journal of the American Statistical Association, 105(491):1241–1255",
      "citeRegEx" : "LW10",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Path integral methods for parabolic partial differential equations with examples from computational finance",
      "author" : [ "A. Lyasoff" ],
      "venue" : "Mathematica Journal, 9(2):399–422",
      "citeRegEx" : "Lya04",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Stochastic Differential Equations: An Introduction With Applications",
      "author" : [ "Bernt K. Øksendal" ],
      "venue" : "Springer, sixth edition,",
      "citeRegEx" : "Øks03",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "The manifold tangent classifier",
      "author" : [ "S. Rifai", "Y. Dauphin", "P. Vincent", "Y. Bengio", "X. Muller" ],
      "venue" : "Advances in Neural Information Processing Systems, volume 24, pages 2294–2302",
      "citeRegEx" : "RDV11",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Efficient learning of sparse representations with an energy-based model",
      "author" : [ "M. Ranzato", "C. Poultney", "S. Chopra", "Y. LeCun" ],
      "venue" : "Advances in Neural Information Processing Systems, volume 19, pages 1137–1144",
      "citeRegEx" : "RPCL06",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Nonlinear dimensionality reduction by locally linear embedding",
      "author" : [ "S.T. Roweis", "L.K. Saul" ],
      "venue" : "Science, 290(5500):2323–2326",
      "citeRegEx" : "RS00",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "volume 1",
      "author" : [ "Michael Spivak. A Comprehensive Introduction to Differential Geometry" ],
      "venue" : "Publish or Perish, third edition,",
      "citeRegEx" : "Spi99",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "Diffusions as integral curves",
      "author" : [ "D.W. Stroock", "S. Taniguchi" ],
      "venue" : "or Stratonovich without Itô. In The Dynkin Festschrift. Markov processes and their applications. In celebration of Eugene B. Dynkin’s 70th birthday, pages 333–369. Boston, MA: Birkhäuser",
      "citeRegEx" : "ST94",
      "shortCiteRegEx" : null,
      "year" : 1994
    }, {
      "title" : "Diffusions as integral curves on manifolds and Lie groups",
      "author" : [ "D.W. Stroock", "S. Taniguchi" ],
      "venue" : "Probability theory and mathematical statistics. Lectures presented at the semester held in St. Petersburg, Russia, March 2–April 23, 1993, pages 219–226. Amsterdam: Gordon and Breach Publishers",
      "citeRegEx" : "ST96",
      "shortCiteRegEx" : null,
      "year" : 1996
    }, {
      "title" : "A new representation for stochastic integrals and equations",
      "author" : [ "R.L. Stratonovich" ],
      "venue" : "SIAM Journal on Control, 4(2):362–371",
      "citeRegEx" : "Str66",
      "shortCiteRegEx" : null,
      "year" : 1966
    }, {
      "title" : "On the growth of stochastic integrals",
      "author" : [ "D.W. Stroock" ],
      "venue" : "Z. Wahrscheinlichkeitstheor. Verw. Geb., 18:340–344",
      "citeRegEx" : "Str71",
      "shortCiteRegEx" : null,
      "year" : 1971
    }, {
      "title" : "Probability Theory: An Analytic View",
      "author" : [ "Daniel W. Stroock" ],
      "venue" : "Cambridge University Press,",
      "citeRegEx" : "Str93",
      "shortCiteRegEx" : null,
      "year" : 1993
    }, {
      "title" : "Gaussian measures in traditional and not so traditional settings",
      "author" : [ "D.W. Stroock" ],
      "venue" : "Bulletin (New Series) of the American Mathematical Society, 33(2):135–155",
      "citeRegEx" : "Str96",
      "shortCiteRegEx" : null,
      "year" : 1996
    }, {
      "title" : "Mathematical Surveys and Monographs",
      "author" : [ "Daniel W. Stroock. An Introduction to the Analysis of Paths on a Riemannian Manifold" ],
      "venue" : "American Mathematical Society,",
      "citeRegEx" : "Str00",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "Itô’s Perspective",
      "author" : [ "K Daniel W. Stroock. Markov Processes from" ],
      "venue" : "Annals of Mathematics Studies. Princeton University Press,",
      "citeRegEx" : "Str03",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Probability Theory: An Analytic View",
      "author" : [ "Daniel W. Stroock" ],
      "venue" : "Cambridge University Press, second edition,",
      "citeRegEx" : "Str11",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Mixtures of probabilistic principal component analyzers",
      "author" : [ "M.E. Tipping", "C.M. Bishop" ],
      "venue" : "Neural Computation, 11(2):443–482",
      "citeRegEx" : "TB99",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "A Global Geometric Framework for Nonlinear Dimensionality Reduction",
      "author" : [ "J.B. Tenenbaum", "V. Silva", "J.C. Langford" ],
      "venue" : "Science, 290(5500):2319–2323",
      "citeRegEx" : "TSL00",
      "shortCiteRegEx" : null,
      "year" : 2000
    } ],
    "referenceMentions" : [ {
      "referenceID" : 8,
      "context" : "Clustering algorithms have been studied for several decades [DH73], and they remain one of the main ingredients in unsupervised learning [DHS01].",
      "startOffset" : 60,
      "endOffset" : 66
    }, {
      "referenceID" : 9,
      "context" : "Clustering algorithms have been studied for several decades [DH73], and they remain one of the main ingredients in unsupervised learning [DHS01].",
      "startOffset" : 137,
      "endOffset" : 144
    }, {
      "referenceID" : 36,
      "context" : "Recently, a variant of the traditional clustering algorithms has attracted some attention, under the rubric of manifold learning : [TSL00] [RS00] [BN03].",
      "startOffset" : 131,
      "endOffset" : 138
    }, {
      "referenceID" : 24,
      "context" : "Recently, a variant of the traditional clustering algorithms has attracted some attention, under the rubric of manifold learning : [TSL00] [RS00] [BN03].",
      "startOffset" : 139,
      "endOffset" : 145
    }, {
      "referenceID" : 4,
      "context" : "Recently, a variant of the traditional clustering algorithms has attracted some attention, under the rubric of manifold learning : [TSL00] [RS00] [BN03].",
      "startOffset" : 146,
      "endOffset" : 152
    }, {
      "referenceID" : 22,
      "context" : "[RDV11], outline three hypotheses that motivate much of this work:",
      "startOffset" : 0,
      "endOffset" : 7
    }, {
      "referenceID" : 11,
      "context" : "Let’s start with a model that will be familiar to most physicists: the Feynman-Kac formula [Fey48] [Kac49].",
      "startOffset" : 91,
      "endOffset" : 98
    }, {
      "referenceID" : 17,
      "context" : "Let’s start with a model that will be familiar to most physicists: the Feynman-Kac formula [Fey48] [Kac49].",
      "startOffset" : 99,
      "endOffset" : 106
    }, {
      "referenceID" : 30,
      "context" : "See [Str93], Section 4.",
      "startOffset" : 4,
      "endOffset" : 11
    }, {
      "referenceID" : 30,
      "context" : "See [Str93], Section 4.",
      "startOffset" : 4,
      "endOffset" : 11
    }, {
      "referenceID" : 34,
      "context" : "2, or [Str11], Section 8.",
      "startOffset" : 6,
      "endOffset" : 13
    }, {
      "referenceID" : 31,
      "context" : "haunts every attempt to deal with Brownian paths,” [Str96], p.",
      "startOffset" : 51,
      "endOffset" : 58
    }, {
      "referenceID" : 10,
      "context" : "See [EM89] or [Hsu02].",
      "startOffset" : 4,
      "endOffset" : 10
    }, {
      "referenceID" : 15,
      "context" : "See [EM89] or [Hsu02].",
      "startOffset" : 14,
      "endOffset" : 21
    }, {
      "referenceID" : 30,
      "context" : "36 in [Str93] or Theorem 10.",
      "startOffset" : 6,
      "endOffset" : 13
    }, {
      "referenceID" : 34,
      "context" : "33 in [Str11].",
      "startOffset" : 6,
      "endOffset" : 13
    }, {
      "referenceID" : 30,
      "context" : "Sources: These results appear in [Str93], Section 4.",
      "startOffset" : 33,
      "endOffset" : 40
    }, {
      "referenceID" : 34,
      "context" : "See [Str11], Section 10.",
      "startOffset" : 4,
      "endOffset" : 11
    }, {
      "referenceID" : 21,
      "context" : "16 of his text [Øks03].",
      "startOffset" : 15,
      "endOffset" : 22
    }, {
      "referenceID" : 21,
      "context" : "3 in [Øks03].",
      "startOffset" : 5,
      "endOffset" : 12
    }, {
      "referenceID" : 21,
      "context" : "See [Øks03], Chapter 4.",
      "startOffset" : 4,
      "endOffset" : 11
    }, {
      "referenceID" : 28,
      "context" : "See [Str66] or [Itô75].",
      "startOffset" : 4,
      "endOffset" : 11
    }, {
      "referenceID" : 16,
      "context" : "See [Str66] or [Itô75].",
      "startOffset" : 15,
      "endOffset" : 22
    }, {
      "referenceID" : 12,
      "context" : "See [Hör67].",
      "startOffset" : 4,
      "endOffset" : 11
    }, {
      "referenceID" : 32,
      "context" : "For these reasons, Stroock relies on the Hörmander formalism extensively in his book on the analysis of Brownian paths on Riemannian manifolds [Str00].",
      "startOffset" : 143,
      "endOffset" : 150
    }, {
      "referenceID" : 21,
      "context" : "Sources: For the basic results on stochastic differential equations, using Itô’s formalism, the reader should consult [Øks03], but Øksendal’s text provides only a cursory treatment of Stratonovich’s formalism.",
      "startOffset" : 118,
      "endOffset" : 125
    }, {
      "referenceID" : 28,
      "context" : "The original paper by Stratonovich [Str66] is still very readable, but his theory was only given a solid mathematical foundation some years later by Itô [Itô75].",
      "startOffset" : 35,
      "endOffset" : 42
    }, {
      "referenceID" : 16,
      "context" : "The original paper by Stratonovich [Str66] is still very readable, but his theory was only given a solid mathematical foundation some years later by Itô [Itô75].",
      "startOffset" : 153,
      "endOffset" : 160
    }, {
      "referenceID" : 33,
      "context" : "Chapter 8 of [Str03] is an excellent contemporary account of Stratonovich’s theory, set in a broader context.",
      "startOffset" : 13,
      "endOffset" : 20
    }, {
      "referenceID" : 31,
      "context" : "There remains the problem that “haunts every attempt to deal with Brownian paths,” [Str96], p.",
      "startOffset" : 83,
      "endOffset" : 90
    }, {
      "referenceID" : 30,
      "context" : "23 in [Str93].",
      "startOffset" : 6,
      "endOffset" : 13
    }, {
      "referenceID" : 30,
      "context" : "32 in [Str93].",
      "startOffset" : 6,
      "endOffset" : 13
    }, {
      "referenceID" : 30,
      "context" : "10 in [Str93].",
      "startOffset" : 6,
      "endOffset" : 13
    }, {
      "referenceID" : 26,
      "context" : "The preceding analysis is not confined to Euclidean R, since a similar construction works when L is given in Hörmander form by (18), see [ST94], and this means that all results can be replicated in an arbitrary Riemannian manifold, see [ST96].",
      "startOffset" : 137,
      "endOffset" : 143
    }, {
      "referenceID" : 27,
      "context" : "The preceding analysis is not confined to Euclidean R, since a similar construction works when L is given in Hörmander form by (18), see [ST94], and this means that all results can be replicated in an arbitrary Riemannian manifold, see [ST96].",
      "startOffset" : 236,
      "endOffset" : 242
    }, {
      "referenceID" : 32,
      "context" : "The theory is explicated further in [Str00], where it serves as the foundation for Stroock’s construction and analysis of Brownian motion on a Riemannian manifold.",
      "startOffset" : 36,
      "endOffset" : 43
    }, {
      "referenceID" : 32,
      "context" : "1 of [Str00] includes a generalization of Lemma 4 above, and Theorem 2.",
      "startOffset" : 5,
      "endOffset" : 12
    }, {
      "referenceID" : 32,
      "context" : "40 of [Str00] is a generalization of Theorem 4.",
      "startOffset" : 6,
      "endOffset" : 13
    }, {
      "referenceID" : 25,
      "context" : ", [Spi99], Chapter 6; [BG68], Chapter 3; [AM77], Chapter 8.",
      "startOffset" : 2,
      "endOffset" : 9
    }, {
      "referenceID" : 2,
      "context" : ", [Spi99], Chapter 6; [BG68], Chapter 3; [AM77], Chapter 8.",
      "startOffset" : 22,
      "endOffset" : 28
    }, {
      "referenceID" : 0,
      "context" : ", [Spi99], Chapter 6; [BG68], Chapter 3; [AM77], Chapter 8.",
      "startOffset" : 41,
      "endOffset" : 47
    }, {
      "referenceID" : 1,
      "context" : "See [BC64], Problem 29, p.",
      "startOffset" : 4,
      "endOffset" : 10
    }, {
      "referenceID" : 5,
      "context" : "23; [Car71], pp.",
      "startOffset" : 4,
      "endOffset" : 11
    }, {
      "referenceID" : 18,
      "context" : "97–98; [LR75], pp.",
      "startOffset" : 7,
      "endOffset" : 13
    }, {
      "referenceID" : 2,
      "context" : ", [BG68], Theorem 3.",
      "startOffset" : 2,
      "endOffset" : 8
    }, {
      "referenceID" : 25,
      "context" : "1, and [Spi99], Lemma 5.",
      "startOffset" : 7,
      "endOffset" : 14
    }, {
      "referenceID" : 2,
      "context" : ", [BG68], Theorem 3.",
      "startOffset" : 2,
      "endOffset" : 8
    }, {
      "referenceID" : 1,
      "context" : "1, or [BC64], Theorem 1.",
      "startOffset" : 6,
      "endOffset" : 12
    }, {
      "referenceID" : 20,
      "context" : "See, for example, [Lya04].",
      "startOffset" : 18,
      "endOffset" : 25
    }, {
      "referenceID" : 29,
      "context" : "The earliest example is in [Str71] and [Itô75].",
      "startOffset" : 27,
      "endOffset" : 34
    }, {
      "referenceID" : 16,
      "context" : "The earliest example is in [Str71] and [Itô75].",
      "startOffset" : 39,
      "endOffset" : 46
    }, {
      "referenceID" : 32,
      "context" : "1 of [Str00] or Theorem 3.",
      "startOffset" : 5,
      "endOffset" : 12
    }, {
      "referenceID" : 15,
      "context" : "4 in [Hsu02].",
      "startOffset" : 5,
      "endOffset" : 12
    }, {
      "referenceID" : 32,
      "context" : "37 in [Str00].",
      "startOffset" : 6,
      "endOffset" : 13
    }, {
      "referenceID" : 29,
      "context" : "To simplify the calculations, we will initially focus our attention on the simple Gaussian case, in which ∇U(x, y, z) = (−ax,−by,−cz), and we will start with a construction borrowed from [Str71] and [Itô75], but",
      "startOffset" : 187,
      "endOffset" : 194
    }, {
      "referenceID" : 16,
      "context" : "To simplify the calculations, we will initially focus our attention on the simple Gaussian case, in which ∇U(x, y, z) = (−ax,−by,−cz), and we will start with a construction borrowed from [Str71] and [Itô75], but",
      "startOffset" : 199,
      "endOffset" : 206
    }, {
      "referenceID" : 36,
      "context" : "Examples of the geometric approach include: [TSL00] [RS00] [BN03] [DG03].",
      "startOffset" : 44,
      "endOffset" : 51
    }, {
      "referenceID" : 24,
      "context" : "Examples of the geometric approach include: [TSL00] [RS00] [BN03] [DG03].",
      "startOffset" : 52,
      "endOffset" : 58
    }, {
      "referenceID" : 4,
      "context" : "Examples of the geometric approach include: [TSL00] [RS00] [BN03] [DG03].",
      "startOffset" : 59,
      "endOffset" : 65
    }, {
      "referenceID" : 7,
      "context" : "Examples of the geometric approach include: [TSL00] [RS00] [BN03] [DG03].",
      "startOffset" : 66,
      "endOffset" : 72
    }, {
      "referenceID" : 4,
      "context" : "Belkin and Niyogi [BN03], for example, work with the eigenvectors of the graph Laplacian and the eigenfunctions of the LaplaceBeltrami operator, and show that the solution to these eigenproblems yields an “optimal” embedding of a low-dimensional manifold into a higher-dimensional space, but their arguments are geometric rather than probabilistic.",
      "startOffset" : 18,
      "endOffset" : 24
    }, {
      "referenceID" : 14,
      "context" : "Examples of the probabilistic approach include: [HR02] [TB99] [CSP10].",
      "startOffset" : 48,
      "endOffset" : 54
    }, {
      "referenceID" : 35,
      "context" : "Examples of the probabilistic approach include: [HR02] [TB99] [CSP10].",
      "startOffset" : 55,
      "endOffset" : 61
    }, {
      "referenceID" : 6,
      "context" : "Examples of the probabilistic approach include: [HR02] [TB99] [CSP10].",
      "startOffset" : 62,
      "endOffset" : 69
    }, {
      "referenceID" : 35,
      "context" : "Tipping and Bishop [TB99] work with a mixture of low-dimensional Gaussians embedded in a higher-dimensional space, each with its own mean and covariance matrix, and they use the EM algorithm to estimate the parameters of this model.",
      "startOffset" : 19,
      "endOffset" : 25
    }, {
      "referenceID" : 6,
      "context" : ", [CSP10] adopt a similar model, along with the assumption that the Gaussian mixture covers a low-dimensional manifold, and they estimate both the number of components in the mixture and the dimensionality of the subspaces, using Bayesian techniques.",
      "startOffset" : 2,
      "endOffset" : 9
    }, {
      "referenceID" : 19,
      "context" : "One exception to this dichotomy between geometric and probabilistic approaches is a paper by Lee and Wasserman [LW10], which has some interesting connections to the present work.",
      "startOffset" : 111,
      "endOffset" : 117
    }, {
      "referenceID" : 4,
      "context" : "Lee and Wasserman are primarily interested in the eigenfunctions of A ,m and At, which have applications to various spectral clustering problems, following the work of Belkin and Niyogi [BN03] and others.",
      "startOffset" : 186,
      "endOffset" : 192
    }, {
      "referenceID" : 19,
      "context" : "The other important contribution of Lee and Wasserman [LW10] is their analysis of the statistical estimators for the population quantities, At and D 2 t .",
      "startOffset" : 54,
      "endOffset" : 60
    }, {
      "referenceID" : 3,
      "context" : "One of the most important applications of manifold learning is to the field known as deep learning [BLPL06] [HOT06] [RPCL06].",
      "startOffset" : 99,
      "endOffset" : 107
    }, {
      "referenceID" : 13,
      "context" : "One of the most important applications of manifold learning is to the field known as deep learning [BLPL06] [HOT06] [RPCL06].",
      "startOffset" : 108,
      "endOffset" : 115
    }, {
      "referenceID" : 23,
      "context" : "One of the most important applications of manifold learning is to the field known as deep learning [BLPL06] [HOT06] [RPCL06].",
      "startOffset" : 116,
      "endOffset" : 124
    }, {
      "referenceID" : 22,
      "context" : "[RDV11], cited and quoted in Section 1, is an example of just such an approach.",
      "startOffset" : 0,
      "endOffset" : 7
    } ],
    "year" : 2014,
    "abstractText" : "This paper develops a theory of clustering and coding which combines a geometric model with a probabilistic model in a principled way. The geometric model is a Riemannian manifold with a Riemannian metric, gij(x), which we interpret as a measure of dissimilarity. The probabilistic model consists of a stochastic process with an invariant probability measure which matches the density of the sample input data. The link between the two models is a potential function, U(x), and its gradient, ∇U(x). We use the gradient to define the dissimilarity metric, which guarantees that our measure of dissimilarity will depend on the probability measure. Finally, we use the dissimilarity metric to define a coordinate system on the embedded Riemannian manifold, which gives us a low-dimensional encoding of our original data.",
    "creator" : "LaTeX with hyperref package"
  }
}