{
  "name" : "1411.1076.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "A statistical model for tensor PCA",
    "authors" : [ "Andrea Montanari" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "tensor unfolding, power iteration and message passing ideas from graphical models. We show that, unless the signal-to-noise ratio diverges in the system dimensions, none of these approaches succeeds. This is possibly related to a fundamental limitation of computationally tractable estimators for this problem.\nWe discuss various initializations for tensor power iteration, and show that a tractable initialization based on the spectrum of the matricized tensor outperforms significantly baseline methods, statistically and computationally. Finally, we consider the case in which additional side information is available about the unknown signal. We characterize the amount of side information that allows the iterative algorithms to converge to a good estimate."
    }, {
      "heading" : "1 Introduction",
      "text" : "Given a data matrix X, Principal Component Analysis (PCA) can be regarded as a ‘denoising’ technique that replaces X by its closest rank-one approximation. This optimization problem can be solved efficiently, and its statistical properties are well-understood. The generalization of PCA to tensors is motivated by problems in which it is important to exploit higher order moments, or data elements are naturally given more than two indices. Examples include topic modeling [AGH+12], video processing, collaborative filtering in presence of temporal/context information, community detection [AGHK13], spectral hypergraph theory and hyper-graph matching [DBKP09]. Further, finding a rank-one approximation to a tensor is a bottleneck for tensor-valued optimization algorithms using conditional gradient type of schemes. While tensor factorization is NP-hard [HL13], this does not necessarily imply intractability for natural statistical models. Over the last ten years, it was repeatedly observed that either convex optimization or greedy methods yield optimal solutions to statistical problems that are intractable from a worst case perspective (wellknown examples include sparse regression [DE03, Tro04, CT07] and low-rank matrix completion [CR09, KMO10]).\n∗Department of Electrical Engineering and Department of Statistics, Stanford University †Department of Electrical Engineering, Stanford University\nar X\niv :1\n41 1.\n10 76\nv1 [\ncs .L\nG ]\n4 N\nov 2\n01 4\nIn order to investigate the fundamental tradeoffs between computational resources and statistical power in tensor PCA, we consider the simplest possible model where this arises, whereby an unknown unit vector v0 is to be inferred from noisy multilinear measurements. Namely, for each unordered k-uple {i1, i2, . . . , ik} ⊆ [n], we measure\nXi1,i2,...,ik = β(v0)i1(v0)i2 · · · (v0)ik + Zi1,i2,...,ik , (1)\nwith Z Gaussian noise (see below for a precise definition) and wish to reconstruct v0. In tensor notation, the observation model reads (see the end of this section for notations)\nX = β v0 ⊗k + Z . Spiked Tensor Model\nThis is analogous to the so called ‘spiked covariance model’ used to study matrix PCA in high dimensions [JL09].\nIt is immediate to see that maximum-likelihood estimator vML is given by a solution of the following problem\nmaximize 〈X,v⊗k〉, Tensor PCA subject to ‖v‖2 = 1 .\nSolving it exactly is –in general– NP hard [HL13]. We next summarize our results. Note that, given a completely observed rank-one symmetric tensor v0 ⊗k (i.e. for β =∞), it is easy to recover the vector v0 ∈ Rn. It is therefore natural to ask the question for which signal-to-noise ratios one can one still reliably estimate v0? The answer appears to depend dramatically on the computational resources1.\nIdeal estimation. Assuming unbounded computational resources, we can solve the Tensor PCA optimization problem and hence implement the maximum likelihood estimator v̂ML. We use recent results in probability theory to show that this approach is successful for β ≥ µk (here µk is a constant given explicitly below, with µk = √ k log k(1 + ok(1))). In particular, above\nthis threshold2 we have, with high probability,\n‖v̂ML − v0‖22 ≤ 2.01µk β . (2)\nWe use an information-theoretic argument to show that no approach can do significantly better, namely no procedure can estimate v0 accurately for β ≤ c √ k (for c a universal constant).\nTractable estimators: Unfolding. We consider two approaches to estimate v0 that can be implemented in polynomial time. The first approach is based on tensor unfolding: starting from the tensor X ∈ ⊗k Rn, we produce a matrix Mat(X) of dimensions nq × nk−q. We then perform matrix PCA on Mat(X). We show that this method is successful for β & n(dk/2e−1)/2\n(provided we choose q = dk/2e). 1Here we write F (n) . G(n) if there exists a constant c independent of n (but possibly dependent on k), such that F (n) ≤ cG(n) 2Note that, for k even, v0 can only be recovered modulo sign. For the sake of simplicity, we assume here that this ambiguity is correctly resolved.\nA heuristics argument suggests that the necessary and sufficient condition for tensor unfolding to succeed is indeed β & n(k−2)/4 (which is below the rigorous bound by a factor n1/4 for k odd). We can indeed confirm this conjecture for k even and under an asymmetric noise model. Numerical simulations confirm the conjecture for k = 3.\nTractable estimators: Power iteration. We then consider a simple tensor power iteration method, that proceeds by repeatedly applying the tensor to a vector. We prove that, initializing this iteration uniformly at random, it converges very rapidly to an accurate estimate provided β & n(k−1)/2. A heuristic argument suggests that the correct necessary and sufficient threshold is given by β & n(k−2)/2. In other words, power iteration is substantially less powerful than unfolding.\nTractable estimators: Warm-start power iteration. Motivated by the last observation, we consider a ‘warm-start’ power iteration algorithm, in which we initialize power iteration with the output of tensor unfolding. This approach appears to have the same threshold signal-tonoise ratio as simple unfolding, but significantly better accuracy above that threshold.\nWe also study a number of variations on this, with improved unfolding methods.\nTractable estimators: Approximate Message Passing. Finally we consider an approximate message passing (AMP) algorithm [DMM09, BM11]. Such algorithms proved effective in compressed sensing and several other estimation problems. We show that the behavior of AMP is qualitatively similar to the one of naive power iteration. In particular, AMP fails for any β bounded as n→∞.\nSide information. Given the above computational complexity barrier, it is natural to study weaker version of the original problem. Here we assume that extra information about v0 is available. This can be provided by additional measurements or by approximately solving a related problem, for instance a matrix PCA problem as in [AGH+12]. We model this additional information as y = γv0+g (with g an independent Gaussian noise vector), and incorporate it in the initial condition of AMP algorithm. We characterize exactly the threshold value γ∗ = γ∗(β) above which AMP converges to an accurate estimator.\nThe thresholds for various classes of algorithms are summarized below.\nMethod Required β (rigorous) Required β (heuristic)\nTensor Unfolding O(n(dk/2e−1)/2) n(k−2)/4\nTensor Power Iteration (with random init.) O(n(k−1)/2) n(k−2)/2\nMaximum Likelihood 1 – Information-theory lower bound 1 –\nWe will conclude the paper with some insights that we believe provide useful guidance for tensor factorization heuristics. We illustrate these insights through simulations.\nThroughout the paper, proofs will be deferred to the Appendices."
    }, {
      "heading" : "1.1 Notations",
      "text" : "We will use lower-case boldface for vectors (e.g. u, v, and so on) and upper-case boldface for matrices and tensors (e.g. X,Z, and so on). The ordinary scalar product and `p norm over vectors\nare denoted by 〈u,v〉 = ∑n\ni=1 uivi, and ‖v‖p. We write Sn−1 for the unit sphere in n dimensions\nSn−1 ≡ { x ∈ Rn : ‖x‖2 = 1 } . (3)\nGiven X ∈ ⊗k Rn a real k-th order tensor, we let {Xi1,...,ik}i1,...,ik denote its coordinates and\ndefine a map X : Rn → Rn, by letting, for v ∈ Rn,\nX{v}i = ∑\nj1,··· ,jk−1∈[n]\nXi,j1,··· ,jk−1 vj1 · · ·vjk−1 . (4)\nThe outer product of two tensors is X⊗Y, and, for v ∈ Rn, we define v⊗k = v⊗ · · · ⊗ v ∈ ⊗k Rn\nas the k-th outer power of v. We define the inner product of two tensors X,Y ∈ ⊗k Rn as\n〈X,Y〉 = ∑\ni1,··· ,ik∈[n]\nXi1,··· ,ikYi1,··· ,ik . (5)\nWe define the Frobenius (Euclidean) norm of a tensor X by ‖X‖F = √ 〈X,X〉, and its operator norm by\n‖X‖op ≡ max{〈X,u1 ⊗ · · · ⊗ uk〉 : ∀i ∈ [k] , ‖ui‖2 ≤ 1}. (6)\nIt is easy to check that this is indeed a norm. For the special case k = 2, it reduces to the ordinary `2 matrix operator norm (equivalently, to the largest singular value of X).\nFor a permutation π ∈ Sk, we will denote by Xπ the tensor with permuted indices Xπi1,··· ,ik = Xπ(i1),··· ,π(ik). We call the tensor X symmetric if, for any permutation π ∈ Sk, X\nπ = X. It is proved [Wat90] that, for symmetric tensors, the value of problem Tensor PCA coincides with ‖X‖op up to a sign. More precisely, for symmetric tensors we have the equivalent representation\n‖X‖op ≡ max{|〈X,u⊗k〉| : ‖u‖2 ≤ 1}. (7)\nWe denote by G ∈ ⊗k Rn a tensor with independent and identically distributed entries Gi1,··· ,ik ∼ N(0, 1) (note that this tensor is not symmetric). We define the symmetric standard normal noise tensor Z ∈\n⊗k Rn by Z = 1\nk!\n√ k\nn ∑ π∈Sk Gπ . (8)\nNote that the subset of entries with unequal indices form an i.i.d. collection {Zi1,i2,...,ik}i1<···<ik ∼ N(0, 1/(n(k − 1)!)). The normalization adopted here is convenient because it yields, for any fixed vector v ∈ Rn,\nX{v} = β〈v0,v〉k−1 v0 + 1√ n ‖v‖k−12 g + o(1) . (9)\nwhere g ∼ N(0, In), and o(1) is a vector with ‖o(1)‖2 → 0 in probability as n → ∞. We further have,\nE { 〈Z,v⊗k〉2 } = k n E { 〈G,v⊗k〉2 } = k n ‖v‖2k2 , (10)\nand\n〈X{v},v〉 = β〈v0,v〉k + √ k\nn g , (11)\nwith g ∼ N(0, 1). Finally notice that, for k even, in Spiked Tensor Model, the vector v0 can always be recovered up to a sign flip. This suggest the use of the loss function\nLoss(v̂,v0) ≡ min ( ‖v̂ − v0‖22, ‖v̂ + v0‖22 ) = 2− 2|〈v̂,v0〉| . (12)"
    }, {
      "heading" : "2 Ideal estimation",
      "text" : "In this section we consider the problem of estimating v0 under the Spiked Tensor Model, when no constraint is imposed on the complexity of the estimator. Our first result is a lower bound on the loss of any estimator.\nTheorem 1. For any estimator v̂ = v̂(X) of v0 from data X, such that ‖v̂(X)‖2 = 1 (i.e. v̂ : ⊗kRn → Sn−1), we have, for all n ≥ 4,\nβ ≤ √ k\n10 ⇒ E Loss(v̂,v0) ≥\n1\n32 . (13)\nIn order to establish a matching upper bound on the loss, we consider the maximum likelihood estimator v̂ML, obtained by solving the Tensor PCA problem. As in the case of matrix denoising, we expect the properties of this estimator to depend on signal to noise ratio β, and on the ‘norm’ of the noise ‖Z‖op (i.e. on the value of the optimization problem Tensor PCA in the case β = 0). For the matrix case k = 2, this coincides with the largest eigenvalue of Z. Classical random matrix theory shows that –in this case– ‖Z‖op concentrates tightly around 2 [Gem80, DS01a, BS10].\nIt turns out that tight results for k ≥ 3 follow immediately from a technically sophisticated analysis of the stationary points of random Morse functions by Auffinger, Ben Arous and Cerny [ABAC13]. (See Appendix B.1 for further background.)\nLemma 2.1. There exists a sequence of real numbers {µk}k≥2, such that\nlim sup n→∞\n‖Z‖op ≤ µk (k odd), (14)\nlim n→∞\n‖Z‖op = µk (k even). (15)\nFurther ‖Z‖op concentrates tightly around its expectation. Namely, for any n, k\nP (∣∣‖Z‖op − E‖Z‖op∣∣ ≥ s) ≤ 2 e−ns2/(2k) . (16)\nFinally µk = √ k log k(1 + ok(1)) for large k.\nAn explicit expression for the quantity µk is given in Appendix B (which also contains a proof, that uses [ABAC13]). Evaluating this expression for small values of k, we get the following explicit values, that we also compare with the large-k asymptotics √ k log k. (It is not hard to increase the number of digits in these evaluations, using the expressions in Appendix.)\nk µk √ k log k 3 2.8700 1.8154 4 3.5882 2.3548 5 4.2217 2.8368 10 6.7527 4.7985 100 27.311 21.460\nFor instance, this table indicates that a large order-3 Gaussian tensor should have ‖Z‖op ≈ 2.87, while a large order 10 tensor has ‖Z‖op ≈ 6.75. As a simple consequence of Lemma 2.1, we establish an upper bound on the error incurred by the maximum likelihood estimator, see Section B.2 for a proof.\nTheorem 2. Let µk be the sequence of real numbers introduced above. Letting v̂ ML denote the maximum likelihood estimator (i.e. the solution of Tensor PCA), we have for n large enough, and all s > 0\nβ ≥ µk ⇒ Loss(v̂ML,v0) ≤ 2\nβ (µk + s) , (17)\nwith probability at least 1− 2e−ns2/(16k).\nThe following upper bound on the value of the problem Tensor PCA is proved using SudakovFernique inequality. While it is looser than Lemma 2.1 (corresponding to the case β = 0), we expect it to become sharp for β ≥ βk a suitably large constant. We refer to Appendix B.3 for its proof.\nLemma 2.2. Under Spiked Tensor Model model, we have\nlim sup n→∞ E‖X‖op ≤ max τ≥0\n{ β ( τ√\n1 + τ2\n)k +\nk√ 1 + τ2\n} . (18)\nFurther, for any s ≥ 0,\nP (∣∣‖X‖op − E‖X‖op∣∣ ≥ s) ≤ 2 e−ns2/(2k) . (19)"
    }, {
      "heading" : "2.1 Historical Background",
      "text" : "At this point it is useful to pause, in order to provide some further background. The random cost function v 7→ HZ(v) ≡ 〈Z,v⊗k〉 (defined on the unit sphere v ∈ Sn−1) was studied in the context of statistical physics under the name of ‘spherical p-spin model.’ In particular, Crisanti and Sömmers [CS92] used the non-rigorous replica method from spin glass theory to compute the asymptotic value µk. Their results were confirmed rigorously by Talagrand [Tal06].\nThe most striking prediction from statistical physics is that the function HZ(v) has an exponential number of local maxima on the unit sphere [CS95]. Furthermore, there exists ηk < µk such that, for each x ∈ [ηk, µk) the number of local maxima with value HZ(v) ≈ x is exp{Θ(n)}. In [ABAC13] rigorous evidence is developed to this support picture.\nIn the Spiked Tensor Model these local maxima translate into undesired local maxima of 〈X,v⊗k〉. It is natural to guess that these local maxima affect local iterative algorithms, and that these do not converge to a good estimate of v0 unless they are initialized within thee ‘basin of attraction’ of v0. The analysis in the next sections confirms this intuition."
    }, {
      "heading" : "3 Tensor Unfolding",
      "text" : "A simple and popular heuristics to obtain tractable estimators of v0 consists in constructing a suitable matrix with the entries of X, and performing principal component analysis on this matrix. Since the number of distinct entries of X is of order nk, the resulting matrix Matq(X) has dimension Θ(nq)×Θ(nk−q). This operation is variously referred as matricization, unfolding, flattening. While the details of this construction can vary, we do not expect them to affect qualitatively our results, that we summarize for the sake of convenience:\n1. The best way to unfold X amounts to form a matrix as square as possible.\n2. Setting b = (dk/2e − 1)/2 (in particular b = 1/2 for k ∈ {3, 4}), the unfolding approach succeeds when β is larger than nb. This is to be compared with β = Θ(1) that is sufficient for the maximum likelihood estimator (see previous section).\nBased on heuristic arguments, we believe that the tight threshold is β & n(k−2)/4 (i.e. that β & n(k−2)/4 is both necessary and sufficient –modulo constants).\n3. A sharper analysis is possible when the symmetric noise tensor Z in our Spiked Tensor Model is replaced by non-symmetric Gaussian noise, and k is even. In particular, we can confirm the above conjecture in this case. (As mentioned, we expect similar results to hold more generally.)\nIn this case, if β ≤ (1−ε)nb, then the estimator from unfolding is essentially orthogonal to the signal v0. On the other hand, if β ≥ (1 + ε)nb, we construct an estimator with |〈v̂,v0〉| → 1.\n4. We achieves the remarkable behavior at the last point by a recursive unfolding method. In a nutshell we perform principal component analysis on Matq(X), construct a matrix out of the principal vector, and then perform again principal component analysis."
    }, {
      "heading" : "3.1 Symmetric noise",
      "text" : "For an integer 0 ≤ q ≤ k, we introduce the unfolding (also referred to as matricization or reshape) operator Matq : ⊗kRn → Rn\nq×nk−q as follows. For any indices i1, i2, · · · , ik ∈ [n], we let a = 1 + ∑q j=1(ij − 1)nj−1 and b = 1 + ∑k j=q+1(ij − 1)nj−q−1, and define\n[Matq(X)]a,b = Xi1,i2,··· ,ik . (20)\nStandard convex relaxations of low-rank tensor estimation problem compute factorizations of Matq(X)[TSHK11, LMWY13, MHG13, RPP13]. Not all unfoldings (choices of q) are equivalent. It is natural to expect that this approach will be successful only if the signal-to-noise ratio exceeds the operator norm of the unfolded noise ‖Matq(Z)‖op. The next lemma suggests that the latter is minimal when Matq(Z) is ‘as square as possible’ . A similar phenomenon was observed in a different context in [MHG13].\nLemma 3.1. For any integer 0 ≤ q ≤ k we have, for some universal constant Ck,\n1√ (k − 1)!\nnmax(q−1,k−q−1)/2 (\n1− Ck nmax(q,k−q)\n) ≤ E‖Matq(Z)‖op ≤ √ k ( n(q−1)/2 + n(k−q−1)/2 ) .\n(21)\nFor all n large enough, both bounds are minimized for q = dk/2e. Further\nP {∣∣‖Matq(Z)‖op − E‖Matq(Z)‖op∣∣ ≥ t} ≤ 2 e−nt2/(2k) . (22)\nProof. The concentration bound (22) follows because, for u ∈ Rnq ,v ∈ Rnk−q of norm 1, the function\n〈u,Matq(Z)v〉 = 1\nk!\n√ k\nn ∑ π∈Sk 〈u,Matq(Gπ)v〉 (23)\nis a Lipschitz function of the Gaussian vector G with modulus at most √ k/n. Hence the same holds for ‖Matq(Z)‖op = maxu,v〈u,Matq(Z)v〉, and the claim follows from Gaussian concentration of measure.\nFor the upper bound in Eq. (21), note that Matq(G π) has i.i.d. standard normal entries. The proof follows from the definition (8) together with triangular inequality and standard bounds on the norm of the random Gaussian matrices Matq(G π) ∈ Rnq×nk−q [DS01a]:\nE‖Matq(Z)‖op = 1\nk!\n√ k\nn E‖Matq( ∑ π Gπ)‖op\n≤ 1 k!\n√ k\nn ∑ π E‖Matq(Gπ)‖op\n≤ ∑ π 1 k!\n√ k\nn\n( nq/2 + n(k−q)/2 ) = √ k ( n(q−1)/2 + n(k−q−1)/2 ) .\nFor the upper bound in Eq. (21), note that\nmin(nq, nk−q)E{‖Matq(Z)‖2op} ≥ E‖Matq(Z)‖2F = 1\nk!\nk\nn ∑ π∈Sk E{〈G,Gπ〉} ≥ k n nk k! , (24)\nwhere the last inequality is proved by considering π = id the identity permutation (all the terms in the sum are positive). The desired lower bound follows since the concentration inequality (22) implies E{‖Matq(Z)‖2op} − E{‖Matq(Z)‖op}2 ≤ (2k/n), and we therefore have\nE{‖Matq(Z)‖op} ≥ E{‖Matq(Z)‖2op}1/2 (\n1− k nE{‖Matq(Z)‖2op}\n) . (25)\nThe last lemma suggests the choice q = dk/2e, which we shall adopt in the following, unless stated otherwise. We will drop the subscript from Mat.\nLet us recall the following standard result derived directly from Wedin perturbation Theorem [Wed72], and stated in the context of the spiked model.\nTheorem 3 (Wedin perturbation). Let M = βu0w0 T + Ξ ∈ Rm×p be a matrix with ‖u0‖2 = ‖w0‖2 = 1. Let ŵ denote the right singular vector of M. If β > 2‖Ξ‖op, then\nLoss(ŵ,w0) ≤ 8‖Ξ‖2op β2 . (26)\nProof of Theorem 3. Note β > 0 is the only singular value of βu0w0 T, while the second singular value of (βu0w0 T + Ξ) is at most ‖Ξ‖op. Wedin Theorem states that, for all β > ‖Ξ‖op, we have\n| sin(ŵ,w0)| ≤ ‖Ξ‖op\nβ − ‖Ξ‖op . (27)\nIn particular | sin(ŵ,w0)| ≤ 2‖Ξ‖op/β for β ≥ 2‖Ξ‖op. Hence the claim (26) follows from\n|〈ŵ,w0〉| = | cos(ŵ,w0)| ≥ √ 1−\n4‖Ξ‖2op β2 ≥ 1− 4‖Ξ‖2op β2 . (28)\nTheorem 4. Letting w = w(X) denote the top right singular vector of Mat(X), we have the following, for some universal constant C = Ck > 0, and b ≡ (1/2)(dk/2e − 1).\nIf β ≥ 5 k1/2 nb then, with probability at least 1− n−2, we have\nLoss ( w, vec ( v0 ⊗bk/2c)) ≤ C kn2b\nβ2 . (29)\nProof of Theorem 4. By definition we have\nMat(X) = βu0w0 T + Mat(Z) , (30)\nwhere u0 = vec(v0 ⊗bk/2c), w0 = vec(v0 ⊗dk/2e). We know by Lemma 3.1 that ‖Mat(Z)‖op ≤ (5/2) √ k nb with the claimed probability. The loss upper bound (29) follows immediately from this upper bound and Wedin’s theorem Eq. (26)."
    }, {
      "heading" : "3.2 Asymmetric noise and recursive unfolding",
      "text" : "A technical complication in analyzing the random matrix Matq(X) lies in the fact that its entries are not independent, because the noise tensor Z is assumed to be symmetric. In the next theorem we consider the case of non-symmetric noise and even k. This allows us to leverage upon known results in random matrix theory [Pau07, FP09, BGN12] to obtain: (i) Asymptotically sharp estimates on the critical signal-to-noise ratio; (ii) A lower bound on the loss below the critical signal-to-noise ratio. Namely, we consider observations\nX̃ = βv0 ⊗k + 1√ n G . (31)\nwhere G ∈ ⊗kRn is a standard Gaussian tensor (i.e. a tensor with i.i.d. standard normal entries).\nLet w = w(X̃) ∈ Rnk/2 denote the top right singular vector of Mat(X). For k ≥ 4 even, and define b ≡ (k − 2)/4, as above. By [Pau07, Theorem 4], or [BGN12, Theorem 2.3], we have the following almost sure limits\nβ ≤ (1− ε)nb ⇒ lim n→∞ 〈w(X̃), vec(v0⊗(k/2))〉 = 0 , (32)\nβ ≥ (1 + ε)nb ⇒ lim inf n→∞ ∣∣〈w(X̃), vec(v0⊗(k/2))〉∣∣ ≥√ ε 1 + ε . (33)\nIn other words w(X̃) is a good estimate of v0 ⊗(k/2) if and only if β is larger than nb.\nWe can use w(X̃) ∈ R2b+1 to estimate v0 as follows. Construct the matricization Mat1(w) ∈ Rn×n2b (slight abuse of notation) of w by letting, for i ∈ [n], and j ∈ [n2b],\nMat1(w)i,j = wi+(j−1)n , (34)\nwe then let v̂ to be the left principal vector of Mat1(X). We refer to this algorithm 3 as to recursive unfolding.\nTheorem 5. Let X̃ be distributed according to the non-symmetric model (31) with k ≥ 4 even, define b ≡ (k − 2)/4. and let v̂ be the estimate obtained by two-steps recursive unfolding.\nIf β ≥ (1 + ε)nb then, almost surely\nlim n→∞ Loss(v̂,v0) = 0 . (35)\nProof of Theorem 5. For the sake of simplicity, we assume β/nb → ε. The limit along other sequences follows from a standard subsequence argument.\nIt follows from the invariance of the noise distribution in Eq. (31) that\nw(X̃) = ρn vec(v0 ⊗(k/2)) + ρn nk/4 g , (36)\nwhere g ∼ N(0, Ink/2). It follows from Eq. (33), together with the almost sure limits limn→∞ ‖g‖2/nk/4 = 1 and limn→∞〈g, vec(v0⊗(k/2))〉/nk/4 = 1 that (almost surely)\nlim n→∞ ρn =\n√ ε\n1 + ε , (37)\nlim n→∞ ρn =\n√ 1\n1 + ε . (38)\nUsing the definition (34), we then have (recall that b = (k − 2)/4)\nMat1(w) = ρnv0 u T + ρn n(2b+1)/2 G′ , (39)\nwhere u = vec(v0 2b) and G′ ∈ Rn×n2b is a matrix with i.i.d standard normal entries. Using [DS01b], we have, with probability 1− e−Θ(n), ρn\nn(2b+1)/2 ‖G′‖op ≤\n1 n(2b+1)/2 ( n1/2 + nb ) ≤ 2√ n . (40)\nSince ρn is bounded away from zero as n→∞, Wedin’s theorem implies limn→∞ |〈v̂,v0〉| = 1, and therefore the claim (35).\n3In practice int might be more effective to use a balanced matricization at the second step. For instance if k is a power of two one could construct a square matricization and repeat the same process. For analysis purposes, we prefer the version described here.\nWe conjecture that the weaker condition n & n(k−2)/4 is indeed sufficient also for our original symmetric noise model model, both for k even and for k odd."
    }, {
      "heading" : "4 Power Iteration",
      "text" : "Iterating over (multi-) linear maps induced by a (tensor) matrix is a standard method for finding leading eigenpairs, see [KM11] and references therein for tensor-related results. In this section we will consider a simple power iteration, and then its possible uses in conjunction with tensor unfolding. Finally, we will compare our analysis with results available in the literature.\nApproximate Message Passing (AMP) provides a different iterative strategy and will be discussed in Section 5. While the qualitative behavior is the same as for naive power iteration, a sharper asymptotic analysis is possible for AMP."
    }, {
      "heading" : "4.1 Naive power iteration",
      "text" : "The simplest iterative approach is defined by the following recursion\nv0 = y\n‖y‖2 , and vt+1 = X{vt} ‖X{vt}‖2 . Power Iteration\nThe following result establishes convergence criteria for this iteration, first for generic noise Z and then for standard normal noise (using Lemma 2.1).\nTheorem 6. Assume\nβ ≥ 2 e(k − 1) ‖Z‖op , (41)\n〈y,v0〉 ‖y‖2\n≥ [\n(k − 1)‖Z‖op β\n]1/(k−1) , (42)\nThen for all t ≥ t0(k), the power iteration estimator satisfies\nLoss(vt,v0) ≤ 2e‖Z‖op\nβ . (43)\nIf Z is a standard normal noise tensor, then conditions (41), (41) are satisfied with high probability provided\nβ ≥ 2ek µk = 6 √ k3 log k ( 1 + ok(1) ) , (44)\n〈y,v0〉 ‖y‖2 ≥ [ kµk β ]1/(k−1) = β−1/(k−1) ( 1 + ok(1) ) . (45)\nWe next discuss two aspects of this result: (i) The requirement of a positive correlation between initialization and ground truth ; (ii) Possible scenarios under which the assumptions of Theorem 6 are satisfied.\nNotice that we require a positive correlation of the initialization y with the ground truth v0. The underlying reason is that, if 〈v0,v0〉 is small, then 〈vt,v0〉 remains small at all subsequent\niterations. In order to clarify this point, it is instructive to compute the distribution of v1 for standard Gaussian noise Z. We let\nτ̃0 ≡ 〈v0,y〉 ‖y‖2 . (46)\nUsing Eq. (9) and the fact that v0 is independent of Z, we get\nv1 = βτk−10 v0 + n −1/2g√ β2τ\n2(k−1) 0 + 1\n+ o(1) (47)\nwhere g ∼ N(0, In), and o(1) is a vector with ‖o(1)‖2 → 0 in probability as n→∞. In particular\nτ̃1 = 〈v1,v0〉 = βτ̃k−10√\nβ2τ̃ 2(k−1) 0 + 1\n+ o(1) . (48)\nIn particular τ1 . τ0 only if βτk−20 & 1, or, equivalently, 〈y,v0〉/‖y‖2 & β−1/(k−2). This suggest that the condition in Eq. (45) is not too far from being tight (in the sense that the exponent −1/(k − 1) can at best replaced by −1/(k − 2)).\nIn general we cannot assume that an initialization satisfying the conditions of Theorem 6. Hence, unlike for ordinary matrix factorization, power iteration is not a practical solution to the tensor principal component problem. There are however circumstances under which a sufficiently good initialization exists.\nExtremely low noise. If y is a uniformly random vector on the unit sphere, then 〈v0,y〉 is approximately normal with mean zero and variance 1/n. For instance |〈v0,y〉| ≥ 1/ √ n with\nprobability roughly 0.32.\nComparing this with condition (42), we obtain that a random initialization succeed with positive probability if\nβ ≥ (2n)(k−1)/2‖Z‖op . (49)\nFor standard Gaussian noise, this amounts to requiring β ≥ (2n)(k−1)/2µk. The above heuristic analysis suggests that the correct condition should be β & n(k−2)/2.\nAdditional side information. Additional information might be available about the vector v0. This information can be used for initiating the power iteration. In the next section we consider the special case in which tensor unfolding is used for initializing power iteration."
    }, {
      "heading" : "4.2 Comparison with Tensor Unfolding",
      "text" : "It is instructive to compare the result of the previous section with the ones for tensor unfolding, cf. Section 3. Summarizing, for standard Gaussian noise\n• Tensor unfolding is guaranteed to succeed provided β & nb, with b = (dk/2e − 1)/2. We conjecture that a necessary and sufficient condition is in fact β & n(k−2)/4 (e.g. β & n1/4 for order 3 tensors).\n• Power iteration, with random initialization requires β & n(k−1)/2. Our heuristic calculation suggests that a necessary and sufficient condition is in fact β & n(k−2)/2 (e.g. β & n1/2 for order 3 tensors)..\nIn other words, tensor unfolding is successful under a signal-to-noise ratio that is order of magnitudes smaller than power iteration. This suggests the following warm start procedure: (i) Compute a first estimate v̂Unfold of v0 using tensor unfolding; (ii) Use this as initialization for the power iteration, hence setting v0 = v̂Unfold. We will explore this approach numerically in Section 6."
    }, {
      "heading" : "4.3 Related work",
      "text" : "As mentioned above, power iteration is a natural approach to tensor factorization and was studied in several earlier papers. Most recently, interest within machine learning was spurred by [AGH+12].\nOur Theorem 6 is analogous to the main result of [AGH+12] although incomparable: • In [AGH+12] the ‘signal’ part of the tensor X is assumed to have an orthogonal decomposition∑n i=1 λiv ⊗k i with mini(λi) bounded away from zero. Here, the signal part has rank one\n(equivalently, all the λi’s but one vanish).\n• In [AGH+12] only the case of third order tensors (k = 3) is considered. We characterize power iteration for general k.\n• We establish convergence in a number of iterations t that is independent of the dimensions n. In [AGH+12] the number of iterations is bounded by a polynomial in n.\n• We evaluate our bounds in the case of Gaussian noise. This allows a comparison with other methods, such as tensor unfolding."
    }, {
      "heading" : "5 Asymptotics via Approximate Message Passing",
      "text" : "Approximate message passing (AMP) algorithms [DMM09, BM11] proved successful in several highdimensional estimation problems including compressed sensing, low rank matrix reconstruction, and phase retrieval [FRVB11, KRFU12, SC11, SR12]. An appealing feature of this class of algorithms is that their high-dimensional limit can be characterized exactly through a technique known as ‘state evolution.’ Here we develop an AMP algorithm for tensor data, and its state evolution analysis focusing on the fixed β, n→∞ limit. Proofs follows the approach of [BM11] and will be presented in a journal publication.\nIn a nutshell, our AMP for Tensor PCA can be viewed as a sophisticated version of the power iteration method of the last section. With the notation f(x) = x/‖x‖2, we define the AMP iteration over vectors vt ∈ Rn by v0 = y, f(v−1) = 0, and{\nvt+1 = X{f(vt)} − bt f(vt−1) , bt = (k − 1) ( 〈f(vt), f(vt−1)〉 )k−2 .\nAMP\n(Note that, unlike in power iteration, we normalize vt ‘before’ multiplying it by X. This choice is equivalent but yields slightly simpler expression.)\nOur main conclusion is that the behavior of AMP is qualitatively similar to the one of power iteration. However, we can establish stronger results in two respects:\n1. We can prove that, unless side information is provided about the signal v0, the AMP estimates remains essentially orthogonal to v0, for any fixed number of iterations. This corresponds to a converse to Theorem 6.\n2. Since state evolution is asymptotically exact, we can prove sharp phase transition results with explicit characterization of their locations.\nWe assume that the additional information takes the form of a noisy observation y = γ v0 + z, where z ∼ N(0, In/n). Our next results summarizes the state evolution analysis. Its proof is deferred to a journal publication.\nProposition 5.1. Let k ≥ 2 be a fixed integer. Let {v0(n)}n≥1 be a sequence of unit norm vectors v0(n) ∈ Sn−1. Let also {X(n)}n≥1 denote a sequence of tensors X(n) ∈ ⊗kRn generated following Spiked Tensor Model. Finally, let vt denote the t-th iterate produced by AMP, and consider its orthogonal decomposition\nvt = vt‖ + v t ⊥ , (50)\nwhere vt‖ is proportional to v0, and v t ⊥ is perpendicular. Then v t ⊥ is uniformly random, conditional on its norm. Further, almost surely\nlim n→∞ 〈vt,v0〉 = lim n→∞ 〈vt‖,v0〉 = τt , (51)\nlim n→∞\n‖vt⊥‖2 = 1 , (52)\nwhere τt is given recursively by letting τ0 = γ and, for t ≥ 0 (we refer to this as to state evolution):\nτ2t+1 = β 2\n( τ2t\n1 + τ2t\n)k−1 . (53)\nNote that state evolution coincides with the equation that we derived for the first iteration of power iteration, cf. Eq. (48) (apart from the different scaling). It is important to notice that for subsequent iterations t ≥ 1, state evolution (53) does not correctly describe naive power iteration. The reason is that vt depends on X, and hence the same argument does not apply. The AMP iteration differ from naive power iteration because of the ‘memory term’, −bt f(vt−1). As shown in [BM11], this memory term approximately cancels dependencies. As a consequence, the resulting algorithm obeys state evolution.\nThe following result characterizes the minimum required additional information γ to allow AMP to escape from those undesired local optima. We will say that {vt}t converges almost surely to a desired local optimum if, almost surely,\nlim t→∞ lim n→∞\nLoss(vt/‖vt‖2,v0) ≤ 6\nβ2 .\nTheorem 7. Consider the Tensor PCA problem with k ≥ 3 and β > ωk ≡ √ (k − 1)k−1/(k − 2)k−2 ∼ √ ek .\nThen AMP converges almost surely to a desired local optimum if and only if γ > √\n1/ k(β)− 1 where k(β) is the largest solution of (1− )(k−2) = β−2,\nIn the special case k = 3, and β > 2, assuming γ > β(1/2 − √\n1/4− 1/β2), AMP tends to a desired local optimum. Numerically β > 2.69 is enough for AMP to achieve 〈v0, v̂〉 ≥ 0.9 if γ > 0.45.\nAs a final remark, we note that the methods of [MR14] can be used to show that, under the assumptions of Theorem 7, for β > βk a sufficiently large constant, AMP asymptotically solves the optimization problem Tensor PCA. Formally, we have, almost surely,\nlim t→∞ lim n→∞ ∣∣∣〈X, (vt)⊗k〉 − ‖X‖op∣∣∣ = 0. (54)"
    }, {
      "heading" : "6 Numerical experiments",
      "text" : "Let us emphasize two practical suggestions that arise from our work:\n• Tensor unfolding is superior to tensor power iteration under our spiked model. For instance, for k = 3, we expect tensor power iteration to require β & n1/4 and unfolding to require β & n1/2.\n• For smaller values of β, iterative methods (tensor power iteration or approximate message passing) only produce a good estimate if the initialization has a scalar product with the ground truth v0 that is bounded away from zero.\n• As a consequence of the above, side information about the unknown vector v0 can greatly improve performances.\nA special case, we will study the behavior of warm start algorithms that first perform a singular value decomposition of Mat(X), and then apply an iterative method (tensor power iteration or approximate message passing).\nIn this section we will illustrate these suggestions through numerical simulations. Section 6.1 describes a refinement of tensor unfolding that provides a tighter relaxation. Section 6.2 compares different algorithms. Finally, Section 6.3 provides additional illustration of how side information can dramatically simplify the estimation problem."
    }, {
      "heading" : "6.1 PSD-constrained principal component",
      "text" : "Note that, for v ∈ Rn, the outer product v⊗v (regarded as an n×n matrix) is positive semi-definite (PSD). Considering the case k = 3, we have\nMat(X) = βvec(v0 ⊗ v0) v0T + Mat(Z) . (55)\nThis remark suggest to perform a cone-constrained principal component analysis of Mat(X), where the left singular vector (viewed as a matrix) belongs to the PDS cone. In order to write this formally, it is convenient to introduce the operator reshapen×n : Rn\n2 → Rn×n that matricizes vectors as reshapen×n(w)i,j = wn(i−1)+j . The PSD-cone-constrained principal component of Mat(X), is defined by\n(ŵ, v̂) ≡ arg max { 〈w,Mat(X)v〉 : reshapen×n(w) 0 , ‖w‖2 ≤ 1 , ‖v‖2 ≤ 1 } . (56)\nThis optimization problem is NP hard, since it includes copositive programming as a special case. However [DMR14] provides rigorous and empirical evidence that problems of this type can be solved efficiently by a projected power iteration, under statistical model dor X.\nDenoting P : Rn 2 → Rn2 the orthogonal projector onto the PSD cone, we iterate the following\nfor t ≥ 0, using random initialization of u0 ∈ Rn,{ wt = P (Mat(X)v t),\nvt+1 = Mat(X)Twt/‖Mat(X)Twt‖2 . (57)"
    }, {
      "heading" : "6.2 Comparison of different algorithms",
      "text" : "In Fig. 1 we compare different algorithms on data generated following Spiked Tensor Model with k = 3, and n ∈ {25, 50, 100, 200, 400, 800} and for a range of values of β ∈ [2, 10]. The plots represent measured values of the absolute correlation |〈v̂,v0〉| versus β, averaged over 50 samples (except for n = 800, where we used 8 samples).\nThe main findings are consistent with the theory developed above:\n• Tensor power iteration (with random initialization) performs poorly with respect to other approaches that use some form of tensor unfolding. The gap widens as the dimension n increases.\n• PSD-constrained principal component analysis (described in the last section) is slightly superior to plain unfolding.\n• All algorithms based on initial unfolding have essentially the same threshold. Above that threshold, those that process the singular component (either by recursive unfolding or by tensor power iteration) have superior performances over simpler one-step algorithms.\nIn addition, we noted that the two iterative algorithms (Power Iteration and AMP) show very close behaviors in our experiments.\nIn Figure 2 we compare the scaling with n of the threshold signal-to-noise ratio for different type of algorithms. Our heuristic arguments suggest that tensor power iteration with random initialization will work for β & n1/2, while unfolding only requires β & n1/4 (our theorems guarantee this for, respectively, β & n and β & n1/2). We plot the average correlation |〈v̂,v0〉| versus (respectively) β/n1/2 and β/n1/4. The curve superposition confirms that our prediction captures the correct behavior already for n of the order of 50."
    }, {
      "heading" : "6.3 The value of side information",
      "text" : "Our next experiment concerns a simultaneous matrix and tensor PCA task: we are given a tensor X ∈ ⊗3Rn of Spiked Tensor Model with k = 3 and the signal to noise ratio β = 3 is fixed. In addition, we observe M = λv0v0\nT + N where N ∈ Rn×n is a symmetric noise matrix with upper diagonal elements i < j iid Ni,j ∼ N(0, 1/n) and the value of λ ∈ [0, 2] varies. This experiment mimics a rank-1 version of topic modeling method presented in [AGH+12] where M is a matrix representing pairwise co-occurences and X triples.\nThe analysis in previous sections suggest to use the leading eigenvector of M as the initial point of AMP algorithm for tensor PCA on X. We performed the experiments on 100 randomly\ngenerated instances with n = 50, 200, 500 and report in Figure 3 the mean values of |〈v0, v̂(X)〉| with confidence intervals.\nRandom matrix theory predicts limn→∞〈v̂1(M),v0〉 = √\n1− λ−2 [FP09]. Thus we can set γ = √ 1− λ−2 and apply the theory of the previous section. In particular, Proposition 5.1 implies\nlim n→∞\n〈v̂(X),v0〉 = β ( 1/2 + √ 1/4− 1/β2 ) if γ > β ( 1/2− √ 1/4− 1/β2 )\nand limn→∞〈v̂(X),v0〉 = 0 otherwise Simultaneous PCA appears vastly superior to simple PCA. Our theory captures this difference quantitatively already for n = 500."
    }, {
      "heading" : "Acknowledgements",
      "text" : "This work was partially supported by the NSF grant CCF-1319979 and the grants AFOSR/DARPA FA9550-12-1-0411 and FA9550-13-1-0036.\nA Information theoretic bound: Proof of Theorem 1\nIntroduce the operator\nU :⊗k Rn → R( n k)\nX 7→ U(X) ,\nwhere for indices i1 < i2 < · · · < ik, we have U(X)a(i1,··· ,ik) = Xi1,··· ,ik with a(i1, · · · , ik) = 1 + ∑k j=1 n\nj−1(ij − 1). Let D(·‖·) denote the Kullback-Leiber divergence where Pw is the law of U(X) conditional on v0 = w. Lemma A.1. For any pairs of vectors w,w′ ∈ Sn−1 we have\nD(Pw‖Pw′) ≤ 2 n\nk β2.\nProof. First note that for any w ∈ Sn−1, Pw is a Gaussian probability distribution\nPw = N\n( βU(w⊗k),\n1\n(k − 1)!n I(nk)\n) .\nOn the other hand for any symmetric tensor W ∈ ⊗kRn we have k!‖U(W)‖22 ≤ ‖W‖2F . Therefore we have\nD(Pw‖Pw′) = n(k − 1)!β2‖U(w⊗k)− U(w′⊗k)‖22 ≤ n k β2‖w⊗k −w′⊗k‖2F\n= 2n\nk β2(1− 〈w,w′〉k)\n≤ 2n k β2.\nWe are now in position to prove Theorem 1. Let V denote the class of estimators v̂ with unit norm:\nV =\n{ v̂ :⊗k Rn → Sn−1\nX 7→ v̂(X)\n} . (58)\nProof of Theorem 1. Recall that the packing number Nn(ε) of Sn−1 is the maximum cardinality of any set N ⊆ Sn−1 such that (‖x − x′‖2 ∧ ‖x + x′‖2) ≥ ε for any x,x′ ∈ N . By a standard argument, letting Mn(ε) the corresponding covering number\n4, we have, for x ∈ Sn−1 a point on the unit sphere,\nNn(ε) ≥Mn(ε) ≥ Voln−1(Sn−1) 2Voln−1(Sn−1 ∩B(x, ε)) ≥ ( 1 ε )n−1 . (59)\n4That is, the minimum cardinality of any set N ∗ such that minx∈N∗(‖u− x‖2 ∧ ‖u + x‖2) ≤ ε for any u ∈ Sn−1.\n(Here Voln−1( · ) denotes the (n− 1)-dimensional volume, and B(x, ε) the ball of radius ε centered at x.)\nLet N denote an ε-packing with cardinality |N | ≥ Nn(ε). Let v0 be uniformly distributed in the set N . For an estimator v̂ ∈ V, we define G(v̂(X)) = arg minw∈N ‖v̂(X) − w‖2. Consider the error event {G(v̂(X)) 6= v0}. By definition of G(v̂(X)), the event G(v̂(X)) 6= v0 implies (‖v̂(X)− v0‖2 ∧ ‖v̂(X) + v0‖2) ≥ ε/2. By Markov inequality we have:\nP{G(v̂(X)) 6= v0} ≤ P { (‖v̂(X)− v0‖2 ∧ ‖v̂(X)− v0‖2) ≥ ε/2}\n≤ 4E{Loss(v̂,v0)} ε2 ≤ 4 ε2 inf v̂∈V\nsup v0∈Sn−1\nE{Loss(v̂,v0)} . (60)\nBy Fano’s inequality [CT12] we have that:\nP{G(v̂(X)) 6= v0} ≥ 1− I(v0; X) + log 2\nlog |N | (61)\n≥ 1− ∆ + log 2 log |N | , (62)\nwhere ∆ = maxw 6=w′∈N D(Pw‖Pw′), and in the second inequality we used [HV94]\nI(v0; X) ≤ 1 |N |2 ∑\nw 6=w′∈N D(Pw‖Pw′) . (63)\nUsing Eq. (59) and Lemma A.1, in Eq. (61), we get\nP{G(v̂(X)) 6= v0} ≥ 1− 2nβ2/k + log 2\n(n− 1) log(1/ε) . (64)\nChoosing ε = 1/2, we get, P{G(v̂(X)) 6= v0} ≥ 1− (5β2/k) for n ≥ 4 and β ≤ √ k/3. In particular\nP{G(v̂(X)) 6= v0} ≥ 1/2 provided β < √ k/10. By Eq. (60) this implies\ninf v̂∈V sup v0∈Sn−1\nE{Loss(v̂,v0)} ≥ 1\n32 . (65)"
    }, {
      "heading" : "B Maximum likelihood: Proof Theorem 2",
      "text" : "B.1 Operator norm of the noise tensor: Proof of Lemma 2.1\nLet Zn ∈ ⊗kRn be a symmetric standard normal tensor, and consider the associated objective function\nHZ : Sn−1 → R , (66) v 7→ HZ(v) ≡ 〈Z,v⊗k〉 . (67)\nWhile the function HZ( · ) is obviously non-convex, it turns out that –for random data Z– it is dramatically so. Namely, it has an exponential number of local maximum, whose value is –typically– only a fraction of the value of the global maximum.\nIn order to quantify this phenomenon, for x ∈ R, let Ck(Zn, x) denote the number of local maxima of HZ( · ) over Sn−1, that have value larger or equal than x. The next Lemma from [ABAC13] characterizes the growth rate of the number of local minima.\nTheorem 8 (Theorem 2.5 and Lemma 6.3 in [ABAC13]). For any k ≥ 3, we have\nlim n→∞\n1 n log ECk(Zn, x) = gk(x) , (68)\nwhere, for x ≥ ηk ≡ 2 √ k − 1\ngk(x) = 1\n2 { 2− k k − log ( kz2 2 ) + k − 1 2 z2 − 2 k2z2 } , z =\n1\n(k − 1) √ 2k\n( x− √ x2 − 4(k − 1) ) .\n(69)\nFurther, for x < ηk, gk(x) = gk(ηk).\nThe function gk(x) is monotone decreasing for x ≥ ηk, and non-negative if and only if x ∈ [ηk, µk] for some µk > 0 (strictly positive for x ∈ [ηk, µk)). In Figure 4, we plot gk(x) for k ∈ {3, 4, 5}. Informally, this means that the function HZ(v) has exponentially many local maxima with value HZ(v) ≈ x for any x ∈ [ηk, µk). To leading exponential order, the number of such maxima is given by exp{n gk(x)}.\nThe value µk can be determined as the unique solution to the equation g(x) = 0. It is immediately to do this numerically, obtaining the values in Section 2.\nThe last result implies that the global maximum of HZ(v) is (asymptotically) at least µk. Indeed the global maximum is necessarily a local maximum as well. The next result implies that indeed the global maximum converges to µk.\nTheorem 9 (Theorem 2.12 in [ABAC13]). Let µk denote the unique non-negative root of the equation gk(x) = 0, for x ≥ ηk ≡ 2 √ k − 1. Then\nlim n→∞\nE‖Z‖op = µk . (70)\nIn order to derive the large-k asymptotics of µk, we rewrite Eq. (69) in terms of the variable y ≡ k2z2/2. We get gk(x) = fk(y(x))/2, where\nfk(y) = 2− k k + log(k)− log(y) + k − 1 k y − 1 y , x = (k − 1)\n√ y\nk +\n√ k\ny . (71)\nFurther y ∈ (0, k/(k − 1)]. The claimed asymptotics follows by showing that the only solution of fk(y) = 0 in this interval obeys yk = (log k)\n−1(1 + ok(1)). This in turns can be showed by using the bounds\nlog(ke−1+(2/k))− log y − 1 y ≤ fk(y) ≤ log(ke2/k)− log(y)− 1 y , (72)\nand showing that the solution of y−1 + log(y) = log(a) for large a is y−1 = a+ Θ(log(a)). Finally, the norm ‖Z‖op concentrates tightly around its expectation.\nLemma B.1. For any s ≥ 0, we have\nP (∣∣‖Z‖op − E‖Z‖op∣∣ ≥ s) ≤ 2 e−ns2/(2k) . (73)\nProof. Note that\n〈Z,v⊗k〉 = √ k\nn 〈G,v⊗k〉 (74)\nis a Lipschitz function with Lipschitz modulus √ k/n (with respect to Euclidean norm) of the Gaussian vector (tensor) G. Hence ‖Z‖op is Lipchitz continuous with the same modulus. The claim follows from Gaussian isoperimetry [Led01].\nRemark B.2. Note that to make the connection with the notations used in [ABAC13], one has to use the proper scaling Hn,k(v) =\nn√ k LZ(v/\n√ n) (Hn,k(v)is the objective function considered in\n[ABAC13]).\nRemark B.3. The upper bound on the tensor operator norm obtained from Sudakov-Fernique inequality is not tight. In fact taking β = 0 in Lemma 2.2 gives the loose upper bound ‖Z‖op ≤ k. Except in the case of random matrices (k = 2), this is loose roughly by a factor √ k.\nB.2 Proof of Theorem 2\nBy optimality of v̂, we have\nβ〈v0, v̂〉k + 〈Z, v̂⊗k〉 ≥ β + 〈Z,v0⊗k〉 , (75)\nwhence\n〈v0, v̂〉k ≥ 1− 1 β 〈Z, v̂⊗k − v0⊗k〉 (76)\n≥ 1− 1 β\n( ‖Z‖op − 〈Z,v0⊗k〉 ) . (77)\nNote that ‖Z‖op − 〈z,v0⊗k〉 is Lipchitz continuous in the Gaussian random variables G, with modulus bounder by 2 √ k/n. Hence, by Gaussian isoperimetry, with probability at least 1 − 2e−ns 2/(8k), we have (since Z and v0 are independent, E〈Z,v0⊗k〉 = 0)\n〈v0, v̂〉k ≥ 1− 1\nβ\n( E‖Z‖op + s ) (78)\nUsing (1−α)1/k ≥ (1−α) which holds for α ∈ [0, 1], and rescaling s, we get |〈v0, v̂〉| ≥ 1−(µk+s)/β with probability at least 1− 2e−ns2/(16k) for all n large enough.\nB.3 Proof of Lemma 2.2\nLemma B.4. For each n ∈ N, let g ∼ N(0, In/n) and v0(n) ∈ Rn be a vector with ‖v0(n)‖2 = 1. Then there exists a sequence δn independent from x, such that limn→∞ δn = 0 and the following happens. With probability one, there exists (a random) n0 such that, for all n ≥ n0,\nsup τ∈[0,τmax] ∣∣∣ ‖g + τv0‖2 −√1 + τ2∣∣∣ ≤ δn . (79) Proof. Since x 7→ √ x is uniformly continuous on bounded intervals [0,M ], it is sufficient to prove\nsup τ∈[0,τmax] ∣∣∣ ‖g + τv0‖22 − (1 + τ2)∣∣∣ ≤ δn . (80) for all n ≥ n0, and an eventually different sequence δn. By triangular inequality and using ‖v0‖2 = 1,\nsup τ∈[0,τmax] ∣∣∣ ‖g + τv0‖22 − (1 + τ2)∣∣∣ ≤ ∣∣‖g‖2 − 1∣∣+ 2τmax∣∣〈v0,g〉∣∣ (81) Next we have limn→∞ ‖g‖2 = 1 almost surely by the strong law of large numbers, and 〈v0,g〉 ∼ N(0, 1/n) whence limn→∞〈v0,g〉 = 0 by Borel-Cantelli.\nProof of Lemma 2.2. For κ ∈ [0, 1], we define\nWκ ≡ { v ∈ Sn−1 : 〈v,v0〉 = κ } , (82)\nMX(κ) ≡ max { 〈X,v⊗k〉 : v ∈ Wκ } , (83)\nM(κ) ≡ EMX(κ) = Emax { 〈X,v⊗k〉 : v ∈ Wκ } . (84)\nNote that\nλ1(X) = max κ∈[0,1] MX(κ). (85)\nThe function X 7→ MX(κ) is a Lipschitz continuous function with Lipschitz constant √ k/n of the standard Gaussian tensor G (namely |MX(κ) −MX′(κ)| ≤ (k/n)1/2‖G −G′‖F ). Hence, by Gaussian isoperimetry, we have\nP {∣∣MX(κ)−M(κ)∣∣ ≥ t} ≤ 2 e−nt2/(2k) . (86)\nFurther we claim that κ 7→MX(κ) is uniformly continuous for κ ∈ [0, 1]. In order to prove this, let v(κ) = κv0 + √ 1− κ2v⊥ = argmax { 〈X,v⊗k〉 : v ∈ Wκ } , (87)\nwhere 〈v⊥,v0〉 = 0. We have, for κ1, κ2 ∈ [0, 1], and by letting v⊥ and w⊥ denote the perpendicular components of v(κ1) and v(κ2), we have for some constant c > 0\nMX(κ1) =〈X,v(κ1)⊗k〉 = 〈X, ( κ1v0 + √ 1− κ21v ⊥ ) ⊗k〉\n≥ 〈X, ( κ1v0 + √ 1− κ21w ⊥ ) ⊗k〉 by optimality\n= 〈X, { κ2v0 + √ 1− κ22w ⊥ + (κ1 − κ2)v0 + (√ 1− κ21 − √ 1− κ22 ) w⊥ } ⊗k〉 = MX(κ2) + k∑ q=1 ( k q ) 〈X,v(κ2)⊗q ⊗ { (κ1 − κ2)v0 + ( √ 1− κ21 − √ 1− κ22)w ⊥ }⊗(k−q) 〉\n(88)\n≥MX(κ2)− c‖X‖op { (κ1 − κ2)2 + (√ 1− κ21 − √ 1− κ22 )2}1/2 . (89)\nwhere Eq. (88) was obtained by exploiting the symmetry of the tensor X and Eq. (89) was derived using the norm of the vector { (κ1 − κ2)v0 + ( √ 1− κ21 − √ 1− κ22)w⊥ } . Using Eq. (86) over a grid κ ∈ {0, 1/n, 2/n, . . . , 1− 1/n, 1}, and the fact5 that ‖X‖op ≤ C for some constant C > 0 with probability 1− e−Θ(n), we have for all t > 0 and some constant c > 0\nP {\nmax κ∈[0,1] ∣∣MX(κ)−M(κ)∣∣ ≥ t} ≤ 2ne−nct2/2 + 2 e−cn . (90) In particular, by Borel-Cantelli we have, almost surely,\nlim n→∞ max κ∈[0,1] ∣∣MX(κ)−M(κ)∣∣ = 0 . (91) In order to upper bound M(κ), we apply Sudakov-Fernique inequality for non-centered Gaussian processes [Vit00, Theorem 1] to the two processes {Xv}, {Yv} indexed by v ∈ Wκ defined as follows:\nXv ≡ 〈X,v⊗v〉 = β〈v0,v〉k + 〈Z,v⊗k〉 , (92) Yv ≡ β〈v0,v〉k + k〈g,v〉 , (93)\n5This follows from Lemma 2.1 and triangular inequality, or from a standard ε-net argument.\nfor random a vector g ∼ N(0, In/n). It is easy to see that EXv = EYv and\nE {[ Xv −Xw ]2} = {EXv − EXw}2 + 2 k\nn\n( 1− 〈v,w〉k ) , (94)\nE {[ Yv − Yw ]2} = {EYv − EYw}2 + 2 k2\nn\n( 1− 〈v,w〉 ) . (95)\nHence E {[ Xv − Xw ]2} ≤ E{[Yv − Yw]2} (this follows from 1− ak ≤ k(1− a) for a ∈ [−1, 1]). We conclude that\nM(κ) ≤ Emax { βκk +\nk√ n 〈g,v〉 : v ∈ Wκ\n} (96)\n≤ max τ≥0 {( τ√ 1 + τ2 )k + k√ 1 + τ2 } + δn (97)\nwhere τ = κ/ √\n1− κ2 and δn satisfies limn→∞ δn = 0 uniformly over κ ∈ [0, 1], by Lemma B.4. We finally conclude that\nlim sup n→∞ E‖X‖op ≤ max τ≥0\n( τ√\n1 + τ2\n)k +\nk√ 1 + τ2 . (98)\nConcentration around the expectation follows by Gaussian isoperimetry as in the proof of Lemma 2.1."
    }, {
      "heading" : "C Power Iteration: Proof of Theorem 6",
      "text" : "Let τt ≡ 〈v0,vt〉 and ξ ≡ ‖Z‖op/β. Let τmin, τ∗ ∈ [0, 1] be the two solutions of\nτk−1(1− τ) = ξ . (99)\nWe will show below that our assumptions imply τ0 > τmin. Further τ ≥ τmin implies τk−1 − ξ ≥ 0. By definition of X, we have\nX{vt} = βτk−1t v0 + Z{vt} , (100)\nwhich implies, by triangular inequality,\n〈v0,X{vt}〉 ≥ βτk−1t − βξ , (101) ‖X{vt}‖2 ≤ βτk−1t + βξ . (102)\nWe will prove the first inequality τt ≥ τmin by induction. It is true at t = 0 by assumption. Assume it is true at t. Then τt+1 ≥ 0 using Eq. (101).\nHence we can divide the two inequalities above obtaining τt+1 ≥ (τk−1t − ξ)/(τ k−1 t + ξ) which\nimplies\nτt+1 ≥ 1− ξ\nτk−1t . (103)\nIn particular τt ≥ τ̃0t where the latter sequence is defined by τ̃t+1 = f(τ̃t), τ̃0 = τ0, for f(x) = 1− ξ x−k+1. The function f( · ) is concave and monotone increasing, and maps [τmin, τ∗] into itself, with f ′(τmin) > 1, f\n′(τ∗) < 1. By standard calculus argument τ̃t → τ∗ exponentially fast, which implies\n〈vt,v0〉 ≥ τ∗ − c0 e−t/c0 . (104)\nTo conclude the proof of Eq. (43), we notice that, for ξ ≤ 1/(2e(k − 1))\nτ∗ > 1− e ξ , (105) τmin < [(k − 1)ξ]1/(k−1) , (106)\nwhere we recall that τmin, τ∗ are the two solutions of gk(x) ≡ xk−1(1 − x) = ξ in the interval [0, 1]. For the first inequality, note that, in the interval [e−1/(k−1), 1], gk(x) is decreasing with gk(x) ≥ e−1(1− x). This implies\ne−1(1− τ∗) ≥ ξ (107)\ni.e. τ∗ ≥ 1− e ξ as long as 1− e ξ ≥ e−1/(k−1), which is implied by ξ ≤ 1/(2e(k − 1)). For the second inequality, note that, in the interval [0, 1− (k − 1)−1], we have gk(x) increasing with gk(x) ≥ xk−1/(k − 1). This implies\nτk−1min k − 1 ≥ ξ , (108)\nas long as [(k − 1)ξ]1/k−1 ≤ 1− (k − 1)−1, which follows, again, by our assumptions. Finally, conditions (44), (45) follow directly by applying Lemma 2.1."
    }, {
      "heading" : "D Approximate Message Passing: Proof of Theorem 7",
      "text" : "Let us recall the state evolution recursion (53)\nτ2t+1 = f(τ 2 t ;β) , (109) f(z;β) ≡ β2 ( z\n1 + z\n)k−1 . (110)\nNotice that f( · ;β) is strictly positive and monotone increasing on R>0. The theorem follows by proving that the following claims hold for β > ωk\n1. The fixed point equation τ2 = f(τ2;β) has two strictly positive solutions τ21 (β) < τ 2 2 (β). 2. The smallest fixed point is given by τ1(β) = √ 1/ k(β)− 1 as in the statement.\n3. The largest fixed point satisfies τ2(β) > 1− (2/β2).\nThe behavior of the function f(τ2;β) is illustrated in Fig. 5.\nIn order to prove the above statements, it is convenient to use the monotone parametrization x ≡ τ2/(1 + τ2) that maps R≥0 onto the interval [0, 1). After some algebra (and discarding the solution at x = 0), fixed point equation then reads\n1\nβ2 = xk−2(1− x) ≡ hk(x) . (111)\nNow, the function x 7→ hk(x) is continuously differentiable and strictly positive in the interval (0, 1), with hk(0) = hk(1) = 0. Further, simple calculus shows it has a unique stationary point (a maximum) at x∗ = (k − 2)/(k − 1) with hk(x∗) = 1/ω2k. This implies that, for β > ωk, Eq. (111) has two fixed points 0 < x2(β) < x∗ < x1(β) < 1 thus implying points 1 and 2 above (the latter immediately follows from inverting the re-parametrization).\nTo prove point 3, note that\nx2 = 1− 1\nxk−22 β 2\n(112)\n≥ 1− 1 xk−2∗ β2\n(113)\n≥ 1− 3 β2\n(114)\nwhere the second inequality follows since x2 > x∗. By state evolution (Proposition 5.1), together with the fact that τt → τ2, we have\nlim t→∞ lim n→∞\nLoss(vt/‖vt‖2,v0) = 2 ( 1− √ τ22\n1 + τ22\n) (115)\n= 2(1− √ x2) ≤ 2(1− x2) ≤ 6\nβ2 . (116)"
    } ],
    "references" : [ {
      "title" : "Random matrices and complexity of spin glasses",
      "author" : [ "A. Auffinger", "G. Ben Arous", "J. Cerny" ],
      "venue" : "Communications on Pure and Applied Mathematics",
      "citeRegEx" : "Auffinger et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Auffinger et al\\.",
      "year" : 2013
    }, {
      "title" : "A tensor spectral approach to learning mixed membership community",
      "author" : [ "Anima Anandkumar", "Rong Ge", "Daniel Hsu", "Sham M Kakade" ],
      "venue" : null,
      "citeRegEx" : "Anandkumar et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Anandkumar et al\\.",
      "year" : 2013
    }, {
      "title" : "The singular values and vectors of low rank perturbations of large rectangular random matrices",
      "author" : [ "Florent Benaych-Georges", "Raj Rao Nadakuditi" ],
      "venue" : "Journal of Multivariate Analysis",
      "citeRegEx" : "Benaych.Georges and Nadakuditi,? \\Q2012\\E",
      "shortCiteRegEx" : "Benaych.Georges and Nadakuditi",
      "year" : 2012
    }, {
      "title" : "The dynamics of message passing on dense graphs, with applications to compressed sensing",
      "author" : [ "M. Bayati", "A. Montanari" ],
      "venue" : "IEEE Trans. on Inform. Theory",
      "citeRegEx" : "Bayati and Montanari,? \\Q2011\\E",
      "shortCiteRegEx" : "Bayati and Montanari",
      "year" : 2011
    }, {
      "title" : "Spectral Analysis of Large Dimensional Random Matrices (2nd edition)",
      "author" : [ "Z. Bai", "J. Silverstein" ],
      "venue" : null,
      "citeRegEx" : "Bai and Silverstein,? \\Q2010\\E",
      "shortCiteRegEx" : "Bai and Silverstein",
      "year" : 2010
    }, {
      "title" : "Exact matrix completion via convex optimization, Foundations of Computational mathematics",
      "author" : [ "E.J. Candès", "B. Recht" ],
      "venue" : null,
      "citeRegEx" : "Candès and Recht,? \\Q2009\\E",
      "shortCiteRegEx" : "Candès and Recht",
      "year" : 2009
    }, {
      "title" : "The spherical p-spin interaction spin glass model: the statics",
      "author" : [ "Andrea Crisanti", "H-J Sommers" ],
      "venue" : "Zeitschrift für Physik B Condensed Matter",
      "citeRegEx" : "Crisanti and Sommers,? \\Q1992\\E",
      "shortCiteRegEx" : "Crisanti and Sommers",
      "year" : 1992
    }, {
      "title" : "Thouless-Anderson-Palmer approach to the spherical p-spin spin glass model",
      "author" : [ "A Crisanti", "H-J Sommers" ],
      "venue" : "Journal de Physique I",
      "citeRegEx" : "Crisanti and Sommers,? \\Q1995\\E",
      "shortCiteRegEx" : "Crisanti and Sommers",
      "year" : 1995
    }, {
      "title" : "The Dantzig selector: Statistical estimation when p is much larger than n, The Annals of Statistics",
      "author" : [ "E. Candes", "T. Tao" ],
      "venue" : null,
      "citeRegEx" : "Candes and Tao,? \\Q2007\\E",
      "shortCiteRegEx" : "Candes and Tao",
      "year" : 2007
    }, {
      "title" : "Elements of information",
      "author" : [ "Thomas M Cover", "Joy A Thomas" ],
      "venue" : null,
      "citeRegEx" : "Cover and Thomas,? \\Q2012\\E",
      "shortCiteRegEx" : "Cover and Thomas",
      "year" : 2012
    }, {
      "title" : "A tensor-based algorithm for high-order graph matching",
      "author" : [ "O. Duchenne", "F. Bach", "I. Kweon", "J. Ponce" ],
      "venue" : "Conference on Computer Vision and Pattern Recognition (CVPR),",
      "citeRegEx" : "Duchenne et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Duchenne et al\\.",
      "year" : 2009
    }, {
      "title" : "Optimally sparse representation in general (nonorthogonal) dictionaries via `1 minimization",
      "author" : [ "D.L. Donoho", "M. Elad" ],
      "venue" : "Proceedings of the National Academy of Sciences",
      "citeRegEx" : "Donoho and Elad,? \\Q2003\\E",
      "shortCiteRegEx" : "Donoho and Elad",
      "year" : 2003
    }, {
      "title" : "Message Passing Algorithms for Compressed Sensing",
      "author" : [ "D.L. Donoho", "A. Maleki", "A. Montanari" ],
      "venue" : "Proceedings of the National Academy of Sciences",
      "citeRegEx" : "Donoho et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Donoho et al\\.",
      "year" : 2009
    }, {
      "title" : "Cone-constrained principal component analysis",
      "author" : [ "Y. Deshpande", "A. Montanari", "E. Richard" ],
      "venue" : "Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "Deshpande et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Deshpande et al\\.",
      "year" : 2014
    }, {
      "title" : "Local operator theory, random matrices and Banach spaces",
      "author" : [ "K.R. Davidson", "S.J. Szarek" ],
      "venue" : "Handbook on the Geometry of Banach spaces,",
      "citeRegEx" : "Davidson and Szarek,? \\Q2001\\E",
      "shortCiteRegEx" : "Davidson and Szarek",
      "year" : 2001
    }, {
      "title" : "Local operator theory, random matrices and banach spaces, Handbook of the geometry of Banach spaces",
      "author" : [ "Kenneth R Davidson", "Stanislaw J Szarek" ],
      "venue" : null,
      "citeRegEx" : "Davidson and Szarek,? \\Q2001\\E",
      "shortCiteRegEx" : "Davidson and Szarek",
      "year" : 2001
    }, {
      "title" : "Péché, The largest eigenvalues of sample covariance matrices for a spiked population: diagonal case",
      "author" : [ "S.D. Féral" ],
      "venue" : "Journal of Mathematical Physics",
      "citeRegEx" : "Féral,? \\Q2009\\E",
      "shortCiteRegEx" : "Féral",
      "year" : 2009
    }, {
      "title" : "Neural reconstruction with approximate message passing (neuramp)",
      "author" : [ "A.K. Fletcher", "S. Rangan", "L.R. Varshney", "A. Bhargava" ],
      "venue" : null,
      "citeRegEx" : "Fletcher et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Fletcher et al\\.",
      "year" : 2011
    }, {
      "title" : "A limit theorem for the norm of random matrices",
      "author" : [ "S. Geman" ],
      "venue" : "Annals of Probability",
      "citeRegEx" : "Geman,? \\Q1980\\E",
      "shortCiteRegEx" : "Geman",
      "year" : 1980
    }, {
      "title" : "Most tensor problems are np-hard",
      "author" : [ "Christopher J Hillar", "Lek-Heng Lim" ],
      "venue" : "Journal of the ACM (JACM)",
      "citeRegEx" : "Hillar and Lim,? \\Q2013\\E",
      "shortCiteRegEx" : "Hillar and Lim",
      "year" : 2013
    }, {
      "title" : "Generalizing the fano inequality",
      "author" : [ "Te Han", "Sergio Verdu" ],
      "venue" : "Information Theory, IEEE Transactions on",
      "citeRegEx" : "Han and Verdu,? \\Q1994\\E",
      "shortCiteRegEx" : "Han and Verdu",
      "year" : 1994
    }, {
      "title" : "On consistency and sparsity for principal components analysis in high dimensions",
      "author" : [ "I. M Johnstone", "A.Y. Lu" ],
      "venue" : "Journal of the American Statistical Association",
      "citeRegEx" : "Johnstone and Lu,? \\Q2009\\E",
      "shortCiteRegEx" : "Johnstone and Lu",
      "year" : 2009
    }, {
      "title" : "Shifted power method for computing tensor eigenpairs",
      "author" : [ "Tamara G Kolda", "Jackson R Mayo" ],
      "venue" : "SIAM Journal on Matrix Analysis and Applications",
      "citeRegEx" : "Kolda and Mayo,? \\Q2011\\E",
      "shortCiteRegEx" : "Kolda and Mayo",
      "year" : 2011
    }, {
      "title" : "Approximate message passing with consistent parameter estimation and applications to sparse learning",
      "author" : [ "U. Kamilov", "S. Rangan", "A.K. Fletcher", "M. Unser" ],
      "venue" : null,
      "citeRegEx" : "Kamilov et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Kamilov et al\\.",
      "year" : 2012
    }, {
      "title" : "The concentration of measure phenomenon",
      "author" : [ "M. Ledoux" ],
      "venue" : "Mathematical Surveys and Monographs,",
      "citeRegEx" : "Ledoux,? \\Q2001\\E",
      "shortCiteRegEx" : "Ledoux",
      "year" : 2001
    }, {
      "title" : "Tensor completion for estimating missing values in visual data",
      "author" : [ "J. Liu", "P. Musialski", "P. Wonka", "J. Ye" ],
      "venue" : "IEEE Transactions on Pattern Analysis and Machine Intelligence",
      "citeRegEx" : "Liu et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2013
    }, {
      "title" : "Square deal: Lower bounds and improved relaxations for tensor recovery",
      "author" : [ "C. Mu", "J. Huang", "B. Wright", "D. Goldfarb" ],
      "venue" : "International Conference in Machine Learning (ICML),",
      "citeRegEx" : "Mu et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Mu et al\\.",
      "year" : 2013
    }, {
      "title" : "Non-negative principal component analysis: Message passing algorithms and sharp asymptotics",
      "author" : [ "Andrea Montanari", "Emile Richard" ],
      "venue" : null,
      "citeRegEx" : "Montanari and Richard,? \\Q2014\\E",
      "shortCiteRegEx" : "Montanari and Richard",
      "year" : 2014
    }, {
      "title" : "Asymptotics of sample eigenstructure for a large dimensional spiked covariance model",
      "author" : [ "Debashis Paul" ],
      "venue" : "Statistica Sinica",
      "citeRegEx" : "Paul,? \\Q2007\\E",
      "shortCiteRegEx" : "Paul",
      "year" : 2007
    }, {
      "title" : "A new convex relaxation for tensor completion",
      "author" : [ "B. Romera-Paredes", "M. Pontil" ],
      "venue" : "Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "Romera.Paredes and Pontil,? \\Q2013\\E",
      "shortCiteRegEx" : "Romera.Paredes and Pontil",
      "year" : 2013
    }, {
      "title" : "Approximate message passing for bilinear models",
      "author" : [ "P. Schniter", "V. Cevher" ],
      "venue" : "Proc. Workshop Signal Process. Adaptive Sparse Struct. Repr.(SPARS),",
      "citeRegEx" : "Schniter and Cevher,? \\Q2011\\E",
      "shortCiteRegEx" : "Schniter and Cevher",
      "year" : 2011
    }, {
      "title" : "Compressive phase retrieval via generalized approximate message passing, Communication, Control, and Computing (Allerton), 2012",
      "author" : [ "P. Schniter", "S. Rangan" ],
      "venue" : "50th Annual Allerton Conference on,",
      "citeRegEx" : "Schniter and Rangan,? \\Q2012\\E",
      "shortCiteRegEx" : "Schniter and Rangan",
      "year" : 2012
    }, {
      "title" : "Free energy of the spherical mean field model, Probability theory and related fields",
      "author" : [ "Michel Talagrand" ],
      "venue" : null,
      "citeRegEx" : "Talagrand,? \\Q2006\\E",
      "shortCiteRegEx" : "Talagrand",
      "year" : 2006
    }, {
      "title" : "Greed is good: Algorithmic results for sparse approximation, Information Theory",
      "author" : [ "J. A Tropp" ],
      "venue" : "IEEE Transactions on",
      "citeRegEx" : "Tropp,? \\Q2004\\E",
      "shortCiteRegEx" : "Tropp",
      "year" : 2004
    }, {
      "title" : "Statistical performance of convex tensor decomposition",
      "author" : [ "R. Tomioka", "T. Suzuki", "K. Hayashi", "H. Kashima" ],
      "venue" : "Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "Tomioka et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Tomioka et al\\.",
      "year" : 2011
    }, {
      "title" : "Some comparisons for gaussian processes",
      "author" : [ "R.A. Vitale" ],
      "venue" : "Proceedings of the American Mathematical Society",
      "citeRegEx" : "Vitale,? \\Q2000\\E",
      "shortCiteRegEx" : "Vitale",
      "year" : 2000
    }, {
      "title" : "The absolute-value estimate for symmetric multilinear forms, Linear Algebra and its Applications",
      "author" : [ "W.C. Waterhouse" ],
      "venue" : null,
      "citeRegEx" : "Waterhouse,? \\Q1990\\E",
      "shortCiteRegEx" : "Waterhouse",
      "year" : 1990
    }, {
      "title" : "Perturbation bounds in connection with singular value decomposition, BIT Numerical Mathematics",
      "author" : [ "P.A. Wedin" ],
      "venue" : null,
      "citeRegEx" : "Wedin,? \\Q1972\\E",
      "shortCiteRegEx" : "Wedin",
      "year" : 1972
    } ],
    "referenceMentions" : [ ],
    "year" : 2014,
    "abstractText" : "We consider the Principal Component Analysis problem for large tensors of arbitrary order k under a single-spike (or rank-one plus noise) model. On the one hand, we use information theory, and recent results in probability theory, to establish necessary and sufficient conditions under which the principal component can be estimated using unbounded computational resources. It turns out that this is possible as soon as the signal-to-noise ratio β becomes larger than C √ k log k (and in particular β can remain bounded as the problem dimensions increase). On the other hand, we analyze several polynomial-time estimation algorithms, based on tensor unfolding, power iteration and message passing ideas from graphical models. We show that, unless the signal-to-noise ratio diverges in the system dimensions, none of these approaches succeeds. This is possibly related to a fundamental limitation of computationally tractable estimators for this problem. We discuss various initializations for tensor power iteration, and show that a tractable initialization based on the spectrum of the matricized tensor outperforms significantly baseline methods, statistically and computationally. Finally, we consider the case in which additional side information is available about the unknown signal. We characterize the amount of side information that allows the iterative algorithms to converge to a good estimate.",
    "creator" : "LaTeX with hyperref package"
  }
}