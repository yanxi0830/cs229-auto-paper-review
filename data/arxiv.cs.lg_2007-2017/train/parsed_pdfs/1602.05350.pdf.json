{
  "name" : "1602.05350.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Relative Error Embeddings of the Gaussian Kernel Distance",
    "authors" : [ "Di Chen", "Jeff M. Phillips" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n60 2.\n05 35\n0v 2\n[ cs\n.L G\n] 2\n0 Se\np 20\nand have diameter bounded by M, then we show that O((d/ε2) logM) dimensions are sufficient, and that this many are required, up to log(1/ε) factors. We empirically confirm that relative error is indeed preserved for kernel PCA using these approximate feature maps."
    }, {
      "heading" : "1 Introduction",
      "text" : "The kernel trick in machine learning allows for non-linear analysis of data using many techniques such as PCA and SVM which were originally designed for linear analysis. The “trick” is that these procedures only access data through inner products between data points, here we consider this non-linear inner product defined by a kernel K(·, ·). Now given n data points, one can compute the n× n gram matrix G of all pairwise inner products; that is so Gi,j = K(xi, xj) for all xi, xj in input data set X. Then the analysis can proceed using just the gram matrix G.\nHowever, for large data sets, constructing this n × n matrix is a computational bottleneck, so methods have been devised for lifting n data points P ⊂ Rd to a high-dimensional space Rm (but where m ≪ n) so that the Euclidean dot-product in this space approximates the non-linear inner product.\nFor reproducing kernels K, there exists a lifting φ : Rd → HK , where HK is the reproducing kernel Hilbert space. It is in general infinite dimensional, but for every finite subset of points Φ(X) = {φ(x) | x ∈ X} spans an n-dimensional Euclidean space. That is K(x, y) = 〈φ(x), φ(y)〉. Moreover, we can define the norm of a point in HK as ‖φ(x)‖HK = √\n〈φ(x), φ(x)〉 using the inner product, and then due to linearity, a distance (the kernel distance) between two points is defined:\nDK(x, y) = √\n‖φ(x)‖2 HK + ‖φ(y)‖2 HK − 2〈φ(x), φ(y)〉\n= √ K(x, x) +K(y, y)− 2K(x, y). ∗Thanks to support by NSF CCF-1350888, IIS-1251019, ACI-1443046, and CNS-1514520.\nFor reproducing kernels (actually a slightly smaller set called characteristic kernels) this is a metric [17, 13].\nThus we may desire an approximate lifting φ̂ : Rd → Rm such that with probability at least 1− δ and all x, y ∈ X\n(1− ε) ≤ DK(x, y) ‖φ̂(x)− φ̂(y)‖ ≤ (1 + ε).\nIt turns out, one can always construct such a lifting with m = O((1/ε2) log(n/δ)) by the famous Johnson-Lindenstrauss (JL) Lemma [7]. However, unlike the JL Lemma, there is not always known an implicit construction. In general, we must first construct the n × n gram matrix, revealing an n-dimensional subspace (through an O(n3) time eigendecomposition) and then apply m = O((1/ε2) log(n/δ)) random projections.\nSo in recent years there have been many types of kernels considered for these implicit embeddings with various sorts of error analysis, such as for Gaussian kernels [16, 11, 18, 19] group invariant kernels [10], min/intersection kernels [12], dot-product kernels [8], information spaces [1], and polynomial kernels [6, 3].\nIn this document we reanalyze one of the most widely used and first variants, the Random Fourier Features, introduced by [16]. It applies to symmetric shift-invariant kernels which include Laplace, Cauchy, and most notably Gaussian. We will primarily focus on Gaussian kernels, defined K(x, y) = e− ‖x−y‖2\n2σ2 , and use this definition for K henceforth unless otherwise specified. It is characteristic, hence DK is a metric."
    }, {
      "heading" : "1.1 Existing Properties of Gaussian Kernel Embeddings",
      "text" : "[16] defined two approximate embedding functions: φ̃ : Rd → Rm and φ̂ : Rd → Rm. Only the former appears in the final version of paper, but the latter is also commonly used throughout the literature [19]. Both features use random variables ωi ∈ Rd drawn uniformly at random from the Fourier transform of the kernel function; in the case of the Gaussian kernel, the Fourier transform is again a Gaussian specifically ωi ∼ Nd(0, σ−2).\nIn the former case, they define m functions of the form f̃i(x) = cos(ω T i x + γi), where γi ∼ Unif(0, 2π], uniformly at random from the interval (0, 2π], is a random shift. Applying each f̃i on a datapoint x gives the ith coordinate of φ̃(x) in Rm as φ̃(x)i = f̃i(x)/ √ m.\nIn the latter case, they define t = m/2 functions of the form\nf̂i(x) =\n[\ncos(〈ωi, x〉) sin(〈ωi, x〉)\n]\nas a single 2 × 1 dimensional vector, and one feature pair. Then applying f̂i on a data point x yields the (2i)th and (2i+ 1)th coordinate of φ̂(x) in Rm as [φ̂(x)2i; φ̂(x)2i+1] = f̂i(x)/ √ t.\nThey showed that E[φ̃(x)T φ̃(y)] = K(x, y) for any x, y ∈ Rd, and that this implied\nPr[|〈φ̃(x), φ̃(y)〉 −K(x, y)| ≥ ε] ≤ δ\n• with m = O((1/ε2) log(1/δ)) for each x, y ∈ Rd, • with m = O((1/ε2) log(n/δ)), for all x, y ∈ X, for X ⊂ Rd of size n, or • with m = O((d/ε2) log(M/δ)), for all x, y ∈ X, for X ⊂ Rd so M = max\nx,y∈X ‖x− y‖/σ.\nRecently [18] tightened the above asymptotic bounds to show actual constants. It is folklore (apparently removed from final version of [16]; reproved in Section 2) that also E[φ̂(x)T φ̂(y)] = K(x, y), and thus all of the above PAC bounds hold for φ̂ as well.\nAlso recently, [19] compared φ̃ and φ̂ (they used symbol φ̆ in place of our symbol φ̂), and demonstrated that φ̂ performs better (for the same m) and has provably lower variance in approximating K(x, y) with φ̂(x)T φ̂(y) as opposed to with φ̃(x)T φ̃(y). However, these results do not obtain a bound on ‖φ̂(x) − φ̂(y)‖/DK(x, y) since for very small distances, the additive error bounds on K(x, y) are not sufficient to say much about DK(x, y)."
    }, {
      "heading" : "1.2 Our Results",
      "text" : "In this paper we show that φ̂ probabilistically induces a kernel K̂(x, y) = 〈φ̂(x), φ̂(y)〉 and a distance\nDK̂(x, y) =\n√\n‖φ̂(x)‖2 + ‖φ̂(y)‖2 − 2K̂(x, y) = ‖φ̂(x)− φ̂(y)‖,\nwhich has strong relative error bounds with respect to DK(x, y), namely for a parameter ε ∈ (0, 1)\n(1− ε) ≤ DK(x, y) DK̂(x, y) ≤ (1 + ε). (1)\nIn Section 2 we show (1) holds for each x, y such that ‖x − y‖/σ ≥ 1, with probability at least 1− δ, with m = O((1/ε2) log(1/δ)). We also review known or folklore properties about φ̂ and DK .\nWe first prove bounds that depend on the size n of a data set X ⊂ Rd. We show that m = O((1/ε2) log n) features are necessary (Section 3) and sufficient (Section 4) to achieve (1) with high probability (e.g., at least 1− 1/n), when d and X are otherwise unrestricted.\nIn Section 5 we prove bounds for X ⊂ Rd where d is small, but the size n = |X| is unrestricted. Let M = maxx,y∈X ‖x− y‖/σ. We show that m = O((d/ε2) log(dε Mδ )) is sufficient to show (1) with probability 1 − δ. Then in Section 6 we show that m = Ω( d\nε2 log(1/ε) log( Mlog(1/ε))) is necessary for\nany feature map.\nWe also empirically confirm the relative error through some simulations in Section 7. This includes showing that kernel PCA using these approximate features obtains relative error.\nFurther Implications in Machine Learning. There has been extensive recent effort to find oblivious subspace embeddings (OSE) of data sets into Euclidian spaces that preserve relative error. Such strong guarantees are, for instance, required to prove results about regression on the resulting set since we may not know the units on different coordinates; additive error bounds do not make sense in directions which are linear combinations of several coordinates.\nThe obliviousness of the features (they can be defined without seeing the data, and in some cases are independent of the data size) are essential for many large-scale settings such as streaming or distributed computing where we are not able to observe all of the data at once.\nOur results to do not describe unrestricted OSEs, as are possible with polynomial kernels [3]. Rather our lower bounds show that any OSE must have dimension depend on n or M.\nHowever, we show that random feature mappings allow for a finer notion of approximating the geometry of RKHS than previously known. In particular, the approximation error of inner products is proportional to the approximation error of distances. This is because both φ and φ̂ map every input point to a unit vector; thus DK(x, y)\n2 = 2 − 2K(x, y) and DK̂(x, y)2 = 2 − 2K̂(x, y), for any distinct x, y ∈ Rd. Therefore |K(x, y) − K̂(x, y)| is the same as 12 |DK(x, y)2 − DK̂(x, y)2|. Hence approximation error of the Gram matrix is bounded in terms of the sum of errors in pairwise distances\n‖G− Ĝ‖1 ≤ 1\n2\n∑\nx,y∈X |DK(x, y)2 −DK̂(x, y)2|.\nMoreover, with our low-dimensional bounds in Section 5, we can see that if an object U ⊂ Rd (such as a non-linear decision boundary) and training data S ⊂ Rd both lie within a ball with finite radiusM, then for any point x ∈ S, the minimum kernel distance between U and x is approximately preserved in the random feature space as miny∈U ‖φ(x)− φ(y)‖. This suggests better performance guarantees for kernelized learning problems regarding the minimization of ℓ2 distances, such as in kernel SVM (hinge-loss) and in kernel PCA (recovery error); see Section 7."
    }, {
      "heading" : "2 Basic Bounds",
      "text" : "We first review some properties of the Gaussian kernel and the theory of Random Fourier Features. An earlier version of Rahimi-Recht [16] seemed to prove the following lemma. We repeat it for completeness.\nLemma 2.1. Given a random d-dimensional Gaussian ωi ∼ Nd(0, σ−2), and any two x, y ∈ Rd then Eωi [〈f̂i(x), f̂i(y)〉] = K(x, y). Proof. First we expand\n〈f̂i(x), f̂i(y)〉 =cos(〈ωi, x〉) cos(〈ωi, y〉) + sin(〈ωi, x〉) sin(〈ωi, y〉) = cos(〈ωi, x− y〉).\nBy the stability and scaling of the Gaussian distribution, we can write 〈ωi, x − y〉 as ωi,x,y ‖x−y‖σ where ωi,x,y ∼ N (0, 1). Now note\nEωi [〈f̂i(x), f̂i(y)〉] = ∫\nz∈R\n1√ 2π exp(−z2/2)) cos(z ‖x − y‖ σ ) dz.\nWe next show that the right hand side evaluates to K(x, y). Using ∆ = x−yσ , let g(‖∆‖) := ∫\nz∈R exp(−z2/(2)) cos(z‖∆‖)dz. Now with integration by parts,\nd d‖∆‖g(‖∆‖) =− ∫ ∈R e− z2 2 · z sin(z‖∆‖) dz\n=\n∫\nz∈R\n∂\n∂z\n(\ne− z2 2 sin(z‖∆‖) ) − ‖∆‖e− z 2 2 cos(z‖∆‖) dz\n=− ‖∆‖ ∫\nz∈R e−\nz2\n2 cos(z‖∆‖) dz = −‖∆‖g(‖∆‖).\nSo g(‖∆‖) = κe− ‖∆‖ 2 2 for some constant κ. But κ = g(0) = ∫ z∈R e − z2\n2 dz = √ 2π, therefore:\nEωi [〈ψi(x), ψi(y)〉] = √ 2π · 1√\n2π e−\n‖∆‖2 2 = e− ‖x−y‖2 2σ2 = K(x, y).\nWe can then state the next simple corollary.\nCorollary 2.1. The higher dimensional random feature maps φ̂ also satisfy for any x, y ∈ Rd\nEω1,...,ωd [〈φ̂(x), φ̂(y)〉] = K(x, y).\nFor any x, y ∈ Rd and any ωi ∼ N (0, ς) we have that 〈ψi(x), ψi(y)〉 ∈ [−2, 2]. Thus we can apply a standard Chernoff-Hoeffding bound to show the next corollary.\nCorollary 2.2. Given a random feature map φ̂ defined by t = O((1/ε2) log(1/δ)) iid random ddimensional Gaussian ωi ∼ Nd(0, σ−2), for ε ∈ (0, 1/2) and δ ∈ (0, 1), then for any x, y ∈ Rd\nPr\n[ ∣\n∣ ∣ K(x, y)− 〈φ̂(x), φ̂(y)〉\n∣ ∣ ∣ ≤ ε ] ≥ 1− δ.\nSince K̂(x, x, ) = 1, then DK̂(x, y) 2 = 2−2K̂(x, y), and additive error bounds between DK(x, y)2 and DK̂(x, y) 2 follow directly. But we can also state some relative error bounds when ‖x − y‖ is large enough.\nLemma 2.2. For each x, y ∈ Rd such that ‖x − y‖ ≥ σ and m = O((1/ε2) log(1/δ)) with ε ∈ (0, 1/10) and δ ∈ (0, 1). Then with probability at least 1− δ, we have DK(x,y)D K̂ (x,y) ∈ [1− ε, 1 + ε].\nProof. By choosing m = O((1/ε2) log(1/δ)) so that |K(x, y) − K̂(x, y)| ≤ ε/4 (via Lemma 2.1), we have that |D2K(x, y) −D2K̂(x, y)| ≤ ε/2. We also note that when ‖x − y‖ ≥ σ then K(x, y) ≤ exp(−σ2/(2σ2)) = 1√\ne ≤ 0.61. Hence D2K(x, y) ≥ 2(1 − 0.61) = 0.78 ≥ 0.5, and we have that\n|D2K(x, y)−D2K̂(x, y)| ≤ ε/2 ≤ εD 2 K(x, y).\nThen |1 − D 2 K̂ (x,y)\nD2 K (x,y)\n| ≤ ε, implying 1 − ε ≤ D 2 K̂ (x,y)\nD2 K (x,y) ≤ 1 + ε. Taking the square root of all parts completes the proof via √ 1 + ε < (1 + ε) and √ 1− ε > (1− ε).\nSo to understand the relative error in DK(x, y), what remains is the case when ‖x− y‖ is small. As we will see, when ‖x − y‖ is small, then DK(x, y) behaves like ‖x − y‖ and we can borrow insights from ℓ2 embeddings. Then combining the two cases (when ‖x − y‖ is large and when ‖x−y‖ is small) we can achieve “for all bounds” either via simple union bounds, or through careful “continuous net” arguments when X is in a bounded range. Similarly, we will show near-matching lower bounds via appealing to near-ℓ2 properties or via net arguments."
    }, {
      "heading" : "3 Approximations and Relation to ℓ2 on Small Distances",
      "text" : "For the remainder of the paper, it will be convenient to let ∆ = (x − y)/σ be the scaled vector between some pair of points x, y ∈ X. Define DK(∆) = DK(x, y) = √\n2− 2e 12‖∆‖2 . Lemma 2.2 already established that when ‖∆‖ ≥ 1, then additive error bounds imply relative error bounds. In this section we consider the alternate case of when ‖∆‖ ≤ 1, and show that in this setting that DK̂ is indeed close to DK , and to varying degrees also approximates ℓ2.\nAs a warm up, and as was similarly observed in [15], a simple Taylor expansion when ‖∆‖ ≤ 1, implies that\n‖∆‖2 − 1 4 ‖∆‖4 ≤ DK(∆)2 = 2− 2 exp(‖∆‖2/2) ≤ ‖∆‖2,\nand by 14‖∆‖4 ≤ 14‖∆‖2 and a square root\n0.86‖∆‖ ≤ DK(∆) ≤ ‖∆‖. (2)\nMoreover, when ‖∆‖ ≤ 2√ε then\n(1− ε)‖∆‖2 ≤ DK(∆)2 ≤ ‖∆‖2. (3)\nIn what follows we derive more nuanced and powerful bounds relating DK(x, y) and DK̂(x, y); which transitively relates DK̂(x, y) to ‖x− y‖.\nUseful expansions. We first observe that by cos(a) cos(b) + sin(a) sin(b) = cos(a− b) that\n〈f̂i(x), f̂i(y)〉 = cos(〈ωi, x〉) cos(〈ωi, y〉) + sin(〈ωi, x〉) sin(〈ωi, y〉) = cos(〈ωi, (x− y)〉).\nHence by 〈f̂i(x), f̂i(x)〉 = cos(〈ωi, 0) = 1 we have DK̂(x, y)2 = 2− 21t ∑t i cos(〈ωi, (x− y)〉). By the rotational stability of the Gaussian distribution we can replace 〈ωi, (x−y)〉 with ωi,x,y ‖x−y‖σ where ωi,x,y ∼ N (0, 1). It will be more convenient to write ωi,x,y as ωi,∆, so 〈ωi, (x − y)〉 = ωi,∆‖∆‖. Thus 〈f̂i(x), f̂i(y)〉 = cos(ωi,∆‖∆‖). Moreover, we can define DK̂(∆) = DK̂(x, y) = √\n2− 21t ∑t i=1 cos(ωi,∆‖∆‖). Now considering\nDK̂(∆) 2 DK(∆)2 = 1− 1t ∑t i=1 cos(ωi,∆‖∆‖) 1− e 12‖∆‖2 ,\nthe following Taylor expansion, for ωi,∆‖∆‖ ≤ 1, will be extremely useful:\n1 t ∑t i=1 1 2ω 2 i,∆‖∆‖2\n1 2‖∆‖2 − ·14‖∆‖4\n≥ DK̂(∆) 2\nDK(∆)2 ≥\n1 t ∑t i=1\n(\n1 2ω 2 i,∆‖∆‖2 − 124(ω4i,∆‖∆‖4)\n)\n1 2‖∆‖2\n.\nSimplifying gives\n1\n1− 12‖∆‖2\n(\n1\nt\nt ∑\ni=1\nω2i,∆\n)\n≥ DK̂(∆) 2 DK(∆)2 ≥ ( 1 t t ∑\ni=1\nω2i,∆\n)\n− ‖∆‖ 2 12 · 1 t\nt ∑\ni=1\nω4i,∆. (4)\nVery small distances. Next we can understand what happens in the limit as we shrink the region containing X. We do so by using a scaling parameter λ, and observe what happens to the ratio DK̂(λ∆) 2/DK(λ∆) in the limit as λ → 0.\nLemma 3.1. For scalar scaling parameter λ,\nlim λ→0\nDK̂(λ∆) 2 DK(λ∆)2 = 1 t\nt ∑\ni=1\nω2i,∆.\nProof. Observe that ωi,∆ = ωi,λ∆, for any λ > 0. Thus in equation (4), limλ→0 1/(1 − 12‖λ∆‖2) goes to 1 so the left hand-side approaches 1t ∑t i=1 ω 2 i,∆. Similarly, limλ→0 ‖λ∆‖2/12 goes to 0, and the right-hand side also approaches 1t ∑t i=1 ω 2 i,∆.\nIf we fix ∆ then ωi,∆, 1 ≤ i ≤ t are i.i.d Gaussian variables with mean 0 and standard deviation 1. Thus\n∑t i=1 ω 2 i,∆ is a χ 2-variable with t degrees of freedom. This observation paired with Lemma 3.1 will be useful for a lower bound.\nBut to prove an upper bound, we do not need to go all the way to the limit. Common concentration results for χ2 variables give us the following.\nLemma 3.2. For ε ∈ (0, 1) and δ ∈ (0, 1/2), if t ≥ 8 1 ε2 ln(2/δ) then\nPr\n[\n1\nt\nt ∑\ni=1\nω2i,∆ /∈ [1− ε, 1 + ε] ] ≤ δ.\nProof. Here we use Lemma 1 from [4]; if X is a χ2 random variable with t degrees of freedom\nPr[t− 2 √ tx ≤ X ≤ t+ 2 √ tx+ 2x] ≥ 1− 2e−x.\nHere we can set x := 18 tε 2 then t−2\n√ tx = t−εt/ √ 2, and t+2 √ tx+2x = t+εt/ √ 2+ 14ε 2t < t+εt.\nAlso, 2e−x = 2e− 1 8 tε2 = 2e− ln(2/δ) = δ/2 ≤ δ for δ ≤ 1/2. Therefore, 1t ∑t i=1 ω 2 i,∆ /∈ [1− ε, 1 + ε] with probability at most δ.\nThis result bounds to [1 − ε, 1 + ε] for some t = O((1/ε2) log(1/δ), the main terms of equation (4). However, the other parts (‖∆‖2/2 and the term containing ω4i,∆) require a further restriction on ‖∆‖ to be handled directly, as we show in the following. Lemma 3.3. For ε ∈ (0, 1) and δ ∈ (0, 1/2), if ‖∆‖ ≤ √ ε\nlog(1/δ) , and t = Ω( 1 ε2 log(1/δ)), then with\nprobability at least 1−O(δ), for all λ ∈ [0, 1] we have DK̂(λ·∆) 2\nDK(λ·∆)2 ∈ [1− ε, 1 + ε].\nProof. If ω is a standard Gaussian variable, then |ω| ≤ C · q √ log(1/δ) with probability at least 1− δq for any q > 0, for some constant C. This means, if ‖∆‖ ≤ √ ε\nlog(1/δ) then ωi,∆‖∆‖ ≤ √ ε log(1/δ)\nwith probability at least 1−O(δ). Also then ωi,∆‖∆‖ ≤ 1, which satisfies the conditions for (4). This also bounds the term ω4i,∆ ≤ ε 2 log2(1/δ) used in equation (4). In particular, along with ‖∆‖ ≤ √ ε\nlog(1/δ) , this implies\n‖∆‖2 12 · 1 t\nt ∑\ni=1\nω4i,∆ ≤ 1 12 ‖∆‖ · ε\n2\nlog2(1/δ) =\nε\n12\nε\nlog2(1/δ)\n1 ‖∆‖2 ≤ ε 12 .\nThen along with Lemma 3.2 and RHS of (4), we have D K̂ (∆)2\nDK(∆)2 ≥ 1−O(ε).\nSimilarly, Lemma 3.2 and 12‖∆‖2 ≤ ε2 log2(1/δ) imply the LHS of (4) is bounded above by 1+O(ε). Thus, after adjusting constants in t, we have D K̂ (∆)2\nDK(∆)2 ∈ [1− ε, 1 + ε].\nFor D K̂ (λ∆)2\nDK(λ∆)2 ∈ [1− ε, 1+ ε], note that the above analysis still holds if we scale ‖∆‖ to be smaller,\ni.e. as long as λ ∈ [0, 1]. In particular, ωi,∆ is unchanged by the scaling λ."
    }, {
      "heading" : "3.1 Lower Bounds, Based on Very Small ‖x− y‖",
      "text" : "Lemma 3.1 implies that when ‖x− y‖ is small, DK̂(x, y) behaves like a Johnson-Lindenstrauss (JL) random projection of ‖x− y‖, and we can invoke known JL lower bounds.\nIn particular, Lemma 3.1 implies if the input data set X ⊂ Rd is in a sufficiently small neighborhood of zero, the relative error is preserved only when\n∑t i=1 ω 2 i,x,y‖λ(x − y)‖2 ∈ [(1− ǫ)‖λ(x−\ny)‖2, (1 + ǫ)‖λ(x − y)‖2] for all x, y ∈ X, and for all arbitrary λ ∈ R. Which implies for arbitrary x, y ∈ X, and λ ∈ R that\n√ √ √ √ t ∑\ni=1\n|ωi · λ(x− y)|2 =\n√ √ √ √ t ∑\ni=1\nω2i,x,yλ‖x− y‖2 ∈ [(1− ε)λ ‖x− y‖ , (1 + ε)λ ‖x− y‖] .\nThe far left hand side is in fact the norm ‖g(x)− g(y)‖ where g(x) is the vector with coordinates (ω1 · λx, ..., ωt · λx). Thus these are the exact conditions for relative error bounds on embedding ℓ2 via the Johnson-Lindenstrauss transforms, which gives the following.\nLemma 3.4. If for any n, d > 0,X ⊂ Rd s.t. |X| = n, using t(n, ε) random Fourier features, D\nK̂ (x,y)\nDK(x,y) ∈ [1 − ε, 1 + ε] with probability 1 − δ, then there exists a random linear embedding with t(n, ε) projected dimensions preserving the ℓ2-norm for all pairs x, y ∈ S up to relative error with probability at least 1− δ.\nTheorem 3.1. There exists a set of n points X ⊂ Rd so that t = Ω( 1ε2 log n) dimensions are necessary so for any x, y ∈ X that DK̂(x,y)DK(x,y) ∈ [1− ε, 1 + ε].\nProof. A lower bound of Ω( 1 ε2\nlog n) projected dimensions for linear embeddings in ℓ2 is given by e.g. [9].\nSection 6 shows another lower bound for point sets with unbounded n, but that are contained in a ball of bounded radius M and bounded dimension d.\nRemark: A new result of Larsen and Nelson [14] provides a t = Ω( 1ε2 log n) lower bound for even non-linear embeddings of a size n point set in Rd into Rt that preserve distances within (1 ± ε). It holds for any ε ∈ (1/min{n, d}0.4999, 1). Since, there exists an isometric embedding of any set of n points in any RKHS into Rn, then this t = Ω( 1\nε2 log n) lower bound suggests that it applies\nto φ̂ and φ̃ or any other technique, for almost any ε. However, it is not clear that any point set (including the ones used in the strong lower bound proof [14]), can result from an isomorphic (or approximate) embedding of RKHS into Rn. Hence, this new result does not immediately imply our lower bound.\nMoreover, the above proof of Theorem 3.1 retains two points of potential interest. First it holds for a (very slightly) larger range of ε ∈ (0, 1). Second, Lemma 3.4 highlights that at very small ranges, φ̂ is indistinguishable from the standard JL embedding."
    }, {
      "heading" : "4 Relative Error Bounds For Small Distances and Small Data Sets",
      "text" : "The Taylor expansion in equation (4) and additive errors via Lemma 2.1 are only sufficient to provide us bounds for ‖∆‖ ≤ √ε/ log n or for ‖∆‖ ≥ 1. In this section we need to use a more powerful technique or moment generating functions to fill in this gap.\nIn particular, 1 − cos (ωi,∆‖∆‖) is a sub-Gaussian random variable so it is expected to have a good concentration, but this fact is not enough for relative error bounds. We will use a more precise bound of the moment generating function of 1− cos (ωi,∆‖∆‖). Recall that the moment generating function M(s) of a random variable X is given by E[esX ].\nLemma 4.1. For ω ∼ N (0, 1) and 0 ≤ ‖∆‖ ≤ 1, let M(s) be the moment generating function of 1− cos (ω‖∆‖)− ( 1− e− 12‖∆‖2 ) = e− 1 2 ‖∆‖2 − cos (ω‖∆‖). Then for all s ∈ [0, 1\n2‖∆‖2 ),\nlnM(s) ≤ 1 4 s2‖∆‖4.\nProof. First we note two Taylor approximations which hold for all x ∈ R:\ncosx ≥ 1− 1 2 x2 and e−x ≤ 1− x+ 1 2 x2.\nNow\nM(s) = E [ exp ( s(e− 1 2 ‖∆‖2 − cos(ω‖∆‖)) )]\n≤ E [ exp ( s(1− 1 2 ‖∆‖2 + 1 8 ‖∆‖4)− s(1− 1 2 ω2‖∆‖2) )] = E [ exp (\n−s 2 ‖∆‖2 + s 8 ‖∆‖4 + s 2 ω2‖∆‖2 )]\n= exp( s 8 · ‖∆‖4 − s 2 ‖∆‖2) · E [ e−s 1 2 ω2‖∆‖2 ] .\nBut\nE\n[\ne−s 1 2 ω2‖∆‖2\n]\n=\n∫ ∞\n−∞\n1√ 2π e− u2 2 · e−s 12u2‖∆‖2du\n=\n∫ ∞\n−∞\n1√ 2π e− 1 2 u2(1+s‖∆‖2)du\n= 1 √ 1 + s‖∆‖2 ∫ ∞ −∞ √ 1 + s‖∆‖2 1√ 2π e− 1 2 u2(1+s‖∆‖2)du = 1 √\n1 + s‖∆‖2 .\nNoting that ln(1 + x) ≥ x− x22 for x ∈ [0, 12), then whenever s ∈ [0, 12‖∆‖2 ):\nlnM(s) ≤ ln ( exp( s8‖∆‖4 − s2‖∆‖2) √\n1 + s‖∆‖2\n)\n= s 8 ‖∆‖4 − s 2 ‖∆‖2 − 1 2 ln(1 + s‖∆‖2) ≤ s 8 ‖∆‖4 − s 2 ‖∆‖2 − 1 2 (s‖∆‖2 − 1 2 s2‖∆‖4) = s2\n4 ‖∆‖4 − s 8 ‖∆‖4 − s‖∆‖2\n≤ s 2\n4 ‖∆‖4.\nWe next combine this result with an existing bound on sub-exponential random variables [5](Lemma 4.1 in Chapter 1). Let X be a random variable, and let M(s) be the moment generating function of X − E[X]. Let X̄t := 1t ∑t i=1 Xi where X1, ...,Xt are i.i.d. samples of X. If lnM(s) ≤ s 2p 2 for all s ∈ [0, 1q ), then\nP (|X̄t − E[X]| ≥ εE[X]) ≤ 2 exp ( −min ( t ε2E[X]2 2p , t εE[X] 2q )) . (5)\nLemma 4.2. If ‖x − y‖ ≤ σ, m = Ω(ε−2 log 1δ ), then D K̂ (x,y) DK(x,y) ∈ [1 − ε, 1 + ε] with probability at least 1− δ.\nProof. Recall that 〈f̂i(x), f̂i(y)〉 = cos(〈ωi, (x− y)〉) and (1/2)DK̂ (x, y)2 = 1t ∑t i=1(1− cos(〈ωi(x− y)〉)). Then define random variable Xi = (1 − cos(〈ωi(x − y)〉)), and X = 1t ∑t i=1Xi. Since E[X] = E[DK̂(x, y) 2] = DK(x, y) 2, then E[Xi] = 1− exp(−12‖∆‖2).\nFor M(s) the moment generating function of Xi−E[Xi], by Lemma 4.1 we have ln(M(s)) ≤ 12s2p for p = 12‖∆‖4 for s ∈ [0, 1q ] with q = 2‖∆‖2. Also recall by equation (2) we have for any x, y ∈ Rd with ‖x− y‖ ≤ σ, that 0.86 ≤ DK(x,y)‖∆‖ ≤ 1. Plugging these values into equation (5) with t = 6\nε2 ‖∆‖4 DK(x,y)4 ln(2/δ) = O( 1 ε2 log 1δ ), we obtain that\nPr[|DK̂(x, y)−DK(x, y)| ≥ εDK(x, y)] = Pr[|X − E[X]| ≥ εE[X]]\n≤2 exp ( −min ( t ε2E[X]2 2p , t εE[X] 2q ))\n=2exp\n( −min ( t ε2E[X]2 ‖∆‖4 , t εE[X] 4‖∆‖2 )) .\n=2exp\n( −min ( 6, 3\n2ε ‖∆‖2 DK(x, y)2 ) ln 2 δ )\n≤2 exp ( −min ( 6, 1\nε\n)\nln 2\nδ\n)\n≤ δ.\nTogether with Lemma 2.2 (for ‖x− y‖ ≥ σ), we apply a union bound over all (n 2 )\npairs vectors from a set of n vectors.\nTheorem 4.1. For any set X ⊂ Rd of size n, then m = Ω( 1 ε2\nlog n) dimensions are sufficient so D\nK̂ (x,y)\nDK(x,y) ∈ [1− ε, 1 + ε] with high probability (e.g., at least 1− 1/n)."
    }, {
      "heading" : "5 Relative Error Bounds for Low Dimensions and Diameter",
      "text" : "A common approach in subspace embeddings replaces n with the size of a sufficiently fine net. Given a smoothness condition, once the error is bound on the net points, the guarantee is extended to the ‘gaps’ in between. To bound the size of the gaps, we derive the Lipschitz constant of DK̂(·)2, with respect to the vector ∆ (not individual points in Rd).\nAs opposed to previous work regarding the norms of the Euclidean space, the Gaussian kernel distance is non-linear; nonetheless we observe that it is near linear close to 0, and make use of a special construct that allows us to take advantage of this insight.\nLemma 5.1. For any ∆ ∈ Rd, |∇DK̂(∆)2| ≤ ∑t i=1 √ d‖ω2i,∆‖‖∆‖.\nProof. We denote by ω (j) i,∆ the j-th coordinate of ωi,∆.\n∣ ∣∇DK̂(∆)2 ∣ ∣ =\n∣ ∣ ∣ ∣ ∣ ∣ t ∑\ni=1\nd ∑\nj=1\nω (j) i,∆ sin(ωi,∆ ·∆)\n∣ ∣ ∣ ∣ ∣ ∣ ≤ t ∑\ni=1\nd ∑\nj=1\n|ω(j)i,∆| |sin(ωi,∆ ·∆)|\n≤ t ∑\ni=1\n‖ωi,∆‖1|ωi,∆ ·∆| ≤ t ∑\ni=1\n‖ωi,∆‖1‖ωi,∆‖‖∆‖ ≤ t ∑\ni=1\n√ d‖ωi,∆‖2‖∆‖\nCorollary 5.1. For any c ≥ 0, over the region ‖∆‖ ≤ c, the Lipschitz constant of DK̂(∆)2 is bounded above by O(c · t √ d log n) with probability at least 1−O( 1n).\nProof. We can bound ‖ωi,∆‖2 ≤ O(log n) with probability at least 1− 1nq for any large fixed q > 0. So the gradient is bounded by √ d‖∆‖∑ti=1 ‖ωi,∆‖2 ≤ √ d‖∆‖t ·O(log n) ≤ O(c · t √ d log n), which also bounds the Lipschitz constant.\nIn case c = √ ε logn , the Lipschitz constant is O(t √ εd). We then have enough ingredients for the following main result. Intuitively, what separates typical net arguments from ours is the scaling λ in Lemma 3.3; our ‘net’ contains, which we call a λ-urchin, a set of line segments extending from the origin, in addition to discrete points.\nLemma 5.2. If t = Ω( 1 ε2 d log\n(\nd ε 1 δ\n)\n), then with probability at least 1 − δ, for all ∆ such that ‖∆‖ ≤ √ ε\nlogn , then D K̂ (∆)2 DK(∆)2 ∈ [1− ε, 1 + ε].\nProof. The proof will first consider distances ∆ such that {∆ : ‖∆‖ = √ ε\nlogn}, and then generalize to smaller distances using Lemma 3.3 and a construction we call a λ-urchin. Fixed distance case: Consider two points ∆1,∆2 from the surface {∆ : ‖∆‖ = √ ε\nlogn}. If ‖∆1 −∆2‖ ≤ ε 1.5\nt √ d log2 n then Corollary 5.1 implies\n∣ ∣DK̂(∆1) 2 −DK̂(∆2)2 ∣ ∣ ≤ O(t √ εd) · ‖∆1 −∆2‖ ≤ O(t √ εd) · ε 1.5\nt √ d log2 n\n= O\n( ε · ( √ ε\nlog n\n)2 )\n= O(ε) ·DK(∆1)2.\nNow let Γγ be a γ-net over {∆ : ‖∆‖ = √ ε logn} where γ ≤ ε 1.5 t √ d log2 n\n. For any ∆1 ∈ {∆ : ‖∆‖ = √ ε\nlogn}, there exists ∆2 ∈ Γγ such that ‖∆1 −∆2‖ ≤ γ. Then the above implies\n(1−O(ε))DK̂(∆2)2 ≤ DK̂(∆1)2 ≤ (1 +O(ε))DK̂(∆2)2. (6)\nBy the triangle inequality, equation (2), and t √ d log n > 1, we have\n|DK(∆1)−DK(∆2)| ≤ DK(∆1,∆2) ≤ ‖∆1−∆2‖ ≤ γ ≤ ε · √ ε\nlog n · 1 t √ d log n ≤ ε ·O(DK(∆1)). (7)\nWe will choose t = Ω( 1 ε2 log |Γγ |) such that the following holds over Γγ with high probability\n(1−O(ε))DK(∆2)2 ≤ DK̂(∆2)2 ≤ (1 +O(ε))DK(∆2)2. (8)\nThese equations (6), (7), and (8) show, respectively that the ratios D K̂ (∆1)\nD K̂ (∆2)\n, D K̂ (∆2) DK(∆2) , and DK(∆2)DK(∆1)\nare all in [1 +O(ε), 1 −O(ε)]; hence we can conclude\n|DK(∆1)−DK̂(∆1)| ≤ O(ε) ·DK(∆1). (9)\nWhich are in turn 1±O(ε) relative error bounds for the kernel distance, over {∆ : ‖∆‖ = √ ε\nlogn}. All distances case: For the region {∆ : ‖∆‖ < √ ε\nlogn}, consider again Γγ . For each net point p ∈ Γγ we draw a line segment from p to the origin, producing the set of line segments Γ̄γ , that we call the γ-urchin. By Lemma 3.3, and t = Ω( 1\nε2 log |Γγ |), we have relative error bounds for the\nGaussian kernel distance over the γ-urchin.\nNow for any λ ∈ (0, 1), consider the intersection {∆ : ‖∆‖ = λ √ ε\nlogn} ∩ Γ̄γ . We see that the γ-urchin induces a net over {∆ : ‖∆‖ = λ √ ε\nlogn}. Due to scaling we can see that, in fact, it is a (λγ)-net. So the distance between any point in {∆ : ‖∆‖ = λ √ ε\nlogn} and the closest net point is bounded above by λ·ε 1.5\nt √ d log2 n\n. From Corollary 5.1, the Lipschitz constant is now O(tλ √ εd).\nBy arguments similar to those leading to (9) we obtain, for any ∆1 ∈ {∆ : ‖∆‖ = λ √ ε logn}\n|DK(∆1)−DK̂(∆1)| ≤ O(ε) · λ · √ ε\nlog n ≤ O(ε) ·DK(∆1). (10)\nSince this holds for all λ ∈ [0, 1], we obtain relative error bounds over {∆ : ‖∆‖σ ≤ √ ε\nlogn}. The size of Γγ is bounded above by O( ( t √ d logn ε )d ). It is sufficient to set log n = O(d log(d/ε)) and\nthus t = O( 1 ε2 d log(dε 1 δ )) so that relative error holds over the γ-net and the γ-urchin simultaneously, which imply (10) and (9), with probability at least 1− δ.\nCorollary 5.2. If t = Ω( 1ε2d log( d ε 1 δ )), then for all ∆ such that ‖∆‖ ≤ 1,\nD K̂ (∆)\nDK(∆) ∈ [1 − ε, 1 + ε]\nwith probability at least 1− δ. Proof. Consider the region 1 ≥ ‖∆‖ > √ ε logn . The Lipschitz constant is bounded above by O(t √ d log n) by Corollary 5.1, so we only need a γ-net where γ ≤ ε2 t √ d logn to give relative error by standard net arguments. The size of this net is at most ( t √ d logn ε2 )d , so again it suffices to set log n = O(d log(dε )) and t = O( 1ε2d log( d ε 1 δ )) for our embeddings as above.\nCombined with Lemma 2.2 for ‖∆‖ > 1 we obtain:\nTheorem 5.1. If t = Ω ( d ε2 log ( d ε M δ )) , then for any M ≥ 0, DK̂(x,y) 2 DK(x,y)2 ∈ [1− ε, 1 + ε] holds for all x, y ∈ Rd such that ‖x− y‖/σ ≤ M with probability at least 1− δ.\nProof. Set t = Ω( 1 ε2 d log(dε 1 δ )) + Ω( 1 ε2 d log(dε M δ )) = Ω ( d ε2 log ( d ε M δ )) to account for both cases ‖∆‖ = ‖x−y‖σ ≤ 1 and 1 ≤ ‖x−y‖ σ ≤ M, respectively."
    }, {
      "heading" : "6 Lower Bounds for Low Dimensions",
      "text" : "When is n is unbounded, a recent paper [18] implies that, even for small d, DK̂ cannot (1 + ε)approximate DK unless M is bounded. Here we provide an explicit and general lower bound depending on M and d that matches the our upper bound up to a O(log 1ε ) factor.\nFirst we need the following general result ([2] Theorem 9.3) related to embedding to ℓ2. Let B be an n× n real matrix with bi,i = 1 for all i and |bi,j | ≤ ε for all i 6= j. If the rank of B is r, and 1√ n < ε < 1/2, then r ≥ Ω( 1 ε2 log(1/ε) log n). Geometrically, r is the minimum number of dimensions that can contain a set of n near-orthogonal vectors. Indeed, any set S of n near-orthogonal vectors can be rotated to form the rows of a matrix of the form of B, and the rank is then the lowest number of dimensions that contain S.\nLemma 6.1. Given M ≥ 0, let BM(0) be the ball in Rd centered at the origin with radius M. Let h : Rd → Rt be a mapping such that for any x 6= y ∈ BM(0) we have |K(x, y)−h(x) ·h(y)| ≤ ε ≤ 14 . Then with sufficiently large M, t = Ω( dε2 log(1/ε) log( Mlog(1/ε))).\nProof. Consider a subset S ⊂ Rd in BM(0) so for all x, y ∈ S, with x 6= y, we have ‖x − y‖ ≥ σ √\n2 log 1ε . Then for any x, y ∈ S, K(x, y) = exp(− ‖x−y‖2 2σ2 ) ≤ ε. In particular, define S as the inter-\nsection of BM(0) with an orthogonal grid of side length σ √ 2 log(1/ε); it has size Ω\n(\n(\nM log(1/ε)\n)d )\n.\nFor any x, y ∈ S, |h(x) ·h(y)| ≤ 2ε, and also |{h(s) | s ∈ S}| = |S|. Then [2] Theorem 9.3 implies the dimension of h must be t = Ω( 1ε2 log(1/ε) log |S|) = Ω( dε2 log(1/ε) log( Mlog(1/ε))).\nTheorem 6.1. Given M ≥ 0, let BM(0) be the ball in Rd centered at the origin with radius M. Let h : Rd → Rt be a mapping such that for any x, y ∈ BM(0) we have 1 − ε ≤ DK(x,y)‖h(x)−h(y)‖ ≤ 1 + ε with ε ≤ 14 . Restrict that for any x ∈ Rd that ‖h(x)‖ = 1. If M is sufficiently large, t = Ω( dε2 log(1/ε) log( M log(1/ε))).\nProof. Consider a set (as in proof of Lemma 6.1) S ⊂ BM(0). If for all x, y ∈ S we have 1 − ε ≤ DK(x,y)\n‖h(x)−h(y)‖ ≤ 1 + ε, then it implies\n|DK(x, y)2 − ‖h(x) − h(y)‖2| ≤ Θ(ε)DK(x, y)2 ≤ Θ(ε),\nsince DK(x, y) < 2. Expanding DK(x, y) 2 = 2 − 2K(x, y) and ‖h(x) − h(y)‖2 = 2 − 2〈h(x), h(y)〉 implies that |K(x, y)−〈h(x), h(y)〉| ≤ Θ(ε) as well. However, Lemma 6.1 implies that for sufficiently small ε (adjusting the constant in Θ(ε)) that we require the t = Ω( dε2 log(1/ε) log( M log(1/ε))).\nThis implies the impossibility of fully embedding into ℓ2 the Gaussian kernel distance over the entire Rd, i.e. for an infinite number of points, answering a question raised by [18]. This argument can also extend to show a dependency on d logM is inevitable when we do not have a bound on n."
    }, {
      "heading" : "7 Empirical Demonstration of Relative Error",
      "text" : "We demonstrate that relative error actually results from the φ̂ kernel embeddings in two ways. First we demonstrate relative error bounds for kernel PCA. Second we show this explicitly for pairwise distances in the embedding."
    }, {
      "heading" : "7.1 Relative Error for Kernel PCA",
      "text" : "We consider two ways of running kernel PCA on the USPS data. By default we use the first n = 2000 data points in Rd for d = 256, the first n/10 data points of each digit. In the first way, we create the n×n (centered) gram matrix G of all inner products, and then use the top k eigenvectors to describe the best subspace of RKHS to represent the data; this is treated as a baseline. Second we embed each point into Rm using φ̂, generating an n×m matrix Q (after centering). The top k right singular values Vk of Q describe the kernel PCA subspace.\nError in PCA is typically measured as the sum of squared residuals, that is for each point q ∈ Q ⊂ Rm, its projection onto Vk is V Tk Vkq, and its residual is rq = ‖q − V Tk Vkq‖2. Thus rq is precisely the squared kernel distance between q and its projection. And then the full error is R̂k = ‖Q− V Tk VkQ‖2F = ∑\nq∈Q ‖q − V Tk Vkq‖2. For the non-approximate case, it can be calculated as the sum of eigenvalues in the tail Rk = ∑n i=k+1 λi.\nGiven Rk and R̂k we can measure the relative error as R̂k/Rk. Our analysis indicates this should be in [1 − ε, 1 + ε] using roughly t = C/ε2 features, where C depends on n or d logM. To isolate ε we calculate | R̂kRk − 1| averaged over 10 trials in the randomness in φ̂. This is shown in Figure 1 using k = 40, with σ ∈ {4, 8, 16} and varying t ∈ {50, 100, 200, 400, 800}. We observe that our measured error decreases quadratically in t as expected. Moreover, this rate is stable as a function of σ as would be expected where the correct way to quantify error is the relative error we measure.\nPairwise Demonstrations of Relative Error Here we provide simulations that confirm our theoretical findings. We randomly generate pairs of points (x1, y1) . . . (xn, yn) with varying ℓ2 distance ‖xi − yi‖; in particular, xi is a random point in a ball or radius 500 and yi is generated to be a random point in the sphere ‖x − y‖ = ri where r1, ..., rn follow a geometric distribution, ranging from approximately 10−4 to 104.\nIn Figure 2(left), for different values of t (the number of features) we generate a fresh sequence\nof 2000 random pairs, and record the maximum relative error εmax = maxi DK(xi,yi)\n‖φ(xi)−φ(yi)‖ . The graph\nshows that t is roughly proportional to ε−2max. In Figure 2(right), we examine the relative errors for all the random pairs at a wide range of ℓ2 norms, for t = 100 and t = 1000. A slight change in the error profile occurs within ‖xi − yi‖/σ ∈ [100, 101], coinciding with the separation of cases ‖x − y‖ ≤ σ and ‖x − y‖ > σ i.e. whether ‖x−y‖\nσ = Θ(1) in the analyses. In either case, the relative error is bounded by a small constant value, even when ‖xi − yi‖ is several magnitudes smaller than 1, demonstrating that the extremely high concentration of the RFF for very small ‖xi − yi‖ results in relative error approximation for the Gaussian kernel distance.\nConclusion: We demonstrate theoretically and empirically tight relative error for kernel distance using random Fourier features, indicating tighter approximations for important learning applications.\n0 200 400 600 800 1000 Number of Features\n0\n50\n100\n150 200 1/ (M a x im u m R e la t iv e E r r o r )2\n10−4 10−3 10−2 10−1 100 101 102 103 104\nOriginal ℓ2 norm ‖x− y‖\n−0.2\n−0.1\n0.0\n0.1\n0.2\nR e la\nti v e\nE rr\no r:\nD K̂ (x ,y )/ D\nK (x ,y ) − 1\nRelative Error for Small to Large Distances\n#features=1000 #features=100"
    } ],
    "references" : [ {
      "title" : "Sketching, embedding, and dimensionality reduction for information spaces",
      "author" : [ "Amirali Abdullah", "Ravi Kumar", "Andrew McGregor", "Sergei Vassilvitskii", "Suresh Venkatasubramanian" ],
      "venue" : "AIStats,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2016
    }, {
      "title" : "Problems and results in extremal combinatorics-i",
      "author" : [ "Noga Alon" ],
      "venue" : "Discrete Math.,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2003
    }, {
      "title" : "Subspace embeddings for the polynomial kernel",
      "author" : [ "Haim Avron", "Huy L. Nguyen", "David P. Woodruff" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2014
    }, {
      "title" : "Adaptive estimation of a quadratic functional by model selection",
      "author" : [ "P. Massart B. Laurent" ],
      "venue" : "The Annals of Statistics,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2000
    }, {
      "title" : "Metric characterization of random variables and random processes. Translations of mathematical monographs",
      "author" : [ "Valerij Vladimirovič Buldygin", "IU.V. Kozachenko", "V. Zaiats" ],
      "venue" : "Providence, R.I. American Mathematical Society,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2000
    }, {
      "title" : "Compact random feature maps",
      "author" : [ "Raffay Hamid", "Ying Xiao", "Alex Gittens", "Dennis DeCoste" ],
      "venue" : "arXiv preprint arXiv:1312.4626,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2013
    }, {
      "title" : "Extensions of Lipschitz maps into a Hilbert space",
      "author" : [ "William B. Johnson", "Joram Lindenstrauss" ],
      "venue" : "Contemporary Mathematics,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 1984
    }, {
      "title" : "Random feature maps for dot product kernels",
      "author" : [ "Purushottam Kar", "Harish Karnick" ],
      "venue" : "arXiv preprint arXiv:1201.6530,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2012
    }, {
      "title" : "The johnson-lindenstrauss lemma is optimal for linear dimensionality reduction",
      "author" : [ "Kasper Green Larsen", "Jelani Nelson" ],
      "venue" : "CoRR, abs/1411.2404,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2014
    }, {
      "title" : "Random fourier approximations for skewed multiplicative histogram kernels",
      "author" : [ "Fuxin Li", "Catalin Ionescu", "Cristian Sminchisescu" ],
      "venue" : "In Pattern Recognition,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2010
    }, {
      "title" : "Randomized nonlinear component analysis",
      "author" : [ "David Lopez-Paz", "Suvrit Sra", "Alex Smola", "Zoubin Ghahramani", "Bernhard Schölkopf" ],
      "venue" : "arXiv preprint arXiv:1402.0119,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2014
    }, {
      "title" : "Max-margin additive classifiers for detection",
      "author" : [ "Subhransu Maji", "Alexander C Berg" ],
      "venue" : "In Computer Vision,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2009
    }, {
      "title" : "Integral probability metrics and their generating classes of functions",
      "author" : [ "Alfred Müller" ],
      "venue" : "Advances in Applied Probability,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 1997
    }, {
      "title" : "Optimality of the johnson-lindenstrauss lemma",
      "author" : [ "Jelani Nelson", "Kasper Green Larsen" ],
      "venue" : "Technical Report 1609.02094,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2016
    }, {
      "title" : "Geomtric inference on kernel density estimates",
      "author" : [ "Jeff M. Phillips", "Bei Wang", "Yan Zheng" ],
      "venue" : "In SOCG,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2015
    }, {
      "title" : "Random features for large-scale kernel machines",
      "author" : [ "Ali Rahimi", "Benjamin Recht" ],
      "venue" : "In Advances in neural information processing systems,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2007
    }, {
      "title" : "Hilbert space embeddings and metrics on probability measures",
      "author" : [ "Bharath K. Sriperumbudur", "Arthur Gretton", "Kenji Fukumizu", "Bernhard Schölkopf", "Gert R.G. Lanckriet" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2010
    }, {
      "title" : "Optimal rates for random fourier features",
      "author" : [ "Bharath K. Sriperumbudur", "Zoltan Szabo" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2015
    }, {
      "title" : "On the error of random fourier features",
      "author" : [ "Dougal J. Sutherland", "Jeff Schneider" ],
      "venue" : "In UAI,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 16,
      "context" : "For reproducing kernels (actually a slightly smaller set called characteristic kernels) this is a metric [17, 13].",
      "startOffset" : 105,
      "endOffset" : 113
    }, {
      "referenceID" : 12,
      "context" : "For reproducing kernels (actually a slightly smaller set called characteristic kernels) this is a metric [17, 13].",
      "startOffset" : 105,
      "endOffset" : 113
    }, {
      "referenceID" : 6,
      "context" : "It turns out, one can always construct such a lifting with m = O((1/ε2) log(n/δ)) by the famous Johnson-Lindenstrauss (JL) Lemma [7].",
      "startOffset" : 129,
      "endOffset" : 132
    }, {
      "referenceID" : 15,
      "context" : "So in recent years there have been many types of kernels considered for these implicit embeddings with various sorts of error analysis, such as for Gaussian kernels [16, 11, 18, 19] group invariant kernels [10], min/intersection kernels [12], dot-product kernels [8], information spaces [1], and polynomial kernels [6, 3].",
      "startOffset" : 165,
      "endOffset" : 181
    }, {
      "referenceID" : 10,
      "context" : "So in recent years there have been many types of kernels considered for these implicit embeddings with various sorts of error analysis, such as for Gaussian kernels [16, 11, 18, 19] group invariant kernels [10], min/intersection kernels [12], dot-product kernels [8], information spaces [1], and polynomial kernels [6, 3].",
      "startOffset" : 165,
      "endOffset" : 181
    }, {
      "referenceID" : 17,
      "context" : "So in recent years there have been many types of kernels considered for these implicit embeddings with various sorts of error analysis, such as for Gaussian kernels [16, 11, 18, 19] group invariant kernels [10], min/intersection kernels [12], dot-product kernels [8], information spaces [1], and polynomial kernels [6, 3].",
      "startOffset" : 165,
      "endOffset" : 181
    }, {
      "referenceID" : 18,
      "context" : "So in recent years there have been many types of kernels considered for these implicit embeddings with various sorts of error analysis, such as for Gaussian kernels [16, 11, 18, 19] group invariant kernels [10], min/intersection kernels [12], dot-product kernels [8], information spaces [1], and polynomial kernels [6, 3].",
      "startOffset" : 165,
      "endOffset" : 181
    }, {
      "referenceID" : 9,
      "context" : "So in recent years there have been many types of kernels considered for these implicit embeddings with various sorts of error analysis, such as for Gaussian kernels [16, 11, 18, 19] group invariant kernels [10], min/intersection kernels [12], dot-product kernels [8], information spaces [1], and polynomial kernels [6, 3].",
      "startOffset" : 206,
      "endOffset" : 210
    }, {
      "referenceID" : 11,
      "context" : "So in recent years there have been many types of kernels considered for these implicit embeddings with various sorts of error analysis, such as for Gaussian kernels [16, 11, 18, 19] group invariant kernels [10], min/intersection kernels [12], dot-product kernels [8], information spaces [1], and polynomial kernels [6, 3].",
      "startOffset" : 237,
      "endOffset" : 241
    }, {
      "referenceID" : 7,
      "context" : "So in recent years there have been many types of kernels considered for these implicit embeddings with various sorts of error analysis, such as for Gaussian kernels [16, 11, 18, 19] group invariant kernels [10], min/intersection kernels [12], dot-product kernels [8], information spaces [1], and polynomial kernels [6, 3].",
      "startOffset" : 263,
      "endOffset" : 266
    }, {
      "referenceID" : 0,
      "context" : "So in recent years there have been many types of kernels considered for these implicit embeddings with various sorts of error analysis, such as for Gaussian kernels [16, 11, 18, 19] group invariant kernels [10], min/intersection kernels [12], dot-product kernels [8], information spaces [1], and polynomial kernels [6, 3].",
      "startOffset" : 287,
      "endOffset" : 290
    }, {
      "referenceID" : 5,
      "context" : "So in recent years there have been many types of kernels considered for these implicit embeddings with various sorts of error analysis, such as for Gaussian kernels [16, 11, 18, 19] group invariant kernels [10], min/intersection kernels [12], dot-product kernels [8], information spaces [1], and polynomial kernels [6, 3].",
      "startOffset" : 315,
      "endOffset" : 321
    }, {
      "referenceID" : 2,
      "context" : "So in recent years there have been many types of kernels considered for these implicit embeddings with various sorts of error analysis, such as for Gaussian kernels [16, 11, 18, 19] group invariant kernels [10], min/intersection kernels [12], dot-product kernels [8], information spaces [1], and polynomial kernels [6, 3].",
      "startOffset" : 315,
      "endOffset" : 321
    }, {
      "referenceID" : 15,
      "context" : "In this document we reanalyze one of the most widely used and first variants, the Random Fourier Features, introduced by [16].",
      "startOffset" : 121,
      "endOffset" : 125
    }, {
      "referenceID" : 15,
      "context" : "1 Existing Properties of Gaussian Kernel Embeddings [16] defined two approximate embedding functions: φ̃ : Rd → Rm and φ̂ : Rd → Rm.",
      "startOffset" : 52,
      "endOffset" : 56
    }, {
      "referenceID" : 18,
      "context" : "Only the former appears in the final version of paper, but the latter is also commonly used throughout the literature [19].",
      "startOffset" : 118,
      "endOffset" : 122
    }, {
      "referenceID" : 17,
      "context" : "Recently [18] tightened the above asymptotic bounds to show actual constants.",
      "startOffset" : 9,
      "endOffset" : 13
    }, {
      "referenceID" : 15,
      "context" : "It is folklore (apparently removed from final version of [16]; reproved in Section 2) that also E[φ̂(x)T φ̂(y)] = K(x, y), and thus all of the above PAC bounds hold for φ̂ as well.",
      "startOffset" : 57,
      "endOffset" : 61
    }, {
      "referenceID" : 18,
      "context" : "Also recently, [19] compared φ̃ and φ̂ (they used symbol φ̆ in place of our symbol φ̂), and demonstrated that φ̂ performs better (for the same m) and has provably lower variance in approximating K(x, y) with φ̂(x)T φ̂(y) as opposed to with φ̃(x)T φ̃(y).",
      "startOffset" : 15,
      "endOffset" : 19
    }, {
      "referenceID" : 2,
      "context" : "Our results to do not describe unrestricted OSEs, as are possible with polynomial kernels [3].",
      "startOffset" : 90,
      "endOffset" : 93
    }, {
      "referenceID" : 15,
      "context" : "An earlier version of Rahimi-Recht [16] seemed to prove the following lemma.",
      "startOffset" : 35,
      "endOffset" : 39
    }, {
      "referenceID" : 14,
      "context" : "As a warm up, and as was similarly observed in [15], a simple Taylor expansion when ‖∆‖ ≤ 1, implies that ‖∆‖ − 1 4 ‖∆‖ ≤ DK(∆) = 2− 2 exp(‖∆‖/2) ≤ ‖∆‖, and by 1 4‖∆‖ ≤ 4‖∆‖ and a square root 0.",
      "startOffset" : 47,
      "endOffset" : 51
    }, {
      "referenceID" : 3,
      "context" : "Here we use Lemma 1 from [4]; if X is a χ2 random variable with t degrees of freedom Pr[t− 2 √ tx ≤ X ≤ t+ 2 √ tx+ 2x] ≥ 1− 2e−x.",
      "startOffset" : 25,
      "endOffset" : 28
    }, {
      "referenceID" : 0,
      "context" : "For ε ∈ (0, 1) and δ ∈ (0, 1/2), if ‖∆‖ ≤ √ ε log(1/δ) , and t = Ω( 1 ε log(1/δ)), then with probability at least 1−O(δ), for all λ ∈ [0, 1] we have DK̂(λ·∆) 2 DK(λ·∆)2 ∈ [1− ε, 1 + ε].",
      "startOffset" : 134,
      "endOffset" : 140
    }, {
      "referenceID" : 0,
      "context" : "as long as λ ∈ [0, 1].",
      "startOffset" : 15,
      "endOffset" : 21
    }, {
      "referenceID" : 8,
      "context" : "[9].",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 13,
      "context" : "Remark: A new result of Larsen and Nelson [14] provides a t = Ω( 1 ε log n) lower bound for even non-linear embeddings of a size n point set in Rd into Rt that preserve distances within (1 ± ε).",
      "startOffset" : 42,
      "endOffset" : 46
    }, {
      "referenceID" : 13,
      "context" : "However, it is not clear that any point set (including the ones used in the strong lower bound proof [14]), can result from an isomorphic (or approximate) embedding of RKHS into Rn.",
      "startOffset" : 101,
      "endOffset" : 105
    }, {
      "referenceID" : 4,
      "context" : "We next combine this result with an existing bound on sub-exponential random variables [5](Lemma 4.",
      "startOffset" : 87,
      "endOffset" : 90
    }, {
      "referenceID" : 0,
      "context" : "(10) Since this holds for all λ ∈ [0, 1], we obtain relative error bounds over {∆ : ‖∆‖ σ ≤ √ ε logn}.",
      "startOffset" : 34,
      "endOffset" : 40
    }, {
      "referenceID" : 17,
      "context" : "6 Lower Bounds for Low Dimensions When is n is unbounded, a recent paper [18] implies that, even for small d, DK̂ cannot (1 + ε)approximate DK unless M is bounded.",
      "startOffset" : 73,
      "endOffset" : 77
    }, {
      "referenceID" : 1,
      "context" : "First we need the following general result ([2] Theorem 9.",
      "startOffset" : 44,
      "endOffset" : 47
    }, {
      "referenceID" : 1,
      "context" : "Then [2] Theorem 9.",
      "startOffset" : 5,
      "endOffset" : 8
    }, {
      "referenceID" : 17,
      "context" : "for an infinite number of points, answering a question raised by [18].",
      "startOffset" : 65,
      "endOffset" : 69
    } ],
    "year" : 2016,
    "abstractText" : "A reproducing kernel defines an embedding of a data point into an infinite dimensional reproducing kernel Hilbert space (RKHS). The norm in this space describes a distance, which we call the kernel distance. The random Fourier features (of Rahimi and Recht) describe an oblivious approximate mapping into finite dimensional Euclidean space that behaves similar to the RKHS. We show in this paper that for the Gaussian kernel the Euclidean norm between these mapped to features has (1 + ε)-relative error with respect to the kernel distance. When there are n data points, we show that O((1/ε) logn) dimensions of the approximate feature space are sufficient and necessary. Without a bound on n, but when the original points lie in R and have diameter bounded by M, then we show that O((d/ε) logM) dimensions are sufficient, and that this many are required, up to log(1/ε) factors. We empirically confirm that relative error is indeed preserved for kernel PCA using these approximate feature maps.",
    "creator" : "LaTeX with hyperref package"
  }
}