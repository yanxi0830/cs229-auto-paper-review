{
  "name" : "1601.00925.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Complex Decomposition of the Negative Distance Kernel",
    "authors" : [ "Tim vor der Brück", "Steffen Eger", "Alexander Mehler" ],
    "emails" : [ "tim.vorderbrueck@hslu.ch", "steeger@em.uni-frankfurt.de", "amehler@em.uni-frankfurt.de" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Keywords—SVM, kernel function, text categorization"
    }, {
      "heading" : "INTRODUCTION",
      "text" : "An SVM has become a very popular machine learning method for text classification [5]. One reason for its popularity relates to the availability of a wide range of kernels including the linear, polynomial and RBF (Gaussian radial basis function) kernel. This paper derives a decomposition of the quadratic Power Kernel (PK) using complex numbers and applies it in the area of text classification. Our evaluation shows that the NDK produces F-scores which are comparable to those produced by the latter reference kernels while being faster to compute – except for the linear kernel. This evaluation refers to the Dewey Document Classification DDC [11] as the target scheme and compares our NDK-based classifier with two competitors described in [6], [10] and [18], respecstively.\nAn SVM is a method for supervised classification introduced by [17]. It determines a hyperplane that allows for the best possible separation of the input data. (Training) data on the same side of the hyperplane is required to be mapped to the same class label. Furthermore, the margin of the hyperplane that is defined by the vectors located closest to the hyperplane is maximized. The vectors on the margin are called Support Vectors (SV). The decision function dc : Rn → {1, 0,−1}, which maps a vector to its predicted class, is given by: dc(x) := sgn(〈w,x〉 + b) where sgn : R → {1, 0,−1} is the Signum function; the vector w and the constant b are determined by means of SV optimization. If dc(x) = 0 then the vector is located directly on the hyperplane and no decision regarding both classes is possible.\nThe decision function is given in the primal form. It can\nbe converted into the corresponding dual form: dc(x) = sgn( ∑m\nj=1 yjαj〈x,xj〉 + b) where αj and b are constants determined by the SV optimization and m is the number of SVs xj (the input vector has to be compared only with these SVs). The vectors that are located on the wrong side of the hyperplane, that is, the vectors which prevent a perfect fit of SV optimization, are also considered as SVs. Thus, the number of SVs can be quite large. The scalar product, which is used to estimate the similarity of feature vectors, can be generalized to a kernel function. A kernel function K is a similarity function of two vectors such that the matrix of kernel values is symmetric and positive semidefinite. The kernel function only appears in the dual form. The decision function is given as follows: dc(x) := sgn( ∑m\nj=1 yjαjK(x,xj) + b). Let Φ : R n → Rl be\na function that transforms a vector into another vector mostly of higher dimensionality with l ∈ N ∪ {∞}. It is chosen in such a way that the kernel function can be represented by: K(x1,x2) = 〈Φ(x1),Φ(x2)〉. Thus, the decision function in the primal form is given by:\ndc(x) := sgn(〈w,Φ(x)〉 + b) (1)\nNote that Φ is not necessarily uniquely defined. Furthermore, Φ might convert the data into a very high dimensional space. In such cases, the dual form should be used for the optimization process as well as for the classification of previously unseen data. One may think that the primal form is not needed. However, it has one advantage: if the normal vector of the hyperplane is known, a previously unseen vector can be classified just by applying the Signum function, Φ and a scalar multiplication. This is often much faster than computing the kernel function for previously unseen vectors and each SV as required when using the dual form.\nThe most popular kernel is the scalar product, also called the linear kernel. In this case, the transformation function Φ is the identity function: Klin(x1,x2) := 〈x1,x2〉. Another popular kernel function is the RBF (Gaussian Radial Basis Function), given by: Kr(x1,x2) := e−γ||x1−x2|| 2\nwhere γ ∈ R, γ > 0 is a constant that has to be manually specified. This kernel function can be represented by a function Φr that transforms the vector into infinite dimensional space [19]. For reasons of simplicity, assume that x is a vector with only one component. Then the transformation function Φr is given by:\nΦr(x) :=e −γx2[1,\n√\n2γ 1! x,\n√\n(2γ)2\n2! x2,\n√\n(2γ)3\n3! x3, . . .]⊤ (2)\nAnother often used kernel is the polynomial kernel:\nKp(x1,x2) := (a〈x1,x2〉+ c)d (3) For d := 2, a = 1, c = 1, and two vector components, the function Φp is given by [9]: Φp : R2 → R6 with\nΦp(x) = (1, x 2 1, √ 2x1x2, x 2 2, √ 2x1, √ 2x2) (4)\nIn general, a vector of dimension n is transformed by Φp into a vector of dimension (\nn+d d\n)\n. Thus, the polynomial kernel leads to a large number of dimensions in the target space. However, the number of dimensions is not infinite as in the case of the RBF kernel."
    }, {
      "heading" : "THE POWER KERNEL",
      "text" : "The PK is a conditionally positive definite kernel given by\nKs(x1,x2) := −||x1 − x2||p (5) for some p ∈ R [16], [2], [12]. A kernel is called conditionally positive-definite if it is symmetric and satisfies the conditions [14]\nn ∑\nj,k=1\ncicjK(xj ,xk) ≥ 0 ∀ci ∈ K with m ∑\ni=1\nci = 0 (6)\nwhere cj is the complex-conjugate of cj . We consider here a generalized form of the PK for p := 2 (also called NDK):\nKpow(x1,x2) := −a||x1 − x2||2 + c (7) with a, c ∈ R and a > 0. The expression −a||x1 − x2||2 + c can also be written as:\n− a〈(x1 − x2), (x1 − x2)〉+ c = − a(〈x1,x1〉 − 2〈x1,x2〉+ 〈x2,x2〉) + c\n(8)\nFor deciding which class a previously unseen vector belongs to we can use the decision function in the dual form:\ndc(x) : = sgn( m ∑\nj=1\nyjαjKpow (x,xj) + b)\n= sgn(\nm ∑\nj=1\nyjαj(−a||x− xj ||2 + c) + b) (9)\nThe decision function shown in formula (9) has the drawback that the previously unseen vector has to be compared with each SV, which can be quite time consuming. This can be avoided, if we reformulate formula (9) to:\ndc(x) = sgn( m ∑\nj=1\nyjαj(−a〈x,x〉+ 2a〈x,xj〉−\na〈xj ,xj〉+ c) + b)\n= sgn(−a m ∑\nj=1\nyjαj〈x,x〉 + 2a m ∑\nj=1\nyjαj〈x,xj〉+\nm ∑\nj=1\nyjαj(−a〈xj ,xj〉+ c) + b)\n= sgn(−a〈x,x〉 m ∑\nj=1\nyjαj + 2〈x, a m ∑\nj=1\nyjαjxj〉−\na\nm ∑\nj=1\nyjαj〈xj ,xj〉+ c m ∑\ni=1\nyjαj + b)\n(10)\nWith z := a ∑m j=1 yjαjxj , u := a ∑m j=1 yjαj〈xj ,xj〉 and c′ = c ∑m\ni=1 yjαj , formula (10) can be rewritten as:\ndc(x) = sgn(−a〈x,x〉 m ∑\ni=1\nyjαj +2〈x, z〉 −u+ c′ + b) (11)\nThe expressions u, z, ( ∑m i=1 yjαj), and c ′ are identical for every vector x and can be precomputed. Note that there exists no primal form for the NDK based on real number vectors which is stated by the following proposition. Proposition 1 Let a,c ∈ R with a > 0 and n, l ∈ N. Then there is no function Φre : Rn → Rl (re indicates that Φre operates on real numbers) with ∀x1,x2 ∈ Rn : 〈Φre(x1),Φre(x2)〉 = −a||x1 − x2||2 + c .\nProof: If such a function existed, then, for all x ∈ Rn, ||Φre(x)||2 = 〈Φre(x),Φre(x)〉 = −a · 0 + c = c\nwhich requires that c ≥ 0 since the square of a real number cannot be negative. Now, consider x,y ∈ Rn with ||x−y||2 > 2c\na ≥ 0. On the one hand, we have, by the Cauchy-\nSchwarz inequality,\n|〈Φre(x),Φre(y)〉| ≤ ||Φre(x)|| · ||Φre(y)|| = √ c · √ c = c.\nOn the other hand, it holds that\n| − a||x− y||2 + c| = |a||x− y||2 − c| > 2c− c = c, a contradiction.\nAlthough no primal form and therefore no function Φre exists for real number vectors, such a function can be given if complex number vectors are used instead. In this case, the function Φc is defined with a real domain and a complex codomain: Φc : Rn → C4n+1 and\nΦc(x) :=( √ a(x21 − 1), √ ai, √ 2ax1, √ aix21, . . . ,√\na(x2n − 1), √ ai, √ 2axn, √ aix2n, √ c)⊤\n(12)\nNote that no scalar product can be defined for complex numbers that fulfills the usual conditions of bilinearity and positive-definiteness simultaneously1. Thus, the bilinearity condition is dropped for the official definition and only sesquilinearity is required. The standard scalar product is defined as the sum of the products of the vector components with the associated complex conjugated vector components of the other vector. Let x1,x2 ∈ Cn, then the scalar product is given by [1]: 〈x1,x2〉 := ∑n\nk=1 x1kx2k. In contrast, we use a modified scalar product (marked by a “∗”) where, analogously to the real vector definition, the products of the vector components are summated: 〈∗x1,x2〉 := ∑n\nk=1 x1kx2k. This product (not a scalar product in strict mathematical sense) is a bilinear form but no longer positive definite. For real\n1This can be verified by a simple calculation: Consider some vector x 6= 0 and x ∈ Cn. Since 〈.〉 is positive definite: 〈x,x〉 > 0, by bilinearity: 〈 √ −ix, √ −ix〉 = −i〈x,x〉 6> 0).\nnumber vectors this modified scalar product is identical to the usual definition. With this modified scalar product we get\n〈∗Φc(x1),Φc(x2)〉 =− ax211 − ax221 + 2ax11x21 − · · · − ax21n− ax22n + 2ax1nx2n + c = −a||x1 − x2||2 + c\n(13)\nwhich is just the result of the NDK. The optimization can be done with the dual form. Thus, no complex number optimization is necessary. For the decision on the class to which a data vector should be assigned we switch to the primal form. The vector w ∈ C4n+1 is calculated by: w := ∑m\nj=1 αjyjΦc(xj) for all SVs xj ∈ Rn. The decision function is then given by: dc(x) := sgn(〈∗w,Φc(x)〉 + b). Note that the modified scalar product 〈∗w,Φc(x)〉 must be a real number. This is stated in the following proposition. Proposition 2 Let w = ∑m\nj=1 αjyjΦc(xj) with xj ∈ Rn, αj ∈ R, yj ∈ {−1, 1}, j = 1, . . . ,m,Φc as defined in formula (12) and x ∈ Rn. Then 〈∗w,Φc(x)〉 is a real number.\nProof: 〈∗w,Φc(x)〉 is given by:\n〈∗w,Φc(x)〉 = 〈∗ m ∑\nj=1\nαjyjΦc(xj),Φc(x)〉\n=\nm ∑\nj=1\n〈∗αjyjΦc(xj),Φc(x)〉 (〈∗.〉 is bilinear)\n=\nm ∑\nj=1\nαjyj〈∗Φc(xj),Φc(x)〉\n=\nm ∑\nj=1\nαjyj(−a||xj − x||2 + c) (see form. (13))\n(14)\nwhich is clearly a real number.\nThe NDK is related to the polynomial kernel since it can also be represented by a polynomial. However, it has the advantage over the polynomial kernel that it is faster to compute, since the number of dimensions in the target space grows only linearly and not exponentially with the number of dimensions in the original space [16], [2]. It remains to show that the decision functions following the primal and dual form are also equivalent for the modified form of the scalar product. This is stated in the follow proposition: Proposition 3 Let x,x1, . . . ,xm ∈ Rn, α ∈ Rm, y ∈ {−1, 1}m, w := ∑m\nj=1 αjyjΦc(xj) and 〈∗Φc(z1),Φc(z2)〉 = K(z1, z2) ∀z1, z2 ∈ Rn. Then sgn(〈∗w,Φc(x)〉 + b) = sgn( ∑m j=1 αjyjK(x,xj) + b).\nProof:\nsgn(〈∗w,Φc(x)〉 + b)\n= sgn(〈∗ m ∑\nj=1\nαjyjΦc(xj),Φc(x)〉 + b)\n= sgn( m ∑\nj=1\n〈∗αjyjΦc(xj),Φc(x)〉 + b) (〈∗.〉 is bilinear)\n= sgn(\nm ∑\nj=1\nαjyj〈∗Φc(xj),Φc(x)〉 + b)\n(15)\n=sgn(\nm ∑\nj=1\nαjyjK(xj ,x) + b)\n= sgn(\nm ∑\nj=1\nαjyjK(x,xj) + b) (K is symmetric)\n(16)\nNormally, feature vectors for document classification represent the weighted occurrences of lemma or word forms [15]. Thus, such a vector contains a large number of zeros and is therefore usually considered sparse. In this case, the computational complexity of the scalar product can be reduced from O(n) (where n is the number of vector components) to some constant runtime, which is the average number of nonzero vector components. Let I1 be the set of indices of nonzero entries of vector x1 (I1 = {k ∈ {1, . . . , n} : x1k 6= 0}) and I2 be analogously defined for vector x2. The scalar product of both vectors can then be computed by ∑\nk∈(I1∩I2) x1k ·x2k.\nLet us now consider the case that both vectors are transformed to complex numbers before the scalar multiplication. In this case, the modified scalar product\n〈∗Φc(x1),Φc(x2)〉 = 〈(φ(x11), . . . , φ(x1n)), (φ(x21), . . . , φ(x2n))〉+ c\n(17)\nis considered where φ(xk) denotes the transformation of a single real vector component to a complex number vector and is defined as:\nφ :R → C4, φ(xk) := ( √ a(x2k − 1), √ ai, √ 2axk, √ aix2k)\n(18)\nNote that the partial modified scalar product 〈∗φ(x1k), φ(x2k)〉 can be non-zero, if at least one of the two vector components x1k and x2k is non-zero, which is easy to see:\n〈∗φ(xk), φ(0)〉 = √ a(x2k − 1) · √ a(−1) + (−1)a\n+ √ 2axk · 0 + √ aix2k · 0 = a− ax2k − a = −ax2k\n(19)\nOnly if both vector components are zero, one can be sure that the result is also zero:\n〈∗φ(0), φ(0)〉 = (− √ a)(− √ a)+ai·i+0+0 = a−a = 0 (20)\nThus, the sparse data handling of two transformed complex vectors has to be modified in such a way that vector components associated with the union and not the intersection of non-zero indices are considered for multiplication:\n〈∗x1,x2〉 = ∑\nk∈(I1∪I2)\n〈∗φ(x1k), φ(x2k)〉+ c (21)\nA further advantage of the NDK is that the Mahalanobis distance [8] can easily be integrated, which is shown in the remainder of this section. Each symmetric matrix A can be represented by the product V−1DV where D is a diagonal matrix with the eigenvalues of A on its diagonals. The square root of a matrix A is then defined as√ A := V−1D0.5V where D0.5 is the matrix with the square root of the eigenvalues of A on its diagonal. It is obvious\nthat √ A · √ A = A. Proposition 4: Let x, z ∈ Rn, a := 1, c := 0, then 〈∗Φc( √ Cov−1x),Φc( √ Cov−1z)〉 = −MH (x, z)2 where MH (x, z) denotes the Mahalanobis distance √\n(x− z)TCov−1(x− z) and Cov denotes the covariance matrix between the feature values of the entire training data set.\nProof: We have that (with C := √ Cov−1):\n〈∗Φc(Cx),Φc(Cz)〉 =− ||Cx−Cz||2 (see formula (13)) =− (Cx−Cz)⊤(Cx−Cz) =− (C(x − z))⊤(C(x− z))\n=− (x− z)⊤ √ Cov−1 ⊤√\nCov−1(x− z) =− (x− z)⊤Cov−1(x− z)\n(22)\n(since the covariance matrix is symmetric and the inverse and square root of a symmetric matrix is also symmetric)\nThis proposition shows that the NDK is just the negative square of the Mahalanobis distance for some vectors x̂ and ẑ if x̂ is set to √ Cov−1x (analogously for ẑ), a is set to 1 and c is set to 0. Furthermore, this proposition shows that we can easily extend our primal form to integrate the Mahalanobis distance. For that, we define a function τ : Rn → Rn as follows: τ(x) = √ Cov−1x. With Φm:=Φc ◦ τ we have:\n〈∗Φm(x),Φm(z)〉 = (MH (x, z))2 (23) which shows that we have indeed derived a primal form.\nWe use the NDK for text classification where a text is automatically labeled regarding its main topics (i.e., categories), employing the DDC as one of the leading classification schemes in digital libraries [11]. Documents are classified regarding the 10 top-level categories of the DDC. To this end, we use training sets of documents for which the DDC categories have already been assigned. Lexical features are extracted from this training set and made input to an SVM library (i.e., libsvm [3]) in order to assign one or more DDC categories to previously unseen texts. Features of instance documents are computed by means of the geometric mean of the tfidf values and the GSS coefficients of their lexical constituents [4], [13]."
    }, {
      "heading" : "EVALUATION",
      "text" : "The evaluation and training was done using 4 000 German documents, containing in total 114 887 606 words and 9 643 022 sentences requiring a storage space of 823.51 MB. There are 400 documents of each top-level DDC category in the corpus. 2 000 of the texts were used for training, 2 000 for evaluation. The corpus consists of texts annotated according to the OAI (Open Archive Initiative) and is presented in [7]. We tested the correctness of category assignment for the 10 toplevel DDC categories. Each document is mapped at least to one DDC category. Multiple assignments are possible. Precision, recall and F-scores were computed for each DDC category using the NDK, the square (polynomial kernel of degree 2),\nthe cubic (polynomial kernel of degree 3), the RBF kernel and the linear kernel (see Tables I and II). The free parameters of the square, the cubic and the RBF kernel are determined by means of a grid search on an independent data set. On the same held-out dataset, we adjusted the SVM-threshold parameter b to optimize the F-scores. The time (on an Intel Core i7) required to obtain the classifications was determined (see Table III for the real / complex NDK, the RBF, square, and linear kernel). The time needed for building the model was not measured because of being required only once and therefore being irrelevant for online text classification.\nThe F-score for the NDK is higher than the F-scores of all other kernels and faster to compute except for the linear kernel – obviously, the classification using the primal form of the linear kernel is faster than the one using the NDK. Furthermore, the complex decomposition of the NDK leads to a considerable acceleration of the classification process compared to its dual form and should therefore be preferred.\nWe conducted a second experiment where we compared the linear kernel with the NDK now using text snippets (i.e., abstracts instead of full texts) taken once more from our OAI corpus. This application scenario is more realistic for digital libraries that mainly harvest documents by means of their meta data and, therefore, should also be addressed by text classifiers. The kernels were trained on a set of 19 000 abstracts2. This time, the categories are not treated independently of each other. Instead, the classifier selected all categories with signed distance values from the SVM hyperplane that are greater or equal to zero. If all distance values are negative, the category with the largest (smallest absolute) distance was chosen. In this experiment, the linear kernel performed better than the NDK. Using three samples of 500 texts, the F-scores of the linear kernel are (macro-averaged over all main 10 DDC categories): 0.753, 0.735, 0.743, and of the NDK: 0.730, 0.731, 0.737. This result indicates that the heuristic of preferring the categories of highest signed distance to the hyperplane is not optimal for the NDK. We compared our classifier with two other systems, the DDC classifier of [18] and of [6]. [18] reaches an F-score of 0.502, 0.457 and 0.450 on these three samples. The Fscores of the DDC-classifier of [6] are 0.615, 0.604, and 0.574. Obviously, our classifier outperforms these competitors.\nFinally, we evaluated the NDK on the Reuters 21578 corpus3 (Lewis split). This test set is very challenging since 35 of all 93 categories occurring in this split have 10 or less training examples. Furthermore, several texts are intentionally assigned to none of the Reuters categories. We created an artificial category for all texts that are not assigned in this sense. The histogram of category assignments is displayed in Figure 1. The F-scores for the Reuters corpus are given in Table IV. We modified the training data by filling up instances for every category by randomly selecting positive instances such that the ratio of positive examples to all instances are the same for all categories. Hence, in most category samples some training examples were used more than once. This approach prevents from preferring categories due to their multitude of training examples. The F-score of the NDK is lower than the\n2Note that in this evaluation we employed the tfidf score for weighting only, since the use of the GSS coefficient didn’t lead to any improvement in F-score.\n3URL: http://www.daviddlewis.com/resources/testcollections/reuters21578\nhighest macro-averaging F-score obtained by the linear kernel, but outperforms the square and the cubic kernel. Again, the parameters of the NDK, the square, the cubic and the RBF kernels were determined by means of a grid search."
    }, {
      "heading" : "CONCLUSION AND FUTURE WORK",
      "text" : "We derived a primal form of the NDK by means of complex numbers. In this way, we obtained a much simpler representation compared to the modified dual form. We showed that the primal form (and in principle also the modified dual form) can speed up text classification considerably. The reason is that it does not require to compare input vectors with all support vectors. Our evaluation showed that the F-scores of the NDK are competitive regarding all other kernels tested here while the NDK consumes less time than the polynomial and the RBF kernel. We have also shown that the NDK performs better than the linear kernel when using full texts rather than text snippets. Whether this is due to problems of feature selection/expansion or a general characteristic of this kernel (in the sense of being negatively affected by ultra-sparse features spaces), will be examined in future work. Additionally, we plan to examine the PK with exponents larger than two, to investigate under which prerequisites the PK performs well and to evaluate the extension of the NDK that includes the Mahalanobis distance."
    }, {
      "heading" : "ACKNOWLEDGEMENT",
      "text" : "We thank Vincent Esche and Tom Kollmar for valuable comments and suggestions for improving our paper."
    } ],
    "references" : [ {
      "title" : "Exploring efficient kernel functions for support vector machine based feasability models for analog circuits",
      "author" : [ "Shri D. Boolchandani", "Vineet Sahula" ],
      "venue" : "International Journal of Design Analysis and Tools for Circuits and Systems,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2011
    }, {
      "title" : "LIBSVM: a library for support vector machines",
      "author" : [ "Chih-Chung Chang", "Chih-Jen Lin" ],
      "venue" : null,
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2001
    }, {
      "title" : "The Maximum Margin Approach to Learning Text Classifiers: Methods, Theory, and Algorithms",
      "author" : [ "Thorsten Joachims" ],
      "venue" : "PhD thesis, Universität Dortmund, Informatik, LS VIII,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2000
    }, {
      "title" : "Learning to classify text using support vector machines",
      "author" : [ "Thorsten Joachims" ],
      "venue" : null,
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2002
    }, {
      "title" : "Talk: Automatische Sacherschließung elektronischer Dokumente nach DDC",
      "author" : [ "Mathias Lösch" ],
      "venue" : null,
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2011
    }, {
      "title" : "Building a DDC-annotated corpus from OAI metadata",
      "author" : [ "Mathias Lösch", "Ulli Waltinger", "Wolfram Horstmann", "Alexander Mehler" ],
      "venue" : "Journal of Digital Information,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2011
    }, {
      "title" : "On the generalised distance in statistics",
      "author" : [ "Prasanta Chandra Mahalanobis" ],
      "venue" : "In Proceedings of the National Institute of Sciences of India,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 1936
    }, {
      "title" : "Introduction to Information Retrieval",
      "author" : [ "Christopher Manning", "Prabhakar Raghavan", "Hinrich Schütze" ],
      "venue" : null,
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2008
    }, {
      "title" : "Enhancing document modeling by means of open topic models: Crossing the frontier of classification schemes in digital libraries by example of the DDC",
      "author" : [ "Alexander Mehler", "Ulli Waltinger" ],
      "venue" : "Library Hi Tech,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2009
    }, {
      "title" : "Scale-invariance of support vector machines based on the triangular kernel",
      "author" : [ "Hichem Sahbi", "Francois Fleuret" ],
      "venue" : "Technical Report 4601,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2002
    }, {
      "title" : "Term-weighting approaches in automatic text retrieval",
      "author" : [ "Gerard Salton", "Christopher Buckley" ],
      "venue" : "Information Processing & Management,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 1998
    }, {
      "title" : "Learning with Kernels - Support Vector Machines, Regularization, Optimization and Beyond",
      "author" : [ "Bernhard Schölkopf", "Alexander J. Smola" ],
      "venue" : null,
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2002
    }, {
      "title" : "Machine learning in automated text categorization",
      "author" : [ "Fabrizio Sebastiani" ],
      "venue" : "ACM Comput. Surv.,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2002
    }, {
      "title" : "Kernel functions for machine learning",
      "author" : [ "César Souza" ],
      "venue" : null,
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2010
    }, {
      "title" : "Statistical Learning Theory",
      "author" : [ "Vladimir Vapnik" ],
      "venue" : null,
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 1998
    }, {
      "title" : "Hierarchical classification of oai metadata using the DDC taxonomy. In Advanced Language Technologies for Digital Libraries, Lecture Notes in Computer Science, pages 29–40",
      "author" : [ "Ulli Waltinger", "Alexander Mehler", "Mathias Lösch", "Wolfram Horstmann" ],
      "venue" : null,
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2011
    }, {
      "title" : "Support vector machine tutorial",
      "author" : [ "Shih-Hung Wu" ],
      "venue" : null,
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2012
    } ],
    "referenceMentions" : [ {
      "referenceID" : 3,
      "context" : "An SVM has become a very popular machine learning method for text classification [5].",
      "startOffset" : 81,
      "endOffset" : 84
    }, {
      "referenceID" : 4,
      "context" : "This evaluation refers to the Dewey Document Classification DDC [11] as the target scheme and compares our NDK-based classifier with two competitors described in [6], [10] and [18], respecstively.",
      "startOffset" : 162,
      "endOffset" : 165
    }, {
      "referenceID" : 8,
      "context" : "This evaluation refers to the Dewey Document Classification DDC [11] as the target scheme and compares our NDK-based classifier with two competitors described in [6], [10] and [18], respecstively.",
      "startOffset" : 167,
      "endOffset" : 171
    }, {
      "referenceID" : 15,
      "context" : "This evaluation refers to the Dewey Document Classification DDC [11] as the target scheme and compares our NDK-based classifier with two competitors described in [6], [10] and [18], respecstively.",
      "startOffset" : 176,
      "endOffset" : 180
    }, {
      "referenceID" : 14,
      "context" : "An SVM is a method for supervised classification introduced by [17].",
      "startOffset" : 63,
      "endOffset" : 67
    }, {
      "referenceID" : 16,
      "context" : "This kernel function can be represented by a function Φr that transforms the vector into infinite dimensional space [19].",
      "startOffset" : 116,
      "endOffset" : 120
    }, {
      "referenceID" : 7,
      "context" : "Kp(x1,x2) := (a〈x1,x2〉+ c) (3) For d := 2, a = 1, c = 1, and two vector components, the function Φp is given by [9]: Φp : R → R with Φp(x) = (1, x 2 1, √ 2x1x2, x 2 2, √ 2x1, √ 2x2) (4)",
      "startOffset" : 112,
      "endOffset" : 115
    }, {
      "referenceID" : 13,
      "context" : "THE POWER KERNEL The PK is a conditionally positive definite kernel given by Ks(x1,x2) := −||x1 − x2|| (5) for some p ∈ R [16], [2], [12].",
      "startOffset" : 122,
      "endOffset" : 126
    }, {
      "referenceID" : 0,
      "context" : "THE POWER KERNEL The PK is a conditionally positive definite kernel given by Ks(x1,x2) := −||x1 − x2|| (5) for some p ∈ R [16], [2], [12].",
      "startOffset" : 128,
      "endOffset" : 131
    }, {
      "referenceID" : 9,
      "context" : "THE POWER KERNEL The PK is a conditionally positive definite kernel given by Ks(x1,x2) := −||x1 − x2|| (5) for some p ∈ R [16], [2], [12].",
      "startOffset" : 133,
      "endOffset" : 137
    }, {
      "referenceID" : 11,
      "context" : "A kernel is called conditionally positive-definite if it is symmetric and satisfies the conditions [14]",
      "startOffset" : 99,
      "endOffset" : 103
    }, {
      "referenceID" : 13,
      "context" : "However, it has the advantage over the polynomial kernel that it is faster to compute, since the number of dimensions in the target space grows only linearly and not exponentially with the number of dimensions in the original space [16], [2].",
      "startOffset" : 232,
      "endOffset" : 236
    }, {
      "referenceID" : 0,
      "context" : "However, it has the advantage over the polynomial kernel that it is faster to compute, since the number of dimensions in the target space grows only linearly and not exponentially with the number of dimensions in the original space [16], [2].",
      "startOffset" : 238,
      "endOffset" : 241
    }, {
      "referenceID" : 12,
      "context" : "Normally, feature vectors for document classification represent the weighted occurrences of lemma or word forms [15].",
      "startOffset" : 112,
      "endOffset" : 116
    }, {
      "referenceID" : 6,
      "context" : "A further advantage of the NDK is that the Mahalanobis distance [8] can easily be integrated, which is shown in the remainder of this section.",
      "startOffset" : 64,
      "endOffset" : 67
    }, {
      "referenceID" : 1,
      "context" : ", libsvm [3]) in order to assign one or more DDC categories to previously unseen texts.",
      "startOffset" : 9,
      "endOffset" : 12
    }, {
      "referenceID" : 2,
      "context" : "Features of instance documents are computed by means of the geometric mean of the tfidf values and the GSS coefficients of their lexical constituents [4], [13].",
      "startOffset" : 150,
      "endOffset" : 153
    }, {
      "referenceID" : 10,
      "context" : "Features of instance documents are computed by means of the geometric mean of the tfidf values and the GSS coefficients of their lexical constituents [4], [13].",
      "startOffset" : 155,
      "endOffset" : 159
    }, {
      "referenceID" : 5,
      "context" : "The corpus consists of texts annotated according to the OAI (Open Archive Initiative) and is presented in [7].",
      "startOffset" : 106,
      "endOffset" : 109
    }, {
      "referenceID" : 15,
      "context" : "We compared our classifier with two other systems, the DDC classifier of [18] and of [6].",
      "startOffset" : 73,
      "endOffset" : 77
    }, {
      "referenceID" : 4,
      "context" : "We compared our classifier with two other systems, the DDC classifier of [18] and of [6].",
      "startOffset" : 85,
      "endOffset" : 88
    }, {
      "referenceID" : 15,
      "context" : "[18] reaches an F-score of 0.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 4,
      "context" : "The Fscores of the DDC-classifier of [6] are 0.",
      "startOffset" : 37,
      "endOffset" : 40
    }, {
      "referenceID" : 5,
      "context" : "F-SCORES OF DIFFERENT KERNELS EVALUATED BY MEANS OF THE OAI CORPUS OF [7].",
      "startOffset" : 70,
      "endOffset" : 73
    } ],
    "year" : 2016,
    "abstractText" : "A Support Vector Machine (SVM) has become a very popular machine learning method for text classification. One reason for this relates to the range of existing kernels which allow for classifying data that is not linearly separable. The linear, polynomial and RBF (Gaussian Radial Basis Function) kernel are commonly used and serve as a basis of comparison in our study. We show how to derive the primal form of the quadratic Power Kernel (PK) – also called the Negative Euclidean Distance Kernel (NDK) – by means of complex numbers. We exemplify the NDK in the framework of text categorization using the Dewey Document Classification (DDC) as the target scheme. Our evaluation shows that the power kernel produces F-scores that are comparable to the reference kernels, but is – except for the linear kernel – faster to compute. Finally, we show how to extend the NDK-approach by including the Mahalanobis distance. Keywords—SVM, kernel function, text categorization",
    "creator" : "LaTeX with hyperref package"
  }
}