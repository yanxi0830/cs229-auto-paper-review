{
  "name" : "1501.00358.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Comprehend DeepWalk as Matrix Factorization",
    "authors" : [ "Cheng Yang", "Zhiyuan Liu" ],
    "emails" : [ "cheng-ya14@mails.tsinghua.edu.cn", "liuzy@tsinghua.edu.cn" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n50 1.\n00 35\n8v 1\n[ cs\n.L G\n] 2\nJ an\n2 01"
    }, {
      "heading" : "1 Notation",
      "text" : "Network G = (V,E). Node-context set D is generated from random walk, where each piece of D is a node-context pair (v, c). V is the set of nodes and VC is the set of context nodes. In most cases, V = VC .\nConsider a node-context pair (v, c): #(v, c) denotes the number of times (v, c) appears inD. #(v) = ∑ c′∈VC\n#(v, c′) and #(c) = ∑ v′∈V #(v\n′, c) denotes the number of times v and c appears in D. Note that |D| = ∑ v′∈V ∑ c′∈VC #(v′, c′).\nDeepWalk algorithm embeds a node v into a d-dimension vector −→v ∈ Rd. Also, a context node c ∈ VC is represented by a d-dimension vector\n−→c ∈ Rd. Let W be a |V | × d matrix where row i is vector −→vi and H be a |VC | × d matrix where row j is vector −→cj . Our goal is to figure out a matrix M = WH T ."
    }, {
      "heading" : "2 Proof",
      "text" : "Perozzi et al. implemented DeepWalk algorithm with Skip-Gram and Hierarchical Softmax model. Note that Hierarchical Softmax [5] [4] is a variant of softmax for speeding the training time. In this section, we give proofs for both Negative Sampling and softmax with Skip-Gram model."
    }, {
      "heading" : "2.1 Negative Sampling",
      "text" : "Negative Sampling approximately maximizes the probability of softmax function by randomly choosing k negative samples from context set. Levy and\nGoldberg showed that Skip-Gram with Negative Sampling model(SGNS) is implicitly factorizing a word-context matrix [1] by assuming that dimensionality d is sufficiently large. In other words, we can assign each product −→v · −→c a value independently of the others.\nIn SGNS model, we have\nP ((v, c) ∈ D) = σ(−→v · −→c ) = 1\n1 + e− −→v ·−→c\nSuppose we choose k negative samples for each node-context pair (v, c) according to the distribution PD(cN ) = #(cN) |D| . Then the objective function for SGNS can be written as\nl = ∑\nv∈V\n∑\nc∈VC\n#(v, c)(log σ(−→v · −→c ) + k · EcN∼PD [log σ(− −→v · −→c )])\n= ∑\nv∈V\n∑\nc∈VC\n#(v, c)(log σ(−→v · −→c ) + k · ∑\nv∈V\n#(v) ∑\ncN∈VC\n#(cN )\n|D| log σ(−−→v · −→c )\n= ∑\nv∈V\n∑\nc∈VC\n(#(v, c)(log σ(−→v · −→c ) + k ·#(v) · #(c)\n|D| log σ(−−→v · −→c )\nDenote x = −→v · −→c . By solving ∂l∂x = 0, we have\n−→v · −→c = x = log #(v, c) · |D|\n#(v) ·#(c) − log k\nThus we have Mij = log #(vi,cj) |D|\n#(vi) |D| · #(cj) |D|\n− log k. Mij can be interpreted as Point-\nwise Mutual Information(PMI) of node-context pair (vi, cj) shifted by log k."
    }, {
      "heading" : "2.2 Softmax",
      "text" : "Since both Negative Sampling and Hierarchical Softmax are variants of softmax, we pay more attention to softmax model and give a further discussion in next section. We also assume that the values of −→v · −→c are independent.\nIn softmax model,\nP ((v, c) ∈ D) = e −→v ·−→c\n∑ c′∈VC e −→v · −→ c′\nAnd the objective function is\nl = ∑\nv∈V\n∑\nc∈VC\n#(v, c) · log e −→v ·−→c\n∑ c′∈VC e −→v · −→ c′\nAfter extracting all terms associated to −→v · −→c as l(v, c), we have\nl(v, c) = #(v, c) log e −→v ·−→c\n∑ c′∈VC ,c′ 6=c e −→v · −→ c′ + e −→v ·−→c +\n∑\nc̃∈VC ,c̃6=c\n#(v, c̃) log e −→v ·\n−→ c̃\n∑ c′∈VC ,c′ 6=c e −→v · −→ c′ + e −→v ·−→c\nNote that l = 1|VC | ∑ v∈V ∑ c∈VC l(v, c). Denote x = −→v · −→c . By solving ∂l∂x = 0 for all such x, we have\n−→v · −→c = x = log #(v, c)\n#(v) + bv\nwhere bv can be any real constant since it will be canceled when we compute P ((v, c) ∈ D). Thus we have Mij = log #(vi,cj) #(vi)\n+ bvi . We will discuss what Mij represents in next section."
    }, {
      "heading" : "3 Discussion",
      "text" : "It is clear that the method of sampling node-context pairs will affect matrix\nM . In this section, we will discuss #(v)|D| , #(c) |D| and #(v,c) #(v) based on an ideal sampling method for DeepWalk algorithm. Assume the graph is connected and undirected and window size is t. We can easily generalize this sampling method to directed graph by only adding (RWi, RWj) into D.\nAlgorithm 1 Ideal node-context pair sampling algorithm\nGenerate an infinite long random walk RW . Denote RWi as the node on position i of RW , where i = 0, 1, 2, . . . for i = 0, 1, 2, . . . do for j ∈ [i+ 1, i+ t] do add (RWi, RWj) into D add (RWj , RWi) into D\nend for\nend for\nEach appearance of node i will be recorded 2t times in D for undirected graph and t times for directed graph. Thus we can figure out that #(vi)|D| is the frequency of vi appears in the random walk, which is exactly the PageRank value of vi. Also note that #(vi,vj) #(vi)/2t is the expectation times that vj is observed in left/right t neighbors of vi. Denote the transition matrix in PageRank algorithm be A. More formally, let di be the degree of node i. Aij = 1 di\nif (i, j) ∈ E and Aij = 0 otherwise. We use ei to denote a |V |-dimension row vector where all entries are zero except the i-th entry is 1.\nSuppose that we start a random walk from node i and use ei to denote the initial state. Then eiA is the distribution over all the nodes where j-th entry is the probability that node i walks to node j. Hence j-th entry of eiA\nt is the probability that node i walks to node j at exactly t steps. Thus [ei(A + A\n2 + · · ·+At)]j is the expectation times that vj appears in right t neighbors of vi.\nHence\n#(vi, vj) #(vi)/2t = 2[ei(A+A 2 + · · ·+At)]j\n#(vi, vj)\n#(vi) = [ei(A+A 2 + · · ·+At)]j t\nThis equality also holds for directed graph. By setting bvi = log 2t for all i, Mij = log #(vi,vj) #(vi)/2t is logarithm of the expec-\ntation times that vj appears in left/right t neighbors of vi.\nBy setting bvi = 0 for all i, Mij = log #(vi,vj) #(vi) = log [ei(A+A 2+···+At)]j t is logarithm of the average probability that node i randomly walks to node j in t steps."
    } ],
    "references" : [ {
      "title" : "Neural word embedding as implicit matrix factorization",
      "author" : [ "O. Levy", "Y. Goldberg" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2014
    }, {
      "title" : "Efficient estimation of word representations in vector space",
      "author" : [ "T. Mikolov", "K. Chen", "G. Corrado", "J. Dean" ],
      "venue" : "arXiv preprint arXiv:1301.3781,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2013
    }, {
      "title" : "Distributed representations of words and phrases and their compositionality",
      "author" : [ "T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2013
    }, {
      "title" : "A scalable hierarchical distributed language model",
      "author" : [ "A. Mnih", "G.E. Hinton" ],
      "venue" : "In Advances in neural information processing systems,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2009
    }, {
      "title" : "Hierarchical probabilistic neural network language model",
      "author" : [ "F. Morin", "Y. Bengio" ],
      "venue" : "In AISTATS,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2005
    }, {
      "title" : "Deepwalk: Online learning of social representations",
      "author" : [ "B. Perozzi", "R. Al-Rfou", "S. Skiena" ],
      "venue" : "arXiv preprint arXiv:1403.6652,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2014
    } ],
    "referenceMentions" : [ {
      "referenceID" : 1,
      "context" : "Abstract Word2vec [2], as an efficient tool for learning vector representation of words has shown its effectiveness in many natural language processing tasks.",
      "startOffset" : 18,
      "endOffset" : 21
    }, {
      "referenceID" : 2,
      "context" : "issued Skip-Gram and Negative Sampling model [3] for developing this toolbox.",
      "startOffset" : 45,
      "endOffset" : 48
    }, {
      "referenceID" : 5,
      "context" : "introduced the Skip-Gram model into the study of social network for the first time, and designed an algorithm named DeepWalk [6] for learning node embedding on a graph.",
      "startOffset" : 125,
      "endOffset" : 128
    }, {
      "referenceID" : 4,
      "context" : "Note that Hierarchical Softmax [5] [4] is a variant of softmax for speeding the training time.",
      "startOffset" : 31,
      "endOffset" : 34
    }, {
      "referenceID" : 3,
      "context" : "Note that Hierarchical Softmax [5] [4] is a variant of softmax for speeding the training time.",
      "startOffset" : 35,
      "endOffset" : 38
    }, {
      "referenceID" : 0,
      "context" : "Goldberg showed that Skip-Gram with Negative Sampling model(SGNS) is implicitly factorizing a word-context matrix [1] by assuming that dimensionality d is sufficiently large.",
      "startOffset" : 114,
      "endOffset" : 117
    } ],
    "year" : 2015,
    "abstractText" : "Word2vec [2], as an efficient tool for learning vector representation of words has shown its effectiveness in many natural language processing tasks. Mikolov et al. issued Skip-Gram and Negative Sampling model [3] for developing this toolbox. Perozzi et al. introduced the Skip-Gram model into the study of social network for the first time, and designed an algorithm named DeepWalk [6] for learning node embedding on a graph. We prove that the DeepWalk algorithm is actually factoring a matrix M where each entry Mij is logarithm of the average probability that node i randomly walks to node j in fix steps. We will explain it in section 3. 1 Notation Network G = (V,E). Node-context set D is generated from random walk, where each piece of D is a node-context pair (v, c). V is the set of nodes and VC is the set of context nodes. In most cases, V = VC . Consider a node-context pair (v, c): #(v, c) denotes the number of times (v, c) appears inD. #(v) = ∑ c∈VC #(v, c) and #(c) = ∑ v′∈V #(v , c) denotes the number of times v and c appears in D. Note that |D| = ∑ v′∈V ∑ c∈VC #(v, c). DeepWalk algorithm embeds a node v into a d-dimension vector −→v ∈ R. Also, a context node c ∈ VC is represented by a d-dimension vector −→c ∈ R. Let W be a |V | × d matrix where row i is vector vi and H be a |VC | × d matrix where row j is vector cj . Our goal is to figure out a matrix M = WH T . 2 Proof Perozzi et al. implemented DeepWalk algorithm with Skip-Gram and Hierarchical Softmax model. Note that Hierarchical Softmax [5] [4] is a variant of softmax for speeding the training time. In this section, we give proofs for both Negative Sampling and softmax with Skip-Gram model. 2.1 Negative Sampling Negative Sampling approximately maximizes the probability of softmax function by randomly choosing k negative samples from context set. Levy and",
    "creator" : "LaTeX with hyperref package"
  }
}