{
  "name" : "1702.06914.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ "Colin Raffel" ],
    "emails" : [ "craffel@gmail.com,", "dieterichl@google.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "We describe a mechanism for subsampling sequences and show how to compute its expected output so that it can be trained with standard backpropagation. We test this approach on a simple toy problem and discuss its shortcomings."
    }, {
      "heading" : "1 SUBSAMPLING SEQUENCES",
      "text" : "Consider a mechanism which, given a sequence of vectors s = {s0, s1, . . . , sT−1}, st ∈ Rd, produces a sequence of “sampling probabilities” e = {e0, e1, . . . , eT−1}, et ∈ [0, 1] which denote the probability of including st in the output sequence y = {y0, y1, . . . , yU−1}. Producing y from s and e is encapsulated by the following pseudo-code and visualized in fig. 2 (appendix):\n# Initialize y as an empty sequence y = [] for t in {0, 1, ..., T - 1}:\n# Draw a random number in [0, 1] and compare to e[t] if rand() < e[t]:\n# Add s[t] to y with probability e[t] y.append(s[t])\nWe call this a “subsampling mechanism”, because by construction, U ≤ T , and each element of y is drawn directly from s. The ability to subsample a sequence has various applications:\n• When the input sequence s is oversampled (i.e. each element st contains much the same information as st−1), subsampling can be an effective way of shortening the sequence without discarding useful information. Using a shorter sequence can facilitate the use of recurrent network models, which have difficulties with long-term dependencies (Bengio et al., 1994; Hochreiter & Schmidhuber, 1997). Simple subsampling schemes such as choosing every other element of s have proven effective in tasks such as speech recognition (Chan et al., 2015).\n• The mechanism can be used in sequence transduction tasks where the output sequence is shorter than the input. We contrast this approach with the commonly used Connectionist Temporal Classification loss (Graves et al., 2006) because subsampling actually shortens the sequence (instead of inserting blanks) and can be inserted arbitrarily into a neural network model (instead of specifically being a loss function). It also implicitly produces a monotonic alignment between elements in s and y; such alignments have proven to be useful (Bahdanau et al., 2014).\n• Applying this subsampling operation multiple times could build a hierarchy of shorter and shorter sequences which capture structure at different scales. A similar approach was recently shown to be effective in langauge modeling tasks (Chung et al., 2016).\nMotivated by these applications, in this extended abstract we present a method for training this subsampling mechanism in expectation, i.e. without sampling. We then test this approach on a simple toy problem and study the resulting model’s behavior. Finally, we discuss shortcomings of our approach and possibilities for future work.\n∗Work done as members of the Google Brain Residency program\nar X\niv :1\n70 2.\n06 91\n4v 3\n[ cs\n.L G\n] 8\nA pr\n2 01\n7"
    }, {
      "heading" : "2 TRAINING IN EXPECTATION",
      "text" : "We are interested in including the mechanism defined in the previous section in the midst of a neural network model. However, the sampling process used to construct y precludes the use of standard backpropagation. A common approach to this issue is to optimize the model according to the expected (or mean-field) output (Graves, 2016; Bahdanau et al., 2014). The following analysis shows how to employ this approach to our proposed subsampling mechanism using a dynamic program which analytically computes p(ym = sn).\nFirst, observe that p(y0 = s0) = e0, i.e. the probability that the first output is the first entry in the sequence is just the probability of sampling at time 0. Next, in order for y0 = s1, we need y0 6= s0 so p(y0 = s1) is the probability that s0 was not sampled at time 0 and that s1 was, giving p(y0 = s1) = e1(1 − e0). Continuing on in this way, we see that p(y0 = sn) = en ∏n−1 i=0 (1 − ei) or, in words, the probability that the first output element y0 is a given element in the sequence sn is the probability that none of s0, . . . , sn−1 were sampled multiplied by the probability of sampling sn. Second, observe that p(ym = sn) = 0 when n < m because in order for the output sequence to be of length m, at least m − 1 symbols must already have been sampled. If n < m, this relation is violated. Finally, in order for ym = sn in general, we must have that ym−1 = sj ∈ s0, . . . , sn−1 (i.e. the previous output must be one of the states before sn), none of sj+1, . . . , sn−1 may be sampled at time m, and sn is sampled at time m. To compute p(ym = sn), we need to sum over all of the the possible cases ym−1 ∈ {s0, . . . , sn−1}. The probability of a single case is the combined probability that sn is sampled, that ym−1 = sj , and that none of sj+1, . . . , sn−1 are sampled at time m. We visualize these possibilities in fig. 3 (appendix). Summing over the possible j yields\np(ym = sn) = en n−1∑ j=0 p(ym−1 = sj) n−1∏ i=j+1 (1− ei)  (1) where for convenience we define the special case ∏m i=n • = 1 when n > m. Once we com-\npute p(ym = sn), it is straightforward to find the expected value of ym simply by computing∑ n snp(ym = sn). Note p(ym = sn) = en((1−en−1)p(ym = sn−1)/en−1+p(ym−1 = sn−1)); it follows that each term p(ym = sn) can be computed in O(1) time by reusing the already-computed terms p(ym = sn−1) and p(ym−1 = sn−1). The resulting dynamic program allows all the terms p(ym = sn) to be computed in O(T 2) time. Note that ∑ n p(ym = sn) ≤ 1 depending on the values of e, so these probabilities may not form a valid probability distribution. Computing the expectation as-is without further normalization effectively associates any additional probability to an implicit zero vector in Rd, which is the convention we will use for the remainder of this extended abstract."
    }, {
      "heading" : "3 TOY PROBLEM EXPERIMENT",
      "text" : "To evaluate the feasibility of this approach, we tested it on the following toy problem: Consider a length-T sequence x of symbols [0, 1, 2] which occur with equal probability. The output is produced as follows for t ∈ {0, . . . , T − 1}, beginning with an empty memory:\n1. If xt is 0, don’t output anything and maintain the current memory state. 2. If xt is 1 or 2 and our memory is empty, place xt in memory and don’t output anything. 3. If xt is 1 and we have 1 in our memory, output a 0 and empty the memory. 4. If xt is 2 and we have 2 in our memory, output a 0 and empty the memory. 5. If xt is 1 and we have 2 in our memory, output a 2 and empty the memory. 6. If xt is 2 and we have 1 in our memory, output a 1 and empty the memory.\nWe also define special cases where if T = 1, the output is x0; if xt = 0 ∀ t ∈ {0, . . . , T − 1}, the output is 0; and if all entries of xt are 0 except one, the output is the single nonzero entry. An example input-output pair for this toy problem is shown in fig. 4 (appendix).\nWe utilized the following model:\nst = LSTM(xt, st−1) (2)\net = σ(W > hest−1 +W > xext−1 + be) (3)\nyt = softmax ( W>y T−1∑ n=0 p(yt = sn)sn + by ) (4)\nwhere xt ∈ R3 is the one-hot encoding of the input sequence, LSTM is a long short-term memory RNN (Hochreiter & Schmidhuber, 1997) with state dimensionality 100, Whe ∈ R100×1,Wxe ∈ R3×1, be ∈ R are the weight matrices and bias scalar for computing emission probabilities, σ(·) is the logistic sigmoid function, and Wy ∈ R100×3, by ∈ R3 are the weight matrix and bias vector of the output softmax function. The p(yt = sn) terms are computed as described in section 2.\nWe fed minibatches of 100 sequences of randomly chosen [0, 1, 2] values, encoded as one-hot vectors, to the network. The network was trained with categorical cross-entropy against analytically computed targets using Adam with the learning hyperparameters suggested in (Kingma & Ba, 2015). We computed the network’s accuracy on a separately generated test set that it was not trained on. As proposed in (Zaremba & Sutskever, 2014), we found it beneficial to use a simple curriculum learning (Bengio et al., 2009) strategy where the loss was only computed for the first T ′ elements of the output sequence, where T ′ was uniformly sampled from the values {1, 2, . . . , T} for each minibatch.\nFor all values of T we tried (up to T = 500), the network was able to achieve > 98% accuracy on the held-out test set after training for a modest number of minibatches (around 10,000). To get a picture of the qualitative behavior of the model, we plot the matrix p(yt = sn) for an example test sequence with T = 50 in fig. 1. Note that emissions do not occur exactly when the model has seen sufficient input to produce them, i.e. once it sees a second nonzero input. In this particular case, this caused the model to emit one too few symbols. To facilitate further research, we provide a TensorFlow implementation of our approach.1\nWhile we have shown that our model can quickly learn the desired behavior on a toy problem, we had issues applying this approach to real-world problems, which we attribute primarily to two factors: First, while a stated goal of the subsampling mechanism is to produce shorter sequences, the O(T 2) complexity of computing the terms p(ym = sn) precludes its practical use on problems with large T . Second, the use of a sigmoid in eq. (4) and the cumulative product in eq. (1) can result in vanishing gradients in practice. The first issue could be mitigated by greedy approximations to the procedure outlined in section 2, for example by selecting which items in st are chosen using discrete latent variables and training with reinforcement learning methods as has been done in recent work (Luo et al., 2016). We hope the encouraging results and analysis presented here inspires future work on utilizing learnable subsampling mechanisms in neural networks.\n1https://github.com/craffel/subsampling_in_expectation"
    }, {
      "heading" : "A FIGURES",
      "text" : "In this appendix we provide additional figures to help illustrate some of the concepts presented in this extended abstract."
    } ],
    "references" : [ {
      "title" : "Neural machine translation by jointly learning to align and translate",
      "author" : [ "Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio" ],
      "venue" : "arXiv preprint arXiv:1409.0473,",
      "citeRegEx" : "Bahdanau et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Bahdanau et al\\.",
      "year" : 2014
    }, {
      "title" : "Learning long-term dependencies with gradient descent is difficult",
      "author" : [ "Yoshua Bengio", "Patrice Simard", "Paolo Frasconi" ],
      "venue" : "IEEE Transactions on Neural Networks,",
      "citeRegEx" : "Bengio et al\\.,? \\Q1994\\E",
      "shortCiteRegEx" : "Bengio et al\\.",
      "year" : 1994
    }, {
      "title" : "Curriculum learning",
      "author" : [ "Yoshua Bengio", "Jérôme Louradour", "Ronan Collobert", "Jason Weston" ],
      "venue" : "In Proceedings of the 26th International Conference on Machine Learning,",
      "citeRegEx" : "Bengio et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Bengio et al\\.",
      "year" : 2009
    }, {
      "title" : "Listen, attend and spell",
      "author" : [ "William Chan", "Navdeep Jaitly", "Quoc V. Le", "Oriol Vinyals" ],
      "venue" : "arXiv preprint arXiv:1508.01211,",
      "citeRegEx" : "Chan et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Chan et al\\.",
      "year" : 2015
    }, {
      "title" : "Hierarchical multiscale recurrent neural networks",
      "author" : [ "Junyoung Chung", "Sungjin Ahn", "Yoshua Bengio" ],
      "venue" : "arXiv preprint arXiv:1609.01704,",
      "citeRegEx" : "Chung et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Chung et al\\.",
      "year" : 2016
    }, {
      "title" : "Adaptive computation time for recurrent neural networks",
      "author" : [ "Alex Graves" ],
      "venue" : "arXiv preprint arXiv:1603.08983,",
      "citeRegEx" : "Graves.,? \\Q2016\\E",
      "shortCiteRegEx" : "Graves.",
      "year" : 2016
    }, {
      "title" : "Connectionist temporal classification: Labelling unsegmented sequence data with recurrent neural networks",
      "author" : [ "Alex Graves", "Santiago Fernández", "Faustino Gomez", "Jürgen Schmidhuber" ],
      "venue" : "In Proceedings of the 23rd International Conference on Machine learning,",
      "citeRegEx" : "Graves et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Graves et al\\.",
      "year" : 2006
    }, {
      "title" : "Long short-term memory",
      "author" : [ "Sepp Hochreiter", "Jürgen Schmidhuber" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "Hochreiter and Schmidhuber.,? \\Q1997\\E",
      "shortCiteRegEx" : "Hochreiter and Schmidhuber.",
      "year" : 1997
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Diederik P. Kingma", "Jimmy Ba" ],
      "venue" : "In Proceedings of the 3rd International Conference on Learning Representations,",
      "citeRegEx" : "Kingma and Ba.,? \\Q2015\\E",
      "shortCiteRegEx" : "Kingma and Ba.",
      "year" : 2015
    }, {
      "title" : "Learning online alignments with continuous rewards policy gradient",
      "author" : [ "Yuping Luo", "Chung-Cheng Chiu", "Navdeep Jaitly", "Ilya Sutskever" ],
      "venue" : "arXiv preprint arXiv:1608.01281,",
      "citeRegEx" : "Luo et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Luo et al\\.",
      "year" : 2016
    }, {
      "title" : "Learning to execute",
      "author" : [ "Wojciech Zaremba", "Ilya Sutskever" ],
      "venue" : "arXiv preprint arXiv:1410.4615,",
      "citeRegEx" : "Zaremba and Sutskever.,? \\Q2014\\E",
      "shortCiteRegEx" : "Zaremba and Sutskever.",
      "year" : 2014
    } ],
    "referenceMentions" : [ {
      "referenceID" : 1,
      "context" : "Using a shorter sequence can facilitate the use of recurrent network models, which have difficulties with long-term dependencies (Bengio et al., 1994; Hochreiter & Schmidhuber, 1997).",
      "startOffset" : 129,
      "endOffset" : 182
    }, {
      "referenceID" : 3,
      "context" : "Simple subsampling schemes such as choosing every other element of s have proven effective in tasks such as speech recognition (Chan et al., 2015).",
      "startOffset" : 127,
      "endOffset" : 146
    }, {
      "referenceID" : 6,
      "context" : "We contrast this approach with the commonly used Connectionist Temporal Classification loss (Graves et al., 2006) because subsampling actually shortens the sequence (instead of inserting blanks) and can be inserted arbitrarily into a neural network model (instead of specifically being a loss function).",
      "startOffset" : 92,
      "endOffset" : 113
    }, {
      "referenceID" : 0,
      "context" : "It also implicitly produces a monotonic alignment between elements in s and y; such alignments have proven to be useful (Bahdanau et al., 2014).",
      "startOffset" : 120,
      "endOffset" : 143
    }, {
      "referenceID" : 4,
      "context" : "A similar approach was recently shown to be effective in langauge modeling tasks (Chung et al., 2016).",
      "startOffset" : 81,
      "endOffset" : 101
    }, {
      "referenceID" : 5,
      "context" : "A common approach to this issue is to optimize the model according to the expected (or mean-field) output (Graves, 2016; Bahdanau et al., 2014).",
      "startOffset" : 106,
      "endOffset" : 143
    }, {
      "referenceID" : 0,
      "context" : "A common approach to this issue is to optimize the model according to the expected (or mean-field) output (Graves, 2016; Bahdanau et al., 2014).",
      "startOffset" : 106,
      "endOffset" : 143
    }, {
      "referenceID" : 2,
      "context" : "As proposed in (Zaremba & Sutskever, 2014), we found it beneficial to use a simple curriculum learning (Bengio et al., 2009) strategy where the loss was only computed for the first T ′ elements of the output sequence, where T ′ was uniformly sampled from the values {1, 2, .",
      "startOffset" : 103,
      "endOffset" : 124
    }, {
      "referenceID" : 9,
      "context" : "The first issue could be mitigated by greedy approximations to the procedure outlined in section 2, for example by selecting which items in st are chosen using discrete latent variables and training with reinforcement learning methods as has been done in recent work (Luo et al., 2016).",
      "startOffset" : 267,
      "endOffset" : 285
    } ],
    "year" : 2017,
    "abstractText" : "We describe a mechanism for subsampling sequences and show how to compute its expected output so that it can be trained with standard backpropagation. We test this approach on a simple toy problem and discuss its shortcomings. 1 SUBSAMPLING SEQUENCES Consider a mechanism which, given a sequence of vectors s = {s0, s1, . . . , sT−1}, st ∈ R, produces a sequence of “sampling probabilities” e = {e0, e1, . . . , eT−1}, et ∈ [0, 1] which denote the probability of including st in the output sequence y = {y0, y1, . . . , yU−1}. Producing y from s and e is encapsulated by the following pseudo-code and visualized in fig. 2 (appendix): # Initialize y as an empty sequence y = [] for t in {0, 1, ..., T 1}: # Draw a random number in [0, 1] and compare to e[t] if rand() < e[t]: # Add s[t] to y with probability e[t] y.append(s[t]) We call this a “subsampling mechanism”, because by construction, U ≤ T , and each element of y is drawn directly from s. The ability to subsample a sequence has various applications: • When the input sequence s is oversampled (i.e. each element st contains much the same information as st−1), subsampling can be an effective way of shortening the sequence without discarding useful information. Using a shorter sequence can facilitate the use of recurrent network models, which have difficulties with long-term dependencies (Bengio et al., 1994; Hochreiter & Schmidhuber, 1997). Simple subsampling schemes such as choosing every other element of s have proven effective in tasks such as speech recognition (Chan et al., 2015). • The mechanism can be used in sequence transduction tasks where the output sequence is shorter than the input. We contrast this approach with the commonly used Connectionist Temporal Classification loss (Graves et al., 2006) because subsampling actually shortens the sequence (instead of inserting blanks) and can be inserted arbitrarily into a neural network model (instead of specifically being a loss function). It also implicitly produces a monotonic alignment between elements in s and y; such alignments have proven to be useful (Bahdanau et al., 2014). • Applying this subsampling operation multiple times could build a hierarchy of shorter and shorter sequences which capture structure at different scales. A similar approach was recently shown to be effective in langauge modeling tasks (Chung et al., 2016). Motivated by these applications, in this extended abstract we present a method for training this subsampling mechanism in expectation, i.e. without sampling. We then test this approach on a simple toy problem and study the resulting model’s behavior. Finally, we discuss shortcomings of our approach and possibilities for future work. ∗Work done as members of the Google Brain Residency program 1 ar X iv :1 70 2. 06 91 4v 3 [ cs .L G ] 8 A pr 2 01 7 Workshop track ICLR 2017 2 TRAINING IN EXPECTATION We are interested in including the mechanism defined in the previous section in the midst of a neural network model. However, the sampling process used to construct y precludes the use of standard backpropagation. A common approach to this issue is to optimize the model according to the expected (or mean-field) output (Graves, 2016; Bahdanau et al., 2014). The following analysis shows how to employ this approach to our proposed subsampling mechanism using a dynamic program which analytically computes p(ym = sn). First, observe that p(y0 = s0) = e0, i.e. the probability that the first output is the first entry in the sequence is just the probability of sampling at time 0. Next, in order for y0 = s1, we need y0 6= s0 so p(y0 = s1) is the probability that s0 was not sampled at time 0 and that s1 was, giving p(y0 = s1) = e1(1 − e0). Continuing on in this way, we see that p(y0 = sn) = en ∏n−1 i=0 (1 − ei) or, in words, the probability that the first output element y0 is a given element in the sequence sn is the probability that none of s0, . . . , sn−1 were sampled multiplied by the probability of sampling sn. Second, observe that p(ym = sn) = 0 when n < m because in order for the output sequence to be of length m, at least m − 1 symbols must already have been sampled. If n < m, this relation is violated. Finally, in order for ym = sn in general, we must have that ym−1 = sj ∈ s0, . . . , sn−1 (i.e. the previous output must be one of the states before sn), none of sj+1, . . . , sn−1 may be sampled at time m, and sn is sampled at time m. To compute p(ym = sn), we need to sum over all of the the possible cases ym−1 ∈ {s0, . . . , sn−1}. The probability of a single case is the combined probability that sn is sampled, that ym−1 = sj , and that none of sj+1, . . . , sn−1 are sampled at time m. We visualize these possibilities in fig. 3 (appendix). Summing over the possible j yields p(ym = sn) = en n−1 ∑ j=0 p(ym−1 = sj) n−1 ∏ i=j+1 (1− ei)  (1) where for convenience we define the special case ∏m i=n • = 1 when n > m. Once we compute p(ym = sn), it is straightforward to find the expected value of ym simply by computing ∑ n snp(ym = sn). Note p(ym = sn) = en((1−en−1)p(ym = sn−1)/en−1+p(ym−1 = sn−1)); it follows that each term p(ym = sn) can be computed in O(1) time by reusing the already-computed terms p(ym = sn−1) and p(ym−1 = sn−1). The resulting dynamic program allows all the terms p(ym = sn) to be computed in O(T ) time. Note that ∑ n p(ym = sn) ≤ 1 depending on the values of e, so these probabilities may not form a valid probability distribution. Computing the expectation as-is without further normalization effectively associates any additional probability to an implicit zero vector in R, which is the convention we will use for the remainder of this extended abstract. 3 TOY PROBLEM EXPERIMENT To evaluate the feasibility of this approach, we tested it on the following toy problem: Consider a length-T sequence x of symbols [0, 1, 2] which occur with equal probability. The output is produced as follows for t ∈ {0, . . . , T − 1}, beginning with an empty memory: 1. If xt is 0, don’t output anything and maintain the current memory state. 2. If xt is 1 or 2 and our memory is empty, place xt in memory and don’t output anything. 3. If xt is 1 and we have 1 in our memory, output a 0 and empty the memory. 4. If xt is 2 and we have 2 in our memory, output a 0 and empty the memory. 5. If xt is 1 and we have 2 in our memory, output a 2 and empty the memory. 6. If xt is 2 and we have 1 in our memory, output a 1 and empty the memory. We also define special cases where if T = 1, the output is x0; if xt = 0 ∀ t ∈ {0, . . . , T − 1}, the output is 0; and if all entries of xt are 0 except one, the output is the single nonzero entry. An example input-output pair for this toy problem is shown in fig. 4 (appendix).",
    "creator" : "LaTeX with hyperref package"
  }
}