{
  "name" : "1704.07483.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Continuously Differentiable Exponential Linear Units",
    "authors" : [ "Jonathan T. Barron" ],
    "emails" : [ "barron@google.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "continuous for all values of α, making the rectifier easier to reason about and making α easier to tune. This alternative parametrization has several other useful properties that the original parametrization of ELU does not: 1) its derivative with respect to x is bounded, 2) it contains both the linear transfer function and ReLU as special cases, and 3) it is scale-similar with respect to α.\nThe Exponential Linear Unit as described in [1] is as follows:\nELU(x, α) = { x if x ≥ 0 α(exp(x)− 1) otherwise\n(1)\nWhere x is the input to the function, and α is a shape parameter. The derivative of this function with respect to x is:\nd\ndx ELU(x, α) = { 1 if x ≥ 0 α exp(x) otherwise\n(2)\nIn Figures 1a and 1b we plot this activation and its derivative with respect to x for different values of α. We see that when α 6= 1, the activation’s derivative is discontinuous at x = 0. Additionally we see that large values of α can cause a large (“exploding”) gradient for small negative values of x, which may make training difficult.\nOur alternative parametrization of the ELU, which we dub “CELU”, is simply the ELU where the activation for negative values has been modified to ensure that the derivative at x = 0 for all values of α is 1:\nCELU(x, α) = { x if x ≥ 0 α ( exp ( x α ) − 1 ) otherwise (3)\nNote that ELU and CELU are identical when α = 1:\n∀x ELU(x, 1) = CELU(x, 1) (4)\nThe derivative of the activation with respect to x and α are as follows:\nd\ndx CELU(x, α) = { 1 if x ≥ 0 exp ( x α ) otherwise\n(5)\nd\ndα CELU(x, α) = { 0 if x ≥ 0 exp ( x α ) ( 1− xα ) − 1 otherwise\nLike in ELU, derivatives for CELU can be computed efficiently by precomputing exp ( x α ) and using it for the activation and its derivatives. Unlike ELU, CELU is scale-similar as a function of x and α:\nCELU(x, α) = 1\nc CELU(cx, cα) (6)\nThe CELU also converges to ReLU as α approaches 0 from the right and converges to a linear “no-op” activation as α approaches∞:\nlim α→0+ CELU(x, α) = max(0, x) (7)\nlim α→∞ CELU(x, α) = x (8)\nThis gives the CELU a nice interpretation as a way to interpolate between a ReLU and a linear function using α. Naturally, CELU can be slightly shifted in x and y such that it converges to any arbitrary shifted ReLU, in case negative activations are desirable even for small values of α."
    } ],
    "references" : [ {
      "title" : "Fast and accurate deep network learning by exponential linear units (elus)",
      "author" : [ "D. Clevert", "T. Unterthiner", "S. Hochreiter" ],
      "venue" : "CoRR, abs/1511.07289,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Exponential Linear Units (ELUs) are a useful rectifier for constructing deep learning architectures, as they may speed up and otherwise improve learning by virtue of not have vanishing gradients and by having mean activations near zero [1].",
      "startOffset" : 236,
      "endOffset" : 239
    }, {
      "referenceID" : 0,
      "context" : "However, the ELU activation as parametrized in [1] is not continuously differentiable with respect to its input when the shape parameter α is not equal to 1.",
      "startOffset" : 47,
      "endOffset" : 50
    }, {
      "referenceID" : 0,
      "context" : "The Exponential Linear Unit as described in [1] is as follows:",
      "startOffset" : 44,
      "endOffset" : 47
    } ],
    "year" : 2017,
    "abstractText" : "Exponential Linear Units (ELUs) are a useful rectifier for constructing deep learning architectures, as they may speed up and otherwise improve learning by virtue of not have vanishing gradients and by having mean activations near zero [1]. However, the ELU activation as parametrized in [1] is not continuously differentiable with respect to its input when the shape parameter α is not equal to 1. We present an alternative parametrization which is C continuous for all values of α, making the rectifier easier to reason about and making α easier to tune. This alternative parametrization has several other useful properties that the original parametrization of ELU does not: 1) its derivative with respect to x is bounded, 2) it contains both the linear transfer function and ReLU as special cases, and 3) it is scale-similar with respect to α. The Exponential Linear Unit as described in [1] is as follows: ELU(x, α) = { x if x ≥ 0 α(exp(x)− 1) otherwise (1) Where x is the input to the function, and α is a shape parameter. The derivative of this function with respect to x is: d dx ELU(x, α) = { 1 if x ≥ 0 α exp(x) otherwise (2) In Figures 1a and 1b we plot this activation and its derivative with respect to x for different values of α. We see that when α 6= 1, the activation’s derivative is discontinuous at x = 0. Additionally we see that large values of α can cause a large (“exploding”) gradient for small negative values of x, which may make training difficult. Our alternative parametrization of the ELU, which we dub “CELU”, is simply the ELU where the activation for negative values has been modified to ensure that the derivative at x = 0 for all values of α is 1: CELU(x, α) = { x if x ≥ 0 α ( exp ( x α ) − 1 ) otherwise (3) Note that ELU and CELU are identical when α = 1: ∀x ELU(x, 1) = CELU(x, 1) (4) The derivative of the activation with respect to x and α are as follows: d dx CELU(x, α) = { 1 if x ≥ 0 exp ( x α ) otherwise (5) d dα CELU(x, α) = { 0 if x ≥ 0 exp ( x α ) ( 1− x α ) − 1 otherwise Like in ELU, derivatives for CELU can be computed efficiently by precomputing exp ( x α ) and using it for the activation and its derivatives. Unlike ELU, CELU is scale-similar as a function of x and α: CELU(x, α) = 1 c CELU(cx, cα) (6) The CELU also converges to ReLU as α approaches 0 from the right and converges to a linear “no-op” activation as α approaches∞: lim α→0+ CELU(x, α) = max(0, x) (7) lim<lb>α→∞<lb>CELU(x, α) = x<lb>(8) This gives the CELU a nice interpretation as a way to in-<lb>terpolate between a ReLU and a linear function using α.<lb>Naturally, CELU can be slightly shifted in x and y such that<lb>it converges to any arbitrary shifted ReLU, in case negative<lb>activations are desirable even for small values of α.<lb>References<lb>[1] D. Clevert, T. Unterthiner, and S. Hochreiter. Fast and accu-<lb>rate deep network learning by exponential linear units (elus).<lb>CoRR, abs/1511.07289, 2015. 1<lb>ar<lb>X<lb>iv<lb>:1<lb>70<lb>4.<lb>07<lb>48<lb>3v<lb>1<lb>[<lb>cs<lb>.L<lb>G<lb>]<lb>2<lb>4<lb>A<lb>pr<lb>2<lb>01<lb>7",
    "creator" : "LaTeX with hyperref package"
  }
}