{
  "name" : "1506.01744.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Spectral Learning of Large Structured HMMs for Comparative Epigenomics",
    "authors" : [ "Chicheng Zhang", "Jimin Song", "Kevin C Chen" ],
    "emails" : [ "chz038@eng.ucsd.edu", "song@dls.rutgers.edu", "kcchen@dls.rutgers.edu", "kamalika@eng.ucsd.edu" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "In this paper, we develop a latent variable model and efficient spectral algorithm motivated by the recent emergence of very large data sets of chromatin marks from multiple human cell types [7, 9]. Chromatin marks are chemical modifications on the genome which are important in many basic biological processes. After standard preprocessing steps, the data consists of a binary vector (one bit for each chromatin mark) for each position in the genome and for each cell type.\nA natural model for chromatin data in one cell type is a Hidden Markov Model (HMM) [8, 13], for which efficient spectral algorithms are known. On biological data sets, spectral algorithms have been shown to have several practical advantages over maximum likelihood-based methods, including speed, prediction accuracy and biological interpretability [24]. Here we extend the approach by modeling multiple cell types together. We model the relationships between cell types by connecting their hidden states by a fixed tree, the standard model in biology for relationships between cell types. This comparative approach leverages the information shared between the different data sets in a statistically unified and biologically motivated manner.\nFormally, our model is an HMM where the hidden state zt at time t has a structure represented by a tree graphical model of known structure. For each tree node u we can associate an individual hidden state zut that depends not only on the previous hidden state z u t−1 for the same tree node u but also on the individual hidden state of its parent node. Additionally, there is an observation variable xut for each node u, and the observation x u t is independent of other state and observation variables\nar X\niv :1\n50 6.\n01 74\n4v 1\n[ st\nat .M\nL ]\nconditioned on the hidden state variable zut . In the bioinformatics literature, [5] studied this model with the additional constraint that all tree nodes share the same emission parameters. In biological applications, the main outputs of interest are the learned observation matrices of the HMM and a segmentation of the genome into regions which can be used for further studies.\nA standard approach to unsupervised learning of HMMs is the Expectation-Maximization (EM) algorithm. When applied to HMMs with very large state spaces, EM is very slow. A recent line of work on spectral learning [18, 1, 23, 6] has produced much more computationally efficient algorithms for learning many graphical models under certain mild conditions, including HMMs. However, a naive application of these algorithms to HMMs with large state spaces results in computational complexity exponential in the size of the underlying tree.\nHere we exploit properties of the tree structure of the hidden states to provide spectral algorithms that are more computationally efficient for current biological datasets. This is achieved by three novel key ideas. Our first key idea is to show that we can treat each root-to-leaf path in the tree separately and learn its parameters using tensor decomposition methods. This step improves the running time because our trees typically have very low depth. Our second key idea is a novel tensor symmetrization technique that we call Skeletensor construction where we avoid constructing the full tensor over the entire root-to-leaf path. Instead we use carefully designed symmetrization matrices to reveal its range in a Skeletensor which has dimension equal to that of a single tree node. The third and final key idea is called Product Projections, where we exploit the independence of the emission matrices along the root-to-leaf path conditioned on the hidden states to avoid constructing the full tensors and instead construct compressed versions of the tensors of dimension equal to the number of hidden states, not the number of observations. Beyond our specific model, we also show that Product Projections can be applied to other graphical models and thus we contribute a general tool for developing efficient spectral algorithms.\nFinally we implement our algorithm and evaluate it on biological data from nine human cell types [7]. We compare our results with the results of [5] who used a variational EM approach. We also compare with spectral algorithms for learning HMMs for each cell type individually to assess the value of the tree model."
    }, {
      "heading" : "1.1 Related Work",
      "text" : "The first efficient spectral algorithm for learning HMM parameters was due to [18]. There has been an explosion of follow-up work on spectral algorithms for learning the parameters and structure of latent variable models [23, 6, 4]. [18] gives a spectral algorithm for learning an observable operator representation of an HMM under certain rank conditions. [23] and [3] extend this algorithm to the case when the transition matrix and the observation matrix respectively are rank-deficient. [19] extends [18] to Hidden Semi-Markov Models.\n[2] gives a general spectral algorithm for learning parameters of latent variable models that have a multi-view structure – there is a hidden node and three or more observable nodes that are not connected to any other nodes and are independent conditioned on the hidden node. Many latent variable models have this structure, including HMMs, tree graphical models, topic models and mixture models. [1] provides a simpler, more robust algorithm that involves decomposing a third order tensor. [21, 22, 25] provide algorithms for learning latent trees and of latent junction trees.\nSeveral algorithms have been designed for learning HMM parameters for chromatin modeling, including stochastic variational inference [16] and contrastive learning of two HMMs [26]. However, none of these methods extend directly to modeling multiple chromatin sequences simultaneously."
    }, {
      "heading" : "2 The Model",
      "text" : "Probabilistic Model. The natural probabilistic model for a single epigenomic sequence is a hidden Markov model (HMM), where time corresponds to position in the sequence. The observation at time t is the sequence value at position t, and the hidden state at t is the regulatory function in this position.\nIn comparative epigenomics, the goal is to jointly model epigenomic sequences from multiple species or cell-types. This is done by an HMM with a tree-structured hidden state [5](THS-HMM),1 where each node in the tree representing the hidden state has a corresponding observation node. Formally, we represent the model by a tuple H = (G,O, T ,W); Figure 1 shows a pictorial representation.\nG = (V,E) is a directed tree with known structure whose nodes represent individual cell-types or species. The hidden state zt and the observation xt are represented by vectors {zut } and {xut } indexed by nodes u ∈ V . If (v, u) ∈ E, then v is the parent of u, denoted by π(u); if v is a parent of u, then for all t, zvt is a parent of z u t . In addition, the observations have the following product structure: if u′ 6= u, then conditioned on zut , the observation xut is independent of zu ′ t and x u′\nt as well as any zu ′\nt′ and x u′ t′ for t 6= t′. O is a set of observation matrices Ou = P (xut |zut ) for each u ∈ V and T is a set of transition tensors Tu = P (zut+1|zut , z π(u) t+1 ) for each u ∈ V . Finally,W is the set of initial distributions where Wu = P (zu1 |z π(u) 1 ) for each z u 1 .\nGiven a tree structure and a number of iid observation sequences corresponding to each node of the tree, our goal is to determine the parameters of the underlying THS-HMM and then use these parameters to infer the most likely regulatory function at each position in the sequences.\nBelow we use the notation D to denote the number of nodes in the tree and d to denote its depth. For typical epigenomic datasets, D is small to moderate (5-50) while d is very small (2 or 3) as it is difficult to obtain data with large d experimentally. Typically m, the number of possible values assumed by the hidden state at a single node, is about 6-25, while n, the number of possible observation values assumed by a single node is much larger (e.g. 256 in our dataset).\nTensors. An order-3 tensor M ∈ Rn1 ⊗Rn2 ⊗Rn3 is a 3-dimensional array with n1n2n3 entries, with its (i1, i2, i3)-th entry denoted as Mi1,i2,i3 .\nGiven ni×1 vectors vi, i = 1, 2, 3, their tensor product, denoted by v1⊗v2⊗v3 is the n1×n2×n3 tensor whose (i1, i2, i3)-th entry is (v1)i1(v2)i2(v3)i3 . A tensor that can be expressed as the tensor product of a set of vectors is called a rank 1 tensor. A tensor M is symmetric if and only if for any permutation π : [3]→ [3], Mi1,i2,i3 = Mπ(i1),π(i2),π(i3).\nLet M ∈ Rn1 ⊗Rn2 ⊗Rn3 . If Vi ∈ Rni×mi , then M(V1, V2, V3) is a tensor of size m1×m2×m3, whose (i1, i2, i3)-th entry is: M(V1, V2, V3)i1,i2,i3 = ∑ j1,j2,j3 Mj1,j2,j3(V1)j1,i1(V2)j2,i2(V3)j3,i3 .\nSince a matrix is a order-2 tensor, we also use the following shorthand to denote matrix multiplication. Let M ∈ Rn1 ⊗ Rn2 . If Vi ∈ Rmi×ni , then M(V1, V2) is a matrix of size m1 × m2, whose (i1, i2)-th entry is: M(V1, V2)i1,i2 = ∑ j1,j2\nMj1,j2(V1)j1,i1(V2)j2,i2 . This is equivalent to V >1 MV2.\n1In the bioinformatics literature, this model is also known as a tree HMM.\nMeta-States and Observations, Co-occurrence Matrices and Tensors. Given observations xut and xut′ at a single node u, we use the notation P u t,t′ to denote their expected co-occurence frequencies: Pu,ut,t′ = E[xut ⊗ xut′ ], and P̂ u,u t,t′ to denote their corresponding empirical version. The tensor Pu,u,ut,t′,t′′ = E[xut ⊗ xut′ ⊗ xut′′ ] and its empirical version P̂ u,u,u t,t′,t′′ are defined similarly.\nOccasionally, we will consider the states or observations corresponding to a subset of nodes in G coalesced into a single meta-state or meta-observation. Given a connected subset S ⊆ V of nodes in the tree G that includes the root, we use the notation zSt and x S t to denote the meta-state represented by (zut , u ∈ S) and the meta-observation represented by (xut , u ∈ S) respectively. We define the observation matrix for S as OS = P (xSt |zSt ) ∈ Rn\n|S|×m|S| and the transition matrix for S as TS = P (zSt+1|zSt ) ∈ Rm |S|×m|S| , respectively.\nFor sets of nodes V1 and V2, we use the notation P V1,V2 t,t′ to denote the expected co-occurrence frequencies of the meta-observations xV1t and x V2 t′ . Its empirical version is denoted by P̂ V1,V2 t,t′ . Similarly, we can define the notation PV1,V2,V3t,t′,t′′ and its empirical version P̂ V1,V2,V3 t,t′,t′′ .\nBackground on Spectral Learning for Latent Variable Models. Recent work by [1] has provided a novel elegant tensor decomposition method for learning latent variable models. Applied to HMMs, the main idea is to decompose a transformed version of the third order co-occurrence tensor of the first three observations to recover the parameters; [1] shows that given enough samples and under fairly mild conditions on the model, this provides an approximation to the globally optimal solution. The algorithm has three main steps. First, the third order tensor of the co-occurrences is symmetrized using the second order co-occurrence matrices to yield a symmetric tensor; this symmetric tensor is then orthogonalized by a whitening transformation. Finally, the resultant symmetric orthogonal tensor is decomposed via the tensor power method.\nIn biological applications, instead of multiple independent sequences, we have a single long sequence in the steady state. In this case, following ideas from [23], we use the average over t of the third order co-occurence tensors of three consecutive observations starting at time t. The second order co-occurence tensor is also modified similarly."
    }, {
      "heading" : "3 Algorithm",
      "text" : "A naive approach for learning parameters of HMMs with tree-structured hidden states is to directly apply the spectral method of [1]. Since this method ignores the structure of the hidden state, its running time is very high, Ω(nDmD), even with optimized implementations. This motivates the design of more computationally efficient approaches.\nA plausible approach is to observe that at t = 1, the observations are generated by a tree graphical model; thus in principle one could learn the parameters of the underlying tree using existing algorithms [22, 21, 25]. However, this approach does not directly produce the HMM parameters; it also does not work for biological sequences because we do not have multiple independent samples at t = 1; instead we have a single long sequence at the steady state, and the steady state distribution of observations is not generated by a latent tree. Another plausible approach is to use the spectral junction tree algorithm of [25]; however, this algorithm does not provide the actual transition and observation matrix parameters which hold important biological information, and instead provides an observable operator representation.\nOur main contribution is to show that we can achieve a much better running time by exploiting the structure of the hidden state. Our algorithm is based on three key ideas – Partitioning, Skeletensor Construction and Product Projections. We explain these ideas next.\nPartitioning. Our first observation is that to learn the parameters at a node u, we can focus only on the unique path from the root to u. Thus we partition the learning problem on the tree into separate learning problems on these paths. This maintains correctness as proved in the Appendix.\nThe Partitioning step reduces the computational complexity since we now need to learn an HMM with md states and nd observations, instead of the naive method where we learn an HMM with mD states and nD observations. As d D in biological data, this gives us significant savings. Constructing the Skeletensor. A naive way to learn the parameters of the HMM corresponding to each root-to-node path is to work directly on the O(nd × nd × nd) co-occurrence tensor. Instead, we show that for each node u on a root-to-node path, a novel symmetrization method can be used to construct a much smaller skeleton tensor Tu of size n × n × n, which nevertheless captures the effect of the entire root-to-node path and projects it into the skeleton tensor, thus revealing the range of Ou. We call this the skeletensor.\nLet Hu be the path from the root to a node u, and let P̂ Hu,u,Hu 1,2,3 be the empirical n |Hu| × n× n|Hu| tensor of co-occurrences of the meta-observations Hu, u and Hu at times 1, 2 and 3 respectively. Based on the data we construct the following symmetrization matrices:\nS1 ∼ P̂u,Hu2,3 (P̂ Hu,Hu 1,3 ) †, S3 ∼ P̂u,Hu2,1 (P̂ Hu,Hu 3,1 ) †\nNote that S1 and S3 are n × n|Hu| matrices. Symmetrizing P̂Hu,u,Hu1,2,3 with S1 and S3 gives us an n×n×n skeletensor, which can in turn be decomposed to give an estimate of Ou (see Lemma 3 in the Appendix).\nEven though naively constructing the symmetrization matrices and skeletensor takes O(Nn2d+1 + n3d) time, this procedure improves computational efficiency because tensor construction is a onetime operation, while the power method which takes many iterations is carried out on a much smaller tensor.\nProduct Projections. We further reduce the computational complexity by using a novel algorithmic technique that we call Product Projections. The key observation is as follows. Let Hu = {u0, u1, . . . , ud−1} be any root-to-node path in the tree and consider the HMM that generates the observations (xu0t , x u1 t , . . . , x ud−1 t ) for t = 1, 2, . . .. Even though the individual observations x uj t , j = 0, 1, . . . , d − 1 are highly dependent, the range of OHu , the emission matrix of the HMM describing the path Hu, is contained in the product of the ranges of Ouj , where Ouj is the emission matrix at node uj (Lemma 4 in the Appendix). Furthermore, even though the Ouj matrices are difficult to find, their ranges can be determined by computing the SVDs of the observation co-occurrence matrices at uj .\nThus we can implicitly construct and store (an estimate of) the range of OHu . This also gives us estimates of the range of P̂Hu,Hu1,3 , the column spaces of P̂ u,Hu 2,1 and P̂ u,Hu 2,3 , and the range of the first and third modes of the tensor P̂Hu,u,Hu1,2,3 . Therefore during skeletensor construction we can avoid explicitly constructing S1, S3 and P̂ Hu,u,Hu 1,2,3 , and instead construct their projections onto their ranges. This reduces the time complexity of the skeletensor construction step to\nO(Nm2d+1 +m3d + dmn2) (recall that the range has dimension m.) While the number of hidden states m could be as high as n, this is a significant gain in practice, as n m in biological datasets (e.g. 256 observations vs. 6 hidden states).\nProduct projections are more efficient than random projections [17] on the co-occurrence matrix of meta-observations: the co-occurrence matrices are nd×nd matrices, and random projections would take Ω(nd) time. Also, product projections differ from the suggestion of [15] since we exploit properties of the model to efficiently find good projections."
    }, {
      "heading" : "3.1 The Full Algorithm",
      "text" : "Our final algorithm follows from combining the three key ideas above. Algorithm 1 shows how to recover the observation matrices Ou at each node u. Once the Ous are recovered, one can use standard techniques to recover T and W ; details are described in Algorithm 2 in the Appendix."
    }, {
      "heading" : "3.2 Product Projections beyond HMMs with Tree-structured Hidden States",
      "text" : "The Product Projections technique is a general technique with applications beyond our model.\nAlgorithm 1 Algorithm for Observation Matrix Recovery\n1: Input: N samples of the three consecutive observations (x1, x2, x3) N i=1 generated by an HMM\nwith tree structured hidden state with known tree structure. 2: for u ∈ V do 3: Perform SVD on P̂u,u1,2 to get the first m left singular vectors Û\nu. 4: end for 5: for u ∈ V do 6: Let Hu denote the set of nodes on the unique path from root r to u. Let ÛHu = ⊗v∈HuÛv . 7: Construct Projected Skeletensor. First, compute symmetrization matrices:\nŜu1 = ((Û u)>P̂u,Hu2,3 Û Hu)((ÛHu)>P̂Hu,Hu1,3 Û Hu)−1, Ŝu3 = ((Û u)>P̂u,Hu2,1 Û Hu)((ÛHu)>P̂Hu,Hu3,1 Û Hu)−1\n8: Compute symmetrized second and third co-occurrences for u:\nM̂u2 = (P̂ Hu,u 1,2 (Û Hu(Ŝu1 ) >, Ûu) + P̂Hu,u1,2 (Û Hu(Ŝu1 ) >, Ûu)>)/2\nM̂u3 = P̂ Hu,u,Hu 1,2,3 (Û Hu(Ŝu1 ) >, Ûu, ÛHu(Ŝu3 ) >)\n9: Orthogonalization and Tensor Decomposition. Orthogonalize M̂u3 using M̂u2 and decompose to recover (θ̂u1 , . . . , θ̂ u m) as in [1] (See Algorithm 3 in the Appendix for details).\n10: Undo Projection onto Range. Estimate Ou as: Ôu = ÛuΘ̂u, where Θ̂u = (θ̂u1 , . . . , θ̂um). 11: end for\nApplication 1: HMM with more general hidden states. Consider an HMM with a hidden state represented by a general graphical modelG = (V,E) with an observation variable xut corresponding to each u ∈ V . xut is independent of all other hidden state and observation nodes, conditioned on its corresponding hidden state variable zut . In this case, O\n|V | = ⊗u∈VOu. Similar graphical models have been used in biology to model gene expression time courses [12].\nApplication 2: HMM with rank-deficient observation matrix. Consider an HMM whose observation matrix O is rank-deficient. In this case, [3] suggests compressing sequences of successive observations of size s for s = 2, 3, . . . until the matrices Of = P (xt, xt+1, . . . , xt+s−1|zt) and Ob = P (xt, xt−1, . . . , xt−s+1|zt) have rank m. A version of [18] is then run using observation sequence pairs P1:s,s+1:2s and triples P1:s,s+1,s+2:2s+1. In this case, we can show that both range(Of ) and range(Ob) are contained in range(O⊗s); we can therefore use Product Projections to improve the Ω(ns) running time to O(mO(s))."
    }, {
      "heading" : "3.3 Performance Guarantees",
      "text" : "We now provide performance guarantees on our algorithm. Since learning parameters of HMMs and many other graphical models is NP-Hard, spectral algorithms make simplifying assumptions on the properties of the model generating the data. Typically these assumptions take the form of some conditions on the rank of certain parameter matrices. We state below the conditions needed for our algorithm to successfully learn parameters of a HMM with tree structured hidden states. Observe that we need two kinds of rank conditions – node-wise and path-wise – to ensure that we can recover the full set of parameters on a root-to-node path.\nAssumption 1 (Node-wise Rank Condition). For all u ∈ V , the matrix Ou has rank m, and the joint probability matrix Pu,u2,1 has rank m.\nAssumption 2 (Path-wise Rank Condition). For any u ∈ V , let Hu denote the path from root to u. Then, the joint probability matrix PHu,Hu1,2 has rank m |Hu|.\nAssumption 1 is required to ensure that the skeletensor can be decomposed, and that Ûu indeed captures the range of Ou. Assumption 2 ensures that the symmetrization operation succeeds. This kind of assumption is very standard in spectral learning [18, 1].\n[3] has provided a spectral algorithm for learning HMMs involving fourth and higher order moments when Assumption 1 does not hold. We believe similar approaches will apply to our problem as well, and we leave this as an avenue for future work.\nIf Assumptions 1 and 2 hold, we can show that Algorithm 1 is consistent – provided enough samples are available, the model parameters learnt by the algorithms are close to the true model parameters. A finite sample guarantee is provided in the Appendix.\nTheorem 1. [Consistency] Suppose we run Algorithm 1 on the first three observation vectors {xi,1, xi,2, xi,3} from N iid sequences generated by an HMM with tree-structured hidden states. Then, for all nodes u ∈ V , the recovered estimates Ôu satisfy the following property: with high probability over the iid samples, there exists a permutation Πu of the columns of Ôu such that as ‖Ou −ΠuÔu‖ ≤ ε(N) where ε(N)→ 0 as N →∞.\nObserve that the observation matrices (as well as the transition and initial probabilities) are recovered upto permutations of hidden states in a globally consistent manner."
    }, {
      "heading" : "4 Experiments",
      "text" : "Data and experimental settings. We ran our algorithm, which we call “Spectral-Tree”, on a chromatin dataset on human chromosome 1 from nine cell types (H1-hESC, GM12878, HepG2, HMEC, HSMM, HUVEC, K562, NHEK, NHLF) from the ENCODE project [7]. Following [5], we used a biologically motivated tree structure of a star tree with H1-hESC, the embryonic stem cell type, as the root. There are data for eight chromatin marks for each cell type which we preprocessed into binary vectors using a standard Poisson background assumption [11]. The chromosome is divided into 1,246,253 segments of length 200, following [11]. The observed data consists of a binary vector of length eight for each segment, so the number of possible observations is the number of all combinations of presence or absence of the chromatin marks (i.e. n = 28 = 256). We set the number of hidden states, which we interpret as chromatin states, to m = 6, similar to the choice of ENCODE. Our goals are to discover chromatin states corresponding to biologically important functional elements such as promoters and enhancers, and to label each chromosome segment with the most probable chromatin state.\nObserve that instead of the first few observations from N iid sequences, we have a single long sequence in the steady state per cell type; thus, similar to [23], we calculate the empirical cooccurrence matrices and tensors used in the algorithm based on two and three successive observations respectively (so, more formally, instead of P̂1,2, we use the average over t of P̂t,t+1 and so on). Additionally, we use a projection procedure similar to [4] for rounding negative entries in the recovered observation matrices. Our experiments reveal that the rank conditions appear to be satisfied for our dataset.\nRun time and memory usage comparisons. First, we flattened the HMM with tree-structured hidden states into an ordinary HMM with an exponentially larger state space. Our Python implementation of the spectral algorithm for HMMs of [18] ran out of memory while performing singular value decomposition on the co-occurence matrix, even using sparse matrix libraries. This suggests that naive application of spectral HMM is not practical for biological data.\nNext we compared the performance of Spectral-Tree to a similar model which additionally constrained all transition and observation parameters to be the same on each branch [5]. That work used several variational approximations to the EM algorithm and reported that SMF (structured mean field) performed the best in their tests. Although we implemented Spectral-Tree in Matlab and did not optimize it for run-time efficiency, Spectral-Tree took ∼2 hr, whereas the SMF algorithm took ∼13 hr for 13 iterations to convergence. This suggests that spectral algorithms may be much faster than variational EM for our model.\nBiological interpretation of the observation matrices. Having examined the efficiency of Spectral-Tree, we next studied the accuracy of the learned parameters. We focused on the observation matrices which hold most of the interesting biological information. Since the full observation matrix is very large (28 × 6 where each row is a combination of chromatin marks), Figure 2 shows\nthe 8× 6 marginal distribution of each chromatin mark conditioned on each hidden state. SpectralTree identified most of the major types of functional elements typically discovered from chromatin data: repressive, strong enhancer, weak enhancer, promoter, transcribed region and background state (states 1-6, respectively, in Figure 2b). In contrast, the SMF algorithm used three out of the six states to model the large background state (i.e. the state with no chromatin marks). It identified repressive, transcribed and promoter states (states 2, 4, 5, respectively, in Figure 2a) but did not identify any enhancer states, which are one of the most interesting classes for further biological studies.\nWe believe these results are due to that fact that the background state in the data set is large: ∼62% of the segments do not have chromatin marks for any cell type. The background state has lower biological interest but is modeled well by the maximum likelihood approach. In contast, biologically interesting states such as promoters and enhancers comprise a relatively small fraction of the genome. We cannot simply remove background segments to make the classes balanced because it would change the length distribution of the hidden states. Finally, we observed that our model estimated significantly different parameters for each cell type which captures different chromatin states (Appendix Figure 3). For example, we found enhancer states with strong H3K27ac in all cell types except for H1-hESC, where both enhancer states (3 and 6) had low signal for this mark. This mark is known to be biologically important in these cells for distinguishing active from poised enhancers [10]. This suggests that modeling the additional branch-specific parameters can yield interesting biological insights.\nComparison of the chromosome segments labels. We computed the most probable state for each chromosome segment using a posterior decoding algorithm. We tested the accuracy of the predictions using an experimentally defined data set and compared it to SMF and the spectral algorithm for HMMs run for individual cell types without the tree (Spectral-HMM). Specifically we assessed promoter prediction accuracy (state 5 for SMF and state 4 for Spectral-Tree in Figure 2) using CAGE data from [14] which was available for six of the nine cell types. We used the F1 score (harmonic mean of precision and recall) for comparison and found that Spectral-Tree was much more accurate than SMF for all six cell types (Table 1). This was because the promoter predictions of SMF were biased towards the background state so those predictions had slightly higher recall but much lower specificity.\nFinally, we compared our predictions to Spectral-HMM to assess the value of the tree model. H1hESC is the root node so Spectral-HMM and Spectral-Tree have the same model and obtain the same accuracy (Table 1). Spectral-Tree predicts promoters more accurately than Spectral-HMM for all other cell types except HepG2. However, HepG2 is the most diverged from the root among the cell types based on the Hamming distance between the chromatin marks. We hypothesize that for HepG2, the tree is not a good model which slightly reduces the prediction accuracy.\nOur experiments show that Spectral-Tree has improved computational efficiency, biological interpretability and prediction accuracy on an experimentally-defined feature compared to variational EM for a similar tree HMM model and a spectral algorithm for single HMMs. A previous study showed improvements for spectral learning of single HMMs over the EM algorithm [24]. Thus our\nalgorithms may be useful to the bioinformatics community in analyzing the large-scale chromatin data sets currently being produced."
    }, {
      "heading" : "5 Acknowledgements",
      "text" : "We thank NSF under IIS-1162581 for support. Part of this work was done while Chaudhuri was visiting the Spectral Learning program at the Simons Foundation in UC Berkeley."
    }, {
      "heading" : "A Recovering the Transition Probabilities and Initial Probabilities",
      "text" : "Algorithm 2 recovers the transition and initial probabilities, given estimates of observation matrices. Theorem 2 provides finite sample guarantees on Algorithm 1 in conjunction with Algorithm 2.\nAlgorithm 2 Recovering the Transition Probabilities and Initial Probabilities\n1: Input: N samples of the first three observations (x1, x2, x3) N i=1 generated by a tree HMM,\nEstimates of observation matrices Ôu. 2: for u ∈ V do 3: if u is root r then 4: Compute Ŵ r = (Ôu)†P̂ r1 . 5: Compute Q̂r = (Ôr)†P̂ r,r2,1 (Ô\nr)†>. 6: Normalize over the zu2 coordinate to get T̂\nu. 7: else 8: Compute Ŵu = (Ôu)†Pu,π(u)1,1 (Ô π(u))†>. 9: Compute Q̂u = Pu,π(u),u2,2,1 ((Ô u)†T , (Ôπ(u))†>, (Ôu)†>).\n10: Normalize over the zu2 coordinate to get T̂ u. 11: end if 12: end for"
    }, {
      "heading" : "B Additional Notations",
      "text" : "For a node u ∈ V , when it is clear from context, we sometimes use H to denote Hu and d to denote du.\nDefine OH2 to be a n d × md matrix whose rows are indexed by elements in [n]d and columns are indexed by elements in [m]d. In particular, (OH2 )(i1,...,id),(j1,...,jd) = P (x r 2 = i1, . . . , x u 2 = id|zr2 = j1, . . . , z u 2 = jd). Similarly we define O H 3 whose entries are (O H 3 )(i1,...,id),(j1,...,jd) = P (x r 3 = i1, . . . , x u 3 = id|zr2 = j1, . . . , zu2 = jd), and OH1 whose its entries are (OH1 )(i1,...,id),(j1,...,jd) = P (xr1 = i1, . . . , x u 1 = id|zr2 = j1, . . . , zu2 = jd). We define Ou2 to be a n × md matrix, whose rows are indexed by elements in [n], and columns are indexed by elements in [m]d. Its entries are (Ou2 )i,(j1,...,jd) = P (x u 1 = i|zr2 = j1, . . . , zu2 = jd).\nDefine πH to be a vector representing the marginal probability of (zr2 , . . . , z u 2 ). In particular, its rows are indexed by elements in [m]d, and πH(i1,...,id) = P (z r 2 = i1, . . . , z u 2 = id). Define π\nu to be a vector representing the marginal probability of zu2 . In particular, its rows are indexed by elements in [m], and πui = P (z u 2 = i). Define π u min as mini π u i . Define ρ\nH as themd dimensional vector representing the marginal probability of (zr1 , . . . , z u 1 ) whose entries are indexed by elements in [m]\nd. In particular, ρH(i1,...,id) = P (z r 1 = i1, . . . , z u 1 = id). T\nH is defined as the md × md matrix representing the conditional probability of zH2 given z H 1 , and its rows and columns are indexed by elements in [m]\nd, in particular, T(i1,...,id),(j1,...,jd) = P (z r 2 = i1, . . . , z u 2 = id|zr1 = j1, . . . , zu1 = jd).\nLet u be a node in V . Define Uu to be a matrix whose columns form an orthonormal basis of Ou. One way to get Uu is to take its columns to be the top m singular vectors of Ou. The specific choice of Uu does not affect our analysis, as we will be only looking at the projection matrix Uu(Uu)> throughout. Define UH to be ⊗v∈HUu. For a matrix M , define ‖M‖ to be its operator norm, that is, max‖u‖=1,‖v‖=1 ‖v>Mu‖. Define the Frobenius norm of M , ‖M‖F to be square root of the sum of the square of its entries, that is,√∑\ni,jM 2 ij . By standard results in linear algebra, ‖M‖ ≤ ‖M‖F . Similarly, for a third order\ntensor T , define ‖T‖ to be its operator norm, that is max‖u‖=1,‖v‖=1,‖w‖=1 T (u, v, w). Define the Frobenius norm of T , ‖T‖F to be square root of the sum of the square of its entries, that is,√∑\ni,j,k T 2 ijk. By standard results of linear algebra, ‖T‖ ≤ ‖T‖F ."
    }, {
      "heading" : "C Main Lemmas",
      "text" : ""
    }, {
      "heading" : "C.1 Partitioning Lemmas",
      "text" : "Lemma 1 (Path Partitioning). Suppose observations and states {xvt , zvt }v∈V,t∈N are drawn from a THS-HMM represented by H = (G,T,O,W ), where G = (V,E), T = {Tv, v ∈ V }, O = {Ov, v ∈ V }, W = {Wv, v ∈ V }. Let u ∈ V , and let Hu denote nodes inside the unique path from root r to u. Then {xvt , zvt }u∈Hu,t∈N are generated by a THS-HMM represented by a tuple H̃ = (G̃, T̃ , Õ, W̃ ), where G̃ = (Ṽ , Ẽ) is the induced subgraph on Hu. In particular, Ṽ = Hu, Ẽ = {(v, π(v))}v∈Hu ), T̃ = {Tv, v ∈ Hu}, Õ = {Ov, v ∈ Hu}, W̃ = {Wv, v ∈ Hu}.\nProof of Lemma 1. To show this lemma, we will calculate the marginal distribution of the variables {xvt , zvt }v∈Hu,t∈[τ ]. Observe that the full joint distribution of {xvt , zvt }v∈G,t∈[τ ] is equal to:∏\nv∈G Pr(zv1 ) τ−1∏ t=1 ∏ v∈Hu Pr(zvt+1|zvt , z π(v) t+1 ) τ∏ t=1 ∏ v∈G Pr(xvt |zvt )\nTo calculate the marginal over {xvt , zvt }v∈Hu,t∈[τ ], we eliminate the rest of the variables one by one. Observe that we can eliminate any observation variable xvt for v /∈ Hu without introducing any extra edges, as xvt is only connected to z v t . Moreover, marginalizing x v t gives: ∑ x Pr(x v t = x|zvt = z) = 1.\nLet G̃ be the current tree; initially G̃ = G. We next eliminate the nodes {zvt , t = τ, . . . , 1} for v /∈ Hu one by one where v /∈ Hu is a leaf node in G̃. We do this in the order zvT , zvT−1, . . . , zv1 ; once we have eliminated these nodes, we delete v from G̃, and we continue until only the nodes in Hu are left. To eliminate a zvt when {zvs , s > t} have been eliminated, we sum over: ∑ z Pr(z v t = z|zvt−1, z π(v) t ) which also sums to 1.\nWe repeat this process until only the nodes {xvt , zvt }u∈Hu,t∈[T ] are left. Since we get 1 from eliminating each variable, the marginal we are left with is:\n∏ v∈Hu Pr(zv1 ) T−1∏ t=1 ∏ v∈Hu Pr(zvt+1|zvt , z π(v) t+1 ) T∏ t=1 ∏ v∈Hu Pr(xvt |zvt ), (1)\nwhich is the marginal distribution of an HMM with tree-structured hidden states described by the tuple (G̃, T̃ , Õ, W̃ ). The lemma follows.\nThe following is a Corollary of Lemma 1. Corollary 1. If observations and states {xvt , zvt }v∈Hu,t∈N are drawn from a THS-HMM represented by (G̃, T̃ , Õ, W̃ ), then the sequence of coalesced observations and states {xHut , z Hu t }t∈N are drawn from an HMM.\nProof. The proof is a simple extension of Lemma 1. (1) gives us the marginal distribution of {xvt , zvt }v∈Hu,t∈N. Observe that for any t, conditioned on z Hu t , x Hu t is d-separated from all the other nodes of the graph – this is because for any node x in the graphical model, xHut , z Hu t and x either form a chain or or a fork structure whose middle node is zHut . Moreover, conditioned on z Hu t , zHut+1 is d-separated from the set of nodes {zHus } t−1 s=1. This is because z Hu s , z Hu t and z Hu t+1 form a chain structure whose middle node is zHut . The lemma thus follows."
    }, {
      "heading" : "C.2 Skeletensor Lemmas",
      "text" : "In this subsection, we justify our construction of a skeletensor. Let u be any node in the tree G and let H be the path from the root of G to u.\nRecall that we define OH1 to be the n d × md matrix, whose entries are (OH1 )(i1,...,id),(j1,...,jd) = P (xr1 = i1, . . . , x u 1 = id|zr2 = j1, . . . , zu2 = jd). Similarly, OH3 is a nd ×md matrix, with entries (OH3 )(i1,...,id),(j1,...,jd) = P (x r 3 = i1, . . . , x u 3 = id|zr2 = j1, . . . , zu2 = jd).\nWe begin by showing that under Assumptions 1 and 2, the matrices OH1 and O H 3 for the three-view mixture model induced by the HMM have full column rank. Lemma 2. Let u be a node in V . Recall that H = Hu is the set of nodes along the path from root r to u. Then: (1) The matrices diag(ρH)(TH)>diag(πH)−1 and TH are of full rank. (2) The matrices OH1 and O H 3 are of full column rank.\nProof. By Lemma 1, xH1 , x H 2 , x H 3 are conditionally independent given h H 2 . Thus,\nPH,H1,2 = O H 1 diag(π H)(OH2 ) >\nSince by Assumption 2, PH,H1,2 is of rank m d, this implies that the matrix OH1 must be of rank m d as well. By Proposition 4.2 of [2],\nOH1 = O Hdiag(ρH)(TH)>diag(πH)−1\nThis implies that diag(ρH)(TH)>diag(πH)−1 is of rank md, which is of full rank. Hence TH is of full rank. By Proposition 4.2 of [2],\nOH3 = O HTH\nThis shows OH3 is of full column rank.\nSecond, we discuss the infinite sample version of our symmetrization matrix. This will be extended in Lemma 8 in our detailed finite sample analysis. Lemma 3. Let u be a node in V . Recall that Hu is the set of nodes along the path from root r to u. Assume Pu,H2,3 , P H,H 1,3 , P u,H 2,1 are given (where P H,H 3,1 = (P H,H 1,3 )\nT ). Let the symmetrization matrices be:\nSu1 = P u,H 2,3 (P H,H 1,3 ) †\nSu3 = P u,H 2,1 (P H,H 3,1 ) †\nand the ground truth symmetrized pair-wise and triple-wise co-occurence tensors be:\nMu2 = P H,u 1,2 (S uT 1 , I)\nMu3 = P H,u,H 1,2,3 (S uT 1 , I, S uT 3 )\nThen, Mu2 = ∑ i πui (O u)i ⊗ (Ou)i\nMu3 = ∑ i πui (O u)i ⊗ (Ou)i ⊗ (Ou)i\nProof. By Lemma 1, xH1 , x u 2 , x H 3 are conditionally independent given z H 2 , thus\nPu,H2,3 = O u 2 diag(π H)OHT3\nPH,H1,3 = O H 1 diag(π H)OHT3\nLemma 2 implies that OH1 is of full column rank, and diag(π H)OHT3 is of full row rank. Therefore by standard properties of pseudoinverse,\n(PH,H1,3 ) † = (diag(πH)OHT3 ) †(OH1 ) †\nTherefore, Su1 = O u 2 (O H 1 ) † Likewise, Su3 = O u 2 (O H 3 ) † Then, Mu2 = P H,u 1,2 (S uT 1 , I)\n= ∑\ni1,...,iD\nπHi1,...,iD (O u 2 )i1,...,iD ⊗ (Ou2 )i1,...,iD\n= ∑\ni1,...,iD\nπHi1,...,iD (O u)iD ⊗ (Ou)iD\n= ∑ i πui (O u)i ⊗ (Ou)i\nMu3 = P H,u,H 1,2,3 (S uT 1 , I, S uT 3 ) = ∑\ni1,...,iD\nπHi1,...,iD (O u 2 )i1,...,iD ⊗ (Ou2 )i1,...,iD ⊗ (Ou2 )i1,...,iD\n= ∑\ni1,...,iD\nπHi1,...,iD (O u)iD ⊗ (Ou)iD ⊗ (Ou)iD\n= ∑ i πui (O u)i ⊗ (Ou)i ⊗ (Ou)i"
    }, {
      "heading" : "C.3 Product Projections Lemmas",
      "text" : ""
    }, {
      "heading" : "C.3.1 Product Projections in HMM with Tree Hidden States",
      "text" : "Lemma 4. OH , the observation matrix of the HMM that generates the meta-states and metaobservations {zHt , xHt }t∈N, equals ⊗ v∈H O v .\nProof. We consider the observation matrix of the HMM that generates the meta-states and metaobservations {zHt , xHt }t∈N. The number of possible meta-hidden states zHt is md, indexed by (zvt )v∈H and the number of possible meta-observations x H t is n\nd, indexed by (xvt )v∈H . Thus, the observation matrix OH is of dimension nd ×md. Entrywise,\n(OHu)(i1,...,id),(j1,...,jd)\n= P(xrt = i1, . . . , xut = id|zrt = j1, . . . , zut = jd) = Oi1,j1 . . . Oid,jd\n= ( ⊗ v∈H Ov)(i1,...,id),(j1,...,jd)\nWhere the second equality uses conditional independence. Therefore, OH = ⊗\nv∈H O v ."
    }, {
      "heading" : "C.3.2 Product Projections Beyond HMM with Tree Hidden States",
      "text" : "We consider the case of a simple HMM when the observation matrix O is not full rank. In this case, we first define forward and backward observation matrices Õfs and Õ b s formally. For a fixed s, Õ f s is a ns × m matrix, with rows indexed by a s-tuple (j1, . . . , js) ∈ [n]s, and columns indexed by i ∈ [m]. Entrywise,\n(Õfs )(i1,...,is),j = P (xt = i1, xt+1 = i2, . . . , xt+s−1 = is|zt = j)\nSimilarly we define backward observation matrices Õbs = P (xt, xt−1, . . . , xt−s+1|zt). Entrywise,\n(Õbs)(i1,...,is),j = P (xt = i1, xt−1 = i2, . . . , xt−s+1 = is|zt = j) The claim is the range of the forward(backward) observation matrices is contained in the range of the s-wise Kronecker product of the original observation matrices. Lemma 5.\nrange(Õfs ) ⊆ range(O⊗s) range(Õbs) ⊆ range(O⊗s)\nProof. We prove the first relationship, since the proof of the second is almost identical. Note that by the law of total probability,\n(Õfs )(i1,i2,...,is),j\n= P (xt = i1, xt+1 = i2, . . . , xt+s−1 = is|zt = j) = ∑ j2,...,js P (xt = i1, xt+1 = i2, . . . , xt+s−1 = is|zt = j, zt+1 = j2, . . . , zt+s−1 = js)\n×P (zt+1 = j2 . . . , zt+s−1 = js|zt = j) = ∑ j2,...,js Oi1,jOi2,j2 . . . Ois,jsP (zt+1 = j2 . . . , zt+s−1 = js|zt = j)\n= ∑\nj2,...,js\n(O⊗s)(i1,i2,...,is),(j,j2,...,js)P (zt+1 = j2 . . . , zt+s−1 = js|zt = j)\nThus, each column of Õfs is a linear combination of the columns of O ⊗s, thus completing the proof."
    }, {
      "heading" : "D Finite Sample Guarantees",
      "text" : "Theorem 2 (Accuracy of Initial Distribution and Transition Probabilities). There exists a universal constant C such that the following hold. Suppose Algorithm 1 is given as input N iid observation triples (xi1, xi2, xi3)Ni=1 generated by a THS-HMM, and outputs estimates of observaton matrices Ôu, for each node u in the tree. Then Algorithm 2 is run on the same sample and has {Ôu}u∈V as input. If the size of sample N is greater than:\nC max ( D2\nσ22σ 2 3\nln D δ , m\nσ21σ 2 2\nln D\nδ ,\nm2\nσ61σ 6 3π 3 min\nln D\nδ ,\nm\nσ22σ 8 1\n2 ln D\nδ ,\nm2\nσ63σ 14 1 π 4 min\n2 ln D\nδ ) where σ1 = minu∈V σm(Ou), σ2 = minu∈V σm(P u,u 1,2 ), σ3 = minu∈V σmd(P Hu,Hu 1,3 ) and πmin = minu,i π u i , then with probability ≥ 1 − δ over the training examples, with probability 0.9 over the random initializations in Algorithm 1, there exist permutation matrices {Πu}u∈V such that for all u ∈ V , ‖Ou − (ÔuΠu)‖ ≤ if u is the root node, then,\n‖Ŵu − (Πu)>Wu‖ ≤\n‖Q̂u −Qu(Πu,Πu)‖ ≤ Otherwise,\n‖Ŵu −Wu(Πu,Ππ(u))‖ ≤\n‖Q̂u −Qu(Πu,Πu,Ππ(u))‖ ≤\nWe emphasize that our algorithm recovers the initial probability and transition probability tensors up to permutations of hidden states in a globally consistent manner. In contrast to [20] where some hidden nodes do not have observations directly associated with them, in our setting, each hidden state has an associated observation, which makes recovery of permutations easier. How to perform parameter recovery in a THS-HMM with internal hidden states where each hidden tree node does not have an associated observation is an interesting question for future work."
    }, {
      "heading" : "E Proofs",
      "text" : "Throughout this section, we first assume a technical condition on the sample size. This will result in concentration of the projection and the symmetrization matrices.\nAssumption 3. Recall that D = |V |. The sample size N is large enough that\n(N, δ) ≤ min (minu∈V σm(Pu,u1,2 ) minu∈V σmd(PH,H1,3 )\n16D ,\nminu∈V σm(P u,u 1,2 ) minu∈V σm(O u)\n4 √ m\n, minu∈V σmd(P\nH,H 1,3 ) 3 minu∈V σm(O u)3π 3/2 min\n1536c1m ) = min (σ2σ3 16D , σ2σ1 4 √ m , π 3/2 minσ 3 1σ 3 3\n1536c1m\n) (2)\nWhere c1 > 0 is a constant given in Lemma 11, and σ1, σ2, σ3 and πmin are defined in Theorem 2."
    }, {
      "heading" : "E.1 Raw Moments Concentration",
      "text" : "We start with standard concentration of raw moments, which uses the fact that all the (vectorized) raw moments can be viewed as a probability vector. Let u be a node in V , recall that H is the set of nodes along the path from root r to u.\nLet (N, δ) = √\n1+ln(10D/δ) N . Define event\nE = {\nfor all u ∈ V : ‖P̂u,u1,2 − P u,u 1,2 ‖F ≤ (N, δ)\n‖P̂H,u1,2 − P H,u 1,2 ‖F ≤ (N, δ)\n‖P̂u,H2,3 − P u,H 2,3 ‖F ≤ (N, δ)\n‖P̂H,H1,3 − P H,H 1,3 ‖F ≤ (N, δ)\n‖P̂H,u,H1,2,3 − P H,u,H 1,2,3 ‖F ≤ (N, δ)\n‖P̂u1 − Pu1 ‖F ≤ (N, δ) ‖P̂u,u1,2 − P u,u 1,2 ‖F ≤ (N, δ)\n‖P̂u,π(u)1,1 − P u,π(u) 1,1 ‖F ≤ (N, δ)\n‖P̂u,π(u),u2,2,1 − P u,π(u),u 2,2,1 ‖F ≤ (N, δ) } Lemma 6 (Concentration of Raw Moments). P(E) ≥ 1− δ.\nProof. Applying Proposition 19 in [18] along with union bound."
    }, {
      "heading" : "E.2 Subspace Concentration",
      "text" : "Next we state a useful lemma that says that conditioned on the event E, performing an SVD on the empirical version of Pu,u1,2 = E[xu1 ⊗ xu2 ] gives us a good approximation to the range of Ou. Recall that Uu is a matrix whose columns form an orthonormal basis of Ou, and define UH is ⊗v∈HUu. Also, recall for a matrix U with orthonormal columns, the projection matrix onto range(U) is UU>.\nLemma 7 (Subspace Concentration). Supposes N is large enough such that Assumption 3 holds. Ûu is the output of line 3 of Algorithm 1. Let u be a node in V , recall that H is the set of nodes along the path from root r to u. Then conditioned on event E, we have: (1) ‖Uu(Uu)> − Ûu(Ûu)>‖ ≤ 2 (N,δ)\nσm(P u,u 1,2 ) ."
    }, {
      "heading" : "In particular,",
      "text" : "‖Uu(Uu)> − Ûu(Ûu)>‖ ≤ min( minu∈V σm(P\nH,H 1,3 )\n8D ,\nminu∈V σm(O u)\n2 √ m\n)\n(2)\n‖UH(UH)> − ÛH(ÛH)>‖ ≤ minu∈V σm(P\nHu,Hu 1,3 )\n8 (3)\nσm((Û u)>Ou) ≥ σm(O\nu)\n2\nProof. (1) Φu, the matrix of principal angles between range(Ûu) and range(Uu), is such that\n‖ sin Φu‖\n≤ (N, δ) σm(P u,u 1,2 )− (N, δ)\n≤ 2 (N, δ) σm(P u,u 1,2 )\n(3)\nwhere the first inequality is by Theorem 4, by takingA = Pu,u1,2 and Ã = P̂ u,u 1,2 ; the second inequality from Assumption 3, which implies that (N, δ) ≤ σm(Pu,u1,2 )/2. Thus, by Equation (2) in Assumption 3,\n‖ sin Φu‖ ≤ min( minu∈V σm(P\nHu,Hu 1,3 )\n8D ,\nminu∈V σm(O u)\n2 √ m\n)\nThe result follows from the fact that\n‖ sin Φu‖ = ‖Uu(Uu)> − Ûu(Ûu)>‖\n(2) First we enumerate the nodes in Hu : Hu = {v1, . . . , vl}.\n‖UH(UH)> − ÛH(ÛH)>‖ ≤ ‖(Uv1(Uv1)> − Ûv1(Ûv1)>)⊗ . . .⊗ (Uvl(Uvl)>)|‖+ . . .+ ‖(Uv1(Uv1)>)⊗ . . .⊗ (Uvl(Uvl)> − Ûvl(Ûvl)>)‖ ≤ ‖Uv1(Uv1)> − Ûv1(Ûv1)>‖+ . . .+ ‖Uvl(Uvl)> − Ûvl(Ûvl)>‖\n≤ ∑ v∈H 2 (N, δ) σm(P v,v 1,2 )\n≤ minu∈V σm(P\nH,H 1,3 )\n8\nwhere the first inequality is by triangle inequality, the second inequality uses standard facts about Kronecker product (‖A ⊗ B‖ = ‖A‖‖B‖), the third inequality is from Equation (3), the fourth inequality is from Equation (2).\n(3) By item (1) we know that\n‖Uu(Uu)> − Ûu(Ûu)>‖ ≤ σm(Ou)/(2 √ m)\nHence\n‖Uu(Uu)>Ou − Ûu(Ûu)>Ou‖ ≤ ‖Uu(Uu)> − Ûu(Ûu)>‖‖Ou‖ ≤ σm(Ou)/2\nwhere the second inequality is from the fact that Ou is a column stochastic matrix, which implies that ‖Ou‖ ≤ ‖Ou‖F ≤ √ m. Therefore by Theorem 3,\nσm((Û u)>Ou) = σm(Û u(Ûu)>Ou) ≥ σm(Ou)/2"
    }, {
      "heading" : "E.3 Symmetrized Moment Concentration",
      "text" : "Lemma 8. Suppose we are given a set of matrices Ûu, u ∈ V such that (Ûu)>Ou is invertible for all u ∈ V . Moreover, assume the expected second order moments Pu,H2,3 , P H,H 1,3 , P u,H 2,1 , and third order moments PH,u,H1,2,3 are given. Consider the symmetrization matrices:\nS̃u1 = ((Û u)>Pu,H2,3 Û H)((ÛH)>PH,H1,3 Û H)−1\nS̃u3 = ((Û u)>Pu,H2,1 Û H)((ÛH)>PH,H3,1 Û H)−1\nand the ground truth symmetrized second order and third order cooccurence matrices be:\nMu2 = P H,u 1,2 (Û H(S̃u1 ) >, Ûu)\nMu3 = P H,u,H 1,2,3 (Û H(S̃u1 ) >, Ûu, ÛH S̃uT3 )\nThen, Mu2 = ∑ i πui ((Û u)>Ou)i ⊗ ((Ûu)>Ou)i\nMu3 = ∑ i πui ((Û u)>Ou)i ⊗ ((Ûu)>Ou)i ⊗ ((Ûu)>Ou)i\nProof. Recall that by Lemma 2\nOH1 = O Hdiag(ρH)(TH)>diag(πH)−1\nwhere diag(ρH)(TH)>diag(πH)−1 is invertible. Thus,\n(ÛH)>OH1 = (Û H)>OHdiag(ρH)(TH)>diag(πH)−1\nThis shows that (ÛH)>OH1 is invertible. On the other hand,\nOH3 = O HTH\nwhere TH is invertible. Thus,\n(ÛH)>OH3 = (Û H)>OHTH\nThis shows that (ÛH)>OH3 is invertible.\nTherefore,\nS̃u1\n= ((Ûu)>Ou2 diag(π H)OHT3 Û H)((ÛH)>OH1 diag(π H)OHT3 Û H)−1 = ((Ûu)>Ou2 )((Û H)>OH1 ) −1\nLikewise,\nS̃u3\n= ((Ûu)>Ou2 diag(π H)OHT1 Û H)((ÛH)>OH3 diag(π H)OHT1 Û H)−1 = ((Ûu)>Ou2 )((Û H)>OH3 ) −1\nThen,\nMu2 = P H,u 1,2 (U H(S̃u1 ) >, Ûu) = ∑\ni1,...,iD\nπHi1,...,iD ((Û u)>Ou2 )i1,...,iD ⊗ ((Ûu)>Ou2 )i1,...,iD\n= ∑\ni1,...,iD\nπHi1,...,iD ((Û u)>Ou)iD ⊗ ((Ûu)>Ou)iD\n= ∑ i πui ((Û u)>Ou)i ⊗ ((Ûu)>Ou)i\nMu3 = P H,u,H 1,2,3 (Û H(S̃u1 ) >, Ûu, ÛH S̃uT3 ) = ∑\ni1,...,iD\nπHi1,...,iD ((Û u)>Ou2 )i1,...,iD ⊗ ((Ûu)>Ou2 )i1,...,iD ⊗ ((Ûu)>Ou2 )i1,...,iD\n= ∑\ni1,...,iD\nπHi1,...,iD ((Û u)>Ou)iD ⊗ ((Ûu)>Ou)iD ⊗ ((Ûu)>Ou)iD\n= ∑ i πui ((Û u)>Ou)i ⊗ ((Ûu)>Ou)i ⊗ ((Ûu)>Ou)i\nWe next establish a result that shows that the symmetrization matrices Ŝu1 and Ŝ u 3 obtained in Line 7 of Algorithm 1 concentrate to S̃u1 and S̃ u 3 defined in Lemma 8. Recall from Algorithm 1 that:\nŜu1 = ((Û u)>P̂u,Hu2,3 Û Hu)((ÛHu)>P̂Hu,Hu1,3 Û Hu)−1, Ŝu3 = ((Û u)>P̂u,Hu2,1 Û Hu)((ÛHu)>P̂Hu,Hu3,1 Û Hu)−1\nLemma 9. Suppose N is large enough that Assumption 3 holds. Recall Ŝu1 and Ŝu3 are the outputs of line 7 in Algorithm 1\n, and S̃u1 and S̃ u 3 are defined in Lemma 8.\nConditioned on event E, the following hold for all u ∈ V .\n‖S̃u1 − Ŝu1 ‖, ‖S̃u3 − Ŝu3 ‖ ≤ 10 (N, δ)\nσmd(P H,H 1,3 ) 2\n‖S̃u1 ‖, ‖Ŝu1 ‖, ‖S̃u3 ‖, ‖Ŝu3 ‖ ≤ 4\nσmd(P H,H 1,3 )\nProof. (1) We first show that σmd((ÛH)>P̂ H,H 1,3 Û H) ≥ 3σmd(P H,H 1,3 )/4, and σmd((Û H)>PH,H1,3 Û H) ≥ σmd(P H,H 1,3 )/2. Under Assumption 3, by Item (2) of Lemma 7, we know that\n‖UH(UH)> − ÛH(ÛH)>‖ ≤ min u∈V σmd(P H,H 1,3 )/8 (4)\nAs a result,\n‖ÛH(ÛH)>PH,H1,3 ÛH(ÛH)> − P H,H 1,3 ‖\n= ‖ÛH(ÛH)>PH,H1,3 ÛH(ÛH)> − UH(UH)>P H,H 1,3 U H(UH)>‖ (5)\n≤ ‖(ÛH(ÛH)> − UH(UH)>)PH,H1,3 ÛH(ÛH)>‖\n+‖UH(UH)>PH,H1,3 (ÛH(ÛH)> − UH(UH)>)‖\n≤ ‖(ÛH(ÛH)> − UH(UH)>)‖‖PH,H1,3 ‖‖ÛH(ÛH)>‖\n+‖UH(UH)>‖‖PH,H1,3 ‖‖(ÛH(ÛH)> − UH(UH)>)‖\n≤ σm(PH,H1,3 )/8 + σm(P H,H 1,3 )/8\n≤ σm(PH,H1,3 )/4 (6) where the first inequality is by triangle inequality, in the second inequality we use the fact that ‖A · B‖ ≤ ‖A‖‖B‖, the third inequality is from the fact that ‖PH,H1,3 ‖ ≤ ‖P H,H 1,3 ‖F ≤ 1, ‖ÛH(ÛH)>‖ = 1, ‖UH(UH)>‖ = 1 and Equation (4). Therefore,\nσmd((Û H)>PH,H1,3 Û H)\n= σmd(Û H(ÛH)>PH,H1,3 Û H(ÛH)>)\n≥ σmd(P H,H 1,3 )− ‖ÛH(ÛH)>P H,H 1,3 Û H(ÛH)> − PH,H1,3 ‖\n≥ 3σmd(P H,H 1,3 )/4 (7)\nwhere the first inequality is by Theorem 3, the second inequality is by Equation 6. In the meantime,\n‖(ÛH)>PH,H1,3 ÛH − (ÛH)>P̂ H,H 1,3 Û H‖\n≤ ‖PH,H1,3 − P̂ H,H 1,3 ‖\n≤ (N, δ) ≤ σdm(P H,H 1,3 )/4 (8)\nwhere in the first inequality we use the fact that ‖ÛH‖ = 1, the second inequality is by the fact that if E happens, ‖PH,H1,3 − P̂ H,H 1,3 ‖ ≤ (N, δ), the third inequality follows from Assumption 3.\nTherefore\nσmd((Û H)>P̂H,H1,3 Û H)\n≥ σmd((ÛH)>P H,H 1,3 Û H)− ‖(ÛH)>PH,H1,3 ÛH − (ÛH)>P̂ H,H 1,3 Û H‖\n≥ σm(PH,H1,3 )/2 where the first inequality is from Theorem 3, the second inequality is from Equation (8).\nWe now have\n‖S̃u1 − Ŝu1 ‖ = ‖((Ûu)>Pu,H2,3 ÛH)((ÛH)>P H,H 1,3 Û H)−1 − ((Ûu)>P̂u,H2,3 ÛH)((ÛH)>P̂ H,H 1,3 Û H)−1‖\n≤ ‖((Ûu)>(Pu,H2,3 − P̂ u,H 2,3 )Û H)((ÛH)>PH,H1,3 Û H)−1‖\n+‖((Ûu)>P̂u,H2,3 ÛH)(((ÛH)>P H,H 1,3 Û H)−1 − ((ÛH)>P̂H,H1,3 ÛH)−1‖\n≤ ‖Ûu(Pu,H2,3 − P̂ u,H 2,3 )Û H‖‖((ÛH)>PH,H1,3 ÛH)−1‖\n+‖(Ûu)>P̂u,H2,3 ÛH‖‖((ÛH)>P H,H 1,3 Û H)−1 − ((ÛH)>P̂H,H1,3 ÛH)−1‖\n≤ 2 (N, δ) σmd(P H,H 1,3 ) + 8 (N, δ) σmd(P H,H 1,3 ) 2\n≤ 10 (N, δ) σmd(P H,H 1,3 ) 2 (9)\nIn the derivation above, the first inequality uses triangle inequality and the second inequality repeatedly uses the fact that ‖A ·B‖ ≤ ‖A‖‖B‖. The third inequality is obtained by bounding each term individually as follows:\n‖(Ûu)>(Pu,H2,3 − P̂ u,H 2,3 )Û H‖ ≤ ‖Pu,H2,3 − P̂ u,H 2,3 ‖ ≤ ‖P u,H 2,3 − P̂ u,H 2,3 ‖F ≤ (N, δ)\n‖((ÛH)>PH,H1,3 ÛH)−1‖ = 1/σmd((ÛH)>P H,H 1,3 Û H) ≤ 2/σmd(P H,H 1,3 )\n‖(Ûu)>P̂u,H2,3 ÛH‖ ≤ ‖P̂ u,H 2,3 ‖ ≤ ‖P̂ u,H 2,3 ‖F ≤ 1\n‖((ÛH)>PH,H1,3 ÛH)−1 − ((ÛH)>P̂ H,H 1,3 Û H)−1‖\n≤ 2‖(ÛH)>(PH,H1,3 − P̂ H,H 1,3 )Û H‖max(‖((ÛH)>P̂H,H1,3 ÛH)−1‖, ‖((ÛH)>P H,H 1,3 Û H)−1‖)\n≤ 8 (N, δ) σmd(P H,H 1,3 ) 2\nwhere the last inequality follows from Theorem 5.\nThe bound of ‖S̃u3 − Ŝu3 ‖ is handled similarly. (2) First,\n‖S̃u1 ‖ ≤ ‖(Ûu)>P u,H 2,1 Û H‖‖((ÛH)>PH,H3,1 ÛH)−1‖ ≤ 2\nσmd(P H,H 1,3 )\nwhere the first inequality is by the fact that ‖A · B‖ ≤ ‖A‖‖B‖, the second inequality is by Equation (7).\nMeanwhile, Assumption 3 implies (N, δ) ≤ σmd(P H,H 1,3 )/5, therefore from Equation (9),\n‖Su1 − Ŝu1 ‖ ≤ 2\nσmd(P H,H 1,3 )\nHence by triangle inequality,\n‖Ŝu1 ‖ ≤ 4\nσmd(P H,H 1,3 )\nThe bounds of ‖S̃u3 ‖ and ‖Ŝu3 ‖ are handled similarly.\nBuilt upon the previous two lemmas, we next provide a result regarding the concentration of symmetrized moments.\nLemma 10. Suppose N is large enough that Assumption 3 holds. Let u be a node in V . Then on the event E, the following hold.\n‖Mu2 − M̂u2 ‖ ≤ 14 (N, δ)\nσmd(P H,H 1,3 ) 2\n‖Mu3 − M̂u3 ‖ ≤ 96 (N, δ)\nσmd(P H,H 1,3 ) 3\nProof. (1) Define Pu = PH,u1,2 (Û H , Ûu) and P̂u = P̂H,u1,2 (Û H , Ûu). Then,\n‖Mu2 − M̂u2 ‖ = ‖Pu((S̃u1 )>, I)− P̂u((Ŝu1 )>, I)‖ ≤ ‖(Pu − P̂u)((S̃u1 )>, I)‖+ ‖P̂u((S̃u1 )> − (Ŝu1 )>, I)‖ ≤ ‖Pu − P̂u‖‖S̃u1 ‖+ ‖P̂u‖‖S̃u1 − Ŝu1 ‖\n≤ 4 (N, δ) σmd(P H,H 1,3 ) + 10 (N, δ) σmd(P H,H 1,3 ) 2 ≤ 14 (N, δ) σmd(P H,H 1,3 ) 2 (10)\nwhere the first inequality is by triangle inequality, the second inequality is by the fact that ‖M(A,B)‖ ≤ ‖M‖‖A‖‖B‖, the third inequality is from the fact that ‖Pu − P̂u‖ ≤ ‖PH,u1,2 − P̂H,u1,2 ‖ ≤ ‖P H,u 1,2 − P̂ H,u 1,2 ‖F ≤ (N, δ) and ‖P̂u‖ ≤ ‖P̂ H,u 1,2 ‖ ≤ ‖P̂ H,u 1,2 ‖F ≤ 1, and Lemma 9.\nAs a result,\n‖Mu2 − M̂u2 ‖ = ‖(Pu((S̃u1 )>, I)> + Pu((S̃u1 )>, I))/2− (P̂u(Ŝu1 , I)> + P̂u(Ŝu1 , I))/2‖ ≤ ‖Pu((S̃u1 )>, I)> − P̂u((Ŝu1 )>, I)>‖/2 + ‖Pu((S̃u1 )>, I)− P̂u((Ŝu1 )>, I)‖/2\n≤ 14 (N, δ) σmd(P H,H 1,3 ) 2\nwhere the first inequality follows from triangle inequality, the second inequality is from Equation (10).\n(2) Define Tu = PH,u,H1,2,3 (Û H , Ûu, ÛH) and T̂u = P̂H,u,H1,2,3 (Û H , Ûu, ÛH). Then,\n‖Mu3 − M̂u3 ‖ = ‖Tu((S̃u1 )>, I, (S̃u3 )>)− T̂u((Ŝu1 )>, I, (Ŝu3 )>)‖ ≤ ‖Tu − T̂u‖‖S̃u1 ‖‖S̃u3 ‖+ ‖T̂u‖‖S̃u1 − Ŝu1 ‖‖S̃u3 ‖+ ‖T̂u‖‖Ŝu1 ‖‖S̃u3 − Ŝu3 ‖\n≤ 16 (N, δ) σmd(P H,H 1,3 ) 2 + 10 (N, δ) σmd(P H,H 1,3 ) 2\n4\nσmd(P H,H 1,3 )\n+ 4\nσmd(P H,H 1,3 )\n10 (N, δ)\nσmd(P H,H 1,3 ) 2\n≤ 96 (N, δ) σmd(P H,H 1,3 ) 3\nwhere the first inequality is from triangle inequality, and the fact that ‖T (A,B,C)‖ ≤ ‖T‖‖A‖‖B‖‖C‖, the second inequality is by the fact that ‖Tu − T̂u‖ ≤ ‖PH,u,H1,2,3 − P̂ H,u,H 1,2,3 ‖ ≤ ‖PH,u,H1,2,3 − P̂ H,u,H 1,2,3 ‖F ≤ (N, δ), ‖T̂u‖ ≤ ‖P̂ H,u,H 1,2,3 ‖ ≤ 1, and Lemma 9, the third inequality is by algebra."
    }, {
      "heading" : "E.4 Accucary of Tensor Decomposition",
      "text" : "Algorithm 3 A Procedure That Finds Symmetric Decomposition based on Second and Third Order Moments\n1: Input: number of components m, perturbed version M̂2 and M̂3 of matrix M2 and tensor M3 satisfying M2 = ∑m i=1 πiθi ⊗ θi, M3 = ∑m i=1 πiθi ⊗ θi ⊗ θi 2: Output: {θ̂i}mi=1, estimate of {θi}mi=1 3: Whiten. Perform an SVD on M̂2 = ÛD̂Û>, and let Ŵ = ÛmD̂ −1/2 m (where Ûm is matrix that\ncontains the firstm columns of Û , D̂m is the diagonal matrix with D̂’s first m diagonal entries), let Ĝ = M̂3(Ŵ , Ŵ , Ŵ ). 4: Decompose Tensor. Apply robust tensor power iteration algorithm in [1] with input Ĝ to get {v̂1, . . . , v̂m} 5: for i = 1, 2, . . . ,m do 6: Let Ẑi = 1T̂ (v̂i,v̂i,v̂i) . 7: Recover θ̂i = (Ŵ>)†v̂i Ẑi 8: end for\nIn this section, we introduce a lemma that is implicit in [1] regarding using orthogonal decomposition as a subprocedure for full rank symmetric tensor decomposition. (See Theorem 5.1 of [1].) For completeness, we include the proof here.\nLemma 11. There are universal constants c1, c2 such that the following holds. Suppose a matrix M2 and a tensor M3 has the following structure:\nM2 = m∑ i=1 πiθi ⊗ θi\nM3 = m∑ i=1 πiθi ⊗ θi ⊗ θi\nwhere πi > 0 for all i. And we are given their perturbed version M̂2 and M̂3, such that\n‖M̂2 −M2‖ ≤ EP\n‖M̂3 −M3‖ ≤ ET where\nEP ≤ σm(Θ)2πmin/2 (11)\nc1( ET\nσm(Θ)3 + EP σm(Θ)2 ) 1\nπ 3/2 min\n≤ 1 m\n(12)\nwhere Θ = (θ1, . . . θm) and πmin = mini πi. Then the outputs {θi}mi=1 of Algorithm 3 on input M̂2 and M̂3 satisfies the following. With appropriate setting of parameters (with respect to parameter η), with probability 1− η, there is a permutation σ : [m]→ [m] such that\n‖θi − θ̂σ(i)‖ ≤ c2 σ1(Θ)\nπ2min ( EP σm(Θ)2 + ET σm(Θ)3 )\nProof. 1. We first put Θ into canonical forms by appropriate scaling of its columns. Let Θ̃ = (θ̃1, . . . , θ̃m) = Θdiag(π) 1 2 , we have\nM2 = m∑ i=1 θ̃i ⊗ θ̃i\nM3 = m∑ i=1 1 √ πi θ̃i ⊗ θ̃i ⊗ θ̃i\nRecall that Ŵ is defined as ÛmD̂ − 12 m , where M̂2 = ÛD̂Û>. Hence Ŵ>M̂2Ŵ = Im. Suppose that Ŵ>M2Ŵ has the following eigendecomposition:\nŴ>M2Ŵ = AΛA >\nThen let W = ŴAΛ− 1 2A>, W is one of the matrices such that W>M2W = Im. Define M = W>Θ̃, M̂ = Ŵ>Θ̃.\n2. If Equation (11) holds, then Ep ≤ σm(Θ)2πmin/2 ≤ σm(M2)/2, then we have the following:\n‖W‖, ‖Ŵ‖ ≤ 2 σm(Θ̃)\n‖W †‖, ‖Ŵ †‖ ≤ 3σ1(Θ̃)\n‖W † − Ŵ †‖ ≤ 6σ1(Θ̃) σm(Θ̃)2 EP\n‖ΘΘ† −WW †‖ ≤ 4EP σm(Θ̃)\n‖M‖, ‖M̂‖ ≤ 2\n‖M − M̂‖ ≤ EP σm(Θ̃)2\n3. Define G = M3(W,W,W ) = ∑ i 1√ πi Mi ⊗Mi ⊗Mi, and recall that Ĝ = M̂3(Ŵ , Ŵ , Ŵ ). We\nhave the following perturbation bound for Ĝ. Define R to be diagnoal tensor ∑ i\n1√ πi ei ⊗ ei ⊗ ei.\nNote that ‖R‖ ≤ 1√πmin . Therefore,\n‖G− Ĝ‖ = ‖M3(W,W,W )− M̂3(Ŵ , Ŵ , Ŵ )‖ ≤ ‖(M3 − M̂3)(Ŵ , Ŵ , Ŵ )‖+ ‖M3(W − Ŵ ,W,W )‖+ ‖M3(Ŵ ,W − Ŵ ,W )‖+ ‖M3(Ŵ , Ŵ ,W − Ŵ )‖ = ‖(M3 − M̂3)(Ŵ , Ŵ , Ŵ )‖+ ‖R(M − M̂,M,M)‖+ ‖R(M̂,M − M̂,M)‖+ ‖R(M̂, M̂ ,M − M̂)‖ ≤ ‖M3 − M̂3‖‖W‖3 + ‖R‖‖M − M̂‖‖M‖2 + ‖R‖‖M̂‖‖M‖‖M − M̂‖+ ‖R‖‖M̂‖2‖M − M̂‖\n≤ 8ET σm(Θ̃)3 + 12EP √ πminσm(Θ̃)2 := E (13)\nwhere the first inequality is by triangle inequality, the second inequality is by the fact that ‖T (A,B,C)‖ ≤ ‖T‖‖A‖‖B‖‖C‖, the third inequality is from results of our step 2 and the fact that ‖M̂3 −M3‖ ≤ ET .\n4. If Equation (12) holds, then E ≤ C1m ≤ C1 mini π\n−1/2 i\nm for C1 required by Theorem 5.1 in [1]. Thus, applying robust tensor power algorithm in [1], with probability at least 1 − η, there exist a permutation σ : [m]→ [m] such that\n‖Mi − v̂σ(i)‖ ≤ 8 √ πiE (14)\n5. We conclude by providing the reconstruction error bound. For notational simplicity, assume σ(·) is identity mapping. Define\nZi = 1\nM3(WMi,WMi,WMi) =\n1 G(Mi,Mi,Mi) = √ πi\nand recall that\nẐi = 1\nM̂3(Ŵ v̂i, Ŵ v̂i, Ŵ v̂i) =\n1\nĜ(v̂i, v̂i, v̂i)\nThe recovery formula is\nθ̂i = (Ŵ>)†v̂i\nẐi\nFirst, | 1Zi − 1 Ẑi | can be bounded as follows:\n| 1 Zi − 1 Ẑi |\n= |G(Mi,Mi,Mi)− Ĝ(v̂i, v̂i, v̂i)| ≤ |(G− Ĝ)(v̂i, v̂i, v̂i)|+ |G(Mi − v̂i, v̂i, v̂i)|+ |G(Mi,Mi − v̂i, v̂i)|+ |G(Mi,Mi,Mi − v̂i)| ≤ ‖G− Ĝ‖‖Mi‖3 + ‖G‖‖Mi − v̂i‖‖v̂i‖2 + ‖G‖‖Mi − v̂i‖‖v̂i‖‖Mi‖+ ‖G‖‖Mi‖2‖Mi − v̂i‖ ≤ E + 3 πi√ πmin E ≤ 4 πi√ πmin E\nwhere the first inequality is by triangle inequality, the second inequality is by the fact that ‖A ·B‖ ≤ ‖A‖‖B‖, the third inequality is by Equation (13) in step 3 and Equation (14) in step 4, the fourth inequality is by algebra.\nThen the reconstruction error can be bounded as follows:\n‖θi − (Ŵ †)>v̂i\nẐi ‖\n≤ ‖θi − (W †)>Mi\nZi ‖+ ‖W †(Mi − v̂i) Zi ‖+ ‖ (W † − Ŵ †)v̂i Zi ‖+ ‖( 1 Zi − 1 Ẑi )Ŵ †v̂i‖\n≤ ‖ΘΘ† −WW †‖‖θi‖+ ‖W †‖ Zi ‖Mi − v̂i‖+ ‖W † − Ŵ †‖ Zi ‖v̂i‖+ | 1 Zi − 1 Ẑi |‖Ŵ †‖‖v̂i‖\n≤ ‖ΘΘ† −WW †‖ σ1(Θ̃)√ πmin + ‖W †‖ Zi ‖Mi − v̂i‖+ ‖W † − Ŵ †‖ Zi + | 1 Zi − 1 Ẑi |‖Ŵ †‖\n≤ 4EP σm(Θ̃)2 σ1(Θ̃)√ πmin + 24σ1(Θ̃)E + 6σ1(Θ̃) σm(Θ̃)2 EP √ πi + 12 √ πmin E √ πiσ1(Θ̃)\n≤ 46σ1(Θ̃)√ πmin ( 8ET σm(Θ̃)3 + 12EP √ πminσm(Θ̃)2 )\n≤ c2 σ1(Θ)\nπ2min ( EP σm(Θ)2 + ET σm(Θ)3 )\nWher the first inequality is by triangle inequality, the second inequality we use the fact that ‖A·B‖ ≤ ‖A‖‖B‖ and the fact that Mi = W>θi √ πi, Zi = √ πi, ΘΘ†θi = θi, WW † = (W †)>W>, the third inequality uses the fact that ‖θi‖ = ‖Θ̃ei‖/ √ πmin ≤ σ1(Θ̃)/ √ πmin and ‖v̂i‖ = 1, in the fourth inequality we use results in item 2 and item 4, the fifth inequality is from the definiton of E and algebra, in the sixth inequality we use the fact that σm(Θ) ≤ σm(Θ̃)π−1/2min and letting c2 = 552.\nNow we apply the above lemma into our symmetrized cooccurence matrices M̂2 and M̂3. Corollary 2. Suppose N is large enough such that Assumption 3 holds. Then, on event E, with probability 0.9 over the randomization of D calls of Algorithm 3, for all u ∈ V , the matrices Θ̂u = (θ̂u1 , . . . , θ̂ u m) obtained at the end of line 9 are such that there exists a permutation matrix Π u,\n‖(Ûu)>Ou − Θ̂uΠu‖ ≤ 2c2 m\n(πumin) 2\n(N, δ)\nσmd(P H,H 1,3 ) 3σm(Ou)3\nProof. By Assumption 3, we first see that conditioned on event E, by Lemma 9, σm(ÛuTO u\n) ≥ σm(O\nu)/2. Thus the conditions of Lemma 11 hold, by taking Θ = (Ûu)>Ou, π = πu. We thus get that with probability greater than 1 − 0.1/D over the randomness of Algorithm 1, there is a permutation matrix Πu such that for all i = 1, 2, . . . ,m,\n‖(Ûu)>Oui − (Θ̂uΠu)i‖\n≤ c2 σ1((Û\nu)>Ou)\n(πumin) 2\n( (N, δ)\nσmd(P H,H 1,3 ) 2σm(Ou)2 +\n(N, δ)\nσmd(P H,H 1,3 ) 3σm(Ou)3 )\n≤ 2c2 √ m\n(πumin) 2\n(N, δ)\nσmd(P H,H 1,3 ) 3σm(Ou)3\nwhere the second inequality we use the fact that σ1((Ûu)>Ou) = ‖(Ûu)>Ou‖ ≤ ‖Ou‖ ≤ √ m, since Ou is a column stochastic matrix. Therefore,\n‖(Ûu)>Ou − (Θ̂uΠu)‖ ≤ ‖(Ûu)>Ou − (Θ̂uΠu)‖F\n≤ 2c2 m\n(πumin) 2\n(N, δ)\nσmd(P H,H 1,3 ) 3σm(Ou)3 (15)\nWe conclude the proof by applying union bound over all u ∈ V ."
    }, {
      "heading" : "F Putting Everything Together – Proof of Theorem 2",
      "text" : "Proof. (Of Theorem 2) (1) We first give the recovery accuracy of observation matrices. The final step of recovery is Ôu = ÛuΘ̂u. Note that if N is at least C max( D 2\nσ22σ 2 3 ln Dδ , m σ21σ 2 2 ln Dδ , m2 σ61σ 6 3π 3 min ln Dδ ), then Assumption 3 holds, hence conditioned on event E, we have\n‖Ûu(Ûu)>Ou −Ou‖ = ‖Ûu(Ûu)>Ou − Ûu(Uu)>Ou‖ ≤ ‖Ûu(Ûu)> − Ûu(Uu)>‖‖Ou‖ ≤ 2 √ m (N, δ)\nσm(P u,u 1,2 )\n(16)\nwhere the first inequality is by the fact that ‖A ·B‖ ≤ ‖A‖‖B‖, the second inequality follows from the fact that ‖Ou‖ ≤ √ m and item (1) of Lemma 7.\nMeanwhile, by Corollary 2, we have\n‖Ûu(Ûu)>Ou − ÛuΘ̂uΠu‖ ≤ ‖(Ûu)>Ou − Θ̂uΠu‖\n≤ 2c2 m\n(πumin) 2\n(N, δ)\nσmd(P H,H 1,3 ) 3σm(Ou)3\nThe above two facts let us conclude that provided the size of sample N is at least C max( m\nσ22σ 8 1\n2 ln D δ ,\nm2 σ63σ 14 1 π 4 min 2 ln D δ ) (where we choose C large enough),\n‖Ou − ÔuΠu‖ ≤ ‖Ûu(Ûu)>Ou −Ou‖+ ‖Ûu(Ûu)>Ou − ÛuΘ̂uΠu‖ ≤ 2 √ m (N, δ)\nσm(P u,u 1,2 )\n+ 2c2 m\n(πumin) 2\n(N, δ)\nσmd(P H,H 1,3 ) 3σm(Ou)3\n≤ min v∈V σm(O v)4 /32 (17) ≤\nwhere the first inequality is by triangle inequality, the second inequality is by Equations (15) and (16), the third inequality follows from the choice of N , in the last inequality we use the fact that σm(Ou) ≤ 1. Therefore by Equation (17) and Theorem 3,\nσm(Ô uΠu) ≥ σm(Ou)−min\nv∈V σm(O\nv)4 /32 ≥ σm(Ou)/2 (18)\n(2) We now provide guarantees on the accuracy of transition probabilities and initial probabilities. In particular, we prove ‖Q̂u −Qu(Πu,Πu,Ππ(u))‖ ≤ , the other three inequalities can be handled similarly. As we have already seen from Equation (17), for all u ∈ V ,\n‖(Ou)T† − (ÔuΠu)T†‖ ≤ 2 max(‖(Ôu)†‖2, ‖(ΠuT (Ou))†‖2)‖Ou − ÔuΠu‖ ≤ min\nv∈V σm(O\nv)2 /16\nwhere the first inequality is by Theorem 5, the second inequality uses the fact that ‖(Ôu)†‖ = 1/σm(O u), ‖(ÔuΠu)†‖ = 1/σm(ÔuΠu) and Equation (18).\nConditioned on event E, by the choice of N , it is also true that the cooccurence tensor P̂u,π(u),u2,2,1 is such that\n‖P̂u,π(u),u2,2,1 − P u,π(u),u 2,2,1 ‖ ≤ min\nv∈V σm(O\nv)3 /32 (19)\nTherefore,\n‖Qu − Q̂u(Πu,Ππ(u),Πu)‖ = ‖Pu,π(u),u2,2,1 ((Ou)T†, (Oπ(u))T†, (Ou)T†)− P̂ u,π(u),u 2,2,1 ((Ô uΠu)T†, (Ôπ(u)Ππ(u))T†, (ÔuΠu)T†)‖\n≤ ‖(Pu,π(u),u2,2,1 − P̂ u,π(u),u 2,2,1 )((O uΠu)T†, (Oπ(u)Ππ(u))T†, (OuΠu)T†)‖\n+‖P̂u,π(u),u2,2,1 ((Ou)T† − (ÔuΠu)T†, (Oπ(u))T†, (Ou)T†)‖\n+‖P̂u,π(u),u2,2,1 ((ÔuΠu)T†, (Oπ(u))T† − (Ôπ(u)Ππ(u))T†, (Ôu)T†)‖\n+‖P̂u,π(u),u2,2,1 ((ÔuΠu)T†, (Ôπ(u)Ππ(u))T†, (Ou)T† − (ÔuΠu)T†)‖\n≤ ‖(Pu,π(u),u2,2,1 − P̂ u,π(u),u 2,2,1 )‖max v∈V ‖Ov†‖3 + ‖P̂u,π(u),u2,2,1 ‖ · ‖(Ou)T† − (ÔuΠu)T†‖(max v∈V ‖Ov†‖2 +\nmax v∈V ‖Ov†‖max v∈V ‖Ôv†‖+ max v∈V ‖Ôv†‖2)\n≤\nwhere the first inequality is by triangle inequality, the second inequality is by the fact that ‖T (A,B,C)‖ ≤ ‖T‖‖A‖‖B‖‖C‖, the third inequality is by Equations (19) and (17)."
    }, {
      "heading" : "G Matrix Perturbation Lemmas",
      "text" : "Theorem 3 (Weyl’s Theorem). If A, E are matrices in Rm×n with m ≥ n. Then,\n|σi(A+ E)− σi(A)| ≤ ‖E‖\nTheorem 4 (Wedin’s Theorem). If A, E are matrices in Rm×n with m ≥ n. Let A have singular value decomposition:  U>1U>2\nU>3\nA ( V1 V2 ) = ( Σ1 00 Σ2 0 0 )"
    }, {
      "heading" : "Let Ã = A+ E have the singular value decomposition: Ũ>1Ũ>2",
      "text" : "Ũ>3  Ã ( Ṽ1 Ṽ2 ) =  Σ̃1 00 Σ̃2 0 0  If there is δ > 0, α > 0 such that mini σi(Σ̃1) ≥ α+ δ, maxi σi(Σ2) ≤ α, then\n‖ sin Φ‖ ≤ ‖E‖ δ\nwhere Φ is the matrix of principal angles between range(U1) and range(Ũ1).\nTheorem 5. If A, E are matrices in Rm×n with m ≥ n, let Ã = A+ E. Then,\n‖Ã† −A†‖ ≤ 2 max(‖Ã†‖2, ‖A†‖2)‖E‖"
    }, {
      "heading" : "H Compressed observation matrices produced by Spectral-Tree for eight",
      "text" : "ENCODE cell types"
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2015,
    "abstractText" : "We develop a latent variable model and an efficient spectral algorithm motivated<lb>by the recent emergence of very large data sets of chromatin marks from multiple<lb>human cell types. A natural model for chromatin data in one cell type is a Hidden<lb>Markov Model (HMM); we model the relationship between multiple cell types by<lb>connecting their hidden states by a fixed tree of known structure.<lb>The main challenge with learning parameters of such models is that iterative meth-<lb>ods such as EM are very slow, while naive spectral methods result in time and<lb>space complexity exponential in the number of cell types. We exploit properties<lb>of the tree structure of the hidden states to provide spectral algorithms that are<lb>more computationally efficient for current biological datasets. We provide sample<lb>complexity bounds for our algorithm and evaluate it experimentally on biological<lb>data from nine human cell types. Finally, we show that beyond our specific model,<lb>some of our algorithmic ideas can be applied to other graphical models.",
    "creator" : "LaTeX with hyperref package"
  }
}