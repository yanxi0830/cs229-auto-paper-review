{
  "name" : "1612.00583.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Active Search for Sparse Signals with Region Sensing",
    "authors" : [ "Yifei Ma", "Roman Garnett", "Jeff Schneider" ],
    "emails" : [ "yifeim@cs.cmu.edu", "garnett@wustl.edu", "schneide@cs.cmu.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "1 Introduction Active search describes the problem where an agent is given a target to search for in an unknown environment and actively makes data-collection decisions so as to locate the target as quickly as possible. Examples of this setting include using aerial robots to detect gas leaks, radiation sources, and human survivors of disasters. The statistical principles for efficient designs of measurements date back to Gergonne (1815), but the growing trend to apply automated search systems in a variety of environments and with a variety of constraints has drawn much research attention recently, due to the need to address the disparate aspects of new applications.\nOne possibility in such active search scenarios we aim to explore, inspired by the robotic aerial search setting but with statistical insights that we hope to generalize, is the opportunity to take aggregate measurements that summarize\nCopyright c© 2017, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\nlarge contiguous regions of space. For example, an aerial robot carrying a radiation sensor will sense a region of space whose area depends on its altitude. How can such a robot dynamically trade off the ability to make noisier observations of larger regions of space against making higher-fidelity measurements of smaller regions?\nTo simplify the discussion, we will limit such region sensing observations to reveal the average value of an underlying function on a rectangular region of space, corrupted by independent observation noise. Noisy binary search is a simple realization of active search using such an observation scheme. This mechanism turns out to be sufficiently informative in the cases that we analyze to offer insights into a variety of search problems.\nThe ability to make aggregate region measurements in noisy environments has rarely been considered in previous work. Bayesian optimization, which has been used for localization of sparse signals (Carpin et al. 2015; Ma et al. 2015; Hernández-Lobato, Hoffman, and Ghahramani 2014; Jones, Schonlau, and Welch 1998), usually considers only point measurements of an objective function. Notice that point observations can be considered in our framework if the allowed region sensing actions are constrained to be arbitrarily small. On the other extreme, compressive sensing (Donoho 2006; Candès and Wakin 2008; Wainwright 2009), considers scenarios where every measurement can reveal information about the entire environment through linear projection with arbitrary coefficients. This is not always a realistic assumption, as for example for an aerial robot, which can only sense its immediate vicinity. Between the two extremes, Jedynak, Frazier, and Sznitman (2012); Rajan et al. (2015); Haupt et al. (2009); Carpentier and Munos (2012); Abbasi-Yadkori (2012); Yue and Guestrin (2011) considered policies for search where observations can be made on any arbitrary subset of the search space, including discontiguous subsets, which is also often incompatible with the constraints in physical search systems.\nAnother assumption we make, common for example in compressive sensing, is sparsity. We assume that there are only a small number of strong signals in the environment; our goal is to recover these signals. Sparsity is necessary for the definition of active search problems; otherwise, for dense or weak signals, there is usually no better search approach than simply exhaustively mapping the entire space.\nIn addition to applicability in real search settings, spar-\nar X\niv :1\n61 2.\n00 58\n3v 1\n[ st\nat .M\nL ]\n2 D\nec 2\n01 6\nsity has unique mathematical properties when considered alongside region sensing. In unconstrained sensing, AriasCastro, Candes, and Davenport (2013) discovered a paradox that active compressive sensing (that is, the ability to adaptively select observations based on previously collected data) does not improve detection efficiency beyond logarithmic terms over random compressive sensing. This limitation is seen also when considering theoretical detection rates for active compressive sensing methods (Abbasi-Yadkori 2012; Carpentier and Munos 2012; Haupt et al. 2009). However, we show that active learning can in fact offer significant improvements in detection rates when observations are constrained to contiguous regions.\nWe propose an algorithm we call Region Sensing Index (RSI) that actively collects data to search for sparse signals using only noisy region sensing measurements. RSI is based on greedy maximization of information gain. Although information gain is a classic principle, we believe that its use in the recovery of sparse signals is novel and a good fit for robotic applications. We show that RSI uses Õ(n/µ2 + k2) measurements to recover all of k true signal locations with small Bayes error, where µ and n are the signal strength and the size of the search space, respectively (Theorem 3). The number of measurements with RSI is comparable with the rates offered by unconstrained compressive sensing, even though our constraints seem strong (i.e., region sensing loses all spatial resolution inside the region of measurement). Furthermore, we show that all passive designs under our contiguous region sensing constraint in 1d search spaces are fundamentally worse, with efficiency no better than sequential scanning of every point location, however strong the signals are. These results provide evidence to promote the use of and research into active methods."
    }, {
      "heading" : "1.1 Related Work",
      "text" : "Arias-Castro, Candes, and Davenport (2013) proved that the minimax sample complexity1 for any (i.e., potentially adaptive) algorithm to recover k sparse signal locations is at least Ω(n/µ2), analyzing the problem in terms of the mean-squared error in the recovery of the underlying signal values. The authors also showed that a passive random design, combined with a nontrivial inference algorithm, e.g., Lasso (Wainwright 2009) or the Dantzig selector (Candes and Tao 2007), can have similar recovery rates (up to O(log n) terms). This result was presented as a paradox, suggesting that the folk statement that active methods have better sample complexity is not always true. Here we show that active search can make a substantial difference in recovery rates when the measurements are subject to the physically plausible constraint of region sensing, especially if the physical space has low dimensions.\nMalloy and Nowak (2014) presented the first active search algorithm that achieves the minimax sample complexity for general k. The algorithm is called Compressive Adaptive Sense and Search (CASS) and it can be adapted to region sensing in one-dimensional physical spaces. CASS directly\n1Sample complexity is equivalent to the number of measurements.\nextends bisection search, by allocating different sensing budgets to measurements at different bisection levels so as to minimize the cumulative error rates. However, CASS may fail if the repeated measurements of the same regions do not contain perfectly independent noise. It also has the limitation that it requires knowledge of the sensing budget apriori, yet produces no signal localization results until the very last measurements at the lowest level. Our paper addresses these practical issues with a redesigned active search algorithm using the Bayesian approach, which compares evidence instead of blindly trust the assumptions, and we use Shannon-information criteria, which implies bisection search in noiseless one-sparse cases.\nBraun, Pokutta, and Xie (2015) also used Shannoninformation criteria for active search but did not analyze their sample complexity under noisy measurements. Jedynak, Frazier, and Sznitman (2012); Rajan et al. (2015) studied a similar search problem where the “regions” are relaxed to any unions of disjoint subsets.\n2 Problem Formulation Consider a discrete space that is the Cartesian product of one-dimensional grids, X = ∏di=1[ni]; [n] = {1, . . . , n}. Let n = ∏ ni be the total number of points in X (here the product symbol is the arithmetic rather than the Cartesian product). We presume there is a latent real-valued nonnegative vector β ∈ Rn that represents the vector of true signals at all locations in X . We further assume that β is sparse: it has value µ > 0 on k n locations in X and has value 0 elsewhere. We consider making observations related to β through rectangular region sensing measurements, defined by\nyt = x > t β + εt, s.t. xtj = wt1j∈At , εt ∼ N (0, σ2t ). (1)\nHere xt ∈ Rn is a sensing vector that has support on At ⊆ X , a rectangular subset of X . We assume that the sensing vector has equal weight wt across its support. The resulting measurement, yt, is equal to the mean value of β on At corrupted by independent Gaussian noise with variance σ2t . Note that selecting At suffices to specify the measurement location.\nIn 1d search environments, At may be any interval of [n], and the corresponding design takes the form xt = (0, . . . , 0, wt, . . . , wt, 0, . . . , 0)\n>. In higher search dimensions, we consider only regions that are contained in a hierarchical spacial pyramid (Lazebnik, Schmid, and Ponce 2006), i.e., a sequence of increasingly finer grid boxes with dyadic side lengths to cover the space at multiple resolutions.\nOur goal is to choose a sequence of designs X = {xt}Tt=1 so as to discover the support of β with high confidence. Given a particular confidence, we will measure sample complexity by assuming ‖xt‖2 = 1 and σt ≡ 1 for each measurement and count the total number of measurements required to achieve that confidence, T . Letting ‖xt‖2 = 1 implies wt = 1/ √ ‖xt‖0, which can be seen as a relaxed notion of the region average, because the signal strength of a region measurement, which is µwt, still decreases as the region size ‖xt‖0 increases.\nAlgorithm 1 Region Sensing Index (RSI)\nRequire: π0(k, n, µ), T or , and the unknown β∗ Ensure: Ŝt // (5) 1: for t = 1, 2, . . . do 2: pick xt = arg maxx∈X I(β; y | x, πt−1) // (3)&(4) 3: observe yt = x>t β\n∗ + εt 4: update πt(β) ∝ πt−1(β)p(yt | β,xt−1) // (2) 5: find (̄t, Ŝt) = arg min|Ŝ|=k 1 kE [ |Ŝ∆S| | πt ] // (5) 6: break if t ≥ T or ̄t < , if either is defined\nThe measure of T is made to be comparable with another common choice of sample complexity, the Frobenius norm of the entire design ‖X‖2F , when the rows of X are normalized (Arias-Castro, Candes, and Davenport 2013). However, the normalization is often overlooked in classical compressive sensing, which allows algorithms to cheat in region sensing by making an enormous number of measurements of small weight and changing the sensing locations frequently. Another measure of complexity is to measure both ‖X‖2F and the number of location changes simultaneously (Malloy and Nowak 2014). However, our discretized counting of measurements is conceptually simpler.\nOur analysis is Bayesian and we will analyze performance in expectation, with prior β ∼ π0(β), a uniform distribution on the model class, Sµ ( n k ) , which includes all k-sparse mod-\nels with µ signal strength among n locations (i.e., it has ( n k ) possible outcomes). The Bayes risk will be measured by the expected Delta loss, ̄T = 1kE|S∆ŜT |, where ŜT is the best estimator of the k signal locations after T measurements and ∆ is the symmetric difference operator on a pair of sets.\n3 Proposed Methods We note that region sensing loses all spatial resolution inside the region of measurement. Here we borrow ideas from noisy binary search, which has a similar property, and use information gain (IG) to drive the observation process. We name our algorithm Region Sensing Index (RSI, Algorithm 1). Like other active learning algorithms, RSI is a combination of an inference subroutine that constantly updates the distribution of β using the collected data and a design subroutine that chooses the next region to sense based on the latest information from the inference subroutine. The inference subroutine. We use exact Bayesian inference with a uniform prior π0(β) on the model class Sµ ( n k ) . Denote the outcome of the first t measurements as Dt = {(xτ , yτ ) : 1 ≤ τ ≤ t}. Even though Dt contains a dependent sequence of data collections, where xτ depends onDτ−1,∀τ , Bayesian inference decomposes into a series of efficient updates:\nπ(β | Dt) ∝ π(β)p(Dt | β) = π0(β) ∏t τ=1 ( p(xτ | Dτ−1)p(yτ | β,xτ ) )\n∝ π0(β) ∏t τ=1 p(yτ | β,xτ ), (2)\nwhere p(xτ | Dτ−1) is the design without knowledge of the true β and thus dropped. Define πt(β) = π(β | Dt); the\nupdates have the form πt(β) ∝ πt−1(β)p(yt | β,xt) = πt−1(β)φ(yt − x>t β), where φ is the standard normal pdf. The design subroutine. The next sensing vector, xt+1 ∈ X , is chosen to maximize the IG:\nI(β; y | x, πt) = H(y | x, πt)− E [ H(y | x,β) | πt ] , (3)\nwhich is the difference between the entropy of the marginal distribution, p(y | x, πt) = ∫ φ(y − x>β)πt(β) dβ, and the expected entropy of the conditional distribution, p(y | β;x) = φ(y − x>β). The latter, i.e., the conditional distribution for any realization of β, has fixed entropy: log √ 2πe. Meanwhile, the marginal entropy has no closed-form solutions; instead, we use numerical integration.\nThe numerical integration is rather straightforward, because the marginal density function is analytical. From now on, we will assume that (x, A, a, wx) correspond to the same design (its sensing vector, its locations, its region size, and its sensing weight per coordinate, respectively). Define two new variables, λ = µwx(= µ/ √ a) and γ = x>β/λ, and one new parameter p = (p0, . . . , pk)> in (4). The goal is to change the variable of the integration for the marginal density function of y to:\np(y | x, πt) = ∫ πt(β)φ(y − x>β) dβ\n= ∑k\nc=0 pc φ(y − cλ) = p(y | λ,p),\nwhere pc = Pr(γ = c) = ∑\nβ:x>β=cλ πt(β). (4)\nNotice, γ only has a finite number of choices: γ = |A∩S| ∈ {0, . . . , k}, where S is the nonzero support of β, because both x and β are constant on their respective supports (xj = wx,∀j ∈ A and βj = µ,∀j ∈ S). We then numerically evaluate H(y | x, πt) = H(y | λ,p) with the obtained (4).\nThe Bayes estimator of signal locations. We pick the ksparse set ŜT to minimize the posterior risk:\nmin |Ŝ|=k\n1 k E [ |Ŝ∆S| | πT ] = 1 k ∑ ı̂∈Ŝ E ( 1{βı̂=0} | πT ) , (5)\nwhere βı̂ is the ı̂-th element of β. In other words, RSI picks the top k locations where the posterior marginal expectation is the largest. When k = 1, this is equivalent to picking β̂T = arg maxπT (β). Otherwise, (5) yields the smallest Bayes risk ̄(DT ) given any collected data DT ."
    }, {
      "heading" : "3.1 Accelerations",
      "text" : "In practice, holding ( n k ) models in memory can be infeasible if k is large, we can instead recover the support of β element-wise by repeatedly applying RSI assuming k = 1. After the posterior distribution πt(β(1)) converges to a pointmass distribution at the most-likely one-sparse model with sufficient confidence, we report its location and move on by\n2In real world experiments, we additionally estimate µ̂̂ using a point measurement on the inferred signal location for better modeling.\nTable 1: Conditions and conclusions for sample complexity.\nDesign Type Region Sensing Algorithm\nPrior for Bayes Risk\nMin T to Guarantee ̄T = 1 kE|S∆ŜT | ≤\nSample Complexity∗\npassive yes (any) π0(µ→∞) T ≥ n2 (1− n−1n−k ) (Theorem 1) Θ(n)\nPoint sensing T ≤ n(1− n−1n−k ) (Corollary A.2)\nactive no (any) π̃0 T ≥ 4nµ2 (1− )2 (Theorem 2) Ω( nµ2 )†\nyes CASS [2014] max risk (incl. π0)\nT ≤ 20 nµ2 log( 8k ) + 2k log2(nk ) Õ( nµ2 + k)‡\nRSI (ours) π0 T̄ ≤ 50( nµ2 + k\n2\n9 ) log2( 2 ) log( n )\n(Theorem 3) Õ( nµ2 + k 2)‡\n∗ Assume = O(1) and k n. † Shown for unconstrained sensing; binary search requires Ω(log2(n)+k) additional measurements. ‡ log(n) terms are left out. T̄ is defined differently; see Section 4.2 for details.\nAlgorithm 2 Region Sensing Index-Any-k (RSI-A)\nRequire: n, µ, , and the unknown β∗"
    }, {
      "heading" : "Ensure: Ŝ",
      "text" : "1: initialize Ŝ = ∅, β̂ = 0 2: for k = 1, 2, . . . , do 3: infer π0(β(k)) ∝ ∏t τ=1 p(yτ | β(k) + β̂,xτ ),\n∀β(k) ∈ {µ1j : j 6∈ Ŝ} 4: call Ŝ(k) = RSI (π0, ,β∗ − β̂) 5: aggregate Ŝ = ∪c≤kŜ(c) and β̂ = ∑ ̂∈Ŝ µ̂̂1̂ . 2\nremoving the reported point from the search and recomputing the posterior distributions using the uniform prior, π0(β(2)), on the new class, Sµ ( n−1 1 ) .\nWe call this alternative algorithm Region Sensing IndexAny-k (RSI-A, Algorithm 2) and use it in our simulations so that the computational cost is no longer exponential in k. Notice, our analysis is for the unmodified RSI; the statistical disadvantage of RSI-A is no more than O(k), multiplicatively.\nWhen implementing RSI-A, we also avoid unnecessary numerical integration (3), if the region is guaranteed to have inferior IG, indicated by its p vector (4), which is easier to compute. We use the fact that I(γ; y | p, λ) with fixed λ > 0 is concave in the probability simplex ∆k = {p ∈ [0, 1]k+1 : p>1 = 1}. Under k = 1 approximation, the region whose marginal probability p1 = ∑ x>β>0 π(β) is closest to 0.5 will provably have the largest IG among all regions of the same size. Thus, we find the region with the highest IG in two steps: (1) compare the p1 value for all regions for every region size and (2) evaluate the IG of only these regions with the best p1 values (closest to 0.5) in their region sizes.\n4 Theoretical Analysis in 1D The analysis is cleanest when the search space is 1d, where the regions can be any integer intervals that subset [1, n]. Without loss of generality (WLOG), assume n is a multiple of k and n ≥ 2k. Our goal is to find the smallest number\nof measurements, T , to guarantee a small Bayes risk ̄T = 1 kE|S∆ŜT | ≤ . Table 1 summarizes our analysis. The sample complexity is best appreciated assuming µ 1, k n, and = O(1). A typical choice is = 1/2, i.e., the number of measurements to guarantee that half of the signal support can be recovered on average."
    }, {
      "heading" : "4.1 Baseline Results",
      "text" : "Here we provide lower bounds on sample complexity. We show that under region-sensing constraints, all passive methods require T ≥ Ω(n) measurements and active methods require T ≥ Ω(n/µ2 +k). When µ 1, active methods have significant potential for improvement over passive methods using region sensing, which contradicts with the view in unconstrained compressive sensing by Arias-Castro, Candes, and Davenport (2013); Soni and Haupt (2014). Theorem 1 (Limits of any passive methods using region sensing). Assume β has prior π0 (uniform random on Sµ ( n k ) ). Any passive method with T noiseless region measurements on 1d must incur Bayes risk ̄T ≥ n−kn−1 (1− 2Tn ). To guarantee ̄T ≤ , T ≥ n2 (1− n−1n−k ) is required.\nThe proof is due to model identifiability, neglecting observation noise. More details can be found in the appendix. It applies to any µ ≥ 0 and particularly µ→∞. Theorem 2 (Limits of any methods, (Arias-Castro, Candes, and Davenport 2013)). Assume β has a slightly different prior, π̃0, that includes each location in X in the support of β independently with probability k/n. Any method (including active and non-region-sensing) must have ̄T ≥ 1− µ2 √ T/n. To guarantee ̄T ≤ , T ≥ 4nµ2 (1− )2 is required. The proof can be found under Theorem 3 of (Arias-Castro, Candes, and Davenport 2013). Arias-Castro, Candes, and Davenport (2013) gave a minimax risk with similar terms by modifying π̃0 to a least favorable prior on all models that are at most k-sparse. However, we only study Bayes risk for technical convenience.\nWhen using Theorem 2 for reference, notice the difference between π̃0 and π0 that the former additionally treats\nthe sparsity to be a random variable k̃ with expectation k. From concentration inequalities, |k̃ − k| ≤ O( √ k), with\nhigh probability. While k̃ and k are not directly comparable, Theorem 2 is still a useful baseline. Under region-sensing constraints, the number of measurements must also be at least Ω(k) to allow visits to most of the nonzero locations at least once, in a nontrivial draw of S where the signals are separated.\nWith respect to Theorem 1, the point sensing or any nonrepeating region sensing will achieve the optimal sample complexity (up to constant factors, see Appendix A for more details). For Theorem 2, the CASS method published by Malloy and Nowak (2014) for active sensing with region constraints3 acheives a nearly optimal rate in theory. Table 1 contains a detailed summary of the sample complexities of several algorithms, including our own."
    }, {
      "heading" : "4.2 Main Result",
      "text" : "For technical convenience, we directly express our main result in terms of the expected number of measurement that are actually taken so as to realize ̄(DT ) ≤ for a given threshold in an experiment. Taking T = T as a random variable, the expected number of actual measurements is different from the pre-determined sampling budget that an algorithm fully consumes to guarantee a desirable averaged risk (see Section 4.1). However, it is a comparable alternative in Bayesian analysis, used by e.g., Lai and Robbins (1985); Kaufmann, Korda, and Munos (2012). When the objective is constant = O(1), our result implies a deterministic budget requirement of the same order of complexity, T ≤ −12 ET 2 , where 2 = 2 , by direct application of Markov’s inequality.\nTheorem 3 (Sample complexity of RSI). In active search for k sparse signals with strength µ in 1d physical space of size n ≥ 2k (WLOG, assume n is a multiple of k), given any > 0 as tolerance of posterior Bayes risk, RSI using region sensing has bounded expected number of actual measurements,\nT̄ = E[min{T : ̄(DT ) ≤ }]\n≤ 50 ( n µ2 + k2 9 ) log2 (2 ) log (n ) = Õ ( n µ2 + k2 ) , (6)\nwhere the expectation is taken over the prior distribution and sensing outcomes."
    }, {
      "heading" : "4.3 Proof Sketch",
      "text" : "The proof for Theorem 3 hinges on an observation that the information gain (IG) where RSI makes measurements is consistently large, before active search terminates with minimal Bayes risk. For example, the IG of any measurement in binary search with k = 1 and noiseless observations is always O(log(2)). However, IG is harder to approximate when the observations are noisy. Therefore, we first show an intuitive lower bound for IG. Recall notations from (4).\n3The original result in Malloy and Nowak (2014) is stronger; it considers the maximum probability of support recovery mistakes, P (S 6= Ŝ) ≤ δ, for any S that are k-sparse and any signals with at least µ strength.\nProposition 4. The IG score of a region sensing design has lower bounds with respect to its design parameters (λ,p), as\nI(γ; y | λ,p) ≥ 2qcq̄c ( 2Φ (λ\n2\n) − 1 )2\n≥ 1 12 min{qc, q̄c}min{λ2, 32}, ∀1 ≤ c ≤ k, (7)\nwhere qc = Pr(γ ≥ c) = ∑ κ≥c pκ, q̄c = 1− qc, and Φ(u) is the standard normal cdf.\nThe proof uses Pinsker’s inequality and is given in Section B in the appendix. Notice using the common choice of Jensen’s inequality will give bounds in the opposite direction. To formalizes our observation that the IG is bounded: Lemma 5. WLOG, assume n is a multiple of k and n ≥ 2k. At any step, if the current Bayes risk ̄(D) > , we can always find a region A of size at most nk , such that λ 2 ≥ µ2a = kµ2 n and 2 ≤ E[γ | D] ≤ 1− 2 (we call this Condition E), which further yields\nI(γ; y | λ,p) ≥ I∗ =\n25k min {k2µ2 n , 32 } . (8)\nThe way to find the region A that satisfies Condition E is given in Lemma B.5 in the appendix. The reason that Condition E is sufficient for (8) can be derived from Proposition 4 for k = 1 and Lemma B.6 in the appendix for k > 1.\nEq (8) shows the minimum decrease in the model entropy in expectation after each measurement, starting from the maximum entropy of a uniform prior distribution, k log(n). However, the posterior entropy can never be negative, which implies a bound on the expected number of times that (8) can be applied, i.e. the expected number of measurements to reach Bayes risk is 25 log(n) (\nn µ2 +\nk2\n9 ). Lemma D.5 in the appendix shows some additional improvements to obtain the logarithmic dependency of in Theorem 3.\n5 Simulation Studies We evaluated RSI or its approximation RSI-A when k > 1. Other baseline algorithms include: • CASS (compressive adaptive sense and search) (Malloy and Nowak 2014): a branch-and-bound algorithm that traverses the region hierarchy from top to down using preallocated budgets per level. We count each xi as ‖xi‖22 region sensing measurements (rounded up to the next integer). • Point sensing: a passive design that uses exhaustive point measurements on all locations. • CS (compressive sensing) (Donoho 2006): a non-regionsensing design that draws xt ∼ N (0, I) and rescales ‖xt‖22 to 1. CS then solves a convex optimization problem to infer the nonzero signals, by minimizing ∑ t ‖yt−x>t β‖22+λ‖β‖1 s.t. β ≥ 0, where λ is chosen to produce exactly k nonzero coefficients using the Lasso regularization path.\nWe picked n = 1024 and various k (sparsity) and d (the dimension of the physical space) annotated below the plots. In the d = 5 case, we chose the region space to be the Cartesian product of [4]5 and allowed regions from a spatial pyramid (Lazebnik, Schmid, and Ponce 2006) of granularity\n45, 25, and 15. Each method was run with 200 repetitions to find its average performance.\nFigure 1(a) compares the recall rates of the algorithms as they progressed in a 1d search for a single true signal of strength µ = 16. RSI was the most efficient, finding the correct location in 50% of the cases with as few as T = 20 measurements. CASS was comparable only at the step points when all the allocated budgets were used, due to its rather rigid designs. We drew multiple curves for CASS to reflect this fact; the turning points were at T = 28 and 56 for = 0.5 and 0.85, respectively. CS was less effective compared with CASS with equal budgets (e.g., ‖X‖2F = 52 > 28 for = 0.5) which agrees with the analysis in Arias-Castro, Candes, and Davenport (2013). Point sensing was the least efficient, using T = n/2 = 512 measurements, which was worse than the other methods by a factor of Ω̃(µ2) (ignoring logarithmic terms). Notice, due to non-identifiability, any passive designs would have equal or worse rates.\nFigure 1(b) extends the comparison on the full spectrum of SNR, 1/4 < µ < 1024, showing the minimum number of measurements T to guarantee constant Bayes risk ̄T < 0.5. RSI led the comparison, showing a sample complexity of Õ(n/µ2) when µ is small and Õ(1) when µ is large. CASS also had a similar trend. CS ignores the region sensing constraints and was inferior to RSI. Notice CS also has a minimum sample complexity, but in order to meet the incoherence conditions for Lasso sparsistency (Candes and Tao 2007; Wainwright 2009; Raskutti, Wainwright, and Yu 2010), the rank of the covariance matrix of the measurements X>SXS must be at least k. Point sensing and other passive region\nsensing would always require at least Ω(n) measurements regardless of µ. Figure 1(c-d) show similar conclusions with other choices of k and d. The number of measurements was largely unaffected by k > 1 if µ is low, which supports the first term of Theorem 3, which is Õ(n/µ2). Comparisons between CS and RSI in high dimensions (d > 1) depend on how region constraints are defined. In our high-dimensional simulations, the region choices were rather limiting for RSI, giving more advantage to the unconstrained CS when µ is large.\n6 Real World Datasets Region sensing was intended to address the problems of real robotic search. Here, we took satellite images like Figure 2a and used natural blue pixels, e.g., the blue roof which we circled near the lower left of the center of the image, as a simulated target of interest. These experiments directly simulate search and rescue in open areas based on life jacket colors or communication signals and also share similarities with gas leaks or radiation detection, where real data is usually sensitive or expensive. Many assumptions were violated in these experiments, e.g., the noise was not iid and the target was a collection of neighboring pixels. For the purpose of more accurate modeling of the actual measurement powers at each region level, we used statistics from the real data to model µw(a) and σ(a) as functions of the region size a(= ‖x‖0).\nFigure 2b shows in the background the actual scalar observations, affinely transformed from the original RGB values to filter out the target blue color. The foreground contains the rectangular regions of measurement, sequentially decided by RSI after observing the average values in previous regions. Feasible region choices were contained in a spatial pyramid (Lazebnik, Schmid, and Ponce 2006). RSI behaved similarly to sequential scanning at the optimal altitude except for occasional bisections into subregions.\nBy comparing the IG of all feasible regions, RSI usually decides to (a) sense the next region in space when the previous outcome is low, (b) investigate the subregions when the last parent region yields a large outcome (we disallow repeated actions for the lack of noise-independence,) or (c) back out from an investigation if the subsequent measurements yield low outcomes. Option (c) demonstrates the ability of error recovery, which is our advantage to CASS thanks to Bayesian modeling. The search in Figure 2b ended after 36 measurements, whereas the image contained 36 000 pixel points.\nFigure 3 compares the performances on 221 image patches of 512× 512 pixels, cropped from National Agriculture Imagery Program (NAIP).4 The other algorithms for comparison include random (point), CS, and CASS*. Here, CASS* is a modified CASS method where each measurement can only be taken once, because repeated measurements yield the same outcome. To fully represent CASS*, in addition to choosing k by the true sparsity, we added fixed choices of k = 64 and 512, yielding three different curves.\nRSI achieved the best performance, finding on average 60% blue pixels with as few as 1700 measurements (0.5% of the total number of feasible observations). CASS* performance highly depended on the parameter choices and produced results only near the end of the experiment. CS did poorly, probably due to the fact the signals were not\niid (a blue object can contain multiple pixels).\n7 Discussions Region sensing is a new setting motivated by robotic search operations where we also found statistical insights to contrast with the unconstrained sensing in Arias-Castro, Candes, and Davenport (2013). RSI performs near-optimally in 1d search domains and fundamentally faster than passive sensing. In higher dimensions, the analysis may be harder, especially for passive baselines. The number of subregions generated by intersecting the measurement regions may be harder to count, unless measurement regions are restricted to grid regions in a spatial pyramid (such that any pair of regions is either nested or disjoint). We also want to establish frequentist analysis in the future. Finally, it is interesting to generalize the measurement model beyond taking the average value of a single region at a time.\nAcknowledgments This work is partially supported by the DARPA grant FA87501220324, National Science Foundation under Award Number IIA-1355406, and ARPA-E TERRA-REF award DEAR0000594. We also appreciate suggestions and discussions from Aarti Singh, Ying Yang, and Yining Wang.\nReferences Abbasi-Yadkori, Y. 2012. Online-to-confidence-set conversions and application to sparse stochastic bandits. Arias-Castro, E.; Candes, E. J.; and Davenport, M. 2013. On the fundamental limits of adaptive sensing. Information Theory, IEEE Transactions on. Braun, G.; Pokutta, S.; and Xie, Y. 2015. Info-greedy sequential adaptive compressed sensing. IEEE Journal of Selected Topics in Signal Processing 9(4):601–611.\n4https://lta.cr.usgs.gov/node/300\nCandes, E., and Tao, T. 2007. The dantzig selector: statistical estimation when p is much larger than n. The Annals of Statistics. Candès, E. J., and Wakin, M. B. 2008. An introduction to compressive sampling. Signal Processing Magazine, IEEE 25(2):21–30. Carpentier, A., and Munos, R. 2012. Bandit theory meets compressed sensing for high dimensional stochastic linear bandit. In AISTATS, volume 22, 190–198. Carpin, M.; Rosati, S.; Khan, M. E.; and Rimoldi, B. 2015. Uavs using bayesian optimization to locate wifi devices. arXiv preprint arXiv:1510.03592. Donoho, D. L. 2006. Compressed sensing. Information Theory, IEEE Transactions on 52(4):1289–1306. Gergonne, J. D. 1815. Application de la méthode des moindre quarrés a l’interpolation des suites. Annales des Math Pures et Appl. Haupt, J. D.; Baraniuk, R. G.; Castro, R. M.; and Nowak, R. D. 2009. Compressive distilled sensing: Sparse recovery using adaptivity in compressive measurements. In Signals, Systems and Computers. IEEE. Hernández-Lobato, J. M.; Hoffman, M. W.; and Ghahramani, Z. 2014. Predictive entropy search for efficient global optimization of black-box functions. In Advances in Neural Information Processing Systems. Jedynak, B.; Frazier, P. I.; and Sznitman, R. 2012. Twenty questions with noise: Bayes optimal policies for entropy loss. Journal of Applied Probability 49(1):114–136. Jones, D. R.; Schonlau, M.; and Welch, W. J. 1998. Efficient global optimization of expensive black-box functions. Journal of Global optimization 13(4):455–492. Kaufmann, E.; Korda, N.; and Munos, R. 2012. Thompson sampling: An asymptotically optimal finite-time analysis. In Algorithmic Learning Theory, 199–213. Springer. Lai, T. L., and Robbins, H. 1985. Asymptotically efficient adaptive allocation rules. Advances in applied mathematics 6(1):4–22. Lazebnik, S.; Schmid, C.; and Ponce, J. 2006. Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories. In Computer Vision and Pattern Recognition. IEEE. Ma, Y.; Sutherland, D.; Garnett, R.; and Schneider, J. 2015. Active pointillistic pattern search. In Proceedings of the Eighteenth International Conference on Artificial Intelligence and Statistics. Malloy, M. L., and Nowak, R. D. 2014. Near-optimal adaptive compressed sensing. Information Theory, IEEE Transactions on. Rajan, P.; Han, W.; Sznitman, R.; Frazier, P.; and Jedynak, B. 2015. Bayesian multiple target localization. In Proceedings of the 32nd International Conference on Machine Learning (ICML). Raskutti, G.; Wainwright, M. J.; and Yu, B. 2010. Restricted eigenvalue properties for correlated gaussian designs. The Journal of Machine Learning Research. Soni, A., and Haupt, J. 2014. On the fundamental limits of recovering tree sparse vectors from noisy linear measurements. Information Theory, IEEE Transactions on. Wainwright, M. J. 2009. Sharp thresholds for high-dimensional and noisy sparsity recovery using-constrained quadratic programming (lasso). Information Theory, IEEE Transactions on. Yue, Y., and Guestrin, C. 2011. Linear submodular bandits and their application to diversified retrieval. In Advances in Neural Information Processing Systems, 2483–2491.\nActive Search for Sparse Signals with Region Sensing (Appendix)\nYifei Ma Carnegie Mellon University\nPittsburgh PA 15213, US yifeim@cs.cmu.edu\nRoman Garnett Washington University in St. Louis\nSt. Louis, MO, USA garnett@wustl.edu\nJeff Schneider Carnegie Mellon University\nPittsburgh PA 15213, US schneide@cs.cmu.edu\nAbstract\nThis supplementary material includes both theoretical details (Section A–B) and additional empirical results (Section C). For the theoretical part, our main paper has two separate results: a hardness result showing passive methods under region sensing constraints in 1d search spaces cannot be efficient (Theorem 1 and the first row in Table 1 in the main paper) and a positive result showing that our Region Sensing Index (RSI) is sample-efficient (Theorem 3), comparable with the optimal efficiency obtained in (Arias-Castro, Candes, and Davenport 2013). The empirical part contains the choice of parameters and a demo of search results. For convenience, in the appendix, we sometimes use K to represent the total number of sparse signals in the system, whereas in the main document we always used k.\nA Theoretical Properties for Passive Sensing\nTheorem A.1 (Theorem 1 in the main document; limits of any passive methods using region sensing). Assume β has prior π0 (uniform random on Sµ ( n k ) ). Any passive method with T noiseless region measurements on 1D must incur Bayes risk ̄T ≥ n−kn−1 (1− 2Tn ); to guarantee ̄T ≤ , it requires T ≥ n2 (1− n−1n−k ).\nProof. We count the number of non-identifiability models with T noiseless observations, particularly when T < n2 .\nAn aggregate measurement on region [ai, bi) ⊂ [1, n+ 1) cannot identify the sparse support inside [ai, bi) (or its complement), unless it intersects with another aggregate measurement. Should two measurement regions intersect, the model is still nonidentifiable inside the intersection, set differences, and the complement of the union of both. To find out the set of all disjoint subsets where the model is non-identifiable given any passive design with m region measurements, {[ai, bi) ⊂ [1, n+ 1) : i = 1, . . . ,m}, we simply sort the unique end points as c1 < · · · < cp ∈ {ai, . . . , am} ∪ {b1, . . . , bm}, where p ≤ 2m, and use the following set of p elementary subsets:\n{ [cj , cj+1)︸ ︷︷ ︸\nCj\n: j = 1, . . . , p− 1 } ∪ {\n[cp, n+ 1) ∪ [1, c1)︸ ︷︷ ︸ Cp\n} , (A.1)\nwhere the last subset is created to ensure that the number of sparse supports in the full set equals k. Notice, (A.1) is also the largest set of disjoint subsets that can be created using intersections, unions, and complements on the regions of measurements.\nWe will continue our discussion assuming that the measurements are made on the subsets contained in (A.1). When the observations are noiseless, (A.1) is a superior design than the original design, whose outcomes can be inferred as\nx>[ai,bi)β = p−1∑\nj=1\ncj+1 − cj bi − ai x>[cj ,cj+1)β. (A.2)\nAt this point, it is easy to see that the minimum sample size to guarantee that the signals can be fully identifiable in the worst case is T ≥ n2 ; the necessary (and sufficient) condition is to have |Cj | = 1,∀j = 1, . . . , p, which requires 2T ≥ p ≥ n. For > 0, we compute the expected Delta-risk given any fixed design which yields p elementary subsets as shown in (A.1). Let nj = |Cj |, j = 1, . . . , p. If the model β distributes kj supports in subset Cj , respectively, then on any region where Copyright c© 2017, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\nnj > kj > 0, the inference algorithm can only make a random guess, e.g., for the first kj elements. Let βCj be the signal vector on subset Cj , the conditional expected error on this subset is:\nE [ |βCj∆β̂Cj | | kj ] = kj∑\nej=1\n( nj−kj ej )( kj kj−ej )\n( nj kj\n) ej = kj∑\nej=1\n( nj−kj−1 ej−1 )( kj kj−ej )\n( nj kj ) (nj − kj)\n=\n( nj−1 kj−1 ) ( nj kj ) (nj − kj) = kj(nj − kj) nj . (A.3)\nThe total risk conditioned on all of kj : j = 1, . . . , p is:\nE [ |β∆β̂| | k1, . . . , kp ] = p∑\nj=1\nE [ |βCj∆β̂Cj | | kj ] = p∑\nj=1\nkj(nj − kj) nj . (A.4)\nUsing the law of total expectation assuming β to be uniformly distributed, we can compute the expected error of the given passive design as\nE|β∆β̂| = ∑\nk1+···+kp=K\n(∏p j=1 ( nj kj ) ( n K ) )( p∑\nj=1\nkj(nj − kj) nj\n)\n=\np∑\nj=1\n∑\nk1+···+kp=K\n∏p j′=1 (nj′ kj′ ) ( n K ) kj(nj − kj) nj\n=\np∑\nj=1\n∑\nk1+···+kp=K\n(nj − 1) ( nj−2 kj−1 )∏ j′ 6=j (nj′ kj′ ) ( n K )\n=\np∑\nj=1\n(nj − 1) ( n−2 K−1 ) ( n K ) = (n− p) ( n−2 K−1 ) ( n K ) = K(n−K) n(n− 1) (n− p) ≤ K (n−K)(n− 2T ) n(n− 1) (A.5)\nTo guarantee E|β∆β̂| ≤ K , by solving (A.5) ≤ K , a passive design requires a minimal sample size of\nT ≥ p 2 ≥ n 2\n( 1− n− 1 n−K ) . (A.6)\nCorollary A.2. Using noiseless region-sensing observations, a passive design in 1D with T ≤ n2 region measurements achieves the optimal average-case Delta-risk, if and only if it can separate the search space into 2m disjoint subsets using intersections, unions, and complements of the measurement regions. The following example is adapted from Gray code:\nt x>\n1 0 0 0 0 1 1 1 1 2 0 0 1 1 1 1 0 0 3 0 1 1 0 0 0 0 0 4 0 0 0 0 0 1 1 0 · · · · · · (the pattern cycles)\n(A.7)\nProof. To minimize (A.5), it is sufficient to find passive designs where p = 2T , given that the region aggregate measurements are noiseless. The expected risk of (A.5) turns out to be independent of the sizes of each elementary subset Cj (which one can verify with a minimal example where n = 4, K = 2, and p = 2), which suggests that all designs that yield p = 2T have the same average-case Delta-risk with noiseless region aggregate measurements.\nNotice that the Gray-code design may not be optimal when the measurements are noisy. For this reason, we also included point sensing in Table 1 in our main paper, which yields the same order of sample complexity and performs better when the measurement noise is large.\nB Theoretical Properties for Active Sensing The main goal of this section is to show that our main algorithm, Region Sensing Index (RSI), has the sample complexity guarantees show as Theorem 3 in the main paper. The main paper includes a proof sketch with three major steps. We show their details in 3 respective subsections."
    }, {
      "heading" : "B.1 Basic Properties of Information Gain (IG)",
      "text" : "Recall that the observation model is yt = x>t β + t, where β ∈ Sµ ( n k ) , β ∼ πt, and t ∼ N (0, σ2t ). Omitting the time index t in this subsection, the information gain (IG) to be maximized in every step is defined as I(β; y | x, π) = H(y | x, π)− E[H(y | x,β) | π]\n⇔ I(γ; y | λ,p) = H(y | λ,p)−H( ), (B.1)\nwhere f(y | λ,p) = K∑\nc=0\npcφ(y − cλ)\nλ = µwx, γ = x>β λ , pc = Pr(γ = c) = ∑\nβ:x>β=cλ\nπt(β). (B.2)\nThere are two basic properties: Lemma B.1 that is both directly applied in Section 3.1 Accelerations and indirectly used in the later proof sketch; and Proposition B.2 that appears as Proposition 4 in the main paper."
    }, {
      "heading" : "Basic Property 1",
      "text" : "Lemma B.1 (Concavity and monotonicity). I(γ; y | λ,p) is concave in p ∈ RK+1+ , which includes the convex simplex of ∆K = {p ∈ [0, 1]K+1 : p>1 = 1}, if 0 < λ < ∞ remains constant. On the other hand, I(γ; y | λ,p) with fixed p ∈ ∆K is monotone-increasing as λ increases.\nProof. Concavity and monotonicity can be verified using derivatives. Notice the second term in (B.1) is constant. Here are the equations for the first term as well as its first and second order derivatives, omitting the dependency on p and λ for simplicity:\nH(y;λ,p) = − ∫ f(y) log f(y)dy, (B.3)\n∂H(y;λ,p) = − ∫ ( 1 + log f(y) ) ∂f(y) dy, (B.4)\n∂2H(y;λ,p) = − ∫ ( ∂f(y)∂f(y)>\nf(y) + ( 1 + log f(y) ) ∂2f(y)\n) dy (B.5)\nPart 1. To show concavity in p(≥ 0), let φλ(y) = (φ(y), φ(y − λ), . . . , φ(y − Kλ))> and write out the gradient and the Hessian of H(y;λ,p) with respect of p as:\n∂H(y;λ,p)\n∂p> = −\n∫ ∞\n−∞\n( 1 + log f(y) ) φλ(y) dy (B.6)\n∂2H(y;λ,p)\n∂p∂p> = −\n∫ ∞\n−∞\n1\nf(y) φλ(y)φλ(y)\n>dy (B.7)\nNotice φλ(y)φλ(y) > is a PSD Gram matrix, which is preserved under integration. Further, the integral returns a PD matrix if the distribution is not degenerate (λ > 0 and pk > 0 for at least two distinct ks)\nPart 2.1 For monotonicity in λ(> 0), in the case when K = 1, the derivative with respect to λ is ∂H(y)\n∂λ = −\n∫ ( 1 + log f(y) ) · p1φ(y − λ) · (y − λ) dy\n= −p1 ∫ log f(y) · φ(y − λ) · (y − λ) dy = −p1 ∫ log f(y + λ) · φ(y) · (y) dy, (B.8)\nwhere the first line removes constant integrals and the second shifts the variable. In order to show that (B.8) is nonnegative, pair up y and −y for y > 0 and notice that, by assuming λ > 0,\nφ(y + λ) ≤ φ(−y + λ)⇒ f(y + λ) ≤ f(−y + λ). The bigger λ, the larger derivative it has.\nPart 2.2 In general when K ≥ 1, we can write out the derivative as ∂H(y;λ,p)\n∂λ = −\n∫ ∞\n−∞\n( 1 + log f(y) ) K∑\nk=0\npkφ(y − kλ)(y − kλ)k dy\n= − K∑\nk=1\n∫ ∞\n−∞ (1 + log f(y))\nK∑\nt=k\nptφ(y − tλ)(y − tλ) dy (B.9)\nDefine hk(y) = ∑K t=k ptφ(y − tλ); we have\n0 = −hk(y) log hk(y) ∣∣∣ ∞\n−∞ =\n∫ ∞\n−∞ (1 + log hk(y))\nK∑\nt=k\nptφ(y − tλ)(y − tλ) dy (B.10)\nConsider each term of k in (B.9) and add the corresponding terms from (B.10); using `k = ∑k−1 s=0 psφ(y − sλ), we get\n∂H(y;λ,p)\n∂λ = −\nK∑\nk=1\n∫ ∞\n−∞ log\n( 1 + `k(y)\nhk(y)\n) K∑\nt=k\nφ(y − tλ)(y − tλ) dy. (B.11)\nThe only remaining task is to show that rk(y) = `k(y) hk(y) is monotone decreasing with respect to y, which is sufficient to guarantee that (B.11) ≥ 0, due to the odd symmetry of the remaining integrand parts around y = tλ. Take the derivative of rk(y) with respect to y:\nr′k(y) = `′k(y)hk(y)− `k(y)h′k(y)\nh2k(y) =\n∑ s<k≤t pspt ( φ′s(y)φt(y)− φs(y)φ′t(y) )\nh2k(y)\n= ∑\ns<k≤t pspt\nφ2t (y) h2k(y)\n( φs(y)\nφt(y)\n)′ = ∑\ns<k≤t pspt\nφ2t (y) h2k(y)\n( φs(y)\nφt(y)\n) · (sλ− tλ) ≤ 0, (B.12)\nwhere to simplify notations, we denote the composite function φs(y) = φ(y− sλ). The inequality is strict if λ > 0 and pk 6= 0 for at least two k ∈ {0, . . . ,K}."
    }, {
      "heading" : "Basic Property 2",
      "text" : "Proposition B.2 (Proposition 4 in the main document; a lower bound for the IG of a design). The IG score of a region sensing design has lower bounds with respect to its design parameters (λ,p), as\nI(γ; y | λ,p) ≥ 2qcq̄c(2Φ(λ2 )− 1)2 ≥ 112 min{qc, q̄c}min{λ2, 32}, ∀1 ≤ c ≤ K, (B.13) where qc = P (γ ≥ c) = ∑ κ≥c pκ, q̄c = 1− qc, and Φ(u) is the standard normal cdf.\nProof of Proposition B.2. To show (B.13), first inequality: Pick any 1 ≤ c ≤ K; let v = 1γ≥c and v̂ = 1y>(c−1/2)λ be two binary truncations of the original variables, γ and y, respectively. These truncations lose information:\nI(γ; y | p, λ) ≥ I(v; v̂ | p, λ) = EvK ( (v̂ | v) ‖ v̂ )\n≥ 2 ∑\nv0∈{0,1} P (v = v0) sup v̂0\n∣∣P (v̂ = v̂0 | v = v0)− P (v̂ = v̂0) ∣∣ ︸ ︷︷ ︸ ,δ̂(v0,v̂0) 2 , (B.14)\nwhere K(· ‖ ·) is the Kullback–Leibler divergence and the second line comes from Pinsker’s inequality. Consider any realization of v = v0 and choose v̂0 = v0; using the rule of total probability and direct calculation,\nδ̂(v0, v0) = ∣∣∣P (v̂ = v0 | v = v0)− P (v = v0)P (v̂ = v0 | v = v0)− P (v 6= v0)P (v̂ = v0 | v 6= v0) ∣∣∣\n= P (v 6= v0) ∣∣∣P (v̂ = v0 | v = v0)− P (v̂ = v0 | v 6= v0) ∣∣∣ ≥ P (v 6= v0) [ Φ (λ\n2\n) − ( 1− Φ (λ\n2\n))] = P (v 6= v0) ( 2Φ (λ\n2\n) − 1 ) , (B.15)\nwhere Φ(λ2 ) is a lower bound on the probability of correct estimation, based on the worst-case draw of γ such that y cannot be more than λ2 away from γ in the direction that leads to estimation errors. Taking (B.15) to (B.14) yields the first part of the result.\nTo show (B.13), second inequality: So far we have shown an analytical lower bound for I(γ; y | λ,p). To make the result even more interpretable, we can further numerically evaluate the Gaussian tail distribution, to find two constants, C1 and C2, such that\nΦ(x)− 1 2 =\n∫ x\n0\nφ(u) du =\n∫ x\n0\n1√ 2π e− u2 2 du ≥ C1 min{x,C2}. (B.16)\nSince Φ(x) is monotone-increasing, we can fix C2 to find the worst difference quotient, Φ(x)/x, ∀x ∈ (0, C2]. In fact, we can directly assign C1 ≤ Φ(C2)/C2, because φ(u) is monotone-decreasing as u increases. We choose C2 = 32 and C1 = 1√12 , which yields (\n2Φ (λ\n2\n) − 1 )2 ≥ 1\n12 min\n{ λ2, 32 } . (B.17)\nProposition B.3 (An upper bound for the IG of a design). When K = 1 and WLOG p1 ≤ 12 , the upper bound of IG derived from Jensen’s inequality and max-entropy principle is I(γ; y | λ,p) ≤ 12p1λ2, which is on the same order of (B.13) when λ < O(1). In the λ 1 case, the IG is naturally upper-bounded by a Bernoulli experiment with noiseless observation, H(B(p1)) = −p1 log(p1)− (1− p1) log(1− p1) = Õ(p1). Therefore, Proposition B.2 is a good approximation to the true IG in all scenarios. (See Figure 1 for an empirical visualization.) The general upper bound is not tight for general k > 1.\nProof. The upper bound can be shown by Jensen’s inequality and max-entropy principle. It is also tight when k = 1. Omitting p and λ, I(γ; y) = I(γ; γ + ) = H(γ + ) +H(γ + | γ) = H(γ + )−H( ). (B.18) We only need to find the largest entropy forH(γ+ ) given p and λ. By Jensen’s inequality, under the same mean and variance, a normal distribution has the largest entropy, where we have:\nE(γ + ) = E(γ) + E( ) = p1λ, σ2mar = Var(γ + ) = Var(γ) + Var( ) + 2Cov(γ, ) = p1λ2 + 1. (B.19)\nWe can then use a normal distribution with the above mean and variance as a upper bound to:\nI(γ; y) = H(γ + )−H( ) ≤ 1 2 log(2πeσ2mar)− 1 2 log(2πe) = 1 2 log(1 + p1λ 2) ≤ 1 2 p1λ 2 (B.20)"
    }, {
      "heading" : "B.2 Minimum Information Gain of the Chosen Region in Each Iteration",
      "text" : "This subsection aims to formalize the main observation in our main paper, which is that the information gain of all of the chosen measurements from RSI remain consistently large, before active search terminates with minimal Bayes risk. This observation implies a constant speed at which the model uncertainty can be reduced in expectation, leading to the upper bounds on sample complexity in Section B.3.\nRecall that the Bayes risk is defined by ̄t = min|Ŝ|=k 1 kE[|Ŝ∆S| | πt], where ∆ is the symmetric set difference operator. If we include the Bayes inference rule πt(β) ∝ π0(β) ∏t τ=1 p(y | xτ ,β), we can see that ̄t is essentially a function of the collected data Dt = {(xτ , yτ ) : 1 ≤ τ ≤ t}. The following lemma paraphrases Lemma 5 in the main document, with the time index t omitted. Lemma B.4 (Minimum IG of the chosen regions). WLOG, assume n is a multiple of 2k. At any step, given the data collection outcomes D and the current Bayes risk ̄(D), we can always find a region A of size at most nk , such that λ 2 ≥ µ2a = kµ2\nn and ̄(D)\n2 ≤ E[γA | D] ≤ 1− ̄(D) 2 (we call it Condition E), which further yields\nI(γ; y | λ,p) ≥ I∗̄ = ̄(D)25k min{kλ2, 32} ≥ ̄(D) 25k min{ k2µ2 n , 3 2}. (B.21)\nCondition E Lemma B.4 states the result in two steps: (a) the fact that the posterior model after collecting data D still has large Bayes risk implies the existence of a very informative region that satisfies Condition E and (b) sensing on this region indeed yields nontrivial information, measured in terms of IG (B.21). We will split the proof into these two steps, accordingly. Lemma B.5 (A region that satisfies Condition E). In 1d search with unit `2-norm measurements, WLOG, assume n is a multiple of 2k. At any step, given the collected data D and the current Bayes risk ̄(D):\n1. There always is a region B of size no larger than nk , such that λ 2 B ≥ µ\n2 |B| = kµ2 n and E[γB | D] ≥ ̄(D) 2\n2. There always is a subregion A ⊂ B that satisfies Condition E:\nλ2A ≥ kµ2\nn and\n̄(D)\n2 ≤ E[γA | D] ≤ 1−\n̄(D)\n2 (B.22)\nProof. Part 1. Suppose the current minimizer of the posterior Bayes risk is Ŝ = Ŝ(D) = arg maxS′ ∑ ̂∈S′ E[β̂ | D]. Evenly split the domain into K disjoint and contiguous regions and take their largest disjoint and contiguous subsets that do not intersect with Ŝ. There are at most G ≤ 2K such sets; let them be B1, . . . , BG. We use γ(Bg) = ∑ j∈Bg 1 > j β to denote the corresponding region latent variables in a region Bg . The region B = arg maxBg′ E[γ(Bg′) | D] yields\nE[γ(B) | D] ≥ ∑G g′=1 E[γ(Bg′) | D]\n2K = K − E[γ(Ŝ) | D] 2K = ̄(D) 2 , (B.23)\ndue to the additivity, ∑ g γ(Bg) + γ(Ŝ) = ∑ j∈[n] 1 > j β = K.\nPart 2. Let A ⊂ B be the smallest contiguous subset such that E[γ(A) | D] ≥ ̄(D)2 . Notice the maximum certainty of any point in j ∈ A ⊆ X \\ Ŝ is\nE[βj | D] ≤ min ̂∈Ŝ (1− E[β̂ | D]) ≤ 1− ̄(D). (B.24)\nWe then use the additivity of expectation to obtain\nE[γ(A) | D] ≤ E[γ(A \\ {j}) | D] + E[βj | D] < ̄(D) 2 + (1− ̄(D)) = 1− ̄(D) 2 , ∀j ∈ A, i.e., j 6∈ Ŝ. (B.25)\nIG of the Chosen Region The following obtains Lemma B.6 with additive terms of K2. It provides advantages over the straight-forward calculation in the main paper (which yields results with multiplicative factors of K). Lemma B.6 (Maximum IG when the outcome expectation is bounded). For any design on K-sparse models, if there exists 0 < ̄ < 1 and a design (x, A, λ, γ) such that ̄2 ≤ Eγ ≤ 1 − ̄2 , where γ = x>β is latent variable of signal counts in the measurement region, then the information of the experiment is lower-bounded by\nI(γ; y | p, λ) ≥ ̄ 25K min{Kλ2, 32} (B.26)\nProof. We use the fact that IG is concave in p and we only check the vertices of the simplex of feasible probabilities to find its lower bound: \n  pk ≥ 0, k = 1, . . . ,K, (Constraint H1, . . . ,HK);∑K k=1 pk ≤ 1, (Constraint H0);∑K k=1 kpk ≥ ̄2 , (Constraint E1);∑K k=1 k(1− pk) ≥ ̄2 , (Constraint E2),\n(B.27)\nwhere p0 = 1− ∑K k=1 pk can be decided explicitly. All vertices of the simplex, including infeasible vertices, can be found by solving K linear systems constructed from the (K + 3) linear constraints. Since E1 and E2 cannot be satisfied simultaneously for any ̄ < 1, we can enumerate all the remaining vertices and write out their respective nonzero values:\npk = 1, from ∩k′ 6=k Hk′ ; pk + p` = 1, kpk + `p` = ̄ 2 , from ∩k′ 6=k,` Hk′ ∩ E1;\npk + p` = 1, kpk + `p` = 1− ̄2 , from ∩k′ 6=k,` Hk′ ∩ E2. (B.28)\nThe first row is infeasible when ̄ > 0. We then bound the IG for the other rows. Without loss of generality, assume ` < k. Then, all feasible cases require ` = 0 and yield min{pk, p`} ≥ ̄2k . Using Proposition 4,\nI(v;u | p, λ) ≥ ̄ 25K min{K2λ2, 32} ≥ ̄ 25K min{Kλ2, 32}. (B.29)\nProof of Lemma B.4. The design from Lemma B.5 satisfies both Condition E and λ ≥ Kµ2n , where we can then apply Lemma B.6 to obtain the conclusion."
    }, {
      "heading" : "B.3 The Proof of Theorem 3",
      "text" : "Lemma B.4 implies that the entropy in the posterior distribution, H(β | πt) = − ∑ β πt(β) log πt(β), decreases at least by I ∗ with every measurement in expectation, starting with H(β | π0) ≤ k log n. Since the posterior entropy cannot be negative, RSI must terminates in finite times in expectation. Theorem B.7 (Theorem 3 in the main document; sample complexity of RSI). In active search of k sparse signals with strength µ in 1d physical space of size n(≥ 2k), given any > 0 as tolerance of posterior Bayes risk, RSI using region sensing has bounded expected number of actual measurements before stopping,\nT̄ = E[min{T : ̄(DT ) ≤ }] ≤ 50 ( n µ2 + k2 9 ) log2 (2 ) log (n ) = Õ ( n µ2 + k2 ) , (B.30)\nwhere the expectation is taken over the prior distribution and sensing outcomes."
    }, {
      "heading" : "The Simple Approach",
      "text" : "Definition B.8 (Stopping time). Define T = minT {̄(DT ) ≤ } to be a random stopping time for an experiment to first yield less than posterior risk, ̄(Dτ ) = 1KE[S∆Ŝ | Dτ ] ≤ . T = T (τ) can be determined given τ . Lemma B.9 (Simple Expectations on the Number of Measurements for Small Errors). Given any 1 > 0, t0 ≥ 0, and the first t0 data collection outcomes Dt0 , the expected number of additional measurements before the RSI stops with posterior risk less than 1 is bounded in terms of H0 = H(β | π0) and I 1 defined in Lemma B.4, as\nE(T 1 − T 0 | Dt0) ≤ H0 I 1 ≤ 25H0 max { n kµ2 , k 9 } . (B.31)\nRemark B.10. By taking t0 = 0 and H0 ≤ k log n, Lemma B.9 implies\nT̄ ≤ 25 log(n)\n1 max { n µ2 , k2 9 } . (B.32)\nProof of Lemma B.9. Let t = t0 + s for any s ≥ 0 and Dt be the random variable for the data collection outcomes until step t. According to Lemma B.4,\n(T 1 | Dt) > t ⇒ H(β | Dt)− Ey [ H(β | Dt ∪ {x, y}) | Dt,xt+1 ] ≥ I 1 (B.33)\n⇒ H(β | Dt) ≥ I 1 + Ey [ H(β | Dt ∪ {x, y}) | Dt,xt+1 ] (B.34)\nTaking expectation over {Dt : (T 1 | Dt) > t,Dt0} = {Dt : ̄(Dt′) > 1,∀t′ ≤ t,Dt0} (B.35)\nyields E [ H(β | Dt) | T 1 > t,Dt0 ] ≥ I 1 + E [ H(β | Dt+1) | T 1 > t,Dt0 ] , (B.36)\nwhere the expectation is taken over (Dt | Dt0 , T 1 > t) and (Dt+1 | Dt0 , T 1 > t), respectively. Next, we hope to apply Lemma B.4 at step (t+ 1), but we have to make sure that the condition still holds, which is not directly implied by (B.36). To guarantee the conditions, we divide Dt+1 into two cases and use the nonnegativity of entropy to relax the second case,\nE [ Ht+1(β) | T 1 > t,Dt0 ] = P ( T 1 > t+ 1 | T 1 > t,Dt0 ) E [ Ht+1(β) | T 1 > t+ 1, Dt0 ]\n+ P ( T 1 = t+ 1 | T 1 > t,Dt0 ) E [ Ht+1(β) | T 1 = t+ 1, Dt0 ] ≥ P ( T 1 > t+ 1 | T 1 > t,Dt0 ) E [ Ht+1(β) | T 1 > t+ 1, Dt0 ] . (B.37)\nWe can then iterate beginning with t = t0 as\nE [ Ht0(β) | Dt0 ] ≥ P ( T 1 > t0 | Dt0 ) E [ Ht0(β) | T 1 > t0, Dt0 ]\n≥ P ( T 1 > t0 | Dt0 ) ( I 1 + P ( T 1 > t0 + 1 | T 1 > t0, Dt0 ) E [ Ht0+1(β) | T 1 > t0 + 1, Dt0 ] )\n= P ( T 1 > t0 | Dt0 ) I 1 + P ( T 1 > t0 + 1 | Dt0 ) E [ Ht0+1(β) | T 1 > t0 + 1, Dt0 ]\n≥ P ( T 1 > t0 | Dt0 ) I 1 + P ( T 1 > t0 + 1 | Dt0 ) ( I 1+\n+ P ( T 1 > t0 + 2 | T 1 > t0 + 1, Dt0 ) E [ Ht0+2(β) | T 1 > t0 + 2, Dt0\n] )\n≥ P ( T 1 > t0 | Dt0 ) I 1 + P ( T 1 > t0 + 1 | Dt0 ) I 1\n+ P ( T 1 > t0 + 2 | Dt0 ) E [ Ht0+2(β) | T 1 > t0 + 2, Dt0 ]\n≥ . . .\n≥ I 1 ∞∑\ns=0\nP ( T 1 > t0 + s | Dt0 ) = I 1E ( T 1 − T 0 | Dt0 ) , (B.38)\nwhich leads to the conclusion given E[Ht0(β) | Dt0 ] = H(β | Dt0) = H0."
    }, {
      "heading" : "The Complex Approach",
      "text" : "Lemma B.11 (Max entropy given Bayes error). For a K-sparse model, β ∈ S ( n K ) , given ̄ ≥ 1K ∑ j∈Ŝ P (βj = 0) = 1 KE|S∆Ŝ|, the posterior entropy is at most\nH(β) ≤ KH(B(̄)) +K̄ log n, (B.39)\n≤ K 2r ( 2r log 2 + log n ) , if ̄ ≤ 1 2r ,∀r = 0, 1, 2, . . . (B.40)\nwhere H(B(̄)) = −̄ log ̄− (1− ̄) log(1− ̄) is denoted as the entropy of a Bernoulli experiment with ̄ success rate.\nProof. Part 1. Let S = {S1, . . . , SK} be the set of supports of the random variable β that is modeled by the posterior distribution given the history data that leads to the current state. We can compute the expectation as\nK∑\nk=0\nkP (|S∆Ŝ| = k) = E|Ŝ∆S| ≤ K̄. (B.41)\nDefine pk = pk(Ŝ) = P (|S∆Ŝ| = k); the total entropy can be bounded:\nH(β) = − K∑\nk=0\n∑\nS:|S∆Ŝ|=k\nπ(βS) log π(βS) (B.42)\n≤ − K∑\nk=0\npk log\n( pk(\nK K−k )( n−K k\n) )\n(B.43)\n≤ − K∑\nk=0\npk log pk + K∑\nk=0\npk log\n( K\nk\n) + K∑\nk=0\nkpk log n (B.44)\n= − K∑\nk=0\npk log pk +\nK∑\nk=0\npk log\n( K\nk\n) +K̄ log n (B.45)\nwhere (B.42) separate the joint probabilities into (K+1) groups according to their values of |S∆Ŝ|. Inside every group, (B.43) realizes a uniform distribution, which maximizes the entropy given any value of group marginal probability, pk. We then relax the number of combination by log ( x K−k ) ≤ (K−k) log x, which yields (B.44). From there, we use the condition, reformulated as (B.41), to obtain (B.45).\nThe next step uses the principle of maximum entropy to realize the optimizer for (B.45), when the moments are bounded by (B.41). The Lagrangian of the constrained optimization is\nL(p; c, ρ) = − K∑\nk=0\npk log pk +\nK∑\nk=0\npk log\n( K\nk\n) + c ( K∑\nk=0\npk − 1 ) + ρ ( K∑\nk=0\nkpk −K̄ ) . (B.46)\nSetting the derivatives to zero yields\n0 = ∂L\n∂pk = − log pk + log\n( K\nk\n) + 1 + c+ kρ ⇒ pk ∝ ( K\nk\n) (eρ)k, (B.47)\nwhich implies that pk is the probability of k outcomes in a binomial distribution with K rounds and an iid outcome probability of p = 11+e−ρ in each round. Since the expectation of the total outcome is K̄, we have p = ̄. Given the max-entropy binomial distribution and let (X1, . . . , XK) to be the outcome of each round; the entropy of their sum is upper bounded by the sum of their marginal entropies, which is K times the entropy of H(̄). So, we proved (B.39).\nPart 2. To move forward to (B.40), we need an interim result when ̄ ≤ 12 : H(̄) ≤ −2̄ log ̄ ⇒ H(β) ≤ −2K̄ log ̄+K̄ logN, (B.48)\nTo show the interim result, let `(̄) = −̄ log ̄ + (1 − ̄) log(1 − ̄); its derivatives are `′(̄) = − log ̄ − log(1 − ̄) − 2 and `′′(̄) = − 1̄ + 11−̄ . The concavity of `(̄) in 0 ≤ ̄ ≤ 12 where `′′(̄) ≤ 0 and `(0) = `( 12 ) = 0 yield `(̄) ≥ 0, i.e., H(̄) ≤ −2̄ log ̄,∀0 ≤ ̄ ≤ 12 . Finally, (B.40) trivially holds when r = 0. Otherwise, substitute ̄ ≤ 2−r with r ≥ 1 in (B.48) yields the final conclusion.\nProof of the final theorem. Let r = 2−r and T r = minT {̄(DT ) ≤ r}, for r = 0, 1, . . . , dlog2( 1 )e. From Lemma B.9, we have\nE(T r+1 − T r | Dt, T r ≤ t) ≤ H(β | Dt) I∗ r+1 . (B.49)\nWe can use Lemma B.4 with r+1 = 2−r−1 to show\nI∗ r+1 ≥ r+1 25k\nmin {k2µ2 n , 9 } ≥ 1 50k2r min {k2µ2 n , 9 }\n(B.50)\nand Lemma B.11 with ̄(Dt) ≤ r = 2−r to bound\nH(β | Dt) ≤ k\n2r (2r log 2 + log n). (B.51)\nPut both bounds to (B.49) to get\nE(T r+1 − T r | Dt, T r ≤ t) ≤ 50 max { n µ2 , k2 9 } (2r log 2 + log n). (B.52)\nNotice the right side is independent of Dt and t, using linearity of expectations,\nE(T r+1 − T r ) ≤ 50 max { n µ2 , k2 9 } (2r log 2 + log n), (B.53)\nwhich further implies, using R = dlog2 1 e < 1 + log2 1 ,\nET ≤ R−1∑\nr=0\nE ( T r+1 − T r )\n≤ 50 max { n\nµ2 , k2 9\n}R−1∑\nr=0\n(2r log 2 + log n)\n≤ 50 max { n\nµ2 , k2 9\n} R((R− 1) log 2 + log n)\n≤ 50 max { n\nµ2 , k2 9\n} log2 2 log n\n(B.54)\nC Real-world experiments\nFor the purpose of more accurate modeling of the actual measurement powers at each region level, we use statistics from the real data to model µw(a) and σ(a) as functions of the region size a. The resulting SNR, λ(a) = µw(a)σ(a) is shown in Table 1. The optimal region size to begin under the uniform prior distribution is 32× 32. Figure 2 show examples of blue objects that are detected from the NAIP dataset.\nReferences\nArias-Castro, E.; Candes, E. J.; and Davenport, M. 2013. On the fundamental limits of adaptive sensing. Information Theory, IEEE Transactions on."
    } ],
    "references" : [ {
      "title" : "Example of positive discoveries in NAIP satellite",
      "author" : [ "E. Arias-Castro", "E.J. Candes", "M. Davenport" ],
      "venue" : null,
      "citeRegEx" : "Arias.Castro et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Arias.Castro et al\\.",
      "year" : 2013
    } ],
    "referenceMentions" : [ ],
    "year" : 2016,
    "abstractText" : "Autonomous systems can be used to search for sparse signals in a large space; e.g., aerial robots can be deployed to localize threats, detect gas leaks, or respond to distress calls. Intuitively, search algorithms may increase efficiency by collecting aggregate measurements summarizing large contiguous regions. However, most existing search methods either ignore the possibility of such region observations (e.g., Bayesian optimization and multi-armed bandits) or make strong assumptions about the sensing mechanism that allow each measurement to arbitrarily encode all signals in the entire environment (e.g., compressive sensing). We propose an algorithm that actively collects data to search for sparse signals using only noisy measurements of the average values on rectangular regions (including single points), based on the greedy maximization of information gain. We analyze our algorithm in 1d and show that it requires Õ(n/μ2 +k) measurements to recover all of k signal locations with small Bayes error, where μ and n are the signal strength and the size of the search space, respectively. We also show that active designs can be fundamentally more efficient than passive designs with region sensing, contrasting with the results of Arias-Castro, Candes, and Davenport (2013). We demonstrate the empirical performance of our algorithm on a search problem using satellite image data and in high dimensions.",
    "creator" : "LaTeX with hyperref package"
  }
}