{
  "name" : "1509.08830.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "How to Formulate and Solve Statistical Recognition and Learning Problems",
    "authors" : [ ],
    "emails" : [ "schles@irtc.org.ua", "waterlaz@gmail.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n50 9.\n08 83\nWe formulate problems of statistical recognition and learning in a common framework of complex hypothesis testing. Based on arguments from multi-criteria optimization, we identify strategies that are improper for solving these problems and derive a common form of the remaining strategies. We show that some widely used approaches to recognition and learning are improper in this sense. We then propose a generalized formulation of the recognition and learning problem which embraces the whole range of sizes of the learning sample, including the zero size. Learning becomes a special case of recognition without learning. We define the concept of closest to optimal strategy, being a solution to the formulated problem, and describe a technique for finding such a strategy. On several illustrative cases, the strategy is shown to be superior to the widely used learning methods based on maximal likelihood estimation.\nKeywords: complex object recognition, learning, multi-criteria op-\ntimization, Bayesian strategy, small sample problem"
    }, {
      "heading" : "1 Introduction",
      "text" : "We consider three wide directions in theoretical and applied statistics: complex hypothesis testing [5, 11, 13, 14], the empirical Bayesian approach [4, 15, 16, 17] and learning in pattern recognition [7, 22]. Each of these fields is wellknown and the list of relevant publications would be longer than this paper.\nHowever, the methods developed in the three fields are not fully consistent with one another. Indeed, the most popular learning methods seem to have been developed independently of complex hypothesis testing and sometimes even contradict it. It seems as if the well-known ideas of self-learning or unsupervised learning [6, 19, 20] appeared independently of the empirical Bayesian approach. Such an isolated existence of learning methods results in their rather fragile foundation.\nLearning in pattern recognition deals with the situation when the statistical model of an object to be recognized is not defined uniquely and only a set of models is known that includes the true model. The common feature of modern learning methods is that a so-called learning sample is given and based on this sample a certain model is selected from the given set. Then the recognition strategy is derived as if the selected model was the true one. Usually, the selected model is a consistent estimate of the true model and therefore this approach is acceptable, provided that the learning sample is large enough. However, if the learning sample has limited size the approach gives no guarantee for subsequent recognition. The situation is known as the ”small sample problem”.\nThus there is an obvious gap in our knowledge of statistical recognition under uncertain statistical model. Strictly speaking, satisfactory clarity is achieved only in two extreme cases. When no learning sample is available the methods for complex hypothesis testing are applicable, such as the minimax approach. When a large enough learning sample is available the learning methods are applicable. Practical situations fall in the gap between these two extreme cases because any sample has a certain size, it is not arbitrarily large. The situation is discussed in more detail in [19, page 272].\nThe paper fills this gap. Using the concepts of multi-criteria optimization [8, 9], we analyze the empirical Bayesian approach, learning in pattern recognition and complex hypothesis testing in a unified framework. Based on this unification, we formulate the learning problem as a special case of recognition without learning, the latter in turn being a special case of complex hypothesis testing. The proposed formulation covers the whole range of learning sample sizes, including zero size. Thus, the small sample problem disappears as an independent problem and is embraced by the proposed formulation.\nThe paper is organized as follows: Section 2. Recognition without learning is considered under an uncertain statistical model of an object to be recognized. The concepts of improper and Bayesian strategies are defined and the dichotomy theorem is proved that each strategy is either Bayesian or improper. It is shown that the strategies based on maximum likelihood model estimate are improper. The concept of closest to optimal recognition is defined.\nSection 3. The seminal ideas of H.Robbins [16], which addressed recognition of compound objects and initiated the empirical Bayesian approach [15], are revisited. Robbins’ ideas are formalized as finding a closest to optimal strategy. It is shown that strategies based on the well-known EM-algorithms differ from the closest to optimal ones and are improper for some objects.\nSection 4. Recognition with learning is formulated as a special case of\nrecognition without learning. It is shown that the widely used strategies based on maximal likelihood model estimate are improper. It is also shown that the minimax approach to statistical decision making is unsuitable for solving the learning problem. The concept of closest to optimal learning is defined and its formal properties are analyzed.\nSection 5. The technique of closest to optimal recognition is described. Section 6. Closest to optimal learning procedures are tested on several illustrative examples. The tests include supervised and unsupervised learning. The test with supervised learning shows that the closest to optimal learning procedure is superior to the procedure based on maximum-likelihood model estimation. The tests with unsupervised learning compare the closest to optimal procedures with EM-algorithms and some heuristic methods. The proposed approach yields the best result.\nSection 7. The results of the paper as well as their consequences are summarized.\nThe results of Sections 2, 3 and 4 are mostly negative, arguing that the commonly used approaches have fundamental drawbacks and thus new approaches are needed. The positive results concern mainly the concept of closest to optimal learning and are described in Sections 4, 5 and 6."
    }, {
      "heading" : "2 Complex object recognition",
      "text" : "The main concepts of complex hypothesis discrimination have been formulated in the general statistical decision theory [5, 11, 13, 14]. Our aim is to relate the result of this theory to statistical recognition and machine learning [7, 22]. Therefore we formulate the main concepts of statistical decision theory using pattern recognition terminology. Basic concepts of our consideration are a set of observable signals, a set of hidden states and a set of models. All three sets are assumed to be finite. This simplification allows to articulate the main ideas of the article using the simplest mathematical tools. Allowing some of the sets to be infinite might obscure the main ideas by more fine mathematical formulations. Nevertheless, some examples below involve intervals of real numbers. Our theoretical results are applied to such examples without further ado. Probability density is simply used instead of probability.\nLet an object to be recognized be characterized by two parameters x and y that take values from finite sets X and Y respectively. The parameter x is an observable signal generated by the object. The parameter y is a hidden state of the object. The signal x and the state y form a random pair, so that a probability distribution p : X × Y → R over the set of pairs (x, y) exists. However, the probability distribution p may be either known or not known and this makes the distinction between simple and complex objects.\nDefinition 1. A simple object is represented by a triple 〈X,Y, p : X × Y → R〉 where X is a set of signals, Y is a set of states, p(x, y) is a joint probability of signal x ∈ X and state y ∈ Y .\nWe consider more complex cases when the probability distribution p is not known and only a class of distributions is known that contains p. This uncertainty is represented by a finite set Θ of models θ, so that the probability of a pair (x, y) is defined for each model θ by p(x, y; θ).\nDefinition 2. A complex object is represented by a quadruple 〈X,Y,Θ, p : X × Y ×Θ → R〉 where X is a set of signals, Y is a set of states, Θ is a set of models and p(x, y; θ) is a joint probability of signal x ∈ X and state y ∈ Y for the model θ ∈ Θ.\nThe above-defined complex object is not to be confused with what can be called a pseudo-complex object . The latter takes place when the model θ is a random variable with a known a priori probability distribution pΘ: Θ → R. In fact, a pseudo-complex object does not differ from a simple one because for such an object the joint probability\np(x, y) = ∑\nθ∈Θ\npΘ(θ) p(x, y; θ)\nis defined for every pair x ∈ X , y ∈ Y . In a complex object the model θ is not random but fixed although unknown. It is only known that the model belongs to a given set. This paper deals with complex objects, neither with simple ones nor with pseudo-complex ones.\nRecognition means making a reasonable decision about the hidden state based on the observed signal. Below, we will also consider cases of the decision about current object state being based not only on the currently observed signal but on the base of additional information called learning information. In order to discriminate these two cases we use a term recognition without learning when the decision is based only on the observed signal and recognition with learning when the decision is based both on the signal and on the learning information.\nRecognition without learning is formalized by a function qX : Y × X → R, called a randomized strategy. The values qX(y\n′ |x), y′ ∈ Y, x ∈ X , of the function qX are conditional probabilities. Let QX be the set of all possible strategies, that is, the set of functions qX : Y ×X → R satisfying qX(y′ |x) ≥ 0 for every pair y′ ∈ Y , x ∈ X and ∑\ny′∈Y\nqX(y ′ |x) = 1 for every x ∈ X . The\nstrategy qX : Y ×X → R defines recognition as follows: if the signal x is observed then the decision ”the object is in state y′” is made with probability qX(y\n′ |x). The reasonableness of a strategy is formalized by a function w: Y × Y → R, called a loss function. Its value w(y, y′), y ∈ Y , y′ ∈ Y , is the loss of making a decision that the object state is y′ when the true object state is y.\nThe statistical theory of complex hypothesis testing is an appropriate mathematical formalization for recognition of complex objects, not only simple ones. However, strategies that are widely used in recognition practice have almost nothing in common with this formalization.\nExample 1. Let x be a picture and y ∈ Y be the name of a letter drawn on the picture. The letters are drawn by some known person. A priori probabilities pY (y), y ∈ Y , of the letters are known for this person, as well as the conditional probabilities pX |Y (x |y). This is the case of a simple object.\nSuppose now that there is a known finite team Θ of persons θ drawing pictures and the person θ that provides a picture for recognition is picked randomly with known probability pΘ(θ). It is necessary to recognize the letter drawn on the picture, not the person who drew it. One can assume that a priori probabilities pY (y), y ∈ Y , of the letters do not depend on the person. However, the picture x depends both on the letter y and on the person θ ∈ Θ who drew it. This dependency is given by conditional probabilities pX |Y (x |y; θ), defined for every picture x ∈ X , every letter y ∈ Y and every person θ ∈ Θ. This is the case of a pseudo-complex object.\nIn fact, this case does not differ from the case of a simple object. Indeed, one may substitute the whole team Θ of persons with a single abstract person θ∗ who draws a random picture x of a letter y with conditional probability p∗(x|y) = ∑\nθ∈Θ pΘ(θ)pX |Y (x |y; θ). Finally, let us consider the case when all pictures presented for recognition are drawn by a single person θ0 from a known team Θ. However, this person is unknown. So, the model θ0 is fixed, not random, and a concept of a priori probability distribution pΘ becomes unappropriated in this case. Same as before, it is necessary to recognize the letter, not the person who drew it. This is the case of a complex object.\nCommonly used algorithms for this task make the decisions\ny∗ = argmax y∈Y max θ∈Θ p(x |y; θ) or y∗ = argmax y∈Y max θ∈Θ p(x, y; θ).\nNeither of these two decisions follow from the statistical decision theory. Moreover, we will show later that both contradict this theory.\nStatistical recognition problems for simple and complex objects are essentially different from each other in nature. In the case of a simple object, every strategy qX is characterized by a single number, namely the expected value RX(qX) of the losses,\nRX(qX) = ∑\nx∈X\np(x) ∑\ny′∈Y\nqX(y ′ |x)\n∑\ny∈Y\np(y |x)w(y, y′),\ncalled the risk of strategy qX . This leads to the known problem of Bayes risk (single-criterion) minimization over the strategy set. In the case of a complex object, every strategy qX is characterized by |Θ| numbers RX(qX , θ), θ ∈ Θ, namely the risks of the strategy with respect to every model,\nRX(qX , θ) = ∑\nx∈X\n∑\ny′∈Y\n∑\ny∈Y\nqX(y ′ |x) p(y, x; θ)w(y, y′). (1)\nThe complex object case goes beyond the scope of single-criterion optimization and is subject to multi-criteria optimization. We follow its main ideas (see, for example, the book [8]) and translate them into the context of our problem.\nFirst of all, we define some particularly bad strategies that do not optimize any reasonable criterion and thus they are useless.\nDefinition 3. A strategy q′X predominates a strategy q ′′ X if the strong inequality RX(q ′ X , θ) < RX(q ′′ X , θ) holds for every model θ ∈ Θ; a strategy q0X is called improper if a strategy q′X exists that predominates q 0 X .\nOne can see that the definition of improper strategy is rather strong. It does not embrace all strategies that can be treated as bad ones. For example, each strategy argmin\nqX∈QX\nRX(qX , θ ∗), θ∗ ∈ Θ, is not improper but its usefulness\nis questionable because each such strategy takes into account only one single model θ∗ and ignores all the others. Therefore, if some strategy is not improper it does not mean that it is unconditionally suitable for concrete application. However, if a strategy is proved to be improper it shows that it has a serious defect independently of the expected application.\nWe want to exclude all improper strategies from consideration and derive the common form of all other strategies. Such a dichotomy is possible due to the known result in multi-criteria optimization [8, Theorem 3.5]. The theorem should be central to complex object recognition both with and without learning. However, the known recognition methods have been developed and continue being developed as if the theorem did not exist. This is not surprising because the theorem has been formulated and proved in its own conceptual and terminological context, which is far from pattern recognition. We translate and even prove the theorem using the concepts of our article.\nLet τ : Θ → R be a function, called the weight function, satisfying τ(θ) ≥ 0 for every θ ∈ Θ and ∑\nθ∈Θ\nτ(θ) = 1. The set of all such functions is denoted by T .\nDefinition 4. The strategy argmin qX∈QX\n∑\nθ∈Θ\nτ(θ)RX (qX , θ) is called Bayesian with\nrespect to the weight function τ . A strategy qX is called Bayesian if a weight function τ ∈ T exists with respect to which the strategy qX is Bayesian.\nBy this definition, every Bayesian strategy is optimal in a sense that it minimizes the Bayesian risk of recognizing a pseudo-complex object with a priori probability τ(θ) of model θ. For a complex model, such treatment of Bayesian strategy is meaningless because the model is not random and its a priori probability distribution is not defined. However, the concept of Bayesian strategies allows to partition the set of all strategies into the set of improper strategies and the set of all other strategies. This dichotomy is given by the following theorem, which states that every strategy is either Bayesian or improper.\nTheorem 1. For every strategy q0X ∈ QX, either a weight function τ∗ ∈ T exists such that\nq0X = argmin qX∈QX\n∑\nθ∈Θ\nτ∗(θ)RX(qX , θ) (2)\nor a strategy q∗X exists such that the inequality\nRX(q ∗ X , θ) < RX(q 0 X , θ) (3)\nholds for every θ ∈ Θ. These two properties of strategies are incompatible. Proof. Let the function F : T ×QX → R be defined by\nF (τ, qX) = ∑\nθ∈Θ\nτ(θ)[RX (qX , θ)−RX(q0X , θ)].\nLet the strategy q∗X and the weight function τ ∗ be defined by\nq∗X = argmin qX∈QX max τ∈T F (τ, q), τ∗ = argmax τ∈T min qX∈QX F (τ, qX).\nBy definition (1), the risk is a linear function of the probabilities qX(y ′ |x), which form a strategy qX . Consequently, the function F is also a linear function of qX for any fixed weight function τ . With a fixed strategy qX , the function F is a linear function of weights τ(θ). The set T of all weight functions and the set QX of all strategies are closed and convex. Due to the well-known duality theorem [2, 3, 10], for such functions F we have\nmax τ∈T min qX∈QX\nF (τ, qX) = F (τ ∗, q∗X) = min\nqX∈QX max τ∈T F (τ, qX).\nIt is obvious that the equality F (τ, q0X) = 0 holds for every τ ∈ T . Therefore the inequality min\nqX∈QX F (τ, qX) ≤ 0 holds for every τ ∈ T and, consequently,\nmax τ∈T min qX∈QX\nF (τ, qX) = F (τ ∗, q∗X) ≤ 0.\nSince the value F (τ∗, q∗X) is not positive, only two cases are possible: either F (τ∗, q∗X) < 0 or F (τ ∗, q∗X) = 0. Let us consider the case F (τ ∗, q∗X) < 0. We have the following chain:\nF (τ∗, q∗X) = min qX∈QX max τ∈T F (τ, qX) = max τ∈T F (τ, q∗X) =\n= max τ∈T\n∑\nθ∈Θ\nτ(θ)[RX (q ∗ X , θ)−RX(q0X , θ)] = max\nθ∈Θ [RX(q\n∗ X , θ)−RX(q0X , θ)].\nThe chain results in the inequality\nmax θ∈Θ\n[RX(q ∗ X , θ)−RX(q0X , θ)] < 0.\nConsequently, the inequality RX(q ∗ X , θ) < RX(q 0 X , θ) holds for every model θ ∈ Θ and property (3) is proved. Note that property (2) does not hold in this case.\nLet us consider the case F (τ∗, q∗X) = 0. We have the following chain:\nF (τ∗, q∗X) = max τ∈T min qX∈QX F (τ, qX) = min qX∈QX F (τ∗, qX) =\n= min qX∈QX\n∑\nθ∈Θ\nτ∗(θ)[RX(qX , θ)−RX(q0X , θ)] =\n= min qX∈QX\n[\n∑\nθ∈Θ\nτ∗(θ)RX(qX , θ)\n]\n− ∑\nθ∈Θ\nτ∗(θ)RX(q 0 X , θ).\nIt implies the equality\nmin qX∈QX\n∑\nθ∈Θ\nτ∗(θ)RX(qX , θ) = ∑\nθ∈Θ\nτ∗(θ)RX(q 0 X , θ)\nand thus property (2) is proved. Note that inequality (3) does not hold in this case.\nThe theorem states that any reasonable strategy for complex object recognition has to minimize the weighted sum of risks and to be of the form\nq∗ = argmin q∈Q\n∑\nθ∈Θ\nτ(θ)R(q, θ)\nfor some weights τ(θ). Therefore, it has to make the decision of the form\ny∗ = argmin y′∈Y\n∑\ny∈Y\n[\n∑\nθ∈Θ\nτ(θ)p(x, y; θ)\n]\nw(y, y′) (4)\nfor a signal x. For the special case of the loss function given by\nw(y, y′) =\n{\n0 if y = y′ 1 if y 6= y′ , (5)\ndecision (4) simplifies to\ny∗ = argmax y∈Y\n∑\nθ∈Θ\nτ(θ) p(x, y; θ). (6)\nThe Theorem 1 does not assign weight function τ that has to be used in (4) or (6) and so it does not uniquely define a strategy. Selecting an appropriate strategy depends on requirements that have to be satisfied in such or other applied situation. The theorem outlines only a strategy subset where one has to look for an appropriate strategy. However, the theorem results in important negative conclusion. It shows which strategies must not be used in any application. For some complex objects, commonly used maximum likelihood strategy that makes a decision\ny∗ = argmax y∈Y max θ∈Θ p(x, y; θ) (7)\noccurs to be just such improper strategy. There are quite ordinary and realistic cases when it is impossible to express the strategy (7) in a form (6).\nExample 2. Let us consider a complex object with a set Θ = {1, 2} of models, a set Y = {A,B} of states and a setX = R2 of signals. A priori probabilities pY (y = A) and pY (y = B) of states are equal and do not depend on the model. For the fixed state and model, the signal x = (x(1), x(2)) is a two-dimensional Gaussian random variable with independent components. The variances of both components x(1), x(2) are σ2 = 1 and depend neither on the state nor on the\nmodel. Expected value µ(y, θ) of the signal x = (x(1), x(2)) depends both on the state and on the model so that µ(y = A, θ = 1) = µ(y = A, θ = 2) = (0, 0), µ(y = B, θ = 1) = (0, 1), µ(y = B, θ = 2) = (1, 0). Probability density distributions pX |Y (x|y; θ) are shown on Figure 1 with circles.\nIf a loss function is given by (5) the risk RX(qX , θ) is a probability of a wrong decision made by strategy qX provided θ is a true model. Let us assume such loss function and compare the maximum likelihood strategy (7) with a family (6) of Bayesian strategies. Each strategy in the family (6) is specified by concrete values of weights τ(1) and τ(2).\nMaximum likelihood strategy (7) makes the decision y∗ = A if the observed signal x = (x(1), x(2)) belongs to the subsetXA = { (x(1), x(2)) ∈ X |x(1) ≤ 0.5, x(2) ≤ 0.5 } and makes the decision y∗ = B for signals from XB = X \\XA. The strategy is represented at Figure 1 with a border between XA and XB. The border consists of two half-lines, horizontal and vertical. In a similar way, several Bayesian strategies from the family (6) are represented with curves. The strategies are built for values 16 , 2 6 , 3 6 , 4 6 , 5 6 of the weight τ(1) and corresponding values of the weight τ(2) = 1− τ(1). One can see that maximum likelihood strategy for recognition of the considered complex object differs from any Bayesian strategy and due to Theorem 1 is improper. In turn, it means that some Bayesian strategy exists that strongly predominates maximum likelihood strategy for both models θ ∈ Θ. The Theorem 1 does not say anything on how this better strategy should be selected from the family (6). However, one can guess that it is a Bayesian strategy with τ(1) = τ(2) = 0.5. Really, if a true model is θ = 1 the maximum likelihood strategy makes a wrong decision with probability ≈ 0.37. The same is the probability of a wrong decision if θ = 2. Bayesian strategy with τ(1) = τ(2) = 0.5 makes a wrong decision with probability ≈ 0.35 both for θ = 1 and for θ = 2.\nWe have excluded from consideration all obviously bad strategies, which left us only with Bayesian strategies. This does not mean that every Bayesian strategy predominates any non-Bayesian one. It only means that for each non-Bayesian strategy some better strategy exists and this better strategy is Bayesian. Moreover, it does not mean that each Bayesian strategy in itself is suitable for any application. Selecting a particular strategy from the set of all Bayesian strategies depends on the requirements on the strategy.\nPerhaps, the most popular requirement in the theory and practice of complex hypothesis discrimination is the following so-called minimax requirement [13]. For given sets X , Y , Θ and probabilities p(x, y; θ), x ∈ X , y ∈ Y , θ ∈ Θ, a strategy qX : Y × X → R has to be found that minimizes c subject to the condition that RX(qX , θ) ≤ c for each θ ∈ Θ . In other words, the strategy\nq∗X = argmin qX∈QX max θ∈Θ RX(qX , θ) (8)\nis called a minimax strategy. Obviously, the minimax strategy is not improper and, consequently, is Bayesian. Indeed, if q∗X would be improper then such strategy q′X would exist that the inequalities\nRX(q ′ X , θ) < RX(q ∗ X , θ), θ ∈ Θ,\nwould be valid as well as the inequality\nmax θ∈Θ\nRX(q ′ X , θ) < max\nθ∈Θ RX(q\n∗ X , θ)\nthat would contradict (8). Minimax strategies are fruitfully used in the theory and practice of recognition without learning [7, 22].\nHowever, the minimax requirement is not the only possible meaningful one. In multi-criteria decision making [23] another reasonable requirement is used that differs from the minimax one. Let us follow the way shown in [23] and define a number min\nqX∈QX RX(qX , θ) for each θ ∈ Θ and call it the risk of optimal strategy.\nIn general, the risk of the optimal strategy depends on the model and so does the optimal strategy argmin\nqX∈QX\nRX(qX , θ). However, for some extraordinary convenient\ncomplex objects the strategy argmin qX∈QX RX(qX , θ) does not depend on the model.\nDefinition 5. A strategy q∗X satisfying the equality RX(q ∗ X , θ) = min\nqX∈QX RX(qX , θ)\nfor every model θ ∈ Θ is called an overall optimal strategy.\nThe existence of an overall optimal strategy is a rare exception rather than the rule. We formulate the problem of complex object recognition so that its solution is an overall optimal strategy when such strategy exists. Otherwise, the obtained strategy risk deviates from the optimal strategy risk as little as possible. More exactly, for a given complex object 〈X,Y,Θ, p : X × Y ×Θ → R〉\nand a loss function w: Y ×Y → R, the strategy qX : Y ×X → R has to be found that minimizes c subject to conditions\nRX(qX , θ)− min q′ X ∈Q RX(q ′ X , θ) ≤ c, θ ∈ Θ.\nDefinition 6. The strategy q∗X = argmin qX∈QX\n[\nmax θ∈Θ\n(\nRX(qX , θ) − min q′ X ∈Q RX(q ′ X , θ)\n)]\nis called a closest to optimal strategy.\nJust as the minimax strategy the closest to optimal strategy is not improper and, consequently, is also Bayesian. The concept of the closest to optimal strategy is a straightforward application of the recommendations in [23] to our special case. On the other hand, the closest to optimal strategy is a generalization of the minimax deviation (regret) strategy from [1]. The latter is a special case of the closest to optimal strategy when the statistical model is defined up to a priori probabilities of states.\nThe minimax approach (8) and the closest to optimal one (6) result in different Bayesian strategies. If the task is only recognition without learning, there is no reason to decide which one of these two approaches is better. However, the situation is different when the approaches are applied to recognition with learning. In this case the minimax strategies are not suitable for recognition with learning at all. They simply ignore the learning sample. This defect of minimax approach has been detected by H.Robbins at the very beginning of his empirical Bayesian approach. This defect is considered in details in the next Section 3. Then in the Section 4, we show that the closest to optimal strategies are free of this defect."
    }, {
      "heading" : "3 Empirical Bayesian approach and unsupervised",
      "text" : "learning\nThe principal property of a complex object is that its model is unknown but it is not random. It is fixed and does not change. The property gives rise to conjecture that recognizing a sample of independent and identically distributed objects can be performed better than isolated recognition of each element in the sample independently. The idea was researched by H.Robbins [16] decades ago and initiated an empirical Bayesian approach [15]. In fact, the same idea is the basis of an approach in pattern recognition known as unsupervised learning. However, these two approaches exist as if they arouse and were being developed independently of one another. We consider them as two different paths to the same goal and formulate a problem that may be considered as one of possible concretizations of empirical Bayesian approach and one of possible modifications of unsupervised learning.\nH.Robbins explains his idea on the following simple example [16], which we will use several times through the article. An object is considered with a set X = R of signals, a set Y = {1, 2} of states and a set Θ = {θ | 0 ≤ θ ≤ 1} of\nmodels. The object generates a Gaussian random signal x ∈ X , whose variance is 1, and whose mean equals 1 if the object is in state 1 and equals (−1) if the object is in state 2. Only a priori probabilities of the states are unknown. They depend on the model so that the probability of state 1 is θ and the probability of state 2 is 1− θ. In other words, the probability densities p(x, y; θ) that define the complex object are\np(x, y; θ) = θy√ 2π e− 1 2 (x−µy) 2 , (9)\nwhere θ1 = θ, θ2 = 1− θ, µ1 = 1, µ2 = (−1). The loss function is given by (5). Let us consider optimal and minimax strategies\nq opt X (θ) = argmin\nqX∈QX\nRX(qX , θ), q minmax X = argmin\nqX∈QX\nmax θ∈Θ RX(qX , θ)\nand see how the risks RX(q opt X (θ), θ) and RX(q minmax X , θ) depend on the model θ (see Figure 2). Each strategy has a drawback compared to the other one. The strategy qoptX (θ) can be used only if the model θ is known. The strategy q minmax X can be used even if the model θ is unknown, however the risk RX(q minmax X , θ) is worse than RX(q opt X (θ), θ) if θ is near to 1 or 0.\nSuppose that rather than a single object, a sample of independent random objects has to be recognized. The model θ according to which the objects are generated is unknown but fixed for the whole sample. H.Robbins formulates the main idea of empirical Bayesian approach that such a sample can be recognized much better than each of its elements independently. His important negative result is that this improvement cannot be achieved in the framework of the minimax approach. Let us state this negative result exactly.\nLet a complex object be defined by the sets X = R, Y = {1, 2}, Θ = {θ | 0 ≤ θ ≤ 1} and the probability density (9). Let the loss function have the form (5). For this complex object and a positive integer n, another complex object is defined, a so called compound complex object. It is characterized by sets Xn, Y n, Θ, probabilities\npn(x̄, ȳ; θ) =\nn ∏\ni=1\np(xi, yi; θ), x̄ ∈ Xn, ȳ ∈ Y n, θ ∈ Θ,\nand losses\nwn(ȳ, ȳ′) = 1\nn\nn ∑\ni=1\nw(yi, y ′ i), ȳ ∈ Y n, ȳ′ ∈ Y n.\nLet qXn : Y n × Xn → R be the recognition strategy for the compound object and let\nRXn(qXn , θ) = ∑\nȳ′∈Y n\n∑\nȳ∈Y n\n∑\nx̄∈Xn\nqXn(ȳ′ | x̄) pn(x̄, ȳ; θ)wn(ȳ, ȳ′)\nbe the risk of the strategy qXn with respect to the model θ. The quoted negative result states that\nmin qXn max θ∈Θ RXn(qXn , θ) = min qX max θ∈Θ RX(qX , θ)\nfor any number n. It means that the quality of the strategy\nqminmaxXN = argmin qXn max θ∈Θ RXn(qXn , θ)\nis not better than qminmaxX = argmin\nqX\nmax θ∈Θ RX(qX , θ).\nMinimax strategies do not take advantage of the situation that multiple objects with a common model have to be recognized. We will show in Section 4 that this drawback of minimax strategies also holds in a more general case.\nDespite of this clearly negative result, H.Robbins presents a heuristic strategy that for the objects of the form (9) and observed signals (x1, x2, . . . , xn) makes the decisions\nyi =\n{\n1 if xi ≥ α 2 if xi < α where α = 1 2 ln n−∑ni=1 xi n+ ∑n i=1 xi . (10)\nThe strategy (10) does not predominate qoptX (θ) because q opt X (θ) is optimal by definition. The strategy (10) does not predominate qminmaxXn because q minmax Xn is Bayesian and no strategy predominates it. However, the strategy (10) can be used even when the model θ is unknown. From this point of view it is better than the strategy qoptX (θ) that makes the decision\nyi =\n{\n1 if xi ≥ α 2 if xi < α where α = 1 2 ln 1− θ θ\n(11)\nthat depends on θ. The strategy (10) is better than qminmaxXn in a sense that its quality improves as n increases while the quality of qminmaxXn does not depend on n and remains equal to the quality of qminmaxX . Moreover, for large enough n the strategy (10) behaves almost like qoptX (θ).\nThe strategy (10) illustrates the main idea of the empirical Bayesian approach [15, 16, 17], namely that an object sample can be recognized much better than by recognizing each object independently. However, the example says nothing about the requirement that the strategy (10) should be derived from. It only makes clear that it is not the minimax one. The problem of compound object recognition is not stated as precisely as the similar problems in the Bayesian or minimax decision theory. Therefore it is unclear how the approach should be formulated in general, not only for the special case (9).\nOne of the possible specifications of the empirical Bayesian approach may be the following problem definition of compound complex object recognition. The input data of this problem consist of a quadruple 〈X,Y,Θ, p : X × Y ×Θ → R〉 representing a complex object, a loss function w: Y × Y → R and a positive integer n. These data define risks\nmin qX∈QX RX(qX , θ) = min qX∈QX\n∑\ny′∈Y\n∑\ny∈Y\n∑\nx∈X\nqX(y ′ |x) p(x, y; θ)w(y, y′)\nof optimal strategies, a compound object 〈Xn, Y n,Θ, pn : Xn × Y n ×Θ → R〉 and a loss function wn: Y n × Y n → R where\npn(x̄, ȳ; θ) =\nn ∏\ni=1\np(xi, yi; θ), x̄ ∈ Xn, ȳ ∈ Y n, θ ∈ Θ,\nwn(ȳ, ȳ′) = 1\nn\nn ∑\ni=1\nw(yi, y ′ i), ȳ ∈ Y n, ȳ′ ∈ Y n.\nLet qXn : Y n ×Xn → R be a randomized strategy that makes a decision about hidden states y1, y2, · · · , yn on the base of observed signals y1, y2, · · · , yn. Let QXn be the set of all such strategies. For qXn ∈ QXn and model θ ∈ Θ, the risk is a number\nRXn(qXn , θ) = ∑\nȳ′∈Y n\n∑\nȳ∈Y n\n∑\nx̄∈Xn\nqXn(ȳ′ | x̄) pn(x̄, ȳ; θ)wn(ȳ, ȳ′).\nThe problem of compound complex object recognition is defined so that for a quadruple 〈X,Y,Θ, p : X × Y ×Θ → R〉 , a loss function w: Y × Y → R and a positive integer n the closest to optimal strategy\nq∗Xn = argmin qXn∈QXn max θ∈Θ\n[\nRXn(qXn , θ)− min qX∈QX RX(qX , θ) ]\n(12)\nhas to be found. Obviously, the solution q∗Xn to this problem is a Bayesian strategy.\nWidely known unsupervised learning is another approach that copes with recognizing compound complex objects. It proceeds as follows. First, for given sets X , Y , Θ, probabilities p(x, y; θ), x ∈ X , y ∈ Y , θ ∈ Θ, and a sample x̄ = (x1, x2, . . . , xn) the maximum likelihood estimate\nθML = argmax θ∈Θ\nn ∏\ni=1\n∑\ny∈Y\np(xi, y; θ) = argmax θ∈Θ\nn ∑\ni=1\nlog ∑\ny∈Y\np(xi, y; θ) (13)\nis found. Then, the elements of the sample are recognized using the strategy\nqMLX = argmin qX∈QX RX(qX , θ ML) (14)\nas if the obtained model θML would be a true model. The maximum likelihood estimation (13) is of the main interest here because it is not trivial even in the simplest cases. To solve (13), the algorithms [20] are used, which later became widely known as the EM-algorithms [6]. The problem (13) is significant in its own right. Nevertheless, for some complex objects the strategy (13, 14) is improper even though each strategy of the family\nq∗X = argmin qX∈QX RX(qX , θ ∗), θ∗ ∈ Θ, (15)\nis Bayesian. Each strategy of the family (15) differs from the strategy (13), (14) at least due to the fact that their formats differ from one another, the first being Y ×X → R and the second Y n ×Xn → R. However, even if n = 1 and their formats become identical, some complex objects exist such that the strategy (13), (14) differs from each strategy of the family (15). Moreover, it differs from every Bayesian strategy.\nExample 3. Let us consider a complex object and a loss function as in the Example 2 and compare the unsupervised learning strategy (13), (14) with strategies of the family (15) for the simplest case when n = 1. For a given signal x = (x(1), x(2)) the strategy (15) makes a decision\ny∗ = argmax y∈Y p(x(1), x(2), y; θ∗), (16)\nthat depends on θ∗. For θ∗ = 1 the strategy q∗X divides the plane X = R 2 into two half-planes X∗(A, θ∗ = 1) and X∗(B, θ∗ = 1) with a horizontal line with coordinate x(2) = 0.5. For θ∗ = 2 it divides X into X∗(A, θ∗ = 2) and X∗(B, θ∗ = 2) with a vertical line with coordinate x(1) = 0.5 (see Figure 3). The strategy (13), (14) makes a decision\nyML = argmax y∈Y p(x(1), x(2), y; argmax θ∈Θ\n∑\ny∈Y\np(x(1), x(2), y; θ)) (17)\nthat does not depend on θ. This decision is A on subset\n{(x(1), x(2)) ∈ X |x(1) ≤ 0.5, x(2) ≤ 0.5} = X∗(A, θ∗ = 1) ∩X∗(A, θ∗ = 2) that differs both from X∗(A, θ∗ = 1) and from X∗(A, θ∗ = 2). Therefore, the strategy (13), (14) differs both from argmin\nqX∈QX\nRX(qX , θ ∗ = 1) and from\nargmin qX∈QX\nRX(qX , θ ∗ = 2). It has already been shown in Example 2 that the\nstrategy differs from any Bayesian strategy.\nThe heuristic strategy (10) by H.Robbins, the closest to optimal strategy (12) and the unsupervised learning strategy (13, 14) are three different strategies. They are compared experimentally for the complex model (9) in Example 9 in Section 6. In this experiment, the closest to optimal strategy yields the best result, the unsupervised learning strategy the worst, and the heuristic strategy an intermediate one."
    }, {
      "heading" : "4 Recognition with learning",
      "text" : "As in the previous sections, we consider complex objects, defined by a quadruple 〈X,Y,Θ, p : X × Y ×Θ → R〉. In addition, a certain source of data is considered. The source generates the so called learning information z that belongs to a finite set Z. The learning information z is random and depends on the model θ ∈ Θ, so that the probability p(z; θ) is defined for every z ∈ Z and θ ∈ Θ. It is crucial that for a fixed model θ, the learning information z depends neither on the current state y nor on the current signal x of the object, so that p(z, x, y; θ) = p(z; θ)p(x, y; θ). Formally, the source of learning information is defined with a triple 〈Z,Θ, p : Z ×Θ → R〉.\nUsually, learning information is obtained in a following way. The object is put into a special condition so that even its state becomes observable. Under this condition, a sample (x1, y1;x2, y2; . . . xn, yn) of n independent random pairs (x, y) is observed so that Z = (X × Y )n and\np(z; θ) = p(x1, y1, x2, y2, . . . , xn, yn; θ) =\nn ∏\ni=1\np(xi, yi; θ).\nOur formulation is more general. In case of unsupervised learning, the learning information z is an element of Xn. When the learning information is absent, we introduce a special element 0 and define Z = {0} and p(z = 0; θ) = 1 for every model θ ∈ Θ. The learning information can have a different nature, for example, it can be an expert opinion about the model. Moreover, the learning information can come from several independent sources simultaneously as it is mentioned in [12]. In this case, the set Zs and the probabilities p(z; θ, s), z ∈ Zs, θ ∈ Θ, s ∈ S, are given for every source s ∈ S.\nThe problem of recognition with learning means that for a given complex object and a learning information source a strategy has to be developed that makes a decision about current object state based both on the learning information and on the current signal generated by the object. We express this statement in the form\n〈X,Y,Θ, p : X × Y ×Θ → R〉 , 〈Z,Θ, p : Z ×Θ → R〉 ↓ (18)\nqX×Z : Y ×X × Z → R.\nThe value qX×Z(y ′|x, z) of the strategy qX×Z is a conditional probability of the decision ”a state of an object is y′” under condition that a signal x is observed and learning information z is available. The form\n〈X,Y,Θ, p : X × Y ×Θ → R〉 ↓ (19)\nqX : Y ×X → R.\nexpresses the problem of recognition without learning. One can see that the form (18) is a special case of (19). Really, (18) can be equivalently expressed as\n〈X∗, Y,Θ, p∗ : X∗ × Y ×Θ → R〉 ↓\nqX∗ : Y ×X∗ → R\nwith X∗ = X × Z and p∗(x∗, y; θ) = p∗(x, z, y; θ) = p(z; θ) · p(x, y; θ). In other words, a complex object with a source of learning information can be replaced with another object that generates specific signals composed of two components. The first component depends both on the current state of the object and on the\nmodel. The second one depends directly only on the model and under a fixed model depends neither on the current state nor on the first component.\nTheorem 1, proved in Section 2 in the general form, remains valid for recognition with learning. However, for subsequent considerations another formulation of the theorem is more convenient. A strategy qX×Z : Y ×X × Z → R will be represented as a superposition of two functions: a learning procedure gZ : Z → Q and a recognition strategy qX : Y ×X → R. Given learning information z, the learning procedure gZ defines the recognition strategy gZ(z): Y ×X → R. This means that after learning with random information z and observing a signal x, the decision ”the object is in state y′” is made with probability gZ(z)(y\n′ |x). Let GZ be the set of all possible learning procedures of the form Z → Q.\nAs before, RX(qX , θ) is the risk of recognition strategy qX ∈ QX with respect to the model θ ∈ Θ. The number RX(gZ(z), θ) is the risk of the strategy gZ(z), obtained by applying the learning procedure gZ : Z → Q to the learning information z ∈ Z. The risk RX(gZ(z), θ) is random because it depends on the random information z. Let RZ(gZ , θ) denote the expectation of the risk over the set of all learning informations,\nRZ(gZ , θ) = ∑\nz∈Z\np(z; θ)RX(gZ(z), θ).\nDefinition 7. A learning procedure g∗Z is called Bayesian if a weight function τ : Θ → R exists such that\ng∗Z = argmin gZ∈GZ\n∑\nθ∈Θ\nτ(θ)RZ (gZ , θ).\nNow the restriction of Theorem 1 to learning procedures reads as follows.\nTheorem 2. For every learning procedure g0Z ∈ G, either a weight function τ∗ ∈ T exists such that\ng0Z = argmin gZ∈GZ\n∑\nθ∈Θ\nτ∗(θ)RZ(gZ , θ)\nor a learning procedure g∗Z exists such that for every model θ ∈ Θ the inequality\nRZ(g ∗ Z , θ) < RZ(g 0 Z , θ)\nholds. These two properties of learning procedures are incompatible.\nThe theorem says that every learning procedure is either Bayesian or improper. In the special case when a sample (x1, y1;x2, y2; . . . ;xn, yn) is the learning information and a signal x0 is observed, the decision y\n∗ about the current object state has to be of the form\ny∗ = argmin y′∈Y\n∑\ny0∈Y\n∑\nθ∈Θ\nτ(θ)\nn ∏\ni=0\np(xi, yi; θ)w(y0, y ′) (20)\nwith some weights τ(θ), θ ∈ Θ. For the loss function (5), the strategy is simplified to\ny∗ = argmax y0∈Y\n∑\nθ∈Θ\nτ(θ)\nn ∏\ni=0\np(xi, yi; θ).\nStrategies that are commonly used in pattern recognition have other form. The most widely known is the method based on maximum likelihood estimation [7, 22]. According to this method, the model\nθML = argmax θ∈Θ\nn ∏\ni=1\np(xi, yi; θ) (21)\nis found and then for an observed signal x the decision\nyML = argmin y′∈Y\n∑\ny∈Y\np(x, y; θML)w(y, y′) (22)\nis made, as if the maximum likelihood model was identical to the true one. For some complex objects, the strategy (21, 22) differs from (20) and thus it is improper. One of such complex objects was considered in Examples 2 and 3. The following example shows more sharply the main defect of maximum likelihood learning (21), (22).\nExample 4. Let us return to complex object 〈X,Y,Θ, p : X × Y ×Θ → R〉 described in a Section 3 with sets X = R, Y = {1, 2}, Θ = {θ|0 ≤ θ ≤ 1} and probability density distribution p : X × Y × Θ → R of the form (9). The loss function is given by (5). If no learning sample is available one can use a minimax recognition strategy that does not depend on a model. The strategy decides that y = 1 if x ≥ 0 and y = 2 if x < 0. The strategy makes a wrong decision with probability ≈ 0.16 independently of a model and seems to be quite reasonable.\nSuppose now that the learning sample is obtained that consists of only pair (x0, y0). It is very small sample but one can expect that it enables at least small improvement as compared with minimax strategy that works if no learning sample is available. However, the maximum likelihood learning (21), (22) results in a strategy that ignores a current signal x at all and decides that the object is in the state y0 independently of signal generated by object. So, if the learning sample is of very small size it is reasonable to ignore it rather than to make use of it with maximum likelihood learning.\nNevertheless, the maximum likelihood learning (21, 22) has an important positive property that can be called asymptotic optimality. Asymptotic optimality qualifies a learning procedure in a similar sense as consistency qualifies a parameter estimate. We formulate and prove this property after several preliminary remarks.\nFirst, we assume that for every two different models θ1 and θ2 a pair (x, y) ∈ X×Y exists such that p(x, y; θ1) 6= p(x, y; θ2). Obviously, this assumption is not\na restriction. Really, if there are two models θ1 and θ2 such that p(x, y; θ1) = p(x, y; θ2) for each pair (x, y) ∈ X ×Y they can be treated as equivalent and be replaced with one model.\nThen, we rely on properties of entropy-like functions n ∑\ni=1\nai log xi of 2n strictly\npositive arguments such that n ∑\ni=1\nai = n ∑\ni=1\nxi = 1. However, we extrapolate such\nfunctions onto the set of non-negative numbers assuming that a log x = 0 if a = x = 0 and a log x = −∞ if a > 0 and x = 0. Under these assumptions the difference\n∑\nx∈X\n∑\ny∈Y\np(x, y; θ∗) log p(x, y; θ′)− ∑\nx∈X\n∑\ny∈Y\np(x, y; θ∗) log p(x, y; θ∗)\nis defined for every pair (θ∗, θ′) and is strictly negative if θ∗ 6= θ′. The statement results from the known property of entropy-like functions that\nmax x1 max x2 . . .max xn\nn ∑\ni=1\nai log xi = n ∑\ni=1\nai log ai\nand if xi∗ 6= ai∗ for some i∗ then n ∑\ni=1\nai log ai > n ∑\ni=1\nai log xi.\nAt last, we use a fundamental relation between the averaged value 1 n\nn ∑\ni=1\nxi\nand the expected value ∑\nx∈X x · p(x). The relation restricted to our special case has the following formulation. Let n be a positive number, X be a finite set of numbers, p(x) be the probability of x ∈ X and Zn = {z ∈ Xn| 1n n ∑\ni=1 xi ≥ 0}. If ∑\nx∈X\nx · p(x) < 0 then\nlim n→∞\n∑\nz∈Zn\nn ∏\ni=1\np(xi) = 0. (23)\nTheorem 3. Let Z = (X × Y )n and the procedure gMLn :(X × Y )n → Q be defined by\ngMLn (z) = argmin qX∈QX RX(qX , argmax θ∈Θ p(z; θ)), z ∈ (X × Y )n. (24)\nThen for every θ∗ ∈ Θ\nlim n→∞\n[RZ(g ML n , θ ∗)− min qX∈QX RX(qX , θ ∗)] = 0. (25)\nProof. Let us choose an arbitrary model θ∗ and fix it for the whole proof. Let us denote\nθML(z) = argmax θ∈Θ p(z; θ) = argmax θ∈Θ\nn ∏\ni=1\np(xi, yi; θ).\nThe model θML(z) is random because it depends on a random sample z. If the probability of the sample z is\np(z; θ∗) = p(x1, y1, x2, y2, . . . xn, yn; θ ∗) =\nn ∏\ni=1\np(xi, yi; θ ∗)\nthen inequality θML(z) 6= θ∗ is a random event with its probability depending on n. We prove that the probability of inequality θML(z) 6= θ∗ converges to zero when n increases. Let us define three subsets of the set Z = (X × Y )n:\nZ(θ∗, θ′, n) = {z|p(z; θ′) ≥ p(z; θ∗)}, Z(θ∗, n) = ⋃\nθ′∈Θ\\{θ∗}\nZ(θ∗, θ′, n),\nZML(θ∗, n) = {z|θML(z) 6= θ∗} and denote P (Z ′) = ∑\nz∈Z′ p(z; θ∗) for each Z ′ ⊂ Z. Evidently,\nZML(θ∗, n) ⊂ Z(θ∗, n) = ⋃\nθ′∈Θ\\{θ∗}\nZ(θ∗, θ′, n)\nthat results in\nP (ZML(θ∗, n)) ≤ P (Z(θ∗, n)) ≤ ∑\nθ′∈Θ\\{θ∗}\nP (Z(θ∗, θ′, n)). (26)\nThe subset Z(θ∗, θ′, n) consists of all samples z = (x1, y1, x2, y2, . . . xn, yn) that satisfy the inequality\n1\nn\nn ∑\ni=1\n[log p(xi, yi; θ ′)− log p(xi, yi; θ∗)] ≥ 0.\nThe left side of the inequality is the average of n independent and identically distributed random numbers log p(x, y; θ′)− log p(x, y; θ∗). The mean value\n∑\nx∈X\n∑\ny∈Y\np(x, y; θ∗) log p(x, y; θ′)− ∑\nx∈X\n∑\ny∈Y\np(x, y; θ∗) log p(x, y; θ∗)\nof these numbers is negative and therefore lim n→∞\nP (Z(θ∗, θ′, n) = 0 due to (23)\nand\nlim n→∞\nP (ZML(θ∗, n)) = 0 due to (26). The proof is concluded by the chain\nlim n→∞\n[RZ(g ML n , θ ∗)− min qX∈QX RX(qX , θ ∗)] =\n= lim n→∞\n∑\nz∈(X×Y )n\np(z; θ∗)[RX(g ML n (z), θ ∗)− min qX∈QX RX(qX , θ ∗)]\n= lim n→∞\n∑\nz∈(X×Y )n θML(z) 6=θ∗\np(z; θ∗)[RX(g ML n (z), θ ∗)− min qX∈QX RX(qX , θ ∗)]\n≤ lim n→∞\n∑\nz∈(X×Y )n θML(z) 6=θ∗\np(z; θ∗)[max y∈Y max y′∈Y w(y, y′)−min y∈Y min y′∈Y w(y, y′)]\n= lim n→∞ [max y∈Y max y′∈Y w(y, y′)−min y∈Y min y′∈Y\nw(y, y′)] ∑\nz∈(X×Y )n θML(z) 6=θ∗\np(z; θ∗)\n= [max y∈Y max y′∈Y w(y, y′)−min y∈Y min y′∈Y w(y, y′)] lim n→∞ P (ZMLn (θ ∗, n)) = 0.\nWe restrict our further considerations to Bayesian learning procedures, focusing in particular on the minimax procedure argmin\ngZ∈GZ\nmax θ∈Θ RZ(gZ , θ) and the\nclosest to optimal procedure argmin gZ∈GZ max θ∈Θ\n[\nRZ(gZ , θ) − min qX∈QX RX(qX , θ) ] . Both\nprocedures belong to the Bayesian class. However, minimax procedures have a fundamental drawback that they sometimes do not make use of a learning sample, no matter how large it is. For a rather wide class of complex objects, the minimax learning procedure simply ignores the learning sample.\nTheorem 4. Let for a complex object 〈X,Y,Θ, p : X × Y ×Θ → R〉 such a model θ∗ and a strategy q∗X exist that\nq∗X = argmin qX∈QX RX(qX , θ ∗), θ∗ = argmax θ∈Θ RX(q ∗ X , θ).\nThen any source 〈Z,Θ, p : Z ×Θ → R〉 of learning information and any learning procedure gZ : Z → Q satisfy the inequality\nmax θ∈Θ RZ(gZ , θ) ≥ max θ∈Θ RX(q ∗ X , θ). (27)\nProof. For any learning procedure gZ , we have the chain\nmax θ∈Θ\nRZ(gZ , θ) ≥ RZ(gZ , θ∗) = ∑\nz∈Z\np(z; θ∗)RX(gZ(z), θ ∗) ≥\n≥ ∑ z∈Z p(z; θ∗)RX(q ∗ X , θ ∗) = RX(q ∗ X , θ ∗) = max θ∈Θ RX(q ∗ X , θ).\nThe theorem shows that there are complex objects for which the minimax approach is particularly inappropriate. Inequality (27) states that any learning procedure, however sophisticated it may be, is useless from the minimax point of view. It cannot yield a recognition strategy that would be better than some strategy that does not use the learning sample at all. The following examples show that such situations are nothing unusual.\nExample 5. The conditions of Theorem 4 are satisfied if for every weight function τ ∈ T , the model set Θ contains a model θ′ such that p(x, y; θ′) = ∑\nθ∈Θ\nτ(θ)p(x, y; θ). One of such model sets is just the set (9), for which H.Robbins\nshows that the minimax strategy does not achieve the result that the empirical Bayesian approach does.\nExample 6. Let X = R2, Y = {1, 2}, pY (y = 1) = pY (y = 2) = 0.5, and pX |Y (x |y) be a two-dimensional Gaussian probability density distribution with the unit covariance matrix. The expected values θy, y ∈ {1, 2}, of the random signal x ∈ X are unknown. It is only known that they belong to given closed convex disjoint sets Θ1 and Θ2. This means, the set of models is Θ = Θ1 ×Θ2. This model set does not satisfy the condition of Example 5 but it satisfies the conditions of Theorem 4. In this case, the model θ∗ ∈ Θ is the pair\n(θ∗1 , θ ∗ 2) = argmin\nθ1∈Θ1,θ2∈Θ2\n(θ1 − θ2)2,\nand the strategy q∗ makes the decision\ny∗ =\n{\n1 if (x− θ∗1)2 ≤ (x− θ∗2)2, 2 if (x− θ∗1)2 > (x− θ∗2)2.\nIt is an acceptable recognition strategy for the complex object considered. However, it is clear that a better strategy could be found if a learning sample was available, in particular if this sample was large enough. Nevertheless, the minimax strategy ignores this opportunity even if the learning sample is very large.\nExample 7. Let py: X → R, y ∈ {1, 2}, be two functions whose values are conditional probabilities pX |Y (x |y), so that py(x) = pX |Y (x |y). It is a typical situation in pattern recognition that the functions p1 and p2 are unknown and only a set P is known that contains both p1 and p2. In this case, the set of models is Θ = P × P , the model θ∗ is any pair p1 = p2, and q∗X is the strategy that for every signal x decides about the state y with equal probabilities q∗X(y = 1 |x) = q∗X(y = 2 |x). This strategy is quite bad but no better strategy is possible. However, it is clear, at least intuitively, that the situation could be\nimproved if a learning sample was available. Minimax strategies are not able to take advantage of a learning sample, no matter how large.\nTheorem 4 is a severe negative result. The result is quite transparent and easy to prove, especially, if one notices that the condition of the theorem states that the function RX : QX × Θ → R has a saddle point and the pair (q∗X , θ∗) is this point. Probably, the property was discovered many times before. In a special case, this property was noticed by H.Robbins when he wanted to derive the empirical Bayesian approach from the ideas of minimax decisions [16]. This clarity may have been the reason for the hasty and pessimistic conclusion that recognition with learning is something very different from recognition without learning, something beyond the statistical decision theory.\nTheorems 3 and 4 show the state of today’s knowledge of complex object recognition as well as a gap in this knowledge. The minimax approach is suited for recognizing hidden state based on the signal that depends both on the state and on the model. However, for some complex objects this approach ignores learning information that depends directly only on the model and does not depend on the state. Unlike minimax approach, maximum likelihood learning makes use of the learning sample. It allows to recognize complex objects almost as if the model of the object is known. However, it is possible only when the learning sample size can be increased almost infinitely. For limited samples and for some objects maximum likelihood learning is not Bayesian and is predominated by some other methods. Therefore, today’s developers have to cope alone with the problem when the model of an object is not completely known but several few examples of object’s behaviour (let us say, 4-5) are available.\nLet us see now how the closest to optimal learning procedures behave. As shown by the following theorem similar to Theorem 4, these procedures are also useless for a certain class of complex objects.\nTheorem 5. Let for a complex object 〈X,Y,Θ, p : X × Y ×Θ → R〉 a model θ∗ and a strategy q∗X exist such that\nq∗X = argmin qX∈QX [RX(qX , θ ∗)− min q′ X ∈QX RX(q ′ X , θ ∗)], (28)\nθ∗ = argmax θ∈Θ [RX(q ∗ X , θ)− min q′ X ∈QX RX(q ′ X , θ)]. (29)\nThen any source 〈Z,Θ, p : Z ×Θ → R〉 of learning information and any learning procedure gZ : Z → Q satisfy the inequality\nmax θ∈Θ [RZ(gZ , θ)− min q′ X ∈QX RX(q ′ X , θ)] ≥ max θ∈Θ [RX(q ∗ X , θ)− min q′ X ∈QX RX(q ′ X , θ)].\nHowever, the consequences of this theorem for closest to optimal learning are not so destructive as those of Theorem 4 for minimax learning. In fact, conditions (28) and (29) imply that an overall optimal Bayesian strategy exists\nfor the recognized object, namely the strategy q∗X . Really, it follows from (28) that\nq∗X = argmin qX∈QX [RX(qX , θ ∗)− min q′ X ∈QX RX(q ′ X , θ ∗)] = argmin qX∈QX RX(qX , θ ∗),\nRX(q ∗ X , θ ∗)− min q′ X ∈QX RX(q ′ X , θ ∗) = 0.\nThus, a condition (29) obtains a form\n0 ≥ RX(q∗X , θ)− min q′ X ∈QX RX(q ′ X , θ) for all θ ∈ Θ.\nEvidently, RX(q ∗ X , θ)− min\nq′ X ∈QX\nRX(q ′ X , θ) ≥ 0 for all θ ∈ Θ,\nand so RX(q ∗ X , θ) = min\nq′ X ∈QX\nRX(q ′ X , θ) for all θ ∈ Θ.\nIn this case, any learning approach is useless, not only closest to optimal one. Just as maximum likelihood learning, closest to optimal learning is also asymptotically optimal in the sense of the following theorem.\nTheorem 6. Let Z = (X × Y )n and the learning procedure g∗n: (X × Y )n → Q be defined by\ng∗n = argmin gZ∈GZ max θ∈Θ [RZ(gZ , θ)− min qX∈QX RX(qX , θ)].\nThen lim n→∞ max θ∈Θ [RZ(g ∗ n, θ)− min qX∈QX RX(qX , θ)] = 0. (30)\nProof. We proved earlier (see (25)) that\nlim n→∞\n[RZ(g ML n , θ)− min\nqX∈QX RX(qX , θ)] = 0\nfor procedure (24) and every model θ. Consequently,\nlim n→∞ max θ∈Θ\n[RZ(g ML n , θ)− min\nqX∈QX RX(qX , θ)] = 0 (31)\nbecause the number |Θ| of models is finite. Clearly,\nmin gZ∈GZ max θ∈Θ [RZ(gZ , θ)− min qX∈QX RX(qX , θ)] ≤ max θ∈Θ [RZ(g ML n , θ)− min qX∈QX RX(qX , θ)]. (32) Equality (31) and inequality (32) imply (30).\nHence, the concept of closest to optimal learning fills the above-mentioned gap in complex object recognition. It embraces the whole range of learning sample sizes, from zero to infinity. Unlike minimax approach it ignores the learning sample only if an overall optimal strategy exists for the recognized object. Certainly, in this case no information that specifies the model is needed. Similarly to the risks of maximum likelihood learning, the risks of closest to optimal learning converge to the risks of the optimal strategy. Unlike maximum likelihood learning, closest to optimal learning is Bayesian and is predominated by no other learning procedure."
    }, {
      "heading" : "5 Developing closest to optimal learning proce-",
      "text" : "dures\nWe consider a complex object, presented with a quadruple 〈X,Y,Θ, p : X × Y ×Θ → R〉 , and a source of learning information, presented with a triple 〈Z,Θ, p : Z ×Θ → R〉. The following two theorems show the way of developing the closest to optimal learning procedure. Clearly, this procedure is predominated with no procedure and therefore it is Bayesian. The next theorem specifies a weight function, with respect to which it is Bayesian.\nTheorem 7. The closest to optimal learning procedure\ng∗Z = argmin gZ∈GZ max θ∈Θ\n[\nRZ(gZ , θ)− min qX∈QX RX(q, θ) ]\nis a Bayesian procedure with respect to weights τ∗(θ), θ ∈ Θ that maximize the difference\n[\nmin gZ∈GZ\n∑\nθ∈Θ\nτ(θ) ·RZ(gZ , θ) ] − [ ∑\nθ∈Θ\nτ(θ) · min qX∈QX RX(qX , θ)\n]\n.\nProof. Let us define a function F : T ×GZ → R given by\nF (τ, gZ) =\n[\n∑\nθ∈Θ\nτ(θ) ·RZ(gZ , θ) ] − [ ∑\nθ∈Θ\nτ(θ) · min qX∈QX RX(qX , θ)\n]\nthat allows to express procedure g∗Z and weight function τ ∗ in a form\ng∗Z = argmin gZ∈GZ max τ∈T F (τ, gZ), τ ∗ = argmax τ∈T min gZ∈GZ F (τ, gZ).\nWe have to prove that g∗Z = argmin gZ∈GZ\n∑\nθ∈Θ τ∗(θ)RZ(gZ , θ). The function F : T × GZ → R is a linear function of gZ for every fixed τ and a linear function of τ for every fixed gZ . It is defined on the Cartesian product of two convex closed sets T and GZ . Due to the well-known duality theorem [2, 3, 10] this function satisfies the equality\nmin gZ∈GZ\n[\nmax τ∈T F (τ, gZ)\n]\n= F (τ∗, g∗Z) = max τ∈T\n[\nmin gZ∈GZ F (τ, gZ)\n]\n.\nThis implies the equality F (τ∗, g∗Z) = min gZ∈GZ F (τ∗, gZ) and the chain\ng∗Z = argmin gZ∈GZ F (τ∗, gZ) = argmin gZ∈GZ\n[\n∑\nθ∈Θ\nτ∗(θ)RZ(gZ , θ)− ∑\nθ∈Θ\nτ∗(θ) min qX∈QX R(q, θ)\n]\n=\n= argmin gZ∈GZ\n∑\nθ∈Θ\nτ∗(θ)RZ(gZ , θ).\nFor each gZ ∈ GZ , the function F (τ, gZ) depends linearly on weights τ(θ), θ ∈ Θ. Therefore, evidently, the function Φ(τ) = mingZ∈GZ F (τ, gZ) is a concave function of these weights. Nevertheless, we deduce this fact directly from the definition of concavity and obtain an explicit expression for the supergradient of the function Φ, which is necessary for its further maximization.\nTheorem 8. The function Φ: RΘ → R with values\nΦ(τ) =\n[\nmin gZ∈GZ\n∑\nθ∈Θ\nτ(θ) ·RZ(gZ , θ) ] − [ ∑\nθ∈Θ\nτ(θ) · min qX∈QX RX(qX , θ)\n]\nis a concave function of weights τ(θ), θ ∈ Θ.\nProof. We show that for every point τ∗ ∈ RΘ some linear function L: RΘ → R exists that satisfies the inequality\nL(τ − τ∗) ≥ Φ(τ) − Φ(τ∗) (33)\nfor every τ ∈ RΘ. Note that the gradient of this linear function is a supergradient of the function Φ at the point τ∗. One of possible linear functions that satisfy (33) is the function with values\nL(τ) = ∑\nθ∈Θ\nτ(θ)\n[\nRZ(g ∗ Z , θ)− min\nqX∈QX RX(qX , θ)\n]\nwhere g∗Z = argmin gZ∈GZ\n∑\nθ∈Θ\nτ∗(θ)RZ (gZ , θ).\nIndeed,\nL(τ − τ∗) =\n= ∑\nθ∈Θ\nτ(θ)\n[\nRZ(g ∗ Z , θ)− min\nqX∈QX RX(qX , θ)\n]\n− ∑\nθ∈Θ\nτ∗(θ)\n[\nRZ(g ∗ Z , θ)− min\nqX∈QX RX(qX , θ)\n]\n≥\n≥ min gZ∈GZ\n∑\nθ∈Θ\nτ(θ)\n[\nRZ(gZ , θ)− min qX∈QX RX(qX , θ)\n]\n−\n− min gZ∈GZ\n∑\nθ∈Θ\nτ∗(θ)\n[\nRZ(gZ , θ)− min qX∈QX RX(qX , θ)\n]\n= Φ(τ)− Φ(τ∗).\nDue to concavity of the function Φ, it can be maximized using well-known methods of convex optimization, such as supergradient hill-climbing [21]. To do that, an auxiliary procedure to project a point τ ∈ RΘ onto the set T is needed. Strictly speaking, this procedure finds the point argminτ ′∈T (τ\n′− τ)2 for a given point τ ∈ RΘ. In our case the procedure is as follows:\n1. for all θ ∈ Θ do τ ′(θ) = τ(θ); 2. for all θ ∈ Θ do τ ′(θ) = τ ′(θ) − (∑θ∈Θ τ ′(θ)− 1) · |Θ|−1; 3. if τ ′(θ) ≥ 0 for all θ ∈ Θ then STOP ; 4. for all θ ∈ Θ do τ ′(θ) = max{τ ′(θ), 0}; 5. go to 2;\nNext we present a technique to compute the closest to optimal learning procedure. We use the word ‘technique’ rather than ‘algorithm’, to emphasize that some of its steps are far from trivial and have to be developed specifically for every task at hand. The technique can be seen as an algorithm only in very simple cases, such as the examples given in Section 6.\nInitially, the following input data have to be given:\n1. sets X , Y , Θ and probabilities p(x, y; θ), x ∈ X , y ∈ Y , θ ∈ Θ, which define the complex object to be recognized;\n2. set Z and probabilities p(z; θ), z ∈ Z, θ ∈ Θ, which define the source of learning information;\n3. values of the loss function w(y, y′) for every y ∈ Y, y′ ∈ Y 4. numbers minqX∈QX RX(qX , θ) for every θ ∈ Θ; 5. sequence of numbers γi, i = 1, 2, . . ., such that\nlimi→∞ γi = 0 and limn→∞ ∑n i=1 γi = ∞; 6. required accuracy ε > 0;\n7. initial weights τ(θ), θ ∈ Θ; for example, τ(θ) = |Θ|−1; 8. initial values of numbers S and s, which will keep an upper and lower\nbound of the quality of the learning procedure; for example S = ∞ and s = 0.\nThe i-th iteration of the technique proceeds as follows:\n1. construct a Bayesian procedure gZ for the current weight function τ ;\n2. for every model θ ∈ Θ calculate numbers ∆(θ) = RZ(gZ , θ)−minqX∈QX RX(qX , θ); they form the gradient of function Φ at the current point τ ;\n3. calculate ∆ = maxθ ∆(θ); it is the quality of the current procedure gZ ; 4. calculate ∆̄ = ∑\nθ τ(θ)∆(θ); it is the value of Φ(τ) at the current point τ ;\n5. update the bounds as S = min{S,∆} and s = max{s, ∆̄}; 6. if S − s < ε then STOP ;\n7. set the weights τ(θ), θ ∈ Θ, to the projection of the weights τ(θ)+ γi∆(θ) on T ;\nSupergradient optimization, which iteratively updates the initial weights, is performed only in steps 2, 3 and 7. The other steps calculate the data used in the stopping condition, which, as will be shown, guarantees the required accuracy.\nLet S and s be the bounds after termination of the algorithm. It means that a learning procedure g′Z with the quality\nmax θ∈Θ\n[RZ(g ′ Z , θ)− min\nqX∈QX RX(qX , θ)] = S (34)\nhas been obtained at some iteration. It also means that a weight function τ ′ with\nΦ(τ ′) =\n[\nmin gZ∈GZ\n∑\nθ∈Θ\nτ ′(θ) ·RZ(gZ , θ) ] − [ ∑\nθ∈Θ\nτ ′(θ) · min qX∈QX RX(qX , θ)\n]\n= s (35)\nhas been obtained at some (possibly different) iteration. As shown by the following theorem, the procedure g′Z and the weight function τ\n′ have the property that the difference between the quality of the obtained procedure and the best procedure is not greater than ε.\nTheorem 9. The learning procedure g′Z satisfies the inequality\nmax θ∈Θ\n[RZ(g ′ Z , θ)− min qX∈QX RX(qX , θ)]− min gZ∈GZ max θ∈Θ [RZ(gZ , θ)− min qX∈QX RX(qX , θ)] < ε.\nProof. As before, we consider the function\nF (τ, gZ) = ∑\nθ∈Θ\nτ(θ) [ RZ(gZ , θ)− min qX∈QX RX(qX , θ) ] .\nBy (34) and (35), for the procedure g′X and the weight function τ ′ we have the chain\ns = min gZ∈GZ\n∑\nθ∈Θ\nτ ′(θ) [ RZ(gZ , θ)− min qX∈QX RX(qX , θ) ] = min gZ∈GZ F (τ ′, gZ)\n≤ max τ∈T min gZ∈GZ F (τ, gZ) = min gZ∈GZ max τ∈T F (τ, gZ)\n= min gZ∈GZ max τ∈T\n∑\nθ∈Θ\nτ(θ) [ RZ(gZ , θ)− min qX∈Q RX(qX , θ) ]\n= min gZ∈GZ max θ∈Θ [RZ(gZ , θ)− min qX∈QX RX(qX , θ) ] ≤ max θ∈Θ [RZ(g ′ Z , θ)− min qX∈QX RX(qX , θ) ] = S.\nFrom this chain and the stopping condition S − s < ε, it follows that\nmax θ∈Θ\n[RZ(g ′ Z , θ)− min qX∈QX RX(qX , θ)]− min gZ∈GZ max θ∈Θ [RZ(gZ , θ)− min qX∈QX RX(qX , θ)] < ε."
    }, {
      "heading" : "6 Examples",
      "text" : "For several simplest cases, the closest to optimal learning procedure g0 is compared with the maximum likelihood learning procedure gML and with another learning procedures gH (to be defined later). In all the examples, we have an object with sets X = R, Y = {1, 2} and probability density distribution of the form\np(x, y; θ) = py · ( √ 2π)−1 · e− 12 (x−µy)2\nwith some unknown parameters specified later. Such object is a complex object in a sense of Definition 2 and the compared learning procedures for it can be implemented almost exactly. We use the loss function (5). The required accuracy for constructing closest to optimal procedures is ε = 0.01.\nExample 8. [18] Let µ1 = 1, µ2 = (−1), and only the a priori probabilities py, y ∈ {1, 2}, be unknown. Thus, the set of models is Θ = {θ | 0 ≤ θ ≤ 1} and the a priori probabilities py of the states are p1 = θ, p2 = 1− θ. This is the example used by H.Robbins to explain his idea of empirical Bayesian approach. We simplify the example even further by assuming that the learning information is not a signal sample (x1, x2, . . . , xn) but a state sample (y1, y2, . . . , yn). To use the technique from Section 5 the set Θ is discretized with a step 0.05.\nFigure 2 (page 12) shows how the probabilities minq∈Q R(q, θ) andR(q minmax, θ)\ndepend on model θ. Note that they do not depend on the sample size n because they do not depend on the sample at all. Figure 4 shows how the risks RG(g ML, θ) and RG(g 0, θ) depend on the model θ for the sample sizes n = 1, 2, 4.\nIt is striking how high the risk RG(g ML, θ) is for some models. Of course, one cannot expect the risk to be too low because the learning samples are very small and thus they bring little information about the true model. Nevertheless, however small the amount of information, it is non-zero and it should be used to improve subsequent recognition, at least to some extent. But the example shows that it is much better to ignore this information than to use it with maximum likelihood learning. The example illustrates quite transparently the main drawback of maximum likelihood learning.\nExample 9. For the same complex object as in Example 8, let the learning information be a sequence (x1, x2, . . . , xn) rather than (y1, y2, . . . , yn). This is exactly the case considered by H.Robbins. The risk RG(g\n0, θ) of closest to optimal learning is compared with the risk RG(g\nML, θ) of maximum likelihood learning. Even for this simple complex object, it is not easy to find the maximum likelihood model\nθ1 = argmax θ∈Θ\nn ∑\ni=1\nlog [ θ · e− 12 (xi−1)2 + (1− θ) · e− 12 (xi+1)2 ] .\nTherefore, we also consider the heuristic procedure (10) by H.Robbins, which is denoted by gH . The procedure is not based on the maximum likelihood\nestimate θ1 but on the consistent estimate θ2 = 1 2n\nn ∑\ni=1\nxi + 1 2 , which can be\neasily calculated. Figure 5 shows how the risks RG(g 0, θ), RG(g ML, θ) and RG(g H , θ) depend on the model θ for the sample sizes n = 1, 2, 4.\nExample 10. Let X = R, Y = {1, 2}, p1 = p2 = 0.5, and µ1 = 0. The expected value µ2 = θ is unknown, it is only known that it belongs to the set Θ = {−6,−5.8,−5.6, . . . , 5.8, 6} of 61 possible values. Thus,\np(x, y = 1; θ) = 0.5 · ( √ 2π)−1 · e− 12x2 , p(x, y = 2; θ) = 0.5 · ( √ 2π)−1 · e− 12 (x−θ)2 .\nThe learning information is a sequence (x1, x2, . . . , xn) of random signals generated by the object in state y = 2. We did not observe any significant difference between maximum likelihood learning and closest to optimal learning.\nExample 11. Here we have the same object as in the previous example but\nthe learning information is more complicated. It is a sequence (x1, x2, . . . , xn) of signals generated by the object with unknown states. In this case, the maximum likelihood estimate\nθ1 = argmax θ∈Θ\nn ∑\ni=1\nlog [ e− 1 2 x2i + e− 1 2 (xi−θ) 2 ]\nis not easy to find. Therefore, besides the maximum likelihood estimate we\nalso consider the consistent estimate θ2 = 2 n\nn ∑\ni=1\nxi along with the corresponding\nlearning procedure gH . Figure 6 shows how the risks RG(g 0, θ), RG(g ML, θ), RG(g\nH , θ) depend on the model θ for the sample sizes n = 1, 2, 4. One can see that neither the maximal likelihood procedure gML nor the heuristic procedure gH dominates the other. However, the closest to optimal procedure g0 dominates both gML and gH , especially for small sample sizes. We have not observed any significant difference between the compared procedures for the learning sample sizes larger than 10.\nThe case of an empty learning sample (n = 0) is of a particular interest. In this case the maximum-likelihood learning problem cannot even be formulated\nbecause any model estimate is meaningless when empirical data are absent. The minimax requirement for recognition strategy may be formally stated but it result in a strategy that makes a wrong decision with probability 0.5 for each true model. As for the closest to optimal requirement, it can be formulated for this case as well and results in a quite reasonable strategy. Figure 6 shows how the risk RG(g 0, θ) depends on θ when the learning sample is absent at all."
    }, {
      "heading" : "7 Results and corollaries",
      "text" : "A common feature of known methods of recognition with learning is that a single model is selected from a given model set. This selected model ‘best’ matches the learning sample, usually it is a consistent estimate of the model. Nevertheless, only a single model is selected from the set of all possible models. Then the\nrecognition strategy is constructed as if the selected model was the true one. Due to consistency of the model estimate, such methods are acceptable when the learning sample is large enough. However, the methods give no guarantees for subsequent recognition when the learning sample has limited size, especially when the size is obviously insufficient to determine the model unambiguously. The drawback is well-known as the small sample problem and forms an obvious gap in today’s knowledge of recognition with uncertain statistical model. Strictly speaking, clarity is achieved only in two extreme cases. If no learning sample is available, then it is appropriate to use the minimax approach. If the sample is large enough, then it is possible to use known methods of recognition with learning. If the situation falls somewhere inbetween, the application developer faces the question whether he should consider the sample large enough and use known methods of learning, or consider the sample too short and ignore it.\nThe concept of closest to optimal strategy allows to consider the recognition with learning from a point of view that differs from the conventional one. It answers some questions that were unanswered up to now. However, new questions arise that are not visible from the conventional viewpoint. Moreover, it turns out that the third and many other points of view on the problem are possible.\n1. Both recognition without learning and recognition with learning can be formalized in a unified framework as developing a closest to optimal recognition strategy. The formalization covers the whole range of learning sample sizes, including the zero size.\n2. The concept of closest to optimal strategy enables to formalize the main idea of empirical Bayesian approach of H.Robbins, namely that simultaneous recognition of multiple objects can be performed much better than recognition of each object independently. The problem of compound object recognition can be formulated as finding a closest to optimal strategy.\n3. The uniform formulation of the recognition problem with respect to the sample size allows to achieve a satisfactory clarity in the small sample problem. Each learning sample, however small, is more useful than its absence. It is not the size of the learning sample that determines its sufficiency for subsequent recognition, it is not even the learning sample itself. Any sample, however small, can be sufficient for recognizing some objects, as well as even a very large sample can be insufficient for recognizing some other objects. It is the pair ”sample– signal” that is sufficient or insufficient for recognition. It is not necessary to amend the developed approach in order to distinguish between sufficient and insufficient pairs ”sample–signal”. The discrimination can be achieved with tools that are already present in the approach. The decision about current object state has to take values from the set Y ∪ {♯} where ♯ is the decision that the given pair ”learning sample–signal” is insufficient to recognize the hidden state with a small enough risk. Thus, if the small sample problem is correctly formulated, it disappears as an independent problem. It is subsumed by the correct formulation of the recognition problem and inside this more general problem it is solved more subtly, more exactly.\n4. Similarly, recognition with learning becomes a special case of recognition without learning. In this particular case, the data available for recognition\nconsist of two parts. The first part depends both on the hidden object state and on the unknown model. The second part directly depends on the model and does not depend on the state when the model is fixed.\n5. To find the closest to optimal strategy for a realistic application, it is necessary to overcome fundamental computational difficulties. This is nothing new in the pattern recognition practice. Maximum likelihood estimation or empirical risk minimization are also far from trivial even in very simple realistic situations. In practice, they need to be implemented with certain acceptable simplifications. Similar difficulties can be expected for the developed approach.\n6. There is only one universal statement in the presented approach. It is the dichotomy between Bayesian and improper strategies. In contrast, closest to optimal strategy can be defined in many other reasonable ways. According to our definition, a strategy q∗X is closest to optimal if it satisfies the conditions\nRX(q ∗ X , θ)− min\nq′ X ∈QX\nRX(q ′ X , θ) ≤ c, θ ∈ Θ,\nwith the minimal value of c. If the risks RX(qX , θ) are not negative, the conditions\nRX(q ∗ X , θ) ≤ c · min\nq′ X ∈QX\nRX(q ′ X , θ), θ ∈ Θ,\nare no less reasonable. Moreover, it is not obligatory to use the optimal risks min q∈QX RX(q, θ) in these conditions. Any other values R ∗(θ) can be used that express the desired parameters of the recognition system under development. However, independently of the possibly modified requirements, the developed strategy has to be Bayesian and its search is reduced to looking for an appropriated weight function over the set of possible models.\nThe article answers the questions that were formulated in [19, page 272] several years ago. The questions gave an opportunity to look at the problem from an unconventional point of view. Now, new questions arise and one can see that in spite of the long history of recognition with learning, a much longer way is ahead.\nWe are grateful to T.Werner, J.Matas and B.Flach for fruitful discussions, critical remarks and valuable advice."
    } ],
    "references" : [ {
      "title" : "Minimax regret classifier for imprecise class distributions",
      "author" : [ "Roćıo Alaiz-Rodŕıguez", "Alicia Guerrero-Curieses", "Jesús Cid-Sueiro" ],
      "venue" : "J. Mach. Learn. Res.,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2007
    }, {
      "title" : "Convex Analysis and Nonlinear Optimization",
      "author" : [ "J.M. Borwein", "A.S. Lewis" ],
      "venue" : null,
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2000
    }, {
      "title" : "Convex Optimization",
      "author" : [ "S. Boyd", "L. Vandenberghe" ],
      "venue" : null,
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2004
    }, {
      "title" : "An introduction to empirical bayes data analysis",
      "author" : [ "George Casella" ],
      "venue" : "The American Statistician,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2009
    }, {
      "title" : "Elementary Decision Theory. Wiley series in probability and mathematical statistics: Applied probability and statistics",
      "author" : [ "H. Chernoff", "H.C.L.E. Moses" ],
      "venue" : null,
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 1959
    }, {
      "title" : "Maximum likelihood from incomplete data via the em algorithm",
      "author" : [ "A.P. Dempster", "N.M. Laird", "D.B. Rubin" ],
      "venue" : "Journal of the Royal Statistical Society. Series B (Methodological),",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 1977
    }, {
      "title" : "Multicriteria Optimization. Lecture notes in economics and mathematical systems",
      "author" : [ "M. Ehrgott" ],
      "venue" : null,
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2005
    }, {
      "title" : "Proper Efficiency and the Theory of Vector Maximization",
      "author" : [ "Arthur M. Geoffrion" ],
      "venue" : "Journal of Mathematics, Analysis and Applications,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 1968
    }, {
      "title" : "Fundamentals of Convex Analysis",
      "author" : [ "J.-B. Hiriart-Urruty", "C. Lemarechal" ],
      "venue" : null,
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2002
    }, {
      "title" : "The Advanced Theory of Statistics: In Three Volumes",
      "author" : [ "M.G. Kendall", "A. Stuart" ],
      "venue" : "Number 2 in The Advanced Theory of Statistics. Griffin,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 1979
    }, {
      "title" : "On combining classifiers",
      "author" : [ "Josef Kittler", "Mohamad Hatef", "Robert P.W. Duin", "Jiri Matas" ],
      "venue" : "IEEE Transaction on Pattern Analysis and Machine Intelligence,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 1998
    }, {
      "title" : "Testing Statistical Hypotheses",
      "author" : [ "E.L.A. Lehmann" ],
      "venue" : null,
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 1986
    }, {
      "title" : "Optimal Statistical Decisions",
      "author" : [ "H.DeGroot Morris" ],
      "venue" : null,
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 1970
    }, {
      "title" : "Two Breakthroughs in the Theory of Statistical Decision Making",
      "author" : [ "J. Neyman" ],
      "venue" : "Revue de l’Institut International de Statistique / Review of the International Statistical Institute,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 1962
    }, {
      "title" : "Asymptotically Subminimax Solutions of Compound Statistical Decision Problems",
      "author" : [ "Herbert Robbins" ],
      "venue" : "In Jerzy Neyman, editor, Proceedings of the Second Berkeley Symposium on Mathematical Statistics and Probability,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 1951
    }, {
      "title" : "An empirical bayes approach to statistics",
      "author" : [ "Herbert Robbins" ],
      "venue" : "Proc. Third Berkeley Symp. on Math. Statist. and Prob.,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 1956
    }, {
      "title" : "On pattern recognition learning problem formulation. (in russian)",
      "author" : [ "M.I. Schlesinger", "A.V. Bondarenko" ],
      "venue" : "Control Systems and Computers,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2009
    }, {
      "title" : "Ten Lectures on Statistical and Structural Pattern Recognition",
      "author" : [ "M.I. Schlesinger", "V. Hlavac" ],
      "venue" : "Computational Imaging and Vision Series. Kluwer Academic Pub,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2002
    }, {
      "title" : "The interaction of learning and self-organization in pattern",
      "author" : [ "M.I. Shlezinger" ],
      "venue" : "recognition. Kibernetika,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 1968
    }, {
      "title" : "Nondifferentiable Optimization and Polynomial Problems",
      "author" : [ "N.Z. Shor" ],
      "venue" : "Nonconvex Optimization and Its Applications. Springer,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 1998
    }, {
      "title" : "Multiple criteria decision making. McGraw-Hill series in quantitative methods for management",
      "author" : [ "M. Zeleny" ],
      "venue" : null,
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 1982
    } ],
    "referenceMentions" : [ {
      "referenceID" : 4,
      "context" : "1 Introduction We consider three wide directions in theoretical and applied statistics: complex hypothesis testing [5, 11, 13, 14], the empirical Bayesian approach [4, 15, 16, 17] and learning in pattern recognition [7, 22].",
      "startOffset" : 115,
      "endOffset" : 130
    }, {
      "referenceID" : 9,
      "context" : "1 Introduction We consider three wide directions in theoretical and applied statistics: complex hypothesis testing [5, 11, 13, 14], the empirical Bayesian approach [4, 15, 16, 17] and learning in pattern recognition [7, 22].",
      "startOffset" : 115,
      "endOffset" : 130
    }, {
      "referenceID" : 11,
      "context" : "1 Introduction We consider three wide directions in theoretical and applied statistics: complex hypothesis testing [5, 11, 13, 14], the empirical Bayesian approach [4, 15, 16, 17] and learning in pattern recognition [7, 22].",
      "startOffset" : 115,
      "endOffset" : 130
    }, {
      "referenceID" : 12,
      "context" : "1 Introduction We consider three wide directions in theoretical and applied statistics: complex hypothesis testing [5, 11, 13, 14], the empirical Bayesian approach [4, 15, 16, 17] and learning in pattern recognition [7, 22].",
      "startOffset" : 115,
      "endOffset" : 130
    }, {
      "referenceID" : 3,
      "context" : "1 Introduction We consider three wide directions in theoretical and applied statistics: complex hypothesis testing [5, 11, 13, 14], the empirical Bayesian approach [4, 15, 16, 17] and learning in pattern recognition [7, 22].",
      "startOffset" : 164,
      "endOffset" : 179
    }, {
      "referenceID" : 13,
      "context" : "1 Introduction We consider three wide directions in theoretical and applied statistics: complex hypothesis testing [5, 11, 13, 14], the empirical Bayesian approach [4, 15, 16, 17] and learning in pattern recognition [7, 22].",
      "startOffset" : 164,
      "endOffset" : 179
    }, {
      "referenceID" : 14,
      "context" : "1 Introduction We consider three wide directions in theoretical and applied statistics: complex hypothesis testing [5, 11, 13, 14], the empirical Bayesian approach [4, 15, 16, 17] and learning in pattern recognition [7, 22].",
      "startOffset" : 164,
      "endOffset" : 179
    }, {
      "referenceID" : 15,
      "context" : "1 Introduction We consider three wide directions in theoretical and applied statistics: complex hypothesis testing [5, 11, 13, 14], the empirical Bayesian approach [4, 15, 16, 17] and learning in pattern recognition [7, 22].",
      "startOffset" : 164,
      "endOffset" : 179
    }, {
      "referenceID" : 5,
      "context" : "It seems as if the well-known ideas of self-learning or unsupervised learning [6, 19, 20] appeared independently of the empirical Bayesian approach.",
      "startOffset" : 78,
      "endOffset" : 89
    }, {
      "referenceID" : 17,
      "context" : "It seems as if the well-known ideas of self-learning or unsupervised learning [6, 19, 20] appeared independently of the empirical Bayesian approach.",
      "startOffset" : 78,
      "endOffset" : 89
    }, {
      "referenceID" : 18,
      "context" : "It seems as if the well-known ideas of self-learning or unsupervised learning [6, 19, 20] appeared independently of the empirical Bayesian approach.",
      "startOffset" : 78,
      "endOffset" : 89
    }, {
      "referenceID" : 6,
      "context" : "Using the concepts of multi-criteria optimization [8, 9], we analyze the empirical Bayesian approach, learning in pattern recognition and complex hypothesis testing in a unified framework.",
      "startOffset" : 50,
      "endOffset" : 56
    }, {
      "referenceID" : 7,
      "context" : "Using the concepts of multi-criteria optimization [8, 9], we analyze the empirical Bayesian approach, learning in pattern recognition and complex hypothesis testing in a unified framework.",
      "startOffset" : 50,
      "endOffset" : 56
    }, {
      "referenceID" : 14,
      "context" : "Robbins [16], which addressed recognition of compound objects and initiated the empirical Bayesian approach [15], are revisited.",
      "startOffset" : 8,
      "endOffset" : 12
    }, {
      "referenceID" : 13,
      "context" : "Robbins [16], which addressed recognition of compound objects and initiated the empirical Bayesian approach [15], are revisited.",
      "startOffset" : 108,
      "endOffset" : 112
    }, {
      "referenceID" : 4,
      "context" : "2 Complex object recognition The main concepts of complex hypothesis discrimination have been formulated in the general statistical decision theory [5, 11, 13, 14].",
      "startOffset" : 148,
      "endOffset" : 163
    }, {
      "referenceID" : 9,
      "context" : "2 Complex object recognition The main concepts of complex hypothesis discrimination have been formulated in the general statistical decision theory [5, 11, 13, 14].",
      "startOffset" : 148,
      "endOffset" : 163
    }, {
      "referenceID" : 11,
      "context" : "2 Complex object recognition The main concepts of complex hypothesis discrimination have been formulated in the general statistical decision theory [5, 11, 13, 14].",
      "startOffset" : 148,
      "endOffset" : 163
    }, {
      "referenceID" : 12,
      "context" : "2 Complex object recognition The main concepts of complex hypothesis discrimination have been formulated in the general statistical decision theory [5, 11, 13, 14].",
      "startOffset" : 148,
      "endOffset" : 163
    }, {
      "referenceID" : 6,
      "context" : "We follow its main ideas (see, for example, the book [8]) and translate them into the context of our problem.",
      "startOffset" : 53,
      "endOffset" : 56
    }, {
      "referenceID" : 1,
      "context" : "Due to the well-known duality theorem [2, 3, 10], for such functions F we have max τ∈T min qX∈QX F (τ, qX) = F (τ , q X) = min qX∈QX max τ∈T F (τ, qX).",
      "startOffset" : 38,
      "endOffset" : 48
    }, {
      "referenceID" : 2,
      "context" : "Due to the well-known duality theorem [2, 3, 10], for such functions F we have max τ∈T min qX∈QX F (τ, qX) = F (τ , q X) = min qX∈QX max τ∈T F (τ, qX).",
      "startOffset" : 38,
      "endOffset" : 48
    }, {
      "referenceID" : 8,
      "context" : "Due to the well-known duality theorem [2, 3, 10], for such functions F we have max τ∈T min qX∈QX F (τ, qX) = F (τ , q X) = min qX∈QX max τ∈T F (τ, qX).",
      "startOffset" : 38,
      "endOffset" : 48
    }, {
      "referenceID" : 11,
      "context" : "Perhaps, the most popular requirement in the theory and practice of complex hypothesis discrimination is the following so-called minimax requirement [13].",
      "startOffset" : 149,
      "endOffset" : 153
    }, {
      "referenceID" : 20,
      "context" : "In multi-criteria decision making [23] another reasonable requirement is used that differs from the minimax one.",
      "startOffset" : 34,
      "endOffset" : 38
    }, {
      "referenceID" : 20,
      "context" : "Let us follow the way shown in [23] and define a number min qX∈QX RX(qX , θ) for each θ ∈ Θ and call it the risk of optimal strategy.",
      "startOffset" : 31,
      "endOffset" : 35
    }, {
      "referenceID" : 20,
      "context" : "The concept of the closest to optimal strategy is a straightforward application of the recommendations in [23] to our special case.",
      "startOffset" : 106,
      "endOffset" : 110
    }, {
      "referenceID" : 0,
      "context" : "On the other hand, the closest to optimal strategy is a generalization of the minimax deviation (regret) strategy from [1].",
      "startOffset" : 119,
      "endOffset" : 122
    }, {
      "referenceID" : 14,
      "context" : "Robbins [16] decades ago and initiated an empirical Bayesian approach [15].",
      "startOffset" : 8,
      "endOffset" : 12
    }, {
      "referenceID" : 13,
      "context" : "Robbins [16] decades ago and initiated an empirical Bayesian approach [15].",
      "startOffset" : 70,
      "endOffset" : 74
    }, {
      "referenceID" : 14,
      "context" : "Robbins explains his idea on the following simple example [16], which we will use several times through the article.",
      "startOffset" : 58,
      "endOffset" : 62
    }, {
      "referenceID" : 13,
      "context" : "The strategy (10) illustrates the main idea of the empirical Bayesian approach [15, 16, 17], namely that an object sample can be recognized much better than by recognizing each object independently.",
      "startOffset" : 79,
      "endOffset" : 91
    }, {
      "referenceID" : 14,
      "context" : "The strategy (10) illustrates the main idea of the empirical Bayesian approach [15, 16, 17], namely that an object sample can be recognized much better than by recognizing each object independently.",
      "startOffset" : 79,
      "endOffset" : 91
    }, {
      "referenceID" : 15,
      "context" : "The strategy (10) illustrates the main idea of the empirical Bayesian approach [15, 16, 17], namely that an object sample can be recognized much better than by recognizing each object independently.",
      "startOffset" : 79,
      "endOffset" : 91
    }, {
      "referenceID" : 18,
      "context" : "To solve (13), the algorithms [20] are used, which later became widely known as the EM-algorithms [6].",
      "startOffset" : 30,
      "endOffset" : 34
    }, {
      "referenceID" : 5,
      "context" : "To solve (13), the algorithms [20] are used, which later became widely known as the EM-algorithms [6].",
      "startOffset" : 98,
      "endOffset" : 101
    }, {
      "referenceID" : 10,
      "context" : "Moreover, the learning information can come from several independent sources simultaneously as it is mentioned in [12].",
      "startOffset" : 114,
      "endOffset" : 118
    }, {
      "referenceID" : 14,
      "context" : "Robbins when he wanted to derive the empirical Bayesian approach from the ideas of minimax decisions [16].",
      "startOffset" : 101,
      "endOffset" : 105
    }, {
      "referenceID" : 1,
      "context" : "Due to the well-known duality theorem [2, 3, 10] this function satisfies the equality",
      "startOffset" : 38,
      "endOffset" : 48
    }, {
      "referenceID" : 2,
      "context" : "Due to the well-known duality theorem [2, 3, 10] this function satisfies the equality",
      "startOffset" : 38,
      "endOffset" : 48
    }, {
      "referenceID" : 8,
      "context" : "Due to the well-known duality theorem [2, 3, 10] this function satisfies the equality",
      "startOffset" : 38,
      "endOffset" : 48
    }, {
      "referenceID" : 19,
      "context" : "Due to concavity of the function Φ, it can be maximized using well-known methods of convex optimization, such as supergradient hill-climbing [21].",
      "startOffset" : 141,
      "endOffset" : 145
    }, {
      "referenceID" : 16,
      "context" : "[18] Let μ1 = 1, μ2 = (−1), and only the a priori probabilities py, y ∈ {1, 2}, be unknown.",
      "startOffset" : 0,
      "endOffset" : 4
    } ],
    "year" : 2015,
    "abstractText" : "We formulate problems of statistical recognition and learning in a common framework of complex hypothesis testing. Based on arguments from multi-criteria optimization, we identify strategies that are improper for solving these problems and derive a common form of the remaining strategies. We show that some widely used approaches to recognition and learning are improper in this sense. We then propose a generalized formulation of the recognition and learning problem which embraces the whole range of sizes of the learning sample, including the zero size. Learning becomes a special case of recognition without learning. We define the concept of closest to optimal strategy, being a solution to the formulated problem, and describe a technique for finding such a strategy. On several illustrative cases, the strategy is shown to be superior to the widely used learning methods based on maximal likelihood estimation.",
    "creator" : "LaTeX with hyperref package"
  }
}