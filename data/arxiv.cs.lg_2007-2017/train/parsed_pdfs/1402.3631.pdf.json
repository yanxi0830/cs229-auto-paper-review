{
  "name" : "1402.3631.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Privately Solving Linear Programs",
    "authors" : [ "Justin Hsu", "Aaron Roth", "Tim Roughgarden", "Jonathan Ullman" ],
    "emails" : [ "aaroth@cis.upenn.edu.", "jullman@seas.harvard.edu." ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n40 2.\n36 31\nv2 [\ncs .D\nS] 8\nM ay\n2 01\n∗Department of Computer and Information Science, University of Pennsylvania. Supported in part by NSF Grant CNS-1065060.\n†Department of Computer and Information Science, University of Pennsylvania. Supported in part by an NSF CAREER award, NSF Grants CCF-1101389 and CNS-1065060, and a Google Focused Research Award. Email: aaroth@cis.upenn.edu.\n‡Department of Computer Science, Stanford University. Supported in part by NSF Awards CCF-1016885 and CCF-1215965, and an ONR PECASE Award.\n§School of Engineering and Applied Sciences and Center for Research on Computation and Society, Harvard University. Supported by NSF grant CNS-1237235. Email: jullman@seas.harvard.edu.\nContents"
    }, {
      "heading" : "1 Introduction 2",
      "text" : "1.1 Our Results and Techniques . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 1.2 Related Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5"
    }, {
      "heading" : "2 Differential Privacy Preliminaries 6",
      "text" : ""
    }, {
      "heading" : "3 Constraint Private LPs 8",
      "text" : "3.1 Solving LPs with Dense Multiplicative Weights . . . . . . . . . . . . . . . . . . . . . 9 3.2 Achieving Constraint Privacy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11 3.3 Private Fractional Set Cover . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13"
    }, {
      "heading" : "4 Low-Sensitivity LPs 15",
      "text" : "4.1 Solving LPs with Multiplicative Weights . . . . . . . . . . . . . . . . . . . . . . . . . 15 4.2 Scalar-Private LPs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16 4.3 Row/Matrix-Private LPs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 4.4 Column Private LPs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21 4.5 Objective Private LPs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23"
    }, {
      "heading" : "5 Lower Bounds 24",
      "text" : "5.1 High-Sensitivity Scalars . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25 5.2 High-Sensitivity Objective . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26 5.3 High-Sensitivity Constraints/Columns . . . . . . . . . . . . . . . . . . . . . . . . . . 26"
    }, {
      "heading" : "6 Discussion 27",
      "text" : ""
    }, {
      "heading" : "1 Introduction",
      "text" : "Linear programming is one of the most fundamental and powerful tools in algorithmic design. It is used ubiquitously throughout computer science: applications include maximum matching, maximum and minimum cost flow, and fractional packing and covering problems. Linear programming relaxations of NP-complete problems also underlie countless efficient approximation algorithms.\nAt the same time, differential privacy is a field where efficient algorithms have been difficult to find. For many problems in differential privacy, the initial focus was on understanding the information-theoretic complexity—the extent to which solving the problem, efficiently or not, is compatible with differential privacy. As a result, there are many central problems that are known to be privately solvable, but for which computationally efficient algorithms are not known. For example, Kasiviswanathan et al. [19] show how to privately PAC learn any PAC learnable concept class (without privacy) with only a small increase in the sample complexity, but via an exponential time algorithm. It remains open whether a computationally efficient algorithm can do this in general. Similarly, Blum et al. [4] show how to privately release a summary of a private database that approximately preserves the answers to rich families of linear queries, again via an exponential time algorithm. In fact, under standard cryptographic assumptions, it is not possible to efficiently and privately answer large collections of general linear queries [10, 28, 27].\nThe two preceding examples are among the many algorithms that use the extremely general exponential mechanism of McSherry and Talwar [23] to achieve near optimal error. However, the exponential mechanism is not efficient in general: it requires running time linear in the size of its output range, which can be extremely large. In contrast, general tools for designing efficient differentially private algorithms are harder to come by (although not non-existent, e.g., the sample and aggregate framework [24] and output/objective perturbation for unconstrained convex optimization [6, 21]).\nOur work contributes to the toolbox of general algorithmic techniques for designing computationally efficient and differentially private algorithms; specifically, we give tools to privately and efficiently solve linear programs (LPs) of various types. An initial problem is to simply define what it means to solve a linear program privately. Differential privacy is defined in terms of neighboring databases. A database is a collection of records from some domain and two databases are neighboring if they differ in a single record. Differential privacy requires the output distribution of an algorithm to be nearly identical when run on either of a pair of neighboring databases. If linear programs can depend on private databases, we naturally have a notion of neighboring linear programs, and we want an algorithm for solving these linear programs that is differentially private with respect to this notion of neighboring inputs.\nThe way in which the linear program is derived from the database gives rise to several distinct notions of neighboring linear programs. For instance, consider an LP with objective c⊤x and constraints Ax ≤ b, where moving to a neighboring LP neighboring database leaves c and A unchanged but perturbs b by only a small amount in each coordinate. Solving this kind of linear programming privately is similar to the well-studied linear query release problem in differential privacy, and techniques for linear query release—such as the private multiplicative weights algorithm of Hardt and Rothblum [15] (and its offline variants [14, 16])—can be adapted with minor changes. (This result may even be considered folklore.) On the other hand, the situation is qualitatively different if moving to a neighboring LP can change either the constraint matrix A or the objective vector c. Some of these private LPs can still be solved; others are provably impossible to solve to nontrivial accuracy under differential privacy.\nIn this paper, we develop a taxonomy of private LPs. For each class, we either present an efficient and accurate differentially private solver, or prove that general LPs of this type cannot be accurately solved while preserving privacy."
    }, {
      "heading" : "1.1 Our Results and Techniques",
      "text" : "We consider linear programs LP (D) defined by a database D, with form\nmax x∈Rd+\nc⊤x\ns.t. Ax ≤ b.\nHere, the vector x represents the variables of the linear program, and c = c(D), A = A(D), and b = b(D) may each depend on the private database D. Our goal is to find an approximate solution to LP (D) in a sense to be defined, while ensuring differential privacy for the underlying database D.\nWe classify private LPs along two dimensions: which part of the LP depends on the database and how sensitive the LP is to changes in the database. Along the second axis, we will consider: 1)\nlow-sensitivity LPs, where changing one record of the database induces a small difference between coefficients that vanishes as the size of the database n grows and 2) high-sensitivity LPs, where changing one record of the database can induce a potentially large change in some coefficient. Lowsensitivity LPs are natural when the coefficients of the LP represent some kind of average over the database, whereas high-sensitivity LPs are natural when the coefficients represent specific records of the database.\nFurthermore, we consider four parts of the LP that might depend on the database: 1) the rows of A, 2) the scalars b, 3) the columns of A, and 4) the objective c. These four parts of the LP, combined with the two notions of sensitivity, lead to the following eight notions of private linear programming:\n1. The constraints: For these linear programs, moving to a neighboring database can affect at most one row of A and the corresponding entry of b, which corresponds to changing one constraint of the LP.\n(a) High-sensitivity: For high-sensitivity constraint private LPs (Section 3), moving to a neighboring database can change a single constraint arbitrarily. That is, for every pair of neighboring databases D,D′, there exists a row i such that for every row j 6= i, A(D)j = A(D ′)j and b(D)j = b(D ′)j . This kind of linear program arises, for example,\nin covering LPs in which each record of the database represents an individual that needs to be covered. We cannot hope to approximately satisfy every constraint while ensuring privacy,1 but we show that by using a variant of multiplicative weights that operates only over a restricted set of distributions, we can still find solutions to such LPs that approximately satisfy most of the constraints. As an example of our technique, we solve a private version of the fractional set cover problem.\n(b) Low-sensitivity: For low-sensitivity constraint private LPs (Section 4.3), moving to a neighboring database can change a single row of A by a small amount in each entry— for some row i, ‖Ai(D) − Ai(D′)‖∞ ≤ 1/n. We show how to solve these LPs using multiplicative weights; our techniques work equally well if the entire constraint matrix can change on neighboring problems (we will sometimes call these low-sensitivity matrix or row private LPs).\n2. The scalars: For these linear programs, c and A are fixed and moving to a neighboring database only affects b = b(D).\n(a) High-sensitivity: For high-sensitivity scalar private LPs, for every neighboring D,D′, there is a row i such that for every j 6= i, b(D)j = b(D′)j . We show that in general, such LPs cannot be solved privately.\n(b) Low-sensitivity: For low-sensitivity scalar private LPs, moving to a neighboring database can change every entry in b slightly, such that ‖b(D)− b(D′)‖∞ ≤ 1/n. These LPs capture the private linear query release problem, so we will sometimes refer to them as query release LPs. In this problem, the database is viewed as a histogram D ∈ Nd+ and\n1For example, given a solution x to LP (D), we can always derive a new constraint (Ai, bi) that is far from being satisfied by x. If we introduce this new constraint in a neighboring linear program LP (D′), by the differential privacy condition, this new constraint must also be far from being satisfied in LP (D′) with high probability.\nthe objective is to find a synthetic database x ∈ Rd+ such that for every linear query q in some family, 〈q, x〉 ≈ 〈q,D〉. We show how to adapt existing techniques for this problem and derive resulting accurate solvers for LPs of this form (Section 4.2).\n3. One column in A: For these linear programs, moving to a neighboring database can affect at most one column of A.\nThese LPs arise literally as the dual linear programs of row private LPs. For example, in a LP where variables represent different tasks, the private coefficients corresponding to a single variable may represent the amount of resources needed for that task. Then, a packing LP seeks to maximize some objective subject to resource constraints.\n(a) High-sensitivity: For high-sensitivity column private LPs, for every neighboringD,D′, the matrices A(D), A(D′) are arbitrarily different in a single column, and identical in all other columns. We show that in general, such LPs cannot be solved privately (Section 5.3).\n(b) Low-sensitivity: For low-sensitivity column private LPs, moving to a neighboring database can change every entry in a single column of A by a small amount. More generally, if Ai is the ith row of A, then ‖A(D)i −A(D′)i‖1 ≤ 1/n for each i. We show how to use these LPs using multiplicative weights (Section 4.4).\n4. The objective: For these linear programs, moving to a neighboring database can affect the objective c. The scalars b and constraints A remain unchanged.\n(a) High-sensitivity: For high-sensitivity objective private LPs, for every neighboring D,D′, a single entry of the objective c(D), c(D′) can change arbitrarily. We show that in general, such LPs cannot be solved privately (Section 5.2).\n(b) Low-sensitivity: For low-sensitivity objective private LPs, for every neighboring D,D′, the objective vectors c(D), c(D′) satisfy ‖c(D) − c(D′)‖1 ≤ 1/n. This kind of linear program can be solved inefficiently to high accuracy by selecting from the set of vertices of the feasible polytope with the exponential mechanism; we show that linear programs in this class can also be solved efficiently and accurately, by directly using randomized response (Section 4.5).\nThis taxonomy is summarized in Table 1. We will formally define accuracy, but roughly speaking, an accurate solution satisfies each constraint to within additive α, and has objective within additive α of optimal (when there is an objective). The exception is constraint privacy (indicated by the asterisk), where our algorithm finds a solution that satisfies only most of the constraints to within additive α, and may violate the other constraints arbitrarily."
    }, {
      "heading" : "1.2 Related Work",
      "text" : "Differential privacy emerged from a line of work initiated by Dinur and Nissim [7], was defined by Dwork et al. [9], and is now a standard definition of privacy in computer science. Below, we discuss relevant results in differential privacy; the survey by Dwork [8] is an excellent source for a more comprehensive overview.\nPrivate optimization has been studied since the work of Blum et al. [3] and Kasiviswanathan et al. [19], who considered how to choose an optimal classifier privately. Blum et al. [3] give an efficient reduction from SQ learning to private SQ learning, and Kasiviswanathan et al. [19] give a very general but inefficient reduction from PAC learning to private PAC learning using the exponential mechanism of McSherry and Talwar [23]. Private learning was placed explicitly into an optimization framework by Chaudhuri et al. [6], who give two techniques for privately solving certain unconstrained convex optimization problems. Gupta et al. [12] give several algorithms for problems in private combinatorial optimization, but these were specialized combinatorial algorithms for specific problems.\nIn parallel, a line of work initiated by Blum et al. [4] and continuing with Dwork et al. [10], Roth and Roughgarden [26], Dwork et al. [11], Hardt and Rothblum [15], Gupta et al. [14], Hardt et al. [16] study the problem of privately producing synthetic data consistent with some private database on many linear queries. (Of particular note is the private multiplicative weights mechanism of Hardt and Rothblum [15], which achieves the optimal accuracy and running time bounds [27, 5].) This problem can be represented as a linear program with queries defining constraints, and indeed, the private multiplicative weights algorithm of Hardt and Rothblum [15] can be directly applied to solve this kind of linear program. This observation motivates our current investigation.\nOur algorithms are mostly based on different variants of the multiplicative weights method of solving linear programs, which was introduced by Plotkin et al. [25] (see the excellent survey by Arora et al. [2] for more details). Whereas Plotkin et al. [25] maintain a distribution over the dual variables with multiplicative weights, depending on the kind of linear program we are solving, we either maintain a distribution over the dual variables or the primal variables. To solve constraint private LPs, we use a combination of the multiplicative weights update method and Bregman projections [2]—Hsu et al. [18] use a similar version of this technique in designing analyst private mechanisms."
    }, {
      "heading" : "2 Differential Privacy Preliminaries",
      "text" : "Differential privacy is a strong notion of privacy, first introduced by Dwork et al. [9]. In the typical setting, we consider a database as a multisets of records, each belonging to a single individual. Then, a randomized function from databases to an output range satisfies differential privacy if, for any change in a single record of the input database, the distribution on outputs remains roughly the same. More formally, we have the following definition.\nDefinition 1 (Dwork et al. [9]). Let ǫ > 0 and 0 ≤ δ < 1 be given. A randomized function M : D → R mapping databases to an output range is (ǫ, δ)-differentially private if for every subset\nS ⊆ R and for every pair of database D,D′ that differ in a single record,\nPr[M(D) ∈ S] ≤ eǫ Pr[M(D′) ∈ S] + δ.\nWhen δ = 0, we will say that M is ǫ-differentially private.\nWe will use two basic mechanisms from differential privacy: the Laplace mechanism and the exponential mechanism. The Laplace mechanism privately releases a number by adding noise drawn from the Laplace distribution.\nDefinition 2 (Dwork et al. [9]). Let ǫ > 0 be given. A function f : D → R is ∆-sensitive if for every pair of database D,D′ that differ in a single record,\n|f(D)− f(D′)| ≤ ∆.\nThe Laplace mechanism applied to a ∆-sensitive function releases\nf(D) + ν,\nwhere ν is a draw from the Laplace distribution with parameter ǫ/∆; that is, with probability density function\nF (ν) = ǫ\n2∆ exp\n( −|ν|ǫ\n∆\n) .\nThe Laplace mechanism is ǫ-differentially private and satisfies the following tail bound, which is also an accuracy guarantee for the Laplace mechanism.\nLemma 3. Let β ∈ (0, 1) be given, and let ν be drawn from the Laplace distribution with scale b. Then,\nPr[|ν| ≥ T ] ≤ β for T = 1 b log(1/β).\nWe will also use the exponential mechanism [23], which can privately produce a non-numeric or discrete output. The exponential mechanism is defined in terms of a quality score that maps a database and an element of the range to a real valued score. For a given a database, the exponential mechanism privately outputs an element of the range that approximately maximizes the quality score.\nDefinition 4 (McSherry and Talwar [23]). Let ǫ > 0 be given, and suppose the quality score Q : R×D → R is ∆-sensitive in the database. On database D, the ǫ-private exponential mechanism with quality score Q outputs r ∈ R with probability proportional to\nexp ( ǫ 2∆ ·Q(r,D) ) .\nThe exponential mechanism is ǫ-differentially private, and satisfies the following accuracy guarantee.\nTheorem 5 (McSherry and Talwar [23]). Let β ∈ (0, 1) and the database D be given. Suppose that the maximum value of the quality score Q on database D is OPT. Then, the ǫ-private exponential mechanism with quality score Q on D outputs r ∈ R such that\nPr [ Q(r,D) ≥ OPT−2∆\nǫ log ( |R| β )] ≥ 1− β.\nTo combine these mechanisms, we will use standard composition theorems.\nTheorem 6 (Dwork et al. [11]). For any δ ∈ (0, 1), the composition of k (adaptively chosen) ǫ′-private mechanisms is (ǫ, δ)-differentially private, for\nǫ′ = ǫ√\n8k log(1/δ) ."
    }, {
      "heading" : "3 Constraint Private LPs",
      "text" : "Let us begin by considering constraint private LPs, with the general form\nmax x∈K\nc⊤x\ns.t. Ax ≤ b,\nwhere A ∈ Rm×d, b ∈ Rm, c ∈ Rd, and K ⊆ Rd. We think of K as the easy constraints, those that are independent of the database, like non-negativity.\nLet KOPT = K ∩ {x ∈ Rd | c⊤x = OPT}. Then, the original LP can be solved approximately by repeatedly solving the feasibility problem\nfind x ∈ KOPT s.t. Ax ≤ b,\nbinary searching on the optimal objective value OPT.2 Thus, unless we specify otherwise, we will restrict our attention to feasibility LPs. Furthermore, since a linear program has a convex feasible region, K (and hence KOPT) are convex. From now on, we will write K for KOPT.\nRoughly, a private database D defines a linear program with objective vector c(D), constraint matrix A(D), and vector b(D), which we will call the scalars. In a constraint private LP, the objective c(D) = c(D′) is independent of the data and for every two neighboring datasets D,D′, the matrices A(D) and A(D′) are exactly the same, except one matrix has an additional row that the other does not. The vectors b(D) and b(D′) are also identical, except there is an entry corresponding to the different constraint in A in one b, but not the other. Then, we want a LP solver that satisfies the differential privacy guarantee with respect to this notion of adjacency. Formally:\nDefinition 7. A randomized algorithm M with inputs m ∈ N, vector b ∈ Rm, and matrix A ∈ Rm×d and outputting a vector in Rd is (ǫ, δ)-high sensitivity constraint private if for any A,A′ such that A′ is equal to A with an additional row appended, and b, b′ such that b′ is equal to b with an additional entry, Pr[M(m, b,A) ∈ S] ≤ eǫ Pr[M(m+ 1, b′, A′) ∈ S] + δ for any set S ⊆ Rd.\n2Binary search will incur an additional overhead in privacy, but in some situations may not be necessary: for instance, if a bound on the sensitivity of the optimal objective is known, we can solve the LP non-privately and estimate OPT with the Laplace mechanism."
    }, {
      "heading" : "3.1 Solving LPs with Dense Multiplicative Weights",
      "text" : "A standard approach to solving LPs is via no-regret algorithms. For a brief summary, these algorithms operate over a series of timesteps, selecting a single action at each step. Once the action is selected, the loss for each action is revealed (perhaps adversarially); the no-regret algorithm then adjusts to favor actions with less loss. While LPs can be solved using any no-regret algorithm, for concreteness we use the multiplicative weights update algorithm.\nThroughout, we will use calligraphic letters (A) to denote sets of actions, Roman letters (A) to denote measures on those actions A : A → [0, 1], and letters with tildes (Ã) to denote a probability distributions over actions. We will write |A| to mean the density of measure A, defined to be∑\na∈AAa. We will use a variant of the standard multiplicative weights algorithm that maintains a dense distribution over the set of constraints, i.e., a distribution that doesn’t place too much probability on any action. We will call this algorithm, due to Herbster and Warmuth [17], the dense multiplicative weights algorithm (Algorithm 1). Roughly, the algorithm projects the MW distribution on actions into the set of dense distributions at each step. The loss at each step will be defined by a point that approximately satisfies the average constraint weighted by the MW distribution—by capping the probability on any constraint, we ensure that this point can be selected privately even when a single constraint can change arbitrarily on neighboring instances.\nWe first define this projection step, also known as a Bregman projection.\nDefinition 8. Let s > 0. Given a measure A such that |A| ≤ s, let ΓsA be the (Bregman) projection of A into the set of 1/s-dense distributions, defined by ΓsAa = 1 s ·min{1, cAa} for every\na ∈ A, where c ≥ 0 is such that s =∑a∈Amin{1, cAa}.\nThen, we can define the Dense Multiplicative Weights algorithm, which uses the standard multiplicative weights update rule combined with a Bregman projection into the set of dense distributions after each step.\nAlgorithm 1 The Dense Multiplicative Weights algorithm, DMWs,η Let A1 be the uniform measure on A For t = 1, 2, . . . , T :\nLet B̃t = ΓsA t Receive loss vector ℓt (may depend on B1, . . . , Bt) Update: For each a ∈ A:\nUpdate At+1a = e −ηℓtaAta\nThen, dense multiplicative weights satisfies the following (regret) guarantee.3\nTheorem 9 (Herbster and Warmuth [17]). Let A1 be the uniform measure of density 1 and let{ B̃t } be the sequence of projected distributions obtained by DMWs,η with arbitrary losses { ℓt }\nsatisfying ‖ℓt‖∞ ≤ 1 and η ≤ 1/2. Let B̃∗ be the uniform distribution on some subset S∗ ⊆ A of 3Note that the regret guarantee is only with respect to dense distributions, rather than arbitrary distributions. This is a result of projecting the MW distribution to a dense distribution—the algorithm may not be able to compete with non-dense distributions.\nsize s. Then,\n1\nT\nT∑\ni=1\n〈ℓt, B̃t〉 ≤ 1 T\nT∑\ni=1\n〈ℓt, B̃∗〉+ η + log |A| ηT .\nRecall we can assume that we know the optimal value OPT, so the objective can be represented as the constraint c⊤x = OPT. Hence, let K = {x ∈ Rd+ | c⊤x = OPT} be the public feasible set. We will assume that there is a known, data-independent upper bound ρ such that\nρ ≥ max D max x∈K ‖A(D)x − b(D)‖∞,\nwhich we call the width of the LP. We will define our algorithm in terms of an approximate oracle for solving a linear minization problem. (For a concrete example of such an oracle in the context of fractional set cover, see the next section.)\nDefinition 10. An (α, β)-approximate, ρ-bounded oracle, given a distribution y ∈ Rm and matrix A ∈ Rm×d, with probability at least 1− β finds x∗ ∈ Rd with\nm∑\ni=1\nyi(Ai · x∗) ≤ min x∈K\nm∑\ni=1\nyi(Ai · x) + α\nand ‖Ax∗ − b‖∞ ≤ ρ.\nTo solve linear programs, we use the dense multiplicative weights algorithm to maintain a distribution over the constraints, and pick points xt ∈ K that best satisfy the weighted combination of constraints at each step. Intuitively, the losses will lead to more weight on violated constraints, leading to points that are more feasible. Taking the average of the points xt will yield an approximately feasible point, if it exists. See Algorithm 2 for the full algorithm.\nWe note that similar techniques for solving linear programs using multiplicative weights have been known since at least Plotkin et al. [25]; the novelty in our approach is that we use multiplicative weights paired with a projection onto the set of dense distributions, and show that the solution approximately satisfies most of the constraints. As we will see, the projection step is needed for privacy.\nTheorem 11. Let 0 < α ≤ 9ρ, and let β ∈ (0, 1). Suppose there is a feasible solution of the linear program. Then with probability at least 1 − β, Algorithm 2 with density parameter s run with an (α/3, β/T )-approximate, ρ-bounded oracle finds a point x∗ in K such that there is a set of constraints S of size at most |S| < s, with Aix∗ ≤ bi + α for every i /∈ S.\nProof. By a union bound over T steps, the oracle succeeds on all steps with probability at least 1− β; condition on this event.\nLet Ks = {y ∈ Rm | 1⊤y, ‖y‖∞ ≤ 1/s} be the set of 1/s-dense distributions. Then, y⊤Ax∗ ≤ y⊤b for any y ∈ Ks, so in particular the oracle finds xt with y⊤Axt < y⊤b+ α/3.\nThus, the loss vectors ℓt = (1/2ρ)(b − Axt) + 1/2 satisfy ℓt · yt ≥ 1/2 − α/6ρ, which is at least −1 if α ≤ 9ρ. Since the oracle is ρ-bounded, ℓt · yt ≤ 1. So, Theorem 9 applies; for p any point in\nAlgorithm 2 Solving for LP feasibility with dense multiplicative weights\nInput A ∈ Rm×d, b ∈ Rm. Let ỹ1 be the uniform distribution in Rm, ρ ≥ maxx∈K ‖Ax− b‖∞ be the width of the LP, s ∈ N be the density parameter, and α > 0 be the desired accuracy. Let Oracle be an (α, β)-accurate, ρ-bounded oracle, and set\nη =\n√ logm\nT , T =\n36ρ2 logm\nα2 .\nFor t = 1, . . . , T : Find xt = Oracle(ỹt, A) Compute losses ℓti := (1/2ρ)(bi −Ai · xt) + 1/2. Update ỹt+1 from ỹt and ℓt via dense multiplicative weights with density s. Output x = (1/T ) ∑T\nt=1 x t.\nKs, we have the following bound:\n1 2 − α 6ρ ≤ 1 T\nT∑\nt=1\n( ℓt · p ) + η + logm\nηT\n= 1\nT\nT∑\nt=1\n( 1\n2ρ\n( b−Axt ) + 1\n2\n) · p+ η + logm\nηT\nThus,\n− α 6ρ ≤ 1 T\nT∑\nt=1\n1\n2ρ\n( b−Axt ) · p+ η + logm\nηT .\nDefine x = (1/T ) ∑T\nt=1 x t, and rearrange:\np⊤Ax ≤ p⊤b+ 2ρη + 2ρ logm ηT + α 3 .\nBy our choice of η and T , we get p⊤Ax ≤ p⊤b+ α.\nSince this holds for any p ∈ Ks, x satisfies all but s−1 constraints with error α—if it didn’t, letting p be the uniform distribution on the s violated constraints would give a contradiction."
    }, {
      "heading" : "3.2 Achieving Constraint Privacy",
      "text" : "Now, we will see how to make Algorithm 2 constraint private. First, the output point depends on the private data (the constraints A) only through the minimization step. Thus, if we can make the minimization private (in a certain sense), then each xt (and hence the final point x) will satisfy constraint privacy. Note that if the oracle privately minimizes over K, the final point x will automatically be in K since K is convex. Hence, we can also think of K as the public constraints, the ones that are always satisfied.\nTheorem 12. Let ǫ, δ, T > 0, and let\nǫ′ = ǫ√\n8T log(1/δ) .\nwith density parameter s ∈ N. Suppose the oracle is ǫ′-private, where on neighboring instances the inputs (distributions) ỹ, ỹ′ satisfy\n‖ỹ‖∞ ≤ 1/s, ‖ỹ′‖∞ ≤ 1/s, ‖ỹ − ỹ′‖1 ≤ 2/s,\nand the matrices A,A′ are exactly the same except one has an additional row, and the vectors b, b′ except one has a corresponding additional entry. Then, Algorithm 2 with density s is (ǫ, δ)-high sensitivity constraint private.\nProof. If the oracle is ǫ′-differentially private, then (ǫ, δ)-constraint privacy for the whole algorithm follows directly by composition (Theorem 6).\nTo show that the oracle is private when adding or removing a constraint from the LP, we know that A,A′ are exactly the same except one has an extra row, and we know that ‖ỹ‖∞ ≤ 1/s since we have projected into the set Ks. Hence, it only remains to check that neighboring ỹ, ỹ′ satisfy ‖ỹ − ỹ′‖1 ≤ 2/s for each timestep t. We use a result about the sensitivity of Bregman projections from from Hsu et al. [18]; we reproduce the proof for completeness.\nLemma 13 (Hsu et al. [18]). Let s > 0 be given. Suppose A,A′ be measures on sets A,A ∪ a′ respectively, and identical on A. If Ã, Ã′ are the respective Bregman projections into the set of 1/s-dense distributions, then ‖Ã− Ã′‖1 ≤ 2/s. Here and below, we treat A,A′ as supported on the same set with Aa′ fixed at 0.\nProof. From the definition of the projection (Definition 8), it’s clear that sÃa ≥ sÃ′a for all a 6= a′. We then have the following:\n∑\na∈A∪a′\n|sÃa − sÃ′a| = |sÃa′ − sÃ′a′ |+ ∑\na6=a′\n|sÃa − sÃ′a|\n≤ 1 + ∑\na6=a′\n|sÃa − sÃ′a|\n= 1 + ∑\na6=a′\nsÃa − sÃ′a\n= 1 + s|Ã′| − s(|Ã′| − Ã′a′) ≤ 1 + s− (s− 1) = 2\nDividing through by s, we are done.\nSince y, y′ are identical except for the weight corresponding to the differing constraint, we are done by the lemma.\nNow that we have presented our algorithm for solving LPs under constraint privacy, we give an example of how to instantiate the oracle and apply Theorem 11."
    }, {
      "heading" : "3.3 Private Fractional Set Cover",
      "text" : "We will consider the example of the fractional set cover LP, though our arguments extend to constraint private LPs with a private oracle that has low width. (For example, many covering and packing LPs satisfy this property.)\nSuppose there are d sets, each covering some subset of m people. Each set has a cost cS , and we wish to select the cheapest collection of sets that covers every person. We will consider the fractional relaxation of this problem, where instead of selecting whole sets for the cover, we can decide to select a fraction of each set, i.e., each set can be chosen to some non-negative degree, and the cost for set S is the degree to which it is open times cS . We again want the cheapest fractional collection of sets, such that at least weight 1 covers each person.4\nTo formulate this as a linear program, let the variables be x ∈ Rd+; variable xS will be the degree that we choose set S in the cover. For the constraints, let Ai ∈ {0, 1}m such that AiS is 1 exactly when set S covers i, otherwise 0.\nWe will assume that the optimal value OPT is known, and the goal is to compute an approximate fractional set covering x∗ corresponding to OPT. This is equivalent to solving the following linear program:\nfind: x ∈ K s.t. Ai · x ≥ 1 for each i\nwhere K = {x ∈ Rd+ | c · x = OPT} is the feasible region. We wish to achieve constraint privacy: if each individual corresponds to a covering constraint, then we want an approximate solution that is hides whether a person i needs to be covered or not. This is not always possible—if each set contains just one person, then the presence of a set in any valid covering will reveal information about the people that need to be covered. Thus, we will find a solution violating a few constraints, so only covering most people.\nTo use our constraint private LP solver, we first define a private oracle solving the minimization problem\nO(y) = argmin x∈K\n∑\ni\nyi(Ai · x).\nSince the oracle is minimizing a linear function, the optimal point lies at a vertex of K and is of the form\nx∗ = OPT\nci ei\nfor some i, where ei is the i’th standard basis vector, i.e., all zeros except for a 1 in the i’th coordinate. We can use the exponential mechanism to privately select this vertex.\nLemma 14. Let γ ∈ (0, 1) be given. Suppose ‖y‖∞ ≤ 1/s, and suppose that ‖y − y′‖∞ ≤ 2/s on adjacent inputs. Let O(y) be the ǫ-private exponential mechanism over the vertices of K with quality score\nQ(j, y) = ∑\ni\nyi ( Ai · OPT\ncj ej − 1\n) = OPT\ncj\n∑\ni\nyiaij − 1.\n4To highlight the constraint private LP, we will only consider the fractional version. It is also possible to round the fractional solution to an integral solution (with slightly worse cost), since randomized rounding is independent of the private data.\nThen O is an (α, γ)-approximate, ρ-bounded oracle, with\nρ = OPT cmin − 1 and α = 6OPT log d log(1/γ) cmin · s · ǫ .\nProof. The width of the oracle is clear: when returning a point x = OPTci ei,\nAi · x− 1 ≤ OPT\ncmin − 1.\nFor accuracy, note that the quality score Q has sensitivity at most\n∆ = 3OPT\ncmin · s .\nWhy? On neighboring databases, there are two possible changes: first we may have |yi− y′i| ≤ 2/s, and second we may have an extra term in the sum on one neighbor (since the sum is taken over all constraints, and one neighboring instance has an extra constraint). The first source contributes sensitivity 2OPT /(cmins), and since ‖y‖∞, ‖y′‖∞ ≤ 1/s, the second source contributes sensitivity OPT /(cmins).\nNow, since there are d possible outputs, the accuracy guarantee for the exponential mechanism (Theorem 5) shows that O selects a point with additive error at most\nα = 2∆\nǫ log d log(1/γ) =\n6OPT cminsǫ log d log(1/γ)\nwith probability at least 1− γ. Hence, we are done.\nNow, it follows that Algorithm 2 solves the private fractional set cover problem with the following accuracy guarantee.\nTheorem 15. Let β ∈ (0, 1). With probability at least 1 − β, Algorithm 2 with the exponential mechanism as an oracle (Lemma 14)—where ρ is the width of the oracle and α ≤ 9ρ—finds a point x∗ such that Aix ∗ ≥ 1− α except for at most s constraints i, where\ns = Õ\n( OPT2 log d log1/2 m log(1/β) log1/2(1/δ)\nc2 · α2 · ǫ\n) .\nAlgorithm 2 is also ǫ-high sensitivity constraint private.\nProof. Let ǫ′ be as in Theorem 12, and let γ = β/T with T as in Algorithm 2. Unfolding the definition of ǫ′ and ρ and applying Lemma 14, the oracle gives accuracy\n6OPT cminsǫ′ log d log(1/γ) =\n96 √ 2OPT2 log d log1/2 m log(1/γ) log1/2(1/δ)\nc2minǫsα\nwith probability at least 1 − γ. Set this equal to α/3. By assumption α ≤ 9ρ, so Theorem 11 applies: with probability at least 1− β, there is a set S of at most s constraints such Aix∗ ≥ 1−α for every i /∈ S, where\ns = O\n( OPT2 log d log1/2 m log(1/γ) log1/2(1/δ)\nc2 · α2 · ǫ\n) .\nand γ = β/T .\nRemark 16. A variant of the efficient private set cover problem has been investigated by Gupta et al. [12]. Our techniques are more general, but the solution we provide here has an imcomparable accuracy guarantee. We include this example to demonstrate how to use Algorithm 2 and Theorem 11. On the one hand, we may fail to satisfy some of the coverage constraints, and if we imagine that each uncovered element can be covered at a cost of 1, our approximation guarantee now depends on OPT unlike the guarantee of Gupta et al. [12].\nOn the other hand, we output an explicit solution whereas the algorithm of Gupta et al. [12] outputs an implicit solution, a “set of instructions” that describes a set cover when paired with the private data. (Their approach can also be interpreted as satisfying the weaker guarantee of joint differential privacy [20] rather than standard differential privacy.) Finally, our techniques apply to general constraint-private linear programs, not just set cover."
    }, {
      "heading" : "4 Low-Sensitivity LPs",
      "text" : "Let us now turn to low-sensitivity LPs. Recall that for these LPs, the distance between adjacent inputs decreases as the size of the database (i.e., the number of individuals) grows. First, a few simplifying assumptions. Like above, we will continue to solve feasibility LPs of the following form:\nfind x ∈ Rd+ s.t. Ax ≤ b\nUnlike the case for general constraint private LPs, we require that the feasible solution is a distribution, i.e., is non-negative and has ℓ1 norm 1. Note that if the optimal solution has ℓ1 norm L, then the rescaled LP\nfind x ∈ Rd+ s.t. Ax ≤ b/L\nhas a distribution as a solution. Our algorithms will find a point x∗ such that Ax∗ ≤ b/L + α · 1, so if we set α = α′/L, then A(Lx∗) ≤ b+α′ gives an approximate solution to the original, unscaled LP."
    }, {
      "heading" : "4.1 Solving LPs with Multiplicative Weights",
      "text" : "Before getting into specific kinds of low-sensitivity LPs, we first review another standard method for solving LPs via the standard multiplicative weights algorithm presented in Algorithm 3.\nAlgorithm 3 The Multiplicative Weights Algorithm, MWη\nLet Ã1 be the uniform distribution on A For t = 1, 2, . . . , T :\nReceive loss vector ℓt (may depend on A1, . . . , At) For each a ∈ A:\nUpdate At+1a = e −ηℓtaÃta for every a ∈ A\nNormalize Ãt+1 = At+1/|At+1|\nUnlike the dense multiplicative weights approach presented earlier (Algorithm 2), we use multiplicative weights to maintain a distribution over the variables rather than the constraints.5 This distribution will be the candidate solution, and we define losses by the maximum constraint violation of this candidate solution at each step. It will be useful to first define an oracle for linear maximizations.\nDefinition 17. For ǫ > 0, γ > 0 an (α, γ)-dual oracle, given A, b, x as input, finds a constraint i ∈ [m] such that\nAix− bi ≥ max j Ajx− bj − α,\nwith probability at least 1− γ.\nWe now give the full algorithm in Algorithm 4.\nAlgorithm 4 Solving for LP feasibility with primal multiplicative weights\nInput A ∈ Rm×d, b ∈ Rm. Let x̃1 be the uniform distribution in Rd, ρ = maxij |Aij | be the width of the LP, α > 0 be the desired accuracy. Let Oracle be a (α, γ)-dual oracle, and set\nη =\n√ log d\nT , T =\n9ρ2 log d\nα2 .\nFor t = 1, . . . , T : Find pt = Oracle(A, b, x̃t) Compute losses ℓti := (1/ρ)Apti Update x̃t+1 from x̃t and ℓt via multiplicative weights. Output x = (1/T ) ∑T\nt=1 x̃ t\nThen, the following accuracy guarantee is known.\nTheorem 18 (Plotkin et al. [25]). Suppose there is a feasible distribution solution of the linear program Ax ≤ b. Then, running Algorithm 4 with an (α/3, γ)-dual oracle finds a point x such that Ax ≤ b+ α · 1 with probability at least 1− Tγ."
    }, {
      "heading" : "4.2 Scalar-Private LPs",
      "text" : "First, we consider linear programs where the objective and constraint coefficients are public data, but the right hand side in the constraints may contain private data. Roughly, a private database D maps to an objective vector c(D), a constraint matrix A(D), and a vector b(D). For every pair of neighboring databases D,D′, we have c(D) = c(D′) and A(D) = A(D′) independent of the data, and ‖b(D)− b(D′)‖∞ ≤ ∆∞. We will think of ∆∞ as decreasing in n; our accuracy guarantees will be trivial if this is not true. As usual, we will assume the LP is in feasibility form, and leave the objective c implicit. Formally:\n5Readers familiar with game theory may notice that we are solving LPs by finding the equilibrium of a two player, zero-sum game. Then, Algorithm 2 is solving the game with MW over the constraints and best response over the variables, while the approach we present in this section swaps the two roles.\nDefinition 19. A randomized algorithm M with inputs vector b ∈ Rm and matrix A ∈ Rm×d, and outputting a vector in Rd is (ǫ, δ)-low sensitivity scalar private with sensitivity ∆∞ if for any b, b ′ such that ‖b− b′‖∞ ≤ ∆∞,\nPr[M(b,A) ∈ S] ≤ eǫ Pr[M(b′, A′) ∈ S] + δ\nfor any set S ⊆ Rd. The algorithm we use is a slight generalization of the offline private multiplicative weights algorithm [13, 16] (building on the influential work of Hardt and Rothblum [15], who introduced the “online” variant). In our framework, we will express the algorithm as a differentially private variant of Algorithm 4 to solve these linear programs while preserving differential privacy.\nThroughout, we assume that the vector b is private data. On neighboring databases, b can change by at most ∆∞ in ℓ∞ norm. Looking at Algorithm 4, we see that the only place we touch the private data is in the dual oracle. Accordingly, if the dual oracle is private in b, then the whole algorithm is private.\nTheorem 20. Let ǫ, δ, T be as in Algorithm 4, and let\nǫ′ = ǫ√\n8T log(1/δ) .\nAlgorithm 4, run with an ǫ′-private dual oracle is (ǫ, δ)-differentially private.\nProof. Direct from composition (Theorem 6).\nJust like in private multiplicative weights for private query release, the exponential mechanism gives an appropriate dual oracle.\nLemma 21. Let ǫ, γ > 0 be given, and suppose the vector b can differ by at most ∆∞ in ℓ∞ norm on neighboring instances. Then, the ǫ-private exponential mechanism with quality score\nQ(i, b) = Aix− bi is an (α, γ)-dual oracle, for\nα = 2∆∞ ǫ\n· log ( m\nγ\n) .\nProof. This is ǫ-private by definition, and the accuracy follows from the accuracy of the exponential mechanism (Theorem 5)—the quality score is ∆∞-sensitive in b, and the output ranges over the constraints, so has size m.\nCombining the MW with the oracle, our private low-sensitivity scalar-private LP solver Algorithm 4 satisfies the following accuracy guarantee.\nTheorem 22. Let α, β ∈ (0, 1) be given. Suppose the linear program Ax ≤ b has a distribution as a feasible solution. Algorithm 4, run with the exponential mechanism as a dual oracle (Lemma 21), is (ǫ, δ)-low sensitiivty scalar private with sensitivity ∆∞, and finds x\n∗ satisfying Ax∗ ≤ b+ α · 1, with probability at least 1− β, where\nα = Õ\n( ρ1/2∆ 1/2 ∞\nǫ1/2 · log1/4 d log1/4(1/δ) log1/2(1/β) log1/2 m\n) .\nProof. Let ǫ′ be as in Theorem 20, and let γ = β/T with T from Algorithm 4. By Lemma 21, the ǫ′-private exponential mechanism with quality score\nQ(i, b) = Aix− bi\nis an (α/3, γ)-dual oracle for\nα = 6∆∞\n√ 8T log(1/δ)\nǫ · log\n( mT\nβ\n) = 18ρ∆∞ √ 8 log d log(1/δ)\nαǫ · log\n( 9ρ2(log d)m\nα2β\n) .\nSolving,\nα = Õ\n( ρ1/2∆ 1/2 ∞\nǫ1/2 · log1/4 d log1/4(1/δ) log1/2(1/β) log1/2 m\n)\nas desired.\nRemark 23. This bound generalizes the guarantee for the private multiplicative weights algorithm when privately generating synthetic data for linear queries [15]. In that setting, there is one variable for each element in some underlying data universe X (and so d = |X |), and there is one equality constraint for each of k linear queries (and so m = k).\nNow, let us consider the low-sensitivity version of constraint privacy: neighboring instances have constraint matrices that differ to a small degree. We distinguish two further subcases: either every coefficient in each constraint can differ, or only a few coefficients in each constraint can differ."
    }, {
      "heading" : "4.3 Row/Matrix-Private LPs",
      "text" : "Suppose we have the feasibility problem\nfind x\ns.t. Ax ≤ b,\nwhere some entries in A may change by at most ∆∞ on a neighboring instance. Roughly, a private database D maps to an objective vector c(D), a constraint matrix A(D), and a vector b(D). For every pair of neighboring databases D,D′, we have c(D) = c(D′) and b(D) = b(D′) independent of the data, and ‖A(D) −A(D′)‖∞ ≤ ∆∞. Again, we will think of ∆∞ as decreasing in n; our accuracy guarantees will be trivial if this is not true. Our techniques work equally well whether only a single row of A or the entire matrix A can differ, so we will assume the latter. We will also assume that the LP is in feasibility form, and leave the objective c implicit. Formally:\nDefinition 24. A randomized algorithm M with inputs vector b ∈ Rm and matrix A ∈ Rm×d, and outputting a vector in Rd is (ǫ, δ)-low sensitivity row private with sensitivity ∆∞ if for any A,A ′ such that ‖A−A′‖∞ ≤ ∆∞,\nPr[M(b,A) ∈ S] ≤ eǫ Pr[M(b′, A′) ∈ S] + δ\nfor any set S ⊆ Rd.\nAlgorithm 5 Row/matrix private LP solver\nInput A ∈ [−1, 1]m×d, b ∈ Rm. Let x̃1 be the uniform distribution in Rd, α > 0 be the desired accuracy, and ∆∞ be the sensitivity. Let Oracle be a (α, γ)-dual oracle, and set\nT = 144 log d\nα2 , ǫ′ =\nǫ\n4 √ dT log(1/δ) , η =\n√ log d\nT .\nFor t = 1, . . . , T : Find pt = Oracle(A, b, x̃t).\nCompute private losses ℓ̂ti := Apti+Lap( ∆∞ ǫ′ ) 2 . For each i, update xt+1i = e −ηℓ̂ti · x̃ti.\nNormalize x̃t+1 = xt+1/|xt+1|. Output x = (1/T ) ∑T t=1 x̃ t.\nWe will normalize the problem so each entry in A is in [−1, 1]. The basic idea is to use multiplicative weights over the primal variables, with a dual oracle selecting the most violated constraint—since the losses fed into the multiplicative weights algorithm now depend on private data (the matrix A), we add Laplace noise to the loss vectors as they are selected. The full algorithm is given in Algorithm 5.\nWe can now show privacy and accuracy for Algorithm 5.\nTheorem 25. Let ǫ, ǫ′, δ,∆∞ be as in Algorithm 5. Algorithm 5 run with an ǫ ′-private dual oracle is (ǫ, δ)-low sensitivity row private with sensitivity ∆∞.\nProof. Algorithm 5 performs dT Laplace operations, and T oracle operations. Each operation is ǫ′-private, so this is at most 2dT ǫ′-private operations. By our choice of ǫ′ and Theorem 6, the whole algorithm is (ǫ, δ)-differentially private.\nWe can show that the exponential mechanism is a private dual oracle.\nLemma 26. Let ǫ, γ > 0 be given, and suppose the matrix A can differ by at most ∆∞ in ℓ∞ norm on neighboring instances. Let x be any distribution. Then, the ǫ-private exponential mechanism with quality score Q(i, b) = Aix− bi is an (α, γ)-dual oracle, for\nα = 2∆∞ ǫ\n· log ( m\nγ\n) .\nProof. Since x is a distribution, the quality score is ∆∞-sensitive and accuracy follows from accuracy of the exponential mechanism (Theorem 5).\nWhile previously our accuracy theorems followed from standard accuracy results for solving LPs using multiplicative weights, the proof for Algorithm 5 does not. Since the constraint matrix is private, Algorithm 4 perturbs the losses and requires a more custom analysis. So, we will need a standard regret bound for multiplicative weights.\nTheorem 27 (Littlestone and Warmuth [22]). Let { Ãt }\nbe the distributions obtained by MWη\nwith arbitrary losses { ℓt } satisfying ‖ℓt‖∞ ≤ 1. Suppose that η ≤ 1/2. Let A∗ = 1a=a∗ , for some a∗ ∈ A. Then,\nT∑\nt=1\n〈ℓt, Ãt〉 ≤ T∑\nt=1\n〈ℓt, A∗〉+ η + log |A| ηT .\nThen, we have the following accuracy guarantee.\nTheorem 28. Let β > 0. Suppose the program has a distribution as a feasible solution. Then, with probability at least 1 − β, Algorithm 5 run with the exponential mechanism as a dual oracle (Lemma 26) finds a solution x∗ such that Ax∗ ≤ b+ α · 1, where\nα = Õ\n( ∆ 1/2 ∞ d1/4\nǫ1/2 · polylog\n( d,m, 1\nβ , 1 δ\n)) .\nProof. Let ǫ′ be as in Theorem 25, T be from Algorithm 5, and γ = β/2dT . By Lemma 26, with probability at least 1/γ, the oracle’s choices satisfy\n( Ai · x̃t − bi ) − ( Apt · x̃t − bpt ) ≤ 2∆∞\nǫ′ · log\n( m\nγ\n)\nfor all constraints i. Note that the left hand side is equal to ( Ai · x̃t − bi ) − ( ℓt · x̃t − bpt ) . Taking a union bound over all T steps, this is true for all pt with probability at least 1− β/2d ≥ 1− β/2; condition on this event.\nBy a tail bound on the Laplace mechanism (Lemma 3), with probability at least 1− γ, a noisy loss ℓ̂ti satisfies ∣∣∣ℓ̂ti − (1/2)ℓti ∣∣∣ ≤ ∆∞ ǫ′ · log ( 1 γ ) .\nTaking a union bound over all T steps and d losses, this is true for all losses with probability at least 1− β/2; condition on this event.\nWe first show that if these errors are small, then the theorem holds. Assume\n2∆∞ ǫ′\n· log ( m\nγ\n) ≤ α\n6 , (1)\nso that both right hand sides above are at most α/6. Since α < 1, this implies that every Laplace noise is at most 1, so the noisy losses are bounded: |ℓ̂ti| ≤ 1.\nLet x∗ be an exactly feasible point, with ℓ1 norm 1. By the regret guarantee for multiplicative weights (Theorem 27),\n1\nT\n∑\nt\nℓ̂t · x̃t ≤ 1 T\n∑\nt\nℓ̂t · x∗ + η + log d ηT\n1\nT\n∑\nt\n( Apt\n2 +\nν\n2\n) · x̃t − bpt\n2 =\n1\nT\n∑\nt\nℓ̂t · x̃t − bpt 2 ≤ 1 T\n∑\nt\nℓ̂t · x∗ − bpt 2 + η + log d ηT ,\nwhere ν is a vector of independent draws from Lap(∆∞/ǫ ′). Let i be any constraint. Since we assumed the error of the exponential mechanism to be small (Equation (1)) and x̃t is a distribution,\n1\nT\n∑\nt\n( 1\n2 Ai · x̃t − bi 2\n) + 1\n2 ν ≤ 1 T\n∑\nt\nℓ̂t · x∗ − bpt 2 + η + log d ηT + α 6 .\nBy assumption (Equation (1)) |ν| ≤ α/6, so\n1\nT\n∑\nt\nAi · x̃t − bi ≤ 1\nT\n∑\nt\n2ℓ̂t · x∗ − bpt + α/3 + α/6 + η + log d\nηT .\nSince x∗ is a feasible point, Ai ·x∗−bi ≤ 0 for all i. By assumption (Equation (1)), ∣∣∣Ai − 2ℓ̂ti ∣∣∣ ≤ α/3. By our choice of η and T ,\nAi · x− bi ≤ 1\nT\n∑\nt\nAi · x∗ − bi + α/3 + α/3 + α/6 + η + log d\nηT\n≤ 5α 6 + η + log d ηT ≤ α,\nas desired. Now, it only remains to show Equation (1). By unfolding definitions like before, it suffices to take\nα ≥ 12∆ 1/2 ∞ d1/4(log d)1/4(log(1/δ))1/4 ǫ1/2 · ( log 288d log dm α2β )1/2 ."
    }, {
      "heading" : "4.4 Column Private LPs",
      "text" : "Rather than an entire row changing, neighboring LPs may differ in a column; that is, they differ in coefficients corresponding to a single variable. Roughly, a private database D maps to an objective vector c(D), a constraint matrix A(D), and a vector b(D). For every pair of neighboring databases D,D′, we have c(D) = c(D′) and b(D) = b(D′) independent of the data, and ‖A(D)i−A(D′)i‖∞ ≤ ∆1 for every row i of the constraint matrix. Again, we will think of ∆1 as decreasing in n; our accuracy guarantees will be trivial if this is not true. Formally:\nDefinition 29. A randomized algorithm M with inputs vector b ∈ Rm and matrix A ∈ Rm×d, and outputting a vector in Rd is (ǫ, δ)-low sensitivity column private with sensitivity ∆1 if for any A,A ′ such that ‖Ai −A′i‖∞ ≤ ∆1 for each row i ∈ [m],\nPr[M(b,A) ∈ S] ≤ eǫ Pr[M(b′, A′) ∈ S] + δ\nfor any set S ⊆ Rd.\nWe can use a very slight modification of Algorithm 5 to solve these LPs privately; the algorithm is given in Algorithm 6.\nLike before, we can show that the exponential mechanism can be used as a private dual oracle.\nAlgorithm 6 Column private LP solver\nInput A ∈ [−1, 1]m×d, b ∈ Rm. Let x̃1 be the uniform distribution in Rd, α > 0 be the desired accuracy, and ∆1 be the sensitivity. Let Oracle be an (α, γ)-dual oracle, and set\nT = 144 log d\nα2 , ǫ′ =\nǫ\n4 √ T log(1/δ) , η =\n√ log d\nT .\nFor t = 1, . . . , T : Find pt = Oracle(A, b, x̃t).\nCompute private losses ℓ̂ti := Apti+Lap\n(\n∆1 ǫ′\n)\n2 .\nFor each i, update xt+1i = e −ηℓ̂ti · x̃ti.\nNormalize x̃t+1 = xt+1/|xt+1|. Output x = (1/T ) ∑T t=1 x̃ t.\nLemma 30. Let ǫ, γ > 0 be given, and suppose neighboring matrices A,A′ satisfy ‖Ai−A′i‖1 ≤ ∆1 for every row i. Let x be any distribution. Then, the ǫ-private exponential mechanism with quality score Q(i, b) = Aix− bi is an (α, γ)-dual oracle, for\nα = 2∆1 ǫ\n· log ( m\nγ\n) .\nProof. Since x is a distribution, the quality score is ∆1-sensitive and accuracy follows from accuracy of the exponential mechanism (Theorem 5).\nTheorem 31. Let ǫ, ǫ′, δ,∆1 be as in Algorithm 6. Algorithm 6 run with an ǫ ′-private dual oracle is (ǫ, δ)-low sensitiivty column private with sensitivity ∆1.\nProof. Since the loss vector ℓt can differ by at most ∆1 in ℓ1 norm, adding Laplace noise with scale ∆1/ǫ suffices for ǫ-differentially privacy. Thus, there are T Laplace and oracle mechanism steps, each ǫ′-private. By choice of ǫ′ and composition, Algorithm 6 is (ǫ, δ)-differentially private.\nTheorem 32. Let β > 0. Suppose the program has a distribution as a feasible solution. Then, with probability at least 1 − β, Algorithm 6 run with the exponential mechanism (Lemma 30) as oracle finds a point x∗ such that Ax∗ ≤ b+ α · 1, where\nα = Õ\n( ∆\n1/2 1\nǫ1/2 · polylog\n( d,m, 1\nβ , 1 δ\n)) .\nProof. Let ǫ′ be as in Theorem 31, T be as inAlgorithm 6, and γ = β/2dT . Letting the dual oracle by the ǫ′-private exponential mechanism, the proof is nearly identical to Theorem 28. The main difference is that we need\n2∆1 ǫ′\n· log ( 1\nγ\n) ≤ α\n6\nfor everything to go through. By unfolding definitions, it suffices to take\nα ≥ 12∆ 1/2 1 (log d) 1/4(log(1/δ))1/4 ǫ1/2 · ( log 288m log d α2β )1/2 .\nComparing the two previous algorithms, note ∆∞ ≤ ∆1 ≤ d∆∞. Algorithm 5 performs better when the right inequality is tighter, i.e., when all the coefficients in a row can differ by a small amount. In contrast, Algorithm 6 performs better when the left inequality is tighter, that is, when a few coefficients in a row can differ by a larger amount."
    }, {
      "heading" : "4.5 Objective Private LPs",
      "text" : "For our final type of low-sensitivity LP, we consider linear programs with objectives that depend on private data. We show that a very simple approach—randomized response—can solve these types of LPs accurately. Throughout, we will assume that the optimal solution to the LP has ℓ1 weight equal to 1. We start with an LP in general form:\nmax c⊤x\ns.t. Ax ≤ b, On instances corresponding to neighboring database D,D′, the objective may change by ∆1 in ℓ1 norm: ‖c(D)− c(D′)‖1 ≤ ∆1. Formally: Definition 33. A randomized algorithm M with inputs vectors b ∈ Rm, c ∈ Rd and matrix A ∈ Rm×d, and outputting a vector in Rd is (ǫ, δ)-low sensitivity objective private with sensitivity ∆1 if for any c, c\n′ such that ‖c− c′‖1 ≤ ∆1, Pr[M(c, b, A) ∈ S] ≤ eǫ Pr[M(c′, b, A) ∈ S] + δ\nfor any set S ⊆ Rd. For a concrete case, a single objective coefficient may change by ∆1. All other parts of the LP do not change: A(D) = A(D′), and b(D) = b(D′). If we add Laplace noise to the objective and solve the resulting LP, we will get an almost optimal, exactly feasible solution.\nTheorem 34. Suppose an objective private LP has optimal objective OPT, and has optimal solution with ℓ1 weight 1. Define\nĉ = c+ Lap\n( ∆1 √ 8d log(1/δ)\nǫ\n)d ,\nwhere the noise is d independent draws from the Laplace distribution with the given parameter. Then, releasing the perturbed LP\nmax ĉ⊤x\ns.t. Ax ≤ b and 1⊤x = 1 is (ǫ, δ)-low sensitivity objective private with sensitivity ∆1. With probability 1 − β, solving the perturbed LP non-privately yields a point x∗ such that Ax∗ ≤ b and c⊤x∗ ≥ OPT−α, where\nα = 4∆1\n√ 8d log(d/δ)\nǫ .\nProof. Since the ℓ1 sensitivity of c is 1 and d numbers are released, (ǫ, δ)-privacy follows from the composition theorem (Theorem 6).6\nFor the accuracy, note that with probability at least 1 − β/d, a single draw of the Laplace distribution is bounded by\nα 2 =\n2∆1 √ 8d log(d/δ)\nǫ .\nBy a union bound, this happens with probability at least 1 − β for all d draws; condition on this event. Then, note that if x∗ is the optimal solution to the original LP, then it is also a feasible solution to the perturbed LP. Let x̂∗ be the optimal solution of the perturbed LP. Since the noise added to each objective coefficient is bounded by α/2, if\nc⊤x̂∗ < OPT−α\nthen ĉ⊤x̂∗ < OPT−α/2 but also ĉ⊤x∗ ≥ OPT−α/2,\ncontradicting optimality of x̂∗ in the perturbed program. Thus, this algorithm finds an exactly feasible, α-optimal solution."
    }, {
      "heading" : "5 Lower Bounds",
      "text" : "Now that we have considered various low-sensitivity LPs, let us turn to high-sensitivity LPs. In this section, we show that most high-sensitivity LPs cannot be solved privately to non-trivial accuracy. The exception is constraint private LPs—as we saw (Section 3), these can be solved in a relaxed sense. Our lower bounds are all reductions to reconstruction attacks: as the following theorem shows, differential privacy precludes reconstructing a non-trivial fraction of a database. The idea of reconstruction being a key feature of privacy violation is due to Dinur and Nissim [7]. The following theorem is folklore; we provide a proof for completeness.\nTheorem 35. Let mechanism M : {0, 1}n → [0, 1]n be (ǫ, δ)-differentially private, and suppose that for all database D, with probability at least 1− β, ‖M(D)−D‖1 ≤ αn. Then,\nα ≥ 1 2 − e\nǫ + δ\n2(1 + eǫ)(1− β) := c(ǫ, δ, β).\nThe same is true even if D is restricted to have exactly n/2 zero entries.\nProof. If we have M as in the hypothesis, then we can round each entry of M(D) to {0, 1} while preserving (ǫ, δ)-differential privacy. Note that by assumption ‖M(D)−D‖1 ≤ αn, so the number of entries M(D)i that are more than 1/2 away from Di is at most 2αn. Thus, rounding reconstructs a database in {0, 1}n at most 2αn distance from the true database in ℓ1 norm; hence we may assume that M(D) ∈ {0, 1}n with ℓ1 norm at most 2αn from D.\nAssume n is even; we prove the case where the input database D has exactly n/2 zero entries. Let D ∈ {0, 1}n have exactly n/2 zero entries, and sample an index i such that Di = 1, and an index j such that Dj = 0, both uniformly at random from [n]. Let D\n′ be identical to D except with bits i and j swapped. By assumption, we have that with probability at least 1− β\n‖M(D) −D‖1 ≤ 2αn and ‖M(D′)−D′‖1 ≤ 2αn. 6This is similar to the case of privately releasing histogram queries.\nSince i is chosen uniformly, we also know\nPr[M(D)i = Di] ≥ (1− 2α)(1 − β) and Pr[M(D′)i = D′i] ≥ (1− 2α)(1 − β).\nHence, Pr[M(D′)i = Di] ≤ 1− (1− 2α)(1 − β) because Di 6= D′i. By (ǫ, δ)-differential privacy, we get\n(1− 2α)(1 − β) ≤ Pr[M(D)i = Di] ≤ eǫ Pr[M(D′)i = Di] + δ ≤ eǫ(1− (1− 2α)(1 − β)) + δ.\nFinally,\n1− 2α ≤ e ǫ + δ\n(1 + eǫ)(1 − β) ,\nas desired.\nFor each type of impossible private LP, we show how to convert a database D ∈ {0, 1}n to a LP, such that neighboring databases D,D′ lead to neighboring LPs. We then show that a LP solver that privately solves this LP to non-trivial accuracy leads to a reconstruction attack on D, violating Theorem 35.\nFirst, some notation. For the general LP\nmax c⊤x\ns.t. Ax ≤ b,\nwe say that x∗ is an α-feasible solution if Ax∗ ≤ b+α · 1. Likewise, we say that x∗ is an α-optimal solution if it is feasible, and\nc⊤x∗ ≥ max x:Ax≤b c⊤x− α."
    }, {
      "heading" : "5.1 High-Sensitivity Scalars",
      "text" : "Consider a database D ∈ {0, 1}n, and the following LP:\nfind x\ns.t. xi = Di for each i\nNote that changing a single bit in D will change a single right hand side in a constraint by 1.\nTheorem 36. Suppose mechanism M is (ǫ, δ)-high sensitivity scalar private, and with probability at least 1− β, finds an α-feasible solution. Then, α ≥ 1/2.\nProof. Consider the gadget LP above. Note that if M guarantees α < 1/2, then |xi −Di| < 1/2 so rounding xi to 0 or 1 will reconstruct Di exactly. By Theorem 35, this is impossible under differential privacy."
    }, {
      "heading" : "5.2 High-Sensitivity Objective",
      "text" : "Consider a database D ∈ {0, 1}n with exactly n/2 zeros, and the following LP:\nmaximize ∑\ni\nDixi − n/2\ns.t. ∑\ni\nxi = n/2, xi ∈ [0, 1]\nNote that swapping a zero and a non-zero bit in D will change exactly two objective coefficients in the LP by 1. Observe that this is similar to the objective private LP (Section 4.5) because we are only allowing the objective to change. However here we consider the setting where a single objective coefficient changes arbitrarily, rather than by a small amount.\nTheorem 37. Suppose mechanism M is (ǫ, δ)-high-sensitivity objective private, and with probability at least 1 − β, finds an exactly feasible, additive (αn)-optimal solution. Then, α ≥ c(2ǫ, δ(1 + eǫ), β).\nProof. Consider the gadget LP above. Note that the optimal solution is xi = Di, with objective 0. With probability at least 1−β, M finds a solution x∗ with objective at least −αn. In this case, x∗ places at most αn mass on indices with Di = 0, so at least (1− α)n mass of D and x∗ are shared. Thus, ‖D − x∗‖1 ≤ αn. Since a change in D leads to a distance two change in the LP, the composition is (2ǫ, δ(1 + eǫ))private. By Theorem 35, α ≥ c(2ǫ, δ(1 + eǫ), β)."
    }, {
      "heading" : "5.3 High-Sensitivity Constraints/Columns",
      "text" : "Consider a database D ∈ {0, 1}n with exactly n/2 zeros, and the following LP:\nfind xi\ns.t. ∑\ni\nDixi = n/2\nxi ∈ [0, 1] and ∑\ni\nxi = n/2\nNote that changing a single bit in D will change coefficients in a single constraint in the LP by 1. Observe that this is similar to the column private LP (Section 4.4) because we are allowing the coefficients for a single variable to change. However here we consider the setting where this coefficient can change arbitrarily, rather than by only a small amount.\nThis problem is also a special case of constraint private LPs (Section 3) because the coefficients in one (i.e., the only) constraint can change arbitrarily. In the current setting, we want a solution that approximately satisfies all constraints, rather than just satisfying most of the constraints.\nTheorem 38. Suppose mechanism M is (ǫ, δ)-high-sensitivity constraint private, and finds an α-feasible solution that satisfies all public constraints with probability at least 1 − β. Then, α ≥ c(2ǫ, δ(1 + eǫ), β).\nProof. Consider the gadget LP above. Suppose with probability 1 − β, M finds x∗ such that ‖Ax∗ − b‖∞ ≤ α for the gadget LP. By reasoning analogous to Theorem 37, at least (1 − α)n of the mass of x∗ will coincide with D, hence α ≥ c(2ǫ, δ(1 + eǫ), β) by Theorem 35.\nAlso note that the LPs produced by this reduction differ only in the coefficients corresponding to two variables. Hence, Theorem 38 also shows that privately solving column private LPs to nontrivial accuracy is impossible. It’s possible that a relaxed solution, similar to allowing unsatisfied constraints in the constraint-private case, could be possible under column privacy.\nHowever, it is not enough to allow some constraints to be unsatisfied. Since we can simply duplicate the constraint in our lower bound gadget multiple times, producing a solution satisfying any single constraint to non-trivial accuracy is impossible under high-sensitivity column privacy. A different relaxation would be needed for non-trivial accuracy under column privacy."
    }, {
      "heading" : "6 Discussion",
      "text" : "In this paper, we have initiated the study of privately solving general linear programs. We have given a taxonomy of private linear programs, classified by how the private data affects the LP. For each type of linear program in the taxonomy, we have either given an efficient algorithm, or an impossibility result.\nOne natural question is, to what extent do our results extend to other private convex programs, e.g., semidefinite programs (SDPs)? A tempting approach is to to use the Matrix Multiplicative Weights algorithm of Arora and Kale [1] for solving SDPs. However, certain features of the standard multiplicative weights algorithm which we use crucially—such as compatibility with Bregman projections—seem more delicate when using Matrix Multiplicative Weights."
    } ],
    "references" : [ {
      "title" : "A combinatorial, primal-dual approach to semidefinite programs",
      "author" : [ "Sanjeev Arora", "Satyen Kale" ],
      "venue" : "In ACM SIGACT Symposium on Theory of Computing (STOC),",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2007
    }, {
      "title" : "The multiplicative weights update method: a meta-algorithm and Theory of Computing",
      "author" : [ "Sanjeev Arora", "Elad Hazan", "Satyen Kale" ],
      "venue" : null,
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2012
    }, {
      "title" : "Practical privacy: the sulq framework",
      "author" : [ "Avrim Blum", "Cynthia Dwork", "Frank McSherry", "Kobbi Nissim" ],
      "venue" : "In ACM SIGACT–SIGMOD–SIGART Symposium on Principles of Database Systems (PODS),",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2005
    }, {
      "title" : "A learning theory approach to noninteractive database privacy",
      "author" : [ "Avrim Blum", "Katrina Ligett", "Aaron Roth" ],
      "venue" : "Journal of the ACM,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2013
    }, {
      "title" : "Fingerprinting codes and the price of approximate differential",
      "author" : [ "Mark Bun", "Jonathan Ullman", "Salil P Vadhan" ],
      "venue" : "In ACM SIGACT Symposium on Theory of Computing (STOC),",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2014
    }, {
      "title" : "Differentially private empirical risk minimization",
      "author" : [ "Kamalika Chaudhuri", "Claire Monteleoni", "Anand D. Sarwate" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2011
    }, {
      "title" : "Revealing information while preserving privacy",
      "author" : [ "Irit Dinur", "Kobbi Nissim" ],
      "venue" : "In ACM SIGACT–SIGMOD–SIGART Symposium on Principles of Database Systems (PODS),",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2003
    }, {
      "title" : "Differential privacy: A survey of results",
      "author" : [ "Cynthia Dwork" ],
      "venue" : "In Theory and Applications of MOdels of Computation (TAMC), Xi’an,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2008
    }, {
      "title" : "Calibrating noise to sensitivity in private data analysis",
      "author" : [ "Cynthia Dwork", "Frank McSherry", "Kobbi Nissim", "Adam Smith" ],
      "venue" : "In IACR Theory of Cryptography Conference (TCC),",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2006
    }, {
      "title" : "On the complexity of differentially private data release: efficient algorithms and hardness results",
      "author" : [ "Cynthia Dwork", "Moni Naor", "Omer Reingold", "Guy N. Rothblum", "Salil Vadhan" ],
      "venue" : "In ACM SIGACT Symposium on Theory of Computing (STOC),",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2009
    }, {
      "title" : "Boosting and differential privacy",
      "author" : [ "Cynthia Dwork", "Guy N. Rothblum", "Salil Vadhan" ],
      "venue" : "In IEEE Symposium on Foundations of Computer Science (FOCS),",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2010
    }, {
      "title" : "Differentially private combinatorial optimization",
      "author" : [ "Anupam Gupta", "Katrina Ligett", "Frank McSherry", "Aaron Roth", "Kunal Talwar" ],
      "venue" : "In ACM–SIAM Symposium on Discrete Algorithms",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2010
    }, {
      "title" : "Privately releasing conjunctions and the statistical query barrier",
      "author" : [ "Anupam Gupta", "Moritz Hardt", "Aaron Roth", "Jonathan Ullman" ],
      "venue" : "In ACM SIGACT Symposium on Theory of Computing (STOC),",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2011
    }, {
      "title" : "Iterative constructions and private data release",
      "author" : [ "AnupamGupta", "Aaron Roth", "Jonathan Ullman" ],
      "venue" : "In IACR Theory of Cryptography Conference (TCC),",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2012
    }, {
      "title" : "A multiplicative weights mechanism for privacy-preserving data analysis",
      "author" : [ "Moritz Hardt", "Guy N. Rothblum" ],
      "venue" : "In IEEE Symposium on Foundations of Computer Science (FOCS),",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2010
    }, {
      "title" : "A simple and practical algorithm for differentially private",
      "author" : [ "Moritz Hardt", "Katrina Ligett", "Frank McSherry" ],
      "venue" : "In Conference on Neural Information Processing Systems (NIPS), Lake Tahoe,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2012
    }, {
      "title" : "Tracking the best linear predictor",
      "author" : [ "Mark Herbster", "Manfred K. Warmuth" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2001
    }, {
      "title" : "Differential privacy for the analyst via private equilibrium computation",
      "author" : [ "Justin Hsu", "Aaron Roth", "Jonathan Ullman" ],
      "venue" : "In ACM SIGACT Symposium on Theory of Computing (STOC),",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2013
    }, {
      "title" : "What can we learn privately",
      "author" : [ "Shiva Prasad Kasiviswanathan", "Homin K. Lee", "Kobbi Nissim", "Sofya Raskhodnikova", "Adam Smith" ],
      "venue" : "SIAM Journal on Computing,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2011
    }, {
      "title" : "Mechanism design in large games: Incentives and privacy",
      "author" : [ "Michael Kearns", "Mallesh Pai", "Aaron Roth", "Jonathan Ullman" ],
      "venue" : "In ACM SIGACT Innovations in Theoretical Computer Science (ITCS),",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2014
    }, {
      "title" : "Private convex empirical risk minimization and high-dimensional regression",
      "author" : [ "Daniel Kifer", "Adam Smith", "Abhradeep Thakurta" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2012
    }, {
      "title" : "The weighted majority algorithm",
      "author" : [ "N. Littlestone", "Manfred K. Warmuth" ],
      "venue" : "Information and Computation,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 1994
    }, {
      "title" : "Mechanism design via differential privacy",
      "author" : [ "Frank McSherry", "Kunal Talwar" ],
      "venue" : "In IEEE Symposium on Foundations of Computer Science (FOCS),",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2007
    }, {
      "title" : "Smooth sensitivity and sampling in private data analysis",
      "author" : [ "Kobbi Nissim", "Sofya Raskhodnikova", "Adam Smith" ],
      "venue" : "In ACM SIGACT Symposium on Theory of Computing (STOC),",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2007
    }, {
      "title" : "Fast approximation algorithms for fractional packing and covering problems",
      "author" : [ "Serge A. Plotkin", "David B. Shmoys", "Éva Tardos" ],
      "venue" : "Mathematics of Operations Research,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 1995
    }, {
      "title" : "Answering n2+o(1) counting queries with differential privacy is hard",
      "author" : [ "Jonathan Ullman" ],
      "venue" : "In ACM SIGACT Symposium on Theory of Computing (STOC), Palo Alto, California,",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2013
    }, {
      "title" : "PCPs and the hardness of generating private synthetic data",
      "author" : [ "Jonathan Ullman", "Salil Vadhan" ],
      "venue" : "In IACR Theory of Cryptography Conference (TCC),",
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 2011
    } ],
    "referenceMentions" : [ {
      "referenceID" : 18,
      "context" : "[19] show how to privately PAC learn any PAC learnable concept class (without privacy) with only a small increase in the sample complexity, but via an exponential time algorithm.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 3,
      "context" : "[4] show how to privately release a summary of a private database that approximately preserves the answers to rich families of linear queries, again via an exponential time algorithm.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 9,
      "context" : "In fact, under standard cryptographic assumptions, it is not possible to efficiently and privately answer large collections of general linear queries [10, 28, 27].",
      "startOffset" : 150,
      "endOffset" : 162
    }, {
      "referenceID" : 26,
      "context" : "In fact, under standard cryptographic assumptions, it is not possible to efficiently and privately answer large collections of general linear queries [10, 28, 27].",
      "startOffset" : 150,
      "endOffset" : 162
    }, {
      "referenceID" : 25,
      "context" : "In fact, under standard cryptographic assumptions, it is not possible to efficiently and privately answer large collections of general linear queries [10, 28, 27].",
      "startOffset" : 150,
      "endOffset" : 162
    }, {
      "referenceID" : 22,
      "context" : "The two preceding examples are among the many algorithms that use the extremely general exponential mechanism of McSherry and Talwar [23] to achieve near optimal error.",
      "startOffset" : 133,
      "endOffset" : 137
    }, {
      "referenceID" : 23,
      "context" : ", the sample and aggregate framework [24] and output/objective perturbation for unconstrained convex optimization [6, 21]).",
      "startOffset" : 37,
      "endOffset" : 41
    }, {
      "referenceID" : 5,
      "context" : ", the sample and aggregate framework [24] and output/objective perturbation for unconstrained convex optimization [6, 21]).",
      "startOffset" : 114,
      "endOffset" : 121
    }, {
      "referenceID" : 20,
      "context" : ", the sample and aggregate framework [24] and output/objective perturbation for unconstrained convex optimization [6, 21]).",
      "startOffset" : 114,
      "endOffset" : 121
    }, {
      "referenceID" : 14,
      "context" : "Solving this kind of linear programming privately is similar to the well-studied linear query release problem in differential privacy, and techniques for linear query release—such as the private multiplicative weights algorithm of Hardt and Rothblum [15] (and its offline variants [14, 16])—can be adapted with minor changes.",
      "startOffset" : 250,
      "endOffset" : 254
    }, {
      "referenceID" : 13,
      "context" : "Solving this kind of linear programming privately is similar to the well-studied linear query release problem in differential privacy, and techniques for linear query release—such as the private multiplicative weights algorithm of Hardt and Rothblum [15] (and its offline variants [14, 16])—can be adapted with minor changes.",
      "startOffset" : 281,
      "endOffset" : 289
    }, {
      "referenceID" : 15,
      "context" : "Solving this kind of linear programming privately is similar to the well-studied linear query release problem in differential privacy, and techniques for linear query release—such as the private multiplicative weights algorithm of Hardt and Rothblum [15] (and its offline variants [14, 16])—can be adapted with minor changes.",
      "startOffset" : 281,
      "endOffset" : 289
    }, {
      "referenceID" : 6,
      "context" : "2 Related Work Differential privacy emerged from a line of work initiated by Dinur and Nissim [7], was defined by Dwork et al.",
      "startOffset" : 94,
      "endOffset" : 97
    }, {
      "referenceID" : 8,
      "context" : "[9], and is now a standard definition of privacy in computer science.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 7,
      "context" : "Below, we discuss relevant results in differential privacy; the survey by Dwork [8] is an excellent source for a more comprehensive overview.",
      "startOffset" : 80,
      "endOffset" : 83
    }, {
      "referenceID" : 2,
      "context" : "[3] and Kasiviswanathan et al.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 18,
      "context" : "[19], who considered how to choose an optimal classifier privately.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 2,
      "context" : "[3] give an efficient reduction from SQ learning to private SQ learning, and Kasiviswanathan et al.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 18,
      "context" : "[19] give a very general but inefficient reduction from PAC learning to private PAC learning using the exponential mechanism of McSherry and Talwar [23].",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 22,
      "context" : "[19] give a very general but inefficient reduction from PAC learning to private PAC learning using the exponential mechanism of McSherry and Talwar [23].",
      "startOffset" : 148,
      "endOffset" : 152
    }, {
      "referenceID" : 5,
      "context" : "[6], who give two techniques for privately solving certain unconstrained convex optimization problems.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 11,
      "context" : "[12] give several algorithms for problems in private combinatorial optimization, but these were specialized combinatorial algorithms for specific problems.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 3,
      "context" : "[4] and continuing with Dwork et al.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 9,
      "context" : "[10], Roth and Roughgarden [26], Dwork et al.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 10,
      "context" : "[11], Hardt and Rothblum [15], Gupta et al.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 14,
      "context" : "[11], Hardt and Rothblum [15], Gupta et al.",
      "startOffset" : 25,
      "endOffset" : 29
    }, {
      "referenceID" : 13,
      "context" : "[14], Hardt et al.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 15,
      "context" : "[16] study the problem of privately producing synthetic data consistent with some private database on many linear queries.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 14,
      "context" : "(Of particular note is the private multiplicative weights mechanism of Hardt and Rothblum [15], which achieves the optimal accuracy and running time bounds [27, 5].",
      "startOffset" : 90,
      "endOffset" : 94
    }, {
      "referenceID" : 25,
      "context" : "(Of particular note is the private multiplicative weights mechanism of Hardt and Rothblum [15], which achieves the optimal accuracy and running time bounds [27, 5].",
      "startOffset" : 156,
      "endOffset" : 163
    }, {
      "referenceID" : 4,
      "context" : "(Of particular note is the private multiplicative weights mechanism of Hardt and Rothblum [15], which achieves the optimal accuracy and running time bounds [27, 5].",
      "startOffset" : 156,
      "endOffset" : 163
    }, {
      "referenceID" : 14,
      "context" : ") This problem can be represented as a linear program with queries defining constraints, and indeed, the private multiplicative weights algorithm of Hardt and Rothblum [15] can be directly applied to solve this kind of linear program.",
      "startOffset" : 168,
      "endOffset" : 172
    }, {
      "referenceID" : 24,
      "context" : "[25] (see the excellent survey by Arora et al.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 1,
      "context" : "[2] for more details).",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 24,
      "context" : "[25] maintain a distribution over the dual variables with multiplicative weights, depending on the kind of linear program we are solving, we either maintain a distribution over the dual variables or the primal variables.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 1,
      "context" : "To solve constraint private LPs, we use a combination of the multiplicative weights update method and Bregman projections [2]—Hsu et al.",
      "startOffset" : 122,
      "endOffset" : 125
    }, {
      "referenceID" : 17,
      "context" : "[18] use a similar version of this technique in designing analyst private mechanisms.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 8,
      "context" : "[9].",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 8,
      "context" : "[9]).",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 8,
      "context" : "[9]).",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 22,
      "context" : "We will also use the exponential mechanism [23], which can privately produce a non-numeric or discrete output.",
      "startOffset" : 43,
      "endOffset" : 47
    }, {
      "referenceID" : 22,
      "context" : "Definition 4 (McSherry and Talwar [23]).",
      "startOffset" : 34,
      "endOffset" : 38
    }, {
      "referenceID" : 22,
      "context" : "Theorem 5 (McSherry and Talwar [23]).",
      "startOffset" : 31,
      "endOffset" : 35
    }, {
      "referenceID" : 10,
      "context" : "[11]).",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 0,
      "context" : "Throughout, we will use calligraphic letters (A) to denote sets of actions, Roman letters (A) to denote measures on those actions A : A → [0, 1], and letters with tildes (Ã) to denote a probability distributions over actions.",
      "startOffset" : 138,
      "endOffset" : 144
    }, {
      "referenceID" : 16,
      "context" : "We will call this algorithm, due to Herbster and Warmuth [17], the dense multiplicative weights algorithm (Algorithm 1).",
      "startOffset" : 57,
      "endOffset" : 61
    }, {
      "referenceID" : 16,
      "context" : "3 Theorem 9 (Herbster and Warmuth [17]).",
      "startOffset" : 34,
      "endOffset" : 38
    }, {
      "referenceID" : 24,
      "context" : "[25]; the novelty in our approach is that we use multiplicative weights paired with a projection onto the set of dense distributions, and show that the solution approximately satisfies most of the constraints.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 17,
      "context" : "[18]; we reproduce the proof for completeness.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 17,
      "context" : "[18]).",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 11,
      "context" : "[12].",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 11,
      "context" : "[12].",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 11,
      "context" : "[12] outputs an implicit solution, a “set of instructions” that describes a set cover when paired with the private data.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 19,
      "context" : "(Their approach can also be interpreted as satisfying the weaker guarantee of joint differential privacy [20] rather than standard differential privacy.",
      "startOffset" : 105,
      "endOffset" : 109
    }, {
      "referenceID" : 24,
      "context" : "[25]).",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 12,
      "context" : "The algorithm we use is a slight generalization of the offline private multiplicative weights algorithm [13, 16] (building on the influential work of Hardt and Rothblum [15], who introduced the “online” variant).",
      "startOffset" : 104,
      "endOffset" : 112
    }, {
      "referenceID" : 15,
      "context" : "The algorithm we use is a slight generalization of the offline private multiplicative weights algorithm [13, 16] (building on the influential work of Hardt and Rothblum [15], who introduced the “online” variant).",
      "startOffset" : 104,
      "endOffset" : 112
    }, {
      "referenceID" : 14,
      "context" : "The algorithm we use is a slight generalization of the offline private multiplicative weights algorithm [13, 16] (building on the influential work of Hardt and Rothblum [15], who introduced the “online” variant).",
      "startOffset" : 169,
      "endOffset" : 173
    }, {
      "referenceID" : 14,
      "context" : "This bound generalizes the guarantee for the private multiplicative weights algorithm when privately generating synthetic data for linear queries [15].",
      "startOffset" : 146,
      "endOffset" : 150
    }, {
      "referenceID" : 21,
      "context" : "Theorem 27 (Littlestone and Warmuth [22]).",
      "startOffset" : 36,
      "endOffset" : 40
    }, {
      "referenceID" : 6,
      "context" : "The idea of reconstruction being a key feature of privacy violation is due to Dinur and Nissim [7].",
      "startOffset" : 95,
      "endOffset" : 98
    }, {
      "referenceID" : 0,
      "context" : "Let mechanism M : {0, 1}n → [0, 1]n be (ǫ, δ)-differentially private, and suppose that for all database D, with probability at least 1− β, ‖M(D)−D‖1 ≤ αn.",
      "startOffset" : 28,
      "endOffset" : 34
    }, {
      "referenceID" : 0,
      "context" : "i xi = n/2, xi ∈ [0, 1]",
      "startOffset" : 17,
      "endOffset" : 23
    }, {
      "referenceID" : 0,
      "context" : "i Dixi = n/2 xi ∈ [0, 1] and ∑",
      "startOffset" : 18,
      "endOffset" : 24
    }, {
      "referenceID" : 0,
      "context" : ", semidefinite programs (SDPs)? A tempting approach is to to use the Matrix Multiplicative Weights algorithm of Arora and Kale [1] for solving SDPs.",
      "startOffset" : 127,
      "endOffset" : 130
    } ],
    "year" : 2014,
    "abstractText" : "In this paper, we initiate the systematic study of solving linear programs under differential privacy. The first step is simply to define the problem: to this end, we introduce several natural classes of private linear programs that capture different ways sensitive data can be incorporated into a linear program. For each class of linear programs we give an efficient, differentially private solver based on the multiplicative weights framework, or we give an impossibility result. Department of Computer and Information Science, University of Pennsylvania. Supported in part by NSF Grant CNS-1065060. Department of Computer and Information Science, University of Pennsylvania. Supported in part by an NSF CAREER award, NSF Grants CCF-1101389 and CNS-1065060, and a Google Focused Research Award. Email: aaroth@cis.upenn.edu. Department of Computer Science, Stanford University. Supported in part by NSF Awards CCF-1016885 and CCF-1215965, and an ONR PECASE Award. School of Engineering and Applied Sciences and Center for Research on Computation and Society, Harvard University. Supported by NSF grant CNS-1237235. Email: jullman@seas.harvard.edu.",
    "creator" : "LaTeX with hyperref package"
  }
}