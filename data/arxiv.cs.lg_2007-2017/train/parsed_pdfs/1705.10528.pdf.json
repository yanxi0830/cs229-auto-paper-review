{
  "name" : "1705.10528.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Constrained Policy Optimization",
    "authors" : [ "Joshua Achiam", "David Held", "Aviv Tamar", "Pieter Abbeel" ],
    "emails" : [ "<jachiam@berkeley.edu>." ],
    "sections" : [ {
      "heading" : null,
      "text" : "We propose Constrained Policy Optimization (CPO), the first general-purpose policy search algorithm for constrained reinforcement learning with guarantees for near-constraint satisfaction at each iteration. Our method allows us to train neural network policies for high-dimensional control while making guarantees about policy behavior all throughout training. Our guarantees are based on a new theoretical result, which is of independent interest: we prove a bound relating the expected returns of two policies to an average divergence between them. We demonstrate the effectiveness of our approach on simulated robot locomotion tasks where the agent must satisfy constraints motivated by safety."
    }, {
      "heading" : "1. Introduction",
      "text" : "Recently, deep reinforcement learning has enabled neural network policies to achieve state-of-the-art performance on many high-dimensional control tasks, including Atari games (using pixels as inputs) (Mnih et al., 2015; 2016), robot locomotion and manipulation (Schulman et al., 2015; Levine et al., 2016; Lillicrap et al., 2016), and even Go at the human grandmaster level (Silver et al., 2016).\n1UC Berkeley 2OpenAI. Correspondence to: Joshua Achiam <jachiam@berkeley.edu>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, 2017. JMLR: W&CP. Copyright 2017 by the author(s).\nIn reinforcement learning (RL), agents learn to act by trial and error, gradually improving their performance at the task as learning progresses. Recent work in deep RL assumes that agents are free to explore any behavior during learning, so long as it leads to performance improvement. In many realistic domains, however, it may be unacceptable to give an agent complete freedom. Consider, for example, an industrial robot arm learning to assemble a new product in a factory. Some behaviors could cause it to damage itself or the plant around it—or worse, take actions that are harmful to people working nearby. In domains like this, safe exploration for RL agents is important (Moldovan & Abbeel, 2012; Amodei et al., 2016). A natural way to incorporate safety is via constraints.\nA standard and well-studied formulation for reinforcement learning with constraints is the constrained Markov Decision Process (CMDP) framework (Altman, 1999), where agents must satisfy constraints on expectations of auxilliary costs. Although optimal policies for finite CMDPs with known models can be obtained by linear programming, methods for high-dimensional control are lacking.\nCurrently, policy search algorithms enjoy state-of-theart performance on high-dimensional control tasks (Mnih et al., 2016; Duan et al., 2016). Heuristic algorithms for policy search in CMDPs have been proposed (Uchibe & Doya, 2007), and approaches based on primal-dual methods can be shown to converge to constraint-satisfying policies (Chow et al., 2015), but there is currently no approach for policy search in continuous CMDPs that guarantees every policy during learning will satisfy constraints. In this work, we propose the first such algorithm, allowing applications to constrained deep RL.\nDriving our approach is a new theoretical result that bounds the difference between the rewards or costs of two different policies. This result, which is of independent interest, tightens known bounds for policy search using trust regions (Kakade & Langford, 2002; Pirotta et al., 2013; Schulman et al., 2015), and provides a tighter connection between the theory and practice of policy search for deep RL. Here, we use this result to derive a policy improvement step that guarantees both an increase in reward and satisfaction of constraints on other costs. This step forms the basis for our algorithm, Constrained Policy Optimization (CPO), which\nar X\niv :1\n70 5.\n10 52\n8v 1\n[ cs\n.L G\n] 3\n0 M\nay 2\n01 7\ncomputes an approximation to the theoretically-justified update.\nIn our experiments, we show that CPO can train neural network policies with thousands of parameters on highdimensional simulated robot locomotion tasks to maximize rewards while successfully enforcing constraints."
    }, {
      "heading" : "2. Related Work",
      "text" : "Safety has long been a topic of interest in RL research, and a comprehensive overview of safety in RL was given by (Garcı́a & Fernández, 2015).\nSafe policy search methods have been proposed in prior work. Uchibe and Doya (2007) gave a policy gradient algorithm that uses gradient projection to enforce active constraints, but this approach suffers from an inability to prevent a policy from becoming unsafe in the first place. Bou Ammar et al. (2015) propose a theoretically-motivated policy gradient method for lifelong learning with safety constraints, but their method involves an expensive inner loop optimization of a semi-definite program, making it unsuited for the deep RL setting. Their method also assumes that safety constraints are linear in policy parameters, which is limiting. Chow et al. (2015) propose a primal-dual subgradient method for risk-constrained reinforcement learning which takes policy gradient steps on an objective that trades off return with risk, while simultaneously learning the trade-off coefficients (dual variables).\nSome approaches specifically focus on application to the deep RL setting. Held et al. (2017) study the problem for robotic manipulation, but the assumptions they make restrict the applicability of their methods. Lipton et al. (2017) use an ‘intrinsic fear’ heuristic, as opposed to constraints, to motivate agents to avoid rare but catastrophic events. Shalev-Shwartz et al. (2016) avoid the problem of enforcing constraints on parametrized policies by decomposing ‘desires’ from trajectory planning; the neural network policy learns desires for behavior, while the trajectory planning algorithm (which is not learned) selects final behavior and enforces safety constraints.\nIn contrast to prior work, our method is the first policy search algorithm for CMDPs that both 1) guarantees constraint satisfaction throughout training, and 2) works for arbitrary policy classes (including neural networks)."
    }, {
      "heading" : "3. Preliminaries",
      "text" : "A Markov decision process (MDP) is a tuple, (S,A,R, P, µ), where S is the set of states, A is the set of actions, R : S ×A× S → R is the reward function, P : S×A×S → [0, 1] is the transition probability function (where P (s′|s, a) is the probability of transitioning to state\ns′ given that the previous state was s and the agent took action a in s), and µ : S → [0, 1] is the starting state distribution. A stationary policy π : S → P(A) is a map from states to probability distributions over actions, with π(a|s) denoting the probability of selecting action a in state s. We denote the set of all stationary policies by Π.\nIn reinforcement learning, we aim to select a policy π which maximizes a performance measure, J(π), which is typically taken to be the infinite horizon discounted total return, J(π) .= E τ∼π [ ∑∞ t=0 γ tR(st, at, st+1)]. Here γ ∈ [0, 1) is the discount factor, τ denotes a trajectory (τ = (s0, a0, s1, ...)), and τ ∼ π is shorthand for indicating that the distribution over trajectories depends on π: s0 ∼ µ, at ∼ π(·|st), st+1 ∼ P (·|st, at).\nLetting R(τ) denote the discounted return of a trajectory, we express the on-policy value function as V π(s) .= Eτ∼π[R(τ)|s0 = s] and the on-policy action-value function as Qπ(s, a) .= Eτ∼π[R(τ)|s0 = s, a0 = a]. The advantage function is Aπ(s, a) .= Qπ(s, a)− V π(s).\nAlso of interest is the discounted future state distribution, dπ , defined by dπ(s) = (1−γ) ∑∞ t=0 γ\ntP (st = s|π). It allows us to compactly express the difference in performance between two policies π′, π as\nJ(π′)− J(π) = 1 1− γ E s∼dπ ′\na∼π′\n[Aπ(s, a)] , (1)\nwhere by a ∼ π′, we mean a ∼ π′(·|s), with explicit notation dropped to reduce clutter. For proof of (1), see (Kakade & Langford, 2002) or Section 10 in the supplementary material."
    }, {
      "heading" : "4. Constrained Markov Decision Processes",
      "text" : "A constrained Markov decision process (CMDP) is an MDP augmented with constraints that restrict the set of allowable policies for that MDP. Specifically, we augment the MDP with a set C of auxiliary cost functions, C1, ..., Cm (with each one a function Ci : S × A × S → R mapping transition tuples to costs, like the usual reward), and limits d1, ..., dm. Let JCi(π) denote the expected discounted return of policy π with respect to cost function Ci: JCi(π) = E τ∼π [ ∑∞ t=0 γ\ntCi(st, at, st+1)]. The set of feasible stationary policies for a CMDP is then\nΠC . = {π ∈ Π : ∀i, JCi(π) ≤ di} ,\nand the reinforcement learning problem in a CMDP is\nπ∗ = arg max π∈ΠC J(π).\nThe choice of optimizing only over stationary policies is justified: it has been shown that the set of all optimal policies for a CMDP includes stationary policies, under mild\ntechnical conditions. For a thorough review of CMDPs and CMDP theory, we refer the reader to (Altman, 1999).\nWe refer to JCi as a constraint return, or Ci-return for short. Lastly, we define on-policy value functions, actionvalue functions, and advantage functions for the auxiliary costs in analogy to V π , Qπ , and Aπ , with Ci replacing R: respectively, we denote these by V πCi , Q π Ci , and AπCi ."
    }, {
      "heading" : "5. Constrained Policy Optimization",
      "text" : "For large or continuous MDPs, solving for the exact optimal policy is intractable due to the curse of dimensionality (Sutton & Barto, 1998). Policy search algorithms approach this problem by searching for the optimal policy within a set Πθ ⊆ Π of parametrized policies with parameters θ (for example, neural networks of a fixed architecture). In local policy search (Peters & Schaal, 2008), the policy is iteratively updated by maximizing J(π) over a local neighborhood of the most recent iterate πk:\nπk+1 = arg max π∈Πθ J(π)\ns.t. D(π, πk) ≤ δ, (2)\nwhere D is some distance measure, and δ > 0 is a step size. When the objective is estimated by linearizing around πk as J(πk) + gT (θ − θk), g is the policy gradient, and the standard policy gradient update is obtained by choosing D(π, πk) = ‖θ − θk‖2 (Schulman et al., 2015).\nIn local policy search for CMDPs, we additionally require policy iterates to be feasible for the CMDP, so instead of optimizing over Πθ, we optimize over Πθ ∩ΠC :\nπk+1 = arg max π∈Πθ J(π)\ns.t. JCi(π) ≤ di i = 1, ...,m D(π, πk) ≤ δ.\n(3)\nThis update is difficult to implement in practice because it requires evaluation of the constraint functions to determine whether a proposed point π is feasible. When using sampling to compute policy updates, as is typically done in high-dimensional control (Duan et al., 2016), this requires off-policy evaluation, which is known to be challenging (Jiang & Li, 2015). In this work, we take a different approach, motivated by recent methods for trust region optimization (Schulman et al., 2015).\nWe develop a principled approximation to (3) with a particular choice of D, where we replace the objective and constraints with surrogate functions. The surrogates we choose are easy to estimate from samples collected on πk, and are good local approximations for the objective and constraints. Our theoretical analysis shows that for our choices of surrogates, we can bound our update’s worst-\ncase performance and worst-case constraint violation with values that depend on a hyperparameter of the algorithm.\nTo prove the performance guarantees associated with our surrogates, we first prove new bounds on the difference in returns (or constraint returns) between two arbitrary stochastic policies in terms of an average divergence between them. We then show how our bounds permit a new analysis of trust region methods in general: specifically, we prove a worst-case performance degradation at each update. We conclude by motivating, presenting, and proving gurantees on our algorithm, Constrained Policy Optimization (CPO), a trust region method for CMDPs."
    }, {
      "heading" : "5.1. Policy Performance Bounds",
      "text" : "In this section, we present the theoretical foundation for our approach—a new bound on the difference in returns between two arbitrary policies. This result, which is of independent interest, extends the works of (Kakade & Langford, 2002), (Pirotta et al., 2013), and (Schulman et al., 2015), providing tighter bounds. As we show later, it also relates the theoretical bounds for trust region policy improvement with the actual trust region algorithms that have been demonstrated to be successful in practice (Duan et al., 2016). In the context of constrained policy search, we later use our results to propose policy updates that both improve the expected return and satisfy constraints.\nThe following theorem connects the difference in returns (or constraint returns) between two arbitrary policies to an average divergence between them.\nTheorem 1. For any function f : S → R and any policies π′ and π, define δf (s, a, s′) . = R(s, a, s′) + γf(s′)− f(s),\nπ ′ f . = max s |Ea∼π′,s′∼P [δf (s, a, s′)]| ,\nLπ,f (π ′) . = E s∼dπ a∼π s′∼P\n[( π′(a|s) π(a|s) − 1 ) δf (s, a, s ′) ] , and\nD±π,f (π ′) . = Lπ,f (π\n′)\n1− γ ±\n2γ π ′\nf\n(1− γ)2 E s∼dπ [DTV (π\n′||π)[s]] ,\nwhere DTV (π′||π)[s] = (1/2) ∑ a |π′(a|s)− π(a|s)| is the total variational divergence between action distributions at s. The following bounds hold:\nD+π,f (π ′) ≥ J(π′)− J(π) ≥ D−π,f (π ′). (4)\nFurthermore, the bounds are tight (when π′ = π, all three expressions are identically zero).\nBefore proceeding, we connect this result to prior work. By bounding the expectation Es∼dπ [DTV (π′||π)[s]] with maxsDTV (π ′||π)[s], picking f = V π , and bounding π′V π\nto get a second factor of maxsDTV (π′||π)[s], we recover (up to assumption-dependent factors) the bounds given by Pirotta et al. (2013) as Corollary 3.6, and by Schulman et al. (2015) as Theorem 1a.\nThe choice of f = V π allows a useful form of the lower bound, so we give it as a corollary. Corollary 1. For any policies π′, π, with π ′ . = maxs |Ea∼π′ [Aπ(s, a)]|, the following bound holds:\nJ(π′)− J(π)\n≥ 1 1− γ\nE s∼dπ a∼π′\n[ Aπ(s, a)− 2γ π′\n1− γ DTV (π\n′||π)[s] ] . (5)\nThe bound (5) should be compared with equation (1). The term (1 − γ)−1Es∼dπ,a∼π′ [Aπ(s, a)] in (5) is an approximation to J(π′)− J(π), using the state distribution dπ instead of dπ ′ , which is known to equal J(π′)− J(π) to first order in the parameters of π′ on a neighborhood around π (Kakade & Langford, 2002). The bound can therefore be viewed as describing the worst-case approximation error, and it justifies using the approximation as a surrogate for J(π′)− J(π).\nEquivalent expressions for the auxiliary costs, based on the upper bound, also follow immediately; we will later use them to make guarantees for the safety of CPO. Corollary 2. For any policies π′, π, and any cost function Ci, with π ′\nCi . = maxs |Ea∼π′ [AπCi(s, a)]|, the follow-\ning bound holds:\nJCi(π ′)− JCi(π)\n≤ 1 1− γ\nE s∼dπ a∼π′\n[ AπCi(s, a) + 2γ π ′ Ci\n1− γ DTV (π\n′||π)[s] ] .\n(6)\nThe bounds we have given so far are in terms of the TV-divergence between policies, but trust region methods constrain the KL-divergence between policies, so bounds that connect performance to the KL-divergence are desirable. We make the connection through Pinsker’s inequality (Csiszar & Körner, 1981): for arbitrary distributions p, q, the TV-divergence and KL-divergence are related by DTV (p||q) ≤ √ DKL(p||q)/2. Combining this with Jensen’s inequality, we obtain\nE s∼dπ\n[DTV (π ′||π)[s]] ≤ E\ns∼dπ\n[√ 1\n2 DKL(π′||π)[s]\n]\n≤ √ 1\n2 E s∼dπ [DKL(π′||π)[s]] (7)\nFrom (7) we immediately obtain the following.\nCorollary 3. In bounds (4), (5), and (6), make the substitution\nE s∼dπ\n[DTV (π ′||π)[s]]→\n√ 1\n2 E s∼dπ [DKL(π′||π)[s]].\nThe resulting bounds hold."
    }, {
      "heading" : "5.2. Trust Region Methods",
      "text" : "Trust region algorithms for reinforcement learning (Schulman et al., 2015; 2016) have policy updates of the form\nπk+1 = arg max π∈Πθ E s∼dπk a∼π\n[Aπk(s, a)]\ns.t. D̄KL(π||πk) ≤ δ, (8)\nwhere D̄KL(π||πk) = Es∼πk [DKL(π||πk)[s]], and δ > 0 is the step size. The set {πθ ∈ Πθ : D̄KL(π||πk) ≤ δ} is called the trust region.\nThe primary motivation for this update is that it is an approximation to optimizing the lower bound on policy performance given in (5), which would guarantee monotonic performance improvements. This is important for optimizing neural network policies, which are known to suffer from performance collapse after bad updates (Duan et al., 2016). Despite the approximation, trust region steps usually give monotonic improvements (Schulman et al., 2015; Duan et al., 2016) and have shown state-of-the-art performance in the deep RL setting (Duan et al., 2016; Gu et al., 2017), making the approach appealing for developing policy search methods for CMDPs.\nUntil now, the particular choice of trust region for (8) was heuristically motivated; with (5) and Corollary 3, we are able to show that it is principled and comes with a worstcase performance degradation guarantee that depends on δ. Proposition 1 (Trust Region Update Performance). Suppose πk, πk+1 are related by (8), and that πk ∈ Πθ. A lower bound on the policy performance difference between πk and πk+1 is\nJ(πk+1)− J(πk) ≥ − √ 2δγ πk+1\n(1− γ)2 , (9)\nwhere πk+1 = maxs ∣∣Ea∼πk+1 [Aπk(s, a)]∣∣.\nProof. πk is a feasible point of (8) with objective value 0, so Es∼dπk ,a∼πk+1 [A\nπk(s, a)] ≥ 0. The rest follows by (5) and Corollary 3, noting that (8) bounds the average KLdivergence by δ.\nThis result is useful for two reasons: 1) it is of independent interest, as it helps tighten the connection between theory and practice for deep RL, and 2) the choice to develop CPO as a trust region method means that CPO inherits this performance guarantee."
    }, {
      "heading" : "5.3. Trust Region Optimization for Constrained MDPs",
      "text" : "Constrained policy optimization (CPO), which we present and justify in this section, is a policy search algorithm for CMDPs with updates that approximately solve (3) with a particular choice of D. First, we describe a policy search update for CMDPs that alleviates the issue of off-policy evaluation, and comes with guarantees of monotonic performance improvement and constraint satisfaction. Then, because the theoretically guaranteed update will take toosmall steps in practice, we propose CPO as a practical approximation based on trust region methods.\nBy corollaries 1, 2, and 3, for appropriate coefficients αk, βik the update\nπk+1 = arg max π∈Πθ E s∼dπk a∼π\n[Aπk(s, a)]− αk √ D̄KL(π||πk)\ns.t. JCi(πk) + E s∼dπk a∼π\n[ AπkCi(s, a)\n1− γ\n] + βik √ D̄KL(π||πk) ≤ di\nis guaranteed to produce policies with monotonically nondecreasing returns that satisfy the original constraints. (Observe that the constraint here is on an upper bound for JCi(π) by (6).) The off-policy evaluation issue is alleviated, because both the objective and constraints involve expectations over state distributions dπk , which we presume to have samples from. Because the bounds are tight, the problem is always feasible (as long as π0 is feasible). However, the penalties on policy divergence are quite steep for discount factors close to 1, so steps taken with this update might be small.\nInspired by trust region methods, we propose CPO, which uses a trust region instead of penalties on policy divergence to enable larger step sizes:\nπk+1 = arg max π∈Πθ E s∼dπk a∼π\n[Aπk(s, a)]\ns.t. JCi(πk) + 1\n1− γ E\ns∼dπk a∼π\n[ AπkCi(s, a) ] ≤ di ∀i\nD̄KL(π||πk) ≤ δ. (10)\nBecause this is a trust region method, it inherits the performance guarantee of Proposition 1. Furthermore, by corollaries 2 and 3, we have a performance guarantee for approximate satisfaction of constraints: Proposition 2 (CPO Update Worst-Case Constraint Violation). Suppose πk, πk+1 are related by (10), and that Πθ in (10) is any set of policies with πk ∈ Πθ. An upper bound on the Ci-return of πk+1 is\nJCi(πk+1) ≤ di + √ 2δγ πk+1 Ci\n(1− γ)2 ,\nwhere πk+1Ci = maxs ∣∣Ea∼πk+1 [AπkCi(s, a)]∣∣."
    }, {
      "heading" : "6. Practical Implementation",
      "text" : "In this section, we show how to implement an approximation to the update (10) that can be efficiently computed, even when optimizing policies with thousands of parameters. To address the issue of approximation and sampling errors that arise in practice, as well as the potential violations described by Proposition 2, we also propose to tighten the constraints by constraining upper bounds of the auxilliary costs, instead of the auxilliary costs themselves."
    }, {
      "heading" : "6.1. Approximately Solving the CPO Update",
      "text" : "For policies with high-dimensional parameter spaces like neural networks, (10) can be impractical to solve directly because of the computational cost. However, for small step sizes δ, the objective and cost constraints are well-approximated by linearizing around πk, and the KLdivergence constraint is well-approximated by second order expansion (at πk = π, the KL-divergence and its gradient are both zero). Denoting the gradient of the objective as g, the gradient of constraint i as bi, the Hessian of the KL-divergence as H , and defining ci . = JCi(πk) − di, the approximation to (10) is:\nθk+1 = arg max θ\ngT (θ − θk)\ns.t. ci + bTi (θ − θk) ≤ 0 i = 1, ...,m 1\n2 (θ − θk)TH(θ − θk) ≤ δ.\n(11) Because the Fisher information matrix (FIM) H is always positive semi-definite (and we will assume it to be positive-definite in what follows), this optimization problem is convex and, when feasible, can be solved efficiently using duality. (We reserve the case where it is not feasible for the next subsection.) With B .= [b1, ..., bm] and c . = [c1, ..., cm] T , a dual to (11) can be expressed as\nmax λ≥0 ν 0\n−1 2λ\n( gTH−1g − 2rT ν + νTSν ) + νT c− λδ\n2 ,\n(12) where r .= gTH−1B, S .= BTH−1B. This is a convex program inm+1 variables; when the number of constraints is small by comparison to the dimension of θ, this is much easier to solve than (11). If λ∗, ν∗ are a solution to the dual, the solution to the primal is\nθ∗ = θk + 1\nλ∗ H−1 (g −Bν∗) . (13)\nOur algorithm solves the dual for λ∗, ν∗ and uses it to propose the policy update (13). For the special case where there is only one constraint, we give an analytical solution in the supplementary material (Theorem 2) which removes the need for an inner-loop optimization. Our experiments\nAlgorithm 1 Constrained Policy Optimization Input: Initial policy π0 ∈ Πθ tolerance α for k = 0, 1, 2, ... do\nSample a set of trajectories D = {τ} ∼ πk = π(θk) Form sample estimates ĝ, b̂, Ĥ, ĉ with D if approximate CPO is feasible then\nSolve dual problem (12) for λ∗k, ν ∗ k\nCompute policy proposal θ∗ with (13) else\nCompute recovery policy proposal θ∗ with (14) end if Obtain θk+1 by backtracking linesearch to enforce satisfaction of sample estimates of constraints in (10)\nend for\nhave only a single constraint, and make use of the analytical solution.\nBecause of approximation error, the proposed update may not satisfy the constraints in (10); a backtracking line search is used to ensure surrogate constraint satisfaction. Also, for high-dimensional policies, it is impractically expensive to invert the FIM. This poses a challenge for computing H−1g and H−1bi, which appear in the dual. Like (Schulman et al., 2015), we approximately compute them using the conjugate gradient method."
    }, {
      "heading" : "6.2. Feasibility",
      "text" : "Due to approximation errors, CPO may take a bad step and produce an infeasible iterate πk. Sometimes (11) will still be feasible and CPO can automatically recover from its bad step, but for the infeasible case, a recovery method is necessary. In our experiments, where we only have one constraint, we recover by proposing an update to purely decrease the constraint value:\nθ∗ = θk − √ 2δ\nbTH−1b H−1b. (14)\nAs before, this is followed by a line search. This approach is principled in that it uses the limiting search direction as the intersection of the trust region and the constraint region shrinks to zero. We give the pseudocode for our algorithm (for the single-constraint case) as Algorithm 1."
    }, {
      "heading" : "6.3. Tightening Constraints via Cost Shaping",
      "text" : "Because of the various approximations between (3) and our practical algorithm, it is important to build a factor of safety into the algorithm to minimize the chance of constraint violations. To this end, we choose to constrain upper bounds on the original constraints, C+i , instead of the original constraints themselves. We do this by cost shaping:\nC+i (s, a, s ′) = Ci(s, a, s ′) + ∆i(s, a, s ′), (15)\nwhere ∆i : S × A × S → R+ correlates in some useful way with Ci.\nIn our experiments, where we have only one constraint, we partition states into safe states and unsafe states, and the agent suffers a safety cost of 1 for being in an unsafe state. We choose ∆ to be the probability of entering an unsafe state within a fixed time horizon, according to a learned model that is updated at each iteration. This choice confers the additional benefit of smoothing out sparse constraints."
    }, {
      "heading" : "7. Connections to Prior Work",
      "text" : "Our method has similar policy updates to primal-dual methods like those proposed by Chow et al. (2015), but crucially, we differ in computing the dual variables (the Lagrange multipliers for the constraints). In primal-dual optimization (PDO), dual variables are stateful and learned concurrently with the primal variables (Boyd et al., 2003). In a PDO algorithm for solving (3), dual variables would be updated according to\nνk+1 = (νk + αk (JC(πk)− d))+ , (16)\nwhere αk is a learning rate. In this approach, intermediary policies are not guaranteed to satisfy constraints—only the policy at convergence is. By contrast, CPO computes new dual variables from scratch at each update to exactly enforce constraints."
    }, {
      "heading" : "8. Experiments",
      "text" : "In our experiments, we aim to answer the following:\n• Does CPO succeed at enforcing behavioral constraints when training neural network policies with thousands of parameters?\n• How does CPO compare with a baseline that uses primal-dual optimization? Does CPO behave better with respect to constraints?\n• How much does it help to constrain a cost upper bound (15), instead of directly constraining the cost?\n• What benefits are conferred by using constraints instead of fixed penalties?\nWe designed experiments that are easy to interpret and motivated by safety. We consider two tasks, and train multiple different agents (robots) for each task:\n• Circle: The agent is rewarded for running in a wide circle, but is constrained to stay within a safe region smaller than the radius of the target circle.\n• Gather: The agent is rewarded for collecting green apples, and constrained to avoid red bombs.\nFor the Circle task, the exact geometry is illustrated in Figure 5 in the supplementary material. Note that there are no physical walls: the agent only interacts with boundaries through the constraint costs. The reward and constraint cost functions are described in supplementary material (Section 10.3.1). In each of these tasks, we have only one constraint; we refer to it as C and its upper bound from (15) as C+.\nWe experiment with three different agents: a point-mass (S ⊆ R9, A ⊆ R2), a quadruped robot (called an ‘ant’) (S ⊆ R32, A ⊆ R8), and a simple humanoid (S ⊆ R102, A ⊆ R10). We train all agent-task combinations except for Humanoid-Gather.\nFor all experiments, we use neural network policies with two hidden layers of size (64, 32). Our experiments are implemented in rllab (Duan et al., 2016)."
    }, {
      "heading" : "8.1. Evaluating CPO and Comparison Analysis",
      "text" : "Learning curves for CPO and PDO are compiled in Figure 1. Note that we evaluate algorithm performance based on the C+ return, instead of the C return (except for in PointGather, where we did not use cost shaping due to that environment’s short time horizon), because this is what the algorithm actually constrains in these experiments.\nFor our comparison, we implement PDO with (16) as the update rule for the dual variables, using a constant learning rate α; details are available in supplementary material (Section 10.3.3). We emphasize that in order for the compari-\nson to be fair, we give PDO every advantage that is given to CPO, including equivalent trust region policy updates. To benchmark the environments, we also include TRPO (trust region policy optimization) (Schulman et al., 2015), a stateof-the-art unconstrained reinforcement learning algorithm. The TRPO experiments show that optimal unconstrained behaviors for these environments are constraint-violating.\nWe find that CPO is successful at approximately enforcing constraints in all environments. In the simpler environments (Point-Circle and Point-Gather), CPO tracks the constraint return almost exactly to the limit value.\nBy contrast, although PDO usually converges to constraintsatisfying policies in the end, it is not consistently constraint-satisfying throughout training (as expected). For example, see the spike in constraint value that it experiences in Ant-Circle. Additionally, PDO is sensitive to the initialization of the dual variable. By default, we initialize ν0 = 0, which exploits no prior knowledge about the\nenvironment and makes sense when the initial policies are feasible. However, it may seem appealing to set ν0 high, which would make PDO more conservative with respect to the constraint; PDO could then decrease ν as necessary after the fact. In the Point environments, we experiment with ν0 = 1000 and show that although this does assure constraint satisfaction, it also can substantially harm performance with respect to return. Furthermore, we argue that this is not adequate in general: after the dual variable decreases, the agent could learn a new behavior that increases the correct dual variable more quickly than PDO can attain it (as happens in Ant-Circle for PDO; observe that performance is approximately constraint-satisfying until the agent learns how to run at around iteration 350).\nWe find that CPO generally outperforms PDO on enforcing constraints, without compromising performance with respect to return. CPO quickly stabilizes the constraint return around to the limit value, while PDO is not consistently able to enforce constraints all throughout training."
    }, {
      "heading" : "8.2. Ablation on Cost Shaping",
      "text" : "In Figure 3, we compare performance of CPO with and without cost shaping in the constraint. Our metric for comparison is theC-return, the ‘true’ constraint. The cost shaping does help, almost completely accounting for CPO’s inherent approximation errors. However, CPO is nearly constraint-satisfying even without cost shaping."
    }, {
      "heading" : "8.3. Constraint vs. Fixed Penalty",
      "text" : "In Figure 4, we compare CPO to a fixed penalty method, where policies are learned using TRPO with rewards R(s, a, s′) + λC+(s, a, s′) for λ ∈ {1, 5, 50}.\nWe find that fixed penalty methods can be highly sensitive to the choice of penalty coefficient: in Ant-Circle, a penalty coefficient of 1 results in reward-maximizing policies that accumulate massive constraint costs, while a coefficient of 5 (less than an order of magnitude difference) results in cost-minimizing policies that never learn how to acquire any rewards. In contrast, CPO automatically picks penalty coefficients to attain the desired trade-off between reward and constraint cost."
    }, {
      "heading" : "9. Discussion",
      "text" : "In this article, we showed that a particular optimization problem results in policy updates that are guaranteed to both improve return and satisfy constraints. This enabled the development of CPO, our policy search algorithm for CMDPs, which approximates the theoretically-guaranteed algorithm in a principled way. We demonstrated that CPO can train neural network policies with thousands of parameters on high-dimensional constrained control tasks, simultaneously maximizing reward and approximately satisfying constraints. Our work represents a step towards applying reinforcement learning in the real world, where constraints on agent behavior are sometimes necessary for the sake of safety."
    }, {
      "heading" : "Acknowledgements",
      "text" : "The authors would like to acknowledge Peter Chen, who independently and concurrently derived an equivalent policy improvement bound.\nJoshua Achiam is supported by TRUST (Team for Research in Ubiquitous Secure Technology) which receives support from NSF (award number CCF-0424422). This project also received support from Berkeley Deep Drive\nand from Siemens."
    }, {
      "heading" : "10. Appendix",
      "text" : ""
    }, {
      "heading" : "10.1. Proof of Policy Performance Bound",
      "text" : ""
    }, {
      "heading" : "10.1.1. PRELIMINARIES",
      "text" : "Our analysis will make extensive use of the discounted future state distribution, dπ , which is defined as\ndπ(s) = (1− γ) ∞∑ t=0 γtP (st = s|π).\nIt allows us to express the expected discounted total reward compactly as\nJ(π) = 1\n1− γ E\ns∼dπ a∼π s′∼P\n[R(s, a, s′)] , (17)\nwhere by a ∼ π, we mean a ∼ π(·|s), and by s′ ∼ P , we mean s′ ∼ P (·|s, a). We drop the explicit notation for the sake of reducing clutter, but it should be clear from context that a and s′ depend on s.\nFirst, we examine some useful properties of dπ that become apparent in vector form for finite state spaces. Let ptπ ∈ R|S| denote the vector with components ptπ(s) = P (st = s|π), and let Pπ ∈ R|S|×|S| denote the transition matrix with components Pπ(s′|s) = ∫ daP (s′|s, a)π(a|s); then ptπ = Pπpt−1π = P tπµ and\ndπ = (1− γ) ∞∑ t=0 (γPπ) tµ\n= (1− γ)(I − γPπ)−1µ. (18)\nThis formulation helps us easily obtain the following lemma.\nLemma 1. For any function f : S → R and any policy π,\n(1− γ) E s∼µ [f(s)] + E s∼dπ a∼π s′∼P [γf(s′)]− E s∼dπ [f(s)] = 0. (19)\nProof. Multiply both sides of (18) by (I − γPπ) and take the inner product with the vector f ∈ R|S|.\nCombining this with (17), we obtain the following, for any function f and any policy π:\nJ(π) = E s∼µ\n[f(s)] + 1\n1− γ E\ns∼dπ a∼π s′∼P\n[R(s, a, s′) + γf(s′)− f(s)] . (20)\nThis identity is nice for two reasons. First: if we pick f to be an approximator of the value function V π , then (20) relates the true discounted return of the policy (J(π)) to the estimate of the policy return (Es∼µ[f(s)]) and to the on-policy average TD-error of the approximator; this is aesthetically satisfying. Second: it shows that reward-shaping by γf(s′)− f(s) has the effect of translating the total discounted return by Es∼µ[f(s)], a fixed constant independent of policy; this illustrates the finding of Ng. et al. (1999) that reward shaping by γf(s′) + f(s) does not change the optimal policy.\nIt is also helpful to introduce an identity for the vector difference of the discounted future state visitation distributions on two different policies, π′ and π. Define the matrices G .= (I − γPπ)−1, Ḡ . = (I − γPπ′)−1, and ∆ = Pπ′ − Pπ . Then:\nG−1 − Ḡ−1 = (I − γPπ)− (I − γPπ′) = γ∆;\nleft-multiplying by G and right-multiplying by Ḡ, we obtain\nḠ−G = γḠ∆G.\nThus\ndπ ′ − dπ = (1− γ) ( Ḡ−G ) µ\n= γ(1− γ)Ḡ∆Gµ = γḠ∆dπ. (21)\nFor simplicity in what follows, we will only consider MDPs with finite state and action spaces, although our attention is on MDPs that are too large for tabular methods."
    }, {
      "heading" : "10.1.2. MAIN RESULTS",
      "text" : "In this section, we will derive and present the new policy improvement bound. We will begin with a lemma:\nLemma 2. For any function f : S → R and any policies π′ and π, define\nLπ,f (π ′) . = E s∼dπ a∼π s′∼P\n[( π′(a|s) π(a|s) − 1 ) (R(s, a, s′) + γf(s′)− f(s)) ] , (22)\nand π ′ f . = maxs |Ea∼π′,s′∼P [R(s, a, s′) + γf(s′)− f(s)]|. Then the following bounds hold:\nJ(π′)− J(π) ≥ 1 1− γ\n( Lπ,f (π ′)− 2 π ′ f DTV (d π′ ||dπ) ) , (23)\nJ(π′)− J(π) ≤ 1 1− γ\n( Lπ,f (π ′) + 2 π ′ f DTV (d π′ ||dπ) ) , (24)\nwhere DTV is the total variational divergence. Furthermore, the bounds are tight (when π′ = π, the LHS and RHS are identically zero).\nProof. First, for notational convenience, let δf (s, a, s′) . = R(s, a, s′) + γf(s′) − f(s). (The choice of δ to denote this quantity is intentionally suggestive—this bears a strong resemblance to a TD-error.) By (20), we obtain the identity\nJ(π′)− J(π) = 1 1− γ  Es∼dπ′ a∼π′ s′∼P [δf (s, a, s ′)]− E s∼dπ a∼π s′∼P [δf (s, a, s ′)] .  Now, we restrict our attention to the first term in this equation. Let δ̄π ′\nf ∈ R|S| denote the vector of components δ̄π ′\nf (s) = Ea∼π′,s′∼P [δf (s, a, s ′)|s]. Observe that\nE s∼dπ ′ a∼π′ s′∼P\n[δf (s, a, s ′)] = 〈 dπ ′ , δ̄π ′\nf\n〉\n= 〈 dπ, δ̄π ′\nf\n〉 + 〈 dπ ′ − dπ, δ̄π ′\nf 〉 This term is then straightforwardly bounded by applying Hölder’s inequality; for any p, q ∈ [1,∞] such that 1/p+1/q = 1, we have 〈\ndπ, δ̄π ′\nf\n〉 + ∥∥∥dπ′ − dπ∥∥∥\np ∥∥∥δ̄π′f ∥∥∥ q ≥ E s∼dπ ′\na∼π′ s′∼P\n[δf (s, a, s ′)] ≥ 〈 dπ, δ̄π ′\nf\n〉 − ∥∥∥dπ′ − dπ∥∥∥\np ∥∥∥δ̄π′f ∥∥∥ q .\nThe lower bound leads to (23), and the upper bound leads to (24).\nWe choose p = 1 and q =∞; however, we believe that this step is very interesting, and different choices for dealing with the inner product 〈 dπ ′ − dπ, δ̄π′f 〉 may lead to novel and useful bounds.\nWith ∥∥∥dπ′ − dπ∥∥∥\n1 = 2DTV (d π′ ||dπ) and ∥∥∥δ̄π′f ∥∥∥∞ = π′f , the bounds are almost obtained. The last step is to observe that,\nby the importance sampling identity,\n〈 dπ, δ̄π ′\nf\n〉 = E\ns∼dπ a∼π′ s′∼P\n[δf (s, a, s ′)]\n= E s∼dπ a∼π s′∼P\n[( π′(a|s) π(a|s) ) δf (s, a, s ′) ] .\nAfter grouping terms, the bounds are obtained.\nThis lemma makes use of many ideas that have been explored before; for the special case of f = V π , this strategy (after bounding DTV (dπ\n′ ||dπ)) leads directly to some of the policy improvement bounds previously obtained by Pirotta et al. and Schulman et al. The form given here is slightly more general, however, because it allows for freedom in choosing f .\nRemark. It is reasonable to ask if there is a choice of f which maximizes the lower bound here. This turns out to trivially be f = V π\n′ . Observe that Es′∼P [δV π′ (s, a, s ′)|s, a] = Aπ′(s, a). For all states, Ea∼π′ [Aπ ′ (s, a)] = 0 (by the definition\nof Aπ ′ ), thus δ̄π ′\nV π′ = 0 and π\n′\nV π′ = 0. Also, Lπ,V π′ (π\n′) = −Es∼dπ,a∼π [ Aπ ′ (s, a) ] ; from (20) with f = V π ′ , we can\nsee that this exactly equals J(π′) − J(π). Thus, for f = V π′ , we recover an exact equality. While this is not practically useful to us (because, when we want to optimize a lower bound with respect to π′, it is too expensive to evaluate V π ′ for each candidate to be practical), it provides insight: the penalty coefficient on the divergence captures information about the mismatch between f and V π ′ .\nNext, we are interested in bounding the divergence term, ‖dπ′ − dπ‖1. We give the following lemma; to the best of our knowledge, this is a new result.\nLemma 3. The divergence between discounted future state visitation distributions, ‖dπ′ −dπ‖1, is bounded by an average divergence of the policies π′ and π:\n‖dπ ′ − dπ‖1 ≤\n2γ\n1− γ E s∼dπ [DTV (π\n′||π)[s]] , (25)\nwhere DTV (π′||π)[s] = (1/2) ∑ a |π′(a|s)− π(a|s)|.\nProof. First, using (21), we obtain\n‖dπ ′ − dπ‖1 = γ‖Ḡ∆dπ‖1\n≤ γ‖Ḡ‖1‖∆dπ‖1.\n‖Ḡ‖1 is bounded by:\n‖Ḡ‖1 = ‖(I − γPπ′)−1‖1 ≤ ∞∑ t=0 γt ‖Pπ′‖t1 = (1− γ) −1\nTo conclude the lemma, we bound ‖∆dπ‖1.\n‖∆dπ‖1 = ∑ s′ ∣∣∣∣∣∑ s ∆(s′|s)dπ(s) ∣∣∣∣∣ ≤\n∑ s,s′ |∆(s′|s)| dπ(s)\n= ∑ s,s′ ∣∣∣∣∣∑ a P (s′|s, a) (π′(a|s)− π(a|s)) ∣∣∣∣∣ dπ(s) ≤\n∑ s,a,s′ P (s′|s, a) |π′(a|s)− π(a|s)| dπ(s)\n= ∑ s,a |π′(a|s)− π(a|s)| dπ(s)\n= 2 E s∼dπ\n[DTV (π ′||π)[s]] .\nThe new policy improvement bound follows immediately. Theorem 1. For any function f : S → R and any policies π′ and π, define δf (s, a, s′) . = R(s, a, s′) + γf(s′)− f(s),\nπ ′ f . = max s |Ea∼π′,s′∼P [δf (s, a, s′)]| ,\nLπ,f (π ′) . = E s∼dπ a∼π s′∼P\n[( π′(a|s) π(a|s) − 1 ) δf (s, a, s ′) ] , and\nD±π,f (π ′) . = Lπ,f (π\n′)\n1− γ ±\n2γ π ′\nf\n(1− γ)2 E s∼dπ [DTV (π\n′||π)[s]] ,\nwhere DTV (π′||π)[s] = (1/2) ∑ a |π′(a|s)− π(a|s)| is the total variational divergence between action distributions at s. The following bounds hold: D+π,f (π ′) ≥ J(π′)− J(π) ≥ D−π,f (π ′). (4)\nFurthermore, the bounds are tight (when π′ = π, all three expressions are identically zero).\nProof. Begin with the bounds from lemma 2 and bound the divergence DTV (dπ ′ ||dπ) by lemma 3."
    }, {
      "heading" : "10.2. Proof of Analytical Solution to LQCLP",
      "text" : "Theorem 2 (Optimizing Linear Objective with Linear and Quadratic Constraints). Consider the problem\np∗ = min x gTx\ns.t. bTx+ c ≤ 0 (26) xTHx ≤ δ,\nwhere g, b, x ∈ Rn, c, δ ∈ R, δ > 0, H ∈ Sn, and H 0. When there is at least one strictly feasible point, the optimal point x∗ satisfies\nx∗ = − 1 λ∗ H−1 (g + ν∗b) ,\nwhere λ∗ and ν∗ are defined by\nν∗ =\n( λ∗c− r\ns ) + ,\nλ∗ = arg max λ≥0\n{ fa(λ) . = 12λ ( r2 s − q ) + λ2 ( c2 s − δ ) − rcs if λc− r > 0\nfb(λ) . = − 12 ( q λ + λδ ) otherwise,\nwith q = gTH−1g, r = gTH−1b, and s = bTH−1b.\nFurthermore, let Λa . = {λ|λc− r > 0, λ ≥ 0}, and Λb . = {λ|λc− r ≤ 0, λ ≥ 0}. The value of λ∗ satisfies\nλ∗ ∈ { λ∗a . = Proj (√ q − r2/s δ − c2/s ,Λa ) , λ∗b . = Proj (√ q δ ,Λb )} ,\nwith λ∗ = λ∗a if fa(λ ∗ a) > fb(λ ∗ b) and λ ∗ = λ∗b otherwise, and Proj(a, S) is the projection of a point x on to a set S. Note: the projection of a point x ∈ R onto a convex segment of R, [a, b], has value Proj(x, [a, b]) = max(a,min(b, x)).\nProof. This is a convex optimization problem. When there is at least one strictly feasible point, strong duality holds by Slater’s theorem. We exploit strong duality to solve the problem analytically.\np∗ = min x\nmax λ≥0 ν≥0\ngTx+ λ\n2\n( xTHx− δ ) + ν ( bTx+ c ) = max\nλ≥0 ν≥0\nmin x\nλ 2 xTHx+ (g + νb) T x+\n( νc− 1\n2 λδ\n) Strong duality\n=⇒ x∗ = − 1 λ H−1 (g + νb) ∇xL(x, λ, ν) = 0\n= max λ≥0 ν≥0\n− 1 2λ (g + νb) T H−1 (g + νb) +\n( νc− 1\n2 λδ\n) Plug in x∗\n= max λ≥0 ν≥0\n− 1 2λ\n( q + 2νr + ν2s ) + ( νc− 1\n2 λδ\n) Notation: q .= gTH−1g, r .= gTH−1b, s .= bTH−1b.\n=⇒ ∂L ∂ν = − 1 2λ (2r + 2νs) + c\n=⇒ ν = ( λc− r s ) +\nOptimizing single-variable convex quadratic function over R+\n= max λ≥0\n{ 1\n2λ\n( r2 s − q ) + λ2 ( c2 s − δ ) − rcs if λ ∈ Λa\n− 12 ( q λ + λδ ) if λ ∈ Λb\nNotation: Λa\n. = {λ|λc− r > 0, λ ≥ 0},\nΛb . = {λ|λc− r ≤ 0, λ ≥ 0}\nObserve that when c < 0, Λa = [0, r/c) and Λb = [r/c,∞); when c > 0, Λa = [r/c,∞) and Λb = [0, r/c).\nNotes on interpreting the coefficients in the dual problem:\n• We are guaranteed to have r2/s− q ≤ 0 by the Cauchy-Schwarz inequality. Recall that q = gTH−1g, r = gTH−1b, s = bTH−1b. The Cauchy-Scwarz inequality gives:\n‖H−1/2b‖22‖H−1/2g‖22 ≥ (( H−1/2b )T ( H−1/2g ))2\n=⇒ ( bTH−1b ) ( gTH−1g ) ≥ ( bTH−1g )2 ∴ qs ≥ r2.\n• The coefficient c2/s−δ relates to whether or not the plane of the linear constraint intersects the quadratic trust region. An intersection occurs if there exists an x such that c+ bTx = 0 with xTHx ≤ δ. To check whether this is the case, we solve\nx∗ = arg min x xTHx : c+ bTx = 0 (27)\nand see if x∗THx∗ ≤ δ. The solution to this optimization problem is x∗ = cH−1b/s, thus x∗THx∗ = c2/s. If c2/s− δ ≤ 0, then the plane intersects the trust region; otherwise, it does not.\nIf c2/s− δ > 0 and c < 0, then the quadratic trust region lies entirely within the linear constraint-satisfying halfspace, and we can remove the linear constraint without changing the optimization problem. If c2/s − δ > 0 and c > 0, the problem is infeasible (the intersection of the quadratic trust region and linear constraint-satisfying halfspace is empty). Otherwise, we follow the procedure below.\nSolving the dual for λ: for any A > 0, B > 0, the problem\nmax λ≥0\nf(λ) . = −1\n2\n( A\nλ +Bλ ) has optimal point λ∗ = √ A/B and optimal value f(λ∗) = − √ AB.\nWe can use this solution form to obtain the optimal point on each segment of the piecewise continuous dual function for λ:\nobjective optimal point (before projection) optimal point (after projection)\nfa(λ) . =\n1\n2λ\n( r2 s − q ) + λ 2 ( c2 s − δ ) − rc s λa . = √ q − r2/s δ − c2/s\nλ∗a = Proj(λa,Λa)\nfb(λ) . = −1\n2 ( q λ + λδ )\nλb . =\n√ q\nδ λ∗b = Proj(λb,Λb)\nThe optimization is completed by comparing fa(λ∗a) and fb(λ ∗ b):\nλ∗ =\n{ λ∗a fa(λ ∗ a) ≥ fb(λ∗b)\nλ∗b otherwise."
    }, {
      "heading" : "10.3. Experimental Parameters",
      "text" : ""
    }, {
      "heading" : "10.3.1. ENVIRONMENTS",
      "text" : "In the Circle environments, the reward and cost functions are\nR(s) = vT [−y, x]\n1 + |‖[x, y]‖2 − d| ,\nC(s) = 1 [|x| > xlim] ,\nwhere x, y are the coordinates in the plane, v is the velocity, and d, xlim are environmental parameters. We set these parameters to be\nPoint-mass Ant Humanoid d 15 10 10\nxlim 2.5 3 2.5\nIn Point-Gather, the agent receives a reward of +10 for collecting an apple, and a cost of 1 for collecting a bomb. Two apples and eight bombs spawn on the map at the start of each episode. In Ant-Gather, the reward and cost structure was the same, except that the agent also receives a reward of −10 for falling over (which results in the episode ending). Eight apples and eight bombs spawn on the map at the start of each episode."
    }, {
      "heading" : "10.3.2. ALGORITHM PARAMETERS",
      "text" : "In all experiments, we use Gaussian policies with mean vectors given as the outputs of neural networks, and with variances that are separate learnable parameters. The policy networks for all experiments have two hidden layers of sizes (64, 32) with tanh activation functions.\nWe use GAE-λ (Schulman et al., 2016) to estimate the advantages and constraint advantages, with neural network value functions. The value functions have the same architecture and activation functions as the policy networks. We found that having different λGAE values for the regular advantages and the constraint advantages worked best. We denote the λGAE used for the constraint advantages as λGAEC .\nFor the failure prediction networks Pφ(s→ U), we use neural networks with a single hidden layer of size (32), with output of one sigmoid unit. At each iteration, the failure prediction network is updated by some number of gradient descent steps using the Adam update rule to minimize the prediction error. To reiterate, the failure prediction network is a model for the probability that the agent will, at some point in the next T time steps, enter an unsafe state. The cost bonus was weighted by a coefficient α, which was 1 in all experiments except for Ant-Gather, where it was 0.01. Because of the short time horizon, no cost bonus was used for Point-Gather.\nFor all experiments, we used a discount factor of γ = 0.995, a GAE-λ for estimating the regular advantages of λGAE = 0.95, and a KL-divergence step size of δKL = 0.01.\nExperiment-specific parameters are as follows:\nParameter Point-Circle Ant-Circle Humanoid-Circle Point-Gather Ant-Gather Batch size 50,000 100,000 50,000 50,000 100,000\nRollout length 50-65 500 1000 15 500 Maximum constraint value d 5 10 10 0.1 0.2 Failure prediction horizon T 5 20 20 (N/A) 20 Failure predictor SGD steps per itr 25 25 25 (N/A) 10 Predictor coeff α 1 1 1 (N/A) 0.01\nλGAEC 1 0.5 0.5 1 0.5\nNote that these same parameters were used for all algorithms.\nWe found that the Point environment was agnostic to λGAEC , but for the higher-dimensional environments, it was necessary to set λGAEC to a value < 1. Failing to discount the constraint advantages led to substantial overestimates of the constraint gradient magnitude, which led the algorithm to take unsafe steps. The choice λGAEC = 0.5 was obtained by a hyperparameter search in {0.5, 0.92, 1}, but 0.92 worked nearly as well."
    }, {
      "heading" : "10.3.3. PRIMAL-DUAL OPTIMIZATION IMPLEMENTATION",
      "text" : "Our primal-dual implementation is intended to be as close as possible to our CPO implementation. The key difference is that the dual variables for the constraints are stateful, learnable parameters, unlike in CPO where they are solved from scratch at each update.\nThe update equations for our PDO implementation are\nθk+1 = θk + s j\n√ 2δ\n(g − νkb)TH−1(g − νkb) H−1 (g − νkb)\nνk+1 = (νk + α (JC(πk)− d))+ ,\nwhere sj is from the backtracking line search (s ∈ (0, 1) and j ∈ {0, 1, ..., J}, where J is the backtrack budget; this is the same line search as is used in CPO and TRPO), and α is a learning rate for the dual parameters. α is an important hyperparameter of the algorithm: if it is set to be too small, the dual variable won’t update quickly enough to meaningfully enforce the constraint; if it is too high, the algorithm will overcorrect in response to constraint violations and behave too conservatively. We experimented with a relaxed learning rate, α = 0.001, and an aggressive learning rate, α = 0.01. The aggressive learning rate performed better in our experiments, so all of our reported results are for α = 0.01.\nSelecting the correct learning rate can be challenging; the need to do this is obviated by CPO."
    } ],
    "references" : [ {
      "title" : "Constrained Markov Decision Processes",
      "author" : [ "Altman", "Eitan" ],
      "venue" : "pp. 260,",
      "citeRegEx" : "Altman and Eitan.,? \\Q1999\\E",
      "shortCiteRegEx" : "Altman and Eitan.",
      "year" : 1999
    }, {
      "title" : "Safe Policy Search for Lifelong Reinforcement Learning with Sublinear Regret",
      "author" : [ "Bou Ammar", "Haitham", "Tutunov", "Rasul", "Eaton", "Eric" ],
      "venue" : "International Conference on Machine Learning,",
      "citeRegEx" : "Ammar et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Ammar et al\\.",
      "year" : 2015
    }, {
      "title" : "Subgradient methods",
      "author" : [ "Boyd", "Stephen", "Xiao", "Lin", "Mutapcic", "Almir" ],
      "venue" : "Lecture Notes of Stanford EE392,",
      "citeRegEx" : "Boyd et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Boyd et al\\.",
      "year" : 2003
    }, {
      "title" : "Risk-Constrained Reinforcement Learning with Percentile Risk Criteria",
      "author" : [ "Chow", "Yinlam", "Ghavamzadeh", "Mohammad", "Janson", "Lucas", "Pavone", "Marco" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Chow et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Chow et al\\.",
      "year" : 2015
    }, {
      "title" : "Information Theory: Coding Theorems for Discrete Memoryless Systems",
      "author" : [ "I Csiszar", "J. Körner" ],
      "venue" : "Book, 244:452,",
      "citeRegEx" : "Csiszar and Körner,? \\Q1981\\E",
      "shortCiteRegEx" : "Csiszar and Körner",
      "year" : 1981
    }, {
      "title" : "Benchmarking Deep Reinforcement Learning for Continuous Control",
      "author" : [ "Duan", "Yan", "Chen", "Xi", "Schulman", "John", "Abbeel", "Pieter" ],
      "venue" : "The 33rd International Conference on Machine Learning (ICML",
      "citeRegEx" : "Duan et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Duan et al\\.",
      "year" : 2016
    }, {
      "title" : "A Comprehensive Survey on Safe Reinforcement Learning",
      "author" : [ "Garcı́a", "Javier", "Fernández", "Fernando" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Garcı́a et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Garcı́a et al\\.",
      "year" : 2015
    }, {
      "title" : "Q-Prop: Sample-Efficient Policy Gradient with An Off-Policy Critic",
      "author" : [ "Gu", "Shixiang", "Lillicrap", "Timothy", "Ghahramani", "Zoubin", "Turner", "Richard E", "Levine", "Sergey" ],
      "venue" : "In International Conference on Learning Representations,",
      "citeRegEx" : "Gu et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Gu et al\\.",
      "year" : 2017
    }, {
      "title" : "Probabilistically Safe Policy Transfer",
      "author" : [ "Held", "David", "Mccarthy", "Zoe", "Zhang", "Michael", "Shentu", "Fred", "Abbeel", "Pieter" ],
      "venue" : "In Proceedings of the IEEE International Conference on Robotics and Automation (ICRA),",
      "citeRegEx" : "Held et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Held et al\\.",
      "year" : 2017
    }, {
      "title" : "Doubly Robust Off-policy Value Evaluation for Reinforcement Learning",
      "author" : [ "Jiang", "Nan", "Li", "Lihong" ],
      "venue" : "International Conference on Machine Learning,",
      "citeRegEx" : "Jiang et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Jiang et al\\.",
      "year" : 2015
    }, {
      "title" : "Approximately Optimal Approximate Reinforcement Learning",
      "author" : [ "Kakade", "Sham", "Langford", "John" ],
      "venue" : "Proceedings of the 19th International Conference on Machine Learning,",
      "citeRegEx" : "Kakade et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Kakade et al\\.",
      "year" : 2002
    }, {
      "title" : "End-to-End Training of Deep Visuomotor Policies",
      "author" : [ "Levine", "Sergey", "Finn", "Chelsea", "Darrell", "Trevor", "Abbeel", "Pieter" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Levine et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Levine et al\\.",
      "year" : 2016
    }, {
      "title" : "Continuous control with deep reinforcement learning",
      "author" : [ "Lillicrap", "Timothy P", "Hunt", "Jonathan J", "Pritzel", "Alexander", "Heess", "Nicolas", "Erez", "Tom", "Tassa", "Yuval", "Silver", "David", "Wierstra", "Daan" ],
      "venue" : "In International Conference on Learning Representations,",
      "citeRegEx" : "Lillicrap et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Lillicrap et al\\.",
      "year" : 2016
    }, {
      "title" : "Combating Deep Reinforcement Learning’s Sisyphean Curse with Intrinsic Fear",
      "author" : [ "Lipton", "Zachary C", "Gao", "Jianfeng", "Li", "Lihong", "Chen", "Jianshu", "Deng" ],
      "venue" : "In arXiv,",
      "citeRegEx" : "Lipton et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Lipton et al\\.",
      "year" : 2004
    }, {
      "title" : "Human-level control through deep reinforcement learning",
      "author" : [ "Dharshan", "Wierstra", "Daan", "Legg", "Shane", "Hassabis", "Demis" ],
      "venue" : "Nature, 518(7540):529–533,",
      "citeRegEx" : "Dharshan et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Dharshan et al\\.",
      "year" : 2015
    }, {
      "title" : "Asynchronous Methods for Deep Reinforcement Learning",
      "author" : [ "Mnih", "Volodymyr", "Badia", "Adrià Puigdomènech", "Mirza", "Mehdi", "Graves", "Alex", "Lillicrap", "Timothy P", "Harley", "Tim", "Silver", "David", "Kavukcuoglu", "Koray" ],
      "venue" : "URL http://arxiv.org/abs/1602",
      "citeRegEx" : "Mnih et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Mnih et al\\.",
      "year" : 2016
    }, {
      "title" : "Safe Exploration in Markov Decision Processes",
      "author" : [ "Moldovan", "Teodor Mihai", "Abbeel", "Pieter" ],
      "venue" : "Proceedings of the 29th International Conference on Machine Learning,",
      "citeRegEx" : "Moldovan et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Moldovan et al\\.",
      "year" : 2012
    }, {
      "title" : "Reinforcement learning of motor skills with policy gradients",
      "author" : [ "Peters", "Jan", "Schaal", "Stefan" ],
      "venue" : "Neural Networks,",
      "citeRegEx" : "Peters et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Peters et al\\.",
      "year" : 2008
    }, {
      "title" : "Safe Policy Iteration",
      "author" : [ "Pirotta", "Matteo", "Restelli", "Marcello", "Calandriello", "Daniele" ],
      "venue" : "Proceedings of the 30th International Conference on Machine Learning,",
      "citeRegEx" : "Pirotta et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Pirotta et al\\.",
      "year" : 2013
    }, {
      "title" : "Trust Region Policy Optimization",
      "author" : [ "Schulman", "John", "Moritz", "Philipp", "Jordan", "Michael", "Abbeel", "Pieter" ],
      "venue" : "International Conference on Machine Learning,",
      "citeRegEx" : "Schulman et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Schulman et al\\.",
      "year" : 2015
    }, {
      "title" : "High-Dimensional Continuous Control Using Generalized Advantage Estimation",
      "author" : [ "Schulman", "John", "Moritz", "Philipp", "Levine", "Sergey", "Jordan", "Michael", "Abbeel", "Pieter" ],
      "venue" : null,
      "citeRegEx" : "Schulman et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Schulman et al\\.",
      "year" : 2016
    }, {
      "title" : "Safe, Multi-Agent, Reinforcement Learning for Autonomous Driving. arXiv, 2016",
      "author" : [ "Shalev-Shwartz", "Shai", "Shammah", "Shaked", "Shashua", "Amnon" ],
      "venue" : "URL http: //arxiv.org/abs/1610.03295",
      "citeRegEx" : "Shalev.Shwartz et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Shalev.Shwartz et al\\.",
      "year" : 2016
    }, {
      "title" : "Mastering the game of Go with deep neural networks and tree search",
      "author" : [ "Sutskever", "Ilya", "Lillicrap", "Timothy", "Leach", "Madeleine", "Kavukcuoglu", "Koray", "Graepel", "Thore", "Hassabis", "Demis" ],
      "venue" : null,
      "citeRegEx" : "Sutskever et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Sutskever et al\\.",
      "year" : 2016
    }, {
      "title" : "Introduction to Reinforcement Learning",
      "author" : [ "Sutton", "Richard S", "Barto", "Andrew G" ],
      "venue" : "ISSN 10743529",
      "citeRegEx" : "Sutton et al\\.,? \\Q1996\\E",
      "shortCiteRegEx" : "Sutton et al\\.",
      "year" : 1996
    }, {
      "title" : "Constrained reinforcement learning from intrinsic and extrinsic rewards",
      "author" : [ "Uchibe", "Eiji", "Doya", "Kenji" ],
      "venue" : "IEEE 6th International Conference on Development and Learning, ICDL,",
      "citeRegEx" : "Uchibe et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Uchibe et al\\.",
      "year" : 2007
    } ],
    "referenceMentions" : [ {
      "referenceID" : 15,
      "context" : "Recent advances in policy search algorithms (Mnih et al., 2016; Schulman et al., 2015; Lillicrap et al., 2016; Levine et al., 2016) have enabled new capabilities in highdimensional control, but do not consider the constrained setting.",
      "startOffset" : 44,
      "endOffset" : 131
    }, {
      "referenceID" : 19,
      "context" : "Recent advances in policy search algorithms (Mnih et al., 2016; Schulman et al., 2015; Lillicrap et al., 2016; Levine et al., 2016) have enabled new capabilities in highdimensional control, but do not consider the constrained setting.",
      "startOffset" : 44,
      "endOffset" : 131
    }, {
      "referenceID" : 12,
      "context" : "Recent advances in policy search algorithms (Mnih et al., 2016; Schulman et al., 2015; Lillicrap et al., 2016; Levine et al., 2016) have enabled new capabilities in highdimensional control, but do not consider the constrained setting.",
      "startOffset" : 44,
      "endOffset" : 131
    }, {
      "referenceID" : 11,
      "context" : "Recent advances in policy search algorithms (Mnih et al., 2016; Schulman et al., 2015; Lillicrap et al., 2016; Levine et al., 2016) have enabled new capabilities in highdimensional control, but do not consider the constrained setting.",
      "startOffset" : 44,
      "endOffset" : 131
    }, {
      "referenceID" : 19,
      "context" : ", 2015; 2016), robot locomotion and manipulation (Schulman et al., 2015; Levine et al., 2016; Lillicrap et al., 2016), and even Go at the human grandmaster level (Silver et al.",
      "startOffset" : 49,
      "endOffset" : 117
    }, {
      "referenceID" : 11,
      "context" : ", 2015; 2016), robot locomotion and manipulation (Schulman et al., 2015; Levine et al., 2016; Lillicrap et al., 2016), and even Go at the human grandmaster level (Silver et al.",
      "startOffset" : 49,
      "endOffset" : 117
    }, {
      "referenceID" : 12,
      "context" : ", 2015; 2016), robot locomotion and manipulation (Schulman et al., 2015; Levine et al., 2016; Lillicrap et al., 2016), and even Go at the human grandmaster level (Silver et al.",
      "startOffset" : 49,
      "endOffset" : 117
    }, {
      "referenceID" : 15,
      "context" : "Currently, policy search algorithms enjoy state-of-theart performance on high-dimensional control tasks (Mnih et al., 2016; Duan et al., 2016).",
      "startOffset" : 104,
      "endOffset" : 142
    }, {
      "referenceID" : 5,
      "context" : "Currently, policy search algorithms enjoy state-of-theart performance on high-dimensional control tasks (Mnih et al., 2016; Duan et al., 2016).",
      "startOffset" : 104,
      "endOffset" : 142
    }, {
      "referenceID" : 3,
      "context" : "Heuristic algorithms for policy search in CMDPs have been proposed (Uchibe & Doya, 2007), and approaches based on primal-dual methods can be shown to converge to constraint-satisfying policies (Chow et al., 2015), but there is currently no approach for policy search in continuous CMDPs that guarantees every policy during learning will satisfy constraints.",
      "startOffset" : 193,
      "endOffset" : 212
    }, {
      "referenceID" : 18,
      "context" : "This result, which is of independent interest, tightens known bounds for policy search using trust regions (Kakade & Langford, 2002; Pirotta et al., 2013; Schulman et al., 2015), and provides a tighter connection between the theory and practice of policy search for deep RL.",
      "startOffset" : 107,
      "endOffset" : 177
    }, {
      "referenceID" : 19,
      "context" : "This result, which is of independent interest, tightens known bounds for policy search using trust regions (Kakade & Langford, 2002; Pirotta et al., 2013; Schulman et al., 2015), and provides a tighter connection between the theory and practice of policy search for deep RL.",
      "startOffset" : 107,
      "endOffset" : 177
    }, {
      "referenceID" : 1,
      "context" : "Bou Ammar et al. (2015) propose a theoretically-motivated policy gradient method for lifelong learning with safety constraints, but their method involves an expensive inner loop optimization of a semi-definite program, making it unsuited for the deep RL setting.",
      "startOffset" : 4,
      "endOffset" : 24
    }, {
      "referenceID" : 1,
      "context" : "Bou Ammar et al. (2015) propose a theoretically-motivated policy gradient method for lifelong learning with safety constraints, but their method involves an expensive inner loop optimization of a semi-definite program, making it unsuited for the deep RL setting. Their method also assumes that safety constraints are linear in policy parameters, which is limiting. Chow et al. (2015) propose a primal-dual subgradient method for risk-constrained reinforcement learning which takes policy gradient steps on an objective that trades off return with risk, while simultaneously learning the trade-off coefficients (dual variables).",
      "startOffset" : 4,
      "endOffset" : 384
    }, {
      "referenceID" : 8,
      "context" : "Held et al. (2017) study the problem for robotic manipulation, but the assumptions they make restrict the applicability of their methods.",
      "startOffset" : 0,
      "endOffset" : 19
    }, {
      "referenceID" : 8,
      "context" : "Held et al. (2017) study the problem for robotic manipulation, but the assumptions they make restrict the applicability of their methods. Lipton et al. (2017) use an ‘intrinsic fear’ heuristic, as opposed to constraints, to motivate agents to avoid rare but catastrophic events.",
      "startOffset" : 0,
      "endOffset" : 159
    }, {
      "referenceID" : 8,
      "context" : "Held et al. (2017) study the problem for robotic manipulation, but the assumptions they make restrict the applicability of their methods. Lipton et al. (2017) use an ‘intrinsic fear’ heuristic, as opposed to constraints, to motivate agents to avoid rare but catastrophic events. Shalev-Shwartz et al. (2016) avoid the problem of enforcing constraints on parametrized policies by decomposing ‘desires’ from trajectory planning; the neural network policy learns desires for behavior, while the trajectory planning algorithm (which is not learned) selects final behavior and enforces safety constraints.",
      "startOffset" : 0,
      "endOffset" : 308
    }, {
      "referenceID" : 19,
      "context" : "When the objective is estimated by linearizing around πk as J(πk) + g (θ − θk), g is the policy gradient, and the standard policy gradient update is obtained by choosing D(π, πk) = ‖θ − θk‖2 (Schulman et al., 2015).",
      "startOffset" : 191,
      "endOffset" : 214
    }, {
      "referenceID" : 5,
      "context" : "When using sampling to compute policy updates, as is typically done in high-dimensional control (Duan et al., 2016), this requires off-policy evaluation, which is known to be challenging (Jiang & Li, 2015).",
      "startOffset" : 96,
      "endOffset" : 115
    }, {
      "referenceID" : 19,
      "context" : "In this work, we take a different approach, motivated by recent methods for trust region optimization (Schulman et al., 2015).",
      "startOffset" : 102,
      "endOffset" : 125
    }, {
      "referenceID" : 18,
      "context" : "This result, which is of independent interest, extends the works of (Kakade & Langford, 2002), (Pirotta et al., 2013), and (Schulman et al.",
      "startOffset" : 95,
      "endOffset" : 117
    }, {
      "referenceID" : 19,
      "context" : ", 2013), and (Schulman et al., 2015), providing tighter bounds.",
      "startOffset" : 13,
      "endOffset" : 36
    }, {
      "referenceID" : 5,
      "context" : "As we show later, it also relates the theoretical bounds for trust region policy improvement with the actual trust region algorithms that have been demonstrated to be successful in practice (Duan et al., 2016).",
      "startOffset" : 190,
      "endOffset" : 209
    }, {
      "referenceID" : 18,
      "context" : "to get a second factor of maxsDTV (π′||π)[s], we recover (up to assumption-dependent factors) the bounds given by Pirotta et al. (2013) as Corollary 3.",
      "startOffset" : 114,
      "endOffset" : 136
    }, {
      "referenceID" : 18,
      "context" : "to get a second factor of maxsDTV (π′||π)[s], we recover (up to assumption-dependent factors) the bounds given by Pirotta et al. (2013) as Corollary 3.6, and by Schulman et al. (2015) as Theorem 1a.",
      "startOffset" : 114,
      "endOffset" : 184
    }, {
      "referenceID" : 19,
      "context" : "Trust region algorithms for reinforcement learning (Schulman et al., 2015; 2016) have policy updates of the form",
      "startOffset" : 51,
      "endOffset" : 80
    }, {
      "referenceID" : 5,
      "context" : "This is important for optimizing neural network policies, which are known to suffer from performance collapse after bad updates (Duan et al., 2016).",
      "startOffset" : 128,
      "endOffset" : 147
    }, {
      "referenceID" : 19,
      "context" : "Despite the approximation, trust region steps usually give monotonic improvements (Schulman et al., 2015; Duan et al., 2016) and have shown state-of-the-art performance in the deep RL setting (Duan et al.",
      "startOffset" : 82,
      "endOffset" : 124
    }, {
      "referenceID" : 5,
      "context" : "Despite the approximation, trust region steps usually give monotonic improvements (Schulman et al., 2015; Duan et al., 2016) and have shown state-of-the-art performance in the deep RL setting (Duan et al.",
      "startOffset" : 82,
      "endOffset" : 124
    }, {
      "referenceID" : 5,
      "context" : ", 2016) and have shown state-of-the-art performance in the deep RL setting (Duan et al., 2016; Gu et al., 2017), making the approach appealing for developing policy search methods for CMDPs.",
      "startOffset" : 75,
      "endOffset" : 111
    }, {
      "referenceID" : 7,
      "context" : ", 2016) and have shown state-of-the-art performance in the deep RL setting (Duan et al., 2016; Gu et al., 2017), making the approach appealing for developing policy search methods for CMDPs.",
      "startOffset" : 75,
      "endOffset" : 111
    }, {
      "referenceID" : 19,
      "context" : "Like (Schulman et al., 2015), we approximately compute them using the conjugate gradient method.",
      "startOffset" : 5,
      "endOffset" : 28
    }, {
      "referenceID" : 2,
      "context" : "In primal-dual optimization (PDO), dual variables are stateful and learned concurrently with the primal variables (Boyd et al., 2003).",
      "startOffset" : 114,
      "endOffset" : 133
    }, {
      "referenceID" : 2,
      "context" : "Our method has similar policy updates to primal-dual methods like those proposed by Chow et al. (2015), but crucially, we differ in computing the dual variables (the Lagrange multipliers for the constraints).",
      "startOffset" : 84,
      "endOffset" : 103
    }, {
      "referenceID" : 5,
      "context" : "Our experiments are implemented in rllab (Duan et al., 2016).",
      "startOffset" : 41,
      "endOffset" : 60
    }, {
      "referenceID" : 19,
      "context" : "To benchmark the environments, we also include TRPO (trust region policy optimization) (Schulman et al., 2015), a stateof-the-art unconstrained reinforcement learning algorithm.",
      "startOffset" : 87,
      "endOffset" : 110
    } ],
    "year" : 2017,
    "abstractText" : "For many applications of reinforcement learning it can be more convenient to specify both a reward function and constraints, rather than trying to design behavior through the reward function. For example, systems that physically interact with or around humans should satisfy safety constraints. Recent advances in policy search algorithms (Mnih et al., 2016; Schulman et al., 2015; Lillicrap et al., 2016; Levine et al., 2016) have enabled new capabilities in highdimensional control, but do not consider the constrained setting. We propose Constrained Policy Optimization (CPO), the first general-purpose policy search algorithm for constrained reinforcement learning with guarantees for near-constraint satisfaction at each iteration. Our method allows us to train neural network policies for high-dimensional control while making guarantees about policy behavior all throughout training. Our guarantees are based on a new theoretical result, which is of independent interest: we prove a bound relating the expected returns of two policies to an average divergence between them. We demonstrate the effectiveness of our approach on simulated robot locomotion tasks where the agent must satisfy constraints motivated by safety.",
    "creator" : "LaTeX with hyperref package"
  }
}