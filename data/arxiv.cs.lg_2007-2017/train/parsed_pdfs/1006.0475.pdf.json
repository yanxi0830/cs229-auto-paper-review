{
  "name" : "1006.0475.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Prediction with Advice of Unknown Number of Experts",
    "authors" : [ "Alexey Chernov" ],
    "emails" : [ "chernov@cs.rhul.ac.uk", "vovk@cs.rhul.ac.uk" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n00 6.\n04 75\nv1 [\ncs .L\nG ]\n2 J\nIn the framework of prediction with expert advice, we consider a recently introduced kind of regret bounds: the bounds that depend on the effective instead of nominal number of experts. In contrast to the NormalHedge bound, which mainly depends on the effective number of experts and also weakly depends on the nominal one, we obtain a bound that does not contain the nominal number of experts at all. We use the defensive forecasting method and introduce an application of defensive forecasting to multivalued supermartingales."
    }, {
      "heading" : "1 Introduction",
      "text" : "We consider the problem of prediction with expert advice (PEA) and its variant, decision-theoretic online learning (DTOL). In the PEA framework (see [3] for details, references and historical notes), at each step Learner gets decisions (also called predictions) of several Experts and must make his own decision. Then the environment generates an outcome and a (real-valued) loss is calculated for each decision as a known function of decision and outcome. The difference between cumulative losses of Learner and one of Experts is the regret to this Expert. Learner aims at minimizing his regret to Experts, for any sequence of Expert decisions and outcomes.\nIn DTOL, introduced in [8], Learner’s decision is a probability distribution on a finite set of actions. Then each action incurs a loss (the vector of the losses can be regarded as the outcome), and Learner suffers the loss equal to the expected loss over all actions (according to the probabilities from his decision). The regret is the difference between the cumulative losses of Learner and one of the actions. One can interpret each action as a rigid Expert that always suggests this action. A precise connection between the DTOL and PEA frameworks will be described in Section 2.\nUsually Learner is required to have small regret to all Experts. In other words, a strategy for Learner must have a guaranteed upper bound on Learner’s regret to the best Expert (one with the minimal loss). In this paper we deal with another kind of bound, recently introduced in [4]. It captures the following\nintuition. Generally speaking, the more Experts (or actions, in the DTOL terminology) Learner must take into account, the worse his performance will be. However, assume that each Expert has several different names, so Learner is given a lot of identical advice. It seems natural that the loss of Learner is big if there is a real controversy between Experts (or a real difference between actions), and small if most of the Experts agree with each other. So a competent regret bound should depend on the real number of Experts instead of the nominal one. Another example: assume that all the actions are different, but many of them are good — there are many ways to achieve some goal. Then Learner has less space to make a mistake and to select a bad action. Again it seems that a competent regret bound should depend on the fraction of the good actions rather than the nominal number of actions.\nIf the effective number of actions (Experts) is significantly less than the nominal one, one can loosely say that the number of actions is unknown in this setting. The following regret bound obtained in [4] for their NormalHedge algorithm holds for this case:\nLT ≤ LǫT +O ( √ T ln 1\nǫ + ln2 N\n)\n, (1)\nwhere N is the nominal number of actions, LT is the cumulative loss of Learner after step T and LǫT is the value such that at least ǫ-fraction of actions have smaller or equal cumulative loss after step T (or LǫT can be interpreted as the loss of ǫN -th best action). It is important that the bound holds uniformly for all ǫ and T and the algorithm does not need to know them in advance. The number 1ǫ plays the role of the effective number of actions. The bound shows, in a sense, that the NormalHedge algorithm can work even if the number of actions is not known.\nOur main result (Theorem 9) is the following bound for a new algorithm:\nLT ≤ LǫT + 2 √ T ln 1\nǫ + 7\n√ T .\nThis bound is also uniform in T and ǫ. In contrast to (1), our bound does not depend on the nominal number of actions, whereas (1) contains a term O(ln2 N). So it is the first (as far as we know) bound strictly in terms of the effective number of actions. Our bound has a simpler structure, but it is generally incomparable to the (precise) bound for Normal Hedge from [4] (see Subsection 4.2 for discussion of different known bounds). Also our bound can be easily adapted to internal regret (see [12] for definition). We describe the application to internal regret in Subsection 4.3.\nOur bound is obtained with the help of the defensive forecasting method (DF). The DF is based on bounding the growth of some supermartingale (a kind of potential function). In [5], the DF was used to obtain bounds of the form LT ≤ cLnT + a, where c and a are some constants. For our form of bounds, we need a new variation of the DF and a new sort of supermartingales. So we introduce the notion of multivalued supermartingale and prove a boundedness result for them (Lemmas 2 and 3). (This result is of certain independent interest: for example, it helps to get rid of additional Assumption 3 in Theorem 3 in [5].)\nThe paper is organized as follows. In Section 2 we describe the setup of prediction with expert advice and of decision-theoretic framework online learn-\ning, and define the ǫ-quantile regret. In Section 3 we describe the Defensive Forecasting Algorithm, define multivalued supermartingales and discuss their properties, and introduce supermartingales of a specific form that are based on Hoeffding inequality. In Subsection 4.1 we prove two loss bounds on the ǫquantile regret, and in Subsection 4.2 we compare them with the bound for the NormalHedge algorithm and with other known bounds. In Subsection 4.3 we show how these bounds can be transformed into bounds on the internal regret. In the last subsection we describe a toy example of an algorithm that guarantees bounds for two very different loss functions simultaneously."
    }, {
      "heading" : "2 Notation and Setup",
      "text" : "Vectors with coordinates p1, . . . , pN are denoted by an arrow over the letter: ~p = (p1, . . . , pN ). For any natural N , by ∆N we denote the standard simplex in R N : ∆N = {~p ∈ [0, 1]N | ∑N n=1 pn = 1}. By ~p · ~q we denote the scalar product: ~p · ~q = ∑Nn=1 pnqn.\nProtocol 1 Decision-theoretic framework for learning\nL0 := 0. Ln0 := 0, n = 1, . . . , N . for t = 1, 2, . . . do Learner announces ~γt ∈ ∆N . Reality announces ~ωt ∈ [0, 1]N . Lt := Lt−1 + ~γt · ~ωt. Lnt := L n t−1 + ωt,n, n = 1, . . . , N . end for\nThe decision-theoretic framework for online learning (DTOL) was introduced in [8]. DTOL protocol is given as Protocol 1. The Learner has N available actions, and at each step t he must assign probability weights γt,1, . . . , γt,N to these actions. Then each action suffers a loss ωt,n, and Learner’s loss is the expected loss over all actions according to the weights he assigned. Learner’s goal is to keep small his regret Rnt = Lt − Lnt to any action n, independent of the losses.\nProtocol 2 Prediction with Expert Advice\nL0 := 0. Ln0 := 0, n = 1, . . . , N . for t = 1, 2, . . . do Expert n announces γnt ∈ Γ, n = 1, . . . , N . Learner announces γt ∈ Γ. Reality announces ωt ∈ Ω. Lt := Lt−1 + λ(γt, ωt). Lnt := L n t−1 + λ(γ n t , ωt), n = 1, . . . , N . end for\nDTOL can be regarded as a special case of prediction with expert advice (PEA), as explained below. The PEA protocol is given as Protocol 2. The\ngame is specified by the set of outcomes Ω, the set of decisions Γ and the loss function λ : Γ × Ω → R. The game is played repeatedly by Learner having access to decisions made by a pool of Experts. At each step, Learner is given N Experts’ decisions and is required to come out with his own decision. The loss λ(γ, ω) measures the discrepancy between the decision γ and the outcome ω. Lt is Learner’s cumulative loss over the first t steps, and Lnt is the n-th Expert’s cumulative loss over the first t steps. The goal of Learner is the same: to keep small his regret Rnt = Lt −Lnt to any Expert n, independent of Experts’ moves and the outcomes.\nAs defined in [4] (for DTOL), the regret to the top ǫ-quantile (at step T ) is the value RǫT such that there are at least ǫN actions (the fraction at least ǫ of all Experts) with RnT ≥ RǫT . Or, equivalently, RǫT = LT − LǫT where LǫT is a value such that at least ǫN actions (the fraction at least ǫ of all Experts) has the loss LnT less than L ǫ T .\nA uniform bound on RǫT (in other words, a bound on Learner’s loss LT in terms of LǫT ) that holds for all ǫ is more general than the standard best Expert bounds. The latter can be obtained as a special case for ǫ = 1/N . For this reason, it is natural to call the value 1/ǫ the effective number of actions: a bound on RǫT can be considered as the best Expert bound in an imaginary game against 1/ǫ Experts.\nLet us say what games (Ω,Γ,Λ) we consider in this paper. For any game (Ω,Γ, λ), we call Λ = {g ∈ RΩ | ∃γ ∈ Γ ∀ω ∈ Ω g(ω) = λ(γ, ω)} the prediction set. The prediction set captures most of the information about the game. The prediction set is assumed to be non-empty. In this paper, we consider bounded convex compact games only. This means that we assume that the set Λ is bounded and compact, and the superprediction set Λ + [0,∞]Ω is convex, that is, for any g1, . . . , gK ∈ Λ and for any p1, . . . , pK ∈ [0, 1]K , ∑K k=1 pk = 1, there exists g ∈ Λ such that g(ω) ≤ ∑Kk=1 pkgk(ω) for all ω ∈ Ω. For such games, we assume without loss of generality that Λ ⊆ [0, 1]Ω (we always can scale the loss function).\nFor DTOL as a special case of PEA, the outcome space is Ω = [0, 1]N , the decision space is Γ = ∆N , and the loss function is λ(~γ, ~ω) = ~γ · ~ω. Experts play fixed strategies always choosing ~γnt such that γ n t,n = 1 and γ n t,k = 0 for k 6= n (see e. g. [13, Example 7] for more details about this game). In an important sense the general PEA protocol for the bounded convex games is equivalent to DTOL. Obviously, if some upper bound on regret is achievable in any PEA game then it is achievable in the special case of the DTOL game. To see how to transfer an upper bound from DTOL to a PEA game, let us interpret the decisions γnt of Experts and the outcome ωt in the PEA game as the outcome ~ω′t in DTOL: ω ′ t,n = λ(γ n t , ωt). If Learner’s decision γt satisfies λ(γt, ωt) ≤ ∑N n=1 γ ′ t,nλ(γ n t , ωt), where ~γ ′ t is Learner’s decision in DTOL, then the regret (at step t) in the PEA game will be not greater than the regret in DTOL. It remains to note that, since the game is convex, for any ~γ′t there exists γt such that λ(γt, ω) ≤ ∑N n=1 γ ′ t,nλ(γ n t , ω), for any ω ∈ Ω.\nHowever, the equivalence between DTOL and PEA is limited. In particular, we can obtain PEA bounds that hold for specific loss functions or classes of loss functions (such as mixable loss functions [13]), and these bounds may be much stronger than the general bounds induced by DTOL.\nIn this paper, we consider PEA and DTOL in parallel for another reason.\nIt is sometimes useful to consider a more general variant of Protocol 2 where the number of Experts is infinite (and maybe uncountably infinite): then PEA can be applied to large families of functions as Experts. With the help of our method, we can cope either with DTOL, where the number of actions is finite, or with PEA when Ω is finite and the number of Experts is arbitrary. So we cannot infer a bound for infinitely many Experts from a DTOL result, but we can obtain a PEA result directly. In the sequel, we will write about N experts, but always allow N to be infinite in the PEA case.\nMost of the presentation below is in the terms of PEA but applicable to DTOL as well. We normally hide the difference between PEA and DTOL behind the common notation (DTOL is considered as the game described above). When the difference is important, we give two parallel fragments of a statement or proof."
    }, {
      "heading" : "3 Defensive Forecasting and Supermartingales",
      "text" : "This section contains the technical results we need to construct our prediction algorithm. They are used in the proofs but not in the theorem statements and discussions in the next section."
    }, {
      "heading" : "3.1 Defensive Forecasting",
      "text" : "The general structure of the Defensive Forecasting Algorithm (DFA) is quite simple. At step t, we define a function ft : Γ×Ω → R (with special properties — see below) and look for γ ∈ Γ such that\n∀ω ∈ Ω ft(γ, ω) ≤ ft−1(γt−1, ωt−1) , (2)\nwhere ft−1 is the function defined at the previous step, γt−1 is Learner’s decision at the previous step, and ωt−1 is the outcome at the previous step. Then γ with this property is announced as the next decision of Learner γt.\nThe choice of ft may depend on all the previous decisions, outcomes, and on this step Experts’ decisions (for PEA), so ft = F({γn1 }Nn=1, γ1, ω1, . . . , {γnt }Nn=1). Having specified F , we call this strategy of Learner an application of the DFA to F .\nThe algorithm guarantees that the values of ft do not increase, in particular, after each step the value ft(γt, ωt) is not greater than some initial value f0. We will choose F so that the inequality F({γn1 }Nn=1, γ1, ω1, . . . , {γnt }Nn=1)(γt, ωt) ≤ f0 implies a loss bound we need.\nAlso we need to guarantee that the algorithm always can find γ satisfying (2). To this end we will choose F so that the sequence ft will be a (multivalued) supermartingale as defined in the next subsection."
    }, {
      "heading" : "3.2 Multivalued Supermartingales",
      "text" : "Let Ω be a compact metric space. Any finite set Ω is considered as a metric space with the discrete metric. Let P(Ω) be the space of all measures on Ω supplied with the weak topology.\nFor any measurable function g ∈ RΩ and any π ∈ P(Ω), denote\nEπg =\n∫\nΩ\ng(ω)π(dω) .\nFor finite Ω, this definition reduces to the scalar product:\nEπg = ∑\nω∈Ω g(ω)π(ω) .\nLet S be an operator that to any sequence e1, π1, ω1, . . . , eT−1, πT−1, ωT−1, eT , where ωt ∈ Ω, πt ∈ P(Ω), t = 1, . . . , T − 1, and et, t = 1, . . . , T are some arbitrary values, assigns a function ST : P(Ω) → RΩ. To simplify notation, we will hide the dependence of ST on all the long argument sequence in the index T . We call S a (game-theoretic) supermartingale if for any sequence of arguments, for any π ∈ P(Ω), for gT−1 = ST−1(πT−1) and for g = ST (π) it holds\nEπg ≤ gT−1(ωT−1) . (3)\nThis definition of supermartingale is equivalent to the one given in [5]. We say that supermartingale S is forecast-continuous if every ST is a continuous function.\nThe main property of forecast-continuous supermartingales that makes them useful in our context is given by Lemma 1. Originally, a variant of the lemma was obtained by Leonid Levin in 1976. The proof is based on fixed-point considerations, see [10, Theorem 16.1] or [6, Lemma 8] for details.\nLemma 1. Let Ω be a compact metric space. Let a function q : P(Ω)×Ω → R be continuous as function from P(Ω) to RΩ. If for all π ∈ P(Ω) it holds that\nEπq(π, ·) ≤ C ,\nwhere C ∈ R is some constant, then\n∃π ∈ P(Ω)∀ω ∈ Ω q(π, ω) ≤ C .\nThe lemma guarantees that for any forecast-continuous supermartingale S we can always choose gt ∈ St such that gt(ω) ≤ gt−1(ωt−1) for all ω. This is exactly the kind of condition we need for the DFA.\nUnfortunately, for the loss bounds we want to obtain, we did not find a suitable forecast-continuous supermartingale. So we define a more general notion of multivalued supermartingale, and prove an appropriate variant of Levin’s lemma.\nTo get the definition of a multivalued supermartingale, we make just three changes in the definition of supermartingale above: operator S depends additionally on gt ∈ St(πt); ST is function from P(Ω) to non-empty subsets of R\nΩ; the condition (3) holds for any g ∈ ST (π). Namely, let S be an operator that to any sequence e1, π1, g1, ω1, . . . , eT−1, πT−1, gT−1, ωT−1, eT , where ωt ∈ Ω, πt ∈ P(Ω), gt ∈ RΩ, t = 1, . . . , T − 1, and et, t = 1, . . . , T are some arbitrary values, assigns a function ST : P(Ω) → 2R Ω\nsuch that ST (π) is a nonempty subset of RΩ for all π ∈ P(Ω). S is called a multivalued supermartingale\nif for any sequence of arguments where gt ∈ St(πt), for any π ∈ P(Ω), ST (π) 6= ∅ and for all g ∈ ST (π) it holds\nEπg ≤ gT−1(ωT−1) . (4)\nA multivalued supermartingale is called forecast-continuous if for every ST , the set {(π, g) | π ∈ P(Ω), g ∈ ST (π)} is closed and additionally for every π ∈ P(Ω) the set ST (π)+[0,∞]Ω = {g ∈ RΩ | ∃g′ ∈ ST (π)∀ω ∈ Ωg′(ω) ≤ g(ω)} is convex.\nNote that if S is a forecast-continuous multivalued supermartingale and St(π) always consists of exactly one element, S is (equivalent to) a forecastcontinuous supermartingale in the former sense: closedness of the graph of ST means that ST (π) is a continuous function of π and the convexity requirement becomes trivial."
    }, {
      "heading" : "3.3 Levin’s Lemma for Multivalued Supermartingales",
      "text" : "Here we prove two version of Levin’s lemma suitable for multivalued supermartingales. The first variant (it is simpler) will be used for PEA with finite outcome set Ω. The second variant will be used for DTOL.\nLemma 2. Let Ω be a finite set. Let X be a compact subset of RΩ. Let q ⊆ P(Ω) × X be a relation. Denote q(π) = {g | (π, g) ∈ q} and ran q = ∪π∈P(Ω)q(π) ⊆ X. Suppose that q is closed, for every π ∈ P(Ω) the set q(π) is non-empty and the set q(π) + [0,∞]Ω is convex. If for some real constant C it holds that for every π ∈ P(Ω)\n∀g ∈ q(π) Eπg ≤ C ,\nthen there exists g ∈ ran q such that\n∀ω ∈ Ω g(ω) ≤ C .\nWe derive the lemma from Lemma 1 similarly to the derivation of Kakutani’s fixed point theorem for multi-valued mappings (see, e. g. [1, Theorem 11.9]) from Brouwer’s fixed point theorem. Unfortunately, we did not find a way just to refer to Kakutani’s theorem and have to repeat the whole construction with appropriate changes.\nProof. Note first that P(Ω) is compact for finite Ω, hence q is compact as a closed subset of a compact set. Let Mq = maxg∈ran q,ω∈Ω |g(ω)|.\nFor every natural m > 0, let us take any (1/m)-net {πmk } on P(Ω) such that for every π ∈ P(Ω) there is at least one net element πmk at the distance less than 1/m from π and for every π ∈ P(Ω) there are at most 4|Ω|2 elements of the net at the distances less than 1/m from π. (One can use here any reasonable distance on P(Ω), for example, the maximum absolute value of the coordinates of the difference.) For every πmk in the net, fix any g m k ∈ q(πmk ) (recall that q(πmk ) is non-empty). Now let us define a function qm : P(Ω)×Ω → R as a linear interpolation of the points (πmk , g m k ). Namely, let {umk } be a partition of unity of P(Ω) subordinate to U1/m(π m k ), the (1/m)-neighborhoods of π m k (that is, u m k (π) are non-negative,\numk (π) = 0 if the distance between π and π m k is 1/m or more, and the sum over k of all umk (π) is 1 at any π). Let q m(π, ω) = ∑ k u m k (π)g m k (ω).\nThe function qm is forecast-continuous. Let us find an upper bound on Eπq m(π, ·):\nEπq m(π, ·) =\n∑\nk\numk (π)Eπg m k\n= ∑\nk\numk (π)Eπmk g m k +\n∑\nk\numk (π) ∑ ω∈Ω (π(ω)− πmk (ω))gmk (ω) ≤ C +Mq|Ω|/m\n(the bound on the first term holds since gmk ∈ q(πmk ) and hence Eπmk gmk ≤ C). By Lemma 1 we can find a point πm ∈ P(Ω) such that\n∀ω ∈ Ω qm(πm, ω) ≤ C +Mq|Ω|/m .\nRecalling that qm(πm, ω) = ∑ k u m k (π m)gmk (ω) and that there are at most 4|Ω|2 non-zero values among umk (π\nm), we get the following statement. There exist some αmk ≥ 0, k = 1, . . . , 4|Ω|2, ∑ k α m k = 1, and some g m k ∈ q(πmk ) with πmk at the distance at most 1/m from πm such that\n∀ω ∈ Ω 4|Ω|2 ∑\nk=1\nαmk g m k (ω) ≤ C +Mq|Ω|/m . (5)\nSince P(Ω) is compact, we can find a limit point π∗ of πm. It will be a limit point of πmk as well. Since q is compact, we can find g ∗ k ∈ q(π∗) such that (π∗, g∗k) are limit points of (π m k , g m k ) for each k. Finally, since P({1, . . . , 4|Ω|2}) is compact, we can find limit points α∗k (corresponding to the points g ∗ k).\nTaking the limits as m → ∞ over the convergent subsequences in (5), we get\n∀ω ∈ Ω 4|Ω|2 ∑\nk=1\nα∗kg ∗ k(ω) ≤ C .\nSince q(π∗)+ [0,∞]Ω is convex, the convex combination ∑4|Ω| 2 k=1 α ∗ kg ∗ k belongs to q(π∗)+[0,∞]Ω. In other words, the combination is minorized by some g∗ ∈ q(π∗) and\ng∗(ω) ≤ 4|Ω|2 ∑\nk=1\nα∗kg ∗ k(ω) ≤ C\nfor all ω ∈ Ω.\nNow let us prove a variant of the lemma suitable for the DTOL framework, where the set of outcomes is infinite. Here we make a strong assumption: the supermartingale values ST (π) depend on π in a very limited way: just on the mean of π.\nLemma 3. Let Ω be [0, 1]N . Let X be a compact subset of RΩ. Let q ⊆ P(Ω)×X be a relation. Denote q(π) = {g | (π, g) ∈ q} and ran q = ∪π∈P(Ω)q(π) ⊆ X. Assume that if ∫ ωπ1(dω) = ∫ ωπ2(dω) then q(π1) = q(π2). Suppose that q is\nclosed, for every π ∈ P(Ω) the set q(π) is non-empty and the set q(π) + [0,∞]Ω is convex. If for some real constant C it holds that for every π ∈ P(Ω)\n∀g ∈ q(π) Eπg ≤ C , then there exists g ∈ ran q such that\n∀ω ∈ Ω g(ω) ≤ C . Proof. Since [0, 1]N is a compact metric space, the space P([0, 1]N) with weak topology is compact too (see, e. g. [10, Prop. B.28]). Hence q is compact as a closed subset of a compact set. Let Mq = maxg∈ran q,ω∈Ω |g(ω)|.\nWe consider P(Ω) as a metric space with Wasserstein distance W (π, π′) = supf |Eπf − Eπ′f |, where the supremum is taken over 1-Lipschitz functions (see [10, Def. B.20]). For every natural m > 0, let us construct a (1/m)-net {πmk } on P(Ω) with the following property. Let ωmi be a (1/(2m))-net on Ω such that at most 4N2 its elements are at the distance less than 1/(2m) from any ω ∈ Ω. For any πmk there exists ωmi such that ∫ ωπmk (dω) = ω m i . (A net with this property exists: note that for any π there is a π′ at the distance at most 1/(2m) such that ∫\nωπ′(dω) = ωmi ; it remains to consider a cover of 1/(2m)-neighborhoods centered in all π with the given expected values.) For every πmk in the net, let us take any g m k ∈ q(πmk ).\nNow let us define a function qm : P(Ω)×Ω → R as a linear interpolation of the points (πmk , g m k ). Namely, let {umk } be a partition of unity of P(Ω) subordinate to U1/m(π m k ), the (1/m)-neighborhoods of π m k (that is, u m k (π) are non-negative, umk (π) = 0 if the distance between π and π m k is 1/m or more, and the sum over k of all umk (π) is 1 at any π). Let q m(π, ω) = ∑ k u m k (π)g m k (ω).\nThe function qm is forecast-continuous. Let us find an upper bound on Eπq m(π, ·):\nEπq m(π, ·) =\n∑\nk\numk (π)Eπg m k\n= ∑\nk\numk (π)Eπmk g m k +\n∑\nk\numk (π)\n( ∫\nΩ\ngmk (ω)π(dω) − ∫\nΩ\ngmk (ω)π m k (dω)\n)\n≤ C +Mq/m (the bound on the first term holds since gmk ∈ q(πmk ) and hence Eπmk gmk ≤ C).\nBy Lemma 1 we can find a point πm ∈ P(Ω) such that ∀ω ∈ Ω qm(πm, ω) ≤ C +Mq/m .\nAmong πmk such that u m k (π m) is non-zero, there are at most 4N2 different expected values. Let us group all gmk corresponding to π m k with a certain expected value. They belong to the same set q(πmk ), thus their convex combination is minorized by another element g̃mi of the same set. Thus we arrive at the following statement: there are some αmi ≥ 0, i = 1, . . . , 4N2, ∑ i α m i = 1, and some g̃mi ∈ q(πmi ) with πmi at the distance at most 1/m from πm such that\n∀ω ∈ Ω 4N2 ∑\ni=1\nαmi g̃ m i (ω) ≤ C +Mq/m . (6)\nThe rest of the proof is the same as in Lemma 2."
    }, {
      "heading" : "3.4 Hoeffding Supermartingale",
      "text" : "Here we introduce a specific multivalued supermartingale, or rather a family of supermartingales, that will be used for our main results.\nFor technical convenience, our definition of supermartingale St consists of two parts: a function G : P(Ω) → 2Γ, which assigns a set of decisions G(π) ⊆ Γ to every π ∈ P(Ω), and a function ft : Γ×Ω → R. The values of St are defined by the formula:\nSt(π) = {g ∈ RΩ | ∃γ ∈ G(π)∀ω ∈ Ω g(ω) = ft(γ, ω)} . (7)\nThe part G(π) depends on the game (Ω,Γ, λ) only and does not change from step to step:\nG(π) = argmin γ∈Γ\nEπλ(γ, ·) = {γ ∈ Γ | ∀γ′ ∈ Γ Eπλ(γ, ·) ≤ Eπλ(γ′, ·)} . (8)\nLemma 4. Let (Ω,Γ, λ) be a game such that its prediction set Λ = {g ∈ RΩ | ∃γ ∈ Γ ∀ω ∈ Ω g(ω) = λ(γ, ω)} is a non-empty compact subset of RΩ and Λ + [0,∞]Ω is convex. Then the set\nGΛ = {(π, g) ∈ P(Ω)× Λ | ∃γ ∈ G(π)∀ω ∈ Ω g(ω) = λ(γ, ω)}\nis closed and for every π ∈ P(Ω) the sets G(π) and GΛ(π) = {g | (π, g) ∈ GΛ} are non-empty and the sets GΛ(π) + [0,∞]Ω are convex. Proof. Since Λ is non-empty and compact, the minimum of Eπg is attained for every π, and hence G(π) and also GΛ(π) is non-empty.\nAssume that g1, g2 ∈ GΛ(π) ⊆ Λ and α ∈ [0, 1]. Then αg1+(1−α)g2 ≥ g ∈ Λ since Λ + [0,∞]Ω is convex, and Eπg ≤ Eπ(αg1 + (1 − α)g2) = Eπg1 = Eπg2. Hence g ∈ GΛ(π) and thus GΛ(π) + [0,∞]Ω is convex.\nIt remains to show that GΛ is closed. Let gi ∈ GΛ(πi) and (πi, gi) converges to (π, g); we need to show that g ∈ GΛ(π). Indeed, g ∈ Λ since Λ is compact and gi → g. Hence g = λ(γ, ·) for some γ ∈ Γ. To show that γ ∈ G(π), let us take any γ′ ∈ Γ and check that Eπg ≤ Eπg′, where g′ = λ(γ′, ·). Clearly, Eπig′ converges to Eπg\n′ since πi → π. Also Eπigi converges to Eπg. Then for any ǫ > 0 we can find sufficiently large i so that Eπg ≤ Eπigi+ǫ and Eπig′ ≤ Eπg′+ǫ. We have Eπigi ≤ Eπig′ since gi ∈ L(πi). These three inequalities imply that Eπg ≤ Eπg′ + 2ǫ. Since ǫ is arbitrary, we have Eπg ≤ Eπg′.\nNote that for convex bounded compact games the conditions of the lemma are satisfied by definition. For DTOL, the set Λ = {g ∈ R[0,1]N | ∃~p ∈ ∆N∀~ω ∈ [0, 1]N g(ω) = ~p · ~ω} is obviously non-empty and it is compact and convex as a linear image of simplex ∆N .\nNow consider a function H : Γ× Ω → R of the form\nH(γ, ω) = eη(λ(γ,ω)−λ(γ ′,ω))−η2/2 , (9)\nwhere parameter γ′ ∈ Γ and η ≥ 0. Lemma 5. Let (Ω,Γ, λ) be a game, the range of λ is included in [0, 1] and G(π) is defined by (8). Then for all γ′ ∈ Γ, for all η ≤ 0, for all π ∈ P(Ω), and for all γ ∈ G(π) it holds\nEπe η(λ(γ,·)−λ(γ′,·))−η2/2 ≤ 1 .\nProof. Since λ(γ, ω) − λ(γ′, ω) ∈ [−1, 1] for any γ, γ′ and ω, the Hoeffding inequality (see e. g. [3, Lemma A.1]) implies that\nEπe η(λ(γ,·)−λ(γ′,·)) ≤ eηEπ(λ(γ,·)−λ(γ′,·))+η2/2 .\nIt remains to note that Eπ(λ(γ, ·)− λ(γ′, ·)) ≤ 0 by definition of G(π).\nNow we can explain what ft will be used in (7):\nft(γ, ω) =\nK ∑\nk=1\npt,kHt,k(γ, ω) , (10)\nwhere pt,k ≥ 0 are some weights and Ht,k are functions of the form (9) with some parameters ηt,k and γt,k, cf. (11), (15), (18), (21), (22), and (23). The sum may be infinite or it can be even an integral over some measure pt(k). As in the definition of supermartingale, the index t may hide the dependence on a long sequence of arguments.\nLemma 6. St defined by (7), (8), and (10) satisfies the conditions of Lemma 2 if (Ω,Γ, λ) is a bounded convex compact game with finite Ω or the conditions of Lemma 3 if (Ω,Γ, λ) is DTOL, where St(π) is taken for q(π) and ∑K\nk=1 pt,k is taken for C.\nProof. If g ∈ St(π) then g = ft(γ, ·) for some γ ∈ G(π). Thus we have Eπg = ∑K\nk=1 pt,kEπHt,k(γ, ·) ≤ ∑K\nk=1 pt,k by Lemma 5. Clearly, St(π) ⊆ Λ and Λ is compact, as remarked after Lemma 4. The set\nSt(π) is non-empty since G(π) is non-empty by Lemma 4.\nLet φt(g) = ∑K k=1 pt,kEπe ηt,k(g−λ(γ′t,k,·))−η2t,k/2. Note that g ∈ GΛ(π) if and only if φ(g) ∈ St(π). Note also that φt is a convex (and hence continuous) function of g. Thus, the graph of St is closed since GΛ closed and St(π)+[0,∞]Ω is convex since GΛ(π) + [0,∞]Ω is convex.\nThe condition St(π1) = St(π2) when ∫ ωπ1(dω) = ∫\nωπ2(dω) for DTOL follows from the equality Eπλ(~γ, ~ω) = Eπ(~γ · ~ω) = ~γ · Eπ~ω."
    }, {
      "heading" : "4 Loss Bounds",
      "text" : "In this section, we consider applications of the supermartingale technique to obtaining the loss bounds in several different settings. Let us begin with a simple theorem that shows a clean application of the DFA.\nTheorem 7. If T is known in advance then the DFA achieves the bound\nLT ≤ min n\nLnT + √ 2T lnN\n(for DTOL with N actions as well as for PEA with N experts).\nProof. Let η = √ 2(lnN)/T and\nft(γ, ω) =\nN ∑\nn=1\n1\nN eη(Lt−1−L n t−1)−η2/2 × eη(λ(γ,ω)−λ(γnt ,ω))−η2/2 . (11)\nAt each step t, the DFA finds γt such that ft(γt, ω) ≤ ft−1(γt−1, ωt−1) for all ω ∈ Ω. Such a γt exists due to Lemma 6 combined with Lemma 3 for DTOL or Lemma 2 for PEA. Clearly, ft−1(γt−1, ωt−1) = ∑N n=1 1 N exp(η(Lt−1 − Lnt−1) − η2/2), and we get that the DFA applied to {ft} guarantees that\nft(γT , ωT ) =\nN ∑\nn=1\n1\nN eη(LT−L n T )−η2/2 ≤ 1 .\nBounding the sum from below by one additive term, we get the bound.\nThis bound is twice as large as the best bound obtained in [3] (see their Theorems 2.2 and 3.7). Our bound is the same as that in Corollary 2.2 in [3]."
    }, {
      "heading" : "4.1 Bounds on ǫ-Quantile Regret",
      "text" : "The bound in Theorem 7 is guaranteed only once, at step T specified in advance. The next bound is uniform, that is, holds for any T , and it holds for ǫ-quantile regret for all ǫ > 0.\nTheorem 8. For DTOL with N actions, the DFA achieves the bound\n∫ 1/e\n0\ne(LT−L ǫ T )η−Tη2/2 dη\nη (\nln 1η\n)2 ≤ 1\nǫ , (12)\nfor any T and any ǫ > 0, where LǫT is a value such that at least ǫ-fraction of actions has the loss after step T not greater than LǫT . In particular, (12) implies for any δ ∈ (0, 1/4)\nLT ≤ LǫT + 2√ 2− δ\n√\nT ln 1\nǫ +\n1 2 T ln 1 δ + 2T ln lnT +max\n{\n4, 400 ln 1\nǫ\n}\n, (13)\nwhich can be further reduced to\nRǫT ≤ ( 1 + 1\nlnT\n)\n√\n2T ln 1\nǫ + 5T ln lnT +O\n(\nln 1\nǫ\n)\n. (14)\nThe bound holds also for PEA; if each of finitely or infinitely many Experts is assigned some positive weight pn, the sum of all weights being not greater than 1, the DFA achieves (12)–(14) with LǫT being a value such that the total weight of Experts that have the loss after step T not greater than LǫT is at least ǫ. Proof. We mix all the supermartingales used in (11) over η ∈ [0, 1/e] according to the probability measure\nµ(dη) = dη\nη (\nln 1η\n)2 , η ∈ [0, 1/e] .\nWe apply the DFA (that is, at each step t, find γt such that ft(γt, ω) ≤ ft−1(γt−1, ωt−1) for all ω ∈ Ω) to\nft(γ, ω) =\nN ∑\nn=1\n1\nN\n∫ 1/e\n0\ndη\nη (\nln 1η\n)2 e η(Lt−1−Lnt−1)−η2/2 × eη(λ(γ,ω)−λ(γnt ,ω))−η2/2\n(15)\n(for PEA with weighed Experts, the term 1/N is replaced by pn) and achieve fT (γT , ωT ) ≤ 1 for all T . Bounding the sum from below by the sum of terms where LnT ≤ LǫT , we get\n∫ 1/e\n0\neη(LT−L ǫ T )−Tη2/2 dη\nη (\nln 1η\n)2 ≤ 1\nǫ . (16)\nLet us estimate the integral. Notice that the exponent Rη−Tη2/2 is positive when 0 ≤ η ≤ 2R/T and attains its maximum R2/(2T ) at the mid-point of this interval, η = R/T . Solving the quadratic inequality\nRη − Tη2/2 ≥ (1/2− δ)R2/T\ngives\nη ∈ [ R\nT\n( 1− √ 2δ ) , R\nT\n( 1 + √ 2δ ) ]\n(0 < δ < 1/2) and so (16) implies\ne(1/2−δ)R 2/T ln(1 +\n√ 2δ)− ln(1− √ 2δ)\n(ln(T/R)− ln(1 + √ 2δ))(ln(T/R)− ln(1− √ 2δ)) ≤ 1 ǫ\nwhen ( 1 + √ 2δ )\nR/T ≤ 1/e. If the last condition does not hold and hence R is close to T , one can get from (16) that T < 400 ln(1/ǫ). Assuming δ < 1/4, we can obtain\ne(2−δ)R 2/T ≤ 1\nǫ √ 2δ\nln2 4T\nR .\nFor R ≥ 4, we further obtain\n(2− δ)R2/T ≤ ln 1 ǫ + 1 2 ln 1 δ + 2 ln lnT ,\nwhich finally leads to (13). Substituting δ = 1/ lnT , we get (14).\nThe bound (14) is not optimal asymptotically in T : it grows as O( √ T ln lnT )\nas T → ∞, instead of O( √ T ). The next theorem gives an asymptotically optimal bound but using a “fake” DFA.\nTheorem 9. For DTOL with N actions, there exists a strategy that achieves the bound\nLT ≤ LǫT + 2 √ T ln 1\nǫ + 7\n√ T (17)\nfor any T and any ǫ, where LǫT is a value such that at least ǫ-fraction of actions has the loss after step T not greater than LǫT . The bound holds also for PEA; if each of finitely or infinitely many Experts is assigned some positive weight pn, the sum of all weights being not greater than 1, the strategy achieves (17) with LǫT being a value such that the total weight of Experts that have the loss after step T not greater than LǫT is at least ǫ.\nProof. The algorithm in this theorem is not the DFA and does not use supermartingales properly: we use values ft(γt, ωt) that may increase at some steps and ft(γt, ωt) ≤ ft−1(γt−1, ωt−1) does not hold. Nevertheless, the increases of ft stay bounded so that it always holds ft(γt, ωt) ≤ 1.\nLet 1/c = ∑∞\ni=1 1 i2 . At step T , our algorithm finds γT such that fT (γT , ω) ≤\nCT for all ω, where\nfT (γ, ω) =\nN ∑\nn=1\n1\nN\n∞ ∑\ni=1\nc i2 e(i/\n√ T)(LT−1−LnT−1)−(i/2 √ T) ∑T−1 t=1 (i/ √ t)\n× e(i/ √ T)(λ(γ,ω)−λ(γnT ,ω))−(i/ √ T)2/2 (18)\nand\nCT =\nN ∑\nn=1\n1\nN\n∞ ∑\ni=1\nc i2 e(i/\n√ T)(LT−1−LnT−1)−(i/2 √ T) ∑T−1 t=1 (i/ √ t) .\nFor PEA with weighed experts, it is sufficient to replace 1/N by pn in the definitions of fT and CT .\nNote that fT has the form (10), hence Lemma 6 applies, and due to Lemma 3 or Lemma 2 such a γT exists.\nLet us prove by induction over T that CT ≤ 1. It is trivial for T = 0, since L0 = L n 0 = 0 and ∑0 t=1 = 0. Assume that CT ≤ 1 and prove that CT+1 ≤ 1. By the choice of γT , we know that fT (γT , ωT ) ≤ CT ≤ 1. Since the function xα is concave for 0 < α < 1, we have\n1 ≥ ( fT (γT , ωT ) )\n√ T/ √ T+1\n=\n(\nN ∑\nn=1\n1\nN\n∞ ∑\ni=1\nc i2 e(i/\n√ T)(LT−LnT )−(i/2 √ T) ∑ T t=1(i/ √ t)\n) √ T/ √ T+1\n≥ N ∑\nn=1\n1\nN\n∞ ∑\ni=1\nc i2 (\ne(i/ √ T )(LT−LnT )−(i/2 √ T) ∑ T t=1(i/ √ t) )\n√ T/ √ T+1\n= CT+1 .\nNow it is easy to get the loss bound. Assume that for an ǫ-fraction of Experts their losses LnT are smaller than or equal to L ǫ T . Then fT (γT , ωT ) can be bounded from below by\nǫ\n∞ ∑\ni=1\nc i2 e(i/\n√ T)(LT−LǫT )−(i/2 √ T) ∑ T t=1(i/ √ t) .\nFurther, bounding the infinite sum by one of the terms, we get\ne(i/ √ T)(LT−LǫT )−(i/2 √ T) ∑T t=1(i/ √ t) ≤ 1\nǫ\ni2 c .\nTaking the logarithm, using ∑T\nt=1\n( 1/ √ t ) ≤ 2 √ T and rearranging the terms,\nwe get\nLT ≤ LǫT + √ T\ni\n(\ni2 + ln 1\nǫ + 2 ln i+ ln(1/c)\n)\n.\nLetting i = ⌈ √ ln(1/ǫ) ⌉ + 1 and using the estimates i ≤ √\nln(1/ǫ) + 2, 1/i ≤ 1, (ln i)/i ≤ 2, (ln(1/ǫ))/i ≤ √\nln(1/ǫ), and ln(1/c) = ln(π2/6) ≤ 1, we obtain the final bound.\nRemark 1. For DTOL and for PEA with the finite number of Experts, the infinite sum over i in the proof can be replaced by the sum up to ⌈ √ lnN) ⌉ +1.\nHowever, one should keep decreasing weights c/i2: for uniform weights the bound will have an additional term of the form O((ln lnN)/ ln(1/ǫ)).\nRemark 2. Probably, the first bound for ǫ-quantile regret was stated (implicitly) in [9]. More precisely, that paper considered even more general regret notion: Theorem 1 in [9] gives a bound for PEA with weighed experts under the logarithmic loss of the form\nLT ≤ N ∑\nn=1\nunL n T +\nN ∑\nn=1\nun ln un pn\nfor any ~u ∈ ∆N ; p1, . . . , pN are weights of Experts. Here pn are known to the algorithm in advance, whereas un are not known and the bound holds uniformly for all un. Taking un = 0 for Experts not from the ǫ-quantile of the best Experts, and uniform un over Experts from the ǫ-quantile, we get the bound in terms of LǫT . It can be easily checked that the strategy in Theorem 9 also achieves the following bound:\nLT ≤ N ∑\nn=1\nunL n T + 2\n√ √ √ √T ( N ∑\nn=1\nun ln un pn\n)\n+ 7 √ T\nfor any ~u ∈ ∆N and any T . In Theorem 8 one can replace LǫT by ∑N n=1 unL n T and ln(1/ǫ) by ∑N\nn=1 un ln(un/pn) as well.\nRemark 3. Theorem 9 can be also adapted to discounted regrets of the form LT = ∑T t=1(1− α)T−tλ(γt, ωt) for a known α. Then ǫ in the bound is replaced by α, and LǫT by L n T = ∑T t=1(1− α)T−tλ(γnt , ωt)."
    }, {
      "heading" : "4.2 Discussion of the Bounds",
      "text" : "For a game with N Experts, the best bound, uniform in T , is given by [3, Theorem 2.3]:\nLT ≤ LnT + √ 2T lnN + √ lnN\n8 . (19)\nThe bounds (14) and (17) with ǫ = 1/N are always worse than (19). In the bound (17) the leading coefficient at √ T lnN is √ 2 times as much. In the\nbound (14) the coefficient at √ T lnN is the same, but the other terms are larger, and even the asymptotics is worse when N is fixed and T → ∞. However, it appears that the bound (19) cannot be transferred to ǫ-quantile regret RǫT = LT −LǫT . The proof of Theorem 2.3 in [3] heavily relies on tracking the loss of only one best Expert, and it is unclear whether the existence of several good (or identical) Experts can be exploited in this proof. The experiments\nreported in [4] show that algorithms with good best Expert bounds may have rather bad performance when the nominal number of Experts is much greater than the effective number of Experts.\nThe first (and the only, as far as we know) bound specifically formulated for ǫ-quantile regret is proven for the NormalHedge algorithm in [4, Theorem 1]:\nLT ≤ LǫT + √ ( 1 + ln 1\nǫ\n)( 3(1 + 50δ)T + 16 ln2 N\nδ\n(\n10.2\nδ2 + lnN\n))\n, (20)\nwhich holds uniformly for all δ ∈ (0, 1/2]. Note that this bound depends on the effective number of actions 1/ǫ and at the same time on the nominal number of actions N . The latter dependence is weak, but probably prevents the use of NormalHedge with infinitely many Experts.\nThe main advantage of our bounds in Theorems 8 and 9 is that they are perfectly in terms of the effective number of Experts. In a sense, the DFA does not need to know in advance the number of Experts.\nRemark 4. To obtain a precise statement about the unknown number of Expert, one can consider the setting where Experts may come at some later steps; the regret to a late Expert is accumulated over the steps after his coming — it is a simple time selection function (see Subsection 4.3), which switches from 0 to 1 only once. Our algorithms and bounds can be easily adapted for this setting: we must consider infinitely many Experts almost all of which are inactive; and then proceed similarly to Theorem 11.\nBoth our bounds are worse than (20) asymptotically when ǫ and N are fixed and T → ∞. In this case, the regret term in (20) grows as √ 3T ln(1/ǫ) + 3T , whereas in (17) it grows as √ 4T ln(1/ǫ) + 7 √ T and in (14), the worst bound, it grows as √\n5T ln lnT + 2T ln(1/ǫ). On the other hand, our bounds are better when T is relatively small. The term ln lnT is small for any reasonable practical application (e. g., ln lnT < 4 if T is the age of the universe expressed in microseconds), and then the main term in (14) is √ 2T ln(1/ǫ), which even fits the optimal bound (19). Bound (17) improves over (20) for T ≤ 106 ln4 N . Now let us say a few words about known algorithms for which an ǫ-quantile regret bounds were not formulated explicitly, but can easily be obtained. The Weighted Average Algorithm, which is used to obtain bound (19), can be analysed in a manner different from [3, Theorem 2.3], see [11]. Then one can obtain the following bound for ǫ-quantile regret:\nLT ≤ LǫT + 1\nc\n√ T ln 1\nǫ + c\n√ T ,\nwhere the constant c > 0 is arbitrary but must be fixed in advance. If ǫ is not known and hence c cannot be adapted to ǫ, the leading term is O( √ T ln 1ǫ ), which is worse than (17) for small ǫ (that is, if we consider a large effective number of actions).\nFor the Aggregating Algorithm [13] (which can be considered as a special case of the DFA for a certain supermartingale, as shown in [5]), the bound can be trivially adapted to ǫ-quantile regret:\nLT ≤ cLǫT + a ,\nwhere the possible constants c ≥ 1 and a depend on the loss function. However, in the case of DTOL or arbitrary convex games, the constant c is strictly greater that 1 and the bound may be much worse than (14) and (17) (when LǫT grows\nsignificantly faster than √ T ). At the same time, this bound is much better when LǫT ≈ 0 (there is at least ǫ fraction of “perfect” Experts ). For the standard setting with the known number of Experts, other “small loss” bounds, of the form LT ≤ LnT + O( √\nLnT ), were obtained. The authors of [4] posed an open question whether similar bounds can be obtained if the (effective) number of actions is not known. We left the question open."
    }, {
      "heading" : "4.3 Internal Regret and Time Selection Functions",
      "text" : "It was shown in [5] and in [7] that the loss bounds obtained by the DFA can be easily transferred to second-guessing experts and sleeping experts models. A second-guessing expert is a (known) function of Learner’s decision. Informally, a second-guessing expert explains how Learner could improve (hopefully) his performance. Sleeping experts (or specialists) introduced in [9] may be inactive at some steps, abstaining from announcing their decision (a specialist may decide that the current problem is outside her expertize area). The regret of Learner to a sleeping expert is counted over the steps when the expert was active.\nThe models similar to second-guessing experts and sleeping experts were studied in DTOL as internal (or wide range) regret and time selection (or activation) functions respectively (see [12] for a review). The internal regret compares Learner’s loss not to the loss of a fixed action, but to the loss of a modification rule of the form “Every time Learner selected action n he should have selected n′ instead” (more formally, all the weight γt,n assigned to action n should have been appended to γt,n′). The wide range regret deals with more general modification rules which may replace each action by some other action. Note that a fixed action n is also a modification rule that suggests to use n instead of any other action.\nA time selection function attached to a modification rule assigns a scaling factor from [0, 1] to each step. The regret of Learner to this rule is a sum of the regrets at each step weighed by these factors. This weight can be regarded as a degree of specialist’s certainty: when the rule is known to be inapplicable for some reason, the weight is zero; and when the rule is partially relevant, the rule agrees for some partial responsibility only.\nAs has been recently shown [12], an algorithm achieving in DTOL with N action some regret bound with respect to N can be transformed into an algorithm that achieves the same bound with respect to K for K modification rules with attached time selection functions. This gives the best regret bound O( √ T lnK).\nWe show how to extend the results of Theorems 8 and 9 to internal regret and time selection settings. We do not apply the general method of [12], but directly modify our supermartingales and proofs. Remarkably, we need very modest changes.\nA modification rule is represented by N×N stochastic matrixM : the matrix elements are non-negative and the sum of every column is 1. The (one-step) regret of Learner’s decision ~γ ∈ ∆N to the modification rule M on the outcome ~ω ∈ [0, 1]N is ~γ ·~ω− (M~γ) ·~ω, where M~γ is the product of matrix M and vectorcolumn ~γ. The total regret after step T on the sequence of outcomes ~ω1, ~ω2, . . .\nof Learner predicting ~γ1, ~γ2, . . . with respect to a modification rule M(t) with attached time selection function I(t) is\nRT =\nT ∑\nt=1\nI(t) ( ~γt · ~ωt − (M(t)~γt) · ~ωt )\n(cf. RH,I,f in [12]).\nRemark 5. The definition above reflects a slightly more general notion of a modification rule, which allows, for example, the rules that mean “instead of n select at random n′ or n′′ equiprobably”. Khot and Ponnuswami [12] do not discuss such rules explicitly, but it appears that their method works for them as well (unless we miss some subtlety in the proof).\nFirst let us obtain an analogue of Theorem 9. We formulate the bound with respect to the effective number of modification rules. It is very probable that the method of [12] also transforms a bound in terms of the effective number of actions into a bound in terms of the effective number of modification rules, but we did not check.\nTheorem 10. In DTOL with N actions, let us have K modifications rules Mk(t), each assigning a stochastic N ×N matrix to each step t, with attached time selection functions Ik(t) assigning a number from [0, 1]. (The modification rule numbered k may arbitrarily change in time and may depend on the whole history, and so is the time selection function.) There is a strategy that achieves the bound\nRǫT ≤ 2 √ T ln 1\nǫ + 7\n√ T\nfor any T and any ǫ, where RǫT is a value such that for at least ǫ-fraction of the rules the regret RkT of rule k after step T is not less than R ǫ T .\nProof. The proof is very similar to the proof of theorem 9. The only change in the algorithm is that in (18) we replace (λ(~γ, ~ω) − λ(~γnT , ~ω)) = ~γ · ~ω − ωn by Ik(t) ( ~γ · ~ω − (Mk(t)~γ) · ~ω ) and thus apply the same algorithm with\nfT (γ, ω) =\nK ∑\nk=1\n1\nK\n∞ ∑\ni=1\nc i2 e(i/\n√ T)RkT−1−(i/2 √ T) ∑T−1 t=1 (i/ √ t)\n× e(i/ √ T)(Ik(T )(~γ·~ω−(Mk(T )~γ)·~ω))−(i/ √ T)2/2 . (21)\nWe need to check that the conditions of Lemma 3 are satisfied. It is enough to observe that I(T ) ≤ 1 and that exp (( i/ √ T ) (Ik(T )(~γ · ~ω − (Mk(T )~γ) · ~ω)) ) is\nconvex in ~γ, then the proof of the Lemma 6 applies without changes. The loss bound is obtained as in Theorem 9.\nTheorem 8 can be adapted in a similar way. But we formulate another analogue of the theorem: The bound includes the total number of modification rules instead of the the effective number of them, but the regret of each rule k is bounded in terms of the actual activity time (or awake time)\n∑T t=1 Ik(t) of\nthe rule, not the total time T . We do not know whether bounds referring to the awake time were explicitly stated anywhere; however, a bound of this kind can be obtained from bounds that depend on the loss of the rule (or action), as in [2, Theorem 16] or [12, Theorem 5].\nTheorem 11. In DTOL with N actions, let us have K modifications rules Mk(t), each assigning a stochastic N ×N matrix to each step t, with attached time selection functions Ik(t) assigning a number from [0, 1]. The DFA achieves the bound\n∫ 1/e\n0\neηR k T−Tk(T )η2/2 dη\nη (\nln 1η\n)2 ≤ K ,\nwhere Tk(T ) = ∑T\nt=1 Ik(t), for any T and k = 1, . . . ,K. In particular, the above bound implies for any δ ∈ (0, 1/4)\nRkT ≤ 2√ 2− δ\n√\nTk(T ) ln 1\nK +\n1 2 Tk(T ) ln 1 δ + 2Tk(T ) ln lnTk(T )\n+ max {4, 400 lnK} , which can be further reduced to\nRǫT ≤ ( 1 + 1\nlnTk(T )\n)\n√\n2Tk(T ) lnK + 5Tk(T ) ln lnTk(T ) +O (lnK) .\nProof. We change the supermartingale used for Theorem 8, similarly to the proof of Theorem 10. Namely, we apply the DFA to the supermartingale\nft(γ, ω) =\nK ∑\nk=1\n1\nK\n∫ 1/e\n0\ndη\nη (\nln 1η\n)2 e ηRkt−1−Tk(t−1)η2/2\n× eηIk(T )(~γ·~ω−(Mk(T )~γ)·~ω)−(ηIk(t))2/2 . (22) Note that in contrast to the proof of Theorem 10, Ik(t) appears also in the “Hoeffding correction term” e−η 2/2. The rest of the proof does not change much. To get the loss bound we observe that ∑T\nt=1(Ik(t)) 2 ≤ Tk(T ) since\nIk(t) ∈ [0, 1]."
    }, {
      "heading" : "4.4 A Toy Example of a Multiobjective Bound",
      "text" : "In this subsection, we discuss bounds with respect to two loss functions. In [7], we showed how to cope with several mixable loss functions. Here we combine a mixable loss function (the square loss) with a non-mixable one (the absolute loss).\nLet us describe an informal prediction setting where such a combination of loss functions can make sense. We want to predict the probability of rain. We have two groups of Experts. The first group consists of Metoffices that give the probability and evaluate the result according to the Brier (square) loss function. The second group is Simpletons, they give a boolean (‘rain’/‘no rain’) prediction and count the number of errors (the simple prediction game). We must provide a pair, a probability and a boolean prediction, and the two components of our prediction must agree in the following sense: if we give probability of rain more than one half, we must predict ‘rain’; if we give probability of rain less than one half, we must predict ‘no rain’; only if we give the probability 1/2, we may choose the boolean prediction arbitrary (so we can randomize here). In the theorem below we bound both Learner’s Brier loss and Learner’s expected (over the internal randomizer) number of errors.\nTheorem 12. Assume that we are given K Experts that give predictions pk ∈ [0, 1] and M Experts that give predictions bm ∈ {0, 1}. Learner is allowed to give predictions (p, p̃) ∈ [0, 1]× [0, 1], with the following restriction: if p < 1/2 then p̃ = 0 and if p > 1/2 then p̃ = 1. Then there exists a strategy for Learner guaranteeing for any sequence of outcomes ω1, ω2, . . . that for any T and for any k it holds\nT ∑\nt=1\n(pt − ωt)2 ≤ T ∑\nt=1\n(pkt − ωt)2 + 1\n2 ln(K +M) ,\nand for any T and for any m it holds\nT ∑\nt=1\n|p̃t − ωt| ≤ T ∑\nt=1\n[bmt 6= ωt] +O( √ T ln(K +M) + T ln lnT ) ,\nwhere [bmt 6= ωt] = 1 if bmt 6= ωt and [bmt 6= ωt] = 0 otherwise.\nProof. Let A = {(p, p̃) ∈ [0, 1]2 | p̃ = 0 if p < 1/2 and p̃ = 1 if p > 1/2 and } = {(p, 0) | p ∈ [0, 1/2)} ∪ {(1/2, p̃) | p̃ ∈ [0, 1]} ∪ {(p, 1) | p ∈ (1/2, 1]}. We apply the DFA to supermatingale ST on Ω = {0, 1} defined by (7) with\nfT (p, p̃, ω) = 1\nK +M\nK ∑\nk=1\ne2 ∑T t=1 ((pt−ωt)2−(pkt −ωt)2) × e2((p−ω)2−(pkT−ω)2)\n+ 1\nK +M\nM ∑\nm=1\n∫ 1/e\n0\ndη\nη (\nln 1η\n)2 e η ∑ T t=1(|p̃t−ωt|−[bmt 6=ωt])−η2/2×eη(|p̃−ω|−[bmT 6=ω])−η2/2\n(23)\nand G(π) = {(p, p̃) ∈ A | p = π(1)}. To ensure that ST is a supermartingale, we need to check that Eπ ( |p̃− ω| − [bmT 6= ω] )\n≤ 0 if (π(1), p̃) ∈ G(π). Then we can refer to Lemma 6 and [5, Lemma 2].\nIndeed, Eπ ( |p̃− ω| − [bmT 6= ω] ) = π(1)(1 − p̃− (1− bmT )) + π(0)(p̃− bmT ) = (π(0)− π(1))(p̃− bmT ). If π(1) > 1/2 then π(0) < 1/2 and p̃ = 1 ≥ bmT . If π(1) < 1/2 then π(0) > 1/2 and p̃ = 0 ≤ bmT . If π(1) = 1/2 then π(0) = 1/2. Obviously, in all the cases (π(0)− π(1))(p̃− bmT ) ≤ 0.\nThe bound follows in the usual way (cf. Theorem 8).\nRemark 6. Let us discuss how to find the numbers p and p̃ such that fT (p, p̃, 0) ≤ 1 and fT (p, p̃, 1) ≤ 1. Consider x ∈ [0, 2] and two functions\np(x) =\n\n \n \nx, if x < 1/2, 1/2, if x ∈ [1/2, 3/2], x− 1, if x > 3/2,\nand p̃(x) = min{1,max{x− 1/2, 0}} .\nClearly, p(x) and p̃(x) are continuous functions of x. Let\ng(x, ω) = fT (p(x), p̃(x), ω)− 1 .\nIt is obvious that if g(x0, 0) ≤ 0 and g(x0, 1) ≤ 0 then we can take p(x0) and p̃(x0) as p and p̃ we are looking for. The supermartingale property of ST and the definition of ST imply that\np(x)g(x, 1) + (1− p(x))g(x, 0) ≤ 0 .\nSubstituting x = 0, we get g(0, 0) ≤ 0. Substituting x = 2, we get g(2, 1) ≤ 0. If g(0, 1) ≤ 0 or g(2, 0) ≤ 0, we can take x0 = 0 or x0 = 2 respectively. Otherwise, consider the function φ(x) = g(x, 1) − g(x, 0). It is continuous, φ(0) > 0 and φ(2) < 0, hence there exists x0 such that φ(x0) = 0. Clearly, g(x0, 0) = g(x0, 1) ≤ 0."
    }, {
      "heading" : "Acknowledgements",
      "text" : "This work was supported by EPSRC, grant EP/F002998/1. We are grateful to Yura Kalnishkan for discussions."
    }, {
      "heading" : "1244 (2008).",
      "text" : "[12] Subhash Khot and Ashok Kumar Ponnuswami. Minimizing Wide Range Regret with Time Selection Functions. In Rocco A. Servedio and Tong Zhang, editors, 21st Annual Conference on Learning Theory - COLT 2008, Helsinki, Finland, July 9-12, 2008, pages 81–86, Omnipress, 2008.\n[13] V. Vovk. A game of prediction of expert advice. Journal of Computer and System Sciences, 56:153-173, 1998."
    } ],
    "references" : [ {
      "title" : "Fixed Point Theory and Applications , volume 141 of Cambridge Tracts in Mathematics",
      "author" : [ "R. Agarwal", "M. Meehan", "D. O’Regan" ],
      "venue" : null,
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2001
    }, {
      "title" : "From External to Internal Regret",
      "author" : [ "Avrim Blum", "Yishay Mansour" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2007
    }, {
      "title" : "Prediction, Learning, and Games",
      "author" : [ "N. Cesa-Bianchi", "G. Lugosi" ],
      "venue" : null,
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2006
    }, {
      "title" : "A Parameter-free Hedging Algorithm",
      "author" : [ "Kamalika Chaudhuri", "Yoav Freund", "Daniel Hsu" ],
      "venue" : "Advances in Neural Information Processing Systems",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2009
    }, {
      "title" : "Supermartingales in prediction with expert advice",
      "author" : [ "Alexey Chernov", "Yuri Kalnishkan", "Fedor Zhdanov", "Vladimir Vovk" ],
      "venue" : "Proceedings of the Nineteenth International Conference on Algorithmic Learning Theory,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2008
    }, {
      "title" : "Supermartingales in prediction with expert advice",
      "author" : [ "Alexey Chernov", "Yuri Kalnishkan", "Fedor Zhdanov", "Vladimir Vovk" ],
      "venue" : "Theoretical Computer Science,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2010
    }, {
      "title" : "Prediction with expert evaluators",
      "author" : [ "Alexey Chernov", "Vladimir Vovk" ],
      "venue" : "Proceedings of the 20th International Conference on Algorithmic Learning Theory,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2009
    }, {
      "title" : "A decision-theoretic generalization of on-line learning and an application to boosting",
      "author" : [ "Yoav Freund", "Robert E. Schapire" ],
      "venue" : "Journal of Computer and System Sciences,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 1997
    }, {
      "title" : "Using and combining predictors that specialize",
      "author" : [ "Yoav Freund", "Robert E. Schapire", "Yoram Singer", "Manfred K. Warmuth" ],
      "venue" : "In Proceedings of the Twenty Ninth Annual ACM Symposium on Theory of Computing,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 1997
    }, {
      "title" : "The weak aggregating algorithm and weak mixability",
      "author" : [ "Yuri Kalnishkan", "Michael V. Vyugin" ],
      "venue" : "Proc. COLT’05,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2008
    }, {
      "title" : "Minimizing Wide Range Regret with Time Selection Functions",
      "author" : [ "Subhash Khot", "Ashok Kumar Ponnuswami" ],
      "venue" : "21st Annual Conference on Learning Theory - COLT",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2008
    }, {
      "title" : "A game of prediction of expert advice",
      "author" : [ "V. Vovk" ],
      "venue" : "Journal of Computer and System Sciences,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 1998
    } ],
    "referenceMentions" : [ {
      "referenceID" : 2,
      "context" : "In the PEA framework (see [3] for details, references and historical notes), at each step Learner gets decisions (also called predictions) of several Experts and must make his own decision.",
      "startOffset" : 26,
      "endOffset" : 29
    }, {
      "referenceID" : 7,
      "context" : "In DTOL, introduced in [8], Learner’s decision is a probability distribution on a finite set of actions.",
      "startOffset" : 23,
      "endOffset" : 26
    }, {
      "referenceID" : 3,
      "context" : "In this paper we deal with another kind of bound, recently introduced in [4].",
      "startOffset" : 73,
      "endOffset" : 76
    }, {
      "referenceID" : 3,
      "context" : "The following regret bound obtained in [4] for their NormalHedge algorithm holds for this case:",
      "startOffset" : 39,
      "endOffset" : 42
    }, {
      "referenceID" : 3,
      "context" : "Our bound has a simpler structure, but it is generally incomparable to the (precise) bound for Normal Hedge from [4] (see Subsection 4.",
      "startOffset" : 113,
      "endOffset" : 116
    }, {
      "referenceID" : 10,
      "context" : "Also our bound can be easily adapted to internal regret (see [12] for definition).",
      "startOffset" : 61,
      "endOffset" : 65
    }, {
      "referenceID" : 4,
      "context" : "In [5], the DF was used to obtain bounds of the form LT ≤ cLT + a, where c and a are some constants.",
      "startOffset" : 3,
      "endOffset" : 6
    }, {
      "referenceID" : 4,
      "context" : "(This result is of certain independent interest: for example, it helps to get rid of additional Assumption 3 in Theorem 3 in [5].",
      "startOffset" : 125,
      "endOffset" : 128
    }, {
      "referenceID" : 0,
      "context" : "For any natural N , by ∆N we denote the standard simplex in R N : ∆N = {~ p ∈ [0, 1] | ∑N n=1 pn = 1}.",
      "startOffset" : 78,
      "endOffset" : 84
    }, {
      "referenceID" : 0,
      "context" : "Reality announces ~ωt ∈ [0, 1] .",
      "startOffset" : 24,
      "endOffset" : 30
    }, {
      "referenceID" : 7,
      "context" : "The decision-theoretic framework for online learning (DTOL) was introduced in [8].",
      "startOffset" : 78,
      "endOffset" : 81
    }, {
      "referenceID" : 3,
      "context" : "As defined in [4] (for DTOL), the regret to the top ǫ-quantile (at step T ) is the value R T such that there are at least ǫN actions (the fraction at least ǫ of all Experts) with R T ≥ R T .",
      "startOffset" : 14,
      "endOffset" : 17
    }, {
      "referenceID" : 0,
      "context" : ", pK ∈ [0, 1] , ∑K k=1 pk = 1, there exists g ∈ Λ such that g(ω) ≤ ∑Kk=1 pkgk(ω) for all ω ∈ Ω.",
      "startOffset" : 7,
      "endOffset" : 13
    }, {
      "referenceID" : 0,
      "context" : "For such games, we assume without loss of generality that Λ ⊆ [0, 1] (we always can scale the loss function).",
      "startOffset" : 62,
      "endOffset" : 68
    }, {
      "referenceID" : 0,
      "context" : "For DTOL as a special case of PEA, the outcome space is Ω = [0, 1] , the decision space is Γ = ∆N , and the loss function is λ(~γ, ~ω) = ~γ · ~ ω.",
      "startOffset" : 60,
      "endOffset" : 66
    }, {
      "referenceID" : 11,
      "context" : "In particular, we can obtain PEA bounds that hold for specific loss functions or classes of loss functions (such as mixable loss functions [13]), and these bounds may be much stronger than the general bounds induced by DTOL.",
      "startOffset" : 139,
      "endOffset" : 143
    }, {
      "referenceID" : 4,
      "context" : "(3) This definition of supermartingale is equivalent to the one given in [5].",
      "startOffset" : 73,
      "endOffset" : 76
    }, {
      "referenceID" : 0,
      "context" : "Let Ω be [0, 1] .",
      "startOffset" : 9,
      "endOffset" : 15
    }, {
      "referenceID" : 0,
      "context" : "Since [0, 1] is a compact metric space, the space P([0, 1]) with weak topology is compact too (see, e.",
      "startOffset" : 6,
      "endOffset" : 12
    }, {
      "referenceID" : 0,
      "context" : "Since [0, 1] is a compact metric space, the space P([0, 1]) with weak topology is compact too (see, e.",
      "startOffset" : 52,
      "endOffset" : 58
    }, {
      "referenceID" : 0,
      "context" : "Assume that g1, g2 ∈ GΛ(π) ⊆ Λ and α ∈ [0, 1].",
      "startOffset" : 39,
      "endOffset" : 45
    }, {
      "referenceID" : 0,
      "context" : "For DTOL, the set Λ = {g ∈ R[0,1]N | ∃~ p ∈ ∆N∀~ω ∈ [0, 1] g(ω) = ~ p · ~ω} is obviously non-empty and it is compact and convex as a linear image of simplex ∆N .",
      "startOffset" : 28,
      "endOffset" : 33
    }, {
      "referenceID" : 0,
      "context" : "For DTOL, the set Λ = {g ∈ R[0,1]N | ∃~ p ∈ ∆N∀~ω ∈ [0, 1] g(ω) = ~ p · ~ω} is obviously non-empty and it is compact and convex as a linear image of simplex ∆N .",
      "startOffset" : 52,
      "endOffset" : 58
    }, {
      "referenceID" : 0,
      "context" : "Let (Ω,Γ, λ) be a game, the range of λ is included in [0, 1] and G(π) is defined by (8).",
      "startOffset" : 54,
      "endOffset" : 60
    }, {
      "referenceID" : 2,
      "context" : "This bound is twice as large as the best bound obtained in [3] (see their Theorems 2.",
      "startOffset" : 59,
      "endOffset" : 62
    }, {
      "referenceID" : 2,
      "context" : "2 in [3].",
      "startOffset" : 5,
      "endOffset" : 8
    }, {
      "referenceID" : 8,
      "context" : "Probably, the first bound for ǫ-quantile regret was stated (implicitly) in [9].",
      "startOffset" : 75,
      "endOffset" : 78
    }, {
      "referenceID" : 8,
      "context" : "More precisely, that paper considered even more general regret notion: Theorem 1 in [9] gives a bound for PEA with weighed experts under the logarithmic loss of the form",
      "startOffset" : 84,
      "endOffset" : 87
    }, {
      "referenceID" : 2,
      "context" : "3 in [3] heavily relies on tracking the loss of only one best Expert, and it is unclear whether the existence of several good (or identical) Experts can be exploited in this proof.",
      "startOffset" : 5,
      "endOffset" : 8
    }, {
      "referenceID" : 3,
      "context" : "reported in [4] show that algorithms with good best Expert bounds may have rather bad performance when the nominal number of Experts is much greater than the effective number of Experts.",
      "startOffset" : 12,
      "endOffset" : 15
    }, {
      "referenceID" : 9,
      "context" : "3], see [11].",
      "startOffset" : 8,
      "endOffset" : 12
    }, {
      "referenceID" : 11,
      "context" : "For the Aggregating Algorithm [13] (which can be considered as a special case of the DFA for a certain supermartingale, as shown in [5]), the bound can be trivially adapted to ǫ-quantile regret: LT ≤ cLT + a ,",
      "startOffset" : 30,
      "endOffset" : 34
    }, {
      "referenceID" : 4,
      "context" : "For the Aggregating Algorithm [13] (which can be considered as a special case of the DFA for a certain supermartingale, as shown in [5]), the bound can be trivially adapted to ǫ-quantile regret: LT ≤ cLT + a ,",
      "startOffset" : 132,
      "endOffset" : 135
    }, {
      "referenceID" : 3,
      "context" : "The authors of [4] posed an open question whether similar bounds can be obtained if the (effective) number of actions is not known.",
      "startOffset" : 15,
      "endOffset" : 18
    }, {
      "referenceID" : 4,
      "context" : "3 Internal Regret and Time Selection Functions It was shown in [5] and in [7] that the loss bounds obtained by the DFA can be easily transferred to second-guessing experts and sleeping experts models.",
      "startOffset" : 63,
      "endOffset" : 66
    }, {
      "referenceID" : 6,
      "context" : "3 Internal Regret and Time Selection Functions It was shown in [5] and in [7] that the loss bounds obtained by the DFA can be easily transferred to second-guessing experts and sleeping experts models.",
      "startOffset" : 74,
      "endOffset" : 77
    }, {
      "referenceID" : 8,
      "context" : "Sleeping experts (or specialists) introduced in [9] may be inactive at some steps, abstaining from announcing their decision (a specialist may decide that the current problem is outside her expertize area).",
      "startOffset" : 48,
      "endOffset" : 51
    }, {
      "referenceID" : 10,
      "context" : "The models similar to second-guessing experts and sleeping experts were studied in DTOL as internal (or wide range) regret and time selection (or activation) functions respectively (see [12] for a review).",
      "startOffset" : 186,
      "endOffset" : 190
    }, {
      "referenceID" : 0,
      "context" : "A time selection function attached to a modification rule assigns a scaling factor from [0, 1] to each step.",
      "startOffset" : 88,
      "endOffset" : 94
    }, {
      "referenceID" : 10,
      "context" : "As has been recently shown [12], an algorithm achieving in DTOL with N action some regret bound with respect to N can be transformed into an algorithm that achieves the same bound with respect to K for K modification rules with attached time selection functions.",
      "startOffset" : 27,
      "endOffset" : 31
    }, {
      "referenceID" : 10,
      "context" : "We do not apply the general method of [12], but directly modify our supermartingales and proofs.",
      "startOffset" : 38,
      "endOffset" : 42
    }, {
      "referenceID" : 0,
      "context" : "The (one-step) regret of Learner’s decision ~γ ∈ ∆N to the modification rule M on the outcome ~ω ∈ [0, 1] is ~γ ·~ω− (M~γ) ·~ω, where M~γ is the product of matrix M and vectorcolumn ~γ.",
      "startOffset" : 99,
      "endOffset" : 105
    }, {
      "referenceID" : 10,
      "context" : "RH,I,f in [12]).",
      "startOffset" : 10,
      "endOffset" : 14
    }, {
      "referenceID" : 10,
      "context" : "Khot and Ponnuswami [12] do not discuss such rules explicitly, but it appears that their method works for them as well (unless we miss some subtlety in the proof).",
      "startOffset" : 20,
      "endOffset" : 24
    }, {
      "referenceID" : 10,
      "context" : "It is very probable that the method of [12] also transforms a bound in terms of the effective number of actions into a bound in terms of the effective number of modification rules, but we did not check.",
      "startOffset" : 39,
      "endOffset" : 43
    }, {
      "referenceID" : 0,
      "context" : "In DTOL with N actions, let us have K modifications rules Mk(t), each assigning a stochastic N ×N matrix to each step t, with attached time selection functions Ik(t) assigning a number from [0, 1].",
      "startOffset" : 190,
      "endOffset" : 196
    }, {
      "referenceID" : 0,
      "context" : "In DTOL with N actions, let us have K modifications rules Mk(t), each assigning a stochastic N ×N matrix to each step t, with attached time selection functions Ik(t) assigning a number from [0, 1].",
      "startOffset" : 190,
      "endOffset" : 196
    }, {
      "referenceID" : 0,
      "context" : "To get the loss bound we observe that ∑T t=1(Ik(t)) 2 ≤ Tk(T ) since Ik(t) ∈ [0, 1].",
      "startOffset" : 77,
      "endOffset" : 83
    }, {
      "referenceID" : 6,
      "context" : "In [7], we showed how to cope with several mixable loss functions.",
      "startOffset" : 3,
      "endOffset" : 6
    }, {
      "referenceID" : 0,
      "context" : "Assume that we are given K Experts that give predictions p ∈ [0, 1] and M Experts that give predictions b ∈ {0, 1}.",
      "startOffset" : 61,
      "endOffset" : 67
    }, {
      "referenceID" : 0,
      "context" : "Learner is allowed to give predictions (p, p̃) ∈ [0, 1]× [0, 1], with the following restriction: if p < 1/2 then p̃ = 0 and if p > 1/2 then p̃ = 1.",
      "startOffset" : 49,
      "endOffset" : 55
    }, {
      "referenceID" : 0,
      "context" : "Learner is allowed to give predictions (p, p̃) ∈ [0, 1]× [0, 1], with the following restriction: if p < 1/2 then p̃ = 0 and if p > 1/2 then p̃ = 1.",
      "startOffset" : 57,
      "endOffset" : 63
    }, {
      "referenceID" : 0,
      "context" : "Let A = {(p, p̃) ∈ [0, 1] | p̃ = 0 if p < 1/2 and p̃ = 1 if p > 1/2 and } = {(p, 0) | p ∈ [0, 1/2)} ∪ {(1/2, p̃) | p̃ ∈ [0, 1]} ∪ {(p, 1) | p ∈ (1/2, 1]}.",
      "startOffset" : 19,
      "endOffset" : 25
    }, {
      "referenceID" : 0,
      "context" : "Let A = {(p, p̃) ∈ [0, 1] | p̃ = 0 if p < 1/2 and p̃ = 1 if p > 1/2 and } = {(p, 0) | p ∈ [0, 1/2)} ∪ {(1/2, p̃) | p̃ ∈ [0, 1]} ∪ {(p, 1) | p ∈ (1/2, 1]}.",
      "startOffset" : 120,
      "endOffset" : 126
    }, {
      "referenceID" : 1,
      "context" : "Consider x ∈ [0, 2] and two functions",
      "startOffset" : 13,
      "endOffset" : 19
    } ],
    "year" : 2013,
    "abstractText" : "In the framework of prediction with expert advice, we consider a recently introduced kind of regret bounds: the bounds that depend on the effective instead of nominal number of experts. In contrast to the NormalHedge bound, which mainly depends on the effective number of experts and also weakly depends on the nominal one, we obtain a bound that does not contain the nominal number of experts at all. We use the defensive forecasting method and introduce an application of defensive forecasting to multivalued supermartingales.",
    "creator" : "LaTeX with hyperref package"
  }
}