{
  "name" : "1503.04843.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "More General Queries and Less Generalization Error in Adaptive Data Analysis",
    "authors" : [ "Raef Bassily", "Adam Smith", "Thomas Steinke", "Jonathan Ullman" ],
    "emails" : [ "bassily@psu.edu", "asmith@psu.edu", "tsteinke@seas.harvard.edu", "jullman@cs.columbia.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Specifically, suppose there is an unknown distribution P and a set of n independent samples x is drawn from P . We seek an algorithm that, given x as input, “accurately” answers a sequence of adaptively chosen “queries” about the unknown distribution P . How many samples n must we draw from the distribution, as a function of the type of queries, the number of queries, and the desired level of accuracy?\nIn this work we make two new contributions towards resolving this question:\n1. We give upper bounds on the number of samples n that are needed to answer statistical queries that improve over the bounds of Dwork et al.\n2. We prove the first upper bounds on the number of samples required to answer more general families of queries. These include arbitrary low-sensitivity queries and the important class of convex risk minimization queries.\nAs in Dwork et al., our algorithms are based on a connection between differential privacy and generalization error, but we feel that our analysis is simpler and more modular, which may be useful for studying these questions in the future.\n∗Pennsylvania State University, Department of Computer Science and Engineering. {bassily,asmith}@psu.edu †Harvard University School of Engineering and Applied Sciences. Supported by NSF grant CCF-1116616. tsteinke@seas.harvard.edu ‡Columbia University Department of Computer Science. Supported by a Junior Fellowship from the Simons Society of Fellows. jullman@cs.columbia.edu\nar X\niv :1\n50 3.\n04 84\n3v 1\n[ cs\n.L G\n] 1\n6 M\nar 2\nContents"
    }, {
      "heading" : "1 Introduction 1",
      "text" : "1.1 Overview of Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 1.2 Overview of Techniques . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 1.3 Related Work in Differential Privacy . . . . . . . . . . . . . . . . . . . . . . . . . 4"
    }, {
      "heading" : "2 Preliminaries 5",
      "text" : "2.1 Queries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 2.2 Oracles for Adaptive Queries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 2.3 Differential Privacy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8"
    }, {
      "heading" : "3 From Privacy to Accuracy for Adaptive Queries 8",
      "text" : "3.1 Differential Privacy Implies Low Expected Error . . . . . . . . . . . . . . . . . . 8 3.2 Amplifying the Success Probability . . . . . . . . . . . . . . . . . . . . . . . . . . 11\n3.2.1 The Bounded Censor Algorithm . . . . . . . . . . . . . . . . . . . . . . . . 13"
    }, {
      "heading" : "4 From Function Approximation to Optimization Queries 15",
      "text" : "4.1 Oracles for Adaptive Minimization Queries . . . . . . . . . . . . . . . . . . . . . 15 4.2 Amplifying the Success Probability . . . . . . . . . . . . . . . . . . . . . . . . . . 17"
    }, {
      "heading" : "5 Applications 17",
      "text" : "5.1 Low-Sensitivity and Statistical Queries . . . . . . . . . . . . . . . . . . . . . . . . 17 5.2 Optimization Queries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\n5.2.1 Minimization Over Arbitrary Finite Sets . . . . . . . . . . . . . . . . . . . 18 5.2.2 Convex Minimization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\nAcknowledgements 19\nReferences 19"
    }, {
      "heading" : "1 Introduction",
      "text" : "A common problem in empirical research is testing the significance of multiple “hypotheses” on a finite sample of data drawn from some population. An observation is deemed significant if it is unlikely to have occurred by chance alone, and a “false discovery” occurs if the analyst incorrectly declares an observation significant. Unfortunately, false discovery has been identified as a substantial problem in the scientific community (see e.g. [Ioa05]). This problem persists despite decades of research by Statisticians on methods for preventing false discovery, such as the widely used Bonferroni Correction [Bon36, Dun61] and the Benjamini-Hochberg Procedure [BH95].\nFalse discovery is often attributed to misuse of statistics. However, an alternative explanation is that the prevalence of false discovery arises from the inherent adaptivity in the data analysis process—the choice of hypotheses to test depends on previous interactions with the data (see e.g. [Ioa05, GL13]). This problem was first formally modeled and studied in a striking recent paper by Dwork, Feldman, Hardt, Pitassi, Reingold, and Roth [DFH+15], who gave the first non-trivial algorithms for provably ensuring statistical validity in adaptive data analysis. The problem was also studied through a computational lens by [HU14, SU14], who showed that there is an inherent computational barrier to preventing false discovery in adaptive settings.\nIn this work, we extend the results of Dwork et al. [DFH+15] along two axes. First, we prove quantitatively stronger bounds on the statistical accuracy that can be achieved. Second, we significantly generalize the types of statistics that can be computed in adaptive settings. Like the algorithms of Dwork et al., all of our algorithms rely on the connection between differential privacy [DMNS06] and statistical validity, and our analysis generalizes, strengthens, and (in our opinion) simplifies this connection."
    }, {
      "heading" : "1.1 Overview of Results",
      "text" : "Following these previous papers, we formalize the problem of adaptive data analysis as follows: There is a distribution P over some finite universe X , and an oracle O that does not know P but is given n samples from P . Using its sample, the oracle must answer queries on P . Here, a query q maps a distribution P to a real-valued answer, and we assume the queries come from some family Q. The oracle’s answer a to a query q is α-accurate if |a − q(P )| ≤ α with high probability. Importantly, the oracle’s goal is to provide answers that “generalize” to the underlying distribution, rather than answers that are specific to the sample.\nWe model adaptivity by allowing the analyst to make a sequence of queries q1,q2, . . . , qk ∈Q to the oracle, who responds with answers a1, a2, . . . , ak . In the adaptive setting, the query qj may depend on the previous queries and answers q1, a1, . . . , qj−1, aj−1 arbitrarily. We say the oracle is α-accurate given n samples for k adaptively chosen queries if, when given n samples from an arbitrary distribution P , the oracle accurately responds to any adaptive analyst that makes at most k queries with high probability.\nDwork et al. [DFH+15] considered the family of statistical queries [Kea93]. A statistical query q is specified by a function p : X → [0,1] and is defined as q(P ) = Ez←RP [p(z)]. A simple analysis shows that, when k queries are specified non adaptively, (i.e. independent of previous answers), then simply evaluating the query on the sample is α-accurate with high probability as long as the number of samples n satisfies n & logk/α2. However, when the queries can be chosen adaptively, the empirical average performs exponentially worse, requiring n & k/α2 to guarantee accuracy.\nDwork et al. [DFH+15], showed how to achieve dramatically better accuracy guarantees for statistical queries, namely that n & min { √ k α2.5 , √ log |X |·log3/2 k α3.5 } samples suffice, which can be\nexponentially smaller than what is needed when outputting the sample mean. However, these bounds suffer from a large dependence on the accuracy level α.\nOur first contribution is to give a tighter and simpler analysis of (a slight variant of) their algorithms that yields the best known accuracy guarantees for adaptive statistical queries. In particular, we show that Õ( √ k/α2) samples suffice, which is the optimal dependence on α.\nAlthough statistical queries are a surprisingly general primitive for data analysis, we would like to be able to ask more general queries on the distribution P that capture a wider variety of machine learning and data mining tasks. Specifically, in this work we consider two general types of queries: low-sensitivity queries and search queries.\nLow-sensitivity queries are a generalization of statistical queries that are specified by an arbitrary function p : X n → R satisfying |p(x) − p(x′)| ≤ 1/n for every x,x′ ∈ X n differing on exactly one element. The query applied to the population is defined to be q(P ) = Ex←RP n[p(x)].\nSearch queries are a broad generalization of low-sensitivity queries to arbitrary output domains. The query is specified by a loss function L : X n ×Θ → R that is low-sensitivity in its first parameter, and the goal is to output θ ∈Θ that is “best” in the sense that it minimizes the average loss. Specifically, q(P ) = argminθ∈ΘEz←RP n[L(z;θ)]. An important special case is when L is convex in the parameter θ, which captures fundamental tasks in machine learning such as linear and logistic regression.\nOur second contribution is to give the first bounds on the sample complexity required to answer a large number of adaptively chosen low-sensitivity queries and search queries. Our sample complexity bounds are summarized in Figure 1.\nOn Optimality: Although our bounds improve on [DFH+15] for statistical queries, and are the first non-trivial bounds for other families of queries, we cannot prove that any of these bounds are optimal. Even for non-adaptive statistical queries, n & log(k)/α2 samples are necessary, and [HU14, SU14] showed that n & min{ √ k, √ log |X |}/α samples are necessary to answer adaptively chosen statistical queries. These lower bounds are information theoretic and apply to computationally unbounded oracles.\nHowever, it is known that, for every family of queries we consider, the differentially private mechanisms we use achieve optimal sample complexity [BUV14, BST14]. It is also known that the privacy parameters we require cannot be improved. Thus, any improvement to our bounds (beyond logarithmic factors) must come from using an oracle that is not differentially private.\nOn Computational Complexity: Throughout, we will assume that the analyst only issues queries q such that q(x) can be evaluated in time poly(n, log |X |), and are not the bottleneck in computation. Thus the empirical answer can be given in time poly(n, log |X |). When k n2 our algorithms have similar running time. However, when answering k n2 queries, our algorithms suffer running time poly(n, |X |). Since the oracle’s input is of size n · log |X |, these algorithms cannot be considered computationally efficient. For example, if X = {0,1}d for some dimension d, then in the non adaptive setting poly(n,d) running time would suffice, whereas our algorithms require poly(n,2d) running time. Unfortunately, this running time is known to be optimal, as [HU14, SU14] (building on hardness results in privacy [Ull13]) showed that, assuming exponentially hard one-way functions exist, any poly(n,2o(d)) time oracle that answers k =ω(n2) statistical queries is not even 1/3-accurate."
    }, {
      "heading" : "1.2 Overview of Techniques",
      "text" : "Following Dwork et al. [DFH+15], the main technique we use is a connection between differential privacy and generalization. Intuitively, differential privacy guarantees that the distribution of outputs given by the oracle does not depend “too much” on any one of the samples it is given. Differential privacy can be seen as a strong stability guarantee that behaves well under adaptive data analysis. Thus, this work fits within the rich line of work in machine learning connecting algorithmic stability and generalization (cf. [BE02, SSSS10]).\nWe show that a differentially private algorithm that provides answers to adaptive queries that are close to the empirical value on the sample gives answers that generalise to the underlying distribution. In particular, for the class of low-sensitivity queries, we obtain the following theorem.\nTheorem 1.1 (Main “Transfer Theorem”). Let O be an oracle that takes as input a sample x ∈ X n and answers k adaptively chosen low-sensitivity queries. Suppose O satisfies the following.\n1. The answers O gives are α-accurate with respect to the sample x. That is,\nE [ max j∈k ∣∣∣qj(x)− aj ∣∣∣] ≤ α, where q1, · · · ,qk : X n→ R are the low-sensitivity queries that are asked and a1, · · · , ak ∈ R are the answers given. The expectation is taken only over O’s random coins—the sample x, and the sequence of queries q1, · · · ,qk can be arbitrary.\n2. O satisfies (α,α)-differential privacy. (Definition 2.6)\nThen there exists an oracleO′ that takes as input a sample x′←R P n ′ where P is an arbitrary distribution over X , and answers k adaptively chosen low-sensitivity queries such that, if n′ =O(n·log(1/β)), then O gives O(α)-accurate answers with respect to the population P with high probability. That is,\nP [ max j∈k ∣∣∣∣∣ Ex←RP n [qj(x)]− aj ∣∣∣∣∣ ≤O(α)] ≥ 1− β,\nwhere the probability is taken only over the choice of x and O’s random coins. Moreover, the running time of O′ is roughly the same as that of O.\nThe results in Figure 1 follow from combining Theorem 1.1 (or an analogous theorem for minimization queries) with known differentially private algorithms.\nCompared to the results of Dwork et al., Theorem 1.1 requires a weaker privacy guarantee: we require (α,α)-differential privacy, instead of (roughly) (α, (β/k)1/α)-differential privacy. It also generalizes this phenomenon beyond statistical queries.\nOur analysis differs from that of Dwork et al. in two key ways. First, Dwork et al. analyze the algorithm one query at a time. That is, they show that if the oracle is differentially private for suitable parameters, then the oracle answers each query qj accurately. In order to argue that the oracle answers every query accurately (even with just a non-zero probability), they need to prove that each query is answered accurately with very high probability. To do so, they rely on an involved analysis that is heavily tailored to statistical queries and introduces a poor dependence on the privacy parameters.\nWe give a simple proof that, if the oracle is differentially private, then it answers every query accurately with at least constant probability. As a feature of its simplicity, the analysis generalizes to richer families of queries. Our proof introduces the concept of a monitoring algorithm. Roughly, the monitoring algorithm simulates the interaction between the oracle and the adversary and outputs a query based on the entire interaction, thereby reducing analyzing the entire interaction to analyzing just the monitoring algorithm. In our setting, we will have the monitoring algorithm output the least accurate query in the whole interaction. Thus, if there were even one query in the interaction that is inaccurate, then the monitoring algorithm would output a single query that is inaccurate. By a simple argument, there is at least a constant probability that the monitor’s query is answered accurately, so there is also a constant probability that every query in the interaction is answered accurately.\nThe next step is to amplify the probability of success. (Dwork et al. avoid this step.) In the non-adaptive setting, amplification can be easily achieved by running the oracle a few times on independent samples, and taking some sort of median of its answers. However, in the adaptive setting, even if each sample is independent, the answers of the oracle may not be independent. Instead, we have the oracle split its sample into a small number of subsamples. To answer any given query, it will use only one of the subsamples. We then design a “censor” algorithm that can identify when the oracle gives an incorrect answer, and when this occurs, the oracle will throw out that subsample and use a new one. For any given subsample, the oracle will answer every query correctly with constant probability, therefore with high probability we only need to throw out a small number of subsamples, and thus are unlikely to exhaust the supply of subsamples."
    }, {
      "heading" : "1.3 Related Work in Differential Privacy",
      "text" : "Each of our results requires instantiating the oracle with a suitable differentially private algorithm. For statistical queries, the optimal mechanisms are the well known Gaussian Mecha-\nnism (slightly refined by [SU15]) when k is small and the Private Multiplicative Weights Mechanism [HR10] when k is large. For arbitrary low-sensitivity queries, the Laplace Mechanism is again optimal when k is small, and for large k we can use the Median Mechanism [RR10].\nWhen considering arbitrary search queries over an arbitrary finite range, the optimal algorithm is the Exponential Mechanism [MT07]. The first efficient differentially private algorithms for the important special case of convex minimization queries over an infinite domain were given by Dwork and Lei [DL09] and by Chaudhuri, Monteleone, and Sarwate [CMS11], leading to a long line of subsequent work. For the case of small k, we use the worst-case optimal algorithm of Bassily, Smith, and Thakurta [BST14]. When k is large, we can use an algorithm of [Ull15] that accurately answers exponentially many such queries."
    }, {
      "heading" : "2 Preliminaries",
      "text" : ""
    }, {
      "heading" : "2.1 Queries",
      "text" : "Given a distribution P over X or a sample x = (x1, · · · ,xn) ∈ X n, we would like to answer queries about P or x from some family Q. We will work with several families of queries.\n• Statistical Queries: These queries are specified by a predicate q : X → [0,1], and (abusing notation) are defined as\nq(P ) = E z←RP [q(z)] , q(x) = 1 n ∑ i∈[n] q(xi).\nThe error of an answer a to a statistical query q with respect to P or x is defined to be\nerrx (q,a) = a− q(x) and errP (q,a) = a− q(P ).\n• ∆-Sensitive Queries: For ∆ ∈ [0,1], n ∈ N, these queries are specified by a function q : X n→ R satisfying |q(x)− q(x′)| ≤ ∆ for every pair x,x′ ∈ X n differing in only one entry. We define (abusing notation)\nq(P ) = E z←RP n [q(z)] .\nThe error of an answer a to a ∆-sensitive query q with respect to P or x is defined to be\nerrx (q,a) = a− q(x) and errP (q,a) = E z←RP n [errz (q,a)] = a− q(P ).\nWe denote the set of all ∆-sensitive queries by Q∆. If ∆ = O(1/n) we say the query is low sensitivity. Note that 1/n-sensitive queries are a strict generalization of statistical queries.\n• Minimization Queries: These queries are specified by a loss function L : X n×Θ→R. We require that L has sensitivity ∆ with respect to its first parameter, that is,\nsup θ∈Θ, x,x′∈X n s.t. dHam(x,x′)=1\n|L(x;θ)−L(x′;θ)| ≤ ∆ .\nHere Θ is an arbitrary set of items (sometimes called “parameter values”) among which we aim to chose the parameter with minimal loss, either with respect to a particular input data set x, or with respect to expectation over a distribution P .\nThe error of an answer θ ∈Θ to a minimization query L : X n ×Θ→R with respect to x is defined to be\nerrx (L,θ) = L(x,θ)−min θ∗∈Θ\nL(x,θ∗)\nand, with respect to P , is\nerrP (L,θ) = E z←RP n [errz (L,θ)] = E z←RP n [L(z,θ)]− E z←RP n [ min θ∗∈Θ L(z,θ∗) ] .\nNote that minθ∗∈Θ E z←RP n [L(z,θ∗)] ≥ E z←RP n [minθ∗∈Θ L(z,θ∗)], whence\nE z←RP n [L(z,θ)]−min θ∗∈Θ E z←RP n\n[L(z,θ∗)] ≤ errP (L,θ) .\nNote that minimization queries (with Θ = R) generalize low-sensitivity queries: Given a ∆-sensitive q : X n → R, we can define L(x,θ) = |θ − q(x)| to obtain a minimization query with the same answer.\nWe denote the set of minimization queries by Qmin. We highlight two special cases:\n– Minimization for Finite Sets We denote by Qmin,D the set of minimization queries where Θ is finite with size at most D.\n– Convex Minimization Queries If Θ ⊂ Rd is closed and convex and L(x; ·) is convex on Θ for every data set x, then the query can be answered nonprivately up to any desired error α, in time polynomial in d and α. We denote the set of all convex minimization queries by QCM."
    }, {
      "heading" : "2.2 Oracles for Adaptive Queries",
      "text" : "Our goal is to design an oracle O that answers queries on P using only independent samples x1, . . . ,xn←R P . Our focus is the case where the queries are chosen adaptively and adversarially.\nSpecifically, O is a stateful algorithm that holds a collection of samples x1, . . . ,xn ∈ X , takes a query q from some family Q as input, and returns an answer a. We require that when x1, . . . ,xn are independent samples from P , the answer a is “close” to q(P ) in a sense that is appropriate for the family of queries. Moreover we require that this condition holds for every query in an adaptively chosen sequence q1, . . . , qk . Formally, we define the an accuracy game between an O and a stateful adversary A in Figure 2.\nDefinition 2.1 (Accuracy). An oracle O is (α,β)-accurate with respect to the population for k adaptively chosen queries from Q given n samples in X if for every adversary A,\nP Accn,k,Q[O,A] [ max j∈[k] ∣∣∣∣errP (qj , aj)∣∣∣∣ ≤ α] ≥ 1− β. We will make use of average accuracy.\nDefinition 2.2 (Average Accuracy). An oracle O is α-accurate on average with respect to the population for k adaptively chosen queries from Q given n samples in X if for every adversary A,\nE Accn,k,Q[O,A] [ max j∈[k] ∣∣∣∣errP (qj , aj)∣∣∣∣] ≤ α. We will also use a definition of accuracy relative to the sample given to the oracle (Figure 3).\nDefinition 2.3 (Sample Accuracy). An oracle O is (α,β)-accurate with respect to samples of size n from X for k adaptively chosen queries from Q if for every adversary A,\nP SampAccn,k,Q[O,A]\n[ ∀j ∈ [k] ∣∣∣∣errx (qj , aj)∣∣∣∣ ≤ α] ≥ 1− β. Definition 2.4 (Average Sample Accuracy). An oracle O is α-accurate on average with respect to samples of size n from X for k adaptively chosen queries from Q if for every adversary A,\nE SampAccn,k,Q[O,A] [ max j∈[k] ∣∣∣∣errx (qj , aj)∣∣∣∣] ≤ α. By Markov’s inequality, α′-accuracy on average implies (α,β) accuracy for suitable choices\nof α,β:\nFact 2.5. For all α,β > 0, ifO is αβ-accurate on average with respect to the population (resp. sample) for k adaptively chosen queries from Q given n samples in X , then O is (α,β)-accurate with respect to the population (resp. sample) for k adaptively chosen queries from Q given n samples in X .\nBecause of the dependence on β in the assumption, we only apply this result when β = Ω(1)."
    }, {
      "heading" : "2.3 Differential Privacy",
      "text" : "A key tool in our analysis is differential privacy [DMNS06]. Informally, an algorithm is differentially private if it is randomized and the distribution of its outputs does not depend “much” on any one element of its input sample. We say that two samples x,x′ ∈ X n are neighbors if they differ on exactly one entry, and denote this relation by x ∼ x′.\nDefinition 2.6 (Differential Privacy). LetM : X n→R be a randomized algorithm. We sayM is (ε,δ)-differentially private if for every x ∼ x′ and every R ⊆R,\nP [M(x) ∈ R] ≤ eε ·P [ M(x′) ∈ R ] + δ.\nA useful fact about differential privacy is that it is robust to post-processing.\nLemma 2.7 ([DMNS06]). If M : X n → R is (ε,δ)-differentially private, and f : R → R′ is any (possibly randomized) function, then f ◦M : X n→R′ is also (ε,δ)-differentially private."
    }, {
      "heading" : "3 From Privacy to Accuracy for Adaptive Queries",
      "text" : "We begin by showing that differentially private algorithms that answer adaptive low-sensitivity queries and are accurate with respect to the sample, are accurate with respect to the population."
    }, {
      "heading" : "3.1 Differential Privacy Implies Low Expected Error",
      "text" : "We start with the following key lemma, which analyzes the expectation of q(x) versus q(P ) when q is allowed to depend on x in a differentially private manner.\nLemma 3.1. Let M : X n → Q∆ be (ε,δ)-differentially private where Q∆ is the class of ∆-sensitive queries q : X n→R. Let P be a distribution on X and let x←R P n. Then∣∣∣∣∣ Ex,M [q(P ) | q =M(x)]− Ex,M [q(x) | q =M(x)]\n∣∣∣∣∣ ≤ 2∆(eε − 1 + δ)n. Proof. The proof proceeds via a hybrid argument. Let x′ ←R P n be independent of x. For ` ∈ [n]∪ {0}, define x` ∈ X n by\nx`i = xi if i > ` and x ` i = x ′ i if i ≤ `.\nThen x0 = x, xn = x′, and x` ∼ x`−1 are neighboring databases for every ` ∈ [n]. For ` ∈ [n], define B` : X n ×X n→R by\nB`(y,z) = q(z)− q(z−`) +∆ for q =M(y),\nwhere z−` is z with the `-th row replaced by some arbitrary fixed element of X .\nSince x`−` = x `−1 −` , we have∣∣∣∣E [q(P ) | q =M(x)]−E [q(x) | q =M(x)]∣∣∣∣ =\n∣∣∣∣E[q(x′) | q =M(x)]−E [q(x) | q =M(x)]∣∣∣∣ ≤\n∑ `∈[n] ∣∣∣∣E [q(x`) | q =M(x)]−E [q(x`−1) | q =M(x)]∣∣∣∣ =\n∑ `∈[n] ∣∣∣∣E [(q(x`)− q(x`−`) +∆)− (q(x`−1)− q(x`−1−` ) +∆) | q =M(x)]∣∣∣∣ =\n∑ `∈[n] ∣∣∣∣E [B`(x,x`)−B`(x,x`−1)]∣∣∣∣ . Thus, it suffices to show that\n∣∣∣∣E [B`(x,x`)−B`(x,x`−1)]∣∣∣∣ ≤ 2∆(eε − 1 + δ) for all ` ∈ [n]. Since q is ∆-sensitive, for every `,y,z, we have 0 ≤ B`(y,z) ≤ 2∆. Moreover, by construction, B`(y,z) is a ( ,δ)-differentially private function of y. We will use these two facts to prove the desired bound.\nWe need one more observation to complete the proof. Note that each x` has the same marginal distribution, namely n independent samples from P . Thus, x` has the same marginal distribution as x (although they are not independent). Moreover, for every ` ∈ [n] ∪ {0}, the pair (x`,x) has the same distribution as (x,x`): Each corresponding pair of rows (x`i ,xi) is independent from the other rows. For i ≤ `, x`i and xi are independent samples from P . For i > `, x`i = xi and they are a sample from P .\nConsider the random variables B`(x,x`) and B`(x,x`−1) for some ` ∈ [n]. Now we use privacy and the earlier observation:\nB`(x,x `) ∼ B`(x`,x) ∼(ε,δ) B`(x`−1,x) ∼ B`(x,x`−1),\nwhere ∼ denotes having the same distribution and ∼(ε,δ) denotes having (ε,δ)-indistinguishable distributions.1 Thus B`(x,x`−1) and B`(x,x`) are (ε,δ)-indistinguishable.\nWe have\nE [ B`(x,x `−1) ] = ∫ 2∆\n0 P\n[ B`(x,x `−1) ≥ t ] dt\n≤ ∫ 2∆\n0\n( eεP [ B`(x,x `) ≥ t ] + δ ) dt\n=eε ∫ 2∆\n0 P\n[ B`(x,x `) ≥ t ] dt + 2∆δ\n=eεE [ B`(x,x `) ] + 2∆δ\n≤E [ B`(x,x `) ] + 2∆(eε − 1 + δ),\nas E [ B`(x,x`) ] ≤ 2∆. This gives one half of the result, the other side is symmetric.\n1In the spirit of differential privacy, distributions A and B over R are (ε,δ)-indistinguishable if for every (measurable) R ⊆R, P [A ∈ R] ≤ eε ·P [B ∈ R] + δ and vice versa.\nNow that we have Lemma 3.1, we can prove the following result that differentially private oracles that are accurate with respect to their sample are also accurate with respect to the population from which that sample was drawn.\nTheorem 3.2. Assume that O is\n1. (ε,δ)-differentially private for ε = δ = α/10∆n ≤ 1/4, and\n2. α/2-accurate on average with respect to samples of size n fromX for k adaptively chosen queries from Q ⊆Q∆ (the family of ∆-sensitive queries on X ).\nThen O is α-accurate on average with respect to the population for k adaptively chosen queries from Q given n samples from X .\nProof. Suppose, for the sake of contradiction, that O is not α-accurate on average with respect to the population for k adaptively chosen queries from Q given n samples from X . That is, there exists an adversary A such that\nE Accn,k,Q[O,A] [ max j∈[k] ∣∣∣∣errP (qj , aj)∣∣∣∣] > α. Fix such an adversaryA and the distribution P that it outputs. Now define a meta algorithm M, which we call the monitor. Note thatM has access to the “true” answers with respect to P .\nAlgorithmM =M(O,A,P ) : X n→Q∆. Draw a sample x ∈ X n from P and let O = O(x, ·) have this sample. Simulate A and O interacting. Let q1, · · · ,qk ∈Q be the queries given by A. Let a1, · · · , ak ∈R be the corresponding answers given by O. Pick the j ∈ [k] that maximizes\n∣∣∣∣errP (qj , aj)∣∣∣∣. If aj − qj(P ) ≥ 0, output qj and halt. If aj − qj(P ) < 0, output −qj and halt.\n(Note that Q∆ is closed under negation.)\nClaim 3.3. M is (ε,δ)-differentially private.\nProof. This follows because O is (ε,δ)-differentially private andM is a post-processing of the output of O (Lemma 2.7).\nClaim 3.4. E x,M [q(x)− q(P ) | q =M(x)] > α/2.\nProof. Let q be the query that M(x) returns and a the corresponding answer from O. By the assumption that O is not α-accurate on average with respect to the population,\nE x,M [a− q(P )] > α.\nThe assumption that O is α/2-accurate on average with respect to the sample implies\nE M [|a− q(x)|] ≤ α/2.\nThus E x,M [q(x)− q(P ) | q =M(x)] = E x,M [a− q(P )]− E x,M [a− q(x)] > α −α/2 = α/2,\nas required.\nNow combine Claim 3.4 and Lemma 3.1 to get\nα 2 < E x,M [q(x)− q(P ) | q =M(x)] ≤ 2∆n(eε − 1 + δ).\nNow we use the assumption that ε,δ = α/10∆n ≤ 1/4. We have eε − 1 ≤ 54ε ≤ α/8∆n. Thus 2∆n(eε − 1 + δ) ≤ 2∆n(α/8∆n+α/10∆n) < α/2, a contradiction. This completes the proof."
    }, {
      "heading" : "3.2 Amplifying the Success Probability",
      "text" : "Theorem 3.2 only gives an average error guarantee, where in applications we would like a high confidence bound on the error.\nBy Markov’s Inequality (Fact 2.5) the maximum error is bounded with at least constant probability. And we will amplify this probability by “serial repetition.” We will run the oracle O along with a “censor” C that ensures that the oracle is giving accurate answers. If C detects that O has given an inaccurate answer, then a fresh sample is drawn and a new instance of O is started. Since O has a constant probability of giving good answers for all k queries, the number of times a fresh sample is needed is bounded by O(log(1/β)) with probability at least 1− β, as required. The challenge is constructing a suitable censor C, that can detect when the oracle is inaccurate using only a small number of samples. We define such a censor as follows.\nDefinition 3.5 (Bounded Censor). An oracle C is a β-sound c-bounded censor for k sensitivity-∆ queries with threshold α given n samples in X if, for every adversaryA, the algorithm answers every query qj with aj ∈ {>,⊥,?} and\nP Accn,k,Q∆ [C,A]\n∀j ∈ [k]  aj => =⇒ ∣∣∣qj(P )∣∣∣ > α2 aj =⊥ =⇒ ∣∣∣qj(P )∣∣∣ ≤ α aj = ? ⇐⇒ |{i ∈ [j − 1] : ai =>}| ≥ c   ≥ 1− β.\nIntuitively, a bounded censor is an algorithm which answers k sensitivity-∆ queries and approximately determines whether or not they are above a threshold. It outputs > if the answer is above the threshold and ⊥ if the answer is below the threshold. However, the bounded censor may cease to give useful answers after c query answers have been above the threshold. Notice that a bounded censor is a weaker object than an accurate oracle, so there is no circularity in our approach of using a bounded censor to boost the accuracy of the oracle.\nFirst, we will show that a bounded censor with suitable parameters, plus an oracle that is accurate on average, implies the existence of an oracle that is accurate with high probability. We do so by way of the construction in Figure 4.\nNote that the semantics of F O,C are a bit different from the semantics of an oracle defined in Section 2.2. For notational convenience, we have written F as if it has access to a sampling\noracle for P , rather than a finite sample. Moreover, a new sample of size n is drawn every time âj = >, which can potentially occur k times, meaning that the number of samples drawn can be as large as n′ + n · k. However, since our analysis shows that with probability at least 1 − β, âj => at most c−1 times. Thus, we could easily rewrite the algorithm to have a finite sample of size n′ +nċ, which is split into one sample of size n′ and c samples of size n, and the algorithm will halt if it exhausts its set of samples, which occurs with probability at most β.\nTheorem 3.6. Assume that\n1. O is α/4-accurate on average with respect to the population for k adaptively chosen queries from Q ⊆Q∆ given n samples in X , and\n2. C is a β/2-sound c-bounded censor for k + c sensitivity-∆ queries with threshold α given n′ samples in X , where c = dlog2(4/β)e.\nThen F O,C is (α,β)-accurate with respect to the population for k adaptively chosen queries from Q given n′ +n · c samples in X .\nWe will show (in Theorem 3.7) that we only need n′ =O(n+ log(1/β) · logk). Thus Theorem 3.6 states that we can amplify from α/4 expected accuracy to (α,β)-accuracy with a log(1/β) factor increase in sample complexity.\nProof of Theorem 3.6. By Definition 3.5 and the fact that E z←RP n\n[ q̂j(z) ] = errP ( qj , aj ) , with proba-\nbility at least 1− β/2, for all j ∈ [k], (i) âj =⊥ implies ∣∣∣∣errP (qj , aj)∣∣∣∣ ≤ α,\n(ii) âj => implies ∣∣∣∣errP (qj , aj)∣∣∣∣ > α2 , and\n(iii) âj = ? implies ∣∣∣∣{j ∈ [k] : âj =>}∣∣∣∣ ≥ c.\nWe condition on the event that these statements hold. By condition (i), for each j ∈ [k],∣∣∣∣errP (qj , aj)∣∣∣∣ ≤ α, so F O,C only outputs accurate answers, as required.\nBy condition (iii), F O,C only halts early if the number of times > is returned by C exceeds c − 1. Note that halting early ensures that the number of samples needed by F O,C is bounded by n′ +n · c, as required.\nIt remains to show that the number of times> is returned by C is at most c−1 with probability at least 1− β/2. By condition (ii), > is only returned by C only when ∣∣∣∣errP (qj , aj)∣∣∣∣ > α2 . However, by Fact 2.5, O is (α/2,1/2)-accurate with respect to the population for k adaptively chosen queries from Q given n samples in X . This means that the probability that\n∣∣∣∣errP (qj , aj)∣∣∣∣ > α2 for any j ∈ [k] is at most 1/2, where the probability is over only the coins of O and the choice of x. Every time > is returned by C, the coins of O and its sample x are redrawn independently. Thus, the probability that > is returned m or more times is at most 2−m. In particular, the probability that > is returned c − 1 or more times is at most β/2, as required."
    }, {
      "heading" : "3.2.1 The Bounded Censor Algorithm",
      "text" : "Now we turn our attention to constructing an appropriate bounded censor C, which is given in Figure 5. This construction is inspired by the sparse vector algorithm [DNR+09, HR10, DNPR10, RR10] and the analysis is inspired by [Rot14].\nTheorem 3.7. The Bounded Censor algorithm C(α,n,c,k) in Figure 5 is a β-sound c-bounded censor for k sensitivity-∆ queries with threshold α given n samples in X assuming\nn ≥ 8(n∆) 2\nα2 (log(2/β) + (c+ 1) · log(k + 1)) .\nIn the common scenario where we have ∆ =O(1/n), the bounded censor C only requires n ≥O (\nlog(1/β) + c · logk α2\n) .\nWhen C is invoked in Theorem 3.6, we have c =O(log(1/β)). So the bounded censor only needs n ≥O(log(1/β) · log(k)/α2), which is small enough not to be a bottleneck in any of our bounds.\nProof of Theorem 3.7. Fix an adversary A. Without loss of generality, we may assume that A is deterministic. By Definition 3.5, we must verify that\nP Accn,k,Q∆ [C,A]\n∀j ∈ [k]  aj => =⇒ ∣∣∣qj(P )∣∣∣ > α2 aj =⊥ =⇒ ∣∣∣qj(P )∣∣∣ ≤ α aj = ? =⇒ |{i ∈ [j − 1] : ai =>}| ≥ c   ≥ 1− β.\nIt suffices to show that, with probability at least 1−β, for every query q thatA could potentially ask2, |q(x)−q(P )| ≤ α/4. We will prove in two steps: (i) We show that for any fixed single query q we have |q(x) − q(P )| ≤ α/4 with high probability over the sample x. And (ii) we bound the number of queries that A could potentially ask and apply a union bound. Note that we must take a union bound over all the queries that A could potentially ask, not just the queries that A does ask in a particular instance of Accn,k,Q∆[C,A].\nFor (i), we use the following standard concentration inequality.\nLemma 3.8 (McDiarmid’s Inequality [McD89]). Let X1, · · · ,Xn ∈ X be independent random variables and f : X n→R. Suppose f is ∆-sensitive—that is,\n∀x ∈ X n ∀i ∈ [n] ∀y ∈ X ∣∣∣f (x1, · · · ,xi−1,xi ,xi+1, · · · ,xn)− f (x1, · · · ,xi−1, y,xi+1, · · · ,xn)∣∣∣ ≤ ∆.\nThen\n∀λ > 0 P [ f (X1, · · · ,Xn)−E [f (X1, · · · ,Xn)] > λ ] ≤ exp ( −2λ2\nn∆2\n) .\nThus, for any q ∈Q∆, we have\nP x←RP n\n[|q(x)− q(P )| > α/4] ≤ 2 · exp ( −α2\n8∆2n\n) . (1)\nFor (ii): Since A is deterministic, the queries it asks during Accn,k,Q∆[C,A] are determined by the answers given by C. In particular, each query A potentially asks can be specified by a string s ∈ ⋃k `=1{>,⊥}\n` of length at most k with at most c occurances of >. This string represents the answers given by C up until the time the query is asked. (Note that we omit the final ? symbol, as is it immaterial.) By counting the number of such strings, we see that the number of possible queries A can ask is bounded by (k + 1)c+1. Thus we take a union bound over the (k + 1)c+1 potential queries of A.\nBy applying this union bound to the bound in (1), the overall failure probability of C is bounded by\n2 · exp ( −α2\n8∆2n\n) · (k + 1)c+1 ≤ β,\nas required.\nCombining our theorem about accuracy on average (Theorem 3.2) with our amplification technique from the previous section (Theorems 3.6 and 3.7) gives the following bounds.\nTheorem 3.9. Assume that O is\n1. (ε,δ)-differentially private for ε = δ = α/40∆n, and\n2. α/8-accurate on average with respect to samples of size n fromX for k adaptively chosen queries from Q ⊆Q∆ (the family of ∆-sensitive queries on X ).\nThen there exists an oracle O′ that is (α,β)-accurate with respect to the population for k adaptively chosen queries fromQ givenO((n+(n∆/α)2 ·logk)·log(1/β)) samples from X . Moreover, the running time of O′ is at most a O(log(1/β)) factor more than that of O.\n2Excluding the queries that are only asked after the t ≥ c condition is reached."
    }, {
      "heading" : "4 From Function Approximation to Optimization Queries",
      "text" : "We now extend our results for low-sensitivity queries to more general minimization queries."
    }, {
      "heading" : "4.1 Oracles for Adaptive Minimization Queries",
      "text" : "Analogous to Theorem 3.2, we can show that an oracle that answers minimization queries on its input and is also differentially private gives answers that generalize to the whole distribution.\nTheorem 4.1. Assume that the oracle O is\n• (ε,δ)-differentially private for ε = δ = α20∆n ≤ 1 2 and,\n• α/2-accurate on average with respect to samples of size n fromX for k adaptively chosen queries from Q ⊆Qmin (the family of ∆-sensitive convex minimization queries on X ).3\nThen O is α-accurate on average with respect to the population for k adaptively chosen queries from Q given n samples from X .\nProof. Suppose, for the sake of contradiction, that O is not α-accurate on average with respect to the population for k adaptively chosen queries from Q given n samples from X . That is, there exists an adversary A such that\nE Accn,k,Q[O,A] [ max j∈[k] ∣∣∣∣errP (Lj ,θj)∣∣∣∣] > α. Fix such an adversary A and the distribution P that it outputs. Now define a monitor M as follows.\nAlgorithmM =M(O,A,P ) : X n→L×Θ. Draw a sample x ∈ X n from P and let O = O(x, ·) have this sample. Simulate A and O interacting. Let L1, · · · ,Lk be the loss functions given by A. Let θ1, · · · ,θk be the corresponding answers given by O. Choose j ∈ [k] that maximizes errP ( Lj ,θj ) .\nOutput (Lj ,θj ).\nAs for Theorem 3.2, we prove the theorem via a series of claims. We omit proofs where they are sufficiently similar to those in the proof of Theorem 3.2.\nClaim 4.2. M is (ε,δ)-differentially private.\nClaim 4.3. E x,M\n[ errP (L,θ) | (L,θ) =M(x) ] > α.\nProof. This follows from our supposition that O is not accurate with respect to the population and the fact that errP (L,θ) ≥ 0 for all L and θ.\n3Recall that for many natural minimization queries, the sensitivity ∆ scales as 1/n, so that ∆n is a constant.\nClaim 4.4. For all x ∈ X n, E M [errx (L,θ) | (L,θ) =M(x)] ≤ α 2 .\nProof. This follows from our assumption that O is accurate with respect to the sample.\nClaim 4.5. Define q : X n → R by q(x) = errx (L,θ) for fixed values of L : X n ×Θ → R and θ ∈ Θ. Then we have q is 2∆-sensitive.\nProof. Let x,x′ ∈ X n be neighboring and θ̂ = argminθ∗∈Θ L(x,θ∗). Since L is ∆-sensitive,\nq(x) = L(x,θ)−min θ∗∈Θ\nL(x,θ∗) = L(x,θ)−L(x, θ̂)\n≤ L(x′ ,θ)−L(x′ , θ̂) + 2∆ ≤ L(x′ ,θ)−min\nθ∗∈Θ L(x′ ,θ∗) + 2∆\n= q(x′) + 2∆.\nClaim 4.6.\nE M,x\n[ errP (L,θ) | (L,θ) =M(x) ] ≤ E M,x [errx (L,θ) | (L,θ) =M(x)] + 4∆n(eε − 1 + δ).\nProof.\nE M,x\n[ errP (L,θ) | (L,θ) =M(x) ] = E M,x [ E x′←RP n [errx′ (L,θ)] | (L,θ) =M(x) ] ≤ E M,x [errx (L,θ) | (L,θ) =M(x)]\n+ ∣∣∣∣∣ EM,x,x′ [errx′ (L,θ)− errx (L,θ) | (L,θ) =M(x)] ∣∣∣∣∣ ≤ E M,x [errx (L,θ) | (L,θ) =M(x)]\n+ 4∆n(eε − 1 + δ),\nwhere the final inequality follows from Lemma 3.1 and Claims 4.2 and 4.5.\nCombining Claims 4.3, 4.4, and 4.6 gives\nα < E M,x\n[ errP (L,θ) | (L,θ) =M(x) ] ≤ E M,x [errx (L,θ) | (L,θ) =M(x)] + 4∆n(eε − 1 + δ) ≤ α/2 + 4∆n(eε − 1 + δ).\nBy assumption eε − 1 ≤ 54ε ≤ α/16∆n and, hence, 4∆n(e ε − 1 + δ) ≤ α/4 + α/5 < α/2, which\ncompletes the proof by contradiction."
    }, {
      "heading" : "4.2 Amplifying the Success Probability",
      "text" : "Similar to Theorem 3.6, we can amplify the success probability of an algorithm for minimization queries.\nTheorem 4.7. Assume that\n1. O is α/4-accurate on average with respect to the population for k adaptively chosen queries from Q ⊆Qmin given n samples in X , and\n2. C is a β/2-sound c-bounded censor for k + c sensitivity-∆ queries with threshold α given n′ samples in X , where c = dlog2(4/β)e.\nThen F O,C is (α,β)-accurate with respect to the population for k adaptively chosen queries from Q given n′ +n · c samples in X .\nThe proof is almost identical to that of Theorem 3.6. The only difference is that, for a ∆-sensitive loss function Lj , the query to the censor q̂j(z) = errz ( Lj ,θj ) is now a 2∆-sensitive real-valued query (Claim 4.5). Combining Theorems 4.1, 4.7, and 3.7 gives the following.\nTheorem 4.8. Assume that O is\n1. (ε,δ)-differentially private for ε = δ = α/80∆n, and\n2. α/8-accurate on average with respect to samples of size n fromX for k adaptively chosen queries from Q ⊆Qmin.\nThen there exists an oracle O′ that is (α,β)-accurate with respect to the population for k adaptively chosen queries fromQ givenO((n+(n∆/α)2 ·logk)·log(1/β)) samples from X . Moreover, the running time of O′ is at most a O(log(1/β)) factor more than that of O."
    }, {
      "heading" : "5 Applications",
      "text" : ""
    }, {
      "heading" : "5.1 Low-Sensitivity and Statistical Queries",
      "text" : "We now plug known differentially private mechanisms in to Theorem 3.9 to obtain oracles that provide strong error guarantees with high probability for both low-sensitivity and statistical queries.\nCorollary 5.1 (Theorem 3.9 and [DMNS06, SU15]). There is an oracle O that is (α,β)-accurate with respect to the population for k adaptively chosen queries from Q∆ where ∆ = O(1/n) given n samples from X for\nn ≥O √k · loglogk · log(1/α) · log(1/β)α2  The oracle runs in time poly(n, log |X |, log(1/β)) per query.\nCorollary 5.2 (Theorem 3.9 and [RR10]). There is an oracle O that is (α,β)-accurate with respect to the population for k adaptively chosen queries from Q∆ where ∆ =O(1/n) given n samples from X for\nn =O  log |X | · logk ·√log(1/α) · log(1/β)α3 \nThe oracle runs in time poly(|X |n) per query. The case where ∆ is not O(1/n) can be handled by rescaling the output of the query.\nCorollary 5.3 (Theorem 3.9 and [HR10]). There is an oracle O that is α-accurate on average with respect to the population for k adaptively chosen queries from QSQ given n samples from X for\nn =O √log |X | · logk ·√log(1/α) · log(1/β)α3 \nThe oracle runs in time poly(n, |X |) per query."
    }, {
      "heading" : "5.2 Optimization Queries",
      "text" : "The results of the Section 4 can be combined with existing differentially private algorithms for minimizing “empirical risk” (that is, loss with respect to the sample x) to obtain algorithms for answering adaptive sequences of minimization queries. We provide a few specific instantiations here, based on known differentially private mechanisms."
    }, {
      "heading" : "5.2.1 Minimization Over Arbitrary Finite Sets",
      "text" : "Corollary 5.4 (Theorem 4.8 and [MT07]). Let Θ be a finite set of size at most D. Let Q ⊂ Qmin be the set of sensitivity-1/n loss functions bounded between 0 and C. Then there is an oracle O that is (α,β)-accurate with respect to the population for k adaptively chosen queries from Qmin given\nn ≥O  log(DC/α) ·√k · log(1/α) · log(1/β)α2  samples from X . The running time of the oracle is dominated by O((k + log(1/β)) ·D) evaluations of the loss function."
    }, {
      "heading" : "5.2.2 Convex Minimization",
      "text" : "We state bounds for convex minimization queries for some of the most common parameter regimes in applications. In the first two corollaries, we consider 1-Lipschitz4 loss functions over a bounded domain.\nCorollary 5.5 (Theorem 4.8 and [BST14]). Let Θ be a closed, convex subset of Rd set such that maxθ∈Θ ‖θ‖2 ≤ 1. LetQ ⊂Qmin be the set of convex 1-Lipschitz loss functions that are 1/n-sensitive. Then there is an oracle O that is (α,β)-accurate with respect to the population for k adaptively chosen queries from Q given\nn = Õ √dk · log(1/β)α2 \nsamples fromQ. The running time of the oracle is dominated by k ·n2 evaluations of the gradient ∇L. 4A loss function L : X ×Rd →R is 1-Lipschitz if for every θ,θ′ ∈Rd , x ∈ X , |L(θ,x)−L(θ′ ,x)| ≤ ‖θ −θ′‖2.\nCorollary 5.6 (Theorem 4.8 and [Ull15]). Let Θ be a closed, convex subset of Rd set such that maxθ∈Θ ‖θ‖2 ≤ 1. LetQ ⊂Qmin be the set of convex 1-Lipschitz loss functions that are 1/n-sensitive. Then there is an oracle O that is (α,β)-accurate with respect to the population for k adaptively chosen queries from Q given\nn = Õ √log |X | · (√d + logk) · log(1/β)α3 \nsamples from X . The running time of the oracle is dominated by poly(n, |X |) and k · n2 evaluations of the gradient ∇L.\nIn the next two corollaries, we consider 1-strongly convex5, Lipschitz loss functions over a bounded domain.\nCorollary 5.7 (Theorem 4.8 and [BST14]). Let Θ be a closed, convex subset of Rd set such that maxθ∈Θ ‖θ‖2 ≤ 1. Let Q ⊂ Qmin be the set of 1-strongly convex, 1-Lipschitz loss functions that are 1/n-sensitive. Then there is an oracle O that is (α,β)-accurate with respect to the population for k adaptively chosen queries from Q given\nn = Õ (√ dk\nα3/2 · log(1/β) ) samples from X . The running time of the oracle is dominated by k ·n2 evaluations of the gradient ∇L.\nCorollary 5.8 (Theorem 4.8 and [Ull15]). Let Θ be a closed, convex subset of Rd set such that maxθ∈Θ ‖θ‖2 ≤ 1. Let Q ⊂ Qmin be the set of 1-strongly convex 1-Lipschitz loss functions that are 1/n-sensitive. Then there is an oracle O that is (α,β)-accurate with respect to the population for k adaptively chosen queries from Q given\nn = Õ (√ log |X | · ( √ d\nα5/2 + logk α3\n) · log(1/β) ) samples from X . The running time of the oracle is dominated by poly(n, |X |) and k · n2 evaluations of the gradient ∇L."
    }, {
      "heading" : "Acknowledgements",
      "text" : "We thank Aaron Roth for suggesting the technique that we used to prove Theorem 3.7. We would also like to thank Mark Bun, Moritz Hardt, and Salil Vadhan for helpful discussions."
    } ],
    "references" : [ {
      "title" : "Stability and generalization",
      "author" : [ "Olivier Bousquet", "André Elisseeff" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Bousquet and Elisseeff.,? \\Q2002\\E",
      "shortCiteRegEx" : "Bousquet and Elisseeff.",
      "year" : 2002
    }, {
      "title" : "Controlling the false discovery rate: a practical and powerful approach to multiple testing",
      "author" : [ "Yoav Benjamini", "Yosef Hochberg" ],
      "venue" : "Journal of the Royal Statistical Society. Series B (Methodological),",
      "citeRegEx" : "Benjamini and Hochberg.,? \\Q1995\\E",
      "shortCiteRegEx" : "Benjamini and Hochberg.",
      "year" : 1995
    }, {
      "title" : "Bonferroni. Teoria statistica delle classi e calcolo delle probabilita",
      "author" : [ "Carlo Emilio" ],
      "venue" : "Pubbl. d. R. Ist. Super. di Sci. Econom. e Commerciali di Firenze.,",
      "citeRegEx" : "Emilio,? \\Q1936\\E",
      "shortCiteRegEx" : "Emilio",
      "year" : 1936
    }, {
      "title" : "Private empirical risk minimization: Efficient algorithms and tight error bounds",
      "author" : [ "Raef Bassily", "Adam Smith", "Abhradeep Thakurta" ],
      "venue" : "In FOCS,",
      "citeRegEx" : "Bassily et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Bassily et al\\.",
      "year" : 2014
    }, {
      "title" : "Fingerprinting codes and the price of approximate differential privacy",
      "author" : [ "Mark Bun", "Jonathan Ullman", "Salil P. Vadhan" ],
      "venue" : "In STOC,",
      "citeRegEx" : "Bun et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Bun et al\\.",
      "year" : 2014
    }, {
      "title" : "Differentially private empirical risk minimization",
      "author" : [ "Kamalika Chaudhuri", "Claire Monteleoni", "Anand D. Sarwate" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Chaudhuri et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Chaudhuri et al\\.",
      "year" : 2011
    }, {
      "title" : "Preserving statistical validity in adaptive data analysis",
      "author" : [ "Cynthia Dwork", "Vitaly Feldman", "Moritz Hardt", "Toniann Pitassi", "Omer Reingold", "Aaron Roth" ],
      "venue" : "In STOC. ACM,",
      "citeRegEx" : "Dwork et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Dwork et al\\.",
      "year" : 2015
    }, {
      "title" : "Differential privacy and robust statistics. In STOC, pages 371–380",
      "author" : [ "Cynthia Dwork", "Jing Lei" ],
      "venue" : "ACM, May 31–June",
      "citeRegEx" : "Dwork and Lei.,? \\Q2009\\E",
      "shortCiteRegEx" : "Dwork and Lei.",
      "year" : 2009
    }, {
      "title" : "Calibrating noise to sensitivity in private data analysis",
      "author" : [ "Cynthia Dwork", "Frank McSherry", "Kobbi Nissim", "Adam Smith" ],
      "venue" : "In TCC,",
      "citeRegEx" : "Dwork et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Dwork et al\\.",
      "year" : 2006
    }, {
      "title" : "Differential privacy under continual observation",
      "author" : [ "Cynthia Dwork", "Moni Naor", "Toniann Pitassi", "Guy N. Rothblum" ],
      "venue" : "In Symposium on Theory of Computing (STOC),",
      "citeRegEx" : "Dwork et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Dwork et al\\.",
      "year" : 2010
    }, {
      "title" : "On the complexity of differentially private data release: efficient algorithms and hardness results",
      "author" : [ "Cynthia Dwork", "Moni Naor", "Omer Reingold", "Guy N. Rothblum", "Salil P. Vadhan" ],
      "venue" : "In STOC,",
      "citeRegEx" : "Dwork et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Dwork et al\\.",
      "year" : 2009
    }, {
      "title" : "Multiple comparisons among means",
      "author" : [ "Olive Jean Dunn" ],
      "venue" : "Journal of the American Statistical Association,",
      "citeRegEx" : "Dunn.,? \\Q1961\\E",
      "shortCiteRegEx" : "Dunn.",
      "year" : 1961
    }, {
      "title" : "The garden of forking paths: Why multiple comparisons can be a problem, even when there is no “fishing expedition",
      "author" : [ "Andrew Gelman", "Eric Loken" ],
      "venue" : "or “phacking” and the research hypothesis was posited ahead of time. Manuscript.,",
      "citeRegEx" : "Gelman and Loken.,? \\Q2013\\E",
      "shortCiteRegEx" : "Gelman and Loken.",
      "year" : 2013
    }, {
      "title" : "A multiplicative weights mechanism for privacypreserving data analysis",
      "author" : [ "Moritz Hardt", "Guy Rothblum" ],
      "venue" : "In Proc. 51st Foundations of Computer Science (FOCS),",
      "citeRegEx" : "Hardt and Rothblum.,? \\Q2010\\E",
      "shortCiteRegEx" : "Hardt and Rothblum.",
      "year" : 2010
    }, {
      "title" : "Preventing false discovery in interactive data analysis is hard",
      "author" : [ "Moritz Hardt", "Jonathan Ullman" ],
      "venue" : "In FOCS. IEEE, October",
      "citeRegEx" : "Hardt and Ullman.,? \\Q2014\\E",
      "shortCiteRegEx" : "Hardt and Ullman.",
      "year" : 2014
    }, {
      "title" : "Why most published research findings are false",
      "author" : [ "John P.A. Ioannidis" ],
      "venue" : "PLoS Medicine,",
      "citeRegEx" : "Ioannidis.,? \\Q2005\\E",
      "shortCiteRegEx" : "Ioannidis.",
      "year" : 2005
    }, {
      "title" : "Efficient noise-tolerant learning from statistical queries",
      "author" : [ "Michael J. Kearns" ],
      "venue" : "In STOC, pages 392–401",
      "citeRegEx" : "Kearns.,? \\Q1993\\E",
      "shortCiteRegEx" : "Kearns.",
      "year" : 1993
    }, {
      "title" : "On the method of bounded differences",
      "author" : [ "Colin McDiarmid" ],
      "venue" : "Surveys in combinatorics,",
      "citeRegEx" : "McDiarmid.,? \\Q1989\\E",
      "shortCiteRegEx" : "McDiarmid.",
      "year" : 1989
    }, {
      "title" : "Mechanism design via differential privacy",
      "author" : [ "Frank McSherry", "Kunal Talwar" ],
      "venue" : "In FOCS,",
      "citeRegEx" : "McSherry and Talwar.,? \\Q2007\\E",
      "shortCiteRegEx" : "McSherry and Talwar.",
      "year" : 2007
    }, {
      "title" : "Interactive privacy via the median mechanism",
      "author" : [ "Aaron Roth", "Tim Roughgarden" ],
      "venue" : "In STOC, pages 765–774",
      "citeRegEx" : "Roth and Roughgarden.,? \\Q2010\\E",
      "shortCiteRegEx" : "Roth and Roughgarden.",
      "year" : 2010
    }, {
      "title" : "Learnability, stability and uniform convergence",
      "author" : [ "Shai Shalev-Shwartz", "Ohad Shamir", "Nathan Srebro", "Karthik Sridharan" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Shalev.Shwartz et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Shalev.Shwartz et al\\.",
      "year" : 2010
    }, {
      "title" : "Interactive fingerprinting codes and the hardness of preventing false discovery",
      "author" : [ "Thomas Steinke", "Jonathan Ullman" ],
      "venue" : "CoRR, abs/1410.1228,",
      "citeRegEx" : "Steinke and Ullman.,? \\Q2014\\E",
      "shortCiteRegEx" : "Steinke and Ullman.",
      "year" : 2014
    }, {
      "title" : "Between pure and approximate differential privacy",
      "author" : [ "Thomas Steinke", "Jonathan Ullman" ],
      "venue" : "CoRR, abs/1501.06095,",
      "citeRegEx" : "Steinke and Ullman.,? \\Q2015\\E",
      "shortCiteRegEx" : "Steinke and Ullman.",
      "year" : 2015
    }, {
      "title" : "Answering n2+o(1) counting queries with differential privacy is hard",
      "author" : [ "Jonathan Ullman" ],
      "venue" : "In STOC, pages 361–370",
      "citeRegEx" : "Ullman.,? \\Q2013\\E",
      "shortCiteRegEx" : "Ullman.",
      "year" : 2013
    }, {
      "title" : "Private multiplicative weights beyond linear queries",
      "author" : [ "Jonathan Ullman" ],
      "venue" : "In PODS. ACM, May 31–June",
      "citeRegEx" : "Ullman.,? \\Q2015\\E",
      "shortCiteRegEx" : "Ullman.",
      "year" : 2015
    } ],
    "referenceMentions" : [ ],
    "year" : 2015,
    "abstractText" : "Adaptivity is an important feature of data analysis—typically the choice of questions asked about a dataset depends on previous interactions with the same dataset. However, generalization error is typically bounded in a non-adaptive model, where all questions are specified before the dataset is drawn. Recent work by Dwork et al. (STOC ’15) and Hardt and Ullman (FOCS ’14) initiated the formal study of this problem, and gave the first upper and lower bounds on the achievable generalization error for adaptive data analysis. Specifically, suppose there is an unknown distribution P and a set of n independent samples x is drawn from P . We seek an algorithm that, given x as input, “accurately” answers a sequence of adaptively chosen “queries” about the unknown distribution P . How many samples n must we draw from the distribution, as a function of the type of queries, the number of queries, and the desired level of accuracy? In this work we make two new contributions towards resolving this question: 1. We give upper bounds on the number of samples n that are needed to answer statistical queries that improve over the bounds of Dwork et al. 2. We prove the first upper bounds on the number of samples required to answer more general families of queries. These include arbitrary low-sensitivity queries and the important class of convex risk minimization queries. As in Dwork et al., our algorithms are based on a connection between differential privacy and generalization error, but we feel that our analysis is simpler and more modular, which may be useful for studying these questions in the future. ∗Pennsylvania State University, Department of Computer Science and Engineering. {bassily,asmith}@psu.edu †Harvard University School of Engineering and Applied Sciences. Supported by NSF grant CCF-1116616. tsteinke@seas.harvard.edu ‡Columbia University Department of Computer Science. Supported by a Junior Fellowship from the Simons Society of Fellows. jullman@cs.columbia.edu ar X iv :1 50 3. 04 84 3v 1 [ cs .L G ] 1 6 M ar 2 01 5",
    "creator" : "LaTeX with hyperref package"
  }
}