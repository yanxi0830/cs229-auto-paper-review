{
  "name" : "1003.0024.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Asymptotic Analysis of Generative Semi-Supervised Learning",
    "authors" : [ "Joshua V Dillon", "Krishnakumar Balasubramanian" ],
    "emails" : [ "jvdillon@gatech.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n00 3.\n00 24\nv1 [\ncs .L\nG ]\n2 6\nSemisupervised learning has emerged as a popular framework for improving modeling accuracy while controlling labeling cost. Based on an extension of stochastic composite likelihood we quantify the asymptotic accuracy of generative semi-supervised learning. In doing so, we complement distribution-free analysis by providing an alternative framework to measure the value associated with different labeling policies and resolve the fundamental question of how much data to label and in what manner. We demonstrate our approach with both simulation studies and real world experiments using naive Bayes for text classification and MRFs and CRFs for structured prediction in NLP."
    }, {
      "heading" : "1 Introduction",
      "text" : "Semisupervised learning (SSL) is a technique for estimating statistical models using both labeled and unlabeled data. It is particularly useful when the costs of obtaining labeled and unlabeled samples are different. In particular, assuming that unlabeled data is more easily available, SSL provides improved modeling accuracy by adding a large number of unlabeled samples to a relatively small labeled dataset.\nThe practical value of SSL has motivated several attempts to mathematically quantify its value beyond traditional supervised techniques. Of particular importance is the dependency of that improvement on the amount of unlabeled and labeled data. In the case of structured prediction the accuracy of the SSL estimator depends also on the specific manner in which sequences are labeled. Focusing on the framework of generative or likelihood-based SSL applied to classification and structured prediction we identify the following questions which we address in this paper. Q1: Consistency (classification). What combinations of labeled and unlabeled data lead to precise models in the limit of large data. Q2: Accuracy (classification). How can we quantitatively express the estimation accuracy for a particular generative model as a function of the amount of labeled and unlabeled data. What is the improvement in estimation accuracy resulting from replacing an unlabeled example with a labeled one. Q3: Consistency (structured prediction). What strategies for sequence labeling lead to precise models in the limit of large data. Q4: Accuracy (structured prediction). How can we quantitatively express the estimation quality for a particular model and structured labeling strategy. What is the improvement in estimation accuracy resulting from replacing one labeling strategy with another. Q5: Tradeoff (classification and structured prediction). How can we quantitatively express the tradeoff between the two competing goals of improved prediction accuracy and low labeling cost. What are the possible ways to resolve that tradeoff optimally within a problem-specific context.\n∗To whom correspondence should be addressed. Email: jvdillon@gatech.edu\nQ6: Practical Algorithms. How can we determine how much data to label in practical settings. The first five questions are of fundamental importance to SSL theory. Recent related work has concentrated on large deviation bounds for discriminative SSL as a response to Q1 and Q2 above. While enjoying broad applicability, such non-parametric bounds are weakened when the model family’s worst-case is atypical. By forgoing finite sample analysis, our approach complements these efforts and provides insights which apply to the specific generative models under consideration. In presenting answers to the last question, we reveal the relative merits of asymptotic analysis and how its employ, perhaps surprisingly, renders practical heuristics for controlling labeling cost.\nOur asymptotic derivations are possible by extending the recently proposed stochastic composite likelihood formalism [5] and showing that generative SSL is a special case of that extension. The implications of this analysis are demonstrated using a simulation study as well as text classification and NLP structured prediction experiments. The developed framework, however, is general enough to apply to any generative SSL problem. As in [7], the delta method transforms our results from parameter asymptotics to prediction risk asymptotics. We omit these results for lack of space."
    }, {
      "heading" : "2 Related Work",
      "text" : "Semisupervised learning has received much attention in the past decade. Perhaps the first study in this area was done by Castelli and Cover [3] who examined the convergence of the classification error rate as a labeled example is added to an unlabeled dataset drawn from a Gaussian mixture model. Nigam et al. [9] proposed a practical SSL framework based on maximizing the likelihood of the observed data. An edited volume describing more recent developments is [4].\nThe goal of theoretically quantifying the effect of SSL has recently gained increased attention. Sinha and Belkin [11] examined the effect of using unlabeled samples with imperfect models for mixture models. Balcan and Blum [1] and Singh et al. [10] analyze discriminative SSL using PAC theory and large deviation bounds. Additional analysis has been conducted under specific distributional assumptions such as the “cluster assumption”, “smoothness assumption” and the “low density assumption.”[4] However, many of these assumptions are criticized in [2].\nOur work complements the above studies in that we focus on generative as opposed to discriminative SSL. In contrast to most other studies, we derive model specific asymptotics as opposed to non-parametric large deviation bounds. While such bounds are helpful as they apply to a broad set of cases, they also provide less information than model-based analysis due to their generality. Our analysis, on the other hand, requires knowledge of the specific model family and an estimate of the model parameter. The resulting asymptotics, however, apply specifically to the case at hand without the need of potentially loose bounds.\nWe believe that our work is the first to consider and answer questions Q1-Q6 in the context of generative SSL. In particular, our work provides a new framework for examining the accuracy-cost SSL tradeoff in a way that is quantitative, practical, and model-specific."
    }, {
      "heading" : "3 Stochastic SSL Estimators",
      "text" : "Generative SSL [9, 4] estimates a parametric model by maximizing the observed likelihood incorporating L labeled and U unlabeled examples\nℓ(θ) =\nL ∑\ni=1\nlog pθ(X (i), Y (i)) +\nL+U ∑\ni=L+1\nlog pθ(X (i)) (1)\nwhere pθ(X (i)) above is obtained by marginalizing the latent label\n∑\ny pθ(X (i), y). A classical example is\nthe naive Bayes model in [9] where pθ(X,Y ) = pθ(X |Y )p(Y ), pθ(X |Y = y) = Mult([θy]1, . . . , [θy]V ). The framework, however, is general enough to apply to any generative model pθ(X,Y ).\nTo analyze the asymptotic behavior of the maximizer of (1) we assume that the ratio between labeled to unlabeled examples λ = L/(L + U) is kept constant while n = L + U → ∞. More generally, we assume a stochastic version of (1) where each one of the n samples X(1), . . . , X(n) is labeled with probability λ\nℓn(θ) = n ∑\ni=1\nZ(i) log pθ(X (i), Y (i)) +\nn ∑\ni=1\n(1− Z(i)) log pθ(X(i)), Z(i) ∼ Bin(1, λ). (2)\nThe variable Z(i) above is an indicator taking the value 1 with probability λ and 0 otherwise. Due to the law of large numbers for large n we will have approximately L = nλ labeled samples and U = n(1−λ) unlabeled samples thus achieving the asymptotic behavior of (1).\nEquation (2) is sufficient to handle the case of classification. However, in the case of structured prediction we may have sequencesX(i), Y (i) where for each i some components of the label sequence Y (i) are missing and some are observed. For example one label sequence may be completely observed, another may be completely unobserved, and a third may have the first half labeled and the second half not.\nMore formally, we assume the existence of a sequence labeling policy or strategy ℘ which maps label\nsequences Y (i) = (Y (i) 1 , . . . , Y (i) m ) to a subset corresponding to the observed labels ℘(Y (i)) ⊂ {Y (i)1 , . . . , Y (i) m }. To achieve full generality we allow the labeling policy ℘ to be stochastic, leading to different subsets of {Y (i)1 , . . . , Y (i) m } with different probabilities. A simple “all or nothing” labeling policy could label the entire sequence with probability λ and otherwise ignore it. Another policy may label the entire sequence, the first half, or ignore it completely with equal probabilities\n℘(Y )=\n\n \n \nY (i) 1 , . . . , Y (i) m with probability 1/3 ∅ with probability 1/3 Y\n(i) 1 , . . . , Y (i) ⌊m/2⌋ with probability 1/3\n. (3)\nWe thus have the following generalization of (2) for structured prediction\nℓn(θ) =\nn ∑\ni=1\nlog pθ(℘(Y (i)), X(i)). (4)\nEquation (4) generalizes standard SSL from all or nothing labeling to arbitrary labeling policies. The fundamental SSL question in this case is not simply what is the dependency of the estimation accuracy on n and λ. Rather we ask what is the dependency of the estimation accuracy on the labeling policy ℘. Of particular interest is the question what labeling policies ℘ achieve high estimation accuracy coupled with low labeling cost. Answering these questions leads to a generative SSL theory that quantitatively balances estimation accuracy and labeling cost.\nFinally, we note that both (2) and (4) are random variables whose outcomes depend on the random variables Z(1), . . . , Z(n) (for (2)) or ℘ (for (4)). Consequentially, the analysis of the maximizer θ̂n of (2) or (4) needs to be done in a probabilistic manner."
    }, {
      "heading" : "4 A1: Consistency (Classification)",
      "text" : "Assuming that the data is generated from pθ0(X,Y ) consistency corresponds to the convergence of\nθ̂n = argmax θ ℓn(θ) (5)\nto θ0 with probability 1 as n → ∞ (ℓn is defined in (2)). This implies that in the limit of large data our estimator would converge to the truth. Note that large data n → ∞ in this case means that both labeled and unlabeled data increase to ∞ (but their relative sizes remain the constant λ).\nWe show in this section that the maximizer of (2) is consistent assuming that λ > 0. This is not an unexpected conclusion but for the sake of completeness we prove it here rigorously. The proof technique will also be used later when we discuss consistency of SSL estimators for structured prediction.\nThe central idea in the proof is to cast the generative SSL estimation problem as an extension of stochastic composite likelihood [5]. Our proof follows similar lines to the consistency proof of [5] with the exception that it does not assume independence of the indicator functions Z(i) and (1 − Z(i)) as is assumed there. Definition 1. A distribution pθ(X,Y ) is said to be identifiable if θ 6= η entails that pθ(X,Y )− pη(X,Y ) is not identically zero.\nProposition 1. Let Θ ⊂ Rr be a compact set, and pθ(x, y) > 0 be identifiable and smooth in θ. Then if λ > 0 the maximizer θ̂n of (2) is consistent i.e., θ̂n → θ0 as n → ∞ with probability 1. Proof. The likelihood function, modified slightly by a linear combination with a constant is ℓ′n(θ) =\n1\nn\nn ∑\ni=1\n(\nZ(i) log pθ(X (i), Y (i))− λ log pθ0(X(i), Y (i))\n) + 1\nn\nn ∑\ni=1\n( (1− Z(i)) log pθ(X(i))− (1− λ) log pθ0(X(i)) ) ,\nconverges by the the strong law of large numbers as n → ∞ to its expectation with probability 1\nµ(θ) = −λD(pθ0(X,Y )||pθ(X,Y ))− (1− λ)D(pθ0(X)||pθ(X))).\nIf we restrict ourselves to the compact set S = {θ : c1 ≤ ‖θ− θ0‖ ≤ c2} then | log pθ(X,Y )| < K(X,Y ) < ∞, ∀θ ∈ S. As a result, the conditions for the uniform strong law of large numbers, cf. chapter 16 of [6], hold on S leading to\nP\n{\nlim n→∞ sup θ∈S\n|ℓ′n(θ)− µ(θ)| = 0 } = 1. (6)\nDue to the identifiability of pθ(X,Y ) we have D(pθ0(X,Y )||pθ(X,Y )) ≥ 0 with equality iff θ = θ0. Since also D(pθ0(X)||pθ(X))) ≥ 0 we have that µ(θ) ≤ 0 with equality iff θ = θ0 (assuming λ > 0). Furthermore, since the function µ(θ) is continuous it attains its negative supremum on the compact S: supθ∈S µ(θ) < 0.\nCombining this fact with (6) we have that there exists N such that for all n > N the likelihood maximizers on S achieves strictly negative values of ℓ′n(θ) with probability 1. However, since ℓ ′ n(θ) can be made to achieve values arbitrarily close to zero under θ = θ0, we have that θ̂n 6∈ S for n > N . Since c1, c2 were chosen arbitrarily θ̂n → θ0 with probability 1.\nThe above proposition is not surprising. As n → ∞ the number of labeled examples increase to ∞ and thus it remains to ensure that adding an increasing number of unlabeled examples does not hurt the estimator. More interesting is the quantitative description of the accuracy of θ̂n and its dependency on θ0, λ, n which we turn to next."
    }, {
      "heading" : "5 A2: Accuracy (Classification)",
      "text" : "The proposition below states that the distribution of the maximizer of (2) is asymptotically normal and\nprovides its variance which may be used to characterize the accuracy of θ̂n as a function of n, θ0, λ. As in Section 4 our proof proceeds by casting generative SSL as an extension of stochastic composite likelihood.\nIn Proposition 2 (below) and in Proposition 4 we use Var θ0(H) to denote the variance matrix of a random vector H under pθ0 . The notations\np→ , denote convergences in probability and in distribution [6] and ∇f(θ), ∇2f(θ) are the r × 1 gradient vector and r × r matrix of second order derivatives of f(θ). Proposition 2. Under the assumptions of Proposition 1 as well as convexity of Θ we have the following convergence in distribution of the maximizer of (2)\n√ n(θ̂n − θ0) N ( 0,Σ−1 )\n(7)\nas n → ∞, where Σ = λVar θ0(V1) + (1− λ)Var θ0(V2) V1 = ∇θ log pθ0(X,Y ), V2 = ∇θ log pθ0(X).\nProof. By the mean value theorem and convexity of Θ, there is η ∈ (0, 1) for which θ′=θ0 + η(θ̂n − θ0) and\n∇ℓn(θ̂n) = ∇ℓn(θ0) +∇2ℓn(θ′)(θ̂n − θ0).\nSince θ̂n maximizes ℓn we have ∇ℓn(θ̂n) = 0 and √ n(θ̂n − θ0) = − √ n ( ∇2ℓn(θ′) )−1 (∇ℓn(θ0)) . (8)\nBy Proposition 1 we have θ̂n p→ θ0 which implies that θ′ p→ θ0 as well. Furthermore, by the law of large numbers and the fact that Wn p→ W implies g(Wn) p→ g(W ) for continuous g,\n(∇2ℓn(θ′))−1 p→ (∇2ℓn(θ0))−1 (9) p→ ( λE θ0∇2 log pθ0(X,Y ) + (1− λ)E θ0∇2 log pθ0(X) )−1 = Σ−1\nwhere in the last equality we used a well known identity concerning the Fisher information. For the remaining term in the rhs of (8) we have\n−√n∇ℓn(θ0) = − √ n 1\nn\nn ∑\ni=1\n(W (i) +Q(i)) (10)\nwhere W (i) = Z(i)∇ log pθ0(X(i), Y (i)), Q(i) = (1 − Z(i))∇ log pθ0(X(i)). Since (10) is an average of iid random vectors W (i) +Q(i) it is asymptotically normal by the central limit theorem with mean\nE θ0(Q +W ) = λE θ0∇ log pθ0(X,Y ) + (1− λ)E∇ log pθ0(X) = λ0 + (1− λ)0. and variance\nVar θ0(W +Q) = E θ0W 2 + E θ0Q 2 + 2E θ0WQ\n= λVar θ0V1 + (1− λ)Var θ0V2 where we used E (Z(1− Z)) = EZ − EZ2 = 0 .\nWe have thus established that\n−√n∇ℓn(θ0) N(0,Σ). (11) We finish the proof by combining (8), (15) and (11) using Slutsky’s theorem.\nProposition 2 characterizes the asymptotic estimation accuracy using the matrix Σ. Two convenient one dimensional summaries of the accuracy are the trace and the determinant of Σ. In some simple cases (such as binary event naive Bayes) tr(Σ) can be brought to a mathematically simple form which exposes its dependency on θ0, n, λ. In other cases the dependency may be obtained using numerical computing.\nFigure 1 displays three error measures for the multinomial naive Bayes SSL classifier [9] and the Reuters RCV1 text classification data. In all three figures the error measures are represented as functions of n (horizontal axis) and λ (vertical axis). The error measures are classification error rate (left), trace of the empirical mse (middle), and log-trace of the asymptotic variance (right). The measures were obtained over held-out sets and averaged using cross validation. Figure 3 (middle) displays the asymptotic variance as a function of n and λ for a randomly drawn θ0.\nAs expected the measures decrease with n and λ in all the figures. It is interesting to note, however, that the shapes of the contour plots are very similar across the three different measures (top row). This confirms that the asymptotic variance (right) is a valid proxy for the finite sample measures of error rates and empirical mse. We thus conclude that the asymptotic variance is an attractive measure that is similar to finite sample error rate and at the same time has a convenient mathematical expression."
    }, {
      "heading" : "6 A3: Consistency (Structured)",
      "text" : "In the case of structured prediction the log-likelihood (4) is specified using a stochastic labeling policy. In this section we consider the conditions on that policy that ensures estimation consistency, or in other word convergence of the maximizer of (4) to θ0 as n → ∞.\nWe assume that the labeling policy ℘ is a probabilistic mixture of deterministic sequence labeling functions χ1, . . . , χk. In other words, ℘(Y ) takes values χi(Y ), i = 1, . . . , k with probabilities λ1, . . . , λk. For example the policy (3) corresponds to χ1(Y ) = Y , χ2(Y ) = ∅, χ3(Y ) = {Y1, . . . , Y⌊m/2⌋} (where Y = {Y1, . . . , Ym}) and λ = (1/3, 1/3, 1/3).\nUsing the above notation we can write (4) as\nℓn(θ) =\nn ∑\ni=1\nk ∑\nj=1\nZ (i) j log pθ(χj(Y (i)), X(i)) (12)\nZ(i) ∼ Mult(1, (λ1, . . . , λk))\nwhich exposes its similarity to the stochastic composite likelihood function in [5]. Note however that (12) is not formally a stochastic composite likelihood since Z (i) j , j = 1, . . . , k are not independent and since χj(Y ) depends on the length of the sequence Y (see for example χ1 and χ3 above). We also use the notation S m j for the subset of labels provided by χj on length-m sequences\nχj(Y1, . . . , Ym) = {Yi : i ∈ Smj }.\nDefinition 2. A labeling policy is said to be identifiable if the following map is injective\n⋃\nm:q(m)>0\nk ⋃\nj=1\n{pθ({Yr : r ∈ Smj }, X)} → pθ(X,Y )\nwhere q is the distribution of sequences lengths. In other words, there is at most one collection of probabilities corresponding to the lhs above that does not contradict the joint distribution.\nThe importance of Definition 2 is that it ensures the recovery of θ0 from the sequences partially labeled using the labeling policy. For example, a labeling policy characterized by χ1(Y ) = Y1, λ1 = 1 (always label only the first sequence element) is non-identifiable for most interesting pθ as the first sequence component is unlikely to provide sufficient information to characterize the parameters associated with transitions Yt → Yt+1.\nProposition 3. Assuming the same conditions as Proposition 1, and λ1, . . . , λk > 0 with identifiable χ1, . . . , χk, the maximizer of (12) is consistent i.e., θ̂n → θ0 as n → ∞ with probability 1. Proof. The log-likelihood (4), modified slightly by a linear combination with a constant is\nℓ′n(θ) = 1\nn\nn ∑\ni=1\nk ∑\nj=1\n(\nZ (i) j log pθ(χj(Y (i)), X(i))− λj log pθ0(χj(Y (i)), X(i)) ) .\nBy the strong law of large numbers ℓ′n(θ) converges to its expectation\nµ(θ) = − k ∑\nj=1\nλj ∑\nm>0\nq(m) ·D(pθ0({Yi : i ∈ Smj }, X)||pθ({Yi : i ∈ Smj }, X)).\nSince µ is a linear combination of KL divergences with positive weights it is non-negative and is 0 if θ = θ0. The identifiability of the labeling policy ensures that µ(θ) > 0 if θ 6= θ0. We have thus established that ℓn(θ) converges to a non-negative continuous function µ(θ) whose maximum is achieved at θ0. The rest of the proof proceeds along similar lines as Proposition 3.\nUltimately, the precise conditions for consistency will depend on the parametric family pθ under consideration. For many structured prediction models such as Markov random fields the consistency conditions are mild. Depending on the precise feature functions, consistency is generally satisfied for every policy that labels contiguous subsequences with positive probability. However, some care need to be applied for models like HMM containing parameters associated with the start label or end label and with models asserting higher order Markov assumptions."
    }, {
      "heading" : "7 A4: Accuracy (Structured)",
      "text" : "We consider in this section the dependency of the estimation accuracy in structured prediction SSL (4) on n, θ0 but perhaps most interestingly on the labeling policy ℘. Doing so provides insight into not only how much data to label but also in what way.\nProposition 4. Under the assumptions of Proposition 3 as well as convexity of Θ we have the following convergence in distribution of the maximizer of (12)\n√ n(θ̂n − θ0) N ( 0,Σ−1 )\n(13)\nas n → ∞, where\nΣ−1 = E q(m)\n\n\n\nk ∑\nj=1\nλjVar θ0(∇Vjm)\n\n\n\nVjm = log pθ0({Yi : i ∈ Smj }, X).\nProof. By the mean value theorem and convexity of Θ there is η ∈ (0, 1) for which θ′=θ0+η(θ̂n − θ0) and\n∇ℓn(θ̂n) = ∇ℓn(θ0) +∇2ℓn(θ′)(θ̂n − θ0).\nSince θ̂n maximizes ℓ, ∇ℓn(θ̂n) = 0 and √ n(θ̂n − θ0) = − √ n(∇2ℓn(θ′))−1∇ℓn(θ0). (14)\nBy Proposition 3 we have θ̂n p→ θ0 which implies that θ′ p→ θ0 as well. Furthermore, by the law of large numbers and the fact that if Wn p→ W then g(Wn) p→ g(W ) for continuous g,\n(∇2ℓn(θ′))−1 p→ (∇2ℓn(θ0))−1 (15)\np→\n\n\n∑\nm>0\nq(m)\nk ∑\nj=1\nλjE θ0(∇2Vjm)\n\n\n−1\n= −\n\n\n∑\nm>0\nq(m)\nk ∑\nj=1\nλjVar θ0(∇Vjm)\n\n\n−1\n.\nwhere in the last equality we used a well known identity concerning the Fisher information. For the remaining term on the rhs of (14) we have\n√ n∇ℓn(θ0) = √ n 1\nn\nn ∑\ni=1\nWi (16)\nwhere the random vectors\nWi = ∑\nm>0\n1{length(Y (i))=m}\nk ∑\nj=1\nZ (i) j ∇V (i) jm\nhave expectation 0 due to the fact that the expectation of the score is 0. The variance of Wi is\nVar θ0Wi =E θ0 ∑\nm>0\n1{length(Y (i))=m}\nk ∑\nj=1\nZ (i) j ∇V (i) jm∇V (i)⊤ jm\n= ∑\nm>0\nq(m)\nk ∑\nj=1\nλjE ( ∇V (i)jm∇V (i)⊤ jm )\nwhere in the first equality we used the fact that Y (i) can have only one length and only one of χ1, . . . , χk is chosen. Using the central limit theorem we thus conclude that\n√ n∇ℓn(θ0) N ( 0,Σ−1 )\nand finish the proof by combining (14), (15), and (11) using Slutsky’s theorem.\nFigure 2 (left, middle) displays the test-set per-sequence perplexity for the CoNLL2000 chunking task as a function of the total number of labeled tokens. We used the Boltzmann chain MRF model that is the MRF corresponding to HMM (though not identical e.g., [8]). We consider labeling policies ℘ that label the entire sequence with probability λ and otherwise label contiguous sequences of length 5 (left) or leave the sequence fully unlabeled (middle). Lighter nodes indicate larger n and unsurprisingly show a decrease in the test-set perplexity as n is increased. Interestingly, the middle figure shows that labeling policies using a smaller amount of labels may outperform other policies. This further motivates our analysis and indicates that naive choices of ℘ may be inefficient, viz. inflating labeling cost with negligible accuracy improvement to accuracy (cf. also Sec. 8 for how to avoid this pitfall)."
    }, {
      "heading" : "7.1 Conditional Structured Prediction",
      "text" : "Thus far our discussion on structured prediction has been restricted to generative models such as HMM or Boltzmann chain MRF. Similar techniques, however, can be used to analyze SSL for conditional models such as CRFs that are estimated by maximizing the conditional likelihood. The key to extending the results in this paper to CRFs is to express conditional SSL estimation in a form similar to (4)\nθ̂n = argmax\nn ∑\ni=1\nlog pθ(℘(Y (i))|X(i))\nand to proceed with an asymptotic analysis that extends the classical conditional MLE asymptotics. We omit further discussion due to lack of space but include some experimental results for CRFs.\nFigure 3 (left) depicts a similar experiment to the one described in the previous section for conditional estimation in CRF models. The figure displays per-sequence perplexity as a function n (x axis) and λ1 (y axis). We observe a trend nearly identical to that of the Boltzmann chain MRF (Figure 2, left, middle)."
    }, {
      "heading" : "8 A5: Tradeoff",
      "text" : "As the figures in the previous sections display, the estimation accuracy increases with the total number of labels. The Cramer-Rao lower bound states that the highest accuracy is obtained by the maximum likelihood operating on fully observed data. However, assuming that a certain cost is associated with labeling data SSL resolves a fundamental accuracy-cost tradeoff. A decrease in estimation accuracy is acceptable in return for decreased labeling cost.\nOur ability to mathematically characterize the dependency of the estimation accuracy on the labeling cost leads to a new quantitative formulation of this tradeoff. Each labeling policy (λ, n in classification and ℘ in structured prediction) is associated with a particular estimation accuracy via Propositions 2 and 4 and with a particular labeling cost. The precise way to measure labeling cost depends on the situation at\nhand, but we assume in this paper that the labeling cost is proportional to the numbers of labeled samples (classification) and of labeled sequence elements (structured prediction). This assumption may be easily relaxed by using other labeling cost functions e.g, obtaining unlabeled data may incur some cost as well.\nGeometrically, each labeling policy may thus be represented in a two dimensional scatter plot where the horizontal and vertical coordinates correspond to labeling cost and estimation error respectively. Three such scatter plots appear in Figure 2 (see Section 7 for a description of the left and middle panels). The right panel corresponds to multinomial naive Bayes SSL classifier and the 20-newsgroups classification dataset. Each point in that panel corresponds to different n, λ.\nThe origin corresponds to the most desirable (albeit unachievable) position in the scatter plot representing zero error at no labeling cost. The cloud of points obtained by varying n, λ (classification) and ℘ (structured prediction) represents the achievable region of the diagram. Most attractive is the lower and left boundary of that region which represents labeling policies that dominate others in both accuracy and labeling cost. The non-achievable region is below and to the left of that boundary (see shaded region in Figure 2, right). The precise position of the optimal policy on the boundary of the achievable region depends on the relative importance of minimizing estimation error and minimizing labeling cost. A policy that is optimal in one context may not be optimal in a different context.\nIt is interesting to note that even in the case of naive Bayes classification (Figure 2, right) some labeling policies (corresponding to specific choices of n, λ) are suboptimal. These policies correspond to points in the interior of the achievable region. A similar conclusion holds for Boltzmann chain MRF. For example, some of the points in Figure 2 (left) denoted by the label 700 are dominated by the more lightly shaded points.\nWe consider in particular three different ways to define an optimal labeling policy (i.e., determining how much data to label) on the boundary of the achievable region\n(λ∗, n∗)1 = argmin (λ,n):λn≤C tr(Σ−1) (17) (λ∗, n∗)2 = argmin (λ,n):tr(Σ−1)≤C λn (18)\n(λ∗, n∗)3 = argmin (λ,n) λn+ α tr(Σ−1). (19)\nThe first applies in situations where the labeling cost is bounded by a certain available budget. The second applies when a certain estimation accuracy is acceptable and the goal is to minimize the labeling cost. The\nthird considers a more symmetric treatment of the estimation accuracy and labeling cost. Equations (17)-(19) may be easily generalized to arbitrary labeling costs f(n, λ). Equations (17)-(19) may also be generalized to the case of structured prediction with ℘ replacing (λ, n) and cost(℘) replacing λn."
    }, {
      "heading" : "9 A6: Practical Algorithms",
      "text" : "Choosing a policy (λ, n) or ℘ resolves the SSL tradeoff of accuracy vs. cost. Such a resolution is tantamount to answering the basic question of how many labels should be obtained (and in the case of structured prediction also which ones). Resolving the tradeoff via (17)-(19) or in any other way, or even simply evaluating the asymptotic accuracy tr(Σ) requires knowledge of the model parameter θ0 that is generally unknown in practical settings.\nWe propose in this section a practical two stage algorithm for computing an estimate θ̂n within a particular accuracy-cost tradeoff. Assuming we have n unlabeled examples, the algorithm begins the first stage by labeling r samples. It then estimates θ′ by maximizing the likelihood over the r labeled and n− r unlabeled samples. The estimate θ̂′ is then used to obtain a plug-in estimate for the asymptotic accuracy tr(Σ). In\nthe second stage the algorithm uses the estimate t̂r(Σ) to resolve the tradeoff via (17)-(19) and determine how many more labels should be collected. Note that the labels obtained at the first stage may be used in the second stage as well with no adverse effect.\nThe two-stage algorithm spends some initial labeling cost in order to obtain an estimate for the quantitative tradeoff parameters. The final labeling cost, however, is determined in a principled way based on the relative importance of accuracy and labeling cost via (17)-(19). The selection of the initial number of labels r is important and should be chosen carefully. In particular it should not exceed the total desirable labeling cost.\nWe provide some experimental results on the performance of this algorithm in Figure 3 (right). It displays\nbox-plots for the differences between tr(Σ) and t̂r(Σ) as a function of the initial labeling cost r for naive Bayes SSL classifier and 20-newsgroups data. The figure illustrates that the two stage algorithm provides a very accurate estimation of tr(Σ) for r ≥ 1000 which becomes almost perfect for r ≥ 1300."
    } ],
    "references" : [ {
      "title" : "Does unlabeled data provably help? worst-case analysis of the sample complexity of semi-supervised learning",
      "author" : [ "S. Ben-David", "T. Lu", "D. Pal" ],
      "venue" : "In International Conference on Learning Theory,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2008
    }, {
      "title" : "The relative value of labeled and unlabeled samples in pattern recognition with an unknown mixing parameter",
      "author" : [ "V. Castelli", "T.M. Cover" ],
      "venue" : "IEEE Transactions on Information Theory,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 1996
    }, {
      "title" : "Statistical and computational tradeoffs in stochastic composite likelihood",
      "author" : [ "J. Dillon", "G. Lebanon" ],
      "venue" : "In Proc. of the 12th International Conference on Aritficial Intelligence and Statistics,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2009
    }, {
      "title" : "A Course in Large Sample Theory",
      "author" : [ "T.S. Ferguson" ],
      "venue" : null,
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 1996
    }, {
      "title" : "An asymptotic analysis of generative, discriminative, and pseudolikelihood estimators",
      "author" : [ "P. Liang", "M.I. Jordan" ],
      "venue" : "In Proc. of the International Conference on Machine Learning,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2008
    }, {
      "title" : "Equivalence of linear boltzmann chains and hidden markov models",
      "author" : [ "D.J.C. MacKay" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 1996
    }, {
      "title" : "Text classification from labeled and unlabeled documents using EM",
      "author" : [ "K. Nigam", "A. McCallum", "S. Thrun", "T. Mitchell" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2000
    }, {
      "title" : "Unlabeled data: Now it helps, now it doesnt",
      "author" : [ "A. Singh", "R. Nowak", "X. Zhu" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2008
    }, {
      "title" : "The value of labeled and unlabeled examples when the model is imperfect",
      "author" : [ "K. Sinha", "M. Belkin" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2008
    } ],
    "referenceMentions" : [ {
      "referenceID" : 2,
      "context" : "Our asymptotic derivations are possible by extending the recently proposed stochastic composite likelihood formalism [5] and showing that generative SSL is a special case of that extension.",
      "startOffset" : 117,
      "endOffset" : 120
    }, {
      "referenceID" : 4,
      "context" : "As in [7], the delta method transforms our results from parameter asymptotics to prediction risk asymptotics.",
      "startOffset" : 6,
      "endOffset" : 9
    }, {
      "referenceID" : 1,
      "context" : "Perhaps the first study in this area was done by Castelli and Cover [3] who examined the convergence of the classification error rate as a labeled example is added to an unlabeled dataset drawn from a Gaussian mixture model.",
      "startOffset" : 68,
      "endOffset" : 71
    }, {
      "referenceID" : 6,
      "context" : "[9] proposed a practical SSL framework based on maximizing the likelihood of the observed data.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 8,
      "context" : "Sinha and Belkin [11] examined the effect of using unlabeled samples with imperfect models for mixture models.",
      "startOffset" : 17,
      "endOffset" : 21
    }, {
      "referenceID" : 7,
      "context" : "[10] analyze discriminative SSL using PAC theory and large deviation bounds.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 0,
      "context" : "”[4] However, many of these assumptions are criticized in [2].",
      "startOffset" : 58,
      "endOffset" : 61
    }, {
      "referenceID" : 6,
      "context" : "3 Stochastic SSL Estimators Generative SSL [9, 4] estimates a parametric model by maximizing the observed likelihood incorporating L labeled and U unlabeled examples",
      "startOffset" : 43,
      "endOffset" : 49
    }, {
      "referenceID" : 6,
      "context" : "A classical example is the naive Bayes model in [9] where pθ(X,Y ) = pθ(X |Y )p(Y ), pθ(X |Y = y) = Mult([θy]1, .",
      "startOffset" : 48,
      "endOffset" : 51
    }, {
      "referenceID" : 2,
      "context" : "The central idea in the proof is to cast the generative SSL estimation problem as an extension of stochastic composite likelihood [5].",
      "startOffset" : 130,
      "endOffset" : 133
    }, {
      "referenceID" : 2,
      "context" : "Our proof follows similar lines to the consistency proof of [5] with the exception that it does not assume independence of the indicator functions Z and (1 − Z) as is assumed there.",
      "startOffset" : 60,
      "endOffset" : 63
    }, {
      "referenceID" : 3,
      "context" : "chapter 16 of [6], hold on S leading to P {",
      "startOffset" : 14,
      "endOffset" : 17
    }, {
      "referenceID" : 3,
      "context" : "The notations p → , denote convergences in probability and in distribution [6] and ∇f(θ), ∇2f(θ) are the r × 1 gradient vector and r × r matrix of second order derivatives of f(θ).",
      "startOffset" : 75,
      "endOffset" : 78
    }, {
      "referenceID" : 6,
      "context" : "Figure 1 displays three error measures for the multinomial naive Bayes SSL classifier [9] and the Reuters RCV1 text classification data.",
      "startOffset" : 86,
      "endOffset" : 89
    }, {
      "referenceID" : 2,
      "context" : ", λk)) which exposes its similarity to the stochastic composite likelihood function in [5].",
      "startOffset" : 87,
      "endOffset" : 90
    }, {
      "referenceID" : 5,
      "context" : ", [8]).",
      "startOffset" : 2,
      "endOffset" : 5
    } ],
    "year" : 2013,
    "abstractText" : "Semisupervised learning has emerged as a popular framework for improving modeling accuracy while controlling labeling cost. Based on an extension of stochastic composite likelihood we quantify the asymptotic accuracy of generative semi-supervised learning. In doing so, we complement distribution-free analysis by providing an alternative framework to measure the value associated with different labeling policies and resolve the fundamental question of how much data to label and in what manner. We demonstrate our approach with both simulation studies and real world experiments using naive Bayes for text classification and MRFs and CRFs for structured prediction in NLP.",
    "creator" : "LaTeX with hyperref package"
  }
}