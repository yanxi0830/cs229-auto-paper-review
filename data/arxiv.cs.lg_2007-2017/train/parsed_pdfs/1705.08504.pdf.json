{
  "name" : "1705.08504.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Interpreting Blackbox Models via Model Extraction",
    "authors" : [ "Osbert Bastani", "Carolyn Kim", "Hamsa Bastani" ],
    "emails" : [ "obastani@cs.stanford.edu", "ckim@cs.stanford.edu", "hsridhar@stanford.edu" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Recent advances in machine learning have revolutionized our ability to use data to inform critical decisions, such as medical diagnosis [30, 16, 44], bail decisions for defendants [28, 27] and the design of aircraft collision avoidance systems [42]. At the same time, machine learning algorithms have been shown to exhibit unexpected defects when deployed in the real world; examples include causality (i.e., inability to distinguish causal effects from correlations) [34, 16], fairness (i.e., internalizing prejudices present in training data) [22, 26], and algorithm aversion (i.e., lack of trust by end users) [19].\nInterpretability is a promising approach to address these challenges [38, 21]—in particular, we can help human users diagnose issues and verify correctness of machine learning models by providing insight into the model’s reasoning [43, 8, 37, 31, 29]. For example, suppose the user is trying to train a model that does not depend on a prejudiced feature (e.g., gender or ethnicity). Omitting the feature might not suffice to avoid prejudice, since the model could reconstruct that feature from other features [35]. A better approach might be to train the model with the prejudiced feature, and then assess the dependence of the model on that feature. This approach requires the ability to understand the model’s reasoning process, i.e., how model predictions are affected by changing the prejudiced feature [21]. Similarly, the user may want to determine whether dependence on a feature is causal, or understand the high-level structure of the model to gain confidence in its correctness.\nIn this paper, we propose an technique that we call model extraction for interpreting the overall reasoning process performed by a model. Given a model f : X → Y , the interpretation produced by our algorithm is an approximation T (x) ≈ f(x), where T is an interpretable model. In this paper, we take T to be a decision tree, which has been established as highly interpretable [31, 37, 8]. Intuitively, if T is a sufficiently good approximation of f , then any issues in f should be reflected in T as well. Thus, the user can understand and debug f by examining T ; then, the original model f can be deployed so that performance is not sacrificed.\nPrevious model extraction approaches have focused on interpreting specific families of models such as random forests [45, 18, 46], enabling them to leverage domain-specific knowledge about the internal structure of the model. In contrast, our approach is fully blackbox, i.e., it is agnostic to the\nar X\niv :1\n70 5.\n08 50\n4v 1\n[ cs\n.L G\n] 2\n3 M\nay 2\n01 7\ninternal model structure and only requires the ability to run the model f on an input x ∈ X and obtain its predicted output f(x) ∈ Y . This way, not only is our approach very flexible in that it can work with any model family, but it is furthermore independent of the implementation details of a specific model family.\nThe key challenge to learning accurate decision trees is that they often overfit and obtain poor performance, whereas complex models such as random forests and deep neural nets are better regularized [9]. For example, random forests use ensembles of trees to avoid overfitting to specific features or training points.\nThus, our algorithm uses active learning to construct T from f—it actively samples a large number of training points, and determines the corresponding label simply by computing y = f(x). The large quantity of data ensures that T does not overfit to the small set of initial training points. We prove that under mild assumptions, by generating a sufficient quantity of data, the extracted tree T converges to the exact greedy decision tree, i.e., it avoids overfitting since the sampling error goes to zero.\nWe implement our algorithm and evaluate its performance in two experiments. First, we use our algorithm to produce interpretations of random forests and neural nets, as well as for control policies trained using reinforcement learning. We show that our active learning approach substantially improves over using CART [11], a standard decision tree learning algorithm. Second, we demonstrate how the decision trees extracted can be used to debug issues with these models, for example, to assess the dependence on prejudiced features, to determine why certain models perform worse, and to understand the high-level structure of a learned control policy."
    }, {
      "heading" : "1.1 Related Work",
      "text" : "A number of approaches for improving interpretability have been proposed, which largely fall into three directions: (i) learning more interpretable models, (ii) providing explanations for individual model predictions, and (iii) providing an overall interpretation of a model.\nCART [11] is considered highly interpretable, but its accuracy is often lacking in practice; [31, 8] tackle this concern by producing sparse rule lists that resemble decision trees yet are non-greedy, thus enabling them to learn more accurate models without sacrificing interpretability. Alternatively, [43] proposes supersparse linear integer models, which are sparse linear models where the coefficients are integer valued, thus resembling risk-scoring systems constructed manually by humans for applications such as medical diagnosis or criminal rescidivism. This approach is extended by [27] for classification problems with binary features. Finally, [15] propose generalized additive models, which are linear combinations of arbitrarily complex single-feature models, as interpretable models.\nHowever, in many domains, the highest performing models are complex blackbox models with limited interpretability, e.g., neural nets or random forests. While interpretable models may achieve nearly comparable performance, sacrificing even a small amount of accuracy can be undesirable in settings with critical consequences such as medicine. One proposed approach is to use the blackbox model, but generate interpretations for every new test point along with the predicted output. For instance, given a new test point x, [37] generates an interpretation for the prediction f(x) by fitting a simple model locally around x and using this simple model to explain the prediction. Similarly, [32] uses the Shapley value to determine the most influential features for a given prediction. These approaches can help the user understand a specific prediction, but they cannot help understand the model as a whole, making it less useful for diagnosing problems with the model itself.\nOur paper takes the third approach, i.e., it interprets the model as a whole. Past techniques have focused on simply identifying influential features. For instance, the relative influence scores the contribution of each feature in tree-based models (e.g., random forests) [24]. Similarly, [39] proposes a method for computing influence scores for features in deep neural nets. However, these approaches cannot help understand more complex reasoning performed by the model, as is needed to understand the dependence of a model on prejudiced or potentially non-causal features.\nFinally, our model extraction algorithm is closely related to the idea of model compression [13], where a larger, more complex model is used to guide the training of a smaller, more efficient model. Our algorithm can be thought of as a model compression algorithm where the target model is a decision tree; it leverages active learning to avoid overfitting the decision tree."
    }, {
      "heading" : "2 Problem Formulation",
      "text" : "Our algorithm learns axis-aligned decision trees. We start by establishing notation. An axis-aligned constraint is a constraint C = (xi ≤ t), where i ∈ [d] and t ∈ R. More general constraints can be built from existing constraints using negations ¬C, conjunctions C1 ∧ C2, and disjunctions C1 ∨ C2. The feasible set of C is F(C) = {x ∈ X | x satisfies C}. A decision tree T is a binary tree. Each internal node N = (NL, NR, C) of T has a left child node NL and a right child node NR, and is labeled with an axis-aligned constraint C = (xi ≤ t). Each leaf node N = (y) of T is associated with a label y ∈ Y . We use NT to denote the root node of T . The decision tree is interpreted as a function T : X → Y in the usual way. More precisely, a leaf node N = (y) is interpreted as a function N(x) = y, an internal node N = (NL, NR, C) is interpreted as a function N(x) = NL(x) if x ∈ F(C), and N(x) = NR(x) otherwise. Then, T (x) = NT (x). For a node N ∈ T , we let CN denote the conjunction of the constraints along the path from the root of T to N . More precisely, CN is defined recursively: for the root NT , we have CNT = True, and for an internal node N = (NL, NR, C), we have CNL = CN ∧ C and CNR = CN ∧ ¬C. Then, given a training set Xtrain ⊆ X and blackbox access to a function f : X → Y , our goal is to learn a decision tree T : X → Y that approximates f . We focus on the case X = Rd and Y = [m] = {1, ...,m} (i.e., classification); our approach easily generalizes to the case where X contains categorical dimensions, and to the case Y = R (i.e., regression). For classification, we measure performance using accuracy relative to f on a held out test set, i.e.,\n1 |Xtest| ∑ x∈Xtest I[T (x) = f(x)]. For binary classification, we use F1 score, and for regression, we use mean-squared error."
    }, {
      "heading" : "3 Decision Tree Extraction Algorithm",
      "text" : "Our algorithm is greedy, both for scalability and because it is a natural fit for interpretability, since more relevant features occur higher in the tree. We first describe the input distribution P constructed by our algorithm to generate data. Then, we describe the exact greedy decision tree T ∗, where all decisions are made exactly according to the distribution P (albeit greedily); note that we cannot construct T ∗ since we treat f as a blackbox. Finally, we describe how our algorithm estimates T ∗ using i.i.d. samples from P . In Section 4, we show the estimated tree converges to T ∗ asymptotically."
    }, {
      "heading" : "3.1 Input Distribution",
      "text" : "Our algorithm constructs a distribution P over X by fitting a mixture of axis-aligned Gaussian distributions to the training data using expectation maximization. It has parameters φ ∈ Rk defining a categorical distribution over [K], and parameters µ ∈ RKd and Σ ∈ RKd2 , where µj ∈ Rd and Σj ∈ Rd 2\n(where Σj is diagonal), for j ∈ [K], defining the jth Gaussian distribution in the mixture. To sample x ∼ P , first sample j ∼ Categorical(φ) and then sample x ∼ N (µj ,Σj)."
    }, {
      "heading" : "3.2 Exact Greedy Decision Tree",
      "text" : "We describe the exact greedy decision tree T ∗ of size k, which closely mirrors CART [11]. It is initialized to a tree with a single leaf node NT∗ = (y), where y is the majority label according to P . At each iteration, a leaf node N = (y) in the current tree is replaced with an internal node N ′ = (NL, NR, C), where NL = (yL) and NR = (yR) are leaf nodes, and C = (xi∗ ≤ t∗) where\n(i∗, t∗) = arg max i∈[d],t∈R G(i, t),\nwhere the gain\nG(i, t) = Gain(f, CN ∧ (xi ≤ t)) + Gain(f, CN ∧ (xi > t))− Gain(f, CN ) (1)\nGain(f, C) = 1−∑ y∈Y Prx∼P [f(x) = y | C]2  · Prx∼P [C] (2)\nis the (weighted) Gini impurity. The gain can be replaced with other gains such as information gain, or MSE for regression. The leaf node labels are\nyL = arg max y∈Y\nPrx∼P [f(x) = y | CN ∧ (xi ≤ t)] (3)\nyR = arg max y∈Y\nPrx∼P [f(x) = y | CN ∧ (xi > t)]. (4)\nThe node chosen to be replaced is that with the highest potential gain (1) in the current tree.\nTo construct the exact greedy decision tree of size k, this process is repeated k − 1 times. If at any iteration, the gain (1) is zero, then the process is terminated (so T ∗ may have fewer than k nodes)."
    }, {
      "heading" : "3.3 Estimated Greedy Decision Tree",
      "text" : "Given n ∈ N, our algorithm constructs a greedy decision tree the same way as the construction of the exact greedy decision tree, except that (1) and (3) are estimated using n i.i.d. samples x ∼ P | CN each. Recall that at each iteration, we have to select the node N with the highest potential gain to replace. Our algorithm does so by estimating the gain for all nodes; these estimates are constructed using a separate set of i.i.d. samples to ensure unbiasedness of the tree parameters.\nNext, we describe how our algorithm samples x ∼ P | C, where C is a conjunction of axis-aligned constraints:\nC = (xi1 ≤ t1) ∧ ... ∧ (xik ≤ tk) ∧ (xj1 > s1) ∧ ... ∧ (xjh > sh).\nSome inequalities in C may be redundant. First, suppose there are two constraints xi ≤ t and xi ≤ t′. Assume without loss of generality that t ≤ t′; then, the first constraint implies the second, so we can discard the latter. For a pair of constraints xi > s and xi > s′, we can similarly discard one of them. Second, given two constraints xi ≤ t and xi > s, we can assume that t ≥ s; otherwise C is unsatisfiable, so the gain (1) would have been zero and the algorithm would have terminated. In summary, we can assume C contains at most one inequality (xi ≤ t) and at most one inequality (xi > s) per i ∈ [d], and if both are present, then the two are not mutually exclusive. For simplicity, we assume C contains both inequalities for each i ∈ [d]:\nC = (s1 ≤ x1 ≤ t1) ∧ ... ∧ (sd ≤ xd ≤ td).\nNow, recall that P is a mixture of axis-aligned Gaussians, so it has probability density function\npP(x) = K∑ j=1 φj · pN (µj ,Σj)(x) = K∑ j=1 φj d∏ i=1 pN (µji,σji)(xi),\nwhere σji = (Σj)ii. The conditional distribution is\npP|C(x) ∝ K∑ j=1 φj d∏ i=1 pN (µji,σji)|C(xi) = K∑ j=1 φj d∏ i=1 pN (µji,σji)|(si≤xi≤ti)(xi).\nSince the Gaussians are axis-aligned, the unnormalized probability of each component is\nφ̃′j = ∫ φj d∏ i=1 pN (µji,σji)|(si≤xi≤ti)(xi)dx = φj d∏ i=1 ( Φ ( ti − µji σji ) − Φ ( si − µji σji )) ,\nwhere Φ is the cumulative density function of the standard Gaussian distribution N (0, 1). Then, the normalization constant is Z = ∑K j=1 φ̃ ′ j , and the component probabilities are φ = Z −1φ′.\nTherefore, to sample x ∼ P | C, we sample j ∼ Categorical(φ̃), and\nxi ∼ N (µji, σji) | (si ≤ xi ≤ ti) (for each i ∈ [d]).\nWe use standard algorithms for sampling truncated Gaussian distributions to sample each xi."
    }, {
      "heading" : "4 Theoretical Guarantees",
      "text" : "We show that our decision tree extraction produces a decision tree that is close to the exact greedy tree for sufficiently large n. A related result is [20], but their analysis is limited to discrete features, for which convergence is much easier to analyze. We give proofs in Appendix A.\nWe begin by describing our assumptions. First, we make mild assumptions about the distribution P . Assumption 1. The probability density function p(x) of the distribution P over X is continuous, bounded (i.e., p(x) ≤ pmax), and has bounded domain (i.e., p(x) = 0 for |x| > xmax).\nTo satisfy this assumption, we can truncate the Gaussian mixture models used by our algorithm to X = {x ∈ Rd | ‖x‖∞ ≤ xmax}, for some xmax ∈ R. Intuitively, for reasonably large xmax, this modification should not affect either the exact greedy tree or the approximate greedy tree by very much, since Gaussian distributions have exponential tails.\nOur next assumption says that the exact greedy tree is well defined: Assumption 2. We assume that the maximizers (i∗, t∗) of (1) and yL, yR of (3) are unique.\nOtherwise, multiple exact greedy trees exist; our extracted tree would converge to an arbitrary one.\nWe now define the notion in which the extracted tree converges to the exact tree. For simplicity, we additionally assume that we are learning complete decision trees of depth D. Definition 1. Let T, T ′ be complete decision trees of depth D. For > 0, we say T is an approximation of T ′ if Prx∼P [T (x) = T ′(x)] ≤ . Let T ∗ denote the exact greedy decision tree that is complete of depth D. For any , δ > 0, we say an extracted decision tree T is ( , δ) exact if Pr[T is an approximation of T ∗] ≥ 1− δ, where the randomness is taken over the training samples x ∼ P obtained by our algorithm. Theorem 1. For any , δ > 0, there exists n ∈ N such that the decision tree extracted by our algorithm using n samples per node is ( , δ) exact.\nThe key challenge to proving Theorem 1 is accounting for the fact that small errors made early on in the tree introduce small errors into the constraints CN , which can recursively introduce small errors into the probabilities Prx∼PX [ · | CN ] in the gain (1). The dependence of n in Theorem 1 on and δ depends on the gap of the exact greedy decision tree. Definition 2. Let P be a distribution over R, and let F (t) be an unnormalized cumulative distribution function for P . We say g : R → R is ( ′, δ′) gapped according to P if it has a unique maximizer t∗ = arg maxt∈R g(t), and for every t ∈ R such that |F (t)−F (t∗)| > ′, we have g(t∗) > g(t) + δ′.\nIn particular, our result depends on the relationship between ′ and δ′. For the special case where δ′ depends linearly on ′, we can obtain sample complexity bounds: Corollary 1. Let f : X → Y . Suppose that for every i ∈ [d], there exists κ > 0 such that for every ′ > 0, the gain function g(t) = G(i, t) in (1) is ( ′, κ · ′) gapped. Then, Theorem 1 holds for\nn ≥ max {( 1 +\n16m\n( /2k)3/2 · κ\n)3D , 2D+1k\nδ , 4\n} .\nThe exponential dependence on depth is unavoidable since errors increase multiplicatively from a parent node to its children; in practice, depth is small (e.g., D ≤ 5), so this is not an issue. Besides the gap in the gain g, the value n must also be lower bounded by a function of the gap between the maxima of the gain functions across different dimensions i, as well as the gap in the objective (3) for choosing leaf node labels; we omit this additional term for clarity.\nAs we discuss in Appendix A.4, we intuitively expect axis-aligned random forests f to satisfy the premise of Corollary 1."
    }, {
      "heading" : "5 Evaluation",
      "text" : "We implement our algorithm on top of scikit-learn [14], and evaluate it in two ways. First, we show that it outperforms a baseline that uses CART [11], a popular decision tree learning algorithm. Second, we show how extracted decision trees can be used to understand and debug models."
    }, {
      "heading" : "5.1 Comparison to Baseline Decision Tree Extraction Algorithm",
      "text" : "First, we compare our algorithm to a baseline.\nDatasets. We study two applications. First, we interpret supervised learning models. We train both a random forest and a neural net on a number of classification and regression tasks, primarily from the UCI Machine Learning Repository [7]. We randomly split each dataset into 70% training set and 30% test set; all results are averaged over 10 splits.\nWe estimate a Gaussian mixture model P using the training set, with K = 50 components for smaller datasets (< 200 samples), and K = 100 components for larger datasets. Then, we extract a decision tree of size at most k = 31 (i.e., same size as a depth 5 tree; we show how our results vary with k later) from the model f according to P , using n = 2000 samples to estimate (1) and (3). The random forests, trained using scikit-learn, consist of 1000 CART trees, using Gini impurity for classification and MSE for regression. The neural nets, also trained using scikit-learn, are trained with 500 hidden units, ReLU activations, L-BFGS optimization, and regularization parameter α = 10−5.\nSecond, we interpret Markov decision process (MDP) control policies π∗ : S → A, learned using reinforcement learning, for several classical control problems from OpenAI Gym [12]. Here, S ⊆ Rd is the state space and A is the action space, which can be discrete or continuous. To obtain a distribution P over S, we sample n = 100 states from the distribution over S induced by executing the learned policy π∗, and then split it into training and test sets as before. We then fit a Gaussian mixture model to the training set with K = 10 components. Finally, we extract a decision tree approximating π∗ according to P using n = 2000 samples to estimate (1) and (3). The cartpole and mountain car control policies are learned using Q-learning [41]. For the cartpole (discrete actions) [2, 10], the learner uses a discretized state space (7 bins per dimension) [1], and for the mountain car (discrete actions) [4, 33], it approximates the Q-function with a linear model over RBF features [3]. For the pendulum (continuous actions) [5], the learner represents the policy using a 3-layer neural net, which it trains using the cross-entropy method [6].\nBaseline. Our baseline is to use CART to train a decision tree approximating f on the training set {(x, f(x)) | x ∈ Xtrain}. As before, we use scikit-learn to train these trees, using Gini impurity for classification and MSE for regression. Additionally, we bound the size of the tree to k = 31 nodes.\nResults. We show results in Table 1. We show the test set performance of the extracted tree compared to ground truth (or for MDPs, estimated the reward when it is used as a policy), as well as the relative performance compared to the model f on the same test set. Note that our goal is to obtain high relative performance, since a better approximation of f is a better interpretation of f , even if f has poor performance. Our algorithm outperforms the baseline on every problem instance.\nAvoiding overfitting. Our algorithm outperforms the baseline by using active learning to avoid overfitting, especially for high dimensional input spaces with few samples. Even on the relatively large breast cancer dataset, the baseline tree Tbase obtains an F1 score of 0.999 on the training set, whereas the extracted tree T obtains only 0.961, i.e., Tbase has overfit. This discrepancy holds across datasets, and is more pronounced for smaller datasets—on the prostate cancer dataset, Tbase obtains an F1 score of 0.981 on the training set, whereas T obtains only 0.860.\nPerformance vs. size. In Figure 1, we show performance as a function of the number of nodes k on three problem instances. Our algorithm outperforms the baseline even as the tree sizes vary, achieving larger gains for larger k. By using active learning, we can learn arbitrarily large decision trees, whereas the baseline would perfectly fit the data if k becomes sufficiently large.\nConsistency of top branch. We examine the consistency of the top branch of the decision trees extracted using our algorithm across the 10 random splits of the datasets, focusing on the case where f is a random forest. In particular, we count the number of features r that occur in the top branch across the splits. We have r ≤ 2 except for the breast cancer dataset (r = 3) and the prostate cancer dataset (m = 4), showing that the top feature remains fairly consistent. For the prostate cancer dataset, r is relatively high since the number of samples is small so the models f vary more."
    }, {
      "heading" : "5.2 Examples of Use Cases",
      "text" : "Next, we show how the extracted decision trees can be used to interpret and debug models.\nUse of invalid features. Using an invalid feature is a common problem when training models. In particular, some datasets contain multiple response variables; then, one response should not be used as a feature when predicting another. For example, the breast cancer dataset contains two response variables indicating cancer recurrence—the length of time before recurrence and whether recurrence occurs within 24 months. This issue can be thought of as a special case of using a non-causal feature, an important problem in healthcare settings. We train a random forest f to predict whether recurrence occurs within 24 months, where time to recurrence is incorrectly included as a feature. Then, we extract a decision tree approximating f of size k = 7 nodes, using 10 random dataset splits. The invalid feature occured in every extracted tree, and as the top branch in 6 of the 10 trees.\nUse of prejudiced features. We can use our algorithm to evaluate how a model f depends on prejudiced features. For example, gender is a feature in the student grade dataset, and may be considered sensitive when estimating student performance. However, if we simply omit gender, then f may reconstruct it from the remaining features. For a model f trained with gender available, we show how a decision tree extracted from f can be used to understand how f depends on gender. Our approach does not guarantee fairness, but it can be useful for evaluating the fairness of f .\nWe extract decision trees T from the random forests f trained on 10 random splits of the student grades dataset. The top features are consistently grades in other classes or number of failing grades received in the past. Gender occurs below these features (at the fourth or fifth level) in 7 of 10 of the trees. We can estimate the overall effect of changing the gender label:\n∆ = Ex∼P [f(x) | gender = M]− Ex∼P [f(x) | gender = F].\nWhen gender occurs, ∆ is between 0.31 and 0.70 grade points (average 0.49) out of 20 total grade points. For the remaining models, ∆ is between 0.11 and 0.32 (average 0.25). Therefore, the extracted tree includes gender when the model has a relatively large dependence on gender.\nMore interestingly, because T approximates f , we can use it to identify a subgroup of students where f has particularly strong dependence on gender. Letting N = (NL, NR, C) be the internal node branching on gender, F(CN ) are a subset of inputs whose label f(x) ∈ Y is determined in part by gender in T . Also, we can use T to measure the dependence on gender within this subset:\n∆N = Ex∼P [f(x) | CNL ]− Ex∼P [f(x) | CNR ].\nFurthermore, we can estimate the fraction of students in this subset using the test set Xtest, i.e., P = ∑ x∈Xtest I[x ∈ F(CN )]. Finally, P ·∆N/∆ measures the fraction of the overall dependence of f on gender that is accounted for by the subtree rooted at N . For models where gender occurs in the extracted tree, the subgroup effect size ∆N ranged from 0.44 to 0.77 grade points, and the estimated fraction of students in this subroup ranged from 18.3% to 39.1%. The two trees that had the largest effect size had ∆N of 0.77 and 0.43, respectively, and P of 39.1% and 35.7%, respectively; the identified subgroup accounted for 67.3% and 65.6% of the total effect of gender, respectively.\nHaving identified a subgroup of students likely to be adversely affected, the user might be able to train a better model specifically for this subgroup. In 5 of the 7 extracted trees where gender occurs, the affected students were students with low grades, in particular, the 27% of students who scored fewer than 8.5 points in another class. This fine-grained understanding of f relies on the extracted model, and cannot be obtained using feature importance metrics alone.\nComparing models. We can use the extracted decision trees to compare different models trained on the same dataset, and gain insight into why some models perform better than others. For example, random forests trained on the wine origin dataset performed very well, all achieving an F1 score of at least 0.961. In contrast, the performance of the neural nets was bimodal—5 had F1 score of at least 0.955, and the remaining had an F1 score of at most 0.741.\nWe examined the top 3 layers of the extracted decision trees T , and made two observations. First, occurrence of the feature “chlorides” in T was almost perfectly correlated with poor performance of the neural nets. This feature occured in only one of the 10 trees extracted from random forests, and in none of the trees extracted from high performing neural nets. A weaker observation was the branching of T on the feature “alcohol”, which is a very important feature—it is the top branch for all but one of the 20 extracted decision trees. For the high performing models, the branch threshold t tended to be higher (749.8 to 999.6) than those for the poorly performing models (574.4 to 837.3). The latter observation relies on having an extracted model—feature influence metrics alone are insufficient.\nUnderstanding control policies. We can use the extracted decision tree to understand a learned control policy. For example, we describe a decision tree of size k = 7 extracted from the cartpole control policy. While its estimated reward of 152.3 is lower than for larger trees, it captures a significant fraction of the policy behavior. The tree says to move the cart to the right exactly when\n(pole velocity ≥ −0.286) ∧ (pole angle ≥ −0.071),\nwhere the pole velocity is in [−2.0, 2.0] and the pole angle is in [−0.5, 0.5]. In other words, move the cart to the right exactly when the pole is already on the right relative to the cart, and the pole is also moving toward the left (or more precisely, not moving fast enough toward the right). This policy is asymmetric, focusing on states where the cart is moving to the left. Examining an animation of simulation, this bias occurs because the cart initially moves toward the left, so the portion of the state space where the cart is moving toward the right is relatively unexplored."
    }, {
      "heading" : "6 Conclusions",
      "text" : "We have proposed an approach for interpreting blackbox models based on decision tree extraction, and shown how it can be used to interpret random forests, neural nets, and control policies. Important directions for future work include devising algorithms for model extraction using more expressive input distributions, and developing new ways to gain insight from the extracted decision trees."
    }, {
      "heading" : "A Proofs of Main Results",
      "text" : "In this section, we give proofs of Theorem 1 and Corollary 1."
    }, {
      "heading" : "A.1 Proof of Main Lemmas",
      "text" : "In this section, we state and prove the main lemmas required to prove Theorem 1 and Corollary 1. Essentially, these results are the inductive steps needed to prove our main results.\nLemma 1. Let X ⊆ Rd, and let P and P̃ be distributions over X , and let p(x) and p̃(x) be unnormalized probability density functions for P and P̃ , respectively. The function p̃ depends on a parameter n ∈ N; in applications of this lemma, p̃ is an estimate of p computed by our algorithm on n samples. Assume that for any , δ > 0, there exists n ∈ N such that ‖p− p̃‖1 ≤ with probability at least 1− δ. Furthermore, let i ∈ [d], and define\nG(t) = − √√√√ ∑ s∈{±1} ∑ y∈Y gs,y(t)2 hs(t)\nG̃(t) = − √√√√ ∑ s∈{±1} ∑ y∈Y g̃s,y(t)2 h̃s(t)\nˆ̃G(t) = − √√√√ ∑ s∈{±1} ∑ y∈Y ˆ̃gs,y(t)2 h̃s(t) ,\nwhere\nhs(t) = ∫ I[s · xi ≤ t] · p(x)dx\nh̃s(t) = ∫ I[s · xi ≤ t] · p̃(x)dx\ngs,y(t) = ∫ I[f(x) = y] · I[s · xi ≤ t] · p(x)dx\ng̃s,y(t) = ∫ I[f(x) = y] · I[s · xi ≤ t]] · p̃(x)dx\nˆ̃gs,y(t) = Z\nn n∑ i=1 I[f(x(k)) = y] · I[s · x(k)i ≤ t],\nwhere x(1), ..., x(n) are i.i.d. samples from P̃ , and where\nZ = ∫ p̃(x)dx\nis a normalization constant. Now, let t∗ = arg max\nt∈R G(t)\nt̃∗ = arg max t∈R ˆ̃G(t),\nand let p′(x) = p(x) · I[xi ≤ t∗] p̃′(x) = p̃(x) · I[xi ≤ t̃∗]\nbe unnormalized probability density functions for P | (xi ≤ t∗) and P̃ | (xi ≤ t̃), respectively. Assume that ‖hs‖∞ ≥ γ > 0 for each s ∈ {±1}. For any ′, δ′ > 0, there exists n ∈ N such that\n‖p′ − p̃′‖1 ≤ ′\nwith probability at least 1− δ′.\nProof. Let ′, δ′ > 0 be arbitrary. First, we show that the L∞ norm of each g = gs,y is bounded with high probability. By Lemma 4, we have\n‖g − ˆ̃g‖∞ ≤ ‖g − g̃‖∞ + ‖g̃ − ˆ̃g‖∞ ≤ ‖g − g̃‖∞ + 4 log n√\nn\nwith probability at least 1 − 2n−3/2. Furthermore, by Lemma 3 and by our assumption, for any , δ > 0, there exists n ∈ N such that\n‖g − ˆ̃g‖∞ ≤ + 4 log n√\nn\nwith probability at least 1− 2n−3/2 − δ. Similarly, for h = hs, by Lemma 3 and by our assumption, using the same ( , δ), we have ‖h− h̃‖∞ ≤ . In particular, it follows that ‖h̃‖∞ ≥ γ − . Next, let\ng(t) = arg max s∈{±1},y∈Y\ngs,y(t)\nhs(t)\nˆ̃g(t) = arg max s∈{±1},y∈Y\nˆ̃gs,y(t)\nh̃s(t) .\nThen, note that∥∥∥∥∥ 1√hs − 1√h̃s ∥∥∥∥∥ ∞ = ∥∥∥∥∥ √ h̃s − √ hs√ hs · h̃s ∥∥∥∥∥ ∞ = ∥∥∥∥∥ h̃s − hs√hs · h̃s ∥∥∥∥∥ ∞ · ∥∥∥∥√h̃s +√hs∥∥∥∥−1 ∞\n≤ √ γ · (γ − )\n· (\n1√ γ − + 1 √ γ ) ≤\n2(γ − )3/2 ,\nfrom which it follows that∥∥∥∥∥ gs,y√hs − ˆ̃gs,y√h̃s ∥∥∥∥∥ ∞ = ∥∥∥∥∥ gs,y√hs − ( 1√ h̃s + 1√ hs − 1√ hs ) · ˆ̃gs,y ∥∥∥∥∥ ∞\n≤ ∥∥∥∥∥gs,y − ˆ̃gs,y√hs ∥∥∥∥∥ ∞ + ∥∥∥∥∥ ( 1√ h̃s − 1√ hs ) · ˆ̃gs,y ∥∥∥∥∥ ∞\n≤ √ γ + 4 log n √ γ · √ n + 2(γ − )3/2\n≤ 2 (γ − )3/2 + 4 log n √ γ · √ n .\nTherefore, letting m = |Y|, we have\n‖G− ˆ̃G‖∞\n≤ ∥∥∥∥∥∥ √√√√ ∑ s∈{±1} ∑ y∈Y g2s,y hs − √√√√ ∑ s∈{±1} ∑ y∈Y ˆ̃g2s,y h̃s ∥∥∥∥∥∥ ∞\n= ∥∥∥∥∥∥ ∑\ns∈{±1} ∑ y∈Y g2s,y hs − ∑ s∈{±1} ∑ y∈Y ˆ̃g2s,y h̃s ∥∥∥∥∥∥ ∞ · ∥∥∥∥∥∥ √√√√ ∑ s∈{±1} ∑ y∈Y g2s,y hs + √√√√ ∑ s∈{±1} ∑ y∈Y ˆ̃g2s,y h̃s ∥∥∥∥∥∥ −1\n∞ ≤ ∑\ns∈{±1} ∑ y∈Y ∥∥∥∥∥g2s,yhs − ˆ̃g 2 s,y h̃s ∥∥∥∥∥ ∞ · ‖g + ˆ̃g‖−1∞\n≤ ∑\ns∈{±1} ∑ y∈Y ∥∥∥∥∥ gs,y√hs − ˆ̃gs,y√h̃s ∥∥∥∥∥ ∞\n≤ 4m (γ − )3/2 + 8m log n √ γ · √ n .\nNow, let Pi be the marginal probability distribution of P along dimension i, let pi(t) ∝ ∫ p(x1, ..., xi−1, t, xi+1, ..., xd)dx1...dxi−1dxi+1...dxd\nbe an unnormalized marginal probability density function for Pi, and let\nFi(t) = ∫ t −∞ pi(t ′)dt′\nbe the corresponding unnormalized cumulative distribution function.\nNext, let ′′ > 0 be arbitrary; we choose its value later. By Lemma 2, for any ′′ > 0, there exists δG( ′′) > 0 such that G is ( ′′, δG( ′′)) gapped according to the function pi.\nRecall that we have assumed that and δ can be made arbitrarily close to zero by taking n to be sufficiently large. In other words, there exists n ∈ N such that both of the following hold:\n4m (γ − )3/2 · + 8m log n√ γ · √ n ≤ δG(\n′′)\n2 (5)\nδ + 2\nn3/2 ≤ δ′, (6)\nin which case\n‖G− ˆ̃G‖∞ ≤ δG(\n′′)\n2\nwith probability at least 1− δ′. Then, by Lemma 5, we have\n|Fi(t̃∗)− Fi(t∗)| ≤ ′′,\nso by Lemma 6, we have\n‖p′ − p̃′‖1 ≤ + ′′.\nThus, the claim follows taking ′ = + ′′.\nCorollary 2. Assume the same setup as in Lemma 1. Assume that γ ≥ 2 , and suppose that\nδg( ′′) ≥ κ ′′\nfor all ′′ > 0 and some constant κ > 0. For any ′, δ′ > 0, we have ‖p′ − p̃‖1 ≤ ′ with probability at least 1− δ′ as long as ‖p− p̃‖1 ≤ with probability at least δ, where\n≤ ( 1 + 4m\n(γ/2)3/2 · 4 κ\n)−1 · ′\nδ ≤ 1 2 · δ′,\nand n ≥ 3 satisfies\n≥ 2(γ/2) 3/2\n√ γ\n· log n√ n\nδ ≥ 2 n3/2 .\nProof. The claim follows from the proof of Lemma 1 by choosing\n′′ = 4m (γ/2)3/2 · 4 κ ·\nin (5)."
    }, {
      "heading" : "A.2 Proof of Theorem 1",
      "text" : "Proof. Let , δ > 0 be arbitrary; we need to show that there exists n ∈ N such that T is an approximation of T ∗ with probability at least 1− δ′′. First, for each leaf node N ∈ leaves(T ∗) of the exact tree, let φ(N) be the corresponding leaf in the learned tree T . We assume that sufficiently many samples are taken so that the estimates of the following information are correct with probability at least 1− δ4k(d+m) (where m = |Y|):\n• For each internal node, the dimension i ∈ [d] along which to branch.\n• For each leaf node, the label y assigned to that leaf node.\nFirst, we describe how to ensure that the optimal dimension i ∈ [d] along which to branch is chosen for each of the k2 internal nodes in T . In particular, by Assumption 2, there exists ∆ > 0 such that for every i 6= i∗ and for every t, we have\nG(i∗, t∗) > G(i, t) + ∆.\nLet Gi(t) = G(i, t). As long as ‖G − ˆ̃G‖∞ ≤ ∆2 for each i ∈ [d], then the optimal dimension is selected. Lemma 1 already shows that ‖G− ˆ̃G‖∞ is arbitrarily small for n ∈ N sufficiently large. Next, we describe how to ensure that the optimal label y is selected for each leaf node. In particular, labels are selected according to (3). To simplify notation, let\npy = ∫ I[f(x) = y] · I[CN ] · p(x)dx\nbe the unnormalized version of (3), which can equivalently be used to select the optimal label (since the normalization factor is a constant across all y ∈ Y). Our goal is to choose the optimal label\ny∗ = arg max y∈Y py\nfor a leaf node N = (y∗) in T ∗. By Assumption 2, there exists a gap ∆ > 0 such that for every y 6= y∗, we have py∗ ≥ py + ∆, so it suffices to show that |py − ˆ̃py| ≤ ∆2 , where ˆ̃py is our estimate of py, as before, with two kinds of error. In particular, for each y ∈ Y , we can break down the error of our estimate ˆ̃py into\n|py − ˆ̃py| ≤ |py − p̃y|+ |p̃y − ˆ̃py|, where\np̃ = ∫ I[f(x) = y] · I[Cφ(N)] · p(x)dx.\nWe assume that N satisfies Prx∼P [CN ] ≥ γ, so we have ‖pN − p̃N‖1 ≤ ′. Then, we have |py − p̃y| = ∣∣∣∣∫ I[f(x) = y] · (I[CN ]− I[Cφ(N)]) · p(x)dx∣∣∣∣ ≤ ‖pN − pφ(N)‖1 ≤ ′.\nFor the estimation error, by Hoeffding’s inequality, we have\nPrx(1),...,x(n)∼P [ | ˆ̃py − p̃y| ≥ ∆\n4\n] ≤ 2 exp ( −2n∆ 2\n16\n) .\nTaking ′ ≤ ∆4 , the total error |py − ˆ̃py| ≤ ∆ 2 . Taking a union bound over y ∈ Y , this inequality holds with probability 2m exp ( − 2n∆ 2\n16\n) , where m = |Y|. Since this probability is exponentially\ndecreasing in n, we can easily bound it by δ2 . Next, let N be a node in the exact tree T ∗, and let φ(N) be its corresponding node in the learned tree T . Let P | CN be the distribution over points that flow to N in T ∗, let P | Cφ(N) be the distribution over points that flow to φ(N) in T , and let\npN (x) = p(x) · I[CN ] pφ(N)(x) = p(x) · I[Cφ(N)]\nbe their respective unnormalized probability density functions.\nWe prove by induction that for each node N ∈ leaves(T ∗), one of the following holds:\n• The premise of Lemma 1 holds, i.e., for any ′, δ′ > 0, there exists n ∈ N such that ‖pN − pφ(N)‖1 ≤ ′ with probability at least 1− δ′.\n• A negligible amount of probability mass flows to N , i.e., Pr[CN ] ≤ γ = k .\nFor the root node NT∗ of T ∗, the hypothesis of Lemma 1 holds for P = P̃ , since then ‖p− p̃‖1 = 0 with probability 1.\nFor any node in N ′, suppose that the inductive hypothesis holds for its parent N . If a negligible amount of probability mass flows to N , then the same is true of N ′. Otherwise, the premise of Lemma 1 holds for N . Consider the gain (1) used to construct the branch CN = (xi∗ ≤ t∗ for N . First, it is easy to check that the maximum t∗ of the function G(t) in Lemma 1 equals the maximizer of the gain—the only difference between G(t) is the square root (which is monotone) and a normalization constant ∫ p(x)dx (since the probability density function p(x) in Lemma 1 may be unnormalized).\nTherefore, either the conclusion of Lemma 1 holds, in which case we have shown the inductive hypothesis, or ‖hs(t)‖∞ ≤ γ for some s ∈ {±1}. But\nγ ≥ ‖hs(t)‖∞ = ∫ I[CN ] · p(x)dx = Pr[CN ],\nso again, the inductive hypothesis holds.\nNow, note that\n|I[CN ]− I[Cφ(N)]| = (I[CN ]− I[Cφ(N)])2\n= I[CN ] + I[Cφ(N)]− 2 · I[CN ] · I[Cφ(N)] = (1− I[Cφ(N)]) · I[CN ] + I[Cφ(N)]− I[CN ] · I[Cφ(N)] ≥ (1− I[Cφ(N)]) · I[CN ].\nLet L be the set of leaves N in T ∗ for which Prx∼P [CN ] ≥ γ = k , i.e., at least γ fraction of the probability mass flows to N , and let L′ be the remaining leaves. Then, the total error of the decision tree is\nPrx∼P [T (x) 6= T ∗(x)] = ∑\nN∈leaves(T∗)\nPrx∼P [T (x) 6= T ∗(x) | CN ] · Prx∼P [CN ]\n≤ ∑ N∈L Prx∼P [¬Cφ(N) | CN ] · Prx∼P [CN ] + ∑ N∈L′ Prx∼P [CN ]\n≤ ∑ N∈L Prx∼P [¬Cφ(N) | CN ] · Prx∼P [CN ] + |L′| · γ\n= ∑ N∈L ∫ (1− I[Cφ(N)]) · I[CN ] · p(x)dx+ |L′| · γ\n≤ ∑ N∈L ∫ |I[CN ]− I[Cφ(N)]| · p(x)dx+ |L′| · γ\n≤ ∑ N∈L ∫ |I[CN ]− I[Cφ(N)]| · p(x)dx+ |L′| · γ\n= ∑ N∈L ‖pN − p̃N‖1 + |L′| · γ ≤ |L| · ′ + |L′| · γ ≤ ,\nwhere in the last step, we choose ′ = k . This inequality holds with probability at least 1− δ 2 − k · δ ′, by taking a union bound, including over the probability 1− δ2 that the label choices described above hold. Therefore, the result follows taking ′ = 2k and δ ′ = δ4k ."
    }, {
      "heading" : "A.3 Proof of Corollary 1",
      "text" : "Proof. Let , δ > 0 be arbitrary. We show that for the claimed value of n, the decision tree T extracted by our algorithm is ( , δ) exact. Following the proof of Thereom 1, it suffices to show that for each node N in T ∗, either\n‖pN − pφ(N)‖1 ≤\n2k\nwith probability at least δ′ = δ4k , or\nPrx∼P [CN ] ≤ γ =\nk .\nConsider the inequality\nPrx(1),...,x(n) [‖pN − p̃N‖1 ≥ ′′] ≤ δ′′.\nRecall that at the root N = NT , this inequality holds for all ′′, δ′′ > 0. By Corollary 2, as we descend the tree from N to a child node N ′, either this relation holds where the parameters ′′ picks up a factor of\n1 + 4m ( /2k)3/2 · 4 κ\n(where m = |Y|) and δ picks up a factor of 2, or the probability mass flowing to that node becomes small, i.e.,\nPrx∼P [CN ′ ] ≤ ′′\nk .\nTherefore, to obtain the desired bound for the leaf nodes (which are at depth D = log k, where k is the number of nodes in T and T ∗), we need\n′′ = ( 1 +\n4m ( /2k)3/2 · 4 κ )−D · k\nδ′′ = 2−D · δ 4k\nat the root node. By Corollary 2, to obtain these parameters, we need n ∈ N samples, where n ≥ 3 and\n2( /2k)3/2√ /k · log n√ n ≤ ′′ =\n( 1 +\n4m ( /2k)3/2 · 4 κ )−D · k\n2 n3/2 ≤ δ′′ = 2−D · δ 4k .\nIt is easy to check that for n ≥ 4, taking\nn ≥ max {( 1 +\n16m\n( /2k)3/2 · κ\n)3D , 2D+1k\nδ\n} .\nsuffices to satisfy these inequalities."
    }, {
      "heading" : "A.4 Gap for Random Forests",
      "text" : "We briefly discuss why intuitively, axis-aligned random forests should satisfy the premise of Corollary 1. In particular, note that axis-aligned random forests are piecewise constant, and furthermore the pieces are axis aligned. Therefore, the label frequencies Prx∼P [f(x) = y | C] are piecewise linear, so the gain in (1) is quadratic. As we showed in Lemma 1, we can extract a square root from the gain without affecting our theoretical results, in which case the gain becomes approximately linear again. Another issue is that we are integrating against p(x), and the presence of the weighting term Prx∼P [C]. However, as long as p(x) is fairly close to uniform, then its effect on the gain is fairly negligible, so we expect that the premises are satisfied."
    }, {
      "heading" : "B Proofs of Technical Lemmas",
      "text" : "In this section, we prove the technical lemmas required for our proofs of Lemma 2, Theorem 1, and Corollary 1."
    }, {
      "heading" : "B.1 Proof of Existence of a Gap",
      "text" : "We prove that a gap exists for reasonably behaved functions.\nLemma 2. Let g : R→ [0, 1] be a continuous function with bounded support, and assume that its global maximizer\nt∗ = arg max t∈R g(t)\nis unique. Then, for any ′ > 0, there exists δ′ > 0 such that g is ( ′, δ′) gapped.\nProof. Let tmax be a bound on the support of g, i.e., g(t) = 0 if |t| > tmax. Let ′ > 0 be arbitrary, and let\nA ′ = {t ∈ R | |t| ≤ tmax and |F (t)− F (t∗)| ≥ ′}.\nNote that A ′ is a compact set, so g achieves its maximum on A ′ , i.e.,\nt∗ ′ = arg max t∈A ′ g(t).\nThen, the result follows for\nδ′ = g(t∗)− g(t∗ ′)\n2 > 0.\nNote that we divide by 2 since the inequality in Definition 2 is strict."
    }, {
      "heading" : "B.2 Bound on Intrinsic Error",
      "text" : "The following lemma enables us to bound the intrinsic error ‖g − g̃‖∞ given that the distributions have probability density functions bounded in L1 norm.\nLemma 3. Let X ⊆ Rd, and let P and P̃ be distributions over X , and let p(x) and p̃(x) be unnormalized probability density functions for P and P̃ , respectively, that satisfy ‖p− p̃‖1 ≤ . Let β : X × R→ [0, 1] be any function, and let\ng(t) = ∫ β(x, t) · p(x)dx\ng̃(t) = ∫ β(x, t) · p̃(x)dx.\nThen, we have\n‖g − g̃‖∞ ≤ .\nProof. Note that\n‖g − g̃‖∞ = sup t∈R |g − g̃| ≤ sup t∈R\n∫ β(x, t)|p(x)− p̃(x)|dx\n≤ sup t∈R\n∫ |p(x)− p̃(x)|dx\n≤ ,\nas claimed."
    }, {
      "heading" : "B.3 Bound on Estimation Error",
      "text" : "The following lemma enables us to bound the estimation error ‖g̃ − ˆ̃g‖∞. Lemma 4. Let X ⊆ Rd, let P be a distribution over X , and let p(x) be an unnormalized probability density function for P . Let α : X → [0, 1] be an arbitrary function, let i ∈ [d], and let\ng(t) = ∫ α(x) · I[xi ≤ t] · p(x)dx.\nLet x(1), ..., x(n) be i.i.d. samples from P , and let\nĝ(t) = Z\nn n∑ i=1 α(x(k)) · I[x(k)i ≤ t]\nbe the empirical estimate of g on these points, where\nZ = ∫ p(x)dx\nis a normalization constant. Then, we have\nPrx(1),...,x(n)∼P [ ‖g − ĝ‖∞ ≥\n4 log n√ n ] ≤ 2 n3/2 ,\nfor any n ≥ 3.\nProof. First, we define points t0, t1, ..., t√n ∈ R that divide R into √ n intervals according to the marginal probability density function pi(t) along dimension i (for convenience, we assume n is a perfect square), which has corresponding cumulative distribution function Fi(t). Then, we choose tj to satisfy\ntj ∈ F−1i ( j · Z√ n ) .\nFor convenience, we choose t0 = −∞ and t√n = ∞, which satisfy the condition. Now, for each j ∈ [ √ n], let Ij = (tj−1, tj ]. Note that these intervals cover R, i.e., R = I1 ∪ ... ∪ I√n.\nThen, we can decompose the quantity ‖g − ĝ‖∞ into three parts:\n‖g − ĝ‖∞ = sup t∈R |g(t)− ĝ(t)|\n= sup j∈[ √ n] sup t∈Ij |g(t)− ĝ(t)|\n≤ sup j∈[ √ n] sup t∈Ij {|g(t)− g(tj)|+ |g(tj)− ĝ(tj)|+ |ĝ(tj)− ĝ(t)|}\n≤ sup j∈[ √ n] sup t∈Ij |g(t)− g(tj)|+ sup j∈[ √ n] |g(tj)− ĝ(tj)|+ sup j∈[ √ n] sup t∈Ij |ĝ(tj)− ĝ(t)|.\nWe show that each of these three parts can be made arbitrarily small with high probability by taking n sufficiently large.\nFirst, consider the term supj∈[√n] supt∈Ij |g(t)− g(tj)|. Since t ≤ tj , we have I[xi ≤ t] ≤ I[xi ≤ tj ]. Thus, for all t ∈ Ij , we have\n|g(t)− g(tj)| = ∣∣∣∣∫ α(x) · (I[xi ≤ t]− I[xi ≤ tj ]) · p(x)dx∣∣∣∣\n≤ ∫ (I[xi ≤ tj ]− I[xi ≤ t]) · p(x)dx\n= Fi(tj)− Fi(t) ≤ n−1/2,\nwhere the last inequality follows from the definition of tj and the fact that t ∈ Ij .\nSecond, consider the term supj∈[√n] |g(tj) − ĝ(tj)|. Note that each Z · α(x(k)) · I[x (k) i ≥ t] is a random variable in [0, 1]. Therefore, by the Hoeffding inequality, we have\nPrx(1),...,x(n)∼P [ |g(tj)− ĝ(tj)| ≥ log n\nn\n] ≤ e−2(logn) 2\n≤ 1 n2\nfor n ≥ 3. By a union bound, this inequality holds for every j ∈ [ √ n] with probability n−3/2. Third, consider the term supj∈[√n] supt∈Ij |ĝ(tj) − ĝ(t)|. We first show that for every j ∈ [ √ n], the interval Ij contains at most n1/2 log n of the points x(1), ..., x(n) with high probability. By the definition of the points tj , the probability that a single randomly selected point x(k) falls in Ij is n−1/2 (since the points tj were constructed according to the cumulative distribution function Fi):\nM = Ex∼P [I[x ∈ Ij ]] = Prx∼P [x ∈ Ij ] = 1√ n .\nThen, the fraction of the n points x(k) that fall in the interval Ij is\nM̂ = 1\nn n∑ i=1 I[x(k) ∈ Ij ].\nNote that each I[x(k) ∈ Ij ] is an random variable in [0, 1], so by Hoeffding’s inequality, we have\nPrx(1),...,x(n)∼P\n[ |M̂ −M | ≥ log n√\nn\n] ≤ e−2(logn) 2\n≤ 1 n2 .\nNow, note that each point x(k) in Ij can increase the value of |ĝ(tj)− ĝ(t)| by at most n−1. Since there are n · M̂ points x(k) in Ij , the total increase is bounded by M̂ , i.e.,\nPrx(1),...,x(n)∼P [ sup t∈Ij |ĝ(tj)− ĝ(t)| ≥ 2 log n√ n ] ≤ 1 n2 .\nAs before, by a union bound, this inequality holds for every j ∈ [ √ n] with probability n−3/2.\nPutting these three results together, we can conclude that for n ≥ 3, we have\nPrx(1),...,x(n)∼P [ ‖g − ĝ‖∞ ≥\n4 log n√ n ] ≤ 2 n3/2 ,\nas claimed."
    }, {
      "heading" : "B.4 Bound on Error of Maximizers",
      "text" : "The following lemma enables us to bound the maximizer of two functions that are close in L∞ norm. Lemma 5. Let P be a probability distribution over R, and let F (t) be an cumulative distribution function for P . Suppose that g : R → R is ( , δ) gapped according to P , and h : R → R satisfies ‖g − h‖∞ ≤ δ2 . Let\nt∗g = arg max t∈R g(t) t∗h = arg max t∈R h(t).\nThen, we have |F (t∗g)− F (t∗h)| ≤ .\nProof. By definition of the L∞ norm, we have\ng(t∗g)− h(t∗g) ≤ δ\n2\nh(t∗h)− g(t∗h) ≤ δ\n2 .\nCombining these gives g(t∗g)− g(t∗h) ≤ δ + h(tg)∗ − h(t∗h) ≤ δ, where the second inequality follows because t∗h is a maximizer of h. Since g is ( , δ) gapped, and since t∗g is the maximizer of g, this inequality implies the claim."
    }, {
      "heading" : "B.5 Bound on Error of Probabiliy Density Functions",
      "text" : "The following lemma enables us to bound the L1 error of the estimated probability density function.\nLemma 6. Let X ⊆ Rd, let P and P̃ be distributions over X , and let p(x) and p̃(x) be unnormalized probability density functions for P and P̃ , respectively, satisfying ‖p− p̃‖ ≤ . Let i ∈ [d], let Pi be the marginal distribution of P along dimension i, let Fi be an unnormalized cumulative distribution function for Pi, and let t, t̃ ∈ R satisfying |Fi(t)− Fi(t̃)| ≤ ′. Let i ∈ [d], and let\np′(x) = p(x) · I[xi ≤ t] p̃′(x) = p̃(x) · I[xi ≤ t̃]\nbe the unnormalized probability density functions for P | (xi ≤ t) and P̃ | (xi ≤ t̃), respectively. Then, we have\n‖p′ − p̃′‖1 ≤ + ′.\nProof. Assume without loss of generality that t ≤ t̃. Then, we have ‖p′ − p̃′‖1 = ∫ |p′(x)− p̃′(x)|dx\n= ∫ |p(x) · I[xi ≤ t]− p̃(x) · I[xi ≤ t̃]|dx\n= ∫ |p(x) · I[xi ≤ t]− (p(x) + p̃(x)− p(x)) · I[xi ≤ t̃]|dx\n≤ ∫ p(x) · |I[xi ≤ t]− I[xi ≤ t̃]|dx+ ∫ |p̃(x)− p(x)| · I[xi ≤ t̃]dx\n≤ ∫ p(x) · |I[xi ≤ t]− I[xi ≤ t̃]|dx+\n= ∫ p(x) · I[t ≤ xi ≤ t̃]dx+\n= |Fi(t̃)− Fi(t)|+ ≤ ′ + ,\nas claimed."
    } ],
    "references" : [ {
      "title" : "Learning certifiably optimal rule lists for categorical data",
      "author" : [ "Elaine Angelino", "Nicholas Larus-Stone", "Daniel Alabi", "Margo Seltzer", "Cynthia Rudin" ],
      "venue" : null,
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2017
    }, {
      "title" : "Do deep nets really need to be deep? In Advances in neural information processing",
      "author" : [ "Jimmy Ba", "Rich Caruana" ],
      "venue" : null,
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2014
    }, {
      "title" : "Neuronlike adaptive elements that can solve difficult learning control problems",
      "author" : [ "Andrew G Barto", "Richard S Sutton", "Charles W Anderson" ],
      "venue" : "IEEE transactions on systems, man, and cybernetics,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 1983
    }, {
      "title" : "Classification and regression trees",
      "author" : [ "Leo Breiman", "Jerome Friedman", "Charles J Stone", "Richard A Olshen" ],
      "venue" : "CRC press,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 1984
    }, {
      "title" : "API design for machine learning software: experiences from the scikit-learn project",
      "author" : [ "Lars Buitinck", "Gilles Louppe", "Mathieu Blondel", "Fabian Pedregosa", "Andreas Mueller", "Olivier Grisel", "Vlad Niculae", "Peter Prettenhofer", "Alexandre Gramfort", "Jaques Grobler", "Robert Layton", "Jake VanderPlas", "Arnaud Joly", "Brian Holt", "Gaël Varoquaux" ],
      "venue" : "In ECML PKDD Workshop: Languages for Data Mining and Machine Learning,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2013
    }, {
      "title" : "Intelligible models for classification and regression",
      "author" : [ "Rich Caruana", "Yin Lou", "Johannes Gehrke" ],
      "venue" : "In Proceedings of the 23rd ACM SIGKDD Conference on Knowledge Discovery and Data Mining. Citeseer,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2012
    }, {
      "title" : "Intelligible models for healthcare: Predicting pneumonia risk and hospital 30-day readmission",
      "author" : [ "Rich Caruana", "Yin Lou", "Johannes Gehrke", "Paul Koch", "Marc Sturm", "Noemie Elhadad" ],
      "venue" : "In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2015
    }, {
      "title" : "Using data mining to predict secondary school student",
      "author" : [ "Paulo Cortez", "Alice Maria Gonçalves Silva" ],
      "venue" : null,
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2008
    }, {
      "title" : "Interpreting tree ensembles with intrees",
      "author" : [ "Houtao Deng" ],
      "venue" : "arXiv preprint arXiv:1408.5456,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2014
    }, {
      "title" : "Algorithm aversion: People erroneously avoid algorithms after seeing them err",
      "author" : [ "Berkeley J Dietvorst", "Joseph P Simmons", "Cade Massey" ],
      "venue" : "Journal of Experimental Psychology: General,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2015
    }, {
      "title" : "Mining high-speed data streams",
      "author" : [ "Pedro Domingos", "Geoff Hulten" ],
      "venue" : "In Proceedings of the sixth ACM SIGKDD international conference on Knowledge discovery and data mining,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2000
    }, {
      "title" : "A roadmap for a rigorous science of interpretability",
      "author" : [ "Finale Doshi-Velez", "Been Kim" ],
      "venue" : "arXiv preprint arXiv:1702.08608,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2017
    }, {
      "title" : "Fairness through awareness",
      "author" : [ "Cynthia Dwork", "Moritz Hardt", "Toniann Pitassi", "Omer Reingold", "Richard Zemel" ],
      "venue" : "In Proceedings of the 3rd Innovations in Theoretical Computer Science Conference,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2012
    }, {
      "title" : "An extendible package for data exploration, classification and correlation",
      "author" : [ "M Forina" ],
      "venue" : "Institute of Pharmaceutical and Food Analisys and Technologies, Via Brigata Salerno,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 1991
    }, {
      "title" : "Greedy function approximation: a gradient boosting machine",
      "author" : [ "Jerome H Friedman" ],
      "venue" : "Annals of statistics,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2001
    }, {
      "title" : "Learning differential diagnosis of erythemato-squamous diseases using voting feature intervals",
      "author" : [ "H Altay Güvenir", "Gülşen Demiröz", "Nilsel Ilter" ],
      "venue" : "Artificial intelligence in medicine,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 1998
    }, {
      "title" : "Equality of opportunity in supervised learning",
      "author" : [ "Moritz Hardt", "Eric Price", "Nati Srebro" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2016
    }, {
      "title" : "Simple rules for complex decisions",
      "author" : [ "Jongbin Jung", "Connor Concannon", "Ravi Shroff", "Sharad Goel", "Daniel G Goldstein" ],
      "venue" : "arXiv preprint arXiv:1702.04690,",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2017
    }, {
      "title" : "Human decisions and machine predictions",
      "author" : [ "Jon Kleinberg", "Himabindu Lakkaraju", "Jure Leskovec", "Jens Ludwig", "Sendhil Mullainathan" ],
      "venue" : "Technical report, National Bureau of Economic Research,",
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 2017
    }, {
      "title" : "Understanding black-box predictions via influence functions",
      "author" : [ "P.W. Koh", "P. Liang" ],
      "venue" : "arXiv preprint arXiv:1703.04730,",
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 2017
    }, {
      "title" : "Machine learning for medical diagnosis: history, state of the art and perspective",
      "author" : [ "Igor Kononenko" ],
      "venue" : "Artificial Intelligence in medicine,",
      "citeRegEx" : "30",
      "shortCiteRegEx" : "30",
      "year" : 2001
    }, {
      "title" : "Interpretable classifiers using rules and bayesian analysis: Building a better stroke prediction model",
      "author" : [ "Benjamin Letham", "Cynthia Rudin", "Tyler H McCormick", "David Madigan" ],
      "venue" : "The Annals of Applied Statistics,",
      "citeRegEx" : "31",
      "shortCiteRegEx" : "31",
      "year" : 2015
    }, {
      "title" : "An unexpected unity among methods for interpreting model predictions",
      "author" : [ "Scott Lundberg", "Su-In Lee" ],
      "venue" : "arXiv preprint arXiv:1611.07478,",
      "citeRegEx" : "32",
      "shortCiteRegEx" : "32",
      "year" : 2016
    }, {
      "title" : "Efficient Memory-based Learning for Robot Control",
      "author" : [ "Andrew William Moore" ],
      "venue" : "PhD thesis, University of Cambridge,",
      "citeRegEx" : "33",
      "shortCiteRegEx" : "33",
      "year" : 1990
    }, {
      "title" : "Discrimination-aware data mining",
      "author" : [ "Dino Pedreshi", "Salvatore Ruggieri", "Franco Turini" ],
      "venue" : "In Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining,",
      "citeRegEx" : "35",
      "shortCiteRegEx" : "35",
      "year" : 2008
    }, {
      "title" : "Combining instance-based and model-based learning",
      "author" : [ "J Ross Quinlan" ],
      "venue" : "In Proceedings of the Tenth International Conference on Machine Learning,",
      "citeRegEx" : "36",
      "shortCiteRegEx" : "36",
      "year" : 1993
    }, {
      "title" : "Why should i trust you?: Explaining the predictions of any classifier",
      "author" : [ "Marco Tulio Ribeiro", "Sameer Singh", "Carlos Guestrin" ],
      "venue" : "In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,",
      "citeRegEx" : "37",
      "shortCiteRegEx" : "37",
      "year" : 2016
    }, {
      "title" : "Algorithms for interpretable machine learning",
      "author" : [ "Cynthia Rudin" ],
      "venue" : "In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining,",
      "citeRegEx" : "38",
      "shortCiteRegEx" : "38",
      "year" : 2014
    }, {
      "title" : "Not just a black box: Learning important features through propagating activation differences",
      "author" : [ "Avanti Shrikumar", "Peyton Greenside", "Anna Shcherbina", "Anshul Kundaje" ],
      "venue" : "arXiv preprint arXiv:1605.01713,",
      "citeRegEx" : "39",
      "shortCiteRegEx" : "39",
      "year" : 2016
    }, {
      "title" : "Prostate specific antigen in the diagnosis and treatment of adenocarcinoma of the prostate. ii. radical prostatectomy treated patients",
      "author" : [ "Thomas A Stamey", "John N Kabalin", "John E McNeal", "Iain M Johnstone", "Fuad Freiha", "Elise A Redwine", "Norman Yang" ],
      "venue" : "The Journal of urology,",
      "citeRegEx" : "40",
      "shortCiteRegEx" : "40",
      "year" : 1989
    }, {
      "title" : "Reinforcement learning: An introduction, volume 1",
      "author" : [ "Richard S Sutton", "Andrew G Barto" ],
      "venue" : "MIT press Cambridge,",
      "citeRegEx" : "41",
      "shortCiteRegEx" : "41",
      "year" : 1998
    }, {
      "title" : "Collision avoidance for unmanned aircraft using markov decision processes",
      "author" : [ "Selim Temizer", "Mykel Kochenderfer", "Leslie Kaelbling", "Tomas Lozano-Pérez", "James Kuchar" ],
      "venue" : "In AIAA guidance, navigation, and control conference,",
      "citeRegEx" : "42",
      "shortCiteRegEx" : "42",
      "year" : 2010
    }, {
      "title" : "Supersparse linear integer models for optimized medical scoring systems",
      "author" : [ "Berk Ustun", "Cynthia Rudin" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "43",
      "shortCiteRegEx" : "43",
      "year" : 2016
    }, {
      "title" : "Mediboost: a patient stratification tool for interpretable decision making in the era of precision medicine",
      "author" : [ "Gilmer Valdes", "José Marcio Luna", "Eric Eaton", "Charles B Simone" ],
      "venue" : "Scientific Reports,",
      "citeRegEx" : "44",
      "shortCiteRegEx" : "44",
      "year" : 2016
    }, {
      "title" : "Seeing the forest through the trees",
      "author" : [ "Anneleen Van Assche", "Hendrik Blockeel" ],
      "venue" : "In International Conference on Inductive Logic Programming,",
      "citeRegEx" : "45",
      "shortCiteRegEx" : "45",
      "year" : 2007
    }, {
      "title" : "Genesim: genetic extraction of a single, interpretable model",
      "author" : [ "Gilles Vandewiele", "Olivier Janssens", "Femke Ongenae", "Filip De Turck", "Sofie Van Hoecke" ],
      "venue" : "arXiv preprint arXiv:1611.05722,",
      "citeRegEx" : "46",
      "shortCiteRegEx" : "46",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 20,
      "context" : "Recent advances in machine learning have revolutionized our ability to use data to inform critical decisions, such as medical diagnosis [30, 16, 44], bail decisions for defendants [28, 27] and the design of aircraft collision avoidance systems [42].",
      "startOffset" : 136,
      "endOffset" : 148
    }, {
      "referenceID" : 6,
      "context" : "Recent advances in machine learning have revolutionized our ability to use data to inform critical decisions, such as medical diagnosis [30, 16, 44], bail decisions for defendants [28, 27] and the design of aircraft collision avoidance systems [42].",
      "startOffset" : 136,
      "endOffset" : 148
    }, {
      "referenceID" : 33,
      "context" : "Recent advances in machine learning have revolutionized our ability to use data to inform critical decisions, such as medical diagnosis [30, 16, 44], bail decisions for defendants [28, 27] and the design of aircraft collision avoidance systems [42].",
      "startOffset" : 136,
      "endOffset" : 148
    }, {
      "referenceID" : 18,
      "context" : "Recent advances in machine learning have revolutionized our ability to use data to inform critical decisions, such as medical diagnosis [30, 16, 44], bail decisions for defendants [28, 27] and the design of aircraft collision avoidance systems [42].",
      "startOffset" : 180,
      "endOffset" : 188
    }, {
      "referenceID" : 17,
      "context" : "Recent advances in machine learning have revolutionized our ability to use data to inform critical decisions, such as medical diagnosis [30, 16, 44], bail decisions for defendants [28, 27] and the design of aircraft collision avoidance systems [42].",
      "startOffset" : 180,
      "endOffset" : 188
    }, {
      "referenceID" : 31,
      "context" : "Recent advances in machine learning have revolutionized our ability to use data to inform critical decisions, such as medical diagnosis [30, 16, 44], bail decisions for defendants [28, 27] and the design of aircraft collision avoidance systems [42].",
      "startOffset" : 244,
      "endOffset" : 248
    }, {
      "referenceID" : 6,
      "context" : ", inability to distinguish causal effects from correlations) [34, 16], fairness (i.",
      "startOffset" : 61,
      "endOffset" : 69
    }, {
      "referenceID" : 12,
      "context" : ", internalizing prejudices present in training data) [22, 26], and algorithm aversion (i.",
      "startOffset" : 53,
      "endOffset" : 61
    }, {
      "referenceID" : 16,
      "context" : ", internalizing prejudices present in training data) [22, 26], and algorithm aversion (i.",
      "startOffset" : 53,
      "endOffset" : 61
    }, {
      "referenceID" : 9,
      "context" : ", lack of trust by end users) [19].",
      "startOffset" : 30,
      "endOffset" : 34
    }, {
      "referenceID" : 27,
      "context" : "Interpretability is a promising approach to address these challenges [38, 21]—in particular, we can help human users diagnose issues and verify correctness of machine learning models by providing insight into the model’s reasoning [43, 8, 37, 31, 29].",
      "startOffset" : 69,
      "endOffset" : 77
    }, {
      "referenceID" : 11,
      "context" : "Interpretability is a promising approach to address these challenges [38, 21]—in particular, we can help human users diagnose issues and verify correctness of machine learning models by providing insight into the model’s reasoning [43, 8, 37, 31, 29].",
      "startOffset" : 69,
      "endOffset" : 77
    }, {
      "referenceID" : 32,
      "context" : "Interpretability is a promising approach to address these challenges [38, 21]—in particular, we can help human users diagnose issues and verify correctness of machine learning models by providing insight into the model’s reasoning [43, 8, 37, 31, 29].",
      "startOffset" : 231,
      "endOffset" : 250
    }, {
      "referenceID" : 0,
      "context" : "Interpretability is a promising approach to address these challenges [38, 21]—in particular, we can help human users diagnose issues and verify correctness of machine learning models by providing insight into the model’s reasoning [43, 8, 37, 31, 29].",
      "startOffset" : 231,
      "endOffset" : 250
    }, {
      "referenceID" : 26,
      "context" : "Interpretability is a promising approach to address these challenges [38, 21]—in particular, we can help human users diagnose issues and verify correctness of machine learning models by providing insight into the model’s reasoning [43, 8, 37, 31, 29].",
      "startOffset" : 231,
      "endOffset" : 250
    }, {
      "referenceID" : 21,
      "context" : "Interpretability is a promising approach to address these challenges [38, 21]—in particular, we can help human users diagnose issues and verify correctness of machine learning models by providing insight into the model’s reasoning [43, 8, 37, 31, 29].",
      "startOffset" : 231,
      "endOffset" : 250
    }, {
      "referenceID" : 19,
      "context" : "Interpretability is a promising approach to address these challenges [38, 21]—in particular, we can help human users diagnose issues and verify correctness of machine learning models by providing insight into the model’s reasoning [43, 8, 37, 31, 29].",
      "startOffset" : 231,
      "endOffset" : 250
    }, {
      "referenceID" : 24,
      "context" : "Omitting the feature might not suffice to avoid prejudice, since the model could reconstruct that feature from other features [35].",
      "startOffset" : 126,
      "endOffset" : 130
    }, {
      "referenceID" : 11,
      "context" : ", how model predictions are affected by changing the prejudiced feature [21].",
      "startOffset" : 72,
      "endOffset" : 76
    }, {
      "referenceID" : 21,
      "context" : "In this paper, we take T to be a decision tree, which has been established as highly interpretable [31, 37, 8].",
      "startOffset" : 99,
      "endOffset" : 110
    }, {
      "referenceID" : 26,
      "context" : "In this paper, we take T to be a decision tree, which has been established as highly interpretable [31, 37, 8].",
      "startOffset" : 99,
      "endOffset" : 110
    }, {
      "referenceID" : 0,
      "context" : "In this paper, we take T to be a decision tree, which has been established as highly interpretable [31, 37, 8].",
      "startOffset" : 99,
      "endOffset" : 110
    }, {
      "referenceID" : 34,
      "context" : "Previous model extraction approaches have focused on interpreting specific families of models such as random forests [45, 18, 46], enabling them to leverage domain-specific knowledge about the internal structure of the model.",
      "startOffset" : 117,
      "endOffset" : 129
    }, {
      "referenceID" : 8,
      "context" : "Previous model extraction approaches have focused on interpreting specific families of models such as random forests [45, 18, 46], enabling them to leverage domain-specific knowledge about the internal structure of the model.",
      "startOffset" : 117,
      "endOffset" : 129
    }, {
      "referenceID" : 35,
      "context" : "Previous model extraction approaches have focused on interpreting specific families of models such as random forests [45, 18, 46], enabling them to leverage domain-specific knowledge about the internal structure of the model.",
      "startOffset" : 117,
      "endOffset" : 129
    }, {
      "referenceID" : 1,
      "context" : "The key challenge to learning accurate decision trees is that they often overfit and obtain poor performance, whereas complex models such as random forests and deep neural nets are better regularized [9].",
      "startOffset" : 200,
      "endOffset" : 203
    }, {
      "referenceID" : 3,
      "context" : "We show that our active learning approach substantially improves over using CART [11], a standard decision tree learning algorithm.",
      "startOffset" : 81,
      "endOffset" : 85
    }, {
      "referenceID" : 3,
      "context" : "CART [11] is considered highly interpretable, but its accuracy is often lacking in practice; [31, 8] tackle this concern by producing sparse rule lists that resemble decision trees yet are non-greedy, thus enabling them to learn more accurate models without sacrificing interpretability.",
      "startOffset" : 5,
      "endOffset" : 9
    }, {
      "referenceID" : 21,
      "context" : "CART [11] is considered highly interpretable, but its accuracy is often lacking in practice; [31, 8] tackle this concern by producing sparse rule lists that resemble decision trees yet are non-greedy, thus enabling them to learn more accurate models without sacrificing interpretability.",
      "startOffset" : 93,
      "endOffset" : 100
    }, {
      "referenceID" : 0,
      "context" : "CART [11] is considered highly interpretable, but its accuracy is often lacking in practice; [31, 8] tackle this concern by producing sparse rule lists that resemble decision trees yet are non-greedy, thus enabling them to learn more accurate models without sacrificing interpretability.",
      "startOffset" : 93,
      "endOffset" : 100
    }, {
      "referenceID" : 32,
      "context" : "Alternatively, [43] proposes supersparse linear integer models, which are sparse linear models where the coefficients are integer valued, thus resembling risk-scoring systems constructed manually by humans for applications such as medical diagnosis or criminal rescidivism.",
      "startOffset" : 15,
      "endOffset" : 19
    }, {
      "referenceID" : 17,
      "context" : "This approach is extended by [27] for classification problems with binary features.",
      "startOffset" : 29,
      "endOffset" : 33
    }, {
      "referenceID" : 5,
      "context" : "Finally, [15] propose generalized additive models, which are linear combinations of arbitrarily complex single-feature models, as interpretable models.",
      "startOffset" : 9,
      "endOffset" : 13
    }, {
      "referenceID" : 26,
      "context" : "For instance, given a new test point x, [37] generates an interpretation for the prediction f(x) by fitting a simple model locally around x and using this simple model to explain the prediction.",
      "startOffset" : 40,
      "endOffset" : 44
    }, {
      "referenceID" : 22,
      "context" : "Similarly, [32] uses the Shapley value to determine the most influential features for a given prediction.",
      "startOffset" : 11,
      "endOffset" : 15
    }, {
      "referenceID" : 14,
      "context" : ", random forests) [24].",
      "startOffset" : 18,
      "endOffset" : 22
    }, {
      "referenceID" : 28,
      "context" : "Similarly, [39] proposes a method for computing influence scores for features in deep neural nets.",
      "startOffset" : 11,
      "endOffset" : 15
    }, {
      "referenceID" : 3,
      "context" : "2 Exact Greedy Decision Tree We describe the exact greedy decision tree T ∗ of size k, which closely mirrors CART [11].",
      "startOffset" : 114,
      "endOffset" : 118
    }, {
      "referenceID" : 10,
      "context" : "A related result is [20], but their analysis is limited to discrete features, for which convergence is much easier to analyze.",
      "startOffset" : 20,
      "endOffset" : 24
    }, {
      "referenceID" : 4,
      "context" : "We implement our algorithm on top of scikit-learn [14], and evaluate it in two ways.",
      "startOffset" : 50,
      "endOffset" : 54
    }, {
      "referenceID" : 3,
      "context" : "First, we show that it outperforms a baseline that uses CART [11], a popular decision tree learning algorithm.",
      "startOffset" : 61,
      "endOffset" : 65
    }, {
      "referenceID" : 29,
      "context" : "945 prostate cancer [40] classify 97 9 random forest 0.",
      "startOffset" : 20,
      "endOffset" : 24
    }, {
      "referenceID" : 15,
      "context" : "818 dermatology [25] classify 366 34 random forest 0.",
      "startOffset" : 16,
      "endOffset" : 20
    }, {
      "referenceID" : 13,
      "context" : "967 wine origin [23] classify 178 13 random forest 0.",
      "startOffset" : 16,
      "endOffset" : 20
    }, {
      "referenceID" : 25,
      "context" : "890 auto mpg [36] regress 398 8 random forest 8.",
      "startOffset" : 13,
      "endOffset" : 17
    }, {
      "referenceID" : 7,
      "context" : "51 student grade [17] regress 382 33 random forest 4.",
      "startOffset" : 17,
      "endOffset" : 21
    }, {
      "referenceID" : 29,
      "context" : "949 prostate cancer [40] classify 97 9 neural net 0.",
      "startOffset" : 20,
      "endOffset" : 24
    }, {
      "referenceID" : 15,
      "context" : "820 dermatology [25] classify 366 34 neural net 0.",
      "startOffset" : 16,
      "endOffset" : 20
    }, {
      "referenceID" : 13,
      "context" : "964 wine origin [23] classify 178 13 neural net 0.",
      "startOffset" : 16,
      "endOffset" : 20
    }, {
      "referenceID" : 25,
      "context" : "905 auto mpg [36] regress 398 8 neural net 13.",
      "startOffset" : 13,
      "endOffset" : 17
    }, {
      "referenceID" : 7,
      "context" : "59 student grade [17] regress 382 33 neural net 6.",
      "startOffset" : 17,
      "endOffset" : 21
    }, {
      "referenceID" : 2,
      "context" : "10 cartpole [10] reinforce 100 4 control policy 200.",
      "startOffset" : 12,
      "endOffset" : 16
    }, {
      "referenceID" : 23,
      "context" : "8% mountain car [33] reinforce 100 2 control policy -140.",
      "startOffset" : 16,
      "endOffset" : 20
    }, {
      "referenceID" : 30,
      "context" : "The cartpole and mountain car control policies are learned using Q-learning [41].",
      "startOffset" : 76,
      "endOffset" : 80
    }, {
      "referenceID" : 2,
      "context" : "For the cartpole (discrete actions) [2, 10], the learner uses a discretized state space (7 bins per dimension) [1], and for the mountain car (discrete actions) [4, 33], it approximates the Q-function with a linear model over RBF features [3].",
      "startOffset" : 36,
      "endOffset" : 43
    }, {
      "referenceID" : 23,
      "context" : "For the cartpole (discrete actions) [2, 10], the learner uses a discretized state space (7 bins per dimension) [1], and for the mountain car (discrete actions) [4, 33], it approximates the Q-function with a linear model over RBF features [3].",
      "startOffset" : 160,
      "endOffset" : 167
    }, {
      "referenceID" : 0,
      "context" : "[8] Elaine Angelino, Nicholas Larus-Stone, Daniel Alabi, Margo Seltzer, and Cynthia Rudin.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 1,
      "context" : "[9] Jimmy Ba and Rich Caruana.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 2,
      "context" : "[10] Andrew G Barto, Richard S Sutton, and Charles W Anderson.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 3,
      "context" : "[11] Leo Breiman, Jerome Friedman, Charles J Stone, and Richard A Olshen.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 4,
      "context" : "[14] Lars Buitinck, Gilles Louppe, Mathieu Blondel, Fabian Pedregosa, Andreas Mueller, Olivier Grisel, Vlad Niculae, Peter Prettenhofer, Alexandre Gramfort, Jaques Grobler, Robert Layton, Jake VanderPlas, Arnaud Joly, Brian Holt, and Gaël Varoquaux.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 5,
      "context" : "[15] Rich Caruana, Yin Lou, and Johannes Gehrke.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 6,
      "context" : "[16] Rich Caruana, Yin Lou, Johannes Gehrke, Paul Koch, Marc Sturm, and Noemie Elhadad.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 7,
      "context" : "[17] Paulo Cortez and Alice Maria Gonçalves Silva.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 8,
      "context" : "[18] Houtao Deng.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 9,
      "context" : "[19] Berkeley J Dietvorst, Joseph P Simmons, and Cade Massey.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 10,
      "context" : "[20] Pedro Domingos and Geoff Hulten.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 11,
      "context" : "[21] Finale Doshi-Velez and Been Kim.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 12,
      "context" : "[22] Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Richard Zemel.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 13,
      "context" : "[23] M Forina et al.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 14,
      "context" : "[24] Jerome H Friedman.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 15,
      "context" : "[25] H Altay Güvenir, Gülşen Demiröz, and Nilsel Ilter.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 16,
      "context" : "[26] Moritz Hardt, Eric Price, Nati Srebro, et al.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 17,
      "context" : "[27] Jongbin Jung, Connor Concannon, Ravi Shroff, Sharad Goel, and Daniel G Goldstein.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 18,
      "context" : "[28] Jon Kleinberg, Himabindu Lakkaraju, Jure Leskovec, Jens Ludwig, and Sendhil Mullainathan.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 19,
      "context" : "[29] P.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 20,
      "context" : "[30] Igor Kononenko.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 21,
      "context" : "[31] Benjamin Letham, Cynthia Rudin, Tyler H McCormick, David Madigan, et al.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 22,
      "context" : "[32] Scott Lundberg and Su-In Lee.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 23,
      "context" : "[33] Andrew William Moore.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 24,
      "context" : "[35] Dino Pedreshi, Salvatore Ruggieri, and Franco Turini.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 25,
      "context" : "[36] J Ross Quinlan.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 26,
      "context" : "[37] Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 27,
      "context" : "[38] Cynthia Rudin.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 28,
      "context" : "[39] Avanti Shrikumar, Peyton Greenside, Anna Shcherbina, and Anshul Kundaje.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 29,
      "context" : "[40] Thomas A Stamey, John N Kabalin, John E McNeal, Iain M Johnstone, Fuad Freiha, Elise A Redwine, and Norman Yang.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 30,
      "context" : "[41] Richard S Sutton and Andrew G Barto.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 31,
      "context" : "[42] Selim Temizer, Mykel Kochenderfer, Leslie Kaelbling, Tomas Lozano-Pérez, and James Kuchar.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 32,
      "context" : "[43] Berk Ustun and Cynthia Rudin.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 33,
      "context" : "[44] Gilmer Valdes, José Marcio Luna, Eric Eaton, Charles B Simone, et al.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 34,
      "context" : "[45] Anneleen Van Assche and Hendrik Blockeel.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 35,
      "context" : "[46] Gilles Vandewiele, Olivier Janssens, Femke Ongenae, Filip De Turck, and Sofie Van Hoecke.",
      "startOffset" : 0,
      "endOffset" : 4
    } ],
    "year" : 2017,
    "abstractText" : "Interpretability has become an important issue as machine learning is increasingly used to inform consequential decisions. We propose an approach for interpreting a blackbox model by extracting a decision tree that approximates the model. Our model extraction algorithm avoids overfitting by leveraging blackbox model access to actively sample new training points. We prove that as the number of samples goes to infinity, the decision tree learned using our algorithm converges to the exact greedy decision tree. In our evaluation, we use our algorithm to interpret random forests and neural nets trained on several datasets from the UCI Machine Learning Repository, as well as control policies learned for three classical reinforcement learning problems. We show that our algorithm improves over a baseline based on CART on every problem instance. Furthermore, we show how an interpretation generated by our approach can be used to understand and debug these models.",
    "creator" : "LaTeX with hyperref package"
  }
}