{
  "name" : "1509.01817.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "On collapsed representation of hierarchical Completely Random Measures",
    "authors" : [ "Gaurav Pandey", "Ambedkar Dukkipati" ],
    "emails" : [ "GP88@CSA.IISC.ERNET.IN", "AD@CSA.IISC.ERNET.IN" ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "Mixed membership modelling is the problem of assigning an object to multiple latent classes/features simultaneously. Depending upon the problem, one can allow a single latent feature to be exhibited single or multiple times by the object. For instance, a document may comprise several topics, with each topic occurring in the document with variable multiplicity. The corresponding problem of mapping the words of a document to topics, is referred to as topic modelling.\nWhile parametric solutions to mixed membership mod-\nProceedings of the 33 rd International Conference on Machine Learning, New York, NY, USA, 2016. JMLR: W&CP volume 48. Copyright 2016 by the author(s).\nelling have been available in literature since more than a decade (Landauer & Dumais, 1997; Hofmann, 1999; Blei et al., 2001), the first non-parametric approach, that allowed the number of latent classes to be determined as well, was the hierarchical Dirichlet process (HDP) (Teh et al., 2006). Both the approaches model the object as a set of repeated draws from an object-specific distribution, whereby the object specific distribution is itself sampled from a common distribution. On the other hand, recent approaches such as hierarchical beta-negative binomial process (Zhou et al., 2012; Broderick et al., 2015) and hierarchical gamma-Poisson process (Titsias, 2008; Zhou & Carin, 2015) model the object as a point process, sampled from an object specific random measure, which is itself sampled from a common random measure. In some sense, these approaches are more natural for mixed membership modelling, since they model the object as a single entity rather than as a sequence of draws from a distribution.\nA straightforward implementation of any of the above nonparametric models would require sampling the atoms in the non-parametric distribution for the base as well as objectspecific measure. However, since the number of atoms in these distributions are often infinite, a truncation step is required to ensure tractability. Alternatively, for the HDP, a Chinese restaurant franchise scheme (Teh et al., 2006) can be used for collapsed inference in the model (that is, without explicitly instantiating the atoms). Fully collapsed inference scheme has also been proposed for beta-negative binomial process (BNBP) (Heaukulani & Roy, 2013; Zhou, 2014) and Gamma-Gamma-Poisson process (Zhou et al., 2015). Of particular relevance is the work by Roy (2014), whereby a Chinese restaurant fanchise scheme has been proposed for hierarchies of beta proceses (and its generalizations), when coupled with Bernoulli process.\nIn this paper, it is our aim to extend fully collapsed sampling so as to allow any completely random measure (CRM) for the choice of base and object-specific measure. As proposed in Roy (2014) for hierarchies of generalized beta processes, we propose Chinese restaurant franchise schemes for hierarchies of CRMs, when coupled with Pois-\nar X\niv :1\n50 9.\n01 81\n7v 2\n[ m\nat h.\nST ]\n2 J\nun 2\n01 6\nson process. We hope that this will encourage the use of hierarchical random measures, other than HDP and BNBP, for mixed-membership modelling and will lead to further research into an understanding of the applicability of the various random measures. To give an idea about the flexibility that can be obtained by using other measures, we propose the sum of generalized gamma process (SGGP), which allows one to determine the power term in the powerlaw distribution of topics with documents, by defining a prior on the parameters of SGGP. Alternatively, one can also define a prior directly on the discount parameter.\nThe main contributions in this paper are as follows:\n• We derive marginal distributions of Poisson process, when coupled with CRMs,\n• We provide an exact approach for generating a Poisson process sampled from a hierarchical CRM, without having to instantiate the infinitely many atoms of the random measure.\n• We provide a Gibbs sampling approach for sampling a Poisson process from a hierarchical CRM.\n• In the experiments section, we propose the sum of generalized gamma process (SGGP), and show its applicability for topic-modelling. By defining a prior on the parameters of SGGP, one can determine the power-law distribution of the topics and words in a Bayesian fashion."
    }, {
      "heading" : "2. Preliminaries and background",
      "text" : "In this section, we fix the notation and recall a few well known results from the theory of point processes."
    }, {
      "heading" : "2.1. Poisson process",
      "text" : "Let (S,S) be a measurable space and Π be a random countable collection of points on S. LetN(A) = |Π∩A|, for any measurable set A. N is also known as the counting process of Π. Π is called a Poisson process if N(A) is independent of N(B), whenever A and B are disjoint measurable sets, and N(A) is Poisson distributed with mean µ(A) for a fixed σ-finite measure µ. In sequel, we refer to both the random collection Π and its counting processN as Poisson process.\nLet (T, T ) be another measurable space and f : S → T be a measurable function. If the push forward measure of µ via f , that is, µ ◦ f−1 is non-atomic, then f(Π) = {f(x) : x ∈ Π} is also a Poisson process with mean measure µ ◦ f−1. This is also known as the mapping proposition for Poisson processes (Kingman, 1992). Moreover, if Π1,Π2, . . . is a countable collection of independent Poisson processes with mean measures µ1, µ2, . . . respectively,\nthen the union Π = ∪∞i=1Πi is also a Poisson process with mean measure µ = ∑∞ i=1 µi. This is known as the superposition proposition. Equivalently, if Ni is the counting process of Πi, then N = ∑∞ i=1Ni is the counting process\nof a Poisson process with mean measure µ = ∑∞ i=1 µi.\nFinally, let g be a measurable function from S to R, and Σ = ∑ x∈Π g(x). By Campbell’s proposition (Kingman, 1992), Σ is absolutely convergent with probability, if and only if ∫\nS\nmin(|g(x)|, 1)µ(dx ) <∞. (1)\nIf this condition holds, then for any t > 0, E[e−tΣ] = exp { − ∫ S (1− e−tg(x))µ(dx ) } . (2)"
    }, {
      "heading" : "2.2. Completely random measures",
      "text" : "Let (Ω,F ,P) be some probability space. Let (M(S),B) be the space of all σ-finite measures on (S,S) supplied with an appropriate σ-algebra. A completely random measure (CRM) Λ on (S,S), is a measurable mapping from Ω to M(S) such that\n1. P{Λ(∅) = 0} = 1,\n2. For any disjoint countable collection of sets A1, A2, . . . , the random variables Λ(Ai), i = 1, 2, . . . are independent, and Λ(∪Ai) = ∑ i Λ(Ai), holds\nalmost surely. (the independent increments property)\nAn important characterization of CRMs in terms of Poisson processes is as follows (Kingman, 1967). For any CRM Λ on (S,S) without any fixed atoms or deterministic component, there exists a Poisson process N on (R+ × S,BR+ ⊗ S), such that Λ(dx ) = ∫ R+ zN(dz ,dx ). Using Campbell’s proposition, the Laplace transform of Λ(A) for a measurable set A, is given by the following formula:\nE[e−tΛ(A)] = exp ( − ∫ R+×A (1− e−tz)ν(dz,dx) ) , t ≥ 0, (3) where ν denotes the mean measure of the underlying Poisson processN . ν is also referred to as the Poisson intensity measure of Λ. If ν(dz,dx) = ρ(dz)µ(dx), for a σ-finite measure µ on S, and a σ-finite measure ρ on R+ that satisfies ∫ R+(1 − e\n−tz)ρ(dz ) < ∞, then Λ(.) is known as homogenous CRM. In sequel, we assume µ(.) to be finite. Moreover, unless specified, whenever we refer to CRM, it means a homogeneous completely random measure without any fixed atoms or deterministic component.\nLet N be the Poisson process of the CRM Λ, that is, Λ(dx ) = ∫ R+ sN(dz ,dx ). If Π is the random collection of points corresponding to N , then Λ can equivalently\nbe written as Λ = ∑\n(z,x)∈Π zδx. {z : (z, x) ∈ Π} constitute the weights of the CRM Λ. By the mapping proposition for Poisson processes, they form a Poisson process with mean measure µ∗(dz ) = µ ◦ f−1(dz ), where f(x, y) = x is the projection map on R+. Hence, the weights of Λ form a Poisson process on R+ with mean measure µ∗(dz ) = ν(dz , S) = ρ(dz )µ(S). We formally state this result below.\nLemma 2.1. The weights of a homogenous CRM with no atoms or deterministic component, whose Poisson intensity measure ν(dz ,dx ) = ρ(dz )µ(dx ) form a Poisson process with mean measure ρ(dz )µ(S).\nNote 1: A completely random measure without any fixed atoms or deterministic component is a purely-atomic random measure.\nNote 2: Every such homogeneous CRM Λ on (S,S) has an underlying Poisson process N on (R+ × S,BR+ ⊗ S), such that\nΛ(dx ) = ∫ R+ zN(dz ,dx ) (4)\nalmost surely."
    }, {
      "heading" : "3. The proposed model",
      "text" : "Let X1, . . . , Xn be n observed samples, for instance, n documents. We assume that each sample Xi is generated as follows:\n• The base measure Φ is CRM(ρ, µ), where ρ and µ are σ-finite and finite (non-atomic) measures on (S,S) respectively.\n• Object specific measures Λi, 1 ≤ i ≤ n are CRM(ρ̄,Φ), where ρ̄ is another σ-finite non-atomic measure on (S,S).\n• The latent feature set Ni for each object Xi is a Poisson process with mean measure Λi.\n• Finally, the visible features Xi are sampled from Ni.\nNote: For topic modelling, S corresponds to the space of all probability measures on the words in the dictionary, also known as topics. Hence, when we sample Φ, we sample a subset of topics, along with the weights for those topics. This follows from the discreteness of Φ. Sampling objectspecific random measures Λi corresponds to sampling the document specific weights for all the topics in Φ. Sampling the latent featuresNi then corresponds to selecting a subset of topics from Λi based on the corresponding documentspecific weights. Since, all the Λ′is have access to the same set of topics, this leads to sharing of topics among Nis. Finally, the words inXi is sampled from the corresponding topic in Ni using categorical distribution.\nOur aim is to infer the latent features Ni, 1 ≤ i ≤ n from Xi, 1 ≤ i ≤ n. By Bayes’ rule\nP (N1, . . . , Nn|X1, . . . , Xn) ∝ P (X1, . . . , Xn|N1, . . . , Nn)P (N1, . . . , Nn) = Πni=1P (Xi|Ni)P (N1, . . . , Nn)\nThe conditional distribution of Xi given Ni are often very simple to compute, for instance, in the case of topic modelling, it is simply the product of categorical distributions. Hence, all we need to compute is the prior distribution of the latent features N1, . . . , Nn. This can be obtained by marginalizing out the base and object-specific random measures Φ and Λi, 1 ≤ i ≤ n. This is what we wish to achieve in the next few sections.\nWe will address the problem of marginalizing out the base and object-specific random measures in two steps. Firstly, in section 3.1, we will derive results for the case when the base measure is held fixed and the object-specific random measure is marginalized out. Next, in section 3.2, we will derive results for the case, when the base random measure Φ is also marginalized out. All the proofs are provided in the appendix."
    }, {
      "heading" : "3.1. Marginalizing out the object specific measure",
      "text" : "Let φ be a realization of the base random measure Φ. Let Λi, 1 ≤ i ≤ n, be independent CRM(ρ̄, φ). It is straightforward to see that if φ is a finite measure Λis will almostsurely be finite. Because of the independence among Λis, we can focus on marginalizing out a single object-specific random measure, say Λ. Although, in our original formulation, only 1 object is sampled from its object-specific random measure, we will present results for the case when n objects, N1, . . . , Nn are sampled from the object specific random measure. This extended result will be needed in the next section when marginalizing the base measure.\nThere are several ways to instantiate the random measure Λ. For instance, one can use the fact that since the underlying base measure φ is purely-atomic, the support of CRM(ρ̄, φ) will be restricted to only those measures whose support is a subset of the support of φ. In particular, if φ = ∑∞ j=1 βjδxj , then Λ will be of the form ∑∞ j=1 Ljδxj , where Lj are independent random variables. The independence of Ljs follows from the complete randomness of the measure.\nHowever, we found that this approach doesn’t lead us far. Hence, we derive the marginal distribution of the Poisson processes N1, . . . , Nn in proposition 3.1 and 3.2, by first assuming φ to be a continuous measure and then generalizing it to the case where φ is any finite measure.\nIn the sequel, ψ(t) = ∫ R+(1− e\n−tz)ρ̄(dz ), and ψ(k) is the kth derivative of ψ.\nProposition 3.1. Let Λ be a CRM on (S,S) with Poisson intensity measure ρ(dz )µ(dx ), where both µ(.) and ρ(.) are non-atomic. Let N1, . . . , Nn be n independent Poisson process with random mean measure Λ, and M be the distinct points of Ni, 1 ≤ i ≤ n. Then, M is a Poisson process with mean measure E[M(dx )] = µ(dx ) ∫ R+(1 − e−nz)ρ(dz ).\nThe above proposition provides the distribution of distinct points of the n point processes, N1, . . . , Nn. In order to complete the description of the distribution of N1, . . . , Nn, we also need to specify the joint distribution of the counts of each distinct feature in each Ni. This distribution is referred to as CRM-Poisson distribution in the rest of the paper. Let M(S) = k and mij be the count of the jth distinct feature in the ith object. Furthermore, let [m·j ] be the count of the jth distinct feature for each object and [mij ]1≤i≤n,1≤j≤k be the set of count vectors for the each latent feature.\nProposition 3.2. The joint distribution of the set of count vectors for the each latent feature [mij ](n,k) is given by\nP ([mij ](n,k)) = (−1) m··−k θke−θψ(n)∏n i=1(mi·)! k∏ j=1 ψ (m·j)(n) , (5) where mi· = ∑k j=1mij , m·j = ∑n i=1mij , m·· =∑n\ni=1 ∑k j=1mij , θ = µ(S) and ψ(t) = ∫ R+(1 − e−tz)ρ(dz ) is the Laplace exponent of Λ, and ψ(l)(t) is the lth derivative of ψ(t). This distribution will be referred to as CRM-Poisson(µ(S), ρ, n).\nCorollary 3.3. Conditioned on M(S) = k, the set of count vectors for the each latent feature [mij ](n,k) is distributed as\nP ([mij ](n,k)|M(S) = k) (6)\n= θk(−1)m··−kk!∏n i=1(mi·)! k∏ j=1 ψ (mi·)(n) ψ(n)\nNote that both ψ(k) and ψ contain a multiple involving µ(S), which cancels out when they are divided in (6). Hence, conditioned on the number of points in the Poisson process M , the distribution of the set of counts for each latent feature [mij ](n,k) does not depend on the measure µ. In sequel, this distribution will be referred to as conditional CRM-Poisson(ρ, n, k) or CCRM-Poisson(ρ, n, k).\nExample 1: The Gamma-Poisson process The Poisson-intensity measure of gamma process is given by ρ(dz ) = e−zz−1 dz . The corresponding Laplace exponent is ψ(t) = ln(1 + t). Replacing it in equation (5), we\nget\nP ([mij ](n,k)) = θk ∏k j=1 Γ(m·j)∏n\ni=1mi·!(1 + n)m··+θ (7)\nNext, we generalize these results for the case when φ is an atomic measure.\nProposition 3.4. Let Λ be a completely random measure with Poisson intensity measure ν(dz ,dx ) = φ(dx )ρ̄(dz ), where ρ̄ is non-atomic. Let N be a Poisson process with mean measure Λ. Then, N can be obtained by sampling a Poisson process with mean measure φ(dx )ψ(1), say M , and then sampling the count of each feature inM using the conditional CRM-Poisson distribution.\nNote: The points in M won’t be distinct anymore, since the underlying mean measure is non-atomic."
    }, {
      "heading" : "3.2. Marginalizing out the base measure",
      "text" : "The previous section derived the marginal distribution of the Poisson processes, for a fixed realization φ of the base random measure Φ. In this section, we want to marginalize the CRM Φ as well. Marginalizing Φ does away with the independence among the latent features Nis, hence, we need to model the joint distribution of N1, . . . , Nn.\nThe model under study is\nΦ ∼ CRM(ρ, µ) , Λi|Φ ∼ CRM(ρ′,Φ), 1 ≤ i ≤ n , Ni|Λi ∼ Poisson Process(Λi), 1 ≤ i ≤ n .\n(8)\nWe use Proposition 3.4 to marginalize out Λi from the above description. Thus Ni can equivalently be obtained by sampling a Poisson processes with mean measure Φ(dx ) ∫ R+(1− e\n−z)ρ(dz ), and then sampling the count of each feature in Mi for each point process Ni using Corollary 3.3. In particular, let Mi be the corresponding Poisson process, and mij be the count of the jth feature in Mi for the point process Ni and ri· = Mi(S). The reason for the symbol ri· will become clear, when we have a picture of the entire generative model. Let [mij ]·,ri· be the set of counts of the latent features for the ith individual. The distribution of the set of counts [mij ]·,ri· conditioned on Mi(S) does not depend on Φ. Hence, an alternative description of the Ni via Mi and mij , 1 ≤ j ≤ ri· is as\nfollows: Mi|Φ ∼ Poisson Process ( Φ(.) ∫ R+ (1− e−z)ρ̄(dz ) ) ,\n[mij ](·,ri·)|{Mi(S) = ri·} ∼ CCRM-Poisson(ρ̄, 1, ri·)\nNi = ri·∑ i=1 mijδMij ,\n(9)\nwhere Mij are the points in the point process Mi.\nMi, 1 ≤ i ≤ n are independent Poisson processes, whose mean measure is a scaled CRM, and hence, also a CRM. Hence, we are again in the domain of CRM-Poisson models. Let ψ̄(1) = ∫ R+(1 − e\n−z)ρ̄(dz ). If we define Φ′(dx ) = ψ̄(1)Φ(dx ), then\nE[e−tΦ ′(A)] = E[e−tψ̄(1)Φ(A)]\n= exp { −µ(A) ∫ R+ (1− e−tψ̄(1)z)ρ(dz) }\n= exp { −µ(A) ∫ R+ (1− e−tz ′ )ρ(d(z′/ψ̄(1))) } Hence, the Poisson intensity measure of the scaled CRM Φ′ is given by ρ(d(z/ψ̄(1)))µ(dx ). Applying Proposition 3.4 to marginalize out Φ, we get that Mi’s can be obtained by sampling a Poisson process R with mean measure\nE[R(dx )] = µ(dx ) ∫ R+ (1− e−nz ′ )ρ(d(z′/ψ̄(1)))\n= µ(dx ) ∫ R+ (1− e−ψ̄(1)nz)ρ(dz) .\nThe count of each feature in R for each point process Mi can then be obtained by using Corollary 3.3. In particular, let rik be the count of the kth point in R for the point process Mi and p = R(S).\nA complete generative model for generating the point processes Ni, 1 ≤ i ≤ n is as follows:\nR ∼ Poisson Process ( µ(.) ∫ R+ (1− e−ψ̄(1)nz)ρ(dz ) ) , [rik](n,p)| {R(S) = p} ∼ CCRM-Poisson(ρ, ψ̄(1)n, p) (10)\nMi = p∑ k=1 rikδRk\n[mij ](·,ri·)|{Mi(S) = ri·} ∼ CCRM-Poisson(ρ̄, 1, ri·)\nNi = ri·∑ j=1 mijδMij ,\nSince R is again a Poisson process, it is straightforward to extend this hierarchy further by sampling µ(.) again from a CRM."
    }, {
      "heading" : "4. Implementation via Gibbs sampling",
      "text" : "Section 3 provided an approach for sampling a Poisson process, when sampled from a hierarchical CRM, without having to instantiate the infinitely many atoms of the base or object-specific CRM. However, it is not clear how the above derivations can be used for determining the latent features N1, . . . , Nn for the objects X1, . . . , Xn, which is the aim of this work.\nIn this section, we provide a Gibbs sampling approach for sampling the latent features from its prior distribution that is P (N1, . . . , Nn). In order to sample from the posterior, one simply needs to multiply the equations in this section with the likelihood of the latent feature. In order to be able to perform MCMC sampling in hierarchical CRM-Poisson models, we need to marginalize out R(S) and Mi(S) from distributions of [rik](n,p) and [mij ](·,ri·) respectively. By marginalizing out the Poisson distributed random variable R(S) from (10), we get that\n[rik](n,p) ∼ CRM-Poisson(µ(S), ρ, ψ̄(1)n) .\nThe marginal distribution of the set of counts of each latent feature for the ith individual [mij ](·,ri·) (where ri· is also random) is given by the following lemma.\nLemma 4.1. Let h(u) = E[e−uψ(S)] = exp { −µ(S) ∫ R+ (1− e−uz)ρ(dz ) } .\nFurthermore, if we let\nψ(u) = ∫ R+ (1− e−uz)ρ(dz )\nψ̄(u) = ∫ R+ (1− e−uz)ρ̄(dz ) ,\nthen, [mij ](·,ri·) is marginally distributed as\nP ([mij ](·,ri·)) = (−1) mi·h(ri·) (ψ̄(1)) ∏ri·j=1 ψ̄(mij)(1)\nmi·! ,\n(11)\nIn the case of topic-modelling, the number of latent features, #Ni is equal to the number of observed features #Xi. Hence, let Xil be the lth observed feature associated with the ith object and Nil be the corresponding latent feature. Here, we discuss the MCMC approach for sampling from the prior distribution of Nil, 1 ≤ l ≤ mi·. As discussed in (Neal, 2000), it is more efficient to sample the index of the latent feature, rather than the latent feature itself. Hence, let Til be the index of the point in Mi associated with Nil, and Dij be the index of the point in R associated with Mij . In an analny with the Chinese restaurant franchise model (Teh et al., 2006), one can think of Til\nto be the index of the table assigned to the lth customer in the ith restaurant, and Dij to be the index of the dish associated with the jth table in ith restaurant. Moreover, mij refers to the number of customers sitting on the jth table in ith restaurant, and rik refers to the number of tables in the ith restaurant with the kth dish. Hence ri· = ∑p k=1 rik is the number of tables in the ith restaurant.\nThe distribution of the number of customers per table in the ith restaurant, [mij ](·,ri·) follows from Lemma 4.1. Hence, in order to sample the table of lth customer, Til, given the indices of the tables of all the other customers in ith restaurant, we treat it as the table corresponding to the last customer of the ith restaurant. Let m−(il)i′j be the number of customers sitting on the jth table in the i′th restaurant, excluding the lth customer. The probability that the lth customer in the ith restaurant occupies the jth table is proportional to P (m−(il)ij′ + 1j′=j , 1 ≤ j′ ≤ ri·) as given in (11). We divide the expression by P (m−(il)ij′ , 1 ≤ j′ ≤ ri·) to get a simpler form for the unnormalized probability distribution. Hence, the probability of assigning an existing table with index j is given by\nP (Til = j|T−(il)) ∝ − ψ̄(m\n−(il) ij +1)(1)\nψ̄(m −(il) ij )(1)\n, (12)\nand the probability of sampling a new table for the customer is given by\nP (Til = ri· + 1|T−(il)) ∝= − h (ri·+1)(ψ̄(1)) h (ri·)(ψ̄(1)) ψ̄ (1)(1) ,\n(13) where ψ̄(t) = ∫ R+(1 − e −tz)ρ̄(dz ) and h(k) is the kth derivative of h.\nMoreover, whenever a new table is sampled for a customer, a dish is sampled for the table from the distribution on tables per dish. By the discussion in the beginning of this section, the number of tables per dish [rik](n,p) follow a CRM-Poisson(µ(S), ρ, ψ̄(1)n) distribution. Hence, in order to sample the dish at jth table, Dij , given the indices of the dishes at all the other tables, we treat it as the dish corresponding to the last table of the last restaurant. Let r −(ij) ·k be the total number of tables served with the kth dish, excluding the jth table of ith restaurant. The probability that the kth dish is served at the jth table in the ith restaurant is proportional to P (r−(ij)i′k′ + 1i′=i′,k′=k, 1 ≤ i′ ≤ n, 1 ≤ k′ ≤ p) as given in (11). We divide the expression by P (r−(ij)i′k′ , 1 ≤ i′ ≤ n, 1 ≤ k′ ≤ p) to get a simpler form for the unnormalized probability distribution. Hence, the probability of serving an existing dish with index k is\ngiven by\nP (Dij = k|D−(ij)) ∝ − ψ\n(r −(ij)·k +1)(ψ̄(1)n) ψ (r −(ij)·k )(ψ̄(1)n) , (14)\nand the probability of sampling a new dish for the table is given by\nP (Dij = p+ 1|D−(ij)) ∝ θψ(1)(ψ̄(1)n) , (15) where ψ(t) = ∫ R+(1− e −tz)ρ(dz ) and θ = µ(S).\nHence, a complete description of one iteration of MCMC sampling, from the prior distribution, in hierarchical CRMPoisson models is as follows:\n1. For each customer in each restaurant, sample his table index conditioned on the indices of table of other customers, according to equations (12) and (13).\n2. If the table selected is a new table, sample the index of dish corresponding to that table from equations (14) and (15).\n3. Sample the index of dish for each table, conditioned on the indices of dishes at the other tables, according to equations (14) and (15).\nExample 2: The Gamma-Gamma-Poisson process We compute the dish and table sampling probabilities for the Gamma-Gamma-Poisson process using the above equations. The Poisson intensity measure for both the base and object specific measures Φ and Λi, 1 ≤ i ≤ n is z−1e−z dz . The corresponding Laplace exponent is given by ψ(t) = ψ̄(t) = ln(1 + t). Moreover, let the mean measure for the base measure Φ be µ(.) and µ(S) = θ. Then, h(u) = Ee−uΦ(S) = 1\n(1+u)θ . The corresponding deriva-\ntives are given by\nψ(k) = ψ̄(k)(t) = (−1)k−1Γ(k)\n(1 + t)k (16)\nh(k)(u) = (−1)kΓ(k + θ) (1 + u)k+θΓ(θ)\n(17)\nThe corresponding dish sampling probabilities are given by\nP (Dij = k|D−(ij)) ∝ r −(ij) ·k\n1 + n ln 2 (18)\nP (Dij = p+ 1|D−(ij)) ∝ θ\n1 + n ln 2 (19)\nfor an existing and new dish respectively. Normalizing these probabilities, we get\nP (Dij = k|D−(ij)) = r −(ij) ·k∑p\nk=1 r·k + θ (20)\nP (Dij = p+ 1|D−(ij)) = θ∑p\nk=1 r·k + θ (21)\nThe table sampling probabilities are given by\nP (Til = j|T−(il)) ∝ m −(il) ij\n1 + ln(2) (22)\nP (Til = ri· + 1|T−(il)) ∝ θ + ri·\n(1 + ln(2))2 (23)\nfor an existing and new table respectively. Normalizing these probabilities, we get\nP (Til = j|T−(il)) = m −(il) ij∑ri·\nj=1m −(il) ij + θ+ri· 1+ln(2)\n(24)\nP (Til = ri· + 1|T−(il)) = (θ + ri·)/(1 + ln(2))∑ri· j=1m −(il) ij + θ+ri· 1+ln(2)\n(25)\nExample 3: The Gamma-Generalized Gamma-Poisson process In this scenario, the base random measure has ρ(dz ) = e−zz−1 dz , whereas the object specific measure has ρ̄(dz ) = e−zz−d−1 dz , where 0 < d < 1 is known as the discount parameter. The corresponding Laplace exponents are given by ψ(t) = ln(1 + t) and ψ̄(t) = (1+t)\nd−1 d\nrespectively. The derivative of ψ̄ is given by\nψ̄(k)(t) = (−1)k−1Γ(k − d)\n(1 + t)k−dΓ(1− d) (26)\n(27)\nOther derivatives remain same as in the previous example. Moreover, the dish sampling probabilities remain same. The table sampling probabilities are given by\nP (Til = j|T−(il)) = m −(il) ij − d∑ri·\nj=1(m −(il) ij − d) + θ+ri· 1+lnd(2)\n(28)\nP (Til = ri· + 1|T−(il)) = (θ + ri·)/(1 + lnd(2))∑ri· j=1(m −(il) ij − d) + θ+ri· 1+lnd(2)\n(29)\nwhere lnd(2) = 2 d−1 d ."
    }, {
      "heading" : "5. Experimental results",
      "text" : "We use hierarchical CRM-Poisson models for learning topics from the NIPS corpus 1.\n1The dataset can be downloaded from http: //psiexp.ss.uci.edu/research/programs_data/ toolbox.htm"
    }, {
      "heading" : "5.1. Evaluation",
      "text" : "For evaluating the different models, we divide each document into a training section and a test section by independently sampling a boolean random variable for each word. The probability of sending the word to the training section is varied from 0.3 to 0.7. We run 2000 iterations of Gibbs sampling. The first 500 iterations are discarded, and every sample in every 5 iterations afterwards is used to update the document-specific distribution on topics and the topic specific distribution on words. In particular, letW be the number of words, K be the number of topics, (βdk)1≤k≤K be the document specific distribution on topics for the document d, and (τkw)1≤w≤W be the topic specific distribution on words for the kth topic. Then, the probability of observing a word w in document d is given by ∑K k=1 βdkτkw. For the evaluation metric, we use perplexity, which is simply the inverse of the geometric mean of the probability of all the words in the test set."
    }, {
      "heading" : "5.2. Varying the Common CRM",
      "text" : "In our experiments, we fix the object specific random measure Λi in (8) to be the gamma process, with ρ̄(dz ) = e−zz−1 dz . For the base CRM Φ, we consider two specific choices of random measures.\n• Generalized gamma process (GGP): The Poisson intensity measure of Φ is given by ν(dz ,dx ) = ρ(dz )µ(dx ), where ρ(dz ) =\nθ Γ(1−d)e −zz−d−1 dz , 0 ≤ d < 1, θ > 0 and µ(S) = 1. The corresponding Laplace exponent is given by θ((1 + t)d − 1)/d.\n• Sum of Generalized gamma processes (SGGP): The Poisson intensity measure of the CRM is given by ν(dz ,dx ) = ρ(dz )µ(dx ), where\nρ(dz ) = m∑ q=1 θq Γ(1− dq) e−zz−dq−1 dz (30)\nand µ(S) = 1. The corresponding Laplace exponent is given by\nψ(t) = ( m∑ q=1 θq (1 + t)dq − 1 dq ) . (31)\nFor the case of GGP, the value of the discount parameter d is chosen from the set {0, .1, .2, .3, .4}. Furthermore, a gamma prior with rate parameter 2 and shape parameter 4 is defined on θ.\nNote: The generalized gamma process with discount parameter 0 corresponds to the Gamma process. Using a gamma process prior for the base and object-specific CRM\ncorresponds exactly to the hierarchical Dirichlet process with a gamma prior on the concentration parameter of the object specific Dirichlet process. We did not add comparison results with HDP separately, because the same perplexity is obtained in both the models.\nFor the case of SGGP, we consider m = 5, and d1 = 0, d2 = .1 . . . , d5 = .4. Furthermore, independent gamma priors with rate parameter 2 and shape parameter 4 are defined for each θq, 1 ≤ q ≤ 5. The posterior of each parameter θq is sampled via uniform sampling. We use equations (12)-(15) to compute the dish sampling and table sampling probabilities. The probability of sampling an existing dish is given by\nP (Dij = k|D−(ij))\n∝ ∑m q=1 θq Γ(r −(ij)·k +1−dq) Γ(1−dq) (1 + ψ̄(1)n) dq\n∑m q=1 θq Γ(r −(ij)·k −dq) Γ(1−dq) (1 + ψ̄(1)n) dq ,\nwhere ψ̄(1) = ∫ R+(1− e\n−z)ρ̄(dz ) = ln(2). Similarly, the probability of a new dish is given by\nP (Dij = p+ 1|D−(ij)) ∝ m∑ q=1 θq(1 + ψ̄(1)n) dq .\nThe table-sampling probabilities can be computed similarly. We approximated the Laplace transform of Φ(S) (h in (13)), by a weighted sum of exponential functions to simplify the computation of its derivatives. The perplexity for the hierarchical CRM-Poisson models as a function of training percentage is plotted in Figure 1. Note that Figure 1 doesn’t necessarily imply that SGGM-based models will always outperform GGM based models as the results have been obtained by defining a specific gamma prior for each hyperparameter, as mentioned above."
    }, {
      "heading" : "6. Conclusion",
      "text" : "For years, hierarchical Dirichlet processes have been the standard tool for nonparametric topic modelling, since collapsed inference in HDP can be performed using the Chinese restaurant franchise scheme. In this paper, our aim was to show that collapsed Gibbs sampling can be extended to a much larger set of hierarchical random measures using the same Chinese restaurant franchise scheme, thereby opening doors for further research into the efficacy of various hierarchical priors. We hope that this will encourage a better understanding of applicability of various hierarchical CRM priors. Furthermore, the results of the paper can be used to prove results for hierarchical CRMs in other contexts, for instance, nonparametric hidden Markov models."
    }, {
      "heading" : "Acknowledgement",
      "text" : "Gaurav Pandey is supported by IBM PhD Fellowship for the academic year 2015-2016."
    }, {
      "heading" : "Appendix",
      "text" : ""
    }, {
      "heading" : "Proof of proposition 3.1",
      "text" : "Proof. Let N = ∑n i=1Ni. Since, conditioned on Λ, N1, . . . , Nn are independent Poisson process with mean measure Λ, by the superposition proposition for Poisson processes, N is a Poisson process conditioned on Λ with mean measure nΛ. Since, E[et1N(A)+t2N(B)|Λ] = E[et1N(A)|Λ(A)]E[et2N(B)|Λ(B)], and Λ(A) and Λ(B) are independent, hence, N(A) and N(B) are also independent and therefore, N is a CRM. Hence, N(dx ) =∫ R+ sN̄(dz ,dx ), for a Poisson process N̄ on S × R\n+. Moreover,\nE[e−tN(A)] = E [ E[e−tN(A)|Λ] ] = E [ exp ( −nΛ(A)(1− e−t)\n)] = exp ( −µ(A)\n∫ R+ (1− e−n(1−e −t)z)ρ(dz ) ) = exp ( −µ(A)\n∫ R+ ( ∞∑ k=0 e−nz(nz)k k!\n− ∞∑ k=0 e−nze−kt(nz)k k!\n) ρ(dz ) )\nwhere, we have used the fact that 1 = ∑∞ k=0 e−nz(nz)k\nk! . Rearranging the terms in the above equation, we get\nE[e−tN(A)]\n= exp ( −µ(A)\n∞∑ k=1 (1− e−kt) ∫ R+ e−nz(nz)k k! ρ(dz ) ) .\nHence, the Poisson intensity measure of N , when viewed as a CRM is given by\nν̄(dk ,dx ) = µ(dx ) ∫ R+ e−nz(nz)k k! ρ(dz )\nwhen k ∈ {1, 2, 3, . . . }, and 0 otherwise. The distinct points of N can be obtained by projecting N on S. Hence, by the mapping proposition for Poisson processes (Kingman, 1992), the distinct points ofN form a Poisson process with mean measure µ∗(dx ) = ν̄(f−1(dx )), where f is the projection map on S. Hence f−1(dx ) = (R+,dx ), and\nµ∗(dx ) = ν̄(R+,dx )\n= µ(dx ) ∫ R+ ∞∑ k=1 e−nz(nz)k k! ρ(dz )\n= µ(dx ) ∫ R+ (1− e−nz)ρ(dz ) .\nThus, the result follows."
    }, {
      "heading" : "Proof of proposition 3.2",
      "text" : "Proof. The proof relies on the simple fact, that conditioned on the number of points to be sampled, the points of a Poisson process are independent (Kingman, 1992). Thus, n point processes can be sampled from a measure Λ, by first sampling the number of points in each point process from a Poisson distribution with mean Λ(S), and then sampling the points independently. Let Λ = ∑n i=1 ∆iδXi . Let (Xl1 , . . . , Xlk) be the features discovered by the n Poisson processes. Let the ith point process Ni consist of mi1 occurrences of Xl1 , mi2 occurrences of Xl2 and mik occurrences of Xlk . Then, the joint distribution of the n point processes conditioned on Λ is given by\nP(N1, . . . , Nn|Λ)\n= n∏ i=1 exp(−T )T ∑k j=1 mij ( ∑k j=1mij)! k∏ j=1 ( ∆lj T )mij ,\nwhere T = Λ(S) = ∑∞ i=1 ∆iδXi(S) = ∑∞ i=1 ∆i. Readjusting the outermost product in the above equation, we get,\nP(N1, . . . , Nn|Λ) = exp(−nT )∏n i=1( ∑k j=1mij)! k∏ j=1 ∆ ∑n i=1 mij lj .\nSince, we are not interested in the actual points Xli ’s, but only the number of occurrences of the different points in the point processes, that is, [mij ](n,k), we can sum over every k-tuple of distinct atoms in the random measure Λ. Hence,\nP ([mij ](n,k)|Λ)\n= exp(−nT )∏n i=1( ∑k j=1mij)! ∑ ∆l1 6=∆l2 6=···6=∆lk k∏ j=1 ∆ ∑n i=1 mij lj ,\nwhere the sum is over all subsets of length k of the set {∆1,∆2, . . . }. Finally, in order to compute the result, we need to take expectation with respect to the distribution of Λ. Towards that end, we note that only the weights of Λ appear in the above equation. From section 2.2, we know that the weights of a CRM with Poisson intensity measure ρ(dz )µ(dx )form a Poisson process with mean measure µ(S)ρ(dz ). Hence, it is enough to take the expectation with respect to the Poisson process.\nP ([mij ](n,k)) (32)\n= 1∏n i=1( ∑k j=1mij)! E\n[ exp (−nT ) (33)\n∑ ∆l1 6=∆l2 6=···6=∆lk k∏ j=1 ∆ ∑n i=1 mij lj\n , (34)\nThe expectation can further be simplified by applying Proposition 2.1 of (James, 2005).\nProposition 6.1 ((James, 2005)). Let N be the space of all σ−finite counting measures on R+, equipped with an appropriate σ-field. Let f : R+ → R+ and g : N → R+ be measurable with respect to their σ-fields. Then, for a Poisson processN with mean measure E[N(dx )] = ρ(dx ),\nE [ g(N)e− ∑ ∆∈N f(∆) ] = E [ e− ∑ ∆∈N f(∆) ] E[g(N̄)]\nwhere N̄ is a Poisson process with mean measure E[N(dx )] = e−f(x)ρ(dx ).\nApplying the above proposition to (32), we get P ([mij ](n,k)) = E [ e− ∑∞ i=1 n∆i ]∏n i=1( ∑k j=1mij)!\n× E  ∑ ∆l1 6=∆l2 6=···6=∆lk∈N̄ k∏ j=1 ∆ ∑n i=1 mij lj  where N̄ is a Poisson process with mean measure E[N(dz )] = e−nzρ(dz )θ. The first expectation can be evaluated using Campbell’s proposition and is given by exp ( −θ ∫ R+(1− e −nz)ρ(dz ) ) . In order to evaluate the second expectation, we construct a new point process from N∗ on R+k by concatenating every set of k distinct points in N̄ . The expression in the second expectation can then be rewritten as\n∑ (∆l1 ,...,∆lk )∈N ∗ k∏ j=1 ∆ ∑n i=1 mij lj\nBy Campbell’s proposition for point processes,\nE [∑ ∆∈N f(∆) ] = ∫ z∈R+ f(z)ρ(dz ) ,\nwhere ρ(dz ) = E[N(dz )]. Moreover, since the point process N∗ is obtained by concatenating distinct points in N , E[N∗(dz 1, . . . ,dzk)] = ∏k j=1 E[N̄(dz j)] =∏k\nj=1 θe −nzρ(dz j), whenever zj’s are distinct. Hence,\nE  ∑ ∆l1 6=∆l2 6=···6=∆lk∈N̄ k∏ j=1 ∆ ∑n i=1 mij lj  =\nk∏ j=1 ∫ z∈R+ θe−nzz ∑n i=1 mijρ(dz ) .\nHence, the final expression for the marginal distribution of the set of counts for each latent feature is given by\nP ([mij ](n,k)) = exp\n( −θ ∫ R+(1− e −nz)ρ(dz ) )∏n\ni=1( ∑k j=1mij)!\n× k∏ j=1 ∫ z∈R+ θe−nzz ∑n i=1 mijρ(dz )\nThe above expression can be simplified by letting ψ(t) = θ ∫ R+(1 − e\n−tz)ρ(dz ). Hence, ψ(l)(t) = (−1)l−1 ∫ R+ θe\n−tzzlρ(dz ). Hence, the above expression can be rewritten as P ([mij ](n,k)) =(−1) ∑n i=1 ∑k j=1 mij−k θ\nke−θψ(n)∏n i=1( ∑k j=1mij)!\n× k∏ j=1 ψ( ∑n i=1 mij)(n)"
    }, {
      "heading" : "Proof of Corollary 3.3",
      "text" : "Proof. From proposition 3.1, the distinct points in the point processes Ni, 1 ≤ i ≤ n, form a Poisson process with mean measure µ(dx)µ(S) ψ(n). Hence, the total number of distinct points k is distributed as Poisson(ψ(n)). Hence, conditioning equation (5) with respect to k, we get the desired result."
    }, {
      "heading" : "Proof of Proposition 3.4",
      "text" : "Proof. Let N = ∑n i=1Ni. From the arguments of proposition 3.1, N is a CRM, and hence, can be written as N(dx ) = ∫ R+ zN̄(dz ,dx ) for some Poisson process N̄ . Let Π be the random collection of points corrsponding to N̄ . Now define a map f : R+ × S → S as the projection map on S, that is, f(x, y) = y and M = f(Π) = {{f(x, y) : (x, y) ∈ Π}}, where the double brackets indicate that M is a multiset. The rest of the arguments remain same as in proposition 3.1 and proposition 3.2."
    }, {
      "heading" : "Proof of Lemma 4.1",
      "text" : "Proof. Using Proposition 3.4 to marginalize Λi from 8, we get that [mij ]1≤j≤ri· is distributed as CRMPoisson(Φ(S), ρ, 1), that is,\nP ([mij ]1≤j≤ri·|Φ(S))\n= exp\n( −Φ(S) ∫ R+(1− e −z)ρ̄(dz ) )\n( ∑ri· j=1mij)!\n× ri·∏ j=1 ∫ z∈R+ Φ(S)e−zzmij ρ̄(dz ) (35)\nLet mi· = ∑ri· j=1mij . Taking expectation with respect to Φ(S), we get the marginal distribution of [mij ]1≤j≤ri· , where ri· is also random. P ([mij ]1≤j≤ri·) = E [ exp ( −Φ(S)\n∫ R+ (1− e−z)ρ̄(dz ) ) Φ(S) ri· ]\n× ∏ri· j=1 ∫ z∈R+ e −zzmij ρ̄(dz )\nmi·! (36)\nIt is given that h(u) = E[e−uΦ(S)]\nψ̄(u) = ∫ R+ (1− e−uz)ρ̄(dz ) ,\nHence\ndri· duri· h(u) = (−1)\nri·E [ Φ(S) ri·e−uΦ(S) ]\nψ̄(mij)(u) = (−1)mij−1 ∫ R+ e−uzzmij ρ̄(dz )\nUsing the above results with u = ψ̄(1), equation (36) can be rewritten as\nP ([mij ]1≤j≤ri·) = (−1)mi·h(ri·)(ψ̄(1)) ∏ri· j=1 ψ̄ (mij)(1)\nmi·! (37)"
    } ],
    "references" : [ {
      "title" : "Latent Dirichlet Allocation",
      "author" : [ "Blei", "David M", "Ng", "Andrew Y", "Jordan", "Michael I" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Blei et al\\.,? \\Q2001\\E",
      "shortCiteRegEx" : "Blei et al\\.",
      "year" : 2001
    }, {
      "title" : "Combinatorial Clustering and the Beta Negative Binomial Process",
      "author" : [ "Broderick", "Tamara", "Mackey", "Lester", "Paisley", "John", "Jordan", "Michael I" ],
      "venue" : "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
      "citeRegEx" : "Broderick et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Broderick et al\\.",
      "year" : 2015
    }, {
      "title" : "The combinatorial structure of beta negative binomial processes",
      "author" : [ "Heaukulani", "Creighton", "Roy", "Daniel M" ],
      "venue" : "arXiv preprint arXiv:1401.0062,",
      "citeRegEx" : "Heaukulani et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Heaukulani et al\\.",
      "year" : 2013
    }, {
      "title" : "Probabilistic latent semantic analysis",
      "author" : [ "Hofmann", "Thomas" ],
      "venue" : "In Proceedings of the Fifteenth conference on Uncertainty in Artificial Intelligence,",
      "citeRegEx" : "Hofmann and Thomas.,? \\Q1999\\E",
      "shortCiteRegEx" : "Hofmann and Thomas.",
      "year" : 1999
    }, {
      "title" : "Bayesian Poisson process Partition Calculus with an application to Bayesian Lévy Moving Averages",
      "author" : [ "James", "Lancelot F" ],
      "venue" : "Annals of Statistics,",
      "citeRegEx" : "James and F.,? \\Q2005\\E",
      "shortCiteRegEx" : "James and F.",
      "year" : 2005
    }, {
      "title" : "Completely Random Measures",
      "author" : [ "Kingman", "John" ],
      "venue" : "Pacific Journal of Mathematics,",
      "citeRegEx" : "Kingman and John.,? \\Q1967\\E",
      "shortCiteRegEx" : "Kingman and John.",
      "year" : 1967
    }, {
      "title" : "A solution to Plato’s problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge",
      "author" : [ "Landauer", "Thomas K", "Dumais", "Susan T" ],
      "venue" : "Psychological review,",
      "citeRegEx" : "Landauer et al\\.,? \\Q1997\\E",
      "shortCiteRegEx" : "Landauer et al\\.",
      "year" : 1997
    }, {
      "title" : "Markov Chain Sampling Methods for Dirichlet Process Mixture Models",
      "author" : [ "Neal", "Radford M" ],
      "venue" : "Journal of Computational and Graphical Statistics,",
      "citeRegEx" : "Neal and M.,? \\Q2000\\E",
      "shortCiteRegEx" : "Neal and M.",
      "year" : 2000
    }, {
      "title" : "The continuum-of-urns scheme, generalized beta and indian buffet processes, and hierarchies thereof",
      "author" : [ "Roy", "Daniel M" ],
      "venue" : "arXiv preprint arXiv:1501.00208,",
      "citeRegEx" : "Roy and M.,? \\Q2014\\E",
      "shortCiteRegEx" : "Roy and M.",
      "year" : 2014
    }, {
      "title" : "Hierarchical Dirichlet Processes",
      "author" : [ "Teh", "Yee Whye", "Jordan", "Michael I", "Beal", "Matthew J", "Blei", "David M" ],
      "venue" : "Journal of the American Statistical Association,",
      "citeRegEx" : "Teh et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Teh et al\\.",
      "year" : 2006
    }, {
      "title" : "The Infinite Gamma-Poisson Feature Model",
      "author" : [ "Titsias", "Michalis K" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Titsias and K.,? \\Q2008\\E",
      "shortCiteRegEx" : "Titsias and K.",
      "year" : 2008
    }, {
      "title" : "Beta-negative binomial process and exchangeable random partitions for mixed-membership modeling",
      "author" : [ "Zhou", "Mingyuan" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Zhou and Mingyuan.,? \\Q2014\\E",
      "shortCiteRegEx" : "Zhou and Mingyuan.",
      "year" : 2014
    }, {
      "title" : "Negative Binomial Process Count and Mixture Modelling",
      "author" : [ "Zhou", "Mingyuan", "Carin", "Lawrence" ],
      "venue" : "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
      "citeRegEx" : "Zhou et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Zhou et al\\.",
      "year" : 2015
    }, {
      "title" : "Beta-Negative Binomial Process and Poisson Factor Analysis",
      "author" : [ "Zhou", "Mingyuan", "Hannah", "Lauren A", "Dunson", "David B", "Carin", "Lawrence" ],
      "venue" : "In AISTATS,",
      "citeRegEx" : "Zhou et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Zhou et al\\.",
      "year" : 2012
    }, {
      "title" : "Priors for random count matrices derived from a family of negative binomial processes",
      "author" : [ "Zhou", "Mingyuan", "Padilla", "Oscar Hernan Madrid", "Scott", "James G" ],
      "venue" : "Journal of the American Statistical Association,",
      "citeRegEx" : "Zhou et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Zhou et al\\.",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "elling have been available in literature since more than a decade (Landauer & Dumais, 1997; Hofmann, 1999; Blei et al., 2001), the first non-parametric approach, that allowed the number of latent classes to be determined as well, was the hierarchical Dirichlet process (HDP) (Teh et al.",
      "startOffset" : 66,
      "endOffset" : 125
    }, {
      "referenceID" : 9,
      "context" : ", 2001), the first non-parametric approach, that allowed the number of latent classes to be determined as well, was the hierarchical Dirichlet process (HDP) (Teh et al., 2006).",
      "startOffset" : 157,
      "endOffset" : 175
    }, {
      "referenceID" : 13,
      "context" : "On the other hand, recent approaches such as hierarchical beta-negative binomial process (Zhou et al., 2012; Broderick et al., 2015) and hierarchical gamma-Poisson process (Titsias, 2008; Zhou & Carin, 2015) model the object as a point process, sampled from an object specific random measure, which is itself sampled from a common random measure.",
      "startOffset" : 89,
      "endOffset" : 132
    }, {
      "referenceID" : 1,
      "context" : "On the other hand, recent approaches such as hierarchical beta-negative binomial process (Zhou et al., 2012; Broderick et al., 2015) and hierarchical gamma-Poisson process (Titsias, 2008; Zhou & Carin, 2015) model the object as a point process, sampled from an object specific random measure, which is itself sampled from a common random measure.",
      "startOffset" : 89,
      "endOffset" : 132
    }, {
      "referenceID" : 9,
      "context" : "Alternatively, for the HDP, a Chinese restaurant franchise scheme (Teh et al., 2006) can be used for collapsed inference in the model (that is, without explicitly instantiating the atoms).",
      "startOffset" : 66,
      "endOffset" : 84
    }, {
      "referenceID" : 12,
      "context" : "Fully collapsed inference scheme has also been proposed for beta-negative binomial process (BNBP) (Heaukulani & Roy, 2013; Zhou, 2014) and Gamma-Gamma-Poisson process (Zhou et al., 2015).",
      "startOffset" : 167,
      "endOffset" : 186
    }, {
      "referenceID" : 0,
      "context" : "elling have been available in literature since more than a decade (Landauer & Dumais, 1997; Hofmann, 1999; Blei et al., 2001), the first non-parametric approach, that allowed the number of latent classes to be determined as well, was the hierarchical Dirichlet process (HDP) (Teh et al., 2006). Both the approaches model the object as a set of repeated draws from an object-specific distribution, whereby the object specific distribution is itself sampled from a common distribution. On the other hand, recent approaches such as hierarchical beta-negative binomial process (Zhou et al., 2012; Broderick et al., 2015) and hierarchical gamma-Poisson process (Titsias, 2008; Zhou & Carin, 2015) model the object as a point process, sampled from an object specific random measure, which is itself sampled from a common random measure. In some sense, these approaches are more natural for mixed membership modelling, since they model the object as a single entity rather than as a sequence of draws from a distribution. A straightforward implementation of any of the above nonparametric models would require sampling the atoms in the non-parametric distribution for the base as well as objectspecific measure. However, since the number of atoms in these distributions are often infinite, a truncation step is required to ensure tractability. Alternatively, for the HDP, a Chinese restaurant franchise scheme (Teh et al., 2006) can be used for collapsed inference in the model (that is, without explicitly instantiating the atoms). Fully collapsed inference scheme has also been proposed for beta-negative binomial process (BNBP) (Heaukulani & Roy, 2013; Zhou, 2014) and Gamma-Gamma-Poisson process (Zhou et al., 2015). Of particular relevance is the work by Roy (2014), whereby a Chinese restaurant fanchise scheme has been proposed for hierarchies of beta proceses (and its generalizations), when coupled with Bernoulli process.",
      "startOffset" : 107,
      "endOffset" : 1764
    }, {
      "referenceID" : 0,
      "context" : "elling have been available in literature since more than a decade (Landauer & Dumais, 1997; Hofmann, 1999; Blei et al., 2001), the first non-parametric approach, that allowed the number of latent classes to be determined as well, was the hierarchical Dirichlet process (HDP) (Teh et al., 2006). Both the approaches model the object as a set of repeated draws from an object-specific distribution, whereby the object specific distribution is itself sampled from a common distribution. On the other hand, recent approaches such as hierarchical beta-negative binomial process (Zhou et al., 2012; Broderick et al., 2015) and hierarchical gamma-Poisson process (Titsias, 2008; Zhou & Carin, 2015) model the object as a point process, sampled from an object specific random measure, which is itself sampled from a common random measure. In some sense, these approaches are more natural for mixed membership modelling, since they model the object as a single entity rather than as a sequence of draws from a distribution. A straightforward implementation of any of the above nonparametric models would require sampling the atoms in the non-parametric distribution for the base as well as objectspecific measure. However, since the number of atoms in these distributions are often infinite, a truncation step is required to ensure tractability. Alternatively, for the HDP, a Chinese restaurant franchise scheme (Teh et al., 2006) can be used for collapsed inference in the model (that is, without explicitly instantiating the atoms). Fully collapsed inference scheme has also been proposed for beta-negative binomial process (BNBP) (Heaukulani & Roy, 2013; Zhou, 2014) and Gamma-Gamma-Poisson process (Zhou et al., 2015). Of particular relevance is the work by Roy (2014), whereby a Chinese restaurant fanchise scheme has been proposed for hierarchies of beta proceses (and its generalizations), when coupled with Bernoulli process. In this paper, it is our aim to extend fully collapsed sampling so as to allow any completely random measure (CRM) for the choice of base and object-specific measure. As proposed in Roy (2014) for hierarchies of generalized beta processes, we propose Chinese restaurant franchise schemes for hierarchies of CRMs, when coupled with Poisar X iv :1 50 9.",
      "startOffset" : 107,
      "endOffset" : 2118
    }, {
      "referenceID" : 9,
      "context" : "In an analny with the Chinese restaurant franchise model (Teh et al., 2006), one can think of Til",
      "startOffset" : 57,
      "endOffset" : 75
    } ],
    "year" : 2016,
    "abstractText" : "The aim of the paper is to provide an exact approach for generating a Poisson process sampled from a hierarchical CRM, without having to instantiate the infinitely many atoms of the random measures. We use completely random measures (CRM) and hierarchical CRM to define a prior for Poisson processes. We derive the marginal distribution of the resultant point process, when the underlying CRM is marginalized out. Using well known properties unique to Poisson processes, we were able to derive an exact approach for instantiating a Poisson process with a hierarchical CRM prior. Furthermore, we derive Gibbs sampling strategies for hierarchical CRM models based on Chinese restaurant franchise sampling scheme. As an example, we present the sum of generalized gamma process (SGGP), and show its application in topicmodelling. We show that one can determine the power-law behaviour of the topics and words in a Bayesian fashion, by defining a prior on the parameters of SGGP.",
    "creator" : "LaTeX with hyperref package"
  }
}