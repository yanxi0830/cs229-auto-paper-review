{
  "name" : "1704.06061.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Multi-view (Joint) Probability Linear Discrimination Analysis for Multi-view Feature Verification",
    "authors" : [ "Ziqiang Shi", "Liu Liu", "Mengjiao Wang", "Rujie Liu" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n70 4.\n06 06\n1v 4\n[ cs\n.L G\n] 7\nJ ul\n2 01\n1 Introduction\nThere is a long tradition of using probability linear dimensionality reduction methods for object recognition, for example face recognition and speaker verification [6, 13, 7]. Most notably, these include Factor Analysis (FA) and Probabilistic Linear Discriminant Analysis (PLDA). While FA captures the main correlations between the coordinates of a feature vector, which may represents a face image or a speech utterance, PLDA split the total data variability into within-individual and between-individual variabilities, both residing on small-dimensional subspaces. This makes PLDA suitable for a wide variety of recognition tasks, such as “one-shot learning” [10], verification (hypothesis testing) [13, 15] and etc..\nBesides the original PLDA formulation [6, 13], there are several alternative variants: Gaussian PLDA [4] and heavy-tailed PLDA [8] assume the priors on the model’s latent variables follow a Gaussian distribution or Student’s t distribution; mixture of PLDA [14, 12] assumes the feature is generated from a mixture of factor analysis models.\nOne of the questions these PLDA methods do not answer is: what shall we do with the situation that the training samples potentially belongs to not only single category, but have different kinds of labels, for example multi-task learned features? Take pose dependent face recognition or text dependent speaker verification for example, the pose or text latent variable is no longer dependent only on the current label, but rather depends on a separate pose or text label. This means the two latent variables related to speaker/individual and pose/text have the equal importance, and both variables are tied across all samples sharing a certain label.\nIn order to solve this problem, we propose a generalization of the PLDA model called multi-view (joint) PLDA explicitly model jointly multi-view information from samples. The relationship between multi-view (joint) PLDA and standard PLDA is analogous to that between joint factor analysis and factor analysis. Here we need to clarify that the term “joint PLDA” also used in the work of [1], however indeed Chen et al.\n∗Fujitsu Research & Development Center, Beijing, China. †shiziqiang@cn.fujitsu.com 1The term “joint” comes from the work [3] of Dr. Ferrer, who propose a similar idea independently almost at the same time, although they have not do experiments to verify the idea in any applications yet. For this, we will not distinguish between the term “joint” and “multi-view” for the description of PLDA in this work\nonly employed the traditional PLDA but to define the class as the “joint” classes considering both speaker and text phrase information.\nThe paper is organized as follows: sections 2 reviews the traditional PLDA method. Section 3 describe the proposed multi-view (joint) PLDA approach. Section 4 shows real life application of the multi-view (joint) PLDA, the experiments and results of the various systems are discussed. Finally, we conclude in section 5. Full derivation of the EM algorithm and scoring function of the multi-view (joint) PLDA are provided in the appendix of this paper.\n2 Probability Linear Discrimination Analysis\nProbabilistic Linear Discriminant Analysis (PLDA) can be thought of as Linear Discriminant Analysis (LDA) with a probability distributions attached to the features, where the probability distribution models the data through the latent variables corresponding to the class and the view [13].\nTake the speaker verification or face recognition for example, assume given the training data consists of I individuals each with Hi utterances or images. PLDA models data generation using the following equation:\nxij = µ+Bzi + ǫij .\nǫij is defined to be Gaussian with diagonal covariance Σ. Let θ = {µ,B,Σ}, xi = {xij : j = 1, ..., Hi} and X = {xij : i = 1, ..., I; j = 1, ..., Hi}. More formally the model can be described in terms of conditional probabilities:\np(xij |zi, θ) = N (xij |µ+Bzi,Σ),\np(zi) = N (zi|0, I),\nwhere N (x|µ,Σ) represents a Gaussian in x with mean µ and covariance Σ. The parameters θ of this PLDA model can be estimated using the Expectation Maximization (EM) [2] algorithm. With the learned PLDA model, given a test xt and an enrolled model xs, the likelihood ratio score is\nl(xt, xs) = P (xt, xs|same-individual)\nP (xt, xs|different-individuals)\n=\n∫\np(xt, xs, z|θ)dz ∫\np(xt, zt|θ)dzt ∫ p(xs, zs|θ)dzs\n=\n∫\np(xt, xs|z, θ)p(z)dz ∫\np(xt|zt, θ)p(zt)dzt ∫ p(xs|zs, θ)p(zs)dzs\n=\nN (\n[\nxt xs\n]\n|\n[\nµ µ\n]\n,\n[\nBBT +Σ BBT\nBBT BBT + Σ\n]\n)\nN (xt|µ,BB T +Σ)N (xs|µ,BB T +Σ) .\nThis standard PLDA cannot properly deal with multi-view features that belong to multiple classes, e.g. the multi-task learning features jointly having different kinds of labels. Typical examples include text dependent speaker verification or pose dependent face recognition. It is noted that we need to define the PLDA latent variable zi as the joint variable considering multiple category information. This means the latent variable zi is dependent on multiple category labels. In this work we try to separate the zi into independent latent variables - each related to the single category information. This intuitive idea result in the following multi-view (joint) PLDA.\n3 Multi-view (joint) PLDA\nIn this section, we propose an effective method to describe the multi-view features as resulting from a generative model which incorporates jointly both within-multi-view and between-multi-view variation. In verification we calculate the likelihoods that the two vectors having all views consistent or not, and the ratio of these two likelihoods will be used for the final decision.\n3.1 Generative model\nIn this work, for notation simplicity we assume the feature has only two kinds of views A and B, and it can be readily generalized to situations with three or more views. Let θ = {µ,S,T,Σ}. We assume that the training (development) data consists of I of A views (for example speaker/individual identities) and J of B views (for example phrases/poses) each pair with Hij features. We denote the k’th feature with the i’th A view and j’th B view by xijk. We model the multi-view feature generation by the process:\nxijk = µ+ Sui +Tvj + ǫijk. (3.1)\nThe model comprises two parts: 1, the signal component µ+Sui +Tvj which depends only on both the {A,B} views but not the particular feature vector (i.e. there is no dependence on k); 2, the noise component ǫijk which is different for every feature vector and represents within-multi-view noise. The term µ represents the overall mean of the training vectors. The columns of the matrix S and T contain a basis for the betweenA and between-B subspaces respectively, while the terms ui and vj represent the position in these spaces. Remaining unexplained data variation is explained by the residual noise term ǫijk which is defined to be Gaussian with diagonal covariance Σ. The latent variables ui and vj are particularly important in real application, e.g. text dependent speaker verification or pose dependent face recognition, as these represents the identity of the speaker/individual i and the content/angle of the text/pose j respectively. In verification, we will consider the likelihood that the two vectors were generated from the same underlying ui and vj .\nFormally the model can be described in terms of conditional probabilities\np(xijk |ui, vj , θ) = N (xijk |µ+ Sui +Tvj ,Σ),\np(ui) = N (ui|0, I),\np(vj) = N (vj |0, I),\nwhere N (x|µ,Σ) represents a Gaussian in x with mean µ and covariance Σ. Here it’s worth to notice that the formulation 3.1 is almost same as that of Joint Factor Analysis (JFA) [?], although the usage is totally different. JFA is mainly used as a front end for robust speaker feature extraction, while in this work the similar formulation is used as back-end for classification of j-vectors. The mathematical relationship between multi-view (joint) PLDA and JFA is analogous (not exactly) to that between PLDA and i-vector [7].\nLet X = {xijk : i = 1, ..., I; j = 1, ..., J ; k = 1, ..., Hij}. In order to find the parameters θ = {µ,S,T,Σ} under which the data set X is most likely, the classical EM algorithm [2] is employed.\n3.2 EM formulation\nThe auxiliary function for EM is\nQ(θ|θt) = EU,V |X,θt [log p(X,U, V |θ)]\n= EU,V |X,θt\n\n\n\nI ∑\ni=1\nJ ∑\nj=1\nHij ∑\nk=1\nlog[p(xijk |ui, vj , θ)p(ui, vj)]\n\n\n\n= EU,V |X,θt\n\n\n\nI ∑\ni=1\nJ ∑\nj=1\nHij ∑\nk=1\nlog[N (xijk |µ+ Sui +Tvj ,Σ)N (ui, vj |0, I)]\n\n\n\nLet zij =\n[\nui vj\n]\n, Z = {zij : i = 1, ..., I; j = 1, ..., J} and B = [ S T ] . By maximizing the auxiliary\nfunction, we obtain the following EM formulations. E steps: we need to calculate the expectations EU|X,θt [ui], EV |X,θt [vj ], EU|X,θt [uiu T i ], EV |X,θt [vjv T j ], and EU,V |X,θt [uiv T j ]. Indeed we have\nEZ|X,θt\n{[\nui vj\n]}\n= (3.2)\n( I+BTΣ−1B )−1 BT Σ−1 Hij ∑\nk=1\n(xijk − µ),\nEZ|X,θt\n{[\nuiu T i uiv T j vju T i vjv T j\n]}\n= ( I+BTΣ−1B )−1\n(3.3)\n+ EZ|X,θt [zij ]EZ|X,θt [zij ] T .\nM steps: we update the values of the parameters θ = {µ,S,T,Σ} and have\nS = { I ∑\ni=1\nJ ∑\nj=1\nHijk ∑\nk=1\n(xijk − µ)EU|X,θt [ui] T\n− I ∑\ni=1\nJ ∑\nj=1\nHijk ∑\nk=1\nTEU,V |X,θt [vju T i ]}\n{ EU|X,θt [uiu T i ] }−1 ,\nT = { I ∑\ni=1\nJ ∑\nj=1\nHijk ∑\nk=1\n(xijk − µ)EV |X,θt [vj ] T\n− I ∑\ni=1\nJ ∑\nj=1\nHijk ∑\nk=1\nSEU,V |X,θt [uiv T j ]}\n{ EV |X,θt [vjv T j ] }−1 ,\nΣ = 1\n∑I i=1 ∑J j=1 ∑Hijk k=1 1\nI ∑\ni=1\nJ ∑\nj=1\nHijk ∑\nk=1\ndiag{(xijk − µ)(xijk − µ) T −(xijk − µ) [ EU|X,θt [ui] TST + EV |X,θt [vi] TTT ] },\nµ =\n∑I i=1 ∑J j=1 ∑Hijk k=1 xijk\n∑I i=1 ∑J j=1 ∑Hijk k=1 1\n,\nwhere diag represents the operation of retaining only the diagonal elements from a matrix. The expectation terms EU|X,θt [ui], EV |X,θt [vj ], EU|X,θt [uiu T i ], EV |X,θt [vjv T j ], and EU,V |X,θt [uiv T j ] can be extracted from Equations (3.2) and (3.3).\n3.3 Likelihood Ration Scores\nWe treat the verification as a kind of hypothesis testing problem with the null hypothesis H0 defined for the all same latent variables, and the alternative hypothesis H1 defined for one or more different latent variables. That is in verification we compare the likelihood of the vectors under the hypothesis H0 where two multiview features match (have the same underlying hidden variables) and the hypothesis H1 where they do not (different underlying A variable with same B variable in model M1, same A variable with different B variables in model M2, or different underlying A variables with different B variables in model M3, as the Fig. 1 shows). If the two j-vectors belong to the same speaker saying same phrase, then they must have the same speaker and phrase variables ui and vj ; otherwise if the two j-vectors belong to different speakers or saying different phrases, they will have different speaker variable or different phrase variables. Given a test j-vector xt and an enrolled j-vector xs, set A = SS\nT +TTT , B = 2SST +TTT , C = SST +2TTT , and let the priori probability of the models M1, M2, M3 as p1 = P (M1|H1), p2 = P (M2|H1), p3 = P (M3|H1), then the likelihood ratio score is\nl(xt, xs) = P (xt, xs|H0)\nP (xt, xs|H1)\n=\n∫ ∫\np(xt, xs|u1, v1, θ)p(u1)p(v1)du1dv1 X\n=\nN (\n[\nxt xs\n]\n|\n[\nµ µ\n]\n,\n[\nA+Σ A A A+Σ\n]\n)\nX ,\nwhere\nX = P (xt, xs|H1)\n= P (xt, xs|M1)P (M1|H1)\n+ P (xt, xs|M2)P (M2|H1) + P (xt, xs|M3)P (M3|H1)\n+ p1\n∫ ∫ ∫\np(xt, xs, u1, u2, v1|θ)du1du2dv1\n= p2\n∫ ∫ ∫\np(xt, xs, u1, v1, v2|θ)du1dv1dv2\n+ p3\n∫ ∫\np(xt, u1, v1|θ)du1dv1\n∫ ∫\np(xs, u2, v2|θ)du2dv2\n= p1N (\n[\nxt xs\n]\n|\n[\nµ µ\n]\n,\n[\nC+Σ C C C+Σ\n]\n)\n+ p2N (\n[\nxt xs\n]\n|\n[\nµ µ\n]\n,\n[\nB+Σ B B B+Σ\n]\n)\n+ p3N (xt|µ,A+Σ)N (xs|µ,A+ Σ).\nNotice that like standard PLDA [13], we do not calculate a point estimate of hidden variable. Instead we compute the probability that the two multi-view vectors had the same hidden variables, regardless of what this actual latent variable was.\n4 Experiments\nMulti-view (joint) PLDA is a general method for object recognition. In this section, the method is evaluated on the task of text dependent speaker verification. We describe the experimental setup and results for the proposed method on the public RSR2015 English corpus [9] and our internal Huiting202 Chinese Mandarin database collected by the Huiting Techonogly2.\n4.1 J-vector extraction\nChen et al. [1] proposed a method to train a DNN to make classifications for both speaker and phrase identities by minimizing a total loss function consisting a sum of two cross-entropy losses as shown in Fig. 2 - one related to the speaker label and the other to the text label. Once training is complete, the output layer is removed, and the rest of the neural network is used to extract speaker-phrase joint features. That is each frame of an utterance is forward propagated through the network, and the output activations of all\n2http://huitingtech.com/\nthe frames are averaged to form an utterance-level feature called j-vector. The enrollment speaker models are formed by averaging the j-vectors corresponding to the enrollment recordings.\n4.2 Experimental setup\nRSR2015 corpus [9] was released by I2R, is used to evaluate the performance of different speaker verification systems. In this work, we follow the setup of [11], the part I of RSR2015 is used for the testing of multi-view (joint) PLDA. The background and development data of RSR2015 part I are merged as new background data to train the j-vector extractor.\nOur internal Huiting202 database is designed for local applications. It contains 202 speakers reading 20 different phrases, 20 sessions each phrase. All speech files are of 16kHz. The gender distribution is balanced on the data set. 132 randomly selected speakers are used for training the background multi-task learned DNN, and the remaining 70 speakers were used for enrollment and evaluation.\nIn this work, 39-dimensional Mel-frequency cepstral coefficients (MFCC, 13 static including the log energy + 13 ∆ + 13 ∆∆) are extracted and normalized using utterance-level mean and variance normalization. The input is stacked normalized MFCCs from 11 frames (5 frames from each side of the current frame). The DNN has 6 hidden layers (with sigmoid activation function) of 2048 nodes each. During the background model development stage, the DNN was trained by the strategy of pre-training with Restricted Boltzmann Machine (RBM) and fine tuning with SGD using cross-entropy criterion. Once the DNN is trained, the j-vector can be extracted during the enrollment and evaluation stages.\n4.3 Results and discussion\nThree systems are evaluated and compared across above conditions:\n• j-vector: the standard j-vector system with cosine similarity.\n• PLDA: the j-vector system with classic PLDA (the j-vector with “joint PLDA” called in [1]).\n• jPLDA: multi-view (joint) PLDA system described in section ?? with j-vector.\nWhen evaluation a speaker is enrolled with 3 utterances of the same phrase. The task concerns on both the phrase content and speaker identity. Nontarget trials are of three types: the impostor pronouncing wrong lexical content (impostor wrong, IW); a target speaker pronouncing wrong lexical content (target wrong, TW); the imposter pronouncing correct lexical content (impostor correct, IC).\nThe PLDA and jPLDA models both are trained using the j-vectors. The class defined in both models is the multi-task label on both the speaker and phrase. For each test session the j-vector is extracted using the same process and then log likelihood from PLDA and likelihood from jPLDA are used to distinguish among different models. The subspace dimension is set to 40 and then the PLDA model is estimated with\n10 iterations; the speaker and the phrase subspace dimensions of jPLDA are both set to 20 in order for fair comparisons and the jPLDA mode is also trained with 10 iterations.\nTable 1 and 2 compare the performances of all above-mentioned systems in terms of equal error rate (EER) for the three types of nontarget trials. Obviously jPLDA is superior thanthe standard PLDA, no matter which database is used in the test. Since multi-view PLDA system can explore both the identity and the lexical information from the j-vector, it always performs better than conventional PLDA systems.\nWe presented a novel generative model that decomposes a pattern into the different classes and the multiple views. Multi-view (joint) Probabilistic Linear Discriminant Analysis (PLDA) is related to PLDA, and can be thought of as PLDA with multiple probability distributions attached to or influence the features. One of the most important advantages of multi-view PLDA, compared to PLDA and its previously proposed probabilistic motivations, is that multiple information can be explicitly modeled and explored from the samples to improve the verification performance. Reported results showed that multi-view PLDA provided significant reduction in error rates over conventional systems in term of EER.\nReferences\n[1] Nanxin Chen, Yanmin Qian, and Kai Yu. Multi-task learning for text-dependent speaker verificaion. In INTERSPEECH, 2015.\n[2] A. P. Dempster. Maximum likelihood estimation from incomplete data via the em algorithm (with discussion. Journal of the Royal Statistical Society, 39(1):1–38, 1977.\n[3] Luciana Ferrer. Joint probabilistic linear discriminant analysis. arXiv preprint arXiv:1704.02346, 2017.\n[4] Daniel Garcia-Romero and Carol Y. Espy-Wilson. Analysis of i-vector length normalization in speaker recognition systems. In INTERSPEECH 2011, Conference of the International Speech Communication Association, Florence, Italy, August, pages 249–252, 2011.\n[5] Higham and J Nicholas. Accuracy and stability of numerical algorithms. Journal of the American Statistical Association, 16(94):285–289, 1996.\n[6] Sergey Ioffe. Probabilistic linear discriminant analysis. Proc Eccv, 22(4):531–542, 2006.\n[7] Y. Jiang, K. A. Lee, Z. Tang, B. Ma, A. Larcher, and H. Li. Plda modeling in i-vector and supervector space for speaker verification. In ACM International Conference on Multimedia, Singapore, November, pages 882–891, 2012.\n[8] P. Kenny. Bayesian speaker verification with heavy tailed priors. In Proc. Odyssey Speaker and Language Recogntion Workshop, Brno, Czech Republic, 2010.\n[9] Anthony Larcher, Kong Aik Lee, Bin Ma, and Haizhou Li. Text-dependent speaker verification: Classifiers, databases and rsr2015. Speech Communication, 60:56–77, 2014.\n[10] Fe Fei Li, R Fergus, and P Perona. A bayesian approach to unsupervised one-shot learning of object categories. In IEEE International Conference on Computer Vision, 2003. Proceedings, pages 1134–1141 vol.2, 2003.\n[11] Yuan Liu, Yanmin Qian, Nanxin Chen, Tianfan Fu, Ya Zhang, and Kai Yu. Deep feature for textdependent speaker verification. Speech Communication, 73:1–13, 2015.\n[12] Man-Wai Mak, Xiaomin Pang, and Jen-Tzung Chien. Mixture of plda for noise robust i-vector speaker verification. IEEE/ACM Transactions on Audio, Speech and Language Processing (TASLP), 24(1):130– 142, 2016.\n[13] Simon J D Prince and James H Elder. Probabilistic linear discriminant analysis for inferences about identity. Proceedings, pages 1–8, 2007.\n[14] Konstantin Simonchik, Timur Pekhovsky, Andrey Shulipa, and Anton Afanasyev. Supervized mixture of plda models for cross-channel speaker verification. In INTERSPEECH, pages 1684–1687, 2012.\n[15] Ehsan Variani, Xin Lei, Erik Mcdermott, and Ignacio Lopez Moreno. Deep neural networks for small footprint text-dependent speaker verification. In ICASSP 2014 - 2014 IEEE International Conference on Acoustics, Speech and Signal Processing, pages 4052–4056, 2014.\nThis appendix provides the derivation of the formulae about multi-view (joint) Probability Linear Discriminant Analysis (PLDA). The appendix starts with the general Expectation Maximization (EM) algorithm and EM to the conventional PLDA. Then, the appendix extends the derivation to multi-view PLDA.\nA The Expectation Maximization (EM) algorithm\nGiven the statistical model which generates a set X of observed data, a set of unobserved latent data or missing values Z, and a vector of unknown parameters θ, along with a likelihood function L(θ;X,Z) = p(X,Z|θ), the maximum likelihood estimate (MLE) of the unknown parameters is determined by the marginal likelihood of the observed data\nL(θ;X) = p(X |θ) = ∑\nZ\np(X,Z|θ).\nHowever, this quantity is often intractable. The EM algorithm seeks to find the MLE of the marginal likelihood by iteratively applying the following two steps:\nExpectation step (E step): Calculate the expected value of the log likelihood function, with respect to the conditional distribution of Z given X under the current estimate of the parameters θt\nQ(θ|θt) = EZ|X,θt [logL(θ;X,Z)].\nMaximization step (M step): Find the parameter that maximizes this quantity:\nθt+1 := argmax θ Q(θ|θt).\nExpectation-maximization works to improve Q(θ|θt) rather than directly improving log p(X |θ). Here we show that improvements to the former imply improvements to the latter.\nFor any Z with non-zero probability p(Z|X, θ), we can write log p(X |θ) = log p(X,Z|θ)− log p(Z|X, θ) . We take the expectation over possible values of the unknown data Z under the current parameter estimate θt by multiplying both sides by p(Z|X, θt) and summing (or integrating) over Z. The left-hand side is the expectation of a constant, so we get:\nlog p(X |θ) = ∑\nZ\np(Z|X, θt) log p(X,Z|θ)− ∑\nZ\np(Z|X, θt) log p(Z|X, θ)\n= Q(θ|θt) +H(θ|θt) ,\nwhere H(θ|θt) is defined by the negated sum it is replacing. This last equation holds for any value of θ including θ = θt, log p(X |θt) = Q(θt|θt) + H(θt|θt) , and subtracting this last equation from the previous equation gives log p(X |θ)− log p(X |θt) = Q(θ|θt)−Q(θt|θt)+H(θ|θt)−H(θt|θt) , However, Gibbs’ inequality tells us that H(θ|θt) ≥ H(θt|θt), so we can conclude that\nlog p(X |θ)− log p(X |θt) ≥ Q(θ|θt) −Q(θt|θt) . In words, choosing θ to improve Q(θ|θt) beyond Q(θt|θt) can not cause log p(X |θ) to decrease below log p(X |θt), and so the marginal likelihood of the data is nondecreasing.\nObservations X = {x1, ..., xn}, latent variables Z = {z1, ..., zm}, the model p(x|θ).\nL(θ) = n ∑\ni=1\nlog p(xi|θ) = n ∑\ni=1\nlog m ∑\nj=1\np(xi, zj |θ).\nn ∑\ni=1\nlog p(xi|θ) = n ∑\ni=1\nlog m ∑\nj=1\np(xi, zj|θ)\n= n ∑\ni=1\nlog m ∑\nj=1\nQi(zj) p(xi, zj |θ)\nQi(zj)\n≥ n ∑\ni=1\nm ∑\nj=1\nQi(zj) log p(xi, zj |θ)\nQi(zj) .\nwhere ∑m\nj=1 Qi(zj) = 1, and ≥ becomes = if and only if Qi(zj) = p(zj|xi, θ). Thus we get the EM algorithm. E steps:\nQi(zj) := p(zj|xi, θt).\nM steps:\nθt+1 := argmax θ\nn ∑\ni=1\nm ∑\nj=1\nQi(zj) log p(xi, zj|θ)\nQi(zj) .\nThe convergence of EM algorithm. We have\nL(θt) = n ∑\ni=1\nm ∑\nj=1\nQi(zj) log p(xi, zj |θt)\nQi(zj) ,\nwhere Qi(zj) := p(zj|xi, θt).\nL(θt+1) = max θ\nn ∑\ni=1\nm ∑\nj=1\nQi(zj) log p(xi, zj|θ)\nQi(zj)\n≥ n ∑\ni=1\nm ∑\nj=1\nQi(zj) log p(xi, zj |θt)\nQi(zj)\n= L(θt).\nB Probability Linear Discriminant Analysis\nWe assume that the training data consists of I speakers or individuals each with Hi sessions or images. We model data generation by the process:\nxij = µ+Bzi + ǫij .\nǫij is defined to be Gaussian with diagonal covariance Σ. Let θ = {µ,B,Σ}, xi = {xij : j = 1, ..., Hi} and X = {xij ∈ Rd : i = 1, ..., I; j = 1, ..., Hi} and B ∈ Rd×N , that is the dimension of the subspace is N .\nMore formally the model can be described in terms of conditional probabilities:\nP (xij |zi, θ) = N (xij |µ+Bzi,Σ),\nP (zi) = N (zi|0, I),\nwhere N (x|µ,Σ) represents a Gaussian in x with mean µ and covariance Σ.\nB.1 Training of PLDA\nWe use EM algorithm updates for learning the PLDA model. E steps: calculate the expectation EZ|X,θt [zi] and EZ|X,θt [ziz T i ].\nP (zi|xi, θ) ∝ P (xi|zi, θ)P (zi) =\n\n\nHi ∏\nj=1\nP (xij |zi, θ)\n\nP (zi)\n=\n\n\nHi ∏\nj=1\nN (xij |µ+Bzi,Σ)\n\nN (zi|0, I)\n∝\n\n\n\nHi ∏\nj=1\nexp\n[\n− 1\n2 (xij − µ−Bzi)\nTΣ−1(xij − µ−Bzi)\n]\n\n\n\nexp\n(\n− 1\n2 zTi zi\n)\n= exp\n\n\n\n− 1\n2\n\n\nHi ∑\nj=1\n(xij − µ−Bzi) TΣ−1(xij − µ−Bzi)\n\n− 1\n2 zTi zi\n\n\n\n∝ exp\n\n\n\nHi ∑\nj=1\n(xij − µ) TΣ−1Bzi −\n1 2 zTi (I+HiB TΣ−1B)zi\n\n\n\nThus we have\nEZ|X,θt [zi] = ( I+HiB TΣ−1B )−1 BTΣ−1\nHi ∑\nj=1\n(xij − µ).\nEZ|X,θt [ziz T i ] =\n(\nI+HiB TΣ−1B )−1 + EZ|X,θt [zi]EZ|X,θt [zi] T .\nM steps:\nθt+1 := argmax θ Q(θ|θt)\n= argmax θ EZ|X,θt [logL(θ;X,Z)] = argmax θ EZ|X,θt [log p(X,Z|θ)]\n= argmax θ EZ|X,θt\n\n\nI ∑\ni=1\nHi ∑\nj=1\nlog p(xij , zi|θ)\n\n\n= argmax θ EZ|X,θt\n\n\n\nI ∑\ni=1\nHi ∑\nj=1\nlog[p(xij |zi, θ)p(zi)]\n\n\n\n= argmax θ EZ|X,θt\n\n\n\nI ∑\ni=1\nHi ∑\nj=1\nlog[N (xij |µ+Bzi,Σ)N (zi|0, I)]\n\n\n\n= argmax θ EZ|X,θt\n\n\n\nI ∑\ni=1\nHi ∑\nj=1\nlog\n[\n1 √\n(2π)D|Σ| exp\n(\n− 1\n2 (xij − µ−Bzi)\nTΣ−1(xij − µ−Bzi)\n)\n]\n\n\n\n= − argmax θ\nI ∑\ni=1\nHi ∑\nj=1\nEZ|X,θt\n{\n1 2 log |Σ|+ 1 2 (xij − µ−Bzi) TΣ−1(xij − µ−Bzi))\n}\n= argmax θ\nI ∑\ni=1\nHi ∑\nj=1\n[\n− 1\n2 log |Σ| −\n1 2 (xij − µ) TΣ−1(xij − µ)\n]\n+\nI ∑\ni=1\nHi ∑\nj=1\n(xij − µ) TΣ−1BEZ|X,θt [zi]−\n1\n2\n\n\nI ∑\ni=1\nHi ∑\nj=1\nEZ|X,θt [z T i B TΣ−1Bzi]\n\n\nwhere the fact that N (zi|0, I) has no dependence on θ is used in the derivation. Take derivatives with respect to B, Σ−1, and µ and then equate these derivatives to zero to proved the update rules. The following is the detailed derivations. We have\n∂Q ∂B =\nI ∑\ni=1\nHi ∑\nj=1\nΣ−1(xij − µ)EZ|X,θt [zi] T −\nI ∑\ni=1\nHi ∑\nj=1\nΣ−1BEZ|X,θt [ziz T i ].\nSetting ∂Q ∂B = 0, result in\nB =\n\n\n\nI ∑\ni=1\nHi ∑\nj=1\n(xij − µ)EZ|X,θt [zi] T\n\n\n\n\n\n\nI ∑\ni=1\nHi ∑\nj=1\nEZ|X,θt [ziz T i ]\n\n\n\n−1\n.\nWe have\n∂Q\n∂Σ−1 =\n1\n2\nI ∑\ni=1\nHi ∑\nj=1\n[ Σ− (xij − µ)(xij − µ) T ]\n+\nI ∑\ni=1\nHi ∑\nj=1\n(xij − µ)EZ|X,θt [zi] TBT −\n1\n2\n\n\nI ∑\ni=1\nHi ∑\nj=1\nBEZ|X,θt [ziz T i ]B T\n\n\n= 1\n2\nI ∑\ni=1\nHi ∑\nj=1\n[ Σ− (xij − µ)(xij − µ) T ]\n+\nI ∑\ni=1\nHi ∑\nj=1\n(xij − µ)EZ|X,θt [zi] TBT −\n1\n2\n\n\nI ∑\ni=1\nHi ∑\nj=1\n(xij − µ)EZ|X,θt [zi] TBT\n\n\nSetting ∂Q ∂Σ−1 = 0, we have\nI ∑\ni=1\nHi ∑\nj=1\nΣ = I ∑\ni=1\nHi ∑\nj=1\n{\n(xij − µ)(xij − µ) T − (xij − µ)EZ|X,θt [zi] TBT } .\nRearranging, result in\nΣ = 1\n∑I i=1 ∑Hi j=1 1\nI ∑\ni=1\nHi ∑\nj=1\n{\n(xij − µ)(xij − µ) T − (xij − µ)EZ|X,θt [zi] TBT } .\nWe have\n∂Q ∂µ =\nI ∑\ni=1\nHi ∑\nj=1\n−(xij − µ) TΣ−1 −\nI ∑\ni=1\nHi ∑\nj=1\nEZ|X,θt [zi] TBTΣ−1\nSetting ∂Q ∂µ\n= 0, we have I\n∑\ni=1\nHi ∑\nj=1\n−(xij − µ) T −\nI ∑\ni=1\nHi ∑\nj=1\nEZ|X,θt [zi] TBT = 0\nSince EZ|X,θt [zi] ≈ 0, we have ∑I i=1 ∑Hi j=1 −(xij − µ) = 0, that is\nµ =\n∑I i=1 ∑Hi j=1 xij\n∑I i=1 ∑Hi j=1 1\n.\nB.2 Verification by using PLDA\nGiven a test xt and target xs, the likelihood ratio score is\nl(xt, xs) = P (xt, xs|same latent variable)\nP (xt, xs|different latent variables) =\n∫\np(xt, xs, z|θ)dz ∫\np(xt, zt|θ)dzt ∫ p(xs, zs|θ)dzs\n=\n∫\np(xt, xs|z, θ)p(z)dz ∫\np(xt|zt, θ)p(zt)dzt ∫ p(xs|zs, θ)p(zs)dzs =\nN (\n[\nxt xs\n]\n|\n[\nµ µ\n]\n,\n[\nBBT +Σ BBT\nBBT BBT +Σ\n]\n)\nN (xt|µ,BB T +Σ)N (xs|µ,BB T +Σ) .\nWe take log of both side\nlog l(xt, xs) = logN (\n[\nxt xs\n]\n|\n[\nµ µ\n]\n,\n[\nBBT +Σ BBT\nBBT BBT +Σ\n]\n)− logN (xt|µ,BB T +Σ)N (xs|µ,BB T +Σ).\nSince µ is a global offset that can be pre-computed and removed from all xij , we set µ = 0, and we have\nlog l(xt, xs) = − 1\n2\n[\nxTt x T s\n]\n[\nBBT +Σ BBT\nBBT BBT +Σ ]−1 [ xt xs ]\n+ 1\n2\n[\nxTt x T s\n]\n[\nBBT +Σ 0\n0 BBT +Σ ]−1 [ xt xs ] + const.\nLet Σ1 = BB T +Σ and Σ2 = BB T , we have\nlog l(xt, xs) = − 1\n2\n[\nxTt x T s\n]\n[\nΣ1 Σ2 Σ2 Σ1 ]−1 [ xt xs ] + 1 2 [ xTt x T s ] [ Σ1 0 0 Σ1 ]−1 [ xt xs ] + const.\n= − 1\n2\n[\nxTt x T s\n]\n[\n(Σ1 − Σ2Σ −1 1 Σ2) −1 −Σ−11 Σ2(Σ1 − Σ2Σ −1 1 Σ2) −1\n−(Σ1 − Σ2Σ −1 1 Σ2) −1Σ2Σ −1 1 (Σ1 − Σ2Σ −1 1 Σ2) −1\n] [\nxt xs\n]\n+ 1\n2\n[\nxTt x T s\n]\n[\nΣ−11 0 0 Σ−11\n] [\nxt xs\n]\n+ const.\n= 1\n2\n[\nxTt x T s\n]\n[\nΣ−11 − (Σ1 − Σ2Σ −1 1 Σ2) −1 Σ−11 Σ2(Σ1 − Σ2Σ −1 1 Σ2) −1 Σ−11 Σ2(Σ1 − Σ2Σ −1 1 Σ2) −1 Σ−11 − (Σ1 − Σ2Σ −1 1 Σ2) −1\n] [\nxt xs\n]\n+ const\n= 1\n2\n[\nxTt x T s\n]\n[\nQ P P Q\n] [\nxt xs\n]\n+ const\n= 1\n2\n[\nxTt Qxt + 2x T t Pxs + x T s Qxs\n]\n+ const\nThe above approach needs to obtain the inverse of the matrix Σ1 and Σ1 −Σ2Σ −1 1 Σ2, both have the size of d×d. When d ≫ 0 (that situation is very common in modern application), the inverse becomes very hard to solve or even cannot be solved. If we look carefully about the definition of Σ1, it is indeed an inverse of a low-rank correction of Σ, which can always be computed by doing a low-rank correction to the inverse of the original matrix using the Woodbury matrix identity [5]:\n(A+ UCV ) −1 = A−1 −A−1U ( C−1 + V A−1U )−1 V A−1,\nwhere A is n-by-n and invertible, U is n-by-k, C is k-by-k and invertible, and V is k-by-n. Using the Woodbury matrix identity, we have\nΣ−11 = (Σ +BB T )−1\n= Σ−1 − Σ−1B ( I+BTΣ−1B )−1 BTΣ−1\nwhere we successfully transform the a d × d matrix inverse into a N × N matrix inverse. Using the same identity, with a little cumbersome computation, we have\n(\nΣ1 − Σ2Σ −1 1 Σ2 )−1 =\n(\nΣ +BBT −BBT ( Σ+BBT )−1 BBT )−1\n=\n(\nΣ +BBT −B\n(\nBT ( Σ+BBT )−1 B\n) BT )−1\n=\n(\nΣ +B\n(\nI−BT ( Σ+BBT )−1 B\n) BT )−1\n= ( Σ+BXBT )−1\n= Σ−1 − Σ−1B ( X−1 +BTΣ−1B )−1 BTΣ−1\nwhere we define X = I−BT ( Σ+BBT )−1 B.\nLet Y = ( Σ1 − Σ2Σ −1 1 Σ2 )−1 then we have\nlog l(xt, xs) = 1\n2\n[\nxTt x T s\n]\n[\nΣ−11 − (Σ1 − Σ2Σ −1 1 Σ2) −1 Σ−11 Σ2(Σ1 − Σ2Σ −1 1 Σ2) −1 Σ−11 Σ2(Σ1 − Σ2Σ −1 1 Σ2) −1 Σ−11 − (Σ1 − Σ2Σ −1 1 Σ2) −1\n] [\nxt xs\n]\n+ const\n= 1\n2\n[\nxTt x T s\n]\n[\nΣ−11 −Y Σ −1 1 Σ2Y Σ−11 Σ2Y Σ −1 1 −Y\n] [\nxt xs\n]\n+ const\n= 1\n2\n[ xTt ( Σ−11 −Y ) xt + x T t ( Σ−11 Σ2Y ) xs + x T s ( Σ−11 Σ2Y ) xt + x T s ( Σ−11 −Y ) xs ] + const\n= 1\n2\n[ xTt ( Σ−11 −Y ) xt + 2x T t ( Σ−11 Σ2Y ) xs + x T s ( Σ−11 −Y ) xs ] + const\n= 1\n2 [xTt\n(\n−Σ−1B ( I+BTΣ−1B )−1 BTΣ−1 +Σ−1B ( X−1 +BTΣ−1B )−1 BTΣ−1 )\nxt\n+ 2xTt\n(\nΣ−1 − Σ−1B ( I+BTΣ−1B )−1 BTΣ−1 ) BBT ( Σ−1 − Σ−1B ( X−1 +BTΣ−1B )−1 BTΣ−1 )\nxs\n+ xTs\n(\n−Σ−1B ( I+BTΣ−1B )−1 BTΣ−1 +Σ−1B ( X−1 +BTΣ−1B )−1 BTΣ−1 )\nxs] + const\n= 1\n2 [xTt\n( −Σ−1BTBTΣ−1 +Σ−1BHBTΣ−1 )\nxt + x T s\n( −Σ−1BTBTΣ−1 +Σ−1BHBTΣ−1 )\nxs\n+ 2xTt\n( Σ−1B− Σ−1BTBTΣ−1B )( BTΣ−1 −BTΣ−1BHBTΣ−1 )\nxs] + const\nwhere we defined T = ( I+BTΣ−1B )−1 and H = ( X−1 +BTΣ−1B )−1 which are inverse of N×N matrix,\nsince Σ1, Σ2 and further Y are all symmetric matrix.\nC Multi-view PLDA\nThe goal of this section is to present the EM algorithm updates for leaning the multi-view (joint) PLDA model.\nC.1 Training of multi-view (joint) PLDA\nLet X = {xijk ∈ Rd : i = 1, ..., I; j = 1, ..., J ; k = 1, ..., Hij} and xij = {xijk : k = 1, ..., Hij}. E steps: we need to calculate the expectations EU|X,θt [ui], EV |X,θt [vj ], EU|X,θt [uiu T i ], EV |X,θt [vjv T j ], and EU,V |X,θt [uiv T j ]. Indeed we have\nP (ui, vj |xij , θ) ∝ P (xij |ui, vj , θ)P (ui, vj) =\n\n\nHij ∏\nk=1\nP (xijk |ui, vj , θ)\n\nP (ui, vj)\n=\n\n\nHij ∏\nk=1\nN (xijk |µ+ Sui +Tvi,Σ)\n\nN (ui, vj |0, I)\n∝\n\n\n\nHij ∏\nk=1\nexp\n[\n− 1\n2 (xijk − µ− Sui −Tvj)\nTΣ−1(xijk − µ− Sui −Tvj)\n]\n\n\n\nexp\n(\n− 1\n2 uTi ui −\n1 2 vTj vj\n)\n= exp\n\n\n\n− 1\n2\n\n\nHij ∑\nk=1\n(xijk − µ−Bzij) TΣ−1(xijk − µ−Bzij)\n\n− 1\n2 uTi ui −\n1 2 vTj vj\n\n\n\n∝ exp\n\n\n\nHij ∑\nk=1\n(xijk − µ) TΣ−1Bzij −\n1 2 zTij(I+B TΣ−1B)zij\n\n\n\n,\nwhere zij =\n[\nui vj\n]\nand B = [ S T ] .\nThus we have\nEZ|X,θt\n{[\nui vj\n]}\n= EZ|X,θt [zij ] = ( I+HijB TΣ−1B )−1 BTΣ−1\nHij ∑\nk=1\n(xijk − µ).\nwhere Z = {zij : i = 1, ..., I; j = 1, ..., J}.\nEZ|X,θt\n{[\nuiu T i uiv T j vju T i vjv T j\n]}\n= EZ|X,θt [zijz T ij ] =\n(\nI+HijB TΣ−1B )−1 + EZ|X,θt [zij ]EZ|X,θt [zij ] T .\nM steps: we update the values of the parameters θ = {µ,S,T,Σ} and have\nθt+1 := argmax θ Q(θ|θt)\n= argmax θ EU,V |X,θt [logL(θ;X,U, V )] = argmax θ EU,V |X,θt [log p(X,U, V |θ)]\n= argmax θ EU,V |X,θt\n\n\nI ∑\ni=1\nJ ∑\nj=1\nHij ∑\nk=1\nlog p(xijk, ui, vj |θ)\n\n\n= argmax θ EU,V |X,θt\n\n\n\nI ∑\ni=1\nJ ∑\nj=1\nHij ∑\nk=1\nlog[p(xijk |ui, vj , θ)p(ui, vj)]\n\n\n\n= argmax θ EU,V |X,θt\n\n\n\nI ∑\ni=1\nJ ∑\nj=1\nHij ∑\nk=1\nlog[N (xijk |µ+ Sui +Tvj ,Σ)N (ui, vj |0, I)]\n\n\n\n= argmax θ EU,V |X,θt \n\n\nI ∑\ni=1\nJ ∑\nj=1\nHij ∑\nk=1\nlog\n[\n1 √\n(2π)D|Σ| exp\n(\n− 1\n2 (xijk − µ− Sui −Tvi)\nTΣ−1(xijk − µ− Sui −Tvj)\n)\n]\n\n\n\n= − argmax θ\nI ∑\ni=1\nJ ∑\nj=1\nHij ∑\nk=1\nEU,V |X,θt\n{\n1 2 log |Σ|+ 1 2 (xijk − µ− Sui −Tvi) TΣ−1(xijk − µ− Sui −Tvj))\n}\n= argmax θ\nI ∑\ni=1\nJ ∑\nj=1\nHijk ∑\nk=1\n[\n− 1\n2 log |Σ| −\n1 2 (xijk − µ) TΣ−1(xijk − µ)\n]\n+\nI ∑\ni=1\nJ ∑\nj=1\nHijk ∑\nk=1\n(xijk − µ) TΣ−1\n{ SEU|X,θt [ui] +TEV |X,θt [vi] }\n− 1\n2\nI ∑\ni=1\nJ ∑\nj=1\nHijk ∑\nk=1\n{\nEU|X,θt [u T i S TΣ−1Sui] + 2EU,V |X,θt [v T j T TΣ−1Sui] + EV |X,θt [v T j T TΣ−1Tvj ] }\nIn this above derivation, we use the fact that N (ui, vj |0, I) has nothing to do with θ. We have\n∂Q ∂S =\nI ∑\ni=1\nJ ∑\nj=1\nHijk ∑\nk=1\nΣ−1(xijk−µ)EU|X,θt [ui] T−\nI ∑\ni=1\nJ ∑\nj=1\nHijk ∑\nk=1\nΣ−1SEU|X,θt [uiu T i ]−\nI ∑\ni=1\nJ ∑\nj=1\nHijk ∑\nk=1\nΣ−1TEU,V |X,θt [vju T i ].\n∂Q ∂T =\nI ∑\ni=1\nJ ∑\nj=1\nHijk ∑\nk=1\nΣ−1(xijk−µ)EV |X,θt [vj ] T−\nI ∑\ni=1\nJ ∑\nj=1\nHijk ∑\nk=1\nΣ−1TEV |X,θt [vjv T j ]−\nI ∑\ni=1\nJ ∑\nj=1\nHijk ∑\nk=1\nΣ−1SEU,V |X,θt [uiv T j ].\nSetting ∂Q ∂S = 0 and ∂Q ∂T = 0, we have\nS =\n\n\n\nI ∑\ni=1\nJ ∑\nj=1\nHijk ∑\nk=1\n(xijk − µ)EU|X,θt [ui] T −\nI ∑\ni=1\nJ ∑\nj=1\nHijk ∑\nk=1\nTEU,V |X,θt [vju T i ]\n\n\n\n{ EU|X,θt [uiu T i ] }−1 .\nT =\n\n\n\nI ∑\ni=1\nJ ∑\nj=1\nHijk ∑\nk=1\n(xijk − µ)EV |X,θt [vj ] T −\nI ∑\ni=1\nJ ∑\nj=1\nHijk ∑\nk=1\nSEU,V |X,θt [uiv T j ]\n\n\n\n{ EV |X,θt [vjv T j ] }−1 .\nWe have\n∂Q\n∂Σ−1 =\n1\n2\nI ∑\ni=1\nJ ∑\nj=1\nHijk ∑\nk=1\n[ Σ− (xijk − µ)(xijk − µ) T ]\n+\nI ∑\ni=1\nJ ∑\nj=1\nHijk ∑\nk=1\n(xijk − µ) [ EU|X,θt [ui] TST + EV |X,θt [vj ] TTT ]\n− 1\n2\n\n\nI ∑\ni=1\nJ ∑\nj=1\nHijk ∑\nk=1\n(\nSEU|X,θt [uiu T i ]S T + 2TEU,V |X,θt [vju T i ]S T +TEV |X,θt [vjv T j ]T\nT )\n\n\n= 1\n2\nI ∑\ni=1\nJ ∑\nj=1\nHijk ∑\nk=1\n[ Σ− (xijk − µ)(xijk − µ) T ]\n+ I ∑\ni=1\nJ ∑\nj=1\nHijk ∑\nk=1\n(xijk − µ) [ EU|X,θt [ui] TST + EV |X,θt [vj ] TTT ]\n− 1\n2\n\n\nI ∑\ni=1\nJ ∑\nj=1\nHijk ∑\nk=1\n(\n(xijk − µ)EU|X,θt [ui] TST + (xijk − µ)EV |X,θt [vj ] TTT )\n\n\nSetting ∂Q ∂Σ−1 = 0, we have\nI ∑\ni=1\nJ ∑\nj=1\nHijk ∑\nk=1\nΣ =\nI ∑\ni=1\nJ ∑\nj=1\nHijk ∑\nk=1\n{\n(xijk − µ)(xijk − µ) T − (xijk − µ)\n[\nEU|X,θt [ui] TST + EV |X,θt [vi] TTT ]} .\nRearranging, result in\nΣ = 1\n∑I i=1 ∑J j=1 ∑Hijk k=1 1\nI ∑\ni=1\nJ ∑\nj=1\nHijk ∑\nk=1\n{\n(xijk − µ)(xijk − µ) T − (xijk − µ)\n[\nEU|X,θt [ui] TST + EV |X,θt [vi] TTT ]} .\nWe have\n∂Q ∂µ =\nI ∑\ni=1\nJ ∑\nj=1\nHijk ∑\nk=1\n−(xijk − µ) TΣ−1 −\nI ∑\ni=1\nJ ∑\nj=1\nHijk ∑\nk=1\n[\nEU|X,θt [ui] TST + EV |X,θt [vi] TTT ] Σ−1\nSetting ∂Q ∂µ = 0, we have\nI ∑\ni=1\nJ ∑\nj=1\nHijk ∑\nk=1\n−(xijk − µ) T −\nI ∑\ni=1\nJ ∑\nj=1\nHijk ∑\nk=1\n[\nEU|X,θt [ui] TST + EV |X,θt [vj ] TTT ] = 0\nSince EU|X,θt [ui] ≈ 0 and EV |X,θt [vj ] ≈ 0, we have ∑I i=1 ∑J j=1 ∑Hijk k=1 −(xij − µ) = 0, that is\nµ =\n∑I i=1 ∑J j=1 ∑Hijk k=1 xijk\n∑I i=1 ∑J j=1 ∑Hijk k=1 1\n.\nC.2 Verification of the latent variables u and v separately\nIn multi-view (joint) PLDA, as described in the main body of this work, the latent variables u and v are verified jointly. Indeed the latent variables u and v can also be verified separately.\nSimilar with the verification of hidden variables u and v jointly together in the main body, we also treat the verification of single variable u as a kind of hypothesis testing problem with the null hypothesis H0 defined for the all same latent variable u, and the alternative hypothesis H1 defined for different u variables. That is in verification we compare the likelihood of the vectors under the hypothesisH0 where two multi-view features have the same underlying hidden variable u (same v variable in model M0, different v variables in model M1, as the Fig. 3 shows) and the hypothesis H1 where they have different latent variables of u (same v variable in model M2, different v variables in model M3, as the Fig. 1 shows). Given a test feature xt and an enrolled feature xs, set A = SS\nT + TTT , B = 2SST + TTT , C = SST + 2TTT , and let the priori probability of the models M0, M1, M2, M3 as p0 = P (M0|H0), p1 = P (M1|H0), p2 = P (M2|H1), p3 = P (M3|H1), then the likelihood ratio score is\nl(xt, xs) = P (xt, xs|H0)\nP (xt, xs|H1)\n= X\nY\nwhere\nX\n= P (xt, xs|H0)\n= P (xt, xs|M0)P (M0|H0) + P (xt, xs|M1)P (M1|H0)\n= p0\n∫ ∫\np(xt, xs, u1, v1|θ)du1adv1 + p2\n∫ ∫ ∫\np(xt, xs, u1, v1, v2|θ)du1dv1dv2\n= p0N (\n[\nxt xs\n]\n|\n[\nµ µ\n]\n,\n[\nA+Σ A A A+Σ\n]\n) + p1N (\n[\nxt xs\n]\n|\n[\nµ µ\n]\n,\n[\nB+Σ B B B+Σ\n]\n).\nand\nY\n= P (xt, xs|H1)\n= P (xt, xs|M2)P (M2|H1) + P (xt, xs|M3)P (M3|H1)\n= p2\n∫ ∫ ∫\np(xt, xs, u1, u2, v1|θ)du1du2dv1 + p3\n∫ ∫\np(xt, u1, v1|θ)du1dv1\n∫ ∫\np(xs, u2, v2|θ)du2dv2\n= p2N (\n[\nxt xs\n]\n|\n[\nµ µ\n]\n,\n[\nC+Σ C C C+Σ\n]\n) + p3N (\n[\nxt xs\n]\n|\n[\nµ µ\n]\n,\n[\nA+Σ 0 0 A+Σ\n]\n)."
    } ],
    "references" : [ {
      "title" : "Multi-task learning for text-dependent speaker verificaion",
      "author" : [ "Nanxin Chen", "Yanmin Qian", "Kai Yu" ],
      "venue" : "In INTERSPEECH,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2015
    }, {
      "title" : "Maximum likelihood estimation from incomplete data via the em algorithm (with discussion",
      "author" : [ "A.P. Dempster" ],
      "venue" : "Journal of the Royal Statistical Society,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 1977
    }, {
      "title" : "Joint probabilistic linear discriminant analysis",
      "author" : [ "Luciana Ferrer" ],
      "venue" : "arXiv preprint arXiv:1704.02346,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2017
    }, {
      "title" : "Analysis of i-vector length normalization in speaker recognition systems",
      "author" : [ "Daniel Garcia-Romero", "Carol Y. Espy-Wilson" ],
      "venue" : "In INTERSPEECH 2011, Conference of the International Speech Communication",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2011
    }, {
      "title" : "Accuracy and stability of numerical algorithms",
      "author" : [ "Higham", "J Nicholas" ],
      "venue" : "Journal of the American Statistical Association,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 1996
    }, {
      "title" : "Probabilistic linear discriminant analysis",
      "author" : [ "Sergey Ioffe" ],
      "venue" : "Proc Eccv,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2006
    }, {
      "title" : "Plda modeling in i-vector and supervector space for speaker verification",
      "author" : [ "Y. Jiang", "K.A. Lee", "Z. Tang", "B. Ma", "A. Larcher", "H. Li" ],
      "venue" : "In ACM International Conference on Multimedia, Singapore,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2012
    }, {
      "title" : "Bayesian speaker verification with heavy tailed priors",
      "author" : [ "P. Kenny" ],
      "venue" : "In Proc. Odyssey Speaker and Language Recogntion Workshop, Brno, Czech Republic,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2010
    }, {
      "title" : "Text-dependent speaker verification: Classifiers, databases and rsr2015",
      "author" : [ "Anthony Larcher", "Kong Aik Lee", "Bin Ma", "Haizhou Li" ],
      "venue" : "Speech Communication,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2014
    }, {
      "title" : "A bayesian approach to unsupervised one-shot learning of object categories",
      "author" : [ "Fe Fei Li", "R Fergus", "P Perona" ],
      "venue" : "In IEEE International Conference on Computer Vision,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2003
    }, {
      "title" : "Deep feature for textdependent speaker verification",
      "author" : [ "Yuan Liu", "Yanmin Qian", "Nanxin Chen", "Tianfan Fu", "Ya Zhang", "Kai Yu" ],
      "venue" : "Speech Communication,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2015
    }, {
      "title" : "Mixture of plda for noise robust i-vector speaker verification",
      "author" : [ "Man-Wai Mak", "Xiaomin Pang", "Jen-Tzung Chien" ],
      "venue" : "IEEE/ACM Transactions on Audio, Speech and Language Processing (TASLP),",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2016
    }, {
      "title" : "Probabilistic linear discriminant analysis for inferences about identity",
      "author" : [ "Simon J D Prince", "James H Elder" ],
      "venue" : null,
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2007
    }, {
      "title" : "Supervized mixture of plda models for cross-channel speaker verification",
      "author" : [ "Konstantin Simonchik", "Timur Pekhovsky", "Andrey Shulipa", "Anton Afanasyev" ],
      "venue" : "In INTERSPEECH,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2012
    } ],
    "referenceMentions" : [ {
      "referenceID" : 5,
      "context" : "1 Introduction There is a long tradition of using probability linear dimensionality reduction methods for object recognition, for example face recognition and speaker verification [6, 13, 7].",
      "startOffset" : 180,
      "endOffset" : 190
    }, {
      "referenceID" : 12,
      "context" : "1 Introduction There is a long tradition of using probability linear dimensionality reduction methods for object recognition, for example face recognition and speaker verification [6, 13, 7].",
      "startOffset" : 180,
      "endOffset" : 190
    }, {
      "referenceID" : 6,
      "context" : "1 Introduction There is a long tradition of using probability linear dimensionality reduction methods for object recognition, for example face recognition and speaker verification [6, 13, 7].",
      "startOffset" : 180,
      "endOffset" : 190
    }, {
      "referenceID" : 9,
      "context" : "This makes PLDA suitable for a wide variety of recognition tasks, such as “one-shot learning” [10], verification (hypothesis testing) [13, 15] and etc.",
      "startOffset" : 94,
      "endOffset" : 98
    }, {
      "referenceID" : 12,
      "context" : "This makes PLDA suitable for a wide variety of recognition tasks, such as “one-shot learning” [10], verification (hypothesis testing) [13, 15] and etc.",
      "startOffset" : 134,
      "endOffset" : 142
    }, {
      "referenceID" : 5,
      "context" : "Besides the original PLDA formulation [6, 13], there are several alternative variants: Gaussian PLDA [4] and heavy-tailed PLDA [8] assume the priors on the model’s latent variables follow a Gaussian distribution or Student’s t distribution; mixture of PLDA [14, 12] assumes the feature is generated from a mixture of factor analysis models.",
      "startOffset" : 38,
      "endOffset" : 45
    }, {
      "referenceID" : 12,
      "context" : "Besides the original PLDA formulation [6, 13], there are several alternative variants: Gaussian PLDA [4] and heavy-tailed PLDA [8] assume the priors on the model’s latent variables follow a Gaussian distribution or Student’s t distribution; mixture of PLDA [14, 12] assumes the feature is generated from a mixture of factor analysis models.",
      "startOffset" : 38,
      "endOffset" : 45
    }, {
      "referenceID" : 3,
      "context" : "Besides the original PLDA formulation [6, 13], there are several alternative variants: Gaussian PLDA [4] and heavy-tailed PLDA [8] assume the priors on the model’s latent variables follow a Gaussian distribution or Student’s t distribution; mixture of PLDA [14, 12] assumes the feature is generated from a mixture of factor analysis models.",
      "startOffset" : 101,
      "endOffset" : 104
    }, {
      "referenceID" : 7,
      "context" : "Besides the original PLDA formulation [6, 13], there are several alternative variants: Gaussian PLDA [4] and heavy-tailed PLDA [8] assume the priors on the model’s latent variables follow a Gaussian distribution or Student’s t distribution; mixture of PLDA [14, 12] assumes the feature is generated from a mixture of factor analysis models.",
      "startOffset" : 127,
      "endOffset" : 130
    }, {
      "referenceID" : 13,
      "context" : "Besides the original PLDA formulation [6, 13], there are several alternative variants: Gaussian PLDA [4] and heavy-tailed PLDA [8] assume the priors on the model’s latent variables follow a Gaussian distribution or Student’s t distribution; mixture of PLDA [14, 12] assumes the feature is generated from a mixture of factor analysis models.",
      "startOffset" : 257,
      "endOffset" : 265
    }, {
      "referenceID" : 11,
      "context" : "Besides the original PLDA formulation [6, 13], there are several alternative variants: Gaussian PLDA [4] and heavy-tailed PLDA [8] assume the priors on the model’s latent variables follow a Gaussian distribution or Student’s t distribution; mixture of PLDA [14, 12] assumes the feature is generated from a mixture of factor analysis models.",
      "startOffset" : 257,
      "endOffset" : 265
    }, {
      "referenceID" : 0,
      "context" : "Here we need to clarify that the term “joint PLDA” also used in the work of [1], however indeed Chen et al.",
      "startOffset" : 76,
      "endOffset" : 79
    }, {
      "referenceID" : 2,
      "context" : "com The term “joint” comes from the work [3] of Dr.",
      "startOffset" : 41,
      "endOffset" : 44
    }, {
      "referenceID" : 12,
      "context" : "2 Probability Linear Discrimination Analysis Probabilistic Linear Discriminant Analysis (PLDA) can be thought of as Linear Discriminant Analysis (LDA) with a probability distributions attached to the features, where the probability distribution models the data through the latent variables corresponding to the class and the view [13].",
      "startOffset" : 330,
      "endOffset" : 334
    }, {
      "referenceID" : 1,
      "context" : "The parameters θ of this PLDA model can be estimated using the Expectation Maximization (EM) [2] algorithm.",
      "startOffset" : 93,
      "endOffset" : 96
    }, {
      "referenceID" : 6,
      "context" : "The mathematical relationship between multi-view (joint) PLDA and JFA is analogous (not exactly) to that between PLDA and i-vector [7].",
      "startOffset" : 131,
      "endOffset" : 134
    }, {
      "referenceID" : 1,
      "context" : "In order to find the parameters θ = {μ,S,T,Σ} under which the data set X is most likely, the classical EM algorithm [2] is employed.",
      "startOffset" : 116,
      "endOffset" : 119
    }, {
      "referenceID" : 12,
      "context" : "Notice that like standard PLDA [13], we do not calculate a point estimate of hidden variable.",
      "startOffset" : 31,
      "endOffset" : 35
    }, {
      "referenceID" : 8,
      "context" : "We describe the experimental setup and results for the proposed method on the public RSR2015 English corpus [9] and our internal Huiting202 Chinese Mandarin database collected by the Huiting Techonogly.",
      "startOffset" : 108,
      "endOffset" : 111
    }, {
      "referenceID" : 0,
      "context" : "[1] proposed a method to train a DNN to make classifications for both speaker and phrase identities by minimizing a total loss function consisting a sum of two cross-entropy losses as shown in Fig.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 8,
      "context" : "2 Experimental setup RSR2015 corpus [9] was released by I2R, is used to evaluate the performance of different speaker verification systems.",
      "startOffset" : 36,
      "endOffset" : 39
    }, {
      "referenceID" : 10,
      "context" : "In this work, we follow the setup of [11], the part I of RSR2015 is used for the testing of multi-view (joint) PLDA.",
      "startOffset" : 37,
      "endOffset" : 41
    }, {
      "referenceID" : 0,
      "context" : "• PLDA: the j-vector system with classic PLDA (the j-vector with “joint PLDA” called in [1]).",
      "startOffset" : 88,
      "endOffset" : 91
    }, {
      "referenceID" : 0,
      "context" : "References [1] Nanxin Chen, Yanmin Qian, and Kai Yu.",
      "startOffset" : 11,
      "endOffset" : 14
    }, {
      "referenceID" : 1,
      "context" : "[2] A.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 2,
      "context" : "[3] Luciana Ferrer.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 3,
      "context" : "[4] Daniel Garcia-Romero and Carol Y.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 4,
      "context" : "[5] Higham and J Nicholas.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 5,
      "context" : "[6] Sergey Ioffe.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 6,
      "context" : "[7] Y.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 7,
      "context" : "[8] P.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 8,
      "context" : "[9] Anthony Larcher, Kong Aik Lee, Bin Ma, and Haizhou Li.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 9,
      "context" : "[10] Fe Fei Li, R Fergus, and P Perona.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 10,
      "context" : "[11] Yuan Liu, Yanmin Qian, Nanxin Chen, Tianfan Fu, Ya Zhang, and Kai Yu.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 11,
      "context" : "[12] Man-Wai Mak, Xiaomin Pang, and Jen-Tzung Chien.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 12,
      "context" : "[13] Simon J D Prince and James H Elder.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 13,
      "context" : "[14] Konstantin Simonchik, Timur Pekhovsky, Andrey Shulipa, and Anton Afanasyev.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 4,
      "context" : "If we look carefully about the definition of Σ1, it is indeed an inverse of a low-rank correction of Σ, which can always be computed by doing a low-rank correction to the inverse of the original matrix using the Woodbury matrix identity [5]: (A+ UCV ) −1 = A −AU ( C + V AU −1 V A, where A is n-by-n and invertible, U is n-by-k, C is k-by-k and invertible, and V is k-by-n.",
      "startOffset" : 237,
      "endOffset" : 240
    } ],
    "year" : 2017,
    "abstractText" : "Multi-view feature has been proved to be very effective in many multimedia applications. However, the current back-end classifiers cannot make full use of such features. In this paper, we propose a method to model the multi-faceted information in the multi-view features explicitly and jointly. In our approach, the feature was modeled as a result derived by a generative multi-view (joint) Probability Linear Discriminant Analysis (PLDA) model, which contains multiple kinds of latent variables. The usual PLDA model only considers one single label. However, in practical use, when using multi-task learned network as feature extractor, the extracted feature are always attached to several labels. This type of feature is called multi-view feature. With multi-view (joint) PLDA, we are able to explicitly build a model that can combine multiple heterogeneous information from the multi-view features. In verification step, we calculated the likelihood to describe whether the two features having consistent labels or not. This likelihood are used in the following decision-making. Experiments have been conducted on large scale verification task. On the public RSR2015 data corpus, the results showed that our approach can achieve 0.02% EER and 0.09% EER for impostor wrong and impostor correct cases respectively.",
    "creator" : "LaTeX with hyperref package"
  }
}