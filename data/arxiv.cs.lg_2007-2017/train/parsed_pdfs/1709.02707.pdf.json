{
  "name" : "1709.02707.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Optimally Learning Populations of Parameters",
    "authors" : [ "Kevin Tian" ],
    "emails" : [ "kjtian@stanford.edu", "whkong@stanford.edu", "valiant@stanford.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "t ) (equivalently,\n`1 distance between the CDFs), we show that, provided n is sufficiently large, we can achieve error O( 1t ) which is information theoretically optimal. We also extend our results to the multi-dimensional parameter case, capturing settings where each member of the population has multiple associated parameters. Beyond the theoretical results, we demonstrate that the recovery algorithm performs well in practice on a variety of datasets, providing illuminating insights into several domains, including politics, and sports analytics."
    }, {
      "heading" : "1 Introduction",
      "text" : "In many domains, from medical records, to the outcomes of political elections, performance in sports, and a number of biological studies, we have enormous datasets that reflect properties of a large number of entities/individuals. Nevertheless, for many of these datasets, the amount of information that we have about each entity is relatively modest—often too little to accurately infer properties about that entity. In this work, we consider the extent to which we can accurately recover an estimate of the population or set of property values of the entities, even in the regime in which there is insufficient data to resolve properties of each specific entity.\nTo give a concrete example, suppose we have a large dataset representing 1M people, that records whether each person had the flu in each of the past 5 years. Suppose each person has some underlying probability of contracting the flu in a given year, with pi representing the probability that the ith person contracts the flu each year (and assuming independence between years). With 5 years of data, the empirical estimates p̂i for each person are quite noisy (and the estimates will all be multiples of 15 ). Despite this, to what extent can we hope to accurately recover the population or set of pis? An accurate recovery of this population of parameters might be very useful—is it the case that most people have similar underlying probabilities of contracting the flu, or is there significant variation between people? Additionally, such an estimate of this population could be fruitfully leveraged as a prior in making concrete predictions about individuals’ pis.\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\nar X\niv :1\n70 9.\n02 70\n7v 1\n[ cs\n.L G\n] 8\nThe following example motivates the hope for significantly improving upon the empirical estimates:\nExample 1 Consider a set of n biased coins, with the ith coin having an unknown bias pi. Suppose we flip each coin twice (independently), and observe that the number of coins where both flips landed heads is roughly n4 , and similarly for the number coins that landed HT, TH, and TT . We can safely conclude that almost all of the pis are almost exactly 12 . The reasoning proceeds in two steps: first, since the average outcome is balanced between heads and tails, the average pi must be very close to 12 . Given this, if there was any significant amount of variation in the pis, one would expect to see significantly more HHs and TT s than the HT and TH outcomes, simply because Pr[Binomial(2, p) = 1] = 2p(1− p) attains a maximum for p = 1/2.\nFurthermore, suppose we now consider the ith coin, and see that it landed heads twice. The empirical estimate of pi would be 1, but if we observe close to n4 coins with each pair of outcomes, using the above reasoning that argues that almost all of the ps are likely close to 12 , we could safely conclude that pi ≈ 12 .\nThis ability to “denoise” the empirical estimate of a parameter based on the observations of a number of independent random variables (in this case, the outcomes of the tosses of the other coins), was first pointed out by Charles Stein in the setting of estimating the means of a set of Gaussians and is known as “Stein’s phenomena” [12]. We discuss this further in Section 1.1. Example 1 was chosen to be an extreme illustration of the ability to leverage having a large number of entities being studied, n, to partially compensate for having a very small amount of data reflecting each entity (2 tosses of each coin).\nOur main result, stated below, demonstrates that even for worst-case sets of pis, significant “denoising” is possible. While we cannot hope to always accurately recover each pi, we show that we can accurately recover the set or histogram of the pis, as measured in the `1 distance between the cumulative distribution functions, or equivalently, the “earth mover’s distance” (also known as 1-Wasserstein distance — we will use “Wasserstein distance” and “earth mover’s distance” interchangeably) between the set of ps regarded as a distribution P that places mass 1n at each pi, and the distribution Q returned by our estimator.1\nTheorem 1 Suppose there is a set of n probabilities, p1, . . . , pn with pi ∈ [0, 1], and we observe the outcomes of t independent flips of each coin, namely X1, . . . , Xn, with Xi ∼ Binomial(t, pi). There is an algorithm that produces a distribution Q supported on [0, 1], such that with probability at least 1− δ over the randomness of X1, . . . , Xn,\n‖P −Q‖W ≤ min 1≤s≤t C1 s + C2 √ s√ nδ s∑ i=1 2i √ Ep∼P [pi] ≤ C1 t +Oδ,t( 1√ n ),\nfor absolute constants C1, C2, where P denotes the distribution that places mass 1n at value pi, and ‖ · ‖W denotes the Wasserstein distance.\nThe inverse linear dependence on t of Theorem 1 is information theoretically optimal, and is attained asymptotically for sufficiently large n:\nProposition 1 Fix an integer t ≥ 1, and let P denote the distribution that assigns mass 1n to each value in p1, . . . , pn. Let X1, . . . , Xn correspond to the random variables distributed as Xi ∼ Binomial(t, pi), and an estimator f maps X1, . . . , Xn to a distribution f(X1, . . . , Xn). Then\nlim n→∞ min f max P\nE [‖f(X1, . . . , Xn)− P‖W ] > 1\n4t .\nOur estimation algorithm, whose performance is characterized by Theorem 1, proceeds via the method of moments. Given X1, . . . , Xn with Xi ∼ Binomial(t, pi), and sufficiently large n, we can obtain accurate estimates of the first t moments of the distribution/histogram P defined by the pis. Accurate estimates of the first t moments can then be leveraged to recover an estimate of P that\n1Equivalently, our returned distribution Q can also be regarded as a set of n values q1, . . . , qn, in which case this earth mover’s distance is precisely 1/n times the `1 distance between the vector of sorted ps, and the vector of sorted qs.\nis accurate to error 1t plus a factor that depends (exponentially on t) on the error in the recovered moments, though this error tends to zero as n gets large.\nThe intuition for the lower bound is that the realizations of Binomial(t, pi) give no information beyond the first t moments. Additionally, there exist distributions P and Q whose first t moments agree exactly, but which differ in their t+ 1st moment, and have ‖P −Q‖W ≥ 12t . Putting these two pieces together establishes the lower bound.\nWe also consider multi-parameter analogs of the setting described above, where the ith datapoint corresponds to a pair, or d-tuple of hidden parameters, p(i,1), . . . , p(i,d), and we observe independent random variables X(i,1), . . . , X(i,d) with X(i,j) ∼ Binomial(t(i,j), p(i,j)). In this setting, the goal is to recover the multivariate set of d-tuples {p(i,1), . . . , p(i,d)}, again in an earth mover’s sense. To give one concrete motivation for this problem, consider a hypothetical setting where we have n genotypes (sets of genetic features), with ti people of each genotype. Let X(i,1) denote the number of people with the ith genotype who exhibit disease 1, and X(i,2) denote the number of people with genotype i who exhibit disease 2. The interpretation of the hidden parameters pi,1 and pi,2 are the respective probabilities of people with the ith genotypes of developing each of the two diseases. Our results imply that provided n is large, one can accurately recover an approximation to the underlying set or two-dimensional distribution of {(pi,1, pi,2)} pairs, even where there are too few people of each genotype to accurately determine which of the genotypes are responsible for elevated disease risk. Recovering this set of pairs would allow one to infer whether there are common genetic drivers of the two diseases—even in the regime where there is insufficient data to resolve which genotypes are the common drivers.\nOur multivariate analog of Theorem 1, also formulated in terms of earth mover’s distance (see Definition 1) is the following:\nTheorem 2 Suppose there is a set of n d-tuples of hidden parameters in [0, 1]d, pi,j , 1 ≤ i ≤ n, 1 ≤ j ≤ d with pi,j ∈ [0, 1], and we observe the outcomes of t independent flips of each coin, namely Xi,j , with Xi,j ∼ Binomial(t, pi,j). There is an algorithm that produces a distribution Q supported on [0, 1]d, such that with probability at least 1− δ over the randomness of Xi,j ,\n‖P −Q‖W ≤ min 1≤s≤t C1 s + C2 √ s√ nδ s∑ |α|=1 (2s)d2s 3|α| √ Ep∼P [pα] ≤ C1 t +Oδ,t,d( 1√ n ),\nfor absolute constants C1, C2, where α is a d-dimensional multi-index consisting of all d-tuples of nonnegative integers summing to at most t, P denotes the distribution that places mass 1n at value pi, and ‖ · ‖W denotes the d-dimensional Wasserstein distance between P and Q."
    }, {
      "heading" : "1.1 Related Work",
      "text" : "The seminal paper of Charles Stein [12] was one of the earliest papers to identify the surprising possibility of leveraging the availability of independent data reflecting a large number of parameters of interest, to partially compensate for having little information about each parameter. The specific setting examined considered the problem of estimating a list of unknown means, µ1, . . . , µn given access to n independent Gaussian random variables, X1, . . . , Xn, withXi ∼ N (µi, 1). Stein showed that, perhaps surprisingly, that there is an estimator for the list of parameters µ1, . . . , µn that has smaller expected squared error than the naive unbiased empirical estimates of µ̂i = Xi. This improved estimator “shrinks” the empirical estimates towards the average of the Xis. In our setting, the process of recovering the set/histogram of unknown pis and then leveraging this recovered set as a prior to correct the empirical estimates of each pi can be viewed as an analog of Stein’s “shrinkage”, and will have the property that the empirical estimates are shifted (in a non-linear fashion) towards the average of the pis.\nMore closely related to the problem considered in this paper is the work on the problem of recovering an approximation to the unlabeled set of probabilities of domain elements, given independent draws from a distribution of large discrete support (see e.g. [10, 2, 13, 14, 1]). Instead of learning the distribution, these works considered the alternate goal of simply returning an approximation to the multiset of probabilities with which the domain elements arise but without specifying which element occurs with which probability. Such a multiset can be used to estimate useful properties of the\ndistribution that do not depend on the labels of the domain of the distribution, such as the entropy or support size of the distribution, or the number of elements likely to be observed in a new, larger sample [11, 15]. The benefit of pursuing this weaker goal of returning the unlabeled multiset is that it can be learned to significantly higher accuracy for a given sample size—essentially as accurate as the empirical distribution of a sample that is a logarithmic factor larger [13, 15].\nBuilding on the above work, the recent work [16] considered the problem of recovering the “frequency spectrum” of rare genetic variants. This problem is similar to the problem we consider, but focuses on a rather different regime. Specifically, the model considered posits that each location i = 1, . . . , n in the genome has some probability pi of being mutated in a given individual. Given the sequences of t individuals, the goal is to recover the set of pis. The work [16] focused on the regime in which many of the pis are significantly less than 1nt , and hence correspond to mutations that have never been observed; one conclusion of that work was that one can accurately estimate the number of such rare mutations that would be discovered in larger sequencing cohorts. Our work, in contrast, focuses on the regime where the pis are constant, and do not scale as a function of n, and the results are incomparable.\nAlso related to the current work are the papers [4, 6] that consider the problem of learning “Poisson Binomials,” namely a sum of independent non-identical Bernoulli (0/1) random variables, given access to samples. In contrast to this work, in the setting they consider, each “sample” consists of only the sum of these n random variables, rather than observing the outcome of each random variable."
    }, {
      "heading" : "1.2 Our contributions and organization of paper",
      "text" : "The novel theoretical results in this work are the efficient distribution estimation algorithm and the proof of Theorems 1 and 2, whose errors asymptotically reach the lower bound. In Section 2 we describe the proof approach in the univariate setting; the extension to the multi-dimensional setting is described in Section 2.4. In Section 3 we validate the empirical performance of our approach on synthetic data, as well as illustrate its potential applications to several real-world settings."
    }, {
      "heading" : "2 Learning the empirical distribution of binomial parameters",
      "text" : ""
    }, {
      "heading" : "2.1 Moment estimation",
      "text" : "Our method-of-moments approach proceeds by estimating the first t moments of P , namely E[pk] where p ∼ P , for k between 1 and t. We limit ourselves to the first t moments because, as show in the proof of the lower bound, Proposition 1, the distribution of the Xis are determined by the first t moments, and hence no additional information can be gleaned regarding the higher moments.\nFor 1 ≤ k ≤ t, our estimate for the kth moment is\nβk = 1\nn n∑ i=1 (∑ 1[Xij=1] k )( t k\n) (1) The motivation for this unbiased estimator is the following: for each 1 ≤ i ≤ n, there exists only one unbiased estimator for pki that uses all t available samples Xij ∼ Bernoulli(pi). In particular, note that given any k i.i.d. samples of a variable distributed according to Bernoulli(pi), an unbiased estimator for pki is 1[Xij = 1∀j], namely the estimator which is 1 if all the tosses come up heads, and otherwise is 0. Thus, if we average over all ( t k ) subsets of size k, and then average over the population, we still derive an unbiased estimator.\nThis estimator has several nice properties that we will use. We show in the appendices that this choice of estimator is the minimum variance unbiased estimator, and has variance bounded by kE[pk] tn\n, implying via Chebyshev’s inequality and a union bound the following lemma characterizing the accuracy of the moment recovery:\nLemma 1 Let αk denote the kth true moment of P . With probability 1− δ, for all 1 ≤ k ≤ s, all the events |αk − βk| ≤ √ s δ √ kE[pk] tn occur.\nBecause of the combinatorial nature of our moment estimates, the variance-to-mean ratio of the higher moments (e.g. k ≈ t) is going to be much larger than those of some of the lower order moments (e.g. k ≈ t2 ). This motivates the benefit of ignoring the estimates of some of the higher moments, particularly in settings where n is more modest."
    }, {
      "heading" : "2.2 Earth mover’s distance bounds of recovered distributions",
      "text" : "Given the ability to accurately recover the moments, we show that we can leverage these estimates to recover a distribution that is not too far from the truth. We define the Wasserstein (earth mover’s) distance between two distributions P and Q:\nDefinition 1 The Wasserstein, or earth mover’s, distance between distributions P,Q, is ||P − Q||W := inf\nγ∈Γ(P,Q)\n∫ [0,1]2d d(x, y)dγ(x, y), where Γ(P,Q) is the set of all couplings on P and Q,\nnamely a distribution whose marginals agree with the distributions. The equivalent dual definition is ||P −Q||W := sup\nLip(g)≤1\n∫ [0,1] g(x)d(P −Q)(x) where the supremum is taken over Lipschitz g.\nThis can be thought of as the minimum amount of “mass” that needs to be moved to change P into Q. In 1 dimension, it’s not hard to see that this distance is the `1 distance between the associated CDFs. In this section we will outline the proof of the following stronger version of Proposition 1 in [9]:\nTheorem 3 For two distributions P and Q supported on [0, 1] whose first s moments are α and β respectively, the Wasserstein distance ||P − Q||W is bounded by C1s + C2 ∑s k=1 2\nk|αk − βk| for absolute constants C1, C2.\nNote combining this result with Lemma 1 immediately yields Theorem 1. Now we give a sketch of the proof of Theorem 1, where some technical details are deferred to the appendices. First we consider the setting where two distributions P,Q supported on [0, 1] have the exact same first s moments, and bound their Wasserstein distance. The main idea is that for any degree-s polynomial f , its inner product with P − Q on the domain is 0. Furthermore, it’s sufficient to show that any Lipschitz h is close to a degree-s polynomial. Indeed, for any such h,\n∫ h(x)(P (x)−Q(x))dx = ∫ (h(x)− f(x))(P (x)−Q(x))dx\n≤ ∫ |f(x)− h(x)||P (x)−Q(x)|dx ≤ 2||h− f ||∞\n(2)\nThus, it suffices to show that there’s some f which is close to h in the infinity norm. That there exists a degree-s polynomial f that is at most 1s away is a special case of Jackson’s theorem, whose proof we defer to the appendices.\nIn the more robust setting where there is noise in our moment estimates, the additional error accumulates as (α− β)Tc, where c is the vector of coefficients in our polynomial interpolation. We show that c grows as 2i in the appendices, whereas the error in our moment estimates decreases roughly as pi/2, yielding Theorem 3."
    }, {
      "heading" : "2.3 Our distribution learning algorithm",
      "text" : "The algorithm consists of two steps. First, from the samples, we compute estimates of moments of P as described in Equation 1. Next, we use a linear program to assign weights on an -net of [0, 1] to match the estimates. The distance between any distribution and its discrete rounding is bounded by 2 , and the difference in moments also propagates as O( ), because the individual contributions of point masses to the kth moment cannot change more than O( k) < O( ). So we pick = 1m , where m is the number of points in the net, fine enough that it does not impact the distance by much.\nFormally, the algorithm takes as input Xij , for 1 ≤ i ≤ n, 1 ≤ j ≤ t, parameter s (moments to include in calculation), granularity m, and weights w. Note that the additional parameter our algorithm takes, s, is due to the fact that we can choose to only “trust” the first s moment estimates.\nAlgorithm 1: Produce estimate of distribution P given samples Xij Result: Vector q of length m+ 1 representing distribution with probability mass qi at im\n1 Compute βi = 1n ∑ (∑1[Xij=1] k )( t k\n) 2 Run subroutine yielding solution to the following linear program:\nminimize ∑\n1≤k≤s\n|β̂k − βk|wk\nwhere β̂k = ∑\n0≤i≤m\nqi( i\nm )k\nsubject to qT 1m+1 = 1and qi ≥ 0 for all i\n(3)\nNote that we are able to specify parameters wk corresponding to how much we weigh each moment difference in the objective function. Our theoretical guarantees hold if we set all wk = 1. In practice, setting eachwk inversely proportional to the standard deviation of the kth moment estimates produced by the population gave the best performance.\nOne interesting observation was that when s was large, Algorithm 1 often produced very “spiky”, or sparse estimates. This may be an artifact of the LP solvers used, and is potentially due to the fact that estimates of high-order moments are very heavy-tailed (and thus tend to be underestimates). However, by bootstrapping our estimates of the moments and averaging over distributions, the performance was shown to empirically converge much more quickly and robustly. We will refer to this bootstrapped version of Algorithm 1 as Algorithm 1B."
    }, {
      "heading" : "2.4 Extension: multivariate distribution estimation",
      "text" : "Suppose, for instance, every member i of a population of size n has two associated binomial parameters p(i,1), p(i,2), as in Theorem 2. One could estimate the marginal distribution of the p(i,1) and p(i,2) separately using Algorithm 1, but it is natural to also want to estimate the joint distribution up to small Wasserstein distance in the 2-d sense. This idea can be extended to d-dimensional distributions on binomial parameters.\nEssentially, the idea is also to include some set of reliable moments represented by multi-indices α with |α| ≤ s, for some 1 ≤ s ≤ t. For example, in a 2-d setting, the moments for members i of the population would look like Epi∼P [pa(i,1)p b (i,2)]. Again, it remains to bound how close an interpolating polynomial can get to any Lipschitz function, and the size of the coefficients of this polynomial. The following theorem from [3] is used:\nLemma 2 Given any Lipschitz function f supported on [0, 1]d, there is a degree s polynomial p(x) such that\nsup x∈[0,1]d |p(x)− f(x)| ≤ Cd s .\nFor the interpolating polynomial, we prove the following bound in the appendices:\n|cα| ≤ (2s)d2s\n3|α| (4)\nwhere cα is the coefficient of the α multinomial term. The variance of the αth moment of the distribution is smaller than |α|E[pα] tn , similarly to before. These statements then yield Theorem 2."
    }, {
      "heading" : "3 Empirical performance",
      "text" : ""
    }, {
      "heading" : "3.1 Recovering distributions with known ground truth",
      "text" : "We begin by demonstrating the effectiveness of our algorithm on several synthetic datasets. We considered three different choices for an underlying distribution P̄ over [0, 1], then drew n independent samples p1, . . . , pn ← P̄ . For a parameter t, for each i ∈ {1, . . . , n}, we then drew Xi ← Binomial(t, pi), and ran our population estimation algorithm on the set X1, . . . , Xn, and measured the extent to which we recovered the distribution P̄ . In all settings, n was sufficiently large that there was little difference between the histogram corresponding to the set {p1, . . . , pn} and the distribution P̄ . The results for three choices of the underlying distribution P̄ are depicted in Figure 1.\nFigure 2 shows representative plots of the CDFs of the recovered histograms and empirical histograms for each of the three distribution P̄ considered above.\nWe also considered recovering distributions from a flight delays dataset, which was a natural dataset that had sufficient data per flight to determine a ground truth distribution. Our algorithm was able to recover this more non-parametric distribution extremely well. This study can be found in the appendices."
    }, {
      "heading" : "3.2 Political tendencies on a county level",
      "text" : "We performed a case study on the political leanings of counties. We assumed the following rough model: n = 3116 counties each have an intrinsic “political-leaning” parameter pi denoting their likelihood of voting Republican in a given election, and there are t = 8 elections which are samples\ndrawn from the corresponding Bernoulli distribution. The specific dataset considered was the voting pattern, binary over Republican and Democratic parties, of each of the counties in the USA, in elections from 1976 to 2004. Thus we were able to produce estimates of the first 8 moments, and used Algorithm 1B to recover an estimate of the histogram of this set of pis.\nIncluding anywhere from 4-8 moments in the recovery algorithm produced CDFs that were all fairly consistent, but with varying degrees of smoothness. An example of a conclusion we can draw is that ≈ 310 of counties are strongly Democratic in their voting, ≈ 3 20 of them are strongly Republican, and most of the other counties have inherent leaning parameters between 0.4 and 0.6."
    }, {
      "heading" : "3.3 Game-to-game shooting of NBA players",
      "text" : "We performed a case study on the shooting of two NBA players. One can think of this experiment as asking whether NBA players, game-to-game, have differences in their intrinsic ability to score field goals (in the sports analytics world, this is the idea of “hot / cold” shooting nights). The experimental setup is that for a player, each game i of n games has some pi representing the player’s latent shooting percentage for that game. Certainly one would expect the empirical shooting of a player to vary significantly - learning the distribution of this intrinsic shooting percentage allows us to determine how much variation is due to natural noise and how much is due to change in the latent parameter.\nThe dataset used was the per-game 3 point shooting of players, with sufficient statistics of “3 pointers made” and “3 pointers attempted”. To generate estimates of the kth moment, we considered games where at least k 3 pointers were attempted. The players chosen were Stephen Curry of the Golden State Warriors (who is believed to be a very consistent shooter) and Danny Green of the San Antonio Spurs (whose nickname “Icy Hot” gives a good idea of his suspected consistency).\nFor various settings of the number of moments to include in Stephen Curry’s shooting distribution estimation, the CDF of the output distribution was nearly a single spike at his empirical mean (or tightly clustered around it), highlighting the consistency of his shooting with respect to all estimates of raw moments. On the contrary, the CDF produced by Algorithm 1 on the shooting of Danny Green suggests that this player does have a significant amount of game-to-game variation."
    }, {
      "heading" : "A Polynomial interpolation, exact moment matching case",
      "text" : "The idea of this section is to prove the existence of a polynomial with small distance to some fixed Lipschitz h in the infinity norm, as in the proof of Theorem 3. We recall the following classic result [5]:\nFact 1 For a given function h ∈ Cs+1[0, 1], there exists a degree s polynomial f such that ||h − f ||∞ ≤ 12s+1 ||h(s+1)||∞ (s+1)! .\nFrom (4), we wish to provide an upper bound on ||h−P ||∞, which is at most ||h− ĥ||∞+ ||ĥ−f ||∞ for any choice of ĥ. In particular we choose ĥ to have well-behaved derivatives, as the convolution of h with cb̂(cx), where c is some constant that we can choose, and b̂ is the Fourier transform of b, which is defined as follows:\nb(y) =\n{ exp(− y 2\n1−y2 ) if |y| < 1, 0 otherwise.\n(5)\nFirst, we bound ||h− ĥ||∞, as follows.\n|h(x)− ĥ(x)| = |h(x)− ∫ h(x− t)b̂(t)dt|\n= |h(x)(1− ∫ b̂(t)dt) + ∫ (h(x)− h(x− t))b̂(t)dt|\n≤ ∫ |b̂(t)t|dt = O(1\nc )\n(6)\nwhere to reach the last inequality we used Lipschitz continuity and the fact that the integral of the Fourier transform yields b(0) = 1. The last equality follows from Lemma A.2 in [7]\nFurthermore, to bound ||ĥ− f ||∞, we have from Fact 1 that it is sufficient to bound the (s + 1)st\nderivative of (̂h) as follows:\n|ĥ(s+1)|∞ = cs+1|(h ∗ b̂(s+1))(x)|∞ ≤ cs+1|h|∞|b̂(s+1)|1 = O(cs+1) (7)\nwhere the last equality uses Lemma A.3 in [7] and the fact that h is 1-Lipschitz. Setting c = s yields Theorem 3 where α = β."
    }, {
      "heading" : "B Error propagation in polynomial interpolation",
      "text" : "We now show how the error propagates when the moment estimates do not exactly match the true moments. As in the proof of the non-robust version in Section A, we have the bound on the integral of h with P −Q:\n||P −Q||W ≤ 2||h− f |∞ + cT (α − β) (8)\nwhere c is the vector of coefficients of the polynomial f used in the interpolation. Thus it suffices to bound in absolute value the size of the ith coefficient of f . Note that f is some polynomial approximation of ĥ which we can construct via interpolation on the Chebyshev nodes. To be precise, c = V −1s+1y , where V −1 s+1 is the s+ 1 by s+ 1 Vandermonde matrix on the Chebyshev nodes, and y is the value of ĥ on these locations. Since ĥ is 1-Lipschitz, it suffices to bound the `1 norm of the rows of the inverse Vandermonde matrix which will in turn bound the size of entries of c.\nWe recall the Chebyshev nodes in the range [0, 1] take the form\nxk = 1 + cos(2k−12s+2π)\n2 , k = 1, . . . , s+ 1 (9)\nThus, their mean is 12 . As shown in [8], the inverse of the Vandermonde matrix has entries\nvij = (−1)i−1  ∑ 1≤m1<...<mn−i≤n m1,...,mn−i 6=j xm1 · · ·xmn−i\n∏ 1≤m≤n m 6=j (xm − xj)\n (10)\nThe denominators of these entries do not change with respect to j, so the question is how quickly do the numerators grow with respect to i, the row indices. Note that the sum of the absolute values of numerators on the ith row is i ( n n−i ) E[xm1xm2 . . . xmn−k ] where the expectation is taken over all permutations. So, the ratio of the sum of the ith row and the i+ 1st row is bounded above by 2(n−i)i , where we used that the expectation of the product is smaller than the product of the expectations, and the expectation of the product for a single term is 12 .\nFinally, a simple induction shows that the norm of the ith row can be bounded above by O(2i), where the constant comes from the smallest of the denominators, as desired.\nC Information-theoretic lower bound\nWe will prove Proposition 1. First, we state the following result from [9]:\nLemma 3 For any t, there exists a pair of distributions DP , DQ each consisting of O(t) point masses, such that DP and DQ have identical first t moments, and ||DP −DQ||W > 12t\nNow, suppose we have a population of n parameters drawn fromDP , constituting P , and a population of n parameters drawn fromDQ, constitutingQ. First, note that if we take expectations over sampling p ∼ DP and then sample X ∼ Binomial(t, p), or we take expectations over sampling q ∼ DQ and then sample Y ∼ Binomial(t, q), the claim is that the distribution of X and Y are identical. Indeed, it’s not hard to see that the distributions of X and Y are given by\nP(X = k) = ∫ 1\n0\n( t\nk\n) xk(1− p)t−kDP (x)dx\nP(Y = k) = ∫ 1\n0\n( t\nk\n) xk(1− q)t−kDQ(x)dx\n(11)\nNoting that the integrand is a degree-t polynomial yields the conclusion that the induced probabilities are equal.\nThus, for any choice of t, there exists some distribution DP for which there is another distribution DQ that is at least 12t different than DP in earth mover’s distance and matches its first t moments. Thus any estimator f which observes samples from the induced binomial distribution will be unable to tell apart DP and DQ, so the distance will always be at least 14t in the worst case, since every distribution must be at least 14t away from either DP or DQ.\nNow, note that P and Q are the realizations of n samples from DP and DQ, so the question is whether we are able to tell apart P and Q. If we were able to tell them apart for small n, we could certainly tell them apart for n′ > n since we could just restrict the estimator to the first n samples. So it suffices to consider large n. Now because the distance between P and DP tends to 0 in the limit, and the same is true for Q and DQ, the induced distributions on X ∼ P and Y ∼ Q are also essentially the same because their moments will be close to the moments of DP and DQ with high\nprobability. So, the same argument shows that we cannot attain a better minimax distance than 14t on P , as desired."
    }, {
      "heading" : "D Multivariate error analysis",
      "text" : "We will prove Theorem 2. First, we will prove a stronger version of Lemma 2:\nLemma 4 Given any Lipschitz function f supported on [0, 1]d, there is a degree s polynomial p(x) = ∑ |α|≤s aαx α where α is multi-index {α1, α2, . . . αd}such that\nsup x∈[0,1]d |p(x)− f(x)| ≤ Cd s , (12)\nand aα ≤ Ad (2s) d2s\n3|α| .\nThis polynomial approximation lemma is basically a restatement of Theorem 1 in [3]. What we need to do is only to give an explicit upper bound of the coefficients.\nThe high level idea is to first convolve f with a holomorphic bump functionG which givesH = f ∗G, then the Maclaurin series of H is a good polynomial approximation of H and also f .\nBy the definition of Maclaurin series, aα = ∂αH(0) |α|! . Suppose H is holomorphic on an open neighborhood of some polydisk ES with radius S, assuming supz∈ES |H(z)| ≤ M , by Cauchy’s integral formula, |aα| = | 12πi ∮ |z|=S H(z) z|α|+1 | ≤ M S|α|\n. By the definition of R in the proof of Theorem 1 in [3], we can set R = 1 such that function f is supported on box BR. Let S = 2R + 1 = 3 and following all the parameter settings, by Equation 14 in [3], we have |aα| ≤ MS|α| ≤ Ad (2s)d(s+1)2s s3|α| ≤ Ad (2s)d2s\n3|α| , where Ad is a constant that depends on d."
    }, {
      "heading" : "E Properties of moment estimates",
      "text" : "Recall that the estimator we derived for pki is(∑ 1[Xij=1] k\n)( t k\n) (13) where the numerator counts the number of subsets of size k that are all 1, and the denominator is the number of subsets of size k.\nWe will show the following properties of the estimator: it is the only unbiased estimator that uses all of the information, and has well-behaved variance.\nFor one thing, imagine some unbiased estimator f of pki which uses all t bits of information, a function of the sufficient statistic m, the number of 1s within the Xij . Then we must have\nt∑ m=0 f(m) ( t m ) pmi (1− pi)t−m = pki ∀pi (14)\nwhich is a polynomial of degree t in pi. However, this must hold identically for all pi, so the polynomials must have equal coefficients everywhere. This yields t+ 1 equations in t+ 1 unknowns, so there is a unique solution set of f(m). The fact that the estimator we derived is known to be unbiased shows that it is unique.\nFurthermore, the variance of this estimator is well-controlled. Consider the following simple bound: the variance of each individual estimator of pki on a set of size k is p k i − p2ki ≤ pki , and there are at least tk independent sets that are averaged to yield estimator (3), so the variance is bounded above by kpki t .\nNow, the natural estimator for the population moments E[pk] where p ∼ f is clearly to average the estimates (3) over all n sets of samples. Thus, the estimator for the population moment is\n1\nn\n∑ (∑1[Xij=1] k )( t k\n) (15) and has variance bounded by\nkE[pk] tn\n. Note that we could have performed the same estimate with respect to a subset of the t samples, but the variance would be strictly greater. So, this is the minimum variance unbiased estimate."
    }, {
      "heading" : "F Lateness in flights",
      "text" : "We performed a case study on the 2015 Flight Delays and Cancellations dataset from Kaggle via the Department of Transportation. There, for about 2,000,000 flights, the lateness of departure and arrival times is documented, as well as flight identifiers (such as airline, flight number, departure and arrival cities). For example, a single “flight” could be some plane which flies from Los Angeles to Seattle every other day routinely. The estimation problem is for a given threshold c and a given routine flight, say c = 15 minutes, to predict the proportion of instances of that flight which are late by at least c.\nWe considered routine flights for which there were records of at least 50 instances. Fixing a threshold c, we defined the ground truth proportions for instances of “flight is at least c late” to be the empirical proportion across all the instances of the flight. There were 25156 such flights. Next, for each given flight we subsampled t instances at random, and computed the binary variables of whether each instance was late by at least c. Then we ran Algorithm 1B to produce estimates Q of the ground truth proportions. In the below plots, the red line is the CDF of the empirical estimator, the blue line is the CDF of the output of Algorithm 1B, and the green line is the ground truth.\nThe estimates were very robust to repeated runs of the experiment, producing CDFs that matched the ground truth extremely closely for various settings of c and t from 4 to 10, under Algorithm 1B. We show the proposal distributions for c = 15 minutes, t = 6, and t = 10. We report the average EMD of the proposal distribution and empirical estimate, and standard deviation of these values, over 5 re-runs, and display one representative example for each setting. Here, we only show performance on estimating departure proportions (the arrival proportion task was essentially identical)."
    } ],
    "references" : [ {
      "title" : "A unified maximum likelihood approach for optimal distribution property estimation",
      "author" : [ "Jayadev Acharya", "Hirakendu Das", "Alon Orlitsky", "Ananda Theertha Suresh" ],
      "venue" : "arXiv preprint arXiv:1611.02960,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2016
    }, {
      "title" : "Recent results on pattern maximum likelihood",
      "author" : [ "Jayadev Acharya", "Alon Orlitsky", "Shengjun Pan" ],
      "venue" : "In Networking and Information Theory,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2009
    }, {
      "title" : "Multivariate simultaneous approximation",
      "author" : [ "Thomas Bagby", "Len Bos", "Norman Levenberg" ],
      "venue" : "Constructive Approximation,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2002
    }, {
      "title" : "Learning poisson binomial distributions",
      "author" : [ "Constantinos Daskalakis", "Ilias Diakonikolas", "Rocco A Servedio" ],
      "venue" : null,
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2015
    }, {
      "title" : "Properly learning poisson binomial distributions in almost polynomial time",
      "author" : [ "Ilias Diakonikolas", "Daniel M Kane", "Alistair Stewart" ],
      "venue" : "In Conference on Learning Theory,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2016
    }, {
      "title" : "On the exact space complexity of sketching and streaming small norms",
      "author" : [ "Daniel M Kane", "Jelani Nelson", "David P Woodruff" ],
      "venue" : null,
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2010
    }, {
      "title" : "The Art of Computer Programming: Volume 1: Fundamental Algorithms (3rd Edition)",
      "author" : [ "Donald E. Knuth" ],
      "venue" : null,
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 1997
    }, {
      "title" : "Spectrum estimation from samples",
      "author" : [ "Weihao Kong", "Gregory Valiant" ],
      "venue" : "arXiv preprint arXiv:1602.00061,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2016
    }, {
      "title" : "On modeling profiles instead of values",
      "author" : [ "Alon Orlitsky", "Narayana P Santhanam", "Krishnamurthy Viswanathan", "Junan Zhang" ],
      "venue" : "In Proceedings of the 20th conference on Uncertainty in artificial intelligence,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2004
    }, {
      "title" : "Optimal prediction of the number of unseen species",
      "author" : [ "Alon Orlitsky", "Ananda Theertha Suresh", "Yihong Wu" ],
      "venue" : "Proceedings of the National Academy of Sciences,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2016
    }, {
      "title" : "Inadmissibility of the usual estimator for the mean of a multivariate normal distribution",
      "author" : [ "Charles Stein" ],
      "venue" : "In Proceedings of the Third Berkeley Symposium on Mathematical Statistics and Probability,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 1956
    }, {
      "title" : "Estimating the unseen: an n/log(n)-sample estimator for entropy and support size, shown optimal via new clts",
      "author" : [ "Gregory Valiant", "Paul Valiant" ],
      "venue" : "In Proceedings of the forty-third annual ACM symposium on Theory of computing,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2011
    }, {
      "title" : "Estimating the unseen: improved estimators for entropy and other properties",
      "author" : [ "Gregory Valiant", "Paul Valiant" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2013
    }, {
      "title" : "Instance optimal learning of discrete distributions",
      "author" : [ "Gregory Valiant", "Paul Valiant" ],
      "venue" : "In Proceedings of the 48th Annual ACM SIGACT Symposium on Theory of Computing,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Consider the following fundamental estimation problem: there are n entities, each with an unknown parameter pi ∈ [0, 1], and we observe n independent random variables,X1, .",
      "startOffset" : 113,
      "endOffset" : 119
    }, {
      "referenceID" : 10,
      "context" : "This ability to “denoise” the empirical estimate of a parameter based on the observations of a number of independent random variables (in this case, the outcomes of the tosses of the other coins), was first pointed out by Charles Stein in the setting of estimating the means of a set of Gaussians and is known as “Stein’s phenomena” [12].",
      "startOffset" : 333,
      "endOffset" : 337
    }, {
      "referenceID" : 0,
      "context" : ", pn with pi ∈ [0, 1], and we observe the outcomes of t independent flips of each coin, namely X1, .",
      "startOffset" : 15,
      "endOffset" : 21
    }, {
      "referenceID" : 0,
      "context" : "There is an algorithm that produces a distribution Q supported on [0, 1], such that with probability at least 1− δ over the randomness of X1, .",
      "startOffset" : 66,
      "endOffset" : 72
    }, {
      "referenceID" : 0,
      "context" : "Theorem 2 Suppose there is a set of n d-tuples of hidden parameters in [0, 1], pi,j , 1 ≤ i ≤ n, 1 ≤ j ≤ d with pi,j ∈ [0, 1], and we observe the outcomes of t independent flips of each coin, namely Xi,j , with Xi,j ∼ Binomial(t, pi,j).",
      "startOffset" : 71,
      "endOffset" : 77
    }, {
      "referenceID" : 0,
      "context" : "Theorem 2 Suppose there is a set of n d-tuples of hidden parameters in [0, 1], pi,j , 1 ≤ i ≤ n, 1 ≤ j ≤ d with pi,j ∈ [0, 1], and we observe the outcomes of t independent flips of each coin, namely Xi,j , with Xi,j ∼ Binomial(t, pi,j).",
      "startOffset" : 119,
      "endOffset" : 125
    }, {
      "referenceID" : 0,
      "context" : "There is an algorithm that produces a distribution Q supported on [0, 1], such that with probability at least 1− δ over the randomness of Xi,j ,",
      "startOffset" : 66,
      "endOffset" : 72
    }, {
      "referenceID" : 10,
      "context" : "The seminal paper of Charles Stein [12] was one of the earliest papers to identify the surprising possibility of leveraging the availability of independent data reflecting a large number of parameters of interest, to partially compensate for having little information about each parameter.",
      "startOffset" : 35,
      "endOffset" : 39
    }, {
      "referenceID" : 8,
      "context" : "[10, 2, 13, 14, 1]).",
      "startOffset" : 0,
      "endOffset" : 18
    }, {
      "referenceID" : 1,
      "context" : "[10, 2, 13, 14, 1]).",
      "startOffset" : 0,
      "endOffset" : 18
    }, {
      "referenceID" : 11,
      "context" : "[10, 2, 13, 14, 1]).",
      "startOffset" : 0,
      "endOffset" : 18
    }, {
      "referenceID" : 12,
      "context" : "[10, 2, 13, 14, 1]).",
      "startOffset" : 0,
      "endOffset" : 18
    }, {
      "referenceID" : 0,
      "context" : "[10, 2, 13, 14, 1]).",
      "startOffset" : 0,
      "endOffset" : 18
    }, {
      "referenceID" : 9,
      "context" : "distribution that do not depend on the labels of the domain of the distribution, such as the entropy or support size of the distribution, or the number of elements likely to be observed in a new, larger sample [11, 15].",
      "startOffset" : 210,
      "endOffset" : 218
    }, {
      "referenceID" : 13,
      "context" : "distribution that do not depend on the labels of the domain of the distribution, such as the entropy or support size of the distribution, or the number of elements likely to be observed in a new, larger sample [11, 15].",
      "startOffset" : 210,
      "endOffset" : 218
    }, {
      "referenceID" : 11,
      "context" : "The benefit of pursuing this weaker goal of returning the unlabeled multiset is that it can be learned to significantly higher accuracy for a given sample size—essentially as accurate as the empirical distribution of a sample that is a logarithmic factor larger [13, 15].",
      "startOffset" : 262,
      "endOffset" : 270
    }, {
      "referenceID" : 13,
      "context" : "The benefit of pursuing this weaker goal of returning the unlabeled multiset is that it can be learned to significantly higher accuracy for a given sample size—essentially as accurate as the empirical distribution of a sample that is a logarithmic factor larger [13, 15].",
      "startOffset" : 262,
      "endOffset" : 270
    }, {
      "referenceID" : 3,
      "context" : "Also related to the current work are the papers [4, 6] that consider the problem of learning “Poisson Binomials,” namely a sum of independent non-identical Bernoulli (0/1) random variables, given access to samples.",
      "startOffset" : 48,
      "endOffset" : 54
    }, {
      "referenceID" : 4,
      "context" : "Also related to the current work are the papers [4, 6] that consider the problem of learning “Poisson Binomials,” namely a sum of independent non-identical Bernoulli (0/1) random variables, given access to samples.",
      "startOffset" : 48,
      "endOffset" : 54
    }, {
      "referenceID" : 0,
      "context" : "Definition 1 The Wasserstein, or earth mover’s, distance between distributions P,Q, is ||P − Q||W := inf γ∈Γ(P,Q) ∫ [0,1]2d d(x, y)dγ(x, y), where Γ(P,Q) is the set of all couplings on P and Q, namely a distribution whose marginals agree with the distributions.",
      "startOffset" : 116,
      "endOffset" : 121
    }, {
      "referenceID" : 0,
      "context" : "[0,1] g(x)d(P −Q)(x) where the supremum is taken over Lipschitz g.",
      "startOffset" : 0,
      "endOffset" : 5
    }, {
      "referenceID" : 7,
      "context" : "In this section we will outline the proof of the following stronger version of Proposition 1 in [9]:",
      "startOffset" : 96,
      "endOffset" : 99
    }, {
      "referenceID" : 0,
      "context" : "Theorem 3 For two distributions P and Q supported on [0, 1] whose first s moments are α and β respectively, the Wasserstein distance ||P − Q||W is bounded by C1 s + C2 ∑s k=1 2 |αk − βk| for absolute constants C1, C2.",
      "startOffset" : 53,
      "endOffset" : 59
    }, {
      "referenceID" : 0,
      "context" : "First we consider the setting where two distributions P,Q supported on [0, 1] have the exact same first s moments, and bound their Wasserstein distance.",
      "startOffset" : 71,
      "endOffset" : 77
    }, {
      "referenceID" : 0,
      "context" : "Next, we use a linear program to assign weights on an -net of [0, 1] to match the estimates.",
      "startOffset" : 62,
      "endOffset" : 68
    }, {
      "referenceID" : 2,
      "context" : "The following theorem from [3] is used:",
      "startOffset" : 27,
      "endOffset" : 30
    }, {
      "referenceID" : 0,
      "context" : "Lemma 2 Given any Lipschitz function f supported on [0, 1], there is a degree s polynomial p(x) such that",
      "startOffset" : 52,
      "endOffset" : 58
    }, {
      "referenceID" : 0,
      "context" : "sup x∈[0,1]d |p(x)− f(x)| ≤ Cd s .",
      "startOffset" : 6,
      "endOffset" : 11
    }, {
      "referenceID" : 0,
      "context" : "We considered three different choices for an underlying distribution P̄ over [0, 1], then drew n independent samples p1, .",
      "startOffset" : 77,
      "endOffset" : 83
    }, {
      "referenceID" : 0,
      "context" : "(a) 3-spike distribution (b) truncated normal (c) Uniform on [0, 1]",
      "startOffset" : 61,
      "endOffset" : 67
    }, {
      "referenceID" : 0,
      "context" : "15, truncated to be supported on [0, 1]; and (c) the uniform distribution over [0, 1].",
      "startOffset" : 33,
      "endOffset" : 39
    }, {
      "referenceID" : 0,
      "context" : "15, truncated to be supported on [0, 1]; and (c) the uniform distribution over [0, 1].",
      "startOffset" : 79,
      "endOffset" : 85
    }, {
      "referenceID" : 0,
      "context" : "(a) 3-spike distribution (b) truncated normal (c) Uniform on [0, 1]",
      "startOffset" : 61,
      "endOffset" : 67
    } ],
    "year" : 2017,
    "abstractText" : "Consider the following fundamental estimation problem: there are n entities, each with an unknown parameter pi ∈ [0, 1], and we observe n independent random variables,X1, . . . , Xn, withXi ∼Binomial(t, pi). How accurately can one recover the “histogram” (i.e. cumulative density function) of the pis? While the empirical estimates would recover the histogram to earth mover distance Θ( 1 √ t ) (equivalently, `1 distance between the CDFs), we show that, provided n is sufficiently large, we can achieve error O( 1t ) which is information theoretically optimal. We also extend our results to the multi-dimensional parameter case, capturing settings where each member of the population has multiple associated parameters. Beyond the theoretical results, we demonstrate that the recovery algorithm performs well in practice on a variety of datasets, providing illuminating insights into several domains, including politics, and sports analytics.",
    "creator" : "LaTeX with hyperref package"
  }
}