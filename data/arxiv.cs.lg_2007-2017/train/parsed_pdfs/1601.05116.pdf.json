{
  "name" : "1601.05116.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "A Theory of Local Matching SIFT and Beyond",
    "authors" : [ "Hossein Mobahi", "Stefano Soatto" ],
    "emails" : [ "hmobahi@csail.mit.edu", "soatto@cs.ucla.edu" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Questions: Why has SIFT been so successful? Why DSP-SIFT [Dong and Soatto, 2015] can further improve SIFT? Is there a theory that can explain both? How can such theory benefit real applications? Can it suggest new algorithms with reduced computational complexity or new descriptors with better accuracy for matching?\nContributions: We construct a general theory of local descriptors for visual matching. Our theory relies on concepts in energy minimization and heat diffusion. We show that SIFT and DSP-SIFT approximate the solution the theory suggests. In particular, DSP-SIFT gives a better approximation to the theoretical solution; justifying why DSP-SIFT outperforms SIFT. We derive new algorithms based on this theory. Specifically, we present a computationally efficient approximation to DSP-SIFT algorithm [Dong and Soatto, 2015] by replacing the sampling procedure in DSP-SIFT with a closed-form approximation that does not need any sampling. This leads to a significantly faster\nar X\niv :1\n60 1.\n05 11\n6v 1\n[ cs\n.C V\n] 1\n9 Ja\nalgorithm compared to DSP-SIFT. In addition, we derive new descriptors directly from this theory. The new descriptors have fewer parameters as well as the potential of better handling affine deformations, compared to SIFT and DSP-SIFT."
    }, {
      "heading" : "2 Contributions",
      "text" : "Throughout this text, isotropic multivariate Gaussian kernel and periodic univariate Gaussian are denoted by k and k̃,\nkσ(x) , (2πσ 2)− dim(x) e−\n‖x‖2 2σ2 , k̃σ(φ) , ∞∑\nk=−∞\nkσ(φ+ 2πk) . (1)\nConsider an image f . Given an origin-centered detected key point x with assigned scale σ and orientation β. The continuous form of a SIFT descriptor can be expressed as [Dong et al., 2015, Vedaldi and Fulkerson, 2010],\nhSIFT (β,x) , ∫ X k̃σr (β − ∠∇f(y)) kσd(y − x) ‖∇f(y)‖ dy , (2)\nwhere σr resembles the size of each orientation bin, e.g. 2π 8 for 8 bins. σd determines the spatial support of the descriptor as a function of σ, e.g. σd , 3σ. By observing that the above descriptor is pooling (weighted averaging) across displacement, [Dong et al., 2015] adds domain size pooling to this construction and suggests, hDSP (β,x) , ∫ S ∫ X k̃σr (β−∠∇f(y)) kσd(y−x) ‖∇f(y)‖ dykσs(σd−σd0) dσd ,\n(3) where S , R and σd0 is a function of key point’s scale σ, e.g. σd0 , 3σ. We develop a theory for descriptor construction by returning to the origin of the problem. Specifically we formulate matching as an energy optimization problem. It is known that the resulted cost function is nonconvex for a any realistic matching setup [Dong and Soatto, 2015]. Ideally, one would need to brute-force search across all possible transformations to find the right match. This is obviously not practical.\nRecently a theory of nonconvex optimization by heat diffusion has been proposed [Mobahi and Fisher III, 2015a, Mobahi and Fisher III, 2015b]. The theory offers the best (in a certain sense) tractable solution for nonconvex problems. We show that SIFT and DSP-SIFT approximate the energy minimization solution that this theory suggests. By leveraging this connection, we present the following contributions.\nThe domain-size integration (3) is approximated by numerical sampling in [Dong et al., 2015], which is slow. Instead, we propose the following two closed-form approximations to this integral.\nhDSP (β,x) ≈ ∫ X k̃σr (β−∠∇f(y)) kσd(y−x) ‖∇f(y)‖ σd√ σ2d + ‖x‖2σ2s e σ2s(‖x‖ 2−xT y−σ2d) 2 2σ2 d (σ2 d +‖x‖2σ2s) .\n(4)\nhDSP (β,x) ≈ ∫ X k̃σr (β − ∠∇f(y)) kσd(y − x) ‖∇f(y)‖ σ2d − σsxTy (σ2d + σ 2 s‖x‖2) 3 2 e −σ 2 d‖x+y‖ 2+σ2s(x T y⊥)2 2σ2 d (σ2 d +σ2s‖x‖ 2) . (5)\nIn addition, through this theory, we propose a new descriptor. This descriptor is exact in terms of what this theory suggests. In addition, this descriptor is derived from an affine matching formulation, hence may better tolerate affine transforms than SIFT and DSP-SIFT1. Interestingly, despite handling a broader transformation space, it has fewer parameters than DSP-SIFT. Finally, it is analytical and does not need any sampling.\nhheat(β,x) , ∫ X e − (y T ∇̃f(y))2 2σ2 d w ( − 12t ∇̃ T f(y) (σ−2d yx T + σ−2a I ) ṽ(β,y) ) ‖∇f(y)‖2 t3\n×k√ σ2d+σ 2 a ‖x‖2\n( (∇f(y))T (x− y)⊥\n‖∇f(y)‖ ) dy , (6)\nwhere ∇̃f(y) , ∇f(y)‖∇f(y)‖ , ṽ(β,y) , (cos(β),sin(β)) ‖∇f(y)‖ , t ,\n√ (xT ṽ(β,y))2\n2σ2d + 12σ2a ‖∇f(y)‖2 ,\nw(x) , √ πex 2\n(1 + 2x2) erfc(x)− 2x, and (a, b)⊥ , (b,−a). Note that compared to DSP-SIFT, this descriptor reduces number of parameters from two (σr and σs) to one (σa). An illustration of how hheat differs from hSIFT is as follows. Consider a pair of images, namely image 1 and image 2, each consisting of two patches returned by some key point detector. The goal is to establish correspondence between patches using the `2 distance between normalized descriptors, d(h1 , h2) , ∫ 2π\n0 ∫ X ( h1(β,x)( ∫ 2π 0 ∫ X h 2 1(β †,x†) dx† dβ† ) 1 2 − h2(β,x)( ∫ 2π 0 ∫ X h 2 2(β †,x†) dx† dβ† ) 1 2 )2 dx dβ ,\n(7) where X , X1 ∩X2. There are two possible matches: P 1A ↔ P 2A ∧ P 1B ↔ P 2B or P 1A ↔ P 2B ∧ P 1B ↔ P 2A; obviously only the former is correct. Distance of matches using SIFT are listed in table 2. Note that SIFT descriptor attains lower distance for the wrong match and thus fails, while the heat descriptor finds the correct match. A visualization of SIFT descriptor and heat descriptor are presented in Figures 2 and 3 respectively.\n1SIFT descriptor gains robustness against displacement by pooling across it. DSP-SIFT gains further robustness against scaling by scale pooling. However, none is robust to affine transform, which would require pooling across more parameters.\nAlthough this work focuses on SIFT, our diffusion theory can possible relate and extend other descriptors as well. For example, the recently developed distribution fields [Mears et al., 2013] are similar to (2) and (3), except that instead of histogram of gradient orientation, the histogram of intensity values are used,\nhDF (l,x) , ∫ X kσl(l − f(y)) kσd(y − x) dy , (8)\nwhere σl determines the smoothing strength of pixel intensity values. Similar to SIFT arguments, the convolution kσd may correspond to diffusion w.r.t. translation, and thus diffusion w.r.t. larger class of transformation, e.g., affine, may lead to geometrically more robust descriptors. Such extensions of distribution fields are not studied in the report, but are subject of future research."
    }, {
      "heading" : "3 Matching as Energy Minimization",
      "text" : "For clarity of presentation, we focus on a restricted matching setup with simplifying assumptions. Nevertheless, this setup has enough complexity to make the point on nonconvexity and diffusion."
    }, {
      "heading" : "3.1 Problem Setup",
      "text" : "Notation: An image is a map of form f : X → [0, 1], where X ⊂ R2. Similarly, a patch is p : P → [0, 1], where P ⊆ X , i.e. the map is defined over a subset of the domain X .\nAssumptions: Given a set of patches pk : Xk → [0, 1] for k = 1, . . . , n. We assume that one of these patches, indexed by k∗, appears somewhere in f up to\na geometric transformation τ∗ : Xk∗ → X and some reasonable intensity noise2, ∃(k∗, τ∗) ∀x ∈ Xk∗ ; f ( τ∗(x) ) ≈ pk∗(x) . (9)\nObjective: The goal is to estimate (k∗, τ∗). For tractability, the space of τ is parameterized by a vector θ. For mathematical convenience, we assume the noise effect is best minimized via `2 discrepancy,\n(k∗,θ∗) , argmin (k,θ) ∫ Xk ( f ( τ(x ; θ) ) − pk(x) )2 dx . (10)\nThe tools we later use apply to continuous variables, while (10) involves the integer variable k. However, we can equivalently rewrite the problem in the following continuous form,\n(c∗,θ∗) , argmin (c,θ) ∑ k c2k ∫ Xk ( f ( τ(x ; θ) ) − pk(x) )2 dx\ns.t ∑ k ck = 1 , ∀k ; ck(1− ck) = 0 . (11)"
    }, {
      "heading" : "3.2 Intractability",
      "text" : "Despite simplicity of the setup, estimation of (k∗,θ∗) is generally intractable because the optimization problem (11) is nonconvex. Hence, local optimization methods may converge to a local minimum. In the following, we illustrate this by a toy example. The example involves a univariate signal f(x), a pair of univariate templates p1(x) and p2(x) and a translation transform τ so that f(τ(x, θ)) , f(x − θ). Thus, (11) can be expressed as below, after eliminating c2 by the equality constraint c1 + c2 = 1,\n(c∗1, θ ∗) , argmin\n(c1,θ)\nc21 ∫ X1 ( f(x− θ)− p1(x) )2 dx+ (1− c1)2 ∫ X2 ( f(x− θ)− p2(x) )2 dx\ns.t c1(1− c1) = 0 . (12)\nThe solution c∗1 determines to which template f belongs to; p1 if c ∗ 1 = 1 and\np2 if c ∗ 1 = 0.\nWe proceed by choosing f , p1, and p2 as the blue, green, and red curves in Figure 4-a. Here X = [−2, 2] and X1 = X2 = [−1.2, 1.2]. The goal is to slide the blue curve to the left or right, such that it coincides with either the green or red curve. Recall from (11) that matching error is examined only over the support of the templates (gray shade). As shown in Figure 4-b, by sliding f to the left by θ = 0.25 units, a perfect match with the green curve is achieved. However, there is no way to attain similar match with the red curve. Thus, by inspection we know that c∗ = (1, 0) and θ = 0.25.\n2In this setting each patch pk may be called a template.\nFor visualization purpose, we replace the equality in (12) by a quadratic penalty3. This encompasses both the objective and constraint into a single objective to be visualized. The resulted optimization landscape is shown in Figure 5. A local minimum is apparent around c1 = 0, θ = 0 while the global minimum is around c1 = 1, θ = 0.25.\n4 Diffusion\nOne way to approximate the solution of a nonconvex optimization problem is by diffusion and the continuation method. The idea is to follow the minimizer of the diffused cost function while progressively transforming that function to the original nonconvex cost. It has recently been shown that, this procedure with the choice of the heat kernel as the diffusion operator, provides the optimal transformation in a certain sense4 [Mobahi and Fisher III, 2015a]. In\n3Similar local minima could be obtained for the exact constrained optimization (12) using Lagrange multiplier technique.\n4It is shown that Gaussian convolution is resulted by the best affine approximation to a nonlinear PDE that generates the convex envelope. Note that computing the convex envelope of a function is generally intractable as well. Thus, it is not surprising that the associated nonlinear PDE lacks a closed form solution. However, by replacing the nonlinear PDE by its best affine approximation, we strike the optimal balance between tractability (closed form solution for the linear PDE) and accuracy of the approximation. The motivation for approximation the convex envelope is that the latter is an optimal object in several senses for the original nonconvex cost function. In particular, global minima of a nonconvex cost are contained in the global minima of its convex envelope.\nAlgorithm 1 Optimization by Diffusion and Continuation\nfact, some performance guarantees have been recently developed for this scheme [Mobahi and Fisher III, 2015b]. The procedure is defined more formally below. Given an unconstrained and nonconvex cost function h : Rn → R to be minimized. Instead of applying a local optimization algorithm directly to h, we embed h into a family of functions parameterized by σ,\ng(x ; σ) , [h ? kσ](x) , (13)\nwhere ? is the convolution operator and kσ(x) is the Gaussian function with zero mean and covariance σ2I. The Gaussian convolution appears here due to the known analytical solution form of the heat diffusion. Observe that limσ→∞ g( . ;σ) = h(x). Thus by starting from a large σ and shrinking it toward zero, a sequence of cost function converging to h is obtained. The optimization process then follows the path of the minimizer of g( . ;σ) through this sequence as listed in Algorithm 1.\nNow let us revisit the problem (12). Like before, we use a quadratic penalty to obtain an unconstrained approximate to (12). A sequence of diffused landscapes of this problem is shown in Figure 6. Note that the problem becomes convex, with a unique strict minimizer at the large σ. The solution path originated from that point eventually lands at the global minimum in this example."
    }, {
      "heading" : "5 Deriving SIFT via the Diffusion Theory",
      "text" : "Instead of pixel intensity as (11) to guide the matching, we switch to orientation of gradient. This change adds limited robustness to illumination changes [Dong and Soatto, 2015]. Nevertheless, the cost function remains nonconvex\nand difficult to minimize. Such nonconvex optimization may be treated via diffusion and continuation by the theory of [Mobahi and Fisher III, 2015a]. We show that SIFT descriptor emerges as an approximation to this process when τ is a similarity transformation, i.e. τ(x ; θ) , esRαx+ b, where θ , (α, s, b).\nThe approximation comes from two sources. First, the theory of [Mobahi and Fisher III, 2015a] suggests a continuation method by gradually reducing σ while following the path of the minimizer. SIFT provides an approximation to this process by solving the optimization at only one value of σ, i.e. it terminates after the first iteration of the algorithm suggested by [Mobahi and Fisher III, 2015a]. Second, the cost function is diffused only w.r.t. a subset of optimization variables (α and b). This deviates from the theory in [Mobahi and Fisher III, 2015a] that requires diffusion of the cost function in all variables, i.e. to use [cost ? kσ](c,θ)."
    }, {
      "heading" : "5.1 Energy Function",
      "text" : "Define the density of gradient orientations of image f as below,\nh(β,x ; f) ,X(∠∇f(x)− β) ‖∇f(x)‖ , (14) where X denotes the Dirac comb of period 2π, i.e. X(x) , ∑∞ n=−∞ δ(x+ 2πn). The Dirac comb accounts for the periodicity of the angle (gradient orientation). Let the dissimilarity between a pair of density functions over a region X be expressed as the negated dot product,\nd(f1, f2,X ) , − ∫ 2π\n0 ∫ X h(β,x ; f1) h(β,x ; f2) dβ dx . (15)\nIn this setting, the problem of template matching is to find θ such that the gradient orientations of f(τ(x ; θ)) match that of a template,\n(c∗,θ∗) , argmin (c,θ) ∑ k ck d(f ◦ τθ, pk,Xk)\ns.t ∑ k ck = 1 , ∀k ; ck(1− ck) = 0 . (16)\nReplacing the equality constraint by some penalty function q leads to the following unconstrained optimization,\ncost(c,θ) , q(c) + ∑ k ck d(f ◦ τθ, pk,Xk) . (17)"
    }, {
      "heading" : "5.2 Solution",
      "text" : "The goal is to tackle the nonconvex problem (17) using the diffusion and continuation theory of [Mobahi and Fisher III, 2015a]. This would give the algorithm listed in Table 5.2-left.\nSIFT based matching can be derived by simplifying this algorithm as described below,\n• Partial Diffusion: Instead of diffusion w.r.t. all variables (α, s, b, c), diffuse the energy function (17) partially, i.e. only with respect to (α, b).\n• Fixed σ: Instead of gradual refinement of the energy function by shrinking σ toward zero, stick to a single choice σ = σ0.\n• Limited Optimization: Rather than searching the entire parameter space for (α, s, b) for the optimal solution, restrict to a small candidate set Θ , ∪Jj=1{(αj , sj , bj)}. This set is generated outside of the optimization loop by a keypoint detector5. Consequently, Θ does not necessarily contain the optimal parameter as keypoint estimation is done for each image in isolation and thus separately from the full matching problem.\nApplying these simplifications yields the algorithm in Table 5.2-middle. The central optimization in this algorithm is the following,\nmin (α,s,b)∈Θ min c\n[ [cost ( c, ( . , s, . ) ) ? kσd ](b) ? k̃σr ] (α) , (18)\nwhere we have replaced the joint optimization minc,(α,s,b)∈Θ by the equivalent nested form min(α,s,b)∈Θ minc. The outer minimization is trivial; it just loops over the candidates and evaluates the resulted cost to pick the best one. Below we only focus on the inner optimization.\nAssuming the penalty function q(c) accurately enforces the constraint ck ∈ {0, 1}, the inner optimization becomes a winner take all problem; the winning patch pk to match f is the one which minimizes the following cost,\n5The location and scale of candidate sets are determined by an interest point detector, and the orientation angle is set to the dominant gradient direction.\nk∗ = argmin k\n[ [d(f ◦ τ( . ,s, . ), pk,Xk) ? kσd ](b) ? k̃σr ] (α) (19)\n= argmax k ∫ 2π 0 ∫ Xk ([ [h(β,x ; f ◦ τ( . ,s, . )) ? kσd ](b) ? k̃σr ] (α) ) h(β,x ; pk) dβ dx .\nWe doubt that the convolutions in (19) are computationally tractable6. Thus, in order to derive a computationally tractable algorithm, we resort to a closed form approximation to the above convolutions. The approximation is stated in the following lemma.\nLemma 1 The following approximation holds,\n[ [h(β,x ; f ◦ τ( . ,s, . )) ? kσd ](b) ? k̃σr ] (α)\n≈ − es ∫ R2 k̃σr (∠∇f(y)− α− β)‖∇f(y)‖ kσd(y − esRαx− b) dy .\nProof See Appendix B for the proof.\nUsing this lemma, the computationally intractable optimization (18) is replaced by the following tractable approximation,\nmax (α,s,b)∈Θ max k\nes ∫ 2π\n0 ∫ Xk ∫ R2 k̃σr (∠∇f(y)− α− β)‖∇f(y)‖ kσd(y − esRαx− b) dy\n×h(β,x ; pk) dx dβ , (20)\nIn the inner optimization, since (α, s, b) is fixed (to some (αj , sj , bj)), the image f can be warped prior to optimization by τ( . ;αj , sj , bj). This allows optimization w.r.t. k to be performed for τ being the identity transform (because the effect of (αj , sj , bj) is already taken care of by the warp) 7, i.e. (α = 0, s = 0, b = 0). Denoting the warped f due to (αj , sj , bj) by fj , f ◦ τθj , the inner optimization simplifies,\nmax k ∫ 2π 0 ∫ Xk ∫ R2 k̃σr (∠∇fj(y)− β)‖∇fj(y)‖ kσd(y − x) dy︸ ︷︷ ︸\nhSIFT (β,x ; fj)\nh(β,x ; pk) dx dβ ,(21)\n6We will later show in Section 7.2 that by a different parameterization of the geometric transform, we can handle a larger class, namely the affine transform, and yet are able to derive a closed form expression for the convolution integrals.\n7In this section we do not consider the full optimization loop (shrinking σ). However, if we wanted to do so, the idea of 1.Gradual reduction of the blur σ and 2.Warping by the current estimate of the geometric transform in each iteration, would lead to a LucasKanade[Lucas and Kanade, 1981] type algorithm. However, the resulted algorithm performs gradient density matching instead of Lucas-Kanade that relies on pixel intensity matching.\nPart of the computation involving fj is independent of pk and can be precomputed. This precomputed result in fact provides a new representation for fj that matches the definition of hSIFT in (2). This gives the algorithm presented in Table 5.2-Right."
    }, {
      "heading" : "6 Deriving DSP-SIFT via the Diffusion Theory",
      "text" : "Here we show that the DSP-SIFT descriptor also relates to partial diffusion of the cost function. Specifically, this descriptor can be derived by considering the diffusion w.r.t. the transformation parameters (α, s, b). Note that this involves more of the optimization variables in diffusion compared to SIFT (which diffuses w.r.t. (α, b)), and thus provides a better approximation to the theory of [Mobahi and Fisher III, 2015a] which suggests the diffusion must be applied to all optimization variables, i.e. to (c,θ). This improvement in approximation fidelity could be an explanation of why DSP-SIFT works better than SIFT in practice. However, it still misses diffusion of c.\nThe derivation is quite similar to that of SIFT in Section 5. The is to minimize the same energy function in (17). However, on top of diffusion w.r.t. variables (α, b) we add a Gaussian convolution in s. By linearity of convolution, we can just take the diffused energy we obtained in (20) and put it under the Gaussian convolution in s. This leads to the following expression,\nmax (α,s,b)∈Θ max k ∫ 2π 0 ∫ Xk ∫ R2 [ ( e . k̃σr (∠∇f(y)− α− β)‖∇f(y)‖ kσd(y − e .Rαx− b))\n? kσs ](s) dy h(β,x ; pk) dx dβ . (22)"
    }, {
      "heading" : "7 Implication for Future Algorithms",
      "text" : ""
    }, {
      "heading" : "7.1 Closed Form Approximations for Domain Size Pooling",
      "text" : "In DSP-SIFT, pooling over the scale is done numerically via sampling [Dong and Soatto, 2015]. Our theory suggests that the scale pooling should also be performed by Gaussian convolution, i.e. (22). Using this form, we present a closed-form approximation, which consequently does not require any sampling. Whether or not this approximation provides a satisfactory fidelity must be investigated by experiments.\nRecall energy minimization formulation of DSP-SIFT (22) has the following form,\nmax (α,s,b)∈Θ max k ∫ 2π 0 ∫ Xk ∫ R2 ( [ ( e . kσd(y − e .Rαx− b) ) ? kσs ](s) ) k̃σr (∠∇f(y)− α− β)‖∇f(y)‖ dy h(β,x ; pk) dx dβ .(23)\nThis convolution does not have a closed form. However, we consider approximating es by its linearized form around s = 0 (identity scaling transform), i.e. es ≈ 1 + s. Then the convolution will have a closed form. Below we present two approximation based on this idea.\nLinearizing only the inner es :\nProposition 2 [ ( e . kσd(y − (1 + . )Rαx− b) ) ? kσs ](s)\n= kσd(y −Rαx− b)\n×k−1σd ‖x‖\n(1 + (Rαx) T (b− y)− σ2d ‖x‖2 )× k√ σ2s+ σ2 d\n‖x‖2\n(s+ 1 + (Rαx) T (b− y)− σ2d ‖x‖2 ) .\nProof See Appendix A for the proof.\nIn particular, when the region is already warped, we can set (α, s, b) = (0, 0,0). This allows the template matching solution (23) as below,\nmax j,k ∫ 2π 0 ∫ Xk ∫ R2 kσd(y − x)× k −1 σd ‖x‖ (1− x Ty + σ2d ‖x‖2 )× k√ σ2s+ σ2 d\n‖x‖2\n(1− x Ty + σ2d ‖x‖2 )\n× k̃σr (∠∇fj(y)− β)‖∇fj(y)‖ dy h(β,x ; pk) dx dβ . (24)\nLinearizing both the inner and outer es : We use the following identity,\n[(1 + . )kσ(y + (1 + . )x) ? kscale](s) (25)\n= σ2(1 + s)− σscalexTy 2πσ(σ2 + σ2scale‖x‖2) 3 2 e −σ\n2‖(1+s)x+y‖2+σ2scale(x T y⊥)2\n2σ2(σ2+σ2 scale ‖x‖2) (26)\nThus,\n[(1 + . )kσd(y − b− (1 + . )Rα x) ? kσs ](s) (27)\n= σ2d(1 + s) + σsx TRTα(y − b) 2πσd(σ2d + σ 2 s‖x‖2) 3 2 e −σ\n2 d‖−(1+s)Rαx+y−b‖ 2+σ2s((Rαx) T (y−b)⊥)2\n2σ2 d (σ2 d +σ2s‖x‖ 2) .(28)\nIn particular, when the region is already warped, we can set (α, s, b) = (0, 0,0). This allows the template matching solution (23) as below,\nmax j,k ∫ 2π 0 ∫ Xk ∫ R2 σ2d + σsx Ty (σ2d + σ 2 s‖x‖2) 3 2 e −σ 2 d‖y−x‖ 2+σ2s(x T y⊥)2 2σ2 d (σ2 d +σ2s‖x‖ 2) (29)\n× k̃σr (∠∇fj(y)− β)‖∇fj(y)‖ dy h(β,x ; pk) dx dβ .(30)"
    }, {
      "heading" : "7.2 Exact Diffusion for Affine Transform",
      "text" : "Using the diffusion theory, by using a different parameterization for the geometric transformation, we can potentially improved SIFT and DSP-SIFT in two ways.\n1. We can extend the descriptor from handling similarity transform to affine transform.\n2. Recall that the computation of the diffusion in SIFT and DSP-SIFT relies on some approximation. In addition, regardless of the diffusion theory, DSP-SIFT involves sampling to approximate one of the required integrals. The finite sampling process is inaccurate and expensive to compute. Here despite working with a larger transformation space, the new parameterization allows deriving exact and closed form expression for the diffusion in all transformation parameters.\nThe formulation of the energy function is similar to that of SIFT and DSPSIFT, except the parameterization. Instead of the similarity transform τ as τ(x ; α, s, b) , esRαx+b we switch to the affine transform τ(x ; A, b) , Ax+b, as listed below,\ncost(c,A, b) , q(c) + ∑ k ck d(f ◦ τA,b, pk,Xk) , (31)\nwhere the dissimilarity functional d is defined as earlier in (15). Recall that the goal is to tackle the nonconvex problem (31) using the diffusion theory of [Mobahi and Fisher III, 2015a] and similar simplifications as in Section 5, we obtain the following solution for the template matching problem.\nk∗ = argmin k\n[ [d(f ◦ τ( . , . ), pk,Xk) ? kσb ](b) ? k̃σa ] (A) (32)\n= argmax k ∫ 2π 0 ∫ Xk ([ [h(β,x ; f ◦ τ( . , . )) ? kσb ](b) ? k̃σa ] (A) ) h(β,x ; pk) dβ dx .\nInteresting, we can replace the above convolutions by a closed form and exact expression. This is stated in the following lemma.\nLemma 3[( [h(β,x ; f ◦ τ( . , . )) ? kσb ](b) ) ? kσa ] (A)\n= e − ((b−y)\nT ∇̃f(y))2 2σ2 b − ‖A T ∇̃f(y)‖2 2σa2 w(−σ −2 b ∇̃\nT f(y)(y−b)xT ṽ(β,y)+σa−2∇̃T f(y)Aṽ(β,y) 2t )\n8 √ 2π 3 2σbσa2 ‖∇f(y)‖2 t3\n,\nwhere ∇̃f(y) , ∇f(y)‖∇f(y)‖ , ṽ(β,y) , v(β) ‖∇f(y)‖ , and t ,\n√ (xT ṽ(β,y))2\n2σ2b + 12σa2 ‖∇f(y)‖2\nand w(x) , √ πex 2 (1 + 2x2) erfc(x)− 2x.\nProof See Appendix C for the proof.\nSimilar to the arguments about SIFT solution in Section 5, the inner optimization in (32) can work with the warped f so that the transformation τ(x ; A, b) simplifies to the identity transform (A = I, b = 0). Letting the warped f be fj , f ◦ τAj ,bj , the inner optimization simplifies,\nwhere hheat is defined as the result in lemma 2 (diffused h) with (A = I, b = 0), σa and σb fixed, and all constants dropped,\n(j∗, k∗) , argmax j,k ∫ 2π 0 ∫ Xk hheat(β,x ; fj)h(β,x ; pk) dx dβ\nhheat(β,x ; f) , e − (y\nT ∇̃f(y))2\n2σ2 b w ( − 12t ∇̃ T f(y) (σ−2b yx T + σa −2 I ) ṽ(β,y) ) ‖∇f(y)‖2 t3\n∇̃f(y) , ∇f(y) ‖∇f(y)‖\nṽ(β,y) , v(β)\n‖∇f(y)‖\nt ,\n√ (xT ṽ(β,y))2\n2σ2b +\n1\n2σa2 ‖∇f(y)‖2\nw(x) , √ πex 2 (1 + 2x2) erfc(x)− 2x ."
    }, {
      "heading" : "A Proof of Proposition 1",
      "text" : "We proceed with the following identity8,\nes × kσ(y − (1 + s)Rαx− b) (35)\n= kσ(y −Rαx− b)× k−1σ ‖Rαx‖\n(1 + (Rαx)\nT (b− y)− σ2 ‖Rαx‖2 )× k σ ‖Rαx‖ (s+ 1 + (Rαx) T (b− y)− σ2 ‖Rαx‖2 )(36\n= kσ(y −Rαx− b)× k−1σ ‖x‖\n(1 + (Rαx)\nT (b− y)− σ2\n‖x‖2 )× k σ ‖x‖ (s+ 1 +\n(Rαx) T (b− y)− σ2\n‖x‖2 ) (37)\n(38)\nIn this form, it is now very easy to compute convolution with kσscale(s),\n[e . × kσ(y − (1 + . )Rαx− b) ? kσscale ](s) (39)\n= kσ(y −Rαx− b)× k−1σ ‖x‖\n(1 + (Rαx)\nT (b− y)− σ2 ‖x‖2 )× k√\nσ2scale+ σ2\n‖x‖2\n(s+ 1 + (Rαx)\nT (b− y)− σ2\n‖x‖2 )(40\n(41)\nTherefore,\n[ ( e . ∫ R2 k̃σ̃(∠∇f(y)− α− β)‖∇f(y)‖ kσ(y − (1 + . )Rαx− b) dy ) ? kσscale ](s) (42)\n= ∫ R2 k̃σ̃(∠∇f(y)− α− β)‖∇f(y)‖ kσ(y −Rαx− b) (43)\n×k−1σ ‖x‖\n(1 + (Rαx)\nT (b− y)− σ2 ‖x‖2 )× k√\nσ2scale+ σ2\n‖x‖2\n(s+ 1 + (Rαx)\nT (b− y)− σ2\n‖x‖2 ) dy .\n8 We essentially have es×kσ(y+(1+s)x) which by completing the square of the exponent w.r.t. s can be expressed as below,\nes × kσ(y + (1 + s)x) (33)\n= kσ(x + y)× k−1σ ‖x‖\n(1 + xTy − σ2\n‖x‖2 )× k σ ‖x‖ (s+ 1 +\nxTy − σ2\n‖x‖2 ) , (34)\nwhere the first k is 2D, and the next two k’s are 1D.\nB Proof of Lemma 1\ncost(c,θ) (44) , q(c) + ∑ k ck d(f ◦ τ(α ,s, b ), pk,Xk) (45)\n= q(c) + ∑ k ck ∫ 2π 0 ( h(β ; f ◦ τ(α,s, b ),Xk)− h(β ; pk,Xk) )2 dβ . (46)\nNote that,\nh(β ; f ◦ τ(α,s, b ),Xk) (47)\n= ∫ Xk X(∠∇(f(esRαx+ b))− β)‖∇f(esRαx+ b)‖ dx (48)\n= ∫ Xk X(∠esRTα [∇f ](esRαx+ b)− β)‖esR T α∇f(esRαx+ b)‖ dx (49)\n= ∫ Xk X(∠∇f(esRαx+ b)− α− β)es‖∇f(esRαx+ b)‖ dx , (50)\nwhere (49) uses the chain rule of derivative ∇ ( f(Ax+b) ) = AT ( [∇f ](Ax+\nb) ) . Also, for any a > 0, (50) uses the identities ‖aRx‖ = a‖x‖ and ∠aRαx = α+ ∠x. Thus, it follows that,\ncost(c,θ) (51) = q(c) + ∑ k ck ∫ 2π 0 (∫ Xk X(∠∇f(esRαx+ b)− α− β)es‖∇f(esRαx+ b)‖ dx− h(β ; pk,Xk) )2 dβ .(52)\nBy the linearity of the convolution operator and the unity of Gaussian’s total mass, smoothed cost amounts only to replacing ( ∫ Xk X(∠∇f(e sRαx+b)−α−\nβ)es‖∇f(esRαx+ b)‖ dx−h(β ; pk,Xk) )2 by its smoothed version. Expansion\nof the quadratic form yields,\n(∫ Xk X(∠∇f(esRαx+ b)− α− β)es‖∇f(esRαx+ b)‖ dx− h(β ; pk,Xk) )2 (53)\n= (∫ Xk X(∠∇f(esRαx+ b)− α− β)es‖∇f(esRαx+ b)‖ dx )2 + h2(β ; pk,Xk)(54)\n−2 (∫ Xk X(∠∇f(esRαx+ b)− α− β)es‖∇f(esRαx+ b)‖ dx ) × h(β ; pk,Xk) .(55)\nThe first term can be rewritten as below,\n(∫ Xk X(∠∇f(esRαx+ b)− α− β)es‖∇f(esRαx+ b)‖ dx )2 (56)\n= e2s ∫ Xk |{x2 |∠∇f(esRαx2 + b) = ∠∇f(esRαx+ b)}| (57)\n×X(∠∇f(esRαx+ b)− α− β) ‖∇f(esRαx+ b)‖2 dx . (58)\nNote that |{x2 |∠∇f(esRαx2 + b) = ∠∇f(esRαx + b)}| ≥ 1 because at least there is one such x2 for which ∠∇f(esRαx2 + b) = ∠∇f(esRαx + b) holds, that is x2 = x. However, we assume that the cardinality of the set is exactly one, i.e. besides x2 = x, there is no other choice for x2 so that the condition ∠∇f(esRαx2 +b) = ∠∇f(esRαx+b) can hold. The rationale is that the variables are continuous and thus their representation has infinite precision. The odds that the gradient orientation at two different points in the image are exactly the same is almost impossible, although they might be very close. With this assumption, the quadratic form simplifies as below,\n(∫ Xk X(∠∇f(esRαx+ b)− α− β)es‖∇f(esRαx+ b)‖ dx− h(β ; pk,Xk) )2\n(59) = e2s ∫ Xk X(∠∇f(esRαx+ b)− α− β) ‖∇f(esRαx+ b)‖2 dx+ h2(β ; pk,Xk) (60)\n−2es (∫ Xk X(∠∇f(esRαx+ b)− α− β)‖∇f(esRαx+ b)‖ dx ) × h(β ; pk,Xk) (61)\n= e2s ∫ Xk ∫ R2 X(∠∇f(y)− α− β) ‖∇f(y)‖2 δ(y − esRαx− b) dy dx+ h2(β ; pk,Xk)(62)\n−2es (∫ Xk ∫ R2 X(∠∇f(y)− α− β)‖∇f(y)‖ δ(y − esRαx− b) dy dx ) × h(β ; pk,Xk) ,(63)\nwhere (62) and (63) use the sifting property of the delta function. The goal is to convolve cost with a multivariate Gaussian kernel of covariance σ2I in variables jointly in (α, b). Due to the diagonal form of the covariance, the convolution can be decoupled to that of α and b.\nWe first proceed with smoothing w.r.t. b. By linearity of the convolution operator and that the Gaussian kernel integrates to one, we obtained the following,\n[cost(c, α, s, . ) ? kσ](b) (64) = q(c) + ∑ k ck ∫ 2π 0 [ (∫ Xk X(∠[∇f ](esRαx+ . )− α− β)es‖[∇f ](esRαx+ . )‖ dx− h(β ; pk,Xk) )2 ? kσ](b) dβ(65)\n= q(c) + ∑ k ck ∫ 2π 0 ( (66)\ne2s ∫ Xk ∫ R2 X(∠∇f(y)− α− β) ‖∇f(y)‖2 kσ(y − esRαx− b) dy dx+ h2(β ; pk,Xk) (67)\n−2es (∫ Xk ∫ R2 X(∠∇f(y)− α− β)‖∇f(y)‖ kσ(y − esRαx− b) dy dx ) × h(β ; pk,Xk) ) dβ . (68)\nWe now continue by trying to smooth w.r.t. α.\n[ [ cost(c, . , s, . ) ? kσ ] (b) ? kσ̃ ] (α) (69) = q(c) + ∑ k ck ∫ 2π 0 ( (70)\ne2s ∫ Xk ∫ R2 ‖∇f(y)‖2 ( [X(∠∇f(y)− . − β)kσ(y − esR .x− b) ? kσ̃](α) ) dy dx+ h2(β ; pk,Xk) (71)\n−2es (∫ Xk ∫ R2 ‖∇f(y)‖ ( [X(∠∇f(y)− . − β)kσ(y − esR .x− b) ? kσ̃](α) ) dy dx ) × h(β ; pk,Xk) ) dβ .(72)\nComputation of the convolution X(∠∇f(y)− . +β)kσ(y−esR .x−b)?kσ̃ is intractable. However, it can be approximated by applying the convolution only to the X function. The rationale behind this approximation is that Gaussian convolution affects delta function much more than the Gaussian factor9.\n[X(∠∇f(y)− . − β)kσ(y − esR .x− b) ? kσ̃](α) (73) ≈ ( [X(∠∇f(y)− . − β) ? kσ̃](α) ) kσ(y − esRαx− b) (74)\n= k̃σ̃(∠∇f(y)− α− β)kσ(y − esRαx− b) . (75)\nUsing this approximation, it follows that,\n9Gaussian smoothing affects high frequency functions more than low frequency ones; essentially it kills high frequency components, while leaving low frequency components intact.\n[ [ cost(c, . , s, . ) ? kσ ] (b) ? kσ̃ ] (α) (76) ≈ q(c) + ∑ k ck ∫ 2π 0 ( (77)\ne2s ∫ Xk ∫ R2 k̃σ̃(∠∇f(y)− α− β) ‖∇f(y)‖2 kσ(y − esRαx− b) dy dx+ h2(β ; pk,Xk) (78)\n−2es (∫ Xk ∫ R2 k̃σ̃(∠∇f(y)− α− β)‖∇f(y)‖ kσ(y − esRαx− b) dy dx ) × h(β ; pk,Xk) ) dβ .(79)"
    }, {
      "heading" : "C Proof of Lemma 2",
      "text" : "1. revert to previous proof with Z, just for the propsotiion. Do poper variable replacement in the proof.\nUse proposition. Justify why Df having no zero component makes sense. Mention integration w.r.t y is over X ∩Xk. We assume that this integral is\nzero outside of the domain R2 −X. 10\nProposition 4\nδ(r v(β)−AT∇f(y)) kσ(y −Ax− b) (80)\n= kσ( xTz − (y − b)T∇f(y)\n‖∇f(y)‖ ) k√ σ2+σ†2 ‖x‖2( (∇f(y))T (Ax+ b− y)⊥ ‖∇f(y)‖ ) kσ† ‖∇f(y)‖(z −AT∇f(y)) .(81)\n∇f(y) ↔ g (82) −z ↔ c (83)\nb− y ↔ d (84) (85)\nProposition 5 [ ( δ( . Tg + c) kσ( . x+ d) ) ? kσ† ](A) (86)\n= kσ( gTd− xT c ‖g‖ ) k√ σ2+σ†2 ‖x‖2( gT (Ax+ d)⊥ ‖g‖ ) kσ† ‖g‖(A Tg + c) .(87)\nProof We first provide an outline of the proof. The δ function can be replaced by the limit of a Gaussian whose variance tends to zero lim →0 k . Now k and kσ† form the product of Gaussians. The idea is to write this product as a new single Gaussian in A (because then we know how to convolve two Gaussians). We do this by replacing the pair with a single exponential whose exponent is trivially the sum of the original exponents. Using completing the square method for the joint exponent, the center and covariance of the single Gaussian emerges. There is a problem though; the resulted quadratic form will have a singular covariance whose inverse does not exist11. We tackle this problem by the change of coordinate system.\n10These conditions can be assumed as granted. The gradient is no where perfectly zero in the image. It is perfectly zero outside of the image f , but that can be taken care of by limiting the integration domain of y from R2 to X . Having x1 = 0 or x2 = 0 has zero measure, and can be removed from the integration w.r.t. x without affecting the integration result.\n11The inverse of the covariance is required as it directly appears in the definition of the Gaussian.\nWe begin with the coordinate system transform. Since the covariance of the Gaussian kernel is isotropic, the resulted Gaussian is radially symmetric, i.e. kσ(x) = kσ(Rx) for any rotation matrix R. Consequently, instead of directly smoothing the above expression, we can rotate the coordinate system, smooth in the latter system, and then invert the rotation to obtain the smoothed function in the original coordinate. In particular, we use the following rotation matrix,\nR , 1\n‖x‖ ‖g‖\n g2x2 sign ( g1x1 ) −g2 |x1| sign ( g1 ) −x2 |g1| sign(x1) |g1x1| −g1x2 sign ( g2x1 ) g1 |x1| sign ( g2 ) −x2 |g2| sign(x1) |g2x1| −g2x1 sign ( g1x2 ) −g2 |x2| sign ( g1 )\nx1 |g1| sign(x2) |g1x2| g1x1 sign ( g2x2 ) g1 |x2| sign ( g2 ) x1 |g2| sign(x2) |g2x2|  . (88)\nDue to the assumptions g1 6= 0, g2 6= 0, x1 6= 0, and x2 6= 0,R is well-defined. Let a , vec(A), i.e. a = (a11, a12, a21, a22), and let U , RA and u , vec(U). Changing the coordinate system from A to U leads to the following identity,\nk (A Tg + c) kσ(Ax+ d) (89)\n= k ( (R TU) T g + c) kσ(R TU x+ d) (90) = 1√ 2π e − 2‖x‖2(dT g⊥)2−σ2‖g‖2(‖g‖2‖d‖2+(2(−d)T g+xT c)(xT c)) 2σ2‖g‖2(σ2‖g‖2+ 2‖x‖2) (91)\n×e (g1(−d2)−g2(−d1))\n2\n2σ2‖g‖2 (92)\n× 1 ‖g‖ k ‖g‖ (u2 − −x1c2 + x2c1 ‖g‖ ‖x‖ sign(g2x1) ) (93) × 1 ‖x‖ k σ ‖x‖ (u3 − g1(−d2)− g2(−d1) ‖g‖ ‖x‖ sign(g1x2) ) (94) × 1√ σ2‖g‖2 + 2‖x‖2 k σ √ σ2‖g‖2 + 2‖x‖2 (u4 − 2‖x‖2(−d)Tg − σ2‖g‖2 xT c ‖g‖ ‖x‖ (σ2‖g‖2 + 2‖x‖2) sign(g2x2) ) .(95)\nThe value of the coordinate transformation is that we can now write this expression as the product of independent Gaussian kernels. Convolution of this expression with the isotropic kernel kσ†(u) is straightforward,\nδ(z −AT∇f(y)) kσ(y −Ax− b) (96)\n↔ 1√ 2π e 2‖x‖2(((y−b)T∇f(y))2−‖∇f(y)‖2‖y−b‖2)−σ2‖∇f(y)‖2(‖∇f(y)‖2‖y−b‖2−(2(y−b)T∇f(y)−xT z)(xT z)) 2σ2‖∇f(y)‖2(σ2‖∇f(y)‖2+ 2‖x‖2) (97)\n×e (f1(y)(y2−b2)−f2(y)(y1−b1))\n2\n2σ2‖∇f(y)‖2 (98)\n× 1 ‖∇f(y)‖ k√ σ†2+\n2\n‖∇f(y)‖2\n(u2 − x1z2 − x2z1\n‖∇f(y)‖ ‖x‖ sign(f2(y)x1) ) (99)\n× 1 ‖x‖ k√ σ†2+ σ 2\n‖x‖2\n(u3 − f1(y)(y2 − b2)− f2(y)(y1 − b1) ‖∇f(y)‖ ‖x‖ sign(f1(y)x2) ) (100)\n× 1√ σ2‖∇f(y)‖2 + 2‖x‖2 k√ σ†2+ σ 2 2\nσ2‖∇f(y)‖2 + 2‖x‖2\n(u4 − 2‖x‖2(y − b)T∇f(y) + σ2‖∇f(y)‖2 xTz\n‖∇f(y)‖ ‖x‖ (σ2‖∇f(y)‖2 + 2‖x‖2) sign(f2(y)x2) ) .(101)\nSetting → 0, and given that σ > 0 and ‖∇f(y)‖ 6= 0, it follows thats,\nδ(z −AT∇f(y)) kσ(y −Ax− b) (102)\n↔ 1√ 2π e − ((b−y)\nT∇f(y)+xT z)2\n2σ2‖∇f(y)‖2 (103)\n× 1 ‖∇f(y)‖ kσ†(u2 − x1z2 − x2z1 ‖∇f(y)‖ ‖x‖ sign(f2(y)x1) ) (104) × 1 ‖x‖ k√ σ†2+ σ 2\n‖x‖2\n(u3 − f1(y)(y2 − b2)− f2(y)(y1 − b1) ‖∇f(y)‖ ‖x‖ sign(f1(y)x2) ) (105)\n× 1 σ‖∇f(y)‖ kσ†(u4 − xTz ‖∇f(y)‖ ‖x‖ sign(f2(y)x2) ) . (106)\nBy inverting the coordinate system from u to (a11, a12, a21, a22) we obtain,\nkσ( xTz − (y − b)T∇f(y)\n‖∇f(y)‖ ) k√ σ2+σ†2 ‖x‖2( (∇f(y))T (Ax+ b− y)⊥ ‖∇f(y)‖ ) kσ† ‖∇f(y)‖(z−AT∇f(y)) .\n(107) As a sanity check, we can see that the above expression becomes the same\nas the original non-smoothed function when σ† → 0,\nlim σ†→0\nkσ( xTz − (y − b)T∇f(y)\n‖∇f(y)‖ ) k√ σ2+σ†2 ‖x‖2( (∇f(y))T (Ax+ b− y)⊥ ‖∇f(y)‖ ) kσ† ‖∇f(y)‖(z −AT∇f(y))(108\n= kσ( xTz − (y − b)T∇f(y)\n‖∇f(y)‖ ) kσ(\n(∇f(y))T (Ax+ b− y)⊥\n‖∇f(y)‖ ) δ(z −AT∇f(y)) (109)\n= kσ( xT (AT∇f(y))− (y − b)T∇f(y)\n‖∇f(y)‖ ) kσ(\n(∇f(y))T (Ax+ b− y)⊥\n‖∇f(y)‖ ) δ(z −AT∇f(y)) (110)\n= kσ,1( (∇f(y))T (Ax+ b− y)\n‖∇f(y)‖ ) kσ,1(\n(∇f(y))T (Ax+ b− y)⊥\n‖∇f(y)‖ ) δ(z −AT∇f(y)) (111)\n= kσ,2 ( 1 ‖∇f(y)‖ ( (∇f(y))T (Ax+ b− y) , (∇f(y))T (Ax+ b− y)⊥ )) δ(z −AT∇f(y)) (112)\n= kσ,2 ( 1 ‖∇f(y)‖ ‖∇f(y)‖ (Ax+ b− y) ) δ(z −AT∇f(y)) (113)\n= kσ(Ax+ b− y) δ(z −AT∇f(y)) (114) (115)\nThe goal is to convolve d(f ◦ τ(A,b), pk,Xk) with the Gaussian kernel. By linearity of the convolution operator we obtain,\n[( [d(f ◦ τ( . , . ), pk,Xk) ? kσb ](b) ) ? kσa ] (A) (116)\n, [( [− ∫ Xk ∫ 2π 0 h(β,x ; f ◦ τ( . , . ))× h(β,x ; pk) dβ dx ? kσb ](b) ) ? kσa ] (A) dx(117)\n= − ∫ Xk ∫ 2π 0 ([( [h(β,x ; f ◦ τ( . , . )) ? kσb ](b) ) ? kσa ] (A) ) × h(β,x ; pk) dβ dx .(118)\nThus in the following we focus on h(β,x ; f ◦ τ( . , . )) ? kσb ? kσa . We first manipulate h(β,x ; f ◦ τ(A,b)) by applying the chain rule of derivate ∇ ( f(Ax+\nb) ) = AT ( [∇f ](Ax+ b) ) followed by the sifting property of the delta function,\nh(β,x ; f ◦ τ(A,b)) (119) , X(β − ∠∇(f(Ax+ b)))‖∇f(Ax+ b)‖ (120) = X(β − ∠AT [∇f ](Ax+ b))‖AT∇f(Ax+ b)‖ (121)\n= ∫ R2 X(β − ∠AT∇f(y))‖AT∇f(y)‖ δ(y −Ax− b) dy . (122)\nComputing the inner convolution, i.e. w.r.t. b, is straightforward,\n[h(β,x ; f ◦ τ(A, . )) ? kσb ](b) (123) = [ (∫\nR2 X(β − ∠AT∇f(y))‖AT∇f(y)‖ δ(y −Ax− b) dy\n) ? kσb ](b)(124)\n= ∫ R2 X(β − ∠AT∇f(y))‖AT∇f(y)‖ kσb(y −Ax− b) dy . (125)\nThe latter can be expressed by the sifting property of the delta function as below,\n[h(β,x ; f ◦ τ(A, . )) ? kσb ](b) (126)\n= ∫ R2 ∫ R2 δ ( z −AT∇f(y) ) X(β − ∠z)‖z‖ kσb(y −Ax− b) dz dy .(127)\nWe now apply a change of variable to move from the Cartesian coordinate (z1, z2) to the polar coordinate (r, φ) such that (z1, z2) = (r cos(φ), r sin(φ)).\nThis results in replacing ∫ R2 f(z1, z2) dz1 dz2 by ∫∞ 0 ∫ 2π 0 r f(r v(φ)) dφ dr, where v(φ) , (cos(φ), sin(φ)).\n[h(β,x ; f ◦ τ(A, . )) ? kσb ](b) (128)\n= ∫ R2 ∫ ∞ 0 ∫ 2π 0 r δ ( rv(φ)−AT∇f(y) ) X(β − ∠rv(φ))‖rv(φ)‖ kσb(y −Ax− b) dφ dr dy(129)\n= ∫ R2 ∫ ∞ 0 ∫ 2π 0 r δ ( rv(φ)−AT∇f(y) ) X(β − ∠v(φ)) r kσb(y −Ax− b) dφ dr dy (130)\n= ∫ R2 ∫ ∞ 0 r2 δ ( rv(β)−AT∇f(y) ) kσb(y −Ax− b) dr dy . (131)\nWe are now ready to smooth this form w.r.t. A. That is, we want to compute convolution of this expression with a multivariate Gaussian in (a11, a12, a21, a22) of covariance σ2I.\nUsing this result, we can continue as below,\n[[cost(c, . , . ) ? kσ](b) ? kσ† ](A) (132) = q(c) + ∑ k ck ∫ 2π 0 ( (133)∫\nXk ∫ R2 ∫ R2 δ(vT (β)z) ‖z‖2 [ ( δ(z − . T∇f(y)) kσ(y − .x− b) ) ? kσ† ](A) dz dy dx+ h 2(β ; pk,Xk) (134)\n−2 (∫ Xk ∫ R2 ∫ R2 δ(vT (β)z) ‖z‖ [ ( δ(z − . T∇f(y)) kσ(y − .x− b) ) ? kσ† ](A) dz dy dx ) × h(β ; pk,Xk) ) dβ(135)\n= q(c) + ∑ k ck ∫ 2π 0 ( (136)∫\nXk ∫ R2 ∫ R2 δ(vT (β)z) ‖z‖2 kσ( xTz − (y − b)T∇f(y) ‖∇f(y)‖ ) kσ† ‖∇f(y)‖(z −AT∇f(y)) dz (137)\n×k√ σ2+σ†2 ‖x‖2(\n(∇f(y))T (Ax+ b− y)⊥\n‖∇f(y)‖ ) dy dx+ h2(β ; pk,Xk) (138)\n−2 (∫ Xk ∫ R2 ∫ R2 δ(vT (β)z) ‖z‖ kσ( xTz − (y − b)T∇f(y) ‖∇f(y)‖ ) kσ† ‖∇f(y)‖(z −AT∇f(y)) dz (139)\n×k√ σ2+σ†2 ‖x‖2(\n(∇f(y))T (Ax+ b− y)⊥\n‖∇f(y)‖ ) dy dx\n) × h(β ; pk,Xk) ) dβ . (140)\nWe now apply a change of variable to move from the Cartesian coordinate (z1, z2) to the polar coordinate (r, φ) such that (z1, z2) = (r cos(φ), r sin(φ)).\nThis transforms the form ∫ R2 f(z1, z2) dz1 dz2 to ∫∞ 0 ∫ 2π 0 r f(r cos(φ), r sin(φ)) dφ dr.\n∫ R2 X(β − ∠z) ‖z‖ kσ( xTz − (y − b)T∇f(y) ‖∇f(y)‖ ) kσ† ‖∇f(y)‖(z −AT∇f(y)) dz (141)\n= ∫ ∞ 0 ∫ 2π 0 rX(β − φ) r kσ( rxTv(φ)− (y − b)T∇f(y) ‖∇f(y)‖ ) kσ† ‖∇f(y)‖(rv(φ)−AT∇f(y)) dφ dr(142)\n= ∫ ∞ 0 r2 kσ(r xTv(β) ‖∇f(y)‖ + (b− y)T∇f(y) ‖∇f(y)‖ ) kσ† ‖∇f(y)‖(rv(β)−AT∇f(y)) dr (143) = e − ((b−y) T ∇̃f(y))2 2σ2 − ‖A T ∇̃f(y)‖2 2σ†2 w(−σ −2∇̃T f(y)(y−b)xT ṽ(β,y)+σ†−2∇̃T f(y)Aṽ(β,y) 2t )\n8 √ 2π 3 2σσ† 2 ‖∇f(y)‖2 t3 , (144)\nwhere ∇̃f(y) , ∇f(y)‖∇f(y)‖ , ṽ(β,y) , v(β) ‖∇f(y)‖ , and t ,\n√ (xT ṽ(β,y))2\n2σ2 + 1 2σ†2 ‖∇f(y)‖2\nand w(x) , √ πex 2\n(1 + 2x2) erfc(x) − 2x. In (144) we use an elementary identity12.\n12 We use the identity,\n∫ ∞ 0 r2 kσ1,1(rc1 + c2)kσ2,2(rc3 + c4) dr = e − c2 2 2σ21 − ‖c4‖ 2 2σ22 ( √ π(1 + 2t22)e t22 erfc(t2)− 2t2) 8 √\n2π 3 2 σ1σ22t 3 1\n,\n(145)\nfor t1 ,\n√ c12\n2σ21 + ‖c3‖2 2σ22 and t2 ,\nc1c2 s12\n+ cT3 c4\nσ22 2t1 . This identity is derived in two steps:\n1. Completing the square of the exponent in the integrand.\n− (rc1 + c2)2 2σ21 − ‖rc3 + c4‖2 2σ22 = − 1 2 (r+ cT3 c4σ 2 1 + c1c2σ 2 2 ‖c3‖2σ21 + c21σ22 )2( c21 σ21 + ‖c3‖2 σ22 )+ 1 2 ( (c1c2σ22 + σ 2 1c T 3 c4) 2 ‖c3‖2σ41σ22 + c21σ21σ42 − c22 σ21 − ‖c4‖2 σ22 ) .\n(146)\n2. Using the identity about Gaussian moments,\n∫ ∞ 0 r2e − (r−a1) 2 2a22 dr = a1a 2 2e − a21 2a22 + √ π 2 a2(a 2 1 + a 2 2)(1 + erf( a1√ 2a2 )) . (147)"
    } ],
    "references" : [ {
      "title" : "Multi-view feature engineering and learning",
      "author" : [ "Dong et al", "J. 2015] Dong", "N. Karianakis", "D. Davis", "J. Hernandez", "J. Balzer", "S. Soatto" ],
      "venue" : null,
      "citeRegEx" : "al. et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 2015
    }, {
      "title" : "Domain-size pooling in local descriptors: Dsp-sift",
      "author" : [ "Dong", "Soatto", "J. 2015] Dong", "S. Soatto" ],
      "venue" : "In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
      "citeRegEx" : "Dong et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Dong et al\\.",
      "year" : 2015
    }, {
      "title" : "An iterative image registration technique with an application to stereo vision",
      "author" : [ "Lucas", "Kanade", "B.D. 1981] Lucas", "T. Kanade" ],
      "venue" : "In Proceedings of the 7th International Joint Conference on Artificial Intelligence Volume 2,",
      "citeRegEx" : "Lucas et al\\.,? \\Q1981\\E",
      "shortCiteRegEx" : "Lucas et al\\.",
      "year" : 1981
    }, {
      "title" : "Distribution fields with adaptive kernels for large displacement image alignment",
      "author" : [ "Mears et al", "B. 2013] Mears", "L. Sevilla-Lara", "E.G. Learned-Miller" ],
      "venue" : "In British Machine Vision Conference,",
      "citeRegEx" : "al. et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "al. et al\\.",
      "year" : 2013
    }, {
      "title" : "On the Link between Gaussian Homotopy Continuation and Convex Envelopes",
      "author" : [ "Mobahi", "Fisher III", "H. 2015a] Mobahi", "J.W. Fisher III" ],
      "venue" : "Energy Minimization Methods in Computer Vision and Pattern Recognition,",
      "citeRegEx" : "Mobahi et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Mobahi et al\\.",
      "year" : 2015
    }, {
      "title" : "A theoretical analysis of optimization by gaussian continuation",
      "author" : [ "Mobahi", "Fisher III", "H. 2015b] Mobahi", "J.W. Fisher III" ],
      "venue" : "In TwentyNinth AAAI Conference on Artificial Intelligence",
      "citeRegEx" : "Mobahi et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Mobahi et al\\.",
      "year" : 2015
    }, {
      "title" : "Vlfeat: An open and portable library of computer vision algorithms",
      "author" : [ "Vedaldi", "Fulkerson", "A. 2010] Vedaldi", "B. Fulkerson" ],
      "venue" : "In Proceedings of the International Conference on Multimedia, MM",
      "citeRegEx" : "Vedaldi et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Vedaldi et al\\.",
      "year" : 2010
    } ],
    "referenceMentions" : [ {
      "referenceID" : 1,
      "context" : "By observing that the above descriptor is pooling (weighted averaging) across displacement, [Dong et al., 2015] adds domain size pooling to this construction and suggests,",
      "startOffset" : 92,
      "endOffset" : 111
    }, {
      "referenceID" : 1,
      "context" : "The domain-size integration (3) is approximated by numerical sampling in [Dong et al., 2015], which is slow.",
      "startOffset" : 73,
      "endOffset" : 92
    } ],
    "year" : 2016,
    "abstractText" : "Why has SIFT been so successful? Why its extension, DSP-SIFT, can further improve SIFT? Is there a theory that can explain both? How can such theory benefit real applications? Can it suggest new algorithms with reduced computational complexity or new descriptors with better accuracy for matching? We construct a general theory of local descriptors for visual matching. Our theory relies on concepts in energy minimization and heat diffusion. We show that SIFT and DSP-SIFT approximate the solution the theory suggests. In particular, DSP-SIFT gives a better approximation to the theoretical solution; justifying why DSP-SIFT outperforms SIFT. Using the developed theory, we derive new descriptors that have fewer parameters and are potentially better in handling affine deformations.",
    "creator" : "LaTeX with hyperref package"
  }
}