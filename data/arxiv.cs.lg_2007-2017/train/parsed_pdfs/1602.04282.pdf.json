{
  "name" : "1602.04282.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Conservative Bandits",
    "authors" : [ "Yifan Wu", "Roshan Shariff", "Tor Lattimore", "Csaba Szepesvári" ],
    "emails" : [ "ywu12@ualberta.ca", "rshariff@ualberta.ca", "tlattimo@ualberta.ca", "szepesva@ualberta.ca" ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "The manager of Zonlex, a fictional company, has just learned about bandit algorithms and is very excited about the opportunity to use this advanced technology to maximize Zonlex’s revenue by optimizing the content on the landing page of the company’s website. Every click on the content of their website pays a small reward; thanks to the high traffic that Zonlex’s website enjoys, this translates into a decent revenue stream. Currently, Zonlex chooses the website’s contents using a strategy designed over the years by its best engineers, but the manager suspects that some\nalternative strategies could potentially extract significantly more revenue. The manager is willing to explore bandit algorithms to identify the winning strategy. The manager’s problem is that Zonlex cannot afford to lose more than 10% of its current revenue during its day-to-day operations and at any given point in time, as Zonlex needs a lot of cash to support its operations. The manager is aware that standard bandit algorithms experiment “wildly”, at least initially, and as such may initially lose too much revenue and jeopardize the company’s stable operations. As a result, the manager is afraid of deploying cutting-edge bandit methods, but notes that this just seems to be a chicken-and-egg problem: a learning algorithm cannot explore due to the potential high loss, whereas it must explore to be good in the long run.\nThe problem described in the previous paragraph is ubiquitous. It is present, for example, when attempting to learn better human-computer interaction strategies, say in dialogue systems or educational games. In these cases a designer may feel that experimenting with sub-par interaction strategies could cause more harm than good (e.g., Rieser and Lemon, 2008; Liu et al., 2014). Similarly, optimizing a production process in a factory via learning (and experimentation) has much potential (e.g., Gabel and Riedmiller, 2011), but deviating too much from established “best practices” will often be considered too dangerous. For examples from other domains see the survey paper of Garcı́a and Fernández (2015).\nStaying with Zonlex, the manager also knows that the standard practice in today’s internet companies is to employ A/B testing on an appropriately small percentage of the traffic for some period of time (e.g., 10% in the case of Zonlex). The manager even thinks that perhaps a best-arm identification strategy from the bandit literature, such as the recent lil’UCB method of Jamieson et al. (2014), could be more suitable. While this is appealing, identifying the best possible option may need too much time even with a good learning algorithm (e.g., this happens when the difference in payoff between the best and second best strate-\nar X\niv :1\n60 2.\n04 28\n2v 1\n[ st\nat .M\nL ]\n1 3\nFe b\n20 16\ngies is small). One can of course stop earlier, but then the potential for improvement is wasted: when to stop then becomes a delicate question on its own. As Zonlex only plans for the next five years anyway, they could adopt the more principled yet quite simple approach of first using their default favorite strategy until enough payoff is collected, so that in the time remaining of the five years the return-constraint is guaranteed to hold regardless of the future payoffs. While this is a solution, the manager suspects that other approaches may exist. One such potential approach is to discourage a given bandit algorithm from exploring the alternative options, while in some way encouraging its willingness to use the default option. In fact, this approach has been studied recently by Lattimore (2015a) (in a slightly more general setting than ours). However, the algorithm of Lattimore (2015a) cannot be guaranteed to maintain the return constraint uniformly in time. It is thus unsuitable for the conservative manager of Zonlex; a modification of the algorithm could possibly meet this stronger requirement, but it appears that this will substantially increase the worst-case regret.\nIn this paper we ask whether better approaches than the above naive one exist in the context of multi-armed bandits, and whether the existing approaches can achieve the best possible regret given the uniform constraint on the total return. In particular, our contributions are as follows: (i) Starting from multi-armed bandits, we first formulate what we call the family of “conservative bandit problems”. As expected in these problems, the goal is to design learning algorithms that minimize regret under the additional constraint that at any given point in time, the total reward (return) must stay above a fixed percentage of the return of a fixed default arm, i.e., the return constraint must hold uniformly in time. The variants differ in terms of how stringent the constraint is (i.e., should the constraint hold in expectation, or with high probability?), whether the bandit problem is stochastic or adversarial, and whether the default arm’s payoff is known before learning starts. (ii) We analyze the naive build-budget-then-learn strategy described above (which we call BudgetFirst) and design a significantly better alternative for stochastic bandits that switches between using the default arm and learning (using a version of UCB, a simple yet effective bandit learning algorithm: Agrawal, 1995; Katehakis and Robbins, 1995; Auer et al., 2002) in a “smoother” fashion. (iii) We prove that the new algorithm, which we call Conservative UCB, meets the uniform return constraint (in various senses), while it can achieve significantly less regret than BudgetFirst. In particular, while BudgetFirst is shown to pay a multiplicative penalty in the regret for maintaining the return constraint, Conservative UCB only pays an additive penalty. We provide both high probability and expectation bounds, consider both high probability and expectation constraints\non the return, and also consider the case when the payoff of the default arm is initially unknown. (iv) We also prove a lower bound on the best regret given the constraint and as a result show that the additive penalty is unavoidable; thus Conservative UCB achieves the optimal regret in a worst-case sense. While Unbalanced MOSS of Lattimore (2015a), when specialized to our setting, also achieves the optimal regret (as follows from the analysis of Lattimore, 2015a), as mentioned earlier it does not maintain the constraint uniformly in time (it will explore too much at the beginning of time); it also relies heavily on the knowledge of the mean payoff of the default strategy. (v) We also consider the adversarial setting where we design an algorithm similar to Conservative UCB: the algorithm uses an underlying “base” adversarial bandit strategy when it finds that the return so far is sufficiently higher than the minimum required return. We prove that the resulting method indeed maintains the return constraint uniformly in time and we also prove a high-probability bound on its regret. We find, however, that the additive penalty in this case is higher than in the stochastic case. Here, the Exp3-γ algorithm of Lattimore (2015a) is an alternative, but again, this algorithm is not able to maintain the return constraint uniformly in time. (vi) The theoretical analysis is complemented by synthetic experiments on simple bandit problems whose purpose is to validate that the newly designed algorithm is reasonable and to show that the algorithms’ behave as dictated by the theory developed. We also compare our method to Unbalanced MOSS to provide a perspective to see how much is lost due to maintaining the return constraint uniformly over time. We also identify future work. In particular, we expect our paper to inspire further works in related, more complex online learning problems, such as contextual bandits, or even reinforcement learning."
    }, {
      "heading" : "1.1. Previous Work",
      "text" : "Our constraint is equivalent to a constraint on the regret to a default strategy, or in the language of prediction-withexpert-advice, or bandit literature, regret to a default action. In the full information, mostly studied in the adversarial setting, much work has been devoted to understanding the price of such constraints (Hutter and Poland, 2005; EvenDar et al., 2008; Koolen, 2013; Sani et al., 2014, e.g.,). In particular, Koolen (2013) studies the Pareto frontier of regret vectors (which contains the non-dominated worst-case regret vectors of all algorithms). The main lesson of these works is that in the full information setting even a constant regret to a fixed default action can be maintained with essentially no increase in the regret to the best action. The situation quickly deteriorates in the bandit setting as shown by Lattimore (2015a). This is perhaps unsurprising given that, as opposed to the full information setting, in the bandit setting one needs to actively explore to get improved estimates\nof the actions’ payoffs. As mentioned earlier, Lattimore describes two learning algorithms relevant to our setting: In the stochastic setting we consider, Unbalanced MOSS (and its relative, Unbalanced UCB) are able to achieve a constant regret penalty while maintaining the return constraint while Exp3-γ achieves a much better regret as compared to our strategy for the adversarial setting. However, neither of these algorithms maintain the return constraint uniformly in time. Neither will the constraint hold with high probability. While Unbalanced UCB achieves problem-dependent bounds, it has the same issues as Unbalanced MOSS with maintaining the return constraint. Also, all these strategies rely heavily on knowing the payoff of the default action.\nMore broadly, the issue of staying safe while exploring has long been recognized in reinforcement learning (RL). Garcı́a and Fernández (2015) provides a comprehensive survey of the relevant literature. Lack of space prevents us from including much of this review. However, the short summary is that while the issue has been considered to be important, no previous approach addresses the problem from a theoretical angle. Also, while it has been recognized that adding constraints on the return is one way to ensure safety, as far as we know, maintaining the constraints during learning (as opposed to imposing them as a way of restricting the set of feasible policies) has not been considered in this literature. Our work, while it considers a much simpler setting, suggest a novel approach to address the safe exploration problem in RL.\nAnother line of work considers safe exploration in the related context of optimization (Sui et al., 2015). However, the techniques and the problem setting (e.g., objective) in this work is substantially different from ours."
    }, {
      "heading" : "2. Conservative Multi-Armed Bandits",
      "text" : "The multi-armed bandit problem is a sequential decisionmaking task in which a learning agent repeatedly chooses an action (called an arm) and receives a reward corresponding to that action. We assume there are K + 1 arms and denote the arm chosen by the agent in round t ∈ {1, 2, . . .} by It ∈ {0, . . . ,K}. There is a reward Xt,i associated with each arm i at each round t and the agent receives the reward corresponding to its chosen arm, Xt,It . The agent does not observe the other rewards Xt, j ( j , It).\nThe learning performance of an agent over a time horizon n is usually measured by its regret, which is the difference between its reward and what it could have achieved by consistently choosing the single best arm in hindsight:\nRn = max i∈{0,...,K} n∑ t=1 Xt,i − Xt,It . (1)\nAn agent is failing to learn unless its regret grows sub-\nlinearly: Rn ∈ o(n); good agents achieve Rn ∈ O( √\nn) or even Rn ∈ O(log n). We also use the notation Ti(n) = ∑n t=1 1{It = i} for the number of times the agent chooses arm i in the first n time steps."
    }, {
      "heading" : "2.1. Conservative Exploration",
      "text" : "Let arm 0 correspond to the conservative default action with the other arms 1, . . . ,K being the alternatives to be explored. We want to be able to choose some α > 0 and constrain the learner to earn at least a 1 − α fraction of the reward from simply playing arm 0:\nt∑ s=1 Xs,Is ≥ (1 − α) t∑ s=1 Xs,0 for all t ∈ {1, . . . , n}. (2)\nFor the introductory example above α = 0.1, which corresponds to losing at most 10% of the revenue compared to the default website. It should be clear that small values of α force the learner to be highly conservative, whereas larger α correspond to a weaker constraint.\nWe introduce a quantity Zn, called the budget, which quantifies how close the constraint (2) is to being violated:\nZt = t∑\ns=1\nXs,Is − (1 − α)Xs,0; (3)\nthe constraint is satisfied if and only if Zt ≥ 0 for all t ∈ {1, . . . , n}. Note that the constraints must hold uniformly in time.\nOur objective is to design algorithms that minimize the regret (1) while simultaneously satisfying the constraint (2). In the following sections, we will consider two variants of multi-armed bandits: the stochastic setting in Section 3 and the adversarial setting in Section 4. In each case we will design algorithms that satisfy different versions of the constraint and give regret guarantees.\nOne may wonder: what if we only care about Zn ≥ 0 instead of Zt ≥ 0 for all t. Although our algorithms are designed for satisfying the anytime constraint on Zt our lower bound, which is based on Zn ≥ 0 only, shows that in the stochastic setting we cannot improve the regret guarantee even if we only want to satisfy the overall constraint Zn ≥ 0."
    }, {
      "heading" : "3. The Stochastic Setting",
      "text" : "In the stochastic multi-armed bandit setting each arm i and round t has a stochastic reward Xt,i = µi + ηt,i, where µi ∈ [0, 1] is the expected reward of arm i and the ηt,i are independent random noise variables that we assume have 1-subgaussian distributions. We denote the expected reward of the optimal arm by µ∗ = maxi µi and the gap between it and the expected reward of the ith arm by ∆i = µ∗ − µi.\nThe regret Rn is now a random variable. We can bound it in expectation, of course, but we are often more interested in high-probability bounds on the weaker notion of pseudoregret:\nR̃n = nµ∗ − n∑\nt=1\nµIt = K∑ i=0 Ti(n)∆i, (4)\nin which the noise in the arms’ rewards is ignored and the randomness arises from the agent’s choice of arm. The regret Rn and the pseudo-regret R̃n are equal in expectation. High-probability bounds for the latter, however, can capture the risk of exploration without being dominated by the variance in the arms’ rewards.\nWe use the notation µ̂i(n) = 1Ti(n) ∑n\nt=1 1{It = i} Xt,i for the empirical mean of the rewards from arm i observed by the agent in the first n rounds. If Ti(n) = 0 then we define µ̂i(n) = 0. The algorithms for the stochastic setting will estimate the µi by µ̂i and will construct and act based on high-probability confidence intervals for the estimates."
    }, {
      "heading" : "3.1. The Budget Constraint",
      "text" : "Just as we substituted regret with pseudo-regret, in the stochastic setting we will use the following form of the constraint (2):\nt∑ s=1 µIs ≥ (1 − α)µ0t for all t ∈ {1, . . . , n}; (5)\nthe budget then becomes\nZ̃t = t∑\ns=1\nµIs − (1 − α)tµ0 . (6)\nThe default arm is always safe to play because it increases the budget by µ0−(1−α)µ0 = αµ0. The budget will decrease for arms i with µi < (1− α)µ0; the constraint Z̃n ≥ 0 is then in danger of being violated (Fig. 1).\nIn the following sections we will construct algorithms that satisfy pseudo-regret bounds and the budget constraint (5) with high probability 1 − δ (where δ > 0 is a tunable parameter). In Section 3.4 we will see how these algorithms can be adapted to satisfy the constraint in expectation and with bounds on their expected regret.\nFor simplicity, we will initially assume that the algorithms know µ0, the expected reward of the default arm. This is reasonable in situations where the default action has been used for a long time and is well-characterized. Even so, in Section 3.5 we will see that having to learn an unknown µ0 is not a great hindrance."
    }, {
      "heading" : "3.2. BudgetFirst — A Naive Algorithm",
      "text" : "Before presenting the new algorithm it is worth remarking on the most obvious naive attempt, which we call the BudgetFirst algorithm. A straightforward modification of UCB leads to an algorithm that accepts a confidence parameter δ ∈ (0, 1) and suffers regret at most\nR̃n = O  √ Kn log ( log(n) δ ) = Rworst . (7) Of course this algorithm alone will not satisfy the constraint (5), but that can be enforced by naively modifying the algorithm to deterministically choose It = 0 for the first t0 rounds where\n(∀ t0 ≤ t ≤ n) tµ0 − Rworst ≥ (1 − α)tµ0 . Subsequently the algorithm plays the high probability version of UCB and the regret guarantee (7) ensures the constraint (5) is satisfied with high probability. Solving the equation above leads to t0 = Õ(Rworst/αµ0), and since the regret while choosing the default arm may be O(1) the worst-case regret guarantee of this approach is\nR̃n = Ω  1µ0α √ Kn log ( log(n) δ ) . This is significantly worse than the more sophisticated algorithm that is our main contribution and for which the price of satisfying (5) is only an additive term rather than a large multiplicative factor."
    }, {
      "heading" : "3.3. Conservative UCB",
      "text" : "A better strategy is to play the default arm only until the budget (6) is large enough to start exploring other arms with a low risk of violating the constraint. It is safe to keep exploring as long as the budget remains large, whereas if it\ndecreases too much then it must be replenished by playing the default arm. In other words, we intersperse the exploration of a standard bandit algorithm with occasional budget-building phases when required. We show that accumulating a budget does not severely curtail exploration and thus gives small regret.\nConservative UCB (Algorithm 1) is based on UCB with the novel twist of maintaining a positive budget. In each round, UCB calculates upper confidence bounds for each arm; let Jt be the arm that maximizes this calculated confidence bound. Before playing this arm (as UCB would) our algorithm decides whether doing so risks the budget becoming negative. Of course, it does not know the actual budget Z̃t because the µi (i , 0) are unknown; instead, it calculates a lower confidence bound ξt based on confidence intervals for the µi. More precisely, it calculates a lower confidence bound for what the budget would be if it played arm Jt. If this lower bound is positive then the constraint will not be violated as long as the confidence bounds hold. If so, the the algorithm chooses It = Jt just as UCB would; otherwise it acts conservatively by choosing It = 0.\n1: Input: K, µ0, δ, ψδ(·) 2: for t ∈ 1, 2, . . . do . Compute confidence intervals. . .\n3: θ0(t), λ0(t)← µ0 . . . . for known µ0, 4: for i ∈ 1, . . . ,K do . . . . for other arms, 5: ∆i(t)← √ ψδ(Ti(t − 1))/Ti(t − 1) 6: θi(t)← µ̂i(t − 1) + ∆i(t) 7: λi(t)← max {0, µ̂i(t − 1) − ∆i(t)} 8: end for 9: Jt ← arg maxi θi(t) . . . . and find UCB arm. . Compute budget and. . .\n10: ξt ← ∑t−1\ns=1 λIs (t) + λJt (t) − (1 − α)tµ0 11: if ξt ≥ 0 then 12: It ← Jt . . . . choose UCB arm if safe, 13: else 14: It ← 0 . . . . default arm otherwise. 15: end if\n16: end for Algorithm 1: Conservative UCB\nRemark 1 (Choosing ψδ). The confidence intervals in Algorithm 1 are constructed using the function ψδ. Let F be the event that for all rounds t ∈ {1, 2, . . .} and every action i ∈ [K], the confidence intervals are valid:\n|µ̂i(t) − µi| ≤\n√ ψδ(Ti(t))\nTi(t) .\nOur goal is to choose ψδ(·) such that P {F} ≥ 1 − δ . (8)\nA simple choice is\nψδ(s) = 2 log(Ks3/δ),\nfor which (8) holds by Hoeffding’s inequality and union bounds. The following choice achieve better performance in practice:\nψδ(s) = log max{3, log ζ} + log(2e2ζ)\n+ ζ(1 + log(ζ)) (ζ − 1) log(ζ) log log(1 + s), (9)\nwhere ζ = K/δ; it can be seen to achieve (8) by more careful analysis motivated by Garivier (2013),\nSome remarks on Algorithm 1\n• µ0 is known, so the upper and lower confidence bounds can both be set to µ0 (line 3). See Section 3.5 for a modification that learns an unknown µ0.\n• The max in the definition of the lower confidence bound λi(t) (line 7) is because we have assumed µi ≥ 0 and so the lower confidence bound should never be less than 0.\n• ξt (line 10) is a lower confidence bound on the budget (6) if action Jt is chosen. More precisely, it is a lower confidence bound on\nZ̃t = t−1∑ s=1 µIs + µJt − (1 − α)tµ0.\n• If the default arm is also the UCB arm (Jt = 0) and the confidence intervals all contain the true values, then µ∗ = µ0 and the algorithm will choose action 0 for all subsequent rounds, incurring no regret.\nThe following theorem guarantees that Conservative UCB satisfies the constraint while giving a high-probability upper bound on its regret. Theorem 2. In any stochastic environment where the arms have expected rewards µi ∈ [0, 1] with 1-subgaussian noise, Algorithm 1 satisfies the following with probability at least 1 − δ and for every time horizon n:\nt∑ s=1 µIs ≥ (1 − α)µ0t for all t ∈ {1, . . . , n}, (5)\nR̃n ≤ ∑\ni>0:∆i>0\n( 4L ∆i + ∆i ) + 2(K + 1)∆0 αµ0\n+ 6L αµ0 K∑ i=1\n∆0\nmax{∆i,∆0 − ∆i} , (10)\nR̃n ∈ O (√\nnKL + KL αµ0\n) , (11)\nwhen ψδ is chosen in accordance with Remark 1 and where L = ψδ(n).\nStandard unconstrained UCB algorithms achieve a regret of order O( √ nKL); Theorem 2 tells us that the penalty our algorithm pays to satisfy the constraint is an extra additive regret of order O(KL/αµ0).\nRemark 3. We take a moment to understand how the regret of the algorithm behaves if α is polynomial in 1/n. Clearly if α ∈ O(1/n) then we have a constant exploration budget and the problem is trivially hard. In the slightly less extreme case when α is as small as n−a for some 0 < a < 1, the extra regret penalty is still not negligible: satisfying the constraint costs us O(na) more regret in the worst case.\nWe would argue that the problem-dependent regret penalty (10) is more informative than the worst case of O(na); our regret increases by\n6L αµ0 K∑ i=1\n∆0\nmax{∆i,∆0 − ∆i} .\nIntuitively, even if α is very small, we can still explore as long as the default arm is close-to-optimal (i.e. ∆0 is small) and most other arms are clearly sub-optimal (i.e. the ∆i are large). Then the sub-optimal arms are quickly discarded and even the budget-building phases accrue little regret: the regret penalty remains quite small. More precisely, if ∆0 ≈ n−b0 and mini>0:∆i>0 ∆i ≈ n−b, then the regret penalty is\nO ( na+min{0,b−b0} ) ;\nsmall ∆0 and large ∆i means b − b0 < 0, giving a smaller penalty than the worst case of O(na).\nRemark 4. Curious readers may be wondering if It = 0 is the only conservative choice when the arm proposed by UCB risks violating the constraint. A natural alternative would be to use the lower confidence bound λi(t) by choosing\nIt = Jt , if ξt ≥ 0 ;arg maxi λi(t) , otherwise . (12) It is easy to see that if F does not occur, then choosing arg maxi λi(t) increases the budget at least as much as choosing action 0 while incurring less regret and so this algorithm is preferable to Algorithm 1 in practice. Theoretically speaking, however, it is possible to show that the improvement is by at most a constant factor so our analysis of the simpler algorithm suffices. The proof of this claim is somewhat tedious so instead we provide two intuitions:\n1. The upper bound approximately matches the lower bound in the minimax regime, so any improvement must be relatively small in the minimax sense.\n2. Imagine we run the unmodified Algorithm 1 and let t be the first round when It , Jt and where there exists an i > 0 with λi(t) ≥ µ0. If F does not hold, then the\nactions chosen by UCB satisfy Ti(t) ∈ Ω min  L∆2i ,maxj T j(t)  , which means that arms are being played in approximately the same frequency until they are proving suboptimal (for a similar proof, see Lattimore, 2015b). From this it follows that once λIt (t) ≥ µ0 for some i it will not be long before either λ j(t + s) ≥ µ0 or T j(t + s) ≥ 4L/∆2i and in both cases the algorithm will cease playing conservatively. Thus it takes at most a constant proportion more time before the naive algorithm is exclusively choosing the arm chosen by UCB.\nNext we discuss how small modifications to Algorithm 1 allow it to handle some variants of the problem while guaranteeing the same order of regret."
    }, {
      "heading" : "3.4. Considering the Expected Regret and Budget",
      "text" : "One may care about the performance of the algorithm in expectation rather than with high probability, i.e. we want an upper bound on E [ R̃n ] and the constraint (5) becomes\nE [ t∑\ns=1\nµIs ] ≥ (1 − α)µ0t, for all t ∈ {1, . . . , n}. (13)\nWe argued in Remark 3 that if α ∈ O(1/n) then the problem is trivially hard; let us assume therefore that α ≥ c/n for some c > 1. By running Algorithm 1 with δ = 1/n and α′ = (α− δ)/(1− δ) we can achieve (13) and a regret bound with the same order as in Theorem 2.\nTo show (13) we have\nE [ t∑\ns=1\nµIs ] ≥ P {F}E [ t∑ s=1 µIs ∣∣∣∣ F] ≥ (1 − δ)(1 − α′)µ0t = (1 − α)µ0t .\nIn the upper bound of E [Rn], we have E [Rn] ≤ E [Rn|F] + δn = E [Rn|F] + 1 .\nE [Rn|F] can be upper bounded by Theorem 2 with two changes: (i) L becomes O(log nK) after replacing δ with 1/n, and (ii) α becomes α′. Since α′/α ≥ 1 − 1/c we get essentially the same order of regret bound as in Theorem 2."
    }, {
      "heading" : "3.5. Learning an Unknown µ0",
      "text" : "Two modifications to Algorithm 1 allow it to handle the case when µ0 is unknown. First, just as we do for the nondefault arms, we need to set θ0(t) and λ0(t) based on confidence intervals. Second, the lower bound on the budget\nneeds to be set as\nξ′t = K∑\ni=1\nTi(t − 1)λi(t) + λJt (t)\n+ (T0(t − 1) − (1 − α)t)θ0(t) . (14)\nTheorem 5. Algorithm 1, modified as above to work without knowing µ0 but otherwise the same conditions as Theorem 2, satisfies with probability 1 − δ and for all time horizons n the constraint (5) and the regret bound\nR̃n ≤ ∑\ni:∆i>0\n( 4L ∆i + ∆i ) + 2(K + 1)∆0 αµ0\n+ 7L αµ0 K∑ i=1\n∆0\nmax{∆i,∆0 − ∆i} . (15)\nTheorem 5 shows that we get the same order of regret for unknown µ0. The proof is very similar to the one for Theorem 2 and is also left for the appendix."
    }, {
      "heading" : "4. The Adversarial Setting",
      "text" : "Unlike the stochastic case, in the adversarial multi-armed bandit setting we do not make any assumptions about how the rewards are generated. Instead, we analyze a learner’s worst-case performance over all possible sequences of rewards (Xt,i). In effect, we are treating the environment as an adversary that has intimate knowledge of the learner’s strategy and will devise a sequence of rewards that maximizes regret. To preserve some hope of succeeding, however, the learner is allowed to behave randomly: in each round it can randomize its choice of arm It using a distribution it constructs; the adversary cannot influence nor predict the result of this random choice.\nOur goal is, as before, to satisfy the constraint (2) while bounding the regret (1) with high probability (the randomness comes from the learner’s actions). We assume that the default arm has a fixed reward: Xt,0 = µ0 ∈ [0, 1] for all t; the other arms’ rewards are generated adversarially in [0, 1]. The constraint to be satisfied then becomes∑t\ns=1 Xs,Is ≥ (1 − α)µ0t for all t.\nSafe-playing strategy: We take any standard any-time high probability algorithm for adversarial bandits and adapt it to play as usual when it is safe to do so, i.e. when Zt ≥ ∑t−1 s=1 Xs,Is − (1 − α)µ0t ≥ 0. Otherwise it should play It = 0. To demonstrate a regret bound, we only require that the bandit algorithm satisfy the following requirement. Definition 6. An algorithm A is R̂δt -admissible (R̂δt sublinear) if for any δ, in the adversarial setting it satisfies\nP { ∀t ∈ {1, 2, . . .},Rt ≤ R̂δt } ≥ 1 − δ.\nNote that this performance requirement is stronger than the\ntypical high probability bound but is nevertheless achievable. For example, Neu (2015) states the following for the any-time version of their algorithm: given any time horizon n and confidence level δ, P { Rn ≤ R̂′n(δ) } ≥ 1 − δ for some sub-linear R̂′t(δ). If we let R̂ δ t = R̂ ′ t(δ/2t\n2) then P { Rt ≤ R̂δt } ≥ 1 − δ2t2 holds for any fixed t. Since the algorithm does not require n and δ as input, a union bound shows it to be R̂δt -admissible.\nHaving satisfied ourselves that there are indeed algorithms that meet our requirements, we can prove a regret guarantee for our safe-playing strategy.\nTheorem 7. Any R̂δt -admissible algorithm A, when adapted with our safe-playing strategy, satisfies the constraint (2) and has a regret bound of Rn ≤ t0 + R̂δn with probability at least 1−δ where t0 = max{t |αµ0t ≤ R̂δt +µ0}.\nCorollary 8. The any-time high probability algorithm of Neu (2015) adapted with our safe-playing strategy gives R̂δt = 7 √ Kt log K log(4t2/δ) and\nRn ≤ 7 √\nKn log K log(4n2/δ) + 49K log K α2µ20 log2 4n2 δ\nwith probability at least 1 − δ.\nCorollary 8 shows that a strategy similar to that of Algorithm 1 also works for the adversarial setting. However, we pay a higher regret penalty to satisfy the constraint:\nO ( KL2\n(αµ0)2\n) rather than the O ( KL αµ0 ) we had in the stochastic\nsetting. Whether this is because (i) our algorithm is sub-optimal, (ii) the analysis is not tight, or (iii) there is some intrinsic hardness in the non-stochastic setting is still not clear and remains an interesting open problem."
    }, {
      "heading" : "5. Lower Bound on the Regret",
      "text" : "We now present a worst-case lower bound where α, µ0 and n are fixed, but the mean rewards are free to change. For any vector µ ∈ [0, 1]K , we will write Eµ to denote expectations under the environment where all arms have normallydistributed unit-variance rewards and means µi (i.e., the fixed value µ0 is the mean reward of arm 0 and the components of µ are the mean rewards of the other arms). We assume normally distributed noise for simplicity: Other subgaussian distributions work identically as long as the subgaussian parameter can be kept fixed independently of the mean rewards.\nTheorem 9. Suppose for any µi ∈ [0, 1] (i > 0) and µ0 satisfying\nmin{µ0, 1 − µ0} ≥ max { 1/2 √ α, √ e + 1/2 } √ K/n,\nan algorithm satisfies Eµ ∑n\nt=1 Xt,It ≥ (1−α)µ0n. Then there is some µ ∈ [0, 1]K such that its expected regret satisfies\nEµRn ≥ B where B = max  K(16e + 8)αµ0 , √ Kn √ 16e + 8 . (16) Theorem 9 shows that our algorithm for the stochastic setting is near-optimal (up to a logarithmic factor L) in the worst case. A problem-dependent lower bound for the stochastic setting would be interesting but is left for future work. Also note that in the lower bound we only use Eµ ∑n t=1 Xt ≥ (1 − α)nµ0 for the last round n, which means that the regret guarantee cannot be improved if we only care about the last-round budget instead of the anytime budget. In practice, however, enforcing the constraint in all rounds will generally lead to significantly worse results because the algorithm cannot explore early on. This is demonstrated empirically in Section 6, where we find that the Unbalanced MOSS algorithm performs very well in terms of the expected regret, but does not satisfy the constraint in early rounds.\nRemark 10. The theorem above almost follows from the lower bound given by Lattimore (2015a), but in that paper µ0 is unknown, while here it may be known. This makes our result strictly stronger, as the lower bound is the same up to constant factors."
    }, {
      "heading" : "6. Experiments",
      "text" : "We evaluate the performance of Conservative UCB compared to UCB and Unbalanced MOSS Lattimore (2015a) using simulated data in two regimes. In the first we fix the horizon and sweep over α ∈ [0, 1] to show the degradation of the average regret of Conservative UCB relative to UCB as the constraint becomes harsher (α close to zero). In the second regime we fix α = 0.1 and plot the long-term average regret, showing that Conservative UCB is eventually nearly as good as UCB, despite the constraint. Each data point is an average of N ≈ 4000 i.i.d. samples, which makes error bars too small to see. All code and data will be made available in any final version. Results are shown for both versions of Conservative UCB: The first knows the mean µ0 of the default arm while the second does not and must act more conservatively while learning this value. As predicted by the theory, the difference in performance between these two versions of the algorithm is relatively small, but note that even when α = 1 the algorithm that knows µ0 is performing better because this knowledge is useful in the unconstrained setting. This is also true of the BudgetFirst algorithm, which is unconstrained when α = 1 and exploits its knowledge of µ0 to eliminate the default arm. This algorithm is so conservative that even when α is nearly zero it must first build a significant budget. We tuned the Unbalanced MOSS algorithm with the following\nparameters.\nB0 = nK\n√ nK + K\nαµ0\nBi = BK = √ nK + K αµ0 .\nThe quantity Bi determines the regret of the algorithm with respect to arm i up to constant factors, and must be chosen to lie inside the Pareto frontier given by Lattimore (2015a). It should be emphasised that Unbalanced MOSS does not constraint the return except for the last round, and has no high-probability guarantees. This freedom allows it to explore early, which gives it a significant advantage over the highly constrained Conservative UCB. Furthermore, it also requires B0, . . . , BK as inputs, which means that µ0 must be known in advance. The mean rewards in both experiments are µ0 = 0.5, µ1 = 0.6, µ2 = µ3 = µ4 = 0.4, which means that the default arm is slightly sub-optimal."
    }, {
      "heading" : "7. Conclusion",
      "text" : "We introduced a new family of multi-armed bandit frameworks motivated by the requirement of exploring conservatively to maintain revenue. We also demonstrated various strategies that act effectively while maintaining such constraints. We expect that similar strategies generalize to other settings, like contextual bandits and reinforcement learning. We want to emphasize that this is just the beginning of a line of research that has many potential applications. We hope that others will join us in improving the current results, closing open problems, and generalizing the model so it is more widely applicable."
    }, {
      "heading" : "A. Proof of Theorem 2",
      "text" : "Theorem 2. In any stochastic environment where the arms have expected rewards µi ∈ [0, 1] with 1-subgaussian noise, Algorithm 1 satisfies the following with probability at least 1 − δ and for every time horizon n:\nt∑ s=1 µIs ≥ (1 − α)µ0t for all t ∈ {1, . . . , n}, (5)\nR̃n ≤ ∑\ni>0:∆i>0\n( 4L ∆i + ∆i ) + 2(K + 1)∆0 αµ0\n+ 6L αµ0 K∑ i=1\n∆0\nmax{∆i,∆0 − ∆i} , (10)\nR̃n ∈ O (√\nnKL + KL αµ0\n) , (11)\nwhen ψδ is chosen in accordance with Remark 1 and where L = ψδ(n).\nProof. By Remark 1, with probability P {F} ≥ 1 − δ the confidence intervals are valid for all t and all arms i ∈ {1, . . . ,K}:\n|µ̂i(t − 1) − µi| ≤ √ ψδ(Ti(t − 1))/Ti(t − 1)\n≤ √\nL/Ti(t − 1); we will henceforth assume that this is the case (i.e. that F holds). By the definition of the confidence intervals and by the construction of Algorithm 1 we immediately satisfy the constraint\nn∑ t=1 µIt ≥ (1 − α)nµ0 for all n.\nWe now bound the regret. Let i > 0 be the index of a sub-optimal arm and suppose It = i. Since the confidence intervals are valid,\nµ∗ ≤ θi(t) ≤ µ̂i(t − 1) + √\nL/Ti(t − 1) ≤ µi + 2 √ L/Ti(t − 1) ,\nwhich implies that arm i has not been chosen too often; in particular we obtain\nTi(n) ≤ Ti(n − 1) + 1 ≤ 4L ∆2i + 1. (17)\nand the regret satisfies\nR̃n = K∑\ni=0\nTi(n)∆i ≤ ∑\ni>0:∆i>0\n( 4L ∆i + ∆i ) + T0(n)∆0.\nIf ∆0 = 0 then the theorem holds trivially; we therefore assume that ∆0 > 0 and find an upper bound for T0(n).\nLet τ = max{t ≤ n | It = 0} be the last round in which the default arm is played. Since F holds and θ0(t) = µ0 < µ∗ < maxi θi(t), it follows that Jt = 0 is never the UCB choice; the default arm was only played because ξτ < 0:\nK∑ i=0 Ti(τ − 1)λi(τ) + λJτ (τ) − (1 − α)µ0τ < 0 (18)\nBy dropping λJτ (τ), replacing τ with ∑K\ni=0 Ti(τ−1) + 1, and rearranging the terms in (18), we get\nαT0(τ − 1)µ0\n< (1 − α)µ0 + K∑\ni=1\nTi(τ − 1) ((1 − α)µ0 − λi(τ))\n≤ (1 − α)µ0\n+ K∑ i=1 Ti(τ − 1) (1 − α)µ0 − µi + √ LTi(τ − 1)  ≤ 1 +\nK∑ i=1 S i . (19)\nwhere ai = (1 − α)µ0 − µi and S i = Ti(τ − 1) · ( (1 − α)µ0 − µi + √ L/Ti(τ − 1) ) = aiTi(τ − 1) + √ LTi(τ − 1)\nis a bound on the decrease in ξt in the first τ− 1 rounds due to choosing arm i. We will now bound S i for each i > 0.\nThe first case is ai ≥ 0, i.e. ∆i ≥ ∆0 + αµ0. Then (17) gives Ti(τ − 1) ≤ 4L/∆2i + 1 and we get\nS i ≤ 4Lai ∆2i + 2L ∆i + 2 ≤ 6L ∆i + 2 . (20)\nThe other case is ai < 0, i.e. ∆i < ∆0 + αµ0. Then S i ≤ √\nLTi(τ − 1) ≤ 2L ∆i + 1, (21)\nand by using ax2 + bx ≤ −b2/4a for a < 0 we have\nS i ≤ − L\n4ai = L 4(∆0 + αµ0 − ∆i) . (22)\nSummarizing (20) to (22) gives\nS i ≤ 6L\nmax{∆i,∆0 − ∆i} + 2 .\nContinuing from (19), we get\nT0(n) = T0(τ − 1) + 1\n≤ 2K + 2 αµ0 + 1 αµ0 K∑ i=1 6L max{∆i,∆0 − ∆i} .\nWe can now upper bound the regret by\nR̃n ≤ ∑\ni>0:∆i>0\n( 4L ∆i + ∆i ) + 2(K + 1)∆0 αµ0\n+ 6L αµ0 K∑ i=1\n∆0\nmax{∆i,∆0 − ∆i} . (10)\nWe will now show (11). To bound the regret due to the non-default arms, Jensen’s inequality gives∑\ni>0\nTi(n)∆i 2 ≤ m2 ∑ i>0 Ti(n) m ∆2i ,\nwhere m ≤ n is the number of times non-default arms were chosen. Combining this with ∆2i ≤ 4L/Ti(n) for suboptimal arms from (17) gives∑\ni>0\nTi(n)∆i ≤ 2 √ mKL ∈ O( √ nKL).\nTo bound the regret due to the default arm, observe that max{∆i,∆0 − ∆i} ≥ ∆0/2 and thus T0(n)∆0 ∈ O(KL/αµ0). Combining these two bounds gives (11)."
    }, {
      "heading" : "B. Proof of Theorem 5",
      "text" : "Theorem 5. Algorithm 1, modified as above to work without knowing µ0 but otherwise the same conditions as Theorem 2, satisfies with probability 1 − δ and for all time horizons n the constraint (5) and the regret bound\nR̃n ≤ ∑\ni:∆i>0\n( 4L ∆i + ∆i ) + 2(K + 1)∆0 αµ0\n+ 7L αµ0 K∑ i=1\n∆0\nmax{∆i,∆0 − ∆i} . (15)\nProof. We proceed very similarly to the proof of Theorem 2 in Appendix A. As we did there, we assume that F holds: the confidence intervals are valid for all rounds and all arms (including the default), which happens with probability P {F} ≥ 1 − δ. To show that the modified algorithm satisfies the constraint (5), we write the budget (6) as\nZ̃t = K∑\ni=1\nTi(t − 1)µi + µJt + (T0(t − 1) − (1 − α)t)µ0\nwhen the UCB arm Jt is chosen and show that it is indeed lower-bounded by\nξ′t = K∑\ni=1\nTi(t − 1)λi(t) + λJt (t)\n+ (T0(t − 1) − (1 − α)t)θ0(t) . (14) This is apparent if T0(t − 1) < (1 − α)t, since the last term\nin (14) is then negative and θ0(t) ≥ µ0. On the other hand, if T0(t − 1) ≥ (1 − α)t then the constraint is still satisfied:\nt∑ s=1 µIs ≥ T0(t − 1)µ0 ≥ (1 − α)µ0t.\nWe now upper-bound the regret. As in the earlier proof, we can show that for any arm i > 0 with ∆i > 0 we have Ti(n) ≤ 4L/∆2i + 1. If this also holds for i = 0 or if ∆0 = 0 then R̃n ≤ ∑ i:∆>0(4L/∆i + ∆i) and the theorem holds trivially. From now on we only consider the case when ∆0 > 0 and T0(n) > 4L/∆20 + 1. As before, we will proceed to upperbound T0(n).\nLet τ be the last round in which Iτ = 0. We can ignore the possibility that Jτ = 0, since then the above bound on Ti(n) would apply even to the default arm, contradicting our assumption above. Thus we can assume that the default arm was played because ξ′τ < 0:\nK∑ i=1 Ti(τ − 1)λi(τ) + λJτ (τ)\n+ ( T0(τ − 1) − (1 − α)τ ) θ0(τ) < 0 ,\nin which we drop λJτ (τ), replace τ with ∑K\ni=0 Ti(τ − 1) + 1, and rearrange the terms to get\nαT0(τ − 1)θ0(τ) < (1 − α)θ0(τ)\n+ K∑ i=1 Ti(τ − 1) ( (1 − α)θ0(τ) − λi(τ) ) . (23)\nWe lower-bound the left-hand side of (23) using θ0(τ) ≥ µ0, whereas we upper-bound the right-hand side using\nθ0(τ) ≤ µ0 + √\nL T0(τ − 1) ≤ µ0 + ∆0 2 ,\nwhich comes from T0(τ − 1) ≥ 4L/∆20. Combining these in (23) with the lower confidence bound λi(τ) ≥ µi −√\nL/Ti(τ − 1) gives αµ0T0(τ − 1) < (1 − α) ( µ0 + ∆0\n2 ) +\nK∑ i=1 Ti(τ − 1) ( (1 − α) ( µ0 + ∆0 2 )\n− µi + √\nL Ti(τ − 1) ) = (1 − α) ( µ0 + ∆0\n2\n) + K∑ i=1 S i\n≤ 1 + K∑\ni=1\nS i , (24)\nwhere ai = (1 − α)(µ0 + ∆0/2) − µi and S i = aiTi(τ − 1) + √ LTi(τ − 1)\nis a bound on the decrease in ξ′t in the first τ− 1 rounds due to choosing arm i. We will now bound S i for each i > 0.\nAnalogously to the previous proof, we get the bounds\nS i ≤ 6L ∆i + 2, when ai ≥ 0 ; (25) S i ≤ 2L ∆i + 1 , otherwise; (26)\nand in the latter case, using ax2 + bx ≤ −b2/4a gives\nS i ≤ − L\n4ai = L 4 ( (1 + α)∆0/2 + αµ0 − ∆i ) . (27) Summarizing (25) to (27) gives\nS i ≤ 6L max { ∆i, 24 ( (1 + α)∆0/2 + αµ0 − ∆i )} + 2 ≤ 7L\nmax{∆i,∆0 − ∆i} + 2 .\nContinuing with (24), if T0(n) > 4L∆20 + 1, we get\nT0(n) = T0(τ − 1) + 1\n≤ 2K + 2 αµ0 + 1 αµ0 K∑ i=1 7L max{∆i,∆0 − ∆i} .\nWe can now upper bound the regret by\nR̃n ≤ ∑\ni:∆i>0\n( 4L ∆i + ∆i ) + 2(K + 1)∆0 αµ0\n+ 7L αµ0 K∑ i=1\n∆0\nmax{∆i,∆0 − ∆i} . (15)"
    }, {
      "heading" : "C. Proof of Theorem 7",
      "text" : "Theorem 7. Any R̂δt -admissible algorithm A, when adapted with our safe-playing strategy, satisfies the constraint (2) and has a regret bound of Rn ≤ t0 + R̂δn with probability at least 1−δ where t0 = max{t |αµ0t ≤ R̂δt +µ0}.\nProof of Theorem 7. It is clear from the description of the safe-playing strategy that it is indeed safe: the constraint (2) is always satisfied.\nThe algorithm plays safe when the following quantity, which is a lower bound on the budget Zt, is negative:\nZ′t = Zt − Xt,It = t−1∑ s=1 Xs,Is − (1 − α)µ0t\nTo upper bound the regret, consider only the rounds in which our safe-playing strategy does not interfere with playingA’s choice of arm. Then with probability 1 − δ,\nmax i∈{0,...,K} t∑ s=1 1 { Z′s ≥ 0 } (Xs,i − Xs,Is ) ≤ R̂δB(t)\nwhere B(t) = ∑t s=1 1 { Z′s ≥ 0 } . Let τ be the last round in which the algorithm plays safe.\nµ0B(τ − 1)\n≤ max i τ−1∑ s=1 1 { Z′s ≥ 0 } Xs,i\n≤ R̂δB(τ−1) + τ−1∑ s=1 1 { Z′s ≥ 0 } Xs,Is\n= R̂δB(τ−1) + τ−1∑ s=1 Xs,Is − µ0(τ − 1 − B(τ − 1)) ≤ R̂δB(τ−1) + (1 − α)µ0τ − µ0(τ − 1 − B(τ − 1)) ,\nwhich indicates αµ0τ ≤ R̂δτ + µ0 and thus τ ≤ t0. It follows that Rn ≤ t0 + R̂δn."
    }, {
      "heading" : "D. Proof of Theorem 9",
      "text" : "Theorem 9. Suppose for any µi ∈ [0, 1] (i > 0) and µ0 satisfying\nmin{µ0, 1 − µ0} ≥ max { 1/2 √ α, √ e + 1/2 } √ K/n,\nan algorithm satisfies Eµ ∑n\nt=1 Xt,It ≥ (1−α)µ0n. Then there is some µ ∈ [0, 1]K such that its expected regret satisfies EµRn ≥ B where\nB = max  K(16e + 8)αµ0 , √ Kn √ 16e + 8 . (16)\nProof of Theorem 9. Pick any algorithm. We want to show that the algorithm’s regret on some environment is at least as large as B. If EµRn > B for some µ ∈ [0, 1]K , there is nothing to be proven. Hence, without loss of generality, we can assume that the algorithm is consistent in the sense that EµRn ≤ B for all µ ∈ [0, 1]K . For some ∆ > 0, define environment µ ∈ RK such that µi = µ0 − ∆ for all i ∈ [K]. For now, assume that µ0 and ∆ are such that µi ≥ 0; we will get back to this condition later. Also define environment µ(i) for each i = 1, . . . ,K by\nµ(i)j = µ0 + ∆, for j = i ;µ0 − ∆, otherwise. In this proof, we use Ti = Ti(n) to denote the number of times arm i was chosen in the first n rounds. We distinguish two cases, based on how large the exploration budget is. Case 1: α ≥ √\nK µ0 √ (16e + 8)n .\nIn this case, B = √\nKn√ 16e+8 and we use ∆ = (4e + 2)B/n. For each i ∈ [K] define event Ai = {Ti ≤ 2B/∆}. First we prove\nthat Pµ(Ai) ≥ 1/2: Pµ{Ti ≤ 2B/∆} = 1 − Pµ{Ti > 2B/∆}\n≥ 1 − ∆Eµ[Ti]\n2B\n≥ 1 − Eµ[Rn] 2B ≥ 1 2 .\nNext we prove that Pµ(i) (Ai) ≤ 1/4e: Pµ(i) {Ti ≤ 2B/∆} = Pµ(i) {n − Ti ≥ n − 2B/∆}\n≤ Eµ(i) [n − Ti] n − 2B/∆ ≤ B ∆n − 2B = 1 4e .\nNote that µ and µ(i) differ only in the ith component: µi = µ0 − ∆ whereas µ(i)i = µ0 + ∆. Then the KL divergence between the reward distributions of the ith arms is KL(µi, µ (i) i ) = (2∆)\n2/2 = 2∆2. Define the binary relative entropy to be\nd(x, y) = x log x y + (1 − x) log 1 − x 1 − y ;\nit satisfies d(x, y) ≥ (1/2) log(1/4y) for x ∈ [1/2, 1] and y ∈ (0, 1). By a standard change of measure argument (see, e.g., Kaufmann et al., 2015, Lemma 1) we get that\nEµ[Ti] · KL(µi; µ(i)i ) ≥ d(Pµ(Ai),Pµ(i) (Ai))\n≥ 1 2 log 1 4(1/4e) = 1 2\nand so Eµ[Ti] ≥ 1/4∆2 for each i ∈ [K]. Hence\nEµ[Rn] = ∆ ∑ i∈[K] Eµ[Ti] ≥ K 4∆ = √ Kn √ 16e + 8 = B .\nCase 2: α < √\nK µ0 √ (16e + 8)n .\nIn this case, B = K(16e+8)αµ0 and we use ∆ = K/4αµ0n. For each i define the event Ai = {Ti ≤ 2αµ0n/∆}. First we prove that Pµ(Ai) ≥ 1/2:\nPµ{Ti ≤ 2αµ0n/∆} = 1 − Pµ{Ti > 2αµ0n/∆}\n≥ 1 − ∆Eµ[Ti] 2αµ0n ≥ 1 − Eµ[Rn] 2αµ0n ≥ 1 2 ,\nwhere we use the fact that Eµ[Rn] = nµ0 − Eµ [ n∑\nt=1\nXt,It ]\n≤ nµ0 − (1 − α)µ0n = αµ0n.\nNext, we show that Pµ(i) (Ai) < 1/4e: Pµ(i) {Ti ≤ 2αµ0n/∆}\n= Pµ(i) {n − Ti ≥ n − 2αµ0n/∆} ≤ Eµ(i) [n − Ti] n − 2αµ0n/∆ ≤ B ∆n − 2αµ0n = K\n(4e + 2)K − (32e + 16)α2µ20n <\n1 4e .\nAs in the other case, we have Eµ[Ti] > 1/4∆2 for each i ∈ [K]. Therefore\nEµ[Rn] = ∆ ∑ i∈[K] Eµ[Ti] > K 4∆ = αµ0n,\nwhich contradicts the fact that Eµ[Rn] ≤ αµ0n. So there does not exist an algorithm whose worst-case regret is smaller than B.\nTo summarize, we proved that\nEµRn ≥  √ Kn √ 16e + 8 , when α ≥\n√ K\nµ0 √\n(16e + 8)n K\n(16e + 8)αµ0 , otherwise,\nfinishing the proof."
    } ],
    "references" : [ {
      "title" : "Sample mean based index policies with o(log n) regret for the multi-armed bandit problem",
      "author" : [ "R. Agrawal" ],
      "venue" : "Advances in Applied Probability,",
      "citeRegEx" : "Agrawal.,? \\Q1995\\E",
      "shortCiteRegEx" : "Agrawal.",
      "year" : 1995
    }, {
      "title" : "Finite-time analysis of the multiarmed bandit problem",
      "author" : [ "P. Auer", "N. Cesa-Bianchi", "P. Fischer" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Auer et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Auer et al\\.",
      "year" : 2002
    }, {
      "title" : "Regret to the best vs. regret to the average",
      "author" : [ "E. Even-Dar", "M. Kearns", "Y. Mansour", "J. Wortman" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Even.Dar et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Even.Dar et al\\.",
      "year" : 2008
    }, {
      "title" : "Distributed policy search reinforcement learning for job-shop scheduling",
      "author" : [ "T. Gabel", "M. Riedmiller" ],
      "venue" : "tasks. International Journal of Production Research,",
      "citeRegEx" : "Gabel and Riedmiller.,? \\Q2011\\E",
      "shortCiteRegEx" : "Gabel and Riedmiller.",
      "year" : 2011
    }, {
      "title" : "A comprehensive survey on safe reinforcement learning",
      "author" : [ "J. Garcı́a", "F. Fernández" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Garcı́a and Fernández.,? \\Q2015\\E",
      "shortCiteRegEx" : "Garcı́a and Fernández.",
      "year" : 2015
    }, {
      "title" : "Informational confidence bounds for selfnormalized averages and applications",
      "author" : [ "A. Garivier" ],
      "venue" : "arXiv preprint arXiv:1309.3376,",
      "citeRegEx" : "Garivier.,? \\Q2013\\E",
      "shortCiteRegEx" : "Garivier.",
      "year" : 2013
    }, {
      "title" : "Adaptive online prediction by following the perturbed leader",
      "author" : [ "M. Hutter", "J. Poland" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Hutter and Poland.,? \\Q2005\\E",
      "shortCiteRegEx" : "Hutter and Poland.",
      "year" : 2005
    }, {
      "title" : "lil’UCB: An optimal exploration algorithm for multiarmed bandits",
      "author" : [ "K. Jamieson", "M. Malloy", "R. Nowak", "S. Bubeck" ],
      "venue" : null,
      "citeRegEx" : "Jamieson et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Jamieson et al\\.",
      "year" : 2014
    }, {
      "title" : "Sequential choice from several populations",
      "author" : [ "M.N. Katehakis", "H. Robbins" ],
      "venue" : "Proceedings of the National Academy of Sciences of the United States of America,",
      "citeRegEx" : "Katehakis and Robbins.,? \\Q1995\\E",
      "shortCiteRegEx" : "Katehakis and Robbins.",
      "year" : 1995
    }, {
      "title" : "On the complexity of best arm identification in multi-armed bandit models",
      "author" : [ "E. Kaufmann", "A. Garivier", "O. Cappé" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Kaufmann et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Kaufmann et al\\.",
      "year" : 2015
    }, {
      "title" : "The Pareto regret frontier",
      "author" : [ "W.M. Koolen" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Koolen.,? \\Q2013\\E",
      "shortCiteRegEx" : "Koolen.",
      "year" : 2013
    }, {
      "title" : "The Pareto regret frontier for bandits",
      "author" : [ "T. Lattimore" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Lattimore.,? \\Q2015\\E",
      "shortCiteRegEx" : "Lattimore.",
      "year" : 2015
    }, {
      "title" : "Optimally confident UCB : Improved regret for finite-armed bandits",
      "author" : [ "T. Lattimore" ],
      "venue" : "Technical report,",
      "citeRegEx" : "Lattimore.,? \\Q2015\\E",
      "shortCiteRegEx" : "Lattimore.",
      "year" : 2015
    }, {
      "title" : "Towards automatic experimentation of educational knowledge",
      "author" : [ "Y.-E. Liu", "T. Mandel", "E. Brunskill", "Z. Popović" ],
      "venue" : "In SIGCHI Conference on Human Factors in Computing Systems (CHI",
      "citeRegEx" : "Liu et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2014
    }, {
      "title" : "Explore no more: Improved high-probability regret bounds for non-stochastic bandits",
      "author" : [ "G. Neu" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Neu.,? \\Q2015\\E",
      "shortCiteRegEx" : "Neu.",
      "year" : 2015
    }, {
      "title" : "Learning effective multimodal dialogue strategies from Wizard-of-Oz data: Bootstrapping and evaluation",
      "author" : [ "V. Rieser", "O. Lemon" ],
      "venue" : "In ACL-08: HLT,",
      "citeRegEx" : "Rieser and Lemon.,? \\Q2008\\E",
      "shortCiteRegEx" : "Rieser and Lemon.",
      "year" : 2008
    }, {
      "title" : "Exploiting easy data in online optimization",
      "author" : [ "A. Sani", "G. Neu", "A. Lazaric" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Sani et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Sani et al\\.",
      "year" : 2014
    }, {
      "title" : "Safe exploration for optimization with Gaussian processes",
      "author" : [ "Y. Sui", "A. Gotovos", "J. Burdick", "A. Krause" ],
      "venue" : "In Proceedings of the 32nd International Conference on Machine Learning",
      "citeRegEx" : "Sui et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Sui et al\\.",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 13,
      "context" : "In these cases a designer may feel that experimenting with sub-par interaction strategies could cause more harm than good (e.g., Rieser and Lemon, 2008; Liu et al., 2014).",
      "startOffset" : 122,
      "endOffset" : 170
    }, {
      "referenceID" : 3,
      "context" : ", Gabel and Riedmiller, 2011), but deviating too much from established “best practices” will often be considered too dangerous. For examples from other domains see the survey paper of Garcı́a and Fernández (2015).",
      "startOffset" : 2,
      "endOffset" : 213
    }, {
      "referenceID" : 7,
      "context" : "The manager even thinks that perhaps a best-arm identification strategy from the bandit literature, such as the recent lil’UCB method of Jamieson et al. (2014), could be more suitable.",
      "startOffset" : 137,
      "endOffset" : 160
    }, {
      "referenceID" : 8,
      "context" : "(ii) We analyze the naive build-budget-then-learn strategy described above (which we call BudgetFirst) and design a significantly better alternative for stochastic bandits that switches between using the default arm and learning (using a version of UCB, a simple yet effective bandit learning algorithm: Agrawal, 1995; Katehakis and Robbins, 1995; Auer et al., 2002) in a “smoother” fashion.",
      "startOffset" : 229,
      "endOffset" : 366
    }, {
      "referenceID" : 1,
      "context" : "(ii) We analyze the naive build-budget-then-learn strategy described above (which we call BudgetFirst) and design a significantly better alternative for stochastic bandits that switches between using the default arm and learning (using a version of UCB, a simple yet effective bandit learning algorithm: Agrawal, 1995; Katehakis and Robbins, 1995; Auer et al., 2002) in a “smoother” fashion.",
      "startOffset" : 229,
      "endOffset" : 366
    }, {
      "referenceID" : 8,
      "context" : "In fact, this approach has been studied recently by Lattimore (2015a) (in a slightly more general setting than ours).",
      "startOffset" : 52,
      "endOffset" : 70
    }, {
      "referenceID" : 8,
      "context" : "In fact, this approach has been studied recently by Lattimore (2015a) (in a slightly more general setting than ours). However, the algorithm of Lattimore (2015a) cannot be guaranteed to maintain the return constraint uniformly in time.",
      "startOffset" : 52,
      "endOffset" : 162
    }, {
      "referenceID" : 0,
      "context" : "(ii) We analyze the naive build-budget-then-learn strategy described above (which we call BudgetFirst) and design a significantly better alternative for stochastic bandits that switches between using the default arm and learning (using a version of UCB, a simple yet effective bandit learning algorithm: Agrawal, 1995; Katehakis and Robbins, 1995; Auer et al., 2002) in a “smoother” fashion. (iii) We prove that the new algorithm, which we call Conservative UCB, meets the uniform return constraint (in various senses), while it can achieve significantly less regret than BudgetFirst. In particular, while BudgetFirst is shown to pay a multiplicative penalty in the regret for maintaining the return constraint, Conservative UCB only pays an additive penalty. We provide both high probability and expectation bounds, consider both high probability and expectation constraints on the return, and also consider the case when the payoff of the default arm is initially unknown. (iv) We also prove a lower bound on the best regret given the constraint and as a result show that the additive penalty is unavoidable; thus Conservative UCB achieves the optimal regret in a worst-case sense. While Unbalanced MOSS of Lattimore (2015a), when specialized to our setting, also achieves the optimal regret (as follows from the analysis of Lattimore, 2015a), as mentioned earlier it does not maintain the constraint uniformly in time (it will explore too much at the beginning of time); it also relies heavily on the knowledge of the mean payoff of the default strategy.",
      "startOffset" : 304,
      "endOffset" : 1227
    }, {
      "referenceID" : 0,
      "context" : "(ii) We analyze the naive build-budget-then-learn strategy described above (which we call BudgetFirst) and design a significantly better alternative for stochastic bandits that switches between using the default arm and learning (using a version of UCB, a simple yet effective bandit learning algorithm: Agrawal, 1995; Katehakis and Robbins, 1995; Auer et al., 2002) in a “smoother” fashion. (iii) We prove that the new algorithm, which we call Conservative UCB, meets the uniform return constraint (in various senses), while it can achieve significantly less regret than BudgetFirst. In particular, while BudgetFirst is shown to pay a multiplicative penalty in the regret for maintaining the return constraint, Conservative UCB only pays an additive penalty. We provide both high probability and expectation bounds, consider both high probability and expectation constraints on the return, and also consider the case when the payoff of the default arm is initially unknown. (iv) We also prove a lower bound on the best regret given the constraint and as a result show that the additive penalty is unavoidable; thus Conservative UCB achieves the optimal regret in a worst-case sense. While Unbalanced MOSS of Lattimore (2015a), when specialized to our setting, also achieves the optimal regret (as follows from the analysis of Lattimore, 2015a), as mentioned earlier it does not maintain the constraint uniformly in time (it will explore too much at the beginning of time); it also relies heavily on the knowledge of the mean payoff of the default strategy. (v) We also consider the adversarial setting where we design an algorithm similar to Conservative UCB: the algorithm uses an underlying “base” adversarial bandit strategy when it finds that the return so far is sufficiently higher than the minimum required return. We prove that the resulting method indeed maintains the return constraint uniformly in time and we also prove a high-probability bound on its regret. We find, however, that the additive penalty in this case is higher than in the stochastic case. Here, the Exp3-γ algorithm of Lattimore (2015a) is an alternative, but again, this algorithm is not able to maintain the return constraint uniformly in time.",
      "startOffset" : 304,
      "endOffset" : 2117
    }, {
      "referenceID" : 6,
      "context" : "In the full information, mostly studied in the adversarial setting, much work has been devoted to understanding the price of such constraints (Hutter and Poland, 2005; EvenDar et al., 2008; Koolen, 2013; Sani et al., 2014, e.g.,). In particular, Koolen (2013) studies the Pareto frontier of regret vectors (which contains the non-dominated worst-case regret vectors of all algorithms).",
      "startOffset" : 143,
      "endOffset" : 260
    }, {
      "referenceID" : 6,
      "context" : "In the full information, mostly studied in the adversarial setting, much work has been devoted to understanding the price of such constraints (Hutter and Poland, 2005; EvenDar et al., 2008; Koolen, 2013; Sani et al., 2014, e.g.,). In particular, Koolen (2013) studies the Pareto frontier of regret vectors (which contains the non-dominated worst-case regret vectors of all algorithms). The main lesson of these works is that in the full information setting even a constant regret to a fixed default action can be maintained with essentially no increase in the regret to the best action. The situation quickly deteriorates in the bandit setting as shown by Lattimore (2015a). This is perhaps unsurprising given that, as opposed to the full information setting, in the bandit setting one needs to actively explore to get improved estimates",
      "startOffset" : 143,
      "endOffset" : 674
    }, {
      "referenceID" : 17,
      "context" : "Another line of work considers safe exploration in the related context of optimization (Sui et al., 2015).",
      "startOffset" : 87,
      "endOffset" : 105
    }, {
      "referenceID" : 4,
      "context" : "Garcı́a and Fernández (2015) provides a comprehensive survey of the relevant literature.",
      "startOffset" : 0,
      "endOffset" : 29
    }, {
      "referenceID" : 5,
      "context" : "where ζ = K/δ; it can be seen to achieve (8) by more careful analysis motivated by Garivier (2013),",
      "startOffset" : 83,
      "endOffset" : 99
    }, {
      "referenceID" : 14,
      "context" : "For example, Neu (2015) states the following for the any-time version of their algorithm: given any time horizon n and confidence level δ, P { Rn ≤ R̂n(δ) } ≥ 1 − δ for some sub-linear R̂t(δ).",
      "startOffset" : 13,
      "endOffset" : 24
    }, {
      "referenceID" : 14,
      "context" : "The any-time high probability algorithm of Neu (2015) adapted with our safe-playing strategy gives R̂t = 7 √ Kt log K log(4t2/δ) and",
      "startOffset" : 43,
      "endOffset" : 54
    }, {
      "referenceID" : 11,
      "context" : "The theorem above almost follows from the lower bound given by Lattimore (2015a), but in that paper μ0 is unknown, while here it may be known.",
      "startOffset" : 63,
      "endOffset" : 81
    }, {
      "referenceID" : 11,
      "context" : "We evaluate the performance of Conservative UCB compared to UCB and Unbalanced MOSS Lattimore (2015a) using simulated data in two regimes.",
      "startOffset" : 84,
      "endOffset" : 102
    }, {
      "referenceID" : 11,
      "context" : "The quantity Bi determines the regret of the algorithm with respect to arm i up to constant factors, and must be chosen to lie inside the Pareto frontier given by Lattimore (2015a). It should be emphasised that Unbalanced MOSS does not constraint the return except for the last round, and has no high-probability guarantees.",
      "startOffset" : 163,
      "endOffset" : 181
    } ],
    "year" : 2016,
    "abstractText" : "We study a novel multi-armed bandit problem that models the challenge faced by a company wishing to explore new strategies to maximize revenue whilst simultaneously maintaining their revenue above a fixed baseline, uniformly over time. While previous work addressed the problem under the weaker requirement of maintaining the revenue constraint only at a given fixed time in the future, the algorithms previously proposed are unsuitable due to their design under the more stringent constraints. We consider both the stochastic and the adversarial settings, where we propose, natural, yet novel strategies and analyze the price for maintaining the constraints. Amongst other things, we prove both high probability and expectation bounds on the regret, while we also consider both the problem of maintaining the constraints with high probability or expectation. For the adversarial setting the price of maintaining the constraint appears to be higher, at least for the algorithm considered. A lower bound is given showing that the algorithm for the stochastic setting is almost optimal. Empirical results obtained in synthetic environments complement our theoretical findings.",
    "creator" : "LaTeX with hyperref package"
  }
}