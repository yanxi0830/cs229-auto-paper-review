{
  "name" : "1401.7898.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n40 1.\n78 98\nv1 [\ncs .L\nG ]\n3 0\nJa n\n√ k, our bound is exponentially sharper. From the\nalgorithmic standpoint, in doubling metric spaces our classifier may be trained on n examples in O(n2 log n) time and evaluated on new points in O(log n) time."
    }, {
      "heading" : "1 Introduction",
      "text" : "Whereas the theory of supervised binary classification is by now fairly well developed, its multiclass extension continues to pose numerous novel statistical and computational challenges. On the algorithmic front, there is the basic question of how to adapt the hyperplane and kernel methods — ideally suited for two classes — to three or more. A host of new problems also arises on the statistical front. In the binary case, the VC-dimension characterizes the distribution-free sample complexity (Anthony & Bartlett, 1999) and tighter distribution-dependent bounds are available via Rademacher techniques (Bartlett & Mendelson, 2002; Koltchinskii & Panchenko, 2002). Characterizing the multiclass distribution-free sample complexity is far less straightforward, though impressive progress has been recently made (Daniely et al., 2011).\nFollowing von Luxburg & Bousquet (2004); Gottlieb et al. (2010), we adopt a proximity-based approach to supervised multicategory classification in metric spaces. The principal motivation for this framework is two-fold:\n(i) Many natural metrics, such as L1, earthmover, and edit distance cannot be embedded in a Hilbert space without a large distortion (Enflo, 1969; Naor & Schechtman, 2007; Andoni & Krauthgamer, 2010). Any kernel\nmethod is thus a priori at a disadvantage when learning to classify nonHilbertian objects, since it cannot faithfully represent the data geometry.\n(ii) Nearest neighbor-based classification sidesteps the issue of k-to-binary reductions — which, despite voluminous research, is still the subject of vigorous debate (Rifkin & Klautau, 2004; El-Yaniv et al., 2008). In terms of time complexity, the reductions approach faces an Ω(k) informationtheoretic lower bound (Beygelzimer et al., 2009), while nearest neighbors admit solutions whose runtime does not depend on the number of classes.\nMain results. Our contributions are both statistical and algorithmic in nature. On the statistical front, we open with the observation that the nearestneighbor classifier’s expected risk is at most twice the Bayes optimal plus a term that decays with sample size at a rate not dependent on the number of classes k (and continues to hold for k = ∞, Theorem 1). Although of interest as apparently the first “k-free” finite-sample result, it has the drawback of being non-adaptive in the sense of depending on properties of the unknown sampling distribution and failing to provide the learner with a usable data-dependent bound. This difficulty is overcome in our main technical contribution (Theorems 4 and 5), where we give a margin-based multiclass bound of order\nmin\n{ 1\nγ\n( log k\nn\n) 1 D+1\n, 1\nγ D 2\n( log k\nn\n) 1 2 } , (1)\nwhere k is the number of classes, n is sample size, D is the doubling dimension of the metric instance space and 0 < γ ≤ 1 is the margin. This matches the state of the art asymptotics in n for metric spaces and significantly improves the dependence on k, which hitherto was of order √ k (Zhang, 2002, 2004) or worse. The exponential dependence on some covering dimension (such as D) is in general inevitable, as shown by a standard no-free-lunch argument (Ben-David & Shalev-Shwartz, 2014), but whether (1) is optimal remains an open question.\nOn the algorithmic front, using the above bounds, we show how to efficiently perform Structural Risk Minimization (SRM) so as to avoid overfitting. This involves deciding how many and which sample points one is allowed to err on. We reduce this problem to minimal vertex cover, which admits a greedy 2-approximation. Our algorithm admits a significantly faster ε-approximate version in doubling spaces with a graceful degradation in ε of the generalization bounds, based on approximate nearest neighbor techniques developed by Gottlieb et al. (2010, 2013a). For a fixed doubling dimension and ε, our runtime is O(n2 logn) for learning and O(log n) for evaluation on a test point. (Exact nearest neighbor requires Θ(n) evaluation time.) Finally, our generalization bounds and algorithm can be made adaptive to the intrinsic dimension of the data via a recent metric dimensionality-reduction technique (Gottlieb et al., 2013b).\nRelated work. Due to space constraints, we are only able to mention the most directly relevant results — and even these, not in full generality but rather with an eye to facilitating comparison to the present work. Supervised k-category classification approaches follow two basic paradigms: (I) defining a score function on point-label pairs and classifying by choosing the label with the optimal score and (II) reducing the problem to several binary classification problems. Regarding the second paradigm, the seminal paper of Allwein et al. (2001) unified the various error correcting output code (ECOC)-based multiclass-to-binary reductions under a single margin-based framework. Their generalization bound requires the base classifier to have VC-dimension dVC < ∞ (and hence does not apply to nearest neighbors or infinite-dimensional Hilbert spaces) and is of the form Õ (\nlog k γ √ dVC n ) . Langford & Beygelzimer (2005); Beygelzimer et al.\n(2009) gave k-free and O(log k) regret bounds, but these are conditional on the performance of the underlying binary classifiers as opposed to the unconditional bounds we provide in this paper.\nAs for the first paradigm, proximity is perhaps the most natural score function — and indeed, a formal analysis of the nearest neighbor classifier (Cover & Hart, 1967) much predated the first multiclass extensions of SVM (Weston & Watkins, 1999). Crammer & Singer (2002a,b) considerably reduced the computational complexity of the latter approach and gave a risk bound decaying as Õ(k2/nγ2), for the separable case with margin γ. In an alternative approach based on choosing q prototype examples, Crammer et al. (2002) gave a risk bound with rate Õ(qk/2/γ √ n). Ben-David et al. (1995) characterized the PAC learnability of k-valued functions in terms of combinatorial dimensions, such as the Natarajan dimension dNat. Guermeur (2007, 2010) gave scale-sensitive analogues of these dimensions. He gave a risk bound decaying\nas Õ (\nlog k γ √ dγNat/n ) , where dγNat is a scale-sensitive Natarajan dimension —\nessentially replacing the finite VC dimension dVC in Allwein et al. (2001) by dγNat. He further showed that for linear function classes in Hilbert spaces, dγNat is bounded by Õ(k2/γ2), resulting in a risk bound decaying as Õ(k/γ2 √ n). To the best of our knowledge, the sharpest current estimate on the Natarajan dimension (for some special function classes) is dNat = Õ(k) with a matching lower bound of Ω(k) (Daniely et al., 2011). A margin-based Rademacher analysis of score functions (Mohri et al., 2012) yields a bound of order Õ(k2/γ √ n), and this is also the k-dependence obtained by Cortes et al. (2013) in a recent paper proposing a multiple kernel approach to multiclass learning. Closest in spirit to our work are the results of Zhang (2002, 2004), who used the chaining technique to achieve a Rademacher complexity with asymptotics Õ (\n1 γ √ k n ) .\nBesides the dichotomy of score functions vs. multiclass-to-binary reductions outlined above, multicategory risk bounds may also be grouped by the trichotomy of (a) combinatorial dimensions (b) Hilbert spaces (c) metric spaces (see Table 1). Category (a) is comprised of algorithm-independent results that give generalization bounds in terms of some combinatorial dimension of a fixed concept class (Allwein et al., 2001; Ben-David et al., 1995; Guermeur, 2007,\n2010; Daniely et al., 2011). Multiclass extensions of SVM and related kernel methods (Weston & Watkins, 1999; Crammer & Singer, 2002a,b; Crammer et al., 2002; Cortes et al., 2013) fall into category (b). Category (c), consisting of agnostic1 metric-space methods is the most sparsely populated. The pioneering asymptotic analysis of Cover & Hart (1967) was cast in a modern, finite-sample version by Ben-David & Shalev-Shwartz (2014), but only for binary classification. Unlike Hilbert spaces, which admit dimension-free margin bounds, we are not aware of any metric space risk bound that does not explicitly depend on some metric dimension D or covering numbers. The bounds in Ben-David & Shalev-Shwartz (2014); Gottlieb et al. (2013b) exhibit a characteristic “curse of dimensionality” decay rate of O(n−1/(D+1)), but more optimistic asymptotics can be obtained (Guermeur, 2007, 2010; Zhang, 2002, 2004; Gottlieb et al., 2010). Although some sample lower bounds for proximity-based methods are known (Ben-David & Shalev-Shwartz, 2014), the optimal dependence on D and k is far from being fully understood."
    }, {
      "heading" : "2 Preliminaries",
      "text" : "Metric Spaces. Given two metric spaces (X , d) and (Z, ρ), a function f : X → Z is called L-Lipschitz if ρ(f(x), f(x′)) ≤ Ld(x, x′) for all x, x′ ∈ X . (The real line R is always considered with its Euclidean metric |·|.) The Lipschitz constant of f , denoted ‖f‖\nLip , is the smallest L for which f is L-Lipschitz. The\ndistance between two sets A,B ⊂ X is defined by d(A,B) = infx∈A,x′∈B d(x, x′). For a metric space (X , d), let λ be the smallest value such that every ball in\n1 in the sense of not requiring an a priori fixed concept class\nX can be covered by λ balls of half the radius. The doubling dimension of X is ddim(X ) := log2 λ. A metric is doubling when its doubling dimension is bounded. The ε-covering number of a metric space (X , d), denoted N (ε,X , d), is defined as the smallest number of balls of radius ε that suffices to cover X . It can be shown (e.g., Krauthgamer & Lee (2004)) that\nN (ε,X , d) ≤ ( 2 diam(X )\nε\n)ddim(X ) , (2)\nwhere diam(X ) = sup x,x′∈X d(x, x′) is the diameter of X .\nThe multiclass learning framework. Let (X , d) be a metric instance space with diam(X ) = 1, ddim(X ) = D < ∞, and Y ⊆ N an at most countable label set. We observe a sample S = (Xi, Yi) n i=1 ∈ {X × Y}n drawn iid from an unknown distribution P over X × Y. In line with paradigm (I) outlined in the Introduction, our classification procedure consists of optimizing a score function. In hindsight, the score at a test point will be determined by its labeled neighbors, but for now, we consider an unspecified collection F of functions mapping X ×Y to R. A score function f ∈ F induces the classifier gf : X → Y via\ngf(x) = argmax y∈Y f(x, y), (3)\nbreaking ties arbitrarily. The margin of f ∈ F on (x, y) is defined by\nγf (x, y) = 1\n2\n( f(x, y)− sup\ny′ 6=y f(x, y′)\n) . (4)\nNote that gf misclassifies (x, y) precisely when γf (x, y) < 0. One of our main objectives is to upper-bound the generalization error\nP(gf (X) 6= Y ) = E[1{γf (X,Y )<0}].\nTo this end, we introduce two surrogate loss functions L : R → R+:\nLcutoff (u) = 1{u<1} Lmargin(u) = T[0,1](1− u),\nwhere\nT[a,b](z) = max {a,min {b, z}} (5)\nis the truncation operator. The empirical loss Ê[L(γf )] induced by any of the loss functions above is 1n ∑n i=1 L(γf (Xi, Yi)). All probabilities P(·) and expectations E[·] are with respect to the sampling distribution P . We will write ES to indicate expectation over a sample (i.e., over Pn)."
    }, {
      "heading" : "3 Risk bounds",
      "text" : "In this section we analyze the statistical properties of nearest-neighbor multicategory classifiers in metric spaces. In Section 3.1, Theorem 1, we record the observation that the 1-nearest neighbor classifier is nearly Bayes optimal, with a risk decay that does not depend on the number of classes k. Of course, the 1-naive nearest neighbor is well-known to overfit. This is reflected in the nonadaptive nature of the analysis: the bound is stated in terms of properties of the unknown sampling distribution, and fails to provide the learner with a usable data-dependent bound.\nTo achieve the latter goal, we develop a margin analysis in Section 3.2. Our main technical result is Lemma 2, from which the logarithmic dependence on k claimed in (1) follows. Although not k-free like the Bayes excess risk bound of Theorem 1, O(log k) is exponentially sharper than the current state of the art (Zhang, 2002, 2004). Whether a k-free metric entropy bound is possible is currently left as an open problem.\nThe metric entropy bound of Lemma 2 facilitates two approaches to bounding the risk: via Rademacher complexity (Section 3.2.2) and via scale-sensitive techniques in the spirit of Guermeur (2007) (Section 3.2.3). In Section 3.2.4 we combine these two margin bounds by taking their minimum. The resulting bound will be used in Section 4 to perform efficient Structural Risk Minimization."
    }, {
      "heading" : "3.1 Multiclass Bayes near-optimality",
      "text" : "In this section, (X , d) is a metric space and Y is an at most countable (possibly infinite) label set. A sample S = (Xi, Yi) n i=1 is drawn iid from an unknown distribution P over X ×Y. For x ∈ X let (Xπ1(x), Yπ1(x)) be its nearest neighbor in S:\nπ1(x) = argmin i∈[n] d(Xi, x).\nThus, the nearest-neighbor classifier gNN is given by\ngNN(x) = Yπ1(x). (6)\nDefine the function η : X → RY by\nη(x) = P(Y = · |X = x).\nThe Bayes optimal classifier g∗ — i.e., one that minimizes P(g(X) 6= Y ) over all measurable g ∈ YX — is well-known to have the form\ng∗(x) = argmax y∈Y ηy(x),\nwhere ties are broken arbitrarily. Our only distributional assumption is that η is L-Lipschitz with respect to the sup-norm. Namely, for all x, x′ ∈ X , we have\n‖η(x)− η(x′)‖∞ ≡ sup y∈Y |ηy(x)− ηy(x′)| ≤ Ld(x, x′).\nThis is a direct analogue of the Lipschitz assumption for the binary case (Cover & Hart, 1967; Ben-David & Shalev-Shwartz, 2014). We make the additional standard assumption that X has a finite doubling dimension: ddim(X ) = D < ∞. The Lipschitz and doubling assumptions are sufficient to extend the finite-sample analysis of binary nearest neighbors (Ben-David & Shalev-Shwartz, 2014) to the multiclass case:\nTheorem 1.\nES [P(gNN(X) 6= Y )] ≤ 2P(g∗(X) 6= Y ) + 4L\nn1/(D+1) .\nNote that the bound is independent of the number of classes k and holds even for k = ∞. The proof is deferred to Appendix A."
    }, {
      "heading" : "3.2 Multiclass margin bounds",
      "text" : "Here again (X , d) is a metric space, but now the label set Y is assumed finite: |Y| = k < ∞. As before, S = (Xi, Yi)ni=1 with (Xi, Yi) ∼ P iid. It will be convenient to write Sy = {Xi : Yi = y, i ∈ [n]} for the subset of examples with label y. The metric induces the natural score function fNN(x, y) = −d(x, Sy) with corresponding nearest-neighbor classifier\ngNN(x) = argmax y∈Y fNN(x, y), (7)\neasily seen to be identical to the one in (6). At this point we make the simple but crucial observation that the function fNN(·, y) : X → R is 1-Lipschitz. This will enable us to generalize the powerful Lipschitz extension framework of von Luxburg & Bousquet (2004) to |Y| > 2.\nWe will need a few definitions. Let FL be the collection of all L-Lipschitz functions from X to R and put FL = FL × Y. Since each f ∈ FL maps X × Y to R, the margin γf (x, y) is well-defined via (4). Putting\ny∗f (x) = argmax y∈Y f(x, y), γ∗f (x) = γf (x, y ∗ f (x)),\nwe define the projection Φf :\nΦf (x, y) =\n{ γ∗f (x), if y = y ∗ f (x)\n−γ∗f (x), otherwise.\nFinally, we define HL as the truncated (as in (5)) projections of functions in FL:\nHL = { (x, y) 7→ T[-1,1] (Φf (x, y)) : f ∈ FL } . (8)\nThus, HL is the set of functions hf : X × Y → [−1, 1], where each hf (·, y) is L-Lipschitz and hf(x, y) = ±T[-1,1](γ∗f (x)), depending upon whether y = y∗f (x) , see Figure 1 (left)."
    }, {
      "heading" : "3.2.1 Bounding the metric entropy",
      "text" : "Our main technical result is a bound on the metric entropy of HL, which will be used to obtain error bounds (Theorems 4 and 5) for classifiers derived from this function class. The analysis differs from previous bounds (see Table 1) by explicitly taking advantage of the mutual exclusive nature of the labels, obtaining an exponential improvement in terms of the number of classes k. Endow HL with the sup-norm\n‖ · ‖∞ = sup x∈X max y∈Y | · | .\nLemma 2. For any ε > 0,\nlogN (ε,HL, ‖ · ‖∞) ≤ ( 16L\nε\n)D log ( 5k\nε\n) .\nProof. By the definition of HL, for all hf ∈ HL and x ∈ X there is at most one y ∈ Y such that hf (x, y) > 0. In addition, if hf (x, y) = c > 0, then hf (x, y\n′) = −c for all y′ 6= y. Since γ∗f (x) ≥ 0, we may reparametrize hf (x, y) by (y∗f (x), γ ∗ f (x)) ∈ Y×[0, 1], see Figure 1. To complete the mapping hf 7→ (y∗f , γ∗f ), define the following star-like metric ρ over Y × [0, 1] (see Figure 2):\nρ((y, γ), (y′, γ′)) = { |γ − γ′| y = y′ γ + γ′ y 6= y′ .\nLet H̃L be the collection of functions h̃ : X → Y × [0, 1] that are L-Lipschitz: ρ(h̃(x), h̃(x′)) ≤ Ld(x, x′), x, x′ ∈ X .\nIt is easily verified that the metric space (HL, ‖ · ‖∞) is isometric to (H̃L, ρ∞) with\nρ∞(h̃, h̃ ′) = sup x∈X ρ(h̃(x), h̃′(x)).\nThus, N (ε,HL, ‖ · ‖∞) = N (ε, H̃L, ρ∞), and we proceed to bound the latter.2 Fix a covering of X consisting of |N | = N (ε/8L,X , d) balls {U1, . . . , U|N |} of radius ε′ = ε/8L and choose |N | points N = {xi ∈ Ui}|N |i=1. Construct Ĥ ⊂ H̃2L as follows. At every point xi ∈ N select one of the classes y ∈ Y and set ĥ(xi) = (y, γ(xi)) with γ(xi) some multiple of 2Lε\n′ = ε/4, while maintaining ‖ĥ‖Lip ≤ 2L. Construct a 2L-Lipschitz extension for ĥ from N to all over X (such an extension always exists, (McShane, 1934; Whitney, 1934)). We claim that every classifier in HL, via its twin h̃ ∈ H̃L, is close to some ĥ ∈ Ĥ, in the sense that ρ∞(h̃, ĥ) ≤ ε. Indeed, every point x ∈ X is 2ε′-close to some point xi ∈ N , and since h̃ is L-Lipschitz and ĥ is 2L-Lipschitz,\nρ(h̃(x), ĥ(x)) ≤ ρ(h̃(x), h̃(xi)) + ρ(h̃(xi), ĥ(xi))\n+ ρ(ĥ(xi), ĥ(x))\n≤ Ld(x, xi) + ε/4 + 2Ld(x, xi) ≤ ε.\nThus, Ĥ provides an ε-cover for H̃L (and hence for HL). Note that |Ĥ | ≤ (⌈4k/ε⌉ + 1)|N | , since by construction, functions ĥ are determined by their values on N , which at a given point can take one of ⌈4k/ε⌉+ 1 possible values. Since by (2) we have |N | = N (ε/8L,X , d) ≤ ( 16L ε )D the bound follows.\nA tighter bound is possible when the metric space (X , d) possesses two additional properties:\n1. (X , d) is connected if for all x, x′ ∈ X and all ε > 0, there is a finite sequence of points x = x1, x2, . . . , xm = x\n′ such that d(xi, xi+1) < ε for all 1 ≤ i < m.\n2. (X , d) is centered if for all r > 0 and all A ⊂ X with diam(A) ≤ 2r, there exists a point x ∈ X such that d(x, a) ≤ r for all a ∈ A.\nLemma 3. If (X , d) is connected and centered, then\nlogN (ε,HL, ‖ · ‖∞) = O (( L\nε\n)D log k + log ( 1\nε\n)) .\nProof. With the additional assumptions on X we follow the proof idea in Kolmogorov & Tikhomirov (1959) and demonstrate the tighter bound |Ĥ | ≤ (⌈4k/ε⌉+ 1)(2k + 1)|N |−1 =\n2The remainder of the proof is based on a technique communicated to us by R. Krauthgamer, a variant of the classic Kolmogorov & Tikhomirov (1959) method.\nO((2k)|N |/ε). Here Ĥ is constructed as in the proof for Lemma 2 but now each xi ∈ N is taken to be a “center” of Ui, as furnished by Property 2 above. Let xj ∈ N . Since X is connected, we may traverse a path from x1 to xj via the cover points x1 = xi1 , xi2 , . . . , xim = xj , such that the distance between any two successive points (xil , xil+1) is at most 2ε ′ = ε/4L. Since ĥ is 2L-Lipschitz, on any two such points the value of ĥ can change by at most ε/2. Thus, given\nthe value ĥ(xil), the value of ĥ(xil+1 ) can take one of at most 2k + 1 values (as Figure 2 shows, at the star’s hub, ĥ(xil+1 ) can take one of 2k + 1 values, while at one of the spokes only 5 values are possible). So we are left to choose the value of ĥ on the point x1 to be one from the ⌈4k/ε⌉+ 1 possible values. The bounds on |Ĥ | and the metric entropy follow."
    }, {
      "heading" : "3.2.2 Rademacher analysis",
      "text" : "The Rademacher complexity of the set of functions HL is defined by\nRn(HL) = E [ sup\nh∈HL\n1\nn\nn∑\ni=1\nσih(Xi, Yi)\n] , (9)\nwhere the σi are n independent random variables with P(σi = +1) = P(σi = −1) = 1/2. In Appendix B, we invoke Lemma 2 to derive the bound\nRn(HL) ≤ 2L ( log 5k\nn\n)1/(D+1) , (10)\nwhich in turn implies “half” of our main risk estimate (1):\nTheorem 4. With probability at least 1 − δ, for all L > 0 and every f ∈ FL with its projected version hf ∈ HL,\nP(gf(X) 6= Y ) ≤ Ê [L(hf )] + ∆Rad(n, L, δ),\nwhere gf is the classifier defined in (3), L is any of the loss functions defined in Section 2 and ∆Rad(n, L, δ) is at most\n8L\n( log 5k\nn\n) 1 D+1\n+\n√( log log2 2L\nn\n)\n+\n+ √ log 2δ 2n ."
    }, {
      "heading" : "3.2.3 Scale-sensitive analysis",
      "text" : "The following Theorem , proved in Appendix C, is an adaptation of Guermeur (2007, Theorem 1), using Lemma 2.\nTheorem 5. With probability at least 1 − δ, for all L > 0 and every f ∈ FL with its induced hf ∈ HL,\nP(gf (X) 6= Y ) ≤ Ê [Lcutoff(hf )] + ∆fat(n, L, δ),\nwhere ∆fat(n, L, δ) is at most\n√ 2\nn\n( 2 (16L) D log (20k) + ln ( 2L\nδ\n)) + 1\nn ."
    }, {
      "heading" : "3.2.4 Combined Bound",
      "text" : "Taking L = Lcutoff in Theorem 4 we can merge the above two bounds by taking their minimum. Namely, Theorem 5 holds with ∆(n, L, δ) = min {∆Rad(n, L, δ),∆fat(n, L, δ)} in place of ∆fat(n, L, δ), see Figure 3. The resulting risk decay rate is of order\nmin { L ( log k\nn\n) 1 D+1\n, L D 2\n( log k\nn\n) 1 2 } ,\nas claimed in (1). In terms of the number of classes k, our bound compares favorably to those in Allwein et al. (2001); Guermeur (2007, 2010), and more recently in Daniely et al. (2011), which have a k-dependence of O(dNat log k), where dNat is the (scale-sensitive, k-dependent) Natarajan dimension of the multiclass hypothesis class. The optimal dependence of the risk on k is an intriguing open problem."
    }, {
      "heading" : "4 Algorithm",
      "text" : "Theorems 4 and 5 yield generalization bounds of the schematic form\nP(g(X) 6= Y ) ≤ Ê[L] + ∆(n, L, δ). (11)\nThe free parameter L in (11) controls (roughly speaking) the bias-variance tradeoff: for larger L, we may achieve a smaller empirical loss Ê[L] at the expense\nof a larger hypothesis complexity ∆(n, L, δ). Our Structural Risk Minimization (SRM) consists of seeking the optimal L — i.e., one that minimizes the right-hand side of (11) — via the following high-level procedure:\n1. For each L > 0, minimize Ê[L(hf )] over f ∈ FL.\n2. Choose the optimal L∗ and its corresponding classifier gf with f ∈ FL∗ .\nMinimizing the empirical loss. Let S = (Xi, Yi) n i=1 be the training sample and L > 0 a given maximal allowed Lipschiz constant. We will say that a function h ∈ HL is inconsistent with a sample point (x, y) if h(x, y) < 1 (i.e., if the margin of h on (x, y) is less than one). Denote by m̂(L) the smallest possible number of sample points on which a function h ∈ HL may be inconsistent:\nm̂(L) = min h∈HL\nÊ[Lcutoff (h)].\nThus, our SRM problem consists of finding\nL∗ = argmin L>0 {m̂(L) + ∆(n, L, δ)} .\nFor k = 2, Gottlieb et al. (2010) reduced the problem of computing m̂(L) to one of finding a minimal vertex cover in a bipartite graph (by König’s theorem, the latter is efficiently computable as a maximal matching). We will extend this technique to k > 2 as follows. Define the k-partite graph GL = ({V y}ky=1, E), where each vertex set V y corresponds to the sample points Sy with label y. Now in order for h ∈ HL to be consistent with the points (Xi, Yi) and (Xj , Yj) for Yi 6= Yj , the following relation must hold:\nLd(Xi, Xj) ≥ 2. (12)\nHence, we define the edges of GL to consist of all point pairs violating (12):\n(Xi, Xj) ∈ E ⇐⇒ (Yi 6= Yj) ∧ (d(Xi, Xj) < 2/L).\nSince removing either of Xi, Xj in (12) also deletes the violating edge, m̂(L) is by construction equivalent to the size of the minimum vertex cover for GL. Although minimum vertex cover is NP-hard to compute (and even hard to approximate within a factor of 1.3606, (Dinur & Safra, 2005)), a 2-approximation may be found in O(n2) time (Papadimitriou & Steiglitz, 1998). This yields a 2-approximation m̃(L) for m̂(L).\nOptimizing over L. Equipped with an efficient routine for computing m̃(L) ≤ 2m̂(L), we now seek an L > 0 that minimizes\nQ(L) := m̃(L) + ∆(n, L, δ). (13)\nSince the Lipschitz constant induced by the data is determined by the ( n 2 ) distances among the sample points, we need only consider O(n2) values of L.\nRather a brute-force searching all of these values, Theorem 7 of Gottlieb et al. (2010) shows that using an O(log n) time binary search over the values of L, one may approximately minimizeQ(L), which in turn yields an approximate solution to (11). The resulting procedure has runtime O(n2 logn) and guarantees an L̃ for which\nQ(L̃) ≤ 4 [m̂(L∗) + ∆(n, L∗, δ)] . (14)\nClassifying test points. Given the nearly optimal Lipschitz constant L̃ computed above we construct the approximate (within a factor of 4) empirical risk minimizer h∗ ∈ HL̃. The latter partitions the sample into S = S0 ∪ S1, where S1 consists of the points on which h\n∗ is consistent and S0 = S \\ S1. Evaluating h∗ on a test point amounts to finding its nearest neighbor in S1. Although in general metric spaces, nearest-neighbors search requires Ω(n) time, for doubling spaces, an exponential speedup is available via approximate nearest neighbors (see Section 5)."
    }, {
      "heading" : "5 Extensions",
      "text" : "In this section, we discuss two approaches that render the methods presented above considerably more efficient in terms of runtime and generalization bounds. The first is based on the fact that in doubling spaces, hypothesis evaluation time may be reduced fromO(n) to O(log n) at the expense of a very slight degradation of the generalization bounds. The second relies on a recent metric dimensionality reduction result. When the data is “close” to being D̃-dimensional, with D̃ much smaller than the ambient metric space dimension D, both the evaluation runtime and the generalization bounds may be significantly improved — depending essentially on D̃ rather than D."
    }, {
      "heading" : "5.1 Exponential speedup via approximate NN",
      "text" : "If (X , d) is a metric space and x∗ ∈ E ⊂ X is a minimizer of d(x, x′) over x′ ∈ E, then x∗ is a nearest neighbor of x in E. A simple information-theoretic argument shows that the time complexity of computing an exact nearest neighbor in general metric spaces has Ω(n) time complexity. However, an exponential speedup is possible if (i) X is a doubling space and (ii) one is willing to settle for approximate nearest neighbors. A (1+η) nearest neighbor oracle returns an x̃ ∈ E such that\nd(x, x∗) ≤ d(x, x̃) ≤ (1 + η)d(x, x∗). (15)\nWe will use the fact that in a doubling space, one may precompute a (1+ η) nearest neighbor data structure in (2O(ddim(X )) logn+ η−O(ddim(X )))n time and evaluate it on a test point in 2O(ddim(X )) logn+η−O(ddim(X )) time Cole & Gottlieb (2006); Har-Peled & Mendel (2006). The approximate nearest neighbor oracle induces an η-approximate version of gNN in defined (7). After performing SRM\nas described in Section 4, we are left with a subset S1 ⊂ S of the sample, which will be used to label test points. More precisely, the predicted label of a test point will be determined by its η-nearest neighbor in S1.\nThe exponential speedup afforded by approximate nearest neighbors comes at the expense of mildly degraded generalization guarantees. The modified generalization bounds are derived in three steps, whose details are deferred to Appendix D: (i) We cast the evaluation of h ∈ HL in (8) as a nearest neighbor calculation with a corresponding h̃ induced by the (1 + η) approximate nearest neighbor oracle. The nearest-neighbor formulation of h is essentially the one obtained by von Luxburg & Bousquet (2004):\nh(x, y) = 1\n2 ( min S1 {ξ(y, y′) + Ld(x, x′)} (16)\n+ max S1\n{ξ(y, y′)− Ld(x, x′)} ) ,\nwhere (x′, y′) ∈ S1 and ξ(y, y′) = 21{y=y′} − 1. (ii) We observe a simple relation between h and h̃:\n‖h− h̃‖∞ ≡ sup x∈X ,y∈Y |h(x, y)− h̃(x, y)| ≤ 2η.\n(iii) Defining the 2η-perturbed function class\nHL,2η = {T[-1,1](h′) : ‖h′ − h‖∞ ≤ 2η, h ∈ HL},\nwe relate its metric entropy to that of HL:\nLemma 6. For ε > 2η > 0, we have\nN (ε,HL,2η, ‖ · ‖∞) ≤ N (ε− 2η,HL, ‖ · ‖∞).\nThe metric entropy estimate for HL,2η readily yields η-perturbed versions of Theorems 4 and 5. From the standpoint of generalization bounds, the effect of the η-perturbation on HL amounts, roughly speaking, to replacing L by L(1 + O(η)), which constitutes a rather benign degradation."
    }, {
      "heading" : "5.2 Adaptive dimensionality reduction",
      "text" : "The generalization bound in (1) and the runtime of our sped-up algorithm in Section 5.1 both depend exponentially on the doubling dimension of the metric space. Hence, even a modest dimensionality reduction could lead to dramatic savings in algorithmic and sample complexities. The standard Euclidean dimensionality-reduction tool, PCA, until recently had no metric analogue — at least not with rigorous performance guarantees. The technique proposed in Gottlieb et al. (2013b) may roughly be described as a metric analogue of PCA.\nA setX = {x1, . . . , xn} ⊂ X inherits the metric d of X and hence ddim(X) ≤ ddim(X ) is well-defined. We say that X̃ = {x̃1, . . . , x̃n} ⊂ X is an (α, β)perturbation of X if ∑n i=1 d(xi, x̃i) ≤ α and ddim(X̃) ≤ β. Intuitively, the data is “essentially” low-dimensional if it admits an (α, β)-perturbation with small α, β, which leads to improved Rademacher estimates. The empirical Rademacher complexity of HL on a sample S = (X,Y ) ∈ Xn × Yn is given by\nR̂n(HL;S) = E [\nsup h∈HL\n1\nn\nn∑\ni=1\nσih(Xi, Yi) ∣∣∣∣∣S ]\nand is related to Rn defined in (9) via\nRn(HL) = ES [ R̂n(HL;S) ]\nP (∣∣∣Rn − R̂n ∣∣∣ ≥ ε ) ≤ 2 exp(−ε2n/2),\nwhere the identity is obvious and the inequality is a simple consequence of measure concentration (Mohri et al., 2012). Hence, up to small changes in constants, the two may be used in generalization bounds such as Theorem 4 interchangeably. The data-dependent nature of R̂n lets us exploit essentially low-dimensional data (see Appendix E):\nTheorem 7. Let S = (X,Y ) ∈ Xn × Yn be the training sample and suppose that X admits an (α, β)-perturbation X̃. Then\nR̂n(HL;S) = O ( L ( α+ ( log k\nn\n) 1 1+β )) . (17)\nA pleasant feature of the bound above is that it does not depend on ddim(X ) (the dimension of the ambient space) or even on ddim(X) (the dimension of the data). Note the inherent tradeoff between the distortion α and dimension β, with some non-trivial (α∗, β∗) minimizing the right-hand side of (17). Although computing the optimal (α∗, β∗) seems computationally difficult, Gottlieb et al. (2013b) were able to obtain an efficient (O(1), O(1))-bicriteria approximation. Namely, their algorithm computes an α̃ ≤ c0α∗ and β̃ ≤ c1β∗, with the corresponding perturbed set X̃, for universal constants c0, c1, with a runtime of 2O(ddim(X))n logn+O(n log5 n).\nThe optimization routine over (α, β) may then be embedded inside our SRM optimization over the Lipschitz constant L in Section 4. The end result will be a nearly optimal (in the sense of (14)) Lipschitz constant L̃, which induces the partition S = S0 ∪ S1, as well as (α̃, β̃), which induce the perturbed set S̃1. To evaluate our hypothesis on a test point, we may invoke the (1+ η)-approximate nearest-neighbor routine from Section 5.1. This involves a precomputation of time complexity (2O(β̃) logn + η−O(β̃))n, after which new points are classified in 2O(β̃) log n+ η−O(β̃) time. Note that the evaluation time complexity depends only on the “intrinsic dimension” β̃ of the data, rather than the ambient metric space dimension."
    }, {
      "heading" : "A Bayes near-optimality proof",
      "text" : "Proof of Theorem 1. Since η is L-Lipschitz, given x, x′ ∈ X we have\nP (Y 6= Y ′ |x,x′) = ∑\nj∈Y ηj(x)(1 − ηj(x′)) (18)\n≤ ∑\nj\nηj(x) ( 1− ηj(x) + Ld(x, x′) )\n= ∑\nj\nηj(x) ( 1− ηj(x) ) + Ld(x, x′).\nBy the definition of the nearest neighbor classifier gNN in (6) we have ES [P (gNN(X) 6= Y )] = ES [P (Yπ1(X) 6= Y )], where the expectation is over the sample S determining gNN. By (18) this error is bounded above by\nES,X [ ∑\nj\nηj(X)(1− ηj(X))] + LES,X [d(X,Xπ1(X))],\nwhere now the expectation is over S and X . Denoting k′ = argmaxj ηj(X) and splitting the sum , the first term (which does not depend on S) satisfies\nEX [ηk′(X)(1−ηk′ (X))] + EX [ ∑\nj 6=k′ ηj(X)(1− ηj(X))]\n≤ EX [1− ηk′(X)] + EX [ ∑\nj 6=k′ ηj(X)]\n= 2EX [1− ηk′ (X)] = 2P (g∗(X) 6= Y ).\nIt remains to bound ES,X [d(X,Xπ1(X))] and we proceed exactly as in Ben-David & Shalev-Shwartz (2014). Let {C1, . . . , CN} be an ε-cover of X of cardinality N = N (ε,X , d). Given a sample S, for x ∈ Ci such that S ∩ Ci 6= ∅ we have d(x,Xπ1(x)) < ε,\nwhile for x ∈ Ci such that S∩Ci = ∅ we have d(x,Xπ1(x)) ≤ diam(X ) = 1, thus ES,X [d(X,Xπ1(X))] is bounded above by\n≤ ES [ N∑\ni=1\nP (Ci) ( ε1{S∩Ci 6=∅} + 1{S∩Ci=∅}\n) ]\n=\nN∑\ni=1\nP (Ci) ( εES [ 1{S∩Ci 6=∅} ] + ES [ 1{S∩Ci=∅} ]) .\nSince P (Ci)ES [1S∩Ci=∅] = P (Ci)(1 − P (Ci))n ≤ 1/en and N = N (ε,X , d) we get\nES,X [d(X,Xπ1(X))] ≤ ε+ N (ε,X , d)\nen\n≤ ε+ 1 en\n( 2\nε\n)D .\nSetting ε = 2n− 1 D+1 concludes the proof."
    }, {
      "heading" : "B Rademacher analysis proofs",
      "text" : "Proof of inequality (10). Dudley’s chaining integral (Dudley, 1967) bounds from above the Rademacher complexity Rn(HL) by\ninf α>0\n( 4α+ 12 ∫ ∞\nα\n√ logN (t,HL, ‖ · ‖∞)\nn dt\n) .\nBy Lemma 2 the integral can be bounded as follows:\n∫ ∞\nα\n√ logN (t,HL, ‖ · ‖∞)\nn dt\n≤ ∫ ∞\nα\n√ 1\nn\n( 16L\nt\n)D log ( 5k\nt\n) dt\n≤ ∫ ∞\nα\n√ log 5k\nn\n( 16L\nt\n)D ( 1\nt\n) dt\n=\n√ log 5k\nn (16L)\nD/2 ∫ ∞\nα\n( 1\nt\n)(D+1)/2 dt\n=\n√ log 5k\nn (16L)\nD/2\n( 2\nD − 1\n)( 1\nα(D−1)/2\n) ,\nwhere in the second inequality we used the fact that for x ∈ (0, 1] and c ≥ e we have log( cx) ≤ log c x . Choosing\nα∗ = ( 9(16L)D log 5k\nn\n)1/(D+1)\nyields the bound.\nProof of Theorem 4. An adaptation3 of Mohri et al. (2012, Theorem 4.5) to HL states that with probability 1− δ, for all L > 0, h ∈ HL,\nE[Lmargin(h)] ≤ Ê[Lmargin(h)] + 4Rn(HL)\n+\n√( log log2 2L\nn\n)\n+\n+ √ log 2δ 2n .\nSince 1{u<0} ≤ Lmargin(u) we have P (gh(X) 6= Y ) ≤ E[Lmargin(h)]. Since Lmargin(u) ≤ Lcutoff(u) we can replace Lmargin in the empirical loss by the loss function Lcutoff . Bounding Rn(HL) using (10) concludes the proof."
    }, {
      "heading" : "C Scale sensitive analysis proof",
      "text" : "Proof of Theorem 5. An application4 of Guermeur (2010, Theorem 1) states that with probability 1− δ, for all L > 0, h ∈ HL,\nP (gh(X) 6= Y ) ≤ 1\nn\nn∑\ni=1\n1{h(Xi,Yi)<1}\n+\n√ 2\nn\n( 2 logN (1/4,HL, ‖ · ‖∞) + ln ( 2L\nδ\n)) + 1\nn .\nApplying the metric entropy bound in Lemma 2 proves the Theorem."
    }, {
      "heading" : "D Approximate NN proofs",
      "text" : "First, we will show that h̃ is indeed a 2η additive perturbation of h, i.e.\n‖h− h̃‖∞ ≤ 2η. (19)\nInstead of working directly with (16) we consider the following L-Lipschitz extension\nh(x, y) = 1\n2 T[-1,1] ( min S1 {ξ(Yi, y) + Ld(Xi, x)} )\n+ 1\n2 T[-1,1] ( max S1 {ξ(Yi, y)− Ld(Xi, x)} ) ,\neasily seen to induce the same classifier gh as (16). Consider the first term (the second term is treated similarly) and its approximate version:\nh̃(x, y) = T[-1,1] ( min S1 { ξ(Yi, y) + Ld̃(Xi, x) }) ,\n3essentially setting α = 1 in Mohri et al. (2012) and doing the stratification on L instead 4setting γ = 1 in Guermeur (2010, Theorem 1) and doing the stratification on L instead\nwhere d ≤ d̃ ≤ (1+η)d, given in (15), is the approximate ”‘distance”’ as provided by the approximate nearest neighbor. For notational convenience, denote\nh(x, y) = T[-1,1](min i qi(x, y))\nh̃(x, y) = T[-1,1](min i q̃i(x, y))\nqi(x, y) = hi(y) + ri(x)\nq̃i(x, y) = h̃i(y) + r̃i(x),\nwhere hi(y) = ξ(Yi, y), ri(x) = Ld(Xi, x), and h̃i, r̃i defined analogously. Observe that if r̃i(x) > 2 then ri(x) > 2/(1+η) ≥ 2(1−η). In this case, since h has range in [−1, 1], the eventual application of truncation operator T[-1,1] will force h̃(x, y)−h(x, y) ≤ 2η. Hence, we may assume that r̃i(x) ≤ 2 and so ri(x) ≤ 2. It is straightforward to verify that for a, b ∈ Rn with maxi∈[n] |ai − bi| ≤ η, we have\n∣∣∣T[-1,1](min i ai)− T[-1,1](min i bi) ∣∣∣ ≤ η.\nThus, establishing |qi(x, y)− q̃i(x, y)| ≤ 2η for all i ∈ [|S1|] and y ∈ Y with r̃i(x), ri(x) ≤ 2 suffices to prove the claim. Indeed, by (15) we have\n|ri(x) − r̃i(x)| ≤ |ri(x)− (1 + η)ri(x)| ≤ 2η.\nProof of Lemma 6. Suppose h̃ ∈ HL,η. By the definition of HL,η, there exists an h ∈ HL such that ‖h̃ − h‖∞ ≤ η. Let h′ be some element in a minimal ε-cover of HL so that ‖h− h′‖∞ ≤ ε. Then\n‖h̃− h′‖∞ ≤ ‖h̃− h‖∞ + ‖h− h′‖∞ ≤ ε+ η.\nHence, N (ε+ η,HL,η, ‖ · ‖∞) ≤ N (ε,HL, ‖ · ‖∞),\nwhence the claim follows."
    }, {
      "heading" : "E Dimensionality reduction proof",
      "text" : "Proof of Theorem 7. Put S̃ = (X̃, Y ). For Xi ∈ X and X̃i ∈ X̃ , define δi(h) = h(Xi, Yi)− h(X̃i, Yi). Then\nR̂n(HL;S) = E [\nsup h∈HL\n1\nn\nn∑\ni=1\nσih(Xi, Yi) ∣∣∣∣∣S ]\n= E [ sup h∈HL 1 n n∑\ni=1\nσi ( h(X̃i, Yi)− δi(h) ) ∣∣∣∣∣S ]\n≤ R̂n(HL; S̃) + E [\nsup h∈HL\n1\nn\nn∑\ni=1\nσiδi(h) ∣∣∣∣∣S ] .\nBy (10), we have\nRn(HL; S̃) ≤ 2L ( log 5k\nn\n)1/(β+1) . (20)\nSince by construction h is L-Lipschitz in its first argument, we have\n∣∣∣∣∣ n∑\ni=1\nσiδi(h) ∣∣∣∣∣ ≤ n∑\ni=1\n|δi(h)| ≤ L n∑\ni=1\nd(Xi, X̃i) ≤ Lα. (21)\nOur claimed bound follows from (20) and (21)."
    } ],
    "references" : [ {
      "title" : "Reducing multiclass to binary: A unifying approach for margin",
      "author" : [ "Allwein", "Erin L", "Schapire", "Robert E", "Singer", "Yoram" ],
      "venue" : "classifiers. JMLR,",
      "citeRegEx" : "Allwein et al\\.,? \\Q2001\\E",
      "shortCiteRegEx" : "Allwein et al\\.",
      "year" : 2001
    }, {
      "title" : "The computational hardness of estimating edit distance. SICOMP",
      "author" : [ "A. Andoni", "R. Krauthgamer" ],
      "venue" : null,
      "citeRegEx" : "Andoni and Krauthgamer,? \\Q2010\\E",
      "shortCiteRegEx" : "Andoni and Krauthgamer",
      "year" : 2010
    }, {
      "title" : "Neural network learning: theoretical foundations",
      "author" : [ "M. Anthony", "P. Bartlett" ],
      "venue" : null,
      "citeRegEx" : "Anthony and Bartlett,? \\Q1999\\E",
      "shortCiteRegEx" : "Anthony and Bartlett",
      "year" : 1999
    }, {
      "title" : "Rademacher and gaussian complexities: Risk bounds and structural results",
      "author" : [ "P. Bartlett", "S. Mendelson" ],
      "venue" : "JMLR, 3:463-482,",
      "citeRegEx" : "Bartlett and Mendelson,? \\Q2002\\E",
      "shortCiteRegEx" : "Bartlett and Mendelson",
      "year" : 2002
    }, {
      "title" : "Understanding machine learning: From theory to algorithms",
      "author" : [ "S. Ben-David", "S. Shalev-Shwartz" ],
      "venue" : null,
      "citeRegEx" : "Ben.David and Shalev.Shwartz,? \\Q2014\\E",
      "shortCiteRegEx" : "Ben.David and Shalev.Shwartz",
      "year" : 2014
    }, {
      "title" : "Characterizations of learnability for classes",
      "author" : [ "S. Ben-David", "N. Cesa-Bianchi", "D. Haussler", "P. Long" ],
      "venue" : "n}-valued functions. J. Comput. System Sci.,",
      "citeRegEx" : "Ben.David et al\\.,? \\Q1995\\E",
      "shortCiteRegEx" : "Ben.David et al\\.",
      "year" : 1995
    }, {
      "title" : "Searching dynamic point sets in spaces with bounded doubling dimension",
      "author" : [ "R. Cole", "L. Gottlieb" ],
      "venue" : null,
      "citeRegEx" : "Cole and Gottlieb,? \\Q2006\\E",
      "shortCiteRegEx" : "Cole and Gottlieb",
      "year" : 2006
    }, {
      "title" : "Multi-class classification with maximum margin multiple kernel",
      "author" : [ "C. Cortes", "M. Mohri", "A. Rostamizadeh" ],
      "venue" : null,
      "citeRegEx" : "Cortes et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Cortes et al\\.",
      "year" : 2013
    }, {
      "title" : "Nearest neighbor pattern classification",
      "author" : [ "T. Cover", "P. Hart" ],
      "venue" : "IEEE Trans. Info. Theo.,",
      "citeRegEx" : "Cover and Hart,? \\Q1967\\E",
      "shortCiteRegEx" : "Cover and Hart",
      "year" : 1967
    }, {
      "title" : "On the algorithmic implementation of multiclass kernel-based vector machines",
      "author" : [ "K. Crammer", "Y. Singer" ],
      "venue" : "JMLR, 2:265-292,",
      "citeRegEx" : "Crammer and Singer,? \\Q2002\\E",
      "shortCiteRegEx" : "Crammer and Singer",
      "year" : 2002
    }, {
      "title" : "On the learnability and design of output codes for multiclass problems",
      "author" : [ "K. Crammer", "Y. Singer" ],
      "venue" : "Mach. Learn.,",
      "citeRegEx" : "Crammer and Singer,? \\Q2002\\E",
      "shortCiteRegEx" : "Crammer and Singer",
      "year" : 2002
    }, {
      "title" : "Multiclass learnability and the erm principle",
      "author" : [ "A. Daniely", "S. Sabato", "S. Ben-David", "S. Shalev-Shwartz" ],
      "venue" : "JMLR - Proceedings Track,",
      "citeRegEx" : "Daniely et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Daniely et al\\.",
      "year" : 2011
    }, {
      "title" : "On the hardness of approximating minimum vertex cover",
      "author" : [ "I. Dinur", "S. Safra" ],
      "venue" : "Ann. Math.,",
      "citeRegEx" : "Dinur and Safra,? \\Q2005\\E",
      "shortCiteRegEx" : "Dinur and Safra",
      "year" : 2005
    }, {
      "title" : "The sizes of compact subsets of hilbert space and continuity of gaussian processes",
      "author" : [ "R.M. Dudley" ],
      "venue" : "J. Func. Anal.,",
      "citeRegEx" : "Dudley,? \\Q1967\\E",
      "shortCiteRegEx" : "Dudley",
      "year" : 1967
    }, {
      "title" : "Better multiclass classification via a margin-optimized single binary problem",
      "author" : [ "R. El-Yaniv", "D. Pechyony", "E. Yom-Tov" ],
      "venue" : null,
      "citeRegEx" : "El.Yaniv et al\\.,? \\Q1954\\E",
      "shortCiteRegEx" : "El.Yaniv et al\\.",
      "year" : 1954
    }, {
      "title" : "On the nonexistence of uniform homeomorphisms between Lp-spaces",
      "author" : [ "P. Enflo" ],
      "venue" : "Ark. Mat.,",
      "citeRegEx" : "Enflo,? \\Q1969\\E",
      "shortCiteRegEx" : "Enflo",
      "year" : 1969
    }, {
      "title" : "Efficient classification for metric data",
      "author" : [ "L. Gottlieb", "A. Kontorovich", "R. Krauthgamer" ],
      "venue" : "COLT,",
      "citeRegEx" : "Gottlieb et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Gottlieb et al\\.",
      "year" : 2010
    }, {
      "title" : "Efficient regression in metric spaces via approximate lipschitz extension",
      "author" : [ "L. Gottlieb", "A. Kontorovich", "R. Krauthgamer" ],
      "venue" : "SIMBAD,",
      "citeRegEx" : "Gottlieb et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Gottlieb et al\\.",
      "year" : 2013
    }, {
      "title" : "Adaptive metric dimensionality reduction",
      "author" : [ "L. Gottlieb", "A. Kontorovich", "R. Krauthgamer" ],
      "venue" : "ALT,",
      "citeRegEx" : "Gottlieb et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Gottlieb et al\\.",
      "year" : 2013
    }, {
      "title" : "VC theory of large margin multi-category",
      "author" : [ "Y. Guermeur" ],
      "venue" : "classifiers. JMLR,",
      "citeRegEx" : "Guermeur,? \\Q2007\\E",
      "shortCiteRegEx" : "Guermeur",
      "year" : 2007
    }, {
      "title" : "Fast construction of nets in low-dimensional metrics and their applications",
      "author" : [ "S. Har-Peled", "M. Mendel" ],
      "venue" : "SIAM J. Comp.,",
      "citeRegEx" : "Har.Peled and Mendel,? \\Q2006\\E",
      "shortCiteRegEx" : "Har.Peled and Mendel",
      "year" : 2006
    }, {
      "title" : "ε-entropy and ε-capacity of sets in function spaces",
      "author" : [ "A. Kolmogorov", "V. Tikhomirov" ],
      "venue" : "Uspekhi Matematicheskikh Nauk,",
      "citeRegEx" : "Kolmogorov and Tikhomirov,? \\Q1959\\E",
      "shortCiteRegEx" : "Kolmogorov and Tikhomirov",
      "year" : 1959
    }, {
      "title" : "Empirical margin distributions and bounding the generalization error of combined classifiers",
      "author" : [ "V. Koltchinskii", "D. Panchenko" ],
      "venue" : "Ann. Statist.,",
      "citeRegEx" : "Koltchinskii and Panchenko,? \\Q2002\\E",
      "shortCiteRegEx" : "Koltchinskii and Panchenko",
      "year" : 2002
    }, {
      "title" : "Navigating nets: Simple algorithms for proximity search",
      "author" : [ "R. Krauthgamer", "J. Lee" ],
      "venue" : null,
      "citeRegEx" : "Krauthgamer and Lee,? \\Q2004\\E",
      "shortCiteRegEx" : "Krauthgamer and Lee",
      "year" : 2004
    }, {
      "title" : "Sensitive error correcting output codes",
      "author" : [ "J. Langford", "A. Beygelzimer" ],
      "venue" : "COLT,",
      "citeRegEx" : "Langford and Beygelzimer,? \\Q2005\\E",
      "shortCiteRegEx" : "Langford and Beygelzimer",
      "year" : 2005
    }, {
      "title" : "Extension of range of functions",
      "author" : [ "E.J. McShane" ],
      "venue" : "Bull. Amer. Math. Soc.,",
      "citeRegEx" : "McShane,? \\Q1934\\E",
      "shortCiteRegEx" : "McShane",
      "year" : 1934
    }, {
      "title" : "Foundations Of Machine Learning",
      "author" : [ "M. Mohri", "A. Rostamizadeh", "A. Talwalkar" ],
      "venue" : null,
      "citeRegEx" : "Mohri et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Mohri et al\\.",
      "year" : 2012
    }, {
      "title" : "Planar earthmover is not in l1",
      "author" : [ "A. Naor", "G. Schechtman" ],
      "venue" : "SICOMP,",
      "citeRegEx" : "Naor and Schechtman,? \\Q2007\\E",
      "shortCiteRegEx" : "Naor and Schechtman",
      "year" : 2007
    }, {
      "title" : "Combinatorial optimization : algorithms and complexity",
      "author" : [ "C. Papadimitriou", "K. Steiglitz" ],
      "venue" : null,
      "citeRegEx" : "Papadimitriou and Steiglitz,? \\Q1998\\E",
      "shortCiteRegEx" : "Papadimitriou and Steiglitz",
      "year" : 1998
    }, {
      "title" : "Distance-based classification with lipschitz functions",
      "author" : [ "U. von Luxburg", "O. Bousquet" ],
      "venue" : "JMLR, 5:669-695,",
      "citeRegEx" : "Luxburg and Bousquet,? \\Q2004\\E",
      "shortCiteRegEx" : "Luxburg and Bousquet",
      "year" : 2004
    }, {
      "title" : "Support vector machines for multi-class pattern recognition",
      "author" : [ "J. Weston", "C. Watkins" ],
      "venue" : "ESANN 99,",
      "citeRegEx" : "Weston and Watkins,? \\Q1999\\E",
      "shortCiteRegEx" : "Weston and Watkins",
      "year" : 1999
    }, {
      "title" : "Analytic extensions of differentiable functions defined in closed sets",
      "author" : [ "H. Whitney" ],
      "venue" : "Trans. Amer. Math. Soc.,",
      "citeRegEx" : "Whitney,? \\Q1934\\E",
      "shortCiteRegEx" : "Whitney",
      "year" : 1934
    }, {
      "title" : "Covering number bounds of certain regularized linear function",
      "author" : [ "T. Zhang" ],
      "venue" : "classes. JMLR,",
      "citeRegEx" : "Zhang,? \\Q2002\\E",
      "shortCiteRegEx" : "Zhang",
      "year" : 2002
    }, {
      "title" : "Statistical analysis of some multi-category large margin classification methods",
      "author" : [ "T. Zhang" ],
      "venue" : "JMLR, 5:1225-1251,",
      "citeRegEx" : "Zhang,? \\Q2004\\E",
      "shortCiteRegEx" : "Zhang",
      "year" : 2004
    }, {
      "title" : "Scale sensitive analysis proof Proof of Theorem 5. An application of Guermeur (2010, Theorem 1) states that with probability 1− δ, for all L > 0, h ∈ HL",
      "author" : [ "C proof" ],
      "venue" : null,
      "citeRegEx" : "proof.,? \\Q2010\\E",
      "shortCiteRegEx" : "proof.",
      "year" : 2010
    }, {
      "title" : "2012) and doing the stratification on L instead setting γ = 1 in Guermeur (2010, Theorem 1) and doing the stratification on L",
      "author" : [ "Mohri" ],
      "venue" : null,
      "citeRegEx" : "Mohri,? \\Q2010\\E",
      "shortCiteRegEx" : "Mohri",
      "year" : 2010
    } ],
    "referenceMentions" : [ {
      "referenceID" : 11,
      "context" : "Characterizing the multiclass distribution-free sample complexity is far less straightforward, though impressive progress has been recently made (Daniely et al., 2011).",
      "startOffset" : 145,
      "endOffset" : 167
    }, {
      "referenceID" : 11,
      "context" : "Characterizing the multiclass distribution-free sample complexity is far less straightforward, though impressive progress has been recently made (Daniely et al., 2011). Following von Luxburg & Bousquet (2004); Gottlieb et al.",
      "startOffset" : 146,
      "endOffset" : 209
    }, {
      "referenceID" : 11,
      "context" : "Characterizing the multiclass distribution-free sample complexity is far less straightforward, though impressive progress has been recently made (Daniely et al., 2011). Following von Luxburg & Bousquet (2004); Gottlieb et al. (2010), we adopt a proximity-based approach to supervised multicategory classification in metric spaces.",
      "startOffset" : 146,
      "endOffset" : 233
    }, {
      "referenceID" : 15,
      "context" : "(i) Many natural metrics, such as L1, earthmover, and edit distance cannot be embedded in a Hilbert space without a large distortion (Enflo, 1969; Naor & Schechtman, 2007; Andoni & Krauthgamer, 2010).",
      "startOffset" : 133,
      "endOffset" : 199
    }, {
      "referenceID" : 11,
      "context" : "To the best of our knowledge, the sharpest current estimate on the Natarajan dimension (for some special function classes) is dNat = Õ(k) with a matching lower bound of Ω(k) (Daniely et al., 2011).",
      "startOffset" : 174,
      "endOffset" : 196
    }, {
      "referenceID" : 26,
      "context" : "A margin-based Rademacher analysis of score functions (Mohri et al., 2012) yields a bound of order Õ(k/γ √ n), and this is also the k-dependence obtained by Cortes et al.",
      "startOffset" : 54,
      "endOffset" : 74
    }, {
      "referenceID" : 0,
      "context" : "Regarding the second paradigm, the seminal paper of Allwein et al. (2001) unified the various error correcting output code (ECOC)-based multiclass-to-binary reductions under a single margin-based framework.",
      "startOffset" : 52,
      "endOffset" : 74
    }, {
      "referenceID" : 0,
      "context" : "Regarding the second paradigm, the seminal paper of Allwein et al. (2001) unified the various error correcting output code (ECOC)-based multiclass-to-binary reductions under a single margin-based framework. Their generalization bound requires the base classifier to have VC-dimension dVC < ∞ (and hence does not apply to nearest neighbors or infinite-dimensional Hilbert spaces) and is of the form Õ ( log k γ √ dVC n ) . Langford & Beygelzimer (2005); Beygelzimer et al.",
      "startOffset" : 52,
      "endOffset" : 452
    }, {
      "referenceID" : 0,
      "context" : "Regarding the second paradigm, the seminal paper of Allwein et al. (2001) unified the various error correcting output code (ECOC)-based multiclass-to-binary reductions under a single margin-based framework. Their generalization bound requires the base classifier to have VC-dimension dVC < ∞ (and hence does not apply to nearest neighbors or infinite-dimensional Hilbert spaces) and is of the form Õ ( log k γ √ dVC n ) . Langford & Beygelzimer (2005); Beygelzimer et al. (2009) gave k-free and O(log k) regret bounds, but these are conditional on the performance of the underlying binary classifiers as opposed to the unconditional bounds we provide in this paper.",
      "startOffset" : 52,
      "endOffset" : 479
    }, {
      "referenceID" : 0,
      "context" : "Regarding the second paradigm, the seminal paper of Allwein et al. (2001) unified the various error correcting output code (ECOC)-based multiclass-to-binary reductions under a single margin-based framework. Their generalization bound requires the base classifier to have VC-dimension dVC < ∞ (and hence does not apply to nearest neighbors or infinite-dimensional Hilbert spaces) and is of the form Õ ( log k γ √ dVC n ) . Langford & Beygelzimer (2005); Beygelzimer et al. (2009) gave k-free and O(log k) regret bounds, but these are conditional on the performance of the underlying binary classifiers as opposed to the unconditional bounds we provide in this paper. As for the first paradigm, proximity is perhaps the most natural score function — and indeed, a formal analysis of the nearest neighbor classifier (Cover & Hart, 1967) much predated the first multiclass extensions of SVM (Weston & Watkins, 1999). Crammer & Singer (2002a,b) considerably reduced the computational complexity of the latter approach and gave a risk bound decaying as Õ(k/nγ), for the separable case with margin γ. In an alternative approach based on choosing q prototype examples, Crammer et al. (2002) gave a risk bound with rate Õ(q/γ √ n).",
      "startOffset" : 52,
      "endOffset" : 1183
    }, {
      "referenceID" : 0,
      "context" : "Regarding the second paradigm, the seminal paper of Allwein et al. (2001) unified the various error correcting output code (ECOC)-based multiclass-to-binary reductions under a single margin-based framework. Their generalization bound requires the base classifier to have VC-dimension dVC < ∞ (and hence does not apply to nearest neighbors or infinite-dimensional Hilbert spaces) and is of the form Õ ( log k γ √ dVC n ) . Langford & Beygelzimer (2005); Beygelzimer et al. (2009) gave k-free and O(log k) regret bounds, but these are conditional on the performance of the underlying binary classifiers as opposed to the unconditional bounds we provide in this paper. As for the first paradigm, proximity is perhaps the most natural score function — and indeed, a formal analysis of the nearest neighbor classifier (Cover & Hart, 1967) much predated the first multiclass extensions of SVM (Weston & Watkins, 1999). Crammer & Singer (2002a,b) considerably reduced the computational complexity of the latter approach and gave a risk bound decaying as Õ(k/nγ), for the separable case with margin γ. In an alternative approach based on choosing q prototype examples, Crammer et al. (2002) gave a risk bound with rate Õ(q/γ √ n). Ben-David et al. (1995) characterized the PAC learnability of k-valued functions in terms of combinatorial dimensions, such as the Natarajan dimension dNat.",
      "startOffset" : 52,
      "endOffset" : 1247
    }, {
      "referenceID" : 0,
      "context" : "Regarding the second paradigm, the seminal paper of Allwein et al. (2001) unified the various error correcting output code (ECOC)-based multiclass-to-binary reductions under a single margin-based framework. Their generalization bound requires the base classifier to have VC-dimension dVC < ∞ (and hence does not apply to nearest neighbors or infinite-dimensional Hilbert spaces) and is of the form Õ ( log k γ √ dVC n ) . Langford & Beygelzimer (2005); Beygelzimer et al. (2009) gave k-free and O(log k) regret bounds, but these are conditional on the performance of the underlying binary classifiers as opposed to the unconditional bounds we provide in this paper. As for the first paradigm, proximity is perhaps the most natural score function — and indeed, a formal analysis of the nearest neighbor classifier (Cover & Hart, 1967) much predated the first multiclass extensions of SVM (Weston & Watkins, 1999). Crammer & Singer (2002a,b) considerably reduced the computational complexity of the latter approach and gave a risk bound decaying as Õ(k/nγ), for the separable case with margin γ. In an alternative approach based on choosing q prototype examples, Crammer et al. (2002) gave a risk bound with rate Õ(q/γ √ n). Ben-David et al. (1995) characterized the PAC learnability of k-valued functions in terms of combinatorial dimensions, such as the Natarajan dimension dNat. Guermeur (2007, 2010) gave scale-sensitive analogues of these dimensions. He gave a risk bound decaying as Õ ( log k γ √ dγNat/n ) , where dγNat is a scale-sensitive Natarajan dimension — essentially replacing the finite VC dimension dVC in Allwein et al. (2001) by dγNat.",
      "startOffset" : 52,
      "endOffset" : 1643
    }, {
      "referenceID" : 0,
      "context" : "Regarding the second paradigm, the seminal paper of Allwein et al. (2001) unified the various error correcting output code (ECOC)-based multiclass-to-binary reductions under a single margin-based framework. Their generalization bound requires the base classifier to have VC-dimension dVC < ∞ (and hence does not apply to nearest neighbors or infinite-dimensional Hilbert spaces) and is of the form Õ ( log k γ √ dVC n ) . Langford & Beygelzimer (2005); Beygelzimer et al. (2009) gave k-free and O(log k) regret bounds, but these are conditional on the performance of the underlying binary classifiers as opposed to the unconditional bounds we provide in this paper. As for the first paradigm, proximity is perhaps the most natural score function — and indeed, a formal analysis of the nearest neighbor classifier (Cover & Hart, 1967) much predated the first multiclass extensions of SVM (Weston & Watkins, 1999). Crammer & Singer (2002a,b) considerably reduced the computational complexity of the latter approach and gave a risk bound decaying as Õ(k/nγ), for the separable case with margin γ. In an alternative approach based on choosing q prototype examples, Crammer et al. (2002) gave a risk bound with rate Õ(q/γ √ n). Ben-David et al. (1995) characterized the PAC learnability of k-valued functions in terms of combinatorial dimensions, such as the Natarajan dimension dNat. Guermeur (2007, 2010) gave scale-sensitive analogues of these dimensions. He gave a risk bound decaying as Õ ( log k γ √ dγNat/n ) , where dγNat is a scale-sensitive Natarajan dimension — essentially replacing the finite VC dimension dVC in Allwein et al. (2001) by dγNat. He further showed that for linear function classes in Hilbert spaces, dγNat is bounded by Õ(k/γ), resulting in a risk bound decaying as Õ(k/γ √ n). To the best of our knowledge, the sharpest current estimate on the Natarajan dimension (for some special function classes) is dNat = Õ(k) with a matching lower bound of Ω(k) (Daniely et al., 2011). A margin-based Rademacher analysis of score functions (Mohri et al., 2012) yields a bound of order Õ(k/γ √ n), and this is also the k-dependence obtained by Cortes et al. (2013) in a recent paper proposing a multiple kernel approach to multiclass learning.",
      "startOffset" : 52,
      "endOffset" : 2177
    }, {
      "referenceID" : 0,
      "context" : "Paper decay rate Õ(·) group Allwein et al. (2001)‡ log k γ √ dVC n (II,a) Daniely et al.",
      "startOffset" : 28,
      "endOffset" : 50
    }, {
      "referenceID" : 0,
      "context" : "Paper decay rate Õ(·) group Allwein et al. (2001)‡ log k γ √ dVC n (II,a) Daniely et al. (2011)∗†‡ dNat log k n (I,a) Guermeur (2010)‡ log k γ √ dγNat n (I,a) Crammer & Singer (2002b)† k 2 γ2n (I,b) Cortes et al.",
      "startOffset" : 28,
      "endOffset" : 96
    }, {
      "referenceID" : 0,
      "context" : "Paper decay rate Õ(·) group Allwein et al. (2001)‡ log k γ √ dVC n (II,a) Daniely et al. (2011)∗†‡ dNat log k n (I,a) Guermeur (2010)‡ log k γ √ dγNat n (I,a) Crammer & Singer (2002b)† k 2 γ2n (I,b) Cortes et al.",
      "startOffset" : 28,
      "endOffset" : 134
    }, {
      "referenceID" : 0,
      "context" : "Paper decay rate Õ(·) group Allwein et al. (2001)‡ log k γ √ dVC n (II,a) Daniely et al. (2011)∗†‡ dNat log k n (I,a) Guermeur (2010)‡ log k γ √ dγNat n (I,a) Crammer & Singer (2002b)† k 2 γ2n (I,b) Cortes et al.",
      "startOffset" : 28,
      "endOffset" : 184
    }, {
      "referenceID" : 0,
      "context" : "Paper decay rate Õ(·) group Allwein et al. (2001)‡ log k γ √ dVC n (II,a) Daniely et al. (2011)∗†‡ dNat log k n (I,a) Guermeur (2010)‡ log k γ √ dγNat n (I,a) Crammer & Singer (2002b)† k 2 γ2n (I,b) Cortes et al. (2013) k 2 γ √ n (I,b) Guermeur (2010) k γ2 √ n (I,b) Zhang (2004) 1 γ √ k n (I,b)",
      "startOffset" : 28,
      "endOffset" : 220
    }, {
      "referenceID" : 0,
      "context" : "Paper decay rate Õ(·) group Allwein et al. (2001)‡ log k γ √ dVC n (II,a) Daniely et al. (2011)∗†‡ dNat log k n (I,a) Guermeur (2010)‡ log k γ √ dγNat n (I,a) Crammer & Singer (2002b)† k 2 γ2n (I,b) Cortes et al. (2013) k 2 γ √ n (I,b) Guermeur (2010) k γ2 √ n (I,b) Zhang (2004) 1 γ √ k n (I,b)",
      "startOffset" : 28,
      "endOffset" : 252
    }, {
      "referenceID" : 0,
      "context" : "Paper decay rate Õ(·) group Allwein et al. (2001)‡ log k γ √ dVC n (II,a) Daniely et al. (2011)∗†‡ dNat log k n (I,a) Guermeur (2010)‡ log k γ √ dγNat n (I,a) Crammer & Singer (2002b)† k 2 γ2n (I,b) Cortes et al. (2013) k 2 γ √ n (I,b) Guermeur (2010) k γ2 √ n (I,b) Zhang (2004) 1 γ √ k n (I,b)",
      "startOffset" : 28,
      "endOffset" : 280
    }, {
      "referenceID" : 7,
      "context" : "Multiclass extensions of SVM and related kernel methods (Weston & Watkins, 1999; Crammer & Singer, 2002a,b; Crammer et al., 2002; Cortes et al., 2013) fall into category (b).",
      "startOffset" : 56,
      "endOffset" : 150
    }, {
      "referenceID" : 16,
      "context" : "(2013b) exhibit a characteristic “curse of dimensionality” decay rate of O(n−1/(D+1)), but more optimistic asymptotics can be obtained (Guermeur, 2007, 2010; Zhang, 2002, 2004; Gottlieb et al., 2010).",
      "startOffset" : 135,
      "endOffset" : 199
    }, {
      "referenceID" : 7,
      "context" : ", 2002; Cortes et al., 2013) fall into category (b). Category (c), consisting of agnostic metric-space methods is the most sparsely populated. The pioneering asymptotic analysis of Cover & Hart (1967) was cast in a modern, finite-sample version by Ben-David & Shalev-Shwartz (2014), but only for binary classification.",
      "startOffset" : 8,
      "endOffset" : 201
    }, {
      "referenceID" : 7,
      "context" : ", 2002; Cortes et al., 2013) fall into category (b). Category (c), consisting of agnostic metric-space methods is the most sparsely populated. The pioneering asymptotic analysis of Cover & Hart (1967) was cast in a modern, finite-sample version by Ben-David & Shalev-Shwartz (2014), but only for binary classification.",
      "startOffset" : 8,
      "endOffset" : 282
    }, {
      "referenceID" : 7,
      "context" : ", 2002; Cortes et al., 2013) fall into category (b). Category (c), consisting of agnostic metric-space methods is the most sparsely populated. The pioneering asymptotic analysis of Cover & Hart (1967) was cast in a modern, finite-sample version by Ben-David & Shalev-Shwartz (2014), but only for binary classification. Unlike Hilbert spaces, which admit dimension-free margin bounds, we are not aware of any metric space risk bound that does not explicitly depend on some metric dimension D or covering numbers. The bounds in Ben-David & Shalev-Shwartz (2014); Gottlieb et al.",
      "startOffset" : 8,
      "endOffset" : 560
    }, {
      "referenceID" : 7,
      "context" : ", 2002; Cortes et al., 2013) fall into category (b). Category (c), consisting of agnostic metric-space methods is the most sparsely populated. The pioneering asymptotic analysis of Cover & Hart (1967) was cast in a modern, finite-sample version by Ben-David & Shalev-Shwartz (2014), but only for binary classification. Unlike Hilbert spaces, which admit dimension-free margin bounds, we are not aware of any metric space risk bound that does not explicitly depend on some metric dimension D or covering numbers. The bounds in Ben-David & Shalev-Shwartz (2014); Gottlieb et al. (2013b) exhibit a characteristic “curse of dimensionality” decay rate of O(n−1/(D+1)), but more optimistic asymptotics can be obtained (Guermeur, 2007, 2010; Zhang, 2002, 2004; Gottlieb et al.",
      "startOffset" : 8,
      "endOffset" : 585
    }, {
      "referenceID" : 19,
      "context" : "2) and via scale-sensitive techniques in the spirit of Guermeur (2007) (Section 3.",
      "startOffset" : 55,
      "endOffset" : 71
    }, {
      "referenceID" : 25,
      "context" : "Construct a 2L-Lipschitz extension for ĥ from N to all over X (such an extension always exists, (McShane, 1934; Whitney, 1934)).",
      "startOffset" : 96,
      "endOffset" : 126
    }, {
      "referenceID" : 31,
      "context" : "Construct a 2L-Lipschitz extension for ĥ from N to all over X (such an extension always exists, (McShane, 1934; Whitney, 1934)).",
      "startOffset" : 96,
      "endOffset" : 126
    }, {
      "referenceID" : 34,
      "context" : "With the additional assumptions on X we follow the proof idea in Kolmogorov & Tikhomirov (1959) and demonstrate the tighter bound |Ĥ | ≤ (⌈4k/ε⌉+ 1)(2k + 1)|N |−1 = The remainder of the proof is based on a technique communicated to us by R.",
      "startOffset" : 51,
      "endOffset" : 96
    }, {
      "referenceID" : 34,
      "context" : "With the additional assumptions on X we follow the proof idea in Kolmogorov & Tikhomirov (1959) and demonstrate the tighter bound |Ĥ | ≤ (⌈4k/ε⌉+ 1)(2k + 1)|N |−1 = The remainder of the proof is based on a technique communicated to us by R. Krauthgamer, a variant of the classic Kolmogorov & Tikhomirov (1959) method.",
      "startOffset" : 51,
      "endOffset" : 310
    }, {
      "referenceID" : 0,
      "context" : "In terms of the number of classes k, our bound compares favorably to those in Allwein et al. (2001); Guermeur (2007, 2010), and more recently in Daniely et al.",
      "startOffset" : 78,
      "endOffset" : 100
    }, {
      "referenceID" : 0,
      "context" : "In terms of the number of classes k, our bound compares favorably to those in Allwein et al. (2001); Guermeur (2007, 2010), and more recently in Daniely et al. (2011), which have a k-dependence of O(dNat log k), where dNat is the (scale-sensitive, k-dependent) Natarajan dimension of the multiclass hypothesis class.",
      "startOffset" : 78,
      "endOffset" : 167
    }, {
      "referenceID" : 16,
      "context" : "For k = 2, Gottlieb et al. (2010) reduced the problem of computing m̂(L) to one of finding a minimal vertex cover in a bipartite graph (by König’s theorem, the latter is efficiently computable as a maximal matching).",
      "startOffset" : 11,
      "endOffset" : 34
    }, {
      "referenceID" : 16,
      "context" : "Rather a brute-force searching all of these values, Theorem 7 of Gottlieb et al. (2010) shows that using an O(log n) time binary search over the values of L, one may approximately minimizeQ(L), which in turn yields an approximate solution to (11).",
      "startOffset" : 65,
      "endOffset" : 88
    }, {
      "referenceID" : 16,
      "context" : "The technique proposed in Gottlieb et al. (2013b) may roughly be described as a metric analogue of PCA.",
      "startOffset" : 26,
      "endOffset" : 50
    }, {
      "referenceID" : 26,
      "context" : "where the identity is obvious and the inequality is a simple consequence of measure concentration (Mohri et al., 2012).",
      "startOffset" : 98,
      "endOffset" : 118
    }, {
      "referenceID" : 16,
      "context" : "Although computing the optimal (α∗, β∗) seems computationally difficult, Gottlieb et al. (2013b) were able to obtain an efficient (O(1), O(1))-bicriteria approximation.",
      "startOffset" : 73,
      "endOffset" : 97
    } ],
    "year" : 2014,
    "abstractText" : "We develop a general framework for margin-based multicategory classification in metric spaces. The basic work-horse is a margin-regularized version of the nearest-neighbor classifier. We prove generalization bounds that match the state of the art in sample size n and significantly improve the dependence on the number of classes k. Our point of departure is a nearly Bayes-optimal finite-sample risk bound independent of k. Although k-free, this bound is unregularized and non-adaptive, which motivates our main result: Rademacher and scale-sensitive margin bounds with a logarithmic dependence on k. As the best previous risk estimates in this setting were of order √ k, our bound is exponentially sharper. From the algorithmic standpoint, in doubling metric spaces our classifier may be trained on n examples in O(n log n) time and evaluated on new points in O(log n) time.",
    "creator" : "LaTeX with hyperref package"
  }
}