{
  "name" : "1409.3768.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Optimization Methods for Sparse Pseudo-Likelihood Graphical Model Selection",
    "authors" : [ "Sang-Yun Oh", "Onkar Dalal", "Kshitij Khare" ],
    "emails" : [ "syoh@lbl.gov", "onkar@alumni.stanford.edu", "kdkhare@stat.ufl.edu", "brajarat@stanford.edu" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : ""
    }, {
      "heading" : "1.1 Background",
      "text" : "Sparse inverse covariance estimation has received tremendous attention in the machine learning, statistics and optimization communities. These sparse models, popularly known as graphical models, have widespread use in various applications, especially in high dimensional settings. The most popular inverse covariance estimation framework is arguably the `1-penalized Gaussian likelihood optimization framework as given by\nminimize Ω∈Sp++ − log det Ω + tr(SΩ) + λ‖Ω‖1\nwhere Sp++ denotes the space of p-dimensional positive definite matrices, and `1-penalty is imposed on the elements of Ω = (ωij)1≤i≤j≤p by the term ‖Ω‖1 = ∑ i,j |ωij | along with the scaling factor λ > 0. The matrix S denotes the\nar X\niv :1\n40 9.\n37 68\nv1 [\nst at\n.C O\n] 1\nsample covariance matrix of the data Y ∈ IRn×p. As the `1-penalized log likelihood is convex, the problem becomes more tractable and has benefited from advances in convex optimization. Recent efforts in the literature on Gaussian graphical models therefore have focused on developing principled methods which are increasingly more and more scalable. The literature on this topic is simply enormous and for the sake of brevity, space constraints and the topic of this paper, we avoid an extensive literature review by referring to the references in the seminal work of Banerjee et al. [2008] and the very recent work of Dalal and Rajaratnam [2014]. These two papers contain references to recent work, including past NIPS conference proceedings."
    }, {
      "heading" : "1.2 The CONCORD method",
      "text" : "Despite their tremendous contributions, one shortcoming of the traditional approaches to `1-penalized likelihood maximization is the restriction to the Gaussian assumption. To address this gap, a number of `1-penalized pseudo-likelihood approaches have been proposed: SPACE Peng et al. [2009] and SPLICE Rocha et al. [2008], SYMLASSO Friedman et al. [2010]. These approaches are either not convex, and/or convergence of corresponding maximization algorithms are not established. In this sense, non-Gaussian partial correlation graph estimation methods have lagged severely behind, despite the tremendous need to move beyond the Gaussian framework for obvious practical reasons. In very recent work, a convex pseudo-likelihood approach with good model selection properties called CONCORD Khare et al. [2014] was proposed. The CONCORD algorithm minimizes\nQcon(Ω) = − p∑ i=1 n logωii + 1 2 p∑ i=1 ‖ωiiYi + ∑ j 6=i ωijYj‖22 + nλ ∑ 1≤i<j≤p |ωij | (1)\nvia cyclic coordinate-wise descent that alternates between updating off-diagonal elements and diagonal elements. It is straightforward to show that operators Tij for updating (ωij)1≤i<j≤p (holding (ωii)1≤i≤p constant) and Tii for updating (ωii)1≤i≤p (holding (ωij)1≤i<j≤p constant) are given by\n(Tij(Ω))ij = Sλ\n( − (∑ j′ 6=j ωij′sjj′ + ∑ i′ 6=i ωi′jsii′ )) sii + sjj\n(2)\n(Tii(Ω))ii = − ∑ j 6=i ωijsij +\n√(∑ j 6=i ωijsij )2 + 4sii\n2sii . (3)\nThis coordinate-wise algorithm is shown to converge to a global minima though no rate is given [Khare et al., 2014]. Note that the equivalent problem assuming a Gaussian likelihood has seen much development in the last ten years, but a parallel development for the recently introduced CONCORD framework is lacking for obvious reasons. We address this important gap by proposing state-of-the-art proximal gradient techniques to minimizeQcon. A rigorous theoretical analysis of the pseudo-likelihood framework and the associated proximal gradient methods which are proposed is undertaken. We establish rates of convergence and also demonstrate that our approach can lead to massive computational speed-ups, thus yielding extremely fast and principled solvers for the sparse inverse covariance estimation problem outside the Gaussian setting."
    }, {
      "heading" : "2 CONCORD using proximal gradient methods",
      "text" : "The penalized matrix version the CONCORD objective function in (1) is given by\nQcon(Ω) = n\n2\n[ − log |Ω2D|+ tr(SΩ2) + λ‖ΩX‖1 ] . (4)\nwhere ΩD and ΩX denote the diagonal and off-diagonal elements of Ω. We will use the notation A = AD +AX to split any matrix A into its diagonal and off-diagonal terms.\nThis section proposes a scalable and thorough approach to solving the CONCORD objective function using recent advances in convex optimization and derives rates of convergence for such algorithms. In particular, we use proximal gradient-based methods to achieve this goal and demonstrate the efficacy of such methods for the non-Gaussian graphical modeling problem. First, we propose CONCORD-ISTA and CONCORD-FISTA in section 2.1: methods which are inspired by the iterative soft-thresholding algorithms in Beck and Teboulle [2009]. We undertake a comprehensive treatment of the CONCORD optimization problem by also investigating the dual of the CONCORD problem. Other popular methods in the literature, including the potential use of alternating minimization algorithm and the second order proximal Newtons method CONCORD-PNOPT, are considered in Supplemental section A.5.\nAlgorithm 1 CONCORD-ISTA Input: sample covariance matrix S, penalty matrix Λ Initialize: Ω(0) ∈ Sp+, τ(0,0) ≤ 1,\nc < 1, ∆subg = 2 subg. while ∆subg > subg or ∆func > func do\nCompute ∇h1: G(k) = − ( Ω\n(k) D )−1 + 12 ( S Ω(k) + Ω(k)S ) Compute τk:\nLargest τk ∈ {cjτ(k,0)}j=0,1,... such that, Ω(k+1) = SτkΛ ( Ω(k) − τkG(k) ) satisfies (8).\nUpdate: Ω(k+1) using the appropriate step size. Compute next initial step size: τ(k+1,0) Compute convergence criteria:\n∆subg = ‖∇h1(Ω(k)) + ∂h2(Ω(k))‖\n‖Ω(k)‖ .\nend while\nAlgorithm 2 CONCORD-FISTA Input: sample covariance matrix S, penalty matrix Λ Initialize: (Θ(1) =)Ω(0) ∈ Sp+, α1 = 1, τ(0,0) ≤ 1,\nc < 1, ∆subg = 2 subg. while ∆subg > subg or ∆func > func do\nCompute∇h1: G(k) = − ( Θ\n(k) D )−1 + 12 ( SΘ(k) + Θ(k)S ) Compute τk:\nLargest τk ∈ {cjτ(k,0)}j=0,1,... such that, Ω(k) = SτkΛ ( Θ(k) − τkG(k) ) satisfies (8)\nUpdate: αk+1 = (1 + √ 1 + 4αk2)/2\nUpdate: Θ(k+1) = Ω(k) + ( αk−1 αk+1 ) ( Ω(k) − Ω(k−1) ) Compute next initial step size: τ(k+1,0) Compute convergence criteria:\n∆subg = ‖∇h1(Ω(k)) + ∂h2(Ω(k))‖\n‖Ω(k)‖ .\nend while"
    }, {
      "heading" : "2.1 Iterative Soft Thresholding Algorithms: CONCORD-ISTA, CONCORD-FISTA",
      "text" : "The iterative soft-thresholding algorithms (ISTA) have recently gained popularity after the seminal paper by Beck and Teboulle Beck and Teboulle [2009]. The ISTA methods are based on the Forward-Backward Splitting method from Rockafellar [1976] and Nesterov’s accelerated gradient methods Nesterov [1983] using soft-thresholding as the proximal operator for the `1-norm. The essence of the proximal gradient algorithms is to divide the objective function into a smooth part and a non-smooth part, then take a proximal step (w.r.t. the non-smooth part) in the negative gradient direction of the smooth part. Nesterov’s accelerated gradient extension Nesterov [1983] uses a combination of gradient and momentum steps to achieve accelerated rates of convergence. In this section, we apply these methods in the context of CONCORD which also has a composite objective function.\nThe matrix CONCORD objective function (4) can be split into a smooth part h1(Ω) and a non-smooth part h2(Ω):\nh1(Ω) = − log det ΩD + 1\n2 tr(ΩSΩ), h2(Ω) = λ‖ΩX‖1. (5)\nThe gradient and hessian of the smooth function h1 are given by\n∇h1(Ω) = ΩD−1 + 1\n2\n( SΩT + ΩS ) ,\n∇2h1(Ω) = i=p∑ i=1 ω−2ii [ eiei T ⊗ eieiT ] + 1 2 (S ⊗ I + I ⊗ S) , (6)\nwhere ei is a column vector of zeros except for a one in the i-th position. The proximal operator for the non-smooth function h2 is given by element-wise soft-thresholding operator Sλ as\nproxh2(Ω) = arg min Θ\n{ h2(Θ) + 1\n2 ‖Ω−Θ‖2F } = SΛ(Ω) = sign(Ω) max{|Ω| − Λ, 0}, (7)\nwhere Λ is a matrix with 0 diagonal and λ for each off-diagonal entry. The details of the proximal gradient algorithm CONCORD-ISTA are given in Algorithm 1, and the details of the accelerated proximal gradient algorithm CONCORD-FISTA are given in Algorithm 2."
    }, {
      "heading" : "2.2 Choice of step size",
      "text" : "In the absence of a good estimate of the Lipschitz constant L, the step size for each iteration of CONCORD-ISTA and CONCORD-FISTA is chosen using backtracking line search. The line search for iteration k starts with an initial step size τ(k,0) and reduces the step with a constant factor c until the new iterate satisfies the sufficient descent condition:\nh1(Ω (k+1)) ≤ Q(Ω(k+1),Ω(k)) (8)\nwhere,\nQ(Ω,Θ) = h1(Θ) + tr ( (Ω−Θ)T∇h1(Θ) ) + 1\n2τ\n∥∥Ω−Θ∥∥2 F .\nIn section 4, we have implemented algorithms choosing the initial step size in three different ways: (a) a constant starting step size (=1), (b) the feasible step size from the previous iteration τk−1, (c) the step size heuristic of BarzilaiBorwein. The Barzilai-Borwein heuristic step size is given by\nτk+1,0 = tr ( (Ω(k+1) − Ω(k))T (Ω(k+1) − Ω(k)) ) tr ( (Ω(k+1) − Ω(k))T (G(k+1) −G(k))\n) . (9) This is an approximation of the secant equation which works as a proxy for second order information using successive gradients (see Barzilai and Borwein [1988] for details)."
    }, {
      "heading" : "2.3 Computational complexity",
      "text" : "After the one time calculation of S, the most significant computation for each iteration in CONCORD-ISTA and CONCORD-FISTA algorithms is the matrix-matrix multiplication W = SΩ in the gradient term. If s is the number of non-zeros in Ω, then W can be computed using O(sp2) operations if we exploit the extreme sparsity in Ω. The second matrix-matrix multiplication for the term tr(Ω(SΩ)) can be computed efficiently using tr(ΩW ) = ∑ ωijwij over the set of non-zero ωij’s. This computation only requires O(s) operations. The remaining computations are all at the element level which can be completed in O(p2) operations. Therefore, the overall computational complexity for each iteration reduces toO(sp2). On the other hand, the proximal gradient algorithms for the Gaussian framework require inversion of a full p × p matrix which is non-parallelizable and requires O(p3) operations. The coordinatewise method for optimizing CONCORD in Khare et al. [2014] also requires cycling through the p2 entries of Ω in specified order and thus does not allow parallelization. In contrast, CONCORD-ISTA and CONCORD-FISTA can use ‘perfectly parallel’ implementations to distribute the above matrix-matrix multiplications. At no step do we need to keep all of the dense matrices S, SΩ,∇h1 on a single machine. Therefore, CONCORD-ISTA and CONCORD-FISTA are scalable to any high dimensions restricted only by the number of machines."
    }, {
      "heading" : "3 Convergence Analysis",
      "text" : "In this section, we prove convergence of CONCORD-ISTA and CONCORD-FISTA methods along with their respective convergence rates of O(1/k) and O(1/k2). We would like to point out that, although the authors in Khare et al. [2014] provide a proof of convergence for their coordinate-wise minimization algorithm for CONCORD, they do not provide any rates of convergence. The arguments for convergence leverage the results in Beck and Teboulle [2009] but require some essential ingredients. We begin with proving lower and upper bounds on the diagonal entries ωkk for Ω belonging to a level set of Qcon(Ω). The lower bound on the diagonal entries of Ω establishes Lipschitz continuity of the gradient ∇h1(Ω) based on the hessian of the smooth function as stated in (6). The proof for the lower bound uses the existence of an upper bound on the diagonal entries. Hence, we prove both bounds on the diagonal entries. We begin by defining a level set C0 of the objective function starting with an arbitrary initial point Ω(0) with a finite function value as\nC0 = { Ω | Qcon(Ω) ≤ Qcon(Ω(0)) = M } . (10)\nFor the positive semidefinite matrix S, let U denote 1√ 2 times the upper triangular matrix from the LU decomposition of S, such that S = 2UTU (the factor 2 simplifies further arithmetic). Assuming the diagonal entries of S to be strictly nonzero (if skk = 0, then the kth component can be ignored upfront since it has zero variance and is equal to a constant for every data point), we have at least one k such that uki 6= 0 for every i. Using this, we prove the following theorem bounding the diagonal entries of Ω.\nTheorem 3.1. For any symmetric matrix Ω satisfying Ω ∈ C0, the diagonal elements of Ω are bounded above and below by constants which depend only on M , λ and S. In other words,\n0 < aM,λ,S ≤ |ωkk| ≤ bM,λ,S , ∀ k = 1, 2, . . . , p, for some constants aM,λ,S and bM,λ,S .\nProof. (a) Upper bound: Suppose |ωii| = max{|ωkk|, for k = 1, 2, . . . , p}. Then, we have M = Qcon(Ω\n(0)) ≥ Qcon(Ω) = h1(Ω) + h2(Ω) ≥ − log det ΩD + tr ( (UΩ)T (UΩ) ) + λ‖ΩX‖1\n= − log det ΩD + ‖UΩ‖2F + λ‖ΩX‖1. (11) Considering only the kith entry in the Frobenious norm term and the ith column penalty in the third term we get\nM ≥ −p log |ωii|+ j=p∑ j=k ukjωji 2 + λ j=p∑ j=k,j 6=i |ωji|. (12)\nNow, suppose |ukiωii| = z and ∑j=p j=k,j 6=i ukjωji = x. Then\n|x| ≤ j=p∑\nj=k,j 6=i\n|ukj ||ωji| ≤ ū j=p∑\nj=k,j 6=i\n|ωji|,\nwhere ū = max{|ukj |, for j = k, k + 1, . . . , p and j 6= i}. Going back to the inequality (12), for λ̄ = λ2ū , we have\nM̄ = M + λ̄2 − p log |uki| ≥ −p log z + (z + x)2 + 2λ̄|x|+ λ̄2 (13) = −p log z + ( z + x+ λ̄sign(x) )2 − 2λ̄z sign(x) (14) Here, if x ≥ 0, then M̄ ≥ −p log z + z2 using the first inequality (13), and if x < 0, then M̄ ≥ −p log z + 2λ̄z using the second inequality (14). In either cases, the functions −p log z + z2 and −p log z + 2λ̄z are unbounded as z →∞. Hence, the upper bound of M̄ on these functions guarantee an upper bound bM,λ,S such that |ωii| ≤ bM,λ,S . Therefore, |ωkk| ≤ bM,λ,S for all k = 1, 2, . . . , p. (b) Lower bound: By positivity of the trace term and the `1 term (for off-diagonals), we have\nM ≥ − log det ΩD = i=p∑ i=1 − log |ωii|. (15)\nThe negative log function g(z) = − log(z) is a convex function with a lower bound at z∗ = bM,λ,S with g(z∗) = − log bM,λ,S . Therefore, for any k = 1, 2, . . . , p, we have\nM ≥ i=p∑ i=1 − log |ωii| ≥ −(p− 1) log bM,λ,S − log |ωkk|. (16)\nSimplifying the above equation, we get the lower bound aM,λ,S on the diagonal entries ωkk. More specifically, log |ωkk| ≥ −M − (p− 1) log bM,λ,S .\nTherefore, |ωkk| ≥ aM,λ,S = e−M−(p−1) log bM,λ,S > 0 serves as a lower bound for all k = 1, 2, . . . , p.\nGiven that the function values are non-increasing along the iterates of Algorithms 1, 2 and 3, the sequence of Ω(k) satisfy Ω(k) ∈ C0 for k = 1, 2, ..... The lower bounds on the diagonal elements of Ω(k) provides the Lipschitz continuity using\n∇2h1(Ω(k)) ( a−2M,λ,S + ‖S‖2 ) (I ⊗ I) . (17)\nTherefore, using the mean-value theorem, the gradient∇h1 satisfies ‖∇h1(Ω)−∇h1(Θ)‖F ≤ L‖Ω−Θ‖F , (18)\nwith the Lipschitz continuity constant L = a−2M,λ,S + ‖S‖2. The remaining argument for convergence follows from the theorems in Beck and Teboulle [2009].\nTheorem 3.2. ([Beck and Teboulle, 2009, Theorem 3.1]). Let {Ω(k)} be the sequence generated by either Algorithm 1 with constant step size or with backtracking line-search. Then for any k ≥ 1,\nQcon(Ω (k))−Qcon(Ω∗) ≤ αL‖Ω(0) − Ω∗‖2F 2k\n(19)\nfor the solution Ω∗, where α = 1 for the constant step size setting and α = c for the backtracking step size setting. Theorem 3.3. ([Beck and Teboulle, 2009, Theorem 4.4]). For the sequences {Ω(k)}, {Θ(k)} generated by Algorithm 2, for any k ≥ 1,\nQcon(Ω (k))−Qcon(Ω∗) ≤ 2αL‖Ω(0) − Ω∗‖2F (k + 1)2\n(20)\nfor the solution Ω∗, where α = 1 for the constant step size setting and α = c for the backtracking step size setting.\nHence, CONCORD-ISTA and CONCORD-FISTA converge at the rates of O(1/k) and O(1/k2) for the kth iteration."
    }, {
      "heading" : "4 Implementation & Numerical Experiments",
      "text" : "In this section, we outline algorithm implementation details and present results of our comprehensive numerical evaluation. Section 4.1 gives performance comparisons from using synthetic multivariate Gaussian datasets. These datasets are generated from a wide range of sample sizes (n) and dimensionality (p). Additionally, convergence of CONCORDISTA and CONCORD-FISTA will be illustrated. Section 4.2 has timing results from analyzing a real breast cancer dataset with outliers. Comparisons are made to the coordinate-wise CONCORD implementation in gconcord package for R available at http://cran.r-project.org/web/packages/gconcord/.\nFor implementing the proposed algorithms, we can take advantage of existing linear algebra libraries. Most of the numerical computations in Algorithms 1 and 2 are linear algebra operations, and, unlike the sequential coordinatewise CONCORD algorithm, CONCORD-ISTA and CONCORD-FISTA implementations can solve increasingly larger problems as more and more scalable and efficient linear algebra libraries are made available. For this work, we opted to using Eigen library [Guennebaud, Jacob, et al., 2010] for its sparse linear algebra routines written in C++. Algorithms 1 and 2 were also written in C++ then interfaced to R for testing. Table 1 gives names for various CONCORD-ISTA and CONCORD-FISTA versions using different initial step size choices."
    }, {
      "heading" : "4.1 Synthetic Datasets",
      "text" : "Synthetic datasets were generated from true sparse positive random Ω matrices of three sizes: p = {1000, 3000, 5000}. Instances of random matrices used here consist of 4995, 14985 and 24975 non-zeros, corresponding to 1%, 0.33% and 0.20% edge densities, respectively. For each p, three random samples of sizes n = {0.25p, 0.75p, 1.25p} were used as inputs. The initial guess, Ω(0), and the convergence criteria was matched to those of coordinate-wise CONCORD implementation. Highlights of the results are summarized below, and the complete set of comparisons are given in Supplementary materials Section A.\nFor synthetic datasets, our experiments indicate that two variations of the CONCORD-ISTA method show little performance difference. However, ccista 0 was marginally faster in our tests. On the other hand, ccfista 1 variation of CONCORD-FISTA that uses τ(k+1,0) = τk as initial step size was significantly faster than ccfista 0. Table 2 gives actual running times for the two best performing algorithms, ccista 0 and ccfista 1, against the coordinate-wise concord. As p and n increase ccista 0 performs very well. For smaller n and λ, coordinate-wise concord performs well (more in Supplemental section A). This can be attributed to min(O(np2),O(p3)) computational complexity of coordinate-wise CONCORD [Khare et al., 2014], and the sparse linear algebra routines used in CONCORD-ISTA and CONCORD-FISTA implementations slowing down as the number of non-zero elements in Ω increases. On the other hand, for large n fraction (n = 1.25p), the proposed methods ccista 0 and ccfista 1 are significantly faster than coordinate-wise concord. In particular, when p = 5000 and n = 6250, the speed-up of ccista 0 can be as much as 150 times over coordinate-wise concord.\nConvergence behavior of CONCORD-ISTA and CONCORD-FISTA methods is shown in Figure 1. The best performing algorithms ccista 0 and ccfista 1 are shown. The vertical axis is the subgradient ∆subg (See Algorithms 1, 2). Plots show that ccista 0 seems to converge at a constant rate much faster than ccfista 1 that appears to slow\ndown after a few initial iterations. While the theoretical convergence results from section 3 prove convergence rates of O(1/k) and O(1/k2) for CONCORD-ISTA and CONCORD-FISTA, in practice, ccista 0 with constant step size performed the fastest for the tests in this section."
    }, {
      "heading" : "4.2 Real Data",
      "text" : "Real datasets arising from various physical and biological sciences often are not multivariate Gaussian and can have outliers. Hence, convergence characteristic may be different on such datasets. In this section, the performance of proposed methods are assessed on a breast cancer dataset [Chang et al., 2005]. This dataset contains expression levels of 24481 genes on 266 patients with breast cancer. Following the approach in Khare et al. Khare et al. [2014], the number of genes are reduced by utilizing clinical information that is provided together with the microarray expression dataset. In particular, survival analysis via univariate Cox regression with patient survival times is used to select a subset of genes closely associated with breast cancer. A choice of p-value < 0.03 yields a reduced dataset with p = 4433 genes.\nOften times, graphical model selection algorithms are applied in a non-Gaussian and n p setting such as the case here. In this n p setting, coordinate-wise CONCORD algorithm is especially fast due to its computational complexityO(np2). However, even in this setting, the newly proposed methods ccista 0, ccista 1, and ccfista 1 perform competitively to, or often better than, concord as illustrated in Table 3. On this real dataset, ccista 1 performed the fastest whereas ccista 0 was the fastest on synthetic datasets."
    }, {
      "heading" : "5 Conclusion",
      "text" : "The Gaussian graphical model estimation or inverse covariance estimation has seen tremendous advances in the past few years. In this paper we propose using proximal gradient methods to solve the general non-Gaussian sparse inverse covariance estimation problem. Rates of convergence were established for the CONCORD-ISTA and CONCORDFISTA algorithms. Coordinate-wise minimization has been the standard approach to this problem thus far, and we provide numerical results comparing CONCORD-ISTA/FISTA and coordinate-wise minimization. We demonstrate that CONCORD-ISTA outperforms coordinate-wise in general, and in high dimensional settings CONCORD-ISTA can outperform coordinate-wise optimization by orders of magnitude. The methodology is also tested on real data sets. We undertake a comprehensive treatment of the problem by also examining the dual formulation and consider methods to maximize the dual objective. We note that efforts similar to ours for the Gaussian case has appeared in not one, but several NIPS and other publications. Our approach on the other hand gives a complete and thorough treatment of the non-Gaussian partial correlation graph estimation problem, all in this one self-contained paper."
    }, {
      "heading" : "A Timing comparison",
      "text" : "A.1 Median Speed-up"
    }, {
      "heading" : "1000 250 0.6 ( 0.7) 0.4 ( 0.3)",
      "text" : ""
    }, {
      "heading" : "1000 750 3.4 ( 1.8) 1.9 ( 0.9)",
      "text" : ""
    }, {
      "heading" : "1000 1250 23.1 ( 5.7) 12.0 ( 3.5)",
      "text" : ""
    }, {
      "heading" : "3000 750 2.7 ( 2.1) 1.9 ( 1.6)",
      "text" : ""
    }, {
      "heading" : "3000 2250 12.8 ( 1.6) 8.8 ( 2.2)",
      "text" : ""
    }, {
      "heading" : "3000 3750 81.9 ( 6.6) 58.2 ( 8.7)",
      "text" : ""
    }, {
      "heading" : "5000 1250 5.6 ( 3.2) 3.0 ( 1.8)",
      "text" : ""
    }, {
      "heading" : "5000 3750 21.1 ( 2.6) 13.5 ( 2.6)",
      "text" : ""
    }, {
      "heading" : "5000 6250 145.8 ( 6.6) 110.1 (16.4)",
      "text" : "A.2 Comparison among CONCORD-ISTA and CONCORD-FISTA variations\nA.3 Comparison with CONCORD algorithm\nA.4 Running times"
    }, {
      "heading" : "23 1000 1250 0.163 0.23 9 43.84 13 1.25 23 2.75",
      "text" : ""
    }, {
      "heading" : "22 1000 1250 0.103 0.44 9 44.16 15 1.93 24 3.02",
      "text" : ""
    }, {
      "heading" : "21 1000 1250 0.077 0.97 9 40.50 15 1.65 24 3.34",
      "text" : ""
    }, {
      "heading" : "20 1000 1250 0.066 2.03 9 44.15 14 1.79 24 4.09",
      "text" : ""
    }, {
      "heading" : "19 1000 1250 0.061 2.91 9 43.84 16 2.36 24 5.38",
      "text" : ""
    }, {
      "heading" : "18 1000 1250 0.059 3.43 9 44.25 16 2.49 24 5.00",
      "text" : ""
    }, {
      "heading" : "17 1000 1250 0.058 3.69 9 44.21 15 2.54 24 5.29",
      "text" : ""
    }, {
      "heading" : "16 1000 750 0.300 0.04 8 6.96 13 1.13 18 2.20",
      "text" : ""
    }, {
      "heading" : "15 1000 750 0.163 0.23 9 8.00 15 1.57 24 2.80",
      "text" : ""
    }, {
      "heading" : "14 1000 750 0.103 0.76 9 8.40 15 1.58 24 3.26",
      "text" : ""
    }, {
      "heading" : "13 1000 750 0.077 3.09 9 8.37 16 3.53 25 4.84",
      "text" : ""
    }, {
      "heading" : "12 1000 750 0.066 5.86 10 10.45 20 4.01 27 6.96",
      "text" : ""
    }, {
      "heading" : "11 1000 750 0.061 7.64 10 9.97 20 5.41 28 7.96",
      "text" : ""
    }, {
      "heading" : "10 1000 750 0.059 8.56 10 9.86 20 5.19 28 9.86",
      "text" : ""
    }, {
      "heading" : "9 1000 750 0.058 8.99 10 9.96 20 4.56 28 12.44",
      "text" : ""
    }, {
      "heading" : "8 1000 250 0.300 0.05 9 2.58 15 1.23 23 2.67",
      "text" : ""
    }, {
      "heading" : "7 1000 250 0.163 0.99 9 2.61 18 1.98 26 3.31",
      "text" : ""
    }, {
      "heading" : "71 5000 6250 0.163 0.04 16 14787.78 26 97.33 34 173.66",
      "text" : ""
    }, {
      "heading" : "70 5000 6250 0.103 0.08 17 15600.83 26 112.48 33 144.42",
      "text" : ""
    }, {
      "heading" : "69 5000 6250 0.077 0.10 17 15671.14 27 101.03 25 123.92",
      "text" : ""
    }, {
      "heading" : "68 5000 6250 0.066 0.11 17 16220.33 27 111.75 25 129.70",
      "text" : ""
    }, {
      "heading" : "67 5000 6250 0.061 0.11 17 15698.53 27 103.06 25 132.57",
      "text" : ""
    }, {
      "heading" : "66 5000 6250 0.059 0.12 17 16221.44 27 115.35 25 130.19",
      "text" : ""
    }, {
      "heading" : "65 5000 6250 0.058 0.12 17 15698.02 27 113.65 25 150.95",
      "text" : ""
    }, {
      "heading" : "64 5000 3750 0.300 0.01 14 1767.63 24 78.77 30 117.03",
      "text" : ""
    }, {
      "heading" : "63 5000 3750 0.163 0.04 16 2021.49 25 82.88 33 133.36",
      "text" : ""
    }, {
      "heading" : "62 5000 3750 0.103 0.08 16 1780.97 26 88.29 32 141.14",
      "text" : ""
    }, {
      "heading" : "61 5000 3750 0.077 0.10 17 2094.73 29 95.84 33 178.13",
      "text" : ""
    }, {
      "heading" : "60 5000 3750 0.066 0.11 17 2183.90 29 98.54 25 121.39",
      "text" : ""
    }, {
      "heading" : "59 5000 3750 0.061 0.13 17 1967.39 29 114.72 35 186.34",
      "text" : ""
    }, {
      "heading" : "58 5000 3750 0.059 0.13 17 1965.36 29 111.53 35 189.05",
      "text" : ""
    }, {
      "heading" : "57 5000 3750 0.058 0.14 17 2324.54 29 99.50 35 165.12",
      "text" : ""
    }, {
      "heading" : "56 5000 1250 0.300 0.01 14 626.20 25 69.71 30 105.65",
      "text" : ""
    }, {
      "heading" : "55 5000 1250 0.163 0.05 16 719.81 25 71.23 34 147.53",
      "text" : ""
    }, {
      "heading" : "54 5000 1250 0.103 0.10 17 667.62 27 81.21 33 163.00",
      "text" : ""
    }, {
      "heading" : "53 5000 1250 0.077 0.53 17 674.71 30 121.39 35 265.84",
      "text" : ""
    }, {
      "heading" : "52 5000 1250 0.066 1.42 17 832.68 32 193.88 37 379.23",
      "text" : ""
    }, {
      "heading" : "51 5000 1250 0.061 2.13 18 892.30 36 272.03 40 604.35",
      "text" : ""
    }, {
      "heading" : "50 5000 1250 0.059 2.52 18 903.05 37 393.77 40 681.49",
      "text" : ""
    }, {
      "heading" : "49 5000 1250 0.058 2.71 18 757.67 38 408.49 40 547.93",
      "text" : "A.5 Other Methods\nA.5.1 Dual problem of CONCORD\nFormulating the dual using the matrix form is challenging since the KKT conditions involving the gradient term SΩ + ΩS do not have a closed form solution as in the case of Gaussian problem in Dalal and Rajaratnam [2014]. Therefore, we consider a vector form of the CONCORD problem by defining two new variables x1 ∈ Rp and x2 ∈ Rp(p−1)/2 as\nx1 = (ω11, ω22, . . . , ωpp) T\nx2 = (ω12, ω13, . . . , ω1p, ω23, . . . , ω2p, . . . , ωp−1p) T . (21)\nWe define two coefficient matrices A1, A2 as\nA1 =  Y1 Y2 . . .\nYp\n , A2 =  Y2 Y3 · · · Yp Y1 Y1 . . .\nY1\nY3 · · · Yp Y2\n. . . Y2\n. . . Yp−1 Yp Yp−2\nYp−2 Yp Yp−1  , (22)\nwhere A1np×p and A2np×p(p−1)/2 dimensional matrices. Using these definitions, the CONCORD problem (4) can be rewritten as\nminimize x1,x2\n− n log x1 + 1\n2 ∥∥A1x1 +A2x2∥∥2 + λ‖x2‖1. (23) where, log(x1) = ∑i=p i=1 log(x1i). We will use x = [ x1 x2 ] for simplicity of notation where ever possible.\nThe transformed CONCORD problem in (23) can be written in composite form using a new variable z = A1x1+A2x2 as\nminimize x1,x2,z\n− n log x1 + 1\n2 ∥∥z∥∥2 + λ‖x2‖1 subject to A1x1 +A2x2 = z (24)\nThe Lagrangian for this problem is given by\nL(x1, x2, z, y) = −n log x1 + 1\n2 ∥∥z∥∥2 + λ‖x2‖1 + yT (A1x1 +A2x2 − z) . (25) Maximizing with respect to the three primal variables yields following optimality conditions (the . notation is adapted from MATLAB to denote element-wise operations),\nz − y = 0 −n./x1 +A1T y = 0\nλsign(x2) +A2T y 3 0. (26)\nSubstituting these the dual problem can be written as\nmaximize y\n− n log n./A1T y + 1\n2 ∥∥y∥∥2 + yT (A1(n./A1T y)− y) subject to ‖A2T y‖∞ ≤ λ,\nor equivalently\nmaximize y\n1\n2 ∥∥y∥∥2 − n log (A1T y) + c subject to ‖A2T y‖∞ ≤ λ, (27)\nwhere, c = n log n− n2 is a constant. This problem can also be written in composite form as\nmaximize y\n1\n2 ∥∥y∥∥2 − n log (A1T y) + 1‖w‖∞≤λ subject to A2T y − w = 0. (28)\nThe gradient and hessian of the smooth function h(y) = 12 ∥∥y∥∥2 − n log (A1T y) is given by\n∇h(y) = y −A1(n./A1T y), ∇2h(y) = I +A1diag ( n./(A1 T y)2 ) A1\nT . (29) Here, the hessian is bounded away from the semi-definite boundary. Hence the function h is strongly convex with parameter 1. Moreover, on lines of Theorem 3.1, we can show that if y is restricted to a convex level set C = {y|h(y) ≤M} for some constant M , then the function h has a Lipschitz continuous gradient. Note that\n−n log (A1T y) ≤ h(y) ≤M\ne− M n ≤ A1T y. (30)\nTherefore, the hessian satisfies ∇2h(y) = I +A1diag ( n./(A1 T y)2 ) A1 T (1 + nρ(A1TA1)e 2M n )I. (31)\nTo conclude, the dual problem provides an alternate method to prove the O( 1k ) and O( 1 k2 ) rates of convergence for CONCORD problem.\nA.5.2 Proximal Newton’s Algorithm for CONCORD\nRecall that the hessian of the smooth function h1 as given in 6 is\n∇2h1(Ω) = i=p∑ i=1 ω−2ii [ eiei T ⊗ eieiT ] + 1 2 (S ⊗ I + I ⊗ S) .\nThe subproblem solved for the direction of descent for the second order PNOPT algorithm is given by\n∆Ω(k) = arg min W 〈G(k),W 〉+ 1 2 i=p∑ i=1 ω−2ii tr ( Weiei TWeiei T ) + tr (WSW ) + λ‖ΩX(k) +W‖1. (32)\nUsing these, the matrix version of the second order algorithm is given in Algorithm 3. Here, the subproblem for the descent step is as a huge Lasso problem. This can be solved by standard Lasso packages which uses coordinate descent methods.\nAlgorithm 3 CONCORD - Proximal Newton Optimization Matrix form (CONCORD-PNOPT)\nInitialize: Ω(0) ∈ Sp+, τ(0,0) = 1,∆opt = 2 opt and ∆term = 2 term while ∆subg > subg or ∆term > term do\nCompute ∇h1: G(k) = ΩD −1 + 12 ( S Ω(k) T + Ω(k)S ) Compute Newton step:\n∆Ω(k) = arg min W 〈G(k),W 〉+ 1 2 i=p∑ i=1 ω−2ii tr ( Weiei TWeiei T ) + tr (WSW ) + λ‖ΩX(k) +W‖1\nCompute sufficient descent ∆(k): ∆(k) = 〈G(k),∆Ω(k)〉+ λ ( ‖ΩX(k) + ∆ΩX(k)‖1 − ‖ΩX(k)‖1 ) Compute τk, such that Qcon(Ω(k+1)) ≤ Qcon(Ω(k)) + ατk∆(k). Update: Ω(k+1) = Ω(k) + τk∆Ω(k) Compute convergence criteria:\n∆subg = ‖∇h(Ω(k)) + ∂g(Ω(k))‖\n‖Ω(k)‖ , ∆term = ‖f(Ω(k+1))− f(Ω(k))‖ ‖f(Ω(k))‖\nend while"
    } ],
    "references" : [ {
      "title" : "DAspremont. Model Selection Through Sparse Maximum Likelihood Estimation for Multivariate Gaussian or",
      "author" : [ "Onureena Banerjee", "Laurent El Ghaoui", "Alexandre" ],
      "venue" : "Binary Data. JMLR,",
      "citeRegEx" : "Banerjee et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Banerjee et al\\.",
      "year" : 2008
    }, {
      "title" : "G-ama: Sparse gaussian graphical model estimation via alternating minimization",
      "author" : [ "Onkar Anant Dalal", "Bala Rajaratnam" ],
      "venue" : "arXiv preprint arXiv:1405.3034,",
      "citeRegEx" : "Dalal and Rajaratnam.,? \\Q2014\\E",
      "shortCiteRegEx" : "Dalal and Rajaratnam.",
      "year" : 2014
    }, {
      "title" : "Partial Correlation Estimation by Joint Sparse Regression Models",
      "author" : [ "Jie Peng", "Pei Wang", "Nengfeng Zhou", "Ji Zhu" ],
      "venue" : "Journal of the American Statistical Association,",
      "citeRegEx" : "Peng et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Peng et al\\.",
      "year" : 2009
    }, {
      "title" : "A path following algorithm for Sparse Pseudo-Likelihood Inverse Covariance Estimation (SPLICE)",
      "author" : [ "Guilherme V Rocha", "Peng Zhao", "Bin Yu" ],
      "venue" : "Technical Report 60628102,",
      "citeRegEx" : "Rocha et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Rocha et al\\.",
      "year" : 2008
    }, {
      "title" : "Applications of the lasso and grouped lasso to the estimation of sparse graphical models",
      "author" : [ "Jerome Friedman", "Trevor Hastie", "Robert Tibshirani" ],
      "venue" : "Technical report,",
      "citeRegEx" : "Friedman et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Friedman et al\\.",
      "year" : 2010
    }, {
      "title" : "A convex pseudo-likelihood framework for high dimensional partial correlation estimation with convergence guarantees",
      "author" : [ "Kshitij Khare", "Sang-Yun Oh", "Bala Rajaratnam" ],
      "venue" : "Journal of the Royal Statistical Society: Series B,",
      "citeRegEx" : "Khare et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Khare et al\\.",
      "year" : 2014
    }, {
      "title" : "A fast iterative shrinkage-thresholding algorithm for linear inverse problems",
      "author" : [ "Amir Beck", "Marc Teboulle" ],
      "venue" : "SIAM Journal on Imaging Sciences,",
      "citeRegEx" : "Beck and Teboulle.,? \\Q2009\\E",
      "shortCiteRegEx" : "Beck and Teboulle.",
      "year" : 2009
    }, {
      "title" : "Monotone operators and the proximal point algorithm",
      "author" : [ "R.T. Rockafellar" ],
      "venue" : "SIAM Journal on Control and Optimization,",
      "citeRegEx" : "Rockafellar.,? \\Q1976\\E",
      "shortCiteRegEx" : "Rockafellar.",
      "year" : 1976
    }, {
      "title" : "Two-point step size gradient methods",
      "author" : [ "J. Barzilai", "J.M. Borwein" ],
      "venue" : "IMA Journal of Numerical Analysis,",
      "citeRegEx" : "Barzilai and Borwein.,? \\Q1988\\E",
      "shortCiteRegEx" : "Barzilai and Borwein.",
      "year" : 1988
    }, {
      "title" : "Robustness, scalability, and integration of a wound-response gene expression signature in predicting breast cancer survival",
      "author" : [ "Howard Y Chang", "Dimitry S A Nuyten", "Julie B Sneddon", "Trevor Hastie", "Robert Tibshirani", "Therese Sø rlie", "Hongyue Dai", "Yudong D He", "Laura J van’t Veer", "Harry Bartelink", "Matt van de Rijn", "Patrick O Brown", "Marc J van de Vijver" ],
      "venue" : "Proceedings of the National Academy of Sciences of the United States of America,",
      "citeRegEx" : "Chang et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Chang et al\\.",
      "year" : 2005
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "The literature on this topic is simply enormous and for the sake of brevity, space constraints and the topic of this paper, we avoid an extensive literature review by referring to the references in the seminal work of Banerjee et al. [2008] and the very recent work of Dalal and Rajaratnam [2014].",
      "startOffset" : 218,
      "endOffset" : 241
    }, {
      "referenceID" : 0,
      "context" : "The literature on this topic is simply enormous and for the sake of brevity, space constraints and the topic of this paper, we avoid an extensive literature review by referring to the references in the seminal work of Banerjee et al. [2008] and the very recent work of Dalal and Rajaratnam [2014]. These two papers contain references to recent work, including past NIPS conference proceedings.",
      "startOffset" : 218,
      "endOffset" : 297
    }, {
      "referenceID" : 2,
      "context" : "To address this gap, a number of `1-penalized pseudo-likelihood approaches have been proposed: SPACE Peng et al. [2009] and SPLICE Rocha et al.",
      "startOffset" : 101,
      "endOffset" : 120
    }, {
      "referenceID" : 2,
      "context" : "To address this gap, a number of `1-penalized pseudo-likelihood approaches have been proposed: SPACE Peng et al. [2009] and SPLICE Rocha et al. [2008], SYMLASSO Friedman et al.",
      "startOffset" : 101,
      "endOffset" : 151
    }, {
      "referenceID" : 2,
      "context" : "To address this gap, a number of `1-penalized pseudo-likelihood approaches have been proposed: SPACE Peng et al. [2009] and SPLICE Rocha et al. [2008], SYMLASSO Friedman et al. [2010]. These approaches are either not convex, and/or convergence of corresponding maximization algorithms are not established.",
      "startOffset" : 101,
      "endOffset" : 184
    }, {
      "referenceID" : 2,
      "context" : "To address this gap, a number of `1-penalized pseudo-likelihood approaches have been proposed: SPACE Peng et al. [2009] and SPLICE Rocha et al. [2008], SYMLASSO Friedman et al. [2010]. These approaches are either not convex, and/or convergence of corresponding maximization algorithms are not established. In this sense, non-Gaussian partial correlation graph estimation methods have lagged severely behind, despite the tremendous need to move beyond the Gaussian framework for obvious practical reasons. In very recent work, a convex pseudo-likelihood approach with good model selection properties called CONCORD Khare et al. [2014] was proposed.",
      "startOffset" : 101,
      "endOffset" : 634
    }, {
      "referenceID" : 5,
      "context" : "(3) This coordinate-wise algorithm is shown to converge to a global minima though no rate is given [Khare et al., 2014].",
      "startOffset" : 99,
      "endOffset" : 119
    }, {
      "referenceID" : 6,
      "context" : "1: methods which are inspired by the iterative soft-thresholding algorithms in Beck and Teboulle [2009]. We undertake a comprehensive treatment of the CONCORD optimization problem by also investigating the dual of the CONCORD problem.",
      "startOffset" : 79,
      "endOffset" : 104
    }, {
      "referenceID" : 6,
      "context" : "The iterative soft-thresholding algorithms (ISTA) have recently gained popularity after the seminal paper by Beck and Teboulle Beck and Teboulle [2009]. The ISTA methods are based on the Forward-Backward Splitting method from Rockafellar [1976] and Nesterov’s accelerated gradient methods Nesterov [1983] using soft-thresholding as the proximal operator for the `1-norm.",
      "startOffset" : 109,
      "endOffset" : 152
    }, {
      "referenceID" : 6,
      "context" : "The iterative soft-thresholding algorithms (ISTA) have recently gained popularity after the seminal paper by Beck and Teboulle Beck and Teboulle [2009]. The ISTA methods are based on the Forward-Backward Splitting method from Rockafellar [1976] and Nesterov’s accelerated gradient methods Nesterov [1983] using soft-thresholding as the proximal operator for the `1-norm.",
      "startOffset" : 109,
      "endOffset" : 245
    }, {
      "referenceID" : 6,
      "context" : "The iterative soft-thresholding algorithms (ISTA) have recently gained popularity after the seminal paper by Beck and Teboulle Beck and Teboulle [2009]. The ISTA methods are based on the Forward-Backward Splitting method from Rockafellar [1976] and Nesterov’s accelerated gradient methods Nesterov [1983] using soft-thresholding as the proximal operator for the `1-norm.",
      "startOffset" : 109,
      "endOffset" : 305
    }, {
      "referenceID" : 6,
      "context" : "The iterative soft-thresholding algorithms (ISTA) have recently gained popularity after the seminal paper by Beck and Teboulle Beck and Teboulle [2009]. The ISTA methods are based on the Forward-Backward Splitting method from Rockafellar [1976] and Nesterov’s accelerated gradient methods Nesterov [1983] using soft-thresholding as the proximal operator for the `1-norm. The essence of the proximal gradient algorithms is to divide the objective function into a smooth part and a non-smooth part, then take a proximal step (w.r.t. the non-smooth part) in the negative gradient direction of the smooth part. Nesterov’s accelerated gradient extension Nesterov [1983] uses a combination of gradient and momentum steps to achieve accelerated rates of convergence.",
      "startOffset" : 109,
      "endOffset" : 665
    }, {
      "referenceID" : 8,
      "context" : "This is an approximation of the secant equation which works as a proxy for second order information using successive gradients (see Barzilai and Borwein [1988] for details).",
      "startOffset" : 132,
      "endOffset" : 160
    }, {
      "referenceID" : 5,
      "context" : "The coordinatewise method for optimizing CONCORD in Khare et al. [2014] also requires cycling through the p entries of Ω in specified order and thus does not allow parallelization.",
      "startOffset" : 52,
      "endOffset" : 72
    }, {
      "referenceID" : 5,
      "context" : "We would like to point out that, although the authors in Khare et al. [2014] provide a proof of convergence for their coordinate-wise minimization algorithm for CONCORD, they do not provide any rates of convergence.",
      "startOffset" : 57,
      "endOffset" : 77
    }, {
      "referenceID" : 5,
      "context" : "We would like to point out that, although the authors in Khare et al. [2014] provide a proof of convergence for their coordinate-wise minimization algorithm for CONCORD, they do not provide any rates of convergence. The arguments for convergence leverage the results in Beck and Teboulle [2009] but require some essential ingredients.",
      "startOffset" : 57,
      "endOffset" : 295
    }, {
      "referenceID" : 6,
      "context" : "The remaining argument for convergence follows from the theorems in Beck and Teboulle [2009].",
      "startOffset" : 68,
      "endOffset" : 93
    }, {
      "referenceID" : 5,
      "context" : "This can be attributed to min(O(np),O(p)) computational complexity of coordinate-wise CONCORD [Khare et al., 2014], and the sparse linear algebra routines used in CONCORD-ISTA and CONCORD-FISTA implementations slowing down as the number of non-zero elements in Ω increases.",
      "startOffset" : 94,
      "endOffset" : 114
    }, {
      "referenceID" : 9,
      "context" : "In this section, the performance of proposed methods are assessed on a breast cancer dataset [Chang et al., 2005].",
      "startOffset" : 93,
      "endOffset" : 113
    }, {
      "referenceID" : 5,
      "context" : "Following the approach in Khare et al. Khare et al. [2014], the number of genes are reduced by utilizing clinical information that is provided together with the microarray expression dataset.",
      "startOffset" : 26,
      "endOffset" : 59
    } ],
    "year" : 2014,
    "abstractText" : "Sparse high dimensional graphical model selection is a popular topic in contemporary machine learning. To this end, various useful approaches have been proposed in the context of `1-penalized estimation in the Gaussian framework. Though many of these inverse covariance estimation approaches are demonstrably scalable and have leveraged recent advances in convex optimization, they still depend on the Gaussian functional form. To address this gap, a convex pseudo-likelihood based partial correlation graph estimation method (CONCORD) has been recently proposed. This method uses coordinate-wise minimization of a regression based pseudo-likelihood, and has been shown to have robust model selection properties in comparison with the Gaussian approach. In direct contrast to the parallel work in the Gaussian setting however, this new convex pseudo-likelihood framework has not leveraged the extensive array of methods that have been proposed in the machine learning literature for convex optimization. In this paper, we address this crucial gap by proposing two proximal gradient methods (CONCORD-ISTA and CONCORD-FISTA) for performing `1-regularized inverse covariance matrix estimation in the pseudo-likelihood framework. We present timing comparisons with coordinate-wise minimization and demonstrate that our approach yields tremendous payoffs for `1-penalized partial correlation graph estimation outside the Gaussian setting, thus yielding the fastest and most scalable approach for such problems. We undertake a theoretical analysis of our approach and rigorously demonstrate convergence, and also derive rates thereof.",
    "creator" : "LaTeX with hyperref package"
  }
}