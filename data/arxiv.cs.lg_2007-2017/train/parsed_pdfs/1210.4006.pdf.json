{
  "name" : "1210.4006.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "The Perturbed Variation",
    "authors" : [ "Maayan Harel", "Shie Mannor" ],
    "emails" : [ "maayanga@tx.technion.ac.il", "shie@ee.technion.ac.il" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "The question of similarity between two sets of examples is common to many fields, including statistics, data mining, machine learning and computer vision. For example, in machine learning, a standard assumption is that the training and test data are generated from the same distribution. However, in some scenarios, such as Domain Adaptation (DA), this is not the case and the distributions are only assumed similar. It is quite intuitive to denote when two inputs are similar in nature, yet the following question remains open: given two sets of examples, how do we test whether or not they were generated by similar distributions? The main focus of this work is providing a similarity score and a corresponding statistical procedure that gives one possible answer to this question.\nDiscrepancy between distributions has been studied for decades, and a wide variety of distance scores have been proposed. However, not all proposed scores can be used for testing similarity. The main difficulty is that most scores have not been designed for statistical testing of similarity but equality, known as the Two-Sample Problem (TSP). Formally, let P and Q be the generating distributions of the data; the TSP tests the null hypothesisH0 : P = Q against the general alternative H1 : P 6= Q. This is one of the classical problems in statistics. However, sometimes, like in DA, the interesting question is with regards to similarity rather than equality. By design, most equality tests may not be transformed to test similarity; see Section 3 for a review of representative works.\nIn this work, we quantify similarity using a new score, the Perturbed Variation (PV). We propose that similarity is related to some predefined value of permitted variations. Consider the gait of two male subjects as an example. If their physical characteristics are similar, we expect their walk to be similar, and thus assume the examples representing the two are from similar distributions. This intuition applies when the distribution of our measurements only endures small changes for people with similar characteristics. Put more generally, similarity depends on what “small changes” are in a given application, and implies that similarity is domain specific. The PV, as hinted by its name, measures the discrepancy between two distributions while allowing for some perturbation of each distribution; that is, it allows small differences between the distributions. What accounts for small differences is a parameter of the PV, and may be defined by the user with regard to a specific domain.\nar X\niv :1\n21 0.\n40 06\nv1 [\ncs .L\nG ]\n1 5\nO ct\n2 01\nFigure 1 illustrates the PV. Note that, like perceptual similarity, the PV turns a blind eye to variations of some rate."
    }, {
      "heading" : "2 The Perturbed Variation",
      "text" : "The PV on continuous distributions is defined as follows:\nDefinition 1. Let P and Q be two distributions on a Banach space X , and let M(P,Q) be the set of all joint distributions on X × X with mariginals P and Q. The PV, with respect to a distance function d : X × X → R and , is defined by\nPV(P,Q, ,d) .= inf µ∈M(P,Q) Pµ[d(X,Y ) > ], (1)\nover all pairs (X,Y ) ∼ µ, such that the marginal of X is P and the marginal of Y is Q.\nPut into words, Equation (1) defines the joint distribution µ that couples the two distributions such that the probability of the event of a pair (X,Y ) ∼ µ being within a distance grater than is minimized.\nThe solution to (1) is a special case of the classical mass transport problem of Monge [1] and its version by Kantorovich: infµ∈M(P,Q) ∫ X×X c(x, y)dµ(x, y),where c : X ×X → R is a measurable cost function. When c is a metric, the problem describes the 1st Wasserstein metric. Problem (1) may be rephrased as the optimal mass transport problem with the cost function c(x, y) = 1[d(x,y)> ], and may be rewritten as infµ ∫∫ 1[d(x,y)> ]µ(y|x)dy P (x)dx. The probability µ(y|x) defines the transportation plan of x to y. The PV optimal transportation plan is obtained by perturbing the mass of each point x in its neighborhood so that it redistributes to the distribution of Q. These small perturbations do not add any cost, while transportation of mass to further areas is equally costly. Note that when P = Q the PV is zero as the optimal plan is simply the identity mapping. Due to its cost function, the PV it is not a metric, as it is symmetric but does not comply with the triangle inequality and may be zero for distributions P 6= Q. Despite this limitation, this cost function fully quantifies the intuition that small variations should not be penalized when similarity is considered. In this sense, similarity is not unique by definition, as more than one distribution can be similar to a reference distribution.\nThe PV is also closely related to the Total Variation distance (TV) that may be written, using a coupling characterization, as TV (P,Q) = infµ∈M(P,Q) Pµ [X 6= Y ] [2]. This formulation argues that any transportation plan, even to a close neighbor, is costly. Due to this property, the TV is known to be an overly sensitive measure that overestimates the distance between distributions. For example, consider two distributions defined by the dirac delta functions δ(a) and δ(a+ ). For any , the TV between the two distributions is 1, while they are intuitively similar. The PV resolves this problem by adding perturbations, and therefore is a natural extension of the TV. Notice, however, that the used to compute the PV need not be infinitesimal, and is defined by the user.\nThe PV can be seen as a conciliatory between the Wasserstein distance and the TV. As explained, it relaxes the sensitivity of the TV; however, it does not “over optimize” the transportation plan. Specifically, distances larger than the allowed perturbation are discarded. This aspect also contributes to the efficiency of estimation of the PV from samples; see Section 2.2."
    }, {
      "heading" : "2.1 The Perturbed Variation on Discrete Distributions",
      "text" : "It can be shown that for two discrete distributions Problem (1) is equivalent to the following problem. Definition 2. Let µ1 and µ2 be two discrete distributions on the unified support {a1, ..., aN}. Define the neighborhood of ai as ng(ai, ) = {z ; d(z, ai) ≤ }. The PV(µ1, µ2, ,d) between the two distributions is:\nmin wi≥0,vi≥0,Zij≥0\n1\n2 N∑ i=1 wi + 1 2 N∑ j=1 vj (2)\ns.t. ∑\naj∈ng(ai, )\nZij + wi = µ1(ai),∀i\n∑ ai∈ng(aj , ) Zij + vj = µ2(aj),∀j\nZij = 0 , ∀(i, j) 6∈ ng(ai, ).\nEach row in the matrix Z ∈ RN×N corresponds to a point mass in µ1, and each column to a point mass in µ2. For each i, Z(i, :) is zero in columns corresponding to non neighboring elements, and non-zero only for columns j for which transportation between µ2(aj)→ µ1(ai) is performed. The discrepancies between the distributions are depicted by the scalarswi and vi that count the “leftover” mass in µ1(ai) and µ2(aj). The objective is to minimize these discrepancies, therefore matrix Z describes the optimal transportation plan constrained to -perturbations. An example of an optimal plan is presented in Figure 2.1."
    }, {
      "heading" : "2.2 Estimation of the Perturbed Variation",
      "text" : "Typically, we are given samples from which we would like to estimate the PV. Given two samples S1 = {x1, ..., xn} and S2 = {y1, ..., ym}, generated by distributions P and Q respectively, P̂V(S1, S2, , d) is:\nmin wi≥0,vi≥0,Zij≥0\n1\n2n n∑ i=1 wi + 1 2m m∑ j=1 vj (3)\ns.t. ∑\nyj∈ng(xi, )\nZij + wi = 1, ∑\nxi∈ng(yj , )\nZij + vj = 1, ∀i, j\nZij = 0 , ∀(i, j) 6∈ ng(xi, ),\nwhere Z ∈ Rn×m. When n = m, the optimization in (3) is identical to (2), as in this case the samples define a discrete distribution. However, when n 6= m Problem (3) also accounts for the difference in the size of the two samples.\nProblem (3) is a linear program with constraints that may be written as a totally unimodular matrix. It follows that one of the optimal solutions of (3) is integral [3]; that is, the mass of each sample is transferred as a whole. This solution may be found by solving the optimal assignment on an appropriate bipartite graph [3]. Let G = (V = (A,B), E) define this graph, with A = {xi, wi ; i = 1, ..., n} and B = {yj , vj ; j = 1, ...,m} as its bipartite partition. The vertices xi ∈ A are linked\nAlgorithm 1 Compute P̂V(S1, S2, ,d)\nInput: S1 = {x1, ..., xn} and S2 = {y1, ..., ym}, rate, and distance measure d. 1. Define Ĝ = (V̂ = (Â, B̂), Ê): Â = {xi ∈ S1}, B̂ = {yj ∈ S2},\nConnect an edge eij ∈ Ê if d(xi, yj) ≤ . 2. Compute the maximum matching on Ĝ. 3. Define Sw and Sv as number of unmatched edges in sets S1 and S2 respectively. Output: P̂ V (S1, S2, , d) = 12 ( Sw n + Sv m ).\nwith edge weight zero to yj ∈ ng(xi) and with weight∞ to yj 6∈ ng(xi). In addition, every vertex xi (yj) is linked with weight 1 to wi (vj). To make the graph complete, assign zero cost edges between all vertices xi and wk for k 6= i (and vertices yj and vk for k 6= j). We note that the Earth Mover Distance (EMD) [4], a sampled version of the transportation problem, is also formulated by a linear program that may be solved by optimal assignment. For the EMD and other typical assignment problems, the computational complexity is more demanding, for example using the Hungarian algorithm it has anO(N3) complexity, whereN = n+m is the number of vertices [5]. Contrarily, graph G, which describes P̂V, is a simple bipartite graph for which maximum cardinality matching, a much simpler problem, can be applied to find the optimal assignment. To find the optimal assignment, first solve the maximum matching on the partial graph between vertices xi, yj that have zero weight edges (corresponding to neighboring vertices). Then, assign vertices xi and yj for whom a match was not found with wi and vj respectively; see Algorithm 1 and Figure 1 for an illustration of a matching. It is easy to see that the solution obtained solves the assignment problem associated with P̂V.\nThe complexity of Algorithm 1 amounts to the complexity of the maximal matching step and of setting up the graph, i.e., additional O(nm) complexity of computing distances between all points. Let k be the average number of neighbors of a sample, then the average number of edges in the bipartite graph Ĝ is |Ê| = n × k. The maximal cardinality matching of this graph is obtained in O(kn √ (n+m)) steps, in the worst case [5]."
    }, {
      "heading" : "3 Related Work",
      "text" : "Many scores have been defined for testing discrepancy between distributions. We focus on representative works for nonparametric tests that are most related to our work. First, we consider statistics for the Two Sample Problem (TSP), i.e., equality testing, that are based on the asymptotic distribution of the statistic conditioned on the equality. Among these tests is the well known Kolmogorov-Smirnov test (for one dimensional distributions), and its generalization to higher dimensions by minimal spanning trees [6]. A different statistic is defined by the portion of k-nearest neighbors of each sample that belongs to different distributions; larger portions mean the distributions are closer [7]. These scores are well known in the statistical literature but cannot be easily changed to test similarity, as their analysis relies on testing equality.\nAs discussed earlier, the 1st Wasserstein metric and the TV metric have some relation to the PV. The EMD and histogram based L1 distance are the sample based estimates of these metrics respectively. In both cases, the distance is not estimated directly on the samples, but on a higher level partition of the space: histogram bins or signatures (cluster centers). As a result, these estimators have inaccuracies. Contrarily, the PV is estimated directly on the samples and converges to its value between the underlying continuous distributions. We note that after a good choice of signatures, the EMD captures perceptual similarity, similar to that of the PV. However, due to the abstraction to signatures, the EMD does not converge to the Wasserstein metric between the continuous distributions, and therefore is commonly used to rate distances and not for statistical testing. It is possible to consider the PV as a refinement of the EMD notion of similarity; instead of clustering the data to signatures and moving the signatures, it perturbs each sample. In this manner it captures a finer notion of the perceptual similarity.\nThe partition of the support to bins allows some relaxation of the TV notion. Therefore, instead of the TV, it may be interesting to consider the L1 as a similarity distance on the measures after discretization. The example in Figure (2) shows that this relaxation is quite rigid and that there is no single partition that captures the perceptual similarity. In general, the problem would remain even if bins with varying width were permitted. Namely, the problem is the choice of a single partition to measure similarity of a reference distribution to multiple distributions, while choosing multiple partitions would make the distances incomparable. Also note that defining a “good” partition is a difficult task, which is exasperated in higher dimensions.\nThe last group of statistics are scores established in machine learning: the dA distance presented by Kifer et al. that is based on the maximum discrepancy on a chosen subset of the support [8], and Maximum Mean Discrepancy (MMD) by Gretton et al., which define discrepancy after embeddings the distributions to a Reproducing Kernel Hilbert Space (RKHS)[9]. These scores have corresponding statistical tests for the TSP; however, since their analysis is based on finite convergence bounds, in principle they may be modified to test similarity. The dA captures some intuitive notion of similarity, however, to our knowledge, it is not known how to compute it for a general subset class 1. The MMD captures the distance between the samples in some RKHS. While this distance perfectly defines an equality test, it is not clear if it translates to a well defined similarity test. As an example, consider testing if the MMD is grater than some value larger than zero using the RBF kernel. To do so, the parameter σ must be chosen in advance. Clearly, the result of the test is highly dependent on this choice, but it is not clear how it should be made. Contrarily, the PV’s parameter is related to the data’s input domain and may be chosen accordingly."
    }, {
      "heading" : "4 Analysis",
      "text" : "We present sample rate convergence analysis of the PV. The proofs of the theorems are provided in the supplementary material. When no clarity is lost, we omit d from the notation. Our main theorem is stated as follows: Theorem 3. Suppose we are given two i.i.d. samples S1 = {x1, ..., xn} ∈ Rd and S2 = {y1, ..., ym} ∈ Rd generated by distributions P and Q, respectively. Let the ground distance be d = ‖ · ‖∞ and let N ( ) be the cardinality of a disjoint cover of the distributions’ support. Then, for any δ ∈ (0, 1), N = min(n,m), and η = √ 2(log(2(2N( )−2))+log(1/δ))\nN we have that P (∣∣∣P̂V (S1, S2, )− PV (P,Q, )∣∣∣ ≤ η) ≥ 1− δ.\nThe theorem is defined using ‖ · ‖∞, but can be rewritten for other metrics (with a slight change of constants). The proof of the theorem exploits the form of the optimization Problem 3. We use the bound of Theorem 3 construct hypothesis tests. A weakness of this bound is its strong dependency on the dimension. Specifically, it is dependent onN ( ), which for ‖·‖∞ isO((1/ )d): the number of disjoint boxes of volume d that cover the support. Unfortunately, this convergence rate is inherent; namely, without making any further assumptions on the distribution, this rate is unavoidable and is an instance of the “curse of dimensionality”. In the following theorem, we present a lower bound on the convergence rate.\n1Most work with the dA has been with the subset of characteristic functions, and approximated by the error of a classifier.\nTheorem 4. Let P = Q be the uniform distribution on Sd−1, a unit (d − 1)–dimensional hypersphere. Let S1 = {x1, ..., xN} ∼ P and S2 = {y1, ..., yN} ∼ Q be two i.i.d. samples. For any , ′, δ ∈ (0, 1), 0 ≤ η < 2/3 and sample size log(1/δ)2(1−3η/2)2 ≤ N ≤ η 2e d(1− 22 )/2, we have PV (P,Q, ′) = 0 and\nP(P̂V (S1, S2, ) > η) ≥ 1− δ. (4)\nFor example, for δ = 0.01, η = 0.5, for any 37 ≤ N ≤ 0.25ed(1− 2\n2 )/2 we have that P̂V > 0.5 with probability at least 0.99. The theorem shows that, for this choice of distributions, for a sample size that is smaller than O(ed), there is a high probability that the value of P̂V is far form PV.\nIt can be observed that the empirical estimate P̂V is stable, that is, it is almost identical for two data sets differing on one sample. Due to its stability, applying McDiarmid inequality yields the following. Theorem 5. Let S1 = {x1, ..., xn} ∼ P and S2 = {y1, ..., ym} ∼ Q be two i.i.d. samples. Let n ≥ m, then for any η > 0\nP ( |P̂V (S1, S2, )− E[P̂V (n,m, )]| ≥ η ) ≤ e−η 2m2/4n,\nwhere E[P̂V (n,m, )] is the expectation of P̂V for a given sample size.\nThis theorem shows that the sample estimate of the PV converges to its expectation without dependence on the dimension. By combining this result with Theorem 3 it may be deduced that only the convergence of the bias – the difference |E[P̂V(n,m, )]−PV(P,Q, )| – may be exponential in the dimension. This convergence is distribution dependent. However, intuitively, slow convergence is not always the case, for example when the support of the distributions lies in a lower dimensional manifold of the space. To remedy this dependency we propose a bootstrapping bias correcting technique, presented in Section 5. A different possibility is to project the data to one dimension; due to space limitations, this extension of the PV is left out of the scope of this paper and presented in Appendix A.2 in the supplementary material."
    }, {
      "heading" : "5 Statistical Inference",
      "text" : "We construct two types of complementary procedures for hypothesis testing of similarity and dissimilarity2. In the first type of procedures, given 0 ≤ θ < 1, we distinguish between the null hypothesis H(1)0 : PV(P,Q, ,d) ≤ θ, which implies similarity, and the alternative hypothesis H(1)1 : PV(P,Q, ,d) > θ. Notice that when θ = 0, this test is a relaxed version of the TSP. Using PV(P,Q) = 0 instead of P = Q as the null, allows for some distinction between the distributions, which gives the needed relaxation to capture similarity. In the second type of procedures, we test whether two distributions are similar. To do so, we flip the role of the null and the alternative. Note that there isn’t an equivalent of this form for the TSP, therefore we can not infer similarity using the TSP test, but only reject equality. Our hypothesis tests are based on the finite sample analysis presented in Section 4; see Appendix A.1 in the supplementary material for the procedures.\nTo provide further inference on the PV, we apply bootstrapping for approximations of Confidence Intervals (CI). The idea of bootstrapping for estimating CIs is based on a two step procedure: approximation of the sampling distribution of the statistic by resampling with replacement from the initial sample – the bootstrap stage – following, a computation of the CI based on the resulting distribution. We propose to estimate the CI by Bootstrap Bias-Corrected accelerated (BCa) interval, which adjusts the simple percentile method to correct for bias and skewness [10]. The BCa is known for its high accuracy; particularly, it can be shown, that the BCa interval converges to the theoretical CI with rate O(N−1), where N is the sample size. Using the CI, a hypothesis test may be formed: the nullH(1)0 is rejected with significance α if the range [0, θ] 6⊂ [CI,CI]. Also, for the second test, we apply the principle of CI inclusion [11], which states that if [CI,CI] ⊂ [0, θ], dissimilarity is rejected and similarity deduced.\n2The two procedures are distinct, as, in general, lacking evidence to reject similarity is not sufficient to infer dissimilarity, and vice versa.\n10 2\n10 3\n10 4\n0\n0.2\n0.4\n0.6\n0.8\n1\nSample size\nT yp\ne− 2\ner ro\nr\nε=0.1 ε=0.2 ε=0.3 ε=0.4 ε=0.5\n(a) The Type-2 error for varying perturbation sizes and values.\n0 0.2 0.4 0.6 0.8 1 0.5\n0.6\n0.7\n0.8\n0.9\n1\nRecall\nP re\nci si\non\nPV MMD FR KNN\n(b) Precision-Recall: Gait data.\n0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 0\n0.2\n0.4\n0.6\n0.8\n1\nRecall\nP re\nci si\non\nPV MMD FR KNN\n(c) Precision-Recall: Video clips."
    }, {
      "heading" : "6 Experiments",
      "text" : ""
    }, {
      "heading" : "6.1 Synthetic Simulations",
      "text" : "In our first experiment, we examine the effect of the choice of on the statistical power of the test. For this purpose, we apply significance testing for similarity on two univariate uniform distributions: P ∼ U [0, 1] and Q ∼ U [∆( ), 1 + ∆( )], where ∆( ) is a varying size of perturbation. We considered values of = [0.1, 0.2, 0.3, 0.4, 0.5] and sample sizes up to 5000 samples from each distribution. For each value ′, we test the null hypothesis H(1)0 : PV (P,Q, ′) = 0 for ten equally spaced values of ∆( ′) in the range [0, 2 ′]. In this manner, we test the ability of the PV to detect similarity for different sizes of perturbations. The percentage of times the null hypothesis was falsely rejected, i.e. the type-1 error, was kept at a significance level α = 0.05. The percentage of times the null hypothesis was correctly rejected, the power of the test, was estimated as a function of the sample size and averaged over 500 repetitions. We repeated the simulation using the tests based on the bounds as well as using BCa confidence intervals.\nThe results in Figure (3(a)) show the type-2 error of the bound based simulations. As expected, the power of the test increases as the sample size grows. Also, when finer perturbations need to be detected, more samples are needed to gain statistical power. For the BCa CI we obtained type-1 and type-2 errors smaller than 0.05 for all the sample sizes. This shows that the convergence of the estimated PV to its value is clearly faster than the bounds. Note that, given a sufficient sample size, any statistic for the TSP would have rejected similarity for any ∆ > 0."
    }, {
      "heading" : "6.2 Comparing Distance Measures",
      "text" : "Next, we test the ability of the PV to measure similarity on real data. To this end, we test the ranking performance of the PV score against other known distributional distances. We compare the PV to the multivariate extension of the Wald-Wolfowitz score of Friedman & Rafsky (FR) [6] , Schilling’s nearest neighbors score (KNN) [7], and the Maximum Mean Discrepancy score of Gretton et al. [9] (MMD)3. We rank similarity for the applications of video retrieval and gait recognition.\nThe ranking performance of the methods was measured by precision-recall curves, and the Mean Average Precision (MAP). Let r be the number of samples similar to a query sample. For each 1 ≤ i ≤ r of these observations, define ri ∈ [1, T − 1] as its similarity rank, where T is the total number of observations. The Average Precision is: AP = 1/r ∑ i i/ri, and the MAP is the average of the AP over the queries. The tuning parameter for the methods – k for the KNN, σ for the MMD (with RBF kernel), and for the PV – were chosen by cross-validation. The Euclidian distance was used in all methods.\nIn our first experiment, we tested raking for video-clip retrieval. The data we used was collected and generated by [12], and includes 1,083 videos of commercials, each of about 1,500 frames (25 fps). Twenty unique videos were selected as query videos, each of which has one similar clip in\n3Note that the statistical tests of these measures test equality while the PV tests similarity and therefore our experiments are not of statistical power but of ranking similarity. Even in the case of the distances that may be transformed for similarity, like the MMD, there is no known function between the PV similarity to other forms of similarity. As a result, there is no basis on which to compare which similarity test has better performance.\nthe collection, to which 8 more similar clips were generated by different transformations: brightness increased/decreased, saturation increased/decreased, borders cropped, logo inserted, randomly dropped frames, and added noise frames. Lastly, each frame of a video was transformed to a 32- RGB representation. We computed the similarity rate for each query video to all videos in the set, and ranked the position of each video. The results show that the PV and the KNN score are invariant to most of the transformations, and outperform the FR and MMD methods (Table 1 and Figure 3(c)). We found that brightness changes were most problematic for the PV. For this type of distortion, the simple RGB representation is not sufficient to capture the similarity.\nWe also tested gait similarity of female and male subjects; same gender samples are assumed similar. We used gait data that was recorded by a mobile phone, available at [13]. The data consists of two sets of 15min walks of 20 individuals, 10 women and 10 men. As features we used the magnitude of the triaxial accelerometer.We cut the raw data to intervals of approximately 0.5secs, without identification of gait cycles. In this manner, each walk is represented by a collection of about 1500 intervals. An initial scaling to [0,1] was performed once for the whole set. The comparison was done by ranking by gender the 39 samples with respect to a reference walk.\nThe precision-recall curves in Figure 3(b) show that the PV is able to retrieve with higher precision in the mid-recall range. For the early recall points the PV did not show optimal performance; Interestingly, we found that with a smaller , the PV had better performance on early recall points. This behavior reflects the flexibility of the PV: smaller should be chosen when the goal is to find very similar instances, and larger when the goal is to find higher level similarity. The MAP results presented in Table 1 show that the PV had better performance on the female subjects. From examination of the subject information sheet we found that the range of weight and hight within the female group is 50-77Kg and 1.6-1.8m, while within the male group it is 47-100Kg and 1.65-1.93m; that is, there is much more variability in the male group. This information provides a reasonable explanation to the PV results, as it appears that a subject from the male group may have a gait that is as dissimilar to the gait of a female subject as it is to a different male. In the female group the subjects are more similar and therefore the precision is higher."
    }, {
      "heading" : "7 Discussion",
      "text" : "We proposed a new score that measures the similarity between two multivariate distributions, and assigns to it a value in the range [0,1]. The sensitivity of the score, reflected by the parameter , allows for flexibility that is essential for quantifying the notion of similarity. The PV is efficiently estimated from samples. Its low computational complexity relies on its simple binary classification of points as neighbors or non-neighbor points, such that optimization of distances of faraway points is not needed. In this manner, the PV captures only the essential information to describe similarity. Although it is not a metric, our experiments show that it captures the distance between similar distributions as well as well known distributional distances. Our work also includes convergence analysis of the PV. Based on this analysis we provide hypothesis tests that give statistical significance to the resulting score. While our bounds are dependent on the dimension, when the intrinsic dimension of the data is smaller than the domains dimension, statistical power can be gained by bootstrapping. In addition, the PV has an intuitive interpretation that makes it an attractive score for a meaningful statistical testing of similarity. Lastly, an added value of the PV is that its computation also gives insight to the areas of discrepancy; namely, the areas of the unmatched samples. In future work we plan to further explore this information, which may be valuable on its own merits."
    }, {
      "heading" : "A Supplementary Material",
      "text" : ""
    }, {
      "heading" : "A.1 Hypothesis Testing Procedures",
      "text" : "The statistical tests in this section are based on the convergence bounds in Section 4.\nNotations Throughout this section the probabilities P0 and P1 represent the probability conditioned on the null hypothesisH0, and the alternative hypothesisH1.\nThe following procedure tests the hypothesis H(1)0 : PV(P,Q, ) ≤ θ against the alternative H (1) 1 : PV(P,Q, ) > θ.\nProcedure 1. Similarity Testing Based on P̂V. Input: , θ and significance level α.\n1. Sample S1 = {x1, ..., xn} ∼ P and S2 = {y1, ..., ym} ∼ Q (define N = min(n,m)). 2. Normalize the data to be in [0, 1]d.\n3. Compute P̂ V (S1, S2, , ‖ · ‖∞) by Algorithm 1. 4. Compute t = √\n(2 log(2(2(1/ )d−2))+2 log(1/α) N .\nOutput: RejectH(1)0 if P̂ V (S1, S2, , ‖ · ‖∞) > t+ θ\n.\nThe probability to reject H(1)0 by applying Procedure 1 when in fact it holds – also known as the Type 1 error – is bounded in the following corollary.\nCorollary 6. Assume that for a given and θ values H(1)0 : PV (P,Q, ,d) ≤ θ holds. Then for the threshold t of Procedure 1 and any α ∈ (0, 1) we have that\nP0 ( P̂ V (S1, S2, , ‖ · ‖∞) ≥ t+ θ ) ≤ α. (5)\nMoreover, the procedure is consistent: when n,m → ∞ we have that t → 0 and P1(P̂ V (S1, S2, , ‖ · ‖∞) > θ) = 1.\nThe corollary is a direct result of Theorem 3.\nNext, we consider the probability that Procedure 1 fails to rejectH(1)0 when the alternative hypothesis H(1)1 holds, also known as the Type 2 error. Unfortunately, it is not possible to bound this probability for a finite sample of any two distributions. To see this, consider the following example: let P,Q be two distributions with PV (P,Q, ) > 0, but differ only in an area of very low probability. Then, for any finite sample size, there is a high probability that the samples are identical, resulting in P̂ V (S1, S2, ) = 0. As a result, the null hypothesis will not be rejected even thoughH(1)1 holds. However, if the PV is larger than some constant the Type 2 error is bounded. Corollary 7. For PV (P,Q, ,d) > θ + t + b, with t of Procedure 1, and b =√\n2(log(2(2(1/ )d−2))+2 log(1/β) N we have that P ( P̂ V (S1, S2, , ‖ · ‖∞) > t+ θ ) ≥ 1− β.\nNote that as N grows, the values of b and t get smaller, and the lower bound PV (P,Q, ,d) > θ + t+ b decreases.\nProof. We have that P1 ( P̂ V (S1, S2, , ‖ · ‖∞) > t+ θ ) = P1 ( P̂ V (S1, S2, , ‖ · ‖∞) > b+ t+ θ − b ) ≥\nP ( P̂ V (S1, S2, , ‖ · ‖∞) > PV (P,Q, , ‖ · ‖∞)− b ) ≥ 1− β.\nThe first inequality holds by inserting the assumption on PV , and the second holds by applying the convergence bound of Theorem 3.\nTo give an estimate of the sample size needed for the procedure, first define the effect size θ0: the minimal value of PV that is significant. Given θ0, set the sample size so that\nN ≥ 4 log(2(2 (1/ )d − 2)) + 2 log(1/α) + 2 log(1/β)\nθ20 .\nUsing this size ensures a false positive rate bounded by α (Corollary 6), and a false negative rate bounded by β (Corollary 7).\nThe second test we consider is an equivalence type test [11]. Equivalence is achieved when PV(P,Q, ) < θ, for some chosen θ, and may be obtained by switching the roles of the null and the alternative of Procedure 1. Namely, to claim similarity we need to rejectH(2)0 : PV(P,Q, ) ≥ θ. To test this hypothesis, a similar procedure to Procedure 1 may be applied, with a principal difference in the rejection area, which is changed to P̂ V (S1, S2, , ‖ · ‖∞) < θ − t.\nA.2 1D Projections\nWe present a method to gain insight on the value of the PV by multiple random projections to one dimension. The PV between two distributions is not retained after projection to a single dimension, as the projection contracts the distance between the points. However, we show that multiple projections can still aid to distinguish between two situations: PV(P,Q, ) = 0 and PV(P,Q, ) 6= 0 4. First, we define a score that is based on the value of the PV after projections.\nDefinition 8. Let fi : Rd → R for i = 1, ...,K, define random projection mappings. Let X and Y be random variables with distributions P and Q. The maximum projected score of two distributions P and Q is\nPPVK(P,Q, ) = max i=1,...,K PV(fi(X), fi(Y ), )."
    }, {
      "heading" : "For two samples S1 ∼ P and S2 ∼ Q the score is",
      "text" : "P̂PVK(S1, S2, ) = max i=1,...,K P̂V(fi(S1), fi(S2), ).\nThe next theorem presents the convergence of P̂PVK to zero for distributions with PV (P,Q) = 0.\nTheorem 9. Let P and Q be two distributions on the space ([0, 1]d,d), and S1 = {x1, ..., xn} ∼ P and S2 = {y1, ..., ym} ∼ Q two i.i.d. samples (N = min(n,m)). Perform K i.i.d. random projections of samples S1 and S2 to one dimension. If PV (P,Q, ,d) = 0, then for any δ ∈ (0, 1), with probability at least 1− δ\nP̂PVK(S1, S2, ) ≤ √\n2 log(2K(21/ − 2)/δ) N .\nProof. Given PV (P,Q, ,d) = 0, we have that for all K projections PPV1i(P,Q, , | · |) = PV (P,Q, ,d) = 0, as the projection to 1D is a non-expansion.\nIn the following we denote by P0(A) the probability of event A under the assumption PV (P,Q, ,d) = 0. Denote P̂PVi( ) = P̂V(fi(S1), fi(S2), ) as the value of PV obtained due to the ith projection.\nWe bound the probability of the event P̂PVK(S1, S2, ) ≥ η:\nP0 (\nmax 1≤i≤K\nP̂PV i( ) ≥ η ) = P0 ( ∃1 ≤ i ≤ K : P̂PV i( ) ≥ η ) ≤ K∑ i=1 P0 ( P̂PV i( ) ≥ η ) ,\n(6)\n4Recall that PV=0 not only when the distributions are equal, but also when they are similar.\nwhere the last inequality is obtained by applying the union bound.\nCombining (6) with Theorem 3, we have that for any η ∈ (0, 1) P0 (\nmax 1≤i≤K\nP̂PV i( ) ≥ η ) ≤ K max 1≤i≤K P0 ( P̂PV i( ) ≥ η ) ≤ 2K(21/ − 2)e−Nη 2/2.\nSetting δ = 2K(21/ − 2)e−Nη2/2 concludes the proof.\nFor PV(P,Q) > 0 , we provide a similar lower bound on the maximum score. We will need a further assumption for this bound. Assumption 1. Given distributions P and Q with PV(P,Q, ) > 0, they are 1D distinguishable if limK→∞ PPVK(P,Q, ) > 0 almost surely.\nThis assumption ensures that the difference in the PV value exists in at least one projection. Theorem 10. Let P and Q be two distributions on the space ([0, 1]d,d). Given i = 1, ...,K i.i.d. samples Si1 = {xi1, ..., xin} ∼ P and Si2 = {yi1, ..., yim} ∼ Q, and K mappings fi, for any two distribution that fulfill Assumption 1 there exists some q ∈ (0, 1), for which for any δ ∈ (0, 1) with probability at least 1− (q − qδ + δ)K\nP̂PV({Si1}, {Si2}, ) ≥ √\n2 log(2K(21/ − 2)/δ) N .\nThe notation P̂PV({Si1}, {Si2}, ) denotes the maximum taken over the projections of the K sets. Notice that q − qδ + δ < 1, and therefore is an exponential decay in the number of projections K.\nProof. Let f : Rd → R define a random projection mappings. Let X and Y be random variables generated by P and Q. Denote PV1 = PV(f(X), f(Y ), ), and P̂V1( ) = P̂V(f(S1), f(S2), ). Note that there are two sources of randomization, the sample’s and the projection’s, and therefore PV1 is also a random variable.\nWe have that\nP(P̂PV({Si1}, {Si2}, ) ≤ η) = P( max 1≤i≤K P̂V(fi(Si1), fi(Si2), ) ≤ η) = (7)\nP(∀ 1 ≤ i ≤ K , P̂V(fi(Si1), fi(Si2), ) ≤ η) = [P(P̂V1( ) ≤ η)]K , where the last equality holds due to the independence of the events. Next, we bound the probability P(P̂V1( ) ≤ η). We define complementary events A : PV1( ) ≥ 2η and Ac : PV1( ) < 2η.\nP(P̂V1( ) ≤ η) = P (A)P (P̂V1( ) ≤ η |A) + P (Ac)P (P̂V1( ) ≤ η |Ac) (8) ≤ P (A)P ( P̂V1( ) ≤ PV1 − η |A ) + P (Ac)P ( P̂V1( ) ≤ η |Ac )\n≤ P (A)2(21/ − 2)e−Nη 2/2 + P (Ac) ≤ P (A)2K(21/ − 2)e−Nη 2/2 + 1− P (A).\nThe inequality before last is obtained by applying Theorem 3 for any η ∈ (0, 1). For δ = 2K(21/ − 2)e−Nη̃ 2/2 we have η̃ = √\n2 log(2K(21/ −2)δ) N . Substituting this η̃ to (8) results in\nP(PV1( ) ≤ η̃) ≤ 1− (1− δ)P (A). (9)\nLet p(η, ) = P(PV1(P,Q, ) ≤ η) be the distribution of the projected PV. Clearly, p(η, ) depends on the generating distributions P and Q, and its support is [0, supi(PPVi(P,Q, ))]. We assume that supi(PPVi(P,Q, )) > 0, and therefore there must be some q ∈ (0, 1) for which\nP(PV1(P,Q, ) < 2η̃) ≤ q. (10)\nCombining the results of (7), (9) and (10), we have that for any 0 < δ < 1\nP(P̂PV({Si1}, {Si2}, ) ≤ η̃) ≤ (1− (1− δ)P(PV1( ) ≥ 2η̃))K ≤ (q − qδ + δ)K .\nTherefore, with probability at least 1− (q − qδ + δ)K P̂PV({Si1}, {Si2}, ) ≥ √\n2 log(2K(21/ − 2)/δ) N .\nNote that for any q < 1 results in q − qα + α < 1, and therefore we get exponential decay. For example for q = 1/2 (2η̃ is smaller then the median) we have (q − qδ + δ)K = ( 1+δ 2 )K .\nTheorems 9 and 10 are complementary, and may be used together to infer whether or not PV(P,Q) = 0. Next, we describe the suitable hypothesis testing procedure for this goal. Procedure 2 provides statistical tests based on the score P̂PVK (Definition 8). The procedure tests an hypothesis of the first type with θ = 0: H(1)0 : PV (P,Q, ) = 0 againstH (1) 1 : PV (P,Q, ) > 0.\nProcedure 2. Similarity testing based on P̂PVK . Input: level, number of projections K, and significance level α. For i = 1, ...,K do\n1. Sample Si1 = {x1, ..., xn} ∼ P and Si2 = {y1, ..., ym} ∼ Q i.i.d. examples on [0, 1]d. 2. Sample a unit random vector ri ∈ Sd−1. 3. Project to 1D: Psi1 = {rTi x1, ..., rTi xn} and Psi2 = {rTi y1, ..., rTi ym}. 4. Compute P̂V(Psi1, Psi2, ).\nend for Compute P̂PVK = maxi=1,...,K P̂V(Psi1, Psi2, ). Compute t = √\nlog(K)+2 log(2(21/ −2))+2 log(1/α) N , for N = min(n,m).\nOutput: RejectH0 if P̂PVK > t.\nThe next corollary bounds the Type 1 error of Procedure 2, and shows that the test is consistent. Corollary 11. Assume that the null hypothesis holds: H0 : PV (P,Q, ,d) = 0. Then, for the threshold t of Procedure 2 and any α ∈ (0, 1) we have that\nP0 ( P̂PVK(S1, S2, ) ≥ t ) ≤ α. (11)\nMoreover, when N →∞,K →∞ and log(K)N → 0 we have that P1(P̂PVK(S1, S2, ) > t) = 1.\nThe bound is obtained by Theorem 9. The consistency is conditioned on Assumption 1, and obtained by Theorem 10.\nTheorem 10 bounds the Type 2 error of Procedure 2, which is dependent on the number of projections K, and the fraction q that is distribution dependent. The bound exponentially decays as K grows, and therefore, to gain statistical power, a larger number of projections can be used."
    }, {
      "heading" : "A.3 Proof of Theorem 3",
      "text" : "We restate the theorem for clarity: Theorem 3. Suppose we are given two i.i.d. samples S1 = {x1, ..., xn} ∈ Rd and S2 = {y1, ..., ym} ∈ Rd generated by distributions P and Q, respectively. Let the ground distance be d = ‖ · ‖∞ and let N ( ) be the cardinality of a disjoint cover of the distributions’ support. Then, for any δ ∈ (0, 1), N = min(n,m), and η = √ 2(log(2(2N( )−2))+log(1/δ))\nN we have that P (∣∣∣P̂V (S1, S2, )− PV (P,Q, )∣∣∣ ≤ η) ≥ 1− δ.\nBefore proving the theorem we present the required definitions and lemmas. The proofs of the lammas are presented immediately after the proof of the theorem. We assume the domain is totally bounded, and, for simplicity of presentation, we assume the metric space is ([0, 1]d,d∞ = ‖ · ‖∞). We define a discretization on the support of the distributions. Definition 12 (Discretization). The -discretization over the space ([0, 1]d,d∞ = ‖ · ‖∞) is a partition on the set C( ) = {a1, ..., aN}, with cardinality N = (1/ )d. Each element in C( ) is the center of a box of volume d. The boxes do not intersect, and their union covers [0, 1]d. Each ai ∈ C( ) has a density equal to the distribution’s mass in its neighborhood: B(ai,d∞, ) = {z : d∞(ai, z) ≤ /2}.\nWe refer to the resulting discretized versions of the distributions P and Q as µ1( ), µ2( ) respectively. Also, let µ̂1( ), µ̂2( ) be the histograms of the samples S1 and S2, defined on the -discretization C( ).\nThe proof of Theorem 3 is based on formulating the relations between P̂V(S1, S2) and PV(µ̂1, µ̂1), and between PV (P,Q) and PV (µ1, µ2); then, turning to the discrete versions, bounding the difference between PV(µ̂1, µ̂1) and PV (µ1, µ2).\nThe relation between the different versions of the PV, continuous, discrete and sampled, is provided in the next lemma. Lemma 13. Let S1 = {x1, ..., xn} ∼ P and S2 = {y1, ..., ym} ∼ Q be two samples. Let µ1(ν) and µ2(ν) be the ν-discretizations of P and Q for any integer T > 1 and ν = T . Let µ̂1(ν) and µ̂2(ν) be their empirical distributions. The following relations hold for any , ′ = (T−1) T ,\n′′ = (T+1) T and d = ‖ · ‖∞ :\nPV(µ̂1, µ̂2, ′′) ≤ P̂V(S1, S2, ) ≤ PV(µ̂1, µ̂2, ′) (12) PV(µ1, µ2, ′′) ≤ PV(P,Q, ) ≤ PV(µ1, µ2, ′). (13)\nWe use the following structure of two discretizations. Definition 14 (Refinement of a discretization). Define an initial -discretization C1( ) = {b1, ..., bN( )} on ([0, 1]d, ‖ · ‖∞). The refinement of the discretization, for any and T > 1, is defined as a ν-discretization on C2(ν) = {a1, ..., aN(ν)}, where ν = /T , such that each element of the refinement is a result of splitting an element of the initial cover to ( /T )d elements of equal volume.\nThe next lemma bounds the difference between the PV on the discrete distributions µ̂1(ν), µ̂2(ν) and the distributions µ1(ν) and µ2(ν). Lemma 15. Let C1( ) be an -discretization on [0, 1]d, and C2(ν) its refined discretization (Definition 14). Let µ̂i( ) and µi( ) be distributions on C1( ), and µ̂i(ν) and µi(ν) distributions on the refinement C2(ν). For any ∈ (0, 1) and d = ‖ · ‖∞ we have that\n|PV(µ̂1(ν), µ̂2(ν), )− PV(µ1(ν), µ2(ν), )| ≤ 1\n2 (‖µ1( )− µ̂1( )‖1 + ‖µ2( )− µ̂2( )‖1) .\nObserve that the L1-norm is computed over the elements of C1( ).\nWe use the following result provided by [14] (Theorem 2.1).\nLemma 16. Let µ be a probability distribution on the set A = 1, ..., a. Let X = x1, x2, ..., xN be i.i.d. random variables distributed according to µ, and µ̂N the resulting empirical distribution. Then, for η > 0\nP(‖µ− µ̂N‖1 ≥ η) ≤ (2a − 2)e−Nη 2/2.\nProof of Theorem 3. Set ′ = (T−1)T and ′′ = (T+1)T , and define\nm(T ) = PV (µ1(ν), µ2(ν), ′)− PV (µ1(ν), µ2(ν), ′′).\nBy Lemma 13, the value of m(T ) is positive. Combining Lemma 13 with Lemma 15 yields\nP̂V(S1, S2, ) ≤ PV(µ̂1(ν), µ̂2(ν), ′) (14)\n≤ PV(µ1(ν), µ2(ν), ′) + 1\n2 ‖µ1( ′)− µ̂1( ′)‖1 +\n1 2 ‖µ2( ′)− µ̂2( ′)‖1\n= PV(µ1(ν), µ2(ν), ′′) +m(T ) + 1\n2 ‖µ1( ′′)− µ̂1( ′′)‖1 +\n1 2 ‖µ2( ′′)− µ̂2( ′′)‖1\n≤ PV(P,Q, ) +m(T ) + 1 2 ‖µ1( ′)− µ̂1( ′)‖1 + 1 2 ‖µ2( ′)− µ̂2( ′)‖1.\nRecall that the number of elements for an -discretization onC1( ) isN ( ) = (1/ )d. Apply Lemma 16 to ‖µ1( ′) − µ̂1( ′)‖1 ≤ η and ‖µ2( ′) − µ̂2( ′)‖1 ≤ η and combine the result with (14) using the union bound. We have that with probability at least 1− 2(2(1/ ′)d − 2)e−Nη2/2\nP̂V(S1, S2, )− PV(P,Q, ) ≤ m(T ) + η. (15)\nIn a similar manner we have\nP̂V(S1, S2, ) ≥ PV(µ̂1(ν), µ̂2(ν), ′′) (16)\n≥ PV(µ1(ν), µ2(ν), ′′)− 1\n2 ‖µ1( ′′)− µ̂1( ′′)‖1 −\n1 2 ‖µ2( ′′)− µ̂2( ′′)‖1\n= PV(µ1(ν), µ2(ν), ′)−m(T )− 1\n2 ‖µ1( ′′)− µ̂1( ′′)‖1 −\n1 2 ‖µ2( ′′)− µ̂2( ′′)‖1\n≥ PV(P,Q, )−m(T )− 1 2 ‖µ1( ′′)− µ̂1( ′′)‖1 − 1 2 ‖µ2( ′′)− µ̂2( ′′)‖1.\nCombining the result with the tail bounds of µ̂1, µ̂2 from Lemma 16, and applying the union bound, we have that with probability at least 1− 2(2(1/ ′′)d − 2)e−Nη2/2\nPV(P,Q, )− P̂V(S1, S2, ) ≤ m(T ) + η. (17)\nFor T we have that ′ ≈ ′′ = , and therefore the value of m(T ) → 0 as T → ∞. Taking T → ∞ in (15) and (17), and combining the result we get that for any δ ∈ (0, 1) and\nη =\n√ 2(log(2(2(1/ )d−2))+log(1/δ))\nN . P (∣∣∣P̂V(S1, S2, )− PV(P,Q, )∣∣∣ ≥ η) ≤ δ."
    }, {
      "heading" : "Proofs of Lemmas 13,15",
      "text" : "Proof. Lemma 13\nLet sample xi ∈ S1 belong to the element ak in the ν-discretization, that is xi ∈ B(ak, ‖ · ‖∞, ν = T ). Recall that the -neighborhood of a sample xi is the set ng(xi, ) = {z : d(xi, z) ≤ }, and the (T+1) T -neighborhood of bin ak is the set ng(ak, (T+1) T ) = {z : d(ak, z) ≤ (T+1) T }. For the left side of (12), observe that for any such xi there exists values of z such that ‖z − ak‖∞ ≤ (T+1)T\nbut ‖z − xi‖∞ > , while for any z for which ‖z − xi‖∞ ≤ also ‖z − ak‖∞ ≤ (T+1)T . As a result, ng(xi, ) ⊆ ng(ak, (T+1)T ). Enlarging the number of neighbors adds edges to the bipartite graph describing the problem, and accordingly, a matching with a larger cardinality may be found. In such a case, the number of unmatched samples is decreased, and therefore the PV is decreased, as it is the normalized sum of the unmatched samples.\nFor the right hand side of (12), observe that when the discretization is (T−1)T , for any point xi ∈ B(ak, ‖ · ‖∞, ν) we have that ng(xi, ) ⊇ ng(ak, (T−1)T ), as the -neighborhood of each point mass encloses the (T−1)T -neighborhood of its ascribed bin. As a result, the PV between the histograms µ̂1 and µ̂2 may correspond to a graph that has less edges, which may result in a maximum matching with a smaller cardinality. As a result, the discrete version may have a larger PV. Inequalities (13) hold, as the same claims apply for the discretization of the distributions.\nThe following representation of Problem (2) will be useful for the proof of Lemma 15.\nLemma 17. The solution of Problem (2) may be obtained by solving the following problem\nmin wi,vi,Zij\n1\n2 N∑ i=1 |wi|+ 1 2 N∑ j=1\n|vj | (18)∑ aj∈ng(ai, ) Zij + wi = µ1(ai), i = 1, ..., N\n∑ ai∈ng(aj , ) Zij + vj = µ2(aj), j = 1, ..., N\nZij ≥ 0, ∀i, j,\nwhich we call PVeq(µ1(ν), µ2(ν), ).\nThe lemma states that the constraints wi ≥ 0, vj ≥ 0 may be removed, and instead the sum in the objective is taken over the absolute values.\nProof. Lemma 17\nFirst note that any solution of Problem (2) is a feasible solution of Problem (18), and so we have that the optimum PV(µ1(ν), µ2(ν), ) ≥ PVeq(µ1(ν), µ2(ν), ). We construct a solution of (2) that realizes the equality, and therefore is optimal. Namely, to show the problems are equivalent it is sufficient to show that any solution of (18) has a corresponding solution of (2) with the same objective value.\nLet wi, vj , Zij be the solution to (18). In the following, we construct a feasible solution w̃i, ṽi, Z̃ij to (2):\nIf wi < 0 and vi > 0 set ∆i = |wi| and w̃i = wi + ∆i = 0, ṽi = vi + ∆i > 0, ∑\naj∈ng(ai)\nZ̃ij = ∑\naj∈ng(ai)\nZij −∆i.\nIf vi < 0 and wi > 0 set Γj = |vj | and ṽi = vi + Γi = 0, w̃i = wi + Γi > 0, ∑\naj∈ng(ai)\nZ̃ji = ∑\naj∈ng(ai)\nZji − Γi.\nIf both wi < 0 and vi < 0 set w̃i = wi + ∆i + Γi > 0, ṽi = vi + ∆i + Γi > 0, ∑\naj∈ng(ai)\n(Z̃ij + Z̃ji) = ∑\naj∈ng(ai)\n(Zij + Zji)−∆i − Γi.\nOtherwise, set w̃i = wi, ṽj = vj , and Z̃ij = Zij .\nThe resulting w̃i, ṽj , Z̃ij obey the equality constraints in (2) while fixing w̃i ≥ 0, ṽj ≥ 0. It is easy to show that there exists Z̃ij ≥ 0 that obeys the equalities above. The objective value of (18) with\nwi, vj , Zij and of (2) with w̃i, ṽj , Z̃ij is equal: N∑ i=1 w̃i + N∑ j=1 ṽj = N∑ i=1 (wi + vi)1[wi≥0 , vi≥0] + N∑ i=1 ((wi + ∆i) + vi + ∆i)1[wi<0 , vi≥0]+\nN∑ j=1 (wj + Γj + (vj + Γj))1[wj≥0 , vj<0] + N∑ i=1 ((wi + ∆i + Γi) + (vi + Γi + ∆i))1[wi<0 , vi<0]\n= N∑ i=1 |wi|+ N∑ j=1 |vj |.\nWe conclude that w̃i, ṽj , Z̃ij attains the optimal solution to Problem (2).\nProof. Lemma 15\nLet Z∗ij , w ∗ i , v ∗ j be the optimal arguments for which PV(µ1, µ2, ) is obtained (Problem (2)). There are two stages to bounding the difference between PV(µ1, µ2, ) and PV(µ̂1, µ̂2, ). First, by Lemma 17 we know that given a solution PVeq(µ̂1, µ̂2, ) we can find an equivalent solution PV(µ̂1, µ̂2, ). As a result, we may bound the difference between PV(µ1, µ2, ) and PVeq(µ̂1, µ̂2, ) instead of the difference between PV(µ1, µ2, ) and PV(µ̂1, µ̂2, ). To bound this difference, we change the solution Z∗ij , w ∗ i , v ∗ j to describe a feasible solution to Problem (18) for distributions µ̂1 and µ̂2.\nTo obtain a feasible solution to Problem (18), we must fix the violations that are made to its constraints by substituting Z∗ij , w ∗ i , v ∗ j into Problem (18). The constraints are fixed in two manners. Some constraints are fixed by optimizing the transportation plan, described by matrix Z, within the refinement of the discretization. Additional violations are fixed by changing the variables wj and vj .\nDefine sk = {ai : ai ∈ B(bk, ‖ · ‖∞, )}; i.e., the set of bins ai ∈ C2(ν) that are a refinement of element bk ∈ C1( ) (Definition 14). Let |sk| be the cardinality of this set. By definition, all the bins in sk are -neighbors: ∀ai ∈ sk, sk ∈ ng(ai, ). For any ai, aj ∈ sk, consider the following feasibility problem:\nFind Cij (19)∑ aj∈sk\nCij = ci, ∀ai ∈ sk,∑ ai∈sk Cij = bj , ∀aj ∈ sk,\nZ∗ij + Cij ≥ 0, ∀ai, aj ∈ sk, where\nci . = (µ̂1(ai)− µ1(ai))−\n1\n|sk| (µ̂1(bk)− µ1(bk)),\nbj . = (µ̂2(aj)− µ2(aj))−\n1\n|sk| (µ̂2(bk)− µ2(bk)).\nNote that ci and bi may be positive or negative, and that ∑ ai∈sk ci = 0 and ∑ aj∈sk bj = 0.\nWe show that the following values w̄i, v̄j , Z̄ij for i, j = 1, ..., N(ν) are a feasible solution to Problem (18).\nw̄i = w ∗ i +\n1\n|sk| (µ̂1(bk)− µ1(bk)) (20)\nv̄j = v ∗ j +\n1\n|sk| (µ̂2(bk)− µ2(bk))\nZ̄ij = { Z∗ij if aj ∈ sck, ai ∈ sk, Z∗ij + Cij if aj ∈ sk, ai ∈ sk,\nwhere Cij is the solution to the (19).\nFirst, we show that Problem (19) is feasible. To do so, we consider its dual representation. Define v = Vec({Cij}ai,aj∈sk) ∈ R|sk|\n2×1, the vector form of the sub-matrix {Cij}ai,aj∈sk . Similarly, let z∗ = Vec({Z∗ij}ai,aj∈sk) ∈ R|sk|\n2×1. Let A ∈ R2|sk|×|sk|2 be the zero-one matrix defined by the left-hand sides of the equality constraints in (19), and d = [c1, ..., c|sk|, b1, ..., b|sk|]\nT ∈ R2|sk|×1, the vector defined by the right-hand sides of these constraints. Using these notations, Problem (19) is equivalent to\nFind v Av = d , −v − z∗ ≤ 0,\nwhose dual representation is the existence of λ ∈ R|sk|2×1, η ∈ R2|sk|×1 for which\ng(λ, η) = inf v λT (−v − z∗) + ηT (Av − d) > 0, (21)\nλ ≥ 0.\nThe value of g(λ, η) in (21) is not −∞ only when AT η − λ = 0, for which\ng(λ, η) = inf v λT (−v − z∗) + ηT (Av − d) = inf v vT (−λ+AT η)− λT z∗ − ηT d = −λT z∗ − ηT d.\nSince z∗ ≥ 0 and λ ≥ 0, we have that−λT z∗ ≤ 0. By noting that 1T d = ∑ ai∈sk ci+ ∑ aj∈sk bj = 0, we have that −ηT d ≤ −min η` · 1T d = 0. We conclude that g(λ, η) ≤ 0, and therefore Problem (21) is infeasible. By the theorem of alternatives [15] Problem (19) is feasible.\nNext, we show that the proposed solution Z̄ij , w̄i, v̄j is indeed a feasible solution of Problem (18). The constraints Z̄ij ≥ 0 hold by the feasibility of (19). The equality constraints also hold:∑\naj∈ng(ai, )\nZ̄ij + w̄i = ∑\naj∈ng(ai, ) Z∗ij + ∑ aj∈sk Cij + w̄i =\n∑ aj∈ng(ai, ) Z∗ij + ci + w̄i = ∑ aj∈ng(ai, ) Z∗ij + µ̂1(ai)− µ1(ai)\n− 1 |sk| (µ̂1(bk)− µ1(bk)) + w∗i + 1 |sk| (µ̂1(bk)− µ1(bk))\n= µ1(ai) + (µ̂1(ai)− µ1(ai)) = µ̂1(ai),\nand ∑ ai∈ng(aj , ) Z̄ij + v̄j = ∑ ai∈ng(aj , ) Z∗ij + ∑ ai∈sk Cij + v̄j =\n∑ ai∈ng(aj , ) Z∗ij + bj + v̄j = ∑ ai∈ng(aj , ) Z∗ij + µ̂2(aj)− µ2(aj)\n− 1 |sk| (µ̂2(bk)− µ2(bk)) + v∗j + 1 |sk| (µ̂2(bk)− µ2(bk))\n= µ2(aj) + (µ̂2(aj)− µ2(aj)) = µ̂2(aj).\nTo conclude the proof, we bound the difference of the objective of Problem (2), obtained with the values Z∗ij , w ∗ i , v ∗ j , and the objective of Problem (18), obtained with the values Z̄ij , w̄i, v̄j .\nSince the discretization defined on C1(ν) is a refinement of C2( ) (Definition 14), we have that\nN(ν)∑ i=1 (|w̄i|+ |v̄i|) = N( )∑ k=1 ∑ ai∈sk (|w̄i|+ |v̄i|). (22)\nSubstituting the values of |w̄i|, |v̄i| by their assignment in (20) we obtain\n1\n2 N(ν)∑ i=1 (|w̄i|+ |v̄i|)− 1 2 N(ν)∑ i=1 (w∗i + v ∗ i ) = (23)\n1\n2 N( )∑ k=1 ∑ ai∈sk (|w̄i|+ |v̄i|)− 1 2 N(ν)∑ i=1 (w∗i + v ∗ i ) =\n1\n2 N( )∑ k=1 ∑ ai∈sk |w∗i + 1 |sk| (µ̂1(bk)− µ1(bk))|+ 1 2 N( )∑ k=1 ∑ ai∈sk |v∗i + 1 |sk| (µ̂2(bk)− µ2(bk))| − 1 2 N(ν)∑ i=1 (w∗i + v ∗ i ) .\nApplying the triangle inequality on each element in the sum:\n|w∗i + 1\n|sk| (µ̂1(bk)− µ1(bk))| ≤ |w∗i |+\n1\n|sk| |µ̂1(bk)− µ1(bk)|\n|v∗i + 1\n|sk| (µ̂2(bk)− µ2(bk))| ≤ |v∗i |+\n1\n|sk| |µ̂2(bk)− µ2(bk)|,\nas well as noting that w∗i , v ∗ j ≥ 0 by definition, we have that\n1\n2 N(ν)∑ i=1 (|w̄i|+ |v̄i|)− 1 2 N(ν)∑ i=1 (w∗i + v ∗ i ) ≤ 1 2 ‖µ̂1( )− µ1( )‖1 + 1 2 ‖µ̂2( )− µ2( )‖1.\nBy Lemma 17 we have that the solution of Problem (2) may be obtained by solving Problem (18). Therefore, combining (24) with Lemma 17 we have that\nPV (µ̂1(ν), µ̂2(ν), )− PV (µ1(ν), µ2(ν), ) = PVeq(µ̂1(ν), µ̂2(ν), )− PV (µ1(ν), µ2(ν), ) ≤\n1\n2 N(ν)∑ i=1 (|w̄i|+ |v̄i|)− 1 2 N(ν)∑ i=1 (w∗i + v ∗ i ) ≤ 1 2 ‖µ̂1( )− µ1( )‖1 + 1 2 ‖µ̂2( )− µ2( )‖1.\nThe first inequality holds as the solution Z̄ij , w̄i, v̄j is a feasible solution of Problem (18), but may not be optimal.\nUsing an analogous procedure starting at the optimal solution PV (µ̂1(ν), µ̂2(ν), ) and finding a feasible solution for distributions µ1(ν), µ2(ν) we obtain\nPV (µ1(ν), µ2(ν), )− PV (µ̂1(ν), µ̂2(ν), ) ≤ ‖µ1( )− µ̂1( ) 1\n2 ‖1 +\n1 2 ‖µ2( )− µ̂2( )‖1.\nCombining the last two inequalities concludes the proof of Lemma 15."
    }, {
      "heading" : "A.4 Proof of Theorem 4",
      "text" : "We restate the theorem:\nTheorem 4. Let P = Q be the uniform distribution on Sd−1, a unit (d − 1)–dimensional hypersphere. Let S1 = {x1, ..., xN} ∼ P and S2 = {y1, ..., yN} ∼ Q be two i.i.d. samples. For any , ′, δ ∈ (0, 1), 0 ≤ η < 2/3 and sample size log(1/δ)2(1−3η/2)2 ≤ N ≤ η/2e\nd(1− 22 )/2, we have PV (P,Q, ′) = 0 and\nP(P̂V (S1, S2, ) > η) ≥ 1− δ. (24)\nProof. We use the following definitions and lemmas.\nDefinition 18. The spherical cap of radius r about a point x is C(r, x) = { z ∈ Sd−1 : d(z, x) ≤ r } .\nLemma 19. The spherical cap of radius r about a point x on a unit sphere is equal to\nC(r, x) = { z ∈ Sd−1 :< z, x >≥ √ 1− r 2\n2\n} .\nLemma 20. Let η = √\n1− r22 . For 0 ≤ η < 1, the cap C(r, x) on S d − 1 has a measure at most\ne−dη 2/2.\nLet p = P(ngS2(x) = ∅) be the probability of an empty neighbor set. The next lemma bounds this probability.\nLemma 21. The probability of an empty neighbor set P(ngS2(x) = ∅) ≥ 1−Ne −d(1− 22 )/2."
    }, {
      "heading" : "Proof.",
      "text" : "p =P(ngS2(x) = ∅) = 1− P(ngS2(x) 6= ∅) = 1− P(∃yj ∈ S2 ; yj ∈ C( , xi))\n≥ 1−NP(y ∈ C( , x)) ≥ 1−Ne−d(1− 2 2 )/2,\nwhere the first inequality is due to the union bound, and the second by Lemma 20.\nWe consider the probability that the P̂ V is grater than some 0 ≤ η < 1. Note, that since PV (P,Q) = 0 this is also the difference between the empirical and distributional PV. Let e = {xi ∈ S1 : ngS2(xi) = ∅} be the set of samples in S1 without neighbors, and Ne its cardinality.\nP(P̂ V (S1, S2, ) > η) ≥ P( Ne N > η) = 1− P(Ne ≤ Nη) ≥ 1− P(Ne ≤ dNηe) (25)\n= 1− dNηe∑ i=0 ( N i ) (p)i(1− p)N−i.\nThe first inequality holds, as P̂ V (S1, S2, ) > η is obtained when Ne > ηN samples from S1 have no neighbors from S2 in their -neighborhood. Note that since n = m there are also exactly Ne sample from S2 which are not matched.\nBy Chernoff’s inequality we have that\ndNηe∑ i=0 ( n i ) (1− p)ipN−i ≤ exp(−2N(p− η)2). (26)\nCombining Equations (25) and (26) we get\nP(P̂ V (S1, S2, ) > η) ≥ 1− exp(−2N(p− η)2). (27)\nBy Lemma 21, we have that p ≥ 1−Ne−d(1− 2 2 )/2.\nIf 0 ≤ η < 2/3 and Ne−d(1− 2 2 )/2 < η/2, we have that\np− η ≥ 1−Ne−d(1− 2 2 )/2 − η > 1− 3η/2 > 0.\nSubstituting the last inequality to (27):\nP(P̂ V (S1, S2, ) > η) ≥ 1− exp(−2N(1− 3η/2)2).\nThe theorem statement is obtained for any N, d and η for which 2N(1− 3η/2)2 ≥ log( 1δ )."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2012,
    "abstractText" : "We introduce a new discrepancy score between two distributions that gives an indi-<lb>cation on their similarity. While much research has been done to determine if two<lb>samples come from exactly the same distribution, much less research considered<lb>the problem of determining if two finite samples come from similar distributions.<lb>The new score gives an intuitive interpretation of similarity; it optimally perturbs<lb>the distributions so that they best fit each other. The score is defined between<lb>distributions, and can be efficiently estimated from samples. We provide conver-<lb>gence bounds of the estimated score, and develop hypothesis testing procedures<lb>that test if two data sets come from similar distributions. The statistical power of<lb>this procedures is presented in simulations. We also compare the score’s capacity<lb>to detect similarity with that of other known measures on real data.",
    "creator" : "LaTeX with hyperref package"
  }
}