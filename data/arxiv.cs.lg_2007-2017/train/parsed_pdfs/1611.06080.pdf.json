{
  "name" : "1611.06080.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "A Generalized Stochastic Variational Bayesian Hyperparameter Learning Framework for Sparse Spectrum Gaussian Process Regression",
    "authors" : [ "Quang Minh Hoang", "Trong Nghia Hoang", "Kian Hsiang Low" ],
    "emails" : [ "lowkh}@comp.nus.edu.sg,", "nghiaht@nus.edu.sg" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "The machine learning community has recently witnessed the Gaussian process (GP) models gaining considerable traction in the research on kernel methods due to its expressive power and capability of performing probabilistic non-linear regression. However, a full-rank GP regression model incurs cubic time in the size of the data to compute its predictive distribution, hence limiting its use to small datasets. To lift this computational curse, a vast literature of sparse GP regression models (Quiñonero-Candela and Rasmussen 2005; Titsias 2009) have exploited a structural assumption of conditional independence based on the notion of inducing variables for achieving linear time in the data size. To scale up these sparse GP regression models further for performing real-time predictions necessary in many time-critical applications and decision support systems (e.g., environmental sensing (Cao, Low, and Dolan 2013; Dolan et al. 2009;\nCopyright c© 2017, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\nLing, Low, and Jaillet 2016; Low, Dolan, and Khosla 2008; 2009; 2011; Low et al. 2012; Podnar et al. 2010; Zhang et al. 2016), traffic monitoring (Chen et al. 2012; Chen, Low, and Tan 2013; Chen et al. 2015; Hoang et al. 2014a; 2014b; Low et al. 2014a; 2014b; Ouyang et al. 2014; Xu et al. 2014; Yu et al. 2012)), (a) distributed (Chen et al. 2013; Hoang, Hoang, and Low 2016; Low et al. 2015) and (b) stochastic (Hensman, Fusi, and Lawrence 2013; Hoang, Hoang, and Low 2015) implementations of such models have been developed to, respectively, (a) reduce their time to train with all the data by a factor close to the number of machines and (b) train with a small, randomly sampled subset of data in constant time per iteration of stochastic gradient ascent update and achieve asymptotic convergence to their predictive distributions.\nOn the other hand, there is a less well-explored, alternative class of low-rank GP approximations that exploit sparsity in the spectral representation of a GP kernel (Gal and Turner 2015; Lázaro-Gredilla et al. 2010) for gaining time efficiency and have empirically demonstrated competitive predictive performance for datasets of up to tens of thousands in size but, surprisingly, not received as much attention and research effort. In contrast to the above literature, such sparse spectrum GP regression models do not need to introduce an additional set of inducing inputs which is computationally challenging to be jointly optimized, especially with a large number of them that is necessary for accurate predictions. Unfortunately, the sparse spectrum GP (SSGP) model of Lázaro-Gredilla et al. (2010) does not scale well to massive datasets due to its linear time in the data size and also finds a point estimate of the spectral frequencies of its kernel that risks overfitting. The recent variational SSGP (VSSGP) model of Gal and Turner (2015) has attempted to address both shortcomings of SSGP with a stochastic implementation and a Bayesian treatment of the spectral frequencies, respectively. However, VSSGP has imposed the following highly restrictive structural assumptions to facilitate analytical derivations but potentially compromise its predictive performance: (a) The spectral frequencies are assumed to be fully independent a posteriori in its variational distribution, and (b) every test output assumes a deterministic relationship with the spectral frequencies in its test conditional and is thus conditionally independent of the training data, including the local data “close” to it (in the correlaar X iv :1 61 1. 06 08 0v 1\n[ st\nat .M\nL ]\n1 8\nN ov\n2 01\ntion sense). As such, open questions remain whether VSSGP can still perform competitively well with these highly restrictive structural assumptions for massive, million-sized datasets and, more importantly, whether these assumptions can be relaxed to improve the predictive performance while preserving scalability to big data.\nTo tackle these questions, this paper presents a novel generalized framework of stochastic variational Bayesian sparse spectrum GP (sVBSSGP) regression models which (a) enables the spectral frequencies to interact a posteriori by modeling them jointly in its variational distribution, and (b) spans a spectrum of test conditionals that can trade off between the contributions of the degenerate test conditional of VSSGP vs. the local SSGP model trained with local data (Section 2). However, such proposed structural improvements over VSSGP and SSGP to boost the predictive performance result in a variational lower bound that is intractable to be optimized. To overcome this computational difficulty, we exploit a variational parameterization trick to make the variational lower bound amenable to stochastic optimization, which still incurs linear time in the data size per iteration of stochastic gradient ascent update (Section 3). Interestingly, we can exploit the linearly decomposable structure of this stochastic gradient to refine our stochastic optimization method to incur only constant time per iteration while preserving its property of being an unbiased estimator of the exact gradient of the variational lower bound (Section 4). We empirically evaluate the predictive performance and time efficiency of sVBSSGP on three real-world datasets, one of which is millions in size (Section 5)."
    }, {
      "heading" : "2 A Generalized Bayesian Sparse Spectrum Gaussian Process Regression Framework",
      "text" : "Let X be a d-dimensional input space such that each input vector x ∈ X is associated with a latent output fx and a noisy output yx , fx + generated by perturbing fx with a random noise ∼ N (0, σ2n) where σ2n is the noise variance. Let {fx}x∈X denote a Gaussian process (GP), that is, every finite subset of {fx}x∈X follows a multivariate Gaussian distribution. Then, the GP is fully specified by its prior mean E[fx] (i.e., assumed to be 0 for notational simplicity) and covariance k(x,x′) , cov[fx, fx′ ] for all x,x′ ∈ X , the latter of which can be defined by the commonly-used squared exponential kernel k(x,x′) , σ2sexp(−0.5(x − x′)>∆−1(x − x′)) where ∆ , diag[`21, `22, . . . , `2d] and σ2s are its squared length-scale and signal variance hyperparameters, respectively. Such a kernel can be expressed as the Fourier transform of a density function p(r) over the domain of frequency vector r whose coefficients form a set of trigonometric basis functions (Lázaro-Gredilla et al. 2010): k(x,x′) = Er∼p(r)[ σ2scos(2πr>(x− x′)) ] (1) where p(r) , N ( 0, (4π2∆)−1 ) . In the same spirit as that of Lázaro-Gredilla et al. (2010), we approximate the kernel in (1) by its unbiased estimator constructed from m i.i.d. sampled spectral frequencies ri for i = 1, . . . ,m: k(x,x′)' σ 2 s\nm m∑ i=1 cos(2πr>i (x−x′))=φ>θ (x)Λφθ(x′) (2)\nwhere φθ(x) , [φ1θ(x), φ 2 θ(x), . . . , φ 2m θ (x)] > denotes a vector of basis functions φ2i−1θ (x) , cos(2πr > i x) and φ2iθ (x) , sin(2πr > i x) for i = 1, . . . ,m, Λ , (σ 2 s/m)I, and θ , vec(r1, r2, . . . , rm). Learning the length-scales in ∆ of the original kernel is thus cast as optimizing θ in this alternative representation (2) of the kernel. Then, the induced covariance matrix K(X,X) for any finite subset X , {xi}ni=1 of training inputs can be written as K(X,X) , Φ>θ (X)ΛΦθ(X) where Φθ(X) , [φθ(x1) . . .φθ(xn)].\nTo learn the optimal parameters of the distribution over θ (i.e., not known in advance) given by N (0, (4π2∆θ)−1) (i.e., derived from p(r) below (1)) where ∆θ , I ⊗∆, we adopt the standard Bayesian treatment for θ by first imposing a prior distribution θ ∼ N (0,Θ) for some covariance Θ designed a priori to reflect our knowledge about θ and then using training data to infer its posterior which is expected to closely approximate the optimal distribution. This signifies a key difference between our generalized framework and the sparse spectrum GP (SSGP) model of Lázaro-Gredilla et al. (2010), the latter of which finds a point estimate of θ via maximum likelihood estimation that risks overfitting.\nAs shall be elucidated later in this paper, the finite trigonometric representation of the kernel (2) can be used to efficiently and scalably compute the predictive distribution of our framework by exploiting some mild structural assumptions. Specifically, we assume that for any finite subset X ⊂ X , a vector s of nuisance variables exists for which the joint distribution of f , [fx]>x∈X and s conditioned on θ is[ s f ] ∼ N ([ 0 0 ] , [ Λ ΛΦθ(X)\nΦ>θ (X)Λ Φ > θ (X)ΛΦθ(X)\n]) . (3)\nIntuitively, s can be interpreted as the latent outputs of some imaginary inputs U , {uj}2mj=1 such that φiθ(uj) , I(i = j). Using (2), it follows immediately that K(U,U) = Λ, K(U,X) = ΛΦθ(X), and K(X,U) = Φ>θ (X)Λ which reproduce the covariance matrix in (3). Secondly, we assume that given α , vec(θ, s), any latent test output depends on only a small subset of local training data of fixed size: Supposing the input space is partitioned into p disjoint subspaces (i.e., X = ⋃p i=1 Xi) which directly induce a partition\non the training inputs X = ⋃p i=1 Xi such that Xi ⊂ Xi, p(fx∗ |y,α) = p(fx∗ |yk,α) for any test input x∗ ∈ Xk and k = 1, . . . , p where y , [yx]>x∈X and yk , [yx] > x∈Xk . Then, the predictive distribution can be computed using p(fx∗ |y) = Eα∼p(α|y)[ p(fx∗ |yk,α) ] (4) which reveals that it can be evaluated by deriving posterior distribution p(α|y) described later in Section 3, and the test conditional p(fx∗ |yk,α) consistent with the above structural assumption of s: Marginalizing out nuisance variables s from such a test conditional should yield p(fx∗ |yk,θ), i.e., p(fx∗ |yk,θ) = ∫ s p(fx∗ |yk,α) p(s|yk,θ) ds where both p(fx∗ |yk,θ) and p(s|yk,θ) can be derived from (3), as shown in Appendix A.\nOur first result below derives a spectrum of consistent test conditionals in our generalized framework that trade off between the use of global information α vs. local data (Xk,yk), albeit to varying degrees controlled by γ:\nProposition 1 For all x∗ ∈ Xk and |γ| ≤ 1, define the test conditional p(fx∗ |yk,α) , N (µx∗(α), σ2x∗(α)) where\nµx∗(α) , γφ > θ (x∗)s + (1− γ)φ>θ (x∗)Γ−1k Φθ(Xk)yk , σ2x∗(α) , (1− γ 2)σ2nφ > θ (x∗)Γ −1 k φθ(x∗) ,\nand Γk , Φθ(Xk)Φ>θ (Xk) + σ 2 nΛ −1. Then, p(fx∗ |yk,α) is consistent with the structural assumption of s in (3). Its proof is in Appendix A. Remark 1 The special case of γ = 1 recovers the degenerate test conditional p(fx∗ |α) = N (φ>θ (x∗)s, 0) induced by the variational SSGP (VSSGP) model of Gal and Turner (2015) (see equation 4 and Section 3 therein) which reveals that it imposes a highly restrictive deterministic relationship between fx∗ and α and also fails to exploit the local data (Xk,yk) (i.e., due to conditional independence between fx∗ and yk given α) that can potentially improve the predictive performance. Unfortunately, VSSGP cannot be trivially extended to span the entire spectrum since it relies on the induced deterministic relationship between fx∗ andα to analytically derive its predictive distribution, which does not hold for γ 6= 1. On the other hand, when γ = 0, the test conditional in Proposition 1 becomes independent of the nuisance variables s and reduces to the predictive distribution p(fx∗ |yk,θ) of the SSGP model of Lázaro-Gredilla et al. (2010) (see equation 7 therein) given its point estimate of the spectral frequencies θ but restricted to the local data (Xk,yk) corresponding to input subspaceXk. Hence, γ can also be perceived as a controlling parameter that trades off between the contributions of the degenerate test conditional of VSSGP vs. the local SSGP model to constructing a test conditional in our generalized framework. To investigate this trade-off, our experiments in Section 5 show that given the learned θ, the predictive performance is maximized at γ = 0 when the test conditional p(fx∗ |yk,α) depends on local data (Xk,yk) but not s, which justifies viewing s as a “nuisance” to prediction despite its crucial role in scalable learning of θ via stochastic optimization (Section 4).\nIn the sections to follow, we will propose a novel stochastic optimization method for deriving a variationally optimal approximation to the posterior distribution p(α|y) that is used to marginalize out the global information α from any test conditional p(fx∗ |yk,α) in Proposition 1 to yield an asymptotic approximation to the predictive distribution p(fx∗ |y) (4) regardless of the value of γ ∈ [−1, 1]."
    }, {
      "heading" : "3 Variational Inference for Bayesian Sparse Spectrum Gaussian Process Regression",
      "text" : "This section presents a variational approximation q(α) of the posterior distribution p(α|y) achieved by using variational inference which involves choosing a parameterization for q(α) (Section 3.1) and optimizing its defining parameters (Section 3.2) to minimize its Kullback-Leibler (KL) distance DKL(q) , KL(q(α)‖p(α|y)) to p(α|y). The optimized q(α) can then be used as a tractably cheap surrogate of p(α|y) for marginalizing outα from test conditional p(fx∗ |yk,α) in Proposition 1 to derive an approximation to predictive distribution p(fx∗ |y) (4) efficiently (Section 4)."
    }, {
      "heading" : "3.1 Variational Parameterization",
      "text" : "Specifically, we parameterize α = vec(θ, s) , Mz + b where the variational parameters η , vec(M,b) are independent of (θ, s), and z is distributed by an analytically tractable user-specified distribution ψ(z) ' p(z|y) that is straightforward to draw samples from (i.e., z ∼ ψ(z)). We assume that ψ(z) is analytically differentiable with respect to z and the affine matrix M is invertible such that z = M−1(α− b) exists. Then, q(θ, s) can be expressed in terms of M, b, and ψ(z): Lemma 1 The variational distribution q(θ, s) can be indirectly parameterized via ψ(z) by q(θ, s) = q(α) = ψ(M−1(α− b))/|M| = ψ(z)/|M| where |M| denotes the absolute value of det(M). Lemma 1 follows directly from the change of variables theorem. Using the above parameterization ofα, we can also express the prior p(θ, s) in terms of variational parameters M and b. To see this, recall that p(θ) = N (0,Θ) and p(s) = N (0,Λ) (see Section 2 and (3)). This implies p(α) = p(vec(θ, s)) = p(θ) p(s) = N (α|0,blkdiag[Θ,Λ]). Plugging in the expression of α yields p(α) = N (Mz + b|0,blkdiag[Θ,Λ])1. At this time, the need to parameterize q(α) in Lemma 1 may not be obvious to a reader because it is motivated by a technical necessity to guarantee the asymptotic convergence of our stochastic optimization method (Remark 4) rather than a conceptual intuition. Remark 2 Our generalized framework enables both the spectral frequencies θ and the nuisance variables s to interact a posteriori by modeling them jointly in the variational distribution q(θ, s), as detailed in Lemma 1, and still preserves scalability (Section 4). In contrast, the VSSGP model of Gal and Turner (2015) assumes r1, . . . , rm, and s to be statistically independent a posteriori in its variational distribution (see section 4 therein). Relaxing this assumption will cause VSSGP to lose its scalability as its induced variational lower bound will inevitably become intractable. Remark 3 Though the model of Titsias and LázaroGredilla (2014) has adopted a similar parameterization but only for the original GP hyperparameters, it incurs cubic time in the data size per iteration of gradient ascent update, as shown in its supplementary materials and experiments. A factorized marginal likelihood has to be further assumed for this parameterization to achieve scalability. Our framework does not require such a strong assumption and still scales well to million-sized datasets (Section 5). This is, perhaps surprisingly, achieved through introducing the nuisance variables s (Section 2), which interestingly embeds a linearly decomposable structure within the log-likelihood of data log p(y|α) instead of log p(y|θ). As shown later in Theorem 1, such a structure is the key ingredient for developing our stochastic optimization method (Section 4)."
    }, {
      "heading" : "3.2 Variational Optimization",
      "text" : "To optimize q(α) (i.e., by minimizing DKL(q)), we first show that the log-marginal likelihood log p(y) can be de-\n1Note that ψ(z) is meant to approximate the posterior distribution of z given y instead of its prior distribution. So, it cannot be used to derive p(α) by applying the affine transformation on z.\ncomposed into a sum of a lower-bound functional L(q) and DKL(q): log p(y) = L(q) + DKL(q) where L(q) , Eα∼q(α)[log p(y|α)− log(q(α)/p(α))], as detailed in Appendix B. So, minimizing DKL(q) is equivalent to maximizing L(q) with respect to the variational parameter η = vec(M,b) of q(α) (Lemma 1) since log p(y) is a constant (i.e., independent of η). In practice, this is usually achieved by setting the derivative ∂L/∂η = 0 and solving for η, which is unfortunately intractable since its exact analytic expression is not known.\nTo sidestep this intractability issue, we instead adopt a stochastic optimization method that is capable of maximizing L(q) via iterative stochastic gradient ascent updates. However, this also requires the stochastic gradient ∂L̂/∂η (6) to be an analytically tractable and unbiased estimator of the exact gradient ∂L/∂η. To derive this, we first exploit Lemma 1 to re-express L(q) as an expectation with respect to z ∼ ψ(z): L(q) = Ez∼ψ(z)[log p(y|α) − log(q(α)/p(α))] where α = Mz + b, as shown in Appendix C. Then, taking the derivatives with respect to η on both sides of the above equation yields ∂L/∂η = Ez∼ψ(z)[ ∂(log p(y|α)− log(q(α)/p(α)))/∂η ] (5) which reveals a simple, analytically tractable (proven in Appendix D using Lemma 1) choice for our stochastic gradient\n∂L̂/∂η , ∂(log p(y|α)− log(q(α)/p(α)))/∂η . (6) Thus, Ez∼ψ(z)[∂L̂/∂η] = ∂L/∂η guarantees that ∂L̂/∂η is indeed an unbiased estimator of ∂L/∂η. Remark 4 (5) reveals that parameterizing q(α) indirectly through ψ(z) in Lemma 1 is essential to enabling stochastic optimization in our generalized framework: Since ψ(z) does not depend on the variational parameters η = vec(M,b), the derivative operator in (5) can be moved inside the expectation, which trivially reveals an unbiased stochastic estimate (6) of the exact gradient via sampling z. Otherwise, suppose that we attempt to derive a stochastic gradient for L(q) using the original expression L(q) = Eα∼q(α)[log p(y|α)−log(q(α)/p(α))] instead of (5) in the same manner with a direct parameterization of q(α) not exploiting ψ(z). Then, after differentiating both sides of the above expression with respect to η, the derivative operator on the RHS cannot be moved inside the expectation over α ∼ q(α) since it depends on η, which suggests that deriving an unbiased estimator for ∂L/∂η has become non-trivial without using the parameterization of q(α) in Lemma 1.\nTo understand why ∂L̂/∂η (6) is analytically tractable, it suffices to show that both its derivatives ∂ log p(y|α)/∂η and ∂ log(q(α)/p(α))/∂η in (6) are analytically tractable, the latter of which is detailed in Appendix D. For the former, since the trigonometric basis functions {φiθ(x)}2mi=1 (Section 2) are differentiable, it can be shown that ∂ log p(y|α)/∂η is analytically tractable by exploiting the following result giving a closed-form expression of log p(y|α) in terms of θ and s: Lemma 2 log p(y|α) = −0.5σ−2n v>v − 0.5n log(2πσ2n) where v , y −Φ>θ (X)s.\nIts proof is in Appendix E. The above choice of ∂L̂/∂η (6) and Lemma 2 show that it is not only an unbiased estimator of the exact gradient but is also analytically tractable, which satisfies all the required conditions to guarantee the asymptotic convergence of our proposed stochastic optimization method. However, a critical issue remains that makes it scale poorly in practice: It can be derived from Lemma 2 that naively evaluating its derivative ∂ log p(y|α)/∂η in (6) in a straightforward manner incurs linear time in data size n per iteration of stochastic gradient ascent update, which is expensive for massive datasets."
    }, {
      "heading" : "4 Stochastic Optimization",
      "text" : "To overcome the issue of scalability in evaluating the stochastic gradient ∂L̂/∂η (6) (Section 3.2), we will show in Theorem 1 below that ∂ log p(y|α)/∂η is decomposable into a linear sum of analytically tractable terms, each of which depends on only a small subset of local data. Interestingly, since only the derivative ∂ log p(y|α)/∂η in (6) involves the training data (X,y), Theorem 1 implies a similar decomposition of ∂L̂/∂η. As a result, we can derive a new stochastic estimate of the exact gradient ∂L/∂η that can be computed efficiently and scalably using only one or a few randomly sampled subset(s) of local data of fixed size and still preserves the property of being its unbiased estimator (Section 4.1). Computing this new stochastic gradient incurs only constant time in the data size n per iteration of stochastic gradient ascent update, which, together with Proposition 1 in Section 2, constitute the foundation of our generalized framework of stochastic variational Bayesian SSGP regression models for big data (Section 4.2)."
    }, {
      "heading" : "4.1 Stochastic Gradient Revisited",
      "text" : "To derive a computationally scalable and unbiased stochastic gradient, we rely on our main result below showing the decomposability of ∂ log p(y|α)/∂η in (6) into a linear sum of analytically tractable terms, each of which depends on only a small subset of local training data of fixed size:\nTheorem 1 Let vi , yi−Φ>θ (Xi)s for i = 1, . . . , p. Then, ∂ log p(y|α)/∂η = ∑p i=1 Fi(η,α) where Fi(η,α) , −0.5σ−2n ∇η(v>i vi) is analytically tractable. Its proof in Appendix F utilizes Lemma 2. Using Theorem 1 , the following linearly decomposable structure of ∂L̂/∂η (6) results:\n∂L̂/∂η = Ei∼U(1,p)[ pFi(η,α)− ∂ log(q(α)/p(α))/∂η ] (7) where i is treated as a discrete random variable uniformly distributed over the set of partition indices {1, 2, . . . , p}. This interestingly reveals an unbiased estimator for the stochastic gradient ∂L̂/∂η which can be constructed stochastically by sampling i:\n∂L̃/∂η , pFi(η,α)− ∂ log(q(α)/p(α))/∂η\nsuch that substituting it into (7) yields Ei∼U(1,p)[∂L̃/∂η] = ∂L̂/∂η. Then, taking the expectation over z ∼ ψ(z) on both sides of this equality gives Ez∼ψ(z)[Ei∼U(1,p)[∂L̃/∂η]] =\nEz∼ψ(z)[∂L̂/∂η] = ∂L/∂η, which proves that ∂L̃/∂η is also an unbiased estimator of ∂L/∂η that can be constructed by sampling both i ∼ U(1, p) and z ∼ ψ(z) (Section 3.1) independently. Replacing ∂L̂/∂η with ∂L̃/∂η produces a highly efficient stochastic gradient ascent update that incurs constant time in the data size n per iteration since ∂L̃/∂η can be computed using only a single randomly sampled subset of local data (Xi,yi) of fixed size.\nInstead of utilizing just a single pair (i, z) of samples, the above stochastic gradient ∂L̃/∂η can be generalized to simultaneously process multiple pairs of independent samples and their corresponding sampled subsets of local data in one stochastic gradient ascent update (detailed in Appendix G), thereby improving the rate of convergence while preserving its property of an unbiased estimator of the exact gradient ∂L/∂η. Asymptotic convergence of the estimate of η (and hence the estimate of q(α)) is guaranteed if the step sizes are scheduled appropriately."
    }, {
      "heading" : "4.2 Approximate Predictive Inference",
      "text" : "In iteration t of stochastic gradient ascent update, an estimate qt(α) of the variationally optimal approximation q(α) can be induced from the current estimate ηt = vec(Mt,bt) of its variational parameters η using the parameterization in Lemma 1. As a result, using the law of iterated expectations, the predictive mean can be approximated by µ̂x∗ = Eα∼qt(α)[E[fx∗ |yk,α]] = Eα∼qt(α)[µx∗(α)] =∫ α qt(α)µx∗(α)dα where α = vec(θ, s) and µx∗(α) is previously defined in Proposition 1. However, since the integration over α is not always analytically tractable, we approximate it by drawing i.i.d. samples α1, . . . ,αr from qt(α) to estimate µ̂x∗ ' r−1 ∑r i=1 µx∗(αi). Similarly, using the variance decomposition formula and definition of variance, the predictive variance can be approximated by σ̂2x∗ = Eα∼qt(α)[σ 2 x∗(α)] + Vα∼qt(α)[µx∗(α)] = Eα∼qt(α)[σ2x∗(α)+µ 2 x∗(α)]− µ̂ 2 x∗ ' r −1∑r i=1(σ 2 x∗(αi)+ µ2x∗(αi))−µ̂ 2 x∗ where σ 2 x∗(α) is previously defined in Proposition 1 and the Vα∼qt(α)[µx∗(α)] term arises due to the uncertainty of α. The samples α1, . . . ,αr are in turn obtained by sampling z1, . . . , zr from ψ(z) and applying the parametric transformation in Lemma 1 with respect to the current estimates Mt and bt, that is, vec(θi, si) = αi = Mtzi+bt for i = 1, . . . , r. Computing the predictive mean µ̂x∗ and variance σ̂2x∗ thus incurs constant time in the data size n, hence achieving efficient approximate predictive inference."
    }, {
      "heading" : "5 Empirical Studies",
      "text" : "This section empirically evaluates the predictive performance and time efficiency of our sVBSSGP model on three real-world datasets: (a) The AIMPEAK dataset (Chen et al. 2013) consists of 41800 traffic speed observations (km/h) along 775 urban road segments during the morning peak hours on April 20, 2011. Each observation features a 5- dimensional input vector of a road segment’s length, number of lanes, speed limit, direction, and its recording time (i.e., discretized into 54 five-minute time slots), and a corresponding output measuring the traffic speed (km/h); (b) the benchmark AIRLINE dataset (Hensman, Fusi, and Lawrence\n2013; Hoang, Hoang, and Low 2015) contains 2000000 information records of US commercial flights in 2008. Each record features a 8-dimensional input vector of the aircraft’s age (year), travel distance (km), the flight’s total airtime, departure time, arrival time (min), and the date given by day of week, day of month, and month, and a corresponding output of the flight’s delay time (min); and (c) the BLOG feedback dataset (Buza 2014) contains 60000 instances of blog posts. Each blog post features a fairly large 60-dimensional input vector associated with its first 60 attributes described at https://archive.ics.uci.edu/ml/datasets/BlogFeedback, and a corresponding output measuring the number of comments in the next 24 hours. The BLOG dataset is used to evaluate the robustness of sVBSSGP to overfitting which usually occurs in training with datasets of high input dimensions. All datasets are modeled using GPs with prior covariance defined in Section 2 and split into 95% training data and 5% test data. All experimental results are averaged over 5 random splits. For the AIMPEAK, AIRLINE, and BLOG datasets, we use, respectively, 2m = 40, 40, and 10 trigonometric basis functions to approximate the GP kernel (2) and sample z from N (0, I) (Section 3.1). All experiments are run on a Linux system with Intelr Xeonr E5-2670 at 2.6GHz with 96 GB memory.\nThe performance of sVBSSGP is compared against the state-of-the-art VSSGP (Gal and Turner 2015) and stochastic implementations of sparse GP models based on inducing variables such as DTC+ and PIC+ (Hensman, Fusi, and Lawrence 2013; Hoang, Hoang, and Low 2015) (i.e., run with their GitHub codes) using the following metrics: (a) Root mean square error (RMSE) {|X|−1 ∑ x∗∈X(yx∗ − µ̂x∗) 2}1/2 over the set X of test inputs, (b) mean neg-\native log probability (MNLP) 0.5|X|−1 ∑\nx∗∈X{(yx∗ − µ̂x∗) 2/σ̂2x∗ + log(2πσ̂ 2 x∗)}, and (c) training time. AIMPEAK Dataset. This dataset is evenly partitioned into p = 200 disjoint subsets using k-means (k = p). Fig. 1 shows results of RMSEs of sVBSSGP for γ = 0, 0.1, 0.2, 0.3 that rapidly decrease by 4- to 5-fold over 30 iterations. It can also be observed that increasing γ from 0 results in higher converged RMSEs. To further investigate this, Fig. 2a reveals that sVBSSGP indeed achieves the lowest converged RMSE at γ = 0 among all tested values of γ. This confirms our hypothesis stated earlier in Remark 1 that given the learned spectral frequencies θ, the information carried by s becomes a “nuisance” to prediction despite its interaction with θ in q(θ, s) = q(α) during stochastic optimization. That is, when the influence of s on the test output is completely removed from the test conditional by setting γ = 0 in Proposition 1, the predictions are no longer inter-\nfered by the nuisance information of s, hence explaining the lowest RMSE achieved by sVBSSGP at γ = 0.\nFig. 2b and Table 1 show that sVBSSGP (γ = 0 and r = 20) significantly outperforms VSSGP, DTC+, and PIC+ (250 inducing variables) in terms of RMSE after 30 iterations. To explain this, DTC+ and PIC+ find point estimates of the kernel hyperparameters, which may have resulted in their poorer performance. Though VSSGP also adopts a Bayesian treatment of the spectral frequencies, it uses a degenerate test conditional corresponding to the case of γ = 1 in Proposition 1. As a result, VSSGP imposes a highly restrictive deterministic relationship between the test output and spectral frequencies and also fails to exploit the local data for prediction (see Remark 1). The results of the MNLP metric are similar and reported in Appendix H. AIRLINE Dataset. This dataset is partitioned into p = 2000 disjoint subsets using k-means. Fig. 2c and Table 1 show that sVBSSGP (γ = 0 and r = 5) significantly outperforms VSSGP, DTC+, and PIC+ (512 inducing variables) in terms of RMSE after 45 iterations, as explained previously. Fig. 2d shows that the total training time of sVBSSGP increases linearly with the number t of iterations, which highlights a principled trade-off between its predictive performance and time efficiency. The training time of sVBSSGP, though longer than DTC+ and PIC+, is only 43 sec. per iteration; the training time of VSSGP is not included since its GitHub code runs on GPU instead of CPU. The results of MNLP metric are similar to that for the AIMPEAK dataset. BLOG Dataset. This dataset is evenly partitioned into p = 200 disjoint subsets using k-means. Fig. 3 shows that with more samples drawn from qt(α) to compute predictive mean µ̂x∗ (Section 4.2), sVBSSGP tends to converge faster and to\na lower RMSE, which suggests a greater robustness to overfitting by exploiting a higher degree of Bayesian model averaging. More interestingly, the effect of overfitting appears to be more pronounced for the BLOG dataset with a much larger input dimension of 60: When r = 1, sVBSSGP effectively reduces to a local SSGP model utilizing the sampled spectral frequencies as its point estimate (see Proposition 1, Remark 1, and Section 4.2) and converges to the poorest performance that stops improving after 10 iterations. It can also be observed that the performance gap between sVBSSGP’s with different number r of samples is much wider at early iterations, thus highlighting the practicality of our Bayesian treatment in the case of limited data."
    }, {
      "heading" : "6 Conclusion",
      "text" : "This paper describes a novel generalized framework of sVBSSGP regression models that addresses the shortcomings of existing sparse spectrum GP models like SSGP (Lázaro-Gredilla et al. 2010) and VSSGP (Gal and Turner 2015) by adopting a Bayesian treatment of the spectral frequencies to avoid overfitting, modeling the spectral frequencies jointly in its variational distribution to enable their interaction a posteriori, and exploiting local data for improving the predictive performance while still being able to preserve its scalability to big data through stochastic optimization. As a result, empirical evaluation on real-world datasets (i.e., including the million-sized benchmark AIRLINE dataset) shows that our proposed sVBSSGP regression model can significantly outperform the existing sparse spectrum GP models like SSGP and VSSGP as well as the stochastic implementations of the sparse GP models based on inducing variables like DTC+ and PIC+ (Hensman, Fusi, and Lawrence 2013; Hoang, Hoang, and Low 2015). Acknowledgments. This research is supported by the National Research Foundation, Prime Minister’s Office, Singapore under its International Research Centre in Singapore Funding Initiative and Campus for Research Excellence and Technological Enterprise (CREATE) programme."
    }, {
      "heading" : "A Proof of Proposition 1",
      "text" : "Since {fx}x∈X denotes a zero-mean Gaussian process with kernel k(x,x′) and the noisy output yx = fx + is generated by perturbing fx with a random noise ∼ N (0, σ2n) (Section 2), p(fx∗ |yk,θ) = N (E[fx∗ |yk,θ],V[fx∗ |yk,θ]) for x∗ ∈ Xk where E[fx∗ |yk,θ] , K(x∗,Xk)Ξkyk , V[fx∗ |yk,θ] , k(x∗,x∗)−K(x∗,Xk)ΞkK(Xk,x∗) , (8)\nand Ξk , (K(Xk,Xk) + σ2nI) −1, which is essentially the standard GP predictive distribution of fx∗ given the noisy outputs yk for the training inputs Xk and the hyperparameters θ. Using (2), Ξk = (Φ>θ (Xk)ΛΦθ(Xk) + σ 2 nI) −1 = σ−2n I−σ−4n Φ>θ (Xk)(σ−2n Φθ(Xk)Φ>θ (Xk)+Λ−1)−1Φθ(Xk) = σ−2n (I−Φ>θ (Xk)Γ−1k Φθ(Xk)) where the second and third equalities are due to the matrix inversion lemma and the definition of Γk, respectively. Using (2), (8) can also be rewritten as\nE [fx∗ |yk,θ] = γHµ+ (1− γ)Hµ V [fx∗ |yk,θ] = (1− γ2)HΣH> + γ2HΣH>\nfor all |γ| ≤ 1 where H , φ>θ (x∗), µ , ΛΦθ(Xk)Ξkyk\n= σ−2n Λ(I−Φθ(Xk)Φ>θ (Xk)Γ−1k )Φθ(Xk)yk = σ−2n Λ(I− (Γk − σ2nΛ−1)Γ−1k )Φθ(Xk)yk = Γ−1k Φθ(Xk)yk\nwhere the third equality is due to the definition of Γk, and\nΣ , Λ−ΛΦθ(Xk)ΞkΦ>θ (Xk)Λ = Λ− Γ−1k Φθ(Xk)Φ > θ (Xk)Λ\n= Λ− Γ−1k (Γk − σ 2 nΛ −1)Λ = σ2nΓ −1 k .\nUsing the Gaussian identity of affine transformation for marginalization,\np (fx∗ |yk,θ) = ∫ s N ( fx∗ |µx∗(α), σ2x∗(α) ) N (s|µ,Σ) ds\n(9) where µx∗(α) , γHs + (1 − γ)Hµ and σ2x∗(α) , (1− γ2)HΣH>. Also, marginalizing out [fx]>x∈X\\Xk from p(f , s|θ) in (3) (Section 2) yields[\ns fk\n] ∼ N ([ 0 0 ] , [ Λ ΛΦθ(Xk)\nΦ>θ (Xk)Λ K(Xk,Xk) ]) where fk , [fx]>x∈Xk . Consequently,[\ns yk\n] ∼ N ([ 0 0 ] , [ Λ ΛΦθ(Xk)\nΦ>θ (Xk)Λ Ξ −1 k\n]) ,\nwhich allows us to derive an analytic expression for p(s|yk,θ) = N (s|µ,Σ) where µ and Σ are defined as above. Plugging this expression into (9) yields\np (fx∗ |yk,θ) = ∫ s N ( fx∗ |µx∗(α), σ2x∗(α) ) p(s|yk,θ) ds .\n(10)\nAlternatively, p(fx∗ |yk,θ) can be expressed in terms of p(fx∗ |yk, s,θ) and p(s|yk,θ) using marginalization:\np (fx∗ |yk,θ) = ∫ s p (fx∗ |yk, s,θ) p(s|yk,θ) ds . (11)\nThen, subtracting (10) from (11) gives 0 = Es∼p(s|yk,θ) [ p(fx∗ |yk, s,θ)−N ( fx∗ |µx∗(α), σ2x∗(α) ) ] .\nSince p(s|yk,θ) > 0 for all s, the above equation suggests that p(fx∗ |yk, s,θ) = N ( fx∗ |µx∗(α), σ2x∗(α) ) is a valid test conditional which is consistent with the structural assumption in (3). Finally, plugging in the above definitions of H, µ, and Σ reproduces the definitions of µx∗(α) and σ2x∗(α) in Proposition 1, thus completing our proof.\nB Derivation of L(q) For all α, p(y) = p(α,y)/p(α|y) which implies\nlog p(y) = log p(α,y)\np(α|y) . (12)\nThen, let q(α) denotes an arbitrary probability density function of α (i.e., ∫ α q(α)dα = 1). Integrating both sides of (12) with q(α) gives\nlog p(y) = ∫ α q(α) log p(α,y) p(α|y) dα . (13)\nThen, plugging\nlog p(α,y)\np(α|y) = log\np(α,y)\nq(α) + log\nq(α)\np(α|y)\ninto the RHS of (13) yields\nlog p(y) = ∫ α q(α) log p(α,y) q(α) dα+KL (q(α)‖p(α|y))\n= Eα∼q(α) [ log p(α,y)\nq(α)\n] +DKL(q)\n(14) where the second equality is due to the definition ofDKL(q) in Section 3. Finally, plugging p(α,y) = p(y|α) p(α) into (14) yields\nlog p(y) = Eα∼q(α) [ log p(y|α)− log q(α)\np(α)\n] +DKL(q)\n= L(q) +DKL(q) ,\nwhich concludes our proof.\nC Reparameterizing L(q) via ψ(z) To do this, let us first define the following function:\ng(α) , q(α) [ log p(y|α)− log q(α)\np(α)\n] , (15)\nwhich allows us to re-express L(q) as\nL(q) = ∫ α g(α) dα . (16)\nThen, applying the change of variables theorem to the RHS of (16) with respect to a sufficiently well-behaved function α , ϕ(z), which transforms a variable z into α, gives\nL(q) = ∫ z g(ϕ(z)) |Jϕ(z)|dz = ∫ z g(α) |Jϕ(z)|dz (17)\nwhere |Jϕ(z)| denotes the absolute value of det(Jϕ(z)) , det(∂ϕ(z)/∂z).\nSo, choosing ϕ(z) , Mz + b yields |Jϕ(z)| = |M| which can be plugged into (17) to give\nL(q) = ∫ z g(α) |M|dz\n= ∫ z q(α) |M| [ log p(y|α)− log q(α) p(α) ] dz (18)\nwhere the last equality follows from our definition of g(α) in (15). Now, using the parameterization in Lemma 1, q(α) = ψ(z)/|M| or, equivalently, ψ(z) = q(α)|M|. Thus, plugging this into (18) yields\nL(q) = ∫ z ψ (z) [ log p(y|α)− log q(α) p(α) ] dz\n= Ez∼ψ(z) [ log p(y|α)− log q(α)\np(α)\n] ,\nwhich completes our proof."
    }, {
      "heading" : "D Closed-Form Evaluation of ∂L̂/∂η",
      "text" : "To show that ∂L̂/∂η is analytically tractable, it suffices to show that both ∂ log p(y|α)/∂η and ∂ log(q(α)/p(α))/∂η are analytically tractable, the former of which is shown in Appendix F. To show the latter, note that η = vec(M,b) (Section 3.1) implies\n∂\n∂η log\nq(α) p(α) = vec\n( ∂\n∂M log\nq(α) p(α) , ∂ ∂b log q(α) p(α)\n) ,\nwhich reveals that ∂ log(q(α)/p(α))/∂η can be analytically evaluated if ∂ log(q(α)/p(α))/∂M and ∂ log(q(α)/p(α))/∂b can be analytically evaluated, as detailed below.\nTo evaluate the derivative ∂ log(q(α)/p(α))/∂M, note that ∂ log(q(α)/p(α))/∂M = ∂ log q(α)/∂M − ∂ log p(α)/∂M. Then, using the parameterization of q(α) , ψ(z)/|M| in Lemma 1, it follows that log q(α) = logψ(z) − log |M| which immediately implies ∂ log q(α)/∂M = −∂ log |M|/∂M = −(M−1)>. Note that ∂ logψ(z)/∂M = 0 since the above derivatives are evaluated with respect to a sampled value of z, which effectively makes logψ(z) a constant that does not depend on either M or b.\nAlso, since p(α) = N (α|0,blkdiag[Θ,Λ]) (Section 3.1), ∂ log p(α)/∂α = −blkdiag[Θ−1,Λ−1]α. Let α , [αk]>k , M , [Mij ]i,j , z , [zj ] > j , and b = [bk] > k . Then, by applying the chain rule of derivatives, it follows that ∂ log p(α)/∂M = ∑ k(∂αk/∂M)(∂ log p(α)/∂αk) where ∂ log p(α)/∂αk corresponds to the k-th component of the gradient vector ∂ log p(α)/∂α and ∂αk/∂M ,\n[∂αk/∂Mij ]i,j . Since α = Mz + b (Section 3.1), αk =∑ jMkjzj + bk which implies ∂αk/∂Mij = I(k = i)zj . Putting all of the above together therefore yields the following analytic expression for ∂ log(q(α)/p(α))/∂M:\n∂\n∂M log\nq(α) p(α) = −\n( M−1 )> −∑ k ∂αk ∂M ∂ log p(α) ∂αk\nwhere ∂ log p(α)/∂α = −blkdiag[Θ−1,Λ−1]α and ∂αk/∂M = [I(k = i)zj ]i,j are derived previously. So, ∂ log(q(α)/p(α))/∂M is analytically tractable.\nLikewise, to derive the derivative ∂ log(q(α)/p(α))/∂b, note that ∂ log q(α)/∂b = 0 as log q(α) = logψ(z) − log |M| does not depend on b since z is a sampled value that makes logψ(z) a constant. Also, ∂ log p(α)/∂b = (∂α/∂b)(∂ log p(α)/∂α) = ∂ log p(α)/∂α since ∂α/∂b = I which is implied by the fact that α = Mz + b (Section 3.1). Hence, ∂ log(q(α)/p(α))/∂b = −∂ log p(α)/∂α is analytically tractable, as shown previously."
    }, {
      "heading" : "E Proof of Lemma 2",
      "text" : "Using a derivation similar to that in Appendix A,\np(y|θ) = N ( y|0,Φ>θ (X)ΛΦθ(X) + σ2nI ) =\n∫ s N ( y|Φ>θ (X)s, σ2nI ) N(s|0,Λ) ds\nwhere the last equality follows from the Gaussian identity of affine transformation for marginalization. Also, from (3), p(s) = N (s|0,Λ) and hence\np(y|θ) = ∫ s N ( y|Φ>θ (X)s, σ2nI ) p(s) ds . (19)\nOn the other hand, p(y|θ) can also be expressed in terms of p(y|θ, s) and p(s) using marginalization:\np(y|θ) = ∫ s p(y, s|θ)ds = ∫ s p(y|θ, s)p(s|θ)ds\n= ∫ s\np(y|θ, s)p(s)ds (20)\nwhere the last equality of (20) follows from our setting in Section 2 and (3) that θ and s are statistically independent a priori. Subtracting both sides of (19) from that of (20) consequently yields\n0 = ∫ s ( p(y|θ, s)−N ( y|Φ>θ (X)s, σ2nI )) p(s) ds ,\nwhich directly implies p(y|θ, s) = N (y|Φ>θ (X)s, σ2nI) since p(s) > 0 for all s. Then, since α , vec(θ, s) (Section 3.1), this result can be rewritten more concisely as p(y|α) = N (y|Φ>θ (X)s, σ2nI). Finally, taking the logarithm on both sides of this equation completes our proof of Lemma 2."
    }, {
      "heading" : "F Proof of Theorem 1",
      "text" : "Note that v>v = ∑p i=1 v > i vi. Plugging this into Lemma 2 yields\nlog p(y|α) = −0.5σ−2n p∑ i=1 v>i vi−0.5n log(2πσ2n) . (21)\nTaking derivatives with respect to η on both sides of (21) gives\n∂\n∂η log p(y|α) = −0.5σ−2n p∑ i=1 ∇η(v>i vi)\n= −0.5 p∑ i=1 ri = p∑ i=1 Fi(η,α)\nwhere the last two equalities follow from the definitions of ri and Fi(η,α) in Theorem 1. This completes our proof.\nClosed-Form Evaluation of Fi(η,α). To show that Fi(η,α) and hence ∂ log p(y|α)/∂η can be analytically evaluated, it suffices to show that ∇η(v>i vi) is analytically tractable.\nTo understand this, note that ∂vi/∂s and ∂vi/∂θ are both analytically tractable since vi is linear in s while the trigonometric basis functions constituting Φθ(Xi) are analytically differentiable with respect to θ. This therefore implies ∂vi/∂α is also analytically tractable since α = vec(θ, s).\nRecall from Appendix D that α , [αk]>k . Then, by applying the chain rule of derivatives,\n∂vi ∂M = ∑ k ∂αk ∂M ∂vi ∂αk , ∂vi ∂b = ∂α ∂b ∂vi ∂α\nwhere ∂αk/∂M and ∂α/∂b = I have previously been derived in Appendix D. Thus, ∂vi/∂η is analytically tractable, which shows that Fi(η,α) is indeed analytically tractable."
    }, {
      "heading" : "G Generalized Stochastic Gradient",
      "text" : "Let S , {I,Z} where I , {ik}ak=1 and Z , {zj}bj=1 denote the sets of i.i.d. random samples drawn from U(1, p) and ψ(z) (Section 3.1), respectively. Define the stochastic gradient of L(q) with respect to η = vec(M,b) as\n∂L̃ ∂η , 1 ab a∑ k=1 b∑ j=1 ( pFik(η,αj)− ∂ ∂η log q(αj) p(αj) ) (22)\nwhere αj = Mzj + b. The result below shows that the generalized stochastic gradient ∂L̃/∂η (22) is an unbiased estimator of the exact gradient ∂L/∂η:\nProposition 2 ES [ ∂L̃/∂η ] = ∂L/∂η.\nProof Since I = {ik}ak=1 and Z = {zj}bj=1 are sampled independently,\nES\n[ ∂L̃\n∂η\n] , EK,Z [ ∂L̃\n∂η\n] = EZ [ EK [ ∂L̃\n∂η\n]] . (23)\nAlso, since i1, . . . , ia are sampled independently from the same uniform distribution U(1, p) over a discrete set of partition indices {1, 2, . . . , p},\nEK\n[ ∂L̃\n∂η\n]\n= 1\nab a∑ k=1 b∑ j=1 ( pEik∼U(1,p)[Fik(η,αj)]− ∂ ∂η log q(αj) p(αj) ) = 1\nab a∑ k=1 b∑ j=1 ( pEi∼U(1,p)[Fi(η,αj)]− ∂ ∂η log q(αj) p(αj) ) = 1\nb b∑ j=1 ( pEi∼U(1,p)[Fi(η,αj)]− ∂ ∂η log q(αj) p(αj) ) .\n(24) To simplify (24), Ei∼U(1,p)[Fi(η,αj)] can be re-expressed as\nEi∼U(1,p)[Fi(η,αj)] = p∑ i=1 U(1, p)Fi(η,αj)\n= 1\np p∑ i=1 Fi(η,αj)\n= 1\np\n∂\n∂η log p(y|αj)\n(25)\nwhere the last equality follows directly from Theorem 1. Then, plugging (25) into (24),\nEK\n[ ∂L̃\n∂η\n] = 1\nb b∑ j=1 ( ∂ ∂η log p(y|αj)− ∂ ∂η log q(αj) p(αj) ) .\n(26) Hence, taking expectation over Z on both sides of (26) yields\nEK,Z\n[ ∂L̃\n∂η\n]\n= 1\nb b∑ j=1 Ezj∼ψ(z) [ ∂ ∂η log p(y|αj)− ∂ ∂η log q(αj) p(αj) ] = 1\nb b∑ j=1 ∂L ∂η\n= ∂L\n∂η (27)\nwhere the first equality is due to the fact that z1, . . . , zb are sampled independently from the same distribution ψ(z) while the second equality follows from (5). Finally, plugging (27) into (23) shows that ∂L̃/∂η is an unbiased estimator of ∂L/∂η, thereby completing our proof."
    }, {
      "heading" : "H Supplementary Experiments",
      "text" : ""
    } ],
    "references" : [ {
      "title" : "Feedback prediction for blogs",
      "author" : [ "K. Buza" ],
      "venue" : "Spiliopoulou, M.; Schmidt-Thieme, L.; and Janning, R., eds., Data Analysis, Machine Learning and Knowledge Discovery. Springer International Publishing. 145–152.",
      "citeRegEx" : "Buza,? 2014",
      "shortCiteRegEx" : "Buza",
      "year" : 2014
    }, {
      "title" : "Multi-robot informative path planning for active sensing of environmental phenomena: A tale of two algorithms",
      "author" : [ "N. Cao", "K.H. Low", "J.M. Dolan" ],
      "venue" : "Proc. AAMAS.",
      "citeRegEx" : "Cao et al\\.,? 2013",
      "shortCiteRegEx" : "Cao et al\\.",
      "year" : 2013
    }, {
      "title" : "Decentralized data fusion and active sensing with mobile sensors for modeling and predicting spatiotemporal traffic phenomena",
      "author" : [ "J. Chen", "K.H. Low", "C.K.-Y. Tan", "A. Oran", "P. Jaillet", "J.M. Dolan", "G.S. Sukhatme" ],
      "venue" : "Proc. UAI, 163–173.",
      "citeRegEx" : "Chen et al\\.,? 2012",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2012
    }, {
      "title" : "Parallel Gaussian process regression with low-rank covariance matrix approximations",
      "author" : [ "J. Chen", "N. Cao", "K.H. Low", "R. Ouyang", "C.K.-Y. Tan", "P. Jaillet" ],
      "venue" : "Proc. UAI, 152–161.",
      "citeRegEx" : "Chen et al\\.,? 2013",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2013
    }, {
      "title" : "Gaussian process decentralized data fusion and active sensing for spatiotemporal traffic modeling and prediction in mobilityon-demand systems",
      "author" : [ "J. Chen", "K.H. Low", "P. Jaillet", "Y. Yao" ],
      "venue" : "IEEE T-ASE 12(3):901–921.",
      "citeRegEx" : "Chen et al\\.,? 2015",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2015
    }, {
      "title" : "Gaussian process-based decentralized data fusion and active sensing for mobility-on-demand system",
      "author" : [ "J. Chen", "K.H. Low", "C.K.-Y. Tan" ],
      "venue" : "Proc. RSS.",
      "citeRegEx" : "Chen et al\\.,? 2013",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2013
    }, {
      "title" : "Cooperative aquatic sensing using the telesupervised adaptive ocean sensor fleet",
      "author" : [ "J.M. Dolan", "G. Podnar", "S. Stancliff", "K.H. Low", "A. Elfes", "J. Higinbotham", "J.C. Hosler", "T.A. Moisan", "J. Moisan" ],
      "venue" : "Proc. SPIE Conference on Remote Sensing of the Ocean, Sea Ice, and Large Water",
      "citeRegEx" : "Dolan et al\\.,? 2009",
      "shortCiteRegEx" : "Dolan et al\\.",
      "year" : 2009
    }, {
      "title" : "Improving the Gaussian process sparse spectrum approximation by representing uncertainty in frequency inputs",
      "author" : [ "Y. Gal", "R. Turner" ],
      "venue" : "Proc. ICML, 655–664.",
      "citeRegEx" : "Gal and Turner,? 2015",
      "shortCiteRegEx" : "Gal and Turner",
      "year" : 2015
    }, {
      "title" : "Gaussian processes for big data",
      "author" : [ "J. Hensman", "N. Fusi", "N.D. Lawrence" ],
      "venue" : "Proc. UAI, 282–290.",
      "citeRegEx" : "Hensman et al\\.,? 2013",
      "shortCiteRegEx" : "Hensman et al\\.",
      "year" : 2013
    }, {
      "title" : "Active learning is planning: Nonmyopic -Bayesoptimal active learning of Gaussian processes",
      "author" : [ "T.N. Hoang", "K.H. Low", "P. Jaillet", "M. Kankanhalli" ],
      "venue" : "Proc. ECML/PKDD Nectar Track, 494–498.",
      "citeRegEx" : "Hoang et al\\.,? 2014a",
      "shortCiteRegEx" : "Hoang et al\\.",
      "year" : 2014
    }, {
      "title" : "Nonmyopic -Bayes-Optimal Active Learning of Gaussian Processes",
      "author" : [ "T.N. Hoang", "K.H. Low", "P. Jaillet", "M. Kankanhalli" ],
      "venue" : "Proc. ICML, 739–747.",
      "citeRegEx" : "Hoang et al\\.,? 2014b",
      "shortCiteRegEx" : "Hoang et al\\.",
      "year" : 2014
    }, {
      "title" : "A unifying framework of anytime sparse Gaussian process regression models with stochastic variational inference for big data",
      "author" : [ "T.N. Hoang", "Q.M. Hoang", "K.H. Low" ],
      "venue" : "Proc. ICML, 569–578.",
      "citeRegEx" : "Hoang et al\\.,? 2015",
      "shortCiteRegEx" : "Hoang et al\\.",
      "year" : 2015
    }, {
      "title" : "A distributed variational inference framework for unifying parallel sparse Gaussian process regression models",
      "author" : [ "T.N. Hoang", "Q.M. Hoang", "K.H. Low" ],
      "venue" : "Proc. ICML, 382–391.",
      "citeRegEx" : "Hoang et al\\.,? 2016",
      "shortCiteRegEx" : "Hoang et al\\.",
      "year" : 2016
    }, {
      "title" : "Sparse spectrum Gaussian process regression",
      "author" : [ "M. Lázaro-Gredilla", "J. Quiñonero-Candela", "C.E. Rasmussen", "A.R. Figueiras-Vidal" ],
      "venue" : "JMLR 1865–1881.",
      "citeRegEx" : "Lázaro.Gredilla et al\\.,? 2010",
      "shortCiteRegEx" : "Lázaro.Gredilla et al\\.",
      "year" : 2010
    }, {
      "title" : "Gaussian process planning with Lipschitz continuous reward",
      "author" : [ "C.K. Ling", "K.H. Low", "P. Jaillet" ],
      "venue" : null,
      "citeRegEx" : "Ling et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Ling et al\\.",
      "year" : 2016
    }, {
      "title" : "Decentralized active robotic exploration and mapping for probabilistic field classification in environmental sensing",
      "author" : [ "K.H. Low", "J. Chen", "J.M. Dolan", "S. Chien", "D.R. Thompson" ],
      "venue" : "Proc. AAMAS, 105–112.",
      "citeRegEx" : "Low et al\\.,? 2012",
      "shortCiteRegEx" : "Low et al\\.",
      "year" : 2012
    }, {
      "title" : "Recent advances in scaling up Gaussian process predictive models for large spatiotemporal data",
      "author" : [ "K.H. Low", "J. Chen", "T.N. Hoang", "N. Xu", "P. Jaillet" ],
      "venue" : "Proc. DyDESS.",
      "citeRegEx" : "Low et al\\.,? 2014a",
      "shortCiteRegEx" : "Low et al\\.",
      "year" : 2014
    }, {
      "title" : "Generalized online sparse Gaussian processes with application to persistent mobile robot localization",
      "author" : [ "K.H. Low", "N. Xu", "J. Chen", "K.K. Lim", "E.B. Özgül" ],
      "venue" : "Proc. ECML/PKDD Nectar Track, 499–503.",
      "citeRegEx" : "Low et al\\.,? 2014b",
      "shortCiteRegEx" : "Low et al\\.",
      "year" : 2014
    }, {
      "title" : "Parallel Gaussian process regression for big data: Low-rank representation meets Markov approximation",
      "author" : [ "K.H. Low", "J. Yu", "J. Chen", "P. Jaillet" ],
      "venue" : "Proc. AAAI.",
      "citeRegEx" : "Low et al\\.,? 2015",
      "shortCiteRegEx" : "Low et al\\.",
      "year" : 2015
    }, {
      "title" : "Adaptive multi-robot wide-area exploration and mapping",
      "author" : [ "K.H. Low", "J.M. Dolan", "P. Khosla" ],
      "venue" : "Proc. AAMAS, 23–30.",
      "citeRegEx" : "Low et al\\.,? 2008",
      "shortCiteRegEx" : "Low et al\\.",
      "year" : 2008
    }, {
      "title" : "Informationtheoretic approach to efficient adaptive path planning for mobile robotic environmental sensing",
      "author" : [ "K.H. Low", "J.M. Dolan", "P. Khosla" ],
      "venue" : "Proc. ICAPS.",
      "citeRegEx" : "Low et al\\.,? 2009",
      "shortCiteRegEx" : "Low et al\\.",
      "year" : 2009
    }, {
      "title" : "Active Markov information-theoretic path planning for robotic environmental sensing",
      "author" : [ "K.H. Low", "J.M. Dolan", "P. Khosla" ],
      "venue" : "Proc. AAMAS, 753–760.",
      "citeRegEx" : "Low et al\\.,? 2011",
      "shortCiteRegEx" : "Low et al\\.",
      "year" : 2011
    }, {
      "title" : "Multirobot active sensing of non-stationary Gaussian processbased environmental phenomena",
      "author" : [ "R. Ouyang", "K.H. Low", "J. Chen", "P. Jaillet" ],
      "venue" : "Proc. AAMAS.",
      "citeRegEx" : "Ouyang et al\\.,? 2014",
      "shortCiteRegEx" : "Ouyang et al\\.",
      "year" : 2014
    }, {
      "title" : "Telesupervised remote surface water quality sensing",
      "author" : [ "G. Podnar", "J.M. Dolan", "K.H. Low", "A. Elfes" ],
      "venue" : "Proc. IEEE Aerospace Conference.",
      "citeRegEx" : "Podnar et al\\.,? 2010",
      "shortCiteRegEx" : "Podnar et al\\.",
      "year" : 2010
    }, {
      "title" : "A unifying view of sparse approximate Gaussian process regression",
      "author" : [ "J. Quiñonero-Candela", "C.E. Rasmussen" ],
      "venue" : "Journal of Machine Learning Research 6:1939–1959.",
      "citeRegEx" : "Quiñonero.Candela and Rasmussen,? 2005",
      "shortCiteRegEx" : "Quiñonero.Candela and Rasmussen",
      "year" : 2005
    }, {
      "title" : "Doubly stochastic variational Bayes for non-conjugate inference",
      "author" : [ "M.K. Titsias", "M. Lázaro-Gredilla" ],
      "venue" : "Proc. ICML, 1971–1979.",
      "citeRegEx" : "Titsias and Lázaro.Gredilla,? 2014",
      "shortCiteRegEx" : "Titsias and Lázaro.Gredilla",
      "year" : 2014
    }, {
      "title" : "Variational learning of inducing variables in sparse Gaussian processes",
      "author" : [ "M.K. Titsias" ],
      "venue" : "Proc. AISTATS.",
      "citeRegEx" : "Titsias,? 2009",
      "shortCiteRegEx" : "Titsias",
      "year" : 2009
    }, {
      "title" : "GP-Localize: Persistent mobile robot localization using online sparse Gaussian process observation model",
      "author" : [ "N. Xu", "K.H. Low", "J. Chen", "K.K. Lim", "E.B. Özgül" ],
      "venue" : "Proc. AAAI, 2585–2592.",
      "citeRegEx" : "Xu et al\\.,? 2014",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2014
    }, {
      "title" : "Hierarchical Bayesian nonparametric approach to modeling and learning the wisdom of crowds of urban traffic route planning agents",
      "author" : [ "J. Yu", "K.H. Low", "A. Oran", "P. Jaillet" ],
      "venue" : "Proc. IAT, 478–485.",
      "citeRegEx" : "Yu et al\\.,? 2012",
      "shortCiteRegEx" : "Yu et al\\.",
      "year" : 2012
    }, {
      "title" : "Near-optimal active learning of multi-output Gaussian processes",
      "author" : [ "Y. Zhang", "T.N. Hoang", "K.H. Low", "M. Kankanhalli" ],
      "venue" : "Proc. AAAI, 2351–2357.",
      "citeRegEx" : "Zhang et al\\.,? 2016",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 24,
      "context" : "To lift this computational curse, a vast literature of sparse GP regression models (Quiñonero-Candela and Rasmussen 2005; Titsias 2009) have exploited a structural assumption of conditional independence based on the notion of inducing variables for achieving linear time in the data size.",
      "startOffset" : 83,
      "endOffset" : 135
    }, {
      "referenceID" : 26,
      "context" : "To lift this computational curse, a vast literature of sparse GP regression models (Quiñonero-Candela and Rasmussen 2005; Titsias 2009) have exploited a structural assumption of conditional independence based on the notion of inducing variables for achieving linear time in the data size.",
      "startOffset" : 83,
      "endOffset" : 135
    }, {
      "referenceID" : 2,
      "context" : "2016), traffic monitoring (Chen et al. 2012; Chen, Low, and Tan 2013; Chen et al. 2015; Hoang et al. 2014a; 2014b; Low et al. 2014a; 2014b; Ouyang et al. 2014; Xu et al. 2014; Yu et al. 2012)), (a) distributed (Chen et al.",
      "startOffset" : 26,
      "endOffset" : 191
    }, {
      "referenceID" : 4,
      "context" : "2016), traffic monitoring (Chen et al. 2012; Chen, Low, and Tan 2013; Chen et al. 2015; Hoang et al. 2014a; 2014b; Low et al. 2014a; 2014b; Ouyang et al. 2014; Xu et al. 2014; Yu et al. 2012)), (a) distributed (Chen et al.",
      "startOffset" : 26,
      "endOffset" : 191
    }, {
      "referenceID" : 9,
      "context" : "2016), traffic monitoring (Chen et al. 2012; Chen, Low, and Tan 2013; Chen et al. 2015; Hoang et al. 2014a; 2014b; Low et al. 2014a; 2014b; Ouyang et al. 2014; Xu et al. 2014; Yu et al. 2012)), (a) distributed (Chen et al.",
      "startOffset" : 26,
      "endOffset" : 191
    }, {
      "referenceID" : 16,
      "context" : "2016), traffic monitoring (Chen et al. 2012; Chen, Low, and Tan 2013; Chen et al. 2015; Hoang et al. 2014a; 2014b; Low et al. 2014a; 2014b; Ouyang et al. 2014; Xu et al. 2014; Yu et al. 2012)), (a) distributed (Chen et al.",
      "startOffset" : 26,
      "endOffset" : 191
    }, {
      "referenceID" : 22,
      "context" : "2016), traffic monitoring (Chen et al. 2012; Chen, Low, and Tan 2013; Chen et al. 2015; Hoang et al. 2014a; 2014b; Low et al. 2014a; 2014b; Ouyang et al. 2014; Xu et al. 2014; Yu et al. 2012)), (a) distributed (Chen et al.",
      "startOffset" : 26,
      "endOffset" : 191
    }, {
      "referenceID" : 27,
      "context" : "2016), traffic monitoring (Chen et al. 2012; Chen, Low, and Tan 2013; Chen et al. 2015; Hoang et al. 2014a; 2014b; Low et al. 2014a; 2014b; Ouyang et al. 2014; Xu et al. 2014; Yu et al. 2012)), (a) distributed (Chen et al.",
      "startOffset" : 26,
      "endOffset" : 191
    }, {
      "referenceID" : 28,
      "context" : "2016), traffic monitoring (Chen et al. 2012; Chen, Low, and Tan 2013; Chen et al. 2015; Hoang et al. 2014a; 2014b; Low et al. 2014a; 2014b; Ouyang et al. 2014; Xu et al. 2014; Yu et al. 2012)), (a) distributed (Chen et al.",
      "startOffset" : 26,
      "endOffset" : 191
    }, {
      "referenceID" : 3,
      "context" : "2012)), (a) distributed (Chen et al. 2013; Hoang, Hoang, and Low 2016; Low et al. 2015) and (b) stochastic (Hensman, Fusi, and Lawrence 2013; Hoang, Hoang, and Low 2015) implementations of such models have been developed to, respectively, (a) reduce their time to train with all the data by a factor close to the number of machines and (b) train with a small, randomly sampled subset of data in constant time per iteration of stochastic gradient ascent update and achieve asymptotic convergence to their predictive distributions.",
      "startOffset" : 24,
      "endOffset" : 87
    }, {
      "referenceID" : 18,
      "context" : "2012)), (a) distributed (Chen et al. 2013; Hoang, Hoang, and Low 2016; Low et al. 2015) and (b) stochastic (Hensman, Fusi, and Lawrence 2013; Hoang, Hoang, and Low 2015) implementations of such models have been developed to, respectively, (a) reduce their time to train with all the data by a factor close to the number of machines and (b) train with a small, randomly sampled subset of data in constant time per iteration of stochastic gradient ascent update and achieve asymptotic convergence to their predictive distributions.",
      "startOffset" : 24,
      "endOffset" : 87
    }, {
      "referenceID" : 7,
      "context" : "On the other hand, there is a less well-explored, alternative class of low-rank GP approximations that exploit sparsity in the spectral representation of a GP kernel (Gal and Turner 2015; Lázaro-Gredilla et al. 2010) for gaining time efficiency and have empirically demonstrated competitive predictive performance for datasets of up to tens of thousands in size but, surprisingly, not received as much attention and research effort.",
      "startOffset" : 166,
      "endOffset" : 216
    }, {
      "referenceID" : 13,
      "context" : "On the other hand, there is a less well-explored, alternative class of low-rank GP approximations that exploit sparsity in the spectral representation of a GP kernel (Gal and Turner 2015; Lázaro-Gredilla et al. 2010) for gaining time efficiency and have empirically demonstrated competitive predictive performance for datasets of up to tens of thousands in size but, surprisingly, not received as much attention and research effort.",
      "startOffset" : 166,
      "endOffset" : 216
    }, {
      "referenceID" : 7,
      "context" : "On the other hand, there is a less well-explored, alternative class of low-rank GP approximations that exploit sparsity in the spectral representation of a GP kernel (Gal and Turner 2015; Lázaro-Gredilla et al. 2010) for gaining time efficiency and have empirically demonstrated competitive predictive performance for datasets of up to tens of thousands in size but, surprisingly, not received as much attention and research effort. In contrast to the above literature, such sparse spectrum GP regression models do not need to introduce an additional set of inducing inputs which is computationally challenging to be jointly optimized, especially with a large number of them that is necessary for accurate predictions. Unfortunately, the sparse spectrum GP (SSGP) model of Lázaro-Gredilla et al. (2010) does not scale well to massive datasets due to its linear time in the data size and also finds a point estimate of the spectral frequencies of its kernel that risks overfitting.",
      "startOffset" : 167,
      "endOffset" : 803
    }, {
      "referenceID" : 7,
      "context" : "On the other hand, there is a less well-explored, alternative class of low-rank GP approximations that exploit sparsity in the spectral representation of a GP kernel (Gal and Turner 2015; Lázaro-Gredilla et al. 2010) for gaining time efficiency and have empirically demonstrated competitive predictive performance for datasets of up to tens of thousands in size but, surprisingly, not received as much attention and research effort. In contrast to the above literature, such sparse spectrum GP regression models do not need to introduce an additional set of inducing inputs which is computationally challenging to be jointly optimized, especially with a large number of them that is necessary for accurate predictions. Unfortunately, the sparse spectrum GP (SSGP) model of Lázaro-Gredilla et al. (2010) does not scale well to massive datasets due to its linear time in the data size and also finds a point estimate of the spectral frequencies of its kernel that risks overfitting. The recent variational SSGP (VSSGP) model of Gal and Turner (2015) has attempted to address both shortcomings of SSGP with a stochastic implementation and a Bayesian treatment of the spectral frequencies, respectively.",
      "startOffset" : 167,
      "endOffset" : 1048
    }, {
      "referenceID" : 13,
      "context" : "Such a kernel can be expressed as the Fourier transform of a density function p(r) over the domain of frequency vector r whose coefficients form a set of trigonometric basis functions (Lázaro-Gredilla et al. 2010): k(x,x′) = Er∼p(r)[ σ scos(2πr(x− x′)) ] (1) where p(r) , N ( 0, (4π2∆)−1 ) .",
      "startOffset" : 184,
      "endOffset" : 213
    }, {
      "referenceID" : 13,
      "context" : "Such a kernel can be expressed as the Fourier transform of a density function p(r) over the domain of frequency vector r whose coefficients form a set of trigonometric basis functions (Lázaro-Gredilla et al. 2010): k(x,x′) = Er∼p(r)[ σ scos(2πr(x− x′)) ] (1) where p(r) , N ( 0, (4π2∆)−1 ) . In the same spirit as that of Lázaro-Gredilla et al. (2010), we approximate the kernel in (1) by its unbiased estimator constructed from m i.",
      "startOffset" : 185,
      "endOffset" : 352
    }, {
      "referenceID" : 13,
      "context" : "This signifies a key difference between our generalized framework and the sparse spectrum GP (SSGP) model of Lázaro-Gredilla et al. (2010), the latter of which finds a point estimate of θ via maximum likelihood estimation that risks overfitting.",
      "startOffset" : 109,
      "endOffset" : 139
    }, {
      "referenceID" : 7,
      "context" : "Remark 1 The special case of γ = 1 recovers the degenerate test conditional p(fx∗ |α) = N (φθ (x∗)s, 0) induced by the variational SSGP (VSSGP) model of Gal and Turner (2015) (see equation 4 and Section 3 therein) which reveals that it imposes a highly restrictive deterministic relationship between fx∗ and α and also fails to exploit the local data (Xk,yk) (i.",
      "startOffset" : 153,
      "endOffset" : 175
    }, {
      "referenceID" : 7,
      "context" : "Remark 1 The special case of γ = 1 recovers the degenerate test conditional p(fx∗ |α) = N (φθ (x∗)s, 0) induced by the variational SSGP (VSSGP) model of Gal and Turner (2015) (see equation 4 and Section 3 therein) which reveals that it imposes a highly restrictive deterministic relationship between fx∗ and α and also fails to exploit the local data (Xk,yk) (i.e., due to conditional independence between fx∗ and yk given α) that can potentially improve the predictive performance. Unfortunately, VSSGP cannot be trivially extended to span the entire spectrum since it relies on the induced deterministic relationship between fx∗ andα to analytically derive its predictive distribution, which does not hold for γ 6= 1. On the other hand, when γ = 0, the test conditional in Proposition 1 becomes independent of the nuisance variables s and reduces to the predictive distribution p(fx∗ |yk,θ) of the SSGP model of Lázaro-Gredilla et al. (2010) (see equation 7 therein) given its point estimate of the spectral frequencies θ but restricted to the local data (Xk,yk) corresponding to input subspaceXk.",
      "startOffset" : 153,
      "endOffset" : 944
    }, {
      "referenceID" : 7,
      "context" : "In contrast, the VSSGP model of Gal and Turner (2015) assumes r1, .",
      "startOffset" : 32,
      "endOffset" : 54
    }, {
      "referenceID" : 7,
      "context" : "In contrast, the VSSGP model of Gal and Turner (2015) assumes r1, . . . , rm, and s to be statistically independent a posteriori in its variational distribution (see section 4 therein). Relaxing this assumption will cause VSSGP to lose its scalability as its induced variational lower bound will inevitably become intractable. Remark 3 Though the model of Titsias and LázaroGredilla (2014) has adopted a similar parameterization but only for the original GP hyperparameters, it incurs cubic time in the data size per iteration of gradient ascent update, as shown in its supplementary materials and experiments.",
      "startOffset" : 32,
      "endOffset" : 390
    }, {
      "referenceID" : 3,
      "context" : "This section empirically evaluates the predictive performance and time efficiency of our sVBSSGP model on three real-world datasets: (a) The AIMPEAK dataset (Chen et al. 2013) consists of 41800 traffic speed observations (km/h) along 775 urban road segments during the morning peak hours on April 20, 2011.",
      "startOffset" : 157,
      "endOffset" : 175
    }, {
      "referenceID" : 0,
      "context" : "Each record features a 8-dimensional input vector of the aircraft’s age (year), travel distance (km), the flight’s total airtime, departure time, arrival time (min), and the date given by day of week, day of month, and month, and a corresponding output of the flight’s delay time (min); and (c) the BLOG feedback dataset (Buza 2014) contains 60000 instances of blog posts.",
      "startOffset" : 321,
      "endOffset" : 332
    }, {
      "referenceID" : 7,
      "context" : "The performance of sVBSSGP is compared against the state-of-the-art VSSGP (Gal and Turner 2015) and stochastic implementations of sparse GP models based on inducing variables such as DTC+ and PIC+ (Hensman, Fusi, and Lawrence 2013; Hoang, Hoang, and Low 2015) (i.",
      "startOffset" : 74,
      "endOffset" : 95
    }, {
      "referenceID" : 13,
      "context" : "49 Table 1: RMSEs achieved by sVBSSGP, DTC+, PIC+, SSGP (Lázaro-Gredilla et al. 2010), and VSSGP after final convergence for AIMPEAK and AIRLINE datasets.",
      "startOffset" : 56,
      "endOffset" : 85
    }, {
      "referenceID" : 13,
      "context" : "This paper describes a novel generalized framework of sVBSSGP regression models that addresses the shortcomings of existing sparse spectrum GP models like SSGP (Lázaro-Gredilla et al. 2010) and VSSGP (Gal and Turner 2015) by adopting a Bayesian treatment of the spectral frequencies to avoid overfitting, modeling the spectral frequencies jointly in its variational distribution to enable their interaction a posteriori, and exploiting local data for improving the predictive performance while still being able to preserve its scalability to big data through stochastic optimization.",
      "startOffset" : 160,
      "endOffset" : 189
    }, {
      "referenceID" : 7,
      "context" : "2010) and VSSGP (Gal and Turner 2015) by adopting a Bayesian treatment of the spectral frequencies to avoid overfitting, modeling the spectral frequencies jointly in its variational distribution to enable their interaction a posteriori, and exploiting local data for improving the predictive performance while still being able to preserve its scalability to big data through stochastic optimization.",
      "startOffset" : 16,
      "endOffset" : 37
    } ],
    "year" : 2016,
    "abstractText" : "While much research effort has been dedicated to scaling up sparse Gaussian process (GP) models based on inducing variables for big data, little attention is afforded to the other less explored class of low-rank GP approximations that exploit the sparse spectral representation of a GP kernel. This paper presents such an effort to advance the state of the art of sparse spectrum GP models to achieve competitive predictive performance for massive datasets. Our generalized framework of stochastic variational Bayesian sparse spectrum GP (sVBSSGP) models addresses their shortcomings by adopting a Bayesian treatment of the spectral frequencies to avoid overfitting, modeling these frequencies jointly in its variational distribution to enable their interaction a posteriori, and exploiting local data for boosting the predictive performance. However, such structural improvements result in a variational lower bound that is intractable to be optimized. To resolve this, we exploit a variational parameterization trick to make it amenable to stochastic optimization. Interestingly, the resulting stochastic gradient has a linearly decomposable structure that can be exploited to refine our stochastic optimization method to incur constant time per iteration while preserving its property of being an unbiased estimator of the exact gradient of the variational lower bound. Empirical evaluation on real-world datasets shows that sVBSSGP outperforms stateof-the-art stochastic implementations of sparse GP models.",
    "creator" : "TeX"
  }
}