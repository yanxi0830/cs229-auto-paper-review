{
  "name" : "1704.03564.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Active classification with comparison queries",
    "authors" : [ "Daniel M. Kane", "Shachar Lovett", "Shay Moran", "Jiapeng Zhang" ],
    "emails" : [ "dakane@ucsd.edu", "slovett@cs.ucsd.edu.", "shaymoran1@gmail.com.", "jpeng.zhang@gmail.com." ],
    "sections" : [ {
      "heading" : null,
      "text" : "We focus on the class of half spaces, and show that under natural assumptions, such as large margin or bounded bit-description of the input examples, it is possible to reveal all the labels of a sample of size n using approximately O(log n) queries. This implies an exponential improvement over classical active learning, where only label queries are allowed. We complement these results by showing that if any of these assumptions is removed then, in the worst case, Ω(n) queries are required.\nOur results follow from a new general framework of active learning with additional queries. We identify a combinatorial dimension, called the inference dimension, that captures the query complexity when each additional query is determined by O(1) examples (such as comparison queries, each of which is determined by the two compared examples). Our results for half spaces follow by bounding the inference dimension in the cases discussed above.\n∗Department of Computer Science and Engineering/Department of Mathematics, University of California, San Diego. dakane@ucsd.edu Supported by NSF Career Award ID 1553288. †Department of Computer Science and Engineering, University of California, San Diego. slovett@cs.ucsd.edu. Research supported by NSF CAREER award 1350481. ‡Department of Computer Science and Engineering, University of California, San Diego, Simons Institute for the Theory of Computing, Berkeley, and Max Planck Institute for Informatics, Saarbrücken, Germany. shaymoran1@gmail.com. §Department of Computer Science and Engineering, University of California, San Diego. jpeng.zhang@gmail.com. Research supported by NSF CAREER award 1350481.\nar X\niv :1\n70 4.\n03 56\n4v 2\n[ cs\n.L G\n] 2\nJ un\n2 01\n7"
    }, {
      "heading" : "1 Introduction",
      "text" : "A central goal of interactive learning is understanding what type of interaction between a learner and a domain expert enhances the learning process, compared to the classical passive learning from labeled examples.\nA basic model that was studied in this context is pool-based active learning (McCallum and Nigam, 1998). Here, the algorithm has an access to a large pool of unlabeled examples from which it can pick examples and query their labels. The goal is to make as few queries as possible while achieving generalization-guarantees which are comparable with these of a passive algorithm with an access to all of the labels.\nA canonical example that demonstrates an advantage of active learning is the class of threshold functions1 over the real line. Indeed, let c denote the learned threshold function, and let x1 < x2 < . . . < xn in R be the given pool of unlabeled examples. It is possible to infer the labels of all n points by making at most log n+2 queries: query the labels of the extreme points c(x1), c(xn); if c(x1) = c(xn) then the remaining points must be labeled the same; otherwise, continue in a binary search fashion, by labeling the middle point of the interval whose extreme points have opposite labels. After at most log n such queries, the labels of all n points are revealed.\nUnfortunately, this exponential improvement in the query complexity breaks for more general concept classes. In fact, even for the class of 2 dimensional threshold functions2, namely the class of half-planes, the (worst-case) query complexity of active learning equals that of passive learning (see e.g. Dasgupta (2004)). Consequently, much of the literature was dedicated to developing theory that takes into consideration further properties of the unknown underlying distribution or the target concept (Balcan et al., 2007; Hanneke, 2007; Dasgupta et al., 2008; Balcan et al., 2009; Beygelzimer et al., 2010; Balcan et al., 2010; Koltchinskii, 2010; Balcan and Hanneke, 2011; Hanneke and Yang, 2012; El-Yaniv and Wiener, 2012; Balcan and Long, 2013; Gonen et al., 2013; Urner et al., 2013; Zhang and Chaudhuri, 2014; Wiener et al., 2015; Berlind and Urner, 2015).\nWe consider another approach by allowing the learning algorithm to further interact with the domain expert by asking additional queries. This poses a question:\nWhich additional queries can the algorithm use?\nAllowing arbitrary queries will result in a very strong learner (indeed, by halving the set of potential hypothesis in every query, the number of queries can be made logarithmic). However, arbitrary queries are useless in practice: as experimental work by Lang and Baum (1992) shows, algorithms that use set of queries that are too rich may result in a poor practical performance. This is not surprising if we keep in mind that the human annotator who answers the queries is restricted (computationally and in other ways). Thus, a crucial factor in choosing the additional queries is compatibility with the annotator who answers them. A popular type of queries, which is used in applications involving human annotators, is relative queries. These queries poll relative information between two or more data points.\nThis work focuses on a basic kind of relative queries — comparison queries. Using such queries is sensible in settings in which there is some natural ordering of the instances with respect to the learned concept. As a toy example, consider the goal of classifying films according to whether a certain individual is likely to enjoy them or not (e.g. for recommending new films for this person). In this context, the input sample consists of films watched by the individual, a label query asks whether the person liked a film, and a comparison query asks which of two given films did the individual prefer. As we will see, comparison queries may significantly help.\n1 These are “R→ {±1}” functions of the form c(x) = sign(a · x− b), where a, b ∈ R. 2 These are “R2 → {±1}” functions of the form c(x) = sign ( 〈a, x〉 − b ) , where a ∈ R2, b ∈ R.\nAnother aspect implied by the restricted nature of the human annotator is that the learned concept resides in the class of concepts that can be computed by the annotator, which presumably has low capacity. Thus, realizability assumptions about the generating data distributions are plausible in this context."
    }, {
      "heading" : "1.1 Active learning with additional comparison queries",
      "text" : "Consider a learned concept of the form c(x) = sign ( f(x) ) , where f is a real valued function (e.g. half spaces, neural nets), and consider two instances x1, x2 such that, say f(x1) = 10, and f(x2) = 1000. Both c(x1) and c(x2) equal +1, however that f(x2) >> f(x1) suggests that x2 is a “more positive instance” than x2. In the setting of film classification this is naturally interpreted as that the person likes the film x2 more than the film x1. We call the query “f(x2) ≥ f(x1)?” a comparison query."
    }, {
      "heading" : "1.1.1 Example: learning half-planes with comparison and label queries",
      "text" : "To be concrete, consider the class of half-planes in R2. Here, a comparison query is equivalent to asking which of two sample points x1, x2 lies closer to the boundary line. Do such queries improve the query complexity over standard active learning? It is known that without such queries, in the worst-case, the learner essentially has to query all labels (Dasgupta, 2004).\nThe following algorithm demonstrates an exponential improvement when comparison queries are allowed. Later, we will present more general results showing that this can be generalized to higher dimensions under some natural restrictions, and that such restrictions are indeed necessary.\nInteractive learning algorithm with comparison queries for half planes in R2 (see Figure 1 for a graphical illustration)\nSetting: an unlabeled input sample of n points x1, . . . , xn ∈ R2 with hidden labels according to a half-plane c(x) = sign ( f(x) ) , where f : R2 → R is affine.\nRepeat until all points are labeled:\n1. Sample uniformly a subsample S of 30 points.\n2. Query the labels of the points in S, and denote by: P — points in S labeled by +1, N — points in S labeled by −1.\n3. Use comparison queries to find: (i) q – the closest point in P to the boundary line, (ii) v – the closest point in N to the boundary line.\n4. Denote by: [p, q], [q, r] — the two edges of the convex hull of P that are incident to q, [u, v], [v, w] — the two edges of the convex hull of N that are incident to v.\n5. Infer that: all points inside the cone ∠pqr are labeled +1, all points inside the cone ∠uvw are labeled −1.\n6. Repeat on the remaining unlabeled points.\nThe algorithm proceeds by iterations: it repeats steps 1-5 until all points are labeled. At each iteration at most 60 queries are performed: 30 label queries in step 2 and at most 30 comparison queries in step 3, for finding the points q, v of minimal distance. In each iteration, the algorithm infers the labels of all points in the union of the angles ∠pqr,∠uvw. We will refer to this region as “the confident region”.\nWe claim that after an expected number of O(log n) iterations (and therefore using only O(log n) queries) the algorithm infers the labels of all input points. Establishing this statement boils down, via a boosting argument (Theorem 3.2) to the following two important properties, which are easy to verify:\n• Confidence: every point which is labeled by the algorithm is labeled correctly.\n• Inference from a small subsample: the confident region is determined by a subsample of 6 labeled points of S (i.e. p, q, r and u, v, w).\nThese properties imply that at each iteration, with probability at least 1/2, half of the remaining unlabeled points are labeled correctly (see Lemma 3.3). Thus, the expected number of queries is O(log n).\nPaper organization. In Section 1.2 we present and discuss our results, later, in Section 1.3 we survey previous related works, and in Section 1.4 we ceil the introduction with suggested directions of future research and open problems. Sections 2, 3, and 4 contain the technical definitions and proofs."
    }, {
      "heading" : "1.2 Results",
      "text" : "We next present and discuss our main results.\n1. Subsection 1.2.1 is dedicated to our results concerning half spaces,\n2. Subsection 1.2.2 is dedicated to the inference dimension, and how it captures the query complexity of active learning with additional comparison queries, and\n3. Subsection 1.2.3 focuses on the general framework of active learning with additional queries."
    }, {
      "heading" : "1.2.1 Interactive learning of half spaces with comparison queries",
      "text" : "We start by discussing our results for interactive learning of half spaces in Rd when both label queries and comparison queries are allowed. We show that a general algorithm, as the one we described for R2, cannot exist for d ≥ 3. However, we identify two useful properties that allow for such a learning algorithm: bounded bit complexity and margin.\nExact recovery of labels. We first describe our results in the context of exact recovery. Here, the labels of all n sample points needs to be revealed, using as few queries as possible.\nWe first show that in the worst case, in Rd this requires Ω(n) queries for any d ≥ 3 (recall that O(log n) queries suffice in R2).\nTheorem 1.1 (Theorem 4.11, informal version). There are n points in R3 that require Ω(n) label and comparison queries for revealing all labels.\nOur first positive result shows that efficient exact recovery of labels is possible if the points have low bit complexity.\nTheorem 1.2 (Theorem 4.1, informal version). Consider an arbitrary realizable sample of n points in Rd whose individual bit complexity is B. The labels of all sample points can be learned using Õ(B log n) label and comparison queries.\nAs an example, consider the sample consisting of the 2N point in the boolean hypercube {0, 1}N . The bit complexity of every point is N . The above thoerem implies that given an unknown threshold function on {0, 1}N , it is possible to reveal all 2N labels using Õ(N2) comparison and label queries. This should be compared to the situation where only label queries are allowed, where all 2N queries are necessary. We discuss this example in more detail in Section 1.4.4, and leave as an open question whether revealing the 2N labels can be done efficiently (the above Õ(N2) bound applies only to the query complexity, and not to the total running time).\nOur second positive result shows that a similar algorithm exists if the margin is large.\nTheorem 1.3 (Theorem 4.7, informal version). Assume a sample of n points in Rd with maximal `2 norm ρ and margin at least γ. The labels of all points can be recovered using Õ ( d log(ρ/γ) log n ) label and comparison queries.\nOur bound is in fact stronger: it is Õ ( d log(1/η) log n ) ,\nwhere η is the minimal-ratio of the input sample, defined by |mini f(xi)||maxi f(xi)| , where x1, . . . , xn are the sample points and sign ( f(x) ) is the learned concept. In Section 4.1.2 we show that the maximal ratio is lower bounded by the margin (and therefore yields a stronger statement). Note that the above bound depends on ρ/γ logarithmically, and therefore applies even in settings when the margin is exponentially small. Similar dependence of on ρ/γ is obtained by the Ellipsoid method (Karmarkar, 1984), and Vaidya Cutting Plane method (Vaidya, 1996),\nthat use O(d2 log(ρ/γ)) and O(d log(dρ/γ)) iterations respectively, when used to find a linear classifier that is consistent with a realizable sample with margin ρ/γ in Rd.\nThe upper bound in the above theorem depends on the dimension, which is often avoided in bounds that depend on the margin. It is therefore natural to ask whether there is a bound that depends only on the margin. We show that it is impossible:\nTheorem 1.4 (Theorem 4.17, informal version). There is a sample of n unit vectors in Rn+1 with Ω(1) margin that require Ω(n) label and comparison queries to recover all the labels.\nStatistical learning. Using standard arguments, we translate the results above to the statistical setting and get bounds on the sample and query complexity: the algorithmic results above directly give a learning algorithm for realizable distributions with sample complexity\nn( , δ) = O ( d+log(1/δ) ) and query complexity approximately logarithmic in n( , δ), where is\nthe error and 1− δ is the confidence of the algorithm. (See Section 4.1.1 for the bit complexity and Section 4.1.2 for the margin.) Our lower bounds translate analogously to realizable distributions that require Ω(1/ ) queries for achieving error and constant confidence (say 1− δ = 5/6). See Section 4.2 for details."
    }, {
      "heading" : "1.2.2 Inference dimension",
      "text" : "Our results for learning half spaces rely on a common combinatorial property that we describe next. Let X be a set and H a concept class where each concept is of the form sign(f(x)) for f : X → R. For example, X may be a finite set X = {x1, . . . , xn} ⊆ Rd and H the class of all half spaces with margin at least 1/100 with respect to X; or X = {0, 1}d and H is all half spaces; or X = Rd and H a class of (signs of) low degree polynomials; etcetera.\nLet S ⊆ X be an unlabeled sample. An S−query is either a label query regarding some x ∈ S, or a comparison query regarding x1, x2 ∈ S. Namely, the allowed queries are “f(x) ≥ 0?” (label query) or “f(x1) ≥ f(x2)” (comparison query). For x ∈ X and c ∈ H, let\nS =⇒ f x\ndenote the statement that the comparison and label queries on S determine the label of x, when the learned concept is c = sign ( f(x) ) .\nThe inference-dimension of (X,H) is the smallest number k such that for every c = sign(f(x)) ∈ H, and every S ⊂ X of size at least k, there exists x ∈ S such that\nS \\ {x} =⇒ f x.\nIn other words, if the inference dimension is k then in every sample of size k or more, there is a point whose label can be inferred from the label and comparison queries on the remaining points.\nFor example the inference dimension of (X,H), where X = R, and H is the class of threshold functions is 3: indeed, to see it is at most 3, note that if all 3 points have the same label, then the label of the midpoint can be inferred from the other two labels, and if not all points have the same label then the midpoint and, say the right point have the same label, and so the label of the right point can be inferred from the other 2 labels. In this example comparison queries are not required. Another example, which requires comparison queries, is where X = R2 and H is the class of half-planes. Here the inference dimension3 is at most 7: indeed, in any sample of at least 7 points there are 4 points with the same label, and the label of one of these points can be inferred from the other 3 (see Figure 1 and Section 1.1.1).\n3One can show the inference dimension here is 5.\nThe next Theorem shows that inference dimension captures the query-complexity in active learning with comparison queries. It is worth noting that in the classical setting of active learning, when only label queries are allowed, the inference dimension specializes to the the star dimension (Hanneke and Yang, 2015), which similarly captures the (worst-case) query complexity in this setting.\nTheorem 1.5. Let k denote the inference dimension of (X,H). Then:\n1. There is an algorithm that reveals the labels of any realizable sample of size n using at most O(k log k log n) queries.\n2. Any algorithm that reveals the labels of any realizable sample of size k must use Ω(k) queries in the worst-case.\nThe upper bound (the first item) is a corollary of Theorem 3.2, and the lower bound is a corollary of Theorem 3.5. Both Theorems are discussed in the next subsection. While the lower bound is relatively straight forward, our derivation of the upper bound requires several steps, which we summarize next.\n(i) Low inference dimension =⇒ weak confident learner: we first show that if the inference dimension is at most k then there is a weak confident learner for (X,H) with query complexity O(k log k) (see Lemma 3.3). A confident learner is a learning algorithm that may abstain from predicting on some points x ∈ X, but must be correct on every point where it does not abstain. A weak confident learner is a confident learner that with constant probability does not abstain on a constant fraction of X (see Section 3.1 for a formal definition).\n(ii) Boosting the weak confident learner: once a weak confident learner is derived, we transform it into the desired learning algorithm using a simple boosting argument.\nWhile the boosting part is rather standard, showing that low inference dimension implies a weak confident learner relies on a symmetrization argument. This symmetrization argument can be replaced by a more standard sample compression argument, however this would result in a suboptimal query complexity bound.\nWe get the following corollary of Theorem 1.5 in the statistical setting:\nCorollary 1.6. Let k denote the inference dimension of (X,H).\n1. Let n( , δ) denote the passive sample complexity of learning (X,H) with error and confidence 1− δ. There is an algorithm that learns (X,H) with sample complexity n( , δ) and query complexity O ( k log k log(n( , δ)) ) .\n2. Any algorithm that learns (X,H) with error at most = 1/k and confidence at least 5/6 must use at least Ω ( 1/ ) queries for some realizable distribution.\nThis Corollary follows from Corrolaries 1.8 and 3.6.\nA dichotomy. The passive sample complexity of (X,H) is n( , δ) = Θ(d+log( 1/δ)\n), where d is the VC-dimension of H (Hanneke, 2015). Thus the above corollary implies a dichotomy: if the inference dimension is finite then the query complexity is logarithmic in 1/ , and if it is infinite then the query complexity is Ω(1/ ).\nThe results presented in the previous section regarding learning half spaces with comparison and label queries are derived by analyzing the inference-dimension of the relevant classes, which we sketch next.\nSketch of upper bounding the inference dimension of half spaces. Our upper bounds in terms of margin and bit-complexity follow a similar outline. We next give a rough sketch of the arguments in order to convey their flavor.\nConsider the case where every instance x ∈ X has bounded bit-complexity, say X ⊂ [N ]d for some bounded N ∈ N. We wish to show that for a sufficiently large Y ⊆ X, and every half space c = sign(f) there is some x ∈ Y such that\nY \\ {x} =⇒ f x.\nBy removing at most half of the elements of Y , we may assume that c is constant on Y , without loss of generality assume that c(x) = +1, for every x ∈ Y . Let x1, x2, . . . be an ordering of the elements of Y such that f(x1) ≤ f(x2) ≤ . . . The first observation is that it suffices to show that there exists i0 such that xi0 − x1 is a nonnegative linear combination of the xi − xj , where i > j (see Claim 4.5).\nThe existence of such an i0 is achieved by a pigeon hole argument showing that if Y is sufficiently large then there are two distinct linear combinations of the xi+1 − xi’s with coefficients from {0, 1} that yield the same vector:\nk/2∑ i=1 βi(xi+1 − xi) = k/2∑ i=1 γi(xi+1 − xi),\nThen a short calculation yields that the maximal j such that βj 6= γj serves as the desired i0. The upper bound in terms of margin is technically more involved. Instead of nonnegative linear combinations, one can consider linear combinations such that every coefficient is at least −γ, where γ is the margin, and the pigeon hole argument is replaced by a volume argument.\nSketch of lower bounding the inference dimension of half spaces. Our lower bounds are based on embedding the class { ∅, {i} : 1 ≤ i ≤ n } as half spaces in a way that the n + 1 half spaces induce the same ordering on the n points in terms of distance from the boundary. Comparison queries are useless for such an embedding, which implies that the inference dimension is at least n, Indeed, consider the half space c∅ that corresponds to the empty-set; then for any subset Y of at most n− 1 points c∅ is indistinguishable from c{i}, the half space corresponding to {i}, where i /∈ Y ."
    }, {
      "heading" : "1.2.3 General framework",
      "text" : "We next describe how the notion of inference-dimension, as well as Theorem 1.5 extend to settings where the additional queries are not necessarily comparison queries.\nConsider an interactive model, where the learning algorithm is allowed to use additional queries from a prescribed set of queries Q. More formally, let S be the unlabeled input sample. In pool-based active learning, the algorithm may query the label of any point in S. Here, the algorithm is allowed to use additional queries from a set Q = Q(S) (we stress that the allowable queries depend on the input sample). For example, in the setting discussed in the previous section, Q(S) contains all comparison queries among points in S. Another example, which is used by crowd-sourcing algorithms (Tamuz et al., 2011) involve 3-wise queries of the form “Is x2 more similar to x1 than to x3?”.\nUpper bound. The next “boosting result” generalizes the upper bound from Theorem 1.5 and shows that if there are not too many queries inQ(S), or alternatively if there is an algorithm that infers the answers to the queries in Q(S) using a few queries, then it is possible to reveal all labels using a logarithmic number of queries:\nTheorem 1.7 (Restatement of Theorem 3.2). Let k denote the inference dimension of (X,H). Assume that there is an algorithm that, given a realizable sample S of size n as input, uses at most q(n) queries and infers the answers to all queries in Q(S) and all labels in S. Then there is a randomized algorithm that infers all the labels in S using at most\n2q(4k) log n\nqueries in expectation.\nFor example, in the setting of comparison queries, q(n) = n+n log n queries suffice to infer all comparison queries and label queries in Q(S), and thus, the upper bound in Theorem 1.5 follows from the above theorem.\nAn interactive algorithm that infers all labels using a few queries can be combined with any passive learner by first using the interactive learner to reveal all labels of the input sample, and then apply the passive learner on the labeled sample. Thus, we get:\nCorollary 1.8. Let (X,H) be as in Theorem 3.2, and let n( , δ) denote the (passive) sample complexity of learning (X,H) with error and confidence 1−δ. Then there exists an algorithm that learns H with sample complexity n( , δ), and query complexity O(q(4k) log n( , δ)).\nLower bound. The next lower bound on the query complexity generalizes the one from Theorem 1.5. It demonstrates a property of comparison and label queries which suffices for the lower bound in Theorem 1.5 to hold. Call an additional query t-local, if its answer is determined by f(x1), . . . , f(xt) for some x1, . . . , xt ∈ X. For example, every label-query is 1-local and every comparison query is 2-local. The following Theorem extends Theorem 1.5 to this setting:\nTheorem 1.9 (Restatement of Theorem 3.5). Assume that the inference dimension of (X,H) is > k. Assume that every additional query is t-local, and that for every sample S the set of allowable queries Q(S) is the set of all queries that are determined by subsets (of size at most t) of S. Then any algorithm that reveals the labels of any realizable sample of size k must use Ω(k/t) queries in the worst-case.\nIn the statistical setting, we get the following Corollary:\nCorollary 1.10 (Restatement of Corollary 3.6). Set = 1k , δ = 1 6 . Then any algorithm that learns (X,H) with error and confidence 1− δ must use Ω(1/t ) queries."
    }, {
      "heading" : "1.3 Related work",
      "text" : "Studying statistical learning where the learner has access to additional queries was considered by various works. A partial list includes: Angluin (1987); Baum (1991); Lang and Baum (1992); Turán (1993); Jackson (1997); Kwek and Pitt (1998); Blum et al. (1998); Bshouty et al. (2004); Feldman (2009a,b); Nowak (2011); Chen et al. (2016). Many of these focus on the case where the additional queries are membership queries. We discuss some of these results in Section 1.4.1.\nIn the context of active learning, which is considered in this paper, Balcan and Hanneke (2011) considered additional queries of two types: Class conditional queries and Mistake queries, in the first type the learner provides the annotator with a list of examples, and a label and asks her to point out an example in this list with the given label. In the second type, the learner gives the oracle a list of examples with proposed labels and she replies whether it is correct or points out a mistake. Note that these queries may have more than two answers, which is different than binary queries, which are considered in this paper.\nWauthier et al. (2012) give an active learning algorithm for clustering using pairwise similarity queries, and Ashtiani et al. (2016) gives a clustering algorithm that uses queries that ask whether two elements belong to the same cluster. Vikram and Dasgupta (2016) consider\nclustering in an interactive setting where the algorithm may present the annotator a clustering of a O(1) size subset of the domain, and the annotator replies whether the target-clustering agrees with this partition, and points out a difference in case it does not.\nJamieson and Nowak (2011) give an active ranking algorithm from pairwise comparisons. They consider a setting that bears resemblance with ours: their goal is to find the ranking among a sample of points in Rd, where the ranking is determined according to the euclidean distance from a fixed, unknown reference point. They present an algorithm that use an expected number of O(d log n) comparisons when the ranking is chosen uniformly at random.\nRecently, Xu et al. (2017) considered a similar setting to ours. They also study active classification with additional comparison queries, but they focus on minimizing the total number of label queries, while the number of comparison queries can be large (more than 1/ , where is the error). In contrast, we study when it is possible to achieve total number of queries which is logarithmic in 1/ ."
    }, {
      "heading" : "1.4 Discussion and suggested future research",
      "text" : "We next offer few potential directions for future research."
    }, {
      "heading" : "1.4.1 Other types of additional queries",
      "text" : "Relative queries It is natural to study other types of additional queries, and when do they enhance the learning process. Comparison queries belong to a type of queries, called relative queries, which is popular in many applications involving human annotators. A popular example of relative queries, which are used in practice, take 3 data points x1, x2, x3 and queries “is x2 more similar to x1 than to x3?”. Such queries were studied in various works, including Xing et al. (2002); Schultz and Joachims (2003); Agarwal et al. (2007); McFee and Lanckriet (2010); Huang et al. (2011); Tamuz et al. (2011); Qian et al. (2013).\nMembership queries from generative models A natural and well-studied type of queries is membership queries; thus, the learner is allowed to query the labels of any point in the domain, including points outside the input sample. Using membership queries to enhance learning was considered in the PAC model (Baum, 1991; Turán, 1993; Jackson, 1997; Kwek and Pitt, 1998; Bshouty et al., 2004; Feldman, 2009a,b; Chen et al., 2016). However, as an experimental work by Lang and Baum (1992) suggested, querying the labels of arbitrary points may result in a poor practical performance. A possible explanation is that some domain points represent noise; for example, if the data distribution is supported on some low dimensional manifold, then querying outside it may make no sense to the human annotator. Consequently, various restrictions on this model were studied (Blum et al., 1998; Sloan and Turán, 1997; Awasthi et al., 2013; Bary-Weisberg et al., 2016). One option, studied by Awasthi et al. (2013); BaryWeisberg et al. (2016)) is to restrict the type of membership queries by considering only local queries, within a small neighborhood of the input sample. Another possible restriction is to consider only membership queries of points that are sums and differences of a few sample points (e.g. a comparison query is a membership query on the difference of the two compared points in the case of half spaces).\nAn alternative direction for dealing with querying noise is to use a generative model for constructing data outside of the input sample: imagine we have an access to a generative model (e.g. Generative Adversarial Networks (Goodfellow et al., 2014)) g that gets as an input any point s ∈ [0, 1]d, and outputs a (non-noisy) unlabeled example g(s) ∈ X. This gives an access to a large pool membership queries the learning algorithm can use.\nFrom a theoretical perspective, this motivates the study of active learning with additional membership queries, where the learned concept is some c : [0, 1]d → {±1} of low complexity (say c is the sign of a low degree polynomial or a small network, etc.). It is worth noting\nthat the case where c is a half space was studied in the discrete geometry community in the context of linear decision trees (Meyer auf der Heide, 1984; Meiser, 1993; Liu, 2004; Cardinal et al., 2016; Ezra and Sharir, 2016). The currently best known upper bound are due to Ezra and Sharir (2016), who give an algorithm that reveals all labels of a realizable sample of size n using Õ(d2 log d log n) membership queries (the best known lower bound is the information theoretical Ω(d log n), which applies more generally for any type of additional yes/no queries)."
    }, {
      "heading" : "1.4.2 Noisy comparisons",
      "text" : "It is natural to extend our algorithms by making them robust in the presence of noisy comparisons. Imagine, for example that f(x1) = 1.001, and f(x2) = 1.0001. Our algorithms are based on the assumption that the answer to the query “f(x1) ≥ f(x2)?” will always be “yes”. It is plausible to consider a noise model that may give the wrong answer in such cases where f(x1) ≈ f(x2). One such model is suggested by Bradley and Terry (1952). According to this model, when4 f(x1), f(x2) > 0 then the answer to the query “f(x1) ≥ f(x2)?”, is “yes” with probability f(x1)f(x1)+f(x2) ."
    }, {
      "heading" : "1.4.3 Streaming-based interactive learning with comparison queries",
      "text" : "This paper focused on the pool-based model. Another natural and well studied model is the streaming-based model. Here, the algorithm gets to observe the unlabeled examples one-byone in an online fashion, and has to decide whether to query the current example. Consider a setting in which the algorithm has a bounded memory in which it may store M past examples. Upon receiving a new example it may query its label and/or compare it with any of the saved examples. In the realizable setting, a possible goal is to minimize the number of queries and the memory size while maintaining a (partial) hypothesis that is correct on all past examples.\nConsider for example the class of threshold function over R. Given an infinite sequence of independent unlabeled examples x1, x2, . . . drawn from an unknown distribution over R. A simple online algorithm with memory M = 2 saves the 2 examples x, y where the sign-change occurs. Upon receiving a new example xn, it checks whether xn is between x and y; if it is then it queries its label and replaces it with x or y (according to the label of xn), otherwise the label of xn can be inferred and the algorithm moves to the next example. One can show that this algorithm makes an expected number of log n queries after observing the first n examples. One can show that a slightly more complicated algorithm, which uses comparison queries and memory M ≤ 10, has a similar behaviour for half-planes in R2. We leave as an open question whether similar algorithms exist for arbitrarty classes (X,H) with low inference dimension."
    }, {
      "heading" : "1.4.4 Exactly learning threshold functions",
      "text" : "Consider the class of thresholds functions over the N -dimensional hypercube {0, 1}N . By Lemma 4.2, the inference dimension of this class is O(N logN). Therefore, Theorem 4.1 implies that the 2n labels of any threshold function can be revealed using at most O(N2 log2N) queries in expectation. This demonstrates the additional power of comparison queries over label queries: indeed, standard arguments show that if only label queries are allowed then at least 2Ω(N) queries are needed.\nWhile just O(N2 log2N) queries suffice to reveal all 2N labels, it remains open whether this can be done efficiently, in a poly(N) time. A naive implementation of the algorithm from Theorem 4.1 would check at each step for each point in {0, 1}N whose label is still no known, whether its be inferred by the queries performed in this step. This takes at least 2N such checks. We leave the question, of efficiently learning all 2N labels of an unknown threshold function as an open question for future work.\n4if f(x1), f(x2) < 0 then it is “yes” with probability f(x2)\nf(x1)+f(x2) .\nAnother comment that seems in place is that the O(N2 log2N) upper bound is tight up to the log2N factor: indeed, it is known that there are some 2Θ(N\n2) threshold functions (Goto and Takahasi, 1962), and therefore an information theoretic lower bound of Ω(N2) queries holds even if the algorithm is allowed to use arbitrary yes/no queries. While it remains open whether the information theoretic barrier can be achieved using comparison queries, it is possible shave one log factor from the upper bound and achieve O(N2 logN) queries in expectation, as we sketch next. This follows since: (i) the number of linear orders over a set {x1, . . . .xm} ⊆ RN that are induced by a linear function is at most some5 mO(2N), and therefore (ii) Fredman’s algorithm (Fredman, 1976) finds such an ordering using q(m) = O(m+N logm) comparisons (which is better than the standard O(m logm) when m >> N). Thus, in Theorem 3.2 we can plug q(4k) = O(k +N log k), instead of q(4k) = O(k log k) (recall that k = O(N logN), which yields O(N2 logN) queries in expectation."
    }, {
      "heading" : "2 Preliminaries",
      "text" : "Basic definitions. A hypothesis class is a pair (X,H), where X is a set, and H is a class of functions h : X → {±1}. Each function h : X → {±1} is called a hypothesis, or a concept. In this paper we study classes H = HF of the form\nH = {sign(f) : f ∈ F},\nwhere F = {f : X → R} is a class of real-valued functions, and sign(f)(x) = sign ( f(x) ) ∈ {±1} is equal +1 if and only if f(x) ≥ 0. For example, when F is the class of Rd → R affine functions then HF is the class of d-dimensional half spaces. Other examples include neural-nets, low degree polynomials, and more. The reason we “remember” the underlying class F is because we will use it to define comparison queries.\nAn example is a pair (x, y) ∈ X×{±1}. A labeled sample S̄ is a finite sequence of examples. We denote by S the unlabeled sample underlying S̄ that is obtained by removing the labels from the examples in S̄. We will sometimes abuse notation and treat S as a subset of X (formally it is a sequence). Given a distribution D on X × {±1}, the (expected) loss of a hypothesis h is defined by:\nLD(h) , E (x,y)∼D\n[ 1h(x)6=y ] .\nGiven a labeled sample S̄, the empirical loss of h is defined by:\nLS̄(h) , 1∣∣S̄∣∣ ∑\n(x,y)∈S̄\n1h(x)6=y.\nA distribution D on X × {±1} is realizable by H if there exists c ∈ H with LD(c) = 0. We will sometimes refer to such a c as the “learned concept”. A labeled sample S̄ is realizable by H if there exists c ∈ H with LS̄(c) = 0.\nPassive learning (PAC learning). A learning algorithm is an (efficiently computable) mapping that gets a sample as an input and outputs a hypothesis. An algorithm A learns H if there exists a sample complexity bound n( , δ), such that for every realizable distribution D, given a labeled sample S̄ of size n ≥ n( , δ), A outputs h = A ( S̄ ) such that:\nPr S̄∼Dn\n[ LD(h) > ] ≤ δ.\n5if f1, f2 : RN → R are linear and induce a different linear orders on {x1, . . . , xm}, then sign(f1(xi − xj)) 6= sign(f2(xi − xj)) for some i, j, and so the number of distinct orders is at most the number of sign patterns (sign(f(xi − xj)))1≤i<j≤m, which is at most O(m2N ).\nThe parameter is called the error of the algorithm, and 1 − δ is called the confidence of the algorithm We will assume throughout that a learning algorithm for H also receives , δ as part of the input and can compute n( , δ)."
    }, {
      "heading" : "2.1 Active learning",
      "text" : "It is helpful to recall the framework of active learning before extending it by allowing additional queries. A (pool-based) active learning algorithm has an access to the unlabeled sample S underlying the input labeled sample S̄. It queries the labels of a subsample of it, and outputs a hypothesis. The choice of which subsample A queries may be adaptive. Each active learning algorithm is associated with two complexity measures: (i) the sample-complexity n( , δ), is the number of examples required to achieve error at most with confidence at least 1− δ (like in the passive setting), and (ii) the query-complexity (also called label-complexity) q( , δ), is the number of queries it makes.\nIn the process of active learning, it is natural to distinguish between points whose label can be inferred and points for which there is uncertainty concerning their label. It is therefore convenient to consider partial hypotheses. A partial hypothesis is a partially labeled hypothesis h : X → (Y ∪ {?}), where if h(x) = ”?” it means that h abstains from labeling x. We extend the 0/1 loss function to ”?”, such that abstaining is always treated as a mistake. The coverage of h with respect to a distribution D is defined as\nCD(h) , Pr x∼D\n[ h(x) 6=? ] ,\nand the empirical-coverage of h with respect to a sample S is defined as\nCS(h) , ∣∣{x ∈ SX : h(x) 6=?}∣∣ |S| .\nSince abstaining counts as error it follows that CD(h) ≤ 1−LD(h) for every partial hypothesis h, and every distribution D.\nConfident algorithms. A learning algorithm is confident if it outputs a partial hypothesis that is correct on all points where it does not abstain:\nDefinition 2.1 (Confident learning algorithm). A learning algorithm A is confident with respect to a class (X,H) if it satisfies the following additional requirement. For every realizable labeled sample S̄ that is consistent with a learned concept c ∈ H, the output hypothesis h , A ( S̄ ) satisfies that whenever h(x) 6=? then h(x) = c(x).\nThus, if A is confident then CD(h) = 1−LD(h) for every distribution D, where h = A ( S̄ ) . Therefore, in the context of confident learners we will only discuss their coverage, and omit explicit reference to their error. For example a class (X,H) is learned by a confident learner A if there exists a sample complexity n( , δ) such that for every realizable distribution D, given a labeled sample S̄ of size n ≥ n( , δ), A outputs h = A ( S̄ ) such that\nPr S̄∼Dn\n[ CD(h) < 1− ] ≤ δ."
    }, {
      "heading" : "2.2 Interactive learning with additional queries",
      "text" : "Consider an extension of the active learning setting, by allowing the learning algorithm to use additional queries from a prescribed set of queries Q. An additional query is modeled as a boolean function q : F → {True, False}. We stress that the query may depend on the function f that underlies the learned concept c = sign(f) (e.g. comparison queries, see below). In this setting, given an input sample S̄, the algorithm is given access to S, the unlabeled sample underlying S̄, and is allowed to:\n• query the label of any of point in S,\n• query an additional query q from a prescribed set of queries Q(S).\nWe stress that the set of allowable queries Q(S) depend on the input sample S̄. For example, a comparison query on x1, x2 is the query “f(x1) ≤ f(x2)?”, where c = sign(f) is the learned concept. An answered query in a pair (q, b), where q is a comparison/label-query, and b is a possible answer to q. For example, ( “f(x1) ≤ f(x2)?”, T rue ) is an answered comparison query,\nand ( “c(x1) = ?”,−1 ) is an answered label query. The standard notions of version space and agreement set are naturally extended to this context: let\nQ̄ = ( (q1, b1), (q2, b2), . . . (qm, bm) ) be a sequence of answered queries. Define the version space, denoted by V (Q̄), as the set of hypotheses in H that are consistent with Q̄:\nV (Q̄) , { h ∈ H : qi(h) = bi, i = 1, . . . ,m } ,\nand the confidence region, Conf(Q̄), as the agreement set of V (Q̄): Conf(Q̄) , { x ∈ X : all hypotheses in V (Q̄) agree on x } ."
    }, {
      "heading" : "3 Inference Dimension",
      "text" : "Let (X,H) be a hypothesis class, where H = HF for some class of real functions F , and let Q be a set of additional queries. For S ⊆ X, x ∈ X, f ∈ F and c = sign(f) ∈ H, let\nS =⇒ f x\ndenote the statement that there exists a sequence Q̄ of answered label queries of S and/or additional queries from Q(S) that determine the label of x, when the learned concept is c. Namely, that x ∈ Conf(Q̄).\nDefinition 3.1 (Inference dimension). The inference dimension of (X,H) is the minimal number k such that for every S ⊆ X of size k, and every c ∈ H there exists x ∈ S such that\nS \\ {x} =⇒ f x.\nIf no such k exists then the inference dimension of (X,H) is defined as ∞."
    }, {
      "heading" : "3.1 Upper bound",
      "text" : "Theorem 3.2 (Boosting). Let k denote the inference dimension of (X,H). Assume that there is an algorithm that, given a realizable sample S̄ of size n as input, uses at most q(n) queries and infers the answers to all queries in Q(S) and all labels in S̄. Then there is a randomized algorithm that infers all the labels in S̄ using at most\n2q(4k) log n\nqueries in expectation.\nWe prove Theorem 3.2 in two steps: (i) First, Lemma 3.3 shows that (X,H) has a weak confident learner A that, given a realizable input sample of size 4k, uses at most q(4k) queries, and outputs a partial hypothesis h with coverage 1/2. (ii) Then, we show that the labels of a given sample S̄ of size n are revealed after applying A on roughly log n random subsamples of S̄.\nLemma 3.3 (Weak confident-learning). Let k denote the inference dimension of (X,H). Then there exists a confident learner for (X,H) that is defined on input samples of length 4k, makes at most q(4k) queries, and has coverage ≥ 1/2 with probability ≥ 1/2. That is, for any distribution D over X,\nPr S∼D4k\n[ CD(h) ≥ 1/2 ] ≥ 1/2,\nwhere h is the output hypothesis of the algorithm. Proof. The learner is defined as follows. Given a realizable input sample S = (( xi, c(xi) ))4k i=1 , from D4k, the algorithm infers the answers to all queries in Q(S) and all labels in S̄ (by assumption, this can be done with q(4k) queries). It outputs the partial hypothesis h, which labels any x whose label can be inferred from the queries. Namely:\nh(x) , c(x) S =⇒f x? otherwise We next claim that the expected coverage of h, CD(h), is at least 3/4. This implies that PrS [CD(h) ≥ 1/2] ≥ 1/2, which shows that the learning algorithm is a weak confident learner.\nTo this end we use the following observation.\nObservation 3.4. For any set Y of size 4k + 1, there are xi1 , . . . , xi3k+1 ∈ Y such that for all 1 ≤ j ≤ 3k + 1 it holds that\nY \\ {xij} =⇒ f xij .\nProof. This follows since the inference dimension of (X,H) is k. Assume we already constructed xi1 , . . . , xij−1 for j ≤ 3k + 1. Let Y ′ = Y \\ {xi1 , . . . , xij−1}. As |Y ′| ≥ k, there exists xij ∈ Y ′ such that Y ′ \\ {xij} =⇒\nf xij . But then also Y \\ {xij} =⇒ f xij .\nNext, we show that this observation implies that ES [CD(h)] ≥ 3/4. Clearly, we have that\nE S\n[ CD(h) ] = Pr(x1,x2,...,x4k+1)∼D4k+1 [ {x1, . . . , x4k} =⇒\nf x4k+1\n] .\nLetting T = {x1, x2, . . . , x4k+1}, this is the probability that T\\{x4k+1} =⇒ f x4k+1. However, by symmetry, this is the same as the probability that T\\{xi} =⇒ f\nxi for any 1 ≤ i ≤ 4k + 1. Taking the average over i, we have\nE S\n[ CD(h) ] = E\nT\n[ 1\n4k + 1 ∣∣{i : T\\{xi} =⇒ f xi} ∣∣] ≥ 3k + 1 4k + 1 ≥ 3 4 .\nProof of Theorem 3.2. Let A denote the weak confident learner from Lemma 3.3. Let S̄ be a realizable sample, corresponding to an unknown concept c ∈ H. Our goal is to fully recover the labels of S̄.\nThe algorithm proceed in iterations t = 1, 2, . . ., At each iteration it applies A on a subsample of size 4k of S̄. Let ht denote the output hypothesis of A on iteration t, and let\nDISt = {x : hs(x) =? for all s ≤ t}.\nSince A is confident it follows for any point x /∈ DISt, the label c(x) is equal to hs(x) for any hs(x) such that hs(x) 6=?. As long as DISt ∩ S 6= ∅, perform the following update step.\nUpdate step at time t:\n(1) Let Dt be the uniform distribution over DISt ∩ S. Sample R̄t ∼ (Dt)4k.\n(2) Apply A to Rt, the unlabeled sample underlying R̄t. (3) Let ht = A ( R̄t ) be the confident partial hypothesis that A outputs on R̄t.\n(4) Compute et = CDt [ht] = Prx∼Dt [ht(x) 6=?].\n(5) If et < 1/2 then go back to step (1). Otherwise set t← t+ 1 and continue.\nSince A is confident, it follows that once DISt ∩S = ∅ then all the labels of S̄ are revealed.\nQuery-complexity. In order to analyze the query-complexity of the algorithm, first observe that since Pr[et ≥ 1/2] ≥ 1/2 then in expectation, we proceed to the next iteration after at most two samples of R̄t. Next, if et ≥ 1/2 then by definition |DISt+1 ∩ SX | ≤ |DISt ∩ SX |/2. Thus, we only apply the update step at most tmax ≤ 2 log n many times. It follows that the expected query-complexity is at most 2q(4k) log n.\nComputational complexity. The algorithm derived in Theorem 3.2 has expected running time of O(Tupdate log n), where Tupdate is the running time of the update step. In every update step the algorithm makes q(4k) queries and determines et, by checking for each unlabeled point, whether its label can be inferred by the queries performed in this step. Assume that testing this for each point takes take Tinfer. So,\nTupdate = O (q(4k) + n · Tinfer)\nand the total running time is\nTtotal = O ((q(4k) + n · Tinfer) log n) .\nFor example, when the hypothesis class is half spaces in Rd, and the the set of additional queries is comparisons, the total running time is polynomial in n. This is since q(4k) = O(k log k) (by sorting), and since checking if the label of a point is inferred by a set of label and comparison queries can be phrased as a linear program and solved in polynomial time (see Claim 4.5)."
    }, {
      "heading" : "3.2 Lower bound",
      "text" : "Next, we show that if the inference dimension is large then many queries are needed to infer all the labels. We further assume that every query is t-local, in the sense that it depends on f(x1), . . . , f(xt) for some x1, . . . , xt ∈ X. We set of allowable queries Q(S) to be all queries that are associated with subsets of S of size t.\nLet (X,H) be a hypothesis class with inference dimension > k, for some k ≥ 3. This means that there exists Z ⊆ X of size k and a concept c ∈ H such that for every z ∈ Z there is cz ∈ C with cz(z) 6= c(z), but cz(x) = c(x) for all x ∈ Z \\ {z}, and moreover, q(c) = q(cz) for every query q ∈ Q ( Z \\ {z} ) .\nTheorem 3.5. Any algorithm that reveals the labels of any realizable sample of size k must use at least k/t queries in the worst-case.\nProof. Consider the realizable sample S̄ = ( x, c(x) ) x∈Z , and assume that less than k/t queries were used on it. This means that there exist z ∈ S that is not associated with any of the queries that were used, and hence only queries from Q ( Z \\ {z} ) were used. However, this implies that both cz and c are consistent with the queries that were used, and cz(z) 6= c(z). Thus, the label of z can not be inferred by the queries that were used.\nCorollary 3.6. Let = 1k , δ = 1 6 , and let D be the uniform distribution over Z. Then any learning algorithm that makes less than 1t queries suffers a loss of , with probability at least δ.\nProof. Let A be a learning algorithm. Consider an adversary that picks the secret-concept c = cz where z ∈ Z is chosen uniformly. Define\nE , { cx : a query associated with x was queried by A } .\nIf A makes less than k2t queries (i.e. |E| < k 2 ) then: 1. Pr [ cz /∈ E ] ≥ 12 , and\n2. Pr [ L(A) ≥ 1k ∣∣ cz /∈ E] ≥ 1− 2k ≥ 13 , where\nL(A) = ∣∣{x ∈ Z : h(x) 6= cz(x)}∣∣ k\nis the loss of the hypothesis h that A outputs. To see why the second item holds, note that since the answers to the queries are the same for every cx /∈ E, it follows that the distribution of cz conditioned on cz /∈ E is uniform over Ec whose size is at least k2 , and so the probability that A outputs ci is at most 2 k . In any other case, the loss of h is at least 1 k .\nCombining the above two items together yields that Pr [ L(A) ≥ 1\nk\n] = Pr [ cz /∈ E ] · Pr [ L(A) ≥ 1\nk ∣∣ cz /∈ E] ≥ 1 6 ."
    }, {
      "heading" : "4 Interactive learning of half spaces with comparison-queries",
      "text" : "In this section we restrict our attention to the class Hd = {sign(f) : f : Rd → R} of half spaces in Rd, where for simplicity of exposition we consider linear functions f (these correspond to homogeneous half spaces). Our results extend to the non-homogeneous case, as non-homogeneous half spaces in dimension d can be embedded as homogeneous half spaces in dimension d+1. The additional queries allowed are comparison queries. That is, a label query returns the answer to sign(f(x)) and a comparison query returns the answer to f(x1) ≥ f(x2).\nIn Subsection 4.1 we present our upper bounds on the query complexity, under two natural conditions: small bit complexity, or large margin. In Subsection 4.2 we present lower bounds showing that these conditions are indeed necessary for obtaining query complexity sub-linear in the sample complexity."
    }, {
      "heading" : "4.1 Upper bounds",
      "text" : ""
    }, {
      "heading" : "4.1.1 Bit-complexity",
      "text" : "Here we show that if the examples can be represented using a bounded number of bits then comparison-queries can reduce the query-complexity. We formalize bounded bit-complexity by assuming that X = [N ]d, where [N ] = {0, . . . , N}. Note that each example can be represented by B = d logN bits. We provide a bound on the query-complexity that depends efficiently ond and logN . Variants of the arguments we use apply to other standard ways of quantifying bounded bit-complexity.\nTheorem 4.1. Consider the class ( [N ]d, Hd ) . There exists an algorithm that reveals the labels of any realizable input sample of size n using at most O(k log k log n) label/comparison-queries in expectation, where k = O ( d log(Nd) ) .\nAs a consequence it follows that the hypothesis class ( [N ]d, Hd ) is learnable with\nsample complexity Õ ( d/ ) and query-complexity Õ ( d log(N) log(1/ ) ) ,\nwhere the Õ notation suppresses lower order terms and the usual log(1/δ) dependence. In order to prove the above theorem, we use Theorem 1.5 that reduces it to the following lemma. Lemma 4.2. Let k such that 2k/2 > 2(kN + 1)d. Then the inference dimension of the class( [N ]d, Hd ) is at most k. In particular, it is at most 16d log(4Nd).\nProof. Let c = sign(f) ∈ Hd. We will use the following claims.\nObservation 4.3. Let x ∈ Rd, and y ∈ {±1}. Then\nyf(x) > 0 =⇒ c(x) = y, f(x) = 0 =⇒ c(x) = +1.\nObservation 4.4. Let x1, x2 ∈ Rd such that c(x1) = c(x2) = y. Then\n|f(x2)| ≥ |f(x1)| ⇐⇒ yf(x2 − x1) ≥ 0.\nClaim 4.5. Let x1, . . . , xm ∈ Rd, y ∈ {±1} such that:\n• (i) c(xi) = y for all i, and\n• (ii) |f(x1)|≤ . . .≤|f(xm)|. Then, if x ∈ Rd satisfies x − x1 = ∑m−1\ni=1 αi(xi+1 − xi) where αi ≥ 0, then c(x) = y. In particular {x1, . . . , xm} =⇒\nf x.\nProof. By linearity of f , and Observation 4.4: yf(x− x1) = ∑ i αiyf(xi+1 − xi) ≥ 0.\nTherefore yf(x)≥yf(x1) ≥ 0, which, by Observation 4.3, implies that c(x) = y .\nLet k be as in the formulation of the lemma, and let Y ⊆ [N ]d of size k. For simplicity of exposition assume k is even, and let m = k/2. We need to show that there exists x ∈ Y such that Y \\ {x} =⇒\nf x. Without loss of generality, assume that at least m of the points\nin Y have a 1-label. Let x1, x2, . . . , xm ∈ Y be an ordering of the 1-labeled points such that |f(x1)| ≤ . . . ≤ |f(xm)|. By Claim 4.5 it suffices to show that there exists 2 ≤ i∗ ≤ m such that\nxi∗ − x1 = i∗−2∑ i=1 αi(xi+1 − xi) where αi ≥ 0. (1)\nThis follows by the following pigeon hole argument: consider all possible boolean combinations of the form ∑m−1 i=1 βi(xi+1 − xi) where βi ∈ {0, 1}. There are 2m−1 boolean combinations, each of which yields a vector in {−mN, . . . ,mN}d. Therefore, by our assumptions on k we have that\n2m−1 > (2mN + 1)d\nthere exists two boolean combinations:\nm−1∑ i=1 βi(xi+1 − xi) = m−1∑ i=1 γi(xi+1 − xi),\nsuch that βi 6= γi for at least one i. Let i0 be the maximal such i. We verify that i∗ = i0 + 1 satisfies Equation (1) above.\nAssume without loss of generality that βi0 = 0, γi0 = 1. Let αi = βi − γi. Thus αi0 = −1, αi = 0 for all i > i0 and αi ∈ {0,±1} for i < i0. So\ni0∑ i=1 αi(xi+1 − xi) = 0.\nNext, add xi0+1 − x1 = ∑i0 i=1(xi+1 − xi) to both sides. This yields\nxi0+1 − x1 = i0∑ i=1 (αi + 1)(xi+1 − xi).\nThis concludes the proof as αi0 + 1 = 0 and αi + 1 ≥ 0 for all i < i0."
    }, {
      "heading" : "4.1.2 Minimal-ratio and margin",
      "text" : "Let X ⊆ Rd and let c = sign(f) ∈ Hd. The minimal-ratio of X with respect to c is defined by\nη = η(c,X) , minx∈X |f(x)| maxx∈X |f(x)| .\nHere we show that it is possible to reveal all labels using at most Õ ( d log(1/η) log n ) , where the\nminimal-ratio is η. Note that the minimal-ratio is invariant under scaling and that it is upper bounded by the margin:\nClaim 4.6. Let η be the minimal-ratio of X with respect to c, let ρ = maxx∈X ||x||2, and let γ be the margin of X with respect to c. Then\nγ ρ ≤ η.\nProof. Let w such that f(x) = 〈w, x〉, and set w′ = w||w|| . Thus,\nγ ρ =\nminx ∣∣〈w′, x〉∣∣\nmaxx||x||2 ≤\nminx ∣∣〈w′, x〉∣∣\nmaxx ∣∣〈w′, x〉∣∣ = minx∈X |f(x)|maxx∈X |f(x)| = η.\nThus, the upper bound in Theorem 4.7 below applies when the minimal-ratio is replaced by the standard margin parameter γ/ρ.\nNote that there are cases where η >> γ/ρ. For example, assume X = {e1, . . . , ed} is the standard basis and cw ∈ Hd is determined by the normal w = 1√d(+1,−1,+1,−1, . . .). In this case, γ/ρ = 1/ √ d << 1 = η.\nWe next state and prove the upper bound. Let X ⊆ Rd, and let Hd,η ⊆ Hd be the set of all half spaces with minimal-ratio at least η with respect to X.\nTheorem 4.7. Consider the class (X,Hd,η). There exists an algorithm that reveals the labels of any realizable input sample of size n using at most O ( k log k log n ) label/comparison-queries\nin expectation, where k = O ( d log(d) log(1/η) ) .\nAs a consequence it follows that the hypothesis class (X,Hd,η) is learnable with sample complexity Õ ( d/ ) and query-complexity Õ ( d log(1/η) log(1/ ) ) ,\nAs before, the Õ notation suppresses lower order terms and the usual log(1/δ) dependence. The above theorem is a corollary of Theorem 1.5 via the following lemma, which upper bounds the inference dimension of the class (X,Hd,η).\nLemma 4.8. Let k such that (k/2 + 1)d < 2k/2(η/6)d. Then the inference dimension of the class ( X,Hd,η ) is at most k. In particular, it is at most 10d log(d+ 1) log(2/η).\nProof. Let c = sign(f) ∈ Hd,η. We will use the following claims (note the analogy with the claims in the proof of Lemma 4.2).\nObservation 4.9. Let x ∈ X, and y ∈ {±1}. Then\nyf(x)\nmaxx′∈X |f(x′)| ≥ −η =⇒ c(x) = y.\nClaim 4.10. Let x1, . . . , xm ∈ X, y ∈ {±1} such that:\n(i) c(xi) = y for all i, and\n(ii) |f(x1)|≤ . . .≤|f(xm)|. Then, if x ∈ X satisfies x − x1 = ∑m−1\ni=1 αi(xi+1 − xi) where αi ≥ −η, then c(x) = y. In particular {x1, . . . , xm} =⇒\nf x.\nProof. Assume towards contradiction that c(x) 6= y. Therefore, yf(x) ≤ 0. By assumption, x satisfies:\nyf(x− x1) = m−1∑ i=1 αiyf(xi+1 − xi) ≥ −ηy m−1∑ i=1 f(xi+1 − xi) = −ηyf(xm − x1),\nwhich follows by linearity of f and our assumptions. Therefore,\nyf(x) ≥ yf(x1)− ηyf(xm) + ηyf(x1) ≥ −ηyf(xm),\nwhich implies that\n0 ≥ yf(x) maxx′∈X |f(x′)| ≥ yf(x) |f(xm)| = yf(x) yf(xm) ≥ −η.\nBy Observation 4.9 this implies that c(x) = y, which contradicts the assumption that c(x) 6= y.\nLet k be as in the formulation of the lemma, and let Y ⊆ X of size k. For simplicity of exposition assume k is even, and let m = k/2. We need to show that there exists x ∈ Y such that Y \\ {x} =⇒\nf x. Without loss of generality, assume that at least half of the points\nin Y have a 1-label. Let x1, x2, . . . , xm ∈ Y be an ordering of the 1-labeled points such that |f(x1)| ≤ . . . ≤ |f(xm)|. By Claim 4.10 it suffices to show that there exists 2 ≤ i∗ ≤ m such that\nxi∗ − x1 = m−1∑ i=1 αi(xi+1 − xi) where αi ≥ −η and αi∗−1 = αi∗ . (2)\nThis follows by a volume argument: let C be the convex hull of {x2−x1, x3−x1, . . . , xm−x1}. For a set A ⊆ [ m ] , let6\nCA , (∑ j∈A xj+1 − xj ) + η 6 C.\nWe claim that there are A 6= B such that CA ∩ CB 6= ∅. Indeed, assume towards contradiction that the CA are all mutually disjoint. Note that for all A: (i) CA ⊆ ( m + η/6 ) C, and (ii) vol(CA) = (η/6)dvol(C). So, if all the CA’s are mutually disjoint then\n(m+ 1)dvol(C) ≥ 2m(η/6)dvol(C),\nwhich means that (m+1)d > 2m(η/6)d, which contradicts the property k is assumed to satisfy. Thus, there exist two combinations:\nm∑ i=1 βi(xi+1 − xi) = m∑ i=1 γi(xi+1 − xi),\nsuch that βi ∈ [bi, bi + γ/6] and γi ∈ [ci, ci + γ/6] for some bi, ci ∈ {0, 1}, not all the same. Let i0 be maximal such that bi0 6= ci0 . We will prove that Equation (2) holds for i∗ = i0 + 1.\nAssume without loss of generality that bi0 = 0, ci0 = 1. Define αi = βi − γi. Thus αi0 ∈ [−1− η/6,−1 + η/6], and αi ≥ −η/6 for all i > i0. Now, adding\n(αi0+1 − αi0)(xi0+1 − x1) = i0∑ j=1 (αi0+1 − αi0)(xi+1 − xi)\nto both sides of ∑m\nj=1 αi(xi+1 − xi) = 0 gives:\n(αi0+1 − αi0)(xi0+1 − x1) = i0−1∑ j=1 (αi + αi0+1 − αi0)(xi+1 − xi) + αi0+1(xi0+2 − xi0) + m∑\nj=i0+2\nαi(xi+1 − xi).\nWe conclude the proof by verifying that Equation (2) is satisfied after after dividing both sides of the above formula by\nαi0+1 − αi0 ≥ 1− η/3 ≥ 1/2.\nIndeed, the coefficients in front of (xi0 − xi0−1) and (xi0+1 − xi0) are both equal to αi0+1; if i ≥ i0 + 2 then as αi ≥ −η/6 we have αi/(αi0+1 − αi0) ≥ (−η/6)/(1/2) ≥ −η; and if i ≤ i0 − 1 then as αi+αi0+1−αi0 ≥ −(3η)/6 = −η/2, we have (αi+αi0+1−αi0)/(αi0+1−αi0) ≥ −η."
    }, {
      "heading" : "4.2 Lower bounds",
      "text" : "In this Section we show that (in the worst-case) comparison-queries yield no advantage when the bit complexity is large in dimension d ≥ 3, or when the dimension is large even if the margin is large."
    }, {
      "heading" : "4.2.1 Dimension d ≥ 3",
      "text" : "We show that (in the worst-case) comparison-queries do not yield a significant saving in query complexity for learning half spaces, already in R3. This is tight since, as discussed in the introduction, in R2 comparison-queries yield an exponential saving.\n6We use here the standard notation of u+ αA = {u+ αa : a ∈ A}, where u ∈ Rd, α ∈ R, and A ⊆ Rd.\nTheorem 4.11. Consider the class (R3, H3) of half spaces in R3, Any algorithm that reveals the labels of any realizable sample of size n must use Ω(n) comparison/label queries in the worst-case.\nIn the statistical setting, we get that\nCorollary 4.12. Let > 0. Then any algorithm that learns (R3, H3) with error and confidence at least 5/6 must use Ω(1/ ) comparison/label queries on some realizable distributions.\nWe derive these statements by showing that the inference dimension of (R3, H3) is∞. Then, Theorem 4.11 and Corollary 4.12 follow by plugging t = 2 in Theorem 3.5 and Corollary 3.6 respectively. (Note that comparison queries are 2-local, and thus t = 2).\nTheorem 4.13. The inference dimension of (R3, H3) is ∞.\nProof. We need to show that for every n, there is c = sign(f) ∈ H3, and a set Xn ⊆ R3 of n points such that for every x ∈ Xn, it is not the case that Xn \\ {x} =⇒\nf x. We use the\nfollowing lemma.\nLemma 4.14. There exists a 3 dimensional linear space Vn of functions g : {1, . . . n} → R, and n+ 1 functions g0, g1, . . . , gn ∈ Vn with the following properties:\n• gi(j) > 0 if and only if i 6= j (in particular, g0(j) > 0 for all j).\n• |gi(1)| < |gi(2)| < . . . < |gi(n)| for all 0 ≤ i ≤ n.\nWe first use Lemma 4.14 to prove Theorem 4.13. Let v1, v2, v3 ∈ Vn be a basis for the linear space from Lemma 4.14. Define\nxj , ( v1(j), v2(j), v3(j) ) ∈ R3\nand set Xn = {x1, . . . , xn}. Note that each function gi can be represented by a linear combination αi,1v1 + αi,2v2 + αi,3v3. Define fi : R3 → R by fi = 〈αi, x〉, and set ci = sign(fi). Thus, for all i, j,\nfi(xj) = gi(j). (3)\nConsider the set Xn \\ {xi}. Equation (3) implies that there is no comparison nor label query that distinguishes c0 = sign(f0) from ci = sign(fi) on this set. This means that it is not the case that Xn \\ {xi} =⇒\nf0 xi, which finishes the proof of Theorem 4.13.\nIt remains to prove Lemma 4.14.\nProof of Lemma 4.14. For a sufficiently large M , let Vn denote the space of functions g : {1, . . . , n} → R that is spanned by the three functions Mx, x ·Mx, x2 ·Mx. Define\ngi(x) = M x ( 1− 2(x− i)2 ) ∈ Vn.\nIt is easy to check that the first item in the conclusion is satisfied by the gi’s, and that if M is sufficiently large, then so does the second item."
    }, {
      "heading" : "4.2.2 Margin",
      "text" : "We show here that, in the worst-case, comparison-queries do not yield a significant saving in query complexity for learning half spaces, even if it is guaranteed that the margin is large, say at least 1/8.\nTheorem 4.15. For every n there is a class (X,H), where X ⊆ Rn+1, and H ⊆ Hn+1 contains all the half spaces with margin at least 1/8 such that the following holds: any algorithm that reveals the labels of any realizable sample of size n must use Ω(n) comparison/label queries in the worst-case.\nIn the statistical setting, we get that\nCorollary 4.16. For every > 0, there is n and a class (X,H), where X ⊆ Rn+1, and H ⊆ Hn+1 contains all the half spaces with margin at least 1/8 such that the following holds: any algorithm that learns (X,H) with error and confidence at least 5/6 must use Ω(1/ ) comparison/label queries on some realizable distributions.\nWe derive these statements by establishing the existence of classes with large margin and large infrence dimension. Then, Theorem 4.15 and Corollary 4.16 follow by plugging t = 2 in Theorem 3.5 and Corollary 3.6 respectively.\nTheorem 4.17. For every n, there is a set of n unit vectors X = {x1, x2, . . . , xn} ⊂ Rn+1 such that the class (X,H) has inference dimension at least n, where H contains all half spaces with margin at least 1/6 with respect to X.\nProof. Define xi , ei+en+1√\n2 where the ei’s are the vectors in the standard basis, and define\nwi ∈ Rn+1 for i = 1, . . . , n by\nwi(j) ,  1 + j 10n2 j = i\n−12 j = n+ 1 − j\n10n2 otherwise\nand w0 ∈ Rn+1 by\nw0(j) , { −12 j = n+ 1 − j\n10n2 otherwise\nLet fi(x) = 〈wi, x〉, and define ci = sign(fi), for i = 0, . . . , n. A short calculation shows that the margin γ(ci) = minj\n〈wi,xj〉 ||wi||2 ≥ 1/8, and therefore, ci ∈ H for all 0 ≤ i ≤ n. Another calculation shows that ci(xj) = +1 if and only if i = j, and that ∣∣fi(xj)∣∣ = 1/2+j/10n2√2 . In particular note that |fi(xj)| does not depend on i. Consider the set Xn \\ {xi}. The previous paragraph implies that there is no comparison nor label query that distinguishes c0 = sign(f0) from ci = sign(fi) on this set. This means that it is not the case that Xn \\ {xi} =⇒\nf0 xi, which finishes the proof of Theorem 4.17."
    }, {
      "heading" : "5 Acknowledgements",
      "text" : "This work has benefited from various discussions during the special program on Foundations of Machine Learning that took place at the Simons Institute for the Theory of Computing, in Berkeley. In particular, the authors would like to thank Sanjoy Dasgupta for inspiring the focus on comparison queries."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2017,
    "abstractText" : "We study an extension of active learning in which the learning algorithm may ask the annotator to compare the distances of two examples from the boundary of their label-class. For example, in a recommendation system application (say for restaurants), the annotator may be asked whether she liked or disliked a specific restaurant (a label query); or which one of two restaurants did she like more (a comparison query). We focus on the class of half spaces, and show that under natural assumptions, such as large margin or bounded bit-description of the input examples, it is possible to reveal all the labels of a sample of size n using approximately O(log n) queries. This implies an exponential improvement over classical active learning, where only label queries are allowed. We complement these results by showing that if any of these assumptions is removed then, in the worst case, Ω(n) queries are required. Our results follow from a new general framework of active learning with additional queries. We identify a combinatorial dimension, called the inference dimension, that captures the query complexity when each additional query is determined by O(1) examples (such as comparison queries, each of which is determined by the two compared examples). Our results for half spaces follow by bounding the inference dimension in the cases discussed above. ∗Department of Computer Science and Engineering/Department of Mathematics, University of California, San Diego. dakane@ucsd.edu Supported by NSF Career Award ID 1553288. †Department of Computer Science and Engineering, University of California, San Diego. slovett@cs.ucsd.edu. Research supported by NSF CAREER award 1350481. ‡Department of Computer Science and Engineering, University of California, San Diego, Simons Institute for the Theory of Computing, Berkeley, and Max Planck Institute for Informatics, Saarbrücken, Germany. shaymoran1@gmail.com. §Department of Computer Science and Engineering, University of California, San Diego. jpeng.zhang@gmail.com. Research supported by NSF CAREER award 1350481. ar X iv :1 70 4. 03 56 4v 2 [ cs .L G ] 2 J un 2 01 7",
    "creator" : "LaTeX with hyperref package"
  }
}