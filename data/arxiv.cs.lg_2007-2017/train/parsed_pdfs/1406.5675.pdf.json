{
  "name" : "1406.5675.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "The Modified Nyström Method: Theories, Algorithms, and Extension",
    "authors" : [ "Shusen Wang", "Zhihua Zhang" ],
    "emails" : [ "wss@zju.edu.cn", "zhihua}@sjtu.edu.cn" ],
    "sections" : [ {
      "heading" : null,
      "text" : "In this paper we provide theoretical analysis, efficient algorithms, and a simple but highly accurate extension for the modified Nyström method. First, we prove that the modified Nyström method is exact under certain conditions, and we establish a lower error bound for the modified Nyström method. Second, we develop two efficient algorithms to make the modified Nyström method efficient and practical. We devise a simple column selection algorithm with a provable error bound. With the selected columns at hand, we propose an algorithm that computes the modified Nyström approximation in lower time complexity than the approach in the previous work. Third, the extension which we call the SS-Nyström method has much stronger error bound than the modified Nyström method, especially when the spectrum of the kernel matrix decays slowly. Our proposed SS-Nyström can be computed nearly as efficiently as the modified Nyström method. Finally, experiments on real-world datasets demonstrate that the proposed column selection algorithm is both efficient and accurate and that the SS-Nyström method always leads to much higher kernel approximation accuracy than the standard/modified Nyström method.\nKeywords: Kernel methods, kernel approximation, matrix factorization, the Nyström method\nar X\niv :1\n40 6.\n56 75\nv1 ["
    }, {
      "heading" : "1. Introduction",
      "text" : "The kernel methods are important tools in machine learning, computer vision, and data mining (Schölkopf and Smola, 2002, Shawe-Taylor and Cristianini, 2004). However, many kernel methods require matrix computations of high time and space complexities. Let n be the number of data instances. For example, the Gaussian process regression, kernel ridge regression, and least squares SVM all compute the inverse of some n × n matrices which costs time O(n3) and space O(n2); the kernel PCA, Isomap, and Laplacian eigenmaps all perform the truncated eigenvalue decomposition which takes time O(n2k) and space O(n2), where k is the target rank of the decomposition.\nBesides high time complexities, these matrix operations also have high space complexities and are difficult to implement in distributed computing facilities. The matrix decomposition and matrix (pseudo) inverse operations are generally solved by numerical iterative algorithms, which go many passes through the data matrix until convergence. Thus the whole data matrix had better be placed in RAM, otherwise in each iteration there would be a swap between RAM and disk, which is extremely slow. Unless the algorithm is pass-efficient, that is, it goes through the data matrix only constant times, the space complexity should be at least the size of the data matrix. Such iterative algorithms cannot be efficiently implemented and performed in distributed computing facilities like MapReduce for two reasons. First, the RAM cost is too expensive for each individual machine to stand. Second, communication and synchronization must be performed in each iteration of the numerical algorithms, so the cost of each iteration is high.\nOne possible approach to making matrix computation and kernel methods scalable is to use randomized matrix approximations to reduce the time and space costs, among which the most famous one is perhaps the Nyström method (Nyström, 1930). The Nyström method approximates an arbitrary symmetric positive semidefinite (SPSD) kernel matrix using a small subset of its columns, and the method reduces the time complexities of many matrix operations from O(n3) or O(n2k) to O(nc2) and space complexities from O(n2) to O(nc), where k is the target rank, c is the number of selected columns, and it holds in general that k < c n. In this way, time and space costs are only linearly in n, so many kernel methods can be efficiently solved even when n is large.\nThe Nyström method has been widely used to speedup various kernel methods, such as the Gaussian process regression (Williams and Seeger, 2001), kernel SVMs (Zhang et al., 2008, Yang et al., 2012), kernel ridge regression (Yang et al., 2012, Cortes et al., 2010), spectral clustering (Fowlkes et al., 2004, Li et al., 2011), kernel PCA and manifold learning (Zhang et al., 2008, Zhang and Kwok, 2010, Talwalkar et al., 2013), determinantal processes (Affandi et al., 2013), etc.\nTo construct a low-rank matrix approximation, the Nyström method requires a small number of columns (say, c columns) to be selected from the kernel matrix by a column sampling technique. The approximation accuracy is largely determined by the sampling technique; that is, a better sampling technique results in a Nyström approximation with lower approximation error. In the previous work much attention has been made on improving the error bounds of the Nyström method: additive-error bound has been explored by Drineas and Mahoney (2005), Shawe-taylor et al. (2005), Kumar et al. (2012), Jin et al.\n(2013), etc. Recently, Gittens and Mahoney (2013) established the first relative-error bound which is more interesting than additive-error bound (Mahoney, 2011).\nHowever, the approximation quality cannot be arbitrarily improved by devising a very good sampling technique. Wang and Zhang (2013) showed that no matter what sampling technique is used to construct the Nyström approximation, the incurred error (in the spectral norm or the squared Frobenius norm) must grow with the matrix size n at least linearly. Thus, the Nyström approximation can be very rough when n is large, unless large number columns are selected. As was pointed out by Cortes et al. (2010), the tighter kernel approximation leads to the better learning accuracy, so it is useful to find a kernel approximation model that is more accurate than the Nyström method.\nTo improve the approximation accuracy, Kumar et al. (2012) devised a variant of the Nyström method called the ensemble Nyström method, which generates t (> 1) standard Nyström approximations and takes the average. With the ensemble Nyström method at hand, some specific n×n linear systems can be solved in timeO(nc2 log t) when implemented in paralled (Kumar et al., 2012). Empirical results show that the ensemble Nyström method is much more accurate than the standard Nyström method. However, the ensemble Nyström method has the same lower error bound as the standard Nyström method (Wang and Zhang, 2013), which implies that the ensemble Nyström method does not lead to substantial improvements in the kernel approximation accuracy.\nRecently Wang and Zhang (2013) proposed a new alternative called the modified Nyström method. The modified Nyström method can be applied in the same way exactly as the standard Nyström method to speedup kernel methods. The modified Nyström method has an advantage that the error does not grow with matrix size n. Therefore, by using the modified Nyström method instead of the standard Nyström method, a significantly smaller number of columns is needed to attain the same accuracy as the standard Nyström method. Although it has higher time complexity than the standard Nyström method, the modified Nyström method is pass-efficient and has low RAM cost, so the modified Nyström method can be used in big data problems.\nWe explore in this paper the theoretical properties of the modified Nyström method and develop algorithms for efficiently computing the modified Nyström method. We also propose a simple extension of the modified Nyström method called the modified Nyström by spectral shifting (SS-Nyström). SS-Nyström is more accurate than the modified Nyström method, especially when the the spectrum of the kernel matrix decays slowly, where the standard and modified Nyström methods are very inaccurate."
    }, {
      "heading" : "1.1 Our Contributions",
      "text" : "Our contributions mainly include three aspects: theoretical analysis, computational efficient algorithms, and extensions.1 They are summarized as follows.\n1. An early version of the results in Section 4 and Section 5 are published in (Wang and Zhang, 2014). The ISS-Nyström method described in Section 6.2 is published in (Wang et al., 2014); an improved model—the SS-Nyström method—is formulated and analyzed in this paper."
    }, {
      "heading" : "1.1.1 Our Contributions: Theories",
      "text" : "Kumar et al. (2009), Talwalkar and Rostamizadeh (2010) previously showed that the standard Nyström approximation is exact when the original kernel matrix is low-rank. In Section 4.1 we show that the modified Nyström method also exactly recovers the original SPSD matrix under the same conditions.\nFurthermore, Wang and Zhang (2013) proved the lower error bounds of the standard Nyström method. Analogously, in Section 4.2 we establish a lower error bound for the modified Nyström method. The lower bound of the modified Nyström method has a strong resemblance with the lower bound of the column selection problem, which is known to be tight, so we conjecture that our established lower bound is tight."
    }, {
      "heading" : "1.1.2 Our Contributions: Algorithms",
      "text" : "Though the modified Nyström method is more accurate than the standard Nyström method, the modified Nyström approximation is more expensive to compute. In this paper we seek to make the modified Nyström method efficient and practical.\nIn Section 5.1 we provide an efficient algorithm for computing the intersection matrix of the modified Nyström method. Under a certain condition, this algorithm can significantly reduce the time cost.\nIn Section 5.2 we devise a simple and efficient column selection algorithm for the modified Nyström method. We call it the uniform+adaptive2 algorithm. Our uniform+adaptive2 algorithm is more efficient and much easier to implement than the near-optimal+adaptive algorithm of Wang and Zhang (2013), yet its error bound is comparable with the nearoptimal+adaptive algorithm."
    }, {
      "heading" : "1.1.3 Our Contributions: Extension",
      "text" : "The standard/modified Nyström methods generate low-rank approximations to kernel matrices, and their approximation errors cannot be better than the rank-c truncated SVD, where c is the number of columns selected by the Nyström methods. When the spectrum of a kernel matrix decays slowly (that is, the c + 1 to n largest eigenvalues are not small enough), the low-rank approximations constructed by the partial eigenvalue decomposition or the standard/modified Nyström methods are far from the original kernel matrix.\nInspired by a very recent work of Zhang (2014), we propose in this paper a new method called the modified Nyström by spectral shifting (SS-Nyström) to make the approximation still effective even when the spectrum of the original kernel matrix decays slowly. Unlike the standard/modified Nyström methods which approximate the kernel matrix K ∈ Rn×n by a low-rank factorization K ≈ CUCT , our SS-Nyström approximates K by K ≈ C̄UssC̄T + δssIn, where C, C̄ ∈ Rn×c, U,Uss ∈ Rc×c, and δss ≥ 0. When the spectrum of K decays slowly, the term δssIn helps improve the approximation accuracy significantly. We show that SS-Nyström method has a provably tighter bound than the standard/modified Nyström methods. In Section 6 we describe the SS-Nyström method in detail."
    }, {
      "heading" : "1.2 Paper Organization",
      "text" : "The remainder of this paper is organized as follows. In Section 2 we define the notation that will be used in the paper. In Section 3 we describe the Nyström approximation models, column selection algorithm, and the applications to kernel methods. Then we present our work—theories, algorithms, and extensions—respectively in Sections 4, 5, 6. Finally in Section 7 we empirically evaluate the models and algorithms proposed in this paper. All proofs are all deferred to the appendix."
    }, {
      "heading" : "2. Notation",
      "text" : "The notation used in this paper follows that of Wang and Zhang (2013). For an m×n matrix A = [aij ], we let a (i) be its i-th row, aj be its j-th column, ‖A‖F = ( ∑ i,j a 2 ij)\n1/2 be its Frobenius norm, and ‖A‖2 = maxx 6=0 ‖Ax‖2/‖x‖2 be its spectral norm.\nLetting ρ = rank(A), we write the condensed singular value decomposition (SVD) of A as A = UAΣAV T A, where the (i, i)-th entry of ΣA ∈ Rρ×ρ is the i-th largest singular value of A, denoted by σi(A). We also let UA,k and VA,k be the first k (< ρ) columns of UA and VA, respectively, and ΣA,k be the k× k top sub-block of ΣA. Then the m× n matrix Ak = UA,kΣA,kV T A,k is the “closest” rank-k approximation to A.\nIf A is a normal, we let A = UAΛAU T A be the eigenvalue decomposition, and denote the i-th diagonal entry of ΛA by λi(A), where |λ1(A)| ≥ · · · ≥ |λn(A)|. When A is SPSD, the SVD and the eigenvalue decomposition of A are equivalent.\nBased on SVD, the matrix coherence of the columns of A relative to the best rank-k\napproximation is defined by µk = n k maxj ∥∥V(j)A,k∥∥22. Let A† = VAΣ−1A UTA be the MoorePenrose inverse of A. When A is nonsingular, the Moore-Penrose inverse is identical to the matrix inverse. Given another n× c matrix C, we define PCA = CC†A as the projection of A onto the column space of C and PC,kA = C · argminrank(X)≤k ‖A−CX‖F as the rank restricted projection. It is obvious that ‖A− PCA‖F ≤ ‖A− PC,kA‖F .\nFinally, we discuss the time complexities of the matrix operations mentioned above. For an m×n general matrix A (assume m ≥ n), it takes O(mn2) flops to compute the full SVD and O(mnk) flops to compute the truncated SVD of rank k (< n). The computation of A† takes O(mn2) flops. It is worth mentioning that although multiplying an m×n matrix by an n×p matrix takes mnp flops, matrix multiplication is pass-efficient, and it can be performed in full parallel by partitioning the matrices into blocks. Thus, the time and space expense of large-scale matrix multiplication is not a challenge in real-world applications. We denote the time complexity of such a matrix multiplication by TMultiply(mnp), which can be tremendously smaller than O(mnp) in parallel computing facilities (Halko et al., 2011). An algorithm can still be efficient even if it demands large-scale matrix multiplications."
    }, {
      "heading" : "3. The Nyström Methods",
      "text" : "In Section 3.1 we formally describe the Nyström approximation methods, including the standard Nyström method of Nyström (1930), Williams and Seeger (2001), the modified Nyström method of Wang and Zhang (2013), and the SS-Nyström method proposed in this work. Since the Nyström approximation is constructed by using a small portion of columns, in Section 3.2 we introduce some popular column sampling algorithms, especially\nthose with theoretical guarantees. In Section 3.3 we discuss how to apply the Nyström methods to make kernel methods scalable."
    }, {
      "heading" : "3.1 Models",
      "text" : "Suppose we are given an n × n symmetric matrix K, our goal is to compute a fast factorization of K such that the matrix inverse (K + αIn)\n−1 and/or the eigenvalue decomposition of K can be approximately computed highly efficiently. The Nyström methods tackle this problem by approximating K in terms of a subset of its columns, denoted by C ∈ Rn×c. Without loss of generality, K and C can be permuted such that\nK = [ W KT21 K21 K22 ] and C = [ W K21 ] , (1)\nwhere W is of size c×c. Based on the above notation, three kinds of Nyström approximation models are defined as follows. As well as the standard Nyström mehtod, the modified/SS Nyström methods can be trivially extended to the ensemble Nyström method (Kumar et al., 2012), the SPSD Sketching Model (Gittens and Mahoney, 2013), and the memory efficient kernel approximation method (Si et al., 2014)."
    }, {
      "heading" : "3.1.1 The standard Nyström method",
      "text" : "The standard Nyström method is defined by\nK̃nysc , CU nysCT = CW†CT ,\nwhere Unys = W† is called the intersection matrix. The (standard) Nyström method was proposed by Nyström (1930), and it was first introduced to the machine learning community by Williams and Seeger (2001). With the selected columns at hand, the standard Nyström method needs not to see the whole matrix K, and it takes only O(c3) + TMultiply(nc2) time and O(c2) space to compute the intersection matrix Unys."
    }, {
      "heading" : "3.1.2 The Modified Nyström Method",
      "text" : "The modified Nyström method is defined by\nK̃modc , CU modCT = C\n( C†K(C†)T ) CT ,\nwhich is essentially the projection of K onto the column space of C and the row space of CT . This model is proposed by Wang and Zhang (2013), and it is not strictly the Nyström method because it uses a different intersection matrix Umod , C†A(C†)T . With the selected columns at hand, the modified Nyström method needs to go only one pass through the data, and the time and space costs are in general O(nc2) + TMultiply(n2c) and O(nc), respectively.\nAlthough more expensive to compute, the modified Nyström method is a more accurate approximation. Since Umod = C†K(C†)T is the minimizer of the optimization problem\nmin U ‖K−CUCT ‖F , (2)\nso the modified Nyström method is in general more accurate than the standard Nyström method in that ∥∥K−CUmodC∥∥ F ≤ ∥∥K−CUnysC∥∥ F ."
    }, {
      "heading" : "3.1.3 The SS-Nyström Method",
      "text" : "We propose in this paper an extension of the modified Nyström method, which we call the modified Nyström method by spectral shifting (SS-Nyström). The SS-Nyström approximation of K is defined as\nK̃ssc = C̄U ssC̄T + δssIn.\nHere δss ≥ 0 is called the spectral shifting term. This approximation is computed in three steps. First, (approximately) compute the initial spectral shifting term\nδ̄ = 1\nn− k\n( tr(K)− k∑ j=1 σj(K) ) , (3)\nand then perform spectral shift K̄ = K − δ̄In, where k ≤ c is the target rank. Actually, exactly setting the initial spectral shifting term to be δ̄ is unnecessary; later in Section 6 we will show that SS-Nyström has better upper error bound than the modified Nyström method whenever the initial spectral shifting term falls in the interval (0, δ̄]. Second, use some column sampling algorithm to select c columns of K̄ to form C̄. Finally, with C̄ at hand, compute Uss and δss by\nδss = 1\nn− rank(C̄)\n( tr(K)− tr ( C̄†KC̄ )) Uss = C̄†K(C̄†)T − δss(C̄T C̄)†. (4)\nWe will show that K̃ssc is positive (semi)definite if K is positive (semi)definite.\nThe SS-Nyström method is an extension of the modified Nyström method. With columns selected from K to form C, the modified Nyström method is obtained by solving the minimization problem (2) to compute the intersection matrix Umod. Analogously, with the columns selected from K̄ = K− δ̄In to form C̄, SS-Nyström is obtained by solving(\nUss, δss )\n= argmin U,δ ∥∥K− C̄UC̄T − δIn∥∥2F , (5) to obtain the intersection matrix Uss and the spectral shifting term δss."
    }, {
      "heading" : "3.2 Column Sampling Algorithms",
      "text" : "The column selection problem has been widely studied in the theoretical computer science community (Boutsidis et al., 2011, Mahoney, 2011, Guruswami and Sinop, 2012) and the numerical linear algebra community (Gu and Eisenstat, 1996, Stewart, 1999), and numerous algorithms have been devised and analyzed. Here we focus on some theoretically guaranteed algorithms studied in the theoretical computer science community.\nIn the previous work much attention has been paid on improving column sampling algorithms such that the Nyström approximation is more accurate. Uniform sampling is the simplest and most time-efficient column selection algorithm, and it has provable error bounds when applied to the standard Nyström method (Gittens, 2011, Kumar et al., 2012, Jin et al., 2013, Gittens and Mahoney, 2013). To improve the approximation accuracy, many importance sampling algorithms have been proposed, among which the adaptive sampling (Deshpande et al., 2006, Kumar et al., 2012, Wang and Zhang, 2013) (see Algorithm 2) and the leverage score based sampling (Drineas et al., 2008, Ma et al., 2014) are widely studied. The leverage score based sampling has provable bounds when applied to the standard Nyström method (Gittens and Mahoney, 2013), and the adaptive sampling has provable bounds when applied to the modified Nyström method (Wang and Zhang, 2013). Additionally, quadratic Rényi entropy based active subset selection (De Brabanter et al., 2010) and k-means clustering based selection (Zhang and Kwok, 2010) are also effective algorithms, but they do not have additive-error or relative-error bounds.\nParticularly, Wang and Zhang (2013) proposed an algorithm for the modified Nyström method by combining the near-optimal column sampling algorithm (Boutsidis et al., 2011) and the adaptive sampling algorithm (Deshpande et al., 2006). The error bound of the algorithm is the tightest among all the feasible algorithms for the Nyström methods. We show it in the following lemma.\nLemma 1 (The Near-Optimal+Adaptive Algorithm) (Wang and Zhang, 2013) Given a symmetric matrix K ∈ Rn×n and a target rank k, the algorithm samples totally c = O(k −2) columns of K to construct the approximation. We run the algorithm t ≥ (2 −1 + 1) log(1/p) times (independently in parallel) and choose the sample that minimizes ‖K−C ( C†K(C†)T ) CT ∥∥ F\n, then the inequality∥∥K−C(C†K(C†)T )CT∥∥ F ≤ (1 + )‖K−Kk‖F\nholds with probability at least 1−p. The algorithm costs O ( nc2 2+nk3 −2/3 ) +TMultiply ( n2c ) time and O(nc) space in computing C.\nThe near-optimal+adaptive algorithm is effective and efficient, but its implementation is very complicated. Its main component—the near-optimal column selection algorithm— consists of three steps: approximate SVD via random projection (Boutsidis et al., 2011, Halko et al., 2011), the dual-set sparsification algorithm (Boutsidis et al., 2011), and the adaptive sampling algorithm (Deshpande et al., 2006). Without careful implementation of the first two steps, the time and space costs roar, making the near-optimal+adaptive algorithm inefficient."
    }, {
      "heading" : "3.3 Applications to Kernel Methods",
      "text" : "We discuss in this section how to speedup matrix inverse and eigenvalue decomposition using the Nyström methods. Many kernel methods will become scalable if the matrix inverse and eigenvalue decomposition can be efficiently solved.\n• Gaussian process regression (Williams and Seeger, 2001), least squares SVM (Suykens and Vandewalle, 1999), and kernel ridge regression (Saunders et al., 1998) all require\nsolving this kind of linear system:\n(K + αIn)b = y, (6)\nwhich amounts to the matrix inverse problem b = (K+αIn) −1y. Here α is a constant.\n• Spectral clustering (Fowlkes et al., 2004, Li et al., 2011), kernel PCA (Zhang and Kwok, 2010), and many manifold learning (Zhang et al., 2008, Talwalkar et al., 2013) need to perform the truncated eigenvalue decomposition. The sampling algorithm of determinantal processes (Hough et al., 2006, Affandi et al., 2013) performs the full eigenvalue decomposition.\nLet K ∈ Rn×n be the kernel matrix, and let the SS-Nyström approximation of K be defined by\nK̃ssc = CUC T + δIn,\nwhere C and U are n×c and c×c matrices, respectively. We show that when K is replaced by K̃ssc , the aforementioned linear system and eigenvalue decomposition can be efficiently solved. When using K̃nysc or K̃modc to replace K, one can still use the results below by setting δ = 0.\nWe first show how to approximately compute the matrix inverse (K + αIn) −1. Let U = ZΛZT be the condensed eigenvalue decomposition of the intersection matrix of SSNyström, where Z ∈ Rc×r, Λ ∈ Rr×r, and r = rank(U) ≤ c. We expand (K̃ssc + αIn)−1 by the Sherman-Morrison-Woodbury formula and obtain(\nK̃ssc + αIn )−1 = ( C̄ZΛZT C̄T + τIn )−1 = τ−1In − τ−1C̄Z ( τΛ−1 + ZT C̄T C̄Z )−1 ZT C̄T , (7)\nwhere τ = δ+α. In this way the linear system (6) can be approximately computed in only O(nc2) time and O(nc) space.\nNow we show how to approximately compute the eigenvalue decomposition of K. We let C = UCΣCVC be the condensed SVD of C. Suppose r = rank(C), we let\nS = ΣCVCUV T CΣ T C ∈ Rr×r,\nand we write the eigenvalue decomposition of S as S = USΛSU T S . Now we can write the eigenvalue decomposition of K̃ssc as\nK̃ssc = (UCUS)ΛS(UCUS) T + δIn = (UCUS)(ΛS + δIr)(UCUS) T + U⊥(δIn)U T ⊥. (8)\nHere U⊥ ∈ Rn×(n−r) is a column orthogonal complementary matrix of (UCUS)."
    }, {
      "heading" : "4. Theories",
      "text" : "In Section 4.1 we show that the modified Nyström approximation is exact when K is lowrank. In Section 4.2 we provide a lower error bound of the modified Nyström method."
    }, {
      "heading" : "4.1 Theoretical Justifications",
      "text" : "Kumar et al. (2009), Talwalkar and Rostamizadeh (2010) showed that the standard Nyström method is exact when rank(W) = rank(K). We present a similar result for the modified Nyström approximation in Theorem 2.\nTheorem 2 For a symmetric matrix K defined in (1), the following three statements are equivalent: (i) rank(W) = rank(K), (ii) K = CW†CT , (iii) K = CC†K(C†)TCT .\nTheorem 2 implies that the standard and modified Nyström methods are equivalent when rank(W) = rank(K), that is, the kernel matrix K is low rank. However, it holds in general that rank(K) c ≥ rank(W), showing that the two models are not equivalent and that\n∥∥K − K̃modc ∥∥F ≤ ∥∥K − K̃nysc ∥∥F is satisfied. In other words, the modified Nyström method is more accurate than the standard Nyström method in general."
    }, {
      "heading" : "4.2 Lower Error Bounds",
      "text" : "We establish a lower error bound of the modified Nyström method in Theorem 3. Theorem 3 shows that whatever a column sampling algorithm is used to construct the modified Nyström approximation, at least c ≥ 2k −1 columns must be chosen to attain the 1 + bound.\nTheorem 3 (Lower Error Bound of the Modified Nyström Method) Whatever a column sampling algorithm is used, there exists an n × n SPSD matrix K such that the error incurred by the modified Nyström method obeys:∥∥K−CUmodCT∥∥2\nF ≥ n− c\nn− k\n( 1 + 2k\nc\n) ‖K−Kk‖2F .\nHere k is an arbitrary target rank, c is the number of selected columns, and Umod = C†K(C†)T .\nBoutsidis et al. (2011) established a lower error bound for the column selection problem, and the lower error bound is tight because it is attained by the optimal column selection algorithm of Guruswami and Sinop (2012). Boutsidis et al. (2011) showed that whatever a column sampling algorithm is used, there exists an m × n matrix A such that the error incurred by the projection of A onto the column space of C is lower bounded by∥∥A−CC†A∥∥2\nF ≥ n− c\nn− k\n( 1 + k\nc\n) ‖A−Ak‖2F , (9)\nwhere k is an arbitrary target rank, c is the number of selected columns.\nInterestingly, the modified Nyström approximation is the projection of K onto the column space of C and the row space of CT simultaneously, so there is a strong resemblance between the modified Nyström approximation and the column selection problem. As we see, the lower error bound of the modified Nyström approximation in Theorem 3 differs from (9) only by a factor of 2. So it is a reasonable conjecture that the lower bound in Theorem 3 is tight, as well as the lower bound of the column selection problem in (9). We leave it as an open problem."
    }, {
      "heading" : "5. Algorithms",
      "text" : "In Section 5.1 we propose a fast approach for computing the intersection matrices of the modified/SS Nyström methods. In Section 5.2 we devise a simple and efficient column selection algorithm which is nearly as accurate as the state-of-the-art algorithm of Wang and Zhang (2013)."
    }, {
      "heading" : "5.1 Fast Computation of the Intersection Matrices",
      "text" : "Naively computing the intersection matrix Umod = C†K(C†)T takes time O(nc2) + TMultiply(n\n2c), which is much more expensive than computing Unys = W† for the standard Nyström method. In this section we propose a more efficient algorithm for computing the intersection matrix, which only takes time O(c3) + TMultiply ( (n − c)2c ) . The algorithm is described in Theorem 4. The algorithm is obtained by expanding the Moore-Penrose inverse of C using the theorem of (Ben-Israel and Greville, 2003, Page 179).\nTheorem 4 For an n×n symmetric matrix K, when the submatrix W is nonsingular, the intersection matrix of the modified Nyström method Umod = C†K(C†)T can be computed in time O(c3) + TMultiply ( (n− c)2c ) by the following formula:\nUmod = C†K(C†)T = T1 ( W + T2 + T T 2 + T3 ) TT1 ,\nwhere the intermediate matrices are computed by\nT0 = K T 21K21, T1 = W −1(Ic + W−1T2)−1, T2 = T0W −1, T3 = W −1(KT21K22K21)W−1.\nThe four intermediate matrices are all of size c× c, and the matrix inverse operations are on c× c small matrices.\nRemark 5 Since the submatrix W is not in general nonsingular, before using the algorithm, the user should first test the rank of W, which takes time O(c3). Empirically, for the radial basis function (RBF) kernel matrix K defined by\nkγij = exp\n( − 1\n2γ2 ‖xi − xj‖22\n) , (10)\nthe submatrix W is usually nonsingular2, and the algorithm is useful; for the linear kernel, W is often singular, so the algorithm does not work.\nTo illustrate the effect of our algorithm for computing the intersection matrix Umod = C†K(C†)T , we generate kernel matrices of the Letters Dataset which has 15, 000 instances and 16 attributes (see Table 2). We first generate a dense RBF kernel matrix with scale parameter γ = 0.2, and then obtain a sparse symmetric matrix by by truncating the entries with small magnitude such that 1% entries are nonzero. The sizes of the kernel matrices are\n2. The matrix W is nonsingular if the scaling parameter γ is positive and the selected c data instances are distinct points (Schölkopf and Smola, 2002, Theorem 2.18).\nboth 15, 000× 15, 000. We sample c columns uniformly to compute the intersection matrix Umod = C†K(C†)T (the modified Nyström) and Unys = W† (the standard Nyström). We vary c from 100 to 2, 000 and plot the time for computing U is plotted in Figure 1. In both cases, our algorithm is faster than the naive approach, and the speedup is particularly significant when K is sparse.\nFollowing the proof of Theorem 4, we derive an algorithm for efficiently computing the intersection matrix Uss and the spectral shifting term δss of the SS-Nyström method. The algorithm is described in the following corollary, whose proof is analogous to that of Theorem 4.\nCorollary 6 For an n× n symmetric matrix K, let δ̄ be the initial spectral shifting term, K̄ = K− δ̄In, and C̄ be c columns selected from K̄. According to the partition in (1), denote the submatrices of K̄ by W̄ = W− δ̄Ic and K̄22 = K22− δ̄In−c. When the submatrix W̄ is nonsingular, the intersection matrix Uss and spectral shifting term δss of the SS-Nyström method defined in (4) can be computed in time O(c3) + TMultiply ( (n− c)2c ) by the following formula:\nδss = 1\nn− c\n( tr(K)− tr ( C̄†KC̄ )) =\n1\nn− c\n( tr(K)− tr(T4)− cδ̄ ) ,\nUss = C̄†K(C̄†)T − δss(C̄T C̄)−1 = T5 + (δ̄ − δss) ( C̄T C̄ )† ,\nwhere the intermediate matrices are computed by\nT0 = K T 21K21, T3 = W̄ −1(Ic + T1W̄−1) T1 = W̄ −1T0,, T4 = T3 ( W̄2 + T0 + T1W̄ + T2 ) ,\nT2 = W̄ −1KT21K̄22K21, T5 = T3 ( W̄ + T1 + T T 1 + T2W̄ −1)TT3 . The intermediate matrices are all of size c × c, and the matrix inverse operations are on c× c small matrices.\nAlgorithm 1 The Uniform+Adaptive2 Algorithm.\n1: Input: an n×n symmetric matrix K, target rank k, error parameter ∈ (0, 1], matrix coherence µ. 2: Uniform Sampling. Uniformly sample\nc1 = 8.7µk log (√ 5k )\ncolumns of K without replacement to construct C1; 3: Adaptive Sampling. Sample\nc2 = 10k −1\ncolumns of K to construct C2 using adaptive sampling algorithm 2 according to the residual K− PC1K;\n4: Adaptive Sampling. Sample\nc3 = 2 −1(c1 + c2)\ncolumns of K to construct C3 using adaptive sampling algorithm 2 according to the residual K− P[C1, C2]K;\n5: return C = [C1,C2,C3] and U = C †K(C†)T ."
    }, {
      "heading" : "5.2 An Efficient Column Sampling Algorithm",
      "text" : "In this paper we propose a column sampling algorithm which is efficient, effective, and very easy to implement. The algorithm consists of a uniform sampling step and two adaptive sampling steps, so we call it the uniform+adaptive2 algorithm. The algorithm is described in Algorithm 1 and analyzed in Theorem 7. We will empirically evaluate the column selection algorithms in Section 7.\nThe idea behind the uniform+adaptive2 algorithm is quite intuitive. Since the modified Nyström method is the simultaneous projection of K onto the column space of C and the row space of CT , the approximation error will get lower if span(C) better approximates span(K). After the initialization by uniform sampling, the columns of K far from span(C1) have large residuals and are thus likely to get chosen by the adaptive sampling. After two rounds of adaptive sampling, columns of K are likely to be near span(C).\nIt is worth mentioning that our uniform+adaptive2 algorithm is a special instance the adaptive-full algorithm of (Kumar et al., 2012, Figure 3). The adaptive-full algorithm consists of a random initialization followed by multiple adaptive sampling steps. Obviously, using multiple adaptive sampling steps can surely reduce the approximation error. However, the update of sampling probability in each step is expensive, so we choose to do only two steps. Importantly, the adaptive-full algorithm of (Kumar et al., 2012, Figure 3) is merely a heuristic scheme without a theoretical guarantee, whereas our uniform+adaptive2 algorithm has a strong error bound which is nearly as good as the state-of-the-art algorithm of Wang and Zhang (2013) (See Theorem 7).\nTheorem 7 (The Uniform+Adaptive2 Algorithm.) Given an n×n symmetric matrix K and a target rank k, we let µk denote the matrix coherence of K. Algorithm 1 samples\nAlgorithm 2 The Adaptive Sampling Algorithm.\n1: Input: a residual matrix B ∈ Rn×n and number of selected columns c (< n). 2: Compute sampling probabilities pj = ‖bj‖22/‖B‖2F for j = 1, · · · , n; 3: Select c indices in c i.i.d. trials, in each trial the index j is chosen with probability pj ; 4: return an index set containing the indices of the selected columns.\ntotally\nc = O ( k −2 + µk −1k log k )\ncolumns of K to construct the approximation. We run Algorithm 1\nt ≥ (20 −1 + 18) log(1/p)\ntimes (independently in parallel) and choose the sample that minimizes ‖K−C ( C†K(C†)T ) CT ∥∥ F\n, then the inequality∥∥K−C(C†K(C†)T )CT∥∥ F ≤ ( 1 +\n)∥∥K−Kk∥∥F holds with probability at least 1 − p. The algorithm costs O ( nc2 2 ) + TMultiply ( n2c ) time and O(nc) space in computing C.\nRemark 8 Theoretically, Algorithm 1 requires to compute the matrix coherence of K in order to determine c1, c2, and c3. However, computing the matrix coherence takes time O(m2k) and is thus impractical; even the fast approximation approach of Drineas et al. (2012) is not feasible here because K is a square matrix. The use of the matrix coherence here is merely for theoretical analysis; setting the parameter µ in Algorithm 1 to be exactly the matrix coherence does not certainly result in the highest accuracy. According to our off-line experiments, the resulting approximation accuracy is not sensitive to the value of µ. Thus we strongly suggest the users to set µ in Algorithm 1 to be a constant (say 1), rather than actually computing the matrix coherence.\nTable 1 presents comparisons between the near-optimal+adaptive algorithm of Wang and Zhang (2013) and our uniform+adaptive2 algorithm over the time and space costs for computing C, the number of columns required to attain 1 + relative-error bound, and the hardness of implementation. The time cost of our algorithm is lower than the nearoptimal+adaptive algorithm, and the space cost of the two algorithms are the same. To attain the same error bound, our algorithm needs to select c = O ( k −2 + µk −1k log k ) columns, which is a little larger than that of the near-optimal+adaptive algorithm. When → 0, we have that O ( k −2 + µk −1k log k ) = O ( k −2). Therefore, the error bound of our algorithm is nearly as good as the near-optimal+adaptive algorithm because is usually set to be a very small value."
    }, {
      "heading" : "6. The SS-Nyström Approximation",
      "text" : "In this section we propose a variant of the modified Nyström method which is still effective when the spectrum of K decays slowly. We call the proposed method the modified Nyström\n( ) ( ) ( ) ( )\nAlgorithm 3 The Modified Nyström by Spectral Shifting (SS-Nyström).\n1: Input: an n× n SPSD matrix K, a target rank k, the oversampling parameter l. 2: // approximately compute the initial spectral shifting term δ̄ 3: Ω←− n× l standard Gaussian matrix; 4: Q←− the l orthonormal basis of KΩ ∈ Rn×l; 5: s←− sum of the top k singular values of QTK ∈ Rl×n; 6: δ̃ = 1n−k ( tr(K)− s ) ≈ δ̄; 7: // spectral shifting and column selection 8: K̄← K− δ̃In ∈ Rn×n; 9: C̄←− c columns of K̄ selected by some column sampling algorithm;\n10: // compute the spectral shifting parameter and the intersection matrix 11: δss ←− 1 n−rank(C̄) ( tr(K)− tr ( C̄†KC̄ )) ; 12: Uss ←− C̄†K(C̄†)T − δss(C̄T C̄)†; 13: return the SS-Nyström approximation K̃ssc = C̄U ssC̄T + δssIn.\nmethod by spectral shifting (SS-Nyström). In Section 6.1 we formulate and justify the SSNyström method. In Section 6.2 we provide upper error bounds of the SS-Nyström method. In Section 6.3 we devise an algorithm for efficiently computing the initial spectral shifting parameter. The whole procedure is described in Algorithm 3. We will empirically compare SS-Nyström with the standard/modified Nyström methods in Section 7."
    }, {
      "heading" : "6.1 Model Formulation",
      "text" : "As was discussed in Section 1.1.3, when the bottom eigenvalues of a kernel matrix are large, low-rank matrix approximation methods—the standard/modified Nyström methods and even the partial eigenvalue decomposition—work poorly. To improve the kernel approximation accuracy, Zhang (2014) proposed a kernel approximation model called the matrix ridge approximation (MRA). MRA approximates any SPSD matrix by K ≈ AAT + δIn where A is an n × c matrix and δ > 0 is the average of the n − c bottom eigenvalues. The MRA AAT + δIn has better condition number than K, so it works well no matter whether the bottom eigenvalues are large or small. However, MRA is solved by an iterative algorithm, so it is not pass-efficient. When the kernel matrix does not fit in RAM, MRA becomes inefficient.\nInspired by MRA of Zhang (2014), we propose a novel kernel approximation model which inherits the efficiency of the Nyström method and is effective when the bottom eigenvalues are large. We call our model the modified Nyström method by spectral shifting (SS-Nyström),\nwhich is defined by K̃ssc = C̄U ssC̄T + δssIn ≈ K. (11) Here δss and Uss are previously defined in (4), C̄ contains c columns of K̄ = K− δ̄In, and δ̄ is the initial spectral shifting term defined in (3). The following theorem shows some properties of the SS-Nyström method.\nTheorem 9 The pair (δss,Uss) defined in (4) is the global optimum minimizer of problem (5), which indicates that using any other (δ,U) to replace (δss,Uss) results in lower approximation accuracy. Furthermore, if K is positive (semi)definite, then the approximation C̄UssC̄T + δssIn is also positive (semi)definite."
    }, {
      "heading" : "6.2 Error Analysis",
      "text" : "Directly analyzing the theoretical error bound of SS-Nyström is not easy, so we formulate a variant of SS-Nyström called the modified Nyström method by inexact spectral shifting (ISSNyström) and instead analyze the error bound of ISS-Nyström. We will provide theoretical error bounds of the ISS-Nyström method in Theorem 12 and Corollary 13 in this subsection. It follows from Theorem 9 that ISS-Nyström is less accurate than SS-Nystöm in that∥∥K− K̃ssc ∥∥F ≤ ∥∥K− K̃issc ∥∥F , thus the error bounds of ISS-Nyström still hold if K̃issc is replaced by K̃ ss c .\nISS-Nyström is defined by\nK̃issc = C̄ŪC̄ T + δIn. (12)\nHere δ > 0 is the spectral shifting term, and C̄ŪC̄T is the modified Nyström approximation of K̄ = K− δIn.\nWe first show how to set the spectral shifting term δ. It follows from the definition in (12) directly that the approximation error is K − K̃issc = K̄ − C̄ŪC̄T ; Lemma 1 and Theorem 7 indicate that by selecting sufficiently many columns of K̄ to construct C̄ and Ū, it holds with high probability that∥∥K− K̃issc ∥∥F = ∥∥K̄− C̄ŪC̄T∥∥F ≤ (1 + )∥∥K̄− K̄k∥∥F . Apparently, for fixed k, the smaller the error ‖K̄ − K̄k‖F is, the tighter error bound the ISS-Nyström has; if ‖K̄− K̄k‖F ≤ ‖K−Kk‖F , then ISS-Nyström has a better error bound than the modified Nyström. Therefore our goal is to make ‖K̄− K̄k‖F as small as possible, so we formulate the following optimization problem to compute δ:\nmin δ≥0 ∥∥K̄− K̄k∥∥2F ; s.t. K̄ = K− δIn. However, since K̄ is in general indefinite, it requires all of the eigenvalues of K to solve the problem exactly. Since computing the full eigenvalue decomposition is expensive, we attempt to relax the problem. Considering that\n∥∥K̄− K̄k∥∥2F = min|J |=n−k∑ j∈J ( σj(K)− δ )2 ≤ n∑ j=k+1 ( σj(K)− δ )2 , (13)\nwe seek to minimize the upper bound of ‖K̄ − K̄k‖2F , which is the righthand side of (13), to compute δ, leading to the solution\nδ̄ = 1\nn− k n∑ j=k+1 σj(K) = 1 n− k ( tr(K)− k∑ j=1 σj(K) ) . (14)\nNotice that δ̄ is also used as the initial spectral shifting term of SS-Nyström. If we choose δ = 0, then ISS-Nyström degenerates to the modified Nyström method. The following theorem indicates that the ISS-Nyström with any δ ∈ (0, δ̄] has a stronger relative-error bound than the modified Nyström method.\nTheorem 10 Give an n×n SPSD matrix K, we let K̄ = K−δIn and δ̄ be defined in (14). Then for any δ ∈ (0, δ̄], the following inequality holds:∥∥K̄− K̄k∥∥2F ≤ ∥∥K−Kk∥∥2F . Remark 11 Using the same column selection algorithm to sample c columns, if the modified Nyström method attains the error bound∥∥K− K̃modc ∥∥F ≤ (1 + ) ∥∥K−Kk‖F with high probability, then the ISS-Nyström method attains the error bound∥∥K− K̃issc ∥∥F ≤ (1 + )∥∥K̄− K̄k‖F with the same probability. Therefore, due to Theorem 10, the upper error bound of ISSNyström is always better than the modified Nyström method.\nWe give an example in Figure 2 to illustrate why ISS-Nyström is better than the modified Nyström method. We use the toy data matrix K: an n × n SPSD matrix whose the t-th eigenvalue is 1.05−t. We set n = 100, k = 30, and thus δ̄ = 0.064. From the plot of the eigenvalues we can see that the “tail” of the eigenvalues becomes thinner after the spectral shifting. Specifically, ‖K−Kk‖2F = 0.52 and ‖K̄− K̄k‖2F ≤ 0.24. When the same number of columns are selected to construct the ISS-Nyström or the modified Nyström approximations, ISS-Nyström has much tighter error bound because ‖K̄ − K̄k‖2F is much smaller than ‖K−Kk‖2F .\nWe provide error analysis for the ISS-Nyström method in Theorem 12, which shows that ISS-Nyström always has tighter error bound than the modified Nyström method. We also demonstrate in Example 1 that in some cases the ISS-Nyström method can be better than any other low-rank matrix approximation methods.\nTheorem 12 Suppose there is a column selection algorithm Acol such that for any n × n symmetric matrix S and target rank k ( n), by selecting c ≥ C(n, k, ) columns of S using algorithm Acol, the modified Nyström method attains the error bound∥∥S− S̃modc ∥∥2F ≤ (1 + )∥∥S− Sk∥∥2F .\nThen for any n × n SPSD matrix K, we compute δ̄ according to (14) and compute K̄ = K − δ̄In. By using Acol to select c ≥ C(n, k, ) columns of K̄, the ISS-Nyström defined in (5) attains the error bound\n∥∥K− K̃issc ∥∥2F ≤ (1 + )(∥∥K−Kk∥∥2F − [∑n i=k+1 λi(K) ]2\nn− k\n) .\nIf the columns of K̄ are selected by the near-optimal+adaptive column sampling algorithm of Wang and Zhang (2013) which is the best practical algorithm for the modified Nyström method, then the error bound incurred by ISS-Nyström is given in the following corollary.\nCorollary 13 Suppose we are given an SPSD matrix K and we sample c = O(k −2) columns of K̄ to form C̄ using the near-optimal+adaptive column sampling algorithm (Lemma 1). We run the algorithm t ≥ (2 −1 + 1) log(1/p) times (independently in parallel) and choose the sample that minimizes ‖K̄− C̄ ( C̄†K̄(C̄†)T ) C̄T ∥∥ F , then the inequality\n∥∥K− K̃issc ∥∥2F ≤ (1 + )(‖K−Kk‖2F − [∑n i=k+1 λi(K) ]2\nn− k ) holds with probability at least 1− p.\nRemark 14 If we set the initial spectral shifting term of the SS-Nyström method equal to the spectral shifting term of ISS-Nyström, that is, SS-Nyström and ISS-Nyström have the same C̄, then it follows directly from Theorem 9 that Theorem 12 and Corollary 13 still hold if K̃issc is replaced by K̃ ss c .\nUsing the near-optimal+adaptive algorithm—the best practical algorithm for the modified Nyström method—to sample c = O(k −2) columns, the upper error bound∥∥K− K̃modc ∥∥2F ≤ (1 + ) ∥∥K−Kk∥∥2F\nholds with high probability (see Lemma 1). When the bottom eigenvalues λk+1(K), · · · , λn(K) are large, we can see from Corollary 13 that the error bound of ISS-Nyström is much better than that of the modified Nyström method. Here we give an example to demonstrate the superiority of SS-Nyström over the the standard/modified Nyström methods and even the truncated SVD of the same scale.\nExample 1 Let K be an n × n SPSD matrix such that λ1(K) ≥ · · · ≥ λk(K) > θ = λk+1(K) = · · · = λn(K) > 0. By sampling c = O(k) columns by the near-optimal+adaptive algorithm of Wang and Zhang (2013), we have that∥∥K− K̃ssc ∥∥2F = ∥∥K− K̃issc ∥∥2F = 0, and that\n(n− c)θ2 = ∥∥K−Kc∥∥2F ≤ ∥∥K− K̃modc ∥∥2F ≤ ∥∥K− K̃nysc ∥∥2F .\nIn this example SS-Nyström is far better than the other approximation methods if we set θ a large constant."
    }, {
      "heading" : "6.3 Efficient Algorithm for Computing δ̄",
      "text" : "The SS-Nyström method uses δ̄ as the initial spectral shifting term. However, computing δ̄ according to (14) requires the partial eigenvalue decomposition which costs time O(n2k) and space O(n2). This can be accelerated by computing the top-k singular values approximately using random projection techniques (Boutsidis et al., 2011, Halko et al., 2011). We depict the algorithm for approximately computing δ̄ using random projections in Lines 2–6 of Algorithm 3. The performance of the approximation is analyzed in the following theorem.\nTheorem 15 Let δ̄ be defined in (14) and δ̃, k, l, n be defined in Algorithm 3. The following inequality holds in expectation:\nE [∣∣δ̄ − δ̃∣∣ / δ̄] ≤ k/√l,\nwhere the expectation is taken w.r.t. the Gaussian random matrix Ω in Algorithm 3. Lines 2–6 in Algorithm 3 compute δ̃ in time O(nl2) + Tmultiply(n2l) and space O(nl).\nBy using Algorithm 3 to compute δ̄ approximately, it costs only O(nl2) + Tmultiply(n2l) more time to compute the SS-Nyström approximation than the modified Nyström approximation.\nWe evaluate the accuracy of the approximation to δ̄ (Lines 2–6 in Algorithm 3) proposed in Theorem 15. We generate RBF kernel matrices of the datasets listed in Table 2, and we set the scaling parameter γ such that η defined in (15) equals to 0.5 or 0.9. The details of experiment settings are described later in Section 7.1. We use the error ratio |δ̄ − δ̃|/δ̄ to evaluate the approximation performance. We repeat the experiments 20 times and plot the average error ratio versus l/k in Figure 3. Here δ̃, l, and k are defined in Theorem 15.\nWe can see from Figure 3 that the approximation of δ̄ is of very high quality: when l = 4k, the error ratios are less than 0.03 in all cases, no matter whether the spectrum\nof K decays fast or slow. So we set l = 4k in all of the subsequent kernel approximation experiments in order to obtain a low over-sampling rate with a high accuracy at the same time. Since it costs O(nc2) + Tmultiply(n2c) time to compute the modified Nyström (in\ngeneral) and c should be set as O(k −2), if we set l = 4k, then the time complexity of computing SS-Nyström is the same as computing the modified Nyström."
    }, {
      "heading" : "7. Experiments on Kernel Approximation",
      "text" : "In this section we empirically compare between the three Nyström approximation models: the standard Nyström method of Nyström (1930), Williams and Seeger (2001), the modified Nyström method of Wang and Zhang (2013), and the SS-Nyström method proposed in this work. We also evaluate our uniform+adaptive2 column sampling algorithm proposed in Section 5.2; the uniform sampling and the near-optimal+adaptive column sampling algorithm of Wang and Zhang (2013) are employed for comparison."
    }, {
      "heading" : "7.1 The Setup",
      "text" : "We perform experiments on several datasets released by UCI (Frank and Asuncion, 2010) and Statlog (Michie et al., 1994). We obtain the data collected on the LIBSVM website3 where the data are scaled to [0,1]. We summarize the datasets in Table 2. For each dataset, we generate a radial basis function (RBF) kernel matrix K defined by kij = exp(− 12γ ‖xi − xj‖ 2 2). Here γ > 0 is the scaling parameter; the larger the scaling parameter γ is, the faster the spectrum of the kernel decays (Gittens and Mahoney, 2013). Experience from previous work indicates that for the same dataset, with different settings of γ, the Nyström methods and the sampling algorithms can have very different performance.\nHere we discuss how to set γ. Let p = d0.05ne, we define\nη ,\n∑p i=1 λ\n2 i (K)∑n\ni=1 λ 2 i (K) = ‖Kp‖2F ‖K‖2F , (15)\nwhich denotes the weight of the top 5% eigenvalues of the kernel matrix K. In general large γ results in large η. For each dataset, we use two different settings of γ such that η = 0.5 or η = 0.9. Obviously, when η is small, the bottom eigenvalues of K are large, and our SS-Nyström is in a favorable position.\nFor each of the three modes , that is, the standard/modified/SS Nyström methods, we use three column selection algorithms: the uniform sampling, the uniform+adaptive2 (Algorithm 1), and the near-optimal+adaptive algorithm of Wang and Zhang (2013). We implement all the algorithms in MATLAB and run the algorithms on a workstation with Intel Xeon 2.40GHz CPUs, 24GB RAM, and 64bit Windows Server 2008 system. To compare the running time, we set MATLAB in single thread mode by the command “maxNumCompThreads(1)”. For the target rank k used throughout this paper, we set k = dn/100e.\n3. http://www.csie.ntu.edu.tw/∼cjlin/libsvmtools/datasets/\nWe report the approximation errors and running time of each algorithm for each method. The approximation accuracy is evaluated by\nApproximation Error = ‖K− K̃‖F /‖K‖F ,\nwhere K̃ is the approximation generated by each method. Every time when we do column sampling, we run each sampling algorithm 10 times and report the minimal approximation error of the 10 repeats. We report the average elapsed time of the 10 repeat rather than the total elapsed time because the 10 repeats can be done in parallel on 10 machines. For the kernel matrices with η = 0.5 (η is defined in (15)), the approximation errors and the average running time are depicted in Figures 4 and 6 respectively. For the kernel matrices with η = 0.9, the approximation errors are depicted in Figures 5; the curve of the running time is very similar to Figure 6, so we do not show it here."
    }, {
      "heading" : "7.2 Comparisons among the Kernel Approximation Models",
      "text" : "The experiments show that our SS-Nyström achieves the highest kernel approximation accuracy among the three Nyström approximation methods. For the kernel matrices with η = 0.5 where the spectrum decays slowly and the bottom eigenvalues are large, our SS-Nyström method is tremendously more accurate than the standard/modified Nyström methods, which is in accordance with our theoretical analysis. When η = 0.9 where the spectrum of kernel matrix decays fast, our SS-Nyström method is still more accurate than the other two methods, but the advantage is not as evident as the η = 0.5 cases.\nAs for the running time, our SS-Nyström is a little slower than the modified Nyström because SS-Nyström needs to compute δ̄ approximately by randomized SVD, which costs time O(nk2)+Tmultiply(n2k) (as we set l = 4k). Since it costs time O(nc2)+Tmultiply(n2c) to compute the modified Nyström approximation, so our SS-Nyström should be only constant times slower than the modified Nyström; this is verified by experiments."
    }, {
      "heading" : "7.3 Comparisons among the Column Selection Algorithms",
      "text" : "The empirical results in the figures show that our uniform+adaptive2 algorithm achieves accuracy comparable with the state-of-the-art algorithm—the near-optimal+adaptive algorithm of Wang and Zhang (2013). Especially, when c is large, those two algorithms have virtually the same accuracy, which is in accordance with our analysis in the last paragraph of Section 5.2: large c implies small error term , and the error bounds of the two algorithms coincide when is small.\nAs for the running time, we can see that our uniform+adaptive2 algorithm performs column selection very efficiently and the elapsed time grows slowly in c. In comparison, our algorithm is much more efficient than the near-optimal+adaptive algorithm."
    }, {
      "heading" : "8. Concluding Remarks",
      "text" : "In this paper we have provided a comprehensive study of the modified Nyström method. First, we have proved that the modified Nyström approximation is exact when the original matrix is low-rank. We have also established a lower error bound for the modified Nyström method: at least c ≥ 2k −1 columns must be chosen to attain the 1 + bound. We have conjectured this lower error bound to be tight. Notice that the best known algorithm for the modified Nyström method requires at most c = k −2 columns to attain the 1 + bound, so there is a gap between the lower and upper error bounds. It remains an open problem whether there exists an algorithm attaining the lower error bound or not. It is worthy of mentioning that the very recent work (Boutsidis and Woodruff, 2014) showed that O(k −1) columns and rows suffice to achieve 1 + relative-error bound for the CUR matrix decomposition problem, which is a generalization of modified Nyström method to general rectangular matrices.\nSecond, we have devised a column selection algorithm called uniform+adaptive2 and provided a relative-error bound for the algorithm. The algorithm is highly efficient and effective as well as very easy to implement. The error bound of the algorithm is nearly as strong as that of the state-of-the-art algorithm—the near-optimal+adaptive algorithm— which is complicated and difficult to implement. The experimental results have shown that our uniform+adaptive2 algorithm is more efficient than the near-optimal+adaptive algorithm, while their accuracies are comparable. We have also devised an algorithm for computing the intersection matrix of the modified Nyström approximation; under certain conditions, our algorithm can significantly improve the time complexity. The speedup induced by this algorithm has also been verified empirically.\nThird, to improve the kernel approximation accuracy when the spectrum of the kernel matrix decays slowly, we have proposed an extension of the modified Nyström method called the SS-Nyström method. The SS-Nyström method can speedup many kernel methods in the same way as the standard/modified Nyström methods. We have shown that SSNyström has a much stronger error bound than the standard/modified Nyström methods. Especially, when the bottom eigenvalues of a kernel matrix are not sufficiently small, the approximation accuracy of the standard/modified Nyström method or even the truncated SVD is unsatisfactory, while our SS-Nyström can still generate approximations of high accuracy. We have also devised an algorithm for computing SS-Nyström efficiently. The experiments have further demonstrated that our SS-Nyström method is tremendously more accurate than the standard/modified Nyström methods when the bottom eigenvalues of the kernel matrix are large.\nTo summarize, the modified/SS Nyström method are much more accurate than the standard Nyström method, both theoretically and empirically; but the modified/SS Nyström methods are slower to compute. If users want higher kernel approximation accuracy, we suggest using the modified Nyström method or the SS-Nyström method. If the spectrum of the kernel matrix decays very fast, then there is little difference between the modified Nyström method and the SS-Nyström method. However, if the spectrum decays slowly, we strongly recommend using the SS-Nyström method, because neither of the standard/modfied Nyström method can achieve satisfactory accuracy. As for the column selection, we recommend using the uniform+adaptive2 algorithm proposed in this paper."
    }, {
      "heading" : "Appendix A. Proof of Theorem 2",
      "text" : "Proof Suppose that rank(W) = rank(K). We have that rank(W) = rank(C) = rank(K) because rank(K) ≥ rank(C) ≥ rank(W). (16) Thus there exists a matrix X such that[\nKT21 K22\n] = CXT = [ WXT\nK21X T\n] ,\nand it follows that K21 = XW and K22 = K21X T = XWXT . Then we have that\nK =\n[ W (XW)T\nXW XWXT\n] = [ I X ] W [ I XT ] , (17)\nCW†CT =\n[ W\nXW\n] W† [ W (XW)T ] = [ I X ] W [ I XT ] . (18)\nHere the second equality in (18) follows from WW†W = W. We obtain that K = CW†C. Then we show that K = CC†K(C†)TCT .\nSince C† = (CTC)†CT , we have that\nC† = ( W(I + XTX)W )† W [I , XT ],\nand thus C†K(C†)TW = ( W(I + XTX)W )† W(I + XTX) [ W(I + XTX)W ( W(I + XTX)W )† W ]\n= ( W(I + XTX)W )† W(I + XTX)W,\nwhere the second equality follows from Lemma 16 because (I + XTX) is positive definite. Similarly we have\nWC†K(C†)TW = W ( W(I + XTX)W )† W(I + XTX)W = W.\nThus we have\nCC†K(C†)TC = [ I X ] WC†K(C†)TW [ I XT ] = [ I X ] W [ I XT ] . (19)\nIt follows from Equations (17) (18) (19) that K = CW†CT = CC†K(C†)TCT . Conversely, when K = CW†CT , we have that rank(K) ≤ rank(W†) = rank(W). By applying (16) we have that rank(K) = rank(W). When K = CC†K(C†)TCT , we have rank(K) ≤ rank(C). Thus there exists a matrix\nX such that [ KT21 K22 ] = CXT = [ WXT K21X T ] ,\nand therefore K21 = XW. Then we have that\nC = [ W K21 ] = [ I X ] W,\nso rank(C) ≤ rank(W). Apply (16) again we have rank(K) = rank(W).\nLemma 16 XTVX ( XTVX )† XT = XT for any positive definite matrix V.\nProof Since the positive definite matrix V have a decomposition V = BTB for some nonsingular matrix B, so we have\nXTVX ( XTVX )† XT = (BX)T ( BX ( (BX)T (BX) )†) (BX)TB(BTB)−1\n= (BX)T ( (BX)T )† (BX)T (BT )−1 = (BX)T (BT )−1 = XT ."
    }, {
      "heading" : "Appendix B. Proof of Theorem 3",
      "text" : "In Section B.1 we provide several key lemmas, and then in Section B.2 we prove Theorem 3 using Lemmas 19 and 18.\nB.1 Key Lemmas\nLemma 17 provides a useful tool for expanding the Moore-Penrose inverse of partitioned matrices, and the lemma will be used to prove Lemma 19 and Theorem 3.\nLemma 17 (Page 179 of Ben-Israel and Greville (2003)) Given a matrix X ∈ Rm×n of rank of at least c which has a nonsingular c × c submatrix X11. By rearrangement of columns and rows by permutation matrices P and Q, the submatrix X11 can be bought to the top left corner of X, that is,\nPXQ = [ X11 X12 X21 X22 ] .\nThen the Moore-Penrose inverse of X is\nX† = Q [ Ic TT ] ( Ic + TT T )−1 X−111 ( Ic + SS T )−1 [ Ic S T ] P,\nwhere T = X−111 X12 and S = X21X −1 11 .\nLemmas 18 and 19 will be used to prove Theorem 3.\nLemma 18 (Lemma 19 of Wang and Zhang (2013)) Given n and k, we let B be an n k× n k matrix whose diagonal entries equal to one and off-diagonal entries equal to α ∈ [0, 1). We let A be an n× n block-diagonal matrix\nA = diag(B, · · · ,B︸ ︷︷ ︸ k blocks ). (20)\nLet Ak be the best rank-k approximation to the matrix A, then we have that\n‖A−Ak‖F = (1− α) √ n− k.\nLemma 19 For an n × n matrix B with diagonal entries equal to one and off-diagonal entries equal to α, the error incurred by the modified Nyström method is lower bounded by\n‖B− B̃modc ‖2F ≥ (1− α)2(n− c) ( 1 + 2\nc − (1− α)1 + o(1) αcn/2\n) .\nProof Without loss of generality, we assume the first c column of B are selected to construct C. We partition B and C as:\nB = [ W BT21 B21 B22 ] and C = [ W B21 ] .\nHere the matrix W can be expressed by W = (1− α)Ic + α1c1Tc . We apply the ShermanMorrison-Woodbury formula\n(X + YZR)−1 = X−1 −X−1Y(Z−1 + RX−1Y)−1RX−1\nto compute W−1, yielding\nW−1 = 1\n1− α Ic −\nα\n(1− α)(1− α+ cα) 1c1\nT c . (21)\nWe expand the Moore-Penrose inverse of C by Lemma 17 and obtain C† = W−1 ( Ic + S TS )−1 [ Ic S T ]\nwhere S = B21W −1 = α\n1− α+ cα 1n−c1\nT c .\nIt is easily verified that STS = (\nα 1−α+cα )2 (n− c)1c1Tc .\nNow we express the matrix constructed by the modified Nyström method in a partitioned form:\nB̃modc = CC †B ( C† )T CT\n= [ W B21 ] W−1 ( Ic + S TS )−1 [ Ic S T ] B [ Ic S ] ( Ic + S TS )−1 W−1 [ W B21 ]T = [ ( Ic + S TS )−1\nB21W −1(Ic + STS)−1\n] [ Ic S T ] B [ Ic S ][ ( Ic + S TS )−1 B21W −1(Ic + STS)−1 ]T . (22)\nWe then compute the submatrices ( Ic + S TS )−1 and B21W −1(Ic + STS)−1 respectively\nas follows. We apply the Sherman-Morrison-Woodbury formula to compute ( Ic + S TS )−1\n, yielding\n( Ic + S TS )−1 = ( Ic + ( α 1− α+ cα )2 (n− c)1c1Tc )−1 = Ic − γ11c1Tc , (23)\nwhere\nγ1 = n− c nc+ (\n1−α α )2 + 2(1−α)cα .\nIt follows from (21) and (23) that W−1 ( Ic + S TS )−1 = (γ2Ic − γ31c1Tc )(Ic − γ11c1Tc ) = γ2Ic + (γ1γ3c− γ1γ2 − γ3)1c1Tc ,\nwhere\nγ2 = 1\n1− α and γ3 =\nα\n(1− α)(1− α+ αc) .\nThen we have that B21W −1(Ic + STS)−1 = α(γ1γ3c2 − γ3c− γ1γ2c+ γ2)1n−c1Tc , γ1n−c1Tc , (24)\nwhere γ = α ( γ1γ3c 2 − γ3c− γ1γ2c+ γ2 ) = α(αc− α+ 1)\n2αc− 2α− 2α2c+ α2 + α2cn+ 1 . (25)\nSince B21 = α1n−c1 T c and B22 = (1− α)In−c + α1n−c1Tn−c, it is easily verified that[\nIc S T ] B [ Ic S ] = [ Ic S T ] [ W BT21\nB21 B22\n] [ Ic S ] = (1− α)Ic + λ1c1Tc , (26)\nwhere\nλ = α(3αn− αc− 2α+ α2c− 3α2n+ α2 + α2n2 + 1)\n(αc− α+ 1)2\nIt follows from (22), (23), (24), and (26) that\nB̃modc = [ Ic − γ11c1Tc γ1n−c1 T c ]( (1− α)Ic + λ1c1Tc )[ Ic − γ11c1Tc γ1n−c1 T c ]T , [ B̃11 B̃ T 21 B̃21 B̃22 ] ,\nwhere\nB̃11 = (1− α)Ic + [ (1− γ1c)(λ− λγ1c− (1− α)γ1)− (1− α)γ1 ] 1c1 T c = (1− α)Ic + η11c1Tc , B̃21 = Ã T 12 = γ(1− γ1c)(1− α+ λc)1n−c1Tc = η21n−c1Tc , B̃22 = γ 2c(1− α+ λc)1n−c1Tn−c = η31n−c1Tn−c,\nwhere\nη1 = (1− γ1c)(λ− λγ1c− (1− α)γ1)− (1− α)γ1, η2 = γ(1− γ1c)(1− α+ λc), η3 = γ 2c(1− α+ λc),\nBy dealing with the four blocks of B̃modc respectively, we finally obtain that\n‖B− B̃modc ‖2F = ‖W − B̃11‖2F + 2‖B21 − B̃21‖2F + ‖B22 − B̃22‖2F = c2(α− η1)2 + 2c(n− c)(α− η2)2\n+(n− c)(n− c− 1)(α− η3)2 + (n− c)(1− η3)2 = (n− c)(α− 1)2 ( 1 + 2 c − ( 1 + o(1) ) 1− α αcn/2 ) .\nB.2 Proof of the Theorem\nNow we prove Theorem 3 using Lemma 19 and Lemma 18. Let C consist of c column sampled from A and Ĉi consist of ci columns sampled from the i-th block diagonal matrix in A. Without loss of generality, we assume Ĉi consists of the first ci columns of B. Then the intersection matrix U is computed by\nU = C†A ( CT )† = [ diag ( Ĉ1, · · · , Ĉk )]† A [ diag ( ĈT1 , · · · , ĈTk )]† = diag ( Ĉ†1B ( Ĉ†1 )T , · · · , Ĉ†kB ( Ĉ†k )T) .\nThe modified Nyström approximation of A is\nÃmodc = CUC T = diag ( Ĉ1Ĉ † 1B ( Ĉ†1 )T ĈT1 , · · · , ĈkĈ † kB ( Ĉ†k )T ĈTk ) ,\nand thus the approximation error is\n∥∥A− Ãmodc ∥∥2F = k∑ i=1 ∥∥∥B − ĈiĈ†iB(Ĉ†i)T ĈTi ∥∥∥2 F\n≥ (1− α)2 k∑ i=1 (p− ci) ( 1 + 2 ci − (1− α) (1 + o(1) αcip/2 ))\n= (1− α)2 ( k∑ i=1 (p− ci) + k∑ i=1 2(p− ci) ci ( 1− (1− α)(1 + o(1)) αp )) ≥ (1− α)2(n− c) ( 1 + 2k\nc\n( 1− k(1− α)(1 + o(1))\nαn\n)) ,\nwhere the former inequality follows from Lemma 19, and the latter inequality follows by minimizing over c1, · · · , ck. Finally we apply Lemma 18, and the theorem follows by setting α→ 1."
    }, {
      "heading" : "Appendix C. Proof of Theorem 4",
      "text" : "Proof Let C ∈ Rm×c consists of a subset of columns of K. By row permutation C can be expressed as\nPC = [ W K21 ] .\nThen according to Lemma 17, the Moore-Penrose inverse of C can be written as\nC† = W−1 ( Ic + S TS )−1 [ Ic S T ] P,\nwhere S = K21W −1. Then the intersection matrix of modified Nyström approximation to K can be expressed as\nU = C†K ( C† )T\n= W−1 ( Ic + S TS )−1 [ Ic S T ] PKPT [ Ic S ] ( Ic + S TS )−1 W−1\n= W−1 ( Ic + S TS )−1 [ Ic S T ] [ W KT21\nK21 K22\n] [ Ic S ] ( Ic + S TS )−1 W−1\n= W−1 ( Ic + S TS )−1( W + KT21S + (K T 21S) T + STK22S )( Ic + S TS )−1 W−1\n, T1 ( W + T2 + T T 2 + T3 ) TT1 .\nHere the intermediate matrices are computed by\nT0 = K T 21K21, T1 = W −1(Ic + STS)−1 = W−1(Ic + W−1T0W−1)−1,\nT2 = K T 21S = K T 21K21W −1 = T0W −1, T3 = S TK22S = W −1(KT21K22K21)W−1. The matrix inverse operations are on c × c matrices which costs O(c3) time. The matrix multiplication KT21K22K21 requires time TMultiply ( (n− c)2c ) ."
    }, {
      "heading" : "Appendix D. Proof of Theorem 7",
      "text" : "The error analysis for the uniform+adaptive2 algorithm relies on Lemma 20, which guarantees the error incurred by its uniform sampling step. The proof of Lemma 20 essentially follows Gittens (2011). We prove Lemma 20 using probability inequalities and some techniques of Boutsidis et al. (2011), Gittens (2011), Gittens and Mahoney (2013), Tropp (2012); the proof is in Appendix D.1.\nLemma 20 (Uniform Column Sampling) Given an m×n matrix A and a target rank k, let µk denote the matrix coherence of A. By sampling\nc = µkk log(k/δ)\nθ log θ − θ + 1 ,\ncolumns uniformly without replacement to construct C, the following inequality∥∥A− PC,kA∥∥2F ≤ (1 + δ−1θ−1)∥∥A−Ak∥∥2F . holds with probability at least 1 − 2δ. Here δ ∈ (0, 0.5) and θ ∈ (0, 1) are arbitrary real numbers.\nThe error analysis for the two adaptive sampling steps of the uniform+adaptive2 algorithm relies on Lemma 21, which follows immediately from (Wang and Zhang, 2013, Corollary 7 and Section 4.5).\nLemma 21 Given an n×n symmetric matrix K and a target rank k, we let C1 contain the c1 columns of K selected by a column sampling algorithm such that the following inequality holds: ∥∥K− PC1K∥∥2F ≤ f∥∥K−Kk∥∥2F . Then we select c2 = kf −1 columns to construct C2 and c3 = (c1 + c2) −1 columns to construct C3, both using the adaptive sampling according to the residual B1 = K − PC1K and B2 = K− P[C1,C2]K, respectively. Let C = [C1,C2,C3], we have that\nP {∥∥K−C(C†K(C†)T )CT∥∥ F∥∥K−Kk∥∥F ≥ 1 + s } ≤ 1 + 1 + s ,\nwhere s is an arbitrary constant greater than 1.\nFinally Theorem 7 is proved by combining Lemma 20 and Lemma 21. The proof is in Appendix D.2.\nD.1 Proof of Lemma 20\nProof We use uniform column sampling to select c column of A to construct C = AS. Here the n× c random matrix S has one entry equal to one and the rest equal to zero in each column, and at most one nonzero entry in each row, and S is uniformly distributed among (nc ) such kind of matrices. Applying Lemma 7 of Boutsidis et al. (2011), we get∥∥A− PC,kA∥∥2F ≤ ∥∥A−Ak∥∥2F + ∥∥(A−Ak)S∥∥2F ∥∥(VTA,kS)†∥∥22. (27) Now we bound\n∥∥(A−Ak)S∥∥22 and ∥∥(VTA,kS)†∥∥22 respectively using the techniques of Gittens (2011), Gittens and Mahoney (2013), Tropp (2012).\nLet I ⊂ [n] be a random index set corresponding to S. The support of I is uniformly distributing among all the index sets in 2[n] with cardinality c. According to Gittens and Mahoney (2013), the expectation of ∥∥(A−Ak)S∥∥2F can be written as E ∥∥(A−Ak)S∥∥2F = E∥∥(A−Ak)I∥∥2F = cE∥∥(A−Ak)i∥∥2F = cn∥∥A−Ak∥∥2F .\nApplying Markov’s inequality, we have that\nP {∥∥(A−Ak)S∥∥2F ≥ cnδ∥∥A−Ak∥∥2F } ≤ E ∥∥(A−Ak)S∥∥2F c nδ\n∥∥A−Ak∥∥2F = δ. (28) Here δ ∈ (0, 0.5) is a real number defined later.\nNow we establish the bound for E ∥∥Ω†2∥∥22 as follows. Let λi(X) be the i-th largest\neigenvalue of X. Following the proof of Lemma 1 of Gittens (2011), we have\n∥∥(VTA,kS)†∥∥22 = λ−1k (VTA,kSSTVA,k) = λ−1k ( c∑ i=1 Xi ) ≤ λ−1min ( c∑ i=1 Xi ) , (29)\nwhere the random matrices X1, · · · ,Xc are chosen uniformly at random from the set{( VTA,k ) i ( VTA,k )T i }n i=1\nwithout replacement. The random matrices are of size k × k. We accordingly define\nR = max i λmax(Xi) = max i ∥∥(VTA,k)i∥∥22 = knµk, where µk is the matrix coherence of A, and define\nβmin = cλmin ( EX1 ) = λmin ( c n VTA,kVA,k ) = c n .\nThen we apply Lemma 22 and obtained the following inequality:\nP [ λmin ( c∑ i=1 Xi ) ≤ θc n ] ≤ k [ eθ−1 θθ ] c kµk , δ, (30)\nwhere θ ∈ (0, 1] is a real number, and it follows that\nc = µkk log(k/δ)\nθ log θ − θ + 1 .\nApplying (29) and (30), we have P {∥∥(VTA,kS)†∥∥22 ≥ nθc} ≤ δ. (31)\nCombining (28) and (31) and applying the union bound, we have the following inequality: P {∥∥(A−Ak)S∥∥2F ≥ cnδ∥∥A−Ak∥∥2F or ∥∥(VTA,kS)†∥∥22 ≥ nθc } ≤ 2δ. (32)\nFinally, from (27) and (32) we have that the inequality∥∥A− PC,kA∥∥2F ≤ (1 + δ−1θ−1)∥∥A−Ak∥∥2F holds with probability at least 1− 2δ, by which the lemma follows.\nLemma 22 (Theorem 2.2 of Tropp (2012)) We are given l independent random d× d SPSD matrices X1, · · · ,Xl with the property\nλmax(Xi) ≤ R for i = 1, · · · , l.\nWe define Y = ∑l i=1 Xi and βmin = lλmin ( EX1 ) . Then for any θ ∈ (0, 1], the following inequality holds:\nP { λmin(Y) ≤ θβmin } ≤ d [ eθ−1\nθθ\n]βmin R\n.\nD.2 Proof of the Theorem\nProof The matrix C1 consists of c1 columns selected by uniform sampling, and C2 ∈ Rn×c2 and C3 ∈ Rn×c3 are constructed by adaptive sampling. We set δ = 1/ √ 5 and θ = √ 5/4 for Lemma 20, then we have\nf = 1 + δ−1θ−1 = 5,\nc1 = µkk log(k/δ)\nθ log θ − θ + 1 = 8.7µkk log(\n√ 5k).\nThen we set\nc2 = kf −1 = 5k −1, and c3 = (c1 + c2) −1,\naccording to Lemma 21. Letting s > 1 be an arbitrary constant, we have that\nP {∥∥K−CUCT∥∥ F∥∥K−Kk∥∥F ≤ 1 + s }\n≥ P {∥∥K−CUCT∥∥ F∥∥K−Kk∥∥F ≤ 1 + s ∣∣∣∣∣ ∥∥K− PC1K∥∥2F∥∥K−Kk∥∥2F ≤ f } · P {∥∥K− PC1K∥∥2F∥∥K−Kk∥∥2F ≤ f }\n≥ (\n1− 1 + 1 + s\n)( 1− 2δ ) .\nwhere the last inequality follows from Lemma 20 and Lemma 21.\nRepeating the sampling procedure for t times and letting C[i] and U[i] be the i-th sample, we obtain an upper error bound on the failure probability:\nP { min i∈[t] {∥∥K−C[i]U[i]CT[i]∥∥F∥∥K−Kk∥∥F } ≥ 1 + s } ≤ ( 1− ( 1− 1 + 1 + s )( 1− 2δ ))t = ( 1 +\n(s− 1)(1− 2δ) −1 + 1 + 2δ(s− 1)\n)−t , p.\nTaking logarithm of both sides of the equality and applying log(1 +x) ≈ x when x is small, we have\nt = [ log ( 1 + (1− 2δ)(s− 1)\n−1 + 1 + 2δ(s− 1)\n)]−1 log 1\np ≈ −1 + 1 + 2δ(s− 1) (1− 2δ)(s− 1) log 1 p .\nSetting s = 2, we have that t ≈ (10 −1 + 18) log(1/p). Hence by sampling totally\nc = ( 1 + −1 )( 5k −1 + 8.7µkk log( √ 5k) )\ncolumns and repeating the procedure for\nt ≥ (10 −1 + 18) log(1/p)\ntimes, the algorithm attains the upper error bound∥∥K−C(C†K(C†)T )CT∥∥ F ≤ ( 1 + 2 )∥∥K−Kk∥∥F with probability at least 1− p. Substituting 2 by ′ yields the error bound in the theorem.\nTime complexity complexity of the uniform+adaptive2 is calculated as follows. The uniform sampling costs O(n) time; the first adaptive sampling round costs O(nc21) + TMultiply(n\n2c1) time; the second adaptive sampling round costsO(n(c1+c2)2)+TMultiply(n2(c1+ c2)) time. So the total time complexity is O(nc2 2) + TMultiply(n2c )."
    }, {
      "heading" : "Appendix E. Proof of Theorem 9",
      "text" : "In Section E.1 we derive the solution to the optimization problem (5). In Section E.2 we prove that the solutions are global optimum. In Section E.3 we prove that the resulting solution is SPSD when K is SPSD.\nE.1 Solution to the Optimization Problem (5)\nWe denote the objective function of the optimization problem (5) by f(U, δ) = ∥∥K− C̄UC̄T − δIn∥∥2F .\nFirst, we take the derivative of f(U, δ) w.r.t. U to be zero\n∂f(U, δ)\n∂U =\n∂\n∂U tr(C̄UC̄T C̄UC̄T − 2KC̄UC̄T + 2δC̄UC̄T )\n= 2C̄T C̄UC̄T C̄− 2C̄TKC̄ + 2δC̄T C̄ = 0,\nand obtain that\nUss = (C̄T C̄)†(C̄TKC̄− δssC̄T C̄)(C̄T C̄)†\n= (C̄T C̄)†C̄TKC̄(C̄T C̄)† − δss(C̄T C̄)†C̄T C̄(C̄T C̄)† = C̄†K(C̄†)T − δss(C̄T C̄)†.\nSimilarly, we take the derivative of f(U, δ) w.r.t. δ to be zero\n∂f(U, δ)\n∂δ =\n∂\n∂δ tr(δ2In − 2δK + 2δC̄UC̄T ) = 2nδ − 2tr(K) + 2tr(C̄UC̄T ) = 0,\nand it follows that\nδss = 1\nn\n( tr(K)− tr(C̄UssC̄T ) ) = 1\nn\n( tr(K)− tr ( C̄C̄†K(C̄†)T C̄T ) + δsstr ( C̄(C̄T C̄)†C̄T )) = 1\nn\n( tr(K)− tr ( C̄T C̄C̄†K(C̄†)T ) + δsstr ( C̄C̄† )) = 1\nn\n( tr(K)− tr ( C̄TK(C̄†)T ) + δssrank(C̄) )\nand thus\nδss = 1\nn− rank(C̄)\n( tr(K)− tr ( C̄†KC̄) )) .\nE.2 Proof of Optimality\nThe Hessian matrix of f(U, δ) with respect to (U, δ) is\nH =  ∂2f(U, δ) ∂vec(U)∂vec(U)T ∂2f(U, δ) ∂vec(U)∂δ ∂2f(U, δ)\n∂δ∂vec(U)T ∂2f(U, δ) ∂δ2\n = 2 [ (C̄T C̄)⊗ (C̄T C̄) vec(C̄T C̄) vec(C̄T C̄)T n ] .\nFor any X ∈ Rc×c and b ∈ R, we let\nq(X, b) = [ vec(X)T b ] H [ vec(X) b ] = vec(X)T ( (C̄T C̄)⊗ (C̄T C̄) ) vec(X) + 2b vec(C̄T C̄)T vec(X) + nb2\n= vec(X)T vec ( (C̄T C̄)X(C̄T C̄) ) + 2b vec(C̄T C̄)T vec(X) + nb2\n= tr(XT C̄T C̄XC̄T C̄) + 2b tr(C̄T C̄X) + nb2.\nLet C̄XC̄T = Y ∈ Rn×n, then\nq(X, b) = tr(C̄XT C̄T C̄XC̄T ) + 2b tr(C̄XC̄T ) + nb2\n= tr(YTY) + 2b tr(Y) + nb2\n= n∑ i=1 n∑ j=1 y2ij + 2b n∑ l=1 yll + nb 2\n= ∑ i 6=j y2ij + n∑ l=1 (yll + b) 2 ≥ 0,\nwhich shows that the Hessian matrix H is SPSD. Hence f(Uss, δss) is the global minimum of f .\nE.3 Proof of SPSD\nWe denote the thin SVD of C̄ by C̄ = UC̄ΣC̄V T C̄ . The approximation is\nC̄UssC̄T + δssIn = C̄ ( C̄†K(C̄†)T − δss(C̄T C̄)† ) C̄T + δssIn\n= C̄ ( C̄†K(C̄†)T ) C̄T + δss ( In − C̄(C̄T C̄)†C̄T ) .\nIf K is positive (semi)definite, the first term C̄ ( C̄†K(C̄†)T ) C̄T is positive (semi)definite. So it remains to be shown that δss ≥ 0 and In − C̄(C̄T C̄)†C̄T is SPSD. The scalar δss is positive because\ntr(K)− tr ( C̄†KC̄ ) = tr(K)− tr ( C̄C̄†K ) = tr(K)− tr ( UC̄U T C̄K ) ≥ 0.\nHere the inequality follows from that σi(K) ≥ σi(UTC̄KUC̄) for i = 1, · · · , c (Horn and Johnson, Lemma 3.3.1).\nLet U⊥ C̄ be an n× (n− c) column orthogonal matrix orthogonal to UC̄, we have that\nIn − C̄(C̄T C̄)†C̄T = In −UC̄UTC̄ = U ⊥ C̄U ⊥ C̄\nT\nis SPSD. Hence the approximation C̄UssC̄T + δssIn is positive (semi)definite when K is positive (semi)definite."
    }, {
      "heading" : "Appendix F. Proof of Theorem 10",
      "text" : "Proof Since the righthand side of (13) is convex and δ̄ is the minimizer of the righthand of (13), so for any δ ∈ (0, δ̄], it holds that\nn∑ j=k+1 ( σj(K)− δ )2 ≤ n∑ j=k+1 ( σj(K)− 0 )2 = ∥∥K−Kk∥∥2F .\nThen the theorem follows by the inequality in (13)."
    }, {
      "heading" : "Appendix G. Proof of Theorem 12",
      "text" : "Proof The error incurred by SS-Nyström is∥∥K− K̃ssc ∥∥2F = ∥∥(K̄ + δ̄In)− (C̄ŪC̄T + δ̄In)∥∥2F = ∥∥K̄− C̄ŪC̄T∥∥2F ≤ (1 + )\n∥∥K̄− K̄k∥∥2F = (1 + ) n∑ i=k+1 σ2i ( K̄ ) = (1 + ) n∑ i=k+1 λi ( K̄2 ) .\nHere the inequality follows from the property of the column selection algorithm Acol. The i-th largest eigenvalue of K̄ is λi(K) − δ̄, so the n eigenvalues of K̄2 are all in the set {(λi(K) − δ̄)2}ni=1. The sum of the smallest n − k of the n eigenvalues of K̄2 must be less than or equal to the sum of any n− k of the eigenvalues, thus we have\nn∑ i=k+1 λi ( K̄2 ) ≤\nn∑ i=k+1 ( λi(K)− δ̄ )2 =\nn∑ i=k+1 λ2i (K)− 2 n∑ i=k+1 δ̄λi(K) + (n− k)(δ̄)2\n= ‖K−Kk‖2F − 1\nn− k [ n∑ i=k+1 λi(K) ]2 ,\nby which the theorem follows."
    }, {
      "heading" : "Appendix H. Proof of Theorem 15",
      "text" : "Proof Let K̃ = Q(QTK)k, where Q is defined in Line 4 in Algorithm 3. Boutsidis et al. (2011) showed that\nE‖K− K̃‖2F ≤ (1 + k/l) ‖K−Kk‖2F , (33)\nwhere the expectation is taken w.r.t. the random Gaussian matrix Ω. It follows from Lemma 23 that\n‖σK − σK̃‖ 2 2 ≤ ‖K− K̃‖2F ,\nwhere σK and σK̃ contain the singular values in a descending order. Since K̃ has a rank at most k, the k + 1 to n entries of σK̃ are zero. We split σK and σK̃ into vectors of length k and n− k:\nσK = [ σK,k σK,−k ] and σK̃ = [ σK̃,k 0 ] and thus\n‖σK,k − σK̃,k‖ 2 2 + ‖σK,−k‖22 ≤ ‖K− K̃‖2F . (34)\nSince ‖σK,−k‖22 = ‖K−Kk‖2F , it follows from (33) and (34) that\nE‖σK,k − σK̃,k‖ 2 2 ≤\nk l ‖σK,−k‖22.\nSince ‖x‖2 ≤ ‖x‖1 ≤ √ k‖x‖2 for any x ∈ Rk, we have that\nE ∥∥σK,k − σK̃,k∥∥1 ≤ k√l ∥∥σK,−k∥∥1.\nThen it follows from (14) and Line 6 in Algorithm 3 that E ∣∣δ̄ − δ̃∣∣ = E[ 1\nn− k ∣∣∣∣ k∑ i=1 σi(K)− k∑ i=1 σi(K̃) ∣∣∣∣ ]\n≤ 1 n− k E ∥∥σK,k − σK̃,k∥∥1 ≤ k√l 1n− k ∥∥σK,−k∥∥1 = k√l δ̄.\nLemma 23 Let A and B be n×n matrices and σA and σB contain the singular values in a descending order. Then we have that\n‖σA − σB‖22 ≤ ‖A−B‖2F .\nProof It is easy to show that\n‖A−B‖2F = tr(ATA) + tr(BTB)− 2tr(ATB)\n= n∑ i=1 σ2i (A) + n∑ i=1 σ2i (B)− 2tr(ATB). (35)\nWe also have\ntr(ATB) ≤ n∑ i=1 σi(A TB) ≤ n∑ i=1 σi(A)σi(B), (36)\nwhere the first inequality follows from (Horn and Johnson, Theorem 3.3.13) and the second inequality follows from (Horn and Johnson, Theorem 3.3.14). Combining (35) and (36) we have that\n‖A−B‖2F ≥ n∑ i=1 ( σ2i (A) + σ 2 i (B)− 2σi(A)σi(B) ) = ‖σA − σB‖22,\nby which the theorem follows."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2017,
    "abstractText" : "Many kernel methods suffer from high time and space complexities, so they are prohibitive<lb>in big-data applications. To tackle the computational challenge, the Nyström method<lb>has been extensively used to reduce time and space complexities by sacrificing some<lb>accuracy. The Nyström method speedups computation by constructing an approximation<lb>of the kernel matrix in question using only a few columns of the matrix. Recently, a<lb>variant of the Nyström method called the modified Nyström method has demonstrated<lb>significant improvement over the standard Nyström method in approximation accuracy,<lb>both theoretically and empirically.<lb>In this paper we provide theoretical analysis, efficient algorithms, and a simple but<lb>highly accurate extension for the modified Nyström method. First, we prove that the<lb>modified Nyström method is exact under certain conditions, and we establish a lower error<lb>bound for the modified Nyström method. Second, we develop two efficient algorithms to<lb>make the modified Nyström method efficient and practical. We devise a simple column<lb>selection algorithm with a provable error bound. With the selected columns at hand,<lb>we propose an algorithm that computes the modified Nyström approximation in lower<lb>time complexity than the approach in the previous work. Third, the extension which we<lb>call the SS-Nyström method has much stronger error bound than the modified Nyström<lb>method, especially when the spectrum of the kernel matrix decays slowly. Our proposed<lb>SS-Nyström can be computed nearly as efficiently as the modified Nyström method.<lb>Finally, experiments on real-world datasets demonstrate that the proposed column selection<lb>algorithm is both efficient and accurate and that the SS-Nyström method always leads to<lb>much higher kernel approximation accuracy than the standard/modified Nyström method.<lb>",
    "creator" : "LaTeX with hyperref package"
  }
}