{
  "name" : "1411.0860.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "CUR Algorithm for Partially Observed Matrices",
    "authors" : [ "Miao Xu", "Rong Jin", "Zhi-Hua Zhou" ],
    "emails" : [ "zhouzh@nju.edu.cn" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n41 1.\n08 60\nv1 [\ncs .L\nG ]\n4 N\nov 2\nCUR matrix decomposition computes the low rank approximation of a given matrix by using the actual rows and columns of the matrix. It has been a very useful tool for handling large matrices. One limitation with the existing algorithms for CUR matrix decomposition is that they need an access to the full matrix, a requirement that can be difficult to fulfill in many real world applications. In this work, we alleviate this limitation by developing a CUR decomposition algorithm for partially observed matrices. In particular, the proposed algorithm computes the low rank approximation of the target matrix based on (i) the randomly sampled rows and columns, and (ii) a subset of observed entries that are randomly sampled from the matrix. Our analysis shows the relative error bound, measured by spectral norm, for the proposed algorithm when the target matrix is of full rank. We also show that only O(nr ln r) observed entries are needed by the proposed algorithm to perfectly recover a rank r matrix of size n × n, which improves the sample complexity of the existing algorithms for matrix completion. Empirical studies on both synthetic and real-world datasets verify our theoretical claims and demonstrate the effectiveness of the proposed algorithm. Key words: Matrix approximation, CUR algorithm, matrix completion"
    }, {
      "heading" : "1. Introduction",
      "text" : "In many machine learning applications, it is convenient to represent data by matrix. Examples include user-item rating matrix in recommender system [SRJ04], gene expression matrix in bioin-\n∗Corresponding author. Email: zhouzh@nju.edu.cn\nPreprint submitted for review November 5, 2014\nformatics [MD08], kernel matrix in kernel learning [WS00], document-term matrix in document retrieval [MD08], and instance-label matrix in multi-label learning [GZR+10]. An effective approach for handling big matrices is to approximate them by their low rank counterparts which can be computed and stored efficiently. Various methods have been developed for low rank matrix approximation, including truncated singular value decomposition, matrix factorization [SRJ04], matrix regression [Kol11], column subset selection [BDMI11], the Nyström method [WS00].\nIn this work, we will focus on the CUR algorithm for low rank matrix approximation [MD09]. It is a randomized algorithm that computes the low rank approximation for a given rectangle matrix by randomly sampled columns and rows of the matrix. Compared to other low rank approximation algorithms, CUR is advantageous in that it has (i) an easy interpretation of the approximation result because the subspace is constructed by the actual columns and rows of the target matrix [MD09], and (ii) strong (near-optimal) theoretical guarantee [BXM10, DKM06, MD08, MD09, WZ12, WZ13]. The CUR matrix decomposition algorithm has been successfully applied to many domains, including bioinformatics [MD09], collaborative filtering [MTJ11], video background modeling [MTJ11], hyperspectral medical image analysis [MMD06], text data analysis [MD08]. In the past decade, many variants of the CUR algorithm have been developed [BXM10, DKM06, MTJ11, MMD06, MD08, MD09, WZ12, WZ13].\nDespite the success, one limitation with the existing CUR algorithms is that to effectively compute the low rank approximation, they require an access to the full matrix, a requirement that can be difficult to fulfill. For instance, in bioinformatics, it is usually too expensive to acquire the full expression information for hundreds of genes and thousands of individuals; in crowdsourcing, when both the number of workers and instances are large, it becomes impractical to request every worker to label all the instances in study ; in social network analysis, it is often the case that only part of the links between individuals can be accurately detected. In all the above cases, due to the physical or financial constraints, we only have a partial observation of the target matrix, making it difficult to apply the existing CUR algorithm.\nOne way to deal with the missing entries is to first compute an unbiased estimation of the target matrix based on the observed entries, and then apply the CUR algorithm to the estimated matrix. The main shortcoming of this simple method is that the unbiased estimate can be far from the target matrix when the number of observation is small, as we will show in the empirical study.\n2\nAnother approach is to recover the target matrix from the observed entries by using the matrix completion technique [CCS10, CR12]. Since most matrix completion algorithms are developed only for matrices of exactly low rank, they usually work poorly for matrices of full rank [EBN11]. We note that although an adaptive sampling approach is developed in [KS13] that does apply to matrices of full rank, they use a different sampling strategy, and their bound has a poor dependence on failure probability δ (i.e. O(1/δ)), which significantly limit its application when both rows and columns are randomly sampled.\nIn this work, we address the challenge by developing a novel CUR algorithm, named CUR+, for partially observed matrix. More specifically, the proposed algorithm computes a low rank approximation of matrix M based on (i) randomly sampled rows and columns from M , and (ii) randomly sampled entries fromM . Unlike most matrix completion algorithms that require solving an optimization problem involving trace norm regularization [Bac08, CCS10, JY09, MHT10, TS10], the proposed algorithm only needs to solve a standard regression problem and therefore is computationally efficient. In addition, we develop a relative error bound for the proposed CUR+ that works for both low-rank and full-rank matrices. In particular, to perfectly recover a rank-r matrix of size n × n, only O(nr ln r) observed entries are needed, significantly lower than O(nr ln2 n) for standard matrix completion theory [CR12, CT10, Gro11, KMO10, Rec11] and lower than O(nr3/2 ln r) for adaptive algorithm for matrix recovery [KS13]. We verify our theoretical claims by empirical studies of low rank matrix approximation.\nThe rest of the paper is organized as follows: Section 2 briefly reviews the related work on the CUR algorithms and matrix completion; Section 3 presents the proposed algorithm and its theoretical properties. Section 4 gives our empirical study. Section 5 concludes our work with future directions."
    }, {
      "heading" : "2. Related Work",
      "text" : "CUR matrix decomposition. CUR algorithms compute a low rank approximation of the target matrix using the actual rows and columns of the matrix [BXM10, DKM06, GZT97, GTZ97, MD08, MD09, Ste99, Tyr00, WZ12, WZ13]. More specially, let M ∈ Rn×m be the given matrix and r be the target rank for approximation. A classical CUR decomposition algorithm [MD08,\n3\nMD09] randomly samples d1 columns and d2 rows from M , according to their leverage scores, to form matrices C and R, respectively. The approximated matrix M̂ is then computed as M̂ = C(C†MR†)R, where † is the pseudoinverse. [DKM06] gives an additive error bound for the CUR decomposition, and a relative error bound, a significantly stronger result, is given in [MD08]. It stated that, with a high probability,\n‖M − M̂‖F ≤ (1 + ǫ)‖M −Mr‖F (1)\nwhere Mr is the best rank-r approximation to M , and ‖ · ‖F is the Frobenius norm of a matrix.\nVarious improved versions of CUR have been developed. [MTJ11] proposes a divide-and-conquer method to compute the CUR decomposition in parallel. [WZ13] proposes an adaptive CUR algorithm with much tighter error bound and much lower time complexity. In [DKM06], the authors suggest a simple uniform sampling of columns and rows for the CUR decomposition when the maximum statistical leverage scores, also referred to as incoherence measure [CR12, CT10, Rec11], is limited. In [MDMIW12], algorithms have been developed to efficiently compute the approximated values of statistical leverage scores without having to calculate the SVD decomposition of a large matrix. As we claimed in the introduction section, all the existing CUR algorithms require the knowledge of every entry in the target matrix and therefore cannot be applied directly to partially observed matrices. More complete list of related work on CUR can be found in [MD08, WZ13].\nCUR decomposition is closely related to column subset selection problem [BDMI11, DR10, MD08], which has been studied extensively in theoretical computer science and numerical analysis communities [MD08, MD09, WZ13]. It samples multiple columns from the target matrix M and use them as the basis to approximate M , and is often viewed as special case of the CUR algorithm. A special case of column subset selection is Nyström methods, which is usually used to approximate Positive Semi-Definitive (PSD) matrix in kernel learning [WS00]. A more complete list of related Nyström methods can be found in [JYM+13].\nMatrix Completion. The objective of matrix completion is to fill out the missing entries of a lowrank matrix based on the observed ones. In the standard matrix completion theory, when entries are missing uniformly at random, it requires O(nr ln2 n) observed entries to perfectly recover the target matrix under the incoherence condition [CR12, CT10, Gro11, KMO10, Rec11]. Multiple\n4\nimprovements have been developed for matrix completion, either to deal with nonuniform missing entries or to develop tighter bounds under more strict coherence conditions. [KS13] developed an adaptive sensing strategy for matrix completion that removes an lnn factor from the sample complexity. In [BJ14, CBSW14], the authors study matrix completion when observed entries are not sampled uniformly at random. [NW10, RT11] generalize matrix completion to matrix regression. In [XJZ13], the authors show that the sample complexity for perfect matrix recovery can be reduced dramatically with appropriate side information.\nAlthough it is appealing to directly combine the CUR algorithm with matrix completion to estimate a low rank approximation of a partially observed matrix, it may not work well in practice. One issue is that most matrix completion algorithms are developed for matrix of exactly low rank, significantly limiting its application to low rank matrix application. Although a few studies develop recovery bounds for matrix of full rank [EBN11, KS13], recovery errors usually deteriorate dramatically when applied to a matrix with a long tail spectrum. In addition, most matrix completion algorithms are computationally expensive, especially for large matrices, since they require, at each iteration of optimization, computing the SVD decomposition of the approximate matrix [Bac08, CCS10, JY09, MHT10, TS10]. In contrast, the proposed CUR algorithm scales to large matrix and works well for matrix of full rank."
    }, {
      "heading" : "3. CUR+ for Partially Observed Matrices",
      "text" : "We describe the proposed CUR+ algorithm, and then present the key theoretical results for it. Due to space limitation, we postpone all the detailed analysis to the supplementary document."
    }, {
      "heading" : "3.1. CUR+ Algorithm",
      "text" : "Let M ∈ Rn×m be the matrix to be approximated, where n ≥ m. To approximate M , we first sample uniformly at random d1 columns and d2 rows from M , denoted by A = (a1, . . . ,ad1) ∈ R n×d1 , and B = (b1, . . . ,bd2) ∈ Rm×d2 , respectively, where each ai ∈ Rn and bj ∈ Rm is the ith row and the jth column of M respectively. We noticed that uniform sampling of rows and columns may not be the best strategy as it does not take into account the difference between individual rows and columns. Other sampling strategies, such as sampling rows/columns based\n5\non their statistical leverage scores [MD08] and adaptive sampling [KS13, WZ12], can be more effective. We do not choose these sampling methods because they either require an access to the full matrix [MD08], introduce serious overhead in computation [WZ12], or result in significantly worse bound when matrix is of full rank [KS13]. Finally, for simplicity of discussion, we will assume d1 = d2 = d throughout the draft even though our algorithm and analysis can easily be extended to the case when d1 6= d2.\nLet r be the target rank for approximation, with r ≤ d. Û = (û1, . . . , ûr) ∈ Rn×r and V̂ = (v̂1, . . . , v̂r) ∈ Rm×r are the first r eigenvectors of AA⊤ and BB⊤, respectively. Besides A and B, we furthermore sample, uniformly at random, entries from matrix M . Let Ω include the indices of randomly sampled entries. Our goal is to estimate a low rank approximation of matrix M using A, B, and randomly sampled entries in Ω. To this end, we need to solve the following optimization\nmin Z∈Rr×r\n1 2 ‖RΩ(M)−RΩ(ÛZV̂ ⊤)‖2F (2)\nwhere given Ω, we define a linear operator RΩ(M) : Rn×m 7→ Rn×m as\n[RΩ(M)]i,j =    Mi,j (i, j) ∈ Ω 0 (i, j) /∈ Ω\nLet Z∗ be an optimal solution to (2). The estimated low rank approximation is given by M̂ = ÛZ∗V̂ ⊤. M̂ can also be expressed using standard C × U ×R formulation by solving a group of linear equations. We note that (2) is a standard regression problem and therefore can be solved efficiently using the standard regression method (e.g. accelerated gradient descent [Nes03]). We refer to the proposed algorithm as CUR+."
    }, {
      "heading" : "3.2. Guarantee for CUR+",
      "text" : "Before presenting the theoretical results, we first describe the notations that will be used throughout the analysis. Let σi, i = 1, . . . ,m be the singular values of M ranked in descending order, and let ui and vi be the corresponding left and right singular vectors. Define U = (u1, . . . ,um) and V = (v1, . . . ,vm). Given r ∈ [m], partitioning the SVD decomposition of M as\nM = UΣV ⊤ = r m− r [U1 U2]\n  Σ1\nΣ2\n    V ⊤ 1\nV ⊤2\n  (3)\n6\nLet ũi, i ∈ [n] be the ith row of U1 and ṽi, i ∈ [m] be the ith row of V1. The incoherence measure for U1 and V1 is defined as\nµ(r) = max ( max i∈[n] n r |ũi|2,max i∈[m] m r |ṽi|2 ) (4)\nSimilarly, we can have the incoherence measure for matrices Û and V̂ that include the first r eigenvectors of AA⊤ and BB⊤, respectively. Let û′i, i ∈ [n] be the ith row of Û and v̂′i, i ∈ [m] be the ith row of V̂ . Define the incoherence measure for Û and V̂ as\nµ̂(r) = max ( max i∈[n] n r |û′i|2,max i∈[m] m r |v̂′i|2 ) (5)\nDefine projection operators PU = UU ⊤, PV = V V ⊤, P Û = Û Û⊤, and P V̂ = V̂ V̂ ⊤. We will use ‖ · ‖2 and ‖ · ‖F respectively for the spectral norm and Frobenius norm of a matrix.\nWe first present the theoretical guarantee for the CUR+ algorithm when the rank of the target matrix M is no greater than r.\nTheorem 1 (Low-Rank Matrix Approximation) Assume rank(M) ≤ r, d ≥ 7µ(r)r(t+ln r), and |Ω| ≥ 7µ2(r)r2(t+2 ln r). Then, with a probability at least 1− 5e−t, we have M = M̂ , where M̂ is a low rank approximation estimated by the CUR+ algorithm.\nRemark. Theorem 1 shows that a rank-r matrix can be perfectly recovered from 2dn + |Ω| = O(nr ln r) observed entries if we set t = Ω(ln r). In Table 1, we compare the sample complexity of the CUR+ algorithm with the sample complexity of the other matrix completion algorithms. We observe that our result significantly improves the sample complexity from previous work. We should note that unlike [KS13] where the incoherence measure is only assumed for column vectors, we assume a small incoherence measure for both row and column vectors here. It is this\n7\nstronger assumption that allows us to sample both rows and columns, leading to the improvement in the sample complexity from O(nr3/2 ln r) in [KS13] to O(nr ln r).\nWe now consider a more general case where matrix M is of full rank. Theorem 7 bounds the difference between M and M̂ , measured in spectral norm,\nTheorem 2 Let r ≤ m be an integer that is no larger than m. Assume (i) d ≥ 7µ(r)r(t+ ln r) , and (ii) |Ω| ≥ 7µ̂2(r)r2(t+ 2 ln r). Then with a probability at least 1− 3e−t\n‖M − M̂‖22 ≤ 8σ2r+1 (1 + 2mn) ( 1 + m+ n\nd\n) .\nAs indicated by Theorem 7, when both µ(r) and µ̂(r), the incoherence measure for the first r singular/eigen vectors of M and the sampled columns/rows, are small, we have\n‖M − M̂‖2 ≤ O (√ mn √ n\nd ‖M −Mr‖2\n)\nprovided that d ≥ O(r ln r) and |Ω| ≥ O(r2 ln r).\nOne limitation with Theorem 7 is that µ̂(r) is a random variable depending on the sampled columns and rows. Since µ̂(r) can be as high as n/r, |Ω|, the number of observed entries required by Theorem 7, can be as large as O(n2), making it practically meaningless. Below, we develop a result that explicitly bounds µ̂ with a high probability. Using the high probability bound for µ̂, we are able to show that under appropriate conditions, we need at most O(n2/d2) observed entries in order to establish a relative error bound for ‖M − M̂‖.\nTo make our analysis simple, we focus on the case whenM is of full rank but with skewed singular value distribution. In particular, we assume σr ≥ √ 2σr+1. In order to effectively capture the skewed singular value distribution, we introduce the concept of numerical rank r(M,η) [GL96] with respect to non-negative constant η > 0\nr(M,η) = m∑\ni=1\nσ2i σ2i +mnη\nNote that when η = 0, the numerical rank is equivalent to the true rank of the matrix. The larger η is , the smaller it compared to the true rank. In the following analysis, we will replace rank r with numerical rank r(M,η).\n8\nWe furthermore generalize the definition of incoherence measure to matrix with numerical rank, that is, we further define incoherence measure µ(η) as\nµ(η) = max ( max 1≤i≤m\nm\nr(M,η) |Vi,∗Σ|2, max 1≤i≤n\nn\nr(M,η) |Ui,∗Σ|2\n) (6)\nIt is easy to verify that µ(η) ≥ 1. Compared to the standard incoherence measure defined in (4), the key difference is that (6) introduces singular values Σ into the definition of incoherence measure, making it appropriate for matrix of full rank.\nThe following two lemmas relate rµ(r) and rµ̂(r), respectively, with r(M,η)µ(η),\nLemma 1 If we choose η = σ2r/mn, we have\nrµ(r) ≤ 2r(M,η)µ(η)\nLemma 2 Assume that d ≥ 16(µ(η)r(M,η) + 1)(t + lnn), and σr ≥ √ 2σr+1. Set η = σ 2 r/mn. With a probability 1− 4e−t, we have\nrµ̂(r) ≤ 2r(M,η)µ(η) + 18nδ2/r where δ2 = 4 d (µ(η)r(M,η) + 1)(t+ lnn)\nUsing Theorem 7, Lemma 1 and 2, we have the result for full-rank matrix with skewed singular value distribution,\nTheorem 3 (Full Rank Matrix Approximation) Assume d ≥ 16(µ(η)r(M,η) + 1)(t+ lnn) and σr ≥ √ 2σr+1. Set η = σ 2 r/mn. We have, with a probability 1− 7e−t,\n‖M − M̂‖22 ≤ 8σ2r+1 (1 + 2mn) ( 1 + m+ n\nd\n) .\nif |Ω| ≥ 7 ( 2µ(η)r(M,η) + 72 n\nd (µ(η)r(M,η) + 1)(t+ lnn)\n)2 (t+ 2 ln r) = O ( n2\nd2\n)\nAs indicated by Theorem 3, we will have a bound similar to that of Theorem 7 if |Ω| ≥ O(n2/d2). The key difference between Theorem 7 and 3 is that in Theorem 7, the requirement for |Ω| depends on µ̂(r), a random variable depending on the sampled rows and columns. In contrast, in Theorem 3, we remove µ̂ and bound |Ω| directly. We finally note that the result |Ω| ≥ O(n2/d2) requires a large number of sampled entries for accurately estimating the low rank approximation of the target matrix. This is mostly due to the potentially loose bound for µ̂. It remains an open question whether it is possible to reduce the number of observed entries for CUR-type low rank approximation.\n9"
    }, {
      "heading" : "4. Experiments",
      "text" : "We first verify the theoretical result in Theorem 1, i.e. the dependence of sample complexity on r and n, using synthetic data. We then evaluate the performance of the proposed CUR+ algorithm by comparing it to the state-of-the-art algorithms for low rank matrix approximation. We implement the proposed algorithm using Matlab, and all the experiments were run on a Linux server with CPU 2.53GHz and 48GB memory."
    }, {
      "heading" : "4.1. Experiment (I): Verifying the Dependence on r and n",
      "text" : "We will verify the sample complexity result in Theorem 1, i.e. d ≥ O(r ln r) and |Ω| ≥ O(r2 ln r). We note both the requirements on d and |Ω| are independent from matrix size.\nSettings. Here we study square matrices of different sizes and ranks, with n varied in {1, 000; 2, 000; 4, 000; 8, 000; 10, 000}, and r varied in {10, 20, 30, 50}. For each special n and r, we search for the smallest d and |Ω| that can lead to almost perfect recovery of the target matrix (i.e. ‖M − M̂‖F /‖M‖F ≤ 2 × 10−4) in all 10 independent trials. To create the rank-r matrix M ∈ Rn×n, we first randomly generate matrix ML ∈ Rn×r and MR ∈ Rr×n with each entry of ML and MR drawn independently at random from N (0, 1), and M is given by M = ML×MR. To create A and B, we sample uniformly at random d rows and columns. We further sample |Ω| entries from M to be partially observed. Under this construction scheme, the difference between the incoherence µ(r) for different sized matrices are relatively small (from minimum 1.4127 to maximum 2.4885). Although we will plot d and |Ω|’s dependence on µ(r), we will ignore their impact in discussion of the results.\n10\nResults. The dependence of minimal d on r and n is given in Figure 1(a) and (b), where (a) plots d against r ln r and (b) shows d versus r2 ln r. We can see clearly that d has a linear dependence on r ln r. We also observed from Figure 1(a) that d is almost independent from n, the matrix size. Figure 1(c) and (d) plot the |Ω|, the minimum number of observed entries, against r ln r and r2 ln r. The result in Figure 1 (d) confirms our theoretical finding, i.e. |Ω| ∝ r2 ln r."
    }, {
      "heading" : "4.2. Experiment(II): Comparison with Baseline Methods for Low Rank Approximation",
      "text" : "We evaluate the performance of the proposed CUR+ algorithm on several benchmark data sets that have been used in the recent studies of the CUR matrix decomposition algorithm, including Enron emails (39, 861×28, 102), Dexter (20, 000×2, 600), Farm Ads (54, 877×4, 143) and Gisette (13, 500 × 5, 000), where each row of the matrix corresponds to a document and each column corresponds to a term/word. Detailed information of these data sets can be found in [WZ13]. All four matrices are of full rank and have skewed singular value distribution, as shown in Figure 2\nBaselines. Since both the rows/columns and entries observed in the proposed algorithm are sampled uniformly at random, we only compare our approach to the standard CUR algorithm using uniformly sampled rows and columns. Although the adaptive sampling based approaches [KS13] usually yield lower errors than the standard CUR algorithm, they do not choose observed entries randomly and therefore are not included in the comparison. Let C be a set of d1 sampled columns and R be the set of d2 sampled rows. The low rank approximation by the CUR algorithm is given by M̂ = CZR, where Z ∈ Rd1×d2 . Two methods are adopted to estimate Z. We first estimated Z by Z = C†MR†. Since this estimation requires an access to the full matrix, we refer to it as CUR-F. In the second method, we first construct an unbiased estimator Me by using\n11\nthe randomly observed entries in Ω, and then estimate matrix Z by Z = C†MeR †. Here, the unbiased estimation Me is given by\n[Me]i,j =    mn |Ω|Mi,j (i, j) ∈ Ω\n0 (i, j) /∈ Ω\nWe call this algorithm CUR-E. Evidently, CUR-F is expected to work better than our proposal and will provide a lower bound for the CUR algorithm for partially observed matrices.\nSettings. To make our result comparable to the previous studies, we adapted the same experiment strategy as in [WZ12, WZ13]. More specially, for each data set, we set d1 = αr and d2 = αd1, with rank r varied in the range of (10, 20, 50) and α varied from 1 to 5. To create partial observations, we randomly sample |Ω| = Ω0 = nmr2/nnz(M) entries from the target matrix M , where nnz(M) is the number of non-zero entries of M . We measure the performance of low rank matrix approximation by the relative spectral-norm difference ℓs = ‖M − M̂‖/‖M −Mr‖ which has solid theoretical guarantee according to Theorem 3. We noticed that most previous work report their results in the form of relative Frobenius norm, thus we will also show the results compared to the state-of-the-art algorithms for low rank matrix approximation measured by the relative Frobenius norm ℓF = ‖M − M̂‖F /‖M − Mr‖F . Finally, we follow the experimental protocol specified in [WZ12] by repeating every experiment 10 times and reporting the mean value.\nResults. Figure 3 shows the results of low rank matrix approximation for r = 10, 20, 50. We observe that the CUR+ works significantly better than the CUR-E method, and yields a similar performance as the CUR-F that has an access to the full target matrix M .\nWe observe that with larger α (i.e. increasing numbers of rows and columns), the approximation errors for CUR+ and CUR-F decrease while, to our surprise, the error of CUR-E increases significantly. This counter-intuitive result can be explained by the fact that CUR-E estimates matrix Z based on the observed entries in Ω. Since the size of Z is d1 × d2, which increases at the rate of α3. But on the other hand, |Ω|, the number of observed entries based on which Z is estimated, remains unchanged. As a result, with increasing values of α, it becomes more and more difficult to come up with an accurate estimation of Z and consequentially a worse and worse approximation of M . We have verified this explanation in Fig 4 by simultaneously increasing the\n12\nnumber of observed entries in Ω and observing that the approximation error of CUR-E decreases with increasing α, although with perturbation. It is also to our surprise that when r is increasing, the relative spectral-norm difference ℓs is increasing. This may due to the fact that we normalize the spectral-norm, dividing it by ‖M −Mr‖ which decreases fast. And we observe that ‖M −M̂‖ decreases when r becomes larger and larger.\nIn the second experiment, we fix the number of sampled rows and columns and vary the number of observed entries from Ω0 to 5Ω0. Figure 5 shows the results for r = 10, 20 and 50. Again, we found that CUR+ yields similar performance as CUR-F, and performs significantly better than CUR-E, although the gap between CUR+ and CUR-E does decline with increasing number of observed entries. It is also to our surprise that for datasets Enron and Farm Ads, the approximation error of CUR+ remains almost unchanged with increasing number of observed entries. We plan to examine this unusual phenomenon in the future.\n13\n14\nResults Measured by Frobenius Norm. Similar results on relative Frobenius norm are also reported. The results are plotted in Figure 6 when |Ω| is fixed and we vary α, and in Figure 7 when α is fixed and we vary |Ω|. We can see that similar as the results measured by spectral norm, the proposed CUR+ works significantly better than the CUR-E method, and yields a similar performance as the CUR-F algorithm that has an access to the full target matrix M ."
    }, {
      "heading" : "5. Conclusion",
      "text" : "In this paper, we propose a CUR-style low rank approximation algorithm for partially observed matrix. Our analysis shows that the proposed algorithm only needs O(nr ln r) number of observed entries to perfectly recover a low-rank matrix, improving the results of the existing algorithms for matrix completion (of course under a slightly stronger condition). We also show the the spectral error bound for the proposed algorithm when the target matrix is of full rank. Empirical studies on both synthetic data and real datasets verify our theoretical claims and furthermore,\n15\ndemonstrate that the proposed algorithm is more effective in handling partially observed matrix than the existing CUR algorithms. Since adaptive sampling has shown promising results for low rank matrix approximation [KS13], in the future, we plan to combine the proposed algorithm with adaptive sampling strategy to further reduce the error bound. We also plan to exploit the recent studies on matrix approximation/completion with non-uniform sampling and extend the CUR algorithm to the case when observed entries are non-uniform sampled."
    }, {
      "heading" : "A. Appendix",
      "text" : "We will first give the supporting theorems we will use in the analysis. Then we will give the detailed proof of the three theorems in the paper."
    }, {
      "heading" : "A.1. Supporting Theorems",
      "text" : "The following results are used throughout the analysis.\n16\nTheorem 4 (Theorem 9.1 in [HMT11]) Let M be an n×m matrix with singular value decomposition M = UΣV ⊤. There is a fixed r > 0. Choose a test matrix Ψ ∈ Rm×d and construct sample matrix Y = MΨ. Partition M as in (7)\nM = UΣV ⊤ = r m− r [U1 U2]\n  Σ1\nΣ2\n    V ⊤ 1\nV ⊤2\n  (7)\nand define Ψ1 = V ⊤ 1 Ψ and Ψ2 = V ⊤ 2 Ψ. Assuming Ψ1 has full row rank, the approximation error satisfies\n‖M − PY (M)‖22 ≤ ‖Σ2‖22 + ‖Σ2Ψ2Ψ†1‖22\nwhere PY (M) projects column vectors in M in the subspace spanned by the column vectors in Y and † denotes the pseudoinverse.\nTheorem 5 (Derived From Theorem 2.2 of [Tro11]) Let X be a finite set of PSD matrices with dimension k (means the size of the square matrix is k × k). λmax(·) and λmin(·) calculate the maximum and minimum eigen value respectively.\nSuppose that\nmax X∈X\nλmax(X) ≤ B.\nSample {X1, . . . ,Xℓ} uniformly at random from X without replacement. Compute\nµmax = ℓλmax(E[X1]), µmin = ℓλmin(E[X1])"
    }, {
      "heading" : "Then",
      "text" : "Pr { λmax ( ℓ∑\ni=1\nXi ) ≥ (1 + ρ)µmax } ≤ k exp −µmax\nB [(1 + ρ) ln(1 + ρ)− ρ] for ρ ∈ [0, 1)\nPr { λmin ( ℓ∑\ni=1\nXi ) ≤ (1− ρ)µmin } ≤ k exp −µmin\nB [(1− ρ) ln(1− ρ) + ρ] for ρ ≥ 0\nTheorem 6 Let A = S⊤HS and Ã = S⊤H̃S be two symmetric matrices of size n × n. Let λi, i ∈ [n] and λ̃i, i ∈ [n] be the eigenvalues of A and Ã, respectively, ranked in descending order. Let UA, ŨA ∈ Rn×r include the first r eigenvectors of A and Ã, respectively. Let ‖ · ‖ be any\n17\ninvariant norm. Define\n∆λ = min\n(√ 2 ( 1− λr+1\nλr ) , 1√ 2 ) ≤ 1√ 2\n∆H = ‖H−1‖‖H − H̃‖√ 1− ‖H−1‖‖H − H̃‖\nIf ∆λ ≥ ∆H/2, we have\n‖ sinΘ(UA, ŨA)‖ ≤ ∆H\n∆λ −∆H/2\n( 1 +\n∆H∆λ 16\n)\nwhere\nΘ(X, X̃) = arccos((X∗X)−1/2X∗X̃(X̃∗X̃)−1X̃∗X(X∗X)−1/2)1/2\ndefines the angle matrix between X and X̃.\nNote that the above Theorem 6 follows directly from Theorem 4.4 and discussion in Section 5 from [Li99]."
    }, {
      "heading" : "A.2. Proof of Theorem 2",
      "text" : "We will first provide the key result for our analysis, and then bound each component of the key result, that is, first, we will show that ‖M − PÛMPV̂ ‖22 is small; then, we will bound the strong convexity of the objective function.\nThe following theorem shows that the difference between M and M̂ is well bounded if both ‖M − P Û MP V̂ ‖22 and the strong convexity of Eq.2 are well bounded,\nTheorem 7 Assume (i) ‖M − P Û MP V̂ ‖22 ≤ ∆, and (ii) the strong convexity of the objective function is no less than |Ω|γ. Then\n‖M − M̂‖22 ≤ 2 ( ∆+ ∆\nγ\n) .\nwhere strongly convexity is defined as,\n18\nDefinition 8 A function f : D → R is ξ-strongly convex w.r.t. norm ‖ · ‖ if f is everywhere differentiable and\nf(w) ≥ f(w′) +∇f(w′)(w − w′) + ξ 2 ‖w − w′‖2."
    }, {
      "heading" : "Then ξ is the strongly convexity of f .",
      "text" : "Proof: Set Z = Û⊤MV̂ . Since ‖M − PÛMPV̂ ‖22 ≤ ∆, we have\n‖M − ÛZV̂ ⊤‖22 ≤ ∆,\nimplying\n‖RΩ(M)−RΩ(ÛZV̂ ⊤)‖2F ≤ ∆\nLet Z∗ be the optimal solution to Eq.2. Using the strongly convexity of Eq.2, we have\n1 2 γ|Ω|‖Z − Z∗‖2F ≤ 1 2 |Ω|∆,\ni.e. ‖Z − Z∗‖2F ≤ ∆/(γ).\nThis is because f(Z) = 12‖RΩ(M) − RΩ(ÛZV̂ ⊤)‖2F , such that ∇f(Z) = ÛT [RΩ(ÛZV̂ T ) − RΩ(M)]V̂ , and ∇f(Z∗) = 0\n|Ω|γ 2 ‖Z − Z∗‖2F ≤ 1 2 ‖RΩ(M)−RΩ(ÛZV̂ ⊤)‖2F − 1 2 ‖RΩ(M)−RΩ(ÛZ∗V̂ ⊤)‖2F\n≤ 1 2 ‖RΩ(M)−RΩ(ÛZV̂ ⊤)‖2F ≤ |Ω|∆ 2\nWe thus have,\n‖M − M̂‖22 ≤ 2‖M − PÛMPV̂ ‖ 2 2 + 2‖PÛMPV̂ − ÛZ∗V̂ ⊤‖22 ≤ 2‖M − PÛMPV̂ ‖ 2 2 + 2‖PÛMPV̂ − ÛZ∗V̂\n⊤‖2F ≤ 2‖M − PÛMPV̂ ‖ 2 2 + 2‖Z − Z∗‖2F ≤ 2 ( ∆+ ∆\nγ|\n)\nIn order to bound ∆, we need the following theorem,\n19\nTheorem 9 With a probability 1− 2e−t, we have,\n‖M −MP V̂ ‖22 ≤ σ2r+1\n( 1 + 2 m\nd\n)\nand\n‖M − PÛM‖2 ≤ σ 2 r+1\n( 1 + 2 n\nd\n)\nprovided that d ≥ 7µ(r)r(t+ ln r).\nProof: Let i1, . . . , id are the d selected columns. Define Ψ = (ei1 , . . . , eid) ∈ Rm×d, where ei is the ith canonical basis. Such that we have A = M ×Ψ, that is, A is composed of the d selected columns of M . To utilize Theorem 4, we need to bound the minimum eigenvalue of Ψ1Ψ ⊤ 1 , where Ψ1 = V T 1 Ψ ∈ Rr×d is full rank. We have\nΨ1Ψ ⊤ 1 = V ⊤ 1 ΨΨ ⊤V1\nLet ṽ⊤i , i ∈ [d] be the ith row vector of V1. We have,\nΨ1Ψ ⊤ 1 =\nd∑\nj=1\nṽij ṽ ⊤ ij\nIt is straightforward to show that\nE [ Ψ1Ψ ⊤ 1 ] = d\nm Ir\nand\nE [ ṽij ṽ ⊤ ij ] = 1\nm Ir.\nTo bound the minimum eigenvalue of Ψ1Ψ ⊤ 1 , we need Theorem 5, where we first need to bound the maximum eigen value of ṽij ṽ ⊤ ij , which is a rank-1 matrix, whose eigen value\nmax 1≤i≤m λmax(ṽij ṽ ⊤ ij ) = max1≤i≤m\n|ṽi|2 ≤ µ(r) r\nm ,\nand\nλmax(E [ ṽij ṽ ⊤ ij ] ) = λmin(E [ ṽij ṽ ⊤ ij ] ) = 1\nm\n20\nThus, we have,\nPr { λmin(Ψ1Ψ ⊤ 1 ) ≤ (1− δ) d\nm\n} ≤ r exp −d/m\nrµ(r)/m [(1− ρ) ln(1− ρ) + ρ]\n= r exp −d\nrµ(r) [(1− ρ) ln(1− ρ) + ρ]\nBy setting δ = 1/2, we have,\nPr { λmin(Ψ1Ψ ⊤ 1 ) ≤ d\n2m\n} ≤ r exp −d\n7rµ(r) = re−d/[7µ(r)r]\nwhere with d ≥ 7µ(r)r(t+ ln r), we have r exp−d/[7µ(r)r] ≤ e−t, that is,\nPr { λmin(Ψ1Ψ ⊤ 1 ) ≥ d\n2m\n} ≥ 1− e−t\nWith\nλmin(Ψ1Ψ ⊤ 1 ) ≥\nd\n2m\naccording to Theorem 4, we have\n‖M −MPV̂ ‖ 2 2 ≤ ‖Σ2‖22 + ‖Σ2Ψ2Ψ†1‖22\n≤ σ2r+1 + ∥∥∥Σ2Ψ2Ψ†1 ∥∥∥ 2\n2\n≤ σ2r+1 + ‖Ψ†1‖22‖Σ2Ψ2‖22 ≤ σ2r+1 + 2m\nd ‖Σ2Ψ2‖22\n≤ σ2r+1 + 2m\nd ‖Σ2‖22‖Ψ2‖22\n≤ σ2r+1 + 2m\nd σ2r+1\n≤ σ2r+1 ( 1 + 2m\nd\n)\n• The 1st inequality is according to Theorem 4.\n• The 3rd inequality is because the two facts, ‖M1M2‖2 ≤ ‖M1‖2 × ‖M2‖2 • The 4th inequality is becuase ‖Ψ†1‖2 = 1/σmin(Ψ1) = √ 1/λmin(Ψ1Ψ⊤1 ) ≤ √ 2m/d\n• The 6th inequality is because ‖Σ2‖2 = σr+1 and ‖Ψ2‖2 ≤ ‖V2‖2‖Ψ‖2 = 1\n21\nWe then bound ∆,\nTheorem 10 With a probability 1− 2e−t, we have,\n∆ := ‖M − PÛMPV̂ ‖ 2 2 ≤ 4σ2r+1\n( 1 + m+ n\nd\n)\nif d ≥ 7µ(r)r(t+ ln r).\nProof: Using Theorem 9, we have, with a probability 1− 2e−t\n‖M − P Û MP V̂ ‖22 ≤ 2‖M −MPV̂ ‖ 2 2 + 2‖(M − PÛM)PV̂ ‖ 2 2\n≤ 2‖M −MPV̂ ‖ 2 2 + 2‖M − PÛM‖ 2 2\n≤ 4σ2r+1 ( 1 + n+m\nd\n)\nWe will then bound the strong convexity of the objective function,\nTheorem 11 With a probability 1−e−t, we have that γ|Ω|, the strongly convexity for the objective function in (2), is bounded from below by |Ω|/[2mn] (that is, γ ≥ 1/(2mn)), provided that\n|Ω| ≥ 7µ̂2(r)r2(t+ 2 ln r)\nProof: To bound the strong convexity, we could instead bound the smallest eigen value of the Hessian matrix. The Hessian matrix is an r2 × r2 matrix. Assuming the second-order derivative of the (i1, j1)th and (i2, j2)th entry of Z is the (r(i1−1)+ j1, r(i2−1)+ j2)th entry of the Hessian matrix, the Hessian matrix could be written as,\nH = ∑\n(i,j)∈Ω\n[vec(ũ⊤i ṽj)][vec(ũ ⊤ i ṽj)] T\nTo bound the minimum eigenvalue of H, we will use Lemma 5. Thus first we need to bound\nmax i,j\nλmax([vec(ũ ⊤ i ṽj)][vec(ũ ⊤ i ṽj)] T ) = max i,j\n|vec(ũ⊤i ṽj)|2 ≤ max ‖ũ⊤i ṽj‖2F ≤ µ̂2(r)r2\nmn\n22\nand\nλmin ( E([vec(ũ⊤i ṽj)][vec(ũ ⊤ i ṽj)] T ) ) = 1\nmn λmin\n( (U ⊗ V )T × (U ⊗ V ) )\n= 1\nmn\nwhere ⊗ is the Kronecker product.\nBased on Theorem 5, we have\nPr { λmin(H) ≤\n|Ω| 2mn\n} ≤ r2e −|Ω| 7µ̂2(r)r2\nHence, with a probability 1− e−t, we have\nλmin(H) ≥ |Ω| 2mn\nprovided that\n|Ω| ≥ 7µ̂2(r)r2(t+ 2 ln r)\nTheorem 2 can be easily proved combining Theorems 7, 10 and 11."
    }, {
      "heading" : "A.3. Proof of Theorem 1",
      "text" : "The following theorem allows us to replace µ̂(r) in Theorem 11 with µ(r) when the rank of M is less than or equal to r.\nTheorem 12 With a probability 1− 2e−t, we have µ̂(r) = µ(r), if d ≥ 7µ(r)r(t+ ln r).\nProof: According to Theorem 10, with a probability 1 − 2e−t, we have M = P Û MP V̂ , provided that d ≥ 7µ(r)r(t+ ln r). Hence PU1 = PÛ and PV1 = PV̂ , which directly implies that µ(r) = µ̂(r).\nTheorem 1 can be proved directly from Theorem 2 and Theorem 12.\n23"
    }, {
      "heading" : "A.4. Proof of Theorem 3",
      "text" : "Define\nHA = ηI + 1\nmn MM⊤, ĤA = ηI +\n1\ndn AA⊤\nand\nHB = ηI + 1\nmn M⊤M, ĤB = ηI +\n1\ndm BB⊤\nWe can have the first r eigen vector of would be HA, because\nHA = ηI + 1\nmn MM⊤\n= ηUUT + 1\nmn U(ΣΣT )UT\n= U(ηI + 1\nmn ΣΣT )UT\nand\nH −1/2 A = Udiag(\n√ mn\nσ21 +mnη , . . . ,\n√ mn\nσ2m +mnη ) =\n√ mnUTUT\nwhere\nT = diag(\n√ 1\nσ21 +mnη , . . . ,\n√ 1\nσ2m +mnη )"
    }, {
      "heading" : "A.4.1. Proof of Lemma 1",
      "text" : "Proof: Just consider the maximization of the norm of rows of U , then we will have\nµ(η) = max i=1,...,n\nm∑\nj=1\nn\nr(M,η) σ2j σ2j +mnη U2i,j\n= max i=1,...,n\nn r\nm∑\nj=1\nr σ2j\nr(M,η)(σ2j +mnη) U2i,j\n≥ max i=1,...,n\nn r\nm∑\nj=1\nr a\nr U2i,j\n= a max i=1,...,n\nn r\nm∑\nj=1\nU2i,j\n= aµ(r)\n24\nwhen η = σ2r/mn, then a ≤ r/2r(M,η), then\nµ(r) ≤ 1 a µ(δ) ≤ 2r(M,η) r µ(η)\ncompletes our proof."
    }, {
      "heading" : "A.4.2. Proof of Lemma 2",
      "text" : "To this end, we need the following theorem.\nTheorem 13 With a probability 1− 4e−t, we have\n1− δ ≤ λk(H−1/2A ĤAH −1/2 A ) ≤ 1 + δ, 1− δ ≤ λk(H −1/2 B ĤBH −1/2 B ) ≤ 1 + δ, ∀k ∈ [n]\nif\nd ≥ 4 δ2 (µ(η)r(M,η) + 1)(t+ lnn)\nProof: It is sufficient to show the result for ĤA.\nDefine\nX = { Mi = (H −1/2 A ) T ( 1\nn M∗,iM\n⊤ ∗,i + ηI\n) H\n−1/2 A , i = 1, . . . ,m\n}\nNote that if ai is the jth column of matrix M , then,\nM∗,i = UΣ(Vi,∗) ⊤\nThus we have\nMi = mnUTU ⊤(\n1 n UΣV ⊤i,∗Vi,∗ΣU ⊤ + ηI)UTU⊤\n= U ( mTΣV ⊤i,∗Vi,∗ΣT +mnηT 2 ) U⊤\n25\nIn this way\nλmax(Mi) ≤ λmax(mUTΣV ⊤i,∗Vi,∗ΣTU⊤) + λmax(mnηUT 2U⊤)\n= m|UTΣV ⊤i,∗|22 + mnη\nσ2m +mnη\n≤ µ(η)r(M,η) + 1\n(this is because |Ax|22 ≤ ‖A‖22|x|22 ≤ ‖A‖2F |x|22) and\nλmax(E[Mi]) = λmax(U ( TΣV ⊤V ΣT +mnηT 2 ) U⊤)\n= λmax(U ( TΣΣT +mnηT 2 ) U⊤) = σ21\nmnη + σ21 +\nmnη\nmnη + σ21 = 1\nSo\nµmax = dλ1(E[Mi]) = d\nwe have (using Lemma 5),\nPr { λmax ( H −1/2 A ĤAH −1/2 A ) ≥ 1 + δ } ≤ n exp ( − d µ(η)r(M,η) + 1 [(1 + δ) ln(1 + δ)− δ] )\nUsing the fact that (at 0 they are the same, but the left increase faster than the right)\n(1 + δ) ln(1 + δ) ≥ δ + 1 4 δ2,∀δ ∈ [0, 1],\nwe have\nPr { λmax ( H −1/2 A ĤAH −1/2 A ) ≥ 1 + δ } ≤ n exp ( − dδ 2\n4(µr(M,η) + 1)\n)\nWe have the result by setting d ≥ 4(µ(η)r(M,η)+1)(ln n+ t)/δ2. Similarly, for the lower bound, we have (using Lemma 5)\nPr { λmin ( H −1/2 A ĤAH −1/2 A ) ≤ 1− δ } ≤ n exp ( − d µ(η)r(M,η) + 1 [(1− δ) ln(1− δ) + δ] )\nUsing the fact that (by Taylor Expansion of ln(1− δ))\n(1− δ) ln(1− δ) ≥ −δ + δ 2\n2\n26\nWe have the result by setting d ≥ 2(µ(η)r(M,η) + 1)(ln n+ t)/δ2.\nUsing Theorem 13, we will prove Lemma 2, Proof: To utilize Theorem 6, we rewrite HA and ĤA, as\nHA = H 1/2 A IHA, ĤA = H 1/2 A DH 1/2 A\nwhere D = H −1/2 A ĤAH −1/2 A . According to Theorem 13, with a probability 1 − 2e−t, we have ‖D − I‖2 ≤ δ, provided that\nd = 4\nδ2 (µ(η)r(M,η) + 1)(t+ lnn)\nWe then compute ∆H defined in Theorem 6 as\n∆H ≤ δ√ 1− δ\nBecause d ≥ 16(µ(η)r(M,η) + 1)(t+ lnn), we have 4\nδ2 (µ(η)r(M,η) + 1)(t+ lnn) ≥ 16(µ(η)r(M,η) + 1)(t+ lnn)\nthat is δ ≤ 1/2. Because σr ≥ √ 2σr+1, we have 1/2 ≤ 1 − σ2r+1/σ2r . Since δ ≤ 1/2 ≤ 1 − σ2r+1/σ2r , we have ∆H ≤ √ 2δ.\nThen according to Theorem 6, we have,\n‖ sinΘ(U1, Û)‖2 ≤ √ 2δ\n∆λ − √ 2δ/2 (1 + √ 2δ∆λ 16 )\n≤ √ 2δ\n∆λ − √ 2δ/2\n(1 + 1\n32 ) < 3\n√ 2δ\nSimilarly, we have,\n‖ sinΘ(V1, V̂ )‖2 < 3 √ 2δ\nThus, with a probability 1− 4e−t, we have\nµ̂(r) ≤ 2r(M,η) r µ(η) + n r ‖ sinΘ(V1, V̂ )‖22 ≤ 2r(M,η) r µ(η) +\n18nδ2\nr\nTheorem 3 can be proved by combining the results of Theorems 7, 11, Lemma 1 and Lemma 2.\n27"
    } ],
    "references" : [ {
      "title" : "Consistency of trace norm minimization",
      "author" : [ "F. Bach" ],
      "venue" : "JMLR, 9:1019–1048",
      "citeRegEx" : "Bac08",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Near optimal column-based matrix reconstruction",
      "author" : [ "C. Boutsidis", "P. Drineas", "M. Magdon-Ismail" ],
      "venue" : "FOCS",
      "citeRegEx" : "BDMI11",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Universal matrix completion",
      "author" : [ "S. Bhojanapalli", "P. Jain" ],
      "venue" : "ICML",
      "citeRegEx" : "BJ14",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Cur from a sparse optimization viewpoint",
      "author" : [ "J. Bien", "Y. Xu", "M. Mahoney" ],
      "venue" : "NIPS",
      "citeRegEx" : "BXM10",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Coherent matrix completion",
      "author" : [ "Y. Chen", "S. Bhojanapalli", "S. Sanghavi", "R. Ward" ],
      "venue" : "ICML",
      "citeRegEx" : "CBSW14",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "A singular value thresholding algorithm for matrix completion",
      "author" : [ "J.-F. Cai", "E. Candès", "Z. Shen" ],
      "venue" : "SIAM J. Opti., 20(4):1956–1982",
      "citeRegEx" : "CCS10",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Exact matrix completion via convex optimization",
      "author" : [ "E. Candès", "B. Recht" ],
      "venue" : "Commun. ACM",
      "citeRegEx" : "CR12",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "The power of convex relaxation: near-optimal matrix completion",
      "author" : [ "E. Candès", "T. Tao" ],
      "venue" : "TIT",
      "citeRegEx" : "CT10",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Fast Monte Carlo algorithms for matrices III: Computing a compressed approximate matrix decomposition",
      "author" : [ "P. Drineas", "R. Kannan", "M.W. Mahoney" ],
      "venue" : "SIAM J. Comput., 36:184–206",
      "citeRegEx" : "DKM06",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Efficient volume sampling for row/column subset selection",
      "author" : [ "A. Deshpande", "L. Rademacher" ],
      "venue" : "FOCS",
      "citeRegEx" : "DR10",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "High-rank matrix completion and subspace clustering with missing data",
      "author" : [ "B. Eriksson", "L. Balzano", "R. Nowak" ],
      "venue" : "CoRR",
      "citeRegEx" : "EBN11",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Loan",
      "author" : [ "C G. Golub" ],
      "venue" : "Matrix computations (3rd ed.). Johns Hopkins University Press,",
      "citeRegEx" : "GL96",
      "shortCiteRegEx" : null,
      "year" : 1996
    }, {
      "title" : "TIT",
      "author" : [ "D. Gross. Recovering low-rank matrices from few coefficients in any basis" ],
      "venue" : "57(3):1548–1566,",
      "citeRegEx" : "Gro11",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "A theory of pseudoskeleton approximations",
      "author" : [ "S.A. Goreinov", "E.E. Tyrtyshnikov", "N.L. Zamarashkin" ],
      "venue" : "Linear Algebra and Its Applications, 261(1-3):1–21",
      "citeRegEx" : "GTZ97",
      "shortCiteRegEx" : null,
      "year" : 1997
    }, {
      "title" : "Transduction with matrix completion: Three birds with one stone",
      "author" : [ "A. Goldberg", "X. Zhu", "B. Recht", "J.-M. Xu", "R. Nowak" ],
      "venue" : "NIPS",
      "citeRegEx" : "GZR+10",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "seudo-skeleton approximations by matrices of maximal volume",
      "author" : [ "S. Goreinov", "N. Zamarashkin", "E. Tyrtyshnikov" ],
      "venue" : "Mathematical Notes, 62(4):515–519",
      "citeRegEx" : "GZT97",
      "shortCiteRegEx" : null,
      "year" : 1997
    }, {
      "title" : "Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions",
      "author" : [ "N. Halko", "P.-G. Martinsson", "J. Tropp" ],
      "venue" : "SIAM Review, 53(2):217–288",
      "citeRegEx" : "HMT11",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Low-rank matrix completion using alternating minimization",
      "author" : [ "P. Jain", "P. Netrapalli", "S. Sanghavi" ],
      "venue" : "STOC",
      "citeRegEx" : "JNS13",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "An accelerated gradient method for trace norm minimization",
      "author" : [ "S. Ji", "J. Ye" ],
      "venue" : "ICML",
      "citeRegEx" : "JY09",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Improved bounds for the nyström method with application to kernel classification",
      "author" : [ "R. Jin", "T. Yang", "M. Mahdavi", "Y.-F. Li", "Z.-H. Zhou" ],
      "venue" : "TIT, 59(10):6939–6949",
      "citeRegEx" : "JYM+13",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Matrix completion from a few entries",
      "author" : [ "R. Keshavan", "A. Montanari", "S. Oh" ],
      "venue" : "TIT",
      "citeRegEx" : "KMO10",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Low rank matrix recovery: nuclear norm penalization",
      "author" : [ "V. Koltchinskii" ],
      "venue" : "Oracle Inequalities in Empirical Risk Minimization and Sparse Recovery Problems. Springer",
      "citeRegEx" : "Kol11",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Low-rank matrix and tensor completion via adaptive sampling",
      "author" : [ "A. Krishnamurthy", "A. Singh" ],
      "venue" : "NIPS",
      "citeRegEx" : "KS13",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Relative perturbation theory: (II) eigenspace and singular subspace variations",
      "author" : [ "R.-C. Li" ],
      "venue" : "SIAM J. Matrix Anal. Appl., 20:471–492,",
      "citeRegEx" : "Li99",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "Relative-error CUR matrix decompositions",
      "author" : [ "M.W. Mahoney", "P. Drineas" ],
      "venue" : "SIAM J. Matrix Anal. Appl., 30:844–881",
      "citeRegEx" : "MD08",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "CUR matrix decompositions for improved data analysis",
      "author" : [ "M.W. Mahoney", "P. Drineas" ],
      "venue" : "Proc. Natl. Acad. Sci. USA, 106:697–702",
      "citeRegEx" : "MD09",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Fast approximation of matrix coherence and statistical leverage",
      "author" : [ "M. Mahoney", "P. Drineas", "M. Magdon-Ismail", "D. Woodruff" ],
      "venue" : "ICML",
      "citeRegEx" : "MDMIW12",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Spectral regularization algorithms for learning large incomplete matrices",
      "author" : [ "R. Mazumder", "T. Hastie", "R. Tibshirani" ],
      "venue" : "JMLR, 11:2287–2322",
      "citeRegEx" : "MHT10",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Tensor-cur decompositions for tensorbased data",
      "author" : [ "M. Mahoney", "M. Maggioni", "P. Drineas" ],
      "venue" : "KDD",
      "citeRegEx" : "MMD06",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Divide-and-conquer matrix factorization",
      "author" : [ "L. Mackey", "A. Talwalkar", "M. Jordan" ],
      "venue" : "NIPS",
      "citeRegEx" : "MTJ11",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Introductory Lectures on Convex Optimization: A Basic Course",
      "author" : [ "Y. Nesterov" ],
      "venue" : "Springer",
      "citeRegEx" : "Nes03",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Estimation of (near) low-rank matrices with noise and high-dimensional scaling",
      "author" : [ "S. Negahban", "M. Wainwright" ],
      "venue" : "ICML",
      "citeRegEx" : "NW10",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "A simpler approach to matrix completion",
      "author" : [ "B. Recht" ],
      "venue" : "JMLR, 12:3413–3430",
      "citeRegEx" : "Rec11",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Estimation of high dimensional low rank matrices",
      "author" : [ "A. Rhode", "A. Tsybakov" ],
      "venue" : "Annual of Statistics, 39(2):887–930",
      "citeRegEx" : "RT11",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Maximum-margin matrix factorization",
      "author" : [ "N. Srebro", "J. Rennie", "T. Jaakkola" ],
      "venue" : "NIPS",
      "citeRegEx" : "SRJ04",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Four algorithms for the the efficient computation of truncated pivoted qr approximations to a sparse matrix",
      "author" : [ "G. Stewart" ],
      "venue" : "Numerische Mathematik",
      "citeRegEx" : "Ste99",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "Data Anal",
      "author" : [ "J. Tropp. Improved analysis of the subsampled randomized hadamard transform. Adv. Adapt" ],
      "venue" : "3:115–126,",
      "citeRegEx" : "Tro11",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "An accelerated proximal gradient algorithm for nuclear norm regularized linear least squares problems",
      "author" : [ "K.-C. Toh", "Y. Sangwoon" ],
      "venue" : "Pacific Journal of Optimization",
      "citeRegEx" : "TS10",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Incomplete cross approximation in the mosaic-skeleton method",
      "author" : [ "E. Tyrtyshnikov" ],
      "venue" : "Computing",
      "citeRegEx" : "Tyr00",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "Using the nyström method to speed up kernel machines",
      "author" : [ "C. Williams", "M. Seeger" ],
      "venue" : "NIPS",
      "citeRegEx" : "WS00",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "A scalable cur matrix decomposition algorithm: Lower time complexity and tighter bound",
      "author" : [ "S. Wang", "Z. Zhang" ],
      "venue" : "NIPS",
      "citeRegEx" : "WZ12",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Improving cur matrix decomposition and the nyström approximation via adaptive sampling",
      "author" : [ "S. Wang", "Z. Zhang" ],
      "venue" : "JMLR, 14(1):2729–2769",
      "citeRegEx" : "WZ13",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Speedup matrix completion with side information: Application to multi-label learning",
      "author" : [ "M. Xu", "R. Jin", "Z.-H. Zhou" ],
      "venue" : "NIPS,",
      "citeRegEx" : "XJZ13",
      "shortCiteRegEx" : null,
      "year" : 2013
    } ],
    "referenceMentions" : [ {
      "referenceID" : 34,
      "context" : "Examples include user-item rating matrix in recommender system [SRJ04], gene expression matrix in bioinCorresponding author.",
      "startOffset" : 63,
      "endOffset" : 70
    }, {
      "referenceID" : 24,
      "context" : "formatics [MD08], kernel matrix in kernel learning [WS00], document-term matrix in document retrieval [MD08], and instance-label matrix in multi-label learning [GZR+10].",
      "startOffset" : 10,
      "endOffset" : 16
    }, {
      "referenceID" : 39,
      "context" : "formatics [MD08], kernel matrix in kernel learning [WS00], document-term matrix in document retrieval [MD08], and instance-label matrix in multi-label learning [GZR+10].",
      "startOffset" : 51,
      "endOffset" : 57
    }, {
      "referenceID" : 24,
      "context" : "formatics [MD08], kernel matrix in kernel learning [WS00], document-term matrix in document retrieval [MD08], and instance-label matrix in multi-label learning [GZR+10].",
      "startOffset" : 102,
      "endOffset" : 108
    }, {
      "referenceID" : 34,
      "context" : "Various methods have been developed for low rank matrix approximation, including truncated singular value decomposition, matrix factorization [SRJ04], matrix regression [Kol11], column subset selection [BDMI11], the Nyström method [WS00].",
      "startOffset" : 142,
      "endOffset" : 149
    }, {
      "referenceID" : 21,
      "context" : "Various methods have been developed for low rank matrix approximation, including truncated singular value decomposition, matrix factorization [SRJ04], matrix regression [Kol11], column subset selection [BDMI11], the Nyström method [WS00].",
      "startOffset" : 169,
      "endOffset" : 176
    }, {
      "referenceID" : 1,
      "context" : "Various methods have been developed for low rank matrix approximation, including truncated singular value decomposition, matrix factorization [SRJ04], matrix regression [Kol11], column subset selection [BDMI11], the Nyström method [WS00].",
      "startOffset" : 202,
      "endOffset" : 210
    }, {
      "referenceID" : 39,
      "context" : "Various methods have been developed for low rank matrix approximation, including truncated singular value decomposition, matrix factorization [SRJ04], matrix regression [Kol11], column subset selection [BDMI11], the Nyström method [WS00].",
      "startOffset" : 231,
      "endOffset" : 237
    }, {
      "referenceID" : 25,
      "context" : "In this work, we will focus on the CUR algorithm for low rank matrix approximation [MD09].",
      "startOffset" : 83,
      "endOffset" : 89
    }, {
      "referenceID" : 25,
      "context" : "Compared to other low rank approximation algorithms, CUR is advantageous in that it has (i) an easy interpretation of the approximation result because the subspace is constructed by the actual columns and rows of the target matrix [MD09], and (ii) strong (near-optimal) theoretical guarantee [BXM10, DKM06, MD08, MD09, WZ12, WZ13].",
      "startOffset" : 231,
      "endOffset" : 237
    }, {
      "referenceID" : 25,
      "context" : "The CUR matrix decomposition algorithm has been successfully applied to many domains, including bioinformatics [MD09], collaborative filtering [MTJ11], video background modeling [MTJ11], hyperspectral medical image analysis [MMD06], text data analysis [MD08].",
      "startOffset" : 111,
      "endOffset" : 117
    }, {
      "referenceID" : 29,
      "context" : "The CUR matrix decomposition algorithm has been successfully applied to many domains, including bioinformatics [MD09], collaborative filtering [MTJ11], video background modeling [MTJ11], hyperspectral medical image analysis [MMD06], text data analysis [MD08].",
      "startOffset" : 143,
      "endOffset" : 150
    }, {
      "referenceID" : 29,
      "context" : "The CUR matrix decomposition algorithm has been successfully applied to many domains, including bioinformatics [MD09], collaborative filtering [MTJ11], video background modeling [MTJ11], hyperspectral medical image analysis [MMD06], text data analysis [MD08].",
      "startOffset" : 178,
      "endOffset" : 185
    }, {
      "referenceID" : 28,
      "context" : "The CUR matrix decomposition algorithm has been successfully applied to many domains, including bioinformatics [MD09], collaborative filtering [MTJ11], video background modeling [MTJ11], hyperspectral medical image analysis [MMD06], text data analysis [MD08].",
      "startOffset" : 224,
      "endOffset" : 231
    }, {
      "referenceID" : 24,
      "context" : "The CUR matrix decomposition algorithm has been successfully applied to many domains, including bioinformatics [MD09], collaborative filtering [MTJ11], video background modeling [MTJ11], hyperspectral medical image analysis [MMD06], text data analysis [MD08].",
      "startOffset" : 252,
      "endOffset" : 258
    }, {
      "referenceID" : 10,
      "context" : "Since most matrix completion algorithms are developed only for matrices of exactly low rank, they usually work poorly for matrices of full rank [EBN11].",
      "startOffset" : 144,
      "endOffset" : 151
    }, {
      "referenceID" : 22,
      "context" : "We note that although an adaptive sampling approach is developed in [KS13] that does apply to matrices of full rank, they use a different sampling strategy, and their bound has a poor dependence on failure probability δ (i.",
      "startOffset" : 68,
      "endOffset" : 74
    }, {
      "referenceID" : 22,
      "context" : "In particular, to perfectly recover a rank-r matrix of size n × n, only O(nr ln r) observed entries are needed, significantly lower than O(nr ln n) for standard matrix completion theory [CR12, CT10, Gro11, KMO10, Rec11] and lower than O(nr3/2 ln r) for adaptive algorithm for matrix recovery [KS13].",
      "startOffset" : 292,
      "endOffset" : 298
    }, {
      "referenceID" : 8,
      "context" : "[DKM06] gives an additive error bound for the CUR decomposition, and a relative error bound, a significantly stronger result, is given in [MD08].",
      "startOffset" : 0,
      "endOffset" : 7
    }, {
      "referenceID" : 24,
      "context" : "[DKM06] gives an additive error bound for the CUR decomposition, and a relative error bound, a significantly stronger result, is given in [MD08].",
      "startOffset" : 138,
      "endOffset" : 144
    }, {
      "referenceID" : 29,
      "context" : "[MTJ11] proposes a divide-and-conquer method to compute the CUR decomposition in parallel.",
      "startOffset" : 0,
      "endOffset" : 7
    }, {
      "referenceID" : 41,
      "context" : "[WZ13] proposes an adaptive CUR algorithm with much tighter error bound and much lower time complexity.",
      "startOffset" : 0,
      "endOffset" : 6
    }, {
      "referenceID" : 8,
      "context" : "In [DKM06], the authors suggest a simple uniform sampling of columns and rows for the CUR decomposition when the maximum statistical leverage scores, also referred to as incoherence measure [CR12, CT10, Rec11], is limited.",
      "startOffset" : 3,
      "endOffset" : 10
    }, {
      "referenceID" : 26,
      "context" : "In [MDMIW12], algorithms have been developed to efficiently compute the approximated values of statistical leverage scores without having to calculate the SVD decomposition of a large matrix.",
      "startOffset" : 3,
      "endOffset" : 12
    }, {
      "referenceID" : 39,
      "context" : "A special case of column subset selection is Nyström methods, which is usually used to approximate Positive Semi-Definitive (PSD) matrix in kernel learning [WS00].",
      "startOffset" : 156,
      "endOffset" : 162
    }, {
      "referenceID" : 22,
      "context" : "[KS13] developed an adaptive sensing strategy for matrix completion that removes an lnn factor from the sample complexity.",
      "startOffset" : 0,
      "endOffset" : 6
    }, {
      "referenceID" : 42,
      "context" : "In [XJZ13], the authors show that the sample complexity for perfect matrix recovery can be reduced dramatically with appropriate side information.",
      "startOffset" : 3,
      "endOffset" : 10
    }, {
      "referenceID" : 24,
      "context" : "on their statistical leverage scores [MD08] and adaptive sampling [KS13, WZ12], can be more effective.",
      "startOffset" : 37,
      "endOffset" : 43
    }, {
      "referenceID" : 24,
      "context" : "We do not choose these sampling methods because they either require an access to the full matrix [MD08], introduce serious overhead in computation [WZ12], or result in significantly worse bound when matrix is of full rank [KS13].",
      "startOffset" : 97,
      "endOffset" : 103
    }, {
      "referenceID" : 40,
      "context" : "We do not choose these sampling methods because they either require an access to the full matrix [MD08], introduce serious overhead in computation [WZ12], or result in significantly worse bound when matrix is of full rank [KS13].",
      "startOffset" : 147,
      "endOffset" : 153
    }, {
      "referenceID" : 22,
      "context" : "We do not choose these sampling methods because they either require an access to the full matrix [MD08], introduce serious overhead in computation [WZ12], or result in significantly worse bound when matrix is of full rank [KS13].",
      "startOffset" : 222,
      "endOffset" : 228
    }, {
      "referenceID" : 30,
      "context" : "accelerated gradient descent [Nes03]).",
      "startOffset" : 29,
      "endOffset" : 36
    }, {
      "referenceID" : 22,
      "context" : "Method CUR+ [KS13] [BJ14] [JNS13] [CR12, CT10, CBSW14, KMO10, Rec11] # Observation nr ln r nr3/2 ln r nr2 nr4.",
      "startOffset" : 12,
      "endOffset" : 18
    }, {
      "referenceID" : 2,
      "context" : "Method CUR+ [KS13] [BJ14] [JNS13] [CR12, CT10, CBSW14, KMO10, Rec11] # Observation nr ln r nr3/2 ln r nr2 nr4.",
      "startOffset" : 19,
      "endOffset" : 25
    }, {
      "referenceID" : 17,
      "context" : "Method CUR+ [KS13] [BJ14] [JNS13] [CR12, CT10, CBSW14, KMO10, Rec11] # Observation nr ln r nr3/2 ln r nr2 nr4.",
      "startOffset" : 26,
      "endOffset" : 33
    }, {
      "referenceID" : 22,
      "context" : "We should note that unlike [KS13] where the incoherence measure is only assumed for column vectors, we assume a small incoherence measure for both row and column vectors here.",
      "startOffset" : 27,
      "endOffset" : 33
    }, {
      "referenceID" : 22,
      "context" : "stronger assumption that allows us to sample both rows and columns, leading to the improvement in the sample complexity from O(nr3/2 ln r) in [KS13] to O(nr ln r).",
      "startOffset" : 142,
      "endOffset" : 148
    }, {
      "referenceID" : 11,
      "context" : "In order to effectively capture the skewed singular value distribution, we introduce the concept of numerical rank r(M,η) [GL96] with respect to non-negative constant η > 0 r(M,η) = m ∑ i=1 σ2 i σ2 i +mnη Note that when η = 0, the numerical rank is equivalent to the true rank of the matrix.",
      "startOffset" : 122,
      "endOffset" : 128
    }, {
      "referenceID" : 41,
      "context" : "Detailed information of these data sets can be found in [WZ13].",
      "startOffset" : 56,
      "endOffset" : 62
    }, {
      "referenceID" : 22,
      "context" : "Although the adaptive sampling based approaches [KS13] usually yield lower errors than the standard CUR algorithm, they do not choose observed entries randomly and therefore are not included in the comparison.",
      "startOffset" : 48,
      "endOffset" : 54
    }, {
      "referenceID" : 40,
      "context" : "Finally, we follow the experimental protocol specified in [WZ12] by repeating every experiment 10 times and reporting the mean value.",
      "startOffset" : 58,
      "endOffset" : 64
    }, {
      "referenceID" : 22,
      "context" : "Since adaptive sampling has shown promising results for low rank matrix approximation [KS13], in the future, we plan to combine the proposed algorithm with adaptive sampling strategy to further reduce the error bound.",
      "startOffset" : 86,
      "endOffset" : 92
    }, {
      "referenceID" : 16,
      "context" : "1 in [HMT11]) Let M be an n×m matrix with singular value decomposition M = UΣV ⊤.",
      "startOffset" : 5,
      "endOffset" : 12
    }, {
      "referenceID" : 36,
      "context" : "2 of [Tro11]) Let X be a finite set of PSD matrices with dimension k (means the size of the square matrix is k × k).",
      "startOffset" : 5,
      "endOffset" : 12
    }, {
      "referenceID" : 23,
      "context" : "4 and discussion in Section 5 from [Li99].",
      "startOffset" : 35,
      "endOffset" : 41
    } ],
    "year" : 2014,
    "abstractText" : "CUR matrix decomposition computes the low rank approximation of a given matrix by using the actual rows and columns of the matrix. It has been a very useful tool for handling large matrices. One limitation with the existing algorithms for CUR matrix decomposition is that they need an access to the full matrix, a requirement that can be difficult to fulfill in many real world applications. In this work, we alleviate this limitation by developing a CUR decomposition algorithm for partially observed matrices. In particular, the proposed algorithm computes the low rank approximation of the target matrix based on (i) the randomly sampled rows and columns, and (ii) a subset of observed entries that are randomly sampled from the matrix. Our analysis shows the relative error bound, measured by spectral norm, for the proposed algorithm when the target matrix is of full rank. We also show that only O(nr ln r) observed entries are needed by the proposed algorithm to perfectly recover a rank r matrix of size n × n, which improves the sample complexity of the existing algorithms for matrix completion. Empirical studies on both synthetic and real-world datasets verify our theoretical claims and demonstrate the effectiveness of the proposed algorithm.",
    "creator" : "LaTeX with hyperref package"
  }
}