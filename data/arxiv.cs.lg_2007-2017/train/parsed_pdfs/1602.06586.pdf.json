{
  "name" : "1602.06586.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Recovering Structured Probability Matrices",
    "authors" : [ "Qingqing Huang", "Sham M. Kakade", "Weihao Kong", "Gregory Valiant" ],
    "emails" : [ "qqh@mit.edu.", "sham@cs.washington.edu", "kweihao@gmail.com", "valiant@stanford.edu." ],
    "sections" : [ {
      "heading" : null,
      "text" : "Our results apply to the setting where B has a particular rank 2 structure. For this setting, we propose an efficient (and practically viable) algorithm that accurately recovers the underlying M × M matrix using Θ(M) samples. This result easily translates to Θ(M) sample algorithms for learning topic models with two topics over dictionaries of size M , and learning hidden Markov Models with two hidden states and observation distributions supported on M elements. These linear sample complexities are optimal, up to constant factors, in an extremely strong sense: even testing basic properties of the underlying matrix (such as whether it has rank 1 or 2) requires Ω(M) samples. Furthermore, we provide an even stronger lower bound where distinguishing whether a sequence of observations were drawn from the uniform distribution over M observations versus being generated by an HMM with two hidden states requires Ω(M) observations. This precludes sublinear-sample hypothesis tests for basic properties, such as identity or uniformity, as well as sublinear sample estimators for quantities such as the entropy rate of HMMs. This impossibility of sublinear-sample property testing in these settings is intriguing and underscores the significant differences between these structured settings and the standard setting of drawing i.i.d samples from an unstructured distribution of support size M .\n∗MIT. Email: qqh@mit.edu. †University of Washington. Email: sham@cs.washington.edu ‡Stanford University. Email: kweihao@gmail.com §Stanford University. Email: valiant@stanford.edu. Gregory and Weihao’s contributions were supported by NSF\nCAREER Award CCF-1351108, and a research grant from the Okawa Foundation.\nar X\niv :1\n60 2.\n06 58\n6v 1\n[ cs\n.L G\n] 2\n1 Fe\nb 20\n16"
    }, {
      "heading" : "1 Introduction",
      "text" : "Consider an unknown M × M matrix of probabilities B, satisfying ∑i,j Bi,j = 1. Suppose one is given N independently drawn (i, j)-pairs, sampled according to the distribution defined by B. How many draws are necessary to accurately recover B? What can one infer about the underlying matrix based on these samples? How can one accurately test whether the underlying matrix possesses certain properties of interest? How do structural assumptions on B — for example, the assumption that B has low rank — affect the information theoretic or computational complexity of these questions? For the majority of these tasks, we currently lack both a basic understanding of the computational and information theoretic lay of the land, as well as algorithms that seem capable of achieving the information theoretic or computational limits.\nThis general question of making accurate inferences about a matrix of probabilities, given a matrix of observed “counts” of discrete outcomes, lies at the core of a number of problems that disparate communities have been tackling independently. On the theoretical side, these problems include both work on community detection in stochastic block models (where the goal is to infer the community memberships from an adjacency matrix of a graph that has been drawn according to an underlying matrix of probabilities expressing the community structure) as well as the line of work on recovering topic models, hidden Markov models (HMMs), and richer structured probabilistic models (where the model parameters can often be recovered using observed count data). On the practical side, these problems include the recent work in the natural language processing community to infer structure from matrices of word co-occurrence counts for the purpose of constructing good “word embeddings”, as well as latent semantic analysis and non-negative matrix factorization.\nIn this work, we start this line of inquiry by focusing on the estimation problem where the probability matrix B possesses a particular rank 2 structure. While this estimation problem is rather specific, it generalizes the basic community detection problem and also encompasses the underlying problem behind learning 2-state HMMs and learning 2-topic models. Furthermore, this rank 2 case also provides a means to study how property testing and estimation problems are different in this structured setting, as opposed to the simpler rank 1 setting that is equivalent to the standard setting of independent draws from a distribution supported on M elements.\nWe focus on the estimation of B in the sparse data regime, near the information theoretic limit. The motivation for this is that in many practical scenarios involving count samples, we seek algorithms capable of extracting the underlying structure in the sparsely sampled regime. To give two examples, consider forming the matrix of word co-occurrences—the matrix whose rows and columns are indexed by the set of words, and whose (i, j)th element consists of the number of times the ith word follows the jth word in a large corpus of text. One could also consider a matrix whose rows are indexed by customers, and whose columns are indexed by products, with the (i, j)th entry corresponding to the number of times the ith customer has purchased the jth product. In both settings, the structure of the probability matrix underlying these observed counts contains insights into the two domains, and in both domains we only have relatively sparse data. This is inherent in many other natural scenarios involving heavy-tailed distributions where, regardless of how much data one collects, a significant fraction of items (e.g. words, genetic mutations) will only be observed a few times.\nSuch estimation questions have been actively studied in the community detection literature, where the objective is to accurately recover the communities in the regime where the average degree (e.g. the row sums of the adjacency matrix) are constant. In contrast, the recent line of works for recovering highly structured models (such as large topic models, HMMs, etc.) are only applicable to the oversampled regime (where the amount of data is well beyond the information theoretic limits), where achieving the information theoretic limits remains a widely open question. This work begins to bridge the divide between these recent algorithmic advances in both communities. We hope that the rank-2 probability matrix setting that studied here serves as a jumping-off point for the more general questions of developing information theoretically optimal algorithms for estimating higher-rank matrices and\ntensors in general, or recovering low-rank approximations to arbitrary probability matrices, in the sparse data regime. While these settings may be more challenging, we believe that some of our algorithmic techniques can be fruitfully extended.\nIn addition to developing algorithmic tools which we hope are applicable to a wider class of problems, a second motivation for considering this particular rank 2 case is that, with respect to distribution learning and property testing, the entire lay-of-the-land seems to change completely when the probability matrix B has rank larger than 1. In the rank 1 setting — where a sample consists of 2 independent draws from a distribution supported on {1, . . . ,M} — the distribution can be learned using Θ(M) draws. Nevertheless, many properties of interest can be tested or estimated using a sample size that is sublinear in M1. However, in the rank 2 setting, even though the underlying matrix B can be represented with O(M) parameters (and, as we show, it can also be accurately and efficiently recovered with O(M) sample counts), sublinear sample property testing and estimation is generally impossible. This result begs the more general question: what conditions must be true of a structured statistical setting in order for property testing to be easier than learning?"
    }, {
      "heading" : "1.1 Problem Formulation",
      "text" : "Assume our vocabulary is the index set M = {1, . . . ,M} of M words and that there is an underlying low rank probability matrix B, of size M ×M , with the following structure:\nB = DWD>, where D = [ p, q ] . (1)\nHere, D = RM×2+ is the dictionary matrix parameterized by two M -dimensional probability vectors p, q, supported on the standard (M − 1)-simplex. Also, W is the 2 × 2 mixing matrix, which is a probability matrix satisfying W ∈ R2×2+ , ∑ i,jWi,j = 1.\nDenote wp = W1,1 +W1,2 and wq = W2,1 +W2,2. Note that ∑\nk Bi,k = wpp+ wqq. Define the covariance matrix of any probability matrix P as:\n[Cov(P )]i,j := Pi,j − ( ∑\nk\nPi,k)( ∑\nk\nPk,j).\nNote that Cov(P )~1 = ~0 and ~1>Cov(P ) = ~0 (where ~1 and ~0 are the all ones and zeros vectors, respectively). This implies that, without loss of generality, the covariance of the mixing matrix, Cov(W ), can be expressed as:\nCov(W ) = [wL,−wL]>[wR,−wR].\nfor some real numbers wL, wR ∈ [−1, 1]. For ease of exposition, we restrict to the symmetric case where wL = wR = w, though our results hold more generally.\nSuppose we obtain N , i.i.d. sample counts from B of the form {(i1, j1) (i2, j2), . . . (iN , jN )}, where each sample (in, jn) ∈ M × M. The probability of obtaining a count (i, j) in a sample is Bi,j . Moreover, assume that the number of samples follows a Poisson distribution: N ∼ Poi(N). The Poisson assumption on the number of samples is made only for the convenience of analysis: if N follows a Poisson distribution, the counts of observing (i, j) follows a Poisson distribution Poi(NBi,j) and is independent from the counts of observing (i′, j′) for (i′, j′) 6= (i, j). This assumption is made only for the convenience of analysis and is not crucial for the correctness of the algorithm. As M is asymptotically large, with high probability, N and N are within a subconstant factor of each other and both upper and lower bounds translate between the Poissonized setting, and the setting of exactly N samples. Throughout, we state our sample complexity results in terms of N rather than N.\n1Distinguishing whether a distribution is uniform versus far from uniform can be accomplished using only O( √ M) draws, testing whether two sets of samples were drawn from similar distributions can be done with O(M2/3) draws, estimating the entropy of the distribution to within an additive can be done with O( M\nlogM ) draws, etc.\nNotation Throughout the paper, we use the following standard shorthand notations. Denote [n] , {1, . . . , n}. Let I denote a subset of indices in M. For a M -dimensional vector x, we use vector xI to denote the elements of x restricting to the indices in I; for two index sets I, J , and a M ×M dimensional matrix X, we use XI×J denote the submatrix of X with rows restricting to indices in I and columns restricting to indices in J .\nWe use Poi(λ) to denote a Poisson distribution with rate λ; we use Ber(λ) to denote a Bernoulli random variable with success probability λ; and we use Mul(x;λ) to denote a multinomial distribution over M outcomes with λ number of trials and event probability vector x ∈ RM+ such that ∑ i xi = 1."
    }, {
      "heading" : "1.2 Main Results",
      "text" : ""
    }, {
      "heading" : "1.2.1 Recovering Rank-2 Probability Matrices",
      "text" : "Throughout, we focus on a class of well-separated model parameters. The separation assumptions guarantee that the rank 2 matrix B is well-conditioned. Furthermore, this assumption also has natural interpretations in each of the different applications (to that of community detection, topic modeling, and HMMs).\nAll of our order notations are with respect to the vocabulary size M , which is asymptotically large. Also, we say that a statement is true “with high probability” if the failure probability of the statement is inverse poly in M ; and we say a statement is true “with large probability” if the failure probability is of some small constant δ, which can be easily boosted to very smaller probabilities with repetitions.\nAssumption 1 (Ω(1) separation). Assume that W is symmetric, where wL = wR = w (all our results extend to the asymmetric case). Define the marginal probability vector, ρ and the dictionary separation vector as:\nρi := ∑\nk\nBi,k, ∆ := w(p− q) . (2)\nAssume that wp and wq are lower bounded by some constant Cw = Ω(1). Assume that the `1-norm of the dictionary separation is lower bounded by some constant C∆ = Ω(1),\n‖∆‖1 ≥ C∆. (3)\nNote that while in general the dictionary matrix D and the mixing matrix W are not uniquely identifiable from B, there does exist an identifiable decomposition. Observe that the matrix Cov(B) admits a unique rank-1 decomposition: Cov(B) := B− ρρ> = ∆∆>, which also implies that:\nB = ρρ> + ∆∆>. (4)\nIt is this unique decomposition we seek to estimate, along with the matrix B. Now we are ready to state our main theorem.\nTheorem 1.1 (Main theorem). Suppose we have access to N i.i.d. samples generated according to the rank 2 probability matrix B with structure given by (1) and satisfying the separation Assumption 1. For > 0, with N = Θ(M/ 2) samples, our algorithm runs in time poly(M) and returns estimators\nB̂, ρ̂, ∆̂, such that with large probability:\n‖B̂ − B‖1 ≤ , ‖ρ̂− ρ‖1 ≤ , ‖∆̂−∆‖1 ≤ .\n(here, the `1-norm of an M ×M matrix P is simply defined as ‖P‖1 = ∑ i,j |Pi,j |).\nFirst, note that we do not bound the spectral error ‖B̂ − B‖2 since when the marginal ρ is not roughly uniform, error bounds in terms of spectral distance are not particularly strong. A natural error measure of estimation for probability distributions is total variation distance (equivalent to our `1 norm here). Second, note that naively estimating a distribution over M2 outcomes requires order M2 samples. Importantly, our algorithm utilizes the rank 2 structure of the underlying matrix B to achieve a sample complexity which is precisely linear in the vocabulary size M (without any additional log factors). The proof of the main theorem draws upon recent advances in the concentration of sparse random matrices from the community detection literature; the well characterized problem in the community detection literature can be viewed as a simple and special case of our problem, where the marginals are homogeneous (which we discuss later).\nWe now turn to the implications of this theorem to testing and learning problems."
    }, {
      "heading" : "1.2.2 Topic Models and Hidden Markov Models",
      "text" : "One of the main motivations for considering the specific rank 2 structure on the underlying matrix B is that this structure encompasses the structure of the matrix of expected bigrams generated by both 2-topic models and two state HMMs. We now make these connections explicit.\nDefinition 1.2. A 2-topic model over a vocabulary of size M is defined by a pair of distributions, p and q supported over M words, and a pair of topic mixing weights πp and πq = 1 − πp. The process of drawing a bigram (i, j) consists of first randomly picking one of the two “topics” according to the mixing weights, and then drawing two independent words from the word distribution corresponding to the chosen topic. Thus the probability of seeing bigram (i, j) is (πppipj + πqqiqj), and so the expected\nbigram matrix can be written as B = DWD> with D = [p, q], and W = [ πp 0 0 πq ] .\nThe following corollary shows that estimation is possible with sample size linear in M :\nCorollary 1.3. (Learning 2-topic models) Suppose we are in the 2-topic model setting. Assume that πp(1 − πp)‖p − q‖1 = Ω(1). There exists an algorithm which, given N = Ω(M/ 2) bigrams, runs in time poly(M) and with large probability returns estimates π̂p, p̂, q̂ such that\n|π̂p − πp| < , ‖p̂− p‖1 ≤ , ‖q̂ − q‖1 ≤ .\nDefinition 1.4. A hidden Markov model with 2 hidden states (sp, sq) and a size M observation vocabulary is defined by a 2 × 2 transition matrix T for the 2 hidden states, and two distributions of observations, p and q, corresponding to the 2 states.\nA sequence of N observations is sampled as follows: First, select an initial state according to the stationary distribution of the underlying Markov chain [πp, πq]; Then evolve the Markov chain according to the transition matrix T for N steps; For each n ∈ {1, . . . , N}, the n-th observation in the sequence is generated by making an independent draw from either distribution p or q according to whether the Markov chain is in state sp or sq at the n-th timestep.\nThe probability that seeing a bigram (i, j) for the n and the (n + 1)-th observation is given by πppi(Tp,ppj + Tp,qqj) + πqqi(Tq,ppj + Tq,qqj), and hence the expected bigram matrix can be written as\nB = DWD> with D = [p, q], and W = [ πp 0 0 πq ] [ Tp,p 1− Tp,p 1− Tq,q Tq,q ] .\nWe have the following learning result:\nCorollary 1.5. (Learning 2-state HMMs) Suppose we are in the 2-state HMM setting. Assume that ‖p−q‖1 ≥ C1 and that πp, Tp,p, Tq,q are lower bounded by C2 and upper bounded by 1−C2, where both C1 and C2 are Ω(1). There exists an algorithm which, given a sampled chain of length N = Ω(M/ 2),\nruns in time poly(M) and returns estimates π̂p, T̂ , p̂, q̂ such that, with high probability, we have (that there is exists a permutation of the model such that)\n|π̂p − πp| < , |T̂p,p − Tp,p| < , |T̂q,q − Tq,q| < , ‖p̂− p‖1 ≤ , ‖q̂ − q‖1 ≤ . Furthermore, it is sufficient for this algorithm to only utilize Ω(M/ 2) random bigrams and only Ω(1/ 2) random trigrams from this chain.\nIt is worth noting that the matrix of bigram probabilities does not uniquely determine the underlying HMM. However, one can recover the model parameters using sampled trigram sequences; this last step is straightforward (and sample efficient as it uses only an additional Ω(1/ 2) trigrams) when given an accurate estimate of B (see [6] for the moment structure in the trigrams)."
    }, {
      "heading" : "1.2.3 Testing vs. Learning",
      "text" : "The above theorem and corollaries are tight in an extremely strong sense. Both for the topic model and HMM settings, while we can learn the models using Ω(M) samples/observations, in both settings, it is information theoretically impossible to perform even the most basic property tests using fewer than Θ(M) samples.\nIn the case of 2-topic models, the community detection lower bounds [41][32][52] imply that Θ(M) bigrams are necessary to even distinguish between the case that the underlying model is simply the uniform distribution over bigrams versus the case of a 2-topic model in which each topic corresponds to a uniform distribution over disjoint subsets of M/2 words. We prove a stronger lower bound for the case of HMMs with two states, where we permit an estimator to have more information, namely the full sequence of observations (not merely bigram and trigram counts). Perhaps surprisingly, even with this extra information, we have the following lower bound:\nTheorem 1.6. Given a sequence of observations from a HMM with two states and emission distributions p, q supported on M elements, even if the underlying Markov process is symmetric, with transition probability 1/4, it is information theoretically impossible to distinguish the case that the two emission distributions, p = q = Uniform[M ] from the case that ||p − q||1 ≥ 1/2 using a sequence of fewer than Θ(M) observations.\nThe proof of this theorem is given in Appendix 5.2, and amounts to a careful comparison of the processes of generating a uniformly random path in a graph, versus generating a path corresponding to a 2-state HMM for which there is significant correlation between consecutive observations. As an immediate corollary of this theorem, it follows that many natural properties of HMMs cannot be estimated using a sublinear length sequence of observations.\nCorollary 1.7. For HMMs with 2 states and emission distributions supported on a domain of size at most M , to estimate the entropy rate up to an additive constant c ≤ 1 requires a sequence of Ω(M) observations.\nThese strong lower bounds for property testing and estimation of HMMs are striking for several reasons. First, the core of our learning algorithm is a matrix reconstruction step that uses only the set of bigram counts (though we do use trigram counts for the final parameter recovery). Conceivably, one could benefit significantly from considering longer sequences of observations — even in HMMs that mix in constant time, there are detectable correlations between observations separated by O(logM) steps. Regardless, our lower bound shows that this is not the case. No additional information from such longer k-grams can be leveraged to yield sublinear sample property testing or estimation.\nA second notable point is the brittleness of sublinear property testing and estimation as we deviate from the standard (unstructured) i.i.d sampling setting. In particular, while it may be natural to expect that testing and estimation would become rapidly more difficult as the number of hidden states of an HMM increase, we see here a (super-constant) increase in the difficulty of testing and estimation problems between the one state setting to the two state setting."
    }, {
      "heading" : "1.3 Related Work",
      "text" : "As mentioned earlier, the general problem of reconstructing an underlying matrix of probabilities given access to a count matrix drawn according to the corresponding distribution, lies at the core of questions that are being actively pursued by several different communities. We briefly describe these questions, and their relation to the present work. Community Detection. With the increasing prevalence of large scale social networks, there has been a flurry of activity from the algorithms and probability communities to both model structured random graphs, and understand how (and when it is possible) to examine a graph and infer the underlying structures that might have given rise to the observed graph. One of the most well studied community models is the stochastic block model [27]. In its most basic form, this model is parameterized by a number of individuals, M , and two probabilities, α, β. The model posits that the M individuals are divided into two equal-sized “communities”, and such a partition defines the following random graph model: for each pair of individuals in the same community, the edge between them is present with probability α (independently of all other edges); for a pair of individuals in different communities, the edge between them is present with probability β < α. Phrased in the notation of our setting, the adjacency matrix of the graph is generated by including each potential edge (i, j) independently, with probability Bi,j , with Bi,j = α or β according to whether i and j are in the same community. Note that B has rank 2 and is expressible in the form of Equation 1 as B = DWD> where D = [p, q] for vectors p = 2M I1 and q = 2 M I2 where I1 is the indicator vector for membership in the first community, and I2 is defined analogously, and W is the 2× 2 matrix with αM 2 4 on the diagonal and β M2\n4 on the off-diagonal.\nWhat values of α, β, and M enable the community affiliations of all individuals to be accurately recovered with high probability? What values of α, β, and M allow for the graph to be distinguished from an Erdos-Renyi random graph (that has no community structure)? The crucial regime is where α, β = O( 1M ), and hence each person has a constant, or logarithmic expected degree. The naive spectral approaches will fail in this regime, as there will likely be at least one node with degree ≈ logM/ log logM , which will ruin the top eigenvector. Nevertheless, in the past four years a number of transformations of the adjacency matrix have been proposed, after which the spectral approach will enable constant factor optimal detection (the problem of distinguishing the community setting from G(n, p) and reconstruction in this constant degree regime (see e.g. [22, 40, 32, 33]). In the past year, for both the exact recovery problem and the detection problem, the exact tradeoffs between α, β, and M were established, down to subconstant factors [41, 1, 36]. More recently, there has been further research investigating more complex stochastic block models, consisting of three or more components, components of unequal sizes, etc. (see e.g. [19, 2]).\nWord Embeddings. On the more applied side, some of the most impactful advances in natural language processing over the past two years has been work on “word embeddings” [37, 35, 46, 9]. The main idea is to map every word w to a vector vw ∈ Rd (typically d ≈ 500) in such a way that the geometry of the vectors captures the semantics of the word.2 One of the main constructions for such embeddings is to form the M ×M matrix whose rows/columns are indexed by words, with i, jth entry corresponding to the total number of times the ith and jth word occur next to (or near) each other in a large corpus of text (e.g. wikipedia). The word embedding is then computed as the rows of the singular vectors corresponding to the top rank d approximation to this empirical count matrix.3 These embeddings have proved to be extremely effective, particularly when used as a way to map\n2The goal of word embeddings is not just to cluster similar words, but to have semantic notions encoded in the geometry of the points: the example usually given is that the direction representing the difference between the vectors corresponding to “king” and “queen” should be similar to the difference between the vectors corresponding to “man” and “woman”, or “uncle” and “aunt”, etc.\n3A number of pre-processing steps have been considered, including taking the element-wise square roots of the entries, or logarithms of the entries, prior to computing the SVD.\ntext to features that can then be trained in downstream applications. Despite their successes, current embeddings seem to suffer from sampling noise in the count matrix (where many transformations of the count data are employed, e.g. see [45])—this is especially noticeable in the relatively poor quality of the embeddings for relatively rare words. The recent theoretical work [10] sheds some light on why current approaches are so successful, yet the following question largely remains: Is there a more accurate way to recover the best rank-d approximation of the underlying matrix than simply computing the best rank-d approximation for the (noisy) matrix of empirical counts?\nEfficient Algorithms for Latent Variable Models. There is a growing body of work from the algorithmic side (as opposed to information theoretic) on how to recover the structure underlying various structured statistical settings. This body of work includes work on learning HMMs [29, 39, 17], recovering low-rank structure [8, 7, 14], and learning or clustering various structured distributions such as Gaussian mixture models [20, 51, 38, 13, 28, 31, 24] and latent dirichlet allocation (a very popular topic model) [5]. A number of these methods essentially can be phrased as solving an inverse moments problem, and the work in [6] provides a unifying viewpoint for computationally efficient estimation for many of these models under a tensor decomposition perspective. In general, this body of work has focussed on the computational issues and has considered these questions in the regime in which the amount of data is plentiful—well above the information theoretic limits.\nSublinear Sample Testing and Estimation. In contrast to the work described in the previous section on efforts to devise computationally efficient algorithms for tackling complex structural settings in the “over–sampled” regime, there is also significant work establishing information theoretically optimal algorithms and (matching) lower bounds for estimation and distributional hypothesis testing in the most basic setting of independent samples drawn from (unstructured) distributions. This work includes algorithms for estimating basic statistical properties such as entropy [43, 26, 47, 49], support size [44, 47], distance between distributions [47, 49, 48], and various hypothesis tests, such as whether two distributions are very similar, versus significantly different [11, 42, 50, 15], etc. While many of these results are optimal in a worst-case (“minimax”) sense, there has also been recent progress on instance optimal (or “competitive”) estimation and testing, e.g. [3, 4, 50], with stronger information theoretic optimality guarantees. There has also been significant work on these tasks in “simply structured” settings, e.g. where the domain of the distribution has a total ordering or where the distribution is monotonic or unimodal [16, 12, 30, 21]."
    }, {
      "heading" : "2 Outline of our estimation algorithm",
      "text" : "In this section, we sketch the outline of our algorithm and explain the intuition behind the key ideas. Given N samples drawn according to the probability matrix B. Let BN denote the matrix of empirical counts, and let 1NBN denote the average. By the Poisson assumption on sample size, we have that [BN ]i,j ∼ Poi(NBi,j).\nFirst, note that it is straightforward to obtain an estimate ρ̂ which is close to the true marginal ρ with accuracy in `1 norm with sample complexity N = Ω(M). Also, recall that B − ρρ> = ∆∆> as per (4), hence after subtracting off the (relatively accurate) rank 1 matrix of marginals, we are essentially left with a rank 1 matrix recovery problem. Our algorithm seeks to accurately perform this rank-1 decomposition using a linear sample size, N = Θ(M).\nBefore introducing our algorithm, let us consider the naive approach of estimating ∆ by taking the rank-1 truncated SVD of the matrix ( 1NBN − ρ̂ρ̂>), which concentrates to ∆∆> in spectral distance asymptotically. Unfortunately, this approach leads to a sample complexity as large as Θ(M2 logM). In the linear sample size regime, the empirical counts matrix is a poor representation of the underlying distribution. Intuitively, due to the high sampling noise, the rows and columns of BN corresponding to words with larger marginal probabilities have higher row and column sums in expectation, as well as higher variances that undermine the spectral concentration of the matrix as a whole. This\nobservation leads to the idea of pre-scaling the matrix so that every word (i.e. row/column) is roughly of unit variance. Indeed, with a slight modification of the truncated SVD, we can improve the sample complexity of this approach to Θ(M log(M)), which is nearly linear. It is, however, not obvious how to further improve this. Appendix E provides a detailed analysis of these aforementioned truncated SVD approaches.\nNext, we briefly discuss the important ideas of our algorithm that lead to the linear sample complexity. Our algorithm consists of two phases: For a small constant 0, Phase I of our algorithm returns estimates ρ̂ and ∆̂ both up to 0 accuracy in `1 norm with Θ(M) samples; After the first phase gets us off the ground, Phase II builds upon the output of Phase I to refine the estimation to any target accuracy with Θ(M/ 2) samples. The outline of the two phases are given in Algorithm 1 and Algorithm 2, separately, and the detailed analysis of the algorithms are deferred to Section 3 and 4.\nThe guarantees of the main algorithm follows immediately from Theorem 2.1 and 2.2.\nPhase I: “binning” and “regularization” In Section 1, we drew the connection between our problem and the community detection problem in sparse random graphs. Recall that when the word marginals are roughly uniform, namely all in the order of O( 1M ), the linear sample regime corresponds to the stochastic block model setup where the expected row / column sums are all in the order of d0 = N M = Ω(1). It is well-known that in this sparse regime, the adjacency matrix, or the empirical count matrix BN in our problem, does not concentrate to the expectation matrix in the spectral distance. Due to some heavy rows with row sum in the order of Ω( logMlog logM ), the leading eigenvectors are polluted by the local properties of these heavy nodes and do not reveal the global structure of the graph, which are precisely the desired information in expectation.\nIn order to enforce spectral concentration in the linear sample size regime, one of the many techniques is to tame the heavy rows and columns by setting them to 0. This simple idea was first introduced by [23], and followed by analysis works in [22] and many others. Recently in [33] and [34] the authors provided clean and clever proofs to show that any such “regularization” essentially leads to better spectral concentration for the adjacency matrix of random graphs whose row/column sums are roughly uniform in expectation.\nIs it possible to leverage such “regularization” ideas in our problem where the marginal probabilities are not uniform? A natural candidate solution would be to partition the vocabulary M into bins of words according to the word marginals, so that the words in the same bin have roughly uniform marginals. Restricting our attention to the diagonal blocks of B whose indices are in the same bin, the expected row / column sums are indeed roughly uniform. Then we can regularize each diagonal block separately to guarantee spectral concentration, to which truncated SVD should then apply. Figure 1a visualizes the two operations of “binning” and “regularization”.\nEven though the “binning” idea seems natural, there are three concerns one needs to rigorously address in order to implement the idea:\n1. We do not have access to the exact marginal ρ. With linear sample size, we only can estimate ρ up to constant accuracy in `1 norm. If we implement binning according to the empirical marginals, there is considerable probability that words with large marginals are placed in a bin intended for words with small marginals — which we call “spillover effect”. When directly applied to the empirical bins with such spillover, the existing results of “regularization” in [34] do not lead to the desired concentration result.\n2. When restricting to each diagonal block corresponding to a bin, we are throwing away all the sample counts outside the block. This greatly reduces the effective sample size, and it is not obvious that we retain enough samples in each diagonal block to guarantee meaningful concentration results and subsequent estimation. This is particularly worrying because we may use a super-constant number of bins, and hence throw away all but a subconstant fraction of data for some words.\nInput: 2N sample counts.\nOutput: Estimator ρ̂, ∆̂, B̂.\nDivide the sample counts into two independent batches of equal size N , and construct two empirical count matrices BN1 and BN2.\n1. (Estimate the marginal) ρ̂i = 1 N ∑ j∈[M ][BN1]i,j .\n2. (Binning) Partition the vocabulary M into: Î0 = { i : ρ̂i <\n0 M } , Îlog = { i : ρ̂i > log(M) M } , Îk = { i : e k M ≤ ρ̂i ≤ e k+1 M } , k = 1 : log log(M).\n3. (Estimate the separation ∆̂)\n(a) (Estimate ∆̂Îlog) (up to sign flip)\nIf ∑ ρ̂Îlog < 0, set ∆̂Îlog = 0, else\ni. (Rescaling): Set E = diag(ρ̂Îlog) −1/2[BN2 − ρ̂ρ̂>]Îlog×Îlogdiag(ρ̂Îlog) −1/2.\nii. (t-SVD): Let ulogu > log be the rank-1 truncated SVD of E.\niii. Set vlog = diag(ρ̂Îlog) 1/2ulog.\n(b) (Estimate ∆̂Îk) (up to sign flip)\nIf ∑ ρ̂Îk < 0e −k, set ∆̂Îk = 0, else\ni. (Regularization): Set B̃ = [BN2]Îk×Îk , set d max k = N( ∑ ρ̂Îk) ek+τ M .\nIf a row/column of B̃ has sum larger than 2dmaxk , set the entire row/column to 0. Set E = (B̃ − ρ̂Îk ρ̂ > Îk ).\nii. (t-SVD): Let vkv > k be the rank-1 truncated SVD of E.\n(c) (Estimate ∆̂Î0) Set ∆̂Î0 = 0. (d) (Stitching ∆̂Îk ’s ) Fix k ∗ = arg maxk ‖vk‖, set ∆̂Î∗k = vk∗ .\nFor all k, define I+k = {i : i ∈ Ik : ∆̂i > 0} and I−k = Ik\\I+k .\nSet ∆̂Îk = vk if\n∑ i∈I+\nk∗ ,j∈I + k [BN2]i,j∑ i∈M,j∈I+\nk [BN2]i,j\n>\n∑ i∈I+\nk∗ ,j∈I − k [BN2]i,j∑ i∈M,j∈I−\nk [BN2]i,j\n,\nand ∆̂Îk = −vk otherwise.\n4. Return ρ̂, ∆̂, and B̂ = ρ̂ρ̂> + ∆̂∆̂>.\nAlgorithm 1: Phase I\n3. Even if the “regularization” trick works for each diagonal block, we need to extract the useful information and “stitch” together this information from each block to provide an estimator for the entire matrix, which includes the off-diagonal blocks.\nPhase I (Algorithm 1) capitalizes on these natural ideas of “binning” and “regularization”, and avoids the above potential pitfalls. We show that the algorithm satisfies the following guarantees:\nInput: Estimator ρ̂ and ∆̂ from Phase I. N sample counts.\nOutput: Refined estimator ρ̂, ∆̂, B̂.\n1. (Construct anchor partition)\nSet A = φ. For k = 1, . . . , log logM, log: If ‖∆̂Îk‖2 ≤ ( √ dmaxk N ) 1/2, skip the bin, else, set A = A ∪ {i ∈ Îk : ∆̂i > 0}.\n2. (Estimate anchor matrix)\nSet BA =\n[ ∑ i∈A,j∈A[BN ]i,j ∑ i∈A,j∈Ac [BN ]i,j∑\ni∈Ac,j∈A[BN ]i,j ∑ i∈Ac,j∈Ac [BN ]i,j\n] . Set vector b = [ ∑ i∈A,j∈M[BN ]i,j∑ i∈Ac,j∈M[BN ]i,j ] .\nSet aa> to be rank-1 truncated SVD of the 2× 2 matrix (BA − bb>).\n3. (Refine the estimation:)\nSet\n[ ρ̂>\n∆̂>\n] = [a, b]−1 [ ∑ i∈A[BN ]i,M∑ i∈Ac [BN ]iM ]\n4. (Return) ρ̂, ∆̂ and B̂ = ρ̂ρ̂> + ∆̂∆̂>.\nAlgorithm 2: Phase II\nTheorem 2.1 (Main theorem 1. Θ(M) sample complexity for achieving constant accuracy in `1 norm). Fix 0 to be a small constant. Given N = Θ(M) samples, with large probability, Phase I (Algorithm 1) estimates ρ and ∆ up to 0 accuracy in `1 norm, namely\n‖ρ̂− ρ‖1 < 0, ‖∆̂−∆‖1 < 0, ‖B̂ − B‖1 < 0.\nPhase II: “Anchor partition” Under the Assumption of Ω(1) separation, Phase II of our algorithm makes use of the estimates of ∆ computed by Phase I, to refine the estimates of ρ and ∆.\nThe key to this refining process is to construct an “anchor partition”, which is a bi-partition of the vocabulary M based on the signs of the estimate of separation ∆̂ given by Phase I. We collapse the M ×M matrix BN into a 2× 2 matrix corresponding to the bi-partition, and accurately estimate the 2 × 2 matrix with the N samples. Given this extremely accurate estimate of this 2 × 2 anchor matrix, we can now iteratively refine our estimates of ρi and ∆i for each word i by solving a simple least square fitting problem.\nThe above description may seem opaque, but similar ideas — estimation refinement based on some crude global information — has appeared in many works for different problems. For example, in a recent paper [19] on community detection, after obtaining a crude classification of nodes using spectral algorithm, one round of a “correction” routine is applied to each node based on its connections to the graph partition given by the first round. This refinement immediately leads to an optimal rate of recovery. Another example is given in [18] in the context of solving random quadratic equations, where local refinement of the solution follows the spectral method initialization. Figure 1b visualize the example of community detection. In our problem, the nodes are the M words, the edges are the sample counts, and instead of re-assigning the label to each node in the refinement routine, we re-estimate the ρi and ∆i for each word i.\nTheorem 2.2 (Main theorem 2. Θ(M/ 2) sample complexity to achieve accuracy in `1 norm). Assume that B satisfies the Ω(1) separation assumption. Given N samples, with large probability, Phase II of our algorithm (Algorithm 2) estimates ρ and ∆ up to accuracy in `1 norm:\n‖ρ̂− ρ‖1 < O( √ M\nN ), ‖∆̂−∆‖1 < O(\n√ M\nN ), ‖B̂ − B‖1 < O(\n√ M\nN )."
    }, {
      "heading" : "3 Algorithm Phase I, achieving constant 0 accuracy",
      "text" : "In this section, we outline the proof for Theorem 2.1, the detailed proofs are provided in Section A and B in the appendix.\nWe denote the ratio between sample size and the vocabulary size by\nd0 = N\nM . (5)\nThroughout our discussion for Phase I algorithm, we assume that d0 = Θ(1) to be some fixed large constant.\nGiven N samples, the goal is to estimate the word marginal vector ρ as well as the dictionary separation vector ∆ up to constant accuracy in `1 norm. We denote the estimates by ρ̂ and ∆̂. Also, we estimate the underlying probability matrix B with\nB̂ = ρ̂ρ̂> + ∆̂∆̂>.\nNote that since ‖∆‖1 ≤ ‖ρ‖1 = 1, constant `1 norm accuracy in ρ̂ and ∆̂ immediately lead to constant accuracy of B̂ also in `1 norm.\nFirst, we show that it is easy to estimate the marginal probability vector ρ up to constant accuracy.\nLemma 3.1 (Estimate the word marginal probability ρ). Given the empirical count matrix BN1 constructed with the first batch of N sample counts, consider the estimate of the marginal probabilities:\nρ̂i = 1\nN\n∑ j∈M [BN1]i,j . (6)\nWith large probability, we can bound the estimation accuracy by:\n‖ρ̂− ρ‖1 ≤ O( √ 1\nd0 ). (7)\nThe hard part is to estimate the separation vector ∆ up to constant accuracy in `1 norm with linear number of sample counts, namely d0 = Θ(1). Recall that naively taking the rank-1 truncated SVD of ( 1NBN − ρ̂ρ̂>) fails to reveal any information about ∆∆>, since in the linear sample size regime, the leading eigenvectors of BN are mostly dominated by the statistical noise of the words with large marginal probabilities. Our Phase I algorithm achieves this with more delicate steps. We analyze each step in the next 5 subsections, structured as follows:\n1. In Section 3.1, we introduce the binning argument and the necessary notations for the rest of the section. We bin the vocabularyM according to the estimates of word marginals, i.e. ρ̂, and we call a bin heavy or light according to the typical word marginals in that bin.\n2. In Section 3.2 we analyze how to estimate the entries of ∆ restricted to the heaviest bin (up to some common sign flip). Because the marginal probabilities of words in this bin are sufficiently large, truncated SVD can be applied to the properly scaled diagonal block of the empirical average count matrix.\n3. In Section 3.3 we analyze how to estimate the entries of ∆ restricted to all the other bins (up to some common sign flip), by examining the corresponding diagonal blocks in the empirical average count matrix.\nThe main challenge here is that due to the estimation error of the word marginal vector ρ̂, the binning is not perfect, in the sense that a lighter bin may include some heavy words by chance. Lemma 3.4 is the key technical lemma, which shows that with high probability, such spillover effect is very small for all bins with high probability. Then we leverage the clever proof techniques from [34] to show that if spillover effect is small, regularized truncated SVD can be applied to estimate the entries of ∆ restricted to each bin.\n4. In Section 3.4, we analyze the lightest bin.\n5. In Section 3.5 we show how to fix the sign flips across different bins, by using the off-diagonal blocks, so that we can concatenate different sections of ∆̂ to obtain an estimator for the entire separation vector ∆."
    }, {
      "heading" : "3.1 Binning according to empirical marginal distribution",
      "text" : "Instead of tackling the empirical count matrix BN as a whole, we focus on its diagonal blocks and analyze the spectral concentration restricted to each block separately. Since the entries Bi,j restricted to each diagonal block are roughly uniform, the block hopefully concentrates well, so that we can estimate segments of the separation vector ∆ by using truncated SVD with the “regularization” trick.\nFor any set of words I, we use [BN ]I,I to denote the diagonal block of BN whose indices are in the set I. Note that when restricting to the diagonal block, the rank 2 decomposition in (4) is given by BI,I = ρIρ>I + ∆I∆>I .\nEmpirical binning We partition the vocabulary M according to the empirical marginal ρ̂ in (6):\nÎ0 = { i : ρ̂i <\n0 M\n} , Îk = { i : ek−1\nM ≤ ρ̂i ≤\nek\nM\n} , Îlog = { i : ρ̂i > log(M)\nM\n} .\nWe call this empirical binning to emphasize the dependence on the empirical estimator ρ̂, which is a random variable built from the first batch of N sample counts. We call Î0 the lightest empirical bin, and Îlog the heaviest empirical bin, and Îk for 1 ≤ k ≤ log logM the moderate empirical bins.\nFor the analysis, we further define the exact bins according to the exact marginal probabilities:\nI0 = { i : ρi <\n0 M\n} , Ik = { i : ek−1\nM ≤ ρi ≤\nek\nM\n} , Ilog = { i : ρi > log(M)\nM\n} . (8)\nSpillover effect As N increases asymptotically, we have Îk coincides with Ik for each k. However, in the linear sample size regime, the estimation error in ρ̂ cause the following two spillover effects:\n1. Words from the heavy bin Ik′ , for k′ much larger than k, are placed in the empirical bin Îk.\n2. Words from the exact bin Ik escape from the corresponding empirical bin Îk;\nThe hope, that we can have good spectral concentration in each diagonal block [BN2]Îk×Îk , crucially relies on the fact that the entries Bi,j restricted to this block are roughly uniform. However, the hope may be ruined by the spillover effects, especially the first one. In the following sections, we show that with high probability the spillover effects are small for all empirical bins of considerable probability mass, in particular:\n1. The total marginal probability of the words in the empirical bin Îk, that are from faraway exact bins, namely ∪{k′:k′≥k+τ}Ik′ , is small and in the order of O(e−e kd0) (see Lemma 3.4).\n2. Most words of Ik stays within the nearest empirical bins, namely ∪{k′:k−τ≤k′≤k+τ}Îk′ , (see Lemma 4.5).\nThroughout the discussion, we fix some small constant number τ to be:\nτ = 1 (9)\nNotations To analyze the spillover effects, we define some additional quantities. We define the total marginal probability mass in the empirical bins to be:\nWk = ∑\ni∈Îk\nρi, (10)\nand let Mk = |Îk| denote the total number of words in the empirical bin. We also define Ŵk = ∑\ni∈Îk ρ̂i.\nWe use Ĵk to denote the set of the words from much heavier bins that are spilled over into the empirical bin Îk (recall that τ is a small constant):\nĴk = Îk ∩ (∪{k′:k′≥k+τ}Ik′), (11)\nand let L̂k denote the “good words” in the empirical bin Îk:\nL̂k = Îk\\Ĵk. (12)\nwhere τ We also denote the total marginal probability mass of the heavy spillover words Ĵk by:\nW k = ∑\ni∈Ĵk\nρi. (13)\nNote that these quantities are random variables determined by the randomness of the first batch of N samples, via the empirical binning. These are fixed quantities when we consider the empirical count matrix with the second batch of N samples.\nDefine the upper bound of the “typical” word marginal in the k-th empirical bin to be:\nρk = e k+τ/M,\nNote that since wp, wq ≥ Cw, we can bound each entry in the true matrix by the product of marginals as\nBi,j ≤ 2\nC2w ρiρj , ∀i, j.\nLet dmaxk denote the expected max row/column sum of the diagonal block corresponding the k-th bin:\ndmaxk = 2\nC2w NWkρk. (14)"
    }, {
      "heading" : "3.2 Estimate ∆ restricted to the heaviest empirical bin",
      "text" : "First, we show that the empirical marginal probabilities restricting the heaviest bin concentrate much better than what Lemma 3.1 implies.\nLemma 3.2 (Concentration of marginal probabilities in the heaviest bin). With high probability, for all the words with marginal probability ρi ≥ log(M)/M , we have that for some universal constant C1, C2,\nC1 ≤ ρ̂i/ρi ≤ C2. (15) Lemma 3.2 says that with high probability, we can estimate the marginal probabilities for every words in the heaviest bin with constant multiplicative accuracy. Note that it also suggests that actually we do not need to worry about the spillover effect of the words from Ilog placed in much lighter bins, since with high probability, all the words stay in the bin Îlog and a constant number of adjacent empirical bins.\nNext two lemmas show that with square root re-scaling, truncated SVD works to estimate the segment of separation restricted to the empirical heaviest bin. Lemma 3.3 (Estimate ∆ restricted to the heaviest empirical bin). Suppose that Ŵlog = ∑ ρ̂Îlog > 0. Define D̂Îlog = diag(ρ̂Îlog), and consider the diagonal block corresponding to the heaviest empirical bin Îlog:\nE = D̂ −1/2 Îlog ( 1 N [BN2]Îlog,Îlog − ρ̂Îlog ρ̂ > Îlog )D̂ −1/2 Îlog . (16)\nLet uu> denote the rank 1 truncated SVD of matrix E, set vlog = D̂ 1/2\nÎlog u. With high probability over\nthe second batch of N samples, we can estimate ∆Îlog , the dictionary separation vector restricted to the heaviest empirical bin, with vlog up to sign flip with accuracy:\nmin{‖∆Îlog − vlog‖1, ‖∆Îlog + vlog‖1} = O ( min { 1/d 1/2 0\n‖∆Îlog‖1 , 1/d\n1/4 0\n}) . (17)\nThe two cases in the above bound correspond to whether the separation is large or small, compared to the statistical noise from sampling, which is in the order 1/d 1/4 0 . If the bin contains a large separation, then the bound follows the standard Wedin’s perturbation bound; if the separation is small, i.e. ‖∆Îlog‖1 1/d 1/4 0 , then the bound 1/d 1/4 0 just corresponds to the magnitude of the statistical noise.\n3.3 Estimate ∆ restricted to the log log(M) moderate empirical bins\nIn this section, we show that the spillover effects are small for all the moderate bins (Lemma 3.4). In particular, we upper bound the total spillover marginal W k for all k with high probability. Provided that the spillover effects are small, we show that (Lemma 3.5 and Lemma 3.6) truncated SVD with regularization can be applied to each diagonal block of the empirical count matrix BN2, to estimate the entries of the separation vector ∆ restricted to each bin. The proofs of this section are provided in Section B in the appendix.\nLemma 3.4 (With high probability, spillover from much heavier bins is small). With high probability over the first batch of N sample counts, for all empirical bins {Î0, Î1, . . . Îlog log(M)}, we can bound the total marginal probability of spillover from much heavier bins, i.e. W k defined in (13), by:\nW k ≤ 2e−e τ+kd0/2. (18)\nAlso, if Wk > 0e −k, we can bound the number of spillover words Mk by:\nMk ≤Mk/dmaxk . (19)\nRecall that τ is set in (9), Wk is defined in (10), and d max k = NWkρk is defined in (14).\nNow consider [BN2]Îk×Îk , the diagonal block corresponding to the empirical bin Îk. To ensure the spectral concentration of this diagonal block, we “regularize” it by removing the rows and columns with very large sum. The spectral concentration of the block with the remaining elements leads to an estimate of the separation vector ∆Îk restricted to L̂k, the set of “good words” defined in (12). To make the operation of “regularization” more precise, we need to introduce some additional notations.\nDefine ρ̃k to be a vector with the same length as ρÎk , with the same entries for the good words,\nand set the entries corresponding to the spillover set Ĵk to be 0, namely\n(ρ̃k)i = ρi1 [ i ∈ L̂k ] .\nSimilarly define vector ∆̃k to be the separation restricted to the good words in the empirical bins:\n(∆̃k)i = ∆i1 [ i ∈ L̂k ] . (20)\nWe define the matrix B̃k (of the same size as [BN2]Îk×Îk):\nB̃k = ρ̃kρ̃>k + ∆̃k∆̃>k . (21)\nRecall that the expected max row sum of the diagonal block is given by dmaxk = NWkρk defined in (14). Let R̂k denote the indices of the rows and columns in [BN2]Îk,Îk whose row sum or column sum are larger than 2dmaxk , namely\nR̂k =   i ∈ Îk : ∑\nj∈Îk\n[BN2]i,j > 2d max k or\n∑\nj∈Îk\n[BN2]j,i > 2d max k    . (22)\nStarting with B̃k = [BN2]Îk×Îk , we set all the rows and columns of B̃k indexed by R̂k to 0. Note that by definition the rows and columns in B̃k and B̃k that are zero-ed out do not necessarily coincide. However, the next lemma shows that B̃k concentrates to B̃k in the spectral distance.\nLemma 3.5 (Concentration of regularized diagonal block B̃k.). Suppose that the marginal of the bin\nÎk is large enough Wk = ∑ ρÎk > 0e\n−k. With high probability at least (1 −M−rk ), where r is some universal constant, we have\n∥∥∥∥ 1\nN B̃k − B̃k ∥∥∥∥ 2 ≤ Cr1.5 √ dmaxk log 2 dmaxk N , (23)\nProof. Detailed proof of this lemma is provided in Section B in the appendix. Here we highlight the key steps of the proof.\nIn Figure 2, the rows and the columns of [BN ]Îk×Îk are sorted according to the exact marginal probabilities of the words in ascending order. The rows and columns that are set to 0 by regularization are shaded. Consider the block decomposition according to the good words L̂k and the spillover words Ĵk. We bound the spectral distance of the 4 blocks (A1, A2, A3, A4) separately. The bound for the entire matrix B̂k is then an immediate result of triangle inequality.\nFor block A1 whose rows and columns all correspond to the “good words” with roughly uniform marginals, we show its concentration by applying the result in [34]. For block A2 and A3, we want to show that after regularization the spectral norm of these two blocks are small. Intuitively, the expected row sum and the expected column sum of block A2 are bounded by 2d max k and 2d max k Wk Wk = O(1), as a result of the bound on the spillover words W k in Lemma 3.4. Therefore the spectral norm of the block are likely to be bounded by O( √ dmaxk ), which we show rigorously with high probability arguments. Lastly for block A4, which rows and columns all correspond to the spillover words. We show that the spectral norm of this block is very small as a result of the small spillover marginal W k.\nIt is also important to note that given Wk > 0e −k and conditional on the high probability even\nthat W k ≤ 2e−e kd0 , we can write dmaxk also as d max k = NMkρ 2 k.\nLemma 3.6 (Given spectral concentration of block B̃k, estimate the separation ∆̃k). Suppose that the\nmarginal of the bin Îk is large enough Wk = ∑ ρÎk > 0e −k. Let vkv > k be the rank-1 truncated SVD\nof the regularized block (\n1 N B̃k − ρ̂Îk ρ̂ > Îk\n) . With high probability over the second batch of N samples,\nmin{‖∆̃k − vk‖2, ‖∆̃k + vk‖2} = O  min    √ dmaxk log 2 dmaxk N\n1\n‖∆Îk‖2 ,\n(√ dmaxk log\n2 dmaxk N )1/2    .\n(24)\nNamely vk is an estimate of the vector ∆̃k (defined as in (21)), up to some unknown sign flip."
    }, {
      "heading" : "3.4 Estimate ∆ restricted to the lightest empirical bin.",
      "text" : "Claim 3.7 (Estimate separation restricted to the lightest bin). Setting ∆̂Î0 = 0 only incurs an `1 error of a small constant, and this is simply because the total marginal of words in the lightest bin with high probability can be bounded by a small constant:\n‖∆Î0‖1 ≤ ‖ρÎ0‖1 ≤ ‖ρI0‖1 +W 0 ≤ 0 M M + e−d0/2 = O( 0),\nwhere we used the assumption that d0 ≥ 1/ 40."
    }, {
      "heading" : "3.5 Stitching the segments of ∆̂ to reconstruct an estimation of the matrix",
      "text" : "Given vk for all k as estimation for ∆Îk ’s up to sign flips. Fix k ∗ to be one good bin (with large bin marginal and large separation). Partition the words into two groups I+k∗ = {i : i ∈ Ik∗ : ∆̂i > 0} and I−k∗ = Ik∗\\I+k∗ . Without loss of generality assume that ∑ i∈I+\nk∗ ∆̂i ≥\n∑ i∈I−\nk∗ ∆̂i. We set ∆̂Îk∗ = vk∗ .\nFor all other good bins k, we similarly define I+k and I−k . The next claim shows how to determine the relative sign flip of vk∗ and vk.\nClaim 3.8 (Pairwise comparison of bins to fix sign flips). For all good bins k ∈ G, we can fix the sign flip to be ∆̂Îk = vk if:\n∑ i∈I+\nk∗ ,j∈I + k [BN2]i,j ∑\ni∈M,j∈I+k [BN2]i,j\n>\n∑ i∈I+\nk∗ ,j∈I − k [BN2]i,j ∑\ni∈M,j∈I−k [BN2]i,j\n,\nand ∆̂Îk = −vk otherwise.\nProof. This claim is straightforward. When restricted to the good bins, the estimates vk are accurate enough. We can determine that the sign flips of k∗ and k are consistent if and only if the conditional distribution of the two word tuple (x, y) ∈ M2 satisfies Pr(x ∈ I+k∗ ∣∣x ∈ I+k ) > Pr(x ∈ I+k∗ ∣∣x ∈ I−k ), and we should revert vk otherwise.\nFinally, concatenate the segments of ∆̂, we can summarize to bound the overall estimation error in `1 norm:\nLemma 3.9 (Accuracy of ∆̂ of Phase I). For fixed 0 = Ω(1) to be a small constant, if d0 =: N/M ≥ 1/ 40, with large probability, Phase I algorithm can estimate the separation vector ∆ with constant accuracy in `1 norm:\n‖∆̂−∆‖ = O( 0)."
    }, {
      "heading" : "4 Algorithm Phase II, achieving arbitrary accuracy",
      "text" : "Given ρ̂ and ∆̂ from the Phase I of our algorithm. Under the Ω(1) separation assumptions, we refine the estimation to achieve arbitrary accuracy in Phase II. In this section, we verify the steps of Algorithm 2, and show the correctness of Theorem 2.2."
    }, {
      "heading" : "4.1 Construct an anchor partition",
      "text" : "Imagine that we have a way to group the M words in the vocabulary into a new vocabulary with a constant number of superwords, and similarly define marginal vector ρA and separation vector ∆A over the superwords. The new probability matrix (of constant size) corresponds to we sum over the rows/columns of the matrix B according to the grouping. If we group the words in a way such that ∆A still has Ω(1) dictionary separation, then with N = Ω(M) samples we can estimate the constantdimensional vector ρA and ∆A to arbitrary accuracy (as M 1). Note that accurate estimates of ρA and ∆A defined over the superwords give us crude “global information” about the true ρ and ∆. Now sum the empirical BN over the rows accordingly, and leave the columns intact, it is easy to recognize that the expected factorization is given by ρAρ > + ∆A∆ >. Therefore, given accurate ρA and ∆A, refining ρ̂ and ∆̂ is as simple as solving a least square problem. We formalize this argument and introduce the definition of anchor partition below.\nDefinition 4.1 (Anchor partition). Consider a partition of the vocabulary into (A,Ac). denote ρA =∑ i∈A ρi and ∆A = ∑ i∈A∆i. We call it an anchor partition if for some constant CA = O(1),\ncond\n([ ρA, ∆A\n1− ρA, −∆A\n]) ≤ CA. (25)\nIn this section, we show that if the dictionary separation ‖∆‖1 = Ω(1), the estimator ∆̂ obtained in Phase I with constant `1 accuracy contains enough information for us to construct an anchor partition.\nFirst note that with Ω(1) separation, it is feasible to find a pair of anchor partition. The next lemma state a sufficient condition for constructing an anchor partition.\nLemma 4.2 (Sufficient condition for constructing an anchor partition). Consider a set of words I, let ∆I be the vector of ∆ restricted to the entries in I. Suppose that for some constant C = Ω(1), we have\n‖∆I‖1 ≥ C‖∆‖1, (26)\nand suppose that for some constant C ′ ≤ 13C, we can estimate ∆I up to precision:\n‖∆̂I −∆I‖1 ≤ C ′‖∆I‖1. (27)\nDenote Â = {i ∈ I : ∆̂i > 0}. We have that (Â,M\\Â) forms an anchor partition defined in 4.1.\nConsider the heaviest bin. If Wlog = Ω(1) and ‖∆log‖1 = Ω(1), then Lemma 3.3 shows that we can estimate the portion of ∆ restricted to the heaviest bin to constant `1 norm accuracy, and then Lemma 4.2 above shows how to find an anchor partition with set A as a subset of Îlog.\nIf either Wlog = o(1) or ‖∆log‖1 = o(1), there must be at least a constant fraction of marginal probabilities as well as a constant fraction of separation located in the moderate empirical bins (Îk’s). Recall that we can always ignore the lightest bin. If this is the case, in the following we show how to construct an anchor partition using the moderate empirical bins.\nDefinition 4.3 (Good bin). Denote the sum of dictionary separation restricted to the “good words”(defined in (12)) L̂k by:\nSk = ∑\ni∈L̂k\n|∆i|, for k = 0, . . . , log log(M). (28)\nNote that equivalently we can write Sk = ‖∆̃k‖1. We call an empirical bin Îk to be a “good bin” if for some fixed constant C1, C2 = Ω(1) it satisfies the following two conditions:\n1. the marginal probability of the bin Wk ≥ C1e−k.\n2. the ratio between the separation and the marginal probability of the bin satisfies Sk2Wk ≥ C2.\nThe next Lemma shows that Phase I algorithm provides a good estimate of the separation restricted to the good bins with high probability.\nLemma 4.4 (Estimate the separation restricted to the k-th good bin). Consider the k-th empirical bin Îk with Mk words. If it is a “good bin”, then with high probability, the estimate for the separation vector ∆ restricted to the k-th empirical bin ∆̂Îk = vk given in Lemma 3.6, up to accuracy:\n‖∆̂Îk − ∆̃k‖1 ≤ 1√ d0 ‖∆Îk‖1. (29)\nLet G ⊆ {1, . . . , log log(M)} denote the set of all the “good bins”. Next, we show that there are many good bins when the dictionary separation is large.\nLemma 4.5 (Most words fall in “good bins” with high probability ). Assume that ‖∆Îlog‖1 < 1 2‖∆‖1. Fix constants C1 = C2 = 1 24‖∆‖1. Note that C1 = C2 = Ω(1) by well separation Assumption.\nWith high probability, we can bound the total marginal probability mass in the “good bins” as:\n∑ k∈G Wk ≥ ‖∆‖1 12 . (30)\nBy definition this implies a bound of total separation in the good words in the good bins:\n∑\ni∈L̃k,k∈G\n|∆i| = ∑\nk∈G Sk ≥ 2C2\n∑ k∈G Wk ≥ 1 24 (‖∆‖1)2 = Ω(1). (31)\nLemma 4.5 together with Lemma 4.4 suggest that we can focus on the estimation of separation vector restricted to the “good words” in the “good bins”, namely ∆̃Îk for all k ∈ G. In particular, set I = ∪k∈GL̂k in Lemma 4.2. By Lemma 4.5 we know the separation contained in I is at least∑\nk∈G Sk = C‖∆‖1; moreover by Lemma 4.4, with linear number of samples (large d0) we can estimate ∆I up to constant `1 accuracy. Therefore we can construct a valid anchor partition (A,M\\A) by setting:\nA = {∆̂i > 0 : k ∈ G}.\nIdeally, we need to restrict to the “good words” and set the anchor partition to be {i ∈ L̃k, ∆̂i > 0 : k ∈ G}. However, empirically we do not know which are the “good words” instead of spillover from heavier bins. Luckily, the bound on the total marginal of spillover ∑ kW k = O(e\n−d0) guarantees that even if we mis-classify all the spillover words, it does not ruin the separation in A constructed above."
    }, {
      "heading" : "4.2 Estimate the anchor matrix",
      "text" : "Now consider grouping the words into two superwords according to the anchor partition we constructed. We define the 2× 2 matrix DA = [\nρA, ∆A 1− ρA, −∆A\n] to be the anchor matrix.\nTo estimate the anchor matrix, we just need to accurately estimate two scalar variables ρA and ∆A. Apply the standard concentration bound, we can argue that with high probability,\n‖ [ ∑ i∈A,j∈A[BN ]i,j ∑\ni∈A,j∈Ac [BN ]i,j∑ i∈Ac,j∈A[BN ]i,j ∑ i∈Ac,j∈Ac [BN ]i,j\n] −DAD>A‖ < O(\n1√ N )\nMoreover we have that |∆A| = Ω(1) since (A,Ac) is an anchor partition. Therefore we can estimate ρA and ∆A to accuracy\n1√ N , and when N = Ω(M) and M is asymptotically large, we can essentially\nobtain a close to exact DA."
    }, {
      "heading" : "4.3 Use anchor matrix to estimate dictionary",
      "text" : "Now given an anchor partition of the vocabulary (A,Ac), and given the exact anchor matrix DA which has Ω(1) condition number, refining the estimation of ρi and ∆i for each i is very easy and achieves optimal rate.\nLemma 4.6 (Estimate ρ and ∆ with accuracy in `2 distance). We have that with probability at least 1− δ,\n‖ρ̂− ρ‖ < √ δ/N, ‖∆̂−∆‖ < √ δ/N.\nCorollary 4.7 (Estimate ρ and ∆ in `1 distance). With large probability we can estimate ρ and ∆ in `1 distance:\n‖ρ̂− ρ‖1 < √ Mδ\nN , ‖∆̂−∆‖1 <\n√ M\nN ."
    }, {
      "heading" : "5 Sample complexity lower bounds for estimation VS testing",
      "text" : ""
    }, {
      "heading" : "5.1 Lower bound for estimating probabilities",
      "text" : "We reduce the estimation problem to the community detection for a specific set of model parameters. Consider the following topic model with equal mixing weights, i.e. w = wc = 1/2. For some constant C∆ = Ω(1), the two word distributions are given by:\np = [ 1 + C∆ M , . . . , 1 + C∆ M , 1− C∆ M , . . . , 1− C∆ M ] ,\nq = [ 1− C∆ M , . . . , 1− C∆ M , 1 + C∆ M , . . . , 1 + C∆ M ] .\nThe expectation of the sum of samples is given by\nE[BN ] = N 1 2 (pp> + qq>) = N M2 [ 1 + C2∆ 1− C2∆ 1− C2∆ 1 + C2∆ ] .\nNote that the expected row sum is in the order of Ω(NM ). When N is small, with high probability the entries of the empirical sum BN only take value either 0 or 1, and BN approximately corresponds to a SBM (G(M,a/M, b/M)) with parameter a = NM (1 + C 2 ∆) and b = N M (1− C2∆).\nIf the number of sample document is large enough for any algorithm to estimating the dictionary vector p and q up to `1 accuracy for a small constant , it can then be used to achieve partial recovery in the corresponding SBM, namely correctly classify a γ proportion of all the nodes for some constant γ = C∆ .\nAccording to Zhang & Zhou [52], there is a universal constant C > 0 such that if (a− b)2/(a+ b) < c log(1/γ), then there is no algorithm that can recover a γ-correct partition in expectation. This suggests that a necessary condition for us to learn the distributions is that\n(2(N/M)C2∆) 2\n2(N/M) ≥ c log(C∆/ ),\nnamely (N/M) ≥ c log(C∆/ )/2C4∆. In the well separated regime, this means that the sample complexity is at least linear in the vocabulary size M .\nNote that this lower bound is in a sense a worst case constructed with a particular distribution of p and q, and for other choices of p and q it is possible that the sample complexity can be much lower than that Ω(M)."
    }, {
      "heading" : "5.2 Lower bound for testing property of HMMs",
      "text" : "In this section, we analyze the information theoretical lower bound to achieve the task of testing whether a sequence of observations is indeed generated by a 2-state HMM. This problem is closely related to the problem of, in our general setup in the main text, testing whether the underlying probability matrix of the observed sample counts is of rank 1 or rank 2. Note that in the context of HMM this would be a stronger lower bound, since we permit an estimator to have more information given the sequence of consecutive observations, instead of merely bigram counts.\nTheorem 5.1 (Theorem 1.6 restated). Consider a sequence of N observations from a HMM with two states {+,−} and emission distributions p, q supported on M elements. For asymptotically large M , using a sequence of N = O(M) observations, it is information theoretically impossible to distinguish the case that the two emission distributions are well separated, i.e. ||p− q||1 ≥ 1/2, from the case that both p and q are uniform distribution over [M ], namely a degenerate HMM of rank 1.\nIn order to derive a lower bound for the sample complexity, it suffices to show that given a sequence of N consecutive observed words, for N = o(M), one can not distinguish whether it is generated by a random instance from a class of 2-state HMMs (Definition 1.4) with well-separated emission distribution p and q, or the sequence is simply N i.i.d. samples from the uniform distribution over M, namely a degenerate HMM with p = q.\nWe shall focus on a class of well-separated HMMs parameterized as below: a symmetric transition\nmatrix T = [ 1− t, t t, 1− t ] , where we set the transition probability to t = 1/4; the initial state distribution is πp = πq = 1/2 over the two states sp and sq; the corresponding emission distribution p and q are uniform over two disjoint subsets of the vocabulary, A and M\\A, separately. Moreover, we treat the set A as a random variable, which can be any of the ( M M/2 ) subset of the vocabulary of size M/2, which equal probability 1/ ( M M/2 ) . Note that there is a one to one mapping between the set A and an instance in the class of well-separated HMM. Now consider a random sequence of N words GN1 = [g1, . . . , gN ] ∈ MN . If this sequence is generated by an instance of 2-state HMM denoted by A, the joint probability of (GN1 ,A) is given by:\nPr2(G N 1 ,A) = Pr2(GN1 |A)Pr2(A) = Pr2(GN1 |A) 1( M M/2 ) (32)\nMoreover, given A, since the support of p and q are disjoint over A and M\\A by our assumption, we can perfectly infer about the sequence of hidden states SN1 (G N 1 ,A) = [s1, . . . , sN ] ∈ {sp, sq}N simply by the rule si = sp if gi ∈ A and si = sq otherwise. Thus we have:\nPr2(G N 1 |A) = Pr2(GN1 , SN1 |A) =\n1/2\nM/2\nM∏\ni=2\n(1− t)1[si = si−1] + t1[si 6= si−1] M/2 . (33)\nOn the other hand, if the sequence GN1 is simply i.i.d. samples from uniform distribution overM, its probability is given by\nPr1(G N 1 ) =\n1\nMN . (34)\nWe further define a joint distribution rule Pr1(G N 1 ,A) such that the marginal probability agrees with Pr1(G N 1 ). In particular, we define:\nPr1(G N 1 ,A) = Pr1(A|GN1 )Pr1(GN1 ) ≡\nPr2(G N 1 |A)∑\nB∈( MM/2) Pr2(GN1 |B)\nPr1(G N 1 ), (35)\nwhere we define the conditional probability Pr1(A|GN1 ) using the properties of the 2-state HMM class. The main idea of the proof to Theorem 5.1 is to show that for N = o(M), the total variation distance between Pr1 and Pr2, is small. It follows immediately from the connection between the error bound of hypothesis testing and total variation distance between two probability rules, that if TV (Pr1(G N 1 ),Pr2(G N 1 )) is too small we are not able to test which probability rule the random sequence GN1 is generated according to. The detailed proofs are provided in Appendix D."
    }, {
      "heading" : "A Proofs for Algorithm Phase I (Section 3.1, 3.2, 3.4)",
      "text" : "Proof. (to Lemma 3.1 (Estimate the word marginal probability ρ)) We analyze how accurate the empirical average ρ̂ is. Note that under the assumption of Poisson number of samples, we have ρ̂i ∼ 1NPoi(Nρi), and V ar(ρ̂i) = ρi/N . Apply Markov inequality:\nPr\n( M∑\ni=1\n∣∣∣∣ ρ̂i − ρi√\nρi\n∣∣∣∣ 2 > t ) ≤ M tN ,\nthus probability at least 1− δ, we can bound M∑\ni=1\n∣∣∣∣ ρ̂i − ρi√\nρi\n∣∣∣∣ 2 < M\nNδ = O\n( 1\nd0\n) . (36)\nThen apply Cauchy-Schwatz, we have\nM∑\ni=1\n|ρ̂i − ρi| ≤ ( M∑\ni=1\n√ ρi\n2 M∑\ni=1\n∣∣∣∣ ρ̂i − ρi√\nρi\n∣∣∣∣ 2 )1/2 = O ( 1√ d0 ) .\nProof. (to Lemma 3.2 (Concentration of marginal probabilities in the heaviest bin)) Fix constants C1 = 1 2 and C2 = 10, apply Corollary F.5 of Poisson tail (note that Nρi > d0 logM is very large), the probability that ρ̂i concentrates well is given by:\nPr(C1Nρi < Poi(Nρi) < C2Nρi) ≥ 1− 4e−Nρi/2 ≥ 1− 4e−N logM/(2M),\nwhere we used the fact that ρi ≥ log(M)/M in the heaviest bin. Note that the number of words in the heaviest bin is upper bounded by Mlog ≤ 1mini∈Ilog ρi ≤ M log(M) . Take a union bound, we have that with high probability, all the estimates ρ̂i’s in the heaviest bin concentrate well:\nPr(∀i ∈ Ilog : C1ρi < ρ̂i < C2ρi) ≥ 1− M logM e−N logM/(2M)\n≥ 1− 4e−N logM/(2M)+logM−log logM\n≥ 1−M−(d0/2−1) ≥ 1−M−1,\nsince we pick d0 to be a large constant and d0 > 4.\nProof. (to Lemma 3.3 (Estimate the dictionary separation restricted to the empirical heaviest bin)) (1) First, we claim that with high probability, no word from Ik for k ≤ log(M)−e2 is placed in ÎlogM . Namely all the words in Îlog have true marginals at least e −e2 logM M , in the same order of Ω( logM M ).\nThis is easy to show, by the Corollary F.5 of Poisson tail bound, each of the word from the much lighter bins is placed in ÎlogM with probability less than 2e−N logM/M . Take a union bound over all words with marginal at least 1/M , we can bound the probability that any of the words being placed in ÎlogM by 2Me−d0 logM = O(M−d0+1). In the following we will just condition on this high probability event and do not consider the words from much lighter bins spillover in to the heaviest empirical bin ÎlogM .\n(2) The appropriate scaling with the diagonal matrix diag(ρ̂Îlog) −1/2 on both sides of the diagonal block is very important, which allows us to apply matrix Bernstein inequality at a sharper rate. Note that with the two independent batches of samples, the empirical count matrix BN2 is independent from the empirical marginal vector ρ̂. Thus for every fixed realization of ρ̂, we apply Bernstein matrix concentration we have that with probability at least 1−M−1,\n‖D̂−1/2 Îlog ( 1 N [BN ]Îlog,Îlog − BÎlog,Îlog)D̂ −1/2 Îlog ‖2 ≤\n√ log(Mlog/δ)\nN +\nM log(M) log(Mlog/δ)\nN\n= O\n( M\nN (1 +\nlog(1/δ)\nlog(M) )\n)\n= O\n( M\nN\n) ,\nwhere we used the fact that the all the marginals in the heaviest bin can be estimated with constant multiplicative accuracy, namely C1 < ρ̂i/ρi < C2 for all i ∈ Îlog, given by Lemma 3.2; also, note that compared to the Bernsten matrix inequality in (47) without scaling, here with the proper scaling we have V ar ≤ 1 and B ≤ Mlog(M) , since ρ̂i > log(M)/M for all i ∈ Îlog. We will show that [BN ]Îlog,Îlog , the diagonal block of the empirical count matrix, concentrates well enough to ensure that we can estimate the separation restricted to the heaviest bin by the leading eigenvector of ([BN ]Îlog,Îlog − ρ̂Îlog ρ̂ > Îlog ).\nNote that\nD̂ −1/2 Îlog BÎlog,ÎlogD̂ −1/2 Îlog = D̂ −1/2 Îlog ρÎlog(D̂ −1/2 Îlog ρÎlog) > + D̂ −1/2 Îlog ∆′Îlog (D̂ −1/2 Îlog ∆Îlog) >.\nApply triangle inequality we have\n‖D̂−1/2 Îlog ( 1 N [BN ]Îlog,Îlog − ρ̂Îlog ρ̂ > Îlog )D̂ −1/2 Îlog − D̂−1/2 Îlog ∆′Îlog (D̂ −1/2 Îlog ∆Îlog) >‖2\n≤ ∥∥∥∥D̂ −1/2 Îlog ( 1 N [BN ]Îlog,Îlog − BÎlog,Îlog)D̂ −1/2 Îlog ∥∥∥∥ 2 + ∥∥∥∥D̂ −1/2 Îlog (ρ̂Îlog ρ̂ > Îlog − ρÎlogρ > Îlog )D̂ −1/2 Îlog ∥∥∥∥ 2\n=O\n( M\nN\n) + √ M\nN\n=O\n(√ M\nN\n) ,\nwhere in the last equality we used the fact that d0 = N/M is some large constant. (3) Let uu> be the rank-1 truncated SVD of D̂ −1/2 Îlog ( 1N [BN ]Îlog,Îlog − ρ̂Îlog ρ̂ > Îlog )D̂ −1/2 Îlog . Let v = D̂ 1/2 Îlog u be our estimate for ∆Îlog . Apply Wedin’s theorem to rank-1 matrix (Lemma F.1), we can bound the distance between vector u and D̂ −1/2 Îlog ∆ Îlog by:\nmin{‖D̂−1/2 Îlog ∆Îlog − u‖2, ‖D̂ −1/2 Îlog ∆Îlog + u‖2} = O\n min    (M/N)1/2\n‖D̂−1/2 Îlog ∆Îlog‖2 , (M/N)1/4\n     .\nNote that ‖D̂1/2 Îlog 1‖2 = ‖ρ̂1/2Îlog‖2 = 1. Apply Cauchy-Schwatz, for any vector x, we have\n‖D̂−1/2 Îlog x‖2‖D̂1/2Îlog1‖2 ≥ ∣∣∣∣< D̂ −1/2 Îlog x, D̂ 1/2 Îlog 1 > ∣∣∣∣ = ‖x‖1,\ntherefore, we can use ‖D̂−1/2 Îlog x‖2 ≥ ‖x‖1 to bound\nmin{‖∆Îlog − v‖1, ‖∆Îlog + v‖1} ≤ O ( min { (M/N)1/2\n‖∆Îlog‖1 , (M/N)1/4\n}) .\nIn the above inequalities we absorb all the universal constants and focus on the scaling factors.\nProof. (of Lemma 3.8 (Pairwise comparison of bins to fix sign flips))\nProof. (to Lemma 3.9 (Accuracy of ∆̂ in Phase I)) Consider for each empirical bin. If Ŵk < 0e −k set ∆̂Îk = 0. We can bound the total `1 norm error incurred in those bins by 0. Also, for the lightest bin, we can bound the total `1 norm error from setting ∆̂Î0 = 0 by 0 small constant. If Ŵk > 0e −k, we can\napply the concentration bounds in Lemma 3.2, 3.6, and note that ‖∆̂Îk−∆Îk‖1 ≤ √ Mk‖∆̂Îk−∆Îk‖2.\nNote that we need to take a union bound of probability that spectral concentration results holds (Lemma 3.5) for all the bins with large enough marginal. This is true because we have at most log logM bins, and each bin’s spectral concentration holds with high probability (1 − 1/poly(M)), thus even after taking the union bound the failure probability is still inverse poly in M .\nActually throughout the paper the small constant failure probability is only incurred when bounding the estimation error of ρ̂, for the same reason of estimating a simple and unstructured distribution.\nOverall, we can bound the estimation error in `1 norm by:\n‖∆̂−∆‖1 ≤ 0︸︷︷︸ lightest bin + 1/d 1/4 0︸ ︷︷ ︸\nheaviestbin\n+ 0︸︷︷︸ moderate bins with small marginal\n+ ∑\nk\n√ Mk(\n√ dmaxk log\n2 dmaxk N )1/2\n︸ ︷︷ ︸ moderate bins with large marginal\n≤ 2 0 + 1/d1/40 + ∑\nk\n( M2kNWkρk\nN2 )1/4\n≤ 2 0 + 1/d1/40 + eτ ∑\nk\n( W 2kMk N )1/4\n≤ 2 + 1/d1/40 (1 + eτ ) = O( 0)\nwhere in the second last inequality used Cauchy-Schwartz and the factWk ≤ 1, so that ∑ k( W 2kMk N )\n1/4 ≤∑ k( √ Wk √ Mk) 1/2 ≤ (∑kWk ∑ kMk) 1/4 ≤M1/4, and in the last inequality above we use the assumption that d0 =: N/M ≥ 1/ 40."
    }, {
      "heading" : "B Proofs for Algorithm Phase I (Section 3.3)",
      "text" : "Proof. (to Lemma 3.4 (Spillover from much heavier bins is small in all bins)) We define dk = e kd0, which is not to be confused with d max k defined in (14). (1) Consider k′ = k + τ . The probability that a word i from Ik′ falls into Îk is bounded by:\nPr ( N ek−1\nM < Poi(Nρi) < N\nek\nM\n) < Pr ( Poi(N e(τ+k)\nM ) < N\nek\nM\n) ≤ 2e−eτ+kd0/2 (37)\nwhere we apply the Poisson tail bound (1) in Corollary F.5, and set c = e−τ < 1/2 for τ ≥ 1. Note that this bound is doubly exponentially decreasing in τ and exponentially decreasing in d0.\nIn expectation, we can bound W k by:\nEW k = E ∑\ni∈M ρi\n∑\nk′:k′≥k+τ+1 1[i ∈ Ik′ ] Pr\n( N ek−1\nM < Poi(Nρk′) < N\nek\nM\n)\n≤ ∑\ni∈M ρi Pr\n( Poi(N e(τ+k)\nM ) < N\nek\nM\n)\n≤ 2e−eτ+kd0/2.\nSimilarly, apply the Poisson tail bound (2) in Corollary F.5, and set c′ = eτ ≥ e for τ ≥ 1, we can bound the probability with which a word i from much lighter bins, namely ∪{k′:k′<k−τ}Ik′ , is placed in the empirical bin Îk by:\nPr ( N ek\nM < Poi(Nρi) ≤ N\nek+1\nM\n) ≤ Pr ( Poi(N e(k−τ)\nM ) > N\nek\nM\n) ≤ 2e−ekd0 , (38)\nand bound the total marginal probability by:\nEW k ≤ 2e−e kd0 .\n(2) Next, we apply Bernstein’s bound to get a high probability argument. We show that with high probability, for all the log log(M) bins, we can bound the spillover probability mass by W k ≤ E[W k] + O( 1poly(M)), which implies that asymptotically as the vocabulary size M → ∞, we have W k ≤ 2e−e\nτ+kd0/2 for all k. Consider the word i from the exact bin Ik′ , for some k′ ≥ k + τ . Let\nλi = 2e −ek′d0/2\ndenote the upper bound (as shown in (37)) of the independent probability with which word i is placed in the empirical bin Îk (recall the Poisson number of samples assumption). The spillover probability mass is a random variable and can be written as\nW k = ∑\ni∈Ik′ :(k+τ)< k′ ≤log log(M)\nρiBer(λi),\nNote that the summation of word i is over all the bin Ik′ for (k + τ) ≤ k′ ≤ log log(M), where recall that in in Lemma 3.2 we showed that with high probability the heaviest words are retained in the empirical bin Îlog. Apply Bernstein’s inequality to bound W k:\nPr(W k − EW k > t) ≤ e − t 2/2∑ i ρ 2 i λi+maxi ρit/3 .\nTo ensure that the right hand side is bounded by e− logM (this is to create space for the union bound over the log logM bins), we can fix some large universal constant C and set t to be\nt = 2 ( ( ∑\ni\nρ2iλi) 1/2 + max\ni ρi\n) log(M).\nwhich right hand side can be bounded by:\n(∑\ni\nρ2iλi\n)1/2 + max\ni ρi ≤\n( max\ni∈Ik′ :(k+τ)≤k′≤log log(M) (\n1 ρi )(ρ2i )(2e −ek′d0/2)\n)1/2 + logM\nM\n≤ (\n2 max (k+τ)≤k′≤log log(M)\nek ′ M e−e k′d0/2\n)1/2 + logM\nM\n≤ 2e −ek+τd0/4 √ M + logM M ,\nwhere the first inequality is uses the worst case to bound the summation, and the last inequality uses the fact that d0 = Ω(1) is a large constant. Therefore, we can set t = 2( e−(e\nk+τ d0/4) logM√ M + (logM) 2 M ).\nFinally, take a union bound over at most log log(M) moderate bins, we argue that with high probability (at least 1−O(1/M)), for all the empirical moderate bins, we can bound the spillover marginal by:\nW k ≤ EW k +O( 1\npoly(M) ).\n(3) Moreover, assume that Wk ≥ e−k, we can bound the number of the heavy spillover words Mk compared to number of words in the exact bin Mk.\nFirst note that Mk ≤ Wkeτ+k/M . Recall that dmaxk = NWk(eτ+k/M) was defined in (14). Also, since Wk ≥ e−k W k ≈ e−e k+τd0 , we can lower bound the number of words in the empirical bin Î1 by:\nMk ≥ Wk\nek+τ/M .\nThus we can bound\nMk dmaxk Mk ≤ ( W k ek+τ/M ) (NWk(e k+τ/M))\n( Wk ek+τ/M )\n≤ ek+τd0W k ≤ 2e k+τd0\neek+τd0\n≤ 1,\nwhere the second last inequality we used the high probability upper bound for W k, and in the last inequality we use the fact that ex > 2x for all x.\nProof. (of Lemma 3.5 (Concentration of the regularized diagonal block B̃k.)) In Figure 2, the rows and the columns of [BN ]Îk×Îk are sorted according to the exact marginal probabilities of the words in ascending order. The rows and columns that are set to 0 by regularization are shaded.\nOn the left hand side, it is the empirical matrix without regularization. We denote the removed elements by matrix E ∈ RMk×Mk+ , whose only nonzero entries are those that are removed from in the regularization step (in the strips with orange color), namely E = [ [BN2]i,j1[i or j ∈ R̂k] ] . We denote\nthe retained elements by matrix B̃k = [BN2]Îk×Îk\\E = [BN2]Îk×Îk − E.\nOn the right hand side it is the same block decomposition applied to the matrix which we want the regularized empirical count matrix converges to. Recall that we defined B̃k = ρ̃kρ̃>k + ∆̃k∆̃>k in (21), where we set entries corresponding to the words in the spillover set Ĵk to 0. Thus by definition, the only non-zero entries of B̃k are equal to those in BL̂k×L̂k .\nWe bound the spectral distance of the 4 blocks (A1, A2, A3, A4) separately. The bound for the\nentire matrix B̂k is then an immediate result of triangle inequality:\n‖B̃k −N B̃k‖ = ‖[BN2]Îk×Îk − E −N B̃k‖ = ‖A1\\E +A2\\E +A3\\E +A4\\E −N B̃k‖ ≤ ‖A1\\E −NBL̂k×L̂k‖+ ‖A2\\E‖+ ‖A3\\E‖+ ‖A4\\E‖.\nWe bound the 4 parts separately below in (a)-(c).\n(a) To bound ‖A1\\E −NBL̂k×L̂k‖, we first make a few observations:\n1. By definition of Ĵk and Îk, every entry of the random matrix A1 is distributed as an independent Poisson variable Poi(λk), where λk ≤ N( e k+τ M ) 2 ≤ d0 log(M)M = o(1).\n2. The expected row sum of A1 is bounded by of d max k .\n3. With the regularization of removing the heavy rows and columns in E, every column sum and the row sum of A1 is bounded by 2d max k .\nTherefore, by applying the Lemma F.6 (an immediate extension of the main theorem in [34]), we can argue that with probability at least 1−M−rk for some constant r = Θ(1),\n‖A1\\E −NBL̂k×L̂k‖2 = O( √ dmaxk ).\n(b) To bound ‖A2\\E‖ and ‖A3\\E‖, the key observations are:\n1. Every row sum of A2\\E and every column sum of A3\\E is bounded by 2dmaxk .\n2. For every non-zero row of A2, its distribution is entry-wise dominated by a multinomial distri-\nbution Mul ( ρL̂k∑ i∈L̂k ρi ; (2dmaxk ) ) , while the entries in E are set to 0, and note that in A2 the columns are restricted to the good words L̂k. Moreover, by the Poisson assumption on N (also in the definition of dmaxk = NWkρk), we have that the distributions of the entries in the row are\nindependently dominated by Poi (\n2dmaxk Mk\n) .\nLemma B.1 (row/column-wise `1 norm to `2 norm bound (Lemma 2.5 in [34])). Consider a matrix B in which each row has L1 norm at most a and each column has L1 norm at most b, then ‖B‖2 ≤ √ ab.\nClaim B.2 (Sparse decomposition of (A2\\E)). With high probability, the index subset Ĵk × L̂k of (A2\\E) can be decomposed into two disjoint subsets R and C such that: each row of R and each column of C has row/column sum at most (r log(dmaxk )).\nRecall that from regularization we know that each column of R and each row of C in A2\\E has column/row sum at most 2dmaxk . Therefore we can apply Lemma B.1 and conclude that with high probability\n‖A2\\E‖2 ≤ 2 √ rdmaxk log(d max k ).\nProof. (to Claim B.2) We sketch the proof of Claim B.2, which mostly follows the sparse decomposition argument in Theorem 6.3 in [34]. We adapt their argument in our setup where the entries are distributed according to independent Poisson distributions. We first show (in (1)) that, with high probability, any square submatrix in (A2\\E) actually contains a sparse column with almost only a constant column sum; then, with this property we can (in (2)) iteratively take out sparse columns and rows from (A2\\E) to construct the R and C. (1) With high probability, in any square submatrix of size m × m in (A2\\E), there exists a sparse column whose sum is at most (r log dmaxk ).\nTo show this, consider an arbitrary column in an arbitrary submatrix of size m ×m in (A2\\E). Recall our observation (b).2, that the column sum is dominated by a Poisson with rate\nλ = 2dmaxk m\nMk .\nTherefore, we can bound the column sum by applying the Chernoff bound for Poisson distribution (Lemma F.2):\nPr (a column sum > (r log dmaxk )) ≤ Pr (Poi(λ) > (r log dmaxk ))\n≤ e−λ ( r log dmaxk\neλ\n)−r log dmaxk\n≤ (\nrMk 2dmaxk m\n)−r log dmaxk\n≤ ( rMk 2m )−r ,\nwhere in the last inequality we used the fact that for dmaxk and r to be large constant, the following simple inequality holds:\nlog(dmaxk ) log\n( rMk\n2dmaxk m\n) ≥ log ( rMk m ) .\nThen consider all the m columns in the submatrix of size m×m, which column sums are independently dominated by Poisson distributions, we have\nPr (every column sum > (r log dmaxk ) ) ≤ ( rMk 2m )−rm .\nNext, take a union bound over all the m×m submatrices of (A2\\E) for m ranging between 1 and Mk, and recall that block (A2\\E) is of size Mk × (Mk −Mk). We can bound for all the submatrices that:\nPr (for every submatrix in (A2\\E), there exist a column whose sum ≤ (r log dmaxk ))\n≥1− Mk∑\nm=1\n( Mk m )( Mk m ) ( rMk 2m )−rm\n≥1− Mk∑\nm=1\n( Mk m )2m(rMk 2m )−rm\n≥1−M−(r−2)k . (39)\nNote that this is indeed a high probability event, since for Wk ≥ 0e−k, we have shown that Mk ≥ Me−2k+τ .\n(2) Perform iterative row and column deletion to construct R and C. Given (A2\\E) of size Mk ×Mk, we apply the argument above in (1) iteratively. First select a sparse column and remove it to C, and apply it to remove columns until the remaining number of columns and rows are equal, then apply it alternatively to the rows (move to R) and columns (move to C) until empty. By construction, there are at most Mk such sparse columns in C, each column of C has sum bounded by (r log dmaxk ), and each row of C bounded by 2dmaxk because it is in the regularized (A2\\E); similarly R has at most Mk rows and each row of R has sum at most (r log dmaxk ) and each column has sum at most 2dmaxk .\nThe proof for the other narrow strip (A3\\E) is in parallel with the above analysis for (A2\\E).\n(c) To bound ‖A4\\E‖, the two key observation are:\n1. The total marginal probability mass of spillover heavy words W k = ∑\ni∈Ĵk ρi ≤ 2e −ek+τd0 .\n(shown in Lemma 3.4).\n2. Similar to the observation in ((b)).2 above, the distributions of the entries in each row of (A4\\E) are independently dominated by a Poisson variable Poi ( 2dmaxk Wk Wk 1 Mk ) .\nIn parallel with Claim B.2, we make a claim about the spectral norm of the block (A4\\E): Claim B.3 (Sparse decomposition of (A4\\E)). With high probability, the index subset Ĵk × Ĵk of A2 can be decomposed into two disjoint subsets R and C such that: each row of R and each column of C has sum at most r; each column of R and each row of C has sum at most dmaxk .\nProof. (to Claim B.3) To show this, we construct sparse decomposition similar to that of (A2\\E). The only difference is that, when considering all the m×m submatrices, we only need to consider all the submatrices contained in the small square (A4\\E) of size Ĵk × Ĵk, instead of all submatrices\nin the wide strip (A2\\E) of size L̂k × Ĵk. In this case, taking the union bound leads to factors of Mk, compared to that of Mk in (39).\nHere we only highlight the difference in the inequalities. Consider an arbitrary column in an arbitrary submatrix of size m×m in (A4\\E). Recall the observation that this column sum is dominated by a Poisson with rate\nλ = 2dmaxk W k Wk m Mk .\nThus we can bound the probability of having a dense column by:\nPr(a column sum > r) ≤ Pr(Poi(λ) > r) ≤ e−λ( r eλ )−r ≤ ( r eλ )−r.\nTake a union over all the square matrices of size m×m in block (A4\\E), we can bound:\nPr(for every submatrix in (A4\\E), there exist a column whose sum ≤ r)\n≥1− Mk∑\nm=1\n( Mk m )2 ( r eλ )−rm\n≥1− Mk∑\nm=1\n( Mk m )2m( Mk m\nrWk\ne2dmaxk W k\n)−rm\n≥1−M−(r−2)k ,\nwhere in the last inequality we used the fact that dmaxk = NWk ek+τ M , and plug in the high probability upper bound of W k ≤ 2e−e k+τd0 as in (18), we have:\nrWk\ne2dmaxk W k =\nrWkM\n2eNWkek+τe−e k+τd0\n= re(e\nk+τd0)\n2e(ek+τd0) 1.\nAgain note that given that the bin has significant total marginal probability, thus Mk ≥ Me−2k, the above probability bound is indeed a high probability statement.\nProof. (to Lemma 3.6 (Given spectral concentration of block B̃k, estimate the separation ∆̃k )) Recall the result of Lemma 3.5 about the concentration of the diagonal block with regularization. For empirical bin with large enough marginal Wk, we have with high probability,\n∥∥∥∥ 1\nN B̃k − B̃k ∥∥∥∥ 2 ≤ C √ dmaxk log 2 dmaxk N .\nAlso recall that ρ̂Îk is defined to be the exact marginal vector restricted to the empirical bin Îk. We can also bound\n∥∥∥∥( 1\nN B̃k − ρ̂Îk ρ̂ > Îk\n)− ∆̃k∆̃>k ∥∥∥∥\n2\n≤ ∥∥∥∥( 1\nN B̃k − ρ̂Îk ρ̂ > Îk\n)− (B̃k − ρ̃kρ̃>k ) ∥∥∥∥\n2 ≤ ∥∥∥∥ 1\nN B̃k − B̃k ∥∥∥∥ 2 + ∥∥∥ρ̂Îk ρ̂ > Îk − ρ̃kρ̃>k ∥∥∥ 2\n≤ C √ dmaxk log\n2 dmaxk N .\nNote that in the last inequality above we ignored the term ‖ρ̂Îk − ρ̃k‖2 as it is small for all bins (with large probability):\n∥∥∥ρ̂Îk ρ̂ > Îk − ρ̃kρ̃>k ∥∥∥ 2 ≤ 4 ∥∥∥ρ̂Îk − ρ̃k ∥∥∥ 2 ∥∥∥ρ̂Îk ∥∥∥ 2 ≤ √√√√ρk(Mk/N)︸ ︷︷ ︸\nover L̂k + ρ2k(W k/ρk)︸ ︷︷ ︸ over Ĵk\n√ Mkρ 2 k\n≤\n√ M2kρk 3 N ≤ √ dmaxk N ,\nwhere in the second inequality we write ∥∥∥ρ̂Îk − ρ̃k ∥∥∥ 2 2 into two parts over the set of good words L̂k and the set of bad words Ĵk. To bound the sum over L̂k we used the Markov inequality as in the proof of Lemma 3.1; and to bound the sum over Ĵk as well as the term ∥∥∥ρ̂Îk ∥∥∥ 2 2 we used the fact that if a word i appears in Îk, we must have ρ̂i ≤ ρk. The last inequality is due to M2kρ3k ≤W 2k ρk ≤Wkρk = dmaxk /N . Let vkv > k be the rank-1 truncated SVD of the regularized block (B̃k − ρ̂kρ̂>k ). Apply Wedin’s theorem to rank-1 matrix (Lemma F.1), we can bound the distance between vector vk and ∆̃k by:\nmin { ‖∆̃k − vk‖, ‖∆̃k + vk‖ } = O(min{\n√ dmaxk log 2 dmaxk /N\n‖∆̃k‖ , ( √ dmaxk log 2 dmaxk /N) 1/2})."
    }, {
      "heading" : "C Proofs for Algorithm Phase II (Section 4)",
      "text" : "Proof. (to Lemma 4.2 (Sufficient condition for constructing an anchor partition)) (1) First, we show that if for some constant c = Ω(1), a set of words A satisfy\n∣∣∣∣∣ ∑\ni∈A ∆i\n∣∣∣∣∣ ≥ c‖∆‖1, (40)\nthen (A, [M ]\\A) is a pair of anchor set defined in 4.1. By the assumption of constant separation (C∆ = Ω(1)), ∑ i∈A∆i = Ω(1). We can bound the condition number of the anchor partition matrix by:\ncond\n([ ρA, ∆A\n1− ρA, −∆A\n]) = √ T 2 − 4D + T√ T 2 − 4D − T ≤ √ 1 + 4cC∆ + 1√ 1 + 4cC∆ − 1 = Ω(1),\nwhere T = ρA −∆A ≤ 1 and D = −ρA∆A − (1− ρA)∆A = −∆A. (2) Next we show that Â defined in the lemma statement satisfies (40).\nDenote A∗ = {i ∈ I : ∆i > 0}. Note that ‖∆I‖1 = ∑ i∈A∗ ∆i − ∑ i∈I\\A∗ ∆i. Without loss\nof generality we assume that ∑\ni∈A∗ ∆i ≥ 12‖∆I‖1 ≥ 12C‖∆‖1, where the last inequality is by the condition in (26).\nGiven ∆̂I that satisfies (27). We look at Â = {i ∈ I : ∆̂i > 0}. ∑\ni∈Â\n∆i = ∑\ni∈Â∩A∗ ∆i −\n∑\ni∈Â∩(I\\A∗)\n∆i\n= ∑\ni∈A∗ ∆i −\n∑\ni∈(Â∩(I\\A∗))∪(A∗∩(I\\Â))\n|∆i|\n≥ ∑\ni∈A∗ ∆i − ‖∆̂I −∆I‖1\n≥ (1 2 C − C ′)‖∆‖1 ≥ 1 6 CC∆,\nwhere in the second last inequality we used the fact that, if the sign of ∆̂i and ∆i are different, it must be that |∆̂i −∆i| > |∆i|.\nProof. (to Lemma 4.4 (Estimate the separation restricted to the k-th good bin)) Since it is a good bin, we have the `2 bound given by Lemma 3.6 as below (assuming the possible sign flip has been fixed as in Lemma 3.8):\n‖∆̃k − vk‖2 ≤ √ dmaxk log\n2 dmaxk N\n1\n‖∆̃k‖2 .\nThen we can convert the bound to `1 distance by:\n‖vk − ∆̃k‖1 ‖∆̃k‖1 ≤ √ Mk‖vk − ∆̃k‖2 ‖∆̃k‖1 ≤ √ Mk‖vk − ∆̃k‖2‖∆̃k‖2 ‖∆̃k‖2‖∆̃k‖1\n≤ Mk‖vkv > k − ∆̃k∆̃>k ‖ ‖∆̃k‖21 ≤ CMk W 2k √ dmaxk log2(d0) N\n≤ CMk W 2k\neτ √ NWk\nWk Mk\nlog2(d0)\nN\n≤ Ceτ √ M log2(d0)\nNWkek = O(\n√ log(d0)\nd0 ),\nwhere in the second last inequality, we used the fact that Mk ek M ≤Wk again, and in the last inequality we used the assumption Wk ≥ 0/ek.\nProof. (to Lemma 4.5 (With Ω(1) separation, most words fall in “good bins” with high probability)) This proof is mostly playing around with the probability mass and converting something obviously true in expectation to high probability argument.\n(1) Note that by their definition we know that Wk ≥ 12Sk, and we have ∑\nk\nWk\n( 1[\nSk 2Wk ≥ C2] + Sk 2Wk 1[ Sk 2Wk < C2]\n)\n≥ ∑\nk\nWk Sk\n2Wk\n( 1[\nSk 2Wk ≥ C2] + 1[ Sk 2Wk < C2]\n)\n= ∑\nk\nWk Sk\n2Wk\n= 1\n2\n∑\nk\nSk,\nMoreover, note that by definition of Wk, we have ∑ kWk = 1, therefore\n∑\nk\nWk Sk\n2Wk 1[ Sk 2Wk\n< C2] ≤ ∑\nk\nC2Wk = C2.\nFrom the above two inequalities we can bound\n∑\nk\nWk1[ Sk\n2Wk ≥ C2] ≥\n1\n2\n∑\nk\nSk − C2.\nAlso note that\n∑\nk\nWk1[Wk < C1 2k ] ≤ C1.\nTherefore according to the definition of “good bins” we have that:\n∑ k∈G Wk = ∑ k Wk1\n[ Sk\n2Wk ≥ C2 and Wk ≥ C1 2k\n] ≥ 1\n2\n∑\nk\nSk − C2 − C1. (41)\n(2) We want to lower bound the quantity ∑\nk Sk to be a constant fraction of ‖∆‖1. Note that by definition of Sk in (28) we can equivalently write the sum as:\n∑\nk\nSk = ∑\nk\n∑\ni∈Îk∩(∪{k′:k′≤k+τ}Ik′ )\n|∆i| = ∑\ni\n|∆i| ∑\nk\n1[i ∈ Ik ∩ ∪{k′:k′≤k+τ}Îk′ ].\nConsider for each word i. Assume that word i ∈ Ik. Given N = d0M for some large constant d0, denote dk = e kd0, we can bound the probability Pr(i ∈ Ik ∩ ∪{k′:k′≤k+τ}Îk′) as follows:\nPr(i ∈ Ik ∩ ∪{k′:k′≤k+τ}Îk′) ≥ 1− Pr(Poi(Nρi) > eτρi)− Pr(Poi(Nρi) < e−τNρi/2)\n≥ 1− e −(τ−1)e(τ+k)d0 √\n2πek+τd0 − e−dk\n≥ 1− 2e−dk .\nTherefore at least in expectation we can lower bound the sum by\nE[ ∑\nk\nSk] = ∑\ni\n|∆i| ∑\nk\nPr(i ∈ Ik ∩ ∪{k′:k′≤k+τ}Îk′) ≥ (1− 2e−d0)‖∆‖1.\n(3) Restrict to the exact good bins, for which we know that the exact ‖ρÎk‖1 ≥ e −k and ‖∆Îk‖1/‖ρIk‖1 ≥ C. Here we know that if Îk is an good bin, the number of words in this exact good bin is lower bounded by Mk ≥ e−k/ρk ≥ M/e−k, and since k ≤ log logM we have that Mk ≥ Mlog(M) . This is important for use to apply Bernstein concentration of the words in the bin.\nSince ‖∆Îk‖1/‖ρÎk‖1 ≥ C, and that |∆i| ≤ ρi, we have that out of the Mk words there are at least a constant fraction of words with |∆i| ≥ 12Cρk. Recall that we denote ρk = ek/M . This is easy to see as xρk + (Mk − x)12Cρk ≥ ‖∆Îk‖1 ≥ CMkρk thus x ≥ C/2− CMk.\nThen we bound the probability that out of these cMk words with good separation, a constant fraction of them do not escape from the closest τ empirical bins. Denote λk = 2e\n−dk , which is the upper bound of the escaping probability for each of the word, and is very small. By a simple application of Bernstein bounds of the Bernoulli sum, for a small constant c0, we have\nPr( ∑\ni=1,...cMk\nBeri(λk) ≥ c0Mk) ≤ Pr( ∑\ni=1,...cMk\nBeri(λk)− λkMk ≥ (c0 − λk)Mk)\n≤ e −\n1 2 (c0−λk) 2M2k Mkλk+ 1 3 (c0−λk)Mk\n≈ e−c0Mk .\nThen union bound over all the exact good bins. That gives a log logM multiply of the probability. We now know that restricting to the non-escaping good words in the exact good bins, they already contribute a constant fraction (due to constant non-escaping, constant ratio ‖∆Îk‖1/‖ρÎk‖1, and constant ∑ k∈exact good binWk) of the total separation ‖∆‖1. Therefore we can conclude that for some universal constant C we have ∑\nk\nSk ≥ C‖∆‖1.\n(4) Finally plug the above bound of ∑\nk Sk into (41), and note the assumption on the constants C1 and C2, we can conclude that the total marginal probability mass contained in “good bins” is large:\n∑ k∈G Wk ≥ (C − 1 24 − 1 24 )‖∆‖1 = 1 12 ‖∆‖1.\nProof. (to Lemma 4.6 (Estimate ρ and ∆ with accuracy in `2 distance)) Consider word i we have that\n[ ρA, ∆A\n1− ρA, −∆A ] [ ρi ∆i ] = [ ∑ j∈A Bj,i∑ j∈Ac Bj,i ]\nSet\n[ ρ̂i ∆̂i ] = [ ρA, ∆A 1− ρA, −∆A ]−1 [ 1 N ∑ j∈A[BN ]j,i 1 N ∑ j∈Ac [BN ]j,i ]\nSince 1N ∑ j∈A[BN ]j,i ∼ 1NPoi(N(ρAρi + ∆A∆i)), apply Markov inequality, we have that\nPr( ∑\ni\n( 1\nN\n∑ j∈A [BN ]j,i − ∑ j∈A Bj,i)2 > 2) ≤ 1 N\n∑ i(ρAρi + ∆A∆i)\n2 = ρA N 2\nNote that ρA = Ω(1) and that cond(DA) = Ω(1), we can propagate the concentration to the estimation error of ρ and ∆ as, for some constant C = Ω(1),\nPr(‖ρ̂− ρ‖ > ) ≤ C N 2 , Pr(‖∆̂−∆‖ > ) ≤ C N 2 ."
    }, {
      "heading" : "D Proofs for HMM testing lower bound (Section 5.2)",
      "text" : "Proof. (to Theorem 5.1)\nTV (Pr1(G N 1 ),Pr2(G N 1 )) ≤ TV (Pr1(GN1 ,A),Pr2(GN1 ,A)) (42)\n= 1\n2\n∑\nGN1 ∈[M ]N ,A∈( M M/2)\n∣∣Pr2(GN1 ,A)− Pr1(GN1 ,A) ∣∣\n= 1\n2\n∑\nGN1 ∈[M ]N ,A∈( M M/2)\nPr1(G N 1 ,A) ∣∣∣∣ Pr2(G N 1 ,A) Pr1(GN1 ,A) − 1 ∣∣∣∣\n(a) ≤ 1 2 √√√√√ ∑\nGN1 ∈[M ]N ,A∈( M M/2)\nPr1(GN1 ,A) (\nPr2(GN1 ,A) Pr1(GN1 ,A)\n− 1 )2\n= 1\n2\n√√√√√ ∑\nGN1 ∈[M ]N ,A∈( M M/2)\nPr1(GN1 ,A) (\nPr2(GN1 ,A) Pr1(GN1 ,A)\n)2 − 1\n(b) =\n1\n2 √√√√√ ( MN( M M/2 ) )2 ∑\nGN1 ∈[M ]N ,A∈( M M/2)\nPr1(GN1 ,A) ( ∑\nB∈( MM/2)\nPr2(GN1 |B) )2 − 1\n(c) =\n1\n2 √√√√√√√ MN ( M M/2 )2 ∑ GN1 ∈[M ]N ( ∑ B∈( MM/2) Pr2(G N 1 |B) )2\n︸ ︷︷ ︸ Y\n−1, (43)\nwhere inequality (a) used the fact that E[X] ≤ (E[X2])1/2; equality (b) used the joint distributions in (32) and (35); and equality (c) takes sum over the summand A first and makes use of the marginal probability Pr1(G N 1 ) as in (34).\nIn order to bound the term Y = M N\n( MM/2) 2\n∑ GN1 ∈[M ]N (∑ B∈( MM/2) Pr2(G N 1 |B) )2 in equation 43, we\nbreak the square and write:\nY = MN ( M M/2 )2 ∑\nB,B′∈( MM/2)\n∑\nGN1 ∈[M ]N Pr2(G\nN 1 |B)Pr2(GN1 |B′). (44)\nIn Claim D.1 below we explicitly compute the term ∑\nGN1 ∈[M ]N Pr2(G\nN 1 |B)Pr2(GN1 |B′) for any two\nsubsets B,B′ ∈ ( M M/2 ) . Then in Claim D.2 we compute the sum over B,B′ and bound Y ≤ √ 2\n2− 4 3 N M\n.\nTo conclude, we can bound the total variation distance as:\nTV (Pr1(G N 1 ),Pr2(G N 1 )) ≤\n1\n2\n√√√√ √\n2\n2− 43 NM − 1.\nTherefore TV (Pr1(G N 1 ),Pr2(G N 1 )) = o(1) if N = o(M).\nClaim D.1. In the same setup of Theorem 5.1, given two subsets B,B′ ∈ M and |B| = |B′| = M/2, let B =M\\B,B′ =M\\B′ denote the corresponding complement. Define x to be:\nx = |B ∩ B′| M/2 . (45)\nNote that we have 0 ≤ x ≤ 1. Also define a1 = 1+(1−2t) 2 4 and a2 = ( 4(1−2t) 1+(1−2t)2 )2 as functions of the transition probability t. We have:\n∑\nGN1 ∈[M ]N Pr2(G\nN 1 |B)Pr2(GN1 |B′) =\n1\n(M/2)N\n( γN1 (1− 2γ2)− γN2 (1− 2γ1)\n2(γ1 − γ2)\n) ,\nwhere γ1 = a1\n( 1 + √ 1− x(1− x)a2 ) , γ2 = a1 ( 1− √ 1− x(1− x)a2 ) are functions of |B ∩B′| and t.\nProof. (to Claim D.1) (1) For an instance of 2-state HMM specified by B ∈ ( M M/2 ) , consider two consecutive outputs (gn−1, gn). We first show how to compute the probability Pr2(gn|gn−1,B). Given B and B′, we can partition the vocabulary M into four subsets as:\nM1 = B ∩ B′, M2 = B ∩ B′, M3 = B ∩ B′, M4 = B ∩ B′.\nNote that we have |M1| = |M4| = xM/2 and |M2| = |M3| = (1− x)M/2. Define a subset of tuples JB ⊂ [4]2 to be\nJB = {(1, 1), (1, 2), (2, 1), (2, 2), (3, 3), (3, 4), (4, 3), (4, 4)}.\nIf gn−1 ∈ Mj′ , gn ∈ Mj and (j′, j) ∈ JB, we know that the hidden state does not change, namely sn−1 = sn, and that Pr2(gn|gn−1,B) = Pr2(sn|sn−1,B)Pr2(gn|sn,B) = 1−tM/2 . Also, if (j′, j) ∈ J cB ≡ [4]2\\JB, we know that there is the state transition and we have Pr2(gn|gn−1,B) = tM/2 .\nSimilarly, for the 2-state HMM specified by B′, we can define\nJB′ = {(1, 1), (1, 3), (3, 1), (3, 3), (2, 2), (2, 4), (4, 2), (4, 4)},\nand Pr2(gn|gn−1,B′) = 1−tM/2 if (gn−1 ∈Mj′ , gn ∈Mj) with (j′, j) ∈ JB′ and equals tM/2 if (j′, j) ∈ J cB′ . (2) Next, we show how to compute the target sum of the claim statement in a recursive way.\nDefine Fn,j for n ≤ N and j = 1, 2, 3, 4 as below\nFn,j = ∑\nGn1∈[M ]n Pr2(G\nn 1 |B)Pr2(Gn1 |B′)1[gn ∈Mj ],\nand the target sum is ∑\nGN1 ∈[M ]N Pr2(G\nN 1 |B)Pr2(GN1 |B′) = ∑ j=1:4 FN,j . Also, we have that\nF1,j = |Mj |/M2.\nMaking use of the recursive property of the probability rule of the 2-state HMM as in (33), we can write the following recursion in terms of Fn,j for n ≥ 2:\nFn,j = ∑\nGn1∈[M ]n Pr2(G\nn 1 |B)Pr2(Gn1 |B′)1[gn ∈Mj ]\n= ∑\nGn1∈[M ]n Pr2(G\nn−1 1 |B)Pr2(Gn−11 |B′)Pr2(gn|Gn−11 ,B)Pr2(gn|Gn−11 ,B′)\n∑\nj′=1:4\n1[gn−1 ∈Mj′ , gn ∈Mj ]\n= ∑\nGn−11 ∈[M ]n−1 Pr2(G\nn−1 1 |B)Pr2(Gn−11 |B′)\n∑\ngn∈[M ]\n∑\nj′=1:4\n1[gn−1 ∈Mj′ , gn ∈Mj ]Pr2(gn|gn−1,B)Pr2(gn|gn−1,B′)\n= |Mj | ∑\nj′=1:4\nFn−1,j′ ( 1− t M/2 1[(j′, j) ∈ JB] + t M/2 1[(j, j′) ∈ J cB] )( 1− t M/2 1[(j′, j) ∈ JB′ ] + t M/2 1[(j, j′) ∈ J cB′ ] )\nwhere we used the probability Pr2(gn|gn−1,B) derived in (1). Equivalently we can write the recursion as:\n \nFn,1 Fn,2 Fn,3 Fn,4\n  = 1\nM/2 DxT\n \nFn−1,1 Fn−1,2 Fn−1,3 Fn−1,4\n  ,\nfor diagonal matrix Dx =   x 1− x\n1− x x\n  and the symmetric stochastic matrix T given by\nT =  \n(1− t)2 (1− t)t (1− t)t t2 (1− t)t (1− t)2 t2 (1− t)t (1− t)t t2 (1− t)2 (1− t)t t2 (1− t)t (1− t)t (1− t)2\n  = 4∑\ni=1\nλiviv > i ,\nwhere the singular values and singular vectors of T are specified as follows: λ1 = 1, λ4 = (1 − 2t)2, and v1 = 1 2 [1, 1, 1, 1] >, v4 = 1 2 [1,−1,−1, 1]>. And λ2 = λ3 = 1 − 2t with v2 = 1√2 [0, 1,−1, 0] > and v2 = 1√ 2 [1, 0, 0,−1]>.\nNote that we can write (F1,1, F1,2, F1,3, F1,4) > = M/2\nM2 Dx(1, 1, 1, 1)\n>.\n(3) Finally we can compute the target sum as:\n∑\nGN1 ∈[M ]N Pr2(G\nN 1 |B)Pr2(GN1 |B′) =\n( 1 1 1 1 ) ( Fn,1 Fn,2 Fn,3 Fn,4 )>\n= ( 1 1 1 1 ) 1\n(M/2)N−1 (DxT )\nN−1M/2 M2 Dx ( 1 1 1 1 )>\n(a) =\n1\n(M/2)N v>1 (DxT ) Nv1\n= 1\n(M/2)N (1 0)\n( 1/2 (1− 2t)2(x− 1/2)\n(x− 1/2) (1− 2t)2/2\n)\n︸ ︷︷ ︸ H(x)N\nN\n(1 0)>\n(b) =\n1\n(M/2)N\n( γ1(x)γ2(x)\nN − γ2(x)γ1(x)N γ1(x)− γ2(x) + 1 2 γ1(x) N − γ2(x)N γ1(x)− γ2(x)\n)\n= 1 (M/2)N γN1 (1− 2γ2)− γN2 (1− 2γ1) 2(γ1 − γ2) ,\nwhere in (a) we used the fact that DxTv1 = (x − 1/2)v1 + v4/2 and DxTv4 = (1 − 2t)2(v1/2 + (x − 1/2)v4); in (b) we used the Calley-Hamilton theorem to obtain that for 2 × 2 matrix H(x) parameterized by x and with 2 distinct eigenvalue γ1(x) and γ2(x), its power can be written as H(x)N = γ1(x)γ2(x) N−γ2(x)γ1(x)N\nγ1(x)−γ2(x) I2×2 + γ1(x)N−γ2(x)N γ1(x)−γ2(x) H(x). Moreover, the distinct eigenvalues of the\n2× 2 matrix H(x) can be written explicitly as follows:\nγ1(x) = 1 + (1− 2t)2\n4\n 1 + √ 1− x(1− x) ( 4(1− 2t) 1 + (1− 2t)2 )2   ,\nγ2(x) = 1 + (1− 2t)2\n4\n 1− √ 1− x(1− x) ( 4(1− 2t) 1 + (1− 2t)2 )2   ,\nwhere recall that we defined x = |B∩B ′|\nM/2 so 0 ≤ x ≤ 1, also we have the transition probability 0 < t < 1/2 to be a constant, therefore we have γ1 > γ2 to be two distinct real roots.\nThe next claim makes use of the above claim and bounds the right hand side of equation 44.\nClaim D.2. In the same setup of Theorem 5.1, we have\nY = MN ( M M/2 )2 ∑\nB,B′∈( MM/2)\n∑\nGN1 ∈[M ]N Pr2(G\nN 1 |B)Pr2(GN1 |B′) ≤\n√ 2\n2− 43 NM .\nProof. (to Claim D.2)\nDefine f(x) = γ1(x) N (1−2γ2(x))−γ2(x)N (1−2γ1(x))\n2(γ1(x)−γ2(x)) with γ1(x) = a1\n( 1 + √ 1− x(1− x)a2 ) and γ2(x) =\na1\n( 1− √ 1− x(1− x)a2 ) as functions of x. Recall that x = |B∩B ′| M/2 , and a1 =\n1+(1−2t)2 4 , a2 =(\n4(1−2t) 1+(1−2t)2\n)2 are constants determined solely by transition probability t.\nUse the result of Claim D.2 we have:\nY = MN ( M M/2 )2 ∑\nB,B′∈( MM/2)\n1\n(M/2)N f(x)\n(a) = MN ( M M/2 )2 1 (M/2)N\n( M\nM/2\n)M/2∑\ni=1\n( M/2\ni\n)2 f ( i\nM/2\n)\n= 2N( M M/2 ) M/2∑ i=1\n( M/2\ni\n)2 f ( i\nM/2\n) , (46)\nwhere equality (a) is obtained by counting the number of subsets B,B′ ∈ ( M M/2 ) with i overlapping entries. Next we approximately bound Y when M is asymptotically large. First, we can bound γ1(x) as:\nγ1(x) = a1\n( 1 + √ 1− x(1− x)a2 )\n= a1(1 + √\n(1− a2/4) + a2(1/2− x)2) ≤ C + (1− 2t)|1/2− x| ≤ 2c(1/2−x)2 ,\nwhere C and c are small constants. Then we bound f(x) for x = iM/2 :\nlim M→∞\nf(x) ≤ γ1(x)N (1− 2γ2(x))\n2(γ1(x)− γ2(x)) ≤ 2c(1/2−x)2N ,\nwhere we used the fact that γ1 ≤ 1/2 and that (1−2γ2(x))2(γ1(x)−γ2(x)) ≤ CC. Second, we use Stirling’s approximation for the combinatorial coefficients ( M/2 i )2 and ( M M/2 ) , we have (the entropy function is given by H2(x) = x log2(x) + (1− x) log2(1− x)) ( M\nM/2\n) ≈ 2 M\n√ πM/2 ,\n( M/2\ni\n)2 ≈ ( 2 (M/2)H2( i M/2 ) )2 ≤ ( 2 1−2( i M/2 − 1 2 )2 )M ,\nFinally we can approximately bound Y in (46) as follows:\nY ≈ 2N−M √ πM/2 M/2∑\ni=1\n( M/2\ni\n)2 f ( i\nM/2\n)\n≤ 2N−M √ πM/2M/2\n∫ 1/2\nx=−1/2 2(1−2x 2)M+cx2N\n= √ πM/2M/22N\n∫ 1/2\nx=−1/2 2−x 22M−cN\n=\n√ 2\n2− 43 NM ,\nwhere in the last equality we take the limit that M →∞ and set t = 1/4."
    }, {
      "heading" : "E Analyze truncated SVD",
      "text" : "The reason that truncated SVD does not concentrate at the optimal rate is as follows. What truncated SVD actually optimizes is the spectral distance from the estimator to the empirical average (minimizing ‖B̂ − 1NBN‖2), yet not to the expected matrix B. It is only “optimal” in some very special setup, for example when ( 1NBN − B) are entry-wise i.i.d. Gaussian. In the asymptotic regime when N → ∞ it is indeed true that under mild condition any sampling noise converges to i.i.d Gaussian. However in the sparse regime where N = Ω(M), the sampling noise from the probability matrix is very different from additive Gaussian noise.\nClaim E.1 (Truncated SVD has sample complexity super linear). In order to achieve accuracy, the sample complexity of rank-2 truncated SVD estimator is in given by N = O(M2 logM).\nExample 1: a = b = w = 1/2, dictionary given by\np = [ 1 + C∆ M , . . . , 1 + C∆ M , 1− C∆ M , . . . , 1− C∆ M ] ,\nq = [ 1− C∆ M , . . . , 1− C∆ M , 1 + C∆ M , . . . , 1 + C∆ M ] .\nSample complexity is O(M logM). Example 2: modify Example 1 so that a constant fraction of the probability mass lies in a common word, namely p1 = q1 = 1/2ρ1 = 0.1, while the marginal probability as well as the separation in all the other words are roughly uniform. Sample complexity is O(M2 logN).\nProof. (to Claim E.1 (Truncated SVD has sample complexity super linear)) (1) We formalize this and examine the sample complexity of t-SVD by applying Bernstein matrix inequality. The concentration of the empirical average matrix at the following rate:\nPr(‖ 1 N BN − B‖ ≥ t) ≤ e−\n(Nt)2\nNV ar+BNt/3 +log(M)\n,\nwhere V ar = ‖E[eie>i ]‖2 = ‖diag(ρ)‖2 = maxi ρi, and B = maxi,j ‖eiej‖2 = 1. Therefore, with probability at least 1− δ, we have that\n‖ 1 N BN − B‖ ≤\n√ maxi ρi log(M/δ)\nN +\n1\n3\n1\nN log(M/δ). (47)\nSince ‖x‖1 ≤ √ M‖x‖2, in order to guarantee that ‖∆̂ − ∆‖1 ≤ , it suffices to ensure that\n‖∆̂−∆‖2 ≤ / √ M . Note that the leading two eigenvectors are given by σ1(B) ≥ ‖ρ‖2 = 1/ √ M and\nσ2(B) = ‖∆‖2 = C∆/ √ M . Assume that we have the exact marginal probability ρ, by Davis-Kahan, it suffices to ensure that\n‖ 1 N BN − B‖2 ≤ ‖∆‖2√ M .\nExample 1. Consider the example of (p, q) in community detection problem, where the marginal probability ρi is roughly uniform. We have ‖∆‖2 = C∆/ √ M and maxi ρi = 1/M , and the concentration bound becomes\n‖ 1 N BN − B‖ ≤\n√ log(M/δ)\nMN , (48)\nand by requiring\n√ log(M/δ)\nMN ≤ ‖∆‖2√ M = C∆ M\nwe get a sample complexity bound N = Ω(M log(M/δ)), which is worse than the lower bound by a log(M) factor.\nExample 2. Moreover, modify Example 1 so that a constant fraction of the probability mass lies in a common word, namely p1 = q1 = 1/2ρ1 = 0.1, while the marginal probability as well as the separation in all the other words are roughly uniform. In this case, ‖∆‖2 is still roughly C∆/ √ M , however we have maxi ρi = 0.1, and the sample complexity becomes N = Ω(M 2 log(M/δ)). This is even worse than the first example, as the same separation gets swamped by the heavy common words.\n(2) (square root of the empirical marginal scaling (from 1st batch of samples) on both side of the empirical count matrix (from 2nd batch of samples)).\nTake a closer look at the above proof and we can identify two misfortunes that make the truncated SVD deviate from linear sample complexity:\n1. In the worst case, the nonuniform marginal probabilities costs us an M factor in the first component of Bernstein’s inequality;\n2. We pay another log(M) factor for the spectral concentration of the M ×M random matrix.\nTo resolve these two issues, the two corresponding key ideas of Phase I algorithm are “binning” and “regularization”:\n1. “Binning” means that we partition the vocabulary according to the marginal probabilities, so that for the words in each bin, their marginal probabilities are roughly uniform. If we are able to apply spectral method in each bin separately, we could possibly get rid of the M factor.\n2. Now restrict our attention to the diagonal block of the empirical average matrix 1NBN whose indices corresponding to the words in a bin. Assume that the bin has sufficiently many words, so that the expected row sum and column sum are at least constant, namely the effective number of samples is at least in the order of the number of of words in the bin.\nWe apply regularized spectral method for the empirical average with indices restricted to the bin. By “regularization” we mean removing the rows and column, whose row and column sum are much higher than the expected row sum, from the empirical. Then we apply t-SVD to the remaining. This regularization idea is motivated by the community detection literature in the sparse regime, where the total number of edges of the random network is only linear in the number of nodes."
    }, {
      "heading" : "F Auxiliary Lemmas",
      "text" : "Lemma F.1 (Wedin’s theorem applied to rank-1 matrix). Denote symmetric matrix X = vv> + E. Let v̂v̂> denote the rank-1 truncated SVD of X. There is a positive universal constant C such that\nmin{‖v − v̂‖, ‖v + v̂‖} ≤ {\nC‖E‖ ‖v‖ if ‖v‖2 > C‖E‖; C‖E‖1/2 if ‖v‖2 < C‖E‖.\nLemma F.2 (Chernoff Bound for Poisson variables).\nPr(Poi(λ) ≥ x) ≤ e−λ ( x eλ )−x , for x > λ, Pr(Poi(λ) ≤ x) ≤ e−λ ( x eλ )−x , for x < λ.\nLemma F.3 (Matrix Bernstein). Consider a sequence of N random matrix {Xk} of dimension M×M which are independent, self-adjoint. Assume that E[Xk] = 0 and λmax(Xk) ≤ R almost surely. Denote the total variance by σ2 = ‖∑Nk=1 E[X2k ]‖. Then the following inequality holds for all t > 0:\nPr ( ‖ N∑\nk=1\nXk‖ ≥ t ) ≤Me− t2 σ2+Rt/3 ≤ { Me− 3t2\n8σ2 , for t ≤ σ2/R; Me− 3t 8R , for t ≥ σ2/R.\nLemma F.4 (Upper bound of Poisson tails (Proposition 1 in [25])). Assume λ > 0, consider the Poisson distribution Poi(λ).\n(1) if 0 ≤ n < λ, the left tail can be upper bounded by:\nPr(Poi(λ) ≤ n) ≤ (1− n λ )−1 Pr(Poi(λ) = n).\n(2) if n > λ− 1, for any m ≥ 1, the right tail can be upper bounded by:\nPr(Poi(λ) ≥ n) ≤ (1− ( λ n+ 1\n)m)−1 n+m−1∑\ni=n\nPr(Poi(λ) = i).\nCorollary F.5. Let λ > C for some large universal constant C. For any constant c′ > e, 0 ≤ c < 1/2, we have the following Poisson tail bounds:\nPr(Poi(λ) ≤ cλ) ≤ 2e−λ/2, Pr(Poi(λ) ≥ c′λ) ≤ 2e−c′λ.\nProof. Apply Stirling’s bound for λ large, we have λ! ≥ (λe )λ. Then, the bound in Lemma F.4 (1) can be written as\nPr(Poi(λ) ≤ cλ) ≤ (1− c)−1 Pr(Poi(λ) = cλ) ≤ 2e−λ(λ)cλ/(cλ)! ≤ 2e−λ(λ)cλ/(cλe−1)cλ\n≤ 2e−λ+cλ log(e/c)\n≤ 2e−λ/2, where in the second inequality we used the assumption that c < 1/2, and in the last inequality we used the inequality 1− c log(e/c) ≥ 1/2 for all 0 ≤ c < 1.\nSimilarly, set m = 1 in Lemma F.4 (2), we can write the bound as\nPr(Poi(λ) ≥ c′λ) ≤ (1− λ c′λ+ 1 )−1 Pr(Poi(λ) = c′λ)\n≤ (1− 1/c′)−1e−λ(λ)c′λ/(c′λ)! ≤ 2e−λ(λ)c′λ/(c′λe−1)c′λ ≤ 2e−c′λ log(c′/e)−1\n≤ 2e−c′λ, where in both the second and the last inequality we used the assumption that c′ > e and λ is a large constant.\nLemma F.6 (Slight variation of Vershynin’s theorem (Poisson instead of Bernoulli)). Consider a random matrix A of size M ×M , where each entry follows an independent Poisson distribution Ai,j ∼ Poi([EA]i,j). Define dmax = M maxi,j [EA]i,j. For any r ≥ 1, the following holds with probability at least 1 − M−r. Consider any subset consisting of at most 10 Mdmax , and decrease the entries in the rows and the columns corresponding to the indices in the subset in an arbitrary way. Then for some universal large constant c the modified matrix A′ satisfies:\n‖A′ − [EA]‖ ≤ Cr3/2( √ dmax + √ d′),\nwhere d′ denote the maximal row sum in the modified random matrix."
    } ],
    "references" : [ {
      "title" : "Exact recovery in the stochastic block model",
      "author" : [ "Emmanuel Abbe", "Afonso S Bandeira", "Georgina Hall" ],
      "venue" : "arXiv preprint arXiv:1405.3267,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2014
    }, {
      "title" : "Community detection in general stochastic block models: fundamental limits and efficient recovery algorithms",
      "author" : [ "Emmanuel Abbe", "Colin Sandon" ],
      "venue" : "arXiv preprint arXiv:1503.00609,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2015
    }, {
      "title" : "Competitive closeness testing",
      "author" : [ "J. Acharya", "H. Das", "A. Jafarpour", "A. Orlitsky", "S. Pan" ],
      "venue" : "In Conference on Learning Theory (COLT),",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2011
    }, {
      "title" : "Competitive classification and closeness testing",
      "author" : [ "J. Acharya", "H. Das", "A. Jafarpour", "A. Orlitsky", "S. Pan" ],
      "venue" : "In Conference on Learning Theory (COLT),",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2012
    }, {
      "title" : "A spectral algorithm for latent dirichlet allocation",
      "author" : [ "Anima Anandkumar", "Yi kai Liu", "Daniel J. Hsu", "Dean P Foster", "Sham M Kakade" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2012
    }, {
      "title" : "Tensor decompositions for learning latent variable models",
      "author" : [ "Animashree Anandkumar", "Rong Ge", "Daniel Hsu", "Sham M. Kakade", "Matus Telgarsky" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2014
    }, {
      "title" : "Computing a nonnegative matrix factorization–provably",
      "author" : [ "Sanjeev Arora", "Rong Ge", "Ravindran Kannan", "Ankur Moitra" ],
      "venue" : "In Proceedings of the forty-fourth annual ACM symposium on Theory of computing,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2012
    }, {
      "title" : "Learning topic models–going beyond svd",
      "author" : [ "Sanjeev Arora", "Rong Ge", "Ankur Moitra" ],
      "venue" : "In Foundations of Computer Science (FOCS),",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2012
    }, {
      "title" : "Random walks on context spaces: Towards an explanation of the mysteries of semantic word embeddings",
      "author" : [ "Sanjeev Arora", "Yuanzhi Li", "Yingyu Liang", "Tengyu Ma", "Andrej Risteski" ],
      "venue" : "arXiv preprint arXiv:1502.03520,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2015
    }, {
      "title" : "Random walks on context spaces: Towards an explanation of the mysteries of semantic word embeddings",
      "author" : [ "Sanjeev Arora", "Yuanzhi Li", "Yingyu Liang", "Tengyu Ma", "Andrej Risteski" ],
      "venue" : "CoRR, abs/1502.03520,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2015
    }, {
      "title" : "Testing closeness of discrete distributions",
      "author" : [ "T. Batu", "L. Fortnow", "R. Rubinfeld", "W.D. Smith", "P. White" ],
      "venue" : "Journal of the ACM (JACM),",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2013
    }, {
      "title" : "Sublinear algorithms for testing monotone and unimodal distributions",
      "author" : [ "T. Batu", "R. Kumar", "R. Rubinfeld" ],
      "venue" : "In Symposium on Theory of Computing (STOC),",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2004
    }, {
      "title" : "Polynomial learning of distribution families",
      "author" : [ "Mikhail Belkin", "Kaushik Sinha" ],
      "venue" : "In Foundations of Computer Science (FOCS),",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2010
    }, {
      "title" : "Smoothed analysis of tensor decompositions",
      "author" : [ "Aditya Bhaskara", "Moses Charikar", "Ankur Moitra", "Aravindan Vijayaraghavan" ],
      "venue" : "In Proceedings of the 46th Annual ACM Symposium on Theory of Computing,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2014
    }, {
      "title" : "Testing closeness with unequal sized samples",
      "author" : [ "B. Bhattacharya", "G. Valiant" ],
      "venue" : "In Neural Information Processing Systems (NIPS) (to appear),",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2015
    }, {
      "title" : "Estimating a density under order restrictions: Nonasymptotic minimax risk",
      "author" : [ "L. Birge" ],
      "venue" : "Annals of Statistics,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 1987
    }, {
      "title" : "Full reconstruction of Markov models on evolutionary trees: Identifiability and consistency",
      "author" : [ "J.T. Chang" ],
      "venue" : "Mathematical Biosciences,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 1996
    }, {
      "title" : "Solving random quadratic systems of equations is nearly as easy as solving linear systems",
      "author" : [ "Yuxin Chen", "Emmanuel J Candes" ],
      "venue" : "arXiv preprint arXiv:1505.05114,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2015
    }, {
      "title" : "Stochastic block model and community detection in the sparse graphs: A spectral algorithm with optimal rate of recovery",
      "author" : [ "Peter Chin", "Anup Rao", "Van Vu" ],
      "venue" : "arXiv preprint arXiv:1501.05021,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2015
    }, {
      "title" : "Learning mixtures of gaussians",
      "author" : [ "Sanjoy Dasgupta" ],
      "venue" : "In Foundations of Computer Science,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 1999
    }, {
      "title" : "Testing k-modal distributions: optimal algorithms via reductions",
      "author" : [ "C. Daskalakis", "I. Diakonikolas", "R. Servedio", "G. Valiant", "P. Valiant" ],
      "venue" : "In Proceedings of the ACM-SIAM Symposium on Discrete Algorithms (SODA),",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2013
    }, {
      "title" : "Spectral techniques applied to sparse random graphs",
      "author" : [ "Uriel Feige", "Eran Ofek" ],
      "venue" : "Random Structures & Algorithms,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2005
    }, {
      "title" : "On the second eigenvalue of random regular graphs",
      "author" : [ "Joel Friedman", "Jeff Kahn", "Endre Szemeredi" ],
      "venue" : "In Proceedings of the twenty-first annual ACM symposium on Theory of computing,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 1989
    }, {
      "title" : "Learning mixtures of gaussians in high dimensions",
      "author" : [ "Rong Ge", "Qingqing Huang", "Sham M. Kakade" ],
      "venue" : "In Proceedings of the Symposium on Theory of Computing,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2015
    }, {
      "title" : "Upper bounds on poisson tail probabilities",
      "author" : [ "Peter W Glynn" ],
      "venue" : "Operations research letters,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 1987
    }, {
      "title" : "Streaming and sublinear approximation of entropy and information distances",
      "author" : [ "S. Guha", "A. McGregor", "S. Venkatasubramanian" ],
      "venue" : "In Proceedings of the ACM-SIAM Symposium on Discrete Algorithms (SODA),",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2006
    }, {
      "title" : "Stochastic blockmodels: First steps",
      "author" : [ "Paul W Holland", "Kathryn Blackmond Laskey", "Samuel Leinhardt" ],
      "venue" : "Social networks,",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 1983
    }, {
      "title" : "Learning mixtures of spherical gaussians: moment methods and spectral decompositions",
      "author" : [ "Daniel Hsu", "Sham M Kakade" ],
      "venue" : "In Proceedings of the 4th conference on Innovations in Theoretical Computer Science,",
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 2013
    }, {
      "title" : "A spectral algorithm for learning hidden markov models",
      "author" : [ "Daniel Hsu", "Sham M Kakade", "Tong Zhang" ],
      "venue" : "Journal of Computer and System Sciences,",
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 2012
    }, {
      "title" : "Estimation of a discrete monotone density",
      "author" : [ "H.K. Jankowski", "J.A. Wellner" ],
      "venue" : "Electronic Journal of Statistics,",
      "citeRegEx" : "30",
      "shortCiteRegEx" : "30",
      "year" : 2009
    }, {
      "title" : "Efficiently learning mixtures of two gaussians",
      "author" : [ "Adam Tauman Kalai", "Ankur Moitra", "Gregory Valiant" ],
      "venue" : "In Proceedings of the 42nd ACM symposium on Theory of computing,",
      "citeRegEx" : "31",
      "shortCiteRegEx" : "31",
      "year" : 2010
    }, {
      "title" : "Spectral redemption in clustering sparse networks",
      "author" : [ "Florent Krzakala", "Cristopher Moore", "Elchanan Mossel", "Joe Neeman", "Allan Sly", "Lenka Zdeborová", "Pan Zhang" ],
      "venue" : "Proceedings of the National Academy of Sciences,",
      "citeRegEx" : "32",
      "shortCiteRegEx" : "32",
      "year" : 2013
    }, {
      "title" : "Sparse random graphs: regularization and concentration of the laplacian",
      "author" : [ "Can M Le", "Elizaveta Levina", "Roman Vershynin" ],
      "venue" : "arXiv preprint arXiv:1502.03049,",
      "citeRegEx" : "33",
      "shortCiteRegEx" : "33",
      "year" : 2015
    }, {
      "title" : "Concentration and regularization of random graphs",
      "author" : [ "Can M Le", "Roman Vershynin" ],
      "venue" : "arXiv preprint arXiv:1506.00669,",
      "citeRegEx" : "34",
      "shortCiteRegEx" : "34",
      "year" : 2015
    }, {
      "title" : "Neural word embedding as implicit matrix factorization",
      "author" : [ "Omer Levy", "Yoav Goldberg" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "35",
      "shortCiteRegEx" : "35",
      "year" : 2014
    }, {
      "title" : "Community detection thresholds and the weak ramanujan property",
      "author" : [ "Laurent Massoulié" ],
      "venue" : "In Proceedings of the 46th Annual ACM Symposium on Theory of Computing,",
      "citeRegEx" : "36",
      "shortCiteRegEx" : "36",
      "year" : 2014
    }, {
      "title" : "Efficient estimation of word representations in vector space",
      "author" : [ "Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean" ],
      "venue" : "arXiv preprint arXiv:1301.3781,",
      "citeRegEx" : "37",
      "shortCiteRegEx" : "37",
      "year" : 2013
    }, {
      "title" : "Settling the polynomial learnability of mixtures of gaussians",
      "author" : [ "Ankur Moitra", "Gregory Valiant" ],
      "venue" : "In Foundations of Computer Science (FOCS),",
      "citeRegEx" : "38",
      "shortCiteRegEx" : "38",
      "year" : 2010
    }, {
      "title" : "Learning nonsingular phylogenies and hidden Markov models",
      "author" : [ "E. Mossel", "S. Roch" ],
      "venue" : "Annals of Applied Probability,",
      "citeRegEx" : "39",
      "shortCiteRegEx" : "39",
      "year" : 2006
    }, {
      "title" : "Stochastic block models and reconstruction",
      "author" : [ "Elchanan Mossel", "Joe Neeman", "Allan Sly" ],
      "venue" : "arXiv preprint arXiv:1202.1499,",
      "citeRegEx" : "40",
      "shortCiteRegEx" : "40",
      "year" : 2012
    }, {
      "title" : "Consistency thresholds for binary symmetric block models",
      "author" : [ "Elchanan Mossel", "Joe Neeman", "Allan Sly" ],
      "venue" : "arXiv preprint arXiv:1407.1591,",
      "citeRegEx" : "41",
      "shortCiteRegEx" : "41",
      "year" : 2014
    }, {
      "title" : "Optimal algorithms for testing closeness of discrete distributions",
      "author" : [ "S. on Chan", "I. Diakonikolas", "G. Valiant", "P. Valiant" ],
      "venue" : "In Proceedings of the ACM-SIAM Symposium on Discrete Algorithms (SODA),",
      "citeRegEx" : "42",
      "shortCiteRegEx" : "42",
      "year" : 2014
    }, {
      "title" : "Estimating entropy on m bins given fewer than m samples",
      "author" : [ "L. Paninski" ],
      "venue" : "IEEE Transactions on Information Theory,",
      "citeRegEx" : "43",
      "shortCiteRegEx" : "43",
      "year" : 2004
    }, {
      "title" : "Strong lower bounds for approximating distribution support size and the distinct elements problem",
      "author" : [ "S. Raskhodnikova", "D. Ron", "A. Shpilka", "A. Smith" ],
      "venue" : "SIAM Journal on Computing,",
      "citeRegEx" : "44",
      "shortCiteRegEx" : "44",
      "year" : 2009
    }, {
      "title" : "Model-based word embeddings from decompositions of count matrices",
      "author" : [ "Karl Stratos", "Michael Collins", "Daniel Hsu" ],
      "venue" : "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing,",
      "citeRegEx" : "45",
      "shortCiteRegEx" : "45",
      "year" : 2015
    }, {
      "title" : "A spectral algorithm for learning class-based n-gram models of natural language",
      "author" : [ "Karl Stratos", "Michael Collins Do-Kyum Kim", "Daniel Hsu" ],
      "venue" : "In Proceedings of the 30th Conference on Uncertainty in Artificial Intelligence,",
      "citeRegEx" : "46",
      "shortCiteRegEx" : "46",
      "year" : 2014
    }, {
      "title" : "Estimating the unseen: an n/ log n-sample estimator for entropy and support size, shown optimal via new clts",
      "author" : [ "G. Valiant", "P. Valiant" ],
      "venue" : "In Symposium on Theory of Computing (STOC),",
      "citeRegEx" : "47",
      "shortCiteRegEx" : "47",
      "year" : 2011
    }, {
      "title" : "The power of linear estimators",
      "author" : [ "G. Valiant", "P. Valiant" ],
      "venue" : "In Symposium on Foundations of Computer Science (FOCS),",
      "citeRegEx" : "48",
      "shortCiteRegEx" : "48",
      "year" : 2011
    }, {
      "title" : "Estimating the unseen: improved estimators for entropy and other properties",
      "author" : [ "G. Valiant", "P. Valiant" ],
      "venue" : "In Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "49",
      "shortCiteRegEx" : "49",
      "year" : 2013
    }, {
      "title" : "An automatic inequality prover and instance optimal identity testing",
      "author" : [ "G. Valiant", "P. Valiant" ],
      "venue" : "In IEEE Symposium on Foundations of Computer Science (FOCS),",
      "citeRegEx" : "50",
      "shortCiteRegEx" : "50",
      "year" : 2014
    }, {
      "title" : "A spectral algorithm for learning mixture models",
      "author" : [ "Santosh Vempala", "Grant Wang" ],
      "venue" : "Journal of Computer and System Sciences,",
      "citeRegEx" : "51",
      "shortCiteRegEx" : "51",
      "year" : 2004
    } ],
    "referenceMentions" : [ {
      "referenceID" : 5,
      "context" : "However, one can recover the model parameters using sampled trigram sequences; this last step is straightforward (and sample efficient as it uses only an additional Ω(1/ 2) trigrams) when given an accurate estimate of B (see [6] for the moment structure in the trigrams).",
      "startOffset" : 225,
      "endOffset" : 228
    }, {
      "referenceID" : 40,
      "context" : "In the case of 2-topic models, the community detection lower bounds [41][32][52] imply that Θ(M) bigrams are necessary to even distinguish between the case that the underlying model is simply the uniform distribution over bigrams versus the case of a 2-topic model in which each topic corresponds to a uniform distribution over disjoint subsets of M/2 words.",
      "startOffset" : 68,
      "endOffset" : 72
    }, {
      "referenceID" : 31,
      "context" : "In the case of 2-topic models, the community detection lower bounds [41][32][52] imply that Θ(M) bigrams are necessary to even distinguish between the case that the underlying model is simply the uniform distribution over bigrams versus the case of a 2-topic model in which each topic corresponds to a uniform distribution over disjoint subsets of M/2 words.",
      "startOffset" : 72,
      "endOffset" : 76
    }, {
      "referenceID" : 26,
      "context" : "One of the most well studied community models is the stochastic block model [27].",
      "startOffset" : 76,
      "endOffset" : 80
    }, {
      "referenceID" : 21,
      "context" : "[22, 40, 32, 33]).",
      "startOffset" : 0,
      "endOffset" : 16
    }, {
      "referenceID" : 39,
      "context" : "[22, 40, 32, 33]).",
      "startOffset" : 0,
      "endOffset" : 16
    }, {
      "referenceID" : 31,
      "context" : "[22, 40, 32, 33]).",
      "startOffset" : 0,
      "endOffset" : 16
    }, {
      "referenceID" : 32,
      "context" : "[22, 40, 32, 33]).",
      "startOffset" : 0,
      "endOffset" : 16
    }, {
      "referenceID" : 40,
      "context" : "In the past year, for both the exact recovery problem and the detection problem, the exact tradeoffs between α, β, and M were established, down to subconstant factors [41, 1, 36].",
      "startOffset" : 167,
      "endOffset" : 178
    }, {
      "referenceID" : 0,
      "context" : "In the past year, for both the exact recovery problem and the detection problem, the exact tradeoffs between α, β, and M were established, down to subconstant factors [41, 1, 36].",
      "startOffset" : 167,
      "endOffset" : 178
    }, {
      "referenceID" : 35,
      "context" : "In the past year, for both the exact recovery problem and the detection problem, the exact tradeoffs between α, β, and M were established, down to subconstant factors [41, 1, 36].",
      "startOffset" : 167,
      "endOffset" : 178
    }, {
      "referenceID" : 18,
      "context" : "[19, 2]).",
      "startOffset" : 0,
      "endOffset" : 7
    }, {
      "referenceID" : 1,
      "context" : "[19, 2]).",
      "startOffset" : 0,
      "endOffset" : 7
    }, {
      "referenceID" : 36,
      "context" : "On the more applied side, some of the most impactful advances in natural language processing over the past two years has been work on “word embeddings” [37, 35, 46, 9].",
      "startOffset" : 152,
      "endOffset" : 167
    }, {
      "referenceID" : 34,
      "context" : "On the more applied side, some of the most impactful advances in natural language processing over the past two years has been work on “word embeddings” [37, 35, 46, 9].",
      "startOffset" : 152,
      "endOffset" : 167
    }, {
      "referenceID" : 45,
      "context" : "On the more applied side, some of the most impactful advances in natural language processing over the past two years has been work on “word embeddings” [37, 35, 46, 9].",
      "startOffset" : 152,
      "endOffset" : 167
    }, {
      "referenceID" : 8,
      "context" : "On the more applied side, some of the most impactful advances in natural language processing over the past two years has been work on “word embeddings” [37, 35, 46, 9].",
      "startOffset" : 152,
      "endOffset" : 167
    }, {
      "referenceID" : 44,
      "context" : "see [45])—this is especially noticeable in the relatively poor quality of the embeddings for relatively rare words.",
      "startOffset" : 4,
      "endOffset" : 8
    }, {
      "referenceID" : 9,
      "context" : "The recent theoretical work [10] sheds some light on why current approaches are so successful, yet the following question largely remains: Is there a more accurate way to recover the best rank-d approximation of the underlying matrix than simply computing the best rank-d approximation for the (noisy) matrix of empirical counts? Efficient Algorithms for Latent Variable Models.",
      "startOffset" : 28,
      "endOffset" : 32
    }, {
      "referenceID" : 28,
      "context" : "This body of work includes work on learning HMMs [29, 39, 17], recovering low-rank structure [8, 7, 14], and learning or clustering various structured distributions such as Gaussian mixture models [20, 51, 38, 13, 28, 31, 24] and latent dirichlet allocation (a very popular topic model) [5].",
      "startOffset" : 49,
      "endOffset" : 61
    }, {
      "referenceID" : 38,
      "context" : "This body of work includes work on learning HMMs [29, 39, 17], recovering low-rank structure [8, 7, 14], and learning or clustering various structured distributions such as Gaussian mixture models [20, 51, 38, 13, 28, 31, 24] and latent dirichlet allocation (a very popular topic model) [5].",
      "startOffset" : 49,
      "endOffset" : 61
    }, {
      "referenceID" : 16,
      "context" : "This body of work includes work on learning HMMs [29, 39, 17], recovering low-rank structure [8, 7, 14], and learning or clustering various structured distributions such as Gaussian mixture models [20, 51, 38, 13, 28, 31, 24] and latent dirichlet allocation (a very popular topic model) [5].",
      "startOffset" : 49,
      "endOffset" : 61
    }, {
      "referenceID" : 7,
      "context" : "This body of work includes work on learning HMMs [29, 39, 17], recovering low-rank structure [8, 7, 14], and learning or clustering various structured distributions such as Gaussian mixture models [20, 51, 38, 13, 28, 31, 24] and latent dirichlet allocation (a very popular topic model) [5].",
      "startOffset" : 93,
      "endOffset" : 103
    }, {
      "referenceID" : 6,
      "context" : "This body of work includes work on learning HMMs [29, 39, 17], recovering low-rank structure [8, 7, 14], and learning or clustering various structured distributions such as Gaussian mixture models [20, 51, 38, 13, 28, 31, 24] and latent dirichlet allocation (a very popular topic model) [5].",
      "startOffset" : 93,
      "endOffset" : 103
    }, {
      "referenceID" : 13,
      "context" : "This body of work includes work on learning HMMs [29, 39, 17], recovering low-rank structure [8, 7, 14], and learning or clustering various structured distributions such as Gaussian mixture models [20, 51, 38, 13, 28, 31, 24] and latent dirichlet allocation (a very popular topic model) [5].",
      "startOffset" : 93,
      "endOffset" : 103
    }, {
      "referenceID" : 19,
      "context" : "This body of work includes work on learning HMMs [29, 39, 17], recovering low-rank structure [8, 7, 14], and learning or clustering various structured distributions such as Gaussian mixture models [20, 51, 38, 13, 28, 31, 24] and latent dirichlet allocation (a very popular topic model) [5].",
      "startOffset" : 197,
      "endOffset" : 225
    }, {
      "referenceID" : 50,
      "context" : "This body of work includes work on learning HMMs [29, 39, 17], recovering low-rank structure [8, 7, 14], and learning or clustering various structured distributions such as Gaussian mixture models [20, 51, 38, 13, 28, 31, 24] and latent dirichlet allocation (a very popular topic model) [5].",
      "startOffset" : 197,
      "endOffset" : 225
    }, {
      "referenceID" : 37,
      "context" : "This body of work includes work on learning HMMs [29, 39, 17], recovering low-rank structure [8, 7, 14], and learning or clustering various structured distributions such as Gaussian mixture models [20, 51, 38, 13, 28, 31, 24] and latent dirichlet allocation (a very popular topic model) [5].",
      "startOffset" : 197,
      "endOffset" : 225
    }, {
      "referenceID" : 12,
      "context" : "This body of work includes work on learning HMMs [29, 39, 17], recovering low-rank structure [8, 7, 14], and learning or clustering various structured distributions such as Gaussian mixture models [20, 51, 38, 13, 28, 31, 24] and latent dirichlet allocation (a very popular topic model) [5].",
      "startOffset" : 197,
      "endOffset" : 225
    }, {
      "referenceID" : 27,
      "context" : "This body of work includes work on learning HMMs [29, 39, 17], recovering low-rank structure [8, 7, 14], and learning or clustering various structured distributions such as Gaussian mixture models [20, 51, 38, 13, 28, 31, 24] and latent dirichlet allocation (a very popular topic model) [5].",
      "startOffset" : 197,
      "endOffset" : 225
    }, {
      "referenceID" : 30,
      "context" : "This body of work includes work on learning HMMs [29, 39, 17], recovering low-rank structure [8, 7, 14], and learning or clustering various structured distributions such as Gaussian mixture models [20, 51, 38, 13, 28, 31, 24] and latent dirichlet allocation (a very popular topic model) [5].",
      "startOffset" : 197,
      "endOffset" : 225
    }, {
      "referenceID" : 23,
      "context" : "This body of work includes work on learning HMMs [29, 39, 17], recovering low-rank structure [8, 7, 14], and learning or clustering various structured distributions such as Gaussian mixture models [20, 51, 38, 13, 28, 31, 24] and latent dirichlet allocation (a very popular topic model) [5].",
      "startOffset" : 197,
      "endOffset" : 225
    }, {
      "referenceID" : 4,
      "context" : "This body of work includes work on learning HMMs [29, 39, 17], recovering low-rank structure [8, 7, 14], and learning or clustering various structured distributions such as Gaussian mixture models [20, 51, 38, 13, 28, 31, 24] and latent dirichlet allocation (a very popular topic model) [5].",
      "startOffset" : 287,
      "endOffset" : 290
    }, {
      "referenceID" : 5,
      "context" : "A number of these methods essentially can be phrased as solving an inverse moments problem, and the work in [6] provides a unifying viewpoint for computationally efficient estimation for many of these models under a tensor decomposition perspective.",
      "startOffset" : 108,
      "endOffset" : 111
    }, {
      "referenceID" : 42,
      "context" : "This work includes algorithms for estimating basic statistical properties such as entropy [43, 26, 47, 49], support size [44, 47], distance between distributions [47, 49, 48], and various hypothesis tests, such as whether two distributions are very similar, versus significantly different [11, 42, 50, 15], etc.",
      "startOffset" : 90,
      "endOffset" : 106
    }, {
      "referenceID" : 25,
      "context" : "This work includes algorithms for estimating basic statistical properties such as entropy [43, 26, 47, 49], support size [44, 47], distance between distributions [47, 49, 48], and various hypothesis tests, such as whether two distributions are very similar, versus significantly different [11, 42, 50, 15], etc.",
      "startOffset" : 90,
      "endOffset" : 106
    }, {
      "referenceID" : 46,
      "context" : "This work includes algorithms for estimating basic statistical properties such as entropy [43, 26, 47, 49], support size [44, 47], distance between distributions [47, 49, 48], and various hypothesis tests, such as whether two distributions are very similar, versus significantly different [11, 42, 50, 15], etc.",
      "startOffset" : 90,
      "endOffset" : 106
    }, {
      "referenceID" : 48,
      "context" : "This work includes algorithms for estimating basic statistical properties such as entropy [43, 26, 47, 49], support size [44, 47], distance between distributions [47, 49, 48], and various hypothesis tests, such as whether two distributions are very similar, versus significantly different [11, 42, 50, 15], etc.",
      "startOffset" : 90,
      "endOffset" : 106
    }, {
      "referenceID" : 43,
      "context" : "This work includes algorithms for estimating basic statistical properties such as entropy [43, 26, 47, 49], support size [44, 47], distance between distributions [47, 49, 48], and various hypothesis tests, such as whether two distributions are very similar, versus significantly different [11, 42, 50, 15], etc.",
      "startOffset" : 121,
      "endOffset" : 129
    }, {
      "referenceID" : 46,
      "context" : "This work includes algorithms for estimating basic statistical properties such as entropy [43, 26, 47, 49], support size [44, 47], distance between distributions [47, 49, 48], and various hypothesis tests, such as whether two distributions are very similar, versus significantly different [11, 42, 50, 15], etc.",
      "startOffset" : 121,
      "endOffset" : 129
    }, {
      "referenceID" : 46,
      "context" : "This work includes algorithms for estimating basic statistical properties such as entropy [43, 26, 47, 49], support size [44, 47], distance between distributions [47, 49, 48], and various hypothesis tests, such as whether two distributions are very similar, versus significantly different [11, 42, 50, 15], etc.",
      "startOffset" : 162,
      "endOffset" : 174
    }, {
      "referenceID" : 48,
      "context" : "This work includes algorithms for estimating basic statistical properties such as entropy [43, 26, 47, 49], support size [44, 47], distance between distributions [47, 49, 48], and various hypothesis tests, such as whether two distributions are very similar, versus significantly different [11, 42, 50, 15], etc.",
      "startOffset" : 162,
      "endOffset" : 174
    }, {
      "referenceID" : 47,
      "context" : "This work includes algorithms for estimating basic statistical properties such as entropy [43, 26, 47, 49], support size [44, 47], distance between distributions [47, 49, 48], and various hypothesis tests, such as whether two distributions are very similar, versus significantly different [11, 42, 50, 15], etc.",
      "startOffset" : 162,
      "endOffset" : 174
    }, {
      "referenceID" : 10,
      "context" : "This work includes algorithms for estimating basic statistical properties such as entropy [43, 26, 47, 49], support size [44, 47], distance between distributions [47, 49, 48], and various hypothesis tests, such as whether two distributions are very similar, versus significantly different [11, 42, 50, 15], etc.",
      "startOffset" : 289,
      "endOffset" : 305
    }, {
      "referenceID" : 41,
      "context" : "This work includes algorithms for estimating basic statistical properties such as entropy [43, 26, 47, 49], support size [44, 47], distance between distributions [47, 49, 48], and various hypothesis tests, such as whether two distributions are very similar, versus significantly different [11, 42, 50, 15], etc.",
      "startOffset" : 289,
      "endOffset" : 305
    }, {
      "referenceID" : 49,
      "context" : "This work includes algorithms for estimating basic statistical properties such as entropy [43, 26, 47, 49], support size [44, 47], distance between distributions [47, 49, 48], and various hypothesis tests, such as whether two distributions are very similar, versus significantly different [11, 42, 50, 15], etc.",
      "startOffset" : 289,
      "endOffset" : 305
    }, {
      "referenceID" : 14,
      "context" : "This work includes algorithms for estimating basic statistical properties such as entropy [43, 26, 47, 49], support size [44, 47], distance between distributions [47, 49, 48], and various hypothesis tests, such as whether two distributions are very similar, versus significantly different [11, 42, 50, 15], etc.",
      "startOffset" : 289,
      "endOffset" : 305
    }, {
      "referenceID" : 2,
      "context" : "[3, 4, 50], with stronger information theoretic optimality guarantees.",
      "startOffset" : 0,
      "endOffset" : 10
    }, {
      "referenceID" : 3,
      "context" : "[3, 4, 50], with stronger information theoretic optimality guarantees.",
      "startOffset" : 0,
      "endOffset" : 10
    }, {
      "referenceID" : 49,
      "context" : "[3, 4, 50], with stronger information theoretic optimality guarantees.",
      "startOffset" : 0,
      "endOffset" : 10
    }, {
      "referenceID" : 15,
      "context" : "where the domain of the distribution has a total ordering or where the distribution is monotonic or unimodal [16, 12, 30, 21].",
      "startOffset" : 109,
      "endOffset" : 125
    }, {
      "referenceID" : 11,
      "context" : "where the domain of the distribution has a total ordering or where the distribution is monotonic or unimodal [16, 12, 30, 21].",
      "startOffset" : 109,
      "endOffset" : 125
    }, {
      "referenceID" : 29,
      "context" : "where the domain of the distribution has a total ordering or where the distribution is monotonic or unimodal [16, 12, 30, 21].",
      "startOffset" : 109,
      "endOffset" : 125
    }, {
      "referenceID" : 20,
      "context" : "where the domain of the distribution has a total ordering or where the distribution is monotonic or unimodal [16, 12, 30, 21].",
      "startOffset" : 109,
      "endOffset" : 125
    }, {
      "referenceID" : 22,
      "context" : "This simple idea was first introduced by [23], and followed by analysis works in [22] and many others.",
      "startOffset" : 41,
      "endOffset" : 45
    }, {
      "referenceID" : 21,
      "context" : "This simple idea was first introduced by [23], and followed by analysis works in [22] and many others.",
      "startOffset" : 81,
      "endOffset" : 85
    }, {
      "referenceID" : 32,
      "context" : "Recently in [33] and [34] the authors provided clean and clever proofs to show that any such “regularization” essentially leads to better spectral concentration for the adjacency matrix of random graphs whose row/column sums are roughly uniform in expectation.",
      "startOffset" : 12,
      "endOffset" : 16
    }, {
      "referenceID" : 33,
      "context" : "Recently in [33] and [34] the authors provided clean and clever proofs to show that any such “regularization” essentially leads to better spectral concentration for the adjacency matrix of random graphs whose row/column sums are roughly uniform in expectation.",
      "startOffset" : 21,
      "endOffset" : 25
    }, {
      "referenceID" : 33,
      "context" : "When directly applied to the empirical bins with such spillover, the existing results of “regularization” in [34] do not lead to the desired concentration result.",
      "startOffset" : 109,
      "endOffset" : 113
    }, {
      "referenceID" : 18,
      "context" : "For example, in a recent paper [19] on community detection, after obtaining a crude classification of nodes using spectral algorithm, one round of a “correction” routine is applied to each node based on its connections to the graph partition given by the first round.",
      "startOffset" : 31,
      "endOffset" : 35
    }, {
      "referenceID" : 17,
      "context" : "Another example is given in [18] in the context of solving random quadratic equations, where local refinement of the solution follows the spectral method initialization.",
      "startOffset" : 28,
      "endOffset" : 32
    }, {
      "referenceID" : 33,
      "context" : "Then we leverage the clever proof techniques from [34] to show that if spillover effect is small, regularized truncated SVD can be applied to estimate the entries of ∆ restricted to each bin.",
      "startOffset" : 50,
      "endOffset" : 54
    }, {
      "referenceID" : 33,
      "context" : "For block A1 whose rows and columns all correspond to the “good words” with roughly uniform marginals, we show its concentration by applying the result in [34].",
      "startOffset" : 155,
      "endOffset" : 159
    } ],
    "year" : 2017,
    "abstractText" : "We consider the problem of accurately recovering a matrix B of size M ×M , which represents a probability distribution overM outcomes, given access to an observed matrix of “counts” generated by taking independent samples from the distribution B. How can structural properties of the underlying matrix B be leveraged to yield computationally efficient and information theoretically optimal reconstruction algorithms? When can accurate reconstruction be accomplished in the sparse data regime? This basic problem lies at the core of a number of questions that are currently being considered by different communities, including community detection in sparse random graphs, learning structured models such as topic models or hidden Markov models, and the efforts from the natural language processing community to compute “word embeddings”. Many aspects of this problem—both in terms of learning and property testing/estimation and on both the algorithmic and information theoretic sides—remain open. Our results apply to the setting where B has a particular rank 2 structure. For this setting, we propose an efficient (and practically viable) algorithm that accurately recovers the underlying M × M matrix using Θ(M) samples. This result easily translates to Θ(M) sample algorithms for learning topic models with two topics over dictionaries of size M , and learning hidden Markov Models with two hidden states and observation distributions supported on M elements. These linear sample complexities are optimal, up to constant factors, in an extremely strong sense: even testing basic properties of the underlying matrix (such as whether it has rank 1 or 2) requires Ω(M) samples. Furthermore, we provide an even stronger lower bound where distinguishing whether a sequence of observations were drawn from the uniform distribution over M observations versus being generated by an HMM with two hidden states requires Ω(M) observations. This precludes sublinear-sample hypothesis tests for basic properties, such as identity or uniformity, as well as sublinear sample estimators for quantities such as the entropy rate of HMMs. This impossibility of sublinear-sample property testing in these settings is intriguing and underscores the significant differences between these structured settings and the standard setting of drawing i.i.d samples from an unstructured distribution of support size M . ∗MIT. Email: qqh@mit.edu. †University of Washington. Email: sham@cs.washington.edu ‡Stanford University. Email: kweihao@gmail.com §Stanford University. Email: valiant@stanford.edu. Gregory and Weihao’s contributions were supported by NSF CAREER Award CCF-1351108, and a research grant from the Okawa Foundation. ar X iv :1 60 2. 06 58 6v 1 [ cs .L G ] 2 1 Fe b 20 16",
    "creator" : "LaTeX with hyperref package"
  }
}