{
  "name" : "1506.04416.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Bayesian Dark Knowledge",
    "authors" : [ "Anoop Korattikara", "Vivek Rathod", "Kevin Murphy" ],
    "emails" : [ "kpmurphy}@google.com", "m.welling@uva.nl" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Deep neural networks (DNNs) have recently been achieving state of the art results in many fields. However, their predictions are often over confident, which is a problem in applications such as active learning, reinforcement learning (including bandits), and classifier fusion, which all rely on good estimates of uncertainty.\nA principled way to tackle this problem is to use Bayesian inference. Specifically, we first compute the posterior distribution over the model parameters, p(θ|DN ) ∝ p(θ) ∏N i=1 p(yi|xi, θ), where DN = {(xi, yi)}Ni=1, xi ∈ XD is the i’th input (where D is the number of features), and yi ∈ Y is the i’th output. Then we compute the posterior predictive distribution, p(y|x,DN ) =∫ p(y|x, θ)p(θ|DN )dθ, for each test point x.\nFor reasons of computational speed, it is common to approximate the posterior distribution by a point estimate such as the MAP estimate, θ̂ = argmax p(θ|DN ). When N is large, we often use stochastic gradient descent (SGD) to compute θ̂. Finally, we make a plug-in approximation to the predictive distribution: p(y|x,DN ) ≈ p(y|x, θ̂). Unfortunately, this loses most of the benefits of the Bayesian approach, since uncertainty in the parameters (which induces uncertainty in the predictions) is ignored.\nVarious ways of more accurately approximating p(θ|DN ) (and hence p(y|x,DN )) have been developed. Recently, [HLA15] proposed a method called “probabilistic backpropagation” (PBP) based on an online version of expectation propagation (EP), (i.e., using repeated assumed density filtering (ADF)), where the posterior is approximated as a product of univariate Gaussians, one per parameter: p(θ|DN ) ≈ q(θ) , ∏ iN (θi|mi, vi).\n[BCKW15] proposed an approach called “Bayes by Backprop” (BBB) based on variational Bayes (VB), extending earlier work of [Gra11, KW14, RMW14]. These approaches also use a product of\nar X\niv :1\n50 6.\n04 41\n6v 1\n[ cs\n.L G\n] 1\n4 Ju\nn 20\n15\nunivariate Gaussians to approximate the posterior, but they optimized a different objective function than the EP approach. Both EP and VB methods use online inference, and both methods approximate the posterior predictive using p(y|x,DN ) ≈ q(y|x) , ∫ p(y|x, θ)q(θ)dθ.\nAlthough EP and VB are relatively fast, there are several problems with these kinds of approximations: (1) they can be poor approximations when the posterior p(θ|DN ) does not factorize, or if it has multi-modality or skew; (2) at test time, computing the predictive density p(y|x,DN ) can be much slower than using the plug-in approximation, because of the need to integrate out the parameters at run time; (3) they need to use double the memory of a standard plug-in method (to store the mean and variance of each parameter), which can be problematic in memory-limited settings such as mobile phones; (4) they can be quite complicated to derive and implement.\nA common alternative to EP and VB is to use MCMC methods to approximate p(θ|DN ). Traditional MCMC methods are batch algorithms, that scale poorly with dataset size. However, recently a method called stochastic gradient Langevin dynamics (SGLD) [WT11] has been devised that can draw samples from the posterior in an online fashion, just as SGD updates a point estimate of the parameters online. Furthermore, various extensions of SGLD have been proposed, including stochastic gradient hybrid Monte Carlo (SGHMC) [CFG14], stochastic gradient Nosé-Hoover Thermostat (SGNHT) [DFB+14] (which improves upon SGHMC), stochastic gradient Fisher scoring (SGFS) [AKW12] (which uses second order information), stochastic gradient Riemannian Langevin Dynamics (for inferring parameters in the probability simplex) [PT13], distributed SGLD [ASW14], etc. However, in this paper, we will just use “vanilla” SGLD [WT11].\nAll these MCMC methods (whether batch or online) produce a Monte Carlo approximation to the posterior, q(θ) = 1S ∑S s=1 δ(θ − θs), where S is the number of samples. Such an approximation can be more accurate than that produced by EP or VB, and the method is much easier to implement (for SGLD, you essentially just add Gaussian noise to your SGD updates). However, at test time, things are S times slower than using a plug-in estimate, since we need to compute q(y|x) = 1S ∑S s=1 p(y|x, θs), and the memory requirements are S times bigger, since we need to store the θs. (For our largest experiment, our DNN has 500k parameters, so we can only afford to store a single sample.)\nIn this paper, we propose to train a parametric model S(y|x,w) to approximate the Monte Carlo posterior predictive distribution q(y|x) in order to gain the benefits of the Bayesian approach while only using the same run time cost as the plugin method. Following [HVD14], we call q(y|x) the “teacher” and S(y|x,w) the “student”. We use SGLD to estimate q(θ) and hence q(y|x) online; we simultaneously train the student online to minimize KL(q(y|x)||S(y|x,w)). We give the details in Section 2.\nSimilar ideas have been proposed in the past. In particular, [SG05] also trained a parametric student model to approximate a Monte Carlo teacher. However, they used batch training and they used mixture models for the student. By contrast, we use online training (and can thus handle larger datasets), and use deep neural networks for the student.\n[HVD14] also trained a student neural network to emulate the predictions of a (larger) teacher network, a process they call “distillation”. This extends earlier work of [BCNM06] which approximated an ensemble of classifiers by a single one. [RBK+14] further extend this approach by training a student network that is deeper, but thinner (hence the term “fit nets”), than the teacher, and injecting supervision at multiple levels. The key difference from our work is that our teacher is generated using MCMC, and our goal is not just to improve classification accuracy, but also to get reliable probabilistic predictions, especially away from the training data. [HVD14] coined the term “dark knowledge” to represent the information which is “hidden” inside the teacher network, and which can then be distilled into the student. We therefore call our approach “Bayesian dark knowledge”.\nIn summary, our contributions are as follows. First, we show how to combine online MCMC methods with model distillation in order to get a simple, scalable approach to Bayesian inference in neural networks (and other kinds of models). Second, we show that our probabilistic predictions lead to improved log likelihood scores on the test set compared to SGD and the recently proposed EP and VB approaches."
    }, {
      "heading" : "2 Methods",
      "text" : "Our goal is to train a student neural network (SNN) to approximate the Bayesian predictive distribution of the teacher, which is a Monte Carlo ensemble of teacher neural networks (TNN).\nIf we denote the predictions of the teacher by p(y|x,DN ) and the parameters of the student network by w, our objective becomes\nL(w|x) = KL(p(y|x,DN )||S(y|x,w)) = −Ep(y|x,DN ) logS(y|x,w) + const = − ∫ [∫ p(y|x, θ)p(θ|DN )dθ ] logS(y|x,w)dy\n= − ∫ p(θ|DN ) ∫ p(y|x, θ) logS(y|x,w)dy dθ\n= − ∫ p(θ|DN ) [ Ep(y|x,θ) logS(y|x,w) ] dθ (1)\nTo make this a function just of w, we need to integrate out x. For this, we need a dataset to train the student network on, which we will denote by D′. Note that points in this dataset do not need ground truth labels; instead the labels (which will be probability distributions) will be provided by the teacher. The choice of student data controls the domain over which the student will make accurate predictions. For low dimensional problems (such as in Section 3.1), we can uniformly sample the input domain. For higher dimensional problems, we can sample “near” the training data, for example by perturbing the inputs slightly. In any case, we will compute a Monte Carlo approximation to the loss as follows:\nL̂mc−x(w) =\n∫ p(x)L(w|x)dx ≈ 1 |D′| ∑ x′∈D′ L(w|x′) (2)\nUnfortunately, we are not quite done, since computing L(w|x) requires computing an integral wrt p(θ|DN ) (see Equation 1), which is not analytically tractable. However, we can approximate this by Monte Carlo as well, as follows:\nL̂mc−θ(w|x) = − 1 |Θ| ∑ θ∗∈Θ Ep(y|x,θ∗) logS(y|x,w) (3)\nwhere Θ is a set of samples from p(θ|DN ). Putting these two approximations together, we get\nL̂(w) = − 1 |Θ| 1 |D′| ∑ θ∗∈Θ ∑ x′∈D′ Ep(y|x,θ∗) logS(y|x,w) (4)\nIn practice, it can take a lot of memory to pre-compute and store the set of parameters samples Θ and the set of data samples D′. Hence we use the online algorithm shown in Algorithm 1 below, which has much lower memory requirements.\nThe hyper-parameters λ and γ from Algorithm 1 control the strength of the priors for the teacher and student networks. We use simple spherical Gaussian priors (equivalent to L2 regularization); we set the precision (strength) of these Gaussian priors by cross-validation. Typically λ γ, since the student gets to “see” more data than the teacher. This is true for two reasons: first, the teacher is trained to predict a single label per input, whereas the student is trained to predict a distribution, which contains more information (as argued in [HVD14]); second, the teacher makes multiple passes over the same training data, whereas the student sees “fresh” randomly generated data D′ at each step."
    }, {
      "heading" : "2.1 Classification",
      "text" : "For classification problems, each teacher network θ∗ models the observations using a standard softmax model, p(y = k|x, θ∗). We want to approximate this using a student network, which also has a\nAlgorithm 1: Distilled SGLD 1 Input: DN = {(xi, yi)}Ni=1, minibatch size M , number of iterations T , teacher learning schedule ηt, student learning schedule ρt, teacher prior λ, student prior γ 2 for t = 1 : T do 3 // Train teacher (SGLD step) 4 Sample minibatch indices S ⊂ [1, N ] of size M 5 Sample zt ∼ N (0, ηtI) 6 Update θt+1 := θt + ηt ( ∇θ log p(θ|λ) + NM ∑ i∈S ∇θ log p(yi|xi, θ) ) + zt 7 // Train student (SGD step) 8 Sample D′ of size M from student data generator 9 wt+1 := wt − ρt ( 1 M ∑ x′∈D′ ∇wL̂(w, θt+1|x′) + γwt )\nsoftmax output, S(y = k|x,w). Hence from Eqn. 3, our loss function estimate is the standard cross entropy loss:\nL̂(w, θ∗|x) = − K∑ k=1 p(y = k|x, θ∗) logS(y = k|x,w) (5)\nThe student network outputs βk(x,w) = logS(y = k|x,w). To estimate the gradient w.r.t. w, we just have to compute the gradients w.r.t. β and back-propagate through the network. These gradients are given by ∂L̂(w,θ\n∗|x) ∂βk(x,w) = −p(y = k|x, θ∗)."
    }, {
      "heading" : "2.2 Regression",
      "text" : "In regression, the observations are modeled as p(yi|xi, θ) = N (yi|f(xi|θ), λ−1n ) where f(x|θ) is the prediction of the TNN and λn is the noise precision. We want to approximate the predictive distribution as p(y|x,DN ) ≈ S(y|x,w) = N (y|µ(x,w), eα(x,w)). We will train a student network to output the parameters of the approximating distribution µ(x,w) and α(x,w); note that this is twice the number of outputs of the teacher network, since we want to capture the (data dependent) variance.1 The SNN outputs α(x,w) instead of directly predicting the variance σ2(x|w) to avoid dealing with positivity constraints during training.\nTo train the SNN, we will minimize the objective defined in Eqn. 3:\nL̂(w, θ∗|x) = 1 2\n[ α(x,w) + e−α(x,w) { (f(x|θ∗)− µ(x,w))2 + 1\nλn\n}] (6)\nNow, to estimate∇wL̂(w, θ∗|x), we just have to compute ∂L̂∂µ(x,w) and ∂L̂\n∂α(x,w) , and back propagate through the network. These gradients are:\n∂L̂(w, θ∗|x) ∂µ(x,w) = e−α(x,w) {µ(x,w)− f(x|θ∗)} (7)\n∂L̂(w, θ∗|x ∂α(x,w) = 1 2\n[ 1− e−α(x,w) { (f(x|θ∗)− µ(x,w))2 + 1\nλn\n}] (8)"
    }, {
      "heading" : "3 Experimental results",
      "text" : "In this section, we compare SGLD and distilled SGLD with other approximate inference methods, including the plugin approximation using SGD, the EP approach of [HLA15], the VB approach of [BCKW15], and Hamiltonian Monte Carlo (HMC) [Nea11], which is considered the “gold standard” for MCMC for neural nets. We implemented SGD and SGLD using the Torch library (torch.ch). For HMC, we used Stan (mc-stan.org). We perform this comparison for various classification and regression problems, as summarized in Table 1.\n1 This is not necessary in the classification case, since the softmax distribution already captures uncertainty.\nIdeally, we would apply all methods to all datasets, to enable a proper comparison. Unfortunately, this was not possible, for various reasons. First, the open source code for the EP approach only supports regression, so we could not evaluate this on classification problems. Second, we did not get access to the code for the VB approach in time for us to compare to it, so we just quote performance numbers from their paper [BCKW15]. Third, HMC is too slow to run on large problems, so we just applied it to the small “toy” problems. Nevertheless, our experiments show that our methods compare favorably to the others."
    }, {
      "heading" : "3.1 Toy 2d classification problem",
      "text" : "We start with a toy 2d binary classification problem, in order to visually illustrate the performance of different methods. We generate a synthetic dataset in 2 dimensions with 2 classes, 10 points per class. We then fit a multi layer perceptron (MLP) with one hidden layer of 10 ReLu units and 2 softmax outputs (denoted 2-10-2) using SGD. The resulting predictions are shown in Figure 1(a). We see the expected sigmoidal probability ramp orthogonal to the linear decision boundary. Unfortunately, this method predicts a label of 0 or 1 with very high confidence, even for points that are far from the training data (e.g., in the top left and bottom right corners).\nIn Figure 1(b), we show the result of HMC using 20k samples. This is the “true” posterior predictive density which we wish to approximate. In Figure 1(c), we show the result of SLGD using about 1000 samples. Specifically, we generate 100k samples, discard the first 2k for burnin, and then keep every 100’th sample. We see that this is a good approximation to the HMC distribution.\nIn Figures 1(d-f), we show the results of approximating the SGLD Monte Carlo predictive distribution with a single student MLP of various sizes. To train this student network, we sampled points at random from the domain of the input, [−10, 10] × [−10, 10]; this encourages the student to predict accurately at all locations, including those far from the training data. In (d), the student has the same size as the teacher (2-10-2), but this is too simple a model to capture the complexity of the predictive distribution (which is an average over models). In (e), the student has a larger hidden layer (2-100- 2); this works better. However, we get best results using a two hidden layer model (2-10-10-2), as shown in (f).\nIn Table 2, we show the KL divergence between the HMC distribution (which we consider as ground truth) and the various approximations mentioned above. We computed this by comparing the probability distributions pointwise on a 2d grid. The numbers match the qualitative results shown in Figure 1."
    }, {
      "heading" : "3.2 MNIST classification",
      "text" : "Now we consider the MNIST digit classification problem, which has N = 60k examples, 10 classes, and D = 784 features. The only preprocessing we do is divide the pixel values by 126 (as in [BCKW15]). We train only on 50K datapoints and use the remaining 10K for tuning hyper-\nparameters. This means our results are not strictly comparable to a lot of published work, which uses the whole dataset for training; however, the difference is likely to be small.\nFollowing [BCKW15], we use an MLP with 2 hidden layers with 400 hidden units per layer, ReLU activations, and softmax outputs; we denote this by 784-400-400-10. This model has 500k parameters.\nWe first fit this model by SGD, using these hyper parameters: fixed learning rate of ηt = 5e−6, prior precision λ = 1, minibatch size M = 100, number of iterations T = 1M . As shown in Table 3, our final error rate on the test set is 1.6%, which is a bit lower than the SGD number reported in [BCKW15].\nNext we fit this model by SGLD, using these hyper parameters: fixed learning rate of ηt = 5e − 6, thinning interval τ = 100, burn in iterations B = 1000, prior precision λ = 1, minibatch size M = 100. As shown in Table 3, our final error rate on the test set is about 1.4%, which is better than the SGD, dropout and BBB results from [BCKW15]. We believe that SGLD outperforms dropout because it is performing weighted model averaging whereas dropout uses an unweighted average.\nFinally, we consider using distillation, where the teacher is either an SGD plugin approximation to the posterior predictive, p(y|x, θ̂t), or an SGLD MC approximation, p(y|x, θ∗). We generate data for the student by adding Gaussian noise (with standard deviation of 0.1) to randomly sampled training points. (In the future, we would like to consider more sophisticated data perturbations, such as elastic distortions.) We consider two sizes of student model: one is the same as the teacher, and one is smaller, having only 200 hidden units in its two layers. We use an initial learning rate of ρ0 = 1e− 3, which we reduce by a factor of 0.5 every 1e5 iterations, a batch size of M = 100, and a prior precision of 0.001 (for the student). We use up to T = 1M iterations.\nAs we see in Table 4, a distilled SGLD network works almost as well as SGLD itself; furthermore, making the student smaller than the teacher does not hurt performance very much. We also see that distilling the predictions made by SGD does not work as well as distilled SGLD, and does not even work as well as “vanilla” SGD.2"
    }, {
      "heading" : "3.3 Toy 1d regression",
      "text" : "We start with a toy 1d regression problem, in order to visually illustrate the performance of different methods. We use the same data and model as [HLA15]. In particular, we use N = 20 points in\n2 We need to be somewhat careful with these conclusions, since they are mostly based on a single trial. (However, the distilled SGLD results on 400 units are the mean over 10 trials; the standard error was only 0.01.)\nD = 1 dimensions, sampled from the function y = x3 + n, where n ∼ N (0, 9). We fit this data with an MLP with 10 hidden units and ReLU activations. For SGLD, we use S = 2000 samples. For distillation, the teacher uses the same architecture as the student.\nThe results are shown in Figure 2. We see that SGLD is a better approximation to the “true” (HMC) posterior predictive density than the VB approximation3 and the plugin SGD approximation, and similar to the PBP/ EP predictions. Finally, we see that distilling SGLD incurs little loss in accuracy but saves a lot computationally."
    }, {
      "heading" : "3.4 Boston housing",
      "text" : "Finally, we consider a larger regression problem, namely the Boston housing dataset, which was also used in [HLA15]. This has N = 506 data points (456 training, 50 testing), with D = 13 dimensions. Since this data set is so small, we repeated all experiments 20 times, using different train/ test splits.\nFollowing [HLA15], we use an MLP with 1 layer of 50 hidden units and ReLU activations. First we use SGD, with these hyper parameters4: Minibatch size M = 1, noise precision λn = 1.25, prior precision λ = 1, number of trials 20, constant learning rate ηt = 1e− 6, number of iterations T = 170K. As shown in Table 5, we get an average log likelihood of −2.7639. Next we fit the model using SGLD. We use an initial learning rate of η0 = 1e− 5, which we reduce by a factor of 0.5 every 80K iterations; we use 500K iterations, a burnin of 10K, and a thinning interval of 10. As shown in Table 5, we get an average log likelihood of −2.306, which is better than SGD.\nFinally, we distil our SGLD model. The student architecture is the same as the teacher. We use the following teacher hyper parameters: prior precision λ = 2.5; initial learning rate of η0 = 1e − 5, which we reduce by a factor of 0.5 every 80K iterations. For the student, we use generated training data with Gaussian noise with standard deviation 0.05, we use a prior precision of γ = 0.001, an initial learning rate of ρ0 = 1e− 2, which we reduce by 0.8 after every 5e3 iterations. As shown in Table 5, we get an average log likelihood of −2.350, which is only slightly worse than SGLD, and much better than SGD. Furthermore, both SGLD and distilled SGLD are better than the PBP/ EP method of [HLA15] and the BBB/ VB method of [BCKW15].\n3 According to a personal communication with the authors of [HLA15] VB numbers are based on the method of [Gra11], but the method of [BCKW15] apparently gives similar results.\n4We choose all hyper-parameters using cross-validation whereas [HLA15] performs posterior inference on the noise and prior precisions, and uses Bayesian optimization to choose the remaining hyper-parameters."
    }, {
      "heading" : "4 Conclusions and future work",
      "text" : "We have shown a very simple method for “being Bayesian” about neural networks (and other kinds of models), that seems to work better than recently proposed alternatives based on EP [HLA15] and VB [BCKW15].\nWe have various things we’d like to do in the future: (1) Show the utility of our model in an end-toend task, where predictive uncertainty is useful (such as with contextual bandits or active learning). (2) Explore more sophisticated SG-MCMC methods, such as distributed SGLD and SG-NHT. (3) Consider ways to reduce the variance of the algorithm, perhaps by keeping a running minibatch of parameters uniformly sampled from the posterior, which can be done online using reservoir sampling. (4) Exploring more intelligent data generation methods for training the student. (5) Investigating if our method is able to reduce the prevalence of confident false predictions on adversarially generated examples, such as those discussed in [SZS+14]."
    }, {
      "heading" : "Acknowledgements",
      "text" : "We thank José Miguel Hernández-Lobato for his comments and for sharing source code for the PBP algorithm. We are also grateful to Jonathan Huang, George Papandreou and Sergio Guadaramma for their comments and Nick Johnston for help with the Torch library."
    } ],
    "references" : [ {
      "title" : "Bayesian Posterior Sampling via Stochastic Gradient Fisher Scoring",
      "author" : [ "S. Ahn", "A. Korattikara", "M. Welling" ],
      "venue" : "ICML",
      "citeRegEx" : "AKW12",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Distributed stochastic gradient MCMC",
      "author" : [ "Sungjin Ahn", "Babak Shahbaba", "Max Welling" ],
      "venue" : "ICML,",
      "citeRegEx" : "ASW14",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Weight uncertainty in neural networks",
      "author" : [ "C. Blundell", "J. Cornebise", "K. Kavukcuoglu", "D. Wierstra" ],
      "venue" : "ICML",
      "citeRegEx" : "BCKW15",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Model compression",
      "author" : [ "Cristian Bucila", "Rich Caruana", "Alexandru Niculescu-Mizil" ],
      "venue" : "KDD,",
      "citeRegEx" : "BCNM06",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Stochastic Gradient Hamiltonian Monte Carlo",
      "author" : [ "Tianqi Chen", "Emily B Fox", "Carlos Guestrin" ],
      "venue" : "ICML,",
      "citeRegEx" : "CFG14",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Bayesian sampling using stochastic gradient thermostats",
      "author" : [ "N Ding", "Y Fang", "R Babbush", "C Chen", "R Skeel", "H Neven" ],
      "venue" : "NIPS",
      "citeRegEx" : "DFB14",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Practical variational inference for neural networks",
      "author" : [ "Alex Graves" ],
      "venue" : "NIPS,",
      "citeRegEx" : "Gra11",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Probabilistic backpropagation for scalable learning of bayesian neural networks",
      "author" : [ "J. Hernández-Lobato", "R. Adams" ],
      "venue" : "ICML",
      "citeRegEx" : "HLA15",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Distilling the knowledge in a neural network",
      "author" : [ "Geoffrey Hinton", "Oriol Vinyals", "Jeff Dean" ],
      "venue" : "NIPS Deep Learning Workshop,",
      "citeRegEx" : "HVD14",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Stochastic gradient VB and the variational auto-encoder",
      "author" : [ "Diederik P Kingma", "Max Welling" ],
      "venue" : "ICLR,",
      "citeRegEx" : "KW14",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "In Handbook of Markov chain Monte Carlo",
      "author" : [ "Radford Neal. MCMC using hamiltonian dynamics" ],
      "venue" : "Chapman and Hall,",
      "citeRegEx" : "Nea11",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Stochastic gradient riemannian langevin dynamics on the probability simplex",
      "author" : [ "Sam Patterson", "Yee Whye Teh" ],
      "venue" : "NIPS,",
      "citeRegEx" : "PT13",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "FitNets: Hints for thin deep nets",
      "author" : [ "Adriana Romero", "Nicolas Ballas", "Samira Ebrahimi Kahou", "Antoine Chassang", "Carlo Gatta", "Yoshua Bengio" ],
      "venue" : "Arxiv, 19",
      "citeRegEx" : "RBK14",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Stochastic backpropagation and approximate inference in deep generative models",
      "author" : [ "D. Rezende", "S. Mohamed", "D. Wierstra" ],
      "venue" : "ICML",
      "citeRegEx" : "RMW14",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Compact approximations to bayesian predictive distributions",
      "author" : [ "Edward Snelson", "Zoubin Ghahramani" ],
      "venue" : "ICML,",
      "citeRegEx" : "SG05",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Intriguing properties of neural networks",
      "author" : [ "Christian Szegedy", "Wojciech Zaremba", "Ilya Sutskever", "Joan Bruna", "Dumitru Erhan", "Ian Goodfellow", "Rob Fergus" ],
      "venue" : "ICLR,",
      "citeRegEx" : "SZS14",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Bayesian learning via stochastic gradient Langevin dynamics",
      "author" : [ "Max Welling", "Yee W Teh" ],
      "venue" : "ICML,",
      "citeRegEx" : "WT11",
      "shortCiteRegEx" : null,
      "year" : 2011
    } ],
    "referenceMentions" : [ {
      "referenceID" : 7,
      "context" : "We compare to two very recent approaches to Bayesian neural networks, namely an approach based on expectation propagation [HLA15] and an approach based on variational Bayes [BCKW15].",
      "startOffset" : 122,
      "endOffset" : 129
    }, {
      "referenceID" : 2,
      "context" : "We compare to two very recent approaches to Bayesian neural networks, namely an approach based on expectation propagation [HLA15] and an approach based on variational Bayes [BCKW15].",
      "startOffset" : 173,
      "endOffset" : 181
    }, {
      "referenceID" : 7,
      "context" : "Recently, [HLA15] proposed a method called “probabilistic backpropagation” (PBP) based on an online version of expectation propagation (EP), (i.",
      "startOffset" : 10,
      "endOffset" : 17
    }, {
      "referenceID" : 2,
      "context" : "[BCKW15] proposed an approach called “Bayes by Backprop” (BBB) based on variational Bayes (VB), extending earlier work of [Gra11, KW14, RMW14].",
      "startOffset" : 0,
      "endOffset" : 8
    }, {
      "referenceID" : 16,
      "context" : "However, recently a method called stochastic gradient Langevin dynamics (SGLD) [WT11] has been devised that can draw samples from the posterior in an online fashion, just as SGD updates a point estimate of the parameters online.",
      "startOffset" : 79,
      "endOffset" : 85
    }, {
      "referenceID" : 4,
      "context" : "Furthermore, various extensions of SGLD have been proposed, including stochastic gradient hybrid Monte Carlo (SGHMC) [CFG14], stochastic gradient Nosé-Hoover Thermostat (SGNHT) [DFB14] (which improves upon SGHMC), stochastic gradient Fisher scoring (SGFS) [AKW12] (which uses second order information), stochastic gradient Riemannian Langevin Dynamics (for inferring parameters in the probability simplex) [PT13], distributed SGLD [ASW14], etc.",
      "startOffset" : 117,
      "endOffset" : 124
    }, {
      "referenceID" : 5,
      "context" : "Furthermore, various extensions of SGLD have been proposed, including stochastic gradient hybrid Monte Carlo (SGHMC) [CFG14], stochastic gradient Nosé-Hoover Thermostat (SGNHT) [DFB14] (which improves upon SGHMC), stochastic gradient Fisher scoring (SGFS) [AKW12] (which uses second order information), stochastic gradient Riemannian Langevin Dynamics (for inferring parameters in the probability simplex) [PT13], distributed SGLD [ASW14], etc.",
      "startOffset" : 177,
      "endOffset" : 184
    }, {
      "referenceID" : 0,
      "context" : "Furthermore, various extensions of SGLD have been proposed, including stochastic gradient hybrid Monte Carlo (SGHMC) [CFG14], stochastic gradient Nosé-Hoover Thermostat (SGNHT) [DFB14] (which improves upon SGHMC), stochastic gradient Fisher scoring (SGFS) [AKW12] (which uses second order information), stochastic gradient Riemannian Langevin Dynamics (for inferring parameters in the probability simplex) [PT13], distributed SGLD [ASW14], etc.",
      "startOffset" : 256,
      "endOffset" : 263
    }, {
      "referenceID" : 11,
      "context" : "Furthermore, various extensions of SGLD have been proposed, including stochastic gradient hybrid Monte Carlo (SGHMC) [CFG14], stochastic gradient Nosé-Hoover Thermostat (SGNHT) [DFB14] (which improves upon SGHMC), stochastic gradient Fisher scoring (SGFS) [AKW12] (which uses second order information), stochastic gradient Riemannian Langevin Dynamics (for inferring parameters in the probability simplex) [PT13], distributed SGLD [ASW14], etc.",
      "startOffset" : 406,
      "endOffset" : 412
    }, {
      "referenceID" : 1,
      "context" : "Furthermore, various extensions of SGLD have been proposed, including stochastic gradient hybrid Monte Carlo (SGHMC) [CFG14], stochastic gradient Nosé-Hoover Thermostat (SGNHT) [DFB14] (which improves upon SGHMC), stochastic gradient Fisher scoring (SGFS) [AKW12] (which uses second order information), stochastic gradient Riemannian Langevin Dynamics (for inferring parameters in the probability simplex) [PT13], distributed SGLD [ASW14], etc.",
      "startOffset" : 431,
      "endOffset" : 438
    }, {
      "referenceID" : 16,
      "context" : "However, in this paper, we will just use “vanilla” SGLD [WT11].",
      "startOffset" : 56,
      "endOffset" : 62
    }, {
      "referenceID" : 8,
      "context" : "Following [HVD14], we call q(y|x) the “teacher” and S(y|x,w) the “student”.",
      "startOffset" : 10,
      "endOffset" : 17
    }, {
      "referenceID" : 14,
      "context" : "In particular, [SG05] also trained a parametric student model to approximate a Monte Carlo teacher.",
      "startOffset" : 15,
      "endOffset" : 21
    }, {
      "referenceID" : 8,
      "context" : "[HVD14] also trained a student neural network to emulate the predictions of a (larger) teacher network, a process they call “distillation”.",
      "startOffset" : 0,
      "endOffset" : 7
    }, {
      "referenceID" : 3,
      "context" : "This extends earlier work of [BCNM06] which approximated an ensemble of classifiers by a single one.",
      "startOffset" : 29,
      "endOffset" : 37
    }, {
      "referenceID" : 12,
      "context" : "[RBK14] further extend this approach by training a student network that is deeper, but thinner (hence the term “fit nets”), than the teacher, and injecting supervision at multiple levels.",
      "startOffset" : 0,
      "endOffset" : 7
    }, {
      "referenceID" : 8,
      "context" : "[HVD14] coined the term “dark knowledge” to represent the information which is “hidden” inside the teacher network, and which can then be distilled into the student.",
      "startOffset" : 0,
      "endOffset" : 7
    }, {
      "referenceID" : 8,
      "context" : "This is true for two reasons: first, the teacher is trained to predict a single label per input, whereas the student is trained to predict a distribution, which contains more information (as argued in [HVD14]); second, the teacher makes multiple passes over the same training data, whereas the student sees “fresh” randomly generated data D′ at each step.",
      "startOffset" : 201,
      "endOffset" : 208
    }, {
      "referenceID" : 7,
      "context" : "In this section, we compare SGLD and distilled SGLD with other approximate inference methods, including the plugin approximation using SGD, the EP approach of [HLA15], the VB approach of [BCKW15], and Hamiltonian Monte Carlo (HMC) [Nea11], which is considered the “gold standard” for MCMC for neural nets.",
      "startOffset" : 159,
      "endOffset" : 166
    }, {
      "referenceID" : 2,
      "context" : "In this section, we compare SGLD and distilled SGLD with other approximate inference methods, including the plugin approximation using SGD, the EP approach of [HLA15], the VB approach of [BCKW15], and Hamiltonian Monte Carlo (HMC) [Nea11], which is considered the “gold standard” for MCMC for neural nets.",
      "startOffset" : 187,
      "endOffset" : 195
    }, {
      "referenceID" : 10,
      "context" : "In this section, we compare SGLD and distilled SGLD with other approximate inference methods, including the plugin approximation using SGD, the EP approach of [HLA15], the VB approach of [BCKW15], and Hamiltonian Monte Carlo (HMC) [Nea11], which is considered the “gold standard” for MCMC for neural nets.",
      "startOffset" : 231,
      "endOffset" : 238
    }, {
      "referenceID" : 2,
      "context" : "Second, we did not get access to the code for the VB approach in time for us to compare to it, so we just quote performance numbers from their paper [BCKW15].",
      "startOffset" : 149,
      "endOffset" : 157
    }, {
      "referenceID" : 2,
      "context" : "The only preprocessing we do is divide the pixel values by 126 (as in [BCKW15]).",
      "startOffset" : 70,
      "endOffset" : 78
    }, {
      "referenceID" : 2,
      "context" : "Following [BCKW15], we use an MLP with 2 hidden layers with 400 hidden units per layer, ReLU activations, and softmax outputs; we denote this by 784-400-400-10.",
      "startOffset" : 10,
      "endOffset" : 18
    }, {
      "referenceID" : 2,
      "context" : "6%, which is a bit lower than the SGD number reported in [BCKW15].",
      "startOffset" : 57,
      "endOffset" : 65
    }, {
      "referenceID" : 2,
      "context" : "4%, which is better than the SGD, dropout and BBB results from [BCKW15].",
      "startOffset" : 63,
      "endOffset" : 71
    }, {
      "referenceID" : 7,
      "context" : "We use the same data and model as [HLA15].",
      "startOffset" : 34,
      "endOffset" : 41
    }, {
      "referenceID" : 2,
      "context" : "SGD [BCKW15] Dropout BBB SGD (our impl.",
      "startOffset" : 4,
      "endOffset" : 12
    }, {
      "referenceID" : 2,
      "context" : "SGD (first column), Dropout and BBB/ VB numbers are quoted from [BCKW15].",
      "startOffset" : 64,
      "endOffset" : 72
    }, {
      "referenceID" : 7,
      "context" : "Finally, we consider a larger regression problem, namely the Boston housing dataset, which was also used in [HLA15].",
      "startOffset" : 108,
      "endOffset" : 115
    }, {
      "referenceID" : 7,
      "context" : "Following [HLA15], we use an MLP with 1 layer of 50 hidden units and ReLU activations.",
      "startOffset" : 10,
      "endOffset" : 17
    }, {
      "referenceID" : 7,
      "context" : "Furthermore, both SGLD and distilled SGLD are better than the PBP/ EP method of [HLA15] and the BBB/ VB method of [BCKW15].",
      "startOffset" : 80,
      "endOffset" : 87
    }, {
      "referenceID" : 2,
      "context" : "Furthermore, both SGLD and distilled SGLD are better than the PBP/ EP method of [HLA15] and the BBB/ VB method of [BCKW15].",
      "startOffset" : 114,
      "endOffset" : 122
    }, {
      "referenceID" : 7,
      "context" : "3 According to a personal communication with the authors of [HLA15] VB numbers are based on the method of [Gra11], but the method of [BCKW15] apparently gives similar results.",
      "startOffset" : 60,
      "endOffset" : 67
    }, {
      "referenceID" : 6,
      "context" : "3 According to a personal communication with the authors of [HLA15] VB numbers are based on the method of [Gra11], but the method of [BCKW15] apparently gives similar results.",
      "startOffset" : 106,
      "endOffset" : 113
    }, {
      "referenceID" : 2,
      "context" : "3 According to a personal communication with the authors of [HLA15] VB numbers are based on the method of [Gra11], but the method of [BCKW15] apparently gives similar results.",
      "startOffset" : 133,
      "endOffset" : 141
    }, {
      "referenceID" : 7,
      "context" : "We choose all hyper-parameters using cross-validation whereas [HLA15] performs posterior inference on the noise and prior precisions, and uses Bayesian optimization to choose the remaining hyper-parameters.",
      "startOffset" : 62,
      "endOffset" : 69
    }, {
      "referenceID" : 7,
      "context" : "test log likelihood EP/ PBP (as reported in [HLA15]) -2.",
      "startOffset" : 44,
      "endOffset" : 51
    }, {
      "referenceID" : 7,
      "context" : "089 VB (as reported in [HLA15]) -2.",
      "startOffset" : 23,
      "endOffset" : 30
    }, {
      "referenceID" : 7,
      "context" : "(Figures a-d kindly provided by the authors of [HLA15].",
      "startOffset" : 47,
      "endOffset" : 54
    }, {
      "referenceID" : 7,
      "context" : "We have shown a very simple method for “being Bayesian” about neural networks (and other kinds of models), that seems to work better than recently proposed alternatives based on EP [HLA15] and VB [BCKW15].",
      "startOffset" : 181,
      "endOffset" : 188
    }, {
      "referenceID" : 2,
      "context" : "We have shown a very simple method for “being Bayesian” about neural networks (and other kinds of models), that seems to work better than recently proposed alternatives based on EP [HLA15] and VB [BCKW15].",
      "startOffset" : 196,
      "endOffset" : 204
    }, {
      "referenceID" : 15,
      "context" : "(5) Investigating if our method is able to reduce the prevalence of confident false predictions on adversarially generated examples, such as those discussed in [SZS14].",
      "startOffset" : 160,
      "endOffset" : 167
    } ],
    "year" : 2015,
    "abstractText" : "We consider the problem of Bayesian parameter estimation for deep neural networks, which is important in problem settings where we may have little data, and/ or where we need accurate posterior predictive densities p(y|x,D), e.g., for applications involving bandits or active learning. One simple approach to this is to use online Monte Carlo methods, such as SGLD (stochastic gradient Langevin dynamics). Unfortunately, such a method needs to store many copies of the parameters (which wastes memory), and needs to make predictions using many versions of the model (which wastes time). We describe a method for “distilling” a Monte Carlo approximation to the posterior predictive density into a more compact form, namely a single deep neural network. We compare to two very recent approaches to Bayesian neural networks, namely an approach based on expectation propagation [HLA15] and an approach based on variational Bayes [BCKW15]. Our method performs better than both of these, is much simpler to implement, and uses less computation at test time.",
    "creator" : "LaTeX with hyperref package"
  }
}