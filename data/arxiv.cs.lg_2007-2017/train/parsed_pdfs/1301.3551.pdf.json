{
  "name" : "1301.3551.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Information Theoretic Learning with Infinitely Divisible Kernels",
    "authors" : [ "Luis G. Sanchez Giraldo", "Jose C. Principe" ],
    "emails" : [ "sanchez@cnel.ufl.edu", "principe@cnel.ufl.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n30 1.\n35 51\nv6 [\ncs .L\nG ]\n4 J\nun 2\nIn this paper, we develop a framework for information theoretic learning based on infinitely divisible matrices. We formulate an entropy-like functional on positive definite matrices based on Renyi’s axiomatic definition of entropy and examine some key properties of this functional that lead to the concept of infinite divisibility. The proposed formulation avoids the plug in estimation of density and brings along the representation power of reproducing kernel Hilbert spaces. As an application example, we derive a supervised metric learning algorithm using a matrix based analogue to conditional entropy achieving results comparable with the state of the art."
    }, {
      "heading" : "1 Introduction",
      "text" : "Information theoretic quantities are descriptors of the distributions of the data that go beyond secondorder statistics. The expressive richness of quantities such entropy or mutual information has been shown to be very useful for machine learning problems where optimality based on linear and Gaussian assumptions no longer holds. Nevertheless, operational quantities in information theory are based on the probability laws underlying the data generation process, which are rarely known in the statistical learning setting where the only information available comes from the sample {zi}ni=1. Therefore, the use of information theoretic quantities as descriptors of data requires the development of suitable estimators. In [1], the use of Renyi’s definition of entropy along with Parzen density estimation is proposed as the main tool for information theoretic learning (ITL). The optimality criteria is expressed in terms of quantities such as Renyi’s entropy, divergences based on the Cauchy-Schwarz inequality, quadratic mutual information, among others. Part of the research effort in this context has pointed out connections to reproducing kernel Hilbert spaces [2]. Here, we show that these connections are not only valuable from a theoretical point of view, but they can also be exploited to derive novel information theoretic quantities with suitable estimators from data.\nPositive definite kernels have been employed in machine learning as a representational tool allowing algorithms that are based on inner products to be expressed in a rather generic way (the so called “kernel trick”). Algorithms that exploit this property are commonly known as kernel methods. Let X be a nonempty set. A function κ : X ×X 7→ R is called a positive definite kernel if for any finite set {xi}Ni=1 ⊆ X and any set of coefficients {αi}Ni=1 ⊂R, it follows that ∑i, j αiα jκ(xi,x j)≥ 0, if at least one i, αi 6= 0. In this case, there exist an implicit mapping φ : X 7→ H that maps any element x ∈ X to an element φ(x) in a Hilbert space H , such that κ(x,y) = 〈φ(x),φ(y)〉. The above map provides an implicit representation of the objects of interest that belong to the set X . The generality of this representation has been exploited in many practical applications, even for data that do not come in standard vector representation Rd [3]. This is possible as long as a kernel function is available.\nMore recently, it has been noticed that kernel induced maps are also useful beyond the above kernel trick in a rather interesting fashion. Namely, kernels can be utilized to compute higher-order statistics of the data in a nonparametric setting. Some examples exploring this idea are: kernel independent component analysis [4], the work on measures of dependence and independence using Hilbert-Schmidt norms [5], and the quadratic measures of independence proposed in [6]. It is not surprising, yet important to mention, that a similar observation have also been reached from the work on ITL since one of the original motivations in using information theoretic quantities is to go beyond second order statistics. The work we introduce in this paper goes along these lines. The twist is that rather than defining an estimator of a conventional information theoretic quantity such as Shannon entropy, we propose a quantity build from the data that satisfies similar axiomatic properties to those of well establish definitions such as Renyi’s definition of entropy\nThe main contribution of this work is to show that the Gram matrix obtained from evaluating a positive definite kernel on samples can be used to define a quantity based on the data with properties similar to those of an entropy without assuming that the probability density is being estimated. Therefore, we look at the axiomatic treatment of entropy and adapt it to the Gram matrices describing the data. In this sense, we think about entropy as a measure inversely related to the amount of statistical regularities (structure) directly from the data that can be applied as the optimality criterion in a learning algorithm. As an application example, we derive supervised metric learning algorithm that uses conditional entropy as the cost function. This is the second contribution of this paper, and the empirical results show that the proposed method is competitive with current approaches. The main body of the paper is organized in two parts. First, we introduce the proposed matrix-based entropy measure using the spectral theorem along with a set of axiomatic properties that our quantity must satisfy. Then, the notion of joint entropy is developed based on Hadamard products. We look at some basic inequalities of information and how they translate to the setting of positive definite matrices, which finally allow us to define an analogue to conditional entropies. In the development of these ideas, we find that the concept of infinitely divisible kernels arises and become key to our purposes. We revisit some of the theory on infinitely divisible matrices, to show how it links to the the proposed information theoretic framework. In the last part, we introduce an information theoretic supervised metric learning algorithm. We show how the proposed analogue to conditional entropy is a suitable cost function leading naturally to a gradient descent procedure. Finally, we provide some conclusions and future directions."
    }, {
      "heading" : "2 Positive Definite Matrices, and Renyi’s Entropy Axioms",
      "text" : "Let us start with an informal observation that motivated our matrix based entropy. In [1], the use of Renyi’s entropy is proposed as an alternative to the more commonly adopted definition of entropy given by Shannon. In particular, it was found that Renyi’s second-order entropy provides an amenable quantity for practical purposes. An empirical plug in estimator of Renyi’s second-order entropy based on the Parzen density estimator f̂ (x) = 1n ∑ n i=1 κ(xi,x), can be obtained as follows:\n− log 1 n2\nn\n∑ i, j=1 h(xi,x j), (1)\nwhere h(x,y) = ∫ X κσ (x,z)κσ (y,z)dz. Note that since h is a positive definite kernel, there exists a mapping φ to a RKHS such that h(x,y) = 〈φ(x),φ(y)〉; and the argument of the log in (1), called the information potential, can be interpreted in this space as a norm:\n〈\n1 n\nn\n∑ i=1 φ(xi), 1 n\nn\n∑ i=1 φ(xi)\n〉\n=\n∥ ∥ ∥ ∥ ∥ 1 n n ∑ i=1 φ(xi) ∥ ∥ ∥ ∥ ∥ 2 , (2)\nwith the limiting case given by ‖E[φ(X)]‖2. Thus, we can think of this estimator as an statistic computed on the representation space provided by the positive definite kernel h. Now, let us look at the case where κσ is the Gaussian kernel; if we construct the Gram matrix K with elements Ki j = κ2σ (xi,x j), it is easy to verify that the estimator of Renyi’s second-order entropy based on (1) corresponds to:\nĤ2(X) =− log ( 1 n2 tr(KK) ) +C(σ). (3)\nwhere C(σ) takes care of the normalization factor of the Parzen window. As we can see, the information potential estimator can be related to the norm of the Gram matrix K defined as ‖K‖2 = tr(KK). From the above informal argument two important questions arise. First, it seems natural to ask whether other functionals on Gram matrices allow information theoretic interpretations that can be further utilized as objective functions in ITL. Secondly, even though h was originally derived from a convolution of Parzen windows, was there anything about the implicit representation that allows to interpret (2) in information theoretic terms?"
    }, {
      "heading" : "2.1 Renyi’s Axioms for Gram matrices",
      "text" : "Real Hermitian matrices are considered generalizations of real numbers. It is possible to define a partial ordering on this set by using positive definite matrices, which are a generalization of the positive real numbers. Let Mn be the set of all n× n real matrices; for two Hermitian matrices A,B ∈ Mn, we say A < B if A−B is positive definite. Likewise, A ≻ B means that A−B is strictly positive definite.\nThe following spectral decomposition theorem [7] relates to the functional calculus on matrices and provides a reasonable way to extend continuous scalar-valued functions to Hermitian matrices.\nTheorem 2.1 Let D ⊂ C be a given set and let Nn(D) := {A ∈ Mn : A is normal and σ(A) ⊂ D}, where σ(A) denotes the spectrum of A. If f (t) is a continuous scalar-valued function on D, then the primary matrix function\nf (A) =U\n\n\n f (λ1) · · · 0 ... . . . ...\n0 · · · f (λn)\n\n  U∗ (4)\nis continuous on Nn(D), where A =UΛU∗, Λ = diag(λ1, . . . ,λn), and U ∈ Mn is unitary.\nEquipped with the above result, we can define matrix functions such as f (A) = Ar for r ∈R+, which will be used in defining the following matrix-based analogue to Renyi’s α-entropy. The functional will then be applied to Gram matrices constructed by pairwise evaluation of a positive definite kernel on the data samples.\nConsider the set ∆+n of positive definite matrices A ∈ Mn for which tr(A)≤ 1. It is clear that this set is closed under finite convex combinations.\nProposition 2.1 Let A ∈ ∆+n and B ∈ ∆+n and also tr(A) = tr(B) = 1. The functional\nSα(A) = 1\n1−α log2 [tr(A α)], (5)\nsatisfies the following set of conditions:\n(i) Sα(PAP∗) = Sα(A) for any orthonormal matrix P ∈ Mn (ii) Sα(pA) is a continuous function for 0 < p ≤ 1.\n(iii) Sα( 1n I) = log2 n.\n(iv) Sα(A⊗B) = Sα(A)+ Sα(B).\n(v) If AB = BA = 0; then for the strictly monotonic and continuous function g(x) = 2(α−1)x for α 6= 1 and α > 0, we have that:\nSα(tA+(1− t)B) = g−1 (tg(Sα(A))+ (1− t)g(Sα(B))) . (6)\nProof 2.1 The proof of (i) easily follows from Theorem 2.1. Take A = UΛU∗ now PU is also a unitary matrix and thus f (A) = f (PAP∗) the trace functional is invariant under unitary transformations. For (ii), the proof reduces to the continuity of 11−α log2(p) α . For (iii), a simple calculation yields trAα = ( 1\nn\n)α−1 . Now, for property (iv), notice that if trA = trB = 1, then, tr(A⊗B) = 1.\nSince A = UΛU∗ and B = VΓV ∗ we can write A ⊗ B = (U ⊗V )(Λ ⊗ Γ)(U ⊗V )∗, from which\ntr(A⊗B)α = tr(Λ⊗Γ)α = tr(Λα)tr(Γα) and thus (iv) is proved. Finally, (v) notice that for any integer power k of tA+(1− t)B we have: (tA+(1− t)B)k = (tA)k +((1− t)B)k since AB = BA = 0. Under extra conditions such as f (0) = 0 the argument in the proof of Theorem 2.1 can be extended to this case. Since the eigen-spaces for the non-null eigenvalues of A and B are orthogonal we can simultaneously diagonalize A and B with the orthonormal matrix U, that is A = UΛU∗ and B = UΓU∗ where Λ and Γ are diagonal matrices containing the eigenvalues of A and B respectively. Since AB = BA = 0, then ΛΓ = 0. Under the extra condition f (0) = 0, we have that f (tA+(1− t)B) = f (tA)+ f ((1− t)B) yielding the desired result for (v).\nNotice also that if the rank of A, ρ(A) = 1, the entropy Sα(A) = 0 for any α 6= 0. It is also true that,\nSα(A)≤ Sα( 1 n I) = log2 n. (7)\nAs we can see (5) satisfies some properties attributed to entropy. Nevertheless, such a characterization may not fully endow all unit-trace positive definite matrices with an information theoretic interpretation. Which descriptors are suitable in representing joint-spaces? What properties should be satisfied by the matrices in order to be applied to concepts that link them to random variables such as conditioning? In what follows, we address these points by developing notions of joint entropy and conditional entropy, for which, additional properties must be fulfilled. Recall that the notions of joint and conditional entropy are not only important for the above reasons, but they also provide the means to propose objective functions for learning that are based on information theoretic quantities."
    }, {
      "heading" : "2.2 Hadamard Products and the Notion of Joint Entropy",
      "text" : "Positive kernels are also useful in integrating multiple modalities. Using the the product kernel, we can readily define the notion of joint-entropy. Consider a sequence of sample pairs {(xi,yi)}Ni=1 where xi ∈ X and yi ∈ Y . Assume, we have a positive definite kernels κ1 defined on X ×X and κ2 defined on X ×X . The product kernel κ((xi,yi),(x j ,y j)) = κ1(xi,x j)κ(yi,y j) is a positive definite kernel on (X ×Y )× (X ×Y ). As we can see the Hadamard product arises as a joint representation in a our matrix based entropy. Consider two matrices A and B in ∆n with unit trace, for which there exists some relation between the elements Ai j and Bi j for all i and j. The joint entropy can be defined as:\nSα\n(\nA◦B tr(A◦B)\n)\n(8)\nIt is important then to verify that the definition of joint entropy (8) satisfies a basic intuition about uncertainty. The joint entropy should never be smaller than any of the individual entropies of the variables that conform it. The following proposition verifies this intuition for a subset of the unit trace, positive definite matrices.\nProposition 2.2 Let A and B be two n× n positive definite matrices with trace 1 with nonnegative entries, and Aii = 1n for i = 1,2, . . . ,n. Then, the following inequality holds:\nSα\n(\nA◦B tr(A◦B)\n)\n≥ Sα(B), (9)"
    }, {
      "heading" : "2.3 Conditional Entropy as a Difference Between Entropies",
      "text" : "The conditional entropy of X given Y , which can be understood as the uncertainty about X that remains after knowing the joint distribution of X and Y , can be obtained from a difference between two entropies. In the Shannon’s definition of conditional entropy, H(X |Y ) can be expressed as H(X |Y ) = H(X ,Y )−H(Y ). The properties of this definition has been recently studied in the case of Renyi’s entropies [8] and in the matrix case, this definition yields:\nSα(A|B) = Sα ( A◦B tr(A◦B) ) − Sα(B), (10)\nfor positive semidefinite matrices A and B with nonnegative entries and unit trace, such that Aii = 1n for all i = 1, . . . ,n. The above quantity is nonnegative and upper bounded by Sα(A). Certainly,\nnormalization is an important property of the matrices involved in the above results. If A and B are normalized to have unit trace, then for r ∈ [0,1] it is true that the Hadamard product of\nA◦r ◦B◦(1−r), (11) is also normalized. However, it is not always true that the resulting matrix (11) is positive definite. This product can be thought as a weighted geometric average for which the resulting matrix will give more emphasis to either one of the matrices. However, if A and B satisfy a property called infinitely divisibility, the product is guaranteed to be positive definite 1."
    }, {
      "heading" : "3 Infinitely Divisible Functions",
      "text" : "The theory of infinitely divisible developed below is not new, but it is included because it provides a basic understanding about the role of infinitely divisible kernels in computing the above information theoretic quantities from data. To avoid confusion, let us describe the key points to bear in mind before we move to the mathematical description. Infinitely divisible kernels and negative definite functions are tied together trough the exponential a logarithm functions. Both functions provide Hilbert space representations of the data. We can think of the RKHS of the infinitely divisible kernel as a representation to compute the higher order descriptors of the data. On the other hand, the Hilbertian metric can be the representation space for which we want to compute the high order statistics. Normalization, as we show below is not only important in satisfying the conditions for the information theoretic quantities already defined, but it also shows that many possible representational choices are equivalent."
    }, {
      "heading" : "3.1 Negative Definite Functions and Hilbertian Metrics",
      "text" : "Let M = (X ,d) be a separable metric space. A necessary and sufficient condition for M to be embeddable in a Hilbert space H is that for any set {xi} ⊂ X of n + 1 points, ∑ni, j=1 αiα j ( d2(x0,xi)+ d2(x0,x j)− d2(xi,x j) )\n≥ 0, for any α ∈ Rn. This condition is equivalent to ∑ni, j=0 αiα jd2(xi,x j) ≤ 0, for any α ∈ Rn+1, such that ∑ni=0 αi = 0. This condition is known as negative definiteness. Interestingly, the above condition implies that exp(−rd2(xi,x j)) is positive definite in X for all r > 0 [9]. Indeed, matrices derived from functions satisfying the above property conform a special class of matrices know as infinitely divisible."
    }, {
      "heading" : "3.2 Infinitely Divisible Matrices",
      "text" : "According to the Schur product theorem A < 0 implies A◦n = A ◦A ◦ · · · ◦A < 0 for any positive integer n. Does the above hold if we to take fractional powers of A? In other words,is the matrix A◦ 1 m < 0 for any positive integer m? This question leads to the concept of infinitely divisible matrices [10, 11]. A nonnegative matrix A is said to be infinitely divisible if A◦r < 0 for every nonnegative r. Infinitely divisible matrices are intimately related to negative definiteness as we can see from the following proposition\nProposition 3.1 If A is infinitely divisible, then the matrix Bi j =− logAi j is negative definite\nFrom this fact it is possible to relate infinitely divisible matrices with isometric embeddings into Hilbert spaces. If we construct the matrix\nDi j = Bi j − 1 2 (Bii +B j j), (12)\nusing the matrix B from proposition 3.1. There exists a Hilbert space H and a mapping φ such that\nDi j = ‖φ(i)−φ( j)‖2H . (13) Moreover, notice that if A is positive definite−A is negative definite and expAi j is infinitely divisible. In a similar way, we can construct a matrix,\nDi j =−Ai j + 1 2 (Aii +A j j), (14)\n1By this, we also mean positive semidefinite\nwith the same property (13). This relation between (12) and (14) suggests a normalization of infinitely divisible matrices with non-zero diagonal elements that can be formalized in the following theorem.\nTheorem 3.1 Let X be a nonempty set, and let d1 and d2 be two metrics on it, such that for any set {xi}ni=1, n ∑\ni, j=1 αiα jd2ℓ (xi,x j)≤ 0, for any α ∈ Rn, and ∑ni=1 αi = 0, is true for ℓ = 1,2. Consider the\nmatrices A(ℓ)i j = exp−d2ℓ (xi,x j) and their normalizations Â(ℓ), defined as:\nÂ(ℓ)i j = A(ℓ)i j √\nA(ℓ)ii\n√\nA(ℓ)j j\n. (15)\nThen, if Â(1) = Â(2) for any finite set {xi}ni=1 ⊆ X , there exist isometrically isomorphic Hilbert spaces H1 and H2, that contain the Hilbert space embeddings of the metric spaces (X ,dℓ), ℓ= 1,2. Moreover, Â(ℓ) are infinitely divisible.\nFigure 1 summarizes the relation between spaces that are considered in the proposed framework. The object space X can be directly mapped into Hκ using an infinitely divisible kernel κ , or it can be mapped to a Hilbert space Hd , if a negative definite function d, is employed as the distance function. The spaces Hκ and Hd are related by the log and exp functions."
    }, {
      "heading" : "4 Application to Metric Learning",
      "text" : ""
    }, {
      "heading" : "4.1 Adaptation Using the Matrix-Based Entropy",
      "text" : "By definition, the matrix entropy functional (5) fall into the family of matrix functions know as spectral functions. These functions only depend on the eigenvalues of matrix and therefore their name [12]. Using theorem (1.1) from [13] it is straightforward to obtain the derivative of (5) at A as\n∇Sα(A) = α\n(1−α)tr(Aα)UΛ α−1U∗, (16)\nwhere A = UΛU∗. It is important to note that this decomposition can be used to our advantage. Instead of computing the full set of eigenvectors and eigenvalues of A, we can approximate the gradient of Sα by using only a few leading eigenvalues. It is easy to see that this approximation will be optimal in the Frobenius norm ‖X‖Fro = √ tr(X∗X)."
    }, {
      "heading" : "4.2 Metric Learning Using Conditional Entropy",
      "text" : "Here, we apply the proposed matrix framework to the problem of supervised metric learning. This problem can be formulated as follows. Given a set of points {(xi, li)}ni=1, we seek a positive\nsemidefinite matrix AAT, that parametrizes a Mahalanobis distance between samples x,x′ ∈ Rd as d(x,x′) = (x− x′)TAAT(x− x′). Our goal is to find parametrization matrix A such that the conditional entropy of the labels li given the projected samples yi = ATxi with yi ∈ Rp and p ≪ d, is minimized. This can be posed as the following optimization problem:\nminimize A∈Rd×p\nSα(L|Y )\nsubject to ATxi = yi, for i = 1, . . . ,n; tr(ATA) = p,\n(17)\nwhere the trace constraint prevents the solution from growing unbounded. We can translate this problem to our matrix-based framework in the following way. Let K be the matrix representing the projected samples\nKi j = 1 n exp\n(\n− (xi − x j) TAAT(xi − x j) 2σ2\n)\n,\nand L be the matrix of class co-occurrences where Li j = 1n if li = l j and zero otherwise. The conditional entropy can be computed as Sα(L|Y ) = Sα (nK◦L)−Sα(K), and its gradient at A, which can be derived based on (24), is given by:\nXT(P− diag(P1))XA (18)\nwhere P = (nL◦∇Sα (nK◦L)−∇Sα(K))◦K (19)\nFinally, we can use (18) to search for A iteratively. UCI Data: To evaluate the results we use the same experimental setup proposed in [14], we compares 5 different approaches to supervised metric learning based on the classification error obtained from two-fold cross-validation using a 4-nearest neighbor classifier. The reported errors are averages errors from 10 runs on the two folds for each algorithm; in our case the parameters are p = 3, α = 1.01 and σ = √ 3. The feature vectors were centered and scaled to have unit variance. Figure 2(a) shows the results of the proposed approach conditional entropy metric learning (CEML), information theoretic metric learning (ITML) proposed in [14], neighborhood component analysis (NCA) from [15], the maximally collapsing metric learning (MCML) method from [16], the large margin nearest neighbor (LMNN) method found in [17], and, as a baseline, the the inverse covariance and Euclidean distances. The results for the Soybean dataset are not reported since there is more than one possible data set in the UCI repository under that name. The errors obtained by the metric learning algorithm using the proposed matrix-based entropy framework are consistently among the best performing methods included in the comparison. Choice of order α: Even though the choice of the entropy order above appears to be arbitrary, there is a motivation in choosing α close to 1. The reason is that the higher the entropy order, the more prone the algorithm is to find unimodal solutions. This can be advantageous if prior knowledge or strong assumptions on the class distributions are taken into consideration. In our experiments, we opted for lower entropy order and give the algorithm more flexibility in finding a good solution. To experimentally show this phenomena, we generated a two-dimensional dataset containing points from two classes. In one direction the classes are very well separated but the distribution has multiple modalities. On the orthogonal direction, the classes are not fully separable, but their distributions are unimodal. Figure 3 shows a sample with points drawn from both classes, as we can see projecting the data onto the horizontal axis provides better separability at the cost of a more complex decision boundary. We run our metric learning algorithm 60 times for different values of α and recorded the direction of the resulting one-dimensional feature extractor. Table 1 shows the number of times a particular direction was picked by our algorithm for different entropy orders. It can be seen that for larger values of α , the algorithm selected the vertical direction more often.\nUMist Faces: We also run the algorithm on the UMist dataset; This data set consists of Grayscale faces (8 bit [0-255]) of 20 different people. The total number of images is 575 and the size of each image is 112x92 pixels for a total of 10304 dimensions. Pixel values were normalized by dividing by 255 and removing the mean. Figure 2(b) shows the images projected into R2. It is remarkable\nhow a linear projection can separate the faces, and it can also be seen from the Gram matrix that it tries to approximate the co-occurrence matrix L."
    }, {
      "heading" : "5 Conclusions",
      "text" : "In this paper, we presented a data-driven framework for information theoretic learning based on infinitely divisible matrices. We define estimators of entropy-like quantities that can be computed from the Gram matrices obtained by evaluating infinitely divisible kernels on pairs of samples. The proposed quantities do not assume that the density of the data has been estimated, this can be advantageous in many scenarios where even defining a density is not feasible. We discuss some key properties of the proposed quantities and show how they can be applied to define useful analogues to quantities such as conditional entropy. Based on the proposed framework, we introduce a supervised metric learning algorithm with results that are competitive with the state of the art. Nevertheless, we believe that many interesting formulations to learning problems based on the proposed framework are yet to be found. It is also important to highlight that the connection between the RKHS provided by the infinitely divisible kernel, and the Hilbertian metrics associated with the negative definite functions, opens an interesting avenue to investigate formulations of information theoretic learning algorithms on both spaces, and the implications of choosing one or the other."
    }, {
      "heading" : "A Additional results and proofs",
      "text" : "To prove (9), we need to introduce the concept of majorization and some results pertaining the ordering that arises from this definition. The proposition is replicated in this appendix for the sake of self containment.\nDefinition A.1 (Majorization): Let p and q be two nonnegative vectors in Rn such that ∑ni=1 pi = ∑ni=1 qi <∞. We say p4 q, q majorizes p, if their respective ordered sequences p[1]≥ p[2]≥ ·· · ≥ p[n] and q[1] ≥ q[2] ≥ ·· · ≥ q[n] denoted by {p[i]}ni=1 and {p[i]}ni=1, satisfy:\nk\n∑ i=1\np[i] ≤ k\n∑ i=1 q[i] for k = 1, . . . ,n (20)\nIt can be shown that if p 4 q then p = Aq for some doubly stochastic matrix A [18]. It is also easy to verify that if p 4 q and p 4 h then p 4 tq+(1− t)h for t ∈ [0,1]. The majorization order is important because it can be associated with the definition of Schur-concave (convex) functions. A real valued function f on Rn is called Schur-convex if p 4 q implies f (p)≤ f (q) and Schur-concave if f (q) ≤ f (p).\nLemma A.1 The function fα : S n 7→ R+ (S n denotes the n dimensional simplex), defined as,\nfα(p) = 1 1−α log2 n ∑ i=1 pαi , (21)\nis Schur-concave for α > 0.\nNotice that, Schur-concavity (Schur-convexity) cannot be confused with concavity (convexity) of a function in the usual sense. Now, we are ready to state the inequality for Hadamard products.\nProposition A.1 Let A and B be two n× n positive definite matrices with trace 1 with nonnegative entries, and Aii = 1n for i = 1,2, . . . ,n. Then, the following inequality holds:\nSα\n(\nA◦B tr(A◦B)\n)\n≥ Sα(B), (22)\nProof A.1 In proving (9), we will use the fact that Sα preserves the majorization order (inversely) of nonnegative sequences on the n-dimensional simplex. First look at the identity\nxT(A◦B)x = tr(ADxBDx) = 1 n\nIn particular, if {xi}ni=1 is an orthonormal basis for Rn, tr(A◦B) = n ∑\ni=1 xTi (A◦B)xi. If we let {xi}ni=1\nbe the eigenvectors of A ◦B ordered according to their respective eigenvalues in decreasing order, then,\nk\n∑ i=1\nxTi (A◦B)xi = k\n∑ i=1 tr(ADxiBDxi) ≤ 1 n\nk\n∑ i=1 tr ( 11TDxi BDxi )\n= 1 n\nk\n∑ i=1 xTi Bxi ≤ 1 n\nk\n∑ i=1 yTi Byi, (23)\nwhere k = 1, . . . ,n and {yi}ni=1 are the eigenvectors of B ordered according to their respective eigenvalues in decreasing order. The inequality (23) is equivalent to say that nλ (A◦B)4 λ (B), that is, the sequence of eigenvalues of (A ◦B)/tr(A◦B) is majorized by the sequence of eigenvalues of B, which implies (9) by Lemma A.1.\nA beautiful observation from Theorem 3.1 is that, according to equation (10), the proposed normalization procedure for infinitely divisible matrices can be thought of as finding the maximum entropy matrix among all matrices for which the Hilbert space embeddings are isometrically isomorphic.\nA.1 Derivatives of Spectral Functions\nLet Hn denote the vector space of real Hermitian matrices of size n× n endowed with inner product 〈X,Y〉= trXY; and let Un denote the set of n×n unitary matrices. A real valued function f defined on a subset of Hn is unitarily invariant if f (UXU∗) = f (X) for any U ∈ Un. Associated with each spectral function f there is a symmetric function F on Rn. By symmetric we mean that F(x) = F(Px) for any n×n permutation matrix P. Let λ (X) denote the vector of ordered eigenvalues of X; then, a spectral function f (X) is of the form F(λ (X)) for F a symmetric. We are interested in the differentiation of the composition (F ◦λ )(·) = F(λ (·)) at X2. The following result [13] allows us to differentiate a spectral function f at X\nTheorem A.1 Let the set Ω ⊂ Rn be open and symmetric, that is, for any x ∈ Ω and any n× n permutation matrix P, Px ∈ Ω. Suppose that F is symmetric, Then, the spectral function F(λ (·)) is differentiable at a matrix X if and only if F is differentiable at the vector λ (X). In this case, the gradient of F ◦λ at X is ∇(F ◦λ )(X) = Udiag(∇F(λ (X)))U∗, (24) for any unitary matrix satisfying X = Udiag(λ (X))U∗.\n2In here, ◦ denotes composition rather than Hadamard product"
    } ],
    "references" : [ {
      "title" : "Information Theoretic Learning: Renyi’s Entropy and Kernel Perspectives, ser",
      "author" : [ "J.C. Principe" ],
      "venue" : "Series in Information Science and Statistics,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2010
    }, {
      "title" : "A reproducing kernel hilbert space framework for information theoretic learning",
      "author" : [ "J.-W. Xu", "A.R.C. Paiva", "I. Park", "J.C. Principe" ],
      "venue" : "IEEE Transactions on Signal Processing, vol. 56, no. 12, pp. 5891–5902, December 2008.",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Kernel Methods for Pattern Analysis",
      "author" : [ "J. Shawe-Taylor", "N. Cristianini" ],
      "venue" : null,
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2004
    }, {
      "title" : "Kernel independent component analysis",
      "author" : [ "F.R. Bach", "M.I. Jordan" ],
      "venue" : "Journal of Machine Learning Research, vol. 3, pp. 1–48, July 2002.",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Measuring statistical dependence with hilbertschmidt norms",
      "author" : [ "A. Gretton", "O. Bousquet", "A. Smola", "B. Schölkopf" ],
      "venue" : "Proceedings of Algorithmic Learning Theory, S. Jain, H. Simon, and E. Tomita, Eds., 2005, pp. 63–77.",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "A unified framework for quadratic measures of independence",
      "author" : [ "S. Seth", "M. Rao", "I. Park", "J.C. Prı́ncipe" ],
      "venue" : "IEEE Transactions on Signal Processing, vol. 59, no. 8, pp. 3624–3635, August 2011.",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Topics in Matrix Analysis",
      "author" : [ "R.A. Horn", "C.R. Johnson" ],
      "venue" : null,
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 1991
    }, {
      "title" : "Conditional rényi entropies",
      "author" : [ "A. Teixeira", "A. Matos", "L. Antunes" ],
      "venue" : "IEEE Transactions on Information Theory, vol. 58, no. 7, pp. 4273–4277, July 2012.",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Metric spaces and positive definite functions",
      "author" : [ "I.J. Schoenberg" ],
      "venue" : "Transactions of the American Mathematical Society, vol. 44, no. 3, pp. 522–536, November 1938.",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 1938
    }, {
      "title" : "Infinite divisible matrices",
      "author" : [ "R. Bhatia" ],
      "venue" : "The American Mathematical Monthly, vol. 113, no. 3, pp. 221– 235, March 2006.",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "The theory of infinitely divisible matrices and kernels",
      "author" : [ "R.A. Horn" ],
      "venue" : "Transactions of the American Mathematical Society, vol. 136, pp. 269–286, February 1969.",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 1969
    }, {
      "title" : "Convex spectral functions",
      "author" : [ "S. Friedland" ],
      "venue" : "Linear and Multilinear Algebra, vol. 9, pp. 299–316, 1981.",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 1981
    }, {
      "title" : "Derivatives of spectral functions",
      "author" : [ "A.S. Lewis" ],
      "venue" : "Mathematics of Operations Research, vol. 21, no. 3, pp. 576–588, August 1996.",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 1996
    }, {
      "title" : "Information-theoretic metric learning",
      "author" : [ "J.V. Davis", "B. Kulis", "P. Jain", "S. Sra", "I.S. Dhillon" ],
      "venue" : "ICML, Corvalis, Oregon, USA, June 2007, pp. 209–216.",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Neighborhood component analysis",
      "author" : [ "J. Goldberger", "S. Roweis", "G. Hinton", "R. Salakhutdinov" ],
      "venue" : "NIPS, 2004.",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Metric learning by collapsing classes",
      "author" : [ "A. Globerson", "S. Roweis" ],
      "venue" : "NIPS, 2005.",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Distance metric learning for large margin nearest neighbor classification",
      "author" : [ "K.Q. Weinberger", "J. Blitzer", "L.K. Saul" ],
      "venue" : "NIPS, 2005.",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2005
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "In [1], the use of Renyi’s definition of entropy along with Parzen density estimation is proposed as the main tool for information theoretic learning (ITL).",
      "startOffset" : 3,
      "endOffset" : 6
    }, {
      "referenceID" : 1,
      "context" : "Part of the research effort in this context has pointed out connections to reproducing kernel Hilbert spaces [2].",
      "startOffset" : 109,
      "endOffset" : 112
    }, {
      "referenceID" : 2,
      "context" : "The generality of this representation has been exploited in many practical applications, even for data that do not come in standard vector representation Rd [3].",
      "startOffset" : 157,
      "endOffset" : 160
    }, {
      "referenceID" : 3,
      "context" : "Some examples exploring this idea are: kernel independent component analysis [4], the work on measures of dependence and independence using Hilbert-Schmidt norms [5], and the quadratic measures of independence proposed in [6].",
      "startOffset" : 77,
      "endOffset" : 80
    }, {
      "referenceID" : 4,
      "context" : "Some examples exploring this idea are: kernel independent component analysis [4], the work on measures of dependence and independence using Hilbert-Schmidt norms [5], and the quadratic measures of independence proposed in [6].",
      "startOffset" : 162,
      "endOffset" : 165
    }, {
      "referenceID" : 5,
      "context" : "Some examples exploring this idea are: kernel independent component analysis [4], the work on measures of dependence and independence using Hilbert-Schmidt norms [5], and the quadratic measures of independence proposed in [6].",
      "startOffset" : 222,
      "endOffset" : 225
    }, {
      "referenceID" : 0,
      "context" : "In [1], the use of Renyi’s entropy is proposed as an alternative to the more commonly adopted definition of entropy given by Shannon.",
      "startOffset" : 3,
      "endOffset" : 6
    }, {
      "referenceID" : 6,
      "context" : "The following spectral decomposition theorem [7] relates to the functional calculus on matrices and provides a reasonable way to extend continuous scalar-valued functions to Hermitian matrices.",
      "startOffset" : 45,
      "endOffset" : 48
    }, {
      "referenceID" : 7,
      "context" : "The properties of this definition has been recently studied in the case of Renyi’s entropies [8] and in the matrix case, this definition yields:",
      "startOffset" : 93,
      "endOffset" : 96
    }, {
      "referenceID" : 0,
      "context" : "If A and B are normalized to have unit trace, then for r ∈ [0,1] it is true that the Hadamard product of A◦r ◦B◦(1−r), (11) is also normalized.",
      "startOffset" : 59,
      "endOffset" : 64
    }, {
      "referenceID" : 8,
      "context" : "Interestingly, the above condition implies that exp(−rd(xi,x j)) is positive definite in X for all r > 0 [9].",
      "startOffset" : 105,
      "endOffset" : 108
    }, {
      "referenceID" : 9,
      "context" : "Does the above hold if we to take fractional powers of A? In other words,is the matrix A◦ 1 m < 0 for any positive integer m? This question leads to the concept of infinitely divisible matrices [10, 11].",
      "startOffset" : 194,
      "endOffset" : 202
    }, {
      "referenceID" : 10,
      "context" : "Does the above hold if we to take fractional powers of A? In other words,is the matrix A◦ 1 m < 0 for any positive integer m? This question leads to the concept of infinitely divisible matrices [10, 11].",
      "startOffset" : 194,
      "endOffset" : 202
    }, {
      "referenceID" : 11,
      "context" : "These functions only depend on the eigenvalues of matrix and therefore their name [12].",
      "startOffset" : 82,
      "endOffset" : 86
    }, {
      "referenceID" : 12,
      "context" : "1) from [13] it is straightforward to obtain the derivative of (5) at A as",
      "startOffset" : 8,
      "endOffset" : 12
    }, {
      "referenceID" : 13,
      "context" : "UCI Data: To evaluate the results we use the same experimental setup proposed in [14], we compares 5 different approaches to supervised metric learning based on the classification error obtained from two-fold cross-validation using a 4-nearest neighbor classifier.",
      "startOffset" : 81,
      "endOffset" : 85
    }, {
      "referenceID" : 13,
      "context" : "Figure 2(a) shows the results of the proposed approach conditional entropy metric learning (CEML), information theoretic metric learning (ITML) proposed in [14], neighborhood component analysis (NCA) from [15], the maximally collapsing metric learning (MCML) method from [16], the large margin nearest neighbor (LMNN) method found in [17], and, as a baseline, the the inverse covariance and Euclidean distances.",
      "startOffset" : 156,
      "endOffset" : 160
    }, {
      "referenceID" : 14,
      "context" : "Figure 2(a) shows the results of the proposed approach conditional entropy metric learning (CEML), information theoretic metric learning (ITML) proposed in [14], neighborhood component analysis (NCA) from [15], the maximally collapsing metric learning (MCML) method from [16], the large margin nearest neighbor (LMNN) method found in [17], and, as a baseline, the the inverse covariance and Euclidean distances.",
      "startOffset" : 205,
      "endOffset" : 209
    }, {
      "referenceID" : 15,
      "context" : "Figure 2(a) shows the results of the proposed approach conditional entropy metric learning (CEML), information theoretic metric learning (ITML) proposed in [14], neighborhood component analysis (NCA) from [15], the maximally collapsing metric learning (MCML) method from [16], the large margin nearest neighbor (LMNN) method found in [17], and, as a baseline, the the inverse covariance and Euclidean distances.",
      "startOffset" : 271,
      "endOffset" : 275
    }, {
      "referenceID" : 16,
      "context" : "Figure 2(a) shows the results of the proposed approach conditional entropy metric learning (CEML), information theoretic metric learning (ITML) proposed in [14], neighborhood component analysis (NCA) from [15], the maximally collapsing metric learning (MCML) method from [16], the large margin nearest neighbor (LMNN) method found in [17], and, as a baseline, the the inverse covariance and Euclidean distances.",
      "startOffset" : 334,
      "endOffset" : 338
    }, {
      "referenceID" : 0,
      "context" : "UMist Faces: We also run the algorithm on the UMist dataset; This data set consists of Grayscale faces (8 bit [0-255]) of 20 different people.",
      "startOffset" : 110,
      "endOffset" : 117
    }, {
      "referenceID" : 1,
      "context" : "UMist Faces: We also run the algorithm on the UMist dataset; This data set consists of Grayscale faces (8 bit [0-255]) of 20 different people.",
      "startOffset" : 110,
      "endOffset" : 117
    }, {
      "referenceID" : 2,
      "context" : "UMist Faces: We also run the algorithm on the UMist dataset; This data set consists of Grayscale faces (8 bit [0-255]) of 20 different people.",
      "startOffset" : 110,
      "endOffset" : 117
    }, {
      "referenceID" : 3,
      "context" : "UMist Faces: We also run the algorithm on the UMist dataset; This data set consists of Grayscale faces (8 bit [0-255]) of 20 different people.",
      "startOffset" : 110,
      "endOffset" : 117
    }, {
      "referenceID" : 4,
      "context" : "UMist Faces: We also run the algorithm on the UMist dataset; This data set consists of Grayscale faces (8 bit [0-255]) of 20 different people.",
      "startOffset" : 110,
      "endOffset" : 117
    }, {
      "referenceID" : 5,
      "context" : "UMist Faces: We also run the algorithm on the UMist dataset; This data set consists of Grayscale faces (8 bit [0-255]) of 20 different people.",
      "startOffset" : 110,
      "endOffset" : 117
    }, {
      "referenceID" : 6,
      "context" : "UMist Faces: We also run the algorithm on the UMist dataset; This data set consists of Grayscale faces (8 bit [0-255]) of 20 different people.",
      "startOffset" : 110,
      "endOffset" : 117
    }, {
      "referenceID" : 7,
      "context" : "UMist Faces: We also run the algorithm on the UMist dataset; This data set consists of Grayscale faces (8 bit [0-255]) of 20 different people.",
      "startOffset" : 110,
      "endOffset" : 117
    }, {
      "referenceID" : 8,
      "context" : "UMist Faces: We also run the algorithm on the UMist dataset; This data set consists of Grayscale faces (8 bit [0-255]) of 20 different people.",
      "startOffset" : 110,
      "endOffset" : 117
    }, {
      "referenceID" : 9,
      "context" : "UMist Faces: We also run the algorithm on the UMist dataset; This data set consists of Grayscale faces (8 bit [0-255]) of 20 different people.",
      "startOffset" : 110,
      "endOffset" : 117
    }, {
      "referenceID" : 10,
      "context" : "UMist Faces: We also run the algorithm on the UMist dataset; This data set consists of Grayscale faces (8 bit [0-255]) of 20 different people.",
      "startOffset" : 110,
      "endOffset" : 117
    }, {
      "referenceID" : 11,
      "context" : "UMist Faces: We also run the algorithm on the UMist dataset; This data set consists of Grayscale faces (8 bit [0-255]) of 20 different people.",
      "startOffset" : 110,
      "endOffset" : 117
    }, {
      "referenceID" : 12,
      "context" : "UMist Faces: We also run the algorithm on the UMist dataset; This data set consists of Grayscale faces (8 bit [0-255]) of 20 different people.",
      "startOffset" : 110,
      "endOffset" : 117
    }, {
      "referenceID" : 13,
      "context" : "UMist Faces: We also run the algorithm on the UMist dataset; This data set consists of Grayscale faces (8 bit [0-255]) of 20 different people.",
      "startOffset" : 110,
      "endOffset" : 117
    }, {
      "referenceID" : 14,
      "context" : "UMist Faces: We also run the algorithm on the UMist dataset; This data set consists of Grayscale faces (8 bit [0-255]) of 20 different people.",
      "startOffset" : 110,
      "endOffset" : 117
    }, {
      "referenceID" : 15,
      "context" : "UMist Faces: We also run the algorithm on the UMist dataset; This data set consists of Grayscale faces (8 bit [0-255]) of 20 different people.",
      "startOffset" : 110,
      "endOffset" : 117
    }, {
      "referenceID" : 16,
      "context" : "UMist Faces: We also run the algorithm on the UMist dataset; This data set consists of Grayscale faces (8 bit [0-255]) of 20 different people.",
      "startOffset" : 110,
      "endOffset" : 117
    } ],
    "year" : 2013,
    "abstractText" : "In this paper, we develop a framework for information theoretic learning based on infinitely divisible matrices. We formulate an entropy-like functional on positive definite matrices based on Renyi’s axiomatic definition of entropy and examine some key properties of this functional that lead to the concept of infinite divisibility. The proposed formulation avoids the plug in estimation of density and brings along the representation power of reproducing kernel Hilbert spaces. As an application example, we derive a supervised metric learning algorithm using a matrix based analogue to conditional entropy achieving results comparable with the state of the art.",
    "creator" : "LaTeX with hyperref package"
  }
}