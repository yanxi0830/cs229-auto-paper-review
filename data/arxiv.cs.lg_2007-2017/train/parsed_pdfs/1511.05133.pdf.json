{
  "name" : "1511.05133.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Fast Proximal Linearized Alternating Direction Method of Multiplier with Parallel Splitting",
    "authors" : [ "Canyi Lu", "Huan Li", "Zhouchen Lin", "Shuicheng Yan" ],
    "emails" : [ "canyilu@gmail.com,", "lihuanss@pku.edu.cn,", "zlin@pku.edu.cn,", "eleyans@nus.edu.sg" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Introduction This work aims to solve the following linearly constrained separable convex problem with n blocks of variables\nmin x1,··· ,xn f(x) = n∑ i=1 fi(xi) = n∑ i=1 (gi(xi) + hi(xi)) ,\ns.t. A(x) = n∑ i=1 Ai(xi) = b, (1)\nwhere xi’s and b can be vectors or matrices and both gi and hi are convex and lower semi-continuous. For gi, we assume that ∇gi is Lipschitz continuous with the Lipschitz constant Li > 0, i.e, ‖∇gi(xi)−∇gi(yi)‖ ≤ Li ‖xi − yi‖ ,∀xi,yi. For hi, we assume that it may be nonsmooth and it is simple, in the sense that the proximal operator problem minx hi(x) + α2 ||x − a||\n2 (α > 0) can be cheaply solved. The bounded mappings Ai’s are linear (e.g., linear transformation or the sub-sampling operator in matrix completion (Candès and Recht 2009)). For the simplicity of discussion, we denote x = [x1;x2; · · · ;xn], A = [A1,A2, · · · ,An] and ∑n i=1Ai(xi) = A(x), fi = gi + hi.\n∗Corresponding author. Copyright © 2016, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\nFor any compact setX , letDX = supx1,x2∈X ||x1−x2|| be the diameter of X . We also denote Dx∗ = ||x0 − x∗||. We assume there exists a saddle point (x∗,λ∗) ∈ X×Λ to (71), i.e., A(x∗) = b and −ATi (λ∗) ∈ ∂fi(x∗i ), i = 1, · · · , n, where AT is the adjoint operator of A, X and Λ are the feasible sets of the primal variables and dual variables, respectively.\nBy using different gi’s and hi’s, a variety of machine learning problems can be cast into (71), including Lasso (Tibshirani 1996) and its variants (Lu et al. 2013; Jacob, Obozinski, and Vert 2009), low rank matrix decomposition (Candès et al. 2011), completion (Candès and Recht 2009) and representation model (Lu et al. 2012; Liu and Yan 2011) and latent variable graphical model selection (Chandrasekaran, Parrilo, and Willsky 2012). Specifically, examples of gi are: (i) the square loss 12 ||Dx − y||\n2 , where D and y are of compatible dimensions. A more special case is the known Laplacian regularizer Tr(XLXT ), where L is the Laplacian matrix which is positive semi-definite; (ii) Logistic loss ∑m i=1 log(1 + exp(−yidTi x)), where di’s and yi’s are the data points and the corresponding labels, respectively; (iii) smooth-zero-one loss ∑m i=1\n1 1+exp(cyidTi x) , c > 0. The possibly nonsmooth hi can be many norms, e.g., `1-norm || · ||1 (the sum of absolute values of all entries), `2-norm || · || or Frobenius norm || · ||F and nuclear norm || · ||∗ (the sum of the singular values of a matrix).\nThis paper focuses on the popular approaches which study problem (71) from the aspect of the augmented Lagrangian function L(x,λ) = f(x)+〈λ,A(x)−b〉+ β2 ||A(x)−b||\n2, where λ is the Lagrangian multiplier or dual variable and β > 0. A basic idea to solve problem (71) based on L(x,λ) is the Augmented Lagrangian Method (ALM) (Hestenes 1969), which is a special case of the Douglas-Rachford splitting (Douglas and Rachford 1956).\nAn influential variant of ALM is the Alternating Direction Mehtod of Multiplier (ADMM) (Boyd et al. 2011), which solves problem (71) with n = 2 blocks of variables. However, the cost for solving the subproblems in ALM and ADMM in each iteration is usually high when fi is not simple and Ai is non-unitary (ATi Ai is not the identity mapping). To alleviate this issue, the Linearized ALM (LALM) (Yang and Yuan 2013) and Linearized ADMM (LADMM) (Lin, Liu, and Su 2011) were proposed by linearizing the\nar X\niv :1\n51 1.\n05 13\n3v 1\n[ m\nat h.\nO C\n] 1\n4 N\nov 2\n01 5\naugmented term β2 ||A(x) − b|| 2 and thus the subproblems are easier to solve. For (71) with n > 2 blocks of variables, the Proximal Jacobian ADMM (Tao 2014) and Linearized ADMM with Parallel Splitting (L-ADMM-PS) (Lin, Liu, and Li 2014) guaranteed to solve (71) when gi = 0 with convergence guarantee. To further exploit the Lipschitz continuous gradient property of gi’s in (71), the work (Lin, Liu, and Li 2014) proposed a Proximal Linearized ADMM with Parallel Splitting (PL-ADMM-PS) by further linearizing the smooth part gi. PL-ADMM-PS requires lower per-iteration cost than L-ADMM-PS for solving the general problem (71).\nBeyond the per-iteration cost, another important way to measure the speed of the algorithms is the convergence rate. Several previous work proved the convergence rates of the augmented Lagrangian function based methods (He and Yuan 2012; Tao 2014; Lin, Liu, and Li 2014). Though the convergence functions used to measure the convergence rate are different, the convergence rates of all the above discussed methods for (71) are all O(1/K), where K is the number of iterations. However, the rate O(1/K) may be suboptimal in some cases. Motivated by the seminal work (Nesterov 1983), several fast first-order methods with the optimal rate O(1/K2) have been developed for unconstrained problems (Beck and Teboulle 2009; Tseng 2008). More recently, by applying a similar accelerating technique, several fast ADMMs have been proposed to solve a special case of problem (71) with n = 2 blocks of variables\nmin x1,x2\ng1(x1) + h2(x2), s.t. A1(x1) +A2(x2) = b. (2)\nA fast ADMM proposed in (Azadi and Sra 2014)1 is able to solve (2) with the convergence rate O ( D2X K2 + D2Λ K ) . But their result is a bit weak since their used function to characterize the convergence can be negative. The work (Ouyang et al. 2015) proposed another fast ADMM with the rate O ( D2x∗ K2 + D2x∗ K ) for primal residual and\nO ( D2x∗ K3/2 + Dx∗+Dλ∗K ) for feasibility residual. However, their result requires that the number of iterations K should be predefined, which is not reasonable in practice. It is usually difficult in practice to determine the optimal K since we usually stop the algorithms when both the primal and feasibility residuals are sufficiently small (Lin, Liu, and Li 2014). The fast ALM proposed in (He and Yuan 2010) owns the convergence rate O(1/K2), but it requires the objective f to be differentiable. This limits its applications for nonsmooth optimization in most compressed sensing problems.\n1The method in (Azadi and Sra 2014) is a fast stochastic ADMM. It is easy to give the corresponding deterministic version by computing the gradient in each iteration exactly to solve (2).\nAnother work (Goldstein et al. 2014) proved a better convergence rate than O(1/K) for ADMM. But their method requires much stronger assumptions, e.g., strongly convexity of fi’s, which are usually violated in practice. In this work, we only consider (71) whose objective is not necessarily strongly convex.\nIn this work, we aim to propose fast ALM type methods to solve the general problem (71) with optimal convergence rates. The contributions are summarized as follows: • First, we consider (71) with n = 1 (or one may regard all n blocks as a superblock) and propose the Fast Proximal Augmented Lagrangian Method (Fast PALM). We prove that Fast PALM converges with the rate O ( D2x∗+D 2 λ∗\nK2\n) ,\nwhich is a significant improvement of ALM/PALM2 with rate O ( D2x∗+D 2 λ∗\nK\n) . To the best of our knowledge, Fast\nPALM is the first improved ALM/PALM which achieves the rate O(1/K2) for the nonsmooth problem (71).\n• Second, we consider (71) with n > 2 and propose the Fast Proximal Linearized ADMM with Parallel Splitting (Fast PL-ADMM-PS), which converges with rate O ( D2x∗ K2 + D2X K + D2Λ K ) . As discussed in Sec-\ntion 1.3 of (Ouyang et al. 2015), such a rate is optimal and thus is better than PL-ADMM-PS with rate O ( D2x∗ K + D2x∗ K + D2λ∗ K ) (Lin, Liu, and Li 2014). To the best of our knowledge, Fast PL-ADMM-PS is the first fast Jacobian type (update the variables in parallel) method to solve (71) when n > 2 with convergence guarantee.\nTable 1 shows the comparison of the convergence rates of previous methods and our fast versions. Note that Fast PALM and Fast PL-ADMM-PS have the same pter-iteration cost as PALM and PL-ADMM-PS, respectively. But the periteration cost of PL-ADMM-PS and Fast PL-ADMM-PS may be much cheaper than PALM and Fast PALM.\nFast Proximal Augmented Lagrangian Method In this section, we consider (71) with n = 1 block of variable,\nmin x f(x) = g(x) + h(x), s.t. A(x) = b, (3)\nwhere g and h are convex and ∇g is Lipschitz continuous with the Lipschitz constant L. The above problem can be solved by the traditional ALM which updates x and λ by xk+1 = arg min x g(x) + h(x) + 〈λk,A(x)− b〉 + β(k) 2 ||A(x)− b||2,\nλk+1 =λk + β(k)(A(xk+1)− b),\n(4)\n2PALM is a variant of ALM proposed in this work.\nInitialize: x0, z0, λ0, β(0) = θ(0) = 1. for k = 0, 1, 2, · · · do\nyk+1 = (1− θ(k))xk + θ(k)zk; (6) zk+1 = argmin\nx\n〈 ∇g(yk+1),x 〉 + h(x)\n+ 〈 λk,A(x) 〉 + β(k)\n2 ‖A(x)− b‖2\n+ Lθ(k)\n2 ‖x− zk‖2; (7)\nxk+1 = (1− θ(k))xk + θ(k)zk+1; (8) λk+1 = λk + β(k)(A(zk+1)− b); (9)\nθ(k+1) = −(θ(k))2 +\n√ (θ(k))4 + 4(θ(k))2\n2 ; (10)\nβ(k+1) = 1\nθ(k+1) . (11)\nend Algorithm 1: Fast PALM Algorithm\nwhere β(k) > 0. Note that ∇g is Lipschitz continuous. We have (Nesterov 2004)\ng(x) ≤ g(xk) + 〈∇g(xk),x− xk〉+ L 2 ||x− xk||2. (5)\nThis motivates us to use the right hand side of (5) as a surrogate of g in (4). Thus we can update x by solving the following problem which is simpler than (5),\nxk+1 = arg min x g(xk) + 〈∇g(xk),x− xk〉+ h(x)\n+ 〈λk,A(x)− b〉+ β (k) 2 ||A(x)− b||2 + L 2 ||x− xk||2.\nWe call the method by using the above updating rule as Proximal Augmented Lagrangian Method (PALM). PALM can be regarded as a special case of Proximal Linearized Alternating Direction Method of Multiplier with Parallel Splitting in (Lin, Liu, and Li 2014) and it owns the convergence rate O (1/K), which is the same as the traditional ALM and ADMM. However, such a rate is suboptimal. Motivated by the technique from the accelerated proximal gradient method (Tseng 2008), we propose the Fast PALM as shown in Algorithm 1. It uses the interpolatory sequences yk and zk as well as the stepsize θ(k). Note that if we set θ(k) = 1 in each iteration, Algorithm 1 reduces to PALM. With careful choices of θ(k) and β(k) in Algorithm 1, we can accelerate the convergence rate of PALM from O (1/K) to O(1/K2).\nProposition 1. In Algorithm 1, for any x, we have\n1− θ(k+1) (θ(k+1))2 ( f(xk+1)− f(x) ) − 1 θ(k) 〈 AT (λk+1),x− zk+1 〉 ≤ 1− θ (k)\n(θ(k))2 ( f(xk)− f(x) ) (12)\n+ L\n2\n( ‖zk − x‖2 − ‖zk+1 − x‖2 ) .\nTheorem 1. In Algorithm 1, for any K > 0, we have f(xK+1)− f(x∗) + 〈 λ∗,A(xK+1)− b 〉 + 1\n2 ‖A(xK+1)− b‖2\n≤ 2 (K + 2)2\n( LD2x∗ +D 2 λ∗ ) . (13)\nWe use the convergence function, i.e., the left hand side of (13), in (Lin, Liu, and Li 2014) to measure the convergence rate of the algorithms in this work. Theorem 4 shows that our Fast PALM achieves the rate O ( LD2x∗+D 2 λ∗\nK2\n) , which\nis much better than O ( LD2x∗+ 1 βD 2 λ∗\nK\n) by PALM3. The\nimprovement of Fast PALM over PALM is similar to the one of Fast ISTA over ISTA (Beck and Teboulle 2009; Tseng 2008). The difference is that Fast ISTA targets for unconstrained problem which is easier than our problem (71). Actually, if the constraint in (71) is dropped (i.e., A = 0, b = 0), our Fast PALM is similar as the Fast ISTA.\nWe would like to emphasize some key differences between our Fast PALM and previous fast ALM type methods (Azadi and Sra 2014; Ouyang et al. 2015; He and Yuan 2010). First, it is easy to apply the two blocks fast ADMM methods in (Azadi and Sra 2014; Ouyang et al. 2015) to solve problem (3). Following their choices of parameters and proofs, the convergence rates are still O(1/K). The key improvement of our method comes from the different choices of θ(k) and β(k) as shown in Theorem 4. The readers can refer to the detailed proofs in the supplementary material. Second, the fast ADMM in (Ouyang et al. 2015) requires predefining the total number of iterations, which is usually difficult in practice. However, our Fast PALM has no such a limitation. Third, the fast ALM in (He and Yuan 2010) also owns the rate O(1/K2). But it is restricted to differentiable objective minimization and thus is not applicable to our problem (71). Our method has no such a limitation.\nA main limitation of PALM and Fast PALM is that their per-iteration cost may be high when hi is nonsmooth and Ai is non-unitary. In this case, solving the subproblem (7) requires calling other iterative solver, e.g., Fast ISTA (Beck and Teboulle 2009), and thus the high per-iteration cost may limit the application of Fast PALM. In next section, we present a fast ADMM which has lower per-iteration cost.\n3It is easy to achieve this since PALM is a special case of Fast PALM by taking θ(k) = 1.\nFast Proximal Linearized ADMM with Parallel Splitting\nIn this section, we consider problem (71) with n > 2 blocks of variables. The state-of-the-art solver for (71) is the Proximal Linearized ADMM with Parallel Splitting (PL-ADMMPS) (Lin, Liu, and Li 2014) which updates each xi in parallel by\nxk+1i = argmin xi gi(x k i ) +\n〈 ∇gi(xki ),xi − xki 〉 + hi(xi)\n+ 〈 λk,Ai(xi) 〉 + 〈 β(k)ATi ( A(xk)− b ) ,xi − xki 〉 + Li + β\n(k)ηi 2 ‖xi − xki ‖2, (14)\nwhere ηi > n||Ai||2 and β(k) > 0. Note that the subproblem (14) is easy to solve when hi is nonsmooth but simple. Thus PL-ADMM-PS has much lower per-iteration cost than PALM and Fast PALM. On the other hand, PL-ADMM-PS converges with the rate O(1/K) (Lin, Liu, and Li 2014). However, such a rate is also suboptimal. Now we show that it can be further accelerated by a similar technique as that in Fast PALM. See Algorithm 2 for our Fast PL-ADMM-PS.\nProposition 2. In Algorithm 2, for any xi, we have\n1− θ(k+1) (θ(k+1))2 ( fi(x k+1 i )− fi(xi) ) − 1 θ(k) 〈 ATi (λ̂k+1),xi − zk+1i\n〉 ≤1− θ (k)\n(θ(k))2 ( fi(x k i )− fi(xi) ) (15)\n+ Li 2\n( ‖zki − xi‖2 − ‖zk+1i − xi‖ 2 )\n+ β(k)ηi 2θ(k)\n( ‖zki − xi‖2 − ‖zk+1i − xi‖ 2 − ‖zk+1i − z k i ‖2 ) ,\nwhere λ̂ k+1 = λk + β(k) ( A(zk)− b ) .\nTheorem 2. In Algorithm 2, for any K > 0, we have f(xK+1)− f(x∗) + 〈 λ∗,A(xK+1)− b 〉 + βα\n2 ∥∥A(xK+1)− b∥∥2 (16) ≤2LmaxD 2 x∗\n(K + 2)2 +\n2βηmaxD 2 X\nK + 2 + 2D2Λ β(K + 2) ,\nwhere α = min {\n1 n+1 ,\n{ ηi−n‖Ai‖2 2(n+1)‖Ai‖2 , i = 1, · · · , n }} ,\nLmax = max{Li, i = 1, · · · , n} and ηmax = max{ηi, i = 1, · · · , n}.\nFrom Theorem 5, it can be seen that our Fast PLADMM-PS partially accelerates the convergence rate of PL-ADMM-PS from O ( LmaxD 2 x∗\nK + βηmaxD\n2 x∗ K + D2λ∗ βK\n) to\nO ( LmaxD 2 x∗\nK2 + βηmaxD\n2 X K + D2Λ βK\n) . Although the improved\nrate is also O(1/K), what makes it more attractive is that it allows very large Lipschitz constants Li’s. In particular,\nInitialize: x0, z0, λ0, θ(0) = 1, fix β(k) = β for k ≥ 0, ηi > n‖Ai‖2, i = 1, · · · , n, for k = 0, 1, 2, · · · do\n//Update yi, zi, xi, i = 1, · · · , n, in parallel by\nyk+1i = (1− θ (k))xki + θ (k)zki ; (17)\nzk+1i = argmin xi\n〈 ∇gi(yk+1i ),xi 〉 + hi(xi)\n+ 〈 λk,Ai(xi) 〉 + 〈 β(k)ATi ( A(zk)− b ) ,xi 〉 + L(gi)θ\n(k) + β(k)ηi 2 ‖xi − zki ‖2; (18)\nxk+1i = (1− θ (k))xki + θ (k)zk+1i ; (19) λk+1 = λk + βk ( A(zk+1)− b ) ; (20)\nθ(k+1) = −(θ(k))2 +\n√ (θ(k))4 + 4(θ(k))2\n2 . (21)\nend Algorithm 2: Fast PL-ADMM-PS Algorithm\nLi can be as large as O(K), without affecting the rate of convergence (up to a constant factor). The above improvement is the same as fast ADMMs (Ouyang et al. 2015) for problem (2) with only n = 2 blocks. But it is inferior to the Fast PALM over PALM. The key difference is that Fast PL-ADMM-PS further linearizes the augmented term 12 ||A(x)− b||\n2. This improves the efficiency for solving the subproblem, but slows down the convergence. Actually, when linearizing the augmented term, we have a new term with the factor β(k)ηi/θ(k) in (77) (compared with (46) in Fast PALM). Thus (93) has a new term by comparing with that in (13). This makes the choice of β(k) in Fast PLADMM-PS different from the one in Fast PALM. Intuitively, it can be seen that a larger value of β(k) will increase the second terms of (93) and decrease the third term of (93). Thus β(k) should be fixed in order to guarantee the convergence. This is different from the choice of β(k) in Fast PALM which is adaptive to the choice of θ(k).\nCompared with PL-ADMM-PS, our Fast PL-ADMM-PS achieves a better rate, but with the price on the boundedness of the feasible primal set X and the feasible dual set Λ. Note that many previous work, e.g., (He and Yuan 2012; Azadi and Sra 2014), also require such a boundedness assumption when proving the convergence of ADMMs. In the following, we give some conditions which guarantee such a boundedness assumption. Theorem 3. Assume the mapping A(x1, · · · ,xn) =∑n i=1Ai(xi) is onto4, the sequence {zk} is bounded, ∂h(x) and ∇g(x) are bounded if x is bounded, then {xk}, {yk} and {λk} are bounded.\nMany convex functions, e.g., the `1-norm, in compressed 4This assumption is equivalent to that the matrix A ≡ (A1, · · · , An) is of full row rank, where Ai is the matrix representation of Ai.\nsensing own the bounded subgradient.\nExperiments In this section, we report some numerical results to demonstrate the effectiveness of our fast PALM and PL-ADMMPS. We first compare our Fast PALM which owns the optimal convergence rate O(1/K2) with the basic PALM on a problem with only one block of variable. Then we conduct two experiments to compare our Fast PL-ADMM-PS with PL-ADMM-PS on two multi-blocks problems. The first one is tested on the synthesized data, while the second one is for subspace clustering tested on the real-world data. We examine the convergence behaviors of the compared methods based on the convergence functions shown in (13) and (93). All the numerical experiments are run on a PC with 8 GB of RAM and Intel Core 2 Quad CPU Q9550.\nComparison of PALM and Fast PALM\nWe consider the following problem\nmin x ||x||1 +\nα 2 ||Ax− b||22, s.t. 1Tx = 1, (22)\nwhere α > 0, A ∈ Rm×n, b ∈ Rm, and 1 ∈ Rn is the all one vector. There may have many fast solvers for problem (22). In this experiment, we focus on the performance comparison of PALM and Fast PALM for (22). Note that the per-iteration cost of these two methods are the same. Both of them requires solving an `1-minimization problem in each iteration. In this work, we use the SPAMS package (Mairal et al. 2010) to solve it which is very fast.\nThe data matrix A ∈ Rm×n, and b ∈ Rm are generated by the Matlab command randn. We conduct four experiments on different sizes of A and b. We use the left hand side of (13) as the convergence function to evaluate the convergence behaviors of PALM and Fast PALM. For the saddle point (x∗,λ∗) in (13), we run the Fast PALM with 10,000 iterations and use the obtained solution as the saddle point. Figure 1 plots the convergence functions value within 1,000 iterations. It can be seen that our Fast PALM converges much faster than PALM. Such a result verifies our theoretical improvement of Fast PALM with optimal rate O(1/K2) over PALM with the rate O(1/K).\nComparison of PL-ADMM-PS and Fast PL-ADMM-PS In this subsection, we conduct a problem with three blocks of variables as follows\nmin X1,X2,X3 3∑ i=1 ( ||Xi||`i + αi 2 ||CiXi −Di||2F ) ,\ns.t. 3∑ i=1 AiXi = B,\n(23)\nwhere || · ||`1 = || · ||1 is the `1-norm, || · ||`2 = || · ||∗ is the nuclear norm, and || · ||`3 = || · ||2,1 is the `2,1-norm defined as the sum of the `2-norm of each column of a matrix. We simply consider all the matrices with the same size Ai,Ci,Di,B,Xi ∈ Rm×m. The matrices Ai,Ci,Di, i = 1, 2, 3, and B are generated by the Matlab command randn. We set the parameters α1 = α2 = α3 = 0.1. Problem (23) can be solved by PL-ADMM-PS and Fast PL-ADMM-PS, which have the same and cheap per-iteration cost. The experiments are conducted on three different values of m =100, 300 and 500. Figure 2 plots the convergence function values of PL-ADMM-PS and Fast PL-ADMM-PS in (93). It can be seen that Fast PL-ADMM-PS converges much faster than PL-ADMM-PS. Though Fast PL-ADMM-PS only accelerates PL-ADMM-PS for the smooth parts gi’s, the improvement of Fast PL-ADMM-PS over PL-ADMM-PS is similar to that in Fast PALM over PALM. The reason behind this is that the Lipschitz constants Li’s are not very small (around 400, 1200, and 2000 for the cases m = 100, m = 300, and m = 500, respectively). And thus reducing the first term of (93) faster by our method is important.\nApplication to Subspace Clustering In this subsection, we consider the following low rank and sparse representation problem for subspace clustering\nmin Z\nα1||Z||∗ + α2||Z||1 + 1\n2 ||XZ−X||2,\ns.t. 1TZ = 1T , (24)\nwhere X is the given data matrix. The above model is motivated by (Zhuang et al. 2012). However, we further consider the affine constraint 1TZ = 1T for affine subspace clustering (Elhamifar and Vidal 2013). Problem (24) can be reformulated as a special case of problem (71) by introducing\nGiven a data matrix X with each column as a sample, we solve (24) to get the optimal solution Z∗. Then the affinity matrix W is defined as W = (|Z| + |ZT |)/2. The normalized cut algorithm (Shi and Malik 2000) is then performed on W to get the clustering results of the data matrix X. The whole clustering algorithm is the same as (Elhamifar and Vidal 2013), but using our defined affinity matrix W above.\nWe conduct experiments on the Extended Yale B database (Georghiades, Belhumeur, and Kriegman 2001), which is challenging for clustering. It consists of 2,414 frontal face images of 38 subjects under various lighting, poses and illumination conditions. Each subject has 64 faces. We construct three matrices X based on the first 5, 8 and 10 subjects. The data matrices X are first projected into a 5 × 6, 8 × 6, and 10×6-dimensional subspace by PCA, respectively. Then we run PL-ADMM-PS and Fast PL-ADMM-PS for 1000 itera-\ntions, and use the solutions Z to define the affinity matrix W = (|Z| + |ZT |)/2. Finally, we can obtain the clustering results by normalized cuts. The accuracy, calculated by the best matching rate of the predicted label and the ground truth of data, is reported to measure the performance. Table 2 shows the clustering accuracies based on the solutions to problem (24) obtained by PL-ADMM-PS and Fast PLADMM-PS. It can be seen that Fast PL-ADMM-PS usually outperfoms PL-ADMM-PS since it achieves a better solution than PL-ADMM-PS within 1000 iterations. This can be verified in Figure 3 which shows the convergence function values in (93) of PL-ADMM-PS and Fast PL-ADMM-PS in each iteration. It can be seen that our Fast PL-ADMM-PS converges much faster than PL-ADMM-PS.\nConclusions This paper presented two fast solvers for the linearly constrained convex problem (71). In particular, we proposed the Fast Proximal Augmented Lagragian Method (Fast PALM) which achieves the convergence rate O(1/K2). Note that such a rate is theoretically optimal by comparing with the rate O(1/K) by traditional ALM/PALM. Our fast version does not require additional assumptions (e.g. boundedness of X and Λ, or a predefined number of iterations) as in the\nprevious works (Azadi and Sra 2014; Ouyang et al. 2015). In order to further reduce the per-iteration complexity and handle the multi-blocks problems (n > 2), we proposed the Fast Proximal Linearized ADMM with Parallel Splitting (Fast PL-ADMM-PS). It also achieves the optimalO(1/K2) rate for the smooth part of the objective. Compared with PL-ADMM-PS, though Fast PL-ADMM-PS requires additional assumptions on the boundedness ofX and Λ in theory, our experimental results show that significant improvements are obtained especially when the Lipschitz constant of the smooth part is relatively large.\nAcknowledgements This research is supported by the Singapore National Research Foundation under its International Research Centre @Singapore Funding Initiative and administered by the IDM Programme Office. Z. Lin is supported by NSF China (grant nos. 61272341 and 61231002), 973 Program of China (grant no. 2015CB3525) and MSRA Collaborative Research Program.\nReferences [Azadi and Sra 2014] Azadi, S., and Sra, S. 2014. Towards an\noptimal stochastic alternating direction method of multipliers. In ICML, 620–628.\n[Beck and Teboulle 2009] Beck, A., and Teboulle, M. 2009. A fast iterative shrinkage-thresholding algorithm for linear inverse problems. SIAM Journal on Imaging Sciences.\n[Boyd et al. 2011] Boyd, S.; Parikh, N.; Chu, E.; Peleato, B.; and Eckstein, J. 2011. Distributed optimization and statistical learning via the alternating direction method of multipliers. Foundations and Trends® in Machine Learning 3(1):1–122.\n[Candès and Recht 2009] Candès, E., and Recht, B. 2009. Exact matrix completion via convex optimization. Foundations of Computational mathematics 9(6):717–772.\n[Candès et al. 2011] Candès, E. J.; Li, X. D.; Ma, Y.; and Wright, J. 2011. Robust principal component analysis? Journal of the ACM 58(3).\n[Chandrasekaran, Parrilo, and Willsky 2012] Chandrasekaran, V.; Parrilo, P. A.; and Willsky, A. S. 2012. Latent variable graphical model selection via convex optimization. 40(4):1935–1967.\n[Douglas and Rachford 1956] Douglas, J., and Rachford, H. H. 1956. On the numerical solution of heat conduction problems in two and three space variables. Transactions of the American mathematical Society 421–439.\n[Elhamifar and Vidal 2013] Elhamifar, E., and Vidal, R. 2013. Sparse subspace clustering: Algorithm, theory, and applications. TPAMI 35(11):2765–2781.\n[Georghiades, Belhumeur, and Kriegman 2001] Georghiades, A. S.; Belhumeur, P. N.; and Kriegman, D. 2001. From few to many: Illumination cone models for face recognition under variable lighting and pose. PAMI 23(6):643–660.\n[Goldstein et al. 2014] Goldstein, T.; O’Donoghue, B.; Setzer, S.; and Baraniuk, R. 2014. Fast alternating direction optimization methods. SIAM Journal on Imaging Sciences 7(3):1588–1623.\n[He and Yuan 2010] He, B., and Yuan, X. 2010. On the acceleration of augmented Lagrangian method for linearly constrained optimization. optimization online www. optimization-online. org/DB FILE/2010/10/2760. pdf.\n[He and Yuan 2012] He, B., and Yuan, X. 2012. On the O(1/n) convergence rate of the Douglas-Rachford alternating direction method. SIAM Journal on Numerical Analysis 50(2):700–709.\n[Hestenes 1969] Hestenes, M. R. 1969. Multiplier and gradient methods. Journal of optimization theory and applications 4(5):303–320.\n[Jacob, Obozinski, and Vert 2009] Jacob, L.; Obozinski, G.; and Vert, J.-P. 2009. Group Lasso with overlap and graph Lasso. In Proceedings of the 26th Annual International Conference on Machine Learning, 433–440. ACM.\n[Lin, Liu, and Li 2014] Lin, Z.; Liu, R.; and Li, H. 2014. Linearized alternating direction method with parallel splitting and adaptive penalty for separable convex programs in machine learning. In Machine Learning.\n[Lin, Liu, and Su 2011] Lin, Z.; Liu, R.; and Su, Z. 2011. Linearized alternating direction method with adaptive penalty for lowrank representation. In NIPS, volume 2, 6.\n[Liu and Yan 2011] Liu, G., and Yan, S. 2011. Latent low-rank representation for subspace segmentation and feature extraction. In ICCV, 1615–1622. IEEE.\n[Lu et al. 2012] Lu, C.-Y.; Min, H.; Zhao, Z.-Q.; Zhu, L.; Huang, D.-S.; and Yan, S. 2012. Robust and efficient subspace segmentation via least squares regression. In ECCV, 347–360. Springer.\n[Lu et al. 2013] Lu, C.; Feng, J.; Lin, Z.; and Yan, S. 2013. Correlation adaptive subspace segmentation by trace Lasso. In CVPR. IEEE.\n[Mairal et al. 2010] Mairal, J.; Bach, F.; Ponce, J.; and Sapiro, G. 2010. Online learning for matrix factorization and sparse coding. JMLR 11:19–60.\n[Nesterov 1983] Nesterov, Y. 1983. A method for unconstrained convex minimization problem with the rate of convergence O(1/k2). In Doklady AN SSSR, volume 269, 543–547.\n[Nesterov 2004] Nesterov, Y. 2004. Introductory lectures on convex optimization: A basic course, volume 87. Springer.\n[Ouyang et al. 2015] Ouyang, Y.; Chen, Y.; Lan, G.; and Pasiliao Jr, E. 2015. An accelerated linearized alternating direction method of multipliers. SIAM Journal on Imaging Sciences 8(1):644–681.\n[Shi and Malik 2000] Shi, J., and Malik, J. 2000. Normalized cuts and image segmentation. PAMI 22(8):888–905.\n[Tao 2014] Tao, M. 2014. Some parallel splitting methods for separable convex program-ming with O(1/t) convergence rate. Pacific Journal of Optimization 10:359–384.\n[Tibshirani 1996] Tibshirani, R. 1996. Regression shrinkage and selection via the Lasso. Journal of the Royal Statistical Society. Series B (Methodological) 267–288.\n[Tseng 2008] Tseng, P. 2008. On accelerated proximal gradient methods for convex-concave optimization. in submission.\n[Yang and Yuan 2013] Yang, J., and Yuan, X. 2013. Linearized augmented Lagrangian and alternating direction methods for nuclear norm minimization. Mathematics of Computation 82(281):301–329.\n[Zhuang et al. 2012] Zhuang, L.; Gao, H.; Lin, Z.; Ma, Y.; Zhang, X.; and Yu, N. 2012. Non-negative low rank and sparse graph for semi-supervised learning. In CVPR, 2328–2335. IEEE.\nSupplementary Material of Fast Proximal Linearized Alternating Direction Method of Multiplier with Parallel Splitting\nCanyi Lu1, Huan Li2, Zhouchen Lin2,3,?, Shuicheng Yan1 1 Department of Electrical and Computer Engineering, National University of Singapore\n2 Key Laboratory of Machine Perception (MOE), School of EECS, Peking University 3 Cooperative Medianet Innovation Center, Shanghai Jiaotong University\ncanyilu@gmail.com, lihuanss@pku.edu.cn, zlin@pku.edu.cn, eleyans@nus.edu.sg\nThis documents provides the proof details of the convergence results of our proposed fast methods. First, in Section 1, we give some useful results which are useful for the convergence analysis of Fast PALM in Section 2 and Fast PL-ADMM-PS in Section 3.\nSome Lemmas Lemma 1. Let g : Rm → R be a continuously differentiable function with Lipschitz continuous gradient and Lipschitz constant L. Then, for any x,y ∈ Rm,\ng(x) ≤ g(y) + 〈x− y,∇g(y)〉+ L 2 ||x− y||2. (25)\nLemma 2. Given any a, b, c, d ∈ Rm, we have\n〈a− b,a− c〉 = 1 2\n( ‖a− b‖2 + ‖a− c‖2 − ‖b− c‖2 ) . (26)\n〈a− b, c− d〉 = 1 2\n( ‖a− d‖2 − ‖a− c‖2 − ‖b− d‖2 + ‖b− c‖2 ) . (27)\nLemma 3. Assume the sequences {a(k)} and {b(k)} satisfy a(0) = 1, 0 < a(k+1) − a(k) ≤ 1 and b(k) > 0. Then we have K∑ k=0 a(k)(b(k) − b(k+1)) ≤ K∑ k=0 b(k). (28)\nProof. We deduce K∑ k=0 a(k)(b(k) − b(k+1)) = a(0)b(0) + K−1∑ k=0 (a(k+1) − a(k))b(k+1) − a(K)b(K+1)\n≤ b(0) + K−1∑ k=0 b(k+1) = K∑ k=0 b(k).\nLemma 4. Define the sequence {θ(k)} as θ(0) = 1, 1−θ (k+1)\n(θ(k+1))2 = 1 (θ(k))2 and θ(k) > 0. Then we have the following properties\nθ(k+1) = −(θ(k))2 +\n√ (θ(k))4 + 4(θ(k))2\n2 , (29)\nK∑ k=0 1 θ(k) =\n1\n(θ(K))2 , (30)\n0 < 1 θ(k+1) − 1 θ(k) < 1, (31)\nθ(k) ≤ 2 k + 2 , (32)\nand θ(k) ≤ 1. (33)\n?Corresponding author.\nProof. From the definition of θ(k+1), it is easy to get that θ(k+1) = −(θ (k))2+\n√ (θ(k))4+4(θ(k))2\n2 . This implies that θ (k) is well\ndefined for any k ≥ 0. Furthermore, since 1 θ(k+1) = 1 (θ(k+1))2 − 1 (θ(k))2 and θ(0) = 1, we have\nK∑ k=0 1 θ(k) = 1 θ(0) + K−1∑ k=0 1 θ(k+1) = 1 θ(0) + K−1∑ k=0 ( 1 (θ(k+1))2 − 1 (θ(k))2 ) = 1 θ(0) +\n1 (θ(K))2 − 1 (θ(0))2 =\n1\n(θ(K))2 . (34)\nFrom 1 θ(k+1) = 1 (θ(k+1))2 − 1 (θ(k))2 , θ(k) > 0 and θ(k−1) > 0, we can easily get\n1 θ(k+1) − 1 θ(k) > 0, (35)\nand 1\nθ(k+1) − 1 θ(k) = 1 θ(k+1) − √ 1− θ(k+1) θ(k+1) = 1− √ 1− θ(k+1) θ(k+1) = 1 1 + √ 1− θ(k+1) < 1. (36)\nNext we proof θ(k) ≤ 2k+2 by induction. First θ (0) = 1 ≤ 20+2 . Now assume that θ (k) ≤ 2k+2 and we prove θ (k+1) ≤ 2k+3 . We deduce\nθ(k+1) = −(θ(k))2 +\n√ (θ(k))4 + 4(θ(k))2\n2 =\n2(θ(k))2 (θ(k))2 + √ (θ(k))4 + 4(θ(k))2\n= 2 1 + √\n1 + 4 (θ(k))2\n≤ 2 1 + √ 1 + (k + 2)2 ≤ 2 k + 3 .\nSo (32) holds. Note that θ(k) is decreasing by (31) and θ(0) = 1, we have (33). The proof is completed.\nConvergence Analysis of Fast PALM In this section, we give the convergence analysis of Fast PALM for solving the following problem\nmin x f(x), s.t. A(x) = b, (37)\nwhere f(x) = g(x) + h(x), both g and h are convex, and g ∈ C1.1:\n‖∇g(x)−∇g(y)‖ ≤ L ‖x− y‖ , ∀x,y. (38)\nFor the completeness, we give the Fast PALM in Algorithm 3. It is worth mentioning that the definition of θ(k+1) in (44) is equivalent to θ(0) = 1, 1−θ (k+1)\n(θ(k+1))2 = 1 (θ(k))2 and θ(k) > 0 in\nLemma 4. Such a property will be used in the following analysis several times. The anaysis of our algorithms is based on the following property:\nLemma 5. x̃ is an optimal solution to (37) if and only if there exists α > 0, such that\nf(x̃)− f(x∗) + 〈λ∗,A(x̃)− b〉+ α 2 ‖A(x̃)− b‖2 = 0. (39)\nProposition 3. In Algorithm 3, for any x, we have\n1− θ(k+1) (θ(k+1))2 ( f(xk+1)− f(x) ) − 1 θ(k) 〈 AT (λk+1),x− zk+1 〉 ≤ 1− θ (k)\n(θ(k))2 ( f(xk)− f(x) ) + L 2 ( ‖zk − x‖2 − ‖zk+1 − x‖2 ) . (46)\nProof. From the optimality of zk+1 to (41), we have\n0 ∈ ∂h(zk+1) +∇g(yk+1) +AT (λk) + β(k)AT (A(zk+1)− b) + Lθ(k)(zk+1 − zk) = ∂h(zk+1) +∇g(yk+1) +AT (λk+1) + Lθ(k)(zk+1 − zk), (47)\nwhere (47) uses (43). From the convexity of h, we have h(x)− h(zk+1) ≥ 〈 −∇g(yk+1)−AT (λk+1)− Lθ(k)(zk+1 − zk),x− zk+1 〉 . (48)\nInitialize: x0, z0, λ0, β(0) = θ(0) = 1. for k = 0, 1, 2, · · · do\nyk+1 = (1− θ(k))xk + θ(k)zk; (40) zk+1 = argmin\nx g(yk+1) +\n〈 ∇g(yk+1),x− yk+1 〉 + h(x)\n+ 〈 λk,A(x)− b 〉 + β(k)\n2 ‖A(x)− b‖2 + Lθ\n(k)\n2 ‖x− zk‖2; (41)\nxk+1 = (1− θ(k))xk + θ(k)zk+1; (42) λk+1 = λk + β(k)(A(zk+1)− b); (43)\nθ(k+1) = −(θ(k))2 +\n√ (θ(k))4 + 4(θ(k))2\n2 ; (44)\nβ(k+1) = 1\nθ(k+1) . (45)\nend Algorithm 3: Fast PALM Algorithm\nOn the other hand,\nf(xk+1) ≤ g(yk+1) + 〈 ∇g(yk+1),xk+1 − yk+1 〉 + L\n2 ‖xk+1 − yk+1‖2 + h(xk+1) (49) = g(yk+1) + 〈 ∇g(yk+1), (1− θ(k))xk + θ(k)zk+1 − yk+1 〉 + L\n2 ‖(1− θ(k))xk + θ(k)zk+1 − yk+1‖2 + h\n( (1− θ(k))xk + θ(k)zk+1 ) (50)\n≤ (1− θ(k)) ( g(yk+1) + 〈 ∇g(yk+1),xk − yk+1 〉 + h(xk) ) +θ(k) ( g(yk+1) + 〈 ∇g(yk+1), zk+1 − yk+1 〉 + h(zk+1) ) + L(θ(k))2\n2 ‖zk+1 − zk‖2 (51) = (1− θ(k)) ( g(yk+1) + 〈 ∇g(yk+1),xk − yk+1 〉 + h(xk) ) +θ(k) ( g(yk+1) + 〈 ∇g(yk+1),x− yk+1 〉 + 〈 ∇g(yk+1), zk+1 − x 〉 + h(zk+1)\n) + L(θ(k))2\n2 ‖zk+1 − zk‖2\n≤ (1− θ(k))f(xk) + θ(k) ( g(x) + 〈 ∇g(yk+1), zk+1 − x 〉 + h(zk+1) ) + L(θ(k))2\n2 ‖zk+1 − zk‖2 (52) ≤ (1− θ(k))f(xk) + θ(k) ( g(x) + h(x) + 〈 AT (λk+1) + Lθ(k)(zk+1 − zk),x− zk+1 〉) (53)\n+ L(θ(k))2\n2 ‖zk+1 − zk‖2\n= (1− θ(k))f(xk) + θ(k)f(x) + θ(k) 〈 AT (λk+1),x− zk+1 〉 − L(θ (k))2\n2\n( ‖zk+1 − x‖2 − ‖zk − x‖2 ) ,(54)\nwhere (49) uses (25), (50) uses (42), (51) is from the convexity of h, (52) is from the convexity of g, (53) uses (48) and (54) uses (26). Rerangging the above inequality leads to\n( f(xk+1)− f(x) ) − θ(k) 〈 AT (λk+1),x− zk+1 〉 ≤ (1− θ(k)) ( f(xk)− f(x) ) + L(θ(k))2\n2\n( ‖zk − x‖2 − ‖zk+1 − x‖2 ) . (55)\nDiving both sides of the above inequality by (θ(k))2 leads to\n1 (θ(k))2 ( f(xk+1)− f(x) ) − 1 θ(k) 〈 AT (λk+1),x− zk+1 〉 ≤ 1− θ (k)\n(θ(k))2 ( f(xk)− f(x) ) + L 2 ( ‖zk − x‖2 − ‖zk+1 − x‖2 ) .\nThe proof is completed by using the property of θ(k) in Lemma 4. Proposition 4. In Algorithm 3, the following result holds for any λ\n〈A(zk+1)− b,λ− λk+1〉+ β (k)\n2 ‖A(zk+1)− b‖2\n= 1 2β(k) ( ‖λk − λ‖2 − ‖λk+1 − λ‖2 ) (56)\nProof. By using (43) and (26), we have\n〈A(zk+1)− b,λ− λk+1〉\n= 1\nβ(k) 〈λk+1 − λk,λ− λk+1〉\n= 1 2β(k) ( ‖λk − λ‖2 − ‖λk+1 − λ‖2 − ‖λk+1 − λk‖2 ) = 1\n2β(k) ( ‖λk − λ‖2 − ‖λk+1 − λ‖2 ) − β (k) 2 ‖A(zk+1)− b‖2.\nThe proof is completed. Theorem 4. In Algorithm 3, for any K > 0, we have\nf(xK+1)− f(x∗) + 〈 λ∗,A(xK+1)− b 〉 + 1\n2 ‖A(xK+1)− b‖2 (57)\n≤ 2 (K + 2)2\n( LD2x∗ +D 2 λ∗ ) . (58)\nProof. Let x = x∗ and λ = λ∗ in (46) and (56). We have\n1− θ(k+1) (θ(k+1))2 ( f(xk+1)− f(x∗) ) − 1− θ (k) (θ(k))2 ( f(xk)− f(x∗) ) + 1 θ(k) 〈 λ∗,A(zk+1)− b 〉 (59)\n≤ L 2\n( ‖zk − x∗‖2 − ‖zk+1 − x∗‖2 ) + 1\nθ(k) 〈λ∗ − λk+1,A(zk+1)− b〉 (60)\n≤ L 2\n( ‖zk − x∗‖2 − ‖zk+1 − x∗‖2 ) +\n1 2θ(k)β(k) ( ‖λk − λ∗‖2 − ‖λk+1 − λ∗‖2 ) − β (k) 2θ(k) ‖A(zk+1)− b‖2 (61)\n= L\n2\n( ‖zk − x∗‖2 − ‖zk+1 − x∗‖2 ) + 1\n2\n( ‖λk − λ∗‖2 − ‖λk+1 − λ∗‖2 ) − 1\n2(θ(k))2 ‖A(zk+1)− b‖2, (62)\nwhere (60) uses the fact A(x∗) = b, (61) uses (56) and (62) uses β(k) = 1 θ(k) . Summing (59)-(62) from k = 0 to K, we have\n1− θ(K+1) (θ(K+1))2 ( f(xK+1)− f(x∗) ) − 1− θ (0) (θ(0))2 ( f(x0)− f(x∗) ) + K∑ k=0 1 θ(k) 〈 λ∗,A(zk+1)− b 〉 (63)\n≤ L 2 ‖z0 − x∗‖2 + 1 2 ‖λ0 − λ∗‖2 − K∑ k=0\n1\n2(θ(k))2 ‖A(zk+1)− b‖2\n≤ L 2 ‖z0 − x∗‖2 + 1 2 ‖λ0 − λ∗‖2 − K∑ k=0 1 2θ(k) ‖A(zk+1)− b‖2, (64)\nwhere (64) uses (33). Also note that θ(0) = 1. So the second term of (63) disappears.\nOn the other hand, by the property of θ(k) in Lemma 4 and (42), we have\nK∑ k=0 zk+1 θ(k)\n= K∑ k=0 ( 1 (θ(k))2 xk+1 − 1− θ (k) (θ(k))2 xk )\n= K∑ k=0 ( 1− θ(k+1) (θ(k+1))2 xk+1 − 1− θ (k) (θ(k))2 xk )\n= 1− θ(K+1) (θ(K+1))2 xK+1 − 1− θ (0) (θ(0))2 x0\n= 1− θ(K+1)\n(θ(K+1))2 xK+1\n= 1\n(θ(K))2 xK+1. (65)\nSo\nK∑ k=0 1 θ(k) 〈 λ∗,A(zk+1)− b 〉 =\n1 (θ(K))2 〈 λ∗,A(xK+1)− b 〉 . (66)\nBy the convexity of ‖ · ‖2, we have\nK∑ k=0 1 2θ(k) ‖A(zk+1)− b‖2\n= 1\n2(θ(K))2 K∑ k=0 θ(K))2 θ(k) ‖A(zk+1)− b‖2\n≥ 1 2(θ(K))2 ∥∥∥∥∥(θ(K))2A ( K∑ k=0 zk+1 θ(k) ) − b ∥∥∥∥∥ 2\n(67)\n= 1\n2(θ(K))2 ‖A(xK+1)− b‖2, (68)\nwhere (67) uses (30) and (68) uses (65). Substituting (66) into (63) and (68) into (64) respectively, we obtain\n1− θ(K+1) (θ(K+1))2 ( f(xK+1)− f(x∗) ) +\n1 (θ(K))2 〈 λ∗,A(xK+1)− b 〉 +\n1\n2(θ(K))2 ‖A(xK+1)− b‖2 (69)\n≤ L 2 ‖z0 − x∗‖2 + 1 2 ‖λ0 − λ∗‖2. (70)\nMultiplying (69) and (70) by (θ(K))2 and using (32) leads to\nf(xK+1)− f(x∗) + 〈 λ∗,A(xK+1)− b 〉 + 1\n2 ‖A(xK+1)− b‖2\n≤ 2 (K + 2)2\n( L‖z0 − x∗‖2 + ‖λ0 − λ∗‖2 ) = 2\n(K + 2)2 ( LD2x∗ +D 2 λ∗ ) .\nThe proof is completed.\nInitialize: x0, z0, λ0, θ(0) = 1, fix β(k) = β for k ≥ 0, ηi > n‖Ai‖2, i = 1, · · · , n, for k = 0, 1, 2, · · · do\n//Update yi, zi, xi, i = 1, · · · , n, in parallel by\nyk+1i = (1− θ (k))xki + θ (k)zki ; (72)\nzk+1i = argmin xi\n〈 ∇gi(yk+1i ),xi 〉 + hi(xi) + 〈 λk,Ai(xi) 〉 + 〈 β(k)ATi ( A(zk)− b ) ,xi 〉 + L(gi)θ\n(k) + β(k)ηi 2 ‖xi − zki ‖2; (73)\nxk+1i = (1− θ (k))xki + θ (k)zk+1i ; (74) λk+1 = λk + βk ( A(zk+1)− b ) ; (75)\nθ(k+1) = −(θ(k))2 +\n√ (θ(k))4 + 4(θ(k))2\n2 . (76)\nend Algorithm 4: Fast PL-ADMM-PS Algorithm\nConvergence Analysis of Fast PL-ADMM-PS\nIn this section, we give the convergence analysis of Fast PL-ADMM-PS for solving the following problem\nmin x1,··· ,xn n∑ i=1 fi(xi), s.t. n∑ i=1 Ai(xi) = b, (71)\nwhere fi(xi) = gi(xi) + hi(xi), both gi and hi are convex, and gi ∈ C1.1. The whole procedure of Fast PL-ADMM-PS is shown in Algorithm 4.\nProposition 5. In Algorithm 4, for any xi, we have\n1− θ(k+1) (θ(k+1))2 ( fi(x k+1 i )− fi(xi) ) − 1 θ(k) 〈 ATi (λ̂k+1),xi − zk+1i 〉 ≤ 1− θ (k)\n(θ(k))2 ( fi(x k i )− fi(xi) ) + Li 2 ( ‖zki − xi‖2 − ‖zk+1i − xi‖ 2 )\n+ β(k)ηi 2θ(k)\n( ‖zki − xi‖2 − ‖zk+1i − xi‖ 2 − ‖zk+1i − z k i ‖2 ) , (77)\nwhere\nλ̂ k+1 = λk + β(k) ( A(zk)− b ) . (78)\nProof. From the optimality of zk+1i to (73), we have\n0 ∈ ∂hi(zk+1i ) +∇gi(y k+1 i ) +A T i (λ k) + β(k)ATi (A(zk)− b) + (Liθ(k) + β(k)ηi)(zk+1i − z k i )\n= ∂hi(z k+1 i ) +∇gi(y k+1 i ) +A T i (λ̂ k+1) + (Liθ (k) + β(k)ηi)(z k+1 i − z k i ), (79)\nwhere (79) uses (78). From the convexity of hi, we have\nhi(xi)− hi(zk+1i ) ≥ 〈 −∇gi(yk+1i )−A T i (λ̂ k+1)− (Liθ(k) + β(k)ηi)(zk+1i − z k i ),xi − zk+1i 〉 . (80)\nOn the other hand,\nfi(x k+1 i ) ≤ gi(y k+1 i ) + 〈 ∇gi(yk+1i ),x k+1 i − y k+1 i 〉 + Li 2 ‖xk+1i − y k+1 i ‖ 2 + hi(x k+1 i ) (81)\n= gi(y k+1 i ) + 〈 ∇gi(yk+1i ), (1− θ (k))xki + θ (k)zk+1i − y k+1 i 〉 + Li 2 ‖(1− θ(k))xki + θ(k)zk+1i − y k+1 i ‖ 2 + hi ( (1− θ(k))xki + θ(k)zk+1i ) (82)\n≤ (1− θ(k)) ( gi(y k+1 i ) + 〈 ∇gi(yk+1i ),x k i − yk+1i 〉 + hi(x k i ) )\n+θ(k) ( gi(y k+1 i ) + 〈 ∇gi(yk+1i ), z k+1 i − y k+1 i 〉 + hi(z k+1 i ) ) + Li(θ (k))2\n2 ‖zk+1i − z k i ‖2 (83) = (1− θ(k)) ( gi(y k+1 i ) + 〈 ∇gi(yk+1i ),x k i − yk+1i 〉 + hi(x k i ) )\n+θ(k) ( gi(y k+1 i ) + 〈 ∇gi(yk+1i ),xi − y k+1 i 〉 + 〈 ∇gi(yk+1i ), z k+1 i − xi 〉 + hi(z k+1 i ) ) + Li(θ (k))2\n2 ‖zk+1i − z k i ‖2 (84)\n≤ (1− θ(k))fi(xki ) + θ(k) ( gi(xi) + 〈 ∇gi(yk+1i ), z k+1 i − xi 〉 + hi(z k+1 i ) ) + Li(θ (k))2\n2 ‖zk+1i − z k i ‖2 (85) ≤ (1− θ(k))fi(xki ) + θ(k) ( gi(xi) + hi(xi) + 〈 ATi (λ̂k+1) + (Liθ(k) + β(k)ηi)(zk+1i − z k i ),xi − zk+1i 〉) + Li(θ (k))2\n2 ‖zk+1i − z k i ‖2 (86) = (1− θ(k))fi(xki ) + θ(k)fi(xi) + θ(k) 〈 ATi (λ̂k+1),xi − zk+1i 〉 −Li(θ\n(k))2 + θ(k)β(k)ηi 2 ( ‖zk+1i − xi‖ 2 − ‖zki − xi‖2 + ‖zk+1i − z k i ‖2 ) + Li(θ (k))2 2 ‖zk+1i − z k i ‖2, (87)\nwhere (81) uses (1), (82) uses (74), (83) is from the convexity of hi, (85) is from the convexity of gi, (86) uses (80) and (87) uses (27). Rerangging the above inequality leads to(\nfi(x k+1 i )− fi(xi)\n) − θ(k) 〈 ATi (λ̂k+1),xi − zk+1i 〉 ≤ (1− θ(k)) ( fi(x k i )− fi(xi) ) + Li(θ (k))2\n2\n( ‖zki − xi‖2 − ‖zk+1i − xi‖ 2 )\n+ θ(k)β(k)ηi\n2\n( ‖zki − xi‖2 − ‖zk+1i − xi‖ 2 − ‖zk+1i − z k i ‖2 )\n(88)\nDividing both sides of the above inequality by (θ(k))2 leads to\n1 (θ(k))2 ( fi(x k+1 i )− fi(xi) ) − 1 θ(k) 〈 ATi (λ̂k+1),xi − zk+1i 〉 ≤ 1− θ (k)\n(θ(k))2 ( fi(x k i )− fi(xi) ) + Li 2 ( ‖zki − xi‖2 − ‖zk+1i − xi‖ 2 )\n+ β(k)ηi 2θ(k)\n( ‖zki − xi‖2 − ‖zk+1i − xi‖ 2 − ‖zk+1i − z k i ‖2 )\nThe proof is completed by using 1−θ (k+1)\n(θ(k+1))2 = 1 (θ(k))2 .\nProposition 6. In Algorithm 4, the following result holds for any λ\n〈A(zk+1)− b,λ− λ̂k+1〉+ β (k)α\n2 ‖A(zk+1)− b‖2\n≤ 1 2β(k)\n( ‖λk − λ‖2 − ‖λk+1 − λ‖2 ) + β(k)\n2 n∑ i=1 ηi‖zk+1i − z k i ‖2, (89)\nwhere α = min {\n1 n+1 , { ηi−n‖Ai‖2 (n+1)‖Ai‖2 , i = 1, · · · , n }} .\nProof. By using (75) and (27), we have\n〈A(zk+1)− b,λ− λ̂k+1〉\n= 1\nβ(k) 〈λk+1 − λk,λ− λ̂k+1〉\n= 1 2β(k) ( ‖λ− λk‖2 − ‖λ− λk+1‖2 ) − 1 2β(k) ( ‖λ̂k+1 − λk‖2 − ‖λk+1 − λ̂k+1‖2 ) . (90)\nNow, consider the last two terms in the above inequality. We deduce\n1\n2β(k)\n( ‖λ̂k+1 − λk‖2 − ‖λk+1 − λ̂k+1‖2 ) = β(k)\n2 ∥∥∥∥∥ n∑ i=1 Ai(zki )− b ∥∥∥∥∥ 2 − ∥∥∥∥∥ n∑ i=1 Ai(zk+1i − z k i ) ∥∥∥∥∥ 2 \n≥ β (k)\n2 ∥∥∥∥∥ n∑ i=1 Ai(zki )− b ∥∥∥∥∥ 2 − n∑ i=1 n‖Ai‖2‖zk+1i − z k i ‖2 \n= β(k)\n2 ∥∥∥∥∥ n∑ i=1 Ai(zki )− b ∥∥∥∥∥ 2 + n∑ i=1 ηi − n‖Ai‖2 ‖Ai‖2 ‖Ai‖2‖zk+1i − z k i ‖2 − n∑ i=1 ηi‖zk+1i − z k i ‖2 \n≥ β (k)\n2 α(n+ 1) ∥∥∥∥∥ n∑ i=1 Ai(zki )− b ∥∥∥∥∥ 2 + n∑ i=1 ‖Ai(zk+1i − z k i )‖2 − n∑ i=1 ηi‖zk+1i − z k i ‖2 \n≥ β (k)\n2 α ∥∥∥∥∥ n∑ i=1 Ai(zk+1i )− b ∥∥∥∥∥ 2 − n∑ i=1 ηi‖zk+1i − z k i ‖2  (91)\n= β(k)α 2 ‖A(zk+1)− b‖2 − β (k) 2 n∑ i=1 ηi‖zk+1i − z k i ‖2 (92)\nThe proof is completed by substituting (92) into (90).\nTheorem 5. In Algorithm 4, for any K > 0, we have\nf(xK+1)− f(x∗) + 〈 λ∗,A(xK+1)− b 〉 + βα\n2 ∥∥A(xK+1)− b∥∥2 ≤ 2LmaxD 2 x∗\n(K + 2)2 +\n2βηmaxD 2 X\nK + 2 + 2D2Λ β(K + 2) ,\n(93)\nwhere α = min {\n1 n+1 ,\n{ ηi−n‖Ai‖2 2(n+1)‖Ai‖2 , i = 1, · · · , n }} , Lmax = max{Li, i = 1, · · · , n} and ηmax = max{ηi, i = 1, · · · , n}.\nProof. Let xi = x∗i and λ = λ ∗ in (77) and (89). We have\n1− θ(k+1)\n(θ(k+1))2 n∑ i=1 ( fi(x k+1 i )− fi(x ∗ i ) ) − 1− θ (k) (θ(k))2 n∑ i=1 ( fi(x k i )− fi(x∗i ) ) + 1 θ(k) n∑ i=1 〈 λ∗,Ai(zk+1i )− b 〉 (94)\n≤ 1 2 n∑ i=1 Li ( ‖zki − x∗i ‖2 − ‖zk+1i − x ∗ i ‖2 ) + β(k) 2θ(k) n∑ i=1 ηi ( ‖zki − x∗i ‖2 − ‖zk+1i − x ∗ i ‖2 − ‖zk+1i − z k i ‖2 )\n1\nθ(k) 〈λ∗ − λ̂k+1,A(zk+1)− b〉 (95)\n≤ 1 2 n∑ i=1 Li ( ‖zki − x∗i ‖2 − ‖zk+1i − x ∗ i ‖2 ) + β(k) 2θ(k) n∑ i=1 ηi ( ‖zki − x∗i ‖2 − ‖zk+1i − x ∗ i ‖2 − ‖zk+1i − z k i ‖2 )\n+ 1 2θ(k)β(k) ( ‖λk − λ∗‖2 − ‖λk+1 − λ∗‖2 ) + β(k) 2θ(k) n∑ i=1 ηi‖zk+1i − z k i ‖2 − β(k)α 2θ(k) ‖A(zk+1)− b‖2 (96)\n= 1\n2 n∑ i=1 Li ( ‖zki − x∗i ‖2 − ‖zk+1i − x ∗ i ‖2 ) + β(k) 2θ(k) n∑ i=1 ηi ( ‖zki − x∗i ‖2 − ‖zk+1i − x ∗ i ‖2 )\n+ 1 2θ(k)β(k) ( ‖λk − λ∗‖2 − ‖λk+1 − λ∗‖2 ) − β (k)α 2θ(k) ‖A(zk+1)− b‖2 (97)\nwhere (95) uses the fact A(x∗) = b and (96) uses (89). Summing (94)-(97) from k = 0 to K and fixing β(k) = β > 0, we have\n1− θ(K+1)\n(θ(K+1))2 n∑ i=1 ( fi(x K+1 i )− fi(x ∗ i ) ) − 1− θ (0) (θ(0))2 n∑ i=1 ( fi(x 0 i )− fi(x∗i ) ) + K∑ k=0 1 θ(k) 〈 λ∗,A(zk+1)− b 〉 (98)\n≤ 1 2 n∑ i=1 Li‖z0i − x∗i ‖2 + K∑ k=0 1 2θ(k) n∑ i=1 βηi ( ‖zki − x∗i ‖2 − ‖zk+1i − x ∗ i ‖2 )\n+ K∑ k=0 1 2θ(k)β ( ‖λk − λ∗‖2 − ‖λk+1 − λ∗‖2 ) − K∑ k=0 βα 2θ(k) ‖A(zk+1)− b‖2\n≤ 1 2 n∑ i=1 Li‖z0i − x∗i ‖2 + 1 2 K∑ k=0 n∑ i=1 βηi‖zki − x∗i ‖2 + 1 2β K∑ k=0 ‖λk − λ∗‖2 − K∑ k=0 βα 2θ(k) ‖A(zk+1)− b‖2, (99)\n≤ 1 2\n( LmaxD 2 x∗ +KβηmaxD 2 X + 1\nβ KDΛ − βα K∑ k=0 1 θ(k) ‖A(zk+1)− b‖2\n) (100)\nwhere (99) uses (28). Also note that θ(0) = 1. So the second term of (98) disappears.\nNote that (66) and (68) also holds here. Substituting (66) into (98) and (68) into (100) respectively and using 1−θ (K+1)\n(θ(K+1))2 =\n1 (θ(K))2 , we obtain\n1\n(θ(K))2 n∑ i=1 ( fi(x K+1 i )− fi(x ∗ i ) ) + 1 (θ(K))2 〈 λ∗,A(xK+1)− b 〉 +\nβα\n2(θ(K))2 ‖A(xK+1)− b‖2\n≤ 1 2\n( LmaxD 2 x∗ +KβηmaxD 2 X + 1\nβ KDΛ\n) .\nThe proof is completed by multiplying both sides of the above inequality with θ(K) and using (32). Theorem 6. Assume the mapping A(x1, · · · ,xn) = ∑n i=1Ai(xi) is onto?, the sequence {zk} is bounded, ∂h(x) and ∇g(x) are bounded if x is bounded, then {xk}, {yk} and {λk} are bounded.\n?This assumption is equivalent to that the matrix A ≡ (A1, · · · , An) is of full row rank, where Ai is the matrix representation of Ai.\nProof. Assume ‖zk‖ ≤ C1 for all k and ‖x0‖ ≤ C1. Then from (74) we can easily get ‖xk‖ ≤ C1 for all k. Then from (72) we have ‖yk‖ ≤ C1 for all k. Assume ‖∂h(x)‖ ≤ C2 and ‖∇g(x)‖ ≤ C2 if ‖x‖ ≤ C1. Then from (79), we have\n0 ∈ ∂h(zk+1) +∇g(yk+1) +AT (λk) + β(k)AT (A(zk)− b) +  (L1θ (k) + β(k)η1)(z k+1 1 − zk1) ... (Liθ (k) + β(k)ηi)(z k+1 i − zki )\n... (Lnθ (k) + β(k)ηn)(z k+1 n − zkn)\n ,\nand\n−AAT (λk) ∈ A  ∂h(zk+1) +∇g(yk+1) + β(k)AT (A(zk)− b) +  (L1θ (k) + β(k)η1)(z k+1 1 − zk1) ... (Liθ (k) + β(k)ηi)(z k+1 i − zki )\n... (Lnθ (k) + β(k)ηn)(z k+1 n − zkn)\n  .\nSo we have\n‖λk‖ ≤ ∥∥∥∥∥∥∥∥∥∥∥∥∥ (AAT )−1A  ∂h(zk+1) +∇g(yk+1) + β(k)AT (Azk − b) +  (L1θ (k) + β(k)η1)(z k+1 1 − zk1) ... (Liθ (k) + β(k)ηi)(z k+1 i − zki ) ... (Lnθ (k) + β(k)ηn)(z k+1 n − zkn)   ∥∥∥∥∥∥∥∥∥∥∥∥∥ ≤ ‖(AAT )−1A‖ ( ‖∂h(zk+1)‖+ ‖∇g(yk+1)‖+ ‖β(k)ATAzk‖+ ‖β(k)ATb‖+ (Lmaxθ(k) + β(k)ηmax)(‖zk+1‖+ ‖zk‖)\n) ≤ ‖(AAT )−1A‖ ( 2C2 + β (k)‖ATA‖C1 + β(k)‖ATb‖+ 2(Lmaxθ(k) + β(k)ηmax)C1 ) .\nfor all k."
    } ],
    "references" : [ {
      "title" : "and Sra",
      "author" : [ "S. Azadi" ],
      "venue" : "S.",
      "citeRegEx" : "Azadi and Sra 2014",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "and Teboulle",
      "author" : [ "A. Beck" ],
      "venue" : "M.",
      "citeRegEx" : "Beck and Teboulle 2009",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Distributed optimization and statistical learning via the alternating direction method of multipliers. Foundations and Trends® in Machine Learning 3(1):1–122",
      "author" : [ "Boyd" ],
      "venue" : null,
      "citeRegEx" : "Boyd,? \\Q2011\\E",
      "shortCiteRegEx" : "Boyd",
      "year" : 2011
    }, {
      "title" : "and Recht",
      "author" : [ "E. Candès" ],
      "venue" : "B.",
      "citeRegEx" : "Candès and Recht 2009",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "X",
      "author" : [ "Candès, E.J.", "Li" ],
      "venue" : "D.; Ma, Y.; and Wright, J.",
      "citeRegEx" : "Candès et al. 2011",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "A",
      "author" : [ "V. Chandrasekaran", "P.A. Parrilo", "Willsky" ],
      "venue" : "S.",
      "citeRegEx" : "Chandrasekaran. Parrilo. and Willsky 2012",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "H",
      "author" : [ "J. Douglas", "Rachford" ],
      "venue" : "H.",
      "citeRegEx" : "Douglas and Rachford 1956",
      "shortCiteRegEx" : null,
      "year" : 1956
    }, {
      "title" : "and Vidal",
      "author" : [ "E. Elhamifar" ],
      "venue" : "R.",
      "citeRegEx" : "Elhamifar and Vidal 2013",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "P",
      "author" : [ "Georghiades, A.S.", "Belhumeur" ],
      "venue" : "N.; and Kriegman, D.",
      "citeRegEx" : "Georghiades. Belhumeur. and Kriegman 2001",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Fast alternating direction optimization methods",
      "author" : [ "Goldstein" ],
      "venue" : "SIAM Journal on Imaging Sciences",
      "citeRegEx" : "Goldstein,? \\Q2014\\E",
      "shortCiteRegEx" : "Goldstein",
      "year" : 2014
    }, {
      "title" : "and Yuan",
      "author" : [ "B. He" ],
      "venue" : "X.",
      "citeRegEx" : "He and Yuan 2010",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "and Yuan",
      "author" : [ "B. He" ],
      "venue" : "X.",
      "citeRegEx" : "He and Yuan 2012",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "M",
      "author" : [ "Hestenes" ],
      "venue" : "R.",
      "citeRegEx" : "Hestenes 1969",
      "shortCiteRegEx" : null,
      "year" : 1969
    }, {
      "title" : "Group Lasso with overlap and graph Lasso",
      "author" : [ "Obozinski Jacob", "L. Vert 2009] Jacob", "G. Obozinski", "J.-P. Vert" ],
      "venue" : "In Proceedings of the 26th Annual International Conference on Machine Learning,",
      "citeRegEx" : "Jacob et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Jacob et al\\.",
      "year" : 2009
    }, {
      "title" : "Linearized alternating direction method with parallel splitting and adaptive penalty for separable convex programs in machine learning",
      "author" : [ "Liu Lin", "Z. Li 2014] Lin", "R. Liu", "H. Li" ],
      "venue" : "In Machine Learning",
      "citeRegEx" : "Lin et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Lin et al\\.",
      "year" : 2014
    }, {
      "title" : "Linearized alternating direction method with adaptive penalty for lowrank representation",
      "author" : [ "Liu Lin", "Z. Su 2011] Lin", "R. Liu", "Z. Su" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Lin et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Lin et al\\.",
      "year" : 2011
    }, {
      "title" : "and Yan",
      "author" : [ "G. Liu" ],
      "venue" : "S.",
      "citeRegEx" : "Liu and Yan 2011",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Robust and efficient subspace segmentation via least squares regression",
      "author" : [ "Lu" ],
      "venue" : null,
      "citeRegEx" : "Lu,? \\Q2012\\E",
      "shortCiteRegEx" : "Lu",
      "year" : 2012
    }, {
      "title" : "Correlation adaptive subspace segmentation by trace Lasso",
      "author" : [ "Lu" ],
      "venue" : null,
      "citeRegEx" : "Lu,? \\Q2013\\E",
      "shortCiteRegEx" : "Lu",
      "year" : 2013
    }, {
      "title" : "Online learning for matrix factorization and sparse coding",
      "author" : [ "Mairal" ],
      "venue" : null,
      "citeRegEx" : "Mairal,? \\Q2010\\E",
      "shortCiteRegEx" : "Mairal",
      "year" : 2010
    }, {
      "title" : "An accelerated linearized alternating direction method of multipliers",
      "author" : [ "Ouyang" ],
      "venue" : "SIAM Journal on Imaging Sciences",
      "citeRegEx" : "Ouyang,? \\Q2015\\E",
      "shortCiteRegEx" : "Ouyang",
      "year" : 2015
    }, {
      "title" : "and Malik",
      "author" : [ "J. Shi" ],
      "venue" : "J.",
      "citeRegEx" : "Shi and Malik 2000",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "Some parallel splitting methods for separable convex program-ming with O(1/t) convergence rate",
      "author" : [ "M. Tao" ],
      "venue" : "Pacific Journal of Optimization",
      "citeRegEx" : "Tao,? \\Q2014\\E",
      "shortCiteRegEx" : "Tao",
      "year" : 2014
    }, {
      "title" : "and Yuan",
      "author" : [ "J. Yang" ],
      "venue" : "X.",
      "citeRegEx" : "Yang and Yuan 2013",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Non-negative low rank and sparse graph for semi-supervised learning",
      "author" : [ "Zhuang" ],
      "venue" : null,
      "citeRegEx" : "Zhuang,? \\Q2012\\E",
      "shortCiteRegEx" : "Zhuang",
      "year" : 2012
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "References [Azadi and Sra 2014] Azadi, S.",
      "startOffset" : 11,
      "endOffset" : 31
    }, {
      "referenceID" : 1,
      "context" : "[Beck and Teboulle 2009] Beck, A.",
      "startOffset" : 0,
      "endOffset" : 24
    }, {
      "referenceID" : 3,
      "context" : "[Candès and Recht 2009] Candès, E.",
      "startOffset" : 0,
      "endOffset" : 23
    }, {
      "referenceID" : 4,
      "context" : "[Candès et al. 2011] Candès, E.",
      "startOffset" : 0,
      "endOffset" : 20
    }, {
      "referenceID" : 5,
      "context" : "[Chandrasekaran, Parrilo, and Willsky 2012] Chandrasekaran, V.",
      "startOffset" : 0,
      "endOffset" : 43
    }, {
      "referenceID" : 6,
      "context" : "[Douglas and Rachford 1956] Douglas, J.",
      "startOffset" : 0,
      "endOffset" : 27
    }, {
      "referenceID" : 7,
      "context" : "[Elhamifar and Vidal 2013] Elhamifar, E.",
      "startOffset" : 0,
      "endOffset" : 26
    }, {
      "referenceID" : 8,
      "context" : "[Georghiades, Belhumeur, and Kriegman 2001] Georghiades, A.",
      "startOffset" : 0,
      "endOffset" : 43
    }, {
      "referenceID" : 10,
      "context" : "[He and Yuan 2010] He, B.",
      "startOffset" : 0,
      "endOffset" : 18
    }, {
      "referenceID" : 11,
      "context" : "[He and Yuan 2012] He, B.",
      "startOffset" : 0,
      "endOffset" : 18
    }, {
      "referenceID" : 12,
      "context" : "[Hestenes 1969] Hestenes, M.",
      "startOffset" : 0,
      "endOffset" : 15
    }, {
      "referenceID" : 16,
      "context" : "[Liu and Yan 2011] Liu, G.",
      "startOffset" : 0,
      "endOffset" : 18
    }, {
      "referenceID" : 21,
      "context" : "[Shi and Malik 2000] Shi, J.",
      "startOffset" : 0,
      "endOffset" : 20
    }, {
      "referenceID" : 22,
      "context" : "[Tao 2014] Tao, M.",
      "startOffset" : 0,
      "endOffset" : 10
    }, {
      "referenceID" : 23,
      "context" : "[Yang and Yuan 2013] Yang, J.",
      "startOffset" : 0,
      "endOffset" : 20
    } ],
    "year" : 2015,
    "abstractText" : "The Augmented Lagragian Method (ALM) and Alternating Direction Method of Multiplier (ADMM) have been powerful optimization methods for general convex programming subject to linear constraint. We consider the convex problem whose objective consists of a smooth part and a nonsmooth but simple part. We propose the Fast Proximal Augmented Lagragian Method (Fast PALM) which achieves the convergence rate O(1/K), compared with O(1/K) by the traditional PALM. In order to further reduce the per-iteration complexity and handle the multi-blocks problem, we propose the Fast Proximal ADMM with Parallel Splitting (Fast PL-ADMM-PS) method. It also partially improves the rate related to the smooth part of the objective function. Experimental results on both synthesized and real world data demonstrate that our fast methods significantly improve the previous PALM and ADMM. Introduction This work aims to solve the following linearly constrained separable convex problem with n blocks of variables min x1,··· ,xn f(x) = n ∑",
    "creator" : "LaTeX with hyperref package"
  }
}