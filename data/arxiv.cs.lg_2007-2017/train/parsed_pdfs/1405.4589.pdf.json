{
  "name" : "1405.4589.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "A Parallel Way to Select the Parameters of SVM Based on the Ant Optimization Algorithm",
    "authors" : [ "Chao Zhang", "Hong-Cen Mei", "Hao Yang" ],
    "emails" : [ "zhch040200@gmail.com", "hongcenmei@yeah.net", "chongqingyanghao@yeah.net" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n40 5.\n45 89\nv2 [\ncs .N\nE ]\n2 0\nM ay\n2 01\nI. SUPPORT VECTOR CLASSIFICATION AND PARAMETERS\nSVM is based on the principle of structural risk minimization,using limited training samples to obtain the higher generalization ability of decision function. Suppose a sample set (xi, yi) , where i = 1, 2...N means the number of training samples, x∈R means the sample characteristics, y ∈ {+1,−1} means the sample classification. SVM Classification function:\ny = ωx+ b (ω means weight vector,b means setover)\nFunctional margin :\nγ1 = y(x+ b) = yf(x)\nfunctional margin is the minimum margin from hyperplane (ω, b) to T (xi, yi)\nGeometrical margin:\nγ2 = y(ωx+ b)\n‖ ω ‖ =\n|f(x)|\n‖ ω ‖\nWhen classifying a data point, the larger the margin is, the more credible the classification is. So to improve the credibility is to maximize the margin. The ω and b can be proportional scaled through the functional margin, thus the value of y = ωx + b can be any large. Such that it is not appropriate to maximize a value. But in the geometrical margin, when scaling the ω and b, the value of γ2 can’t be change. So it is appropriate to maximize a value. Slack variable: εi > 0 Using to allow the data to deviate from the hyperplane\nto a certain extent. Radial Basis Function kernel:\nk〈x1, x2〉 = e −\n‖x1−x2‖ 2\n2σ2\nMaximum margin classifier:\nMAXγ2 s.tγ1i ≥ γ1 − εi, i = 1, 2, 3...n\nLet: γ1 = 1,so\nMAX 1\n‖ ω ‖ + C\nn∑\ni=1\nεis.tγ1i ≥ γ1 − εi, i = 1, 2, 3...n\nThe constraints is associated with objective function through the Lagrange function:\nL(ω, b, α) = 1\n2 ‖ ω ‖\n2 −\nn∑\ni=1\nαi(yi((ωx+ b)− 1) + C\nn∑\ni=1\nεi\nLet: Θ(ω) = MAXL(ω, b, α)\nWhen all the constraints is contented, the Θ(ω) = 1 2 ‖ ω ‖2 is the value that we first want to minimized.So objective function:\nMINΘ(ω) = MINMAXL(ω, b, α)\nDual function:\nMINΘ(ω) = MAXMINL(ω, b, α)\nTo solve the problem, requiring:\n∂L ∂ω = 0 ⇒ ω =\nn∑\ni=1\nαixiyi\n∂L ∂b = 0 ⇒\nn∑\ni=1\nαiyi = 0\n∂L ∂εi = 0 means C − αi − γi = 0, i = 1, 2...n\nMAXMINL(ω, b, α) = 1\n2 ‖ ω ‖2 −\nn∑\ni=1\nαi(yi((ωx+ b)− 1)\n= MAX\nn∑\ni=1\nαi − 1\n2\nn∑\ni,j=1\nαiαjyiyjk〈x1, x〉\nThe minimized duel problem:\nMAX\nn∑\ni=1\nαi − 1\n2\nn∑\ni,j=1\nαiαjyiyjk〈x1, x〉\ns.t0 ≤ αi ≤ C, i = 1, 2, 3...n;\nn∑\ni=1\nαiyi = 0\nThe final classification function:\ny = ωx+ b\nn∑\ni=1\nαiyik〈x1, x〉+ b\nII. MODIFIED ANT COLONY OPTIMIZATION ALGORITHM\nDifferent from traditional problem of TCP, in this algorithm,the coordinate is used to represented the node. In a twodimensional rectangular coordinate system, the significance of X is defined as the significant digit of parameters C and δ, Y is varied from 0 to 10.[1] The significant digit of C are assumed to be the five, and the highest level of C is hundred’s place. Similarly, assume that the significant digit of δ are also assumed to be the five, and the highest level of δ is The Unit. To realize the ant colony optimization, we follow the steps below:\n1) Suppose there are m ants. Each ant k(k = 1∼m)has a one-dimensional array Path(k) which has n elements(n is the total significant digit of C and δ), its used to store the vertical coordinates y of each point which the ant k visited. 2) Set the loop time N=0 to Nmax. Initialize the each points pheromone concentration τ(x, y) = τ0(x = 0 ∼ n, y = 0 ∼ 9).Set x = 1. 3) Set y = 1 to n.Calculate the deflection probability Pk of each ant move to the node vertical line Lx.Then select the next point via roulette wheel and store the points vertical coordinates y to Pathk\nPk(x, y) = τα(x, y)ηβ(x, y)∑9 j=0 τ α(x, y)ηβ(x, y)\nτ(x, y) = ρτ(x, y) + ∆τ(x, y)\n∆τ(x, y) =\nm∑\nk=1\n∆τk(x, y)\n∆τ(x, y) =\n{ Q\n1−Acck ,if ant k visited the point(x,y) 0 ,others\nThe Acck mean the ant k s accuracy of cross-validation\nPk means the deflection probability point(x− 1, y) to (x, y)\nQ is a constant W:the weight coefficient;\nSet:the minimum acceptable accuracy; 4) Let :x = x + 1,if x ≤ n,turn to step 3;else turn to step 5. 5) Record this motion path Pathk calculate the mapping\ndata (c, δ).\n6) Make the training samples evenly divided into k mutually exclusive subsets of S1, S2, ..., Sk; 7) Calculate the cross-validation accuracy: a) Initialize i=1; b) Make the Si subset reserved for test sets, and\nset the rest as the training set, training SVM; c) Calculate the ith subsets Sample Classifi-\ncation Accuracy Acci,set i = i + 1,When i < k + 1, repeat the step b);\nd) Calculate the mean of k Sample Classification Accuracy Âcc :\nAcc = Right\nError +Right\nÂcc = ∑k i=1 Acci k\nRight: Correctly classified (+1) number of samples Error:Misclassification (-1) number of samples Q̂ :the mean of Sample Classification Accuracy\n8) Update the pheromone concentration τ(x, y) at each points by Âcc . Clear Path(ki); 9) Reset N = N + 1.When N ≤ Nmax , and the entire colony has not converged to follow the same path, then turn to step 3; N ≤ Nmax, and the entire colony substantially converged to follow the same path, then the algorithm ends. Take the last update of the path and its mapping data to (c, δ),that is the SVM parameters C, δ final optimization results.\nIn order to validate the algorithm,we do simulation experiment on matlab R2010b and PC with windows7 64-bit operating system, 4 G memory, core i5 processor. We divide the wine SVM data set into a sample set and a test set. Then we will have 90 training datas and 88 test datas. We set ACO parameter m=30, N=500, ρ=0.7, Q=100, α=1, β=1. In order to calculate the classification accuracy, we introduced the LIBSVM.[3].LIBSVM has gained wide popularity in machine learning and many other areas.[2]\n0 2 4 6 8 10 0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nc,sigma\ny\n(a) The best accuracy’s path\n0 2 4 6 8 10 0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n(b) All accuracy’s paths\nThe best accuracy is 95.4545% and the convergence accuracy is 86.3636% (76/88). The parameter c: 18.605, parameter σ: 0.6643\nIII. PARALLEL OPTIMIZE THE PARAMETERS\nThe Open Computing Language, is an open specification for heterogeneous computing released by the Khronos Group2 in 2008. It resembles the NVIDIA CUDA3 platform, but can\nbe considered as a superset of the latter, they basically differ in the following points[4]:\n• OpenCL is an open specification that is managed by a set of distinct representatives from industry, software development, academia and so forth.\n• OpenCL is meant to be implemented by any compute device vendor, whether they produce CPUs, GPUs, hybrid processors, or other accelerators such as digital signal processors (DSP) andfield-programmable gate arrays (FPGA).\n• OpenCL is portable across architectures, meaning that a parallel code written in OpenCL is guaranteed to correctly run on every other supporte device.\nIn this ant colony optimization algorithm, we have m ants and we loop it N max times. If we give m a large number, such as ten thousand or one hundred thousand, our update of node pheromone concentration will be more accurate and reliable, but we will spend more time. As we all know, every cycle, each ant’s access to nodes are unrelated with others,so we can parallel the ant access process. In this article, we use openCL to realize the parallel of ant[4].\nOpenCL kernel for the ant-based solution construction globalsize = numberants; visited[globalesize × numbernodes] = {−1};\nfor ( i=0 to n-1) do { sumprob = 0; for (j=0 to 9) do { selectionprob[globalid × numbernodes + j] = choiceinfo[i× n+ j];\nSumprob = sumprob+ selectionprob[globalid × numbernodes + j]; } j=0; p = selectionprob[globalid × numbernodes + j]; while p < random(0, sumprob) do {\nj=j+1; p = p+ selectionprob[globalid × numbernodes + j];\n} visited[globalid × numbernodes + i] = j;\n}\nTraining samples are divided into subset number subset on average and each subset has sample number samples. One subset(Si) see as test set and the other subsets (S1∼i, Si∼k) see as a training set(each subset has c samples), then according to the current parameters (c, δ)training the SVM, calculating error of K-flod cross validation.\nOpenCL kernel for sample classification accuracy groupsize = subset number; localsize = sample number;\nfor (i=0 to groupsize − 1) { Right = Error = Q = 0; for(j = 0 to localsize − 1) { f(x) = sign( ∑n i=1 αiyik〈groupid, localid〉+b);\nif f(x) = 1 Right++; esle Error ++;\n} Q = Q + Right Right+Error\n; } Q = Q/groupsize;\nIV. CONCLUSION\nThrough the ant colony optimization algorithm, we can find a satisfactory parameter of SVM, and the convergence accuracy can be guaranteed more than 85%. There are also many other ways to optimize parameters, such as Genetic algorithm (GA)[5], dynamic encoding algorithm [6] for handwritten digit recognition, Particle swarm optimization(PSO)[7]. We also can parallel Ant Colony Optimization,article [8] introduced a new way which parallel Ant Colony Optimization on Graphics Processing Units.Article [9] improving ant colony optimization algorithm for data clustering.\nACKNOWLEDGMENT\nREFERENCES\n[1] P. F. LIU Chun-bo, WANG Xian-fang, “Paramters selection and stimulation of support vector machines based on ant colony optimization algorithm,” J.Cent,South Univ, 2008.\n[2] C. chung Chang and C.-J. Lin, “Libsvm : a library for support vector machines,” Linux Journal, 2001.\n[3] C.-C. Chang and C.-J. Lin, “LIBSVM: A library for support vector machines,” vol. 2, pp. 1–27, 2011.\n[4] E. by Helio J.C. Barbosa, Ant Colony Optimization - Techniques and Applications. InTech, Chapters published February 20, 2013 under CC BY 3.0 license, ISBN 978-953-51-1001-9,203 pages.\n[5] J. L.-c. ZHENG Chun-hong, “Automatic parameters selection for SVM based on GA[C],” NJ:IEEE Press,2004:1869-1872.\n[6] Y. Park, S.-W. Kim, and H.-S. Ahn, “Support vector machine parameter tuning using dynamic encoding algorithm for handwritten digit recognition,” 2005.\n[7] X. Li, S. dong Yang, and J. xun Qi, “A new support vector machine optimized by improved particle swarm optimization and its application,” Journal of Central South University of Technology, vol. 13, pp. 568–572, 2006.\n[8] A. Delevacq, P. Delisle, and M. Gravel, “Parallel Ant Colony Optimization on Graphics Processing Units,” Journal of Parallel and Distributed Computing, vol. 73, 2013.\n[9] R. Tiwari, M. Husain, S. Gupta, and A. Srivastava, “Improving ant colony optimization algorithm for data clustering,” pp. 529–534, 2010."
    } ],
    "references" : [ {
      "title" : "Paramters selection and stimulation of support vector machines based on ant colony optimization algorithm",
      "author" : [ "P.F. LIU Chun-bo", "WANG Xian-fang" ],
      "venue" : "J.Cent,South Univ, 2008.",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Libsvm : a library for support vector machines",
      "author" : [ "C. chung Chang", "C.-J. Lin" ],
      "venue" : "Linux Journal, 2001.",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "LIBSVM: A library for support vector machines",
      "author" : [ "C.-C. Chang", "C.-J. Lin" ],
      "venue" : "vol. 2, pp. 1–27, 2011.",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Ant Colony Optimization - Techniques and Applications",
      "author" : [ "E. by Helio J.C. Barbosa" ],
      "venue" : "InTech, Chapters published February",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2013
    }, {
      "title" : "Automatic parameters selection for SVM based on GA[C",
      "author" : [ "J.L.-c. ZHENG Chun-hong" ],
      "venue" : "NJ:IEEE Press,2004:1869-1872.",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Support vector machine parameter tuning using dynamic encoding algorithm for handwritten digit recognition",
      "author" : [ "Y. Park", "S.-W. Kim", "H.-S. Ahn" ],
      "venue" : "2005.",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "A new support vector machine optimized by improved particle swarm optimization and its application",
      "author" : [ "X. Li", "S. dong Yang", "J. xun Qi" ],
      "venue" : "Journal of Central South University of Technology, vol. 13, pp. 568–572, 2006.",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Parallel Ant Colony Optimization on Graphics Processing Units",
      "author" : [ "A. Delevacq", "P. Delisle", "M. Gravel" ],
      "venue" : "Journal of Parallel and Distributed Computing, vol. 73, 2013.",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Improving ant colony optimization algorithm for data clustering",
      "author" : [ "R. Tiwari", "M. Husain", "S. Gupta", "A. Srivastava" ],
      "venue" : "pp. 529–534, 2010.",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2010
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "[1] The significant digit of C are assumed to be the five, and the highest level of C is hundred’s place.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 2,
      "context" : "[3].",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 1,
      "context" : "[2]",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 3,
      "context" : "be considered as a superset of the latter, they basically differ in the following points[4]:",
      "startOffset" : 88,
      "endOffset" : 91
    }, {
      "referenceID" : 3,
      "context" : "In this article, we use openCL to realize the parallel of ant[4].",
      "startOffset" : 61,
      "endOffset" : 64
    } ],
    "year" : 2014,
    "abstractText" : "A large number of experimental data shows that Support Vector Machine (SVM) algorithm has obvious a large advantages in text classification, handwriting recognition, image classification, bioinformatics, and some other fields. To some degree, the optimization of SVM depends on its kernel function and Slack variable, the determinant of which is its parameters δ and c in the classification function. That is to say, to optimize the SVM algorithm, the optimization of the two parameters play a huge role. Ant Colony Optimization (ACO) is optimization algorithm which simulate ants to find the optimal path. In the available literature, we mix the ACO algorithm and Parallel algorithm together to find a well parameters. Keyword: SVM, Parameters, ACO, OpenCL, Parallel I. SUPPORT VECTOR CLASSIFICATION AND PARAMETERS SVM is based on the principle of structural risk minimization,using limited training samples to obtain the higher generalization ability of decision function. Suppose a sample set (xi, yi) , where i = 1, 2...N means the number of training samples, x∈R means the sample characteristics, y ∈ {+1,−1} means the sample classification. SVM Classification function: y = ωx+ b (ω means weight vector,b means setover)",
    "creator" : "LaTeX with hyperref package"
  }
}