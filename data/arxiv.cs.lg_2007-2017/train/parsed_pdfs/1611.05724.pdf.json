{
  "name" : "1611.05724.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Unimodal Thompson Sampling for Graph–Structured Arms",
    "authors" : [ "Stefano Paladino", "Marcello Restelli", "Nicola Gatti" ],
    "emails" : [ "nicola.gatti}@polimi.it" ],
    "sections" : [ {
      "heading" : "Introduction",
      "text" : "Multi–Armed Bandit (MAB) algorithms (Auer, CesaBianchi, and Fischer 2002) have been proven to provide effective solutions for a wide range of applications fitting the sequential decisions making scenario. In this framework, at each round over a finite horizon T , the learner selects an action (usually called arm) from a finite set and observes only the reward corresponding to the choice she made. The goal of a MAB algorithm is to converge to the optimal arm, i.e., the one with the highest expected reward, while minimizing the loss incurred in the learning process and, therefore, its performance is measured through its expected regret, defined as the difference between the expected reward achieved by an oracle algorithm always selecting the optimal arm and the one achieved by the considered algorithm. We focus on the so–called Unimodal MAB (UMAB), introduced in (Combes and Proutiere 2014a), in which each arm corresponds to a node of a graph and each edge is associated with a relationship specifying which node of the edge gives the largest expected reward (providing thus a partial ordering over the arm space). Furthermore, from any node there is a path leading to the unique node with the maximum expected reward along which the expected reward is monotonically\nCopyright c© 2017, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\nincreasing. While the graph structure may be (not necessarily) known a priori by the UMAB algorithm, the relationship defined over the edges is discovered during the learning. In the present paper, we propose a novel algorithm relying on the Bayesian learning approach for a generic UMAB setting.\nModels presenting a graph structure have become more and more interesting in last years due to the spread of social networks. Indeed, the relationships among the entities of a social network have a natural graph structure. A practical problem in this scenario is the targeted advertisement problem, whose goal is to discover the part of the network that is interested in a given product. This task is heavily influenced by the graph structure, since in social networks people tend to have similar characteristics to those of their friends (i.e., neighbor nodes in the graph), therefore interests of people in a social network change smoothly and neighboring nodes in the graph look similar to each other (McPherson, Smith-Lovin, and Cook 2001; Crandall et al. 2008). More specifically, an advertiser aims at finding those users that maximize the ad expected revenue (i.e., the product between click probability and value per click), while at the same time reducing the amount of times the advertisement is presented to people not interested in its content.\nUnder the assumption of unimodal expected reward, the learner can move from low expected rewards to high ones just by climbing them in the graph, preventing from the need of a uniform exploration over all the graph nodes. This assumption reduces the complexity in the search for the optimal arm, since the learning algorithm can avoid to pull the arms corresponding to some subset of non– optimal nodes, reducing thus the regret. Other applications might benefit from this structure, e.g., recommender systems which aims at coupling items with those users are likely to enjoy them. Similarly, the use of the unimodal graph structure might provide more meaningful recommendations without testing all the users in the social network. Finally, notice that unimodal problems with a single variable, e.g., in sequential pricing (Jia and Mannor 2011), bidding in online sponsored search auctions (Edelman and Ostrovsky 2007) and single–peak preferences economics and voting settings (Mas-Collel, Whinston, and Green 1995), are graph–structured problems in which the graph is a line.\nFrequentist approaches for UMAB with graph structure\nar X\niv :1\n61 1.\n05 72\n4v 2\n[ cs\n.L G\n] 2\n2 N\nov 2\n01 6\nare proposed in (Jia and Mannor 2011) and (Combes and Proutiere 2014a). Jia and Mannor (2011) introduce the GLSE algorithm with a regret of order O( √ T log(T )). However, GLSE performs better than classical bandit algorithms only when the number of arms is Θ(T ). Combes and Proutiere (2014a) present the OSUB algorithm—based on KLUCB—achieving asymptotic regret ofO(log(T )) and outperforming GLSE in settings with a few arms. To the best of our knowledge, no Bayesian approach has been proposed for unimodal bandit settings, included the UMAB setting we study. However, it is well known that Bayesian MAB algorithms—the most popular is Thompson Sampling (TS)—usually suffer of same order of regret as the best frequentist one (e.g., in unstructured settings (Kaufmann, Korda, and Munos 2012)), but they outperform the frequentist methods in a wide range of problems (e.g., in bandit problems without structure (Chapelle and Li 2011) and in bandit problems with budget (Xia et al. 2015)). Furthermore, in problems with structure, the classical Thompson Sampling (not exploiting the problem structure) may outperform frequentist algorithms exploiting the problem structure. For this reason, in this paper we explore Bayesian approaches for the UMAB setting. More precisely, we provide the following original contributions:\n• we design a novel Bayesian MAB algorithm, called UTS and based on the TS algorithm;\n• we derive a tight upper bound over the pseudo–regret for UTS, which asymptotically matches the lower bound for the UMAB setting;\n• we describe a wide experimental campaign showing better performance of UTS in applicative scenarios than those of state–of–the–art algorithms, evaluating also how the performance of the algorithms (ours and of the state of the art) varies as the graph structure properties vary."
    }, {
      "heading" : "Related work",
      "text" : "Here, we mention the main works related to ours. Some works deal with unimodal reward functions in continuous armed bandit setting (Jia and Mannor 2011; Combes and Proutiere 2014b; Kleinberg, Slivkins, and Upfal 2008). In (Jia and Mannor 2011) a successive elimination algorithm, called LSE, is proposed achieving regret of O( √ T log T ). In this case, assumptions over the minimum local decrease and increase of the expected reward is required. Combes and Proutiere (2014b) consider stochastic bandit problems with a continuous set of arms and where the expected reward is a continuous and unimodal function of the arm. They propose the SP algorithm, based on the stochastic pentachotomy procedure to narrow the search space. Unimodal MABs on metric spaces are studied in (Kleinberg, Slivkins, and Upfal 2008).\nAn application–dependent solution to the recommendation systems which exploits the similarity of the graph in social network in targeted advertisement has been proposed in (Valko et al. 2014). Similar information has been considered in (Caron and Bhagat 2013) where the problem of cold–start users (i.e., new users) is studied. Another type\nof structure considered in sequential games is the one of monotonicity of the conversion rate in the price (Trovò et al. 2015). Interestingly, the assumptions of monotonicity and unimodality are orthogonal, none of them being a special case of the other, therefore the results for monotonic setting cannot be used in unimodal bandits. In (Alon et al. 2013; Mannor and Shamir 2011), a graph structure of the arm feedback in an adversarial setting is studied. More precisely, they assume to have correlation over rewards and not over the expected values of arms."
    }, {
      "heading" : "Problem Formulation",
      "text" : "A learner receives in input a finite undirected graph MAB settingG = (A,E), whose verticesA = {a1, . . . , aK} with K ∈ N correspond to the arms and an edge (aiaj) ∈ E exists only if there is a direct partial order relationship between the expected rewards of arms ai and aj . The leaner knows a priori the nodes and the edges (i.e., she knows the graph), but, for each edge, she does not know a priori which is the node of the edge with the largest expected reward (i.e., she does not know the ordering relationship). At each round t over a time horizon of T ∈ N the learner selects an arm ai and gains the corresponding reward xi,t. This reward is drawn from an i.i.d. random variable Xi,t (i.e., we consider a stochastic MAB setting) characterized by an unknown distribution Di with finite known support Ω ⊂ R (as customary in MAB settings, from now on we consider Ω ⊆ [0, 1]) and by unknown expected value µi := E[Xi,t]. We assume that there is a single optimal arm, i.e., there exists a unique arm ai∗ s.t. its expected value µi∗ = maxi µi and, for sake of notation, we denote µi∗ with µ∗.\nHere we analyze a graph bandit setting with unimodality property, defined as: Definition 1. A graph unimodal MAB (UMAB) settingG = (A,E) is a graph bandit setting G s.t. for each sub–optimal arm ai, i 6= i∗ it exists a finite path p = (i1 = i, . . . , im = i∗) s.t. µik < µik+1 and (aik , aik+1) ∈ E for each k ∈ {1, . . . ,m− 1}.\nThis definition assures that if one is able to identify a non– decreasing path in G of expected rewards, she be able to reach the optimum arm, without getting stuck in local optima. Note that the unimodality property implies that the graph G is connected and therefore we consider only connected graphs from here on.\nA policy U over a UMAB setting is a procedure able to select at each round t an arm ait by basing on the history ht, i.e., the sequence of past selected arms and past rewards gained. The pseudo–regretRT (U) of a generic policy U over a UMAB setting is defined as:\nRT (U) := Tµ ∗ − E [ T∑ t=1 Xit,t ] , (1)\nwhere the expected value E[·] is taken w.r.t. the stochasticity of the gained rewards Xit,t and of the policy U.\nLet us define the neighborhood of arm ai as N(i) := {j|(aiaj) ∈ E}, i.e., the set of each index j of the arm aj connected to the arm ai by an edge (aiaj) ∈ E. It has been\nshown in (Combes and Proutiere 2014a) that the problem of learning in a UMAB setting presents a lower bound over the regret RT (U) of the following form:\nTheorem 1. Let U be a uniformly good policy, i.e., a policy s.t. RT (U) = o(T c) for each c > 0. Given a UMAB setting G = (A,E) we have:\nlim inf T→∞\nRT (U) log(T ) = ∑ i∈N(i∗) µ∗ − µi KL(µi, µ∗)\n(2)\nwhere KL(p, q) = p log ( p q ) + (1− p) log ( 1−p 1−q ) , i.e., the Kullaback–Leibler divergence of two Bernoulli distributions with means p and q, respectively.\nThis result is similar to the one provided in (Lai and Robbins 1985), with the only difference that the summation is restricted to the arms laying in the neighborhood of the optimal arm N(i∗) and reduces to it when the optimal arm is connected to all the others (i.e., N(i∗) ≡ {1, . . . ,K}) or the graph is completely connected (i.e., N(i) ≡ {1, . . . ,K},∀i). We would like to point out that by relying on the assumption of having a single maximum of the expected rewards, we also assure that the optimal arm neighborhoodN(i∗) is uniquely defined and, thus, the lower bound inequality in Equation 2 is well defined."
    }, {
      "heading" : "The UTS algorithm",
      "text" : "We describe the UTS algorithm and we show that its regret is asymptotically optimal, i.e., it asymptotically matches the lower bound of Theorem 1. The algorithm is an extension of the Thompson Sampling (Thompson 1933) that exploits the graph structure and the unimodal property of the UMAB setting. Basically, the rationale of the algorithm is to apply a simple variation of the TS algorithm to only the arms associated with the nodes that compose the neighborhood of the arm with the highest empirical mean reward, called leader."
    }, {
      "heading" : "The UTS pseudo–code",
      "text" : "Algorithm 1 UTS 1: Input: UMAB setting G = (V,E), Horizon T , Priors {πi}Ki=1\n2: for t ∈ {1, . . . , T} do 3: Compute µ̂i,Ti,t for each i ∈ {1, . . . ,K} 4: Find the leader al(t) 5: if Ll(t),t mod |N+(l(t))| = 0 then 6: Collect reward xl(t),t 7: else 8: Draw θi,t from πi,t for each i ∈ N+(l(t)) 9: Collect reward xit,t where it = arg maxi θi,t\nThe pseudo–code of the UTS algorithm is presented in Algorithm 1. The algorithm receives in input the graph structure G, the time horizon T , and a Bayesian prior πi for each expected reward µi. At each round t, the algorithm computes\nthe empirical expected reward for each arm (Line 3):\nµ̂i,t :=  Sit Ti,t if Ti,t > 0\n0 otherwise ,\nwhere Si,t = ∑t−1 h=1Xi,h1{U(h) = ai} is the cumulative\nreward of arm ai up to round t and Ti,t = ∑t−1 h=1 1{U(h) = ai} is the number of times the arm ai has been pulled up to round t.1 After that, UTS selects the arm denoted as the leader al(t) for round t, i.e., the one having the maximum empirical expected reward:\nal(t) = arg max ai∈A µ̂i,t. (3)\nOnce the leader has been chosen, we restrict the selection procedure to it and its neighborhood, considering only arms with indexes in N+(l(t)) := N(l(t)) ∪ {l(t)}. Denote with Li,t := ∑t−1 h=1 1{l(h) = i} the number of times the arm ai has been selected as leader before round t. If Ll(t),t is a multiple of |N+(l(t))|, then the leader is pulled and reward xl(t),t is gained (Line 6).2 Otherwise, the TS algorithm is performed over arms ai s.t. i ∈ N+(l(t)) (Lines 8–9).\nBasically, under the assumption of having a prior πi, we can compute the posterior distribution πi,t for µi after t rounds, using the information gathered from the rounds in which ai has been pulled. We denote with θi,t a sample drawn from πi,t, called Thompson sample. For instance, for Bernoulli rewards and by assuming uniform priors we have that πi,t = Beta(1+Si,t, 1+Ti,t−Si,t), whereBeta(α, β) is the beta distribution with parameters α and β. Finally, the UTS algorithm pulls the arm with the largest Thompson sample θi,n and collects the corresponding reward xit,t. See (Kaufmann, Korda, and Munos 2012) for further details. Remark 1. Assuming that the UTS algorithm receives in input the whole graph G is unnecessary. The algorithm just requires an oracle that, at each round t, is able to return the neighborhood N(l(t)) of the arm which is currently the leader al(t). This is crucial in all the applications in which the graph is discovered by means of a series of queries and the queries have a non–negligible cost (e.g., in social networks a query might be computationally costly). Finally, we remark that the frequentist counterpart of our algorithm (i.e., the OSUB algorithm) requires the computation of the maximum node degree γ := maxi |N(i)|, thus requiring at least an initial analysis of the entire graph G."
    }, {
      "heading" : "Finite–time analysis of UTS",
      "text" : "Theorem 2. Given a UMAB setting G = (A,E), the expected pseudo–regret of the UTS algorithm satisfies, for every ε > 0:\nRT (UTS) ≤ (1 + ε) ∑\ni∈N(i∗)\nµ∗ − µi KL(µi, µ∗) [log(T ) + log log(T )] + C̃,\nwhere C̃ > 0 is a constant depending on ε, the number of arms K and the expected rewards {µ1, . . . , µK}.\n1We here denote with 1{·} the indicator function. 2We here denote with | · | the cardinality operator.\nSketch of proof. (The complete version of the proof is reported in the appendices.) At first, we remark that a straightforward application of the proof provided for OSUB is not possible in the case of UTS. Indeed, the use of frequentist upper bounds over the expected reward in OSUB implies that in finite time and with high probability the bounds are ordered as the expected values. Since we are using a Bayesian algorithm, we would require the same assurance over the Thompson samples θi,t, but we do not have a direct bound over P(θi,t > θi′,t) where ai′ is the optimal arm in the neighborhoodN+(i). This fact requires to follow a completely different strategy when we analyze the case in which the leader is not the optimal arm.\nThe regret of the UTS algorithmRT (UTS) can be divided in two parts: the one obtained during those rounds in which the optimal arm a∗ is the leader, called R1, and the summation of the regrets in the rounds in which the leader is the arm ai 6= a∗, calledRi.R1 is obtained when i∗ is the leader, thus, the UTS algorithms behaves like Thompson Sampling restricted to the optimal arm and its neighborhood N+(i∗), and the regret upper bound is the one derived in (Kaufmann, Korda, and Munos 2012) for the TS algorithm. Ri is upper bounded by the expected number of rounds the arm ai has been selected as leader E[Li,T ] over the horizon T . Let us consider L̂i,T defined as the number of rounds spent with ai as leader when restricting the problem to its neighborhood N+(i). E[L̂i,T ] is an upper bound over E[Li,T ], since there is nonzero probability that the UTS algorithm moves in another neighborhood. Since i 6= i∗ and the setting is unimodal, there exists an optimal arm ai′ , i\n′ 6= i among those in the neighborhood N(i) s.t. µi′ = maxi|ai∈N(i) µi and µ̂i,t ≥ µ̂i′ . Thus:\nRi ≤ E[L̂i,T ] = T∑\nt=1\nE [ 1{µ̂i,t = max\naj∈N+(i) µ̂j,t}\n]\n= T∑ t=1 P\n( µ̂i,t ≥ max\naj∈N+(i) µ̂j,t\n) ≤\nT∑ t=1 P ( µ̂i,t ≥ µ̂i′,t ) =\nT∑ t=1 P ( µ̂i,t − µi − ∆i 2 − µ̂i′,t + µi′ − ∆i 2 ≥ 0 )\n≤ T∑\nt=1\nP ( µ̂i,t − µi − ∆i 2 ≥ 0 )\n︸ ︷︷ ︸ Ri1\n+ T∑ t=1 P ( µ̂i′,t − µi′ + ∆i 2 ≤ 0 )\n︸ ︷︷ ︸ Ri2\n,\nwhere ∆i = maxi′|ai∈N(i) µi′ − µi is the expected loss incurred in choosing ai instead of its best adjacent one ai′ . Ri1 can be upper bounded by a constant by relying on conditional probability definition and the Hoeffding inequality (Hoeffding 1963). Specifically, we rely on the fact that the leader is chosen at least ⌊ Ll(t),t |N+(l(t))| ⌋ times. Upper bounding Ri2 by a constant term requires the use of Proposition 1 in (Kaufmann, Korda, and Munos 2012), which limits the expected number of times the optimal arm is pulled less than tb times by TS, where b ∈ (0, 1) is a constant, and\nthe use of a technique already used onRi1. Summing up the regret over i 6= i∗ and considering the three obtained bounds concludes the proof."
    }, {
      "heading" : "Experimental Evaluation",
      "text" : "In this section, we compare the empirical performance of the proposed algorithm UTS with the performance of a number of algorithms. We study the performance of the state– of–the–art algorithm OSUB (Combes and Proutiere 2014a) to evaluate the improvement due to the employment of Bayesian approaches w.r.t. frequentist approaches. Furthermore, we study the performance of TS (Thompson 1933) to evaluate the improvement in Bayesian approaches due to the exploitation of the problem structure. For completeness, we study also the performance of KLUCB (Garivier and Cappé 2011), being a frequentist algorithm that is optimal for Bernoulli distributions.\nFigures of merit Given a policy U, we evaluate the average and 95%–confidence intervals of the following figures of merit:\n• the pseudo–regret RT (U) as defined in Equation 1; the lower RT (U) the better the performance;\n• the regret ratio R%(U1,U2) = RT (U1)RT (U2) showing the ratio between the total regret of policy U1 after T rounds and the one obtained with U2; the lowerR%(U1,U2) the larger the relative improvement of U1 w.r.t. U2.\nLine graphs We initially consider the same experimental settings, composed of line graphs, that are studied in (Combes and Proutiere 2014a). They consider graphs with K ∈ {17, 129} arms, where the arms are ordered on a line from the arm with smallest index to the arm with the largest index and with Bernoulli rewards whose averages have a triangular shape with the maximum on the arm in the middle of the line. More precisely, the minimum average is 0.1, associated with arms a1 and a17 when K = 17 and with arms a1 and a129 with K = 129, while the maximum average reward is µ∗ = 0.9, associated with arm a9 when K = 17 and with arm a65 with K = 129. The averages decrease linearly from the maximum one to the minimum one.\nFor both the experiments, we average the regret over 100 independent trials of length T = 105. We report Rt(U) for each policy U as t varies in Fig. 1(a), for K = 17, and in Fig. 1(b), for K = 129. The UTS algorithm outperforms all the other algorithms along the whole time horizon, providing a significant improvement in terms of regret w.r.t. the\nstate–of–the–art algorithms. In order to have a more precise evaluation of the reduction of the regret w.r.t. OSUB algorithm, we report R%(U,OSUB) in Tab. 1. As also confirmed below by a more exhaustive series of experiments, in line graphs the relative improvement of performance due to UTS w.r.t. OSUB reduces as the number of arms increases, while the relative improvement of performance due to UTS w.r.t. TS increases as the number of arms increases.\nErdős-Rényi graphs To provide a thorough experimental evaluation of the considered algorithms in settings in which the space of arms has a graph structure, we generate graphs using the model proposed by Erdős and Rényi (1959), which allows us to simulate graph structures more complex than a simple line. An Erdős-Rényi graph is generated by connecting nodes randomly: each edge is included in the graph with probability p, independently from existing edges. We consider connected graphs with K ∈ {5, 10, 20, 50, 100, 1000} and with probability p ∈ {1, 12 , log(K) K , `}, where p = 1 corresponds to have a fully connected graph and therefore the graph structure is useless, p = 12 corresponds to have a number of edges that increases linearly in the number of nodes, p = log(K)K corresponds to have a few edges w.r.t. the nodes, and we use p = ` to denote line graphs (these line graphs differ from those used for the experimental evaluation discussed above for the reward function, as discussed in what follows). We use different values of p in order to see how the performance of UTS changes w.r.t. the number of edges in the graph; we remark that such an analysis is unexplored in the literature so far. The optimal arm is chosen randomly among the existing arms and its reward is given by a Bernoulli distribution with expected value 0.9. The rewards of the suboptimal arms are given by Bernoulli distributions with expected value depending on their distance from the optimal one. More precisely, let d∗i be the shortest path from the i–th arm to the optimal arm and let:\nd∗max = max i∈{1,...,K} d∗i\nbe the maximum shortest path of the graph. The expected reward of the i–th arm is:\nµi = 0.9− d∗i (0.9− 0.1) d∗max ,\ni.e., the arm with d∗max has a value equal to 0.1 and the expected rewards of the arms along the path from it to the optimal arm are evenly spaced between 0.1 and 0.9. We generate 10 different graphs for each combination of K and p and we run 100 independent trials of length T = 105 for each graph. We average the regret over the results of the 10 graphs.\nIn Tab. 2, we report RT (U) for each combination of policy U, K, and p. It can be observed that the UTS algorithm outperforms all the other algorithms, providing in every case the smallest regret except for K = 1000 and p = `. Below we discuss how the relative performance of the algorithms vary as the values of the parameters K and p vary.\nConsider the case with p = 1. The performance of UTS and TS are approximately equal and the same holds for the performance of OSUB and KLUCB. This is due to the fact that the neighborhood of each node is composed by all the arms, the graphs being fully connected, and therefore UTS and OSUB cannot take any advantage from the structure of the problem. We notice, however, that UTS and TS have not the same behavior and that UTS always performs slightly better than TS. It can be observed in Fig. 2 with K = 5 and p = 1 that the relative improvement is mainly at the beginning of the time horizon and that it goes to zero as K increases (the same holds for OSUB w.r.t. KLUCB). The reason behind this behavior is that UTS reduces the exploration performed by TS in the first rounds, forcing the algorithm to pull the leader—chosen as the arm maximizing the empirical mean—for a larger number of rounds.\nConsider the case with p = 12 . In the considered experimental setting, the relative performance of the algorithms does not depend on K. The ordering, from the best to the worst, over the performance of the algorithms is: UTS, TS, OSUB, and finally KLUCB. Surprisingly, even the dependency of the following ratios on K is negligible: R%(UTS,TS) = 0.68 ± 0.03, R%(UTS,OSUB) = 0.47± 0.01, and R%(OSUB,KLUCB) = 0.68± 0.03. This shows that the relative improvement due to UTS is constant w.r.t. TS and OSUB as K varies. These results raise the question whether the relative performance of OSUB and TS would be the same, except for the numerical values, for every p constant w.r.t. K. To answer to this question, we consider the case in which p = 0.1, corresponding to the case in which the number of edges is linear in K, but it is smaller than the case with p = 12 . The results in terms of RT (U), reported in Table 3 show that OSUB outperforms TS for K ≥ 10, suggesting that, when p is constant in K, OSUB may or may not outperform TS depending on the specific pair (p,K).\nConsider the case with p = log(K)K . The ordering over the performance of the algorithms changes as K varies. More precisely, while UTS keeps to be the best algorithm for every K and KLUCB the worst algorithm for every K, the ordering between TS and OSUB changes. When K ≤ 10 TS performs better than OSUB, instead when K ≥ 20 OSUB outperforms TS, see Fig. 3. This is due to the fact that, with a small number of arms, exploiting the graph structure is not sufficient for a frequentist algorithm to outperform the performance of TS, while with many arms exploiting the graph structure even with a frequentist algorithm is much better than employing a general-purpose Bayesian algorithm. The ratio R%(UTS,TS) monotonically decreases as K increases, from 0.66 when K = 5 to 0.19 when K = 1000, suggesting that exploiting the graph structure provides ad-\nvantages asK increases. Instead, the ratioR%(UTS,OSUB) monotonically increases as K increases, from 0.45 when K = 5 to 0.94 when K = 1000, suggesting that the improvement provided by employing Bayesian approaches reduces as K increases as observed above in line graphs.\nConsider the case with p = `. As in the case discussed above, OSUB is outperformed by TS for a small number of arms (K ≤ 10), while it outperforms TS for many arms (K ≥ 20). The reason is the same above. Similarly, the ratio R%(UTS,TS) monotonically decreases as K increases, from 0.58 when K = 5 to 0.18 when K = 1000, and the ratio R%(UTS,OSUB) monotonically increases as K increases, from 0.45 when K = 5 to 1.00 when K = 1000. This confirms that the performance of UTS and the one of OSUB asymptotically match as K increases when p = ` (as well as p = log(K)K ). In order to investigate the reasons behind such a behavior, we produce an additional experiment with the line graphs of Combes and Proutiere (2014a) except that the maximum expected reward is set to 0.108 when K = 17 and 0.165 when K = 129 (thus, given any edge with terminals i and i + 1, we have |µi − µi+1| = 0.001). What we observe (details of these experiments and those described below are in the appendices) is that, on average, OSUB outperforms UTS at T = 105 suggesting that, when it is necessary to repeatedly distinguish between three arms that have very similar expected rewards, frequentist methods may outperform the Bayesian ones. This is no longer true when T is much larger, e.g., T = 107, where UTS outperforms OSUB (interestingly, differently from what happens in the other topologies, in line graphs with very small |µi − µi+1|, the average RT (UTS) and RT (OSUB) cross a number of times during the time horizon). Futhermore, we evaluate how the relative performance of OSUB w.r.t. UTS varies for |µi − µi+1| ∈ {0.001, 0.002, 0.005}, observing it improves as |µi − µi+1| decreases. Finally, we evaluate whether this behavior emerges also in Erdős-Rényi graphs in which p = cK where c is a constant (we use p = 5 K , 10 K ) and we observe that UTS outperforms OSUB, suggesting that line graphs with very small |µi−µi+1| are pathological instances for UTS."
    }, {
      "heading" : "Conclusions and Future Work",
      "text" : "In this paper, we focus on the Unimodal Multi–Armed Bandit problem with graph structure in which each arm corresponds to a node of a graph and each edge is associated with a relationship in terms of expected reward between its arms. We propose, to the best of our knowledge, the first Bayesian algorithm for the UMAB setting, called UTS, which is based on the well–known Thompson Sampling algorithm. We derive a tight upper bound for UTS that asymptotically matches the lower bound for the UMAB setting, providing a non-trivial derivation of the bound. Furthermore, we present a thorough experimental analysis showing that our algorithm outperforms the state–of–the–art methods.\nIn future, we will evaluate the performance of the algorithms considered in this paper with other classes of graphs, e.g., Barabási–Albert and lattices. Future development of this work may consider an analysis of the proposed algorithm in the case of time–varying environments, i.e., the expected reward of each arm varies over time, assuming that the unimodal structure is preserved. Another interesting study may consider the case of a continuous decision space."
    }, {
      "heading" : "Appendix A: Proof of Theorem 2",
      "text" : "Theorem 2. Given a UMAB setting G = (A,E), the expected pseudo–regret of the UTS algorithm satisfies, for every ε > 0:\nRT (UTS) ≤ (1 + ε) ∑\ni∈N(i∗)\nµ∗ − µi KL(µi, µ∗) [log(T ) + log log(T )] + C̃,\nwhere C̃ > 0 is a constant depending on ε, the number of arms K and the expected rewards {µ1, . . . , µK}.\nProof. At first, the regret of the UTS algorithm RT (UTS) can be rewritten by dividing the T rounds in two sets: those rounds in which the best arm a∗ is the leader , i.e., l(t) = i∗, and those in which the leader is another arm, i.e., l(t) 6= i∗:\nRT (UTS) = ∑ i6=i∗ (µ∗ − µi)E[Ti,T ]\n= ∑ i6=i∗ (µ∗ − µi)E [ T∑ t=1 1{it = i} ] = ∑ i6=i∗ (µ∗ − µi)E [ T∑ t=1 1{l(t) = i∗ ∧ it = i} ] ︸ ︷︷ ︸\nR1\n+\n+ ∑ i6=i∗ (µ∗ − µi)E [ T∑ t=1 1{l(t) 6= i∗ ∧ it = i} ] ︸ ︷︷ ︸\nR2\nLet us focus on R1. When i∗ is the leader, the proposed algorithm behaves like Thompson Sampling restricted to the optimal arm and its neighborhood N+(i∗), and the regret upper bound is the one presented in Theorem 1 in (Kaufmann, Korda, and Munos 2012) for TS algorithm, i.e., for every ε > 0:\nR1 ≤ (1 + ε) ∑\ni∈N(i∗)\nµ∗ − µi KL(µi, µ∗) [log(T ) + log log(T )] + C1, (4)\nwhere C1 is an appropriate constant depending on ε and on the expected rewards µi of arms in N+(i∗). Now let us considerR2, we have:\nR2 = ∑ i 6=i∗ (µ∗ − µi)︸ ︷︷ ︸ ≤1 E [ T∑ t=1 1{l(t) 6= i∗ ∧ it = i} ]\n≤ ∑ i 6=i∗ E [Li,T ] .\nHere we want to upper bound the number of times ai has been the leader Li,T with L̂i,T defined as the number of rounds spent with ai as leader in the case only its neighborhood is considered during the whole time horizon T . This is clearly an upper bound over Li,T , since there is nonzero probability that the UTS algorithms moves in another neighborhood. From now on in the proof the analysis is carried on an algorithm working only on a unique neighborhood N(i).\nR2 ≤ ∑ i 6=i∗ E [Li,T ] ≤ ∑ i6=i∗ E [ L̂i,T ] = ∑ i 6=i∗ T∑ t=1 E [1{l(t) = i}]\n= ∑ i 6=i∗ T∑ t=1 E [ 1{µ̂i,t = max aj∈N(i) µ̂j,t} ] ,\nwhere, with abuse of notation, l(t) is the leader at round t in this new problem where only N(i) is considered.\nWhen i 6= i∗ is the leader, ai is not the optimal arm. Thus, since we are in a unimodal setting, it exists an optimal arm ai′ ∈ N(i), i′ 6= i s.t. µi′ = maxi|ai∈N(i) µi. Nonetheless, since ai is the leader, its empirical mean is the maximum in its neighborhood and, in particular, µ̂i,t ≥ µ̂i′ . Thus, we have:\nR2 ≤ ∑ i6=i∗ T∑ t=1 E [ 1{µ̂i,t = max aj∈N(i) µ̂j,t} ]\n≤ ∑ i 6=i∗ T∑ t=1 E [1{µ̂i,t ≥ µ̂i′,t}] = ∑ i 6=i∗ T∑ t=1 P (µ̂i,t ≥ µ̂i′,t) = ∑ i 6=i∗ T∑ t=1 P ( µ̂i,t − µi − ∆i 2 − µ̂i′,t + µi′ − ∆i 2 ≥ 0 )\n≤ ∑ i 6=i∗  T∑ t=1 P ( µ̂i,t − µi − ∆i 2 ≥ 0 )\n︸ ︷︷ ︸ Ri1\n+ T∑ t=1 P ( µ̂i′,t − µi′ + ∆i 2 ≤ 0 )\n︸ ︷︷ ︸ Ri2\n ,\nwhere ∆i = maxi′|ai∈N(i) µi′ − µi denotes the expected loss incurred in choosing arm ai instead of its best adjacent one ai′ . Let us focus onRi1:\nRi1 = T∑ t=1 P ( µ̂i,t ≥ µi + ∆i 2 )\n= T∑ t=1 t∑ h=1 P ( Ti,t = h ∧ µ̂i,t ≥ µi + ∆i 2 )\n= T∑ t=1 t∑ h=1 P ( Ti,t = h | µ̂i,t ≥ µi + ∆i 2 ) P ( µ̂i,t ≥ µi + ∆i 2 )\n≤ T∑ t=1 t∑ h=1 P ( Ti,t = h | µ̂i,t ≥ µi + ∆i 2 ) e− h∆2i 2\nWhere the last inequality is due to the Hoeffding inequality (Hoeffding 1963). By relying on the fact that ∑∞ h=x+1 e\n−kh ≤ 1 ke −kx and by considering x = t|N+(i)| we have:\nRi1 ≤ T∑ t=1  t |N+(i)|∑ h=1 P ( Ti,t = h | µ̂i,t ≥ µi + ∆i 2 ) ︸ ︷︷ ︸\n=0\ne h∆2i 2 + 2\n∆2i e−\nt |N+(i)|∆ 2 i\n2  =\nT∑ t=1 2 ∆2i e − t|N+(i)|∆ 2 i 2 ≤ C2\nwhere P ( Ti,t = h | µ̂i,t ≥ µi + ∆i2 ) = 0 for h ≤ t|N+(i)| is due to the fact that the leader is chosen at least t |N+(i)| over t rounds and C2 is a constant. Let us focus onRi2 and the following proposition provided in (Kaufmann, Korda, and Munos 2012):\nProposition 1. If we use a TS policy over a set of finite arms {ai} where ai′ is the optimal one, there exist constants b ∈ (0, 1) and Cb ≤ ∞ s.t.: ∞∑\nt=1\nE [ 1{Ti′,t ≤ tb} ] ≤ Cb. (5)\nSimilarly to what has been derived forRi1 we have: Ri2 = T∑ t=1 P ( µ̂i′,s ≤ µi′ − ∆i 2 )\n= T∑ t=1 t∑ h=1 P ( Ti′,t = h ∧ µ̂i′,s ≤ µi′ − ∆i 2 )\n= T∑ t=1 tb∑ h=1 P ( Ti′,t = h ∧ µ̂i′,s ≤ µi′ − ∆i 2 ) + T∑ t=1 t∑ h=tb+1 P ( Ti′,t = h | µ̂i′,s ≤ µi′ − ∆i 2 ) ︸ ︷︷ ︸\n≤1\nP ( µ̂i′,s ≤ µi′ −\n∆i 2\n)\n≤ ∞∑ t=1 E [ 1{Ti′,t ≤ tb} ] + T∑ t=1 t∑ h=tb+1 P ( µ̂i′,s ≤ µi′ − ∆i 2 )\n≤ Cb + T∑ t=1 t∑ h=tb+1 e− t∆2i 2\n≤ Cb + T∑ t=1 2 ∆2i e− tb∆2i 2 ≤ C3\nsince we are using TS in among arms in N(i) and the last inequality holds for all b ∈ (0, 1). By considering the three partial results onR1,Ri1,Ri2 we have:\nRT (UTS) ≤ R1 + ∑ i 6=i∗ (Ri1 +Ri2)\n= (1 + ε) ∑\ni∈N(i∗) (µ∗ − µi)\nlog(T ) + log log(T )\nKL(µi, µ∗) + C1 + (K − 1)(C2 + C3)\nconsidering C̃ = C1 + (K − 1)(C2 + C3) concludes the proof."
    }, {
      "heading" : "Appendix B: Additional Results on p = `",
      "text" : "In order to investigate the reasons why the performance of UTS and the one of OSUB asymptotically match as K increases when p = `, we produce additional experiments with the line graphs described in (Combes and Proutiere 2014a). We generated line graphs where the minimum expected reward is set to 0.1 and the maximum expected reward varies: given any edge with terminals two consecutive nodes i and i + 1, we generated graphs where ∆ = |µi − µi+1| ∈ {0.001, 0.002, 0.005}. More precisely, when K = 17, the expected reward of the central arm a8 is set to, respectively, 0.108, 0.116 and 0.14. When K = 129, the expected reward of the central arm a65 is set to, respectively, 0.165, 0.23 and 0.425. The results for T = 107 are reported in Figure 4 for K = 17 and in Figure 5 for K = 129.\nWe observe that, at T = 107 with K = 17 and ∆ = 0.001, on average OSUB outperforms UTS while, with ∆ ∈ {0.002, 0.005}, at the end of the experiments UTS outperforms OSUB. This behavior suggests that, even in the case with ∆ = 0.001, UTS will perform better than OSUB for T > 107. In the case with K = 129, ∆ = 0.005 and T = 107, UTS outperforms OSUB at the end of the experiments while with ∆ ∈ {0.001, 0.002} OSUB performs better. Following the same line of reasoning, for T > 107 this could no longer be true. All these results suggest that when it is necessary to repeatedly distinguish between three arms that have very similar expected rewards and very low expected rewards, frequentist methods may outperform the Bayesian ones at the beginning of the learning process, while Bayesian methods asymptotically outperform frequentist ones. In particular, we observe that the relative performance of OSUB w.r.t. UTS varies for ∆ ∈ {0.001, 0.002, 0.005}, observing it improves as ∆ decreases.\nFinally, we evaluate whether this behavior emerges also in Erdős-Rényi graphs in which p = cK where c is a constant. We use p ∈ { 5K , 10 K } and T = 10\n6. We observe that UTS outperforms OSUB, suggesting that line graphs with very small ∆ represent pathological instances for UTS. The results are reported in Figure 6."
    } ],
    "references" : [ {
      "title" : "From bandits to experts: A tale of domination and independence",
      "author" : [ "N. Alon", "N. Cesa-Bianchi", "C. Gentile", "Y. Mansour" ],
      "venue" : "Advances in Neural Information Processing Systems, 1610–1618.",
      "citeRegEx" : "Alon et al\\.,? 2013",
      "shortCiteRegEx" : "Alon et al\\.",
      "year" : 2013
    }, {
      "title" : "Finitetime analysis of the multiarmed bandit problem",
      "author" : [ "P. Auer", "N. Cesa-Bianchi", "P. Fischer" ],
      "venue" : "Machine learning 47(2-3):235–256.",
      "citeRegEx" : "Auer et al\\.,? 2002",
      "shortCiteRegEx" : "Auer et al\\.",
      "year" : 2002
    }, {
      "title" : "Mixing bandits: A recipe for improved cold-start recommendations in a social network",
      "author" : [ "S. Caron", "S. Bhagat" ],
      "venue" : "Proceedings of the 7th Workshop on Social Network Mining and Analysis, 11. ACM.",
      "citeRegEx" : "Caron and Bhagat,? 2013",
      "shortCiteRegEx" : "Caron and Bhagat",
      "year" : 2013
    }, {
      "title" : "An empirical evaluation of thompson sampling",
      "author" : [ "O. Chapelle", "L. Li" ],
      "venue" : "Advances in neural information processing systems, 2249–2257.",
      "citeRegEx" : "Chapelle and Li,? 2011",
      "shortCiteRegEx" : "Chapelle and Li",
      "year" : 2011
    }, {
      "title" : "Unimodal bandits: Regret lower bounds and optimal algorithms",
      "author" : [ "R. Combes", "A. Proutiere" ],
      "venue" : "ICML, 521– 529.",
      "citeRegEx" : "Combes and Proutiere,? 2014a",
      "shortCiteRegEx" : "Combes and Proutiere",
      "year" : 2014
    }, {
      "title" : "Unimodal bandits without smoothness",
      "author" : [ "R. Combes", "A. Proutiere" ],
      "venue" : "arXiv preprint arXiv:1406.7447.",
      "citeRegEx" : "Combes and Proutiere,? 2014b",
      "shortCiteRegEx" : "Combes and Proutiere",
      "year" : 2014
    }, {
      "title" : "Feedback effects between similarity and social influence in online communities",
      "author" : [ "D. Crandall", "D. Cosley", "D. Huttenlocher", "J. Kleinberg", "S. Suri" ],
      "venue" : "Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining, 160–168. ACM.",
      "citeRegEx" : "Crandall et al\\.,? 2008",
      "shortCiteRegEx" : "Crandall et al\\.",
      "year" : 2008
    }, {
      "title" : "Strategic bidder behavior in sponsored search auctions",
      "author" : [ "B. Edelman", "M. Ostrovsky" ],
      "venue" : "Decision support systems 43(1):192–198.",
      "citeRegEx" : "Edelman and Ostrovsky,? 2007",
      "shortCiteRegEx" : "Edelman and Ostrovsky",
      "year" : 2007
    }, {
      "title" : "On random graphs i",
      "author" : [ "P. Erdős", "A. Rényi" ],
      "venue" : "Publ. Math. Debrecen 6:290–297.",
      "citeRegEx" : "Erdős and Rényi,? 1959",
      "shortCiteRegEx" : "Erdős and Rényi",
      "year" : 1959
    }, {
      "title" : "The kl-ucb algorithm for bounded stochastic bandits and beyond",
      "author" : [ "A. Garivier", "O. Cappé" ],
      "venue" : "COLT, 359–376.",
      "citeRegEx" : "Garivier and Cappé,? 2011",
      "shortCiteRegEx" : "Garivier and Cappé",
      "year" : 2011
    }, {
      "title" : "Probability inequalities for sums",
      "author" : [ "W. Hoeffding" ],
      "venue" : null,
      "citeRegEx" : "Hoeffding,? \\Q1963\\E",
      "shortCiteRegEx" : "Hoeffding",
      "year" : 1963
    }, {
      "title" : "Unimodal bandits",
      "author" : [ "Y.Y. Jia", "S. Mannor" ],
      "venue" : "Proceedings of the 28th International Conference on Machine Learning (ICML-11), 41–48.",
      "citeRegEx" : "Jia and Mannor,? 2011",
      "shortCiteRegEx" : "Jia and Mannor",
      "year" : 2011
    }, {
      "title" : "Thompson sampling: An asymptotically optimal finite-time analysis",
      "author" : [ "E. Kaufmann", "N. Korda", "R. Munos" ],
      "venue" : "ALT, volume 7568 of Lecture Notes in Computer Science, 199–213. Springer.",
      "citeRegEx" : "Kaufmann et al\\.,? 2012",
      "shortCiteRegEx" : "Kaufmann et al\\.",
      "year" : 2012
    }, {
      "title" : "Multi-armed bandits in metric spaces",
      "author" : [ "R. Kleinberg", "A. Slivkins", "E. Upfal" ],
      "venue" : "Proceedings of the fortieth annual ACM symposium on Theory of computing, 681–690. ACM.",
      "citeRegEx" : "Kleinberg et al\\.,? 2008",
      "shortCiteRegEx" : "Kleinberg et al\\.",
      "year" : 2008
    }, {
      "title" : "Asymptotically efficient adaptive allocation rules",
      "author" : [ "T.L. Lai", "H. Robbins" ],
      "venue" : "Advances in applied mathematics 6(1):4–22.",
      "citeRegEx" : "Lai and Robbins,? 1985",
      "shortCiteRegEx" : "Lai and Robbins",
      "year" : 1985
    }, {
      "title" : "From bandits to experts: On the value of side-observations",
      "author" : [ "S. Mannor", "O. Shamir" ],
      "venue" : "NIPS. 684–692.",
      "citeRegEx" : "Mannor and Shamir,? 2011",
      "shortCiteRegEx" : "Mannor and Shamir",
      "year" : 2011
    }, {
      "title" : "Birds of a feather: Homophily in social networks",
      "author" : [ "M. McPherson", "L. Smith-Lovin", "J.M. Cook" ],
      "venue" : "Annual review of sociology 415–444.",
      "citeRegEx" : "McPherson et al\\.,? 2001",
      "shortCiteRegEx" : "McPherson et al\\.",
      "year" : 2001
    }, {
      "title" : "On the likelihood that one unknown probability exceeds another in view of the evidence of two samples",
      "author" : [ "W.R. Thompson" ],
      "venue" : "Biometrika 25(3/4):285–294.",
      "citeRegEx" : "Thompson,? 1933",
      "shortCiteRegEx" : "Thompson",
      "year" : 1933
    }, {
      "title" : "Multi–armed bandit for pricing",
      "author" : [ "F. Trovò", "S. Paladino", "M. Restelli", "N. Gatti" ],
      "venue" : "2th European Workshop on Reinforcement Learning (EWRL). https://ewrl. wordpress.com/past-ewrl/ewrl12-2015/.",
      "citeRegEx" : "Trovò et al\\.,? 2015",
      "shortCiteRegEx" : "Trovò et al\\.",
      "year" : 2015
    }, {
      "title" : "Spectral bandits for smooth graph functions",
      "author" : [ "M. Valko", "R. Munos", "B. Kveton", "T. Kocak" ],
      "venue" : "Proceedings of The 31st International Conference on Machine Learning, ICML, 46–54.",
      "citeRegEx" : "Valko et al\\.,? 2014",
      "shortCiteRegEx" : "Valko et al\\.",
      "year" : 2014
    }, {
      "title" : "Thompson sampling for budgeted multi-armed bandits",
      "author" : [ "Y. Xia", "H. Li", "T. Qin", "N. Yu", "T.-Y. Liu" ],
      "venue" : "TwentyFourth International Joint Conference on Artificial Intelligence.",
      "citeRegEx" : "Xia et al\\.,? 2015",
      "shortCiteRegEx" : "Xia et al\\.",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 4,
      "context" : "We focus on the so–called Unimodal MAB (UMAB), introduced in (Combes and Proutiere 2014a), in which each arm corresponds to a node of a graph and each edge is associated with a relationship specifying which node of the edge gives the largest expected reward (providing thus a partial ordering over the arm space).",
      "startOffset" : 61,
      "endOffset" : 89
    }, {
      "referenceID" : 6,
      "context" : ", neighbor nodes in the graph), therefore interests of people in a social network change smoothly and neighboring nodes in the graph look similar to each other (McPherson, Smith-Lovin, and Cook 2001; Crandall et al. 2008).",
      "startOffset" : 160,
      "endOffset" : 221
    }, {
      "referenceID" : 11,
      "context" : ", in sequential pricing (Jia and Mannor 2011), bidding in online sponsored search auctions (Edelman and Ostrovsky 2007) and single–peak preferences economics and voting settings (Mas-Collel, Whinston, and Green 1995), are graph–structured problems in which the graph is a line.",
      "startOffset" : 24,
      "endOffset" : 45
    }, {
      "referenceID" : 7,
      "context" : ", in sequential pricing (Jia and Mannor 2011), bidding in online sponsored search auctions (Edelman and Ostrovsky 2007) and single–peak preferences economics and voting settings (Mas-Collel, Whinston, and Green 1995), are graph–structured problems in which the graph is a line.",
      "startOffset" : 91,
      "endOffset" : 119
    }, {
      "referenceID" : 11,
      "context" : "are proposed in (Jia and Mannor 2011) and (Combes and Proutiere 2014a).",
      "startOffset" : 16,
      "endOffset" : 37
    }, {
      "referenceID" : 4,
      "context" : "are proposed in (Jia and Mannor 2011) and (Combes and Proutiere 2014a).",
      "startOffset" : 42,
      "endOffset" : 70
    }, {
      "referenceID" : 3,
      "context" : ", in bandit problems without structure (Chapelle and Li 2011) and in bandit problems with budget (Xia et al.",
      "startOffset" : 39,
      "endOffset" : 61
    }, {
      "referenceID" : 20,
      "context" : ", in bandit problems without structure (Chapelle and Li 2011) and in bandit problems with budget (Xia et al. 2015)).",
      "startOffset" : 97,
      "endOffset" : 114
    }, {
      "referenceID" : 3,
      "context" : "are proposed in (Jia and Mannor 2011) and (Combes and Proutiere 2014a). Jia and Mannor (2011) introduce the GLSE algorithm with a regret of order O( √ T log(T )).",
      "startOffset" : 43,
      "endOffset" : 94
    }, {
      "referenceID" : 3,
      "context" : "are proposed in (Jia and Mannor 2011) and (Combes and Proutiere 2014a). Jia and Mannor (2011) introduce the GLSE algorithm with a regret of order O( √ T log(T )). However, GLSE performs better than classical bandit algorithms only when the number of arms is Θ(T ). Combes and Proutiere (2014a) present the OSUB algorithm—based on KLUCB—achieving asymptotic regret ofO(log(T )) and outperforming GLSE in settings with a few arms.",
      "startOffset" : 43,
      "endOffset" : 294
    }, {
      "referenceID" : 11,
      "context" : "Some works deal with unimodal reward functions in continuous armed bandit setting (Jia and Mannor 2011; Combes and Proutiere 2014b; Kleinberg, Slivkins, and Upfal 2008).",
      "startOffset" : 82,
      "endOffset" : 168
    }, {
      "referenceID" : 5,
      "context" : "Some works deal with unimodal reward functions in continuous armed bandit setting (Jia and Mannor 2011; Combes and Proutiere 2014b; Kleinberg, Slivkins, and Upfal 2008).",
      "startOffset" : 82,
      "endOffset" : 168
    }, {
      "referenceID" : 11,
      "context" : "In (Jia and Mannor 2011) a successive elimination algorithm, called LSE, is proposed achieving regret of O( √ T log T ).",
      "startOffset" : 3,
      "endOffset" : 24
    }, {
      "referenceID" : 19,
      "context" : "An application–dependent solution to the recommendation systems which exploits the similarity of the graph in social network in targeted advertisement has been proposed in (Valko et al. 2014).",
      "startOffset" : 172,
      "endOffset" : 191
    }, {
      "referenceID" : 2,
      "context" : "Similar information has been considered in (Caron and Bhagat 2013) where the problem of cold–start users (i.",
      "startOffset" : 43,
      "endOffset" : 66
    }, {
      "referenceID" : 18,
      "context" : "Another type of structure considered in sequential games is the one of monotonicity of the conversion rate in the price (Trovò et al. 2015).",
      "startOffset" : 120,
      "endOffset" : 139
    }, {
      "referenceID" : 0,
      "context" : "In (Alon et al. 2013; Mannor and Shamir 2011), a graph structure of the arm feedback in an adversarial setting is studied.",
      "startOffset" : 3,
      "endOffset" : 45
    }, {
      "referenceID" : 15,
      "context" : "In (Alon et al. 2013; Mannor and Shamir 2011), a graph structure of the arm feedback in an adversarial setting is studied.",
      "startOffset" : 3,
      "endOffset" : 45
    }, {
      "referenceID" : 2,
      "context" : "Some works deal with unimodal reward functions in continuous armed bandit setting (Jia and Mannor 2011; Combes and Proutiere 2014b; Kleinberg, Slivkins, and Upfal 2008). In (Jia and Mannor 2011) a successive elimination algorithm, called LSE, is proposed achieving regret of O( √ T log T ). In this case, assumptions over the minimum local decrease and increase of the expected reward is required. Combes and Proutiere (2014b) consider stochastic bandit problems with a continuous set of arms and where the expected reward is a continuous and unimodal function of the arm.",
      "startOffset" : 104,
      "endOffset" : 427
    }, {
      "referenceID" : 4,
      "context" : "shown in (Combes and Proutiere 2014a) that the problem of learning in a UMAB setting presents a lower bound over the regret RT (U) of the following form: Theorem 1.",
      "startOffset" : 9,
      "endOffset" : 37
    }, {
      "referenceID" : 14,
      "context" : "This result is similar to the one provided in (Lai and Robbins 1985), with the only difference that the summation is restricted to the arms laying in the neighborhood of the optimal arm N(i∗) and reduces to it when the optimal arm is connected to all the others (i.",
      "startOffset" : 46,
      "endOffset" : 68
    }, {
      "referenceID" : 17,
      "context" : "The algorithm is an extension of the Thompson Sampling (Thompson 1933) that exploits the graph structure and the unimodal property of the UMAB setting.",
      "startOffset" : 55,
      "endOffset" : 70
    }, {
      "referenceID" : 10,
      "context" : "Ri1 can be upper bounded by a constant by relying on conditional probability definition and the Hoeffding inequality (Hoeffding 1963).",
      "startOffset" : 117,
      "endOffset" : 133
    }, {
      "referenceID" : 4,
      "context" : "We study the performance of the state– of–the–art algorithm OSUB (Combes and Proutiere 2014a) to evaluate the improvement due to the employment of Bayesian approaches w.",
      "startOffset" : 65,
      "endOffset" : 93
    }, {
      "referenceID" : 17,
      "context" : "Furthermore, we study the performance of TS (Thompson 1933) to evaluate the improvement in Bayesian approaches due to the exploitation of the problem structure.",
      "startOffset" : 44,
      "endOffset" : 59
    }, {
      "referenceID" : 9,
      "context" : "For completeness, we study also the performance of KLUCB (Garivier and Cappé 2011), being a frequentist algorithm that is optimal for Bernoulli distributions.",
      "startOffset" : 57,
      "endOffset" : 82
    }, {
      "referenceID" : 4,
      "context" : "Line graphs We initially consider the same experimental settings, composed of line graphs, that are studied in (Combes and Proutiere 2014a).",
      "startOffset" : 111,
      "endOffset" : 139
    }, {
      "referenceID" : 4,
      "context" : "Figure 1: Results for the pseudo–regret Rt(U) in line graphs settings with K = 17 (a) and K = 129 (b) as defined in (Combes and Proutiere 2014a).",
      "startOffset" : 116,
      "endOffset" : 144
    }, {
      "referenceID" : 8,
      "context" : "Erdős-Rényi graphs To provide a thorough experimental evaluation of the considered algorithms in settings in which the space of arms has a graph structure, we generate graphs using the model proposed by Erdős and Rényi (1959), which allows us to simulate graph structures more complex than a simple line.",
      "startOffset" : 203,
      "endOffset" : 226
    }, {
      "referenceID" : 4,
      "context" : "In order to investigate the reasons behind such a behavior, we produce an additional experiment with the line graphs of Combes and Proutiere (2014a) except that the maximum expected reward is set to 0.",
      "startOffset" : 120,
      "endOffset" : 149
    } ],
    "year" : 2016,
    "abstractText" : "We study, to the best of our knowledge, the first Bayesian algorithm for unimodal Multi–Armed Bandit (MAB) problems with graph structure. In this setting, each arm corresponds to a node of a graph and each edge provides a relationship, unknown to the learner, between two nodes in terms of expected reward. Furthermore, for any node of the graph there is a path leading to the unique node providing the maximum expected reward, along which the expected reward is monotonically increasing. Previous results on this setting describe the behavior of frequentist MAB algorithms. In our paper, we design a Thompson Sampling–based algorithm whose asymptotic pseudo–regret matches the lower bound for the considered setting. We show that—as it happens in a wide number of scenarios—Bayesian MAB algorithms dramatically outperform frequentist ones. In particular, we provide a thorough experimental evaluation of the performance of our and state– of–the–art algorithms as the properties of the graph vary. Introduction Multi–Armed Bandit (MAB) algorithms (Auer, CesaBianchi, and Fischer 2002) have been proven to provide effective solutions for a wide range of applications fitting the sequential decisions making scenario. In this framework, at each round over a finite horizon T , the learner selects an action (usually called arm) from a finite set and observes only the reward corresponding to the choice she made. The goal of a MAB algorithm is to converge to the optimal arm, i.e., the one with the highest expected reward, while minimizing the loss incurred in the learning process and, therefore, its performance is measured through its expected regret, defined as the difference between the expected reward achieved by an oracle algorithm always selecting the optimal arm and the one achieved by the considered algorithm. We focus on the so–called Unimodal MAB (UMAB), introduced in (Combes and Proutiere 2014a), in which each arm corresponds to a node of a graph and each edge is associated with a relationship specifying which node of the edge gives the largest expected reward (providing thus a partial ordering over the arm space). Furthermore, from any node there is a path leading to the unique node with the maximum expected reward along which the expected reward is monotonically Copyright c © 2017, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. increasing. While the graph structure may be (not necessarily) known a priori by the UMAB algorithm, the relationship defined over the edges is discovered during the learning. In the present paper, we propose a novel algorithm relying on the Bayesian learning approach for a generic UMAB setting. Models presenting a graph structure have become more and more interesting in last years due to the spread of social networks. Indeed, the relationships among the entities of a social network have a natural graph structure. A practical problem in this scenario is the targeted advertisement problem, whose goal is to discover the part of the network that is interested in a given product. This task is heavily influenced by the graph structure, since in social networks people tend to have similar characteristics to those of their friends (i.e., neighbor nodes in the graph), therefore interests of people in a social network change smoothly and neighboring nodes in the graph look similar to each other (McPherson, Smith-Lovin, and Cook 2001; Crandall et al. 2008). More specifically, an advertiser aims at finding those users that maximize the ad expected revenue (i.e., the product between click probability and value per click), while at the same time reducing the amount of times the advertisement is presented to people not interested in its content. Under the assumption of unimodal expected reward, the learner can move from low expected rewards to high ones just by climbing them in the graph, preventing from the need of a uniform exploration over all the graph nodes. This assumption reduces the complexity in the search for the optimal arm, since the learning algorithm can avoid to pull the arms corresponding to some subset of non– optimal nodes, reducing thus the regret. Other applications might benefit from this structure, e.g., recommender systems which aims at coupling items with those users are likely to enjoy them. Similarly, the use of the unimodal graph structure might provide more meaningful recommendations without testing all the users in the social network. Finally, notice that unimodal problems with a single variable, e.g., in sequential pricing (Jia and Mannor 2011), bidding in online sponsored search auctions (Edelman and Ostrovsky 2007) and single–peak preferences economics and voting settings (Mas-Collel, Whinston, and Green 1995), are graph–structured problems in which the graph is a line. Frequentist approaches for UMAB with graph structure ar X iv :1 61 1. 05 72 4v 2 [ cs .L G ] 2 2 N ov 2 01 6 are proposed in (Jia and Mannor 2011) and (Combes and Proutiere 2014a). Jia and Mannor (2011) introduce the GLSE algorithm with a regret of order O( √ T log(T )). However, GLSE performs better than classical bandit algorithms only when the number of arms is Θ(T ). Combes and Proutiere (2014a) present the OSUB algorithm—based on KLUCB—achieving asymptotic regret ofO(log(T )) and outperforming GLSE in settings with a few arms. To the best of our knowledge, no Bayesian approach has been proposed for unimodal bandit settings, included the UMAB setting we study. However, it is well known that Bayesian MAB algorithms—the most popular is Thompson Sampling (TS)—usually suffer of same order of regret as the best frequentist one (e.g., in unstructured settings (Kaufmann, Korda, and Munos 2012)), but they outperform the frequentist methods in a wide range of problems (e.g., in bandit problems without structure (Chapelle and Li 2011) and in bandit problems with budget (Xia et al. 2015)). Furthermore, in problems with structure, the classical Thompson Sampling (not exploiting the problem structure) may outperform frequentist algorithms exploiting the problem structure. For this reason, in this paper we explore Bayesian approaches for the UMAB setting. More precisely, we provide the following original contributions: • we design a novel Bayesian MAB algorithm, called UTS and based on the TS algorithm; • we derive a tight upper bound over the pseudo–regret for UTS, which asymptotically matches the lower bound for the UMAB setting; • we describe a wide experimental campaign showing better performance of UTS in applicative scenarios than those of state–of–the–art algorithms, evaluating also how the performance of the algorithms (ours and of the state of the art) varies as the graph structure properties vary. Related work Here, we mention the main works related to ours. Some works deal with unimodal reward functions in continuous armed bandit setting (Jia and Mannor 2011; Combes and Proutiere 2014b; Kleinberg, Slivkins, and Upfal 2008). In (Jia and Mannor 2011) a successive elimination algorithm, called LSE, is proposed achieving regret of O( √ T log T ). In this case, assumptions over the minimum local decrease and increase of the expected reward is required. Combes and Proutiere (2014b) consider stochastic bandit problems with a continuous set of arms and where the expected reward is a continuous and unimodal function of the arm. They propose the SP algorithm, based on the stochastic pentachotomy procedure to narrow the search space. Unimodal MABs on metric spaces are studied in (Kleinberg, Slivkins, and Upfal 2008). An application–dependent solution to the recommendation systems which exploits the similarity of the graph in social network in targeted advertisement has been proposed in (Valko et al. 2014). Similar information has been considered in (Caron and Bhagat 2013) where the problem of cold–start users (i.e., new users) is studied. Another type of structure considered in sequential games is the one of monotonicity of the conversion rate in the price (Trovò et al. 2015). Interestingly, the assumptions of monotonicity and unimodality are orthogonal, none of them being a special case of the other, therefore the results for monotonic setting cannot be used in unimodal bandits. In (Alon et al. 2013; Mannor and Shamir 2011), a graph structure of the arm feedback in an adversarial setting is studied. More precisely, they assume to have correlation over rewards and not over the expected values of arms. Problem Formulation A learner receives in input a finite undirected graph MAB settingG = (A,E), whose verticesA = {a1, . . . , aK} with K ∈ N correspond to the arms and an edge (aiaj) ∈ E exists only if there is a direct partial order relationship between the expected rewards of arms ai and aj . The leaner knows a priori the nodes and the edges (i.e., she knows the graph), but, for each edge, she does not know a priori which is the node of the edge with the largest expected reward (i.e., she does not know the ordering relationship). At each round t over a time horizon of T ∈ N the learner selects an arm ai and gains the corresponding reward xi,t. This reward is drawn from an i.i.d. random variable Xi,t (i.e., we consider a stochastic MAB setting) characterized by an unknown distribution Di with finite known support Ω ⊂ R (as customary in MAB settings, from now on we consider Ω ⊆ [0, 1]) and by unknown expected value μi := E[Xi,t]. We assume that there is a single optimal arm, i.e., there exists a unique arm ai∗ s.t. its expected value μi∗ = maxi μi and, for sake of notation, we denote μi∗ with μ∗. Here we analyze a graph bandit setting with unimodality property, defined as: Definition 1. A graph unimodal MAB (UMAB) settingG = (A,E) is a graph bandit setting G s.t. for each sub–optimal arm ai, i 6= i∗ it exists a finite path p = (i1 = i, . . . , im = i∗) s.t. μik < μik+1 and (aik , aik+1) ∈ E for each k ∈ {1, . . . ,m− 1}. This definition assures that if one is able to identify a non– decreasing path in G of expected rewards, she be able to reach the optimum arm, without getting stuck in local optima. Note that the unimodality property implies that the graph G is connected and therefore we consider only connected graphs from here on. A policy U over a UMAB setting is a procedure able to select at each round t an arm ait by basing on the history ht, i.e., the sequence of past selected arms and past rewards gained. The pseudo–regretRT (U) of a generic policy U over a UMAB setting is defined as: RT (U) := Tμ ∗ − E [ T ∑",
    "creator" : "TeX"
  }
}