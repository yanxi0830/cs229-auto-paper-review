{
  "name" : "1704.00454.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Clustering in Hilbert simplex geometry",
    "authors" : [ "Frank Nielsen", "Ke Sun" ],
    "emails" : [ "Frank.Nielsen@acm.org", "sunk@ieee.org" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Keywords: Fisher-Rao geometry, information geometry, Hilbert simplex geometry, Finsler geometry, center-based clustering."
    }, {
      "heading" : "1 Introduction",
      "text" : "The multinomial distribution is an important representation in machine learning that is often met in applications [32, 18] as normalized histograms (with non-empty bins). A multinomial distribution (or categorical distribution) p ∈ ∆d can be thought as a point lying in the probability simplex ∆d (standard simplex) with coordinates p = (λ0p, . . . , λ d p) such that λ i p > 0 and ∑d i=0 λ i p = 1. The open probability\nsimplex ∆d sits in Rd+1 on the hyperplane H∆d : ∑d i=0 xi = 1. We consider the task of clustering a set Λ = {p1, . . . , pn} of n categorical distributions [18] (multinomials) of ∆d using center-based kmeans++ or k-center clustering algorithms [6, 23] that rely on a dissimilarity measure (loosely called distance or divergence when smooth) between any two categorical distributions. In this work, we consider three distances with their underlying geometries for clustering: (1) Fisher-Rao distance ρFHR, (2) Kullback-Leibler divergence ρIG, and (3) Hilbert distance ρHG. The geometric structures are necessary in algorithms, for example, to define midpoint distributions. Figure 1 displays the k-center clustering results obtained with these three geometries as well as the Euclidean L1 distance ρL1. We shall now explain the Hilbert simplex geometry applied to the probability simplex, describe how to perform k-center clustering in Hilbert geometry, and report experimental results that demonstrate superiority of the Hilbert geometry when clustering multinomials."
    }, {
      "heading" : "1.1 Paper outline",
      "text" : "The rest of this paper is organized as follows: Section 2 formally introduces the distance measures of ∆d. Section 3 introduces how to efficiently compute the Hilbert distance. Section 4 presents algorithms for Hilbert minimax centers and Hilbert clustering. Section 5 performs an empirical study of clustering multinomial distributions, comparing Riemannian geometry, information geometry and Hilbert geometry.\n∗École Polytechnique, France and Sony Computer Science Laboratories Inc., Japan. E-mail: Frank.Nielsen@acm.org †King Abdullah University of Science and Technology, Saudi Arabia. E-mail: sunk@ieee.org\nar X\niv :1\n70 4.\n00 45\n4v 2\n[ cs\n.L G\n] 3\n0 A\npr 2\n01 7\nSection 6 concludes this work by summarizing the pros and cons of each geometry. Although some contents require prior knowledge on geometric structures, we give clearly the algorithms so that general audience can still benefit from this work."
    }, {
      "heading" : "2 Three distances with their underlying geometries",
      "text" : ""
    }, {
      "heading" : "2.1 Fisher-Hotelling-Rao geometry",
      "text" : "The Rao distance between two multinomial distributions is [28, 32]:\nρFHR(p, q) = 2 arccos ( d∑ i=0 √ λipλ i q ) . (1)\nIt is a Riemannian metric length distance (satisfying the symmetric and triangular inequality axioms) obtained by setting the metric tensor g to the Fisher information matrix (FIM) I of the categorical distribution: I(p) = [gij(p)] with\ngij(p) = δij λip + 1 λ0p .\nWe term this geometry the Fisher-Hotelling-Rao (FHR) geometry [26, 57, 49, 50]. The metric tensor g allows to define an inner product on each tangent plane Tp of the probability simplex manifold: 〈u, v〉p = u>g(p)v. When g is the identity matrix, we recover the Euclidean (Riemannian) geometry with the inner product being the scalar product: 〈u, v〉 = u>v. The geodesics γ(p, q;α) are defined by the Levi-Civita metric connection [2, 15]. The FHR manifold can be embedded in the positive orthant of the Euclidean\nunit d-sphere of Rd+1 by using the square root representation p 7→ √p, see [28]. Therefore the FHR manifold modeling of ∆d has constant positive curvature: It is a spherical geometry restricted to the positive orthant with the metric distance measuring the arc length on a great circle."
    }, {
      "heading" : "2.2 Information geometry",
      "text" : "A divergence D is a smooth C3 differentiable dissimilarity measure [3] that allows to define a dual structure in Information Geometry [56, 15, 2] (IG). A f -divergence is defined for a strictly convex function f with f(1) = 0 by:\nIf (p : q) = d∑ i=0 λipf ( λiq λip ) .\nIt is a separable divergence since the d-variate divergence can be written as a sum of d univariate divergences: If (p : q) = ∑d i=0 If (λ i p : λ i q). The class of f -divergences plays an essential role in information theory since they are provably the only separable divergences that satisfy the information monotonicity property [2, 35]. That is, by coarse-graining the histograms we obtain lower-dimensional multinomials, say p′ and q′, such that 0 ≤ If (p′ : q′) ≤ If (p : q), see [2]. The Kullback-Leibler (KL) divergence ρIG is a f -divergence obtained for f(u) = − log u:\nρIG(p, q) = d∑ i=0 λip log λip λiq . (2)\nIt is an asymmetric non-metric distance: ρIG(p, q) 6= ρIG(q, p). In differential geometry, the structure of a manifold is defined by two components: (i) A metric tensor g that allows to define an inner product 〈·, ·〉p at each tangent space (for measuring vector lengths and angles between vectors), and (ii) a connection ∇ that defines parallel transport ∏∇p,q , i.e., a way to move a vector from one tangent plane Tp to any other one Tq . For FHR geometry, the implicitly used connection is called the Levi-Civita connection that is induced by the metric g: ∇LC = ∇(g). It is a metric connection since it ensures that 〈u, v〉p = 〈∏∇LCp,q u,∏∇LCp,q v〉q . The underlying information-geometric structure of KL is characterized by a pair of dual connections [2] ∇ = ∇(−1) (mixture connection) and ∇∗ = ∇(1) (exponential connection) that induces two dual geodesics (technically, ±1-autoparallel curves [15]). Those connections are said flat as they define two dual affine coordinate systems θ and η on which the θ- and η-geodesics are straight line segments, respectively. For multinomials, the expectation parameters are: η = (λ1, . . . , λd) and they 1-to-1 correspond to the natural parameters: θ = ( log λ 1\nλ0 , . . . , log λd λ0\n) . Thus in IG, we have two kinds\nof midpoint multinomials of p and q depending on whether we perform the (linear) interpolation on the θor the η-geodesics. Informally speaking, the dual connections∇(±1) are said coupled to the FIM since we have ∇+∇ ∗\n2 = ∇(g) = ∇LC. Those dual connections are not metric connections but enjoy the following property: 〈u, v〉p = 〈 ∏ p,qu, ∏∗ p,qv〉q , where ∏ = ∏∇ and ∏∗ = ∏∇∗ are the corresponding induced dual parallel transports. The geometry of f -divergences [3] is the α-geometry (for α = 3 + 2f ′′′(1)) with the dual ±α-connections, where ∇(α) = 1+α2 ∇∗ + 1−α2 ∇. The Levi-Civita metric connection is ∇LC = ∇(0). More generally, it was shown how to build a dual information-geometric structure for any divergence [3]. For example, we can build a dual structure from the symmetric Cauchy-Schwarz divergence [27]:\nρCS(p, q) = − log 〈λp, λq〉√\n〈λp, λp〉〈λq, λq〉 . (3)"
    }, {
      "heading" : "2.3 Hilbert simplex geometry",
      "text" : "In Hilbert Geometry [25] (HG), we are given a bounded convex domain C (here, C = ∆d), and the distance between any two points M ,M ′ of C is defined as follows: Consider the two intersection points AA′ of the line (MM ′) with C, and order them on the line so that we have A,M,M ′, A′. Then the Hilbert metric distance [14] is defined by:\nρHG(M,M ′) = { ∣∣∣log |A′M ||AM ′||A′M ′||AM | ∣∣∣ , M 6= M ′, 0 M = M ′.\n(4)\nIt is also called the Hilbert cross-ratio metric distance [24, 33]. Notice that we take the absolute value of the logarithm since the Hilbert distance is a signed distance [52]. When C is the unit ball, HG let us recover the Klein hyperbolic geometry [33]. When C is a quadric bounded convex domain, we obtain the Cayley-Klein hyperbolic geometry [12] which can be studied with the Riemannian structure and corresponding metric distance called the curved Mahalanobis distances [39, 38]. Cayley-Klein hyperbolic geometries have negative curvature.\nIn Hilbert geometry, the geodesics are straight Euclidean lines making it convenient for computation. Furthermore, the domain bounday ∂C need not to be smooth: One may also consider bounded polytopes [11]. This is particularly interesting for modeling ∆d, the d-dimensional open standard simplex. We call this geometry: The Hilbert simplex geometry. In Fig. (2), we show that the Hilbert distance\nbetween two multinomial distributions p (M ) and q (M ′) can be computed by finding the two intersection points of the line p(t) = (1 − t)p + tq with ∂∆d, denoted as t0 ≤ 0 and t1 ≥ 1. Then ρHG(p, q) = ∣∣∣log (1−t0)t1(−t0)(t1−1) ∣∣∣ = ∣∣∣log(1− 1t0 )− log(1− 1t1 )∣∣∣. The shape of balls in polytope-domain HG are Euclidean polytopes [33], as depicted in Figure 3. Furthermore, the Euclidean shape of the balls do not change with the radius. Hilbert balls have hexagons shapes in 2D [45], rhombic dodecahedra shapes in 3D, and are polytopes [33] with d(d+ 1) facets in dimension d. When the polytope domain is not a simplex, the combinatorial complexity of balls depends on the center location [45], see Figure 4. The HG of the probability simplex yields a non-Riemannian metric geometry because at infinitesimal radius value, the balls are polytopes and not ellipsoidal balls (corresponding to squared Mahalanobis distance balls used to visualize metric tensors [31]). The isometries in Hilbert polyhedral geometries are studied in [34]. In Appendix B, we recall that any Hilbert geometry induces a Finslerian structure that becomes Riemannian iff the boundary is an ellipsoid (yielding the hyperbolic Cayley-Klein geometries [52]). Let us notice that in Hilbert simplex/polytope geometry, the geodesics are not unique (see Figure 2 of [24]).\nTable 1 summarizes the characteristics of the three introduced geometries: FHR, IG, and HG."
    }, {
      "heading" : "3 Computing Hilbert distance in ∆d",
      "text" : "Let us first start by the simplest case: The 1D probability simplex ∆1, the space of Bernoulli distributions. Any Bernoulli distribution is represented by its activation probability p ∈ ∆1, and corresponds to a point in the interval ∆1 = (0, 1)."
    }, {
      "heading" : "3.1 1D probability simplex of Bernoulli distributions",
      "text" : "By definition, the Hilbert distance has the closed form:\nρHG(p, q) = ∣∣∣∣log q(1− p)p(1− q) ∣∣∣∣ = ∣∣∣∣log p1− p − log q1− q ∣∣∣∣ . Note that θp = log p1−p is the exponential family canonical parameters of the Bernoulli distribution.\nFor comparison, let us report the distance formula for KL and FHR: The Fisher information metric is given by: g = 1p + 1 1−p = 1 p(1−p) . The FHR distance is obtained by integration as:\nρFHR(p, q) = 2 arccos (√ pq + √ (1− p)(1− q) ) .\nThe KL divergence of the ±1-geometry is:\nρIG(p, q) = p log p q + (1− p) log 1− p 1− q .\nThe KL divergence belongs to the family of α-divergences [2]."
    }, {
      "heading" : "3.2 Arbitrary dimension case",
      "text" : "Given p, q ∈ ∆d, we first need to compute the intersection of line (pq) with the border of the ddimensional probability simplex to get the two intersection points p′ and q′ so that p′, p, q, q′ are ordered on (pq). Once this is done, we simply apply the formula in Eq. 4 to get the Hilbert distance.\nA d-dimensional simplex consists of d + 1 vertices with their corresponding (d − 1)-dimensional facets. For the probability simplex ∆d, let ei = (0, . . .︸ ︷︷ ︸\ni−1\n, 1, 0, . . . , 0) denote the d + 1 vertices of the\nstandard simplex embedded in the hyperplane H∆ : ∑d i=0 λ\ni = 1 in Rd+1. Let f\\j denote the simplex facets that is the convex hull of all points ei except ej : f\\j = hull(e1, . . . , ej−1, ej+1, ed+1). Let H\\j denote the hyperplane supporting this facet: The affine hull f\\j = affine(e1, . . . , ej−1, ej+1, ed+1).\nTo compute the two intersection points of (pq) with ∆d, a naive algorithm consists in computing the unique intersection point r of the line (pq) with each hyperplane H\\j (j = 0, · · · , d) and checking whether r belongs to f\\j , or not.\nA more efficient implementation given by Alg. (1) calculates the intersection points of the line x(t) = (1−t)p+tq with all facets. These intersection points are represented using the coordinate t. For example, x(0) = p, x(1) = q, and any intersection point with H\\j must satisfy either t ≤ 0 or t ≥ 1. Then, the two intersection points are obtained by t0 = max{t : t ≤ 0} and t1 = min{t : t ≥ 1}. This algorithm only requires O(d) time.\nLemma 1 The Hilbert distance in the probability simplex can be computed in optimal Θ(d) time.\nAlgorithm 1: Computing the Hilbert distance Data: Two points p = (λ0p, · · · , λdp), q = (λ0q, · · · , λdq) in the d-dimensional simplex ∆d Result: Their Hilbert distance ρHG(p, q) 1 begin 2 t0 ← −∞; t1 ← +∞; 3 for i = 0 · · · d do 4 if λip 6= λiq then 5 t← λip/(λip − λiq); 6 if t0 < t ≤ 0 then 7 t0 ← t; 8 else if 1 ≤ t < t1 then 9 t1 ← t;\n10 if t0 = −∞ or t1 = +∞ then 11 Output ρHG(p, q) = 0; 12 else if t0 = 0 or t1 = 1 then 13 Output ρHG(p, q) =∞; 14 else 15 Output ρHG(p, q) = ∣∣∣log(1− 1t0 )− log(1− 1t1 )∣∣∣;\nOnce an arbitrary distance ρ is chosen, we can define a ball centered at c and of radius r asBρ(c, r) = {x : ρ(c, x) ≤ r}. Figure 3 displays the hexagonal shapes of the Hilbert balls for various center locations in ∆2.\nTheorem 1 (Balls in a simplicial Hilbert geometry [33]) A ball in a Hilbert simplex geometry has a Euclidean polytope shape with d(d+ 1) facets.\nNote that when the domain is not simplicial, the Hilbert balls can have varying combinatorial complexity depending on the center location. In 2D, the Hilbert ball polygonal shapes can range from s to 2s edges where s is the number of edges of the boundary Hilbert domain ∂C.\nSince a Riemannian geometry is locally defined by a metric tensor, at infinitesimal scales, Riemannian balls have Mahalanobis smooth ellipsoidal shapes: Bρ(c, r) = {x : (x − c)>g(c)(x − c) ≤ r2}. This property allows one to visualize Riemannian metric tensors [31]. Thus we conclude that:\nLemma 2 ([33]) Hilbert simplex geometry is a non-manifold metric length space.\nAs a remark, let us notice that slicing a simplex with a hyperplane does not always produce a lowerdimensional simplex. For example, slicing a tetrahedron by a plane yields either a triangle or a quadrilateral. Thus the restriction of a d-dimensional ball B in a Hilbert simplex geometry ∆d to a hyperplane H is a (d − 1)-dimensional ball B′ = B ∩H of varying combinatorial complexity corresponding to a ball in the induced Hilbert sub-geometry with convex sub-domain H ∩∆d."
    }, {
      "heading" : "3.3 Visualizing distance profiles",
      "text" : "Figure 5 displays the distance profile from any point in the probability simplex to a fixed reference point (trinomial) for the following common distance measures [15]: Euclidean distance (metric), CauchySchwarz (CS) divergence, Hellinger distance (metric), Fisher-Rao distance (metric), KL divergence and the Hilbert simplicial distance (metric). The Euclidean and Cauchy-Schwarz divergence are clipped to ∆2. The Cauchy-Schwarz distance is a projective distance ρCS(λp, λ′q) = ρCS(p, q) for any λ, λ′ > 0, see [46]."
    }, {
      "heading" : "4 Center-based clustering",
      "text" : "We concentrate on comparing the efficiency of Hilbert simplex geometry for clustering multinomials. We shall compare the experimental results of k-means++ and k-center multinomial clustering for the three distances: Rao and Hilbert metric distances, and KL divergence. We describe how we implemented those clustering algorithms when dealing with a Hilbert distance.\n0 1 Euclidean distance CS divergence Riemannian distance KL divergence\n0\n1 Hellinger distance Hilbert distance L1 distance (-2.0)-divergence\n0\n1 (-1.5)-divergence (-1.0)-divergence (-0.5)-divergence (+0.0)-divergence\n0 1 0\n1 (+0.5)-divergence\n0 1\n(+1.0)-divergence\n0 1\n(+1.5)-divergence\n0 1\n(+2.0)-divergence\n0.00\n0.08\n0.16\n0.24\n0.0\n0.1\n0.2\n0.3\n0.4\n0.00\n0.15\n0.30\n0.45\n0.60\n0.00\n0.15\n0.30\n0.45\n0.00\n0.05\n0.10\n0.15\n0.20\n0.0\n0.5\n1.0\n1.5\n2.0\n0.00\n0.15\n0.30\n0.45\n0.0 0.2 0.4 0.6 0.8 1.0\n0.0 0.2 0.4 0.6 0.8 1.0\n0.0 0.2 0.4 0.6 0.8 1.0\n0.0 0.2 0.4 0.6 0.8 1.0\n0.0 0.2 0.4 0.6 0.8 1.0\n0.0 0.2 0.4 0.6 0.8 1.0\n0.0 0.2 0.4 0.6 0.8 1.0\n0.0 0.2 0.4 0.6 0.8 1.0\n0.0 0.2 0.4 0.6 0.8 1.0\n(a) Reference point (3/7,3/7,1/7)\n4.1 k-Means++ clustering The celebrated k-means clustering minimizes the sum of cluster variances, where each cluster has a center representative element. When dealing with k = 1 cluster, the center (also called centroid or cluster prototype) is the center of mass. For an arbitrary dissimilarity measure D(· : ·), the centroid c defined as the minimizer of\nED(Λ, c) = 1\nn n∑ i=1 D(λi : c),\nmay not be available in closed form. Nevertheless, using a generalization of the k-means initialization [6] (picking randomly seeds), one can bypass the centroid computation, and yet guarantee probabilistically a good clustering.\nLet C = {c1, . . . , ck} denote the set of k cluster centers. Then the generalized k-means energy to minimize is defined by:\nED(Λ, C) = 1\nn n∑ i=1 min j∈{1,...,k} D(λi : cj).\nBy defining the distance D(λ,C) = minj∈{1,...,k}D(λ : cj) of a point to a set, we rewrite the objective function as follows: ED(Λ, C) = 1n ∑n i=1D(λi, C). Let E ∗ D(Λ, k) = minC : |C|=k ED(Λ, C) denote the global minimum. The k-means++ seeding proceeds for an arbitrary divergenceD as follows: Pick uniformly at random at first seed c1, and iteratively choose the (k − 1) remaining seeds according to the following probability distribution:\nPr(ci = x) = D(x, {c1, . . . , ci−1})∑ y∈X D(y, {c1, . . . , ci−1}) .\nSince its inception (2007), this k-means++ seeding has been extensively studied [7]. We state the general theorem established in [43]:\nTheorem 2 (Generalized k-means++ performance [43]) Let κ1 and κ2 be two constants such that κ1 defines the quasi-triangular inequality property:\nD(x : z) ≤ κ1 (D(x : y) +D(y : z)) ,∀x, y, z ∈ ∆d, and κ2 handles the symmetry inequality:\nD(x : y) ≤ κ2D(y : x),∀x, y ∈ ∆d. Then the generalized k-means++ seeding guarantees with high probability a configuration C of cluster centers such that: ED(Λ, C) ≤ 2κ21(1 + κ2)(2 + log k)E∗D(Λ, k). (5) The ratio ED(Λ,C)E∗D(Λ,k) is called the competitive factor [6]. The seminal result of ordinary k-means++ yields a 8(2 + log k)-competitive factor. Since divergences may be asymmetric, one can further consider mixed divergence M(p : q : r) = λD(p : q) + (1 − λ)D(q : r) for λ ∈ [0, 1], and extend the k-means++ seeding procedure and analysis, see [44].\nNote that squared metric distances are not metric because they do not satisfy the triangular inequality. For example, the squared Euclidean distance is not a metric but it satisfies the 2-quasi triangular inequality.\nAs an empirical study, we randomly generate n = 105 tuples (x, y, z) based on the uniform distribution in ∆d. For each tuple (x, y, z), we evaluate the ratio\nκ1 = D(x : z)\nD(x : y) +D(y : z) .\nFigure 7 shows the statistics for three different choices of D: (1) D(x : y) = ρ2FHR(x, y); (2) D(x : y) = 1 2KL(x : y) + 1 2KL(y : x); (3) D(x : y) = ρ 2 HG(x, y). We find experimentally that κ1 is upper bounded by 2 for both ρFHR and ρHG, while the average κ1 value is much smaller. For all the compared distances, κ2 = 1. Therefore ρFHR and ρHG have better k-means++ performance guarantee as compared to ρIG.\nWe state the following general performance theorem:\nTheorem 3 (k-means++ in an inner product space) In any inner product space (X , 〈·, ·〉), the kmeans++ seeding is 16(2 + log k)-competitive.\nProof: In an inner product space (X , 〈·, ·〉), we can define the norm ‖x‖ = √ 〈x, x〉 and induced distance D(x, y) = ‖x− y‖. Furthermore, the parallelogram law holds in an inner product space:\n2‖x‖2 + 2‖y‖2 = ‖x+ y‖2 + ‖x− y‖2."
    }, {
      "heading" : "It follows that ‖x′ − y′‖2 ≤ 2(‖x′‖2 + ‖y′‖2) since ‖x′ + y′‖2 ≥ 0. Let x′ = x− z and y′ = y − z so",
      "text" : "that x′ − y′ = x− y and, we get the 2-quasi triangular inequality.\n‖x− y‖2 ≤ 2(‖x− z‖2 + ‖z − y‖2).\nPlugging κ1 = 2 and κ2 = 1 in Eq. 5, we get the 16(2 + log k)-competitive factor. The KL divergence can be interpreted as a separable Bregman divergence [1]. The Bregman kmeans++ performance has been studied in [1, 36], and a competitive factor of O( 1µ ) is reported using the notion of Bregman µ-similarity (that is suited for data-sets on a compact domain).\nIn [21], spherical k-means++ is studied with respect to the distance dS(x, y) = 1−〈x, y〉 for any pair of points x, y on the unit sphere. Since 〈x, y〉 = ‖x‖2‖y‖2 cos(θx,y) = cos(θx,y), we have dS(x, y) = 1 − cos(θx,y), where θx,y denotes the angle between unit vectors x and y. This distance is called the cosine distance since it amounts to one minus the cosine similarity. Notice that the cosine distance is related to the squared Euclidean distance via the identity: dS(x, y) = 12‖x− y‖2. The cosine distance is different from the spherical distance that relies on the arccos function.\nWe need to report a bound for the squared Hilbert symmetric distance (κ2 = 1). In [33] (Theorem 3.3), it was shown that Hilbert geometry of a bounded convex domain C is isometric to a normed vector space iff C is an open simplex: (∆d, ρHG) ' (Vd, ‖ · ‖H) where ‖x‖H is the corresponding norm. Appendix A recalls the construction due to De La Harpe [24] in 1991. We write NH for short for this equivalent normed Hilbert geometry. The squared Hilbert distance fails the triangle inequality, and it is not a distance induced by an inner product (see Appendix A).\nHowever, in a finite dimensional vector space, all norms are “equivalent,” meaning that there exist two positive constants α and β such that:\nα‖v‖2 ≤ ‖v‖H ≤ β‖v‖2. (6)\nFigure 8 shows an empirical range of the ratio ‖v‖H‖v‖l for l = 1, · · · , 4. The ratio α and β depend on the dimension of the simplex ∆d. For example, when d = 10, one can observe that empirically α ≈ 0.7, β ≈ 1.4.\nWe therefore get the following lemma:\nLemma 3 (Quasi-triangular inequality in Hilbert simplex geometry) In a Hilbert simplex geometry, κ1 ≤ 2 ( β α )2 .\nProof: From α‖v‖2 ≤ ‖v‖H ≤ β‖v‖2, we get:\nα2‖vp − vq‖22 ≤ ρ2HG(p, q) = ‖vp − vq‖2H ≤ β2‖vp − vq‖22,\nwhere vp and vq are the equivalent points in the normed space corresponding to point p and q, respectively. Let DE(vp, vq) = ‖vp − vq‖ denote the Euclidean distance. Since ρ2HG(p, q) ≤ β2D2E(vp, vq) and that D2E satisfies the 2-quasi-triangular inequality, we get:\nρ2HG(p, q) ≤ 2β2(D2E(vp, vr) +D2E(vr, vq)), ∀vp, vq, vr ∈ Vd.\nBut DE(vx, vy)2 ≤ 1α2 ρ2HG(x, y). It follows that:\nρ2HG(p, q) ≤ 2 β2\nα2 (ρ2HG(p, r) + ρ 2 HG(r, q)).\nTherefore the squared Hilbert distance in a simplex satisfies the κ1-triangular inequality for κ1 = 2 ( β α )2 .\nThus we get by applying Theorem 3:\nTheorem 4 (k-means++ in Hilbert simplex geometry) The k-means++ seeding in a Hilbert simplex geometry in fixed dimension is 16 ( β α )4 (2 + log k)-competitive.\nThe constants α and β depend on the dimension of the Hilbert simplex geometry. Figure 6 displays the results of a k-means++ clustering in Hilbert simplex geometry for k ∈ {3, 5}.\nNote that for a given data set, we can compute κ1 or κ2 by inspecting triples and pairs of points, and get data-dependent competitive factor improving the bounds mentioned above.\n4.2 k-Center clustering Let X be a finite point set. The cost function for a k-center clustering with centers C (|C| = k) is:\nmax x∈X min y∈C D(x : y).\nThe farthest first traversal heuristic of Gonzalez [23] has a guaranteed approximation factor of 2 for any metric distance (see Algorithm 2).\nAlgorithm 2: A 2-approximation of the k-center clustering for any metric distance ρ. Data: A set X , a number k of clusters and a metric ρ Result: A 2-approximation of the k-center clustering 1 begin 2 c1 ← RandomPointOf(X ); 3 C ← {c1}; 4 for i = 2 · · · k do 5 ci = arg maxx∈X ρ(x, C); 6 C ← C ∪ {ci};\n7 Output C;\nAlgorithm 3: Geodesic walk for approximating the Hilbert minimax center, generalizing [9] Data: A set of points p1, · · · , pn ∈ ∆d. The maximum number T of iterations. Result: c = arg minc maxi ρHG(pi, c) 1 begin 2 c0 ← RandomPointOf({p1, · · · , pn}); 3 for t = 1, · · · , T do 4 p← arg maxpi ρHG(pi, ct−1); 5 ct ← ct−1#ρ t\nt+1\np;\n6 Output cT ;\nIn order to use the k-center clustering algorithm described in Algorithm 4, we need to be able to compute a 1-center (or minimax center) for the Hilbert simplex geometry, that is the Minimum Enclosing Ball (MEB, also called the Smallest Enclosing Ball, SEB).\nWe may consider the SEB equivalently either in ∆d or in the normed space Vd. In both spaces, the shapes of the balls are convex. Let X = {x1, . . . , xd} denote the point set in ∆d, and V = {v1, . . . , vd} the equivalent point set in the normed vector space (following the mapping explained in Appendix A). Then the SEBs BHG(X ) in ∆d and BNH(V) in Vd have radii r∗HG and r∗H defined by:\nr∗HG = min c∈∆d max i∈{1,...,n} ρHG(xi, c), (7)\nr∗H = min v∈Vd max i∈{1,...,n} ‖vi − v‖H . (8)\nIt follows from the inequalities of Eq. 6 that:\nαr∗E ≤ rH ≤ βr∗E ,\nwhere r∗E is the radius of the SEB with respect to the Euclidean distance of the points v1 . . . , vn. The SEB in the normed vector space (Vd, ‖·‖H) amounts to find the minimum covering norm polytope of a finite point set. This problem has been well-studied in computational geometry: See [53, 13, 48]. By considering the equivalent Hilbert norm polytope with d(d+ 1) facets, we state the result of [53]:\nTheorem 5 (SEB in Hilbert polytope normed space [53]) A (1 + )-approximation of the SEB in Vd can be computed in O(d3 n ).\nWe shall now report two algorithms for computing the SEBs: One exact algorithm in Vd that do not scale well in high dimensions, and one approximation algorithm in ∆d that works well for large dimensions."
    }, {
      "heading" : "4.2.1 Exact smallest enclosing ball in a Hilbert simplex geometry",
      "text" : "Given a finite point set {x1, . . . , xn} ∈ ∆d, we define the Smallest Enclosing Ball (SEB) in Hilbert simplex geometry as:\nr∗ = min c∈∆d max i∈{1,...,n} ρHG(c, xi). (9)\nThe radius of the SEB is r∗. Consider the equivalent problem of finding the SEB in the isometric normed vector space via the mapping reported in Appendix A. To each simplex point xi corresponds a point vi in the normed vector space Vd.\nFigure 10 displays some examples of the exact smallest enclosing balls in the Hilbert simplex geometry and the corresponding normed vector space.\nTo compute the SEB, one may also consider the generic LP-type randomized algorithm [41]. We notice that an enclosing ball for a point set in general position as a number k of points on the border of the ball, with 2 ≤ k ≤ d(d+1)2 . Let D = d(d+1) 2 denote the varying size of the combinatorial basis: Then we can apply the LP-type framework (we check the axioms of locality and monotonicity [54]) to solve efficiently for the SEBs.\nTheorem 6 (Smallest Enclosing Hilbert ball is LP-type [59, 54]) The smallest enclosing Hilbert ball amounts to find the smallest enclosing ball in a vector space with respect to a polytope norm that can be solved using a LP-type randomized algorithm.\nThe Enclosing Ball Decision Problem [42] (EBDP) asks for a given value r, whether r ≥ r∗ or not. The decision problem amounts to find whether a set {rBV + vi} of translates can be stabbed by a point [42]: That is, whether ∩ni=1(rBV + vi) is empty or not. Since the translates are polytopes with d(d+ 1) facets, this can be solved in linear time using Linear Programming.\nTheorem 7 (Enclosing Hilbert Ball Decision Problem) The decision problem to test whether r ≥ r∗ or not can be solved by Linear Programming.\nThis yields a simple scheme to approximate the optimal value r∗: Let r0 = maxi∈{1,...,n} ‖vi−v1‖H . Then r∗ ∈ [ r02 , r0] = [a0, b0]. At stage i, perform a dichotomic search on [ai, bi] by answering the decision problem for ri = bi−ai2 , and update the radius range accordingly, see [42].\nHowever, the LP-type randomized algorithm or the decision problem-based algorithm do not scale well with dimensions. Next, we introduce a simple approximation algorithm that relies on the fact that the line segment [pq] is a geodesic in Hilbert simplex geometry. (Geodesics are not unique, see Figure 2 of [24])"
    }, {
      "heading" : "4.2.2 Geodesic bisection approximation heuristic",
      "text" : "In Riemannian geometry, the 1-center can be arbitrarily finely approximated by a simple geodesic bisection algorithm [9, 5] This algorithm can be extended to HG straightforwardly as detailed in Algorithm 3: The algorithm first picks up a point c0 at random from X for the initial center, computes the farthest point fi (with respect to the distance ρ), and walk on the geodesic from c0 to fi by a certain amount to define c1, etc. For an arbitrary distance ρ, we define the operator #ρα as follows:\np#ραq = v = γ(p, q, α), ρ(p : v) = αρ(p : q),\nwhere γ(p, q, α) is the geodesic passing through p and q, and parameterized by α (0 ≤ α ≤ 1). When the equations of the geodesics are explicitly known, we can either get a closed form solution for #ρα or perform a bisection search on α to approximately compute α′ such that ρ(p : γ(p, q, α′)) = αρ(p : q). See [37] for an extension and analysis in hyperbolic geometry.\nFurthermore, this iterative algorithm implies a core-set [10] (namely, the set of farthest points visited when iterating the geodesic walks) that is useful for clustering large data-sets [8]. See [13] for coreset results concerning containment problems with respect to a convex homothetic object (the equivalent Hilbert polytope norm in our case).\nPanigrahy [48] described as simple algorithm dubbed MINCON for finding an approximation of the Minimum Enclosing Polytope. The algorithm induces a core-set of size O( 1 2 ) although the theorem is challenged in [13].\nSee Fig (9) to get an intuitive idea on the experimental convergence rate of Algorithm 3. Thus by combining the k-center seeding of Gonzalez [23] with the iteration Lloyd-like batched iterations, we get an efficient k-center clustering algorithm for the FHR and Hilbert metric geometries. When\nAlgorithm 4: k-center clustering Data: A set of points p1, · · · , pn ∈ ∆d. A distance measure ρ on ∆d. The maximum number k of\nclusters. The maximum number T of iterations. Result: A clustering scheme assigning each pi a label li ∈ {1, . . . , k}\n1 begin 2 Randomly pick k cluster centers c1, . . . , ck using the kmeans++ heuristic; 3 for t = 1, · · · , T do 4 for i = 1, · · · , n do 5 li ← arg minkl=1 ρ(pi, cl); 6 for l = 1, · · · , k do 7 cl ← arg minc maxi:li=l ρ(pi, c);\n8 Output {li}ni=1;\ndealing with the Kullback-Leibler divergence, we use the fact that KL is a Bregman divergence, and use the 1-center algorithm described in [47, 40] (approximation in any dimension) and [41] (exact but limited to small dimensions).\nSince Hilbert simplex geometry is isomorphic to a normed vector space [33] with a polytope norm with d(d + 1) facets, the Voronoi diagram in Hilbert geometry of ∆d amounts to compute a Voronoi diagram with respect to a polytope norm [30, 51, 19]."
    }, {
      "heading" : "5 Experiments",
      "text" : "We generate a dataset consisting of a set of clusters in a high dimensional statistical simplex ∆d. Each cluster is generated independently as follows. We first pick a random c = (λ0c , . . . , λ d c) based on the uniform distribution on ∆d. Then we generate a random sample p = (λ0, . . . , λd) based on\nλi = exp(log λic + σ i)∑d i=0 exp(log λ i c + σ i)\nwhere σ > 0 is a noise level parameter, and each i follows independently a standard Gaussian distribution. Let σ = 0, we get λi = λic. Therefore p is randomly distributed around c. We repeat generating random samples for each cluster center, and make sure that different clusters have almost the same number of samples. Then we run k-center clustering in Alg. (4) based on the configurations n ∈ {50, 100}, d ∈ {9, 255}, σ ∈ {0.5, 0.9, 1.3}, ρ ∈ {ρFHR, ρIG, ρHG}. The number of clusters k is set to the true number of clusters to avoid model selection. For each configuration, we repeat the clustering experiment based on 300 different random datasets. The performance is measured by the cluttering accuracy, which is the percentage of clustering labels coinciding with the ground truth labels during the data generating process.\nThe results are shown in Table 2. The large variance of the accuracy is because that each experiment is performed on different datasets given by the same generator based on different random seeds. Generally, the performance deteriorates as we increase the number of clusters, increase the noise level or decrease the dimensionality, which have the same effect to reduce the gap among the clusters.\nThe key comparison is the three columns ρFHR, ρHG and ρIG, as they are based on exactly the same algorithm (k-center) with the only difference being the underlying geometry. We see clearly that the performance of the three compared geometries presents the order HG > FHR > IG. The performance\nof HG is superior to the other two geometries, especially when the noise level is large. Intuitively, the Hilbert balls are more compact and therefore can better capture the clustering structure (see Fig. (1)).\nThe column ρIG (k-means) is the k-means clustering based on ρIG. It shows better performance than ρIG because k-means is more robust than k-center to noise. Ideally we should compare k-means based on FHR, IG and HG. However the centroid computation of FHR and HG are not developed yet. This is left to future work.\nThe column ρEUC represents k-center based on the Euclidean enclosing ball. It shows the worst scores because the intrinsic geometry of the probability simplex is far from being Euclidean."
    }, {
      "heading" : "6 Conclusion",
      "text" : "We introduced the Hilbert metric distance and its underlying non-Riemannian geometry for modeling the space of multinomials of the open probability simplex, and compared experimentally this geometry with the traditional differential-geometric modelings (either FHR metric connection or dually coupled nonmetric affine connection of information geometry [2]) for clustering tasks. The main feature of HG is that it is a metric non-manifold geometry where geodesics are straight (Euclidean) line segments. For simplex domains, the Hilbert balls have fixed combinatorial (Euclidean) polytope structures, and HG is known to be isometric to a normed space [24, 22]. This latter isometry allows one to generalize easily the standard\nproofs of clustering (e.g., k-means or k-center). We demonstrated it for the k-means++ competitive performance analysis, and for the convergence of the 1-center heuristic [9] (smallest enclosing Hilbert ball allows one to implement efficiently the k-center clustering). Our experimental k-means++/k-center comparisons of HG algorithms with the manifold modeling approach yield striking superior performance: This may be explained by the sharpness of Hilbert balls with respect to the FHR/IG ball profiles.\nChentsov [17] defined statistical invariance on a probability manifold under Markov morphisms, and proved that the Fisher Information Metric (FIM) is the unique Riemannian metric (up to rescaling) for multinomials. However, this does not rule out that other distances (with underlying geometric structures) may be used to model statistical manifolds (eg., Finsler statistical manifolds [16, 55], or the total variation distance — the only metric f -divergence [29]). Defining statistical invariance related to geometry is the cornerstone problem of information geometry that can be tackled from many directions (see [20] and references therein for a short review). We hope to have fostered interest in considering the potential of Hilbert probability simplex geometry in artificial intelligence. One future direction is to consider the Hilbert metric for regularization and sparsity in machine learning (due to its equivalence with a polytope normed distance).\nOur Python codes are freely available online for reproducible research: https://www.lix.polytechnique.fr/˜nielsen/HSG/"
    }, {
      "heading" : "B Hilbert geometry with Finslerian/Riemannian structures",
      "text" : "In a Riemannian geometry, each tangent plane TpM of the d-dimensional manifold M is equivalent to Rd: TpM ' Rd. The inner product at each tangent plane TpM can be visualized by an ellipsoid shape, a convex symmetric object centered at point p. In a Finslerian geometry, a norm ‖ · ‖p is defined in each tangent plane TpM , and this norm is visualized as a symmetric convex object with non-empty interior. Finslerian geometry thus generalizes Riemannian geometry by taking into account generic symmetric convex objects instead of ellipsoids for inducing norms at each tangent plane. Any Hilbert geometry induced by a compact convex domain C can be expressed by en equivalent Finslerian geometry by defining the norm in Tp at p as follows [58]:\n‖v‖p = FC(p, v) = ‖v‖ 2\n( 1\npp+ +\n1\npp−\n) , (13)\nwhere ‖ · ‖ is an arbitrary norm on Rd, and p+ and p− are the intersection points of the line passing through p with direction v:\np+ = p+ t+v, p− = p+ t−v\nFC is the Finsler metric. A geodesic γ in a Finslerian geometry satisfies:\ndC(γ(t1), γ(t2)) = ∫ t2 t1 FC(γ(t), γ̇(t))dt. (14)\nIn TpM , a ball of center c and radius r is defined by:\nB(c, r) = {v : FC(c, v) ≤ r}. (15)\nThus any Hilbert geometry induces an equivalent Finslerian geometry, and since Finslerian geometries include Riemannian geometries, one may wonder which Hilbert geometries induce Riemannian structures? The only Riemannian geometries induced by Hilbert geometries are the hyperbolic Cayley-Klein geometries [52, 39, 38] with the domain C being an ellipsoid. The Finslerian modeling of information geometry has been studied in [16, 55].\n1 Consider A = (1/3, 1/3, 1/3), B = (1/6, 1/2, 1/3), C = (1/6, 2/3, 1/6) and D = (1/3, 1/2, 1/6). Then 2AB2 + 2BC2 = 4.34 but AC2 +BD2 = 3.84362411135.\nThere is not a canonical way of defining measures in a Hilbert geometry since Hilbert geometries are Finslerian but not necessary Riemannian geometries [58]. The Busemann measure is defined according to the Lebesgue measure λ of Rd: Let Bp denote the unit ball wrt. to the Finsler norm at point p ∈ C, and Be the Euclidean unit ball. Then the Busemann measure for a Borel set B is defined by [58]:\nµC(B) = ∫ B λ(Be) λ(Bp) dλ(p).\nThe existence and uniqueness of center points of a probability measure in Finsler geometry have been investigated in [4].\n0 1 Riemannian center IG center\n0 1 0\n1 Hilbert center\n0 1\nL1 center\n0.00\n0.25\n0.50\n0.75\n1.00\n0.0\n0.2\n0.4\n0.6\n0\n1\n2\n3\n4\n0.0\n0.2\n0.4\n0.6\n0.8\n(a) Point Cloud 1\n0\n1 Riemannian center IG center\n0 1 0\n1 Hilbert center\n0 1\nL1 center\n0.0\n0.3\n0.6\n0.9\n1.2\n0.0\n0.2\n0.4\n0.6\n0.8\n0\n1\n2\n3\n4\n0.0\n0.2\n0.4\n0.6\n0.8\n(b) Point Cloud 2"
    } ],
    "references" : [ {
      "title" : "Bregman clustering for separable instances",
      "author" : [ "Marcel R Ackermann", "Johannes Blömer" ],
      "venue" : "In Scandinavian Workshop on Algorithm Theory,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2010
    }, {
      "title" : "Information Geometry and Its Applications. Applied Mathematical Sciences",
      "author" : [ "Shun-ichi Amari" ],
      "venue" : null,
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2016
    }, {
      "title" : "Information geometry of divergence functions",
      "author" : [ "Shun-ichi Amari", "Andrzej Cichocki" ],
      "venue" : "Bulletin of the Polish Academy of Sciences: Technical Sciences,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2010
    }, {
      "title" : "Medians and means in Finsler geometry",
      "author" : [ "Marc Arnaudon", "Frank Nielsen" ],
      "venue" : "LMS Journal of Computation and Mathematics,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2012
    }, {
      "title" : "On approximating the Riemannian 1-center",
      "author" : [ "Marc Arnaudon", "Frank Nielsen" ],
      "venue" : "Computational Geometry,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2013
    }, {
      "title" : "k-means++: The advantages of careful seeding",
      "author" : [ "David Arthur", "Sergei Vassilvitskii" ],
      "venue" : "In ACM- SIAM symposium on Discrete algorithms,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2007
    }, {
      "title" : "Approximate k-means++ in sublinear time",
      "author" : [ "Olivier Bachem", "Mario Lucic", "S. Hamed Hassani", "Andreas Krause" ],
      "venue" : "In AAAI,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2016
    }, {
      "title" : "Scalable and distributed clustering via lightweight coresets",
      "author" : [ "Olivier Bachem", "Mario Lucic", "Andreas Krause" ],
      "venue" : "arXiv preprint arXiv:1702.08248,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2017
    }, {
      "title" : "Smaller core-sets for balls",
      "author" : [ "Mihai Bâdoiu", "Kenneth L. Clarkson" ],
      "venue" : "In ACM-SIAM Symposium on Discrete Algorithms,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2003
    }, {
      "title" : "Optimal core-sets for balls",
      "author" : [ "Mihai Bădoiu", "Kenneth L Clarkson" ],
      "venue" : "Computational Geometry,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2008
    }, {
      "title" : "Hilbert geometry of polytopes",
      "author" : [ "Andreas Bernig" ],
      "venue" : "Archiv der Mathematik,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2009
    }, {
      "title" : "Beyond Mahalanobis metric: Cayley-Klein metric learning",
      "author" : [ "Yanhong Bi", "Bin Fan", "Fuchao Wu" ],
      "venue" : "In CVPR,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2015
    }, {
      "title" : "No dimension-independent core-sets for containment under homothetics",
      "author" : [ "René Brandenberg", "Stefan König" ],
      "venue" : "Discrete & Computational Geometry,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2013
    }, {
      "title" : "The Geometry of Geodesics",
      "author" : [ "H. Busemann" ],
      "venue" : "Pure and Applied Mathematics. Elsevier Science,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2011
    }, {
      "title" : "Geometric Modeling in Probability and Statistics",
      "author" : [ "Ovidiu Calin", "Constantin Udriste" ],
      "venue" : "Mathematics and Statistics. Springer International Publishing,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2014
    }, {
      "title" : "Statistical Decision Rules and Optimal Inference. Translations of mathematical monographs",
      "author" : [ "N.N. Cencov" ],
      "venue" : "American Mathematical Society,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2000
    }, {
      "title" : "Finding metric structure in information theoretic clustering",
      "author" : [ "Kamalika Chaudhuri", "Andrew McGregor" ],
      "venue" : "In COLT,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2008
    }, {
      "title" : "Voronoi polytopes for polyhedral norms on lattices",
      "author" : [ "Michel Deza", "Mathieu Dutour Sikirić" ],
      "venue" : "Discrete Applied Mathematics,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2015
    }, {
      "title" : "Chentsov’s theorem for exponential families",
      "author" : [ "J.G. Dowty" ],
      "venue" : "ArXiv e-prints,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2017
    }, {
      "title" : "Spherical k-means++ clustering",
      "author" : [ "Yasunori Endo", "Sadaaki Miyamoto" ],
      "venue" : "In Modeling Decisions for Artificial Intelligence,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2015
    }, {
      "title" : "Hilbert metrics and Minkowski norms",
      "author" : [ "Thomas Foertsch", "Anders Karlsson" ],
      "venue" : "Journal of Geometry,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2005
    }, {
      "title" : "Clustering to minimize the maximum intercluster distance",
      "author" : [ "Teofilo F Gonzalez" ],
      "venue" : "Theoretical Computer Science,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 1985
    }, {
      "title" : "On Hilbert’s metric for simplices, volume 1, pages 97–118",
      "author" : [ "Pierre De La Harpe" ],
      "venue" : null,
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 1991
    }, {
      "title" : "Spaces of statistical parameters",
      "author" : [ "Harold Hotelling" ],
      "venue" : "In Bulletin AMS,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 1930
    }, {
      "title" : "The Cauchy-Schwarz divergence and Parzen windowing: Connections to graph theory and mercer kernels",
      "author" : [ "Robert Jenssen", "Jose C Principe", "Deniz Erdogmus", "Torbjørn Eltoft" ],
      "venue" : "Journal of the Franklin Institute,",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2006
    }, {
      "title" : "Geometrical Foundations of Asymptotic Inference",
      "author" : [ "Robert E. Kass", "Paul W. Vos" ],
      "venue" : "Wiley- Interscience,",
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 1997
    }, {
      "title" : "Confliction of the convexity and metric properties in f -divergences",
      "author" : [ "Mohammadali Khosravifard", "Dariush Fooladivanda", "T Aaron Gulliver" ],
      "venue" : "IEICE Transactions on Fundamentals of Electronics, Communications and Computer Sciences,",
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 2007
    }, {
      "title" : "Minisum hyperspheres, volume 51",
      "author" : [ "Mark-Christoph Körner" ],
      "venue" : "Springer Science & Business Media,",
      "citeRegEx" : "30",
      "shortCiteRegEx" : "30",
      "year" : 2011
    }, {
      "title" : "Visualization and Processing of Tensor Fields: Advances and Perspectives",
      "author" : [ "David H Laidlaw", "Joachim Weickert" ],
      "venue" : "Springer Science & Business Media,",
      "citeRegEx" : "31",
      "shortCiteRegEx" : "31",
      "year" : 2009
    }, {
      "title" : "Learning Riemannian metrics",
      "author" : [ "Guy Lebanon" ],
      "venue" : "In UAI, pages 362–369,",
      "citeRegEx" : "32",
      "shortCiteRegEx" : "32",
      "year" : 2002
    }, {
      "title" : "Birkhoffs version of Hilberts metric and its applications in analysis",
      "author" : [ "Bas Lemmens", "Roger Nussbaum" ],
      "venue" : "Handbook of Hilbert Geometry,",
      "citeRegEx" : "33",
      "shortCiteRegEx" : "33",
      "year" : 2014
    }, {
      "title" : "Isometries of polyhedral hilbert geometries",
      "author" : [ "Bas Lemmens", "Cormac Walsh" ],
      "venue" : "Journal of Topology and Analysis,",
      "citeRegEx" : "34",
      "shortCiteRegEx" : "34",
      "year" : 2011
    }, {
      "title" : "A note on divergences",
      "author" : [ "Xiao Liang" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "35",
      "shortCiteRegEx" : "35",
      "year" : 2016
    }, {
      "title" : "Worst-case and smoothed analysis of k-means clustering with Bregman divergences",
      "author" : [ "Bodo Manthey", "Heiko Röglin" ],
      "venue" : "Journal of Computational Geometry,",
      "citeRegEx" : "36",
      "shortCiteRegEx" : "36",
      "year" : 2013
    }, {
      "title" : "Approximating covering and minimum enclosing balls in hyperbolic geometry",
      "author" : [ "Frank Nielsen", "Gaëtan Hadjeres" ],
      "venue" : "In International Conference on Networked Geometric Science of Information,",
      "citeRegEx" : "37",
      "shortCiteRegEx" : "37",
      "year" : 2015
    }, {
      "title" : "Classification with mixtures of curved Mahalanobis metrics",
      "author" : [ "Frank Nielsen", "Boris Muzellec", "Richard Nock" ],
      "venue" : "In IEEE International Conference on Image Processing (ICIP),",
      "citeRegEx" : "38",
      "shortCiteRegEx" : "38",
      "year" : 2016
    }, {
      "title" : "Large margin nearest neighbor classification using curved Mahalanobis distances",
      "author" : [ "Frank Nielsen", "Boris Muzellec", "Richard Nock" ],
      "venue" : null,
      "citeRegEx" : "39",
      "shortCiteRegEx" : "39",
      "year" : 2016
    }, {
      "title" : "On approximating the smallest enclosing Bregman balls",
      "author" : [ "Frank Nielsen", "Richard Nock" ],
      "venue" : "In Proceedings of the twenty-second annual symposium on Computational geometry,",
      "citeRegEx" : "40",
      "shortCiteRegEx" : "40",
      "year" : 2006
    }, {
      "title" : "On the smallest enclosing information disk",
      "author" : [ "Frank Nielsen", "Richard Nock" ],
      "venue" : "Information Processing Letters,",
      "citeRegEx" : "41",
      "shortCiteRegEx" : "41",
      "year" : 2008
    }, {
      "title" : "Approximating smallest enclosing balls with applications to machine learning",
      "author" : [ "Frank Nielsen", "Richard Nock" ],
      "venue" : "International Journal of Computational Geometry & Applications,",
      "citeRegEx" : "42",
      "shortCiteRegEx" : "42",
      "year" : 2009
    }, {
      "title" : "Total Jensen divergences: Definition, properties and k-means++ clustering",
      "author" : [ "Frank Nielsen", "Richard Nock" ],
      "venue" : "arXiv preprint arXiv:1309.7109,",
      "citeRegEx" : "43",
      "shortCiteRegEx" : "43",
      "year" : 2013
    }, {
      "title" : "On clustering histograms with k-means by using mixed α-divergences",
      "author" : [ "Frank Nielsen", "Richard Nock", "Shun-ichi Amari" ],
      "venue" : null,
      "citeRegEx" : "44",
      "shortCiteRegEx" : "44",
      "year" : 2014
    }, {
      "title" : "On balls in a polygonal Hilbert geometry",
      "author" : [ "Frank Nielsen", "Laëtitia Shao" ],
      "venue" : "In 33st International Symposium on Computational Geometry (SoCG 2017),",
      "citeRegEx" : "45",
      "shortCiteRegEx" : "45",
      "year" : 2017
    }, {
      "title" : "Fitting the smallest enclosing Bregman ball",
      "author" : [ "Richard Nock", "Frank Nielsen" ],
      "venue" : "In ECML,",
      "citeRegEx" : "47",
      "shortCiteRegEx" : "47",
      "year" : 2005
    }, {
      "title" : "Minimum enclosing polytope in high dimensions",
      "author" : [ "Rina Panigrahy" ],
      "venue" : "arXiv preprint cs/0407020,",
      "citeRegEx" : "48",
      "shortCiteRegEx" : "48",
      "year" : 2004
    }, {
      "title" : "Information and accuracy attainable in the estimation of statistical parameters",
      "author" : [ "C Radhakrishna Rao" ],
      "venue" : "Bull. Cal. Math. Soc.,",
      "citeRegEx" : "49",
      "shortCiteRegEx" : "49",
      "year" : 1945
    }, {
      "title" : "Information and the accuracy attainable in the estimation of statistical parameters",
      "author" : [ "C Radhakrishna Rao" ],
      "venue" : "In Breakthroughs in statistics,",
      "citeRegEx" : "50",
      "shortCiteRegEx" : "50",
      "year" : 1992
    }, {
      "title" : "The geometric stability of Voronoi diagrams in normed spaces which are not uniformly convex",
      "author" : [ "Daniel Reem" ],
      "venue" : "arXiv preprint arXiv:1212.1094,",
      "citeRegEx" : "51",
      "shortCiteRegEx" : "51",
      "year" : 2012
    }, {
      "title" : "Perspectives on projective geometry: A guided tour through real and complex geometry",
      "author" : [ "Jürgen Richter-Gebert" ],
      "venue" : "Springer Science & Business Media,",
      "citeRegEx" : "52",
      "shortCiteRegEx" : "52",
      "year" : 2011
    }, {
      "title" : "New approximation algorithms for minimum enclosing convex shapes",
      "author" : [ "Ankan Saha", "SVN Vishwanathan", "Xinhua Zhang" ],
      "venue" : "In Proceedings of the twenty-second annual ACM-SIAM symposium on Discrete Algorithms,",
      "citeRegEx" : "53",
      "shortCiteRegEx" : "53",
      "year" : 2011
    }, {
      "title" : "A combinatorial bound for linear programming and related problems",
      "author" : [ "Micha Sharir", "Emo Welzl" ],
      "venue" : "STACS 92,",
      "citeRegEx" : "54",
      "shortCiteRegEx" : "54",
      "year" : 1992
    }, {
      "title" : "Riemann-Finsler geometry with applications to information geometry",
      "author" : [ "Zhongmin Shen" ],
      "venue" : "Chinese Annals of Mathematics-Series B,",
      "citeRegEx" : "55",
      "shortCiteRegEx" : "55",
      "year" : 2006
    }, {
      "title" : "The Geometry of Hessian Structures",
      "author" : [ "Hirohiko Shima" ],
      "venue" : "World Scientific,",
      "citeRegEx" : "56",
      "shortCiteRegEx" : "56",
      "year" : 2007
    }, {
      "title" : "The epic story of maximum likelihood",
      "author" : [ "Stephen M Stigler" ],
      "venue" : "Statistical Science,",
      "citeRegEx" : "57",
      "shortCiteRegEx" : "57",
      "year" : 2007
    }, {
      "title" : "Introduction aux géométries de hilbert",
      "author" : [ "Constantin Vernicos" ],
      "venue" : "Séminaire de théorie spectrale et géométrie,",
      "citeRegEx" : "58",
      "shortCiteRegEx" : "58",
      "year" : 2004
    } ],
    "referenceMentions" : [ {
      "referenceID" : 29,
      "context" : "1 Introduction The multinomial distribution is an important representation in machine learning that is often met in applications [32, 18] as normalized histograms (with non-empty bins).",
      "startOffset" : 129,
      "endOffset" : 137
    }, {
      "referenceID" : 16,
      "context" : "1 Introduction The multinomial distribution is an important representation in machine learning that is often met in applications [32, 18] as normalized histograms (with non-empty bins).",
      "startOffset" : 129,
      "endOffset" : 137
    }, {
      "referenceID" : 16,
      "context" : ", pn} of n categorical distributions [18] (multinomials) of ∆d using center-based kmeans++ or k-center clustering algorithms [6, 23] that rely on a dissimilarity measure (loosely called distance or divergence when smooth) between any two categorical distributions.",
      "startOffset" : 37,
      "endOffset" : 41
    }, {
      "referenceID" : 5,
      "context" : ", pn} of n categorical distributions [18] (multinomials) of ∆d using center-based kmeans++ or k-center clustering algorithms [6, 23] that rely on a dissimilarity measure (loosely called distance or divergence when smooth) between any two categorical distributions.",
      "startOffset" : 125,
      "endOffset" : 132
    }, {
      "referenceID" : 21,
      "context" : ", pn} of n categorical distributions [18] (multinomials) of ∆d using center-based kmeans++ or k-center clustering algorithms [6, 23] that rely on a dissimilarity measure (loosely called distance or divergence when smooth) between any two categorical distributions.",
      "startOffset" : 125,
      "endOffset" : 132
    }, {
      "referenceID" : 25,
      "context" : "1 Fisher-Hotelling-Rao geometry The Rao distance between two multinomial distributions is [28, 32]:",
      "startOffset" : 90,
      "endOffset" : 98
    }, {
      "referenceID" : 29,
      "context" : "1 Fisher-Hotelling-Rao geometry The Rao distance between two multinomial distributions is [28, 32]:",
      "startOffset" : 90,
      "endOffset" : 98
    }, {
      "referenceID" : 23,
      "context" : "We term this geometry the Fisher-Hotelling-Rao (FHR) geometry [26, 57, 49, 50].",
      "startOffset" : 62,
      "endOffset" : 78
    }, {
      "referenceID" : 53,
      "context" : "We term this geometry the Fisher-Hotelling-Rao (FHR) geometry [26, 57, 49, 50].",
      "startOffset" : 62,
      "endOffset" : 78
    }, {
      "referenceID" : 45,
      "context" : "We term this geometry the Fisher-Hotelling-Rao (FHR) geometry [26, 57, 49, 50].",
      "startOffset" : 62,
      "endOffset" : 78
    }, {
      "referenceID" : 46,
      "context" : "We term this geometry the Fisher-Hotelling-Rao (FHR) geometry [26, 57, 49, 50].",
      "startOffset" : 62,
      "endOffset" : 78
    }, {
      "referenceID" : 1,
      "context" : "The geodesics γ(p, q;α) are defined by the Levi-Civita metric connection [2, 15].",
      "startOffset" : 73,
      "endOffset" : 80
    }, {
      "referenceID" : 14,
      "context" : "The geodesics γ(p, q;α) are defined by the Levi-Civita metric connection [2, 15].",
      "startOffset" : 73,
      "endOffset" : 80
    }, {
      "referenceID" : 25,
      "context" : "unit d-sphere of R by using the square root representation p 7→ √p, see [28].",
      "startOffset" : 72,
      "endOffset" : 76
    }, {
      "referenceID" : 2,
      "context" : "2 Information geometry A divergence D is a smooth C differentiable dissimilarity measure [3] that allows to define a dual structure in Information Geometry [56, 15, 2] (IG).",
      "startOffset" : 89,
      "endOffset" : 92
    }, {
      "referenceID" : 52,
      "context" : "2 Information geometry A divergence D is a smooth C differentiable dissimilarity measure [3] that allows to define a dual structure in Information Geometry [56, 15, 2] (IG).",
      "startOffset" : 156,
      "endOffset" : 167
    }, {
      "referenceID" : 14,
      "context" : "2 Information geometry A divergence D is a smooth C differentiable dissimilarity measure [3] that allows to define a dual structure in Information Geometry [56, 15, 2] (IG).",
      "startOffset" : 156,
      "endOffset" : 167
    }, {
      "referenceID" : 1,
      "context" : "2 Information geometry A divergence D is a smooth C differentiable dissimilarity measure [3] that allows to define a dual structure in Information Geometry [56, 15, 2] (IG).",
      "startOffset" : 156,
      "endOffset" : 167
    }, {
      "referenceID" : 1,
      "context" : "The class of f -divergences plays an essential role in information theory since they are provably the only separable divergences that satisfy the information monotonicity property [2, 35].",
      "startOffset" : 180,
      "endOffset" : 187
    }, {
      "referenceID" : 32,
      "context" : "The class of f -divergences plays an essential role in information theory since they are provably the only separable divergences that satisfy the information monotonicity property [2, 35].",
      "startOffset" : 180,
      "endOffset" : 187
    }, {
      "referenceID" : 1,
      "context" : "That is, by coarse-graining the histograms we obtain lower-dimensional multinomials, say p′ and q′, such that 0 ≤ If (p′ : q′) ≤ If (p : q), see [2].",
      "startOffset" : 145,
      "endOffset" : 148
    }, {
      "referenceID" : 1,
      "context" : "The underlying information-geometric structure of KL is characterized by a pair of dual connections [2] ∇ = ∇(−1) (mixture connection) and ∇∗ = ∇(1) (exponential connection) that induces two dual geodesics (technically, ±1-autoparallel curves [15]).",
      "startOffset" : 100,
      "endOffset" : 103
    }, {
      "referenceID" : 14,
      "context" : "The underlying information-geometric structure of KL is characterized by a pair of dual connections [2] ∇ = ∇(−1) (mixture connection) and ∇∗ = ∇(1) (exponential connection) that induces two dual geodesics (technically, ±1-autoparallel curves [15]).",
      "startOffset" : 243,
      "endOffset" : 247
    }, {
      "referenceID" : 2,
      "context" : "The geometry of f -divergences [3] is the α-geometry (for α = 3 + 2f ′′′(1)) with the dual ±α-connections, where ∇(α) = 1+α 2 ∇∗ + 1−α 2 ∇.",
      "startOffset" : 31,
      "endOffset" : 34
    }, {
      "referenceID" : 2,
      "context" : "More generally, it was shown how to build a dual information-geometric structure for any divergence [3].",
      "startOffset" : 100,
      "endOffset" : 103
    }, {
      "referenceID" : 24,
      "context" : "For example, we can build a dual structure from the symmetric Cauchy-Schwarz divergence [27]: ρCS(p, q) = − log 〈λp, λq〉 √ 〈λp, λp〉〈λq, λq〉 .",
      "startOffset" : 88,
      "endOffset" : 92
    }, {
      "referenceID" : 13,
      "context" : "Then the Hilbert metric distance [14] is defined by:",
      "startOffset" : 33,
      "endOffset" : 37
    }, {
      "referenceID" : 22,
      "context" : "It is also called the Hilbert cross-ratio metric distance [24, 33].",
      "startOffset" : 58,
      "endOffset" : 66
    }, {
      "referenceID" : 30,
      "context" : "It is also called the Hilbert cross-ratio metric distance [24, 33].",
      "startOffset" : 58,
      "endOffset" : 66
    }, {
      "referenceID" : 48,
      "context" : "Notice that we take the absolute value of the logarithm since the Hilbert distance is a signed distance [52].",
      "startOffset" : 104,
      "endOffset" : 108
    }, {
      "referenceID" : 30,
      "context" : "When C is the unit ball, HG let us recover the Klein hyperbolic geometry [33].",
      "startOffset" : 73,
      "endOffset" : 77
    }, {
      "referenceID" : 11,
      "context" : "When C is a quadric bounded convex domain, we obtain the Cayley-Klein hyperbolic geometry [12] which can be studied with the Riemannian structure and corresponding metric distance called the curved Mahalanobis distances [39, 38].",
      "startOffset" : 90,
      "endOffset" : 94
    }, {
      "referenceID" : 36,
      "context" : "When C is a quadric bounded convex domain, we obtain the Cayley-Klein hyperbolic geometry [12] which can be studied with the Riemannian structure and corresponding metric distance called the curved Mahalanobis distances [39, 38].",
      "startOffset" : 220,
      "endOffset" : 228
    }, {
      "referenceID" : 35,
      "context" : "When C is a quadric bounded convex domain, we obtain the Cayley-Klein hyperbolic geometry [12] which can be studied with the Riemannian structure and corresponding metric distance called the curved Mahalanobis distances [39, 38].",
      "startOffset" : 220,
      "endOffset" : 228
    }, {
      "referenceID" : 10,
      "context" : "Furthermore, the domain bounday ∂C need not to be smooth: One may also consider bounded polytopes [11].",
      "startOffset" : 98,
      "endOffset" : 102
    }, {
      "referenceID" : 30,
      "context" : "The shape of balls in polytope-domain HG are Euclidean polytopes [33], as depicted in Figure 3.",
      "startOffset" : 65,
      "endOffset" : 69
    }, {
      "referenceID" : 42,
      "context" : "Hilbert balls have hexagons shapes in 2D [45], rhombic dodecahedra shapes in 3D, and are polytopes [33] with d(d+ 1) facets in dimension d.",
      "startOffset" : 41,
      "endOffset" : 45
    }, {
      "referenceID" : 30,
      "context" : "Hilbert balls have hexagons shapes in 2D [45], rhombic dodecahedra shapes in 3D, and are polytopes [33] with d(d+ 1) facets in dimension d.",
      "startOffset" : 99,
      "endOffset" : 103
    }, {
      "referenceID" : 42,
      "context" : "When the polytope domain is not a simplex, the combinatorial complexity of balls depends on the center location [45], see Figure 4.",
      "startOffset" : 112,
      "endOffset" : 116
    }, {
      "referenceID" : 28,
      "context" : "The HG of the probability simplex yields a non-Riemannian metric geometry because at infinitesimal radius value, the balls are polytopes and not ellipsoidal balls (corresponding to squared Mahalanobis distance balls used to visualize metric tensors [31]).",
      "startOffset" : 249,
      "endOffset" : 253
    }, {
      "referenceID" : 31,
      "context" : "The isometries in Hilbert polyhedral geometries are studied in [34].",
      "startOffset" : 63,
      "endOffset" : 67
    }, {
      "referenceID" : 48,
      "context" : "In Appendix B, we recall that any Hilbert geometry induces a Finslerian structure that becomes Riemannian iff the boundary is an ellipsoid (yielding the hyperbolic Cayley-Klein geometries [52]).",
      "startOffset" : 188,
      "endOffset" : 192
    }, {
      "referenceID" : 22,
      "context" : "Let us notice that in Hilbert simplex/polytope geometry, the geodesics are not unique (see Figure 2 of [24]).",
      "startOffset" : 103,
      "endOffset" : 107
    }, {
      "referenceID" : 1,
      "context" : "The KL divergence belongs to the family of α-divergences [2].",
      "startOffset" : 57,
      "endOffset" : 60
    }, {
      "referenceID" : 30,
      "context" : "Theorem 1 (Balls in a simplicial Hilbert geometry [33]) A ball in a Hilbert simplex geometry has a Euclidean polytope shape with d(d+ 1) facets.",
      "startOffset" : 50,
      "endOffset" : 54
    }, {
      "referenceID" : 28,
      "context" : "This property allows one to visualize Riemannian metric tensors [31].",
      "startOffset" : 64,
      "endOffset" : 68
    }, {
      "referenceID" : 30,
      "context" : "Thus we conclude that: Lemma 2 ([33]) Hilbert simplex geometry is a non-manifold metric length space.",
      "startOffset" : 32,
      "endOffset" : 36
    }, {
      "referenceID" : 14,
      "context" : "3 Visualizing distance profiles Figure 5 displays the distance profile from any point in the probability simplex to a fixed reference point (trinomial) for the following common distance measures [15]: Euclidean distance (metric), CauchySchwarz (CS) divergence, Hellinger distance (metric), Fisher-Rao distance (metric), KL divergence and the Hilbert simplicial distance (metric).",
      "startOffset" : 195,
      "endOffset" : 199
    }, {
      "referenceID" : 5,
      "context" : "Nevertheless, using a generalization of the k-means initialization [6] (picking randomly seeds), one can bypass the centroid computation, and yet guarantee probabilistically a good clustering.",
      "startOffset" : 67,
      "endOffset" : 70
    }, {
      "referenceID" : 6,
      "context" : "Since its inception (2007), this k-means++ seeding has been extensively studied [7].",
      "startOffset" : 80,
      "endOffset" : 83
    }, {
      "referenceID" : 40,
      "context" : "We state the general theorem established in [43]: Theorem 2 (Generalized k-means++ performance [43]) Let κ1 and κ2 be two constants such that κ1 defines the quasi-triangular inequality property: D(x : z) ≤ κ1 (D(x : y) +D(y : z)) ,∀x, y, z ∈ ∆d, and κ2 handles the symmetry inequality: D(x : y) ≤ κ2D(y : x),∀x, y ∈ ∆d.",
      "startOffset" : 44,
      "endOffset" : 48
    }, {
      "referenceID" : 40,
      "context" : "We state the general theorem established in [43]: Theorem 2 (Generalized k-means++ performance [43]) Let κ1 and κ2 be two constants such that κ1 defines the quasi-triangular inequality property: D(x : z) ≤ κ1 (D(x : y) +D(y : z)) ,∀x, y, z ∈ ∆d, and κ2 handles the symmetry inequality: D(x : y) ≤ κ2D(y : x),∀x, y ∈ ∆d.",
      "startOffset" : 95,
      "endOffset" : 99
    }, {
      "referenceID" : 5,
      "context" : "(5) The ratio ED(Λ,C) E∗ D(Λ,k) is called the competitive factor [6].",
      "startOffset" : 65,
      "endOffset" : 68
    }, {
      "referenceID" : 0,
      "context" : "Since divergences may be asymmetric, one can further consider mixed divergence M(p : q : r) = λD(p : q) + (1 − λ)D(q : r) for λ ∈ [0, 1], and extend the k-means++ seeding procedure and analysis, see [44].",
      "startOffset" : 130,
      "endOffset" : 136
    }, {
      "referenceID" : 41,
      "context" : "Since divergences may be asymmetric, one can further consider mixed divergence M(p : q : r) = λD(p : q) + (1 − λ)D(q : r) for λ ∈ [0, 1], and extend the k-means++ seeding procedure and analysis, see [44].",
      "startOffset" : 199,
      "endOffset" : 203
    }, {
      "referenceID" : 0,
      "context" : "The KL divergence can be interpreted as a separable Bregman divergence [1].",
      "startOffset" : 71,
      "endOffset" : 74
    }, {
      "referenceID" : 0,
      "context" : "The Bregman kmeans++ performance has been studied in [1, 36], and a competitive factor of O( 1 μ ) is reported using the notion of Bregman μ-similarity (that is suited for data-sets on a compact domain).",
      "startOffset" : 53,
      "endOffset" : 60
    }, {
      "referenceID" : 33,
      "context" : "The Bregman kmeans++ performance has been studied in [1, 36], and a competitive factor of O( 1 μ ) is reported using the notion of Bregman μ-similarity (that is suited for data-sets on a compact domain).",
      "startOffset" : 53,
      "endOffset" : 60
    }, {
      "referenceID" : 19,
      "context" : "In [21], spherical k-means++ is studied with respect to the distance dS(x, y) = 1−〈x, y〉 for any pair of points x, y on the unit sphere.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 30,
      "context" : "In [33] (Theorem 3.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 22,
      "context" : "Appendix A recalls the construction due to De La Harpe [24] in 1991.",
      "startOffset" : 55,
      "endOffset" : 59
    }, {
      "referenceID" : 21,
      "context" : "The farthest first traversal heuristic of Gonzalez [23] has a guaranteed approximation factor of 2 for any metric distance (see Algorithm 2).",
      "startOffset" : 51,
      "endOffset" : 55
    }, {
      "referenceID" : 8,
      "context" : "Algorithm 3: Geodesic walk for approximating the Hilbert minimax center, generalizing [9] Data: A set of points p1, · · · , pn ∈ ∆d.",
      "startOffset" : 86,
      "endOffset" : 89
    }, {
      "referenceID" : 49,
      "context" : "This problem has been well-studied in computational geometry: See [53, 13, 48].",
      "startOffset" : 66,
      "endOffset" : 78
    }, {
      "referenceID" : 12,
      "context" : "This problem has been well-studied in computational geometry: See [53, 13, 48].",
      "startOffset" : 66,
      "endOffset" : 78
    }, {
      "referenceID" : 44,
      "context" : "This problem has been well-studied in computational geometry: See [53, 13, 48].",
      "startOffset" : 66,
      "endOffset" : 78
    }, {
      "referenceID" : 49,
      "context" : "By considering the equivalent Hilbert norm polytope with d(d+ 1) facets, we state the result of [53]: Theorem 5 (SEB in Hilbert polytope normed space [53]) A (1 + )-approximation of the SEB in Vd can be computed in O(d n ).",
      "startOffset" : 96,
      "endOffset" : 100
    }, {
      "referenceID" : 49,
      "context" : "By considering the equivalent Hilbert norm polytope with d(d+ 1) facets, we state the result of [53]: Theorem 5 (SEB in Hilbert polytope normed space [53]) A (1 + )-approximation of the SEB in Vd can be computed in O(d n ).",
      "startOffset" : 150,
      "endOffset" : 154
    }, {
      "referenceID" : 38,
      "context" : "To compute the SEB, one may also consider the generic LP-type randomized algorithm [41].",
      "startOffset" : 83,
      "endOffset" : 87
    }, {
      "referenceID" : 50,
      "context" : "Let D = d(d+1) 2 denote the varying size of the combinatorial basis: Then we can apply the LP-type framework (we check the axioms of locality and monotonicity [54]) to solve efficiently for the SEBs.",
      "startOffset" : 159,
      "endOffset" : 163
    }, {
      "referenceID" : 50,
      "context" : "Theorem 6 (Smallest Enclosing Hilbert ball is LP-type [59, 54]) The smallest enclosing Hilbert ball amounts to find the smallest enclosing ball in a vector space with respect to a polytope norm that can be solved using a LP-type randomized algorithm.",
      "startOffset" : 54,
      "endOffset" : 62
    }, {
      "referenceID" : 39,
      "context" : "The Enclosing Ball Decision Problem [42] (EBDP) asks for a given value r, whether r ≥ r∗ or not.",
      "startOffset" : 36,
      "endOffset" : 40
    }, {
      "referenceID" : 39,
      "context" : "The decision problem amounts to find whether a set {rBV + vi} of translates can be stabbed by a point [42]: That is, whether ∩i=1(rBV + vi) is empty or not.",
      "startOffset" : 102,
      "endOffset" : 106
    }, {
      "referenceID" : 39,
      "context" : "At stage i, perform a dichotomic search on [ai, bi] by answering the decision problem for ri = bi−ai 2 , and update the radius range accordingly, see [42].",
      "startOffset" : 150,
      "endOffset" : 154
    }, {
      "referenceID" : 22,
      "context" : "(Geodesics are not unique, see Figure 2 of [24])",
      "startOffset" : 43,
      "endOffset" : 47
    }, {
      "referenceID" : 8,
      "context" : "2 Geodesic bisection approximation heuristic In Riemannian geometry, the 1-center can be arbitrarily finely approximated by a simple geodesic bisection algorithm [9, 5] This algorithm can be extended to HG straightforwardly as detailed in Algorithm 3: The algorithm first picks up a point c0 at random from X for the initial center, computes the farthest point fi (with respect to the distance ρ), and walk on the geodesic from c0 to fi by a certain amount to define c1, etc.",
      "startOffset" : 162,
      "endOffset" : 168
    }, {
      "referenceID" : 4,
      "context" : "2 Geodesic bisection approximation heuristic In Riemannian geometry, the 1-center can be arbitrarily finely approximated by a simple geodesic bisection algorithm [9, 5] This algorithm can be extended to HG straightforwardly as detailed in Algorithm 3: The algorithm first picks up a point c0 at random from X for the initial center, computes the farthest point fi (with respect to the distance ρ), and walk on the geodesic from c0 to fi by a certain amount to define c1, etc.",
      "startOffset" : 162,
      "endOffset" : 168
    }, {
      "referenceID" : 34,
      "context" : "See [37] for an extension and analysis in hyperbolic geometry.",
      "startOffset" : 4,
      "endOffset" : 8
    }, {
      "referenceID" : 9,
      "context" : "Furthermore, this iterative algorithm implies a core-set [10] (namely, the set of farthest points visited when iterating the geodesic walks) that is useful for clustering large data-sets [8].",
      "startOffset" : 57,
      "endOffset" : 61
    }, {
      "referenceID" : 7,
      "context" : "Furthermore, this iterative algorithm implies a core-set [10] (namely, the set of farthest points visited when iterating the geodesic walks) that is useful for clustering large data-sets [8].",
      "startOffset" : 187,
      "endOffset" : 190
    }, {
      "referenceID" : 12,
      "context" : "See [13] for coreset results concerning containment problems with respect to a convex homothetic object (the equivalent Hilbert polytope norm in our case).",
      "startOffset" : 4,
      "endOffset" : 8
    }, {
      "referenceID" : 44,
      "context" : "Panigrahy [48] described as simple algorithm dubbed MINCON for finding an approximation of the Minimum Enclosing Polytope.",
      "startOffset" : 10,
      "endOffset" : 14
    }, {
      "referenceID" : 12,
      "context" : "The algorithm induces a core-set of size O( 1 2 ) although the theorem is challenged in [13].",
      "startOffset" : 88,
      "endOffset" : 92
    }, {
      "referenceID" : 21,
      "context" : "Thus by combining the k-center seeding of Gonzalez [23] with the iteration Lloyd-like batched iterations, we get an efficient k-center clustering algorithm for the FHR and Hilbert metric geometries.",
      "startOffset" : 51,
      "endOffset" : 55
    }, {
      "referenceID" : 43,
      "context" : "dealing with the Kullback-Leibler divergence, we use the fact that KL is a Bregman divergence, and use the 1-center algorithm described in [47, 40] (approximation in any dimension) and [41] (exact but limited to small dimensions).",
      "startOffset" : 139,
      "endOffset" : 147
    }, {
      "referenceID" : 37,
      "context" : "dealing with the Kullback-Leibler divergence, we use the fact that KL is a Bregman divergence, and use the 1-center algorithm described in [47, 40] (approximation in any dimension) and [41] (exact but limited to small dimensions).",
      "startOffset" : 139,
      "endOffset" : 147
    }, {
      "referenceID" : 38,
      "context" : "dealing with the Kullback-Leibler divergence, we use the fact that KL is a Bregman divergence, and use the 1-center algorithm described in [47, 40] (approximation in any dimension) and [41] (exact but limited to small dimensions).",
      "startOffset" : 185,
      "endOffset" : 189
    }, {
      "referenceID" : 30,
      "context" : "Since Hilbert simplex geometry is isomorphic to a normed vector space [33] with a polytope norm with d(d + 1) facets, the Voronoi diagram in Hilbert geometry of ∆d amounts to compute a Voronoi diagram with respect to a polytope norm [30, 51, 19].",
      "startOffset" : 70,
      "endOffset" : 74
    }, {
      "referenceID" : 27,
      "context" : "Since Hilbert simplex geometry is isomorphic to a normed vector space [33] with a polytope norm with d(d + 1) facets, the Voronoi diagram in Hilbert geometry of ∆d amounts to compute a Voronoi diagram with respect to a polytope norm [30, 51, 19].",
      "startOffset" : 233,
      "endOffset" : 245
    }, {
      "referenceID" : 47,
      "context" : "Since Hilbert simplex geometry is isomorphic to a normed vector space [33] with a polytope norm with d(d + 1) facets, the Voronoi diagram in Hilbert geometry of ∆d amounts to compute a Voronoi diagram with respect to a polytope norm [30, 51, 19].",
      "startOffset" : 233,
      "endOffset" : 245
    }, {
      "referenceID" : 17,
      "context" : "Since Hilbert simplex geometry is isomorphic to a normed vector space [33] with a polytope norm with d(d + 1) facets, the Voronoi diagram in Hilbert geometry of ∆d amounts to compute a Voronoi diagram with respect to a polytope norm [30, 51, 19].",
      "startOffset" : 233,
      "endOffset" : 245
    }, {
      "referenceID" : 1,
      "context" : "6 Conclusion We introduced the Hilbert metric distance and its underlying non-Riemannian geometry for modeling the space of multinomials of the open probability simplex, and compared experimentally this geometry with the traditional differential-geometric modelings (either FHR metric connection or dually coupled nonmetric affine connection of information geometry [2]) for clustering tasks.",
      "startOffset" : 366,
      "endOffset" : 369
    }, {
      "referenceID" : 22,
      "context" : "For simplex domains, the Hilbert balls have fixed combinatorial (Euclidean) polytope structures, and HG is known to be isometric to a normed space [24, 22].",
      "startOffset" : 147,
      "endOffset" : 155
    }, {
      "referenceID" : 20,
      "context" : "For simplex domains, the Hilbert balls have fixed combinatorial (Euclidean) polytope structures, and HG is known to be isometric to a normed space [24, 22].",
      "startOffset" : 147,
      "endOffset" : 155
    } ],
    "year" : 2017,
    "abstractText" : "Clustering categorical distributions in the probability simplex is a fundamental primitive often met in applications dealing with histograms or mixtures of multinomials. Traditionally, the differentialgeometric structure of the probability simplex has been used either by (i) setting the Riemannian metric tensor to the Fisher information matrix of the categorical distributions, or (ii) defining the informationgeometric structure induced by a smooth dissimilarity measure, called a divergence. In this paper, we introduce a novel computationally-friendly non-Riemannian framework for modeling the probability simplex: Hilbert simplex geometry. We discuss the pros and cons of those three statistical modelings, and compare them experimentally for clustering tasks.",
    "creator" : "LaTeX with hyperref package"
  }
}