{
  "name" : "1706.03958.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Accelerated Dual Learning by Homotopic Initialization",
    "authors" : [ "Hadi Daneshmand", "Hamed Hassani", "Thomas Hofmann" ],
    "emails" : [ "hadi.daneshmand@inf.ethz.ch", "hamed@inf.ethz.ch", "thomas.hofmann@inf.ethz.ch" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "The standard approach to supervised machine learning is to cast it as an optimization problem with a suitable loss function and a regularizer. Learning then amounts to minimizing the regularized training risk over a chosen parametric model family. However, this view obstructs the fact that the ultimate goal is in minimizing the expected risk on unseen data and that the regularized empirical risk serves merely as a proxy for the former. Optimization for machine learning is thus by design a quest for approximate solutions. This offers learning-specific tradeoffs between statistical accuracy and computational complexity, such as early stopping [22], convex relaxation [4], data sub-sampling [7], or accepting locally optimal solutions as in deep learning [6, 21]. Whereas optimization algorithms are often analyzed in terms of their convergence to the optimum, in learning the interest should primarily be on how quickly one can find suboptimal solutions of sufficient quality (relative to the size of the data sample).\nOne key question in this context is what initialization strategy to use for the weights of a model, as the initial parameter choice has a huge impact on the transient phase of iterative learning algorithms. There are many examples that clearly demonstrate that initialization matters, for instance in deep learning via weight transfer [23], data-dependent initialization [15], or in avoidance of saddle points [16]. The same is true for unsupervised learning problems like matrix completion [14] orK-means [2]. Notably, for convex learning, the role of initialization has been somewhat neglected, mainly because of the guaranteed convergence to the global optimum. However, for massive data sets, initialization can have a huge practical impact on the scalability of an algorithm. Moreover, the (easier) convex setting also allows for a more rigorous analysis of the effect of initialization and the reasons for the slow-down caused by poor starting points. In this vein, the current paper provides a detailed analysis of convex learning, specifically of ridge regression and generalized linear models, that suggests to pre-train models with artificially increased regularization and to use this as an initialization in the spirit of homotopy or continuation methods [1, 18]. The focus of our work is on the dual problem, because it offers more flexibility in the data representation (i.e. through the use of kernels), allows for fast algorithms like stochastic coordinate descent [20] that exhibit linear convergence and is\nar X\niv :1\n70 6.\n03 95\n8v 1\n[ cs\n.L G\n] 1\n3 Ju\nn 20\nalso amenable to data sharding and communication-efficient distributed implementations [13]. In particular, our method offers an yet unexplored direction to get speed-ups for sequential or parallel dual algorithms."
    }, {
      "heading" : "2 Ridge Regression",
      "text" : ""
    }, {
      "heading" : "2.1 Primal Formulation",
      "text" : "For concreteness, we perform an in-depth analysis of ridge regression [12]. Given a training set of n observations (xi, yi), with inputs xi ∈ Rd and reponses yi ∈ R, we denote by X ∈ Rn×d the data matrix and by y ∈ Rn the response vector. Let us assume w.l.o.g that y is mean zero and unit variance and that the data is centered.The ridge regression objective with regularization µ > 0, can be expressed as\nQ(β) = 1 2 β>Hβ − β>b, H := 1 n X>X + µI ∈ Rd×d, b := 1 n X>y ∈ Rd (1)\nThe optimal solution is explicitly given by the normal equations\n∇βQ ! = 0 ⇐⇒ β∗ = H−1b = ( X>X + nµI )−1 X>y . (2)\nNote that we can interpret key quantities as expectations under the training distribution, which will subsequently become relevant. In particular note that bj = E[XjY ] is the covariance between the j-th feature dimension and the response. Obviously, the elements of 1nX\n>X can be expressed as E[XiXj ], i.e. they encode the empirical variances and covariances of the features. We also denote by Eµ[X2i ] = E[X 2 i ] + µ the biased varince estimated that we get through regularization.\nFor the purpose of analysis and to ease the exposition, we orthogonally transform the data into an eigenfeature representation. To that extend we use the scaled SVD of X = √ nUΣV>, such that X>X = nVΣ>ΣV> and define Z := XV = √ nUΣ. Correspondingly the parameters are transformed via β ← V>β, we arrive at the diagonalized objective\nQ(β) = 1 2 β> ( Σ>Σ + µI )︸ ︷︷ ︸ =:Λ β − β> 1 n Z>y︸ ︷︷ ︸ =:c = 1 2 d∑ j=1 β2j E µ[Z2j ]︸ ︷︷ ︸ =λj −βj E[Y Zj ]︸ ︷︷ ︸ =cj\n(3)"
    }, {
      "heading" : "2.2 Gradient Descent Analysis",
      "text" : "Gradient descent (GD) optimizes an objective through iterative gradient updates with step size γ > 0 βt+1 = βt − γ∇βQ(βt). (4)\nFor the diagonalized quadratic objective, the iterate sequence is explicitly given as follows. Lemma 1. GD initialized at β0 yields the iterate sequence\nβt = β∗ + (I− γΛ)t (β0 − β∗) (5) Corollary 2. For any γ < 2λ−11 and initial β0, the iterate sequence generated by GD is guaranteed to converge to β∗ at a linear rate. Corollary 3. Define the condition number as κ := λ1/λd. The rate of parameter convergence of βd is lower bounded by 1− γλd > 1− 2/κ.\nOur interest is in the convergence of the objective value. We can easily (and exactly) relate distance in parameter space to suboptimality through Λ. Lemma 4. Let β∗ be the minimizer of (1) and Q∗ , Q(β∗). Then for any β\nQ(β)−Q∗ = 1 2 (β − β∗)Λ(β − β∗) = 1 2 d∑ j=1 Eµ[Z2j ](βj − β∗j )2 . (6)\nLet us compare the worst case rate we would expect based on κ in Corollary 3 with the empirical suboptimality of the GD iterate sequence on some (randomly selected) data sets. The plots in Figure 1 show that initially (and for the relevant transient phase) the observed reduction of suboptimality is typically much better than what may be expected based on the convergence rates in parameter space. This is a striking behavior that our work aims to explain and to better exploit."
    }, {
      "heading" : "2.3 Analysis of Initialization",
      "text" : "We study a single coordinate in the diagonalized problem.\nQ(βtj)−Q∗ = (1− γλj)2t · λj 2\n( β0j − β∗j )2 (7)\nThere are two factors here, one that decreases exponentially with t and a constant that depends on the initialization. In an asymptotic setting, only the first would matter, leading to the well-known slow-down in convergence of GD for ill-conditioned problems, in directions where γλj becomes very small. But what if we can tolerate some suboptimality ? Then we could try to find an initialization such that the second term would be less than . There could be two cases: (i) we devise a smart strategy to find a good β0 that is sufficiently close to β∗, (ii) we set β0 = 0, but can reasonably assume that β∗ is small. We come back to (i) and first investigate (ii) by noting that\nQ(0)−Q∗ = 1 2\nE[Y Zj ] 2\nEµ[Z2i ] ≤ 1 2\nE[Y Zj ] 2\nE[Z2j ] =\n1 2 ρ2j . (8)\nHere ρj is the correlation coefficient between Y and the eigenfeature Zj . We now formulate the following hypothesis: the observed empirical sub-optimality of GD iterates is due to the fact that eigenfeatures Zj with small variance also have a small correlation with the response Y . This seems to be related to a similar assumption made in early stopping [22] and in general when using norm-based regularization: we do not want to trust features that need to be amplified a lot (low variance, but high output covariance). Hence, a reasonable feature representation should avoid encoding relevant information in such a manner. What we show here though is that such considerations not only avoid overfitting, but also accelerate GD training.\nIn order to subject this idea to a more formal treatment, let us introduce a suitable regularity concept. A dataset exhibits τ -bounded response correlation, if for any eigen-feature Zj with non-zero variance\nρ2j = E[Y Zj ]\n2\nE[Z2j ] ≤ τE[Z2j ] = τσ2j (9)\nWith this definition, we can immediately see that we can upper bound the suboptimality of GD iterates by making use of the regularity parameter for all eigenfeatures with E[Z2j ] ≤ ζ, where ζ is an arbitrary threshold. Effectively, τ -boundedness allows us to pay a constant approximation cost for eigenfeatures with small variance and to trade that off with the faster rates obtained for the remaining eigenfeatures. We capture the gist of this effect in the following lemma.\nLemma 5. Assume that the data set has τ -bounded response correlation. Then for any ζ ≥ 0, the suboptimality of the GD iterate sequence can be upper bounded as\nQ(βt)−Q∗ ≤ 1 2\n[ r(ζ)(1− γζ)2t + (d− r(ζ)) ] τζ, r(ζ) := {j : E[Z2j ] > ζ} (10)\nProof. Follows directly from the boundedness assumptions.\nFigure 2 shows that our notion of regularity seems to agree well with the empirical observations that we pointed out before. Note that regularization increases the variance of eigenfeatures and thus leads to more favorable τ -bounds. However, for clarity, we wanted to capture this notion for the worst case of µ = 0."
    }, {
      "heading" : "3 Dual Ridge Regression",
      "text" : "We will now consider the dual problem for ridge regression. It turns out that the initialization β0 = 0, which we analyzed for the primal in the previous section, is not advantageous for the dual. However, we can use the insight about data τ -boundedness to derive an effective initialization for the dual problem."
    }, {
      "heading" : "3.1 Dual Objective",
      "text" : "It is straightforward to derive dual objectives for convex optimization problems using Fenchel duality, e.g. see [8]. For ridge regression, we obtain the following dual objective with dual parameters α\nQµ(α) = 1 2 α>Gα− b>α, nG = 1 µn XX> + I, nb = y (11)\nIn the dual space, where typically n d, there is an n− d dimensional subspace ker(X>), such that for the corresponding dual eigen-vectors v, v>Gv = 1n . This means that we have at least (n− d) orthogonal directions with a variance of 1/n accounting for a total variance of (n− d)/n. While in the primal case, there are at most d terms to bound and typically µ ∝ 1/n, in the dual we may get a suboptimality contribution that is O(1), even under a τ -boundedness assumption.\nSimilar to the primal objective, we use (to carry out the analysis, not the optimization!) the following change of variables α← U>α. This change of variables leads to the following diagonalization of the dual objective:\nQµ(α) = 1 2 α>Γα− c>α, nΓ := 1 µ Σ>Σ + I, c := 1 n U>y, α∗µ = 1 n Γ−1c (12)\nand suboptimality can be written as\nQµ(α)−Qµ(α∗µ) = 1\n2n d∑ i=1 λi µ (αi −α∗i ) 2 + 1 2n n∑ i=d+1 (αi −α∗i ) 2 (13)\nIndeed, suboptimality can be decomposed to two terms: the suboptimality in image of the data matrix (the first sum), and the suboptimality in the kernel of the data matrix (the second sum).\nLemma 6. Suppose that α∗ν is the minimizer of Qν , then for all ν, µ > 0:\nα∗νj = α ∗µ j (∀j > d) and (α ∗ν i −α ∗µ i ) 2 = n\n( (µ− ν)σ2i\n(σ2i + µ)(σ 2 i + ν)\n)2 E[Y Zi] 2 (14)\nProof. Follows directly from the closed form solution of minimizers.\nThe above lemma shows that (n − d) coefficients of two dual solutions with respect to different regularizer are exactly the same and the squared difference between other d coefficients is propotional to (µ− ν)2. Computing the minimizer α∗ν is relatively cheaper than α∗µ for ν µ as rates often depend on the strong convexity parameter. Hence we suggest initializing gradient descent for Qµ with α∗ν for a suitably chosen ν.\nLemma 7. Suppose that the data set has τ -bounded response correlations. Then for any ζ ≥ 1n , the suboptimality of the GD iterates starting from α∗ν with step size (nµ)γ can be upper bounded as\nQ(αt)−Q∗ ≤ τζ(ν − µ) 2\n2νµ\n[ r(ζ)(1− γζ)2t + (d− r(ζ)) ] , r(ζ) := |{j : E[Z2j ] > ζ}| (15)\nProof. Plugging the result of lemma 6 into suboptimality of the dual objective in Eq. (13) concludes the proof.\nThe above result closely mirrors Lemma 5 with the addition of a factor that depends on the chosen ν (in relation to µ). A smaller homotopic parameter ν – closer to µ – enjoys the convergence with a smaller scaling factor (ν − µ)2/(νµ). However, computing the initial vector β∗ν is more expensive for a small ν. Setting that aside, the significance of the homotopic initialization is the fact that the (n− d) directions of eigenvalue n−1 do no longer have en effect on the suboptimality. Note that in the eigensystem of Eq. (12), we can easily solve (irrespective of µ)\nα∗j = u > j y min←− 1 2n α2j − cjαj , (∀j > d) (16)\nHowever, this is not a practical computation as the basis vectors uj of the kernel of X are not known and the eigen-parameterization is not the one accessible to the algorithm. The trick of the homotopic initialization is that effectively we set α∗j , without having to perform the diagonalization of X >X."
    }, {
      "heading" : "3.2 Accelerated RCDM by Homotopic Initialization",
      "text" : "In the large scale setting (when the sample size n is large), the gradient step is computationally expensive. Random coordinate descent method (RCDM) [19, 20] is computationally more attractive than GD and offers a competitive convergence. A RCD-step is obtained by a coordinate-wise approximation of the gradient. More precisely, it randomly picks a random coordinate r ∈ {1, . . . , n} and updates it using the corresponding coordinate of the gradient (denoted by Q′r(·)) as\nα+r = αr − γrQ′r(α), (17)\nwhere γr is the coordinate-wise step size of RCDM. We ask if homotopic initialization also accelerates the convergence – up to a suboptimal solution. The convergence dependency on initialization is more subtle in a stochastic setting, where each optimization step is perturbed by the noise of the gradient approximation. Here, we theoretically prove that homotopic initialization accelerates RCDM. To this end, we provide a different convergence analysis for RCDM on the dual objective of ridge regression. Later, we will show how this result provides a better convergence rate in the objective using homotopic initialization. Theorem 8. Let Gi,j denote the (i, j)-th element of the Hessian matrix G. Let the parameter vector α(t) be obtained by t RCDM-steps on Q, starting from α0 with coordinate-wise step sizes\nγ−1r = Gr,r + ∑ j |Gr,j |, γmin := min r γr, γmax := max r γr. (18)\nFor the above parameters, either the norm of gradient is bounded as E‖Q′µ(α(t))‖2 ≤ 2ρ2 ( γmax γmin ) ‖α0 −α∗‖2 (19)\nor suboptimality is bounded as E [ Qµ(α(t))−Q∗µ ] ≤ 1\n2\n( 1− ργmin\nn\n)t ( Qµ(α0)−Q∗µ ) + 1\n2 ρ ( γmax γmin ) ‖α0 −α∗‖2 (20)\nfor every 1n ≤ ρ ≤ ‖G‖ (expectation is over the random choice of coordinates).\nSuppose that ∑ j |x>i xj | ≤ B for all i. Using coordinate-wise step sizes of Eq. (18), the classical analysis of RCDM [19] suggests the rate 1− (n+ 2B/µ)−1, while our analysis improves the rate by a factor of ρ/n > 1 to 1− (ρ/n)(n+ 2B/µ)−1. This improvement is not only with respect to the strong-convexity factor µ (e.g., as in the catalyst method [17]), but also with respect to the sample\nsize n. The acceleration of Theorem 8 is up to a suboptimal solution: either the norm of the gradient is bounded by O(ρ2‖α0 − α∗‖2) or the suboptimality is at most O(ρ‖α0 − α∗‖2). Both the suboptimality bounds highly depend on the initial Euclidean distance to the minimizer. In fact, one can take advantage of the accelerated rate only if the initial distance ‖α0 − α∗‖2 is small. We observed that this distance is significantly large for initialization with the all-zero vector. Nonetheless, we theoretically bound this distance for homotopic initialization. Lemma 9. Assume that the data set has τ -bounded response correlation. Then for any ζ the distance between two minimizers of the dual objective with different regularizers is bounded as\n‖α∗ν −α∗µ‖2 ≤ ( (µ− ν)2/ν ) (d− r(ζ))nτζ. (21)\nLemma 9 implies that for a smaller ν, closer to µ, the initial distance is smaller and hence the acceleration of homotopic initialization is up to a better suboptimality. However, convergence to α∗ν is slower as it directly relates to the ν. In our experiments, we observed that setting ν = 1 4 √ µ – which is considerably cheaper than computing the minimizer α∗µ– provides a significant acceleration. The coordinate-wise step sizes of Eq. (18) are quite pessimistic. In our experiments, we used larger step sizes γr = G−1r,r , which is equal to the coordinate-wise Lipschitz constants [19] of the dual ridge objective."
    }, {
      "heading" : "4 Generalized Linear Model",
      "text" : "The accelerated convergence for an approximate solution also applies to generalised linear models. Given a convex differentiable smooth function ϕ, a generalized linear model (GLM) aims at minimizing the non-quadratic objective [9]\nR(z) = E [ ϕ(x>i z)− yi ( x>i z )] , (22)\nwhere the expectation is over the population distribution of the data. Let ϕ(k)(·) denote the k-th derivative of ϕ(·). The above formulation obtains logistic regression, with ϕ(a) = log(1 + exp(a)), and ridge regression, with ϕ(a) = a2. To take advantage of the boundedness property of a dataset, we modify the gradient descent step as\nz+ = z− γtR′(z)− ηtE [yx] (23)\nwhere γt and ηt are two step-sizes of the modified gradient step. Indeed, we suggest to use a biased gradient step to accelerate the initial convergence. If the input vectors xi are drawn i.i.d from a gaussian distribution, then we can obtain an accelerated convergence of the modified gradient descent on the generalized linear model. This is made precise in the next lemma. Lemma 10. Suppose that inputs are drawn i.i.d from a multivariate normal distribution with mean 0 and covariance matrix Σ, i.e. xi ∼ N(~0,Σ) where ‖Σ‖ = L. Assume that |ϕ(2)(a)| ≤ φ. If the dataset is τ -bounded, then there is a step size schedule for modified gradient descent such that iterates of GD starting from zero obtains the following suboptimality bound for all 0 < ζ < L:\nR(z(t))−R∗ ≤ cz∗τφ ( (1− ζ/L)2t (1− r(ζ)) + r(ζ)ζ ) , (24)\nr(ζ) := |{j : E[Z2j ] > ζ}|, cz∗ := (Ex [ ϕ(2)(x>z∗) ] )−1. (25)\nAlthough the data is assumed to be generated from a normal distribution in the last lemma, we believe that this result can be extended to an arbitrary distribution using zero-bias transformations (see [9] for more details). Furthermore, we believe that homotopic initialization can also obtain a similar acceleration, up to a suboptimal solution, on the dual objective of GLM."
    }, {
      "heading" : "5 Initialization for Deep Neural Networks",
      "text" : "So far we have seen that if a dataset has the boundedness property, then initialization with the all-zero vector will accelerate optimization, on the primal objective, up to a sub-optimal solution. We proved this for ridge regression as well as for generalized linear models in a simplified setting. Our experiments show that non-linear features of a trained neural network provide such a representation\nof the data (see figure 4). This observation justifies the effectiveness of one of the most common initialization schemes of deep neural networks: Use resulting weights of a trained shallow network to initialize a deep neural network [10]. We attribute the gain of such initialization to the boundedness of features obtained by a smaller network. Suppose that we used a trained network with N − 1 layer to initialize the first N − 1 layers of a larger network (assume that the number of hidden units is the same as the number of output unites). If we freeze weights of the first N − 1 layers, then optimization with respect to weights of the last layer is a convex programming task using a convex loss function, which can be modelled by a generalized linear model. Since features obtained by a trained network are τ -bounded, we expect an initial acceleration of GD by initialising the last layer to all-zero. In practice, we do not need to freeze weights of early layers because back propagation naturally causes small changes in early layers due to the vanishing gradient phenomenon [11]. Furthermore, we do not need to increase the number of layers by one. In our experiments, we have observed that even a network with N/2 layers yields a good initialization for a network with N layers (see figure 4). Since optimization of shallow networks is relatively cheaper than a deep neural network, this initialization is relatively cheap to compute. Our argument can also be extended to the layer-wise training for deep neural networks [3]. Indeed, the boundedness property is a statistical property of a representation that plays an important role in optimization for machine learning."
    }, {
      "heading" : "6 Experimental Results",
      "text" : "6.1 Datasets and Protocol\nIn this section, we present our empirical results on real datasets, selected from LIBSVM library [5] (see Table 1 for more details). We measured the boundedness constant τ for these datasets, which is used in our analysis. The regularizer is µ = 10−6 for all datasets except for GISETTE, which has relatively less samples, and hence we used regularizer µ = 10−3."
    }, {
      "heading" : "6.2 Initialization for Dual Objective",
      "text" : "We ran an experiment to assess the advantage of the homotopic initialization on dual programming. Our experiment is on initialization of Random Coordinate Descent Method (RCDM) for optimizing the dual objective of ridge regression. Throughout all the experiments, we used the homotopic parameter ν = 0.25 √ µ, which is computationally a favourable choice. We used coordinate-wise step sizes γr = G−1r,r , which are equal coordinate-wise Lipchitz constants and its a common choice for RCDM [19]. Our sampling scheme of coordinates is random permutation in each epoch. Figure 3 shows the dual suboptimality, primal suboptimality, and test error through optimization. To compute the primal suboptimality, we mapped the dual parameter to the primal one using the mapping β(t) = (nµ)−1Xα(t) (used in [20]). This mapping obtains the primal minimizer given the dual minimizer, i.e. β∗ = (nµ)−1Xα∗ holds. For the test error, we computed the average of the squared loss on the test data. Overall, we observe that the homotopic initialization causes a worse initial primal and dual suboptimality compared to the initialization with all-zero vector. Nonetheless, the suboptimality decays quickly and reachs a better suboptimality compared to the initialization with zero. Indeed, the homotopic initial vector lies in the space of coarser eigen-features (features associated with larger eigenvalues) that accelerates optimization. Although the accelerated convergence is up to a suboptimal solution, this suboptimal solution achieves a test error that is comparable to the test error of the empirical minimizer on most of datasets. The acceleration, obtained by the homotopic initialization, is related to the boundedness factor τ of the dataset reported in table 1. For example, the homotopic initialization on GISETTE, which has a large τ , obtains relatively less gain.\nAlthough our analysis of homotopic initialization was limited to the ridge regression problem, our experiments on dual SVM show the same behaviour for this initialization. Our results on dual SVM are included in the appendix."
    }, {
      "heading" : "6.3 Initialization for neural networks",
      "text" : "We ran an experiment to highlight the role of boundedness of a representation in the convergence speed of gradient descent on neural networks. In this experiment, we train a multilayer perceptron (MLP) with 10 hidden layers and 100 hidden units in each layer. We used two datasets MNIST and CIFAR with 50000, and 20000 samples, respectively. Here, we compared the boundedness of features obtained from the last layer of the network before and after training. Our observations show that a trained neural network provides boundedness (see figures 4.a and 4.b). Based on this observation, we use a layer-wise initialization strategy. We trained a MLP with 5 layers and the same number of hidden units. Then we initialized the first 5 layers of the main network (with 10 layers) by the trained weights of the smaller network and we set the rest of weights to zero.We compared the convergence of GD with step size 0.1 using these two different initialisation schemes. The layer-wise initialization, which yields a representation with the boundedness property, significantly accelerates the initial convergence of GD."
    }, {
      "heading" : "Acknowledgments",
      "text" : "We would like to thank Aurelien Lucchi, Octavian Ganea, and Dünner Celestine for helpful discussions."
    }, {
      "heading" : "7 Appendix",
      "text" : "Here, we present the proof of lemmas and theorems of the manuscript with our experiments on the SVM problem. We start with the analysis of random coordinate descent method, then we provide the proof of the claim on generalized linear model. Finally, we represent the experimental result."
    }, {
      "heading" : "7.1 Analysis of RCDM",
      "text" : "We proved that homotopic initialization accelerates convergence of gradient descent, up to a suboptimal solution, on the dual objective. In this section, we extend the result to the random coordinate descent method. For gradient descent, iterates could be tracked in a closed form, and the role of the initialisation was closely reflected in the convergence. The analysis of random coordinate descent, however, is more involved due to the noisy estimation of the gradient. In the next lemma, we decompose the convergence bound of RCDM to two terms: a term that depends on the initial sub-optimality on the dual objective, the second term is determined by the initial Euclidean distance to the dual minimizer. Later, we will use this result to prove that homotopic initialization provides an acceleration up to a suboptimal. Theorem (8). Let Gi,j denote the (i, j)-element of the Hessian matrix G. Let the parameter vector α(t) is obtained by t RCDM-steps on Q starting from α0 with coordinate-wise step sizes\nγr = Gr,r +∑ j |Gr,j | −1 , γmin = min r γr, γmax = max r γr (26)\nFor the obtained parameter, either the norm of gradient is bounded as E‖Q′µ(α(t))‖2 ≤ 2ρ2 ( γmax γmin ) ‖α0 −α∗‖2 (27)\nor suboptimality is bounded as E [ Qµ(α(t))−Q∗µ ] ≤ 1\n2\n( 1− γminρ\nn\n)t ( Qµ(α0)−Q∗µ ) + 1\n2 ρ ( γmax γmin ) ‖α0 −α∗‖2 (28)\nfor every 1n ≤ ρ ≤ ‖G‖ (expectation is over the random choice of coordinates).\nProof. We rewrite the RCDM update in a form that facilitates our future analysis. Consider matrix Ψr whose r-th diagonal element is one and remaining elements are zero; using this matrix, RCDM step can be written as\nα+ = α− γrΨrQ′(α) = α− γrΨr (Q′(α)−Q′(α∗)) = (I− γrΨrG)(α−α∗) + α∗. (29)\nNote that we used the optimality condition of α∗ in the second step. We skipped the subscript µ for Qµ throughout the proof. To obtain the desired convergence guarantee, we decompose the Hessian matrix, of the dual objective, as G = Lρ + Sρ where matrices Lρ and Sρ are obtained from SVD decomposition of the data matrix:\nLρ = (nµ)−1 ∑ i:λi≥ρ λiuiu > i (30)\nSρ = (nµ)−1 ∑ j:λj<ρ λjuju > j . (31)\nThe second smallest eigenvalue of the Lρ is ρ, the spectral norm of Sρ is bounded by ρ, and LρSρ = 0. Using the above decomposition, the expected suboptimality after one RCD-step can be written as\nEr [ Q(α+)−Q∗ ] (6) = 1 2 E [ ‖α+ −α∗‖G ] = 1\n2 E [ ‖α+ −α∗‖Lρ ] + 1 2 E [ ‖α+ −α∗‖Sρ ] (32)\nWe prove that the first term, which depends on directions with large eigenvalues, decays in a favourable rate (in lemma 11) and the second term is bounded (by a factor of the threshold ρ).\nLemma 11. RCMD with coordinate-wise step sizes\nγr ≤ (2Gr,r)−1 (33)\nguarantees the decrement E [ ‖α+ −α∗‖Lρ ] ≤ (\n1− γminρ n\n) E [ ‖α−α∗‖Lρ ] (34)\nas long as\n‖Q′(α)‖2 ≥ 2‖Sρ (α−α∗) ‖2. (35)\nProof. Consider the compact notations ∆ = α−α∗ and ∆+ = α+ −α∗. We rewrite the expected suboptimality after one setp of RCDM as\nE‖∆+‖Lρ (29) = E‖(I− γrΨrG)∆‖2Lρ = E [ ∆> (I− γrGΨr)Lρ (I− γrΨrG) ∆ ] = T1 + T2 (36)\nwhere terms T1 and T2 are formulated as T1 := E [ ∆>Lρ (I− γrΨrG) ∆ ] (37)\nT2 := E [ ∆> ( γ2rGΨrLρΨrG− γrLρΨrG ) ∆ ]\n(38)\nThe first term is bounded as:\nT1 = ∆ >Lρ (I−Er [γrΨr] G) ∆\n[1] ≤ ∆>Lρ ( I− γmin n G ) ∆\n[2] ≤ ∆> ( Lρ −\nγmin n\n( L2ρ + LρSρ )) ∆\n≤ ∆>Lρ (I− (γmin/n)Lρ) ∆ ≤ ∆>L1/2ρ (I− (γmin/n)Lρ)L 1/2 ρ ∆\n[3]\n≤ (1− (γminρ/n)) ∆>Lρ∆. (39) In steps [1]–[3], we used the fact that E [Ψr] = I/n, G = Lρ + Sρ, and the second smallest eigenvalue of the matrix Lρ is greater than ρ, respectively. It remains to show that T2 is negative:\nT2 = E [ ∆> ( γ2rGΨrLρΨrG− γrLρΨrG ) ∆ ]\n[1] = E [ ∆> ( Lρ,rγ2rGΨrG− γrLρΨrG ) ∆ ]\n[2] = E [ γr∆ > (GrGΨrG− LρΨrG) ∆ ]\n[3] ≤ γminE [ ∆> ( 1\n2 GΨrG− LρΨrG\n) ∆ ] ≤ γmin∆> ( 1\n2 GE [Ψr]G− LρE [Ψr]G\n) ∆\n[4] ≤ γmin n\n∆> ( 1\n2 G2 − LρG\n) ∆\n[5] ≤ γmin 2\n∆> ( S2ρ − L2ρ ) ∆\n[6] ≤ 0 (40)\nwhere we used following facts in each step:\n[1]: Note that Lρ,r denotes the diagonal element r of matrix Lρ. Matrix Ψr has only one non-zero element: the r-th diagonal which is one, hence ΨrLρΨr = Lρ,rΨr .\n[2]: Recall (L = G−S)→ (Lρ,r = Gr −Sρ,r). Since Sρ is positive definite, its diagonal elements are positive. This concludes that Lρ,r ≤ Gr .\n[3]: The choice of step sizes leads to γrGr ≤ 1/2.\n[4]: We know that Er [Ψr] = I/n according to the definition of matrix Ψr .\n[5]: Matrix Lρ and Sρ are orthogonal.\n[6]: As long as the norm of the gradient is sufficiently large (the condition in Eq. (35)), this inequality holds. To prove this, we write the norm of the gradient as:\n‖Q′(α)‖2 = ‖Q′(α)−Q′(α∗)‖2\n= ‖G∆‖2\n= ‖Lρ∆‖2 + ‖Sρ∆‖2 (41) Now the condition of Eq. (35) implies\n‖Q′(α)‖2 ≥ 2‖Sρ∆‖2\n‖Lρ∆‖2 + ‖Sρ∆‖2 ≥ 2‖Sρ∆‖2\n‖Lρ∆‖2 − ‖Sρ∆‖2 ≥ 0 (42)\nThe last lemma implies that the convergence of E [ ‖α+ −α∗‖Lρ ] is dominated by ρ up to a suboptimal solution in term of the norm of the gradient. It remains to prove that the second term of suboptimality, i.e. E [ ‖α+ −α∗‖Sρ ] , scales with a factor of ρ:\nE [ ‖α+ −α∗‖2Sρ ] ≤ ‖Sρ‖E [ ‖α+ −α∗‖2 ] ≤ ρE [ ‖α+ −α∗‖2 ] . (43)\nLet parameter vector α(t) denote the obtained parameter vector after t RCDM steps. We know that RCMD with step size of γr ≤ (Gr,r)−1 monotonically convergences to the minimizer in terms of the objective value. Furthermore, (1/n)-strong convexity of the dual function implies that ‖α(t) −α∗‖2 ≤ n(Q(t) −Q∗). Therefore, ‖α(t) −α∗‖ asymptotically convergences to zero; however, this convergence is not monotone. In other words, the distance might diverge. In the next lemma, we bound this distance by a factor of the initial distance, i.e. ‖α0 −α∗‖2.\nLemma 12. The distance of RCD iterates α(t) from the minimizer are bounded by a constant factor of the initial distance from the minimizer:\nE‖α(t) −α∗‖2 ≤ γmin γmax ‖α(0) −α∗‖2 (44)\nfor the choice of step sizes in Eq. (26).\nProof. We introduce the diagonal matrix Λ whose r diagonal element is the coordinate-wise step size r. The definition of Λ implies γminI Λ γmaxI. Furthermore, the choice of step sizes of Eq. (26) ensures matrix Λ −G is diagonal dominant, therefore it is positive definite. Using these facts, we prove that E‖α(t) −α∗‖2Λ does not diverge. Some straightforward algebra leads to\nE‖α(t) −α∗‖2Λ ≤ E‖α(t−1) −α∗‖2Λ + (α(t−1) −α∗)>T1(α(t−1) −α∗) where T1 is a negative definite matrix:\nT1 = E [ γ2rGΨrΛΨrG− γrGΨrΛ ] [1]\n≤ E [γrGΨrG− γrGΨrΛ] ≤ γmin (GE [Ψr] G−GE [Ψr] Λ) [2] ≤ γmin n G (G− Λ)\n[3] ≤ 0, (45)\nwhere steps [1]–[3] are obtained by the choice of step size, E [Ψr] = I/n, and 0 Λ − G, respectively. Using this result, we bound the desired distance as\nE‖α(t) −α∗‖2 = E‖Λ − 1/2Λ1/2(α(t−1) −α∗)‖2\n≤ ‖Λ−1‖E‖Λ1/2(α(t−1) −α∗)‖2 [1] ≤ ‖Λ−1‖‖Λ1/2(α0 −α∗)‖2 ≤ ‖Λ−1‖‖Λ‖‖α0 −α∗‖2 [2]\n≤ ( γmax γmin ) ‖α0 −α∗‖2. (46)\nStep [1] is obtained by recursion on t. Step [2] is derived by the definition of the matrix Λ.\nUsing last two lemmas, we prove the theorem: E [ Q(α(t))−Q∗ ] [1] = 1 2 E [ ‖α(t) −α∗‖2Lρ ] + 1 2 E [ ‖α(t) −α∗‖2Sρ ] [2]\n≤ 1 2\n( 1− γminρ\nn\n)t (Q(α0)−Q∗) + 1 2 E [ ‖S1/2ρ ( α(t) −α∗ ) ‖2 ]\n[3] ≤ 1 2\n( 1− γminρ\nn\n)t (Q(α0)−Q∗) + 1\n2 ( γmax γmin ) ρ‖α0 −α∗‖2 (47)\nwhere we used following facts:\n[1]: This step is obtained from spectral decomposition of the hessian matrix (see Eq. (32)).\n[2]: Recursion on the result of lemma 11 yields the inequality.\n[3]: This inequality is a direct result of lemma 12.\nThe suboptimality bound of the last theorem provides a fast convergence rate up to a suboptimal solution that is dominated by the initial distance ‖α0 − α∗‖2. In the next lemma, we bound this distance using the homotopic parameter ν. Lemma (9). Assume that the data set is τ -bounded. Then the distance between two minimizers of the dual objective with different regularizers is bounded as\n‖α∗ν −α∗µ‖2 ≤ ( (µ− ν)2/ν ) |{j : λj < ρ}|nτρ. (48)\nProof. Lemma 6 with some straight forward algebra conclude the result."
    }, {
      "heading" : "7.2 Generalized Linear Model",
      "text" : "The gradient of GLM Recall the objective of GLM: R(z) = E [ ϕ(x>z)− y ( x>z )] . (49)\nSuppose that the x are from a multivariate normal distribution, i.e. x ∼ N(~0,Σ). Then one can use integration by parts (Stein’s lemma [9]) to write the gradient of the above objective as\nR′(z) = −Ex,y [yx] + Ex [ xϕ′(x>z) ] (50)\n[Stein’s lemma] = −Ex,y [yx] + Ex [ ϕ(2)(x>z) ] Σz. (51)\nwhere ϕ(2) denote the second derivate of ϕ(.). The Hessian matrix of R highly depends on the covariance matrix of the distribution of input:\nR′′(z) = E [ ϕ(2)(x>z) ] Σ + E [ ϕ(3)(x>i z)x ] z>Σ (52)\n[Stien’s lemma] = E [ ϕ(2)(x>z) ] Σ + E [ ϕ(4)(x>z) ] Σzz>Σ. (53)\nUsing the above the Hessian and the gradient, we provide a useful expression of the gradient that facilitates the our convergence analysis. Lemma 13. There exists constants ξ1, and ξ2, which depend on z, and z∗ such that R′(z) = ξ1Σ(z− z∗) + ξ2Eyx [yx] (54) holds.\nProof. According to mean-value theorem, there is a z̄ = (1− θ)z + θz∗, θ ∈ (0, 1) such that R′(z)−R′(z∗) = R′′(z̄)(z− z∗) (55)\n(52) = E [ ϕ(2)(x>z̄) ] Σ(z− z∗) + E [ ϕ(4)(x>z̄) ] Σz̄ ( z̄>Σ(z− z∗) ) (56)\n= E [ ϕ(2)(x>z̄) ] Σ(z− z∗) + c1Σz̄ (57)\n= c2Σ(z− z∗) + c1Σz (58) (50) = c2Σ(z− z∗) + c3 (R′(z) + Ex,y [yx]) , (59)\nwhere constants are\nc1 = E [ ϕ(4)(x>z̄) ] ( z̄>Σ(z− z∗) ) (60)\nc2 = E [ ϕ(2)(x>z̄) ] − θc1 (61)\nc3 = c1\n( E [ ϕ(2)(x>z̄) ])−1 . (62)\nThe Eq. 59 with the optimality conditionR′(z∗) = 0 conclude the proof for ξ1 = c2/(1− c3), and ξ2 = c3/(1− c3).\nA biased gradient step We suggest our modified gradient step as\nz(t) = z(t−1) − γtR′(z(t−1))− ηtE [yx] (63) where constant γt and ηt are two step sizes. In the next lemma, we prove that the convergence of such a modified gradient descent depends on the covariance matrix of the distribution. Lemma 14. Let iterate z(t) is obtained by t GD steps on GLM. There is a schedule for γt and ηt such that modified GD steps can be written as\nz(t+1) − z∗ = (I− Σ/L) ( z(t) − z∗ ) , (64)\nwhere L = ‖Σ‖.\nProof. We prove the above bound by induction on t.\nz(t+1) − z∗ = z(t) − z∗ − γtR′(z(t))− ηtE [yx] (65) Lemma 13\n= z(t) − z∗ − γtξ1Σ(z(t) − z∗)− γtξ2E [yx]− ηtE [yx] (66) The above equation with step size γt = (Lξ1)−1 and η = −γtξ2 completes the proof. holds. Replacing the above equation in the convergence bound with the step size γt = (LEx [ ϕ(2)(x>z̄(t)) ] )−1 concludes the proof.\nAlthough the last lemma does not specify step sizes, one can find this schedule by line-search. The result allows us to track iterates of GD in a closed form, which provides an accelerated convergence up to a suboptimal solution for GLM – similar to ridge regression. The next lemma proves this. Lemma (10). Suppose that inputs are drawn i.i.d from a multivariate normal distribution with mean 0 and covariance matrix Σ, i.e. xi ∼ N(~0,Σ) where ‖Σ‖ = L. Assume that |ϕ(2)(a)| ≤ φ. If the dataset is τ -bounded, then there is a step size schedule for modified gradient descent such that iterates of GD starting from zero obtains the following suboptimality bound for all 0 < ζ < L:\nR(z(t))−R∗ ≤ cz∗τφ ( (1− ζ/L)2t (1− r(ζ)) + r(ζ)ζ ) , (67)\nr(ζ) := |{j : E[Z2j ] > ζ}|, cz∗ := (Ex [ ϕ(2)(x>z∗) ] )−1. (68)\nProof. Using the mean-value theorem, there is z̄(t) = θz(t) + (1− θ)z∗, θ ∈ [0, 1] such that\nR(z(t))−R(z∗) = ‖z(t) − z∗‖R′′(z̄(t)) (69) (52) = E [ ϕ(2)(x>z̄(t)) ] ‖z(t) − z∗‖Σ (70)\n≤ φ‖z(t) − z∗‖Σ (71) Lemma 14 ≤ φ‖(I− Σ/L)t ( z(0) − z∗ ) ‖Σ (72)\nIt remains to relate the z∗ to the boundedness assumption. To this end, we use optimality condition of z∗ as\nR′(z∗) != 0 z∗ (50)= cz∗Σ−1E[xy], cz∗ := ( Ex [ ϕ(2)(x>z∗) ])−1 (73)\nThe above result, which is also provided in [9], shows that the minimizer of the GLM can be obtained by scaling the minimizer of ridge regression. The convergence bound of Eq. (72) is also scaled convergence of the gradient descent on ridge regression. Plugging the minimizer into the Eq. (72) with the boundedness assumption completes the proof.\n7.3 Experiments"
    } ],
    "references" : [ {
      "title" : "Numerical continuation methods: an introduction, volume 13",
      "author" : [ "Eugene L Allgower", "Kurt Georg" ],
      "venue" : "Springer Science & Business Media,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2012
    }, {
      "title" : "k-means++: The advantages of careful seeding",
      "author" : [ "David Arthur", "Sergei Vassilvitskii" ],
      "venue" : "In Proceedings of the eighteenth annual ACM-SIAM symposium on Discrete algorithms,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2007
    }, {
      "title" : "Greedy layer-wise training of deep networks",
      "author" : [ "Yoshua Bengio", "Pascal Lamblin", "Dan Popovici", "Hugo Larochelle" ],
      "venue" : "Advances in neural information processing systems,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2007
    }, {
      "title" : "Computational and statistical tradeoffs via convex relaxation",
      "author" : [ "Venkat Chandrasekaran", "Michael I Jordan" ],
      "venue" : "Proceedings of the National Academy of Sciences,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2013
    }, {
      "title" : "LIBSVM: A library for support vector machines",
      "author" : [ "Chih-Chung Chang", "Chih-Jen Lin" ],
      "venue" : "ACM Transactions on Intelligent Systems and Technology,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2011
    }, {
      "title" : "The loss surfaces of multilayer networks",
      "author" : [ "Anna Choromanska", "Mikael Henaff", "Michael Mathieu", "Gérard Ben Arous", "Yann LeCun" ],
      "venue" : "In AISTATS,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2015
    }, {
      "title" : "Starting small-learning with adaptive sample sizes",
      "author" : [ "Hadi Daneshmand", "Aurelien Lucchi", "Thomas Hofmann" ],
      "venue" : "In ICML,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2016
    }, {
      "title" : "Primal-dual rates and certificates",
      "author" : [ "Celestine Dunner", "Simone Forte", "Martin Takac", "Martin Jaggi" ],
      "venue" : "In International Conference on Machine Learning,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2016
    }, {
      "title" : "Scaled least squares estimator for glms in large-scale problems",
      "author" : [ "Murat A Erdogdu", "Lee H Dicker", "Mohsen Bayati" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2016
    }, {
      "title" : "Deep Learning",
      "author" : [ "Ian Goodfellow", "Yoshua Bengio", "Aaron Courville" ],
      "venue" : null,
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2016
    }, {
      "title" : "Gradient flow in recurrent nets: the difficulty of learning",
      "author" : [ "Sepp Hochreiter", "Yoshua Bengio", "Paolo Frasconi", "Jurgen Schmidhuber" ],
      "venue" : "long-term dependencies,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2001
    }, {
      "title" : "Ridge regression: Biased estimation for nonorthogonal problems",
      "author" : [ "Arthur E Hoerl", "Robert W Kennard" ],
      "venue" : null,
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 1970
    }, {
      "title" : "Communication-efficient distributed dual coordinate ascent",
      "author" : [ "Martin Jaggi", "Virginia Smith", "Martin Takac", "Jonathan Terhorst", "Sanjay Krishnan", "Thomas Hofmann", "Michael I Jordan" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2014
    }, {
      "title" : "Matrix completion from a few entries",
      "author" : [ "Raghunandan H Keshavan", "Andrea Montanari", "Sewoong Oh" ],
      "venue" : "IEEE Transactions on Information Theory,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2010
    }, {
      "title" : "Data-dependent initializations of convolutional neural networks",
      "author" : [ "Philipp Krähenbühl", "Carl Doersch", "Jeff Donahue", "Trevor Darrell" ],
      "venue" : "arXiv preprint arXiv:1511.06856,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2015
    }, {
      "title" : "Gradient descent only converges to minimizers",
      "author" : [ "Jason D Lee", "Max Simchowitz", "Michael I Jordan", "Benjamin Recht" ],
      "venue" : "In Conference on Learning Theory (COLT),",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2016
    }, {
      "title" : "A universal catalyst for first-order optimization",
      "author" : [ "Hongzhou Lin", "Julien Mairal", "Zaid Harchaoui" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2015
    }, {
      "title" : "Adaptive newton method for empirical risk minimization to statistical accuracy",
      "author" : [ "Aryan Mokhtari", "Hadi Daneshmand", "Aurelien Lucchi", "Thomas Hofmann", "Alejandro Ribeiro" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2016
    }, {
      "title" : "Efficiency of coordinate descent methods on huge-scale optimization problems",
      "author" : [ "Yu Nesterov" ],
      "venue" : "SIAM Journal on Optimization,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2012
    }, {
      "title" : "Stochastic dual coordinate ascent methods for regularized loss minimization",
      "author" : [ "Shai Shalev-Shwartz", "Tong Zhang" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2013
    }, {
      "title" : "No bad local minima: Data independent training error guarantees for multilayer neural networks",
      "author" : [ "Daniel Soudry", "Yair Carmon" ],
      "venue" : "arXiv preprint arXiv:1605.08361,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2016
    }, {
      "title" : "On early stopping in gradient descent learning",
      "author" : [ "Yuan Yao", "Lorenzo Rosasco", "Andrea Caponnetto" ],
      "venue" : "Constructive Approximation,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2007
    }, {
      "title" : "How transferable are features in deep neural networks? In Advances in neural information processing",
      "author" : [ "Jason Yosinski", "Jeff Clune", "Yoshua Bengio", "Hod Lipson" ],
      "venue" : null,
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2014
    } ],
    "referenceMentions" : [ {
      "referenceID" : 21,
      "context" : "This offers learning-specific tradeoffs between statistical accuracy and computational complexity, such as early stopping [22], convex relaxation [4], data sub-sampling [7], or accepting locally optimal solutions as in deep learning [6, 21].",
      "startOffset" : 122,
      "endOffset" : 126
    }, {
      "referenceID" : 3,
      "context" : "This offers learning-specific tradeoffs between statistical accuracy and computational complexity, such as early stopping [22], convex relaxation [4], data sub-sampling [7], or accepting locally optimal solutions as in deep learning [6, 21].",
      "startOffset" : 146,
      "endOffset" : 149
    }, {
      "referenceID" : 6,
      "context" : "This offers learning-specific tradeoffs between statistical accuracy and computational complexity, such as early stopping [22], convex relaxation [4], data sub-sampling [7], or accepting locally optimal solutions as in deep learning [6, 21].",
      "startOffset" : 169,
      "endOffset" : 172
    }, {
      "referenceID" : 5,
      "context" : "This offers learning-specific tradeoffs between statistical accuracy and computational complexity, such as early stopping [22], convex relaxation [4], data sub-sampling [7], or accepting locally optimal solutions as in deep learning [6, 21].",
      "startOffset" : 233,
      "endOffset" : 240
    }, {
      "referenceID" : 20,
      "context" : "This offers learning-specific tradeoffs between statistical accuracy and computational complexity, such as early stopping [22], convex relaxation [4], data sub-sampling [7], or accepting locally optimal solutions as in deep learning [6, 21].",
      "startOffset" : 233,
      "endOffset" : 240
    }, {
      "referenceID" : 22,
      "context" : "There are many examples that clearly demonstrate that initialization matters, for instance in deep learning via weight transfer [23], data-dependent initialization [15], or in avoidance of saddle points [16].",
      "startOffset" : 128,
      "endOffset" : 132
    }, {
      "referenceID" : 14,
      "context" : "There are many examples that clearly demonstrate that initialization matters, for instance in deep learning via weight transfer [23], data-dependent initialization [15], or in avoidance of saddle points [16].",
      "startOffset" : 164,
      "endOffset" : 168
    }, {
      "referenceID" : 15,
      "context" : "There are many examples that clearly demonstrate that initialization matters, for instance in deep learning via weight transfer [23], data-dependent initialization [15], or in avoidance of saddle points [16].",
      "startOffset" : 203,
      "endOffset" : 207
    }, {
      "referenceID" : 13,
      "context" : "The same is true for unsupervised learning problems like matrix completion [14] orK-means [2].",
      "startOffset" : 75,
      "endOffset" : 79
    }, {
      "referenceID" : 1,
      "context" : "The same is true for unsupervised learning problems like matrix completion [14] orK-means [2].",
      "startOffset" : 90,
      "endOffset" : 93
    }, {
      "referenceID" : 0,
      "context" : "In this vein, the current paper provides a detailed analysis of convex learning, specifically of ridge regression and generalized linear models, that suggests to pre-train models with artificially increased regularization and to use this as an initialization in the spirit of homotopy or continuation methods [1, 18].",
      "startOffset" : 309,
      "endOffset" : 316
    }, {
      "referenceID" : 17,
      "context" : "In this vein, the current paper provides a detailed analysis of convex learning, specifically of ridge regression and generalized linear models, that suggests to pre-train models with artificially increased regularization and to use this as an initialization in the spirit of homotopy or continuation methods [1, 18].",
      "startOffset" : 309,
      "endOffset" : 316
    }, {
      "referenceID" : 19,
      "context" : "through the use of kernels), allows for fast algorithms like stochastic coordinate descent [20] that exhibit linear convergence and is ar X iv :1 70 6.",
      "startOffset" : 91,
      "endOffset" : 95
    }, {
      "referenceID" : 12,
      "context" : "also amenable to data sharding and communication-efficient distributed implementations [13].",
      "startOffset" : 87,
      "endOffset" : 91
    }, {
      "referenceID" : 11,
      "context" : "1 Primal Formulation For concreteness, we perform an in-depth analysis of ridge regression [12].",
      "startOffset" : 91,
      "endOffset" : 95
    }, {
      "referenceID" : 21,
      "context" : "This seems to be related to a similar assumption made in early stopping [22] and in general when using norm-based regularization: we do not want to trust features that need to be amplified a lot (low variance, but high output covariance).",
      "startOffset" : 72,
      "endOffset" : 76
    }, {
      "referenceID" : 7,
      "context" : "see [8].",
      "startOffset" : 4,
      "endOffset" : 7
    }, {
      "referenceID" : 18,
      "context" : "Random coordinate descent method (RCDM) [19, 20] is computationally more attractive than GD and offers a competitive convergence.",
      "startOffset" : 40,
      "endOffset" : 48
    }, {
      "referenceID" : 19,
      "context" : "Random coordinate descent method (RCDM) [19, 20] is computationally more attractive than GD and offers a competitive convergence.",
      "startOffset" : 40,
      "endOffset" : 48
    }, {
      "referenceID" : 18,
      "context" : "(18), the classical analysis of RCDM [19] suggests the rate 1− (n+ 2B/μ)−1, while our analysis improves the rate by a factor of ρ/n > 1 to 1− (ρ/n)(n+ 2B/μ)−1.",
      "startOffset" : 37,
      "endOffset" : 41
    }, {
      "referenceID" : 16,
      "context" : ", as in the catalyst method [17]), but also with respect to the sample",
      "startOffset" : 28,
      "endOffset" : 32
    }, {
      "referenceID" : 18,
      "context" : "In our experiments, we used larger step sizes γr = G−1 r,r , which is equal to the coordinate-wise Lipschitz constants [19] of the dual ridge objective.",
      "startOffset" : 119,
      "endOffset" : 123
    }, {
      "referenceID" : 8,
      "context" : "Given a convex differentiable smooth function φ, a generalized linear model (GLM) aims at minimizing the non-quadratic objective [9] R(z) = E [ φ(xi z)− yi ( xi z )] , (22) where the expectation is over the population distribution of the data.",
      "startOffset" : 129,
      "endOffset" : 132
    }, {
      "referenceID" : 8,
      "context" : "Although the data is assumed to be generated from a normal distribution in the last lemma, we believe that this result can be extended to an arbitrary distribution using zero-bias transformations (see [9] for more details).",
      "startOffset" : 201,
      "endOffset" : 204
    }, {
      "referenceID" : 9,
      "context" : "This observation justifies the effectiveness of one of the most common initialization schemes of deep neural networks: Use resulting weights of a trained shallow network to initialize a deep neural network [10].",
      "startOffset" : 206,
      "endOffset" : 210
    }, {
      "referenceID" : 10,
      "context" : "In practice, we do not need to freeze weights of early layers because back propagation naturally causes small changes in early layers due to the vanishing gradient phenomenon [11].",
      "startOffset" : 175,
      "endOffset" : 179
    }, {
      "referenceID" : 2,
      "context" : "Our argument can also be extended to the layer-wise training for deep neural networks [3].",
      "startOffset" : 86,
      "endOffset" : 89
    }, {
      "referenceID" : 4,
      "context" : "2 In this section, we present our empirical results on real datasets, selected from LIBSVM library [5] (see Table 1 for more details).",
      "startOffset" : 99,
      "endOffset" : 102
    }, {
      "referenceID" : 18,
      "context" : "We used coordinate-wise step sizes γr = G−1 r,r , which are equal coordinate-wise Lipchitz constants and its a common choice for RCDM [19].",
      "startOffset" : 134,
      "endOffset" : 138
    }, {
      "referenceID" : 19,
      "context" : "To compute the primal suboptimality, we mapped the dual parameter to the primal one using the mapping β = (nμ)−1Xα(t) (used in [20]).",
      "startOffset" : 127,
      "endOffset" : 131
    } ],
    "year" : 2017,
    "abstractText" : "Gradient descent and coordinate descent are well understood in terms of their asymptotic behavior, but less so in a transient regime often used for approximations in machine learning. We investigate how proper initialization can have a profound effect on finding near-optimal solutions quickly. We show that a certain property of a data set, namely the boundedness of the correlations between eigenfeatures and the response variable, can lead to faster initial progress than expected by commonplace analysis. Convex optimization problems can tacitly benefit from that, but this automatism does not apply to their dual formulation. We analyze this phenomenon and devise provably good initialization strategies for dual optimization as well as heuristics for the non-convex case, relevant for deep learning. We find our predictions and methods to be experimentally well-supported.",
    "creator" : "LaTeX with hyperref package"
  }
}