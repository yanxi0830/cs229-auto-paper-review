{
  "name" : "1602.02196.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "BISTRO: An Efficient Relaxation-Based Method for Contextual Bandits",
    "authors" : [ "Alexander Rakhlin", "Karthik Sridharan" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n60 2.\n02 19\n6v 1\n[ cs\n.L G\n] 6\nF eb"
    }, {
      "heading" : "1 Introduction",
      "text" : "A multi-armed bandit with covariates (also known as a contextual bandit) is a generalization of the classical multi-armed bandit problem [LR85]. As the name suggests, in this natural formulation the quality of the arms may depend on the observed set of covariates. Contextual bandits arise in many application areas, from ad placement and news recommendation to personalized medical care and clinical trials. In recent years, there has been a strong push to develop computationally efficient regret minimization methods with respect to a given set of policies [LZ08, DHK+11, BLL+11, AHK+14]. The grand goal here would be to develop efficient and statistically optimal methods for large (and possibly uncountable) sets of policies, just as machine learning and statistics succeeded in developing methods that perform well relative to rich classes of predictors (linear separators, SVMs, and so forth). Compared to batch learning, however, the state of affairs at the moment is quite poor. It appears to be difficult to develop scalable methods even for a finite set of policies, as witnessed by the papers mentioned earlier. To some extent, the reason is not surprising: while in statistical learning the batch nature of the problem suggests the empirical objective to optimize, the scope of algorithms for contextual bandits is not at all clear.\n[AHK+14] exhibit a computationally attractive method for a finite class of policies, given an ERM (empirical risk minimization) oracle for the class. The oracle model allows one to address the question of how much more difficult (computationally) the bandit problem is in comparison to the batch learning problem.\nIn the present paper, we introduce a family of efficient methods (and, more generally, a new algorithmic approach based on relaxations) for minimizing regret against a potentially uncountable\nclass F , given that the value of the ERM objective can be computed. In addition, we require access to i.i.d. draws of contexts (e.g. unlabeled data) — a realistic assumption in many application areas mentioned earlier. Our method requires only d oracle calls per round, irrespective of the size of the policy class. Furthermore, the results hold in the hybrid scenario where the contexts are i.i.d. but rewards evolve according to an arbitrary process.\nLet us now describe the scenario in more detail. On each round t = 1, . . . , n, we observe covariates xt ∈ X , select an action ŷt ∈ {1, . . . , d} ≜ [d], and observe the cost ct(ŷt) of the chosen action. Here ct ∈ [0,1]d is a cost assignment to all actions, chosen by Nature independently of ŷt. This cost vector remains unknown to us, except for the coordinate ct(ŷt). Since we include randomized prediction methods, we denote the distribution over the d choices on round t by qt ∈ ∆d, and draw ŷt ∼ qt. The goal is to design a prediction method with small expected cumulative cost ∑nt=1 qTt ct.\nWe assume that x1, . . . , xn are drawn i.i.d. from some unknown distribution Px on X . At the same time, we do not place any assumption on the sequence of costs c1, . . . , cn, which may evolve according to some arbitrary stochastic process, or be an “individual sequence,” or even be chosen adaptively and adversarially. As such, our setting may be termed “hybrid i.i.d.-adversarial.” Our results also hold in the so-called transductive setting, where the side information is presented ahead of time.1\nWe have in mind machine learning applications such as online ad or product placement, whereby the contextual information x1, . . . , xn of website visitors may be viewed as an i.i.d. sequence, yet the decisions made by these customers might be too complex to be described in a probabilistic form.\nA common way to encode the prior knowledge about the problem is to take a class F of functions (or, deterministic policies) X → [d], with the hope that one of the functions will incur small cost on the presented contexts. With this “inductive bias,” we then aim to make predictions as to minimize regret\nReg = n∑ t=1 qTt ct − inf f∈F n∑ t=1 f(xt)Tct, (1) where henceforth we abuse the notation by identifying the value f(x) ∈ [d] with the standard basis vector ef(x). This regret formulation encodes the prior knowledge of the practitioner. If the modeling choice F is good and (1) is small, the algorithm is guaranteed to incur small loss∑nt=1 qTt ct. Modeling the set of solutions F to the problem is a more direct approach (in the spirit of statistical learning) as compared to the harder problem of positing distributional assumptions on the relationship between contexts and the rewards. (The latter approach typically suffers from the curse of dimensionality.)\nThe difficulty of the problem arises from the form of the feedback. The customer seeking to buy a product different from what is presented by the recommendation engine may leave the site without revealing her valuation for all the items. Similarly, in personalized care, we may only observe the effect of the drug choice selected for the given patient. It is well recognized that exploration—or randomization—is required in these problems. Yet, in the contextual bandit setting the explorationexploitation trade-off is not simple, as the quality of the arms changes with the context in a way that is only indirectly captured by the benchmark term.\nOnline multiclass classification with one bit (correct-or-not) feedback can be seen as an example of our setting. In that case ct is a standard basis vector eyt for some class yt ∈ [d], and the feedback\n1In Section 6 we also discuss the fully-adversarial case (see [ACBFS02, MS09] for the famous EXP4 algorithm for finite F).\nis ct(ŷt) = I{ŷt ≠ yt}. Unlike [KSST08], we posit that side information is i.i.d.—an assumption that will play a key role in developing computationally efficient methods, even for the indicator (rather than the easier hinge) loss.\nThe hybrid i.i.d.-adversarial scenario has been studied in both the full information and contextual bandit settings in [LM09]. Their algorithm, as well as the algorithm of [BLL+11], maintain distributions over the set of functions and, hence, computation can be linear in the size of F .\nFor the case whenF is finite, the upper bound for BISTRO provided in Theorem 2 isO(n3/4(log ∣F ∣)1/4). The work of [AHK+14] gives a better O(n1/2(log ∣F ∣)1/2) rate for the case when rewards are i.i.d. On the other hand, our results hold for\n• arbitrary F and arbitrary reward sequences,\n• approximate ERM values and a way to address the computational problem associated to ERM.\nWe remark that if contexts are arbitrary as well, our setting subsumes the problem of multiclass prediction with bandit feedback and indicator loss, as described above. Even for the multiclass hinge loss, it is still unclear (at least to the authors) whether the rate O(n2/3) for the linear classifier considered in [KSST08] can be improved.2 It is, therefore, an open question whether the O(n3/4) rates achieved by our method for the hybrid scenario for arbitrary classes F can be improved.\nThere are several new techniques that make it possible to develop computationally feasible prediction methods with nontrivial regret guarantees:\n• First is the idea of relaxations, presented in [RSS12] for the full-information setting. An extension to partial information case has been a big roadblock for developing new bandit methods. We present this extension here.\n• Second is the idea of a random playout, also employed in [RS15]. We show that by having access to unlabeled contexts, the computational (and statistical) difficulty of integrating with respect to the unknown distribution simply disappears.\n• We extend the notion of classical Rademacher averages to the case of vector-valued functions. The symmetrization technique in this case is of independent interest.\n• In many cases, the offline ERM optimization problem (which we assume away as an “oracle call”) may be NP hard. Building on the technique of [RS15], we employ optimization-based relaxations for integer programs. We prove that the regret bound of the resulting algorithm only worsens by a multiplicative factor that is related to the ratio of average widths of the relaxed and the original sets.\nIt is worth emphasizing again that the family of prediction methods presented in this work is drived from the partial-information extension of the relaxation framework, and the resulting algorithms are distinct from the ones appearing in the literature. We believe that this approach is systematic and can partially fill the gap in our understanding of the algorithmic possibilities for contextual bandits.\n2The O(n1/2) rate in [HK11] is only proved for the case of log-loss."
    }, {
      "heading" : "2 Notation",
      "text" : "We denote [d] ≜ {1, . . . , d} and a1∶t ≜ {a1, . . . , at}. Let ∆d be the probability simplex over d coordinates. The vector of ones is denoted by 1 and an indicator of event A by I{A}. For a matrix M , we use Mt to refer to its t-th column."
    }, {
      "heading" : "3 Setup",
      "text" : "Let us recall the online protocol. On each round t ∈ [n], we observe side information xt ∈ X , predict ŷt ∼ qt ∈∆d, and observe feedback ct(ŷt) for some ct ∈ [0,1]d.\nGiven x1∶n, it is convenient to work with a matrix representation of the class F projected on these data. Each f ∈ F yields sequence (f(x1), . . . , f(xn)), which we collect as a d × n matrix Mf , defined as Mf(j, t) = I{f(xt) = j} . (2) Let M̂ = M̂[x1∶n] = {Mf ∶ f ∈ F} denote the collection of matrices. (The hat on M̂ will remind us of the dependence of this set on x1∶n, even if not explicitly mentioned).\nWe may now define the oracle employed by the prediction method:\nDefinition 1. Given a class F of policies X → [d], a set of covariates x1∶n, and a real-valued d ×n matrix Y , a value-of-ERM oracle returns the value\ninf M∈M̂[x1∶n] n∑ t=1 MTt Yt . (3)\nThe oracle is called δ-approximate if the reported value is within δ from the minimum.\nWe may express the comparator term in (1) as an ERM objective (3) with Y = [c1, . . . , cn]. Closely related to this expression is a new (to the best of our knowledge) definition of Rademacher averages for vector-valued functions: given x1∶n, define\nR(F ;x1∶n) ≜R(M̂) ≜ Eǫ1∶n sup M∈M̂ n∑ t=1 MTt ǫt (4)\nwhere ǫ1, . . . ,ǫn are d-dimensional vectors with independent Rademacher random variables. We observe that Rademacher complexity is nothing but a (negative of) the ERM objective with the random matrix [−ǫ1, . . . ,−ǫn]. Indeed, as in the classical case, correlation of the vector valued function class F with noise measures its complexity."
    }, {
      "heading" : "4 Relaxations for Partial Information",
      "text" : "Let us write the information obtained on round t as a tuple It(xt, qt, ŷt, ct) = (xt, qt, ŷt, ct(ŷt)), keeping in mind that xt is revealed before qt is chosen. In full information problems, It contains the vector ct, but not so in our bandit case. For partial information problems, it turns out to be crucial to include qt in the definition of It, in addition to the value ct(ŷt).\nA partial-information relaxation Rel () is a function that maps (I1, . . . , It) to a real value, for any t ∈ [n]. We say that the partial-infromation relaxation Rel (I1, . . . , It) is admissible if for any t ∈ [n], for all I1, . . . , It−1,\nE xt inf qt max ct E ŷt∼qt {ct(ŷt) +Rel (I1∶t−1, It(xt, qt, ŷt, ct))} ≤Rel (I1∶t−1) (5) and for all x1∶n,c1∶n, and q1∶n,\nE\nŷ1∶n∼q1∶n Rel (I1∶n) ≥ − inf f∈F n∑ t=1 f(xt)Tct . (6) In the above expressions, xt follows the (unknown) distribution Px, qt ranges over distributions on[d], and ct over [0,1]d.\nAny randomized strategy (qt)nt=1 that certifies the inequalities (5) and (6) is called an admissible strategy. Lemma 1. Let Rel () be an admissible relaxation and (qt)nt=1 an admissible strategy. Then for any c1∶n,\nE[Reg] ≤Rel (∅) . The above partial-information relaxation setup appears to be “the right” analogue of the fullinformation relaxation framework. While we do not present it here, one may recover the EXP4 algorithm through the above approach, with the correct regret bound.\nWe will now present an admissible strategy for the contextual bandit problem, assuming we can sample from the distribution Px, or have access to unlabeled data."
    }, {
      "heading" : "5 The BISTRO Algorithm",
      "text" : "For any t ∈ [n], define a d × n matrix Y (t) as Y (t) = [c1, . . . , ct−1, ct,2ǫt+1, . . . ,2ǫn] with ǫs ∈ {±1}d a vector of independent Rademacher random variables. At each step t ∈ [n], the randomized method presented below calculates a distribution qt ∈∆d with each coordinate at least γ and defines an unbiased estimate c̃t of ct in a usual manner as c̃t(j) = I{ŷt = j} × ct(ŷt)/qt(j). It is standard to verify that Eŷt∼qt c̃t = ct. We then define Ỹ (t) = [c̃1, . . . , c̃t−1, c̃t,2γ−1ǫt+1, . . . ,2γ−1ǫn], (7) and recall that Ỹ (t) s denotes the s-th column of this matrix. The next theorem is the main result of the paper.\nTheorem 2. The partial-information relaxation\nRel (I1∶t) = E (x,ǫ)t+1∶n sup M∈M̂ {− n∑ s=1 MTs Ỹ (t) s } + (n − t)γ (8)\nis admissible. An admissible randomized strategy for this relaxation is given by BISTRO (Algorithm 1). The expected regret of the algorithm with γ = √ 2ER(F ;x1∶n)/(nd) is upper bounded by 2 √ 2d ⋅ n ⋅ER(F ;x1∶n).\nAlgorithm 1 BISTRO: BandItS wiTh RelaxatiOns input Parameter γ ∈ (0,1/d) 1: for t = 1, . . . , n do 2: Observe xt. Draw xt+1∶n ∼ Px and ǫt+1∶n . 3: Construct Ỹ (t) and define q∗t to be a minimizer of\nmax j∈[d] ⎧⎪⎪⎨⎪⎪⎩q T ej − min M∈M̂[x1∶n] {∑ s≠t γMTs Ỹ (t) s +M T t ej}⎫⎪⎪⎬⎪⎪⎭ over q ∈∆d and set\nqt = (1 − γd)q∗t + γ1. (9) 4: Predict ŷt ∼ qt and observe ct(ŷt). 5: Create an estimate c̃t:\nc̃t(j) = I{ŷt = j} × ct(ŷt)/qt(j). 6: end for\nThe draw xt+1∶n ∼ Px can be realized by drawing from a pool of unlabeled data. The random signs comprising the matrix Ỹ provide a form of “regularization”. We remark that in experiments, one may obtain better performance by replacing the factor 2 in (7) with a smaller value, or even with zero. A theoretical justification for this (which is related to using a surrogate loss) is beyond the scope of this paper.\nLemma 3. The calculation of q∗t in BISTRO 3 can be done by a water-filling argument and requires d calls to the ERM oracle.\nProof of Lemma 3. The optimization problem in Algorithm 1 is of the form\nmin q∈∆d max j∈[d] {qj − ψj} where ψj is the value of the infimum over M̂ corresponding to ej , and it is solved by a water-filling argument which we describe next. Each value ψj is a value-of-ERM oracle call. Let ψ(1) ≥ . . . ≥ ψ(d) be a sorted order of these values, and let q(1) = . . . = q(d) = 0 be the initial values of the corresponding coordinates of the solution q. Start with a unit amount and assign q(1) = ψ(1) − ψ(2). Then add ψ(2)−ψ(3) to both q(1) and q(2), and proceed until either the unit mass is exhausted, or the smallest coordinate (d) in the ordering is reached and filled. In the former case, q is the solution, and the latter case requires us to uniformly fill all the coordinates of q until they sum to one. It is easy to see that this procedure minimizes the maximum difference.\n3‘Bistro’ means ‘fast’ in Russian.\nThe algorithm only requires the value of the ERM objective, not the solution. Furthermore, this value can be δ-approximate, and the additional error is O(nδ) over the n rounds. This provides extra flexibility, since approximate ERM values may be obtained via optimization methods.\nPerhaps the most unusual aspect of the algorithm is the use of unlabeled data. It is an example of a general random playout idea. In the setting of online linear optimization, the Follow-thePerturbed-Leader method is an example of such a random playout, yet the idea extends well beyond this scenario. As shown in [RSS12], the random playout technique can be applied when a certain worst-case-choice can be replaced with a known bad-enough distribution. However, when side information xt is i.i.d., the step is not even required. Furthermore, an inspection of the proof shows that we may deal with x’s coming from a non-i.i.d. stochastic process, as long as we are able to draw future samples from it.\nWe also remark that (9) may be applied only to the coordinates that are close to zero, if any. The potential suboptimality of the O(n3/4) bound stems from the uniform exploration. It is an open question whether this can be improved systematically for all classes F , or whether there is a different structural property that allows one to avoid this form of exploration."
    }, {
      "heading" : "6 Extensions",
      "text" : "In this section, we outline several extensions of BISTRO. Specifically, we show how to incorporate additional data-based constraints, and how to use further optimization-based relaxations (such as LP or SDP), to obtain polynomial time methods for the ERM (or regularized ERM) solution. We show that one obtains a regret bound that only worsens by a factor related to the integrality gap of the integer program relaxation. With an eye on both computation and prediction performance, these techniques expand the applicability of BISTRO."
    }, {
      "heading" : "6.1 Data-dependent policy classes",
      "text" : "An inspection of the proof reveals that all the steps go through if define regret in (1) with respect to a data-dependent class F[x1∶n]:\nn∑ t=1 qTt ct − inf f∈F[x1∶n] n∑ t=1 f(xt)Tct. (10) In this case, given x1∶n, to each f ∈ F[x1∶n] we associate Mf as defined in (2), and take M̂ = {Mf ∶ f ∈ F[x1∶n]}. The BISTRO algorithm is then identical, while the regret upper bound of Theorem 2 now replaces ER(F ;x1∶n) with ER(F[x1∶n];x1∶n).\nThe ability to change the set of policies according to the actual data allows an extra degree of flexibility. This flexibility can be realized via additional global constraints in terms of x1∶n, as we show in the next few sections. We also discuss a concrete example."
    }, {
      "heading" : "6.2 Data-based constraints",
      "text" : "A particular way to define a data-dependent subset of F is via constraints. Suppose we let C(f ;x1∶n) be the degree to which f ∈ F violates constraints with respect to the given data x1∶n. We then\ndefine FK[x1∶n] = {f ∈ F ∶ C(f ;x1∶n) ≤K}, (11) a pruning of the original class that keeps only those policies that do not violate the constraints by more than K. Let us give an example.\nExample: Product Recommendation Suppose at each time step we are asked to recommend one of d products to a person, based on her covariate information xt. Let F be a set of policies that map xt to the particular choice of the product (e.g. the label achieving maximum projection of xt onto d vectors wj; here F may consist of all such unit vector tuples). The payoff is whether the person decided to buy the recommended product. However, suppose xt also encodes the location (physical, or within a network), and we believe it is a good idea to focus recommendations such that near-by people are targeted with the same product. The marketing motivation here is two-fold: first, the recommendations would reinforce each other when individuals communicate, or if one of them buys the product; second, in a social network near-by individuals (friends) tend to have similar tastes, and thus a good policy would suggest similar items.\nThe objective of enforcing similarity of recommendations is a global constraint that can only be checked once we know all the x1, . . . , xn. We can easily incorporate the constraint into the definition of FK[x1∶n] as follows. Let w(xs, xr) be the cost of providing different recommendations to xs and xr (which is smaller if the two individuals are “far”). In the case of a network, we may set, for instance, w(xs, xr) = 0 if the sth person is more than a hop away from the rth person. Define\nC(f ;x1∶n) = ∑ s,r∈[n] w(xs, xr)I{f(xs) ≠ f(xr)} , (12) the constraint violation by f in assigning products to the given set of individuals. Let FK[x1∶n] be defined as in (11). Note that the constraint is not on the behavior of the recommendation engine, but on the set of policies that we hope will do well for the problem. If there is indeed the effect of reinforcement of recommendations or similarity of tastes within the local neighborhood, the restriction to a smaller set FK[x1∶n] is justified.\nWithin the same setting of product recommendation, we might instead take a set of policies ensuring that within each neighborhood at least k individuals receive each particular product recommendation. This constraint, which roughly corresponds to “coverage” of the relevant population, can be written as\nC(f ;x1∶n) =∑ ℓ ∑ j∈[d] ⎡⎢⎢⎢⎢⎣k − ∑s∈Tℓ f(xs)[j] ⎤⎥⎥⎥⎥⎦+\nwhere {Tℓ}ℓ is a partition of [n] into neighborhoods according to information contained in x1∶n. The above two examples give a flavor of the constraints that can be encoded — the framework is flexible enough to fit a wealth of scenarios.\nFrom the computational point of view, it might be difficult to obtain the ERM value over a constrained set FK[x1∶n]. Instead, we consider an additional form of relaxation, where the constraint is subtracted off as a Lagrangian term. We will then employ certain linear programming relaxations to solve the product recommendation problem. Notably, by going to a regularized version of relaxations we are not changing the regret definition, which is still with respect to the constrained set."
    }, {
      "heading" : "6.3 Regularized relaxation",
      "text" : "Let FK[x1∶n] = {f ∈ F ∶ C(f ;x1∶n) ≤ K} be the constrained set for some value K and a constraint function C, as in the previous section. Let us write C(M ;x1∶n) for the matrix representation the corresponding f ∈ F . The following form of a relaxation may be better suited for approximation algorithms than the one where the constraint is strictly enforced.\nLemma 4. For any λ,K > 0, the partial-information relaxation\nE\n(x,ǫ)t+1∶n sup M∈M̂ {− n∑ s=1 MTs Ỹ (t) s − λC(M ;x1∶n)}\n+ λK + (n − t)γ (13) is admissible, where M̂ denotes the matrix representation of the original (unconstrained) set F of policies.\nProof of Lemma 4. We check that the initial condition is satisfied. For this purpose, let M̂K be the set of matrices corresponding to the constrained set FK[x1∶n]. Similarly to (18) in the proof of Theorem 2,\n− inf f∈FK[x1∶n] n∑ t=1 f(xt)Tct ≤ E sup M∈M̂K n∑ t=1 −MTt Ỹ (n) t ≤ E sup M∈M̂ { n∑ t=1 −MTt Ỹ (n) t − λC(M ;x1∶n)} + λK.\nThe second inequality holds since all the matrices in the former supremum have the constraint value bounded by K. The recursive condition argument follows exactly as in the proof of Theorem 2.\nThe only change required for BISTRO is to define the optimization objective in terms of regularized ERM values\nmin M∈M̂ {∑ s≠t γMTs Ỹ (t) s +M T t ej + γ −1λC(M ;x1∶n)} (14)\nover the unconstrained set of matrices corresponding to F . While the required minimization problem is over an unconstrained set of policies, we can control the expected regret\nn∑ t=1 qTt ct − inf f∈FK[x1∶n] n∑ t=1 f(xt)Tct. (15) of the modified BISTRO with respect to the constrained set FK[x1∶n], which is the original goal. The regret is given by Rel (∅), which is at most\nE sup M∈M̂ {−γ−1 n∑ t=1 MTt ǫt − λC(M ;x1∶n)} + ndγ + λK. It is possible to optimally balance λ with respect toK and the Rademacher averages in a data-driven manner, but we omit this step for brevity.\nAs we illustrate in the next section, optimization problems of the form (14) may admit a linear programming (or other) relaxation, offering an alternative to the optimization problem over the constrained set."
    }, {
      "heading" : "6.4 Optimization-based relaxations",
      "text" : "To make the algorithm of this paper more applicable, we discuss here the situation where the ERM oracle or the regularized ERM oracle for the class FK[x1∶n] (or the unconstrained set F) is a difficult or even an NP-hard integer program. The idea is to choose a superset M̃ ⊇ M̂ for which the linear optimization problem is easier. Lemma 5. Let M̃ ⊇ M̂ be a set of matrices such that the column sum ∑dj=1Mt(j) ≤ 1 for any M ∈ M̃ and t ∈ [n]. Then the partial information relaxation\nRel (I1∶t) = E (x,ǫ)t+1∶n sup M∈M̃ {− n∑ s=1 MTs Ỹ (t) s } + (n − t)γ\nis admissible. BISTRO (with ERM over M̃ rather than M) is an admissible strategy for this relaxation and the expected regret is upper bounded by\n2 √\n2d ⋅ n ⋅ER(M̃). Similarly, using M̃ in (13) yields an admissible relaxation, and BISTRO with the corresponding regularized ERM is an admissible strategy.\nThe set M̃[x1∶n] may be defined via linear programming or SDP relaxations for integer programs, or via Lasserre/Parrilo hierarchies [Las01, Par03]. There is a large body of literature that aims at understanding the integrality gap in relaxing the integer program. These results are directly applicable to the present problem.\nAs a concrete example, consider the product recommendation example in the previous section, and consider the cost (12) for each policy and the restriction FK[x1∶n] in (11). We assume here that F is the set of all possible labelings, since in general the optimization problem will depend on the structure of F and its description. Let us phrase the regularized ERM integer program (14) as a Metric Labeling Constraint [KT02] problem. The general form of this integer program is given for z ∈ [d]n by\ng(z) = ∑ v∈V d1(v, zv) + ∑ (u,v)∈E W(u,v)d2(zu, zv) (16) where G = (V,E,W ) is a graph with nonnegative weights, ∣V ∣ = n, the value d1 ∶ V × [d] → R is a cost of assigning a label to a node, and the separation cost d2 ∶ [d] × [d] → R≥0 on the edges is a metric on the space of labels. The Metric Labeling Constraint problem asks for a solution that minimizes g(z) over [d]n.\nFor our application to product recommendation we convert the regularized minimization objective of (14) with the constraint (12) into the above form (16) by matching the assignment costs to the linear part and the separation costs to the constraint part (12). More precisely, let G be a fully connected graph with weights W(s,r) = γ\n−1λ ⋅w(xs, xr) between nodes corresponding to xs and xr. The indices of vertices correspond to time steps in [n], and zv corresponds to the coordinate chosen by the particular M at time v. We take d1(v, zv) to be the value γeTzv Ỹ (t)v if v ≠ t and eTzvej if v = t. Define d2(a, b) = I{a ≠ b} to be the uniform metric. We may also define a metric on the space of products, assigning smaller distance to similar items.\n[KT02] give an LP relaxation for the Metric Labeling Constraint problem. The set that defines the relaxation is precisely the set M̃ we seek. Furthermore, the authors prove a 2-approximation ratio for the uniform metric, which is the case here. ([CKNZ04] prove an integrality gap of O(log k) for the general case).\nGiven the 2-approximation ratio result, we conclude that the regret bound for BISTRO with the LP program as the relaxation of the regularized ERM is only a constant worse than the bound with the constrained set FK[x1∶n]. The exact optimization over the latter set may be computationally intractable, while we provide an efficient method to achieve a bound, optimal to within a constant. As already noted in [RS15], such an approach that fuses approximation algorithms and online relaxations is able to produce polynomial-time methods with regret defined as 1× the benchmark, while the benchmark itself may be NP-hard. This phenomenon can be attributed to the improper nature of the predictions, which need not be consistent with any particular policy in F .\nMore generally, by obtaining a multiplicative approximation of gap for the integer program, one may derive ER(M̃[x1∶n]) ≤ O(gap) × ER(M[x1∶n]). (17) Then one obtains a method with better computational properties and a regret bound which is only O(√gap) worse. Once again, the factor in front of the comparator in the definition (1) of regret is still one when using M̃ as a relaxation.\nFinally, we remark that (17) is comparing an average width of M̃ (largest projection onto noise) with an average width of M. Such a comparison of average widths (and, therefore, “average gap”) for useful sets of contextual bandit policies F appears to be an interesting area of further investigation. We refer to [RS15], where some of these ideas have been developed in the context of cut-based constraints for node prediction on graphs."
    }, {
      "heading" : "6.5 Adversarial contexts",
      "text" : "Suppose we place no assumption on the evolution of xt’s, which may now be treated as worst-case. This problem subsumes the full information online classification setting, and, hence, one cannot hope to have nontrivial regret against policy classes F with infinite Littlestone dimension. More generally, the best one can hope for is to say that the adversarial contextual bandit problem can be solved whenever the corresponding full information problem may be solved. We now present essentially this result: if there is a full-information relaxation, then one may use it to solve the adversarial contextual bandit problem. Moreover, based on the work of [RSS12, FRS15], all the known online learning methods appear to be relaxation based. Hence, we essentially prove below that\nIf a problem is online learnable in the full-information adversarial setting, then it is learnable in the adversarial contextual bandit setting. Furthermore, if the former is computationally tractable, then so is the latter.\nTo be precise, the full information version of contextual problem is as follows. On round t, we observe xt ∈ X , predict ŷt ∈ [d], and observe ct ∈ [0,1]d. The regret is defined as before, with our cumulative cost being ∑ ct(ŷt).\nA full information relaxation Rel† (c1, . . . , ct) is admissible if sup xt inf qt max ct E ŷt∼qt {ct(ŷt) +Rel† (c1∶t)} ≤Rel† (c1∶t−1)\nand\nRel† (c1∶n) ≥ − inf f∈F n∑ t=1 f(xt)Tct . Similarly, a partial information relaxation is admissible in this adversarial case when c1∶t are replaced with I1∶t in the above admissibility definition, as in Section 4. Lemma 6. If Rel† () is an admissible full-information relaxation for the adversarial scenario, then Rel (I1∶t) ≜ γ−1Rel† (γc̃1, . . . , γc̃t) + (n − t)dγ is admissible for the partial information scenario. Prediction qt is obtained as qt = (1 − dγ)q∗t + γ1 where q∗t is computed by solving for a full-information strategy with the scaled unbiased estimates of costs. The resulting regret upper bound is\n2 √ d ⋅ n ⋅Rel† (∅).\nProof of Lemma 6. Let us first check the initial condition. We have that\nE ŷ1∶n∼q1∶n Rel (I1∶n) = E ŷ1∶n∼q1∶n γ−1Rel† (γc̃1, . . . , γc̃n) ≥ E\nŷ1∶n∼q1∶n − inf f∈F n∑ t=1 f(xt)Tc̃t ≥ − inf f∈F n∑ t=1 f(xt)Tct where the first inequality is due to admissibility of the full-information relaxation, and the second is due to Jensen’s inequality and unbiasedness of c̃t. For the recursive part, we follow the proof of Theorem 2 and note that all the statements, until the end, are done conditionally on xt. Define the strategy q∗t as\nq∗t = argmin q∈∆d sup c̃∈γ−1[0,1]d {qT(γc̃t) +Rel† (γc̃1, . . . , γc̃t)} and let qt = (1 − dγ)q∗t + γ1. Given xt, (22) tells us\nmax ct∈[0,1]d Eŷt∼qt{ct(ŷt) +Rel (I1, . . . , It) } ≤ sup c̃t∈γ−1[0,1]d {(q∗t )Tc̃t +Rel (I1, . . . , It)} + dγ which is equal to\nγ−1 sup c̃t {(q∗t )T(γc̃t) +Rel† (γc̃1, . . . , γc̃t)} + (n − t + 1)dγ ≤ γ−1Rel† (γc̃1, . . . , γc̃t−1) + (n − t + 1)dγ\nby admissibility of the full-information relaxation. Observe that the use of the full-information relaxation on γc̃t’s is warranted since these vectors are in [0,1]d. This concludes the proof.\nWe remark that the time complexity of the adversarial contextual bandit solution in Lemma 6 is the same as the time complexity of the corresponding full information procedure."
    }, {
      "heading" : "7 Open Problems and Future Directions",
      "text" : "The main open problem is whether the regret upper bound for BISTRO or a related method can be improved. In the inequality (22) we decouple the distribution q′t from qt, and this appears to be the source of the loseness, at least in the analysis. A more precise analysis at this step might resolve the issue. It is unclear what kind of structure of F may be used to improve computation and/or regret guarantees of BISTRO.\nUnder structural assumptions on F one may come up with sufficient statistics for the information I1∶t and, therefore, avoid keeping around all the estimates c̃t. Of course, this is the case in noncontextual bandits, where the sum ∑ c̃t is sufficient (at least as evidenced by existing near-optimal bandit methods).\nAn interesting avenue of investigation is to study the more general case when x’s are drawn from a stochastic process with a parametrized form. One may then attempt to estimate the parameters of the process on-the-go and use the estimate to hallucinate future data for random playout."
    }, {
      "heading" : "8 Proofs",
      "text" : "Proof of Lemma 1. In the proof, we use the shorthand ⟪. . .⟫nt=1 do denote repeated application of the operators within the brackets from t = 1 to n. As an example, the sequence of operators\nE x1 max c1 E x2 max c2 [G(x1, c1, x2, c2)] acting on the function G is abbreviated as ⟪Ext maxct⟫2t=1 [G(x1, c1, x2, c2)].\nLet q1, . . . , qn be an admissible strategy. The expected regret of this strategy can be upper bounded by\nE[Reg] ≤ sup c1∶n E[Reg] ≤ ⟪E xt sup ct ⟫n t=1 [ n∑ t=1 qTt ct − inf f∈F n∑ t=1 f(xt)Tct] by Jensen’s inequality (pulling Ext out of multiple suprema until its t-th position). The last expression is further upper bounded by\n⟪E xt sup ct ⟫n t=1 [ n∑ t=1 qTt ct + E ŷ1∶n∼q1∶n Rel (I1∶n)] by admissibility of the partial information relaxation. By linearity of expectation for Eŷt and Jensen’s inequality (to pull it out through multiple suprema as before), we obtain an upper bound of\n⟪E xt sup ct E ŷt∼qt ⟫n t=1 [ n∑ t=1 ct(ŷt) +Rel (I1∶n)] . We now start from step n and observe that ∑n−1t=1 ct(ŷt) does not depend on xn, cn, ŷn, and thus we rewrite the preceding expression as\n⟪E xt sup ct E ŷt∼qt ⟫n−1 t=1 [n−1∑ t=1 ct(ŷt) + E xt sup ct E ŷt∼qt {cn(ŷn) +Rel (I1∶n)}] .\nBy admissibility of qt and (5), we pass to the upper bound of\n⟪E xt sup ct E ŷt∼qt ⟫n−1 t=1 [n−1∑ t=1 ct(ŷt) +Rel (I1∶n−1)] . Continuing in this fashion leads to a bound of Rel (∅)."
    }, {
      "heading" : "A Proof of Theorem 2",
      "text" : "Admissibility: initial condition For any c1∶n, q1∶n, x1∶n, it holds that\n− inf f∈F n∑ t=1 f(xt)Tct = sup M∈M[x1∶n] − n∑ t=1 MTt Y (n) t ≤ Eŷ1∶n∼q1∶n sup M∈M[x1∶n] − n∑ s=1 MTs Ỹ (n) s = Eŷ1∶n∼q1∶nRel (I1∶n) .\n(18)\nIn the remainder of the proof we will often write M instead of M[x1∶n] for brevity. Admissibility: recursion Let D ≜ {γ−1ej ∶ j ∈ [d]} ∪ {0}, the set of scaled standard basis vectors, together with the origin. Observe that c̃t ∈ conv(D) by our definition of unbiased estimates (in fact, it is only a scaling of one coordinate).\nWe now reason conditionally on xt. As before, let ǫs ∈ {±1}d denote a vector of independent Rademacher random variables. Let us abbreviate by ρ = (ǫt+1∶n, xt+1∶n), a draw of independent Rademacher variables and covariates from Px for the “future rounds”, as part of the random playout procedure. Together with the estimates c̃s for s < t, we may now construct Ỹ\n(t) and M matrices and define the randomized prediction algorithm as\nq∗t (ρ) = argmin q∈∆d sup c̃∈D ⎧⎪⎪⎨⎪⎪⎩q Tc̃ + sup M∈M[x1∶n] −∑ s≠t MTs Ỹ (t) s −M T t c̃ ⎫⎪⎪⎬⎪⎪⎭ (19) = argmin\nq∈∆d sup ŷt,q ′ t max ct ⎧⎪⎪⎨⎪⎪⎩q Tc̃t(ct, q′t, ŷt) + sup M∈M[x1∶n] −∑ s≠t MTs Ỹ (t) s −M T t c̃t(ct, q′t, ŷt)⎫⎪⎪⎬⎪⎪⎭ (20) We remark that xt enters the above definition of q ∗ t (ρ), but we leave this dependence implicit until the end of the proof. For the purposes of the proof also define\nqt(ρ) = (1 − dγ) ⋅ q∗t (ρ) + γ1, (21)\na version of q∗t (ρ) that is shifted away from the boundary of the simplex (a step that allows for estimation of ct). Also define qt = Eρ[qt(ρ)] and q∗ = Eρ[q∗t (ρ)]. Observe that Eŷt∼qt[ct(ŷt)] = qTt ct ≤ (q∗t )Tct + γ1Tct ≤ Eŷt∼qt[(q∗t )Tc̃t(ct, qt, ŷt)] + dγ Hence,\nmax ct∈[0,1]d Eŷt∼qt{ct(ŷt) +Rel (I1, . . . , It) } ≤ max\nct∈[0,1]d Eŷt∼qt{(q∗t )Tc̃t(ct, qt, ŷt) +Rel (I1∶t−1, It(xt, qt, ŷt, ct)) } + dγ\n≤ sup ŷt∈[d],q′t max ct∈[0,1]d {(q∗t )Tc̃t(ct, q′t, ŷt) +Rel (I1∶t−1, It(xt, q′t, ŷt, ct))} + dγ. (22) In the last expression, the supremum is over q′t of the form (1− dγ) ⋅ q + γ1, q ∈∆d. This last upper bound holds because qt is one of such distributions. The importance of this upper bound is that it decouples the q∗t from q ′ t in the first term, a step that yields a simple optimization problem that\ndefines q∗t (ρ). Writing out the form of the relaxation, the last expression is equal to sup ŷt,q′t max ct {(q∗t )Tc̃t(ct, q′t, ŷt) + Eρ sup M∈M −∑ s≠t MTs Ỹ (t) s −M T\nt c̃t(ct, q′t, ŷt)} + (n − t + 1)dγ ≤ sup\nc̃t∈conv(D)\n{(q∗t )Tc̃t + Eρ sup M∈M −∑ s≠t MTs Ỹ (t) s −M T t c̃t} + (n − t + 1)dγ since c̃t(ct, q′t, ŷt) ∈ conv(D). The expression inside the supremum is a convex function of c̃t, and thus the supremum is achieved at a vertex, an element of D. Since q∗t = Eρ[q∗t (ρ)], we upper bound the last expression via Jensen’s inequality (omitting (n − t + 1)dγ to simplify the exposition) by\nEρ sup c̃t∈D {q∗t (ρ)Tc̃t + sup M∈M −∑ s≠t MTs Ỹ (t) s −M T t c̃t} (23) Since q∗t (ρ) is precisely defined to be the minimizer (given ρ) of the supremum in (23), the preceding expression is equal to\nEρ inf q∈∆d sup c̃t∈D {qTc̃t + sup M∈M −∑ s≠t MTs Ỹ (t) s −M T t c̃t} The rest of the upper bounds will be derived conditionally on ρ. Observe that\ninf q∈∆d sup c̃t∈D {qTc̃t + sup M∈M −∑ s≠t MTs Ỹ (t) s −M T t c̃t} = sup pt inf q Ec̃t∼pt {qTc̃t + sup M∈M −∑ s≠t MTs Ỹ (t) s −M T t c̃t} by the minimax theorem, where pt ranges over the set of distributions on D. By linearity of expectation, the preceding expression is equal to\nsup pt inf q {qTEc̃t∼pt[c̃t] + Ec̃t∼pt sup M∈M −∑ s≠t MTs Ỹ (t) s −M T t c̃t} = sup\npt ⎧⎪⎪⎨⎪⎪⎩minj∈[d]e T jEc̃t∼pt[c̃t] + Ec̃t∼pt sup M∈M −∑ s≠t MTs Ỹ (t) s −M T t c̃t ⎫⎪⎪⎬⎪⎪⎭ . (24)\nObserve that for any M ∈M, ∑dj=1Mj,t = 1 and the elements of Mt are nonnegative. Thus min j e T\njEc̃t∼pt[c̃t] ≤MTt Ec̃t∼pt[c̃t] Therefore, (24) is equal to\nsup pt ⎧⎪⎪⎨⎪⎪⎩Ec̃t∼pt supM∈M−∑s≠tM T s Ỹ (t) s +min j∈[d] e T jEc̃t∼pt[c̃t] −MTt c̃t⎫⎪⎪⎬⎪⎪⎭ ≤ sup\npt {Ec̃t∼pt sup M∈M −∑ s≠t MTs Ỹ (t) s +M T t Ec̃t∼pt[c̃t] −MTt c̃t} = sup\npt {Ec̃t,c̃′t∼pt sup M∈M −∑ s≠t MTs Ỹ (t) s +M T t (c̃′t − c̃t)} Since exchanging c̃t and c̃ ′ t switches the sign in the last term, we may introduce an independent Rademacher random variable δt via the standard technique of symmetrization. The last expression is then equal to\nsup pt {Ec̃t,c̃′t∼ptEδt sup M∈M −∑ s≠t MTs Ỹ (t) s + δtM T t (c̃′t − c̃t)} ≤ sup\npt {Ec̃t∼ptEδt sup M∈M −∑ s≠t MTs Ỹ (t) s + 2δtM T t c̃t} The above inequality follows by splitting the supremum into two parts equal parts. Let us now reason conditionally on c̃t. There are two cases: either c̃t = 0 or c̃t = γ −1 ej for some coordinate\nj ∈ [d]. Let us consider the second case, and the first follows from the same reasoning. Take Z to be a random vector with independent coordinates and values in {−γ−1, γ−1}d. For the jth coordinate, Zj is identically γ\n−1, while for all other coordinates i ≠ j the distribution Zi is symmetric. Clearly, EZ = c̃t. By Jensen’s inequality,\nEδt sup M∈M {−∑ s≠t MTs Ỹ (t) s + 2δtM T t c̃t} ≤ EδtEZ sup M∈M {−∑ s≠t MTs Ỹ (t) s + 2δtM T t Z} It is not hard to see that the distribution of δtZ is uniform on {−γ−1, γ−1}d, and we can write it as γ−1ǫt, a scaled vector of independent Rademacher random variables. The overall bound (together with the omitted term (n − t + 1)dγ) is then max\nct∈[0,1]d Eŷt∼qt{ct(ŷt) +Rel (I1, . . . , It) } ≤ Eρ sup pt {Ec̃t∼ptEǫt sup M∈M −∑ s≠t MTs Ỹ (t) s + 2γ −1MTt ǫt} + (n − t + 1)dγ = EρEǫt sup\nM∈M {−∑\ns≠t\nMTs Ỹ (t) s + 2γ −1MTt ǫt} + (n − t + 1)dγ since the expression no longer depends on pt and c̃t. The above inequality holds for any xt. Hence, we may take expectation on both sides, yielding\nExt max ct∈[0,1]d Eŷt∼qt{ct(ŷt) +Rel (I1, . . . , It) } ≤ Eǫt∶n,xt∶n sup M∈M[x1∶n] {−∑ s≠t MTs Ỹ (t) s + 2γ −1MTt ǫt} + (n − t + 1)dγ =Rel (I1∶t−1) because ρ = (ǫt+1∶n, xt+1∶n). This proves admissibility.\nOmitting 0 from objective Examining the algorithm in (19), we note that the optimization problem may be taken over c̃ ∈ {e1, . . . ,ed}; that is, the argmin over q does not change upon the removal of 0. To see this, suppose that q∗t (ρ) is the optimal response when c̃ ∈ {e1, . . . ,ed}. Then it is also an optimal response to c̃ ∈ {e1, . . . ,ed} ∪ {0} since for c̃ = 0 the value of q does not make any difference in terms of the value. This proves our claim, and is reflected in the definition of Algorithm 1.\nRegret bound The final bound is given by\nRel (∅) = ExEǫ sup M∈M[x1∶n] − n∑ t=1 MTt Ỹ (0) t + ndγ = 2 γ ER(F ;x1∶n) + ndγ = 2√2dnER(F ;x1∶n)"
    } ],
    "references" : [ {
      "title" : "The nonstochastic multiarmed bandit problem",
      "author" : [ "P. Auer", "N. Cesa-Bianchi", "Y. Freund", "R.E. Schapire" ],
      "venue" : "SIAM Journal on Computing,",
      "citeRegEx" : "Auer et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Auer et al\\.",
      "year" : 2002
    }, {
      "title" : "Taming the monster: A fast and simple algorithm for contextual bandits",
      "author" : [ "A. Agarwal", "D. Hsu", "S. Kale", "J. Langford", "L. Li", "R.E. Schapire" ],
      "venue" : "arXiv preprint arXiv:1402.0555,",
      "citeRegEx" : "Agarwal et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Agarwal et al\\.",
      "year" : 2014
    }, {
      "title" : "Contextual bandit algorithms with supervised learning guarantees",
      "author" : [ "A. Beygelzimer", "J. Langford", "L. Li", "L. Reyzin", "R.E. Schapire" ],
      "venue" : "In Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics",
      "citeRegEx" : "Beygelzimer et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Beygelzimer et al\\.",
      "year" : 2011
    }, {
      "title" : "A linear programming formulation and approximation algorithms for the metric labeling problem",
      "author" : [ "C. Chekuri", "S. Khanna", "J. Naor", "L. Zosin" ],
      "venue" : "SIAM Journal on Discrete Mathematics,",
      "citeRegEx" : "Chekuri et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Chekuri et al\\.",
      "year" : 2004
    }, {
      "title" : "Efficient optimal learning for contextual bandits",
      "author" : [ "M. Dudik", "D. Hsu", "S. Kale", "N. Karampatziakis", "J. Langford", "L. Reyzin", "T. Zhang" ],
      "venue" : "arXiv preprint arXiv:1106.2369,",
      "citeRegEx" : "Dudik et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Dudik et al\\.",
      "year" : 2011
    }, {
      "title" : "Adaptive online learning",
      "author" : [ "D. Foster", "A. Rakhlin", "K. Sridharan" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Foster et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Foster et al\\.",
      "year" : 2015
    }, {
      "title" : "Newtron: an efficient bandit algorithm for online multiclass prediction",
      "author" : [ "E. Hazan", "S. Kale" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Hazan and Kale.,? \\Q2011\\E",
      "shortCiteRegEx" : "Hazan and Kale.",
      "year" : 2011
    }, {
      "title" : "Efficient bandit algorithms for online multiclass prediction",
      "author" : [ "S.M. Kakade", "S. Shalev-Shwartz", "A. Tewari" ],
      "venue" : "In Proceedings of the 25th international conference on Machine learning,",
      "citeRegEx" : "Kakade et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Kakade et al\\.",
      "year" : 2008
    }, {
      "title" : "Approximation algorithms for classification problems with pairwise relationships: Metric labeling and markov random fields",
      "author" : [ "J. Kleinberg", "E. Tardos" ],
      "venue" : "Journal of the ACM (JACM),",
      "citeRegEx" : "Kleinberg and Tardos.,? \\Q2002\\E",
      "shortCiteRegEx" : "Kleinberg and Tardos.",
      "year" : 2002
    }, {
      "title" : "Global optimization with polynomials and the problem of moments",
      "author" : [ "J. B Lasserre" ],
      "venue" : "SIAM Journal on Optimization,",
      "citeRegEx" : "Lasserre.,? \\Q2001\\E",
      "shortCiteRegEx" : "Lasserre.",
      "year" : 2001
    }, {
      "title" : "Hybrid stochastic-adversarial on-line learning",
      "author" : [ "A. Lazaric", "R. Munos" ],
      "venue" : "In Conference on Learning Theory,",
      "citeRegEx" : "Lazaric and Munos.,? \\Q2009\\E",
      "shortCiteRegEx" : "Lazaric and Munos.",
      "year" : 2009
    }, {
      "title" : "Asymptotically efficient adaptive allocation rules",
      "author" : [ "T.L. Lai", "H. Robbins" ],
      "venue" : "Advances in Applied Mathematics,",
      "citeRegEx" : "Lai and Robbins.,? \\Q1985\\E",
      "shortCiteRegEx" : "Lai and Robbins.",
      "year" : 1985
    }, {
      "title" : "The epoch-greedy algorithm for multi-armed bandits with side information",
      "author" : [ "J. Langford", "T. Zhang" ],
      "venue" : "In Advances in neural information processing systems,",
      "citeRegEx" : "Langford and Zhang.,? \\Q2008\\E",
      "shortCiteRegEx" : "Langford and Zhang.",
      "year" : 2008
    }, {
      "title" : "Tighter bounds for multi-armed bandits with expert advice",
      "author" : [ "H. B McMahan", "M. J Streeter" ],
      "venue" : "In COLT,",
      "citeRegEx" : "McMahan and Streeter.,? \\Q2009\\E",
      "shortCiteRegEx" : "McMahan and Streeter.",
      "year" : 2009
    }, {
      "title" : "Semidefinite programming relaxations for semialgebraic problems",
      "author" : [ "P.A. Parrilo" ],
      "venue" : "Mathematical programming,",
      "citeRegEx" : "Parrilo.,? \\Q2003\\E",
      "shortCiteRegEx" : "Parrilo.",
      "year" : 2003
    }, {
      "title" : "Hierarchies of relaxations for online prediction problems with evolving constraints",
      "author" : [ "A. Rakhlin", "K. Sridharan" ],
      "venue" : "In COLT,",
      "citeRegEx" : "Rakhlin and Sridharan.,? \\Q2015\\E",
      "shortCiteRegEx" : "Rakhlin and Sridharan.",
      "year" : 2015
    }, {
      "title" : "Relax and randomize: From value to algorithms",
      "author" : [ "A. Rakhlin", "O. Shamir", "K. Sridharan" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Rakhlin et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Rakhlin et al\\.",
      "year" : 2012
    } ],
    "referenceMentions" : [ ],
    "year" : 2016,
    "abstractText" : "We present efficient algorithms for the problem of contextual bandits with i.i.d. covariates, an arbitrary sequence of rewards, and an arbitrary class of policies. Our algorithm BISTRO requires d calls to the empirical risk minimization (ERM) oracle per round, where d is the number of actions. The method uses unlabeled data to make the problem computationally simple. When the ERM problem itself is computationally hard, we extend the approach by employing multiplicative approximation algorithms for the ERM. The integrality gap of the relaxation only enters in the regret bound rather than the benchmark. Finally, we show that the adversarial version of the contextual bandit problem is learnable (and efficient) whenever the full-information supervised online learning problem has a non-trivial regret guarantee (and efficient).",
    "creator" : "LaTeX with hyperref package"
  }
}