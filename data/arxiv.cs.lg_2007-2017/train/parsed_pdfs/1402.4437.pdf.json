{
  "name" : "1402.4437.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Learning the Irreducible Representations of Commutative Lie Groups",
    "authors" : [ ],
    "emails" : [ "T.S.COHEN@UVA.NL", "M.WELLING@UVA.NL" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n40 2.\n44 37\nv1 [\ncs .L\nG ]\n1 8\nFe b\n20 14"
    }, {
      "heading" : "1. Introduction",
      "text" : "Recently, the field of deep learning has produced some remarkable breakthroughs. The hallmark of the deep learning approach is to learn multiple layers of representation of data, and much work has gone into the development of representation learning modules such as RBMs and their generalizations (Welling et al., 2005), and autoencoders (Vincent et al., 2008). However, at this point it is not quite clear what makes a good representation. In this paper, we take a fresh look at the basic principles behind unsupervised representation learning from the perspective of Lie group theory1.\n1We will at times assume a passing familiarity with Lie groups, but the main ideas of this paper should be accessible to a broad audience.\nProceedings of the 31 st International Conference on Machine Learning, Beijing, China, 2014. JMLR: W&CP volume 32. Copyright 2014 by the author(s).\nVarious desiderata for learned representations have been expressed, including meaningful (Bengio & Lecun, 2014), invariant (Goodfellow et al., 2009), abstract and disentangled (Bengio et al., 2013) representations, but so far most of these notions have not been defined in a mathematically precise way. We focus on the notions of invariance and disentangling, leaving the search for meaning for future work.\nWhat do we mean, intuitively, when we speak of invariance and disentangling? A disentangled representation is one that explicitly represents the distinct factors of variation in the data. For example, visual data can be thought of as a composition of object identity, position and pose, lighting conditions, etc. Once disentangling is achieved, invariance follows easily: to build a representation that is invariant to a transformation (e.g. a change in object position) that is considered a nuisance for a particular task (e.g. object classification), one can simply ignore the units in the representation that encode position.\nTo get a mathematical handle on the heretofore vague concept of disentangling, we borrow a fundamental principle from physics, which we refer to as Weyl’s principle, following Kanatani (1990). In physics, this idea is used to tease appart (i.e. disentangle) the elementary particles of a physical system from mere measurement values that have no inherent physical significance. We apply this principle to the area of vision, for after all, pixels are nothing but physical measurements.\nWeyl’s principle presupposes a symmetry group that acts on the data. By this we mean a set of transformations that does not change the “essence” of the measured phenomenon, although it may change the “superficial appearance”, i.e. the measurement values. As a concrete example that we will use throughout this paper, consider the group known as SO(2), acting on images by 2D rotation about the origin. A transformation from this group (a rotation) may change the value of every pixel in the image, but leaves invariant the identity of the imaged object. Weyl’s principle states that the elementary components of this system (the world as measured by a camera) are given by the irre-\nducible representations of the symmetry group – a concept that will be explained in this paper.\nWe demonstrate this theoretical principle using the socalled toroidal subgroups of the special orthogonal group as symmetry groups. This class includes several groups of practical interest, such as image rotations and translations. We introduce a probabilistic model, which we call Toroidal Subgroup Analysis (TSA), and show how it can be learned from pairs of images related by arbitrary and unobserved transformations in the group. Using a novel conjugate prior, the model integrates probability theory and Lie group theory in a very elegant way. All the relevant probabilistic quantities such as normalization constants, moments, KL-divergences, the posterior density over the transformation group, the marginal density in data space, and their gradients can be obtained in closed form. This complete tractability is a rather exceptional property among probabilistic graphical models with non-linear interactions.\nThe parameters of the posterior distribution over the transformation group provide a disentangled representation of the data that can be split into invariant and equivariant components. The structure of the learned group dictates a particular pooling scheme for computing invariants, reproducing a key feature of convolutional networks (LeCun & Bottou, 1998). When trained on shifts, the model learns to perform a Discrete Fourier Transform (DFT), thus providing a probablistic interpretation of Fourier analysis and opening up the possibility of learning appropriate probabilistic generalizations of the DFT for a particular signal."
    }, {
      "heading" : "1.1. Related work",
      "text" : "The first to propose an algorithm for learning Lie groups from data were Rao & Ruderman (1999). This work was later extended by Miao and Rao (2007). The first model deals only with one-parameter Lie groups, while the later work incorporates multiple transformation types. These works left open the problem of efficiently inferring transformation parameters for non-infinitesimal transformations. This problem was solved by Sohl-Dickstein et al. (2010) using an elegant adaptive smoothing technique, making it possible to learn from large transformations.\nOther, non group theoretical approaches to learning transformations and invariant representations exist that do something similar to the irreducible reduction of a toroidal group, but this has not been recognized as a general principle for disentangling that is applicable to other groups, too. Gating models (Memisevic & Hinton, 2010) were found to perform a kind of eigenspace analysis (Memisevic, 2012), which is similar in spirit to the TSA model. However, gating models require far more parameters because they must tile each eigenspace with many pairs of filters whereas TSA\nrequires only two per eigenspace. Motivated by a number of statistical phenomena observed in natural images, Cadieu & Olshausen (2012) describe a model that decomposes a signal into invariant amplitudes and model transformations using phase variables.\nNone of the mentioned methods take into account the full uncertainty over transformation parameters, as does TSA. Because a unique transformation relating two images cannot in general be inferred, a multimodal posterior is required to give a complete description of the geometric situation. Furthermore, posterior inference in our model is very fast, whereas the MAP inference algorithm by SohlDicksteint et al. requires a more expensive iterative optimization."
    }, {
      "heading" : "2. Preliminaries",
      "text" : ""
    }, {
      "heading" : "2.1. Equivalence, Invariance and Reducibility",
      "text" : "In this section, we discuss three fundamental concepts on which the analysis in the rest of this paper is based: equivalence, invariance and reducibility.\nConsider a function Φ : RD → X that assigns to each possible data point x ∈ RD a class-label (X = {1, . . . , L} being a discrete set of labels in this case) or some distributed representation (e.g. X = RL). Such a function induces an equivalence relation on the input space RD: we say that two vectors x,y ∈ RD are Φ-equivalent if they are mapped onto the same representation by Φ. Symbolically, x ≡Φ y ⇔ Φ(x) = Φ(y). Every equivalence relation on the input space fully determines a symmetry group acting on the space. This group, call it G, contains all transformations ρ : RD → RD that leave Φ invariant: G = {ρ |Φ(ρ(x)) = Φ(x)}. G describes the symmetries of Φ, or, stated differently, the label function/representationΦ is invariant to transformations in G acting on the input space. Hence, we can speak of Geqiuvalence: x ≡G y ⇔ ∃ρ ∈ G : ρ(x) = y. For example, if G is the group of 2D image rotations, two images are G-equivalent if they are rotations of each other.\nBefore we can introduce Weyl’s principle, we need one more concept: the reduction of a group representation. Let us assume that ρ acts linearly on our measurement space R\nD so that we can write ρ(x) = Qx for a matrix Q ∈ G. In general, every coordinate yi of y = Qx can depend on every coordinate xj of x. Since x ≡G y, it makes no sense to consider the coordinates xi as separate quantities; we can only consider the vector x as a unit because a symmetry transformation Q tangles all coordinates.\nHowever, we are free to change the basis of the measurement space. It may be possible to use a change of basis to expose an invariant subspace: a subspace V ⊂ RD that\nis mapped onto itself by every transformation in the group: ∀Q ∈ G : x ∈ V ⇒ Qx ∈ V . If such a subspace exists and its orthogonal complement V ⊥ ⊂ RD is also an invariant subspace, then it makes sense to consider the two parts of x that lie in V and V ⊥ be distinct quantities because they remain distinct under symmetry transformations.\nLet W be a change of basis matrix that exposes the invariant subspaces, that is,\nQ = W\n[\nQ1 Q2\n]\nW−1, (1)\nfor all Q ∈ G. The matricesQ1 andQ2 are functions of Q, and each forms a representation of the same abstract group as represented byQ. The group representationsQ1 and Q2 describe how the individual parts x1 ∈ V and x2 ∈ V ⊥ are transformed by the elements of the group. Following common practice, we refer to both the group representations Q1,Q2 and the subspaces V and V ⊥ corresponding to these group representations as “representations”.\nThe process of reduction can be applied recursively to Q1 and Q2. If at some point there is no more (non-trivial) invariant subspace, the representation is called irreducible. Weyl’s principle states that the elementary components of a system are the irreducible representations of the symmetry group of the system. Properly understood, it is not a physics principle at all, but generally applicable to any situation where there is a well-defined notion of equivalence. It is completely abstract and therefore agnostic about the type of data (images, optical flows, sound, etc.), making it eminently useful for representation learning.\nIn the rest of this paper, we will demonstrate Weyl’s principle in the simple case of a compact commutative subgroup of the special orthogonal group (these terms are explained below). We want to stress though, that there is no reason the basic ideas cannot be applied to non-commutative groups acting on non-linear latent representation spaces."
    }, {
      "heading" : "2.2. Maximal Tori in the Orthogonal Group",
      "text" : "In order to facilitate analysis, we will from here on consider only compact commutative subgroups of the special orthogonal group SO(D). For reasons that will become clear shortly, such groups are called toroidal subgroups of SO(D). Intuitively, the toroidal subgroups of general compact Lie groups can be thought of as the “commutative part” of these groups. This fact, combined with their analytic tractability (evidenced by the results in this paper) makes them suitable as the starting point of a theory of probabilistic Lie-group learning.\nImposing the constraint of orthogonality will make the computation of matrix inverses very cheap, because for orthogonal Q, Q−1 = QT . Orthogonal matrices also avoid\nnumerical problems, because their condition number is always equal to 1. Another important property of orthogonal transformations is that they leave the Euclidean metric invariant: ‖Qx‖ = ‖x‖. Therefore, orthogonal matrices cannot express transformations such as contrast scaling, but they can still model the interesting structural changes in images (Bethge et al., 2007). For example, since 2D image rotation and (cyclic) translation are linear and do not change the total energy (norm) of the image, they can be represented by orthogonal matrices acting on vectorized images.\nAs is well known, commuting matrices can be simultaneously diagonalized, so one could represent a toroidal group in terms of a basis of eigenvectors shared by every element in the group, and one diagonal matrix of eigenvalues for each element of the group, as was done in (Sohl-Dickstein et al., 2010) for 1-parameter Lie groups. However, orthogonal matrices do not generally have a complete set of real eigenvectors. One could use a complex basis instead, but this introduces redundancies because the eigenvalues and eigenvectors of an orthogonal matrix come in complex conjugate pairs. For machine learning applications, this is clearly an undesirable feature, so we opt for a joint block-diagonalization of the elements Q of the toroidal group: Q(ϕ) = WR(ϕ)WT , where W is orthogonal and R(ϕ) is a block-diagonal rotation matrix2:\nR(ϕ) =\n\n  R(ϕ1) . . .\nR(ϕJ)\n\n  . (2)\nThe diagonal of R(ϕ) contains 2× 2 rotation matrices\nR(ϕj) =\n[\ncos(ϕj) − sin(ϕj) sin(ϕj) cos(ϕj)\n]\n(3)\nIn this parameterization, the real, orthogonal basis W identifies the group representation, while the vector of rotation angles ϕ identifies a particular element Q(ϕ) of the group. It is now clear why such groups are called “toroidal”: the parameter space ϕ is periodic in each element ϕj and hence is a topological torus. For a J-parameter toroidal group, all the ϕj can be chosen freely. Such a group is known as a maximal torus in SO(D), for which we write T J = {ϕ |ϕj ∈ [0, 2π], j = 1, . . . J}.\nTo gain insight into the structure of toroidal groups with fewer parameters, we rewrite eq. 2 using the matrix exponential:\nR(ϕ) = exp\n\n\nJ ∑\nj=1\nϕjAj\n\n (4)\nThe the anti-symmetric matrices Aj = ddϕj R(ϕ) ∣ ∣ 0 are\n2For ease of exposition, we asume an even dimensional space D = 2J , but the equations are easily generalized.\nknown as the Lie algebra generators, and the ϕj are Liealgebra coordinates.\nThe Lie algebra is a structure that largely determines the structure of the corresponding Lie group, while having the important advantage of forming a linear space. That is, all linear combinations of the generators belong to the Lie algebra, and each element of the Lie algebra corresponds to a transformation in the Lie group, which itself is a non-linear manifold. Furthermore, every subgroup of the Lie group corresponds to a subalgebra (not defined here) of the Lie algebra. All toroidal groups are the subgroup of some maximal torus, so we can learn a general toroidal group by first learning a maximal torus and then learning a subalgebra of its Lie algebra. Due to commutativity, the structure of the Lie algebra of toroidal groups is such that any subspace of the Lie algebra is in fact a subalgebra. The relevance of this observation to our machine learning problem is that to learn a toroidal group with I parameters (I < J), we can simply learn a maximal toroidal group and then learn an I-dimensional linear subspace in the space of ϕ.\nIn this work, we are interested in compact subgroups only3, which is to say that the parameter space should be closed and bounded. To see that not all subgroups of a maximal torus are compact, consider a 4D space and a maximal torus with 2 generators A1 and A2. Let us define a subalgebra with one generator A = ω1A1 + ω2A2, for real numbers ω1 and ω2. The group elements generated by this algebra through the exponential map take the form\nR(s) = exp (sA) =\n[\nR(ω1s) R(ω2s)\n]\n. (5)\nEach R(ωjs) is periodic with period 2π/ωj , but their combination R(s) need not be. When ω1 and ω2 are not commensurate, all values of s ∈ R will produce different R(s). To obtain a compact one-parameter group with parameter space s ∈ [0, 2π], we restrict the frequencies ωj to be integers, so that R(s) = R(s+2π) (see figure 1). To get a feel for what this means in the case of 2D image rotation, see figure 3: the radial frequency of each filter pair corresponds to the value ωj for that pair.\nIt is easy to see that each block of R(s) forms an irreducible representation of the entire group that acts on the data. From the point of view expounded in section 2.1, we should view the vector x as a tangle of elementary components uj = WTj x, where Wj = (W(:, 2j−1),W(:, 2j)) denotes the D × 2 submatrix of W corresponding to the j-th block in R(s). Each one of the elementary parts uj is functionally independent of the others under symmetry\n3The main reason for this restriction is that compact groups are simpler and better understood than non-compact groups. In practice, many non-compact groups can be compactified, so not much is lost.\ntransformations.\nThe variable ωj is known as the weight of the representation (Kanatani, 1990). When the representations are equivalent (i.e. they have the same weight), the parts are “of the same kind” and are transformed identically. Elementary components with different weights transform differently.\nIn the following section, we show how a maximal toroidal group and a 1-parameter subgroup can be learned from correspondence pairs, and how these can be used to generate invariant representations."
    }, {
      "heading" : "3. Toroidal Subgroup Analysis",
      "text" : "We will start by modelling a maximal torus. A data pair (x,y) is related by a transformation Q = WR(ϕ)WT from the group representation:\ny = WR(ϕ)WTx+ ǫ, (6)\nwhere ǫ ∼ N (0, σ2) represents isotropic Gaussian noise. In other symbols, p(y|x, ϕ) = N (y|WR(ϕ)WTx, σ2). We will find it useful to introduce some notation for indexing invariant subspaces. As before, Wj = (W(:, 2j−1),W(:, 2j)). Let uj = WTj x and vj = W T j y. If we want to access one of the coordinates of u or v, we write uj1 = W T (:, 2j−1)x or uj2 = W T (:, 2j)x.\nWe assume the ϕj to be marginally indepent and von-Mises distributed. The von-Mises distribution is an exponential family that assigns equal density to the endpoints of any length-2π interval of the real line, making it a suitable choice for periodic variables such as ϕj . We will find it useful to move back and forth between the conventional and natural parameterizations of this distribution. The conventional parameterization of the von-Mises distribution M(ϕ|µ, κ) uses a mean µ and precision κ:\np(ϕj) = 1\n2πI0(κj) exp (κj cos(ϕj − µj)). (7)\nThe function I0 that appears in the normalizing constant is known as the modified Bessel function of order 0.\nSince the von-Mises distribution is an exponential family, we can write it in terms of natural parameters ηj = (ηj1 , ηj2) T as follows:\np(ϕj) = 1\n2πI0(‖ηj‖) exp (ηTj T (ϕj)), (8)\nwhere T (ϕj) = (cos(ϕj), sin(ϕj))T are the sufficient statistics. The natural parameters can be computed from conventional parameters using,\nηj = κj [cos(µj), sin(µj)] T (9)\nand vice versa,\nκj = ‖ηj‖, µj = tan−1(ηj2/ηj1) (10)\nUsing the natural parameterization, it is easy to see that the prior is conjugate to the likelihood, so that the posterior p(ϕ|x,y) is again a product of von-Mises distributions. Such conjugacy relations are of great utility in Bayesian statistics, because they simplify sequential inference. To our knowledge, this conjugacy relation has not been described before. First observe that the likelihood term splits into a sum over the invariant subspaces indexed by j:\np(ϕ|x,y) ∝ p(y|x, ϕ)p(ϕ)\n∝ exp ( − 1 2σ2 ‖y −WR(ϕ)WTx‖2 ) p(ϕ)\n∝ exp\n\n\nJ ∑\nj=1\nvTj R(ϕj)uj\nσ2 + ηTj T (ϕj)\n\n\nBoth the bilinear forms vTj R(ϕj)uj and the the prior terms ηTj T (ϕ) are linear functions of cos(ϕj) and sin(ϕj), so that they can be combined into a single dot product:\np(ϕ|x,y) ∝ exp\n\n\nJ ∑\nj=1\nη̂Tj T (ϕj)\n\n, (11)\nwhich we recognize as a product of von-Mises in natural form.\nThe parameters η̂j of the posterior are given by:\nη̂j = ηj + 1\nσ2 [uj1vj1 + uj2vj2 , uj1vj2 − uj2vj1 ]T\n= ηj + ‖uj‖‖vj‖\nσ2 [cos(θj), sin(θj)]\nT ,\n(12)\nwhere θj is the angle between uj and vj . Geometrically, we can interpret the Bayesian updating procedure in eq. 12 as follows. The orientation of the natural parameter vector\nηj determines the mean of the von-Mises, while its magnitude determines the precision. To update this parameter with new information obtained from data uj , vj , one should add the vector (cos(θj), sin(θj))T to the prior, using a scaling factor that grows with the magnitude of uj and vj and declines with the square of the noise level σ. The longer uj and vj and the smaller the noise level, the greater the precision of the observation. This geometrically sensible result follows directly from the consistent application of the rules of probability.\nObserve that when using a uniform prior (i.e. ηj = 0), the posterior mean µ̂j (computed from η̂j by eq. 10) will be exactly equal to the angle θj between uj and vj . We will use this fact in section 3.1 when we derive the formula for the orbit distance in a toroidal group.\nPrevious approaches to Lie group learning only provide point estimates of the transformation parameters, which have to be obtained using an expensive iterative optimization procedure (Sohl-Dickstein et al., 2010). In contrast, TSA provides a full tractable posterior distribution which is obtained using a simple feed-forward computation. Compared to the work of Cadieu & Olshausen (2012), our model deals well with low-energy subspaces, by simply describing the uncertainty in the estimate instead of providing inaccurate estimates that have to be discarded."
    }, {
      "heading" : "3.1. Invariant Representation and Metric",
      "text" : "One way of doing invariant classification is by using an invariant metric known as the manifold distance. This metric d(x,y) is defined as the minimum distance between the orbits Ox = {Qx |Q ∈ G} and Oy = {Qy |Q ∈ G}. Observe that this is only a true metric that satisfies the coincidence axiom d(x,y) = 0 ⇔ x = y if we take the condition x = y to mean “equivalence up to symmetry transformations” or x ≡G y, as discussed in section 2.1. In practice, it has proven difficult to compute this distance exactly, so approximations such as tangent distance have been invented (Simard et al., 2000). But for a maximal torus, we can easily compute the exact manifold distance:\nd2(x,y) = min ϕ ‖y−WR(ϕ)WTx‖2\n= ∑\nj\nmin ϕj\n‖vj −R(ϕj)uj‖2\n= ∑\nj\n‖vj −R(µ̂j)uj‖2,\n(13)\nwhere µ̂j is the mean of the posterior p(ϕj |x,y), obtained using a uniform prior (κj = 0). The last step of eq. 13 follows, because as we saw in the previous section, µ̂j is simply the angle between uj and vj when using a uniform prior. Therefore, R(µ̂j) aligns uj and vj , thereby minimizing the distance.\nAnother approach to invariant classification is through an invariant representation. Although the model presented above aims to describe the transformation between observations x and y, an invariant-equivariant representation appears automatically in terms of the parameters of the posterior over the group. To see this, consider all the transformations in the learned toroidal group G that take an image x to itself. This set is known as the stabilizer stabG(x) of x. It is a subgroup of G and describes the symmetries of x with respect to G. When a transformation Q ∈ G is applied to x, the stabilizer subgroup is left invariant, for if P ∈ stabG(x) then P(Qx) = Q(Px) = Qx and hence P ∈ stabG(Qx). The posterior of x transformed into itself, p(ϕ|x,x, µ, κ = 0) = ∏\nj M(ϕj |µ̂j , κ̂j) gives a probabilistic description of the stabilizer of x, and hence must be invariant. Clearly, the angle between x and x is zero, so µ̂ = 0. On the other hand, κ̂ contains information about x and is invariant. To see this, recall that κ̂j = ‖η̂j‖. Using eq. 12 we obtain κ̂j = ‖uj‖2σ−2 = ‖WTj x‖2σ−2. Since every transformation Q in the learned toroidal group acts on the 2D vector uj by rotation, the length of uj is left invariant.\nWe recognize the computation of κ̂ as the square pooling operation often applied in convolutional networks to gain invariance: project an image onto filters W2j−1,. and W2j,. and sum the squares. This computation follows as a necessary consequence of our model setup. In section 3.3, we will find that the model for non-maximal tori is even more informative about the proper pooling scheme.\nSince we want to use κ̂ as an invariant representation, we should try to find an appropriate metric on κ̂space. Let κ̂(x) be defined by p(ϕ|x,x, κ = 0) = ∏\nj M(ϕ|µ̂j , κ̂j(x)). We suggest using the Hellinger distance:\nH2(κ̂(x), κ̂(y)) = 1\n2\n∑\nj\n(\n√ κ̂j(x)− √ κ̂j(y)\n)2\n= 1\n2σ2\n∑\nj\n‖uj‖2 + ‖vj‖2 − 2‖uj‖‖vj‖\n= 1\n2σ2\n∑\nj\n‖vj −R(µ̂j)uj‖2,\nwhich is equal to the exact manifold distance (eq. 13) up to a factor of 12σ2 . The first step of this derivation uses eq. 12 under a uniform prior (ηj = 0), while the second step again makes use of the fact that µ̂j is the angle between uj and vj so that ‖uj‖‖vj‖ = uTj R(µ̂j)vj ."
    }, {
      "heading" : "3.2. Relation to the Discrete Fourier Transform",
      "text" : "We show that the DFT is a special case of TSA. The DFT of a discrete 1D signal x = (x1, . . . , xD)T is defined:\nXj =\nD−1 ∑\nn=0\nxnρ −jn (14)\nwhere ρ = e2πi/D is the D-th primitive root of unity. If we choose a basis of sinusoids for the filters in W,\nW(:, 2j−1) = R(ρ −j, . . . , ρ−j(D−1))T\n= (cos(2πj/D), . . . , cos(2πj(D − 1)/D))T\nW(:, 2j) = I(ρ −j , . . . , ρ−j(D−1))T\n= (sin(−2πj/D), . . . , sin(−2πj(D − 1)/D))T , (15)\nthen the change of basis performed by W is a DFT. Specifically, R(Xj) = uj1 and I(Xj) = uj2 .\nNow suppose we are interested in the transformation taking some arbitrary fixed vector e = W(1, 0, . . . , 1, 0)T to x. The posterior over ϕj is p(ϕj |e,x, ηj = 0, σ = 1) = M(ϕj |η̂j), where (by eq. 12) we have η̂j = ‖uj‖[cos(θj), sin(θj)]T , θj being the angle between uj and the “real axis” ej = (1, 0)T . In conventional coordinates, the precision of the posterior is equal to the modulus of the DFT, κ̂j = ‖uj‖ = |Xj |, and the mean of the posterior is equal to the phase of the Fourier transform, µ̂ = θj = arg(Xj). Therefore, TSA provides a probabilistic interpretation of the DFT, and makes it possible to learn an appropriate generalized transform from data."
    }, {
      "heading" : "3.3. Modeling a Lie subalgebra",
      "text" : "Typically, one is interested in learning groups with fewer than J degrees of freedom. Since the stabilizer representation is invariant to all transformations from the maximal torus which contains the symmetry group of interest, it is certainly also invariant to the subgroup of the maximal torus that is the true symmetry group. However, this representation will identify vectors that are in fact distinct from the point of view of the true symmetry group.\nAs we have seen, for one parameter compact subgroups of a maximal torus, the weights of the irreducible representations must be integers. We model this using a coupled rotation matrix, as follows:\nQ(s) = W\n\n  R(ω1s) . . .\nR(ωJs)\n\n  WT (16)\nWhere s ∈ [0, 2π] is the scalar parameter of this subgroup. The likelihood then becomes y ∼ N (y|Q(s)x, σ2).\nThe right prior for this likelihood is the generalized vonMises (Gatto & Jammalamadaka, 2007):\np(s) = M+(s|η+) = exp ( η+ · T+(s) ) / Z+\n= exp\n\n\nK ∑\nj=1\nκ+j cos(js− µ+j )\n\n / Z+\nwhere T+(s) = [cos(s), sin(s), . . . , cos(Ks), sin(Ks)]T .\nUsing similar reasoning as in the non-coupled case, we find that this prior is conjugate to the coupled likelihood, so that p(s|x,y) ∝ exp (η̂+ · T+(s)), with:\nη̂+j = η + j +\n∑\nk:ωk=j\nη̂k (17)\nwhere η̂k is obtained from eq. 12 using a uniform prior ηk = 0. The sum in this update equation performs a pooling operation over a weight space, meaning those invariant subspaces k whose weight ωk = j. In fact, the norm of any linear combination of same-weight representations is invariant, and the maximum of any two η̂j or η̂ + j is also invariant (Kanatani, 1990). The similarity to sum-pooling and max-pooling in convnets is striking, but whereas in convnets the filters adapt to a fixed pooling scheme, a TSA model can adapt the representation weights ω so as to perform the right kind of pooling.\nIn the non-coupled model, there are J = D/2 degrees of freedom in the group and the invariant representation is D − J = J-dimensional (κ̂1, . . . , κ̂J ). For the coupled model, there is only one degree of freedom in the group, so the invariant representation should be D − 1 dimensional. In the mathematically clean case where all ωk are distinct, we have J variables κ+1 , . . . , κ + J that are invariant. Furthermore, from eq. 16 we see that as x is transformed, the angle between x and an arbitrary fixed reference vector in subspace j transforms as θj(s) = δj + ωjs for some data-dependent initial phase δj . It follows that ωjθk(s) − ωkθj(s) = ωj(δk + ωks) − ωk(δj + ωjs) = ωjδk − ωkδj is invariant. In this way, we can easily construct another J − 1 invariants, but unfortunately these are not stable because the angle estimates can be inaccurate for low-energy subspaces. We leave the problem of devising a stable representation and a sensible metric on this space for future work.\nThe normalization constant Z+ for the GvM has so far only been described for the case of K = 2 harmonics, but we have found a closed form solution in terms of the socalled modified Generalized Bessel Functions (GBF) of Kvariables κ+ = κ+1 , . . . , κ + K and parameters exp (−iµ+) = exp (−iµ+1 ), . . . , exp (−iµ+K) (Dattoli et al., 1991):\nZ+(κ+, µ+) = 2πI0(κ +; e−iµ\n+\n). (18)\nWe have developed a novel, highly scalable algorithm for the computation of GBF of many variables, which is described in the supplementary material.\nFigure 2 shows the posterior over s for three image pairs related by different rotations and containing different symmetries. The weights W and ω were learned by the procedure described in the next section. It is quite clear from this figure that MAP inference does not give a complete description of the possible transformations relating the images when the images have a degree of rotational symmetry. The posterior distribution of our model provides a sensible way to deal with this kind of uncertainty, which (in the case of 2D translations) is at the heart of the well known aperture problem in vision. Thus, our model could be used to estimate “multimodal optical flows”. Having a tractable posterior is also particularly important if the model is to be used to estimate longer sequences (akin to HMM/LDS models, but non-linear), where one may encounter multiple high-density trajectories.\nIf required, accurate MAP inference can be performed using the algorithm of Sohl-Dickstein et al. (2010), as described in the supplementary material. This allows us to compute the exact manifold distance for the coupled model."
    }, {
      "heading" : "3.4. Maximum Marginal Likelihood Learning",
      "text" : "We train the model by gradient descent on the marginal likelihood. Perhaps surprisingly given the non-linearities in the model, the integrations required for the evaluation of the marginal likelihood can be obtained in closed form for both the coupled and decoupled models. For the decoupled model we obtain:\np(y|x) = ∫\nϕ∈TJ N (y|WR(ϕ)WTx)\n∏\nj\nM(ϕj |ηj)dϕ\n= exp\n( − 12σ2 (‖x‖2 + ‖y‖2) )\n√\n(2πσ)D\n∏\nj\nI0(κ̂j) I0(κj) .\n(19)\nObserving that I0(κ̂j)/I0(κj) is the a ratio of normalization constants of regular von-Mises distributions, the analogous expression for the coupled model is easily seen to be equal to eq. 19, only replacing ∏\nj I0(κ̂j) / I0(κj)\nby Z+(κ̂+, µ̂+)/Z+(κ+, µ+). The derivation of this result\ncan be found in the supplementary material.\nThe gradient of the log marginal likelihood of the uncoupled model w.r.t. a batch X,Y (both storing N vectors in the columns) is:\nd\ndW ln p(Y|X) = X(RT (µ)WTY A+WTX By)T\n+Y(R(µ)WTX A+WTY Bx)T .\nwhere we have used (P Q)2j,n = Qj,nP2j,n and (P Q)2j−1,n = Qj,nP2j−1,n as a “subspace weighting” operation. A, B(x) and B(y) are D×N matrices with elements\najn = I1(κ̂jn)κj\nI0(κ̂jn)κ̂jnσ2 ,\nbjn = ‖Wjy(n)‖2\nκjσ2 ,\nwhere the κ̂jn is the posterior precision in subspace j for image pair x(n), y(n) (the n-th column of X, resp. Y).\nThe gradient of the coupled model is easily computed using the differential recurrence relations that hold for the GBF (Dattoli et al., 1991).\nWe use minibatch Stochastic Gradient Descent (SGD) on the log-likelihood of the uncoupled model. After every parameter update, we orthogonalizeW by setting all singular values to 1: Let U,S,V = svd(W), then set W := UV. This procedure and all previous derivations still work when the basis is undercomplete, i.e. has fewer columns (filters) than rows (dimensions in data space). To learn ωj , we estimate the relative angular velocity ωj = θj/δ from a batch of image patches rotated by a sub-pixel amount δ = 0.1◦."
    }, {
      "heading" : "4. Experiments",
      "text" : "We trained a TSA model with 100 filters on a stream of 250.000 16 × 16 image patches x(t), y(t). The patches x(t) were drawn from a standard normal distribution, and y(t) was obtained by rotating x(t) by an angle s drawn uniformly at random from [0, 2π]. The learning rate α was initialized at α0 = 0.25 and decayed as α = α0/ √ T , where T was incremented by one with each pass through the data. Each minibatch consisted of 100 data pairs. After learning W, we estimate the weights ωj and sort the filter pairs by increasing absolute value for visualization. As can be seen in fig. 3, the filters are very clean and the weights are estimated correctly except for a few filters on row 1 and 2 that are assigned weight 0 when in fact they have a higher frequency.\nWe tested the utility of the model for invariant classification on a rotated version of the MNIST dataset, using a 1-Nearest Neighbor classifier. Each digit was rotated by a random angle and rescaled to 16 × 16 pixels, resulting in\n60k training examples and 10k testing examples, with no rotated duplicates. We compared the Euclidean distance (ED) in pixel space, tangent distance (TD) (Simard et al., 2000), Euclidean distance on the space of √ κ̂ (equivalent to the exact manifold distance for the maximal torus, see section 3.1), the true manifold distance for the 1-parameter 2D rotation group (MD), and the Euclidean distance on the non-rotated version of the MNIST dataset (ED-NR). The results in fig. 4 show that TD outperforms ED, but is outperformed by √ κ̂ and MD by a large margin. In fact, the MD-classifier is about as accurate as ED on a much simpler dataset, demonstrating that it has almost completely modded out the variation caused by rotation."
    }, {
      "heading" : "5. Conclusions and outlook",
      "text" : "We have presented a novel principle for learning disentangled representations, and worked out its consequences for a simple type of symmetry group. This leads to a completely tractable and very elegant model with potential applications to invariant classification and Bayesian estimation of motion. The model reproduces the pooling operations of convolutional networks from a probabilistic and Lie-group theoretic perspective, and provides a probabilistic interpretation of the DFT and its generalizations.\nThe type of disentangling obtained in this paper is contingent upon the rather minimalist assumption that all that can be said about images is that they are equivalent (rotated copies) or inequivalent. However, the universal nature of Weyl’s principle bodes well for future applications in deep, non-linear and non-commutative disentangling."
    } ],
    "references" : [ {
      "title" : "Representation Learning: A Review and New Perspectives",
      "author" : [ "Bengio", "Yoshua", "Courville", "Aaron", "Vincent", "Pascal" ],
      "venue" : "IEEE transactions on pattern analysis and machine intelligence,",
      "citeRegEx" : "Bengio et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Bengio et al\\.",
      "year" : 2013
    }, {
      "title" : "Unsupervised learning of a steerable basis for invariant image representations",
      "author" : [ "Bethge", "Matthias", "Gerwinn", "Sebastian", "Macke", "Jakob H" ],
      "venue" : "Proceedings of SPIE Human Vision and Electronic Imaging XII (EI105),",
      "citeRegEx" : "Bethge et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Bethge et al\\.",
      "year" : 2007
    }, {
      "title" : "Learning intermediate-level representations of form and motion from natural movies",
      "author" : [ "Cadieu", "Charles F", "Olshausen", "Bruno A" ],
      "venue" : "Neural computation,",
      "citeRegEx" : "Cadieu et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Cadieu et al\\.",
      "year" : 2012
    }, {
      "title" : "A Note on the Theory of n-Variable Generalized Bessel Functions",
      "author" : [ "G. Dattoli", "C. Chiccoli", "S. Lorenzutta", "G. Maino", "M. Richetta", "A. Torre" ],
      "venue" : "Il Nuovo Cimento B,",
      "citeRegEx" : "Dattoli et al\\.,? \\Q1991\\E",
      "shortCiteRegEx" : "Dattoli et al\\.",
      "year" : 1991
    }, {
      "title" : "The generalized von Mises distribution",
      "author" : [ "Gatto", "Riccardo", "Jammalamadaka", "SR" ],
      "venue" : "Statistical Methodology,",
      "citeRegEx" : "Gatto et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Gatto et al\\.",
      "year" : 2007
    }, {
      "title" : "Measuring invariances in deep networks",
      "author" : [ "I Goodfellow", "Q Le", "A. Saxe" ],
      "venue" : "Advances in Neural Information Processing Systems, pp",
      "citeRegEx" : "Goodfellow et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Goodfellow et al\\.",
      "year" : 2009
    }, {
      "title" : "Group Theoretical Methods in Image Understanding",
      "author" : [ "Kanatani", "Kenichi" ],
      "venue" : null,
      "citeRegEx" : "Kanatani and Kenichi.,? \\Q1990\\E",
      "shortCiteRegEx" : "Kanatani and Kenichi.",
      "year" : 1990
    }, {
      "title" : "Gradient-based learning applied to document recognition",
      "author" : [ "Y LeCun", "L. Bottou" ],
      "venue" : "Proceedings of the IEEE,",
      "citeRegEx" : "LeCun and Bottou,? \\Q1998\\E",
      "shortCiteRegEx" : "LeCun and Bottou",
      "year" : 1998
    }, {
      "title" : "On multi-view feature learning",
      "author" : [ "Memisevic", "Roland" ],
      "venue" : "International Conference on Machine Learning,",
      "citeRegEx" : "Memisevic and Roland.,? \\Q2012\\E",
      "shortCiteRegEx" : "Memisevic and Roland.",
      "year" : 2012
    }, {
      "title" : "Learning to Represent Spatial Transformations with Factored Higher-Order Boltzmann Machines",
      "author" : [ "Memisevic", "Roland", "Hinton", "Geoffrey E" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "Memisevic et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Memisevic et al\\.",
      "year" : 2010
    }, {
      "title" : "Learning the Lie groups of visual invariance",
      "author" : [ "Miao", "Xu", "Rao", "Rajesh P N" ],
      "venue" : "Neural computation,",
      "citeRegEx" : "Miao et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Miao et al\\.",
      "year" : 2007
    }, {
      "title" : "Learning Lie groups for invariant visual perception",
      "author" : [ "RPN Rao", "Ruderman", "DL" ],
      "venue" : "Advances in neural information processing systems,",
      "citeRegEx" : "Rao et al\\.,? \\Q1999\\E",
      "shortCiteRegEx" : "Rao et al\\.",
      "year" : 1999
    }, {
      "title" : "Transformation invariance in pattern recognition: Tangent distance and propagation",
      "author" : [ "Simard", "Patrice Y", "Le Cun", "Yann a", "Denker", "John S", "Victorri", "Bernard" ],
      "venue" : "International Journal of Imaging Systems and Technology,",
      "citeRegEx" : "Simard et al\\.,? \\Q2000\\E",
      "shortCiteRegEx" : "Simard et al\\.",
      "year" : 2000
    }, {
      "title" : "Actionable information in vision",
      "author" : [ "Soatto", "Stefano" ],
      "venue" : "IEEE 12th International Conference on Computer Vision, pp. 2138–2145",
      "citeRegEx" : "Soatto and Stefano.,? \\Q2009\\E",
      "shortCiteRegEx" : "Soatto and Stefano.",
      "year" : 2009
    }, {
      "title" : "An unsupervised algorithm for learning lie group transformations",
      "author" : [ "J Sohl-Dickstein", "JC Wang", "Olshausen", "BA" ],
      "venue" : "arXiv preprint,",
      "citeRegEx" : "Sohl.Dickstein et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Sohl.Dickstein et al\\.",
      "year" : 2010
    }, {
      "title" : "Extracting and composing robust features with denoising autoencoders",
      "author" : [ "Vincent", "Pascal", "Larochelle", "Hugo", "Bengio", "Yoshua", "Manzagol", "Pierre-Antoine" ],
      "venue" : "Proceedings of the 25th international conference on Machine learning,",
      "citeRegEx" : "Vincent et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Vincent et al\\.",
      "year" : 2008
    }, {
      "title" : "Exponential Family Harmoniums with an Application to Information Retrieval",
      "author" : [ "Welling", "Max", "Rosen-zvi", "Michal", "Hinton", "Geoffrey" ],
      "venue" : "Advances in neural information processing systems,",
      "citeRegEx" : "Welling et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Welling et al\\.",
      "year" : 2005
    } ],
    "referenceMentions" : [ {
      "referenceID" : 16,
      "context" : "The hallmark of the deep learning approach is to learn multiple layers of representation of data, and much work has gone into the development of representation learning modules such as RBMs and their generalizations (Welling et al., 2005), and autoencoders (Vincent et al.",
      "startOffset" : 216,
      "endOffset" : 238
    }, {
      "referenceID" : 15,
      "context" : ", 2005), and autoencoders (Vincent et al., 2008).",
      "startOffset" : 26,
      "endOffset" : 48
    }, {
      "referenceID" : 5,
      "context" : "Various desiderata for learned representations have been expressed, including meaningful (Bengio & Lecun, 2014), invariant (Goodfellow et al., 2009), abstract and disentangled (Bengio et al.",
      "startOffset" : 123,
      "endOffset" : 148
    }, {
      "referenceID" : 0,
      "context" : ", 2009), abstract and disentangled (Bengio et al., 2013) representations, but so far most of these notions have not been defined in a mathematically precise way.",
      "startOffset" : 35,
      "endOffset" : 56
    }, {
      "referenceID" : 14,
      "context" : "This problem was solved by Sohl-Dickstein et al. (2010) using an elegant adaptive smoothing technique, making it possible to learn from large transformations.",
      "startOffset" : 27,
      "endOffset" : 56
    }, {
      "referenceID" : 1,
      "context" : "Therefore, orthogonal matrices cannot express transformations such as contrast scaling, but they can still model the interesting structural changes in images (Bethge et al., 2007).",
      "startOffset" : 158,
      "endOffset" : 179
    }, {
      "referenceID" : 14,
      "context" : "As is well known, commuting matrices can be simultaneously diagonalized, so one could represent a toroidal group in terms of a basis of eigenvectors shared by every element in the group, and one diagonal matrix of eigenvalues for each element of the group, as was done in (Sohl-Dickstein et al., 2010) for 1-parameter Lie groups.",
      "startOffset" : 272,
      "endOffset" : 301
    }, {
      "referenceID" : 14,
      "context" : "Previous approaches to Lie group learning only provide point estimates of the transformation parameters, which have to be obtained using an expensive iterative optimization procedure (Sohl-Dickstein et al., 2010).",
      "startOffset" : 183,
      "endOffset" : 212
    }, {
      "referenceID" : 14,
      "context" : "Previous approaches to Lie group learning only provide point estimates of the transformation parameters, which have to be obtained using an expensive iterative optimization procedure (Sohl-Dickstein et al., 2010). In contrast, TSA provides a full tractable posterior distribution which is obtained using a simple feed-forward computation. Compared to the work of Cadieu & Olshausen (2012), our model deals well with low-energy subspaces, by simply describing the uncertainty in the estimate instead of providing inaccurate estimates that have to be discarded.",
      "startOffset" : 184,
      "endOffset" : 389
    }, {
      "referenceID" : 12,
      "context" : "In practice, it has proven difficult to compute this distance exactly, so approximations such as tangent distance have been invented (Simard et al., 2000).",
      "startOffset" : 133,
      "endOffset" : 154
    }, {
      "referenceID" : 3,
      "context" : ", exp (−iμ+K) (Dattoli et al., 1991): Z(κ, μ) = 2πI0(κ ; e + ).",
      "startOffset" : 14,
      "endOffset" : 36
    }, {
      "referenceID" : 14,
      "context" : "If required, accurate MAP inference can be performed using the algorithm of Sohl-Dickstein et al. (2010), as described in the supplementary material.",
      "startOffset" : 76,
      "endOffset" : 105
    }, {
      "referenceID" : 3,
      "context" : "The gradient of the coupled model is easily computed using the differential recurrence relations that hold for the GBF (Dattoli et al., 1991).",
      "startOffset" : 119,
      "endOffset" : 141
    }, {
      "referenceID" : 12,
      "context" : "We compared the Euclidean distance (ED) in pixel space, tangent distance (TD) (Simard et al., 2000), Euclidean distance on the space of √ κ̂ (equivalent to the exact manifold distance for the maximal torus, see section 3.",
      "startOffset" : 78,
      "endOffset" : 99
    } ],
    "year" : 2017,
    "abstractText" : "We present a new probabilistic model of commutative Lie groups, that produces invariantequivariant and disentangled representations of data. We borrow a fundamental principle from physics, used to define the elementary particles of a physical system, and use it to give a mathematically precise definition of the popular but heretofore rather vague notion of a “disentangled representation”. Our model is based on a newfound Bayesian conjugacy relation that enables us to perform fully tractable probabilistic inference over so-called Toroidal Lie groups – a class that includes practically relevant groups such as rotations and translations of images. We train the model on pairs of transformed image patches, and show that it produces a completely invariant representation which is highly effective for classification.",
    "creator" : "LaTeX with hyperref package"
  }
}