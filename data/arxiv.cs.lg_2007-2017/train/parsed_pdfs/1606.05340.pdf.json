{
  "name" : "1606.05340.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Exponential expressivity in deep neural networks through transient chaos",
    "authors" : [ "Ben Poole", "Surya Ganguli" ],
    "emails" : [ "poole@cs.stanford.edu", "sulahiri@stanford.edu", "maithrar@gmail.com", "jaschasd@google.com", "sganguli@stanford.edu" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Deep feedforward neural networks, with multiple hidden layers, have achieved remarkable performance across many domains [1–4]. A key factor thought to underlie their success is their high expressivity. This informal notion has manifested itself primarily in two forms of intuition. The first is that deep networks can compactly express highly complex functions over input space in a way that shallow networks with one hidden layer and the same number of neurons cannot. The second piece of intuition, which has captured the imagination of machine learning [5] and neuroscience [6] alike, is that deep neural networks can disentangle highly curved manifolds in input space into flattened manifolds in hidden space, to aid the performance of simple linear readouts. These intuitions, while attractive, have been difficult to formalize mathematically, and thereby rigorously test.\nFor the first intuition, seminal works have exhibited examples of particular functions that can be computed with a polynomial number of neurons (in the input dimension) in a deep network but require an exponential number of neurons in a shallow network [7–11]. This raises a central open question: are such functions merely rare curiosities, or is any function computed by a generic deep network not efficiently computable by a shallow network? The theoretical techniques employed in prior work both limited the applicability of theory to specific nonlinearities and dictated the particular measure of deep functional complexity involved. For example [7] focused on ReLu nonlinearities and number of linear regions as a complexity measure, while [8] focused on sum-product networks and the number of monomials as complexity measure, and [12] focused on Pfaffian nonlinearities and topological measures of complexity, like the sum of Betti numbers of a decision boundary. However,\nCode to reproduce all results available at: https://github.com/ganguli-lab/deepchaos\nar X\niv :1\n60 6.\n05 34\n0v 1\n[ st\nat .M\nL ]\n1 6\nsee [13] for an interesting analysis of a general class of compositional functions. The limits of prior theoretical techniques raise another central question: is there a unifying theoretical framework for deep neural expressivity that is simultaneously applicable to arbitrary nonlinearities, generic networks, and a natural, general measure of functional complexity?\nHere we attack both central problems of deep neural expressivity by combining a very different set of tools, namely Riemannian geometry [14] and dynamical mean field theory [15]. This novel combination enables us to show that for very broad classes of nonlinearities, even random deep neural networks can construct hidden internal representations whose global extrinsic curvature grows exponentially with depth but not width. Our geometric framework enables us to quantitatively define a notion of disentangling and verify this notion even in deep random networks. Furthermore, our methods yield insights into the emergent, deterministic nature of signal propagation through large random feedforward networks, revealing the existence of an order to chaos transition as a function of the statistics of weights and biases. We find that the transient, finite depth evolution in the chaotic regime underlies the origins of exponential expressivity in deep random networks.\nIn our companion paper [16], we study several related measures of expressivity in deep random neural networks with piecewise linear activations."
    }, {
      "heading" : "2 A mean field theory of deep nonlinear signal propagation",
      "text" : "Consider a deep feedforward network with D layers of weights W1, . . . ,WD and D + 1 layers of neural activity vectors x0, . . . ,xD, with Nl neurons in each layer l, so that xl ∈ RNl and Wl is an Nl ×Nl−1 weight matrix. The feedforward dynamics elicited by an input x0 is given by xl = φ(hl) hl = Wl xl−1 + bl for l = 1, . . . , D, (1) where bl is a vector of biases, hl is the pattern of inputs to neurons at layer l, and φ is a single neuron scalar nonlinearity that acts component-wise to transform inputs hl to activities xl. We wish to understand the nature of typical functions computable by such networks, as a consequence of their depth. We therefore study ensembles of random networks in which each of the synaptic weights Wlij are drawn i.i.d. from a zero mean Gaussian with variance σ 2 w/Nl−1, while the biases are drawn i.i.d. from a zero mean Gaussian with variance σ2b . This weight scaling ensures that the input contribution to each individual neuron at layer l from activities in layer l − 1 remains O(1), independent of the layer width Nl−1. This ensemble constitutes a maximum entropy distribution over deep neural networks, subject to constraints on the means and variances of weights and biases. This ensemble induces no further structure in the resulting set of deep functions, so its analysis provides an opportunity to understand the specific contribution of depth alone to the nature of typical functions computed by deep networks.\nIn the limit of large layer widths, Nl 1, certain aspects of signal propagation through deep random neural networks take on an essentially deterministic character. This emergent determinism in large random neural networks enables us to understand how the Riemannian geometry of simple manifolds in the input layer x0 is typically modified as the manifold propagates into the deep layers. For example, consider the simplest case of a single input vector x0. As it propagates through the network, its length in downstream layers will change. We track this changing length by computing the normalized squared length of the input vector at each layer:\nql = 1\nNl\nNl∑\ni=1\n(hli) 2. (2)\nThis length is the second moment of the empirical distribution of inputs hli across all Nl neurons in layer l. For large Nl, this empirical distribution converges to a zero mean Gaussian since each hli = ∑ jW l ijφ(h l−1 j ) + b l i is a weighted sum of a large number of uncorrelated random variables - i.e. the weights Wlij and biases b l i, which are independent of the activity in previous layers. By propagating this Gaussian distribution across one layer, we obtain an iterative map for ql in (11):\nql = V(ql−1 |σw, σb) ≡ σ2w ∫ Dz φ (√ ql−1z )2 + σ2b , for l = 2, . . . , D, (3)\nwhereDz = dz√ 2π e−\nz2\n2 is the standard Gaussian measure, and the initial condition is q1 = σ2wq 0 +σ2b ,\nwhere q0 = 1N0x 0 · x0 is the length in the initial activity layer. See Supplementary Material (SM)\nfor a derivation of (13). Intuitively, the integral over z in (13) replaces an average over the empirical distribution of hli across neurons i in layer l at large layer width Nl.\nThe function V in (13) is an iterative variance, or length, map that predicts how the length of an input in (11) changes as it propagates through the network. This length map is plotted in Fig. 1A for the special case of a sigmoidal nonlinearity, φ(h) = tanh(h). For monotonic nonlinearities, this length map is a monotonically increasing, concave function whose intersections with the unity line determine its fixed points q∗(σw, σb). For σb = 0 and σw < 1, the only intersection is at q∗ = 0. In this bias-free, small weight regime, the network shrinks all inputs to the origin. For σw > 1 and σb = 0, the q∗ = 0 fixed point becomes unstable and the length map acquires a second nonzero fixed point, which is stable. In this bias-free, large weight regime, the network expands small inputs and contracts large inputs. Also, for any nonzero bias σb, the length map has a single stable non-zero fixed point. In such a regime, even with small weights, the injected biases at each layer prevent signals from decaying to 0. The dynamics of the length map leads to rapid convergence of length to its fixed point with depth (Fig. 1B,D), often within only 4 layers. The fixed points q∗(σw, σb) are shown in Fig. 1C."
    }, {
      "heading" : "3 Transient chaos in deep networks",
      "text" : "Now consider the layer-wise propagation of two inputs x0,1 and x0,2. The geometry of these two inputs as they propagate through the network is captured by the 2 by 2 matrix of inner products:\nqlab = 1\nNl\nNl∑\ni=1\nhli(x 0,a)hli(x 0,b) a, b ∈ {1, 2}. (4)\nThe dynamics of the two diagonal terms are each theoretically predicted by the length map in (13). We derive (see SM) a correlation map C that predicts the layer-wise dynamics of ql12:\nql12 = C(cl−112 , q l−1 11 , q l−1 22 |σw, σb) ≡ σ2w ∫ Dz1Dz2 φ (u1)φ (u2) + σ2b , (5)\nu1 = √ ql−111 z1, u2 = √ ql−122 [ cl−112 z1 + √ 1− (cl−112 )2z2 ] ,\nwhere cl12 = q l 12(q l 11q l 22) −1/2 is the correlation coefficient. Here z1 and z2 are independent standard Gaussian variables, while u1 and u2 are correlated Gaussian variables with covariance matrix 〈uaub〉 = ql−1ab . Together, (13) and (15) constitute a theoretical prediction for the typical evolution of the geometry of 2 points in (14) in a fixed large network.\nAnalysis of these equations reveals an interesting order to chaos transition in the σw and σb plane. In particular, what happens to two nearby points as they propagate through the layers? Their relation to\neach other can be tracked by the correlation coefficient cl12 between the two points, which approaches a fixed point c∗(σw, σb) at large depth. Since the length of each point rapidly converges to q∗(σw, σb), as shown in Fig. 1BD, we can compute c∗ by simply setting ql11 = q l 22 = q\n∗(σw, σb) in (15) and dividing by q∗ to obtain an iterative correlation coefficient map, or C-map, for cl12:\ncl12 = 1 q∗ C(cl−112 , q∗, q∗ |σw, σb). (6)\nThis C-map is shown in Fig. 2A. It always has a fixed point at c∗ = 1 as can be checked by direct calculation. However, the stability of this fixed point depends on the slope of the map at 1, which is\nχ1 ≡ ∂cl12 ∂cl−112 ∣∣∣∣∣ c=1 = σ2w ∫ Dz [ φ′ (√ q∗z )]2 . (7)\nSee SM for a derivation of (19). If the slope χ1 is less than 1, then the C-map is above the unity line, the fixed point at 1 under the C-map in (16) is stable, and nearby points become more similar\nover time. Conversely, if χ1 > 1 then this fixed point is unstable, and nearby points separate as they propagate through the layers. Thus we can intuitively understand χ1 as a multiplicative stretch factor. This intuition can be made precise by considering the Jacobian Jlij = W l ijφ ′(hl−1j ) at a point hl−1j with length q ∗. Jl is a linear approximation of the network map from layer l − 1 to l in the vicinity of hl−1. Therefore a small random perturbation hl−1 + u will map to hl + Ju. The growth of the perturbation, ||Ju||22/||u||22 becomes χ1(q∗) after averaging over the random perturbation u, weight matrix Wl, and Gaussian distribution of hl−1i across i. Thus χ1 directly reflects the typical multiplicative growth or shrinkage of a random perturbation across one layer.\nThe dynamics of the iterative C-map and its agreement with network simulations is shown in Fig. 2B. The correlation dynamics are much slower than the length dynamics because the C-map is closer to the unity line (Fig. 2A) than the length map (Fig. 1A). Thus correlations typically take about 20 layers to approach the fixed point, while lengths need only 4. The fixed point c∗ and slope χ1 of the C-map are shown in Fig. 2CD. For any fixed, finite σb, as σw increases three qualitative regions occur. For small σw, c∗ = 1 is the only fixed point, and it is stable because χ1 < 1. In this strong bias regime, any two input points converge to each other as they propagate through the network. As σw increases, χ1 increases and crosses 1, destabilizing the c∗ = 1 fixed point. In this intermediate regime, a new stable fixed point c∗ appears, which decreases as σw increases. Here an equal footing competition between weights and nonlinearities (which de-correlate inputs) and the biases (which correlate them), leads to a finite c∗. At larger σw, the strong weights overwhelm the biases and maximally de-correlate inputs to make them orthogonal, leading to a stable fixed point at c∗ = 0.\nThus the equation χ1(σw, σb) = 1 yields a phase transition boundary in the (σw, σb) plane, separating it into a chaotic (or ordered) phase, in which nearby points separate (or converge). In dynamical systems theory, the logarithm of χ1 is related to the well known Lyapunov exponent which is positive (or negative) for chaotic (or ordered) dynamics. However, in a feedforward network, the dynamics is truncated at a finite depth D, and hence the dynamics are a form of transient chaos."
    }, {
      "heading" : "4 The propagation of manifold geometry through deep networks",
      "text" : "Now consider a 1 dimensional manifold x0(θ) in input space, where θ is an intrinsic scalar coordinate on the manifold. This manifold propagates to a new manifold hl(θ) = hl(x0(θ)) in the vector space of inputs to layer l. The typical geometry of the manifold in the l’th layer is summarized by ql(θ1, θ2), which for any θ1 and θ2 is defined by (14) with the choice x0,a = x0(θ1) and x0,b = x0(θ2). The theory for the propagation of pairs of points applies to all pairs of points on the manifold, so intuitively, we expect that in the chaotic phase of a sigmoidal network, the manifold should in some sense de-correlate, and become more complex, while in the ordered phase the manifold should contract around a central point. This theoretical prediction of equations (13) and (15) is quantitatively confirmed in simulations in Fig. 3, when the input is a simple manifold, the circle, h1(θ) = √ N1q [ u0 cos(θ) + u1 sin(θ) ] , where u0 and u1 form an orthonormal basis for a 2 dimensional subspace of RN1 in which the circle lives. The scaling is chosen so that each neuron has input activity O(1). Also, for simplicity, we choose the fixed point radius q = q∗ in Fig. 3.\nTo quantitatively understand the layer-wise growth of complexity of this manifold, it is useful to turn to concepts in Riemannian geometry [14]. First, at each point θ, the manifold h(θ) (we temporarily suppress the layer index l) has a tangent, or velocity vector v(θ) = ∂θh(θ). Intuitively, curvature is related to how quickly this tangent vector rotates in the ambient space RN as one moves along the manifold, or in essence the acceleration vector a(θ) = ∂θv(θ). Now at each point θ, when both are nonzero, v(θ) and a(θ) span a 2 dimensional subspace of RN . Within this subspace, there is a unique circle of radius R(θ) that has the same position, velocity and acceleration vector as the curve h(θ) at θ. This circle is known as the osculating circle (Fig. 4A), and the extrinsic curvature κ(θ) of the curve is defined as κ(θ) = 1/R(θ). Thus, intuitively, small radii of curvature R(θ) imply high extrinsic curvature κ(θ). The extrinsic curvature of a curve depends only on its image in RN and is invariant with respect to the particular parameterization θ → h(θ). For any parameterization, an explicit expression for κ(θ) is given by κ(θ) = (v ·v)−3/2 √ (v · v)(a · a)− (v · a)2 [14]. Note that\nunder a unit speed parameterization of the curve, so that v(θ) · v(θ) = 1, we have v(θ) · a(θ) = 0, and κ(θ) is simply the norm of the acceleration vector.\nAnother measure of the curve’s complexity is the length LE of its image in the ambient Euclidean space. The Euclidean metric in RN induces a metric gE(θ) = v(θ) · v(θ) on the curve, so that the distance dLE moved in RN as one moves from θ to θ + dθ on the curve is dLE = √ gE(θ)dθ. The total curve length is LE = ∫ √\ngE(θ)dθ. However, even straight line segments can have a large Euclidean length. Another interesting measure of length that takes into account curvature, is the length of the image of the curve under the Gauss map. For aK dimensional manifoldM embedded in RN , the Gauss map (Fig. 4B) maps a point θ ∈M to itsK dimensional tangent plane TθM∈ GK,N , where GK,N is the Grassmannian manifold of allK dimensional subspaces in RN . In the special case of K = 1, GK,N is the sphere SN−1 with antipodal points identified, since a 1-dimensional subspace can be identified with a unit vector, modulo sign. The Gauss map takes a point θ on the curve and maps it to the unit velocity vector v̂(θ) = v(θ)/ √ v(θ) · v(θ). In particular, the natural metric on SN−1 induces a Gauss metric on the curve, given by gG(θ) = (∂θv̂(θ)) · (∂θv̂(θ)), which measures how quickly the unit tangent vector v̂(θ) changes as θ changes. Thus the distance dLG moved in the Grassmannian GK,N as one moves from θ to θ + dθ on the curve is dLG = √ gG(θ)dθ, and the\nlength of the curve under the Gauss map is LG = ∫ √\ngG(θ)dθ. Furthermore, the Gauss metric is related to the extrinsic curvature and the Euclidean metric via the relation gG(θ) = κ(θ)2gE(θ) [14].\nTo illustrate these concepts, it is useful to compute all of them for the circle h1(θ) defined above: gE(θ) = Nq, LE = 2π √ Nq, κ(θ) = 1/ √ Nq, gG(θ) = 1, and LG = 2π. As expected, κ(θ) is the inverse of the radius of curvature, which is √ Nq. Now consider how these quantities change if the circle is scaled up so that h(θ) → χh(θ). The length LE and radius scale up by χ, but the curvature κ scales down as χ−1, and so LG does not change. Thus linear expansion increases length and decreases curvature, thereby maintaining constant Grassmannian length LG. We now show that nonlinear propagation of this same circle through a deep network can behave very differently from linear expansion: in the chaotic regime, length can increase without any decrease in extrinsic curvature! To remove the scaling with N in the above quantities, we will work with the renormalized quantities κ̄ = √ Nκ, ḡE = 1N g E , and L̄E = 1√ N LE . Thus, 1/(κ̄)2 can be thought\nof as a radius of curvature squared per neuron of the osculating circle, while (L̄E)2 is the squared Euclidean length of the curve per neuron. For the circle, these quantities are q and 2πq respectively. For simplicity, in the inputs to the first layer of neurons, we begin with a circle h1(θ) with squared radius per neuron q1 = q∗, so this radius is already at the fixed point of the length map in (13). In the SM, we derive an iterative formula for the extrinsic curvature and Euclidean metric of this manifold as it propagates through the layers of a deep network:\nḡE,l = χ1 ḡ E,l−1 (κ̄l)2 = 3 χ2 χ21 + 1 χ1 (κ̄l−1)2, ḡE,1 = q∗, (κ̄1)2 = 1/q∗. (8)\nwhere χ1 is the stretch factor defined in (19) and χ2 is defined analogously as\nχ2 = σ 2 w\n∫ Dz [ φ′′ (√ q∗z )]2 . (9)\nχ2 is closely related to the second derivative of the C-map in (16) at cl−112 = 1; this second derivative is χ2q∗. See App. C for a derivation of the evolution equations (27) for the extrinsic geometry of a curve as it propagates through a deep network.\nIntriguingly for a sigmoidal neural network, these evolution equations behave very differently in the chaotic (χ1 > 1) versus ordered (χ1 < 1) phase. In the chaotic phase, the Euclidean metric ḡE grows exponentially with depth due to multiplicative stretching through χ1. This stretching does multiplicatively attenuate any curvature in layer l − 1 by a factor 1/χ1 (see the update equation for κ̄l in (27)), but new curvature is added in due to a nonzero χ2, which originates from the curvature of the single neuron nonlinearity in (28). Thus, unlike in linear expansion, extrinsic curvature is not lost, but maintained, and ultimately approaches a fixed point κ̄∗. This implies that the global curvature measure L̄G grows exponentially with depth. These highly nontrivial predictions of the metric and curvature evolution equations in (27) are quantitatively confirmed in simulations in Figure 4C-E.\nIntuitively, this exponential growth of global curvature L̄G in the chaotic phase implies that the curve explores many different tangent directions in hidden representation space. This further implies that the coordinate functions of the embedding hli(θ) become highly complex curved basis functions on the input manifold coordinate θ, allowing a deep network to compute exponentially complex functions over simple low dimensional manifolds (Figure 5A-C, details in SM). In our companion paper [16], we further develop the relationship between length and expressivity in terms of the number of achievable classification patterns on a set of inputs. Moreover, we explore how training a single layer at different depths from the output affects network performance.\nA neurons B regression\nC angular error for deep networks\nD angular error for wide networks"
    }, {
      "heading" : "5 Shallow networks cannot achieve exponential expressivity",
      "text" : "Consider a shallow network with 1 hidden layer x1, one input layer x0, with x1 = φ(W1x0) + b1, and a linear readout layer. How complex can the hidden representation be as a function of its width N1, relative to the results above for depth? We prove a general upper bound on LE (see SM):\nTheorem 1. Suppose φ(h) is monotonically non-decreasing with bounded dynamic range R, i.e. maxh φ(h)−minh φ(h) = R. Further suppose that x0(θ) is a curve in input space such that no 1D projection of ∂θx(θ) changes sign more than s times over the range of θ. Then for any choice of W1 and b1 the Euclidean length of x1(θ), satisfies LE ≤ N1(1 + s)R.\nFor the circle input, s = 1 and for the tanh nonlinearity,R = 2, so in this special case, the normalized length L̄E ≤ 2 √ N1. In contrast, for deep networks in the chaotic regime L̄E grows exponentially with depth in h space, and so consequently also in x space. Therefore the length of curves typically expand exponentially in depth even for random deep networks, but can only expand as the square root of width no matter what shallow network is chosen. Moreover, as we have seen above, it is the exponential growth of L̄E that fundamentally drives the exponential growth of L̄G with depth. Indeed shallow random networks exhibit minimal growth in expressivity even at large widths (Figure 5D)."
    }, {
      "heading" : "6 Classification boundaries acquire exponential local curvature with depth",
      "text" : "We have focused so far on how simple manifolds in input space can acquire both exponential Euclidean and Grassmannian length with depth, thereby exponentially de-correlating and filling up hidden representation space. Another natural question is how the complexity of a decision boundary grows as it is backpropagated to the input layer. Consider a linear classifier y = sgn(β · xD − β0) acting on the final layer. In this layer, the N − 1 dimensional decision boundary is the hyperplane β ·xD−β0 = 0. However, in the input layer x0, the decision boundary is a curvedN−1 dimensional manifoldM that arises as the solution set of the nonlinear equation G(x0) ≡ β · xD(x0)− β0 = 0, where xD(x0) is the nonlinear feedforward map from input to output.\nAt any point x∗ on the decision boundary in layer l, the gradient ~∇G is perpendicular to the N − 1 dimensional tangent plane Tx∗M (see Fig. 4F). The normal vector ~∇G, along with any unit tangent vector v̂ ∈ Tx∗M, spans a 2 dimensional subspace whose intersection withM yields a geodesic curve inM passing through x∗ with velocity vector v̂. This geodesic will have extrinsic curvature κ(x∗, v̂). Maximizing this curvature over v̂ yields the first principal curvature κ1(x∗). A sequence of successive maximizations of κ(x∗, v̂), while constraining v̂ to be perpendicular to all previous solutions, yields the sequence of principal curvatures κ1(x∗) ≥ κ2(x∗) ≥ · · · ≥ κN−1(x∗). These principal curvatures arise as the eigenvalues of a normalized Hessian operator projected onto the tangent plane Tx∗M: H = ||~∇G||−12 P ∂ 2G ∂x∂xT\nP, where P = I− ∇̂G∇̂GT is the projection operator onto Tx∗M and ∇̂G is the unit normal vector [14]. Intuitively, near x∗, the decision boundaryM can be approximated as a paraboloid with a quadratic form H whose N − 1 eigenvalues are the principal curvatures κ1, . . . , κN−1 (Fig. 4F).\nWe compute these curvatures numerically as a function of depth in Fig. 4G (see SM for details). We find, remarkably, that a subset of principal curvatures grow exponentially with depth. Here the principal curvatures are signed, with positive (negative) curvature indicating that the associated geodesic curves towards (away from) the normal vector ~∇G. Thus the decision boundary can become exponentially curved with depth, enabling highly complex classifications. Moreover, this exponentially curved boundary is disentangled and mapped to a flat boundary in the output layer."
    }, {
      "heading" : "7 Discussion",
      "text" : "Fundamentally, neural networks compute nonlinear maps between high dimensional spaces, for example from RN1 → RND , and it is unclear what the most appropriate mathematics is for understanding such daunting spaces of maps. Previous works have attacked this problem by restricting the nature of the nonlinearity involved (e.g. piecewise linear, sum-product, or Pfaffian) and thereby restricting the space of maps to those amenable to special theoretical analysis methods (combinatorics, polynomial relations, or topological invariants). We have begun a preliminary exploration of the expressivity of such deep functions based on Riemannian geometry and dynamical mean field theory. We demonstrate that networks in a chaotic phase compactly exhibit functions that exponentially grow the global curvature of simple one dimensional manifolds from input to output and the local curvature of simple co-dimension one manifolds from output to input. The former captures the notion that deep neural networks can efficiently compute highly expressive functions in ways that shallow networks\ncannot, while the latter quantifies and demonstrates the power of deep neural networks to disentangle curved input manifolds, an attractive idea that has eluded formal quantification.\nMoreover, our analysis of a maximum entropy distribution over deep networks constitutes an important null model of deep signal propagation that can be used to assess and understand different behavior in trained networks. For example, the metrics we have adapted from Riemannian geometry, combined with an understanding of their behavior in random networks, may provide a basis for understanding what is special about trained networks. Furthermore, while we have focused on the notion of input-output chaos, the duality between inputs and synaptic weights imply a form of weight chaos, in which deep neural networks rapidly traverse function space as weights change (see SM). Indeed, just as autocorrelation lengths between outputs as a function of inputs shrink exponentially with depth, so too will autocorrelations between outputs as a function of weights.\nBut more generally, to understand functions, we often look to their graphs. The graph of a map from RN1 → RND is an RN1 dimensional submanifold of RN1+ND , and therefore has both high dimension and co-dimension. We speculate that many of the secrets of deep learning may be uncovered by studying the geometry of this graph as a Riemannian manifold, and understanding how it changes with both depth and learning."
    }, {
      "heading" : "A Derivation of a transient dynamical mean field theory for deep networks",
      "text" : "We study a deep feedforward network with D layers of weights W1, . . . ,WD and D + 1 layers of neural activity vectors x0, . . . ,xD, with Nl neurons in each layer l, so that xl ∈ RNl and Wl is an Nl ×Nl−1 weight matrix. The feedforward dynamics elicited by an input x0 is given by\nxl = φ(hl) hl = Wl xl−1 + bl for l = 1, . . . , D, (10)\nwhere bl is a vector of biases, hl is the pattern of inputs to neurons at layer l, and φ is a single neuron scalar nonlinearity that acts component-wise to transform inputs hl to activities xl. The synaptic weights Wlij are drawn i.i.d. from a zero mean Gaussian with variance σ 2 w/Nl−1, while the biases are drawn i.i.d. from a zero mean Gaussian with variance σ2b . This weight scaling ensures that the input contribution to each individual neuron at layer l from activities in layer l − 1 remains O(1), independent of the layer width Nl−1.\nA.1 Derivation of the length map\nAs a single input point x0 propagates through the network, it’s length in downstream layers can either grow or shrink. To track the propagation of this length, we track the normalized squared length of the input vector at each layer,\nql = 1\nNl\nNl∑\ni=1\n(hli) 2. (11)\nThis length is the second moment of the empirical distribution of inputs hli across all Nl neurons in layer l for a fixed set of weights. This empirical distribution is expected to be Gaussian for large Nl, since each individual hli = w\nl,i · φ(hl−1) + bli is Gaussian distributed, as a sum of a large number of independent random variables, and each hli is independent of h l j for i 6= j because the synaptic weights vectors and biases into each neuron are chosen independently.\nWhile the mean of this Gaussian is 0, its variance can be computed by considering the variance of the input to a single neuron:\nql = 〈 (hli) 2 〉 = 〈[ wl,i · φ(hl−1) ]2〉 + 〈 (bli) 2 〉 = σ2w 1\nNl−1\nNl−1∑\ni=1\nφ(hl−1i ) 2 + σ2b , (12)\nwhere 〈·〉 denotes an average over the distribution of weights and biases into neuron i at layer l. Here we have used the identity 〈wl,ij w l,i k 〉 = δjk σ2w/Nl−1. Now the empirical distribution of inputs across layer l− 1 is also Gaussian, with mean zero and variance ql−1. Therefore we can replace the average over neurons in layer l − 1 in (12) with an integral over a Gaussian random variable, obtaining\nql = V(ql−1 |σw, σb) ≡ σ2w ∫ Dz φ (√ ql−1z )2 + σ2b , for l = 2, . . . , D, (13)\nwhere Dz = dz√ 2π e−\nz2\n2 is the standard Gaussian measure, and the initial condition for the variance map is q1 = σ2wq0 + σ 2 b , where q 0 = 1N0x 0 · x0 is the length in the initial activity layer. The function V in (13) is an iterative variance map that predicts how the length of an input in (11) changes as it propagates through the network. Its derivation relies on the well-known self-averaging assumption in the statistical physics of disordered systems, which, in our context, means that the empirical distribution of inputs across neurons for a fixed network converges for large width, to the distribution of inputs to a single neuron across random networks.\nA.2 Derivation of a correlation map for the propagation of two points\nNow consider the layer-wise propagation of two inputs x0,1 and x0,2. The geometry of these two inputs as they propagate through the layers is captured by the 2 by 2 matrix of inner products\nqlab = 1\nNl\nNl∑\ni=1\nhli(x 0,a)hli(x 0,b) a, b ∈ {1, 2}. (14)\nThe joint empirical distribution of hli(x 0,a) and hli(x 0,a) across i at large Nl will converge to a 2 dimensional Gaussian distribution with covariance qlab. Propagating this joint distribution forward one layer using ideas similar to the derivation above for 1 input yields\nql12 = C(cl−112 , q l−1 11 , q l−1 22 |σw, σb) ≡ σ2w ∫ Dz1Dz2 φ (u1)φ (u2) + σ2b , (15)\nu1 = √ ql−111 z1, u2 = √ ql−122 [ cl−112 z1 + √ 1− (cl−112 )2z2 ] ,\nwhere cl12 = ql12√\nql11 √ ql12 is the correlation coefficient (CC). Here z1 and z2 are independent standard\nGaussian variables, while u1 and u2 are correlated Gaussian variables with covariance matrix 〈uaub〉 = ql−1ab . The integration over z1 and z2 can be thought of as the large Nl limit of sums over hli(x 0,a) and hli(x 0,a).\nWhen both input points are at their fixed point length, q∗, the dynamics of their correlation coefficient can be obtained by simply setting ql11 = q l 22 = q\n∗(σw, σb) in (15) and dividing by q∗ to obtain a recursion relation for cl12:\ncl12 = 1 q∗ C(cl−112 , q∗, q∗ |σw, σb) (16)\nDirect calculation reveals that cl12(1) = 1 as expected. Of particular interest is the slope χ1 of this map at 1. A direct, if tedious calculation shows that\n∂cl12 ∂cl−112 = σ2w\n∫ Dz1Dz2 φ′ (u1)φ′ (u2) . (17)\nTo obtain this result, one has to apply the chain rule and product rule from calculus, as well as employ the identity ∫\nDzF (z)z = ∫ DzF ′(z), (18)\nwhich can be obtained via integration by parts. Evaluating the derivative at 1 yields\nχ1 ≡ ∂cl12 ∂cl−112 ∣∣∣∣∣ c=1 = σ2w ∫ Dz [ φ′ (√ q∗z )]2 . (19)"
    }, {
      "heading" : "B Derivation of evolution equations for Riemannian curvature",
      "text" : "Here we derive recursion relations for Riemannian curvature quantitites.\nB.1 Curvature and length in terms of inner products\nConsider a translation invariant manifold, or 1D curve h(θ) ∈ RN that is on some constant radius sphere so that q(θ1, θ2) = Q(θ1 − θ2) = h(θ1) · h(θ2), (20) with Q(0) = Nq∗. At large N , the inner-product structure of translation invariant manifolds remains approximately translation invariant as it propagates through the network. Therefore, at large N , we can express inner products of derivatives of h in terms of derivatives of Q. For example, the Euclidean metric gE is given by\ngE(θ) = ∂θh(θ) · ∂θh(θ) = −Q̈(0). (21)\nHere, each dot is a short hand notation for derivative w.r.t. θ. Also, the extrinsic curvature\nκ(θ) =\n√ (v · v)(a · a)− (v · a)2\n(v · v)3 , (22)\nwhere v(θ) = ∂θh(θ) and a(θ) = ∂2θh(θ), simplifies to\nκ(θ) =\n.... Q(0) Q̈(0)2 . (23)\nNow if the translation invariant manifold lives on a sphere of radius Nq∗ where q∗ is the fixed point radius of the length map, then its radius does not change as it propagates through the system. Then we can also express gE and κ in terms of the correlation coefficient function c(θ) = Q(θ)/q∗ (up to a factor of N ). Thus to understand the propagation of local quantities like Euclidean length and curvature, we need to understand the propagation of derivatives of c(θ) at θ = 0 under the C-map in (16). Note that c(θ) is symmetric and achieves a maximum value of 1 at θ = 0. Thus the function H1(θ) = 1 − c(θ) is symmetric with a minimum at θ = 0. We consider the propagation of H1 though the C-map. But first we consider the propagation of derivatives under function composition in general.\nB.2 Behavior of first and second derivatives under function composition\nAssume H1(∆t) is an even function and H1(0) = 0, so that its Taylor expansion can be written as H1(∆t) = 12Ḧ 1(0)∆t2 + 14 .... H 1 (0)∆t4 + . . . . We are interested in determining how the second and fourth derivatives of H propagate under composition with another function G, so that H2 = G(H1(∆t)) . We assume G(0) = 0. We can use the chain rule and the product rule to derive:\nḦ2(0) = Ġ(0)Ḧ1(0) (24) .... H 2 (0) = 3G̈(0)Ḧ1(0)2 + Ġ(0) .... H 1 (0). (25)\nB.3 Evolution equations for curvature and length\nWe now apply the above iterations withH1(θ) = 1−c(θ) andG(c) = 1− 1q∗ C(1−c, q ∗, q∗ |σw, σb). Clearly, G(0) = 0 the symmetric H1 obeys H1(0) = 0, satisfying the above iterations of second and fourth derivatives. Taking into account these derivative recursions, using the expressions for κ and gE in terms of derivatives of c(θ) at 0, and carefully accounting for factors of q∗ and N , we obtain the final evolution equations that have been successfully tested against experiments:\nḡE,l = χ1 ḡ E,l−1 (26)\n(κ̄l)2 = 3 χ2 χ21 + 1 χ1 (κ̄l−1)2, (27)\nwhere χ1 is the stretch factor defined in (19) and χ2 is defined analogously as\nχ2 = σ 2 w\n∫ Dz [ φ′′ (√ q∗z )]2 . (28)\nχ2 is closely related to the second derivative of the correlation coefficient map in (16) at cl−112 = 1. Indeed this second derivative is χ2q∗."
    }, {
      "heading" : "C Upper bounds on the complexity of shallow neural representations",
      "text" : "Consider a shallow network with 1 hidden layer x1 and one input layer x0, so that x1 = φ(W1x0)+b. The network can compute functions through a linear readout of the hidden layer x1. We are interested in how complex these neural representations can get, with one layer of synaptic weights and nonlinearities, as a function the number of hidden units N1. In particular, we are interested in how the length and curvature of an input manifold x0(θ) changes as it propagates to become x1(θ) in the hidden layer. We would like to upper bound the maximal achievable length and curvature over all possible choices of W1 and b.\nC.1 Upper bound on Euclidean length\nHere, we derive such an upper bound on the Euclidean length for a very general class of nonlinearities φ(h). We simply assume that (1) φ(h) is monotonically non-decreasing (so that φ′(h) ≥ 0∀h) and (2) has with bounded dynamic range R, i.e. maxh φ(h)−minh φ(h) = R. The Euclidean length in hidden space is\nLE = ∫ dθ √√√√ N1∑\ni=1\n(∂θx1i (θ)) 2 ≤\nN1∑\ni=1\n∫ dθ ∣∣∂θx1i (θ) ∣∣ , (29)\nwhere the inequality follows from the triangle inequality. Now suppose that for any i, ∂θx1i (θ) never changes sign across θ. Furthermore, assume that θ ranges from 0 to Θ. Then\n∫ Θ\n0\ndθ ∣∣∂θx1i (θ) ∣∣ = x1i (Θ)− x1i (0) ≤ (\nmax h φ(h)−min h φ(h)\n) = R. (30)\nMore generally, let r1 denote the maximal number of times that any one neuron has a change in sign of the derivative ∂θx1i (θ) across θ. Then applying the above argument to each segment of constant sign yields ∫ Θ\n0\ndθ ∣∣∂θx1i (θ) ∣∣ ≤ (1 + r1)R. (31)\nNow how many times can ∂θx1i (θ) change sign? Since ∂θx 1 i (θ) = φ ′(hi) ∂θhi, where ∂θhi(θ) = [Wl∂θx\n0(θ)]i, and φ(hi) is monotonically increasing, the number of times ∂θx1i (θ) changes sign equals the number of times the input ∂θhi(θ) changes sign. In turn, suppose s0 is the maximal number of times any one dimensional projection of the derivative vector ∂θx0(θ) changes sign across θ. Then the number of times the sign of ∂θhi(θ) changes for any i cannot exceed s0 because hi is a linear projection of x0. Together this implies r1 ≤ s0. We have thus proven:\nLE ≤ N1(1 + s0)R. (32)"
    }, {
      "heading" : "D Simulation details",
      "text" : "All neural network simulations were implemented in Keras and Theano. For all simulations (except Figure 5C), we used inputs and hidden layers with a width of 1,000 and tanh activations. We found that our results were mostly insensitive to width, but using larger widths decreased the fluctuations in the averaged quantities. Simulation error bars are all standard deviations, with the variance computed across the different inputs, h1(θ). If not mentioned, the weights in the network are initialized in the chaotic regime with σb = 0.3, σw = 4.0.\nComputing κ(θ) requires the computation of the velocity and acceleration vectors, corresponding to the first and second derivatives of the neural network hl(θ) with respect to θ. As θ is always onedimensional, we can greatly speed up these computations by using forward-mode auto-differentiation, evaluating the Jacobian and Hessian in a feedforward manner. We implemented this using the R-op in Theano.\nD.1 Details on Figure 4G: backpropagating curvature\nTo identify the curvature of the decision boundary, we first had to identify points that lied along the decision boundary. We randomly initialized data points and then optimized G(xD(xl))2 with respect to the input x using Adam. This yields a set of inputs xl where we compute the Jacobian and Hessian of G(xD(xl)) to evaluate principal curvatures.\nD.2 Details on Figure 5C-D: evaluating expressivity\nTo evaluate the set of functions reachable by a network, we first parameterized function space using a Fourier basis up to a particular maximum frequency, ωmax on a sampled set of one dimensional inputs parameterized by θ. We then took the output activations of each neural network and linearly regressed the output activations onto each Fourier basis. For each basis, we computed the angle between the\npredicted basis vector and the true basis vector. These are the quantities that appear in Figure 5C-D. Given any function with bounded frequency, we can represent it in this Fourier basis, and decompose the error in the prediction of the function into the error in prediction of each Fourier component. Thus error in the predicting the Fourier basis is a reasonable proxy for error in prediction of functions with bounded frequency."
    }, {
      "heading" : "E Additional visualization of hidden actions",
      "text" : ""
    }, {
      "heading" : "F A view from the function space perspective",
      "text" : "We have shown above that for a fixed set of weights and biases in the chaotic regime, the internal representation hl(x0) at large depth l, rapidly de-correlates from itself as the input x0 changes (see e.g. Fig. 3B in the main paper). Here we ask a dual question: for a fixed input manifold, how does a deep network move in a function space over this manifold as the weights in a single layer change? Consider for example, a random one parameter family of deep networks parameterized by ∆ ∈ [−1, 1]. In this family, we assume that the bias vectors bl in each layer are chosen as i.i.d. random Gaussian vectors with zero mean and variance σ2b , independent of ∆. Moreover, we assume the weight matrix Wl has elements that are drawn i.i.d. from zero mean Gaussians with variance σ2w, independent of ∆ for all layers except l = 2. The only dependence on ∆ in this family of networks\noriginates in the weights in layer l = 2, chosen as\nWl(∆) = √ 1− |∆|W + √ |∆|dW. (33)\nHere both a base matrix W and a perturbation matrix dW have matrix elements that are zero mean i.i.d. Gaussians with variance σ2w. Each matrix element of W\n2(∆) thus also has variance σ2w just like all the other layers. In turn, this family of networks induces a family of functions hD(h1,∆). For simplicity, we restrict these functions to a simple input manifold, the circle,\nh1(θ) = √ N1q∗ [ u0 cos(θ) + u1 sin(θ) ] , (34)\nas considered previously. This circle is at the fixed point radius q∗(σw, σb), and the family of networks induces a family of functions from the circle to the hidden representation space in layer l, namely RNl . We denote these functions by hl(θ,∆). How similar are these functions as ∆ changes? This can be quantified through the correlation in function space\nQl(∆1,∆2) ≡ ∫ dθ\n2π\n1\nND\nND∑\ni=1\nhli(θ,∆1)h l i(θ,∆2), (35)\nand the associated correlation coefficient,\nCl(∆) = Ql(0,∆)√\nQl(0, 0)Ql(∆,∆) . (36)\nBecause of our restriction to an input circle at the fixed point radius, Ql(0, 0) = Ql(∆,∆) = q∗ for all l and ∆ in the large width limit. By using logic similar to the derivation of (15), we can derive a recursion relation for the function space correlation Ql(0,∆):\nQl(0,∆) = σ2w ∫ Dz1Dz2 φ (u1)φ (u2) + σ2b , l = 3, . . . , D (37)\nQl(0,∆) = √ 1− |∆|σ2w ∫ Dz1Dz2 φ (u1)φ (u2) + σ2b , l = 2,\nu1 = √ q∗z1, u2 = √ q∗ [ Cl−1(∆)z1 + √ 1− (Cl−1(∆))2z2 ] ,\nwhere Cl(∆) = Ql(0,∆)/q∗. The initial condition for this recursion is C1(∆) = 1, since the family of functions in the first layer of inputs is independent of ∆. Now, the difference in weights at a nonzero ∆ reduces the function space correlation to C2(∆) < 1. At this point, the representation in h2 is different for the two networks at parameter values 0 and ∆. Moreover, in the chaotic regime, this difference will amplify due to the similarity between the function space evolution equation in (37) and the evolution equation for the similarity of two points in (15). In essence, just as two points in the input exponentially separate as they propagate through a single network in the chaotic regime, a pair of different functions separate when computed in the final layer. Thus a small perturbation in the weights into layer 2 can yield a very large change in the space of functions from the input manifold to layer D. Moreover, as ∆ varies from -1 to 1, the function hD(θ,∆) roughly undergoes a random walk in function space whose autocorrelation length decreases exponentially with depth D. This weight chaos, or a sensitive dependence of the function computed by a deep network with respect to weight changes far from the final layer, is another manifestation of deep neural expressivity. Our companion paper [16] further explores the expressivity of deep random networks in function space and also finds an exponential growth in expressivity with depth."
    } ],
    "references" : [ {
      "title" : "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing",
      "author" : [ "Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton" ],
      "venue" : null,
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2012
    }, {
      "title" : "Playing atari with deep reinforcement learning",
      "author" : [ "Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Alex Graves", "Ioannis Antonoglou", "Daan Wierstra", "Martin Riedmiller" ],
      "venue" : "arXiv preprint arXiv:1312.5602,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2013
    }, {
      "title" : "Deep speech: Scaling up end-to-end speech recognition",
      "author" : [ "Awni Hannun", "Carl Case", "Jared Casper", "Bryan Catanzaro", "Greg Diamos", "Erich Elsen", "Ryan Prenger", "Sanjeev Satheesh", "Shubho Sengupta", "Adam Coates" ],
      "venue" : "arXiv preprint arXiv:1412.5567,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2014
    }, {
      "title" : "Deep knowledge tracing",
      "author" : [ "Chris Piech", "Jonathan Bassen", "Jonathan Huang", "Surya Ganguli", "Mehran Sahami", "Leonidas J Guibas", "Jascha Sohl-Dickstein" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2015
    }, {
      "title" : "Representation learning: A review and new perspectives",
      "author" : [ "Yoshua Bengio", "Aaron Courville", "Pierre Vincent" ],
      "venue" : "Pattern Analysis and Machine Intelligence, IEEE Transactions on,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2013
    }, {
      "title" : "Untangling invariant object recognition",
      "author" : [ "James J DiCarlo", "David D Cox" ],
      "venue" : "Trends in cognitive sciences,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2007
    }, {
      "title" : "On the number of linear regions of deep neural networks",
      "author" : [ "Guido F Montufar", "Razvan Pascanu", "Kyunghyun Cho", "Yoshua Bengio" ],
      "venue" : "In Advances in neural information processing systems,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2014
    }, {
      "title" : "Shallow vs. deep sum-product networks",
      "author" : [ "Olivier Delalleau", "Yoshua Bengio" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2011
    }, {
      "title" : "The power of depth for feedforward neural networks",
      "author" : [ "Ronen Eldan", "Ohad Shamir" ],
      "venue" : "arXiv preprint arXiv:1512.03965,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2015
    }, {
      "title" : "Representation benefits of deep feedforward networks",
      "author" : [ "Matus Telgarsky" ],
      "venue" : "arXiv preprint arXiv:1509.08101,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2015
    }, {
      "title" : "On the representational efficiency of restricted boltzmann machines",
      "author" : [ "James Martens", "Arkadev Chattopadhya", "Toni Pitassi", "Richard Zemel" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2013
    }, {
      "title" : "On the complexity of neural network classifiers: A comparison between shallow and deep architectures",
      "author" : [ "Monica Bianchini", "Franco Scarselli" ],
      "venue" : "Neural Networks and Learning Systems, IEEE Transactions on,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2014
    }, {
      "title" : "Learning real and boolean functions: When is deep better than shallow",
      "author" : [ "Hrushikesh Mhaskar", "Qianli Liao", "Tomaso Poggio" ],
      "venue" : "arXiv preprint arXiv:1603.00988,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2016
    }, {
      "title" : "Riemannian manifolds: an introduction to curvature, volume 176",
      "author" : [ "John M Lee" ],
      "venue" : "Springer Science & Business Media,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2006
    }, {
      "title" : "Chaos in random neural networks",
      "author" : [ "Haim Sompolinsky", "A Crisanti", "HJ Sommers" ],
      "venue" : "Physical Review Letters,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 1988
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Deep feedforward neural networks, with multiple hidden layers, have achieved remarkable performance across many domains [1–4].",
      "startOffset" : 120,
      "endOffset" : 125
    }, {
      "referenceID" : 1,
      "context" : "Deep feedforward neural networks, with multiple hidden layers, have achieved remarkable performance across many domains [1–4].",
      "startOffset" : 120,
      "endOffset" : 125
    }, {
      "referenceID" : 2,
      "context" : "Deep feedforward neural networks, with multiple hidden layers, have achieved remarkable performance across many domains [1–4].",
      "startOffset" : 120,
      "endOffset" : 125
    }, {
      "referenceID" : 3,
      "context" : "Deep feedforward neural networks, with multiple hidden layers, have achieved remarkable performance across many domains [1–4].",
      "startOffset" : 120,
      "endOffset" : 125
    }, {
      "referenceID" : 4,
      "context" : "The second piece of intuition, which has captured the imagination of machine learning [5] and neuroscience [6] alike, is that deep neural networks can disentangle highly curved manifolds in input space into flattened manifolds in hidden space, to aid the performance of simple linear readouts.",
      "startOffset" : 86,
      "endOffset" : 89
    }, {
      "referenceID" : 5,
      "context" : "The second piece of intuition, which has captured the imagination of machine learning [5] and neuroscience [6] alike, is that deep neural networks can disentangle highly curved manifolds in input space into flattened manifolds in hidden space, to aid the performance of simple linear readouts.",
      "startOffset" : 107,
      "endOffset" : 110
    }, {
      "referenceID" : 6,
      "context" : "For the first intuition, seminal works have exhibited examples of particular functions that can be computed with a polynomial number of neurons (in the input dimension) in a deep network but require an exponential number of neurons in a shallow network [7–11].",
      "startOffset" : 253,
      "endOffset" : 259
    }, {
      "referenceID" : 7,
      "context" : "For the first intuition, seminal works have exhibited examples of particular functions that can be computed with a polynomial number of neurons (in the input dimension) in a deep network but require an exponential number of neurons in a shallow network [7–11].",
      "startOffset" : 253,
      "endOffset" : 259
    }, {
      "referenceID" : 8,
      "context" : "For the first intuition, seminal works have exhibited examples of particular functions that can be computed with a polynomial number of neurons (in the input dimension) in a deep network but require an exponential number of neurons in a shallow network [7–11].",
      "startOffset" : 253,
      "endOffset" : 259
    }, {
      "referenceID" : 9,
      "context" : "For the first intuition, seminal works have exhibited examples of particular functions that can be computed with a polynomial number of neurons (in the input dimension) in a deep network but require an exponential number of neurons in a shallow network [7–11].",
      "startOffset" : 253,
      "endOffset" : 259
    }, {
      "referenceID" : 10,
      "context" : "For the first intuition, seminal works have exhibited examples of particular functions that can be computed with a polynomial number of neurons (in the input dimension) in a deep network but require an exponential number of neurons in a shallow network [7–11].",
      "startOffset" : 253,
      "endOffset" : 259
    }, {
      "referenceID" : 6,
      "context" : "For example [7] focused on ReLu nonlinearities and number of linear regions as a complexity measure, while [8] focused on sum-product networks and the number of monomials as complexity measure, and [12] focused on Pfaffian nonlinearities and topological measures of complexity, like the sum of Betti numbers of a decision boundary.",
      "startOffset" : 12,
      "endOffset" : 15
    }, {
      "referenceID" : 7,
      "context" : "For example [7] focused on ReLu nonlinearities and number of linear regions as a complexity measure, while [8] focused on sum-product networks and the number of monomials as complexity measure, and [12] focused on Pfaffian nonlinearities and topological measures of complexity, like the sum of Betti numbers of a decision boundary.",
      "startOffset" : 107,
      "endOffset" : 110
    }, {
      "referenceID" : 11,
      "context" : "For example [7] focused on ReLu nonlinearities and number of linear regions as a complexity measure, while [8] focused on sum-product networks and the number of monomials as complexity measure, and [12] focused on Pfaffian nonlinearities and topological measures of complexity, like the sum of Betti numbers of a decision boundary.",
      "startOffset" : 198,
      "endOffset" : 202
    }, {
      "referenceID" : 12,
      "context" : "see [13] for an interesting analysis of a general class of compositional functions.",
      "startOffset" : 4,
      "endOffset" : 8
    }, {
      "referenceID" : 13,
      "context" : "The limits of prior theoretical techniques raise another central question: is there a unifying theoretical framework for deep neural expressivity that is simultaneously applicable to arbitrary nonlinearities, generic networks, and a natural, general measure of functional complexity? Here we attack both central problems of deep neural expressivity by combining a very different set of tools, namely Riemannian geometry [14] and dynamical mean field theory [15].",
      "startOffset" : 420,
      "endOffset" : 424
    }, {
      "referenceID" : 14,
      "context" : "The limits of prior theoretical techniques raise another central question: is there a unifying theoretical framework for deep neural expressivity that is simultaneously applicable to arbitrary nonlinearities, generic networks, and a natural, general measure of functional complexity? Here we attack both central problems of deep neural expressivity by combining a very different set of tools, namely Riemannian geometry [14] and dynamical mean field theory [15].",
      "startOffset" : 457,
      "endOffset" : 461
    }, {
      "referenceID" : 13,
      "context" : "To quantitatively understand the layer-wise growth of complexity of this manifold, it is useful to turn to concepts in Riemannian geometry [14].",
      "startOffset" : 139,
      "endOffset" : 143
    }, {
      "referenceID" : 13,
      "context" : "For any parameterization, an explicit expression for κ(θ) is given by κ(θ) = (v ·v)−3/2 √ (v · v)(a · a)− (v · a)2 [14].",
      "startOffset" : 115,
      "endOffset" : 119
    }, {
      "referenceID" : 13,
      "context" : "Furthermore, the Gauss metric is related to the extrinsic curvature and the Euclidean metric via the relation g(θ) = κ(θ)g(θ) [14].",
      "startOffset" : 126,
      "endOffset" : 130
    }, {
      "referenceID" : 13,
      "context" : "These principal curvatures arise as the eigenvalues of a normalized Hessian operator projected onto the tangent plane Tx∗M: H = ||~ ∇G||−1 2 P ∂ G ∂x∂xT P, where P = I− ∇̂G∇̂G is the projection operator onto Tx∗M and ∇̂G is the unit normal vector [14].",
      "startOffset" : 247,
      "endOffset" : 251
    } ],
    "year" : 2016,
    "abstractText" : "We combine Riemannian geometry with the mean field theory of high dimensional chaos to study the nature of signal propagation in generic, deep neural networks with random weights. Our results reveal an order-to-chaos expressivity phase transition, with networks in the chaotic phase computing nonlinear functions whose global curvature grows exponentially with depth but not width. We prove this generic class of deep random functions cannot be efficiently computed by any shallow network, going beyond prior work restricted to the analysis of single functions. Moreover, we formalize and quantitatively demonstrate the long conjectured idea that deep networks can disentangle highly curved manifolds in input space into flat manifolds in hidden space. Our theoretical analysis of the expressive power of deep networks broadly applies to arbitrary nonlinearities, and provides a quantitative underpinning for previously abstract notions about the geometry of deep functions.",
    "creator" : "LaTeX with hyperref package"
  }
}