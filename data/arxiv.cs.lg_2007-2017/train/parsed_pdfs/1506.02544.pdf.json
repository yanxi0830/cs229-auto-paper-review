{
  "name" : "1506.02544.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Learning with Group Invariant Features: A Kernel Perspective",
    "authors" : [ "Youssef Mroueh", "Stephen Voinea", "Tomaso Poggio" ],
    "emails" : [ "mroueh@us.ibm.com" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Encoding signals or building similarity kernels that are invariant to the action of a group is a key problem in unsupervised learning as it reduces the complexity of the learning task and mimics how our brain represents information invariantly to symmetries and various nuisance factors (change in lighting in image classification and pitch variation in speech recognition) [1, 2, 3, 4]. Convolutional neural networks [5, 6] achieve state of the art performance in many computer vision and speech recognition tasks but require a large amount of labeled examples as well as augmented data, where we reflect symmetries of the world through virtual examples [7, 8] obtained by transforming the training examples with identity-preserving transformations such as shearing, rotation, translation, etc. In this work, we adopt the approach of [1] where the representation of the signal is designed to reflect the invariant properties and model the world symmetries with group actions. The ultimate aim is to bridge unsupervised learning of invariant representations with invariant kernel methods, where we can easily address the statistical consistency and sample complexity questions, using tools from classical supervised learning [9, 10]. Indeed many invariant kernel methods and related invariant kernel networks have been proposed. We refer the reader to the related work section for a review (Section 5) and we start by showing how to accomplish this invariance through Haar-integration group-invariant kernels [11], and then show how random features derived from a memory based theory of invariances introduced in [1] approximate such a kernel."
    }, {
      "heading" : "1.1 Group Invariant Kernels",
      "text" : "We start by reviewing Haar-integration group-invariant kernels introduced in [11], and their use in a binary classification problem. This section highlights the conceptual advantages of such kernels as well as their practical inconvenience, putting into perspective the advantage of approximating such kernels with explicit and invariant random feature maps.\nar X\niv :1\n50 6.\n02 54\n4v 1\n[ cs\n.L G\n] 8\nJ un\nInvariant Haar-Integration Kernels. We consider a subset X of the hypersphere in d dimensions Sd−1. Let ρX be a measure on X . Consider a kernel k0 on X , such as a radial basis function kernel. Let G be a group acting on X , with a normalized Haar measure µ. G is assumed to be a compact and unitary group. Define an invariant kernel K between x, z ∈ X through Haar-integration [11] as follows:\nK(x, z) = ∫ G ∫ G k0(gx, g ′z)dµ(g)dµ(g′). (1)\nAs we are integrating over the entire group, it is easy to see that: K(g′x, gz) = K(x, z), ∀g, g′ ∈ G,∀x, z ∈ X . Hence the Haar-integration kernel is invariant to the group action. The symmetry of K is obvious. Moreover, if k0 is a positive definite kernel, it follows that K is positive definite as well [11]. One can see the Haar-integration kernel framework as another form of data augmentation, since we have to produce group transformed points in order to compute the kernel.\nInvariant Decision Boundary. Turning now to a pattern classification problem, we assume that we are given a training set S of labeled data points among two classes: S = {(xi, yi), i = 1 . . . N | xi ∈ X , yi ∈ Y = {±1}}. In order to learn a decision function f : X → Y , we minimize the following empirical risk induced by an L-Lipschitz and convex loss function V , with V ′(0) < 0 [12]: minf∈HK ÊV (f) := 1N ∑N i=1 V (yif(xi)), where we restrict f to belong to a hypothesis class induced by the invariant kernel K, the so called reproducing kernel hilbert space HK. The representer theorem [13] shows that the solution of such a problem, or the optimal decision boundary f∗N has the following form: f ∗ N (x) = ∑N i=1 α ∗ iK(x, xi). Since the kernel K is group-invariant it\nfollows that : f∗N (gx) = ∑N i=1 αiK(gx, xi) = ∑N i=1 αiK(x, xi) = f∗N (x), ∀g ∈ G. Hence the the decision boundary f∗is group-invariant as well, and we have: f∗N (gx) = f ∗ N (x),∀g ∈ G,∀x ∈ X .\nReduced Sample Complexity. We have shown that a group-invariant kernel induces a groupinvariant decision boundary, but how does this translate to the sample complexity of the learning algorithm? To answer this question, we will assume that the input set X has the following structure: X = X0 ∪ GX0, GX0 = {z|z = gx, x ∈ X0, g ∈ G/ {e}}, where e is the identity group element. This structure implies that for a function f in the invariant RKHSHK, we have:\n∀z ∈ GX0,∃ x ∈ X0,∃ g ∈ G such that, z = gx, and f(z) = f(x). Let ρy(x) = P(Y = y|x) be the label posteriors. We assume that ρy(gx) = ρy(x),∀g ∈ G,. This is a natural assumption since the label is unchanged given the group action. Assume that the set X is endowed with a measure ρX that is also group-invariant. Let f be the group-invariant decision function , i.e ( such as f∗N ∈ HK). Consider the expected risk induced by the loss V , EV (f), defined as follows:\nEV (f) = ∫ X ∑ y∈Y V (yf(x))ρy(x)ρX (x)dx, (2)\nEV (f) is a proxy to the misclassification risk [12]. Using the invariant properties of the function class and the data distribution we have by invariance of f , ρy , and ρ:\nEV (f) = ∫ X0 ∑ y∈Y V (yf(x))ρy(x)ρX (x)dx+ ∫ GX0 ∑ y∈Y V (yf(z))ρy(z)ρX (z)dz\n= ∫ G dµ(g) ∫ X0 ∑ y∈Y V (yf(gx))ρy(gx)ρX (x)dx\n= ∫ G dµ(g) ∫ X0 ∑ y∈Y V (yf(x))ρy(x)ρX (x)dx (By invariance of f , ρy , and ρ )\n= ∫ X0 ∑ y∈Y V (yf(x))ρy(x)ρX (x)dx.\nHence, given an invariant kernel to a group action that is identity preserving, it is sufficient to minimize the empirical risk on the core set X0, and it generalizes to samples in GX0. Let us imagine that X was finite, and of cardinality |X |, the cardinality of the core set X0 is a small fraction of the cardinality of X : |X0| = α|X |, where 0 < α < 1. Hence when we sample the training set from X0 the maximum size of the training set isN = α|X | << |X |, hence the reduction in the sample complexity."
    }, {
      "heading" : "1.2 Contributions",
      "text" : "We have just reviewed the Haar-integration group-invariant kernel setup. In summary, a groupinvariant kernel implies the existence of a decision function that is invariant to the group action, as well as a reduction in the sample complexity as we have to sample the training set in a reduced set, aka the core set X0. Kernel methods with Haar-integration kernels come at a very expensive computational price at both training and test time: computing the Kernel is computationally cumbersome as we have to integrate over the group and produce virtual examples by transforming points explicitly through the group action, moreover the training complexity of kernel methods scales cubicly in the sample size. Those practical considerations make the usefulness of such kernels very limited. The contributions of this paper are on three folds:\n1. We first show that a non linear random feature map Φ : X → RD derived from a memory based theory of invariances introduced in [1] induces an expected Haar-integration groupinvariant kernel K. For fixed points x, z ∈ X , we have: E 〈Φ(x),Φ(z)〉 = K(x, z), where K satisfies: K(gx, g′z) = K(x, z),∀g, g′ ∈ G, x, z ∈ X .\n2. We show a Johnson Lindenstrauss type result that holds uniformly on a set of N points that assess the concentration of this random feature map around its expected induced kernel. For sufficiently large D, we have 〈Φ(x),Φ(z)〉 ≈ K(x, z), uniformly on an N points set.\n3. We show that, with a linear model, an invariant decision function can be learned in this random feature space by sampling points from the core set X0 i.e: f∗N (x) ≈ 〈w∗,Φ(x)〉 , and generalizes to unseen points in GX0 and reducing the sample complexity. Moreover, we show that those features define a function space that approximates a dense subset of the Invariant RKHS, and assess the error rates of the empirical risk minimization using such random features.\n4. We demonstrate the validity of these claims on three datasets: text (artificial), vision (MNIST), and speech (TIDIGITS)."
    }, {
      "heading" : "2 From Group Invariant Kernels to Feature Maps",
      "text" : "In this paper we show that a random feature map based on I-theory [1]: Φ : X → RD approximates a Haar-integration group-invariant kernel K having the form given in Equation (1):\n〈Φ(x),Φ(z)〉 ≈ K(x, z). We start with some notation that will be useful for defining the feature map. Denote the cumulative distribution function of a random variable X by,\nFX(τ) = P(X ≤ τ), Fix x ∈ X , Let g ∈ G be a random variable drawn according to the normalized Haar measure µ and let t be a random template whose distribution will be defined latter. For s > 0, define the following truncated cumulative distribution function (CDF) of the dot product 〈x, gt〉:\nψ(x, t, τ) = Pg(〈x, gt〉 ≤ τ) = F〈x,gt〉(τ), τ ∈ [−s, s], x ∈ X ,\nLet ε ∈ (0, 1). We consider the following Gaussian vectors (sampling with rejection) for the templates t: t = n ∼ N ( 0, 1\nd Id\n) , if ‖n‖22 < 1 + ε, t =⊥ else .\nThe reason behind this sampling is to keep the range of 〈x, gt〉 under control: The squared norm ‖n‖22 will be bounded by 1 + ε with high probability by a classical concentration result (See proof of Theorem 1 for more details). The group being unitary and x ∈ Sd−1, we know that : | 〈x, gt〉 | ≤ ‖n‖ < √ 1 + ε ≤ 1 + ε, for ε ∈ (0, 1).\nRemark 1. We can also consider templates t, drawn uniformly on the unit sphere Sd−1. Uniform templates on the sphere can be drawn as follows:\nt = ν\n‖ν‖2 , ν ∼ N (0, Id),\nsince the norm of a gaussian vector is highly concentrated around its mean √ d, we can use the gaussian sampling with rejection. Results proved for gaussian templates (with rejection) will hold true for templates drawn at uniform on the sphere with different constants.\nDefine the following kernel function, Ks(x, z) = Et ∫ s −s ψ(x, t, τ)ψ(z, t, τ)dτ,\nwhere swill be fixed throughout the paper to be s = 1+ε since the gaussian sampling with rejection controls the dot product to be in that range. Let ḡ ∈ G. As the group is closed, we have ψ(t, ḡx, τ) = ∫ G\n1I〈gḡx,t〉≤τdµ(g) =∫ G\n1I〈gx,t〉≤τdµ(g) = ψ(t, x, τ) and hence K(gx, g′z) = K(x, z), for all g, g′ ∈ G. It is clear now that K is a group-invariant kernel. In order to approximate K, we sample |G| elements uniformly and independently form the group G, i.e. gi, i = 1 . . . |G|, and define the normalized empirical CDF :\nφ(x, t, τ) = 1\n|G| √ m |G|∑ i=1 1I〈git,x〉≤τ ,−s ≤ τ ≤ s.\nWe discretize the continuous threshold τ as follows:\nφ ( x, t, sk\nn\n) = √ s√\nnm|G| |G|∑ i=1 1I〈git,x〉≤k sn ,−n ≤ k ≤ n.\nWe sample m templates independently according to the Gaussian sampling with rejection, tj , j = 1 . . .m. We are now ready to define the random feature map Φ:\nΦ(x) = [ φ ( x, tj , sk\nn )] j=1...m,k=−n...n ∈ R(2n+1)×m.\nIt is easy to see that:\nlim n→∞ Et,g 〈Φ(x),Φ(z)〉R(2n+1)×m = limn→∞Et,g m∑ j=1 n∑ k=−n φ ( x, tj , sk n ) φ ( z, tj , sk n ) = Ks(x, z).\nIn Section 3 we study the geometric information captured by this kernel by stating explicitly the similarity it computes. Remark 2 (Efficiency of the representation). 1) The main advantage of such a feature map as outlined in [1], is that we store transformed templates in order to compute Φ, while if we needed to compute an invariant kernel of typeK (Equation (1)), we need to expliclitly transform the points. The latter is computationally expensive. Storing transformed templates and computing the signature Φ is much more efficient. It falls in the category of memory-based learning, and is biologically plausible [1]. 2) As |G|,m,n get large enough the feature map Φ, approximates a group-invariant Kernel, as we will see in next section."
    }, {
      "heading" : "3 An Equivalent Expected Kernel and a Uniform Concentration Result",
      "text" : "In this section we present our main results, with proofs given in the supplementary material . Theorem 1 shows that the random feature map Φ, defined in the previous section, corresponds in expectation to a group-invariant, Haar-integration kernel Ks(x, z). Moreover s−Ks(x, z) computes the average pairwise distance between all points in the orbits of x and z, where the orbit is defined as the collection of all group-transformed points of a given point x : Ox = {gx, g ∈ G}. Theorem 1 (Expectation). Let ε ∈ (0, 1) and x, z ∈ X . Define the distance dG between the orbits Ox and Oz:\ndG(x, z) = 1√ 2πd ∫ G ∫ G ‖gx− g′z‖2 dµ(g)dµ(g ′),\nand the group-invariant expected kernel\nKs(x, z) = lim n→∞ Et,g 〈Φ(x),Φ(z)〉R(2n+1)×m = Et ∫ s −s ψ(x, t, τ)ψ(z, t, τ)dτ, s = 1 + ε.\n1. The following inequality holds with probability 1:\nε− δ2(d, ε) ≤ Ks(x, z)− (1− dG(x, z)) ≤ ε+ δ1(d, ε), (3)\nwhere δ1(ε, d) = e −dε2/16 √ d − 12 e−εd/2(1+ε) d 2√ d and δ2(ε, δ) = e −dε2/16 √ d + (1 + ε)e−dε 2/8.\n2. For any ε ∈ (0, 1) as the dimension d → ∞ we have δ1(ε, d) → 0 and δ2(ε, d) → 0, and we have asymptotically Ks(x, z)→ 1− dG(x, z) + ε = s− dG(x, z).\n3. Ks is symmetric and Ks is positive semi definite. Remark 3. 1) ε, δ1(d, ε), and δ2(d, ε) are due to the truncation and are a technical artifact of the proof, and are not errors due to results holding with high probability. 2) Local invariance can be defined by restricting the sampling of the group elements to a subset G ⊂ G, and the equivalent kernel has asymptotically the following form:\nKs(x, z) ≈ s− 1√ 2πd ∫ G ∫ G ‖gx− g′z‖2 dµ(g)dµ(g ′).\n3) The norm-one constraint can be relaxed , let R = supx∈X ‖x‖2 < ∞, hence we can set s = R(1 + ε), and\n−δ2(d, ε) ≤ Ks(x, z)− (R(1 + ε)− dG(x, z)) ≤ δ1(d, ε), (4)\nwhere δ1(ε, d) = R e −dε2/16 √ d − R2 e−εd/2(1+ε) d 2√ d and δ2(ε, δ) = R e −dε2/16 √ d +R(1 + ε)e−dε 2/8.\nTheorem 2 is in a sense an invariant Johnson Lindenstrauss [14] type result where we show that the dot product defined by the random feature map Φ , i.e 〈Φ(x),Φ(z)〉 is concentrated around the invariant expected kernel, uniformly on a data set of N points, given a sufficiently large number of templates m, a large number of sampled group elements |G|, and a large bin number n. The error naturally decomposes to a numerical error ε0 and statistical errors ε1, ε2 due to the sampling of the templates and the group elements respectively. Theorem 2. [Johnson Lindenstrauss type Theorem- N point Set] Let D = {x1 . . . xN |xi ∈ X , i = 1 . . . N}, be a finite dataset. Fix ε0, ε1, ε2, δ1, δ2 ∈ (0, 1). For a number of bins n ≥ 1ε0 , a number of templates m ≥ C1\nε21 log(Nδ1 ), and a number of group elements |G| ≥ C2 ε22 log(Nmδ2 ), where C1, C2 are universal numeric constants, we have:\n|〈Φ(xi),Φ(xj)〉 −Ks(xi, xj)| ≤ ε0 + ε1 + ε2, i = 1 . . . N, j = 1 . . . N, (5) with probability 1− δ1 − δ2.\nPutting together Theorems 1 and 2, the following Corollary shows how the random, group-invariant feature map Φ captures the invariant distance between points uniformly on a dataset of N points. Corollary 1 (Invariant Features Maps and Distances between Orbits). Let D = {x1 . . . xN |xi ∈ X , i = 1 . . . N}, be a finite dataset. Fix ε0, δ ∈ (0, 1). For a number of bins n ≥ 3ε0 , a number of templates m ≥ 9C1\nε20 log(Nδ ), and a number of group elements |G| ≥ 9C2 ε20 log(Nmδ ), where C1, C2 are universal numeric constants, we have:\nε− δ2(d, ε)− ε0 ≤ 〈Φ(xi),Φ(xj)〉 − (1− dG(xi, xj)) ≤ ε0 + ε+ δ1(d, ε), (6) i = 1 . . . N, j = 1 . . . N , with probability 1− 2δ. Remark 4. Assuming that the templates are unitary and drawn form a general distribution p(t), the equivalent kernel has the following form:\nKs(x, z) = ∫ G ∫ G dµ(g)dµ(g′) (∫ s−max(〈x, gt〉 , 〈z, g′t〉)p(t)dt ) .\nIndeed when we use the gaussian sampling with rejection for the templates, the integral∫ max(〈x, gt〉 , 〈z, g′t〉)p(t)dt is asymptotically proportional to ∥∥∥g−1x− g′,−1z∥∥∥ 2\n. It is interesting to consider different distributions that are domain-specific for the templates and assess the number of the templates needed to approximate such kernels. It is also interesting to find the optimal templates that achieve the minimum distortion in equation 6, in a data dependent way, but we will address these points in future work."
    }, {
      "heading" : "4 Learning with Group Invariant Random Features",
      "text" : "In this section, we show that learning a linear model in the invariant, random feature space, on a training set sampled from the reduced core setX0, has a low expected risk, and generalizes to unseen test points generated from the distribution on X = X0 ∪ GX0. The architecture of the proof follows ideas from [15] and [16]. Recall that given an L-Lipschitz convex loss function V , our aim is to minimize the expected risk given in Equation (2). Denote the CDF by ψ(x, t, τ) = P(〈gt, x〉 ≤ τ), and the empirical CDF by ψ̂(x, t, τ) = 1|G| ∑|G| i=1 1I〈git,x〉≤τ . Let p(t) be the distribution of templates\nt. The RKHS defined by the invariant kernel Ks, Ks(x, z) = ∫ ∫ s −s ψ(x, t, τ)ψ(z, t, τ)p(t)dtdτ denotedHKs , is the completion of the set of all finite linear combinations of the form:\nf(x) = ∑ i αiKs(x, xi), xi ∈ X , αi ∈ R. (7)\nSimilarly to [16], we define the following infinite-dimensional function space: Fp = { f(x) = ∫ ∫ s −s w(t, τ)ψ(x, t, τ)dtdτ | sup τ,t |w(t, τ)| p(t) ≤ C } .\nLemma 1. Fp is dense in HKs . For f ∈ Fp we have EV (f) = ∫ X0 V (yf(x))ρy(x)dρX (x), where X0 is the reduced core set.\nSince Fp is dense in HKs , we can learn an invariant decision function in the space Fp, instead of learning in HKs . Let Ψ(x) = [ ψ̂ ( x, tj , sk n )] j=1...m,k=−n...n . Ψ, and Φ are equivalent up to constants. We will approximate the set Fp as follows:\nF̃ = f(x) = 〈w,Ψ(x)〉 = sn m∑ j=1 n∑ k=−n wj,kψ̂ ( x, tj , sk n ) , tj ∼ p, j = 1 . . .m | ‖w‖∞ ≤ C m  . Hence, we learn the invariant decision function via empirical risk minimization where we restrict the function to belong to F̃ , and the sampling in the training set is restricted to the core set X0. Note that with this function space we are regularizing for convenience the norm infinity of the weights but this can be relaxed in practice to a classical Tikhonov regularization. Theorem 3 (Learning with Group invariant features). Let S = {(xi, yi) | xi ∈ X0, yi ∈ Y, i = 1 . . . N}, a training set sampled from the core set X0. Let f∗N = arg minf∈F̃ ÊV (f) = 1 N ∑N i=1 V (yif(xi)).Fix δ > 0, then\nEV (f∗N ) ≤ min f∈Fp EV (f) + 2 1√ N\n( 4LsC + 2V (0) + LC √ 1\n2 log\n( 1\nδ\n))\n+ 2sLC√ m\n( 1 + √ 2 log ( 1\nδ\n)) + L ( 2sC√ |G| ( 1 + √ 2 log (m δ )) + 2sC n ) ,\nwith probability at least 1− 3δ on the training set and the choice of templates and group elements.\nThe proof of Theorem 3 is given in Appendix B. Theorem 3 shows that learning a linear model in the invariant random feature space defined by Φ (or equivalently Ψ), has a low expected risk. More importantly, this risk is arbitrarily close to the optimal risk achieved in an infinitedimensional class of functions, namely Fp. The training set is sampled from the reduced core set X0, and invariant learning generalizes to unseen test points generated from the distribution on X = X0 ∪ GX0, hence the reduction in the sample complexity. Recall that Fp is dense in the RKHS of the Haar-integration invariant Kernel, and so the expected risk achieved by a linear model in the invariant random feature space is not far from the one attainable in the invariant RKHS. Note that the error decomposes into two terms. The first, O( 1√\nN ), is statistical and it\ndepends on the training sample complexity N . The other is governed by the approximation error of functions Fp, with functions in F̃ , and depends on the number of templates m, number of group elements sampled |G|, the number of bins n, and has the following formO( 1√\nm )+O (√ logm |G| ) + 1n ."
    }, {
      "heading" : "5 Relation to Previous Work",
      "text" : "We now put our contributions in perspective by outlining some of the previous work on invariant kernels and approximating kernels with random features. Approximating Kernels. Several schemes have been proposed for approximating a non-linear kernel with an explicit non-linear feature map in conjunction with linear methods, such as the Nyström method [17] or random sampling techniques in the Fourier domain for translation-invariant kernels [15]. Our features fall under the random sampling techniques where, unlike previous work, we sample both projections and group elements to induce invariance with an integral representation. We note that the relation between random features and quadrature rules has been thoroughly studied in [18], where sharper bounds and error rates are derived, and can apply to our setting. Invariant Kernels. We focused in this paper on Haar-integration kernels [11], since they have an integral representation and hence can be represented with random features [18]. Other invariant kernels have been proposed: In [19] authors introduce transformation invariant kernels, but unlike our general setting, the analysis is concerned with dilation invariance. In [20], multilayer arccosine kernels are built by composing kernels that have an integral representation, but does not explicitly induce invariance. More closely related to our work is [21], where kernel descriptors are built for visual recognition by introducing a kernel view of histogram of gradients that corresponds in our case to the cumulative distribution on the group variable. Explicit feature maps are obtained via kernel PCA, while our features are obtained via random sampling. Finally the convolutional kernel network of [22] builds a sequence of multilayer kernels that have an integral representation, by convolution, considering spatial neighborhoods in an image. Our future work will consider the composition of Haar-integration kernels, where the convolution is applied not only to the spatial variable but to the group variable akin to [2]."
    }, {
      "heading" : "6 Numerical Evaluation",
      "text" : "In this paper, and specifically with Theorems 2 and 3, we showed that the random, group-invariant feature map Φ captures the invariant distance between points, and that learning a linear model trained in the invariant, random feature space will generalize well to unseen test points. In this section, we validate these claims through three experiments. For the claims of Theorem 2, we will use a nearest neighbor classifier, while for Theorem 3, we will rely on the regularized least squares (RLS) classifier, one of the simplest algorithms for supervised learning. While our proofs focus on norm-infinity regularization, RLS corresponds to Tikhonov regularization with square loss. Specifically, for performing T−way classification on a batch of N training points in Rd, summarized in the data matrix X ∈ RN×d and label matrix Y ∈ RN×T , RLS will perform the optimization, minW∈Rm×T { 1 N ||Y − Φ(X)W || 2 F + λ||W ||2F } , where || · ||F is the Frobenius norm, λ is the regularization parameter, and Φ is the feature map, which for the representation described in this paper will be a CDF pooling of the data projected onto group transformed random templates. All RLS experiments in this paper were completed with the GURLS toolbox [23]. The three datasets we explore are: Xperm (Figure 1): An artificial dataset consisting of all sequences of length 5 whose elements come from an alphabet of 8 characters. We want to learn a function which assigns a positive value to any sequence that contains a target set of characters (in our case, two of them) regardless of their position. Thus, the function label is globally invariant to permutation, and so we project our data onto all permuted versions of our random template sequences. MNIST (Figure 2): We seek local invariance to translation and rotation, and so all random templates are translated by up to 3 pixels in all directions and rotated between -20 and 20 degrees. TIDIGITS (Figure 3): We use a subset of TIDIGITS consisting of 326 speakers (men, women, children) reading the digits 0-9 in isolation, and so each datapoint is a waveform of a single word. We seek local invariance to pitch and speaking rate [25], and so all random templates are pitch shifted up and down by 400 cents and warped to play at half and double speed. The task is 10-way classification with on class-per-digit. See [24] for more detail."
    }, {
      "heading" : "A Proofs of Theorems 1 and 2",
      "text" : "Proof of Theorem 1. 1) Ks(x, z) = Et ∫ s −s Eg [ 1I〈x,gt〉≤τ ] Eg′ [ 1I〈z,g′t〉≤τ ] dτ\n= Et ∫ dµ(g)dµ(g′) ∫ s −s 1I〈x,gt〉≤τ1I〈x,g′t〉≤τdτ\n= ∫ dµ(g)dµ(g′)Et (s−max(〈x, gt〉 , 〈z, g′t〉)) .\nwhere the second equality is by Fubini theorem and the last one holds since for a, b ∈ [−s, s] :∫ s −s 1Ia≤τ1Ib≤τdτ = s−max(a, b).\nRecall that the sampling of t is the following for ε ∈ (0, 1) let : t = n ∼ N ( 0, 1\nd Id\n) , if ‖n‖22 < 1 + ε, t =⊥ else ,\nsince our group is unitary, x being norm one, and by virtue of this sampling the dot product |〈x, gt〉| ≤ ‖n‖2 ≤ √ 1 + ε ≤ 1 + ε . Hence 〈x, gt〉 ∈ [−(1 + ε), 1 + ε], and we can choose s = 1 + ε. Using again the fact the group is unitary and compact we have:\nKs(x, z) = ∫ dµ(g)dµ(g′)Et(s−max (〈 g−1x, t 〉 , 〈 g ′,−1z, t 〉) .\nNow using this particular sampling of templates we have:\nKs(x, z) = ∫ G ∫ G dµ(g)dµ(g′)En ( 1I‖n‖22<1+ε [ 1 + ε−max (〈 g−1x, n 〉 , 〈 g′−1z, n 〉)]) .\nLet Zx,z(n, g, g ′) = max (〈 g−1x, n 〉 , 〈 g′−1z, n 〉) ,\nIt follows that:\nKs(x, z) = ∫ G ∫ G dµ(g)dµ(g′)En ( 1I‖n‖22<1+ε [1 + ε− Zx,z(n, g, g ′)] )\n= (1 + ε)P(‖n‖22 < 1 + ε)− ∫ G ∫ G dµ(g)dµ(g′)En ( 1I‖n‖22<1+εZx,z(n, g, g ′) )\n= (1 + ε)P(‖n‖22 < 1 + ε)− ∫ G ∫ G dµ(g)dµ(g′)En ( (1− 1I‖n‖22≥1+ε)Zx,z(n, g, g ′) )\n= (1 + ε)P(‖n‖22 < 1 + ε)− ∫ G ∫ G dµ(g)dµ(g′)EnZx,z(n, g, g′)\n+ ∫ G ∫ G dµ(g)dµ(g′)En ( 1I‖n‖22≥1+εZx,z(n, g, g ′) )\n(8)\nWe are left with evaluating or bounding two expectations: I1 = EnZx,z(n, g, g′), and I2 = En ( 1I‖n‖22≥1+εZx,z(n, g, g ′) ) , that involve the maximum of correlated gaussian variables as we will see in the following.\nBy rotation invariance of Gaussians we have that 〈 g−1x, n 〉 , and 〈 g′−1z, n 〉 are two correlated\nrandom gaussian variables with correllation coefficient that we note by cos(θg,g′) = 〈 g−1x, g,−1z 〉 . Hence by a change of a basis we can write:〈 g−1x, n 〉 =\n1√ d u, 〈 g′−1z, n 〉 = 1√ d cos(θg,g′)u+ 1√ d √ 1− cos2(θg,g′)v\nwhere cos(θg,g′) = 〈 g−1x, g′−1z 〉 , and u, v ∼ N (0, 1) iids.\nHence,\nI1 = 1√ d Eu,v max\n( u, cos(θg,g′)u+ √ 1− cos2(θg,g′)v ) .\nThe following Lemma from [26] gives the expectation and the variance of the maximum of two gaussians with correllation coefficient ρ.\nLemma 2 (Mean and Variance of Maximum of Correlated Gaussians [26] ). Let X ∼ N (µX , σ2X) and Y ∼ N (µY , σ2Y ), two correlated gaussians with correllation coefficient ρ. Define φN (x) = 1√ 2π exp(−x2/2), and ΦN (y) = ∫ y −∞ φN (x)dx. Let a = √ σ2X + σ 2 Y − 2ρσXσY , and α = µX−µY a . The mean µZ and variance σ2Z of Z = max(X,Y ) are expressed analytically as follows:\nµZ = µXΦN (α) + µY ΦN (−α) + aφN (α). (9) σ2Z = ( σ2X + µ 2 X ) ΦN (α) + ( σ2Y + µ 2 Y ) ΦN (−α) + (µX + µY ) aφN (α)︸ ︷︷ ︸\nEZ2\n−µ2Z . (10)\nApplying Lemma 2 to our case (µX = µY = 0, σX = σY = 1, ρ = cos(θg,g′)). We have: a = √ 2(1− cos(θg,g′)) and α = 0.\nI1 = 1√ d aφN (0)\n= 1√ 2πd\n√ 2(1− cos(θg,g′))\n= 1√ 2πd\n∥∥g−1x− g′−1z∥∥ 2 . (11)\nWe turn now to I2 that we bound using Cauchy-Schwarz inequality: |I2| = ∣∣∣En (1I‖n‖22≥1+εZx,z(n, g, g′))∣∣∣\n≤ √ E(1I‖n‖22≥1+ε) √ E(Z2x,z(n, g, g ′))\n= √ P ( ‖n‖22 ≥ 1 + ε )√ E(Z2x,z(n, g, g ′)). (12)\nOn the first hand, applying again Lemma 2 (for EZ2) we have:\nE(Z2x,z(n, g, g ′) =\n1 d Eu,v\n( max ( u, cos(θg,g′)u+ √ 1− cos2(θg,g′)v ))2 = 1\nd (2ΦN (0))\n= 1\nd . (13)\nOn the other hand, note that ‖n‖22 has a (normalized) chi squared distribution with d degree of freedom χ2d , with mean 1 . The following Lemma gives upper bounds for the upper and lower tails of a chi square distribution.\nLemma 3 (χ2 tail bounds). Let X ∼ χ2k, a chi squared random variable with k degree of freedom. The following hold true for any ε ∈ (0, 1):\n• Upper Bound for the upper tail [27]: P (\n1 kX ≥ 1 + ε\n) ≤ e−kε2/8.\n• Upper Bound for the lower tail [28]: For all k ≥ 2, u ≥ k − 1 we have:\nP (X < u) ≤ 1− 1 2 exp\n( −1\n2 (u− k − (k − 2) log(u/k) + log(k))\n) .\nMore specifically for u = k(1 + ε) we have:\nP ( 1\nk X < 1 + ε\n) ≤ 1− 1\n2\ne−εk/2 (1 + ε) k−2 2\n√ k\n.\nApplying Lemma 3, for ‖n‖22. We have ‖n‖ 2 2 = 1 dX , where X ∼ χ 2 d, hence:\nP ( ‖n‖22 ≥ 1 + ε ) ≤ e−dε 2/8, (14)\nPutting together Equations (12),(14), (13) we have finally:\n|I2| ≤ e−dε 2/16\n√ d . (15)\nPutting together Equations (8), (11), and (15), and using upper and lower bounds for P(‖n‖22 < 1+ε) from Lemma 3:\nKs(x, z) ≤ (1 + ε)P(‖n‖22 < 1 + ε)− 1√ 2πd ∫ G ∫ G ∥∥g−1x− g′−1z∥∥ 2 dµ(g)dµ(g′) + e−dε 2/16 √ d\n≤ (1 + ε) ( 1− 1\n2\ne−εd/2 (1 + ε) d−2 2\n√ d\n) − 1√\n2πd ∫ G ∫ G ∥∥g−1x− g′−1z∥∥ 2 dµ(g)dµ(g′)\n+ e−dε 2/16\n√ d .\nKs(x, z) ≥ P(‖n‖22 < 1 + ε)− 1√ 2πd ∫ G ∫ G ∥∥g−1x− g′−1z∥∥ 2 dµ(g)dµ(g′)− e −dε2/16 √ d\n≥ (1 + ε) ( 1− e−dε 2/8 ) − 1√\n2πd ∫ G ∫ G ∥∥g−1x− g′−1z∥∥ 2 dµ(g)dµ(g′)− e −dε2/16 √ d .\nNoting by dG the integral and using that the group is compact and unitary:\ndG(x, z) = 1√ 2πd ∫ G ∫ G ∥∥g−1x− g′−1z∥∥ 2 dµ(g)dµ(g′)\n= 1√ 2πd ∫ G ∫ G ‖gx− g′z‖2 dµ(g)dµ(g ′).\nWe finally have:\n−e −dε2/16 √ d −(1+ε)e−dε 2/8 +ε ≤ Ks(x, z)−(1− dG(x, z)) ≤ e−dε 2/16 √ d − 1 2 e−εd/2 (1 + ε) d 2 √ d +ε. (16) For any ε ∈ (0, 1) , as the dimension d→∞, we have asymptotically:\nKs(x, z)→ 1− dG(x, z) + ε = s− dG(x, z).\n2) The symmetry of K is obvious. Let p(t) be the distribution of the templates t. Define the following weighted dot product: 〈f(x, ., .), g(z, ., .)〉 = ∫ t p(t) ∫ s −s dτf(x, t, τ)g(z, t, τ). Recall that:\nKs(x, z) =\n∫ p(t)dt ∫ s −s ψ(x, t, τ)ψ(z, t, τ)dτ\n= 〈ψ(x, ., .), ψ(z, ., .)〉 .\nHence K is symmetric and positive semidefinite.\nProof of Theorem 2. In the following we fix two points x and z in X and a random template t. Let Xj = ∫ s −s P(〈gtj , x〉 ≤ τ)P(〈gtj , z〉 ≤ τ)dτ , we have 0 ≤ Xj ≤ 2s, where s = 1 + ε. Recall that Ks(x, z) = 1 mEt( ∑m j=1Xj). By Hoeffding’s inequality we have:\nPt  ∣∣∣∣∣∣ 1m m∑ j=1 Xj −Ks(x, z) ∣∣∣∣∣∣ >  ≤ 2 exp ( −2m 2 (2s)2 )\nTurning now to the CDF ψ(x, t, τ) = P(〈gt, x〉 ≤ τ), and the empirical CDF ψ̂(x, t, τ) = 1 |G| ∑|G| i=1 1I〈git,x〉≤τ . By the theorem on convergence of the empirical CDF [29] (Theorem 4 given in Appendix D ) we have, for γ > 0:\nPg {\nsup τ ∣∣∣ψ̂(x, t, τ)− ψ(x, t, τ)∣∣∣ > γ} ≤ 2 exp(−2|G|γ2) Hence we have ∀τ ∈ [−s, s]:∣∣∣ψ̂(x, t, τ)− ψ(x, t, τ)∣∣∣ ≤ γ and ∣∣∣ψ̂(x, t, τ)− ψ(z, t, τ)∣∣∣ ≤ γ with a probability at least 1− 4 exp(−2|G|γ2). Define X = ∫ s −s ψ(x, t, τ)ψ(z, t, τ)dτ , X̂ = ∫ s −s ψ̂(x, t, τ)ψ̂(z, t, τ)dτ , and X̃ = (2s) n ∑n k=−n ψ̂(x, t, ks n )ψ̂(z, t, ks n ), choose 0 < γ < 1:\n|X̂ −X| = ∣∣∣∣∫ s −s ( ψ̂(x, t, τ)ψ̂(z, t, τ)− ψ(x, t, τ)ψ(z, t, τ) ) dτ ∣∣∣∣ =\n∣∣∣∣∫ s −s ( ψ̂(x, t, τ)− ψ(x, t, τ) + ψ(x, t, τ) )( ψ̂(z, t, τ)− ψ(z, t, τ) + ψ(z, t, τ) ) − ψ(x, t, τ)ψ(z, t, τ)dτ ∣∣∣∣ ≤ (2γ + γ2)2s ≤ 6sγ,\nwith probability 1 − 4 exp(−2|G|γ2). Define Xj = ∫ s −s ψ(x, tj , τ)ψ(z, tj , τ)dτ , X̂j =∫ s\n−s ψ̂(x, tj , τ)ψ̂(z, tj , τ)dτ , and X̃j = (2s) n ∑n k=−n ψ̂(x, tj , ks n )ψ̂(z, tj , ks n ), Then for all j =\n1 . . .m, we have |X̂j −Xj | ≤ 6sγ\nwith probability 1− 4m exp(−2|G|γ2)− 2 exp ( −2m 2 (2s)2 ) .\nNow we turn to the numerical approximation of the integra by a Riemann sum, we have for all j = 1 . . .m : ∣∣∣X̂j − X̃j∣∣∣ ≤ s\nn .\nHence the error decomposes in the following way:\n|〈Φ(x),Φ(z)〉 −Ks(x, z)| = ∣∣∣∣∣∣ 1m m∑ j=1 X̃j −Ks(x, z) ∣∣∣∣∣∣ = ∣∣∣∣∣∣  1 m m∑ j=1 X̃j − 1 m m∑ j=1 X̂j +  1 m m∑ j=1 X̂j − 1 m m∑ j=1 Xj +  1 m m∑ j=1 Xj −Ks(x, z)\n∣∣∣∣∣∣ ≤\n∣∣∣∣∣∣ 1m m∑ j=1 X̃j − 1 m m∑ j=1 X̂j ∣∣∣∣∣∣︸ ︷︷ ︸ Numerical Binning Error + ∣∣∣∣∣∣ 1m m∑ j=1 X̂j − 1 m m∑ j=1 Xj ∣∣∣∣∣∣︸ ︷︷ ︸ Group CDF Approximation Error + ∣∣∣∣∣∣ 1m m∑ j=1 Xj −Ks(x, z) ∣∣∣∣∣∣︸ ︷︷ ︸ Templates Concentration Error\n≤ s n + 6sγ + .\nwith probability 1− 4m exp(−2|G|γ2)− 2 exp ( −2m 2 (2s)2 ) . For this to hold on all pairs of points in a set of cardinality N we have:\n|〈Φ(xi),Φ(xj)〉 −K(xi, xj)| ≤ s\nn + 6sγ + , i = 1 . . . N, j = 1 . . . N, with probability 1− 4mN(N − 1) exp(−2|G|γ2)− 2N(N − 1) exp ( −m 2 2(s)2 ) . Hence we have for numerical constants C1, and C2, 0 < δ1, δ2 < 1, and 0 < ε0, ε1, ε2 < 1, for n ≥ sε0 , m ≥ C1 ε21 log(Nδ1 ),|G| ≥ C2 ε22 log(Nmδ2 ), :\n|〈Φ(xi),Φ(xj)〉 −Ks(xi, xj)| ≤ ε0 + ε1 + ε2, i = 1 . . . N, j = 1 . . . N, with probability 1− δ1 − δ2."
    }, {
      "heading" : "B Proof of Theorem 3",
      "text" : "Proof of Lemma 1. Our proof parallels similar proofs in [16]. Note that functions of the form (7) are dense inHK . f(x) = ∑ i αiKs(x, xi) = ∑ i αi ∫ ∫ s −s ψ(x, t, τ)ψ(xi, t, τ)p(t)dtdτ\n= ∫ ∫ s −s (p(t) ∑ i αiψ(xi, t, τ))ψ(x, t, τ)dtdτ. Let β(t, τ) = p(t) ∑ i αiψ(xi, t, τ), since 0 ≤\nψ(x, t, τ) ≤ 1, ∀x, t, τ , we have |β(t,τ)|p(t) ≤ ∑ i |αi| < ∞, since αi are finite. Hence f can be written in the form:\nf(x) = ∫ ∫ s −s β(t, τ)ψ(x, t, τ)dtdτ, sup τ,t |β(t, τ)| p(t) <∞,\nand f ∈ Fp.\nIn order to prove Theorem 3, we need some preliminary lemmas. The following Lemma assess the approximation of any function f ∈ Fp, by a certain f̃ ∈ F̃ .\nLemma 4 (F̃ Approximation of Fp). Let f be a function in Fp. Then for δ1, δ2 > 0, there exists a function f̃ ∈ F̃ such that:∥∥∥f̃ − f∥∥∥\nL2(X ,ρX ) ≤ 2sC√ m\n( 1 + √ 2 log ( 1\nδ1\n)) +\n2sC√ |G|\n( 1 + √ 2 log ( m\nδ2\n)) + 2sC\nn ,\nwith probability at least 1− δ1 − δ2.\nProof of Lemma 4. Let f ∈ Fp, f(x) = ∫ ∫ s −s w(t, τ)ψ(x, t, τ)dτdt.\nLet fj(x) = ∫ s −s w(tj ,τ) p(tj) ψ(x, tj , τ)dτ, f̂j(x) = ∫ s −s w(tj ,τ) p(tj)\nψ̂(x, tj , τ)dτ, and f̃j(x) = s n ∑n k=−n w(tj , ks n ) p(tj) ψ̂(x, tj , ks n ). We have the following: Et(fj) = f , and 1 mEt( ∑m j=1 fj) = f .\nConsider the Hilbert space L2(X , ρX ), with dot product: 〈f, g〉L2(X ,ρX ) = ∫ X f(x)g(x)dρX (x).\n||fj ||L2(X ,ρX ) = √∫ X ∫ s −s ∫ s −s w(tj , τ ′)w(tj , τ) p2(tj) ψ(x, tj , τ)ψ(x, tj , τ ′)dτdτ ′dρX (x) ≤ (2sC),\nFix δ1 > 0, applying Lemma 7 we have therefore with probability 1− δ1:∥∥∥∥∥∥ 1m m∑ j=1 fj − f ∥∥∥∥∥∥ L2(X ,ρX ) ≤ 2sC√ m ( 1 + √ 2 log ( 1 δ1 )) , (17)\nNow turn to: ∥∥∥∥∥∥ 1m m∑ j=1 (f̂j − fj) ∥∥∥∥∥∥ L2(X ,ρX ) ≤ 1√ m m∑ j=1 ∥∥∥f̂j − fj∥∥∥ L2(X ,ρX ) ,\n∥∥∥f̂j − fj∥∥∥2 = ∫ X ∫ s −s w2(tj , τ) p2(tj) (ψ̂(x, tj , τ)− ψ(x, tj , τ))2dτdρX (x)\n≤ C2 ∫ s −s ∥∥∥ψ̂(., tj , τ)− ψ(., tj , τ)∥∥∥2 L2(X ,ρX ) dτ.\nRecall that: ψ̂(x, t, τ) = 1|G| ∑|G| i=1 1I〈git,x〉≤τ , and ψ(x, t, τ) = Egψ̂(x, t, τ).\nClearly ∥∥1I〈.,gt〉≤τ∥∥L2(X ,ρX ) ≤ 1, hence applying again Lemma 7, for δ2 > 0 we have with probability 1− δ2: ∥∥∥ψ̂(., tj , τ)− ψ(., tj , τ)∥∥∥2 L2(X ,ρX ) ≤ 1 |G| ( 1 + √ 2 log ( 1 δ2 ))2 , It follows that: ∀j = 1 . . .m, ∥∥∥f̂j − fj∥∥∥ ≤ 2Cs√|G| ( 1 + √ 2 log ( 1 δ2 )) , with probability 1 −mδ2. Hence with probability 1−mδ2, we have:∥∥∥∥∥∥ 1m m∑ j=1 (f̂j − fj) ∥∥∥∥∥∥ L2(X ,ρX ) ≤ 2Cs√ |G| ( 1 + √ 2 log ( 1 δ2 )) . (18)\nand by the approximation of a Riemann sum we have that:∥∥∥∥∥∥ 1m m∑ j=1 (f̂j − f̃j) ∥∥∥∥∥∥ L2(X ,ρX ) ≤ 2sC n . (19)\nIt is clear that f̃ = 1m ∑m j=1 f̃j ∈ F̃ , hence, putting together equations (17),(18), and (19) we finally have:∥∥∥∥∥∥ 1m m∑ j=1 f̃j − f ∥∥∥∥∥∥ L2(X ,ρX ) ≤ ∥∥∥∥∥∥ 1m m∑ j=1 (f̃j − f̂j) ∥∥∥∥∥∥ L2(X ,ρX ) + ∥∥∥∥∥∥ 1m m∑ j=1 (f̂j − fj) ∥∥∥∥∥∥ L2(X ,ρX ) + ∥∥∥∥∥∥ 1m m∑ j=1 fj − f ∥∥∥∥∥∥ L2(X ,ρX )\n≤ 2sC n + 2Cs√ |G|\n( 1 + √ 2 log ( 1\nδ2\n)) +\n2sC√ m\n( 1 + √ 2 log ( 1\nδ1 )) with probability 1− δ1 −mδ2.\nThe following Lemma shows how the approximation of functions inFp, by functions in F̃ , translates to the expected Risk:\nLemma 5 (Bound on the Approximation Error). Let f ∈ Fp, fix δ1, δ2 > 0. There exists a function f̃ ∈ F̃ , such that:\nEV (f̃) ≤ EV (f) + 2sLC√ m\n( 1 + √ 2 log ( 1\nδ1\n)) + L ( 2sC√ |G| ( 1 + √ 2 log ( m δ2 )) + 2sC n ) ,\nwith probability at least 1− δ1 − δ2. Proof of Lemma 5. EV (f̃) − EV (f) ≤ ∫ X ∣∣∣V (yf̃(x))− V (yf(x))∣∣∣ dρX (x) ≤ L ∫X |f̃(x) − f(x)|dρX (x) ≤ L √∫ X (f̃(x)− f(x))2dρX (x) = L ∥∥∥f̃ − f∥∥∥ L2(X ,ρX ) , where we used the Lipschitz condition and Jensen inequality. The rest of the proof follows from Lemma 4.\nThe following Lemma gives a bound on the estimation of the expected Risk with finite training samples:\nLemma 6 (Bound on the Estimation Error). Fix δ > 0, then\nsup f∈F̃ ∣∣∣EV (f)− ÊV (f)∣∣∣ ≤ 1√ N ( 4LsC + 2V (0) + LC √ 1 2 log ( 1 δ )) ,\nwith probability 1− δ.\nProof. The proof follows from Theorem 5 given in Appendix D. It is sufficient to bound the Rademacher complexity of the class F̃ :\nRN (F̃) = Ex,σ [ sup f∈F̃ ∣∣∣∣∣ 1N N∑ i=1 σif(xi) ∣∣∣∣∣ ] = Ex,σ sup f∈F̃ ∣∣∣∣∣∣ sNn N∑ i=1 σi  m∑ j=1 n∑ k=−n wj,kψ̂ ( xi, tj , sk n )∣∣∣∣∣∣ \n= Ex,σ sup f∈F̃ ∣∣∣∣∣∣ sNn m∑ j=1 n∑ k=−n wj,k N∑ i=1 σiψ̂ ( xi, tj , sk n )∣∣∣∣∣∣ \n≤ Ex,σ sC\nmNn m∑ j=1 n∑ k=−n ∣∣∣∣∣ N∑ i=1 σiψ̂ ( xi, tj , sk n )∣∣∣∣∣ By Holder inequality: 〈a, b〉 ≤ ‖a‖∞ ‖b‖1 ≤ sC mNn Ex m∑ j=1 n∑ k=−n √√√√Eσ ( N∑ i=1 σiψ̂ ( xi, tj , sk n ))2 Jensen inequality, concavity of square root\nNote that E(σiσj) = 0, for i 6= j it follows that: Eσ (∑N i=1 σiψ̂ ( xi, tj , sk n ))2 = Eσ ∑N i=1 ∑N `=1 σiσ`ψ̂ ( xi, tj , sk n ) ψ̂ ( x`, tj , sk n ) =∑N\ni=1 ψ̂ 2 ( xi, tj , sk n ) ≤ N , since ψ̂(., ., .) ≤ 1. Finally:\nRm(F̃) ≤ Cs√ N .\nWe are now ready to prove Theorem 3:\nProof of Theorem 3. Let f∗N = arg minf∈F̃ ÊV (f), f̃ = arg minf∈F̃ EV (f), fp = arg minf∈Fp EV (f).\nEV (f∗N )− min f∈Fp\nEV (f) = ( EV (f∗N )− EV (f̃) ) ︸ ︷︷ ︸\nStatistical Error\n+ ( EV (f̃)− EV (fp) ) ︸ ︷︷ ︸\nApproximation Error\nThe first term is the usual estimation or statistical error than we can bound using Lemma 6, we have: EV (f∗N )− EV (f̃) = ( EV (f∗N )− ÊV (f∗N ) ) + ( ÊV (f∗N )− ÊV (f̃) ) ︸ ︷︷ ︸ ≤0,by optimality of f∗N + ( ÊV (f̃)− EV (f̃) )\n≤ 2 sup f∈F̃ ∣∣∣EV (f)− ÊV (f)∣∣∣ ≤ 2 1√\nN\n( 4LsC + 2V (0) + LC √ 1\n2 log\n( 1\nδ\n)) ,\nwith probability 1 − δ over the training samples. Let f̃p, the function defined in Lemma 4, that approximates fp in F̃ . By Lemma 5 we know that:\nEV (f̃p) ≤ EV (fp) + 2sLC√ m\n( 1 + √ 2 log ( 1\nδ1\n)) + L ( 2sC√ |G| ( 1 + √ 2 log ( m δ2 )) + 2sC n ) ,\nwith probability 1 − δ1 − δ2, on the choice of the templates and the sampled group elements. By optimality of f̃ ∈ F̃ , we have\nEV (f̃) ≤ EV (f̃p) ≤ EV (fp)+ 2sLC√ m\n( 1 + √ 2 log ( 1\nδ1\n)) +L ( 2sC√ |G| ( 1 + √ 2 log ( m δ2 )) + 2sC n ) Hence by a union bound with probability 1− δ− δ1 − δ2, on the training set , the templates and the group elements we have:\nEV (f∗N )− min f∈Fp EV (f) ≤ 2 1√ N\n( 4LsC + 2V (0) + LC √ 1\n2 log\n( 1\nδ\n))\n+ 2sLC√ m\n( 1 + √ 2 log ( 1\nδ1\n)) + L ( 2sC√ |G| ( 1 + √ 2 log ( m δ2 )) + 2sC n ) ."
    }, {
      "heading" : "C Technical tools",
      "text" : "Theorem 4. [29] Let X1, X2, ..., Xm be i.i.d. random variables with cumulative distribution function F , and let F̂m be the associated empirical cumulative density function F̂m = 1m ∑m i=1 1IXi≤τ . Then for any γ > 0\nP {\nsup τ ∣∣∣F̂m(τ)− F (τ)∣∣∣ > γ} ≤ 2 exp (−2mγ2) . Lemma 7 ([15],Concentration of the mean of bounded random variables in a Hilbert Space). Let (H, 〈., .〉H) be a Hilbert space. Let Xj , j = 1 . . .K, be iid random, such that ||Xj ||H ≤ M . Then for any δ > 0, with probability 1− δ,∥∥∥∥∥∥ 1K K∑ j=1 Xj − 1 K E K∑ j=1 Xj ∥∥∥∥∥∥ H ≤ M√ K ( 1 + √ 2 log ( 1 δ )) .\nTheorem 5 ([15]). Let F be a bounded class of function, supx∈X |f(x)| ≤ C for all f ∈ F . Let V be an L-Lipschitz loss. Then with probability 1 − δ, with respect to training samples {xi, yi}i=1...N ,every f satisfies:\nEV (f) ≤ ÊV (f) + 4LRN (F) + 2V (0)√ N + LC\n√ 1\n2N log\n1 δ ,\nwhereRN (F) is the Rademacher complexity of the class F:\nRN (F) = Ex,σ [ sup f∈F ∣∣∣∣∣ 1N N∑ i=1 σif(xi) ∣∣∣∣∣ ] ,\nthe variables σi are iid symmetric Bernoulli random variables taking value in {−1, 1}, with equal probability and are independent form xi."
    }, {
      "heading" : "D Numerical Evaluation",
      "text" : "D.1 Permutation Invariance Experiment\nFor our first experiment, we created an artificial dataset which was designed to exploit permutation invariance, providing us with a finite group to which we had complete access. The dataset Xperm consists of all sequences of length L = 5, where each element of the sequence is taken from an alphabet A of 8 characters, giving us a total of 32,768 data points. Two characters c1, c2 ∈ A were randomly chosen and designated as targets, so that a sequence x ∈ Xperm is labeled positive if it contains both c1 and c2, where the position of these characters in the sequence does not matter.\nLikewise, any sequence that does not contain both characters is labeled negative. This provides us with a binary classification problem (positive sequences vs. negative sequences), for which the label is preserved by permutations of the sequence indices, i.e. two sequences will belong to the same orbit if and only if they are permuted versions of one another. The ith character in A is encoded as an 8-dimensional vector which is 0 in every position but the ith, where it is 1. Each sequence x ∈ Xperm is formed by concatenating the 5 such vectors representing its characters, resulting in a binary vector of length 40. To build the permutation-invariant representation, we project a binary sequences onto an equal-length sequence consisting of standard-normal gaussian vectors, as well as all of its permutations, and then pool over the projections with a CDF. As a baseline, we also used a bag-of-words representation, where each x ∈ Xperm was encoded with an 8-dimensional vector with ith element equal to the count of how many times character i appears in x. Note that this representation is also invariant to permutations, and so should share many of the benefits of our feature map. For all classification results, 4000 points were randomly chosen fromXperm to form the training set, with an even split of 2000 positive points and 2000 negative points. The remaining 28,768 points formed the test set. We know from Theorem 3 that the expected risk is dependent on the number of templates used to encode our data and on the number of bins used in the CDF-pooling step. The right panel of Figure 1 shows RLS classification accuracy on Xperm for different numbers of templates and bins. We see that, for a fixed number of templates, increasing the number of bins will improve accuracy, and for a fixed number of bins, adding more templates will improve accuracy. We also know there is a further dependence on the number of transformation samples from the group G. The left panel of figure 1 shows how classification accuracy, for a fixed number of training points, bins, and templates, depends on the number of transformation we have access to. We see the curve is rather flat, and there is a very graceful degradation in performance. In Figure 2, we include the sample complexity plot (for RLS) with the error bars added.\nD.2 TIDIGITS Experiment\nHere, we add plots showing performance as a function of number of templates and bins for some other splits of the TIDIGITS data."
    } ],
    "references" : [ {
      "title" : "Unsupervised learning of invariant representations in hierarchical architectures",
      "author" : [ "F. Anselmi", "J.Z. Leibo", "L. Rosasco", "J. Mutch", "A. Tacchetti", "T. Poggio" ],
      "venue" : "CoRR, vol. abs/1311.4158, 2013.",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Invariant scattering convolution networks",
      "author" : [ "J. Bruna", "S. Mallat" ],
      "venue" : "CoRR, vol. abs/1203.1513, 2012.",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Representation learning: A review and new perspectives",
      "author" : [ "Y. Bengio", "A.C. Courville", "P. Vincent" ],
      "venue" : "IEEE Trans. Pattern Anal. Mach. Intell., vol. 35, no. 8, pp. 1798–1828, 2013.",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 1828
    }, {
      "title" : "Gradient-based learning applied to document recognition",
      "author" : [ "Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner" ],
      "venue" : "Proceedings of the IEEE, vol. 86, pp. 2278–2324, 1998.",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "Imagenet classification with deep convolutional neural networks",
      "author" : [ "A. Krizhevsky", "I. Sutskever", "G.E. Hinton" ],
      "venue" : "NIPS, pp. 1106–1114, 2012.",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Incorporating prior information in machine learning by creating virtual examples",
      "author" : [ "P. Niyogi", "F. Girosi", "T. Poggio" ],
      "venue" : "Proceedings of the IEEE, pp. 2196–2209, 1998.",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "Learning from hints in neural networks",
      "author" : [ "Y.-A. Mostafa" ],
      "venue" : "Journal of complexity, vol. 6, pp. 192–198, June 1990.",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 1990
    }, {
      "title" : "Statistical learning theory",
      "author" : [ "V.N. Vapnik" ],
      "venue" : "A Wiley-Interscience Publication",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 1998
    }, {
      "title" : "Support vector machines",
      "author" : [ "I. Steinwart", "A. Christmann" ],
      "venue" : "Information Science and Statistics,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2008
    }, {
      "title" : "Invariance in kernel methods by haar-integration kernels",
      "author" : [ "B. Haasdonk", "A. Vossen", "H. Burkhardt" ],
      "venue" : "SCIA , Springer, 2005.",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Convexity, classification, and risk bounds",
      "author" : [ "P.L. Bartlett", "M.I. Jordan", "J.D. McAuliffe" ],
      "venue" : "Journal of the American Statistical Association, vol. 101, no. 473, pp. 138–156, 2006.",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Spline models for observational data, vol. 59 of CBMS-NSF",
      "author" : [ "G. Wahba" ],
      "venue" : "Regional Conference Series in Applied Mathematics. Philadelphia, PA: SIAM,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 1990
    }, {
      "title" : "Extensions of lipschitz mappings into a hilbert space",
      "author" : [ "W.B. Johnson", "J. Lindenstrauss" ],
      "venue" : "Conference in modern analysis and probability, 1984.",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 1984
    }, {
      "title" : "Weighted sums of random kitchen sinks: Replacing minimization with randomization in learning",
      "author" : [ "A. Rahimi", "B. Recht" ],
      "venue" : "NIPS 2008.",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Uniform approximation of functions with random bases",
      "author" : [ "A. Rahimi", "B. Recht" ],
      "venue" : "Proceedings of the 46th Annual Allerton Conference, 2008.",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Using the nystrm method to speed up kernel machines",
      "author" : [ "C. Williams", "M. Seeger" ],
      "venue" : "NIPS, 2001.",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "On the equivalence between quadrature rules and random features",
      "author" : [ "F.R. Bach" ],
      "venue" : "CoRR, vol. abs/1502.06800, 2015.",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2015
    }, {
      "title" : "Learning with transformation invariant kernels",
      "author" : [ "C. Walder", "O. Chapelle" ],
      "venue" : "NIPS, 2007.",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Kernel methods for deep learning",
      "author" : [ "Y. Cho", "L.K. Saul" ],
      "venue" : "NIPS, pp. 342–350, 2009.",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Kernel descriptors for visual recognition",
      "author" : [ "L. Bo", "X. Ren", "D. Fox" ],
      "venue" : "NIPS., 2010.",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Convolutional kernel networks",
      "author" : [ "J. Mairal", "P. Koniusz", "Z. Harchaoui", "C. Schmid" ],
      "venue" : "NIPS, 2014.",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Gurls: a least squares library for supervised learning",
      "author" : [ "A. Tacchetti", "P.K. Mallapragada", "M. Santoro", "L. Rosasco" ],
      "venue" : "CoRR, vol. abs/1303.0934, 2013.",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "Word-level invariant representations from acoustic waveforms",
      "author" : [ "S. Voinea", "C. Zhang", "G. Evangelopoulos", "L. Rosasco", "T. Poggio" ],
      "venue" : "vol. 14, pp. 3201–3205, September 2014.",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Automatic speech recognition and speech variability: A review",
      "author" : [ "M. Benzeghiba", "R. De Mori", "O. Deroo", "S. Dupont", "T. Erbes", "D. Jouvet", "L. Fissore", "P. Laface", "A. Mertins", "C. Ris", "R. Rose", "V. Tyagi", "C. Wellekens" ],
      "venue" : "Speech Communication, vol. 49, pp. 763–786, 01 2007.",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "The greatest of a finite set of random variables",
      "author" : [ "C.E. Clark" ],
      "venue" : "Operations Research, vol. 9, pp. 145–162, Mar-Apr 1961.",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 1961
    }, {
      "title" : "Introduction to the non-asymptotic analysis of random matrices",
      "author" : [ "R. Vershynin" ],
      "venue" : "Compressed Sensing: Theory and Applications, Y. Eldar and G. Kutyniok, Eds. Cambridge University Press., 2011.",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Inequalities for quantiles of the chi-square distribution",
      "author" : [ "T.Inglot" ],
      "venue" : "Probability and Mathematical Statistics, vol. 30(2):339351, 2010.",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Asymptotic minimax character of the sample distribution function and of the classical multinomial estimator",
      "author" : [ "A. Dvoretzky", "J. Kiefer", "J. Wolfowitz" ],
      "venue" : "Ann. Math. Statist., vol. 27, pp. 642–669, 09 1956. 20",
      "citeRegEx" : "29",
      "shortCiteRegEx" : null,
      "year" : 1956
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "We analyze in this paper a random feature map based on a theory of invariance (I-theory) introduced in [1].",
      "startOffset" : 103,
      "endOffset" : 106
    }, {
      "referenceID" : 0,
      "context" : "Encoding signals or building similarity kernels that are invariant to the action of a group is a key problem in unsupervised learning as it reduces the complexity of the learning task and mimics how our brain represents information invariantly to symmetries and various nuisance factors (change in lighting in image classification and pitch variation in speech recognition) [1, 2, 3, 4].",
      "startOffset" : 374,
      "endOffset" : 386
    }, {
      "referenceID" : 1,
      "context" : "Encoding signals or building similarity kernels that are invariant to the action of a group is a key problem in unsupervised learning as it reduces the complexity of the learning task and mimics how our brain represents information invariantly to symmetries and various nuisance factors (change in lighting in image classification and pitch variation in speech recognition) [1, 2, 3, 4].",
      "startOffset" : 374,
      "endOffset" : 386
    }, {
      "referenceID" : 2,
      "context" : "Encoding signals or building similarity kernels that are invariant to the action of a group is a key problem in unsupervised learning as it reduces the complexity of the learning task and mimics how our brain represents information invariantly to symmetries and various nuisance factors (change in lighting in image classification and pitch variation in speech recognition) [1, 2, 3, 4].",
      "startOffset" : 374,
      "endOffset" : 386
    }, {
      "referenceID" : 3,
      "context" : "Convolutional neural networks [5, 6] achieve state of the art performance in many computer vision and speech recognition tasks but require a large amount of labeled examples as well as augmented data, where we reflect symmetries of the world through virtual examples [7, 8] obtained by transforming the training examples with identity-preserving transformations such as shearing, rotation, translation, etc.",
      "startOffset" : 30,
      "endOffset" : 36
    }, {
      "referenceID" : 4,
      "context" : "Convolutional neural networks [5, 6] achieve state of the art performance in many computer vision and speech recognition tasks but require a large amount of labeled examples as well as augmented data, where we reflect symmetries of the world through virtual examples [7, 8] obtained by transforming the training examples with identity-preserving transformations such as shearing, rotation, translation, etc.",
      "startOffset" : 30,
      "endOffset" : 36
    }, {
      "referenceID" : 5,
      "context" : "Convolutional neural networks [5, 6] achieve state of the art performance in many computer vision and speech recognition tasks but require a large amount of labeled examples as well as augmented data, where we reflect symmetries of the world through virtual examples [7, 8] obtained by transforming the training examples with identity-preserving transformations such as shearing, rotation, translation, etc.",
      "startOffset" : 267,
      "endOffset" : 273
    }, {
      "referenceID" : 6,
      "context" : "Convolutional neural networks [5, 6] achieve state of the art performance in many computer vision and speech recognition tasks but require a large amount of labeled examples as well as augmented data, where we reflect symmetries of the world through virtual examples [7, 8] obtained by transforming the training examples with identity-preserving transformations such as shearing, rotation, translation, etc.",
      "startOffset" : 267,
      "endOffset" : 273
    }, {
      "referenceID" : 0,
      "context" : "In this work, we adopt the approach of [1] where the representation of the signal is designed to reflect the invariant properties and model the world symmetries with group actions.",
      "startOffset" : 39,
      "endOffset" : 42
    }, {
      "referenceID" : 7,
      "context" : "The ultimate aim is to bridge unsupervised learning of invariant representations with invariant kernel methods, where we can easily address the statistical consistency and sample complexity questions, using tools from classical supervised learning [9, 10].",
      "startOffset" : 248,
      "endOffset" : 255
    }, {
      "referenceID" : 8,
      "context" : "The ultimate aim is to bridge unsupervised learning of invariant representations with invariant kernel methods, where we can easily address the statistical consistency and sample complexity questions, using tools from classical supervised learning [9, 10].",
      "startOffset" : 248,
      "endOffset" : 255
    }, {
      "referenceID" : 9,
      "context" : "We refer the reader to the related work section for a review (Section 5) and we start by showing how to accomplish this invariance through Haar-integration group-invariant kernels [11], and then show how random features derived from a memory based theory of invariances introduced in [1] approximate such a kernel.",
      "startOffset" : 180,
      "endOffset" : 184
    }, {
      "referenceID" : 0,
      "context" : "We refer the reader to the related work section for a review (Section 5) and we start by showing how to accomplish this invariance through Haar-integration group-invariant kernels [11], and then show how random features derived from a memory based theory of invariances introduced in [1] approximate such a kernel.",
      "startOffset" : 284,
      "endOffset" : 287
    }, {
      "referenceID" : 9,
      "context" : "1 Group Invariant Kernels We start by reviewing Haar-integration group-invariant kernels introduced in [11], and their use in a binary classification problem.",
      "startOffset" : 103,
      "endOffset" : 107
    }, {
      "referenceID" : 9,
      "context" : "Define an invariant kernel K between x, z ∈ X through Haar-integration [11] as follows: K(x, z) = ∫",
      "startOffset" : 71,
      "endOffset" : 75
    }, {
      "referenceID" : 9,
      "context" : "Moreover, if k0 is a positive definite kernel, it follows that K is positive definite as well [11].",
      "startOffset" : 94,
      "endOffset" : 98
    }, {
      "referenceID" : 10,
      "context" : "In order to learn a decision function f : X → Y , we minimize the following empirical risk induced by an L-Lipschitz and convex loss function V , with V ′(0) < 0 [12]: minf∈HK ÊV (f) := 1 N ∑N i=1 V (yif(xi)), where we restrict f to belong to a hypothesis class induced by the invariant kernel K, the so called reproducing kernel hilbert space HK.",
      "startOffset" : 162,
      "endOffset" : 166
    }, {
      "referenceID" : 11,
      "context" : "The representer theorem [13] shows that the solution of such a problem, or the optimal decision boundary f∗ N has the following form: f ∗ N (x) = ∑N i=1 α ∗ iK(x, xi).",
      "startOffset" : 24,
      "endOffset" : 28
    }, {
      "referenceID" : 10,
      "context" : "EV (f) is a proxy to the misclassification risk [12].",
      "startOffset" : 48,
      "endOffset" : 52
    }, {
      "referenceID" : 0,
      "context" : "We first show that a non linear random feature map Φ : X → R derived from a memory based theory of invariances introduced in [1] induces an expected Haar-integration groupinvariant kernel K.",
      "startOffset" : 125,
      "endOffset" : 128
    }, {
      "referenceID" : 0,
      "context" : "2 From Group Invariant Kernels to Feature Maps In this paper we show that a random feature map based on I-theory [1]: Φ : X → R approximates a Haar-integration group-invariant kernel K having the form given in Equation (1): 〈Φ(x),Φ(z)〉 ≈ K(x, z).",
      "startOffset" : 113,
      "endOffset" : 116
    }, {
      "referenceID" : 0,
      "context" : "1) The main advantage of such a feature map as outlined in [1], is that we store transformed templates in order to compute Φ, while if we needed to compute an invariant kernel of typeK (Equation (1)), we need to expliclitly transform the points.",
      "startOffset" : 59,
      "endOffset" : 62
    }, {
      "referenceID" : 0,
      "context" : "It falls in the category of memory-based learning, and is biologically plausible [1].",
      "startOffset" : 81,
      "endOffset" : 84
    }, {
      "referenceID" : 12,
      "context" : "Theorem 2 is in a sense an invariant Johnson Lindenstrauss [14] type result where we show that the dot product defined by the random feature map Φ , i.",
      "startOffset" : 59,
      "endOffset" : 63
    }, {
      "referenceID" : 13,
      "context" : "The architecture of the proof follows ideas from [15] and [16].",
      "startOffset" : 49,
      "endOffset" : 53
    }, {
      "referenceID" : 14,
      "context" : "The architecture of the proof follows ideas from [15] and [16].",
      "startOffset" : 58,
      "endOffset" : 62
    }, {
      "referenceID" : 14,
      "context" : "Similarly to [16], we define the following infinite-dimensional function space: Fp = { f(x) = ∫ ∫ s",
      "startOffset" : 13,
      "endOffset" : 17
    }, {
      "referenceID" : 15,
      "context" : "Several schemes have been proposed for approximating a non-linear kernel with an explicit non-linear feature map in conjunction with linear methods, such as the Nyström method [17] or random sampling techniques in the Fourier domain for translation-invariant kernels [15].",
      "startOffset" : 176,
      "endOffset" : 180
    }, {
      "referenceID" : 13,
      "context" : "Several schemes have been proposed for approximating a non-linear kernel with an explicit non-linear feature map in conjunction with linear methods, such as the Nyström method [17] or random sampling techniques in the Fourier domain for translation-invariant kernels [15].",
      "startOffset" : 267,
      "endOffset" : 271
    }, {
      "referenceID" : 16,
      "context" : "We note that the relation between random features and quadrature rules has been thoroughly studied in [18], where sharper bounds and error rates are derived, and can apply to our setting.",
      "startOffset" : 102,
      "endOffset" : 106
    }, {
      "referenceID" : 9,
      "context" : "We focused in this paper on Haar-integration kernels [11], since they have an integral representation and hence can be represented with random features [18].",
      "startOffset" : 53,
      "endOffset" : 57
    }, {
      "referenceID" : 16,
      "context" : "We focused in this paper on Haar-integration kernels [11], since they have an integral representation and hence can be represented with random features [18].",
      "startOffset" : 152,
      "endOffset" : 156
    }, {
      "referenceID" : 17,
      "context" : "Other invariant kernels have been proposed: In [19] authors introduce transformation invariant kernels, but unlike our general setting, the analysis is concerned with dilation invariance.",
      "startOffset" : 47,
      "endOffset" : 51
    }, {
      "referenceID" : 18,
      "context" : "In [20], multilayer arccosine kernels are built by composing kernels that have an integral representation, but does not explicitly induce invariance.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 19,
      "context" : "More closely related to our work is [21], where kernel descriptors are built for visual recognition by introducing a kernel view of histogram of gradients that corresponds in our case to the cumulative distribution on the group variable.",
      "startOffset" : 36,
      "endOffset" : 40
    }, {
      "referenceID" : 20,
      "context" : "Finally the convolutional kernel network of [22] builds a sequence of multilayer kernels that have an integral representation, by convolution, considering spatial neighborhoods in an image.",
      "startOffset" : 44,
      "endOffset" : 48
    }, {
      "referenceID" : 1,
      "context" : "Our future work will consider the composition of Haar-integration kernels, where the convolution is applied not only to the spatial variable but to the group variable akin to [2].",
      "startOffset" : 175,
      "endOffset" : 178
    }, {
      "referenceID" : 21,
      "context" : "All RLS experiments in this paper were completed with the GURLS toolbox [23].",
      "startOffset" : 72,
      "endOffset" : 76
    }, {
      "referenceID" : 23,
      "context" : "We seek local invariance to pitch and speaking rate [25], and so all random templates are pitch shifted up and down by 400 cents and warped to play at half and double speed.",
      "startOffset" : 52,
      "endOffset" : 56
    }, {
      "referenceID" : 22,
      "context" : "See [24] for more detail.",
      "startOffset" : 4,
      "endOffset" : 8
    }, {
      "referenceID" : 23,
      "context" : "[25].",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 24,
      "context" : "The following Lemma from [26] gives the expectation and the variance of the maximum of two gaussians with correllation coefficient ρ.",
      "startOffset" : 25,
      "endOffset" : 29
    }, {
      "referenceID" : 24,
      "context" : "Lemma 2 (Mean and Variance of Maximum of Correlated Gaussians [26] ).",
      "startOffset" : 62,
      "endOffset" : 66
    }, {
      "referenceID" : 25,
      "context" : "• Upper Bound for the upper tail [27]: P ( 1 kX ≥ 1 + ε ) ≤ e−kε2/8.",
      "startOffset" : 33,
      "endOffset" : 37
    }, {
      "referenceID" : 26,
      "context" : "• Upper Bound for the lower tail [28]: For all k ≥ 2, u ≥ k − 1 we have: P (X < u) ≤ 1− 1 2 exp ( − 2 (u− k − (k − 2) log(u/k) + log(k)) ) .",
      "startOffset" : 33,
      "endOffset" : 37
    }, {
      "referenceID" : 27,
      "context" : "By the theorem on convergence of the empirical CDF [29] (Theorem 4 given in Appendix D ) we have, for γ > 0:",
      "startOffset" : 51,
      "endOffset" : 55
    }, {
      "referenceID" : 14,
      "context" : "Our proof parallels similar proofs in [16].",
      "startOffset" : 38,
      "endOffset" : 42
    }, {
      "referenceID" : 27,
      "context" : "[29] Let X1, X2, .",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 13,
      "context" : "Lemma 7 ([15],Concentration of the mean of bounded random variables in a Hilbert Space).",
      "startOffset" : 9,
      "endOffset" : 13
    }, {
      "referenceID" : 13,
      "context" : "Theorem 5 ([15]).",
      "startOffset" : 11,
      "endOffset" : 15
    } ],
    "year" : 2017,
    "abstractText" : "We analyze in this paper a random feature map based on a theory of invariance (I-theory) introduced in [1]. More specifically, a group invariant signal signature is obtained through cumulative distributions of group transformed random projections. Our analysis bridges invariant feature learning with kernel methods, as we show that this feature map defines an expected Haar integration kernel that is invariant to the specified group action. We show how this non-linear random feature map approximates this group invariant kernel uniformly on a set of N points. Moreover, we show that it defines a function space that is dense in the equivalent Invariant Reproducing Kernel Hilbert Space. Finally, we quantify error rates of the convergence of the empirical risk minimization, as well as the reduction in the sample complexity of a learning algorithm using such an invariant representation for signal classification, in a classical supervised learning setting.",
    "creator" : "LaTeX with hyperref package"
  }
}