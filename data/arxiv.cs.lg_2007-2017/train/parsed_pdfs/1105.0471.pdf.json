{
  "name" : "1105.0471.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Suboptimal Solution Path Algorithm for Support Vector Machine",
    "authors" : [ "Masayuki Karasuyama", "Ichiro Takeuchi" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n10 5.\n04 71\nv1 [\ncs .L\nG ]\n3 M"
    }, {
      "heading" : "1 Introduction",
      "text" : "Recently, the solution path algorithm (Efron et al., 2004; Hastie et al., 2004; Cauwenberghs & Poggio, 2001) has been widely recognized as one of the effective tools in machine learning. It can efficiently compute a sequence of the solutions of a parametrized optimization problem. This technique is originally developed as parametric programming in the optimization community (Best, 1982).\nIn a class of parametric quadratic programs (QPs), the solution path is represented as a piecewise-linear function of the problem parameters. If we regard the regularization parameter of the Support Vector Machine (SVM) as problem parameter, the optimization problem for the SVM is categorized in this class. Therefore, the SVM solutions are represented as piecewise-linear functions of the regularization parameter.\nThe solutions of these parametric QPs are characterized by active constraint set in the current solution. The linearity of the path comes from the fact that the Karush-Khun-Tucker (KKT) optimality conditions of these problems are represented as a linear system defined by the current active set, while the “piecewise-\nness” is the consequence of the changes in the active set. The piecewise-linear solution path algorithm repeatedly updates the linear system and active set. The point of active set change is called breakpoint in the literature. The path of solutions generated by this algorithm is very accurate and they satisfy the optimality conditions more strictly than other algorithms.\nMany machine learning problems, however, do not require strict optimality of the solution. In fact, one of the popular SVM optimization algorithm, called sequential minimal optimization (SMO) Platt (1999), is known to produce suboptimal (approximated) solution, where the tolerance to the optimality (degree of approximated) can be specified by users. In many experimental studies, it has been demonstrated that the generalization performances of these suboptimal solutions are not significantly different from those of strictly optimal ones.\nTherefore, the strict optimality of the solution path algorithm is often unnecessary. Furthermore, it adversely affects the computational efficiency of the algorithm. In fact, the solution path algorithm can be very slow when it encounters a large number of (seemingly redundant) breakpoints. Although some empirical studies suggest that the number of breakpoints grows linearly in the input size, in the worst case, it can grow exponentially (Gärtner et al., 2009). Another difficulty is in starting the solution path algorithm from an approximated solution, for example obtained by SMO, because it does not satisfy the strict optimality requirement.\nIn order to address these issues in the current solution path algorithm, we introduce a suboptimal solution path algorithm. Our algorithm also generates piecewise-linear solution path, but the optimality tolerance (approximation level) can be arbitrary controlled by users. It allows to control the trade-off between the accuracy of the solution and the computational cost.\nThe presented suboptimal solution path algorithm has the following properties.\n• First, the algorithm can reduce the number of breakpoints (which is the main computational bottleneck in solution path algorithm) by allowing multiple active set changes at one breakpoint. Although this modification causes what is called degeneracy problem, we provide an efficient and accurate way to solve this issue. We empirically show that reducing the number of breakpoints can work effectively to the computational efficiency.\n• Second, the suboptimal solutions obtained by the algorithm can be interpreted as the solution of a perturbed optimization problem from the original one. This novel interpretation provides several insights into the properties of our suboptimal solutions. We present some theoretical analyses of our suboptimal solutions using this interpretation.\nWe also empirically investigate several practical properties of our approach. Although, our algorithm updates multiple active constraints at one breakpoint, we observe that the entire changing patterns of the active sets are very similar to those of the exact path. Moreover, despite its computational efficiency, the generalization performance of our suboptimal path is comparable to conventional\none. To the best of our knowledge, there are no previous works for suboptimal solution path algorithm with controllable optimality tolerance that can be applicable to standard SVM formulation 1. Although many authors mimic the solution path by just repeating the warm-start on finely grid points (e.g., Friedman et al., 2007), this approach does not provide any guarantee about the intermediate solutions between grid points. In this paper we focus our attention to the solution path algorithm for standard SVM, but the presented approach can be applied to other problems in the aforementioned QP class."
    }, {
      "heading" : "2 Solution Path for Support Vector Machine",
      "text" : "In this section, we describe the solution path algorithm for regularization parameters of Support Vector Machine (SVM)."
    }, {
      "heading" : "2.1 Support Vector Machine",
      "text" : "Suppose we have a set of training data {(xi, yi)} n i=1, where xi ∈ X ⊆ R p is the input and yi ∈ {−1,+1} is the output class label. SVM learns a linear discriminant function f(x) = w⊤Φ(x) + α0 in a feature space F , where Φ : X → F is a map from the input space X to the feature space F , w ∈ F is a coefficient vector and α0 ∈ R is a bias term.\nIn this paper, we consider the optimization problem of the following form:\nmin w,α0,{ξi}ni=1\n1 2‖w‖ 2 2 + ∑n i=1 Ciξi, (1)\ns.t. yif(xi) ≥ 1− ξi, ξi ≥ 0, i = 1, . . . , n,\nwhere {Ci} n i=1 denotes regularization parameters. This formulation reduces to the standard formulation of the SVM when all Ci’s are the same. Our discussion in this paper holds for arbitrary choice of Ci’s.\nWe formulate the dual problem of (1) as:\nmax α\n− 1\n2 α⊤Qα+ 1⊤α\ns.t. y⊤α = 0, 0 ≤ α ≤ c, (2)\nwhere α = [α1, . . . , αn] ⊤, c = [C1, . . . , Cn] ⊤ and (i, j) element of Q ∈ Rn×n is Qij = yiyjΦ(xi)\n⊤Φ(xj). Note that, we use inequalities between vectors as the element-wise inequality (i.e., α ≤ c ⇔ αi ≤ Ci for i = 1, . . . , n ). Using kernel function K(xi,xj) = Φ(xi) ⊤Φ(xj), discriminant function f is represented as:\nf(x) = n∑\ni=1\nαiyiK(x,xi) + α0.\n1 Giesen et al. (2010) proposed approximated path algorithm with some optimality guarantee that can be applicable to L2-SVM without bias term.\nIn what follows, the subscript by an index set such as vI for a vector v = [v1, · · · , vn]\n⊤ indicates a sub-vector of v whose elements are indexed by I = {i1, . . . , i|I|}. For example, for v = [a, b, c] ⊤ and I = {1, 3}, vI = [a, c] ⊤. Similarly, the subscript by two index sets such as MI1,I2 for a matrix M ∈ R\nn×n denotes a sub-matrix whose rows and columns are indexed by I1 and I2, respectively. The principal sub-matrix such as MI,I is abbreviated as MI ."
    }, {
      "heading" : "2.2 Solution Path Algorithm for SVM",
      "text" : "In this paper, we consider the solution path with respect to the regularization parameter vector c. To follow the path, we parametrized c in the following form:\nc(θ) = c(0) + θd,\nwhere c(0) = [C (0) 1 , . . . , C (0) n ]⊤ is some initial parameter, d = [d1, . . . , dn] ⊤ is a direction of the path and θ ≥ 0. We trace the change of the optimal solution of the SVM when θ increases from 0.\nLet {α (θ) i } n i=0 be the optimal parameters and {f (θ) i } n i=1 be the outputs f(xi)\nat θ. The KKT optimality conditions are summarized as:\nyif (θ) i ≥ 1, if α (θ) i = 0, (3a) yif (θ) i = 1, if 0 < α (θ) i < C (θ) i , (3b) yif (θ) i ≤ 1, if α (θ) i = C (θ) i , (3c)\ny⊤α = 0. (3d)\nWe separate data points into three index sets M,O, I ⊆ {1, . . . , n} in such a way that these sets satisfy\ni ∈ O ⇒ yif (θ) i ≥ 1, α (θ) i = 0, (4a)\ni ∈ M ⇒ yif (θ) i = 1, α (θ) i ∈ [0, Ci], (4b)\ni ∈ I ⇒ yif (θ) i ≤ 1, α (θ) i = Ci, (4c)\nand we denote these partitions altogether as π := (O,M, I). If every data point belongs to one of the three index sets and equality (3d) holds, the KKT conditions (3) are satisfied. As long as these index sets are unchanged, we have analytical expression of the optimal solution in the form of α (θ+∆θ) i = α (θ) i +∆θβi, i = 0, . . . , n, where ∆θ is the change of θ and {βi} n i=0 are constants derived from sensitivity analysis theory:\nTheorem 1. Let π = (O,M, I) be the partition at the optimal solution at θ and assume that\nM =\n[ 0 y⊤M\nyM QM\n]\nis non-singular2. Then, as long as π is unchanged, {βi} n i=0 is given by\n[ β0 βM ] =−M−1 [ y⊤I QM,I ] dI , βO = 0, βI = dI . (5)\nThe proof is in Appendix A. This theorem can be viewed as one of the specific forms of the sensitivity theorem Fiacco (1976). It can be derived from the KKT conditions (3) and the similar properties are repeatedly used in various solution path algorithms in machine learning (Cauwenberghs & Poggio, 2001; Hastie et al., 2004).\nUsing the above theorem, we can update the solution by α (θ+∆θ) i = α (θ) i + ∆θβi as long as π is unchanged. However, if we changes θ, the optimal partition π could also changes. Those change points are called breakpoints. In the solution path algorithm, the optimality conditions are always kept satisfied by precisely detecting the breakpoints and updating π properly."
    }, {
      "heading" : "3 Suboptimal Solution Path",
      "text" : "In this section, we develop a suboptimal solution path algorithm for the SVM, where the tolerance to the optimality conditions can be arbitrary controlled by users. The basic idea is to relax the KKT optimality conditions and allow multiple data points to move among the partition π at the same time. Note that it reduces the number of breakpoints and leads to the improvement in its computational efficiency: allowing us to control the balance between the accuracy of the solution and the computational cost."
    }, {
      "heading" : "3.1 Approximate Optimality Conditions",
      "text" : "First, we relax the conditions (4) as\ni ∈ O ⇒ yif (θ) i ≥ 1− ε1, α (θ) i ∈ [−ε2, 0], (6a)\ni ∈ M ⇒ yif (θ) i ∈ [1−ε1, 1+ε1], α (θ) i ∈ [−ε2, C (θ) i +ε2], (6b)\ni ∈ I ⇒ yif (θ) i ≤ 1+ε1, α (θ) i ∈ [C (θ) i , C (θ) i +ε2], (6c)\nwhere ε1 ≥ 0 and ε2 ≥ 0 specify the degree of approximation. If we set ε1 = ε2 = 0, these conditions reduce to (4).\nOur algorithm changes θ while keeping the above conditions (6) satisfied. Let θ0 = 0 be the initial value of θ and the non-decreasing sequence θ0 ≤ θ1 ≤ θ2 ≤ . . ., be the breakpoints. Suppose we are currently at θk, the next breakpoint θk+1 is characterized as the point that we can not increase θ without violating the conditions (6) or changing index sets π.\n2The invertibility of the matrix M is assured if and only if the submatrix QM is positive definite in subspace {z ∈ R|M| | y⊤Mz = 0}.\nIf we set {βi} n i=0 by (5), then yif (θ) i , i ∈ M, and α (θ) i , i ∈ O∪I, are constants.\nTo increase θ from θk, we only need to check the following inequalities:\nyif (θk) i +∆θgi ≥ 1− ε1, i ∈ O,\nα (θk) i +∆θβi ∈ [−ε2, C (θk) i + ε2], i ∈ M,\nyif (θk) i +∆θgi ≤ 1− ε1, i ∈ I,\nwhere gi is the change of output yifi which is defined by g = Qβ + yβ0. We want to know the maximum ∆θ which satisfies all of the above inequalities. We can easily calculate the maximum ∆θ for each inequality as follows:\nΘO = { (1− ε1 − yif (θk) i )/gi ∣∣∣i ∈ O, gi < 0 } ,\nΘMℓ = { −(α (θk) i + ε2)/βi ∣∣∣ i ∈ M, βi < 0 } , ΘMu = { (C (θk) i + ε2 − α\n(θk) i )/(βi − di)∣∣∣i ∈ M, βi > di } ,\nΘI = { (1 + ε1 − yif (θk) i )/gi ∣∣∣i ∈ I, gi > 0 } ,\nSince we have to keep all of the inequalities satisfied, we take the minimum of these values: ∆θ = minΘ, where Θ = {ΘO,ΘMℓ ,ΘMu ,ΘI}. Then we can find θk+1 = θk +∆θ.\nAlthough we detect θk+1, it is necessary to update π to go beyond the breakpoint. Conventional solution path algorithms allow only one data point to move between the partition π at each breakpoint. For example, αi, i ∈ M, reaches 0, the algorithm transfers the index i from M to O (Figure 1(a)). In our algorithm, multiple data points are allowed to move between the partitions π at the same time in order to reduce the number of breakpoints."
    }, {
      "heading" : "3.2 Update Index Sets",
      "text" : "At a breakpoint, our algorithm handles all the data points that violate the strict inequality conditions (4) rather than the relaxed ones (6) (Figure 1(b)). This situation can be interpreted as what is called degeneracy in the parametric programming (Ritter, 1984). Here, degeneracy means that multiple constraints hit their boundaries of inequalities simultaneously. Although degenerate situation rarely happens in conventional solution path algorithms, it is not the case in ours. The simultaneous change of multiple data points inevitably brings about “highly” degenerate situations involved with many constraints. In degenerate case, we have a problem called the cycling. For example, if we move two indices i and j from M to O at the breakpoint, then both or either of them may immediately return to M. To avoid the cycling, we need to design an update strategy for π that can circumvent cycling.\nThe degeneracy can be handled by several approaches which are known in the parametric programming literature. Ritter (1984) showed that the cycling can be dealt with through the well-known Bland’s minimum index rule in the\nlinear programming (Bland, 1977). However, in the worst case, this approach must go through all the possible patterns of next π. Since we need to evaluate {βi} n i=0 in each iteration, a large number of iterations may cause additional computational cost. In this paper, we provide more essential solution to this problem based on (Berkelaar et al., 1997).\nSuppose we are currently on the breakpoint θk. Let\nBO = {i | α (θk) i ≤ 0, βi < 0, i ∈ M}∪\n{i | yif (θk) i ≤ 1, gi < 0, i ∈ O},\nBI = {i | α (θk) i ≥ C (θk) i , βi > di, i ∈ M}∪\n{i | yif (θk) i ≥ 1, gi > 0, i ∈ I}.\nBO is the set of indices which satisfy the conditions (6a) and (6b) for being the member of M and O simultaneously at θk. Similarly, indices in BI satisfy the conditions (6b) and (6c) for being the member of M and I at θk. Moreover, let us define sum of these two sets as\nB = BO ∪ BI .\nOur task is to partition these indices to O, M and I correctly so that it does not cause the cycling.\nIn our formulation, due to the approximation by ε1 and ε2, the cycling may not occur at ∆θ = 0 immediately. For example, suppose that i move to M from O and its parameter is αi = 0. In the next iteration, we need to check αi + ∆θβi ≥ −ε2. If βi < 0, then we obtain ∆θ ≤ −ε2/βi > 0. Although it allows ∆θ > 0, the index i may return back to O. This situation can also be considered as cycling.\nLet πk = (Ok,Mk, Ik) be π in [θk, θk+1]. At θk+1, if and only if the cycling does not occur, it can be shown that the following conditions hold:\nβi ≥ 0, gi = 0, for i ∈ Mk+1 ∩ BO, (7a)\nβi = 0, gi ≥ 0, for i ∈ Ok+1 ∩ BO, (7b)\nβi ≤ di, gi = 0, for i ∈ Mk+1 ∩ BI , (7c)\nβi = di, gi ≤ 0, for i ∈ Ik+1 ∩ BI . (7d)\nAlthough βi and gi are usually calculated using π, our approach allows us to calculate βi and gi without knowing π so that they can satisfy the above conditions. If the gradient β, which is defined in (5), satisfies the following conditions, we can find the next partition πk+1 to satisfy (7). The conditions are:\ngiβi = 0, gi ≥ 0, βi ≥ 0, i ∈ BO,\ngi(di − βi) = 0, gi ≤ 0, βi ≤ di, i ∈ BI , (8)\nIf we know such β and g, using the following update rule, we can determine πk+1 as:\nMk = Mk+ 1 2 ∪ {i | βi > 0, gi = 0, i ∈ BO}\n∪ {i | βi < di, gi = 0, i ∈ BI}, Ok = Ok+ 1\n2 ∪ {i | βi = 0, gi ≥ 0, i ∈ BO},\nIk = Ik+ 1 2 ∪ {i | βi = di, gi ≤ 0, i ∈ BI},\n(9)\nwhere Ok+ 1 2 = Ok \\ B, Mk+ 1 2 = Mk \\ B and Ik+ 1 2 = Ik \\ B.\nRemark 1. By definition, the update rule (9) guarantees that the non-cycling conditions (7) hold.\nTo use (9), we need β (5) which satisfies (8). The following theorem shows that it can be obtained from a quadratic programming problem (QP):\nTheorem 2. Let β̂0, β̂ and ĝ be the optimal solutions of the following QP\nproblem:\nmin β̂0, ̂β,ĝ\n∑\ni∈BO\nĝiβ̂i + ∑\ni∈BI\nĝi(β̂i − di) (10)\ns.t.    ĝBO ≥ 0, β̂BO ≥ 0, ĝBI ≤ 0, β̂BI ≤ dI , ĝM k+1 2 = 0, β̂O k+1 2 = 0, β̂I k+1 2 = dI k+1 2 ,\ny⊤β̂ = 0, ĝ = Qβ̂ + yβ̂0,\nand π is determined by (9) using β̂ and ĝ. Then β̂0, β̂ and ĝ satisfy (8) and they are equal to the gradient β0, β and g, respectively.\nAlthough the detailed proof is in Appendix, we can provide clear interpretation of this optimization problem. The objective function and inequality constraints corresponds to (8) and the other constraints correspond to the linear system (5). It can be shown that the optimal value of the objective function is 0. Given the non-negativity of each term in the objective, we see that (8) holds (see Appendix B for detail).\nThe optimization problem (10) has 2n+ 1 variables and 2|B|+ 2n+ 1 constraints. However, we can reduce these sizes to |B| variables and 2|B| constraints by arranging the equality constraints3. The detailed formulation of the reduced problem is in Appendix C. If the size of |B| is large, it may take large computational cost to solve (10). To avoid this, we set the upper bound B for the number of elements of B. In the case of |B| > B, we choose top B elements from the original B by increasing order of Θ as the elements of B."
    }, {
      "heading" : "3.3 Algorithm and Computational Complexity",
      "text" : "Here, we summarize our algorithm and analyze its computational complexity. At the k-th breakpoint, our algorithm performs the following procedure:\nstep1 Using πk, calculate β0,β and g by (5)\nstep2 Calculate the next breakpoint θk+1 and update α (θ) 0 ,α (θ), c(θ);\nstep3 Solve (10) and calculate πk+1 by (9)\nIn step1, we need to solve the linear system (5). In conventional solution path algorithms, we can update it using rank-one-update of an inverse matrix or a Cholesky factor from previous iteration by O(|M|2) computations. In our case, we need rank-m-update at each breakpoint, where 1 ≤ m ≤ B. When we set B as some small constant, the computational cost still remains O(|M|2). Including the other processes in this step, the computational cost becomes O(n|M|). In step2, given β and g, we can calculate all the possible step length Θ by O(n). In step3, since the optimization problem (10) becomes convex QP problem with\n3\nIn the case of |M k+ 1\n2\n| = 0, the reduced problem has |B|+ 1 variables 2|B|+ 1 constraints.\n|B| variables, it can be solved efficiently by some standard QP solvers in the situation |B| is relatively small compared to n. When we set B as some constant, the time for solving this optimization problem is then independent of n.\nPut it all together, in the case of constant B, the computational cost of each breakpoint is O(n|M|). This is the same as the conventional solution path algorithm. However, as we will see later in experiments, our algorithm drastically reduces the number of breakpoints especially when we use large ε1 and ε2."
    }, {
      "heading" : "4 Analysis",
      "text" : "In this section, we provide some theoretical analyses of our suboptimal solution path."
    }, {
      "heading" : "4.1 Interpretation as Perturbed Problem",
      "text" : "An interesting property of our approach is that the solutions always keep the optimality of an optimization problem which is slightly perturbed from the original one. The following theorem gives the formulation of the perturbed problem:\nTheorem 3. Every solution α(θ) in the suboptimal solution path is the optimal solution of the following optimization problem:\nmax α\n− 1\n2 α⊤Qα+ (1+ p)⊤α\ns.t. y⊤α = 0, −q ≤ α ≤ c(θ) + q. (11)\nwhere perturbation parameters p, q ∈ Rn are in −ε11 ≤ p ≤ ε11 and 0 ≤ q ≤ ε21, respectively.\nProof. Let ξ+, ξ− ∈ Rn+ and κ ∈ R be the Lagrange multipliers. The Lagrangian is\nL = − 12α ⊤Qα+ (1+ p)⊤α\n+(α+ q)⊤ξ− + (c(θ) + q −α)⊤ξ+ + κy⊤α,\nand the KKT conditions are\n∂L ∂α = −Qα+ 1+ p+ ξ − − ξ+ + κy = 0, (12a)\nξ+, ξ− ≥ 0, (12b)\nξ−i (αi + qi) = 0, i = 1, . . . , n, (12c)\nξ+i (C (θ) i + qi − αi) = 0, i = 1, . . . , n, (12d)\n−q ≤ α ≤ c(θ) + q. (12e)\ny⊤α = 0, (12f)\nSubstituting α = α(θ) and κ = −α (θ) 0 , i-th element of (12a) can be written as\nyif (θ) i = 1 + pi + ξ − i − ξ + i . Considering this and the conditions of suboptimal solution α(θ) (6), there exist pi ∈ [−ε1, ε1] and ξ ± i which satisfy ξ + i = ξ − i = 0 for i ∈ M, ξ+i = 0, ξ − i ≥ 0, for i ∈ O and ξ + i ≥ 0, ξ − i = 0, for i ∈ I. These ξ±i ’s satisfy the non-negativity constraint (12b). The complementary conditions (12c) and (12d) for i ∈ M hold from ξ+i = ξ−i = 0. For i ∈ O, since ξ + i = 0, we don’t have to check (12d). In this case, if we set qi = −α (θ) i ∈ [0, ε2], then (12c) holds. It can be shown in a similar way that (12c) and (12d) hold for i ∈ I. Our suboptimal solution path algorithm always satisfies the equality constraint of the dual (2) and the box constraint (12e) satisfied. Therefore, we see (12) holds.\nThe problem (11) can be interpreted as the dual problem of the following form of the SVM:\nmin w,α0\n1 2 w⊤w +\nn∑\ni=1\nℓ(1 + pi − yifi), (13)\nwhere\nℓ(ξi) =\n{ (C\n(θ) i + qi)ξi, for ξi ≥ 0, −qiξi, for ξi < 0,\nis a loss function. We see that the perturbations present in the loss term."
    }, {
      "heading" : "4.2 Error Analysis",
      "text" : "We have shown that the solution of the suboptimal solution path can be interpreted as the optimal solution of the perturbed problem (13). Here, we consider how close the optimal solution of the perturbed problem to the solution of the original problem in terms of the optimal objective value.\nLet D(α) and D̃(α) be the dual objective functions of the original optimization problem (2) and the perturbed problem (11), respectively. From the affine\nlower bound of D̃(α), we obtain\nD̃(α) ≤ D(α∗) + p⊤α∗ + (−Qα∗ + 1+ p)⊤(α−α∗),\nwhere α∗ is the optimal solution of the original problem. Let α̃ be the optimal solution of the perturbed problem. Substituting α = α̃ and adding α∗0y\n⊤(α̃− α∗) = 0 to the right hand side, we obtain\nD̃(α̃)−D(α∗) ≤ p⊤α∗ + (ξ∗ + p)⊤(α̃−α∗), (14)\nwhere ξ∗ = −Qα∗ − yα∗0 + 1. Note that ξ ∗ I ≥ 0, ξ ∗ M = 0 and ξ ∗ O ≤ 0, where I, M and O represent the optimal partition of the original problem (2). Here,\nwe define Ĩ = {i | ξ∗i + pi ≥ 0, i ∈ I}, Õ = {i | ξ ∗ i + pi ≤ 0, i ∈ O} and M̃ = {1, . . . , n} \\ (Õ ∪ Ĩ). From the right hand side of (14), we obtain\nD̃(α̃)−D(α∗) ≤ ∑\ni∈M∪I |pi| C (θ) i +\n∑ i∈Ĩ∪Õ |ξ ∗ i + pi| qi + ∑ i∈M̃ |pi| (C (θ) i + qi)\nFrom the duality theorem, this also bounds the difference of the primal objective value. Comparing the original objective function (1), this bound can be considered small when pi and qi is enough small compared to ξ ∗ i and Ci. In this view point, this bound gives theoretical justification for our intuitive interpretation. The bound for D(α∗)− D̃(α̃) can be also derived in the same manner."
    }, {
      "heading" : "5 Experiments",
      "text" : "In this section, we illustrate the empirical performance of the proposed approach compare to the conventional exact solution path algorithm. Our task is to trace the solution path from c(0) = 10−1/n × 1 to c(1) = 106/n × 1. Since all the elements of c(θ) takes the same value in this case, we sometimes refer to this common value as C(θ) (i.e., c(θ) = C(θ) × 1). The RBF kernel K(xi,xj) = exp(−γ‖xi − xj‖ 2 2) is used with γ = 1/p where p is the number of features. To circumvent possible numerical instability in the solution path, we add small positive constant 10−6 to the diagonals of the matrix Q.\nLet e ≥ 0 be a parameter which controls the degree of approximations. In this paper, using e, we set ε1 and ε2 as ε1 = e and ε2 = e×C\n(θk), respectively, where θk is the previous breakpoint. We set ε2 using relative scale to C\n(θk). Table 1 lists the statistics of data sets. These data sets are available from LIBSVM site (Chang & Lin, 2001) and UCI data repository (Asuncion & Newman, 2007). We randomly sampled n data points from the original data set 10 times (we set n be approximately 80% of the original number of data points in the table). The input x of each data set is linearly scaled to [0, 1]p.\nFigure 2 shows the comparison of the CPU time and the number of breakpoints. To make fair comparison, the initialization is not included in the CPU time. In these results, we set B = 10 and we investigated the relationship between the computational cost and the degree of approximation by examining several settings of e ∈ {0.001, 0.01, 0.1, 0.5}. The results indicate that our approach can reduce the CPU time especially when e is large. The number of\nbreakpoints were also reduced, in the same way as the CPU time. In our approach, since we need rank-m-update of matrix in each breakpoint (1 ≤ m ≤ B), an update in a breakpoint may take longer time than rank-one-update which is needed in the conventional solution path algorithm. We conjecture that this is why the decrease in the number of breakpoints was slightly faster than the CPU time. However, since the maximum value of |B| was set as B = 10 in this experiment, this additional cost was relatively small compared to the effect of the reduction of the number of breakpoints.\nNext, we investigated the effect of B. Figure 3 shows the CPU time and the number of breakpoints for w1a data (n = 2477, p = 300) with B = 10 and B = n. When B = n, there are no upper bounds for |B|. In the left plot, when B = n, we see that the CPU time is longer than the case of B = 10. In this data set, this difference of the CPU time mainly comes from the cost of the matrix update and QP (10) whose size is proportional to |B| (data not shown). On the\nother hand, in the left plot, the number of breakpoints is stable in the both case of B = n and B = 10, and interestingly, the number itself is almost the same in these two settings. Our results suggest that too many B does not contribute to reduce the number of breakpoint. Although these unstable results in B = n is not always happen, we observed that it is more stable to use B = 10 or B = 100 in several other data sets.\nWe also compared the difference of π between the exact solution path and the suboptimal path in order to see the degree of approximation in terms of the active set. Let Ii ∈ {0, 1} be an indicator variable which has 1 when a data point i belongs to different set among M, O and I between two solution paths. Figure 4(a) shows plots of 10 runs average of ∑n i=1 Ii/n for e = 0.5 in a5a data set. We see that the difference is at most about 10%. Figure 4(b) shows the size of each index set (this plot is one of 10 runs). Although the small differences exist, the changing patterns are similar each other.\nTable 2 shows results of test error rate comparison for e = 0.5. We used 60% of the data for training, 20% for validation and 20% for testing. In each data set, we see that the performances of our suboptimal solutions are comparable to the exact solution path."
    }, {
      "heading" : "6 Conclusion",
      "text" : "In this paper, we have developed a suboptimal solution path algorithm which traces the changes of solutions under the relaxed optimality conditions. Our algorithm can reduce the number of breakpoints by moving multiple indices in π at one breakpoint. Another interesting property of our approach is that the suboptimal solutions exactly correspond to the optimal solutions of the perturbed problems from the original SVM optimization problems. The experimental results demonstrate that our algorithm efficiently follows the path and it has similar patterns of active sets and classification performances compared to the exact path."
    }, {
      "heading" : "A Proof of Theorem 1",
      "text" : "Here, we provide a proof of the following theorem:\nTheorem 1. Let π = (O,M, I) be the partition at the optimal solution at θ and assume that\nM =\n[ 0 y⊤M\nyM QM\n]\nis non-singular. Then, as long as π is unchanged, {βi} n i=0 is given by\n[ β0 βM ] = −M−1 [ y⊤I QM,I ] dI ,\nβO = 0, βI = dI .\n(A.1)\nProof. As long as π is unchanged, αi for i ∈ O and i ∈ I must be\nαi = 0, i ∈ O, αi = C (θ) i , i ∈ I.\nTherefore, we see that βO = 0 and βI = dI . From the definition of M, at the optimal, the following linear system holds\nQMα (θ) M +QM,Ic (θ) I + yMα (θ) 0 = 1.\nCombining with the equality constraint of the dual problem y⊤α = 0, we obtain the following linear system:\nM\n[ α (θ) 0\nα (θ) M\n] + [ y⊤I\nQM,I\n] c (θ) I = [ 0 1 ] .\nSolving this, we obtain\n[ α (θ) 0\nα (θ) M\n]\n= −M−1 [\ny⊤I QM,I\n] c (θ) I +M −1 [ 0 1 ] .\nUsing c(θ+∆θ) = c(θ) + θd, we can write\n[ α (θ+∆θ) 0\nα (θ+∆θ) M\n] = [ α (θ) 0\nα (θ) M\n] − θM−1 [ y⊤I\nQM,I\n] dI\nThen, we obtain (A.1)."
    }, {
      "heading" : "B Proof of Theorem 2",
      "text" : "Here, we provide a proof of Theorem 2. First, we prove the following lemma.\nLemma 1. Suppose β̂ ∈ Rn, β̂0 ∈ R and ĝ = Qβ̂ + yβ̂0 satisfy the following conditions:\nĝiβ̂i = 0, ĝi ≥ 0, β̂i ≥ 0, i ∈ BO , (B.1a)\nĝi(di − β̂i) = 0, ĝi ≤ 0, β̂i ≤ di, i ∈ BI , (B.1b)\nĝM k+1\n2\n= 0, β̂O k+1\n2\n= 0, β̂I k+ 1\n2\n= dI , (B.1c)\ny ⊤ β̂ = 0. (B.1d)\nThen, β̂0, β̂ and ĝ are equal to β0, β and g, respectively, where π is determined by the update rule\nMk = Mk+ 1 2 ∪ {i | βi > 0, gi = 0, i ∈ BO}\n∪ {i | βi < di, gi = 0, i ∈ BI},\nOk = Ok+ 1 2 ∪ {i | βi = 0, gi ≥ 0, i ∈ BO},\nIk = Ik+ 1 2 ∪ {i | βi = di, gi ≤ 0, i ∈ BI},\n(B.2)\nusing β̂ and ĝ.\nProof. Since the conditions (B.1a) and (B.1b) hold, all of the elements of B is assigned to one of the three index sets by (B.2). From the definitions of Mk+1, Ok+1 and Ik+1 (B.2), we see ĝMk+1 = 0, β̂Ok+1 = 0 and β̂Ik+1 = dIk+1 . Using these three equations and (B.1d), we can easily obtain the same linear system as (A.1).\nNext, we consider theorem 2.\nTheorem 2. Let β̂0, β̂ and ĝ be the optimal solutions of the following QP problem:\nmin β̂0, ̂β,ĝ\n∑\ni∈BO\nĝiβ̂i + ∑\ni∈BI\nĝi(β̂i − di) (B.3)\ns.t.\n \n\nĝBO ≥ 0, β̂BO ≥ 0, ĝBI ≤ 0, β̂BI ≤ dI ,\nĝM k+1\n2\n= 0, β̂O k+1\n2\n= 0, β̂I k+1\n2\n= dI k+ 1\n2\n,\ny ⊤ β̂ = 0, ĝ = Qβ̂ + yβ̂0,\nand π is determined by (B.2) using β̂ and ĝ. Then β̂0, β̂ and ĝ satisfy (B.1) and they are equal to the gradient β0, β and g, respectively.\nProof. In this proof, we omit subscript k+ 1 2 to simplify the notation. First, we rewrite the optimization problem (B.3) as follows:\nmin β̂0, ̂β,ĝ\n∑\ni∈BO∪M∪O\ngiβ̂i + ∑\ni∈BI∪I\ngi(β̂i − di)\ns.t.\n \n\nĝ = Qβ̂ + yβ̂0,\nĝBO ≥ 0, β̂BO ≥ 0,\nĝBI ≤ 0, β̂BI ≤ dI ,\nĝM = 0, β̂O = 0, β̂I = dI , y ⊤ β̂ = 0.\nAlthough we slightly modified the expression of the objective function, its value is the same as (B.3) as long as the equality constraints hold. From the inequality constraints, we see that the objective value is always non-negative in the feasible region.\nTo simplify the notation, we introduce the following new variables:\nβ̃ = E(2)d+Eβ̂, g̃ = Eĝ,\nỹ = Ey,\nQ̃ = EQE,\nwhere\nE (1) ij = { 1 for {(i, j) | i = j, i ∈ BO ∪M∪O}, 0 others ,\nE (2) ij = { 1 for {(i, j) | i = j, i ∈ BI ∪ I}, 0 others ,\nE = E(1) −E(2).\nMoreover, if we set T = O ∪ I, the optimization problem (B.3) is written as\nmin β0, ˜β,g̃ β̃\n⊤ Q̃β̃ + r0β0 + r ⊤ β̃\ns.t.\n \n\nQ̃β̃ + ỹβ0 + r − g̃ = 0,\ng̃M = 0, g̃B ≥ 0,\nβ̃T = 0, β̃B ≥ 0, ỹ ⊤ β̃ = r0,\nwhere\nr = −Q̃:,IdI − Q̃:,BcIdBI , r0 = −ỹ ⊤ I dI − ỹBcIdBI ,\nare constants. Let ξ ∈ Rn,µM ∈ R |M|,µB ∈ R |B|,νT ∈ R |T |,νB ∈ R |B|, ρ ∈ R be the Lagrange multipliers. Then, the Lagrangian is\nL = β̃ ⊤ Q̃β̃ + r0β0 + r ⊤ β̃ + ξ⊤ ( Q̃β̃ + ỹβ0 + r − g̃ )\n+µ⊤Mg̃M − µ ⊤ B g̃B + ν ⊤ T β̃T − ν ⊤ B β̃B +ρ ( ỹ ⊤ β̃ − r0 ) ,\nwhere µB ≥ 0, νB ≥ 0. Differentiating L, we obtain\n∂L ∂β̃ = 2Q̃β̃ + r + Q̃ξ + ν̃ + ρỹ = 0,\n∂L ∂β0 = r0 + ξ ⊤ ỹ = 0,\n∂L ∂g̃ = −ξ + µ̃ = 0,\nwhere ν̃ ∈ Rn is a vector whose components are ν̃M = 0, ν̃B = −νB, ν̃T = νT and µ̃ ∈ Rn has µ̃M = µM, µ̃B = −µB, µ̃T = 0. Using these equations, we obtain the following dual problem:\nmax ˜β,ξ,ν̃ ,µ̃,ρ\n−β̃ ⊤ Q̃β̃ − ρr0 + ξ ⊤ r (B.4)\ns.t.\n \n\n2Q̃β̃ + r + Q̃ξ + ν̃ + ρỹ = 0,\nr0 + ξ ⊤ ỹ = 0,\n− ξ + µ̃ = 0,\nν̃M = 0, ν̃B ≤ 0,\nµ̃T = 0, µ̃B ≤ 0.\n(B.5)\nUsing the constraints of this problem (B.5), we can derive the following bound of the objective function (B.4):\n− β̃ ⊤ Q̃β̃ − ρr0 + ξ ⊤ r = −β̃ ⊤ Q̃β̃ + ρξ⊤ỹ + ξ⊤r\n= −β̃ ⊤ Q̃β̃ − 2β̃ ⊤ Q̃ξ − ξ⊤Q̃ξ − ξ⊤ν̃\n= −(β̃ + ξ)⊤Q̃(β̃ + ξ)− ξ⊤ν̃\n= −(β̃ + ξ)⊤Q̃(β̃ + ξ)− µ̃⊤ν̃ ≤ 0.\nFrom this we see that the dual objective function is less than or equal to 0. Thus, the optimal objective value of the optimization problem is 0. Then the conditions (B.1) is satisfied. From lemma 1, the claim is proved."
    }, {
      "heading" : "C Reformulate the Optimization Problem (10)",
      "text" : "We reformulate the optimization problem (10) to reduce the number of variables and constraints. Here again, we omit subscript of M, O and I to simplify the notation.\nDefine B = {b1, . . . , b|B|}, SO = {i ∈ {1, . . . , |B|} | bi ∈ BO} and SI = {i ∈ {1, . . . , |B|} | bi ∈ BI}. When |B| 6= 0, the optimization problem (10) can be reformulated as\nmin β\nB\nβ ⊤ BQ ′ βB + (vB −Q ′ :,SI dBI ) ⊤ βB\ns.t.\n \n\nQ ′ SO ,: βB + vBO ≥ 0, Q ′ SI ,: βB + vBI ≤ 0, βBO ≥ 0, βBI ≤ dBI ,\nwhere\nQ ′ = QB − [ yB QB,M ] M −1\n[ y⊤B\nQB,M\n] ,\nu = −M−1 [\ny⊤I QM,I\n] dI ,\nv = [ y Q:,M ] u+Q:,IdI .\nOn the other hand, when |B| = 0, (10) becomes\nmin β\nB ,β0\nβ ⊤ BQBβB + (QB,IdI −QB,BIdBI ) ⊤ βB\n−(y⊤I dI + y ⊤ BI dBI )β0\ns.t.\n \n\ny ⊤ BβB + y ⊤ I dI = 0 QBO ,BβB +QBO ,IdI + yBOβ0 ≥ 0 QBI ,BβB +QBI ,IdI + yBIβ0 ≤ 0 βBO ≥ 0, βBI ≤ dBI ."
    } ],
    "references" : [ {
      "title" : "The optimal set and optimal partition approach to linear and quadratic programming",
      "author" : [ "A.B. Berkelaar", "K. Roos", "T. Terláky" ],
      "venue" : "Advances in Sensitivity Analysis and Parametric Programming,",
      "citeRegEx" : "Berkelaar et al\\.,? \\Q1997\\E",
      "shortCiteRegEx" : "Berkelaar et al\\.",
      "year" : 1997
    }, {
      "title" : "An algorithm for the solution of the parametric quadratic programming problem",
      "author" : [ "M.J. Best" ],
      "venue" : "Technical Report 82-24,",
      "citeRegEx" : "Best,? \\Q1982\\E",
      "shortCiteRegEx" : "Best",
      "year" : 1982
    }, {
      "title" : "New finite pivoting rules for the simplex method",
      "author" : [ "R.G. Bland" ],
      "venue" : "Mathematics of Operations Research,",
      "citeRegEx" : "Bland,? \\Q1977\\E",
      "shortCiteRegEx" : "Bland",
      "year" : 1977
    }, {
      "title" : "LIBSVM: a library for support vector machines",
      "author" : [ "Chang", "C.-C", "Lin", "C.-J" ],
      "venue" : null,
      "citeRegEx" : "Chang et al\\.,? \\Q2001\\E",
      "shortCiteRegEx" : "Chang et al\\.",
      "year" : 2001
    }, {
      "title" : "Least angle regression",
      "author" : [ "B. Efron", "T. Hastie", "L. Johnstone", "R. Tibshirani" ],
      "venue" : "Annals of Statistics,",
      "citeRegEx" : "Efron et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Efron et al\\.",
      "year" : 2004
    }, {
      "title" : "Sensitivity analysis for nonlinear programming using penalty methods",
      "author" : [ "A.V. Fiacco" ],
      "venue" : "Mathematical Programming,",
      "citeRegEx" : "Fiacco,? \\Q1976\\E",
      "shortCiteRegEx" : "Fiacco",
      "year" : 1976
    }, {
      "title" : "Pathwise coordinate optimization",
      "author" : [ "J. Friedman", "T. Hastie", "H. Höfling", "R. Tibshirani" ],
      "venue" : "Annals of Applied Statistics,",
      "citeRegEx" : "Friedman et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Friedman et al\\.",
      "year" : 2007
    }, {
      "title" : "An exponential lower bound on the complexity of regularization",
      "author" : [ "B. Gärtner", "J. Giesen", "M. Jaggi" ],
      "venue" : "paths. CoRR,",
      "citeRegEx" : "Gärtner et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Gärtner et al\\.",
      "year" : 2009
    }, {
      "title" : "Approximating parameterized convex optimization problems",
      "author" : [ "J. Giesen", "M. Jaggi", "S. Laue" ],
      "venue" : "18th European Symposium on Algorithms,",
      "citeRegEx" : "Giesen et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Giesen et al\\.",
      "year" : 2010
    }, {
      "title" : "The entire regularization path for the support vector machine",
      "author" : [ "T. Hastie", "S. Rosset", "R. Tibshirani", "J. Zhu" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Hastie et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Hastie et al\\.",
      "year" : 2004
    }, {
      "title" : "Fast training of support vector machines using sequential minimal optimization",
      "author" : [ "J.C. Platt" ],
      "venue" : "Advances in Kernel Methods — Support Vector Learning,",
      "citeRegEx" : "Platt,? \\Q1999\\E",
      "shortCiteRegEx" : "Platt",
      "year" : 1999
    }, {
      "title" : "On parametric linear and quadratic programming problems",
      "author" : [ "K. Ritter" ],
      "venue" : "Mathematical Programming: Proceedings of the International Congress on Mathematical Programming,",
      "citeRegEx" : "Ritter,? \\Q1984\\E",
      "shortCiteRegEx" : "Ritter",
      "year" : 1984
    } ],
    "referenceMentions" : [ {
      "referenceID" : 4,
      "context" : "Recently, the solution path algorithm (Efron et al., 2004; Hastie et al., 2004; Cauwenberghs & Poggio, 2001) has been widely recognized as one of the effective tools in machine learning.",
      "startOffset" : 38,
      "endOffset" : 108
    }, {
      "referenceID" : 9,
      "context" : "Recently, the solution path algorithm (Efron et al., 2004; Hastie et al., 2004; Cauwenberghs & Poggio, 2001) has been widely recognized as one of the effective tools in machine learning.",
      "startOffset" : 38,
      "endOffset" : 108
    }, {
      "referenceID" : 1,
      "context" : "This technique is originally developed as parametric programming in the optimization community (Best, 1982).",
      "startOffset" : 95,
      "endOffset" : 107
    }, {
      "referenceID" : 7,
      "context" : "Although some empirical studies suggest that the number of breakpoints grows linearly in the input size, in the worst case, it can grow exponentially (Gärtner et al., 2009).",
      "startOffset" : 150,
      "endOffset" : 172
    }, {
      "referenceID" : 9,
      "context" : "In fact, one of the popular SVM optimization algorithm, called sequential minimal optimization (SMO) Platt (1999), is known to produce suboptimal (approximated) solution, where the tolerance to the optimality (degree of approximated) can be specified by users.",
      "startOffset" : 101,
      "endOffset" : 114
    }, {
      "referenceID" : 8,
      "context" : "1 Giesen et al. (2010) proposed approximated path algorithm with some optimality guarantee that can be applicable to L2-SVM without bias term.",
      "startOffset" : 2,
      "endOffset" : 23
    }, {
      "referenceID" : 9,
      "context" : "It can be derived from the KKT conditions (3) and the similar properties are repeatedly used in various solution path algorithms in machine learning (Cauwenberghs & Poggio, 2001; Hastie et al., 2004).",
      "startOffset" : 149,
      "endOffset" : 199
    }, {
      "referenceID" : 5,
      "context" : "This theorem can be viewed as one of the specific forms of the sensitivity theorem Fiacco (1976). It can be derived from the KKT conditions (3) and the similar properties are repeatedly used in various solution path algorithms in machine learning (Cauwenberghs & Poggio, 2001; Hastie et al.",
      "startOffset" : 83,
      "endOffset" : 97
    }, {
      "referenceID" : 11,
      "context" : "This situation can be interpreted as what is called degeneracy in the parametric programming (Ritter, 1984).",
      "startOffset" : 93,
      "endOffset" : 107
    }, {
      "referenceID" : 10,
      "context" : "This situation can be interpreted as what is called degeneracy in the parametric programming (Ritter, 1984). Here, degeneracy means that multiple constraints hit their boundaries of inequalities simultaneously. Although degenerate situation rarely happens in conventional solution path algorithms, it is not the case in ours. The simultaneous change of multiple data points inevitably brings about “highly” degenerate situations involved with many constraints. In degenerate case, we have a problem called the cycling. For example, if we move two indices i and j from M to O at the breakpoint, then both or either of them may immediately return to M. To avoid the cycling, we need to design an update strategy for π that can circumvent cycling. The degeneracy can be handled by several approaches which are known in the parametric programming literature. Ritter (1984) showed that the cycling can be dealt with through the well-known Bland’s minimum index rule in the",
      "startOffset" : 94,
      "endOffset" : 869
    }, {
      "referenceID" : 2,
      "context" : "linear programming (Bland, 1977).",
      "startOffset" : 19,
      "endOffset" : 32
    }, {
      "referenceID" : 0,
      "context" : "In this paper, we provide more essential solution to this problem based on (Berkelaar et al., 1997).",
      "startOffset" : 75,
      "endOffset" : 99
    } ],
    "year" : 2013,
    "abstractText" : "We consider a suboptimal solution path algorithm for the Support Vector Machine. The solution path algorithm is an effective tool for solving a sequence of a parametrized optimization problems in machine learning. The path of the solutions provided by this algorithm are very accurate and they satisfy the optimality conditions more strictly than other SVM optimization algorithms. In many machine learning application, however, this strict optimality is often unnecessary, and it adversely affects the computational efficiency. Our algorithm can generate the path of suboptimal solutions within an arbitrary user-specified tolerance level. It allows us to control the trade-off between the accuracy of the solution and the computational cost. Moreover, We also show that our suboptimal solutions can be interpreted as the solution of a perturbed optimization problem from the original one. We provide some theoretical analyses of our algorithm based on this novel interpretation. The experimental results also demonstrate the effectiveness of our algorithm.",
    "creator" : "dvips(k) 5.98 Copyright 2009 Radical Eye Software"
  }
}