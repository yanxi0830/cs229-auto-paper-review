{
  "name" : "1609.04495.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Tsallis Regularized Optimal Transport and Ecological Inference",
    "authors" : [ "Boris Muzellec", "Richard Nock", "Giorgio Patrini" ],
    "emails" : [ "boris.muzellec@polytechnique.edu", "richard.nock@data61.csiro.au", "giorgio.patrini@anu.edu.au", "Frank.Nielsen@acm.org" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Optimal transport (ot) allows to compare probability distributions by exploiting the underlying metric space on their supports [22, 26]. A number of prominent applications allow for a natural definition of this underlying metric space, from image processing [32] to natural language processing [25], music processing [13] and computer graphics [36].\nOne key problem of ot is its processing complexity — cubic in the support size, ignoring low order terms (on state of the art LP solvers [8]). Moreover, the optimal transportation plan has often many zeroes, which is not desirable in some applications. An important workaround was found and\nar X\niv :1\n60 9.\n04 49\n5v 1\n[ cs\nconsists in penalizing the transport cost with a Shannon entropic regularizer [8]. At the price of changing the transport distance, for a distortion with metric related properties, comes an algorithm with geometric convergence rates [8, 16]. As a result, we can picture two separate approches to ot: one essentially relies on the initial Monge-Kantorovitch formulation optimizing the transportation cost itself [39], but is computationally expensive; the other is based on tweaking the transportation cost by Shannon regularizer [8]. The corresponding optimization algorithm, grounded in a variety of different works [7, 34, 37], is fast and can be very efficiently parallelized [8].\nOur paper brings three contributions. (i) We interpolate these two worlds using a family of entropies celebrated in nonextensive statistical mechanics, Tsallis entropies [38], and hence we define the Tsallis regularized optimal transport (trot). We show that the metric properties for Shannon entropy still hold in this more general case, and prove new properties that are key to our application. (ii) We provide efficient optimization algorithms to compute trot and the optimal transportation plan. (iii) Last but not least, we provide a new application of trot to a field in which this optimal transportation plan is the key unknown: the problem of ecological inference.\nEcological inference deals with recovering information from aggregate data. It arises in a diversity of applied fields such as econometrics [6, 4], sociology and political science [23, 24] and epidemiology [40], with a long history [31]; interestingly, the empirical software engineering com-\nmunity has also explored the idea [28]. Its iconic application is inferring electorate behaviour: given turnout results for several parties and proportions of some population strata, e.g. percentages of ethnic groups, for many geographical regions such as counties, the aim is to recover contingency tables for parties × groups for all those counties. In the language of probability the problem is isomorphic to the following: given two random variables and their respective marginal distributions — conditioned to another variable, the geography —, compute their conditional joint distribution (See Figure 1).\nThe problem is fundamentally under-determined and any solution can only either provide loose deterministic bounds [12, 6, 4] or needs to enforce additional assumptions and prior knowledge on the data domain [23]. More recently, the problem has witnessed a period of renaissance along with the publication of a diversity of methods from the second family, mostly inspired by distributional assumptions as summarised in [24]. Closer to our approach, [21] follows the road of a minimal subset of assumptions and frame the inference as an optimization problem. The method favors one solution according to some information-theoretic solution, e.g. the Cressie-Read power divergence, intended as an entropic measure of the joint distribution.\nThere is an intriguing link between optimal transport and ecological inference: if we can figure out the computation of the ground metric, then the optimal transportation plan provides a solution to the ecological inference problem. This is appealing because it ties the computation of the joint distribution to a ground individual distance between people. Figure 1 gives an example. As recently advocated in ecological inference [14], it turns out that we have access to more and more side information that helps to solve ecological inference — in our case, the computation of this ground metric. Polls, census, social networks are as many sources of public or private data that can be of help. It is not our objective to show how to best compute the ground metric, but we show an example on real world data for which a simple approach gives very convincing results.\nTo our knowledge, there is no former application of optimal transport (regularized or not) to ecological inference. The closest works either assume that the joint distribution follows a random distribution constrained to structural or marginal constraints [15] (and references therein) or modify the constraints to the marginals and / or add constraints to the problem [11]. In all cases, there is no ground metric (or anything that looks like a cost) among supports that ties the computation of the joint distribution. More importantly, as noted in [14], traditional ecological inference would not use side information of the kind that would be useful to estimate our ground metric.\nThis paper is organized as follows. In Section § 2, we present the main definitions for ot. § 3 presents trot and its geometric properties. § 4 presents the algorithms to compute trot and the optimal transportation plan, and their properties. § 5 details experiments. A last Section concludes with open problems. All proofs, related comments, and some experiments are deferred to a Supplementary Material (sm)."
    }, {
      "heading" : "2 Basic definitions and concepts",
      "text" : "In the following, we let 4n . = {x ∈ Rn+ : x>1 = 1} denote the probability simplex (bold faces like x denote vectors). 〈P,Q〉 .= vec(P )>vec(Q) denotes Frobenius product (vec(.) is the vectorization of a matrix). For any two r, c ∈ 4n, we define their transportation polytope U(r, c) . = {P ∈ Rn×n+ : P1 = r, P>1 = c}. For any cost matrix M ∈ Rn×n, the transportation distance between r and c\nas the solution of the following minimization problem:\ndM (r, c) .\n= min P∈U(r,c)\n〈P,M〉 . (1)\nIts argument, P ? .\n= arg minP∈U(r,c)〈P,M〉 is the (optimal) transportation plan between r and c. Assuming M 6= 0, P ? is unique. Furthermore, if M is a metric matrix, then dM is also a metric [39, §6.1].\nIn current applications of optimal transport, the key unknown is usually the distance dM [8, 9, 19, 29, 36] (etc). In the context of ecological inference [21], it is rather P ?: P ? describes a joint distribution between two discrete random variables R and C with respective marginals r and c, p?ij = Pr(R = ri ∧ C = cj), for example the support of R being the votes for year Y US presidential election, and C being the ethnic breakdown in the US population in year Y , see Figure 1. In this case, p?ij denotes an ”ideal” joint distribution of votes within ethnicities, ideal in the sense that it minimizes a distance based on the belief that votes correlate positively with a similarity between an ethnic profile and a party’s profile. While we will carry out most of our theory on formal transportation grounds, requiring in particular that M be a distance matrix, it should be understood that requiring just ”correlation” alleviates the need for M to formally be a distance for ecological inference."
    }, {
      "heading" : "3 Tsallis Regularized Optimal Transport",
      "text" : "For any p ∈ Rn+, q ∈ R, the Tsallis entropy of p, Hq(p) is:\nHq(p) . = 1 1− q · ∑ i (pqi − pi) , (2)\nand for any P ∈ Rn×n+ , we letHq(P ) . = Hq(vec(P )). Notably, we have limq→1Hq(p) = − ∑ i pi ln pi .\n= H1(p), which is just Shannon’s entropy. For any λ > 0, we define the Tsallis Regularized Optimal Transport (trot) distance.\nDefinition 1 The trot(q, λ,M) distance (or trot distance for short) between r and c is:\ndλ,qM (r, c) .\n= min P∈U(r,c) 〈P,M〉 − 1 λ ·Hq(P ) . (3)\nA simple yet important property is that trot distance unifies both usual modalities of optimal transport. It generalizes optimal transport (ot) when q → 0, since Hq converges to a constant and so the ot-distance is obtained up to a constant additive term [22, 26]. It also generalizes the regularized optimal transport approach of [8] since limq→1 d λ,q M (r, c) = d λ M (r, c), the Sinkhorn distance between r and c [8]. There are several important structural properties of dλ,qM that motivate the unification of both approaches. To state them, we respectively define the q-logarithm,\nlogq(x) . = (1− q)−1 · (x1−q − 1) , (4)\nthe q-exponential, expq(x) . = (1 + (1 − q) · x)1/(1−q) and Tsallis relative q-entropy between P,R ∈ Rn×n+ as:\nKq(P,R) . = 1 1− q · ∑ i,j ( qpij + (1− q)rij − pqijr 1−q ij ) . (5)\nTaking joint distribution matrices P,R and q → 1 allows to recover the natural logarithm, the exponential and Kullback-Leibler (kl) divergence, respectively [1]. Other notable examples include (i) Pearson’s χ2 statistic (q = 2), (ii) Neyman’s statistic (q = −1), (iii) square Hellinger distance (q = 1/2) and the reverse kl divergence if scaled appropriately by q [21], which also allows to span Amari’s α divergences for α = 1− 2q [1]. For any function f : R→ R, denoting f(P ) for matrix P as the matrix whose general term is f(pij).\nLemma 2 Let Ũ . = expq(−1) exp−1q (λM). Then:\ndλ,qM (r, c) = 1\nλ · min P∈U(r,c) K1/q(P q, Ũ q) + g(M) , (6)\nwhere g(M) .\n= (1/λ) · 〈Ũ q, 1〉 does not play any role in the minimization of K1/q(.‖.).\nLemma 2 shows that the trot distance is a divergence involving escort distributions [1, § 4], a particularity that disappears in Sinkhorn distances since it becomes an ordinary kl divergence between distributions. Predictably, the generalization is useful to create new solutions to the regularized optimal transport problem that are not captured by Sinkhorn distances (solution refers to (optimal) transportation plans, i.e. the argument of the min in eq. (3)).\nTheorem 3 Let Sλ,q(r, c) denote the set of solutions of eq. (3) when M ranges over all distance matrices. Then ∀q, q′ such that q 6= q′, ∀λ, λ′, Sλ,q(r, c) 6= Sλ′,q′(r, c).\nFigure 2 provides examples of solutions. Adding the free parameter q is not just interesting for the reason that we bring new solutions to the table: (1/q) ·Kq(p, r) turns out to be Cressie-Read Power Divergence (for q = λ + 1, [21]), and so trot has an applicability in ecological inference that Sinkhorn distances alone do not have. In addition, we also generalize two key facts already known for Sinkhorn distances [8]. First, the solution to trot is unique (for q 6= 0) and satisfies a simple analytical expression amenable to convenient optimization.\nTheorem 4 There exists exactly one matrix P ∈ U(r, c) solution to trot(q, λ,M). It satisfies:\npij = expq(−1) exp−1q (αi + λmij + βj) ,∀i, j . (7)\n(α,β ∈ Rn are unique up to an additive constant).\nSecond, we can tweak trot to meet distance axioms. Let\ndM,α,q(r, c) .\n= min P∈U(r,c)\nHq(P )−Hq(r)−Hq(c)≥α\n〈P,M〉 , (8)\nwhere α ≥ 0. For any M, r, c, λ ≥ 0, ∃α ≥ 0 such that dM,α,q(r, c) = dλ,qM (r, c). Also, the following holds.\nTheorem 5 For q ≥ 1, α ≥ 0 and if M is a metric matrix, function (r, c)→ 1{r=c}dM,α,q(r, c) is a distance.\nTheorem 5 is a generalization of [8, Theorem 1] (for q = 1). As we explain more precisely in sm (Section 10), there is a downside to using dM,α,q as proof of the good properties of d λ,q M : the triangle inequality, key to Euclidean geometry, transfers to dλ,qM with varying and uncontrolled parameters — in the inequality, the three values of λ may all be different! This does not break down the good properties of dλ,qM , it just calls for workarounds. We now give one, which replaces dM,α,q by the quantity (β ∈ R is a constant):\ndλ,q,βM (r, c) . = dλ,qM (r, c) + β\nλ · (Hq(r) +Hq(c)) . (9)\nThis has another trivial advantage that dM,α,q does not have: the solutions (optimal transportation plans) are always the same on both sides. Also, the right-hand side is lowerbounded for any r, c and the trick that ensures the identity of the indiscernibles still works on dλ,q,βM . The good news is that if q = 1, dλ,q,βM , as is, can satisfy the triangle inequality.\nTheorem 6 dλ,1,βM satisfies the triangle inequality, ∀β ≥ 1.\nHence, the solutions to dλ,1M are optimal transport plans for distortions that meet the triangle inequality. This is new compared to [8]. For a general q ≥ 1, the proof, in Supplementary Material (Section 10), shows more, namely that d\nλ,q,1/2 M satisfies a weak form of the identity of the\nindiscernibles. Finally, there always exist a value β ≥ 0 such that dλ,q,βM is non negative (d λ,q,β M is lowerbounded ∀β ≥ 0)."
    }, {
      "heading" : "4 Efficient trot optimizers",
      "text" : "The key idea behind Sinkhorn-Cuturi’s solution is that the KKT conditions ensure that the optimal transportation plan P ? satisfies P ? = diag(u) exp(−λM)diag(v). Sinkhorn’s balancing normalization can then directly be used for a fast approximation of P ? [34, 33]. This trick does not fit at first sight for Tsallis regularization because the q-exponential is not multiplicative for general q and KKT conditions do not seem to be as favorable. We give however workarounds for the optimization, that work for any q ∈ R+.\nFirst, we assume wlog that q 6= 0, 1 since in those cases, any efficient LP solver (q = 0) or Sinkhorn balancing normalization (q = 1) can be used. The task is non trivial because for q ∈ (0, 1), the function minimized in dλ,qM is not Lipschitz, which impedes the convergence of gradient methods. In this case, our workaround is Algorithm 1 (so–trot), which relies on a Second Order approximation of a fundamental quantity used in its convergence proof, auxiliary functions [10].\nAlgorithm 1 Second Order Row–trot (so–trot) Input: marginal r, matrix M , params λ ∈ R+∗, q ∈ (0, 1) 1: A← λM 2: P ← expq(−1) exp−1q (A) 3: repeat 4: P1 ← P A,P2 ← P1 A // = Kronecker divide 5: d← r − P1, b← P11,a← (2− q)P21 6: for i = 1, 2, ..., n 7: if di ≥ 0 then 8: yi ← −bi+ √ b2i+4aidi\n2ai 9: else\n10: yi ← di/bi 11: end if 12: if |yi| > q\n(6−4q)·maxj p1−qij then\nyi ← q · sign(ri −\n∑ j pij)\n(6− 4q) ·maxj p1−qij . (10)\n13: A← A− y1> 14: P ← expq(−1) exp−1q (A) 15: until convergence\nOutput: P\nTheorem 7 (Convergence of so–trot) For any fixed q ∈ (0, 1), matrix P output by so–trot converges to P ? with:\nP ? = arg min P∈Rn×n+ :P1=r K1/q(P q, Ũ q) .\nThe proof (in Supplementary Material, Section 11) is involved but interesting in itself because it represents one of the first use of the theory of auxiliary functions outside the realm of Bregman\nAlgorithm 2 KL Projected Gradient –trot (kl–trot)\nInput: Marginals r, c, Matrix Ũ , Gradient steps {tk} 1: P (0) ← Ũ 2: repeat 3: P (k+1) ← SK(P (k) ⊗ exp(−tk∇fq(P (k))), r, c) 4: until convergence\ndivergences in machine learning [5, 10]. Some important remarks should be made. First, since so–trot uses only one of the two marginal constraints, it would need to be iterated (”wrapped”), swapping the row and column constraints like in Sinkhorn balancing. In practice, this is not efficient. Furthermore, iterating so–trot over constraint swapping does not necessarily converge. For these reasons, we swap constraints in the algorithm, making one iteration of Steps 4-14 over rows, and then one iteration of Steps 4-14 over columns (this boils down to transposing matrices in so–trot), and so on. This converges, but still is not the most efficient. To improve efficiency we perform two modifications, that do not impede convergence experimentally. First, we remove Step 12. In doing so, we not only save O(n2) computations for each outer loop, we essentially make so–trot as parallelizable as Sinkhorn balancing [8]. Second, we remarked experimentally that convergence is faster when multiplying yi by 2 in Step 10, and dividing a by 2 in Step 5.\nFor simplicity, we still refer to this algorithm (balancing constraints in the algorithm, with the modifications for Steps 5, 10, 12) as so–trot in the experiments.\nLast, when q ≥ 1, the function minimized in dλ,qM becomes Lipschitz. In this case, we take the particular geometry of U(r, c) into account by using mirror gradient methods, which are equivalent to gradient methods projected according to some suitable divergence [2]. In our case, we consider Kullback-Leibler divergence, which can save a factor O(n/ √ log n) iterations [2]. Furthermore, the Kullback-Leibler projection can be written in terms of Sinkhorn-Knopp’s (SK) algorithm with marginals constraints r, c [35], as is shown in Algorithm 2, named kl–trot (⊗ is Kronecker product). Theorem 8 If q > 1 and the gradient steps {tk} are s.t. ∑ k tk → ∞ and ∑ k t 2 k ∞, matrix P output by kl–trot converges to P ? with:\nP ? = arg min P∈U(r,c) K1/q(P q, Ũ q) .\n(proof omitted, follows [2, 35])"
    }, {
      "heading" : "5 Experiments",
      "text" : "We evaluate empirically the trot framework with its application to ecological inference. The dataset we use describes about 10 millions individual voters from Florida for the 2012 US presidential elections, as obtained from [20]. The data is much richer than is required for ecological inference: surely we could estimate the joint distribution of every voters’ available attributes by counting. This is itself a particularly rare case of data quality in political science, where any analysis is often carried out on aggregate measurements. In fact, since ground truth distributions are effectively available, the Florida dataset has been used to test methodological advances in the field\n[14, 20]. As a demonstrative example, we focus on inferring the distributions of ethnicity and party for all Florida counties.\nDataset description and preprocessing. The data contains the following attributes for each voter : location (district, county), gender, age, party (Democrat, Republican, Other), ethnicity (White, African-american, Hispanic, Asian, Native, Other), 2008 vote (yes, no). About 800K voters with missing attributes are excluded from the study. Thanks to the richness of the data, marginal probabilities of ethnic groups and parties can be obtained by counting: for each county we obtain marginals r, c for the optimal transport problems.\nEvaluation assumptions. Two assumptions are made in terms of information available for inference. First, the ground truth joint distributions for one district are known; we chose district number 3 which groups 9 out of 68 counties of about 285K voters in total. This information will be used to tune hyper-parameters. Second, a cost matrix M rbf is computed based on mean voter’s attributes at state level. For the sake of simplicity, we retain only age (normalized in [0, 1]), gender and the 2008 vote; notice that in practice geographical attributes may encode relevant information for computing distances between voter behaviours [14]. We do not use this. For distance matrix M rbf, we aggregate those features over all Florida for each party to obtain the vectors µp of the party’s expected profile and for each ethnic group to obtain the vectors µe of the ethnicity’s expected profile. The dissimilarity measure relies on a Gaussian kernel between average county profiles:\nmrbfij .\n= √\n2− 2 exp(−γ · ‖µpi − µej‖2) , (11)\nwith γ = 10. The given function is actually the Hilbert metric in the RBF space. Table 1 shows the resulting cost matrix. Notice how it does encode some common-sense knowledge: White and Republican is the best match, while Hispanic and Asians are the worst match with Republican profiles. It is rather surprising that only 3 features such as age, gender and whether people voted at the last election can reflect so well those relative political traits; these results are indeed much in line with survey-based statistics [18]. We also try another cost matrix M , M sur, derived from the ID proportions of parties composition given in [18]; msurij is computed as 1 − pij , where pij is the proportion of people registered to party j belonging to ethnic group i. Finally, we consider a ”no prior” matrix Mno, in which mnoij = 1,∀i, j.\nCross-validation of q. We study the solution of trot for a grid of λ ∈ [0.01, 1000], q ∈ [0.5, 4], inferring the joint distributions of all counties of district number 3. We measure average KLdivergence between inferred and ground truth joint distributions. Notice that each county defines a different optimal transport problem; inferring the joint distributions for multiple counties at a time is therefore trivial to parallelize. This is somewhat counter-intuitive since we may believe that geographically wider spread data should improve inference at a local level, that is, more data better inference. Indeed, the implicit coupling of the problem is represented by cost matrix, which expresses some prior knowledge of the problem by means of all data from Florida.\nBaselines and comparisons with other methods. To evaluate quantitatively the solution of trot is useful to define a set of baseline methods: i) Florida-average, which the same state-level joint distribution (assumed prior knowledge) for each of the 67 county; ii) Simplex, that is the solution of optimal transport with no regularization as given by the Simplex algorithm; iii) Sinkhorn(Cuturi)’s algorithm, which is trot with q = 1; iv) trot. ii-iv are tested with M ∈ {M rbf,M sur}, and we provide in addition the results for trot with M = Mno. Hyper-parameters are crossvalidated independently for each algorithm.\nTable 2 reports a quantitative comparison. From the most general to the most specific, there are three remarks to make. First, optimal transport can be (but is not always) better than the default distribution (Florida average). Second, regularizing optimal transport consistently improves upon these baselines. Third, trot successfully matches Sinkhorn’s approach when q = 1 is be the best solution in trot’s range of q (M = M rbf), and manages to tune q to significantly beat Sinkhorn’s when better alternatives exist: with M = M sur, trot divides the expected KL divergence by more than seven (7) compared to Sinkhorn. This is a strong advocacy to allow for the tuning of q. Notice that in this case, λ is larger compared to M = M rbf, which makes sense since M = M sur is more accurate for the optimal transport problem (see the Simplex results) and so the weight of the regularizer predictably decreases in the regularized optimal transport distance. We conjecture that M = M sur beats M = M rbf in part because it is somehow finer grained: M rbf is computed from sufficient statistics for the marginals alone, while M sur exploits information computed from the cartesian product of the supports. Figure 3 compares all 1 836 inferred probabilities (3× 6 per county) with respect to the ground truth for Sinkhorn vs trot using M = M sur. Remark that the figures in Table 2 translate to per-county ecological inference results that are significantly more in favor of trot, which basically has no ”hard-to-guess” counties compared to Sinkhorn for which the absolute difference between inference and ground truth can exceed 10%.\nTo finish up, additional experiments, displayed in sm (Sections 12 and 13) also show that trot with M = M sur manages to have a distribution of per county errors extremely peaked around zero error, compared to the simplest baselines (Florida average and trot with M = Mno). These are good news, but there are some local discrepancies. For example, there exists one county on which trot with M = M sur is beaten by trot with M = Mno."
    }, {
      "heading" : "6 Discussion and conclusion",
      "text" : "In this paper, we have bridged Shannon regularized optimal transport and unregularized optimal transport, via Tsallis entropic regularization. There are three main motivations to the generalization, the two first have already been discussed: trot allows to keep the properties of Sinkhorn distances, and fields like ecological inference bring natural applications for the general trot family. The application to ecological inference is also interesting because the main unknown is the optimal transportation plan and not necessarily the transportation distance obtained. The third and last motivation is important for applications at large and ecological inference in particular. trot spans a subset of f -divergences, and f -divergences satisfy the information monotonicity property that coarse graining does not increase the divergence [1, § 3.2]. Furthermore, f -divergences are invariant under diffeomorphic transformations [30, Theorem 1]. This is a powerful statement: if the ground metric is affected by such a transformation h (for example, we change the underlying manifold coordinate system, e.g. for privacy reasons), then, from the optimal trot transportation plan P ?, the transportation plan corresponding to the initial coordinate system can be recovered from the sole knowledge of h−1.\nThe algorithms we provide allow for the efficient optimization of the regularized optimal transport for all values of q ≥ 0, and include notable cases for which conventional gradient-based ap-\nproaches would probably not be the best approaches due to the fact that the function to optimize is not Lipschitz for the q chosen. In fact, the main notable downside of the generalization is that we could not prove the same (geometric) convergence rates as the ones that are known for Sinkhorn’s approach [16].\nOur results display that there can be significant discrepancies in the regularized optimal transport results depending on how cost matrix M is crafted, yet the information we used for our best experiments is readily available from public statistics (matrices M rbf,M sur). Even the instantiation without prior knowledge (M = 11>) does not strictly fail in returning useful solutions (compared e.g. to Florida average and unregularized optimal transport). This may be a strong advocacy to use trot even on domains for which little prior knowledge is available."
    }, {
      "heading" : "Acknowledgments",
      "text" : "The authors wish to thank Seth Flaxman and Wendy K. Tam Cho for numerous stimulating discussions. Work done while Boris Muzellec was visiting Nicta / Data61. Nicta was funded by the Australian Government through the Department of Communications and the Australian Research Council through the ICT Center of Excellence Program."
    }, {
      "heading" : "7 Supplementary Material — Table of contents",
      "text" : "Supplementary material on proofs Pg 16 Proof of Theorem 3 Pg 16 Proof of Theorem 4 Pg 17 Proof of Theorems 5 and 6 Pg 18 Proof of Theorem 7 Pg 23\nSupplementary material on experiments Pg 29 Per county error distribution, trot survey vs Florida average Pg 29 Per county errors, trot survey vs trot 11> Pg 29\nSupplementary Material: proofs"
    }, {
      "heading" : "8 Proof of Theorem 3",
      "text" : "Let M ∈ Rn×n+ be a distance matrix, and q, q′ ∈ R−{1}, q 6= q′ (the case when q = 1 xor q′ = 1 can be treated in a similar fashion). We suppose wlog that the support does not reduce to a singleton (otherwise the solution to optimal transport is trivial). Rescaling M and a constant row vector and a constant column vector, the solution of trot(q, λ,M) can be written wlog as\npij = expq(−1) exp−1q (mij) . (12)\nAssume there exists a λ′ ∈ R such that the solution of trot(q′, λ′,M) is equal to that of trot(q, λ,M). This is equivalent to saying that there exists α,β ∈ Rn such that\nexpq(mij) = expq′(αi + λ ′mij + βj) ,∀i, j . (13)\nComposing with logq′ and rearranging, this implies that\nfλ ′\nq′,q(mij) = αi + βj , ∀i, j , (14)\nwhere\nfλ ′ q′,q(x) . = logq′ ◦ expq −λ′Id . (15)\nNow, remark that, since M is a distance, mii = 0, ∀i because of the identity of the indiscernibles, and so αi + βi = f λ′ q′,q(0) = 0, implying α = −β. fλ ′ q′,q is differentiable. Let:\ngλ ′ q′,q(x) . =\nd\ndx fλ ′ q′,q(x)\n= expq−q ′ q (x)− λ′ ; (16)\nhλ ′ q′,q(x) . =\nd\ndx gλ ′ q′,q(x)\n= (q − q′) · exp2q−q′−1q (x) . (17)\nIf we assume wlog that q > q′, then gλ ′ q′,q is increasing and zeroes at most once over R, eventually on some m∗ that we define as m∗ = logq ( λ′ 1 q−q′ ) if (λ′ > 1)∧(0 ∈ Imgλ′q′,q) (and +∞ otherwise). Notice that m∗ > 0 and fλ ′\nq′,q is bijective over (0,m ∗). Suppose wlog that mij ≤ m∗, ∀i, j. Otherwise, all\ndistances are scaled by the same real so that mij ≤ m∗,∀i, j: this does not alter the property of M being a distance. A distance being symmetric, we also have mij = mji and since f λ′ q′,q is strictly increasing in the range of distances, then we get from eq. (14) that αi + βj = αj + βi, ∀i, j and so αi − αj = βi − βj = −(αi − αj) (since α = −β). Hence, there exists a real α such that α = α · 1. We get, in matrix form\nfλ ′ q′,q(M) = α1 > + 1β> (18)\n= α · 11> − α · 11> = 0 . (19)\nHence, mij = mii,∀i, j and the support reduces to a singleton (because of the identity of the indiscernibles), which is impossible.\nRemark that the proof also works when M is not a distance anymore, but for example contains all arbitrary non negative matrices. To see this, we remark that the right hand side of eq. (18) is a matrix of rank no larger than 2. Since fλ\n′ q′,q is continuous, we have\nIm(fλ ′ q′,q) . = I ⊆ R\nwhere I is not reduced to a singleton and so the left hand side of eq. (18) spans matrices of arbitrary rank. Hence, eq. (18) cannot always hold."
    }, {
      "heading" : "9 Proof of Theorem 4",
      "text" : "Denote\nfij : pij → pijmij − 1\nλ(1− q) (pqij − pij) .\nfij is twice differentiable on R+∗, and\nd2\ndx2 fij(x) =\nq λ xq−2 > 0\nfor any fixed q > 0, and so fij is strictly convex on R+∗. We also remark that U(r, c) is a nonempty compact subset of Rn×n. Indeed, rc> ∈ U(r, c), ∀P ∈ U(r, c), ‖P‖1 = 1 (which proves boundedness) and U(r, c) is a closed subset of U(r, c) (being the intersection of the pre-images of singletons by continuous functions). Hence, since 〈P,M〉 − 1λHq(P ) = ∑ i,j fij(pij), there exists a unique minimum of this function in U(r, c).\nTo prove the analytic shape of the solution, we remark that trot(q, λ,M) consists in minimizing a convex function given a set of affine constraints, and so the KKT conditions are necessary and sufficient. The KKT conditions give\npij = expq(−1) exp−1q (αi + λmij + βj) ,\nwhere α,β ∈ Rn are Lagrange multipliers. Finally, let us show that Lagrange multipliers α,β ∈ Rn are unique up to an additive constant. Assume that α,α′,β,β′ ∈ Rn are such that\n∀i, j, pij = expq(−1) exp−1q (λmij + αi + βj) = expq(−1) exp−1q (λmij + α′i + β′j) ,\nwhere P is the unique solution of trot(q, λ,M). This implies\nαi + βj = α ′ i + β ′ j , ∀i, j ,\ni.e.\nαi − α′i = β′j − βj , ∀i, j .\nIn particular, if there exists i0 and C 6= 0 such that αi0 − α′i0 = C, then ∀j, β ′ j = βj + C and in turn ∀i, αi = α′i + C, which proves our claim."
    }, {
      "heading" : "10 Proof of Theorems 5 and 6",
      "text" : "For reasons that we explain now, we will in fact prove Theorem 6 before we prove Theorem 5. Had we chosen to follow [8], we would have replaced trot(q, λ,M) by:\ndM,α,q(r, c) .\n= min P∈U(r,c)\nHq(P )−Hq(r)−Hq(c)≥α\n〈P,M〉 , (20)\nfor some α > 0. Both problems are equivalent since λ in trot(q, λ,M) plays the role of the Lagrange multiplier for the entropy constraint in eq. (20) [8, Section 3], and so there exists an equivalent value of α∗ for which both problems coincide:\ndM,α∗,q(r, c) = d λ,q M (r, c) , (21)\nso eq. (20) indeed matches trot(q, λ,M). It is clear from eq. (21) that α does not depend solely on λ, but also (eventually) on all other parameters, including r, c.\nThis would not be a problem to state the triangle inequality for dM,α,q, as in [8] (∀x,y, z ∈ 4n):\ndM,α,q(x, z) ≤ dM,α,q(x,y) + dM,α,q(y, z) . (22)\nHowever, α is fixed and in particular different from the α∗ that guarantee eq. (21) — and there might be three different sets of parameters for dλ,qM as it would equivalently appear from eq. (22). Under the simplifying assumption that only λ changes, we might just get from eq. (22):\ndλ ∗,q M (x, z) ≤ d λ′∗,q M (x,y) + d λ′′∗,q M (y, z) , (23)\nwith λ∗ 6= λ′∗ 6= λ′′∗. Worse, the transportation plans may change with λ: for example, we may have\narg min P∈U(x,z) dλ1,qM (x, z) 6= arg min P∈U(x,z) dλ2,qM (x, z) ,\nwith λ1 6= λ2 and λ1, λ2 ∈ {λ∗, λ′∗, λ′′∗}. So, the triangle inequality for dλ,qM that follows from ineq. (22) does not allow to control the parameters of trot(q, λ,M) nor the optimal transportation plans that follows. It does not show a problem in regularizing the optimal transport distance, but rather that the distance dM,α,q chosen from eq. (21) does not completely fulfill its objective in showing that regularization in dλ,qM still keeps some of the attractive properties that unregularized optimal transport meets.\nTo bypass this problem and establish a statement involving a distance in which all parameters are in the clear and optimal transportation plans still coincide with dλ,qM , we chose to rely on measure:\ndλ,q,βM (r, c) .\n= min P∈U(r,c)\n〈P,M〉\n− 1 λ · (Hq(P )− β · (Hq(r) +Hq(c))) ,\nwhere β is some constant. There is one trivial but crucial fact about dλ,q,βM (r, c): regardless of the choice of β, its optimal transportation plan is the same as for trot(q, λ,M).\nLemma 9 For any r, c ∈ 4n and constant β ∈ R, let\nP1 .\n= arg min P∈U(r,c)\n〈P,M〉\n− 1 λ · (Hq(P )− β · (Hq(r) +Hq(c))) . (24)\nP2 .\n= arg min P∈U(r,c)\n〈P,M〉\n− 1 λ · (Hq(P )) . (25)\nThen P1 = P2.\nTheorem 10 The following holds for any fixed q ≥ 1 (unless otherwise stated):\n• for any β ≥ 1, dλ,1,βM satisfies the triangle inequality;\n• for the choice β = 1/2, dλ,q,1/2M satisfies the following weak version of the identity of the indiscernibles: if r = c, then d\nλ,q,1/2 M (r, c) ≤ 0.\n• for the choice β = 1/2, ∀r ∈ 4n, choosing the (no) transportation plan P = Diag(r) brings 〈P,M〉 − 1 λ · ( Hq(P )− 1 2 · (Hq(r) +Hq(r)) ) = 0 .\nRemark: the last property is trivial but worth stating since the (no) transportation plan P = Diag(r) also satisfies P = arg minQ∈U(r,r)〈Q,M〉, which zeroes the (no) transportation distance dM (r, r). Proof To prove the Theorem, we need another version of the Gluing Lemma with entropic constraints [8, Lemma 1], generalized to handle Tsallis entropy.\nLemma 11 (Refined gluing Lemma) Let x,y, z ∈ 4n. Let P ∈ U(x,y) and Q ∈ U(y, z). Let S ∈ Rn×n defined by general term\nsik . = ∑ j pijqjk yj . (26)\nThe following holds about S:\n1. S ∈ U(x, z);\n2. if q ≥ 1, then:\nHq(S)−Hq(x)−Hq(z) ≥ Hq(P )−Hq(x)−Hq(y) . (27)\nProof The proof essentially builds upon [8, Lemma 1]. We remark that S can be built by sik = ∑ j tijk , (28)\nwhere ∀i, j, k ∈ {1, 2, ..., n}, we have\ntijk . = pijqjk yj\n(29)\nif yj 6= 0 (and tijk = 0 otherwise) S is a transportation matrix between x and z. Indeed,∑\ni ∑ j sijk = ∑ j ∑ i pijqjk yj\n= ∑ j qjk yj ∑ i pij\n= ∑ j qjk yj yj = ∑ j\nqjk = zk ;∑ k ∑ j sijk = ∑ j ∑ k pijqjk yj\n= ∑ j pij yj ∑ k qjk\n= ∑ j pij yj yj = ∑ j pij = xi .\nSo, S ∈ U(x, z). To prove ineq. (27), we need the following definition from [17].\nDefinition 12 [17] Let X and Y denote random variables. The Tsallis conditional entropy of X given Y, and Tsallis joint entropy of X and Y, are respectively given by:\nHq(X|Y) . = − ∑ x,y p(x, y)q logq p(x|y) , Hq(X,Y) .\n= − ∑ x,y p(x, y)q logq p(x, y) .\nThe Tsallis mutual entropy of X and Y is defined by\nIq(X;Y) .\n= Hq(X)−Hq(X|Y) = Hq(X) +Hq(Y)−Hq(X,Y) .\nWe have made use of the simplifying notation that removes variables names when unambiguous, like p(x) . = p(X = x). Let X,Y,Z be random variables jointly distributed as T , that is, for any x, y, z,\np(x, y, z) = p(x, y)p(y, z)\np(y) (30)\nIt follows from that and Bayes rule that:\np(x|y) = p(x, y) p(y)\n= p(x, y, z)\np(y, z) , ∀z\n= p(x|y, z) , ∀z , (31)\nand so\nIq(X;Z|Y) .\n= Hq(X|Y)−Hq(X|Y,Z) = 0 . (32)\nIt comes from [17, Theorem 4.3],\nIq(X;Y,Z) = Iq(X;Z) + Iq(X;Y|Z) (33) = Iq(X;Y) + Iq(X;Z|Y) , (34)\nbut since Iq(X;Z|Y) = 0, we obtain\nIq(X;Y) = Iq(X;Z) + Iq(X;Y|Z) . (35)\nIt also follows from [17, Theorem 3.4] that Iq(X;Y|Z) ≥ 0 whenever q ≥ 1, and so\nIq(X;Y) ≥ Iq(X;Z) , ∀q ≥ 1 . (36)\nNow, it comes from Definition 12 and the definition of X,Y and Z from eq. (30),\n−Iq(X;Y) = Hq(X,Y)−Hq(X)−Hq(Y) = Hq(P )−Hq(x)−Hq(y) , (37) −Iq(X;Z) = Hq(X,Z)−Hq(X)−Hq(Z) = Hq(S)−Hq(x)−Hq(z) . (38)\nSince P ∈ Uλ(x,y), by assumption, we obtain from ineq. (36) that whenever q ≥ 1,\nHq(S)−Hq(x)−Hq(z) ≥ Hq(P )−Hq(x)−Hq(y) ,\nas claimed.\nWe can now prove Theorem 10. Shannon’s entropy is denoted H1 for short. Define for short\n∆ . = H1(P ) +H1(Q)−H1(S)− 2β ·H1(y) , (39)\nwhere P,Q, S are defined in Lemma 11. It follows from the definition of S and [8, Proof of Theorem 1] that\ndλ,q,βM (x, z)\n. = min R∈U(x,z) 〈R,M〉 − 1 λ · (H1(R)− β · (H1(x) +H1(z)))\n≤ 〈S,M〉 − 1 λ · (H1(S)− β · (H1(x) +H1(z))) ≤ 〈P,M〉+ 〈Q,M〉 − 1 λ · (H1(S)− β · (H1(x) +H1(z)))\n= 〈P,M〉 − 1 λ · (H1(P )− β · (H1(x) +H1(y))) +〈Q,M〉 − 1 λ · (H1(Q)− β · (H1(y) +H1(z))) + 1\nλ · (H1(P ) +H1(Q)−H1(S)− 2β ·H1(y))\n. = dλ,q,βM (x,y) + d λ,q,β M (y, z) +\n1 λ ·∆ . (40)\nWe now show that ∆ ≤ 0. For this, observe that ineq. (27) yields:\n∆\n≤ (H1(S) +H1(y)−H1(z)) +H1(Q)−H1(S)− 2β ·H1(y) = H1(Q)−H1(y)−H1(z) + 2(1− β)H1(y) , (41)\nand, by definition of Q,y, z,\nH1(Q)−H1(y)−H1(z) .\n= H1(Y,Z)−H1(Y)−H1(Z) . (42)\nShannon’s entropy of a joint distribution is maximal with independence: H1(Y,Z) ≤ H1(Y × Z) = H1(Y) +H1(Z), so we get from eq. (41) after simplifying\n∆ ≤ 2(1− β)H1(y) . (43)\nHence if β ≥ 1, then ∆ ≤ 0. We get that for any β ≥ 1,\ndλ,1,βM (x, z) ≤ d λ,1,β M (x,y) + d λ,1,β M (y, z) , (44)\nand dλ,1,βM satisfies the triangle inequality. For β = 1/2, it is trivial to check that for any x ∈ 4n, the (no) transportation plan P = Diag(x) is in U(x,x) and satisfies\n〈P,M〉 − 1 λ · ( Hq(P )− 1 2 · (Hq(x) +Hq(x)) ) = 0− 1\nλ · (Hq(x)−Hq(x)) = 0 . (45)\nThis ends the proof of Theorem 10.\nNotice that Theorem 6 is in fact a direct consequence of Theorem 10. To finish up, we now prove Theorem 5. To simplify notations, let\nUα(r, c) . = {P ∈ U(r, c) : Hq(P )−Hq(r)−Hq(c) ≥ α(λ)} . (46)\nSuppose P,Q in Lemma 11 are such that P,Q ∈ Uλ(x,y). In this case,\nHq(P )−Hq(x)−Hq(y) ≥ α (47)\nand so point 2. in Lemma 11 brings\nHq(S)−Hq(x)−Hq(z) ≥ α , (48)\nso S ∈ Uλ(x, z). The proof of [8, Theorem 1] can then be used to show that ∀x,y, z ∈ 4n,\ndM,α,q(x, z) ≤ dM,α,q(x,y) + dM,α,q(y, z) . (49)\nIt is easy to check that dM,α,q is non negative and that 1{r=c}dM,α,q(r, c) meets, in addition, the identity of the indiscernibles. This achieves the proof of Theorem 5."
    }, {
      "heading" : "11 Proof of Theorem 7",
      "text" : "Basic facts and definitions — In this proof, we make two simplifying assumptions: (i) we consider matrices either as matrices or as vectorized matrices without ambiguity, and (ii) we let φ(P ) . = −Hq(P ), noting that the domain of φ is 4n2 (nonnegative matrices with row- and columnsums in the simplex) when P ∈ U(r, c). Since φ is convex, we can define a Bregman divergence with generator Dφ [3] as:\nDφ(P‖R) . = φ(P )− φ(R)− 〈∇φ(R), P −R〉 .\nWe define\naij . = αi + λmij + βj , (50)\nso that\npij = expq(−1) exp−1q (aij) (51)\nin eq. (7). Finally, let us denote for short\nDq(P‖R) . = K1/q(P q, Rq) , (52)\nso that we can, reformulate eq. (6) as:\ndλ,qM (r, c) = 1\nλ · min P∈U(r,c) Dq(P‖Ũ) + g(M) , (53)\nand our objective ”reduces” to the minimization of Dq(P‖Ũ) over U(r, c). In so–trot (Algorithm 1), we just care for a single constraint out of the two possible in U(r, c), so we will focus without loss of generality on the row constraint and therefore to the solution of:\nP ? .\n= arg min P∈Rn×n+ :P1=r\nDq(P‖Ũ) . (54)\nThe same result would apply to the column constraint. Convergence proof — We reuse the theory of auxiliary functions developed for the iterative constrained minimization of Bregman divergences [3, 10]. We reuse notation ” ” following [5, 27] and define for any y ∈ Rn, P ∈ Rn×n matrix y q P ∈ Rn×n such that\n(y q P )ij .\n= exp−1q (yi)pij\nexpq [ (1− q)yi exp1−qq (yi) logq(pij) ] . (55) We also define key matrix P̃ ∈ Rn×n with:\nP̃ . = rc> . (56)\nLet us denote\nQ . = { Q ∈ Rn×n : Q = expq(−1) exp−1q (α>1 + λM + 1>β) } . P .\n= {P ∈ 4n2 : P1 = P̃1 = r} .\nOne function will be key.\nDefinition 13 We define A(P,y) . = ∑\niAi(P,y), with:\nAi(P,y) . = yiri + ∑ j (pqij − exp q q(−1) exp−qq (aij − yi)) . (57)\nHere aij is defined in eq. (50), ri is the i-th coordinate in r (the row marginal constraint), and y ∈ Rn.\nLemma 14 For any y,\nA(P,y) = Dφ(P̃‖P )−Dφ(P̃‖y q P ) . (58)\nFurthermore, A(P,0) = 0.\nProof We have\nDφ(P̃‖P )−Dφ(P̃‖y q P ) = −Dφ(P‖y q P )\n+〈P̃ − P,∇φ(y q P )−∇φ(P )〉 .\nBecause a Bregman divergence is non-negative and A(P,0) = 0, if, as long as there exists some y for which A(P,y) > 0 we keep on updating P by replacing it by y∗ q P such that A(P,y∗) > 0, then the sequence\nP0 = Ũ → P1 . = y∗0 q P0 → P2 . = y∗1 q P1 · · · (59)\nwill converge to a limit matrix in the sequence,\nlim j Pj\n. = y∗j−1 q Pj−1 . (60)\nThis matrix turns out to be the one we seek.\nTheorem 15 Let Pj+1 . = yj q Pj (with P0 .\n= Ũ) be such that A(Pj ,yj) > 0, ∀j ≥ 0, and the sequence ends when no such yj exists. Then S . = {Pj}j≥0 ⊂ Q̄. If furthermore S lies in a compact of Q̄, then it satisfies\nP ? .\n= lim j Pj = arg min P∈P Dq(P‖Ũ) . (61)\nProof sketch: The proof relies on two steps, first that\nP ? .\n= lim j Pj = arg min P∈P Dφ(P‖Ũ) , (62)\nand then the fact that (61) holds as well, which ”amounts” to replacing Dφ, which is Bregman, by Dq, which is not. Because it is standard in Bregman divergences, we sketch the first step. The fundamental result we use is adapted from [10] (see also [5, Theorem 1]).\nTheorem 16 Suppose that Dφ(P̃ , Ũ) <∞. Then there exists a unique P ? satisfying the following four properties:\n1. P ? ∈ P ∩ Q̄\n2. ∀P ∈ P, ∀R ∈ Q̄, Dφ(P‖R) = Dφ(P‖P ?) +Dφ(P ?‖R)\n3. P ? = arg min P∈P Dφ(P‖Ũ)\n4. P ? = arg min R∈Q̄ Dφ(P̃‖R)\nMoreover, any of these four properties determines P ? uniquely.\nIt is not hard to check that Ũ ∈ Q̄ and whenever Pj ∈ Q̄, then y q Pj ∈ Q̄, ∀y, so we indeed have S ⊂ Q̄. With the constraint that A(Pj ,yj) > 0,∀j ≥ 0, it follows from Lemma 14 that A(P,y) is an auxiliary function for S [5] if we can show in addition that if y = 0 is a maximum of A(P,y), then P ∈ P. To remark that this is true, we have\n∇A(P,y)y = r − P1 , (63)\nso whenever A(P,y) reaches a maximum in y, we indeed have P1 = r and so P ∈ P, and if y = 0 then because a Bregman divergence satisfies the identity of the indiscernibles, if y = 0 is the maximum, then S has converged to some P ?. From 4. above, we get\nP ? = arg min R∈Q̄ Dφ(P̃‖R) , (64)\nand so from 3. above, we also get\nP ? = arg min P∈P Dφ(P‖Ũ) . (65)\nTo ”transfer” this result to Dq, we just need to remark that there is one remarkable trivial equality: Dφ(P‖R) = Dq(P‖R)− ∑ i,j (pqij − r q ij) , (66)\nso that even when K1/q is not a Bregman divergence for a general q, it still meets the Bregman triangle equality [1].\nLemma 17 We have;\nDq(P‖R) +Dq(R‖S)−Dq(P‖S) = Dφ(P‖R) +Dφ(R‖S)−Dφ(P‖S) = 〈P −R,∇φ(S)−∇φ(R)〉 . (67)\nHence, point 2. implies as well\nDq(P‖R) = Dq(P‖P ?) +Dq(P ?‖R) , (68)\n∀P ∈ P, ∀R ∈ Q̄, and so Dq(P‖Ũ) = Dq(P‖P ?) + Dq(P ?‖Ũ), ∀P ∈ P, so that we also have (since Dq is non negative and satisfies Dq(P‖P ) = 0)\nP ? = arg min P∈P Dq(P‖Ũ) ,\nas claimed (end of the proof of Theorem 15). Figure 4 summarizes Theorem 15. We are left with the problem of finding an auxiliary function for the sequence S, which we recall boils down to finding, whenever it exists, some y such that A(P,y) > 0.\nTheorem 18 A(P,y) is an auxiliary function for S for the sequence of updates y given as in steps 6-11 of so–trot (Algorithm 1).\nProof We shall need the complete Taylor expansion of A(P,y).\nLemma 19 Let us denote for short γ .\n= 1− q. The Taylor series expansion of Ai(P,y) (as defined in Definition 13) is:\nAi(P,y) = yi(ri − ∑ j pij)\n− ∑ j pij ∞∑ k=2\n[ 1\nk k−1∏ l=1 (γ + q/l) ] yki ( pγij q )k−1 . (69)\nProof Let us denote f(x) = exp−qq (x). We have:\nd\ndx f(x) = q exp1−qq (x)\nd\ndx exp−1q (x)\n= −q exp−1q (x) . (70)\nA simple recursion also shows (∀k ≥ 2):\ndk\ndxk exp−1q (x) = (−1)k [ k∏ i=1 (i− (i− 1)q) ] expkq−(k+1)q (x) ,\nwhich yields ∀k ≥ 1,\ndk\ndxk f(x) = −q d\nk−1\ndxk−1 exp−1q (x)\n= (−1)kq [ k−1∏ i=1 (iγ + q) ] exp−(k−1)γ−1q (x) .\nSince expqq(−1) = expq(−1)/q and ∀i, j, pij = expq(−1) exp−1q (aij), writing the Taylor development of f at point aij evaluated at yi, and adding the yiri + ∑ j p q ij term, we obtain the desired result.\nWe have two special reals to define, ti and zi. If ri ≤ ∑\nj pij , we let ti denote the maximum of the second order approximation of Ai(P,y),\nT (2) i (yi) . = yi(ri − ∑ j pij)− y2i 2 ∑ j p1+γij q , (71)\ni.e. the root of\nd\ndy T (2)(yi) = (ri − ∑ j pij)− yi ∑ j p1+γij q .\nIf ∑\nj pij ≤ ri, we let zi be the the largest root of\nRi . = (ri − ∑ j pij)\n−yi ∑ j p1+γij q − y2i (2− q) ∑ j p1+2γij q2 . (72)\nWe shall see that zi is positive. Let y ∗ i . = ti if ri ≤ ∑ j pij , and y ∗ i . = zi otherwise. We first make the assumption that ∣∣∣∣∣y∗i p γ ij q · ( γ + q 3 )∣∣∣∣∣ ≤ 12 , ∀i, j . (73)\nUnder this assumption, we have two cases. (?) Case ri ≤ ∑ j pij . By definition, we have in this case that yi = ti ≤ 0 in so–trot (Step 10). We also have\nAi(P,y)\n= T (2)(yi)\n− ∑ j pij ∞∑ k=3\n[ 1\nk k−1∏ l=1 (γ + q/l) ] yki ( pγij q )k−1 ︸ ︷︷ ︸\n. =S3\n. (74)\nSince yi = ti ≤ 0, S3 is an alternating series, that is a series whose general term is alternatively positive and negative. Under assumption (73), the module of its general term is decreasing. A classic result on series allows us to deduce from this fact that (a) S3 ∞ and (b) the sign of S3 is that of its first term, i.e., it is negative. Since Ai(P,y) = T (2)(yi)− S3, we have that\nAi(P,y) ≥ T (2)(yi) = 0 . (75)\nNote also that Ai(P,y) = 0 iff ∑ j pij = ri as T (2)(yi) is decreasing on [ti, 0] and T\n(2)(0) = 0. Hence, for the choice in Step 10, Ai(P,y) is an auxiliary function for variable i. (?) Case ∑\nj pij ≤ ri: we still have Ai(P,y) = T (2)(yi) − S3, but this time yi will be positive, ensuring yi(ri − ∑ j pij) ≥ 0. We first show that S3 is upperbounded by a geometric series under assumption (73):\nS3\n= ∑ j pijy 3 i ( pγij q )2 ∞∑ k=0 yki k + 3 [ k+2∏ l=1 (γ + q/l) ]( pγij q )k ≤ ∑ j pij(1− q/2) y3i 3 ( pγij q )2 ∞∑ k=0 ( yip γ ij q (γ + q/3) )k = ∑ j pij(1− q/2) y3i 3 ( pγij q )2 × 1 1− yip γ ij\nq (γ + q/3)\n≤ (2− q) ∑ j pij y3i 3 ( pγij q )2 ,\nwhich conveniently yields\nAi(P,y) ≥ T (2)(yi)− (2− q) ∑ j pij y3i 3 ( pγij q )2 . (76)\nThe derivative of the right-hand term of (76) is Ri defined in eq. (72) above. Let us define:\na . = (2− q) ∑ j p1+2γij q2 , (77)\nb . = ∑ j p1+γij q , (78) c .\n= −(ri − ∑ j pij) . (79)\nWe have ac < 0 and consequently the discriminant ∆ .\n= b2 − 4ac > b2, implying Ri has a positive root zi . = (−b+ √ ∆)/(2a) which maximises the right-hand term of 76, and is such that this right-\nhand term is positive. Further, we again have that zi = 0 iff ∑\nj pij = ri. It is easy to check that zi = yi in Step 8 of so–trot, for which we check that Ai(P,y) ≥ 0, wich equality iff ∑ j pij = ri. Hence, for the choice in Step 8, Ai(P,y) is an auxiliary function for variable i.\nWe can now conclude that under assumption (73), A(P,y) is an auxiliary function.\nIf assumption (73) does not hold, then notice that this cannot not hold at convergence for coordinate i. For this reason, ri 6= ∑ j pij and the sign sign(ri− ∑ j pij) is also well defined. Therefore, we just need to pick a value for yi 6= 0 which guarantees Ai(P,y) > 0. To do so, we pick\nyi = q · sign(ri −\n∑ j pij)\n(6− 4q) ·maxj p1−qij , (80)\nremarking that this yi indeed violates (73) (recalling γ . = 1− q). We also have |yi| ∈ (0, n2(1−q)/2]. Notice that this choice guarantees Ai(P,y) > 0. (end of the proof of Theorem 18)\nTheorems 15 and 18 altogether prove Theorem 7.\nSupplementary Material: experiments"
    }, {
      "heading" : "12 Per county error distribution, trot survey vs Florida average",
      "text" : "Figure 5 displays the empirical distribution of the errors for trot vs Florida average. While not being a true distribution of the solution error of trot — in a Bayesian sense —, the graph should convey the intuition that algorithms with a distribution that shrinks around zero provide better inference.\n13 Per county errors, trot survey vs trot 11>\nFigure 6 confronts the prediction errors by county of trot when we use M = M sur (survey) and M = Mno(= 11>) as cost matrix: while the overall performance of the two algorithms is very close, the graph demonstrates that trot optimized with M sur achieves very often smaller error, although the average error is worsen by few particularly bad counties."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2016,
    "abstractText" : "<lb>Optimal transport is a powerful framework for computing distances between probability dis-<lb>tributions. We unify the two main approaches to optimal transport, namely Monge-Kantorovitch<lb>and Sinkhorn-Cuturi, into what we define as Tsallis regularized optimal transport (trot).<lb>trot interpolates a rich family of distortions from Wasserstein to Kullback-Leibler, encompass-<lb>ing as well Pearson, Neyman and Hellinger divergences, to name a few. We show that metric<lb>properties known for Sinkhorn-Cuturi generalize to trot, and provide efficient algorithms for<lb>finding the optimal transportation plan with formal convergence proofs. We also present the<lb>first application of optimal transport to the problem of ecological inference, that is, the recon-<lb>struction of joint distributions from their marginals, a problem of large interest in the social<lb>sciences. trot provides a convenient framework for ecological inference by allowing to compute<lb>the joint distribution — that is, the optimal transportation plan itself — when side information<lb>is available, which is e.g. typically what census represents in political science. Experiments<lb>on data from the 2012 US presidential elections display the potential of trot in delivering a<lb>faithful reconstruction of the joint distribution of ethnic groups and voter preferences.",
    "creator" : "TeX"
  }
}