{
  "name" : "1605.07588.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "A Consistent Regularization Approach for Structured Prediction",
    "authors" : [ "Carlo Ciliberto", "Alessandro Rudi", "Lorenzo Rosasco" ],
    "emails" : [ "cciliber@mit.edu", "ale_rudi@mit.edu", "lrosasco@mit.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n60 5.\n07 58\n8v 3\n[ cs\n.L G\n] 2"
    }, {
      "heading" : "1 Introduction",
      "text" : "Many machine learning applications require dealing with data-sets having complex structures, e.g. natural language processing, image segmentation, reconstruction or captioning, pose estimation, protein folding prediction to name a few. [1–3]. Structured prediction problems pose a challenge for classic off-the-shelf learning algorithms for regression or binary classification. Indeed, this has motivated the extension of methods such as support vector machines to structured problems [4]. Dealing with structured prediction problems is also a challenge for learning theory. While the theory of empirical risk minimization provides a very general statistical framework, in practice computational considerations make things more involved. Indeed, in the last few years, an effort has been made to analyze specific structured problems, such as multiclass classification [5], multi-labeling [6], ranking [7] or quantile estimation [8]. A natural question is then whether a unifying learning theoretic framework can be developed encompassing a wide range of problems as special cases.\nIn this paper we take a step in this direction proposing and analyzing a regularization approach to a wide class of structured prediction problems defined by loss functions\n1Laboratory for Computational and Statistical Learning - Istituto Italiano di Tecnologia, Genova, Italy & Massachusetts Institute of Technology, Cambridge, MA 02139, USA.\n2Università degli Studi di Genova, Genova, Italy.\nsatisfying mild conditions. Indeed, our starting observation is that a large class of loss functions naturally define an embedding of the structured outputs in a linear space. This fact allows to define a (least squares) surrogate loss and cast the problem in a multi-output regularized learning framework [9–11]. The corresponding algorithm essentially reduces to a form of kernel ridge regression and generalizes the approach proposed in [12], hence providing also a novel derivation for this latter algorithm. Our theoretical analysis allows to characterize the generalization properties of the proposed approach. In particular, it allows to quantify the impact due to the surrogate approach and establishes universal consistency as well as finite sample bounds. An experimental analysis shows promising results on a variety of structured prediction problems.\nThe rest of this paper is organized as follows: in Sec. 2 we introduce the structured prediction problem in its generality and present our algorithm to approach it. In Sec. 3 we introduce and discuss a surrogate framework for structured prediction, from which we derive our algorithm. In Sec. 4, we analyze the theoretical properties of the proposed algorithm. In Sec. 5 we draw connections with previous work in structured prediction while in Sec. 6 we report a preliminary empirical analysis and comparison of the proposed approach. Finally, Sec. 7 concludes the paper outlining relevant directions for future research."
    }, {
      "heading" : "2 A Regularization Approach to Structured prediction",
      "text" : "The goal of supervised learning is to learn functional relations f : X → Y between two sets X ,Y, given a finite number of examples. In particular in this work we are interested to structured prediction, namely the case where Y is a set of structured outputs (such as histograms, graphs, time sequences, points on a manifold, etc.). Moreover, structure on Y can be implicitly induced by a suitable loss △ : Y × Y → R (such as edit distance, ranking error, geodesic distance, indicator function of a subset, etc.). Then, the problem of structured prediction becomes\nminimize f:X→Y\nE(f), with E(f) = ∫\nX×Y △(f(x), y) dρ(x, y) (1)\nand the goal is to find a good estimator for the minimizer of the above equation, given a finite number of (training) points {(xi, yi)}ni=1 sampled from a unknown probability distribution ρ on X × Y. In the following we introduce an estimator f̂ : X → Y to approach Eq. (1). The rest of this paper is devoted to prove that f̂ it a consistent estimator for a minimizer of Eq. (1).\nOur Algorithm for Structured Prediction. In this paper we propose and analyze the following estimator\nf̂(x) = argmin y∈Y\nn∑\ni=1\nαi(x)△ (y, yi) with α(x) = (K + nλI)−1Kx ∈ Rn (Alg. 1)\ngiven a positive definite kernel k : X × X → R and training set {(xi, yi)}ni=1. In the above expression, αi(x) is i-th entry in α(x), K ∈ Rn×n is the kernel matrix Ki,j = k(xi, xj),\nKx ∈ Rn the vector with entires (Kx)i = k(x, xi), λ > 0 a regularization parameter and I the identity matrix. From a computational perspective, the procedure in Alg. 1 is divided in two steps: a learning step where input-dependents weights αi(·) are computed (which essentially consists in solving a kernel ridge regression problem) and a prediction step where the αi(x)-weighted linear combination in Alg. 1 is optimized, leading to a prediction f̂(x) given an input x. This idea was originally proposed in [13], where a “score” function F(x, y) was learned to estimate the “likelihood” of a pair (x, y) sampled from ρ, and then used in\nf̂(x) = argmin y∈Y − F(x, y), (2)\nto predict the best f̂(x) ∈ Y given x ∈ X . This strategy was extended in [4] for the popular SVMstruct and adopted also in a variety of approaches for structured prediction [1,12,14].\nIntuition. While providing a principled derivation of Alg. 1 for a large class of loss functions is a main contribution of this work, it is useful to first consider the special case where △ is induced by a reproducing kernel h : Y × Y → R on the output set, such that\n△(y, y ′) = h(y, y) − 2h(y, y ′) + h(y ′, y ′). (3)\nThis choice of△ was originally considered in Kernel Dependency Estimation (KDE) [15]. In particular, for the case of normalized kernels (i.e. h(y, y) = 1 ∀y ∈ Y), Alg. 1 essentially reduces to [12, 14] and recalling their derivation is insightful. Note that, since a kernel can be written as h(y, y ′) = 〈ψ(y), ψ(y ′)〉HY , with ψ : Y → HY a non-linear map into a feature space HY [16], then Eq. (3) can be rewritten as\n△(f(x), y ′) = ‖ψ(f(x)) −ψ(y ′)‖2HY . (4)\nDirectly minimizing the equation above with respect to f is generally challenging due to the non linearity ψ. A possibility is to replace ψ ◦ f by a function g : X → HY that is easier to optimize. We can then consider the regularized problem\nminimize g∈G\n1\nn\nn∑\ni=1\n‖g(xi) −ψ(yi)‖2HY + λ‖g‖ 2 G (5)\nwith G a space of functions1 g : X → HY of the form g(x) = ∑\ni=1 k(x, xi)ci with ci ∈ HY and k a reproducing kernel. Indeed, in this case the solution to Eq. (5) is\nĝ(x) =\nn∑\ni=1\nαi(x)ψ(yi) with α(x) = (K+ nλI)−1Kx ∈ Rn (6)\n1G is the reproducing kernel Hilbert space for vector-valued functions [9] with inner product 〈k(xi, ·)ci, k(xj, ·)cj〉G = k(xi, xj)〈ci, cj〉HY\nwhere the αi are the same as in Alg. 1. Since we replaced △(f(x), y) by ‖g(x) −ψ(y)‖2HY , a natural question is how to recover an estimator f̂ from ĝ. In [12] it was proposed to consider\nf̂(x) = argmin y∈Y ‖ψ(y) − ĝ(x)‖2HY = argmin y∈Y h(y, y) − 2\nn∑\ni=1\nαi(x)h(y, yi), (7)\nwhich corresponds to Alg. 1 when h is a normalized kernel. The discussion above provides an intuition on how Alg. 1 is derived but raises also a few questions. First, it is not clear if and how the same strategy could be generalized to loss functions that do not satisfy Eq. (3). Second, the above reasoning hinges on the idea of replacing f̂ with ĝ (and then recovering f̂ by Eq. (7)), however it is not clear whether this approach can be justified theoretically. Finally, we can ask what are the statistical properties of the resulting algorithm. We address the first two questions in the next section, while the rest of the paper is devoted to establish universal consistency and generalization bounds for algorithm Alg. 1.\nNotation and Assumptions. We introduce here some minimal technical assumptions that we will use throughout this work. We will assume X and Y to be Polish spaces, namely separable completely metrizable spaces equipped with the associated Borel sigmaalgebra. Given a Borel probability distribution ρ on X × Y we denote with ρ(·|x) the associated conditional measure on Y (given x ∈ X ) and with ρX the marginal distribution on X ."
    }, {
      "heading" : "3 Surrogate Framework and Derivation",
      "text" : "To derive Alg. 1 we consider ideas from surrogate approaches [7,17,18] and in particular [5]. The idea is to tackle Eq. (1) by substituting △(f(x), y) with a “relaxation” L(g(x), y) that is easy to optimize. The corresponding surrogate problem is\nminimize g:X→HY\nR(g), with R(g) = ∫\nX×Y L(g(x), y) dρ(x, y), (8)\nand the question is how a solution g∗ for the above problem can be related to a minimizer f∗ of Eq. (1). This is made possible by the requirement that there exists a decoding d : HY → Y, such that\nFisher Consistency: E(d ◦ g∗) = E(f∗), (9) Comparison Inequality: E(d ◦ g) − E(f∗) ≤ ϕ(R(g) −R(g∗)), (10)\nhold for all g : X → HY , where ϕ : R → R is such that ϕ(s) → 0 for s → 0. Indeed, given an estimator ĝ for g∗, we can “decode” it considering f̂ = d ◦ ĝ and use the excess risk R(ĝ) − R(g∗) to control E(f̂) − E(f∗) via the comparison inequality in Eq. (10). In particular, if ĝ is a data-dependent predictor trained on n points and R(ĝ) → R(g∗) when n → +∞, we automatically have E(f̂) → E(f∗). Moreover, if ϕ in Eq. (10) is known explicitly, generalization bounds for ĝ are automatically extended to f̂.\nProvided with this perspective on surrogate approaches, here we revisit the discussion of Sec. 2 for the case of a loss function induced by a kernel h. Indeed, by assuming the surrogate L(g(x), y) = ‖g(x) − ψ(y)‖2HY , Eq. (5) becomes the empirical version of the surrogate problem at Eq. (8) and leads to an estimator ĝ of g∗ as in Eq. (6). Therefore, the approach in [12, 14] to recover f̂(x) = argminy L(g(x), y) can be interpreted as the result f̂(x) = d◦ ĝ(x) of a suitable decoding of ĝ(x). An immediate question is whether the above framework satisfies Eq. (9) and (10). Moreover, we can ask if the same idea could be applied to more general loss functions.\nIn this work we identify conditions on△ that are satisfied by a large family of functions and moreover allow to design a surrogate framework for which we prove Eq. (9) and (10). The first step in this direction is to introduce the following assumption.\nAssumption 1. There exists a separable Hilbert space HY with inner product 〈·, ·〉HY , a continuous embedding ψ : Y → HY and a bounded linear operator V : HY → HY , such that\n△ (y, y ′) = 〈ψ(y), Vψ(y ′)〉HY ∀y, y ′ ∈ Y (11)\nAsm. 1 is similar to Eq. (4) and in particular to the definition of a reproducing kernel. Note however that by not requiring V to be positive semidefinite (or even symmetric), we allow for a surprisingly wide range of functions. Indeed, below we discuss some examples of functions that satisfy Asm. 1 (see Appendix Sec. C for more details):\nExample 1. The following functions of the form △ : Y × Y → R satisfy Asm. 1: 1. Any loss on Y of finite cardinality. Several problems belong to this setting, such as\nMulti-Class Classification, Multi-labeling, Ranking, predicting Graphs (e.g. protein foldings).\n2. Regression and Classification Loss Functions: Least-squares, Logistic, Hinge, ǫ-insensitive, τ-Pinball.\n3. Robust Loss Functions Most loss functions used for robust estimation [19] such as the absolute value, Huber, Cauchy, German-McLure, “Fair” and L2 − L1. See [19] or the Appendix for their explicit formulation.\n4. KDE. Loss functions △ induced by a kernel such as in Eq. (3).\n5. Distances on Histograms/Probabilities. The χ2 and the squared Hellinger distances.\n6. Diffusion distances on Manifolds. The squared diffusion distance induced by the heat kernel (at time t > 0) on a compact Reimannian manifold without boundary [20].\nThe Least Squares Loss Surrogate Framework. Asm. 1 implicitly defines the space HY similarly to Eq. (4). The following result motivates the choice of the least squares surrogate and moreover suggests a possible choice for the decoding.\nLemma 1. Let △ : Y ×Y → R satisfy Asm. 1 with ψ : Y → HY bounded. Then the expected risk in Eq. (1) can be written as\nE(f) = ∫\nX 〈ψ(f(x)), Vg∗(x)〉HY dρX (x) (12)\nfor all f : X → Y, where g∗ : X → HY minimizes\nR(g) = ∫\nX×Y ‖g(x) −ψ(y)‖2HY dρ(x, y). (13)\nLemma 1 shows how Eq. (13) arises naturally as surrogate problem. In particular, Eq. (12) suggests how to chose the decoding. Indeed, consider an f : X → Y such that for each x ∈ X , f(x) is a minimizer of the argument in Eq. (1), namely 〈ψ(f(x)), Vg∗(x)〉HY ≤ 〈ψ(y,Vg∗(x)〉HY for all y ∈ Y. Then we have E(f) ≤ E(f ′) for any f ′ : X → Y. Following this observation, in this work we consider the decoding d : HY → Y such that\nd(h) = argmin y∈Y 〈 ψ(y) , Vh 〉HY ∀h ∈ HY . (14)\nSo that E(d ◦ g∗) ≤ E(f ′) for all f ′ : X → Y. Indeed, for this choice of decoding, we have the following result.\nTheorem 2. Let △ : Y × Y → R satisfy Asm. 1 with Y a compact set. Then, for every measurable g : X → HY and d : HY → Y satisfying Eq. (14), the following holds\nE(d ◦ g∗) = E(f∗) (15) E(d ◦ g) − E(f∗) ≤ 2c△ √ R(g) −R(g∗). (16)\nwith c△ = ‖V‖maxy∈Y ‖ψ(y)‖HY . Thm. 2 shows that for all △ satisfying Asm. 1, the corresponding surrogate framework identified by the surrogate in Eq. (13) and decoding Eq. (14) satisfies Fisher consistency and the comparison inequality in Eq. (16). We recall that a finite set Y is always compact, and moreover, assuming the discrete topology on Y, we have that any ψ : Y → HY is continuous. Therefore, Thm. 2 applies in particular to any structured prediction problem on Y with finite cardinality.\nThm. 2 suggest to approach structured prediction by first learning ĝ and then decoding it to recover f̂ = d ◦ ĝ. A natural question is how to choose ĝ in order to compute f̂ in practice. In the rest of this section we propose an approach to this problem.\nDerivation for Alg. 1. Minimizing R in Eq. (13) corresponds to a vector-valued regression problem [9–11]. In this work we adopt an empirical risk minimization approach to learn ĝ as in Eq. (5). The following result shows that combining ĝ with the decoding in Eq. (14) leads to the f̂ in Alg. 1.\nLemma 3. Let △ : Y × Y → R satisfy Asm. 1 with Y a compact set. Let ĝ : X → HY be the minimizer of Eq. (5). Then, for all x ∈ X\nd ◦ ĝ(x) = argmin y∈Y\nn∑\ni=1\nαi(x)△ (y, yi) α(x) = (K+ nλI)−1Kx ∈ Rn (17)\nLemma 3 concludes the derivation of Alg. 1. An interesting observation is that computing f̂ does not require explicit knowledge of the embedding ψ and the operator V , which are implicitly encoded within the loss △ by Asm. 1. In analogy to the kernel trick [21] we informally refer to such assumption as the “loss trick”. We illustrate this effect with an example.\nExample 2 (Ranking). In ranking problems the goal is to predict ordered sequences of a fixed number ℓ of labels. For these problems, Y corresponds to the set of all ordered sequences of ℓ labels and has cardinality |Y | = ℓ!, which is typically dramatically larger than the number n of training examples (e.g. for ℓ = 15, ℓ! ≃ 1012). Therefore, given an input x ∈ X , directly computing ĝ(x) ∈ R|Y | is impractical. On the opposite, the loss trick allows to express d◦ ĝ(x) only in terms of the n weights αi(x) in Alg. 1, making the computation of the argmin easier to approach in general. For details on the rank loss △rank and the corresponding optimization over Y we refer to the empirical analysis of Sec. 6.\nIn this section we have shown a derivation for the structured prediction algorithm proposed in this work. In Thm. 2 we have shown how the expected risk of the proposed estimator f̂ is related to an estimator ĝ via a comparison inequality. In the following we will make use of these results to prove consistency and generalization bounds for Alg. Alg. 1."
    }, {
      "heading" : "4 Statistical Analysis",
      "text" : "In this section we study the statistical properties of Alg. 1. In particular we made use of the relation between the structured and surrogate problems via the comparison inequality in Thm. 2. We begin our analysis by proving that Alg. 1 is universally consistent.\nTheorem 4 (Universal Consistency). Let△ : Y×Y → R satisfy Asm. 1, X and Y be compact sets and k : X × X → R a continuous universal reproducing kernel2. For any n ∈ N and any distribution ρ on X × Y let f̂n : X → Y be obtained by Alg. 1 with {(xi, yi)}ni=1 training points independently sampled from ρ and λn = n −1/4. Then,\nlim n→+∞\nE(f̂n) = E(f∗) with probability 1 (18)\nThm. 4 shows that, when the △ satisfies Asm. 1, Alg. 1 approximates a solution f∗ to Eq. (1) arbitrarily well, given a sufficient number of training examples. To the best of our knowledge this is the first consistency result for structured prediction in the general setting considered in this work and characterized by Asm. 1, in particular for the case of Y with infinite cardinality (dense or discrete).\nThe No Free Lunch Theorem [22] states that it is not possible to prove uniform convergence rates for Eq. (18). However, by imposing suitable assumptions on the regularity of g∗ it is possible to prove generalization bounds for ĝ and then, using Thm. 2, extend them to f̂. To show this, it is sufficient to require that g∗ belongs to G the reproducing kernel Hilbert space used in the ridge regression of Eq. (5). Note that in the proofs of Thm. 4 and Thm. 5, our analysis on ĝ borrows ideas from [10] and extends their result to our setting for the case of HY infinite dimensional (i.e. when Y has infinite cardinality). Indeed, note that in this case [10] cannot be applied to the estimator ĝ considered in this work (see Appendix Sec. B.3, Lemma 18 for details).\nTheorem 5 (Generalization Bound). Let △ : Y × Y → R satisfy Asm. 1, Y be a compact set and k : X × X → R a bounded continuous reproducing kernel. Let f̂n denote the solution\n2This is a standard assumption for universal consistency (see [18]). An example of continuous universal kernel is the Gaussian k(x, x ′) = exp(−‖x− x ′‖2/σ).\nof Alg. 1 with n training points and λ = n−1/2. If the surrogate risk R defined in Eq. (13) admits a minimizer g∗ ∈ G, then\nE(f̂n) − E(f∗) ≤ cτ2 n− 1 4 (19)\nholds with probability 1− 8e−τ for any τ > 0, with c a constant not depending on n and τ.\nThe bound in Eq. (5) is of the same order of the generalization bounds available for the least squares binary classifier [23]. Indeed, in Sec. 5 we show that in classification settings Alg. 1 reduces to least squares classification.\nRemark 1 (Better Comparison Inequality). The generalization bounds for the least squares classifier can be improved by imposing regularity conditions on ρ via the Tsybakov condi-\ntion [23]. This was observed in [23] for binary classification with the least squares surrogate,\nwhere a tighter comparison inequality than the one in Thm. 2 was proved. Therefore, a nat-\nural question is whether the inequality of Thm. 2 could be similarly improved, consequently\nleading to better rates for Thm. 5. Promising results in this direction can be found in [5],\nwhere the Tsybakov condition was generalized to the multi-class setting and led to a tight\ncomparison inequality analogous to the one for the binary setting. However, this question\ndeserves further investigation. Indeed, it is not clear how the approach in [5] could be further generalized to the case where Y has infinite cardinality.\nRemark 2 (Other Surrogate Frameworks). In this paper we focused on a least squares surrogate loss function and corresponding framework. A natural question is to ask whether other\nloss functions could be considered to approach the structured prediction problem, sharing the\nsame or possibly even better properties. This question is related also to Remark 1, since dif-\nferent surrogate frameworks could lead to sharper comparison inequalities. This seems an\ninteresting direction for future work."
    }, {
      "heading" : "5 Connection with Previous Work",
      "text" : "In this section we draw connections between Alg. 1 and previous methods for structured prediction learning.\nBinary and Multi-class Classification. It is interesting to note that in classification settings, Alg. 1 corresponds to the least squares classifier [23]. Indeed, let Y = {1, . . . , ℓ} be a set of labels and consider the misclassification loss△(y, y ′) = 1 for y 6= y ′ and 0 otherwise. Then △(y, y ′) = e⊤y Vey ′ with ei ∈ Rℓ the i-the element of the canonical basis of Rℓ and V = 1 − I, where I is the ℓ × ℓ identity matrix and 1 the matrix with all entries equal to 1. In the notation of surrogate methods adopted in this work, HY = Rℓ and ψ(y) = ey. Note that both Least squares classification and our approach solve the surrogate problem at Eq. (5)\n1\nn\nn∑\ni=1\n‖g(xi) − eyi‖2RT + λ ‖g‖2G (20)\nto obtain a vector-valued predictor ĝ : X → RT as in Eq. (6). Then, the least squares classifier ĉ and the decoding f̂ = d ◦ ĝ are respectively obtained by\nĉ(x) = argmax i=1,...,T ĝ(x) f̂(x) = argmin i=1,...,T Vĝ(x). (21)\nHowever, since V = 1− I, it is easy to see that ĉ(x) = f̂(x) for all x ∈ X .\nKernel Dependency Estimation. In Sec. 2 we discussed the relation between KDE [12, 15] and Alg. 1. In particular, we have observed that if △ is induced by a kernel h : Y × Y → R as in Eq. (3) and h is normalized, i.e. h(y, y) = 1 ∀y ∈ Y, then algorithm Eq. (7) proposed in [12] leads to the same predictor as Alg. 1. Therefore, we can apply Thm. 4 and 5 to prove universal consistency and generalization bounds for methods such as [12,14]. We are not aware of previous results proving consistency (and generalization bounds) for the KDE methods in [12, 14]. Note however that when the kernel h is not normalized, the “decoding” in Eq. (7) is not equivalent to Alg. 1. In particular, given the surrogate solution g∗, applying Eq. (7) leads to predictors that are do not minimize Eq. (1). As a consequence the approach in [12] is not consistent in the general case.\nSupport Vector Machines for Structured Output. A popular approach to structured prediction is the Support Vector Machine for Structured Outputs (SVMstruct) [4] that extends ideas from the well-known SVM algorithm to the structured setting. One of the main advantages of SVMstruct is that it can be applied to a variety of problems since it does not impose strong assumptions on the loss. In this view, our approach, as well as KDE, shares similar properties, and in particular allows to consider Y of infinite cardinality. Moreover, we note that generalization studies for SVMstruct are available [3] (Ch. 11). However, it seems that these latter results do not allow to derive universal consistency of the method."
    }, {
      "heading" : "6 Experiments",
      "text" : "Ranking Movies. We considered the problem of ranking movies in the MovieLens dataset [26] (ratings (from 1 to 5) of 1682 movies by 943 users). The goal was to predict preferences of a given user, i.e. an ordering of the 1682 movies, according to the user’s partial ratings. Note that, as observed in Example 2, in ranking problems the output set Y is the collection of all ordered sequences of a predefined length. Therefore, Y is finite (albeit extremely large) and we can apply Alg. 1.\nWe applied Alg. 1 to the ranking problem using the rank loss [7]\n△rank(y, y ′) = M∑\ni,j=1\nγ(y ′)ij (1− sign(yi − yj))/2, (22)\nwith M is the number of movies in the database, y ∈ Y a vector of the M integers yi ∈ {1, . . . ,M} without repetition, where yi corresponding to the rank assigned by y to movie i. In the definition of △rank, γ(y)ij denotes the costs (or reward) of having movie j ranked higher than movie i and, similarly to [7], we set γ(y)ij equal to the difference of ratings provided by user associated to y (from 1 to 5). We chose as k in Alg. 1, a linear kernel on features similar to those proposed in [7], which were computed based on users’ profession, age, similarity of previous ratings, etc. Since solving Alg. 1 for △rank is NPhard (see [7]) we adopted the Feedback Arc Set approximation (FAS) proposed in [27] to approximate the f̂(x) of Alg. 1. Results are reported in Tab. 1 comparing Alg. 1 (Ours) with surrogate ranking methods using a Linear [7], Hinge [24] or Logistic [25] loss and Struct SVM [4]3. We randomly sampled n = 643 users for training and tested on the remaining 300. We performed 5-fold cross-validation for model selection. We report the normalized△rank, averaged over 10 trials to account for statistical variability. Interestingly, our approach appears to outperform all competitors, suggesting that Alg. 1 is a viable approach to ranking.\nImage Reconstruction with Hellinger Distance. We considered the USPS4 digits reconstruction experiment originally proposed in [15]. The goal is to predict the lower half of an image depicting a digit, given the upper half of the same image in input. The standard approach is to use a Gaussian kernel kG on images in input and adopt KDE methods such as [12,14,15] with loss △G(y, y ′) = 1− kG(y, y ′). Here we take a different approach and, following [28], we interpret an image depicting a digit as an histogram and normalize it to sum up to 1. Therefore, Y becomes is the unit simplex in R128 (16× 16 images) and we adopt the squared Hellinger distance △H\n△H(y, y ′) = n∑\ni=1\n( √ yi − √ yi ′ )2 for y = (yi) M i=1 (23)\nto measure distances on Y. We used the kernel kG on the input space and compared Alg. 1 using respectively △H and △G. For △G Alg. 1 correpsponds to [12]. We performed digit\n3implementation from http://svmlight.joachims.org/svm_struct.html 4http://www.cs.nyu.edu/~roweis/data.html\nreconstruction experiments by training on 1000 examples evenly distributed among the 10 digits of USPS and tested on 5000 images. We performed 5-fold cross-validation for model selection. Tab. 2 reports the performance of Alg. 1 and the KDE methods averaged over 10 runs. Performance are reported according to the Gaussian loss △G Hellinger loss △H. Unsurprisingly, methods trained with respect to a specific loss perform better than the competitor with respect to such loss. Therefore, as a further measure of performance we also introduced the “Recognition” loss △R. This loss has to be intended as a measure of how “well” a predictor was able to correctly reconstruct an image for digit recognition purposes. To this end, we trained an automatic digit classifier and defined △R to be the misclassification error of such classifier when tested on images reconstructed by the two prediction algorithms. This automatic classifier was trained using a standard SVM [21] (with LIBSVM5) on a separate subset of USPS images and achieved an average 0.04% error rate on the true 5000 test sets. In this case a clear difference in performance can be observed between using two different loss functions, suggesting that △H is more suited to the reconstruction problem.\nRobust Estimation. We considered a regression problems with many outliers and evaluated Alg. 1 using the Cauchy loss (see Example 1 - (3)) for robust estimation. Indeed, in this setting, Y = [−M,M] ⊂ R is not structured, but the non-convexity of △ can be an obstacle to the learning process. We generated a dataset according to the model y = sin(6πx)+ǫ+ ζ, where x was sampled uniformly on [−1, 1] and ǫ according to a zeromean Gaussian with variance 0.1. ζ modeled the outliers and was sampled according to a zero-mean random variable that was 0 with probability 0.90 and a value uniformly at random in [−3, 3] with probability 0.1). We compared Alg. 1 with the Nadaraya-Watson robust estimator (RNW) [29] and kernel ridge regression (KRR) with a Gaussian kernel as baseline. To train Alg. 1 we used a Gaussian kernel on the input and performed predictions (i.e. solved Eq. (17)) using Matlab FMINUNC function for unconstrained minimization. Experiments were performed with training sets of increasing dimension (100 repetitions each) and test set of 1000 examples. 5-fold cross-validation for model selection. Results are reported in Fig. 1, showing that our estimator significantly outperforms the others.\n5https://www.csie.ntu.edu.tw/~cjlin/libsvm/\nMoreover, our method appears to greatly benefit from training sets of increasing size."
    }, {
      "heading" : "7 Conclusions and Future Work",
      "text" : "In this work we considered the problem of structured prediction from a Statistical Learning Theory perspective. We proposed a learning algorithm for structured prediction that is split into a learning and prediction step similarly to previous methods in the literature. We studied the statistical properties of the proposed algorithm by adopting a strategy inspired to surrogate methods. In particular, we identified a large family of loss functions for which it is natural to identify a corresponding surrogate problem. This perspective allows to prove a derivation of the algorithm proposed in this work. Moreover, by exploiting a comparison inequality relating the original and surrogate problems we were able to prove universal consistency and generalization bounds under mild assumption. In particular, the bounds proved in this work recover those already known for least squares classification, of which our approach can be seen as a generalization. We supported our theoretical analysis with experiments showing promising results on a variety of structured prediction problems.\nA few questions were left opened. First, we ask whether the comparison inequality can be improved (under suitable hypotheses) to obtain faster generalization bounds for our algorithm. Second, the surrogate problem in our work consists of a vector-valued regression (in a possibly infinite dimensional Hilbert space), we solved this problem by plain kernel ridge regression but it is natural to ask whether approaches from the multitask learning literature could lead to substantial improvements in this setting. Finally, an interesting question is whether alternative surrogate frameworks could be derived for the setting considered in this work, possibly leading to tighter comparison inequalities. We will investigate these questions in the future."
    }, {
      "heading" : "Appendix",
      "text" : "The Appendix of this work is divided in the following three sections:\nA Proofs of Fisher consistency and comparison inequality (Thm. 2).\nB Universal Consistency and Generalization Bounds for Alg. 1. (Thm. 4 and 5).\nC The characterization of a large family of △s satisfying Asm. 1 (Thm. 19)."
    }, {
      "heading" : "Mathematical Setting",
      "text" : "In the following we will always assume X and Y to be Polish spaces, namely separable complete metrizable spaces, equipped with the associated Borel sigma-algebra. When referring to a probability distribution ρ on X × Y we will always assume it to be a Borel probability measure, with ρX the marginal distribution on X and ρ(·|x) the conditional measure on Y given x ∈ X . We recall [1] that ρ(y|x) is a regular conditional distribution and its domain, which we will denoteDρ|X in the following, is a measurable set contained in the support of ρX and corresponds to the support of ρX up to a set of measure zero.\nFor convenience, we recall here the main assumption of our work.\nAssumption 1. There exists a separable Hilbert space HY with inner product 〈·, ·〉HY , a continuous embedding ψ : Y → HY and a bounded linear operator V : HY → HY , such that\n△ (y, y ′) = 〈ψ(y), Vψ(y ′)〉HY ∀y, y ′ ∈ Y (11)\nBasic notation We recall that a Hilbert space H is a vector space with inner product 〈·, ·〉H, closed with respect to the norm ‖h‖H = √ 〈h, h〉H for any h ∈ H. We denote with L2(X , ρX ,H) the Lebesgue space of square integrable functions on X with respect to a measure ρX and with values in a separable Hilbert space H. We denote with 〈f, g〉ρX the inner product ∫ 〈f(x), g(x)〉HdρX (x), for all f, g ∈ L2(X , ρX ,H). In particular when H = R we denote with L2(X , ρX ) the space L2(X , ρX ,R). Given a linear operator V : H → H ′ between two Hilbert spaces H,H ′, we denote with Tr(V) the trace of V and with V∗ : H ′ → H the adjoint operator associated to V , namely such that 〈Vh, h ′〉 ′H = 〈h,V∗h ′〉H ′ for every h ∈ H, h ′ ∈ H ′. Moreover, we denote with ‖V‖ = sup‖h‖H≤1 ‖Vh‖H ′ and ‖V‖HS = √ Tr(V∗V) respectively the operator norm and Hilbert-Schmidt norm of V . We recall that a linear operator V is continuous if and only if ‖V‖ < +∞ and we denote B(H,H ′) the set of all continuous linear operators from H to H ′. Moreover, we denote B2(H,H ′) the set of all operators V : H → H ′ with ‖V‖HS < +∞ and recall that B2(H,H ′) is isometric to the space H ′ ⊗H, with ⊗ denoting the tensor product. Indeed, for the sake of simplicity, with some abuse of notation we will not make the distinction between the two spaces.\nNote that in most of our results we will require Y to be non-empty and compact, so that a continuous functional over Y always attains a minimizer on Y and therefore the operator argminy∈Y is well defined. Note that for a finite set Y, we will always assume it endowed with the discrete topology, so that Y is compact and any function△ : Y×Y → R is continuous.\nOn the Argmin Notice that for simplicity of notation, in the paper we denoted the minimizer of Alg. 1 as\nf̂(x) = argmin y∈Y\nn∑\ni=1\nαi(x)△ (y, yi). (24)\nHowever note that the correct notation should be\nf̂(x) ∈ argmin y∈Y\nn∑\ni=1\nαi(x)△ (y, yi) (25)\nsince a loss function △ can have more than one minimizer in general. In the following we keep this more pedantic, yet correct notation.\nExpected Risk Minimization Note that whenever we write an expected risk minimization problem, we implicitly assume the optimization domain to be the space of measurable functions. For instance, Eq. (1) would be written more rigorously as\nminimize {∫\nX×Y △(f(x), y) dρ(x, y) | f : X → Y measurable\n}\n(26)\nIn the next Lemma, following [2] we show that the problem in Eq. (1) admits a measurable pointwise minimizer.\nLemma 6 (Existence of a solution for Eq. (1)). Let△ : Y×Y → R be a continuous function. Then, the expected risk minimization at Eq. (1) admits a measurable minimizer f∗ : X → Y such that\nf∗(x) ∈ argmin y∈Y\n∫\nY △(y, y ′)dρ(y ′|x) (27)\nfor every x ∈ Dρ|X . Moreover, the function m : X → R defined as follows, is measurable\nm(x) = inf y∈Y r(x, y), with r(x, y) =\n{ ∫ Y △(y, y ′)dρ(y ′|x) if x ∈ Dρ|X\n0 otherwise (28)\nProof. Since △ is continuous and ρ(y|x) is a regular conditional distribution, then r is a Carathéodory function (see Definition 4.50 (pp. 153) of [3]), namely continuous in y for each x ∈ X and measurable in x for each y ∈ Y. Thus, by Theorem 18.19 (pp. 605) of [3] (or Aumann’s measurable selection principle [2, 4]), we have that m is measurable and that there exists a measurable f∗ : X → Y such that r(x, f∗(x)) = m(x) for all x ∈ X . Moreover, by definition of m, given any measurable f : X → Y, we have m(x) ≤ r(x, f(x)). Therefore,\nE(f∗) = ∫ r(x, f∗(x))dρX (x) = ∫ m(x)dρX (x) ≤ ∫ r(x, f(x))dρX (x) = E(f). (29)\nWe conclude E(f∗) ≤ inff:X→Y E(f) and, since f∗ is measurable, E(f∗) = minf:X→Y E(f) and f∗ is a global minimizer.\nWe have an immediate Corollary to Lemma 6.\nCorollary 7. With the hypotheses of Lemma 6, let f̃ : X → Y such that\nf̃(x) ∈ argmin y∈Y\n∫\nY △(y, y ′)dρ(y ′|x)\nfor almost every x ∈ Dρ|X . Then E(f̃) = inff:X→Y E(f).\nProof. The result follows directly from Lemma 6 by noting that r(x, f̃(x)) = m(x) almost everywhere onDρ|X . Hence, sinceDρ|X is equal to the support of ρX up to a set of measure zero, E(f̃) =\n∫ X m(x)dρX (x) = E(f∗) = inff E(f).\nWith the above basic notation and results, we can proceed to prove the results presented in this work."
    }, {
      "heading" : "A Surrogate Problem, Fisher Consistency and Comparison In-",
      "text" : "equality\nIn this section we focus on the surrogate framework introduced in Sec. 3 and prove that it is Fisher consistent and that the comparison inequality. To do so, we will first characterizes the solution(s) of the surrogate expected risk minimization introduced at Eq. (13). We recall that in our setting, the surrogate risk was defined as the functional R(g) =\n∫ X×Y ‖ψ(y) − g∗(x)‖2HYdρ(x, y), where ψ : Y → HY is continuous (by Asm. 1).\nIn the following, when ψ is bounded, we will denote with Q = supy∈Y ‖ψ(y)‖HY . Note that in most our results we will assume Y to be compact. In these settings we always have Q = maxy∈Y ‖ψ(y)‖HY by the continuity of ψ.\nWe start with a preliminary lemma necessary to prove Lemma 1 and Thm. 2.\nLemma 8. Let HY a separable Hilbert space and ψ : Y → HY measurable and bounded. Then, the function g∗ : X → HY such that\ng∗(x) = ∫\nY ψ(y)dρ(y|x) ∀x ∈ Dρ|X (30)\nand g∗(x) = 0 otherwise, belongs to L2(X , ρX ,HY) and is a minimizer of the surrogate expected risk at Eq. (13). Moreover, any minimizer of Eq. (13) is equal to g∗ almost everywhere on the domain of ρX .\nProof. By hypothesis, ‖ψ‖HY is measurable and bounded. Therefore, since ρ(y|x) is a regular conditional probability, we have that g∗ is measurable on X (see for instance [2]). Moreover, the norm of g∗ is dominated by the constant function of value Q, thus g∗ is integrable on X with respect to ρX and in particular it is in L2(X , ρX ,HY) since ρX is a finite regular measure. Recall that since ρ(y|x) is a regular conditional distribution, for any measurable g : X → HY , the functional in Eq. (13) can be written as\nR(g) = ∫\nX×Y ‖g(x) −ψ(y)‖2HYdρ(x, y) =\n∫\nX\n∫\nY ‖g(x) −ψ(y)‖2HYdρ(y|x)dρX (x). (31)\nNotice that g∗(x) = argminη∈HY ∫ Y ‖η − ψ(y)‖2HYdρ(y|x) almost everywhere on Dρ|X . Indeed, ∫\nY ‖η−ψ(y)‖2HYdρ(y|x) = ‖η‖ 2 HY − 2〈η,\n(∫\nY ψ(y)dρ(y|x)\n) 〉 + ∫\nY ‖ψ(y)‖2HYdρ(y|x)\n(32)\n= ‖η‖2HY − 2〈η, g ∗(x)〉HY + const. (33)\nfor all x ∈ Dρ|X , which is minimized by η = g∗(x) for all x ∈ Dρ|X . Therefore, since Dρ|X is equal to the support of ρX up to a set of measure zero, we conclude that R(g∗) ≤ infg:X→HY R(g) and, since g∗ is measurable, R(g∗) = ming:X→HY R(g) and g∗ is a global minimizer as required.\nFinally, notice that for any g : X → HY we have\nR(g) −R(g∗) = ∫\nX×Y ‖g(x) −ψ(y)‖2HY − ‖g ∗(x) −ψ(y)‖2HYdρ(x, y) (34)\n=\n∫\nX ‖g(x)‖2HY − 2〈g(x),\n(∫\nY ψ(y)dρ(y|x)\n) 〉HY + ‖g∗(x)‖2HYdρX (x) (35)\n=\n∫\nX ‖g(x)‖2HY − 2〈g(x), g ∗(x)〉HY + ‖g∗(x)‖2HYdρX (x) (36)\n=\n∫\nX ‖g(x) − g∗(x)‖2HYdρX (x) (37)\nTherefore, for any measurable minimizer g ′ : X → HY of the surrogate expected risk at Eq. (13), we have R(g ′) −R(g∗) = 0 which, by the relation above, implies g ′(x) = g∗(x) a.e. on Dρ|X .\nLemma 1. Let △ : Y ×Y → R satisfy Asm. 1 with ψ : Y → HY bounded. Then the expected risk in Eq. (1) can be written as\nE(f) = ∫\nX 〈ψ(f(x)), Vg∗(x)〉HY dρX (x) (12)\nfor all f : X → Y, where g∗ : X → HY minimizes\nR(g) = ∫\nX×Y ‖g(x) −ψ(y)‖2HY dρ(x, y). (13)\nProof. By Lemma 8 we know that g∗(x) = ∫ Y ψ(y)dρ(y|x) almost everywhere onDρ|X and is the minimizer of R. Therefore we have\n〈ψ(y), Vg∗(x)〉HY = 〈ψ(y), V ∫\nY ψ(y ′)dρ(y ′|x)〉HY (38)\n=\n∫\nY 〈ψ(y), Vψ(y ′)〉HYdρ(y ′|x) =\n∫\nY △(y, y ′)dρ(y ′|x) (39)\nfor almost every x ∈ Dρ|X . Thus, for any measurable function f : X → Y we have\nE(f) = ∫\nX×Y △(f(x), y)dρ(x, y) =\n∫\nX\n∫\nY △(f(x), y)dρ(y|x)dρX (x) (40)\n=\n∫\nX\n〈ψ(f(x)), Vg∗(x)〉HYdρX (x). (41)\nTheorem 2. Let △ : Y × Y → R satisfy Asm. 1 with Y a compact set. Then, for every measurable g : X → HY and d : HY → Y satisfying Eq. (14), the following holds\nE(d ◦ g∗) = E(f∗) (15) E(d ◦ g) − E(f∗) ≤ 2c△ √ R(g) −R(g∗). (16)\nwith c△ = ‖V‖maxy∈Y ‖ψ(y)‖HY . Proof. For the sake of clarity, the result for the fisher consistency and the comparison inequality are proven respectively in Thm. 9, Thm. 12. The two results are proven below.\nTheorem 9 (Fisher Consistency). Let △ : Y × Y → R satisfy Asm. 1 with Y a compact set. Let g∗ : X → HY be a minimizer of the surrogate problem at Eq. (13). Then, for any decoding d : HY → Y satisfying Eq. (14)\nE(d ◦ g∗) = inf f:X→Y E(f) (42)\nProof. It is sufficient to show that d ◦ g∗ satisfies Eq. (27) almost everywhere on Dρ|X . Indeed, by directly applying Cor. 7 we have E(d ◦ g∗) = E(f∗) = inff E(f) as required.\nWe recall that a mapping d : HY → Y is a decoding for our surrogate framework if it satisfies Eq. (14), namely\nd(η) ∈ argmin y∈Y 〈ψ(y), Vη〉HY ∀η ∈ HY . (43)\nBy Lemma 8 we know that g∗(x) = ∫ Y ψ(y)dρ(y|x) almost everywhere onDρ|X . Therefore, we have\n〈ψ(y), Vg∗(x)〉HY = 〈ψ(y), V ∫\nY ψ(y ′)dρ(y ′|x)〉HY (44)\n=\n∫\nY 〈ψ(y), Vψ(y ′)〉HYdρ(y ′|x) =\n∫\nY △(y, y ′)dρ(y ′|x) (45)\nfor almost every x ∈ Dρ|X . As a consequence, for any d : HY → Y satisfying Eq. (14), we have\nd ◦ g∗(x) ∈ argmin y∈Y 〈ψ(y), Vg∗(x)〉HY = argmin y∈Y\n∫\nY △(y, y ′)dρ(y ′|x) (46)\nalmost everywhere on Dρ|X . We are therefore in the hypotheses of Cor. 7 with f̃ = d ◦ g∗, as desired.\nThe Fisher consistency of the surrogate problem allows to prove the comparison inequality (Thm. 12) between the excess risk of the structured prediction problem, namely E(d ◦ g) − E(f∗), and the excess risk R(g) − R(g∗) of the surrogate problem. However, before showing such relation, in the following result we prove that for any measurable g : X → HY and measurable decoding d : HY → Y, the expected risk E(d ◦ g) is well defined.\nLemma 10. Let Y be compact and △ : Y × Y → R satisfying Asm. 1. Let g : X → HY be measurable and d : HY → Y a measurable decoding satisfying Eq. (14). Then E(d ◦ g) is well defined and moreover |E(d ◦ g)| ≤ Q2‖V‖.\nProof. △(d ◦ g(x), y) is measurable in both x and y since △ is continuous and d ◦ g is measurable by hypothesis (combination of measurable functions). Now, △ is pointwise bounded by Q2‖V‖ since\n|△ (y, y ′)| = |〈ψ(y), Vψ(y ′)〉HY | ≤ ‖ψ(y)‖2HY ‖V‖ ≤ Q 2‖V‖. (47)\nHence, by Theorem 11.23 pp. 416 in [3] the integral of △(d ◦ g(x), y) exists and therefore\n|E(d ◦ g)| ≤ ∫\nX |△ (d ◦ g(x), y)|dρ(x, y) ≤ Q2‖V‖ < +∞. (48)\nA question introduced by Lemma 10 is whether a measurable decoding always exists. The following result guarantees that, under the hypotheses introduced in this work, a decoding d : HY → Y satisfying Eq. (14) always exists.\nLemma 11. Let Y be compact and ψ : Y → HY and V : HY → HY satisfy the requirements in Asm. 1. Define m : X → R as\nm(η) = min y∈Y 〈ψ(y), Vη〉HY ∀y ∈ Y, η ∈ HY . (49)\nThen, m is measurable and there exists a measurable decoding d : HY → Y satisfying Eq. (14), namely such that m(η) = 〈ψ(d(η)), Vη〉HY for each η ∈ HY .\nProof. Similarly to the proof of Lemma 6, the result is a direct application of Theorem 18.19 (pp. 605) of [3] (or Aumann’s measurable selection principle [2,4]).\nWe now prove the comparison inequality at Eq. (16).\nTheorem 12 (Comparison Inequality). Let △ : Y ×Y → R satisfy Asm. 1 with Y a compact or finite set. Let f∗ : X → Y and g∗ : X → HY be respectively solutions to the structured and surrogate learning problems at Eq. (1) and Eq. (13). Then, for every measurable g : X → HY and d : HY → Y satisfying Eq. (14)\nE(d ◦ g) − E(f∗) ≤ 2Q‖V‖ √ R(g) −R(g∗). (50)\nProof. Let us denote f = d◦g and f0 = d◦g∗. By Thm. 9 we have that E(f0) = inff:X→Y E(f) and so E(f∗) = E(f0). Now, by combining Asm. 1 with Lemma 8, we have\nE(f) − E(f∗) = E(f) − E(f0) = ∫\nX×Y △(f(x), y) −△(f0(x), y)dρ(x, y) (51)\n=\n∫\nX×Y 〈ψ(f(x)) −ψ(f0(x)), Vψ(y)〉HYdρ(x, y) (52)\n=\n∫\nX 〈ψ(f(x)) −ψ(f0(x)), V\n(∫\nY ψ(y)dρ(y|x)\n) 〉HYdρX (x) (53)\n=\n∫\nX 〈ψ(f(x)) −ψ(f0(x)), Vg∗(x)〉HYdρX (x) (54) = A+ B. (55)\nwhere\nA =\n∫\nX 〈ψ(f(x)), V(g∗(x) − g(x))〉HY dρX (x) (56)\nB =\n∫\nX 〈ψ(f(x)), Vg(x)〉HYdρX (x) −\n∫\nX 〈ψ(f0(x)), Vg∗(x)〉HYdρX (x) (57)\nNow, the term A can be minimized by taking the supremum over Y so that\nA ≤ ∫\nX sup y∈Y\n∣∣∣〈ψ(y), V(g∗(x) − g(x))〉HY ∣∣∣dρX (x). (58)\nFor B, we observe that, by the definition of the decoding d, we have\n〈ψ(f0(x)), Vg∗(x)〉HY = inf y ′∈Y 〈ψ(y ′), Vg∗(x)〉HY , (59)\n〈ψ(f(x)), Vg(x)〉HY = inf y ′∈Y 〈ψ(y ′), Vg(x)〉HY , (60)\nfor all x ∈ X l. Therefore,\nB =\n∫\nX inf y∈Y 〈ψ(y), Vg(x)〉HY − inf y∈Y 〈ψ(y), Vg∗(x)〉HY dρX (x) (61)\n≤ ∫\nX sup y∈Y\n∣∣∣〈ψ(y), V(g(x) − g∗(x))〉HY ∣∣∣dρX (x) (62)\nwhere we have used the fact that for any given two functions η, ζ : Y → R we have\n| inf y∈Y η(y) − inf y∈Y ζ(y)| ≤ sup y∈Y |η(y) − ζ(y)|. (63)\nTherefore, by combining the bounds on A and B we have\nE(f) − E(f∗) ≤ 2 ∫\nX sup y∈Y\n∣∣∣〈ψ(y), V(g∗(x) − g(x))〉HY ∣∣∣dρX (x)\n≤ 2 ∫\nX sup y∈Y\n‖V∗ψ(y)‖HY ‖g∗(x) − g(x)‖HYdρX (x)\n≤ 2Q‖V‖ ∫\nX ‖g∗(x) − g(x)‖HYdρX (x)\n≤ 2Q‖V‖ √∫\nX ‖g∗(x) − g(x)‖2HYdρX (x),\nwhere for the last inequality we have used the Jensen’s inequality. The proof is concluded by recalling that (see Eq. (34))\nR(g) −R(g∗) = ∫\nX ‖g(x) − g∗(x)‖2HYdρX (x) (64)"
    }, {
      "heading" : "B Learning Bounds for Structured Prediction",
      "text" : "In this section we focus on the analysis of the structured prediction algorithm proposed in this work (Alg. 1). In particular, we will first prove that, given the minimizer ĝ : X → HY of the empirical risk at Eq. (5), its decoding can be computed in practice according to Alg. 1. Then, we report the proofs for the universal consistency of such approach (Thm. 4) and generalization bounds (Thm. 5)."
    }, {
      "heading" : "Notation",
      "text" : "Let k : X ×X → R a positive semidefinite function on X , we denote HX the Hilbert space obtained by the completion\nHX = span{k(x, ·) | x ∈ X } (65)\naccording to the norm induced by the inner product 〈k(x, ·), k(x ′, ·)〉HX = k(x, x ′). Spaces HX constructed in this way are known as reproducing kernel Hilbert spaces and there is a one-to-one relation between a kernel k and its associated RKHS. For more details on RKHS we refer the reader to [5]. Given a kernel k, in the following we will denote with ϕ : X → HX the feature map ϕ(x) = k(x, ·) ∈ HX for all x ∈ X . We say that a kernel is bounded if ‖ϕ(x)‖HX ≤ κ with κ > 0. Note that k is bounded if and only if k(x, x ′) = 〈ϕ(x), ϕ(x ′)〉HX ≤ ‖ϕ(x)‖HX ‖ϕ(x ′)‖ ≤ κ2 for every x, x ′ ∈ X . In the following we will always assume k to be continuous and bounded by κ > 0. The continuity of k with the fact that X is Polish implies HX to be separable [5].\nWe introduce here the ideal and empirical operators that we will use in the following to prove the main results of this work.\n• S : HX → L2(X , ρX ) s.t. f ∈ HX 7→ 〈f,ϕ(·)〉HX ∈ L2(X , ρX ), with adjoint • S∗ : L2(X , ρX ) → HX s.t. h ∈ L2(X , ρX ) 7→ ∫ X h(x)ϕ(x)dρX (x) ∈ HX ,\n• Z : HY → L2(X , ρX ) s.t. h ∈ HY 7→ 〈h, g∗(·)〉HY ∈ L2(X , ρX ), with adjoint • Z∗ : L2(X , ρX ) → HY s.t. h ∈ L2(X , ρX ) 7→ ∫ X h(x)g ∗(x)dρX (x) ∈ HY ,\n• C = S∗S : HX → HX and L = SS∗ : L2(X , ρX ) → L2(X , ρX ),\nwith g∗(x) = ∫ Y ψ(y)dρ(y|x) defined according to Eq. (30), (see Lemma 8).\nGiven a set of input-output pairs {(xi, yi)}ni=1 with (xi, yi) ∈ X × Y independently sampled according to ρ on X × Y, we define the empirical counterparts of the operators just defined as\n• Ŝ : HX → Rn s.t. f ∈ HX 7→ 1√n(〈ϕ(xi), f〉HX ) n i=1 ∈ Rn, with adjoint • Ŝ∗ : Rn → HX s.t. v = (vi)ni=1 ∈ Rn 7→ 1√n ∑n i=1 viϕ(xi),\n• Ẑ : HY → Rn s.t. h ∈ HY 7→ 1√n(〈ψ(yi), h〉HY ) n i=1 ∈ Rn, with adjoint • Ẑ∗ : Rn → HY s.t. v = (vi)ni=1 ∈ Rn 7→ 1√n ∑n i=1 viψ(yi),\n• Ĉ = Ŝ∗Ŝ : HX → HX and K = nŜŜ∗ ∈ Rn×n is the empirical kernel matrix.\nIn the rest of this section we denote with A + λ, the operator A + λI, for any symmetric linear operator A, λ ∈ R and I the identity operator.\nWe recall here a basic result characterizing the operators introduced above.\nProposition 13. With the notation introduced above,\nC =\n∫\nX ϕ(x)⊗ϕ(x)dρX (x) and Z∗S =\n∫\nX×Y ψ(y)⊗ϕ(x)dρ(x, y) (66)\nwhere ⊗ denotes the tensor product. Moreover, when ϕ and ψ are bounded by respectively κ and Q, we have the following facts\n(i) Tr(L) = Tr(C) = ‖S‖2HS = ∫ X ‖ϕ(x)‖2HX dρX (x) ≤ κ 2\n(ii) ‖Z‖2HS = ∫ X ‖g∗(x)‖2dρX (x) = ‖g∗‖2ρX < +∞.\nProof. By definition of C = S∗S, for each h, h ′ ∈ HX we have\n〈h,Ch ′〉HX = 〈Sh, Sh ′〉ρX = ∫\nX 〈h,ϕ(x)〉HX 〈ϕ(x), h ′〉HX dρX (x) (67)\n=\n∫\nX\n〈 h, ( ϕ(x)〈ϕ(x), h ′〉HX )〉 HX dρX (x) (68)\n=\n∫\nX\n〈 h, ( ϕ(x)⊗ϕ(x) ) h ′ 〉 dρX (x) (69)\n= 〈h, ( ∫\nX ϕ(x)⊗ϕ(x)dρX (x)\n) h ′〉HX (70)\nsince ϕ(x)⊗ϕ(x) : HX → HX is the operator such that h ∈ HX 7→ ϕ(x)〈ϕ(x), h〉HX . The characterization for Z∗S is analogous.\nNow, (i). The relation Tr(L) = Tr(C) = Tr(S∗S) = ‖S‖2HS holds by definition. Moreover\nTr(C) = ∫\nX Tr(ϕ(x)⊗ϕ(x))dρX (x) =\n∫\nX ‖ϕ(x)‖2HX dρX (x) (71)\nby linearity of the trace. (ii) is analogous. Note that ‖g∗‖2ρX < +∞. by Lemma 8 since ψ is bounded by hypothesis."
    }, {
      "heading" : "B.1 Reproducing Kernel Hilbert Spaces for Vector-valued Functions",
      "text" : "We begin our analysis by introducing the concept of reproducing kernel Hilbert space (RKHS) for vector-valued functions. Here we provide a brief summary of the main properties that will be useful in the following. We refer the reader to [6,7] for a more in-depth introduction on the topic.\nAnalogously to the case of scalar functions, a RKHS for vector-valued functions g : X → H, with H a separable Hilbert space, is uniquely characterized by a so-called kernel of positive type, which is an operator-valued Γ : X ×X → B(H,H) generalizing the concept of scalar reproducing kernel.\nDefinition 14. Let X be a set and H be a Hilbert space, then Γ : X × X → B(H,H) is a kernel of positive type if for each n ∈ N, x1, . . . , xn ∈ X , c1, . . . , cn ∈ H we have\nn∑\ni,j=1\n〈Γ(xi, xj)ci, cj〉H ≥ 0 (72)\nA kernel of positive type Γ defines an inner product 〈Γ(x, ·)c, Γ(x ′, ·)c ′〉G0 = 〈Γ(x, x ′)c, c ′〉H on the space G0 = span{Γ(x, ·)c | x ∈ X , c ∈ H}. (73) Then, the completion G = G0 with respect to the norm induced by 〈·, ·〉G0 is known as the reproducing Kernel Hilbert space (RKHS) for vector-valued functions associated to the kernel Γ . Indeed, we have that a reproducing property holds also for RKHS of vectorvalued functions, namely for any x ∈ X , c ∈ H and g ∈ G we have\n〈g(x), c〉H = 〈g, Γ(x, ·)c〉G (74)\nand that for each x ∈ X the function Γ(x, ·) : G → H is the evaluation functional in x on G, namely Γ(x, ·)(g) = g(x)."
    }, {
      "heading" : "B.1.1 Separable Vector Valued Kernels",
      "text" : "In this work we restrict to the special case of RKHS for vector-valued functions with associated kernel Γ : X × X → HY of the form Γ(x, x ′) = k(x, x ′)IHY for each x, x ′ ∈ X , where k : X × X → R is a scalar reproducing kernel and IHY is the identity operator on HY . Notice that this choice is not restrictive in terms of the space of functions that can\nbe learned by our algorithm. Indeed, it was proven in [8] (see Example 14) that if k is a universal scalar kernel, then Γ(·, ·) = k(·, ·)IHY is universal. Below, we report a useful characterization of RKHS G associated to a separable kernel.\nLemma 15. The RKHS G associated to the kernel Γ(x, x ′) = k(x, x ′)IHY is isometric to HY ⊗HX and for each g ∈ G there exists a unique G ∈ HY ⊗HX such that\ng(x) = Gϕ(x) ∈ HY for each x ∈ X (75)\nProof. We explicitly define the isometry T : G → HY ⊗HX as the linear operator such that T( ∑n i=1 αiΓ(xi, ·)ci) = ∑n\ni=1 αi ci ⊗ ϕ(xi) for each n ∈ N, x1, . . . , xn ∈ X and c1, . . . , cn ∈ HY . By construction, T(G0) ⊆ HY ⊗HX , with G0 the linear space defined at Eq. (73) and moreover\n〈T(Γ(x, ·)c), T(Γ(x ′, ·)c ′)〉HS = 〈c⊗ϕ(x)∗, c ′ ⊗ϕ(x ′)∗〉HS = 〈c, c ′〉HY 〈ϕ(x), ϕ(x ′)〉HX (76)\n= 〈k(x, x ′)c, c ′〉HY = 〈Γ(x, x ′)c, c ′〉HY = 〈Γ(x, ·)c, Γ(x ′, ·)c ′〉G (77)\nimplying that G0 is isometrically contained in HY ⊗HX . Since HY ⊗HX is complete, also T(G0) ⊆ HY ⊗HX . Therefore G is isometrically contained in HY ⊗HX , since T(G) = T(G0) = T(G0). Moreover note that\nT(G0) = span{c⊗ϕ(x) | x ∈ X , c ∈ HY } = span{c | c ∈ HY }⊗ span{ϕ(x) | x ∈ X } (78) = HY ⊗ span{ϕ(x) | x ∈ X } = HY ⊗ span{ϕ(x) | x ∈ X } = HY ⊗HX (79)\nfrom which we conclude that G is isometric to HY ⊗HX via T . To prove Eq. (75), let us consider x ∈ X and g ∈ G with G = T(g) ∈ HY ⊗HX . Then, ∀c ∈ HY we have that\n〈c, g(x)〉HY = 〈Γ(x, ·)c, g〉G = 〈T(Γ(x, ·)c), G〉HS (80) = 〈c⊗ϕ(x), G〉HS = Tr((ϕ(x) ⊗ c)G) = Tr(c∗Gϕ(x)) = 〈c,Gϕ(x)〉HY . (81)\nSince the equation above is true for each c ∈ HY we can conclude that g(x) = T(g)ϕ(x) as desired.\nThe isometry G ≃ HY ⊗HX allows to characterize the closed form solution for the surrogate risk introduced in Eq. (13). We recall that in Lemma 8, we have shown that R always attains a minimizer on L2(X , ρX ,HY). In the following we show that if R attains a minimum on G ≃ HY ⊗HX , we are able to provide a close form solution for one such element.\nLemma 16. Let ψ and HY satisfying Asm. 1 and assume that the surrogate expected risk minimization of R at Eq. (13) attains a minimum on G, with G ≃ HY ⊗HX . Then the minimizer g∗ ∈ G of R with minimal norm ‖ · ‖G is of the form\ng∗(x) = Gϕ(x), ∀x ∈ X with G = Z∗SC† ∈ HY ⊗HX . (82)\nProof. By hypothesis we have g∗ ∈ G. Therefore, by applying Lemma 15 we have that there exists a unique linear operator G : HX → HY such that g∗(x) = Gϕ(x),∀x ∈ X . Now, expanding the least squares loss on HY , we obtain\nR(g) = ∫\nX×Y ‖Gϕ(x) −ψ(y)‖2HYdρ(x, y) (83)\n= Tr(G(ϕ(x)⊗ϕ(x))G∗)dρX (x, y) − 2Tr(G(ϕ(x)⊗ψ(y))) + ∫\nX×Y ‖ψ(y)‖2HY (84)\n= Tr(GCG∗) − 2Tr(GS∗Z) + const. (85)\nwhere we have used Prop. 13 and the linearity of the trace. Therefore R is a quadratic functional, and is convex since C is positive semidefinite. We can conclude that R attains a minimum on G if and only if the range of S∗Z is contained in the range of C, namely Ran(S∗Z) ⊆ Ran(C) ⊂ HX (see [9] Chap. 2). In this case G = Z∗SC† ∈ HY ⊗HX exists and is the minimum norm minimizer for R, as desired.\nAnalogously to Lemma 16, a closed form solution exists for the regularized surrogate empirical risk minimization problem introduced in Eq. (5). We recall that the associated functional is R̂λ : G → R defined as\nR̂λ(g) = R̂(g) + λ ‖g‖2G = 1\nn\nn∑\ni=1\n‖g(xi) −ψ(yi)‖2HY + λ ‖g‖ 2 HY (86)\nfor all g ∈ G and {(xi, yi)}ni=1 points in X ×Y. The following result characterizes the closed form solution for the empirical risk minimization when G ≃ HY ⊗HX and guarantees that such a solution always exists.\nLemma 17. Let G ≃ HY ⊗HX . For any λ > 0, the solution ĝλ ∈ G of the empirical risk minimization problem at Eq. (5) exists, is unique and is such that\nĝλ(x) = Ĝλϕ(x), ∀x ∈ X with Ĝλ = Ẑ∗Ŝ(Ĉ + λ)−1 ∈ HY ⊗HX . (87)\nProof. The proof of is analogous to that of Lemma 16 and we omit it. Note that, since (Ĉ + λ)−1 is always bounded for λ > 0, its range corresponds to HX and therefore the range of S∗Z is always contained in it. Then Ĝλ exists for any λ > 0 and is unique since ‖ · ‖2HY is strictly convex.\nThe closed form solutions provided by Lemma 16 and Lemma 17 will be key in the analysis of the structured prediction algorithm Alg. 1 in the following."
    }, {
      "heading" : "B.2 The Structured Prediction Algorithm",
      "text" : "In this section we prove that Alg. 1 corresponds to the decoding of the surrogate empirical risk minimizer ĝ (Eq. (5)) via a map d : HY → Y satisfying Eq. (14).\nRecall that in Lemma 15 we proved that the vector-valued RKHS G induced by a kernel Γ(x, x ′) = k(x, x ′)IHY , for a scalar kernel k on X , is isometric to HY ⊗HX . For the sake of simplicity, in the following, with some abuse of notation, we will not make the distinction between G and HY ⊗HX when it is clear from context.\nLemma 3. Let △ : Y × Y → R satisfy Asm. 1 with Y a compact set. Let ĝ : X → HY be the minimizer of Eq. (5). Then, for all x ∈ X\nd ◦ ĝ(x) = argmin y∈Y\nn∑\ni=1\nαi(x)△ (y, yi) α(x) = (K+ nλI)−1Kx ∈ Rn (17)\nProof. From Lemma 17 we know that ĝ(x) = Ẑ∗Ŝ(Ĉ + λ)−1ϕ(x) for all x ∈ X . Recall that Ĉ = Ŝ∗Ŝ and K = nŜŜ∗ ∈ Rn×n, is the empirical kernel matrix associated to the inputs, namely such that Kij = k(xi, xj) for each i, j = 1, . . . , n. Therefore we have Ŝ(Ĉ + λ)−1 =√ n(K + λn)−1Ŝ : HX → Rn. Now, by denoting Kx = √ nŜϕ(x) = (k(xi, x)) n i=1 ∈ Rn, we have\nŜ(Ĉ + λ)−1ϕ(x) = (K + λnI)−1Kx = α(x) ∈ Rn. (88)\nTherefore, by applying the definition of the operator Ẑ : HY → Rn, we have\nĝ(x) = Ẑ∗Ŝ(Ĉ+ λ)−1ϕ(x) = Ẑ∗α(x) = n∑\ni=1\nαi(x)ψ(yi), ∀x ∈ X (89)\nBy plugging ĝ(x) in the functional minimized by the decoding (Eq. (14)),\n〈ψ(y), Vĝ(x)〉HY = 〈ψ(y),VẐ∗α(x)〉HY = 〈ψ(y), n∑\ni=1\nαi(x)ψ(yi)〉HY (90)\n=\nn∑\ni=1\nαi(x)〈ψ(y), Vψ(yi)〉HY = n∑\ni=1\nαi(x)△ (y, yi), (91)\nwhere we have used the bilinear form for △ in Asm. 1 for the final equation. We conclude that\nd ◦ ĝ(x) ∈ argmin y∈Y 〈ψ(y), Vĝ(x)〉HY = argmin y∈Y\nn∑\ni=1\nαi(x)△ (y, yi) (92)\nas required.\nLemma 3 focuses on the computational aspects of Alg. 1. In the following we will analyze its statistical properties."
    }, {
      "heading" : "B.3 Universal Consistency",
      "text" : "In following, we provide a probabilistic bound on the excess risk E(d ◦ g) − E(f∗) for any g ∈ G ≃ HY ⊗HX that will be key to prove both universal consistency (Thm. 4) and generalization bounds (Thm. 5). To do so, we will make use of the comparison inequality from Thm. 12 to control the structured excess risk by means of the excess risk of the surrogate R(g) − R(g∗). Note that the surrogate problem consists in a vector-valued kernel ridge regression estimation. In this setting, the problem of finding a probabilistic bound has been studied (see [10] and references therein). Indeed, our proof will consist\nof a decomposition of the surrogate excess risk that is similar to the one in [10]. However, note that we cannot direct apply [10] to our setting, since in [10] the operator-valued kernel Γ associated to G is required to be such that Γ(x, x ′) is trace class ∀x, x ′ ∈ X , which does not hold for the kernel used in this work, namely Γ(x, x ′) = k(x, x ′)IHY when HY is infinite dimensional.\nIn order to express the bound on the excess risk more compactly, here we introduce a measure for the approximation error of the surrogate problem. According to [10], we define the following quantity\nA(λ) = λ‖Z∗(L+ λ)−1‖HS. (93)\nLemma 18. Let Y be compact, △ : Y × Y → R satisfying Asm. 1 and k a bounded positive definite kernel on X with supx∈X k(x, x) ≤ κ2 and associated RKHS HX . Let ρ a Borel probability measure on X × Y and {(xi, yi)}ni=1 independently sampled according to ρ. Let f∗ be a solution of the problem in Eq. (1), f̂ = d◦ ĝ as in Alg. 1. Then, for any λ ≤ κ2 and δ > 0, the following holds with probability 1− δ:\nE(f̂) − E(f∗) ≤ 8κQ‖V‖Q + B(λ)√ λn\n( 1+ √ 4κ2\nλ √ n\n) log2 8\nδ + 2Q‖V‖A(λ), (94)\nwith B(λ) = κ‖Z∗S(C+ λ)−1‖HS.\nProof. According to Thm. 12\nE(d ◦ ĝ) − E(f∗) ≤ 2Q‖V‖ √ R(ĝ) −R(g∗). (95)\nFrom Lemma 17 we know that ĝ(x) = Ĝϕ(x) for all x ∈ X , with Ĝ = Ẑ∗Ŝ(Ĉ + λ)−1 ∈ HY ⊗HX . By Lemma 8, we know that g∗(x) = ∫ Y ψ(y)dρ(y|x) almost everywhere on the support of ρX . Therefore, a direct application of Prop. 13 leads to\nR(ĝ) −R(g∗) = ∫\n‖ĝ(x) − g∗(x)‖2HYdρX (x) = (96)\n=\n∫\nX ‖Ĝϕ(x)‖2HY − 2〈Ĝϕ(x), g ∗(x)〉HY + ‖g∗(x)‖2HYdρX (x) (97)\n=\n∫\nX Tr\n( Ĝ ( ϕ(x)⊗ϕ(x) ) Ĝ∗ ) − 2Tr ( Ĝ ( ϕ(x)⊗ g∗(x) )) + Tr(g∗(x)⊗ g∗(x))dρX (x)\n(98)\n= Tr(ĜS∗SĜ) − 2Tr(ĜS∗Z) + Tr(Z∗Z) = ‖ĜS∗ − Z∗‖2HS (99)\nTo bound ‖ĜS∗ − Z∗‖HS, we proceed with a decomposition similar to the one in [10]. In particular ‖ĜS∗ − Z∗‖HS ≤ A1 +A2 +A3, with\nA1 = ‖Ẑ∗Ŝ(Ĉ+ λ)−1S∗ − Z∗S(Ĉ+ λ)−1S∗‖HS (100) A2 = ‖Z∗S(Ĉ+ λ)−1S∗ − Z∗S(C+ λ)−1S∗‖HS (101) A3 = ‖Z∗S(C+ λ)−1S∗ − Z∗‖HS. (102)\nLet τ = δ/4. Now, for the term A1, we have\nA1 ≤ ‖Ẑ∗Ŝ− Z∗S‖HS‖(Ĉ + λ)−1S∗‖. (103)\nTo control the term ‖Ẑ∗Ŝ−Z∗S‖HS, note that Ẑ∗Ŝ = 1n ∑n\ni=1 ζi with ζi the random variable ζi = ψ(yi)⊗ϕ(xi). By Prop. 13, for any 1 ≤ i ≤ n we have\nEζi =\n∫\nψ(y)⊗ϕ(x)dρ(x, y) = Z∗S, (104)\nand\n‖ζi‖HS ≤ sup y∈Y ‖ψ(y)‖HY sup x∈X ‖ϕ(x)‖HX ≤ Qκ, (105)\nalmost surely on the support of ρ on X × Y, and so E‖ζi‖2HS ≤ Q2κ2. Thus, by applying Lemma 2 of [11], we have\n‖Ẑ∗Ŝ− Z∗S‖HS ≤ 2Qκ log 2 τ\nn +\n√ 2Q2κ2 log 2\nτ n ≤ 4Qκ log 2 τ n (106)\nwith probability 1− τ, since log 2/τ ≥ 1. To control ‖(Ĉ+ λ)−1S∗‖ we proceed by recalling that C = S∗S and that for any λ > 0 ‖(Ĉ + λ)−1‖ ≤ λ−1 and ‖Ĉ(Ĉ+ λ)−1‖ ≤ 1. We have\n‖(Ĉ + λ)−1S∗‖ = ‖(Ĉ + λ)−1C(Ĉ+ λ)−1‖1/2 (107) ≤ ‖(Ĉ + λ)−1(C− Ĉ)(Ĉ + λ)−1‖1/2 + ‖(Ĉ + λ)−1Ĉ(Ĉ + λ)−1‖1/2 (108) ≤ ‖(Ĉ + λ)−1‖‖C − Ĉ‖1/2 + ‖(Ĉ + λ)−1‖1/2‖Ĉ(Ĉ+ λ)−1‖1/2 (109) ≤ λ−1/2(1+ λ−1/2‖C − Ĉ‖1/2). (110)\nTo control ‖C − Ĉ‖, note that Ĉ = 1n ∑n\ni=1 ζi where ζi is the random variable defined as ζi = ϕ(xi) ⊗ ϕ(xi) for 1 ≤ i ≤ n. Note that Eζi = C, ‖ζi‖ ≤ κ2 almost surely and so E‖ζi‖2 ≤ κ4 for 1 ≤ i ≤ n. Thus we can again apply Lemma 2 of [11], obtaining\n‖C− Ĉ‖ ≤ ‖C − Ĉ‖HS ≤ 2κ2 log 2 τ\nn +\n√ 2κ4 log 2\nτ n ≤ 4κ 2 log 2 τ√ n , (111)\nwith probability 1− τ. Thus, by performing an intersection bound, we have\nA1 ≤ 4Qκ log 2\nτ√ λn\n 1+ √ 4κ2 log 2 τ\nλ √ n\n  . (112)\nwith probability 1− 2τ. The term A2 can be controlled as follows\nA2 = ‖Z∗S(Ĉ+ λ)−1S∗ − Z∗S(C+ λ)−1S∗‖HS (113) = ‖Z∗S((Ĉ+ λ)−1 − (C + λ)−1)S∗‖HS (114) = ‖Z∗S(C+ λ)−1(C− Ĉ)(Ĉ + λ)−1S∗‖HS (115) ≤ ‖Z∗S(C+ λ)−1‖HS‖C− Ĉ‖‖(Ĉ + λ)−1S∗‖ (116) = k−1B(λ)‖C − Ĉ‖‖(Ĉ + λ)−1S∗‖ (117)\nwhere we have used the fact that for two invertible operatorsA and Bwe haveA−1−B−1 = A−1(B−A)B−1. Now, by controlling ‖C− Ĉ‖, ‖(Ĉ+ λ)−1S∗‖ as for A1 and performing an intersection bound, we have\nA2 ≤ 4B(λ)κ log 2τ√\nλn\n 1+ √ 4κ2 log 2τ λ √ n   (118)\nwith probability 1− 2τ. Finally the term A3 is equal to\nA3 = ‖Z∗(S(C+ λ)−1S∗ − I)‖HS = ‖Z∗(L(L + λ)−1 − I)‖HS (119) = ‖Z∗(L(L+ λ)−1 − (L + λ)(L + λ)−1)‖HS = λ‖Z∗(L+ λ)−1‖HS = A(λ) (120)\nwhere I denotes the identity operator. Thus, by performing an intersection bound of the events for A1 and A2, we have\nE(f̂) − E(f∗) ≤ 8κQ‖V‖Q + B(λ)√ λn\n( 1+ √ 4κ2\nλ √ n\n) log2 2\nτ + 2Q‖V‖A(λ). (121)\nwith probability 1− 4τ. Since δ = 4τ we obtain the desired bound.\nNow we are ready to give the universal consistency result.\nTheorem 4 (Universal Consistency). Let△ : Y×Y → R satisfy Asm. 1, X and Y be compact sets and k : X × X → R a continuous universal reproducing kernel6. For any n ∈ N and any distribution ρ on X × Y let f̂n : X → Y be obtained by Alg. 1 with {(xi, yi)}ni=1 training points independently sampled from ρ and λn = n −1/4. Then,\nlim n→+∞\nE(f̂n) = E(f∗) with probability 1 (18)\nProof. By applying Lemma 18, we have\nE(f̂) − E(f∗) ≤ 8κQ‖V‖Q + B(λ)√ λn\n( 1+ √ 4κ2\nλ √ n\n) log2 8\nδ + 2Q‖V‖A(λ), (122)\nwith probability 1− δ. Note that, since C = S∗S, ‖(C+ λ)−1‖ ≤ λ−1 and ‖C(C+ λ)−1‖ ≤ 1, we have\nκ−1B(λ) = ‖Z∗S(C+ λ)−1‖HS ≤ ‖Z‖HS‖S(C+ λ)−1‖ (123) ≤ ‖Z‖HS‖(C + λ)−1S∗S(C+ λ)−1‖1/2 (124) ≤ ‖Z‖HS‖(C + λ)−1‖1/2‖C(C + λ)−1‖1/2 (125) ≤ ‖Z‖HSλ−1/2. (126)\n6This is a standard assumption for universal consistency (see [2]). An example of continuous universal kernel is the Gaussian k(x, x ′) = exp(−‖x− x ′‖2/σ).\nTherefore\nE(f̂) − E(f∗) ≤ 8κQ‖V‖ Q + κ√ λ ‖Z‖HS\n√ λn\n( 1+ √ 4κ2\nλ √ n\n) log2 8\nδ + 2Q‖V‖A(λ), (127)\nNow by choosing λ = κ2n−1/4, we have\nE(f̂n) − E(f∗) ≤ 24Q‖V‖(Q + ‖Z‖HS)n−1/4 log2 8\nδ + 2Q‖V‖A(λ), (128)\nwith probability 1− δ. Now we study A(λ), let L = ∑i∈N σi ui ⊗ ui be the eigendecomposition of the compact operator L, with σi ≥ σj > 0 for 1 ≤ i ≤ j ∈ N and ui ∈ L2(X , ρX ). Now, let w2i = 〈ui, ZZ∗ui〉L2(X ,ρX ,R) = ∫ 〈g∗(x), ui〉2HYdρX (x) for i ∈ N. We need to prove that (ui)i∈N is a basis for L2(X , ρX ). Let W ⊆ X be the support of ρX , note that W is compact and Polish since it is closed and subset of the compact Polish space X . Let L be the RKHS defined by L = span{k(x, ·) | x ∈ W}, with the same inner product of HX . By the fact that W is a compact polish space and k is continuous, then L is separable. By the universality of k we have that L is dense in C(W), and, by Corollary 5 of [12], we have C(W) = span{ui | i ∈ N}. Thus, since C(W) is dense in L2(X , ρX ), we have (ui)i∈N is a basis of L2(X , ρX ). Thus ∑ i∈N w 2 i = ∫ ‖g∗(x)‖2HYdρX (x) = ‖Z‖ 2 HS < ∞. Therefore\nA(λn)2 = λ2n‖Z∗(L+ λn)−1‖2HS = ∑\ni∈N\nλ2nw 2 i\n(σi + λ)2 . (129)\nLet tn = n−1/8, and Tn = {i ∈ N | σi ≥ tn} ⊂ N. For any n ∈ N we have\nA(λn) = ∑\ni∈Tn\nλ2nw 2 i\n(σi + λn)2 +\n∑\ni∈N\\Tn\nλ2nw 2 i\n(σi + λn)2 (130)\n≤ λ 2 n\nt2n\n∑ i∈Tn w2i + ∑ i∈N\\Tn w2i ≤ κ4‖Z‖2HS n−1/4 + ∑ i∈N\\Tn w2i (131)\nsince λn/tn = κ2n−1/4/n−1/8 = κ2n−1/8. We recall that L is a trace class operator, namely Tr(L) = ∑+∞ i=1 σi < +∞. Therefore σi → 0 for i → +∞, from which we conclude\n0 ≤ lim n→∞ A(λn) ≤ lim n→∞\nκ4‖Z‖2HSn−1/4 + ∑\ni∈N\\Tn w2i = 0. (132)\nNow, for any n ∈ N, let δn = n−2 and En be the event associated to the equation E(f̂n) − E(f∗) > 24Q‖V‖(Q + ‖Z‖HS)n−1/4 log2(8n2) + 2Q‖V‖A(λ). (133) By Lemma 18, we know that the probability of En is at most δn. Since ∑+∞\nn=1 δn < +∞, we can apply the Borel-Cantelli lemma (Theorem 8.3.4. pag 263 of [1]) on the sequence (En)n∈N and conclude that the statement\nlim n→∞\nE(f̂n) − E(f∗) > 0, (134)\nholds with probability 0. Thus, the converse statement\nlim n→∞\nE(f̂n) − E(f∗) = 0. (135)\nholds with probability 1."
    }, {
      "heading" : "B.4 Generalization Bounds",
      "text" : "Finally, we show that under the further hypothesis that g∗ belongs to the RKHS G ≃ HY ⊗HX , we are able to prove generalization bounds for the structured prediction algorithm.\nTheorem 5 (Generalization Bound). Let △ : Y × Y → R satisfy Asm. 1, Y be a compact set and k : X × X → R a bounded continuous reproducing kernel. Let f̂n denote the solution of Alg. 1 with n training points and λ = n−1/2. If the surrogate risk R defined in Eq. (13) admits a minimizer g∗ ∈ G, then\nE(f̂n) − E(f∗) ≤ cτ2 n− 1 4 (19)\nholds with probability 1− 8e−τ for any τ > 0, with c a constant not depending on n and τ.\nProof. By applying 18, we have\nE(f̂) − E(f∗) ≤ 8κQ‖V‖Q + B(λ)√ λn\n( 1+ √ 4κ2\nλ √ n\n) log2 8\nδ + 2Q‖V‖A(λ), (136)\nwith probability 1− δ. By assumption, g∗ ∈ G and therefore there exists a G ∈ HY ⊗HX such that\ng∗(x) = Gϕ(x), ∀x ∈ X . (137) This implies that Z∗ = GS∗ since, by definition of Z and S, for any h ∈ L2(X , ρX ),\nZ∗h = ∫\nX g∗(x)h(x)dρX (x) =\n∫\nX Gϕ(x)h(x)dρX (x) = GS\n∗h. (138)\nThus, since L = SS∗ and ‖(L + λ)−1L‖ ≤ 1 and ‖(L + λ)−1‖ ≤ λ−1 for any λ > 0, we have A(λ) = λ‖Z∗(L + λ)−1‖HS = λ‖GS∗(L+ λ)−1‖HS (139)\n≤ λ‖G‖HS‖S∗(L+ λ)−1‖ = λ‖G‖HS‖(L + λ)−1SS∗(L+ λ)−1‖1/2 (140) ≤ λ‖G‖HS‖(L + λ)−1L‖1/2‖(L + λ)−1‖1/2 (141) ≤ λ1/2‖G‖HS. (142)\nMoreover, since C = S∗S, we have\nκ−1B(λ) = ‖Z∗S(C+ λ)−1‖HS = ‖GS∗S(C+ λ)−1‖HS (143) ≤ ‖G‖HS‖C(C + λ)−1‖ (144) ≤ ‖G‖HS. (145)\nNow, let λ = κ2n−1/4, we have\nE(f̂) − E(f∗) ≤ 24Q‖V‖(Q + κ‖G‖HS)n−1/4 log2 8\nδ + 2Q‖V‖κn−1/4 (146)\n≤ 24Q‖V‖(Q + κ‖G‖HS + κ)n−1/4 log2 8\nδ (147)\n= cτ2n−1/4 (148)\nwith probability 1− 8e−τ, where we have set δ = 8e−τ and c = 24Q‖V‖(Q + κ‖G‖HS + κ) to obtain the desired inequality."
    }, {
      "heading" : "C Examples of Loss Functions",
      "text" : "In this section we prove Thm. 19 to show that a wide range of functions △ : Y × Y → R useful for structured prediction learning satisfies the loss trick (Asm. 1). In the following we state Thm. 19, then we use it to prove that all the losses considered in Example 1 satisfy Asm. 1. Finally we give two lemmas, necessary to prove Thm. 19 and then conclude with its proof.\nTheorem 19. Let Y be a set. A function △ : Y × Y → R satisfy Asm. 1 when at least one of the following conditions hold:\n1. Y is a finite set, with discrete topology.\n2. Y = [0, 1]d with d ∈ N, and the mixed partial derivative L(y, y ′) = ∂ 2d△(y1,...,yd,y ′1,...,y ′d) ∂y1,...,∂yd,∂y ′ 1 ,...,∂y ′ d\nexists almost everywhere, where y = (yi) d i=1, y ′ = (y ′i) d i=1 ∈ Y, and satisfies\n∫\nY×Y |L(y, y ′)|1+ǫdydy ′ < ∞, with ǫ > 0. (149)\n3. Y is compact and △ is a continuous kernel, or △ is a function in the RKHS induced by a kernel K. Here K is a continuous kernel on Y × Y, of the form\nK((y1, y2), (y ′ 1, y ′ 2)) = K0(y1, y ′ 1)K0(y2, y ′ 2), ∀yi, y ′i ∈ Y, i = 1, 2,\nwith K0 a bounded and continuous kernel on Y.\n4. Y is compact and Y ⊆ Y0, △ = △0|Y ,\nthat is the restriction of △0 : Y0 × Y0 → R on Y, and △0 satisfies Asm. 1 on Y0,\n5. Y is compact and △(y, y ′) = f(y)△0 (F(y), G(y ′))g(y ′),\nwith F,G continuous maps from Y to a set Z with △0 : Z × Z → R satisfying Asm. 1 and f, g : Y → R, bounded and continuous.\n6. Y compact and △ = f(△1, . . . ,△p),\nwhere f : [−M,M]d → R is an analytic function (e.g. a polynomial), p ∈ N and △1, . . . ,△p satisfy Asm. 1 on Y. Here M ≥ sup1≤i≤p ‖Vi‖Ci where Vi is the operator associated to the loss △i and Ci is the value that bounds the norm of the feature map ψi associated to △i, with i ∈ {1, . . . , p}.\nBelow we expand Example 1 by proving that the considered losses satisfy Asm. 1. The proofs are typically a direct application of Thm. 19 above.\n1. Any loss with, Y finite. This is a direct application of Thm. 19, point 1.\n2. Regression and classification loss functions. Here Y is an interval on R and the listed loss functions satisfies Thm. 19, point 2. For example, let Y = [−π, π] the mixed partial derivative of the Hinge loss △(y, y ′) = max(0, 1 − yy ′) is defined almost everywhere as L(y, y ′) = −1 when yy ′ < 1 and L(y, y ′) = 0 otherwise. Note that L satisfies Eq. (149), for any λ > 0.\n3. Robust loss functions. Here, again Y is an interval on R. The listed loss functions are: Cauchy γ log(1 + |y − y ′|2/γ), German-McLure |y − y ′|2/(1 + |y − y ′|2) “Fair” γ|y − y ′| − γ2 log(1 + |y − y ′|/γ) or the “L2 − L1” √ 1+ |y− y ′|2 − 1. They are\ndifferentiable on R, hence satisfy Thm. 19, point 2. The Absolute value |y − y ′| is Lipschitz and satisfies Thm. 19, point 2, as well.\n4. KDE. When Y is a compact set and△ is a kernel, the point 3 of Thm. 19 is applicable.\n5. Diffusion Distances on Manifolds. Let M ∈ N and Y ⊂ RM be a compact Reimannian manifold. The heat kernel (at time t > 0), kt : Y × Y → R induced by the LaplaceBeltrami operator of Y is a reproducing kernel [13]. The squared diffusion distance is defined in terms of kt as follows △(y, y ′) = 1 − kt(y, y ′). Then, point 3 of Thm. 19 is applicable.\n6. Distances on Histograms/Probabilities. LetM ∈ N. A discrete probability distribution (or a normalized histogram) over M entries can be represented as a y = (yi)Mi=1 ∈ Y ⊂ [0, 1]M the M-simplex, namely ∑Mi=1 yi = 1 and yi ≥ 0 ∀i = 1, . . . ,M. A typical measure of distance on Y is the squared Hellinger (or Bhattacharya) △H(y, y ′) = (1/ √ 2)‖√y − √y ′‖22, with √ y = ( √ yi) M i=1. By Thm. 19, points 4, 6 we have that\n△H satisfies Asm. 1. Indeed, consider the kernel k on R, k(r, r ′) = ( √ rr ′ + 1)2 with\nfeature map ϕ(r) = (r, √ 2r, 1)⊤ ∈ R3, Then\n△0(r, r ′) = ( √ r− √ r ′)2 = r−2 √ rr ′+r ′ = ϕ(r)⊤Vϕ(r ′) with V =\n  0 0 1\n0 −1 0\n1 0 0\n  .\n△H is obtained by M summations of △0 on [0, 1], by Thm. 19, point 6 (indeed △H(y, y ′) = f(△0(y1, y ′1), . . . ,△0(yM, y ′M)) with y = (yi)Mi=1, y ′ = (y ′i)Mi=1 ∈ Y and the function f : RM → R defined as f(t1, . . . , tM) = ∑M i=1 ti, which is analytic onR\nM), and then restriction on Y Thm. 19, point 4. A similar reasoning holds when the loss function is the χ2 distance on histograms. Indeed the function (r − r ′)2/(r + r ′) satisfies point 2 on Y = [0, 1], then point 6 and 4 are applied.\nTo prove Thm. 19 we need the following two Lemmas.\nLemma 20 (multiple Fourier series). Let D = [−π, π]d, (f̂h)h∈Zd ∈ C and f : D → C with d ∈ N defined as\nf(y) = ∑\nh∈Zd f̂he\nih⊤y, ∀y ∈ D, with ∑\nh∈Zd |f̂h| ≤ B,\nfor a B < ∞ and i = √ −1. Then the function f is continuous and\nsup y∈D\n|f(y)| ≤ B.\nProof. For the continuity, see [14] (pag. 129 and Example 2). For the boundedness we have\nsup y∈D |f(y)| ≤ sup y∈D\n∑\nh∈Zd |f̂h||e\nih⊤y| ≤ ∑\nh∈Zd |f̂h| ≤ B.\nLemma 21. Let Y = [−π, π]d with d ∈ N, and △ : Y × Y → R defined by\n△(y, z) = ∑\nh,k∈Zd △̂h,keh(y)ek(z), ∀y, z ∈ Y,\nwith eh(y) = e ih⊤y for any y ∈ Y, i = √ −1 and △̂h,k ∈ C for any h, k ∈ Zd. The loss △ satisfies Asm. 1 when ∑\nh,k∈Zd |△̂h,k| < ∞.\nProof. Note that by applying Lemma 20 with D = Y × Y, the function △ is bounded continuous. Now we introduce the following sequences\nαh = ∑\nk∈Zd |△̂h,k|, fh(z) =\n1\nαh\n∑\nh∈Zd △̂hkek(z) ∀h ∈ Zd, z ∈ Y.\nNote that (αh)h∈Zd ∈ ℓ1 and that fh bounded by 1 and is continuous by Lemma 20 for any h ∈ Zd. Therefore\n△(y, z) = ∑\nh,k∈Zd △̂h,keh(y)eh(z) =\n∑\nh∈Zd αheh(y)fh(z).\nNow we define two feature maps ψ1, ψ2 : Y → H0, with H0 = ℓ2(Zd), as\nψ1(y) = ( √ αheh(y))h∈Zd , ψ2(y) = ( √ αhfh(y))h∈Zd , ∀y ∈ Y.\nNowwe prove that the two feature maps are continuous. Define k1(y, z) = 〈ψ1(y), ψ1(z)〉H0 and k2(y, z) = 〈ψ2(y), ψ2(z)〉H0 for all y, z ∈ Y. We have\nk1(y, z) = ∑\nh∈Zd αheh(y)eh(z), (150)\nk2(y, z) = ∑\nh∈Zd αhfh(y)fh(z) =\n∑\nk,l∈Zd βk,lek(y)el(z) (151)\nwith βk,l = ∑ h∈Zd △̂h,k△̂h,l αh , for k, l ∈ Zd, therefore k1, k2 are bounded and continuous by\nLemma 20 with D = Y × Y, since ∑h∈Zd αh < ∞ and ∑\nk,l∈Zd |βk,l| < ∞. Note that ψ1 and ψ2 are bounded, since k1 and k2 are. Moreover for any y, z ∈ Y, we have\n‖ψ1(y) −ψ1(z)‖2 = 〈ψ1(z), ψ1(z)〉H0 + 〈ψ1(y), ψ1(y)〉H0 − 2〈ψ1(z), ψ1(y)〉H0 (152) = k1(z, z) + k1(y, y) − 2k1(z, y) ≤ |k1(z, z) − k1(z, y)| + |k1(z, y) − k1(y, y)|,\n(153)\nand the same holds for ψ2 with respect to k2. Thus the continuity of ψ1 is entailed by the continuity of k1 and the same for ψ2 with respect to k2. Now we define HY = H0 ⊕ H0 and ψ : Y → HY and V : HY → HY as\nψ(y) = (ψ1(y), ψ2(y)), ∀y ∈ Y and V = ( 0 I\n0 0\n) ,\nwhere I : H0 → H0 is the identity operator. Note that ψ is bounded continuous, V is bounded and\n〈ψ(y), Vψ(z)〉HY = 〈ψ1(y), ψ2(z)〉H0 = ∑\nh∈Zd αheh(y)fh(z) = △(y, z).\nWe can now prove Thm. 19.\nProof. (Thm. 19)\n1 Let N = {1, . . . , |Y |} and q : Y → N be a one-to-one function. Let HY = R|Y |, ψ(y) : Y → HY defined by ψ(y) = eq(y) for any y ∈ Y with (ei)|Y |i=1 the canonical basis for HY , finally V ∈ R|Y |×|Y | with Vi,j = △(q−1(i), q−1(j)) for any i, j ∈ N. Then △ satisfies Asm. 1, with ψ and V .\n2 By Lemma 21, we know that any loss △ whose Fourier expansion is absolutely summable, satisfies Asm. 1. The required conditions in points 2 are sufficient ( see Theorem 6’ pag. 291 of [15]).\n3 Let Y be a compact space. For the first case let △ be a bounded and continuous reproducing kernel on Y and letHY the associated RKHS, then there exist a bounded and continuous map ψ : Y → HY such that △(y, y ′) = 〈ψ(y), ψ(y ′)〉 for any y, y ′ ∈ Y, which satisfies Asm. 1 with V = I the identity on HY . For the second case, let K defined in terms of K0 as in equation. Let H0 be the RKHS induced by K0 and ψ the associated feature map, then, by definition, the RKHS induced by K will be H = H0 ⊗ H0. Since △ belongs to H, then there exists a v ∈ H such that △(y, y ′) = 〈v,ψ(y) ⊗ ψ(y ′)〉H0⊗H0 . Now note that H = H0 ⊗ H0 is isomorphic to B2(H0,H0), that is the linear space of Hilbert-Schmidt operators fromH0 toH0, thus, there exist an operator V ∈ B2(H0,H0) such that\n△(y, y ′) = 〈v,ψ(y)⊗ψ(y ′)〉H0⊗H0 = 〈ψ(y), Vψ(y ′)〉H0 , ∀y, y ′ ∈ Y.\nFinally note that ψ is continuous and bounded, since it is K0, and V is bounded since it is Hilbert-Schmidt. Thus △ satisfies Asm. 1.\n4 Since△0 satisfies Asm. 1 we have that there exists a kernel on Y0 such that Equation 11 holds. Note that the restriction of a kernel on a subset of its domain is again a kernel. Thus, let ψ = ψ0|Y , we have that △|Y satisfies Equation 11 with ψ and the same bounded operator V as △.\n5 Let△0 : Z×Z → R satisfy Asm. 1, then△0(z, z ′) = 〈ψ0(z), V0ψ0(z ′)〉 for all z, z ′ ∈ Z with ψ0 : Y → H0 bounded and continuous and H0 a separable Hilbert space. Now we define two feature maps ψ1(y) = f(y)ψ0(F(y)) and ψ2(y) = g(y)ψ0(G(y)). Note that both ψ1, ψ2 : Y → H0 are bounded and continuous. We define HY = H0 ⊕H0, ψ : Y → HY as ψ(y) = (ψ1(y), ψ2(y ′)) for any y ∈ Y, and V = ( 0 V0 0 0 ) . Note that\nnow 〈ψ(y), Vψ(y ′)〉HY = (ψ1(y) ψ2(y)) ( 0 V0 0 0 )( ψ1(y)\nψ2(y)\n) = 〈ψ1(y), V0ψ2(y ′)〉HY\n(154)\n= f(y)〈ψ(F(y)), V0ψ(G(y ′))〉HYg(y ′) = f(y)△0 (F(y), G(y ′))g(y ′). (155)\n6 Let Y be compact and △i satisfies Asm. 1 with Hi the associated RKHS, with continuous feature maps ψi : Y → Hi bounded by Ci and with a bounded operator Vi, for i ∈ {1, . . . , p}. Since an analytic function is the limit of a series of polynomials, first of all we prove that a finite polynomial in the losses satisfies Asm. 1, then we take the limit. First of all, note that α△1 +β△2, satisfies Asm. 1, for any α1, α2 ∈ R. Indeed we defineHY = H1⊕H2, and ψ(y) = ( √ |α|‖V1‖ψ1(y), √ |β|‖V2‖ψ2(y)) for\nany y ∈ Y, so that α1△1(y, y ′)+α2△2(y, y ′) = 〈ψ(y), Vψ(y ′)〉HY , with V = ( sign(α1) ‖V1‖ V1 0\n0 sign(α2) ‖V2‖ V2\n) ,\nfor any y, y ′ ∈ Y, where ψ is continuous, supy∈Y ‖ψ‖HY ≤ |α1|‖V1‖C1 + |α2|‖V2‖C2 and ‖V‖ ≤ 1. In a similar way we have that△1△2 satisfies Asm. 1, indeed, we define HY = H1 ⊗H2 and ψ to be ψ(y) = ψ1(y)⊗ψ2(y) for any y ∈ Y, thus\n△1(y, y ′)△2 (y, y ′) = 〈ψ(y), Vψ(y)〉HY , with V = V1 ⊗ V2,\nfor any y, y ′ ∈ Y, where ψ is continuous, supy∈Y ‖ψ‖HY ≤ C1C2 and ‖V‖ ≤ ‖V1‖‖V2‖. Given a polynomial P(△), with △ = (△1, . . . ,△p) we write it as\nP(△) = ∑\nt∈Np αt△t, with △t =\np∏\ni=1\n△tii , ∀t ∈ Np.\nwhere the α’s are the coefficents of the polynomial and such that only a finite number of them are non-zero. By applying the construction of the product on each monomial and of the sum on the resulting monomials, we have that P(△) is a loss satisfying Asm. 1 for a continuous ψ and a V such that\nsup y∈Y ‖ψ‖HY ≤ P̄(‖V1‖C1, . . . , ‖V1‖C1)\nand ‖V‖ ≤ 1, where P̄(△) = ∑t∈Np |αt|△t and HY = ⊕t∈Np ⊗ p i=1 H ti i . Note that HY is again separable. Let now consider\nf(△) = ∑\nt∈Np αt△t, f̄(△) =\n∑ t∈Np |αt|△t .\nAssume that f̄(‖V1‖C1, . . . , ‖Vp‖Cp) < ∞. Then by repeating the construction for the polynomials, we produce a bounded ψ and a bounded V such that\nf(△1(y, y ′), . . . ,△p(y, y ′)) = 〈ψ(y), Vψ(y ′)〉HY , ∀ y, y ′ ∈ Y,\nin particularHY is the same for the polynomial case andψ = ⊕t∈Np √ |αt|vt⊗pi=1ψ ⊗ti i , with vt = ∏p\ni=1 ‖Vi‖ti , for any t ∈ Np. Now we prove that ψ is continuous on Y. Let ψq be the feature map defined for the polynomial P̄q(△) = ∑ t∈Np, ‖t‖≤q |αt|△t. We have that\nsup y∈Y ‖ψ(y) −ψq(y)‖2HY = sup y∈Y\n∥∥∥⊕t∈Np,‖t‖>q √ |αt|vt ⊗pi=1 ψ ⊗ti i ∥∥∥ 2\nHY (156)\n≤ ∑\nt∈Nd,‖t‖>q |αt|vt sup y∈Y ‖ ⊗pi=1 ψ ⊗ti i ‖2HY ≤\n∑\nt∈Nd,‖t‖>q |αt|vtct,\n(157)\nwith ct = ∏p i=1 C ti i for any t ∈ Np. No note that\n∑ t∈Nd |αt|vtct = f̄(‖V1‖C1, . . . , ‖Vp‖Cp) < ∞,\nthus limq→∞ ∑ t∈Nd,‖t‖>q |αt|vtct = 0. Therefore\nlim q→∞ sup y∈Y ‖ψ(y) −ψq(y)‖2HY ≤ limq→∞ ∑\nt∈Np, ‖t‖>q |αt|vtct = 0. (158)\nNow, since ψq is a sequence of continuous bounded functions, and the sequence converges uniformly to ψ, then ψ is continuous bounded. So f(△) is a loss function satisfying Asm. 1, with a continuous ψ and an operator V such that\nsup y∈Y ‖ψ‖HY ≤ f̄(‖V1‖C1, . . . , ‖V1‖C1),\nand ‖V‖ ≤ 1."
    } ],
    "references" : [ {
      "title" : "Real analysis and probability, volume 74",
      "author" : [ "Richard M Dudley" ],
      "venue" : null,
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2002
    }, {
      "title" : "Support Vector Machines. Information Science and Statistics",
      "author" : [ "Ingo Steinwart", "Andreas Christmann" ],
      "venue" : null,
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2008
    }, {
      "title" : "Infinite dimensional analysis: a hitchhiker’s",
      "author" : [ "Charalambos D Aliprantis", "Kim Border" ],
      "venue" : "guide. Springer Science & Business Media,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2006
    }, {
      "title" : "Convex analysis and measurable multifunctions, volume 580",
      "author" : [ "Charles Castaing", "Michel Valadier" ],
      "venue" : null,
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2006
    }, {
      "title" : "Reproducing kernel Hilbert spaces in probability and statistics",
      "author" : [ "Alain Berlinet", "Christine Thomas-Agnan" ],
      "venue" : "Springer Science & Business Media,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2011
    }, {
      "title" : "Kernels for multi–task learning",
      "author" : [ "Charles A Micchelli", "Massimiliano Pontil" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2004
    }, {
      "title" : "Vector valued reproducing kernel hilbert spaces of integrable functions and mercer theorem",
      "author" : [ "Claudio Carmeli", "Ernesto De Vito", "Alessandro Toigo" ],
      "venue" : "Analysis and Applications,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2006
    }, {
      "title" : "Umanitá. Vector valued reproducing kernel hilbert spaces and universality",
      "author" : [ "Claudio Carmeli", "Ernesto De Vito", "Alessandro Toigo", "Veronica" ],
      "venue" : "Analysis and Applications,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2010
    }, {
      "title" : "Regularization of inverse problems, volume 375",
      "author" : [ "Heinz Werner Engl", "Martin Hanke", "Andreas Neubauer" ],
      "venue" : "Springer Science & Business Media,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 1996
    }, {
      "title" : "Optimal rates for the regularized leastsquares algorithm",
      "author" : [ "Andrea Caponnetto", "Ernesto De Vito" ],
      "venue" : "Foundations of Computational Mathematics,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2007
    }, {
      "title" : "Learning theory estimates via integral operators and their approximations",
      "author" : [ "Steve Smale", "Ding-Xuan Zhou" ],
      "venue" : "Constructive approximation,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2007
    }, {
      "title" : "Fourier series and wavelets",
      "author" : [ "J-P Kahane" ],
      "venue" : null,
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 1995
    }, {
      "title" : "On the absolute convergence of multiple fourier series",
      "author" : [ "Ferenc Móricz", "Antal Veres" ],
      "venue" : "Acta Mathematica Hungarica,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2007
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "[1–3].",
      "startOffset" : 0,
      "endOffset" : 5
    }, {
      "referenceID" : 1,
      "context" : "[1–3].",
      "startOffset" : 0,
      "endOffset" : 5
    }, {
      "referenceID" : 2,
      "context" : "[1–3].",
      "startOffset" : 0,
      "endOffset" : 5
    }, {
      "referenceID" : 3,
      "context" : "Indeed, this has motivated the extension of methods such as support vector machines to structured problems [4].",
      "startOffset" : 107,
      "endOffset" : 110
    }, {
      "referenceID" : 4,
      "context" : "Indeed, in the last few years, an effort has been made to analyze specific structured problems, such as multiclass classification [5], multi-labeling [6], ranking [7] or quantile estimation [8].",
      "startOffset" : 130,
      "endOffset" : 133
    }, {
      "referenceID" : 5,
      "context" : "Indeed, in the last few years, an effort has been made to analyze specific structured problems, such as multiclass classification [5], multi-labeling [6], ranking [7] or quantile estimation [8].",
      "startOffset" : 150,
      "endOffset" : 153
    }, {
      "referenceID" : 6,
      "context" : "Indeed, in the last few years, an effort has been made to analyze specific structured problems, such as multiclass classification [5], multi-labeling [6], ranking [7] or quantile estimation [8].",
      "startOffset" : 163,
      "endOffset" : 166
    }, {
      "referenceID" : 7,
      "context" : "Indeed, in the last few years, an effort has been made to analyze specific structured problems, such as multiclass classification [5], multi-labeling [6], ranking [7] or quantile estimation [8].",
      "startOffset" : 190,
      "endOffset" : 193
    }, {
      "referenceID" : 8,
      "context" : "This fact allows to define a (least squares) surrogate loss and cast the problem in a multi-output regularized learning framework [9–11].",
      "startOffset" : 130,
      "endOffset" : 136
    }, {
      "referenceID" : 9,
      "context" : "This fact allows to define a (least squares) surrogate loss and cast the problem in a multi-output regularized learning framework [9–11].",
      "startOffset" : 130,
      "endOffset" : 136
    }, {
      "referenceID" : 10,
      "context" : "This fact allows to define a (least squares) surrogate loss and cast the problem in a multi-output regularized learning framework [9–11].",
      "startOffset" : 130,
      "endOffset" : 136
    }, {
      "referenceID" : 3,
      "context" : "This strategy was extended in [4] for the popular SVMstruct and adopted also in a variety of approaches for structured prediction [1,12,14].",
      "startOffset" : 30,
      "endOffset" : 33
    }, {
      "referenceID" : 0,
      "context" : "This strategy was extended in [4] for the popular SVMstruct and adopted also in a variety of approaches for structured prediction [1,12,14].",
      "startOffset" : 130,
      "endOffset" : 139
    }, {
      "referenceID" : 11,
      "context" : "This strategy was extended in [4] for the popular SVMstruct and adopted also in a variety of approaches for structured prediction [1,12,14].",
      "startOffset" : 130,
      "endOffset" : 139
    }, {
      "referenceID" : 12,
      "context" : "(3) This choice of△ was originally considered in Kernel Dependency Estimation (KDE) [15].",
      "startOffset" : 84,
      "endOffset" : 88
    }, {
      "referenceID" : 11,
      "context" : "1 essentially reduces to [12, 14] and recalling their derivation is insightful.",
      "startOffset" : 25,
      "endOffset" : 33
    }, {
      "referenceID" : 8,
      "context" : "G is the reproducing kernel Hilbert space for vector-valued functions [9] with inner product 〈k(xi, ·)ci, k(xj, ·)cj〉G = k(xi, xj)〈ci, cj〉HY",
      "startOffset" : 70,
      "endOffset" : 73
    }, {
      "referenceID" : 6,
      "context" : "1 we consider ideas from surrogate approaches [7,17,18] and in particular [5].",
      "startOffset" : 46,
      "endOffset" : 55
    }, {
      "referenceID" : 4,
      "context" : "1 we consider ideas from surrogate approaches [7,17,18] and in particular [5].",
      "startOffset" : 74,
      "endOffset" : 77
    }, {
      "referenceID" : 11,
      "context" : "Therefore, the approach in [12, 14] to recover f̂(x) = argminy L(g(x), y) can be interpreted as the result f̂(x) = d◦ ĝ(x) of a suitable decoding of ĝ(x).",
      "startOffset" : 27,
      "endOffset" : 35
    }, {
      "referenceID" : 8,
      "context" : "(13) corresponds to a vector-valued regression problem [9–11].",
      "startOffset" : 55,
      "endOffset" : 61
    }, {
      "referenceID" : 9,
      "context" : "(13) corresponds to a vector-valued regression problem [9–11].",
      "startOffset" : 55,
      "endOffset" : 61
    }, {
      "referenceID" : 10,
      "context" : "(13) corresponds to a vector-valued regression problem [9–11].",
      "startOffset" : 55,
      "endOffset" : 61
    }, {
      "referenceID" : 9,
      "context" : "5, our analysis on ĝ borrows ideas from [10] and extends their result to our setting for the case of HY infinite dimensional (i.",
      "startOffset" : 40,
      "endOffset" : 44
    }, {
      "referenceID" : 9,
      "context" : "Indeed, note that in this case [10] cannot be applied to the estimator ĝ considered in this work (see Appendix Sec.",
      "startOffset" : 31,
      "endOffset" : 35
    }, {
      "referenceID" : 4,
      "context" : "Promising results in this direction can be found in [5], where the Tsybakov condition was generalized to the multi-class setting and led to a tight comparison inequality analogous to the one for the binary setting.",
      "startOffset" : 52,
      "endOffset" : 55
    }, {
      "referenceID" : 4,
      "context" : "Indeed, it is not clear how the approach in [5] could be further generalized to the case where Y has infinite cardinality.",
      "startOffset" : 44,
      "endOffset" : 47
    }, {
      "referenceID" : 6,
      "context" : "Rank Loss Linear [7] 0.",
      "startOffset" : 17,
      "endOffset" : 20
    }, {
      "referenceID" : 3,
      "context" : "012 SVM Struct [4] 0.",
      "startOffset" : 15,
      "endOffset" : 18
    }, {
      "referenceID" : 12,
      "context" : "Loss KDE [15] (Gaussian) Alg.",
      "startOffset" : 9,
      "endOffset" : 13
    }, {
      "referenceID" : 12,
      "context" : "Table 2: Digit reconstruction on USPS with KDE method [15] (with Gaussian loss) and Alg.",
      "startOffset" : 54,
      "endOffset" : 58
    }, {
      "referenceID" : 12,
      "context" : "2 we discussed the relation between KDE [12, 15] and Alg.",
      "startOffset" : 40,
      "endOffset" : 48
    }, {
      "referenceID" : 11,
      "context" : "4 and 5 to prove universal consistency and generalization bounds for methods such as [12,14].",
      "startOffset" : 85,
      "endOffset" : 92
    }, {
      "referenceID" : 11,
      "context" : "We are not aware of previous results proving consistency (and generalization bounds) for the KDE methods in [12, 14].",
      "startOffset" : 108,
      "endOffset" : 116
    }, {
      "referenceID" : 3,
      "context" : "A popular approach to structured prediction is the Support Vector Machine for Structured Outputs (SVMstruct) [4] that extends ideas from the well-known SVM algorithm to the structured setting.",
      "startOffset" : 109,
      "endOffset" : 112
    }, {
      "referenceID" : 2,
      "context" : "Moreover, we note that generalization studies for SVMstruct are available [3] (Ch.",
      "startOffset" : 74,
      "endOffset" : 77
    }, {
      "referenceID" : 6,
      "context" : "1 to the ranking problem using the rank loss [7]",
      "startOffset" : 45,
      "endOffset" : 48
    }, {
      "referenceID" : 6,
      "context" : "In the definition of △rank, γ(y)ij denotes the costs (or reward) of having movie j ranked higher than movie i and, similarly to [7], we set γ(y)ij equal to the difference of ratings provided by user associated to y (from 1 to 5).",
      "startOffset" : 128,
      "endOffset" : 131
    }, {
      "referenceID" : 6,
      "context" : "1, a linear kernel on features similar to those proposed in [7], which were computed based on users’ profession, age, similarity of previous ratings, etc.",
      "startOffset" : 60,
      "endOffset" : 63
    }, {
      "referenceID" : 6,
      "context" : "1 for △rank is NPhard (see [7]) we adopted the Feedback Arc Set approximation (FAS) proposed in [27] to approximate the f̂(x) of Alg.",
      "startOffset" : 27,
      "endOffset" : 30
    }, {
      "referenceID" : 6,
      "context" : "1 (Ours) with surrogate ranking methods using a Linear [7], Hinge [24] or Logistic [25] loss and Struct SVM [4]3.",
      "startOffset" : 55,
      "endOffset" : 58
    }, {
      "referenceID" : 3,
      "context" : "1 (Ours) with surrogate ranking methods using a Linear [7], Hinge [24] or Logistic [25] loss and Struct SVM [4]3.",
      "startOffset" : 108,
      "endOffset" : 111
    }, {
      "referenceID" : 12,
      "context" : "We considered the USPS4 digits reconstruction experiment originally proposed in [15].",
      "startOffset" : 80,
      "endOffset" : 84
    }, {
      "referenceID" : 11,
      "context" : "The standard approach is to use a Gaussian kernel kG on images in input and adopt KDE methods such as [12,14,15] with loss △G(y, y ′) = 1− kG(y, y ′).",
      "startOffset" : 102,
      "endOffset" : 112
    }, {
      "referenceID" : 12,
      "context" : "The standard approach is to use a Gaussian kernel kG on images in input and adopt KDE methods such as [12,14,15] with loss △G(y, y ′) = 1− kG(y, y ′).",
      "startOffset" : 102,
      "endOffset" : 112
    }, {
      "referenceID" : 0,
      "context" : "[1] Pedro F Felzenszwalb, Ross B Girshick, David McAllester, and Deva Ramanan.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 1,
      "context" : "[2] Andrej Karpathy and Li Fei-Fei.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 2,
      "context" : "[3] Thomas Hofmann Bernhard Schölkopf Alexander J.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 3,
      "context" : "[4] Ioannis Tsochantaridis, Thorsten Joachims, Thomas Hofmann, and Yasemin Altun.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 4,
      "context" : "[5] Youssef Mroueh, Tomaso Poggio, Lorenzo Rosasco, and Jean-Jacques Slotine.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 5,
      "context" : "[6] Wei Gao and Zhi-Hua Zhou.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 6,
      "context" : "[7] John C Duchi, Lester W Mackey, and Michael I Jordan.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 7,
      "context" : "[8] Ingo Steinwart, Andreas Christmann, et al.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 8,
      "context" : "[9] Charles A Micchelli and Massimiliano Pontil.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 9,
      "context" : "[10] Andrea Caponnetto and Ernesto De Vito.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 10,
      "context" : "[11] M.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 11,
      "context" : "[14] H.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 12,
      "context" : "[15] Jason Weston, Olivier Chapelle, Vladimir Vapnik, André Elisseeff, and Bernhard Schölkopf.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 0,
      "context" : "We recall [1] that ρ(y|x) is a regular conditional distribution and its domain, which we will denoteDρ|X in the following, is a measurable set contained in the support of ρX and corresponds to the support of ρX up to a set of measure zero.",
      "startOffset" : 10,
      "endOffset" : 13
    }, {
      "referenceID" : 1,
      "context" : "In the next Lemma, following [2] we show that the problem in Eq.",
      "startOffset" : 29,
      "endOffset" : 32
    }, {
      "referenceID" : 2,
      "context" : "153) of [3]), namely continuous in y for each x ∈ X and measurable in x for each y ∈ Y.",
      "startOffset" : 8,
      "endOffset" : 11
    }, {
      "referenceID" : 2,
      "context" : "605) of [3] (or Aumann’s measurable selection principle [2, 4]), we have that m is measurable and that there exists a measurable f∗ : X → Y such that r(x, f∗(x)) = m(x) for all x ∈ X .",
      "startOffset" : 8,
      "endOffset" : 11
    }, {
      "referenceID" : 1,
      "context" : "605) of [3] (or Aumann’s measurable selection principle [2, 4]), we have that m is measurable and that there exists a measurable f∗ : X → Y such that r(x, f∗(x)) = m(x) for all x ∈ X .",
      "startOffset" : 56,
      "endOffset" : 62
    }, {
      "referenceID" : 3,
      "context" : "605) of [3] (or Aumann’s measurable selection principle [2, 4]), we have that m is measurable and that there exists a measurable f∗ : X → Y such that r(x, f∗(x)) = m(x) for all x ∈ X .",
      "startOffset" : 56,
      "endOffset" : 62
    }, {
      "referenceID" : 1,
      "context" : "Therefore, since ρ(y|x) is a regular conditional probability, we have that g∗ is measurable on X (see for instance [2]).",
      "startOffset" : 115,
      "endOffset" : 118
    }, {
      "referenceID" : 2,
      "context" : "416 in [3] the integral of △(d ◦ g(x), y) exists and therefore |E(d ◦ g)| ≤ ∫",
      "startOffset" : 7,
      "endOffset" : 10
    }, {
      "referenceID" : 2,
      "context" : "605) of [3] (or Aumann’s measurable selection principle [2,4]).",
      "startOffset" : 8,
      "endOffset" : 11
    }, {
      "referenceID" : 1,
      "context" : "605) of [3] (or Aumann’s measurable selection principle [2,4]).",
      "startOffset" : 56,
      "endOffset" : 61
    }, {
      "referenceID" : 3,
      "context" : "605) of [3] (or Aumann’s measurable selection principle [2,4]).",
      "startOffset" : 56,
      "endOffset" : 61
    }, {
      "referenceID" : 4,
      "context" : "For more details on RKHS we refer the reader to [5].",
      "startOffset" : 48,
      "endOffset" : 51
    }, {
      "referenceID" : 4,
      "context" : "The continuity of k with the fact that X is Polish implies HX to be separable [5].",
      "startOffset" : 78,
      "endOffset" : 81
    }, {
      "referenceID" : 5,
      "context" : "We refer the reader to [6,7] for a more in-depth introduction on the topic.",
      "startOffset" : 23,
      "endOffset" : 28
    }, {
      "referenceID" : 6,
      "context" : "We refer the reader to [6,7] for a more in-depth introduction on the topic.",
      "startOffset" : 23,
      "endOffset" : 28
    }, {
      "referenceID" : 7,
      "context" : "Indeed, it was proven in [8] (see Example 14) that if k is a universal scalar kernel, then Γ(·, ·) = k(·, ·)IHY is universal.",
      "startOffset" : 25,
      "endOffset" : 28
    }, {
      "referenceID" : 8,
      "context" : "We can conclude that R attains a minimum on G if and only if the range of S∗Z is contained in the range of C, namely Ran(S∗Z) ⊆ Ran(C) ⊂ HX (see [9] Chap.",
      "startOffset" : 145,
      "endOffset" : 148
    }, {
      "referenceID" : 9,
      "context" : "In this setting, the problem of finding a probabilistic bound has been studied (see [10] and references therein).",
      "startOffset" : 84,
      "endOffset" : 88
    }, {
      "referenceID" : 9,
      "context" : "of a decomposition of the surrogate excess risk that is similar to the one in [10].",
      "startOffset" : 78,
      "endOffset" : 82
    }, {
      "referenceID" : 9,
      "context" : "However, note that we cannot direct apply [10] to our setting, since in [10] the operator-valued kernel Γ associated to G is required to be such that Γ(x, x ′) is trace class ∀x, x ′ ∈ X , which does not hold for the kernel used in this work, namely Γ(x, x ′) = k(x, x )IHY when HY is infinite dimensional.",
      "startOffset" : 42,
      "endOffset" : 46
    }, {
      "referenceID" : 9,
      "context" : "However, note that we cannot direct apply [10] to our setting, since in [10] the operator-valued kernel Γ associated to G is required to be such that Γ(x, x ′) is trace class ∀x, x ′ ∈ X , which does not hold for the kernel used in this work, namely Γ(x, x ′) = k(x, x )IHY when HY is infinite dimensional.",
      "startOffset" : 72,
      "endOffset" : 76
    }, {
      "referenceID" : 9,
      "context" : "According to [10], we define the following quantity A(λ) = λ‖Z∗(L+ λ)‖HS.",
      "startOffset" : 13,
      "endOffset" : 17
    }, {
      "referenceID" : 9,
      "context" : "X Tr ( Ĝ ( φ(x)⊗φ(x) ) Ĝ∗ ) − 2Tr ( Ĝ ( φ(x)⊗ g∗(x) )) + Tr(g∗(x)⊗ g(x))dρX (x) (98) = Tr(ĜS∗SĜ) − 2Tr(ĜS∗Z) + Tr(Z∗Z) = ‖ĜS∗ − Z‖HS (99) To bound ‖ĜS∗ − Z∗‖HS, we proceed with a decomposition similar to the one in [10].",
      "startOffset" : 215,
      "endOffset" : 219
    }, {
      "referenceID" : 10,
      "context" : "Thus, by applying Lemma 2 of [11], we have",
      "startOffset" : 29,
      "endOffset" : 33
    }, {
      "referenceID" : 10,
      "context" : "Thus we can again apply Lemma 2 of [11], obtaining",
      "startOffset" : 35,
      "endOffset" : 39
    }, {
      "referenceID" : 1,
      "context" : "(126) This is a standard assumption for universal consistency (see [2]).",
      "startOffset" : 67,
      "endOffset" : 70
    }, {
      "referenceID" : 0,
      "context" : "pag 263 of [1]) on the sequence (En)n∈N and conclude that the statement lim n→∞ E(f̂n) − E(f∗) > 0, (134) holds with probability 0.",
      "startOffset" : 11,
      "endOffset" : 14
    }, {
      "referenceID" : 0,
      "context" : "Y = [0, 1] with d ∈ N, and the mixed partial derivative L(y, y ′) = ∂ △(y1,.",
      "startOffset" : 4,
      "endOffset" : 10
    }, {
      "referenceID" : 0,
      "context" : "A discrete probability distribution (or a normalized histogram) over M entries can be represented as a y = (yi)i=1 ∈ Y ⊂ [0, 1] the M-simplex, namely ∑Mi=1 yi = 1 and yi ≥ 0 ∀i = 1, .",
      "startOffset" : 121,
      "endOffset" : 127
    }, {
      "referenceID" : 0,
      "context" : "△H is obtained by M summations of △0 on [0, 1], by Thm.",
      "startOffset" : 40,
      "endOffset" : 46
    }, {
      "referenceID" : 0,
      "context" : "Indeed the function (r − r ′)2/(r + r ′) satisfies point 2 on Y = [0, 1], then point 6 and 4 are applied.",
      "startOffset" : 66,
      "endOffset" : 72
    }, {
      "referenceID" : 11,
      "context" : "For the continuity, see [14] (pag.",
      "startOffset" : 24,
      "endOffset" : 28
    }, {
      "referenceID" : 12,
      "context" : "291 of [15]).",
      "startOffset" : 7,
      "endOffset" : 11
    } ],
    "year" : 2017,
    "abstractText" : "We propose and analyze a regularization approach for structured prediction problems. We characterize a large class of loss functions that allows to naturally embed structured outputs in a linear space. We exploit this fact to design learning algorithms using a surrogate loss approach and regularization techniques. We prove universal consistency and finite sample bounds characterizing the generalization properties of the proposed methods. Experimental results are provided to demonstrate the practical usefulness of the proposed approach.",
    "creator" : "LaTeX with hyperref package"
  }
}