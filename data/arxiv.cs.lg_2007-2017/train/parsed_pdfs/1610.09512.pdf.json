{
  "name" : "1610.09512.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Contextual Decision Processes with Low Bellman Rank are PAC-Learnable",
    "authors" : [ "Nan Jiang", "Akshay Krishnamurthy", "Alekh Agarwal", "John Langford", "Robert E. Schapire" ],
    "emails" : [ "nanjiang@umich.edu", "akshay@cs.umass.edu", "alekha@microsoft.com", "jcl@microsoft.com", "schapire@microsoft.com" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "How can we tractably solve sequential decision making problems where the agent receives rich observations? This question is at the core of both theoretical and empirical reinforcement learning research. There is a rich body of theoretical literature on learning Markov Decision Processes (MDPs) with a small state space [Kearns and Singh, 2002, Brafman and Tennenholtz, 2003, Strehl et al., 2006], with an emphasis on sophisticated exploration techniques that find near-optimal policies in a sample-efficient manner. While there have been attempts to extend these techniques to large state spaces [Kakade et al., 2003, Jong and Stone, 2007, Pazis and Parr, 2016], these approaches fail to be a good fit for practical scenarios where the environment is typically perceived through complex sensory observations such as image, text or audio signals. Alternatively, Monte Carlo Tree Search (MCTS) methods can handle arbitrarily large state spaces, but the advantage comes at the cost of exponential dependence on the planning horizon [Kearns et al., 2002, Kocsis and Szepesvári, 2006].\nOn the empirical side, the prominent recent success on both the Atari platform [Mnih et al., 2015, Wang et al., 2015] and Go [Silver et al., 2016] have sparked a flurry of research interest. These approaches leverage advances in deep learning for powerful function approximation, while, in most cases, using simple heuristic strategies, such as -greedy, for exploration. More advanced exploration strategies include extending the methods for small state spaces to real-world settings (e.g., the use of pseudo-counts in Bellemare et al. [2016]), and combining MCTS with function approximation (e.g., Silver et al. [2016]). Both types of approaches often require strong domain knowledge and large amounts of data to be successful.\nIn this work, we study reinforcement learning settings where the agent receives rich sensory observations from the environment, forms complex contexts from these sensorimotor streams for learning and decisionmaking, and uses function approximation to generalize to unseen contexts. Our work departs from existing efforts by aiming at a sample complexity that depends neither on the number of unique contexts nor exponentially on the horizon. Similar goals have been attempted by Wen and Van Roy [2013] and Krishnamurthy et al. [2016] where attention is restricted to decision processes with special structures. In contrast, we consider a much broader class of problems with relatively mild conditions.\nConcretely, we study a large class of sequential decision-making problems which we call Contextual Decision Processes (CDPs). A CDP involves an agent repeatedly observing some rich contextual information,\nar X\niv :1\n61 0.\n09 51\n2v 1\n[ cs\n.L G\n] 2\n9 O\nct 2\n01 6\nusing this information to take an action, and, as a consequence, eliciting a reward and a new context. The agent’s goal is to learn a policy mapping contexts to actions, such that its long-term reward is maximized. This setting clearly generalizes MDPs, where the state forms the context (Example 1), and POMDPs, where the history forms the context (Example 2). Our goal is to design a learning algorithm that identifies a near-optimal policy in a CDP in a sample-efficient manner.\nIn many cases of interest, the context space is likely to be highly complex, and the agent uses a valuefunction approximator to generalize from a small fraction of presented contexts to unseen ones. Unfortunately, a compact function approximator alone does not guarantee sample-efficient learning of CDPs, as we show in an exponential lower bound (Proposition 1). We circumvent this hardness by imposing a structural assumption that holds in a wide range of scenarios. As our first major contribution, we define a notion of Bellman Factorization (Definition 5), and focus on problems with low Bellman Rank .\nAt a high level, Bellman Rank is a form of algebraic dimension on the interplay between the CDP and the value-function approximator that we show is naturally small for many well-studied models. For example, every MDP with a tabular value-function has Bellman Rank bounded by the number of states. If the number of states is large, but the transition matrix has a low-rank structure, then the Bellman Rank is at most the rank of the transition matrix. For a POMDP with reactive value-functions, the Bellman Rank is at most the number of hidden states and has no dependence on the observation space, allowing environments with rich sensory observations. Moreover, the notion continues to be useful beyond these models, and leads to non-trivial results for well-studied models, such as Linear Quadratic Regulators from control theory and Predictive State Representations. Overall, studying CDPs with a small Bellman Rank provides a unified framework to handle a large class of sequential decision making problems.\nFollowing the introduction of Bellman Rank, our next contribution is a novel algorithm for episodic reinforcement learning, which finds a near-optimal value-function, and consequently policy, in a sample efficient manner, whenever the problem has low Bellman Rank. Concretely, when the optimal valuefunction in a CDP can be represented by the function approximator used by the learner, the algorithm uses Õ(M2H3K log(N/δ)/ 2) trajectories to find an -suboptimal policy, where M is the Bellman Rank, H is the length of an episode, K is the number of actions available at each decision point, N is the size of the value-function class in consideration, and δ is the failure probability. Importantly, the sample complexity bound has a logarithmic dependence on the number of value functions, thus enabling powerful function approximation, and no dependence on the size of the context space, which can be very large or even infinite. As many existing models, including the ones mentioned above, have low Bellman Rank, our result immediately implies sample-efficient learning in all of these settings, as highlighted in Table 1.\nWe also present several extensions of the main result, showing robustness to the failure of our assumptions, including the assumption that the optimal value-function is captured by our function approximator, adaptivity to unknown Bellman Rank and extension to infinite function classes of bounded statistical complexity. Altogether, our results show that the notion of Bellman Rank robustly captures the difficulty of exploration in sequential decision-making problems.\nTo summarize, this work constitutes significant progress in understanding reinforcement learning with complex observations where long-term planning and exploration are critical. There are of course several additional questions that must be resolved before we have satisfactory tools for these problems. The biggest caveat of our algorithm is its computational complexity, which is polynomial in the number of value functions hence intractable for the powerful classes of interest. This issue must be resolved before we can empirically\nevaluate the effectiveness of our principled exploration technique. We leave this, and further open questions, for future work."
    }, {
      "heading" : "2 Contextual Decision Processes (CDPs)",
      "text" : "We introduce a new model, called a Contextual Decision Process, as a unified framework for reinforcement learning with rich observations. We first present the model, before the relevant notation and definitions."
    }, {
      "heading" : "2.1 Model and Examples",
      "text" : "Traditionally, Markov Decision Processes (MDPs) have been used as a standard model of the environment in RL, but the Markovian assumption is often unrealistic in real-world problems. While models such as Partially Observable MDPs (POMDPs) allow for non-Markovian environments, the emphasis is typically on the finite and small observation spaces or on the latent structures which allow the partial observability to be resolved, making the techniques unsuitable in several practical applications. Contextual Decision Processes use minimal assumptions to capture a very general class of RL problems.\nDefinition 1 (Contextual Decision Process (CDP)). A (finite-horizon) Contextual Decision Process (CDP for short) is defined as an 〈X ,A, H, P 〉 tuple, where X is the context space, A is the action space, and H is the horizon of the problem. P = (P∅, P+) is the system descriptor, where P∅ ∈ ∆(X ) is a distribution over initial contexts, that is x1 ∼ P∅, and P+ : (X ×A× R)∗ × X ×A → ∆(R× X ) elicits the next reward and context from the interactions so far x1, a1, r1, . . . , xh, ah:\n(rh, xh+1) ∼ P+(x1, a1, r1, . . . , xh, ah).\nIn a CDP, the agent’s interaction with the environment proceeds in episodes. In each episode, the agent observes a context x1, take action a1, receives reward r1 and observes x2, repeating H times. A policy π : X → A specifies the decision-making strategy of an agent, that is ah = π(xh) ∀h ∈ [H], and induces a distribution over the trajectory (x1, a1, r1, . . . , xH , aH , rH , xH+1) according to the system descriptor P . 1 The value of a policy, V π, is defined as\nV π = EP [∑H h=1 rH ∣∣∣ a1:H ∼ π] , (1) where a1:H ∼ π abbreviates for a1 = π(x1), . . . , aH = π(xH). Here, and in the sequel, the expectation is always taken over contexts and rewards drawn according to the system descriptor P , so that we will suppress the subscript P for brevity. The goal of the agent is to find a policy π that attains the largest value.\nTo get some intuition about CDPs, we will typically think of the context space X as some finitedimensional vector space (such as a collection of sensor measurements or features), and A as a small and discrete set. The model, however, is faily general and inclusive and we next show a few examples on how CDPs capture classical RL models. We start with MDPs. Note that in finite-horizon MDPs, optimal policies can be non-stationary in general, i.e., they depend on the number of remaining time steps, so we explicitly provide the number of time steps as part of the context.\nExample 1 (MDPs with states as contexts). Consider a finite-horizon MDP 〈S,A, H,Γ1,Γ, R〉, where S is the state space, A is the action space, H is the horizon, Γ1 ∈ ∆(S) is the initial state distribution, Γ : S × A → ∆(S) is the state transition function, R : S × A → ∆([0, 1]) is the reward function, and an episode takes the form of (s1, a1, r1, . . . , sH , aH , rH). If the agent chooses from the space of all non-stationary policies {π : S×[H]→ A}, we can convert the MDP to a CDP 〈X ,A, H, P 〉 by letting X = S×[H] and xh = (sh, h). The system descriptor is P = (P∅, P+), where P∅ = Γ1, and P+(rh, xh+1 |x1, a1, r1, . . . , xh, ah) = R(rh|sh, ah) Γ(sh+1|sh, ah).\n1More generally, non-stationary and stochastic policies induce trajectory distributions in a similar way. That is, π1, . . . , πH : X → ∆(A) also specifies a distribution over the trajectory where ah ∼ πh(xh) ∀h ∈ [H].\nThe system descriptor for a particular model is usually obvious from definition, and here we give its detailed form as an illustration. We omit the specification of system descriptor in the remaining examples.\nNext we turn to POMDPs. We might naively suspect that a CDP describes a similar process as a POMDP but limits the agent’s decision-making strategies to memoryless (or reactive) policies. This is not true. We clarify this issue by showing that we can use the history as context, and the induced CDP suffers no loss in the ability to represent optimal policies.\nExample 2 (POMDPs with histories as contexts). Consider a finite-horizon POMDP, which can be specified by an underlying MDP with state space S (i.e., the hidden-state space), an observation space O, and an emission processDs that associates each s ∈ S with a distribution overO. Suppose (s1, a1, r1, . . . , sH , aH , rH) is an episode in the underlying MDP, then the POMDP episode takes the form of (o1, a1, r1, . . . , oH , aH , rH), where oh ∼ Dsh and sh itself is unobserved. If the agent chooses from the space of all policies that depend on history, we can convert the POMDP to a CDP 〈X ,A, H, P 〉 by letting X = (O × A × R)∗ × O and xh = (o1, a1, r1, . . . , oh). Moreover, xh is Markovian and the CDP can be viewed as an MDP.\nIt should also be clear from this example that we can assume that contexts are Markovian in CDPs without loss of generality (because we can always use history as context). While we do not commit to this assumption to allow for a flexible framework with simple notations (see Example 3), we will connect to well-known results in MDP literature based on this observation so that readers can transfer their insights on MDPs to CDPs.\nExample 3 (POMDPs with sliding windows of observations as contexts). In some application scenarios, partial observability can be resolved by using a small sliding window: for example, in Atari games, it is common to keep track of the last 4 frames of images [Mnih et al., 2015]. In this case, we can represent the problem as a CDP by letting xh = (oh−3, oh−2, oh−1, oh).\nFinally, we introduce some common regularity assumptions on the rewards.\nAssumption 1 (Boundedness of rewards). We assume that regardless of how actions are chosen, for any h = 1, . . . ,H, rh ≥ 0 and ∑H h=1 rh ≤ 1 almost surely.\nNow that we have a model in place, we will next discuss a few important solution concepts."
    }, {
      "heading" : "2.2 Value-based RL and Function Approximation",
      "text" : "Note that a CDP makes no assumption on the cardinality of the context space, and we will typically consider continuous context sets. This makes it critical to generalize across contexts, since the agent might not encounter the same context twice. With this motivation, we consider value-based RL with function approximation. That is, the agent is given a space of functions F ⊆ X×A → [0, 1] and uses it to approximate an action-value function (or Q-value function). Without loss of generality we assume that f(xH+1, a) ≡ 0.2 For the purpose of presentation, we assume that F is a finite space with |F| = N <∞ for most part of the paper. In Section 5.3 we relax this assumption and allow infinite function classes with bounded capacity.\nOptimal value functions in MDPs are typically specified through the Bellman equations that they satisfy. Such value-functions also induce an optimal policy, one that maps every state to the action with the highest long-term reward. A direct extension of these notions to CDPs would entail that the agent should learn to map every single context to the optimal action, and that the corresponding value-function be contained in the function class F . As we show below, however, this assumption can be substantially relaxed to better account for the learner only having access to a restricted function class F .\nAs in typical value-based RL, our goal is also to identify a f ∈ F which respects a particular set of Bellman equations, and achieves a high value with its greedy policy πf (x) = argmaxa∈A f(x, a). To more precisely describe this goal, we introduce our variant of Bellman equations and the optimal value V ?F through a series of definitions.\n2This frees us from having to treat the last level (h = H) differently in the Bellman equations.\nDefinition 2 (Average Bellman error). Given any policy π : X → A and a function f : X ×A → [0, 1], the average Bellman error of f under roll-in policy π at level h is defined as\nE(f, π, h) = E [ f(xh, ah)− rh − f(xh+1, ah+1) ∣∣ a1:h−1 ∼ π, ah:h+1 ∼ πf ]. (2) In words, the average Bellman error measures the self-consistency of a function f between its predictions at levels h and h + 1 when all the previous actions are taken according to some policy π. 3 Given this definition, we now define a set of Bellman equations.\nDefinition 3 (Bellman equations and validity of f). Given an (f, π, h) triple, a Bellman equation posits E(f, π, h) = 0. We say f ∈ F is valid if the Bellman equation on (f, πf ′ , h) holds for every f ′ ∈ F , h ∈ [H].\nIn the MDP setting, each of our Bellman equation can be viewed as the linear combination of the standard Bellman optimality equations for Q?, 4 with the coefficients in this combination being the probability with which the roll-in policy π visits each state. This leads to the following consequence.\nFact 1 (Q? is always valid). Given an MDP and a space of functions F : S× [H]×A → [0, 1], if the optimal Q-value function of the MDP, Q?, lies in F , then in the corresponding CDP with X = S × [H], Q? is valid.\nWhile Q? satisfies our Bellman equations and yields the optimal policy π? = πQ? , there can be many more other functions which also satisfy the equations but may yield suboptimal policies. This happens because Eq. 2 only considers ah, ah+1 drawn according to πf and does not use the values on other actions. For instance, consider a CDP where at every context, action a always gets a reward of 0 and action a′ always gets a reward of 1. A function that predicts f(x, a) = f(x, a′) = 0 ∀x, a is trivially valid as long as tie-breaks always favor a.\nSince validity alone does not imply that we get a good policy, it is natural to search for a valid value function which also induces a high-value policy. We formalize this goal in the next definition.\nDefinition 4 (Optimal value). Define f? = argmaxf∈F : f is valid V πf , and V ?F = V πf? .\nFact 2. For the same setting as in Fact 1, when Q? ∈ F , we have f? = Q?, and V ?F = V ?, which is the optimal long-term value.\nThis final definition implicitly assumes that there is at least one valid f ∈ F . This is weaker than the realizability assumption made in the value-based RL literature, that F contains Q?, the optimal Q-value function of an MDP. Indeed, our setup subsumes realizability, as evidenced by Fact 1 and 2. When Q? ∈ F , our algorithm aims to identify a policy achieving value close to V ?, the optimal value achievable by any agent. When no functions in F approximate Q? well, our algorithm still delivers non-trivial guarantees, in that it finds the best valid value function. In this sense, our setting requires substantially weaker realizability-type assumptions than related theoretical results for value-based RL [Antos et al., 2008, Krishnamurthy et al., 2016], which assume Q? ∈ F often in addition to several stronger requirements.\nApproximation to Bellman Equations. In value-based RL, since we use a restricted class of value functions, we may not have Q? ∈ F or even worse, we may not have any valid functions in F . In our setup, this corresponds to V ?F = −∞ (no valid functions) or V ?F V ?, in which case competing with V ?F may be vacuous. Instead, a practical algorithm should be robust to this situation, and produce a good policy, even when no value functions are strictly valid. In Section 5.4, we show that if F contains good functions that approximately satisfy the Bellman equations (Definition 8), then our algorithm can compete with the value achieved by these functions, which can be much higher than V ?F as defined above.\n3In many existing approaches (e.g., LSPI [Lagoudakis and Parr, 2003] and FQI [Ernst et al., 2005]), the Bellman errors are defined as taking the expectation of a squared error unlike in our definition.\n4Readers who are not familiar with the definition of Q? are advised to consult a textbook, such as [Sutton and Barto, 1998]."
    }, {
      "heading" : "3 Bellman Factorization and Bellman Rank",
      "text" : "Hopefully, the development so far has convinced the reader that CDPs are fairly general models for RL. While this is good in that any algorithms for CDPs apply to all the subsumed special cases, this generality also poses substantial challenges for sample-efficient learning. Indeed, learning in CDPs with large or infinite context spaces is generally hard, since they subsume MDPs and POMDPs with arbitrarily large state/observation spaces. Intuitively, one might expect that a simple function class F with low statistical capacity can generalize over the large space thereby enabling efficient exploration.\nIn the following lower bound, we show that this intuition does not hold: while simple function classes do generalize effectively in supervised learning where data come from a fixed distribution,5 in RL the data consists of trajectories whose distribution crucially depends on the agent’s policy. Consequently, low statistical capacity is not sufficient to guarantee generalization in RL problems where exploration is critical. In particular, even when logN , the statistical capacity for finite classes, is small, there exists an exponential (in horizon) lower bound for the sample complexity of exploration. The result is due to Krishnamurthy et al. [2016], and we restate it here for completeness.\nProposition 1 (Restatement of Proposition 2 in Krishnamurthy et al. [2016]). For any H,K ∈ N with K ≥ 2, and any ∈ (0, √ 1/8), there exists a family of finite-horizon MDPs with horizon H and |A| = K, such that we can construct a function space F with |F| = KH to guarantee that Q? ∈ F for all MDP instances in the family, yet there exists a universal constant c such that for any algorithm and any T ≤ cKH/ 2, the probability that the algorithm outputs a policy π̂ with V π̂ ≥ V ? − after collecting T trajectories is at most 2/3 when the problem instance is chosen from the family by an adversary.\nProof sketch for completeness. The proof relies on the fact that CDPs include MDPs where the state space is arbitrarily large. Each instance of the MDP family is a complete tree with branching factor K and depth H. Transition dynamics are deterministic, and only leaf nodes have non-zero rewards. All leaves give Ber(1/2) rewards, except for one that gives Ber(1/2+ ). Changing the position of the most rewarding leaf node yields a family of KH MDP instances, and collecting their optimal Q-value functions forms the desired function class F . Since F provides no information other than the fact that the true MDP lies in this family, the problem is equivalent to identifying the best arm in a multi-arm bandit with KH arms, and the remaining analysis follows exactly the same as in Krishnamurthy et al. [2016].\nThe existence of such a lower bound implies that we need to impose some further structure on the set of all CDPs before sample-efficient learning is possible. We do so by proposing a new complexity measure that characterizes the difficulty of exploration in RL, so that we can exclude the hard instances by restricting attention to settings where the measure is low, and develop algorithms that have sample complexity polynomial in the measure.\nOur complexity measure is a structural characterization of the set of Bellman equations induced by the CDP and value-function class (recall Definition 2), which we need to search through to find valid functions. These equations are implicitly structured in many existing sample-efficient reinforcement learning results; for example, in tabular MDPs, the Bellman error of f on every state is sufficient to judge the validity of f , since the error under any roll-in policy is just a stochastic combination of these single-state errors. On the other hand, without such structure, the naive approach of checking the validity of all functions might necessitate the collection of samples according to πf ′ for every f\n′ ∈ F , which is intractable for large F . Together, these observations hint toward a more a general phenomenon: whenever the collection of Bellman errors across all roll-in policies can be concisely represented, we may be able to check the validity of all functions in a sample-efficient manner.\nThis motivates the definition of a new complexity measure for CDPs which we call the Bellman Rank . Consider the Bellman error matrices, that is, |F| × |F| matrices with the (f, f ′)th entry being the Bellman error E(f, πf ′ , h), one for every level h. Informally, we can think of the Bellman Rank for a CDP and a given value-function class F to be a uniform upper bound on the rank of all the H Bellman error matrices.\nBased on our earlier discussion, it is immediate that the Bellman Rank defined this way is controlled by the number of states in a tabular MDP setting (the formal statement will be given in Proposition 2). More\n5For a finite hypothesis class, its generalization error in supervised learning is controlled by the log of its cardinality.\ngenerally, whenever there are some compact statistics that can summarize the influence of the roll-in policy π for the purpose of calculating the Bellman error of any f , the Bellman Rank of the resulting problem is small. For tabular MDPs, such a statistic is the state at level h. Moreover, the observability of the statistic is not required; in POMDPs for example, the hidden state can also serve as such a statistic (Proposition 4).\nNow we give the formal definition below.\nDefinition 5 (Bellman Factorization and Bellman Rank). We say that a CDP 〈X ,A, H, P 〉 and F ⊂ X × A → [0, 1] admit Bellman Factorization with Bellman Rank M and norm parameter ζ, if there exists νh : F → RM , ξh : F → RM for each h ∈ [H], such that for any f, f ′ ∈ F , h ∈ [H],\nE(f, πf ′ , h) = 〈νh(f ′), ξh(f)〉, (3)\nand ‖νh(f ′)‖2 · ‖ξh(f)‖2 ≤ ζ <∞.\nThe exact factorization in Eq. (3) can be relaxed to an approximate version as will be discussed in Section 5.4. In the remaining sections of this paper we introduce the main algorithm, and prove its sampleefficiency in problems with low Bellman Rank. Before that, we use the rest of this section to showcase the generality of Definition 5 by presenting a number of common RL settings that have a small Bellman Rank. Throughout, we will see how the Bellman Rank captures the process-specific structures that allow for efficient exploration.\nWe start with the tabular MDP setting, and show that the Bellman Rank is at most the number of states.\nProposition 2 (Bellman Rank bounded by number of states in MDPs). Consider an MDP introduced in Example 1 with |S| < ∞, and let 〈X ,A, H, P 〉 be the corresponding CDP with X = S × [H]. With any F ⊂ X ×A → [0, 1], this model admits a Bellman Factorization with M = |S| and ζ = 2 √ M .\nThis result is proved in Appendix B.1. We also note that there is a previous effort in extending tabular PAC-MDP methods to large MDPs using a form of state abstractions [Li, 2009, Section 8.2.3], which we are also able to subsume. See Appendix B.2 for details.\nNext we consider large MDPs whose transition dynamics have a low-rank structure. A closely related setting has been considered by Barreto et al. [2011, 2014] where the low-rank structure is exploited to speed up MDP planning. For our purpose, such a structure yields low Bellman Rank hence enables efficient exploration, and this result is novel as far as we know. To align with existing literature we assume that S is finite (but can be arbitrarily large); extension to the infinite case is straight-forward.\nProposition 3 (Bellman Rank bounded by rank of transition matrices in MDPs). Consider an MDP introduced in Example 1. With a slight abuse of notation let Γ denote its transition matrix of size |S×A|×|S|, whose element indexed by ((s, a), s′) is Γ(s′|s, a). Assume that there are two row-stochastic matrices Γ(1) and Γ(2) with sizes |S × A| ×M and M × |S| respectively, such that\nΓ = Γ(1)Γ(2). (4)\nRecall that we convert an MDP into a CDP by letting X = S×[H], xh = (sh, h). For any F ⊂ X×A → [0, 1], this model admits a Bellman Factorization with Bellman Rank M and ζ = 2 √ M .\nOur next example considers POMDPs with large observations spaces and reactive value functions, where the Bellman Rank is at most the number of hidden states. This result subsumes and generalizes the setting of Krishnamurthy et al. [2016] which requires deterministic transitions in the underlying MDP. In contrast, we eliminate the need for the deterministicity assumption, and still guarantee sample-efficient learning.\nProposition 4 (Bellman Rank bounded by hidden states in reactive POMDPs). Consider a POMDP introduced in Example 3 with |S| < ∞ and a sliding window of size 1. Let 〈X ,A, H, P 〉 be an induced CDP with X = O × [H] and xh = (oh, h). Given any F ⊂ X × A → [0, 1], this model admits a Bellman Factorization with M = |S| and ζ = 2 √ M .\nPerhaps surprisingly, Proposition 3 and 4 can be proved under a unified model that generalizes POMDPs by allowing the transition function and the reward function to depend on the observation. See Figure 1 (a)\n– (c) for graphical representations of these models and Appendix B.3 for the proof of both Propositions 4 and 3. This unified model captures the experimental settings considered in state-of-the-art empirical RL work (Figure 1 (d)), where agents act in a grid-world (|S| is small) and receives complex and rich observations such as raw pixel images (|O| is large).\nNext, we consider Predictive State Representations (PSRs), which are alternative models of partially observable systems with parameters grounded in observable quantities [Littman et al., 2001]. Similar to the case of POMDPs, we can bound the Bellman Rank in terms of the rank of the PSR6 when the candidate value functions are reactive. For presentation purposes, we only give an informal result here, and defer the definitions, properties of PSRs, formal statement, and its proof to Appendix B.4\nProposition 5 (Informal result on bounding Bellman Rank in PSRs). Consider a partially observable system with observation space O, and the induced CDP 〈X ,A, H, P 〉 with xh = (oh, h). If the linear dimension of the system (i.e., rank of its PSR model) is at most L, then given any F : X ×A → [0, 1], the Bellman Rank is bounded by LK.\nOur last example considers a class of linear control problems well studied in control theory, called Linear Quadratic Regulators (LQRs). In LQRs, we show that the Bellman Rank is bounded by the dimension of the state space. Exploration in this class of problems has been previously considered by Osband and Van Roy [2014]. Again for presentation purposes, we only state an informal result here and defer the formal statement to Appendix B.5. Note that the algorithm to be introduced in the next section does not directly apply to LQRs due to the continuous action space, and adaptations that exploit the structure of the action space may be needed, which we leave for future work.\nProposition 6 (Informal result on bounding Bellman Rank in LQRs). An LQR can be viewed as an MDP with continuous state space Rd and action space RK , where the dynamics are described by some linear equations. Given any function class F consisting of non-stationary quadratic functions of the state, the Bellman Rank is bounded by d2 + 1."
    }, {
      "heading" : "4 Algorithm and Main Results",
      "text" : "In this section we present our algorithm for learning CDPs which have Bellman Factorization with a small Bellman Rank and our main sample complexity guarantee. To aid presentation and help convey the main ideas, we make three simplifying assumptions:\n1. We assume the Bellman Rank parameter M corresponding to the Bellman Factorization is known to the agent.7\n2. We assume the function class F is finite with |F| = N .\n3. We assume exact validity (Definition 3) and exact Bellman Factorization (Definition 5).\nAll three assumptions can be relaxed, and we sketch these relaxations in Section 5. We are interested in designing an algorithm for PAC Learning CDPs. We say that an algorithm PAC learns if given F , two parameters , δ ∈ (0, 1), and access to a CDP, the algorithm outputs a policy π̂ with V π̂ ≥ V ?F − with probability at least 1 − δ. The sample complexity is the number of episodes needed to achieve such a guarantee, and is typically expressed in terms of , δ and other relevant parameters. Our goal is to design an algorithm with sample complexity that is Poly(M,K,H, , log(N), log(1/δ)) where M is the Bellman Rank, K is the number of actions, and H is the time horizon. We say that such a sample complexity bound is polynomial in all relevant parameters. Importantly, the bound should have no dependence on the number of unique contexts |X |.\n6Every POMDP has an equivalent PSR whose rank is bounded by the number of hidden states [Singh et al., 2004]. 7We will also assume knowledge of the corresponding norm parameter, but this is relatively minor."
    }, {
      "heading" : "4.1 Algorithm",
      "text" : "Pseudocode for our algorithm, which we call Olive (Optimism Led Iterative Value-Function Elimination), is displayed in Algorithm 1. Theorem 1 describes how to set the parameters nest, neval, n, and φ.\nAt a high level, the algorithm aims to eliminate functions f ∈ F that fail to satisfy the validity condition in Definition 3. This is done by Eq. (6) and (7) inside the loop of the algorithm. Observe that, since the actions aht are chosen uniformly at random, Eq. (6) produces an unbiased estimate of E(f, πt, ht), the average Bellman error for function f on roll-in policy πt at time ht. Thus, Eq. (7) eliminates functions that have high average Bellman error on this distribution, which means they fail to satisfy the validity criteria.\nThe other major component of the algorithm involves choosing the roll-in policy and level on which to do the learning step. At round t, we choose the roll-in policy πt optimistically, by choosing ft that predicts the highest value at starting context distribution, and letting πt = πft . To pick the level, we compute ft’s average Bellman error on its own roll-in distribution (Eq. (5)), and set ht to be any level for which this average Bellman error is high (See Line 11). As we will see, these choices ensure that substantial learning happens on each iteration, guaranteeing that the algorithm progresses rapidly and uses polynomially many episodes.\nThe last component is the termination criterion. The algorithm terminates if ft has small average Bellman error on its own roll-in distribution at all levels. As we will see in our analysis, this criteria guarantees that πt is near optimal.\nComputationally, the algorithm requires enumeration of the value-function class, which we expect to be extremely large or infinite in practice. While computational efficiency is essential for a practical algorithm, our focus here is on statistical issues, so we do not dwell further on this deficiency.\nIntuition for OLIVE. To convey intuition, it is helpful to ignore any sampling effects, which means replacing all empirical estimates with their population values. The first important fact is that the algorithm never eliminates a valid function, since the learning step in Eq. (7) only eliminates a function f , if we can find a distribution on which it has large average Bellman error. If f is valid, then E(f, π, h) = 0 for all π, h, so f will never be eliminated.\nThe second fact is that if a function f is valid, then its predicted value is exactly the value achieved by the greedy policy πf , that is Vf = E[f(x1, πf (x1))] = V πf . This follows by telescoping the recursion in the definition of average Bellman error. Therefore, since ft is chosen optimistically as the maximizer of the value prediction among the surviving functions and since we never eliminate valid functions, if Olive terminates, it must output a near optimal valid policy. In our analysis, we incorporate sampling effects to derive robust versions of these two facts, and thus we can guarantee that the algorithm always outputs a policy that is at most -suboptimal.\nThe more challenging component is ensuring that the algorithm terminates in polynomially many iterations, which is critical for obtaining a polynomial sample complexity bound. This argument crucially relies on the Bellman Factorization (recall Definition 5), which enables us to embed the distributions into M dimensions and measure progress in this low-dimensional space.\nIf we ignore sampling effects we can set φ = 0, and by using the Bellman Factorization to write E(f, πf ′ , h) as an inner product, we can think of the learning step in Eq. (7) as introducing a homogenous linear constraint on the set of ξh(f) vectors. Now, if we ever execute the learning step at level h again, that means we found a new function f ′ for which 〈νh(f ′), ξh(f ′)〉 is very large. Importantly, this means that νh(f ′) must be linearly independent from all other constraints we have introduced, since f ′ satisfies all of those homogenous linear equalities. Therefore, introducing the constraint involving νh(f\n′) guarantees we eliminate one more dimension from the null space of the constraint matrix. Since there are only M dimensions, this leads to a bound on the number of iterations.\nThe above heuristic reasoning, despite relying on the brittle notion of linear independence, can be made robust. With sampling effects, rather than homogeneous linear equalities, our learning step for level h introduces linear inequality constraints to the ξh(f) vectors. But if f\n′ is a surviving function that forces us to train at level h, it means that 〈νh(f ′), ξh(f ′)〉 is very large, while 〈νh(·), ξh(f ′)〉 is very small for all previous νh(·) vectors used in the learning step. Intuitively this means that the new ν(f ′) vector is quite different from all of the previous ones, and our proof uses a volumetric argument to show that this suffices to guarantee substantial learning takes place.\nThe optimistic choice for ft is critical for driving the agent’s exploration. With this choice, if ft is valid,\nAlgorithm 1 Olive (F ,M, ζ, , δ) – Optimism Led Iterative Value-function Elimination\n1: Collect nest trajectories with actions taken in an arbitrary manner; save initial contexts {x(i)1 } nest i=1 . 2: Estimate the predicted value for each f ∈ F : V̂f = 1nest ∑nest i=1 f(x (i) 1 , πf (x (i) 1 )). 3: F0 ← F . 4: for t = 1, 2, . . . do 5: Choose policy ft = argmaxf∈Ft−1 V̂f , πt = πft . 6: Collect neval trajectories {(x(i)1 , a (i) 1 , r i 1, . . . , x (i) H , a (i) H , r (i) H )} neval i=1 by following πt (i.e. a (i) h = πt(x (i) h )\nfor all h, i). 7: Estimate ∀h ∈ [H],\nẼ(ft, πt, h) = 1\nneval neval∑ i=1 [ ft(x (i) h , a (i) h )− r (i) h − ft(x (i) h+1, a (i) h+1) ] . (5)\n8: if ∑H h=1 Ẽ(ft, πt, h) ≤ 5 /8 then\n9: Terminate and ouptut πt. 10: end if 11: Pick any ht ∈ [H] for which Ẽ(ft, πt, ht) ≥ 5 /8H (One is guaranteed to exist). 12: Collect trajectories {(x(i)1 , a (i) 1 , r (i) 1 , . . . , x (i) H , a (i) H , r (i) H )}ni=1 where a (i) h = πt(x (i) h ) for all h 6= ht and a (i) ht\nis drawn uniformly at random. 13: Estimate\nÊ(f, πt, ht) = 1\nn n∑ i=1 1[a (i) ht = πf (x (i) ht )] 1/K ( f(x (i) ht , a (i) ht )− r(i)ht − f(x (i) ht+1 , πf (x (i) ht+1 )) ) . (6)\n14: Learn\nFt = { f ∈ Ft−1 : ∣∣∣Ê(f, πt, ht)∣∣∣ ≤ φ} . (7) 15: end for\nthen the algorithm terminates correctly, and if ft is not valid, then substantial progress is made. Thus the agent does not get stuck exploring with many valid but suboptimal functions, which could result in exponential sample complexity."
    }, {
      "heading" : "4.2 Sample Complexity",
      "text" : "We now turn to our main result, which guarantees that Olive PAC-learns Contextual Decision Processes with polynomial sample complexity.\nTheorem 1. For any , δ ∈ (0, 1), any Contextual Decision Process and function class F that admits a Bellman Factorization with parameters M, ζ, run Olive with the following parameters:\nφ = 12H √ M , nest =\n32\n2 log(6N/δ),\nneval = 288H2\n2 log\n( 12H2M log(6H √ Mζ/ )\nδ\n) ,\nn = 4608H2MK\n2 log\n( 12NHM log(6H √ Mζ/ )\nδ\n) .\nThen, with probability at least 1 − δ, Olive halts and returns a policy π̂ that satisfies V π̂ ≥ V ?F − (V ?F in\nDefinition 3), and the number of episodes required is at most8\nÕ ( M2H3K\n2 log(Nζ/δ)\n) . (8)\nAccording to this theorem, if a Contextual Decision Process and function class F admit a Bellman Factorization with small Bellman Rank and F contains valid functions, Olive is guaranteed to find a near optimal valid function using only polynomially many episodes. Recall that many popular models, including MDPs with small state spaces, POMDPs with reactive value functions and small hidden-state spaces, and linear quadratic regulators admit factorizations with small Bellman Rank. Moreover, recall that our validity requirement is considerably weaker than realizability assumptions in prior works characterizing reinforcement learning with function approximation. To our knowledge, this is simultaneously the most general polynomial sample complexity bound for reinforcement learning with rich observation spaces and for reinforcement learning with function approximation. Our result also certifies that the notion of Bellman Factorization, which is quite general, is sufficient for efficient exploration and learning in sequential decision making problems.\nIt is worth briefly comparing this result with prior work.\n1. The most closely related result is the recent work of Krishnamurthy et al. [2016], who also consider episodic reinforcement learning with infinite observation spaces and function approximation. The model studied in their paper is a form of Contextual Decision Process with Bellman Rank M , so our result applies as is to their setting. Importantly, we eliminate the need for their deterministic transition assumption, resolving an open problem posed in their paper. Moreover, our sample complexity bound improves on theirs in the dependence on H and , but at the cost of a worse dependence on M . We emphasize that our result applies to a much more general class of models.\n2. Another related line of literature provides sample complexity bounds for Fitted Value/Policy Iteration methods (e.g., [Munos, 2003, Antos et al., 2008, Munos and Szepesvári, 2008]). These works consider the infinite-horizon discounted MDP setting, and require much stronger assumptions than we do, that the function class not only captures Q?, but is also closed under Bellman update operators, and the degree of violation to these assumptions shows up in the approximation error guarantee (see e.g., the inherent Bellman error in [Antos et al., 2008]). More importantly, the analyses rely on the so-called concentrability coefficients to correct the mismatch between training and test distributions [Farahmand et al., 2010, Lazaric et al., 2012], implicitly assuming that an exploration distribution is given, hence this line of literature does not address the exploration issue which is our main focus.\n3. Since CDPs include small-state MDPs, our algorithm can be applied as is to these problems. Unfortu-\nnately, our sample complexity is polynomially worse than the state of the art Õ(Mpoly(H)K 2 log(1/δ)) bounds for PAC-learning MDPs [Dann and Brunskill, 2015]. On the other hand, our algorithm also applies to MDPs with infinite state spaces, which cannot be handled by tabular methods.\n4. Our setup can be applied to learning reactive policies in POMDPs (see Proposition 4). Azizzadenesheli et al. [2016] provided a sample-efficient algorithm in a closely related setting, where both the observation space and the hidden-state space are small in cardinality. While their approach does not require realizable value-functions, their sample complexity depends polynomially on the number of unique observations, and the method relies on additional mixing assumptions of the underlying MDP, which we do not.\n5. Finally, Contextual Decision Processes also encompass contextual bandits, where the optimal sample complexity is O(K log(N)/ 2) [Agarwal et al., 2012]. As contextual bandits have M = 1 and H = 1, our algorithm achieves this optimal sample complexity in this special case.\nTurning briefly to lower bounds, since the CDP setting with Bellman Factorization is new, general lower bounds for the broad class do not exist. However, we can use MDP lower bounds for guidance on the question\n8We use Õ(·) notation to suppress poly-logarithmic dependence on everything except N and δ.\nof optimality, since the small-state MDPs in Example 1 are a special case. While no existing MDP lower bounds apply as is, since formulations vary, in Appendix A we adapt ideas from the literature [Auer et al., 2002] to obtain a Ω(MKH/ 2) sample complexity lower bound for learning the MDPs in Example 1.\nComparing with this lower bound, the sample complexity in Theorem 1 is worse in M,H, and log(N) factors, but of course the small-state MDP is a significantly simpler special case of our setting. We leave as future work the question of optimal sample complexity for learning CDPs with low Bellman Rank."
    }, {
      "heading" : "5 Extensions",
      "text" : "We introduce four important extensions to our algorithm and analysis."
    }, {
      "heading" : "5.1 Unknown Bellman Rank",
      "text" : "The first extension deals with the difficulty that M might not be known to the agent (note that Algorithm 1 requires M as an input parameter). However, a simple procedure described in Algorithm 2, can guess the value of M on a doubling schedule and handle this situation with no consequences to asymptotic sample complexity.9\nAlgorithm 2 GuessM(F , ζ, , δ) 1: for i = 1, 2, . . . do 2: M ′ ← 2i. 3: Call Olive(F ,M ′, , δi(i+1) ) with parameters specified on Theorem 1.\n4: Terminate the subroutine when t > HM ′ log ( 6H √ M ′ζ ) / log(5/3) in Line 4 (the for-loop).\n5: if a policy π is returned from Olive then return π. 6: end if 7: end for\nTheorem 2. For any , δ ∈ (0, 1), any Contextual Decision Process and function class F that admits a Bellman Factorization with parameters M, ζ, if we run GuessM(F , , δ), then with probability at least 1− δ, Olive halts and returns a policy which satisfies V π̂ ≥ V ?F − , and the number of episodes required is at most\nÕ ( M2H3K\n2 log(Nζ/δ)\n) .\nWe give some intuition about the proof here, with details deferred details to Appendix D. In Algorithm 2, M ′ is our guess of M , and it grows exponentially. When M ′ ≥ M , analysis of the main algorithm shows that Olive(F ,M ′, , δi(i+1) ) will terminate and return a near-optimal policy with high probability, and by our doubling schedule, we know that M ′ ≤ 2M which is not too large. On the other hand, Olive may not explore effectively when M ′ < M , because we use too few samples (chosen according to M ′) to estimate average Bellman errors in Eq. (6). This worse accuracy is inadequate to guarantee sufficient progress in learning\nHowever, the high-probability guarantee that f? will not be eliminated is unaffected, because the threshold φ on Line 7 of Olive is set in accordance with the sample size n specified in Theorem 1, regardless ofM . Consequently, if the algorithm ever terminates when M ′ < M , we still get a near-optimal policy. The only issue is that Olive subroutine may not terminate, which we address by explicitly terminating the subroutine once its sample complexity bound, which depends on M ′, is reached (Line 4 in Algorithm 2). Finally, by splitting the failure probability δ appropriately among all guesses of M ′, we obtain the same order of sample complexity as in Theorem 1.\n9In Algorithm 2 we assume that ζ is known. In the examples provided in Proposition 2, 3, and 4, however, ζ grows with M in the form of ζ = 2 √ M . In this case, we can compute ζ′ = 2 √ M ′ and call Olive with ζ′ instead of ζ. As long as ζ is a polynomial term and non-decreasing in M the same analysis applies and Theorem 2 holds."
    }, {
      "heading" : "5.2 Separation of Policy Class and V-value Class",
      "text" : "So far, we have assumed that the agent has access to a class of Q-value functions F ⊂ X × A → [0, 1]. In this section we show that our algorithm allows separate representations of policies and V-value functions.\nFor every f ∈ F , and any x ∈ X , a 6= πf (x), we note that the value of f(x, a) is not used by Algorithm 1, and changing it to arbitrary values does not affect the execution of the algorithm as long as f(x, a) ≤ f(x, πf (x)) (so that πf does not change). In other words, our algorithm only interacts with f in two forms:\n1. f ’s greedy policy πf .\n2. A mapping gf : x 7→ f(x, πf (x)). We call such mappings V-value functions to contrast the previous use of Q-value functions.10\nHence, supplying F is equivalent to supplying the following space of (policy, V-value function) pairs:\n{ ( πf , gf ) : f ∈ F}.\nThis observation provides further evidence that Definition 3 is significantly less restrictive than standard realizability assumptions. Validity of f means that (πf , gf ) obeys the Bellman Equations for Policy Evaluation (i.e., gf predicts the long-term value of following πf ), as opposed to the more common Bellman Optimality Equations. In MDPs, there are many ways to satisfy the policy evaluation equations at every state simultaneously, while Q? is the only function that satisfies all optimality equations.\nMore generally, instead of using a Q-value function class, we can run Olive with a policy space Π ⊂ X → A and a V-value function class G ⊂ X → [0, 1] where we assemble (policy,V-value function) pairs by taking the Cartesian product of Π and G. Olive can be run here with the understanding that each Q-value function f in Olive is associated with a (π, g) pair, and the algorithm uses π instead of πf and g(x) instead of f(x, πf (x)). All the analysis applies directly with this transformation, and the log |F| dependence in sample complexity is replaced by log |Π| + log |G|. Note also that the definition of Bellman Factorization also extends naturally to this case, where the first argument is the (π, g) pair and the second argument is a roll-in policy, π′."
    }, {
      "heading" : "5.3 Infinite Hypothesis Classes",
      "text" : "The arguments in Section 4 assume that |F| = N < ∞. However, almost all commonly used function approximators are infinite classes, which restricts the applicability of our algorithm. On the other hand, the size of the function class appears in our analysis only through deviation bounds, so techniques from empirical process theory can be used to generalize our results to infinite classes. This section establishes parallel versions of those deviation bounds for function classes with finite combinatorial dimensions, and together with the rest of the original analysis we can show that our algorithm enjoys similar guarantees when working with infinite hypothesis classes.\nSpecifically, we consider the setting where Π and G are given (see Section 5.2), and they are infinite classes with finite combinatorial dimensions. We assume that Π has finite Natarajan dimension (Definition 6), and G has finite Pseudo-dimension (Definition 7). These two dimensions are extensions of VC-dimension to multi-class classification and regression respectively, and are standard in literature of statistical learning theory.\nDefinition 6 (Natarajan dimension [Natarajan, 1989]). Suppose X is a feature space and Y is a finite label space. Given hypothesis class H ⊂ X → Y, its Natarajan dimension Ndim(H) is defined as the maximum cardinality of a set A ⊆ X that satisfies the following: there exists h1, h2 : A → Y such that (1) ∀x ∈ A, h1(x) 6= h2(x), and (2) ∀B ⊆ A, ∃h ∈ H such that ∀x ∈ B, h(x) = h1(x) and ∀x ∈ A \\B, h(x) = h2(x).\nDefinition 7 (Pseudo-dimension [Haussler, 1992]). Suppose X is a feature space. Given hypothesis class H ⊂ X → R, its pseudo-dimension Pdim(H) is defined as Pdim(H) = VC-dim(H+), where H+ = {(x, ξ) 7→ 1[h(x) > ξ] : h ∈ H} ⊂ X × R→ {0, 1}.\n10In the MDP setting, such functions are also known as state-value functions.\nThe definition of Pseudo-dimension relies on that of VC-dimension, whose definition and basic properties are recalled in the appendix. We state the final sample complexity result here. Since the algorithm parameters are somewhat complex expressions, we omit them in the theorem statement and provide their specification in the proof, which is deferred to Appendix D.\nTheorem 3. Let Π ⊂ X → A with Ndim(Π) ≤ dΠ < ∞ and G ⊂ X → [0, 1] with Pdim(G) ≤ dG < ∞. For any , δ ∈ (0, 1), any Contextual Decision Process with policy space Π and function space G that admits a Bellman Factorization with parameters M, ζ, if we run Olive with appropriate parameters, then with probability at least 1− δ, Olive halts and returns a policy π̂ that satisfies V π̂ ≥ V ?F − , and the number of episodes required is at most\nÕ ( M2H3K2\n2\n( dΠ + dG + log(ζ/δ) )) . (9)\nCompared to Theorem 1, the sample complexity we get for infinite hypothesis classes has two differences: (1) logN is replaced by dΠ + dG , which is expected, based on the discussion in Section 5.2, and (2) the dependence on K is quadratic as opposed to linear. In fact, in the proof of Theorem 1, we exploited the low-variance property of importance weights in Eq. (6), and applied Bernstein’s inequalities to avoid a factor of K. With infinite hypothesis classes, the same approach does not apply directly. However, we believe this is only a technical issue and a more refined analysis might recover the linear dependence (e.g., using tools from Panchenko [2002])."
    }, {
      "heading" : "5.4 Approximate Validity and Approximate Bellman Rank",
      "text" : "Recall that the sample-efficiency guarantee of Olive relies on two major assumptions:\n• We assumed that F contains valid functions (Definition 3). In practice, however, it is hard to specify a function class that contains strictly valid functions, as the notion of validity depends on the environment dynamics, which are unknown. A much more realistic situation is that some functions in F satisfy validity only approximately.\n• We assumed that the average Bellman errors have an exact low-rank factorization (Definition 5). While this is true for a number of RL models (Section 3), it is worth keeping in mind that these are only models of the environments, which are different from and only approximations to the real environments themselves. Therefore, it is more realistic to assume that an approximate factorization exists when defining Bellman Factorization.\nIn this section, we show that the algorithmic idea of Olive is indeed robust against both types of approximation errors, and degrades gracefully with the extent that the two assumptions are violated. Below we first introduce the approximate versions of Definition 3 and 5. Then we give a slightly extended version of our algorithm, Oliver (for Optimism-Led Iterative Value-function Elimination with Robustness, see Algorithm 3), and state its sample complexity guarantee in Theorem 4.\nDefinition 8 (Approximate validity of f). Given any CDP and function class F , we say f ∈ F is θ-valid if for any f ′ ∈ F and any h ∈ [H], |E(f, πf ′ , h)| ≤ θ.\nThe approximation error θ introduced in Definition 8 allows our algorithm to compete against a broader range of functions; hence the notions of optimal function and value need to be re-defined accordingly.\nDefinition 9. For a fixed θ, define f?θ = argmaxf∈F : f is θ-valid V πf , and V ?F,θ = V\nπf? θ .\nBy definition, V ?F,θ is non-decreasing in θ, and Definition 3 is its special case with θ = 0. When θ > 0, we compete against some functions that do not obey Bellman equations, breaking an essential element of value-based RL. As a consequence, returning a policy with value close to V ?F,θ in a sample-efficient manner is very challenging, and the value that our algorithm can guarantee is suboptimal to V ?F,θ by a term that is proportional to θ and does not diminish with more data.\nAlgorithm 3 Oliver (F , θ,M, ζ, η, , δ) 1: Let ′ = + 2H(3 √ M(θ + η) + η).\n2: Collect nest trajectories with actions taken in an arbitrary manner; save initial contexts {x(i)1 } nest i=1 . 3: Estimate the predicted value for each f ∈ F : V̂f = 1nest ∑nest i=1 f(x (i) 1 , πf (x (i) 1 )). 4: F0 ← F . 5: for t = 1, 2, . . . do 6: Choose policy ft = argmaxf∈Ft−1 V̂f , πt = πft . 7: Collect neval trajectories {(x(i)1 , a (i) 1 , r i 1, . . . , x (i) H , a (i) H , r (i) H )} neval i=1 by following πt (i.e. a (i) h = πt(x (i) h )\nfor all h, i). 8: Estimate ∀h ∈ [H],\nẼ(ft, πt, h) = 1\nneval neval∑ i=1 [ ft(x (i) h , a (i) h )− r (i) h − ft(x (i) h+1, a (i) h+1) ] . (11)\n9: if ∑H h=1 Ẽ(ft, πt, h) ≤ 5 ′/8 then\n10: Terminate and ouptut πt. 11: end if 12: Pick any ht ∈ [H] for which Ẽ(ft, πt, ht) ≥ 5 ′/8H (One is guaranteed to exist). 13: Collect trajectories {(x(i)1 , a (i) 1 , r (i) 1 , . . . , x (i) H , a (i) H , r (i) H )}ni=1 where a (i) h = πt(x (i) h ) for all h 6= ht and a (i) ht\nis drawn uniformly at random. 14: Estimate\nÊ(f, πt, ht) = 1\nn n∑ i=1 1[a (i) ht = πf (x (i) ht )] 1/K ( f(x (i) ht , a (i) ht )− r(i)ht − f(x (i) ht+1 , πf (x (i) ht+1 )) ) . (12)\n15: Learn\nFt = { f ∈ Ft−1 : ∣∣∣Ê(f, πt, ht)∣∣∣ ≤ φ+ θ} . (13) 16: end for\nDefinition 10 (Approximate Bellman Rank). We say that a CDP 〈X ,A, H, P 〉 and F ⊂ X × A → [0, 1], admits a Bellman Factorization with Bellman Rank M , norm parameter ζ, and approximation error η, if there exists νh : F → RM , ξh : F → RM for each h ∈ [H], such that for any f, f ′ ∈ F , h ∈ [H],\n|E(f, πf ′ , h)− 〈νh(f ′), ξh(f)〉| ≤ η, (10)\nand ‖νh(f ′)‖2 · ‖ξh(f)‖2 ≤ ζ <∞.\nA modified version of Olive that deals with these approximation errors, Oliver (for Olive with Robustness), is specified in Algorithm 3. Here, we use to denote the component of the suboptimality that diminish as more data is collected, and the total suboptimality that we can guarantee is plus a term proportional to θ and η (see Eq. (14) in Theorem 4). The algorithm is almost identical to Olive except for two differences: (1) it uses ′ on Line 9 in the termination condition as opposed to , and (2) it uses a higher threshold that depends on θ in Eq. (13) to avoid eliminating θ-valid functions.\nTheorem 4. For any , δ ∈ (0, 1), any Contextual Decision Process and function class F that admits a Bellman Factorization with parameters M , ζ, and η, suppose we run Oliver with any θ ∈ [0, 1], and nest, neval, n, φ as specified in Theorem 1. Then with probability at least 1 − δ, Oliver halts and returns a policy π̂ which is at most\n+ 8H √ M(θ + η) (14)\nsuboptimal compared to V ?F,θ defined in Definition 9, and the number of episodes required is at most\nÕ ( M2H3K\n2 log(Nζ/δ)\n) . (15)"
    }, {
      "heading" : "6 Proofs of Main Results",
      "text" : "In this section, we will provide the high-level intuition as well as the key lemmas involved in proving Theorem 1. We will also show how the lemmas are put together in proving the theorem. Detailed proofs of the lemmas are in Appendix C.\nAt a high-level, the proof follows an explore or exploit argument common to most sample-efficient RL algorithms. Concretely, we argue that the optimistic policy chosen in Step 5 of Algorithm 1 is either approximately optimal, or visits a context distribution under which it has a large Bellman error. This implies that using this policy for exploration leads to learning on a new context distribution. For sample efficiency, we then need to establish that this event cannot happen too many times. This is done by leveraging the Bellman Factorization of the process and arguing that the number of times an sub-optimal policy is found can be no larger than Õ(MH log(1/ )). Combining with the number of samples collected for every sub-optimal policy, this immediately yields our PAC learning guarantee. We now make this intuition concrete with the key lemmas in our proof."
    }, {
      "heading" : "6.1 Key Lemmas for Theorem 1",
      "text" : "We begin by decomposing a policy loss-like term into the sum of Bellman errors.\nLemma 1 (Policy loss decomposition). Define Vf = E[f(x1, πf (x1))]. Then ∀f : X ×A → [0, 1],\nVf − V πf = H∑ h=1 E(f, πf , h). (16)\nThe structure of this lemma is similar to many existing results in RL that upper-bound the loss of following an approximate value function greedily using the function’s Bellman errors (e.g., Singh and Yee [1994]). However, most existing results are inequalities that use max-norm relaxations to deal with mismatch in distributions hence likely to be loose. Our lemma, on the other hand, is an equality, thanks to the fact that we are comparing V πf to Vf , not V\n?. As the remaining analysis will show, this simple equation allows us to relate policy loss (from the LHS) with average Bellman error (the RHS) that we use to drive exploration. In particular, this lemma implies an explore-or-terminate behavior for our algorithm.\nLemma 2 (Optimism drives exploration). Suppose the estimates V̂f and Ẽ(ft, πt, h) in Line 2 and 7 always satisfy\n|V̂f − Vf | ≤ /8, and |Ẽ(ft, πt, h)− E(ft, πt, h)| ≤\n8H (17)\nthroughout the execution of the algorithm. Assume further that f? is never eliminated. Then in any round t, one of the following two statements holds:\n(i) the algorithm does not terminate and\nE(ft, πt, ht) ≥\n2H , (18)\n(ii) or the algorithm terminates and the output policy πt satisfies V πt ≥ V ?F − .\nThe lemma essentially guarantees that the policy πt used at the iteration t in Algorithm 1 has a sufficiently large Bellman error on at least one of the levels, provided two conditions are met: (1) we have reasonably accurate value function estimates from Step 2, and (2) we collected enough samples to form reliable Bellman\nerror estimates under ft at each level h in Step 7 of the algorithm. The result of Theorem 1 can then be obtained using two further ingredients. First, we need to make sure that the first case in Lemma 2 does not happen too many times. Second, we need to collect enough samples to ensure the preconditions in the lemma in Steps 2 and 7. We first establish a bound on the number of iterations using the Bellman Rank of the problem, before moving on to sample complexity questions.\nLemma 3 (Iteration Complexity). If Ê(f, πt, ht) in Eq. (6) always satisfies that\n|Ê(f, πt, ht)− E(f, πt, ht)| ≤ φ (19)\nthroughout the execution of the algorithm (φ is the threshold in the elimination criterion), then f? is never eliminated. Furthermore, for any particular level h, if whenever ht = h we have\n|E(ft, πt, ht)| ≥ 6 √ Mφ, (20)\nthen the number of rounds that ht = h is at most M log ( ζ\n2φ ) / log 53 .\nPrecondition (19) simply posits that we collect enough samples for reliable Bellman error estimation in Step 13. Intuitively, since f? has no Bellman error, this is sufficient to ensure that it is never eliminated. Precondition 20 is naturally satisfied by our exploration policies πt given Lemma 2. Given this, the above lemma bounds the number of iterations at which we can find a large Bellman error at any particular level in Algorithm 1.\nThe intuition behind this claim is most clear in the POMDP setting of Proposition 4. In this case, νh(f ′) in Definition 5 corresponds to the distribution over hidden states induced by πf ′ at level h. At iteration t, the exploration policy πft induces such a hidden-state distribution p = νh(ft) at the chosen level h = ht, which results in the elimination of all functions which have a large Bellman error on p. Thanks to the Bellman Factorization, [ξht(f)]s be the Bellman error of f on the hidden state s, this corresponds to the elimination of all functions this corresponds to the elimination of all f with a large |p>ξh(f)|, where ξh(f) is also defined in Definition 5. In this case, it can be easily shown that ξh(f) ∈ [−2, 2]M , so the space of all such vectors {ξh(f) : f ∈ F} at each level h is originally contained in an `∞ ball in M dimensions with radius 2, and, whenever ht = h, we intersect this set with two parallel halfspaces. We use a geometric argument to prove that each such intersection reduces the volume of the space by a constant factor, and we also show that the volume of the set is bounded from below. Together, these two facts lead to the iteration complexity upper bound in Lemma 3. The mathematical techniques used here are analogous to the analysis of the Ellipsoid method in linear programming.\nFinally, we need to ensure that the number of samples collected in each of Steps 2, 7, and 13 of Algorithm 1 can be upper bounded, which yields the overall PAC learning result in Theorem 1. The next three lemmas present precisely the deviation bounds required for this argument. The first two follow from simple applications of Hoeffding’s inequality.\nLemma 4 (Deviation Bound for V̂f ). With probability at least 1− δ,\n|V̂f − Vf | ≤ √ 1\n2nest log\n2N\nδ\nholds for all f ∈ F simultaneously. Hence, we can set nest ≥ 32 2 log 2N δ to guarantee that |V̂f − Vf | ≤ /8.\nThis helps control the number of samples required in Step 2 of Algorithm 1.\nLemma 5 (Deviation Bound for Ẽ(ft, πt, h)). For any fixed ft, with probability at least 1− δ,\n|Ê(ft, πt, h)− E(ft, πt, h)| ≤ 3 √ 1\n2neval log\n2H\nδ\nholds for all h ∈ [H] simultaneously. Hence, we can set neval ≥ 288H 2 2 log 2H δ to guarantee that |Ẽ(ft, πt, h)− E(ft, πt, h)| ≤ 8H .\nThis lemma can be seen as the sample complexity at each iteration in Step 7 of Algorithm 1. Note that no union bound over F is needed here, since Step 7 only estimates the average Bellman error for a single policy, which is fixed before data is collected. Finally, we bound the sample complexity of the main exploration step.\nLemma 6 (Deviation Bound for Ê(f, πt, ht)). For any fixed πt and ht, with probability at least 1− δ,\n|Ê(f, πt, ht)− E(f, πt, ht)| ≤\n√ 8K log 2Nδ\nn + 2K log 2Nδ n\nholds for all f ∈ F simultaneously. Hence, we can set n ≥ 32Kφ2 log 2N δ to guarantee that |Ê(f, πt, ht) − E(f, πt, ht)| ≤ φ as long as φ ≤ 4.\nThis lemma uses Bernstein’s inequality to exploit the small variance of the importance weighted estimates. We are now ready to prove Theorem 1 given these lemmas."
    }, {
      "heading" : "6.2 Proof of Theorem 1",
      "text" : "Suppose the preconditions of Lemma 2 (Eq. (17)) and Lemma 3 (Eq. (19)) hold; we will show them via concentration inequalities later. Applying Lemma 2, in every round t before the algorithm terminates,\nE(ft, πt, ht) ≥ 2H = 6 √ Mφ,\nthanks to the choice of φ. For level h = ht, Eq. (20) is satisfied. According to Lemma 3, the event ht = h can happen at most M log ( ζ\n2φ ) / log 53 times for every h ∈ [H]. Hence, the total number of rounds in the\nalgorithm is at most\nHM log\n( ζ\n2φ\n) / log 5\n3 = HM log\n( 6H √ Mζ ) / log 5\n3 .\nNow we are ready to apply the concentration inequalities to show that Eq. (17) and 19 hold with high probability. We split the total failure probability δ among the following estimation events:\n1. Estimation of V̂f (Lemma 4; only once): δ/3. 2. Estimation of Ẽ(ft, πt, h) (Lemma 5; every round): δ/ ( 3HM log ( 6H √ Mζ ) / log 53 ) .\n3. Estimation of Ê(f, πt, ht) (Lemma 6; every round): same as above.\nSince these events happen in a particular sequence, our proof actually bounds the probability of these failure events conditioned on all previous events succeeding. This imposes no technical challenge as fresh data is collected for every event, so it effectively reduces to a standard union bound.\nApplying Lemma 4, 5, 6 with the above failure probabilities, we can verify that the choice of nest, neval, n in the algorithm statement satisfies the preconditions of Lemma 2 and 3. Finally, we upper bound the total number of episodes as\nnest + neval ·HM log\n( 6H √ Mζ ) / log 5\n3 + n ·HM log\n( 6H √ Mζ ) / log 5\n3\n= Õ\n( log(N/δ)\n2 + MH3 2 log(ζ/δ) + M2H3K 2 log(Nζ/δ)\n) = Õ ( M2H3K\n2 log(Nζ/δ)\n) ."
    }, {
      "heading" : "7 Conclusions and Discussions",
      "text" : "In this paper, we present a new model for RL with rich observations, called Contextual Decision Processes, and a structural property, the Bellman Factorization, of these models that enables sample-efficient learning. Our unified approach allows us to address several settings of practical interest that have largely eluded RL theory to date. Via extensions of our main result, we also demonstrate that our techniques are quite robust and degrade gracefully with violation of our assumptions. While the results are certainly an exciting development for RL theory, we believe they also elicit several further questions that we look forward to studying in future work:\n1. Most importantly, can we obtain a computationally efficient algorithm for even a smaller, but novel, subset of this setting? Prior related work (for instance in contextual bandits [Dudik et al., 2011, Agarwal et al., 2014]) have used supervised learning oracles for computationally efficient approaches. Is there a suitable oracle for this more challenging setting?\n2. Our sample complexity depends polynomially on the cardinality of the action space. Can we extend our results to handle large or continuous action spaces?\n3. Can we address sample-efficient RL given only a policy class rather than a value function class? Empirical approaches in policy search often rely on policy gradients, which are subject to local optima. Can we develop parallel results to this work, without access to value functions?\nWe believe that understanding these questions will serve to bridge RL theory and practice, and will be critical to success in challengin reinforcement learning problems."
    }, {
      "heading" : "Appendix",
      "text" : ""
    }, {
      "heading" : "A A New Lower Bound",
      "text" : "In this section, we prove a new lower bound for layered episodic MDPs that meet the assumptions we make in this paper.\nWe first recall some definitions. A layered episodic MDP is defined by a time horizon H, a state space S, partitioned into sets S1, . . . ,SH , each of size at most M , and an action space A of size K. The system descriptor is replaced with a transition function Γ that associates a distribution over states with each state action pair. More formally, for any sh ∈ Sh, and a ∈ A, Γ(sh, a) ∈ ∆(Sh+1). The starting state is drawn from Γ1 ∈ ∆(S1), and all transitions from SH are terminal.\nThere is also a reward distribution R that associates a random reward with each state-action pair. We use r ∼ R(s, a) to denote the random instantaneous reward for taking action a at state s. We assume that the cumulative reward ∑H h=1 rh ∈ [0, 1], where rh is the reward received at level hth as in Assumption 1.\nObserve that this process is a special case of the finite-horizon Contextual Decision Process and moreover, with the set of all value functions F = (S ×A → [0, 1]), admits a Bellman Factorization with Bellman Rank at most M (by Proposition 2). Thus our upper bounds for PAC learning apply directly to this setting.\nWe now state the lower bound.\nTheorem 5. Fix M ≥ 4, H,K ≥ 2 and ∈ (0, 1 48 √ 8 ). For any algorithm and any n ≤ cMKH/ 2, there exists a layered episodic MDP with H layers, M states per layer, and K actions, such that the probability that the algorithm outputs a policy π̂ with V (π̂) ≥ V ? − after collecting n trajectories is at most 11/12. Here c > 0 is a universal constant.\nThe result precludes a o(MKH/ 2) PAC-learning sample complexity bound since in this case the algorithm must fail with constant probability. The result is similar in spirit to other lower bounds for PAClearning MDPs [Dann and Brunskill, 2015, Krishnamurthy et al., 2016], but we are not aware of any lower bound that applies directly to our setting. There are two main differences between our bound and the lower bound due to Dann and Brunskill [2015] for episodic MDPs. First, their bound assumes that the total reward is in [0, H], so the H2 dependence in the sample complexity is a consequence of scaling the rewards. Second, their MDP is not layered, but instead has M total states shared across all layers. In contrast, our process is layered with M distinct states per layer and total reward bounded in [0, 1]. Intuitively, our additional H dependence arises simply from having MH total states.\nAt a high level, the proof is based on embedding Θ(MH) independent multi-arm bandit instances into a MDP and requiring that the algorithm identify the best action in Ω(MH) of them to produce a near-optimal policy. By appealing to a sample complexity lower bound for best arm identification, this implies that the algorithm requires Ω(MHK/ 2) samples to identify a near-optimal policy.\nWe rely on a fairly standard lower bound for best arm identification. We reproduce the formal statement from Krishnamurthy et al. [2016], although the proof is based on earlier lower bounds due to Auer et al. [2002].\nProposition 7. For any K ≥ 2 and τ ≤ √\n1/8 and any best arm identification algorithm that produces an estimate â, there exists a multi-arm bandit problem for which the best arm a? is τ better than all others, but P[â 6= a?] ≥ 1/3 unless the number of samples T is at least K72τ2 .\nIn particular, the problem instance used in this lower bound is one where the best arm a? has reward Ber(1/2 + ), while all other arms have reward Ber(1/2). Our construction will embed precisely these instances into the MDP.\nProof. We construct an MDP with M states per level, H levels, and K actions per state. At each level, we allocate three special states, wh, gh, and bh for “waiting”, “good”, and “bad.” The remaining M−3 “bandit” states are denoted sh,i, i ∈ [M − 3]. Each bandit state has an unknown optimal action a?h,i.\nThe dynamics are as follows.\n• For waiting states wh, all actions are equivalent and with probability 1 − 1/H they transition to the next waiting state wh+1. With the remaining 1/H probability, they transition randomly to one of the bandit state sh+1,i so each subsequent bandit state is visited with probability 1 H(M−3) .\n• For bandit states sh,i, the optimal action a?h,i transitions to the good state gh+1 with probability 1/2+τ and otherwise to the bad state bh+1. All other actions transition to gh+1 and bh+1 with probability 1/2. Here τ is a parameter we will set toward the end of the proof.\n• Good states always transition to the next good state and bad states always transition to bad states.\n• The starting state is w1 with probability 1−1/H and s1,i with probability 1H(M−3) for each i ∈ [M−3].\nThe reward at all states except gH is zero, and the reward at gH is one. Clearly the optimal policy takes actions a?h,i for each bandit state, and takes arbitrary actions at the waiting, good, and bad states.\nThis construction embeds H(M−3) best arm identification problems that are identical to the one used in Proposition 7 into the MDP. Moreover, these problems are independent in the sense that samples collected from one provides no information about any others. Appealing to Proposition 7, for each bandit state (h, i), unless K72τ2 samples are collected from that state, the learning algorithm will fail to identify the optimal action a?h,i with probability at least 1/3.\nAfter the execution of the algorithm, let B be the set of (h, s) pairs for which the algorithm identifies the correct action. Let C be the set of (h, s) pairs for which the algorithm collects fewer than K72τ2 samples. For a set S, we use SC to denote the complement.\nE[|B|] = E ∑ (h,s) 1[ah,s = a ? h,s]  ≤ ((M − 3)H − |C|) +\n∑ (h,s)∈C E1[ah,s = a?h,s]\n≤ ((M − 3)H − |C|) + 2 3 |C| = (M − 3)H − |C|/3\nThe second inequality is based on Proposition 7. Now, by the pigeonhole principle, if n ≤ (M−3)H2 × K 72τ2 , then the algorithm can collect K72τ2 samples from at most half of the bandit problems. Thus |C| ≥ (M − 3)H/2, which implies,\nE[|B|] ≤ 5 6 (M − 3)H\nBy Markov’s inequality, P [ |B| ≥ 11\n12 (M − 3)H\n] ≤ E[|B|]11\n12 (M − 3)H ≤ 5/6 11/12 = 10/11\nThus with probability at least 1/11 we know that |B| ≤ 1112 (M − 3)H, so the algorithm failed to identify the optimal action on 1/12 fraction of the bandit problems. Under this event, the suboptimality of the policy produced by the algorithm is,\nV ? − V (π̂) = P[visit BC ]× τ = P[ ⋃\n(h,i)∈BC visit (h, i)]× τ = ∑ (h,i)∈BC P[visit (h, i)]× τ\n= ∑\n(h,i)∈BC\n1\nH(M − 3) (1− 1/H)h−1τ ≥ ∑ (h,i)∈BC\n1\nH(M − 3) (1− 1/H)Hτ\n≥ ∑\n(h,i)∈BC\n1 H(M − 3) 1 4 τ ≥ H(M − 3) 12\n1 H(M − 3) 1 4 τ = τ 48\nHere we use the fact that the probability of visiting a bandit state is independent of the policy and that the policy can only visit one bandit state per episode, so the events are disjoint. Moreover, if we visit a bandit state for which the algorithm failed to identify the optimal action, the difference in value is τ , since the optimal action visits the good state with τ more probability than a suboptimal one. The remainder of the calculation uses the transition model, the fact that H ≥ 2, and finally the fact that |B| ≤ 11/12(M − 3)H. Setting τ = 48 and using the requirement on τ gives a stricter requirement on and proves the result."
    }, {
      "heading" : "B Models with Low Bellman Rank",
      "text" : ""
    }, {
      "heading" : "B.1 Proof of Propositon 2",
      "text" : "Let M = |S| and each element of νh(·) and ξh(·) be indexed by s ∈ S. We explicitly construct νh and ξh as follows: let [νh(f ′)]s = Pr [xh = (s, h) | a1:h−1 ∼ πf ′ ], and [ξh(f)]s = E [ f(xh, ah)− rh−f(xh+1, ah+1) ∣∣ xh = (s, h), ah:h+1 ∼ πf ] . In other words, νh(f\n′) is the distribution over states induced by πf ′ at time step h, and the s-th element of ξh is the traditional notion of Bellman error for state s. It is easy to verify that Eq. (3) holds. For the norm constraint, since ‖νh(·)‖1 = 1 and ‖ξh(·)‖∞ ≤ 2, we have ‖νh(·)‖2 ≤ 1 and ‖ξh(·)‖2 ≤ 2 √ M , hence ζ = 2 √ M is a valid upper bound on the product of vector norms."
    }, {
      "heading" : "B.2 Generalization of Li [2009]’s setting",
      "text" : "Li [2009] considers the setting where the learner is given an abstraction φ that maps the large state space S in an MDP to some finite abstract state space S̄ in an MDP. |S̄| is potentially much smaller than |S|, and it is guaranteed that Q? can be expressed as a function of (φ(s), a). Li shows that when delayed Q-learning is applied to this setting, the sample complexity has polynomial dependence on |S̄| with no direct dependence on |S|.\nIn the next proposition, we show that a similar setting for finite-horizon problems admits Bellman Factorization with low Bellman Rank. In particular, we subsume Li’s setting by viewing it as a POMDP, where φ is a deterministic emission process that maps hidden state s ∈ S to discrete observations φ(s) ∈ S̄ = O, and the candidate value functions are reactive so they depend on φ(s) but not directly on s or any previous state. More generally, Proposition 8 claims that for POMDPs with large hidden-state spaces and finite observation spaces, the Bellman Rank is polynomial in the number of observations if the function class is reactive.\nProposition 8 (A generalization of [Li, 2009]’s setting). Consider a POMDP introduced in Example 2 with |O| <∞, and assume that rewards can only take CR different discrete values.11 The CDP 〈X ,A, H, P 〉 induced by letting X = O×[H] and xh = (oh, h), with any F : X×A → [0, 1], admits a Bellman Factorization with M = |O|2CRK and ζ = 2|O|K √ CR.\nProof. For any f, f ′ ∈ F , h ∈ [H], let νh(f ′) and ξh(f) be vectors of length |O|2CRK. Let the entry of νh(f ′) indexed by (oh, ah, rh, oh+1) be\nP [oh, rh, oh+1 | a1:h−1 ∼ πf ′ , ah],\ninterpreted as the following: conditioned on the fact that the first h− 1 actions are chosen according to πf ′ , what is the probability of seeing a particular tuple of (oh, rh, oh+1) when taking a particular action for ah? For ξh(f), let the corresponding entry be (with xh = (oh, h) and xh+1 = (oh+1, h+ 1) as the corresponding contexts in the CDP)\n1[ah = πf (xh)] ( f(xh, ah)− rh − f(xh+1, πf (xh+1)) ) .\nIt is not hard to verify that E(f, πf ′ , h) = 〈νh(f ′), ξh(f)〉. Since fixing ah to any non-adaptive choice of action induces a valid distribution over (oh, rh, oh+1), we have ‖νh(f ′)‖1 = K and ‖νh(f ′)‖2 ≤ K. On the other hand, ‖ξh(f)‖∞ ≤ 2 but the vector only has |O|2CR non-zero entries, so ‖ξh(f)‖2 ≤ 2|O| √ CR. Together the norm bound follows.\n11The discrete reward assumption is made to simplify presentation and can be relaxed. For arbitrary rewards, we can always discretize the reward distribution onto a grid of resolution CR, which incurs η = O(1/CR) approximation error in Definition 10."
    }, {
      "heading" : "B.3 POMDP-like models",
      "text" : "We prove Propositions 3 and 4 by studying a slightly more general model (See Figure 1c). The model behaves like a POMDP except that both the transition function and the reward depends also on the observation, that is Γ : S × X × A → ∆(S) and R : S × X × A → ∆([0, 1]). Clearly this model generalizes standard POMDPs, where the transition and reward are both assumed to be independent of the current observation.\nThis model also generalizes the MDP with low-rank dynamics described in Proposition 3: if the future hidden-state is independent of the current hidden-state conditioned on the observation, we can treat the hidden-state as the low-rank factor in Eq. (4) (see Figure 1). The low-rank dynamics requires that for x at level h, Γ(x′|x, a) = 〈q(x, a), wh(x′)〉 for two M -dimensional vectors. If we write [q(x, a)]s′ = Γ(s′|x, a) and [wh(x ′)]s′ = Ds′(x ′) in the above model, we have identified an M -dimensional decomposition of the transition dynamics. Thus Proposition 3 follows as a special case of our analysis for this more general model. As in Proposition 4, we consider a class F reactive value functions. Observe that for the MDP with low rank dynamics, this provides essentially no loss of generality, since the optimal value function is reactive.\nProposition 9. Let 〈X ,A, H, P 〉 be the CDP induced by the above model which generalizes POMDPs, with X = O × [H] and xh = (oh, h). Given any F : X ×A → [0, 1], the Bellman Rank M ≤ |S| with ζ = 2 √ |S|.\nProof. For any f, f ′ ∈ F , h ∈ [H], consider\na1:h−1 ∼ πf ′ , ah:h+1 ∼ πf ,\nwhich is how actions are chosen in the definition of E(f, πf ′ , h) (see Definition 2). Such a decision-making strategy will induce a distribution over the following set of variables\n(sh, oh, ah, rh, oh+1, ah+1).\nWe use µf,f ′ to denote this distribution, and the subscript emphasizes its dependence on both f and f ′. Note that the marginal distribution of sh only depends on f ′ and has no dependence on f , which we denote as µf ′ . Then, sampling from µf,f ′ is equivalent to the following sampling procedure: (recall that xh = (oh, h))\nsh ∼ µf ′ , oh ∼ Dsh , ah = πf (xh), rh ∼ R(sh, oh, ah), sh+1 ∼ Γ(sh, oh, ah), oh+1 ∼ Dsh+1 , ah+1 = πf (xh).\nThat is, we first sample sh from the marginal µf ′ , and then sample the remaining variables conditioned on sh. Notice that once we condition on sh, the sampling of the remaining variable has no dependence on f\n′, so we denote the joint distribution over the remaining variables (conditioned on the value of sh) µf |sh .\nFinally, we express the factorization of E(f, πf ′ , h) as follows:\nE(f, πf ′ , h) = Eµf,f′ [f(xh, ah)− rh − f(xh+1, ah+1)] = Esh∼µf′Eµf|sh [f(xh, ah)− rh − f(xh, ah+1)]\n= ∑ s∈S µf ′(s) · Eµf|s [f(xh, ah)− rh − f(xh, ah+1)].\nWe define νh(·) and ξh(·) explicitly with dimension M = |S|: given f and f ′, we index the elements of νh(f\n′) and those of ξh(f) by s ∈ S, and let [νh(f ′)]s = µf ′(s), [ξh(f)]s = Eµf|s [f(xh, ah)− rh− f(xh, ah+1)]. ζ = 2 √ M follows from the fact that ‖νh(f ′)‖1 = 1 and ‖ξh(f)‖∞ ≤ 2."
    }, {
      "heading" : "B.4 Predictive State Representations",
      "text" : "In this subsection we state and prove the formal version of Proposition 5. We first recall the definitions and some basic properties of PSRs, which can be found in Singh et al. [2004], Boots et al. [2011]. Consider dynamical systems with discrete and finite observation space O and action space A. Such systems can be fully specified by moment matrices PT |H, where H is a set of histories (past events) and T is a set of tests (future events). Elements of T and H are sequences of alternating actions and observations, and the entry\nof PT |H indexed by t ∈ T on the row and τ ∈ H on the column is Pt|τ , the probability that the test t would succeed conditioned on a particular past τ . For example, if t = aoa′o′, success of t means seeing o and o′ in the next two steps after τ is observed, if interventions a and a′ were to be taken.\nAmong all such systems, we are concerned about those that have finite linear dimension, defined as supT ,H rank(PT |H). As an example, the linear dimension of a POMDP is bounded by the number of hiddenstates. Systems with finite linear dimension have many nice properties, which allow them to be expressed by compact models, namely PSRs. In particular, fixing any T and H such that rank(PT |H) is equal to the linear dimension (such (H, T ) are called core histories and core tests), we have:\n1. For any history τ ∈ (A×O)∗, the conditional predictions of core tests PT |{τ} (we will also write PT |τ ) is always a state, that is, a sufficient statistics of history. This gives rise to the name “predictive state representation”.\n2. Based on PT |τ , the conditional prediction of any test t can be computed from a PSR model, parame-\nterized by square matrices {Bao} and a vector b∞ with dimension |T |. Letting t(i) be the i-th (action, observation) pair in t, and |t| be the number of such pairs, the prediction rule is\nPt|τ = b > ∞Bt(|t|) · · ·Bt(1)PT |τ . (21)\nAnd these parameters can be computed as\nBao = PT ,ao,HP † T ,H , b > ∞ = P > HP † T ,H (22)\nwhere\n• PT ,H is a matrix whose element indexed by (t ∈ T , τ ∈ H) is Pτt|∅, where τt is the concatenation of τ and t and ∅ is the null history. • PH = P{∅},H. • PT ,ao,H = PT ,Hao , where Hao = {τao : τ ∈ H}.\nNow we are ready to state and prove the formal version of Proposition 5.\nProposition 10 (Formal version of Proposition 5). Consider a partially observable system with observation space O, and the induced CDP 〈X ,A, H, P 〉 with xh = (oh, h). To handle some subtleties, we assume that\n1. |O| <∞ (classical PSR results assume discrete observations).\n2. o1 is deterministic (PSR trajectories always start with an action), and rh is a deterministic function of oh+1 (reward is usually omitted or assumed to be part of the observation).\nIf the linear dimension of the original system is at most L, then with any F : X×A → [0, 1], this model admits a Bellman Factorization with M = LK. Assuming further that the PSR’s parameters are non-negative under some choice of core histories and tests (H, T ) of size |H| = |T | = L, then we have ζ ≤ 2K2L3 √ L/σ3min, where σmin is the minimal non-zero singular value of PT ,H.\nProof. For any f, f ′ ∈ F , h ∈ [H], define\n1. µf ′,h as the distribution vector over (a1, o2, . . . , oh−1, ah−1) ∈ (A×O)h−2×A induced by a1:h−1 ∼ πf ′ . (Recall that o1 is deterministic.)\n2. P2|h−1 as a moment matrix whose element with column index (oh, ah, oh+1) ∈ O ×A×O and row index (a1, o2, . . . , oh−1, ah−1) ∈ (A×O)h−2 ×A is\nP [oh, oh+1 ‖ ah−1, ah | a1, o2, . . . , oh−1].12\n12PSR literature often emphasizes the intervention aspect of the actions in tests via the uses “‖” symbol; mathematically they can be treated as the conditioning operator in most cases.\n3. Ff,h as a vector whose element indexed by (oh, ah, oh+1) ∈ O ×A×O is (recall that xh = (oh, h) and rh is function of oh+1)\n1[ah 6= πf (xh)] ( f(xh, ah)− rh − f(xh+1, πf (xh+1)) ) .\nFirst we verify that E(f, πf ′ , h) = µ>f ′,hP2|h−1Ff,h.\nTo show this, first observe that µ>f ′,hP2|h−1 is a row vector whose element indexed by (oh, ah, oh+1) is\nP [oh, oh+1 ‖ ah | a1:h−1 ∼ πf ′ ].\nMultiplied by Ff,h, we further get\nE[f(xh, ah)− rh − f(xh+1, πf (xh+1)) | a1:h−1 ∼ πf ′ , ah ∼ πf ] = E(f, πf ′ , h).\nNext, we explicit construct ξh(f) and νh(f ′) by factorizing P2|h−1 = P1 × P2, where both P1 and P2 have no dependence on either f or f ′. Recall that for PSRs, any history (a1, o2, . . . , oh−1) has a sufficient statistics PT |a1,o2,...,oh−1 , that is a vector of predictions over the selected core tests T conditioned on the observed history. P1 consists of row vectors of length LK, and for the row indexed by (a1, o2, . . . , oh−1, ah−1) the vector is\nPadah−1 ( P>T |a1,o2,...,oh−1 ) ,\nwhere Pada(·) is a function that takes a L-dimensional vector, puts it in the a-th block of a vector of length LK, and fills the remaining entries with 0.\nWe construct P2 to be a matrix whose column vector indexed by (oh, ah, oh+1) isB>a(1),ohB>ah,oh+1b∞. . . B> a(K),oh B>ah,oh+1b∞  , where A = {a(1), . . . , a(K)}. It is easy to verify that P2|h−1 = P1 × P2 by recalling the prediction rules of PSRs in Eq. (21):\nP [oh, oh+1 ‖ ah−1, ah | a1, o2, . . . , oh−1] = b>∞Bah,oh+1Bah−1,ohPT |a1,o2,...,oh−1 = P>T |a1,o2,...,oh−1(B > ah−1,oh B>ah,oh+1b∞).\nGiven this factorization, we can write\nE(f, πf ′ , h) = (µ>f ′,hP1)× (P2Ff,h).\nSo we let νh(f ′) = P>1 µf ′,h and ξh(f) = P2Ff,h. It remains to be shown that we can bound their norms. Notice that the entries of a state vector PT |(·) are predictions of probabilities, so ‖P1‖∞ ≤ 1. Since µf ′,h is a probability vector, its dot product with every column in P1 is bounded by 1, hence ‖νh(f ′)‖2 ≤ √ LK.\nAt last, we consider bounding the norm of P2Ff,h. We will upper bound each entry of P2Ff,h by providing an `1 bound on the row vectors of P2, and then applying the Hölder’s inequality with ‖Ff,h‖∞ ≤ 2. Since we assumed that all model parameters of the PSRs are non-negative, P2 is a non-negative matrix, and bounding the `1 norm of its row vectors is equivalent to bounding each entry of the vector P2 1, where 1 is an all-1 vector. This vector is equal to\nP2 1 =\n ∑ (oh,ah,oh+1) B> a(1),oh\nB>ah,oh+1b∞ . . .∑\n(oh,ah,oh+1) B> a(K),oh B>ah,oh+1b∞\n =  (∑ oh B> a(1),oh )(∑ (ah,oh+1) B>ah,oh+1 ) b∞\n. . .(∑ oh B> a(K),oh )(∑ (ah,oh+1) B>ah,oh+1 ) b∞.\n (23)\nSince we care about the `∞ norm of this vector, we can bound the `∞ norm of each component vector. Using the PSR learning equations, we have\n∑ a,o Bao = ∑ a,o PT ,ao,HP † T ,H = (∑ a,o PT ,ao,H ) P †T ,H.\nNote that for any fixed a = a(i), every entry of ∑ o PT ,ao,H is the probability that the event t ∈ T happens after h ∈ H happens with a one step delay in the middle, where a is intervened in that delayed time step. Such entries are predicted probabilities of events, hence lie in [0, 1]. Consequently, ‖ ∑ a,o PT ,ao,H‖∞ ≤ K, and\nwe can upper bound the matrix `2 norm by Frobenius norm: ‖ ∑ a,o PT ,ao,H‖2 ≤ ‖ ∑ a,o PT ,ao,H‖F ≤ KL. Hence, ∥∥∥∥∥∑ a,o Bao ∥∥∥∥∥ 2 ≤ ∥∥∥∥∥∑ a,o PT ,ao,H ∥∥∥∥∥ 2 · ∥∥∥P †T ,H∥∥∥ 2 ≤ KL/σmin.\nUsing a similar argument, for any fixed a = a(i), ‖ ∑ oBao‖2 ≤ L/σmin. We also recall the definition of b∞ and bound its norm similarly:\n‖b∞‖2 = ∥∥∥P>HP †T ,H∥∥∥ 2 ≤ √ L/σmin.\nFinally, we have\n‖P2 1‖∞ ≤ max a∈A ∥∥∥∥∥∥ (∑\noh\nB>a,oh ) ∑ (ah,oh+1) B>ah,oh+1  b∞ ∥∥∥∥∥∥ ∞\n(Eq. (23))\n≤ max a∈A ∥∥∥∥∥∥ (∑\noh\nB>a,oh ) ∑ (ah,oh+1) B>ah,oh+1  b∞ ∥∥∥∥∥∥\n2\n≤ ( max a∈A ∥∥∥∥∥∑ o Bao ∥∥∥∥∥ 2 )∥∥∥∥∥∑ a,o Bao ∥∥∥∥∥ 2 ‖b∞‖2 ≤ KL2 √ L/σ3min.\nSo each row of P2 has `1 norm bounded by the above expression. Applying Hölder’s inequality we have each entry of P2Ff,h bounded by 2KL 2 √ L/σ3min, hence ‖ξh(f)‖2 = ‖P2Ff,h‖2 ≤ 2L3K √ K/σ3min. Combined with the bound on ‖νh(f ′)‖2 the proposition follows."
    }, {
      "heading" : "B.5 Linear Quadratic Regulators",
      "text" : "In this subsection we prove that Linear Quadratic Regulators (LQR) (See e.g., Anderson and Moore [2007] for a standard reference) admit Bellman Factorization with low Bellman Rank. We study a finite-horizon, discrete-time LQR, governed by the equations:\nx1 = 0, xh+1 = Axh +Bah + h, and ch = x > hQxh + a > h ah + τh,\nwhere xh ∈ Rd, ah ∈ RK and the noise variables are centered with E[ h >h ] = Σ, and Eτ2h = σ2. We operate with costs ch and the goal is to minimize cumulative cost. We assume that all parameters A,B,Σ, Q, σ\n2 are bounded in spectral norm by some Θ ≥ 1, that λmin(B>B) ≥ κ > 0, and that Q is strictly positive definite. Other formulations of LQR include replace a>h ah in our cost with a > hRah for a positive definite matrix R, but this can be accounted for by a change of variables. Generalization to non-stationary parameters is straightforward.\nThis model describes an MDP with continuous state and action spaces, and the corresponding CDP has context space Rd× [H], although we will always explicitly write both parts of the context in this section. It is well known that in a discrete time LQR, the optimal policy is a non-stationary linear policy π?(x, h) = P?,hx [Anderson and Moore, 2007], where P?,h ∈ RK×d is a h-dependent control matrix. Moreover, if all of the\nparameters are known to have spectral norm bounded by Θ then the optimal policy has matrices with bounded spectral norm as well, as we will see in our proof.\nOur arguments for LQR use decoupled policy and value function classes as in Section 5.2. We use a policy class and value function class defined below for parameters B1, B2, B3 that we will set in the proof.\nΠ = {π~P : π~P (x, h) = Phx, ~P ∈ H∏ i=1 RK×d, ‖Ph‖2 ≤ B1}\nG = {f~Λ, ~O : f~Λ, ~O(x, h) = x >Λhx+Oh, ~Λ ∈ H∏ i=1 Rd×d, ‖Λh‖2 ≤ B2, ~O ∈ RH , |Oh| ≤ B3}\nThe policy class consists of linear non-stationary policies, while the value functions are nonstationary quadratics with constant offset.\nProposition 11 (Formal version of Proposition 6). Consider an LQR under the assumptions outlined above. Let G be a class of non-stationary quadratic value functions with offsets and let Π be a class of linear non-stationary policies, defined above. Then, at level h, for any (π, g) pair and any roll-in policy π′ ∈ Π, the average Bellman error can be written as\nE(g, π, π′, h) = 〈ξh(π, g), νh(π′)〉,\nwhere ν, ξ ∈ Rd2+1. If Π,G are defined as above with bounds B1, B2, B3 and if all problem parameters have spectral norm at most Θ, then\n‖ξh(π, g)‖22 ≤ d ( B2 + Θ +B 2 1 − (Θ + ΘB1)2B2 ) + 4B23 + d 2Θ2B22\n‖νh(π′)‖22 ≤ dH+1Θ(ΘB1)2H + 1.\nHence, the problem admits Bellman Factorization with Bellman Rank at most d2 +1 and ζ that is exponential in H but polynomial in all other parameters. Moreover, if we set B1, B2, B3 as,\nB1 = Θ 2/κ,B2 =\n( 6Θ6\nκ2\n)H Θ, B3 = ( 6Θ6\nκ2\n)H dHΘ2,\nthen the optimal policy and value function belong to Π,G respectively.\nWe prove the proposition in several components. First, we study the relationship between policies and value functions, showing that linear policies induce quadratic value functions. Then, we turn to the structure of the optimal policy, showing that it is linear. Next, we derive bounds on the parameters B1, B2, B3 which ensure that the optimal policy and value function belong to Π,G. Lastly, we demonstrate the Bellman Factorization.\nThe next lemma derives a relationship between linear policies and quadratic value functions.\nLemma 7. If π is a linear non-stationary policy, πh(x) = Pπ,hx, then V π(x, h) = x>Λπ,hx + Oπ,h where Λπ,h ∈ Rd×d depends only on π and h and Oπ,h ∈ R. These parameters are defined inductively by,\nΛπ,H = Q+ P > π,HPπ,H , Oπ,H = 0\nΛπ,h = Q+ P > π,hPπ,h + (A+BPπ,h) >Λπ,h+1(A+BPπ,h)\nOπ,h = tr(Λπ,h+1Σ) +Oπ,h+1,\nwhere we recall that Σ is the covariance matrix of the h random variables.\nProof. The proof is by backward induction on h, starting from level H. Clearly,\nV π(x,H) = x>Qx+ πH(x) >πH(x) = x >Qx+ x>P>π,HPπ,Hx , x >Λπ,Hx\nso V π(·, H) is a quadratic function.\nFor the inductive step, consider level h and assume that for all x, V π(x, h + 1) = x>Λπ,h+1x + Oπ,h+1. Then, expanding definitions,\nV π(x, h) = x>Qx+ πh(x) >πh(x) + Ex′∼(x,πh(x))V π(x′, h+ 1) = x>Qx+ x>P>π,hPπ,hx+ Ex′∼(x,πh(x)) [ (x′)>Λπ,h+1(x ′) +Oπ,h+1 ]\n= x>Qx+ x>P>π,hPπ,hx+ E h [ (Ax+Bπh(x) + h) >Λπ,h+1(Ax+Bπh(x) + h) +Oπ,h+1 ]\n= x>Qx+ x>P>π,hPπ,hx+ E h [ (Ax+BPπ,hx+ h) >Λπ,h+1(Ax+BPπ,hx+ h) +Oπ,h+1 ] = x>Qx+ x>P>π,hPπ,hx+ x >(A+BPπ,h)\n>Λπ,h+1(A+BPπ,h)x+ E h >h Λπ,h+1 h +Oπ,h+1 = x>Qx+ x>P>π,hPπ,hx+ x >(A+BPπ,h) >Λπ,h+1(A+BPπ,h)x+ tr(Λπ,h+1Σ) +Oπ,h+1\nThus, setting,\nΛπ,h = Q+ P > π,hPπ,h + (A+BPπ,h) >Λπ,h+1(A+BPπ,h)\nOπ,h = tr(Λπ,h+1Σ) +Oπ,h+1\nWe have shown that V π(x, h) is a quadratic function of x.\nThe next lemma shows that the optimal policy is linear.\nLemma 8. In an LQR, the optimal policy π? is a non-stationary linear policy given by π?(x, h) = P?,hx, with parameter matrices P?,h ∈ RK×d at each level h. The optimal value function V ? is a non-stationary quadratic function given by V ?(x, h) = x>Λ?,hx + O?,h with parameter matrix Λ?,h ∈ Rd×d and offset O?,h ∈ R. The optimal parameters are defined recursively by,\nP?,H = 0 Λ?,H = Q O?,H = 0\nP?,h = (I +B >Λ?,h+1B) −1B>Λ?,h+1A Λ?,h = Q+ P > ?,hP?,h + (A+BP?,h) >Λ?,h+1(A+BP?,h)\nO?,h = tr(Λ?,h+1Σ) +O?,h+1.\nProof. We will explicitly calculate the optimal policy π? and demonstrate that it is linear. Then we will instantiate these matrices in Lemma 7 to compute the optimal value function.\nFor the optimal policy, we use backward induction on H. At the last level, we have,\nπ?(x,H) = argmin a x>Qx+ a>a = 0.\nRecall that we are working with costs, so the optimal policy minimizes the expected cost. Thus P?,H = 0 ∈ RK×d and π?(x,H) is a linear function of x.\nPlugging into Lemma 7 the value function has parameters,\nΛ?,H = Q, O?,H = 0\nFor the induction step, assume that π?(x, h + 1) = P?,h+1x is linear and V ?(x, h + 1) is quadratic with\nparameter Λ?,h+1 0 and O?,h+1. We then have,\nπ?(x, h) = argmin a x>Qx+ a>a+ Ex′∼(x,a)V ?(x′, h+ 1)\n= argmin a\nx>Qx+ a>a+ E h(Ax+Ba+ h)>Λ?,h+1(Ax+Ba+ h) +O?,h+1\n= argmin a\na>(I +B>Λ?,h+1B)a+ 2〈Λ?,h+1Ax,Ba〉\nThis follows by applying definitions and eliminating terms that are independent of a. Since R,Λ?,h+1 0 by assumption and our inductive hypothesis, we can analytically minimize. Setting the derivative equal to zero gives,\na = (I +B>Λ?,h+1B) −1B>Λ?,h+1Ax\nThus P?,h = (I +B >Λ?,h+1B) −1B>Λ?,h+1A.\nAs a consequence, we can now derive bounds on the policy and value function parameters. Recall that we assume that all system parameters are bounded in spectral norm by Θ ≥ 1 and that (B>B)−1 has minimum eigenvalue at least κ.\nCorollary 1. With Θ and κ defined above, we have\n‖P?,h‖f ≤ Θ2\nκ , ‖Λ?,h‖ ≤\n( 6Θ6\nκ2\n)H−h Θ, |O?,h| ≤ (H − h) ( 6Θ6\nκ2\n)H−h dΘ2.\nProof. Again we proceed by backward induction, using Lemma 8. Clearly ‖P?,H‖F = 0, ‖Λ?,H‖F ≤ Θ, |O?,H | = 0.\nFor the inductive step we can actually compute P?,h without any assumption on Λ?,h+1, except for the fact that it is symmetric positive definite, which follows from Lemma 8. First, we consider just the matrix B>Λ?,h+1A. Diagonalizing Λ?,h+1 = U >DU where U is orthonormal and D is diagonal, gives,\nB>Λ?,h+1A = (UB) >D(UA) = (UB)>D(UB)(B>U>UB)−1(UB)>(UA)\n= (UB)>D(UB)(B>B)−1B>A = B>Λ?,h+1ΠBA\nHere ΠB = B(B >B)−1B> is an orthogonal projection operator. This derivation uses the fact that since (UB)>D has rows in the column space of UB, we can right multiply by the projector onto UB. We also use that U>U = I since U has orthonormal rows and columns.\nThus, by the submultiplicative property of spectral norm, we obtain\n‖(I +B>Λ?,h+1B)−1B>Λ?,h+1A‖2 ≤ ‖(I +B>Λ?,h+1B)−1B>Λ?,h+1B‖2‖(B>B)−1B>A‖2 ≤ ‖(B>B)−1B>A‖2 ≤ Θ2/κ\nHere κ is a lower bound on the minimum eigenvalue of B>B. Using this bound on ‖P?,h‖, we can now bound the optimal value function,\n‖Λ?,h‖ ≤ Θ + Θ4/κ2 + (Θ + Θ3/κ)2‖Λ?,h+1‖ ≤ 6Θ6/κ2‖Λ?,h+1‖\nThe last bound uses the fact we apply a bound for ‖Λ?,h+1‖2 that is larger than one, so the last term dominates. We also use the inequalities Θ2/κ ≥ 1 and Θ ≥ 1. This recurrence yields,\n‖Λ?,h‖2 ≤ ( 6Θ6\nκ2\n)H−h Θ.\nA naive upper bound on O?,h gives,\nO?,h ≤ ‖Λ?,h+1‖ tr(Σ) + |O?,h+1| ≤ (H − h) ( 6Θ6\nκ2\n)H−h dΘ2.\nThe final component of the proposition is to demonstrate the Bellman Factorization.\nProof of Proposition 11. Fix h and a value function g parametrized by matrices Λ and offset O at time h and Λ′, O′ at time h+ 1. Also fix π which uses operator Pπ at time h.\nE(π, g, π′, h) = Ex∼(π′,h)x>Λx+O − x>Qx− x>P>π Pπx− Ex′∼(x,π(x))(x′)>Λ′x′ +O′\n= Ex∼(π′,h)x>Λx+O − x>Qx− x>P>π Pπx− E (Ax+BPπx+ )>Λ′(Ax+BPπx+ ) +O′ = tr [( Λ−Q− P>π Pπ − (A+BPπ)>Λ′(A+BPπ) ) Ex∼(π′,h)xx> ] +O −O′ − tr(ΛΣ)\nThus we may write ξh(π, g) = vec(Λ−Q−P>π Pπ− (A+BPπ)>Λ′(A+BPπ)) in the first d2 coordinates and O−O′− tr(ΛΣ) in the last coordinate. We also write νh(π′) = vec(Ex∼(π′,h)xx>) in the first d2 coordinates and 1 in the last coordinate.\nThe norm bound on ξ is straightforward, since all terms in its decomposition have an exponential in H bound.\nFor ν, since the distribution is based on applying a bounded policy π′ at level h − 1 iteration, we can write x = Ax̃+BPπ′ x̃+ where x̃ is obtained by rolling in with π\n′ for h− 1 steps. If (π′, h− 1) denotes the distribution at the previous level, this gives,\n‖Ex∼(π′,h)xx>‖F ≤ ‖Σ‖F + tr ( (A+BP )>(A+BP )Ex̃∼(π′,h−1)x̃x̃> ) ≤ ‖Σ‖F + d(Θ + ΘB1)2‖Ex̃∼(π′,h−1)x̃x̃>‖F\nSince at level one we have that the norm is at most ‖Σ‖F , we obtain a recurrence which produces a bound, at level h of,\n‖Ex∼(π′,h)xx>‖F ≤ ‖Σ‖F h∑ i=1 di−1(Θ + ΘB1) 2(i−1) ≤ ‖Σ‖FHdH(ΘB1)2H\nif Θ, B1 ≥ 1, which is our regime of interest."
    }, {
      "heading" : "C Auxiliary Proofs of the Main Lemmas",
      "text" : "In this appendix we give the full proofs of the lemmas sketched in Section 6. Rather than directly analyze Olive and prove Theorem 1, we instead focus our analysis on the robust variant, Oliver, introduced in Section 5.4. Oliver (Algorithm 3) with parameters θ = 0 and η = 0 is precisely Olive, and the two analyses are identical. To avoid repetition, in this appendix we analyze Oliver (Algorithm 3) and prove the versions of the lemmas that can be used for Theorem 4. Readers can easily recover the detailed proofs of the lemmas in Section 6 for Olive by letting θ = 0, η = 0, ′ = , f?θ = f ?, V ?F,θ = V ? F .\nTo facilitate understanding we break up the proofs into 3 parts. The main proofs appear in C.1, and two types of technical lemmas will be invoked from there: (1) a series of lemmas that adapt the work of Todd [1982] for our purpose, which are given in C.2; (2) deviation bounds, which are given in C.3."
    }, {
      "heading" : "C.1 Main Proofs",
      "text" : "Lemma (Restatement of Lemma 1 from main text for convenience) With Vf = E[f(x1, πf (x1))], we have\nVf − V πf = H∑ h=1 E(f, πf , h). (24)\nProof. Recall from Definition 2 that the average Bellman errors are defined as E(f, π, h) = E [ f(xh, ah)− rh − f(xh+1, ah+1) ∣∣ a1:h−1 ∼ π, ah:h+1 ∼ πf ]. Expanding RHS of Eq. (24), we get\nH∑ h=1 E [ f(xh, ah)− rh − f(xh+1, ah+1) ∣∣ a1:h−1 ∼ πf , ah:h+1 ∼ πf ]. Since all H expected values share the same distribution over trajectories, which is the one induced by a1:H ∼ πf , the above expression is equal to\nH∑ h=1 E [ f(xh, ah)− rh − f(xh+1, ah+1) ∣∣ a1:H ∼ πf ] = E\n[ H∑ h=1 ( f(xh, ah)− rh − f(xh+1, ah+1) ) ∣∣ a1:H ∼ πf] = E [ f(x1, πf (x1)) ] − E [ rh ∣∣ a1:H ∼ πf ] = Vf − V πf .\nLemma 9 (Optimism drives exploration, analog of Lemma 2). If the estimates V̂f and Ẽ(ft, πt, h) in Line 3 and 8 of Algorithm 3 always satisfy\n|V̂f − Vf | ≤ ′/8, |Ẽ(ft, πt, h)− E(ft, πt, h)| ≤ ′\n8H (25)\nthroughout the execution of the algorithm (recall that ′ is defined on Line 1), and f?θ is never eliminated, then in any round t, either the algorithm does not terminate and\nE(ft, πt, ht) ≥ ′\n2H , (26)\nor the algorithm terminates and the output policy πt satisfies V πt ≥ V ?F,θ − ′ −Hθ.\nProof. Eq. (26) follows directly from the termination criterion and Eq. (25). Suppose the algorithm terminates in round t. Let fmax := argmaxf∈Ft−1 Vf , and we have\nV πt = Vft − H∑ h=1 E(ft, πt, h) (Lemma 1)\n≥ V̂ft − H∑ h=1 Ẽ(ft, πt, h)− ′/4 (Eq. (25)) ≥ V̂ft − 7 ′/8 (termination criterion) ≥ V̂fmax − 7 ′/8 (ft is the maximizer of V̂f ) ≥ Vfmax − ′ ≥ Vf?θ − ′ (f?θ is not eliminated) ≥ V ?F,θ −Hθ − ′. (Lemma 1)\nThe last inequality uses Lemma 1 on Vf?θ and the definition of V ? F,θ, which is the reward for policy πf?θ . Lemma 1 relates these two quantities to the average Bellman errors, which, since f?θ is θ-valid are each upper bounded by θ.\nLemma 10 (Volumetric argument, analog of Lemma 3). If Ê(f, πt, ht) in Eq. (12) always satisfies\n|Ê(f, πt, ht)− E(f, πt, ht)| ≤ φ (27)\nthroughout the execution of the algorithm (φ is the threshold in the elimination criterion), then f?θ is never eliminated. Furthermore, for any particular level h, if whenever ht = h, we have\n|E(ft, πt, ht)| ≥ 3 √ M(2φ+ θ + η) + η, , (28)\nthen the number of rounds that ht = h is at most\nM log ζ\n2φ / log\n5 3 . (29)\nProof. The first claim that f?θ is never eliminated follows directly from the fact |E(f?θ , πt, ht)| ≤ θ (Definition 8), Eq. (27), and the elimination threshold φ+ θ. Below we prove the second claim.\nFor any particular level h, suppose i1 < · · · < iτ < · · · < iTh are the round indices with ht = h, {t : ht = h} ordered from first to last, and Th = |{t : ht = h}|. For convenience define i0 = 0. Our goal is to prove an upper bound on Th.\nDefine notations:\n• p1, . . . , pTh . pτ := νh(fiτ ) where νh(·) is given in Definition 10. Recall that fiτ is the optimistic function used for exploration in round t = iτ .\n• U(Fi0),U(Fi1), . . . ,U(FiTh ). U(Fiτ ) = {ξh(f) : f ∈ Fiτ } where ξh(f) ∈ R M is given in Definition 10.\n• Ψ = supf∈F ‖νh(f)‖2, and Φ = supf∈F ‖ξh(f)‖2. By Definition 10, Ψ · Φ ≤ ζ.\n• V0, V1, . . . , VTh . V0 := {v : ‖v‖2 ≤ Φ}, and Vτ := {v ∈ Vτ−1 : |p>τ v| ≤ 2φ+ θ + η}.\n• B0, B1, . . . , BTh . Bτ is a minimum volume enclosing ellipsoid (MVEE) of Vτ .\nFor every τ = 0, . . . , Th, we first show that U(Fiτ ) ⊆ Vτ . When τ = 0 this is obvious. For τ ≥ 1, we have ∀f ∈ Fiτ , |E(f, πfiτ , h)| ≤ 2φ+ θ. by the elimination criterion and Eq. (27). By Definition 10, this implies that, ∀v ∈ U(Fiτ ),\n|p>τ v| ≤ 2φ+ θ + η,\nso U(Fiτ ) ⊆ Vτ . Next we show that ∃v ∈ Vτ−1 such that |p>τ v| ≥ 3 √ M(2φ + θ + η). In fact, Eq. (28) and the fact that fit was chosen (implying that it survived) implies that this v can be chosen as\nv = ξh(fiτ ) ∈ U(Fiτ−1) ⊆ U(Fiτ−1) ⊆ Vτ−1.\n(The first “⊆” follows from the fact that Ft shrinks monotonically in Algorithm 3, since the learning steps between t = iτ−1 + 1 and t = iτ − 1 on other levels can only eliminate functions.) We verify that this v satisfies the desired property, given by Definition 10 and Eq. (28):\n|p>τ v| = |〈νh(fiτ ), ξh(fiτ )〉| ≥ |E(fiτ , πiτ , h)| − η ≥ 3 √ M(2φ+ θ + η).\nNow we apply Lemma 11 and Fact 4 with the variables set to d := M,B := Bτ−1, κ := 3 √ M(2φ + θ +\nη), γ := 2φ+ θ + η. We obtain that vol(B+)\nvol(Bt−1) ≤ 0.6,\nwhere B+ is the MVEE of V ′τ := {v ∈ Bτ−1 : |p>τ v| ≤ 2φ + θ + η}. Note that Vτ = {v ∈ Vτ−1 : |p>τ v| ≤ 2φ+ θ+ η} ⊆ V ′τ given that Vτ−1 ⊆ Bτ−1. Since B+ is an enclosing ellipsoid of Vτ , and Bτ is the MVEE of Vτ , we have vol(Bτ ) ≤ vol(B+). Altogether we claim that\nvol(Bτ )\nvol(Bτ−1) ≤ 0.6.\nThis result shows that the volume of Bτ shrinks exponentially with τ . To prove that Th is small, it suffices to show that the volume of B0 is not too large, and that of BTh is not too small. Let cM be the volume of Euclidean sphere with unit radius in RM . By definition, vol(B0) = cM (Φ)M .\nFor vol(BTh), since ‖pτ‖2 ≤ Ψ always holds, we can guarantee that\nVT ⊇ q ∈ RM : ⋂ p∈RM :‖p‖2≤Ψ |〈p, q〉| ≤ 2φ+ θ + η  ⊇ { q ∈ RM : ‖q‖2 ≤ (2φ+ θ + η)/Ψ } (Hölder’s inequality)\n⊇ { q ∈ RM : ‖q‖2 ≤ 2φ/Ψ } .\nHence, vol(BTh) ≥ cM (2φ/Ψ) M , and\ncM (2φ/Ψ) M\ncM (Φ)M ≤ vol(BTh) vol(B0) = Th∏ t=1 vol(Bt) vol(Bt−1) ≤ 0.6Th .\nAlgebraic manipulations give\nM log\n( ΨΦ\n2φ\n) ≥ Th log 5\n3 .\nThe second claim of the lemma statement follows by recalling that ΨΦ ≤ ζ."
    }, {
      "heading" : "C.2 Lemmas for the volumetric argument",
      "text" : "We adapt the work of Todd [1982] to derive lemmas that we use in C.1. The main result of this section is Lemma 11. As this section focuses on generic geometric results, we adopt notation that is more standard for these arguments. The notation used in this section should not be confused with the rest of the paper.\nTheorem 6 (Theorem 2 of Todd [1982]). Define E = {w ∈ Rd : w>w ≤ 1} and Eβ = {w ∈ E : |e>1 w| ≤ β} for 0 < β ≤ d−1/2. The ellipsoid,\nE+ = {w ∈ Rd |w>(ρ(I − σe1e>1 ))−1w ≤ 1}, (30)\nis a minimum volume enclosing ellipsoid (MVEE) for Eβ if\nσ = 1− dβ2\n1− β2 and ρ = d(1− β2) d− 1 .\nFact 3. With E,E+, σ, ρ as in Theorem 6, we have\nVol(E+) Vol(E) = √ dβ\n( d\nd− 1\n)(d−1)/2 ( 1− β2 )(d−1)/2 . (31)\nProof. For convenience, let us define G = ρ(I − σe1e>1 ) so that E+ = {w ∈ Rd : w>G−1w ≤ 1}. Notice that E can be obtained from E+ by the affine transformation v = G\n−1/2w, which means that if w ∈ E+ then v = G−1/2w ∈ E. Via change of variables this implies that\nVol(E+)\nVol(E) = det(G1/2).\nThe determinant is simply the product of the eigenvalues, which is easy to calculate since G is diagonal,\ndet(G1/2) = ρ(d−1)/2(ρ(1− σ))1/2.\nPlugging in the definitions of ρ, σ from Theorem 6 proves the statement.\nLemma 11. Consider a closed and bounded set V ⊂ Rd and a vector p ∈ Rd. Let B be any enclosing ellipsoid of V . Suppose there exists v ∈ V with |p>v| ≥ κ and define B+ as the minimum volume enclosing ellipsoid of {v ∈ B : |p>v| ≤ γ}. If γ/κ ≤ 1/ √ d, we have\nvol(B+) vol(B) ≤ √ d γ κ\n( d\nd− 1\n)(d−1)/2( 1− γ 2\nκ2\n)(d−1)/2 . (32)\nProof. The first claim is to prove a bound on p>Bp.\nκ ≤ |p>v| = |p>B1/2B−1/2v| ≤ √ p>Bp √ v>B−1v ≤ √ p>Bp.\nThe last inequality applies since v ∈ B so that v>B−1v ≤ 1. Now we will proceed to work with the ellipsoids, let L = {v : |v>p| ≤ γ}. Set B+ = MVEE(B ⋂ L). We will apply two translations of the coordinate system so that B gets mapped to the unit ball and so that p gets mapped to αe1 (i.e. a scaled multiple of the first standard basis vector). The first translation is done by setting w = B−1/2v where w is in the new coordinate system and v is in the old coordinate system. Let p1 = B\n1/2p so that we can equivalently write L = {w : |w>p1| ≤ γ}. The second translation maps p1 to αe1 via a rotation matrix R such that RB1/2p = Rp1 = αe1. We also translate w to Rw but this doesn’t affect the now spherically symmetric ellipsoid, so we do not change the variable names.\nTo summarize, after applying the scaling and the rotation, we are interested in MVEE(I ⋂ {w : |w>e1| ≤\nγ/α}) and specifically, since volume ratios are invariant under affine transformation, we have\nVol(B+)\nVol(B) =\nVol(MVEE(I ⋂ {w : |w>e1| ≤ γ/α}))\nVol(I) .\nHere I is the unit ball (i.e. the ellipsoid with identity matrix). Further applying Fact 3, we obtain\nVol(B+) Vol(B) = √ d γ α\n( d\nd− 1\n)(d−1)/2( 1− γ 2\nκ2\n)(d−1)/2 .\nIt remains to lower bound α, which is immediate since\nα = ‖RB1/2p‖2 = ‖B1/2p‖2 ≥ κ.\nSubstituting this lower bound on α completes the proof.\nFact 4. When γ/κ = 1 3 √ d , the RHS of Eq. (32) is less than 0.6.\nProof. Plugging in the numbers, we have the RHS of Eq. (32) equal to\n1\n3\n( d\nd− 1 9d− 1 9d\n)(d−1)/2 = 1\n3\n( 1 +\n8\n9(d− 1)\n)9(d−1)/8 · 4/9 ≤ 1\n3 exp(4/9) ≤ 0.52.\nHere we used the fact that (1 + 1x ) x is monotonically increasing towards e on x ∈ [1,∞)."
    }, {
      "heading" : "C.3 Deviation Bounds",
      "text" : "In this section we prove the deviation bounds. Note that the statement of the lemmas in this section, which are for Oliver, coincide with those stated in Section 6 for Olive. This should not be surprising as the two algorithms draw data and estimate quantities in the same way.\nLemma 12 (Deviation Bound for V̂f ). With probability at least 1− δ,\n|V̂f − Vf | ≤ √ 1\n2nest log\n2N\nδ\nholds for all f ∈ F simultaneously. Hence, we can set nest ≥ 32 2 log 2N δ to guarantee that |V̂f − Vf | ≤ /8.\nProof. The bound follows from a straight-forward application of Hoeffding’s inequality and the union bound, and we only need to verify that the Vf is the expected value of the V̂f , and the range of the random variables is [0, 1].\nLemma 13 (Deviation Bound for Ẽ(ft, πt, h)). For any fixed ft, with probability at least 1− δ,\n|Ẽ(ft, πt, h)− E(ft, πt, h)| ≤ 3 √ 1\n2neval log\n2H\nδ\nholds for all h ∈ [H] simultaneously. Hence, for any neval ≥ 288H 2 2 log 2H δ , with probability at least 1− δ we have |Ẽ(ft, πt, h)− E(ft, πt, h)| ≤ 8H .\nProof. This bound is another straight-forward application of Hoeffding’s inequality and the union bound, except that the random variables that go into the average have range [−1, 2], and we have to realize that Ẽ(ft, πt, h) is an unbiased estimate of E(ft, πt, h).\nLemma 14 (Deviation Bound for Ê(f, πt, ht)). For any fixed πt and ht, with probability at least 1− δ,\n|Ê(f, πt, ht)− E(f, πt, ht)| ≤\n√ 8K log 2Nδ\nn + 2K log 2Nδ n\nholds for all f ∈ F simultaneously. Hence, for any n ≥ 32Kφ2 log 2N δ and φ ≤ 4, with probability at least 1− δ we have |Ê(f, πt, ht)− E(f, πt, ht)| ≤ φ.\nProof. We first show that Ê(f, πt, ht) is an average of i.i.d. random variables with mean E(f, πt, ht). We use µ as a shorthand for the distribution over trajectories induced by a1, . . . , aht−1 ∼ πt, ah ∼ unif(A), which is the distribution of data used to estimate Ê(f, πt, ht). On the other hand, let µ′ denote the distribution over trajectories induced by a1, . . . , aht−1 ∼ πt, ah ∼ πf . The importance weight used in Eq. (12) essentially converts the distribution from µ to µ′, hence the expected value of Ê(f, πt, ht) can be written as\nEµ [K1[ah = πf (xh)] (f(xh, ah)− rh − f(xh+1, πf (xh+1)))] = Eµ′ [f(xh, ah)− rh − f(xh+1, πf (xh+1))] = E(f, πt, ht).\nNow, we apply Bernstein’s inequality. We first analyze the 2nd-moment of the random variable. Defining y(xh, ah, rh, xh+1) = f(xh, ah)− rh − f(xh+1, πf (xh+1)) ∈ [−2, 1], the 2nd-moment is\nEµ [ (K1[ah = πf (xh)]y(xh, ah, rh, xh+1)) 2 ]\n= Pr µ\n[ah = πf (xh)] · Eµ [ (Ky(xh, ah, rh, xh+1)) 2 ∣∣ ah = πf (xh)]+ Pr µ [ah 6= πf (xh)] · 0\n≤ 1 K\nEµ [ K2 · 4 ∣∣ ah = πf (xh)] = 4K. Next we check the range of the centered random variable. The uncentered variable lies in [−2K,K], and the expected value is in [−2, 1], so the centered variable lies in [−2K − 1,K + 2] ⊆ [−3K, 3K]. Applying Bernstein’s inequality, we have with probability at least 1− δ,\n|Ê(f, πt, ht)− E(f, πt, ht)| ≤\n√ 2 Var [K1[ah = πf (xh)]y(xh, ah, rh, xh+1)] log 2N δ\nn + 6K log 2Nδ 3n\n≤\n√ 8K log 2Nδ\nn + 2K log 2Nδ n . (variance is bounded by 2nd-moment)\nAs long as 2K log 2Nδ n ≤ 1, the above is bounded by 2 √ 8K log 2Nδ n . The choice of n follows from solving\n2\n√ 8K log 2Nδ\nn = φ for n, which indeed guarantees that 2K log 2Nδ n ≤ 1 as φ ≤ 4."
    }, {
      "heading" : "D Proofs of Extensions",
      "text" : ""
    }, {
      "heading" : "D.1 Proof for unknown Bellman Rank (Theorem 2)",
      "text" : "Since we assign δi(i+1) failure probability to the i-th call of Algorithm 2, the total failure probability is at most\n∞∑ i=1\nδ\ni(i+ 1) = δ ∞∑ i=1 ( 1 i − 1 i+ 1 ) = δ.\nSo with probability at least 1 − δ, all high probability events in the analysis of Olive occur for every i = 1, 2, . . .. Note that regardless of whether M ′ < M , we never eliminate f? according to Lemma 3. Hence Lemma 2 holds and whenever the algorithm returns a policy it is near-optimal.\nWhile the algorithm returns a near-optimal policy if it terminates, we still must prove that the algorithm will terminate. Since when M ′ < M Eq. (20) and Lemma 10 do not apply, we cannot naively use arguments from the analysis of Olive. However, we monitor the number of rounds that have passed in each execution to Olive and stop the subroutine when the actual number of rounds exceeds the iteration complexity bound (Lemma 3) to prevent wasting more samples on the wrong M ′.\nOlive is guaranteed to terminate within the sample complexity bound and output near-optimal policy when M ≤ M ′. Since M ′ grows on a doubling schedule, for the first M ′ that satisfies M ≤ M ′, we have M ′ ≤ 2M and i ≤ log2M + 1. Hence, the total number of calls is bounded by log2M + 1.\nFinally, since the sample complexity bound in Theorem 1 is monotonically increasing in M and 1/δ and our schedule for δ′ is increasing, we can bound the total sample complexity by that of the last call to Olive multiplied by the number of calls. The last call to Olive has M ′ ≤ 2M , and i(i+1)δ ≤ (log2 M+2)(log2 M+1)\nδ , so the sample complexity bound is only affected by factors that are at most logarithmic in the relevant parameters.\nD.2 Proofs for infinite hypothesis classes\nIn this section we prove sample complexity guarantee for using infinite hypothesis classes in Section 5.3. Recall that we are working with separated policy class Π and V-value function class G, and when running Olive any occurrence of f ∈ F should be replaced appropriately by (π, g) ∈ Π×G. For clarity, we use (π, g) instead of f in the derivations in this section. We assume that the two function classes have finite Natarajan dimension and Pseudo-dimension respectively.\nThe key technical step for the sample complexity guarantee is to establish the necessary deviation bounds for infinite classes. Among these deviation bounds, the bound on Ẽ((πt, gt), πt, h) (Lemma 5) does not involve union bound over F , so it can be reused without modification. The other two bounds need to be replaced by Lemma 15 and 16, stated below. With these lemmas, Theorem 3 immediately follows simply by replacing the deviation bounds.\nDefinition 11. Define dΠ = max(Ndim(Π), 6), dG = max(Pdim(G), 6), and d = dΠ + dG.\nLemma 15. If\nnest ≥ 8192\n2\n( dG log 128e + log(8e(dG + 1)) + log 1\nδ\n) , (33)\nthen with probability at least 1− δ, |V̂(π,g) − V(π,g)| ≤ /8, ∀(π, g) ∈ Π× G.\nWe remark that both the estimate V̂(π,g) and population quantity V(π,g) are independent of π in the separable case, and hence the sample complexity is independent of dΠ.\nLemma 16. If\nn ≥1152K 2\nφ2\n( 6d log (2eKd) log 48eK\nφ + log\n( 8e(6d log (2eKd) + 1) ) + log 3\nδ\n) , (34)\nthen for any fixed πt and ht, with probability at least 1− δ,\n|Ê((π, g), πt, ht)− E((π, g), πt, ht)| ≤ φ, ∀(π, g) ∈ Π× G.\nProof of Theorem 3. Set the algorithm parameters to:\nφ = 12H √ M , nest =\n8192\n2\n( dG log 128e + log(8e(dG + 1)) + log 3\nδ\n) ,\nneval = 288H2\n2 log\n( 12H2M log(6H √ Mζ/ )\nδ\n) ,\nn = 1152K2\nφ2\n( 6d log ( 2eK ) log 48eK\nφ + log\n( 8e(6d log (2eKd) + 1) ) + log 18HM log(6H √ Mζ/ )\nδ\n) .\nThe rest of the proof is essentially the same as the proof of Theorem 1, and the sample complexity follows by noticing that nest = Õ(dG+log(1/δ) 2 ) and n = Õ(K 2(dΠ + dG + log(1/δ))/φ 2).\nLemma 15 is a straight-forward application of Corollary 2 introduced in D.2.1 and will not be proved separately. The remainder of this section, we prove Lemma 16. Before that, we review some standard definitions and results from statistical learning theory."
    }, {
      "heading" : "D.2.1 Definitions and Basic Lemmas",
      "text" : "Notations X , x, n, d, ξ in this section are used according to conventions in the literature and may not share semantics with the same symbols used elsewhere in this paper.\nDefinition 12 (VC-Dimension). Given hypothesis class H ⊂ X → {0, 1}, its VC-dimension VC-dim(H) is defined as the maximal cardinality of a set X = {x1, . . . , x|X|} ⊂ X that satisfies |HX | = 2|X| (or X is shattered by H), where HX is the restriction of H to X, namely {(h(x1), . . . , h(x|X|)) : h ∈ H}.\nLemma 17 (Sauer’s Lemma). Given hypothesis class H ⊂ X → {0, 1} with d = VC-dim(H) <∞, we have ∀X = (x1, . . . , xn) ∈ Xn,\n|HX | ≤ (n+ 1)d.\nLemma 18 (Sauer’s Lemma for Natarajan dimension [Ben-David et al., 1992, Haussler and Long, 1995]). Given hypothesis class H ⊂ X → Y with Ndim(H) ≤ d, we have ∀X = (x1, . . . , xn) ∈ Xn,\n|HX | ≤ ( ne(K + 1)2\n2d\n)d ,\nwhere K = |Y|.\nDefinition 13 (Covering number). Given hypothesis class H ⊂ X → R, > 0, X = (x1, . . . , xn) ∈ Xn, the covering number N1(α,H, X) is defined as the minimal cardinality of a set C ⊂ Rn, such that for any h ∈ H there exists c = (c1, . . . , cn) ∈ C where 1n ∑n i=1 |h(xi)− ci| ≤ α.\nLemma 19 (Bounding covering number by pseudo-dimension [Haussler, 1995]). Given hypothesis class H ⊂ X → R with Pdim(H) ≤ d, we have for any X ∈ Xn,\nN1(α,H, X) ≤ e(d+ 1) ( 2e\nα\n)d .\nLemma 20 (Uniform deviation bound using covering number [Pollard, 1984]; also see Devroye et al. [1996], Theorem 29.1). Let H ⊂ X → [0, b] be a hypothesis class, and (x1, . . . , xn) be i.i.d. samples drawn from some distribution supported on X . For any α > 0,\nPr { sup h∈H ∣∣∣∣∣ 1n n∑ i=1 h(xi)− E[h(x1)] ∣∣∣∣∣ > α } ≤ 8E [ N1 ( α/8,H, (x1, . . . , xn) )] exp ( − nα 2 128b2 ) .\nCorollary 2 (Uniform deviation bound using pseudo-dimension). Suppose Pdim(H) ≤ d, then\nPr { sup h∈H ∣∣∣∣∣ 1n n∑ i=1 h(xi)− E[h(x1)] ∣∣∣∣∣ > α } ≤ 8e(d+ 1) ( 16e α )d exp ( − nα 2 128b2 ) .\nTo guarantee that this probability is upper bounded by δ, it suffices to have\nn ≥ 128 α2\n( d log 16e\nα + log(8e(d+ 1)) + log\n1\nδ\n) ."
    }, {
      "heading" : "D.2.2 Proof of Lemma 16",
      "text" : "The idea is to establish deviation bounds for each of the three terms in the definition of Ê((π, g), πt, ht) (Eq. (6)). Each term takes the form of an importance weight multiplied by a real-valued function, and we first show that the function space formed by these products has bounded pseudo-dimension. We state this supporting lemma in terms of an arbitrary value-function class V which might operate on an input space X ′ different from the context space X . In the sequel, we will instantiate V and X ′ in the the lemma with specific choices to prove the desired results.\nLemma 21. Let Y be a label space with |Y| = K, let Π ⊆ X → Y be a function class with Natarajan dimension at most dΠ ∈ [6,∞), and let V ⊆ X ′ → [0, 1] be a class with pseudo-dimension at most dV ∈ [6,∞). The hypothesis class H = {(x, a, x′) 7→ 1[a = π(x)]g(x′) : π ∈ Π, g ∈ V} has pseudo-dimension Pdim(H) ≤ 6(dΠ + dV) log (2eK(dΠ + dV)).\nProof. Recall that Pdim(H) = VC-dim(H+), so it suffices to show that for any X = {(x1, a1, x′1, ξ1), . . . , (xd, ad, x′d, ξd)} ∈ (X × A × X ′ × R)d where d = 6(dΠ + dV) log (2eK(dΠ + dV)), |H+X | < 2d. Note that since g(x) ∈ [0, 1] for all g, x\nH+ = {(x, a, x′, ξ) 7→ 1 [ 1[a = π(x)]g(x′) > ξ ] }\n= {(x, a, x′, ξ) 7→ 1[ξ < 0] + 1[ξ ≥ 0] · 1[a = π(x)] · 1[g(x′) > ξ]}\nFor points where ξi < 0, all hypotheses in H+ produce label 1, so without loss of generality we can assume that ξi ≥ 0, i = 1, . . . , d.\nWith a slight abuse of notation, let ΠX denote the restriction of Π to the set of contexts {x1, . . . , xd} (actions and future contexts (a1, x ′ 1), . . . , (ad, x ′ d) are ignored since Π does not operate on them), and V + X denote the restriction of V+ to {(x′1, ξ1), . . . , (x′d, ξd)}. H + X can be produced by the Cartesian product of ΠX and V+X as follows:\nH+X = {(1[a1 = α1]β1, . . . ,1[ad = αd]βd) : (α1, . . . , αd) ∈ ΠX , (β1, . . . , βd) ∈ V + X}.\nTherefore, |H+X | ≤ |ΠX | |V + X |. Recall that Ndim(Π) ≤ dΠ and VC-dim(V+) = Pdim(V) ≤ dV . Applying Lemma 18 and 17:\n|H+X | ≤ ( de(K + 1)2\n2dΠ\n)dΠ (d+ 1)dV .\nThe logarithm of the RHS is\ndΠ log\n( de(K + 1)2\n2dΠ\n) + dV log(d+ 1) < dΠ log(de(K + 1) 2) + dV log(d+ 1)\n≤ dΠ log d+ 2dΠ log(2eK) + dV log(d+ 1) ≤ 2(dΠ + dV) log(2eK) + (dΠ + dV) log(2d).\nIt remains to be shown that this is less than log(2d) = d log 2. Note that\nd log 2 > 3(dΠ + dV)(log(2eK) + log(dΠ + dV)),\nso we only need to show that (dΠ + dV) log(2d) ≤ (dΠ + dV) log(2eK) + 3(dΠ + dV) log(dΠ + dV). Now (dΠ + dV) log(2d) = (dΠ + dV) ( log(12(dΠ + dV)) + log log(2eK(dΠ + dV)) )\n≤ 2(dΠ + dV) log(dΠ + dV) + (dΠ + dV) log ( log(2eK) + log(dΠ + dV) )\n(dΠ + dV ≥ 12) ≤ 2(dΠ + dV) log(dΠ + dV) + (dΠ + dV) ( log(2eK) + log(dΠ + dV) ) .\nProof of Lemma 16. Recall that when we are given a policy class Π and separate V-value function class G, for every π ∈ Π, g ∈ G, we instead estimate average Bellman error with\nÊ((π, g), πt, ht) = 1\nn n∑ i=1 1[a (i) ht = π(x (i) ht )] 1/K ( g(x (i) ht )− r(i)ht − g(x (i) ht+1 ) ) .\nSo it suffices to show that the averages of 1[a (i) ht = π(x (i) ht )]g(x (i) ht ), 1[a (i) ht = π(x (i) ht )]r (i) ht , 1[a (i) ht = π(x (i) ht )]g(x (i) ht+1 ) are φ3K -close to their expectations with probability at least 1−δ/3, respectively. It turns out that, we can use Lemma 21 for all the three terms. For the first and the third terms, we apply Lemma 21 with V = G,X ′ = X , and obtain the necessary sample size directly from Corollary 2. For the second term, we apply Lemma 21 with V = {x 7→ x},X ′ = R. Note that in this case V is a singleton with the only element being the identity function over R, so it is clear that Pdim(V) < 6 ≤ dG , hence the sample size for the other two terms is also adequate for this term."
    }, {
      "heading" : "D.3 Proofs for OLIVER",
      "text" : "Recall that the main lemmas for analyzing Oliver have been proved in Appendix C.1, so below we directly prove Theorem 4.\nProof of Theorem 4. Suppose the preconditions of Lemma 9 (Eq. (25)) and Lemma 10 (Eq. (27)) hold; we will show them by invoking the deviation bounds later. By Lemma 9, when the algorithm terminates, the value of the output policy is at least\nV ?F,θ − ′ −Hθ.\nRecall that ′ = + 2H(3 √ M(θ + η) + η) (Line 1), so the suboptimality compared to V ?F,θ is at most\n+ 2H(3 √ M(θ + η) + η) +Hθ ≤ + 8H √ M(θ + η),\nwhich establishes the suboptimality claim. It remains to show the sample complexity bound. Applying Lemma 9, in every round t before the algorithm terminates,\nE(ft, πt, ht) ≥ ′\n2H = 2H + 3 √ M(θ + η) + η = 3 √ M(2φ+ θ + η) + η,\nthanks to the choice of φ and ′. For level h = ht, Eq. (28) is satisfied. According to Lemma 10, the event ht = h can happen at most M log ( ζ\n2φ ) / log 53 times for every h ∈ [H]. Hence, the total number of rounds\nin the algorithm is at most\nHM log\n( ζ\n2φ\n) / log 5\n3 = HM log\n( 6H √ Mζ ) / log 5\n3 .\nNow we are ready to apply the deviation bounds to show that Eq. (25) and 19 hold with high probability. We split the total failure probability δ among the following events:\n1. Estimation of V̂f (Lemma 12; only once): δ/3. 2. Estimation of Ẽ(ft, πt, h) (Lemma 13; every round): δ/ ( 3HM log ( 6H √ Mζ ) / log 53 ) .\n3. Estimation of Ê(f, πt, ht) (Lemma 14; every round): same as above.\nApplying Lemma 12, 13, 14 with the above failure probabilities, the choices of nest, neval, n in the algorithm statement satisfy the preconditions of Lemmas 9 and 10. In particular, the choice of nest and neval guarantee that |V̂f − Vf | ≤ /8 and |Ẽ(ft, πt, h) − E(ft, πt, h)| ≤ /8H, which are tighter than needed as ≤ ′ (only ′/8 and ′/8H are needed respectively, but tightening these bounds does not improve the sample complexity significantly, so we keep them the same as in Theorem 1 for simplicity). The remaining calculation of sample complexity is exactly the same as in the proof of Theorem 1."
    }, {
      "heading" : "Acknowledgements",
      "text" : "Part of this work was completed while NJ and AK were at Microsoft Research."
    } ],
    "references" : [ {
      "title" : "Contextual bandit learning with predictable rewards",
      "author" : [ "Alekh Agarwal", "Miroslav Dud́ık", "Satyen Kale", "John Langford", "Robert E. Schapire" ],
      "venue" : "In International Conference on Artificial Intelligence and Statistics (AISTATS),",
      "citeRegEx" : "Agarwal et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Agarwal et al\\.",
      "year" : 2012
    }, {
      "title" : "Taming the monster: A fast and simple algorithm for contextual bandits",
      "author" : [ "Alekh Agarwal", "Daniel Hsu", "Satyen Kale", "John Langford", "Lihong Li", "Robert E. Schapire" ],
      "venue" : "In International Conference on Machine Learning (ICML),",
      "citeRegEx" : "Agarwal et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Agarwal et al\\.",
      "year" : 2014
    }, {
      "title" : "Optimal control: linear quadratic methods",
      "author" : [ "Brian D.O. Anderson", "John B. Moore" ],
      "venue" : "Courier Corporation,",
      "citeRegEx" : "Anderson and Moore.,? \\Q2007\\E",
      "shortCiteRegEx" : "Anderson and Moore.",
      "year" : 2007
    }, {
      "title" : "Learning near-optimal policies with bellman-residual minimization based fitted policy iteration and a single sample path",
      "author" : [ "András Antos", "Csaba Szepesvári", "Rémi Munos" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Antos et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Antos et al\\.",
      "year" : 2008
    }, {
      "title" : "The nonstochastic multiarmed bandit problem",
      "author" : [ "Peter Auer", "Nicolo Cesa-Bianchi", "Yoav Freund", "Robert E. Schapire" ],
      "venue" : "SIAM Journal on Computing,",
      "citeRegEx" : "Auer et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Auer et al\\.",
      "year" : 2002
    }, {
      "title" : "Reinforcement learning of POMDPs using spectral methods",
      "author" : [ "Kamyar Azizzadenesheli", "Alessandro Lazaric", "Animashree Anandkumar" ],
      "venue" : "Conference on Learning Theory (COLT),",
      "citeRegEx" : "Azizzadenesheli et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Azizzadenesheli et al\\.",
      "year" : 2016
    }, {
      "title" : "Policy iteration based on stochastic factorization",
      "author" : [ "André da Motta Salles Barreto", "Joelle Pineau", "Doina Precup" ],
      "venue" : "Journal of Artificial Intelligence Research,",
      "citeRegEx" : "Barreto et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Barreto et al\\.",
      "year" : 2014
    }, {
      "title" : "Reinforcement Learning using Kernel-based Stochastic Factorization",
      "author" : [ "Andre S Barreto", "Doina Precup", "Joelle Pineau" ],
      "venue" : "In Advances in Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "Barreto et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Barreto et al\\.",
      "year" : 2011
    }, {
      "title" : "Unifying count-based exploration and intrinsic motivation",
      "author" : [ "Marc G. Bellemare", "Sriram Srinivasan", "Georg Ostrovski", "Tom Schaul", "David Saxton", "Remi Munos" ],
      "venue" : null,
      "citeRegEx" : "Bellemare et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Bellemare et al\\.",
      "year" : 2016
    }, {
      "title" : "Characterizations of learnability for classes of {0",
      "author" : [ "Shai Ben-David", "Nicolo Cesa-Bianchi", "Philip M. Long" ],
      "venue" : "n}-valued functions. In Conference on Learning Theory (COLT),",
      "citeRegEx" : "Ben.David et al\\.,? \\Q1992\\E",
      "shortCiteRegEx" : "Ben.David et al\\.",
      "year" : 1992
    }, {
      "title" : "Closing the learning-planning loop with predictive state representations",
      "author" : [ "Byron Boots", "Sajid M. Siddiqi", "Geoffrey J. Gordon" ],
      "venue" : "International Journal of Robotics Research,",
      "citeRegEx" : "Boots et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Boots et al\\.",
      "year" : 2011
    }, {
      "title" : "R-max-a general polynomial time algorithm for near-optimal reinforcement learning",
      "author" : [ "Ronen I. Brafman", "Moshe Tennenholtz" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Brafman and Tennenholtz.,? \\Q2003\\E",
      "shortCiteRegEx" : "Brafman and Tennenholtz.",
      "year" : 2003
    }, {
      "title" : "Sample complexity of episodic fixed-horizon reinforcement learning",
      "author" : [ "Christoph Dann", "Emma Brunskill" ],
      "venue" : "In Advances in Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "Dann and Brunskill.,? \\Q2015\\E",
      "shortCiteRegEx" : "Dann and Brunskill.",
      "year" : 2015
    }, {
      "title" : "A probabilistic theory of pattern recognition",
      "author" : [ "Luc Devroye", "László Györfi", "Gábor Lugosi" ],
      "venue" : null,
      "citeRegEx" : "Devroye et al\\.,? \\Q1996\\E",
      "shortCiteRegEx" : "Devroye et al\\.",
      "year" : 1996
    }, {
      "title" : "Efficient optimal learning for contextual bandits",
      "author" : [ "Miroslav Dudik", "Daniel Hsu", "Satyen Kale", "Nikos Karampatziakis", "John Langford", "Lev Reyzin", "Tong Zhang" ],
      "venue" : "In Uncertainty in Artificial Intelligence (UAI),",
      "citeRegEx" : "Dudik et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Dudik et al\\.",
      "year" : 2011
    }, {
      "title" : "Tree-based batch mode reinforcement learning",
      "author" : [ "Damien Ernst", "Pierre Geurts", "Louis Wehenkel" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Ernst et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Ernst et al\\.",
      "year" : 2005
    }, {
      "title" : "Error propagation for approximate policy and value iteration",
      "author" : [ "Amir-Massoud Farahmand", "Csaba Szepesvári", "Rémi Munos" ],
      "venue" : "In Advances in Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "Farahmand et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Farahmand et al\\.",
      "year" : 2010
    }, {
      "title" : "Decision theoretic generalizations of the PAC model for neural net and other learning applications",
      "author" : [ "David Haussler" ],
      "venue" : "Information and computation,",
      "citeRegEx" : "Haussler.,? \\Q1992\\E",
      "shortCiteRegEx" : "Haussler.",
      "year" : 1992
    }, {
      "title" : "Sphere packing numbers for subsets of the boolean n-cube with bounded vapnik-chervonenkis dimension",
      "author" : [ "David Haussler" ],
      "venue" : "Journal of Combinatorial Theory, Series A,",
      "citeRegEx" : "Haussler.,? \\Q1995\\E",
      "shortCiteRegEx" : "Haussler.",
      "year" : 1995
    }, {
      "title" : "A generalization of sauer’s lemma",
      "author" : [ "David Haussler", "Philip M. Long" ],
      "venue" : "Journal of Combinatorial Theory, Series A,",
      "citeRegEx" : "Haussler and Long.,? \\Q1995\\E",
      "shortCiteRegEx" : "Haussler and Long.",
      "year" : 1995
    }, {
      "title" : "The Malmo Platform for artificial intelligence experimentation",
      "author" : [ "Matthew Johnson", "Katja Hofmann", "Tim Hutton", "David Bignell" ],
      "venue" : "In International Joint Conference on Artificial Intelligence (IJCAI),",
      "citeRegEx" : "Johnson et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Johnson et al\\.",
      "year" : 2016
    }, {
      "title" : "Model-based exploration in continuous state spaces",
      "author" : [ "Nicholas K. Jong", "Peter Stone" ],
      "venue" : "In Abstraction, Reformulation, and Approximation,",
      "citeRegEx" : "Jong and Stone.,? \\Q2007\\E",
      "shortCiteRegEx" : "Jong and Stone.",
      "year" : 2007
    }, {
      "title" : "Exploration in metric state spaces",
      "author" : [ "Sham Kakade", "Michael Kearns", "John Langford" ],
      "venue" : "In International Conference on Machine Learning (ICML),",
      "citeRegEx" : "Kakade et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Kakade et al\\.",
      "year" : 2003
    }, {
      "title" : "Near-optimal reinforcement learning in polynomial time",
      "author" : [ "Michael Kearns", "Satinder Singh" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Kearns and Singh.,? \\Q2002\\E",
      "shortCiteRegEx" : "Kearns and Singh.",
      "year" : 2002
    }, {
      "title" : "A sparse sampling algorithm for near-optimal planning in large Markov decision processes",
      "author" : [ "Michael Kearns", "Yishay Mansour", "Andrew Y. Ng" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Kearns et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Kearns et al\\.",
      "year" : 2002
    }, {
      "title" : "Bandit based monte-carlo planning",
      "author" : [ "Levente Kocsis", "Csaba Szepesvári" ],
      "venue" : "In European Conference on Machine Learning (ECML),",
      "citeRegEx" : "Kocsis and Szepesvári.,? \\Q2006\\E",
      "shortCiteRegEx" : "Kocsis and Szepesvári.",
      "year" : 2006
    }, {
      "title" : "PAC reinforcement learning with rich observations",
      "author" : [ "Akshay Krishnamurthy", "Alekh Agarwal", "John Langford" ],
      "venue" : "In Advances in Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "Krishnamurthy et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Krishnamurthy et al\\.",
      "year" : 2016
    }, {
      "title" : "Least-squares policy iteration",
      "author" : [ "Michail G. Lagoudakis", "Ronald Parr" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Lagoudakis and Parr.,? \\Q2003\\E",
      "shortCiteRegEx" : "Lagoudakis and Parr.",
      "year" : 2003
    }, {
      "title" : "Finite-sample analysis of least-squares policy iteration",
      "author" : [ "Alessandro Lazaric", "Mohammad Ghavamzadeh", "Rémi Munos" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Lazaric et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Lazaric et al\\.",
      "year" : 2012
    }, {
      "title" : "A unifying framework for computational reinforcement learning theory",
      "author" : [ "Lihong Li" ],
      "venue" : "PhD thesis, Rutgers, The State University of New Jersey,",
      "citeRegEx" : "Li.,? \\Q2009\\E",
      "shortCiteRegEx" : "Li.",
      "year" : 2009
    }, {
      "title" : "Predictive representations of state",
      "author" : [ "Michael L. Littman", "Richard S. Sutton", "Satinder Singh" ],
      "venue" : "In Advances in Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "Littman et al\\.,? \\Q2001\\E",
      "shortCiteRegEx" : "Littman et al\\.",
      "year" : 2001
    }, {
      "title" : "Human-level control through deep reinforcement learning",
      "author" : [ "Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Andrei A Rusu", "Joel Veness", "Marc G Bellemare", "Alex Graves", "Martin Riedmiller", "Andreas K Fidjeland", "Georg Ostrovski", "Stig Petersen", "Charles Beattie", "Amir Sadik", "Ioannis Antonoglou", "Helen King", "Dharshan Kumaran", "Daan Wierstra", "Shane Legg", "Demis Hassabis" ],
      "venue" : "Nature,",
      "citeRegEx" : "Mnih et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Mnih et al\\.",
      "year" : 2015
    }, {
      "title" : "Error bounds for approximate policy iteration",
      "author" : [ "Rémi Munos" ],
      "venue" : "In International Conference on Machine Learning (ICML),",
      "citeRegEx" : "Munos.,? \\Q2003\\E",
      "shortCiteRegEx" : "Munos.",
      "year" : 2003
    }, {
      "title" : "Finite-time bounds for fitted value iteration",
      "author" : [ "Rémi Munos", "Csaba Szepesvári" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Munos and Szepesvári.,? \\Q2008\\E",
      "shortCiteRegEx" : "Munos and Szepesvári.",
      "year" : 2008
    }, {
      "title" : "On learning sets and functions",
      "author" : [ "Balas K. Natarajan" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Natarajan.,? \\Q1989\\E",
      "shortCiteRegEx" : "Natarajan.",
      "year" : 1989
    }, {
      "title" : "Model-based reinforcement learning and the eluder dimension",
      "author" : [ "Ian Osband", "Benjamin Van Roy" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Osband and Roy.,? \\Q2014\\E",
      "shortCiteRegEx" : "Osband and Roy.",
      "year" : 2014
    }, {
      "title" : "Some extensions of an inequality of Vapnik and Chervonenkis",
      "author" : [ "Dmitriy Panchenko" ],
      "venue" : "Electronic Communications in Probability,",
      "citeRegEx" : "Panchenko.,? \\Q2002\\E",
      "shortCiteRegEx" : "Panchenko.",
      "year" : 2002
    }, {
      "title" : "Efficient PAC-optimal exploration in concurrent, continuous state MDPs with delayed updates",
      "author" : [ "Jason Pazis", "Ronald Parr" ],
      "venue" : "In Conference on Artificial Intelligence (AAAI),",
      "citeRegEx" : "Pazis and Parr.,? \\Q2016\\E",
      "shortCiteRegEx" : "Pazis and Parr.",
      "year" : 2016
    }, {
      "title" : "Convergence of Stochastic Processes",
      "author" : [ "D Pollard" ],
      "venue" : null,
      "citeRegEx" : "Pollard.,? \\Q1984\\E",
      "shortCiteRegEx" : "Pollard.",
      "year" : 1984
    }, {
      "title" : "Mastering the game of Go with deep neural networks and tree search. Nature, 2016",
      "author" : [ "David Silver", "Aja Huang", "Chris J. Maddison", "Arther Guez", "Laurent Sifre", "George van den Driessche", "Julian Schrittwieser", "Ioannis Antonoglou", "Veda Penneershelvam", "Marc Lanctot", "Sander Dieleman", "Dominik Grewe", "John Nham", "Nal Kalchbrenner", "Ilya Sutskever", "Timothy Lillicrap", "Madeleine Leach", "Koray Kavukcuoglu", "Thore Graepel", "Demis Hassabis" ],
      "venue" : null,
      "citeRegEx" : "Silver et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Silver et al\\.",
      "year" : 2016
    } ],
    "referenceMentions" : [ {
      "referenceID" : 39,
      "context" : ", 2015] and Go [Silver et al., 2016] have sparked a flurry of research interest.",
      "startOffset" : 15,
      "endOffset" : 36
    }, {
      "referenceID" : 8,
      "context" : ", the use of pseudo-counts in Bellemare et al. [2016]), and combining MCTS with function approximation (e.",
      "startOffset" : 30,
      "endOffset" : 54
    }, {
      "referenceID" : 8,
      "context" : ", the use of pseudo-counts in Bellemare et al. [2016]), and combining MCTS with function approximation (e.g., Silver et al. [2016]).",
      "startOffset" : 30,
      "endOffset" : 131
    }, {
      "referenceID" : 8,
      "context" : ", the use of pseudo-counts in Bellemare et al. [2016]), and combining MCTS with function approximation (e.g., Silver et al. [2016]). Both types of approaches often require strong domain knowledge and large amounts of data to be successful. In this work, we study reinforcement learning settings where the agent receives rich sensory observations from the environment, forms complex contexts from these sensorimotor streams for learning and decisionmaking, and uses function approximation to generalize to unseen contexts. Our work departs from existing efforts by aiming at a sample complexity that depends neither on the number of unique contexts nor exponentially on the horizon. Similar goals have been attempted by Wen and Van Roy [2013] and Krishnamurthy et al.",
      "startOffset" : 30,
      "endOffset" : 742
    }, {
      "referenceID" : 8,
      "context" : ", the use of pseudo-counts in Bellemare et al. [2016]), and combining MCTS with function approximation (e.g., Silver et al. [2016]). Both types of approaches often require strong domain knowledge and large amounts of data to be successful. In this work, we study reinforcement learning settings where the agent receives rich sensory observations from the environment, forms complex contexts from these sensorimotor streams for learning and decisionmaking, and uses function approximation to generalize to unseen contexts. Our work departs from existing efforts by aiming at a sample complexity that depends neither on the number of unique contexts nor exponentially on the horizon. Similar goals have been attempted by Wen and Van Roy [2013] and Krishnamurthy et al. [2016] where attention is restricted to decision processes with special structures.",
      "startOffset" : 30,
      "endOffset" : 774
    }, {
      "referenceID" : 26,
      "context" : ", POMDPs with large observation spaces and reactive value functions [Krishnamurthy et al., 2016]) and “new” means that we provide the first sample efficient algorithm (e.",
      "startOffset" : 68,
      "endOffset" : 96
    }, {
      "referenceID" : 31,
      "context" : "In some application scenarios, partial observability can be resolved by using a small sliding window: for example, in Atari games, it is common to keep track of the last 4 frames of images [Mnih et al., 2015].",
      "startOffset" : 189,
      "endOffset" : 208
    }, {
      "referenceID" : 27,
      "context" : ", LSPI [Lagoudakis and Parr, 2003] and FQI [Ernst et al.",
      "startOffset" : 7,
      "endOffset" : 34
    }, {
      "referenceID" : 15,
      "context" : ", LSPI [Lagoudakis and Parr, 2003] and FQI [Ernst et al., 2005]), the Bellman errors are defined as taking the expectation of a squared error unlike in our definition.",
      "startOffset" : 43,
      "endOffset" : 63
    }, {
      "referenceID" : 26,
      "context" : "The result is due to Krishnamurthy et al. [2016], and we restate it here for completeness.",
      "startOffset" : 21,
      "endOffset" : 49
    }, {
      "referenceID" : 26,
      "context" : "Proposition 1 (Restatement of Proposition 2 in Krishnamurthy et al. [2016]).",
      "startOffset" : 47,
      "endOffset" : 75
    }, {
      "referenceID" : 26,
      "context" : "Since F provides no information other than the fact that the true MDP lies in this family, the problem is equivalent to identifying the best arm in a multi-arm bandit with K arms, and the remaining analysis follows exactly the same as in Krishnamurthy et al. [2016].",
      "startOffset" : 238,
      "endOffset" : 266
    }, {
      "referenceID" : 26,
      "context" : "This result subsumes and generalizes the setting of Krishnamurthy et al. [2016] which requires deterministic transitions in the underlying MDP.",
      "startOffset" : 52,
      "endOffset" : 80
    }, {
      "referenceID" : 20,
      "context" : "(d) A popular RL experiment setting [Johnson et al., 2016].",
      "startOffset" : 36,
      "endOffset" : 58
    }, {
      "referenceID" : 30,
      "context" : "Next, we consider Predictive State Representations (PSRs), which are alternative models of partially observable systems with parameters grounded in observable quantities [Littman et al., 2001].",
      "startOffset" : 170,
      "endOffset" : 192
    }, {
      "referenceID" : 29,
      "context" : "Our last example considers a class of linear control problems well studied in control theory, called Linear Quadratic Regulators (LQRs). In LQRs, we show that the Bellman Rank is bounded by the dimension of the state space. Exploration in this class of problems has been previously considered by Osband and Van Roy [2014]. Again for presentation purposes, we only state an informal result here and defer the formal statement to Appendix B.",
      "startOffset" : 101,
      "endOffset" : 322
    }, {
      "referenceID" : 26,
      "context" : "The most closely related result is the recent work of Krishnamurthy et al. [2016], who also consider episodic reinforcement learning with infinite observation spaces and function approximation.",
      "startOffset" : 54,
      "endOffset" : 82
    }, {
      "referenceID" : 3,
      "context" : ", the inherent Bellman error in [Antos et al., 2008]).",
      "startOffset" : 32,
      "endOffset" : 52
    }, {
      "referenceID" : 12,
      "context" : "Unfortunately, our sample complexity is polynomially worse than the state of the art Õ( 2 log(1/δ)) bounds for PAC-learning MDPs [Dann and Brunskill, 2015].",
      "startOffset" : 129,
      "endOffset" : 155
    }, {
      "referenceID" : 5,
      "context" : "Azizzadenesheli et al. [2016] provided a sample-efficient algorithm in a closely related setting, where both the observation space and the hidden-state space are small in cardinality.",
      "startOffset" : 0,
      "endOffset" : 30
    }, {
      "referenceID" : 0,
      "context" : "Finally, Contextual Decision Processes also encompass contextual bandits, where the optimal sample complexity is O(K log(N)/ ) [Agarwal et al., 2012].",
      "startOffset" : 127,
      "endOffset" : 149
    }, {
      "referenceID" : 4,
      "context" : "While no existing MDP lower bounds apply as is, since formulations vary, in Appendix A we adapt ideas from the literature [Auer et al., 2002] to obtain a Ω(MKH/ ) sample complexity lower bound for learning the MDPs in Example 1.",
      "startOffset" : 122,
      "endOffset" : 141
    }, {
      "referenceID" : 34,
      "context" : "Definition 6 (Natarajan dimension [Natarajan, 1989]).",
      "startOffset" : 34,
      "endOffset" : 51
    }, {
      "referenceID" : 17,
      "context" : "Definition 7 (Pseudo-dimension [Haussler, 1992]).",
      "startOffset" : 31,
      "endOffset" : 47
    }, {
      "referenceID" : 36,
      "context" : ", using tools from Panchenko [2002]).",
      "startOffset" : 19,
      "endOffset" : 36
    }, {
      "referenceID" : 11,
      "context" : "The result is similar in spirit to other lower bounds for PAClearning MDPs [Dann and Brunskill, 2015, Krishnamurthy et al., 2016], but we are not aware of any lower bound that applies directly to our setting. There are two main differences between our bound and the lower bound due to Dann and Brunskill [2015] for episodic MDPs.",
      "startOffset" : 76,
      "endOffset" : 311
    }, {
      "referenceID" : 11,
      "context" : "The result is similar in spirit to other lower bounds for PAClearning MDPs [Dann and Brunskill, 2015, Krishnamurthy et al., 2016], but we are not aware of any lower bound that applies directly to our setting. There are two main differences between our bound and the lower bound due to Dann and Brunskill [2015] for episodic MDPs. First, their bound assumes that the total reward is in [0, H], so the H dependence in the sample complexity is a consequence of scaling the rewards. Second, their MDP is not layered, but instead has M total states shared across all layers. In contrast, our process is layered with M distinct states per layer and total reward bounded in [0, 1]. Intuitively, our additional H dependence arises simply from having MH total states. At a high level, the proof is based on embedding Θ(MH) independent multi-arm bandit instances into a MDP and requiring that the algorithm identify the best action in Ω(MH) of them to produce a near-optimal policy. By appealing to a sample complexity lower bound for best arm identification, this implies that the algorithm requires Ω(MHK/ ) samples to identify a near-optimal policy. We rely on a fairly standard lower bound for best arm identification. We reproduce the formal statement from Krishnamurthy et al. [2016], although the proof is based on earlier lower bounds due to Auer et al.",
      "startOffset" : 76,
      "endOffset" : 1280
    }, {
      "referenceID" : 4,
      "context" : "[2016], although the proof is based on earlier lower bounds due to Auer et al. [2002].",
      "startOffset" : 67,
      "endOffset" : 86
    }, {
      "referenceID" : 29,
      "context" : "2 Generalization of Li [2009]’s setting Li [2009] considers the setting where the learner is given an abstraction φ that maps the large state space S in an MDP to some finite abstract state space S̄ in an MDP.",
      "startOffset" : 20,
      "endOffset" : 30
    }, {
      "referenceID" : 29,
      "context" : "2 Generalization of Li [2009]’s setting Li [2009] considers the setting where the learner is given an abstraction φ that maps the large state space S in an MDP to some finite abstract state space S̄ in an MDP.",
      "startOffset" : 20,
      "endOffset" : 50
    }, {
      "referenceID" : 29,
      "context" : "Proposition 8 (A generalization of [Li, 2009]’s setting).",
      "startOffset" : 35,
      "endOffset" : 45
    }, {
      "referenceID" : 10,
      "context" : "[2004], Boots et al. [2011]. Consider dynamical systems with discrete and finite observation space O and action space A.",
      "startOffset" : 8,
      "endOffset" : 28
    }, {
      "referenceID" : 2,
      "context" : "It is well known that in a discrete time LQR, the optimal policy is a non-stationary linear policy π(x, h) = P?,hx [Anderson and Moore, 2007], where P?,h ∈ RK×d is a h-dependent control matrix.",
      "startOffset" : 115,
      "endOffset" : 141
    }, {
      "referenceID" : 2,
      "context" : ", Anderson and Moore [2007] for a standard reference) admit Bellman Factorization with low Bellman Rank.",
      "startOffset" : 2,
      "endOffset" : 28
    }, {
      "referenceID" : 18,
      "context" : "Lemma 19 (Bounding covering number by pseudo-dimension [Haussler, 1995]).",
      "startOffset" : 55,
      "endOffset" : 71
    }, {
      "referenceID" : 38,
      "context" : "Lemma 20 (Uniform deviation bound using covering number [Pollard, 1984]; also see Devroye et al.",
      "startOffset" : 56,
      "endOffset" : 71
    }, {
      "referenceID" : 13,
      "context" : "Lemma 20 (Uniform deviation bound using covering number [Pollard, 1984]; also see Devroye et al. [1996], Theorem 29.",
      "startOffset" : 82,
      "endOffset" : 104
    } ],
    "year" : 2017,
    "abstractText" : "This paper studies systematic exploration for reinforcement learning with rich observations and function approximation. We introduce a new formulation, called contextual decision processes, that unifies and generalizes most prior settings. Our first contribution is a new complexity measure, the Bellman Rank , that we show enables tractable learning of near-optimal behavior in these processes and is naturally small for many well-studied reinforcement learning settings. Our second contribution is a new reinforcement learning algorithm that engages in systematic exploration to learn contextual decision processes with low Bellman Rank. The algorithm provably learns near-optimal behavior with a number of samples that is polynomial in all relevant parameters but independent of the number of unique observations. The algorithm uses Bellman error minimization with optimistic exploration and provides new insights into efficient exploration for reinforcement learning with function approximation.",
    "creator" : "LaTeX with hyperref package"
  }
}