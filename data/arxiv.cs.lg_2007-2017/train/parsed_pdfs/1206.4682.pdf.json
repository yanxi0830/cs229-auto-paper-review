{
  "name" : "1206.4682.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Copula-based Kernel Dependency Measures",
    "authors" : [ "Barnabás Póczos", "Zoubin Ghahramani", "Jeff Schneider" ],
    "emails" : [ "bapoczos@cs.cmu.edu", "zoubin@eng.cam.ac.uk", "schneide@cs.cmu.edu" ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "Measuring dependence between random variables is an important problem in statistics, information theory, and machine learning with a wide range of applications in science and engineering. The most well-known dependence measure is the Shannon mutual information, which has found numerous applications recently. Although this is the most popular dependence measure, it is only one of the many other existing ones. In particular, it is a special case of the Rényi-α (Rényi, 1961) and Tsallis-α mutual information (Tsallis, 1988).\nAppearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s).\nOther interesting dependence measures include the maximal correlation coefficient (Rényi, 1959), kernel mutual information (Gretton et al., 2003), the generalized variance and kernel canonical correlation analysis (Bach, 2002), the Hilbert-Schmidt independence criterion (Gretton et al., 2005), the Schweizer-Wolff measure (Schweizer & Wolff, 1981), and the distance based correlation (Székely et al., 2007).\nThere is a tremendous list of dependence applications. They have been used, for example, in causality detection, feature selection, active learning, structure learning, boosting, image registration, independent component and subspace analysis. For more applications and references, please see the supplementary material.\nOne reason why so many dependence measures have been defined in the literature is that the problem is challenging and researchers and practitioners are not satisfied with the available measures and estimators (Fernandes & Gloor, 2010). As Schweizer & Wolff (1981) formalized in their dependence axioms, a good dependence measure I has to have several properties. The most important ones are as follows. (i) Dependence I(X) is defined for X = (X1, . . . , Xd) ∈ Rd d-dimensional random variables. (ii) I(X1, . . . , Xd) is invariant to permutation. (iii) 0 ≤ I(X), and I(X) = 0 iff (X1, . . . , Xd) are independent variables. (iv) I(X1, . . . , Xd) is invariant to strictly increasing transformation of Xi variables. For more discussion on these axioms, see the Appendix. Among the above mentioned dependence measures, only the Rényi, Tsallis information, and the Schweizer-Wolff measure is invariant to strictly increasing transformations.\nIn addition to these constraints on the dependence measure, we also want an efficient estimator that is consistent, robust to outliers, has fast convergence rate, and can be used in high-dimensions too. De-\npendence estimation is very challenging, especially in nonparametric situations when we cannot assume that the observations have an underlying density function belonging to some parametric family. Many of the above mentioned dependence measures can be defined as some functionals of the density, thus an obvious way for their estimation would be to estimate the densities first. The density function, however, is a nuisance parameter in our case, and its estimation—especially in higher dimensions—is known to be very difficult.\nDue to these difficulties, all the existing dependence estimators have their own shortcomings. For example, the bound on the convergence rate of the Rényi and Tsallis information estimator (Pál et al., 2010) suffers from the curse of dimensionality. The available reproducing kernel based dependence measures are not invariant to strictly increasing transformation of the Xi marginal random variables. The estimator of Székely et al. (2007) is not robust; one single large enough outlier can arbitrarily ruin the estimator.\nThe main contributions of the paper are as follows. (i) We introduce a new dependency measure I that satisfies the above listed axioms. (ii) We prove that I can be efficiently estimated, and the calculation of the estimator is simple. The estimator is consistent, robust to outliers, and uses rank statistics only. (iii) We also provide an upper bound on the rate of convergence and derive a test of independence. This bound shows that the estimator can be efficiently used in large dimensions too.\nOur main idea is to combine empirical copula transformations with reproducing kernel based divergence estimators. We will show that the empirical copula transformation only slightly affects the convergence rate, but the resulting dependence estimator possesses all the above mentioned required properties. The proposed method is illustrated in Figure 1.\nOne might wonder why it is important for a dependence measure to be invariant to strictly increasing transformations of the marginal variables. One reason for this is that in many scenarios we need to compare the estimated dependencies. This is the case for example in feature selection and low-dimensional em-\nbedding of random variables. In these problems we can think of dependence as a “distance” between random variables in the sense that when the dependence is large, then the random variables are “close” to each other, and when the dependence is small, then the variables are far. However, if certain variables are measured on different scales, then this distance can be much different from the distance using other scales. As a result, it might happen that different features would be selected by the feature selection algorithm if we measured a quantity e.g. in grams, kilograms, pounds, or if we used log-scale. This is an odd situation that can be avoided with dependence measures that are invariant to strictly increasing transformations of the marginal variables. As an application, we will show how the proposed dependence measure can be used for feature selection and low-dimensional embedding of distributions.\nThe proofs can be found in the supplementary material. There we also discuss the robustness properties of the estimators and show how to use them in independence tests.\nNotation: In the rest of the paper X ∼ P will denote that the random variable X has distribution P . E(X) and σ(X) stand for the expectation and standard deviation of X, respectively. For a random variable X ∈ R, Ξ[X] denotes the standardized variable, that is, Ξ[X] . = (X−E[X])/σ(X), which has zero mean and unit variance. U [a, b] stands for the uniform distribution in the interval [a, b]. X1:m is shorthand notation for the set of random variables {X1, . . . , Xm}. The cardinality of a set S is denoted by |S|."
    }, {
      "heading" : "2. Maximum Mean Discrepancy",
      "text" : "In this section we review some important properties of the Maximum Mean Discrepancy (MMD), which is a quantity used to measure the distance between distributions (Borgwardt et al., 2006; Fortet & Mourier, 1953). An appealing property of this quantity is that it can be efficiently estimated from independent and identically distributed (i.i.d.) samples.\nDefinition 1. Let F be a class of functions, P , Q be probability distributions. The MMD between P and Q on the function class F is defined as follows,\nM[F , P,Q] .= sup f∈F (EX∼P [f(X)]− EY∼Q[f(Y)]).\nLet H = {f : X → R} be a reproducing kernel Hilbert Space (RKHS) with feature map ϕ(x) ∈ H (x ∈ X ), and kernel k(x, y) = ⟨ϕ(x), ϕ(y)⟩H. It is well known that ϕ(x) = k(·, x), and f(x) = ⟨f, ϕ(x)⟩H, which is called the reproducing property of the RKHS. Later we will also need the definition of universal kernels.\nDefinition 2 (Universal kernel). A kernel k : X × X → R is universal whenever the associated RKHS H is dense in C(X ), the space of bounded continuous functions over X , with respect to the L∞ norm.\nSteinwart (2001) has shown that the Gaussian and Laplace kernels are universal. Let µ[P ] . = EX∼P [ϕ(X)] = EX∼P [k(·,X)]. A sufficient condition for this quantity to exist is EX∼P,X′∼P k(X,X′) < ∞, where X and X′ are independent variables having distribution P .\nFor general F function sets,M[F , P,Q] can be difficult to calculate and is not even symmetric in P and Q. Nonetheless, when F is a unit ball of RKHSH, then for all f ∈ F we also have that−f ∈ F , which implies that M[F , P,Q] = M[F , Q, P ]. Furthermore, in this case M2[F , P,Q] has a simple form that makes efficient estimations possible. This is stated formally in the following lemma (Borgwardt et al., 2006).\nLemma 3. When F is a unit ball of RKHS H and µ[P ] < ∞, µ[Q] < ∞, then\nM2[F , P,Q] = ∥µ[P ]− µ[Q]∥2H = EX,X′∼P [k(X,X′)] − 2EX∼P,Y∼Q[k(X,Y)] + EY,Y′∼Q[k(Y,Y′)],\nwhere X and X′ have distribution P , Y and Y′ have distribution Q, and these random variables are all independent from each other.\nIn the remainder of the paper we will always assume that F is a unit ball of RKHS H. Let X1:m = (X1, . . . ,Xm) be an independent and identically distributed (i.i.d.) sample drawn from distribution P , and similarly let Y1:n = (Y1, . . . ,Yn) be an i.i.d. sample with distribution Q.\nA biased (but asymptotically unbiased) estimator for M[F , P,Q] can be easily given using the law of large numbers:\nMb[F ,X1:m,Y1:n] . =\n[ 1\nm2 m∑ i,j=1 k(Xi,Xj) (1)\n+ 1\nn2 n∑ i,j=1 k(Yi,Yj)− 2 mn m,n∑ i,j=1 k(Xi,Yj) ]1/2 .\nAn unbiased estimator for M2[F , P,Q] (when m = n) has also been derived in Borgwardt et al. (2006):\nM2u[F ,X1:m,Y1:m] = 1 m(m− 1) ∑ i,j h(Λi,Λj), (2) which is a one sample U -statistic with h(Λi,Λj) . = k(Xi,Xj)+ k(Yi,Yj)− k(Xi,Yj)− k(Xj ,Yi), where Λi . = (Xi,Yi), and Λ1:m = (Λ1, . . . ,Λm) are i.i.d. random variables. From the r.h.s. of Lemma 3, one can see that E[h(Λi,Λj)] = M2[F , P,Q], which proves the unbiasedness of the estimator M2u[F ,X1:m,Y1:m]."
    }, {
      "heading" : "3. The Copula of Distributions",
      "text" : "Below we review a few important properties of the copula of multivariate distributions that we will use in our work (Nelsen, 1998).\nThe copula plays an important role when we study the dependence among random variables. The marginal variables X1, . . . , Xd are independent from each other, if and only if the copula distribution is the multivariate uniform distribution. In turn, we can measure the dependence of the X1, . . . , Xd random variables by measuring how far the copula distribution is from the uniform distribution. The copula contains all the information that we need to measure dependence, and it is invariant to any nonlinear strictly increasing transformations of the marginal variables.\nThe copula can be defined by the Sklar’s theorem (Sklar, 1959) as follows. Let X = (X1, . . . , Xd) ∈ Rd be a random variable. Denote the marginal cdf’s of Xj by Fj : R → [0, 1]. Sklar’s theorem states that a multivariate cumulative distribution function H(x1, . . . , xd) = Pr(X\n1 ≤ x1, . . . , Xd ≤ xd) can be written as H(x1, . . . , xd) = C (F1(x1), . . . , Fd(xd)), where C is a unique distribution function on the range of the Fi cdf functions. This distribution function is called the copula of the joint distribution H. The distribution of the copula C is the same as the joint distribution of Z = (Z1, . . . , Zd) . = F(X) = (F1(X 1), . . . Fd(X\nd)) ∈ Rd random variables. When the Fi cumulative distribution functions are invertible, then F(X) have uniformly distributed marginal distributions on [0, 1], and the copula distribution can be calculated as C (y1, . . . , yd) = H ( F−11 (y1) , . . . , F −1 d (yd) ) , where 0 ≤ yi ≤ 1. The relation of the joint distribution H, marginal distributions Fi, and copula C is illustrated in Figure 2."
    }, {
      "heading" : "4. Dependence Estimation",
      "text" : "Let U = (U1, . . . , Ud) ∈ [0, 1]d be a random variable with uniform distribution on the d-dimensional unit cube, U ∼ U [0, 1]d. We define the dependence among continuous random variables X1, . . . , Xd as the MMD distance between the joint copula and the ddimensional uniform distribution:\nI(X1, . . . , Xd) . = M(F , PZ, PU).\nDefinition 4. Let x1, x2 ∈ R. A function g : R → R is strictly increasing, if g(x1) < g(x2) for all x1 < x2.\nIt is easy to see that I(X1, . . . , Xd) ≥ 0, and I(X1, . . . , Xd) = I(g1(X 1), . . . , gd(X d)) for any gi strictly increasing functions. In other words, I(X1, . . . , Xd) is invariant to strictly increasing transformations of the marginal variables.\nThe following lemma states that I(X1, . . . , Xd) is indeed a well-defined dependence measure when kernel k is universal.\nLemma 5. Let the kernel k be universal on [0, 1]d × [0, 1]d. Then I(X1, . . . , Xd) = 0, if and only if X1, . . . , Xd are independent of each other.\nIn what follows we will provide a consistent estimator for I(X) = I(X1, . . . , Xd). Let k : Rd × Rd → R be a kernel function of RKHS H, and let Z .= F(X) be a random variable drawn from the copula. Introduce the following terms:\nµ[PZ] . = EZ∼PZ [k((Z1, . . . , Zd), ·)], µ[PU] . = EU∼PU [k((U1, . . . , Ud), ·)].\nThanks to Lemma 3, it is easy to see that I2(X) = M2(F , PZ, PU) = ∥µ[PZ] − µ[PU]∥2H = EZ,Z′∼PZ [k(Z,Z′)] − 2EZ∼PZ,U∼PU [k(Z,U)] + EU,U′∼PU [k(U,U′)].\nOur goal is to estimate I(X) using the X1:m i.i.d. sample. This expression is the expected value of the kernel k evaluated in random variables drawn from the uniform and the copula distributions. Assume that we already have a Z1:m i.i.d. sample from the copula distribution. For simple kernel functions, the expectation w.r.t. the uniform distribution has a simple form. For example, when we use the Gaussian kernel, we have the following unbiased estimator for I2(X):\nM2u[F ,Z1:m, PU] = 1 m(m− 1) ∑ i ̸=j k(Zi,Zj)\n− 2 m m∑ i=1 d∏ j=1 ∫ 1 0 exp ( −(Zji − u) 2 2σ2 ) du\n+ (∫ 1 0 ∫ 1 0 exp ( −(u− u′)2 2σ2 ) dudu′ )d ,\nwhich can be expressed by the erf Gauss error function. For more complicated kernels, however, these integrals can not be calculated analytically, therefore we need to approximate them by sampling. In what follows we will investigate this case.\nLet U1:n = U1, . . . ,Un be an i.i.d. sample drawn from the U [0, 1]d distribution, and let X, X1,. . . ,Xm be i.i.d. samples having distribution PX. The F1, . . . , Fd distribution functions are unknown, but we can estimate them efficiently using the empirical distribution functions. For x, xj ∈ R and 1 ≤ j ≤ d, let\nF̂j(x) . = F̂j(x;X j 1:m) . =\n1 m |{i : 1 ≤ i ≤ m,x ≤ Xji }|\nF̂(x1, . . . , xd) . = (F̂1(x 1), . . . , F̂d(x d)) ∈ Rd.\nWe call the maps F, F̂ the copula transformation, and the empirical copula transformation, respectively. The sample (Ẑ1, . . . , Ẑm) . = (F̂(X1), . . . , F̂(Xm)) ∈ Rd is called the empirical copula (Dedecker et al., 2007). Note that the j-th coordinate of Ẑi (1 ≤ i ≤ m) equals\nẐji = 1\nm rank(Xji , {X j 1 , X j 2 , . . . , X j m}) ,\nwhere rank(x,A) is the number of elements of A less than or equal to x. Also, observe that the random variables Ẑ1, . . . , Ẑm are not even independent. Nonetheless, as we will see from Lemma 7, the empirical copula (Ẑ1, . . . , Ẑm) is a good approximation of an i.i.d. sample (Z1, . . . ,Zm) . = (F(X1), . . . ,F(Xm)) from the copula distribution of PX. Using (2), we have that\nM2u[F ,Z1:m,U1:m] = 1 m(m− 1) ∑ i ̸=j [ k(Zi,Zj)\n+k(Ui,Uj)− k(Zi,Uj)− k(Ui,Zj) ] .\nFrom (1), we can also see that Mb[F ,Z1:m,U1:n] = [ 1\nm2 m∑ i,j=1 k(Zi,Zj)\n+ 1\nn2 n∑ i,j=1 k(Ui,Uj)− 2 mn m,n∑ i,j=1 k(Zi,Uj) ]1/2 .\nIn these expressions Z1:m is not available to us. We estimate them using the empirical copula, Ẑj . = F̂(Xj), j = 1, . . . ,m. An estimator for I2(X) can be given by Î2u(X1:m), where\nm(m− 1)Î2u(X1:m) . = m(m− 1)M2u[F , Ẑ1:m,U1:m] =∑\ni ̸=j\n[ k(Ẑi, Ẑj) + k(Ui,Uj)− k(Ẑi,Uj)− k(Ui, Ẑj) ] .\nTo calculate this quantity, we only need the ranks of the marginal variables in the sample. Note that\nÎ2u(X1:m) is not an unbiased estimator of I(X), but we keep the notation Î2u to denote that it is derived from the estimator M2u.\nUsing the definition of Mb, we can also propose another estimator for I(X):\nÎb(X1:m) . = Mb[F , Ẑ1:m,U1:n] =\n[ 1\nm2 m∑ i,j=1 k(Ẑi, Ẑj)\n+ 1\nn2 n∑ i,j=1 k(Ui,Uj)− 2 mn m,n∑ i,j=1 k(Ẑi,Uj) ]1/2 .\nBoth estimators are extremely simple to implement requiring only kernel evaluations on the transformed data and the uniform variables. One can also see that the estimators are robust assuming k is bounded in [0, 1]d × [0, 1]d (but can be unbounded outside of this region, e.g. polynomial kernel). Thanks to the empirical copula transformation, we only need rank statistics (Ẑ1:m) in the estimation, but the actual values of X1:m sample points are not used. The contribution of one single sample point is diminishing in the estimator as we increase the sample size. Therefore, one arbitrarily large outlier sample point cannot ruin the statistics arbitrarily badly. For more discussion on this, see the Appendix.\nIn what follows we will analyze the theoretical properties of these estimators. Assume that the kernel function k(·, z) is uniformly Lipschitz continuous on [0, 1]d, i.e. there exists L > 0 such that for all z, z1, z2 ∈ [0, 1]d we have that |k(z1, z)−k(z2, z)| ≤ L∥z1−z2∥. A typical example is the Gaussian kernel, for which it holds that there exists L > 0 such that for all z, z1, z2 ∈ [0, 1]d∣∣∣∣exp(−∥z1 − z∥22σ2 )− exp(−∥z2 − z∥22σ2 )\n∣∣∣∣ ≤ L∥z1 − z2∥. Lemma 6. For all zi ∈ [0, 1]d, 1 ≤ i ≤ 4, we have that\n|k(z1, z2)− k(z3, z4)| ≤ L∥z1 − z3∥+ L∥z2 − z4∥.\nThe effect of the empirical copula transformation can be studied by a version of the classical KieferDvoretzky-Wolfowitz theorem due to Massart; see e.g. Devroye & Lugosi (2001). As a simple implication\nof this theorem, one can show that F̂ is a consistent estimator of F, and the convergence is uniform:\nLemma 7 (Convergence of the empirical copula). Let X1, . . . ,Xm be an i.i.d. sample from a probability distribution over Rd with marginal cdf’s F1, . . . , Fd. Let F(X) be the copula defined above, and let F̂(X1:m) be\nthe empirical copula transformation. Then, for any ϵ ≥ 0,\nPr [ sup x∈Rd ∥F(x)− F̂(x)∥2 > ϵ ] ≤ 2d exp(−2mϵ 2 d ) .\nLet 0 ≤ k(x, y) ≤ K be a bounded kernel function. The following theorems state the almost sure consistency of the dependence estimators, and provide upper bounds on the rate of convergence.\nTheorem 8 (Almost sure consistency). Almost surely we have that\n|Î2u(X1:m)− I2(X)|\n= O ( max {√ dL2\nm log(4dm2),\n√ K2\nm log(4m2)\n}) .\nFrom the below theorem it follows that when n grows fast enough, then Îb is almost surely consistent as well. Theorem 9 (Almost sure consistency). Let n = g(m) for some function g such that limm→∞ g(m) = ∞. Almost surely it holds that\n|Îb(X1:m)− I(X)| = O ( max {( 8dL2\nm log(4dm2)\n)1/4 ,\n( 2K(m+ n)\nmn log(4m2)\n)1/2} + ( K\nm\n)1/2 + ( K\nn\n)1/2 ) .\nAs these bounds show, the proposed dependence estimators can be used in high-dimensions as well; they do not suffer from the curse of dimensionality. Based on these estimators, one can derive independence tests too. For details, see the Appendix."
    }, {
      "heading" : "5. Feature Selection",
      "text" : "The above defined I(X) dependence measure is invariant to strictly increasing transformations of the marginal variables. In this section we discuss the benefits of this property in the feature selection problem.\nLet us have d real valued features {X1, . . . , Xd}, and a target value Y . Numerous feature selection methods use dependence estimation for selecting the most relevant features to predict the target value Y . If we want to select h features, then one obvious approach would be to select those h features that together have the highest dependence with Y . This subset selection problem, unfortunately, is very difficult. Therefore, several approximations and heuristics have been proposed. For example, according to the so-called maxrelevance criterion (Peng & Ding, 2005), our goal is to select a feature set S ⊆ {X1, . . . , Xd}, which maximizes the average dependence between the features\nand the target:\nŜ = argmax S\n1 |S| ∑ Xi∈S I(Xi, Y ). (3)\nThis approach might select highly redundant features, i.e. the dependence among these features could be large. This redundancy can be measured by the expression ∑ Xi,Xj∈S I(X i, Xj)/|S|2.\nWhen two features highly depend on each other, then probably we do not lose too much if we remove one of them. Therefore, our goal is to maximize relevance while minimizing the redundancy among the features\nŜ = argmax S ∑ Xi∈S I(Xi, Y ) |S| − ∑ Xi,Xj∈S I(Xi, Xj) |S|2 . (4)\nAll we need is a good estimator for I(Xi, Xj) and I(Xi, Y ) dependencies. Equation (3) and (4) objectives are popular tools for feature selection. Here we will not discuss the advantages and disadvantages of them. We, however, would like to point out that when someone uses objectives that involves dependence estimation, then we want these dependencies to be invariant to strictly increasing transformations of the marginal variables."
    }, {
      "heading" : "6. Numerical Illustrations",
      "text" : "We illustrate the theoretical contributions of this paper through a series of numerical experiments demonstrating properties of the copula-based kernel dependency measure.\nThe M(F , PX, ∏d\ni=1 PXi) measure could also be used directly, without copula transformation, to estimate dependence. In order to use this approach, we need to generate m sample points from the product distributions of the marginals. Let τi(1 : m), (1 ≤ i ≤ d) denote independent random permutations of {1, . . . ,m}. Then Π[X1:m] . = (X1τ1(1:m), X 2 τ2(1:m) , . . . , Xdτd(1:m)) T\ncan be considered as samples from the ∏d\ni=1 PXi distribution. In other words, if X1:m is stored in a d × m dimensional sample matrix and we independently permute the elements of each row, then the distributions of the rows (the marginal distributions of X) remain the same, but they become independent from each other. For brevity, we will call the M(F , PX, ∏d i=1 PXi) quantity MMD dependence measure."
    }, {
      "heading" : "6.1. Feature Selection",
      "text" : "In this experiment we show that I(X) can achieve better performance in feature selection than MMD without copula transformation (M(F , PX, ∏d i=1 PXi)).\nWe constructed the following random variables: X1 ∼ U [0, 1], X2 ∼ U [0, 500], Y = 500 sin(4πX1). The task in this experiment was to choose the feature between X1 and X2 that contains the most information about Y . This feature is of course X1 since Y is a deterministic function of it, and X2 is independent of Y ; it does not contain any information about Y . 300 sample points from the joint distrbutions of (X1,Y ) and (X2,Y ) are shown in Figure 3(a) and Figure 3(b), respectively. The empirical copula transformed points of (Y,X1) and (Y,X2) are displayed in Figure 3(c) and Figure 3(d). When we simply use MMD without copula transformation (M(F , PY,Xi , PY ×PXi)), then interestingly we got that the estimated dependence between Y and X1 (Mb(F , (Y,X1)1:m,Π[(Y,X1)1:m]), column (A) of Figure 3(e)) was smaller than the estimated dependence between between Y and X2 (Mb(F , (Y,X2)1:m,Π[(Y,X2)1:m]), column (B) of Figure 3(e)). As we can see in this problem, the MMD without copula transformation could not select the right feature. However, when we used copula transformation, then the estimated dependence was larger between Y and X1 than between Y and X2. The values of Îb((Y,X1)1:m) and Î\nb((Y,X2)1:m) are shown in the (C) and (D) columns of Figure 3(e). In this experiment we used Gaussian kernel with σ = 1."
    }, {
      "heading" : "6.2. Feature Standardization",
      "text" : "A frequently used feature preprocessing step is to standardize the features, that is, linearly transform them to have zero mean and unit variance (Ξ[X]). One might wonder if this simple transformation can solve the problem of Section 6.1. Below we show an example, where that we have only two zero mean unit variance features, and the MMD feature selection method that is not invariant to the strictly increasing transformations of the features selects a feature that is actually independent from the target value. Let U ∼ U [0, 1], X1 .= Ξ[1/U2], V ∼ U [0, 1] X2 . = Ξ[V ] independent random variables, and let Y . = Ξ[sin(4πX1)]. The variables are standardized so they have zero mean and standard variation 1. We sampled 4,000 i.i.d. observations from our observed features X1 and X2. The task again was to select the feature that contains the most information about Y . The solution to this problem is X1 again. The meanings of the columns in Figure 4 are the same as in Figure 3(e). When we simply use MMD without copula transformation, then the estimated dependence between between Y and X1 was smaller than between Y and X2 (Mb(F , (Y,X1)1:m,Π[(Y,X1)1:m]) and Mb(F , (Y,X2)1:m,Π[(Y,X2)1:m]) in column (A) and (B), respectively). The MMD without copula\ntransformation could not select the right feature. However, when we use copula transformation first, then we can see that the estimated dependence between Y and X1 is larger than between Y and X2, as expected. (C) and (D) show Îb((Y,X1)1:m) and Î\nb((Y,X2)1:m), respectively."
    }, {
      "heading" : "6.3. Housing Dataset",
      "text" : "In the following experiment we study our estimators on the Housing dataset from the UCI repository (Frank & Asuncion, 2010). The dataset contains 506 instances of 14 real valued attributes. The attributes contain various features including per capita crime rate by town, full-value property-tax rate per $10000, average number of rooms per dwelling, percentage of lower status of the population, median value of owneroccupied homes in $1000’s, etc. Our goal is to predict some of these attributes and select the most important features for this prediction. Since the dataset contains very different features, it is highly nontrivial how to scale them for feature selection when the applied dependence measure is not invariant to strictly increasing transformations of the marginals. This, however, is not an issue for our proposed dependence measure. In this experiment our goal was to predict the “median\nvalue of owner-occupied homes in $1000’s” (feature 14) using one single feature. We used m = n = 300 instances for training, and the rest of the data for testing. We applied Gaussian kernel (σ2 = 1/12) in the estimators. The MMD without copula transformation chose the “average number of rooms per dwelling” (feature 6) as the closest feature. When instead of MMD we\nused the proposed Îb estimator, it selected the “lower status of the population” (feature 13). To study the prediction errors of the selected features, we trained linear regressors for each feature using them as explanatory variables. The prediction errors on the test data are shown in Figure 5. In this experiment the smallest error was achieved by the feature that Îb selected (feature 13). MMD without the copula transformation selected the feature that gave only the second smallest error (feature 6).\nLow-dimensional embedding can help us visualize the pairwise dependence structure of random variables. For each feature Xi, Xj , we estimated the d(i, j) = exp(−I(Xi, Xj)) quantities. This d(i, j) is large when Xi, Xj is independent, and small when the dependence between them is large. We considered them as “distances” (although the triangle inequality does not hold between them), and then applied multidimensional scaling to embed them into a 2d space. The Housing dataset was used in this experiment too using the same set-up as in the previous study. To estimate the dependence between the features, we tested again Îb (Figure 6(a)) and MMD without copula transformation (Figure 6(b)). We can observe that the locations of these embedded points are very different. If we applied any strictly increasing transformations to the marginal variables, it would not affect the embedding with copula transformation, but we would get very different results when we use MMD without copula transformation. For more numerical experiments, see the supplementary material."
    }, {
      "heading" : "7. Discussion and Conclusion",
      "text" : "We introduced a new RKHS-based dependence measure that operates on the copula of continuous distributions. We have shown that the dependence measure is invariant to strictly increasing transformations of the marginal variables, and this property is important in feature selection and low-dimensional embedding of distributions. We also proposed estimators that are almost surely consistent, robust, use rank statistics only, and do not suffer from the curse of dimensionality. We derived upper bounds on the rates of convergence and illustrated the theory through a series of numerical experiments."
    } ],
    "references" : [ {
      "title" : "Kernel independent component analysis",
      "author" : [ "Bach", "Francis R" ],
      "venue" : "JMLR, 3:1–48,",
      "citeRegEx" : "Bach and R.,? \\Q2002\\E",
      "shortCiteRegEx" : "Bach and R.",
      "year" : 2002
    }, {
      "title" : "Integrating structured biological data by kernel maximum mean discrepancy",
      "author" : [ "K. Borgwardt", "A. Gretton", "M. Rasch", "H. Kriegel", "B. Schölkopf", "A. Smola" ],
      "venue" : "Bioinformatics, 22(14):e49–e57,",
      "citeRegEx" : "Borgwardt et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Borgwardt et al\\.",
      "year" : 2006
    }, {
      "title" : "Weak Dependence: With Examples and Applications, volume 190 of Lecture",
      "author" : [ "J. Dedecker", "P. Doukhan", "G. Lang", "J.R. Leon", "S. Louhichi", "C. Prieur" ],
      "venue" : "Notes in Statistics. Springer,",
      "citeRegEx" : "Dedecker et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Dedecker et al\\.",
      "year" : 2007
    }, {
      "title" : "Combinatorial Methods in Density Estimation",
      "author" : [ "L. Devroye", "G. Lugosi" ],
      "venue" : null,
      "citeRegEx" : "Devroye and Lugosi,? \\Q2001\\E",
      "shortCiteRegEx" : "Devroye and Lugosi",
      "year" : 2001
    }, {
      "title" : "Mutual information is critically dependent on prior assumptions: would the correct estimate of mutual information please identify",
      "author" : [ "A. Fernandes", "G. Gloor" ],
      "venue" : "itself? Bioinformatics,",
      "citeRegEx" : "Fernandes and Gloor,? \\Q2010\\E",
      "shortCiteRegEx" : "Fernandes and Gloor",
      "year" : 2010
    }, {
      "title" : "Convergence de laréparation empirique vers la réparation théorique",
      "author" : [ "R. Fortet", "E. Mourier" ],
      "venue" : "Ann. Scient. École Norm,",
      "citeRegEx" : "Fortet and Mourier,? \\Q1953\\E",
      "shortCiteRegEx" : "Fortet and Mourier",
      "year" : 1953
    }, {
      "title" : "The kernel mutual information",
      "author" : [ "A. Gretton", "R. Herbrich", "A. Smola" ],
      "venue" : "In Proc. ICASSP,",
      "citeRegEx" : "Gretton et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Gretton et al\\.",
      "year" : 2003
    }, {
      "title" : "Measuring statistical dependence with HilbertSchmidt norms",
      "author" : [ "A. Gretton", "O. Bousquet", "A. Smola", "B. Schölkopf" ],
      "venue" : "In ALT, pp",
      "citeRegEx" : "Gretton et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Gretton et al\\.",
      "year" : 2005
    }, {
      "title" : "An Introduction to Copulas (Lecture Notes in Statistics)",
      "author" : [ "R. Nelsen" ],
      "venue" : null,
      "citeRegEx" : "Nelsen,? \\Q1998\\E",
      "shortCiteRegEx" : "Nelsen",
      "year" : 1998
    }, {
      "title" : "Estimation of renyi entropy and mutual information based on generalized nearest-neighbor graphs",
      "author" : [ "D. Pál", "B. Póczos", "Szepesvári", "Cs" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Pál et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Pál et al\\.",
      "year" : 2010
    }, {
      "title" : "Feature selection based on mutual information: Criteria of max-dependency, maxrelevance, and min-redundancy",
      "author" : [ "H. Peng", "C. Ding" ],
      "venue" : null,
      "citeRegEx" : "Peng and Ding,? \\Q2005\\E",
      "shortCiteRegEx" : "Peng and Ding",
      "year" : 2005
    }, {
      "title" : "On measures of dependence",
      "author" : [ "A. Rényi" ],
      "venue" : "Acta. Math. Acad. Sci. Hungar,",
      "citeRegEx" : "Rényi,? \\Q1959\\E",
      "shortCiteRegEx" : "Rényi",
      "year" : 1959
    }, {
      "title" : "On measure of entropy and information",
      "author" : [ "A. Rényi" ],
      "venue" : "In 4th Berkeley Symposium on Math., Stat., and Prob.,",
      "citeRegEx" : "Rényi,? \\Q1961\\E",
      "shortCiteRegEx" : "Rényi",
      "year" : 1961
    }, {
      "title" : "On nonparametric measures of dependence for random variables",
      "author" : [ "B. Schweizer", "E. Wolff" ],
      "venue" : "The Annals of Statistics,",
      "citeRegEx" : "Schweizer and Wolff,? \\Q1981\\E",
      "shortCiteRegEx" : "Schweizer and Wolff",
      "year" : 1981
    }, {
      "title" : "Fonctions de rpartition n dimensions et leurs marges",
      "author" : [ "A. Sklar" ],
      "venue" : "Publ. Inst. Statist. Univ. Paris,",
      "citeRegEx" : "Sklar,? \\Q1959\\E",
      "shortCiteRegEx" : "Sklar",
      "year" : 1959
    }, {
      "title" : "On the influence of the kernel on the consistency of support vector machines",
      "author" : [ "I. Steinwart" ],
      "venue" : null,
      "citeRegEx" : "Steinwart,? \\Q2001\\E",
      "shortCiteRegEx" : "Steinwart",
      "year" : 2001
    }, {
      "title" : "Measuring and testing dependence by correlation of distances",
      "author" : [ "G.J. Székely", "M.L. Rizzo", "N.K. Bakirov" ],
      "venue" : "Annals of Statistics,",
      "citeRegEx" : "Székely et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Székely et al\\.",
      "year" : 2007
    }, {
      "title" : "Possible generalization of boltzmann-gibbs statistics",
      "author" : [ "C. Tsallis" ],
      "venue" : "J. Statist. Phys.,",
      "citeRegEx" : "Tsallis,? \\Q1988\\E",
      "shortCiteRegEx" : "Tsallis",
      "year" : 1988
    } ],
    "referenceMentions" : [ {
      "referenceID" : 12,
      "context" : "In particular, it is a special case of the Rényi-α (Rényi, 1961) and Tsallis-α mutual information (Tsallis, 1988).",
      "startOffset" : 51,
      "endOffset" : 64
    }, {
      "referenceID" : 17,
      "context" : "In particular, it is a special case of the Rényi-α (Rényi, 1961) and Tsallis-α mutual information (Tsallis, 1988).",
      "startOffset" : 98,
      "endOffset" : 113
    }, {
      "referenceID" : 11,
      "context" : "Other interesting dependence measures include the maximal correlation coefficient (Rényi, 1959), kernel mutual information (Gretton et al.",
      "startOffset" : 82,
      "endOffset" : 95
    }, {
      "referenceID" : 6,
      "context" : "Other interesting dependence measures include the maximal correlation coefficient (Rényi, 1959), kernel mutual information (Gretton et al., 2003), the generalized variance and kernel canonical correlation analysis (Bach, 2002), the Hilbert-Schmidt independence criterion (Gretton et al.",
      "startOffset" : 123,
      "endOffset" : 145
    }, {
      "referenceID" : 7,
      "context" : ", 2003), the generalized variance and kernel canonical correlation analysis (Bach, 2002), the Hilbert-Schmidt independence criterion (Gretton et al., 2005), the Schweizer-Wolff measure (Schweizer & Wolff, 1981), and the distance based correlation (Székely et al.",
      "startOffset" : 133,
      "endOffset" : 155
    }, {
      "referenceID" : 16,
      "context" : ", 2005), the Schweizer-Wolff measure (Schweizer & Wolff, 1981), and the distance based correlation (Székely et al., 2007).",
      "startOffset" : 99,
      "endOffset" : 121
    }, {
      "referenceID" : 9,
      "context" : "For example, the bound on the convergence rate of the Rényi and Tsallis information estimator (Pál et al., 2010) suffers from the curse of dimensionality.",
      "startOffset" : 94,
      "endOffset" : 112
    }, {
      "referenceID" : 9,
      "context" : "For example, the bound on the convergence rate of the Rényi and Tsallis information estimator (Pál et al., 2010) suffers from the curse of dimensionality. The available reproducing kernel based dependence measures are not invariant to strictly increasing transformation of the Xi marginal random variables. The estimator of Székely et al. (2007) is not robust; one single large enough outlier can arbitrarily ruin the estimator.",
      "startOffset" : 95,
      "endOffset" : 346
    }, {
      "referenceID" : 1,
      "context" : "In this section we review some important properties of the Maximum Mean Discrepancy (MMD), which is a quantity used to measure the distance between distributions (Borgwardt et al., 2006; Fortet & Mourier, 1953).",
      "startOffset" : 162,
      "endOffset" : 210
    }, {
      "referenceID" : 1,
      "context" : "This is stated formally in the following lemma (Borgwardt et al., 2006).",
      "startOffset" : 47,
      "endOffset" : 71
    }, {
      "referenceID" : 1,
      "context" : "An unbiased estimator for M[F , P,Q] (when m = n) has also been derived in Borgwardt et al. (2006):",
      "startOffset" : 75,
      "endOffset" : 99
    }, {
      "referenceID" : 8,
      "context" : "Below we review a few important properties of the copula of multivariate distributions that we will use in our work (Nelsen, 1998).",
      "startOffset" : 116,
      "endOffset" : 130
    }, {
      "referenceID" : 14,
      "context" : "The copula can be defined by the Sklar’s theorem (Sklar, 1959) as follows.",
      "startOffset" : 49,
      "endOffset" : 62
    }, {
      "referenceID" : 2,
      "context" : ", F̂(Xm)) ∈ R is called the empirical copula (Dedecker et al., 2007).",
      "startOffset" : 45,
      "endOffset" : 68
    } ],
    "year" : 2012,
    "abstractText" : "The paper presents a new copula based method for measuring dependence between random variables. Our approach extends the Maximum Mean Discrepancy to the copula of the joint distribution. We prove that this approach has several advantageous properties. Similarly to Shannon mutual information, the proposed dependence measure is invariant to any strictly increasing transformation of the marginal variables. This is important in many applications, for example in feature selection. The estimator is consistent, robust to outliers, and uses rank statistics only. We derive upper bounds on the convergence rate and propose independence tests too. We illustrate the theoretical contributions through a series of experiments in feature selection and low-dimensional embedding of distributions.",
    "creator" : " TeX output 2012.05.17:1636"
  }
}