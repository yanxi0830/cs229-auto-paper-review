{
  "name" : "1011.5668.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ "Alexey Chernov" ],
    "emails" : [ "chernov@cs.rhul.ac.uk" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n01 1.\n56 68\nv1 [\ncs .L\nG ]\n2 5\nN ov\n2 01\n0\nOn Theorem 2.3 in “Prediction, Learning, and\nGames” by Cesa-Bianchi and Lugosi.\nAlexey Chernov∗\nComputer Learning Research Centre and Dept Computer Science\nRoyal Holloway, University of London, Egham, Surrey TW20 0EX, UK\nchernov@cs.rhul.ac.uk\nThis note proves a loss bound for the exponentially weighted average forecaster with time-varying potential, see [1, § 2.3] for context and definitions. The present proof gives a better constant in the regret term than Theorem 2.3 in [1]. This proof first appeared in [2] (Theorem 2), where a more general algorithm is considered. Here the proof is rewritten using the notation of [1].\nTheorem 1. Assume that the loss function ℓ is convex in the first argument and ℓ(p, y) ∈ [0, 1] for all p ∈ D and y ∈ Y. For any positive reals η1 ≥ η2 ≥ . . ., for any n ≥ 1 and for any y1, . . . , yn ∈ Y, the regret of the exponentially weighted average forecaster with time-varying learning rate ηt satisfies\nL̂n − min i=1,...,N\nLi,n ≤ lnN\nηn +\n1\n8\nn∑\nt=1\nηt . (1)\nIn particular, for ηt = √ 4 lnN t , t = 1, . . . , n, we have\nL̂n − min i=1,...,N\nLi,n ≤ √ n lnN .\nProof. The forecaster at step t predicts p̂t = ∑N\ni=1 wi,t−1 Wt−1 fi,t, where wi,t−1 =\ne−ηtLi,t−1 and Wt−1 = ∑N j=1wj,t−1. Due to convexity of ℓ we have\nℓ(p̂t, yt) ≤ N∑\ni=1\nwi,t−1 Wt−1 ℓ(fi,t, yt) .\n∗Supported by EPSRC grant EP/F002998/1.\nUsing the Hoeffding inequality ([1, Lemma A.1]), we get\ne −ηt\n∑N i=1 wi,t−1\nWt−1 ℓ(fi,t,yt) ≥\nN∑\ni=1\nwi,t−1 Wt−1 e−ηtℓ(fi,t,yt)−η 2 t /8\nand thus\ne−ηtℓ(p̂t,yt) ≥ N∑\ni=1\nwi,t−1 Wt−1 e−ηtℓ(fi,t,yt)−η 2 t /8 . (2)\nConsider the values\nsi,t−1 = e −ηt−1Li,t−1+ηt−1L̂t−1−\n1 8 ηt−1 ∑t−1 k=1 ηk\nand note that\nwi,t−1 Wt−1\n= 1 N (si,t−1)\nηt ηt−1\n∑N j=1 1 N (sj,t−1) ηt ηt−1 . (3)\nLet us show that ∑N\nj=1 1 N sj,t ≤ 1 by induction over t. For t = 0 this is\ntrivial, since sj,0 = 1 for all j. Assume that ∑N j=1 1 N sj,t−1 ≤ 1. Then\nN∑\nj=1\n1\nN (sj,t−1)\nηt ηt−1 ≤\n( N∑\nj=1\n1\nN sj,t−1\n) ηt ηt−1\n≤ 1 , (4)\nsince the function x 7→ xα is concave and monotone for x ≥ 0 and α ∈ [0, 1] and since ηt−1 ≥ ηt > 0. Using (4) to bound the right-hand side of (3), we get\nwi,t−1 Wt−1 ≥ 1 N (si,t−1)\nηt ηt−1 ; and combining with (2), we get\ne−ηtℓ(p̂t,yt) ≥ N∑\ni=1\n1\nN (si,t−1)\nηt ηt−1 e−ηtℓ(fi,t,yt)−η 2 t /8 .\nIt remains to note that\nsi,t = (si,t−1) ηt ηt−1 e−ηtℓ(fi,t,yt)+ηtℓ(p̂t,yt)−η 2 t /8\nand we get ∑N\ni=1 1 N si,t ≤ 1.\nFor any i, we have 1 N si,n ≤ ∑N j=1 1 N sj,n ≤ 1, thus\n−ηnLi,n + ηnL̂n − 1\n8 ηn\nn∑\nk=1\nηk ≤ lnN ,\nand (1) follows.\nTheorem 1 recommends the learning rate ηt = √ (4 lnN)/t instead of√\n(8 lnN)/t used in Theorem 2.3 in [1] and achieves the regret term √ n lnN\ninstead of √ 2n lnN + √ 0.125 lnN .\nTo compare the bounds for arbitrary learning rates, let us observe that the proof of Theorem 2.3 in [1] actually implies (under the assumptions of Theorem 1):\nL̂n − min i=1,...,N\nLi,n ≤ ( 2\nηn − 1 η1\n) lnN + 1\n8\nn∑\nt=1\nηt .\nThe right-hand side of this inequality is larger than the right-hand side of (1) if ηn 6= η1. If ηt are equal for all t, the bounds coincide and give the bound of Theorem 2.2 in [1].\nReferences\n[1] N. Cesa-Bianchi, G. Lugosi. Prediction, Learning, and Games. Cambridge University Press, Cambridge, England, 2006.\n[2] A. Chernov, F. Zhdanov. Prediction with expert advice under discounted loss. Proc. of ALT 2010, LNCS 6331, pp. 255-269. See also: arXiv:1005.1918v1 [cs.LG]."
    } ],
    "references" : [ {
      "title" : "Prediction, Learning, and Games",
      "author" : [ "N. Cesa-Bianchi", "G. Lugosi" ],
      "venue" : null,
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2006
    }, {
      "title" : "Prediction with expert advice under discounted loss",
      "author" : [ "A. Chernov", "F. Zhdanov" ],
      "venue" : "Proc. of ALT 2010,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 1918
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "3 in [1].",
      "startOffset" : 5,
      "endOffset" : 8
    }, {
      "referenceID" : 1,
      "context" : "This proof first appeared in [2] (Theorem 2), where a more general algorithm is considered.",
      "startOffset" : 29,
      "endOffset" : 32
    }, {
      "referenceID" : 0,
      "context" : "Here the proof is rewritten using the notation of [1].",
      "startOffset" : 50,
      "endOffset" : 53
    }, {
      "referenceID" : 0,
      "context" : "Assume that the loss function l is convex in the first argument and l(p, y) ∈ [0, 1] for all p ∈ D and y ∈ Y.",
      "startOffset" : 78,
      "endOffset" : 84
    }, {
      "referenceID" : 0,
      "context" : "since the function x 7→ x is concave and monotone for x ≥ 0 and α ∈ [0, 1] and since ηt−1 ≥ ηt > 0.",
      "startOffset" : 68,
      "endOffset" : 74
    }, {
      "referenceID" : 0,
      "context" : "3 in [1] and achieves the regret term √ n lnN instead of √ 2n lnN + √ 0.",
      "startOffset" : 5,
      "endOffset" : 8
    }, {
      "referenceID" : 0,
      "context" : "3 in [1] actually implies (under the assumptions of Theorem 1): L̂n − min i=1,.",
      "startOffset" : 5,
      "endOffset" : 8
    } ],
    "year" : 2010,
    "abstractText" : null,
    "creator" : "LaTeX with hyperref package"
  }
}