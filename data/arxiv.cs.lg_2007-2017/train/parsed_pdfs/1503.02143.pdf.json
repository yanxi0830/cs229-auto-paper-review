{
  "name" : "1503.02143.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Model selection of polynomial kernel regression",
    "authors" : [ "Shaobo Lin", "Xingping Sun", "Zongben Xu", "Jinshan Zeng" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n50 3.\n02 14\n3v 1\n[ cs\n.L G\n] 7\nM ar\nPolynomial kernel regression is one of the standard and state-of-the-art learning strategies. However, as is well known, the choices of the degree of polynomial kernel and the regularization parameter are still open in the realm of model selection. The first aim of this paper is to develop a strategy to select these parameters. On one hand, based on the worst-case learning rate analysis, we show that the regularization term in polynomial kernel regression is not necessary. In other words, the regularization parameter can decrease arbitrarily fast when the degree of the polynomial kernel is suitable tuned. On the other hand,taking account of the implementation of the algorithm, the regularization term is required. Summarily, the effect of the regularization term in polynomial kernel regression is only to circumvent the “ ill-condition” of the kernel matrix. Based on this, the second purpose of this paper is to propose a new model selection strategy, and then design an efficient learning algorithm. Both theoretical and experimental analysis show that the new strategy outperforms the previous one. Theoretically, we prove that the new learning strategy is almost optimal if the regression function is smooth. Experimentally, it is shown that the new strategy can significantly reduce the computational burden without loss of generalization capability.\nIndex Terms\nModel selection, regression, polynomial kernel, learning rate.\nS. Lin is with the College of Mathematics and Information Science, Wenzhou University, Wenzhou 325035, China, Z. Xu and J. Zeng are with the Institute for Information and System Sciences, School of Mathematics and Statistics, Xi’an Jiaotong University, Xi’an 710049, P R China, X. Sun is with the Department of Mathematics, Missouri State University, Springfield, MO 65897, USA\nMarch 10, 2015 DRAFT\n2 I. INTRODUCTION\nIn many scientific fields, large amount of data (xi, yi)mi=1 arise from sampling unknown functions. Scientists train data and then synthesize a function f such that f(x) is an efficient estimate of the output y when a new input x is given. The training process is usually divided into two steps. The one is to select a suitable model and the other focuses on designing an efficient learning algorithm based on the selected model. Generally speaking, the model selection strategy comprises choosing a hypothesis space, a family of parameterized functions that regulate the forms and properties of the estimator to be found, and selecting an optimization criterion, the sense in which the estimator is defined. The learning algorithm is an inference process to yield the objective estimator from a finite set of data. The central question of learning theory is how to select a feasible model and then develop an efficient algorithm such that the synthesized function can approximate the original unknown but definite function.\nIf the kernel methods [7], [35] are used, then the model selection problem boils down to choosing a suitable kernel and the corresponding regularization parameter. After verifying the existences of the optimal kernel [18] and regularization parameter [8], there are two trends of model selection. The one is to pursue some prominent kernels containing multi-kernel learning [25], [16], hyperkernel learning [23], [24], and other kernel selection methods [1], [32], [19]. The other focuses on selecting optimal regularization parameters for some prevailing kernels, comprising Gaussian kernel [15], [30], [37], polynomial kernel [41], [33] [40], and other more general kernels [6], [11], [31], [36]. The topic of the current paper falls into the second category. We study the parameter selection problem in polynomial kernel regression.\nDifferent from other widely used kernels [9], the reproducing kernel Hilbert space Hs of the polynomial kernel Ks = (1 + x · y)s is a finite-dimensional vector space, and its dimension depends only on s. Therefore, one can tune s directly to control the capacity of Hs. Using this fact, [41] found that the regularization parameter in polynomial kernel regression should decrease exponentially fast with the sample size for appropriately selected s. They then attributed it as an essential feature of the polynomial kernel learning. The first purpose of this paper is to continue the study of [41]. Surprisingly, after the rigorous proof, we find that, as far as the learning rate is concerned, the regularization parameter can decrease arbitrarily fast for a suitable s. An extreme case is that in the framework of model selection, the regularization term can be omitted. This\nMarch 10, 2015 DRAFT\n3 automatically arises the following question: What is the essential effect of the regularization term in polynomial kernel regression?\nTo answer the above question, we recall that the purpose of introducing regularization term in kernel methods is to avoid the overfitting phenomenon [8], [9], which is the special case that the synthesized function fits the sample very well but fails to fit other points. However, what factor causes overfitting in the learning process is usually neglected by numerous programmers. Therefore, the essential role of the regularization term can not be captured. To the best of our knowledge, there are two main reasons cause the overfitting phenomenon. The one is the algorithm-based factor such as ill-condition of the kernel matrix and the other is the modelbased factor like too large capacity of the hypothesis space. We find that there is only one job the regularization term in polynomial kernel regression doing: to assure that a simple matrixinverse technique can finish the learning task. This phenomenon is quite different from the other kernel-based methods. For example, since the Gaussian-RKHS is an infinite dimensional vector space, the introducing of regularization term in Gaussian kernel regression is to control both the condition number of the kernel matrix and capacity of the hypothesis space [15].\nBased on the above assertions, the second purpose of this paper is to propose a new model selection method. By the well known representation theorem [7] in learning theory, the essential hypothesis space of polynomial kernel regression is the linear space H := span{(1+x1·x)s, · · · , (1+ xm ·x)s}. Since the algorithm-based factor is the only reason of over-fitting in polynomial kernel regression. We can choose n points {ti}ni=1 such that the matrix ((1 + tj · xi)s)m,ni=1,j=1 is nonsingular. The set {ti}ni=1 can be easily obtained. For example, we can draw {ti}ni=1 identically and independently according to the uniform distribution. Then the pseudo-inverse technique [27] can conduct the estimator easily. In the new model, it can be found in Section 4 that there is only one parameter, s, need tuning. We also give an efficient strategy to select s based on the theoretical analysis. Surprisingly, we find that the difficulty of model selection in our setting depends heavily on the dimension of input space. It is shown that the higher the dimension, the easier the model selection.\nBoth theoretical analysis and experimental simulations are provided to illustrate the performance of the new model selection strategy. Theoretically, the new method is proved to be the almost optimal strategy if the so-called regression function is smooth. Furthermore, it is also shown that the pseudo-inverse technique can realize the almost optimality. Experimentally, both\nMarch 10, 2015 DRAFT\n4 toy simulations and UCI standard data experiments imply that the new method is more efficient than the previous model selection strategy. More concretely, the new method can significantly reduce the computational burden without loss of generalization capability. The most highlight of the proposed model is that there is only a parameter (or almost no parameter of high-dimensional case) need to be tuned in the learning process.\nThe rest of paper is organized as follows. In the next section, we give a fast review of statistical learning theory and kernel method. In Section 3, we study the model selection problem of the classical polynomial kernel regression. Section 4 describes a new model selection strategy and provide its theoretical properties. In Section 5, both toy and real world simulation results are reported to verify the theoretical results. Section 6 is devoted to proving the main results, and Section 7 draw a simple conclusion."
    }, {
      "heading" : "II. A FAST REVIEW OF STATISTICAL LEARNING THEORY AND KERNEL METHODS",
      "text" : "Let X ⊆ Rd be the input space and Y ⊆ R be the output space. Suppose that the unknown probability measure ρ on Z := X × Y admits the decomposition\nρ(x, y) = ρX(x)ρ(y|x).\nLet z = (xi, yi)mi=1 be a finite random sample of size m, m ∈ N, drawn independently and identically according to ρ. Suppose further that f : X → Y is a function that one uses to model the correspondence between X and Y, as induced by ρ. One natural measurement of the error incurred by using f of this purpose is the generalization error, defined by\nE(f) := ∫\nZ (f(x)− y)2dρ,\nwhich is minimized by the regression function [7], defined by\nfρ(x) := ∫\nY ydρ(y|x).\nWe do not know this ideal minimizer fρ, since ρ is unknown, but have access to random examples from X × Y sampled according to ρ. Let L2ρ\nX be the Hilbert space of ρX square integrable function on X , with norm denoted by\n‖ · ‖ρ. With the assumption that fρ ∈ L2ρ X , it is known that, for every f ∈ L2ρX , there holds\nE(f)− E(fρ) = ‖f − fρ‖2ρ. (1)\nMarch 10, 2015 DRAFT\n5 So, the goal of learning is to find the best approximation of the regression function fρ within a hypothesis space H. Let fH ∈ H be the best approximation of fρ, i.e., fH := argming∈H ‖f−g‖ρ. If there is an estimator fz ∈ H based on the samples z in hand, then we have\nE(fz)− E(fρ) = ‖fρ − fH‖2ρ + E(fz)− E(fH). (2)\nIt is well known [7], [9], [14] that a small H will derive a large bias ‖fρ−fH‖2ρ, while a large H will deduce a large variance E(fz)−E(fH). The best hypothesis space H∗ is obtained when the best comprise between the conflicting requirements of small bias and small variance is achieved. This is the well known “bias-variance” dilemma of model selection.\nLet K : X × X → R be continuous, symmetric and positive semidefinite, i.e., for any finite set of distinct points {x1, x2, . . . , xm} ⊂ X , the kernel matrix (K(xi, xj))mi,j=1 is positive semidefinite. If HK is the reproducing kernel Hilbert space associated with the kernel K. Then HK (see [2]) is the closure of the linear span of the set of functions {Kx = K(x, ·) : x ∈ X} with the inner product 〈·, ·〉K satisfying 〈Kx, Ky〉K = K(x, y) and\n〈Kx, f〉K = f(x), ∀x ∈ X, f ∈ HK .\nThe following Aronszajn Theorem (see [2]) describes an essential relationship between the RKHS and reproducing kernel.\nLemma 1: Let H be a separable Hilbert space of functions over X with orthonormal basis {φk}∞k=0. H is a reproducing kernel Hilbert space if and only if\n∞ ∑\nk=0\n|φk(x)|2 < ∞\nfor all x ∈ X . The unique reproducing kernel K is defined by\nK(x, y) := ∞ ∑\nk=0\nφk(x)φk(y).\nThe regularized least square algorithm in HK is defined by\nfz,λ := arg min f∈HK\n{\n1\nm\nm ∑\ni=1\n(f(xi)− yi)2 + λ‖f‖2K } .\nHere λ ≥ 0 is a constant called the regularization parameter. Usually, it depends on the sample number m. If the empirical error is defined by\nEz(f) := 1\nm\nm ∑\ni=1\n(f(xi)− yi)2,\nMarch 10, 2015 DRAFT\n6 then the corresponding problem can be represented as\nfz,λ = arg min f∈HK\n{ Ez(f) + λ‖f‖2K } ."
    }, {
      "heading" : "III. MODEL SELECTION IN POLYNOMIAL KERNEL REGRESSION",
      "text" : "Let X = Bd, Y = [−M,M ], where Bd denotes the unit ball in Rd, and M < ∞. We employ the polynomial kernel Ks(x, y) = (1 + x · y)s to tackle regression problem. From Lemma 1, it is easy to check that the RKHS of Ks, Hs, coincides with the space (Pds , 〈·, ·〉s), where 〈·, ·〉s be the inner product deduced from Ks according to 〈Ks(x, ·), Ks(·, y)〉s = Ks(x, y), and Pds be the set of algebraic polynomials of degree at most s.\nWe study the parameter selection for the following model\nfz,λ,s := arg min f∈Hs\n1\nm\nm ∑\ni=1\n(f(xi)− yi)2 + λ‖f‖2s. (3)\nFrom (1, the main purpose of model selection is to yield an optimal estimate for\nE(fz,λ,s)− E(fρ). (4)\nThe error (4) clearly depends on z and therefore has a stochastic nature. As a result, it is impossible to say something about (4) in general for a fixed z. Instead, we can look at its behavior in expectation as measured by the expectation error\nEρm(‖fz − fρ‖ρ) := ∫\nZm ‖fz − fρ‖dρm, (5)\nwhere the expectation is taken over all realizations z obtained for a fixed m, and ρm is the m fold tensor product of ρ. Obviously, the error (5) depends on m, s, λ and fρ.\nRecall that we do not know ρ so that the best we can say about it is that it lies in M(Θ), where M(Θ) is the class of all Borel measures ρ on Z such that fρ ∈ Θ ⊂ L2ρX . We enter into a competition over all estimators Am : z → fz and define\nem(Θ) := inf Am sup ρ∈M(Θ)\nEρm(‖fρ − fz‖2ρ).\nIt is easy to see that em(Θ) quantitively measures the quality of fz.\nNow, we are in a position to discuss the model selection of polynomial kernel regression. Let\nk = (k1, k2, . . . , kd), ki ∈ N, and define the derivative\nDkf(x) := ∂|k|f\n∂k1x1 · · ·∂kdxd ,\nMarch 10, 2015 DRAFT\n7 where |k| := k1 + · · ·+ kd. The classical Sobolev class is then defined for any r ∈ N by\nW rp :=\n{\nf : max 0≤|k|≤r\n‖Dkf‖Lp(X) < ∞, r ∈ N } .\nLet ΠM t denote the clipped value of t at ±M , that is, ΠM t := min{M, |t|}sgnt. Then it is obvious that [41] for all t ∈ R and y ∈ [−M,M ] there holds\nE(ΠMfz,λ,s)− E(fρ) ≤ E(fz,λ,s)− E(fρ).\nFor arbitrary C > 0, ⌈C⌉ denotes the smallest integer not larger than C. The following Theorem 1 shows the actions of the parameters in deducing the learning rate and how to select optimal parameters.\nTheorem 1: Let r ∈ N and fz,λ,s be defined as in (3). Then, for arbitrary fρ ∈ L∞(Bd), there holds\nEρm{‖ΠMfz,λ,s − fρ‖2ρ} ≤ C   sd logm\nm + inf\nP∈Pd [s/2]\n‖fρ − P‖2∞ + λ(4d)2s   . (6)\nFurthermore, if fρ ∈ W r∞ and s = ⌈ m 1 d+2r ⌉ , then for all 0 ≤ λ ≤ m− 2r2r+d (4d) −1 (d+2r) , there exist constants C1 and C2 depending only on d, r,M and p such that,\nC1m − 2r d+2r ≤ em(W r∞) ≤ sup ρ∈M(W r ∞ ) Eρm(‖fρ − ΠMfz,λ,s‖2ρ) ≤ C2m− 2r d+2r logm. (7)\nAt first, we introduce some related work and compare them with Theorem 1. The first result, to the best of our knowledge, concerning selection of the optimal regularization parameter in the framework of learning theory belongs to [8]. As a streamline work of the seminal paper [7], Cucker and Smale [8] gave a rigorous proof of the existence of the optimal regularization parameter. They declared that there is an optimal regularization parameter λ > 0 which makes the generalization error the smallest. This leads a prevailing conception that the error estimate (6) should have more terms containing λ, besides the term λ(4d)2s. However, it is not what our result has witnessed, which seems a contradiction at the first glance. After checking the proof of [8] carefully, we find that there is nothing to surprise. On one hand, the optimal parameter mentioned in [8] aims to the generalization error, containing learning rate and the constant C2, while our result only concerns the learning rate. On the other hand, [8]’s result is more suitable to describe the performance of K(·, ·) satisfying ‖f‖∞ ≤ C‖f‖K , where C is a constant independent of m. However, this property doesn’t hold for the polynomial kernel since the only thing we can confirm is that ‖f‖∞ ≤ 2s/2‖f‖s, where s depends on m.\nMarch 10, 2015 DRAFT\n8 After [8], we have witnessed the multiple emergence of the selection strategies of regularization parameter. The typical results were reported in [5], [6], [15], [30], [33], [11], [31], [36], [29], [40] and [41]. The optimal parameter may depend on the effective dimension of the marginal probability measure over the input space [5], [6], the eigenvalue of the integral operate with respect to the kernel [31], [29], or the smoothness of the regression function [15], [36]. The most different job we done is that we find the regularization parameter for polynomial kernel learning can decrease arbitrarily fast, an extreme case of which is that non-regularization leastsquare can also conduct the almost optimal learning rate. In other words, Theorem (1) shows that as far as the model selection is concerned, the choice of s is much more important than the choice of λ.\nFor polynomial kernel learning, there are two papers [33], [41] focusing on selection of the optimal parameter. It can be easily deduced from [33] and [41] that the learning rate of the regularized least square regression regularized with the polynomial kernel behaves as O(m− 2r2r+d+1 ), which is improved by Theorem 1 in the following three directions. Firstly, the learning rate analysis in Theorem 1 is based on distribution-free theory: we do not impose any assumptions to the marginal distribution ρX . Secondly, the optimal estimate is established for arbitrary W rp (0 < r < ∞) rather than 0 < r ≤ 2. Thirdly, Theorem 1 states that the learning rate can be improved into the almost optimal one. Therefore, as far as the learning rate is concerned, polynomial kernel is almost optimal choice if the smoothness information of the regression function is known.\nEberts and Steinwart [15] have already built a similar learning rate analysis for Gaussian kernel regression. It is valuable to compare the performance between Gaussian kernel regression and polynomial kernel regression. In the former one, there are two parameters need tuning. The one is the width of the Gaussian kernel and the other is the regularization parameter. Both the width and the regularization parameter are real number in some intervals. Thus, a wisdom method is to use the so-called cross-validation strategy [14, Chpater 8] to fix them, which causes tremendous computation if the size of samples is large. Differently, the kernel parameter of polynomial kernel is a discrete quantity and our result shows that s = ⌈ m 1 d+2r ⌉ is almost optimal choice for arbitrary fρ ∈ W rp . Although, the smoothness parameter r is usually unknown in practice, Theorem 1 gives us a criterion to chose s. Since s is discrete, and s may be smaller than ⌈m1/d⌉, there are only [m1/d] possible value of s. Noting that if d is large, no matter how large m is,\nMarch 10, 2015 DRAFT\n9 [m1/d] can not be large than 10 (the most possible case is s ≤ 2 or s ≤ 3 (see Section 5)). Therefore, it is very easy to fix the kernel parameter through the cross-validation method. Under this circumstance, the computational burden of polynomial kernel regression can be reduced and much less than that of Gaussian kernel regression (See Table 4 in Section 5).\nBy using the well known plug-in rules, which define a classier gz,λ,s has the form\ngz,λ,s :=\n  \n  1, if fz,λ,s ≥ 12 , 0, if fz,λ,s < 12 ,\n(8)\nTheorem 1 and [39] imply that the classier defined as in (8) is also almost optimal if the well known Bayes decision function satisfies a certain smoothness assumption. Therefore, Ks is also one of the best choice to deal with pattern recognition problem under this setting.\nAt last, we discuss the effect of the regularization term playing in polynomial kernel regression. The purpose of introducing regularization term in kernel learning is to overcome the overfitting phenomenon. However, the factor causing overfitting is a little sophisticated. It may attribute to the high capacity of the hypothesis, the ill-condition of the kernel matrix, or both of them. In short, there are two main factors leading to overfitting in kernel learning. The one is the modelbased factor, i.e., a large capacity of the hypothesis space and the other is the algorithm-based factor, i.e., ill-condition of the kernel matrix. In the polynomial kernel regression, Theorem 1 shows that arbitrary small λ (an extreme case is λ = 0) can deduce almost optimal learning rate if s is appropriately tuned. Thus, the overfitting phenomenon in polynomial kernel learning is not caused by the model-based factor for suitable s. Recall that the kernel matrix, A := ((1+xi · xj)) m i,j=1, is singular if m > ( s+d s ) , which is most possible in the learning process. This makes the simple matrix-inverse technique can not deduce the estimator directly. Thus, a regularization term is required to guarantee the non-singularity of the kernel matrix A. A small λ leads to the ill-condition of the matrix A + λI and a large λ conducts large approximation error. This reflects the known tug of war between variance and bias. In short, the overfitting phenomenon of polynomial kernel is caused by the algorithm-based factor rather than model selection. Thus, we can design a more efficient algorithm directly instead of imposing regularization term in the model to reduce the computational burden. This will be the main topic of the next section.\nMarch 10, 2015 DRAFT\n10"
    }, {
      "heading" : "IV. AN EFFICIENT MODEL SELECTION FOR POLYNOMIAL KERNEL REGRESSION",
      "text" : "In this section, we propose a feasible model selection method for polynomial kernel regression based on the theoretical analysis proposed in Section 3 and design a learning algorithm with low computational complexity. It is analyzed that the regularization term in the model\narg min f∈Hs\n{\n1\nm\nm ∑\ni=1\n(f(xi)− yi)2 + λ‖f‖2s }\nis to overcome the ill-condition of the kernel matrix A. And the model\narg min f∈Hs\n{\n1\nm\nm ∑\ni=1\n(f(xi)− yi)2 }\ncan realize the almost optimality of regression. Both of these make it possible to select a new and more efficient model for polynomial kernel regression. Noting that for arbitrary s, Hs = Pds , we can rewrite the above optimization problem as\narg min f∈Pds\n{\n1\nm\nm ∑\ni=1\n(f(xi)− yi)2 } .\nRecalling that the dimension of Pds is n = ( s+d s ) , we can find {ηi}ni=1 ⊂ Bd such that {(1 + ηi · x)s}ni=1 is a linear independent system. Then,\nPds = { n ∑\ni=1\nci(1 + ηi · x)s : ci ∈ R } =: Hη,n. (9)\nHence, the above optimization problem can be converted to {\narg min f∈Hη,n\n1\nm\nm ∑\ni=1\n(f(xi)− yi)2 } . (10)\nThus, there are two things we should do. The one is to give a selection strategy of {ηi}ni=1 and the other is to guarantee the non-singularity of the matrix Am,n := ((1 + xi · ηj)s)m,ni,j=1.\nTo this end, we should introduce the conceptions of Haar space and fundamental system [34]. Let V ∈ C(Bd) be an N-dimensional linear space. V is called a Haar space of dimension N if for arbitrary distinct points x1, . . . , xN ∈ Bd and arbitrary f1, . . . , fN ∈ R there exists exactly one function s ∈ V with s(xi) = fi, 1 ≤ i ≤ N . The following Lemma 2 [34, Theorem 2.2] shows some important properties of Haar space.\nLemma 2: The following statements are equivalent. (1) V is N-dimensional Haar space. (2) Every u ∈ V /{0} has at most N − 1 zeros. (3) For any distinct points x1, . . . , xN ∈ Bd and any basis u1, . . . , uN of V , the matrix (uj(xi))Ni,j=1 is non-singular.\nMarch 10, 2015 DRAFT\n11\nOf course, if we can find a set of points in Bd, {ηi}ni=1, such that Hη,n is the Haar space of dimension n + 1, then it follows from Lemma 2 that all above problems can be resolved. However, for d ≥ 2, this conjecture does not hold [34, Theorem 2.3]. Lemma 3: Suppose d ≥ 2. Then there does not exist Haar space on Bd of dimension N ≥ 2. Based on this, we introduce the conception of fundamental system with respect to the polynomial kernel Ks\nDefinition 1: Let ζ := {ζi}ni=1 ⊂ Bd. ζ is called a Ks-fundamental system if\ndimHζ,n = ( s+d s ) .\nFrom the above definition, it is easy to see that arbitrary Ks-fundamental system implies (9). The following Proposition 1 reveals that almost all n = (n+ss ) points set is the Ks-fundamental system.\nProposition 1: Let s, n ∈ N and n = (n+ss ) . Then the set\n{ζ = (ζi)ni=1 : dimHζ,n < n}\nhas Lebesgue measure 0.\nBased on Proposition 1, we can design a simple strategy to choose the centers {ηj}nj=1. Since the uniform distribution is continuous with respect to Lebesgue measure [4], we can draw {ηj}nj=1 independently and identically according to the uniform distribution. Then with probability 1, there holds\nPds = { n ∑\ni=1\nci(1 + ηi · x)s : ci ∈ R } .\nNow we turn to prove the non-singularity of the matrix Am,n, which can be implied from the\nfollowing Proposition 2.\nProposition 2: Let s,m, n ∈ N. If {xi}mi=1 are i.i.d. random variables drawn according to arbitrary distribution µ, and {ηj}nj=1 is a Ks-fundamental system. Then for arbitrary vector c = (c1, . . . , cn),\n∫\nBd\n\n\nn ∑\nj=1\ncjKs(ηj , x)\n\n\n2 dx\n√ 1− |x|2 ≤ 1 m (Am,nc) TAm,nc ≤ 3 ∫ Bd\n\n\nn ∑\nj=1\ncjKs(ηj , x)\n\n\n2 dx\n√ 1− |x|2 .\nholds with probability at least 1− Cnd m , where C is a constant depending only on d.\nMarch 10, 2015 DRAFT\n12\nIt can be easily deduced from Proposition 2 that with probability at least 1− Cnd m , the matrix Am,n is non-singular. Indeed, if Am,n is non-singular, then it follows from Proposition 2 that there exists a nontrivial set {cj}nj=1 such that ∫\nSd\n\n\nn ∑\nj=1\ncjKs(ηj, x)\n\n\n2 dx\n√ 1− |x|2 = 0.\nThis implies n ∑\nj=1\ncjKs(ηj , x) = 0, x ∈ Bd,\nwhich is impossible since {ηj} is a Ks-fundamental system. In the help of the above two propositions, we give an efficient algorithm, called efficient polynomial kernel regression (EPKR), based on the model selection strategy (10).\nAlgorithm 1 Efficient polynomial kernel regression (EPKR) Input: Let (xi, yi)mi=1 be m samples, s ∈ N be the degree of polynomial kernel and Ks(x, x′) = (1 + x · x′)s be the polynomial kernel Step 1: Let n = (\ns+d s\n)\nbe the number of centers and {ηj}nj=1 be the set of centers, which is a Ks fundamental system. {ηj}nj=1 can be drawn independently and identically according to the uniform distribution. Set Am,n := (Ks(xi, ηj)) m,n i,j=1, y = (y1, . . . , ym) T . Step 2: Set c = pinv(Am,n)y = (c1, . . . , cn)T , where pinv(Am,n) denotes the pseudo-inverse operator in matlab. Output: fz,s(x) = ∑n j=1 cjKs(ηj, x).\nIt can be found that there is only one parameter s in EPKR. To fix s, we can use the socalled “cross-validation” method [14, Chapter 8] or “hold out” method [14, Chapter 7]. To be precise, we explain the latter one. There are three steps to implement the “hold out ” strategy: (i) Splitting the sample set into two independent subsets z1 and z2, (ii) using z1 to build the sequence {fz,s}⌈m 1/d⌉ s=1 and (iii) using z2 to select a proper value of s and thus yield the final estimator fz. Noting that the choice of s is from 1 to ⌈m1/d⌉, if d is large, then then ⌈m1/d⌉ is always a small value. The following Theorem 2 illustrates the generalization capability of EPKR.\nMarch 10, 2015 DRAFT\n13\nTheorem 2: Let r ∈ N, fρ ∈ W r∞, and fz be the EPKR estimator. Then,\nC1m − 2r d+2r ≤ em(W r∞) ≤ sup ρ∈M(W r ∞ ) Eρm(‖fρ −ΠMfz‖2ρ) ≤ C2m− 2r d+2r logm.\nTheorem 2 shows that the selected model (10) is almost optimal choice if the smoothness information of the regression function is known. Furthermore, the pseudo-inverse technique is sufficient to realize the almost optimality of the model (10). Furthermore, it can be easily deduced that the computational complexity of EPKR is very small compared to the classical polynomial kernel regression method (3). Indeed, for fixed s and n = (\ns+d d\n)\n, the computational complexity\nis mn2, while that of (3) is m3."
    }, {
      "heading" : "V. EXPERIMENTAL RESULTS",
      "text" : "In this section, we give both toy and UCI standard data simulations of the model section strategy for polynomial kernel regression and the EPKR algorithm. All the numerical simulations are carried out in Matlab R2011b environment running Windows 7, Intel(R) Core(TM) i7-3770K CPU@ 3.50GHz 3.50 GHz."
    }, {
      "heading" : "A. Toy simulation",
      "text" : "1) Experimental setting: In this part, we introduce the simulation setting of the toy experiment. Method choices: In the toy simulation, there are four methods being employed. The first one is Gaussian kernel regression; the second one is the classical polynomial kernel regression; the third one is the efficient polynomial kernel regression (EPKR) whose centers {ηj}nj=1 are drawn independently and identically to the uniform distribution; the last one is the EPKR whose centers {ηj}nj=1 be the first n points in the sample data x. Samples: In the simulation, the training samples are generated as follows. Let f(t) = (1 − 2t)5+(32t 2 + 10t + 1), where t ∈ [0, 1] and a+ = max{a, 0}. Then it is easy to see that f ∈ W 4∞([0, 1]) and f /∈ W 5∞([0, 1]). Let x = {xi}mi=1 be drawn independently and identically according to the uniform distribution with m = 1000 and y = {yi}mi=1 = f(xi) + δi, where the noise {δi}mi=1 are drawn independently and identically according to the Gaussian distribution N(0, σ2) with σ2 = 0.1. The test samples are generated as follows. x′ = {x′i}m ′ i=1 are drawn independently and identically according to the uniform distribution with m′ = 1000 and yi = f(xi). In the numerical experiment, TestRMSE is the mean square root error (RMSE)\nMarch 10, 2015 DRAFT\n14\nof the testing data via 10 times simulation. TrainRMSE is the mean RMSE of the training data via 10 times simulation. TrainMT denotes the mean training time via 10 times simulation. And TestMT denotes the mean testing time via 10 times simulation.\n2) Simulation results: In the first simulation, we study the action of the regularization term in the classical polynomial kernel regression (3). In the left figure of Fig.1, it can be found that there exists an optimal λ minimizing the TestRMSE for optimal selected s. This only means that introducing the penalty in (3) can avoid overfitting. Recalling Theorem 1, the action of regularization term in (3) is to avoid the ill-condition of the kernel matrix. Thus, more simulations are required. To this end, we introduce the coefficient-based regularization EPKR (CBR EPKR). The CBR EPKR is the algorithm which using\nc = pinv(Am,n + λIm,n)y\ninstead of Step 2 in the EPKR algorithm, where Im,n = (ai,j) m,n i,j=1 is the matrix with ai,i = 1, and ai,j = 0, i 6= j. For λ = 0, the coefficient-based regularization EPKR algorithm coincides with EPKR. If the overfitting phenomenon is caused by the model-based factor, i.e., the hypothesis space of (3) or (10) is too large, then there may exist an optimal λ > 0 in the middle figure of Fig. 1, which has not witnessed in Fig. 1. Indeed, the TestRMSE is a monotonously increasing function with respect to the regularization parameter λ. This means that the capacity of hypothesis space of (3) is not large and suitable for the learning task. Thus, we can draw a conclusion form Fig.1 that the essential effect of the penalty in (10) is to overcome the ill-condition of the kernel matrix.\nReaders can find an interesting phenomenon in Fig.1. There is a λ1 in the middle and right figures of Fig.1 such that for 0 ≤ λ ≤ λ1, the TestRMSE is a linear function with respect to λ, while for λ > λ1, the slope decreases. We give a simple explanation of this phenomenon. The generalization error can be decomposed into approximation error and sample error. It is obvious that the approximation error is a linear function with respect to the regularization parameter λ. However, the relation between the sample error and λ is more sophisticated. It is easy to see that the hypothesis space belongs to the set {\nP ∈ Hn,η : n ∑\ni=1\n|ai|2 ≤ M2/λ, P = n ∑\ni=1\naiKs(ηi, x)\n}\n.\nRoughly speaking, if λ < λ1, the covering number of the hypothesis space is larger than the quantity appearing in Lemma 5 for a fixed ε. When λ increase to λ1, the covering number of\nMarch 10, 2015 DRAFT\n15\nthe hypothesis space decreases. Once the covering number of the hypothesis space is strictly smaller than the mentioned quantity, the sample error decreases with respect to λ. Thus, the plus of approximation and sample errors is not a linear function with respect to λ and the slope decreases according to λ.\nIn the next simulation, we study the importance of s in both model (3) and model (10). Based on the results in Fig. 1, in the upper left figure of Fig. 2, we study the relation between TestRMSE and s for model (3), where λ is the optimal value of 50 candidates drawn equally spaced in [10−5, 1]. It can be found that there exists an optimal s minimizing TestRMSE. Since f ∈ W 2∞([0, 1]), it follows from Theorem 1 that the optimal s may close to the value ⌈m1/(2r+d)⌉ = 4. It is shown in the upper right figure that the optimal s of (3) is 7 in our setting. The lower figures depict the relation between TestRMSE and s for EPKR. It can found in both of the lower figures of Fig. 2 that there is an optimal s minimizing TestRMSE and the optimal value of s is 5, which also coincides with the theoretical analysis in Theorem 2.\nIn the third simulation, we study the action of the choice of η in EPKR. We compare the following four methods of choosing η in (10). EPKR denotes that η = {ηi}ni=1 are drawn i.i.d according to the uniform distribution. EPKR1 denotes that {ηi}ni=1 are selected as the first n elements of samples. EPKRF denotes that {ηi}ni=1 are chosen as the n equally spaced points in [0, 1]. EPKRG denotes that {ηi}ni=1 are generated i.i.d. according to the Gaussian distribution N (1/2, 1). It can be found in Fig. 3 that for for suitable s, the choice of η doesn’t effect the\nMarch 10, 2015 DRAFT\n16\nlearning capability. This verifies the theoretical result of Proposition 1.\nIn the last simulation, we study the learning capabilities of four methods: classical polynomial kernel regression (3), Gaussian kernel regression [15, eqs.(4)], EPKR and EPKR1. It can be seen from Fig. 4 that the learned functions of all the mentioned methods are almost the same. Since both the Gaussian kernel and polynomial kernel are infinitely smooth function and the regression function is at most 2-th smoothness, all of them cannot approximate the regression function within a very small tolerance. This coincides with the lower bound of Theorem 1 and Theorem 2.\nTable 1 shows a quantative comparison among the aforementioned methods. It can be found that all of them possess the similar TestRMSE. However, since there are two parameters in Gaussian kernel regression and classical polynomial kernel regression, large amount of computations are required to select a suitable model, i.e., to tune λG, δ in Gaussian kernel regression\nMarch 10, 2015 DRAFT\n17\nand λP , s in classical polynomial kernel regression. In this simulation, we use the three-fold cross-validation [14, Chapter 8] to choose these parameters. We choose 50 candidates of λG and λP as {10−5, 10−5 + 10−2, . . . , 10−5 + 49× 10−2}, 50 candidates of s as {1, 2, . . . , 50}, 40 candidates of δ as {0.01, 0.01 + 0.025, . . . , 0.01 + 39× 0.025}. There are only one parameter s of EPKR and EPKR1. We also use the three-fold cross-validation method to choose the optimal s from {1, . . . , 50}. It can be found in Table 1 that the training time of EPKR and EPKR1 are much less than that of the other two methods. The main reason of this phenomenon is based on the following assertions. On one hand, there is only one parameter need tuning in EPKR. On the other hand, the computational complexity of EPKR is O(mn2), which is smaller than O(m3) for small s. Noting that the deduced EPKR (or EPKR1) estimator is a linear combination of n = 5 basis function, while those of Gaussian kernel regression and polynomial kernel regression are 1000, the test time of EPKR and EPKR1 is much less than that of the other two methods.\nMarch 10, 2015 DRAFT\n18"
    }, {
      "heading" : "B. UCI data",
      "text" : "1) Experimental setting: In this part, we introduce the simulation setting of the UCI data ex-\nperiment. All the data are cited from http://www.niaad.liacc.up.pt/∼ltorgo/Regression/ds menu.html. Method choices: In the UCI data experiment, we compare four methods containing support vector machine (SVM) [35], Gaussian kernel regression (GKR) [15, Eqs.(4)], classical polynomial kernel regression (3) (PKR) and EPKR on 9 real-world benchmark data sets covering various\nMarch 10, 2015 DRAFT\n19\nfields. We use three-fold cross-validation to select parameters of the aforementioned methods among 40 candidates of the width of Gaussian kernel and 50 candidates of the regularization parameter λ. However, due to the theoretical analysis proposed in Theorem 1, there are only {1, . . . , ⌈m1/d⌉} candidates of polynomial kernel parameter s. The centers of EPKR are drawn i.i.d according to the uniform distribution on [0, 1].\nSamples: The training and testing samples are drawn according to the following Table 2.\n2) Experimental results: As shown in Table 3, the TrainRMSE and TestRMSE of all the mentioned methods are similar. But as far as the TrainMT is concerned, it can be found in\nTable 4 that EPKR outperforms the others. It can also be found in Table 4 that the TrainMT of PKR is smaller than GKR and SVM. This is because we use the theoretical result in Theorem 1 to select the kernel parameter s. It is shown in Theorem 1 that it suffices to select s in the set {1, . . . , ⌈m1/d⌉}. This degrades the difficulty of model selection of PKR. Since the TestMT depends heavily on the sparsity of the estimator, we give a comparison of the sparsity of the mentioned methods in Table 4, too. In short, as far as the generalization capability is concerned, all of these methods are of high quality. However, as far as the computational burden is concerned, EPKR is superior to the others. Furthermore, different from the classical polynomial kernel regression, EPKR can deduce sparse estimators. In addition, by our theoretical analysis, the computational burden of PKR can be heavily reduced.\nMarch 10, 2015 DRAFT\n20"
    }, {
      "heading" : "VI. PROOFS",
      "text" : "tration inequality can be found in [3, Lemma 3.2].\nLemma 4: Let F be a class of functions that are all bounded by M . For all m and α, β > 0, we have\nPρm { ∃f ∈ F : ‖f − fρ‖2ρ ≥ 2(‖y − f‖2m − ‖y − fρ‖2m) + α + β }\nMarch 10, 2015 DRAFT\n21\n≤ 14 sup x\nN ( β\n40M ,F , L1(νx)\n)\nexp ( − αn 2568M4 ) ,\nwhere x = (x1, . . . , xm) ∈ Xm and N (t,F , L1(νx)) is the covering number for the class F by balls of radius t in L1(νx), with νx = 1m ∑m i=1 δxi the empirical discrete measure.\nThe second one focusing on covering number estimation is deduced from [14, Chapter 9]. Lemma 5: Let ΠMHs := {ΠMf : f ∈ Hs}. Then,\nN (ǫ, πMHs, L1(νx)) ≤ 3 ( 2eBp\nǫp log\n3eBp\nǫp\n)(s+dd ) .\nThe third one presents the minimal eigenvalue estimator of a matrix generated by the poly-\nnomial kernel, which can be found in [20, Theorem 20].\nLemma 6: Let s ∈ N, n = (\ns+d d\n)\n, Sd−1 be the unit sphere in Rd, and {ξi}ni=1 ⊂ Sd−1. Then the minimal eigenvalue of the matrix Aξ := (1 + ξiξj)ni,j=1, µmin(Aξ), satisfies\nµmin(Aξ) ≥ s!Γ(d/2)\n2sΓ(s+ d/2) .\nTo provide the last lemma, we should introduce the best approximation operator. A function\nη is said to be admissible [26] if η ∈ C∞[0,∞), η(t) ≥ 0, and\nsuppη ⊂ [0, 2], η(t) = 1 on [0, 1], and 0 ≤ η(t) ≤ 1 on [1, 2].\nSuch a function can be easily constructed out of an orthogonal wavelet mask [10]. Let\nhk := π1/2Γ(d+ k)Γ((d+ 1)/2)\n(k + d/2)k!Γ(d/2) Γ(d).\nDefine\nUk := (hk) −1/2G d/2 k , k = 0, 1, . . . , (11)\nwhere Gµk is the well known Gegenbauer polynomial with order µ [26]. The best approximation kernel is defined by\nLs(x, y) := ∞ ∑\nk=0\nη\n(\nk\n2s\n)\nv2k\n∫\nSd−1 Uk(x · ξ)Uk(y · ξ)dωd−1(ξ),\nwhere dωd−1 stands for the aero element of Sd−1. It can easily deduced from [26] that\n|Ls(x, y)| ≤ Csd, x, y ∈ Bd. (12)\nLet\nEs(f)p := inf P∈Pds\n‖f − P‖Lp(Bd)\nMarch 10, 2015 DRAFT\n22\nbe the best approximation error of Pds . Define\nLsf(x) := ∫\nBd Ls(x, y)f(y)dy. (13)\nIt is obvious that Lsf ∈ Pds . The following Lemma 7 which can be found [?, Section 3] shows the best approximation property of (Lsf)(x).\nLemma 7: Let 1 ≤ p ≤ ∞, and Ls be defined in (13), then for arbitrary f ∈ Lp(Bd), there exists a constant C depending only on d and p such that\n‖f − Lsf‖Lp(Bd) ≤ CE[s/2](f)p.\nNow we give the proof of Theorem 1.\nProof of Theorem 1: We write\n‖ΠMfz,λ,s − fρ‖2ρ := T1(z, λ, s) + T2(z, λ, s),\nwhere\nT1(z, λ, s) := ‖ΠMfz,λ,s − fρ‖2ρ − 2(‖y −ΠMfz,λ,s‖2m − ‖y − fρ‖2m)\nand\nT2(z, λ, s) := 2(‖y − ΠMfz,λ,s‖2m − ‖y − fρ‖2m).\nTo bound T1(z, λ, s), we use Lemma 4 and Lemma 5 and obtain\nPρm{T1(z, λ, s) > u}\n= Pρm { ‖ΠMfz,λ,s − fρ‖2ρ ≥ 2(‖y −ΠMfz,λ,s‖2m − ‖y − fρ‖2m) + u\n2 +\nu 2\n}\n≤ Pρm { ∃f ∈ ΠMHs : ‖f − fρ‖2ρ ≥ 2(‖y − f‖2m − ‖y − fρ‖2m) + u\n2 +\nu 2\n}\n≤ 14 sup x\nN ( u\n80M ,ΠMHs, L1(νx)\n) exp ( − um 5136M4 )\n≤ 42 ( 160eM2\nu log\n240eM2\nu\n)(s+dd ) exp (\n− um 5136M4\n)\nFor arbitrary u > 160eL4/m, we then obtain\nPρm{T1(z, λ, s) > u} ≤ 42 ( mM2 )2(s+dd ) exp ( − um 5136M4 ) .\nMarch 10, 2015 DRAFT\n23\nThen, we get, for any v > 160eL4/m,\nEρm{T1(z, λ, s)} ≤ v + ∫ ∞\nv Pρm{T1(z, λ, s) > t}dt\n≤ v + ∫ ∞\nv 42\n( mM2 )2(s+dd ) exp ( − tm 5136M4 ) dt\n≤ v + 42 ( mM2 )2(s+dd ) 5136M\n4\nm exp\n(\n− vm 5136M4\n)\n.\nSetting\nv = 5136M4\nm log\n(\n42 ( mM2 )2(s+dd )\n)\n,\nwe have\nEρm{T1(z, λ, s)} ≤ C sd logm\nm . (14)\nNow we turn to bound T2(z, λ, s). It follows from the definition of the truncation operator\nΠM and fz,λ,s that\nT2(z, λ, s) = 2(‖y −ΠMfz,λ,s‖2m − ‖y − fρ‖2m)\n≤ 2(‖y − fz,λ,s‖2m − ‖y − fρ‖2m) ≤ 2(‖y − fz,λ,s‖2m + λ‖fz,λ,s‖2 − ‖y − fρ‖2m) ≤ 2(‖y −Lsfρ‖2m + λ‖Lsfρ‖2 − ‖y − fρ‖2m).\nTherefore, the definition of fρ yields that\nEρm{T2(z, λ, s)} ≤ 2(E(Lsfρ)− E(fρ) + λ‖Lsfρ‖2).\nThen, (1) yields that\nEρm{T2(z, λ, s)} ≤ 2‖Lsfρ − fρ‖2ρ + 2λ‖Lsfρ‖2∞.\nSince fρ ∈ L∞(Bd), Lemma 7 implies\nEρm{T2(z, λ, s)} ≤ C(E[s/2](fρ)∞)2 + 2λ‖Lsfρ‖2∞, (15)\nwhere C is a constant depending only on d. The only thing remainder is to bound ‖Lsfρ‖2∞. To this end, let x0 ∈ Bd satisfying\n‖Ls(x0, ·)‖2∞ := sup x∈Bd ‖Ls(x, ·)‖2∞.\nMarch 10, 2015 DRAFT\n24\nThen it follows from ‖fρ‖∞ ≤ M almost surely that\n‖Lsfρ‖2∞ = ∥ ∥ ∥ ∥ ∫\nBd Ls(x, ·)fρ(x)dx\n∥ ∥ ∥ ∥ 2\ns ≤ CM2‖Ls(x0, ·)‖2s.\nAs Ls(x0, ·) ∈ Pds , for arbitrary {ξi}ni=1 ⊂ Sd−1 with n = ( s+d s ) , there holds\nLs(x0, x) = n ∑\ni=1\nci(1 + ξi · x)s.\nAnd c = (c1, . . . , cn)T satisfies\nc = A−1ξ L,\nwhere Aξ := ((1 + ξiξj)s)ni,j=1 and L := (Ls(x0, ξ1), . . . , Ls(x0, ξn)). Furthermore, it follows from Lemma 6 that n ∑\ni=1\n|ci|2 ≤ 2sΓ(s+ d/2)\ns!Γ(d/2)\nn ∑\ni=1\n|Ls(x0, ξi)|2.\nThen (12) together with simple computation implies n ∑\ni=1\n|ci|2 ≤ C2ss3d.\nTherefore,\n‖Lsfρ‖s ≤ CM2‖Ls(x0, ·)‖s = CM2 ∥ ∥ ∥ ∥\n∥\nn ∑\ni=1\nci(1 + ξi·)s ∥ ∥ ∥ ∥\n∥\n2\ns\n≤ CM2 n ∑\ni=1\n|ci|‖(1 + ξi·)s‖s\n≤ C12s/2 √ n ( n ∑\ni=1\n|ci|2 )1/2 ≤ C22ss2d ≤ C3(4d)s.\nThe above inequalities together with (15) and (14) yield\nEρm{‖ΠMfz,λ,s − fρ‖2ρ} ≤ C ( sd logm\nm + (E[s/2](fρ)∞)\n2 + λ(4d)2s ) .\nThe proof of (6) is finished.\nTo prove (7) noting that the middle inequality can be deduced directly from the definition of em(W r∞) and the left inequality is proved in [13, Chapter 3], it suffices to prove the right inequality. Since fρ ∈ W r∞, the well known Jackson inequality [12] shows that\n(E[s/2](fρ)∞) 2 ≤ Cs−2r.\nThus, let s = ⌈m1/(2r+d)⌉, (7) holds obviously for any 0 ≤ λ ≤ m− 2r2r+d (4d)− 12r+d . This completes the proof of Theorem 1.\nMarch 10, 2015 DRAFT\n25\nProof of Proposition 1: Let ζ := {ζj}nj=1 ⊂ Bd. Suppose that there exists a non-trivial set {ai}nj=1 such that\nn ∑\nj=1\naj(1 + ζj · x)s = 0, x ∈ Bd.\nThen the system of equations n ∑\nj=1\naj(1 + ζj · ζk)s = 0, k = 1, . . . , n\nis solvable. Noting that\n(1 + ζj · ζi)s = s ∑\nk=0\n(sk) (ζj · ζi)k = s ∑\nk=0\n(sk) ∑\n|α|=k\nCkαζ α j ζ α i ,\nwe obtain n ∑\ni,j=1\naiaj(1 + ζi · ζj)s = s ∑\nk=0\n(sk) ∑\n|α|=k\nCkα\n(\nn ∑\ni=1\naiζ α i\n)2\n,\nwhere\nCkα = d!\nα1! · · ·αd! , α := (α1, . . . , αd).\nLet\nP (x) := n ∑\ni=1\naix α.\nIf\n{ζ = (ζi)ni=1 : dimHζ,n < n} ,\nthen ζi, i = 1, . . . , n are n distinct zero points of P . Noting that the degree of P is at most s, then it can be easily deduced from [4, Lemma 3.1] that the zero set of P ,\nZ(p) := {x ∈ Bd : P (x) = 0}\nhas Lebesgue measure 0. This completes the proof of Proposition 1.\nTo prove Proposition 2, we need the following two lemmas. The first one establishes a relation between the d-dimension unit ball Bd and the d + 1 dimension unit sphere Sd, which can be found in [38, Lemma 2.1].\nLemma 8: For any continuous function f defined on Sd, there holds ∫\nSd f(ξ)dωd(ξ) =\n∫\nBd\n[\nf(x, √ 1− |x|2) + f(x,− √ 1− |x|2) ] dx √\n1− |x|2 .\nMarch 10, 2015 DRAFT\n26\nLet hΛ be the mesh norm of a set of points Λ = {ξi}mi=1 ⊂ Sd defined by\nhΛ := max ξ∈Sd min j d(ξ, ξj),\nwhere d(ξ, ξ′) is the geodesic (great circle) distance between the points ξ and ξ′ on Sd. The second one is the well known cubature formula on the sphere, which can be found in [21].\nLemma 9: If there exists a constant c such that hΛ ≤ n−c/d, then there exists a set of numbers {ai}mi=1 satisfying\nm ∑\ni=1\n|ai|p ≤ Cm1−p.\nsuch that ∫\nSd P (y)dω(y) =\nm ∑\ni=1\naiP (xi) for any P ∈ Πd2n.\nProof of Proposition 2: Based on Lemma 8 and Lemma 9, it suffices to prove for arbitrary\nε > 0, with confidence at least 1− c mεd , there holds hΛ ≤ ε. At first, we present an upper bound of hΛ. Let D(ξ, r) be the spherical cap with center ξ and radius r. Then for arbitrary ε > 0, due to the definition of the mesh norm, we obtain\nP{hΛ > ε} = P{max ξ∈Sd min j d(ξ, ξj) > ε} ≤ E{(1− µ(D(ξ, ε)))m}.\nLet t1, . . . , tN be the quasi-uniform points [34] on the sphere. Then it is easy to deduce that there exists a constant c > 0 such that\nN ≤ c εd , and Sd ⊂\nN ⋃\nj=1\nD(tj , ε/2).\nIf ξ ∈ D(tj , ε/2), then D(tj, ε/2) ⊂ D(ξ, ε). Therefore, we get\nE{(1− µ(D(ξ, ε)))m} ≤ N ∑\nj=1\n∫\nD(tj ,ε/2) (1− µ(D(ξ, ε)))mdµ\n≤ N ∑\nj=1\n∫\nD(tj ,ε/2) (1− µ(D(tj, ε/2)))mdµ =\nN ∑\nj=1\nµ(D(tj, ε/2))(1− µ(D(tj, ε/2)))m\n≤ N ∑\nj=1\nmax u\nu(1− u)m ≤ N ∑\nj=1\nmax u\nue−mu = eN\nm\n≤ c mεd .\nThat is,\nP{hΛ > ε} ≤ c\nmεd .\nMarch 10, 2015 DRAFT\n27\nThis finishes the proof of Proposition 2\nProof of Theorem 2: The proof of Theorem 2 is almost the same as that of Theorem 1.\nNoting s ∈ [ 1, ⌈m1/d⌉ ] and ⌈ m1/(2r+d) ⌉ ∈ [ 1, ⌈m1/d⌉ ]\nfor arbitrary r ≥ 0, Theorem 2 can be easily deduced from Theorem 1. For the sake of brevity, we omit the details."
    }, {
      "heading" : "VII. CONCLUSION",
      "text" : "The main contributions of the present paper can be summarized as follows. Firstly, we study the parameter selection problem in polynomial kernel regression. After our analysis, we find that the essential role of the regularization term is to overcome the ill-condition phenomenon of the kernel matrix. Indeed, as far as the model selection is concerned, arbitrarily small regularization parameter can yield the almost optimal learning rate. Secondly, we improve the existing results about polynomial kernel regression in the following directions: building a distribution-free theoretical analysis, extending the range of regression function and establishing the almost optimal learning rate. Thirdly, based on the aforementioned theoretical analysis, we propose a new model concerning polynomial kernel regression and design an efficient learning algorithm. Both theoretical and experimental results show that the new method is of high quality."
    }, {
      "heading" : "ACKNOWLEDGEMENT",
      "text" : "The research was supported by the National 973 Programming (2013CB329404) and the\nNational Natural Science Foundation of China (Grant No. 11401462)."
    } ],
    "references" : [ {
      "title" : "A survey of the state of the art in learning the kernels",
      "author" : [ "M. Abbasnejad", "D. Ramachandram", "R. Mandava" ],
      "venue" : "Knowl. Inf. Syst., 31:193-221",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Theory of reproducing kernels",
      "author" : [ "N. Aronszajn" ],
      "venue" : "Trans. Amer. Soc., 68: 337-404",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 1950
    }, {
      "title" : "Approximation and learning by greedy algorithms",
      "author" : [ "A.R. Barron", "A. Cohen", "W. Dahmen", "R.A. Devore" ],
      "venue" : "Ann. Statist., 36: 64-94",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Random sampling of multivariate trigonometric polynomials",
      "author" : [ "R. Bass", "K. Gröchenig" ],
      "venue" : "SIAM. J. Math. Anal., 36: 773- 795",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Fast rates for regularized least-squares algorithm",
      "author" : [ "A. Caponnetto", "E. De Vito" ],
      "venue" : "No. AI-MEMO-2005-013. Massachusetts inst of tech Cambridge computer science and artificial intelligence lab",
      "citeRegEx" : "5",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Optimal rates for the regularized least squares algorithm",
      "author" : [ "A. Caponnetto", "E. DeVito" ],
      "venue" : "Found. Comput. Math., 7: 331-368",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "On the mathematical foundations of learning",
      "author" : [ "F. Cucker", "S. Smale" ],
      "venue" : "Bull. Amer. Math. Soc., 39: 1-49",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Best choices for regularization parameters in learning theory: on the bias-variance problem",
      "author" : [ "F. Cucker", "S. Smale" ],
      "venue" : "Found. Comput. Math., 2: 413-428",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Learning Theory: An Approximation Theory Viewpoint",
      "author" : [ "F. Cucker", "D.X. Zhou" ],
      "venue" : "Cambridge University Press, Cambridge",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Ten Lectures on Wavelets",
      "author" : [ "I. Daubechies" ],
      "venue" : "SIAM, Philadelphia",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 1992
    }, {
      "title" : "Model selection for regularized least-squares algorithm in learning theory",
      "author" : [ "E. De Vito", "A. Caponnetto", "L. Rosasco" ],
      "venue" : "Found. Comput. Math., 5: 59-85",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Constructive Approximation",
      "author" : [ "R.A. DeVore", "G.G. Lorentz" ],
      "venue" : "Springer-Verlag, Berlin",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 1993
    }, {
      "title" : "Approximation methods for supervised learning",
      "author" : [ "R.A. Devore", "G. Kerkyacharian", "D. Picard", "V. Temlyakov" ],
      "venue" : "Found. Comput. Math., 6: 3-58",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "A Distribution-Free Theory of Nonparametric Regression",
      "author" : [ "L. Györfy", "M. Kohler", "A. Krzyzak", "H. Walk" ],
      "venue" : "Springer, Berlin",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Optimal learning rates for least squares SVMs using Gaussian kernels",
      "author" : [ "M. Eberts", "I. Steinwart" ],
      "venue" : "Advances in Neural Information Processing Systems 24 : 1539-1547",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Multiple Kernel Learning Algorithms",
      "author" : [ "M. Gönen", "E. Alpaydm" ],
      "venue" : "J. Mach. Learn. Res., 12: 2211-2268",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "The Elements of Statistical Learning",
      "author" : [ "T. Hastie", "R. Tibshirani", "J. Friedman" ],
      "venue" : "Springer, New York",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Learning the Kernel Function via Regularization",
      "author" : [ "C.A. Micchelli", "M. Pontil" ],
      "venue" : "J. Mach. Learn. Res., 6: 1099-1125",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Universal Kernels",
      "author" : [ "C.A. Micchelli", "Y. Xu", "H. Zhang" ],
      "venue" : "J. Mach. Learn. Res., 7: 2651-2667",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Reproducing kernel Hilbert spaces in learning theory",
      "author" : [ "H.Q. Minh" ],
      "venue" : "Ph. D. Thesis in Mathematics, Brown University",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Spherical Marcinkiewicz-Zygmund inequalities and positive quadrature",
      "author" : [ "H.N. Mhaskar", "F.J. Narcowich", "J.D. Ward" ],
      "venue" : "Math. Comput., 70: 1113-1130",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "Localized tight frames on spheres",
      "author" : [ "F.J. Narcowich", "P. Petrushev", "J.D. Ward" ],
      "venue" : "Siam J. Math. Anal., 38: 574-594",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Machine learning using hyperkernels",
      "author" : [ "C.S. Ong", "A.J. Smola" ],
      "venue" : "Proceedings of the International Conference on Machine Learning,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2003
    }, {
      "title" : "A",
      "author" : [ "C.S. Ong" ],
      "venue" : "J. Smola and R. C.Williamson. Learning the kernel with hyperkernels. J. Mach. Learn. Res., 6: 1043-1071",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "More efficiency in multiple kernel learning",
      "author" : [ "A. Rakotomamonjy", "F. Bach", "S. Canu", "Y. Grandvalet" ],
      "venue" : "Proceedings of the 24th international conference on Machine learning,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2007
    }, {
      "title" : "Localized polynomial frames on the ball",
      "author" : [ "P.P. Petrushev", "Y. Xu" ],
      "venue" : "Constr. Approx., 27: 121-148",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Generalized inverse of matrices and its application",
      "author" : [ "C.R. Rao", "S.K. Mitra" ],
      "venue" : "Wiley, New York",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 1971
    }, {
      "title" : "Learning with Kernel: Support Vector Machine",
      "author" : [ "B. Schölkopf", "A.J. Smola" ],
      "venue" : "Regularization, Optimization, and Beyond (Adaptive Computation and Machine Learning). The MIT Press, Cambridge",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Learning theory estimates via integral operators and their approximations",
      "author" : [ "S. Smale", "D.X. Zhou" ],
      "venue" : "Constr. Approx., 26: 153-172",
      "citeRegEx" : "29",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Fast rates for support vector machines using Gaussian kernels",
      "author" : [ "I. Steinwart", "C. Scovel" ],
      "venue" : "Ann. Statist., 35: 575-607",
      "citeRegEx" : "30",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Optimal rates for regularized least squares regression",
      "author" : [ "I. Steinwart", "D. Hush", "C. Scovel" ],
      "venue" : "Proceedings of the 22nd Conference on Learning Theory",
      "citeRegEx" : "31",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Composite kernel learning",
      "author" : [ "M. Szafranski", "Y. Grandvalet", "A. Rakotomamonjy" ],
      "venue" : "Proceedings of the 25th international conference on Machine learning,",
      "citeRegEx" : "32",
      "shortCiteRegEx" : "32",
      "year" : 2015
    }, {
      "title" : "Learning rates for regularized classifiers using multivariate polynomial kernels",
      "author" : [ "H.Z. Tong", "D.R. Chen", "Z.P. Li" ],
      "venue" : "J. Complex., 24: 619-631",
      "citeRegEx" : "33",
      "shortCiteRegEx" : null,
      "year" : 2008
    }, {
      "title" : "Scattered Data Approximation",
      "author" : [ "H. Wendland" ],
      "venue" : "Cambridge University Press, Cambridge",
      "citeRegEx" : "34",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Kernel Methods for Pattern Analysis",
      "author" : [ "J.S. Taylor", "N. Cristianini" ],
      "venue" : "Cambridge University Press, Cambridge",
      "citeRegEx" : "35",
      "shortCiteRegEx" : null,
      "year" : 2004
    }, {
      "title" : "Learning rates of least square regularized regression",
      "author" : [ "Q. Wu", "Y.M. Ying", "D.X. Zhou" ],
      "venue" : "Found. Comput. Math., 6: 171-192",
      "citeRegEx" : "36",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Orthogonal polynomials and cubature formulae on spheres and on balls",
      "author" : [ "Y. Xu" ],
      "venue" : "SIAM J. Math. Anal., 29: 779-793",
      "citeRegEx" : "38",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "Minimax nonparametric classification",
      "author" : [ "Y. Yang" ],
      "venue" : "I. Rates of convergence. II. Model selection for adaptation. IEEE Trans. Inform. Theory, 45: 2271-2292",
      "citeRegEx" : "39",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "Estimation of learning rate of least square algorithm via Jackson operator",
      "author" : [ "Y.Q. Zhang", "F.L. Cao", "Z.B. Xu" ],
      "venue" : "Neurocomputing, 74: 516-521",
      "citeRegEx" : "40",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Approximation with polynomial kernels and SVM classifiers",
      "author" : [ "D.X. Zhou", "K. Jetter" ],
      "venue" : "Adv. Comput. Math., 25: 323-344",
      "citeRegEx" : "41",
      "shortCiteRegEx" : null,
      "year" : 2006
    } ],
    "referenceMentions" : [ {
      "referenceID" : 6,
      "context" : "If the kernel methods [7], [35] are used, then the model selection problem boils down to choosing a suitable kernel and the corresponding regularization parameter.",
      "startOffset" : 22,
      "endOffset" : 25
    }, {
      "referenceID" : 34,
      "context" : "If the kernel methods [7], [35] are used, then the model selection problem boils down to choosing a suitable kernel and the corresponding regularization parameter.",
      "startOffset" : 27,
      "endOffset" : 31
    }, {
      "referenceID" : 17,
      "context" : "After verifying the existences of the optimal kernel [18] and regularization parameter [8], there are two trends of model selection.",
      "startOffset" : 53,
      "endOffset" : 57
    }, {
      "referenceID" : 7,
      "context" : "After verifying the existences of the optimal kernel [18] and regularization parameter [8], there are two trends of model selection.",
      "startOffset" : 87,
      "endOffset" : 90
    }, {
      "referenceID" : 24,
      "context" : "The one is to pursue some prominent kernels containing multi-kernel learning [25], [16], hyperkernel learning [23], [24], and other kernel selection methods [1], [32], [19].",
      "startOffset" : 77,
      "endOffset" : 81
    }, {
      "referenceID" : 15,
      "context" : "The one is to pursue some prominent kernels containing multi-kernel learning [25], [16], hyperkernel learning [23], [24], and other kernel selection methods [1], [32], [19].",
      "startOffset" : 83,
      "endOffset" : 87
    }, {
      "referenceID" : 22,
      "context" : "The one is to pursue some prominent kernels containing multi-kernel learning [25], [16], hyperkernel learning [23], [24], and other kernel selection methods [1], [32], [19].",
      "startOffset" : 110,
      "endOffset" : 114
    }, {
      "referenceID" : 23,
      "context" : "The one is to pursue some prominent kernels containing multi-kernel learning [25], [16], hyperkernel learning [23], [24], and other kernel selection methods [1], [32], [19].",
      "startOffset" : 116,
      "endOffset" : 120
    }, {
      "referenceID" : 0,
      "context" : "The one is to pursue some prominent kernels containing multi-kernel learning [25], [16], hyperkernel learning [23], [24], and other kernel selection methods [1], [32], [19].",
      "startOffset" : 157,
      "endOffset" : 160
    }, {
      "referenceID" : 31,
      "context" : "The one is to pursue some prominent kernels containing multi-kernel learning [25], [16], hyperkernel learning [23], [24], and other kernel selection methods [1], [32], [19].",
      "startOffset" : 162,
      "endOffset" : 166
    }, {
      "referenceID" : 18,
      "context" : "The one is to pursue some prominent kernels containing multi-kernel learning [25], [16], hyperkernel learning [23], [24], and other kernel selection methods [1], [32], [19].",
      "startOffset" : 168,
      "endOffset" : 172
    }, {
      "referenceID" : 14,
      "context" : "The other focuses on selecting optimal regularization parameters for some prevailing kernels, comprising Gaussian kernel [15], [30], [37], polynomial kernel [41], [33] [40], and other more general kernels [6], [11], [31], [36].",
      "startOffset" : 121,
      "endOffset" : 125
    }, {
      "referenceID" : 29,
      "context" : "The other focuses on selecting optimal regularization parameters for some prevailing kernels, comprising Gaussian kernel [15], [30], [37], polynomial kernel [41], [33] [40], and other more general kernels [6], [11], [31], [36].",
      "startOffset" : 127,
      "endOffset" : 131
    }, {
      "referenceID" : 39,
      "context" : "The other focuses on selecting optimal regularization parameters for some prevailing kernels, comprising Gaussian kernel [15], [30], [37], polynomial kernel [41], [33] [40], and other more general kernels [6], [11], [31], [36].",
      "startOffset" : 157,
      "endOffset" : 161
    }, {
      "referenceID" : 32,
      "context" : "The other focuses on selecting optimal regularization parameters for some prevailing kernels, comprising Gaussian kernel [15], [30], [37], polynomial kernel [41], [33] [40], and other more general kernels [6], [11], [31], [36].",
      "startOffset" : 163,
      "endOffset" : 167
    }, {
      "referenceID" : 38,
      "context" : "The other focuses on selecting optimal regularization parameters for some prevailing kernels, comprising Gaussian kernel [15], [30], [37], polynomial kernel [41], [33] [40], and other more general kernels [6], [11], [31], [36].",
      "startOffset" : 168,
      "endOffset" : 172
    }, {
      "referenceID" : 5,
      "context" : "The other focuses on selecting optimal regularization parameters for some prevailing kernels, comprising Gaussian kernel [15], [30], [37], polynomial kernel [41], [33] [40], and other more general kernels [6], [11], [31], [36].",
      "startOffset" : 205,
      "endOffset" : 208
    }, {
      "referenceID" : 10,
      "context" : "The other focuses on selecting optimal regularization parameters for some prevailing kernels, comprising Gaussian kernel [15], [30], [37], polynomial kernel [41], [33] [40], and other more general kernels [6], [11], [31], [36].",
      "startOffset" : 210,
      "endOffset" : 214
    }, {
      "referenceID" : 30,
      "context" : "The other focuses on selecting optimal regularization parameters for some prevailing kernels, comprising Gaussian kernel [15], [30], [37], polynomial kernel [41], [33] [40], and other more general kernels [6], [11], [31], [36].",
      "startOffset" : 216,
      "endOffset" : 220
    }, {
      "referenceID" : 35,
      "context" : "The other focuses on selecting optimal regularization parameters for some prevailing kernels, comprising Gaussian kernel [15], [30], [37], polynomial kernel [41], [33] [40], and other more general kernels [6], [11], [31], [36].",
      "startOffset" : 222,
      "endOffset" : 226
    }, {
      "referenceID" : 8,
      "context" : "Different from other widely used kernels [9], the reproducing kernel Hilbert space Hs of the polynomial kernel Ks = (1 + x · y) is a finite-dimensional vector space, and its dimension depends only on s.",
      "startOffset" : 41,
      "endOffset" : 44
    }, {
      "referenceID" : 39,
      "context" : "Using this fact, [41] found that the regularization parameter in polynomial kernel regression should decrease exponentially fast with the sample size for appropriately selected s.",
      "startOffset" : 17,
      "endOffset" : 21
    }, {
      "referenceID" : 39,
      "context" : "The first purpose of this paper is to continue the study of [41].",
      "startOffset" : 60,
      "endOffset" : 64
    }, {
      "referenceID" : 7,
      "context" : "automatically arises the following question: What is the essential effect of the regularization term in polynomial kernel regression? To answer the above question, we recall that the purpose of introducing regularization term in kernel methods is to avoid the overfitting phenomenon [8], [9], which is the special case that the synthesized function fits the sample very well but fails to fit other points.",
      "startOffset" : 283,
      "endOffset" : 286
    }, {
      "referenceID" : 8,
      "context" : "automatically arises the following question: What is the essential effect of the regularization term in polynomial kernel regression? To answer the above question, we recall that the purpose of introducing regularization term in kernel methods is to avoid the overfitting phenomenon [8], [9], which is the special case that the synthesized function fits the sample very well but fails to fit other points.",
      "startOffset" : 288,
      "endOffset" : 291
    }, {
      "referenceID" : 14,
      "context" : "For example, since the Gaussian-RKHS is an infinite dimensional vector space, the introducing of regularization term in Gaussian kernel regression is to control both the condition number of the kernel matrix and capacity of the hypothesis space [15].",
      "startOffset" : 245,
      "endOffset" : 249
    }, {
      "referenceID" : 6,
      "context" : "By the well known representation theorem [7] in learning theory, the essential hypothesis space of polynomial kernel regression is the linear space H := span{(1+x1·x), · · · , (1+ xm ·x)s}.",
      "startOffset" : 41,
      "endOffset" : 44
    }, {
      "referenceID" : 26,
      "context" : "Then the pseudo-inverse technique [27] can conduct the estimator easily.",
      "startOffset" : 34,
      "endOffset" : 38
    }, {
      "referenceID" : 6,
      "context" : "Z (f(x)− y)dρ, which is minimized by the regression function [7], defined by fρ(x) := ∫",
      "startOffset" : 61,
      "endOffset" : 64
    }, {
      "referenceID" : 6,
      "context" : "(2) It is well known [7], [9], [14] that a small H will derive a large bias ‖fρ−fH‖ρ, while a large H will deduce a large variance E(fz)−E(fH).",
      "startOffset" : 21,
      "endOffset" : 24
    }, {
      "referenceID" : 8,
      "context" : "(2) It is well known [7], [9], [14] that a small H will derive a large bias ‖fρ−fH‖ρ, while a large H will deduce a large variance E(fz)−E(fH).",
      "startOffset" : 26,
      "endOffset" : 29
    }, {
      "referenceID" : 13,
      "context" : "(2) It is well known [7], [9], [14] that a small H will derive a large bias ‖fρ−fH‖ρ, while a large H will deduce a large variance E(fz)−E(fH).",
      "startOffset" : 31,
      "endOffset" : 35
    }, {
      "referenceID" : 1,
      "context" : "Then HK (see [2]) is the closure of the linear span of the set of functions {Kx = K(x, ·) : x ∈ X} with the inner product 〈·, ·〉K satisfying 〈Kx, Ky〉K = K(x, y) and 〈Kx, f〉K = f(x), ∀x ∈ X, f ∈ HK .",
      "startOffset" : 13,
      "endOffset" : 16
    }, {
      "referenceID" : 1,
      "context" : "The following Aronszajn Theorem (see [2]) describes an essential relationship between the RKHS and reproducing kernel.",
      "startOffset" : 37,
      "endOffset" : 40
    }, {
      "referenceID" : 39,
      "context" : "Then it is obvious that [41] for all t ∈ R and y ∈ [−M,M ] there holds E(ΠMfz,λ,s)− E(fρ) ≤ E(fz,λ,s)− E(fρ).",
      "startOffset" : 24,
      "endOffset" : 28
    }, {
      "referenceID" : 7,
      "context" : "The first result, to the best of our knowledge, concerning selection of the optimal regularization parameter in the framework of learning theory belongs to [8].",
      "startOffset" : 156,
      "endOffset" : 159
    }, {
      "referenceID" : 6,
      "context" : "As a streamline work of the seminal paper [7], Cucker and Smale [8] gave a rigorous proof of the existence of the optimal regularization parameter.",
      "startOffset" : 42,
      "endOffset" : 45
    }, {
      "referenceID" : 7,
      "context" : "As a streamline work of the seminal paper [7], Cucker and Smale [8] gave a rigorous proof of the existence of the optimal regularization parameter.",
      "startOffset" : 64,
      "endOffset" : 67
    }, {
      "referenceID" : 7,
      "context" : "After checking the proof of [8] carefully, we find that there is nothing to surprise.",
      "startOffset" : 28,
      "endOffset" : 31
    }, {
      "referenceID" : 7,
      "context" : "On one hand, the optimal parameter mentioned in [8] aims to the generalization error, containing learning rate and the constant C2, while our result only concerns the learning rate.",
      "startOffset" : 48,
      "endOffset" : 51
    }, {
      "referenceID" : 7,
      "context" : "On the other hand, [8]’s result is more suitable to describe the performance of K(·, ·) satisfying ‖f‖∞ ≤ C‖f‖K , where C is a constant independent of m.",
      "startOffset" : 19,
      "endOffset" : 22
    }, {
      "referenceID" : 7,
      "context" : "After [8], we have witnessed the multiple emergence of the selection strategies of regularization parameter.",
      "startOffset" : 6,
      "endOffset" : 9
    }, {
      "referenceID" : 4,
      "context" : "The typical results were reported in [5], [6], [15], [30], [33], [11], [31], [36], [29], [40] and [41].",
      "startOffset" : 37,
      "endOffset" : 40
    }, {
      "referenceID" : 5,
      "context" : "The typical results were reported in [5], [6], [15], [30], [33], [11], [31], [36], [29], [40] and [41].",
      "startOffset" : 42,
      "endOffset" : 45
    }, {
      "referenceID" : 14,
      "context" : "The typical results were reported in [5], [6], [15], [30], [33], [11], [31], [36], [29], [40] and [41].",
      "startOffset" : 47,
      "endOffset" : 51
    }, {
      "referenceID" : 29,
      "context" : "The typical results were reported in [5], [6], [15], [30], [33], [11], [31], [36], [29], [40] and [41].",
      "startOffset" : 53,
      "endOffset" : 57
    }, {
      "referenceID" : 32,
      "context" : "The typical results were reported in [5], [6], [15], [30], [33], [11], [31], [36], [29], [40] and [41].",
      "startOffset" : 59,
      "endOffset" : 63
    }, {
      "referenceID" : 10,
      "context" : "The typical results were reported in [5], [6], [15], [30], [33], [11], [31], [36], [29], [40] and [41].",
      "startOffset" : 65,
      "endOffset" : 69
    }, {
      "referenceID" : 30,
      "context" : "The typical results were reported in [5], [6], [15], [30], [33], [11], [31], [36], [29], [40] and [41].",
      "startOffset" : 71,
      "endOffset" : 75
    }, {
      "referenceID" : 35,
      "context" : "The typical results were reported in [5], [6], [15], [30], [33], [11], [31], [36], [29], [40] and [41].",
      "startOffset" : 77,
      "endOffset" : 81
    }, {
      "referenceID" : 28,
      "context" : "The typical results were reported in [5], [6], [15], [30], [33], [11], [31], [36], [29], [40] and [41].",
      "startOffset" : 83,
      "endOffset" : 87
    }, {
      "referenceID" : 38,
      "context" : "The typical results were reported in [5], [6], [15], [30], [33], [11], [31], [36], [29], [40] and [41].",
      "startOffset" : 89,
      "endOffset" : 93
    }, {
      "referenceID" : 39,
      "context" : "The typical results were reported in [5], [6], [15], [30], [33], [11], [31], [36], [29], [40] and [41].",
      "startOffset" : 98,
      "endOffset" : 102
    }, {
      "referenceID" : 4,
      "context" : "The optimal parameter may depend on the effective dimension of the marginal probability measure over the input space [5], [6], the eigenvalue of the integral operate with respect to the kernel [31], [29], or the smoothness of the regression function [15], [36].",
      "startOffset" : 117,
      "endOffset" : 120
    }, {
      "referenceID" : 5,
      "context" : "The optimal parameter may depend on the effective dimension of the marginal probability measure over the input space [5], [6], the eigenvalue of the integral operate with respect to the kernel [31], [29], or the smoothness of the regression function [15], [36].",
      "startOffset" : 122,
      "endOffset" : 125
    }, {
      "referenceID" : 30,
      "context" : "The optimal parameter may depend on the effective dimension of the marginal probability measure over the input space [5], [6], the eigenvalue of the integral operate with respect to the kernel [31], [29], or the smoothness of the regression function [15], [36].",
      "startOffset" : 193,
      "endOffset" : 197
    }, {
      "referenceID" : 28,
      "context" : "The optimal parameter may depend on the effective dimension of the marginal probability measure over the input space [5], [6], the eigenvalue of the integral operate with respect to the kernel [31], [29], or the smoothness of the regression function [15], [36].",
      "startOffset" : 199,
      "endOffset" : 203
    }, {
      "referenceID" : 14,
      "context" : "The optimal parameter may depend on the effective dimension of the marginal probability measure over the input space [5], [6], the eigenvalue of the integral operate with respect to the kernel [31], [29], or the smoothness of the regression function [15], [36].",
      "startOffset" : 250,
      "endOffset" : 254
    }, {
      "referenceID" : 35,
      "context" : "The optimal parameter may depend on the effective dimension of the marginal probability measure over the input space [5], [6], the eigenvalue of the integral operate with respect to the kernel [31], [29], or the smoothness of the regression function [15], [36].",
      "startOffset" : 256,
      "endOffset" : 260
    }, {
      "referenceID" : 32,
      "context" : "For polynomial kernel learning, there are two papers [33], [41] focusing on selection of the optimal parameter.",
      "startOffset" : 53,
      "endOffset" : 57
    }, {
      "referenceID" : 39,
      "context" : "For polynomial kernel learning, there are two papers [33], [41] focusing on selection of the optimal parameter.",
      "startOffset" : 59,
      "endOffset" : 63
    }, {
      "referenceID" : 32,
      "context" : "It can be easily deduced from [33] and [41] that the learning rate of the regularized least square regression regularized with the polynomial kernel behaves as O(m 2r 2r+d+1 ), which is improved by Theorem 1 in the following three directions.",
      "startOffset" : 30,
      "endOffset" : 34
    }, {
      "referenceID" : 39,
      "context" : "It can be easily deduced from [33] and [41] that the learning rate of the regularized least square regression regularized with the polynomial kernel behaves as O(m 2r 2r+d+1 ), which is improved by Theorem 1 in the following three directions.",
      "startOffset" : 39,
      "endOffset" : 43
    }, {
      "referenceID" : 14,
      "context" : "Eberts and Steinwart [15] have already built a similar learning rate analysis for Gaussian kernel regression.",
      "startOffset" : 21,
      "endOffset" : 25
    }, {
      "referenceID" : 37,
      "context" : "  1, if fz,λ,s ≥ 12 , 0, if fz,λ,s < 1 2 , (8) Theorem 1 and [39] imply that the classier defined as in (8) is also almost optimal if the well known Bayes decision function satisfies a certain smoothness assumption.",
      "startOffset" : 63,
      "endOffset" : 67
    }, {
      "referenceID" : 33,
      "context" : "To this end, we should introduce the conceptions of Haar space and fundamental system [34].",
      "startOffset" : 86,
      "endOffset" : 90
    }, {
      "referenceID" : 3,
      "context" : "Since the uniform distribution is continuous with respect to Lebesgue measure [4], we can draw {ηj}j=1 independently and identically according to the uniform distribution.",
      "startOffset" : 78,
      "endOffset" : 81
    }, {
      "referenceID" : 0,
      "context" : "Let f(t) = (1 − 2t)+(32t 2 + 10t + 1), where t ∈ [0, 1] and a+ = max{a, 0}.",
      "startOffset" : 49,
      "endOffset" : 55
    }, {
      "referenceID" : 0,
      "context" : "Then it is easy to see that f ∈ W 4 ∞([0, 1]) and f / ∈ W 5 ∞([0, 1]).",
      "startOffset" : 38,
      "endOffset" : 44
    }, {
      "referenceID" : 0,
      "context" : "Then it is easy to see that f ∈ W 4 ∞([0, 1]) and f / ∈ W 5 ∞([0, 1]).",
      "startOffset" : 62,
      "endOffset" : 68
    }, {
      "referenceID" : 9,
      "context" : "2, we study the relation between TestRMSE and s for model (3), where λ is the optimal value of 50 candidates drawn equally spaced in [10, 1].",
      "startOffset" : 133,
      "endOffset" : 140
    }, {
      "referenceID" : 0,
      "context" : "2, we study the relation between TestRMSE and s for model (3), where λ is the optimal value of 50 candidates drawn equally spaced in [10, 1].",
      "startOffset" : 133,
      "endOffset" : 140
    }, {
      "referenceID" : 0,
      "context" : "Since f ∈ W 2 ∞([0, 1]), it follows from Theorem 1 that the optimal s may close to the value ⌈m1/(2r+d)⌉ = 4.",
      "startOffset" : 16,
      "endOffset" : 22
    }, {
      "referenceID" : 0,
      "context" : "EPKRF denotes that {ηi}i=1 are chosen as the n equally spaced points in [0, 1].",
      "startOffset" : 72,
      "endOffset" : 78
    }, {
      "referenceID" : 34,
      "context" : "Method choices: In the UCI data experiment, we compare four methods containing support vector machine (SVM) [35], Gaussian kernel regression (GKR) [15, Eqs.",
      "startOffset" : 108,
      "endOffset" : 112
    }, {
      "referenceID" : 0,
      "context" : "d according to the uniform distribution on [0, 1].",
      "startOffset" : 43,
      "endOffset" : 49
    }, {
      "referenceID" : 25,
      "context" : "A function η is said to be admissible [26] if η ∈ C∞[0,∞), η(t) ≥ 0, and suppη ⊂ [0, 2], η(t) = 1 on [0, 1], and 0 ≤ η(t) ≤ 1 on [1, 2].",
      "startOffset" : 38,
      "endOffset" : 42
    }, {
      "referenceID" : 1,
      "context" : "A function η is said to be admissible [26] if η ∈ C∞[0,∞), η(t) ≥ 0, and suppη ⊂ [0, 2], η(t) = 1 on [0, 1], and 0 ≤ η(t) ≤ 1 on [1, 2].",
      "startOffset" : 81,
      "endOffset" : 87
    }, {
      "referenceID" : 0,
      "context" : "A function η is said to be admissible [26] if η ∈ C∞[0,∞), η(t) ≥ 0, and suppη ⊂ [0, 2], η(t) = 1 on [0, 1], and 0 ≤ η(t) ≤ 1 on [1, 2].",
      "startOffset" : 101,
      "endOffset" : 107
    }, {
      "referenceID" : 0,
      "context" : "A function η is said to be admissible [26] if η ∈ C∞[0,∞), η(t) ≥ 0, and suppη ⊂ [0, 2], η(t) = 1 on [0, 1], and 0 ≤ η(t) ≤ 1 on [1, 2].",
      "startOffset" : 129,
      "endOffset" : 135
    }, {
      "referenceID" : 1,
      "context" : "A function η is said to be admissible [26] if η ∈ C∞[0,∞), η(t) ≥ 0, and suppη ⊂ [0, 2], η(t) = 1 on [0, 1], and 0 ≤ η(t) ≤ 1 on [1, 2].",
      "startOffset" : 129,
      "endOffset" : 135
    }, {
      "referenceID" : 9,
      "context" : "Such a function can be easily constructed out of an orthogonal wavelet mask [10].",
      "startOffset" : 76,
      "endOffset" : 80
    }, {
      "referenceID" : 25,
      "context" : ", (11) where Gμk is the well known Gegenbauer polynomial with order μ [26].",
      "startOffset" : 70,
      "endOffset" : 74
    }, {
      "referenceID" : 25,
      "context" : "It can easily deduced from [26] that |Ls(x, y)| ≤ Cs, x, y ∈ B.",
      "startOffset" : 27,
      "endOffset" : 31
    }, {
      "referenceID" : 11,
      "context" : "Since fρ ∈ W r ∞, the well known Jackson inequality [12] shows that (E[s/2](fρ)∞) 2 ≤ Cs.",
      "startOffset" : 52,
      "endOffset" : 56
    }, {
      "referenceID" : 20,
      "context" : "The second one is the well known cubature formula on the sphere, which can be found in [21].",
      "startOffset" : 87,
      "endOffset" : 91
    }, {
      "referenceID" : 33,
      "context" : ", tN be the quasi-uniform points [34] on the sphere.",
      "startOffset" : 33,
      "endOffset" : 37
    } ],
    "year" : 2015,
    "abstractText" : "Polynomial kernel regression is one of the standard and state-of-the-art learning strategies. However, as is well known, the choices of the degree of polynomial kernel and the regularization parameter are still open in the realm of model selection. The first aim of this paper is to develop a strategy to select these parameters. On one hand, based on the worst-case learning rate analysis, we show that the regularization term in polynomial kernel regression is not necessary. In other words, the regularization parameter can decrease arbitrarily fast when the degree of the polynomial kernel is suitable tuned. On the other hand,taking account of the implementation of the algorithm, the regularization term is required. Summarily, the effect of the regularization term in polynomial kernel regression is only to circumvent the “ ill-condition” of the kernel matrix. Based on this, the second purpose of this paper is to propose a new model selection strategy, and then design an efficient learning algorithm. Both theoretical and experimental analysis show that the new strategy outperforms the previous one. Theoretically, we prove that the new learning strategy is almost optimal if the regression function is smooth. Experimentally, it is shown that the new strategy can significantly reduce the computational burden without loss of generalization capability. Index Terms Model selection, regression, polynomial kernel, learning rate. S. Lin is with the College of Mathematics and Information Science, Wenzhou University, Wenzhou 325035, China, Z. Xu and J. Zeng are with the Institute for Information and System Sciences, School of Mathematics and Statistics, Xi’an Jiaotong University, Xi’an 710049, P R China, X. Sun is with the Department of Mathematics, Missouri State University, Springfield, MO 65897, USA March 10, 2015 DRAFT",
    "creator" : "LaTeX with hyperref package"
  }
}