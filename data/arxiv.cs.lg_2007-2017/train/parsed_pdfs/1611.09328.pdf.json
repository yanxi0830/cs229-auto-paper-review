{
  "name" : "1611.09328.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Accelerated Gradient Temporal Difference Learning",
    "authors" : [ "Yangchen Pan", "Adam White", "Martha White" ],
    "emails" : [ "yangpan@indiana.edu", "adamw@indiana.edu", "martha@indiana.edu" ],
    "sections" : [ {
      "heading" : "Introduction",
      "text" : "In reinforcement learning, a common strategy to learn an optimal policy is to iteratively estimate the value function for the current decision making policy—called policy evaluation— and then update the policy using the estimated values. The overall efficiency of this policy iteration scheme is directly influenced by the efficiency of the policy evaluation step. Temporal difference learning methods perform policy evaluation: they estimate the value function directly from the sequence of states, actions, and rewards produced by an agent interacting with an unknown environment.\nThe family of temporal difference methods span a spectrum from computationally-frugal, linear, stochastic approximation methods to data efficient but quadratic least squares TD methods. Stochastic approximation methods, such as temporal difference (TD) learning (Sutton 1988) and gradient TD methods (Maei 2011) perform approximate gradient descent on the mean squared projected Bellman error (MSPBE). These methods require linear (in the number of features) computation per time step and linear memory. These linear TD-based algorithms are well suited to problems with high dimensional feature vectors —compared to available resources— and domains where agent interaction occurs at\nCopyright c© 2017, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\na high rate (Szepesvari 2010). When the amount of data is limited or difficult to acquire, the feature vectors are small, or data efficiency is of primary concern, quadratic least squares TD (LSTD) methods may be preferred. These methods directly compute the value function that minimizes the MSPBE, and thus LSTD computes the same value function to which linear TD methods converge. Of course, there are many domains for which neither light weight linear TD methods, nor data efficient least squares methods may be a good match.\nSignificant effort has focused on reducing the computation and storage costs of least squares TD methods in order to span the gap between TD and LSTD. The iLSTD method (Geramifard and Bowling 2006) achieves sub-quadratic computation per time step, but still requires memory that is quadratic in the size of the features. The tLSTD method (Gehring et al. 2016) uses an incremental singular value decomposition (SVD) to achieve both sub-quadratic computation and storage. The basic idea is that in many domains the update matrix in LSTD can be replaced with a low rank approximation. In practice tLSTD achieves runtimes much closer to TD compared to iLSTD, while achieving better data efficiency. A related idea is to use random projections to reduce computation and storage of LSTD (Ghavamzadeh et al. 2010). In all these approaches, a scalar parameter (descent dimensions, rank, and number of projections), controls the balance between computation cost and quality of solution.\nIn this paper we explore a new approach called Accelerated gradient TD (ATD), that performs quasi-second-order gradient descent on the MSPBE. Our aim is to develop a family of algorithms that can interpolate between linear TD methods and LSTD, without incurring bias. ATD, when combined with a low-rank approximation, converges in expectation to the TD fixed-point, with convergence rate dependent on the choice of rank. Unlike previous subquadratic methods, consistency is guaranteed even when the rank is chosen to be one. We demonstrate the performance of ATD versus many linear and subquadratic methods in three domains, indicating that ATD (1) can match the data efficiency of LSTD, with significantly less computation and storage, (2) is unbiased, unlike many of the alternative subquadratic methods, (3) significantly reduces parameter sensitivity for the step-size, versus linear TD methods, and (4) is significantly less sensitive to the choice of rank parameter than tLSTD, enabling a smaller rank to be chosen and so providing a more efficient increar X\niv :1\n61 1.\n09 32\n8v 2\n[ cs\n.A I]\n9 M\nar 2\n01 7\nmental algorithm. Overall, the results suggest that ATD may be the first practical subquadratic complexity TD method suitable for fully incremental policy evaluation."
    }, {
      "heading" : "Background and Problem Formulation",
      "text" : "In this paper we focus on the problem of policy evaluation, or that of learning a value function given a fixed policy. We model the interaction between an agent and its environment as a Markov decision process (S,A,P, r), where S denotes the set of states,A denotes the set of actions, and P : S×A× S → [0,∞) encodes the one-step state transition dynamics. On each discrete time step t = 1, 2, 3, ..., the agent selects an action according to its behavior policy, At ∼ µ(St, ·), with µ : S ×A → [0,∞) and the environment responds by transitioning into a new state St+1 according to P, and emits a scalar reward Rt+1 def = r(St, At, St+1).\nThe objective under policy evaluation is to estimate the value function, vπ : S → R, as the expected return from each state under some target policy π : S ×A → [0,∞):\nvπ(s) def = Eπ[Gt|St = s],\nwhere Eπ denotes the expectation, defined over the future states encountered while selecting actions according to π. The return, denoted by Gt ∈ R is the discounted sum of future rewards given actions are selected according to π:\nGt def = Rt+1 + γt+1Rt+2 + γt+1γt+2Rt+3 + ... (1) = Rt+1 + γt+1Gt+1\nwhere γt+1 ∈ [0, 1] is a scalar that depends on St, At, St+1 and discounts the contribution of future rewards exponentially with time. The generalization to transition-based discounting enables the unification of episodic and continuing tasks (White 2016) and so we adopt it here. In the standard continuing case, γt = γc for some constant γc < 1 and for a standard episodic setting, γt = 1 until the end of an episode, at which point γt+1 = 0, ending the infinite sum in the return. In the most common on-policy evaluation setting π = µ, otherwise π 6= µ and policy evaluation problem is said to be off-policy.\nIn domains where the number of states is too large or the state is continuous, it is not feasible to learn the value of each state separately and we must generalize values between states using function approximation. In the case of linear function approximation the state is represented by fixed length feature vectors x : S → Rd, where xt def = x(St) and the approximation to the value function is formed as a linear combination of a learned weight vector, w ∈ Rd, and x(St): vπ(St) ≈ w>xt. The goal of policy evaluation is to learn w from samples generated while following µ.\nThe objective we pursue towards this goal is to minimize the mean-squared projected Bellman error (MSPBE):\nMSPBE(w,m)=(bm −Amw)>C−1 (bm −Amw) (2) where m : S → [0,∞) is a weighting function,\nAm def = Eµ[em,t(xt − γt+1xt+1)>]\nbm def = Eµ[Rt+1em,t]\nC is any positive definite matrix, typically C = Eµ[xtx>t ]\nwith bm −Amw = Eµ[δt(w)em,t] for TD-error δt(w) = Rt+1 + γt+1x > t+1w − x>t w. The vector em,t is called the eligibility trace\nem,t def = ρt(γt+1λem,t−1 +Mtxt) . ρt def = π(st, at)\nµ(st, at)\nMt s.t. Eµ[Mt|St = st] = m(s)/dµ(s) if dµ(s) 6= 0. where λ ∈ [0, 1] is called the trace-decay parameter and dµ : S → [0,∞) is the stationary distribution induced by following µ. The importance sampling ratio ρt reweights samples generated by µ to give an expectation over π\nEµ[δt(w)em,t] = ∑ s∈S dµ(s)Eπ[δt(w)(γλem,t−1 +Mtxt)|St = s].\nThis re-weighting enables vπ to be learned from samples generated by µ (under off-policy sampling).\nThe most well-studied weighting occurs when Mt = 1 (i.e., m(s) = dµ(s)). In the on-policy setting, with µ = π, ρt = 1 for all t and m(s) = dπ(s) the w that minimizes the MSPBE is the same as the w found by the on-policy temporal difference learning algorithm called TD(λ). More recently, a new emphatic weighting was introduced with the emphatic TD (ETD) algorithm, which we denote mETD. This weighting includes long-term information about π (see (Sutton et al. 2016, Pg. 16)),\nMt = λt + (1− λt)Ft . Ft = γtρt−1Ft−1 + 1. Importantly, the AmETD matrix induced by the emphatic weighting is positive semi-definite (Yu 2015; Sutton et al. 2016), which we will later use to ensure convergence of our algorithm under both on- and off-policy sampling. The Adµ used by TD(λ) is not necessarily positive semi-definite, and so TD(λ) can diverge when π 6= µ (off-policy).\nTwo common strategies to obtain the minimum w of this objective are stochastic temporal difference techniques, such as TD(λ) (Sutton 1988), or directly approximating the linear system and solving for the weights, such as in LSTD(λ) (Boyan 1999). The first class constitute linear complexity methods, both in computation and storage, including the family of gradient TD methods (Maei 2011), true online TD methods (van Seijen and Sutton 2014; van Hasselt et al. 2014) and several others (see (Dann et al. 2014; White and White 2016) for a more complete summary). On the other extreme, with quadratic computation and storage, one can approximate Am and bm incrementally and solve the system Amw = bm. Given a batch of t samples {(Si, Ai, Si+1, Ri+1)}ti=1, one can estimate\nAm,t def =\n1\nt t∑ i=1 em,i(xi − γxi+1)>\nbm,t def =\n1\nt t∑ i=1 em,iRi+1,\nand then compute solution w such that Am,tw = bm,t. Least-squares TD methods are typically implemented incrementally using the Sherman-Morrison formula, requiring O(d2) storage and computation per step.\nOur goal is to develop algorithms that interpolate between these two extremes, which we discuss in the next section."
    }, {
      "heading" : "Algorithm derivation",
      "text" : "To derive the new algorithm, we first take the gradient of the MSPBE (in 2) to get\n− 1 2 ∇wMSPBE(w,m) = A>mC−1Eµ[δt(w)em,t]. (3)\nConsider a second order update by computing the Hessian: H = A>mC\n−1A>m. For simplicity of notation, let A = Am and b = bm. For invertible A, the second-order update is\nwt+1 = wt − αt2 H −1∇wMSPBE(w,m)\n= wt + αt(A >C−1A)−1A>C−1Eµ[δt(w)em,t]\n= wt + αtA −1CA−>A>C−1Eµ[δt(w)em,t]\n= wt + αtA −1Eµ[δt(w)em,t]\nIn fact, for our quadratic loss, the optimal descent direction is A−1Eµ[δt(w)em,t] with αt = 1, in the sense that argmin∆w loss(wt+ ∆w) = A\n−1Eµ[δt(w)em,t]. Computing the Hessian and updating w requires quadratic computation, and in practice quasi-Newton approaches are used that approximate the Hessian. Additionally, there have been recent insights that using approximate Hessians for stochastic gradient descent can in fact speed convergence (Schraudolph et al. 2007; Bordes et al. 2009; Mokhtari and Ribeiro 2014). These methods maintain an approximation to the Hessian, and sample the gradient. This Hessian approximation provides curvature information that can significantly speed convergence, as well as reduce parameter sensitivity to the step-size.\nOur objective is to improve on the sample efficiency of linear TD methods, while avoiding both quadratic computation and asymptotic bias. First, we need an approximation Â to A that provides useful curvature information, but that is also sub-quadratic in storage and computation. Second, we need to ensure that the approximation, Â, does not lead to a biased solution w.\nWe propose to achieve this by approximating only A−1 and sampling Eµ[δt(w)em,t] = b − Aw using δt(wt)et as an unbiased sample. The proposed accelerated temporal difference learning update—which we call ATD(λ)—is\nwt+1 = wt + (αtÂ † t + ηI)δtet\nwith expected update\nwt+1 = wt + (αtÂ † + ηI)Eµ[δt(w)em,t] (4)\nwith regularization η > 0. If Â is a poor approximation of A, or discards key information—as we will do with a low rank approximation— then updating using only b − Âw will result in a biased solution, as is the case for tLSTD (Gehring et al. 2016, Theorem 1). Instead, sampling b − Aw = Eµ[δt(w)em,t], as we show in Theorem 1, yields an unbiased solution, even with a poor approximation Â. The regularization η > 0 is key to ensure this consistency, by providing a full rank preconditioner αtÂ † t + ηI.\nGiven the general form of ATD(λ), the next question is how to approximate A. Two natural choices are a diagonal approximation and a low-rank approximation. Storing and using a diagonal approximation would only require linear O(d) time and space. For a low-rank approximation Â, of rank k, represented with truncated singular value decomposition Â = UkΣkV>k , the storage requirement is O(dk) and the required matrix-vector multiplications are only O(dk) because for any vector v, Âv = UkΣk(V>k v), is a sequence of O(dk) matrix-vector multiplications. Exploratory experiments revealed that the low-rank approximation approach significantly outperformed the diagonal approximation. In general, however, many other approximations to A could be used, which is an important direction for ATD.\nWe opt for an incremental SVD, that previously proved effective for incremental estimation in reinforcement learning (Gehring et al. 2016). The total computational complexity of the algorithm is O(dk + k3) for the fully incremental update to Â and O(dk) for mini-batch updates of k samples. Notice that when k = 0, the algorithm reduces exactly to TD(λ), where η is the step-size. On the other extreme, where k = d, ATD is equivalent to an iterative form of LSTD(λ). See the appendix for a further discussion, and implementation details."
    }, {
      "heading" : "Convergence of ATD(λ)",
      "text" : "As with previous convergence results for temporal difference learning algorithms, the first key step is to prove that the expected update converges to the TD fixed point. Unlike previous proofs of convergence in expectation, we do not require the true A to be full rank. This generalization is important, because as shown previously, A is often low-rank, even if features are linearly independent (Bertsekas 2007; Gehring et al. 2016). Further, ATD should be more effective if A is low-rank, and so requiring a full-rank A would limit the typical use-cases for ATD.\nTo get across the main idea, we first prove convergence of ATD with weightings that give positive semi-definite Am; a more general proof for other weightings is in the appendix.\nAssumption 1. A is diagonalizable, that is, there exists invertible Q ∈ Rd×d with normalized columns (eigenvectors) and diagonal Λ ∈ Rd×d, Λ = diag(λ1, . . . , λd), such that A = QΛQ−1. Assume the ordering λ1 ≥ . . . ≥ λd. Assumption 2. α ∈ (0, 2) and 0 < η ≤ λ−11 max(2−α, α).\nFinally, we introduce an assumption that is only used to characterize the convergence rate. This condition has been previously used (Hansen 1990; Gehring et al. 2016) to enforce a level of smoothness on the system.\nAssumption 3. The linear system defined by A = QΛQ−1 and b satisfy the discrete Picard condition: for some p > 1, |(Q−1b)j | ≤ λpj for all j = 1, . . . , rank(A).\nTheorem 1. Under Assumptions 1 and 2, for any k ≥ 0, let Â be the rank-k approximation Â = QΛkQ−1 of Am, where Λk ∈ Rd×d with Λk(j, j) = λj for j = 1, . . . , k and zero otherwise. If m = dµ or mETD, the expected updating rule in (4) converges to the fixed-point w? = A†mbm.\nFurther, if Assumption 3 is satisfied, the convergence rate is ‖wt −w?‖ ≤ max (\nmax j∈{1,...,k}\n|1− α− ηλj |tλp−1j ,\nmax j∈{k+1,...,rank(A)}\n|1− ηλj |tλp−1j )\nProof: We use a general result about stationary iterative methods which is applicable to the case where A is not full rank. Theorem 1.1 (Shi et al. 2011) states that given a singular and consistent linear system Aw = b where b is in the range of A, the stationary iteration with w0 ∈ Rd for t = 1, 2, . . .\nwt = (I−BA)wt−1 + Bb (5)\nconverges to the solution w = A†b if and only if the following three conditions are satisfied.\nCondition I: the eigenvalues of I−BA are equal to 1 or have absolute value strictly less than 1.\nCondition II: rank(BA) = rank[(BA)2]. Condition III: nullspace(BA) = nullspace(A).\nWe verify these conditions to prove the result. First, because we use the projected Bellman error, b is in the range of A and the system is consistent: there exists w s.t. Aw = b.\nTo rewrite our updating rule (4) to be expressible in terms of (5), let B = αÂ† + ηI, giving\nBA = αÂ†A + ηA = αQΛ†kQ −1QΛQ−1 + ηQΛQ−1\n= αQIkQ −1 + ηQΛQ−1\n= Q(αIk + ηΛ)Q −1 (6)\nwhere Ik is a diagonal matrix with the indices 1, . . . , k set to 1, and the rest zero.\nProof for condition I. Using (6), I − BA = Q(I − αIk − ηΛ)Q−1. To bound the maximum absolute value in the diagonal matrix I − αIk − ηΛ, we consider eigenvalue λj in Λ, and address two cases. Because Am is positive semi-definite for the assumed m (Sutton et al. 2016), λj ≥ 0 for all j = 1, . . . , d.\nCase 1: j ≤ k. |1− α− ηλj | . for 0 < η < max ( 2− α λ1 , α λ1 ) < max(|1− α|, |1− α− (2− α)|, |1− α− α|) = max(|1− α|, 1, 1) < 1 . because α ∈ (0, 2).\nCase 2: j > k. |1 − ηλj | < 1 if 0 < η < 2/λj which is true for η = λ−11 max(2−α, α) for any α ∈ (0, 2). Proof for condition II. (BA)2 does not change the number of positive eigenvalues, so the rank is unchanged. Proof for condition III. To show the nullspaces of BA and A are equal, it is sufficient to prove BAw = 0 if and only if Aw = 0. B = Q(αΛk + ηI)Q−1, is invertible because η > 0 and λj ≥ 0. For any w ∈ nullspace(A), we get BAw = B0 = 0, and so w ∈ nullspace(BA). For any w ∈ nullspace(BA), BAw = 0 =⇒ Aw = B−10 = 0, and so w ∈ nullspace(A).\nConvergence rate. Assume w0 = 0. On each step, we update with wt+1 = (I−BA)wt+Bb = ∑t−1 i=0(I−BA)iBb. This can be verified inductively, where\nwt+1 = (I−BA) t−2∑ i=0 (I−BA)iBb + (I−BA)0Bb\n= t−1∑ i=0 (I−BA)iBb.\nFor Λ̄ = I− αIk − ηΛ, because (I−BA)i = QΛ̄iQ−1,\nwt = Q ( t−1∑ i=0 Λ̄i ) Q−1Q(αΛ†k + ηI)Q −1b\n= Q ( t−1∑ i=0 Λ̄i ) (αΛ†k + ηI)Q −1b\nand because wt → w?,\n‖wt −w?‖ = ‖Q ( ∞∑ i=0 Λ̄i − t∑ i=0 Λ̄i ) (αΛ†k + ηI)Q −1b‖\n= ‖QΛ̄t(αΛ†k + ηI)Q −1b‖ . Λ̄t(j, j) def = λ̄tj 1− λ̄j ≤ ‖Q‖‖Λ̄t(αΛ†k + ηI)Q −1b‖\nwhere ‖Q‖ ≤ 1 because Q has normalized columns. For j = 1, . . . , k, we have that the magnitude of the values in Λ̄t(αΛ † k + ηI) are\n(1− α− ηλj)t\nα+ ηλj (αλ−1j + η) =\n(1− α− ηλj)t\nλj .\nFor j = k, . . . , rank(A), we get (1−ηλj) t\nλj .\nUnder the discrete Picard condition, |(Q−1b)j | ≤ λpj and so the denominator λj cancels, giving the desired result.\nThis theorem gives insight into the utility of ATD for speeding convergence, as well as the effect of k. Consider TD(λ), which has positive definite A in on-policy learning (Sutton 1988, Theorem 2). The above theorem guarantees ATD convergences to the TD fixed-point, for any k. For k = 0, the expected ATD update is exactly the expected TD update. Now, we can compare the convergence rate of TD and ATD, using the above convergence rate.\nTake for instance the setting α = 1 for ATD, which is common for second-order methods and let p = 2. The rate of convergence reduces to the maximum of maxj∈{1,...,k} ηtλ t+1 j and maxj∈{k+1,...,rank(A)} |1−ηλj |tλj . In early learning, the convergence rate for TD is dominated by |1 − ηλ1|tλ1, because λj is largest relative to |1− ηλj |t for small t. ATD, on the other hand, for a larger k, can pick a smaller η and so has a much smaller value for j = 1, i.e., ηtλt+11 , and |1−ηλj |tλj is small because λj is small for j > k. As k gets smaller, |1− ηλk+1|tλk+1 becomes larger, slowing convergence. For low-rank domains, however, k could be quite small and the preconditioner could still improve the convergence rate in early learning—potentially significantly outperforming TD.\nATD is a quasi-second order method, meaning sensitivity to parameters should be reduced and thus it should be simpler to set the parameters. The convergence rate provides intuition that, for reasonably chosen k, the regularizer η should be small—smaller than a typical stepsize for TD. Additionally, because ATD is a stochastic update, not the expected update, we make use of typical conventions from stochastic gradient descent to set our parameters. We set αt = α0t , as in previous stochastic second-order methods (Schraudolph et al. 2007), where we choose α0 = 1 and set η to a small fixed value. Our choice for η represents a small final step-size, as well as matching the convergence rate intuition.\nOn the bias of subquadratic methods. The ATD(λ) update was derived to ensure convergence to the minimum of the MSPBE, either for the on-policy or off-policy setting. Our algorithm summarizes past information, in Â, to improve the convergence rate, without requiring quadratic computation and storage. Prior work aspired to the same goal, however, the resultant algorithms are biased. The iLSTD algorithm can be shown to converge for a specific class of feature selection mechanisms (Geramifard et al. 2007, Theorem 2); this class, however, does not include the greedy mechanism that is used in iLSTD algorithm to select a descent direction. The random projections variant of LSTD (Ghavamzadeh et al. 2010) can significantly reduce the computational complexity compared with conventional LSTD, with projections down to size k, but the reduction comes at a cost of an increase in the approximation error (Ghavamzadeh et al. 2010). Fast LSTD (Prashanth et al. 2013) does randomized TD updates on a batch of data; this algorithm could be run incrementally with O(dk) by using mini-batches of size k. Though it has a nice theoretical characterization, this algorithm is restricted to λ = 0. Finally, the most related algorithm is tLSTD, which also uses a low-rank approximation to A.\nIn ATD Ât is used very differently, from how Ât is used in tLSTD. The tLSTD algorithm uses a similar approximation Ât as ATD, but tLSTD uses it to compute a closed form solution wt = Â † tbt, and thus is biased (Gehring et al. 2016, Theorem 1). In fact, the bias grows with decreasing k, proportionally to the magnitude of the kth largest singular value of A. In ATD, the choice of k is decoupled from the fixed point, and so can be set to balance learning speed and computation with no fear of asymptotic bias."
    }, {
      "heading" : "Empirical Results",
      "text" : "All the following experiments investigate the on-policy setting, and thus we make use of the standard version of ATD for simplicity. Future work will explore off-policy domains with the emphatic update. The results presented in this section were generated over 756 thousand individual experiments run on three different domains. Due to space constraints detailed descriptions of each domain, error calculation, and all other parameter settings are discussed in detail in the appendix. We included a wide variety of baselines in our experiments, additional related baselines excluded from our study are also discussed in the appendix."
    }, {
      "heading" : "1 2 3 4 5 6 7 8 9 101112131415161718",
      "text" : "Constant step-size Decayed step-size\nOur first batch of experiments were conducted on Boyan’s chain—a domain known to elicit the strong advantages of LSTD(λ) over TD(λ). In Boyan’s chain the agent’s objective is to estimate the value function based on a low-dimensional, dense representation of the underlying state (perfect representation of the value function is possible). The ambition of this experiment was to investigate the performance of ATD in a domain where the pre-conditioner matrix is full rank; no rank truncation is applied. We compared five linear-complexity methods (TD(0), TD(λ), true online TD(λ), ETD(λ), true online ETD(λ)), against LSTD(λ) and ATD, reporting the percentage error relative to the true value function over the first 1000 steps, averaged over 200 independent runs. We swept a large range of step-size parameters, trace decay rates, and regularization parameters, and tested both fixed and decaying step-size schedules. Figure 1 summarizes the results.\nBoth LSTD(λ) and ATD achieve lower error compared to all the linear baselines—even thought each linear method was tuned using 864 combinations of step-sizes and λ. In terms of sensitivity, the choice of step-size for TD(0) and ETD exhibit large effect on performance (indicated by sharp valleys), whereas true-online TD(λ) is the least sensitive to learning rate. LSTD(λ) using the Sherman-Morrison update (used in many prior empirical studies) is sensitive to the regularization parameter; the parameter free nature of LSTD may be slightly overstated in the literature.1\nOur second batch of experiments investigated characteristics of ATD in a classic benchmark domain with a sparse high-dimensional feature representation where perfect approximation of the value function is not possible—Mountain car with tile coding. The policy to be evaluated stochastically takes the action in the direction of the sign of the velocity, with performance measured by computing a truncated Monte\n1We are not the first to observe this. Sutton and Barto (2016) note that η plays a role similar to the step-size for LSTD.\nCarlo estimate of the return from states sampled from the stationary distribution (detailed in the appendix). We used a fine grain tile coding of the the 2D state, resulting in a 1024 dimensional feature representation with exactly 10 units active on every time step. We tested TD(0), true online TD(λ) true online ETD(λ), and sub-quadratic methods, including iLSTD, tLSTD, random projection LSTD, and fast LSTD (Prashanth et al. 2013). As before a wide range of parameters (α, λ, η) were swept over a large set. Performance was averaged over 100 independent runs. A fixed step-size schedule was used for the linear TD baselines, because that achieved the best performance. The results are summarized in figure 2.\nLSTD and ATD exhibit faster initial learning compared to all other methods. This is particularly impressive since k is less than 5% of the size of A. Both fast LSTD and projected LSTD perform considerably worse than the linear TD-methods, while iLSTD exhibits high parameter sensitivity. tLSTD has no tunable parameter besides k, but performs poorly due to the high stochasticity in the policy—additional experiments with randomness in action selection of 0% and 10% yielded better performance for tLSTD, but never equal to ATD. The true online linear methods perform very well compared to ATD, but this required sweeping hundreds of combinations of α and λ, whereas ATD exhibited little sensitivity to it’s regularization parameter (see Figure 2 RHS); ATD achieved excellent performance with the same parameter setting as we used in Boyan’s chain.2\nWe ran an additional experiment in Mountain car to more clearly exhibit the benefit of ATD over existing methods. We used the same setting as above, except that 100 additional features were added to the feature vector, with 50 of them randomly set to one and the rest zero. This noisy feature vector is meant to emulate a situation such as a robot that has a sensor that becomes unreliable, generating noisy data, but the remaining sensors are still useful for the task at hand. The results are summarized in Figure 4. Naturally all methods are adversely effected by this change, however ATD’s low\n2For the remaining experiments in the paper, we excluded the TD methods without true online traces because they perform worse than their true online counterparts in all our experiments. This result matches the results in the literature (van Seijen et al. 2016).\n2000 4000 6000 8000 10000\n10-1\n100\n1000 2000 3000 4000 5000\ntime steps\ntLSTD\niLSTD\nTD(0)/R-LSTD/F-LSTD\nLSTD\nTO-TD\nTO-ETD\nATD\nMountain Car w noisy features\ntime steps\nRP-LSTD\nTD(0)\nfast-LSTD\nTO-ETD\ntLSTD ATD\nTO-TD\nEnergy domain\nFigure 3: Learning curves on Mountain Car with noisy features (LHS) and on Energy allocation (RHS), in logscale.\nrank approximation enables the agent to ignore the unreliable feature information and learn efficiently. tLSTD, as suggested by our previous experiments does not seem to cope well with the increase in stochasticity.\nOur final experiment compares the performance of several sub-quadratic complexity policy evaluation methods in an industrial energy allocation simulator with much larger feature dimension (see Figure 4). As before we report percentage error computed from Monte Carlo rollouts, averaging performance over 50 independent runs and selecting and testing parameters from an extensive set (detailed in the appendix). The policy was optimized ahead of time and fixed, and the feature vectors were produced via tile coding, resulting in an 8192 dimensional feature vector with 800 units active on each step. Although the feature dimension here is still relatively small, a quadratic method like LSTD nonetheless would require over 67 million operations per time step, and thus methods that can exploit low rank approximations are of particular interest. The results indicate that both ATD and tLSTD achieve the fastest learning, as expected. The instrinsic rank in this domain appears to be small compared to the feature dimension—which is exploited by ATD and tLSTD with r = 40—while the performance of tLSTD indicates that the domain exhibits little stochasticity. The appendix contains additional results for this domain—in the small rank setting ATD significantly outperforms tLSTD.\nConclusion and future work In this paper, we introduced a new family of TD learning algorithms that take a fundamentally different approach from previous incremental TD algorithms. The key idea is to use a preconditioner on the temporal difference update, similar to a quasi-Newton stochastic gradient descent update. We prove that the expected update is consistent, and empirically demonstrated improved learning speed and parameter insensitivity, even with significant approximations in the preconditioner.\nThis paper only begins to scratch the surface of potential preconditioners for ATD. There remains many avenues to explore the utility of other preconditioners, such as diagonal approximations, eigenvalues estimates, other matrix factorizations and approximations to A that are amenable to inversion. The family of ATD algorithms provides a promising avenue for more effectively using results for stochastic gradient descent to improve sample complexity, with feasible computational complexity."
    }, {
      "heading" : "Convergence proof",
      "text" : "For the more general setting, where m can also equal Dµ, we redefine the rank-k approximation. We say the rank-k approximation Â to A is composed of eigenvalues {λi1 , . . . , λik} ⊆ {λ1, . . . , λd} if Â = QΛkQ−1 for diagonal Λk ∈ Rd×d, Λ(ij , ij) = λij for j = 1, . . . , k, and zero otherwise.\nTheorem 2. Under Assumptions 1 and 2, let Â be the rank-k approximation composed of eigenvalues {λi1 , . . . , λik} ⊆ {λ1, . . . , λd}. If λd ≥ 0 or {λi1 , . . . , λik} contains all the negative eigenvalues in {λ1, . . . , λd}, then the expected updating rule in (4) converges to the fixed-point w? = A†b.\nProof: We use a general result about stationary iterative methods (Shi et al. 2011), which is applicable to the case where A is not full rank. Theorem 1.1 (Shi et al. 2011) states that given a singular and consistent linear system Aw = b where b is in the range of A, the stationary iteration with w0 ∈ Rd for t = 1, 2, . . .\nwi = (I−BA)wt−1 + Bb (5)\nconverges to the solution w = A†b if and only if the following three conditions are satisfied.\nCondition I: the eigenvalues of I−BA are equal to 1 or have absolute value strictly less than 1.\nCondition II: rank(BA) = rank[(BA)2]. Condition III: the null space N (BA) = N (A).\nWe verify these conditions to prove the result. First, because we are using the projected Bellman error, we know that b is in the range of A and the system is consistent: there exists w s.t. Aw = b.\nTo rewrite our updating rule (4) to be expressible in terms of (5), let B = αÂ† + ηI, giving\nBA = αÂ†A + ηA\n= αQΛ†kQ −1QΛQ−1 + ηQΛQ−1 = αQIkQ −1 + ηQΛQ−1\n= Q(αIk + ηΛ)Q −1 (6)\nwhere Ik is a diagonal matrix with the indices i1, . . . , ik set to 1, and the rest zero.\nProof for condition I. Using (6), I − BA = Q(I − αIk − ηΛ)Q−1. To bound the maximum absolute value in the diagonal matrix I − αIk − ηΛ, we consider eigenvalue λj in Λ, and address three cases.\nCase 1: j ∈ {i1, . . . , ik}, λj ≥ 0: |1− α− ηλj | . for 0 < η < max (\n2− α λ1 , α λ1 ) < max(|1− α|, |1− α− (2− α)|, |1− α− α|) = max(|1− α|, 1, 1) < 1 . because α ∈ (0, 2).\nCase 2: j ∈ {i1, . . . , ik}, λj < 0: |1 − α − ηλi| = |1 − α+ η|λi|| < 1 if 0 ≤ 1− α+ η|λi| < 1 =⇒ η < α/|λi|.\nCase 3: j /∈ {i1, . . . , ik}. For this case, λj ≥ 0, by assumption, as {i1, . . . , ik} contains the indices for all negative eigenvalues of A. So |1− ηλi| < 1 if 0 < η < 2/λi.\nAll three cases are satisfied by the assumed α ∈ (0, 2) and η ≤ λ−1max max(2 − α, α). Therefore, the absolute value of the eigenvalues of I−BA are all less than 1 and so the first condition holds.\nProof for condition II. (BA)2 does not change the number of positive eigenvalues, so the rank is unchanged.\nBA = Q(αIk + ηΛ)Q −1\n(BA)2 = Q(αIk + ηΛ)Q −1Q(αIk + ηΛ)Q −1\n= Q(αIk + ηΛ) 2Q−1\nProof for condition III. To show that the nullspaces of BA and A are equal, it is sufficient to prove BAw = 0 if and only if Aw = 0. Because B = Q(αΛk+ηI)Q−1, we know that B is invertible as long as α 6= −ηλj . Because η > 0, this is clearly true for λj ≥ 0 and also true for λj < 0 because η is strictly less than α/|λj |. For any w ∈ nullspace(A), we get BAw = B0 = 0, and so w ∈ nullspace(BA). For any w ∈ nullspace(BA), we get BAw = 0 =⇒ Aw = B−10 = 0, and so w ∈ nullspace(A), completing the proof.\nWith k = d, the update is a gradient descent update on the MSPBE, and so will converge even under off-policy sampling. As k << d, the gradient is only approximate and theoretical results about (stochastic) gradient descent no longer obviously apply. For this reason, we use the iterative update analysis above to understand convergence properties. Iterative updates for the full expected update, with preconditioners, have been studied in reinforcement learning (c.f. (Wang and Bertsekas 2013)); however, they typically analyzed different preconditioners, as they had no requirements for reducing computation below quadratic computation. For example, they consider a regularized preconditioner B = (A + ηI)−1, which is not compatible with an incremental singular value decomposition and, to the best of our knowledge, current iterative eigenvalue decompositions require symmetric matrices.\nThe theorem is agnostic to what components of A are approximated by the rank-k matrix Â. In general, a natural choice, particularly in on-policy learning or more generally with a positive definite A, is to select the largest magnitude eigenvalues of A, which contain the most significant information about the system and so are likely to give the most useful curvature information. However, Â could also potentially be chosen to obtain convergence for off-policy learning with m = dµ, where A is not necessarily positive semi-definite. This theorem indicates that if the rank k approximation Â contains the negative eigenvalues of A, even if it does not contain the remaining information in A, then we obtain convergence under off-policy sampling. We can of course use the emphatic weighting more easily for off-policy learning, but if the weighting m = dµ is desired rather than mETD, then carefully selecting Â for ATD enables that choice.\nAlgorithm 1 Accelerated Temporal Difference Learning . where U0 = [],V0 = [],Σ0 = [],b0 = 0, e0 = 0, initialized w0 arbitrarily\nfunction ATD(k, η, λ) x0 = first observation η = a small final stepsize value, e.g., η = 10e− 4 for t = 0, 1, 2, ... do\nIn xt, select action At ∼ π, observe xt+1, reward rt+1, discount γt+1 (could be zero if terminal state) βt = 1/(t+ 1) δt = rt+1 + γt+1w\n>xt+1 −w>xt et = TRACE_UPDATE(et−1,xt, γt, λt) . or call EMPAHTIC_TRACE_UPDATE to use emphatic weighting . UtΣtV > t = (1− βt)Ut−1Σt−1V>t−1 + βtet(xt − γt+1xt+1)>\n[Ut,Σt,Vt]=SVD-UPDATE(Ut−1, (1− βt)Σt−1,Vt−1, √ βtet, √ βt(xt−γt+1xt+1), k) . see (Gehring et al. 2016) . Ordering of matrix operations important, first multiply U>t et in O(dk) time . to get a new vector, then by Σ†t and Vt to maintain only matrix-vector multiplications wt+1 = wt + ( 1 t+1VtΣ † tU > t + ηI)(δtet) . where Σ † t = diag(σ̂ −1 1 , . . . , σ̂ −1 k ,0)"
    }, {
      "heading" : "Algorithmic details",
      "text" : "In this section, we outline the implemented ATD(λ) algorithm. The key choices are how to update the approximation to Â, and how to update the eligibility trace to obtain different variants of TD. We include both the conventional and emphatic trace updates in Algorithms 2 and 3 respectively. The low-rank update to Â uses an incremental singular value decomposition (SVD). This update to Â is the same one used for tLSTD, and so we refer the reader to (Gehring et al. 2016, Algorithm 3). The general idea is to incorporate the rank one update et(xt − γt+1xt+1)> into the current SVD of Â. In addition, to maintain a normalized Â, we multiply by βt:\nÂt+1 = (1− βt)Â + βtet(xt − γt+1xt+1)>\n= tt+1Â + 1 t+1et(xt − γt+1xt+1)\n>\nMultiplying Â by a constant corresponds to multiplying the singular values. We also find that multiplying each component of the rank one update by √ 1/(t+ 1) is more effective than multiplying only one of them by 1/(t+ 1).\nAlgorithm 2 Conventional trace update for ATD\nfunction TRACE_UPDATE(et−1,xt, γt, λt) return γtλtet−1 + xt\nAlgorithm 3 Emphatic trace update for ATD . where F0 ← 0, M0 ← 0 is initialized globally, before executing the for loop in ATD(λ)\nfunction EMPHATIC_TRACE_UPDATE(et−1,xt, γt, λt) ρt ← π(st,at)µ(st,at) . Where ρt = 1 in the on-policy case Ft ← ρt−1γtFt−1 + it . For uniform interest, it = 1 Mt ← λtit + (1− λt)Ft\nreturn ρt(γtλtet−1 +Mtxt)\nDetailed experimental specification In both mountain car and energy storage domains we do not have access to the parameter’s of the underlying MDPs (as we do in Boyan’s chain), and thus must turn to Monte Carlo rollouts to estimate vπ in order to evaluate our value function approximation methods. In both domains we followed the same strategy.\nTo generate training data we generated 100 trajectories of rewards and observations under the target policy, starting in randomly from a small area near a start state. Each trajectory is composed of a fixed number of steps, either 5000 or 10000, and, in the case of episodic tasks like mountain car, may contain many episodes. The start states for each trajectory were sampled uniform randomly from (1) near the bottom of the hill with zero velocity for mountain car, (2) a small set of valid start states specified by the energy storage domains (Salas and Powell 2013). Each trajectory represents one independent run of the domain.\nThe testing data was sampled according to the on-policy distribution induced by the target policy. For both domains we generated a single long trajectory selecting actions according to π. Then we randomly sampled 2000 states from this one trajectory. In mountain car domain, we ran 500 Monte Carlo rollouts to compute undiscounted sum of future rewards until termination, and take the average as an estimate true value. In the energy allocation domain, we ran 300 Monte Carlo rollouts for each evaluation state, each with length 1000 steps3, averaging over 300 trajectories from each of the evaluation states. We evaluated the algorithms’ performances by comparing the agent’s prediction value with the estimated value of the 2000 evaluation states, at every 50 steps during training. We measured the percentage absolution mean error:\nerror(w) = 1\n2000 2000∑ i=1 |wTx(si)− v̂π(si)| |v̂π(si)| ,\n3After 1000 steps, for γ = 0.99, the reward in the return is multiplied by γ1000 < 10−5 and so contributes a negligible amount to the return.\nwhere v̂π(si) ∈ R denotes the Monte Carlo estimate of the value of evaluation state si."
    }, {
      "heading" : "Algorithms",
      "text" : "The algorithms included in the experiments constitute a wide range of stochastic approximation algorithms and matrixbased (subquadratic) algorithms. There are a few related algorithms, however, that we chose not to include; for completeness we explain our decision making here.\nThere have been some accelerations proposed to gradient TD algorithms (Mahadevan et al. 2014; Meyer et al. 2014; Dabney and Thomas 2014). However, they have either shown to perform poorly in practice (White and White 2016), or were based on applying accelerations outside their intended use (Meyer et al. 2014; Dabney and Thomas 2014). Dabney and Thomas (2014) explored a similar update to ATD, but for the control setting and with an incremental update to the Fisher information matrix rather than A used here. As they acknowledge, this approach for TD methods is somewhat adhoc, as the typical update is not a gradient, and rather their method is better suited for the policy gradient algorithms explored in that paper. Meyer et al. (2014) applied an accelerated Nesterov technique, called SAGE, to the two timescale gradient algorithms. Their approach does not take advantage of the simpler quadratic form of the MSPBE, and so only uses an approximate Lipschitz constant to improve selection of the stepsize. Diagonal approximations to A constitute a strictly more informative stepsize approach, and we found these to be inferior to our low-rank strategy. The results by Meyer et al. (2014) using SAGE for GTD similarly indicated little to no gain. Finally, Givchi and Palhang (2014) adapted SGD-QN for TD, and showed some improvements using this diagonal step-size approximation.\nOn the other hand, the true-online methods have consistently been shown to have surprisingly strong performance (White and White 2016), and so we opt instead for these practical competitors."
    }, {
      "heading" : "Boyan’s Chain",
      "text" : "This domain was implemented exactly as describe in Boyan’s paper (Boyan 1999). The task is episodic and the true value function is known, and thus we did not need to compute rollouts. Otherwise evaluation was performed exactly as described above. We tested the following parameter settings: • α0 ∈ {0.1×2.0j |j = −12,−11,−10, ..., 4, 5}, 18 values\nin total\n• n0 ∈ {102, 106}\n• λ ∈ {0.0, 0.1, ..., 0.9, 0.91, 0.93, 0.95, 0.97, 0.99, 1.0}, 16 values in total\n• η ∈ {10j |j = −4,−3.5,−3, ..., 3.5, 4, 4.5}, 18 values in total.\nThe linear methods, (e.g., TD(0) true online ETD(λ)), made use of α0, n0, and λ, whereas the LSTD made use of η to initialize the incremental approximation of A inverse and\nλ. For the linear methods we also tested decaying step size schedule as originally investigated by Boyan\nαt = α0 n0 + 1\nn0 + #terminations .\nWe also tested constant step-sizes where αt = α0. The ATD algorithm, as proposed was tested with one fixed parameter setting."
    }, {
      "heading" : "Mountain Car",
      "text" : "Our second batch of experiments was conducted on the classic RL benchmark domain Mountain Car. We used the Sutton and Barto (1998) specification of the domain, where the agent’s objective is to select one of three discrete actions (reverse, coast, forward), based on the continuous position and velocity of an underpowered car to drive it out of a valley, at which time the episode terminates. This is an undiscounted task. Each episode begins at the standard initial location — randomly near the bottom of the hill — with zero velocity. Actions were selected according to a stochastic Bang-bang policy, where reverse is selected if the velocity is negative and forward is selected if the velocity is positive and occasionally a random action is selected—we tested randomness in action selection of 0%, 10%, and 20%.\nWe used tile coding to convert the continuous state variable into high-dimensional binary feature vectors. The position and velocity we tile coded jointly with 10 tilings, each forming a two dimensional uniform grid partitioned by 10 tiles in each dimension. This resulted in a binary feature vector of length 1000, with exactly 10 components equal to one and the remaining equal to zero. We requested 1024 memory size to guarantee the performance of tile coder, resulted in finally 1024 features. We used a standard freely available implementation of tile coding 4, which is described in detail in Sutton and Barto (1998).\nWe tested the following parameter settings for Mountain Car:\n• α0 ∈ {0.1 × 2.0j |j = −7,−6, ..., 4, 5} divided by number of tilings, 13 values in total\n• λ ∈ {0.0, 0.1, ..., 0.9, 0.93, 0.95, 0.97, 0.99, 1.0}, 15 values in total\n• η ∈ {10j |j = −4,−3.25,−2.5, ..., 3.5, 4.25, 5.0}, 13 values in total.\nThe linear methods (e.g., TD(0)), iLSTD, and fast LSTD made use of α0 as stepsize, ATD uses α0/100 as regularizer, whereas the LSTD, and random projection LSTD made use of the η as regularization for Sherman-Morrison matrix initialization. All methods except fast LSTD and TD(0) made use of the λ parameter. iLSTD used decaying step-sizes with n0 = 102. In addition we fixed the number of descent dimensions for iLSTD to one (recommended by previous studies (Geramifard and Bowling 2006; Geramifard et al. 2007)). We found that the linear methods,\n4https://webdocs.cs.ualberta.ca/ sutton/tiles2.html\non the other hand, performed worse in this domain with decayed step-sizes so we only reported the performance for the constant step size setting. In this domain we tested several settings for the regularization parameter for ATD. However, as the results demonstrate ATD is insensitive to this parameter. Therefore we present results with the same fixed parameter setting for ATD as used in Boyan’s chain. The low rank matrix methods—including ATD—were tested with rank equal to 20, 30, 40, 50, and 100. With rank 20, 30, 40, we observed that ATD can still do reasonably well but converges slower. However, rank = 100 setting does not show obvious strength, likely due to the fact that the threshold for inversing A remains unchanged."
    }, {
      "heading" : "Energy Allocation",
      "text" : "Our final experiments were run on a simulator of a complex energy storage and allocation task. This domain simulates control of a storage device that interacts with a market and stochastic source of energy as a continuing discounted RL tasks. The problem was originally modeled as a finite horizon undiscounted task (Salas and Powell 2013), with four state variables at each time step: the amount of energy in the storage device Rt, the net amount of wind energy Et, time aggregate demand Dt, and price of electricity Pt in spot market. The reward function encodes the revenue earned by the agent’s energy allocation strategy as a real value number. The policy to be evaluated was produced by an approximate dynamic programming algorithm from the literature (Salas and Powell 2013). The simulation program is from Energy storage datasets II from http://castlelab.princeton.edu.\nWe made several minor modifications to the simulator to allow generating training or testing data for policy evaluation. First, we modified the original policy by setting the input time index as (#timeindex mod 24) so that we can remove the restriction that time index must be no greater than 24. Though no longer an optimal policy, this still constitutes a valid policy that provides the same distribution over actions for a given state. Second, we added an additional variable, Dt−1, to the state at time t, encoding the state as five variables. This addition was to ensure a Markov state, are using only the original four variables results in a time-dependent state. Third, we considered the problem as a continuing task by setting discount rate (γ = 0.99) when estimating values of states.\nWe used the following parameter setting of generating the training data and testing data. The stochastic processes associated with Pt, Et, Dt are jump process, uniform process and sinusoidal process. The ranges of Rt, Et, Pt, Dt are: [0, 30], [1, 7], [30, 70], [0, 7]. When generating the training trajectories, we randomly choose each state values from the ranges: [0, 10], [1, 5], [30, 50], [0, 7].\nAgain, we used tile coding to convert the state variable into high-dimensional binary feature vectors, similar to how the acrobot domain was encoded in prior work (see Sutton & Barto, 1998). We tile coded all 3-wise combinations, all pair-wise combinations, and each of the five state variables independently (sometimes called stripped tilings). More specifically we used:\n• all five one-wise tilings of 5 state variables, with gridsize\n= 4, numtilings = 32 (memory = 5× 4× 32)\n• all ten two-wise tilings of 5 state variables, with gridsize = 4, numtilings = 32 (memory = 10× 42 × 32)\n• all ten three-wise tilings of 5 state variables, with gridsize = 2, numtilings = 32 (memory = 10× 23 × 32).\nThis resulted in a binary feature vector of length 8320, which we hashed down to 8192 = 213. Training data and evaluation were conducted in the exact same manner as the Mountain car experiment. We tested a similar set of parameters as before: • α0 ∈ {0.1 × 2.0j |j = −7,−6, ..., 4, 5} divided by\nnumber of tilings, 13 values in total\n• λ ∈ {0.0, 0.1, ..., 0.9, 1.0}, 10 values in total\n• η ∈ {10j |j = −4,−3.25,−2.5, ..., 3.5, 4.25, 5.0}, 13 values in total.\nDue to the size of the feature vector we excluded LSTD from the results, while iLSTD was also excluded due to it’s slow runtime and poor performance in Mountain Car. Note that though iLSTD avoids O(d2) computation per step for sparse features, it still needs to store and update anO(d2) matrix, and so does not scale as well as the other sub-quadratic methods."
    } ],
    "references" : [ {
      "title" : "SGD-QN: Careful quasi-Newton stochastic gradient descent",
      "author" : [ "Bordes" ],
      "venue" : "Journal of Machine Learning Research",
      "citeRegEx" : "Bordes,? \\Q2009\\E",
      "shortCiteRegEx" : "Bordes",
      "year" : 2009
    }, {
      "title" : "J",
      "author" : [ "Boyan" ],
      "venue" : "A.",
      "citeRegEx" : "Boyan 1999",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "P",
      "author" : [ "W. Dabney", "Thomas" ],
      "venue" : "S.",
      "citeRegEx" : "Dabney and Thomas 2014",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Policy evaluation with temporal differences: a survey and comparison",
      "author" : [ "Dann" ],
      "venue" : "The Journal of Machine Learning Research",
      "citeRegEx" : "Dann,? \\Q2014\\E",
      "shortCiteRegEx" : "Dann",
      "year" : 2014
    }, {
      "title" : "Incremental Truncated LSTD",
      "author" : [ "Gehring" ],
      "venue" : "In International Joint Conference on Artificial Intelligence",
      "citeRegEx" : "Gehring,? \\Q2016\\E",
      "shortCiteRegEx" : "Gehring",
      "year" : 2016
    }, {
      "title" : "and Bowling",
      "author" : [ "A. Geramifard" ],
      "venue" : "M.",
      "citeRegEx" : "Geramifard and Bowling 2006",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "iLSTD: Eligibility traces and convergence analysis",
      "author" : [ "Geramifard" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Geramifard,? \\Q2007\\E",
      "shortCiteRegEx" : "Geramifard",
      "year" : 2007
    }, {
      "title" : "O",
      "author" : [ "M. Ghavamzadeh", "A. Lazaric", "Maillard" ],
      "venue" : "A.; and Munos, R.",
      "citeRegEx" : "Ghavamzadeh et al. 2010",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "and Palhang",
      "author" : [ "A. Givchi" ],
      "venue" : "M.",
      "citeRegEx" : "Givchi and Palhang 2014",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "P",
      "author" : [ "Hansen" ],
      "venue" : "C.",
      "citeRegEx" : "Hansen 1990",
      "shortCiteRegEx" : null,
      "year" : 1990
    }, {
      "title" : "P",
      "author" : [ "S. Mahadevan", "B. Liu", "Thomas" ],
      "venue" : "S.; Dabney, W.; Giguere, S.; Jacek, N.; Gemp, I.; and 0002, J. L.",
      "citeRegEx" : "Mahadevan et al. 2014",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "Accelerated gradient temporal difference learning algorithms",
      "author" : [ "Meyer" ],
      "venue" : "In IEEE Symposium on Adaptive Dynamic Programming and Reinforcement Learning",
      "citeRegEx" : "Meyer,? \\Q2014\\E",
      "shortCiteRegEx" : "Meyer",
      "year" : 2014
    }, {
      "title" : "and Ribeiro",
      "author" : [ "A. Mokhtari" ],
      "venue" : "A.",
      "citeRegEx" : "Mokhtari and Ribeiro 2014",
      "shortCiteRegEx" : null,
      "year" : 2014
    }, {
      "title" : "L",
      "author" : [ "Prashanth" ],
      "venue" : "A.; Korda, N.; and Munos, R.",
      "citeRegEx" : "Prashanth et al. 2013",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "W",
      "author" : [ "D.F. Salas", "Powell" ],
      "venue" : "B.",
      "citeRegEx" : "Salas and Powell 2013",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "A stochastic quasi-Newton method for online convex optimization",
      "author" : [ "Schraudolph" ],
      "venue" : "In International Conference on Artificial Intelligence and Statistics",
      "citeRegEx" : "Schraudolph,? \\Q2007\\E",
      "shortCiteRegEx" : "Schraudolph",
      "year" : 2007
    }, {
      "title" : "Convergence of general nonstationary iterative methods for solving singular linear equations",
      "author" : [ "Shi" ],
      "venue" : "SIAM Journal on Matrix Analysis and Applications",
      "citeRegEx" : "Shi,? \\Q2011\\E",
      "shortCiteRegEx" : "Shi",
      "year" : 2011
    }, {
      "title" : "A",
      "author" : [ "R. Sutton", "Barto" ],
      "venue" : "G.",
      "citeRegEx" : "Sutton and Barto 1998",
      "shortCiteRegEx" : null,
      "year" : 1998
    }, {
      "title" : "A",
      "author" : [ "R. Sutton", "Barto" ],
      "venue" : "G.",
      "citeRegEx" : "Sutton and Barto 2016",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "A",
      "author" : [ "Sutton, R.S.", "Mahmood" ],
      "venue" : "R.; and White, M.",
      "citeRegEx" : "Sutton et al. 2016",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "Off-policy TD (λ) with a true online equivalence",
      "author" : [ "van Hasselt" ],
      "venue" : "In Conference on Uncertainty in Artificial Intelligence",
      "citeRegEx" : "Hasselt,? \\Q2014\\E",
      "shortCiteRegEx" : "Hasselt",
      "year" : 2014
    }, {
      "title" : "True online TD(lambda)",
      "author" : [ "van Seijen", "H. Sutton 2014] van Seijen", "R. Sutton" ],
      "venue" : "In International Conference on Machine Learning",
      "citeRegEx" : "Seijen et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Seijen et al\\.",
      "year" : 2014
    }, {
      "title" : "True Online Temporal-Difference Learning",
      "author" : [ "van Seijen" ],
      "venue" : "In Journal of Machine Learning Research",
      "citeRegEx" : "Seijen,? \\Q2016\\E",
      "shortCiteRegEx" : "Seijen",
      "year" : 2016
    }, {
      "title" : "D",
      "author" : [ "M. Wang", "Bertsekas" ],
      "venue" : "P.",
      "citeRegEx" : "Wang and Bertsekas 2013",
      "shortCiteRegEx" : null,
      "year" : 2013
    }, {
      "title" : "and White",
      "author" : [ "A.M. White" ],
      "venue" : "M.",
      "citeRegEx" : "White and White 2016",
      "shortCiteRegEx" : null,
      "year" : 2016
    }, {
      "title" : "On convergence of emphatic temporal-difference learning",
      "author" : [ "H. Yu" ],
      "venue" : "In Annual Conference on Learning Theory",
      "citeRegEx" : "Yu,? \\Q2015\\E",
      "shortCiteRegEx" : "Yu",
      "year" : 2015
    }, {
      "title" : "There have been some accelerations proposed to gradient TD algorithms (Mahadevan et al",
      "author" : [ "Meyer" ],
      "venue" : "Dabney and Thomas",
      "citeRegEx" : "Meyer,? \\Q2014\\E",
      "shortCiteRegEx" : "Meyer",
      "year" : 2014
    }, {
      "title" : "Dabney and Thomas (2014) explored a similar update to ATD, but for the control setting and with an incremental update to the Fisher information matrix rather than A used here",
      "author" : [ "Meyer" ],
      "venue" : "Dabney and Thomas",
      "citeRegEx" : "Meyer,? \\Q2014\\E",
      "shortCiteRegEx" : "Meyer",
      "year" : 2014
    }, {
      "title" : "SAGE for GTD similarly indicated little to no gain. Finally, Givchi and Palhang (2014) adapted SGD-QN for TD, and showed some improvements using this diagonal step-size approximation",
      "author" : [ "Meyer" ],
      "venue" : null,
      "citeRegEx" : "Meyer,? \\Q2014\\E",
      "shortCiteRegEx" : "Meyer",
      "year" : 2014
    } ],
    "referenceMentions" : [ ],
    "year" : 2017,
    "abstractText" : "The family of temporal difference (TD) methods span a spectrum from computationally frugal linear methods like TD(λ) to data efficient least squares methods. Least square methods make the best use of available data directly computing the TD solution and thus do not require tuning a typically highly sensitive learning rate parameter, but require quadratic computation and storage. Recent algorithmic developments have yielded several sub-quadratic methods that use an approximation to the least squares TD solution, but incur bias. In this paper, we propose a new family of accelerated gradient TD (ATD) methods that (1) provide similar data efficiency benefits to least-squares methods, at a fraction of the computation and storage (2) significantly reduce parameter sensitivity compared to linear TD methods, and (3) are asymptotically unbiased. We illustrate these claims with a proof of convergence in expectation and experiments on several benchmark domains and a large-scale industrial energy allocation domain.",
    "creator" : "LaTeX with hyperref package"
  }
}