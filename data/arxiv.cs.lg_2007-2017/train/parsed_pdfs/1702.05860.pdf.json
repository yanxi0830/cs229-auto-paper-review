{
  "name" : "1702.05860.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Robust Sparse Estimation Tasks in High Dimensions",
    "authors" : [ "Jerry Li" ],
    "emails" : [ "jerryzli@mit.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n70 2.\n05 86\n0v 2\n[ cs\n.L G\n] 2\n8 Fe\nb 20\nIn this paper we initiate the study of whether or not sparse estimation tasks can be performed efficiently in high dimensions, in the robust setting where an ε-fraction of samples are corrupted adversarially. We study the natural robust version of two classical sparse estimation problems, namely, sparse mean estimation and sparse PCA in the spiked covariance model. For both of these problems, we provide the first efficient algorithms that provide non-trivial error guarantees in the presence of noise, using only a number of samples which is similar to the number required for these problems without noise. In particular, our sample complexities are sublinear in the ambient dimension d. Our work also suggests evidence for new computational-vs-statistical gaps for these problems (similar to those for sparse PCA without noise) which only arise in the presence of noise."
    }, {
      "heading" : "1 Introduction",
      "text" : "In the last couple of decades, there has been a large amount of work in machine learning and statistics on how to exploit sparsity in high dimensional data analysis. Motivated by the ever-increasing quantity and dimensionality of data, the goal at a high level is to utilize the underlying sparsity of natural data to extract meaningful guarantees using a number of samples that is sublinear in the dimensionality of the data. In this paper, we will consider the unsupervised setting, where we have sample access to some distribution with some underlying sparsity, and our goal is to recover this distribution by exploiting this structure. Two natural and well-studied problems in this setting that attempt to exploit sparsity are sparse mean estimation and sparse PCA. In both problems, the shared theme is that we assume that one wishes to find a distinguished sparse direction of a Gaussian data set. However, the algorithms inspired by this line of work tend to be quite brittle—it can be shown that they fail when the model is slightly perturbed.\nThis connects to a major concern in high dimensional data analysis: that of model misspecification. At a high level, the worry is that our algorithms should be able to tolerate the case when our assumed model and the true model do not perfectly coincide. In the distributional setting, this (more or less) corresponds to the regime when a small fraction of our samples are adversarially corrupted. The study of these so-called robust estimators, i.e., estimators which work in the presence of such noise, is a classical subfield of statistics. Unfortunately, the classical algorithms for these problems fail to scale as the dimensionality of the problem grows—either the algorithms run in time which is exponential in the dimension, or the error guarantees for these algorithms degrade substantially as the dimension grows. In a flurry of recent work, we now know new algorithms which circumvent this “curse of dimensionality”: they run efficiently, and provide dimension independent error guarantees. However, these algorithms are unable to exploit any inherent sparsity in the problem.\nThis raises the natural “meta-question”:\nQuestion 1.1. Do the statistical gains (achievable by computationally efficient algorithms) for sparse estimation problems persist in the presence of noise?\n∗Supported by NSF grant CCF-1217921, DOE grant de-sc0008923, NSF CAREER Award CCF-145326, and a NSF Graduate Research Fel-\nlowship\nMore formally: Suppose we are asked to solve some estimation task given samples from some distributionD with some underlying sparsity constraint (e.g. sparse PCA). Suppose now an ε-fraction of the samples are corrupted. Can we still solve the same sparse estimation problem? In this work, we initiate the study of such issues. Interestingly, new gaps between computational and statistical rates seem to emerge in the presence of noise. In particular, while the sparse mean estimation problem was previously quite simple to solve, the efficient algorithms which achieve the minimax rate for this problem break down in the presence of this adversarial noise. More concretely, it seems that the efficient algorithms which are robust to noise run into the same computational issues as those which plague sparse PCA. A very interesting question is whether this phenomenon is inherent to any computationally efficient algorithm."
    }, {
      "heading" : "1.1 Our Contribution",
      "text" : "We study the natural robust versions of two classical, well-studied statistical tasks involving sparsity, namely, sparse mean estimation, and sparse PCA.\nRobust sparse mean estimation Here, we get a set of d-dimensional samples from N (µ, I), where µ is k-sparse, and an ε-fraction of the points are corrupted adversarially. Our goal then is to recover µ. Our main contribution is the following:\nTheorem 1.2 (informal, see Theorem 2.1). There is an efficient algorithm, which given a set of ε-corrupted samples of size Õ(k 2 log d ε2 ) from N (µ, I) where µ is k-sparse, outputs a µ̂ so that with high probability, ‖µ̂− µ‖2 ≤ ε √ log 1/ε.\nThe recovery guarantee we achieve, namely O(ε √\nlog 1/ε), is off by the optimal guarantee by only a factor of√ log 1/ε. Moreover, results of [DKS16] imply that our bound is tight for any efficient SQ algorithm. One can show that information theoretically, it suffices to take O(k log dε2 ) samples to learn the mean to ℓ2 error O(ε), even with corrupted data. Without model misspecification, this problem is quite simple algorithmically: it turns out that the truncated empirical mean achieves the information theoretically optimal rate. However, efficient algorithms for this task break down badly given noise, and to our knowledge there is no simple way of fixing them. Very interestingly, the rate we achieve is off from this information theoretic rate by a k2 vs k factor—the same computational vs. statistical gap that arises in sparse PCA. This phenomenon only seems to appear in the presence of noise, and we conjecture that this is inherent:\nConjecture 1.1. Any efficient algorithm for robust sparse mean estimation needs Ω̃(k 2 log d ε2 ) samples.\nIn Appendix D we give some intuition for why it seems to be true. At a high level, it seems that any technique to detect outliers for the mean must look for sparse directions in which the variance is much larger than it should be; at which point the problem faces the same computational difficulties as sparse PCA. We leave closing this gap as an interesting open problem.\nRobust sparse PCA Here, we study the natural robust analogue of the spiked covariance model. Classically, two problems are studied in this setting. The detection problem is given as follows: given sample access to the distributions, we are asked to distinguish betweenN (0, I), and N (0, I + ρvvT ) where v is a k-sparse unit vector. That is, we wish to understand if we can detect the presence of any sparse principal component. Our main result is the following: Theorem 1.3 (informal, see Theorem 2.2). Fix ρ > 0, and let η = O(ε √ log 1/ε). If ρ > η, there is an efficient algorithm, which given a set of ε-corrupted samples of size O(k 2 log d ρ2 ) which distinguishes between N (0, I), and N (0, I + ρvvT ) with high probability.\nThe condition that ε = Õ(ρ) is necessary (up to log factors), as otherwise the problem is impossible information theoretically. Observe that this (up to log factors) matches the optimal rate for computationally efficient detection for sparse PCA without noise (under reasonable complexity theoretic assumptions, see [BR13, WBS16]), and so it seems that noise does not introduce an additional gap here. The recovery problem is similar, except now we want to recover the planted spike v, i.e. find a u minimizing L(u, v) = 1√ 2 ‖uuT − vvT ‖, which turns out to be the natural measure for this problem. For this, we show:\nTheorem 1.4 (informal, see Theorem 2.3). Fix ε > 0 and 0 < ρ = O(1), and let η = O(ε √ log 1/ε). There is an efficient algorithm, which given a set of ε-corrupted samples of size O(k 2 log d η2 ) from N (0, I + ρvvT ), outputs a u so\nthat L(u, v) = O (\nη ρ\n) with high probability.\nThis rate is non-trivial—in particular, it provides guarantees for recovery of v when the number of samples we take is at the detection threshold. Moreover, up to log factors, our rate is optimal for computationally efficient algorithms– [WBS16] gives an algorithm with rate roughlyO(ε/ρ), and show that this is necessary.\nTechniques We first introduce a simple way to describe the optimization problems used for solving sparse mean estimation and sparse PCA. This approach is very similar to the approach taken by [CRPW12] for solving underdetermined linear systems. We observe that any set S in a Hilbert space naturally induces a dual norm ‖x‖∗S = maxy∈S |〈x, y〉|, and that well-known efficient algorithms for sparse mean estimation and sparse PCA simply compute this norm, and the corresponding dual witness y ∈ S which maximizes this norm, for appropriate choices of S. These norms give us a language to only consider deviations in directions we care about, which allows us to prove concentration bounds which are not true for more traditional norms.\nWe now describe our techniques for robust sparse mean estimation. Our starting point is the convex programming approach of [DKK+16]. We assign each sample point a weight, which morally corresponds to our belief about whether the point is corrupted, and we optimize these weights. In previous work of [DKK+16], the approach was to find weights so that the empirical covariance with these weights looked like the identity in spectral norm.\nUnfortunately, such an approach fundamentally fails for us because the spectrum of the covariance will never concentrate for us with the number of samples we take. Instead, we utilize a novel connection to sparse PCA. We show that if instead we find weights so that the empirical covariance with these weights looks like the identity in the dual norm induced by a natural SDP for sparse PCA (in the noiseless setting), then this suffices to show that the trucnated empirical mean with these weights is close to the truth. We do so by convex programming. While we cannot explicitly write down the feasible set of weights, it is a convex set. Thus, by the classical theory of convex optimization, it suffices to give a separation oracle for this convex set to optimize over this set. We show that in fact the SDP for sparse PCA gives us such a separation oracle, if one is sufficiently careful to always work with sparsity preserving objects. This in turns suffices to allow us to (approximately) find a point in the desired feasible set of points, which we show suffices to recover the true mean.\nWe now turn to robust sparse PCA. We first consider the detection problem, which is somewhat easier technically. Here, we again use the dual norm induced by the SDP for sparse PCA. We show that if we can find weights on the samples (as before) so that the empirical covariance with these samples has minimal dual norm, then the value of the dual norm gives us a distinguisher between the spiked and non-spiked case. To find such a set of weights, we observe that norms are convex, and thus our objective is convex. Thus, as before, to optimize over this set it suffices to give a separation oracle, which again the SDP for sparse PCA allows us to do.\nWe now turn our attention to the recovery problem. Here, the setup is very similar, except now we simultaneously find a set of weights and an “explainer” matrix A so that the empirical covariance with these weights is “maximally explained” by A, in a norm very similar to the one induced by the sparse PCA SDP. Utilizing that norms are convex, we show that this can be done via a convex program using the types of techniques described above, and that the top eigenvector of the optimal A gives us the desired solution. While the convex program would be quite difficult to write down in one shot, it is quite easily expressible using the abstraction of dual norms."
    }, {
      "heading" : "1.2 Related Work",
      "text" : "As mentioned previously, there has been a large amount of work on various ways to exploit sparsity for machine learning and statistics. In the supervised setting, perhaps the most well-known of these is compressive sensing and its variants (see [CW08, HTW15] for more details). We do not attempt to provide an exhaustive overview the field here. Other well-known problems in the same vein include general classes of linear inverse problems, see [CRPW12] and matrix completion ([CR12]).\nThe question of estimating a sparse mean is very related to a classical statistical model known as the Gaussian sequence model, and the reader is referred to [Tsy09, Joh11, Rig15] for in-depth surveys on the area. This problem\nhas also garnered a lot of attention recently in various distributed and memory-limited settings, see [GMN14, SD15, BGM+16]. The study of sparse PCA was initiated in [Joh01] and since yielded a very rich algorithmic and statistical theory ([dEGJL07, dBG08, AW08, WTH09, JNRS10, ACCD11, LZ12, Ma13, BJNP13, CMW13, OMH14, GWL14, CRZ16, BMVX16, PWBM16]). In particular, we highlight a very interesting line of work [BR13, KNV15, MW15, WGL15, WBS16], which give evidence that any computationally efficient estimator for sparse PCA must suffer a sub-optimal statistical rate rate. We conjecture that a similar phenomenon occurs when we inject noise into the sparse mean estimation problem.\nIn this paper we consider the classical notion of corruption studied in robust statistics, introduced back in the 70’s in seminal works of [HR09, Tuk75, HRRS86]. Unfortunately, essentially all robust estimators require exponential time in the dimension to compute ([JP78, Ber06, HM13]). Subsequent work of [LT15, BD15] gave efficient SDPbased estimators for these problems which unfortunately had error guarantees which degraded polynomially with the dimension. However, a recent flurry of work ([DKK+16, LRV16, CSV16, DKK+17, DKS17, DKS16]) have given new, computationally efficient, robust estimators for these problems and other settings which avoid this loss, and are often almost optimal. Independent work of [DSS17] also considers the robust sparse setting. They give a similar result for robust mean estimation, and also consider robust sparse PCA, though in a somewhat different setting than we do, as well as robust sparse linear regression.\nThe questions we consider are similar to learning in the presence of malicious error studied in [Val85, KL93], which has received a lot of attention, particularly in the setting of learning halfspaces ([Ser03, KLS09, ABL14]). They also are connected to work on related models of robust PCA ([Bru09, CLMW11, LMTZ12, ZL14]). We refer the reader to [DKK+16] to a detailed discussion on the relationships between these questions and the ones we study."
    }, {
      "heading" : "2 Definitions",
      "text" : "Throughout this paper, if v is a vector, we will let ‖v‖2 denote its ℓ2 norm. If M is a matrix, we let ‖M‖ denote its spectral norm, we let ‖M‖F denote its Frobenius norm, and we let ‖M‖1 = ∑ ij |Mij | be its ℓ1-norm if it were considered a vector. For any two distributions F,G over Rd, we let dTV(F,G) = 1 2 ∫ Rd\n|F − G|dx denote the total variation distance between the two distributions.\nWe will study the following contamination model:\nDefinition 2.1 (ε-corruption). We say a a set of samples X1, X2, . . . , Xn is an ε-corrupted set of samples from a distribution D if it is generated by the process following process. First, we draw n independent samples from D. Then, an adversary inspects these samples, and changes an ε-fraction of them arbitrarily, then returns these new points to us, in any order. Given an ε-corrupted set of samples, we let Sgood ⊆ [n] denote the indices of the uncorrupted samples, and we let Sbad ⊆ [n] denote the indices of the corrupted samples.\nAs discussed in [DKK+16], this is a strong notion of sample corruption that is able to simulate previously defined notions of error. In particular, this can simulate (up to constant factors) the scenario when our samples do not come fromD, but come from a distributionD′ with total variation distance at most O(ε) fromD. We may now formally define the algorithmic problems we consider.\nRobust sparse mean estimation Here, we assume we get an ε-corrupted set of samples from N (µ, I), where µ is k-sparse. Our goal is to recover µ in ℓ2. It is not hard to show that there is an exponential time estimator which achieves rate Õ(k log d/ε2), and moreover, this rate is optimal (see Appendix A). However, this algorithm requires highly exponential time. We show: Theorem 2.1 (Efficient robust sparse mean estimation). Fix ε, δ > 0, and let k be fixed. Let η = O(ε √ log 1/ε). Given an ε-corrupted set of samplesX1, . . . , Xn ∈ Rd from N (µ, I), where µ is k-sparse, and\nn = Ω\n( min(k2, d) + log ( d2\nk2\n) + log 1/δ\nη2\n) ,\nthere is a poly-time algorithm which outputs µ̂ so that w.p. 1− δ, we have ‖µ− µ̂‖2 ≤ O(η).\nIt is well-known that information theoretically, the best error one can achieve is Θ(ε), as achieved by Fact A.1. We show that it is possible to efficiently match this bound, up to a √ log 1/ε factor. Interestingly, our rate differs from that in Fact A.1: our sample complexity is (roughly) Õ(k2 log d/ε2) versusO(k log d/ε2). We conjecture this is necessary for any efficient algorithm.\nRobust sparse PCA We will consider both the detection and recovery problems for sparse PCA. We first focus detection problem for sparse PCA. Here, we are given ρ > 0, and an ε-corrupted set of samples from a d-dimensional distribution D, where D can is either N (0, I) or N (0, I + ρvvT ) for some k-sparse unit vector v. Our goal is to distinguish between the two cases, using as few samples as possible. It is not hard to show that information theoretically,O(k log d/ρ2) samples suffice for this problem, with an inefficient algorithm (see Appendix A). Our first result is that efficient robust sparse PCA detection is possible, at effectively the best computationally efficient rate: Theorem 2.2 (Robust sparse PCA detection). Fix ρ, δ, ε > 0. Let η = O(ε √\nlog 1/ε). Then, if η = O(ρ), and we are given a we are given a ε-corrupted set of samples from either N (0, I) or N (0, I + ρvvT ) for some k-sparse unit vector v of size\nn = Ω\n( min(d, k2) + log ( d2\nk2\n) + log 1/δ\nρ2\n)\nthen there is a polynomial time algorithm which succeeds with probability 1− δ for detection.\nIt was shown in [BR13] that even without noise, at least n = Ω(k2 log d/ε2) samples are required for any polynomial time algorithm for detection, under reasonable complexity theoretic assumptions. Up to log factors, we recover this rate, even in the presence of noise.\nWe next consider the recovery problem. Here, we are given an ε-corrupted set of samples from N (0, I + ρvvT ), and our goal is to output a u minimizing L(u, v), where L(u, v) = 1√\n2 ‖uuT − vvT ‖. For the recovery problem, we\nrecover the following efficient rate:\nTheorem 2.3 (Robust sparse PCA recovery). Fix ε, ρ > 0. Let η be as in Theorem 2.2. There is an efficient algorithm, which given a set of ε-corrupted samples of size n from N (0, I + ρvvT ), where\nn = Ω\n( min(d, k2) + log ( d2\nk2\n) + log 1/δ\nη2\n) ,\noutputs a u so that\nL(u, v) = O\n( (1 + ρ)η\nρ\n) .\nIn particular, observe that when η = O(ρ), so when ε = Õ(ρ), this implies that we recover v to some small constant error. Therefore, given the same number of samples as in Theorem 2.2, this algorithm begins to provide non-trivial recovery guarantees. Thus, this algorithm has the right “phase transition” for when it begins to work, as this number of samples is likely necessary for any computationally efficient algorithm. Moreover, our rate itself is likely optimal (up to log factors), when ρ = O(1). In the non-robust setting, [WBS16] showed a rate of (roughly) O(ε/ρ) with the same number of samples, and that any computationally efficient algorithm cannot beat this rate. We leave it as an interesting open problem to show if this rate is achievable or not in the presence of error when ρ = ω(1)."
    }, {
      "heading" : "3 Preliminaries",
      "text" : "In this section we provide technical preliminaries that we will require throughout the paper."
    }, {
      "heading" : "3.1 Naive pruning",
      "text" : "We will require the following (straightforward) preprocessing subroutine from [DKK+16] to remove all points which are more than Ω̃(d) away from the true mean.\nFact 3.1 (c.f. Fact 4.18 in [DKK+16]). LetX1, . . . , Xn be an ε-corrupted set of samples fromN (µ, I), and let δ > 0. There is an algorithm NAIVEPRUNE(X1, . . . , Xn, δ) which runs in O(εd\n2n2) time so that with probability 1 − δ, we have that (1) NAIVEPRUNE removes no uncorrupted points, and (2) if Xi is not removed by NAIVEPRUNE, then ‖Xi − µ‖2 ≤ O( √ d log(n/δ)). If these two conditions happen, we say that NAIVEPRUNE has succeeded."
    }, {
      "heading" : "3.2 Concentration inequalities",
      "text" : "In this section we give a couple of concentration inequalities that we will require in the remainder of the paper. These “per-vector” and “per-matrix” concentration guarantees are well-known and follow from (scalar) Chernoff bounds, see e.g. [DKK+16].\nFact 3.2 (Per-vector Gaussian concentration). Fix ε, δ > 0. Let v ∈ Rd be a unit vector, and let X1, . . . , Xn ∼ N (0, I), where\nn = Ω\n( log 1/δ\nε2\n) .\nThen, with probability 1− δ, we have ∣∣∣∣∣ 〈 1 n n∑\ni=1\nXi, v 〉∣∣∣∣∣ ≤ ε .\nFact 3.3 (Per-matrix Gaussian concentration). Fix ε, δ > 0, and suppose ε ≤ 1. Let M ∈ Rd×d be a symmetric matrix, and letX1, . . . , Xn ∼ N (0, I), where\nn = Ω\n( log 1/δ\nε2\n) .\nThen, with probability 1− δ, we have: ∣∣∣∣∣ 〈 1 n n∑\ni=1\nXiX T i − I,M 〉∣∣∣∣∣ ≤ ε ."
    }, {
      "heading" : "3.3 The set Sn,ε",
      "text" : "For any n, ε, define the set\nSn,ε =\n{ w ∈ Rn : n∑\ni=1\nwi = 1, and 0 ≤ wi ≤ 1 (1 − ε)n, ∀i } . (1)\nWe make the following observation. For any subset I ⊆ [n], if we let wI be the vector whose ith coordinate is 1/|I| if i ∈ I and 0 otherwise, we have\nSn,ε = conv {wI : |I| = (1− ε)n} .\nThe set Sn,ε will play a key role in our algorithms. We will think of elements in Sn,ε as weights we place upon our sample points, where higher weight indicates a higher confidence that the sample is uncorrupted, and a lower weight will indicate a higher confidence that the sample is corrupted."
    }, {
      "heading" : "4 Concentration for sparse estimation problems via dual norms",
      "text" : "In this section we give a clean way of proving concentration bounds for various objects which arise in sparse PCA and sparse mean estimation problems. We do so by observing they are instances of a very general “meta-algorithm” we call dual norm maximization. This will prove crucial to proving the correctness of our algorithms for robust sparse recovery. While this may sound similar to the “dual certificate” techniques often used in the sparse estimation literature, these techniques are actually quite different.\nDefinition 4.1 (Dual norm maximization). Let H be a Hilbert space with inner product 〈·, ·〉. Fix any set S ⊆ H. Then the dual norm induced by S, denoted ‖ · ‖∗S , is defined by ‖x‖∗S = supy∈S |〈x, y〉|. The dual norm maximizer of x, denoted dS(x), is the vector dS(x) = argmaxv∈S |〈v, x〉|.\nIn particular, we will use the following two sets. Equip the space of symmetric d× d matrices with the trace inner product, i.e., 〈A,B〉 = tr(AB), so that it is a Hilbert space, and let\nUk = {u ∈ Rd : ‖u‖2 = 1, ‖u‖0 = k} (2) Xk = {X ∈ Rd×d : tr(X) = 1, ‖X‖1 ≤ k,X 0} . (3)\nWe show in Appendix B.1 that existing well-known algorithms for sparse mean recovery and sparse PCA without\nnoise can be naturally written in this fashion.\nAnother detail we will largely ignore in this paper is the fact that efficient algorithms for these problems can only approximately solve the dual norm maximization problem. However, we explain in Appendix B.2 why this does not affect us in any meaningful way. Thus, for the rest of the paper we will assume we have access to the exact maximizer, and the exact value of the norm."
    }, {
      "heading" : "4.1 Concentration for dual norm maximization",
      "text" : "We now show how the above concentration inequalities allow us to derive very strong concentration results for the dual norm maximization problem for Uk andXk. Conceptually, we view these concentration results as being the major distinction between sparse estimation and non-sparse estimation tasks. Indeed, these results are crucial for adapting the convex programming framework for robust estimation to sparse estimation tasks. Additionally, they allow us to give an easy proof that the L1 relaxation works for sparse PCA.\nCorollary 4.1. Fix ε, δ > 0. Let X1, . . . , Xn ∼ N (0, I), where\nn = Ω\n( k + log ( d k ) + log 1/δ\nε2\n) .\nThen ‖ 1n ∑n i=1 Xi‖∗Uk ≤ ε.\nProof. Fix a set of k coordinates, and let S be the set of unit vectors supported on these k coordinates. By Fact 3.2 and a net argument, one can show that for all δ, given n = Ω (\nk+log 1/δ ε2\n) , we have that\n∣∣∣∣∣ 〈 v, 1 n n∑\ni=1\nXi 〉∣∣∣∣∣ ≤ ε ,\nwith probability 1−δ. The result then follows by setting δ′ = ( d k )−1 δ and union bounding over all sets of k coordinates.\nThe second concentration bound, which bounds deviation in Xk norm, uses ideas which are similar at a high level, but requires a bit more technical work.\nTheorem 4.2. Fix ε, δ > 0. Let X1, . . . Xn ∼ N (0, I), where\nn = Ω\n( min(d, k2) + log ( d2\nk2\n) + log 1/δ\nε2\n) .\nThen\n∥∥∥∥∥ 1 n n∑\ni=1\nXiX T i − I ∥∥∥∥∥ ∗\nXk\n≤ ε .\nLet us first introduce the following definition.\nDefinition 4.2. A symmetric sparsity pattern is a set S of indices (i, j) ∈ [d]× [d] so that if (i, j) ∈ S then (j, i) ∈ S. We say that a symmetric matrixM ∈ Rd×d respects a symmetric sparsity pattern S if supp(M) = S.\nWith this definition, we now show:\nLemma 4.3. Let n = O\n( min(d,k2)+log (d 2\nk2)+log 1/δ ε2\n) . Then, with probability 1− δ, the following holds:\n|tr((Σ̂− I)X)| ≤ O(ε), for all symmetric X with ‖X‖0 = k2 and ‖X‖F ≤ 1. (4)\nProof. Fix any symmetric sparsity pattern S so that |S| ≤ k2. By classical arguments one can show that there is a (1/3)-net over all symmetric matrices X with ‖X‖F = 1 respecting S of size at most 9O(min(d,k 2)). By Fact 3.3 and a basic net argument, we know that for any δ′, we know that except with probability 1 − δ′, if we take n = O ( min(d,k2)+log 1/δ′\nε2\n) samples, then for all symmetricX respectingS so that ‖X‖F ≤ 1, we have |tr((Σ̂−I)X)| ≤ ε.\nThe claim then follows by further union bounding over allO (( d2\nk2\n)) symmetric sparsity patterns S with |S| ≤ k2.\nWe will also require the following structural lemma.\nLemma 4.4. Any PSD matrixX so that tr(X) = 1 and ‖X‖1 ≤ k can be written as\nX =\nO(n2/k2)∑\ni=1\nYi ,\nwhere each Yi is symmetric, have ∑O(n2/k2) i=1 ‖Yi‖F ≤ 4, and each Yi is k2-sparse.\nProof. Observe that since X is PSD, then ‖X‖F ≤ tr(X) = 1. For simplicity of exposition, let us ignore that the Yi must be symmetric for this proof. We will briefly mention how to in addition ensure that the Yi are symmetric at the end of the proof. Sort the entries of X in order of decreasing |Xij |. Let Yi be the matrix whose nonzeroes are the ik2 + 1 through (i + 1)k2 largest entries of X , in the same positions as they appear in X . Then we clearly have that ∑ Yi = Xi, and each Yi is exactly k 2-sparse.1 Thus it suffices to show that ∑ ‖Yi‖F ≤ 4. We have ‖Y1‖F ≤ ‖X‖F ≤ 1. Additionally, we have ‖Yi+1‖F ≤ 1 T |Yi|1 k , which follows simply because every nonzero entry of Yi+1 is at most the smallest entry of Yi, and each has exactly k 2 nonzeros (except potentially the last one, but it is not hard to see this cannot affect anything). Thus, in aggregate we have\nO(n2/k2)∑\ni=1\n‖Yi‖F ≤ 1 + O(n2/k2)∑\ni=2\n‖Yi‖F ≤ 1 + O(n2/k2)∑\ni=1\n1T |Yi|1 k = 1+ 1T |X |1 k ≤ 2 ,\nwhich is stronger than claimed.\n1Technically the last Yi may not be k 2 sparse but this is easily dealt with, and we will ignore this case here\nHowever, as written it is not clear that the Yi’s must be symmetric, and indeed they do not have to be. The only real condition we needed was that the Yi’s (1) had disjoint support, (2) summed to X , (3) are each Θ(k\n2) sparse (except potentially the last one), and (4) the largest entry of Yi+1 is bounded by the smallest entry of Yi. It should be clear that this can be done while respecting symmetry by doubling the number of Yi, which also at most doubles the bound in the sum of the Frobenius norms. We omit the details for simplicity.\nProof of Theorem 4.2. Let us condition on the event that (4) holds. We claim then that for all X ∈ X , we must have |tr((Σ̂− I)X)| ≤ O(ε), as claimed. Indeed, by Lemma 4.4, for allX ∈ X , we have that\nX =\nO(d2/k2)∑\ni=1\nYi ,\nwhere each Yi is symmetric, have ∑O(d2/k2) i=1 ‖Yi‖F ≤ 4, and each Yi is k2-sparse. Thus,\n|tr((Σ̂− I)X)| ≤ O(d2/k2)∑\ni=1\n∣∣∣tr((Σ̂− I)Yi) ∣∣∣\n=\nO(d2/k2)∑\ni=1\n‖Yi‖F ∣∣∣∣tr ( (Σ̂− I) Yi‖Yi‖F )∣∣∣∣\n(a) ≤ O(d2/k2)∑\ni=1\n‖Yi‖F ·O(ε)\n(b) ≤ O(ε) ,\nwhere (a) follows since each Yi/‖Yi‖F satisfies the conditions in (4), and (b) follows from the bound on the sum of the Frobenius norms of the Yi."
    }, {
      "heading" : "4.2 Concentration for Sn,ε",
      "text" : "We will require the following concentration inequalities for weighted sums of Gaussians, where the weights come from Sn,ε, as these objects will naturally arise in our algorithms. These bounds follow by applying the above bounds, then carefully union bounding over all choices of possible subsets of ( n εn ) subsets. We need to be careful here since the number of things we are union bounding over increases as n increases. We include the proofs in Appendix C. Theorem 4.5. Fix ε ≤ 1/2 and δ ≤ 1, and fix k ≤ d. There is a η1 = O(ε √ log 1/ε) so that for any η > η1, if\nX1, . . . , Xn ∼ N (0, I) and n = Ω ( min(d,k2)+log (d 2 k2)+log 1/δ η2 ) , then\nPr  ∃w ∈ Sn,ε : ∥∥∥∥∥ 1 n n∑\ni=1\nwiXi ∥∥∥∥∥ ∗\nUk\n≥ η   ≤ δ .\nTheorem 4.6. Fix ε ≤ 1/2 and δ ≤ 1, and fix k ≤ d. There is a η = O(ε √\nlog 1/ε) so that ifX1, . . . , Xn ∼ N (0, I) and n = Ω ( min(d,k2)+log (d 2\nk2)+log 1/δ η2\n) , then we have\nPr  ∃w ∈ Sn,ε : ∥∥∥∥∥ 1 n n∑\ni=1\nwiXiX T i − I ∥∥∥∥∥ ∗\nXk\n≥ η   ≤ δ ."
    }, {
      "heading" : "5 A robust algorithm for robust sparse mean estimation",
      "text" : "This section is dedicated to the description of an algorithm RECOVERROBUSTSMEAN for robustly learning Gaussian sequence models, and the proof of the following theorem: Theorem 5.1. Fix ε, τ > 0. Let η = O(ε √\nlog 1/ε). Given an ε-corrupted set of samples of size n from N (µ, I), where µ is k-sparse\nn = Ω\n( min(k2, d) + log ( d2\nk2\n) + log 1/τ\nη2\n) ,\nthen RECOVERROBUSTSMEAN outputs a µ̂ so that with probability 1− τ, we have ‖µ̂− µ‖2 ≤ O(η).\nOur algorithm builds upon the convex programming framework developed in [DKK+16]. Roughly speaking, the algorithm proceeds as follows. First, it does a simple naive pruning step to remove all points which are more than\nroughly Ω( √ d) away from the mean. Then, for an appropriate choice of δ, it will attempt to (approximately) find a point within the following convex set:\nRobustSMeanCτ =   w ∈ Sn,ε : ∥∥∥∥∥ n∑\ni=1\nwi(Xi − µ)(Xi − µ)T − I ∥∥∥∥∥ ∗\nXk\n≤ τ    . (5)\nThe main difficulty with finding a point in Cτ is that µ is unknown. A key insight of [DKK +16] is that it suffices to create an (approximate) separation oracle for the feasible set, as then we may use classical convex optimization algorithms (i.e. ellipsoid or cutting plane methods) to find a feasible point. In their setting (for a differentCτ ), it turns out that a simple spectral algorithm suffices to give such a separation oracle.\nOur main contribution is the design of separation oracle for Cτ , which requires more sophisticated techniques. In particular, we will ideas developed in analogy to hard thresholding and SDPs similar to those developed for sparse PCA to design such an oracle."
    }, {
      "heading" : "5.1 Additional preliminaries",
      "text" : "Throughout this section, we let X1, . . . , Xn denote an ε-corrupted set of samples fromN (µ, I), where µ is k-sparse. We let Sgood denote the set of uncorrupted samples, and we let Sbad denote the set of corrupted samples. For any set of weights w ∈ Sn,ε, we let wg = ∑ i∈Sgood wi and w b = ∑\ni∈Sbad wi. Throughout this section, we will condition on the following three deterministic events occurring:\nNAIVEPRUNE(X1, . . . , Xn, δ) succeeds, (6)∥∥∥∥∥∥ ∑ i∈Sgood wi(Xi − µ) ∥∥∥∥∥∥ ∗\nUk\n≤ η , ∀w ∈ Sn,2ε , and (7)\n∥∥∥∥∥∥ ∑ i∈Sgood wi(Xi − µ)(Xi − µ)T − wgI ∥∥∥∥∥∥ ∗\nXk\n≤ η , ∀w ∈ Sn,2ε , (8)\nwhere η = O(ε √ log 1/ε). When n = Ω\n( min(k2,d)+log (k 2\nd2)+log 1/δ η2\n) these events simultaneously happen with\nprobability at least 1 − O(δ) by Fact 3.1, Theorem 4.5, Theorem 4.6 and a union bound, and the observation that if w ∈ Sn,ε, then w/wg restricted to the indices in Sgood is in S(1−ε)n,2ε."
    }, {
      "heading" : "5.2 The separation oracle",
      "text" : "Our main result in this section is the description of a polynomial time algorithm ROBUSTSMEANORACLE and the proof of the following theorem of its correctness:\nTheorem 5.2. Fix ε > 0 sufficiently small. Suppose that (7) and (8) hold. Let w∗ denote the set of weights which are uniform over the uncorrupted points. Then, there is a constant 1 ≤ c ≤ 21 so that ROBUSTSMEANORACLE satisfies:\n1. (Completeness) If w = w∗, ROBUSTSMEANORACLE outputs “YES”.\n2. (Soundness) If w 6∈ Ccη the algorithm outputs a hyperplane ℓ : Rn → R so that ℓ(w) ≥ 0 but ℓ(w∗) < 0. Moreover, if the algorithm ever outputs a hyperplane, we have ℓ(w∗) < 0.\nPlugging these guarantees into an ellipsoid (or cutting-plane) method, we obtain the following:\nCorollary 5.3. Fix ε > 0 sufficiently small. Suppose that (7) and (8) hold. There is an algorithm APPROXRECOVERROBUSTSMEAN which queries ROBUSTSMEANORACLE at most poly(d, 1/ε, log 1/δ) times, and so runs in time poly(d, 1/ε, 1/δ) which outputs a w′ so that ‖w − w′‖∞ ≤ ε/(n √ d logn/δ), for some w ∈ Ccτ .\nOur separation oracle, formally described in Algorithm 1, proceeds as follows. Given w ∈ Sn,ε, it forms µ̂ = ‖µ̂′‖∗Uk · dUk(µ̂′), where µ̂ = ∑ wiXi. It then forms the matrix Σ̂ = ∑ wi(Xi − µ̂)(Xi − µ̂)T , and computes\nA = dXk(Σ̂). The algorithm then checks if ∣∣∣〈A, Σ̂〉 ∣∣∣ > C for appropriately chosen threshold C. If it does not, the algorithm outputs “YES”. Otherwise, the algorithm outputs a separating hyperplane given by this matrix A.\nAlgorithm 1 Separation oracle for robust sparse mean estimation.\n1: function ROBUSTSMEANORACLE(X1, . . . , Xn, w) 2: Let µ̂ = ∑ wiXi 3: Let Σ̂ = ∑\nwi(Xi − µ̂)(Xi − µ̂)T 4: Let A = dXk(Σ̂) 5: if |〈A, Σ̂− I〉| ≥ 20η then 6: Let σ = sgn ( 〈A, Σ̂− I〉 )\n7: return the hyperplane ℓ given by\nℓ(w) = σ\n( n∑\ni=1\nwi 〈 A, (Xi − µ̂)(Xi − µ̂)T 〉 − 1 ) − |〈A, Σ̂− I〉| .\n8: else 9: return “YES”\n10: end 11: end function\nWe will require the following two lemmata:\nLemma 5.4. Let ω1, . . . , ωm be a set of non-negative weights that sum to 1. Let a1, . . . , am be any sequence of scalars. Then\nm∑\ni=1\nωia 2 i ≥\n( m∑\ni=1\nωiai\n)2 .\nProof. Let Z be a random variable which is ai with probability ωi. Then E[Z] = ∑ ωiai and E[Z 2] = ∑ ωia 2 i . Then the inequality follows from the fact that E[Z2]− E[Z2] = Var[Z] ≥ 0.\nLemma 5.5. Let u ∈ Rd. Then (‖u‖∗Uk)2 ≤ ‖uuT‖∗Xk ≤ 4(‖u‖∗Uk)2.\nProof. Let v = dUk(u). Then since vv T ∈ Xk, we have that (‖uuT‖∗Xk) ≥ 〈vvT , uuT 〉 = 〈u, v〉2 = (‖u‖∗Uk)2. This proves the first inequality.\nTo prove the other inequality, we first prove the intermediate claim that supM∈Y k2 uTMu ≤ (‖u‖∗Uk)2, where Yk2 is the set of symmetric matrices M with at most k2-non-zeroes satisfying ‖M‖F = 1. Indeed, fix any M ∈ Yk . Let S ⊆ [n] be the set of non-zeroes of dUk(u). This is exactly the set of the k largest elements in u, sorted by absolute value. LetP be the symmetric sparsity pattern respected byM . Fix an arbitrary bijection φ : P \\(S×S) → (S×S)\\P , and letM ′ be the following matrix:\nM ′i,j =    Mij if (i, j) ∈ P ⋂ (S × S) ,\nsgn (uiuj)Mφ−1(i,j) if (i, j) ∈ (S × S) \\ P , 0 otherwise.\nThen we claim that uTMu ≤ uTM ′u. Indeed, we have uTM ′u− uTMu = ∑\n(i,j)∈P\\(S×S) |Mij(uuT )φ(i,j)| −Mij(uuT )i,j\n≥ ∑\n(i,j)∈P\\(S×S) |Mi,j |\n( |(uuT )φ(i,j)| − |(uuT )i,j | ) ≥ 0 ,\nfrom the definition of S. Moreover, for anyM respecting S × S with ‖M‖F = 1, it is not hard to see that uTMu ≤ (‖u‖∗Uk)2. This is because now the problem is equivalent to restricting our attention to the coordinates in S, and asking for the symmetric matrix M ∈ RS×S with ‖M‖F = 1 maximizing uTSMuS , where uS is u restricted to the coordinates in S. This is clearly maximized by M = 1‖uS‖22 uSu T S , which yields the desired expression, since ‖uS‖2 = ‖u‖Uk . We can now prove the original lemma. By Lemma 4.4 we may write A = ∑O(n2/k2) i=1 Yi where each Yi is symmetric, k2-sparse, and have ∑O(n2/k2)\ni=1 ‖Yi‖F ≤ 4. We therefore have\nuTAu =\nO(n2/k2)∑\ni=1\nuTYiu\n=\nO(n2/k2)∑\ni=1\n‖Yi‖F (‖u‖∗Uk)2\n≤ 4(‖u‖∗Uk)2 , as claimed, where the second line follows from the arguments above.\nThroughout the rest of this section, let Yi = Xi − µ, so that so that Yi ∼ N (0, I) if i ∈ Sgood. We first prove the following crucial proposition: Proposition 5.6. Let w ∈ Sn,ε, and let τ ≥ η. Assuming (7) and (8) hold, if ‖ ∑n\ni=1 wiYi‖ ∗ Uk ≥ 3τ , then∥∥∑n\ni=1 wiYiY T i − I ∥∥∗ Xk ≥ τ2 ε .\nProof. Observe that (7) and a triangle inequality together imply that ∥∥∑ i∈Sbad wiYi ∥∥∗ Uk\n≥ 2τ . By definition, this implies there is a k-sparse unit vector u so that\n∣∣〈u,∑i∈Sbad wiYi〉 ∣∣ ≥ 2τ . WLOG assume that 〈u,∑i∈Sbad wiYi〉 ≥ η\n(if the sign is negative a symmetric argument suffices). This is equivalent to the statement that\n∑\ni∈Sbad\nwi wb 〈u, Yi〉 ≥ 2τ wb .\nObserve that the wi/w b are a set of non-negative weights summing to 1. Hence, by Lemma 5.4, we have\n∑\ni∈Sbad\nwi wb\n〈u, Yi〉2 ≥ ( 2τ\nwb\n)2 .\nLet A = uuT . Observe that A ∈ Xk. Then the above inequality is equivalent to the statement that ∑\ni∈Sbad wiY\nT i AYi ≥\nτ2\nwb ≥ 4τ\n2\nε .\nMoreover, by (8), we have\n∣∣∣∣∣∣ ∑ i∈Sgood wiY T i AYi − I ∣∣∣∣∣∣ ≤ η ,\nand together these two inequalities imply that\nn∑\ni=1\nwiYiAYi ≥ 4τ2 ε − η ≥ τ 2 ε ,\nas claimed. The final inequality follows from the definition of η, and since 4 > 2.\nProof of Theorem 5.2. Completeness follows from (8). We will now show soundness. Suppose w 6∈ C21η . We wish to show that we will output a separating hyperplane. From the description of the algorithm, this is equivalent to showing that ‖Σ̂− I‖Xk ≥ 20η. Let µ̂ = ∑n\ni=1 wiXi, and let ∆ = µ− µ̂. By elementary manipulations, we may write ∥∥∥∥∥ n∑\ni=1 wi(Xi − µ̂)(Xi − µ̂)T − I ∥∥∥∥∥ Xk = ∥∥∥∥∥ n∑ i=1 wi(Yi +∆)(Yi +∆) T − I ∥∥∥∥∥ Xk\n(a) = ∥∥∥∥∥ n∑\ni=1\nwiYiY T i +∆∆ T − I ∥∥∥∥∥ Xk\n(b) ≥ ∥∥∥∥∥ n∑\ni=1\nwiYiY T i − I ∥∥∥∥∥ Xk − ∥∥∆∆T ∥∥ Xk\n(c) ≥ ∥∥∥∥∥ n∑\ni=1\nwiYiY T i − I ∥∥∥∥∥ Xk − 4 ‖∆‖2Uk ,\nwhere (a) follows since ∑n\ni=1 wiYi = ∆ by definition, (b) follows from a triangle inequality, and (c) follows from\nLemma 5.5. If ‖∆‖Uk ≤ √ η/2, then the RHS is at least 21η since the second term is at most η, and the first term\nis at least 21η since we assume that w 6∈ C21η . Conversely, if ‖∆‖Uk ≥ √ η/2, then by Proposition 5.6, we have\n‖∑ni=1 wiYiYi − I‖Xk ≥ ‖∆‖2Xk/(6ε) > 48‖∆‖2Xk as long as ε ≤ 1/288. This implies that the RHS is at least 40‖∆‖X 2 k ≥ 20η, as claimed.\nHence, this implies that if w 6∈ C4η, then we output a hyperplane ℓ. It is clear by construction that ℓ(w) ≥ 0; thus, it suffices to show that if we output a hyperplane, that ℓ(w∗) < 0. Letting µ̃ = 1(1−ε)n ∑ i∈Sgood wiYi, we have Observe that we have\nn∑\ni=1\nw∗i (Xi − µ̂)(Xi − µ̂)T − I = 1 (1− ε)n ∑\ni∈Sgood (Yi +∆)(Yi +∆)\nT − I\n= 1\n(1− ε)n\n  ∑\ni∈Sgood YiY\nT i − I\n +∆µ̃T + µ̃∆T +∆∆T\n= 1\n(1− ε)n\n  ∑\ni∈Sgood YiY\nT i − I\n + (∆ + µ̃)(∆ + µ̃)T − µ̃µ̃T .\nHence by the triangle inequality and Lemma 5.5, we have\n∥∥∥∥∥ n∑\ni=1 w∗i (Xi − µ̂)(Xi − µ̂)T − I ∥∥∥∥∥ Xk ≤ ∥∥∥∥∥∥ 1 1(1− ε)n ∑ i∈Sgood YiY T i − I ∥∥∥∥∥∥ ∗\nXk\n+ 4 ( ‖∆+ µ̃‖∗Uk )2 + 4 ( ‖µ̃‖∗Uk )2\n≤ ∥∥∥∥∥∥ 1 1(1− ε)n ∑ i∈Sgood YiY T i − I ∥∥∥∥∥∥ Xk + 8 ( ‖∆‖∗Uk )2\n+ 8 ( ‖µ̃‖∗Uk )2 + 4 ( ‖µ̃‖∗Uk )2\n≤ 13η + 8 ( ‖∆‖∗Uk )2 , (9)\nby (7) and (8).\nObserve that to show that ℓ(w∗) < 0 it suffices to show that ∥∥∥∥∥ n∑\ni=1\nw∗i (Xi − µ̂)(Xi − µ̂)− I ∥∥∥∥∥ ∗\nXk\n< ∥∥∥Σ̂− I ∥∥∥ ∗\nXk . (10)\nIf ‖∆‖∗Uk ≤ √ η/2, then this follows since the quantity on the RHS is at least 20η by assumption, and the quantity on\nthe LHS is at most 17η by (9). If ‖∆‖∗Uk ≥ √ η/2, then by Proposition 5.6, the RHS of (10) is at least ( ‖∆‖∗Uk )2 /(3ε), which dominates the LHS as long as ‖∆‖∗Uk ≥ η and ε ≤ 1/288, which completes the proof."
    }, {
      "heading" : "5.3 Putting it all together",
      "text" : "We now have the ingredients to prove our main theorem. Given what we have, our full algorithm RECOVERROBUSTSMEAN is straightforward: first run NAIVEPRUNE, then run APPROXRECOVERROBUSTSMEAN on the pruned points to output some set of weightsw. We then output ‖µ̂‖UkdUk(µ̂). The algorithm is formally defined in Algorithm 2.\nAlgorithm 2 An efficient algorithm for robust sparse mean estimation\n1: function RECOVERROBUSTSMEAN(X1, . . . , Xn, ε, δ) 2: Let S be the set output by NAIVEPRUNE(X1, . . . , Xn, δ). WLOG assume S = [n]. 3: Let w′ = APPROXRECOVERROBUSTSMEAN(X1, . . . , Xn, ε, δ). 4: Let µ̂ = ∑n i=1 w ′ iXi. 5: return ‖µ̂‖∗UkdUk(µ̂) 6: end function\nProof of Theorem 5.1. Let us condition on the event that (6), (7), and (8) all hold simultaneously. As previously\nmentioned, when n = Ω\n( min(k2,d)+log (k 2\nd2)+log 1/δ η2\n) these events simultaneously happen with probability at least\n1 − O(δ). For simplicity of exposition, let us assume that NAIVEPRUNE does not remove any points. This is okay since if it succeeds, it never removes any good points, so if it removes any points, it can only help us. Moreover, since\nit succeeds, we know that ‖Xi−µ‖2 ≤ O( √\nd log(n/δ)) for all i ∈ [n]. By Corollary 5.3, we know that there is some w ∈ C21η so that ‖w − w′‖∞ ≤ ε/(n √ d logn/δ). We have\n‖µ̂− µ‖Uk = ∥∥∥∥∥ n∑\ni=1\nw′iXi − µ̂ ∥∥∥∥∥ ∗\nUk\n≤ ∥∥∥∥∥ n∑\ni=1\nwiXi − µ̂ ∥∥∥∥∥ ∗\nUk\n+\nn∑\ni=1\n|wi − w′i| ‖Xi − µ‖2\n≤ O(η) +O(ε) ,\nby Proposition 5.6. We now show that this implies that if we let µ′ = ‖µ̂‖∗UkdUk(µ̂), then ‖µ′ − µ‖2 ≤ O(η). Let S be the support of µ′, and let T be the support of µ. Then we have\n‖µ′ − µ‖22 = ∑\ni∈S∩T (µ′i − µi)2 +\n∑\ni∈S\\T (µ′i)\n2 + ∑\ni∈T\\S µ2i .\nObserve that ∑\ni∈S∩T (µ ′ i − µi)2 + ∑ i∈S\\T (µ ′ i) 2 ≤ ( ‖µ̂− µ‖∗Uk )2 , since µ was originally nonzero on the entries in\nS \\ T . Moreover, for all i ∈ T \\ S and j ∈ S \\ T , we have (µ′i)2 ≤ (µ′j)2. Thus we have\n∑\ni∈T\\S µ2i ≤ 2\n  ∑\ni∈T\\S (µ− µ′i)2 +\n∑\ni∈S\\T (µ′j) 2\n  ≤ 2 ( ‖µ̂− µ‖∗Uk )2 .\nTherefore we have ‖µ′ − µ‖22 ≤ 3 ( ‖µ̂− µ‖∗Uk )2 , which implies that ‖µ′ − µ‖2 ≤ O(η), as claimed."
    }, {
      "heading" : "6 An algorithm for robust sparse PCA detection",
      "text" : "In this section, we give an efficient algorithm for detecting a spiked covariance matrix in the presence of adversarial noise. Our algorithm is fairly straightforward: we ask for the set of weights w ∈ Sn,ε so that the empirical second moment with these weights has minimal deviation from the identity in the dual Xk norm. We may write this as a convex program. Then, we check the value of the optimal solution of this convex program. If this value is small, then we say it is N (0, I). if this value is large, then we say it is N (0, I + ρvvT ). We refer to the former as Case 1 and the latter as Case 2. The formal description of this algorithm is given in Algorithm.\nAlgorithm 3 Learning a spiked covariance model, robustly\n1: function DETECTROBUSTSPCA(X1, . . . , Xn, ε, δ, ρ) 2: Let γ be the value of the solution\nmin w∈Sn,ε ∥∥∥∥∥ n∑\ni=1\nwi(XiX T i − I) ∥∥∥∥∥ ∗\nXk\n(11)\n3: if γ < ρ/2 then return Case 1 else return Case 2 4: end function"
    }, {
      "heading" : "6.1 Implementing DETECTROBUSTSPCA",
      "text" : "We first show that the algorithm presented above can be efficiently implemented. Indeed, one can show that by taking the dual of the SDP defining the ‖ · ‖∗Xk norm, this problem can be re-written as an SDP with (up to constant factor blowups) the same number of constraints and variables, and therefore we may solve it using traditional SDP solver techniques.\nAlternatively, one may observe that to optimize Algorithm 4 via ellipsoid or cutting plane methods, it suffices to, given w ∈ Sn,ε, produce a separating hyperplane for the constraint (11). This is precisely what dual norm maximization allows us to do efficiently. It is straightforward to show that the volume of Sn,ε ×Xk is at most exponential in the relevant parameters. Therefore, by the classical theory of convex optimization, (see e.g. [CITE]), for any ξ, we may find a solution w′ and γ′ so that ‖w′ − w∗‖∞ ≤ ξ and γ′ so that |γ − γ′| < ξ for some exact minimizer w∗, where γ is the true value of the solution, in time poly(d, n, 1/ε, log 1/ξ),\nAs mentioned in Section B.2, neither approach will in general give exact solutions, however, both can achieve inverse polynomial accuracy in the parameters in polynomial time. We will ignore these issues of numerical precision throughout the remainder of this section, and assume we work with exact γ.\nObserve that in general it may be problematic that we don’t have exact access to the minimizer w∗, since some of theXi may be unboundedly large (in particular, if it’s corrupted) in norm. However, we only use information about γ. Since γ lives within a bounded range, and our analysis is robust to small changes to γ, these numerical issues do not change anything in the analysis."
    }, {
      "heading" : "6.2 Proof of Theorem 2.2",
      "text" : "We now show that Algorithm 4 provides the guarantees required for Theorem 2.2. We first show that if we are in Case 1, then γ is small:\nLemma 6.1. Let ρ, δ > 0. Let ε, η be as in Theorem 2.2. Let X1, . . . , Xn be an ε-corrupted set of samples from N (0, I) of size n, where n is as in Theorem 2.2. Then, with probability 1− δ, we have γ ≤ ρ/2. Proof. Letw be the uniformweights over the uncorrupted points. Then it from Theorem4.2 that ‖∑w ∑n i=1 wi(XiX T i − I)‖∗Xk ≤ O(η) with probability 1− δ. Since w ∈ Sn,ε, this immediately implies that γ ≤ O(ρ). By setting constants appropriately, we obtain the desired guarantee.\nWe now show that if we are in Case 2, then γ must be large:\nLemma 6.2. Let ρ, δ > 0. Let ε, η, n be as in Theorem 2.2. Let X1, . . . , Xn be an ε-corrupted set of samples from N (0, I) of size n. Then, with probability 1 − δ, we have γ ≥ (1 − ε)ρ − (2 + ρ)η. In particular, for ε sufficiently small, and η = O(ρ), we have that γ > ρ/2.\nProof. Let Σ = I + ρvvT , and let Yi = Σ −1/2Xi, so that if Yi is uncorrupted, then Yi ∼ N (0, I). Let w∗ be\nthe optimal solution to (11). By Theorem 4.6, we have that with probability 1 − δ, we can write ∑ni=1 w∗i YiY Ti = wg(I + N) + B, where ‖N‖∗Xk ≤ η, and B = ∑ i∈Sbad w ∗ i YiY T i . Therefore, we have ∑n i=1 w ∗XiXTi = w g(Σ + Σ1/2NΣ1/2) + Σ1/2BΣ1/2 . By definition, we have\n∥∥∥∥∥ n∑\ni=1\nwi(XiX T i − I) ∥∥∥∥∥ ∗\nXk\n≥ 〈wg(Σ + Σ1/2NΣ1/2) + Σ1/2BΣ1/2 − I, vvT 〉\n≥ wg〈(Σ + Σ1/2NΣ1/2), vvT 〉 − 1 = wg(1 + ρ) + wgvTΣ1/2NΣ1/2v − 1 ≥ (1− ε)ρ+ (1− ε)vTΣ1/2NΣ1/2v − ε .\nIt thus suffices to show that |vTΣ1/2NΣ1/2v| < (1 + ρ)η. Since v is an eigenvector for Σ with eigenvalue 1 + ρ, we have that Σ1/2v = √ ρ+ 1 · v and thus\nvTΣ1/2NΣ1/2v = (1 + ρ)vTNv = (1 + ρ)〈N, vvT 〉 ≤ (1 + ρ)‖N‖∗Xk ≤ (1 + ρ)η .\nLemmas 6.1 and 6.2 together imply the correctness of DETECTROBUSTSPCA and Theorem 2.2."
    }, {
      "heading" : "7 An algorithm for robust sparse PCA recovery",
      "text" : "In this section, we prove Theorem 2.3. We give some intuition here. Perhaps the first naive try would be to simply run the same SDP in (11), and hope that the dual norm maximizer gives you enough information to recover the hidden spike. This would more or less correspond to the simplest modification SDP of the sparse PCA in the nonrobust setting that one could hope gives non-trivial information in this setting. However, this cannot work, for the following straightforward reason: the value of the SDP is always at least O(ρ), as we argued in Section 6. Therefore, the noise can pretend to be some other sparse vector u orthogonal to v, so that the covariance with noise looks like wg(I + ρvvT ) + wgρuuT , so that the value of the SDP can be minimized with the uniform set of weights. Then it is\neasily verified that both vvT and uuT are dual norm maximizers, and so the dual norm maximizer does not uniquely determine v.\nTo circumvent this, we simply add an additional slack variable to the SDP, which is an additional matrix in Xk, which we use to try to maximally explain away the rank-one part of I + ρvvT . This forces the value of the SDP to be very small, which allows us to show that the slack variable actually captures v."
    }, {
      "heading" : "7.1 The algorithm",
      "text" : "Our algorithms and analyses will make crucial use of the following convex set, which is a further relaxation of Xk:\nW(2)k = { X ∈ Rd×d : tr(X) ≤ 2, ‖X‖2 ≤ 1, ‖X‖1 ≤ 3k,X 0 } .\nOur algorithm, given formally in Algorithm 4, will be the following. We solve a convex program which simultaneously chooses a weights in Sn,ε and a matrix A ∈ Wk to minimize theWk distance between the sample covariance with these weights, and A. Our output is then just the top eigenvector of A.\nAlgorithm 4 Learning a spiked covariance model, robustly\n1: function RECOVERROBUSTSPCA(X1 , . . . , Xn, ε, δ, ρ) 2: Let w∗, A∗ be the solution to\nargmin w∈Sn,ε,A∈Xk\n∥∥∥∥∥ n∑\ni=1\nwi(XiX T i − I)− ρA ∥∥∥∥∥ ∗\nW2k\n(12)\n3: Let u be the top eigenector of A∗ 4: return The dUk(u)‖u‖∗Uk , i.e., the vector with all but the top k coordinates of v zeroed out. 5: end function\nThis algorithm can be run efficiently for the same reasons as explained for DETECTROBUSTSPCA. For the rest of the section we will assume that we have an exact solution for this problem. As before, we only use information about A, and since A comes from a bounded space, and our analysis is robust to small perturbations in A, this does not change anything."
    }, {
      "heading" : "7.2 More concentration bounds",
      "text" : "Before we can prove correctness of our algorithm, we require a couple of concentration inequalities for the set Wk.\nLemma 7.1. Fix ε, δ > 0. Let X1, . . . , Xn ∼ N (0, I), where n is as in Theorem 4.2. Then with probability 1− δ ∥∥∥∥∥ 1 n n∑\ni=1\nXiX T i − I ∥∥∥∥∥ ∗\nWk\n≤ O(ε) .\nProof. Let Σ̂ denote the empirical covariance. Observe thatWk ⊆ ⋃∞ i=0 2 −iX2i+1k. Moreover, for any i, by Theorem 4.2, if we take\nn = Ω\n min(d, (2 i+1k)2) + log ( d2 (2i+1k)2 ) + log 1/δ\n(2−iε)2\n \n= Ω\n( min(d, k2) + log ( d2\nk2\n) + 22i log 1/δ\nε2\n) ,\nthen |〈M, Σ̂〉| ≤ ε for all M ∈ 2−iX2i+1k with probability 1− δ/2. In particular, if we take\nn = Ω\n( min(d, k2) + log ( d2\nk2\n) + log 1/δ\nε2\n)\nsamples, then for any i, we have |〈M, Σ̂〉| ≤ ε for allM ∈ 2−1X2i+1k with probability at least 1− δ2 2i /2. By a union bound over all these events, since ∑∞\ni=0 δ 22i ≤ 2δ, we conclude that if we take n to be as above, then |〈M, Σ̂〉| ≤ ε for\nallM ∈ ⋃∞i=0 2−iX2i+1k with probability 1−δ. SinceWk is contained in this set, this implies that ‖Σ̂−Σ‖∗Wk ≤ O(ε) with probability at least 1− δ, as claimed.\nBy the same techniques as in the proofs of Theorems 4.5 and 4.6, we can show the following bound. Because of\nthis, we omit the proof for conciseness.\nCorollary 7.2. Fix ε, δ > 0. Let X1, . . . , Xn ∼ N (0, I) where n is as in Theorem 4.6. Then there is an η = O(ε √ log 1/ε) so that\nPr  ∃w ∈ Sn,ε : ∥∥∥∥∥ n∑\ni=1\nwiXiX T i − I ∥∥∥∥∥ ∗\nWk\n≥ η   ≤ δ ."
    }, {
      "heading" : "7.3 Proof of Theorem 2.3",
      "text" : "In the rest of this section we will condition on the following deterministic event happening:\n∀w ∈ Sn,ε : ∥∥∥∥∥ n∑\ni=1\nwiXiX T i − I ∥∥∥∥∥ ∗\nW2k\n≤ η , (13)\nwhere η = O(ε log 1/ε). By Corollary 7.2, this holds if we take\nn = Ω\n( min(d, k2) + log ( d2\nk2\n) + log 1/δ\nη22\n)\nsamples.\nThe rest of this section is dedicated to the proof of the following theorem, which immediately implies Theorem\n2.3.\nTheorem 7.3. Fix ε, δ, and let η be as in (13). Assume that (13) holds. Let v̂ be the output of RECOVERYROBUSTSPCA(X1, . . . , Xn, ε, δ, ρ). Then L(v̂, v) ≤ O( √ (1 + ρ)η/ρ).\nOur proof proceeds in a couple of steps. LetΣ = I+ρvvT denote the true covariance. We first need the following, technical lemma:\nLemma 7.4. Let M ∈ Wk. Then Σ1/2MΣ1/2 ∈ (1 + ρ)Wk. Proof. Clearly, Σ1/2MΣ1/2 0. Moreover, since Σ1/2 = I + (√1 + ρ − 1)vvT , we have that the maximum value of any element of Σ1/2 is upper bounded by √ 1 + ρ. Thus, we have ‖Σ1/2MΣ1/2‖1 ≤ (1 + ρ)‖M‖1. We also have\ntr(Σ1/2MΣ1/2) = tr(ΣM)\n= tr(M) + ρvTMv ≤ 1 + ρ ,\nsince ‖M‖ ≤ 1. Thus Σ1/2MΣ1/2 ∈ (1 + ρ)Wk , as claimed.\nLet w∗, A∗ be the output of our algorithm. We first claim that the value of the optimal solution is quite small:\nLemma 7.5. ∥∥∥∥∥ n∑\ni=1\nw∗i (XiX T i − I)− ρA∗ ∥∥∥∥∥ ∗\nW2k\n≤ η(1 + ρ) .\nProof. Indeed, if we let w be the uniform set of weights over the good points, and we let A = vvT , then by (13), we have\nn∑\ni=1\nwiXiX T i = Σ 1/2(I +N)Σ1/2 ,\nwhere ‖N‖∗Xk ≤ η, and Σ = I + ρvvT . Thus we have that ∥∥∥∥∥ n∑\ni=1\nwi(XiX T i − I)− ρvvT ∥∥∥∥∥ ∗\nW2k\n= ‖Σ1/2NΣ1/2‖∗W2k\n= max M∈Wk\n∣∣∣tr(Σ1/2NΣ1/2M) ∣∣∣\n= max M∈Wk\n∣∣∣tr(NΣ1/2MΣ1/2) ∣∣∣\n≤ (1 + ρ)‖N‖∗W2k ,\nby Lemma 7.4.\nWe now show that this implies the following:\nLemma 7.6. vTA∗v ≥ 1− (2 + 3ρ)η/ρ. Proof. By (13), we know that we may write ∑n i=1 wi(XiX T i − I) = wgρvvT + B − (1 − wg)I + N , where\nB = ∑\ni∈Sbad wiXiX T i , and ‖N‖∗Wk ≤ (1 + ρ)η. Thus, by Lemma 7.5 and the triangle inequality, we have that\n∥∥wgρvvT +B − ρA ∥∥∗ Wk ≤ η + ‖N‖ ∗ Wk + (1 − wg)‖I‖∗Wk + (1− wg)‖ρA‖∗Wk\n≤ (1 + ρ)η + ε+ ρε ≤ (1 + 2ρ)η + ε .\nNow, since vvT ∈ Wk, the above implies that\n|wgρ+ vTBv − ρvTA∗v| ≤ (1 + 2ρ)η + ε ,\nwhich by a further triangle inequality implies that\n|ρ(1− vTA∗v) + vTBv| ≤ (1 + 2ρ)η + ε+ ερ ≤ (2 + 3ρ)η .\nSince 0 ≤ vTA∗v ≤ 1 (since A ∈ Xk) and B is PSD, this implies that in fact, we have\n0 ≤ ρ(1− vTA∗v) ≤ (2 + 3ρ)η .\nHence vTA∗v ≥ 1− (2 + 3ρ)η/ρ, as claimed.\nLet γ = (2 + 3ρ)η/ρ. The lemma implies that the top eigenvalue of A∗ is at least 1 − γ. Moreover, since A∗ ∈ Xk, as long as γ ≤ 1/2, this implies that the top eigenvector of A∗ is unique up to sign. By the constraint that η ≤ O(min(ρ, 1)), for an appropriate choice of constants, we that γ ≤ 1/10, and so this condition is satisfied. Recall that u is the top eigenvector of A∗. Since tr(A∗) = 1 and A∗ is PSD, we may write A∗ = λ1uuT + A1, where u is the top eigenvector of A∗, λ1 ≥ 1− γ, and ‖A1‖ ≤ γ. Thus, by the triangle inequality, this implies that\n‖ρ(vvT − λ1uuT ) +B‖∗X2k ≤ O(ργ)\nwhich by a further triangle inequality implies that\n‖ρ(vvT − uuT ) +B‖∗X2k ≤ O(ργ) . (14)\nWe now show this implies the following intermediate result:\nLemma 7.7. (vTu)2 ≥ 1−O(γ). Proof. By Lemma 7.6, we have that vTA∗v = λ1(vTu)2 + vTA1v ≥ 1− γ. In particular, this implies that (vTu)2 ≥ (1− 2γ)/λ1 ≥ 1− 3γ, since 1− γ ≤ λ ≤ 1.\nWe now wish to control the spectrum of B. For any subsets S, T ⊆ [d], and for any vector x and any matrix M , let xS denote x restricted to S andMS,T denote the matrix restricted to the rows in S and the columns in T . Let I be the support of u, and let J be the support of the largest k elements of v.\nLemma 7.8. ‖BI,I‖ ≤ O(ργ). Proof. Observe that the condition (14) immediately implies that\n‖ρ(vIvTI − uIuTI ) +BI,I‖ ≤ cργ , (15)\nfor some c, since any unit vector x supported on I satisfies xxT ∈ X2k . Suppose that ‖BI,I‖ ≥ Cγ for some sufficiently large C. Then (15) immediately implies that ‖ρ(vIvTI − uIuTI )‖ ≥ (C − c)ργ. Since (vIvTI − uIuTI ) is clearly rank 2, and satisfies tr(vIv T I −uIuTI ) = 1−‖uI‖22 ≥ 0, this implies that the largest eigenvalue of vIvTI −uIuTI is positive. Let x be the top eigenvector of vIv T I −uIuTI . Then, we have xT (vIvTI −uIuTI )x+xTBx = (C− c)ργ+ xTBx ≥ (C − c)ργ by the PSD-ness of B. If C > c, this contradicts (15), which proves the theorem.\nThis implies the following corollary:\nCorollary 7.9. ‖uI‖22 ≥ 1−O(γ). Proof. Lemma 7.8 and (15) together imply that ‖vIvTI − uIuTI ‖ ≤ O(γ). The desired bound then follows from a reverse triangle inequality.\nWe now show this implies a bound on BJ\\I,J\\I :\nLemma 7.10. ‖BJ\\I,J\\I‖ ≤ O(ργ). Proof. Suppose ‖BJ\\I,J\\I‖ ≥ Cγ for some sufficiently large C. Since u is zero on J \\ I , (14) implies that\n‖ρvJ\\IvTJ\\I + BJ\\I,J\\I‖ ≤ cργ ,\nfor some universal c. By a triangle inequality, this implies that ‖vJ\\I‖22 = ‖vJ\\IvTJ\\I‖ ≥ (C − c)γ. Since v is a unit vector, this implies that ‖vI‖22 ≤ 1− (C − c)γ, which for a sufficiently large C, contradicts Corollary 7.9.\nWe now invoke the following general fact about PSD matrices:\nLemma 7.11. SupposeM is a PSD matrix, written in block form as\nM = ( C D DT E ) .\nSuppose furthermore that ‖C‖ ≤ ξ and ‖E‖ ≤ ξ. Then ‖M‖ ≤ O(ξ). Proof. It is easy to see that ‖M‖ ≤ O(max(‖C‖, ‖D‖, ‖E‖)). Thus it suffices to bound the largest singular value of D. For any vectors φ, ψ with appropriate dimension, we have that\n(φT − ψT ) M (\nφ −ψ\n) = φTAφ− 2φTDψ + ψTCψ ≥ 0 ,\nwhich immediately implies that the largest singular value ofD is at most (‖A‖+‖B‖)/2, which implies the claim.\nTherefore, Lemmas 7.8 and 7.10 together imply:\nCorollary 7.12. ‖vI∪JvTI∪J − uI∪JuTI∪J‖ ≤ O(γ) .\nProof. Observe (14) immediately implies that ‖ρ(vI∪JvTI∪J −uI∪JuTI∪J)+BI∪J,I∪J‖ ≤ O(ργ), since |I ∪J | ≤ 2k. Moreover, Lemmas 7.8 and 7.10 with Lemma 7.11 imply that ‖BI∪J,I∪J‖ ≤ O(ργ), which immediately implies the statement by a triangle inequality.\nFinally, we show this implies ‖vvT − uJuTJ ‖ ≤ O(γ), which is equivalent to the theorem.\nProof of Theorem 7.3. We will in fact show the slightly stronger statement, that ‖uuT − vJvTJ ‖F ≤ O(γ). Observe that since uuT − vvT is rank 2, Corollary 7.12 implies that ‖vI∪JvTI∪J − uI∪JuTI∪J‖F ≤ O(γ), since for rank two matrices, the spectral and Frobenius norm are off by a constant factor. We have\n‖uuT − vvT ‖2F = ∑\n(i,j)∈I∩J×I∩J (uiuj − vivj)2 +\n∑\n(i,j)∈I×I\\J×J (vivj)\n2 + ∑\n(i,j)∈J×J\\I×I (uiuj)\n2 .\nWe have\n∑\n(i,j)∈I∩J×I∩J (uiuj − vivj)2 +\n∑\n(i,j)∈J×J\\I×I (uiuj)\n2 ≤ ‖vI∪JvTI∪J − uI∪JuTI∪J‖2 ≤ O(γ) ,\nby Corollary 7.12. Moreover, we have that\n∑\n(i,j)∈I×I\\J×J (vivj)\n2 ≤ 2\n \n∑\n(i,j)∈I×I\\J×J (vivj − uiuj)2 +\n∑\n(i,j)∈I×I\\J×J (uiuj)\n2  \n≤ 2  ‖vI∪JvTI∪J − uI∪JuTI∪J‖2 +\n∑\n(i,j)∈I×I\\J×J (uiuj)\n2  \n≤ 2  ‖vI∪JvTI∪J − uI∪JuTI∪J‖2 +\n∑\n(i,j)∈J×J\\I×I (uiuj)\n2  \n≤ O(γ) .\nsince J × J contains the k2 largest entries of uuT . This completes the proof."
    }, {
      "heading" : "Acknowledgements",
      "text" : "The author would like to thank Ankur Moitra for helpful advice throughout the project, and Michael Cohen for some surprisingly2 useful conversations.\n2Is it really surprising though?"
    }, {
      "heading" : "B Omitted Details from Section 4",
      "text" : "B.1 Writing non-robust algorithms as dual norm maximization\nIn this section we will briefly review well-known non-robust algorithms for sparse mean recovery and for sparse PCA, and write them using our language.\nThresholding Recall that in the (non-robust) sparse mean estimation problem, one is given samples X1, . . . , Xn ∼ N (µ, I) where µ is k-sparse. The goal is then to recover µ. It turns out the simple thresholding algorithm THRESHOLDMEAN given in Algorithm 5 suffices for recovery:\nAlgorithm 5 Thresholding for sparse mean estimation\n1: function THRESHOLDMEAN(X1, . . . , Xn) 2: Let µ̂ = 1n ∑n i=1 Xi 3: Let S be the set of k coordinates of µ̂ with largest magnitude 4: Let µ̂′ be defined to be µ̂′i = µ̂i if i ∈ S, 0 otherwise 5: return µ̂′ 6: end function\nThe correctness of this algorithm follows from the following folklore result, whose proof we shall omit for conciseness:\nFact B.1 (c.f. [Rig15]). Fix ε, δ > 0, and let X1, . . . , Xn be samples from N (µ, I), where µ is k-sparse and\nn = Ω\n( log ( d k ) + log 1/δ\nε2\n) .\nThen, with probability 1− δ, if µ̂′ is the output of THRESHOLDMEAN, we have ‖µ̂′ − µ̂‖2 ≤ ε.\nTo write this in our language, observe that\nTHRESHOLDSMEAN(X1, . . . , Xn) = ‖µ̂‖∗Uk · dUk(µ̂) ,\nwhere µ̂ = 1n ∑n i=1 Xi.\nL1 relaxation In various scenarios, including recovery of a spiked covariance, one may envision the need to take k-sparse eigenvalues a matrix A, that is, vectors which solve the following non-convex optimization problem:\nmax vTAv\ns.t. ‖v‖2 = 1, ‖v‖0 ≤ k . (16)\nHowever, this problem is non-convex and cannot by solved efficiently. This motivates the following SDP relaxation of (16): First, one rewrites the problem as\nmax tr(AX)\ns.t. tr(X) = 1, ‖X‖0 ≤ k2 , X 0 , rank(X) = 1 (17)\nwhere ‖X‖0 is the number of non-zeros of X . Observe that since X is rank 1 if we let X = vvT these two problems are indeed equivalent. Then to form the SDP, one removes the rank constraint, and relaxes the ℓ0 constraint to a ℓ1 constraint:\nmax tr(AX)\ns.t. tr(X) = 1, ‖X‖1 ≤ k ,X 0 . (18)\nThe work of [dEGJL07] shows that this indeed detects the presence of a spike (but at an information theoretically suboptimal rate).\nFinally, by definition, for any PSD matrix A, ifX is the solution to (18) with input A, we haveX = dXk(A).\nB.2 Numerical precision\nIn general, we cannot find closed form solutions for dXk(A) in finite time. However, it is well-known that we can find these to very high numerical precision in polynomial time. For instance, using the ellipsoid method, we can find an M ′ so that ‖M ′ − dXk(A)‖∞ ≤ ε in time poly(d, log 1/ε). It is readily verified that if we set ε′ = poly(ε, 1/d) then the numerical precision of the answer will not effect any of the calculations we make further on. Thus for simplicity of exposition we will assume throughout the paper that given any A, we can find dXk(A) exactly in polynomial time."
    }, {
      "heading" : "C Omitted Proofs from Section 4",
      "text" : "Proof of Theorem 4.5. Fix n as in Theorem 4.5, and let δ1 = ( n εn )−1 δ. By convexity of Sn,ε and the objective function, it suffices to show that with probability 1− δ, the following holds:\n∀wI s.t. |I| = (1− ε)n : ∥∥∥∥∥ n∑\ni=1\nwiXi ∥∥∥∥∥ ∗\nUk\n≤ η .\nCondition on the event that ∥∥∥∥∥ 1 n n∑\ni=1\nXi ∥∥∥∥∥ ∗\nUk\n≤ ε . (19)\nBy Corollary 4.1, this occurs with probability 1−O(δ).\nFix any I ⊆ [n] so that |I| = (1 − ε)n. By Corollary 4.1 applied to Ic, we have that there is some universal constant C so that as long as\nεn ≥ C · min(d, k 2) + log\n( d2\nk2\n) + log ( n εn ) + log 1/δ α2 , (20)\nthen with probability 1− δ′, ∥∥∥∥∥∥ 1 εn ∑ i6∈I Xi ∥∥∥∥∥∥ ∗\nUk\n≤ α . (21)\nSince log ( n εn ) = Θ(nε log 1/ε), (20) is equivalent to the condition that\nn ( ε− C ε log 1/ε\nα2\n) ≥ C · min(d, k 2) + log ( d2 k2 ) + log 1/δ\nα2 .\nLet α = O( √\nlog 1/ε). By our choice of η, we have that 0 ≤ ε− ε log 1/εη2 ≤ ε/(2C), and by an appropriate setting of constants, since by our choice of n we have\nεn 2 ≥ C · min(d, k\n2) + log ( d2\nk2\n) + log 1/δ\nα2 ,\nwe have that (21) holds with probability 1−δ′. Thus by a union bound over all ( n εn ) choices of I so that |I| = (1−ε)n, we have that except with probability 1− δ, we have that (21) holds simultaneously for all I with |I| = (1− ε)n. The desire result then follows from this and (19), and a union bound.\nProof of Theorem 4.6. This follows from the exact same techniques as the proof of Theorem 4.5, by replacing all Uk with Xk, and using Theorem 4.2 instead of Corollary 4.1."
    }, {
      "heading" : "D Computational Barriers for sample optimal robust sparse mean estimation",
      "text" : "We conjecture that the rate achieved by Theorem 5.1 is tight for computationally efficient algorithms (up to log factors). Intuitively, the major difficulty is that distinguishing between N (µ1, I) and N (µ2, I) given corrupted samples seems to inherently require second moment (or higher) information, for any µ1, µ2 ∈ Rd. Certainly first moment information by itself is insufficient. In this sparse setting, this is very problematic, as this inherently asks for us to detect a large sparse eigenvector of the empirical covariance. This more or less reduces to the problem solved by (16). This in turn requires us to relax to the problem solved by SDPs for sparse PCA, for which we know Ω(k2 log d/ε2) samples are necessary for non-trivial behavior to emerge. We leave resolving this gap as an interesting open problem."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2017,
    "abstractText" : "In this paper we initiate the study of whether or not sparse estimation tasks can be performed efficiently in high dimensions, in the robust setting where an ε-fraction of samples are corrupted adversarially. We study the natural robust version of two classical sparse estimation problems, namely, sparse mean estimation and sparse PCA in the spiked covariance model. For both of these problems, we provide the first efficient algorithms that provide non-trivial error guarantees in the presence of noise, using only a number of samples which is similar to the number required for these problems without noise. In particular, our sample complexities are sublinear in the ambient dimension d. Our work also suggests evidence for new computational-vs-statistical gaps for these problems (similar to those for sparse PCA without noise) which only arise in the presence of noise.",
    "creator" : "LaTeX with hyperref package"
  }
}