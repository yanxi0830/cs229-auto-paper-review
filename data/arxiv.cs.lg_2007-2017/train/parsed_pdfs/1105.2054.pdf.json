{
  "name" : "1105.2054.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Generalized Boosting Algorithms for Convex Optimization",
    "authors" : [ "Alexander Grubb", "Andrew Bagnell" ],
    "emails" : [ "agrubb@cmu.edu", "dbagnell@ri.cmu.edu" ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "Boosting (Schapire, 2002) is a versatile meta-algorithm for combining together multiple simple hypotheses, or weak learners, to form a single complex hypothesis with superior performance. The power of this metaalgorithm lies in its ability to craft hypotheses which can achieve arbitrary performance on training data using only weak learners that perform marginally better than random. This weak to strong learning guarantee is a critical feature of boosting.\nTo date, much of the work on boosting has focused on optimizing the performance of this meta-algorithm\nAppearing in Proceedings of the 28 th International Conference on Machine Learning, Bellevue, WA, USA, 2011. Copyright 2011 by the author(s)/owner(s).\nwith respect to specific loss functions and problem settings. The AdaBoost algorithm (Freund & Schapire, 1997) is perhaps the most well known and most successful of these. AdaBoost focuses specifically on the task of classification via the minimization of the exponential loss by boosting weak binary classifiers together, and can be shown to be near optimal in this setting. Looking to extend upon the success of AdaBoost, related algorithms have been developed for other domains, such as RankBoost (Freund et al., 2003) and mutliclass extensions to AdaBoost (Mukherjee & Schapire, 2010). Each of these algorithms provides both strong theoretical and experimental results for their specific domain, including corresponding weak to strong learning guarantees, but extending boosting to these and other new settings is non-trivial.\nRecent attempts have been successful at generalizing the boosting approach to certain broader classes of problems, but their focus is also relatively restricted. Mukherjee and Schapire (2010) present a general theory of boosting for multiclass classification problems, but their analysis is restricted to the multiclass setting. Zheng et al. (2007) give a boosting method which utilizes the second-order Taylor approximation of the objective to optimize smooth, convex losses. Unfortunately, the corresponding convergence result for their algorithm does not exhibit the typical weak to strong guarantee seen in boosting analyses and their results apply only to weak learners which solve the weighted squared regression problem.\nOther previous work on providing general algorithms for boosting has shown that an intuitive link between algorithms like AdaBoost and gradient descent exists (Mason et al., 1999; Friedman, 2000), and that many existing boosting algorithms can be reformulated to fit within this gradient boosting framework. Under this view, boosting algorithms are seen as performing a modified gradient descent through the space of all hypotheses, where the gradient is calculated and then used to find the weak learner which will provide the best descent direction.\nar X\niv :1\n10 5.\n20 54\nv2 [\ncs .L\nG ]\n1 4\nFe b\n20 12\nIn the case of smooth convex functionals, Mason et al. (1999) give a proof of eventual convergence for this previous work, but no rates of convergence are given. Additionally, convergence rates of these algorithms have been analyzed for the case of smooth convex functionals (Rätsch et al., 2002) and for specific potential functions used in classification (Duffy & Helmbold, 2000) under the traditional PAC weak learning setting.\nOur work aims to rigorously define the mathematics underlying this connection and show how standard boosting notions such as that of weak learner performance can be extended to the general case. Using this foundation, we will present weak to strong learning results for the existing gradient boosting algorithm (Mason et al., 1999; Friedman, 2000) for the special case of smooth convex objectives under our more general setting.\nFurthermore, we will also demonstrate that this existing algorithm can fail to converge on non-smooth objectives, even in finite dimensions. To rectify this issue, we present new algorithms which do have corresponding strong convergence guarantees for all convex objectives, and demonstrate experimentally that these new algorithms often outperform the existing algorithm in practice.\nOur analysis is modeled after existing work on gradient descent algorithms for optimizing over vector spaces. For convex problems standard gradient descent algorithms are known to provide good convergence results (Zinkevich, 2003; Boyd & Vandenberghe, 2004; Hazan et al., 2006) and are widely applicable. However, as detailed above, the modified gradient descent procedure which corresponds to boosting does not directly follow the gradient, instead selecting a descent direction from a restricted set of allowable search directions. This restricted gradient descent procedure requires new extensions to the previous work on gradient descent optimization algorithms.\nA related form of gradient descent with gradient errors has previously been studied in the analysis of budgeted learning (Sutskever, 2009), and general results related to gradient projection errors are given in the literature. While these results apply to the boosting setting, they lack any kind of weak to strong guarantee. Conversely, we are primarily interested in studying what algorithms and assumptions are needed to overcome projection error and achieve strong final performance even in the face of mediocre weak learner performance.\nThe rest of the paper is as follows. We first explicitly detail the Hilbert space of functions and various operations within this Hilbert space. Then, we discuss\nhow to quantify the performance of a weak learner in terms of this vector space. Following that, we present theoretical weak to strong learning guarantees for both the existing and our new algorithms. Finally we provide experimental results comparing all algorithms discussed on a variety of tasks."
    }, {
      "heading" : "2. L2 Function Space",
      "text" : "Previous work (Mason et al., 1999; Friedman, 2000) has presented the theory underlying function space gradient descent in a variety of ways, but never in a form which is convenient for convergence analysis. Recently, Ratliff (2009) proposed the L2 function space as a natural match for this setting. This representation as a vector space is particularly convenient as it dovetails nicely with the analysis of gradient descent based algorithms. We will present here the Hilbert space of functions most relevant to functional gradient boosting, but the later convergence analysis for restricted gradient descent algorithms can be generalized to any Hilbert space.\nGiven a measurable input set X , an output vector space V, and measure µ, the function space L2(X ,V, µ) is the set of all equivalence classes of functions f : X → V such that the Lebesgue integral∫\nX ‖f(x)‖2V dµ (1)\nis finite. We will specifically consider the special case where µ is a probability measure P with density function p(x), so that (1) is equivalent to EP [‖f(x)‖2].\nThis Hilbert space has a natural inner product and norm:\n〈f, g〉P = ∫ X 〈f(x), g(x)〉V p(x) dx\n= EP [〈f(x), g(x)〉V ]\n‖f‖2P = 〈f, f〉P = EP [‖f(x)‖2V ].\nWe parameterize these operations by P to denote their reliance on the underlying data distribution. In the case of the empirical probability distribution P̂ these quantities are simply the corresponding empirical expected value. For example, the inner product becomes\n〈f, g〉P̂ = 1\nN N∑ n=1 〈f(xn), g(xn)〉V\nIn order to perform gradient descent over such a space, we need to compute the gradient of functionals over\nsaid space. We will use the standard definition of a subgradient to allow for optimization of non-smooth functions. Define ∇R[f ] to be a subgradient iff:\nR[f ] ≥ R[g] + 〈f − g,∇R[f ]〉P Here ∇R[f ] is a (function space) subgradient of the functional R : L2(P )→ R at f . Using this definition, these subgradients are straightforward to compute for a number of functionals.\nFor example, for the point-wise loss over a set of training examples,\nRemp[f ] = 1\nN N∑ n=1 l(f(xn), yn)\nthe subgradients in L2(X ,V, P̂ ) are the set:\n∇Remp[f ] = {g | g(xn) ∈ (∇1l)(f(xn), yn)}\nwhere (∇1l)(f(xn), yn) is the set of subgradients of the pointwise loss l with respect to f(xn). For differentiable l, this is just the partial derivative of l with respect to input f(xn).\nSimilarly the expected loss,\nR[f ] = EP [EY [l(f(x), y)]],\nhas the following subgradients in L2(X ,V, P ):\n∇R[f ] = {g | g(x) ∈ EY [(∇1l)(f(x), y)]} ."
    }, {
      "heading" : "3. Restricted Gradient Descent",
      "text" : "We now outline the gradient-based view of boosting (Mason et al., 1999; Friedman, 2000) and how it relates to gradient descent. In contrast to the standard gradient descent algorithm, boosting is equivalent to what we will call the restricted gradient descent setting, where the gradient is not followed directly, but is instead replaced by another search direction from a set of allowable descent directions. We will refer to this set of allowable directions as the restriction set.\nFrom a practical standpoint, a projection step is necessary when optimizing over function space because the functions representing the gradient directly are computationally difficult to manipulate and do not generalize to new inputs well. In terms of the connection to boosting, the restriction set corresponds directly to the set of hypotheses generated by a weak learner.\nWe are primarily interested in two aspects of this restricted gradient setting: first, appropriate ways to find the best allowable direction of descent, and second, a means of quantifying the performance of a restriction set. Conveniently, the function space view of\nAlgorithm 1 Naive Gradient Projection Algorithm\nGiven: starting point f0, step size schedule {ηt}Tt=1\nfor t = 1, . . . , T do Compute subgradient ∇t ∈ ∇R[f ]. Project ∇t onto hypothesis space H, finding nearest direction h∗. Update f : ft ← ft−1 − ηt 〈h∗,∇t〉 ‖h∗‖2 h ∗. end for\nboosting provides a simple geometric explanation for these concerns.\nGiven a gradient∇ and candidate direction h, the closest point h′ along h can be found using vector projection:\nh′ = 〈∇, h〉 ‖h‖2 h (2)\nNow, given a set of possible descent directions H the vector h∗ which minimizes the resulting projection error (2) also maximizes the projected length:\nh∗ = arg max h∈H 〈∇, h〉 ‖h‖ . (3)\nThis is a generalization of the projection operation in Mason et al. (1999) to functions other than classifiers.\nFor the special case whenH is closed under scalar multiplication, one can instead find h∗ by directly minimizing the distance between ∇ and h∗,\nh∗ = arg min h∈H ‖∇ − h‖2 (4)\nthereby reducing the final projected distance found using (2). This projection operation is equivalent to the one given by Friedman (2000).\nThese two projection methods provide relatively simple ways to search over any restriction set for the ‘best’ descent direction. The straightforward algorithm (Mason et al., 1999; Friedman, 2000) for peforming restricted gradient descent which uses these projection operations is given in Algorithm 1.\nIn order to analyze the restricted gradient descent algorithms, we need a way quantify the relative strength of a given restriction set. A guarantee on the performance of each projection step, typically referred to in the traditional boosting literature as the edge of a given weak learner is crucial to the convergence analysis of restricted gradient algorithms.\nFor the projection which maximizes the inner product as in (3), we can use the generalized geometric notion\nof angle to bound performance by requiring that\n〈∇, h〉 ≥ cos θ‖∇‖‖h‖\nwhile the equivalent requirement for the norm-based projection in (4) is\n‖∇ − h‖2 ≤ (1− (cos θ)2)‖∇‖2.\nParameterizing by cos θ, we can now concisely define the performance potential of a restricted set of search directions, which will prove useful in later analysis.\nDefinition 1. A restriction set H has edge γ if for every projected gradient ∇ there exists a vector h ∈ H such that either 〈∇, h〉 ≥ γ‖∇‖‖h‖ or ‖∇ − h‖2 ≤ (1− γ2)‖∇‖2.\nThis definition of edge is parameterized by γ ∈ [0, 1], with larger values of edge corresponding to lower projection error and faster algorithm convergence."
    }, {
      "heading" : "3.1. Relationship to Previous Boosting Work",
      "text" : "Though these projection operations apply to any L2 hypothesis set, they also have convenient interpretations when it comes to specific function classes traditionally used as weak learners in boosting.\nFor a classification-based weak learner with outputs in {−1,+1} and an optimization over single output functions f : X → R, projecting as in (3) is equivalent to solving the weighted classification problem over examples {xn, sgn(∇(xn))}Nn=1 and weights wn = |∇(xn)|.\nThe projection via norm minimization in (4) is equivalent to solving the regression problem\nh∗ = arg min h∈H\n1\nN N∑ n=1 ‖∇(xn)− f(xn)‖2\nusing the gradient outputs as regression targets.\nSimilarly, our notion of weak learner performance in Definition 1 can be related to previous work. Like our measure of edge which quantifies performance over the trivial hypothesis h(x) = 0,∀x, previous work has used similar quantities which capture the advantage over baseline hypotheses.\nFor weak learners which are binary classifiers, as is the case in AdaBoost (Freund & Schapire, 1997), there is an equivalent notion of edge which refers to the improvement in performance over predicting randomly. We can show that Definition 1 is an equivalent measure:\nTheorem 1. For a weak classifier space H with outputs in {−1,+1}, the following statements are equivalent: (1) H has edge γ for some γ > 0, and (2) for any\nnon-negative weights wn over training data xn, there is a classifier h ∈ H which achieves an error of at most ( 12 − δ 2 ) ∑ n wn for some δ > 0.\nA similar result can be shown for more recent work on multiclass weak learners (Mukherjee & Schapire, 2010) when optimizing over functions with multiple outputs f : X → Rk: Theorem 2. For a weak multiclass classifier space H with outputs in {1, . . . ,K}, let the modified hypothesis space H′ contain a hypothesis h′ : X → RK for each h ∈ H such that h′(x)k = 1 if h(x) = k and h′(x) = − 1K−1 otherwise. Then, the following statements are equivalent: (1) H′ has edge γ for some γ > 0, and (2) H satisfies the performance over baseline requirements detailed in Theorem 1 of (Mukherjee & Schapire, 2010).\nProofs and more details on these equivalences can be found in Appendix A."
    }, {
      "heading" : "4. Convergence Analysis",
      "text" : "We now focus on analyzing the behavior of variants of the basic restricted gradient descent algorithm shown in Algorithm 1 on problems of the form:\nmin f∈F Remp[f ],\nwhere allowable descent directions are taken from some restriction set H ⊂ F .\nIn line with previous boosting work, we will specifically consider cases where the edge requirement in Definition 1 is met for some γ, and seek convergence results where the empirical objective Remp[ft] approaches the optimal training performance minf∈F Remp[f ]. This work does not attempt to analyze the convergence of the true risk, R[f ].\nWhile we consider L2 function space specifically, the convergence analysis presented can be extended to optimization over any Hilbert space using restricted gradient descent."
    }, {
      "heading" : "4.1. Smooth Convex Optimization",
      "text" : "An earlier result showing O((1− 1C ) T ) convergence of the objective to optimality for smooth functionals is given by Rätsch, et al. (Rätsch et al., 2002) using results from the optimization literature on coordinate descent. Alternatively, this gives a O(log( 1 )) result for the number of iterations required to achieve error . Similar to our result, this work relies on the smoothness of the objective as well as the weak learner performance, but uses the more restrictive notion of edge\nfrom previous boosting literature specifically tailored to PAC weak learners (classifiers). This previous result also has an additional dependence on the number of weak learners and number of training examples.\nWe will now give a generalization of the result in (Rätsch et al., 2002) which uses our more general definition of weak learner edge. The convergence analysis of Algorithm 1 relies on two critical properties of the objective functional R.\nA functional R is λ-strongly convex if ∀f, f ′ ∈ F :\nR[f ′] ≥ R[f ] + 〈∇R[f ], f ′ − f〉 + λ 2 ‖f ′ − f‖2\nfor some λ > 0, and Λ-strongly smooth if\nR[f ′] ≤ R[f ] + 〈∇R[f ], f ′ − f〉 + Λ 2 ‖f ′ − f‖2\nfor some Λ > 0. Using these two properties, we can now derive a convergence result for unconstrained optimization over smooth functions.\nTheorem 3 (Generalization of Theorem 4 in (Rätsch et al., 2002)). Let Remp be a λ-strongly convex and Λstrongly smooth functional over L2(X , P̂ ) space. Let H ⊂ L2 be a restriction set with edge γ. Let f∗ = arg minf∈FRemp[f ]. Given a starting point f0 and step size ηt = 1 Λ , after T iterations of Algorithm 1 we have:\nRemp[fT ]−Remp[f∗] ≤ (1− γ2λ\nΛ )T (Remp[f0]−Remp[f∗]).\nThe result above holds for the fixed step size 1Λ as well as for step sizes found using a line search along the descent direction. The analysis uses the strong smoothness requirement to obtain a quadratic upper bound on the function and then makes guaranteed progress by selecting the step size which minimizes this bound, with larger gains made for larger values of γ. A complete proof is provided in Appendix B.\nTheorem 3 gives, for strongly smooth objective functionals, a convergence rate of O((1 − γ 2λ Λ ) T ). This is very similar to the O((1− 4γ2)T2 ) convergence of AdaBoost (Freund & Schapire, 1997), with both requiring O(log( 1 )) iterations to get performance within of optimal. While the AdaBoost result generally provides tighter bounds, this relatively naive method of gradient projection is able to obtain reasonably competitive convergence results while being applicable to a much wider range of problems. This is expected, as the proposed method derives no benefit from loss-specific optimizations and can use a much broader class of weak learners. This comparison is a common scenario within\noptimization: while highly specialized algorithms can often perform better on specific problems, general solutions often obtain equally impressive results, albeit less efficiently, while requiring much less effort to implement.\nUnfortunately, the naive approach to restricted gradient descent breaks down quickly in more general cases such as non-smooth objectives. Consider the following example objective over two points x1, x2: R[f ] = 2|f(x1)| + |f(x2)|. Now consider the hypothesis set h ∈ H such that either h(x1) ∈ {−1,+1} and h(x2) = 0 or h(x1) = 0 and h(x2) ∈ {−1,+1}. The algorithm will always select h∗ such that h∗(x2) = 0 when projecting gradients from the example objective, giving a final function with perfect performance on x1 and arbitrarily poor unchanged performance on x2. Even if the loss on training point x2 is substantial, the naive algorithm will not correct it.\nAn algorithm which only ever attempts to project subgradients ofR, such as Algorithm 1, will not be able to obtain strong performance results for cases like these. The algorithms in the next section overcome this obstacle by projecting modified versions of the subgradients of the objective at each iteration."
    }, {
      "heading" : "4.2. General Convex Optimization",
      "text" : "For the convergence analysis of general convex functions we now switch to analyzing the average optimality gap:\n1\nT T∑ t=1 [R[ft]−R[f∗]],\nwhere f∗ = arg min f∈F\n∑T t=1R[f ] is the fixed hypothesis\nwhich minimizes loss.\nBy showing that the average optimality gap approaches 0 as T grows large, for decreasing step sizes, it can be shown that the optimality gap R[ft]−R[f∗] also approaches 0.\nThis analysis is similar to the standard no-regret online learning approach, but we restrict our analysis to the case when Rt = R. This is because the true online setting typically involves receiving a new dataset at every time t, and hence a different data distribution P̂t, effectively changing the underlying L2 function space at every time step, making comparison of quantities at different time steps difficult in the analysis. The convergence analysis for the online case is beyond the scope of this paper and is not presented here.\nThe convergence results to follow are similar to previous convergence results for the standard gradient de-\nAlgorithm 2 Repeated Gradient Projection Algorithm\nGiven: starting point f0, step size schedule {ηt}Tt=1\nfor t = 1, . . . , T do Compute subgradient ∇t ∈ ∇R[f ]. Let ∇′ = ∇t, h∗ = 0. for k = 1, . . . , t do\nProject ∇′ onto hypothesis space H, finding nearest direction h∗k. h∗ ← h∗ + 〈 h∗k,∇\n′〉 ‖h∗k‖2 h∗k.\n∇′ ← ∇′ − h∗k. end for Update f : ft ← ft−1 − ηth∗.\nend for\nscent setting (Zinkevich, 2003; Hazan et al., 2006), but with a number of additional error terms due to the gradient projection step. Sutskever (2009) has previously studied the convergence of gradient descent with gradient projection errors using an algorithm similar to Algorithm 1, but the analysis does not focus on the weak to strong learning guarantee we seek. In order to obtain this guarantee we now present two new algorithms.\nOur first general convex solution, shown in Algorithm 2, overcomes this issue by using a meta-boosting strategy. At each iteration t instead of projecting the gradient ∇t onto a single hypothesis h∗, we use the naive algorithm to construct h∗ out of a small number of restricted steps, optimizing over the distance ‖∇t − h∗‖2. By increasing the number of weak learners trained at each iteration over time, we effectively decrease the gradient projection error at each iteration. As the average projection error approaches 0, the performance of the combined hypothesis approaches optimal. We now give convergence results for this algorithm for both strongly convex and convex functionals.\nTheorem 4. Let Remp be a λ-strongly convex functional over F . Let H ⊂ F be a restriction set with edge γ. Let ‖∇R[f ]‖P̂ ≤ G. Let f∗ = arg minf∈FRemp[f ]. Given a starting point f0 and step size ηt = 2 λt , after T iterations of Algorithm 2 we have:\n1\nT T∑ t=1 [Remp[ft]−Remp[f∗]] ≤ G2 λT (1 + lnT + 1− γ2 γ2 ).\nThe proof (Appendix C) relies on the fact that as the number of iterations increases, our gradient projection error approaches 0 at the rate given in Theorem 3,\nAlgorithm 3 Residual Gradient Projection Algorithm\nGiven: starting point f0, step size schedule {ηt}Tt=1\nLet ∆ = 0. for t = 1, . . . , T do\nCompute subgradient ∇t ∈ ∇R[f ]. ∆← ∆ +∇t.\nProject ∆ onto hypothesis space H, finding nearest direction h∗. Update f : ft ← ft−1 − ηt 〈h∗,∆〉 ‖h∗‖2 h ∗. Update residual: ∆← ∆− 〈h ∗,∆〉 ‖h∗‖2 h ∗\nend for\ncausing the behavior of Algorithm 2 to approach the standard gradient descent algorithm. The additional error term in the result is a bound on the geometric series describing the errors introduced at each time step.\nTheorem 5. Let Remp be a convex functional over F . Let H ⊂ F be a restriction set with edge γ. Let ‖∇R[f ]‖P̂ ≤ G and ‖f‖P̂ ≤ F for all f ∈ F . Let f∗ = arg minf∈FRemp[f ]. Given a starting point f0 and step size ηt =\n1√ t , after T iterations of Algorithm\n2 we have:\n1\nT T∑ t=1 [Remp[ft]−Remp[f∗]] ≤ F 2 2 √ T + G2√ T +2FG 1− γ2 γ2 .\nAgain, the result is similar to the standard gradient descent result, with an added error term dependent on the edge γ.\nAn alternative version of the repeated projection algorithm allows for a variable number of weak learners to be trained at each iteration. An accuracy threshold for each gradient projection can be derived given a desired accuracy for the final hypothesis, and this threshold can be used to train weak learners at each iteration until the desired accuracy is reached.\nAlgorithm 3 gives a second method for optimizing over convex objectives. Like the previous approach, the projection error at each time step is used again in projection, but a new step is not taken immediately to decrease the projection error. Instead, this approach keeps track of the residual error left over after projection and includes this error in the next projection step. This forces the projection steps to eventually account for past errors, preventing the possibility of systematic error being adversarially introduced through the weak learner set.\nAs with Algorithm 2, we can derive similar conver-\ngence results for strongly-convex and general convex functionals for this new residual-based algorithm.\nTheorem 6. Let Remp be a λ-strongly convex functional over F . Let H ⊂ F be a restriction set with edge γ. Let ‖∇R[f ]‖P̂ ≤ G. Let f∗ = arg minf∈FRemp[f ]. Let c = 2γ2 . Given a starting point f0 and step size ηt = 1 λt , after T iterations of Algorithm 3 we have:\n1\nT T∑ t=1 [R[ft]−Remp[f∗]] ≤ 2c2G2 λT (1 + lnT + 2 T ).\nTheorem 7. Let Remp be a convex functional over F . Let H ⊂ F be a restriction set with edge γ. Let ‖∇R[f ]‖P̂ ≤ G and ‖f‖P̂ ≤ F for all f ∈ F . Let f∗ = arg minf∈FRemp[f ]. Let c = 2γ2 . Given a starting point f0 and step size ηt =\n1√ t , after T iterations of\nAlgorithm 3 we have:\n1\nT T∑ t=1 [Remp[ft]−Remp[f∗]] ≤ F 2 2 √ T + c2G2√ T + c2G2 2T 3 2 .\nAgain, the results are similar bounds to those from the non-restricted case. Like the previous proof, the extra terms in the bound come from the penalty paid in projection errors at each time step, but here the residual serves as a mechanism for pushing the error back to later projections. The analysis relies on a bound on the norm of the residual ∆, derived by observing that it is increased by at most the norm of the gradient and then multiplicatively decreased in projection due to the edge requirement. This bound on the size of the residual presents itself in the c term present in the bound. Complete proofs are presented in Appendix C.\nIn terms of efficiency, these two algorithms are similarly matched. For the strongly convex case, the repeated projection algorithm uses O(T 2) weak learners to obtain an average regret O( lnTT + 1 γ2T ), while the residual algorithm uses O(T ) weak learners and has average regretO( lnTγ4T ). The major difference lies in frequency of the gradient evaluation, where the repeated projection algorithm evaluates the gradient much less often than the than the residual algorithm."
    }, {
      "heading" : "5. Experimental Results",
      "text" : "We present preliminary experimental results for these new algorithms on three tasks: an imitation learning problem, a ranking problem and a set of sample classification tasks.\nThe first experimental setup is an optimization problem which results from the Maximum Margin Planning (Ratliff et al., 2009) approach to imitation learning.\nIn this setting, a demonstrated policy is provided as example behavior and the goal is to learn a cost function over features of the environment which produce policies with similar behavior. This is done by optimizing over a convex, non-smooth loss function which minimizes the difference in costs between the current and demonstrated behavior. Previous attempts in the literature have been made to adapt boosting to this setting (Ratliff et al., 2009; Bradley, 2009), similar to the naive algorithm presented here, but no convergence results for this settings are known.\nFigure 1 shows the results of running all three of the algorithms presented here on a sample planning dataset from this domain. The weak learners used were neural networks with 5 hidden units each.\nThe second experimental setting is a ranking task from the Microsoft Learning to Rank Datasets, specifically MSLR-WEB10K (ms:, 2010), using the ranking version of the hinge loss and decision stumps as weak learners. Figure 2 shows the test set disagreement (the percentage of violated ranking constraints) plotted against the number of weak learners.\nAs a final test, we ran our boosting algorithms on several multiclass classification tasks from the UCI Machine Learning Repository (Frank & Asuncion, 2010), using the ‘connect4’, ‘letter’, ‘pendigits’ and ‘satimage’ datasets. All experiments used the multiclass extension to the hinge loss (Crammer & Singer, 2002), along with multiclass decision stumps for the weak learners.\nOf particular interest are the experiments where the naive approach to restricted gradient descent clearly fails to converge (‘connect4’ and ‘letter’). In line\nwith the presented convergence results, both nonsmooth algorithms approach optimal training performance at relatively similar rates, while the naive approach cannot overcome the particular conditions of these datasets and fails to achieve strong performance. In these cases, the naive approach repeatedly cycles through the same weak learners, impeding further optimization progress."
    }, {
      "heading" : "Acknowledgements",
      "text" : "We would like to thank Kevin Waugh, Daniel Munoz and the ICML reviewers for their helpful feedback. This work was conducted through collaborative participation in the Robotics Consortium sponsored by the U.S Army Research Laboratory under the Collaborative Technology Alliance Program, Cooperative Agreement W911NF-10-2-0016."
    }, {
      "heading" : "A. Equivalence of boosting requirements",
      "text" : "First, we demonstrate that our requirement is equivalent to the AdaBoost style weak learning requirement on weak classifiers.\nTheorem 1. For a weak classifier space H with outputs in {−1,+1}, the following statements are equivalent: (1) H has edge γ for some γ > 0, and (2) for any non-negative weights wn over training data xn, there is a classifier h ∈ H which achieves an error of at most ( 12 − δ 2 ) ∑ n wn for some δ > 0.\nProof. To relate the weighted classification setting and our inner product formulation, let weights wn = |∇(xn)| and labels yn = sgn(∇(xn)). We examine classifiers h with outputs in {−1,+1}.\nConsider the AdaBoost weak learner requirement re-written as a sum over the correct examples:∑ n,h(xn)=yn wn ≥ ( 1 2 + δ 2 ) ∑ n wn.\nBreaking the sum over weights into the sum of correct and incorrect weights:\n1 2 ( ∑ n,h(xn)=yn wn − ∑ n,h(xn)6=yn wn) ≥ δ 2 ∑ n wn.\nThe left hand side of this inequality is just N times the inner product 〈∇, h〉, and the right hand side can be re-written as the 1-norm of the weight vector w, giving:\nN〈∇, h〉 ≥ δ‖w‖1 ≥ δ‖w‖2\nFinally, using ‖h‖ = 1 and ‖∇‖2 = 1N ‖w‖ 2 2:\n〈∇, h〉 ≥ δ√ N ‖∇‖‖h‖\nshowing that the AdaBoost requirement implies our requirement for edge γ > δ√ N > 0.\nWe can show the converse by starting with our weak learner requirement and expanding:\n〈∇, h〉 ≥ γ‖∇‖‖h‖ 1\nN ( ∑ n,h(xn)=yn wn − ∑ n,h(xn) 6=yn wn) ≥ γ‖∇‖\nThen, because ‖∇‖2 = 1N ‖w‖ 2 2 and ‖w‖2 ≥ 1√ N ‖w‖1 we get:∑\nn,h(xn)=yn\nwn − ∑\nn,h(xn)6=yn\nwn ≥ γ 1\nN ‖w‖1\n≥ γ ∑ n wn\n∑ n,h(xn)=yn wn ≥ ( 1 2 + γ 2 ) ∑ n wn,\ngiving the final AdaBoost edge requirement.\nIn the first part of this proof, the scaling of 1√ N shows that our implied edge weakens as the number of data points increases in relation to the AdaBoost style edge requirement, an unfortunate but necessary feature. This weakening is necessary because our notion of strong learning is much more general than other boosting frameworks. In those settings, strong learning only guarantees that any dataset can be classified with 0 training error, while our strong learning guarantee gives optimal performance on any convex loss function.\nTheorem 2. For a weak multiclass classifier space H with outputs in {1, . . . ,K}, let the modified hypothesis space H′ contain a hypothesis h′ : X → RK for each h ∈ H such that h′(x)k = 1 if h(x) = k and h′(x) = − 1K−1 otherwise. Then, the following statements are equivalent: (1) H′ has edge γ for some γ > 0, and (2) H satisfies the performance over baseline requirements detailed in Theorem 1 of (Mukherjee & Schapire, 2010).\nProof. In this section we consider the multiclass extension of the previous setting. Instead of a weight vector we now have a matrix of weights w where wnk is the weight or reward for classifying example xn as class k. We can simply let weights wnk = ∇(xnk) and use the same weak learning approach as in (Mukherjee & Schapire, 2010). Given classifiers h(x) which output a label in {1, . . . ,K}, we convert to an appropriate weak learner for our setting by building a function h′(x) which outputs a vector y ∈ RK such that yk = 1 if h(x) = k and yk = − 1K−1 otherwise.\nThe equivalent AdaBoost style requirement uses costs cnk = −wnk and minimizes instead of maximizing, but here we state the weight or reward version of the requirement. More details on this setting can be found in (Mukherjee & Schapire, 2010). We also make the additional assumption that ∑ kwnk = 0,∀n without loss of generality. This assumption is fine as we can take a given weight matrix w and modify each row so it has 0 mean, and still have a valid classification matrix as per (Mukherjee & Schapire, 2010). Furthermore, this modification does not affect the edge over random performance of a multiclass classifier under their framework.\nAgain consider the multiclass AdaBoost weak learner requirement re-written as a sum of the weights over the predicted class for each example:\n∑ n wnh(xn) ≥ ( 1 K − δ K ) ∑ n,k wnk + δ ∑ n wnyn\nwe can then convert the sum over correct labels to the max-norm on weights and multiply through by KK−1 :\n∑ n wnh(xn) ≥ 1 K ∑ n,k wnk − δ K ∑ n,k wnk + δ ∑ n wnyn\nK K − 1 ∑ n wnh(xn) ≥ 1 K − 1 ∑ n,k wnk + K K − 1 (δ ∑ n ‖wn‖∞ − δ K ∑ n,k wnk)\nK K − 1 ∑ n wnh(xn) − 1 K − 1 ∑ n,k wnk ≥ K K − 1 (δ ∑ n ‖wn‖∞ − δ K ∑ n,k wnk)\nby the fact that the correct label yn = arg maxk wnk.\nThe left hand side of this inequality is just the function space inner product:\nN〈∇, h′〉 ≥ K K − 1 (δ ∑ n ‖wn‖∞ − δ K ∑ n,k wnk).\nUsing the fact that ∑ k wnk = 0 along with ‖∇‖ ≤ 1√ N ∑ n ‖wn‖2 and ‖h′‖ = √ K K−1 we can now bound the\nright hand side:\nN〈∇, h′〉 ≥ K K − 1 δ ∑ n ‖wn‖∞\n≥ K K − 1 δ ∑ n ‖wn‖2 ≥ K K − 1 δ √ N‖∇‖\n≥ √ K\nK − 1 δ √ N‖∇‖‖h′‖\n〈∇, h〉 ≥ √ K\nK − 1 δ 1√ N ‖∇‖‖h′‖\nFor K ≥ 2 we get γ ≥ δ√ N , showing that the existence of the AdaBoost style edge implies the existence of ours. Again, while the requirements are equivalent for some fixed dataset, we see a weaking of the implication as the dataset grows large, an unfortunate consequence of our broader strong learning goals.\nNow to show the other direction, start with the inner product formulation:\n〈∇, h′〉 ≥ δ‖∇‖‖h′‖ 1 N ( ∑ n wnh(xn) − 1 K − 1 ∑\nn,k 6=h(xn)\nwnk) ≥ δ‖∇‖‖h′‖\n1\nN (\nK K − 1 ∑ n wnh(xn) − 1 K − 1 ∑ n,k wnk) ≥ δ‖∇‖‖h′‖\nUsing ‖h′‖ = √\nK K−1 and ‖∇‖ ≥ 1 N ∑ n ‖wn‖2 we can show:\nK K − 1 ∑ n wnh(xn) − 1 K − 1 ∑ n,k wnk ≥ δ ∑ n ‖wn‖2\n√ K\nK − 1 .\nRearranging we get:\nK K − 1 ∑ n wnh(xn) ≥ 1 K − 1 ∑ n,k wnk + δ ∑ n ‖wn‖2\n√ K\nK − 1∑ n wnh(xn) ≥ 1 K ∑ n,k wnk + K − 1 K √ K K − 1 δ ∑ n ‖wn‖2\n∑ n wnh(xn) ≥ 1 K ∑ n,k wnk +\n√ K\nK − 1 δ( ∑ n ‖wn‖2 − 1 K ∑ n ‖wn‖2)\nNext, bound the 2-norms using ‖wn‖2 ≥ 1√ K ‖wn‖1 and ‖wn‖2 ≥ ‖wn‖∞ and then rewrite as sums of corresponding weights to show the multiclass AdaBoost requirement holds:∑ n wnh(xn) ≥ ( 1 K − δ√ K − 1K ) ∑ n,k wnk + √ K K − 1 δ ∑ n ‖wn‖∞\n∑ n wnh(xn) ≥ ( 1 K − δ K ) ∑ n,k wnk + δ ∑ n wnyn"
    }, {
      "heading" : "B. Smooth Convergence Results",
      "text" : "For the proofs in this section, all norms and inner products are assumed to be with respect to the empirical distribution P̂ .\nTheorem 3. Let Remp be a λ-strongly convex and Λ-strongly smooth functional over L2(X , P̂ ) space. Let H ⊂ L2 be a restriction set with edge γ. Let f∗ = arg minf∈FRemp[f ]. Given a starting point f0 and step size ηt = 1 Λ , after T iterations of Algorithm 1 we have:\nRemp[fT ]−Remp[f∗] ≤ (1− γ2λ\nΛ )T (Remp[f0]−Remp[f∗]).\nProof. Starting with the definition of strong smoothness, and examining the objective value at time t + 1 we have:\nR[ft+1] ≤ R[ft] + 〈∇R[ft], ft+1 − ft〉 + Λ\n2 ‖ft+1 − ft‖2\nThen, using ft+1 = 1 Λ 〈∇R[ft],ht〉 ‖ht‖2 ht we get:\nR[ft+1] ≤ R[ft]− 1\n2Λ\n〈∇R[ft], ht〉2\n‖ht‖2\nSubtracting the optimal value from both sides and applying the edge requirement we get:\nR[ft+1]−R[f∗] ≤ R[ft]−R[f∗]− γ\n2Λ ‖∇R[ft]‖2\nFrom the definition of strong convexity we know ‖∇R[ft]‖2 ≥ 2λ(R[ft]−R[f∗]) where f∗ is the minimum point. Rearranging we can conclude that:\nR[ft+1]−R[f∗] ≤ (R[ft]−R[f∗])(1− γλ\nΛ )\nRecursively applying the above bound starting at t = 0 gives the final bound on R[fT ]−R[f0]."
    }, {
      "heading" : "C. General Convergence Results",
      "text" : "For the proofs in this section, all norms and inner products are assumed to be with respect to the empirical distribution P̂ .\nTheorem 4. Let Remp be a λ-strongly convex functional over F . Let H ⊂ F be a restriction set with edge γ. Let ‖∇R[f ]‖P̂ ≤ G. Let f∗ = arg minf∈FRemp[f ]. Given a starting point f0 and step size ηt = 2 λt , after T iterations of Algorithm 2 we have:\n1\nT T∑ t=1 [Remp[ft]−Remp[f∗]] ≤ G2 λT (1 + lnT + 1− γ2 γ2 ).\nProof. First, we start by bounding the potential ‖ft − f∗‖2, similar to the potential function arguments in (Zinkevich, 2003; Hazan et al., 2006), but with a different descent step:\n‖ft+1 − f∗‖2 ≤ ‖ft − ηt(ht)− f∗‖2\n= ‖ft − f∗‖2 + η2t ‖ht‖ 2 − 2ηt〈ft − f∗, ht −∇t〉 − 2ηt〈ft − f∗,∇t〉\n〈f∗ − ft,∇t〉 ≤ 1\n2ηt ‖ft+1 − f∗‖2 −\n1\n2ηt ‖ft − f∗‖2 − ηt 2 ‖ht‖2 − 〈f∗ − ft, ht −∇t〉\nUsing the definition of strong convexity and summing:\nT∑ t=1 R[f∗] ≥ T∑ t=1 R[ft] + T∑ t=1 〈f∗ − ft,∇t〉 + T∑ t=1 λ 2 ‖f∗ − ft‖2\n≥ T∑ t=1 R[ft]− 1 η1 ‖f1 − f∗‖2 + T−1∑ t=1 1 2 ‖ft+1 − f∗‖2( 1 ηt − 1 ηt+1 + λ)−\nT∑ t=1 ηt 2 ‖ht‖2 − T∑ t=1 〈f∗ − ft, ht −∇t〉\nSetting ηt = 2 γt and use bound ‖ht‖ ≤ 2‖∇t‖ ≤ 2G :\nT∑ t=1 R[f∗] ≥ T∑ t=1 R[ft]− 4G2 2 T∑ t=1 2 λt − λ 4 T∑ t=1 (‖ft − f∗‖2 − T∑ t=1 〈f∗ − ft, ht −∇t〉)\n≥ T∑ t=1 R[ft]− 4G2 λ (1 + lnT )− 1 λ T∑ t=1 ‖ht −∇t‖2\nUsing the result from 3 we can bound the error at each step t:\nT∑ t=1 R[f∗] ≥ T∑ t=1 R[ft]− 4G2 λ (1 + lnT )− G 2 λ T∑ t=1 (1− γ2)t\n≥ T∑ t=1 R[ft]− 4G2 λ (1 + lnT )− G 2 λ 1− γ2 γ2\ngiving the final bound.\nTheorem 5. Let Remp be a convex functional over F . Let H ⊂ F be a restriction set with edge γ. Let ‖∇R[f ]‖P̂ ≤ G and ‖f‖P̂ ≤ F for all f ∈ F . Let f∗ = arg minf∈FRemp[f ]. Given a starting point f0 and step size ηt =\n1√ t , after T iterations of Algorithm 2 we have:\n1\nT T∑ t=1 [Remp[ft]−Remp[f∗]] ≤ F 2 2 √ T + G2√ T + 2FG 1− γ2 γ2 .\nProof. Like the last proof, we start with the altered potential and sum over the definition of convexity:\nT∑ t=1 R[f∗] ≥ T∑ t=1 R[ft]− 1 η1 ‖f1 − f∗‖2 + T−1∑ t=1 1 2 ‖ft+1 − f∗‖2( 1 ηt − 1 ηt+1 )−\nT∑ t=1 ηt 2 ‖ht‖2 − T∑ t=1 〈f∗ − ft, ht −∇t〉\nSetting ηt = 1√ t and using bound ‖ht‖ ≤ ‖∇t‖ ≤ G and the result from 3 we can bound the error at each step t:\nT∑ t=1 R[f∗] ≥ T∑ t=1 R[ft]− 1 ηT ‖fT − f∗‖2 − G2 2 T∑ t=1 1√ t − T∑ t=1 〈f∗ − ft, ht −∇t〉\n≥ T∑ t=1 R[ft]− F 2 √ T 2 −G2 √ T − FG T∑ t=1 √ (1− γ2)t\n≥ T∑ t=1 R[ft]− F 2 √ T 2 −G2 √ T − 2FG1− γ 2 γ2\ngiving the final bound.\nTheorem 6. Let Remp be a λ-strongly convex functional over F . Let H ⊂ F be a restriction set with edge γ. Let ‖∇R[f ]‖P̂ ≤ G. Let f∗ = arg minf∈FRemp[f ]. Let c = 2 γ2 . Given a starting point f0 and step size ηt = 1 λt , after T iterations of Algorithm 3 we have:\n1\nT T∑ t=1 [R[ft]−Remp[f∗]] ≤ 2c2G2 λT (1 + lnT + 2 T ).\nProof. Like the proof of Theorem 4, we again use a potential function and sum over the definition of convexity:\nT∑ t=1 R[f∗] ≥ T∑ t=1 R[ft]− 1 η1 ‖f1 − f∗‖2 + T−1∑ t=1 1 2 ‖ft+1 − f∗‖2( 1 ηt − 1 ηt+1 + λ)−\nT∑ t=1 ηt 2 ‖ht‖2 − T∑ t=1 〈f∗ − ft, ht − (∆t +∇t)〉 − T−1∑ t=0 〈f∗ − ft+1,∆t+1〉\n≥ T∑ t=1 R[ft]− 1 η1 ‖f1 − f∗‖2 + T−1∑ t=1 1 2 ‖ft+1 − f∗‖2( 1 ηt − 1 ηt+1 + λ)−\nT∑ t=1 ηt 2 ‖ht‖2 − T∑ t=1 〈f∗ − ft, ht − (∆t +∇t)〉 − T−1∑ t=0 〈f∗ − ft,∆t+1〉 − T−1∑ t=0 〈ηtht,∆t+1〉\nwhere ht is the augmented step taken in Algorithm 3.\nSetting ηt = 1 γt and use bound ‖ht‖ ≤ ‖∇t‖ ≤ G, along with ∆t+1 = (∆t +∇t)− ht:\nT∑ t=1 R[f∗] ≥ T∑ t=1 R[ft] T∑ t=1 ηt 2 ‖ht‖2 − (〈f∗ − fT+1,∆t+1〉 − λT 2 ‖f∗ − fT+1‖2)− T∑ t=1 〈ηtht,∆t+1〉\nWe can bound the norm of ∆t by considering that (a) it start at 0 and (b) at each time step it increases by at\nmost ∇t and is multiplied by 1− γ2. This implies that ‖∆t‖ ≤ cG where c = √ 1−γ2\n1− √ 1−γ2 < 2γ2 .\nFrom here we can get a final bound:\nT∑ t=1 R[f∗] ≥ T∑ t=1 R[ft]− c2G2 λ (1 + lnT )− 2c 2G2 λT − c 2G2 λ (1 + lnT )\nTheorem 7. Let Remp be a convex functional over F . Let H ⊂ F be a restriction set with edge γ. Let ‖∇R[f ]‖P̂ ≤ G and ‖f‖P̂ ≤ F for all f ∈ F . Let f∗ = arg minf∈FRemp[f ]. Let c = 2 γ2 . Given a starting point\nf0 and step size ηt = 1√ t , after T iterations of Algorithm 3 we have:\n1\nT T∑ t=1 [Remp[ft]−Remp[f∗]] ≤ F 2 2 √ T + c2G2√ T + c2G2 2T 3 2 .\nProof. Similar to the last few proofs, we get a result similar to the standard gradient version, with the error term from the last proof:\nT∑ t=1 R[f∗] ≥ T∑ t=1 R[ft]− 1 η1 ‖f1 − f∗‖2 + T−1∑ t=1 1 2 ‖ft+1 − f∗‖2( 1 ηt − 1 ηt+1 )−\nT∑ t=1 ηt 2 ‖ht‖2 − (〈f∗ − fT+1,∆t+1〉 − √ T 2 ‖f∗ − fT+1‖2)− T∑ t=1 〈ηtht,∆t+1〉\nUsing the bound on ‖∆t‖ ≤ c from above and setting ηt = 1√t :\nT∑ t=1 R[f∗] ≥ T∑ t=1 R[ft]− F 2 √ T 2 − c2G2 √ T − c 2G2 2 √ T\ngiving the final bound."
    } ],
    "references" : [ {
      "title" : "Convex Optimization",
      "author" : [ "S. Boyd", "L. Vandenberghe" ],
      "venue" : null,
      "citeRegEx" : "Boyd and Vandenberghe,? \\Q2004\\E",
      "shortCiteRegEx" : "Boyd and Vandenberghe",
      "year" : 2004
    }, {
      "title" : "Learning in Modular Systems",
      "author" : [ "D.M. Bradley" ],
      "venue" : "PhD thesis,",
      "citeRegEx" : "Bradley,? \\Q2009\\E",
      "shortCiteRegEx" : "Bradley",
      "year" : 2009
    }, {
      "title" : "On the algorithmic implementation of multiclass kernel-based vector machines",
      "author" : [ "K. Crammer", "Y. Singer" ],
      "venue" : "J. Mach. Learn. Res.,",
      "citeRegEx" : "Crammer and Singer,? \\Q2002\\E",
      "shortCiteRegEx" : "Crammer and Singer",
      "year" : 2002
    }, {
      "title" : "Potential boosters? In Advances in Neural Information",
      "author" : [ "Duffy", "Nigel", "Helmbold", "David" ],
      "venue" : "Processing Systems",
      "citeRegEx" : "Duffy et al\\.,? \\Q2000\\E",
      "shortCiteRegEx" : "Duffy et al\\.",
      "year" : 2000
    }, {
      "title" : "A decision-theoretic generalization of on-line learning and an application to boosting",
      "author" : [ "Y. Freund", "R.E. Schapire" ],
      "venue" : "Journal of Computer and System Sciences,",
      "citeRegEx" : "Freund and Schapire,? \\Q1997\\E",
      "shortCiteRegEx" : "Freund and Schapire",
      "year" : 1997
    }, {
      "title" : "An efficient boosting algorithm for combining preferences",
      "author" : [ "Y. Freund", "R. Iyer", "R.E. Schapire", "Y. Singer" ],
      "venue" : "J. Mach. Learn. Res.,",
      "citeRegEx" : "Freund et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Freund et al\\.",
      "year" : 2003
    }, {
      "title" : "Greedy function approximation: A gradient boosting machine",
      "author" : [ "J.H. Friedman" ],
      "venue" : "Annals of Statistics,",
      "citeRegEx" : "Friedman,? \\Q2000\\E",
      "shortCiteRegEx" : "Friedman",
      "year" : 2000
    }, {
      "title" : "Logarithmic regret algorithms for online convex optimization",
      "author" : [ "E. Hazan", "A. Kalai", "S. Kale", "A. Agarwal" ],
      "venue" : "In Proceedings of the 19th Annual Conference on Learning Theory, pp",
      "citeRegEx" : "Hazan et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Hazan et al\\.",
      "year" : 2006
    }, {
      "title" : "Functional gradient techniques for combining hypotheses. In Advances in Large Margin Classifiers",
      "author" : [ "L. Mason", "J. Baxter", "P.L. Bartlett", "M. Frean" ],
      "venue" : null,
      "citeRegEx" : "Mason et al\\.,? \\Q1999\\E",
      "shortCiteRegEx" : "Mason et al\\.",
      "year" : 1999
    }, {
      "title" : "A theory of multiclass boosting",
      "author" : [ "I. Mukherjee", "R.E. Schapire" ],
      "venue" : "In Advances in Neural Information Processing Systems 22,",
      "citeRegEx" : "Mukherjee and Schapire,? \\Q2010\\E",
      "shortCiteRegEx" : "Mukherjee and Schapire",
      "year" : 2010
    }, {
      "title" : "Learning to Search: Structured Prediction Techniques for Imitation Learning",
      "author" : [ "N. Ratliff" ],
      "venue" : "PhD thesis,",
      "citeRegEx" : "Ratliff,? \\Q2009\\E",
      "shortCiteRegEx" : "Ratliff",
      "year" : 2009
    }, {
      "title" : "Learning to search: Functional gradient techniques for imitation learning",
      "author" : [ "N. Ratliff", "D. Silver", "J.A. Bagnell" ],
      "venue" : "Autonomous Robots,",
      "citeRegEx" : "Ratliff et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Ratliff et al\\.",
      "year" : 2009
    }, {
      "title" : "On the convergence of leveraging",
      "author" : [ "Rätsch", "Gunnar", "Mika", "Sebastian", "Warmuth", "Manfred K" ],
      "venue" : null,
      "citeRegEx" : "Rätsch et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Rätsch et al\\.",
      "year" : 2002
    }, {
      "title" : "The boosting approach to machine learning: An overview",
      "author" : [ "R.E. Schapire" ],
      "venue" : "In MSRI Workshop on Nonlinear Estimation and Classification,",
      "citeRegEx" : "Schapire,? \\Q2002\\E",
      "shortCiteRegEx" : "Schapire",
      "year" : 2002
    }, {
      "title" : "A simpler unified analysis of budget perceptrons",
      "author" : [ "I. Sutskever" ],
      "venue" : "In Proceedings of the 26th Annual International Conference on Machine Learning,",
      "citeRegEx" : "Sutskever,? \\Q2009\\E",
      "shortCiteRegEx" : "Sutskever",
      "year" : 2009
    }, {
      "title" : "A general boosting method and its application to learning ranking functions for web search",
      "author" : [ "Z. Zheng", "H. Zha", "T. Zhang", "O. Chapelle", "K. Chen", "G. Sun" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Zheng et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Zheng et al\\.",
      "year" : 2007
    }, {
      "title" : "Online convex programming and generalized infinitesimal gradient ascent",
      "author" : [ "M. Zinkevich" ],
      "venue" : "In Proceedings of the 20th International Conference on Machine Learning,",
      "citeRegEx" : "Zinkevich,? \\Q2003\\E",
      "shortCiteRegEx" : "Zinkevich",
      "year" : 2003
    } ],
    "referenceMentions" : [ {
      "referenceID" : 8,
      "context" : "Following previous work (Mason et al., 1999; Friedman, 2000) on general boosting frameworks, we analyze gradient-based descent algorithms for boosting with respect to any convex objective and introduce a new measure of weak learner performance into this setting which generalizes existing work.",
      "startOffset" : 24,
      "endOffset" : 60
    }, {
      "referenceID" : 6,
      "context" : "Following previous work (Mason et al., 1999; Friedman, 2000) on general boosting frameworks, we analyze gradient-based descent algorithms for boosting with respect to any convex objective and introduce a new measure of weak learner performance into this setting which generalizes existing work.",
      "startOffset" : 24,
      "endOffset" : 60
    }, {
      "referenceID" : 13,
      "context" : "Introduction Boosting (Schapire, 2002) is a versatile meta-algorithm for combining together multiple simple hypotheses, or weak learners, to form a single complex hypothesis with superior performance.",
      "startOffset" : 22,
      "endOffset" : 38
    }, {
      "referenceID" : 5,
      "context" : "Looking to extend upon the success of AdaBoost, related algorithms have been developed for other domains, such as RankBoost (Freund et al., 2003) and mutliclass extensions to AdaBoost (Mukherjee & Schapire, 2010).",
      "startOffset" : 124,
      "endOffset" : 145
    }, {
      "referenceID" : 8,
      "context" : "Other previous work on providing general algorithms for boosting has shown that an intuitive link between algorithms like AdaBoost and gradient descent exists (Mason et al., 1999; Friedman, 2000), and that many existing boosting algorithms can be reformulated to fit within this gradient boosting framework.",
      "startOffset" : 159,
      "endOffset" : 195
    }, {
      "referenceID" : 6,
      "context" : "Other previous work on providing general algorithms for boosting has shown that an intuitive link between algorithms like AdaBoost and gradient descent exists (Mason et al., 1999; Friedman, 2000), and that many existing boosting algorithms can be reformulated to fit within this gradient boosting framework.",
      "startOffset" : 159,
      "endOffset" : 195
    }, {
      "referenceID" : 5,
      "context" : "Looking to extend upon the success of AdaBoost, related algorithms have been developed for other domains, such as RankBoost (Freund et al., 2003) and mutliclass extensions to AdaBoost (Mukherjee & Schapire, 2010). Each of these algorithms provides both strong theoretical and experimental results for their specific domain, including corresponding weak to strong learning guarantees, but extending boosting to these and other new settings is non-trivial. Recent attempts have been successful at generalizing the boosting approach to certain broader classes of problems, but their focus is also relatively restricted. Mukherjee and Schapire (2010) present a general theory of boosting for multiclass classification problems, but their analysis is restricted to the multiclass setting.",
      "startOffset" : 125,
      "endOffset" : 647
    }, {
      "referenceID" : 5,
      "context" : "Looking to extend upon the success of AdaBoost, related algorithms have been developed for other domains, such as RankBoost (Freund et al., 2003) and mutliclass extensions to AdaBoost (Mukherjee & Schapire, 2010). Each of these algorithms provides both strong theoretical and experimental results for their specific domain, including corresponding weak to strong learning guarantees, but extending boosting to these and other new settings is non-trivial. Recent attempts have been successful at generalizing the boosting approach to certain broader classes of problems, but their focus is also relatively restricted. Mukherjee and Schapire (2010) present a general theory of boosting for multiclass classification problems, but their analysis is restricted to the multiclass setting. Zheng et al. (2007) give a boosting method which utilizes the second-order Taylor approximation of the objective to optimize smooth, convex losses.",
      "startOffset" : 125,
      "endOffset" : 804
    }, {
      "referenceID" : 12,
      "context" : "Additionally, convergence rates of these algorithms have been analyzed for the case of smooth convex functionals (Rätsch et al., 2002) and for specific potential functions used in classification (Duffy & Helmbold, 2000) under the traditional PAC weak learning setting.",
      "startOffset" : 113,
      "endOffset" : 134
    }, {
      "referenceID" : 8,
      "context" : "Using this foundation, we will present weak to strong learning results for the existing gradient boosting algorithm (Mason et al., 1999; Friedman, 2000) for the special case of smooth convex objectives under our more general setting.",
      "startOffset" : 116,
      "endOffset" : 152
    }, {
      "referenceID" : 6,
      "context" : "Using this foundation, we will present weak to strong learning results for the existing gradient boosting algorithm (Mason et al., 1999; Friedman, 2000) for the special case of smooth convex objectives under our more general setting.",
      "startOffset" : 116,
      "endOffset" : 152
    }, {
      "referenceID" : 16,
      "context" : "For convex problems standard gradient descent algorithms are known to provide good convergence results (Zinkevich, 2003; Boyd & Vandenberghe, 2004; Hazan et al., 2006) and are widely applicable.",
      "startOffset" : 103,
      "endOffset" : 167
    }, {
      "referenceID" : 7,
      "context" : "For convex problems standard gradient descent algorithms are known to provide good convergence results (Zinkevich, 2003; Boyd & Vandenberghe, 2004; Hazan et al., 2006) and are widely applicable.",
      "startOffset" : 103,
      "endOffset" : 167
    }, {
      "referenceID" : 14,
      "context" : "A related form of gradient descent with gradient errors has previously been studied in the analysis of budgeted learning (Sutskever, 2009), and general results related to gradient projection errors are given in the literature.",
      "startOffset" : 121,
      "endOffset" : 138
    }, {
      "referenceID" : 6,
      "context" : "In the case of smooth convex functionals, Mason et al. (1999) give a proof of eventual convergence for this previous work, but no rates of convergence are given.",
      "startOffset" : 42,
      "endOffset" : 62
    }, {
      "referenceID" : 8,
      "context" : "L Function Space Previous work (Mason et al., 1999; Friedman, 2000) has presented the theory underlying function space gradient descent in a variety of ways, but never in a form which is convenient for convergence analysis.",
      "startOffset" : 31,
      "endOffset" : 67
    }, {
      "referenceID" : 6,
      "context" : "L Function Space Previous work (Mason et al., 1999; Friedman, 2000) has presented the theory underlying function space gradient descent in a variety of ways, but never in a form which is convenient for convergence analysis.",
      "startOffset" : 31,
      "endOffset" : 67
    }, {
      "referenceID" : 6,
      "context" : ", 1999; Friedman, 2000) has presented the theory underlying function space gradient descent in a variety of ways, but never in a form which is convenient for convergence analysis. Recently, Ratliff (2009) proposed the L function space as a natural match for this setting.",
      "startOffset" : 8,
      "endOffset" : 205
    }, {
      "referenceID" : 8,
      "context" : "Restricted Gradient Descent We now outline the gradient-based view of boosting (Mason et al., 1999; Friedman, 2000) and how it relates to gradient descent.",
      "startOffset" : 79,
      "endOffset" : 115
    }, {
      "referenceID" : 6,
      "context" : "Restricted Gradient Descent We now outline the gradient-based view of boosting (Mason et al., 1999; Friedman, 2000) and how it relates to gradient descent.",
      "startOffset" : 79,
      "endOffset" : 115
    }, {
      "referenceID" : 8,
      "context" : "This is a generalization of the projection operation in Mason et al. (1999) to functions other than classifiers.",
      "startOffset" : 56,
      "endOffset" : 76
    }, {
      "referenceID" : 8,
      "context" : "The straightforward algorithm (Mason et al., 1999; Friedman, 2000) for peforming restricted gradient descent which uses these projection operations is given in Algorithm 1.",
      "startOffset" : 30,
      "endOffset" : 66
    }, {
      "referenceID" : 6,
      "context" : "The straightforward algorithm (Mason et al., 1999; Friedman, 2000) for peforming restricted gradient descent which uses these projection operations is given in Algorithm 1.",
      "startOffset" : 30,
      "endOffset" : 66
    }, {
      "referenceID" : 6,
      "context" : "This projection operation is equivalent to the one given by Friedman (2000). These two projection methods provide relatively simple ways to search over any restriction set for the ‘best’ descent direction.",
      "startOffset" : 60,
      "endOffset" : 76
    }, {
      "referenceID" : 12,
      "context" : "(Rätsch et al., 2002) using results from the optimization literature on coordinate descent.",
      "startOffset" : 0,
      "endOffset" : 21
    }, {
      "referenceID" : 12,
      "context" : "We will now give a generalization of the result in (Rätsch et al., 2002) which uses our more general definition of weak learner edge.",
      "startOffset" : 51,
      "endOffset" : 72
    }, {
      "referenceID" : 12,
      "context" : "Theorem 3 (Generalization of Theorem 4 in (Rätsch et al., 2002)).",
      "startOffset" : 42,
      "endOffset" : 63
    }, {
      "referenceID" : 16,
      "context" : "scent setting (Zinkevich, 2003; Hazan et al., 2006), but with a number of additional error terms due to the gradient projection step.",
      "startOffset" : 14,
      "endOffset" : 51
    }, {
      "referenceID" : 7,
      "context" : "scent setting (Zinkevich, 2003; Hazan et al., 2006), but with a number of additional error terms due to the gradient projection step.",
      "startOffset" : 14,
      "endOffset" : 51
    }, {
      "referenceID" : 7,
      "context" : "scent setting (Zinkevich, 2003; Hazan et al., 2006), but with a number of additional error terms due to the gradient projection step. Sutskever (2009) has previously studied the convergence of gradient descent with gradient projection errors using an algorithm similar to Algorithm 1, but the analysis does not focus on the weak to strong learning guarantee we seek.",
      "startOffset" : 32,
      "endOffset" : 151
    }, {
      "referenceID" : 11,
      "context" : "The first experimental setup is an optimization problem which results from the Maximum Margin Planning (Ratliff et al., 2009) approach to imitation learning.",
      "startOffset" : 103,
      "endOffset" : 125
    }, {
      "referenceID" : 11,
      "context" : "Previous attempts in the literature have been made to adapt boosting to this setting (Ratliff et al., 2009; Bradley, 2009), similar to the naive algorithm presented here, but no convergence results for this settings are known.",
      "startOffset" : 85,
      "endOffset" : 122
    }, {
      "referenceID" : 1,
      "context" : "Previous attempts in the literature have been made to adapt boosting to this setting (Ratliff et al., 2009; Bradley, 2009), similar to the naive algorithm presented here, but no convergence results for this settings are known.",
      "startOffset" : 85,
      "endOffset" : 122
    }, {
      "referenceID" : 16,
      "context" : "First, we start by bounding the potential ‖ft − f∗‖, similar to the potential function arguments in (Zinkevich, 2003; Hazan et al., 2006), but with a different descent step: ‖ft+1 − f∗‖ ≤ ‖ft − ηt(ht)− f∗‖ = ‖ft − f∗‖ + η t ‖ht‖ 2 − 2ηt〈ft − f∗, ht −∇t〉 − 2ηt〈ft − f,∇t〉 〈f∗ − ft,∇t〉 ≤ 1 2ηt ‖ft+1 − f∗‖ − 1 2ηt ‖ft − f∗‖ − ηt 2 ‖ht‖ − 〈f∗ − ft, ht −∇t〉",
      "startOffset" : 100,
      "endOffset" : 137
    }, {
      "referenceID" : 7,
      "context" : "First, we start by bounding the potential ‖ft − f∗‖, similar to the potential function arguments in (Zinkevich, 2003; Hazan et al., 2006), but with a different descent step: ‖ft+1 − f∗‖ ≤ ‖ft − ηt(ht)− f∗‖ = ‖ft − f∗‖ + η t ‖ht‖ 2 − 2ηt〈ft − f∗, ht −∇t〉 − 2ηt〈ft − f,∇t〉 〈f∗ − ft,∇t〉 ≤ 1 2ηt ‖ft+1 − f∗‖ − 1 2ηt ‖ft − f∗‖ − ηt 2 ‖ht‖ − 〈f∗ − ft, ht −∇t〉",
      "startOffset" : 100,
      "endOffset" : 137
    } ],
    "year" : 2012,
    "abstractText" : "Boosting is a popular way to derive powerful learners from simpler hypothesis classes. Following previous work (Mason et al., 1999; Friedman, 2000) on general boosting frameworks, we analyze gradient-based descent algorithms for boosting with respect to any convex objective and introduce a new measure of weak learner performance into this setting which generalizes existing work. We present the weak to strong learning guarantees for the existing gradient boosting work for strongly-smooth, strongly-convex objectives under this new measure of performance, and also demonstrate that this work fails for non-smooth objectives. To address this issue, we present new algorithms which extend this boosting approach to arbitrary convex loss functions and give corresponding weak to strong convergence results. In addition, we demonstrate experimental results that support our analysis and demonstrate the need for the new algorithms we present.",
    "creator" : "LaTeX with hyperref package"
  }
}