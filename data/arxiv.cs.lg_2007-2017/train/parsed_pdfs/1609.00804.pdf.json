{
  "name" : "1609.00804.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Randomized Prediction Games for Adversarial Machine Learning",
    "authors" : [ "Samuel Rota Bulò", "Ignazio Pillai", "Fabio Roli" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "Index Terms—Pattern classification, adversarial learning, game theory, randomization, computer security, evasion attacks.\nI. INTRODUCTION Machine-learning algorithms have been increasingly adopted in adversarial settings like spam, malware and intrusion detection. However, such algorithms are not designed to operate against intelligent and adaptive attackers, thus making them inherently vulnerable to carefully-crafted attacks. Evaluating security of machine learning against such attacks and devising suitable countermeasures, are two among the main open issues under investigation in the field of adversarial machine learning [1]–[11]. In this work we focus on the issue of designing secure classification algorithms against evasion attacks, i.e., attacks in which malicious samples are manipulated at test time to evade detection. This is a typical setting, e.g., in spam filtering, where spammers manipulate the content of spam emails to get them past the anti-spam filters [1], [2], [12]–[14], or in malware detection, where hackers obfuscate malicious software (malware, for short) to evade detection of either known or zero-day exploits [8], [9], [15], [16]. Although out of the scope of this work, it is worth mentioning here another pertinent attack scenario, referred to as classifier poisoning. Under this setting, the\nS. Rota Bulò is with ICT-Tev, Fondazione Bruno Kessler, Trento, Italy M. Pelillo is with DAIS, Università Ca’ Foscari, Venezia, Italy B. Biggio, I. Pillai, F. Roli are with DIEE, University of Cagliari, Italy\nattacker can manipulate the training data to mislead classifier learning and cause a denial of service; e.g., by increasing the number of misclassified samples [6], [7], [17]–[20].\nTo date, several authors have addressed the problem of designing secure learning algorithms to mitigate the impact of evasion attacks [1], [6], [10], [11], [21]–[27] (see Sect. VII for further details). The underlying rationale of such approaches is to learn a classification function that accounts for potential malicious data manipulations at test time. To this end, the interactions between the classifier and the attacker are modeled as a game in which the attacker manipulates data to evade detection, while the classification function is modified to classify them correctly. This essentially amounts to incorporating knowledge of the attack strategy into the learning algorithm. However, both the classification function and the simulated data manipulations have been modeled in a deterministic manner, without accounting for any form of randomization.\nRandomization is often used by attackers to increase their chances of evading detection, e.g., malware code is typically obfuscated using random strings or byte sequences to hide known exploits, and spam often contains bogus text randomly taken from English dictionaries to reduce the “spamminess” of the overall message. Surprisingly, randomization has also been proposed to improve classifier security against evasion attacks [3], [6], [28]. In particular, it has been shown that randomizing the learning algorithm may effectively hide information about the classification function to the attacker, requiring her to select a less effective attack (manipulation) strategy. In practice, the fact that the adversary may not know the classification function exactly (i.e., in a deterministic sense) decreases her (expected) payoff on each attack sample. This means that, to achieve the same expected evasion rate attained in the deterministic case, the attacker has to increase the number of modifications made to the attack samples [28].\nMotivated by the aforementioned facts, in this work we generalize static prediction games, i.e., the game-theoretical formulation proposed by Brückner et al. [10], [11], to account for randomized classifiers and data manipulation strategies. For this reason, we refer to our game as a randomized prediction game. A randomize prediction game is a non-cooperative game between a randomized learner and a randomized attacker (also called data generator), where the player’s strategies are replaced with probability distributions defined over the respective strategy sets. Our goal is twofold. We do not only aim to assess whether randomization helps achieving a better trade-off in terms of false alarms and attack detection (with respect to state-of-the-art secure classifiers), but also whether\nar X\niv :1\n60 9.\n00 80\n4v 1\n[ cs\n.L G\n] 3\nS ep\n2 01\n6\n2 our approach remains more secure against attacks that are different from those hypothesized during design. In fact, given that our game considers randomized players, it is reasonable to expect that it may be more robust to potential deviations from its original hypotheses about the players’ strategies.\nThe paper is structured as follows. Randomized prediction games are presented in Sect. II, where sufficient conditions for the existence and uniqueness of a Nash equilibrium in these games are also given. In Sect. III we focus on a specific game instance involving a linear Support Vector Machine (SVM) learner, for which we provide an effective method to find an equilibrium by overcoming some computational problems. We discuss how to enable the use of nonlinear (kernelized) SVMs in Sect. IV. In Sect. V we report a simple example to intuitively explain how the proposed methods enforce security in adversarial settings. Related work is discussed in Sect. VII. In Sect. VI we empirically validate the soundness of the proposed approach on an handwritten digit recognition task, and on realistic adversarial application examples involving spam filtering and malware detection in PDF files. Notably, to evaluate robustness of our approach and state-of-the-art secure classification algorithms, we also consider attacks that deviate from the models hypothesized during classifier design. Finally, in Sect. VIII, we summarize our contributions and sketch potential directions for future work."
    }, {
      "heading" : "II. RANDOMIZED PREDICTION GAMES",
      "text" : "Consider an adversarial learning setting involving two actors: a data generator and a learner.1 The data generator produces at training time a set D̂ = {x̂i, yi}ni=1 ⊆ X ×Y of n training samples, sampled from an unknown probability distribution. Sets X and Y denote respectively the input and output spaces of the learning task. At test time, the data generator modifies the samples in D̂ to form a new dataset D ⊆ X ×Y , reflecting a test distribution, which differs in general from the training distribution and it is not available at training time. We assume binary learners, i.e. Y = {−1,+1}, and we assume also that the data transformation process leaves the labels of the samples in D̂ unchanged, i.e., D = {(xi, yi)}ni=1. Hence, a perturbed dataset will simply be represented in terms of a tuple X = (x1, . . . ,xn) ∈ X n, each element being the perturbation of the original input sample x̂i, while we implicitly assume the label to remain yi. The role of the learner is to classify samples x ∈ X according to the prediction function h(x) = sign[f(x;w)], which is expressed in terms of a linear generalized decision function f(x;w) = w>φ(x), where w ∈ Rm, x ∈ X and φ : X → Rm is a feature map.\nStatic prediction games have been introduced in [11] by modeling the learner and the data generator as players of a non-cooperative game that we identify as l-player and dplayer, respectively. The strategies of l-player correspond to the parametrizationsw of the prediction function f . The strategies of the data generator, instead, are assumed to live directly in the feature space, by regarding Ẋ = (ẋ>1 , . . . , ẋ > n ) > ∈ Rmn as a data generator strategy, where ẋi = φ(xi). By doing so, the decision function f becomes linear in either players’\n1We adopt here the same terminology used in [11].\nstrategies. Each player is characterized also by a cost function that depends on the strategies played by either players. The cost function of d-player and l-player are denoted by cd and cl, respectively, and are given by\ncl(w, Ẋ) = ρlΩl(w) +\nn∑\ni=1\n`l(w >ẋi, yi) , (1)\ncd(w, Ẋ) = ρdΩd(Ẋ) + n∑\ni=1\n`d(w >ẋi, yi) , (2)\nwhere w ∈ Rm is the strategy of l-player, Ẋ ∈ Rmn is the strategy of the d-player, and yi denotes the label of ẋi as per D̂. Moreover, ρd/l > 0 is a trade-off parameter, `d/l(w>ẋi, y) measures the loss incurred by the l/d-player when the decision function yields w>ẋi for the ith training sample while the true label is y, and Ωd/l can be regarded as a penalization for playing a specific strategy. For the d-player, this term quantifies the cost of perturbing D̂ in feature space.\nThe goal of our work is to introduce a randomization component in the model of [11], particularly to what concerns the players’ behavior. To this end, we take one abstraction step with respect to the aforementioned prediction game, where we let the learner and the data generator sample their playing strategy in the prediction game from a parametrized distribution, under the assumption that they are expected costminimizing (a.k.a. expected utility-maximizing). By doing so, we introduce a new non-cooperative game that we call randomized prediction game between the l-player and the d-player with strategies being mapped to the possible parametrizations of the players’ respective distributions, and cost functions being expected costs under the same distributions."
    }, {
      "heading" : "A. Definition of randomized prediction game",
      "text" : "Consider a prediction game as described before. We inject randomness in the game by assigning each player a parametrized probability distribution, i.e., pl(w;θl) for the learner and pd(Ẋ;θd) for the data generator, that governs the players’ strategy selection. Players are allowed to select the parametrization θl and θd for the respective distributions. For any choice of θl, the l-player plays a strategy w sampled from pl(·;θl). Similarly, for any choice of θd, the d-player plays a strategy Ẋ sampled from pd(·;θd). If the players adhere to the new rules, we obtain a randomized prediction game.\nA randomized prediction game is a non-cooperative game between a learner (l-player) and data generator (d-player) that has the following components:\n1) an underlying prediction game with cost functions cl/d(w, Ẋ) as defined in (1) and (2), 2) two parametrized probability distributions pl/d(·;θl/d) with parameters in Θl/d, 3) Θl/d are non-empty, compact and convex subsets of a finite-dimensional metric space Rsl/d .\nThe sets of parameters Θl/d are the pure strategy sets (a.k.a. action spaces) for the l-player and d-player, respectively. The costs functions of the two players, which quantify the cost that each player incurs when a strategy profile (θl,θd) ∈ Θl ×Θd is played, coincide with the expected costs, denoted\n3 by cl/d(θl,θd), that the two players have in the underlying prediction game if strategies are sampled from pl(·;θl) and pd(·;θd), according to the expected cost-minimizing hypothesis:\ncl(θl,θd) = Ew∼pl(·;θl) Ẋ∼pd(·;θd) [cl(w, Ẋ)] , (3) cd(θl,θd) = Ew∼pl(·;θl) Ẋ∼pd(·;θd) [cd(w, Ẋ)] , (4)\nwhere E[·] denotes the expectation operator. We assume cl/d to be well-defined functions, i.e. the expectations to be finite for any (θl,θd) ∈ Θl ×Θd. To avoid confusion between cl/d and cl/d, in the remainder of this paper we will refer them respectively as cost functions, and expected cost functions.\nBy adhering to a non-cooperative setting, the two players involved in the prediction game are not allowed to communicate and they play their strategies simultaneously. Each player has complete information of the game setting by knowing the expected cost function and strategy set of either players. Under rationality assumption, each player’s interest is to achieve the greatest personal advantage, i.e., to incur the lowest possible cost. Accordingly, the players are prone to play a Nash equilibrium, which in the context of our randomized prediction game is a strategy profile (θ?l ,θ ? d) ∈ Θl × Θd such that no player is interested in changing his/her own playing strategy. In formal terms, this yields:\nθ?l ∈ arg min θl∈Θl cl(θl,θ ? d) , θ ? d ∈ arg min θd∈Θd cd(θ ? l ,θd) . (5)"
    }, {
      "heading" : "B. Existence of a Nash equilibrium",
      "text" : "The existence of a Nash equilibrium of a randomized prediction game is not granted in general. A sufficient condition for the existence of a Nash equilibrium is thus given below.\nTheorem 1 (Existence). A randomized prediction game admits at least one Nash equilibrium if\n(i) cl/d are continuous in Θl ×Θd, (ii) cl(·,θd) is quasi-convex in Θl for any θd ∈ Θd,\n(iii) cd(θl, ·) is quasi-convex in Θd for any θl ∈ Θl.\nProof: The result follows directly from the DebreuGlicksberg-Fan Theorem [29]."
    }, {
      "heading" : "C. Uniqueness of a Nash equilibrium",
      "text" : "In addition to the existence of a Nash equilibrium, it is of interest to investigate if the equilibrium is unique. However, determining tight conditions that guarantee the uniqueness of the Nash equilibrium for any randomized prediction game is challenging; in particular, due to the additional dependence on a probability distribution for the learner and the data generator.\nWe will make use of a classical result due to Rosen [30] to formulate sufficient conditions for the uniqueness of the Nash equilibrium of randomized prediction games in terms of the so-called pseudo-gradient of the game, defined as\ngr = [ rl∇θl c̄l rd∇θd c̄d ] , (6)\nwith any fixed vector r = [rl, rd]> ≥ 0. Specifically, a randomized prediction game admits a unique Nash equilibrium if the following assumption is verified\nAssumption 1. (i) cl/d are twice differentiable in Θl ×Θd,\n(ii) cl(·,θd) is convex in Θl for any θd ∈ Θd, (iii) cd(θl, ·) is convex in Θd for any θl ∈ Θl,\nand gr is strictly monotone for some fixed r > 0, i.e.,\n[gr(θl,θd)− gr(θ′l,θ′d)]> [ θl − θ′l θd − θ′d ] > 0 ,\nfor any distinct strategy profiles (θl,θd), (θ′l,θ ′ d) ∈ Θl×Θd.2\nIn his paper, Rosen provides also a useful sufficient condition that guarantees a strictly monotone pseudo-gradient. This requires the Jacobian of the pseudo-gradient, a.k.a. pseudoJacobian, given by\nJr = [ rl∇2θlθlcl rl∇2θlθdcl rd∇2θdθlcd rd∇2θdθdcd ] , (7)\nto be positive definite.\nTheorem 2. A randomized prediction game admits a unique Nash equilibrium if Assumption 1 holds, and the pseudoJacobian Jr(θl,θd) is positive definite for all (θl,θd) ∈ Θl ×Θd and some fixed r > 0.\nProof: Under Assumption 1, the positive definiteness of Jr for all strategy profiles and some fixed vector r > 0 implies the strict monotonicity of gr, which in turn implies the uniqueness of the Nash equilibrium [30, Thm. 6].\nIn the rest of the section, we provide sufficient conditions that ensure the positive definiteness of the pseudo-Jacobian and thus the uniqueness of the Nash equilibrium via Thm. 2. To this end we decompose c̄l/d(θl,θd) as follows\nc̄l(θl,θd) = ρlΩl(θl) + L̄l(θl,θd) , c̄d(θl,θd) = ρdΩd(θd) + L̄d(θl,θd) , (8)\nwhere Ωl/d and L̄l/d are the expected regularization and loss terms given by\nΩl(θl) = Ew∼pl(·,θl)[Ωl(w)] , Ωd(θd) = EẊ∼pd(·,θd)[Ωd(Ẋ)] ,\nL̄l(θl,θd) = Ew∼pl(·;θl) Ẋ∼pd(·;θd)\n[ n∑\ni=1\n`l(w >xi, yi) ] ,\nL̄d(θl,θd) = Ew∼pl(·;θl) Ẋ∼pd(·;θd)\n[ n∑\ni=1\n`d(w >xi, yi) ] .\nMoreover, we require the following convexity and differentiability conditions on Ωl/s and L̄l/d:\nAssumption 2. (i) Ωl/d is strongly convex and twice continuously differen-\ntiable in Θl/d, (ii) L̄l(·,θd) is convex and twice continuously differentiable\nin Θl for all θd ∈ Θd, and (iii) L̄d(θl, ·) is convex and twice continuously differentiable\nin Θd for all θl ∈ Θl. 2Assumption 1.(i) could be relaxed to continuously differentiable.\n4 Finally, we introduce some quantities that are used in the subsequent lemma, which gives sufficient conditions for the positive-definiteness of the pseudo-Jacobian:\nλΩl = inf θl∈Θl\nλmin [ ∇2θlθlΩl(θl) ] ,\nλΩd = inf θd∈Θd\nλmin [ ∇2θdθdΩd(θd) ] ,\nλLl = inf (θl,θd)∈Θl×Θd\nλmin [ ∇2θlθlLl(θl,θd) ] ,\nλLd = inf (θl,θd)∈Θl×Θd\nλmin [ ∇2θdθdLd(θl,θd) ] ,\nτ = sup (θl,θd)∈Θl×Θd\nλmax [ R(θl,θd)R(θl,θd) >] ,\nwhere R(θl,θd) = 12 [ ∇2θlθdL̄l(θl,θd)> +∇2θdθlL̄d(θl,θd) ] and λmax/min give the maximum/minimum eigenvalue of the matrix in input. Note that the quantities listed above are finite and positive if Assumption 2 holds, given the compactness of Θl/d.\nLemma 1. If Assumption 2 holds and\n(ρlλ Ω l + λ L l )(ρdλ Ω d + λ L d ) > τ\nthen the pseudo-Jacobian Jr(θl,θd) is positive definite for all (θl,θd) ∈ Θl ×Θd by taking r = (1, 1)>.\nProof: The pseudo-Jacobian in (7) can be written as follows given the decomposition of c̄l/d in (8):\nJr =\n[ ρl∇2θlθlΩl +∇2θlθlL̄l ∇2θlθdL̄l\n∇2θdθlL̄d ρd∇2θdθdΩd +∇2θdθdL̄d\n] ,\nwhere we omitted the arguments of Ωl/d and L̄l/d for notational convenience. Let us denote by Jllr , J ld r , J dl r , and J dd r the four matrices composing Jr (in top-down, left-right order). Consider the following matrix:\nH =\n[ Hll Hld\nHdl Hdd\n] = [ ρlλ Ω l + λ L l R(θl,θd) >\nR(θl,θd) ρdλ Ω d + λ L d\n] .\nThen we have for all t = (t>l , t > d ) 6= 0\nt>Jrt = t Jr + J\n> r\n2 t>\n= tlJ ll r tl︸ ︷︷ ︸\n≥tlHlltl\n+ tdJ dd r td︸ ︷︷ ︸\n≥t>d Hddtd\n+t>l Jldr + J dl> r\n2︸ ︷︷ ︸ Hld+Hdl>\ntd ≥ t>Ht ,\nwhere the under-braced relations follow from the definitions of λΩl/d, λ L l/d and R. Accordingly, the positive-definiteness of Jr can be derived from the positive-definiteness of matrix H. To prove the latter, we will show that all roots of the characteristic polynomial det(H−λI) of H are positive. By properties of the determinant3 we have\ndet(H− λI) = det((ρlλΩl + λLl − λ)I)\n· det (\n(ρdλ Ω d + λ L d − λ)I−\nS\nρlλΩl + λ L l − λ\n) ,\n3det\n[ aI B>\nB dI\n] = det(aI) det(dI − 1\na BB>) and if USU> is the eigen-\ndecomposition of BB> then the latter determinant becomes det(U(dI − 1 a S)U>) = det(dI− 1 a S)\nAlgorithm 1 Extragradient descent (adapted from [11]) Input: Cost functions c̄l/d; parameter spaces Θl,Θd; a small\npositive constant . Output: The optimal parameters θl,θd.\n1: Randomly select θ(0) = (θ(0)l ,θ (0) d ) ∈ Θl ×Θd. 2: Set iteration count k = 0, and select σ, β ∈ (0, 1). 3: Set r = (rl, rd) > = (1, ρl/ρd) > . 4: repeat 5: Set d(k) = ΠΘl×Θd ( θ(k) − gr ( θ (k) l ,θ (k) d )) − θ(k). 6: Find maximum step size t(k) ∈ {βp|p ∈ N} s.t.\n−gr ( θ̄ (k) l , θ̄ (k) d )> d(k) ≥ σ (∥∥∥d(k) ∥∥∥ 2\n2\n) ,\nwhere θ̄(k) = θ(k) + t(k)d(k).\n7: Set η(k) = − t(k)∥∥∥gr ( θ̄ (k) l ,θ̄ (k) d )∥∥∥ 2\n2\ngr\n( θ̄\n(k) l , θ̄ (k) d\n)> d(k).\n8: Set θ(k+1) = ΠΘl×Θd ( θ(k) − η(k)gr ( θ̄ (k) l , θ̄ (k) d )) .\n9: Set k = k + 1. 10: until ∥∥∥θ(k) − θ(k−1) ∥∥∥ 2\n2 ≤\n11: return θl = θ (k) l , θd = θ (k) d\nwhere S is a diagonal matrix with the eigenvalues of R(θl,θd)R(θl,θd)\n>. The roots of the first determinant term are all equal to ρlλΩl + λ L l , which is positive because ρl > 0 by construction and λΩl > 0 follows from the strong-convexity of Ωl in Assumption 2-i. As for the second determinant term, take the ith diagonal element Sii of S. Then two roots are the solution of the following quadratic equation\nλ2 − λ(a+ b) + ab− Sii = 0 ,\nwhich are given by\nλ (i) 1,2 = a+ b± √ (a− b)2 + 4Sii .\nwhere a = ρlλΩl + λ L l and b = ρdλ Ω d + λ L d . Among the two, λ (i) 2 (the one with the minus) is the smallest one, which is strictly positive if\nab = (ρlλ Ω l + λ L l )(ρdλ Ω d + λ L d ) > Sii .\nSince the condition has to hold for any choice of the eigenvalue Sii in the right-hand-side of the inequality, we take the maximum one maxi Sii, which coincides with λmax(R(θl,θd)R(θl,θd)\n>). We further maximize the latter quantity with respect to (θl,θd) ∈ Θl × Θd, because we want the result to hold for any parametrization. Therefrom we recover the variable τ and the condition (ρlλΩl +λ L l )(ρdλ Ω d + λLd ) > τ , which guarantees that all roots of the characteristic polynomial of H are strictly positive for any choice of (θl,θd) ∈ Θl × Θd and, hence, Jr is positive definite over Θl ×Θd.\nIn addition to Lem. 1, we provide in the supplementary material alternative (stronger) sufficient conditions, which generalize the ones given in [11].\n5"
    }, {
      "heading" : "D. Finding a Nash equilibrium",
      "text" : "From the computational perspective, we can find a Nash equilibrium in our game by exploiting algorithms similar to the ones adopted for static prediction games [11]. In particular, we consider a modified extragradient descent algorithm [11], [31], [32] that finds a solution to the following variational inequality problem, provided that gr is continuous and monotone:\ngr(θ ? l ,θ ? d) > (θ − θ?) ≥ 0 ,∀(θl,θd) ∈ Θl ×Θd , (9)\nwhere θ = [θ>l ,θ > d ] > and similarly for θ?. Any solution θ? to this problem can be shown to correspond bijectively to a Nash equilibrium of a game having gr as pseudo-gradient [11], [32].\nIf Theorem 1 holds, the pseudo-Jacobian J̄r can be shown to be positive semidefinite, and gr is thus continuous and monotone. Hence, the variational inequality can be solved by the modified extragradient descent algorithm given as Algorithm 1, which is guaranteed to converge to a Nash equilibrium point [31], [33]. The algorithm generates a sequence of feasible points whose distance from the equilibrium solution is monotonically decreased. It exploits a projection operator ΠΘl×Θd(θ) to map the input vector θ onto the closest admissible point in Θl×Θd, and a simple line-search algorithm to find the maximum step t on the descent direction d.4\nIn the next section, we apply our randomized prediction game to the case of linear SVM learners, and compute the corresponding pseudo-gradient, as required by Algorithm 1."
    }, {
      "heading" : "III. RANDOMIZED PREDICTION GAMES FOR SUPPORT VECTOR MACHINES",
      "text" : "In this section, we consider a randomized prediction game involving a linear SVM learner [34], and Gaussian distributions as the underlying probabilities pl/d.\nThe learner. The decision function of the learner is of the type f(x;w) = w>φ(x) where the feature map is given by φ(x) = [ x> 1 ]> . For convenience, we consider a decomposition of w into [ w̃> b ]> , where w̃ ∈ Rm−1 and b ∈ R. Hence, the decision function can also be written as f(x;w) = w̃>x + b. Accordingly, the input space X is a (m− 1)-dimensional vector space, i.e. X ⊆ Rm−1. The distribution pl for the learner is assumed to be Gaussian. In order to guarantee the theoretical existence of the Nash equilibrium through Thm. 1, we assume the parameters of the Gaussian distribution to be bounded. For the sake of clarity, we use in this section axis-aligned Gaussians (i.e. with diagonal covariance matrices) for our analysis, even though general covariances could be adopted as well. Under these assumptions, we define the strategy set for the learner as Θl = { (µw,σw) ∈ Rm × Rm+ } ∩ Bl, where Bl ⊂ Rm × Rm+ is an application-dependent non-empty, convex, bounded set, restricting the set of feasible parameters. The parameter vectors µw and σw encode the mean and standard deviation of the axis-aligned Gaussian distributions. The loss function `l of the learner corresponds to the hinge loss of the SVM, i.e., `l(z, y) = [1−zy]+ with [z]+ = max(0, z), while the strategy\n4We refer the reader to [11], [31], [32] (and references therein) for detailed proofs that derive conditions for which d is effectively a descent direction.\npenalization term Ωl(w) is the squared Euclidean norm of w̃. As a result, the cost function cl corresponds to the C-SVM objective function, and it is convex in w:\ncl(w, X) = ρl 2 ‖w̃‖2 +\nn∑\ni=1\n[1− yi(w̃>xi + b)]+ . (10)\nThe data generator. For convenience, we consider X rather than Ẋ as the quantity undergoing the randomization. This comes without loss of generality, because there is a oneto-one correspondence between ẋi and xi if we consider the linear feature map ẋi = φ(xi) = [x>i , 1]\n>. Moreover, we assume that samples xi can be perturbed independently. Accordingly, the distribution pd for the data generator factorizes as pd(X;θd) = ∏n i=1 pd ( xi;θ (i) d ) , where θd = (θ (1) d , . . . ,θ (n) d ). We consider pd ( xi;θ (i) d ) to be a k-variate axis-aligned Gaussian distribution with bounded mean and standard deviation given by θ(i)d = (µxi ,σxi). In summary, the strategy set adopted for the data generator is given by Θd =∏n i=1 Θ (i) d , where Θ (i) d = { (µxi ,σxi) ∈ Rk × Rk+ } ∩ Bd. Here, Bd ⊂ Rk × Rk+ is a non-empty, convex, bounded set. The loss function `d of the data generator is the hinge loss under wrong labelling, i.e., `d(z, y) = [1 + zy]+. In this way the data generator is penalized if the learner correctly classifies a sample point. Finally, the strategy penalization function Ωd is the squared Euclidean distance of the perturbed samples in X from the ones in the original training set D̂, i.e. Ωd(X) = ∑n i=1 ‖xi − x̂i‖2. The resulting cost function cd is convex in X:\ncd(w, X) = ρd 2\nn∑\ni=1\n‖xi − x̂i‖2\n+ n∑\ni=1\n[1 + yi(w̃ >xi + b)]+ . (11)\nExistence of a Nash equilibrium. The proposed randomized prediction game for the SVM learner admits at least one Nash equilibrium. This can be proven by means of Thm. 1. Indeed, the required continuity of c̄l/d hold and, as for the quasi-convexity conditions, we can rewrite (3) as follows by exploiting the fact that pl is a Gaussian distribution with mean µw and standard deviation σw:\ncl(θl,θd) = Ez∼N (0,I) X∼pd(·;θd) [cl(µw +D(σw)z, X)] , (12) where N (0, I) is a m-dimensional standard normal distribution and D(σw) is a diagonal matrix having σw on the diagonal. Since cl is convex in its first argument and convexity is preserved under addition of convex functions, positive rescaling, and composition with linear functions, we have that cl is convex (and thus quasi-convex) in θl = (µw,σw). As for the quasi-convexity condition of the data generator’s cost, we can exploit the separability of cd to rewrite (4) as follows:\ncd(θl,θd) = n∑\ni=1\nEw∼pl(·;θl) z∼N (0,I) [c (i) d (w,µxi +D(σxi)z)] ,\nwhere\nc (i) d (w,x) = ρd 2 ‖x− x̂i‖2 + [1 + yi(w>x̃i + b)]+ .\n6 Since c(i)d is convex in its second argument, by following the same reasoning used to show the quasi-convexity of the learner’s expected cost, we have that each expectation in cd is convex in θ(i)d = (µxi ,σxi), 1 ≤ i ≤ n. As a consequence, cd is convex and, hence, quasi-convex in θd, being the sum of convex functions.\nUniqueness of a Nash equilibrium. In the previous section we have shown that c̄l(·,θd) and c̄d(θl, ·) are convex as required by Assumption 1-(ii-iii). In particular we have that the single expected regularization terms Ωl/d(·) and loss terms L̄l(·,θd), L̄d(θl, ·) are convex as well. Moreover, they are twice-continuously differentiable by having Gaussian distributions for pl/d. It is then sufficient to have Ωl/d are strongly convex to prove the uniqueness of the Nash equilibrium via Lem. 1. While it is easy to see that Ωd is strongly convex, we have that Ωl is not strongly convex with respect to b due to the presence of an unregularized bias term b in the learner. The problem derives from the fact that the SVM itself may not have a unique solution when the bias term is present and non-regularized (see [35], [36] for a characterization of the degenerate cases). As a result, the proposed game is not guaranteed to have a unique Nash equilibrium in its actual form. On the other hand, a unique Nash equilibrium may be obtained by either considering an unbiased SVM, i.e., by setting b = 0 as in [11], or a regularized bias term, e.g., by adding ε2b\n2 to the learner’s objective function with ε > 0. In both cases, all conditions that ensure the uniqueness of the Nash equilibrium via Thm. 2 and Lem. 1 would be satisfied, under proper choices of ρl/d.\nIt is worth noting however that the necessary and sufficient conditions under which a biased (non-regularized) SVM has no unique solution are quite restricted [35], [36]. For this reason, we believe that uniqueness of the Nash equilibrium could be proven also for the biased SVM under mild assumptions. However, this requires considerable effort in trying to relax the sufficiency conditions of Rosen [30], which is beyond the scope of our work. We thus leave this challenge to future investigations. Moreover, we believe that enforcing a unique Nash equilibrium in our game by making the original SVM formulation strictly convex may lead to worse results, similarly to exploiting convex approximations to solve originally nonconvex problems in machine learning [37], [38]. For the above reasons, in this paper, we choose to retain the original SVM formulation for the learner, by sacrificing the uniqueness of the Nash Equilibrium. We nevertheless provide in Sect. V a discussion of why having a unique Nash Equilibrium is not so important in practice for our game, and we empirically show in Sect. VI that our approach can anyway achieve competitive performances with respect to other state-of-the-art approaches.\nThe rest of this section is devoted to showing how to compute the pseudo-gradient (6) by providing explicit formulae for ∇θl c̄l and ∇θd c̄d."
    }, {
      "heading" : "A. Gradient of the learner’s cost",
      "text" : "In this section, we focus on computing the gradient ∇θl c̄l(θl,θd), where c̄l is defined as in (10). By properties of expectation and since w follows an axis-aligned Gaussian distribution with mean µw and standard deviation σw, we can\nreduce the cost of the learner to:\nc̄l(θl,θd) = ρl 2\n( ‖µw̃‖2 + ‖σw̃‖2 )\n+ n∑\ni=1 E w∼pl(·;θl) xi∼pd(·;θ(i)d )\n[ [1− yi(w̃>xi + b)]+ ] , (13)\nwhere we are assuming the following decompositions for the mean µw = [ µ>w̃ µb ]> and standard deviation σw =[\nσ>w̃ σb ]>\n. The hard part for the minimization is the term in the expectation, which can not be expressed to our knowledge in a closed-form function of the Gaussian’s parameters. We thus resort to a Central-Limit-Theorem-like approximation, by regarding si = 1 − yi(w̃>xi + b) as a Gaussiandistributed variable with mean µsi and standard deviation σsi , i.e. si ∼ N (µsi , σsi). In general, si does not follow a Gaussian distribution, since the product of two normal deviates is not normally distributed. However, if the number of features k is large, the approximation becomes reasonable. Under this assumption, we can rewrite the expectation as follows:\nE w∼pl(·;θl) xi∼pd(·;θ(i)d )\n[ [1− yi(w̃>xi + b)]+ ]\n= Esi∼N (µsi ,σsi )[[si]+] . (14)\nThe mean and variance of the Gaussian distribution in the right-hand-side of Eq. (14) are respectively given by\nµsi = E w∼pl(·;θl) xi∼pd(·;θ(i)d )\n[ 1− yi(w̃>xi + b) ]\n= 1− yi(µ>w̃µxi + µb) , (15) σ2si = V w∼pl(·;θl)\nxi∼pd(·;θ(i)d )\n[ 1− yi(w̃>xi + b) ]\n= σ2w̃ > (σ2xi + µ 2 xi) + µ 2 w̃ > σ2xi + σ 2 b , (16)\nwhere V is the variance operator, and we assume that squaring a vector corresponds to squaring each single component.\nThe expectation in Eq. (14) can be transformed after simple manipulations into the following function involving the Gauss error function (integral function of the standard normal distribution) denoted as erf():\nh(µsi , σsi) = σsi√ 2π exp\n( − µ 2 si\n2σ2si\n)\n+ µsi 2\n[ 1− erf ( − µsi√\n2σsi\n)] . (17)\nThe learner’s cost in Eq. (13) can thus be approximated as:\nc̄l(θl,θd) ≈ Ll(µw,σw) = ρl 2\n( ‖µw̃‖2 + ‖σw̃‖2 )\n+\nn∑\ni=1\nh(µsi(θl), σsi(θl)) . (18)\nWe can now approximate the gradient ∇θlcl in terms of ∇θlLl. In the following, we denote the Hadamard (a.k.a. entry-wise) product between any two vectors a and b as a◦b,\n7 and we assume any scalar-by-vector derivative to be a column vector. The gradients of interest are given as:\n∂Ll ∂µw = ρl [ µw̃ 0 ] + n∑\ni=1\n( ∂h\n∂µsi ∂µsi ∂µw + ∂h ∂σ2si ∂σ2si ∂µw\n) , (19)\n∂Ll ∂σw = ρl [ σw̃ 0 ] + n∑\ni=1\n( ∂h\n∂µsi ∂µsi ∂σw + ∂h ∂σ2si ∂σ2si ∂σw\n) , (20)\nwhere it is not difficult to show that ∂h\n∂µsi =\n1\n2\n[ 1− erf ( − 1√\n2 µsi σsi\n)] , (21)\n∂h\n∂σ2si =\n1\n2 1√ 2πσsi exp\n( −1\n2 µ2si σ2si\n) , (22)\nand that ∂µsi ∂µw = −yi [ µxi 1 ] ,\n∂µsi ∂σw = 0 , (23)\n∂σ2si ∂µw =\n[ 2σ2xi ◦ µw̃\n0\n] ,\n∂σ2si ∂σw\n= 2σw ◦ [ σ2xi + µ 2 xi\n1\n] .\n(24)"
    }, {
      "heading" : "B. Gradient of the data generator’s cost",
      "text" : "In this section we turn to the data generator and we focus on approximating ∇θdcd, where cd is defined as in Eq. (11). We can separate cd into the sum of n functions acting on each data sample independently, i.e. cd(θl,θd) = ∑n i=1 c (i) d (θl,θ (i) d ), where for each i ∈ {1, . . . , n}:\nc (i) d (θl,θ (i) d ) = E w∼pl(·;θl)\nxi∼pd(·;θ(i)d )\n[ρd 2 ‖xi − x̂i‖2\n+[1 + yi(w̃ >xi + b)]+ ] . (25)\nBy exploiting properties of the expectation and since pd(·;θ(i)d ) is an axis-aligned Gaussian distribution with mean µxi and standard deviation σxi , we can simplify Eq. (25) as:\nc (i) d (θl,θ (i) d ) = ρd 2 ( ‖µxi − x̂i‖2 + ‖σxi‖2 )\n+ E w∼pl(·;θl) xi∼pd(·;θ(i)d )\n[ [1 + yi(w̃ >xi + b)]+ ] . (26)\nAs in the case of the learner, the expectation is a troublesome term having the same form of (14), except for an inverted sign. We adopt the same approximation used in Sect. III-A to obtain a closed-form function. Accordingly, ti = 1 + yi(w̃>xi + b) is assumed to be normally distributed with mean µti and σti . Then the expectation in Eq. (26) can be approximated as h(µti , σti), where function h is defined as in Eq. (17). The variance σ2ti is equal to σ 2 si (Eq. 16), while µti is given by:\nµti = E w∼pl(·;θl) xi∼pd(·;θ(i)d )\n[ 1 + yi(w̃ >xi + b) ]\n= 1 + yi(µ > w̃µxi + µb) .\nThe sample-wise cost of the data generator (Eq. 26) can thus be approximated as\nc (i) d (θl,θ (i) d ) ≈ Ld(µxi ,σxi) = ρd 2 ( ‖µxi − x̂i‖2 + ‖σxi‖2 )\n+ h(µti(θ (i) d ), σti(θ (i) d )) . (27)\nThe corresponding gradient is given by\n∂Ld ∂µxi\n= (µxi − x̂i) + ρd ( ∂h\n∂µti ∂µti ∂µxi + ∂h ∂σ2ti ∂σ2ti ∂µxi\n) ,\n(28)\n∂Ld ∂σxi = σxi + ρd\n( ∂h\n∂µti ∂µti ∂σxi + ∂h ∂σ2ti ∂σ2ti ∂σxi\n) , (29)\nwhere ∂h∂µti and ∂h ∂σ2ti are given as in Eqs. (21)-(22), and\n∂µti ∂µxi = yiµw̃ , ∂µti ∂σxi = 0 , (30) ∂σ2ti ∂µxi = 2σ2w̃ ◦ µxi , ∂σ2ti ∂σxi = 2σxi ◦ ( σ2w̃ + µ 2 w̃ ) . (31)"
    }, {
      "heading" : "IV. KERNELIZATION",
      "text" : "Our game, as in Bruckner et al. [11], assumes explicit knowledge of the feature space φ, where the data generator is assumed to randomize the samples ẋ = φ(x). However, in many applications, the feature mapping is only implicitly given in terms of a positive semidefinite kernel function k : X ×X → R that measures the similarity between samples as a scalar product in the corresponding kernel Hilbert space, i.e., there exists φ : X → R such that k(x,x′) = φ(x)>φ(x′). Note that in this setting the input space X is not restricted to a vector space like in the previous section (e.g. it might contain graphs or other structured entities).\nFor the representer theorem to hold [39], we assume that the randomized weight vectors of the learner live in the same subspace of the reproducing kernel Hilbert space, i.e., w = ∑ j αjφ(x̂j), where α ∈ Rn. Analogously, we restrict the randomized samples obtained by the data generator to live in the span of the mapped training instances, i.e., ẋi =∑n j=1 ξijφ(x̂j), where ξi = (ξi1, . . . , ξin)\n> ∈ Rn. Now, instead of randomizing w and Ẋ, we let the data generator and the learner randomize Ξ = (ξ1, . . . , ξn) and α, respectively. Moreover, we assume that the expected costs c̄l/d can be rewritten in terms of α and Ξ in a way that involves only inner products of φ(x), to take advantage of the kernel trick. This is clearly possible for the termw>ẋi = α>Kξi in (1) and (2), where K is the kernel matrix. Hence, the applicability of the kernel trick only depends on the choice of the regularizers. It is easy to see that due to the linearity of the variable shift, existence and uniqueness of a Nash equilibrium in our kernelized game hold under the same conditions given for the linear case.5\nAlthough the data generator is virtually randomizing strategies in some subspace of the reproducing kernel Hilbert space, in reality manipulations should occur in the original input space. Hence, to construct the real attack samples {xi}ni=1 corresponding to the data generator’s strategy at the Nash equilibrium, one should solve the so-called pre-image problem, inverting the implicit feature mapping φ−1(Kξi) for each sample. This problem is in general neither convex, nor\n5Note that, on the contrary, manipulating samples directly in the input space would not even guarantee the existence of a Nash equilibrium, as the data generator’s expected cost becomes non-quasi-convex in x for many (nonlinear) kernels, invalidating Theorem 1.\nit admits a unique solution. However, reliable solutions can be easily found using well-principled approximations [11], [39]. It is finally worth remarking that solving the pre-image problem is not even required from the learner’s perspective, i.e., to train the corresponding, secure classification function."
    }, {
      "heading" : "V. DISCUSSION",
      "text" : "In this section, we report a simple case study on a twodimensional dataset to visually demonstrate the effect of randomized prediction games on SVM-based learners. From a pragmatic perspective, this example suggests also that uniqueness of the Nash Equilibrium should not be taken as a strict requirement in our game.\nAn instance of the proposed randomized prediction game for a linear SVM and for a non-linear SVM with the RBF kernel is reported in Fig. 1. As one may note from the plots, the main effect of simulating the presence of an attacker that manipulates malicious data to evade detection is to cause the linear decision boundary to gradually shift towards the legitimate class, and the nonlinear boundary to find a better enclosing of the legitimate samples. This should generally improve the learner’s robustness to any kind of evasion attempt, as it requires the attacker to mimic more carefully the feature values of legitimate samples – a task typically harder in several adversarial settings than just obfuscating the content of a malicious sample to make it sufficiently different from the known malicious ones [7], [9].\nBased on this observation, any attempt aiming to satisfy the sufficient conditions for uniqueness of the Nash Equilibrium will result in an increase of the regularization strength in either the learner’s or the attacker’s cost function. Indeed, to satisfy the condition in Lem. 1, one could sufficiently increase ρl, ρd, or both. This amounts to increasing the regularization strength of either players, which in turn reduces in some sense their power. Hence, it should be clear that enforcing satisfaction\nof the sufficient conditions that guarantee the uniqueness of the Nash equilibrium might be counterproductive, by inducing the learner to weakly enclose the legitimate class, either due to a too strong regularization of the learners’ parameters, or by limiting the ability of the attacker to manipulate the malicious samples, thus allowing the leaner to keep a loose boundary. This will in general compromise the quality of the adversarial learning procedure. This argument shares similarities with the idea of addressing non-convex machine learning problems directly, without resorting to convex approximations [37], [38].\nBesides improving classifier robustness, finding a better enclosure of the legitimate class may however cause a higher number of legitimate samples to be misclassified as malicious. There is indeed a trade-off between the desired level of robustness and the fraction of misclassified legitimate samples. The benefit of using randomization here is to make the attacker’s strategy less pessimistic than in the case of static prediction games [10], [11], which should allow us to eventually find a better trade-off between robustness and legitimate misclassifications. This aspect is investigated more systematically in the experiments reported in the next section."
    }, {
      "heading" : "VI. EXPERIMENTS",
      "text" : "In this section we present a set of experiments on handwritten digit recognition, spam filtering, and PDF malware detection. Despite handwritten digit recognition is not a proper adversarial learning task as spam and malware detection, we consider it in our experiments to provide a visual interpretation of how secure learning algorithms are capable of improving robustness to evasion attacks.\nWe consider only linear classifiers, as they are a typical choice in these settings, and especially in spam filtering [2], [7], [14]. This also allows us to carry out a fair comparison with state-of-the-art secure learning algorithms, as they yield linear classification functions. We compare our secure linear\nSVM learner (Sect. III) with the standard linear SVM implementation [40], and with the state-of-the-art robust classifiers InvarSVM [21], [22], and NashSVM [11] (see Sect. VII).\nThe goal of these experiments is to test whether these secure algorithms work well also under attack scenarios that differ from those hypothesized during design – a typical setting in security-related tasks; e.g., what happens if game-based classification techniques like that proposed in this paper and NashSVM are used against attackers that exploit a different attack strategy, i.e., attackers that may not act rationally according to the hypothesized objective function? What happens when the attacker does not play at the expected Nash equilibrium? These are rather important questions to address, as we do not have any guarantee that real-world attackers will play according to the hypothesized objective function.\nSecurity evaluation. To address the above issues, we consider the security evaluation procedure proposed in [7]. It evaluates the performance of the considered classifiers under attack scenarios of increasing strength. We consider the True Positive (TP) rate (i.e., the fraction of detected attacks) evaluated at 1% False Positive (FP) rate (i.e., the fraction of misclassified legitimate samples) as performance measure. We evaluate the performance of each classifier in the absence of attack, as in standard performance evaluation techniques, and then start manipulating the malicious test samples to simulate attacks of different strength. We assume a worst-case adversary, i.e., an adversary that has perfect knowledge of the attacked classifier, since we are interested in understanding the worst-case performance degradation. Note however that other choices are possible, depending on specific assumptions on the adversary’s knowledge and capability [7], [13], [14]. In this setting, we assume that the optimal (worst-case) sample manipulation x∗ operated by the attacker is obtained by solving the following optimization problem:\nx∗ ∈ arg min x yf(x;w),\ns.t. d(x, x̂i) ≤ dmax, (32)\nwhere y is the malicious class label, d(x,xi) measures the distance between the perturbed sample x and the ith malicious data sample x̂i (in this case, we use the `2 norm, as done by the considered classifiers). The maximum amount of modifications is bounded by dmax, which is a parameter representing the attack strength. It is obvious that the more modifications the adversary is allowed to make on the attack samples, the\nhigher the performance degradation incurred by the classifier is expected to be. Accordingly, the performance of more secure classifiers is expected to degrade more gracefully as the attack strength increases [7], [14].\nThe solution of the above problem is trivial when we consider linear classifiers, the Euclidean distance, and x is unconstrained: it amounts to setting x∗ = x̂i − ydmax w||w|| . If x lies within some constrained domain, e.g. [0, 1], then one may consider a simple gradient descent with box constraints on x (see, e.g., [9]). If x takes on binary values, e.g., {0, 1}, then the attack amounts to switching from 0 to 1 or vice-versa the value of a maximum of dmax features which have been assigned the highest absolute weight values by the classifier. In particular, if y wk > 0 (y wk < 0) and the k-th feature satisfies x̂ik = 1 (x̂ik = 0), then x∗k = 1 (x ∗ k = 0) [7], [14].\nParameter selection. The considered methods require setting different parameters. From the learners’ perspective, we have to tune the regularization parameter C for the standard linear SVM and InvarSVM, while we respectively have ρ−1 and ρl for NashSVM and for our method. In addition, the robust classifiers require setting the parameters of their attacker’s objective. For InvarSVM, we have to set K, i.e., the number of modifiable features, while for NashSVM and for our method, we have to set the value of the regularization parameter ρ+1 and ρd, respectively. Further, to guarantee existence of a Nash Equilibrium point, we have to enforce some box constraints on the distribution’s parameters. For the attacker, we restrict the mean of the attack points to lie in [0, 1] (as the considered datasets are normalized in that interval), and their variance in [10−3, 0.5]. For the learner, the variance of w is allowed to vary in [10−6, 10−3], while its mean takes values on [−W,W ], where W is optimized together with the other parameters. All the above mentioned parameters are set by performing a grid-search on the parameter space (C, ρ−1, ρd ∈ {0.01, 0.1, 1, 10, 100}; K ∈ {8, 13, 25, 30, 47, 52, 63}; ρ+1, ρd ∈ {0.01, 0.05, 0.1, 1, 10}; W ∈ {0.01, 0.05, 0.1, 1}), and retaining the parameter values that maximize the area under the security evaluation curve on a validation set. The reason is to find a parameter configuration for each method that attains the best average robustness over all attack intensities (values of dmax), i.e. the best average TP rate at FP=1%."
    }, {
      "heading" : "A. Handwritten Digit Recognition",
      "text" : "Similarly to [21], we focus on two two-class sub-problems of discriminating between two distinct digits from the MNIST\n10\ndataset [41], i.e. 1 vs 3, and 6 vs 7, where the second digit in each pair represents the attacking class (y = +1). The digits are originally represented as gray-scale images of 28 × 28 pixels. They are simply mapped to feature vectors by ordering the pixels in raster scan order. The overall number of features is thus d = 784. We normalize each feature (pixel value) in [0, 1], by dividing its value by 255. We build a training and a validation set of 1,000 samples each by randomly sampling the original training data available for MNIST. As for the test set, we use the default set provided with this data, which consists of approximately 1,000 samples for each digit (class). The results averaged on 5 repetitions are shown in the first and second plot of Fig. 2 respectively for 1 vs 3, and 6 vs 7. As one may notice, in the absence of attack (i.e. when dmax = 0), all classifiers achieved comparable performance (the TP rate is almost 100% for all of them), due to a conservative choice of the operating point (FP=1%), that should also guarantee a higher robustness against the attack. In the presence of attack, our approach (RNashSVM) exhibits comparable performance to NashSVM on the problem of discriminating 1 vs 3, and to InvarSVM on 6 vs 7. NashSVM outperforms the standard SVM implementation in both cases, but exhibits lower security (robustness) than InvarSVM on 6 vs 7, despite the attacker’s regularizer in InvarSVM is not even based on the `2 norm.\nFinally, in Fig. 3 we report two attack samples (a digit from class 1 and one from class 6) and show how they are obfuscated by the attack strategy of Eq. (32) to evade detection against each classifier. Notice how the original attacking samples (1 and 6) tend to resemble more the corresponding attacked classes (3 and 7) when natively robust classifiers are used. This visual example confirms the analysis of Sect. V, i.e., that higher robustness is achieved when the adversary is required to mimic the feature values of samples of the legitimate class, instead of slightly modifying the attack samples to differentiate them from the rest of the malicious data."
    }, {
      "heading" : "B. Spam Filtering",
      "text" : "In these experiments we use the benchmark, publicly available, TREC 2007 email corpus [42], which consists of 75,419 real emails (25,220 legitimate and 50,199 spam messages) collected between April and July 2007. We exploit the bagof-words feature model, in which each binary feature denotes the absence (0) or presence (1) of the corresponding word in a given email [7], [11], [13], [14]. Features (words) are extracted from training emails using the tokenization method\nof the widely-known anti-spam filter SpamAssassin,6 and then n = 1, 000 distinct features are selected using a supervised feature selection approach based on the information gain criterion [43]. We build a training and a validation set of 1,000 samples each by randomly sampling the first 5,000 emails in chronological order of the original dataset, while a test set of about 2,000 samples is randomly sampled from the subsequent set of 5,000 emails. The results averaged on 5 repetitions are shown in the third plot of Fig. 2. As in the previous case, in the absence of attack (dmax = 0) all the classifiers exhibit a very high (and similar) performance. However, as the attack intensity (dmax) increases, their performance degrades more or less gracefully, i.e. their robustness to the attack is different. Surprisingly, one may notice that only InvarSVM and RNashSVM exhibited an improved level of security. The reason is that these two classifiers are able to find a more uniform set of weights than SVM and NashSVM, and, in this case, this essentially requires the adversary to manipulate a higher number of features to significantly decrease the value of the classifier’s discriminant function. Note that a similar result has been heuristically found also in [13], [14]."
    }, {
      "heading" : "C. PDF Malware Detection",
      "text" : "We consider here another relevant application example in computer security, i.e., the detection of malware in PDF files. The main reason behind the diffusion of malware in PDF files is that they exhibit a very flexible structure that allows embedding several kinds of resources, including Flash, JavaScript and even executable code. Resources simply consists of keywords that denote their type, and of data streams that contain the actual object; e.g., an embedded resource in a PDF file may be encoded as follows:\n13 0 obj << /Kids [ 1 0 R 11 0 R ] /Type /Page ... >> end obj\nwhere keywords are highlighted in bold face. Recent work has exploited machine learning techniques to discriminate between malicious and legitimate PDF files, based on the analysis of their structure and, in particular, of the embedded keywords [44]–[48]. We exploit here a similar feature representation to that proposed in [45], where each feature denotes the presence of a given keyword in the PDF file. We collected 5993 recent malware samples from the Contagio dataset,7 and 5951 benign samples from the web. Following the procedure described in [45], we extracted 114 keywords from the first 1,000 samples (in chronological order) to build our feature set. Then, we build training, validation and test sets as in the spam filtering case, and average results over 5 repetitions. Attacks in this case are simulated by allowing the attacker only to increase the feature values of malicious samples, which corresponds to adding the constraint x ≥ x̂i (where the inequality holds for all features) to Problem 32. The reason is that removing objects (and keywords) from malicious PDFs may compromise the intrusive nature of the embedded exploitation code, whereas adding objects can be easily done through the PDF versioning mechanism [9], [46], [48].\n6http://spamassassin.apache.org 7http://contagiodump.blogspot.it\n11\nResults are shown in the 4th plot of Fig. 2. The considered methods mostly exhibit the same behavior shown in the spam filtering case, besides the fact that, here, there is a clearer trade-off between the performance in the absence of attack, and robustness under attack. In particular, InvarSVM and RNashSVM are significantly more robust under attack (i.e., when dmax > 0) than SVM and NashSVM, at the expense of a slightly worsened detection rate in the absence of attack (i.e., when dmax = 0).\nTo summarize, the reported experiments show that, even if the attacker does not play the expected attack strategy at the Nash equilibrium, most of the proposed state-of-the-art secure classifiers are still able to outperform classical techniques, and, in particular, that the proposed RNashSVM classifier may guarantee an even higher level of robustness. Understanding how this property relates to the use of probability distributions over the set of the classifier’s and of the attacker’s strategies remains an interesting future question."
    }, {
      "heading" : "VII. RELATED WORK",
      "text" : "The problem of devising secure classifiers against different kinds of manipulation of samples at test time has been widely investigated in previous work [1], [6], [10], [11], [21]–[27]. Inspired by the seminal work by Dalvi et al. [1], several authors have proposed a variety of modifications to existing learning algorithms to improve their security against different kinds of attack. Globerson et al. [21], [22] have formulated the so-called Invariant SVM (InvarSVM) in terms of a minimax approach (i.e., a zero-sum game) to deal with worst-case feature manipulations at test time, including feature addition, deletion, and rescaling. This work has been further extended in [23] to allow features to have different a-priori importance levels, instead of being manipulated equally likely. Notably, more recent research has also considered the development of secure learning algorithms based on zero-sum games for sensor networks, including distributed secure algorithms [27] and algorithms for detecting adversarially-corrupted sensors [26].\nThe rationale behind shifting from zero-sum to non-zerosum games for adversarial learning is that the classifier and the attacker may not necessarily aim at maximizing antagonistic objective functions. This in turn implies that modeling the problem as a zero-sum game may lead one to design overlypessimistic classifiers, as pointed out in [11]. Even considering a non-zero-sum Stackelberg game may be too pessimistic, since the attacker (follower) is supposed to move after the classifier (leader), while having full knowledge of the chosen classification function (which again is not realistic in practical settings) [11], [24]. For these reasons, Brückner et al. [10], [11] have formalized adversarial learning as a non-zero-sum game, referred to as static prediction game. Assuming that the players act simultaneously (conversely to Stackelberg games [24]), they devised conditions under which a unique Nash equilibrium for this game exists, and developed algorithms for learning the corresponding robust classifiers, including the socalled NashSVM. Our work essentially extends this approach by introducing randomization over the players’ strategies.\nFor completeness, we also mention here that in [25] Bayesian games for adversarial regression tasks have been\nrecently proposed. In such games, uncertainty on the objective function’s parameters of either player is modeled by considering a probability distribution over their possible values. To the best of our knowledge, this is the first attempt towards modeling the uncertainty of the attacker and the classifier on the opponent’s objective function."
    }, {
      "heading" : "VIII. CONCLUSIONS AND FUTURE WORK",
      "text" : "In this paper, we have extended the work in [11] by introducing randomized prediction games. To operate this shift, we have considered parametrized, bounded families of probability distributions defined over the set of pure strategies of either players. The underlying idea, borrowed from [3], [6], [28], consists of randomizing the classification function to make the attacker select a less effective attack strategy. Our experiments, conducted on an handwritten digit recognition task and on realistic application examples involving spam and malware detection, show that competitive, secure SVM classifiers can be learnt using our approach, even when the conditions behind uniqueness of the Nash equilibrium may not hold, i.e., when the attacker may not play according to the objective function hypothesized for her by the classifier. This mainly depends on the particular kind of decision function learnt by the learning algorithm under our game setting, which tends to find a better ‘enclosing’ of the legitimate class. This generally requires the attacker to make more modifications to the malicious samples to evade detection, regardless of the attack strategy chosen. We can thus argue that the proposed methods exhibit robustness properties particularly suited to adversarial learning tasks. Moreover, the fact that the proposed methods may perform well also when the Nash equilibrium is not guaranteed to be unique suggests us that the conditions behind its uniqueness may hold under less restrictive assumptions (e.g., when the SVM admits a unique solution [35], [36]). We thus leave a deeper investigation of this aspect to future work.\nAnother interesting extension of this work may be to apply randomized prediction games in the context of unsupervised learning, and, in particular, clustering algorithms. It has been recently shown that injecting a small percentage of wellcrafted poisoning attack samples into the input data may significantly subvert the clustering process, compromising the subsequent data analysis [49], [50]. In this respect, we believe that randomized prediction games may help devising secure countermeasures to mitigate the impact of such attacks; e.g., by explicitly modeling the presence of poisoning samples (generated according to a probability distribution chosen by the attacker) during the clustering process.\nIt is worth finally mentioning that our work is also slightly related to previous work on security games, in which the goal of the defender is to adopt randomized strategies to protect his or her assets from the attacker, by allocating a limited number of defensive resources; e.g., police officers for airport security, protection mechanisms for network security [51]–[53]. Although our game is not directly concerned to the protection of a given set of assets, we believe that investigating how to bridge the proposed approach within this well-grounded field of study may provide promising research\n12\ndirections for future work, e.g., in the context of network security [52], [53], or for suggesting better user attitudes towards security issues [54]. This may also suggest interesting theoretical advancements; e.g., to establish conditions for the equivalence of Nash and Stackelberg games [51], and to address issues related to the uncertainty on the players’ strategies, or on their (sometimes bounded) rationality, e.g., through the use of Bayesian games [25], security strategies and robust optimization [52], [53]. Another suggestion to overcome the aforementioned issues is to exploit higher-level models of the interactions between attackers and defenders in complex, real-world problems; e.g., through the use of replicator equations to model adversarial dynamics in securityrelated tasks [55]. Exploiting conformal prediction may be also an interesting research direction towards improving current adversarial learning systems [56]. To conclude, we believe these are all relevant research directions for future work."
    } ],
    "references" : [ {
      "title" : "Static prediction games for adversarial learning problems",
      "author" : [ "M. Brückner", "C. Kanzow", "T. Scheffer" ],
      "venue" : "J. Mach. Learn. Res., vol. 13, pp. 2617– 2654, September 2012. 1det [  aI B> B dI  ] = det(aI) det(dI − 1 a BB>) and if USU> is the eigendecomposition of BB> then the latter deterimnant becomes det(U(dI − 1 a  S)U>) = det(dI− 1 a  S)",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2012
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Evaluating security of machine learning against such attacks and devising suitable countermeasures, are two among the main open issues under investigation in the field of adversarial machine learning [1]–[11].",
      "startOffset" : 200,
      "endOffset" : 203
    }, {
      "referenceID" : 0,
      "context" : ", in spam filtering, where spammers manipulate the content of spam emails to get them past the anti-spam filters [1], [2], [12]–[14], or in malware detection, where hackers obfuscate malicious software (malware, for short) to evade detection of either known or zero-day exploits [8], [9], [15], [16].",
      "startOffset" : 113,
      "endOffset" : 116
    }, {
      "referenceID" : 0,
      "context" : "To date, several authors have addressed the problem of designing secure learning algorithms to mitigate the impact of evasion attacks [1], [6], [10], [11], [21]–[27] (see Sect.",
      "startOffset" : 134,
      "endOffset" : 137
    }, {
      "referenceID" : 0,
      "context" : "[0, 1], then one may consider a simple gradient descent with box constraints on x (see, e.",
      "startOffset" : 0,
      "endOffset" : 6
    }, {
      "referenceID" : 0,
      "context" : "For the attacker, we restrict the mean of the attack points to lie in [0, 1] (as the considered datasets are normalized in that interval), and their variance in [10−3, 0.",
      "startOffset" : 70,
      "endOffset" : 76
    }, {
      "referenceID" : 0,
      "context" : "We normalize each feature (pixel value) in [0, 1], by dividing its value by 255.",
      "startOffset" : 43,
      "endOffset" : 49
    }, {
      "referenceID" : 0,
      "context" : "The problem of devising secure classifiers against different kinds of manipulation of samples at test time has been widely investigated in previous work [1], [6], [10], [11], [21]–[27].",
      "startOffset" : 153,
      "endOffset" : 156
    }, {
      "referenceID" : 0,
      "context" : "[1], several authors have proposed a variety of modifications to existing learning algorithms to improve their security against different kinds of attack.",
      "startOffset" : 0,
      "endOffset" : 3
    } ],
    "year" : 2016,
    "abstractText" : "In spam and malware detection, attackers exploit randomization to obfuscate malicious data and increase their chances of evading detection at test time; e.g., malware code is typically obfuscated using random strings or byte sequences to hide known exploits. Interestingly, randomization has also been proposed to improve security of learning algorithms against evasion attacks, as it results in hiding information about the classifier to the attacker. Recent work has proposed game-theoretical formulations to learn secure classifiers, by simulating different evasion attacks and modifying the classification function accordingly. However, both the classification function and the simulated data manipulations have been modeled in a deterministic manner, without accounting for any form of randomization. In this work, we overcome this limitation by proposing a randomized prediction game, namely, a non-cooperative game-theoretic formulation in which the classifier and the attacker make randomized strategy selections according to some probability distribution defined over the respective strategy set. We show that our approach allows one to improve the trade-off between attack detection and false alarms with respect to state-of-the-art secure classifiers, even against attacks that are different from those hypothesized during design, on application examples including handwritten digit recognition, spam and malware detection.",
    "creator" : "LaTeX with hyperref package"
  }
}