{
  "name" : "1605.08722.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "An algorithm with nearly optimal pseudo-regret for both stochastic and adversarial bandits",
    "authors" : [ "Peter Auer", "Chao-Kai Chiang", "C.-K. Chiang", "AUER CHIANG" ],
    "emails" : [ "AUER@UNILEOBEN.AC.AT", "CHAOKAI@GMAIL.COM" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n60 5.\n08 72\n2v 1\n[ cs\n.L G\n(√ Kn logn )\nand against stochastic bandits the regret is O ( ∑\ni (logn)/∆i). We also show that no algorithm with O (logn)\npseudo-regret against stochastic bandits can achieve Õ ( √ n) expected regret against adaptive adversarial bandits. This complements previous results of Bubeck and Slivkins (2012) that show Õ ( √ n) expected adversarial regret with O ( (log n)2 ) stochastic pseudo-regret."
    }, {
      "heading" : "1. Introduction",
      "text" : "We consider the multi-armed bandit problem, which is the most basic example of a sequential decision problem with an exploration-exploitation trade-off. In each time step t = 1, 2, . . . , n, the player has to play an arm It ∈ {1, . . . ,K} from this fixed finite set and receives reward xIt(t) ∈ [0, 1] depending on its choice1. The player observes only the reward of the chosen arm, but not the rewards of the other arms xi(t), i 6= It. The player’s goal is to maximize its total reward ∑n\nt=1 xIt(t), and this total reward is compared to the best total reward of a single arm, ∑n\nt=1 xi(t). To identify the best arm the player needs to explore all arms by playing them, but it also needs to limit this exploration to often play the best arm. The optimal amount of exploration constitutes the exploration-exploitation trade-off.\nDifferent assumptions on how the rewards xi(t) are generated have led to different approaches and algorithms for the multi-armed bandit problem. In the original formulation (Robbins, 1952) it is assumed that the rewards are generated independently at random, governed by fixed but unknown probability distributions with means µi for each arm i = 1, . . . ,K. This type of bandit problem is called stochastic. The other type of bandit problem that we consider in this paper is called non-stochastic or adversarial (Auer et al., 2002b). Here the rewards may be selected arbitrarily by\n∗ Accepted for presentation at the Conference on Learning Theory (COLT) 2016. 1. We assume that the player knows the total number of time steps n.\nc© P. Auer & C.-K. Chiang.\nan adversary and the player should still perform well for any selection of rewards. An extensive overview of multi-armed bandit problems is given in (Bubeck and Cesa-Bianchi, 2012).\nA central notion for the analysis of stochastic and adversarial bandit problems is the regret R(n), the difference between the total reward of the best arm and the total reward of the player:\nR(n) = max 1≤i≤K\nn ∑\nt=1\nxi(t)− n ∑\nt=1\nxIt(t).\nSince the player does not know the best arm beforehand and needs to do exploration, we expect that the total reward of the player is less than the total reward of the best arm. Thus the regret is a measure for the cost of not knowing the best arm. In the analysis of bandit problems we are interested in high probability bounds on the regret or in bounds on the expected regret. Often it is more convenient, though, to analyze the pseudo-regret\nR(n) = max 1≤i≤K E\n[\nn ∑\nt=1\nxi(t)− n ∑\nt=1\nxIt(t)\n]\ninstead of the expected regret\nE [R(n)] = E\n[\nmax 1≤i≤K\nn ∑\nt=1\nxi(t)− n ∑\nt=1\nxIt(t)\n]\n.\nWhile the notion of pseudo-regret is weaker than the expected regret with R(n) ≤ E [R(n)], bounds on the pseudo-regret imply bounds on the expected regret for adversarial bandit problems with oblivious rewards xi(t) selected independently from the player’s choices. The pseudo-regret also allows for refined bounds in stochastic bandit problems."
    }, {
      "heading" : "1.1. Previous results",
      "text" : "For adversarial bandit problems, algorithms with high probability bounds on the regret are known (Bubeck and Cesa-Bianchi, 2012, Theorem 3.3): with probability 1− δ,\nRadv(n) = O ( √ n log(1/δ) ) .\nFor stochastic bandit problems, several algorithms achieve logarithmic bounds on the pseudo-regret, e.g. Auer et al. (2002a):\nRsto(n) = O (log n) .\nBoth of these bounds are known to be best possible. While the result for adversarial bandits is a worst-case — and thus possibly pessimistic — bound that holds for any sequence of rewards, the strong assumptions for stochastic bandits may sometimes be unjustified. Therefore an algorithm that can adapt to the actual difficulty of the problem is of great interest. The first such result was obtained by Bubeck and Slivkins (2012), who developed the SAO algorithm that with probability 1− δ achieves\nRadv(n) ≤ O ( (log n) √ n log(n/δ) )\nregret for adversarial bandits and\nRsto(n) = O ( (log n)2 )\npseudo-regret for stochastic bandits. It has remained as an open question if a stochastic pseudo-regret of order O ( (log n)2 )\nis necessary or if the optimal O (log n) pseudo-regret can be achieved while maintaining an adversarial regret of order √ n."
    }, {
      "heading" : "1.2. Summary of new results",
      "text" : "We give a twofold answer to this open question. We show that stochastic pseudo-regret of order O ( (log n)2 )\nis necessary for a player to achieve high probability adversarial regret of order √ n against an oblivious adversary, and to even achieve expected regret of order √ n against an adaptive adversary. But we also show that a player can achieve O (log n) stochastic pseudoregret and Õ ( √ n) adversarial pseudo-regret at the same time. This gives, together with the results of (Bubeck and Slivkins, 2012), a quite complete characterization of algorithms that perform well both for stochastic and adversarial bandit problems.\nMore precisely, for any player with stochastic pseudo-regret bound of order O ( (log n)β )\n, β < 2, and any ǫ > 0, α < 1, there is an adversarial bandit problem for which the player suffers Ω(nα) regret with probability Ω(n−ǫ). Furthermore, there is an adaptive adversary against which the player suffers Ω(nα) expected regret. Secondly, we construct an algorithm with\nRsto(n) = O (log n)\nand Radv(n) = O ( √ n log n ) .\nAt first glance these two results may appear contradictory for α− ǫ > 1/2, as the lower bound seems to suggest a pseudo-regret of Ω(nα−ǫ). This is not the case, though, since the regret may also be negative. Indeed, consider an adversarial multi-armed bandit that initially gives higher rewards for one arm, and from some time step on gives higher rewards for a second arm. A player that detects this change and initially plays the first arm and later the second arm, may outperform both arms and achieve negative regret. But if the player misses the change and keeps playing the first arm, it may suffer large regret against the second arm.\nIn our analysis we use both mechanisms. For the lower bound on the pseudo-regret we show that a player with little exploration (which is necessary for small stochastic pseudo-regret) will miss such a change with significant probability and then will suffer large regret. For the upper bound we explicitly compensate possible large regret that occurs with small probability by negative regret that occurs with sufficiently large probability. For the lower bound on the expected regret we construct an adaptive adversary that prevents such negative regret. Consequently, our results exhibit one of the rare cases where there is a significant gap between the achievable pseudo-regret and the achievable expected regret.\nThe explicit consideration of negative regret is one of the technical contributions of this work. Another, maybe even more significant contribution, is a weak testing scheme for non-stochastic arms. This weak testing scheme is necessary since O (log n) stochastic pseudo-regret allows only for very little exploration. Each individual weak test has a constant false positive rate (predicting\na non-stochastic arm although the arm is stochastic) and a constant false negative rate (missing a non-stochastic arm). To avoid classifying a stochastic arm as non-stochastic, an arm is classified as non-stochastic only after O (log n) positive tests. This reduces the false positive rate of a decision to acceptable O (1/n). Conversely, this delayed detection needs to be accounted for in the regret analysis when the arms are indeed non-stochastic."
    }, {
      "heading" : "2. Definitions and statement of results",
      "text" : "In a multi-armed bandit problem with arms i = 1, . . . ,K the interaction of a player with its environment is governed by the following protocol:\nFor time steps t = 1, . . . , n:\n1. The player chooses an arm It ∈ {1, . . . ,K}, possibly using randomization. 2. The player receives and observes the reward xIt(t).\nIt does not observe the reward from any other arm i 6= It.\nThe player’s choice It may depend only on information available at this time, namely I1, . . . , It−1 and xI1(1), . . . , xIt−1(t − 1). If the bandit problem is stochastic, then the rewards xi(t) are generated independently at random. If the bandit problem is adversarial, then the rewards are generated arbitrarily by an adversary. We assume that all rewards xi(t) ∈ [0, 1] and that the number of time steps n is known to the player."
    }, {
      "heading" : "2.1. Stochastic multi-armed bandit problems",
      "text" : "In a stochastic multi-armed bandit problem the rewards for each arm i are generated by a fixed but unknown probability distribution νi on [0, 1]. All rewards xi(t), 1 ≤ i ≤ K , 1 ≤ t ≤ n, are generated independently at random with xi(t) ∼ νi.\nImportant quantities are the average rewards of the arms, µi = E [xi(t)], the average reward of the best arm µ∗ = maxi µi, and the resulting gaps ∆i = µ∗ − µi.\nThe goal of the player is to achieve low pseudo-regret which for a stochastic bandit problem can be written as\nRsto(n) = max 1≤i≤K E\n[\nn ∑\nt=1\nxi(t)− n ∑\nt=1\nxIt(t)\n]\n=\nK ∑\ni=1\n∆iE [Ti(n)] ,\nwhere Ti(n) = #{1 ≤ t ≤ n : It = i} is the number of plays of arm i. It can be shown (Auer et al., 2002a) that — among others — upper confidence bound algorithms achieve\nE [Ti(n)] = O\n(\nlog n\n∆2i\n)\nfor any arm i with ∆i > 0 such that\nRsto(n) = O\n\n\n∑\ni:∆i>0\nlog n\n∆i\n\n .\nIt can be even shown that for arms i with ∆i > 0,\nTi(n) = O\n(\nlog(n/δ)\n∆2i\n)\nwith probability 1− δ when n is known to the player."
    }, {
      "heading" : "2.2. Adversarial multi-armed bandit problems",
      "text" : "In adversarial bandit problems the rewards are selected by an adversary. If this is done beforehand (before the player interacts with the environment), then the adversary is called oblivious as the selection of rewards is independent from the arms It chosen by the player. In this case any upper bound on the pseudo-regret that holds for any selection of rewards is also an upper bound on the expected regret.\nIf the selection of rewards xi(t), 1 ≤ i ≤ K , depends on which arms I1, . . . , It−1 the player has chosen in the past, then the adversary is called adaptive. In this case a bound on the pseudo-regret does not necessarily translate into a bound on the expected regret. Nevertheless, strong bounds on the regret against an adaptive adversary are known for the EXP3.P algorithm (Auer et al., 2002b):\nTheorem 1 (Bubeck and Cesa-Bianchi, 2012, Theorem 3.3) When EXP3.P is run with appropriate parameters depending on n, K , and δ, then with probability 1− δ its regret satisfies\nRada(n) = O ( √ nK log(K/δ) ) ."
    }, {
      "heading" : "2.3. Results",
      "text" : "First, we state our lower bounds for oblivious and adaptive adversaries.\nTheorem 2 Let α < 1, ǫ > 0, β < 2, and C > 0. Consider a player that achieves pseudo-regret\nRsto(n) ≤ C(log n)β\nfor any stochastic bandit problem with two arms and gap ∆ = 1/8. Then for large enough n there is an adversarial bandit problem with two arms and an oblivious adversary such that the player suffers regret\nRobl(n) ≥ nα/8− 4 √ n log n\nwith probability at least 1/(16nǫ) − 2/n2. Furthermore, there is an adversarial bandit problem with two arms and an adaptive adversary such that the player suffers expected regret\nE [Rada(n)] ≥ nα−ǫ\n128 − 3\n√\nn log n.\nIn Section 3 we present our SAPO algorithm (Stochastic and Adversarial Pseudo-Optimal) that achieves optimal pseudo-regret in stochastic bandit problems and nearly optimal pseudo-regret in adversarial bandit problems. Its performance is summarized in the following theorem.\nTheorem 3 For large enough n and any δ > 0, algorithm SAPO achieves the following bounds for suitable constants Csto, Cadv, and C1b:\n• For stochastic bandit problems with gaps ∆i such that C1b ∑ i:∆i>0 log(n/δ) ∆i ≤ √ nK log(n/δ),\nTi(n) ≤ Csto log(n/δ)\n∆2i\nwith probability 1− δ for any arm i with ∆i > 0, and thus\nRsto(n) ≤ Csto ∑\ni:∆i>0\nlog(n/δ)\n∆i + δn.\n• For adversarial bandit problems\nRada(n) ≤ CadvK √ n log(n/δ) + δn.\nRemark 4 Our bound for adversarial bandit problems shows a worse dependency on K than Theorem 1. This is an artifact of our current analysis and can be improved to a bound Rada(n) = O ( √ nK log(n/δ) ) ."
    }, {
      "heading" : "2.4. Comparison with related work",
      "text" : "Bubeck and Slivkins (2012) show for their SAO algorithm that with probability 1− δ, K ∑\ni=1\n∆iTi(n) ≤ O ( K logK(log n/δ)2\n∆\n)\nfor stochastic bandits where ∆ = mini:∆i>0 ∆i, and\nRada(n) ≤ O ( (logK)(log n) √ nK log n/δ )\nfor adaptive adversarial bandits. While our bounds in Theorem 3 are somewhat tighter, in particular showing the optimal dependency on the gaps ∆i for stochastic bandits, we have only a result on the pseudo-regret for adversarial bandits. We conjecture though, that our analysis can be used to construct an algorithm that with probability 1− δ achieves Ti(n) ≤ O ( (log n/δ)2/∆2i ) for stochastic bandits and Rada(n) ≤ O ( (logK)(log n) √ nK log n/δ ) for adaptive adversarial bandits.\nOur SAPO algorithm follows the general strategy of the SAO algorithm by essentially employing an algorithm for stochastic bandit problems that is equipped with additional tests to detect nonstochastic arms. A different approach is taken in (Seldin and Slivkins, 2014): here the starting point is an algorithm for adversarial bandit problems that is modified by adding an additional exploration parameter to achieve also low pseudo-regret in stochastic bandit problems. While this approach has not yet allowed for the tight O (log n) regret bound in stochastic bandit problems (they achieve a O ( log3 n )\nbound), the approach is quite flexible and more generally applicable than the SAO and SAPO algorithms."
    }, {
      "heading" : "2.5. Proof sketch of the lower bound (Theorem 2)",
      "text" : "We present here the main idea of the proof. The proof itself is given in Appendix B. We consider a stochastic bandit problem with constant reward x1(t) = 1/2 for arm 1 and Bernoulli rewards with µ2 = 1/2 −∆ for arm 2, ∆ = 1/8. We divide the time steps into phases of increasing length Lj = 3jnα, j = 0, . . . , J with J = Ω(log n). Since the pseudo-regret of the player is O ( (log n)β )\n, there is a phase j∗ where the expected number of plays of arm 2 in this phase is O ( (log n)β−1 )\n. We construct an oblivious adversarial bandit by modifying the Bernoulli distribution of arm 2 in phase j∗ and beyond by setting µ2 = 1/2 +∆. By this modification arm 2 gives larger total reward than arm 1.\nBecause of the limited number of plays in phase j∗, a standard argument shows that the player will not detect this modification during phase j∗ with probability exp{−O(logβ−1 n)} = Ω(n−ǫ). When the modification is not detected during phase j∗, then in this phase the player suffers roughly regret ∆Lj∗ against arm 2. This is not compensated by negative regret against arm 2 in previous phases since ∆\n∑j∗−1 j=0 Lj ≤ ∆Lj∗/2. Thus in this case the overall regret of the player against arm 2\nis roughly ∆Lj∗/2 = Ω(nα). In a very similar way we can construct also an adaptive adversarial bandit: As for the oblivious bandit, we set µ2 = 1/2 + ∆ in phase j∗. If the player chooses arm 2 only C(log n)β−1 times in phase j∗, then we keep µ2 = 1/2 + ∆ also for the remaining phases. As for the oblivious bandit this happens with probability Ω(n−ǫ) and gives regret Ω(nα). To avoid negative regret, we switch back to µ2 = 1/2 − ∆, as soon as there more than C(log n)β−1 plays of arm 2 in phase j∗. In this case the reward of the algorithm is roughly n/2+C∆(log n)β−1 such that in this case R(n) ≥ −C∆(log n)β−1. Hence the expected regret is E [R(n)] ≥ Ω(nα−ǫ)− C∆(log n)β−1 = Ω(nα−ǫ)."
    }, {
      "heading" : "3. The SAPO algorithm",
      "text" : "In its core the algorithm is an elimination procedure for stochastic bandits that is augmented by tests safeguarding against non-stochastic arms. If there is sufficient evidence for non-stochastic arms, then the algorithm switches to the adversarial bandit algorithm EXP3.P, starting with the current time step.\nThe algorithm maintains a set of active arms A and a set of supposedly suboptimal “bad” arms B. For each arm i it maintains the sample mean µ̂i(s),\nµ̂i(s) = 1\nTi(s)\ns ∑\nt=1\nxi(t)I [It = i] ,\nTi(s) = s ∑\nt=1\nI [It = i] ,\nand also an unbiased estimate to deal with non-stochastic arms,\nµ̄i(s) = 1\ns\ns ∑\nt=1\nxi(t) I [It = i]\npi(t) ,\nAlgorithm 1 : SAPO Input: Number of arms K , number of rounds n ≥ K , and confidence parameter δ. Initialization: All arms are active, A(0) = {1, . . . ,K}, B(0) = ∅. For t = 1, . . . , n:\n1. (a) If there is an arm i ∈ A(t− 1) with µ̄i(t− 1) 6∈ [lcbi(t− 1),ucbi(t− 1)], then switch to EXP3.P.\n(b) If ∑t−1 s=1[lcb ∗(s)− xIs(s)] > C1b\n√\nKn log(n/δ), then switch to Exp3.P.\n2. Evict arms from A:\n(a) Let B(t) = {i ∈ A(t− 1) : Ti(t− 1) ≥ Cinit · log(n/δ) ∧ µ̂i(t− 1) +Cgap · widthi(t− 1) < lcb∗(t− 1)},\nA(t) = A(t− 1) \\B(t), B(t) = B(t− 1) ∪B(t). (b) For all i ∈ B(t) set µ̃i = µ̂i(t− 1), ∆̃i = Cgap · widthi(t− 1),\nni(t) = t, Li(t) = L0i := ⌈CpK/∆̃2i ⌉, and Ei(t) = 0.\n3. Choose It = i with probabilities\npi(t) =\n{\nL0i /(KLi(t)) for i ∈ B(t) (\n1−∑j∈B(t) pj(t) )/ |A(t)| for i ∈ A(t)\n4. Test and update all arms i ∈ B(t):\n(a) If ∃s : ni(t) ≤ s ≤ t : D̂i(s, t) ≥ C4a∆̃iLi(t)pi(t), (b) then ni(t+ 1) = t+ 1, Li(t+ 1) = max{Li(t)/2, L0i },\nand Ei(t+ 1) = Ei(t) + 1,\n(c) if Ei(t+ 1) = E0 := ⌈CE · log(n/δ)⌉, then switch to EXP3.P; (d) else if t = ni(t) + Li(t)− 1 then ni(t+ 1) = t+ 1, Li(t+ 1) = 2Li(t),\nand Ei(t+ 1) = Ei(t);\n(e) else ni(t+ 1) = ni(t), Li(t+ 1) = Li(t), and Ei(t+ 1) = Ei(t).\nwhere pi(t) is the probability of choosing arm i at time t. Confidence bounds2 around the estimated means are used to evict arms from the active set A,\nlcbi(s) = max{lcbi(s− 1), µ̂i(s)− widthi(s)}, lcbi(s) = max{lcbi(s− 1), µ̄i(s)− width(s)}, ucbi(s) = min{ucbi(s − 1), µ̄i(s) + width(s)}, lcb∗(s) = max\n1≤i≤K max{lcbi(s), lcbi(s)},\nwidthi(s) = √ Cw log(n/δ)/Ti(s),\nwidth(s) = √\nCwK log(n/δ)/s.\n2. We start with lcbi(0) = lcbi(0) = 0 and ucbi(0) = 1.\nNote that lcbi(s), lcbi(s), and lcb ∗(s) are non-decreasing and ucbi(s) are non-increasing. This reflects the intuition that confidence intervals should be shrinking and is used to safeguard against non-stochastic arms.\nAn arm i is evicted from A in Step 2.a, if it has a sufficient number of plays (Cinit · log(n/δ)) for reasonably accurate estimates, and if its sample mean µ̂i(t− 1) is significantly smaller than the optimal lower confidence bound lcb∗(t − 1). The additional distance Cgap · widthi(t − 1) is used to estimate the gap ∆i. For evicted arms, in Step 2.b an estimate for the gap ∆̃i and the current estimated mean are frozen, µ̃i = µ̂i(t − 1). For stochastic bandits the accuracy of this estimate is proportional to the estimated gap ∆̃i. These quantities are used in the tests for detecting nonstochastic arms. Also the starting time ni(t) and the length Li(t) = L0i of the first testing phase (see below), as well as the number of detections Ei(t) = 0 are set.\nSince SAPO needs to perform well also against adversaries, all choices of arms are randomized. In Step 3 an active arm is chosen uniformly at random, or with some smaller probabilitya bad arm i is chosen where the probability depends on the length of its current testing phase Li(t). Choosing also bad arms is necessary to detect non-stochastic arms among the bad arms."
    }, {
      "heading" : "3.1. Tests for detecting non-stochastic arms",
      "text" : "The most important test is in Step 4.a for detecting that a bad arm receives larger rewards than it should if it were stochastic. Such an arm could be optimal if the bandit problem is adversarial. The best way to view this test is by dividing the time steps of an evicted arm i into testing phases\nτi,1, . . . , τi,2 − 1; τi,2, . . . , τi,3 − 1; τi,3, . . . , τi,4 − 1; . . .\nThe first phase starts when arm i is evicted from A. A phase k ends at time τi,k+1 − 1 if either the phase has exhausted its length (Step 4.d), or when the test in Step 4.a reports a detection.3 Thus the length parameter Li(t) is only the maximal length of a phase and the phase may end earlier. In the notation of the algorithm ni(t) denotes the start of the current phase. Within a phase the probability pi(t) for choosing arm i is constant since the length parameter Li(t) does not change (Step 4.e). For notational convenience we denote by pik the probability for choosing arm i in its k-th testing phase, and by Lik the corresponding length parameter,\npi(t) = pik for i ∈ B(t) and τi,k ≤ t < τi,k+1, Li(t) = Lik for i ∈ B(t) and τi,k ≤ t < τi,k+1, ni(t) = τi,k for i ∈ B(t) and τi,k ≤ t < τi,k+1.\nNow the test in Step 4.a checks if a bad arm i has received significantly more rewards in the current phase then expected, given the estimated mean µ̃i, the maximal phase length Li(t) and the probability for choosing arm i, pi(t), where\nD̂i(s1, s2) =\ns2 ∑\nt=s1\n[xi(t)− µ̃i]I [It = i] .\nIf arm i is stochastic, then E [ D̂i(s1, s2) ] = O ( Li(t)∆̃ipi(t) ) such that a positive test suggests that the arm is non-stochastic. Since the expected number of plays of arm i is L0i /K in each phase,\n3. The last phase ends when the total number of time steps n is exhausted or when the algorithm switches to EXP3.P.\nthe test is weak, though, with constant false positive and false negative rates. To avoid incorrectly classifying a stochastic arm as non-stochastic, the test is repeated several times. To make the tests independent, a new phase is started in Step 4.b after a detection is reported. To avoid that too much regret accumulates in the case of a non-stochastic arm, the phase length is halved. If there have been E0 independent detections, then in Step 4.c there is sufficient evidence for a non-stochastic arm and the algorithm switches to EXP3.P.\nIn Step 4.d the phase ends because it has exhausted its length. Since the test in Step 4.a has given no detection, arm i has performed as expected and the algorithm has accumulated negative regret against this bad arm. This negative regret allows to start the next phase with a doubled phase length, even if the arm were non-stochastic. Doubling the phase length is necessary to avoid too many phases for a stochastic arm. (Remember that the expected number of plays of a bad arm is L0i /K in each phase.)\nIn Step 4.e none of the above condition is satisfied and the phase continues. Additional simpler tests for non-stochastic arms are performed in Step 1. Step 1.a checks whether for all active arms the unbiased estimates of the means obey the corresponding confidence intervals. Finally, Step 1.b checks if the algorithm receives significantly less reward than expected from the best lower confidence bound. This may happen if a non-stochastic arm first appears close to optimal but then receives less rewards."
    }, {
      "heading" : "3.2. Choice of constants in the SAPO algorithm",
      "text" : "In the algorithm we keep the constant names because we find them easier to read than actual values. Proper values for the constants are as follows: Cw = 16, C1b = 522, Cinit = 100/9, Cgap = 60, Cp = 1300, C4a = 1/10, and CE = 15."
    }, {
      "heading" : "4. Preliminaries for the analysis of SAPO",
      "text" : "An important tool for our analysis are concentration inequalities, in particular Bernstein’s inequality for martingales and a variant of Hoeffding-Azuma’s inequality for the maximum of partial sums, max1≤s≤t≤n ∑t i=s Yi. These inequalities are given in Appendix A. We denote by Ht the past up to and including time t. The next lemma states some properties of algorithm SAPO. Let\nTi(s1, s2) = #{t : s1 ≤ t ≤ s2 : It = i}\ndenote the number of plays of arm i in time steps s1 to s2, let nB,i be the time when arm i is evicted from A, i ∈ A(nB,i − 1) and i ∈ B(nB,i), and let nS be the time step when SAPO switches to EXP3.P. If SAPO never switches to EXP3.P, then nS = n.\nLemma 5 (a) If i ∈ B(t) then µ̃i + ∆̃i < lcb∗(t). (b) For each arm the number of testing phases k, τi,k · · · τi,k+1 − 1 is\nat most M = ⌈log2 n⌉+ 2E0. (c) With probability 1−O (δ), the number of plays of any bad arm i is bounded as\nTi(nB,i, nS) ≤ 101100L0iM/K = O ( M/∆̃2i ) .\nProof (Sketch) Statement (a) follows immediately from Step 2 of the algorithm since µ̃i = µ̂i(nB,i−1), ∆̃i = Cgap ·widthi(nB,i−1), µ̂i(nB,i−1)+Cgap ·widthi(nB,i−1) < lcb∗(nB,i−1), and lcb∗(t) is non-decreasing.\nStatement (b) follows from the fact that Step 4.b (where the phase length is halved) is executed at most E0 times. In the other phases the phase length is doubled in Step 4.d. Since the phase length is at most n, the number of phases is at most log2 n+ 2E\n0. For statement (c) we observe that by the definition of pi(t) the expected number of plays in any testing phase of a bad arm i is L0i /K . Thus the expected number of plays in all phases is L 0 iM/K . Since the variance is bounded by the same quantity, an application of Bernstein’s inequality gives the result.\nDetailed proofs are given in Appendix C."
    }, {
      "heading" : "5. Analysis of SAPO for adversarial bandits",
      "text" : "In this section we prove pseudo-regret bounds for SAPO against adversarial and possibly adaptive bandits. Since we know from Theorem 1 that EXP3.P suffers small regret, we only need to bound the pseudo-regret of SAPO before it switches to EXP3.P. For the remaining section we fix some arm i. We have\nnS ∑\nt=1\nxi(t)− nS ∑\nt=1\nxIt(t) =\nnS ∑\nt=1\n[xi(t)− lcb∗(t)] + nS ∑\nt=1\n[lcb∗(t)− xIt(t)]\n=\nnB,i−1 ∑\nt=1\n[xi(t)− lcb∗(t)] + nS ∑\nt=nB,i\n[xi(t)− lcb∗(t)] + nS ∑\nt=1\n[lcb∗(t)− xIt(t)] (1)\nThe first sum in (1) bounds the regret for the time when i is an active arm. For stochastic arms, the best lower confidence bound lcb∗(t) would be not too far from the rewards of the arms that are still active. For non-stochastic arms, though, we need the tests in SAPO, in particular those in Step 1, to guarantee a similar behavior and achieve\nE\n\n\nnB,i−1 ∑\nt=1\n[xi(t)− lcb∗(t)]\n\n = O ( √ Kn log(n/δ) ) , (2)\nsee Appendix D.1. The crucial part of the analysis concerns the second sum in (1) which bounds the regret for the time when i is a bad arm. For its analysis we explicitly track negative regret to compensate for positive regret. In Section 5.1 below we sketch the main ideas for handling this sum (formal proofs are given in Appendix D.2), showing that\nE\n\n\nnS ∑\nt=nB,i\n[xi(t)− lcb∗(t)]\n\n = O\n(\nK log(n/δ)\n∆̃i\n)\n. (3)\nNote that 1/∆̃i = O (widthi(nB,i − 1)) = O (√ Ti(nB,i)/ log(n/δ) ) = O ( √ n/ log(n/δ) ) such that O (\nK log(n/δ)/∆̃i\n) = O ( K √ n log(n/δ) ) .\nFinally, the third sum can be observed by the algorithm and is taken care of by the test in Step 1.b, such that\nnS ∑\nt=1\n[lcb∗(t)− xIt(t)] = O ( √ Kn log(n/δ) ) . (4)\nTogether, inequalities (1)–(4) and the bound on EXP3.P in Theorem 1 give the bound on the pseudoregret in Theorem 3."
    }, {
      "heading" : "5.1. Bounding the regret for bad arms",
      "text" : "If a bad arm is non-stochastic, then it may first appear suboptimal but still be optimal after all. We need to show that the tests of our algorithm, in particular the test in Step 4.a, are sufficient to detect such a situation. Since the algorithm checks arms in B(t) only rarely, it will take some time for such detection. In our analysis we explicitly compensate the regret during this delayed detection by the negative regret accumulated while arm i was performing suboptimally.\nWe consider the testing phases k, τi,k . . . τi,k+1 − 1, of arm i, and recall that Lik is the length parameter for phase k and pik = L0i /(KLik) is the probability for choosing arm i in phase k. Furthermore, let Eik the value of Ei(t) in phase k. Note that these quantities may change only when a new phase begins. We denote by Pik {·} = P { ·|Hτi,k−1 } and Eik [·] = E [ ·|Hτi,k−1 ]\nthe probabilities and expectations conditioned on the past before phase k.\nFor any phase we have\nτi,k+1−1 ∑\nt=τi,k\n[xi(t)− lcb∗(t)] = τi,k+1−1 ∑\nt=τi,k\n[xi(t)− µ̃i + µ̃i − lcb∗(t)]\n<\nτi,k+1−1 ∑\nt=τi,k\n[xi(t)− µ̃i]− ∆̃i[τi,k+1 − τi,k] (5)\nby Lemma 5a. Thus we want to prevent that the rewards of arm i are significantly larger than the estimated mean µ̃i. In particular, the test in Step 4.a is supposed to detect events Di(s1, s2) > 2C4a∆̃iLik with\nDi(s1, s2) :=\ns2 ∑\nt=s1\n[xi(t)− µ̃i].\nSince on average arm i is chosen only L0i /K times per phase, there is a constant false negative rate qadv for missing such events. For appropriate Cp, though, the false negative rate qadv is sufficiently small, qadv ≤ 1/25: Since E [ D̂i(s1, s2) ]\n= pikDi(s1, s2) for τi,k ≤ s1 ≤ s2 < τi,k+1, and Step 4.a tests for D̂i(s1, s2) > C4a∆̃iLikpik, we can bound qadv by Bernstein’s inequality using that 1 ≤ ∆̃2iL0i /(KCp) and a bound on the variance,\nV\n[ D̂i(s1, s2) ] ≤ Likpik = L0i /K ≤ (∆̃iL0i /K)2/Cp = (∆̃iLikpik)2/Cp.\nThe formal proof is given in Lemma 13. We use the false negative rate qadv to bound Eik [Di(τi.k, τi.k+1 − 1)]. Each time an event Di(s, t) > 2C4a∆̃iLik is missed (we consider only non-overlapping such events), Di(τi.k, t)\nhas increased by at most 2C4a∆̃iLik + 1, and the probability for the m-th miss is at most qmadv. When such an event is detected, then the phase ends and Di(τi.k, t) again has increased by at most 2C4a∆̃iLik + 1. Thus (see Lemma 15 for the formal proof)\nEik [Di(τi.k, τi.k+1 − 1)] ≤ (2C4a∆̃iLik + 1) ∑\nm≥0\nqmadv = 2C4a∆̃iLik + 1\n1− qadv\nwhich by (5) gives\nEik\n\n\nτi,k+1−1 ∑\nt=τi,k\n[xi(t)− lcb∗(t)]\n\n < 2C4a∆̃iLik + 1\n1− qadv − ∆̃iEik [τi,k+1 − τi,k] . (6)\nSince the bound in (6) is large for large Lik, we show that such a large contribution to the regret can be compensated by negative regret in previous phases due to the term −∆̃i[τi,k+1 − τi,k]. We show by backward induction over the phases that the expected regret starting from phase k can be bounded,\nEik\n\n\nnS ∑\nt=τi,k\n[xi(t)− lcb∗(t)]\n\n ≤ Φi(k, Lik) := Lik∆̃i/2 + 3L0i ∆̃i(M − k + 1)\nwhere M is the maximal number of phases from Lemma 5b.\nLemma 6 Let\nFik =\nnS ∑\nt=τi,k\n[xi(t)− lcb∗(t)] .\nThen\nEik [Fik] ≤ Φi(k, Lik).\nProof Let kS be the last phase before the algorithm switches to EXP3.P with τkS+1 − 1 = nS . By Lemma 5b we have kS ≤ M . For k = kS + 1 the lemma holds trivially since Fi,kS+1 = 0.\nBy (6) we have\nEik [Fik] ≤ 2C4a∆̃iLik + 1\n1− qadv + Eik\n[ Fi,k+1 − ∆̃i(τi,k+1 − τi,k) ] .\nFor the expectation on the right hand side we distinguish three cases, depending on the termination condition of phase k and the value of Lik.\nCase 1: Phase k is terminated by the condition in Step 4.d. Then Li,k+1 = 2Lik and\nEik\n[ Fi,k+1 − ∆̃i(τi,k+1 − τi,k) ∣ ∣ ∣ Case 1 ] ≤ Φi(k + 1, 2Lik)− ∆̃iLik (7)\nusing the induction hypothesis. This is the case where negative regrets accumulate since 2C4a/(1− qadv) < 1.\nCase 2: Phase k is terminated by the condition in Step 4.a (4) and Lik > L0i . Then Li,k+1 = Lik/2 and\nEik\n[ Fi,k+1 − ∆̃i(τi,k+1 − τi,k) ∣ ∣ ∣ Case 2 ] ≤ Φi(k + 1, Lik/2). (8)\nCase 3: Phase k is terminated by the condition in Step 4.a and Lik = L0i . Then Li,k+1 = L0i and\nEik\n[ Fi,k+1 − ∆̃i(τi,k+1 − τi,k) ∣ ∣ ∣Case 3 ] ≤ Φi(k + 1, L0i ). (9)\nTo complete the induction proof, we need to show that for all three cases the right hand side of (7)– (9) is upper bounded by\nΦi(k, Lik)− 2C4a∆̃iLik + 1\n1− qadv .\nThis can be verified by straightforward calculation.\nNow (3) follows from Lemma 6 for k = 1:\nE\n\n\nnS ∑\nt=nB,i\n[xi(t)− lcb∗(t)]\n\n ≤ Φi(1, L0i ) = O ( L0i ∆̃iM ) = O\n(\nK log(n/δ)\n∆̃i\n)\n."
    }, {
      "heading" : "6. The stochastic analysis",
      "text" : "In this section we assume that all arms i are indeed stochastic with means µi. Recall that ∆i = µ∗ − µi, µ∗ = maxi µi. We show that with high probability the algorithm does not switch to EXP3.P and any suboptimal arm i is chosen at most O (\nlog(n/δ)/∆2i ) times.\nWe already have from Lemma 5c that with probability 1 − O (δ), Ti(nB,i, nS) = O ( M/∆̃2i ) for all arms. Thus we only need to bound the number of plays before an arm is evicted from A, Ti(1, nB,i − 1). The next lemma summarizes some properties of SAPO against stochastic bandits.\nLemma 7 With probability 1−O (δ) the following holds for all time steps t and all arms i: (a) If i ∈ A(t) then |µ̄i(t)− µi| ≤ width(t)/2. (b) If i ∈ A(t) then |µ̂i(t)− µi| ≤ widthi(t)/2. (c) If i ∈ A(t) then µ̄i(t), µi ∈ [lcbi(t),ucbi(t)] and µ̂i(t), µi ≥ lcbi(t). (d) If ∆i∗ = 0 then i∗ ∈ A(t). Furthermore, µ∗ ≥ lcb∗(t). (e) If i ∈ B(t) then ∆̃i ≤ 2∆i.\nProof (Sketch) Statements (a) and (b) follow from Hoeffding-Azuma’s inequality. Details are given in Appendix E.1.\nFor statement (c) we observe that by construction there is a time s ≤ t with µ̄i(s)−width(s) = lcbi(t). Thus (a) implies µ̄i(t) ≥ µi−width(t)/2 ≥ µ̄i(s)−width(s)/2−width(t)/2 ≥ µ̄i(s)− width(s) = lcbi(t). The other inequalities follow analogously.\n4. If k is the last phase and the phase is terminated by a condition in Step 1, then the same analysis applies but the value of Lk+1,i is irrelevant, since Fi,k+1 = 0.\nStatement (d) is proven by induction on t. Let i∗ be an arm with µi∗ = µ∗. If i∗ ∈ A(t − 1) then we have by (c) that µ∗ ≥ lcb∗(t−1). If any arm i is evicted at time t, then we have by Step 2.a and (b) that ∆i = µ∗−µi ≥ lcb∗(t−1)−µ̂i(t−1)−widthi(t−1)/2 ≥ (Cgap−1/2)widthi(t−1) > 0. Thus i 6= i∗ and i∗ ∈ A(t).\nThis also shows that when arm i is evicted, ∆̃i = Cgap ·widthi(t−1) ≤ Cgap/(Cgap−1/2)∆i, which is statement (e).\nTo get a bound on Ti(1, nB,i − 1), we show that ∆̃i = Cgap ·widthi(nB,i − 1) cannot be too small.\nLemma 8 With probability 1−O (δ) it holds for all times t and all arms i ∈ A(t) with Ti(t−1) ≥ Cinit log(n/δ), that\nCgap · widthi(t− 1) ≥ ∆i/2.\nThe argument behind the lemma is that if i ∈ A(t) then Cgap ·widthi(t−1) ≥ lcb∗(t−1)−µ̂i(t−1) where lcb∗(t − 1) is sufficiently close to µ∗ and µ̂i(t − 1) is sufficiently close to µi. The proof is given in Appendix E.2.\nSince i ∈ A(nB,i − 1), we get from Lemma 8 that with probability 1−O (δ),\nTi(nB,i − 1) ≤ Ti(nB,i − 2) + 1 = Cw log(n/δ)\n[widthi(nB,i − 2)]2 + 1 ≤\n4CwC 2 gap log(n/δ)\n∆2i + 1.\nTogether with Lemma 5c we have with probability 1−O (δ) that for all arms,\nTi(nS) ≤ 101\n100 L0iM/K +\n4CwC 2 gap log(n/δ)\n∆2i + 1 = O\n(\nlog(n/δ)\n∆2i\n)\n. (10)\nFinally, we need to bound the probability the SAPO switches to EXP3.P. Switching in Step 1.a is already handled by Lemma 7c. Switching in Step 1.b is also unlikely, since it would mean that the algorithm has accumulated large regret. This contradicts the upper bound (10). Lemma 17 shows that SAPO switches in Step 1.b only with probability 1−O (δ).\nThe difficult part, though, is to show that the condition in Step 4.a is not triggered too often such that Step 4.c switches to EXP3.P. We first calculate the false positive rate qsto, the probability that during a given phase the condition in Step 4.a is triggered. The false positive rate is again a constant but small, qsto ≤ 0.21, see Lemma 18.\nNow for a fixed arm the probability that in exactly E ≥ E0 out of at most M phases the condition in Step 4.a is triggered, is at most\n(M E ) qsto E . We set p = qsto/(1 + qsto) and use a tail\nbound for the binomial distribution to sum over E = E0, . . . ,M :\nM ∑\nE=E0\n(\nM\nE\n)\nqsto E = (1 + qsto)\nM M ∑\nE=E0\n(\nM\nE\n)\npE(1− p)M−E\n≤ (1 + qsto)M exp { −M ·D(E0/M ||p) }\nwhere D(a||p) = a log ap + (1 − a) log 1−a1−p is the relative entropy. Since E 0 M ≥ CE2CE+1/ log 2 , this sum is O (δ/n) and a union bound over the arms completes the proof."
    }, {
      "heading" : "Acknowledgments",
      "text" : "We thank the anonymous reviewers for their very valuable comments. The research leading to these results has received funding from the European Community’s Seventh Framework Programme (FP7/2007-2013) under grant agreement n◦ 231495 (CompLACS) and from the Austrian Science Fund (FWF) under contract P 26219-N15."
    }, {
      "heading" : "Appendix A. Concentration inequalities",
      "text" : "Lemma 9 ((McDiarmid, 1998, Theorem 3.15)) Let Y1, . . . , YN be a martingale difference sequence with SN = Y1 + . . . + YN with the corresponding filtration F0 ⊆ F1 ⊆ . . . ⊆ FN . Let Yi ≤ b and ∑N i=1 E [ Y 2i |Fi−1 ]\n≤ V . Then for any z ≥ 0, P {SN ≥ z} ≤ exp ( −z2/(2V + 2bz/3) ) .\nLemma 10 ((McDiarmid, 1998, Theorem 3.13)) Let Y1, . . . , YN be a martingale difference sequence with ak ≤ Yk ≤ bk for suitable constants ak, bk. Then for any z ≥ 0,\nP\n{\nmax 1≤m≤N\nm ∑\nk=1\nYk ≥ z } ≤ exp ( −2z2 / N ∑\nk=1\n(bk − ak)2 ) .\nCorollary 11 Let Y1, . . . , YN be a martingale difference sequence with ak ≤ Yk ≤ bk for suitable constants ak, bk. Then for any z ≥ 0,\nP\n{\nmax 1≤s≤t≤N\nt ∑\nk=s\nYk ≥ z } ≤ 2 exp ( −z2 / 2 ∑N\nk=1 (bk − ak)2\n)\n.\nProof\nP\n{\nmax 1≤s≤t≤N\nt ∑\nk=s\nYk ≥ z } ≤ P {\nmax 1≤t≤N\nt ∑\nk=1\nYk ≥ z/2 } + P {\nmax 1≤s≤N\ns−1 ∑\nk=1\n(−Yk) ≥ z/2 }\n≤ 2 exp ( −z2 / 2 ∑N\nk=1 (bk − ak)2\n)\n."
    }, {
      "heading" : "Appendix B. Proof of the lower bound (Theorem 2)",
      "text" : "Let ∆ = 1/8. We consider a stochastic bandit problem with constant reward x1(t) = 1/2 for arm 1 and Bernoulli rewards with µ2 = 1/2 − ∆ for arm 2. We divide the time steps into phases of increasing length Lj = 3j⌊nα⌋, j = 0, . . . , J − 1 with J ≥ 1−αlog 3 log n and an incomplete last phase j = J . Since the pseudo-regret of the player is at most C(log n)β , there is a phase j∗ < J where the expected number of plays of arm 2 in this phase is at most B with\nB = 8C log 3\n1− α (log n) β−1.\nWe construct an adversarial bandit problem by modifying the Bernoulli distribution of arm 2. Before phase j∗ the distribution remains unchanged with µ2 = 1/2−∆, but in phase j∗ and beyond we set µ2 = 1/2+∆. Since this bandit problem depends only on the player strategy (for identifying phase j∗) but not on the actual choices of the player, this adversary is oblivious.\nLet T j ∗ 2 be the number of plays of arm 2 in phase j ∗, and let Padv {·} and Eadv [·] denote the probability and expectation in respect to this adversarial bandit problem. By Lemma 12 below we have\nPadv\n{\nT j ∗ 2 ≤ 4B } ≥ 1/(16nǫ).\nSince xIt(t) − Eadv [xIt(t)|Ht−1], t = 1, . . . , n, forms a martingale difference sequence, we can apply Azuma-Hoeffding’s inequality (Lemma 10) and obtain\nPadv\n{\nn ∑\nt=1\nxIt(t) ≥ n ∑\nt=1\nEadv [xIt|Ht−1] + √ 2n log n\n}\n≤ 1/n2\nand\nPadv\n{\nT j ∗ 2 ≤ 4B ∧ n ∑\nt=1\nxIt(t) <\nn ∑\nt=1\nEadv [xIt|Ht−1] + √ 2n log n\n}\n≥ 1/(16nǫ)− 1/n2. (11)\nBy the construction of the adversarial bandit problem, T j ∗ 2 ≤ 4B implies that n ∑\nt=1\nEadv [xIt |Ht−1] ≤ n/2 + 4B∆+ (n − tj∗)∆, (12)\nwhere tj∗ denotes the time step at the end of phase j∗. For arm 2 we have\nn ∑\nt=1\nEadv [x2(t)] = n/2− j∗−1 ∑\nj=0\nLj∆+ Lj∗∆+ (n − tj∗)∆\n= n/2 + ⌊nα⌋∆ ( 3j ∗ − 3 j∗ − 1 2 ) + (n− tj∗)∆ ≥ n/2 + ⌊nα⌋∆+ (n− tj∗)∆.\nApplying Azuma-Hoeffdings’s inequality for arm 2 and combining with (11) and (12) we get\nPadv\n{\nn ∑\nt=1\nx2(t)− n ∑\nt=1\nxIt(t) ≥ ⌊nα⌋∆ − 4B∆− 2 √ 2n log n\n}\n≥ 1/(16nǫ)− 2/n2.\nBy the condition on n, 4B∆ ≤ (ǫ log n)/(16∆) such that ⌊nα⌋∆− 4B∆− 2√2n log n ≥ nα/8− 4 √ n log n, which completes the proof of the high probability lower bound. For the lower bound on the expected regret we construct an adaptive adversary by modifying the construction above: Let T j ∗\n2 (t) be the number of plays of arm 2 in phase j ∗ up to and including time\nstep t. If T j ∗ 2 = T j∗ 2 (tj∗) ≤ 4B then the adversarial bandit problem above remains unmodified. If there is a time step t ≤ tj∗ with T j ∗\n2 (t) > 4B, then for all time steps > t we set again µ2 = 1/2−∆. From the argument for the oblivious adversary we have\nE\n[\nn ∑\nt=1\nx2(t)− n ∑\nt=1\nxIt(t)\n∣ ∣ ∣ ∣ ∣ T j ∗ 2 ≤ 4B ] P { T j ∗ 2 ≤ 4B }\n≥ [ nα/8− 4 √ n log n ] P\n{\nT j ∗ 2 ≤ 4B ∧ n ∑\nt=1\nx2(t)− n ∑\nt=1\nxIt(t) ≥ nα/8− 4 √ n log n\n}\n−n · P { T j ∗ 2 ≤ 4B ∧ n ∑\nt=1\nx2(t)− n ∑\nt=1\nxIt(t) < n α/8− 4\n√\nn log n\n}\n≥ [ nα/8− 4 √ n log n ] [ 1/(16nǫ)− 2/n2 ] − 2/n\n≥ [ nα/8− 4 √ n log n ] 1\n(16nǫ) − 3.\nAnalogously we get\nE\n[\nn ∑\nt=1\nx1(t)− n ∑\nt=1\nxIt(t)\n∣ ∣ ∣ ∣ ∣ T j ∗ 2 > 4B ] P { T j ∗ 2 > 4B }\n≥ −(4B + 1)∆ − √ 2n log n\n−n · P { T j ∗ 2 > 4B ∧ n ∑\nt=1\nx1(t)− n ∑\nt=1\nxIt(t) < −(4B + 1)∆ − √ 2n log n\n}\n≥ −(4B + 1)∆ − √ 2n log n− 1/n ≥ −2 √ n log n.\nThus\nE\n[\nmax i\nn ∑\nt=1\nxi(t)− n ∑\nt=1\nxIt(t)\n]\n≥ [ nα/8− 4 √ n log n ] 1\n(16nǫ) − 3− 2\n√\nn log n\n≥ n α−ǫ\n128 − 3\n√\nn log n.\nLemma 12 For any n with (log n)2−β ≥ 64C log 3(1−α)ǫ ,\nPadv\n{\nT j ∗ 2 ≤ 4B } ≥ 1/(16nǫ).\nProof The proof follows a standard argument, e.g. (Mannor and Tsitsiklis, 2004). Let Psto {·} and Esto [·] denote the probability and expectation in respect to the stochastic bandit problem defined above. Since Esto [ T j ∗\n2\n] ≤ B we have Psto { T j ∗ 2 > 4B } < 1/4 and thus\nPsto\n{\nT j ∗ 2 ≤ 4B } > 3/4. (13)\nLet Gj ∗ 2 be the sum of rewards received when playing arm 2 in phase j ∗. Conditioned on T j ∗ 2 , G j∗ 2 is a binomial random variable with parameters T j ∗\n2 and µ2. Hence by (Kaas and Buhrman, 1980),\nPsto\n{\nGj ∗ 2 ≤ ⌊T j ∗ 2 (1/2 −∆)⌋ } ≤ 1/2. (14)\nLet ω denote a particular realization of rewards xi(t), i ∈ {1, 2}, 1 ≤ t ≤ tj∗ , and player choices I1, . . . , Itj∗ . For any realization ω the probabilities Psto {ω} and Padv {ω} are related by\nPadv {ω} = Psto {ω} (1/2 + ∆)G\nj∗ 2 (ω)(1/2 −∆)T j ∗ 2 (ω)−Gj ∗ 2 (ω)\n(1/2 −∆)Gj ∗ 2 (ω)(1/2 + ∆)T j∗ 2 (ω)−Gj ∗ 2 (ω)\n= Psto {ω} ( 1− 2∆ 1 + 2∆\n)T j ∗ 2 (ω)−2Gj ∗ 2 (ω)\n.\nIf Gj ∗ 2 (ω) ≥ ⌊(1/2 −∆)T j∗ 2 (ω)⌋ then\nPadv {ω} ≥ Psto {ω} ( 1− 2∆ 1 + 2∆\n)T j ∗ 2 (ω)−2((1/2−∆)T j ∗ 2 (ω)−1)\n= Psto {ω} ( 1− 2∆ 1 + 2∆\n)2∆T j ∗\n2 (ω)+2\n.\nIf furthermore T j ∗ 2 (ω) ≤ 4B, then\nPadv {ω} ≥ Psto {ω} ( 1− 2∆ 1 + 2∆ )8∆B+2 .\nHence\nPadv\n{\nT j ∗ 2 ≤ 4B } ≥ Padv { T j ∗ 2 ≤ 4B ∧Gj ∗ 2 ≥ ⌊T j ∗ 2 (1/2 −∆)⌋ }\n≥ Psto { T j ∗ 2 ≤ 4B ∧G j∗ 2 ≥ ⌊T j∗ 2 (1/2 −∆)⌋ }\n(\n1− 2∆ 1 + 2∆\n)8∆B+2\n[by (13) and (14)]\n≥ 1 4\n(\n1− 2∆ 1 + 2∆\n)8∆B+2\n≥ 1 4 (1− 4∆)8∆B+2 [∆ = 1/8, 1− x ≥ e−2x for 0 ≤ x ≤ 1/2]\n≥ 1 16 exp{−64∆2B} ≥ 1 16nǫ\nfor (log n)2−β ≥ 64C log 3(1−α)ǫ ."
    }, {
      "heading" : "Appendix C. Proof of Lemma 5",
      "text" : "Proof of (b) We fix some arm i. By the condition in Step 4.c, Step 4.b can be executed for this arm at most E0 times. Let m be the number of executions of Step 4.d for arm i, such that the number of phases is at most m + E0 + 1 and the length of the longest phase is at least 2m−E0−1 · L0i . Then n ≥ ∑m+E0k=1 (τi,k+1−τi,k) ≥ 2m−E 0−1+2E0 and m ≤ E0+⌊log2(n−1)⌋ ≤ E0+⌈log2 n⌉−1.\nProof of (c) We fix some arm i and use Bernstein’s inequality (Lemma 9) with the martingale differences\nYt = I [It = i]− pi(t)\nfor nB,i ≤ t ≤ nS and Yt = 0 otherwise. Then Yj ≤ 1 and n ∑\nt=1\nE[Y 2t |Ht−1] = n ∑\nt=1\nE[Y 2t |pi(t)] ≤ nS ∑\nt=nB,i\npi(t).\nIn any testing phase k, pi(t) = L0i /(KLik) for τi,k ≤ t < τi,k+1 ≤ τi,k + Lik. Thus in each phase ∑τi,k+1−1\nt=τi,k pi(t) ≤ L0i /K and ∑nS t=nB,i pi(t) ≤ L0iM/K . Hence Bernstein’s inequality gives\nP { Ti(nB,i, nS) ≥ (1 + C)L0iM/K }\n≤ P { n ∑\nt=1\nYt ≥ CL0iM/K }\n≤ exp { −C 2L0iM/K\n2 + 2C/3\n} ≤ exp { −2C 2CpCE\n2 + 2C/3 log(n/δ)\n}\n≤ δ/n\nfor C ≥ 1/100. A union bound for i completes the proof."
    }, {
      "heading" : "Appendix D. Proofs for SAPO against adversarial bandits",
      "text" : "D.1. Proof of inequality (2)\nWe need to show that\nE\n\n\nnB,i−1 ∑\nt=1\n[xi(t)− lcb∗(t)]\n\n = O ( √ Kn log(n/δ) ) .\nBy the definition of µ̄i(t) and by Step 1.a of SAPO we have by Wald’s equation that\nE\n\n\nnB,i−1 ∑\nt=1\nxi(t)\n\n = E [(nB,i − 1) · µ̄i(nB,i − 1)] ≤ E [ (nB,i − 1) · ucbi(nB,i − 1) ] .\nSince lcbi(t) ≤ lcb∗(t) and ucbi(t) is non-increasing,\nE\n\n\nnB,i−1 ∑\nt=1\n[xi(t)− lcb∗(t)]\n\n ≤ E\n\n\nnB,i−1 ∑\nt=1\n[ ucbi(t)− lcbi(t) ]\n\n\n≤ 2E\n\n\nnB,i−1 ∑\nt=1\nwidth(t)\n\n = 2E\n\n\nnB,i−1 ∑\nt=1\n√\n2CwK log(n/δ)\nt\n\n ≤ 4 √\n2CwKn log(n/δ).\nD.2. Proof of inequality (3)\nLemma 13 We fix some phase k and s ≥ τi,k. Let tC(s) = min{s ≤ t < τi,k+1 : Di(s, t) > 2C4a∆̃iLik}. (15)\nIf no such t exists, we set tC(s) = τi,k+1 − 1. Then\nP\n{ Di(s, tC(s)) > 2C4a∆̃iLik ∧ D̂i(s, tC(s)) < C4a∆̃iLikpik ∣ ∣ ∣ Hs−1 } ≤ qadv := 1/25.\nProof We use Bernstein’s inequality for martingales (Lemma 9) on the martingale differences\nYt = pik[xi(t)− µ̃i]− I [It = i] [xi(t)− µ̃i]\nfor s ≤ t ≤ tC(s) and Yj = 0 otherwise; with b = 1 and V = pikLik = L0i /K . We get\nP\n{ Di(s, tC(s)) > 2C4a∆̃iLik ∧ D̂i(s, tC(s)) < C4a∆̃iLikpik ∣ ∣ ∣Hs−1 }\n≤ P { pikDi(s, tC(s))− D̂i(s, tC(s)) > C4a∆̃iLikpik ∣ ∣ ∣ Hs−1 }\n= P { pikDi(s, tC(s))− D̂i(s, tC(s)) > C4a∆̃iL0i /K ∣ ∣ ∣ Hs−1 }\n≤ exp ( −min{CpC24a/4, CpC4a/2} ) ≤ 1/25.\nLemma 14 Consider some phase k. Then\nPik\n{ Di(τi,k, τi,k+1 − 1) ≥ m(2C4a∆̃iLik + 1) } ≤ qmadv.\nProof Since Di(s, t + 1) −Di(s, t) ≤ 1, Di(τi,k, τi,k+1 − 1) ≥ m(2C4a∆̃iLik + 1) implies that there are time steps τi,k = s1 < s2 < · · · < sm+1 ≤ τi,k+1 with D(sj, sj+1 − 2) ≤ 2C4a∆̃iLik and 2C4a∆̃iLik < D(sj, sj+1 − 1) ≤ 2C4a∆̃iLik + 1. Furthermore, by the condition in Step 4.a, D̂i(sj , t) < C4a∆̃iLikpik for j = 1, . . . ,m and sj ≤ t < τi,k+1 (otherwise the phase would have ended before τi,k+1). We define the event\nNDj = {sj+1 = tC(sj) + 1 ∧ Di(sj , sj+1 − 1) > 2C4a∆̃iLik ∧ D̂i(sj , sj+1 − 1) < C4a∆̃iLikpik}.\nThen\nPik\n{ Di(τi,k, τi,k+1 − 1) ≥ m(2C4a∆̃iLik + 1) } ≤ Pik\n\n\n\nm ∧\nj=1\nNDj\n\n\n\n=\nm ∏\nj=1\nPik\n\n\n\nNDj\n∣ ∣ ∣ ∣ ∣ ∣ j−1 ∧\nj′=1\nNDj′\n\n\n\n≤ qmadv by Lemma 13.\nLemma 15 For any phase k,\nEik [Di(τi,k, τi,k+1 − 1)] ≤ 2C4a∆̃iLik + 1\n1− qadv\nProof\nEik [Di(τi,k, τi,k+1 − 1)] ≤ (2C4a∆̃iLik + 1) ∑\nm≥0\nPik\n{ Di(τi,k, τi,k+1 − 1) ≥ m(2C4a∆̃iLik + 1) }\n≤ 2C4a∆̃iLik + 1 1− qadv\nby Lemma 14."
    }, {
      "heading" : "Appendix E. Proofs for SAPO against stochastic bandits",
      "text" : "E.1. Proof of Lemma 7\nWe show that (a) and (b) hold with probability 1−O (δ). The other statements of the lemma follow from the events in (a) and (b).\nProof of (a) and (b) We fix some step t and some arm i, and condition on Ti(t) = T . Using Hoeffding-Azuma’s inequality (Lemma 10) we find\nP {µ̂i(t)− µi > widthi(t)/2|Ti(t) = T} ≤ exp {−Cw log(n/δ)/2} ≤ δ/(16Kn2).\nAnalogously we bound µi − µ̂i(t). A union bound over t, i, and T gives (b). Since i ∈ A(t) implies pi(t) ≥ 1/K , Bernstein’s inequality (Lemma 9) with b = K and V = Kt gives\nP { µ̄i(t)− µi > width(t)/2 } ≤ exp { −Cw log(n/δ) 4(2 + 2/3) } ≤ δ/(16Kn).\nUsing the same bound for µi − µ̄i(t) and summing over t and i gives (a).\nE.2. Proof of Lemma 8\nLemma 16 With probability 1− O (δ) the following holds for all time steps t and all arms i, i′: If i′ ∈ A(t) and Ti(t) ≥ Cinit log(n/δ), then Ti′(t) ≥ Ti(t)/4.\nProof We fix t, i, and i′. By the construction of SAPO we have P {It = i′|Ht−1, i′ ∈ A(t)} ≥ P {It = i|Ht−1, i′ ∈ A(t)}. From Is, . . . , It we select those with It′\n1 , . . . , It′ k ∈ {i, i′} and define a\nsuper-martingale with differences Yj = I [ It′j = i ] − I [ It′j = i ′ ] for t′j ≤ t and Yj = 0 for t′j > t.\nThen\nP { Ti′(t) < Ti(t)/4 ∧ i′ ∈ A(t) ∧ Ti(s, t) ≥ Cinit log(n/δ) }\n= P\n{\n3 8 [Ti′(s, t) + Ti(s, t)] < 5 8 [Ti(s, t)− Ti′(s, t)] ∧ i′ ∈ A(t) ∧ Ti(s, t) ≥ Cinit log(n/δ)\n}\n≤ ∑\nk≥Cinit log(n/δ)\nP\n{\n3 8 k < 5 8 [Ti(s, t)− Ti′(s, t)] ∧ i′ ∈ A(t) ∧ Ti′(s, t) + Ti(s, t) = k\n}\n≤ ∑\nk≥Cinit log(n/δ)\nP\n\n\n\n3k\n5 <\nk ∑\nj=1\nYj\n\n\n\n≤ ∑\nk≥Cinit log(n/δ)\nexp\n{\n−9k 50\n} ≤ exp { −9Cinit 50 log(n/δ) }\n1\n1− exp{−9/50}\nby Hoeffding-Azuma’s inequality (Lemma 10). A union bound for t, i, and i′ completes the proof.\nProof of Lemma 8 Let arm i∗ be optimal, µi∗ = µ∗, such that i∗ ∈ A(t) by Lemma 7d. By Lemma 7b, with probability 1−O (δ) we have |µ̂i(t−1)−µi| ≤ widthi(t−1)/2 for arms i and i∗. By construction, lcb∗(t−1) ≥ lcbi∗(t − 1) ≥ µ̂i∗(t − 1) − widthi∗(t − 1). By Lemma 16, with probability 1 − O (δ) we have Ti∗(t− 1) ≥ Ti(t− 1)/4. Then\n∆i = µ ∗ − µi\n≤ µ̂i∗(t− 1) + widthi∗(t− 1)/2 − µ̂i(t− 1) + widthi(t− 1)/2 ≤ lcb∗(t− 1) + 3widthi∗(t− 1)/2 − µ̂i(t− 1) + widthi(t− 1)/2 ≤ (Cgap + 3 + 1/2)widthi(t− 1) ≤ 2Cgap · widthi(t− 1).\nE.3. Considering Step 1.b\nLemma 17 The probability that there is a time t with ∑t−1 s=1[lcb ∗(s) − xIs(s)] > C1b √ Kn log(n/δ) is at most O (δ).\nProof By Lemma 7d we have lcb∗(s) ≤ µ∗ for all s with probability 1−O (δ). Thus (10) implies that with probability 1−O (δ),\nt−1 ∑\ns=1\n(lcb∗(s)− E [xIs(s)|Hs−1]) ≤ t−1 ∑\ns=1\n(µ∗(s)− E [xIs(s)|Hs−1])\n=\nK ∑\ni=1\n∆iTi(t− 1) ≤ K ∑\ni=1\nC log(n/δ) ∆i ≤ C C1b √ Kn log(n/δ)\nfor C > 101100Cp(2CE+1)+4CwC 2 gap. By Hoeffding-Azuma’s inequality (Lemma 10) we also have\nP\n{\nmax 1≤t≤n\nt−1 ∑\ns=1\n(xIs(s)− E [xIs(s)|Hs−1]) ≥ √ 2n log(n/δ)\n}\n≤ (δ/n) ≤ 3δ/4.\nThus the lemma follows for C1b ≥ 522 which satisfies C1b ≥ C/C1b + 1.\nE.4. Considering Step 4.a\nLemma 18 If the statements in Lemma 7 hold, then\nPik {The condition of Step 4.a is triggered for arm i in its phase k} ≤ qsto := 0.21.\nProof The probability of triggering the condition in phase k is\nPik\n{\nmax τi,k≤s≤t<τi,k+1\nD̂(s, t) ≥ C4a∆̃iLikpik } .\nWe first bound the number of plays in this round, Ti(τi,k, τi,k+1−1). Applying Bernstein’s inequality (Lemma 9) with b = 1, V = Likpik, and z = Likpik we get\nPik {Ti(τi,k, τi,k+1 − 1) ≥ 2Likpik} ≤ exp ( − L 2 ikp 2 ik\n2Likpik + 2Likpik/3\n) ≤ exp (\n−3Cp 8∆̃2i\n)\n.\nBy Lemma 7b and Step 2.b, µi − µ̃i ≤ widthi(nB,i − 1)/2 ≤ ∆̃i/(2Cgap). Conditioning on Ti(τi,k, τi,k+1 − 1) < 2Likpik and applying Corollary 11 of Hoeffding-Azuma’s inequality with\nz = C4a∆̃iLikpik − (µi − µ̃i)2Likpik = (C4a − 1/Cgap)∆̃iLikpik\nyields\nPik\n{\nmax τi,k≤s≤t<τi,k+1\nD̂(s, t) ≥ C4a∆̃iLikpik ∣ ∣ ∣\n∣\nTi(τi,k, τi,k+1 − 1) < 2Likpik }\n≤ 2 exp ( − ( C4a − 1\nCgap\n)2 (∆̃iLikpik) 2\n4Likpik\n) ≤ 2 exp ( − ( C4a − 1\nCgap\n)2 Cp 4\n)\n≤ 0.21"
    } ],
    "references" : [ {
      "title" : "Finite-time analysis of the multiarmed bandit problem",
      "author" : [ "Peter Auer", "Nicolò Cesa-Bianchi", "Paul Fischer" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Auer et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Auer et al\\.",
      "year" : 2002
    }, {
      "title" : "The nonstochastic multiarmed bandit problem",
      "author" : [ "Peter Auer", "Nicolò Cesa-Bianchi", "Yoav Freund", "Robert E. Schapire" ],
      "venue" : "SIAM J. Comput.,",
      "citeRegEx" : "Auer et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Auer et al\\.",
      "year" : 2002
    }, {
      "title" : "Regret analysis of stochastic and nonstochastic multiarmed bandit problems",
      "author" : [ "Sébastien Bubeck", "Nicolò Cesa-Bianchi" ],
      "venue" : "Foundations and Trends in Machine Learning,",
      "citeRegEx" : "Bubeck and Cesa.Bianchi.,? \\Q2012\\E",
      "shortCiteRegEx" : "Bubeck and Cesa.Bianchi.",
      "year" : 2012
    }, {
      "title" : "The best of both worlds: Stochastic and adversarial bandits",
      "author" : [ "Sébastien Bubeck", "Aleksandrs Slivkins" ],
      "venue" : "In COLT - The 25th Annual Conference on Learning Theory,",
      "citeRegEx" : "Bubeck and Slivkins.,? \\Q2012\\E",
      "shortCiteRegEx" : "Bubeck and Slivkins.",
      "year" : 2012
    }, {
      "title" : "Mean, median and mode in binomial distributions",
      "author" : [ "R. Kaas", "J.M. Buhrman" ],
      "venue" : "Statistica Neerlandica,",
      "citeRegEx" : "Kaas and Buhrman.,? \\Q1980\\E",
      "shortCiteRegEx" : "Kaas and Buhrman.",
      "year" : 1980
    }, {
      "title" : "The sample complexity of exploration in the multi-armed bandit problem",
      "author" : [ "Shie Mannor", "John N. Tsitsiklis" ],
      "venue" : "JMLR, 5:623–648,",
      "citeRegEx" : "Mannor and Tsitsiklis.,? \\Q2004\\E",
      "shortCiteRegEx" : "Mannor and Tsitsiklis.",
      "year" : 2004
    }, {
      "title" : "Concentration, volume 16 of Algorithms Combin., pages 195–248",
      "author" : [ "Colin McDiarmid" ],
      "venue" : null,
      "citeRegEx" : "McDiarmid.,? \\Q1998\\E",
      "shortCiteRegEx" : "McDiarmid.",
      "year" : 1998
    }, {
      "title" : "Some aspects of the sequential design of experiments",
      "author" : [ "Herbert Robbins" ],
      "venue" : "Bull. Amer. Math. Soc.,",
      "citeRegEx" : "Robbins.,? \\Q1952\\E",
      "shortCiteRegEx" : "Robbins.",
      "year" : 1952
    }, {
      "title" : "One practical algorithm for both stochastic and adversarial bandits",
      "author" : [ "Yevgeny Seldin", "Aleksandrs Slivkins" ],
      "venue" : "In Proceedings of the 31th International Conference on Machine Learning,",
      "citeRegEx" : "Seldin and Slivkins.,? \\Q2014\\E",
      "shortCiteRegEx" : "Seldin and Slivkins.",
      "year" : 2014
    }, {
      "title" : "Concentration inequalities Lemma",
      "author" : [ "A. Appendix" ],
      "venue" : "((McDiarmid,",
      "citeRegEx" : "Appendix,? \\Q1998\\E",
      "shortCiteRegEx" : "Appendix",
      "year" : 1998
    } ],
    "referenceMentions" : [ {
      "referenceID" : 7,
      "context" : "In the original formulation (Robbins, 1952) it is assumed that the rewards are generated independently at random, governed by fixed but unknown probability distributions with means μi for each arm i = 1, .",
      "startOffset" : 28,
      "endOffset" : 43
    }, {
      "referenceID" : 1,
      "context" : "This complements previous results of Bubeck and Slivkins (2012) that show Õ ( √ n) expected adversarial regret with O ( (log n) ) stochastic pseudo-regret.",
      "startOffset" : 37,
      "endOffset" : 64
    }, {
      "referenceID" : 2,
      "context" : "An extensive overview of multi-armed bandit problems is given in (Bubeck and Cesa-Bianchi, 2012).",
      "startOffset" : 65,
      "endOffset" : 96
    }, {
      "referenceID" : 0,
      "context" : "Auer et al. (2002a): Rsto(n) = O (log n) .",
      "startOffset" : 0,
      "endOffset" : 20
    }, {
      "referenceID" : 0,
      "context" : "Auer et al. (2002a): Rsto(n) = O (log n) . Both of these bounds are known to be best possible. While the result for adversarial bandits is a worst-case — and thus possibly pessimistic — bound that holds for any sequence of rewards, the strong assumptions for stochastic bandits may sometimes be unjustified. Therefore an algorithm that can adapt to the actual difficulty of the problem is of great interest. The first such result was obtained by Bubeck and Slivkins (2012), who developed the SAO algorithm that with probability 1− δ achieves Radv(n) ≤ O (",
      "startOffset" : 0,
      "endOffset" : 473
    }, {
      "referenceID" : 3,
      "context" : "This gives, together with the results of (Bubeck and Slivkins, 2012), a quite complete characterization of algorithms that perform well both for stochastic and adversarial bandit problems.",
      "startOffset" : 41,
      "endOffset" : 68
    }, {
      "referenceID" : 3,
      "context" : "Comparison with related work Bubeck and Slivkins (2012) show for their SAO algorithm that with probability 1− δ, K ∑",
      "startOffset" : 29,
      "endOffset" : 56
    }, {
      "referenceID" : 8,
      "context" : "A different approach is taken in (Seldin and Slivkins, 2014): here the starting point is an algorithm for adversarial bandit problems that is modified by adding an additional exploration parameter to achieve also low pseudo-regret in stochastic bandit problems.",
      "startOffset" : 33,
      "endOffset" : 60
    } ],
    "year" : 2016,
    "abstractText" : "We present an algorithm that achieves almost optimal pseudo-regret bounds against adversarial and stochastic bandits. Against adversarial bandits the pseudo-regret is O (√ Kn logn ) and against stochastic bandits the regret is O ( ∑ i (logn)/∆i). We also show that no algorithm with O (logn) pseudo-regret against stochastic bandits can achieve Õ ( √ n) expected regret against adaptive adversarial bandits. This complements previous results of Bubeck and Slivkins (2012) that show Õ ( √ n) expected adversarial regret with O ( (log n) ) stochastic pseudo-regret.",
    "creator" : "LaTeX with hyperref package"
  }
}