{
  "name" : "1508.04582.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Learning to Predict Independent of Span",
    "authors" : [ "Hado van Hasselt", "Richard S. Sutton" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Learning long-term predictions",
      "text" : "The span of a multi-step prediction is the number of steps elapsing between when the prediction is made and when its target or ideal value is known. We consider the case in which predictions are made repeatedly, at each of a sequence of discrete time steps. For example, if on each day we predict what a stock market index will be in 30 days, then the span is 30, whereas if we predict at each hour what the stock market index will be in 30 days, then the span is 30× 24 = 720.\nThe span may vary for individual predictions in a sequence. For example, if we predict on each day what the stock-market index will be at the end of the year, then the span will be much longer for predictions made in January than it is for predictions made in\n∗Google DeepMind †Reinforcement Learning and Artificial Intelligence Laboratory\nDepartment of Computing Science, University of Alberta Edmonton, Alberta, Canada T6G 2E8\nar X\niv :1\n50 8.\n04 58\n2v 1\n[ cs\n.L G\n] 1\n9 A\nug 2\nDecember. If the span may vary in this way, then we consider the span of the prediction sequence to be the maximum possible span of any individual prediction in the sequence. For example, the span of a daily end-of-year stock-index prediction is 365. Often the span is infinite. For example, in reinforcement learning we often learn value functions that are predictions of the discounted sum of all future rewards in the potentially infinite future (Sutton and Barto 1998).\nIn this paper we consider computational and algorithmic issues in efficiently learning long-term predictions, defined as predictions of large integer span. Predictions could be long term in this sense either because a great deal of clock time passes, as in predicting something at the end of the year, or because predictions are made very often, with a short time between steps (e.g., as in high-frequency financial trading). The per-step computational complexity of some algorithms for learning accurate predictions depends on the span of the predictions, and this can become a significant concern if the span is large. Therefore, we focus on the construction of learning algorithms whose computational complexity per time step (in both time and memory) is constant (does not scale with time) and independent of span.\nThis paper features two recurring themes, the first of which is the repeated spontaneous emergence of, often well-known, algorithmic constructs, directly from our derivations. We start each derivation by formalizing a desired property and constructing an algorithm that fulfills it, without considering computationally efficiency. Then, we derive a spanindependent algorithm that results on each step in exactly the same predictions. Interestingly, each time a specific algorithmic construct emerges, demonstrating a clear connection between the desideratum (the ‘why’) and the algorithmic construct (the ‘how’). For instance, the desire to be independent of span leads to a dutch eligibility trace, which was previously derived only in the more specific context of online temporal difference (TD) learning (van Seijen and Sutton 2014).\nThe second theme is that we unify the algorithms at each step. Each time, we make sure to obtain an algorithm that is strictly more general than the previous ones, so that in the end we obtain one single algorithm that can fulfill all the desiderata while remaining computationally congenial."
    }, {
      "heading" : "2 Outline of the paper",
      "text" : "In this section, we briefly describe the high-level narrative of the paper, without going into technical detail. In each of the Sections 3 to 8, we describe and formalize one or more desirable properties for our algorithms and then derive a computationally congenial algorithm that achieves this exactly. We build up to the final, most general, algorithm that is ultimately derived in Section 8 to highlight the connections between desired properties and algorithmic constructs. Making these connections clear is one of the main goals of this paper.\nSpecifically, in Section 3 we derive a span-independent algorithm to update the predictions for a single final outcome. The algorithm is offline in the sense that does not change its predictions before observing the outcome. The dutch trace emerges spontaneously, which shows that this trace is closely tied to the requirement of span-independent computation. This emergence is surprising and intriguing because it shows that these traces\nare not specific to online TD learning, for which they were first proposed (van Seijen and Sutton 2014).\nIn Section 4 we derive span-independent updates that update the predictions online, towards interim targets that temporarily stand in for the final outcome. We show that the desire to be online results in the spontaneous emergence of TD errors (Sutton 1984; Sutton 1988). In this paper we are mostly agnostic to the origin of the interim targets. These may for instance be given by external experts or by own online predictions, as in standard TD learning (e.g., see Sutton and Barto 1998).\nIt can be beneficial to be able to switch smoothly between online and offline updates, on a step-by-step basis, for instance when we do not full trust some of the interim targets that we would use for our online updates. This allows us to have the best of both worlds: the online predictions stay trustworthy even if some interim targets are wrong, and we are still able to use any useful information immediately when it is observed. In Section 5 we consider how to do this efficiently and from our derivation an update emerges that averages the online weights in a separate trusted weight vector. This is interesting because such averaging is known to improve the convergence rates of online learning algorithms (Polyak and Juditsky 1992; Bach and Moulines 2013), but seems to only rarely be used in reinforcement learning (as noted, e.g., by Szepesvári 2010).\nSome interim targets may be so informative that we want their effect to persist in the predictions even after observing the final outcome. For instance, if the final outcome is stochastic and the interim targets are drawn independently from the same distribution it makes sense to average these instead of committing fully only to the final outcome. In the extreme, we might see an interim target that we trust so much that we do not even care about the actual outcome anymore, for instance because the interim target already takes into account all possible outcomes from that moment rather than only the specific one that will happen to materialize this time, resulting in a more accurate prediction on average than a single final outcome. In Section 6, we formalize these ideas and show they lead naturally to a form of TD(λ) (Sutton 1988; Sutton and Barto 1998).\nThe λ parameter that governs the amount of persistency of the interim targets can be interpreted as representing a degree of trust: if we trust an interim target fully (λ = 0) we do not need to consider later observations, while if we distrust it fully (λ = 1) it will be replaced by later targets and leave no trace in the final predictions. This is a different notion of trust than the one considered for the smooth switching between online and offline updates, where the trust was relative to the actual final outcome rather than the expected outcome. These two forms of trust are compatible and complimentary, and in Section 7 we show how to combine them into a single algorithm.\nUp to Section 7, we have only considered predicting a single final outcome in an episodic setting. In Section 8 we consider how to deal with two important generalizations of the problem setting: cumulative returns, and soft terminations. Cumulative returns allow us to see part of the return on each step, and allow us to start learning from these partial returns immediately in the online setting. Soft terminations allow us to learn about predictions that may conditionally terminate even if the actual process continues, and they allow for non-episodic predictions that may terminal softly on each step rather than completely at a single point in time. This leads to a single final algorithm that subsumes all previous algorithms as special cases. The algorithm is similar to the conventional TD(λ) algorithm but with important differences that ensure that it is exactly equivalent to the desired, but\ninefficient, algorithm and therefore inherits all its desirable properties. Because our final algorithm is novel, it is appropriate to analyze it. In Section 9 we prove that the algorithm is convergent under typical mild conditions, and that it converges to the same solution as similar previous algorithms, including TD(λ).\nWe conclude the paper with a short discussion in Section 10."
    }, {
      "heading" : "3 Independence of span and the emergence of traces",
      "text" : "We start with a supervised learning setting—predicting the final numeric outcome of an episodic process. An episode of the process starts at time t = 0 and moves stochastically from state to state generating feature vectors φt until termination with a final numeric outcome Z at final time T . For example, Z could be the price of a particular stock that we want to predict, and each episode may be a year, such that time T corresponds to the end of the year.\nWe consider the general case of multi-step predictions (T > 1), where a prediction is made on each step. The standard supervised learning setting is a special case where in each episode we only make one prediction (such that, without loss of generality, we can take T = 1).\nOur predictions are linear, that is, the prediction at time t is the inner product of φt and a learned weight vector θ, denoted φ>t θ. The algorithms are indifferent to the origins of the features, which may be handcrafted or learned.1 The weights have an initial value θ0 that is presumably due to previous episodes. We analyze how the weights change in a single episode (and thus we do not include the episode number in our notation).\nAt the final time, when Z is observed, we can update all the predictions towards the target as in the classical least mean squares (LMS) algorithm defined by the updates:\nθt+1 . = θt + αtφt ( Z − φ>t θt ) , t = 0, . . . , T − 1, (1)\nwhere αt > 0 is a step-size parameter that may vary from time step to time step (e.g., as a function of the state at that time). We call this a forward view, because to update the prediction at time t we need to look forward in time to the outcome Z which is observed at the later time T .\nTo perform the updates (1) we have to wait until Z is known and then do the update for all previous time steps t. This requires storing and then computing updates for all the preceding feature vectors. The required computational resources scale with the span of the prediction (the maximum length of an episode), which is what we wish to avoid. We seek incremental computations whose per-time-step complexity is O(n), where n is the number of parameters, and that result in the same weights as (1) at the end of the episode. That is, the incremental updates should compute the same θT as (1) if they are given the same input (the same θ0, the same sequence {φt}T−1t=0 , and the same Z).\nIt may seem that the best we can hope for is to approximate the result computed by the LMS algorithm, because of the strict computational restriction. Such a trade off between computation and accuracy is not uncommon. We will however now derive an algorithm\n1This includes, for instance, the case where φt is the last hidden layer of a neural network.\nthat finds the exact same final predictions with much more congenial computation, by carefully analyzing the total change to the weight vector due to the LMS algorithm.\nThe final step of the algorithm in (1) can be rewritten as θT = θT−1 + αT−1φT−1 ( Z − φ>T−1θT−1 ) = θT−1 + αT−1φT−1Z − αT−1φT−1φ>T−1θT−1 = ( I− αT−1φT−1φ>T−1 ) θT−1 + αT−1φT−1Z\n= FT−1θT−1 + αT−1φT−1Z .\nHere Ft . = I − αtφtφ>t is a fading matrix that will be important throughout this paper. Now, continuing,\nθT = FT−1 (FT−2θT−2 + αT−2φT−2Z) + αT−1φT−1Z (expanding θT−1)\n= FT−1FT−2θT−2 + (FT−1αT−2φT−2 + αT−1φT−1)Z (regrouping)\n= FT−1FT−2 (FT−3θT−3 + αT−3φT−3Z) + (FT−1αT−2φT−2 + αT−1φT−1)Z (recursing on θT−2)\n= FT−1FT−2FT−3θT−3 + (FT−1FT−2αT−3φT−3 + FT−1αT−2φT−2 + αT−1φT−1)Z (regrouping)\n... (recursing further)\n= FT−1FT−2 · · ·F0θ0︸ ︷︷ ︸ . = aT−1 +\n( T−1∑ t=0 FT−1FT−2 · · ·Ft+1αtφt ) ︸ ︷︷ ︸\n. = eT−1\nZ\n= aT−1 + eT−1Z , (2)\nwhere at and et are two auxiliary memory vectors. Importantly, the auxiliary vectors can be updated without knowledge of Z, and with complexity independent of span and proportional to the number of features. The at vector stores the effect of the initial weights on the updated weights. It is initialized as a0 = θ0 and can then be updated efficiently with\nat . = FtFt−1 · · ·F0θ0 = Ft (Ft−1 · · ·F1θ1) = Ftat−1\n= at−1 − αtφtφ>t at−1 = at−1 + αtφt(0− φ>t at−1) , t = 1, . . . , T − 1. (3)\nThe et vector is analogous to the conventional eligibility trace (see: Sutton 1988; Sutton and Barto 1998, and references therein) but has a special form as first proposed by van Seijen and Sutton (2014). It is initialized to e−1 = 0 (or, equivalently, to e0 = α0φ0) and then updated according to\net . = t∑ k=0 FtFt−1 · · ·Fk+1αkφk\n= t−1∑ k=0 FtFt−1 · · ·Fk+1αkφk + αtφt\n= Ft t−1∑ k=0\nFt−1Ft−2 · · ·Fk+1αkφk︸ ︷︷ ︸ = et−1 + αtφt\n= Ftet−1 + αtφt (4) = et−1 − αtφtφ>t et−1 + αtφt = et−1 + αtφt(1− φ>t et−1), t = 0, . . . , T − 1. (5)\nAn eligibility trace of this special form is called a dutch trace (van Hasselt, Mahmood, and Sutton 2014). For comparison, the conventional accumulating trace that it replaces can be written as e−1 . = 0 and et . = et−1 + αtφt. 2\nThe emergence of the dutch trace here is surprising and intriguing because, in contrast to previous work (van Seijen and Sutton 2014; van Hasselt, Mahmood, and Sutton 2014), the dutch trace has arisen in a setting without temporal-difference (TD) learning. Eligibility traces are not specific to TD learning at all; they are more fundamental than that. The need for eligibility traces seems to arise whenever one tries to learn long-term predictions in an efficient manner, that is, with computational complexity that is independent of predictive span.\nThe auxiliary vectors at and et are updated on each time step t < T and then, after observing Z at time T , are used to compute θT = aT−1 + eT−1Z, as in (2). This way we achieve exactly the same final result as the forward view (1), but with an algorithm whose time and memory complexity per step is uniformly O(n) and independent of span. The complete algorithm can be summarized as:\na0 . = θ0 , then at+1 . = at + αtφt(0− φ>t at), t = 1, . . . , T − 1 , e−1 . = 0 , then et . = et−1 + αtφt(1− φ>t et−1), t = 0, . . . , T − 1 , θT . = aT−1 + ZeT−1 .\n(6)\nThe vector aT−1 can be interpreted as storing the remaining effect of the initial weights θ0 after all updates in (1) have concluded. The trace eT−1 can be interpreted as storing all we need to know about the feature vectors that were observed during the episode. Together, these vectors allow us to replace all the T updates of the forward view with one fully equivalent update at the end of the episode.\nWe call the span-independent algorithm (6) the backward view corresponding to the forward view defined in (1), because on each step all updates only use information that is available at that time step: we only look backwards in time. The advantage of this is that the updates can be computed immediately and we do not have to wait and store observations until later. Until recently, exactly equivalences between forward and backward views were only known to exist for algorithms that update their predictions in batch (Sutton and Barto 1998). Van Seijen & Sutton (2014) were the first to derive an online backward\n2We incorporate the step size into both trace updates. This is a slight deviation from the way these traces are usually written to allow for time-changing step sizes and increased generality.\nview that was exactly equivalent to its forward view in terms of the learned predictions. The derivation in this section shows that such equivalences exist more generally, including for the LMS update in (1).\nWhen we consider the episode as a whole, there is no gain in total computation time for the span-independent algorithm (6) compared to the conventional algorithm (1): both algorithms use O(nT ) computation for the entire episode. However, in the span-independent algorithm the computation is spread out more evenly with a uniform per-step complexity of O(n), whereas the conventional algorithm performs the bulk of computation at the end, when we finally observe Z.\nAdditionally, there is a gain in terms of required memory. Algorithm (1) needs to store all previously observed features, leading to memory requirements of order O(nT ), whereas algorithm (6) only needs to store a and e and therefore has constant span-independent memory requirements of order O(n). In real-world problems, for instance in robotics, it is not uncommon to extract millions of features from the sensory inputs at each step (e.g., Montemerlo and Thrun 2003), where each step lasts only a fraction of a second. Consider a robot that generates one million features each 10 ms, where each feature is a real number represented with single precision using 4 bytes of memory. Figure 1 shows the resulting memory requirements for the conventional algorithm and the span-independent algorithm for different spans of the predictions. The required storage of the conventional algorithm ranges from 0.4 GB for predictions spanning one second to about 35 Terabytes for predictions spanning one day. While a few Gigabytes of on-board storage is feasible with today’s resources, several Terabits will be a significant burden for an autonomous\nmobile robot. Concretely, this means that with the conventional algorithm we would have to restrict either the number of features, the frequency at which we make predictions, or the maximum amount of wall time a prediction can span. The span-independent algorithm scales much better. In the example in Figure 1 it only needs to store two vectors with 106 components of 4 bytes each, resulting in memory requirements that are constant at 8 Megabytes."
    }, {
      "heading" : "4 Online updating and the emergence of TD errors",
      "text" : "The algorithms in the previous section do not make any changes to the predictions during the episode; these are offline algorithms. Yet it would sometimes make sense to update the predictions during an episode, especially if the span is very long and we do not want to wait that long before we start learning. In this section we introduce an online forward view and derive a span-independent algorithm (the backward view) that on each time step computes the exact same predictions.\nAn online algorithm cannot update the predictions towards the final outcome Z during the episode, because Z is not yet available. Instead, if we only have observations up to a horizon h < T we may want to move the predictions for all earlier times t < h towards some informed guess of what the final outcome will be. Such a guess plays the role of a target for the updates, like Z in the forward view (1), but it is used prior to Z being available; it is an interim target. We use Zh to denote the interim target at time h, which may be based on all the data available up that horizon. The interim target might be from a human expert or it might be, as in TD learning, the current prediction corresponding to the feature vector φh. Interim targets at times closer to T might produce more accurate predictions. In the example of the stock market, as we get closer to the end of the year we may be able to more accurately estimate the final stock price. For now we consider the general case and do not specify the source of the interim targets Zh, for h = 1, . . . , T − 1. Notationally it is convenient to define ZT . = Z. Note that the time index on Zh is a superscript rather than a subscript. Our convention is that the superscript position is reserved for the upper limit of the data considered available in an online update. The subscript position is used for the time step whose prediction is being modified.\nTo clarify the notation under online updating, we introduce the notation θht for the weights at step t based on all the data up through time h. Using these double subscripts, what we previously called θt would now be θ T t , because these weight vectors depend on Z which is considered to arrive at time T . The complete set of online updates is then\nθh0 . = θ0, h = 0, . . . , T ; θht+1 . = θht + αtφt ( Zh − φ>t θht ) , t = 0, . . . , h− 1 , h = 1, . . . , T ;\n= Ftθ h t + αtφtZ\nh. (7)\nThis online algorithm defines a set of h updates for each interim horizon h ∈ {1, . . . , T}. For each horizon, all predictions are updated towards the latest, and presumably best available, interim target. Although most updates do not involve the final outcome Z, this algorithm is still considered a forward view, because the prediction at some time t (e.g., φ>t θ h t ) is updated using interim targets that arrive later (e.g., Z h).\nWe can write out all the double-subscripted weight vectors in a triangle as\nθ00 θ10 θ 1 1\n... ...\n. . .\nθh0 θ h 1 . . . θ h h θh+10 θ h+1 1 . . . θ h+1 h θ h+1 h+1\n... ...\n... ...\n. . .\nθT0 θ T 1 . . . θ T h θ T h+1 . . . θ T T .\n(8)\nThe computation proceeds from top to bottom, one row at a time from left to right. Each row starts with the same initial values θh0 = θ0 and then computes the sequence θ h 1 , θ h 2 and so on If we really computed all the different weight vectors in the triangle then the algorithm would be inefficient. With a computational complexity of O(nh) on each time h ∈ {1, . . . , T}, the computation would not be constant per time step and would not be independent of the span of the prediction; the last row alone has complexity O(nT ) and it can only be computed after observing Z at time T . However, instead of computing the whole triangle, perhaps there is a way to incrementally compute just the diagonal, to somehow obtain θh+1h+1 from θ h h efficiently on each step. If this can be done, then the entire computation will be of uniform O(n) complexity per time step, independent of span. To find an efficient update along the diagonal of the triangle, notice first that the forward view (7) already provides a way to efficiently move right one step in the triangle. In other words, we can get to θh+1h+1 from θ h+1 h for any h with constant O(n) computation. If we can find an efficient way to step down in the triangle, that is to get to θh+1h from θ h h for any h with constant O(n) computation, then we can combine these two steps into a single O(n) update. To see if this is possible, we first write down explicitly how each weight vector in the triangle depends on the initial weights and the interim targets. Similar to our derivation of the final weights in (2) in the previous section, we can unroll the forward-view updates repeatedly starting from θht and obtaining\nθht = Ft−1θ h t−1 + αt−1φt−1Z h (applying (7) to θht )\n= Ft−1(Ft−2θ h t−2 + αt−2φt−2Z h) + αt−1φt−1Z h (applying (7) to θht−1) = Ft−1Ft−2θ h t−2 + (Ft−1αt−2φt−2 + αt−1φt−1)Z h (regrouping) = Ft−1Ft−2(Ft−3θ h t−3 + αt−3φt−3Z h) + (Ft−1αt−2φt−2 + αt−1φt−1)Z h\n(applying (7) to θht−2)\n= Ft−1Ft−2Ft−3θ h t−3 + (Ft−1Ft−2αt−3φt−3 + Ft−1αt−2φt−2 + αt−1φt−1)Z h\n... (continuing until we reach θh0 )\n= Ft−1 · · ·F0θ0︸ ︷︷ ︸ at−1 + t−1∑ j=0 Ft−1 · · ·Fj+1αjφj  ︸ ︷︷ ︸\net−1\nZh\n= at−1 + et−1Z h . (9)\nNotice that at−1 and et−1 depend only on time t and not on the data horizon h. We can use this result to find the difference between θh+1h and θ h h as\nθh+1h − θ h h = (ah−1 + eh−1Z h+1)− (ah−1 + eh−1Zh) = eh−1(Z h+1 − Zh) . (10)\nHere we see the emergence of a temporal difference error Zh+1 − Zh. We know from the previous section that eh−1 can be computed incrementally with (5) and therefore does not have to be recomputed from scratch for each new observation. Therefore, we now have an efficient way to compute θh+1h from θ h h for any h with constant O(n) computation per step that is independent of span. Now that we have θh+1h we can efficiently compute θ h+1 h+1 using (7), and we can merge these two steps to compute θh+1h+1 directly from θ h h. The complete update can then be written as\nθh+1h+1 = Fhθ h+1 h + αhφhZ h+1 (using (7)) = Fh ( θhh + eh−1(Z h+1 − Zh) ) + αhφhZ h+1 (using (10))\n= Fhθ h h + Fheh−1(Z h+1 − Zh) + αhφhZh+1 = Fhθ h h + (eh − αhφh)(Zh+1 − Zh) + αhφhZh+1 (using (4)) = Fhθ h h + eh(Z h+1 − Zh)− αhφh(Zh+1 − Zh) + αhφhZh+1 = Fhθ h h + eh(Z h+1 − Zh) + αhφhZh = (I− αhφhφ>h )θhh + eh(Zh+1 − Zh) + αhφhZh (by definition of Fh) = θhh + eh(Z h+1 − Zh) + αhφh ( Zh − φ>h θhh ) .\nThis update holds for all h ≥ 1. The update for θ11 is given directly by (7) as\nθ11 = θ 1 0 + α0φ0(Z 1 − φ>0 θ10) .\nFor any Z0 we can rewrite this as\nθ11 = θ 0 0 + α0φ0(Z 1 − φ>0 θ00) (using θ10 . = θ0 . = θ00)\n= θ00 + α0φ0(Z 1 − Z0 + Z0 − φ>0 θ00) = θ00 + α0φ0(Z 1 − Z0) + α0φ0(Z0 − φ>0 θ00) = θ00 + e0(Z 1 − Z0) + α0φ0(Z0 − φ>0 θ00) , (using e0 . = α0φ0)\nwhich means that then the update derived above for h ≥ 1 in fact also holds for h = 0. For concreteness, we will define Z0 . = 0, even though this value does not actually affect any of the weights. Now that we have an update that can compute θt+1t+1 from θ t t for any t, we can drop the redundant superscript. The resulting algorithm is\ne−1 . = 0, then et . = et−1 + αtφt(1− φ>t et−1), t = 0, . . . , T − 1, θt+1 . = θt + et ( Zt+1 − Zt ) + αtφt(Z t − φ>t θt), t = 0, . . . , T − 1. (11)\nBy construction this backward view is equivalent to the less efficient forward view (7) in the sense that θt = θ t t for all t. In contrast to the offline backward view (6) that we derived\nin the previous section, we no longer need to compute and store the auxiliary vector at. All relevant information that was contained therein is now stored directly in the online weights θt.\nAlthough the online backward view yields different predictions during the episode, the final weights θT are exactly equal to those computed by the conventional LMS algorithm (1) that constituted our first, offline, forward view. In terms of the triangle in (8), the online forward view (7) computes the whole triangle, the online backward view (11) efficiently computes only the diagonal, the offline forward view (1) computes only the last row, and the offline backward view (6) from the previous section computes only the final weights. All three algorithms ultimately result in the same final weights."
    }, {
      "heading" : "5 Unifying online and offline learning and the emer-",
      "text" : "gence of averaging\nThe online algorithms from the previous section do not quite subsume the offline algorithms from Section 3. Although they all reach the same weights by the end of the episode, during the episode their weights are different. The offline algorithm does not change the weights during the episode, and the online algorithm must change them.\nOne might think that the online algorithm is always better because it can immediately use any incoming relevant information, but it is not so. Suppose the interim targets are always wildly wrong (say due to a poor human ‘expert’). They would cause the weights of the online algorithm to also be wildly wrong for all steps except the last one at the end of the episode. In this case the weights of the online algorithm would be worse than those of the offline algorithm almost all of the time.\nBecause interim targets can sometimes be misleading, we might want to reduce their effect on some steps, based on how much we trust these targets. In this section θ̃t denotes the online weights, which are computed by the span-independent backward view (11). The unified weights θt take into account the degree of trust for each interim target and are used to make our predictions. If we trust Zt fully, we want to obtain the same weights as in the online algorithm, so that θt = θ̃t. If we do not trust Z\nt at all, we want the predictions to remain unchanged, so that θt = θt−1. For intermediate degrees of trust, βt ∈ (0, 1), the algorithm should smoothly move from one extreme to the other, so the final result of the update should be something like\nθt+1 = (1− βt+1)θt + βt+1θ̃t+1 . (12)\nThe above reasoning may sound plausible, but is it sound? In this section we construct a forward view for partially trusted interim targets, and then derive an equivalent spanindependent backward view. It turns out the resulting update indeed changes the weights precisely as in (12).\nIn the forward view, the online weights that always trust the latest interim target fully will be denoted θ̃ht to differentiate them from the trusted interim weights θ h t . If we have data up to horizon h but we trust the latest interim target Zh only with degree βh ∈ [0, 1], then the predictions prior to h should update towards Zh only to this degree and for the rest, with degree (1−βh), fall back on earlier interim targets. Similarly we update towards\nZh−1 only as far as this interim target was trusted, so with total degree (1 − βh)βh−1, and then further fall back to Zh−2 with total degree (1 − βh)(1 − βh−1)βh−2, and so on. If we do not trust any of the interim targets observed between the time t of making the prediction and the current horizon h, the predictions should remain wherever they were at time t. This can be achieved by updating the prediction at time t towards the then-current prediction φ>t θ t t with the remaining degree (1 − βh)(1 − βh−1) · · · (1 − βt+1). Note that the different multipliers sum to one:\nβh + (1− βh)βh−1 + (1− βh)(1− βh−1)βh−2 + . . . + (1− βh) · · · (1− βt+1) = 1 .\nThe total forward view for a prediction at time t with a horizon of h is therefore given by\nθht+1 = θ h t + βhαtφt(Z h − φ>t θht ) + (1− βh)βh−1αtφt(Zh−1 − φ>t θht ) + (1− βh)(1− βh−1)βh−2αtφt(Zh−2 − φ>t θht ) ...\n+ (1− βh) · · · (1− βt+2)βt+1αtφt(Zt+1 − φ>t θht ) + (1− βh) · · · (1− βt+2)(1− βt+1)αtφt(φ>t θtt − φ>t θht ) .\n(now grouping terms αtφt(· − φ>t θht ) with total weight equal to one)\n= θht + αtφt ( βhZ h+\n(1− βh)βh−1Zh−1+ (1− βh)(1− βh−1)βh−2Zh−2+\n...\n(1− βh) · · · (1− βt+2)βt+1Zt+1+ (1− βh) · · · (1− βt+2)(1− βt+1)φ>t θtt − φ>t θht )\n= θht + αtφt(Z h t − φ>t θht ) = Ftθ h t + αtφtZ h t , t = 0, . . . , h− 1 ; h = 1, . . . , T, (13)\nwhere Zht . = βhZ h\n+ (1− βh)βh−1Zh−1 + (1− βh)(1− βh−1)βh−2Zh−2\n+ . . .\n+ (1− βh) · · · (1− βt+2)βt+1Zt+1\n+ (1− βh) · · · (1− βt+2)(1− βt+1)φ>t θt = βhZ\nh + (1− βh)Zh−1t , t = 0, . . . , h− 1 ; h = 1, . . . , T , and (14) Ztt . = φ>t θ t t t = 0, . . . , T − 1 . (15)\nTo derive a span-independent variant of algorithm (13), we first identify how a general weight vector θht depends on the combined interim targets and the observed feature vectors by applying the recursive definition in (13) repeatedly, yielding\nθht = Ft−1θ h t−1 + αt−1φt−1Z h t−1 (applying (13) to θ h t ) = Ft−1 ( Ft−2θ h t−2 + αt−2φt−2Z h t−2 ) + αt−1φt−1Z h t−1 (applying (13) to θ h t−1)\n= Ft−1Ft−2θ h t−2 + Ft−1αt−2φt−2Z h t−2 + αt−1φt−1Z h t−1 = Ft−1Ft−2(Ft−3θ h t−3 + αt−3φt−3Z h t−3) + Ft−1αt−2φt−2Z h t−2 + αt−1φt−1Z h t−1\n(applying (13) to θht−2)\n= Ft−1Ft−2Ft−3θ h t−3 + Ft−1Ft−2αt−3φt−3Z h t−3 + Ft−1αt−2φt−2Z h t−2 + αt−1φt−1Z h t−1\n...\n= Ft−1 · · ·F0θt0︸ ︷︷ ︸ . = at−1 +\nt−1∑ k=0 Ft−1 · · ·Fk+1αkφkZhk\n= at−1 + t−1∑ k=0 Ft−1 · · ·Fk+1αkφkZhk . (16)\nThe last step uses the fact that the initial trusted weights θt0 are equal to the initial online weights, such that θt0 = θ̃ t 0 = θ0 for any t, which means at is the same as before. Notice that we have not yet used the definition of Zht in any way; the derivation so far holds for any combined target.\nWe now first examine if we can efficiently go down in the triangle, that is, to get to θh+1h from θ h h:\nθh+1h − θ h h = ( ah−1 + h−1∑ k=0 Fh−1 · · ·Fk+1αkφkZh+1k ) (θh+1h from (16))\n− ( ah−1 +\nh−1∑ k=0 Fh−1 · · ·Fk+1αkφkZhk ) (θhh from (16))\n= h−1∑ k=0 Fh−1 · · ·Fk+1αkφk ( Zh+1k − Z h k ) (merge sums, cancel ah−1)\n= h−1∑ k=0 Fh−1 · · ·Fk+1αkφk ( βh+1Z h+1 + (1− βh+1)Zhk − Zhk )\n(using (14) on Zh+1k )\n= h−1∑ k=0 Fh−1 · · ·Fk+1αkφkβh+1(Zh+1 − Zhk )\n= βh+1 ( h−1∑ k=0 Fh−1 · · ·Fk+1αkφk ) ︸ ︷︷ ︸\n= eh−1\nZh+1 − βh+1 h−1∑ k=0\nFh−1 · · ·Fk+1αkφkZhk︸ ︷︷ ︸ = θhh − ah−1, from (16)\n= βh+1eh−1Z h+1 − βh+1(θhh − ah−1) = βh+1 (ah−1 + eh−1Z h+1)︸ ︷︷ ︸\n= θ̃h+1h , from (9)\n− βh+1θhh\n= βh+1(θ̃ h+1 h − θ h h) (17)\nThus θh+1h can be written as a simple combination of the previous trusted weights θ h h and the interim online weights θ̃h+1h with\nθh+1h = (1− βh+1)θ h h + βh+1θ̃ h+1 h . (18)\nWe can plug this value into the definition of θh+1h+1 to find\nθh+1h+1 = Fhθ h+1 h + αhZ h+1 h φh (using (13))\n= Fh ( (1− βh+1)θhh + βh+1θ̃h+1h ) + αhZ h+1 h φh (using (18))\n= Fh ( (1− βh+1)θhh + βh+1θ̃h+1h ) + αh ( βh+1Z h+1 + (1− βh+1)Zh ) φh .\n(using (14))\n= Fh ( (1− βh+1)θhh + βh+1θ̃h+1h ) + αh ( βh+1Z h+1 + (1− βh+1)φ>h θhh ) φh .\n(using (15))\nNow we group the terms depending on whether they are trusted (multiplied with βh+1) or untrusted (multiplied with (1− βh+1)) to simply further to\nθh+1h+1 = (1− βh+1) ( Fhθ h h + αhφhφ > h θ h h ) ︸ ︷︷ ︸ =θhh , using Fh=I−αhφhφ > h + βh+1 ( Fhθ̃ h+1 h + αhZ h+1φh ) ︸ ︷︷ ︸\n= θ̃h+1h+1, using (7)\n= (1− βh+1)θhh + βh+1θ̃h+1h+1 .\nAll superscripts now match their corresponding subscripts and so we can write down an algorithm that is equivalent to the forward view in the sense that θtt = θt for all t, with\ne−1 . = 0, then et . = et−1 + αtφt(1− φ>t et−1), t = 0, . . . , T − 1, θ̃t+1 . = θ̃t + et ( Zt+1 − Zt ) + αtφt(Z\nt − φ>t θ̃t), t = 0, . . . , T − 1, θt+1 . = θt + βt+1(θ̃t+1 − θt), t = 0, . . . , T − 1,\n(19)\nwhere βt ∈ [0, 1] is the degree of trust we place in Zt. The first two lines compute the online weights, and are equal to the online backward view (11) from the previous section.\nThe last line effectively computes a weighted running average the online weights, according to the sequence {βt}Tt=1.\nAlgorithm (19) subsumes the previous algorithms. If βt = 1, ∀t, then the predictions are equal those of the online algorithm on each step. If βt = 0 for t ∈ {1, . . . , T − 1} and βT = 1, then the predictions are equal those of the offline algorithm. As long as we trust the final outcome, such that βT = 1, then all these algorithms result in exactly the same weights at the end of the episode, and algorithm (19) allows us to be flexible about how much we change the predictions during the episode, without requiring us to commit to either fully online or fully offline updates for the whole episode."
    }, {
      "heading" : "6 Bootstrapping",
      "text" : "So far we have always fully trusted the actual final outcome. All interim targets have been deemed irrelevant by the end of the episode, leaving no effect on the computed final weights. There are cases in which we do not want to discard all interim targets. For instance, consider a stock that crashes down just before the end of the year. Certainly our updated predictions should include the possibility of such a crash, but we may not want to predict it will always crash just before the year’s end. Similarly, suppose it rains on a certain date for which we want to predict the weather. It then seems wasteful to ignore the sunny weather on the days leading up to that date. These are examples of cases in which the interim targets are almost as informative as the final outcome. In some cases, an interim target may even be more informative. For instance, it may be due to a highly-trusted expert that takes into account all possible outcomes from that point in time. Surely, this expert should not be ignored completely in favor of one random final outcome.\nThe general idea of updating predictions using other predictions, such as the interim targets, is called bootstrapping (see, e.g., Sutton and Barto 1998). One way to obtain persistence of interim targets in our final predictions, and to achieve bootstrapping, is to drop the requirement in the previous section that we trust the final outcome fully, and allow βT < 1. The eventual target for our updates is then a weighted average of the interim targets and the final outcome. For instance, if βt = 1/2 for all t ∈ {1, . . . , T} the final updates will place a weight of βT = 1/2 on the final outcome, a weight of (1− βT )βT−1 = 1/4 on the interim target immediate before then, and so on.\nThe notion of trust from the previous section applies a single degree of trust for an interim target uniformly to all prior predictions, but this is not always desirable. Using this definition of trust, if we fully trust an interim target then it replaces all earlier interim targets. However, even if an interim target if fully trusted for the most recent prediction, it may be inherently less trustworthy for earlier predictions. To illustrate this, consider flipping a coin three times and predicting the total number of heads. The possible final outcomes are 0, 1, 2, and 3 heads. If a trusted expert tells us before the first flip that the coin is fair, that is equivalent to observing a trustworthy interim target of Z1 = 1.5. Suppose then the first two flips both result in heads, such that the only remaining possible final outcomes are 2 and 3. If the coin is indeed fair, an interim target of 2.5 would now be trustworthy target for the prediction made after observing two heads. However, we would probably not want to replace the earlier interim target of 1.5 for the first prediction.\nUnfortunately, this is exactly what happens with the algorithm from the previous section. This suggests a different notion of trust, based on the degree of trust we place in an interim target as a stand-in for the expected final outcome rather than the actual (random) outcome. If an interim target precisely matches the expected outcome at that point in time, then later targets can then only be noisier or more specific but not more informative, and it should never be replaced by later targets.\nSuppose, concretely, that we fully trust Zt under this new notion of trust. Then, the update for the prediction made at time t−1 should disregard any targets that arrive later, including even the final outcome. Conversely, if we do not trust Zt at all, then it should leave no trace in the final updates. More generally we can update towards Zt with an intermediate degree of trust ηt ∈ [0, 1]. If the next interim target Zt+1 is trusted with degree ηt+1, we then update our prediction at time t towards it with a total weight of (1 − ηt)ηt+1. The update towards Zt+2 will get a total weight of (1 − ηt)(1 − ηt+1)ηt+2, and so on until we reach either the final outcome or the current data horizon. The latest interim target (and the final outcome) is always trusted fully until we move to the next time horizon. Therefore, at horizon h we always place any remaining weight on Zh and update towards it with total weight (1− ηt)(1− ηt+1) · · · (1− ηh−1).\nThe corresponding total update to the prediction at time t with a current horizon h is then\nθht+1 = θ h t + ηt+1αtφt(Z t+1 − φ>t θht ) + (1− ηt+1)ηt+2αtφt(Zt+2 − φ>t θht ) + (1− ηt+1)(1− ηt+2)ηt+3αtφt(Zt+3 − φ>t θht ) + . . .\n+ (1− ηt+1) · · · (1− ηh−2)ηh−1αtφt(Zh−1 − φ>t θht ) + (1− ηt+1) · · · (1− ηh−1)αtφt(Zh − φ>t θht )\n. = θht + αtφt ( Zht − φ>t θht ) . (20)\nwhere we have grouped the updates into a single update towards a combined target Zht , just as in the previous section. This update is perhaps more familiar when we change the notation slightly. For all t, we define λt . = 1− ηt such that λt essentially specifies to what degree we distrust Zt. The combined target is then defined as\nZht . = (1− λt+1)Zt+1\n+ λt+1(1− λt+2)Zt+2\n. . .\n+ λt+1 · · ·λh−2(1− λh−1)Zh−1 + λt+1 · · ·λh−1Zh . (21)\nThis target Zht is known as a λ-return (Sutton and Barto 1998). The version that truncates at the current horizon h was first proposed by van Seijen and Sutton (2014). The total set of updates is\nθt0 . = θ0 , t = 0, . . . , T ;\nθht+1 . = θht + αtφt(Z h t − φ>t θht ) , (22)\n. = Ftθ h t + αtφtZ h t , t = 0, . . . , h− 1 ; h = 1, . . . , T .\nIf we ultimately distrust all interim targets, then λt = 1 for all t and Z h t = Z h. The algorithm then reduces to the online algorithm from Section 4. Otherwise, at least some interim targets persist and contribute to the final weights. At the other extreme, if we trust all interim targets, then λt = 0 for all t and Z h t = Z\nt+1. Then, the updates reduce to single-step updates that only use the immediate next interim target. In that case each update depends only on the immediate next time step and we can drop the superscript h and the forward view (22) reduces to an efficient span-independent algorithm\nθt+1 . = θt + αtφt(Z t+1 − φ>t θt) , t = 0, . . . , T .\nApart from this special case, the forward view (22) is computationally inefficient and we desire an efficient span-independent algorithm to get from θhh to θ h+1 h+1. In the previous section we derived how a weight vector depends on any sequence of combined targets Zht , independent on the definition of those targets. We repeat the result of that derivation, as first given in (16), here for clarity:\nθht = at−1 + t−1∑ k=0 Fh−1 · · ·Fk+1αkφkZhk t = 0, . . . , h− 1 ; h = 1, . . . , T .\nBecause this equation holds regardless of the definition of Zht , we can apply it to the current algorithm. In particular we use it to try to find an efficient algorithm to go from θhh to θ h+1 h . If this is possible, we can then use the update (22) to go from θ h+1 h to θ h+1 h+1. We start by writing out the difference as\nθh+1h − θ h h = h−1∑ k=0 Fh−1 · · ·Fk+1αkφkZh+1k − h−1∑ k=0 Fh−1 · · ·Fk+1αtφkZhk\n(ah−1 cancels)\n= h−1∑ k=0 Fh−1 · · ·Fk+1αkφk ( Zh+1k − Z h k ) . (23)\nThe combined targets Zh+1k and Z h k share many terms: going back to (21) we can see that all interim targets up to Zh−1 will have the exact same multipliers. These terms cancel, and the remaining difference is given by\nZh+1k − Z h k = λk+1 · · ·λh−1(1− λh)Zh + λk+1 · · ·λhZh+1︸ ︷︷ ︸\ndue to Zh+1k\n−λk+1 · · ·λh−1Zh︸ ︷︷ ︸ due to Zhk\n= λk+1 · · ·λh ( Zh+1 − Zh ) . (24)\nWe can then continue from (23) with\nθh+1h − θ h h = h−1∑ k=0 Fh−1 · · ·Fk+1αkφk ( Zh+1k − Z h k )\n= h−1∑ k=0 Fh−1 · · ·Fk+1αkφkλk+1 · · ·λh(Zh+1 − Zh) (using (24))\n= λh ( h−1∑ k=0 Fh−1 · · ·Fk+1λk+1 · · ·λh−1αkφk ) ︸ ︷︷ ︸\n. = eh−1\n(Zh+1 − Zh)\n= λheh−1(Z h+1 − Zh) . (25)\nWe again encounter the TD error Zh+1 − Zh and, more importantly, a new trace vector et that can be updated efficiently with\net . = t∑ k=0 Ft · · ·Fk+1λk+1 · · ·λtαkφk\n= t−1∑ k=0 Ft · · ·Fk+1λk+1 · · ·λtαkφk + αtφt\n= λtFt t−1∑ k=0\nFt−1 · · ·Fk+1λk+1 · · ·λt−1αkφk︸ ︷︷ ︸ et−1 + αtφt\n= λtFtet−1 + αtφt (26) = λtet−1 + αtφt(1− λtφ>t et−1) .\nThis trace is similar to the one we encountered before, but with the difference that the value of the vector decays towards zero by multiplication with λt on each step. Predictions made prior to a fully trusted interim target (for which λt = 0) will never be affected by later interim targets because the trace vector is set to zero. The extent to which the trace extends backward in time depends on the extent to which we have not yet trusted the corresponding interim targets.\nWe now combine the derived update from θhh to θ h+1 h with the update from θ h+1 h to\nθh+1h+1 to derive a single efficient update, given by\nθh+1h+1 = Fhθ h+1 h + αhφhZ h+1 (using (22)) = Fh ( θhh + λheh−1(Z h+1 − Zh) ) + αhφhZ h+1 (using (25))\n= Fhθ h h + λhFheh−1(Z h+1 − Zh) + αhφhZh+1 = Fhθ h h + (eh − αhφh) ( Zh+1 − Zh ) + αhφhZ h+1 (using λhFheh−1 = eh − αhφh, from (26)) = Fhθ h h + eh ( Zh+1 − Zh ) + αhφhZ h\n= (I− αhφhφ>h )θhh + eh ( Zh+1 − Zh ) + αhφhZ h\n= θhh + eh ( Zh+1 − Zh ) + αhφh ( Zh − φ>h θhh ) .\nThis concludes our derivation because the value of θh+1h+1 is now defined fully in terms of the previous weights on the diagonal θhh and other quantities that are either directly\navailable upon reaching our new data horizon h+ 1 or, in the case of the trace vector, can be computed with constant O(n) computation per step. The span-independent algorithm with persistent interim targets is\ne−1 . = 0, then et . = λtet−1 + αtφt(1− λtφ>t et−1), t = 1, . . . , T−1, θt+1 . = θt + et ( Zt+1 − Zt ) + αtφt(Z t − φ>t θt) t = 0, . . . , T−1. (27)\nCompared to the online backward view (11), the only difference is the appearance of λt in the update of the trace. If λt = 1 for all t, we regain the online backward view precisely, demonstrating that the new algorithm is strictly more general. In contrast to the averaging backward view (19), from the previous section, we see that for the notion of trust corresponding to λ we do not need to maintain separate online and trusted weights. Instead, the degree of trust is used to scale the trace vector et down accordingly. If λt < 1 for any t ∈ {1, . . . , T}, the corresponding interim targets have a lasting effect on the final weight vector and therefore for the first time we may obtain predictions that differ not just during the episode but also at its end."
    }, {
      "heading" : "7 Combining two notions of trust and the emergence",
      "text" : "of averaged TD(λ)\nAlgorithm (27) is a strict generalization of the online algorithm (11), but it does not subsume the offline algorithm (6), or the averaging algorithm (19) that switches smoothly between online and offline updates. In this section, we combine the ideas from the last two sections to arrive at an algorithm that generalizes and subsumes all previous algorithms, thereby unifying all that came before into a single, general-purpose algorithm.\nAn offline version of the TD(λ) algorithm can be obtained by using the online algorithm in (27) to update an online weight vector θ̃t and then defining the trusted weight vector θt to remain equal to the initial weights until the last step, at which time we replace them with the online weights. An algorithm that switches smoothly between the offline and online cases can then be obtained similar to before, resulting in\ne−1 . = 0, then et . = λtet−1 + αtφt(1− λtφ>t et−1), t = 0, . . . , T − 1, θ̃t+1 . = θ̃t + et ( Zt+1 − Zt ) + αtφt(Z\nt − φ>t θ̃t), t = 0, . . . , T − 1, θt+1 . = θt + βt+1(θ̃t+1 − θt), t = 0, . . . , T − 1.\n(28)\nThe first two lines are the online algorithm (27), from the previous section. The last line is equal to the last line in the unified algorithm without persistency of interim targets, as given in (19), but now using the online weights that use persistent interim targets weighted according to λ-returns, as computed in the first two lines. When λt = 1 for all t we regain the averaging algorithm (19) without persistent interim targets. When βt = 1 for all t we regain the online algorithm (27) with persistent interim targets. So, we have again successfully unified all previously seemingly difference approaches to trust and have arrived at a single general algorithm that subsumes all that came before.\nThe merits of λ-returns are well known (Sutton 1988; Sutton and Barto 1998) but the β-weighting of the online weights is novel to this paper, and it is appropriate to discuss it in a little more detail. So far we have considered only a single episode, but a major\npotential benefit of including β appears when we consider multiple episodes. For clarity, consider the extreme case where all episodes last only a single step. Then λ plays no role because there is no interim within each episode; there is only a beginning and an end. If we use m to enumerate episodes, such that Zm is the true outcome of episode m, then the updates for single-step episodes are\nθ̃m+1 = θ̃m + αm(Zm − φ>mθ̃m)φm , θm+1 = θm + βm+1(θ̃m+1 − θm) ,\nwhere we used the fact that the final weights of episode n are the first weights of episode m + 1. Using β, we can do something here that we cannot do with λ alone: we can weight the relative impact of different episodes. For instance, we can choose to keep the weights and the predictions stationary over multiple episodes, by setting βm = 0, to reduce the impact of the final outcomes observed in those episodes on our predictions. Another possibility is to decay the trust, for instance according to βm = 1 m , such that we trust the outcome of the first episode fully (β1 = 1) and then reduce the trust for each subsequent episode. Such a choice of β makes sense if we view the trust we place in outcomes as being relative to the trust we place in the predictions we already have. As our predictions improve over time, the outcomes become relatively less trustworthy. With this definition of trust, the trusted weights are the average of the weights of all previous episodes: θm = 1 m ∑m i=0 θ̃i. This specific algorithm is interesting because the predictions according to the averages θm are known to converge to the optimal predictions faster than the predictions according to any sequence of online weights θ̃m (Polyak and Juditsky 1992; Bach and Moulines 2013). This shows that the notion of trust as provided by β gives us something that cannot be obtained with λ alone. To our knowledge, algorithm (28) is the first to generalize this idea of averaging online weights, in a principled fashion, to long-term predictions."
    }, {
      "heading" : "8 Generalizing to cumulative returns and soft termi-",
      "text" : "nations\nIn this section, we discuss how to extend our algorithms to handle soft terminations and cumulative returns. Both extensions generalize the episodic final-outcome setting considered above, and the algorithm we derive in this section will subsume all previous algorithms.\nOften, we want to predict the cumulation of a signal {Xt}Tt=0 rather than a single final outcome. In the episodic setting, with termination at time T , we then aim to predict\nZTt . = Xt+1 +Xt+2 + . . .+XT ,\nwhere, as before, t is the time of the prediction. In contrast to final outcomes, these cumulative outcomes depend on the time step of the prediction because at later time steps there will be less signal left to accumulate before the episode ends. We call this time-dependent outcome the cumulative return.\nWe may wish to update our predictions online, before observing the full cumulative return. To do so, we need to define interim targets to temporarily take the place of the\nactual return. An interim target Zht up to a horizon h < T should in any case include the part of the signal that was already observed. In addition, we introduce a residual prediction Ph that stands in for the unseen part of the signal, from horizon h to the end of the episode T . The full interim target at time t up to horizon h is then\nZht = Xt+1 +Xt+2 + . . .+Xh + Ph .\nThe residual prediction Ph may for instance be given by an external expert or by our own predictions at time h. For now we are agnostic to its origin and consider the general case. In any case, we define PT = 0 because at termination there is no remaining signal left to predict. If all signals except the one coinciding with termination are zero, we regain the final-outcome setting where the last signal XT = Z takes the role of the final outcome. If we further define the interim target at each horizon to be equal to the residual prediction at that horizon, such that Zh = Ph, we are back in the online final-outcome setting. Therefore, cumulative returns are strictly more general than final outcomes.\nSo far, we have considered episodic predictions where each episode ends with a single final outcome that we wish to predict. The algorithms extend naturally to multiple episodes by using the final weights of the episode as the initial weights of the next episode. However, some predictive questions do not fit nicely into this strictly episodic format because they are better thought of as terminating softly on each time step.\nA soft termination is a conceptual and potentially partial termination of the signal. Such a soft termination could represent a probability of termination, for instance when we want to take into account the probability of a robot breaking while learning in a simulation in which it never actually does. Or the soft termination could represent a desire to trade off the imminence and the magnitude of a signal, for instance when we do not just want more money rather than less, but we also want it sooner rather than later. In both cases the prediction is about a diminishing version of the ‘raw’ signal (e.g., money). Here, we are not concerned with the potential reasons for using soft terminations and consider the general case where the termination of the prediction can vary per time step and be anywhere between full continuation and full termination. Soft terminations allow us to ask more general predictive questions, and even to simultaneously consider multiple predictions that may resolve at different times.\nSoft terminations can be modeled by using a continuation parameter γt ∈ [0, 1] to denote the amount of termination of our prediction upon reaching time t. This quantity is often called a discount factor, because it discounts the impact of later outcomes compared to earlier ones. If γt = 1, no termination happens at time t; if γt = 0, the prediction terminates fully, even if the trajectory may continue. We consider general sequences of γt and only require that eventually every prediction resolves completely, potentially asymptotically, such that ∏∞ i=t γi = 0 for all t. The episodic setting considered in the previous sections is a special case where γt = 0 only if t = T is the final step of the episode, and γt = 1 on all other steps.\nWe are now almost ready to formulate the target for a prediction about a discounted cumulative signal. In order to maintain full generality and compatibility with previous sections, we immediately include persistency of the residual predictions according to a sequence of {λt}∞t=0 and trust of the resulting combined targets according to a sequence {βt}∞t=0. For any horizon h > t, the combined interim target for the prediction at time\nt should include at least the immediate next signal Xt+1. Beyond this first signal we continue with γt+1 and observe the residual prediction Pt+1. We trust this prediction with degree 1 − λt+1 as a stand-in for the expected cumulative discounted return, and so its total multiplier is γt+1(1 − λt+1). If we trust this prediction fully, so that λt = 0, there is no need to continue further. Otherwise, we continue for as much as we have not yet terminated according to both γt+1 and λt+1, so that the next signal Xt+2 gets a total weight of γt+1λt+1. This line of reasoning then continues until we reach the horizon of our current data at time h, at which point we place any remaining trust on the most recent residual prediction Ph. All together, this gives us a combined target for the prediction at time t with data up to time h > t:\nZht . = Xt+1 + γt+1(1− λt+1)Pt+1\n+ γt+1λt+1(Xt+2 + γt+2(1− λt+2)Pt+2) + γt+1λt+1γt+2λt+2(Xt+3 + γt+3(1− λt+3)Pt+3) ...\n+ γt+1 · · · γh−2λt+1 · · ·λh−2(Xh−1 + γh−1(1− λh−1)Ph−1) + γt+1 · · · γh−1λt+1 · · ·λh−1(Xh + γhPh) . (29)\nThis can be written recursively as\nZht . = Xt+1 + γt+1 ( (1− λt+1)Pt+1 + λt+1Zht+1 ) , (30)\nand Ztt . = Pt , (31)\nEach of these interim targets is trusted according to the trust associated with its horizon, βh. Recall from Section 5 that this trust propagates backward. If we trust the last target Zht fully, there is no need to consider earlier targets. Otherwise, we multiply Z h t with its associated trust βh and continue to the previous target with degree (1 − βh). This then continues until either we find a target we fully trust, or we reach the then-current predictions φ>t θ t t at time t, when we made the prediction we are currently updating. Our final interim targets are then given by\nZ̄ht . = βhZ h t\n+ (1− βh)βh−1Zh−1t + (1− βh)(1− βh−1)βh−2Zh−2t ...\n+ (1− βh) · · · (1− βt+2)βt+1Zt+1t + (1− βh) · · · (1− βt+1)φ>t θtt\n= βhZ h t + (1− βh)Z̄h−1t (32)\nand Z̄tt . = φ>t θ t t (33)\nwhere θtt are the trusted weights for time t that we will compute. The forward-viewing update is then given by\nθt0 . = θ0 t = 0, . . . , T\nθht+1 . = θht + αtφt(Z̄ h t − φ>t θht ) , t = 0, . . . h− 1 ; h = 0, . . . , T , (34)\n. = Ftθ h t + αtφtZ̄ h t , t = 0, . . . h− 1 ; h = 0, . . . , T .\nTo find an efficient backward view, again we can start by writing down explicitly the value of θht . Following a derivation identical to the one for when we first consider trusted weights, as shown in (16) but with the complex target Z̄hk replacing Z h k , we obtain\nθht = at−1 + t−1∑ k=0 Ft−1 · · ·Fk+1αkφkZ̄hk t = 0, . . . , h− 1 ; h = 1, . . . , T . (35)\nAs before, we can apply this to both θh+1h and θ h h to find the difference\nθh+1h − θ h h = h−1∑ k=0 Fh−1 · · ·Fk+1αkφkZ̄h+1k − h−1∑ k=0 Fh−1 · · ·Fk+1αkφkZ̄hk\n(ah−1 cancels)\n= h−1∑ k=0 Fh−1 · · ·Fk+1αkφk ( Z̄h+1k − Z̄ h k )\n= h−1∑ k=0 Fh−1 · · ·Fk+1αkφk ( βh+1Z h+1 k + (1− βh+1)Z̄ h k − Z̄hk ) (from (32))\n= h−1∑ k=0 Fh−1 · · ·Fk+1αkφkβh+1 ( Zh+1k − Z̄ h k )\n= βh+1 h−1∑ k=0 Fh−1 · · ·Fk+1αkφkZh+1k − βh+1 h−1∑ k=0\nFh−1 · · ·Fk+1αkφkZ̄hk︸ ︷︷ ︸ . = θhh − ah−1, from (35)\n= βh+1 ( ah−1 +\nh−1∑ k=0 Fh−1 · · ·Fk+1αkφkZh+1k ) − βh+1θhh . (36)\nThe first part of this result, within the brackets, looks familiar: notice the similarity to (35). The term can be interpreted as the intermediate result θ̃h+1h of a forward view with targets Zht , as defined in (30), and updates defined by\nθ̃t0 . = θ̃0 t = 0, . . . , T θ̃ht+1 . = θ̃ht + αtφt(Z h t − φ>t θ̃ht ) , t = 0, . . . h− 1 ; h = 0, . . . , T .\n(37)\nThis is an online algorithm, comparable to the algorithm for λ-returns derived before, but implicitly including cumulative returns and discounting through the definition of Zht . The complete forward view (34) can then be interpreted as switching smoothly between this online algorithm and an offline algorithm that in the extreme can delay updating\nthe predictions indefinitely. Analogous to the interim weights shown in (35), the interim weights of this online algorithm satisfy\nθ̃ht = at−1 + t−1∑ k=0 Ft−1 · · ·Fk+1αkφkZhk t = 0, . . . , h ; h = 1, . . . , T . (38)\nWe can then continue our derivation from (36) with\nθh+1h − θ h h = βh+1 ( ah−1 + h−1∑ k=0 Fh−1 · · ·Fk+1αkφkZh+1k ) ︸ ︷︷ ︸\n= θ̃h+1h , using (38)\n− βh+1θhh\n= βh+1(θ̃ h+1 h − θ h h) .\nThis implies that if we have θhh and θ̃ h+1 h we can then compute θ h+1 h efficiently using\nθh+1h = (1− βh+1)θ h h + βh+1θ̃ h+1 h . (39)\nFor now, we ignore the question of how to obtain θ̃h+1h and first focus on how to go from θh+1h to θ h+1 h+1 and, resultingly, from θ h h to θ h+1 h+1. We can now derive\nθh+1h+1 = Fhθ h+1 h + αhφhZ̄ h+1 h (using (34))\n= Fh ( βh+1θ̃ h+1 h + (1− βh+1)θ h h ) + αhφhZ̄ h+1 h (using (39))\n= Fh ( βh+1θ̃ h+1 h + (1− βh+1)θ h h ) + αhφh ( βh+1Z h+1 h + (1− βh+1)Z̄ h h ) (using (32))\n= Fh ( βh+1θ̃ h+1 h + (1− βh+1)θ h h ) + αhφh ( βh+1Z h+1 h + (1− βh+1)φ > h θ h h ) (using (33))\n= βh+1 ( Fhθ̃ h+1 h + αhφhZ h+1 h ) ︸ ︷︷ ︸\n. = θ̃h+1h+1, using (37)\n+ (1− βh+1) ( Fhθ h h + αhφhφ > h θ h h ) ︸ ︷︷ ︸\n= θhh (regrouping)\n= βh+1θ̃ h+1 h+1 + (1− βh+1)θ h h .\nWe see that θ̃h+1h is not needed and instead we can use θ̃ h+1 h+1. Therefore, if θ̃ h+1 h+1 can be computed with constant computation independent of span, then θh+1h+1 can be computed from θhh efficiently as well, using\nθh+1h+1 = (1− βh+1)θ h h + βh+1θ̃ h+1 h+1 . (40)\nIt now remains to be shown that the online weights θ̃h+1h+1 can be computed efficiently.\nWe start with the difference between θ̃h+1h and θ̃ h h for which we can use (38) to obtain\nθ̃h+1h − θ̃ h h = h−1∑ k=0 Fh−1 · · ·Fk+1αkφk ( Zh+1k − Z h k ) . (41)\nWe can find the difference Zh+1k −Zhk from the definition in (29). All terms not involving Ph, Xh+1 or Ph+1 cancel, leaving us with\nZh+1k − Z h k = γk+1 · · · γhλk+1 · · ·λh−1 ((1− λh)Ph + λhXh+1 + λhγh+1Ph+1)︸ ︷︷ ︸\nfrom Zh+1k\n− γk+1 · · · γhλk+1 · · ·λh−1Ph︸ ︷︷ ︸ from Zhk\n= γk+1 · · · γhλk+1 · · ·λh−1 (−λhPh + λhXh+1 + λhγh+1Ph+1) = γk+1 · · · γhλk+1 · · ·λh (Xh+1 + γh+1Ph+1 − Ph) .\nHere we see the emergence of a general form of the classical temporal-difference error for cumulative discounted returns: δh . = Xh+1 + γh+1Ph+1 − Ph.3 Using this, we can now continue from (41) with\nθ̃h+1h − θ̃ h h = h−1∑ k=0 Fh−1 · · ·Fk+1αkφk ( Zh+1k − Z h k ) =\nh−1∑ k=0 Fh−1 · · ·Fk+1αkφkγk+1 · · · γhλk+1 · · ·λhδh\n= γhλh ( h−1∑ k=0 γk+1 · · · γh−1λk+1 · · ·λh−1Fh−1 · · ·Fk+1αkφk ) ︸ ︷︷ ︸\n. = eh−1\nδh\n= γhλheh−1δh , (42)\nwhere, similar to before, the trace vector et can be updated recursively according to\net . = t∑ k=0 γk+1 · · · γtλk+1 · · ·λtFt · · ·Fk+1αkφk\n= t−1∑ k=0 γk+1 · · · γtλk+1 · · ·λtFt · · ·Fk+1αkφk + αtφt\n3If we use our current predictions as interim targets, the TD error is δt = Xt+1+γt+1φ>t+1θt−φ>t θt−1. This TD error uses the weights at two consecutive time steps and is therefore slightlt different from the classic TD error defined, using only the current weights θt, as δt = Xt+1 + γt+1φ>t+1θt − φ>t θt. The difference is important to achieve exact equivalence, although it is also possible to rewrite the new algorithms to use a more standard TD error.\n= γtλtFt t−1∑ k=0 γk+1 · · · γt−1λk+1 · · ·λt−1Ft−1 · · ·Fk+1αkφk + αtφt\n= γtλtFtet−1 + αtφt (43) = γtλtet−1 + αtφt(1− γtλtφ>t et−1) .\nThe trace now decays according to both λ and γ. Using this trace vector, we can compute θ̃h+1h efficiently from θ̃ h h using (42). We now combine this with the final step to θ̃ h+1 h+1 from θ̃h+1h and derive\nθ̃h+1h+1 = Fhθ̃ h+1 h + αhφhZ h+1 h (using (37))\n= Fhθ̃ h+1 h + αhφh (Xh+1 + γh+1Ph+1) (using (30), (31))\n= Fh ( θ̃hh + γhλheh−1 (Xh+1 + γh+1Ph+1 − Ph) ) + αhφh (Xh+1 + γh+1Ph+1)\n(using (42))\n= Fhθ̃ h h + γhλhFheh−1 (Xh+1 + γh+1Ph+1 − Ph) + αhφh (Xh+1 + γh+1Ph+1) = Fhθ̃ h h + (eh − αhφh) (Xh+1 + γh+1Ph+1 − Ph) + αhφh (Xh+1 + γh+1Ph+1) (using γhλhFheh−1 = eh − αhφh, from (43)) = Fhθ̃ h h + eh (Xh+1 + γh+1Ph+1 − Ph) + αhφhPh = (I− αhφhφ>h )θ̃hh + eh (Xh+1 + γh+1Ph+1 − Ph) + αhφhPh\n= θ̃hh + eh (Xh+1 + γh+1Ph+1 − Ph) + αhφh ( Ph − φ>h θ̃hh ) = θ̃hh + ehδh + αhφh ( Ph − φ>h θ̃hh ) .\nAll weights again have matching sub- and superscripts and an equivalent TD algorithm, in the sense that θ̃t = θ̃ t t and θt = θ t t, for the fully general case including cumulative discounted returns is given by\ne−1 . = 0, then et . = γtλtet−1 + αtφt(1− γtλtφ>t et−1), t = 0, . . . , T − 1, θ̃t+1 . = θ̃t + etδt + αtφt(Pt − φ>t θ̃t), t = 0, . . . , T − 1, θt+1 . = θt + βt+1(θ̃t+1 − θt), t = 0, . . . , T − 1.\n(44)\nThe first two lines constitute the online algorithm we just derived. The last line is from (40) and extends this algorithm to include smooth switching between offline and online updates. If βt = 1 for all t, the algorithm reduces to a variant of TD(λ) known as true online TD(λ) (van Seijen and Sutton 2014), but extended to include general, potentially non-constant, sequences of {αt}, {γt} and {λt}. The extension to averaging according to βt is new to this paper.\nSoft termination generalizes the episodic setting we considered previously. This means that as far as the learning update is concerned, we do not have to treat steps on which the process actually terminates and restarts for a new episode in any special way. To see how this works, we first renumber the time steps on consecutive episodes: if the first episode ends at time T , the initial time step of the second episode will be taken to be T rather than 0. If the second episode lasts T ′ steps, the third episode is then taken to\nbegin on T + T ′, and so on. Together with the requirement that γT = 0 on every actual termination, this is sufficient to get updates that are completely equivalent to treating the subsequent episodes completely separately. Notice that the update on termination at some time T , and resulting in θT , uses the residual prediction on termination PT only when multiplied with γT . Previously we required that PT = 0 because there is no further signal to predict. This is now no longer necessary, because γT = 0 already fulfills the requirement that γTPT = 0. If for instance we use our current predictions, such that Pt+1 . = φ>t+1θt for all t, we can simply keep the update as is even though PT will then be the prediction for the cumulative return of the next episode, because φT is now defined to be its first feature vector. Therefore, both hard and soft termination can be handled seamlessly using algorithm (44), and that will be our final, general algorithm."
    }, {
      "heading" : "9 Convergence Analysis",
      "text" : "The algorithm (44) differs from related earlier algorithms such as TD(λ) in a few subtle but important ways. The most notable differences are the updates to the traces and the averaging due to βt. Known results on convergence therefore do not automatically transfer to this new algorithm and it is appropriate to take a moment to analyze it.\nThe convergence of the trusted weights θ depends on the convergence of the online weights θ̃ and so we must investigate these jointly. The online weights, in turn, depend on the sequences of parameters and residual predictions that are supplied. We want our analysis to be general, which means we want to be able to handle general sequences of discounts {γt}∞t=1, persistency parameters, {λt}∞t=1, and residual predictions {Pt}∞t=1. Naturally, if any of these can change completely arbitrarily, we can have no hope of converging to any predeterminable solution. Therefore, as in Sutton et al. (2014), we allow the features, discounts and persistency parameters to be stationary functions of an underlying unobserved state, such that φt . = φ(St), γt . = γ(St) and λt . = λ(St) for some fixed functions φ : S → Rn, γ : S → [0, 1] and λ : S → [0, 1], where S is a state space and St ∈ S is the, unobserved, state of the world at time t. We assume there is a steady-state distribution over these states such that all expectations used below are well-defined with respect to a distribution over states defined by limt→∞ Pr(St = s). This setting generalizes the more standard approach where γt = γ and λt = λ are constants, because now these parameters can still change over time, but it avoids the possibility of arbitrary non-stationarity that would ruin convergence.\nWe first consider convergence when the residual predictions are also due to a fixed function of state, for instance because they are due to otherwise stationary experts or oracles. Theorem 1. Let Xt . = X(St), φt . = φ(St), γt . = γ(St), λt . = λ(St) and Pt . = P (St) all be fixed functions of (unobserved) states St ∈ S, with a stable steady-state distribution d. Then, if ∑∞ t=0 αt = ∑∞ t=0 βt = ∞, and ∑∞ t=0 α 2 t < ∞, algorithm (44) converges almost surely to the fixed-point solution\nθ∗ . = E[φtφt]−1 E[Z∞t φt] .\nProof. We start by analyzing the online weights θ̃t. Because of the equivalence of the forward and backward views, we can investigate the forward view, which is easier to\nanalyze. In other words, instead of investigating limt→∞ θ̃t as updated through (44), we investigate limt→∞ θ̃ t t as updated through (37). By construction, the end result is exactly the same. The asymptotic forward view as the horizon goes to infinity is\nθ̃∞t+1 = θ̃ ∞ t + αtφt(Z ∞ t − φ>t θ̃∞t ) , t = 0, . . . ,\nwhere Z∞t . = lim h→∞ Zht , t = 0, . . . .\nBecause Zht does not depend on the weights, this is a standard stochastic gradient-descent update θ̃t+1 = θ̃t − αt∇θ̃l(θ̃)|θ̃t on the quadratic loss function\nl(θ̃) . = E [ (Z∞t − φ>t θ̃)2 ] .\nIf the step sizes are suitably chosen, for instance such that ∑∞ t=0 αt =∞ and ∑∞ t=0 α 2 t <∞ (Robbins and Monro 1951), and if the means and variances of Z∞t and φt are well-defined and bounded for all t, this update converges to the fixed-point solution θ∗ that minimizes the quadratic loss (cf. Kushner and Yin 2003), such that\nlim t→∞\nθ̃t = θ∗ . = E [ φtφ > t ]−1 E[φtZ∞t ] . It is straightforward to see that θt will have the same limit; it suffices to have ∑∞ t=0 βt = ∞.\nAlthough convergence is already guaranteed when βt = 1 for all t, recent work has shown that for similar stochastic gradient algorithms the optimal rate of convergence is attained if βt decreases much faster than αt, specifically when βt = O(t\n−1) while αt = α for some constant α (Bach and Moulines 2013). More generally, it seems likely that convergence also holds if ∑∞ t=0 β 2 t <∞ and ∑∞ t=0 α 2 t =∞. The observation that βt should perhaps decrease over time for faster learning may seem at odds with our introduction of this parameter as a degree of trust. However, these two views are quite compatible if we consider βt to be the degree of trust we place in the online updates relative to the trust we place in our current predictions due to the trusted weights. When the trust in the predictions increases over time, the relative trust in the inherently noisy online targets should then decrease.\nAlthough Theorem 1 is already fairly general, it does not cover the important case when the residual predictions additionally depend on the weights we are updating. It makes sense to use the predictions we trust most and therefore we now consider what happens when Pt . = φ>t θt−1. Notice that we have to use θt−1 rather than θt, because Pt is used in the computation of θt and so the latter is not yet available when we compute Pt. The analysis of this case is more complex than the previous one, because Pt is no longer a constant function of state. This means the update is no longer a standard gradient-descent update on a quadratic loss, because the target Zht for the online weights itself depends on the trusted weights that we are simultaneously updating.\nThe results by Bach and Moulines (2013) on stochastic gradient descent indicate that perhaps the most interesting case is where βt decreases faster than αt, such that limt→∞ βt/αt = 0. This suggests an analysis on two time scales is appropriate.\nTheorem 2. Let Xt . = X(St), φt . = φ(St), γt . = γ(St) and λt . = λ(St) all be fixed bounded functions of (unobserved) states St ∈ S, with a stable steady-state distribution d. Define Pt . = φ>t θt−1. Then, if ∑∞ t=0 αt = ∑∞ t=0 βt = ∞, ∑∞ t=0 α 2 t < ∞, and limt→∞ βt αt\n= 0, algorithm (44) converges almost surely to the TD fixed-point solution θ∗ that minimizes the mean-squared projected Bellman error (Sutton, Szepesvári, and Maei 2008; Sutton et al. 2009), such that\nE [ (Zt(θ∗)− φ>t θ∗)φ>t ] E[φtφt]−1 E [ φt(Zt(θ∗)− φ>t θ∗) ] = 0 , (45)\nwhere Zt(θ) . = Xt+1 + γt+1(1− λt+1)φ>t θ + Zt(θ) , ∀θ .\nProof. In two-time-scale analyses, we are allowed to analyze the faster updates as if the slower updates have stopped. This means that in analyzing the updates to the online weights θ̃t, we can assume the trusted weights θt are constant to analyze where θ̃t converges towards as a function of the stationary θt. On the other hand, when we analyze the slower updates to the trusted weights θt we are allowed to assume the faster updates to the online weights converge completely between each two steps. For more detail on analyzing stochastic approximations on two times scales, we refer to Borkar (1997), Borkar (2008), Kushner and Yin (2003), and Konda and Tsitsiklis (2004).\nWe first analyze the convergence of the faster updates to the online weights, where we can assume that the trusted weights are stationary at some value θ. Then, using Pt . = φ>t θ, the targets for the updates of the forward view are\nZtt (θ) = φ > t θ , Zht (θ) = Xt+1 + γt+1(1− λt+1)φ>t θ + γt+1λt+1Zht+1(θ) ,\nwhere we have extended the notation slightly to make the dependence of Zht (θ) on θ explicit. Notice that the residual predictions on each time step depend on the same stationary trusted weights θ. Because of the assumed stationarity of θ, the updates to the weights θ̃ht of the forward view can again be considered standard stochastic gradient updates and therefore these weights converge towards the fixed point θ̃∗(θ), where again we make the dependence on θ explicit, defined by\nθ̃∗(θ) = E [ φtφ > t ]−1 E[φtZ∞t (θ)] , where Z∞t (θ) = limh→∞ Z h t (θ) denotes the limit of the target of the update as the horizon grows to infinity. In an episodic setting, Z∞t = Z T t for all t < T , where T denotes the first\ntermination after time k. More in general, Z∞t is always well-defined because we require∏∞ t=0 γt = 0. For the analysis of the slower updates to θt, we can now assume the faster time scale has already converged to its fixed point θ̃∗(θt) for the current weights. Therefore, we analyze the update\nθt+1 = θt + βt+1(θ̃∗(θt)− θt) = θt + βt+1(E [ φkφ > k ]−1 E[φkZ∞k (θt)]− θt) .\nThis is a stochastic-approximation update that, under the conditions that ∑∞ t=0 βt = ∞\nand ∑∞ t=0 β 2 t <∞, converges almost surely to the fixed point θ∗ that satisfies\nθ∗ = E [ φtφ > t ]−1 E[φtZ∞t (θ∗)] . If we multiply both sides with E [ φtφ > t ] , this implies that E [ φtφ > t θ∗ ] = E[φtZ∞t (θ∗)] and\ntherefore, by moving both terms to the same side and then multiplying with E [ φtφ > t ]−1 ,\nE [ φtφ > t ]−1 E[φt(Z∞t (θ∗)− φ>t θ∗)] = 0 . It follows immediately that θ∗ minimizes the mean-squared projected Bellman error completely to zero, as desired."
    }, {
      "heading" : "10 Discussion",
      "text" : "In this paper, we have considered how to answer predictive questions with algorithms that use constant computation per time step that is proportional to the number of learned weights, and that is independent of the span of the prediction. We considered both final and cumulative outcomes, under online and offline updating, with and without persistency of the residual predictions we encounter during an episode, and with hard and soft termination. In the end, we obtained a single general algorithm that can be used for all these different predictive questions, which is shown in (44). This algorithm is guaranteed to be convergent under typical, fairly mild, technical conditions.\nSome extensions remain for future work. In particular, we have not considered how different policies of behavior can influence our predictions, and as a result have not talked about the problem of control in which the goal is to find the optimal policy for a given (reward) signal. Our analysis already extends naturally to the prediction of action values, from which control policies can be easily distilled. Then, using a form of policy iteration (Bellman 1957; Howard 1960), we can repeatedly switch between estimating and improving the policy to tackle the problem of optimal control. However, to properly and fully include adaptable policies, we would in addition need to carefully consider the problem of learning off-policy, about action-selection policies that differ from the one used to generate the data (Sutton and Barto 1998). This is consistent but orthogonal to the ideas outlined in this paper, and such off-policy predictions (including those about the greedy and, ultimately, optimal policy) are learnable through a proper use of rejection sampling, as in Q-learning, or importance sampling (Precup, Sutton, and Singh 2000; Precup and Sutton 2001; Maei 2011; Sutton et al. 2014; van Hasselt, Mahmood, and Sutton 2014; Mahmood, van Hasselt, and Sutton 2014).\nAll algorithms considered in this paper are in a sense descendent from a linear stochastic gradient, or LMS, update. The main idea of span-independent computation is more general and can be applied quite naturally to other settings, including for instance non-linear functions such as deep neural networks (LeCun, Bengio, and Hinton 2015; Mnih et al. 2015) or to quadratic-time linear-function algorithms as in LSTD (Bradtke and Barto 1996). Not all updates may have fully equivalent span-independent counterparts, but even then it may be more important to be independent of span than to be exactly equivalent."
    } ],
    "references" : [ {
      "title" : "Non-strongly-convex smooth stochastic approximation with convergence rate O(1/n)",
      "author" : [ "F. Bach", "E. Moulines" ],
      "venue" : "Advances in Neural Information Processing Systems 26, pp. 773–781.",
      "citeRegEx" : "Bach and Moulines,? 2013",
      "shortCiteRegEx" : "Bach and Moulines",
      "year" : 2013
    }, {
      "title" : "Dynamic Programming",
      "author" : [ "R. Bellman" ],
      "venue" : "Princeton University Press.",
      "citeRegEx" : "Bellman,? 1957",
      "shortCiteRegEx" : "Bellman",
      "year" : 1957
    }, {
      "title" : "Stochastic approximation with two time scales",
      "author" : [ "V.S. Borkar" ],
      "venue" : "Systems & Control Letters 29(5), pp. 291–294.",
      "citeRegEx" : "Borkar,? 1997",
      "shortCiteRegEx" : "Borkar",
      "year" : 1997
    }, {
      "title" : "Stochastic approximation",
      "author" : [ "V.S. Borkar" ],
      "venue" : "Cambridge Books.",
      "citeRegEx" : "Borkar,? 2008",
      "shortCiteRegEx" : "Borkar",
      "year" : 2008
    }, {
      "title" : "Linear least-squares algorithms for temporal difference learning",
      "author" : [ "S.J. Bradtke", "A.G. Barto" ],
      "venue" : "Machine Learning 22, pp. 33–57.",
      "citeRegEx" : "Bradtke and Barto,? 1996",
      "shortCiteRegEx" : "Bradtke and Barto",
      "year" : 1996
    }, {
      "title" : "Dynamic programming and Markov processes",
      "author" : [ "R.A. Howard" ],
      "venue" : "MIT Press.",
      "citeRegEx" : "Howard,? 1960",
      "shortCiteRegEx" : "Howard",
      "year" : 1960
    }, {
      "title" : "Convergence rate of linear two-time-scale stochastic approximation",
      "author" : [ "V.R. Konda", "J.N. Tsitsiklis" ],
      "venue" : "Annals of applied probability 14(2), pp. 796–819.",
      "citeRegEx" : "Konda and Tsitsiklis,? 2004",
      "shortCiteRegEx" : "Konda and Tsitsiklis",
      "year" : 2004
    }, {
      "title" : "Stochastic approximation and recursive algorithms and applications",
      "author" : [ "H.J. Kushner", "G. Yin" ],
      "venue" : "Vol. 35. Springer Science & Business Media.",
      "citeRegEx" : "Kushner and Yin,? 2003",
      "shortCiteRegEx" : "Kushner and Yin",
      "year" : 2003
    }, {
      "title" : "Gradient temporal-difference learning algorithms",
      "author" : [ "H.R. Maei" ],
      "venue" : "PhD thesis. University of Alberta.",
      "citeRegEx" : "Maei,? 2011",
      "shortCiteRegEx" : "Maei",
      "year" : 2011
    }, {
      "title" : "Weighted importance sampling for off-policy learning with linear function approximation",
      "author" : [ "A.R. Mahmood", "H.P. van Hasselt", "R.S. Sutton" ],
      "venue" : "Advances in Neural Information Processing Systems",
      "citeRegEx" : "Mahmood et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Mahmood et al\\.",
      "year" : 2014
    }, {
      "title" : "Human-level control through deep reinforcement learning",
      "author" : [ "V. Mnih", "K. Kavukcuoglu", "D. Silver", "A.A. Rusu", "J. Veness", "M.G. Bellemare", "A. Graves", "M. Riedmiller", "A.K. Fidjeland", "G. Ostrovski", "S. Petersen", "C. Beattie", "A. Sadik", "I. Antonoglou", "H. King", "D. Kumaran", "D. Wierstra", "S. Legg", "D. Hassabis" ],
      "venue" : "Nature 518(7540), pp. 529–533.",
      "citeRegEx" : "Mnih et al\\.,? 2015",
      "shortCiteRegEx" : "Mnih et al\\.",
      "year" : 2015
    }, {
      "title" : "Simultaneous localization and mapping with unknown data association using FastSLAM",
      "author" : [ "M. Montemerlo", "S. Thrun" ],
      "venue" : "In: IEEE International Conference on Robotics and Automation, 2003. Vol. 2. IEEE, pp. 1985–1991.",
      "citeRegEx" : "Montemerlo and Thrun,? 2003",
      "shortCiteRegEx" : "Montemerlo and Thrun",
      "year" : 2003
    }, {
      "title" : "Acceleration of stochastic approximation by averaging",
      "author" : [ "B.T. Polyak", "A.B. Juditsky" ],
      "venue" : "SIAM Journal on Control and Optimization 30(4), pp. 838–855.",
      "citeRegEx" : "Polyak and Juditsky,? 1992",
      "shortCiteRegEx" : "Polyak and Juditsky",
      "year" : 1992
    }, {
      "title" : "Off-policy temporal-difference learning with function approximation",
      "author" : [ "D. Precup", "R.S. Sutton" ],
      "venue" : "In: Proceedings of the eighteenth International Conference on Machine Learning. Morgan Kaufmann, pp. 417–424.",
      "citeRegEx" : "Precup and Sutton,? 2001",
      "shortCiteRegEx" : "Precup and Sutton",
      "year" : 2001
    }, {
      "title" : "Eligibility traces for off-policy policy evaluation",
      "author" : [ "D. Precup", "R.S. Sutton", "S.P. Singh" ],
      "venue" : "In: Proceedings of the Seventeenth International Conference on Machine Learning. Morgan Kaufmann, pp. 766–773.",
      "citeRegEx" : "Precup et al\\.,? 2000",
      "shortCiteRegEx" : "Precup et al\\.",
      "year" : 2000
    }, {
      "title" : "A stochastic approximation method",
      "author" : [ "H. Robbins", "S. Monro" ],
      "venue" : "The Annals of Mathematical Statistics 22(3), pp. 400–407.",
      "citeRegEx" : "Robbins and Monro,? 1951",
      "shortCiteRegEx" : "Robbins and Monro",
      "year" : 1951
    }, {
      "title" : "Temporal credit assignment in reinforcement learning",
      "author" : [ "R.S. Sutton" ],
      "venue" : "PhD thesis. University of Massachusetts.",
      "citeRegEx" : "Sutton,? 1984",
      "shortCiteRegEx" : "Sutton",
      "year" : 1984
    }, {
      "title" : "Learning to predict by the methods of temporal differences",
      "author" : [ "R.S. Sutton" ],
      "venue" : "Machine Learning 3, pp. 9–44. 31",
      "citeRegEx" : "Sutton,? 1988",
      "shortCiteRegEx" : "Sutton",
      "year" : 1988
    }, {
      "title" : "Reinforcement Learning: An Introduction",
      "author" : [ "R.S. Sutton", "A.G. Barto" ],
      "venue" : "The MIT press, Cambridge MA.",
      "citeRegEx" : "Sutton and Barto,? 1998",
      "shortCiteRegEx" : "Sutton and Barto",
      "year" : 1998
    }, {
      "title" : "A convergent O(n) algorithm for off-policy temporal-difference learning with linear function approximation",
      "author" : [ "R.S. Sutton", "Szepesvári", "Cs.", "H.R. Maei" ],
      "venue" : "Advances in Neural Information Processing Systems 21, pp. 1609–1616.",
      "citeRegEx" : "Sutton et al\\.,? 2008",
      "shortCiteRegEx" : "Sutton et al\\.",
      "year" : 2008
    }, {
      "title" : "Fast gradient-descent methods for temporal-difference learning with linear function approximation",
      "author" : [ "R.S. Sutton", "H.R. Maei", "D. Precup", "S. Bhatnagar", "D. Silver", "Szepesvári", "Cs.", "E. Wiewiora" ],
      "venue" : "In: Proceedings of the 26th Annual International Conference on Machine Learning. ACM, pp. 993–1000.",
      "citeRegEx" : "Sutton et al\\.,? 2009",
      "shortCiteRegEx" : "Sutton et al\\.",
      "year" : 2009
    }, {
      "title" : "A new Q(λ) with interim forward view and Monte Carlo equivalence",
      "author" : [ "R.S. Sutton", "A.R. Mahmood", "D. Precup", "H.P. van Hasselt" ],
      "venue" : "JMLR W&CP",
      "citeRegEx" : "Sutton et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Sutton et al\\.",
      "year" : 2014
    }, {
      "title" : "Algorithms for reinforcement learning",
      "author" : [ "Szepesvári", "Cs." ],
      "venue" : "Synthesis Lectures on Artificial Intelligence and Machine Learning 4(1), pp. 1–103.",
      "citeRegEx" : "Szepesvári and Cs.,? 2010",
      "shortCiteRegEx" : "Szepesvári and Cs.",
      "year" : 2010
    }, {
      "title" : "Off-policy TD(λ) with a true online equivalence",
      "author" : [ "H.P. van Hasselt", "A.R. Mahmood", "R.S. Sutton" ],
      "venue" : "Proceedings of the 30th Conference on Uncertainty in Artificial Intelligence",
      "citeRegEx" : "Hasselt et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Hasselt et al\\.",
      "year" : 2014
    }, {
      "title" : "True online TD(λ)",
      "author" : [ "H. van Seijen", "R.S. Sutton" ],
      "venue" : "JMLR W&CP",
      "citeRegEx" : "Seijen and Sutton,? \\Q2014\\E",
      "shortCiteRegEx" : "Seijen and Sutton",
      "year" : 2014
    } ],
    "referenceMentions" : [ {
      "referenceID" : 18,
      "context" : "For example, in reinforcement learning we often learn value functions that are predictions of the discounted sum of all future rewards in the potentially infinite future (Sutton and Barto 1998).",
      "startOffset" : 170,
      "endOffset" : 193
    }, {
      "referenceID" : 16,
      "context" : "We show that the desire to be online results in the spontaneous emergence of TD errors (Sutton 1984; Sutton 1988).",
      "startOffset" : 87,
      "endOffset" : 113
    }, {
      "referenceID" : 17,
      "context" : "We show that the desire to be online results in the spontaneous emergence of TD errors (Sutton 1984; Sutton 1988).",
      "startOffset" : 87,
      "endOffset" : 113
    }, {
      "referenceID" : 12,
      "context" : "This is interesting because such averaging is known to improve the convergence rates of online learning algorithms (Polyak and Juditsky 1992; Bach and Moulines 2013), but seems to only rarely be used in reinforcement learning (as noted, e.",
      "startOffset" : 115,
      "endOffset" : 165
    }, {
      "referenceID" : 0,
      "context" : "This is interesting because such averaging is known to improve the convergence rates of online learning algorithms (Polyak and Juditsky 1992; Bach and Moulines 2013), but seems to only rarely be used in reinforcement learning (as noted, e.",
      "startOffset" : 115,
      "endOffset" : 165
    }, {
      "referenceID" : 17,
      "context" : "In Section 6, we formalize these ideas and show they lead naturally to a form of TD(λ) (Sutton 1988; Sutton and Barto 1998).",
      "startOffset" : 87,
      "endOffset" : 123
    }, {
      "referenceID" : 18,
      "context" : "In Section 6, we formalize these ideas and show they lead naturally to a form of TD(λ) (Sutton 1988; Sutton and Barto 1998).",
      "startOffset" : 87,
      "endOffset" : 123
    }, {
      "referenceID" : 16,
      "context" : "The et vector is analogous to the conventional eligibility trace (see: Sutton 1988; Sutton and Barto 1998, and references therein) but has a special form as first proposed by van Seijen and Sutton (2014). It is initialized to e−1 = 0 (or, equivalently, to e0 = α0φ0) and then updated according to",
      "startOffset" : 71,
      "endOffset" : 204
    }, {
      "referenceID" : 18,
      "context" : "Until recently, exactly equivalences between forward and backward views were only known to exist for algorithms that update their predictions in batch (Sutton and Barto 1998).",
      "startOffset" : 151,
      "endOffset" : 174
    }, {
      "referenceID" : 16,
      "context" : "Until recently, exactly equivalences between forward and backward views were only known to exist for algorithms that update their predictions in batch (Sutton and Barto 1998). Van Seijen & Sutton (2014) were the first to derive an online backward",
      "startOffset" : 152,
      "endOffset" : 203
    }, {
      "referenceID" : 18,
      "context" : "This target Z t is known as a λ-return (Sutton and Barto 1998).",
      "startOffset" : 39,
      "endOffset" : 62
    }, {
      "referenceID" : 16,
      "context" : "This target Z t is known as a λ-return (Sutton and Barto 1998). The version that truncates at the current horizon h was first proposed by van Seijen and Sutton (2014). The total set of updates is",
      "startOffset" : 40,
      "endOffset" : 167
    }, {
      "referenceID" : 17,
      "context" : "The merits of λ-returns are well known (Sutton 1988; Sutton and Barto 1998) but the β-weighting of the online weights is novel to this paper, and it is appropriate to discuss it in a little more detail.",
      "startOffset" : 39,
      "endOffset" : 75
    }, {
      "referenceID" : 18,
      "context" : "The merits of λ-returns are well known (Sutton 1988; Sutton and Barto 1998) but the β-weighting of the online weights is novel to this paper, and it is appropriate to discuss it in a little more detail.",
      "startOffset" : 39,
      "endOffset" : 75
    }, {
      "referenceID" : 12,
      "context" : "This specific algorithm is interesting because the predictions according to the averages θm are known to converge to the optimal predictions faster than the predictions according to any sequence of online weights θ̃m (Polyak and Juditsky 1992; Bach and Moulines 2013).",
      "startOffset" : 217,
      "endOffset" : 267
    }, {
      "referenceID" : 0,
      "context" : "This specific algorithm is interesting because the predictions according to the averages θm are known to converge to the optimal predictions faster than the predictions according to any sequence of online weights θ̃m (Polyak and Juditsky 1992; Bach and Moulines 2013).",
      "startOffset" : 217,
      "endOffset" : 267
    }, {
      "referenceID" : 16,
      "context" : "Therefore, as in Sutton et al. (2014), we allow the features, discounts and persistency parameters to be stationary functions of an underlying unobserved state, such that φt .",
      "startOffset" : 17,
      "endOffset" : 38
    }, {
      "referenceID" : 15,
      "context" : "If the step sizes are suitably chosen, for instance such that ∑∞ t=0 αt =∞ and ∑∞ t=0 α 2 t <∞ (Robbins and Monro 1951), and if the means and variances of Z∞ t and φt are well-defined and bounded for all t, this update converges to the fixed-point solution θ∗ that minimizes the quadratic loss (cf.",
      "startOffset" : 95,
      "endOffset" : 119
    }, {
      "referenceID" : 0,
      "context" : "Although convergence is already guaranteed when βt = 1 for all t, recent work has shown that for similar stochastic gradient algorithms the optimal rate of convergence is attained if βt decreases much faster than αt, specifically when βt = O(t −1) while αt = α for some constant α (Bach and Moulines 2013).",
      "startOffset" : 281,
      "endOffset" : 305
    }, {
      "referenceID" : 0,
      "context" : "Although convergence is already guaranteed when βt = 1 for all t, recent work has shown that for similar stochastic gradient algorithms the optimal rate of convergence is attained if βt decreases much faster than αt, specifically when βt = O(t −1) while αt = α for some constant α (Bach and Moulines 2013). More generally, it seems likely that convergence also holds if ∑∞ t=0 β 2 t <∞ and ∑∞ t=0 α 2 t =∞. The observation that βt should perhaps decrease over time for faster learning may seem at odds with our introduction of this parameter as a degree of trust. However, these two views are quite compatible if we consider βt to be the degree of trust we place in the online updates relative to the trust we place in our current predictions due to the trusted weights. When the trust in the predictions increases over time, the relative trust in the inherently noisy online targets should then decrease. Although Theorem 1 is already fairly general, it does not cover the important case when the residual predictions additionally depend on the weights we are updating. It makes sense to use the predictions we trust most and therefore we now consider what happens when Pt . = φt θt−1. Notice that we have to use θt−1 rather than θt, because Pt is used in the computation of θt and so the latter is not yet available when we compute Pt. The analysis of this case is more complex than the previous one, because Pt is no longer a constant function of state. This means the update is no longer a standard gradient-descent update on a quadratic loss, because the target Z t for the online weights itself depends on the trusted weights that we are simultaneously updating. The results by Bach and Moulines (2013) on stochastic gradient descent indicate that perhaps the most interesting case is where βt decreases faster than αt, such that limt→∞ βt/αt = 0.",
      "startOffset" : 282,
      "endOffset" : 1709
    }, {
      "referenceID" : 20,
      "context" : "Then, if ∑∞ t=0 αt = ∑∞ t=0 βt = ∞, ∑∞ t=0 α 2 t < ∞, and limt→∞ βt αt = 0, algorithm (44) converges almost surely to the TD fixed-point solution θ∗ that minimizes the mean-squared projected Bellman error (Sutton, Szepesvári, and Maei 2008; Sutton et al. 2009), such that",
      "startOffset" : 205,
      "endOffset" : 260
    }, {
      "referenceID" : 2,
      "context" : "For more detail on analyzing stochastic approximations on two times scales, we refer to Borkar (1997), Borkar (2008), Kushner and Yin (2003), and Konda and Tsitsiklis (2004).",
      "startOffset" : 88,
      "endOffset" : 102
    }, {
      "referenceID" : 2,
      "context" : "For more detail on analyzing stochastic approximations on two times scales, we refer to Borkar (1997), Borkar (2008), Kushner and Yin (2003), and Konda and Tsitsiklis (2004).",
      "startOffset" : 88,
      "endOffset" : 117
    }, {
      "referenceID" : 2,
      "context" : "For more detail on analyzing stochastic approximations on two times scales, we refer to Borkar (1997), Borkar (2008), Kushner and Yin (2003), and Konda and Tsitsiklis (2004).",
      "startOffset" : 88,
      "endOffset" : 141
    }, {
      "referenceID" : 2,
      "context" : "For more detail on analyzing stochastic approximations on two times scales, we refer to Borkar (1997), Borkar (2008), Kushner and Yin (2003), and Konda and Tsitsiklis (2004). We first analyze the convergence of the faster updates to the online weights, where we can assume that the trusted weights are stationary at some value θ.",
      "startOffset" : 88,
      "endOffset" : 174
    }, {
      "referenceID" : 1,
      "context" : "Then, using a form of policy iteration (Bellman 1957; Howard 1960), we can repeatedly switch between estimating and improving the policy to tackle the problem of optimal control.",
      "startOffset" : 39,
      "endOffset" : 66
    }, {
      "referenceID" : 5,
      "context" : "Then, using a form of policy iteration (Bellman 1957; Howard 1960), we can repeatedly switch between estimating and improving the policy to tackle the problem of optimal control.",
      "startOffset" : 39,
      "endOffset" : 66
    }, {
      "referenceID" : 18,
      "context" : "However, to properly and fully include adaptable policies, we would in addition need to carefully consider the problem of learning off-policy, about action-selection policies that differ from the one used to generate the data (Sutton and Barto 1998).",
      "startOffset" : 226,
      "endOffset" : 249
    }, {
      "referenceID" : 13,
      "context" : "This is consistent but orthogonal to the ideas outlined in this paper, and such off-policy predictions (including those about the greedy and, ultimately, optimal policy) are learnable through a proper use of rejection sampling, as in Q-learning, or importance sampling (Precup, Sutton, and Singh 2000; Precup and Sutton 2001; Maei 2011; Sutton et al. 2014; van Hasselt, Mahmood, and Sutton 2014; Mahmood, van Hasselt, and Sutton 2014).",
      "startOffset" : 269,
      "endOffset" : 434
    }, {
      "referenceID" : 8,
      "context" : "This is consistent but orthogonal to the ideas outlined in this paper, and such off-policy predictions (including those about the greedy and, ultimately, optimal policy) are learnable through a proper use of rejection sampling, as in Q-learning, or importance sampling (Precup, Sutton, and Singh 2000; Precup and Sutton 2001; Maei 2011; Sutton et al. 2014; van Hasselt, Mahmood, and Sutton 2014; Mahmood, van Hasselt, and Sutton 2014).",
      "startOffset" : 269,
      "endOffset" : 434
    }, {
      "referenceID" : 21,
      "context" : "This is consistent but orthogonal to the ideas outlined in this paper, and such off-policy predictions (including those about the greedy and, ultimately, optimal policy) are learnable through a proper use of rejection sampling, as in Q-learning, or importance sampling (Precup, Sutton, and Singh 2000; Precup and Sutton 2001; Maei 2011; Sutton et al. 2014; van Hasselt, Mahmood, and Sutton 2014; Mahmood, van Hasselt, and Sutton 2014).",
      "startOffset" : 269,
      "endOffset" : 434
    }, {
      "referenceID" : 10,
      "context" : "The main idea of span-independent computation is more general and can be applied quite naturally to other settings, including for instance non-linear functions such as deep neural networks (LeCun, Bengio, and Hinton 2015; Mnih et al. 2015) or to quadratic-time linear-function algorithms as in LSTD (Bradtke and Barto 1996).",
      "startOffset" : 189,
      "endOffset" : 239
    }, {
      "referenceID" : 4,
      "context" : "2015) or to quadratic-time linear-function algorithms as in LSTD (Bradtke and Barto 1996).",
      "startOffset" : 65,
      "endOffset" : 89
    } ],
    "year" : 2015,
    "abstractText" : "We consider how to learn multi-step predictions efficiently. Conventional algorithms wait until observing actual outcomes before performing the computations to update their predictions. If predictions are made at a high rate or span over a large amount of time, substantial computation can be required to store all relevant observations and to update all predictions when the outcome is finally observed. We show that the exact same predictions can be learned in a much more computationally congenial way, with uniform per-step computation that does not depend on the span of the predictions. We apply this idea to various settings of increasing generality, repeatedly adding desired properties and each time deriving an equivalent span-independent algorithm for the conventional algorithm that satisfies these desiderata. Interestingly, along the way several known algorithmic constructs emerge spontaneously from our derivations, including dutch eligibility traces, temporal difference errors, and averaging. This allows us to link these constructs one-to-one to the corresponding desiderata, unambiguously connecting the ‘how’ to the ‘why’. Each step, we make sure that the derived algorithm subsumes the previous algorithms, thereby retaining their properties. Ultimately we arrive at a single general temporal-difference algorithm that is applicable to the full setting of reinforcement learning. 1 Learning long-term predictions The span of a multi-step prediction is the number of steps elapsing between when the prediction is made and when its target or ideal value is known. We consider the case in which predictions are made repeatedly, at each of a sequence of discrete time steps. For example, if on each day we predict what a stock market index will be in 30 days, then the span is 30, whereas if we predict at each hour what the stock market index will be in 30 days, then the span is 30× 24 = 720. The span may vary for individual predictions in a sequence. For example, if we predict on each day what the stock-market index will be at the end of the year, then the span will be much longer for predictions made in January than it is for predictions made in ∗Google DeepMind †Reinforcement Learning and Artificial Intelligence Laboratory Department of Computing Science, University of Alberta Edmonton, Alberta, Canada T6G 2E8 1 ar X iv :1 50 8. 04 58 2v 1 [ cs .L G ] 1 9 A ug 2 01 5 December. If the span may vary in this way, then we consider the span of the prediction sequence to be the maximum possible span of any individual prediction in the sequence. For example, the span of a daily end-of-year stock-index prediction is 365. Often the span is infinite. For example, in reinforcement learning we often learn value functions that are predictions of the discounted sum of all future rewards in the potentially infinite future (Sutton and Barto 1998). In this paper we consider computational and algorithmic issues in efficiently learning long-term predictions, defined as predictions of large integer span. Predictions could be long term in this sense either because a great deal of clock time passes, as in predicting something at the end of the year, or because predictions are made very often, with a short time between steps (e.g., as in high-frequency financial trading). The per-step computational complexity of some algorithms for learning accurate predictions depends on the span of the predictions, and this can become a significant concern if the span is large. Therefore, we focus on the construction of learning algorithms whose computational complexity per time step (in both time and memory) is constant (does not scale with time) and independent of span. This paper features two recurring themes, the first of which is the repeated spontaneous emergence of, often well-known, algorithmic constructs, directly from our derivations. We start each derivation by formalizing a desired property and constructing an algorithm that fulfills it, without considering computationally efficiency. Then, we derive a spanindependent algorithm that results on each step in exactly the same predictions. Interestingly, each time a specific algorithmic construct emerges, demonstrating a clear connection between the desideratum (the ‘why’) and the algorithmic construct (the ‘how’). For instance, the desire to be independent of span leads to a dutch eligibility trace, which was previously derived only in the more specific context of online temporal difference (TD) learning (van Seijen and Sutton 2014). The second theme is that we unify the algorithms at each step. Each time, we make sure to obtain an algorithm that is strictly more general than the previous ones, so that in the end we obtain one single algorithm that can fulfill all the desiderata while remaining computationally congenial. 2 Outline of the paper In this section, we briefly describe the high-level narrative of the paper, without going into technical detail. In each of the Sections 3 to 8, we describe and formalize one or more desirable properties for our algorithms and then derive a computationally congenial algorithm that achieves this exactly. We build up to the final, most general, algorithm that is ultimately derived in Section 8 to highlight the connections between desired properties and algorithmic constructs. Making these connections clear is one of the main goals of this paper. Specifically, in Section 3 we derive a span-independent algorithm to update the predictions for a single final outcome. The algorithm is offline in the sense that does not change its predictions before observing the outcome. The dutch trace emerges spontaneously, which shows that this trace is closely tied to the requirement of span-independent computation. This emergence is surprising and intriguing because it shows that these traces",
    "creator" : "LaTeX with hyperref package"
  }
}