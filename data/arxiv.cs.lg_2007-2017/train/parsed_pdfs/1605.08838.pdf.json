{
  "name" : "1605.08838.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Dueling Bandits with Dependent Arms",
    "authors" : [ "Bangrui Chen", "Peter I. Frazier" ],
    "emails" : [ "bc496@cornell.edu", "pf98@cornell.edu" ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "In the dueling bandits problem, we are faced with a collection of arms, and pull a pairs of arms while observing noisy binary feedback indicating which arm is better for each pulled pair. As in the classical multi-armed bandit problem, we wish to pull arms to quickly learn which arm is best and minimize the number of pulls to suboptimal arms.\nDueling bandits were introduced by Yue and Joachims (2009), motivated by interactive optimization of web search and other information retrieval systems. The advantage of the dueling bandits formulation over the classical multi-armed bandits formulation in this application setting is that pairwise comparison results can be reliably inferred from implicit feedback, for example through interleaved rankings in Radlinski et al. (2008), in contrast with cardinal evaluation obtained from explicit feedback, which is typically difficult to obtain, biased, and requires careful calibration (Joachims et al., 2007; Yue et al., 2012).\nDueling bandits have been studied most frequently assuming strong regret, in which the regret is 0 if and only if both pulled arms are optimal. Several algorithms have been devised that assume the existence of a Condorcet winner, i.e., one that is preferred in comparison with each other arm. Algorithms with order-optimal strong regret, O(N log(T )), in this setting include BTM (Yue and Joachims, 2011), RUCB (Zoghi et al., 2014) and RMED (Komiyama et al., 2015). Zoghi et al. (2015) points out points out that a Condorcet winner does not necessarily exist, and that its probability of existence decreases dramatically with the number of arms. That work instead studies the dueling bandits assuming a Copeland\nar X\niv :1\n60 5.\n08 83\n8v 2\n[ cs\n.L G\n] 1\n5 Ju\nn 20\nwinner, which is guaranteed to exist, and propose two algorithms, CCB and SCB, which achieve O(N log(T )) strong regret in this more general setting.\nThe above papers on strong regret bound the binary strong regret, in which the regret is 1 whenever it is strictly positive. Ailon et al. (2014) considered strong utility-based regret, in which each arm has a utility score from which preferences are derived, and the regret for failing to pull the maximum utility arm twice is a function of that maximal arm’s utility and the utilities of the pulled arms.\nBandits have also been considered, though less frequently, in the weak regret setting, introduced by Yue et al. (2012), in which regret is 0 if either of the pulled arms is optimal. This setting is more appropriate for recommender systems, in which we offer the user a pair of items, and she selects the one that is preferred. 0 regret is incurred as long as the best item is made available. While Yue et al. (2012) introduced weak regret, an algorithm with regret bounds first appeared in Chen and Frazier (2017), which proposed the Winner Stays (WS) algorithm that achieves O(N log(N)) cumulative binary weak regret when arms have a total order and O(N2) in the Cordorcet winner setting. These bounds on binary weak regret have corresponding bounds on utility-based weak regret inflated by the difference in utility between the best and worst arms.\nWe consider utility-based weak regret, in the total order setting, when the total order is induced by a utility which is in turn a function of observable arm features, an unknown latent preference vector, and a known utility function. This framework includes the commonly used logit or Bradley-Terry (Revelt and Train, 1998; Yue et al., 2012) and probit models (Franses and Montgomery, 2002). We provide an algorithm, Comparing with the Best (CTB) that has expected cumulative utility-based weak regret that is constant in T , and that leverages the dependence between preferences over arms induced by the arm features and utility function to provide excellent empirical performance when prior information is available. While our regret bound’s dependence on N is looser than Chen and Frazier (2017) (our dependence is 2N in the worst case, and is N2d when the utility function is linear over a d-dimensional space of preferences and arm features), our algorithm is more flexible in its ability to problem structure induced by the feature vectors, and outperforms it empirically by a substantial margin when N is small enough to allow computation that fully takes advantage of this problem structure.\nOur exploitation of arm features is similar in spirit to work in the traditional (cardinal) multi-armed bandit setting on linear bandits (Rusmevichientong and Tsitsiklis, 2010; AbbasiYadkori et al., 2011).\nThe paper is structured as follows. In section 2, we formulate our problem. In section 3, we introduce Comparing The Best (CTB) which we show in section 4 has CTB constant expected cumulative regret. In section 5, we discuss a efficient implementation method for a specific class of prior information. In section 6, we provide a Bayesian interpretation for CTB. In section 7, we compare CTB with three benchmarks using simulated datasets, in which CTB outperforms all benchmarks considered."
    }, {
      "heading" : "2. Problem Formulation",
      "text" : "There are N ≥ 2 arms, and each arm i has an observable and distinct d-dimensional feature vector Ai. Preferences between pairs of arms i, j are described by fixed but unknown probabil-\nities pi,j , where pi,j = 1−pj,i and pi,j 6= 0.5 when i 6= j. We denote p = mini<j max(pi,j , pj,i). By construction, p > 0.5.\nAt each time t, we pull two arms Xt,0 and Xt,1 (this act is called a “duel”) and we observe feedback Yt ∈ {0, 1} indicating the winning arm: Yt = 0 indicates arm Xt,0 won and Yt = 1 indicates arm Xt,1 won. Conditioned on the arms pulled and the history (the arms pulled and the identity of the winner at times t′ < t), Yt is equal to 0 with probability pi,j .\nWe suppose that the arms have a total order, i.e., that there exists an ordering of the arms such that pi,j > 0.5 if and only if arm i is before arm j in this order. Moreover, we suppose this ordering is determined by a utility associated with each arm, u(θ,Ai), where u is a known utility function and θ ∈ Rd′ is an unknown preference vector. In particular, pi,j > 0.5 if and only if u(θ,Ai) > u(θ,Aj). The assumption that the total order be determined by u(θ,Ai) is without loss of generality if we are willing to select d′ to be sufficiently large and u to allow sufficient flexibility, although one may also choose a smaller d′ and a less flexible u with the goal of obtaining smaller regret (described below) when these more restrictive modeling assumptions hold. We assume without loss of generality that the indices correspond to their ordering by utility, so u(θ,A1) > u(θ,A2) > · · · > u(θ,AN ).\nSeveral commonly used discrete choice models fall within this framework. For example, our framework includes the logit or Bradley-Terry model (Revelt and Train, 1998; Yue et al., 2012), in which d′ = d, the utility function is u(θ,Ai) = θ ·Ai and pi,j = exp(u(θ,Ai))exp(u(θ,Ai)+u(θ,Aj)) . Our framework also includes the probit model (Franses and Montgomery, 2002) in which d′ = d and the utility function is the inner product as with the logit model, but pi,j = Φ(u(θ,Ai)− u(θ,Aj)) where Φ(·) is the standard normal cdf.\nWe define the utility-based weak regret r(t) (henceforce referred to simply as the regret) at time t as r(t) = u(θ,A1)−max{u(θ,AXt,0), u(θ,AXt,1)}, which is the difference in utility between the best arm overall and the best arm available to the user from those offered. The cumulative regret up to time T is R(T ) = ∑T t=1 r(t). We measure the quality of an algorithm by its expected cumulative regret. We now develop an algorithm CTB, and show it has constant expected cumulative regret.\n3. The Comparing The Best (CTB) Algorithm\nIn this section we propose an algorithm Comparing The Best (CTB) for this problem setting. This algorithm is based on the idea of “cells”, which correspond to possible orderings of the arms by utility. It maintains a score for each cell, either explicitly or implicitly, which it initializes using optional prior information, and updates with the results from each duel.\nWe present a general version of CTB in this section that admits any prior information and explicitly maintains a score for each cell. Because the number of cells is exponential in the number of arms, explicitly maintaining scores for each cell is computationally infeasible for large problems. Thus, after presenting our theoretical results for the general CTB algorithm in section 4, we present a computationally efficient implementation of our algorithm in section 5 that can be used when the prior information can be expressed in terms of an initial score for each pair of arms. Although we present our algorithm in a frequentist setting, we show in section 6 that the scores used for each cell correspond to a Bayesian posterior on the value of θ, and CTB has a natural Bayesian interpretation.\nTo define CTB, we first define some terminology and notation: winning spaces, cells, a score, and the best arm corresponding to a cell. We begin with winning spaces.\nDefinition 3.1. Each pair of arms i, j defines a winning space Hi,j := {X ∈ Rd : u(X,Ai) ≥ u(X,Aj)}.\nWhen θ ∈ Hi,j , arm i is preferred over arm j. We use the phrases “arm Ai wins over arm Aj in a duel”, and “winning space Hi,j wins the duel” interchangeably.\nEach pair of arm determines two winning spaces and all winning spaces partition the space Rd into cells, where each cell is an intersection of winning spaces. To define notation to support working with cells, we first define Hi,j(k) = Hi,j when k = 0 and Hi,j(k) = Hj,i when k = 1. For a binary vector V , we let V [k] denote the kth element of V . Then, we have the following definition.\nDefinition 3.2. The cell C corresponding to a length N(N−1)2 binary vector V is\nC(V ) := ∩i<jHi,j ( V [ 1\n2 (2N − i)(i− 1) + j − i\n]) .\nWe assign binary vectors indexing cells, all of length N(N−1)2 , to integers lexicographically. Let Vk denote the kth such binary vector, let M = 2N denote the number of cells, and let Ci = C(Vi). With this definition, C1 = C(V1) = C([0, 0, · · · , 0]) and thus C1 = ∩i<jHi,j and θ ∈ C1. Some cells Ci may be empty. We call these empty cells. Let Jk = {(i, j)|Ck ⊆ Hi,j}, which is the collection of indices of the winning spaces that contains Ck.\nFigure 1 illustrates winning spaces and cells.\nWe define a score mi(t) associated with each cell Ci at time t. Later in section 6 we will interpret this score as a monotone transformation of the posterior probability that θ is in this cell. This score will be initialized to some value mi(0), discussed below, and then will be incremented each time a winning space containing Ci wins a duel. That is,\nmi(t) = mi(0) + t∑ k=1 1{Ci ⊆ HXk,1,Xk,2(Yk)}. (1)\nEach cell Ci assigns a preference order to the arms. Let B(i) be the arm that would be best if θ were in Ci. More formally, B(i) is the unique j such that Ci ⊆ Hj,k, ∀k 6= j. Since θ ∈ C1, we know B(1) = 1.\nWith this notation, we now define the Comparing The Best (CTB) algorithm in Algorithm 1. CTB pulls the arm that is best according to the cell with the highest score mi(t), and the arm that is best according to the cell with the highest score among those that have different best arm from the first arm chosen. If we interpret mi(t) as being a monotone transformation of the posterior probability that θ ∈ Ci, then we are selecting arms by selecting two cells that have different best arms, and are together most likely to contain θ.\nfor t ≤ T do Step 1: Pick Xt,0 = B(argmaximi(t)), breaking ties arbitrarily Step 2: Pick Xt,1 = B ( argmaxi:B(i)6=Xt,0 mi(t) ) , breaking ties arbitrarily\nStep 3: Observe the noisy feedback Yt and update mi(t) using Equation (1) Step 4: t=t+1\nend Algorithm 1: Comparing The Best (CTB)\nChoice of mi(0): Here we offer guidance on the choice of mi(0), which is left general in the description of CTB to allow the user the flexibility to influence the arms pulled with prior information about the value of θ, and to trade off regret against CTB’s computational performance. In doing so, there are four considerations:\nFirst, by setting mi(0) larger for those cells that the user believes are more likely to contain θ, the user encourages CTB to select those cells more often. If the user correctly sets mi(0) larger for the cell that contains θ, this tends to pull the best arm more often and decrease regret. We show in section 6 that mi(0) can be interpreted in terms of the prior probability that θ ∈ Ci, and one can leverage this relationship to convert prior information on θ into values for mi(0).\nSecond, by setting mi(0) to be −∞ for those cells that user is certain do not contain θ, she can lead CTB to never select those cells. One may safely do this for empty cells, in which model assumptions imply θ cannot reside. Doing this for other cells is dangerous, as setting cell m1(0) to −∞ can cause CTB to have linear regret.\nThird, in the absence of prior information, one may simply set mi(0) = 0 for all cells that may contain θ. We show in the next section show that as long as m1(0) > −∞, the expected cumulative regret is finite.\nFourth, there is a computational aspect to setting mi(0). We show below in section 5 that if each mi(0) can be written as a sum across pairs of arms of a score associated with each pair, then we can implement CTB in a computationally efficient manner that scales to many arms. In contrast, if one sets mi(0) without enforcing structure, the computation required to implement Algorithm 1 grows exponentially with the number of arms.\nWith these considerations in mind, we propose 3 specific ways to set mi(0), and evaluate them in numerical experiments:\n• For situations with loose computational requirements or few arms, and no prior information, we recommend setting mi = 0 for all non-empty cells and mi = −∞ for all empty cells. We call this CTB−1.\n• For situations with strict computational requirements and no prior information, we recommend setting mi = 0 for all cells. Then CTB can be implemented using the efficient method described in section 5. We call this CTB−2.\n• For situations with loose computational requirements or few arms, and strong prior information, we recommend settingmi from the prior according to the method described in section 6. We call this CTB−3."
    }, {
      "heading" : "4. Theoretical Results",
      "text" : "In this section, we prove the expected cumulative regret of CTB is bounded by a constant. The main idea behind our proof is to show that for each cell Ci with B(i) 6= 1, E[ ∑∞ t=0 1{mi(t) ≥ m1(t)}] is bounded by a constant. We show this in turn by relating m1(t) − mi(t) to a random walk with a larger probability of increasing than of decreasing. The following lemma, whose proof is in the supplement, allows us to bound the number of times this stochastic process takes values less a constant.\nLemma 1. Let p ∈ (0.5, 1]. Suppose Z(t) is a stochastic process with filtration Ft, Z(0) = 0 and P (Z(t + 1) = Z(t) + 1|Ft) ≥ p, then we have E [ ∑∞ t=0 1{Z(t) ≤ S}] ≤ p+S(2p−1) (2p−1)2 for S ∈ N.\nWe now proceed with the larger proof by defining\nqi,j(t) = t∑\nk=1\n1{Xk,0 = i,Xk,1 = j, Yk = 0}+ t∑\nk=1\n1{Xk,0 = j,Xk,1 = i, Yk = 1}, (2)\nwhich is the number of times up to time t that arm i beats arm j in a duel. Then we can rewrite mi(t) in terms of qi,j(t) as,\nmk(t) = mk(0) + ∑\n(i,j)∈Jk\nqi,j(t). (3)\nThe definition of C1 implies J1 = {(i, j), ∀i < j} and m1(t) = m1(0) + ∑\ni<j qi,j(t). Let Ni,j(t) = qi,j(t) + qj,i(t) denote the number of times we have pulled arms i and j. The next lemma shows E[Ni,j(t)] is bounded by a constant for 1 < i < j.\nLemma 2. For 1 < i < j, if m1(0) > −∞, we have E[Ni,j(t)] ≤M ′ p−∆(2p−1)(2p−1)2 , where M ′ is the number of cells i with mi(0) > −∞, and ∆ = mins=1,···M{m1(0)−ms(0)} ≤ 0.\nProof. Let 1 < i < j. Let Di,j(t) be an indicator function equal to 1 if and only if we pull arms i and j at time t. Given that we pull arm i, we can only also pull arm j when there is a cell Cs under which j is the best arm and for which ms(t) ≥ m1(t). Moreover, under the assumption that m1(0) > −∞, ms(t) ≥ m1(t) is only possible if ms(0) > −∞. Thus, Di,j(t) = 1 implies maxs:B(s)=j,ms(0)>−∞ms(t) ≥ m1(t). Adopting the convention here and\nin the rest of the proof that maxima and sums over sets of cells are taken only over those cells with ms(0) > −∞, we have\nDi,j(t) = Di,j(t) · 1 {\nmax s:B(s)=j\nms(t) ≥ m1(t) }\n≤ Di,j(t) ∑\ns:B(s)=j\n1{ms(t) ≥ m1(t)}\n= Di,j(t) ∑\ns:B(s)=j\n1  ∑ (i′ ,j′ )∈Js qi′ ,j′ (t) +ms(0) ≥ ∑ (i′ ,j′ )∈J1 qi′ ,j′ (t) +m1(0)  = Di,j(t)\n∑ s:B(s)=j 1  ∑ (i′ ,j′ )∈Js\\J1 qi′ ,j′ (t) +ms(0) ≥ ∑ (i′ ,j′ )∈J1\\Js qi′ ,j′ (t) +m1(0)  = Di,j(t)\n∑ s:B(s)=j 1  ∑ (i ′ ,j ′ )∈Js\\J1 qi′ ,j′ (t)− qj′ ,i′ (t) ≥ m1(0)−ms(0)  ≤ Di,j(t)\n∑ s:B(s)=j 1  ∑ (i′ ,j′ )∈Js\\J1 qi′ ,j′ (t)− qj′ ,i′ (t) ≥ ∆  , where the fourth equation holds because Js has the property that (i′, j′) ∈ Js ⇐⇒ (j′, i′) /∈ Js, and similarly for J1. Thus, (i ′ , j ′ ) ∈ Js \\ J1 ⇐⇒ i′, j′ ∈ Js and i′, j′ /∈ J1 ⇐⇒ j′, i′ /∈ Js and j′, i′ ∈ J1 ⇐⇒ (j ′ , i ′ ) ∈ J1 \\ Js.\nThus, we have\nNi,j(t) = t∑\nk=1\nDi,j(k) ≤ Di,j(k) ∑\ns:B(s)=j t∑ k=1 1  ∑ (i ′ ,j ′ )∈Js\\J1 qi′ ,j′ (k)− qj′ ,i′ (k) ≥ ∆  . Fix an s with B(s) = j and let Z(k) = ∑ (i′ ,j′ )∈Js\\J1 qi′ ,j′ (k)− qj′ ,i′ (k), so that\nNi,j(t) ≤ ∑\ns:B(s)=j t∑ k=1 Di,j(k) · 1 {Z(k) ≥ ∆} .\nWe observe that Z(k) is like a random walk, except that changes in only some time periods. We now describe the conditional distribution of Z(k + 1) given the history up to time k. Later, we will refer to the σ-algebra generated by this history as mathcalHk.\n• If the arms Xk,0, Xk,1 that we pull satisfy (Xk,0, Xk,1) ∈ Js \\ J1, then Z(k + 1) ∈ {Z(k) − 1, Z(k) + 1} and the conditional probability that Z(k + 1) = Z(k) − 1 is pXk,1,Xk,0 ≥ p. This lower bound holds because (Xk,0, Xk,1) /∈ J1 implies Xk,1 < Xk,0.\n• Similarly, if (Xk,1, Xk,0) ∈ Js \\ J1, then Z(k + 1) ∈ {Z(k) − 1, Z(k) + 1} as before, and the conditional probability that Z(k + 1) = Z(k) − 1 is pXk,0,Xk,1 ≥ p, because (Xk,1, Xk,0) /∈ J1 implies Xk,0 < Xk,1.\n• Otherwise, if neither (Xk,0, Xk,1) nor (Xk,1, Xk,0) is in Js \\ J1, then Z(k + 1) = Z(k).\n• The definition of J1 prevents having both (Xk,0, Xk,1) and (Xk,1, Xk,0) in Js \\ J1.\nWhen Di,j(k) = 1, so that we pull arms i and j (either Xt,0 = i and Xt,1 = j or vice versa) we will be in one of the first two cases, because B(s) = j implies cell s considers j to be the best arm, and so (j, i) ∈ Js, and i < j implies (j, i) /∈ J1. Thus, Di,j(k) = 1 implies Z(k + 1) 6= Z(k), and we have\nNi,j(t) ≤ ∑\ns:B(s)=j t∑ k=1 1 {Z(k + 1) 6= Z(k), Z(k) ≥ ∆} .\nWe will perform a random time change to study the dynamics over only those time periods where Z(k) changes. Define τ0 = 0, τm = mink{k > τm−1, Z(k) 6= Z(k + 1)}. Because the event Z(k) 6= Z(k + 1) is measurable given the history at time k, Hk, as described in the dynamics of Z(·) above, each τm is a stopping time. Define ζ = inf{m : τm =∞}, which is the lifetime of the random change of time. We have,\nNi,j(t) ≤ ∑\ns:B(s)=j\nζ−1∑ m=1 1 {Z(τm) ≥ ∆} . (4)\nWe let W (m) = Z(τm) for m < ζ (i.e., m with τm <∞), and W (m) = W (m− 1) + m for m ≥ ζ, where m are iid random variables taking value −1 with probability p and value 1 with probability 1− p. Observe that ζ is measurable with respect to H∞, so that the event m < ζ is measurable with respect to Hτm . We define an augmented filtration, letting Fm to be the σ-algebra generated by Hτmin(m,ζ) and ( m′ : m′ ≤ m). With this construction, W (m+1)−W (m) ∈ {−1,+1} and P (W (m+ 1) = W (m)− 1|Fm) ≥ p. Thus, by Lemma 1,\nζ∑ m=1 1 {Z(τm) ≥ ∆} = ζ∑ m=1 1 {W (m) ≥ ∆} ≤ ∞∑ m=1 1 {W (m) ≥ ∆} ≤ p−∆(2p− 1) (2p− 1)2 .\nCombining this with (4) and using the fact that the number of cells with ms(0) > −∞, M ′, bounds the sum over s, we obtain our result.\nBased on Lemma 2 and a union bound, we obtain our main theorem:\nTheorem 3. Let Λ = u(θ,A1) − u(θ,AN ). If m1(0) > −∞, CTB’s expected cumulative regret is bounded by (N−1)(N−2)2 M ′ p−∆(2p−1) (2p−1)2 Λ.\nIn general, M ′ can be as large as 2N . However, as discussed above, we may set mi(0) = −∞ for all the empty cells and assign finite mi(0) to empty cells (CTB−1). In this setting, since each cell assigns a ranking over arms and different cells give different rankings, we can bound M ′ by the number of permutations of N arms, N !. Moreover, when the utility function is linear and d′ = d, results in Jamieson and Nowak (2011) show M ′ is O(N2d′)."
    }, {
      "heading" : "5. Computation for Decomposable mi",
      "text" : "CTB achieves a constant expected cumulative regret. However, a naive implementation of Algorithm 1 requires a great deal of memory to store mi(t) for each cell, which makes it computationally challenging for problems with many arms. In this section, we consider a special case of CTB where mi(0) can be expressed in terms of an initial score for each pair of arms. Specifically, we suppose that there exists a ri,j such that\nmk(0) = ∑\n(i,j)∈Jk\nri,j ∀k. (5)\nHere ri,j can be interpreted as a prior indicating the extent to which we believe that arm i is preferred over arm j. In this special case, we describe an efficient computation method that scales to problems with many arms.\nInstead of storing mi(t), this method stores ri,j and qi,j(t) and uses them to reconstruct mi(t) with Equation 3. Then, Steps 1 and 2 in Algorithm 1 are written as optimization problems in which mi(t) is replaced by this expression in terms of qi,j(t) and ri,j . Toward this end, let ei,j denote a binary variable that will take value ei,j = 1 if we are to select a cell in Hi,j and 0 otherwise. Then, based on Equation 3, maximizing mi(t) is equivalent to maximizing ∑ i,j:i 6=j ei,j × (qi,j(t) + ri,j).\nTo find the best arm suggested by argmaximi(t) in Step 1, and suggested by a similar argmax in Step 2, it is sufficient to find maxi:B(i)=kmi(t) for each arm k. This is the cell with largest mi(t) among those that believe k is best. This problem is:\nmaximize ∑ i,j:i 6=j ei,j × (qi,j(t) + ri,j)\nsubject to ek,j = 1, ∀j 6= k ei,j + ej,i = 1, i, j = 1, ..., N, i 6= j ei,j ∈ {0, 1}, ∀i 6= j\n(6)\nThere are three conditions in Equation 6. The first condition is ek,j = 1 ∀j 6= k, which means cell C` that satisfies the first condition must lie in the winning space Hk,j , ∀j 6= k. In other words, C` ranks arm Ak better than any others and thus B(`) = k. The second and third condition together guarantee that cell C` either belongs to Hi,j or Hj,i.\nThough Equation 6 is an integer linear programming problem, which are usually computationally challenging, it is in fact easy to solve: the maximum value of this problem is reached when ei,j = 1 if ri,j + qi,j(t) > qj,i(t) + rj,i for all i 6= j, ei,j = 0 if this strict inequality is reversed, and breaking ties arbitrarily between the solutions (ei,j = 1, ei,j = 0) and (ei,j = 0, ei,j = 1) for those i, j with equality.\nDenote the maximum value of this problem at time t as f(k, t). After knowing f(k, t) = maxB(i)=kmi(t), finding the arm with largest mi(t) in Step 1 is equivalent to finding argmaxk f(k, t). Finding the arm with large mi(t) among those with a different best arm than Xt,0 in Step 2 is equivalent to finding argmaxk 6=Xt,0 f(k, t).\nFor general values of mi(0) that do not satisfy (5), finding the largest mi(t) is computationally challenging. However, in applications, instead of setting mi(0) directly, we may have some prior information about the probability that the user prefers arm i over arm j. This\ninformation can be used to construct ri,j since CTB guarantees constant regret regardless of the values that mi(0) take."
    }, {
      "heading" : "6. Bayesian Interpretation",
      "text" : "Although our problem is formulated in a frequentist setting, we show here that CTB has a Bayesian interpretation. In this section, we construct a Bayesian posterior on θ given a prior and given an assumption that pi,j = q > 0.5 for all i < j, where q may be the same or different from p, and pi,j may or may not be constant across i, j in reality.\nWe put a prior distribution p0 on θ, which induces a prior on the identity of the cell containing θ. The prior probability that θ is in cell i is written p0(Ci), and is obtained by integrating p0 over Ci. Let pt(Ci) indicate the posterior probability that θ is in cell Ci, at time t, given pi,j = q for all i < j. The following pair of lemmas give recursive and non-recursive expressions for pt.\nLemma 4. For compactness of notation, let i = Xt,0 and j = Xt,1. Then the posterior distribution pt+1 is,\npt+1(x) =\n{ pt(x)q\npt(Hi,j(Yk))q+(1−pt(Hi,j(Yk)))(1−q) if x ∈ Hi,j(Yt) pt(x)(1−q) pt(Hi,j(Yk))q+(1−pt(Hi,j(Yk)))(1−q) if x /∈ Hi,j(Yt)\nBased on this lemma, we can rewrite the posterior distribution in terms of mi(t)−mi(0).\nLemma 5. For each cell Ci, the posterior distribution after t comparison is\npt(Ci) ∝ p0(Ci)qmi(t)−mi(0)(1− q)t−mi(t)+mi(0).\nWe leave the proof of both Lemmas to the appendix. Lemma 5 allows us to rewrite pt(Ci) as\npt(Ci) ∝ p0(Ci)qmi(t)−mi(0)(1− q)t−mi(t)+mi(0)\n∝ p0(Ci)( q 1− q )mi(t)−mi(0).\nThus, choosing the cell to maximize the posterior probability is equivalent to choosing the cell to maximize log(p0(Ci)) + (mi(t)−mi(0)) log\n( q\n1−q\n) . Thus, if\nmi(0) = log(p0(Ci)) / log\n( q\n1− q\n) , (7)\nthen maximizing the posterior probability that θ is in Ci is equivalent to maximizing mi(t), the first cell selected by CTB is the cell with the largest posterior probability of containing θ, and the second cell selected is the largest among those with a different best arm from the first.\nThus, if one has prior information about the location of θ and an estimate q of a typical value of pij , then a natural way to set mi(0) is via (7). In addition, since p0(Ci) = 0 for empty cells, following (7) also sets mi(0) = −∞ for these cells as discussed before."
    }, {
      "heading" : "7. Numerical Experiments",
      "text" : "In this section, we compare the three variants of CTB described in section 3, CTB-1, CTB-2, and CTB-3, with three benchmarks: Thompson Sampling, Relative Upper Confidence Bound (RUCB) and Winner-Stays (WS).\n• Thompson sampling uses a posterior distribution over θ computed by beginning with a prior distribution on the location of θ, and updating it using Bayes rule and knowledge of pi,j . At time t, it generates θt from this posterior distribution pt and pulls the two arms that θt ranks as best and second best. In our implementation, we track the prior/posterior explicitly by storing a probability for each cell. We emphasize that Thompson sampling as we consider it here requires knowledge of pi,j which is not typically not available.\n• RUCB is as described in Zoghi et al. (2014). We choose it as our benchmark over other algorithms designed for strong regret from the literature because it works well relative to other algorithms designed for strong regret in previous literature when a Condorcet winner exists, and existence of a Condorcet winner is a consequence of our total order assumption. Though there are algorithms that outperform RUCB in some settings such as CCB and SCB (Zoghi et al., 2015), they typically work better when a Condorcet winner does not exist.\n• WS is as described in Chen and Frazier (2017), and is selectetd because it is designed for the weak regret setting. In our plots, WS-W is the variant of WS designed specifically for weak regret.\nWe consider two experimental settings described below, with results pictured in Figure 2.\nSince RUCB performs poorly in both experiments compared with other algorithms, we set the y-axis to emphasize the relative performance of the other algorithms. We include a plot over a wider y-axis showing RUCB’s performance in the supplement.\n7.1 Binary Regret and Constant pi,j\nIn this experimental setting, we set pi,j = 0.8 for all i < j. We have N = 20 arms uniformly generated from the 2-dimensional unit circle. The preference vector θ is generated uniformly at random from the 2-dimensional unit circle. We set regret to 1 if both of the pulled arms are not optimal, i.e. u(θ,A1) = 1 and u(θ,Ai) = 0 for i 6= 1. To satisfy our previous assumption that u(θ,Ai) be distinct across i, we may equivalently set u(θ,Ai) = i · , and take small.\nFigure 2a shows that CTB−1 and CTB−3 perform comparably and both outperform WS-W and Thompson Sampling. CTB−2 does not perform as well as WS-W and Thompson Sampling. Both Thompson sampling and CTB−3 have access to the correct prior and use the true value of p to perform updating.\n7.2 Bradley-Terry Regret and pi,j\nIn this experimental setting, we set utility using the Bradley-Terry model described in section 2. As in the first experimental setting, we have N = 20 arms on the 2-dimensional unit circle. Among these arms, 19 are uniformly generated from {x < 0, y < 0, x2 + y2 = 1} and 1 arm is uniformly generated from {x > 0, y > 0, x2 + y2 = 1}. The user’s preference θ is also uniformly generated from {x > 0, y > 0, x2 + y2 = 1}, but the Bayesian algorithms (CTB−3and Thompson sampling) use another less information prior: that θ is uniform on the unit circle. Thompson sampling performs its update using the true pi,j , while CTB−3 uses a rough approximation of q = 0.6 to set mi(0) to model the fact that we would not know p or pi,j in practice.\nFigure 2b shows that both CTB−3 and Thompson Sampling takes advantage of the prior information and the dependence among arms. CTB−3 uses this information more efficiently and significantly outperforms Thompson Sampling. Among the four algorithms (CTB−1, CTB−2, RUCB and WS) that do not use prior information, CTB−1 performs best. Though CTB−2 does not perform as well as WS at t = 100, 200, it outperforms WS when t = 300, 400, 500."
    }, {
      "heading" : "8. Conclusion",
      "text" : "In this paper, we consider dueling bandits for weak regret, with application to recommender systems and online content recommendation. We formulate a new setting which differs from the traditional dueling bandits in which arms are dependent. We propose an algorithm CTB, and show it has constant expected cumulative regret and strong empirical performance."
    }, {
      "heading" : "Appendix A.",
      "text" : ""
    }, {
      "heading" : "Proof of Lemma 1",
      "text" : "First we prove another lemma.\nLemma 6. Suppose Z(k) is a random walk starting with Z(0) = 0, Z(k + 1) = Z(k) + 1 with probability p > 0.5 and Z(k+ 1) = Z(k)− 1 with probability 1− p. Then for S ∈ N we have\nE [ ∞∑ t=0 1{Z(t) ≤ S} ] = p+ S(2p− 1) (2p− 1)2 . (8)\nProof. Denote A = E[t : mint>1 Z(t) = 0|Z(1) = −1] and B = P (∃t, Z(t) = 0|Z(1) = 1), then we know\nE [ ∞∑ t=0 1{Z(t) ≤ 0} ] = 1 + (1− p) ( A+ E [ ∞∑ t=0 1{Z(t) ≤ 0} ]) + pBE [ ∞∑ t=0 1{Z(t) ≤ 0} ] .\nNow we need to calculate the expression for A and B respectively. Based on the definition of A, we can rewrite A as E[t : mint>1 Z(t) = 1|Z(t) = 0]. It is easy to show that Y (t) := Z(t) − (2p − 1)t is a martingale. Here we define a stopping time τ as min{t > 1 : Z(1) = 1}. Then we know Y (t) stops at τ is a martingale and thus E[Y (τ)] = E[Z(τ)]− (2p− 1)E[τ ] = 0. Thus A = 12p−1 .\nFor B, based on the first step analysis, we know\nB = (1− p) + p×B2.\nSolving this equation, we get B = 1−pp . Plus in A and B’s expression, we have\nE [ ∞∑ t=0 1{Z(t) ≤ 0} ] =\np\n(2p− 1)2 .\nNow we compute E [ ∑∞\nt=0 1{Z(t) ≤ 1}]. Based on the same reasoning, we know\nE [ ∞∑ t=0 1{Z(t) ≤ 1} ] = 1 + (1− p) ( A+ E [ ∞∑ t=0 1{Z(t) ≤ 1} ]) + p× E [ ∞∑ t=0 1{Z(t) ≤ 0} ] . Solving it, we get E [ ∑∞\nt=0 1{Z(t) ≤ 1}] = p+(2p−1) (2p−1)2 . For general S, we have\nE [ ∞∑ t=0 1{Z(t) ≤ S} ] = 1 + (1− p) ( A+ E [ ∞∑ t=0 1{Z(t) ≤ S} ]) + p× E [ ∞∑ t=0 1{Z(t) ≤ S − 1} ] ,\nby induction, we know our Lemma is true.\nNow we return to the proof of Lemma 1.\nProof. Suppose W(t) is a random walk and W (t + 1) = W (t) + 1 with probability p and W (t+ 1) = W (t)− 1 with probability 1-p. Based on the previous Lemma, we just need to show\nE [ ∞∑ t=0 1{Z(t) ≤ S} ] ≤ E [ ∞∑ t=0 1{W (t) ≤ S} ] . (9)\nBecause E[ ∑∞ t=0 1{W (t) ≤ S}] = ∑∞ t=0 P (W (t) ≤ S) and\nP (W (t) ≤ S) = ∑\n2m≥t−S\n( t\nm\n) pt−m(1− p)m ≥ P (Z(t) ≤ S),\nwe know Equation 9 holds true."
    }, {
      "heading" : "Proof of Lemma 4",
      "text" : "Proof. We first prove it for Yt = 0 and x ∈ Hi,j . This is because\npt+1(x) = pt+1(θ ∈ x) = P (θ ∈ x|Yt = 0, pt(·))\n= P (θ ∈ x, Yt = 0, pt(·))\nP (Yt = 0, pt(·))\n= P (θ ∈ x, Yt = 0, pt(·))\nP (Yt = 0, pt(·)|θ ∈ Hi,j)P (θ ∈ Hi,j) + P (Yt = 0, pt(·)|θ /∈ Hi,j)P (θ /∈ Hi,j)\n= pt(x)q\npt(Hi,j)q + (1− pt(Hi,j))(1− q) .\nThe other three cases follow the same reasoning and we omit the proof."
    }, {
      "heading" : "Proof of Lemma 5",
      "text" : "Proof. We prove this lemma using induction. This is obviously true when t=0. Suppose this is true at time t-1. Without loss of generality, we write\npt−1(Ck) = p0(Ci)q\nmi(t−1)−mi(0)(1− q)t−1−mi(t−1)+mi(0)\nM(t− 1) ,\nwhere M(t− 1) is a scaling constant. At time t, suppose we choose Ai and Aj for comparison and Ai wins the duel. Denote M(t) = M(t− 1) ∗ [pt−1(Hij) ∗ q + (1− pt−1(Hi,j))(1− q)], then if Ck ∈ Hi,j :\npt(Ck) = pt−1(Ck)q\npt−1(Hi,j)q + (1− pt−1(Hi,j))(1− q)\n= p0(Ck)q mk(t−1)−mk(0)(1− q)t−1−mk(t−1)+mk(0)q M(t− 1)[pt−1(Hi,j)q + (1− pt−1(Hi,j))(1− q)]\n= p0(Ck)q\nmk(t)−mk(0)(1− q)t−mk(t)+mk(0)\nM(t) ,\nwhere the last line is based on the definition of mk(t) and M(t). Similarly, if Ck /∈ Hij, then\npt(Ck) = pt−1(Ck)(1− q)\npt−1(Hi,j)q + (1− pt−1(Hi,j))(1− q)\n= p0(Ck)q mk(t−1)−mk(0)(1− q)t−1−mk(t−1)+mk(0)(1− q) M(t− 1)[pt−1(Hi,j)q + (1− pt−1(Hi,j))(1− q)]\n= p0(Ck)q\nmk(t)−mk(0)(1− q)t−mk(t)+mk(0)\nM(t) ."
    }, {
      "heading" : "Full Plot of Section 7",
      "text" : "We include a plot which contains full information for RUCB. See Figure 3 for details."
    } ],
    "references" : [ {
      "title" : "Improved algorithms for linear stochastic bandits",
      "author" : [ "Yasin Abbasi-Yadkori", "Dávid Pál", "Csaba Szepesvári" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Abbasi.Yadkori et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Abbasi.Yadkori et al\\.",
      "year" : 2011
    }, {
      "title" : "Reducing dueling bandits to cardinal bandits",
      "author" : [ "Nir Ailon", "Zohar Shay Karnin", "Thorsten Joachims" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Ailon et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Ailon et al\\.",
      "year" : 2014
    }, {
      "title" : "Dueling bandits with weak regret",
      "author" : [ "Bangrui Chen", "Peter Frazier" ],
      "venue" : "In Proceedings of the 34st International Conference on Machine Learning",
      "citeRegEx" : "Chen and Frazier.,? \\Q2017\\E",
      "shortCiteRegEx" : "Chen and Frazier.",
      "year" : 2017
    }, {
      "title" : "Econometric models in marketing, volume 16",
      "author" : [ "Philip Hans Franses", "Alan L Montgomery" ],
      "venue" : null,
      "citeRegEx" : "Franses and Montgomery.,? \\Q2002\\E",
      "shortCiteRegEx" : "Franses and Montgomery.",
      "year" : 2002
    }, {
      "title" : "Active ranking using pairwise comparisons",
      "author" : [ "Kevin G Jamieson", "Robert Nowak" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Jamieson and Nowak.,? \\Q2011\\E",
      "shortCiteRegEx" : "Jamieson and Nowak.",
      "year" : 2011
    }, {
      "title" : "Evaluating the accuracy of implicit feedback from clicks and query reformulations in web search",
      "author" : [ "Thorsten Joachims", "Laura Granka", "Bing Pan", "Helene Hembrooke", "Filip Radlinski", "Geri Gay" ],
      "venue" : "ACM Transactions on Information Systems (TOIS),",
      "citeRegEx" : "Joachims et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Joachims et al\\.",
      "year" : 2007
    }, {
      "title" : "Regret lower bound and optimal algorithm in dueling bandit problem",
      "author" : [ "Junpei Komiyama", "Junya Honda", "Hisashi Kashima", "Hiroshi Nakagawa" ],
      "venue" : "In COLT,",
      "citeRegEx" : "Komiyama et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Komiyama et al\\.",
      "year" : 2015
    }, {
      "title" : "How does clickthrough data reflect retrieval quality",
      "author" : [ "Filip Radlinski", "Madhu Kurup", "Thorsten Joachims" ],
      "venue" : "In Proceedings of the 17th ACM conference on Information and knowledge management,",
      "citeRegEx" : "Radlinski et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Radlinski et al\\.",
      "year" : 2008
    }, {
      "title" : "Mixed logit with repeated choices: households’ choices of appliance efficiency level",
      "author" : [ "David Revelt", "Kenneth Train" ],
      "venue" : "Review of economics and statistics,",
      "citeRegEx" : "Revelt and Train.,? \\Q1998\\E",
      "shortCiteRegEx" : "Revelt and Train.",
      "year" : 1998
    }, {
      "title" : "Linearly parameterized bandits",
      "author" : [ "Paat Rusmevichientong", "John N Tsitsiklis" ],
      "venue" : "Mathematics of Operations Research,",
      "citeRegEx" : "Rusmevichientong and Tsitsiklis.,? \\Q2010\\E",
      "shortCiteRegEx" : "Rusmevichientong and Tsitsiklis.",
      "year" : 2010
    }, {
      "title" : "Interactively optimizing information retrieval systems as a dueling bandits problem",
      "author" : [ "Yisong Yue", "Thorsten Joachims" ],
      "venue" : "In Proceedings of the 26th Annual International Conference on Machine Learning,",
      "citeRegEx" : "Yue and Joachims.,? \\Q2009\\E",
      "shortCiteRegEx" : "Yue and Joachims.",
      "year" : 2009
    }, {
      "title" : "Beat the mean bandit",
      "author" : [ "Yisong Yue", "Thorsten Joachims" ],
      "venue" : "In Proceedings of the 28th International Conference on Machine Learning",
      "citeRegEx" : "Yue and Joachims.,? \\Q2011\\E",
      "shortCiteRegEx" : "Yue and Joachims.",
      "year" : 2011
    }, {
      "title" : "The k-armed dueling bandits problem",
      "author" : [ "Yisong Yue", "Josef Broder", "Robert Kleinberg", "Thorsten Joachims" ],
      "venue" : "Journal of Computer and System Sciences,",
      "citeRegEx" : "Yue et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Yue et al\\.",
      "year" : 2012
    }, {
      "title" : "Relative upper confidence bound for the k-armed dueling bandit problem",
      "author" : [ "Masrour Zoghi", "Shimon Whiteson", "Remi Munos", "Maarten de Rijke" ],
      "venue" : "In JMLR Workshop and Conference Proceedings,",
      "citeRegEx" : "Zoghi et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Zoghi et al\\.",
      "year" : 2014
    }, {
      "title" : "Copeland dueling bandits",
      "author" : [ "Masrour Zoghi", "Zohar S Karnin", "Shimon Whiteson", "Maarten De Rijke" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Zoghi et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Zoghi et al\\.",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 5,
      "context" : "(2008), in contrast with cardinal evaluation obtained from explicit feedback, which is typically difficult to obtain, biased, and requires careful calibration (Joachims et al., 2007; Yue et al., 2012).",
      "startOffset" : 159,
      "endOffset" : 200
    }, {
      "referenceID" : 12,
      "context" : "(2008), in contrast with cardinal evaluation obtained from explicit feedback, which is typically difficult to obtain, biased, and requires careful calibration (Joachims et al., 2007; Yue et al., 2012).",
      "startOffset" : 159,
      "endOffset" : 200
    }, {
      "referenceID" : 11,
      "context" : "Algorithms with order-optimal strong regret, O(N log(T )), in this setting include BTM (Yue and Joachims, 2011), RUCB (Zoghi et al.",
      "startOffset" : 87,
      "endOffset" : 111
    }, {
      "referenceID" : 13,
      "context" : "Algorithms with order-optimal strong regret, O(N log(T )), in this setting include BTM (Yue and Joachims, 2011), RUCB (Zoghi et al., 2014) and RMED (Komiyama et al.",
      "startOffset" : 118,
      "endOffset" : 138
    }, {
      "referenceID" : 6,
      "context" : ", 2014) and RMED (Komiyama et al., 2015).",
      "startOffset" : 17,
      "endOffset" : 40
    }, {
      "referenceID" : 7,
      "context" : "Dueling bandits were introduced by Yue and Joachims (2009), motivated by interactive optimization of web search and other information retrieval systems.",
      "startOffset" : 35,
      "endOffset" : 59
    }, {
      "referenceID" : 5,
      "context" : "The advantage of the dueling bandits formulation over the classical multi-armed bandits formulation in this application setting is that pairwise comparison results can be reliably inferred from implicit feedback, for example through interleaved rankings in Radlinski et al. (2008), in contrast with cardinal evaluation obtained from explicit feedback, which is typically difficult to obtain, biased, and requires careful calibration (Joachims et al.",
      "startOffset" : 257,
      "endOffset" : 281
    }, {
      "referenceID" : 5,
      "context" : "(2008), in contrast with cardinal evaluation obtained from explicit feedback, which is typically difficult to obtain, biased, and requires careful calibration (Joachims et al., 2007; Yue et al., 2012). Dueling bandits have been studied most frequently assuming strong regret, in which the regret is 0 if and only if both pulled arms are optimal. Several algorithms have been devised that assume the existence of a Condorcet winner, i.e., one that is preferred in comparison with each other arm. Algorithms with order-optimal strong regret, O(N log(T )), in this setting include BTM (Yue and Joachims, 2011), RUCB (Zoghi et al., 2014) and RMED (Komiyama et al., 2015). Zoghi et al. (2015) points out points out that a Condorcet winner does not necessarily exist, and that its probability of existence decreases dramatically with the number of arms.",
      "startOffset" : 160,
      "endOffset" : 688
    }, {
      "referenceID" : 8,
      "context" : "This framework includes the commonly used logit or Bradley-Terry (Revelt and Train, 1998; Yue et al., 2012) and probit models (Franses and Montgomery, 2002).",
      "startOffset" : 65,
      "endOffset" : 107
    }, {
      "referenceID" : 12,
      "context" : "This framework includes the commonly used logit or Bradley-Terry (Revelt and Train, 1998; Yue et al., 2012) and probit models (Franses and Montgomery, 2002).",
      "startOffset" : 65,
      "endOffset" : 107
    }, {
      "referenceID" : 3,
      "context" : ", 2012) and probit models (Franses and Montgomery, 2002).",
      "startOffset" : 26,
      "endOffset" : 56
    }, {
      "referenceID" : 9,
      "context" : "Our exploitation of arm features is similar in spirit to work in the traditional (cardinal) multi-armed bandit setting on linear bandits (Rusmevichientong and Tsitsiklis, 2010; AbbasiYadkori et al., 2011).",
      "startOffset" : 137,
      "endOffset" : 204
    }, {
      "referenceID" : 1,
      "context" : "Ailon et al. (2014) considered strong utility-based regret, in which each arm has a utility score from which preferences are derived, and the regret for failing to pull the maximum utility arm twice is a function of that maximal arm’s utility and the utilities of the pulled arms.",
      "startOffset" : 0,
      "endOffset" : 20
    }, {
      "referenceID" : 1,
      "context" : "Ailon et al. (2014) considered strong utility-based regret, in which each arm has a utility score from which preferences are derived, and the regret for failing to pull the maximum utility arm twice is a function of that maximal arm’s utility and the utilities of the pulled arms. Bandits have also been considered, though less frequently, in the weak regret setting, introduced by Yue et al. (2012), in which regret is 0 if either of the pulled arms is optimal.",
      "startOffset" : 0,
      "endOffset" : 400
    }, {
      "referenceID" : 1,
      "context" : "Ailon et al. (2014) considered strong utility-based regret, in which each arm has a utility score from which preferences are derived, and the regret for failing to pull the maximum utility arm twice is a function of that maximal arm’s utility and the utilities of the pulled arms. Bandits have also been considered, though less frequently, in the weak regret setting, introduced by Yue et al. (2012), in which regret is 0 if either of the pulled arms is optimal. This setting is more appropriate for recommender systems, in which we offer the user a pair of items, and she selects the one that is preferred. 0 regret is incurred as long as the best item is made available. While Yue et al. (2012) introduced weak regret, an algorithm with regret bounds first appeared in Chen and Frazier (2017), which proposed the Winner Stays (WS) algorithm that achieves O(N log(N)) cumulative binary weak regret when arms have a total order and O(N2) in the Cordorcet winner setting.",
      "startOffset" : 0,
      "endOffset" : 697
    }, {
      "referenceID" : 1,
      "context" : "Ailon et al. (2014) considered strong utility-based regret, in which each arm has a utility score from which preferences are derived, and the regret for failing to pull the maximum utility arm twice is a function of that maximal arm’s utility and the utilities of the pulled arms. Bandits have also been considered, though less frequently, in the weak regret setting, introduced by Yue et al. (2012), in which regret is 0 if either of the pulled arms is optimal. This setting is more appropriate for recommender systems, in which we offer the user a pair of items, and she selects the one that is preferred. 0 regret is incurred as long as the best item is made available. While Yue et al. (2012) introduced weak regret, an algorithm with regret bounds first appeared in Chen and Frazier (2017), which proposed the Winner Stays (WS) algorithm that achieves O(N log(N)) cumulative binary weak regret when arms have a total order and O(N2) in the Cordorcet winner setting.",
      "startOffset" : 0,
      "endOffset" : 795
    }, {
      "referenceID" : 1,
      "context" : "Ailon et al. (2014) considered strong utility-based regret, in which each arm has a utility score from which preferences are derived, and the regret for failing to pull the maximum utility arm twice is a function of that maximal arm’s utility and the utilities of the pulled arms. Bandits have also been considered, though less frequently, in the weak regret setting, introduced by Yue et al. (2012), in which regret is 0 if either of the pulled arms is optimal. This setting is more appropriate for recommender systems, in which we offer the user a pair of items, and she selects the one that is preferred. 0 regret is incurred as long as the best item is made available. While Yue et al. (2012) introduced weak regret, an algorithm with regret bounds first appeared in Chen and Frazier (2017), which proposed the Winner Stays (WS) algorithm that achieves O(N log(N)) cumulative binary weak regret when arms have a total order and O(N2) in the Cordorcet winner setting. These bounds on binary weak regret have corresponding bounds on utility-based weak regret inflated by the difference in utility between the best and worst arms. We consider utility-based weak regret, in the total order setting, when the total order is induced by a utility which is in turn a function of observable arm features, an unknown latent preference vector, and a known utility function. This framework includes the commonly used logit or Bradley-Terry (Revelt and Train, 1998; Yue et al., 2012) and probit models (Franses and Montgomery, 2002). We provide an algorithm, Comparing with the Best (CTB) that has expected cumulative utility-based weak regret that is constant in T , and that leverages the dependence between preferences over arms induced by the arm features and utility function to provide excellent empirical performance when prior information is available. While our regret bound’s dependence on N is looser than Chen and Frazier (2017) (our dependence is 2N in the worst case, and is N2d when the utility function is linear over a d-dimensional space of preferences and arm features), our algorithm is more flexible in its ability to problem structure induced by the feature vectors, and outperforms it empirically by a substantial margin when N is small enough to allow computation that fully takes advantage of this problem structure.",
      "startOffset" : 0,
      "endOffset" : 1932
    }, {
      "referenceID" : 8,
      "context" : "For example, our framework includes the logit or Bradley-Terry model (Revelt and Train, 1998; Yue et al., 2012), in which d′ = d, the utility function is u(θ,Ai) = θ ·Ai and pi,j = exp(u(θ,Ai)) exp(u(θ,Ai)+u(θ,Aj)) .",
      "startOffset" : 69,
      "endOffset" : 111
    }, {
      "referenceID" : 12,
      "context" : "For example, our framework includes the logit or Bradley-Terry model (Revelt and Train, 1998; Yue et al., 2012), in which d′ = d, the utility function is u(θ,Ai) = θ ·Ai and pi,j = exp(u(θ,Ai)) exp(u(θ,Ai)+u(θ,Aj)) .",
      "startOffset" : 69,
      "endOffset" : 111
    }, {
      "referenceID" : 3,
      "context" : "Our framework also includes the probit model (Franses and Montgomery, 2002) in which d′ = d and the utility function is the inner product as with the logit model, but pi,j = Φ(u(θ,Ai)− u(θ,Aj)) where Φ(·) is the standard normal cdf.",
      "startOffset" : 45,
      "endOffset" : 75
    }, {
      "referenceID" : 4,
      "context" : "Moreover, when the utility function is linear and d′ = d, results in Jamieson and Nowak (2011) show M ′ is O(N2d).",
      "startOffset" : 69,
      "endOffset" : 95
    }, {
      "referenceID" : 14,
      "context" : "Though there are algorithms that outperform RUCB in some settings such as CCB and SCB (Zoghi et al., 2015), they typically work better when a Condorcet winner does not exist.",
      "startOffset" : 86,
      "endOffset" : 106
    }, {
      "referenceID" : 12,
      "context" : "• RUCB is as described in Zoghi et al. (2014). We choose it as our benchmark over other algorithms designed for strong regret from the literature because it works well relative to other algorithms designed for strong regret in previous literature when a Condorcet winner exists, and existence of a Condorcet winner is a consequence of our total order assumption.",
      "startOffset" : 26,
      "endOffset" : 46
    }, {
      "referenceID" : 2,
      "context" : "• WS is as described in Chen and Frazier (2017), and is selectetd because it is designed for the weak regret setting.",
      "startOffset" : 24,
      "endOffset" : 48
    } ],
    "year" : 2017,
    "abstractText" : "We study dueling bandits with weak utility-based regret when preferences over arms have a total order and carry observable feature vectors. The order is assumed to be determined by these feature vectors, an unknown preference vector, and a known utility function. This structure introduces dependence between preferences for pairs of arms, and allows learning about the preference over one pair of arms from the preference over another pair of arms. We propose an algorithm for this setting called Comparing The Best (CTB), which we show has constant expected cumulative weak utility-based regret. We provide a Bayesian interpretation for CTB, an implementation appropriate for a small number of arms, and an alternate implementation for many arms that can be used when the input parameters satisfy a decomposability condition. We demonstrate through numerical experiments that CTB with appropriate input parameters outperforms all benchmarks considered.",
    "creator" : "LaTeX with hyperref package"
  }
}