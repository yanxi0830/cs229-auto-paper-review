{
  "name" : "1706.01383.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Sparse Stochastic Bandits",
    "authors" : [ "Joon Kwon", "Vianney Perchet" ],
    "emails" : [ "joon.kwon@ens-lyon.org", "vianney.perchet@normalesup.org", "claire.vernade@telecom-paristech.fr" ],
    "sections" : [ {
      "heading" : null,
      "text" : "√ d in the minimax sense). We here con-\nsider the sparse case of this classical problem in the sense that only a small number of arms, namely s < d, have a positive expected reward. We are able to leverage this additional assumption to provide an algorithm whose regret scales with s instead of d. Moreover, we prove that this algorithm is optimal by providing a matching lower bound – at least for a wide and pertinent range of parameters that we determine – and by evaluating its performance on simulated data.\n*J. Kwon was supported by a public grant as part of the Investissement d’avenir project, reference ANR-11-LABX-0056-LMH. V. Perchet has benefitted from the support of the ANR (grant ANR13-JS01-0004-01), of the FMJH Program Gaspard Monge in optimization and operations research (supported in part by EDF) and from the Labex LMH. C. Vernade was also partially supported by the Machine Learning for Big Data Chair at Télécom ParisTech.\nAccepted for presentation at Conference on Learning Theory (COLT) 2017\nar X\niv :1\n70 6.\n01 38"
    }, {
      "heading" : "1 Introduction",
      "text" : "We consider the celebrated stochastic multi-armed bandit problem Robbins (1985), where a decision maker sequentially samples from d > 1 processes, also called arms, aiming at maximizing its cumulative reward. Specifically, those arms are characterized by their distributions ν1, . . . , νd and pulling arm i ∈ [d] := {1, ..., d} at time t yields a reward Xi(t) ∼ νi, the sequence (Xi(t))t>1 being assumed to be i.i.d. There are many motivations behind the study of those models, ranging from random clinical trials, to maximization of the click through rate, portfolio optimization, etc.\nAn algorithm (or policy) maps anterior observations to the next arm I(t) ∈ [d] to be pulled. The performance of a given algorithm is evaluated by its cumulative regret Reg(T ) defined as the difference between the rewards gathered by the sequence (I(t))16t6T and those that might have been obtained in expectation by always behaving optimally, that is, by pulling the arm with maximal mean µi := Eνi [X] at each round:\nReg(T ) = Tµ∗ − T∑ t=1 XI(t)(t) where µ∗ = max i∈[d] µi .\nThe classical multi-armed bandit problem is now well understood, and there exist algorithms minimizing the regret such that\nE [Reg(T )] . ∑ i∈[d] ∆i>0 log(T ) ∆i , where ∆i = µ∗ − µi ,\nand where the notation . indicates that the inequality holds up to some universal multiplicative constants and some additive constants1. Converse statements have also been proved, first by Lai and Robbins (1985) and then by Burnetas and Katehakis (1996): Any consistent policy (i.e. whose regret is always less than Tα for all α > 0) always have a regret larger than ∑d i=1 log(T ) ∆i\n(again, up to some constants). When T is fixed and the parameters µi are chosen to maximize regret, the\ndistribution-independent bounds are of order √ dT as first shown in Cesa-Bianchi and Lugosi (2006).\nThe main drawback of those results is that the regret scales linearly with the number of arms d,or with √ d in the minimax analysis. Since upper and lower bounds match, this is actually ineluctable. On the other hand, we aim at leveraging 1We focus, for the sake of clarity, on the leading terms in T with explicit dependencies in the different parameters of the problems.\nan additional assumption to reduce that (linear) dependency in d and even get rid of it, if possible. We therefore define and investigate the sparse bandit problem (SPB) where the decision maker knows a priori that only s of the d arms have a significant mean µi.\nSpecifically, we assume that exactly s arms have positive means2. Without loss of generality, we number the arms in nonincreasing order and write\nµ∗ = µ1 > µ2 > · · · > µs > 0 > µs+1 > · · · > µd.\nA key quantity will be the lowest positive mean µs: if µs is arbitrarily close to 0, then the sparsity assumption is useless. On the other side of the spectrum, if µs 0, then the sparsity assumption will turn out to be helpful.\nInformally, we aim at replacing the dependency in the total number of arms d with the same dependency in the number s of arms with positive means. In other words, we wish to achieve an upper bound of the following kind\nE [Reg(T )] . ∑ i∈[s] ∆i>0 log(T ) ∆i ,\nwhenever this is possible. Notice that the above is precisely the optimal regret bound if the agent knew in advance which are s arms with positive means. In the worst case, this gives a distribution independent upper bound of the order of √ sT\ninstead of the classical √ dT (up to logarithmic terms).\nThe Sparse Bandit problem is therefore a variation of the classical stochastic multi-armed bandit problem (see Bubeck and Cesa-Bianchi (2012) for a survey) in which the agent knows the number of arms with positive means.\nThere have been some works regarding sparsity assumptions in bandit problems. In the full information setting, some of them focus on sparse reward vectors, i.e., at most s components of (X1(t), . . . , Xd(t)) are positive (see for instance Langford et al. (2009); Kwon and Perchet (2016)). Another considered problem is the one of sparse linear bandits Carpentier et al. (2012); Abbasi-Yadkori et al. (2012); Gerchinovitz (2013); Lattimore et al. (2015) in which the underlying unknown vector of parameter is assumed to be sparse – that is with a constraint on its L1 norm – or even spiky in the crude and very specific case where s = 1 Bubeck et al. (2013). However, none of the previously cited work tackles the following concrete problem. Assume that you are planning a marketing campaign for wich you have thousands of possible products to display. Most probably, many of them\n2Equivalently, we could be given a threshold τ and the exact number s of arms with means strictly greater than τ .\nwill be similar and have similar, very low expected returns but you do not have any possibility to know that in advance. In many common datasets such as Yandex’s one 3, depending on the query it is usual to have only 50 items out of 1500 that can be considered as relevant. Consequently, in order to avoid exploring thoses bad items, you want to be able to set rules to eliminate them as quickly as possible and get a regret that scales in the number of good arms."
    }, {
      "heading" : "1.1 Contributions",
      "text" : "We introduce and investigate the sparse bandits problem by deriving an asymptotic lower bound on the regret. We give an analogous result to the seminal bound of Lai and Robbins (1985), and we construct an anytime algorithm SPARSEUCB that uses the optimistic principle of Auer et al. (2002) together with the sparsity information available in order to reach optimal performance, up to constant terms.\nConcretely, the lower bound that we prove distinguishes the possible behavior of any uniformly efficient algorithm according to the value of the sparsity information available to the agent. To fix ideas, assume that µ1 = 1 and for 2 6 i 6 s, µi = µ, ∆i = ∆ = 1− µ. Then, we show that, if ds > ∆µ2 + 1, the sparsity of the problem is highly relevant so that regret is asymptotically lower-bounded as\nlim inf T→+∞\nReg(T )\nlog(T ) > max { s 2∆ , s∆ 2µ2 } = s 2∆ , if µ > 1 2 .\nThe performance of the SPARSEUCB algorithm matches the lower bound as it guarantees\nReg(T ) . max {s log(T )\n2∆ , s∆ log(T ) 2µ2\n} = s log(T )\n2∆ , if µ >\n1 2 ."
    }, {
      "heading" : "2 The Stochastic Sparse Bandits Problem",
      "text" : "We consider the classical stochastic multi-armed bandit problem, where a decision maker samples sequentially from d ∈ N i.i.d. processes ((Xi(t))t>1)i∈[d]. We will keep denoting by νi the probability distribution of Xi(t) and Eνi [Xi(t)] = µi its mean.\nThe decision maker pulls at stage t > 1 an arm I(t) ∈ [d], and receives reward Xi(t) which is his only observation (specifically, he does not observeXi(t) for i 6= I(t)). The (expected) cumulated reward of the decision maker after T > 1 stages is then ∑T t=1 µI(t) and his performance is evaluated through his regret, defined as the\n3see https://www.kaggle.com/c/yandex-personalized-web-search-challenge\ndifference between the highest possible expected reward (had the means µ1, . . . , µd been known in advance), and the actual reward. In other words:\nReg(T ) := Tµ∗ − T∑ t=1 XI(t)(t), where µ∗ = max i∈[d] µi.\nIf we introduce the notations ∆i = µ∗ − µi and Ni(t) := ∑t−1\nτ=1 1{I(τ) = i}, the number of times the decision maker pulled arm i up to time t−1, then the expected regret writes\nE [Reg(T )] = d∑ i=1 ∆iE [Ni(T + 1)] .\nThis expression indicates that the (expected) regret should scale with d. And this is indeed the case without further assumption to leverage.\nAssumption 1. s arms have positive means (i.e., µi > 0), while the other d − s arms have nonpositive means (i.e., µi′ 6 0).\nAn arm with positive (resp. nonpositive) mean will also be refered to as good (resp. bad). In the remaining of the paper, we will assume, without loss of generality and to simplify notation, that the means are re-ordered in nonincreasing order:\nµ1 > µ2 > . . . > µs > 0 > µs+1 > . . . > µd .\nWe will also denote by Xi(n) the empirical mean of the n first realizations of arm i so that\nXi(Ni(t)) = 1\nNi(t) t−1∑ τ=1 1{I(τ) = i}Xi(τ),\nwith the convention that Xi(0) = 0. Besides, we assume that the distributions νi are sub-Gaussians, meaning that for all arm i ∈ [d] and all a > 0 and t > 1, we have\nP [|Xi(t)− µi| > a] 6 2e−a 2/2.\nFor instance, this is the case if the Xi(t) are assumed to be bounded, with support included in [−1, 1]. Together with a Chernoff bound, one can easily see that this implies for all arm i ∈ [d] and all a > 0 and n > 1,\nP [ Xi(n)− µi > a ] 6 e−na 2/2."
    }, {
      "heading" : "3 Lower Bound",
      "text" : "This section is devoted to proving a lower bound on the regret of any uniformly efficient algorithm for the sparse bandit problem. To avoid too heavy expressions, the lower bound we establish holds for problems where the bad arms have a null expected reward, though handling general negative means does not require huge modifications from the given proof. Our goal is to provide a result that is easily generalizable to any stochastic bandit problem containing a sparsity information in the form of a threshold on the values of the expected return of the arms of interest. A generalization of the presented bound can be found in Appendix B.\nDefinition 1. An algorithm is uniformly efficient if for any sparse bandit problem and all α ∈ (0, 1], its expected regret satisfies E [Reg(T )] = o(Tα).\nWe state the bound for Gaussian bandit models with a fixed variance equal to 1/4. In that case, a distribution is simply characterized by its mean µ and the Kullback-Leibler (KL) divergence between two models µ and µ′ is equal to 2(µ− µ′)2. Consider\nS(d, s) = { µ = (µ1, . . . , µd) ∈ Rd+ ∣∣∣µ has exactly s positive components} . Theorem 1. Let µ ∈ S(d, s) such that its components are nonincreasing:\nµ1 > µ2 > · · · > µs > µs+1 = · · · = µd = 0,\nand we denote ∆i = µ1 − µi for all i ∈ [d]. Then, for any uniformly efficient algorithm, played against arms whose distributions are Gaussian with variance 1/4 and with respective means µ1, . . . , µd, one of the following asymptotic lower bounds hold.\n• If d−sµ1 − ∑\ni∈[s] ∆i>0\n∆i µ2i > 0,\nlim inf T→∞\nReg(T ) log(T ) > ∑ i∈[s] ∆i>0 max { 1 2∆i , ∆i 2µ2i } (1)\n• otherwise, there exist k 6 s such that d−sµ1 − ∑s i=k ∆i µ2i\n< 0, and the lower bound is\nlim inf T→∞\nReg(T ) log(T ) > ∑ i∈[k] ∆i>0 1 2∆i + s∑ i=k+1 µ2k µ2i ∆i 2∆2k + (d− s) 2µ1 ( 1− µ 2 k ∆2k ) . (2)\nRemarks. Since the decision maker has more knowledge on the parameters of the problem than in the classical multi-armed bandit problem, we expect the lower bound to be less than the traditional one (without the sparsity assumption), which is\nlim inf T→∞\nReg(T ) log(T ) > ∑ i∈[d] ∆i>0 1 2∆i .\nIndeed, since the max is smaller than the sum, in the first case, we have∑ i∈[s] ∆i>0 max { 1 2∆i , ∆i 2µ2i } 6 ∑ i∈[s] ∆i>0 1 2∆i + ∑ i∈[s] ∆i>0 ∆i 2µ2i 6 ∑ i∈[s] ∆i>0 1 2∆i + d− s µ1 = ∑ i∈[d] ∆i>0 1 2∆i .\nSimilarly, in the second case, we get that k∑ i=1 1 2∆i + s∑ i=k+1 µ2k µ2i ∆i 2∆2k + (d− s) 2µ1 ( 1− µ 2 k ∆2k ) = ∑ i∈[d] ∆i>0 1 2∆i − µ 2 k ∆2k ( d− s 2µ1 − s∑ i=k+1 ∆i 2µ2i︸ ︷︷ ︸\n>0\n) − s∑ i=k+1 1 2∆i .\nMoreover, if ∆s µ2s > d−sµ1 , then both lower bounds match. Stated otherwise, the sparsity assumption is irrelevant as soon as\nµs 6 µ1 −1 +\n√ 1 + 4(d− s)\n2(d− s) ' µ1√ d− s .\nProof. The proof relies on changes of measure arguments originating from Graves and Lai (1997). First, consider the set of changes of distributions that modify the best arm without changing the marginal of the best arm in the original sparse bandit problem:\nB(µ) = { µ′ ∈ S(d, s) ∣∣∣∣µ′1 = µ1 and maxi∈[d] µi < maxi∈[d] µ′i } .\nConcretely, if one considers an alternative sparse bandit model µ′ such that one of the originally null arms becomes the new best arm, then one of the originally non-null arms in µ must be taken to zero in µ′ in order to keep the sparse structure of the problem.\nIn general, the equivalent of Th. 17 in (Kaufmann et al., 2015) or Proposition 3 in (Lagrée et al., 2016) can be stated in our case as follows: For all changes of measure µ′ ∈ B(µ),\nlim inf T→∞\n∑d i=1 2E[Ni(T + 1)](µi − µ′i)2\nlog(T ) > 1. (3)\nDetails on this type of informational lower bounds can be found in Garivier et al. (2017.To appear.) and references therein. Now, following general ideas from Graves and Lai (1997) and lower bound techniques from Lagrée et al. (2016) and Combes et al. (2015), we may give a variational form of the lower bound on the regret satisfying the above constraint.\nlim inf T→∞\nReg(T )\nlog(T ) > inf c∈C ∑ i∈[d] ci∆i, (4)\nwhere the set C corresponds to constraints that are directly implied by Eq.(3) above:\nC = (ci)i∈[d] ∈ Rd+ ∣∣∣∣∣∣ ∀µ′ ∈ B(µ), 2 ∑ i∈[d] ci(µi − µ′i)2 > 1  . We aim at obtaining a lower bound of the infimum from Eq.(4). In this constrained optimization problem, the constraints set is huge because every change of measure µ′ ∈ B(µ) must satisfy (3). On the other side, relaxing some constraints – or considering only a subset of B(µ)– simply allows to reach even lower values 4.\nWe consider C̃ defined as C̃ = {\n(ci)i∈[d] ∈ Rd+ ∣∣∣∣∣ for all i ∈ [s] \\ {1} and j ∈ [d] \\ [s], ci∆ 2 i > 1/2\ncjµ 2 1 + ciµ 2 i > 1/2\n}\nand we prove that C is a subset of C̃, namely that there are more acceptable vectors of coefficients in C allowing us to reach lower values of the argument.\n4At that point, we may lose the optimality of the finally obtained lower bound but this is a price we accept to pay in order to obtain a computable solution.\nLet (ci)i∈[d] ∈ C and let us prove that it belongs to C̃. Let i ∈ [s] \\ {1}, γ > 0 and consider µ(i,γ) ∈ Rd defined as the following modification of µ:\nµ (i,γ) k = { µ1 + γ if k = i µk otherwise, k ∈ [d].\nWe easily see that µ(i,γ) belongs to B(µ). Therefore, by definition of C, (ci)i∈[d] satisfies: ∑\nk∈[d]\nck(µk − µ(i,γ)k )2 > 1\n2 ,\nwhich, by definition of µ(i,γ) boils down to ci(∆i + γ)2 > 1/2. This being true for all γ > 0, we have ci∆2i > 1/2. The first condition in the defintion of C̃ is then satisfied.\nSimilary, for i ∈ [s] \\ {1}, j ∈ [d] \\ [s] and γ > 0 we consider µ(i,j,γ) ∈ Rd defined by:\nµ (i,j,γ) k =  0 if k = i µ1 + γ if k = j µk otherwise, k ∈ [d].\nµ(i,j,γ) also belongs to B(µ). By definition of C, (ci)i∈[d] satisfies:∑ k∈[d] ck(µk − µ(i,j,γ)k )2 > 1 2 ,\nwhich boils down to ciµ2i + cj∆ 2 j > 1/2 (after taking the infinimum over γ > 0). Since ∆j = µ1, the second condition in the definition of C̃ is satisfied. We have proved that (ci)i∈[d] belongs to C̃ and consequently that C is a subset of C̃. Therefore,\nlim inf T→+∞\nReg(T )\nlog(T ) > inf c∈C ∑ i∈[d] ci∆i > inf c∈C̃ ∑ i∈[d] ci∆i.\nThe computation of the optimization problem over C̃ is deferred to Appendix A.\nRemark 1. This is a linear optimization problem under inequality constraints so there exist algorithmic methods such as the celebrated Simplex algorithm Dantzig (2016) to compute a numeric solution of it. Nonetheless, in our case, it is possible to give an explicit solution."
    }, {
      "heading" : "4 Sparse UCB",
      "text" : ""
    }, {
      "heading" : "4.1 The SPARSEUCB algorithm",
      "text" : "The SPARSEUCB algorithm is formally defined in Algorithm 1 but let us first provide an informal description. The algorithm can be in different phases (denoted r, f and u), depending on past observations, and its behavior radically changes from one phase to another. At each time t > 1, the variable ω(t) ∈ {r, f, u} will specify the phase the algorithm is in. The variable is not useful for the algorithm itself, but will be handy for reference in the analysis. We now describe the different phases.\nRound-robin The algorithm starts with a round-robin phase, which corresponds to ω(t) = r. Each of the d arms is pulled once successively.\nThen, for each time t > d+ 1, the following sets are defined:\nJ (t) := { i ∈ [d] ∣∣∣∣∣Xi(Ni(t)) > 2 √ log(Ni(t))\nNi(t)\n} ,\nK(t) := { i ∈ [d] ∣∣∣∣∣Xi(Ni(t)) > 2 √ log(t)\nNi(t)\n} .\nWe will refer to the arms in J (t) as the active arms and those in K(t) as active and sufficiently sampled.\nIf there are less than s active arms, i.e., |J (t)| < s, the algorithm enters a round-robin phase, and pulls each arm successively. This implies that ω(t) = r for the next d stages.\nForce-log If there are at least s active arms, but less than s sufficiently sampled arms (|K(t)| < s), the algorithm enters a force-log phase (ω(t) = f). In this phase, the algorithm pulls any arm in the set J (t) \\ K(t).\nUCB If the set K(t) contains at least s arms, the algorithm enters a UCB phase (ω(t) = u). The algorithm selects an arm in K(t) according to the UCB rule, i.e. it chooses the arm i ∈ K(t) which maximizes the quantity:\nXi(Ni(t)) + 2\n√ log(t)\nNi(t) .\nThe pseudo-code of the whole procedure is given in Algorithm 1 and the skeleton\nin Figure 2.\nInput: the total number of arms d and the number of arms with positive means s Initialization: t← 1\nfor k = 1 . . . d do /* round-robin */ I(t)← k ; ω(t)← r ; t← t+ 1 ; end while t 6 T do\nCompute J (t)← { i ∈ [d] ∣∣∣Xi(Ni(t)) > 2√ log(Ni(t))Ni(t) } Compute K(t)← { i ∈ [d]\n∣∣∣Xi(Ni(t)) > 2√ log(t)Ni(t)} if |J (t)| < s then\nfor k = 1 . . . d do /* round-robin */ I(t)← k ; ω(t)← r ; t← t+ 1 ;\nend else if |K(t)| < s then /* force-log */\nI(t) ∈ J (t) \\ K(t) ; ω(t)← f ; t← t+ 1 ;\nelse /* UCB */ I(t) ∈ arg maxi∈K(t) { Xi(Ni(t)) + 2 √ log(t) Ni(t) } ; ω(t)← u ; t← t+ 1 ;\nend end\nAlgorithm 1: SPARSEUCB The broad idea is that the algorithm should quickly identify the s good arms, and then pull those arms according to an UCB rule (or, alternatively, any other\npolicy). At the end, only those s good arms would be pulled an infinite number of times.\nThe set J (t) of active arms is defined in such a way that the expected number of pulls needed for a good arm to become active is finite. Therefore, only a finite number (in expectation) of round-robin phases is needed for all s good arms to become active (see Lemma 7).\nReciprocally, a bad arm (with non-positive mean) is only pulled while active, that is a finite number of times in expectation. The main issue occurs when a bad arm happens to be active. In that case, the delay between two successive pulls of an active null arm typically increases exponentially fast because the regret scales with log(t). Consequently, it would take an exponential number of stages for this arm to become inactive again. And this could be dramatic for the regret if the best arm was, at the same time, inactive, as the regret would increase by a fixed constant of at least ∆2 on all those stages. The purpose of the force-log phases is to make sure that each active arm gets pulled sufficiently often so that the expected number of steps a bad arm remains active is finite. If the best arm happened to be inactive, then the number of active arms would drop below s, and performing a round-robin phase would allow it to quickly become active again.\nTheorem 2. The SPARSEUCB algorithm guarantees\nE [Reg(T )] . log(T ) ∑ i∈[s] ∆i>0 ( 1 ∆i + ∆i µ2i ) ,\nwhere notation . removes universal multiplicative constants and additive datadependant constants. The detailed statement can be found in Appendix C."
    }, {
      "heading" : "4.2 Sketch of the proof",
      "text" : "We decompose the event {I(t) = i} of a good arm i ∈ [s] being pulled at time t > 1 with respect to the different phases of SPARSEUCB:\n{I(t) = i} = Ri(t) t Fi(t) t Ui(t) t Vi(t), t > 1, (5)\nwhere the different events are defined as follows:\n• Ri(t) := {I(t) = i, ω(t) = r} is event of arm i being pulled at time t during a round-robin phase;\n• Fi(t) := {I(t) = i, ω(t) = f} is the event of arm i being pulled at time t during a force-log phase;\n• Ui(t) := {I(t) = i, ω(t) = u, 1 ∈ K(t)} is the event of arm i being pulled at time t during a UCB phase while the optimal arm is active and sufficiently sampled;\n• Vi(t) := {I(t) = i, ω(t) = u, 1 6∈ K(t)} is the event of arm i being pulled at time t during a UCB phase while the optimal arm is not active or not sufficiently sampled.\nFor the bad arms i ∈ {s+1, . . . , d}, we consider a simpler decomposition. For t > d+ 1, we introduce Ai(t) := {I(t) = i, i ∈ J (t)}, which is the event of arm i being pulled at time t while active, so that\n{I(t) = i} = Ri(t) tAi(t), t > 1,\nsee, e.g. Property (v) from Lemma 6.\nUsing the above decompositions, we can write the regret as:\nE [Reg(T )] = ∑ i∈[d] ∆i>0 ∆iE [ T∑ t=1 1 {Ri(t)} ] + ∑ i∈[s] ∆i>0 ∆iE [ T∑ t=1 1 {Fi(t)} ] + ∑ i∈[s] ∆i>0 ∆iE [ T∑ t=1 1 {Ui(t)} ]\n+ ∑ i∈[s] ∆i>0 ∆iE [ T∑ t=1 1 {Vi(t)} ] + d∑ i=s+1 ∆iE [ T∑ t=1 1 {Ai(t)} ] .\nWe upper-bound independently the five above quantities. For the sake of clarity, we only provide here the main ideas of proof. A detailed analysis can be found in Appendix C.\nLemma 1. The regret induced the round-robin phases is controlled by:\nE [ +∞∑ t=1 1 {Ri(t)} ] 6 1 + 3s+ 8 s∑ j=1 1 µ2j ( 1 + 4 log ( 16 µ2j )) , i ∈ [d].\nMain argument of proof. The algorithm performs a round-robin phase only if less than s arms are active, thus necessarily when one of the good arms j ∈ [s] is not active. This implies that Xj(Nj(t)) < 2 √ log(Nj(t)) Nj(t)\n. The probability of this happening decreases exponentially fast and as a consequence, the expected number of round-robin phases is bounded. 2\nLemma 2. The regret induced by a good arm i ∈ [s] during force-log phases is controlled by:\nE [ T∑ t=1 1 {Fi(t)} ] 6 16 log(T ) + 8 µ2i .\nMain argument of proof. Arm i ∈ [s] is pulled during a force-log phase if its empirical mean is below 2 √ log(t) Ni(t)\n. Because arm i has a positive mean, the probability of this happening turns out to decrease exponentially, as soon as Ni(t) >\n16 log(T ) µ2i . Therefore, the expected number of times arm i is pulled during\na force-log phase is bounded by 16 log(T ) µ2i plus a constant term. 2\nLemma 3. The regret induced by a good arm i ∈ [s] during UCB phases, while 1 ∈ K(t), is controlled by\nE [ T∑ t=1 1 {Ui(t)} ] 6 16 log(T ) + 8 ∆2i + 3.\nMain argument of proof. The proof basically follows the steps of the classic UCB analysis by Auer et al. (2002). 2\nLemma 4. The regret induced by good arms during UCB phases, while 1 6∈ K(t), is controlled by: ∑\ni∈[s]\n∆iE [ T∑ t=1 1 {Vi(t)} ] 6 d∆sπ 2 6 .\nMain argument of proof. The algorithm performs a UCB phase if the setK(t) has at least s arms. If it does not contain the best arm 1, it must necessarily contain an arm j ∈ {s+ 1, . . . , d}, i.e. with nonpositive mean. As a consequence, the\nempirical mean of arm j is above 2 √\nlog(t) Nj(t) . Because arm j has a nonpositive mean,\nthe probability of this happening turns out to decrease as t−1/2. Consequently, the expected number of times a good arm is pulled during a UCB phase while the best arm does not belong to K(t) is finite. 2\nLemma 5. The regret induced by a bad arm i ∈ {s+ 1, . . . , d} while active is controlled by:\nE [ +∞∑ t=1 1 {Ai(t)} ] 6 π2 6 .\nMain argument of proof. Arm i is active if its empirical mean is above 2 √\nlog(Ni(t)) Ni(t) . Because its mean is nonpositive, this happens with a total probability of the order of ∑ t−2. Therefore, the regret incurred when i is active is bounded. 2\nIt only remains to combine the above results, to upper bound the expected regret as\nE [Reg(T )] . log(T ) ∑ i∈[s] ∆i>0 ( 1 ∆i + ∆i µ2i ) + d s∑ j=1 µ1 log(1/µ 2 j ) µ2j ,\nwhere we omitted multiplicative universal constants. We emphasize the fact that the last term is independent of T , hence the dominating term is\nE [Reg(T )] . log(T ) ∑ i∈[s] ∆i>0 max { 1 ∆i , ∆i µ2i } ."
    }, {
      "heading" : "5 Optimality, ranges of sparsity and constants optimization",
      "text" : "We prove in this section that the algorithm SPARSEUCB is optimal, up to multiplicative factor, for a wide range of parameters. For this purpose, we recall the different bounds we obtained, up to multiplicative universal constants and additive data-dependent constants."
    }, {
      "heading" : "5.1 Strong sparsity",
      "text" : "The regime of strong sparsity is attained when d−sµ1 − ∑\ni∈[s] ∆i>0\n∆i µ2i > 0. In that case,\nthe lower bound of Theorem 1 rewrites as\nlim inf T→∞\nReg(T ) log(T ) & ∑ i∈[s] ∆i>0 max { 1 ∆i , ∆i µ2i }\nwhile SPARSEUCB suffers an expected regret bounded as\nReg(T ) log(T ) . ∑ i∈[s] ∆i>0 1 ∆i + ∆i µ2i . ∑ i∈[s] ∆i>0 max { 1 ∆i , ∆i µ2i } .\nObviously, SPARSEUCB is optimal for all those values of parameters. More importantly, its regret scales linearly with s and is independent of the number of arms with non-positive means.\nThe minimax regret of SPARSEUCB is necessarily of the same order of UCB, as they achieve the same regret when d−sµ1 − ∑ i∈[s] ∆i>0 ∆i µ2i is arbitrarily close to 0. So we consider instead the minimax regret with respect to the distributions with a relative sparsity level bounded away from 0, i.e., such that for some θ ∈ (0, 1), µs > θµ1. In particular, this yields that d−ss > 1−θ θ2\nso that the strong sparsity assumption is satisfied.\nThen, for this class of parameters, it is quite straightforward to get that the minimax regret of UCB scales as √ dT log(T ) √ 1 + θ1−θ s d while the minimax re-\ngret of SPARSEUCB increases as √ sT log(T ) √ (1−θ)2 θ2\n+ 1. As a consequence, for any fixed class of parameters, the dependency in the number of arms in the minimax regret shrinks from √ d to √ s"
    }, {
      "heading" : "5.2 Variants & small improvements",
      "text" : "Of course, the minimax regret of SPARSEUCB exhibits an extra √\nlog(T ) term, which is due to the fact that we used UCB as a basic algorithm. In the order hand, we could have used instead of UCB, any variant such as UCB-2, improved-UCB, ETC, MOSS... In the same line of thoughts, the threshold of the force-log phase\ncould also be updated to 2 √\nlog(T/Ni(t)) Ni(t)\nso that the term √ log(T ) can be replaced\nby √ log(s), which gives a regret scaling in\nReg(T ) .  ∑ i∈[s] ∆i>0 log(T∆2i ) ∆i + log(Tµ2i )∆i µ2i in the distribution dependent sense √ sT √\n(1−θ)2 θ2\n+ 1 √ log(s) + log( (1−θ) 2\nθ2 + 1) in the minimax sense\nSimilarly, under some additional assumptions on the probility distribution at stake, one might use KL-UCB instead of UCB to replace the dependency in ∆i\n( 1\n∆2i +\n∆i µ2i\n) into ∆i ( 1\nKL(µi,µ∗) + ∆iKL(0,µi)\n) when this makes sense5.\nAnother way to slightly improve the guarantees of the algorithm is to change the round-robin phases into sampling phases in which arms are not selected uniformly at random but with probability depending on the past performances of the different arms, as in Bubeck et al. (2013). Unfortunately, this does not improve the leading term (in T ) of the regret, but merely the terms uniformly bounded (in T )."
    }, {
      "heading" : "6 Experiments",
      "text" : "This section aims at experimentally validating the theoretical results we obtained. We empirically compare the regret of UCB and SPARSEUCB for various levels of sparsity: we either fix d and s and allow µs/∆s to vary or conversely fix the expected returns and allow s/d to vary. According to the conclusions of Section 5, we observe that for a range of settings, SPARSEUCB does behave nearoptimally in the long run, up to multiplicative constants. We also see that even when SPARSEUCB is not optimal, it is still almost always preferable to UCB as soon as there is some sparsity in the problem. Without loss of generality, experiments are performed on problems for which µ1 = 0.9 and for 2 6 i 6 s, all µi’s are equal to µs = µ1 −∆s."
    }, {
      "heading" : "6.1 Varying µs",
      "text" : "We fix d = 15 and s = 7 such that the limit between weak and strong sparsity as defined in Section 5 is reached at µs = 0.4. We allow ∆s to vary in [0.1, 0.7]. We compare the behavior of SPARSEUCB and UCB for these bandit problems, and we also compute and display the lower bound of Corollary 1, that is for the smallest class of sparse of problems containing ours – for ε = µs.\nOn Figure 3 we present the expected regret averaged over Monte-Carlo 100 repetitions for each experiment. When the sparsity of the problem is not strong,\n5Notice that the sparse bandit problem is trivial with Bernoulli distributions.\nthat is when ∆s = 0.7, UCB has a lower regret than SPARSEUCB for a long time but the asymptotic behavior of the latter tends to show that UCB will eventually be worse in the long run. However, when the sparsity gets stronger, SPARSEUCB is much closer to optimal than UCB and reaches a much lower regret."
    }, {
      "heading" : "6.2 Influence of the number of arms",
      "text" : "We now fix d = 15, µ∗ = 0.9 and ∆s = 0.3 and we allow the number of effective arms s to vary in {2, 6, 12}. Note that given the fixed parameters, the regime of weak sparsity defined in previous section only holds when s > 10.\nOn Figure 4 we present the expected regret averaged over 100 Monte-Carlo repetitions. Clearly, when s = 12, we are in the weak sparsity regime and there is no real improvement brought by SPARSEUCB as compared to the usual UCB policy. On the contrary, as s gets smaller, SPARSEUCB gets closer and closer to optimal."
    }, {
      "heading" : "7 Conclusions and Open Questions",
      "text" : "We introduced a new variation of the celebrated stochastic multi-armed bandit problem that include an additional sparsity information on the expected return of\nthe arms. We characterized the range of parameters that lead to interesting sparse problems and gave a lower bound on the regret that scales in O(s log(T )) in the Strong Sparsity domain. We provide SPARSEUCB that is a good alternative to the classical UCB in the sparse bandit situation as it has both good theoretical guarantees and good empirical performances. However, we noticed in the experiments that for parameters lying in the Weak Sparsity domain, one would rather switch to the classical UCB policy as the price of focusing on the s best arms paid by SPARSEUCB is too high as compared to the resulting improvement on the regret. Moreover, it appears in many real applications that the learner often knows the existence of s without knowing its exact value and that leverages a new and unsolved stochastic sparse problem."
    }, {
      "heading" : "A End of Proof of Lower Bound",
      "text" : "Recall Theorem 1:\nTheorem 2. For a Gaussian sparse bandit problem ν = (ν1, . . . , νs, νs+1, . . . , νd) ∈ S(d, s) ∈ Rd, an asymptotic lower bound on the regret is given by the solution to the following linear optimization problem:\nf(µ) > inf c 0 ci∆i (6)\ns.t. ∀i ∈ {2, ..., s}, 2ci∆2i > 1; (7) ∀i ∈ {2, . . . , s}, ∀j ∈ {s+ 1, ...d}, 2cjµ21 + 2ciµ2i > 1 (8)\n∀i ∈ {1, . . . , d}, ci > 0 (9)\nwhose solution can be computed explicitly and gives the following problem-dependent lower bound:\n• If d−sµ1 − ∑\ni∈[s] ∆i>0\n∆i µ2i > 0,\nlim inf T→∞\nReg(T ) log(T ) > ∑ i∈[s] ∆i>0 max { 1 2∆i , ∆i 2µ2i } (10)\n• otherwise, there exist k 6 s such that d−sµ1 − ∑s i=k ∆i µ2i\n< 0, and the lower bound is\nlim inf T→∞\nReg(T ) log(T ) > ∑ i∈[k] ∆i>0 1 2∆i + s∑ i=k+1 µ2k µ2i ∆i 2∆2k + (d− s) 2µ1 ( 1− µ 2 k ∆2k ) . (11)\nProof. We now solve the following linear programming in order to obtain the given explicit form for the Lower Bound.\nf(µ) > inf c 0 ci∆i\ns.t. ∀i ∈ {2, ..., s}, 2ci∆2i > 1; ∀i ∈ {2, . . . , s}, ∀j ∈ {s+ 1, ...d}, 2cjµ21 + 2ciµ2i > 1\n∀i ∈ {1, . . . , d}, ci > 0\nFirst, remark that for all the best arms i ∈ {1, . . . , s}, we must have ci > 1/2∆2i so if µ 2 i /∆ 2 i > 1, the d−s corresponding constraints on suboptimal cj , j >\ns are empty. We define S∗ := { i ∈ [s] ∣∣µ2i /∆2i > 1}. It remains s−|S∗| constrains on each cj for j > s:\ncj > max i∈[s]\\S∗ 1− 2ciµ2i 2µ21 =: λ 2µ21\nIt remains to properly identify λ as a function of the parameters of the problem. Because of the first set of constraints, for all i /∈ S∗,\nci = max\n{ 1\n2∆2i , 1− λ 2µ2i } For those coefficients i 6 s such that ci = 1−λ2µ2i , we have\nλ 6 1− ( µi ∆i )2 := Θi ∈ [0, 1]\nwhere the quantity Θi increases with i, i.e. the worse the arm is, the bigger his Θi. Let k /∈ S∗ be the smaller index such that\nΘk−1 < λ 6 Θk. (12)\nThen, we set the values of the coefficients as{ ci = 1/2∆ 2 i i < k\nci = (1− λ)/2µ21 i > k\nWe can rewrite the optimization problem as a function of λ > Θk−1:\nf(µ) > k−1∑ i=1 1 2∆i + s∑ i=k ∆i 1− λ 2µ2i + (d− s) λ 2µ1\n= k−1∑ i=1 1 2∆i + s∑ i=k ∆i 2µ2i + λ 2 ( d− s µ1 − s∑ i=k ∆i µ2i ) (13)\nNow we must distinguish two cases depending on the sign of\nd− s µ1 − s∑ i=k ∆i µ2i\n(14)\nStrong sparsity. If d−sµ1 − ∑\ni∈[s] ∆i>0\n∆i µ2i > 0, then we must set the coefficients\nsuch that λ reaches its lowest allowed value, which is λ = 0. Hence, ci = max{ 1\n2∆2i , 1 2µ2i } for all i 6 s. The lower bound is then\nf(µ) = ∑ i∈[s] ∆i>0 max { 1 2∆i , ∆i 2µ2i } .\nWeak sparsity Otherwise, there exists k 6 s such that the expression of Eq. 14 is negative. Then, we have by definition of k,\nλ = Θk\nand, rearranging the terms of Eq.(13), the Lower Bound finally writes\nlim inf T→∞\nReg(T )\nlog(T ) > k∑ i=1 1 2∆i + s∑ i=k+1 µ2k µ2i ∆i 2∆2k + (d− s) 2µ1 ( 1− µ 2 k ∆2k ) .\nA special case of the above bound is when k = s. Then\nλ = 1− µ 2 s\n∆2s .\nIn that case, the lower bound is\nf(µ) > s∑ i=1 1 2∆i + d∑ i=s+1 ∆i(1− µ2s/∆2s) 2µ21 = d∑ i=1 1 2∆i − (d− s) 2µ1 µ2s ∆2s"
    }, {
      "heading" : "B Generalization of Theorem 1",
      "text" : "The lower bound of Theorem 1 can be generalized to a wider class of problems including a sparsity information. We assume that we know ε > 0 such that µs+1 > µs− ε. A sparse bandit problem as defined in Section 2 is at least included in such class of problem for ε = µs. Introducing ε > 0 allows us to provide a result that applies to our problem as well as to similar ones such as the Stochastic Thresholded Bandit 6 for which one would assume that there exists a threshold µs > τ > 0 such that the s arms of interest have an expected return at of at least τ . In that case, a change of variable ε ← µs − τ in the following Theorem provides a lower bound on the regret of any uniformly efficient algorithm for that problem. We chose to introduce this wilder class of problems in order to provide a generic result and its associated proof technique, but we state the specific lower bound for our own problem in Theorem 1 below.\nTheorem 3. For a Gaussian sparse bandit problem ν = (ν1, . . . , νs, νs+1, . . . , νd) ∈ S(d, s, ε) ∈ Rd, an asymptotic lower bound on the regret is given by\n6This problem has not been studied yet in a regret minimization setting to our knowledge but its setting is close to ours.\n• If d−sµ1 − ∑\ni∈[d] ∆i>0\n∆i (µi−µs+ε)2 > 0,\nf(µ) = ∑ i∈[s] ∆i>0 max { 1 2∆i , ∆i 2(µi − µs + ε)2 } ;\n• otherwise, there exist k 6 s such that d−sµ1 − ∑s i=k ∆i\n(µi−µs+ε)2 < 0, and the lower bound is\nlim inf T→∞\nReg(T )\nlog(T ) > k∑ i=1 1 2∆i + s∑ i=k+1 (µk − µs + ε)2 (µi − µs + ε)2 ∆i 2∆2k + (d− s) 2µ1 ( 1− (µk − µs + ε) 2 ∆2k ) ..\n(15)"
    }, {
      "heading" : "C Analysis of the SPARSEUCB algorithm",
      "text" : "We provide in this section the detailed statements and proofs concerning the upper bound guaranteed by the SPARSEUCB algorithm.\nTheorem 4. For T > 1, the SPARSEUCB algorithm guarantees: E [Reg(T )] 6 16 log(T ) ∑ i∈[s] ∆i>0 ( 1 ∆i + ∆i µ2i ) + ∑ i∈[d] ∆i 1 + 3s+ s∑ j=1 1 + 4 log(16/µ2j ) µ2j \n+ ∑ i∈[s] ∆i>0 ∆i ( 3 + 8 µ2i + 8 ∆2i ) + π2 6 d∑ i=s+1 ∆i + d∆sπ 2 6 .\nWe gather without proof in the following lemma a few properties which are immediate from the definition of the algorithm.\nLemma 6. By definition of the algorithm, we have:\n(i) For all t > d + 1 and i ∈ [d], Ni(t) > 1 and the sets J (t) and K(t) are well-defined.\n(ii) For all t > d+ 1 and i ∈ [d], if arm i is pulled at time t during a round-robin phase, then the set J (t− i+ 1) contains less than s arm. In other words,\nRi(t) ⊂ {|J (t− i+ 1)| < s} .\n(iii) For all t > 1 and i, k ∈ [d], arm i is pulled at time t during a round-robin phase if, and only if arm k is also pulled during a round-robin phase at time t− i+ k. In other words,\n{I(t) = i, ω(t) = r} = {I(t− i+ k) = k, ω(t− i+ k) = r} .\n(iv) For all t > d + 1 and i ∈ [d], if arm i is pulled at time t during a force-log phase, it does not belong to K(t) i.e. its empirical mean at time t is strictly below 2 √ log(t)/Ni(t). In other words,\nFi(t) ⊂ { Xi(Ni(t)) < 2 √ log(t)\nNi(t) , I(t) = i\n} .\n(v) For all t > 1 and i ∈ [d], arm i is pulled at time t during a force-log or a UCB phase only if it belongs to the set J (t). In other words, Fi(t), Ui(t) and Vi(t) are subsets of Ai(t).\n(vi) For all t > d + 1, if an arm is pulled at time t during an UCB phase while the best arm does not belong to the set K(t), necessarily, a bad arm j ∈ {s+ 1, . . . , d} belongs to K(t). In other words,\n⊔ i∈[d] Vi(t) ⊂ ⋃ j>s\n{ Xj(Nj(t)) > 2 √ log(t)\nNj(t)\n} .\nLemma 7. For i ∈ [d], the number of times arm i is pulled, while the algorithm is performing a round-robin phase, is bounded in expectation as:\nE [ +∞∑ t=1 1 {Ri(t)} ] 6 1 + 3s+ s∑ j=1 1 µ2j ( 8 + 32 log ( 16 µ2j )) .\nProof. By definition of the algorithm, arm i is pulled exactly once during the first d stages:\nd∑ t=1 1 {Ri(t)} = 1.\nLet t > d + 1. Using the definition of Ri(t) and property (ii) from Lemma 6, we write\nRi(t) = {I(t) = i, ω(t) = r} ∩ {|J (t− i+ 1)| < s} .\nIf |J (t− i+ 1)| < s, necessarily, there exists j ∈ [s] such that j 6∈ J (t− i+ 1), in other words, such that:\nXj(Nj(t− i+ 1)) < 2 √\nlog(Nj(t− i+ 1)) Nj(t− i+ 1) .\nThus, we write: Ri(t) ⊂ s⋃ j=1 { Xj(Nj(t− i+ 1)) < 2 √ log(Nj(t− i+ 1)) Nj(t− i+ 1) , I(t) = i, ω(t) = r } .\nTherefore,\n+∞∑ t=d+1 1 {Ri(t)} 6 s∑ j=1 +∞∑ t=d+1 1\n{ Xj(Nj(t− i+ 1)) < 2 √ log(Nj(t− i+ 1)) Nj(t− i+ 1) , I(t) = i, ω(t) = r }\n= s∑ j=1 ∑ d<t<+∞ I(t)=i ω(t)=r 1\n{ Xj(Nj(t− i+ 1)) < 2 √ log(Nj(t− i+ 1)) Nj(t− i+ 1) } .\n(16)\nFor a given arm j ∈ [s], the quantityNj(t− i+1) in the above last sum is (strictly) increasing. Indeed, let t < t′ such that I(t) = I(t′) = i and ω(t) = ω(t′) = r. As a consequence of property (iii) from Lemma 6, we have\n(i) I(t− i+ k) 6= 1 for k ∈ {2, . . . , d};\n(ii) I(t′ − i+ 1) = 1.\n(iii) I(t− i+ j) = j;\nThe above properties (i) and (ii) imply t − i + d 6 t′ − i, which in turn, together with property (iii), gives that arm j is pulled at least once between time t − i + 1 and t′− i (at time t− i+ j). Therefore, Nj(t− i+ 1) < Nj(t′− i+ 1). Therefore, the last sum in Equation (16) can be bounded, with a change of variable, as\n∑ d<t<+∞ I(t)=i ω(t)=r 1\n{ Xj(Nj(t− i+ 1)) < 2 √ log(Nj(t− i+ 1)) Nj(t− i+ 1) } 6 +∞∑ u=1 1 { Xj(u) < 2 √ log(u) u } .\nGoing back to Equation (16), we can now bound the expectation of the number of times arm i was pulled after time d as follows:\nE [ +∞∑ t=d+1 1 {Ri(t)} ] 6 s∑ j=1 +∞∑ u=1 P [ Xj(u) < 2 √ log(u) u ]\n6 s∑ j=1 +∞∑ u=1 P\n[ Xj(u)− µj < 2 √ log(u)\nu − µj\n] .\nOne can easily check that\nu > 3 + 32\nµ2j log\n( 16\nµ2j\n) implies 2 √ log(u)\nu − µj 6 − µj 2 .\nTherefore, we set uj := 3 + d(32/µ2j ) log(16/µ2j )e and write:\nE [ +∞∑ t=d+1 1 {Ri(t)} ] 6 s∑ j=1 +∞∑ u=1 P [ Xj(u)− µj < 2 √ log(u) u − µj ]\n6 s∑ j=1 3 + 32 µ2j log ( 16 µ2j ) + +∞∑ u=uj P [ Xj(u)− µj < − µj 2 ] 6\ns∑ j=1 3 + 32 µ2j log ( 16 µ2j ) + +∞∑ u=uj e−uµ 2 j/8  6 3s+\ns∑ j=1 1 µ2j\n( 8 + 32 log ( 16\nµ2j\n)) .\nLemma 8. For i ∈ [s] and T > d + 1, the number of times arm i is pulled up to time T during force-log phases, is bounded in expectation as:\nE [ T∑ t=1 1 {Fi(t)} ] 6 16 log(T ) + 8 µ2i .\nProof. Let i ∈ [s] and T > d + 1. By definition of the algorithm, 1 {Fi(t)} = 0\nfor t 6 d. Using property (iv) from Lemma 6, we write\nT∑ t=d+1 1 {Fi(t)} 6 T∑ t=d+1 1\n{ Xi(Ni(t)) < 2 √ log(t)\nNi(t) , I(t) = i\n}\n6 ∑\nd+16t<+∞ I(t)=i\n1 { Xi(Ni(t)) < 2 √ log(T )\nNi(t)\n} .\nThe quantity Ni(t) being increasing in the above sum, with a change of variable, we write:\nT∑ t=d+1 1 {Fi(t)} 6 +∞∑ u=1 1\n{ Xi(u) < 2 √ log(T )\nu\n} .\nWe now take the expectation:\nE\n[ T∑\nt=d+1\n1 {Fi(t)} ] 6\n+∞∑ u=1 P\n[ Xi(u) < 2 √ log(T )\nu\n] =\n+∞∑ u=1 P\n[ Xi(u)− µi < 2 √ log(T )\nu − µi\n] .\nWe consider u0 := d16 log(T )/µ2i e which gives that 2 √\nlog(T )/u− µi 6 −µi/2 as soon as u > u0. Therefore, +∞∑ u=1 P [ Xi(u)− µi < 2 √ log(T ) u − µi ] 6 16 log(T ) µ2i + +∞∑ u=u0 P [ Xi(u)− µi < − µi 2\n] 6 16 log(T )\nµ2i + +∞∑ u=u0 e−uµ 2 i /8\n6 16 log(T )\nµ2i +\n8\nµ2i ,\nhence the result.\nLemma 9. For i ∈ [s] such that ∆i > 0 and T > 1, we have\nE [ T∑ t=1 1 {Ui(t)} ] 6 16 log(T ) + 8 ∆2i + 3.\nProof. Let i ∈ [s] such that ∆i > 0 and t > 1, and assume that:\ni ∈ argmaxj∈K(t) { Xj(Nj(t)) + 2 √ log(t)\nNj(t)\n} and 1 ∈ K(t).\nIn particular, we have:\nXi(Ni(t)) + 2\n√ log(t)\nNi(t) > X1(N1(t)) + 2\n√ log(t)\nN1(t) .\nUsing the definition of ∆i, the above inequality can be equivalently written:\nXi(Ni(t))−µi > ∆i 2 + ( ∆i 2 − 2 √ log(t)\nNi(t)\n) + ( X1(N1(t))− µ1 + 2 √ log(t)\nN1(t)\n) .\nWe consider τi := d16 log(t)/∆2i e and we can see that as soon as Ni(t) > τi, we have: ∆i 2 − 2 √ log(t) Ni(t) > 0.\nWhen this is the case, we either have\nXi(Ni(t))− µi > ∆i 2\nor X1(N1(t))− µ1 6 −2 √ log(t)\nN1(t) .\nWith the above in mind, we can write\nE [ T∑ t=1 1 {Ui(t)} ] 6 τi + E  ∑ 16t6n Ni(t)>τi ( 1 { Xi(Ni(t))− µi > ∆i 2 }\n+ 1 { X1(N1(t))− µ1 6 −2 √ log(t)\nN1(t)\n})]\n6 τi + +∞∑ u=τi P [ Xi(u)− µi > ∆i 2 ] + +∞∑ t=1 P [ X1(N1(t))− µ1 6 −2 √ log(t) N1(t) ] .\nThe arms being subgaussian, we bound the above probabilities as follows:\nE [ T∑ t=1 1 {Ui(t)} ] 6 τi + +∞∑ u=τi e−u∆ 2 i /8 + T∑ t=1 1 t2 6 1 + 16 ∆2i log(T ) + 8 ∆2i + π2 6\n6 16 log(T ) + 8\n∆2i + 3.\nLemma 10. For T > 1, we have\n∑ i∈[s] ∆iE [ T∑ t=1 1 {Vi(t)} ] 6 d∆sπ 2 6 .\nProof. Using the fact that ∆i 6 ∆s for all i ∈ [s],\n∑ i∈[s] ∆iE [ T∑ t=1 1 {Vi(t)} ] 6 ∆s T∑ t=1 E ∑ i∈[s] 1 {Vi(t)}  . Using property (vi) from Lemma 6, we bound the above expectation as follows:\nE ∑ i∈[s] 1 {Vi(t)}  6∑ j>s P [ Xj(Nj(t)) > 2 √ log(t) Nj(t) ] .\nUsing the fact the arms are subgaussian and that µj 6 0 (for j > s), we bound the above probability as:\nP [ Xj(Nj(t)) > 2 √ log(t)\nNj(t)\n] = P [ Xj(Nj(t))− µj > 2 √ log(t)\nNj(t)\n] 6 e−2 log(t) = t−2.\nThe result follows.\nLemma 11. For i ∈ {s+ 1, . . . , d}, we have\nE [ +∞∑ t=1 1 {Ai(t)} ] 6 π2 6 .\nProof. Let i ∈ {s+ 1, . . . , d}. By definition of the algorithm, 1 {Ai(t)} = 0 for t 6 d. Using the definition of Ai(t), we write +∞∑ t=1 1 {Ai(t)} = +∞∑ t=d+1 1 {Ai(t)} = ∑\nd<t<+∞ I(t)=i\n1 { Xi(Ni(t)) > 2 √ log(Ni(t))\nNi(t)\n} .\nIn the last sum above, the quantity Ni(t) is (strictly) incrasing. Using a change of variable, and taking the expectation, we get:\nE [ +∞∑ t=d+1 1 {Ai(t)} ] 6 E [ +∞∑ u=1 1 { Xi(u) > 2 √ log(u) u }] = +∞∑ u=1 P [ Xi(u) > 2 √ log(u) u ] .\nWe now use the assumption that the arms have subgaussian laws and that µi 6 0 to bound the above probability as:\nP [ Xi(u) > 2 √ log(u)\nu\n] 6 P [ Xi(u)− µi > 2 √ log(u)\nu\n] 6 e−2 log(u) = u−2.\nThe result follows."
    } ],
    "references" : [ {
      "title" : "Online-to-confidence-set conversions and application to sparse stochastic bandits",
      "author" : [ "Yasin Abbasi-Yadkori", "David Pal", "Csaba Szepesvari" ],
      "venue" : "In AISTATS,",
      "citeRegEx" : "Abbasi.Yadkori et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Abbasi.Yadkori et al\\.",
      "year" : 2012
    }, {
      "title" : "Finite-time analysis of the multiarmed bandit problem",
      "author" : [ "Peter Auer", "Nicolo Cesa-Bianchi", "Paul Fischer" ],
      "venue" : "Machine learning,",
      "citeRegEx" : "Auer et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Auer et al\\.",
      "year" : 2002
    }, {
      "title" : "Regret analysis of stochastic and nonstochastic multi-armed bandit problems",
      "author" : [ "Sébastien Bubeck", "Nicolo Cesa-Bianchi" ],
      "venue" : "arXiv preprint arXiv:1204.5721,",
      "citeRegEx" : "Bubeck and Cesa.Bianchi.,? \\Q2012\\E",
      "shortCiteRegEx" : "Bubeck and Cesa.Bianchi.",
      "year" : 2012
    }, {
      "title" : "Bounded regret in stochastic multi-armed bandits",
      "author" : [ "Sébastien Bubeck", "Vianney Perchet", "Philippe Rigollet" ],
      "venue" : "In COLT,",
      "citeRegEx" : "Bubeck et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Bubeck et al\\.",
      "year" : 2013
    }, {
      "title" : "Optimal adaptive policies for sequential allocation problems",
      "author" : [ "Apostolos N Burnetas", "Michaël N Katehakis" ],
      "venue" : "Advances in Applied Mathematics,",
      "citeRegEx" : "Burnetas and Katehakis.,? \\Q1996\\E",
      "shortCiteRegEx" : "Burnetas and Katehakis.",
      "year" : 1996
    }, {
      "title" : "Bandit theory meets compressed sensing for high dimensional stochastic linear bandit",
      "author" : [ "Alexandra Carpentier", "Rémi Munos" ],
      "venue" : "In AISTATS,",
      "citeRegEx" : "Carpentier and Munos,? \\Q2012\\E",
      "shortCiteRegEx" : "Carpentier and Munos",
      "year" : 2012
    }, {
      "title" : "Prediction, learning, and games",
      "author" : [ "Nicolo Cesa-Bianchi", "Gábor Lugosi" ],
      "venue" : "Cambridge university press,",
      "citeRegEx" : "Cesa.Bianchi and Lugosi.,? \\Q2006\\E",
      "shortCiteRegEx" : "Cesa.Bianchi and Lugosi.",
      "year" : 2006
    }, {
      "title" : "Combinatorial bandits revisited",
      "author" : [ "Richard Combes", "Alexandre Proutiere" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Combes et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Combes et al\\.",
      "year" : 2015
    }, {
      "title" : "Linear programming and extensions",
      "author" : [ "George Dantzig" ],
      "venue" : "Princeton university press,",
      "citeRegEx" : "Dantzig.,? \\Q2016\\E",
      "shortCiteRegEx" : "Dantzig.",
      "year" : 2016
    }, {
      "title" : "Explore first, exploit next: The true shape of regret in bandit problems",
      "author" : [ "Aurélien Garivier", "Pierre Ménard", "Gilles Stoltz" ],
      "venue" : "Mathematics of Operations Research,",
      "citeRegEx" : "Garivier et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Garivier et al\\.",
      "year" : 2017
    }, {
      "title" : "Sparsity regret bounds for individual sequences in online linear regression",
      "author" : [ "Sébastien Gerchinovitz" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Gerchinovitz.,? \\Q2013\\E",
      "shortCiteRegEx" : "Gerchinovitz.",
      "year" : 2013
    }, {
      "title" : "Asymptotically efficient adaptive choice of control laws in controlled markov chains",
      "author" : [ "Todd L Graves", "Tze Leung Lai" ],
      "venue" : "SIAM journal on control and optimization,",
      "citeRegEx" : "Graves and Lai.,? \\Q1997\\E",
      "shortCiteRegEx" : "Graves and Lai.",
      "year" : 1997
    }, {
      "title" : "On the complexity of best arm identification in multi-armed bandit models",
      "author" : [ "Émilie Kaufmann", "Olivier Cappé", "Aurélien Garivier" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Kaufmann et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Kaufmann et al\\.",
      "year" : 2015
    }, {
      "title" : "Gains and losses are fundamentally different in regret minimization: the sparse case",
      "author" : [ "Joon Kwon", "Vianney Perchet" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Kwon and Perchet.,? \\Q2016\\E",
      "shortCiteRegEx" : "Kwon and Perchet.",
      "year" : 2016
    }, {
      "title" : "Multiple-play bandits in the position-based model",
      "author" : [ "Paul Lagrée", "Claire Vernade", "Olivier Cappé" ],
      "venue" : "arXiv preprint arXiv:1606.02448,",
      "citeRegEx" : "Lagrée et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Lagrée et al\\.",
      "year" : 2016
    }, {
      "title" : "Asymptotically efficient adaptive allocation rules",
      "author" : [ "Tze Leung Lai", "Herbert Robbins" ],
      "venue" : "Advances in applied mathematics,",
      "citeRegEx" : "Lai and Robbins.,? \\Q1985\\E",
      "shortCiteRegEx" : "Lai and Robbins.",
      "year" : 1985
    }, {
      "title" : "Sparse online learning via truncated gradient",
      "author" : [ "John Langford", "Lihong Li", "Tong Zhang" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Langford et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Langford et al\\.",
      "year" : 2009
    }, {
      "title" : "Linear multi-resource allocation with semi-bandit feedback",
      "author" : [ "Tor Lattimore", "Koby Crammer", "Csaba Szepesvári" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Lattimore et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Lattimore et al\\.",
      "year" : 2015
    }, {
      "title" : "Some aspects of the sequential design of experiments",
      "author" : [ "Herbert Robbins" ],
      "venue" : "In Herbert Robbins Selected Papers,",
      "citeRegEx" : "Robbins.,? \\Q1985\\E",
      "shortCiteRegEx" : "Robbins.",
      "year" : 1985
    } ],
    "referenceMentions" : [ {
      "referenceID" : 18,
      "context" : "1 Introduction We consider the celebrated stochastic multi-armed bandit problem Robbins (1985), where a decision maker sequentially samples from d > 1 processes, also called arms, aiming at maximizing its cumulative reward.",
      "startOffset" : 80,
      "endOffset" : 95
    }, {
      "referenceID" : 13,
      "context" : "Converse statements have also been proved, first by Lai and Robbins (1985) and then by Burnetas and Katehakis (1996): Any consistent policy (i.",
      "startOffset" : 52,
      "endOffset" : 75
    }, {
      "referenceID" : 4,
      "context" : "Converse statements have also been proved, first by Lai and Robbins (1985) and then by Burnetas and Katehakis (1996): Any consistent policy (i.",
      "startOffset" : 87,
      "endOffset" : 117
    }, {
      "referenceID" : 4,
      "context" : "Converse statements have also been proved, first by Lai and Robbins (1985) and then by Burnetas and Katehakis (1996): Any consistent policy (i.e. whose regret is always less than Tα for all α > 0) always have a regret larger than ∑d i=1 log(T ) ∆i (again, up to some constants). When T is fixed and the parameters μi are chosen to maximize regret, the distribution-independent bounds are of order √ dT as first shown in Cesa-Bianchi and Lugosi (2006). The main drawback of those results is that the regret scales linearly with the number of arms d,or with √ d in the minimax analysis.",
      "startOffset" : 87,
      "endOffset" : 451
    }, {
      "referenceID" : 1,
      "context" : "The Sparse Bandit problem is therefore a variation of the classical stochastic multi-armed bandit problem (see Bubeck and Cesa-Bianchi (2012) for a survey) in which the agent knows the number of arms with positive means.",
      "startOffset" : 111,
      "endOffset" : 142
    }, {
      "referenceID" : 1,
      "context" : "The Sparse Bandit problem is therefore a variation of the classical stochastic multi-armed bandit problem (see Bubeck and Cesa-Bianchi (2012) for a survey) in which the agent knows the number of arms with positive means. There have been some works regarding sparsity assumptions in bandit problems. In the full information setting, some of them focus on sparse reward vectors, i.e., at most s components of (X1(t), . . . , Xd(t)) are positive (see for instance Langford et al. (2009); Kwon and Perchet (2016)).",
      "startOffset" : 111,
      "endOffset" : 484
    }, {
      "referenceID" : 1,
      "context" : "The Sparse Bandit problem is therefore a variation of the classical stochastic multi-armed bandit problem (see Bubeck and Cesa-Bianchi (2012) for a survey) in which the agent knows the number of arms with positive means. There have been some works regarding sparsity assumptions in bandit problems. In the full information setting, some of them focus on sparse reward vectors, i.e., at most s components of (X1(t), . . . , Xd(t)) are positive (see for instance Langford et al. (2009); Kwon and Perchet (2016)).",
      "startOffset" : 111,
      "endOffset" : 509
    }, {
      "referenceID" : 1,
      "context" : "The Sparse Bandit problem is therefore a variation of the classical stochastic multi-armed bandit problem (see Bubeck and Cesa-Bianchi (2012) for a survey) in which the agent knows the number of arms with positive means. There have been some works regarding sparsity assumptions in bandit problems. In the full information setting, some of them focus on sparse reward vectors, i.e., at most s components of (X1(t), . . . , Xd(t)) are positive (see for instance Langford et al. (2009); Kwon and Perchet (2016)). Another considered problem is the one of sparse linear bandits Carpentier et al. (2012); Abbasi-Yadkori et al.",
      "startOffset" : 111,
      "endOffset" : 599
    }, {
      "referenceID" : 0,
      "context" : "(2012); Abbasi-Yadkori et al. (2012); Gerchinovitz (2013); Lattimore et al.",
      "startOffset" : 8,
      "endOffset" : 37
    }, {
      "referenceID" : 0,
      "context" : "(2012); Abbasi-Yadkori et al. (2012); Gerchinovitz (2013); Lattimore et al.",
      "startOffset" : 8,
      "endOffset" : 58
    }, {
      "referenceID" : 0,
      "context" : "(2012); Abbasi-Yadkori et al. (2012); Gerchinovitz (2013); Lattimore et al. (2015) in which the underlying unknown vector of parameter is assumed to be sparse – that is with a constraint on its L1 norm – or even spiky in the crude and very specific case where s = 1 Bubeck et al.",
      "startOffset" : 8,
      "endOffset" : 83
    }, {
      "referenceID" : 0,
      "context" : "(2012); Abbasi-Yadkori et al. (2012); Gerchinovitz (2013); Lattimore et al. (2015) in which the underlying unknown vector of parameter is assumed to be sparse – that is with a constraint on its L1 norm – or even spiky in the crude and very specific case where s = 1 Bubeck et al. (2013). However, none of the previously cited work tackles the following concrete problem.",
      "startOffset" : 8,
      "endOffset" : 287
    }, {
      "referenceID" : 14,
      "context" : "We give an analogous result to the seminal bound of Lai and Robbins (1985), and we construct an anytime algorithm SPARSEUCB that uses the optimistic principle of Auer et al.",
      "startOffset" : 52,
      "endOffset" : 75
    }, {
      "referenceID" : 1,
      "context" : "We give an analogous result to the seminal bound of Lai and Robbins (1985), and we construct an anytime algorithm SPARSEUCB that uses the optimistic principle of Auer et al. (2002) together with the sparsity information available in order to reach optimal performance, up to constant terms.",
      "startOffset" : 162,
      "endOffset" : 181
    }, {
      "referenceID" : 11,
      "context" : "The proof relies on changes of measure arguments originating from Graves and Lai (1997). First, consider the set of changes of distributions that modify the best arm without changing the marginal of the best arm in the original sparse bandit problem: B(μ) = { μ′ ∈ S(d, s) ∣∣∣∣μ′1 = μ1 and max i∈[d] μi < max i∈[d] μi } .",
      "startOffset" : 66,
      "endOffset" : 88
    }, {
      "referenceID" : 12,
      "context" : "17 in (Kaufmann et al., 2015) or Proposition 3 in (Lagrée et al.",
      "startOffset" : 6,
      "endOffset" : 29
    }, {
      "referenceID" : 14,
      "context" : ", 2015) or Proposition 3 in (Lagrée et al., 2016) can be stated in our case as follows: For all changes of measure μ′ ∈ B(μ),",
      "startOffset" : 28,
      "endOffset" : 49
    }, {
      "referenceID" : 8,
      "context" : "(3) Details on this type of informational lower bounds can be found in Garivier et al. (2017.To appear.) and references therein. Now, following general ideas from Graves and Lai (1997) and lower bound techniques from Lagrée et al.",
      "startOffset" : 71,
      "endOffset" : 185
    }, {
      "referenceID" : 8,
      "context" : "(3) Details on this type of informational lower bounds can be found in Garivier et al. (2017.To appear.) and references therein. Now, following general ideas from Graves and Lai (1997) and lower bound techniques from Lagrée et al. (2016) and Combes et al.",
      "startOffset" : 71,
      "endOffset" : 238
    }, {
      "referenceID" : 7,
      "context" : "(2016) and Combes et al. (2015), we may give a variational form of the lower bound on the regret satisfying the above constraint.",
      "startOffset" : 11,
      "endOffset" : 32
    }, {
      "referenceID" : 8,
      "context" : "This is a linear optimization problem under inequality constraints so there exist algorithmic methods such as the celebrated Simplex algorithm Dantzig (2016) to compute a numeric solution of it.",
      "startOffset" : 143,
      "endOffset" : 158
    }, {
      "referenceID" : 1,
      "context" : "The proof basically follows the steps of the classic UCB analysis by Auer et al. (2002). 2 Lemma 4.",
      "startOffset" : 69,
      "endOffset" : 88
    }, {
      "referenceID" : 3,
      "context" : "Another way to slightly improve the guarantees of the algorithm is to change the round-robin phases into sampling phases in which arms are not selected uniformly at random but with probability depending on the past performances of the different arms, as in Bubeck et al. (2013). Unfortunately, this does not improve the leading term (in T ) of the regret, but merely the terms uniformly bounded (in T ).",
      "startOffset" : 257,
      "endOffset" : 278
    } ],
    "year" : 2017,
    "abstractText" : "In the classical multi-armed bandit problem, d arms are available to the decision maker who pulls them sequentially in order to maximize his cumulative reward. Guarantees can be obtained on a relative quantity called regret, which scales linearly with d (or with √ d in the minimax sense). We here consider the sparse case of this classical problem in the sense that only a small number of arms, namely s < d, have a positive expected reward. We are able to leverage this additional assumption to provide an algorithm whose regret scales with s instead of d. Moreover, we prove that this algorithm is optimal by providing a matching lower bound – at least for a wide and pertinent range of parameters that we determine – and by evaluating its performance on simulated data. *J. Kwon was supported by a public grant as part of the Investissement d’avenir project, reference ANR-11-LABX-0056-LMH. V. Perchet has benefitted from the support of the ANR (grant ANR13-JS01-0004-01), of the FMJH Program Gaspard Monge in optimization and operations research (supported in part by EDF) and from the Labex LMH. C. Vernade was also partially supported by the Machine Learning for Big Data Chair at Télécom ParisTech. Accepted for presentation at Conference on Learning Theory (COLT) 2017 1 ar X iv :1 70 6. 01 38 3v 1 [ cs .L G ] 5 J un 2 01 7",
    "creator" : "LaTeX with hyperref package"
  }
}