{
  "name" : "1204.5309.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Analysis Operator Learning and Its Application to Image Reconstruction",
    "authors" : [ "Simon Hawe", "Martin Kleinsteuber", "Klaus Diepold" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "In this work, we present an algorithm for learning an analysis operator from training images. Our method is based on an `p-norm minimization on the set of full rank matrices with normalized columns. We carefully introduce the employed conjugate gradient method on manifolds, and explain the underlying geometry of the constraints. Moreover, we compare our approach to state-of-the-art methods for image denoising, inpainting, and single image super-resolution. Our numerical results show competitive performance of our general approach in all presented applications compared to the specialized state-of-the-art techniques.\nIndex Terms\nAnalysis Operator Learning, Inverse Problems, Image Reconstruction,Geometric Conjugate Gradient, Oblique Manifold\nI. INTRODUCTION"
    }, {
      "heading" : "A. Problem Description",
      "text" : "L INEAR inverse problems are ubiquitous in the field of image processing. Prominent examples are imagedenoising [1], inpainting [2], super-resolution [3], or image reconstruction from few indirect measurements as in Compressive Sensing [4]. Basically, in all these problems the goal is to reconstruct an unknown image s ∈ Rn as accurately as possible from a set of indirect and maybe corrupted measurements y ∈ Rm with n ≥ m, see [5] for a detailed introduction to inverse problems. Formally, this measurement process can be written as\ny = As + e, (1) where the vector e ∈ Rm models sampling errors and noise, and A ∈ Rm×n is the measurement matrix modeling the sampling process. In many cases, reconstructing s by simply inverting Equation (1) is highly ill-posed because either the exact measurement process and hence A is unknown as in blind image deconvolution, or the number of observations is much smaller compared to the dimension of the signal, which is the case in Compressive Sensing or image inpainting. To overcome the ill-posedness and to stabilize the solution, prior knowledge or assumptions about the general statistics of images can be exploited."
    }, {
      "heading" : "B. Synthesis Model and Dictionary Learning",
      "text" : "One assumption that has proven to be successful in image reconstruction, cf. [6], is that natural images admit a sparse representation x ∈ Rd over some dictionary D ∈ Rn×d with d ≥ n. A vector x is called sparse when most of its coefficients are equal to zero or small in magnitude. When s admits a sparse representation over D, it can be\nThe authors are with the Department of Electrical Engineering, Technische Universität München, Arcisstraße 21, Munich 80290, Germany (e-mail: {simon.hawe,kleinsteuber,kldi}@tum.de, web: www.gol.ei.tum.de)\nar X\niv :1\n20 4.\n53 09\nv1 [\ncs .L\nG ]\n2 4\nA pr\n2 01\n2\nexpressed as a linear combination of only very few columns of the dictionary {di}di=1, called atoms, which reads as\ns = Dx. (2) For d > n, the dictionary is said to be overcomplete or redundant.\nNow, using the knowledge that (2) allows a sparse solution, an estimation of the original signal in (1) can be obtained from the measurements y by first solving\nx? = arg min x∈Rd g(x) subject to ‖ADx− y‖22 ≤ , (3)\nand afterwards synthesizing the signal from the computed sparse coefficients via s? = Dx?. Therein, g : Rd → R is a function that promotes or measures sparsity, and ∈ R+ is an estimated upper bound on the noise power ‖e‖22. Common choices for g include `p-norms\n‖v‖pp := ∑ i |vi|p, (4)\nwith 0 < p ≤ 1 and differentiable approximations of (4). As the signal is synthesized from the sparse coefficients, the reconstruction model (3) is called the synthesis reconstruction model [7].\nTo find the minimizer of Problem (3), various algorithms based on convex or non-convex optimization, greedy pursuit methods, or Bayesian frameworks exist that may employ different choices of g. For a broad overview of such algorithms, we refer the interested reader to [8]. What all these algorithms have in common, is that their performance regarding the reconstruction quality severely depends on an appropriately chosen dictionary D. Ideally, one is seeking for a dictionary where s can be represented most accurately with a coefficient vector x that is as sparse as possible. Basically, dictionaries can be assigned to two major classes: analytic dictionaries and learnt dictionaries.\nAnalytic dictionaries are built on mathematical models of a general type of signal, e.g. natural images, they should represent. Popular examples include Wavelets [9], Bandlets [10], and Curvlets [11] among several others, or a concatenation of various such bases/dictionaries. They offer the advantages of low computational complexity and of being universally applicable to a wide set of signals. However, this universality comes at the cost of not giving the optimally sparse representation for more specific classes of signals, e.g. face images.\nIt is now well known that signals belonging to a specific class can be represented with fewer coefficients over a dictionary that has been learnt using a representative training set, than over analytic dictionaries. This is desirable for various image reconstruction applications as it readily improves their performance and accuracy [12]–[14]. Basically, the goal is to find a dictionary over which a training set admits a maximally sparse representation. In contrast to analytic dictionaries that can be applied globally to an entire image, learnt dictionaries are small dense matrices that have to be applied locally to small image patches. Hence, the training set consists of small patches extracted from some example images. This restriction to patches mainly arises from limited memory, and limited computational resources.\nRoughly speaking, starting from some initial dictionary the learning algorithms iteratively update the atoms of the dictionary, such that the sparsity of the training set is increased. This procedure is often performed via blockcoordinate relaxation, which alternates between finding the sparsest representation of the training set while fixing the atoms, and optimizing the atoms that most accurately reproduce the training set using the previously determined sparse representation. Three conceptually different approaches for learning a dictionary became well established, which are probabilistic ones like [15] or [16], clustering based ones such as K-SVD [17], and recent approaches which aim at learning dictionaries with specific matrix structures that allow fast computations like [18]. For a comprehensive overview of dictionary learning techniques see [19]."
    }, {
      "heading" : "C. Analysis Model",
      "text" : "An alternative to the synthesis model (3) for reconstructing a signal, is to solve\ns? = arg min s∈Rn g(Ωs) subject to ‖As− y‖22 ≤ , (5)\nwhich is known as the analysis model [7]. Therein, Ω ∈ Rk×n with k ≥ n is called the analysis operator, and the analyzed vector Ωs ∈ Rk is assumed to be sparse, where sparsity is again measured via an appropriate function g. In contrast to the synthesis model, where a signal is fully described by the non-zero elements of x, in the analysis model the zero elements of the analyzed vector Ωs described the subspace containing the signal. To emphasize this difference, the term cosparsity has been introduced in [20], which simply counts the number of zero elements of Ωs. As the sparsity in the synthesis model depends on the chosen dictionary, the cosparsity of an analyzed signal depends on the choice of the analysis operator Ω.\nDifferent analysis operators proposed in the literature include the fused Lasso [21], the translation invariant wavelet transform [22], and probably best known the finite difference operator which is closely related to the total-variation [23]. They all have shown very good performance when used within the analysis model for solving diverse inverse problems in imaging. The question is: can the performance of analysis based signal reconstruction be improved upon when a learnt analysis operator is applied instead of a predefined one, as it is the case for the synthesis model where learnt dictionaries outperform analytic dictionaries? As it has been discussed in [7], the two models differ significantly, and the naïve way of learning a dictionary and simply employing its transposed or its pseudo-inverse as the learnt analysis operator fails. Hence, different algorithms are required for analysis operator learning."
    }, {
      "heading" : "D. Contributions",
      "text" : "We introduce a new algorithm to learn a patch based analysis operator from a set of training samples. The method relies on several constraints on the operator, which are motivated in Section II-B. One of these constraints requires the analysis operator to have full rank and normalized rows. To enforce this constraint, we propose an efficient geometric conjugate gradient method on the so-called oblique manifold in Section III. Thereby, an initial operator is iteratively updated such that the `p-pseudo-norm of the corresponding analyzed training samples is minimized, while preventing overfitting to a subset of the training samples. Furthermore, in Section IV we explain how to apply the local patch based analysis operator to achieve global reconstruction results. We use the learnt analysis operator in Section V for the tasks of image denoising, inpainting, and single image super-resolution. The examples and numerical results show the broad and effective applicability of our general approach."
    }, {
      "heading" : "E. Notations",
      "text" : "Matrices are written as capital calligraphic letters like X , column vectors are denoted by boldfaced small letters e.g. x whereas scalars are either capital or small letters like n,N . By vi we denote the ith element of the vector v, vij denotes the ith element in the jth column of a matrix V . The vector v:,i denotes the ith column of V whereas vi,: denotes the transposed of the ith row of V . By Eij , we denote a matrix whose ith entry in the jth column is equal to one, and all others are zero. Ik denotes the identity matrix of dimension (k×k), 0 denotes the zero-matrix of appropriate dimension, and ddiag(V) is the diagonal matrix whose entries on the diagonal are those of V . By ‖V‖2F = ∑ i,j v 2 ij we denote the squared Frobenius norm of a matrix V , tr(V) is the trace of V , and rk(V) denotes the rank.\nII. ANALYSIS OPERATOR LEARNING"
    }, {
      "heading" : "A. Prior Art",
      "text" : "The topic of analysis operator learning has only recently started to be investigated, and only few prior work exist. In the sequel, we shortly review analysis operator learning methods that are applicable for image processing tasks.\nGiven a set of M training samples { si ∈ Rn }M i=1\n, the goal of analysis operator learning is to find a matrix Ω ∈ Rk×n with k ≥ n, which results in a maximally cosparse representation Ωsi of each training sample. As mentioned in Subsection I-B, the training samples are distinctive vectorized image patches extracted from a set of example images. Let S = [s1, . . . , sM ] ∈ Rn×M be a matrix where the training samples constitute its columns, the goal can be formulated as\nΩ? = arg min Ω g(ΩS), (6)\nwhere Ω is subject to some constraints, and g is some function that measures the sparsity of the matrix ΩS. In [24], an algorithm is proposed where the rows of the analysis operator are found sequentially by identifying directions that are orthogonal to a subset of the training samples. Starting from a randomly initialized vector ω ∈ Rn, a candidate row is found by first computing the inner product of ω with the entire training set, followed by extracting the reduced training set SR of samples whose inner product with ω is smaller than a threshold. Thereafter, ω is updated by taking the eigenvector corresponding to the smallest eigenvalue of SRS>R . This procedure is iterated several times until a convergence criterion is met. If the determined candidate vector is sufficiently distinctive from already found ones, it is added as a new row to Ω, otherwise it is discarded. This process is repeated until the desired number k of rows have been found.\nAn adaption of the widely known K-SVD dictionary learning algorithm to the problem of analysis operator learning is presented in [25]. As in the original K-SVD algorithm, g(ΩS) = ‖ΩS‖0 is employed as the sparsifying function and the target cosparsity is required as an input to the algorithm. The arising optimization problem is solved by alternating between a sparse coding stage over each training sample while fixing Ω using an ordinary analysis pursuit method, and updating the analysis operator using the optimized training set. Thereby, each row of Ω is updated in a similar way as described in the previous paragraph for the method of [24]. Interestingly, the operator learnt on piecewise constant image patches by [24] and [25] closely mimics the finite difference operator.\nIn [26], the authors employ g(ΩS) = ‖ΩS‖1 as the sparsity promoting function and suggest a constrained optimization technique that utilizes a projected subgradient method for iteratively solving (6). To exclude the trivial solution, the set of possible analysis operators is restricted to the set of Uniform Normalized Tight Frames, i.e. matrices with uniform row norm and orthonormal columns. The authors state that this algorithm has the limitation of requiring noiseless training samples whose analyzed vectors {Ωsi}Mi=1 are exactly cosparse.\nTo overcome this restriction, the same authors propose an extension of this algorithm that simultaneously learns the analysis operator and denoises the training samples, cf. [27]. This is achieved by alternating between updating the analysis operator via the projected subgradient algorithm and denoising the samples using an Augmented Lagrangian method. Therein, the authors state that their results for image denoising using the learnt operator are only slightly worse compared to employing the commonly used finite difference operator."
    }, {
      "heading" : "B. Motivation of Our Approach",
      "text" : "The learning algorithm presented here finds an analysis operator Ω from a set of training samples S by solving a problem related to (6). Therein, we employ the mixed (p, q)-pseudo-norm as a measure of sparsity, which for some matrix V ∈ Rk×M is given by\nJ(p,q)(V) := 1q M∑ j=1 ( 1 p k∑ i=1 |vij |p )q = 1q M∑ j=1 ( 1 p‖v:,j‖pp )q , (7)\nwith 0 ≤ p ≤ 1 and q > 1. Here, we employ V = ΩS and the interpretation of the sparsity measure (7) is as follows. The `p-pseudo-norm promotes sparsity of each analyzed training sample Ωsi. The exponent q > 1 ensures that sparsity is distributed over all analyzed training samples, and not concentrated onto only a small part. In this way, we find an analysis operator which is applicable to a broad set of signals, and not overfitted to some specific subset.\nCertainly, without additional prior assumptions on Ω, the useless solution Ω = 0 is the global minimizer of Problem (6). To avoid the trivial solution and for other reasons explained later in this section, we regularize the problem by imposing the following three constraints on Ω.\n(i) The rows of Ω have unit Euclidean norm, i.e. ‖ωi,:‖2 = 1 for i = 1, . . . , k. (ii) The analysis operator Ω has full rank, i.e. rk(Ω) = n.\n(iii) The analysis operator Ω does not have trivially linear dependent rows, i.e. ωi,: 6= ±ωj,: for i 6= j. The rank condition (ii) on Ω is motivated by the fact that different input samples s1, s2 ∈ Rn with s1 6= s2 should be mapped to different analyzed vectors Ωs1 6= Ωs2. With Condition (iii) redundant transform coefficients in an analyzed vector are avoided.\nThese constraints motivate the consideration of the set of full rank matrices with normalized columns, which admits a manifold structure known as the oblique manifold [28]\nOB(n, k) := {X ∈ Rn×k| rk(X ) = n, ddiag(X>X ) = Ik}. (8)\nNote, that this definition only yields a non-empty set if k ≥ n, which is the interesting case in this work. Thus, from now on, we assume k ≥ n. Remember that we require the rows of Ω to have unit Euclidean norm, hence, we restrict the transposed of the learnt analysis operator to be an element of OB(n, k).\nSince OB(n, k) is open and dense in the set of matrices with normalized columns, we need a penalty function that ensures the rank constraint (ii) and prevents iterates to approach the boundary of OB(n, k).\nLemma 1: The inequality 0 < det( 1kXX>) ≤ ( 1n)n holds true for all X ∈ OB(n, k), where 1 < n ≤ k. Proof: Due to the full rank condition on X , the product XX> is positive definite, consequently the strict\ninequality 0 < det( 1kXX>) applies. To see the second inequality of Lemma 1, observe that\n‖X‖2F = tr(XX>) = k, (9) which implies tr( 1kXX>) = 1. Since the trace of a matrix is equal to the sum of its eigenvalues, which are strictly positive in our case, it follows that the strict inequality 0 < λi < 1 holds true for all eigenvalues λi of 1kXX>. From the well known relation between the arithmetic and the geometric mean we see\nn √ Πλi ≤ 1n ∑ λi. (10)\nNow, since the determinant of a matrix is equal to the product of its eigenvalues, and with ∑ λi = tr( 1 kXX>) = 1, we have det( 1kXX>) = Πλi ≤ ( 1n)n, (11)\nwhich completes the proof. Recalling that Ω> ∈ OB(n, k) and considering Lemma 1, we can enforce the full rank constraint with the penalty function\nh(Ω) := − 1n log(n) log det( 1kΩ>Ω). (12) Regarding Condition (iii), the following result proves useful. Lemma 2: For a matrix X ∈ OB(n, k) with 1 < n ≤ k, the inequality |x>:,ix:,j | ≤ 1 applies, where equality holds true if and only if x:,i = ±x:,j . Proof: By the definition of OB(n, k) the columns of X are normalized, consequently Lemma 2 follows directly from the Cauchy-Schwarz inequality. Thus, Condition (iii) can be enforced via the logarithmic barrier function of the scalar products between all distinctive rows of Ω, i.e.\nr(Ω) := − ∑\n1≤i<j≤k log(1− (ω>i,:ωj,:)2). (13)\nFinally, combining all the introduced constraints, our optimization problem for learning the transposed analysis operator reads as\nΩ> = arg min X∈OB(n,k) J(p,q)(X>S) + κ h(X>) + µ r(X>). (14)\nTherein, the two weighting factors κ, µ ∈ R+ control the influence of the two constraints on the final solution. The following lemma clarifies the role of κ. Lemma 3: Let Ω be a minimum of h in the set of transposed oblique matrices, i.e.\nΩ> ∈ arg min X∈OB(n,k) h(X>), (15)\nthen the condition number of Ω is equal to one. Proof: It is well known that equality of the arithmetic and the geometric mean in Equation (10) holds true, if and only if all eigenvalues λi of 1kXX> are equal, i.e. λ1 = . . . = λn. Hence, if Ω> ∈ arg min X∈OB(n,k) h(X>), then det( 1kΩ >Ω) = ( 1n)\nn, and consequently all singular values of Ω coincide. This implies that the condition number of Ω, which is defined as the quotient of the largest to the smallest singular value, is equal to one.\nFrom Lemma 3 we can conclude that with larger κ the condition number of Ω approaches one. Now, recall the inequality\nσmin‖s1 − s2‖2 ≤ ‖Ω(s1 − s2)‖2 ≤ σmax‖s1 − s2‖2, (16) with σmin being the smallest and σmax being the largest singular value of Ω. From this it follows that an analysis operator found with a large κ, i.e. obeying σmin ≈ σmax, carries over distinctness of different signals to their analyzed versions.\nThe parameter µ regulates the redundancy between the rows of the analysis operator and consequently avoids redundant coefficients in the analyzed vector Ωs.\nLemma 4: The difference between any two entries of the analyzed vector Ωs is bounded by |ω>i,:s− ω>j,:s| ≤ √ 2(1− ω>i,:ωj,:) ‖s‖2. (17)\nProof: From the Cauchy-Schwarz inequality we get\n|ω>i,:s− ω>j,:s| = |(ωi,: − ωj,:)>s| ≤ ‖ωi,: − ωj,:‖2‖s‖2. (18) Since by definition ‖ωi,:‖2 = ‖ωj,:‖2 = 1, it follows that ‖ωi,: − ωj,:‖2 = √\n2(1− ω>i,:ωj,:). The above lemma implies, that if the ith entry of the analyzed vector is significantly larger than 0 and if ω>i,:ωj,: is close to 1, then the jth entry is also significantly larger than 0. In order to achieve large cosparsity, we aim to avoid this effect.\nIn the next section, we explain how the manifold structure of OB(n, k) can be exploited to efficiently learn the analysis operator."
    }, {
      "heading" : "III. ANALYSIS OPERATOR LEARNING ALGORITHM",
      "text" : "Knowing that the feasible set of solutions to Problem (14) is restricted to a smooth manifold allows us to formulate a geometric conjugate gradient (CG-) method to learn the analysis operator. Geometric CG-methods have been proven efficient in various applications, due to the combination of moderate computational complexity and good convergence properties, see e.g. [29] for a CG-type method on the oblique manifold.\nTo make this work self contained, we start by shortly reviewing the general concepts of optimization on matrix manifolds. After that we present the concrete formulas and implementation details for our optimization problem on the oblique manifold. For an in-depth introduction on optimization on matrix manifolds, we refer the interested reader to [30]."
    }, {
      "heading" : "A. Optimization on Matrix Manifolds",
      "text" : "Let M be a smooth Riemannian submanifold of Rn×k with the standard Frobenius inner product 〈Q,P〉 := tr(Q>P), and let f : Rn×k → R be a differentiable cost function. We consider the problem of finding\narg min X∈M\nf(X ). (19)\nThe concepts presented in this subsection are visualized in Figure 1 to alleviate the understanding. To every point X ∈ M one can assign a tangent space TXM. The tangent space at X is a real vector space containing all possible directions that tangentially pass through X . An element Ξ ∈ TXM is called a tangent vector at X . Each tangent space is associated with an inner product inherited from the surrounding Rn×k, which allows to measure distances and angles on M.\nThe Riemannian gradient of f at X is an element of the tangent space TXM that points in the direction of steepest ascent of the cost function on the manifold. As we require M to be a submanifold of Rn×k and since by assumption f is defined on the whole Rn×k, the Riemannian gradient G(X ) is simply the orthogonal projection of the (standard) gradient ∇f(X ) onto the tangent space TXM. In formulas, this reads as\nG(X ) := ΠTXM(∇f(X )). (20)\nTECHNICAL REPORT, TECHNISCHE UNIVERSITÄT MÜNCHEN 7\nX Y\nTXM\nTYM\n∇f(X ) G := ΠTXM(∇f(X ))\nH Γ(X ,H, t)\nT (G,X ,H, α) M\nX Y\nTXM\nTYM\n∇f(X ) G := ΠTXM(∇f(X ))\nH Γ(X ,H, t)\nT (G,X ,H, α) M\nX Y\nTXM\nTYM\n∇f(X ) G := ΠTXM(∇f(X ))\nH Γ(X ,H, t)\nT (G,X ,H, α) M\nX Y\nTXM\nTYM\n∇f(X ) G := ΠTXM(∇f(X ))\nH Γ(X ,H, t)\nT (G,X ,H, α)\nM\nX Y\nTXM\nTYM\n∇f(X ) G := ΠTXM(∇f(X ))\nH Γ(X ,H, t)\nT (G,X ,H, α) M\nX Y\nTXM\nTYM\n∇f(X ) G := ΠTXM(∇f(X ))\nH Γ(X ,H, t)\nT (G,X ,H, α) M\nX Y\nTXM\nTYM\n∇f(X ) G := ΠTXM(∇f(X ))\nH Γ(X ,H, t)\nT (G,X ,H, α) M\nX Y\nTXM\nTYM\n∇f(X ) G := ΠTXM(∇f(X ))\nH Γ(X ,H, t)\nT (G,X ,H, α) M\nX Y\nT M\nT M\n∇f(X ) G := Π X (∇f(X ))\nH Γ(X ,H, t)\nT (G,X ,H, α) M\nX Y\nTXM\nTYM\n∇f(X ) G := ΠTXM(∇f(X ))\nH Γ(X ,H, t)\nT (G,X ,H, α) M\nFig. 1. This figure shows two points X and Y on a manifold M together with their tangent spaces TXM and TYM. Furthermore, the Euclidean gradient ∇f(X ) and its projection onto the tangent space ΠTXM(∇f(X )) are depicted. The geodesic Γ(X ,H, t) in the direction of H ∈ TXM connecting the two points is shown. The dashed line typifies the role of a parallel transport of the gradient in TXM to TYM.\nA geodesic is a smooth curve Γ(X ,Ξ, t) emanating from X in the direction of Ξ ∈ TXM, which locally describes the shortest path between two points. Intuitively, it can be interpreted as the generalization of a straight line to a manifold.\nConventional line search methods search for the next iterate along a straight line. This is generalized to the manifold setting as follows.Given a current optimal point X (i) and a search direction H(i) ∈ TX (i)M at the ith iteration, the step size α(i) which results in sufficient decrease of f can be determined by finding the minimizer of\nα(i) = arg min t≥0 f(Γ(X (i),H(i), t)). (21)\nOnce α(i) has been determined, the new iterate is computed by\nX (i+1) = Γ(X (i),H(i), α(i)). (22)\nNow, one straightforward approach to minimize f is to alternate Equations (20), (21), and (22) usingH(i) = −G(i), with the short hand notation G(i) := G(X (i)), which corresponds to the steepest descent on a Riemmanian manifold. However, as in standard optimization, steepest descent only has a linear rate of convergence. Therefore, we employ a conjugate gradient method on a manifold, as it offers a superlinear rate of convergence, while still being applicable to large scale optimization problems with low computational complexity.\nThereby, the updated search direction H(i+1) ∈ TX (i+1)M is a linear combination of the gradient G(i+1) ∈ TX (i+1)M and the previous search direction H(i) ∈ TX (i)M. Since addition of vectors from different tangent spaces is not defined, we need to map H(i) from TX (i)M to TX (i+1)M. This is done by the so-called parallel transport T (Ξ,X (i),H(i), α(i)), which transports a tangent vector Ξ ∈ TX (i)M along the geodesic Γ(X (i),H(i), t) to the tangent space TX (i+1)M. Now, using the shorthand notation\nT (i+1)Ξ := T (Ξ,X (i),H(i), α(i)), (23) the new search direction is computed by\nH(i+1) = −G(i+1) + β(i)T (i+1)H(i) , (24)\nwhere β(i) ∈ R is calculated by some update formula adopted to the manifold setting. Most popular are the updates known as Fletcher-Reeves (FR), Polak-Ribière (PR), Hestenes-Stiefel (HS), and Dai-Yuan (DY) formulas. With Y(i+1) = G(i+1) − T (i+1)G(i) , they read as\nβ (i) FR = 〈G(i+1),G(i+1)〉 〈G(i),G(i)〉 , (25)\nβ (i) PR = 〈G(i+1),Y(i+1)〉 〈G(i),G(i)〉 , (26)\nβ (i) HS = 〈G(i+1),Y(i+1)〉 〈T (i+1)H(i) ,Y(i+1)〉 , (27)\nβ (i) DY = 〈G(i+1),G(i+1)〉 〈T (i+1)H(i) ,Y(i+1)〉 . (28)\nNow, a solution to Problem (19) is computed by alternating between finding the search direction on M and updating the current optimal point until some user specified convergence criterion is met, or a maximum number of iterations has been reached."
    }, {
      "heading" : "B. Geometric Conjugate Gradient for Analysis Operator Learning",
      "text" : "In this subsection we derive all ingredients to implement the geometric conjugate gradient method as described in the previous subsection for the task of learning the analysis operator. Results regarding the geometry of OB(n, k) are derived e.g. in [30]. To enhance legibility, and since the dimensions n and k are fixed throughout the rest of the paper, the oblique manifold is further on denoted by OB.\nThe tangent space at X ∈ OB is given by TXOB = {Ξ ∈ Rn×k| ddiag(X>Ξ) = 0}. (29)\nThe orthogonal projection of a matrix Q ∈ Rn×k onto the tangent space TXOB is ΠTXOB(Q) = Q−X ddiag(X>Q). (30)\nRegarding geodesics, note that in general a geodesic is the solution of a second order ordinary differential equation, meaning that for arbitrary manifolds, its computation as well as computing the parallel transport is not feasible. Fortunately, as the oblique manifold is a Riemannian submanifold of a product of k unit spheres Sn−1, the formulas for parallel transport and the exponential mapping allow an efficient implementation.\nLet x ∈ Sn−1 be a point on a sphere and h ∈ TxSn−1 be a tangent vector at x, then the geodesic in the direction of h is a great circle\nγ(x,h, t) = { x, if ‖h‖2 = 0 x cos(t‖h‖2) + h sin(t‖h‖2)‖h‖2 , otherwise.\n(31)\nThe associated parallel transport of a tangent vector ξ ∈ TxSn−1 along the great circle γ(x,h, t) reads as\nτ(ξ,x,h, t) = ξ − ξ >h\n‖h‖22\n( x‖h‖2 sin(t‖h‖2)+\nh(1− cos(t‖h‖2)) ) . (32)\nAs OB is a submanifold of the product of unit spheres, the geodesic through X ∈ OB in the direction of H ∈ TXOB is simply the combination of the great circles emerging by concatenating each column of X with the corresponding column of H, i.e.\nΓ(X ,H, t) = [ γ(x:,1,h:,1, t), . . . , γ(x:,k,h:,k, t) ] . (33)\nAccordingly, the parallel transport of Ξ ∈ TXOB along the geodesic Γ(X ,H, t) is given by T (Ξ,X ,H, t) =[\nτ(ξ:,1,x:,1,h:,1, t), . . . , τ(ξ:,k,x:,k,h:,k, t)\n] .\n(34)\nNow, to use the geometric CG-method for learning the analysis operator, we require a differentiable cost function f . However, the cost function presented in Problem (14) is not differentiable as the mixed (p, q)-pseudo-norm (7) is not smooth. We overcome this problem by exchanging function (7) with a smooth approximation, explicitly given by\nJ(p,q),ν(V) := M∑ j=1 1 q ( 1 p k∑ i=1 (v2ij + ν) p 2 )q , (35)\nwith ν ∈ R+ being the smoothing parameter. The smaller ν is, the more closely the approximation resembles the original function. Again, taking V = ΩS and with the shorthand notation zij := (ΩS)ij , the gradient of the applied sparsity promoting function (35) reads as\n∂ ∂ΩJ(p,q),ν(ΩS) =  M∑ j=1 ( 1 p k∑ i=1 (z2ij + ν) p 2 )q−1 k∑ i=1 { zij(z 2 ij + ν) p 2 −1Eij }] S>. (36)\nThe gradient of the rank penalty term (12) is ∂ ∂Ωh(Ω) = − 2kn log(n)Ω( 1kΩ>Ω)−1 (37)\nand the gradient of the logarithmic barrier function (13) is\n∂ ∂Ωr(Ω) =  ∑ 1≤i<j≤k\n2ω>i,:ωj,:\n1− (ω>i,:ωj,:)2 (Eij + Eji) Ω. (38) Combining Equations (36), (37), and (38), the gradient of the cost function\nf(X ) := J(p,q),ν(X>S) + κ h(X>) + µ r(X>) (39) applied for learning the analysis operator reads as\n∇f(X ) = ∂∂X J(p,q),ν(X>S) + κ ∂∂X h(X>) + µ ∂∂X r(X>). (40)\nRegarding the CG-update parameter β(i), we employ a hybridization of the Hestenes-Stiefel Formula (27) and the Dai Yuan formula (28)\nβ (i) hyb = max\n( 0,min(β\n(i) DY , β (i) HS) ) , (41)\nwhich has been suggested in [31]. As explained therein, formula (41) combines the good numerical performance of HS with the desirable global convergence properties of DY.\nFinally, to compute the step size α(i), we use an adaption of the well-known backtracking line search to the geodesic Γ(X (i),H(i), t). Thereby, an initial step size t(i)0 is iteratively decreased by a constant factor c1 < 1 until the Armijo condition is met, see Algorithm 1 for the entire procedure. In our implementation we empirically chose c1 = 0.9 and c2 = 10−2. As an initial guess for the step size at the first CG-iteration i = 0, we choose\nt (0) 0 = ‖G(0)‖−1F , (42)\nas proposed in [32]. In the subsequent iterations, the backtracking line search is initialized by the previous step size divided by the line search parameter, i.e. t(i)0 = α(i−1) c1 . Our complete approach for learning the analysis operator is summarized in Algorithm 2.\nAlgorithm 1 Backtracking Line Search on Oblique Manifold\nInput: t(i)0 > 0, 0 < c1 < 1, 0 < c2 < 0.5, X (i),G(i),H(i) Set: t← t(i)0\nwhile f(Γ(X (i),H(i), t)) > f(X (i)) + tc2〈G(i),H(i)〉 do t← c1t\nend while Output: α(i) ← t\nAlgorithm 2 Analysis Operator Learning Input: Initial analysis operator Ωinit, training data S, parameters p, q, ν, κ, µ Set: i← 0, X (0) ← Ω>init, H(0) ← −G(0)\nrepeat α(i) ← arg min\nt≥0 f(Γ(X (i),H(i), t)), cf. Algorithm 1 in conjunction with Equation (39)\nX (i+1) ← Γ(X (i),H(i), α(i)), cf. Equation (33) G(i+1) ← ΠTX(i+1)M(∇f(X (i+1))), cf. Equations (30) and (40) β(i) ← max ( 0,min(β\n(i) DY , β (i) HS) ) , cf. Equations (27), (28)\nH(i+1) ← −G(i+1) + β(i)T (i+1)H(i) , cf. Equation (34) i← i+ 1\nuntil ‖X (i) −X (i−1)‖F < 10−4 ∨ i = maximum # iterations Output: Ω? ← X (i)>"
    }, {
      "heading" : "IV. ANALYSIS OPERATOR BASED IMAGE RECONSTRUCTION",
      "text" : "In this section we explain how the analysis operator Ω? ∈ Rk×n is utilized for reconstructing an unknown image s ∈ RN from some measurements y ∈ Rm following the analysis approach (5). Here, the vector s ∈ RN denotes a vectorized image of dimension N = wh, with w being the width and h being the height of the image, respectively, obtained by stacking the columns of the image among each other. In the following, we will loosely speak of s as the image.\nRemember, that the size of Ω? is very small compared to the size of the image, and it has to be applied locally to small image patches rather than globally to the entire image. For global reconstruction, the simplest way is to partition the image into non-overlapping patches, reconstruct each patch individually, and stick the reconstructed patches together to form the final image. However, the success of this approach highly depends on the partitioning of the image. Furthermore, it leads to spurious artifacts at patch boundaries, and fails for example in inpainting tasks where large holes compared to the patch size have to be filled. Commonly, artifacts at patch boundaries are reduced by taking overlapping patches, reconstruct them again individually, and form the entire image by averaging the final reconstruction results in the overlapping regions. Still, this method misses global support during the reconstruction process, hence, it leads to poor inpainting results and is not applicable for Compressive Sensing tasks. To overcome these drawbacks, we introduce a method related to the patch based synthesis approach presented in [12] and the MRF approach from [33], which provides global support from local information. Instead of optimizing over each patch individually and combining them in a final step, we optimize over the entire image demanding that the analysis coefficients of each patch that is part of some predefined partition are sparse. When all possible patch positions are taken into account, the proposed reconstruction procedure is entirely partitioning-invariant. For legibility, we assume square patches of odd size, i.e. of size ( √ n×√n) with √n being odd. The non-square case can be easily adapted. Formally, let r ⊆ {1, . . . , h} and c ⊆ {1, . . . , w} denote a set of row and column indices, respectively, with ri+1 − ri = dv, ci+1 − ci = dh and 1 ≤ dv, dh ≤ √ n. Thereby, dv, dh determine the degree of overlap between two adjacent patches in vertical, and horizontal direction, respectively. We consider all image patches whose center pixel position is an element of the cartesian product set r× c. Hence, with | · | denoting the cardinality of a set, the\ntotal number of patches being considered is equal to |r||c|. Now, let Prc be a binary (n×N) matrix that extracts the patch centered at position (r, c). With this notation, we formulate the (global) sparsity promoting function as∑\nr∈r ∑ c∈c k∑ i=1 ((Ω?Prcs)2i + ν) p 2 . (43)\nIt measures the overall approximated `p-pseudo-norm of the considered analyzed image patches. Again as in Equation (35), ν ∈ R+ is a smoothing parameter. To use the same notation as in the standard analysis model formulation (5), we compactly rewrite Equation (43) as\ng(ΩF s) := K∑ i=1 ( (ΩF s)2i + ν ) p 2 , (44)\nwith K = k|r||c| and\nΩF :=  Ω?Pr1c1 Ω?Pr1c2 ...\nΩ?Pr|r|c|c|  ∈ RK×N (45) being the global analysis operator that expands the patch based one to the entire image. We treat image boundary effects by employing constant padding, i.e. replicating the values at the image boundaries b √ n\n2 c times, where b·c denotes rounding to the smaller integer. Certainly, for a real image processing application ΩF is too huge for being created explicitly and being applied in terms of matrix vector multiplication. Fortunately, applying ΩF and its transposed can be implemented efficiently using sliding window techniques, and the matrix vector notation is solely used to facilitate legibility.\nIn addition to the sparsity assumption, we exploit the fact that the range of pixel intensities is generally limited by a lower bound bl and an upper bound bu. We enforce this bounding constraint by minimizing the differentiable\nfunction b(s) := N∑ i=1 b(si), where b is a penalty term given as\nb(s) =  |s− bu|2 if s ≥ bu |s− bl|2 if s ≤ bl\n0 otherwise\n. (46)\nFinally, combining the two constraints (44) and (46) with the data fidelity term, the analysis based image recovery problem is to solve\ns? = arg min s∈RN 1 2‖As− y‖22 + b(s) + λg(ΩF s). (47)\nTherein, λ ∈ R+ balances between the sparsity of the solution’s analysis coefficients and the solution’s fidelity to the measurements. The measurement matrix A ∈ Rm×N and the measurements y ∈ Rm depend on the respective application to be handled."
    }, {
      "heading" : "V. APPLICATIONS AND EXPERIMENTAL RESULTS",
      "text" : "This section demonstrates various results for classical image reconstruction problems by employing an analysis operator Ω? that has been learnt with our algorithm proposed in Section III. We compare all our results with a total variation based approach [34]. It serves as a general benchmark for all the considered reconstruction applications since it is the most commonly used analysis operator. The present evaluation is limited to grayscale images, but the general concept can be straightforwardly extended to color images. Considering the dimension of Ω? ∈ Rk×n, we found from various tests that a two times overcomplete operator, i.e. k = 2n, results in good reconstruction performance independent from the selected patch size n. For the following experimental results we chose patches of size (7× 7), i.e. n = 49.\nNote that we learnt one general analysis operator Ω? ∈ R98×49 from the five test images shown in Figure 2, and used it unaltered in all presented applications. The employed training set consisted of M = 200 000 7 × 7- dimensional image patches each normalized to have unit Euclidean norm that have been randomly extracted from the five training images. Certainly, these images are not considered within the subsequent performance evaluations.\nWe initialized the learning process by a random R98×49 matrix with normalized rows. Tests with various other initializations like an overcomplete DCT showed that the initializing does not influence the final operator. For the sparsity promoting function (39), we chose p = 0.4 and q = 2. To closely approximate the `p-pseudo-norm, we chose a small smoothing parameter ν = 10−6. The optimal weighting factors κ and µ depend on the number of training samples and on the parameters used in (39). For the present situation we empirically found that κ = 106 and µ = 2000 lead to a suitable Ω?. In Figure 3 we show the learnt atoms of the analysis operator Ω? used throughout all our experiments.\nWe want to point out that this choice of parameters leads to a suitable general analysis operator, i.e. an operator with sparsifying property for the class of all natural images. It is thinkable that for more specific classes of images, for example medical imagery, other parameters may lead to a more suitable operator.\nFor all applications we reconstructed the images by solving the minimization problem (47) using the conjugate gradient method proposed in [35]. Considering the pixel intensity bounds, we used bl = 0 and bu = 255, which is the common intensity range in 8-bit grayscale image formats. For the sparsity promoting function (44), we chose the exponent p = 0.4 and the smoothing parameter ν = 10−6, which are the same values used for learning the operator Ω?. As explained in the previous section, our reconstruction algorithm works on overlapping image patches. We achieved the best results for the maximum possible overlap dh = dv = 1. The Lagrange multiplier λ and the measurements matrix A depend on the application, and are briefly discussed in each of the following subsections individually. The respective application scenarios show the general applicability of the learnt analysis operator.\nA. Image Denoising\nFirst, we present our results for denoising images that are corrupted by additive white Gaussian noise (AWGN) of different standard deviation σnoise, and compare them with those achieved by current state-of-the-art denoising methods. To allow a fair comparison, the images and the noise levels presented here are an excerpt of those commonly used in the literature. As usual, the peak signal-to-noise ratio (PSNR)\nPSNR = 10 log (\n2552N∑N i=1(si−s?i )2\n) (48)\nis used to quantify the reconstruction quality. For the case of image denoising, the measurements matrix A is simply the identity matrix, i.e. A = IN . Regarding the Lagrange multiplier λ, the larger it is chosen, the more noise is expected to be present. As it is common in the denoising literature, we assume the noise level σnoise to be given and adjust λ accordingly. From our experiments, we found that λ = σnoise16 is a good choice. We terminate our algorithm after 6−30 iterations depending on the noise level, i.e. the higher the noise level is the more iterations are required.\nTable I presents a comparison of the reconstruction results achieved by our method with (i) the K-SVD approach specifically designed for image denoising [12], (ii) the currently best denoising algorithm BM3D [36], and (iii) total-variation based denoising [34]. It can be seen, that in most of the cases our method performs slightly better than the K-SVD approach, especially for higher noise levels, and it is at most ≈ 0.5dB worse than BM3D. Compared to the TV approach, the reconstruction quality of our method is always significantly better independent of the noise level or the image content. For a visual assessment, Figure 4 shows exemplarily results achieved by the four denoising approaches compared here. Basically BM3D, K-SVD, and our method create natural similar looking images, whereas the TV based denoising method results in images showing the well known cartoon-like structure.\nB. Image Inpainting\nIn image inpainting as originally proposed in [2], the goal is to fill up a set of damaged or disturbing pixels such that the resulting image is visually appealing. This is necessary for the restoration of old damaged photographs, for removing disturbances caused by defect hardware, or for deleting unwanted objects from an image. Typically, the positions of the pixels to be filled up are a priori given. In our formulation, when N −m pixels must be inpainted, this leads to a binary m×N dimensional measurements matrix A, where each row contains exactly one entry equal to one. Its position corresponds to a pixel with known grayscale value. In other words, A reflects the available\nimage information. Regarding λ, it can be used in a way that our method simultaneously inpaints missing pixels and denoises the remaining ones.\nFor the first experiment, we disturbed some ground-truth images artificially by removing N −m pixels randomly distributed over the entire image as shown in Figure 5(a) and 5(c). In that way, the reconstruction quality can be judged both visually, and quantitatively in PSNR. We assumed the data to be free of noise, and empirically selected λ = 10−2. Figure 5(b) and 5(d) present two exemplary reconstruction results for reconstructing the \"Lena\" image from 40% and 10% of all pixels, respectively.\nThe second experiment demonstrates another typical inpainting problem taken from the literature [34], where a text overlaying an image has to be removed, see Figure 6. Additionally, we corrupted the remaining pixels by AWGN with σnoise = 10 to perform simultaneous inpainting and denoising. As for the denoising examples, we set λ = σnoise16 . Note that the regions that have to be filled are severely bigger than the dimension of the patches our analysis operator has been trained on. In Figure 6(b) we show the results achieved by our algorithm using the learnt general analysis operator Ω?. For comparison, we present the TV based results from [34] in Figure 6(c)."
    }, {
      "heading" : "C. Single Image Super-Resolution",
      "text" : "In single image super-resolution (SR), the goal is to reconstruct a high resolution image s ∈ RN from an observed low resolution image y ∈ Rm. Thereby, y is assumed to be a blurred and downsampled version of s. Mathematically, this process can be formulated as y = DBs + e with D ∈ Rm×N being a decimation operator and B ∈ RN×N being a blur operator. Hence, the measurement matrix is given by A = DB. In the ideal case, the\nexact blur kernel is known or an estimate is given. Here, we consider the more realistic case of an unknown blur kernel. Therefore, to apply our approach for magnifying an image by a factor of d in both vertical and horizontal dimension, we model the blur via a Gaussian kernel of dimension (2d− 1)× (2d− 1) and with standard deviation σblur = d 3 .\nFor our experiments, we artificially created a low resolution image by downsampling a ground-truth image by a factor of d using bicubic interpolation. Then, we magnified the low resolution image by the same factor, and compared them with the original image. In Figure 7(e) and (j), we show the reconstruction results of our method for magnifying the \"Girl\" and the \"Baboon\" image by d = 3 and compare it with bicubic interpolation Figure 7(b) and (g), TV based upsampling Figure 7(d) and (i), and the SR algorithm presented in [37] Figure 7(c) and (h). It can be seen, that our method can compete with the current state-of-the-art. We want to emphasize that the blur kernel used for downsampling is different from the blur kernel used in our upsampling scheme.\nNote that many single image super-resolution algorithms rely on clean noise free input data, whereas the general analysis approach as formulated in Equation (47) naturally handles noisy data, and is able to perform simultaneous upsampling and denoising. In Figure 8 we present some results achieved for simultaneously denoising and upsampling a low resolution image by a factor of d = 3 which has been corrupted by AWGN with σnoise = 8. As it can be seen, our method produces the best results both visually and quantitatively."
    }, {
      "heading" : "VI. CONCLUSION",
      "text" : "This paper dealt with the topic of learning the analysis operator from example image patches, and how to apply it for accurately solving several inverse problems in imaging. To learn the operator, we motivated an `pminimization on the set of full-rank matrices with normalized columns. A geometric conjugate gradient method on the oblique manifold is suggested to solve the arising optimization task. Furthermore, we gave a partitioning invariant method for employing the local patch based analysis operator such that globally consistent reconstruction results are achieved. For the famous tasks of image denoising, image inpainting, and single image super-resolution, we provide promising results that are competitive with current state-of-the-art techniques. Similar as for the synthesis signal reconstruction model with dictionaries, we expect that depending on the application at hand, the performance of the analysis approach can be further increased by learning the particular operator with regard to the specific problem, or employing a specialized training set."
    } ],
    "references" : [ {
      "title" : "Image denoising using scale mixtures of gaussians in the wavelet domain",
      "author" : [ "J. Portilla", "V. Strela", "M. Wainwright", "E. Simoncelli" ],
      "venue" : "IEEE Transactions on Image Processing, vol. 12, no. 11, pp. 1338–1351, 2003.",
      "citeRegEx" : "1",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Image inpainting",
      "author" : [ "M. Bertalmìo", "G. Sapiro", "V. Caselles", "C. Ballester" ],
      "venue" : "ACM SIGGRAPH, 2000, pp. 417–424.",
      "citeRegEx" : "2",
      "shortCiteRegEx" : null,
      "year" : 2000
    }, {
      "title" : "Example-based super-resolution",
      "author" : [ "W.T. Freeman", "T.R. Jones", "E.C. Pasztor" ],
      "venue" : "IEEE Computer Graphics and Applications, vol. 22, no. 2, pp. 56–65, 2002.",
      "citeRegEx" : "3",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Robust uncertainty principles: exact signal reconstruction from highly incomplete frequency information",
      "author" : [ "E.J. Candès", "J. Romberg", "T. Tao" ],
      "venue" : "IEEE Transactions on Information Theory, vol. 52, no. 2, pp. 489–509, 2006.",
      "citeRegEx" : "4",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "An introduction to the mathematical theory of inverse problems",
      "author" : [ "A. Kirsch" ],
      "venue" : null,
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 1991
    }, {
      "title" : "On the role of sparse and redundant representations in image processing",
      "author" : [ "M. Elad", "M.A.T. Figueiredo", "Y.M." ],
      "venue" : "Proceedings of the IEEE, vol. 98, no. 6, pp. 972–982, 2010.",
      "citeRegEx" : "6",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Analysis versus synthesis in signal priors",
      "author" : [ "M. Elad", "P. Milanfar", "R. Rubinstein" ],
      "venue" : "Inverse Problems, vol. 3, no. 3, pp. 947–968, 2007.",
      "citeRegEx" : "7",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Computational methods for sparse solution of linear inverse problems",
      "author" : [ "J.A. Tropp", "S.J. Wright" ],
      "venue" : "Proceedings of the IEEE, vol. 98, no. 6, pp. 948–958, 2010.",
      "citeRegEx" : "8",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "A theory for multiresolution signal decomposition: the wavelet representation",
      "author" : [ "S. Mallat" ],
      "venue" : "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 11, no. 7, pp. 674–693, 1989.",
      "citeRegEx" : "9",
      "shortCiteRegEx" : null,
      "year" : 1989
    }, {
      "title" : "Sparse geometric image representations with bandelets",
      "author" : [ "E. Le Pennec", "S. Mallat" ],
      "venue" : "IEEE Transactions on Image Processing, vol. 14, no. 4, pp. 423–438, 2005.",
      "citeRegEx" : "10",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "The curvelet transform for image denoising",
      "author" : [ "J.-L. Starck", "E.J. Candès", "D.L. Donoho" ],
      "venue" : "IEEE Transactions on Image Processing, vol. 11, no. 6, pp. 670–684, 2002.",
      "citeRegEx" : "11",
      "shortCiteRegEx" : null,
      "year" : 2002
    }, {
      "title" : "Image denoising via sparse and redundant representations over learned dictionaries",
      "author" : [ "M. Elad", "M. Aharon" ],
      "venue" : "IEEE Transactions on Image Processing, vol. 15, no. 12, pp. 3736–3745, 2006.",
      "citeRegEx" : "12",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Online learning for matrix factorization and sparse coding",
      "author" : [ "J. Mairal", "F. Bach", "J. Ponce", "G. Sapiro" ],
      "venue" : "Journal of Machine Learning Research, vol. 11, no. 1, pp. 19–60, 2010.",
      "citeRegEx" : "13",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Nonparametric bayesian dictionary learning for analysis of noisy and incomplete images",
      "author" : [ "M. Zhou", "H. Chen", "J. Paisley", "L. Ren", "L. Li", "Z. Xing", "D. Dunson", "G. Sapiro", "L. Carin" ],
      "venue" : "IEEE Transactions on Image Processing, vol. 21, no. 1, pp. 130–144, 2012.",
      "citeRegEx" : "14",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Dictionary learning algorithms for sparse representation",
      "author" : [ "K. Kreutz-Delgado", "J.F. Murray", "B.D. Rao", "K. Engan", "T.W. Lee", "T.J. Sejnowski" ],
      "venue" : "Neural computation, vol. 15, no. 2, pp. 349–396, 2003.",
      "citeRegEx" : "15",
      "shortCiteRegEx" : null,
      "year" : 2003
    }, {
      "title" : "Method of optimal directions for frame design",
      "author" : [ "K. Engan", "S. Aase", "J. Hakon Husoy" ],
      "venue" : "IEEE International Conference on Acoustics, Speech, and Signal Processing, 1999, pp. 2443–2446.",
      "citeRegEx" : "16",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "K-svd: An algorithm for designing overcomplete dictionaries for sparse representation",
      "author" : [ "M. Aharon", "M. Elad", "A. Bruckstein" ],
      "venue" : "IEEE Transactions on Signal Processing, vol. 54, no. 11, pp. 4311–4322, 2006.",
      "citeRegEx" : "17",
      "shortCiteRegEx" : null,
      "year" : 2006
    }, {
      "title" : "Double sparsity: Learning sparse dictionaries for sparse signal approximation",
      "author" : [ "R. Rubinstein", "M. Zibulevsky", "M. Elad" ],
      "venue" : "IEEE Transactions on Signal Processing, vol. 58, no. 3, pp. 1553–1564, 2010.  TECHNICAL REPORT, TECHNISCHE UNIVERSITÄT MÜNCHEN  18",
      "citeRegEx" : "18",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Dictionary learning",
      "author" : [ "I. Tošić", "P. Frossard" ],
      "venue" : "IEEE Signal Processing Magazine, vol. 28, no. 2, pp. 27–38, 2011.",
      "citeRegEx" : "19",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Cosparse analysis modeling - uniqueness and algorithms",
      "author" : [ "S. Nam", "M. Davies", "M. Elad", "R. Gribonval" ],
      "venue" : "IEEE International Conference on Acoustics, Speech and Signal Processing, 2011, pp. 5804–5807.",
      "citeRegEx" : "20",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Sparsity and smoothness via the fused lasso",
      "author" : [ "R. Tibshirani", "M. Saunders", "S. Rosset", "J. Zhu", "K. Knight" ],
      "venue" : "Journal of the Royal Statistical Society Series B, pp. 91–108, 2005.",
      "citeRegEx" : "21",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Signal restoration with overcomplete wavelet transforms: Comparison of analysis and synthesis priors",
      "author" : [ "I.W. Selesnick", "M.A.T. Figueiredo" ],
      "venue" : "In Proceedings of SPIE Wavelets XIII, 2009.",
      "citeRegEx" : "22",
      "shortCiteRegEx" : null,
      "year" : 2009
    }, {
      "title" : "Nonlinear total variation based noise removal algorithms",
      "author" : [ "L.I. Rudin", "S. Osher", "E. Fatemi" ],
      "venue" : "Physica D, vol. 60, no. 1-4, pp. 259–268, 1992.",
      "citeRegEx" : "23",
      "shortCiteRegEx" : null,
      "year" : 1992
    }, {
      "title" : "Sequential minimal eigenvalues - an approach to analysis dictionary learning",
      "author" : [ "B. Ophir", "M. Elad", "N. Bertin", "M.D. Plumbley" ],
      "venue" : "European Signal Processing Conference, 2011, pp. 1465–1469.",
      "citeRegEx" : "24",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "K-SVD dictionary-learning for the analysis sparse model",
      "author" : [ "R. Rubinstein", "T. Faktor", "M. Elad" ],
      "venue" : "IEEE International Conference on Acoustics, Speech, and Signal Processing, 2012.",
      "citeRegEx" : "25",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Analysis operator learning for overcomplete cosparse representations",
      "author" : [ "M. Yaghoobi", "S. Nam", "R. Gribonval", "M.E. Davies" ],
      "venue" : "European Signal Processing Conference, 2011, pp. 1470–1474.",
      "citeRegEx" : "26",
      "shortCiteRegEx" : null,
      "year" : 2011
    }, {
      "title" : "Noise aware analysis operator learning for approximately cosparse signals",
      "author" : [ "——" ],
      "venue" : "IEEE International Conference on Acoustics, Speech, and Signal Processing, 2012.",
      "citeRegEx" : "27",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "A continuous-time approach to the oblique procrustes problem",
      "author" : [ "N.T. Trendafilov" ],
      "venue" : "Behaviormetrika, vol. 26, no. 2, pp. 167–181, 1999.",
      "citeRegEx" : "28",
      "shortCiteRegEx" : null,
      "year" : 1999
    }, {
      "title" : "Blind source separation with compressively sensed linear mixtures",
      "author" : [ "M. Kleinsteuber", "H. Shen" ],
      "venue" : "IEEE Signal Processing Letters, vol. 19, no. 2, pp. 107–110, 2012.",
      "citeRegEx" : "29",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Optimization Algorithms on Matrix Manifolds",
      "author" : [ "P.-A. Absil", "R. Mahony", "R. Sepulchre" ],
      "venue" : null,
      "citeRegEx" : "30",
      "shortCiteRegEx" : "30",
      "year" : 2008
    }, {
      "title" : "An efficient hybrid conjugate gradient method for unconstrained optimization",
      "author" : [ "Y. Dai", "Y. Yuan" ],
      "venue" : "Annals of Operations Research, vol. 103, no. 1-4, pp. 33–47, 2001.",
      "citeRegEx" : "31",
      "shortCiteRegEx" : null,
      "year" : 2001
    }, {
      "title" : "Global convergence properties of conjugate gradient methods for optimization",
      "author" : [ "J.C. Gilbert", "J. Nocedal" ],
      "venue" : "SIAM Journal on Optimization, vol. 2, no. 1, pp. 21–42, 1992.",
      "citeRegEx" : "32",
      "shortCiteRegEx" : null,
      "year" : 1992
    }, {
      "title" : "Fields of experts: a framework for learning image priors",
      "author" : [ "S. Roth", "M. Black" ],
      "venue" : "IEEE Conference on Computer Vision and Pattern Recognition, 2005, pp. 860–867.",
      "citeRegEx" : "33",
      "shortCiteRegEx" : null,
      "year" : 2005
    }, {
      "title" : "Algorithms and software for total variation image reconstruction via first-order methods",
      "author" : [ "J. Dahl", "P.C. Hansen", "S. Jensen", "T.L. Jensen" ],
      "venue" : "Numerical Algorithms, vol. 53, no. 1, pp. 67–92, 2010.",
      "citeRegEx" : "34",
      "shortCiteRegEx" : null,
      "year" : 2010
    }, {
      "title" : "Cartoon-like image reconstruction via constrained `p-minimization",
      "author" : [ "S. Hawe", "M. Kleinsteuber", "K. Diepold" ],
      "venue" : "IEEE International Conference on Acoustics, Speech, and Signal Processing, 2012, pp. 717–720.",
      "citeRegEx" : "35",
      "shortCiteRegEx" : null,
      "year" : 2012
    }, {
      "title" : "Image denoising by sparse 3-d transform-domain collaborative filtering",
      "author" : [ "K. Dabov", "A. Foi", "V. Katkovnik", "K. Egiazarian" ],
      "venue" : "IEEE Transactions on Image Processing, vol. 16, no. 8, pp. 2080–2095, 2007.",
      "citeRegEx" : "36",
      "shortCiteRegEx" : null,
      "year" : 2007
    }, {
      "title" : "Image super-resolution via sparse representation",
      "author" : [ "J. Yang", "J. Wright", "T. Huang", "Y. Ma" ],
      "venue" : "IEEE Transactions on Image Processing, vol. 19, no. 11, pp. 2861–2873, 2010.",
      "citeRegEx" : "37",
      "shortCiteRegEx" : null,
      "year" : 2010
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Prominent examples are image denoising [1], inpainting [2], super-resolution [3], or image reconstruction from few indirect measurements as in Compressive Sensing [4].",
      "startOffset" : 39,
      "endOffset" : 42
    }, {
      "referenceID" : 1,
      "context" : "Prominent examples are image denoising [1], inpainting [2], super-resolution [3], or image reconstruction from few indirect measurements as in Compressive Sensing [4].",
      "startOffset" : 55,
      "endOffset" : 58
    }, {
      "referenceID" : 2,
      "context" : "Prominent examples are image denoising [1], inpainting [2], super-resolution [3], or image reconstruction from few indirect measurements as in Compressive Sensing [4].",
      "startOffset" : 77,
      "endOffset" : 80
    }, {
      "referenceID" : 3,
      "context" : "Prominent examples are image denoising [1], inpainting [2], super-resolution [3], or image reconstruction from few indirect measurements as in Compressive Sensing [4].",
      "startOffset" : 163,
      "endOffset" : 166
    }, {
      "referenceID" : 4,
      "context" : "Basically, in all these problems the goal is to reconstruct an unknown image s ∈ Rn as accurately as possible from a set of indirect and maybe corrupted measurements y ∈ Rm with n ≥ m, see [5] for a detailed introduction to inverse problems.",
      "startOffset" : 189,
      "endOffset" : 192
    }, {
      "referenceID" : 5,
      "context" : "[6], is that natural images admit a sparse representation x ∈ Rd over some dictionary D ∈ Rn×d with d ≥ n.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 6,
      "context" : "As the signal is synthesized from the sparse coefficients, the reconstruction model (3) is called the synthesis reconstruction model [7].",
      "startOffset" : 133,
      "endOffset" : 136
    }, {
      "referenceID" : 7,
      "context" : "For a broad overview of such algorithms, we refer the interested reader to [8].",
      "startOffset" : 75,
      "endOffset" : 78
    }, {
      "referenceID" : 8,
      "context" : "Popular examples include Wavelets [9], Bandlets [10], and Curvlets [11] among several others, or a concatenation of various such bases/dictionaries.",
      "startOffset" : 34,
      "endOffset" : 37
    }, {
      "referenceID" : 9,
      "context" : "Popular examples include Wavelets [9], Bandlets [10], and Curvlets [11] among several others, or a concatenation of various such bases/dictionaries.",
      "startOffset" : 48,
      "endOffset" : 52
    }, {
      "referenceID" : 10,
      "context" : "Popular examples include Wavelets [9], Bandlets [10], and Curvlets [11] among several others, or a concatenation of various such bases/dictionaries.",
      "startOffset" : 67,
      "endOffset" : 71
    }, {
      "referenceID" : 11,
      "context" : "This is desirable for various image reconstruction applications as it readily improves their performance and accuracy [12]–[14].",
      "startOffset" : 118,
      "endOffset" : 122
    }, {
      "referenceID" : 13,
      "context" : "This is desirable for various image reconstruction applications as it readily improves their performance and accuracy [12]–[14].",
      "startOffset" : 123,
      "endOffset" : 127
    }, {
      "referenceID" : 14,
      "context" : "Three conceptually different approaches for learning a dictionary became well established, which are probabilistic ones like [15] or [16], clustering based ones such as K-SVD [17], and recent approaches which aim at learning dictionaries with specific matrix structures that allow fast computations like [18].",
      "startOffset" : 125,
      "endOffset" : 129
    }, {
      "referenceID" : 15,
      "context" : "Three conceptually different approaches for learning a dictionary became well established, which are probabilistic ones like [15] or [16], clustering based ones such as K-SVD [17], and recent approaches which aim at learning dictionaries with specific matrix structures that allow fast computations like [18].",
      "startOffset" : 133,
      "endOffset" : 137
    }, {
      "referenceID" : 16,
      "context" : "Three conceptually different approaches for learning a dictionary became well established, which are probabilistic ones like [15] or [16], clustering based ones such as K-SVD [17], and recent approaches which aim at learning dictionaries with specific matrix structures that allow fast computations like [18].",
      "startOffset" : 175,
      "endOffset" : 179
    }, {
      "referenceID" : 17,
      "context" : "Three conceptually different approaches for learning a dictionary became well established, which are probabilistic ones like [15] or [16], clustering based ones such as K-SVD [17], and recent approaches which aim at learning dictionaries with specific matrix structures that allow fast computations like [18].",
      "startOffset" : 304,
      "endOffset" : 308
    }, {
      "referenceID" : 18,
      "context" : "For a comprehensive overview of dictionary learning techniques see [19].",
      "startOffset" : 67,
      "endOffset" : 71
    }, {
      "referenceID" : 6,
      "context" : "which is known as the analysis model [7].",
      "startOffset" : 37,
      "endOffset" : 40
    }, {
      "referenceID" : 19,
      "context" : "To emphasize this difference, the term cosparsity has been introduced in [20], which simply counts the number of zero elements of Ωs.",
      "startOffset" : 73,
      "endOffset" : 77
    }, {
      "referenceID" : 20,
      "context" : "Different analysis operators proposed in the literature include the fused Lasso [21], the translation invariant wavelet transform [22], and probably best known the finite difference operator which is closely related to the total-variation [23].",
      "startOffset" : 80,
      "endOffset" : 84
    }, {
      "referenceID" : 21,
      "context" : "Different analysis operators proposed in the literature include the fused Lasso [21], the translation invariant wavelet transform [22], and probably best known the finite difference operator which is closely related to the total-variation [23].",
      "startOffset" : 130,
      "endOffset" : 134
    }, {
      "referenceID" : 22,
      "context" : "Different analysis operators proposed in the literature include the fused Lasso [21], the translation invariant wavelet transform [22], and probably best known the finite difference operator which is closely related to the total-variation [23].",
      "startOffset" : 239,
      "endOffset" : 243
    }, {
      "referenceID" : 6,
      "context" : "The question is: can the performance of analysis based signal reconstruction be improved upon when a learnt analysis operator is applied instead of a predefined one, as it is the case for the synthesis model where learnt dictionaries outperform analytic dictionaries? As it has been discussed in [7], the two models differ significantly, and the naïve way of learning a dictionary and simply employing its transposed or its pseudo-inverse as the learnt analysis operator fails.",
      "startOffset" : 296,
      "endOffset" : 299
    }, {
      "referenceID" : 23,
      "context" : "In [24], an algorithm is proposed where the rows of the analysis operator are found sequentially by identifying directions that are orthogonal to a subset of the training samples.",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 24,
      "context" : "An adaption of the widely known K-SVD dictionary learning algorithm to the problem of analysis operator learning is presented in [25].",
      "startOffset" : 129,
      "endOffset" : 133
    }, {
      "referenceID" : 23,
      "context" : "Thereby, each row of Ω is updated in a similar way as described in the previous paragraph for the method of [24].",
      "startOffset" : 108,
      "endOffset" : 112
    }, {
      "referenceID" : 23,
      "context" : "Interestingly, the operator learnt on piecewise constant image patches by [24] and [25] closely mimics the finite difference operator.",
      "startOffset" : 74,
      "endOffset" : 78
    }, {
      "referenceID" : 24,
      "context" : "Interestingly, the operator learnt on piecewise constant image patches by [24] and [25] closely mimics the finite difference operator.",
      "startOffset" : 83,
      "endOffset" : 87
    }, {
      "referenceID" : 25,
      "context" : "In [26], the authors employ g(ΩS) = ‖ΩS‖1 as the sparsity promoting function and suggest a constrained optimization technique that utilizes a projected subgradient method for iteratively solving (6).",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 26,
      "context" : "[27].",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 27,
      "context" : "These constraints motivate the consideration of the set of full rank matrices with normalized columns, which admits a manifold structure known as the oblique manifold [28] OB(n, k) := {X ∈ Rn×k| rk(X ) = n, ddiag(X>X ) = Ik}.",
      "startOffset" : 167,
      "endOffset" : 171
    }, {
      "referenceID" : 28,
      "context" : "[29] for a CG-type method on the oblique manifold.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 29,
      "context" : "For an in-depth introduction on optimization on matrix manifolds, we refer the interested reader to [30].",
      "startOffset" : 100,
      "endOffset" : 104
    }, {
      "referenceID" : 29,
      "context" : "in [30].",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 30,
      "context" : "which has been suggested in [31].",
      "startOffset" : 28,
      "endOffset" : 32
    }, {
      "referenceID" : 31,
      "context" : "as proposed in [32].",
      "startOffset" : 15,
      "endOffset" : 19
    }, {
      "referenceID" : 11,
      "context" : "To overcome these drawbacks, we introduce a method related to the patch based synthesis approach presented in [12] and the MRF approach from [33], which provides global support from local information.",
      "startOffset" : 110,
      "endOffset" : 114
    }, {
      "referenceID" : 32,
      "context" : "To overcome these drawbacks, we introduce a method related to the patch based synthesis approach presented in [12] and the MRF approach from [33], which provides global support from local information.",
      "startOffset" : 141,
      "endOffset" : 145
    }, {
      "referenceID" : 33,
      "context" : "We compare all our results with a total variation based approach [34].",
      "startOffset" : 65,
      "endOffset" : 69
    }, {
      "referenceID" : 34,
      "context" : "For all applications we reconstructed the images by solving the minimization problem (47) using the conjugate gradient method proposed in [35].",
      "startOffset" : 138,
      "endOffset" : 142
    }, {
      "referenceID" : 35,
      "context" : "(b) BM3D denoising [36], PSNR = 30.",
      "startOffset" : 19,
      "endOffset" : 23
    }, {
      "referenceID" : 11,
      "context" : "(c) K-SVD denoising [12], PSNR = 29.",
      "startOffset" : 20,
      "endOffset" : 24
    }, {
      "referenceID" : 33,
      "context" : "(d) TV denoising [34], PSNR = 28.",
      "startOffset" : 17,
      "endOffset" : 21
    }, {
      "referenceID" : 11,
      "context" : "Table I presents a comparison of the reconstruction results achieved by our method with (i) the K-SVD approach specifically designed for image denoising [12], (ii) the currently best denoising algorithm BM3D [36], and (iii) total-variation based denoising [34].",
      "startOffset" : 153,
      "endOffset" : 157
    }, {
      "referenceID" : 35,
      "context" : "Table I presents a comparison of the reconstruction results achieved by our method with (i) the K-SVD approach specifically designed for image denoising [12], (ii) the currently best denoising algorithm BM3D [36], and (iii) total-variation based denoising [34].",
      "startOffset" : 208,
      "endOffset" : 212
    }, {
      "referenceID" : 33,
      "context" : "Table I presents a comparison of the reconstruction results achieved by our method with (i) the K-SVD approach specifically designed for image denoising [12], (ii) the currently best denoising algorithm BM3D [36], and (iii) total-variation based denoising [34].",
      "startOffset" : 256,
      "endOffset" : 260
    }, {
      "referenceID" : 1,
      "context" : "Image Inpainting In image inpainting as originally proposed in [2], the goal is to fill up a set of damaged or disturbing pixels such that the resulting image is visually appealing.",
      "startOffset" : 63,
      "endOffset" : 66
    }, {
      "referenceID" : 35,
      "context" : "EACH CELL PRESENTS FOUR PSNR RESULTS IN DB ACHIEVED BY DENOISING THE RESPECTIVE IMAGE WITH FOUR DIFFERENT ALGORITHMS, WHICH ARE: TOP LEFT BM3D [36](THE CURRENTLY BEST RATED DENOISING ALGORITHM), TOP RIGHT TOTAL-VARIATION DENOISING (BEST WORKING ANALYSIS METHOD), BOTTOM LEFT K-SVD DENOISING [12], BOTTOM RIGHT PROPOSED METHOD.",
      "startOffset" : 143,
      "endOffset" : 147
    }, {
      "referenceID" : 11,
      "context" : "EACH CELL PRESENTS FOUR PSNR RESULTS IN DB ACHIEVED BY DENOISING THE RESPECTIVE IMAGE WITH FOUR DIFFERENT ALGORITHMS, WHICH ARE: TOP LEFT BM3D [36](THE CURRENTLY BEST RATED DENOISING ALGORITHM), TOP RIGHT TOTAL-VARIATION DENOISING (BEST WORKING ANALYSIS METHOD), BOTTOM LEFT K-SVD DENOISING [12], BOTTOM RIGHT PROPOSED METHOD.",
      "startOffset" : 291,
      "endOffset" : 295
    }, {
      "referenceID" : 33,
      "context" : "The second experiment demonstrates another typical inpainting problem taken from the literature [34], where a text overlaying an image has to be removed, see Figure 6.",
      "startOffset" : 96,
      "endOffset" : 100
    }, {
      "referenceID" : 33,
      "context" : "For comparison, we present the TV based results from [34] in Figure 6(c).",
      "startOffset" : 53,
      "endOffset" : 57
    }, {
      "referenceID" : 36,
      "context" : "In Figure 7(e) and (j), we show the reconstruction results of our method for magnifying the \"Girl\" and the \"Baboon\" image by d = 3 and compare it with bicubic interpolation Figure 7(b) and (g), TV based upsampling Figure 7(d) and (i), and the SR algorithm presented in [37] Figure 7(c) and (h).",
      "startOffset" : 269,
      "endOffset" : 273
    }, {
      "referenceID" : 36,
      "context" : "(c) Method [37], PSNR = 32.",
      "startOffset" : 11,
      "endOffset" : 15
    }, {
      "referenceID" : 36,
      "context" : "(h) Method [37], PSNR = 23.",
      "startOffset" : 11,
      "endOffset" : 15
    }, {
      "referenceID" : 36,
      "context" : "(d) Method [37], PSNR = 22.",
      "startOffset" : 11,
      "endOffset" : 15
    } ],
    "year" : 2017,
    "abstractText" : "Exploiting a priori known structural information lies at the core of many image reconstruction methods that can be stated as inverse problems. The synthesis model, which assumes that images can be decomposed into a linear combination of very few atoms of some dictionary, is now a well established tool for the design of image reconstruction algorithms. An interesting alternative is the analysis model, where the signal is multiplied by an analysis operator and the outcome is assumed to be the sparse. This approach has only recently gained increasing interest. The quality of reconstruction methods based on an analysis model severely depends on the right choice of the suitable operator. In this work, we present an algorithm for learning an analysis operator from training images. Our method is based on an `p-norm minimization on the set of full rank matrices with normalized columns. We carefully introduce the employed conjugate gradient method on manifolds, and explain the underlying geometry of the constraints. Moreover, we compare our approach to state-of-the-art methods for image denoising, inpainting, and single image super-resolution. Our numerical results show competitive performance of our general approach in all presented applications compared to the specialized state-of-the-art techniques.",
    "creator" : "LaTeX with hyperref package"
  }
}