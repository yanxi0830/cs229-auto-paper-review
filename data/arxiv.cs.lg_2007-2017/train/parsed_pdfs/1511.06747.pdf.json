{
  "name" : "1511.06747.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ "Behnam Neyshabur", "Ryota Tomioka" ],
    "emails" : [ "bneyshabur@ttic.edu", "ryoto@microsoft.com", "rsalakhu@cs.toronto.edu", "nati@ttic.edu" ],
    "sections" : [ {
      "heading" : "1 INTRODUCTION",
      "text" : "The choice of optimization method for non-convex, over-parametrized models such as feed-forward neural networks is crucial to the success of learning—not only does it affect the runtime until convergence, but it also effects which minimum (or potentially local minimum) we will converge to, and thus the generalization ability of the resulting model. Optimization methods are inherently tied to a choice of geometry over parameter space, which in turns induces a geometry over model space, which plays an important role in regularization and generalization (Neyshabur et al., 2015c).\nIn the past year, two efficient alternative optimization approaches for feed-forward neural networks were proposed, based on intuitions about parametrization, normalization and the geometry of parameter space: Path-SGD (Neyshabur et al., 2015a) was derived as steepest descent algorithm with respect to the `2-path regularizer; The `2-path regularizer is the sum over all paths in the network of the squared product over all weights in the path (Neyshabur et al., 2015b). The `2-path regularizer is equivalent to per-unit `2 regularization (maximum over all units of `2 norm of incoming weights to the unit) but is invariant to weight reparametrization. On the other hand, batch-normalization (Ioffe & Szegedy, 2015) was derived as a way of controlling the variance of the input each unit receives. In this paper, we propose a unified framework which includes both approaches, and allows us to obtain additional methods which interpolate between them. Using our unified framework, we can also tease apart and combine two different aspects of these two approaches: data-dependence and invariance to weight-rebalancing.\nMore specifically, our unified framework is based on a quantity we refer to as γv (defined in Section 2) that can be calculated for each node v in the network recursively. This measure is parametrized by a choice of “normalization matrix” R, and different choices for this matrix incorporate different amounts of data dependencies: for path-SGD, R is a non-data-dependent diagonal matrix, while for batch normalization it is a data-dependent covariance matrix, and we can interpolate between the two extremes. Once γv is defined, and for any choice of R, we define two different optimization approaches: one relying on a normalized re-parameterization at each layer, as in batch normalization (Section 3), and the other an approximate steepest descent as in path-SGD, which we refer to as DDP-SGD (Data Dependent Path SGD) and can be implemented efficiently via forward and backward propagation on the network (Section 4). We can now mix and match between the choice of R and the choice of optimization approach.\nar X\niv :1\n51 1.\n06 74\n7v 1\n[ cs\n.L G\n] 2\n0 N\nov 2\nOne particular advantage of the approximate steepest descent approach (DDP-SGD) over the normalization approach is that it is invariant to weight rebalancing (discussed in Section 5). This is true regardless of the amount of data-dependence used. That is, it operates more directly on the model (the function defined by the weights) rather than the parametrization (the values of the weights themselves). This brings us to a more general discussion of parametrization invariance in feedforward networks (Section 6).\nOur unified framework and study of in invariances also allows us to relate the different optimization approaches to Natural Gradients (Amari, 1998). In particular, we show that DDP-SGD with full data-dependence can be seen as an efficient approximation of the natural gradient using only the diagonal of the Fisher information matrix (as opposed to the full matrix).\nRELATED WORKS\nThere has been an ongoing effort for better understanding of the optimization in deep networks and several heuristics have been suggested to improve the training (Le Cun et al., 1998; Larochelle et al., 2009; Glorot & Bengio, 2010; Sutskever et al., 2013). In order to incorporate the curvature information, several methods have proposed different ways to approximate Fisher information matrix in deep learning models (Desjardins et al., 2015; Martens & Grosse, 2015; Grosse & Salakhudinov, 2015). Pascanu & Bengio (2014) also discusses the connections between Natural Gradients and some of the other proposed methods for training neural networks: Hessian-Free Optimization (Martens, 2010), Krylov Subspace Descent (Vinyals & Povey, 2011) and TONGA (Roux et al., 2008).\nFEEDFORWARD NEURAL NETS\nLet G = (V,E) be a directed acyclic graph with nin input nodes (those with only outgoing connections) and nout output nodes (those with only incoming connection). We associate a weight we for e ∈ E and define fw : Rnin → Rnout recursively as follows:\nfw(x)[v] = zv (v ∈ Vout), (1) yv = σ (zv) , zv = 〈 w→v, yN in(v) 〉 = ∑ u∈N in(v) wu→v · yu (v /∈ Vin),\nyv = x[v] (v ∈ Vin),\nwhere x[v] denotes the element of vector x corresponding to input unit v, Vin and Vout are the set of input and output nodes, respectively, N in(v) and N out(v) are the set of nodes that are endpoints of incoming edges to v and outgoing edges from v respectively, and σ is the Rectified Linear Unit (ReLU) activation, i.e. σ(z) = max(0, z). Moreover, we refer to the vector of incoming and outgoing edges to node v by w→v and wv→ respectively. Similarly, yN in(v) and yN out(v) are defined as vectors of values of y for all nodes in N in(v) and N out(v) respectively."
    }, {
      "heading" : "2 A UNIFIED FRAMEWORK",
      "text" : "We define a measure on each node as follows:\nγ2v(w) = w > →vRvw→v (2)\nwhere Rv is a positive semidefinite matrix. We will focus on a specific class of matrices Rv; that is for any 0 ≤ α ≤ 1, we look at:\nRv = αE [ yN in(v)y > N in(v) ] + (1− α) diag ( γ2N in(v)(w) ) (3)\nTherefore, for each node v the measure γ can be calculated as: γ2v(w) = αE [ z2v ] + (1− α)w>→v diag ( γ2N in(v)(w) ) w→v (v /∈ Vin), (4)\nwhere for any v ∈ Vin, we define γv = 1. Intuitively, γ2v is a convex combination of the variance of the input node v receives and a regularization term that is a weighted norm of incoming weights to\nv. This provides a regularized approximation of the variance for the node v. The interesting point here is that the weight of this weighted norm is nothing but the regularized approximation of the variances of nodes in the previous layer.\nWe can now define the DDP-regularizer of a given network as follows: γ2net(w) = ∑ v∈Vout γ2v(w) (5)\nSuch a measure induces a complexity measure on functions f that can be presented by network G. The complexity of f based on DDP-regularizer over G can be defined as the complexity of the simplest setting that represent the function:\nγ2(f) = inf fG,w=f γ2net(w) (6)\nThe DDP-regularizer coincides with the (squared) per-unit `2 norm for α = 0 and the per-unit input variance for α = 1. Therefore, normalizing each unit based on this measure leads to a generalization of batch normalization and taking the steepest descent direction with respect to this measure leads to a generalization of path-SGD.\nFor simplicity of the notation, we use γ2 instead of γ2(w) in the rest of the paper."
    }, {
      "heading" : "3 BATCH-NORMALIZED NETWORKS",
      "text" : "Here we formulate Batch-Normalization based on the measure γ. In the Batch-Normalization formulation, for each node v we normalize the output of the linear layer before the activations which is equivalent to the reparameterization w̃→v = 1γvw→v\n1. Suppose that during the training, we use mini-batches of size n to estimate the second moments. Then the gradients can be calculated as follows:\n∂`\n∂w→v = n∑ i=1 ∂` ∂z (i) v y(i)N(v) γv − z(i)v α n ∑n i=1 y (i) N in(v) 〈 y (i) N in(v) , w→v 〉 + (1− α)w→v γv  (7) ∂`\n∂y (i) u\n= ∑\nv∈N(u)→\nwu→v γv ∂`\n∂z (i) v\n− α n n∑ j=1 ∂` ∂z (j) v z (i) v z (j) v wu→v γv  (8) =\n∑ v∈N(u)→ wu→v γv  ∂` ∂z (i) v − 1 n n∑ j=1 ∂` ∂z (j) v − αz (i) v n n∑ j=1 ∂` ∂z (j) v z(j)v  Here, α = 1 corresponds to the original batch-normalization (Ioffe & Szegedy, 2015); i.e. normalizing the outputs based by the standard deviation, α = 0 corresponds to normalizing the output of each hidden unit by `2 norm of the incoming weights to that unit and 0 < α < 1 combine this two measure and to obtain a regularized variance.\nInterestingly, we can observe that for any node v the updates direction of incoming weights to v is exactly orthogonal to the weights. i.e. we have that 〈 w→v,\n∂` ∂w→v\n〉 = 0. That means weight updates\n1In Batch Normalization as presented in (Ioffe & Szegedy, 2015), the output of node v is calculated as yv = [rẑv+θ]+ where r and θ are extra scaling and bias parameters and ẑv = zv−E[zv ]γv . While our framework can be easily updated to include these changes, we have removed them for simplicity. Also, note that the for networks with RELU activations, extra scaling parameters in the middle layers are not affecting the expressive power of the network because one can simply propagate the scalings to the top layer.\nin Batch-Normalized networks are done in a way that it prevents the norm of weights to change considerably after each updates."
    }, {
      "heading" : "4 APPROXIMATE STEEPEST DESCENT OPTIMIZATION",
      "text" : "We now consider the steepest descent direction with respect to DDP regularizer γ2. Note that γ2net is not convex. However, we can approximate γ2net by a convex quadratic function γ̂ 2 net. For any parameter w, if the diagonal of the hessian Hγ2net has positive entries, we can approximate γ 2 net by γ̂2net where hessian Hγ̂2net is the diagonal of Hγ2net . Now, the Bregman Divergence for γ̂ 2 net can be written as:\nDγ̂2net(w,w ′) = ‖w − w′‖H\nγ̂2net = ∑ e∈E ∂2γ2net ∂w2e (we − w′e)2 (9)\nTherefore, the steepest descent update with respect to the distance Dγ̂2net can be obtained as follows:\nw(t+1) = min w η 〈 ∇L(w), w − w(t) 〉 + 1 2 Dγ̂2net(w,w (t))\nSolving for w gives us the following update rule:\nw(t+1)e = we − η\nκe(w)\n∂L\n∂we (w(t)) (10)\nwhere κe(w) = 12 ∂2γ2net ∂w2e ."
    }, {
      "heading" : "4.1 IMPLEMENTATION",
      "text" : "In order to compute the second derivatives κe(w) = ∂2γ2net ∂w2e , we first calculate the first derivative. The backpropagation can be done through γ2u and z (i) u but this makes it difficult to find the second derivatives. Instead we propagate the loss through γ2u and the second order terms of the form z (i) u1 z (i) u2 :\n∂γ2net ∂γ2u\n= (1− α) ∑\nv∈N out(u)\n∂γ2net ∂γ2v w2u→v (11)\n∂γ2net\n∂(z (i) u1 z (i) u2 )\n= α [ ∂γ2net ∂γ2u1 ] u1=u2 +  ∑ (v1,v2)∈(N out(u1))2 ∂γ2net ∂(z (i) v1 z (i) v2 ) wu1→v1wu2→v2  z (i) u1 >0,z (i) u2 >0\n(12) Now we can calculate the partials for wu→v as follows:\n∂γ2net ∂wu→v = 2(1− α)∂γ 2 net ∂γ2v γ2uwu→v + 2 n∑ i=1 ∑ v′∈N out(u) ∂γ2net ∂(z (i) v z (i) v′ ) y(i)u z (i) v′ (13)\nSince the partials ∂γ 2 net ∂γ2u and ∂γ 2 net\n∂(z (i) u1 z (i) u2\n) do not depend on wu→v , the second order derivative can be\ncalculated directly:\nκu→v(w) = 1\n2 ∂2γ2net ∂w2u→v = (1− α)∂γ 2 net ∂γ2v γ2u + n∑ i=1 ∂γ2net ∂ ( z (i) v 2) (y(i)u )2 (14) The above computation can be done in the time equal to |Vout| backpropagation on the mini-batch (plus only a single backpropagation for γ2) as follows:\nκu→v(w) = 1\n2 ∂2γ2net ∂w2u→v = (1− α)∂γ 2 net ∂γ2v γ2u + n∑ i=1 ( y(i)u )2 ∑ v′∈Vout\n( ∂z\n(i) v′ ∂z (i) v\n)2 (15)\nIt is also sensible to define the path-regularizer as the maximum of γ over output units (instead of sum):\nγ̃2net = max v∈Vout γ2v\nIn that case, the computational complexity boils down to a single extra backpropagation on the mini-batch:\nκu→v(w) = 1\n2 ∂2γ̃2net ∂w2u→v = (1− α)∂γ̃ 2 net ∂γ2v γ2u + n∑ i=1 ( y(i)u )2∂z(i)vmaxi ∂z (i) v 2 (16) where vmaxi = arg maxv∈Vout\n∣∣γ2v ∣∣"
    }, {
      "heading" : "4.2 RELATION TO NATURAL GRADIENT",
      "text" : "In this section, we show that DDP-SGD is a diagonal approximation of Natural Gradient. For any network and a probability distribution defined on the outputs, we can approximate the Fisher information matrix with its diagonal elements (see appendix A for more information). In this case the update step normalizes each dimension of the gradient with the corresponding element on the diagonal of Fisher information matrix:\nw(t+1)e = we − η\nF (w)[e, e]\n∂L\n∂we (w(t)). (17)\nFor a multivariate gaussian with unit variance and centralized at the output of the network, namely log q(c|x) = 12 ∑ v′∈Vout(cv′ − zv′) 2 + const., the diagonal can be calculated as:\nF (w)[u→ v, u→ v] = Ex∼p(x) [ y2u ∑ v′∈Vout ( ∂zv′ ∂zv )2] , (18)\nusing (34). This is exactly equal to DDP-SGD for α = 1. In classification tasks, we usually use softmax activations and it that case, the diagonal of Fisher Information can be calculated as:\nF (w)[u→ v, u→ v] = E [ y2u ∑ v′∈Vout ∑ v′′∈Vout (1v′=v′′ · σsoft(zv′)− σsoft(zv′)σsoft(zv′′)) ∂zv′ ∂zv · ∂zv ′′ ∂zv ] (19) where σsoft is the softmax function and we have that σsoft(zv) = e zv∑\nv′∈Vout ezv′\n. The above values\ncan be calculated as fast as |Vout| backpropagation on the mini-batch. Moreover, considering the connection between DDP-SGD and the diagonal of outer product of Jacobians, we can consider adding the path-regularizer to the diagonal of Fisher information."
    }, {
      "heading" : "5 NODE-WISE INVARIANCE",
      "text" : "We say that network G is invariant to transformation T : R|E| → R|E|, if fw = fT (w). We also say that an update rule A : R|E| → R|E| is invariant to transformation T if and only if fA(w) = fA(T (w)). It is desirable for an update rule to have the same invariances as its inputs (Neyshabur et al., 2015a).\nAn example of invariance in feedforward networks is node-wise rescaling (or rebalancing). For any positive scalar α and for any internal node v (v /∈ Vin and v /∈ Vout), the following transformation w̃ = T (w) satisfies fw = fw̃:\nw̃v→j = αwv→j (j ∈ N out(v)), (20) w̃k→v = α −1wk→v (k ∈ N in(v))"
    }, {
      "heading" : "5.1 DDP-SGD ON FEEDFORWARD NETWORKS",
      "text" : "Here we show that DDP-SGD is node-wise rescaling invariant. We already observed that feedforward networks are node-wise rescaling invariant. To see if DDP-SGD is also node-wise rescaling invariant, we calculate the update rule for w̃v→j in equation (20):\nw̃+v→j = αwv→j − α2η\nκv→j(w)\n∂L\nα∂wv→j (wv→j)\n= α ( w − η\nκv→j(w)\n∂L\n∂wv→j (wv→j)\n) = αw+v→j\nTherefore, DDP-SGD is node-wise rescaling invariant."
    }, {
      "heading" : "5.2 SGD ON BATCH NORMALIZED NETWORKS",
      "text" : "The SGD update rule on Batch Normalized networks is not invariant to the network invariances. Batch-Normalized networks are not node-wise rescaling invariant. Instead, they are invariant to rescaling the incoming weights of nodes. To check the invariance of SGD, we consider the following transformation for a scalar α > 0:\nw̃k→v = αwk→v (k ∈ N in(v))\nThe Batch-Normalized network is invariant to the above transformation because the output of each node is normalized. The SGD update rule is however not invariant to this transformation:\nw̃+k→v = αwk→v − η ∂L\nα∂wk→v (wk→v) 6= α ( wk→v − η ∂L\n∂wk→v (wk→v)\n) = αw+k→v"
    }, {
      "heading" : "6 UNDERSTANDING INVARIANCES",
      "text" : "The neural network model fw(x) can alternatively be expressed as the sum over all directed paths from every input node to each output node as follows:\nfw(x)[v] = ∑\np∈Π(v)\ngp(x) · πp(w) · x[head(p)], (21)\nwhere Π(v) is the set of all directed path from any input node to v, head(p) is the first node of path p, gp(x) takes 1 if all the rectified linear units along path p is active and zero otherwise, and\nπp(w) = ∏\ne∈E(p)\nw(e) (22)\nis the product of the weights along path p; E(p) denotes the set of edges that appear along path p."
    }, {
      "heading" : "6.1 CONTINUOUSLY DEFORMABLE TRANSFORMATIONS",
      "text" : "We say that network G is invariant to transformation T : R|E| → R|E|, if fw = fT (w). Another example of transformation T such that fw = fT (w) is switching two units. Suppose two units u, v ∈ V share the same set of children and parents. This is typically the case for internal nodes of fully-connected layered network. Then we can define w̃ = T (w) so that\nw̃u→j = wv→j , w̃v→j = wu→j (j ∈ N out(u)), w̃k→u = wk→v, w̃k→v = wk→u (k ∈ N in(u)).\nThis transformation is smooth (it is actually linear) but not continuously deformable in the sense that the two parameter vectors w and T (w) can be arbitrarily far away in the parameter space. Invariances that are not continuously deformable induce isolated sets of points that correspond to the same input-output mapping in the parameter space. We will leave the characterization of non-deformable invariances as future work as they will not affect the behavior of gradient based optimization algorithms at least locally.\nIn contrast to the node-switching invariance, the node-wise rescaling (20) is continuously deformable (take α = 1). More precisely, a transformation T is continuously deformable if there is a continuous family of transformations Tt (0 ≤ t ≤ 1) such that\nT0(w) = w, T1(w) = T (w).\nThis notion of invariance allows us to talk about the subspace spanned by the infinitesimal changes in the parameters to which the model is invariant at a point w, namely\nN(w) = {δ ∈ R|E| : fw+δ = fw +O(‖δ‖2)}. (23) In fact, by Taylor expansion\nfw+δ = fw + (∇wfw)δ +O(‖δ‖2), so N(w) is the null space of the Jacobian ∇wfw of fw at w, and thus a linear space. Note that although it is possible to find many more direction than the dimension of N(w) at any given input point x that fw+δ(x) = fw(x) + O(‖δ‖2), we are interested in characterizing the set of directions that the model is invariant to regardless of the input x as we define in (23)."
    }, {
      "heading" : "6.2 THE FISHER INFORMATION AND NATURAL GRADIENT",
      "text" : "One approach for addressing the issue of invariance for neural network training is the natural gradient algorithm (Amari, 1998).\nUsing the path-based definition of the network model (21), we have\n∂ log q(c|x) ∂we = ∑ v∈Vout ∂ log q(c|x) ∂yv ∂yv ∂we\n= ∑ v∈Vout ∑ p∈Π(v) ∂πp(w) ∂we · ∂ log q(c|x) ∂yv · gp(x) · x[head(p)].\nThus the Fisher information matrix (33) can be rewritten as F (w)[e, e′] = ∑ p∈Π ∑ p′∈Π ∂πp(w) ∂we ∂πp′(w) ∂we′ C[p, p′]\nwhere Π = ∪v∈VoutΠ(v) is the set of all the paths , and the (uncentered) path covariance matrix C is defined as follows:\nC[p, p′] = Ex∼p(x)Ec∼q(c|x) [ ∂ log q(c|x) ∂ytail(p) ∂ log q(c|x) ∂ytail(p′) · gp(x)gp′(x) · x[head(p)]x[head(p′)] ] .\n(24)\nHOW NATURAL GRADIENT RELATES TO INVARIANCE\nConsider a simple fully-connected two layer network with one output node, two hidden units, and two input units (see Figure 2(a)). There are four paths and the path Jacobian matrix J = (∂πp(w)/∂we) can be written as follows:\nJ =  w5 0 0 0 0 w5 0 0 0 0 w6 0 0 0 0 w6 w1 w2 0 0 0 0 w3 w4 \nWe consider two cases, in the first case, the hidden units have the rectified linear activation function and the path covariance matrix can be written as\nC = Ex∼p(x)Ec∼q(c|x) [( ∂ log q(c|x) ∂yout )2 · ( g21(x) g1(x)g2(x) g1(x)g2(x) g 2 2(x) ) ⊗ ( (x[1])2 x[1]x[2] x[1]x[2] (x[2])2 )] ,\n(25) where ⊗ denotes the Kronecker product. Note that the first two paths and the last two paths each share the same hidden unit. Therefore the path covariance matrix (before expectation) can be written as the Kronecker product of the (uncentered) covariance of the activations of the two hidden units and the (uncentered) covariance of the input x.\nGenerally speaking the rank of the Kronecker product of two matrices is the product of their ranks. Typically the rank of the path covariance matrix (25) is 4 if the input covariance matrix is not degenerate and the connections to the two hidden units are not identical, although we cannot take the expectation of the two matrices independently. Since all the columns of the path Jacobian matrix are linearly independent, the rank of the Fisher information matrix for this network with the rectified linear activation is 4.\nIn the second case, the hidden units have the linear activation function σj(z) = z. Therefore gj(x) = 1 regardless of the input x.Thus the path covariance matrix simplifies to\nC = Ex∼p(x)Ec∼q(c|x) [( ∂ log q(c|x) ∂yout )2 · ( 1 1 1 1 ) ⊗ ( (x[1])2 x[1]x[2] x[1]x[2] (x[2])2 )] , (26)\nNow even if the input covariance matrix has full rank, the rank of the path covariance matrix (26) and the Fisher information matrix cannot be larger than 2. This is expected, because if the hidden units are linear, the network is equivalent to a network with one output unit directly connected to the two input units.\nAs we have seen in the above example, the rank of the Fisher information captures the degrees of freedom of the model (which equals the number of parameters minus the number of invariances) in a parameterization independent but source distribution dependent manner. It is sensitive to both the invariance induced by the network structure (captured in the path Jacobian matrix J) and the correlation of the paths (captured in the path covariance matrix C)."
    }, {
      "heading" : "6.3 THE JACOBIAN",
      "text" : "In order to remove the source distribution dependence of the Fisher information, let us study the structure of the Jacobian∇wfw in more detail. From (21), we have\n∇wfw(x)[v] = ∑\np∈Π(v)\n∇wπp(w) · gp(x) · x[head(p)] (27)\n= Jvφv(x), (28) where Jv = (∇wπp(w))p∈Π(v) is the path Jacobian corresponding to output node v and has the dimension of the number of parameters |E| times the number of paths |Π(v)|, and φv(x) is a |Π(v)| dimensional vector with gp(x)x[i] in the corresponding entry. Note that although path activation gp(x) is a function of w, it is insensitive to an infinitesimal change in the parameter, unless the input to one of the rectified linear activation functions along path p is at exactly zero, which happens with probability zero. Thus we treat gp(x) as constant here.\nIn order to study how many invariances there are, we can look at the rank of Jacobian ∇wf(w), because the subspace of locally invariant directions N(w) is exactly the null space of∇wf(w). We define the rank of the Jacobian ∇wfw as the dimension of the space spanned by vectors of the form ∇wfw(x)[v] for all output node v and all input x. The above definition allows us to sidestep the source distribution dependence of Fisher information matrix. Nevertheless the rank of the Jacobian defined above is equal to the rank of the Fisher information if the support of the source distribution is unbounded.\nNow we reduce the rank of the Jacobian ∇wfw to the rank of the path Jacobian matrix J , which is purely a combinatorial quantity as we see in the next two theorems.\nTheorem 1. The rank of the Jacobian is at most the rank of the path Jacobian matrix J = (Jv)v∈Vout , i.e., the column-wise concatenation of Jv for all v ∈ Vout. The equality holds if the dimension of the space spanned by φ(x) = (φ>1 (x), . . . , φ > |Vout(x))\n>, i.e. the row-wise concatenation of φv(x), equals the total number of paths ∑ v |Π(v)|.\nProof. The first part is true because any vector of the form ∇wfw(x)[v] defined in (27) lies in the span of Jv . The second part is trivially true.\nTheorem 2. The rank of the path Jacobian matrix J is equal to the number of parameters |E| minus the number of internal nodes of the network.\nNote that the dimension of the space spanned by node-wise rescaling (20) equals the number of internal nodes. Therefore, node-wise rescaling is the only type of continuously deformable invariance for ReLU networks that holds in a distribution independent sense.\nLet us consider a simple 3 layer network with 2 nodes in each layer except for the output layer, which has only 1 node (see Figure 2(b)). The number of parameters is 10 (4, 4, and 2 in each layer respectively) and 8 paths. The Jacobian∇wfw(x) can be written as∇wfw(x) = Jφ(x), where:\nJ =  w5w9 w5w9 w6w9 w6w9 w9w1 w9w2 w9w3 w9w4\nw5w1 w5w2 w6w3 w6w4\nw7w10 w7w10\nw8w10 w8w10\nw10w1 w10w2 w10w3 w10w4\nw7w1 w7w2 w8w3 w8w4\n (29)\nand\nφ(x)> = [g1(x)x[1] g2(x)x[2] g3(x)x[1] g4(x)x[2] g5(x)x[1] g6(x)x[2] g7(x)x[1] g8(x)x[2]] .\nThe rank of J in (29) is equal to 6, which is smaller than both the number of parameters and the number of paths.\nCONCLUSION AND FUTURE WORK\nWe proposed a unified framework as a complexity measure or regularizer for neural networks and discussed normalization and optimization with respect to this regularizer. We further showed how this measure interpolates between data-dependent and data-independent regularizers and discussed how Path-SGD and Batch-Normalization are special cases of optimization with respect to this measure. We also looked at the issue of invariances and brought new insights to this area. Empirical evaluation of different settings in the suggested unified framework is the subject of future work."
    }, {
      "heading" : "A NATURAL GRADIENT",
      "text" : "The natural gradient algorithm achieves invariance to local reparameterization by applying the inverse of the Fisher information matrix F (w(t)) at the current parameter w(t) to the negative gradient direction as follows:\nw(t+1) = w(t) + η∆(natural),\nwhere\n∆(natural) = argmin ∆∈R|E| 〈 −∂L ∂w (w(t)),∆ 〉 , s.t. ∆>F (w(t))∆ ≤ δ2 (30)\n= −F−1(w(t))∂L ∂w (w(t)). (31)\nHere F (w) is the Fisher information matrix at pointw and is defined with respect to the probabilistic view of the neural network model (1), which we describe in more detail below.\nSuppose that we are solving a classification problem and the final layer of the network is fed into a softmax layer that determines the probability of candidate classes given the input x. Then the neural network with the softmax layer can be viewed as a conditional probability distribution\nq(c|x) = exp(fw(x)[vc])∑ v∈v∈Vout exp(fw(x)[v]) , (32)\nwhere vc is the output node corresponding to the candidate class c. If we are solving a regression problem a Gaussian distribution is probably more appropriate for q(c|x). Given the conditional probability distribution q(c|x), the Fisher information matrix can be defined as follows:\nF (w)[e, e′] = Ex∼p(x)Ec∼q(c|x) [ ∂ log q(c|x)\n∂we\n∂ log q(c|x) ∂we′\n] , (33)\nwhere p(x) is the marginal distribution of the data.\nSince we have ∂ log q(c|x) ∂wu→v = ∂ log q(c|x) ∂zv · yu = ∑ v′∈Vout ∂ log q(c|x) ∂zv′ · ∂zv ′ ∂zv · yu (34)\nusing the chain rule, each entry of the Fisher information matrix can be computed efficiently by forward and backward propagations on a minibatch."
    }, {
      "heading" : "B PROOF OF THEOREM 2",
      "text" : "Proof. First, J can be written as an Hadamard product between path incidence matrix M and a rank-one matrix as follows:\nJ = M ◦ ( w−1π>(w) ) ,\nwhere M is the path incidence matrix whose i, j entry is one if the ith edge is part of the jth path, w−1 is an entry-wise inverse of the parameter vector w, π(w) = (πp(w)) is a vector containing the product along each path in each entry, and > denotes transpose. Since we can rewrite\nJ = diag(w−1) ·M · diag(π(w)),\nwe see that (generically) the rank of J is equal to the rank of zero-one matrix M .\nNote that the rank of M is equal to the number of linearly independent columns of M , in other words, the number of linearly independent paths. In general, most paths are not independent. For example, in Figure 2(b), we can see that the path w2w7w10 can be produced by combining 3 paths w1w5w9, w1w7w10, and w2w5w9.\nIn order to count the number of independent paths, we use mathematical induction. For simplicity, consider a layered graph with d layers. All the edges from the (d − 1)th layer nodes to the output layer nodes are linearly independent, because they correspond to different parameters. So far we have ndnd−1 independent paths.\nNext, take one node u0 (e.g., the leftmost node) from the layer below. All the paths starting from this node through the layers above are linearly independent. However, other nodes in this layer only contributes linearly to the number of independent paths. This is the case because we can take an edge (u, v), where u is one of the remaining nd−2 − 1 vertices in the (d− 2)th layer and v is one of the nd−1 nodes in the (d− 1)th layer, and we can take any path (say p0) from there to the top layer. Then this is the only independent path that uses the edge (u, v), because any other combination of edge (u, v) and path p from v to the top layer can be produced as follows (see Figure 3):\n(u, v)→ p = (u, v)→ p0 − (u0, v)→ p0 + (u0, v)→ p.\nTherefore after considering all nodes in the d− 2th layer, we have\nndnd−1 + nd−1(nd−2 − 1)\nindependent paths. Doing this calculation inductively, we have\nndnd−1 + nd−1(nd−2 − 1) + · · ·+ n1(n0 − 1)\nindependent paths, where n0 is the number of input units. This number is clearly equal to the number of parameters (ndnd−1 + · · ·+ n1n0) minus the number of internal nodes (nd−1 + · · ·+ n1)."
    } ],
    "references" : [ {
      "title" : "Natural gradient works efficiently in learning",
      "author" : [ "Amari", "Shun-Ichi" ],
      "venue" : "Neural computation,",
      "citeRegEx" : "Amari and Shun.Ichi.,? \\Q1998\\E",
      "shortCiteRegEx" : "Amari and Shun.Ichi.",
      "year" : 1998
    }, {
      "title" : "Understanding the difficulty of training deep feedforward neural networks",
      "author" : [ "Glorot", "Xavier", "Bengio", "Yoshua" ],
      "venue" : "In AISTATS,",
      "citeRegEx" : "Glorot et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Glorot et al\\.",
      "year" : 2010
    }, {
      "title" : "Scaling up natural gradient by sparsely factorizing the inverse Fisher matrix",
      "author" : [ "Grosse", "Roger", "Salakhudinov", "Ruslan" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Grosse et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Grosse et al\\.",
      "year" : 2015
    }, {
      "title" : "Batch normalization: Accelerating deep network training by reducing internal covariate shift",
      "author" : [ "Ioffe", "Sergey", "Szegedy", "Christian" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Ioffe et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Ioffe et al\\.",
      "year" : 2015
    }, {
      "title" : "Exploring strategies for training deep neural networks",
      "author" : [ "Larochelle", "Hugo", "Bengio", "Yoshua", "Louradour", "Jérôme", "Lamblin", "Pascal" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "Larochelle et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Larochelle et al\\.",
      "year" : 2009
    }, {
      "title" : "Springer Verlag, 1998",
      "author" : [ "Le Cun", "Yann", "Bottou", "Léon", "Orr", "Genevieve B.", "Müller", "Klaus-Robert. Efficient backprop. In Neural Networks", "Tricks of the Trade", "Lecture Notes in Computer Science LNCS" ],
      "venue" : "URL http://leon.bottou.org/papers/lecun-98x.",
      "citeRegEx" : "Cun et al\\.,? 1524",
      "shortCiteRegEx" : "Cun et al\\.",
      "year" : 1524
    }, {
      "title" : "Deep learning via hessian-free optimization",
      "author" : [ "Martens", "James" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Martens and James.,? \\Q2010\\E",
      "shortCiteRegEx" : "Martens and James.",
      "year" : 2010
    }, {
      "title" : "Optimizing neural networks with Kronecker-factored approximate curvature",
      "author" : [ "Martens", "James", "Grosse", "Roger" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Martens et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Martens et al\\.",
      "year" : 2015
    }, {
      "title" : "Path-SGD: Path-normalized optimization in deep neural networks",
      "author" : [ "Neyshabur", "Behnam", "Salakhutdinov", "Ruslan", "Srebro", "Nathan" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Neyshabur et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Neyshabur et al\\.",
      "year" : 2015
    }, {
      "title" : "Norm-based capacity control in neural networks",
      "author" : [ "Neyshabur", "Behnam", "Tomioka", "Ryota", "Srebro", "Nathan" ],
      "venue" : "In COLT,",
      "citeRegEx" : "Neyshabur et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Neyshabur et al\\.",
      "year" : 2015
    }, {
      "title" : "In search of the real inductive bias: On the role of implicit regularization in deep learning",
      "author" : [ "Neyshabur", "Behnam", "Tomioka", "Ryota", "Srebro", "Nathan" ],
      "venue" : "International Conference on Learning Representations (ICLR) workshop track,",
      "citeRegEx" : "Neyshabur et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Neyshabur et al\\.",
      "year" : 2015
    }, {
      "title" : "Revisiting natural gradient for deep networks",
      "author" : [ "Pascanu", "Razvan", "Bengio", "Yoshua" ],
      "venue" : "In ICLR,",
      "citeRegEx" : "Pascanu et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Pascanu et al\\.",
      "year" : 2014
    }, {
      "title" : "Topmoumoute online natural gradient algorithm",
      "author" : [ "Roux", "Nicolas L", "Manzagol", "Pierre-Antoine", "Bengio", "Yoshua" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Roux et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Roux et al\\.",
      "year" : 2008
    }, {
      "title" : "On the importance of initialization and momentum in deep learning",
      "author" : [ "Sutskever", "Ilya", "Martens", "James", "Dahl", "George", "Hinton", "Geoffrey" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Sutskever et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Sutskever et al\\.",
      "year" : 2013
    }, {
      "title" : "Krylov subspace descent for deep learning",
      "author" : [ "Vinyals", "Oriol", "Povey", "Daniel" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Vinyals et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Vinyals et al\\.",
      "year" : 2011
    } ],
    "referenceMentions" : [ {
      "referenceID" : 4,
      "context" : "RELATED WORKS There has been an ongoing effort for better understanding of the optimization in deep networks and several heuristics have been suggested to improve the training (Le Cun et al., 1998; Larochelle et al., 2009; Glorot & Bengio, 2010; Sutskever et al., 2013).",
      "startOffset" : 176,
      "endOffset" : 269
    }, {
      "referenceID" : 13,
      "context" : "RELATED WORKS There has been an ongoing effort for better understanding of the optimization in deep networks and several heuristics have been suggested to improve the training (Le Cun et al., 1998; Larochelle et al., 2009; Glorot & Bengio, 2010; Sutskever et al., 2013).",
      "startOffset" : 176,
      "endOffset" : 269
    }, {
      "referenceID" : 12,
      "context" : "Pascanu & Bengio (2014) also discusses the connections between Natural Gradients and some of the other proposed methods for training neural networks: Hessian-Free Optimization (Martens, 2010), Krylov Subspace Descent (Vinyals & Povey, 2011) and TONGA (Roux et al., 2008).",
      "startOffset" : 251,
      "endOffset" : 270
    }, {
      "referenceID" : 4,
      "context" : ", 1998; Larochelle et al., 2009; Glorot & Bengio, 2010; Sutskever et al., 2013). In order to incorporate the curvature information, several methods have proposed different ways to approximate Fisher information matrix in deep learning models (Desjardins et al., 2015; Martens & Grosse, 2015; Grosse & Salakhudinov, 2015). Pascanu & Bengio (2014) also discusses the connections between Natural Gradients and some of the other proposed methods for training neural networks: Hessian-Free Optimization (Martens, 2010), Krylov Subspace Descent (Vinyals & Povey, 2011) and TONGA (Roux et al.",
      "startOffset" : 8,
      "endOffset" : 346
    } ],
    "year" : 2017,
    "abstractText" : "We propose a unified framework for neural net normalization, regularization and optimization, which includes Path-SGD and Batch-Normalization and interpolates between them across two different dimensions. Through this framework we investigate issue of invariance of the optimization, data dependence and the connection with natural gradients.",
    "creator" : "LaTeX with hyperref package"
  }
}