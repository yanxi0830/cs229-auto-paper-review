{
  "name" : "1306.1326.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Performance Analysis of Unsupervised Feature Selection Methods",
    "authors" : [ "A.Nisthana Parveen", "H.Hannah Inbarani", "E.N. Sathish Kumar" ],
    "emails" : [ "nisthana@gmail.com", "hhinba@gmail.com", "en.sathishkumar@yahoo.co.in" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Feature selection (FS) is a process which attempts to select more informative features. In some cases, too many redundant or irrelevant features may overpower main features for classification. Feature selection can remedy this problem and therefore improve the prediction accuracy and reduce the computational overhead of classification algorithms. The main aim of feature selection is to determine a minimal feature subset from a problem domain while retaining a suitably high accuracy in representing the original features. In this paper, Principal Component Analysis (PCA), Rough PCA, Unsupervised Quick Reduct (USQR) algorithm and Empirical Distribution Ranking (EDR) approaches are applied to discover discriminative features that will be the most adequate ones for classification. Efficiency of the approaches is evaluated using standard classification metrics."
    }, {
      "heading" : "Keywords: Feature Selection, Principal Component Analysis, Rough-PCA, Empirical Distribution, Unsupervised Quick Reduct.",
      "text" : "I. INTRODUCTION\nFeature selection, is a problem closely related to dimension reduction. The objective of feature selection is to identify features in the data-set as important, and discard any other feature as irrelevant and redundant information. Since feature selection reduces the dimensionality of the data, it holds out the possibility of more effective & rapid operation of data mining algorithm (i.e. Data Mining algorithms can be operated faster and more effectively by using feature selection).\nConventional supervised FS methods evaluate various feature subsets using an evaluation function or metric to select only those features which are related to the decision classes of the data under consideration. However, for many data mining applications, decision class labels are often unknown or incomplete, thus indicating the significance of\nunsupervised feature selection. In unsupervised learning, decision class labels are not provided.\nPrincipal Components Analysis (PCA) is the predominant linear dimensionality reduction technique, and it has been widely applied on datasets in all scientific domains. In words, PCA seeks to map or embed data points from a high dimensional space to a low dimensional space while keeping all the relevant linear structure intact. To improve the efficiency and accuracy of data mining task on high dimensional data, the data must be preprocessed by an efficient dimensionality reduction method. Principal Component Analysis (PCA) is a popular linear feature extractor used for unsupervised feature selection based on eigenvectors analysis to identify critical original features for principal component. PCA is a statistical technique for determining key variables in a high dimensional data set that explain the differences in the observations and can be used to simplify the analysis and visualization of high dimensional data set, without much loss of information. Rough set theory is employed to generate reducts, which represent the minimal sets of non-redundant features capable of discerning between all objects, in a multiobjective framework. Rough-PCA approach is the combination of PCA and Rough set theory.\nThe rest of the paper is organized as follows: Section 2, briefs about the feature selection algorithm such as PCA, Rough-PCA, Unsupervised Quick Reduct and Empirical Distribution. Section 3 explains briefly about experimental analysis and results. Section 4 presents a conclusion for this paper."
    }, {
      "heading" : "II. FEATURE SELECTION METHODS",
      "text" : ""
    }, {
      "heading" : "A. Principal Component Analysis",
      "text" : "Principal Component Analysis is an unsupervised Feature Reduction method for projecting high dimensional data into a new lower dimensional\nrepresentation of the data that describes as much of the variance in the data as possible with minimum reconstruction error. Principal Component Analysis is a quantitatively rigorous method for achieving this simplification. The method generates a new set of variables, called principal components. Each principal component is a linear combination of the original variables. All the principal components are orthogonal to each other, so there is no redundant information. The principal components as a whole form an orthogonal basis for the space of the data. Thus we propose unsupervised feature selection algorithms based on eigenvectors analysis to identify critical original features for principal component [5].\nPCs are calculated using the Eigen value decomposition of the data covariance matrix/correlation matrix or singular value decomposition of a data matrix. Usually after mean centering the data for each attribute. Covariance matrix is preferred when the variances of variables are very high compared to correlation. It would be better to choose the type correlation when the variables are of different types. Similarly the SVD method is used for numerical accuracy.\nSingular value decomposition (SVD) can be looked at from three mutually compatible points of view. On the one hand, we can see it as a method for transforming correlated variables into a set of uncorrelated ones that better expose the various relationships among the original data items. At the same time, SVD is a method for identifying and ordering the dimensions along which data points exhibit the most variation.\nSVD and PCA are common techniques for analysis of multivariate data, and gene expression data are well suited to analysis using SVD/PCA. We can use SVD to perform PCA. SVD is based on a theorem from linear algebra which says that a rectangular matrix X can be broken down into the product of three matrices – an orthogonal matrix U, a diagonal matrix S, and the transpose of an orthogonal matrix S, and the transpose of an orthogonal matrix V. The theorem is usually presented something like this:\n\uD835\uDC34\uD835\uDC5A\uD835\uDC5A = \uD835\uDC48\uD835\uDC5A\uD835\uDC5A \uD835\uDC46\uD835\uDC5A\uD835\uDC5B \uD835\uDC49\uD835\uDC5B\uD835\uDC5B \uD835\uDC47 (1)\nwhere UTU=1, VTV=1; the columns of U are of U are orthonormal eigenvectors of AAT, the columns of V are orthonormal eigenvectors of ATA, and S is a diagonal matrix containing the square roots of eigen values from U or V in descending order. The resulting algorithm is given below."
    }, {
      "heading" : "Algorithm: PCA Input: Data Matrix Output: Reduced set of features",
      "text" : "Step-1: X  Create N x d data matrix, with one row vector xn per data point. Step-2: X subtract mean x from each row vector xn in X. Step-3: Σ  covariance matrix of X. Step-4: Find eigenvectors and eigen values of Σ. Step-5: PC’s  the M eigenvectors with largest eigen values. Step-6: Output PCs.\nAlgorithm1: Principal Component Analysis"
    }, {
      "heading" : "B. Rough-PCA",
      "text" : "1. Rough Set Theory\nRough set theory (RST) has been used as a tool to discover data dependencies and to reduce the number of attributes contained in a dataset using the data alone, requiring no additional information [3] [4]. Over the past ten years, RST has become a topic of great interest to researchers and has been applied to many domains. Given a dataset with discretized attribute values, it is possible to find a subset (termed a reduct) of the original attributes using RST that are the most informative; all other attributes can be removed from the dataset with minimal information loss. An information table is defined as a tuple T = (U, A) where U and A are two finite, non-empty sets, U the universe of primitive objects and A the set of attributes. Each attribute or feature a∈ A is associated with a set Va of its value, called the domain of a. We may partition the attribute set A into two subsets C and D, called condition and decision attributes, respectively[8].\nLet P ⊂ A be a subset of attributes. The indiscernibility relation, denoted by IND (P), is an equivalence relation defined as:\nIND (P) = {(x, y) ∈ U×U: ∀ a∈ P, a(x) = a(y) (2)\nwhere a(x) denotes the value of feature a of object x. If (x, y) ∈ IND (P), x and y are said to be indiscernible with respect to P.\nThe family of all equivalence classes of IND (P) (Partition of U determined by P) is denoted by U/IND (P). Each element in U/IND (P) is a set of indiscernible objects with respect to P. Equivalence classes U/IND(C) and U/IND (D) are called\ncondition and decision classes, and it can be calculated as follows: U/IND (P) = ⊗ {a∈P: U/IND ({a})} (3)\nWhere A⊗B={X ∩ Y: ∀ X∈A, ∀ Y∈B, X ∩ Y≠ Ø} (4) If (x, y) ∈ IND (P), then x and y are indiscernible by attributes from P. The equivalence classes of the Pindiscernibility relation are denoted [x] P.\nA rough set is defined by the lower and upper approximations of a concept. The lower approximation contains all elements that necessarily belong to the concept, while the upper approximation contains those that possibly belong to the concept. In rough set theory, a concept is considered a classical set.\nLet X ⊆ U. X can be approximated using only the information contained within P by constructing the Plower and P-upper approximations of X:\n\uD835\uDC43X= {x | [x]p ⊆ X} (5)\n\uD835\uDC43X= {x| [x]p ∩ X≠ Ø} (6)\nWhere [x]p denotes the equivalence class of object x ∈ U relative to Ip, are called the P-lower and P-upper approximation of X and denoted by \uD835\uDC43X, \uD835\uDC43X respectively.\nLet P and Q be equivalence relations over U, then the positive, negative and boundary regions can be defined as:\nPOSp (Q) = UX∈U/Q \uD835\uDC43X (7)\nThe positive region contains all objects of U that can be classified to classes of U/Q using the information in attributes P.\nRough set reducts can be found by using degree of dependency or by using discernibility matrix.\nk= \uD835\uDEFEP (Q) = POSp Q |U |\n(8)\nWhere\nPOSp Q = UX∈U/Q \uD835\uDC43X (9)\nThe reduction of attributes is achieved by comparing equivalence relations generated by sets of attributes. Attributes are removed so that the reduced set provides the same predictive capability of the decision feature as the original. A reduct is defined as\na subset of minimal cardinality Rmin of the conditional attribute set C such that \uD835\uDEFER (D) = \uD835\uDEFEC (D).\nR= {X: X ⊆C, γX (D) =γC (D)} (10)\nRmin = {X: X € R, Y € R, |X| |Y|} (11)\n2. Rough-PCA Algorithm\nPrincipal component analysis is an unsupervised linear feature reduction method for projecting highdimensional data into a low-dimensional space with minimum loss of information. It discovers the directions of maximal variances in the data. The Rough set approach to feature selection is used to discover the data dependencies and reduction in the number of attributes contained in a dataset using the data alone, requiring no additional information. For selecting discriminative features from principal components, the Rough set theory can be applied jointly with PCA, which guarantees that the selected principal components will be the most adequate for classification. We call this method Rough-PCA. The method is successfully applied for choosing the principal features and then applying the upper and lower approximations to find the reduced set of features. The resulting algorithm is given below [5]."
    }, {
      "heading" : "Algorithm: Rough PCA Input: Data Matrix Output: Reduced set of features",
      "text" : "Step-1: Normalize the original data set. Step-2: Calculate the Principal Components using Singular Value Decomposition of the Normalized data matrix. Step-3: Determine the number of meaningful PCs to retain. Step-4: Find the reduced data set using the reduced PCs. Step-5: Discretize the data set. Step-6: Find the reduct using Rough set theory (RST). Algorithm 2: Rough-PCA"
    }, {
      "heading" : "C. Empirical Distribution Ranking",
      "text" : "Let (x1, x2 … xn) be iid or independent identically\ndistributed real random variables with common cdf F (t). Then the empirical distribution function is defined as\n\uD835\uDC39\uD835\uDC5B(t) = 1\n\uD835\uDC5B \uD835\uDC3C \uD835\uDC4B\uD835\uDC56 ≤ \uD835\uDC61\n\uD835\uDC5B \uD835\uDC56=1 (12)\nWhere t is the mean of Xi , \uD835\uDC3C\uD835\uDC34 is the so-called indicator random variable which is defined to be\nequal to 1 when the property A holds, and equal to 0\notherwise. Thus, while the distribution function gives\nas a function of t the probability with which each of\nthe random variables Xi will be ≤ t, the empirical distribution function calculated from data gives the\nrelative frequency with which the observed values are\n≤ t. Sorting the values of \uD835\uDC39\uD835\uDC5B(t), then choosing the minimum value attributes for ranking [3]."
    }, {
      "heading" : "Algorithm: EDR Input: Data Matrix Output: Reduced set of features",
      "text" : "Step-1: Sort the original data set.\n\uD835\uDC65\uD835\uDC561 ′ < \uD835\uDC65\uD835\uDC562 ′ < ⋯\uD835\uDC65\uD835\uDC56\uD835\uDC5A ′\nStep-2: Calculate the mean value of sorted data Step-3: Find ED using \uD835\uDC39\uD835\uDC5B(t).\n\uD835\uDC39\uD835\uDC5B(t) = 1\n\uD835\uDC5B \uD835\uDC3C \uD835\uDC4B\uD835\uDC56 ≤ \uD835\uDC61\n\uD835\uDC5B \uD835\uDC56=1\nStep-4: Rank the features based on ED."
    }, {
      "heading" : "D. Unsupervised Quick Reduct (USQR) Algorithm",
      "text" : "The USQR algorithm attempts to calculate a reduct\nwithout exhaustively generating all possible subsets.\nIt starts off with an empty set and adds in turn, one at\na time, those attributes that result in the greatest\nincrease in the rough set dependency metric, until this\nproduces its maximum possible value for the dataset\n[2]. According to the algorithm, the mean\ndependency of each attribute subset is calculated and\nthe best candidate is chosen:\n\uD835\uDEFE\uD835\uDC43 (\uD835\uDC4E) = \uD835\uDC43\uD835\uDC42\uD835\uDC46\uD835\uDC43 (\uD835\uDC4E)\n\uD835\uDC48 ,∀\uD835\uDC4E ∈ \uD835\uDC34.\nAlgorithm: USQR (C) C, the set of all conditional features; (1) R ← {} (2) do (3) T ← R (4) ∀\uD835\uDC65 ∈ (C – R) (5) ∀\uD835\uDC66 ∈ C\n(6) \uD835\uDEFE\uD835\uDC45 ∪ {\uD835\uDC65}(\uD835\uDC66) = \uD835\uDC43\uD835\uDC42\uD835\uDC46 \uD835\uDC45 ∪ {\uD835\uDC65}(\uD835\uDC66)\n\uD835\uDC48\n(7) if \uD835\uDEFE\uD835\uDC45 ∪ {\uD835\uDC65}(\uD835\uDC66) , ∀\uD835\uDC66 ∈ C > \uD835\uDEFE\uD835\uDC47(\uD835\uDC66) , ∀\uD835\uDC66 ∈ C (8) T ← R ∪ \uD835\uDC65 (9) R ← T (10) until \uD835\uDEFE\uD835\uDC45(\uD835\uDC66 ) , ∀\uD835\uDC66 ∈ C = \uD835\uDEFE\uD835\uDC36(\uD835\uDC66) , ∀\uD835\uDC66 ∈ C (11) return R\nAlgorithm 4: Unsupervised QuickReduct"
    }, {
      "heading" : "III. EXPERIMENTAL RESULTS",
      "text" : "This section presents the results of experimental studies using both crisp-valued and real-valued data sets. Initially we evaluated the algorithm on a datasets, which are available in the UCI machine learning repository. In our experiment, PCA, RoughPCA, Unsupervised Quick Reduct and Empirical distribution were implemented using Matlab. A short experimental evaluation for benchmark datasets is presented. The information of the data sets contains names of dataset, number of objects, number of classes and number of attributes, which are given in Table 1.\nThe features are reduced by the PCA, Rough-PCA, Unsupervised Quick Reduct and Empirical distribution algorithms. The selected features are tabulated in table 2."
    }, {
      "heading" : "A. Weka Classification",
      "text" : "The Waikato Environment for Knowledge Analysis (Weka) is a comprehensive suite of Java class libraries that implement many state-of-the-art machine learning and data mining algorithms. Weka is freely available on the World-Wide Web and accompanies a new text on data mining [2] which documents and fully explains all the algorithms it contains. Applications written using the Weka class libraries can be run on any computer with a Web browsing capability; this allows users to apply machine learning techniques to their own data regardless of computer platform. Tools are provided for pre-processing data, feeding it into a variety of learning schemes, and analyzing the resulting classifiers and their performance [4].\nAn important resource for navigating through Weka is its on-line documentation, which is automatically generated from the source. The primary learning methods in Weka are ―classifiers‖, and they induce a rule set or decision tree that models the data. Weka also includes algorithms for learning association rules and clustering data.\nThe core package contains classes that are accessed from almost every other class in Weka. The most important classes in it are Attribute, Instance, and Instances. An object of class Attribute represents an attribute—it contains the attribute’s name, its type, and, in case of a nominal attribute, it’s possible values. An object of class Instance contains the attribute values of a particular instance; and an object of class Instances contains an ordered set of instances—in other words, a dataset.\nIn this paper we have taken the classifiers such as JRip, J48, RBFN, Decision Table, K-Star and Naive Bayes. The determined datasets that are taken from feature selection methods such as Rough PCA, PCA, USQR and Empirical distribution are classified using the above referred classifiers. Table 3, 4, 5, 6 shows the correctly classified instances of mentioned feature selection methods respectively.\nFigure 1, depicts the performance of the discussed feature selection algorithms after classification for diabetes dataset. On the average EDR method exhibits highest classification accuracy and is the best unsupervised feature selection method for diabetes data set.\nDiabetes\nFigure 2, depicts the performance of the discussed feature selection algorithms after classification for breast cancer dataset. On the average PCA and Rough-PCA method exhibits highest classification accuracy and is the best unsupervised feature selection method for breast cancer data set.\nFigure 2: Classification Accuracy for Breast Cancer\nTABLE 5: CLASSIFICATION ACCURACY FOR LUNG CANCER\nClassifiers\nPCA Rough-PCA EDR USQR\nJRip\n87.5\n65.625\n81.25\n62.5\nJ48\n87.8\n59.375\n81.25\n56.25\nRBFN\n90.625\n40.625\n65.625 46.875\nNaive Bayes 84.375\n56.25\n78.125 62.5\nDecision Table 81.25\n59.375\n81.25 53.125\nK-Star\n90.625\n50\n71.875 56.25\nFigure 3, depicts the performance of the discussed feature selection algorithms after classification for lung cancer dataset. On the average PCA method exhibits highest classification accuracy and is the best unsupervised feature selection method for lung cancer data set.\n70\n75\n80\n85\n90\n95\nJR ip\nJ4 8\nR B\nF N\nN ai\nv e\nB ay\nes\nD ec\nis io\nn T\nab le\nK -S\nta r\nA c c u\nr a c y\nClassifiers\nBreast Cancer\nPCA\nRough-PCA\nED Ranking\nUSQR 0\n10 20 30 40 50 60 70 80 90\n100\nJR ip\nJ4 8\nR B\nF N\nN ai\nv e\nB ay\nes\nD ec\nis io\nn T\na b le\nA c c u\nr a c y\nClassifiers\nLung Cancer\nFigure 4, depicts the performance of the discussed feature selection algorithms after classification for Ecoli dataset. On the average EDR method exhibits highest classification accuracy and is the best unsupervised feature selection method for Ecoli data set.\nTABLE 7: CLASSIFICATION ACCURACY FOR HEART\nClassifiers\nPCA\nRough-\nPCA\nEDR\nUSQR\nJRip\n73.3333\n64.0741\n70.0000\n67.7778\nJ48\n75.1852\n60.7407\n66.6667\n67.7778\nRBFN\n71.8519\n65.5556\n73.3333\n69.2593\nNaive Bayes\n73.3333\n67.037\n71.4815\n66.6667\nDecision\nTable\n71.4815\n67.7778\n70.3704\n70.7407\nK-Star\n71.4815\n63.3333\n70.3704\n64.0741\nFigure 5, depicts the performance of the discussed feature selection algorithms after classification for Heart dataset. On the average PCA method exhibits highest classification accuracy and is the best unsupervised feature selection method for heart data set.\nEcoli\nHeart"
    }, {
      "heading" : "IV. CONCLUSION",
      "text" : "In this paper, PCA, EDR, Unsupervised Quick Reduct and Rough-PCA based on rough set theory has been implemented on some synthetic and biological datasets from data repository. The WEKA tool is used to compute classification accuracy of the selected subset of features. EDR outperforms other methods for several data sets than other methods and has proven to be the best method for unsupervised feature selection."
    } ],
    "references" : [ {
      "title" : "Singular value decomposition for genomewide expression data processing and modeling",
      "author" : [ "O. Alter", "P.O. Brown", "D Bostein" ],
      "venue" : "Proc. Natl. Acad. Sci. USA,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2000
    }, {
      "title" : "Unsupervised Quick Reduct Algorithm Using Rough Set Theory",
      "author" : [ "C. Velayutham", "K. Thangavel" ],
      "venue" : "Journal Of Electronic Science And Technology,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2011
    }, {
      "title" : "Principal component analysis‖,New",
      "author" : [ "I.T. Jolliffe" ],
      "venue" : "York: Springer-Verlag,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2002
    }, {
      "title" : "A Hybridized Rough-PCA Approach of Attribute Reduction for High Dimensional Data Set",
      "author" : [ "Rajashree Dash", "Rasmita Dash", "Debahuti Mishra" ],
      "venue" : "European Journal of Scientific Research, ISSN 1450-216X Vol.44(1),",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2010
    }, {
      "title" : "Rough Sets: Theoretical Aspects of Reasoning about Data",
      "author" : [ "Z. Pawlak" ],
      "venue" : "Dordrecht: Kluwer Academic Publishers,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 1991
    } ],
    "referenceMentions" : [ {
      "referenceID" : 2,
      "context" : "Thus we propose unsupervised feature selection algorithms based on eigenvectors analysis to identify critical original features for principal component [5].",
      "startOffset" : 152,
      "endOffset" : 155
    }, {
      "referenceID" : 4,
      "context" : "We may partition the attribute set A into two subsets C and D, called condition and decision attributes, respectively[8].",
      "startOffset" : 117,
      "endOffset" : 120
    }, {
      "referenceID" : 2,
      "context" : "The resulting algorithm is given below [5].",
      "startOffset" : 39,
      "endOffset" : 42
    }, {
      "referenceID" : 1,
      "context" : "It starts off with an empty set and adds in turn, one at a time, those attributes that result in the greatest increase in the rough set dependency metric, until this produces its maximum possible value for the dataset [2].",
      "startOffset" : 218,
      "endOffset" : 221
    }, {
      "referenceID" : 1,
      "context" : "Weka is freely available on the World-Wide Web and accompanies a new text on data mining [2] which documents and fully explains all the algorithms it contains.",
      "startOffset" : 89,
      "endOffset" : 92
    } ],
    "year" : 2013,
    "abstractText" : "Feature selection (FS) is a process which attempts to select more informative features. In some cases, too many redundant or irrelevant features may overpower main features for classification. Feature selection can remedy this problem and therefore improve the prediction accuracy and reduce the computational overhead of classification algorithms. The main aim of feature selection is to determine a minimal feature subset from a problem domain while retaining a suitably high accuracy in representing the original features. In this paper, Principal Component Analysis (PCA), Rough PCA, Unsupervised Quick Reduct (USQR) algorithm and Empirical Distribution Ranking (EDR) approaches are applied to discover discriminative features that will be the most adequate ones for classification. Efficiency of the approaches is evaluated using standard classification metrics.",
    "creator" : "Microsoft® Office Word 2007"
  }
}