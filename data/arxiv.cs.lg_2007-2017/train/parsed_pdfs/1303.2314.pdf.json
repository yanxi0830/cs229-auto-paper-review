{
  "name" : "1303.2314.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Mini-Batch Primal and Dual Methods for SVMs",
    "authors" : [ ],
    "emails" : [ "martin.taki@gmail.com", "abijral@ttic.edu", "peter.richtarik@ed.ac.uk", "nati@ttic.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n30 3.\n23 14\nv1 [\ncs .L\nG ]\n1 0"
    }, {
      "heading" : "1. Introduction",
      "text" : "Stochastic optimization approaches have been shown to have significant theoretical and empirical advantages in training linear Support Vector Machines (SVMs), as well as in many other learning applications, and are often the methods of choice in practice. Such methods use a single, randomly chosen, training example at each iteration. In the context of SVMs, approaches of this form include primal stochastic gradient descent (SGD) methods (e.g., Pegasos, Shalev-Shwartz et al. 2011, NORMA, Zhang 2004) and dual stochastic coordinate ascent (Hsieh et al., 2008).\nHowever, the inherent sequential nature of such approaches becomes a problematic limitation for parallel and distributed computations as the predictor must be updated after each training point is processed, providing very little opportunity for parallelization. A popular remedy is to use mini-batches. That is, to use several training points at each iteration, instead of just\none, calculating the update based on each point separately and aggregating the updates. The question is then whether basing each iteration on several points can indeed reduce the number of required iterations, and thus yield parallelization speedups.\nIn this paper, we consider using mini-batches with Pegasos (SGD on the primal objective) and with Stochastic Dual Coordinate Ascent (SDCA). We show that for both methods, the quantity that controls the speedup obtained using mini-batching/parallelization is the spectral norm of the data.\nIn Section 3 we provide the first analysis of minibatched Pegasos (with the original, non-smooth, SVM objective) that provably leads to parallelization speedups (Theorem 1). The idea of using mini-batches with Pegasos is not new, and is discussed already by Shalev-Shwartz et al. (2011), albeit without a theoretical justification. The original Pegasos theoretical analysis does not benefit from using mini-batches—the same number of iterations is required even when large mini-batches are used, there is no speedup, and the serial runtime (overall number of operations, in this case data accesses) increases linearly with the minibatch size. In fact, no parallelization speedup can be guaranteed based only on a bound on the radius of the data, as in the original Pegasos analysis. Instead, we provide a refined analysis based on the spectral norm of the data.\nWe then move on to SDCA (Section 4). We show the situation is more involved, and a modification to the method is necessary. SDCA has been consistently shown to outperform Pegasos in practice (Hsieh et al., 2008; Shalev-Shwartz et al., 2011), and is also popular as it does not rely on setting a step-size as in Pegasos.\nIt is thus interesting and useful to obtain mini-batch variants of SDCA as well. We first show that a naive mini-batching approach for SDCA can fail, in particular when the mini-batch size is large relative to the spectral norm (Section 4.1). We then present a “safe” variant of mini-batched SDCA, which depends on the spectral norm, and an analysis for this safe variant that establishes the same spectral-norm-dependent parallelization speedups as for Pegasos (Section 4.2). Similar to a recent analysis of non-mini-batched SDCA by Shalev-Shwartz & Zhang (2012), we establish a guarantee on the duality gap, and thus also on the suboptimality of the primal SVM objective, when using mini-batched SDCA (Theorem 2). We then go on to describe a more aggressive, adaptive, method for mini-batched SDCA, which is based on the analysis of the “safe” approach, and which we show often outperforms it in practice (Section 4.3, with experiments in Section 5).\nFor simplicity of presentation we focus on the hinge loss, as in the SVM objective. However, all our results for both Pegasos and SDCA are valid for any Lipschitz continuous loss function.\nRelated Work. Several recent papers consider the use of mini-batches in stochastic gradient descent, as well as stochastic dual averaging and stochastic mirror descent, when minimizing a smooth loss function (Dekel et al., 2012; Agarwal & Duchi, 2011; Cotter et al., 2011). These papers establish parallelization speedups for smooth loss minimization with mini-batches, possibly with the aid of some “acceleration” techniques, and without relying on, or considering, the spectral norm of the data. However, these results do not apply to SVM training, where the objective to be minimized is the non-smooth hinge loss. In fact, the only data assumption in these papers is an assumption on the radius of the data, which is not enough for obtaining parallelization guarantees when the loss is non-smooth. Our contribution is thus orthogonal to these papers, showing that it is possible to obtain parallelization speedups even for non-smooth objectives, but only with a dependence on the spectral norm. We also analyze SDCA, which is a substantially different method from the methods analyzed in these papers. It is interesting to note that a bound of the spectral norm could perhaps indicate that it is easier to “smooth” the objective, and thus allow obtaining results similar to ours (i.e. on the suboptimality of the original non-smooth objective) by smoothing the objective and relying on mini-batched smooth SGD, where the spectral norm might control how well the smoothed loss captures the original loss. But we are\nnot aware of any analysis of this nature, nor whether such an analysis is possible.\nThere has been some recent work on mini-batched coordinate descent methods for ℓ1-regularized problems (and, more generally, regularizes by a separable convex function), similar to the SVM dual. Bradley et al. (2011) presented and analyzed SHOTGUN, a parallel coordinate descent method for ℓ1-regularized problems, showing linear speedups for mini-batch sizes bounded in terms of the spectral norm of the data. The analysis does not directly apply to the SVM dual because of the box constraints, but is similar in spirit. Furthermore, Bradley et al. (2011) do not discuss a “safe” variant which is applicable for any mini-batch size, and only study the analogue of what we refer to as “naive” mini-batching (Section 4.1). More directly related is recent work of Richtárik & Takáč (2013; 2012) which provided a theoretical framework and analysis for a more general setting than SHOTGUN, that includes also the SVM dual as a special case. However, guarantees in this framework, as well as those of Bradley et al. (2011), are only on the dual suboptimality (in our terminology), and not on the more relevant primal suboptimality, i.e., the suboptimality of the original SVM problem we are interested in. Our theoretical analysis builds on that of Richtárik & Takáč (2012), combined with recent ideas of Shalev-Shwartz & Zhang (2012) for “standard” (serial) SDCA, to obtain bounds on the duality gap and primal suboptimality."
    }, {
      "heading" : "2. Support Vector Machines",
      "text" : "We consider the optimization problem of training a linear1 Support Vector Machine (SVM) based on n labeled training examples {(xi, yi)} n i=1, where xi ∈ R d and yi ∈ ±1. We use X = [x1, . . . ,xn] ∈ R d×n to denote the matrix of training examples. We assume the data is normalized such that maxi ‖xi‖ ≤ 1, and thus suppress the dependence on maxi ‖xi‖ in all results. Training a SVM corresponds to finding a linear predictor w ∈ Rd with low ℓ2-norm ‖w‖ and small (empirical) average hinge loss L̂(w) := 1n ∑n i=1 ℓ(yi 〈w,xi〉), where ℓ(z) := [1 − z]+ = max{0, 1 − z}. This biobjective problem can be serialized as\nmin w∈Rd\n[\nP(w) := 1n\nn ∑\ni=1\nℓ(yi 〈w,xi〉) + λ 2 ‖w‖\n2\n]\n, (1)\n1Since both Pegasos and SDCA can be kernelized, all methods discussed are implementable also with kernels, and all our results hold. However, the main advantage of SGD and SDCA is where the feature map is given explicitly, and so we focus our presentation on this setting.\nwhere λ > 0 is a regularization trade-off parameter. It is also useful to consider the dual of (1):\nmax α∈Rn,0≤αi≤1\n[\nD(α) := −12λn2α ⊤Qα+ 1n\nn ∑\ni=1\nαi\n]\n, (2)\nwhere\nQ ∈ Rn×n, Qi,j = yiyj 〈xi,xj〉 , (3)\nis the Gram matrix of the (labeled) data. The (primal) optimum of (1) is given by w∗ = 1λn ∑n i=1 α ∗ i yixi, where α∗ is the (dual) optimum of (2). It is thus natural to associate with each dual solution α a primal solution (i.e., a linear predictor)\nw(α) := 1λn\nn ∑\ni=1\nαiyixi. (4)\nWe will be discussing “mini-batches” of size b, represented by random subsets A ⊆ 〈n〉 := {1, 2, . . . , n} of examples, drawn uniformly at random from all subsets of 〈n〉 of cardinality b. Whenever we draw such a subset, we will for simplicity write A ∈ Rand(b). For A ∈ Rand(b) we use QA ∈ R\nb×b to denote the random submatrix of Q corresponding to rows and columns indexed by A, vA ∈ R\nb to denote a similar restriction of a vector v ∈ Rn, and v[A] ∈ R\nn for the “censored” vector where entries inside A are as in v and entries outside A are zero. The average hinge loss on examples in A is denoted by\nL̂A(w) := 1 b\n∑\ni∈A\nℓ(yi 〈w,xi〉). (5)"
    }, {
      "heading" : "3. Mini-Batches in Primal Stochastic Gradient Descent Methods",
      "text" : "Algorithm 1 Pegasos with Mini-Batches\nInput: {(xi, yi)} n i=1, λ > 0, b ∈ 〈n〉, T ≥ 1 Initialize: set w(1) = 0 ∈ Rd for t = 1 to T do Choose random mini-batch At ∈ Rand(b) ηt = 1 λt , A + t = {i ∈ At : yi〈w (t),xi〉 < 1}\nw(t+1) = (1− ηtλ)w (t) + ηtb ∑ i∈A+t yixi\nend for Output: w̄(T ) = 2T ∑T t=⌊T/2⌋+1 w (t)\nPegasos is an SGD approach to solving (1), where at each iteration the iterate w(t) is updated based on an unbiased estimator of a sub-gradient of the objective P(w). Whereas in a “pure” stochastic setting, the subgradient is estimated based on only a single training\nexample, in our mini-batched variation (Algorithm 1) at each iteration we consider the partial objective:\nPt(w) := L̂At(w) + λ 2 ‖w‖ 2 , (6)\nwhere At ∈ Rand(b). We then calculate the subgradient of the partial objective Pt at w (t):\n∇ (t) := ∇Pt(w (t)) (6) = ∇L̂At(w (t)) + λw(t), (7)\nwhere\n∇L̂A(w) (5) = − 1b ∑\ni∈A\nχi(w)yixi (8)\nand χi(w) := 1 if yi 〈w,xi〉 < 1 and 0 otherwise (indicator for not classifying example i correctly with a margin). The next iterate is obtained by setting w(t+1) = w(t) − ηt∇ (t). We can now write\nw(t+1) (7)+(8) = (1−ηtλ)w (t)+ ηtb ∑\ni∈At\nχi(w (t))yixi. (9)\nAnalysis of mini-batched Pegasos rests on bounding the norm of the subgradient estimates ∇(t). An unconditional bound on this norm, used in the standard Pegasos analysis, follows from bounding\n‖∇L̂A(w)‖ (8) ≤ 1b ∑\ni∈A\n‖χi(w)yixi‖ ≤ 1 b\n∑\ni∈A\n1 = 1.\nFrom (7) we then get ‖∇(t)‖ ≤ λ‖w(t)‖+ 1; the standard Pegasos analysis follows. This bound relies only on the assumption maxi ‖xi‖ ≤ 1, and is the tightest bound without further assumptions on the data.\nThe core novel observation here is that the expected (square) norm of ∇L̂A can be bounded in terms of (an upper bound on) the spectral norm of the data:\nσ2 ≥ 1n ‖X‖ 2 = 1n‖\n∑\ni\nxix ⊤ i ‖ (3) = 1n ‖Q‖ , (10)\nwhere ‖·‖ denotes the spectral norm (largest singular value) of a matrix. In order to bound ∇L̂A, we first perform the following calculation, introducing the key quantity βb, useful also in the analysis of SDCA. Lemma 1. For any v ∈ Rn, Q̃ ∈ Rn×n, A ∈ Rand(b),\nE[v⊤[A]Q̃v[A]] = b n [(1− b−1 n−1 )\nn ∑\ni=1\nQ̃iiv 2 i + b−1 n−1v ⊤Q̃v].\nMoreover, if Q̃ii ≤ 1 for all i and 1 n‖Q̃‖ ≤ σ 2, then\nE[v⊤[A]Q̃v[A]] ≤ b nβb ‖v‖ 2 , where\nβb := 1 + (b−1)(nσ2−1)\nn−1 . (11)\nProof.\nE[v⊤[A]Q̃v[A]] = E[ ∑\ni∈A\nv2i Q̃ii + ∑\ni,j∈A,i6=j\nvivjQ̃ij ]\n(∗) = bEi[v 2 i Q̃ii] + b(b− 1)Ei,j [vivjQ̃ij ]\n= bn\n∑\ni\nQ̃iiv 2 i + b(b−1) n(n−1)v ⊤(Q̃− diag(Q̃))v\n= bn [(1− b−1 n−1 )\n∑\ni\nQ̃iiv 2 i + b−1 n−1v ⊤Q̃v],\nwhere in (∗) the expectations are over i, j chosen uniformly at random without replacement. Now using Q̃ii ≤ 1 and ‖Q̃‖ ≤ nσ\n2, we can upper-bound the expectation as follows:\n≤ bn [(1− b−1 n−1 ) ‖v‖ 2 + b−1n−1nσ 2 ‖v‖2] = bnβb ‖v‖ 2 .\nWe can now apply Lemma 1 to ∇L̂A: Lemma 2. For any w ∈ Rd and A ∈ Rand(b) we have E[‖∇L̂A(w)‖ 2] ≤ βbb , where βb is as in Lemma 1.\nProof. If χ ∈ Rn is the vector with entries χi(w), then\nE[‖∇L̂A(w)‖ 2] (8) = E[‖ 1b ∑\ni∈A\nχiyixi‖ 2]\n(3) = 1b2E[χ ⊤ [A]Qχ[A]]\n(Lem1)\n≤ 1b2 b nβb ‖χ‖ 2 ≤ βbb .\nUsing the by-now standard analysis of SGD for strongly convex functions, we obtain the main result of this section:\nTheorem 1. After T iterations of Pegasos with minibatches (Algorithm 1), we have that for the averaged iterate w̄(T ) = 2T ∑T t=⌊T/2⌋+1 w (t):\nE\n[ P(w̄(T )) ]\n− inf w∈Rd\nP(w) ≤ βbb · 30 λT .\nProof. Unrolling (9) with ηt = 1/(λt) yields\nw(t) = − 1λ(t−1)\nt−1 ∑\nτ=1\ng(τ), (12)\nwhere g(τ) := ∇L̂Aτ (w (τ)). Using the inequality ‖ ∑t−1\nτ=1 g (τ)‖2 ≤ (t− 1) ∑t−1 τ=1 ‖g (τ)‖2, we now get\nE[‖w(t)‖2] (12) ≤\nt−1 ∑\nτ=1\nE[‖g(τ)‖2] λ2(t−1)\n(Lem2)\n≤ βbλ2b , (13)\nE[‖∇(t)‖2] (7)+(Lem2) ≤ 2(λ2E[‖w(t)‖2] + βbb ) (13) ≤ 4βbb .\nThe performance guarantee is now given by the analysis of SGD with tail averaging (Theorem 5 of Rakhlin et al. 2012, with α = 12 and G 2 = 4βbb ).\nParallelization speedup. When b = 1 we have βb = 1 (see (11)) and Theorem 1 agrees with the standard (serial) Pegasos analysis2 (Shalev-Shwartz et al., 2011). For larger mini-batches, the guarantee depends on the quantity βb, which in turn depends on the spectral norm σ2. Since 1n ≤ σ 2 ≤ 1, we have 1 ≤ βb ≤ b.\nThe worst-case situation is at a degenerate extreme, when all data points lie on a single line, and so σ2 = 1 and βb = b. In this case Lemma 2 degenerates to the worst-case bound of E[‖∇L̂A(w)‖\n2] ≤ 1, and in Theorem 1 we have βbb = 1, indicating that using larger mini-batches does not help at all, and the same number of iteration (i.e., the same parallel runtime, and b times as much serial runtime) is required.\nHowever, when σ2 < 1, and so βb < 1, we see a benefit in using mini-batches in Theorem 1, corresponding to a parallelization speedup of bβb . The best situation is when σ2 = 1n , and so βb = 1, which happens when all training points are orthogonal. In this case there is never any interaction between points in the minibatch, and using a mini-batch of size b is just as effective as making b single-example steps. When βb = 1 we indeed see that the speedup speedup is equal to the number of mini-batches, and that the behavior in terms of the number of data accesses (equivalently, serial runtime) bT , does not depend on b; that is, even with larger mini-batches, we require no more data accesses, and we gain linearly from being able to perform the accesses in parallel. The case σ2 = 1n is rather extreme, but even for intermediate values 1n < σ\n2 < 1 we get speedup. In particular, as long as b ≤ 1σ2 , we have βb ≤ 2, and an essentially linear speedup. Roughly speaking, 1σ2 captures the number of examples in the mini-batch beyond which we start getting significant interactions between points."
    }, {
      "heading" : "4. Mini-Batches in Dual Stochastic Coordinate Ascent Methods",
      "text" : "An alternative stochastic method to Pegasos is Stochastic Dual Coordinate Ascent (SDCA, Hsieh et al. 2008), aimed to solve the dual problem (2). At each iteration we choose a single training example (xi, yi), uniformly at random, corresponding to a single dual variable (coordinate) αi = e ⊤ i α. Subsequently, αi is updated so as to maximize the (dual) objective, keeping all other coordinates of α unchanged and maintaining the box constraints. At\n2Except that we avoid the logarithmic factor by relying on tail averaging and a more modern SGD analysis.\niteration t, the update δ (t) i to α (t) i is computed via\nδ (t) i := argmax\n0≤α (t) i +δ≤1\nD(α(t) + δei)\n(2) = argmax\n0≤α (t) i +δ≤1\n(λn− (Qei) ⊤α(t))δ − Qi,i 2 δ 2\n= clip [−α\n(t) i ,1−α (t) i ]\nλn−(Qei) ⊤ α\n(t)\nQi,i\n(3),(4) = clip\n[−α (t) i ,1−α (t) i ]\nλn(1−yi〈w(α(t)),xi〉) ‖xi‖2 , (14)\nwhere clipI is projection onto the interval I. Variables α (t) j for j 6= i are unchanged. Hence, a single iteration has the form α(t+1) = α(t) + δ (t) i ei. Similar to a Pegasos update, at each iteration a single, random, training point is considered, the “response” yi 〈 w(α(t)),xi 〉\nis calculated (this operation dominates the computational effort), and based on the response, a multiple of xi is added to the weight vector w (corresponding to changing αi). The two methods thus involve fairly similar operations at each iteration, with essentially identical computational costs. They differ in that in Pegasos, αi is changed according to some pre-determined step-size, while SDCA changes it optimally so as to maximize the dual objective (and maintain dual feasibility); there is no step-size parameter.\nSDCA was suggested and studied empirically by Hsieh et al. (2008), where empirical advantages over Pegasos were often observed. In terms of a theoretical analysis, by considering the dual problem (2) as an ℓ1-regularized, box-constrained quadratic problem, it is possible to obtain guarantees on the dual suboptimality, D(α∗) − D(α(t)), after a finite number of SDCA iterations (Shalev-Shwartz & Tewari, 2011; Nesterov, 2012; Richtárik & Takáč, 2013). However, such guarantees do not directly imply guarantees on the primal suboptimality of w(α(t)). Recently, Shalev-Shwartz & Zhang (2012) bridged this gap, and provided guarantees on P(w(α(t))) − P(w∗) after a finite number of SDCA iterations. These guarantees serve as the starting point for our theoretical study."
    }, {
      "heading" : "4.1. Naive Mini-Batching",
      "text" : "A naive approach to parallelizing SDCA using minibatches is to compute δ (t) i in parallel, according to (14), for all i ∈ At, all based on the current iterate α(t), and then update α (t+1) i = α (t) i + δ (t) i for i ∈ At, and keep α (t+1) j = α (t) j for j 6∈ At. However, not only might this approach not reduce the number of required iterations, it might actually increase the number of required iterations. This is because the dual objective need not improve monotonically (as it does for “pure” SDCA), and even not converge.\nTo see this, consider an extreme situation with only two identical training examples: Q = [ 1 11 1 ], λ = 1 n = 1 2 and mini-batch size b = 2 (i.e., in each iteration we use both examples). If we start with α(0) = 0 with D(α(0)) = 0 then δ (0) 1 = δ (0) 2 = 1 and following the naive approach we have α(1) = (1, 1)T with objective value D(α(1)) = 0. In the next iteration δ (1) 1 = δ (1) 2 = −1 which brings us back to α\n(2) = 0. So the algorithm will alternate between those two solutions with objective value D(α) = 0, while at the optimum D(α∗) = D((0.5, 0.5)⊤) = 0.25.\nThis is of course a simplistic toy example, but the same phenomenon will occur when a large number of training examples are identical or highly correlated. This can also be observed empirically in some of our experiments discussed later, e.g., in Figure 2.\nThe problem here is that since we update each αi independently to its optimal value as if all other coordinates were fixed, we are ignoring interactions between the updates. As we see in the extreme example above, two different i, j ∈ At, might suggest essentially the same change to w(α(t)), but we would then perform this update twice, overshooting and yielding a new iterate which is actually worse then the previous one."
    }, {
      "heading" : "4.2. Safe Mini-Batching",
      "text" : "Properly accounting for the interactions between coordinates in the mini-batch would require jointly optimizing over all αi, i ∈ At. This would be a very powerful update and no-doubt reduce the number of required iterations, but would require solving a boxconstrained quadratic program, with a quadratic term of the form δ⊤AQAδA, δA ∈ R\nb, at each iteration. This quadratic program cannot be distributed to different machines, each handling only a single data point.\nInstead, we propose a “safe” variant, where the term δ⊤AQAδA is approximately bounded by the separable surrogate β ‖δA‖ 2 , for some β > 0 which we will discuss later. That is, the update is given by:\nδ (t) i := argmax\n0≤α (t) i +δ≤1\n(λn− (Qei) ⊤α(t))δ − β2 δ 2\n= clip [−α\n(t) i ,1−α (t) i ]\nλn(1−yi〈w(α(t)),xi〉) β , (15)\nwith α (t+1) i = α (t) i + δ (t) i for i ∈ At, and α (t+1) j = α (t) j for j 6∈ At. In essence, 1 β serves as a step-size, where we are now careful not to take steps so big that they will accumulate together and overshoot the objective. If handling only a single point at each iteration, such a short-step approach is not necessary, we do not need a step-size, and we can take a “full step”, setting αi\noptimally (β = 1). But with the potential for interaction between coordinates updated in parallel, we must use a smaller step, depending on the potential for such interactions.\nWe will first rely on the bound (10), and establish that the choice β = βb as in (11) provides for a safe step size. To do so, we consider the dual objective at α+δ,\nD(α+δ) = − (α ⊤Qα+2α⊤Qδ+δ⊤Qδ)\n2λn2 +\nn ∑\ni=1\nαi+δi n , (16)\nand the following separable approximation to it:\nH(δ,α) := − (α ⊤Qα+2α⊤Qδ+βb‖δ‖ 2) 2λn2 +\nn ∑\ni=1\nαi+δi n ,\n(17)\nin which βb ‖δ‖ 2 replaces δ⊤Qδ. Our update (15) with β = βb can be written as δ = arg max δ:0≤α+δ≤1 H(δ,α) (we then use the coordinates δi for i ∈ A and ignore the rest). We are essentially performing parallel coordinate ascent on the separable approximation H(δ,α) instead of on D(α + δ). To understand this approximation, we note that H(0,α) = D(α), and show that H(δ,α) provides an approximate expected lower bound on D(α+ δ):\nLemma 3. For any α, δ ∈ Rn and A ∈ Rand(b),\nEA[D(α+ δ[A])] ≥ (1− b n )D(α) + b nH(δ,α).\nProof. Examining (16) and (17), the terms that do not depend on δ are equal on both sides. For the linear term in δ, we have that E[δ[A]] = b nδ, and again we have equality on both sides. For the quadratic term we use Lemma 1 which yields E[δ⊤[A]Qδ[A]] ≤ b nβb ‖δ‖ 2 , and after negation establishes the desired bound.\nInequalities of this general type are also studied in (Richtárik & Takáč, 2012) (see Sections 3 and 4). Based on the above lemma, we can modify the analysis of Shalev-Shwartz & Zhang (2012) to obtain (see complete proof in the appendix):\nTheorem 2. Consider the SDCA updates given by (15), with At ∈ Rand(b), starting from α\n(0) = 0 and with β = βb (given in eq. (11)). For any ǫ > 0 and\nt0 ≥ max{0, ⌈ n b log( 2λn βb )⌉}, (18)\nT0 ≥ t0 + βb b\n[\n4 λǫ − 2 n βb\n]\n+ , (19)\nT ≥ T0 +max{⌈ n b ⌉, βb b 1 λǫ}, (20)\nᾱ := 1T−T0\nT−1 ∑\nt=T0\nα(t), (21)\nwe have\nE[P(w(ᾱ))]−P(w∗) ≤ E[P(w(ᾱ))−D(ᾱ)] ≤ ǫ.\nThe number of iterations of mini-batched SDCA, sufficient to reach primal suboptimality ǫ, is by Theorem 2 equal to\nÕ (\nn b + βb b · 1 λǫ\n)\n. (22)\nWe observe the same speedup as in the case of minibatched Pegasos: factor of bβb , with an essentially linear speedup when b ≤ 1σ2 . It is interesting to note that the quantity βb only affects the second, ǫ-dependent, term in (22). The “fixed cost” term, which essentially requires a full pass over the data, is not affected by βb, and is always scaled down by b."
    }, {
      "heading" : "4.3. Aggressive Mini-Batching",
      "text" : "Using β = βσ is safe, but might be too safe/conservative. In particular, we used the spectral norm to bound δ⊤Qδ ≤ ‖Q‖ ‖δ‖ 2 in Lemma 3 (through Lemma 1), but this is a worst case bound over all possible vectors, and might be loose for the relevant vectors δ. Relying on a worst-case bound might mean we are taking much smaller steps then we could be. Furthermore, the approach we presented thus far relies on knowing the spectral norm of the data, or at least a bound on the spectral norm (recall (10)), in order to set the step-size. Although it is possible to estimate this quantity by sampling, this can certainly be inconvenient.\nInstead, we suggest a more aggressive variant of minibatched SDCA which gradually adapts β based on the actual values of ‖δ (t) [At] ‖2 and δ (t) [At] Qδ (t) [At] . In Section 5 one can observe advantages of this aggressive strategy.\nIn this variant, at each iteration we calculate the ratio δ̃ ⊤ [A]Qδ̃ ⊤ [A]/‖δ̃[A]‖ 2, and nudge the step size towards it by updating it to a weighted geometric average of the previous step size and “optimal” step size based on the step δ considered. One complication is that due to the box constraints, not only the magnitude but also the direction of the step δ depends on the step-size β, leading to a circular situation. The approach we take is as follows: we maintain a “current step size” β. At each iteration, we first calculate a tentative step δ̃A, according to (15), with the current β. We then calculate ρ = δ̃ ⊤ [A]Qδ̃ ⊤ [A]\n‖δ̃[A]‖ 2 , according to this step direction, and\nupdate β to βγρ1−γ for some pre-determined parameter 0 < γ < 1 that controls how quickly the step-size adapts. But, instead of using δ̃ calculated with the previous β, we actually re-compute δA using the stepsize ρ. We note that this means the ratio ρ does not\nAlgorithm 2 SDCA with Mini-Batches (aggressive)\nInput: {(xi, yi)} n i=1, λ > 0, b ∈ R d, T ≥ 1, γ = 0.95\nInitialize: set α(0) = 0, w(0) = 0, β(0) = βb for t = 0 to T do Choose At ∈ Rand(b) For i ∈ At, compute δ̃i from (15) using β = β (t)\nSum ζ := ∑ i∈At δ̃ 2 i and ∆̃ := ∑ i∈At δ̃iyixi\nCompute ρ = clip[1,βb]\n(\n‖∆̃‖ 2\nζ\n)\nFor i ∈ At, compute δi from (15) using β = ρ. β(t+1) := (β(t))γρ1−γ if D(α(t) + δ[At]) > D(α (t)) then\nα(t+1) = α(t) + δ[At], w(t+1) = w(t) + 1λn ∑ i∈At δiyixi\nelse\nα(t+1) = α(t), w(t+1) = w(t)\nend if\nend for\ncorrespond to the step δA actually taken, but rather to the tentative step δ̃A. We could potentially continue iteratively updating ρ according to δA and δA according to ρ, but we found that this does not improve performance significantly and is generally not worth the extra computational effort. This aggressive strategy is summarized in Algorithm 2. Note that we initialize β = βb, and also constrain β to remain in the range [1, βb], but we can use a very crude upper bound σ2 for calculating βb. Also, in our aggressive strategy, we refuse steps that do not actually increase the dual objective, corresponding to overly aggressive step sizes.\nCarrying out the aggressive strategy requires computing δ̃ ⊤\n[A]Qδ̃[A] and the dual objective efficiently and in parallel. The main observation here is that:\nδ̃ ⊤\n[A]Qδ̃[A] =\n∥ ∥ ∥ ∥ ∥ ∑\ni∈A\nδ̃iyixi\n∥ ∥ ∥ ∥ ∥ 2\n(23)\nand so the main operation to be performed is an aggregation of ∑\ni∈A δ̃iyixi, similar to the operation required in mini-batched Pegasos. As for the dual objective, it can be written as D(α) = −‖w(α)‖\n2 − 1n ‖α‖1\nand can thus be readily calculated if we maintain w(α), its norm, and ‖α‖1."
    }, {
      "heading" : "5. Experiments",
      "text" : "Figure 1 shows the required number of iterations (corresponding to the parallel runtime) required for achieving a primal suboptimality of 0.001 using Pegasos,\nnaive SDCA, safe SDCA and aggressive SDCA, on four benchmark datasets detailed in Table 1, using different mini-batch sizes. Also shown (on an independent scale; right axis) is the leading term βbb in our complexity results. The results confirm the advantage of SDCA over Pegasos, at least for b = 1, and that both Pegasos and SDCA enjoy nearly-linear speedups, at least for small batch sizes. Once the mini-batch size is such that βbb starts flattening out (corresponding to b ≈ 1σ2 , and so significant correlations inside each mini-batch), the safe variant of SDCA follows a similar behavior and does not allow for much parallelization speedup beyond this point, but at least does not deteriorate like the naive variant. Pegasos and the aggressive variant do continue showing speedups beyond b ≈ 1σ2 . The experiments clearly demonstrate the aggressive modification allows SDCA to continue enjoying roughly the same empirical speedups as Pegasos, even for large mini-batch sizes, maintaining an advantage throughout. It is interesting to note that the aggressive variant continues improving even past the point of failure of the naive variant, thus establishing that it is empirically important to adjust the step-size to achieve a balance between safety and progress.\nIn Figure 2 we demonstrate the evolution of solutions using the various methods for two specific data sets. Here we can again see the relative behaviour of the methods, as well as clearly see the failure of the naive approach, which past some point causes the objective to deteriorate and does not converge to the optimal solution."
    }, {
      "heading" : "6. Conclusion",
      "text" : "Contribution. Our contribution in this paper is twofold: (i) we identify the spectral norm of the data, and through it the quantity βb, as the important quantity controlling guarantees for minibatched/parallelized Pegasos (primal method) and\nSDCA (dual method). We provide the first analysis of mini-batched Pagasos, with the non-smooth hingeloss, that shows speedups, and we analyze for the first time mini-batched SDCA with guarantees expressed in terms of the primal problem (hence, our mini-batched SDCA is a primal-dual method); (ii) based on our analysis, we present novel variants of mini-batched SDCA which are necessary for achieving speedups similar to those of Pegasos, and thus open the door to effective mini-batching using the often-empirically-better SDCA.\nRelated work. Our safe SDCA mini-batching approach is similar to the parallel coordinate descent methods of Bradley et al. (2011) and Richtárik & Takáč (2012), but we provide an analysis in terms of the primal SVM objective, which is the more relevant object of interest. Furthermore, Bradley et al.’s analysis does not use a step-size and is thus limited only to small enough mini-batches— if the spectral norm is unknown and too large a mini-batch is used, their method might not converge. Richtárik & Takáč’s method does incorporate a fixed step-size, similar to our safe variant, but as we discuss this step-size might be too conservative for achieving the true potential of mini-batching.\nGenerality. We chose to focus on Pegasos and SDCA with regularized hinge-loss minimization, but all our results remain unchanged for any Lipschitz loss functions. Furthermore, Lemma 2 can also be used to es-\ntablish identical speedups for mini-batched SGD optimization of min‖w‖≤B L̂(w), as well as for direct stochastic approximation of the population objective (generalization error) minL(w). In considering the population objective, the sample size is essentially infinite, we sample with replacements (from the population), σ2 is a bound on the second moment of the data distribution, and βb = 1 + (b − 1)σ 2.\nExperiments. Our experiments confirm the empirical advantages of SDCA over Pegasos, previously observed without mini-batching. However, we also point out that in order to perform mini-batched SDCA effectively, a step-size is needed, detracting from one of the main advantages of SDCA over Pegasos. Furthermore, in the safe variant, this stepsize needs to be set according to the spectral norm (or bound on the spectral norm), with too small a setting for β (i.e., too large steps) possibly leading to non-convergence, and too large a setting for β yielding reduced speedups. In contrast, the Pegasos stepsize is independent of the spectral norm, and in a sense Pegasos adapts implicitly (see, e.g., its behavior compared to aggressive SDCA in the experiments). We do provide a more aggressive variant of SDCA, which does match Pegasos’s speedups empirically, but this requires an explicit heuristic adaptation of the stepsize.\nParallel Implementation. In this paper we analyzed the iteration complexity, and behavior of the iterates, of mini-batched Pegasos and SDCA. Un-\nlike “pure” (b=1) Pegasos and SDCA, which are not amenable to parallelization, using mini-batches does provide opportunities for it. Of course, actually achieving good parallelization speedups on a specific architecture in practice requires an efficient parallel, possibly distributed, implementation of the iterations. In this regard, we point out that the core computation required for both Pegasos and SDCA is that of computing ∑\ni∈A gi(〈w,xi〉)xi, where g is some scalar function. Parallelizing such computations efficiently in a distributed environment has been studied by e.g., Dekel et al. (2012); Hsu et al. (2011); their methods can be used here too. Alternatively, one could also consider asynchronous or delayed updates (Agarwal & Duchi, 2011; Niu et al., 2011)."
    }, {
      "heading" : "A. Proof of Theorem 2",
      "text" : "The proof of Theorem 2 follows mostly along the path of Shalev-Shwartz & Zhang (2012), crucially using Lemma 3, and with a few other required modifications detailed below.\nWe will prove the theorem for a general L-Lipschitz loss function ℓ(·). For consistency with Shalev-Shwartz & Zhang, we will also allow example-specific loss functions ℓi, i = 1, 2, . . . , n, and only require each ℓi be individually Lipschitz, and thus refer to the primal and dual problems (expressed slightly differently but equivalently):\nmin w∈Rd\n[\nP(w) := 1n\nn ∑\ni=1\nℓi(〈w,xi〉) + λ 2 ‖w‖ 2\n]\n, (P)\nmax α∈Rn\n[\nD(α) := − 1n\nn ∑\ni=1\nℓ∗i (−αi)− λ 2\n∥ ∥\n1 λnX ⊤α ∥ ∥\n2 2\n]\n, (D)\nwhere ℓ∗i (u) = maxz(zu − ℓi(z)) is the Fenchel conjugate of ℓi. In the above we dropped without loss of generality the labels yi since we can always substitute xi ← yixi. For the hinge loss ℓi(a) = [1 − a]+ we have ℓ∗i (−a) = −a for a ∈ [0, 1] and ℓ ∗ i (−a) = ∞ otherwise, thus encoding the box constraints. Recall also (from (4)) that w(α) = 1λn ∑n i=1 αixi and so ‖w(α)‖ 2 = 1λ2n2α ⊤XX⊤α = ∥ ∥ 1 λnX ⊤α ∥ ∥ 2 .\nThe separable approximation H(δ,α) defined in (17) now has the more general form:\nH(δ,α) := − 1n\nn ∑\ni=1\nℓ∗i (−(αi + δi))− λ 2\n(\n‖w(α)‖ 2 + βb 1 λn\nn ∑\ni=1\n‖xi‖ 2 δ2i + 2 ( 1 λnδ )⊤ Xw(α)\n)\n(24)\nand all the properties mentioned in Section 4, including Lemma 3, still hold.\nOur goal here is to get a bound on the duality gap, which we will denote by\nG(α) := P(w(α))−D(α) = 1n\nn ∑\ni=1\n[ℓi(〈w(α),xi〉) + ℓ ∗ i (−αi) +αi 〈w(α),xi〉] . (25)\nThe analysis now rests on the following lemma, paralleling Lemma 1 of Shalev-Shwartz & Zhang (2012), which bounds the expected improvement in the dual objective after a single iteration in terms of the duality gap:\nLemma 4. For any t and any s ∈ [0, 1] we have\nEAt [D(α (t+1))]−D(α(t)) ≥ b\n(\ns nG(α (t))− ( s n )2 βb 2λG\n(t) )\n, (26)\nwhere\nG(t) := 1n\nn ∑\ni=1\n‖xi‖ 2(χ (t) i −α (t) i ) 2 ≤ G, (27)\nwith G = 4L for general L-Lipschitz loss, and G = 1 for the hinge loss, and −χ (t) i ∈ ℓ ′ i( 〈 w(α(t)),xi 〉 ).\nProof. The situation here is trickier then in the case b = 1 considered by Shalev-Shwartz & Zhang, and we will first bound the right hand side of (26) by H−D and then use the fact that δ(t) is a minimizer of H(·,α):\n− n\nb\n( E[D(α(t+1))]−D(α(t)) ) = − n\nb\n(\nE[D(α(t) + δ (t) [At]\n)]−D(α(t)) ) (Lemma 3) ≤ −H(δ(t),α(t)) +D(α(t))\n= 1\nn\nn ∑\ni=1\n(\nℓ∗i (−(α (t) i + δ (t) i ))− ℓ ∗ i (−α (t) i ) ) + λ\n2\n(\nβb\n∥ ∥ ∥ ∥ 1 λn δ(t) ∥ ∥ ∥ ∥ 2\nX\n+ 2\n(\n1 λn δ(t) )⊤ Xw(α(t))\n)\n,\nwhere we denote ‖u‖ 2 X := ∑n i=1 u 2 i ‖xi‖ 2. We will now use the optimally of δ(t) to upper bound the above, noting that if we replace δ(t) with any quantity, and in particular with s(χ(t) −α(t)), we can only decrease H(·,α(t)), and thus increase the right-hand-side above:\n≤ 1n\nn ∑\ni=1\n[\nℓ∗i (−(α (t) i + s(χ (t) i −α (t) i )))− ℓ ∗ i (−α (t) i ) ]\n+ λ2\n(\nβb\n∥ ∥ ∥\n1 λns(χ\n(t) −α(t)) ∥ ∥\n∥\n2 X + 2 ( 1 λns(χ (t) −α(t)) )⊤ Xw(α(t))\n)\nNow from convexity we have ℓ∗i (−(α (t) i + s(χ (t) i −α (t) i ))) ≤ sℓ ∗ i (−χ (t) i ) + (1− s)ℓ ∗ i (−α (t) i ), and so:\n≤ 1n\nn ∑\ni=1\n(\nsℓ∗i (−χ (t) i ) + sχ (t) i\n〈\nw(α(t)),xi\n〉 − sℓ∗i (−α (t) i ) )\n+ λ2\n(\nβb\n∥ ∥ ∥\n1 λns(χ\n(t) −α(t)) ∥ ∥\n∥\n2 X + 2 ( 1 λns(−α (t)) )⊤ Xw(α(t))\n)\nand from conjugacy we have ℓ∗i (−χ (t) i ) = −χ (t) i 〈 w(α(t)),xi 〉 − ℓi( 〈 w(α(t)),xi 〉 ), and so:\n≤ sn\nn ∑\ni=1\n(\n−χ (t) i\n〈\nw(α(t)),xi\n〉\n− ℓi\n(〈\nw(α(t)),xi\n〉)\n+ χ (t) i\n〈\nw(α(t)),xi\n〉 − ℓ∗i (−α (t) i ) )\n+ λ2\n(\nβb\n∥ ∥ ∥\n1 λns(χ\n(t) −α(t)) ∥ ∥\n∥\n2 X + 2 ( 1 λns(−α (t)) )⊤ Xw(α(t))\n)\n≤ sn\nn ∑\ni=1\n(\n−ℓi\n(〈\nw(α(t)),xi\n〉)\n− ℓ∗i (−α (t) i )−α (t) i\n〈\nw(α(t)),xi\n〉)\n+ λ2βb\n∥ ∥ ∥\n1 λns(χ\n(t) −α(t)) ∥ ∥\n∥\n2\nX\n(25) = −sG(α(t)) + 12λ ( s n\n)2 (\nβb\n∥ ∥ ∥ (χ(t) −α(t)) ∥ ∥ ∥ 2\nX\n)\n.\nMultiplying both sides of the resulting inequality by −bn we obtain (26). To get the bound on G (t), recall that ℓ(·) is L-Lipschitz, hence −L ≤ χ (t) i ≤ L. Furthermore, α (t) is dual feasible, hence ℓ∗i (−α (t) i ) < ∞ and so (−α (t) i ) is a (sub)derivative of ℓi and so we also have −L ≤ α (t) i ≤ L and for each i, and (χ (t) i −α (t) i ) 2 ≤ 4L. For the hinge loss we have 0 ≤ χ (t) i ,α (t) i ≤ 1, and so (χ (t) i −α (t) i ) 2 ≤ 1.\nWe are now ready to prove the theorem.\nProof of Theorem 2. We will bound the change in the dual sub-optimality ǫ (t) D := D(α ∗)−D(α(t)):\nEAt [ǫ (t+1) D ] = E[D(α (t))−D(α(t+1)) + ǫ (t) D ]\n(Lemma 4)\n≤ −b (\ns nG(α (t))− ( s n )2 βb 2λG\n)\n+ ǫ (t) D\nǫ (t) D ≤G(α(t))\n≤ −b snǫ (t) D + b\n( s\nn\n)2 βb 2λG+ ǫ (t) D = (1− b s n )ǫ (t) D + b ( s\nn\n)2 βb 2λG. (28)\nUnrolling this recurrence, we have:\nE[ǫ (t) D ] ≤ (1 − b s n ) tǫ (0) D + b ( s n )2 βb 2λ\nG t−1 ∑\ni=0\n(1− b sn ) i ≤ (1− b sn ) tǫ (0) D + ( s n ) βbG 2λ .\nSetting s = 1 and\nt0 := [⌈ n b log(2λnǫ (0) D /(Gβb))⌉]+ (29)\nyields:\nE[ǫ (t0) D ] ≤ (1− b n ) t0ǫ (0) D +\ns\nn\nβbG\n2λ ≤ Gβb 2λnǫD ǫD + 1 n βbG 2λ = βbG λn . (30)\nFollowing the proof of Shalev-Shwartz & Zhang we will now show by induction that\n∀t ≥ t0 : E[ǫ (t) D ] ≤\n2βbG\nλ(2n+ b(t− t0)) . (31)\nClearly, (30) implies that (31) holds for t = t0. Now, if it holds for some t ≥ t0, we show that it also holds for t+ 1. Using s = 2n2n+b(t−t0) in (28) we have:\nE[ǫ (t+1) D ]\n(28)\n≤ (1 − b sn )E[ǫ (t) D ] + b\n( s\nn )2 βb 2λ G (31) ≤ (1− b sn ) 2βbG λ(2n+ b(t− t0)) + b ( s n )2 βb 2λ G\n= (1− b 2\n2n+ b(t− t0) )\n2βbG\nλ(2n+ b(t− t0)) + b\n(\n2\n2n+ b(t− t0)\n)2 βb 2λ G\n= 2Gβbλ(2n+b(t−t0)+b) (2n+b(t−t0)+b)(2n+b(t−t0)−b) (2n+b(t−t0))2 ≤ 2Gβb λ(2n+ b(t− t0) + b) , (32)\nwhere in the last inequality we used the arithmetic-geometric mean inequality. This establishes (31).\nNow, for the average ᾱ defined in (21) we have:\nE[G(ᾱ)] = E\n[\nG\n(\nT−1 ∑\nt=T0\n1 T−T0 α(t)\n)]\n≤ 1T−T0E\n[\nT−1 ∑\nt=T0\nG ( α(t) )\n]\nApplying Lemma 4 with s = nb(T−T0) :\n≤ nb(T − T0)\nnb\n1\nT − T0\n( E[D(α(T ))]− E[D(α(T0))] ) + Gβbn\n2nb(T − T0)λ\n≤ ( D(α∗)− E[D(α(T0))] ) + Gβb\n2b(T − T0)λ\n(31)\n≤\n(\n2βbG\nλ(2n+ b(T0 − t0))\n)\n+ Gβb\n2b(T − T0)λ\nand if T ≥ ⌈nb ⌉+ T0 and T0 ≥ t0:\n≤ βbG\nbλ\n(\n2\n2nb + (T0 − t0) +\n1\n2(T − T0)\n)\n. (33)\nNow, we can ensure the above is at most ǫ if we require:\nT0 − t0 ≥ βb b\n(\n4G λǫ − 2 n\nβb\n)\n(34)\nT − T0 ≥ βb b G λǫG . (35)\nCombining the requirements (29), (34) and (35) with T ≥ ⌈nb ⌉+T0 and T0 ≥ t0, and recalling that for the hinge loss G = 1 and with α(0) = 0 we have ǫ (0) D = D(α ∗)−D(0) ≤ 1−0 = 1 gives the requirements in Theorem 2."
    } ],
    "references" : [ {
      "title" : "Distributed delayed stochastic optimization",
      "author" : [ "A. Agarwal", "J. Duchi" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Agarwal and Duchi,? \\Q2011\\E",
      "shortCiteRegEx" : "Agarwal and Duchi",
      "year" : 2011
    }, {
      "title" : "Parallel coordinate descent for l1-regularized loss minimization",
      "author" : [ "J.K. Bradley", "A. Kyrola", "D. Bickson", "C. Guestrin" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Bradley et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Bradley et al\\.",
      "year" : 2011
    }, {
      "title" : "Better mini-batch algorithms via accelerated gradient methods",
      "author" : [ "A. Cotter", "O. Shamir", "N. Srebro", "K. Sridharan" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Cotter et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Cotter et al\\.",
      "year" : 2011
    }, {
      "title" : "Optimal distributed online prediction using minibatches",
      "author" : [ "O. Dekel", "R. Gilad-Bachrach", "O. Shamir", "L. Xiao" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Dekel et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Dekel et al\\.",
      "year" : 2012
    }, {
      "title" : "A dual coordinate descent method for large-scale linear svm",
      "author" : [ "Hsieh", "C-J", "Chang", "K-W", "Lin", "S.S. Keerthi", "S. Sundarajan" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Hsieh et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Hsieh et al\\.",
      "year" : 2008
    }, {
      "title" : "Efficiency of coordinate descent methods on huge-scale optimization problems",
      "author" : [ "Nesterov", "Yu" ],
      "venue" : "SIAM J. Optimization,",
      "citeRegEx" : "Nesterov and Yu.,? \\Q2012\\E",
      "shortCiteRegEx" : "Nesterov and Yu.",
      "year" : 2012
    }, {
      "title" : "Hogwild: A lock-free approach to parallelizing stochastic gradient descent",
      "author" : [ "F. Niu", "B. Recht", "C. Re", "S. Wright" ],
      "venue" : "K.Q. (eds.),",
      "citeRegEx" : "Niu et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Niu et al\\.",
      "year" : 2011
    }, {
      "title" : "Making gradient descent optimal for strongly convex stochastic optimization",
      "author" : [ "A. Rakhlin", "O. Shamir", "K. Sridharan" ],
      "venue" : null,
      "citeRegEx" : "Rakhlin et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Rakhlin et al\\.",
      "year" : 2012
    }, {
      "title" : "Parallel coordinate descent methods for big data optimization",
      "author" : [ "P. Richtárik", "M. Takáč" ],
      "venue" : null,
      "citeRegEx" : "Richtárik and Takáč,? \\Q2012\\E",
      "shortCiteRegEx" : "Richtárik and Takáč",
      "year" : 2012
    }, {
      "title" : "Iteration complexity of randomized block-coordinate descent methods for minimizing a composite function",
      "author" : [ "P. Richtárik", "M. Takáč" ],
      "venue" : "Mathematical Programming,",
      "citeRegEx" : "Richtárik and Takáč,? \\Q2013\\E",
      "shortCiteRegEx" : "Richtárik and Takáč",
      "year" : 2013
    }, {
      "title" : "Stochastic Methods for l1-regularized",
      "author" : [ "S. Shalev-Shwartz", "A. Tewari" ],
      "venue" : "Loss Minimization. JMLR,",
      "citeRegEx" : "Shalev.Shwartz and Tewari,? \\Q2011\\E",
      "shortCiteRegEx" : "Shalev.Shwartz and Tewari",
      "year" : 2011
    }, {
      "title" : "Stochastic dual coordinate ascent methods for regularized loss minimization",
      "author" : [ "S. Shalev-Shwartz", "T. Zhang" ],
      "venue" : null,
      "citeRegEx" : "Shalev.Shwartz and Zhang,? \\Q2012\\E",
      "shortCiteRegEx" : "Shalev.Shwartz and Zhang",
      "year" : 2012
    }, {
      "title" : "Pegasos: Primal estimated sub-gradient solver for svm",
      "author" : [ "S. Shalev-Shwartz", "Y. Singer", "N. Srebro", "A. Cotter" ],
      "venue" : "Mathematical Programming: Series A and B- Special Issue on Optimization and Machine Learning,",
      "citeRegEx" : "Shalev.Shwartz et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Shalev.Shwartz et al\\.",
      "year" : 2011
    }, {
      "title" : "Solving large scale linear prediction using stochastic gradient descent algorithms",
      "author" : [ "T. Zhang" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Zhang,? \\Q2004\\E",
      "shortCiteRegEx" : "Zhang",
      "year" : 2004
    } ],
    "referenceMentions" : [ {
      "referenceID" : 4,
      "context" : "2011, NORMA, Zhang 2004) and dual stochastic coordinate ascent (Hsieh et al., 2008).",
      "startOffset" : 63,
      "endOffset" : 83
    }, {
      "referenceID" : 4,
      "context" : "SDCA has been consistently shown to outperform Pegasos in practice (Hsieh et al., 2008; Shalev-Shwartz et al., 2011), and is also popular as it does not rely on setting a step-size as in Pegasos.",
      "startOffset" : 67,
      "endOffset" : 116
    }, {
      "referenceID" : 12,
      "context" : "SDCA has been consistently shown to outperform Pegasos in practice (Hsieh et al., 2008; Shalev-Shwartz et al., 2011), and is also popular as it does not rely on setting a step-size as in Pegasos.",
      "startOffset" : 67,
      "endOffset" : 116
    }, {
      "referenceID" : 4,
      "context" : "2011, NORMA, Zhang 2004) and dual stochastic coordinate ascent (Hsieh et al., 2008). However, the inherent sequential nature of such approaches becomes a problematic limitation for parallel and distributed computations as the predictor must be updated after each training point is processed, providing very little opportunity for parallelization. A popular remedy is to use mini-batches. That is, to use several training points at each iteration, instead of just one, calculating the update based on each point separately and aggregating the updates. The question is then whether basing each iteration on several points can indeed reduce the number of required iterations, and thus yield parallelization speedups. In this paper, we consider using mini-batches with Pegasos (SGD on the primal objective) and with Stochastic Dual Coordinate Ascent (SDCA). We show that for both methods, the quantity that controls the speedup obtained using mini-batching/parallelization is the spectral norm of the data. In Section 3 we provide the first analysis of minibatched Pegasos (with the original, non-smooth, SVM objective) that provably leads to parallelization speedups (Theorem 1). The idea of using mini-batches with Pegasos is not new, and is discussed already by Shalev-Shwartz et al. (2011), albeit without a theoretical justification.",
      "startOffset" : 64,
      "endOffset" : 1290
    }, {
      "referenceID" : 13,
      "context" : "Similar to a recent analysis of non-mini-batched SDCA by Shalev-Shwartz & Zhang (2012), we establish a guarantee on the duality gap, and thus also on the suboptimality of the primal SVM objective, when using mini-batched SDCA (Theorem 2).",
      "startOffset" : 74,
      "endOffset" : 87
    }, {
      "referenceID" : 3,
      "context" : "Several recent papers consider the use of mini-batches in stochastic gradient descent, as well as stochastic dual averaging and stochastic mirror descent, when minimizing a smooth loss function (Dekel et al., 2012; Agarwal & Duchi, 2011; Cotter et al., 2011).",
      "startOffset" : 194,
      "endOffset" : 258
    }, {
      "referenceID" : 2,
      "context" : "Several recent papers consider the use of mini-batches in stochastic gradient descent, as well as stochastic dual averaging and stochastic mirror descent, when minimizing a smooth loss function (Dekel et al., 2012; Agarwal & Duchi, 2011; Cotter et al., 2011).",
      "startOffset" : 194,
      "endOffset" : 258
    }, {
      "referenceID" : 1,
      "context" : "Bradley et al. (2011) presented and analyzed SHOTGUN, a parallel coordinate descent method for l1-regularized problems, showing linear speedups for mini-batch sizes bounded in terms of the spectral norm of the data.",
      "startOffset" : 0,
      "endOffset" : 22
    }, {
      "referenceID" : 1,
      "context" : "Bradley et al. (2011) presented and analyzed SHOTGUN, a parallel coordinate descent method for l1-regularized problems, showing linear speedups for mini-batch sizes bounded in terms of the spectral norm of the data. The analysis does not directly apply to the SVM dual because of the box constraints, but is similar in spirit. Furthermore, Bradley et al. (2011) do not discuss a “safe” variant which is applicable for any mini-batch size, and only study the analogue of what we refer to as “naive” mini-batching (Section 4.",
      "startOffset" : 0,
      "endOffset" : 362
    }, {
      "referenceID" : 1,
      "context" : "Bradley et al. (2011) presented and analyzed SHOTGUN, a parallel coordinate descent method for l1-regularized problems, showing linear speedups for mini-batch sizes bounded in terms of the spectral norm of the data. The analysis does not directly apply to the SVM dual because of the box constraints, but is similar in spirit. Furthermore, Bradley et al. (2011) do not discuss a “safe” variant which is applicable for any mini-batch size, and only study the analogue of what we refer to as “naive” mini-batching (Section 4.1). More directly related is recent work of Richtárik & Takáč (2013; 2012) which provided a theoretical framework and analysis for a more general setting than SHOTGUN, that includes also the SVM dual as a special case. However, guarantees in this framework, as well as those of Bradley et al. (2011), are only on the dual suboptimality (in our terminology), and not on the more relevant primal suboptimality, i.",
      "startOffset" : 0,
      "endOffset" : 823
    }, {
      "referenceID" : 1,
      "context" : "Bradley et al. (2011) presented and analyzed SHOTGUN, a parallel coordinate descent method for l1-regularized problems, showing linear speedups for mini-batch sizes bounded in terms of the spectral norm of the data. The analysis does not directly apply to the SVM dual because of the box constraints, but is similar in spirit. Furthermore, Bradley et al. (2011) do not discuss a “safe” variant which is applicable for any mini-batch size, and only study the analogue of what we refer to as “naive” mini-batching (Section 4.1). More directly related is recent work of Richtárik & Takáč (2013; 2012) which provided a theoretical framework and analysis for a more general setting than SHOTGUN, that includes also the SVM dual as a special case. However, guarantees in this framework, as well as those of Bradley et al. (2011), are only on the dual suboptimality (in our terminology), and not on the more relevant primal suboptimality, i.e., the suboptimality of the original SVM problem we are interested in. Our theoretical analysis builds on that of Richtárik & Takáč (2012), combined with recent ideas of Shalev-Shwartz & Zhang (2012) for “standard” (serial) SDCA, to obtain bounds on the duality gap and primal suboptimality.",
      "startOffset" : 0,
      "endOffset" : 1074
    }, {
      "referenceID" : 1,
      "context" : "Bradley et al. (2011) presented and analyzed SHOTGUN, a parallel coordinate descent method for l1-regularized problems, showing linear speedups for mini-batch sizes bounded in terms of the spectral norm of the data. The analysis does not directly apply to the SVM dual because of the box constraints, but is similar in spirit. Furthermore, Bradley et al. (2011) do not discuss a “safe” variant which is applicable for any mini-batch size, and only study the analogue of what we refer to as “naive” mini-batching (Section 4.1). More directly related is recent work of Richtárik & Takáč (2013; 2012) which provided a theoretical framework and analysis for a more general setting than SHOTGUN, that includes also the SVM dual as a special case. However, guarantees in this framework, as well as those of Bradley et al. (2011), are only on the dual suboptimality (in our terminology), and not on the more relevant primal suboptimality, i.e., the suboptimality of the original SVM problem we are interested in. Our theoretical analysis builds on that of Richtárik & Takáč (2012), combined with recent ideas of Shalev-Shwartz & Zhang (2012) for “standard” (serial) SDCA, to obtain bounds on the duality gap and primal suboptimality.",
      "startOffset" : 0,
      "endOffset" : 1135
    }, {
      "referenceID" : 12,
      "context" : "When b = 1 we have βb = 1 (see (11)) and Theorem 1 agrees with the standard (serial) Pegasos analysis (Shalev-Shwartz et al., 2011).",
      "startOffset" : 102,
      "endOffset" : 131
    }, {
      "referenceID" : 4,
      "context" : "SDCA was suggested and studied empirically by Hsieh et al. (2008), where empirical advantages over Pegasos were often observed.",
      "startOffset" : 46,
      "endOffset" : 66
    }, {
      "referenceID" : 4,
      "context" : "SDCA was suggested and studied empirically by Hsieh et al. (2008), where empirical advantages over Pegasos were often observed. In terms of a theoretical analysis, by considering the dual problem (2) as an l1-regularized, box-constrained quadratic problem, it is possible to obtain guarantees on the dual suboptimality, D(α) − D(α), after a finite number of SDCA iterations (Shalev-Shwartz & Tewari, 2011; Nesterov, 2012; Richtárik & Takáč, 2013). However, such guarantees do not directly imply guarantees on the primal suboptimality of w(α). Recently, Shalev-Shwartz & Zhang (2012) bridged this gap, and provided guarantees on P(w(α)) − P(w) after a finite number of SDCA iterations.",
      "startOffset" : 46,
      "endOffset" : 583
    }, {
      "referenceID" : 13,
      "context" : "Based on the above lemma, we can modify the analysis of Shalev-Shwartz & Zhang (2012) to obtain (see complete proof in the appendix): Theorem 2.",
      "startOffset" : 73,
      "endOffset" : 86
    }, {
      "referenceID" : 12,
      "context" : "cov is the forest covertype dataset of Shalev-Shwartz et al. (2011), astro-ph consists of abstracts of papers from physics also of Shalev-Shwartz et al.",
      "startOffset" : 39,
      "endOffset" : 68
    }, {
      "referenceID" : 12,
      "context" : "cov is the forest covertype dataset of Shalev-Shwartz et al. (2011), astro-ph consists of abstracts of papers from physics also of Shalev-Shwartz et al. (2011), rcv1 is from the Reuters collection and news20 is from the 20 news groups both obtained from libsvm collection (Libsvm).",
      "startOffset" : 39,
      "endOffset" : 160
    }, {
      "referenceID" : 1,
      "context" : "Our safe SDCA mini-batching approach is similar to the parallel coordinate descent methods of Bradley et al. (2011) and Richtárik & Takáč (2012), but we provide an analysis in terms of the primal SVM objective, which is the more relevant object of interest.",
      "startOffset" : 94,
      "endOffset" : 116
    }, {
      "referenceID" : 1,
      "context" : "Our safe SDCA mini-batching approach is similar to the parallel coordinate descent methods of Bradley et al. (2011) and Richtárik & Takáč (2012), but we provide an analysis in terms of the primal SVM objective, which is the more relevant object of interest.",
      "startOffset" : 94,
      "endOffset" : 145
    } ],
    "year" : 2013,
    "abstractText" : "We address the issue of using mini-batches in stochastic optimization of SVMs. We show that the same quantity, the spectral norm of the data, controls the parallelization speedup obtained for both primal stochastic subgradient descent (SGD) and stochastic dual coordinate ascent (SCDA) methods and use it to derive novel variants of mini-batched SDCA. Our guarantees for both methods are expressed in terms of the original nonsmooth primal problem based on the hinge-loss.",
    "creator" : "LaTeX with hyperref package"
  }
}