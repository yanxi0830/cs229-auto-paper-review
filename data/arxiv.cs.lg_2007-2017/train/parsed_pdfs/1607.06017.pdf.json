{
  "name" : "1607.06017.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Doubly Accelerated Methods for Faster CCA and Generalized Eigendecomposition",
    "authors" : [ "Zeyuan Allen-Zhu", "Yuanzhi Li" ],
    "emails" : [ "zeyuan@csail.mit.edu", "yuanzhil@cs.princeton.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "Furthermore, our algorithms are doubly-accelerated : our running times depend only on the square root of the matrix condition number, and on the square root of the eigengap. This is the first such result for both k-GenEV or k-CCA. We also provide the first gap-free results, which provide running times that depend on 1/ √ ε rather than the eigengap."
    }, {
      "heading" : "1 Introduction",
      "text" : "The Generalized Eigenvector (GenEV) problem and the Canonical Correlation Analysis (CCA) are two fundamental problems in scientific computing, machine learning, operations research, and statistics. Algorithms solving these problems are often used to extract features to compare largescale datasets, as well as used for problems in regression [13], clustering [7], classification [14], word embeddings [8], and many others.\nGiven two symmetric matrices A,B ∈ Rd×d where B is positive definite. The GenEV problem is to find generalized eigenvectors v1, . . . , vd where each vi satisfies\nvi ∈ arg max v∈Rd\n∣∣v>Av ∣∣ such that\n{ v>Bv = 1 v>Bvj = 0 ∀j ∈ [i− 1]\nGiven matrices X ∈ Rn×dx , Y ∈ Rn×dy and denoting by Sxx = 1nX>X, Sxy = 1nX>Y , Syy = 1 nY >Y , the CCA problem is to find canonical-correlation vectors {(φi, ψi)}ni=1 where each pair\n(φi, ψi) ∈ arg max φ∈Rdx ,ψ∈Rdy\n{ φ>Sxyψ } such that\n{ φ>Sxxφ = 1 ∧ φ>Sxxφj = 0 ∀j ∈ [i− 1] ψ>Syyψ = 1 ∧ ψ>Syyψj = 0 ∀j ∈ [i− 1]\nIn GenEV, the values λi = v > i Avi are known as the generalized eigenvalues; in CCA, the values φ>i Sxyψi are known as the canonical-correlation coefficients. It is a folklore that the exact solution of a CCA problem can reduce to that of a GenEV problem, if one defines B = diag{Sxx, Syy} and A = [[0, Sxy]; [S > xy, 0]] (see Lemma 2.3).\nDespite the fundamental importance and the frequent necessity in applications, there are few results on obtaining provably efficient algorithms for GenEV and CCA until very recently. In the\nar X\niv :1\n60 7.\n06 01\n7v 1\n[ m\nat h.\nO C\n] 2\n0 Ju\nbreakthrough result of Ma, Lu and Foster [16], they proposed to study algorithms to find top k generalized eigenvectors or top k canonical-correlation vectors. They designed an alternating minimization algorithm whose running time is only linear in terms of the number of non-zero elements of the matrix (that we denote by nnz(A) for a matrix A in this paper), and also nearlylinear in k. Such algorithms are very appealing because in real-life applications, it is often only relevant to obtain top correlation vectors, as opposed to the less meaningful vectors in the directions where the datasets do not correlate.\nUnfortunately, the method of Ma, Lu and Foster has a running time that linearly scales with κ and 1/gap, where • κ ≥ 1 is the condition number of matrix B in GenEV, or respectively the condition number\nof matrices X>X,Y >Y in CCA; and • gap ∈ [0, 1) is the (relative) eigengap between λk and λk+1 in GenEV, or respectively the gap\nbetween σk and σk+1 in CCA. These parameters are usually not constants and do scale with the problem size. In the extreme case, gap can even be zero. At the same time, for many easier scientific computing problems, we are often able to design algorithms that have better dependencies on κ and 1/gap. As three concrete examples:\n• Block Krylov method computes top eigenvectors with a running time linearly in 1/√gap rather than 1/gap [12], or with a gap-free running time that depends on 1/ √ ε rather than\n1/gap or 1/ √ gap, where ε is the approximation error [19].\n• Conjugate gradient [22], Chebyshev method [6], and Nesterov’s method [20] compute B−1w for a vector w with a running time linearly in √ κ rather than κ, where κ is the condition\nnumber of matrix B.\n• If B = 1nX>X is given explicitly as the covariance matrix of X ∈ Rn×d, stochastic gradient methods can be used to compute B−1w (see Lemma 2.4) with a running time linearly in (1 + √ κ′/n) instead of\n√ κ, where κ′ = Tr(B)λmin(B) ∈ [ κ, dκ ] .\nTherefore, it is a natural question to improve the dependency of κ and 1/gap for the more challenging problems GenEV and CCA as well. Indeed, two groups of authors independently attempted to answer such questions [11, 24].\n• For the k-GenEV problem, GenELin [11] improved the dependency of κ to √κ. In a separate work, SI [24] also obtained the √ κ dependency but only for the simpler k = 1 case; at the\nsame time, SI enjoys a √ gap dependency but paying an additional factor λ1. In sum, it was unknown how to obtain the straight √ gap dependency even for the k = 1 case, and unknown how to obtain gap-free running times.\n• For the k-CCA problem, the landscape is more complicated. The two groups of authors proposed three methods: CCALin [11], ALS [24] and SI [24]. To sum up, (1) only CCALin works for k ≥ 1 and ALS/SI do not work for k > 1, (2) only SI has a nearly √gap dependency but loses an additional factor σ1 and an additional factor n\n1/4 in the stochastic case, (3) it is unknown how to obtain gap-free running times.\nThe detailed comparisons of these methods are in Table 1 for the k = 1 case, and in Table 2 for the general k ≥ 1 case. Both tables are included at the beginning of the appendix. Our Results. We provide algorithms LazyEV and LazyCCA with faster running times than all aforementioned results and for all cases k ≥ 1. We also provide the first gap-free running time for both GenEV and CCA. Since our running-time statements are very different between GenEV\nλ1\n∈ [0, 1], λ1 ∈ [0, 1], and κB = λmax(B)λmin(B) > 1.\nIn CCA, gap = σ1−σ2 σ1 ∈ [0, 1], σ1 ∈ [0, 1], κ = λmax(diag{Sxx,Syy})λmin(diag{Sxx,Syy}) > 1, and κ ′ = Tr(diag{Sxx,Syy}) λmin(diag{Sxx,Syy}) ∈ [κ, dκ]. a\naStochastic methods have to depend on a modified condition number as opposed to κ. Our immediate prior works\n(such as CCALin, ALS and SI) used κ′′ = 2maxi{‖Xi‖ 2,‖Yi‖2} λmin(diag{Sxx,Syy}) , which is a larger quantity than our κ ′. We are aware of appropriate modifications to these methods to improve their running times to depend on κ′. For this reason, we have included these faster results in our tables for a stronger comparison.\nλk\n∈ [0, 1] and κB = λmax(B)λmin(B) > 1.\nIn CCA, gap =\nσk−σk+1 σk ∈ [0, 1], κ = λmax(diag{Sxx,Syy}) λmin(diag{Sxx,Syy}) > 1, and κ ′ = Tr(diag{Sxx,Syy}) λmin(diag{Sxx,Syy}) ∈ [κ, dκ].\nand CCA, between gap-dependent and gap-free cases, and between non-stochastic and stochastic methods, we summarize them in full in Table 1 and Table 2 for a clean comparison (see appendix).\nTo mention just two of these results, for the general k-GenEV problem our running time is\nÕ (knnz(B)√κB\ngap + knnz(A) + k2d gap\n) and Õ (knnz(B)√κB√ ε + knnz(A) + k2d√ ε )\nin the gap-dependent and gap-free cases respectively. Since our running time only linearly depends on √ κ and √ gap (resp. √ ε), our algorithms are doubly-accelerated. Somewhat surprisingly, even in the simple case k = 1, our results outperform known ones on 1-GenEV and 1-CCA, see Table 1.\nOther Contributions. Besides the aforementioned running time improvements, we summarize some other virtues of our algorithms as follows:\n• For GenEV, our LazyEV method distinguishes positive generalized eigenvalues from negative ones. For instance, if A has two generalized eigenvectors v1, v2 with respect to B, one with eigenvalue λ and the other with −λ. Then, previous results such as GenELin and SI only find the subspace spanned by v1, v2 but cannot distinguish v1 from v2.\n• For CCA with k > 1, previous results such as CCALin only output the subspace spanned by the top k correlation vectors but not identify which vector gives (approximately) the highest correlation and so on. Instead, our LazyCCA provides per-vector guarantees on all top k correlation vectors, see Corollary 6.3.\n• Our LazyEV and LazyCCA reduce the non-convex problem to multiple calls of quadratic minimization. Since quadratic minimization is a well-studied convex optimization problem, many efficient and robust algorithms can be found. In contrast, previous results for the k > 1 case rely on more sophisticated nonconvex optimization; and the previous work of [24] —although uses convex optimization to solve 1-CCA— requires one to work with a sum-of-non-convex function which is less efficient to minimize.\nOther Related Works. For the easier problem of PCA and SVD, the first gap-free result was obtained by Musco and Musco [19], the first stochastic result was obtained by Shamir [21], and the first accelerated stochastic result was obtained by Garber et al. [9, 10]. The shift-and-invert preconditioning framework of Garber et al. is also used in this paper for GenEV and CCA.\nAs for GenEV and CCA, many scalable algorithms have been designed recently [15, 17, 18, 23, 25]. However, as for instance summarized by the authors of CCALin, these cited methods are more or less heuristics and do not have provable guarantees. Furthermore, for k > 1, the AppGrad result of [17] only provides local convergence guarantees and thus requires a warm-start whose computational complexity is not discussed in their paper.\nFinally, our algorithms on GenEV and CCA are based on finding vectors one-by-one, which is advantageous in practice because one does not need k to be known and can stop the algorithm whenever the eigenvalues (or correlation values) are too small. Known approaches for k > 1 cases (such as GenELin, CCALin, AppGrad) find all k vectors at once (through subspace power method on a d×k matrix), therefore requiring k to be known beforehand. As a separate note, these known approaches do not need the user to know the desired accuracy a priori but our LazyEV and LazyCCA algorithms do."
    }, {
      "heading" : "2 Preliminaries",
      "text" : "For a vector x we denote by ‖x‖ or ‖x‖2 the Euclidean norm of x. Given a matrix A we denote by ‖A‖2 and ‖A‖F respectively the spectral and Frobenius norms of A. For q ≥ 1, we denote by ‖A‖Sq the Schatten q-norm of A. We write A B if A,B are symmetric and A − B is positive semi-definite (PSD), and write A B if A,B are symmetric but A − B is positive definite (PD). We denote by λmax(M) and λmin(M) the largest and smallest eigenvalue of a symmetric matrix M , and by κM or sometimes κ(M) the condition number λmax(M)/λmin(M) of a PSD matrix M .\nThroughout this paper, for a matrix A ∈ Rn×d, we define nnz(A) def= max{n, d,N} where N is the number of non-zero entries of A. For two matrices X,Y , we denote by nnz(X,Y ) = nnz(X)+nnz(Y ). We also use poly(x1, x2, . . . , xt) to represent a quantity that is asymptotically at most polynomial in terms of variables x1, . . . , xt. Given a column orthonormal matrix U ∈ Rn×k, we denote by U⊥ ∈ Rn×(n−k) the column orthonormal matrix consisting of an arbitrary basis in the space orthogonal to the span of U ’s columns.\nGiven a PSD matrix B and a vector v, the value v>Bv is the so-called B-inner product. For this reason, two vectors v, w satisfying v>Bw = 0 are known as B-orthogonal. Given a PSD matrix B, we denote by B−1 the Moore-Penrose pseudoinverse of B which is also PSD, and denote by B1/2 an arbitrary matrix square root of B. All occurrences of B−1, B1/2 and B−1/2 are for analysis purpose only. When implementing our algorithms, it only requires one to multiply B to a vector. Definition 2.1 (GenEV). Given symmetric matrices A,B ∈ Rd×d where B is positive definite. The generalized eigenvectors of A with respect to B are v1, . . . , vd, where each vi is\nvi ∈ arg max v∈Rd\n{ ∣∣v>Av\n∣∣ such that { v>Bv = 1 v>Bvj = 0 ∀j ∈ [i− 1]\n}\nThe corresponding generalized eigenvalues λ1, . . . , λn satisfy λi = v > i Avi which is possibly negative. Definition 2.2 (CCA). Given X ∈ Rn×dx , Y ∈ Rn×dy , letting Sxx = 1nX>X, Sxy = 1nX>Y , Syy = 1 nY >Y , the canonical-correlation vectors are {(φi, ψi)}ri=1 where r = min{dx, dy} and ∀i:\n(φi, ψi) ∈ arg max φ∈Rdx ,ψ∈Rdy\n{ φ>Sxyψ such that\n{ φ>Sxxφ = 1 ∧ φ>Sxxφj = 0 ∀j ∈ [i− 1] ψ>Syyψ = 1 ∧ ψ>Syyψj = 0 ∀j ∈ [i− 1]\n}\nThe corresponding canonical-correlation coefficients σ1, . . . , σr satisfy σi = φ > i Sxyψi ∈ [0, 1].\nWhen dealing with a CCA problem, we also denote by d = dx + dy.\nLemma 2.3. Given a CCA problem with matrices X ∈ Rn×dx , Y ∈ Rn×dy , and suppose the canonical-correlation vectors and coefficients are {(φi, ψi, σi)}ri=1 where r = min{dx, dy}.\nDefine A = ( 0 Sxy S>xy 0 ) and B = ( Sxx 0 0 Syy ) . Then, the GenEV problem of A with respect\nto B has 2r eigenvalues {±σi}ri=1 and corresponding generalized eigenvectors {(\nφi ψi\n) , ( −φi ψi )}n\ni=1\n.\nThe remaining dx + dy − 2r eigenvalues are zeros. Lemma 2.4. Given matrices X ∈ Rn×dx , Y ∈ Rn×dy , let A and B be as defined in Lemma 2.3. For every w ∈ Rd, Katyusha method [1] finds a vector w′ ∈ Rd satisfying ‖w′ −B−1Aw‖ ≤ ε\nin time O ( nnz(X,Y ) · ( 1 + √ κ′/n ) · log κ‖w‖ 2\nε\n) .\nwhere κ = λmax(B)/λmin(B) is the condition number and κ ′ = Tr(B)/λmin(B) ∈ [κ, dκ]."
    }, {
      "heading" : "3 Leading Eigenvector via Two-Sided Shift-and-Invert",
      "text" : "In this section we define AppxPCA±, the multiplicative approximation algorithm for computing the two-sided leading eigenvector of a symmetric matrix using the shift-and-invert preconditioning framework [9, 10]. Our pseudo-code Algorithm 1 is a modification of Algorithm 5 appeared in [9].\nThe main differences between AppxPCA± and Algorithm 5 of [9] are two-fold. First, given a symmetric matrix M , AppxPCA± simultaneously considers an upper-bounding shift together with a lower-bounding shift, and try to invert both λI −M and λI + M . This allows us to determine approximately how close λ is to the largest and the smallest eigenvalues of M , and decrease λ accordingly; in the end, it outputs an approximate eigenvector of M that corresponds to a negative eigenvalue if needed. Second, we provide a multiplicative-error guarantee rather than additive as originally appeared in [9]. Without this multiplicative-error guarantee, our final running time will depend on 1gap·λmax(M) rather than 1 gap . 1 Of course, we believe the bulk of the credit for conceiving AppxPCA± belongs to the original authors of [9, 10].\nTheorem 3.1 (AppxPCA±). Let M ∈ Rd×d be a symmetric matrix with eigenvalues 1 ≥ λ1 ≥ · · · ≥ λd ≥ −1 and corresponding eigenvectors u1, . . . , ud. Let λ∗ = ‖M‖2 = max{λ1,−λd}. With probability at least 1− p, AppxPCA± produces a pair (sgn,w) satisfying\nif sgn = +, then ∑\ni∈[d],λi≤(1−δ×/2)λ∗ (w>ui)2 ≤ ε and w>Mw ≥ (1− δ×/2)(1− 3ε)λ∗ , and\nif sgn = −, then ∑\ni∈[d],λi≥−(1−δ×/2)λ∗ (w>ui)2 ≤ ε and w>Mw ≤ −(1− δ×/2)(1− 3ε)λ∗ .\n1This is why the only known CCA result using shift-and-invert preconditioning [24] depends on 1 gap·λ1 in Table 1.\nAlgorithm 1 AppxPCA±(A,M, δ×, ε, p) Input: A, an approximate matrix inversion method; M ∈ Rd×d, a symmetric matrix satisfying −I M I; δ× ∈ (0, 0.5], a multiplicative error; ε ∈ (0, 1), a numerical accuracy parameter; and p ∈ (0, 1), the confidence parameter.\n1: ŵ0 ← RanInit(d) be a random unit vector; s← 0; λ(0) ← 1 + δ×; see Definition 3.2 for RanInit\n2: m1 ← ⌈ 4 log ( 288dθ p2 )⌉ , m2 ← ⌈ log ( 36dθ p2ε )⌉ ; θ is the parameter of RanInit m1 = TPM(8, 1/32, p) and m2 = TPM(2, ε/4, p) using Lemma B.1 3: ε̃1 ← 164m1 ( δ× 48 )m1 and ε̃2 ← ε8m2 ( δ× 48\n)m2 4: repeat 5: s← s+ 1; 6: for t = 1 to m1 do 7: Apply A to find ŵt satisfying ∥∥ŵt − (λ(s−1)I −M)−1ŵt−1 ∥∥ ≤ ε̃1; 8: wa ← ŵm1/‖ŵm1‖; 9: Apply A to find va satisfying ∥∥va − (λ(s−1)I −M)−1wa ∥∥ ≤ ε̃1;\n10: for t = 1 to m1 do 11: Apply A to find ŵt satisfying ∥∥ŵt − (λ(s−1)I +M)−1ŵt−1 ∥∥ ≤ ε̃1; 12: wb ← ŵm1/‖ŵm1‖; 13: Apply A to find vb satisfying ∥∥vb − (λ(s−1)I +M)−1wb ∥∥ ≤ ε̃1; 14: ∆(s) ← 12 · 1max{w>a va,w>b vb}−ε̃1 and λ (s) ← λ(s−1) − ∆(s)2 ; 15: until ∆(s) ≤ δ×λ(s)12 16: f ← s; 17: if the last w>a va ≥ w>b vb then 18: for t = 1 to m2 do 19: Apply A to find ŵt satisfying ∥∥ŵt − (λ(f)I −M)−1ŵt−1 ∥∥ ≤ ε̃2; 20: return (+, w) where w def = ŵm2/‖ŵm2‖. 21: else 22: for t = 1 to m2 do 23: Apply A to find ŵt satisfying ∥∥ŵt − (λ(f)I +M)−1ŵt−1 ∥∥ ≤ ε̃2; 24: return (−, w) where w def= ŵm2/‖ŵm2‖. 25: end if\nFurthermore, the total number of oracle calls to A is O(log(1/δ×)m1 +m2), and each time we call A it satisfies that λmax(λ(s)I−M)\nλmin(λ(s)I−M) , λmax(λ(s)I+M) λmin(λ(s)I+M) ∈ [1, 96δ× ].\nWe remark here that, unlike the original shift-and-invert method which chooses a random (Gaussian) unit vector in Line 1 of AppxPCA±, we allow this initial vector to be generated from an arbitrary θ-conditioned random vector generator, defined as follows:\nDefinition 3.2. An algorithm RanInit(d) is a θ-conditioned random vector generator if w = RanInit(d) is a d-dimensional unit vector and, for every p ∈ (0, 1), every unit vector u ∈ Rd, with probability at least 1− p, it satisfies (u>w)2 ≤ p2θ9d .\nThis modification is needed in order to obtain our efficient implementations of GenEV and CCA algorithms. One can construct θ-conditioned random vector generator as follows:\nAlgorithm 2 LazyEV(A,M, k, δ×, εpca, p) Input: A, an approximate matrix inversion method; M ∈ Rd×d, a matrix satisfying −I M I;\nk ∈ [d], the desired rank; δ× ∈ (0, 1), a multiplicative error; εpca ∈ (0, 1), a numerical accuracy parameter; and p ∈ (0, 1), a confidence parameter.\n1: M0 ←M ; V0 = []; 2: for s = 1 to k do 3: v′s ← AppxPCA±(A,Ms−1, δ×/2, εpca, p/k); 4: vs ← ( (I − Vs−1V >s−1)v′s ) / ∥∥(I − Vs−1V >s−1)v′s\n∥∥; project v′s to V ⊥s−1 5: Vs ← [Vs−1, vs]; 6: Ms ← (I − vsv>s )Ms−1(I − vsv>s ) we also have Ms = (I − VsV >s )M(I − VsV >s ) 7: end for 8: return Vk.\nProposition 3.3. Given PSD matrix B ∈ Rd×d, if we set RanInit(d) def= B1/2v (v>Bv)0.5 where v is a random Gaussian vector, then RanInit(d) is a θ-conditioned random vector generator for θ = κB."
    }, {
      "heading" : "4 Main Algorithm for Generalized Eigendecomposition",
      "text" : "In this section, we propose LazyEV (see Algorithm 2) to compute approximately the k “leading” eigenvectors corresponding to the k largest absolute eigenvalues of some symmetric matrix M ∈ Rd×d. Later, we solve the k-GenEV problem by setting M = B−1/2AB−1/2 and using LazyEV to find the k leading eigenvectors of M , which correspond to the k leading generalized eigenvectors of A with respect to B.\nOur algorithm LazyEV is formally stated in Algorithm 2. It applies k times AppxPCA±, each time with a multiplicative error δ×/2, and projects the matrix M into the orthogonal space with respect to the obtained leading eigenvector. We state our main approximation theorem below.\nTheorem 4.1 (approximation of LazyEV). Let M ∈ Rd×d be a symmetric matrix with eigenvalues λ1, . . . , λd ∈ [−1, 1] and corresponding eigenvectors u1, . . . , ud, and assume |λ1| ≥ · · · ≥ |λd|.\nFor every k ∈ [d], δ×, p ∈ (0, 1), there exists some εpca ≤ O ( poly(δ×,\n|λ1| |λk+1 , 1 d) ) such that2\nLazyEV outputs a (column) orthonormal matrix Vk = (v1, . . . , vk) ∈ Rd×k which, with probability at least 1− p, satisfies all of the following properties. (Denote by Ms = (I − VsV >s )M(I − VsV >s ).)\n(a) Correlation guarantee: ‖V >k U‖2 ≤ ε, where U = (uj , . . . , ud) and j is the smallest index satisfying |λj | ≤ (1− δ×)‖Mk−1‖2. (b) Spectral norm guarantee: |λk+1| ≤ ‖Mk‖2 ≤ |λk+1|1−δ× . (c) Rayleigh quotient guarantee: (1− δ×)|λk| ≤ |v>kMvk| ≤ 11−δ× |λk|. (d) Schatten-q norm guarantee: for every q ≥ 1,\n‖Mk‖Sq ≤ (1 + δ×)2 (1− δ×)2 ( d∑\ni=k+1\nλqi\n)1/q = (1 + δ×)2\n(1− δ×)2 min V ∈Rd×k,V >V=I\n{ ‖(I−V V >)M(I−V V >)‖Sq } .\nThe next theorem states that, if M = B−1/2AB−1/2, then LazyEV can be implemented without ever needing to compute B1/2 or B−1/2.\n2The complete specifications of εpca is included in Appendix E. Since our final running time only depends on log(1/εpca), we have not attempted to improve the constants in this polynomial dependency.\nTheorem 4.2 (running time of LazyEV). Let A,B ∈ Rd×d be two symmetric matrices satisfying B 0 and −B A B. Suppose M = B−1/2AB−1/2 and RanInit(d) is the random vector generator defined in Proposition 3.3 with respect to B. Then, the computation of V← B−1/2LazyEV(A,M, k, δ×, εpca, p) can be implemented to run in time\n• Õ ( knnz(B)+k2d+kΥ√\nδ×\n) where Υ is the time to multiply B−1A to a vector with error ε where\nlog(1/ε) = Õ(1), or\n• Õ ( k √ κBnnz(B)+knnz(A)+k\n2d√ δ×\n) if we use Conjugate gradient to multiply B−1A to a vector.\nAbove, the Õ notation hides polylogarithmic factors with respect to 1/εpca, 1/δ×, 1/p, κB, d."
    }, {
      "heading" : "4.1 Application to GenEV",
      "text" : "Our main theorems above imply the following corollaries.\nCorollary 4.3 (gap-dependent k-GenEV). Let A,B ∈ Rd×d be two symmetric matrices satisfying B 0 and −B A B. Suppose the generalized eigenvalue and eigenvector pairs of A with respect to B are {(λi, ui)}di=1, and it satisfies 1 ≥ |λ1| ≥ · · · ≥ |λd|. Let gap = |λk|−|λk+1| |λk| ∈ [0, 1] be the relative gap. For fixed ε, p > 0, consider the output\nVk ← B−1/2LazyEV ( A, B−1/2AB−1/2, k, gap, O ( ε4·gap k3(σ1/σk)4 ) , p ) ∈ Rd×k .\nThen, defining W = (uk+1, . . . , ud), we have with probability at least 1− p:\nV>k BVk = I and ‖V>k BW‖2 ≤ ε .\nMoreover, our running time is Õ ( k √ κBnnz(B)+knnz(A)+k\n2d√ gap\n)\nCorollary 4.4 (gap-free k-GenEV). Consider the same setting as Corollary 4.3. For fixed ε, p > 0, consider the output\nVk ← B−1/2LazyEV ( A, B−1/2AB−1/2, k, ε, O ( ε5\nk3d4(σ1/σk+1)12\n) , p ) .\nThen, we have with probability at least 1− p:\nV>k BVk = I and max w∈Rd∧w>BVk=0 w>Aw w>Bw ≤ 1 1− ε |λk+1|\nMoreover, our running time is Õ ( k √ κBnnz(B)+knnz(A)+k\n2d√ ε\n)"
    }, {
      "heading" : "5 High Level Ideas Behind Theorems 4.1 and 4.2",
      "text" : "Our LazyEV algorithm reduces the problem of finding generalized eigenvectors to finding regular eigenvectors of M = B−1/2AB−1/2. In Section 5.1 we discuss how to ensure accuracy: that is, why does LazyEV guarantee to find approximately the top absolute eigenvectors of M ; and in Section 5.2 we discuss how to implement LazyEV without ever needing to compute B1/2 or B−1/2."
    }, {
      "heading" : "5.1 Ideas Behind Theorem 4.1: Approximation Guarantee of GenEV",
      "text" : "Our approximation guarantee in Theorem 4.1 is a natural generalization of the recent work on fast iterative methods to find the top k eigenvectors of a PSD matrix M [2]. That method is called LazySVD. At a high level, LazySVD finds the top k eigenvectors of M one by one but only approximately. Starting with M0 = M , in the s-th iteration where s ∈ [k], LazySVD computes approximately the leading eigenvector of matrix Ms−1 and call it vs using shift-and-invert [9]. Then, LazySVD performs a projection Ms ← (I − vsv>s )Ms−1(I − vsv>s ) and proceeds to the next iteration.\nWhile the algorithmic idea of LazySVD is simple, the analysis requires some careful linear algebraic lemmas. Most notably, if vs is an approximate leading eigenvector of Ms−1, then one needs to prove that the small eigenvectors of Ms−1 somehow still “embed” into that of Ms after projection. This is achieved by a gap-free variant of the Wedin theorem plus a few other technical lemmas, and we recommend interested readers to see the high-level overview section of [2].\nIn this paper, to relax the assumption that M is PSD, and to find leading eigenvectors whose absolute eigenvalues are large, we have to make some non-trivial changes in the algorithm and the analysis. On the algorithm side, LazyEV replaces the use of the shift-and-invert protocol in LazySVD with our two-sided variant developed in Section 3. On the analysis side, we have to make sure all lemmas properly deal with negative eigenvalues: for instance, if we perform a projection M ′ ← (I−vv>)M(I−vv>) where v correlates by at most ε with all eigenvectors ofM whose absolute eigenvalues are smaller than a threshold µ, then, after the projection, we need to prove that these eigenvectors can be approximately “embedded” into the eigenspace spanned by all eigenvectors of M ′ whose absolute eigenvalues are smaller than µ+τ . The approximation of this embedding should depend on ε, µ and τ . See Lemma C.4 in the appendix.\nThe detailed proof of Theorem 4.1 is included in Appendix E, and the matrix algebraic lemmas are included in Appendix C."
    }, {
      "heading" : "5.2 Proof of Theorem 4.2: Fast Implementation of GenEV",
      "text" : "We can implement LazyEV efficiently without the necessity of computing B1/2 or B−1/2. In each iteration of LazyEV, we call AppxPCA± and compute a vector v′s. We do not explicitly store v ′ s, but rather write it as v′s = B 1/2v′s and store only v ′ s ∈ Rd. We shall later ensure that AppxPCA± outputs v′s directly. Similarly, we also write vs = B 1/2vs and only store vs. All together, we do not explicitly compute Vs, but instead write Vs = B 1/2Vs and only keep track of Vs ∈ Rd×s.\nNow, the computation of vs becomes the B-projection into the Vs−1 space:\n‖(I − Vs−1V >s−1)v′s‖ = ‖B1/2v′s −B1/2Vs−1V>s−1Bv′s‖ = (( v′s −Vs−1V>s−1Bv′s )> B ( v′s −Vs−1V>s−1Bv′s ))1/2 ‖(I − Vs−1V >s−1)v′s‖ · vs = B−1/2(I − Vs−1V >s−1)v′s = B−1/2(I − Vs−1V >s−1)B1/2v′s = v′s −Vs−1V>s−1Bv′s\nand this can be implemented to run in O(kd+ nnz(B)) time. Finally, we write\nMs = (I − VsV >s )B−1/2AB−1/2(I − VsV >s ) = B−1/2(I −BVsV>s )A(I −VsV>s B)B−1/2\nand only pass it implicity to AppxPCA± (without directly computing this matrix). To implement AppxPCA±, we again write all vectors ŵt = B1/2wt and only store wt. Thus, the normalization wa ← ŵm1/‖ŵm1‖2 becomes the B-normalization wa ← wm1/(w>m1Bwm1)1/2 which runs in O(nnz(B)) time. Recall that AppxPCA± makes a polylogarithmic number of calls to the matrix inversion subroutine A, each time requesting to approximately invert either λI −Ms or\nλI +Ms. Let us only focus on inverting λI −Ms and the other case is similar. We write\nN def = B−1/2 ( λI −Ms ) B1/2 = λI − (I − V̂sV̂ >s B)B−1A(I − V̂sV̂ >s B) .\nNow, the accuracy requirement in AppxPCA± becomes\nfind wt satisfying ‖B1/2wt − (λI −Ms)−1B1/2wt−1‖ ≤ ε̃ ⇐⇒ find wt satisfying ‖B1/2wt −B1/2N−1wt−1‖ ≤ ε̃ ⇐= find wt satisfying ‖wt −N−1wt−1‖ ≤ ε̃/ √ λmax(B)\nUsing Theorem D.1, we can reduce this approximate inversion wt ← N−1wt−1 to T times of approximate matrix-vector multiplication (i.e., w′ ← Nw) for T = Õ( √ κ(λI −Ms)).3 We can\nfurther derive that T = Õ(1/ √ δ×) owing to Theorem 3.1. Notice that Theorem D.1 implies that each time we compute w′ ← Nw it suffices to compute it to an additive accuracy ‖w′ −Nw‖ ≤ ε where the error satisfies log(1/ε) = Õ(1).\nFinally, the matrix-vector multiplication Nw = λw− (I− V̂sV̂ >s B)B−1A(I− V̂sV̂ >s B)w consists of two rank-s B-projections which run in time O(nnz(B) + kd), plus the time needed to multiply B−1A to a vector. This finishes the proof that LazyEV can be implemented so that\n• It computes matrix-vector multiplication of the form w′ ← B−1Aw a total of Õ(k/ √ δ×)\ntimes, each time to an accuracy ε where log(1/ε) = Õ(1);\n• The rest of the computation costs a total of Õ ( (knnz(B) + k2d)/ √ δ× ) time.\nThis finishes the proof of the first half of the theorem. As for the second item, we simply notice that whenever we want to compute w′ ← B−1Aw, we can first compute Aw in time O(nnz(A)), and then use Conjugate gradient [22] to compute B−1 applied to this vector. The running time of Conjugate gradient is at most Õ (√ κB · nnz(B) ) where the Õ factor hides a logarithmic factor on the accuracy."
    }, {
      "heading" : "6 Main Algorithm for Canonical Correlation Analysis",
      "text" : "In this section, we propose LazyCCA (see Algorithm 3), a variant of LazyEV that is specially designed for matrices M of the form M = B−1/2AB−1/2, where A and B come from a CCA problem following Lemma 2.3.\nMore specifically, recall from Lemma 2.3 that the eigenvectors of matrices M arising from CCA instances are symmetric: if (ξ, ζ) is a normalized eigenvector of M with eigenvalue σ where ξ ∈ Rdx , ζ ∈ Rdy , then (−ξ, ζ) is also a normalized eigenvector but with eigenvalue −σ. Furthermore, since (ξ, ζ) is orthogonal to (−ξ, ζ), we must have ‖ξ‖ = ‖ζ‖ = 1/ √ 2. Our LazyCCA method is designed to ensure such symmetry and orthogonality as well. When an approximate eigenvector vs = (ξ ′ s, ζ ′ s) is obtained, we re-scale the pair to ξs and ζs where both of them have norm exactly 1/ √ 2 (see Line 7 of LazyCCA). Then, we simultaneously add two (orthogonal) approximate eigenvectors (ξs, ζs) and (−ξs, ζs) to the column orthonormal matrix Vs.\n3This reduction would be obvious if we required the matrix-vector multiplication to be exact, and for instance Chebyshev method serves for exactly this purpose. However, in order to relax the multiplication to be approximate, and without incurring an error that blows up exponentially with T , we build our own inexact variant of the accelerate gradient descent method AGDinexact in Appendix D that could be of independent interest.\nAlgorithm 3 LazyCCA(A,M, k, δ×, εpca, p) Input: A, an approximate matrix inversion method;\nM ∈ Rd×d, a matrix satisfying −I M I; k ∈ [d], the desired rank; δ× ∈ (0, 1), a multiplicative error; εpca ∈ (0, 1), a numerical accuracy parameter; and p ∈ (0, 1), a confidence parameter.\n1: M0 ←M ; 2: V0 = []; 3: for s = 1 to k do 4: v′s ← AppxPCA±(A,Ms−1, δ×/2, εpca, p/k); 5: vs ← ( (I − Vs−1V >s−1)v′s ) / ∥∥(I − Vs−1V >s−1)v′s\n∥∥; project v′s to V ⊥s−1 6: write vs = (ξ ′ s, ζ ′ s) where ξ ′ s ∈ Rdx and ζ ′s ∈ Rdy ;\n7: ξs ← ξ′s/( √ 2‖ξ′s‖2) and ζs ← ζ ′s/( √\n2‖ζ ′s‖2); 8: Vs ← [ Vs−1,\n( ξs −ξs ζs ζs )] ;\n9: Ms ← ( I − 2diag(ξsξ>s , ζsζ>s ) ) Ms−1 ( I − 2diag(ξsξ>s , ζsζ>s ) )\nor equivalently, Ms = (I − VsV >s )M(I − VsV >s ) 10: end for 11: return Vk.\nRemark 6.1. This re-scaling step, together with the fact that we find vector pairs one by one, allows us to provide per-vector guarantee on the obtained approximate correlation vectors (see Corollary 6.3). This is in contrast to CCALin which is based on subspace power method so can only find the subspace spanned by the top k correlation vectors but not distinguish them.\nOne can prove a similar approximation guarantee (see Theorem F.1) as compared with Theorem 4.1, and a similar running time guarantee (see Theorem F.2) as compared with Theorem 4.2. The main idea behind the “delta” between the proofs of Theorem F.1 and of Theorem 4.1 is to show that, after re-scaling, the vector (ξs, ζs) is also an approximate leading eigenvector of Ms−1 just like the one vs = (ξ ′ s, ζ ′ s) before scaling. More specifically, its Rayleigh quotient can only become better after scaling (see (F.3) in the appendix). We include the details in Appendix F, and state below the final statements on LazyCCA.\nCorollary 6.2 (gap-dependent k-CCA). Let X ∈ Rn×dx , Y ∈ Rn×dy be two matrices with canonicalcorrelation coefficients 1 ≥ σ1 ≥ · · ·σr ≥ 0 and the corresponding correlation vectors {(φi, ψi)}ri=1, where r = min{dx, dy}. Let gap = σk−σk+1σk ∈ [0, 1] be the relative gap, and define A = [[0, Sxy]; [S > xy, 0]] and B = diag(Sxx, Syy) following Definition 2.2. For every ε, p > 0, consider the output\n( ±φ′1 . . . ±φ′k ψ′1 . . . ψ ′ k ) def = Vk ← √ 2B−1/2LazyCCA ( A, B−1/2AB−1/2, ε, gap, O ( ε4·gap k3(σ1/σk)4 ) , p ) .\nThen, letting Vφ = (φ ′ 1, . . . , φ ′ k), Vψ = (ψ ′ 1, . . . , ψ ′ k), Wφ = (φk+1, φk+2, . . . ) and Wψ = (ψk+1, ψk+2, . . . ), we have with probability at least 1− p:\nVφ ∈ Rdx×k satisfies V>φ SxxVφ = I and ‖V>φ SxxWφ‖2 ≤ ε , Vψ ∈ Rdy×k satisfies V>ψSyyVψ = I and ‖V>ψSyyWψ‖2 ≤ ε .\nFurthermore, the running time is Õ ( k √ κnnz(X,Y )+k2d√\ngap\n) if we use Conjugate gradient to multiply\nB−1A to a vector, or Õ ( knnz(X,Y )·\n( 1+ √ κ′/n )\n+k2d√ gap\n) if we use Katyusha.\nCorollary 6.3 (gap-free k-CCA). In the same setting as Corollary 6.2, for every ε, p > 0, consider the output\n( ±φ′1 . . . ±φ′k ψ′1 . . . ψ ′ k ) = Vk ← √ 2B−1/2LazyCCA ( A, B−1/2AB−1/2, ε, gap, O ( ε4·gap k3(σ1/σk)4 ) , p ) .\nLetting Vφ = (φ ′ 1, . . . , φ ′ k) ∈ Rdx×k and Vψ = (ψ′1, . . . , ψ′k) ∈ Rdy×k, with probability at least 1− p,\n• V>φ SxxVφ = I, V>ψSyyVψ = I; • (1− ε)σi ≤ |φ′iSxyψi| ≤ (1 + ε)σi for every i ∈ [k]; and • maxφ∈Rdx ,ψ∈Rdy { φ>Sxyψ ∣∣∣ φ>SxxVφ = 0 ∧ ψ>SyyVψ = 0 } ≤ (1 + ε)σk+1 .\nFurthermore, the running time is Õ ( k √ κnnz(X,Y )+k2d√\nε\n) if we use Conjugate gradient to multiply\nB−1A to a vector, or Õ ( knnz(X,Y )·\n( 1+ √ κ′/n )\n+k2d√ ε\n) if we use Katyusha.\nFinally, we note that the Katyusha-based running times in Corollary 6.2 and 6.3 need to use Lemma 2.4 which gives an accelerated stochastic running time for multiplying B−1A to a vector.\nAppendix"
    }, {
      "heading" : "A Missing Miscellaneous Proofs",
      "text" : "Proof of Lemma 2.4. First of all, computing B−1Aw is equivalent to minimizing f(x) def= 12x >Bx− x>Aw. Suppose we write x = (x1, x2) and w = (w1, w2) where x1, w1 ∈ Rdx , x2, w2 ∈ Rdy , then one can rewrite f as f(x) = 12n ( ‖Xx1 − Y w2‖22 + ‖Xw1 − Y x2‖22 ) +C where C is a fixed constant. Therefore, one can also write\nf(x) = 1\n2n\nn∑\ni=1\n(〈Xi, x1〉 − 〈Yi, w2〉)2 + (〈Yi, x2〉 − 〈Xi, w1〉)2\nwhere each Xi is a row vector of X and Yi is a row vector of Y . In such a case, we observe that\n• f(x) is an average of 2n smooth functions, where function (〈Xi, x1〉 − 〈Yi, w2〉)2 is smooth with parameter 2‖Xi‖22 (meaning Hessian bounded by ‖Xi‖22 in spectral norm) and (〈Yi, x2〉− 〈Xi, w1〉)2 is smooth with parameter ‖Yi‖22. In other words, the average smoothness of these 2n functions is exactly (‖X‖2F + ‖Y ‖2F )/n = Tr(B).\n• f(x) is at least min{λmin(Sxx), λmin(Syy)} strongly convex, meaning Hessian lower bounded by this quantity.\nFor such reason, one can apply the convergence theorem of Katyusha [1] to find an additive ε̃ approximate minimizer of f(x) in time O ( nnz(X,Y ) · ( 1 + √ κ′/n ) log f(x\n0)−f(x∗) ε̃\n) where x0 is an\narbitrary starting vector fed into Katyusha and x∗ is the exact minimizer. If we choose x0 to be the zero vector, it is easy to verify that f(x0)− f(x∗) ≤ O(λmax(B) · ‖w‖2).\nFinally, it is not hard to see that an additive ε̃ minimizer of f(x) implies an ε-approximate solution for the inverse ‖w′ − B−1Aw‖ ≤ ε where ε2 = 2ε̃/λmin(B). This finishes the proof of Lemma 2.4. Proof of Proposition 3.3. We have\n(u>w)2 = Tr(uu>vBv>)\nv>Bv\n¬ ≥ Tr(uu >vBv>) λmax(B)  ≥ λmin(B) · Tr(uu >vv>) λmax(B) = θ(u>v)2 .\nAbove, ¬ is because v>Bv ≤ λmax(B) · ‖v‖22 = λmax(B), and  follows from the fact that vBv> v ( λmin(B)I ) v> = λmin(B)vv>. Finally, using for instance [5, Lemma 5], it holds with probability at least 1− p that (u>v)2 ≥ p29d ."
    }, {
      "heading" : "B Proof Details for Section 3: Two-Sided Shift-and-Invert",
      "text" : "B.1 Inexact Power Method\nIn this subsection we review some classical convergence lemmas regarding power method and its inexact variant. These lemmas almost directly follow from previous results such as [9, 11], and are more similar to [2]. We skip the proofs in this paper.\nConsider power method that starts with a random unit vector w0 ← RanInit(d) and apply wt ←Mwt−1/‖Mwt−1‖ iteratively.\nLemma B.1 (Exact Power Method). Let M be a PSD matrix with eigenvalues λ1 ≥ · · · ≥ λd and the correpsonding eigenvectors u1, . . . , ud. Fix an error tolerance ε > 0, parameter κ ≥ 1, and failure probability p > 0, define\nTPM(κ, ε, p) = ⌈κ 2 log (9dθ p2ε )⌉\nThen, with probability at least 1− p it holds that ∀t ≥ TPM(κ, ε, p): ∑\ni∈[d],λi≤(1−1/κ)λ1 (w>t ui) 2 ≤ ε and w>t Mwt ≥ (1− 1/κ− ε)λ1 .\nLemma B.2 (Lemma 4.1 of [9]). Let M be a PSD matrix with eigenvalues λ1 ≥ · · ·λd. Fix an accuracy parameter ε̃ > 0, and consider two update sequences\nŵ∗0 = w0, ∀t ≥ 1: ŵ∗t ←Mŵ∗t−1 ŵ0 = w0, ∀t ≥ 1: ŵt satisfies ‖ŵt −Mŵt−1‖ ≤ ε̃,\nThen, defining wt = ŵt/‖ŵt‖ and w∗t = ŵ∗t /‖ŵ∗t ‖, it satisfies\n‖wt − w∗t ‖ ≤ ε̃ · Γ(M, t),\nwhere\nΓ(M, t) def =\n2\nλtd { t, if λ1 = 1; (λt1 − 1)/(λ1 − 1), if λ1 6= 1. and we have Γ(M, t) ≤ 2t · max{1, λ t 1} λtd\nTheorem B.3 (Inexact Power Method). Let M be a PSD matrix with eigenvalues λ1 ≥ · · · ≥ λd and the corresponding eigenvectors u1, . . . , ud. With probability at least 1−p it holds that, for every ε ∈ (0, 1) and every t ≥ TPM(κ, ε/4, p), if wt is generated by the power method with per-iteration error ε̃ = ε4Γ(M,t) , then\n∑\ni∈[d],λi≤(1−1/κ)λ1 (w>t ui) 2 ≤ ε and w>t Mwt ≥ (1− 1/κ− ε)λ1 ."
    }, {
      "heading" : "B.2 Proof of Theorem 3.1",
      "text" : "We prove Theorem 3.1 by first showing the following lemma. Most of these properties are analogous to their original variants in [9, 10], but here we take extra care also on negative eigenvalues and thus allowing M to be non-PSD.\nLemma B.4 (useful properties of AppxPCA±). With probability at least 1 − p, it holds that (by letting λ∗ = ‖M‖2):\n(a) ε̃1 ≤ 132Γ((λ(s−1)I−M)−1,m1) and ε̃1 ≤ 1 32Γ((λ(s−1)I+M)−1,m1) for each iteration s ≥ 1; (b) ε̃2 ≤ ε4Γ((λ(f)I−M)−1,m2) and ε̃2 ≤ ε 4Γ((λ(f)I+M)−1,m2) when the repeat-until loop is over; (c) 0 ≤ 34(λ(s−1) − λ∗) ≤ ∆(s) ≤ λ(s−1) − λ∗ and 12(λ(s−1) − λ∗) ≤ λ(s) − λ∗ for each iteration s ≥ 1; and (d) λ(f) − λ∗ ∈ [ δ×48 λ(f), δ× 13 λ ∗] when the repeat-until loop is over. (e) when the repeat-until loop is over,\nif w>a va ≥ w>b vb then λ(f) − λmax(M) ≤ 10 3 (λ(f) − λ∗); or if w>a va ≤ w>b vb then λ(f) + λmin(M) ≤ 10\n3 (λ(f) − λ∗) .\nProof. We denote by C(s) def = (λ(s)I −M)−1 and by D(s) def= (λ(s)I +M)−1 for notational simplicity. Below we prove all the items by induction for a specific iteration s ≥ 2 assuming that the items of the previous s− 1 iterations are true. The base case of s = 1 can be verified similar to the general arguments after some notational changes. We omitted the proofs of the base case s = 1.\n(a) Recall that\nΓ(C(s−1), t) ≤ 2t · max{1, λmax(C (s−1))t} λmin(C(s−1))t and Γ(D(s−1), t) ≤ 2t · max{1, λmax(D (s−1))t} λmin(D(s−1))t\nOn one hand, we have λmax(C (s−1)) = 1 λ(s−1)−λ∗ ≤ 2 λ(s−2)−λ∗ ≤ 2 ∆(s−1) using Lemma B.4.c of the previous iteration. Combining this with the termination criterion ∆(s−1) ≥ δ×12 λ(s−1), we have λmax(C\n(s−1)) ≤ 24 δ×λ(s−1) . On the other hand, we have λmin(C (s−1)) = 1 λ(s−1)−λmin(M) ≥ 1\nλ(s−1)+λ∗ ≥ 12λ(s−1) . Combining the two bounds we conclude that Γ(C (s−1), t) ≤ 2t(48/δ×)t. It is now obvious that ε̃1 ≤ 132Γ(C(s−1),m1) is satisfied because ε̃1 = 1 64m1 ( δ× 48 )m1 . Similarly, on one hand, we have λmax(D (s−1)) = 1\nλ(s−1)+λmin(M) ≤ 1 λ(s−1)−λ∗ ≤ 2 λ(s−2)−λ∗ ≤ 2 ∆(s−1) using Lemma B.4.c of the previous iteration. Combining this with the termination criterion ∆(s−1) ≥ δ×12 λ(s−1), we have λmax(D(s−1)) ≤ 24δ×λ(s−1) . On the other hand, we have λmin(D\n(s−1)) = 1 λ(s−1)+λmax(M) ≥ 1 λ(s−1)+λ∗ ≥ 12λ(s−1) . Combining the two bounds we conclude that Γ(D(s−1), t) ≤ 2t(48/δ×)t. It is now obvious that ε̃1 ≤ 132Γ(D(s−1),m1) is satisfied.\n(b) The same analysis as in the proof of Lemma B.4.a suggests that Γ(C(f), t) ≤ 2t(48/δ×)t and Γ(D(f), t) ≤ 2t(48/δ×)t. These immediately imply ε̃2 ≤ ε4Γ(C(f),m2) and ε̃2 ≤ ε 4Γ(D(f),m2)\nbecause ε̃2 = ε\n8m2\n( δ× 48 )m2\n(c) Because Lemma B.4.a holds for the current iteration s we can apply Theorem B.3 (with ε = 1/16 and κ = 16) and get\nw>a C (s−1)wa ≥\n7 8 λmax(C (s−1)) and w>b D (s−1)wb ≥ 7 8 λmax(D (s−1)) .\nBy the definition of v in AppxPCA± and the Cauchy-Schwartz inequality it holds that\nw>a va = w > a C (s−1)wa + w>a ( va − C(s−1)wa ) ∈ [ w>a C (s−1)wa − ε̃1, w>a C(s−1)wa + ε̃1 ] , and w>b vb = w > b D (s−1)wb + w > b ( vb −D(s−1)wb ) ∈ [ w>b D (s−1)wb − ε̃1, w>b D(s−1)wb + ε̃1 ] .\nCombining the above equations we have\nw>a va − ε̃1 ∈ [7\n8 λmax(C\n(s−1))− 2ε̃1, λmax(C(s−1)) ]\n⊆ [3\n4 λmax(C\n(s−1)), λmax(C(s−1)) ] = [3 4 , 1 ] · 1 λ(s−1) − λmax(M) , and\nw>b vb − ε̃1 ⊆ [3\n4 λmax(D\n(s−1)), λmax(D(s−1)) ] = [3 4 , 1 ] · 1 λ(s−1) + λmin(M) . (B.1)\nIn other words, ∆(s) def = 34 · 1max{w>a va,w>b wb}−ε̃1 ∈\n[ 3 4(λ (s−1) − λ∗), λ(s−1) − λ∗ ] because λ∗ =\nmax{λmax(M),−λmin(M)}. At the same time, our update rule λ(s) = λ(s−1) −∆(s)/2 ensures that λ(s) − λ∗ = λ(s−1) − λ∗ −∆(s)/2 ≥ λ(s−1) − λ∗ − λ(s−1)−λ∗2 = 12(λ(s−1) − λ∗).\n(d) The upper bound holds because λ(f)−λ∗ = λ(f−1)− ∆(f)2 −λ∗ ≤ ( 4 3 − 12 ) ∆(f) ≤ 5δ×λ(f)72 where\nthe first inequality follows from Lemma B.4.c of this last iteration, and the second inequality follows from our termination criterion ∆(f) ≤ δ×λ(f)12 . Simply rewriting this inequality we have λ(f) − λ∗ ≤ 5δ×/721−5δ×/72λ ∗ < δ×13 λ ∗.\nThe lower bound is because using Lemma B.4.c (of this and the previous iteration) we have λ(f) − λ∗ ≥ 14 ( λ(f−2) − λ∗ ) ≥ ∆(f−1)4 ¬ ≥ δ×λ(f−1)48 ≥ δ×λ(f) 48 . Here, inequality ¬ is because ∆(f−1) > δ×λ (f−1)\n12 due to the termination criterion.\n(e) We only prove the case when w>a va ≥ w>b vb and the other case is similar. We compute that\nλ(f) − λmax(M) = λ(f−1) − λmax(M)− ∆(f)\n2\n¬ ≤ 4 3 (λ(f−1) + λmin(M))−\n∆(f)\n2\n= 4\n3 (λ(f) + λmin(M)) +\n∆(f)\n2\n ≤ 4\n3 (λ(f) + λmin(M)) +\nδ×λ(f)\n24 ® ≤ 4\n3 (λ(f) + λmin(M)) + 2(λ\n(f) − λ∗) ¯ ≤ 10 3 (λ(f) − λ∗) .\nAbove, ¬ is from (B.1) together with the fact that w>a va ≥ w>b vb;  is using the termination criterion ∆(f) ≤ δ×λ(f)12 ; ® is from Lemma B.4.d\nFinally, since the success of Theorem B.3 only depends on the randomness of ŵ0, we have that with probability at least 1− p all the above items are satisfied.\nWe are now ready to prove Theorem 3.1.\nProof of Theorem 3.1. We only focus on the case when sgn = + and the other case is similar. It follows from Theorem B.3 (with κ = 2) that, letting µi = 1/(λ\n(f)−λi) be the i-th largest eigenvalue of the matrix (λ(f)I −M)−1, then\n∑\ni∈[d],µi≤µ1/2 (w>ui)2 ≤ ε .\nNote that if an index i ∈ [d] satisfies λ∗ − λi ≥ δ×2 λ∗, then we must have λ∗ − λi ≥ 132 (λ(f) − λ∗) owing to λ(f) − λ∗ ≤ δ×13 λ∗ from Lemma B.4.d. This further implies that λ(f) − λi ≥ 152 (λ(f) − λ∗). Plugging in Lemma B.4.e we further have λ(f) − λi ≥ 152 · 310(λ(f) − λ1) > 2(λ(f) − λ1). Using the definition of µi, we must have µ1/2 > µi. In sum, we also have\n∑\ni∈[d],λi≤(1−δ×/2)λ∗ (w>ui)2 ≤ ε .\nOn the other hand,\nw>Mw = d∑\ni=1\nλi(w >ui)2 ≥ −ελ∗ +\n∑\ni∈[d],λi>(1−δ×/2)λ∗ λi(w\n>ui)2\n≥ −ελ∗ + (1− δ×/2)λ∗ · ∑\ni∈[d],λi>(1−δ×/2)λ∗ (w>ui)2\n≥ −ελ∗ + (1− δ×/2)(1− ε)λ∗ ≥ (1− δ×/2)(1− 3ε)λ∗ .\nThe number of oracle calls to A is determined by the number of iterations in the repeat-until loop. It is easy to verify that there are at most O(log(1/δ×)) such iteartions, so the total number of oracle calls to A is only O(log(1/δ×)m1 +m2).\nAs for the condition number, each time we call A we have\nλmax(λ (s)I −M)\nλmin(λ(s)I −M) ≤ λ (s) − λd λ(s) − λ1 ≤ λ (s) + λ∗ λ(s) − λ∗ ≤ 2λ(s) λ(s) − λ∗ and λmax(λ (s)I +M) λmin(λ(s)I +M) ≤ 2λ (s) λ(s) − λ∗\nIf s = 0 then we have λ (0) λ(0)−λ∗ ≤ 1+δ× δ× because λ∗ ≤ 1. If s ≤ f − 2 then we have λ(s) λ(s)−λ∗ ≤ λ(s) ∆(s+1) ≤\nλ(s)\nδ×λ(s+1)/12 ≤ 12δ× where the first inequality follows from Lemma B.4.c, the second inequality follows from the stopping criterion, and the third inequality follows from the monotonicity of λ(s). If s = f − 1 then we have λ(s) λ(s)−λ∗ ≤ 2λ(s) λ(s−1)−λ∗ ≤ 2λ(s) ∆(s) ≤ 2λ(s) δ×λ(s)/12 = 24δ× where the first two inequalities follow from Lemma B.4.c and the third inequality follows from our stopping criterion. If s = f then we have λ (s)\nλ(s)−λ∗ ≤ 48 δ×\nowing to Lemma B.4.d. In all cases we have λmax(λ (s)I−M)\nλmin(λ(s)I−M) ≤ 96 δ× and\nsimilarly for λ(s)I +M ."
    }, {
      "heading" : "C Lemmas Needed for Proving Our Main Theorem",
      "text" : "In this section we provide some necessary lemmas on matrix algebra that shall become essential for our proof of Theorem 4.1. Many of these lemmas are analogous to those ones used in the SVD algorithm by the same authors of this paper [2], however, we need some extra care in this paper because the underlying matrix M is no longer PSD.\nProposition C.1. Let A,B be two (column) orthonormal matrix such that for η ≥ 0,\nA>BB>A (1− η)I\nThen we have: there exists a matrix Q, ‖Q‖2 ≤ 1 such that\n‖A−BQ‖2 ≤ √ η\nProof. Since A>A = I and A>BB>A (1− η)I, we know that A>B⊥(B⊥)>A ηI. By the fact that\nA = (BB> +B⊥(B⊥)>)A = BB>A+B⊥(B⊥)>A\nwe can let Q = B>A and obtain\n‖A−BQ‖2 ≤ ‖B⊥(B⊥)>A‖2 ≤ √ η ."
    }, {
      "heading" : "C.1 Approximate Projection Lemma",
      "text" : "The next lemma states that, projecting a symmetric matrix M into the orthogonal space of Vs ∈ Rd×s is almost equivalent to projecting it into the orthogonal space of Qs ∈ Rd×s, if Qs is the projection of Vs into the orthogonal space of U but ‖V >s U‖ is small. This lemma is obvious if “small” means zero correlation: if Vs were completely orthogonal to U then Qs would equal to Vs, so projecting M into the orthogonal space of Vs would be equivalent to that of Qs. However, even in the inexact scenario, this argument is true.\nLemma C.2. Let M be a symmetric matrix with (not necessarily sorted) eigenvalues λ1, . . . , λd and the corresponding (normalized) eigenvectors u1, . . . , ud ∈ Rd. For every k ≥ 1, define U⊥ = (u1, . . . , uk) ∈ Rd×k and U = (uk+1, . . . , ud) ∈ Rd×(d−k). For every ε ∈ (0, 12), let Vs ∈ Rd×s be a column orthogonal matrix such that ‖V >s U‖2 ≤ ε, define Qs ∈ Rd×s to be an arbitrary orthogonal basis of the column span of U⊥(U⊥)>Vs, then we have:\n∥∥∥ ( I −QsQ>s ) M ( I −QsQ>s ) − ( I − VsV >s ) M ( I − VsV >s )∥∥∥ 2 ≤ 13ε‖M‖2 .\nProof of Lemma C.2. Since Qs is an orthogonal basis of the column span of U ⊥(U⊥)>Vs, there is a matrix R ∈ Rs×s such that Qs = U ⊥(U⊥)>VsR\nUsing the fact that Q>s Qs = I, we have:\n(U⊥(U⊥)>VsR)>(U⊥(U⊥)>VsR) = I =⇒ R>V >s U⊥(U⊥)>VsR = I .\nBy the fact that V >s Vs = I and U ⊥(U⊥)> + UU> = I, we can rewrite the above equality as:\nR> ( I − V >s UU>Vs ) R = I (C.1)\nFrom our lemma assumption, we have: ‖V >s U‖2 ≤ ε, which implies 0 V >s UU>Vs ε2I. Putting this into (C.1), we obtain:\nI R>R 1 1− ε2 I\n( 1 + 4 3 ε2 ) I\nThe above inequality directly implies that I RR> ( 1 + 43ε 2 ) I. Therefore,\n∥∥∥QsQ>s − VsV >s ∥∥∥\n2 = ∥∥∥U⊥(U⊥)>VsRR>V >s U⊥(U⊥)> − VsV >s ∥∥∥ 2\n= ∥∥∥U⊥(U⊥)>VsRR>V >s U⊥(U⊥)> − (U⊥(U⊥)> + UU>)VsV >s (U⊥(U⊥)> + UU>) ∥∥∥ 2\n≤ ∥∥∥U⊥(U⊥)>Vs(RR> − I)V >s U⊥(U⊥)> ∥∥∥ 2 + ∥∥∥UU>VsV >s UU> ∥∥∥ 2 + 2 ∥∥∥U⊥(U⊥)>VsV >s UU> ∥∥∥ 2 ≤ ∥∥∥RR> − I\n∥∥∥ 2 + ∥∥∥U>VsV >s U ∥∥∥ 2 + 2 ∥∥∥V >s UU>Vs ∥∥∥ 1/2 2\n≤ 4 3 ε2 + ε2 + 2ε < 19 6 ε .\nFinally, we have\n∥∥∥ ( I −QsQ>s ) M ( I −QsQ>s ) − ( I − VsV >s ) M ( I − VsV >s )∥∥∥ 2\n≤ 2 ∥∥∥ ( QsQ > s − VsV >s ) M ∥∥∥ 2 + ∥∥∥ ( QsQ > s − VsV >s ) MQsQ > s ∥∥∥ 2 + ∥∥∥ ( QsQ > s − VsV >s ) MVsV > s ∥∥∥ 2 ≤ 19× 4 6 ε‖M‖2 < 13ε‖M‖2 ."
    }, {
      "heading" : "C.2 Gap-Free Wedin Theorem",
      "text" : "Lemma C.3 (two-sided gap-free Wedin theorem). For ε ≥ 0, let A,B be two symmetric matrices such that ‖A − B‖2 ≤ ε. For every µ ≥ 0, τ > 0, let U be column orthonormal matrix consisting of eigenvectors of A with absolute eigenvalues ≤ µ, let V be column orthonormal matrix consisting of eigenvectors of B with absolute eigenvalues ≥ µ+ τ , then we have:\n‖U>V ‖ ≤ ε τ .\nProof of Lemma C.3. We write A and B in terms of eigenvalue decomposition:\nA = UΣU> + U ′Σ′U ′> and B = V Σ̃V > + V ′Σ̃′V ′> ,\nwhere U ′ is orthogonal to U and V ′ is orthogonal to V . Letting R = A−B, we obtain:\nΣU> = U>A = U>(B +R)\n=⇒ ΣU>V = U>BV + U>RV = U>V Σ̃ + U>RV =⇒ ΣU>V Σ̃−1 = U>V + U>RV Σ̃−1 .\nTaking spectral norm on both sides, we obtain:\n‖Σ‖2‖U>V ‖2‖Σ̃−1‖2 ≥ ‖ΣU>V Σ̃−1‖2 ≥ ‖U>V ‖2 − ‖U>RV Σ̃−1‖2 .\nThis can be simplified to µ\nµ+ τ ‖U>V ‖2 ≥ ‖U>V ‖2 −\nε\nµ+ τ ,\nand therefore we have ‖U>V ‖2 ≤ ετ as desired."
    }, {
      "heading" : "C.3 Eigenvector Projection Lemma",
      "text" : "Our next technical lemma studies the projection of a matrix M into the orthogonal direction of a vector v, where v has little correlation with M ’s leading eigenvectors below some threshold µ (denoted by U). The conclusion of the lemma says that, after the projection, if we study the leading eigenvectors of M ′ = (I − vv>)M(I − vv>) below some threshold µ+ τ and denote it by V1, then U approximately embeds into V1, meaning that although V1 could be of a larger dimension of U , however, there exists a matrix Q with spectral norm no more than 1 such that ‖U−V1Q‖2 is small.\nLemma C.4. Let M ∈ Rd×d be a symmetric matrix with eigenvalues λ1, . . . , λd and corresponding eigenvectors u1, . . . , ud. Suppose |λ1| ≥ · · · ≥ |λd|. Define U = (uj+1, . . . , ud) ∈ Rd×(d−j) to be the matrix consisting of all eigenvectors with absolute eigenvalues ≤ µ. Let v ∈ Rd be a unit vector such that ‖v>U‖2 ≤ ε ≤ 1/2, and define\nM ′ = ( I − vv> ) M ( I − vv> ) .\nThen, denoting by [V2, V1, v] ∈ Rd×d the unitary matrix consisting of (column) eigenvectors of M ′, where V1 consists of eigenvectors with absolute eigenvalue ≤ µ+ τ , then there exists a matrix Q with spectral norm ‖Q‖2 ≤ 1 such that\n‖U − V1Q‖2 ≤ √\n169ε2‖M‖22 τ2 + ε2 .\nProof of Lemma C.4. Using Lemma C.2, let q = U ⊥(U⊥)>v\n‖U⊥(U⊥)>v‖2 be the projection of v to U ⊥, we\nknow that ∥∥∥ ( I − qq> ) M ( I − qq> ) − ( I − vv> ) M ( I − vv> )∥∥∥ 2 ≤ 13ε‖M‖2 .\nDenote ( I − qq> ) M ( I − qq> ) as M ′′. We know that uj+1, . . . , ud are still eigenvectors of M ′′\nwith eigenvalue λj+1, . . . , λd. Apply Lemma C.3 on A = M ′′, U and B = M ′, V = V2, we obtain:\n‖U>V2‖2 ≤ 13ε‖M‖2\nτ .\nThis implies that\nU>V1V >1 U = I − U>V2V >2 U − U>vv>U (\n1− 169ε 2‖M‖22 τ2\n− ε2 ) I ,\nwhere the inequality uses the assumption ‖v>U‖2 ≤ ε. Apply Proposition C.1 to A = U and B = V1, we conclude that there exists a matrix Q, ‖Q‖2 ≤ 1 such that\n‖U − V1Q‖2 ≤ √\n169ε2‖M‖22 τ2 + ε2 ."
    }, {
      "heading" : "D Matrix Inversion via Approx Accelerated Gradient Descent",
      "text" : "Given a positive definite matrix N, it is well-known that one can reduce the (approximate) matrix inversion problem N−1χ to multiple computations of the matrix-vector multiplication (i.e., of the form w′ ← Nw). In particular, Chebyshev method [6] uses the so-called Chebyshev polynomial for this purpose, and the number of matrix-vector multiplications is determined by the degree of that polynomial.\nIn this section, we revisit this problem by allowing matrix-vector multiplications to be computed only approximately. We emphasize that this is not a simple task in general. If matrix inversion is reduced to T matrix-vector multiplications, then a standard analysis implies that each of these multiplications must be computed up to a very small error 2−Ω(T ). If the actual matrix-vector multiplication subroutine has a logarithmic dependency on the error in its running time, then we will have a total running time at least quadratically dependent on T .4\nTo avoid such an exponentially accuracy loss, we abandon known results (such as Chebyshev method) and design our own method. We prove the following theorem in this section:\nTheorem D.1. Given a positive definite matrix N, we can reduce the problem of computing ξ ← N−1χ to multiple computations w′ ← Nw.\nMore specifically, if N satisfies N = B−1/2NB1/2 where N and B are both d×d positive definite matrices, for every ε̃ > 0 and χ ∈ Rd, in order to obtain ξ satisfying ‖ξ −N−1χ‖ ≤ ε̃, • it suffices to compute w′ ← Nw only Õ (√ κ(N) ) times, and\n• each time of accuracy ‖w′ −Nw‖ ≤ O(1/poly(κB, ε̃, λmin(N))). Our reduction is based on an inexact variant of the accelerated gradient descent (AGD) method originally put forward by Nesterov [20], which relies on some convex optimization techniques and can be proved using the linear-coupling framework [3]. We prove this inexact AGD result in Appendix D.1. Our final proof of Theorem D.1 is included in Appendix D.2.\n4Indeed, for instance in the ALS algorithm of [24] for solving CCA, the authors obtained a running time proportional to 1/gap2 although there are only 1/gap iterations.\nAlgorithm 4 AGDinexact(f, x0, T )\nInput: f an L-smooth and σ-strongly convex function; x0 some initial point; and T the number of iterations. Output: yT .\n1: τ ← 2 1+ √ 8L/σ+1 , η ← 1τL . τ = O( √ σ√ L ) and η = O( 1√ σL ) 2: y0 ← x0, z0 ← x0. 3: for k ← 0 to T − 1 do 4: xk+1 ← τzk + (1− τ)yk. 5: Compute approximate gradient ∇̃f(xk+1) satisfying ‖∇̃f(xk+1)−∇f(xk+1)‖2 ≤ ε̃. 6: yk+1 ← xk+1 − 1L∇̃f(xk+1) 7: zk+1 ← 11+ησ ( zk + ησxk+1 − η∇̃f(xk+1) ) 8: end for 9: return yT .\nD.1 Inexact Accelerated Gradient Descent\nWe study an inexact version of the classical accelerated gradient descent (AGD) method, and our pseudocode is presented in Algorithm 4. The difference between our method and known AGD methods is that we only require the algorithm to know an approximate gradient ∇̃f(xk+1) in each iteration k, as opposed to the exact full gradient ∇f(xk+1). We require ‖∇̃f(xk+1)−∇f(xk+1)‖2 to be upper bounded by some parameter ε̃ in each iteration. Our next convergence theorem states that this inexact AGD method only incurs an additive loss proportional to O(ε̃2).\nTheorem D.2 (inexact AGD). If f(x) is L-smooth and σ-strongly convex, then AGDinexact(f, x0, T ) produces an output yT satisfying\nf(yT )− f(x∗) ≤ O(1) · (1− τ)T (f(x0)− f(x∗)) +O ( ε̃2 σ ) ,\nwhere τ = Ω( √ σ/L). In other words, if the approximate gradient oracle satisfies ε̃ ≤ O(√εσ) and\nT = O( √ L/σ · log(1/ε)), then we have f(yT )− f(x∗) ≤ ε.\nTheorem D.2 can be proved using the linear-coupling framework of [3]. In this framework, accelerated methods are analyzed by a gradient descent lemma (Lemma D.3 below), a mirror descent lemma (Lemma D.4 below), and a coupling step (Lemma D.5 and D.6 below).\nLemma D.3 (gradient descent). f(yk+1) ≤ f(xk+1)− 12L‖∇f(xk+1)‖22 + ε̃ 2 2L .\nProof. Abbreviating xk+1 by x and yk+1 by y, the smoothness property of function f(·) tells us\nf(y)− f(x) ≤ 〈∇f(x), y − x〉+ L 2 ‖y − x‖2 .\nNow, since y − x = −∇f(x)+χL where ‖χ‖2 ≤ ε̃, we have\n〈∇f(x), y − x〉+ L 2 ‖y − x‖22 = −1 L 〈∇f(x),∇f(x) + χ〉+ 1 2L 〈∇f(x) + χ,∇f(x) + χ〉\n≤ − 1 2L ‖∇f(x)‖22 +\nε̃2 2L .\nSince our update on z can be written in the following minimization form, known as mirrordescent form in optimization literatures:\nz (i) k+1 = minz {1 2 ‖z − zk‖22 + η〈∇̃f(xk+1), z〉+ ησ 2 ‖z − xk+1‖22 } . (D.1)\nIt implies the following classical lemma (see for instance [4, Lemma 5.4]):\nLemma D.4 (mirror descent). For every u ∈ Rn,\nη〈∇̃f(xk+1), zk+1 − u〉 − ησ\n2 ‖xk+1 − u‖22 ≤ −\n1 2 ‖zk − zk+1‖22 + 1 2 ‖zk − u‖22 − 1 + ησ 2 ‖zk+1 − u‖22 .\nThe following inequality is a nature linear combination of the two lemmas above:\nLemma D.5 (coupling 1). For every u ∈ Rn,\nη〈∇f(xk+1), zk − u〉 − ησ\n2 ‖u− xk+1‖22\n≤ η2L ( f(xk+1)− f(yk+1) ) + 1\n2 ‖zk − u‖22 −\n1 + ησ/2\n2 ‖zk+1 − u‖22 + ε̃2(\nη σ + η2 2 ) .\nProof. Combining Lemma D.3 and Lemma D.4 we deduce that for each i ∈ [n],\n〈η∇f(xk+1), zk − u〉 − ησ\n2 ‖xk+1 − u‖22\n≤ 〈η∇f(xk+1), zk − zk+1〉+ 〈η∇̃f(xk+1), zk+1 − u〉+ ε̃η‖zk+1 − u‖2 − ησ\n2 ‖xk+1 − u‖22\n¬ ≤ 〈η∇f(xk+1), zk − zk+1〉 − 1\n2 ‖zk − zk+1‖22 +\n1 2 ‖zk − u‖22 − 1 + ησ 2 ‖zk+1 − u‖22 + ε̃η‖zk+1 − u‖2\n ≤ η\n2\n2 ‖∇f(xk+1)‖22 +\n1 2 ‖zk − u‖22 − 1 + ησ/2 2 ‖zk+1 − u‖22 +\nε̃2η\nσ ® ≤ η2L ( f(xk+1)− f(yk+1) ) + 1\n2 ‖zk − u‖22 −\n1 + ησ/2\n2 ‖zk+1 − u‖22 + ε̃2(\nη σ + η2 2 ) .\nAbove, ¬ uses Lemma D.4,  uses the Young’s inequality which states 2〈a, b〉 ≤ ‖a‖2 + ‖b‖2, ® uses Lemma D.3.\nTaking into account xk+1 = τzk + (1 − τ)yk and the convexity of f(·), we can rewrite some terms of Lemma D.5 and obtain\nLemma D.6 (coupling 2).\n0 ≤ (1− τ)η τ (f(yk)−f(x∗))− η τ (f(yk+1)−f(x∗))+ 1 2 ‖zk−x∗‖22− 1 + ησ/2 2 ‖zk+1−x∗‖22+ ε̃2( η σ + η2 2 )\nProof.\nη(f(xk+1)− f(x∗)) ¬ ≤ η〈∇f(xk+1), xk+1 − x∗〉 − ησ\n2 ‖x∗ − xk+1‖22\n= η〈∇f(xk+1), xk+1 − zk〉+ η〈∇f(xk+1), zk − x∗〉 − ησ 2 ‖x∗ − xk+1‖22\n = (1− τ)η τ 〈∇f(xk+1), yk − xk+1〉+ η〈∇f(xk+1), zk − x∗〉 − ησ 2 ‖x∗ − xk+1‖22 ® ≤ (1− τ)η\nτ (f(yk)− f(xk+1)) + η2L\n( f(xk+1)− f(yk+1) ) + 1\n2 ‖zk − u‖22 −\n1 + ησ/2\n2 ‖zk+1 − u‖22 + ε̃2 (η σ + η2 2 ) .\nAbove, ¬ is owing to the strong convexity of f(·),  uses the fact that xk+1 = τzk + (1− τ)yk, and ® uses the convexity of f(·) as well as Lemma D.5 with the choice of u = x∗. Recall η = 1τL , we arrive at the desired inequality.\nWe are now ready to prove Theorem D.2.\nProof of Theorem D.2. We choose τ = 2 1+ √ 8L/σ+1 ∈ [0, 1), and this choice ensures that 1 +ησ/2 =\n1 1−τ . Under these parameter choices, Lemma D.6 becomes ( f(yk+1)−f(x∗) ) + τ\n2η(1− τ)‖zk+1−x ∗‖22 ≤ (1−τ)\n( (f(yk)−f(x∗))+\nτ 2η(1− τ)‖zk−x ∗‖22 ) +ε̃2τ ( 1 σ + η 2 )\nTelescoping it for all iterations k = 0, 1, . . . , T − 1, we conclude that f(yT )−f(x∗) ≤ (1−τ)T ( f(y0)−f(x∗)+ τ\n2η ‖z0−x∗‖22\n) +ε̃2( 1\nσ + η 2 ) ≤ O(1)·(1−τ)T (f(x0)−f(x∗))+O ( ε̃2 σ ) .\nwhere the last inequality is because (i) x0 = y0 = z0, (ii) τ/η = O(σ) and (iii) the strong convexity of f(·) which implies f(x0)− f(x∗) ≥ σ2 ‖x0 − x∗‖22."
    }, {
      "heading" : "D.2 Proof of Theorem D.1",
      "text" : "Proof of Theorem D.1. We first verify accuracy. Since\n‖ξ −N−1χ‖ ≤ ε̃⇐= ‖B1/2ξ −B1/2N−1χ‖ ≤ ε̃ · √ λmin(B)\n⇐= ‖B1/2ξ −N−1B1/2χ‖ ≤ ε̃ · √ λmin(B) , (D.2)\nit suffices to find ξ to satisfy (D.2) in order to satisfy the accuracy requirement ‖ξ −N−1χ‖ ≤ ε̃. Define f(x)\ndef = 12x\n>Nx− ( B1/2χ )> x and let x∗ be its minimizer. Then it satisfies x∗ = N−1B1/2χ\nand f(x) − f(x∗) = 12(x − x∗)>N(x − x∗). For this reason, it suffices to find an approximate minimizer of f(x) satisfying\n1 2 (x− x∗)>N(x− x∗) = f(x)− f(x∗) ≤ ε̃\n2\n2 λmin(B)λmin(N) =: ε̃\n′ (D.3)\nbecause if we let ξ = B−1/2x then the above inequality implies 12‖x− x∗‖2 ≤ ε̃ 2 2 · λmin(B) which is the same as (D.2). In sum, we can call AGDinexact to find an approximate minimizer x with additive error no more than ε̃′, and then defining ξ = B−1/2x gives a solution of ξ satisfying ‖ξ−N−1χ‖ ≤ ε̃.\nWe now focus on the actual implementation of AGDinexact. If we choose x0 = 0 as the initial vector, we can write xk, yk, zk implicitly as xk = B 1/2xk, yk = B 1/2yk, zk = B\n1/2yk (thus only keep track of xk,yk, zk) throughout the algorithm. Under these notations, we claim that it suffices to perform matrix vector multiplication on N (i.e., of the form w′ ← Nw) for at most O(T ) times on those implicit vectors where T = O (√ λmax(N)/λmin(N) log(1/ε̃ ′) )\nis the number of iterations of AGDinexact according to Theorem D.2.\nThis is so because ∇f(xk) = Nxk−B1/2χ = B1/2 ( Nxk−χ ) and therefore for instance yk+1 ← xk+1 − 1L∇̃f(xk+1) can be implemented as yk+1 ← xk+1 − 1L(Nxk+1 − χ) so only matrix-vector multiplication on N is needed. In addition, as long as each w′ ← Nw is computed to an additive error ‖w′−Nw‖ ≤ O ( ε̃ ·λmin(N) √ λmin(B)/λmax(B) ) , we can use B1/2(w′−χ) as the approximate\ngradient which is different from the true gradient ∇f(xk) by an additive amount O( √ λmin(N)ε̃′). This satisfies the approximation require of Theorem D.2, and thus the accuracy guarantee provided by Theorem D.2 is satisfied."
    }, {
      "heading" : "E Proof Details for Section 4: GenEV Theorems",
      "text" : ""
    }, {
      "heading" : "E.1 Proof of Theorem 4.1",
      "text" : "In this section we prove Theorem 4.1 formally.\nTheorem 4.1 (restated). Let M ∈ Rd×d be a symmetric matrix with eigenvalues λ1, . . . , λd ∈ [−1, 1] and corresponding eigenvectors u1, . . . , ud. Suppose without loss of generality that |λ1| ≥ · · · ≥ |λd|.\nSuppose k ∈ [d], δ×, p ∈ (0, 1). Then, LazyEV outputs a (column) orthonormal matrix Vk = (v1, . . . , vk) ∈ Rd×k which, with probability at least 1 − p, satisfies all of the following properties. (Denote by Mk = (I − VkV >k )M(I − VkV >k ).)\n(a) Core lemma: if εpca ≤ ε 4δ× 212k3(|λ1|/|λk|)2 , then ‖V > k U‖2 ≤ ε, where U = (uj , . . . , ud) is the\n(column) orthonormal matrix and j is the smallest index satisfying |λj | ≤ (1− δ×)‖Mk−1‖2. (b) Spectral norm guarantee: if εpca ≤ δ 5 ×\n228k3(|λ1|/|λk+1|)6 , then |λk+1| ≤ ‖Mk‖2 ≤ |λk+1| 1−δ× .\n(c) Rayleigh quotient guarantee: if εpca ≤ δ 5 × 228k3(|λ1|/|λk+1|)6 , then (1 − δ×)|λk| ≤ |v > kMvk| ≤\n1 1−δ× |λk|.\n(d) Schatten-q norm guarantee: for every q ≥ 1, if εpca ≤ δ 5 ×\n228k3d4/q(|λ1|/|λk+1|)6 , then\n‖Mk‖Sq ≤ (1 + δ×)2 (1− δ×)2 ( d∑\ni=k+1\nλqi\n)1/q = (1 + δ×)2\n(1− δ×)2 min V ∈Rd×k,V >V=I\n{ ‖(I−V V >)M(I−V V >)‖Sq } .\nProof of Theorem 4.1. Let Vs = (v1, . . . , vs), so we can write Ms = (I − VsV >s )M(I − VsV >s ) = (I − vsv>s )Ms−1(I − vsv>s ) . We first claim that ‖Ms−1‖2 ≥ |λs| for every s = 1, . . . , k. This can be proved by the Cauchy interlacing theorem. Indeed, M2s−1 = (I − Vs−1V >s−1)M2(I − Vs−1V >s−1) is a projection of M2 into a d − s + 1 dimensional space, and therefore its largest eigenvalue ‖M2s−1‖2 should be at least as large as |λs|2, the s-th largest eigenvalue of M2. In other words, we have shown ‖Ms−1‖2 ≥ |λs|.\n(a) Define λ̂ = ‖Mk−1‖2 ≥ |λk|. Note that all column vectors in Vs are automatically eigenvectors of Ms with eigenvalues zero. For analysis purpose only, let Ws be the column matrix of eigenvectors in V ⊥ s of Ms that have\nabsolute eigenvalues in the range [0, (1− δ× + τs)λ̂], where τs def= s2kδ×. We now show that for every s = 0, . . . , k, there exists a matrix Qs such that ‖U −WsQs‖2 is small and ‖Qs‖2 ≤ 1. We will do this by induction.\nIn the base case: since τ0 = 0, we have W0 = U by the definition of U . We can therefore define Q0 to be the identity matrix. For every s = 0, 1, . . . , k − 1, suppose there exists a matrix Qs with ‖Qs‖2 ≤ 1 that satisfies ‖U −WsQs‖2 ≤ ηs for some ηs > 0, we construct Qs+1 as follows. First we observe that AppxPCA± outputs a vector v′s+1 satisfying ‖v′>s+1Ws‖22 ≤ εpca and ‖v′>s+1Vs‖22 ≤ εpca with probability at least 1 − p/k. This follows from Theorem 3.1 (using M = Ms) because [0, (1− δ× + τs)λ̂] ⊆ [0, (1− δ×/2)λ̂], together with the fact that ‖Ms‖2 ≥ ‖Mk−1‖2 ≥ λ̂. Now, since vs+1 is the projection of v′s+1 into V ⊥s , we have\n‖v>s+1Ws‖22 ≤ ‖v′>s+1Ws‖22\n‖(I − VsV >s )v′s+1‖22 = ‖v′>s+1Ws‖22 1− ‖V >s v′s+1‖22 ≤ εpca 1− εpca < 1.5εpca . (E.1)\nNext we apply Lemma C.4 with M = Ms, M ′ = Ms+1, U = Ws, V = Ws+1, v = vs+1, µ = (1− δ× + τs)λ̂, and τ = (τs+1 − τs)λ̂. We obtain a matrix Q̃s, ‖Q̃s‖2 ≤ 1 such that5\n‖Ws −Ws+1Q̃s‖2 ≤ √\n169(λ1/λ̂)2 · 1.5εpca (τs+1 − τs)2 + εpca < 32λ1k\n√ εpca\nλkδ× ,\nand this implies that\n‖Ws+1Q̃sQs − U‖2 ≤ ‖Ws+1Q̃sQs −WsQs‖2 + ‖WsQs − U‖2 ≤ ηs + 32λ1k\n√ εpca\nλkδ× .\nLet Qs+1 = Q̃sQs we know that ‖Qs+1‖2 ≤ 1 and\n‖Ws+1Qs+1 − U‖2 ≤ ηs+1 def= ηs + 32λ1k\n√ εpca\nλkδ× .\nTherefore, after k-iterations of LazyEV, we obtain:\n‖WkQk − U‖2 ≤ ηk = 32λ1k 2√εpca λkδ×\nMultiply U> from the left, we obtain ‖U>WkQk − I‖2 ≤ ηk. Since ‖Qk‖2 ≤ 1, we must have σmin(U\n>Wk) ≥ 1− ηk (here σmin denotes the smallest singular value). Therefore, U>WkW > k U (1− ηk)2I .\nSince Vk and Wk are orthogonal of each other, we have\nU>VkV > k U U>(I −WkW>k )U I − (1− ηk)2I 2ηkI\nTherefore,\n‖V >k U‖2 ≤ 8 (|λ1|/|λk|)1/2kε1/4pca\nδ 1/2 ×\n≤ ε .\n(b) The statement is obvious when k = 0. For every k ≥ 1, the lower bound is obvious. We prove the upper bound by contradiction. Suppose that ‖Mk‖2 > |λk+1|1−δ× . Then, since ‖Mk−1‖2 ≥ ‖Mk‖2 and therefore |λk+1|, . . . , |λd| < (1 − δ×)‖Mk−1‖2, we can apply Theorem 4.1.a of the current k to deduce that ‖V >k U>k‖2 ≤ ε where U>k def = (uk+1, . . . , ud). We now apply\nLemma C.2 with Vs = Vk and U = U>k, we obtain a matrix Qk ∈ Rd×k whose columns are spanned by u1, . . . , uk and satisfy∥∥∥ ( I −QkQ>k ) M ( I −QkQ>k ) − ( I − VkV >k ) M ( I − VkV >k )∥∥∥ 2 < 16|λ1|ε . However, our assumption says that the second matrix ( I − VkV >k ) M ( I − VkV >k ) has spectral\nnorm at least |λk+1|/(1−δ×), but we know that ( I −QkQ>k ) M ( I −QkQ>k ) has spectral norm exactly |λk+1| due to the definition of Qk. Therefore, we must have |λk+1|1−δ× − |λk+1| ≤ 16|λ1|ε due to triangle inequality. In other words, by selecting ε in Theorem 4.1.a to satisfy ε ≤ δ×16|λ1|/|λk+1| (which is satisfied by our assumption on εpca), we get a contradiction so can conclude that ‖Mk‖2 ≤ |λk+1|1−δ× .\n5Technically speaking, to apply Lemma C.4 we need U = Ws to consist of all eigenvectors of Ms with absolute eigenvalues ≤ µ. However, we only defined Ws to be such eigenvectors that are also orthogonal to Vs. It is straightforward to verify that the same result of Lemma C.4 remains true because vs+1 is orthogonal to Vs.\n(c) We compute that\n|v>kMvk| = |v>kMk−1vk| ¬ ≥ |v ′> k Mk−1v ′ k| ‖(I − Vk−1V >k−1)v′k‖22  ≥ |v ′> k Mk−1v ′ k|\n(1−√εpca)2 ® ≥ 1− εpca\n(1−√εpca)2 (1− δ×/2)‖Mk−1‖2 ≥ (1− δ×)‖Mk−1‖2 . (E.2)\nAbove, ¬ is because vk is the projection of v ′ k into V ⊥ k−1,  is because ‖V >k−1v′k‖22 ≤ εpca following the same reason as (E.1), and ® is owing to Theorem 3.1. Next, since ‖Mk−1‖2 ≥ |λk|, we automatically have |v>kMvk| ≥ (1−δ×)|λk|. On the other hand, |v>kMvk| = |v>kMk−1vk| ≤ ‖Mk−1‖2 ≤ |λk|1−δ× where the last inequality is owing to Theorem 4.1.b.\n(d) Since ‖V >k U‖2 ≤ εc def = 8 (|λ1|/|λk|)1/2kε1/4pca δ 1/2 × from the analysis of Theorem 4.1.a, we can apply\nLemma C.2 to obtain a (column) orthogonal matrix Qk ∈ Rd×k such that\n‖M ′k −Mk‖2 ≤ 16|λ1|εc, where M ′k def = (I −QkQ>k )M(I −QkQ>k ) (E.3)\nSuppose U = (ud−p+1, . . . , ud) is of dimension d×p, that is, there are exactly p eigenvalues of M whose absolute value is ≤ (1−δ×)‖Mk−1‖2. Then, the definition of Qk in Lemma C.2 tells us U>Qk = 0 so M ′k agrees with M on all the eigenvalues and eigenvectors {(λj , uj)}dj=d−p+1 because an index j satisfies |λj | ≤ (1−δ×)‖Mk−1‖2 if and only if j ∈ {d−p+1, d−p+2, . . . , d}. Denote by µ1, . . . , µd−k the eigenvalues of M ′k excluding the k zero eigenvalues in subspace Qk, and assume without loss of generality that {µ1, . . . , µp} = {λd−p+1, . . . , λd}. Then,\n‖M ′k‖qSq = d−k∑\ni=1\n|µi|q = p∑\ni=1\n|µi|q + d−k∑\ni=p+1\n|µi|q = d∑\ni=d−p+1 |λi|q +\nd−k∑\ni=p+1\n|µi|q\n¬ ≤\nd∑\ni=d−p+1 |λi|q + (d− k − p)‖M ′k‖q2  ≤\nd∑\ni=d−p+1 |λi|q + (d− k − p)(‖Mk‖2 + 16|λ1|εc)q\n® ≤\nd∑\ni=d−p+1 |λi|q + (d− k − p) ( |λk+1| (1− δ×) + 16|λ1|εc )q\nAbove, ¬ is because each |µi| is no greater than ‖M ′k‖2, and  is owing to (E.3), and ® is because of Theorem 4.1.b. Suppose we choose εc so that εc ≤ |λk+1|δ×16λ1 (and this is indeed satisfied by our assumption on εpca), then we can continue and write\n‖M ′k‖qSq ≤ d∑\ni=d−p+1 |λi|q + (d− k − p)\n(1 + δ×)q (1− δ×)q |λk+1|q\n¯ ≤\nd∑\ni=d−p+1 |λi|q +\n(1 + δ×)q (1− δ×)2q d−p∑\ni=k+1\n|λi|q ≤ (1 + δ×)q (1− δ×)2q d∑\ni=k+1\n|λi|q .\nAbove, ¯ is because for each eigenvalue λi where i ∈ {k + 1, k + 2, . . . , d − p}, we have\n|λi| > (1− δ×)‖Mk−1‖2 ≥ (1− δ×)|λk| ≥ (1− δ×)|λk+1|. Finally, using (E.3) again we have\n‖Mk‖Sq ≤ ‖M ′k‖Sq + ‖Mk −M ′k‖Sq ≤ ‖M ′k‖Sq + d1/p‖Mk −M ′k‖2\n≤ 1 + δ× (1− δ×)2\n( d∑\ni=k+1\n|λi|q )1/q + 16d1/p|λ1|εc\nAs long as εc ≤ δ×|λk+1|16d1/pλ1 , we have\n‖Mk‖Sq ≤ (1 + δ×)2 (1− δ×)2 ( d∑\ni=k+1\n|λi|q )1/q\nas desired. Finally, we note that εc ≤ δ×λk+116d1/pλ1 is satisfied with our assumption on εpca, and note that minV ∈Rd×k,V >V=I { ‖(I−V V >)M(I−V V >)‖Sq } = (∑d i=k+1 |λi|q )1/q which follows easily from Cauchy interlacing theorem."
    }, {
      "heading" : "E.2 Proofs of Corollaries 4.3 and 4.4",
      "text" : "Proof of Corollary 4.3. Define Vk = B 1/2Vk = LazyEV(· · · ) to be the direct output of LazyEV. It is clear from the definition of generalized eigenvectors that B1/2u1, . . . , B 1/2ud are eigenvectors of M def = B−1/2AB−1/2 with eigenvalues λ1, . . . , λd. Applying Theorem 4.1.a, we have: ‖V >k U‖2 ≤ ε where U = (B1/2uj , . . . , B 1/2ud) is a (column) orthonormal matrix and j is the smallest index satisfying |λj | ≤ (1− δ×)‖Mk−1‖2. Since it satisfies ‖Mk−1‖2 ≥ |λk|, we have\n|λk+1| = |λk|(1− gap) = |λk|(1− δ×) ≤ (1− δ×)‖Mk−1‖2 .\nTherefore, j must be equal to k+1 according to its definition, so we have U = B1/2W. This implies ‖V>k BW‖2 = ‖V >k U‖2 ≤ ε.\nThe running time of the algorithm comes directly from Theorem 4.2 by putting in the parameters. Proof of Corollary 4.4. Define Vk = B\n1/2Vk = LazyEV(· · · ) to be the direct output of LazyEV. It is clear from the definition of generalized eigenvectors that B1/2u1, . . . , B\n1/2ud are eigenvectors of M def = B−1/2AB−1/2 with eigenvalues λ1, . . . , λd. Applying Theorem 4.1.b, we have: ∥∥(I − VkV > k )B −1/2AB−1/2(I − VkV >k ) ∥∥ 2 ≤ |λk+1|1−ε .\nNext, for every vector w ∈ Rd that is B-orthogonal to Vk, that is, w>BVk = 0, we can define w def = B1/2w and we know w is orthogonal to Vk. We can apply the above spectral upper bound and get\nw>Aw = w>B−1/2AB−1/2w = w>(I − VkV >k )B−1/2AB−1/2(I − VkV >k )w\n≤ ‖w‖22 · |λk+1| 1− ε = w >Bw · |λk+1| 1− ε\nas desired. The running time of the algorithm comes directly from Theorem 4.2 by putting in the parameters."
    }, {
      "heading" : "F Proof Details for Section 6: CCA Theorems",
      "text" : ""
    }, {
      "heading" : "F.1 The Main Convergence Theorem",
      "text" : "Since LazyCCA only admits minor changes on top of LazyEV, the next theorem is an almost identical copy of Theorem 4.1. To make this paper concise, instead of reproving Theorem F.1 line by line, we here only sketch the main changes needed in the new proof.\nTheorem F.1. Let M = B−1/2AB−1/2 ∈ Rd×d be a symmetric matrix where A and B are matrices coming from a CCA instance using Lemma 2.3. Suppose M has eigenvalues λ1, . . . , λd ∈ [−1, 1] and corresponding eigenvectors u1, . . . , ud. Suppose without loss of generality that |λ1| ≥ · · · ≥ |λd|.\nFor every k ∈ [d], δ×, p ∈ (0, 1), there exists some εpca ≤ O ( poly(δ×,\n|λ1| |λk+1 , 1 d) ) such that LazyCCA\noutputs a (column) orthonormal matrix Vk = (v1, . . . , v2k) ∈ Rd×2k which, with probability at least 1− p, satisfies all of the following properties. (Denote by Ms = (I − VsV >s )M(I − VsV >s ).)\n(a) Correlation guarantee: ‖V >k U‖2 ≤ ε, where U = (uj , . . . , ud) and j is the smallest index satisfying |λj | ≤ (1− δ×)‖Mk−1‖2. (b) Spectral norm guarantee: |λ2k+1| ≤ ‖Mk‖2 ≤ |λ2k+1|1−δ× . (c) Rayleigh quotient guarantee: (1− δ×)|λ2k| ≤ |v>2k−1Mv2k−1| = |v>2kMv2k| ≤ 11−δ× |λ2k|. (d) Schatten-q norm guarantee: for every q ≥ 1,\n‖Mk‖Sq ≤ (1 + δ×)2 (1− δ×)2 ( d∑\ni=2k+1\nλqi\n)1/q = (1 + δ×)2\n(1− δ×)2 min V ∈Rd×2k,V >V=I\n{ ‖(I−V V >)M(I−V V >)‖Sq } .\nProof sketch of Theorem F.1. Recall that when a vector vs ∈ Rd is obtained in iteration s of LazyEV, the proof of Theorem 4.1 suggest that the following two properties hold\n∣∣v>s Ms−1vs ∣∣∣ ≥ (1− δ×)‖Ms−1‖2 and ∥∥v>s Ws−1 ∥∥∥ 2\n2 ≤ 1.5εpca . (F.1)\n(The first property is shown in (E.2), and the second property is shown in (E.1). Recall that Ws−1 is the column orthonormal matrix containing all eigenvectors of Ms−1 whose absolute eigenvalues are below some threshold.) Then, the proof of Theorem 4.1 proceeds by heavily relying on (F.1).\nIn our LazyCCA, after obtaining this same vector vs, we write it as vs = (ξ ′ s, ζ ′ v) and perform\nblock-scaling ξs = ξ ′ s/( √ 2‖ξ′s‖) and ζs = ζ ′s/( √\n2‖ζ ′s‖), see Line 7 of LazyCCA. Therefore, in order for the same proof of Theorem 4.1 to hold, we need to show that this new vector (ξs, ζs) satisfies the same properties up to constants:\n∣∣∣ ( ξs ζs )> Ms−1 ( ξs ζs )∣∣∣ ≥ (1− δ×)‖Ms−1‖2 and ∥∥∥ ( ξs ζs )> Ws−1 ∥∥∥ 2 2 ≤ 12.5εpca . (F.2)\nSuppose Vs−1 = ( ±ξ1 · · · ±ξs−1 ζ1 · · · ζs−1 ) . Since vs is orthogonal to all vectors in the column span of Vs−1 according to Line 5 of LazyCCA, we automatically have ξ>s ξi = 0 and ζ > s ζi for all i ∈ [s − 1]. We also have ‖ξs‖2 + ‖ζs‖2 = 1/2 + 1/2 = 1 so the new vector (ξs, ζs) has Euclidean norm 1. As for the first property in (F.2), we observe that the new vector (ξs, ζs) enjoys an (absolute) Rayleigh quotient value that is no worse than the original vs = (ξ ′ s, ζ ′ s). This is so because (without\nloss of generality we consider v>s Ms−1vs > 0):\n( ξs ζs )> Ms−1 ( ξs ζs ) ¬ = ( ξs ζs )> M ( ξs ζs )  = 2ξ>s ( S−1/2xx SxyS −1/2 yy ) ζs\n® =\n1\n‖ξ′s‖‖ζ ′s‖ ·ξ′>s\n( S−1/2xx SxyS −1/2 yy ) ζ ′s ¯ =\n1\n2‖ξ′s‖‖ζ ′s‖ ·v>s Mvs ° =\n1\n2‖ξ′s‖‖ζ ′s‖ ·v>s Ms−1vs ± ≥ v>s Ms−1vs .\n(F.3)\nAbove, ¬ is because (ξs, ζs) is orthogonal to Vs−1;  is by the definition of M = B−1/2AB−1/2 as well as the definition of A and B; ® is by the definitions of ξs and ζs; ¯ is by vs = (ξ ′ s, ζ ′ s) and again by the definition of M ; ° follows from the fact that vs is orthogonal to Vs−1; and ± follows from AM-GM together with the fact that ‖ξ′s‖2 + ‖ζ ′s‖2 = ‖vs‖2 = 1. This finishes proving the first property in (F.2) because the original vector vs satisfies |v>s Ms−1vs| ≥ (1− δ×)‖Ms−1‖2 according to (F.1).\nWe make an additional observation here: ‖ξ′s‖2 and ‖ζ ′s‖2 must be in the range [0.06, 0.94] before scaling. Indeed, suppose for instance ‖ξ′s‖2 = c for some c ∈ [0, 1]. Then, it satisfies 2‖ξ′s‖‖ζ ′s‖ = 2 √ c(1− c) and therefore (F.3) becomes\n( ξs ζs )> Ms−1 ( ξs ζs ) ≥ 1−δ× 2 √ c(1−c)\n‖Ms−1‖2, meaning that 1−δ×\n2 √ c(1−c)\n≤ 1. If δ× ≤ 1/2, this implies c− 1/2 ∈ [− √ 3/4, √\n3, 4] and thus c ∈ [0.06, 0.94]. As for the second property in (F.2), for every matrix Ws−1 that is in the proof of Theorem 4.1.a, its columns are all eigenvectors of Ms−1 whose absolute eigenvalues are below some threshold, so must consist of only symmetric vectors in this CCA setting: that is, Ws−1 = ( ±a1 . . . ±at\nb1 . . . bt\n) .6\nAccording to (F.1) we already know ‖v>s Ws−1‖2 ≤ 1.5εpca, which implies\n‖v>s Ws−1‖2 = t∑\ni=1\n(ξ′>s ai+ζ ′> s bi) 2+(ξ′>s ai−ζ ′>s bi)2 = 2‖ξ′>s (a1, . . . , at)‖2+2‖ζ ′>s (b1, . . . , bt)‖2 ≤ 1.5εpca .\nNow we can compute\n∥∥∥ ( ξs ζs )> Ws−1 ∥∥∥ 2 2 = 2‖ξ>s (a1, . . . , at)‖2 + 2‖ζ>s (b1, . . . , bt)‖2\n≤ 0.5 0.06\n· ( 2‖ξ′>s (a1, . . . , at)‖2 + 2‖ζ ′>s (b1, . . . , bt)‖2 ) ≤ 12.5εpca ,\nwhere the first inequality is because ‖ξ′s‖2, ‖ζ ′s‖2 ≥ 0.06. This finishes proving the two properties in (F.2), so Theorem F.1 holds after plugging the rest of the proof of Theorem 4.1 in but changing constants slightly."
    }, {
      "heading" : "F.2 Fast Implementation of CCA",
      "text" : "Theorem F.2 (running time of LazyCCA). Let X ∈ Rn×dx , Y ∈ Rn×dy be two matrices, and define A and B according to Lemma 2.3. Suppose M = B−1/2AB−1/2, and RanInit(d) is the random vector generator defined in Proposition 3.3, and we want to compute V← B−1/2LazyCCA(A,M, k, δ×, εpca, p). Then, this procedure can be implemented to run in time\n6This is because matrix Ms is always of the form D −1/2CD1/2 where D = diag{D1, D2} is block diagonal and positive definite, while C = [[0, C1]; [C > 1 , 0]] has only zero on its two block diagonal locations. The same proof of Lemma 2.3 shows that the eigenvectors of D−1/2CD1/2 must be symmetric. In fact, to be precise, Ws may also contain some eigenvectors corresponding to zero eigenvalues. However, adding them will make our notations heavier, so we refrain from doing that in this sketched proof.\n• Õ ( knnz(B)+k2d+kΥ√\nδ×\n) where Υ is the time needed to compute B−1Aw for a vector w to an\naccuracy ε where log(1/ε) = Õ(1), or\n• Õ ( knnz(X,Y ) √ κ+k2d√\nδ×\n) if we simply use Conjugate gradient to compute B−1Aw.\n• Õ ( knnz(X,Y )\n( 1+ √ κ′/n )\n+k2d√ δ×\n) if we simply use Katyusha to compute B−1Aw.\nAbove, κ = λmax(B)λmin(B) = max{λmax(Sxx),λmax(Syy)} min{λmin(Sxx),λmin(Syy)} , and κ ′ = Tr(B)λmin(B) ∈ [κ, dκ].\nProof. The proof of the first item is almost identical to the proof of Theorem 4.2 so ignored here. As for the second item, it follows from the first item together with the fact that applying matrix B−1 to a vector costs running time Õ(nnz(B) ·√κ) using conjugate gradient. As for the third item, it follows from the first item together with the running time of Katyusha in Lemma 2.4."
    }, {
      "heading" : "F.3 Proofs of Corollaries 6.2 and 6.3",
      "text" : "The two corollaries follow from Theorem F.1 for the similar reason as Corollary 4.3 and Corollary 4.4 following from Theorem 4.1.\nSketch Proof Corollary 6.2. The approximation guarantees ‖V>φ SxxWφ‖2 ≤ ε and ‖V>ψSyyWψ‖2 ≤ ε follow from Theorem F.1.a, and the running time follows from Theorem F.2. Sketch Proof of Corollary 6.3. The approximation guarantees maxφ∈Rdx ,ψ∈Rdy { φ>Sxyψ\n∣∣∣ φ>SxxVφ = 0 ∧ ψ>SyyVψ = 0 } ≤ (1 + ε)σk+1 follow from Theorem F.1.b and the definition of M , and the approximation guarantee (1− ε)σi ≤ |φ′iSxyψi| ≤ (1 + ε)σi follows from Theorem F.1.c and the fact that |λ2i−1| = |λ2i| = σi. The running time follows from Theorem F.2."
    } ],
    "references" : [ {
      "title" : "Katyusha: The First Direct Acceleration of Stochastic Gradient Methods",
      "author" : [ "Zeyuan Allen-Zhu" ],
      "venue" : "ArXiv e-prints,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2016
    }, {
      "title" : "Even Faster SVD Decomposition Yet Without Agonizing Pain",
      "author" : [ "Zeyuan Allen-Zhu", "Yuanzhi Li" ],
      "venue" : "ArXiv e-prints,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2016
    }, {
      "title" : "Linear coupling: An ultimate unification of gradient and mirror descent",
      "author" : [ "Zeyuan Allen-Zhu", "Lorenzo Orecchia" ],
      "venue" : "ArXiv e-prints,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2014
    }, {
      "title" : "Even faster accelerated coordinate descent using non-uniform sampling",
      "author" : [ "Zeyuan Allen-Zhu", "Peter Richtárik", "Zheng Qu", "Yang Yuan" ],
      "venue" : "In ICML,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2016
    }, {
      "title" : "Expander flows, geometric embeddings and graph partitioning",
      "author" : [ "Sanjeev Arora", "Satish Rao", "Umesh V. Vazirani" ],
      "venue" : "Journal of the ACM,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2009
    }, {
      "title" : "A survey of preconditioned iterative methods for linear systems of algebraic equations",
      "author" : [ "Owe Axelsson" ],
      "venue" : "BIT Numerical Mathematics,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 1985
    }, {
      "title" : "Multi-view clustering via canonical correlation analysis",
      "author" : [ "Kamalika Chaudhuri", "Sham M Kakade", "Karen Livescu", "Karthik Sridharan" ],
      "venue" : "In ICML,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2009
    }, {
      "title" : "Multi-view learning of word embeddings via cca",
      "author" : [ "Paramveer Dhillon", "Dean P Foster", "Lyle H Ungar" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2011
    }, {
      "title" : "Fast and simple PCA via convex optimization",
      "author" : [ "Dan Garber", "Elad Hazan" ],
      "venue" : "ArXiv e-prints,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2015
    }, {
      "title" : "Robust shift-and-invert preconditioning: Faster and more sample efficient algorithms for eigenvector computation",
      "author" : [ "Daniel Garber", "Elad Hazan", "Chi Jin", "Sham M. Kakade", "Cameron Musco", "Praneeth Netrapalli", "Aaron Sidford" ],
      "venue" : null,
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2016
    }, {
      "title" : "Efficient Algorithms for Large-scale Generalized Eigenvector Computation and Canonical Correlation Analysis",
      "author" : [ "Rong Ge", "Chi Jin", "Sham M. Kakade", "Praneeth Netrapalli", "Aaron Sidford" ],
      "venue" : "ArXiv e-prints,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2016
    }, {
      "title" : "Multi-view regression via canonical correlation analysis",
      "author" : [ "Sham M Kakade", "Dean P Foster" ],
      "venue" : "In Learning theory,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2007
    }, {
      "title" : "Discriminative features via generalized eigenvectors",
      "author" : [ "Nikos Karampatziakis", "Paul Mineiro" ],
      "venue" : "In ICML, pages 494–502,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2014
    }, {
      "title" : "Large scale canonical correlation analysis with iterative least squares",
      "author" : [ "Yichao Lu", "Dean P Foster" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2014
    }, {
      "title" : "Finding linear structure in large datasets with scalable canonical correlation analysis",
      "author" : [ "Zhuang Ma", "Yichao Lu", "Dean Foster" ],
      "venue" : "In ICML,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2015
    }, {
      "title" : "Finding linear structure in large datasets with scalable canonical correlation analysis",
      "author" : [ "Zhuang Ma", "Yichao Lu", "Dean Foster" ],
      "venue" : "In ICML,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2015
    }, {
      "title" : "Nonparametric canonical correlation analysis",
      "author" : [ "Tomer Michaeli", "Weiran Wang", "Karen Livescu" ],
      "venue" : "arXiv preprint,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2015
    }, {
      "title" : "Randomized block krylov methods for stronger and faster approximate singular value decomposition",
      "author" : [ "Cameron Musco", "Christopher Musco" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2015
    }, {
      "title" : "A method of solving a convex programming problem with convergence rate O(1/k2)",
      "author" : [ "Yurii Nesterov" ],
      "venue" : "In Doklady AN SSSR (translated as Soviet Mathematics Doklady),",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 1983
    }, {
      "title" : "A Stochastic PCA and SVD Algorithm with an Exponential Convergence Rate",
      "author" : [ "Ohad Shamir" ],
      "venue" : "In icml,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2015
    }, {
      "title" : "An introduction to the conjugate gradient method without the agonizing pain",
      "author" : [ "Jonathan Richard Shewchuk" ],
      "venue" : null,
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 1994
    }, {
      "title" : "Large-scale approximate kernel canonical correlation analysis",
      "author" : [ "Weiran Wang", "Karen Livescu" ],
      "venue" : "arXiv preprint,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2015
    }, {
      "title" : "Efficient Globally Convergent Stochastic Optimization for Canonical Correlation Analysis",
      "author" : [ "Weiran Wang", "Jialei Wang", "Dan Garber", "Nathan Srebro" ],
      "venue" : "ArXiv e-prints,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2016
    }, {
      "title" : "A penalized matrix decomposition, with applications to sparse principal components and canonical correlation analysis",
      "author" : [ "Daniela M Witten", "Robert Tibshirani", "Trevor Hastie" ],
      "venue" : "Biostatistics, page kxp008,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2009
    } ],
    "referenceMentions" : [ {
      "referenceID" : 11,
      "context" : "Algorithms solving these problems are often used to extract features to compare largescale datasets, as well as used for problems in regression [13], clustering [7], classification [14], word embeddings [8], and many others.",
      "startOffset" : 144,
      "endOffset" : 148
    }, {
      "referenceID" : 6,
      "context" : "Algorithms solving these problems are often used to extract features to compare largescale datasets, as well as used for problems in regression [13], clustering [7], classification [14], word embeddings [8], and many others.",
      "startOffset" : 161,
      "endOffset" : 164
    }, {
      "referenceID" : 12,
      "context" : "Algorithms solving these problems are often used to extract features to compare largescale datasets, as well as used for problems in regression [13], clustering [7], classification [14], word embeddings [8], and many others.",
      "startOffset" : 181,
      "endOffset" : 185
    }, {
      "referenceID" : 7,
      "context" : "Algorithms solving these problems are often used to extract features to compare largescale datasets, as well as used for problems in regression [13], clustering [7], classification [14], word embeddings [8], and many others.",
      "startOffset" : 203,
      "endOffset" : 206
    }, {
      "referenceID" : 14,
      "context" : "breakthrough result of Ma, Lu and Foster [16], they proposed to study algorithms to find top k generalized eigenvectors or top k canonical-correlation vectors.",
      "startOffset" : 41,
      "endOffset" : 45
    }, {
      "referenceID" : 17,
      "context" : "As three concrete examples: • Block Krylov method computes top eigenvectors with a running time linearly in 1/√gap rather than 1/gap [12], or with a gap-free running time that depends on 1/ √ ε rather than 1/gap or 1/ √ gap, where ε is the approximation error [19].",
      "startOffset" : 260,
      "endOffset" : 264
    }, {
      "referenceID" : 20,
      "context" : "• Conjugate gradient [22], Chebyshev method [6], and Nesterov’s method [20] compute B−1w for a vector w with a running time linearly in √ κ rather than κ, where κ is the condition number of matrix B.",
      "startOffset" : 21,
      "endOffset" : 25
    }, {
      "referenceID" : 5,
      "context" : "• Conjugate gradient [22], Chebyshev method [6], and Nesterov’s method [20] compute B−1w for a vector w with a running time linearly in √ κ rather than κ, where κ is the condition number of matrix B.",
      "startOffset" : 44,
      "endOffset" : 47
    }, {
      "referenceID" : 18,
      "context" : "• Conjugate gradient [22], Chebyshev method [6], and Nesterov’s method [20] compute B−1w for a vector w with a running time linearly in √ κ rather than κ, where κ is the condition number of matrix B.",
      "startOffset" : 71,
      "endOffset" : 75
    }, {
      "referenceID" : 10,
      "context" : "Indeed, two groups of authors independently attempted to answer such questions [11, 24].",
      "startOffset" : 79,
      "endOffset" : 87
    }, {
      "referenceID" : 22,
      "context" : "Indeed, two groups of authors independently attempted to answer such questions [11, 24].",
      "startOffset" : 79,
      "endOffset" : 87
    }, {
      "referenceID" : 10,
      "context" : "• For the k-GenEV problem, GenELin [11] improved the dependency of κ to √κ.",
      "startOffset" : 35,
      "endOffset" : 39
    }, {
      "referenceID" : 22,
      "context" : "In a separate work, SI [24] also obtained the √ κ dependency but only for the simpler k = 1 case; at the same time, SI enjoys a √ gap dependency but paying an additional factor λ1.",
      "startOffset" : 23,
      "endOffset" : 27
    }, {
      "referenceID" : 10,
      "context" : "The two groups of authors proposed three methods: CCALin [11], ALS [24] and SI [24].",
      "startOffset" : 57,
      "endOffset" : 61
    }, {
      "referenceID" : 22,
      "context" : "The two groups of authors proposed three methods: CCALin [11], ALS [24] and SI [24].",
      "startOffset" : 67,
      "endOffset" : 71
    }, {
      "referenceID" : 22,
      "context" : "The two groups of authors proposed three methods: CCALin [11], ALS [24] and SI [24].",
      "startOffset" : 79,
      "endOffset" : 83
    }, {
      "referenceID" : 10,
      "context" : "1-GenEV GenELin [11] Õ ( nnz(B)κB gap + nnz(A) gap ) no no",
      "startOffset" : 16,
      "endOffset" : 20
    }, {
      "referenceID" : 22,
      "context" : "SI [24] Õ ( nnz(B)κB √ gap·λ1 + nnz(A) √ gap·λ1 ) no no",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 14,
      "context" : "1-CCA AppGrad [16] nnz(X,Y ) · Õ ( κ gap ) no no CCALin [11] nnz(X,Y ) · Õ (√κ gap ) no no",
      "startOffset" : 14,
      "endOffset" : 18
    }, {
      "referenceID" : 10,
      "context" : "1-CCA AppGrad [16] nnz(X,Y ) · Õ ( κ gap ) no no CCALin [11] nnz(X,Y ) · Õ (√κ gap ) no no",
      "startOffset" : 56,
      "endOffset" : 60
    }, {
      "referenceID" : 22,
      "context" : "ALS [24] nnz(X,Y ) · Õ ( √κ gap2 ) no no",
      "startOffset" : 4,
      "endOffset" : 8
    }, {
      "referenceID" : 22,
      "context" : "SI [24] nnz(X,Y ) · Õ ( √κ √ gap·σ1 ) no no",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 10,
      "context" : "CCALin [11] nnz(X,Y ) · Õ ( 1+√κ′/n gap ) no yes",
      "startOffset" : 7,
      "endOffset" : 11
    }, {
      "referenceID" : 22,
      "context" : "ALS [24] nnz(X,Y ) · Õ ( 1+√κ′/n gap2 ) no yes",
      "startOffset" : 4,
      "endOffset" : 8
    }, {
      "referenceID" : 22,
      "context" : "SI [24] nnz(X,Y ) · Õ ( 1+ √ κ′/ √ n √ gap·σ1 ) no yes",
      "startOffset" : 3,
      "endOffset" : 7
    }, {
      "referenceID" : 0,
      "context" : "In GenEV, gap = λ1−λ2 λ1 ∈ [0, 1], λ1 ∈ [0, 1], and κB = λmax(B) λmin(B) > 1.",
      "startOffset" : 27,
      "endOffset" : 33
    }, {
      "referenceID" : 0,
      "context" : "In GenEV, gap = λ1−λ2 λ1 ∈ [0, 1], λ1 ∈ [0, 1], and κB = λmax(B) λmin(B) > 1.",
      "startOffset" : 40,
      "endOffset" : 46
    }, {
      "referenceID" : 0,
      "context" : "In CCA, gap = σ1−σ2 σ1 ∈ [0, 1], σ1 ∈ [0, 1], κ = λmax(diag{Sxx,Syy}) λmin(diag{Sxx,Syy}) > 1, and κ ′ = Tr(diag{Sxx,Syy}) λmin(diag{Sxx,Syy}) ∈ [κ, dκ].",
      "startOffset" : 25,
      "endOffset" : 31
    }, {
      "referenceID" : 0,
      "context" : "In CCA, gap = σ1−σ2 σ1 ∈ [0, 1], σ1 ∈ [0, 1], κ = λmax(diag{Sxx,Syy}) λmin(diag{Sxx,Syy}) > 1, and κ ′ = Tr(diag{Sxx,Syy}) λmin(diag{Sxx,Syy}) ∈ [κ, dκ].",
      "startOffset" : 38,
      "endOffset" : 44
    }, {
      "referenceID" : 10,
      "context" : "k-GenEV GenELin [11] Õ (knnz(B)√κB gap + knnz(A)+kd gap ) no no",
      "startOffset" : 16,
      "endOffset" : 20
    }, {
      "referenceID" : 14,
      "context" : "k-CCA AppGrad [16] Õ (knnz(X,Y )·κ+kd gap ) (local converge) no no CCALin [11] Õ (knnz(X,Y )·√κ+k2d gap ) no no",
      "startOffset" : 14,
      "endOffset" : 18
    }, {
      "referenceID" : 10,
      "context" : "k-CCA AppGrad [16] Õ (knnz(X,Y )·κ+kd gap ) (local converge) no no CCALin [11] Õ (knnz(X,Y )·√κ+k2d gap ) no no",
      "startOffset" : 74,
      "endOffset" : 78
    }, {
      "referenceID" : 10,
      "context" : "CCALin [11] Õ (knnz(X,Y )· ( 1+ √ κ′/n ) +kd gap ) no yes",
      "startOffset" : 7,
      "endOffset" : 11
    }, {
      "referenceID" : 0,
      "context" : "In GenEV, gap = λk−λk+1 λk ∈ [0, 1] and κB = λmax(B) λmin(B) > 1.",
      "startOffset" : 29,
      "endOffset" : 35
    }, {
      "referenceID" : 0,
      "context" : "In CCA, gap = σk−σk+1 σk ∈ [0, 1], κ = λmax(diag{Sxx,Syy}) λmin(diag{Sxx,Syy}) > 1, and κ ′ = Tr(diag{Sxx,Syy}) λmin(diag{Sxx,Syy}) ∈ [κ, dκ].",
      "startOffset" : 27,
      "endOffset" : 33
    }, {
      "referenceID" : 22,
      "context" : "In contrast, previous results for the k > 1 case rely on more sophisticated nonconvex optimization; and the previous work of [24] —although uses convex optimization to solve 1-CCA— requires one to work with a sum-of-non-convex function which is less efficient to minimize.",
      "startOffset" : 125,
      "endOffset" : 129
    }, {
      "referenceID" : 17,
      "context" : "For the easier problem of PCA and SVD, the first gap-free result was obtained by Musco and Musco [19], the first stochastic result was obtained by Shamir [21], and the first accelerated stochastic result was obtained by Garber et al.",
      "startOffset" : 97,
      "endOffset" : 101
    }, {
      "referenceID" : 19,
      "context" : "For the easier problem of PCA and SVD, the first gap-free result was obtained by Musco and Musco [19], the first stochastic result was obtained by Shamir [21], and the first accelerated stochastic result was obtained by Garber et al.",
      "startOffset" : 154,
      "endOffset" : 158
    }, {
      "referenceID" : 8,
      "context" : "[9, 10].",
      "startOffset" : 0,
      "endOffset" : 7
    }, {
      "referenceID" : 9,
      "context" : "[9, 10].",
      "startOffset" : 0,
      "endOffset" : 7
    }, {
      "referenceID" : 13,
      "context" : "As for GenEV and CCA, many scalable algorithms have been designed recently [15, 17, 18, 23, 25].",
      "startOffset" : 75,
      "endOffset" : 95
    }, {
      "referenceID" : 15,
      "context" : "As for GenEV and CCA, many scalable algorithms have been designed recently [15, 17, 18, 23, 25].",
      "startOffset" : 75,
      "endOffset" : 95
    }, {
      "referenceID" : 16,
      "context" : "As for GenEV and CCA, many scalable algorithms have been designed recently [15, 17, 18, 23, 25].",
      "startOffset" : 75,
      "endOffset" : 95
    }, {
      "referenceID" : 21,
      "context" : "As for GenEV and CCA, many scalable algorithms have been designed recently [15, 17, 18, 23, 25].",
      "startOffset" : 75,
      "endOffset" : 95
    }, {
      "referenceID" : 23,
      "context" : "As for GenEV and CCA, many scalable algorithms have been designed recently [15, 17, 18, 23, 25].",
      "startOffset" : 75,
      "endOffset" : 95
    }, {
      "referenceID" : 15,
      "context" : "Furthermore, for k > 1, the AppGrad result of [17] only provides local convergence guarantees and thus requires a warm-start whose computational complexity is not discussed in their paper.",
      "startOffset" : 46,
      "endOffset" : 50
    }, {
      "referenceID" : 0,
      "context" : ", σr satisfy σi = φ > i Sxyψi ∈ [0, 1].",
      "startOffset" : 32,
      "endOffset" : 38
    }, {
      "referenceID" : 0,
      "context" : "For every w ∈ Rd, Katyusha method [1] finds a vector w′ ∈ Rd satisfying ‖w′ −B−1Aw‖ ≤ ε",
      "startOffset" : 34,
      "endOffset" : 37
    }, {
      "referenceID" : 8,
      "context" : "3 Leading Eigenvector via Two-Sided Shift-and-Invert In this section we define AppxPCA±, the multiplicative approximation algorithm for computing the two-sided leading eigenvector of a symmetric matrix using the shift-and-invert preconditioning framework [9, 10].",
      "startOffset" : 255,
      "endOffset" : 262
    }, {
      "referenceID" : 9,
      "context" : "3 Leading Eigenvector via Two-Sided Shift-and-Invert In this section we define AppxPCA±, the multiplicative approximation algorithm for computing the two-sided leading eigenvector of a symmetric matrix using the shift-and-invert preconditioning framework [9, 10].",
      "startOffset" : 255,
      "endOffset" : 262
    }, {
      "referenceID" : 8,
      "context" : "Our pseudo-code Algorithm 1 is a modification of Algorithm 5 appeared in [9].",
      "startOffset" : 73,
      "endOffset" : 76
    }, {
      "referenceID" : 8,
      "context" : "The main differences between AppxPCA± and Algorithm 5 of [9] are two-fold.",
      "startOffset" : 57,
      "endOffset" : 60
    }, {
      "referenceID" : 8,
      "context" : "Second, we provide a multiplicative-error guarantee rather than additive as originally appeared in [9].",
      "startOffset" : 99,
      "endOffset" : 102
    }, {
      "referenceID" : 8,
      "context" : "1 Of course, we believe the bulk of the credit for conceiving AppxPCA± belongs to the original authors of [9, 10].",
      "startOffset" : 106,
      "endOffset" : 113
    }, {
      "referenceID" : 9,
      "context" : "1 Of course, we believe the bulk of the credit for conceiving AppxPCA± belongs to the original authors of [9, 10].",
      "startOffset" : 106,
      "endOffset" : 113
    }, {
      "referenceID" : 22,
      "context" : "This is why the only known CCA result using shift-and-invert preconditioning [24] depends on 1 gap·λ1 in Table 1.",
      "startOffset" : 77,
      "endOffset" : 81
    }, {
      "referenceID" : 0,
      "context" : "Let gap = |λk|−|λk+1| |λk| ∈ [0, 1] be the relative gap.",
      "startOffset" : 29,
      "endOffset" : 35
    }, {
      "referenceID" : 1,
      "context" : "1 is a natural generalization of the recent work on fast iterative methods to find the top k eigenvectors of a PSD matrix M [2].",
      "startOffset" : 124,
      "endOffset" : 127
    }, {
      "referenceID" : 8,
      "context" : "Starting with M0 = M , in the s-th iteration where s ∈ [k], LazySVD computes approximately the leading eigenvector of matrix Ms−1 and call it vs using shift-and-invert [9].",
      "startOffset" : 168,
      "endOffset" : 171
    }, {
      "referenceID" : 1,
      "context" : "This is achieved by a gap-free variant of the Wedin theorem plus a few other technical lemmas, and we recommend interested readers to see the high-level overview section of [2].",
      "startOffset" : 173,
      "endOffset" : 176
    }, {
      "referenceID" : 20,
      "context" : "As for the second item, we simply notice that whenever we want to compute w′ ← B−1Aw, we can first compute Aw in time O(nnz(A)), and then use Conjugate gradient [22] to compute B−1 applied to this vector.",
      "startOffset" : 161,
      "endOffset" : 165
    }, {
      "referenceID" : 0,
      "context" : "Let gap = σk−σk+1 σk ∈ [0, 1] be the relative gap, and define A = [[0, Sxy]; [S > xy, 0]] and B = diag(Sxx, Syy) following Definition 2.",
      "startOffset" : 23,
      "endOffset" : 29
    }, {
      "referenceID" : 0,
      "context" : "For such reason, one can apply the convergence theorem of Katyusha [1] to find an additive ε̃ approximate minimizer of f(x) in time O ( nnz(X,Y ) · ( 1 + √ κ′/n ) log f(x 0)−f(x∗) ε̃ ) where x0 is an arbitrary starting vector fed into Katyusha and x∗ is the exact minimizer.",
      "startOffset" : 67,
      "endOffset" : 70
    }, {
      "referenceID" : 8,
      "context" : "These lemmas almost directly follow from previous results such as [9, 11], and are more similar to [2].",
      "startOffset" : 66,
      "endOffset" : 73
    }, {
      "referenceID" : 10,
      "context" : "These lemmas almost directly follow from previous results such as [9, 11], and are more similar to [2].",
      "startOffset" : 66,
      "endOffset" : 73
    }, {
      "referenceID" : 1,
      "context" : "These lemmas almost directly follow from previous results such as [9, 11], and are more similar to [2].",
      "startOffset" : 99,
      "endOffset" : 102
    }, {
      "referenceID" : 8,
      "context" : "1 of [9]).",
      "startOffset" : 5,
      "endOffset" : 8
    }, {
      "referenceID" : 8,
      "context" : "Most of these properties are analogous to their original variants in [9, 10], but here we take extra care also on negative eigenvalues and thus allowing M to be non-PSD.",
      "startOffset" : 69,
      "endOffset" : 76
    }, {
      "referenceID" : 9,
      "context" : "Most of these properties are analogous to their original variants in [9, 10], but here we take extra care also on negative eigenvalues and thus allowing M to be non-PSD.",
      "startOffset" : 69,
      "endOffset" : 76
    }, {
      "referenceID" : 2,
      "context" : "w> a va − ε̃1 ∈ [7 8 λmax(C (s−1))− 2ε̃1, λmax(C(s−1)) ] ⊆ [3 4 λmax(C (s−1)), λmax(C(s−1)) ] = [3 4 , 1 ] · 1 λ(s−1) − λmax(M) , and",
      "startOffset" : 96,
      "endOffset" : 106
    }, {
      "referenceID" : 3,
      "context" : "w> a va − ε̃1 ∈ [7 8 λmax(C (s−1))− 2ε̃1, λmax(C(s−1)) ] ⊆ [3 4 λmax(C (s−1)), λmax(C(s−1)) ] = [3 4 , 1 ] · 1 λ(s−1) − λmax(M) , and",
      "startOffset" : 96,
      "endOffset" : 106
    }, {
      "referenceID" : 0,
      "context" : "w> a va − ε̃1 ∈ [7 8 λmax(C (s−1))− 2ε̃1, λmax(C(s−1)) ] ⊆ [3 4 λmax(C (s−1)), λmax(C(s−1)) ] = [3 4 , 1 ] · 1 λ(s−1) − λmax(M) , and",
      "startOffset" : 96,
      "endOffset" : 106
    }, {
      "referenceID" : 2,
      "context" : "w> b vb − ε̃1 ⊆ [3 4 λmax(D (s−1)), λmax(D(s−1)) ] = [3 4 , 1 ] · 1 λ(s−1) + λmin(M) .",
      "startOffset" : 53,
      "endOffset" : 63
    }, {
      "referenceID" : 3,
      "context" : "w> b vb − ε̃1 ⊆ [3 4 λmax(D (s−1)), λmax(D(s−1)) ] = [3 4 , 1 ] · 1 λ(s−1) + λmin(M) .",
      "startOffset" : 53,
      "endOffset" : 63
    }, {
      "referenceID" : 0,
      "context" : "w> b vb − ε̃1 ⊆ [3 4 λmax(D (s−1)), λmax(D(s−1)) ] = [3 4 , 1 ] · 1 λ(s−1) + λmin(M) .",
      "startOffset" : 53,
      "endOffset" : 63
    }, {
      "referenceID" : 1,
      "context" : "Many of these lemmas are analogous to those ones used in the SVD algorithm by the same authors of this paper [2], however, we need some extra care in this paper because the underlying matrix M is no longer PSD.",
      "startOffset" : 109,
      "endOffset" : 112
    }, {
      "referenceID" : 5,
      "context" : "In particular, Chebyshev method [6] uses the so-called Chebyshev polynomial for this purpose, and the number of matrix-vector multiplications is determined by the degree of that polynomial.",
      "startOffset" : 32,
      "endOffset" : 35
    }, {
      "referenceID" : 18,
      "context" : "Our reduction is based on an inexact variant of the accelerated gradient descent (AGD) method originally put forward by Nesterov [20], which relies on some convex optimization techniques and can be proved using the linear-coupling framework [3].",
      "startOffset" : 129,
      "endOffset" : 133
    }, {
      "referenceID" : 2,
      "context" : "Our reduction is based on an inexact variant of the accelerated gradient descent (AGD) method originally put forward by Nesterov [20], which relies on some convex optimization techniques and can be proved using the linear-coupling framework [3].",
      "startOffset" : 241,
      "endOffset" : 244
    }, {
      "referenceID" : 22,
      "context" : "Indeed, for instance in the ALS algorithm of [24] for solving CCA, the authors obtained a running time proportional to 1/gap although there are only 1/gap iterations.",
      "startOffset" : 45,
      "endOffset" : 49
    }, {
      "referenceID" : 2,
      "context" : "2 can be proved using the linear-coupling framework of [3].",
      "startOffset" : 55,
      "endOffset" : 58
    }, {
      "referenceID" : 0,
      "context" : "Indeed, suppose for instance ‖ξ′ s‖ = c for some c ∈ [0, 1].",
      "startOffset" : 53,
      "endOffset" : 59
    } ],
    "year" : 2017,
    "abstractText" : "We study k-GenEV, the problem of finding the top k generalized eigenvectors, and k-CCA, the problem of finding the top k vectors in canonical-correlation analysis. We propose algorithms LazyEV and LazyCCA to solve the two problems with running times linearly dependent on the input size and on k. Furthermore, our algorithms are doubly-accelerated : our running times depend only on the square root of the matrix condition number, and on the square root of the eigengap. This is the first such result for both k-GenEV or k-CCA. We also provide the first gap-free results, which provide running times that depend on 1/ √ ε rather than the eigengap.",
    "creator" : "LaTeX with hyperref package"
  }
}