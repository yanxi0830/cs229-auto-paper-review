{
  "name" : "1503.01673.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "High Dimensional Bayesian Optimisation and Bandits via Additive Models",
    "authors" : [ "Kirthevasan Kandasamy", "Jeff Schneider", "Barnabás Póczos" ],
    "emails" : [ "KANDASAMY@CS.CMU.EDU", "SCHNEIDE@CS.CMU.EDU", "BAPOCZOS@CS.CMU.EDU" ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "In many applications we are tasked with zeroth order optimisation of an expensive to evaluate function f in D dimensions. Some examples are hyper parameter tuning in expensive machine learning algorithms, experiment design, optimising control strategies in complex systems, and scientific simulation based studies. In such applications, f is a blackbox which we can interact with only by querying for the value at a specific point. Related to optimisation is the bandits problem arising in applications such as online advertising and reinforcement learning. Here the objective is to maximise the cumulative sum of all queries. In either case, we need to find the optimum of f using as few queries as possible by managing exploration and exploitation.\nProceedings of the 32nd International Conference on Machine Learning, Lille, France, 2015. JMLR: W&CP volume 37. Copyright 2015 by the author(s).\nBayesian Optimisation (Mockus & Mockus, 1991) refers to a suite of methods that tackle this problem by modeling f as a Gaussian Process (GP). In such methods, the challenge is two fold. At time step t, first estimate the unknown f from the query value-pairs. Then use it to intelligently query at xt where the function is likely to be high. For this, we first use the posterior GP to construct an acquisition function ϕt which captures the value of the experiment at a point. Then we maximise ϕt to determine xt.\nGaussian process bandits and Bayesian optimisation (GPB/ BO) have been successfully applied in many applications such as tuning hyperparameters in learning algorithms (Snoek et al., 2012; Bergstra et al., 2011; Mahendran et al., 2012), robotics (Lizotte et al., 2007; Martinez-Cantin et al., 2007) and object tracking (Denil et al., 2012). However, all such successes have been in low (typically < 10) dimensions (Wang et al., 2013). Expensive high dimensional functions occur in several problems in fields such as computer vision (Yamins et al., 2013), antenna design (Hornby et al., 2006), computational astrophysics (Parkinson et al., 2006) and biology (Gonzalez et al., 2014). Scaling GPB/ BO methods to high dimensions for practical problems has been challenging. Even current theoretical results suggest that GPB/ BO is exponentially difficult in high dimensions without further assumptions (Srinivas et al., 2010; Bull, 2011). To our knowledge, the only approach to date has been to perform regular GPB/ BO on a low dimensional subspace. This works only under strong assumptions.\nWe identify two key challenges in scaling GPB/ BO to high dimensions. The first is the statistical challenge in estimating the function. Nonparametric regression is inherently difficult in high dimensions with known lower bounds depending exponentially in dimension (Györfi et al., 2002). The often exponential sample complexity for regression is invariably reflected in the regret bounds for GPB/ BO. The second is the computational challenge in maximising ϕt. Commonly used global optimisation heuristics used to maximise ϕt themselves require computation exponential in dimension. Any attempt to scale GPB/ BO to high dimensions must effectively address these two concerns.\nar X\niv :1\n50 3.\n01 67\n3v 3\n[ st\nat .M\nL ]\n1 3\nM ay\n2 01\nIn this work, we embark on this challenge by treating f as an additive function of mutually exclusive lower dimensional components. Our contributions in this work are:\n1. We present the Add-GP-UCB algorithm for optimisation and bandits of an additive function. An attractive property is that we use an acquisition function which is easy to optimise in high dimensions.\n2. In our theoretical analysis we bound the regret for Add-GP-UCB. We show that it has only linear dependence on the dimension D when f is additive1.\n3. Empirically we demonstrate that Add-GP-UCB outperforms naive BO on synthetic experiments, an astrophysical simulator and the Viola and Jones face detection problem. Furthermore Add-GP-UCB does well on several examples when the function is not additive.\nA Matlab implementation of our methods is available online at github.com/kirthevasank/add-gp-bandits."
    }, {
      "heading" : "2. Related Work",
      "text" : "GPB/ BO methods follow a family of GP based active learning methods which select the next experiment based on the posterior (Osborne et al., 2012; Ma et al., 2015; Kandasamy et al., 2015). In the GPB/ BO setting, common acquisition functions include Expected improvement (Mockus, 1994), probability of improvement (Jones et al., 1998), Thompson sampling (Thompson, 1933) and upper confidence bound (Auer, 2003). Of particular interest to us, is the Gaussian process upper confidence bound (GPUCB). It was first proposed and analysed in the noisy setting by Srinivas et al. (2010) and extended to the noiseless case by de Freitas et al. (2012). Some literature studies variants, such as combining several acquisition functions (Hoffman et al., 2011) and querying in batches (Azimi et al., 2010).\nTo our knowledge, most literature for GPB/ BO in high dimensions are in the setting where the function varies only along a very low dimensional subspace (Chen et al., 2012; Wang et al., 2013; Djolonga et al., 2013). In these works, the authors do not encounter either challenge as they perform GPB/ BO in either a random or carefully selected lower dimensional subspace. However, assuming that the problem is an easy (low dimensional) one hiding in a high dimensional space is often too restrictive. Indeed, our experimental results confirm that such methods perform poorly on real applications when the assumptions are not met. While our additive assumption is strong in its own right, it is considerably more expressive. It is more\n1Post-publication it was pointed out to us that there was a bug in our analysis. We are working on resolving it and will post an update shortly. See Section 6 for more details.\ngeneral than the setting in Chen et al. (2012). Even though it does not contain the settings in Djolonga et al. (2013); Wang et al. (2013), unlike them, we still allow the function to vary along the entire domain.\nUsing an additive structure is standard in high dimensional regression literature both in the GP framework and otherwise. Hastie & Tibshirani (1990); Ravikumar et al. (2009) treat the function as a sum of one dimensional components. Our additive framework is more general. Duvenaud et al. (2011) assume a sum of functions of all combinations of lower dimensional coordinates. These literature argue that using an additive model has several advantages even if f is not additive. It is a well understood notion in statistics that when we only have a few samples, using a simpler model to fit our data may give us a better trade off for estimation error against approximation error. This observation is crucial: in many applications for Bayesian optimisation we are forced to work in the low sample regime since calls to the blackbox are expensive. Though the additive assumption is biased for nonadditive functions, it enables us to do well with only a few samples. While we have developed theoretical results only for additive f , empirically we show that our additive model outperforms naive GPB/ BO even when the underlying function is not additive.\nAnalyses of GPB/ BO methods focus on the query complexity of f which is the dominating cost in relevant applications. It is usually assumed that ϕt can be maximised to arbitrary precision at negligible cost. Common techniques to maximise ϕt include grid search, Monte Carlo and multistart methods (Brochu et al., 2010). In our work we use the Dividing Rectangles (DiRect) algorithm of Jones et al. (1993). While these methods are efficient in low dimensions they require exponential computation in high dimensions. It is widely acknowledged in the community that this is a critical bottleneck in scaling GPB/ BO to high dimensions (de Freitas, 2014). While we still work in the paradigm where evaluating f is expensive and characterise our theoretical results in terms of query complexity, we believe that assuming arbitrary computational power to optimise ϕt is too restrictive. For instance, in hyperparameter tuning the budget for determining the next experiment is dictated by the cost of the learning algorithm. In online advertising and robotic reinforcement learning we need to act in under a few seconds or real time.\nIn this manuscript, Section 3 formally details our problem and assumptions. We present Add-GP-UCB in Section 4 and our theoretical results in Section 4.3. All proofs are deferred to Appendix B. We summarize the regrets for AddGP-UCB and GP-UCB in Table 1. In Section 5 we present the experiments."
    }, {
      "heading" : "3. Problem Statement & Set up",
      "text" : "We wish to maximise a function f : X → R where X is a rectangular region in RD. We will assume w.l.o.g X = [0, 1]D. f may be nonconvex and gradient information is not available. We can interact with f only by querying at some x ∈ X and obtain a noisy observation y = f(x)+ . Let an optimum point be x∗ = argmaxx∈X f(x). Suppose at time t we choose to query at xt. Then we incur instantaneous regret rt = f(x∗) − f(xt). In the bandit setting, we are interested in the cumulative regret RT = ∑T t=1 rt = ∑T t=1 f(x∗) − f(xt), and in the optimisation setting we are interested in the simple regret ST = mint≤T rt = f(x∗) − maxxt f(xt). For a bandit algorithm, a desirable property is to have no regret: limT→∞ 1 TRT = 0. Since ST ≤ 1 TRT , any such procedure is also a consistent procedure for optimisation.\nKey structural assumption: In order to make progress in high dimensions, we will assume that f decomposes into the following additive form,\nf(x) = f (1)(x(1)) + f (2)(x(2)) + · · ·+ f (M)(x(M)). (1)\nHere each x(j) ∈ X (j) = [0, 1]dj are lower dimensional components. We will refer to the X (j)’s as “groups” and the grouping of different dimensions into these groups {X (j)}Mj=1 as the “decomposition”. The groups are disjoint – i.e. if we treat the elements of the vector x as a set, x(i)∩x(j) = ∅. We are primarily interestd in the case when D is very large and the group dimensionality is bounded: dj ≤ d D. We have D dM ≥ ∑ j dj . Paranthesised superscripts index the groups and a union over the groups denotes the reconstruction of the whole from the groups (e.g. x = ⋃ j x (j) and X = ⋃ j X (j)). xt denotes the point chosen by the algorithm for querying at time t. We will ignore logD terms in O(·) notation. Our theoretical analysis assumes that the decomposition is known but we also present a modified algorithm to handle unknown decompositions and non-additive functions.\nSome smoothness assumptions on f are warranted to make the problem tractable. A standard in the Bayesian paradigm is to assume f is sampled from a Gaussian Process (Rasmussen & Williams, 2006) with a covarince kernel κ : X × X → R and that ∼ N (0, η2). Two commonly used kernels are the squared exponential (SE) κσ,h and the Matérn κν,h kernels with parameters (σ, h) and (ν, h) re-\nspectively. Writing r = ‖x− x′‖2, they are defined as\nκσ,h(x, x ′) = σ exp\n( −r2\n2h2\n) , (2)\nκν,h(x, x ′) =\n21−ν\nΓ(ν)\n(√ 2νr\nh\n)ν Bν (√ 2νr\nh\n) . (3)\nHere Γ, Bν are the Gamma and modified Bessel functions. A principal convenience in modelling our problem via a GP is that posterior distributions are analytically tractable.\nIn keeping with this, we will assume that each f (j) is sampled from a GP, GP(µ(j), κ(j)) where the f (j)’s are independent. Here, µ(j) : X (j) → R is the mean and κ(j) : X (j) × X (j) → R is the covariance for f (j). W.l.o.g let µ(j) = 0 for all j. This implies that f itself is sampled from a GP with an additive kernel κ(x, x′) = ∑ j κ (j)(x(j), x(j) ′ ). We state this formally for nonzero mean as we will need it for the ensuing discussion.\nObservation 1. Let f be defined as in Equation (1), where f (j) ∼ GP(µ(j)(x), κ(j)(x(i), x(j)′)). Let y = f(x) + where ∼ N (0, η2). Denote δ(x, x′) = 1 if x = x′, and 0 otherwise. Then y ∼ GP(µ(x), κ(x, x′) + η2δ(x, x′)) where\nµ(x) = µ(1)(x(1)) + · · ·+ µ(M)(x(M)) (4)\nκ(x, x′) = κ(1)(x(1), x(1) ′ ) + · · ·+ κ(M)(x(M), x(M) ′ ).\nWe will call a kernel such as κ(j) which acts only on d variables a dth order kernel. A kernel which acts on all the variables is a Dth order kernel. Our kernel for f is a sum of M at most dth order kernels which, we will show, is statistically simpler than a Dth order kernel.\nWe conclude this section by looking at some seemingly straightforward approaches to tackle the problem. The first natural question is of course why not directly run GP-UCB using the additive kernel? Since it is simpler than a Dth order kernel we can expect statistical gains. While this is true, it still requires optimising ϕt inD dimensions to determine the next point which is expensive.\nAlternatively, for an additive function, we could adopt a sequential approach where we use 1/M fraction of our query budget to maximise the first group by keeping the rest of the coordinates constant. Then we proceed to the second\ngroup and so on. While optimising a d dimensional acquisition function is easy, this approach is not desirable for several reasons. First, it will not be an anytime algorithm as we will have to pre-allocate our query budget to maximise each group. Once we proceed to a new group we cannot come back and optimise an older one. Second, such an approach places too much faith in the additive assumption. We will only have explored M d-dimensional hyperplanes in the entire space. Third, it is not suitable as a bandit algorithm as we suffer high regret until we get to the last group. We further elaborate on the deficiencies of this and other sequential approaches in Appendix A.2."
    }, {
      "heading" : "4. Algorithm",
      "text" : "Under an additive assumption, our algorithm has two components. First, we obtain the posterior GP for each f (j) using the query-value pairs until time t. Then we maximise a d dimensional GP-UCB-like acquisition function on each GP to construct the next query point. Since optimising ϕt depends exponentially in dimension, this is cheaper than optimising one acquisition on the combined GP."
    }, {
      "heading" : "4.1. Inference on Additive GPs",
      "text" : "Typically in GPs, given noisy labels, Y = {y1, . . . , yn} at points X = {x1, . . . , xn}, we are interested in inferring the posterior distribution for f∗ = f(x∗) at a new point x∗. In our case though, we will be primarily interested in the distribution of f (j)∗ = f (j)(x (j) ∗ ) conditioned on X,Y . We have illustrated this graphically in Figure 1. The joint distribution of f (j)∗ and Y can be written as\n( f\n(j) ∗ Y\n) ∼ N ( 0, [ κ(j)(x (j) ∗ , x (j) ∗ ) κ (j)(x (j) ∗ , X (j))\nκ(j)(X(j), x (j) ∗ ) κ(X,X) + η 2In\n]) .\nThe pth element of κ(j)(X(j), x(j)∗ ) ∈ Rn is κ(x(j)p , x(j)∗ ) and the (p, q)th element of κ(X,X) ∈ Rn×n is κ(xp, xq). We have used the fact Cov(f (i) ∗ , yp) =\nCov(f (i) ∗ , ∑ j f (j)(x (j) p ) + η2 ) = Cov(f (i) ∗ , f (i)(x (i) p )) = κ(i)(x (i) ∗ , x (i) p ) as f (j) ⊥ f (i),∀i 6= j. By writing ∆ = κ(X,X) + η2In ∈ Rn×n, the posterior for f (j)∗ is,\nf (j) ∗ |x∗, X, Y ∼ N ( κ(j)(x (j) ∗ , X (j))∆−1Y, (5)\nκ(j)(x (j) ∗ , x (j) ∗ )− κ(j)(x(j)∗ , X(j))∆−1κ(j)(X,x(j)) ) 4.2. The Add-GP-UCB Algorithm\nIn GPB/ BO algorithms, at each time step t we maximise an acquisition function ϕt to determine the next point: xt = argmaxx∈X ϕt(x). The acquisition function is itself constructed using the posterior GP. The GP-UCB acquisition function, which we focus on here is,\nϕt(x) = µt−1(x) + β 1/2 t σt−1(x).\nIntuitively, the µt−1 term in the GP-UCB objective prefers points where f is known to be high, the σt−1 term prefers points where we are uncertain about f and β1/2t negotiates the tradeoff. The former contributes to the “exploitation” facet of our problem, in that we wish to have low instantaneous regret. The latter contributes to the “exploration” facet since we also wish to query at regions we do not know much about f lest we miss out on regions where f is high. We provide a brief summary of GP-UCB and its theoretical properties in Appendix A.1.\nAs we have noted before, maximising ϕt which is typically multimodal to obtain xt is itself a difficult problem. In any grid search or branch and bound methods such as DiRect, maximising a function to within ζ accuracy, requires O(ζ−D) calls to ϕt. Therefore, for large D maximising ϕt is extremely difficult. In practical settings, especially in situations where we are computationally constrained, this poses serious limitations for GPB/ BO as we may not be able to optimise ϕt to within a desired accuracy.\nFortunately, in our setting we can be more efficient. We propose an alternative acquisition function which applies to an additive kernel. We define the Additive Gaussian Process Upper Confidence Bound (Add-GP-UCB) to be\nϕ̃t(x) = µt−1(x) + β 1/2 t M∑ j=1 σ (j) t−1(x (j)). (6)\nWe immediately see that we can write ϕ̃t as a sum of functions on orthogonal domains: ϕ̃t(x) = ∑ j ϕ̃ (j) t (x (j)) where ϕ̃(j)t (x (j)) = µ (j) t−1(x (j)) + β 1/2 t σ (j) t−1(x\n(j)). This means that ϕ̃t can be maximised by maximising each ϕ̃\n(j) t separately on X (j). As we need to solve M at\nmost d dimensional optimisation problems, it requires only O(Md+1ζ−d) calls to the utility function in total – far more favourable than maximising ϕt.\nSince the cost for maximising the acquisition function is a key theme in this paper let us delve into this a bit more. One call to ϕt requires O(Dt2) effort. For ϕ̃t we need M calls each requiring O(djt2) effort. So both ϕt and ϕ̃t require the same effort in this front. For ϕt, we need to know the posterior for only f whereas for ϕ̃t we need to know the posterior for each f (j). However, the brunt of the work in obtaining the posterior is the O(t3) effort in inverting the t×tmatrix ∆ in (5) which needs to be done for both ϕt and ϕ̃t. For ϕ̃t, we can obtain the inverse once and reuse it M times, so the cost of obtaining the posterior isO(t3+Mt2). Since the number of queries needed will be super linear in D and hence M , the t3 term dominates. Therefore obtaining each posterior f (j) is only marginally more work than obtaining the posterior for f . Any difference here is easily offset by the cost for maximising the acquisition function.\nThe question remains then if maximising ϕ̃t would result in low regret. Since ϕt and ϕ̃t are neither equivalent nor have the same maximiser it is not immediately apparent that this should work. Nonetheless, intuitively this seems like a reasonable scheme since the ∑ j σ (j) t−1 term captures some notion of the uncertainty and contributes to exploration. In Theorem 5 we show that this intuition is reasonable – maximising ϕ̃t achieves the same rates as ϕt for cumulative and simple regrets if the kernel is additive.\nWe summarise the resulting algorithm in Algorithm 1. In brief, at time step t, we obtain the posterior distribution for f (j) and maximise ϕ̃(j)t to determine the coordinates x (j) t . We do this for each j and then combine them to obtain xt.\nAlgorithm 1 Add-GP-UCB Input: Kernels κ(1), . . . , κ(M), Decomposition (X (j))Mj=1 • D0 ← ∅, • for j = 1, . . . ,M , (µ(j)0 , κ (j) 0 )← (0, κ(j)).\n• for t = 1, 2, . . . 1. for j = 1, . . . ,M ,\nx (j) t ← argmaxz∈X (j) µ (j) t−1(z) + √ βtσ (j) t−1(z)\n2. xt ← ⋃M j=1 x (j) t . 3. yt ← Query f at xt. 4. Dt = Dt−1 ∪ {(xt,yt)}. 5. Perform Bayesian posterior updates conditioned\non Dt to obtain µ(j)t , σ (j) t for j = 1, . . . ,M ."
    }, {
      "heading" : "4.3. Main Theoretical Results",
      "text" : "Now, we present our main theoretical contributions. We bound the regret for Add-GP-UCB under different kernels. Following Srinivas et al. (2010), we first bound the\nstatistical difficulty of the problem as determined by the kernel. We show that under additive kernels the problem is much easier than when using a full Dth order kernel. Next, we show that the Add-GP-UCB algorithm is able to exploit the additive structure and obtain the same rates as GP-UCB. The advantage to using Add-GP-UCB is that it is much easier to optimise the acquisition function. For our analysis, we will need Assumption 2 and Definition 3.\nAssumption 2. Let f be sampled from a GP with kernel κ. κ(·, x) is L-Lipschitz for all x. Further, the partial derivatives of f satisfies the following high probability bound. There exists constants a, b > 0 such that,\nP (\nsup x ∣∣∣∂f(x) ∂xi ∣∣∣ > J) ≤ ae−(J/b)2 . The Lipschitzian condition is fairly mild and the latter condition holds for four times differentiable stationary kernels such as the SE and Matérn kernels for ν > 2 (Ghosal & Roy, 2006). Srinivas et al. (2010) showed that the statistical difficulty of GPB/ BO is determined by the Maximum Information Gain as defined below. We bound this quantity for additive SE and Matérn kernels in Theorem 4. This is our first main theorem.\nDefinition 3. (Maximum Information Gain) Let f ∼ GP(µ, κ), yi = f(xi) + where ∼ N (0, η2). Let A = {x1, . . . , xT } ⊂ X be a finite subset, fA denote the function values at these points and yA denote the noisy observations. Let I be the Shannon Mutual Information. The Maximum Information Gain between yA and fA is\nγT = max A⊂X ,|A|=T I(yA; fA).\nTheorem 4. Assume that the kernel κ has the additive form of (4), and that each κ(j) satisfies Assumption 2. W.l.o.g assume κ(x, x′) = 1. Then,\n1. If each κ(j) is a dthj order squared exponential kernel (2) where dj ≤ d, then γT ∈ O(Ddd(log T )d+1).\n2. If each κ(j) is a dthj order Matérn kernel (3) where dj ≤ d and ν > 2, then γT ∈ O(D2dT d(d+1) 2ν+d(d+1) log(T )).\nWe use bounds on the eigenvalues of the SE and Matérn kernels from Seeger et al. (2008) and a result from Srinivas et al. (2010) which bounds the information gain via the eigendecay of the kernel. We bound the eigendecay of the sum κ via M and the eigendecay of a single κ(j). The complete proof is given in Appendix B.1. The important observation is that the dependence on D is linear for an additive kernel. In contrast, for a Dth order kernel this is exponential (Srinivas et al., 2010).\nNext, we present our second main theorem which bounds the regret for Add-GP-UCB for an additive kernel as given in Equation 4.\nTheorem 5. Suppose f is constructed by sampling f (j) ∼ GP(0, κ(j)) for j = 1, . . . ,M and then adding them. Let all kernels κ(j) satisfy assumption 2 for some L, a, b. Further, we maximise the acquisition function ϕ̃t to within ζ0t −1/2 accuracy at time step t. Pick δ ∈ (0, 1) and choose\nβt = 2 log\n( Mπ2t2\n2δ\n) + 2d log ( Dt3 ) ∈ O (d log t) .\nThen, Add-GP-UCB attains cumulative regret RT ∈ O (√ DγTT log T ) and hence simple regret ST ∈\nO (√ DγT log T/T ) . Precisely, with probability > 1− δ,\n∀T ≥ 1, RT ≤ √ 8C1βTMTγt + 2ζ0 √ T + C2.\nwhere C1 = 1/ log(1 + η−2) and C2 is a constant depending on a, b, D, δ, L and η.\nPart of our proof uses ideas from Srinivas et al. (2010). We show that ∑ j βtσ (j) t−1(·) forms a credible interval for f(·) about the posterior mean µt(·) for an additive kernel in Add-GP-UCB. We relate the regret to this confidence set using a covering argument. We also show that our regret doesn’t suffer severely if we only approximately optimise the acquisition provided that the accuracy improves at rate O(t−1/2). For this we establish smoothness of the posterior mean. The correctness of the algorithm follows from the fact that Add-GP-UCB can be maximised by individually maximising ϕ̃(j)t on each X (j). The complete proof is given in Appendix B.2. When we combine the results in Theorems 4 and 5 we obtain the rates given in Table 12.\nOne could consider alternative lower order kernels – one candidate is the sum of all possible dth order kernels (Duvenaud et al., 2011). Such a kernel would arguably allow us to represent a larger class of functions than our kernel in (4). If, for instance, we choose each of them to be a SE kernel, then it can be shown that γT ∈ O(Dddd+1(log T )d+1). Even though this is worse than our kernel in poly(D) factors, it is still substantially better than using a Dth order kernel. However, maximising the corresponding utility function, either of the form ϕt or ϕ̃t, is still a D dimensional problem. We reiterate that what renders our algorithm attractive in large D is not just the statistical gains due to the simpler kernel. It is also the fact that our acquisition function can be efficiently maximised."
    }, {
      "heading" : "4.4. Practical Considerations",
      "text" : "Our practical implementation differs from our theoretical analysis in the following aspects.\n2See Footnote 1.\nChoice of βt: βt as specified by Theorems 5, usually tends to be conservative in practice (Srinivas et al., 2010). For good empirical performance a more aggressive strategy is required. In our experiments, we set βt = 0.2d log(2t) which offered a good tradeoff between exploration and exploitation. Note that this captures the correct dependence on D, d and t in Theorems 5 and 6.\nData dependent prior: Our analysis assumes that we know the GP kernel of the prior. In reality this is rarely the case. In our experiments, we choose the hyperparameters of the kernel by maximising the GP marginal likelihood (Rasmussen & Williams, 2006) every Ncyc iterations.\nInitialisation: Marginal likelihood based kernel tuning can be unreliable with few data points. This is a problem in the first few iterations. Following the recommendations in Bull (2011) we initialise Add-GP-UCB (and GP-UCB) using Ninit points selected uniformly at random.\nDecomposition & Non-additive functions: If f is additive and the decomposition is known, we use it directly. But it may not always be known or f may not be additive. Then, we could treat the decomposition as a hyperparameter of the additive kernel and maximise the marginal likelihood w.r.t the decomposition. However, given that there are D!/d!MM ! possible decompositions, computing the marginal likelihood for all of them is infeasible. We circumvent this issue by randomly selecting a few (O(D)) decompositions and choosing the one with the largest marginal likelihood. Intuitively, if the function is not additive, with such a “partial maximisation” we can hope to capture some existing marginal structure in f . At the same time, even an exhaustive maximisation will not do much better than a partial maximisation if there is no additive structure. Empirically, we found that partially optimising for the decomposition performed slightly better than using a fixed decomposition or a random decomposition at each step. We incorporate this procedure for finding an appropriate decomposition as part of the kernel hyper parameter learning procedure every Ncyc iterations.\nHow do we choose (d,M) when f is not additive? If d is large we allow for richer class of functions, but risk high variance. For small d, the kernel is too simple and we have high bias but low variance – further optimising ϕ̃t is easier. In practice we found that our procedure was fairly robust for reasonable choices of d. Yet this is an interesting theoretical question. We also believe it is a difficult one. Using the marginal likelihood alone will not work as the optimal choice of d also depends on the computational budget for optimising ϕ̃t. We hope to study this question in future work. For now, we give some recommendations at the end. Our modified algorithm with these practical considerations is given below. Observe that in this specification if we use d = D we have the original GP-UCB algorithm.\nAlgorithm 2 Practical-Add-GP-UCB Input: Ninit, Ncyc, d, M • D0 ← Ninit points chosen uniformly at random. • for t = 1, 2, . . .\n1. if (t mod Ncyc = 0), Learn the kernel hyper parameters and the decomposition {Xj} by maximising the GP marginal likelihood.\n2. Perform steps 1-3 in Algorithm 1 with βt = 0.2d log 2t.\n3. Dt = Dt−1 ∪ {(xt,yt)}. 4. Perform Bayesian posterior updates conditioned\non Dt to obtain µ(j)t , σ (j) t for j = 1, . . . ,M ."
    }, {
      "heading" : "5. Experiments",
      "text" : "To demonstrate the efficacy of Add-GP-UCB over GPUCB we optimise the acquisition function under a constrained budget. Following, Brochu et al. (2010) we use DiRect to maximise ϕt, ϕ̃t. We compare Add-GP-UCB against GP-UCB, random querying (RAND) and DiRect3. On the real datasets we also compare it to the Expected Improvement (GP-EI) acquisition function which is popular in BO applications and the method of Wang et al. (2013) which uses a random projection before applying BO (REMBO). We have multiple instantiations of Add-GPUCB for different values for (d,M). For optimisation, we perform comparisons based on the simple regret ST and for bandits we use the time averaged cumulative regret RT /T .\nFor all GPB/ BO methods we set Ninit = 10, Ncyc = 25 in all experiments. Further, for the first 25 iterations we set the bandwidth to a small value (10−5) to encourage an explorative strategy. We use SE kernels for each additive kernels and use the same scale σ and bandwidth h hyperparameters for all the kernels. Every 25 iterations we maximise the marginal likelihood with respect to these 2 hyperparameters in addition to the decomposition.\n3There are several optimisation methods based on simulated annealing, cross entropy and genetic algorithms. We use DiRect since its easy to configure and known to work well in practice.\nIn contrast to existing literature in the BO community, we found that the UCB acquisitions outperformed GP-EI. One possible reason may be that under a constrained budget, UCB is robust to imperfect maximisation (Theorem 5) whereas GP-EI may not be. Another reason may be our choice of constants in UCB (Section 4.4)."
    }, {
      "heading" : "5.1. Simulations on Synthetic Data",
      "text" : "First we demonstrate our technique on a series of synthetic examples. For this we construct additive functions for different values for the maximum group size d′ and the number of groups M ′. We use the prime to distinguish it from Add-GP-UCB instantiations with different combinations of (d,M) values. The d′ dimensional function fd′ is,\nfd′(x) = log\n( 0.1 1\nhd ′ d′\nexp\n( ‖x− v1‖2\n2h2d′\n) + (7)\n0.1 1\nhd ′ d′\nexp\n( ‖x− v2‖2\n2h2d′\n) + 0.8 1\nhd ′ d′\nexp\n( ‖x− v3‖2\n2h2d′\n))\nwhere v1, v2, v3 are fixed d′ dimensional vectors and hd′ = 0.01d′\n0.1. Then we create M ′ groups of coordinates by randomly adding d′ coordinates into each group. On each such group we use fd′ and then add them up to obtain the composite function f . Precisely,\nf(x) = fd′(x (1)) + · · ·+ fd′(x(M))\nThe remaining D − d′M ′ coordinates do not contribute to the function. Since fd′ has 3 modes, f will have 3M ′ modes. We have illustrated fd′ for d′ = 2 in Figure 2.\nIn the synthetic experiments we use an instantiation of Add-GP-UCB that knows the decomposition–i.e. (d,M) = (d′,M ′) and the grouping of coordinates. We refer to this as Add-?. For the rest we use a (d,M) decomposition by creating M groups of size at most d and find a good grouping by partially maximising the marginal likelihood (Section 4.4). We refer to them as Add-d/M .\nFor GP-UCB we allocate a budget of min(5000, 100D) DiRect function evaluations to optimise the acquisition function. For all Add-d/M methods we set it to 90% of this amount4 to account for the additional overhead in posterior inference for each f (j). Therefore, in our 10D problem we maximise ϕt with βt = 2 log(2t) with 1000 DiRect evaluations whereas for Add-2/5 we maximise each ϕ̃(j)t with βt = 0.4 log(2t) with 180 evaluations.\nThe results are given in Figures 3 and 4. We refer to each example by the configuration of the additive function–its (D, d′,M ′) values. In the (10, 3, 3) example Add-? does\n4While the 90% seems arbitrary, in our experiments this was hardly a factor as the cost was dominated by the inversion of ∆.\nbest since it knows the correct model and the acquisition function can be maximised within the budget. However Add-3/4 and Add-5/2 models do well too and outperform GP-UCB. Add-1/10 performs poorly since it is statistically not expressive enough to capture the true function. In the (24, 11, 2), (40, 18, 2), (40, 35, 1), (96, 29, 3) and (120, 55, 2) examples Add-? outperforms GP-UCB. However, it is not competitive with the Add-d/M for small d. Even though Add-? knew the correct decomposition, there are two possible failure modes since d′ is large. The kernel is complex and the estimation error is very high in the absence of sufficient data points. In addition, optimising the acquisition is also difficult. This illustrates our previous argument that using an additive kernel can be advantageous even if the function is not additive or the decomposition is not known. In the (24, 6, 4), (40, 5, 8) and (96, 5, 19) examples Add-? performs best as d′ is small enough. But again, almost all Add-d/M instantiations outperform GPUCB. In contrast to the small D examples, for large D, GP-UCB and Add-d/M with large d perform worse than DiRect. This is probably because our budget for maximising ϕt is inadequate to optimise the acquisition function to sufficient accuracy. For some of the large D examples the cumulative regret is low for Add-GP-UCB and Addd/M with large d. This is probably since they have already started exploiting where as the Add-d/M with small\nd methods are still exploring. We posit that if we run for more iterations we will be able to see the improvements."
    }, {
      "heading" : "5.2. SDSS Astrophysical Dataset",
      "text" : "Here we used Galaxy data from the Sloan Digital Sky Survey (SDSS). The task is to find the maximum likelihood estimators for a simulation based astrophysical likelihood model. Data and software for computing the likelihood are taken from Tegmark et al (2006). The software itself takes in only 9 parameters but we augment this to 20 dimensions to emulate the fact that in practical astrophysical problems we may not know the true parameters on which the problem is dependent. This also allows us to effectively demonstrate the superiority of our methods over alternatives. Each query to this likelihood function takes about 2-5 seconds. In order to be wall clock time competitive with RAND and DiRectwe use only 500 evaluations for GP-UCB, GP-EI and REMBO and 450 for Add-d/M to maximise the acquisition function.\nWe have shown the Maximum value obtained over 400 iterations of each algorithm in Figure 5(a). Note that RAND outperforms DiRect here since a random query strategy is effectively searching in 9 dimensions. Despite this advantage to RAND all BO methods do better. Moreover, despite the fact that the function may not be additive, all\nAdd-d/M methods outperform GP-UCB. Since the function only depends on 9 parameters we use REMBO with a 9 dimensional projection. Yet, it is not competitive with the Add-d/M methods. Possible reasons for this may include the scaling of the parameter space by √ d in REMBO and the imperfect optimisation of the acquisition function. Here Add-5/4 performs slightly better than the rest since it seems to have the best tradeoff between being statistically expressive enough to capture the function while at the same time be easy enough to optimise the acquisition function within the allocated budget."
    }, {
      "heading" : "5.3. Viola & Jones Face Detection",
      "text" : "The Viola & Jones (VJ) Cascade Classifier (Viola & Jones, 2001) is a popular method for face detection in computer vision based on the Adaboost algorithm. The K-cascade has K weak classifiers which outputs a score for any given image. When we wish to classify an image we pass that image through each classifier. If at any point the score falls below a certain threshold the image is classified as negative. If the image passes through all classifiers then it is classified as positive. The threshold values at each stage are usually pre-set based on prior knowledge. There is no reason to believe that these threshold values are optimal. In this experiment we wish to find an optimal set of values for these thresholds by optimising the classification accuracy over a training set.\nFor this task, we use 1000 images from the Viola & Jones face dataset containing both face and non-face images. We use the implementation of the VJ classifier that comes with OpenCV (Bradski & Kaehler, 2008) which uses a 22-stage cascade and modify it to take in the threshold values as a parameter. As our domain X we choose a neighbourhood around the configuration given in OpenCV. Each function call takes about 30-40 seconds and is the the dominant cost in this experiment. We use 1000 DiRect evaluations to optimise the acquisition function for GP-UCB, GPEI and REMBO and 900 for the Add-d/M instantiations. Since we do not know the structure of the function we use REMBO with a 5 dimensional projection. The results are given in Figure 5(b). Not surprisingly, REMBO performs worst as it is only searching on a 5 dimensional space. Barring Add-1/22 all other instantiations perform better than GP-UCB and GP-EI with Add-6/4 performing the best. Interestingly, we also find a value for the thresholds that outperform the configuration used in OpenCV."
    }, {
      "heading" : "6. Conclusion",
      "text" : "Recommendations: Based on our experiences, we recommend the following. If f is known to be additive, the decomposition is known and d is small enough so that ϕ̃t can be efficiently optimised, then running Add-GP-UCB\nwith the known decomposition is likely to produce the best results. If not, then use a small value for d and run AddGP-UCB while partially optimising for the decomposition periodically (Section 4.4). In our experiments we found that using d between 3 an 12 seemed reasonable choices. However, note that this depends on the computational budget for optimising the acquisition, the query budget for f and to a certain extent the the function f itself.\nSummary: Our algorithm takes into account several practical considerations in real world GPB/ BO applications such as computational constraints in optimising the acquisition and the fact that we have to work with a relatively few data points since function evaluations are expensive. Our framework effectively addresses these concerns without considerably compromising on the statistical integrity of the model. We believe that this provides a promising direction to scale GPB/ BO methods to high dimensions.\nFuture Work: Our experiments indicate that our methods perform well beyond the scope suggested by our theory. Developing an analysis that takes into account the biasvariance and computational tradeoffs in approximating and optimising a non-additive function via an additive model is an interesting challenge. We also intend to extend this framework to discrete settings, other acquisition functions\nand handle more general decompositions."
    }, {
      "heading" : "Acknowledgements",
      "text" : "We wish to thank Akshay Krishnamurthy and Andrew Gordon Wilson for the insightful discussions and Andreas Krause, Sham Kakade and Matthias Seeger for the helpful email conversations. This research is partly funded by DOE grant DESC0011114.\nOur current analysis, specifically equation 14, has an error. We are working on resolving this and will post an update shortly. We would like to thank Felix Berkenkamp and Andreas Krause from ETH Zurich for pointing this out."
    }, {
      "heading" : "A. Some Auxiliary Material",
      "text" : "A.1. Review of the GP-UCB Algorithm\nIn this subsection we present a brief summary of the GP-UCB algorithm in (Srinivas et al., 2010). The algorithm is given in Algorithm 3.\nThe following theorem gives the rate of convergence for GP-UCB. Note that under an additive kernel, this is the same rate as Theorem 5 which uses a different acquisition function. Note the differences in the choice of βt.\nTheorem 6. (Modification of Theorem 2 in (Srinivas et al., 2010)) Suppose f is constructed by sampling f (j) ∼ GP(0, κ(j)) for j = 1, . . . ,M and then adding them. Let all kernels κ(j) satisfy assumption 2 for some L, a, b. Further, we maximise the acquisition function ϕ̃t to within ζ0t−1/2 accuracy at time step t. Pick δ ∈ (0, 1) and choose\nβt = 2 log\n( 2t2π2\nδ\n) + 2D log ( Dt3 ) ∈ O (D log t) .\nThen, GP-UCB attains cumulative regretRT ∈ O (√ DγTT log T ) and hence simple regret ST ∈ O (√ DγT log T/T )\n. Precisely, with probability > 1− δ,\n∀T ≥ 1, RT ≤ √ 8C1βTMTγt + 2ζ0 √ T + C2.\nwhere C1 = 1/ log(1 + η−2) and C2 is a constant depending on a, b, D, δ, L and η.\nProof. Srinivas et al. (2010) bound the regret for exact maximisation of the GP-UCB acquisition ϕt. By following an analysis similar to our proof of Theorem 5 the regret can be shown to be the same for an ζ0t−1/2- optimal maximisation.\nAlgorithm 3 GP-UCB Input: Kernel κ, Input Space X . For t = 1, 2 . . . • D0 ← ∅, • (µ0, κ0)← (0, κ) • for t = 1, 2, . . .\n1. xt ← argmaxz∈X µt−1(z) + √ βtσt−1(z) 2. yt ← Query f at xt. 3. Dt = Dt−1 ∪ {(xt,yt)}. 4. Perform Bayesian posterior updates to obtain µt, σt for j = 1, . . . ,M .\nA.2. Sequential Optimisation Approaches\nIf the function is known to be additive, we could consider several other approaches for maximisation. We list two of them here and explain their deficiencies. We recommend that the reader read the main text before reading this section.\nA.2.1. OPTIMISE ONE GROUP AND PROCEED TO THE NEXT\nFirst, fix the coordinates of x(j), j 6= 1 and optimise w.r.t x(1) by querying the function for a pre-specified number of times. Then we proceed sequentially optimising with respect to x(2), x(3) . . . . We have outlined this algorithm in Algorithm 4. There are several reasons this approach is not desirable.\n• First, it places too much faith on the additive assumption and requires that we know the decomposition at the start of the algorithm. Note that this strategy will only have searched the space in M d-dimensional subspaces. In our approach even if the function is not additive we can still hope to do well since we learn the best additive approximation to the true function. Further, if the decomposition is not known we could learn the decomposition “on the go” or at least find a reasonably good decomposition as we have explained in Section 4.4.\n• Such a sequential approach is not an anytime algorithm. This in particular means that we need to predetermine the number of queries to be allocated to each group. After we proceed to a new group it is not straightforward to come back and improve on the solution obtained for an older group.\n• This approach is not suitable for the bandits setting. We suffer large instantaneous regret up until we get to the last group. Further, after we proceed beyond a group since we cannot come back, we cannot improve on the best regret obtained in that group.\nOur approach does not have any of these deficiencies.\nAlgorithm 4 Seq-Add-GP-UCB Input: Kernels κ(1), . . . , κ(M), Decomposition (X (j))Mj=1, Query Budget T , • RD 3 θ = ⋃M j=1 θ\n(j) = rand([0, 1]d) • for j = 1, . . . ,M\n1. D(j)0 ← ∅, 2. (µ(j)0 , κ (j) 0 )← (0, κ(j)). 3. for t = 1, 2, . . . T/M (a) x(j)t ← argmaxz∈X (j) µ(j)(z) + √ βtσ (j)(z)\n(b) xt ← x(j)t ⋃ k 6=j θ\n(k). (c) yt ← Query f at xt. (d) D(j)t = D (j) t−1 ∪ {(x (j) t ,yt)}. (e) Perform Bayesian posterior updates to obtain µ(j)t , σ (j) t .\n4. θ(j) ← x(j)T/M • Return θ\nA.2.2. ONLY CHANGE ONE GROUP PER QUERY\nIn this strategy, the approach would be very similar to Add-GP-UCB except that at each query we will only update one group at time. If it is the kth group the query point is determined by maximising ϕ̃(k)t for x (k) t and for all other groups we use values from the previous rotation. After M iterations we cycle through the groups. We have outlined this in Algorithm 5.\nThis is a reasonable approach and does not suffer from the same deficiencies as Algorithm 4. Maximising the acquisition function will also be slightly easier O(ζ−d) since we need to optimise only one group at a time. However, the regret for this approach would beO(M √ DγTT log T ) which is a factor ofM worse than the regret in our method (This can be show by following an analysis similar to the one in section B.2. This is not surprising, since at each iteration you are moving in d-coordinates of the space and you have to wait M iterations before the entire point is updated.\nAlgorithm 5 Add-GP-UCB-Buggy Input: Kernels κ(1), . . . , κ(M), Decomposition (X (j))Mj=1 • D0 ← ∅, • for j = 1, . . . ,M , (µ(j)0 , κ (j) 0 )← (0, κ(j)).\n• for t = 1, 2, . . . 1. k = j mod M\n2. x(k)t ← argmaxz∈X (k) µ(k)(z) + √ βtσ (k)(z) 3. for j 6= k, x(j)t ← x (j) t−1\n4. xt ← ⋃M j=1 x (j) t . 5. yt ← Query f at xt. 6. Dt = Dt−1 ∪ {(xt,yt)}. 7. Perform Bayesian posterior updates to obtain µ(j)t , σ (j) t for j = 1, . . . ,M ."
    }, {
      "heading" : "B. Proofs of Results in Section 4.3",
      "text" : "B.1. Bounding the Information Gain γT\nFor this we will use the following two results from Srinivas et al. (2010).\nLemma 7. (Information Gain in GP, (Srinivas et al., 2010) Lemma 5.3) Using the basic properties of a GP, they show that\nI(yA; fA) = 1\n2 n∑ t=1 log(1 + η−2σ2t−1(xt)).\nwhere σ2t−1 is the posterior variance after observing the first t− 1 points.\nTheorem 8. (Bound on Information Gain, (Srinivas et al., 2010) Theorem 8) Suppose that X is compact and κ is a kernel on d dimensions satisfying Assumption 2. Let nT = C9T τ log T where C9 = 4d+ 2. For any T∗ ∈ {1, . . . ,min(T, nT )}, let Bκ(T∗) = ∑ s>T∗ λs. Here (λn)n∈N are the eigenvalues of κ w.r.t the uniform distribution over X . Then,\nγT ≤ inf τ\n( 1/2\n1− e−1 max r∈{1,...,T}\n( T∗ log(rnT /η 2) + C9η 2(1− r/T )(T τ+1Bκ(T∗) + 1) log T ) +O(T 1−τ/d) ) .\nB.1.1. PROOF OF THEOREM 4-1\nProof. We will use some bounds on the eigenvalues for the simple squared exponential kernel given in (Seeger et al., 2008). It was shown that the eigenvalues {λ(i)s } of κ(i) satisfied λ(i)s ≤ cdBs\n1/di where B < 1 (See Remark 9). Since the kernel is additive, and x(i) ∩ x(j) = ∅ the eigenfunctions corresponding to κ(i) and κ(j) will be orthogonal. Hence the eigenvalues of κ will just be the union of the eigenvalues of the individual kernels – i.e. {λs} = ⋃M j=1{λ (j) s }. As B < 1, λ (i) s ≤ cdBs 1/d . Let T+ = bT∗/Mc and α = − logB. Then,\nBκ(T∗) = ∑ s>T∗ λs ≤Mc ∑ s>T+ Bs 1/d\n≤ cdM ( BT 1/d + + ∫ ∞ T+ exp(−αx1/d) ) dx\n≤ cdM ( BT 1/d + + dα−dΓ(d, αT\n1/d + ) ) ≤ cdMe−αT 1/d + ( 1 + d!dα−d(αT\n1/d + )\nd−1 ) .\nThe last step holds true whenever αT 1/d+ ≥ 1. Here in the second step we bound the series by an integral and in the third step we used the substitution y = αx1/d to simplify the integral. Here Γ(s, x) = ∫∞ x ts−1e−tdt is the (upper) incomplete Gamma function. In the last step we have used the following identity and the bound for integral s and x ≥ 1\nΓ(s, x) = (s− 1)!e−x s−1∑ k=0 xk k! ≤ s!e−xxd−1.\nBy using τ = d and by using T∗ ≤ (M + 1)T+, we use Theorem 8 to obtain the following bound on γT ,\nγT ≤ 1/2\n1− e−1 max r∈{1,...,T}\n( (M + 1)T+ log(rnT /η 2)+\nC9η 2(1− r/T ) log T ( 1 + cdMe−αT 1/d + T d+1 ( 1 + d!dα−d(αT 1/d + ) d−1 ))) . (8)\nNow we need to pick T+ so as to balance these two terms. We will choose T+ = ( log(TnT ) α )d which is less than min(T, nT )/M for sufficiently large T . Then e−αT 1/d + = 1/TnT . Then the first term S1 inside the paranthesis is,\nS1 = (M + 1) log d ( TnT α ) log ( rnT η2 ) ∈ O ( M (log(TnT )) d log(rnT ) )\n∈ O ( M ( log(T d+1 log T ) )d log(rT d log T ) ) ∈ O ( Mdd+1(log T )d+1 +Mdd(log T )d log(r) ) .\nNote that the constant in front has exponential dependence on d but we ignore it since we already have dd, (log T )d terms. The second term S2 becomes,\nS2 = C9η 2(1− r/T ) log T ( 1 + cdM\nTnT T d+1\n( 1 + d!dα−d(log(TnT ) d−1))) ≤ C9η2(1− r/T ) ( log T + cdM\nC9\n( 1 + d!dα−d(log(TnT ) d−1))) ≤ C9η2(1− r/T ) ( O(log T ) +O(1) +O(d!dd(log T )d−1)\n)) ∈ O ( (1− r/T )d!dd(log T )d−1 ) .\nSince S1 dominates S2, we should choose r = T to maximise the RHS in (8). This gives us,\nγT ∈ O ( Mdd+1(log T )d+1 ) ∈ O ( Ddd(log T )d+1 ) .\nB.1.2. PROOF OF THEOREM 4-2\nProof. Once again, we use bounds given in (Seeger et al., 2008). It was shown that the eigenvalues {λ(i)s } for κ(i) satisfied λ(i)s ≤ cds − 2ν+dj dj (See Remark 9). By following a similar argument to above we have {λs} = ⋃M j=1{λ (j) s } and λ(i)s ≤ cds− 2ν+d d . Let T+ = bT∗/Mc. Then,\nBκ(T∗) = ∑ s>T∗ λs ≤Mcd ∑ s>T+ s− 2ν+d d ≤Mcd ( T − 2ν+dd + + ∫ ∞ T+ s− 2ν+d d ) ≤ C82dMT 1− 2ν+dd + .\nwhere C8 is an appropriate constant. We set T+ = (TnT ) d 2ν+d (log(TnT )) − d2ν+d and accordingly we have the following bound on γT as a function of T+ ∈ {1, . . . ,min(T, nT )/M},\nγT ≤ inf τ\n( 1/2\n1− e−1 max r∈{1,...,T}\n( (M + 1)T+ log(rnT /η 2) + C9η 2(1− r/T ) ( log T + C82 dMT+ log(TnT ) )) +O(T 1−τ/d) ) .\n(9)\nSince this is a concave function on r we can find the optimum by setting the derivative w.r.t r to be zero. We get r ∈ O(T/2d log(TnT )) and hence,\nγT ∈ inf τ\n( O ( MT+ log ( TnT\n2d log(TnT )\n)) +O ( M2dT+ log(TnT ) ) +O(T 1−τ/d) ) ∈ inf\nτ\n( O ( M2d log(TnT ) ( T τ+1 log(T )\n(τ + 1) log(T ) + log log T\n) d 2ν+d ) +O(T 1−τ/d) ) ∈ inf\nτ\n( O ( M2d log(TnT )T (τ+1)d 2ν+d ) +O(T 1−τ/d) ) ∈ O ( M2dT d(d+1) 2ν+d(d+1) log(T ) ) .\nHere in the second step we have substituted the values for T+ first and then nT . In the last step we have balanced the polynomial dependence on T in both terms by setting τ = 2νd2ν+d(d+1) .\nRemark 9. The eigenvalues and eigenfunctions for the kernel are defined with respect to a base distribution on X . In the development of Theorem 8, Srinivas et al. (2010) draw nT samples from the uniform distribution on X . Hence, the eigenvalues/eigenfunctions should be w.r.t the uniform distribution. The bounds given in Seeger et al. (2008) are for the uniform distribution for the Matérn kernel and a Gaussian Distribution for the Squared Exponential Kernel. For the latter case, Srinivas et al. (2010) argue that the uniform distribution still satisfies the required tail constraints and therefore the bounds would only differ up to constants.\nB.2. Rates on Add-GP-UCB\nOur analysis in this section draws ideas from Srinivas et al. (2010). We will try our best to stick to their same notation. However, unlike them we also handle the case where the acquisition function is optimised within some error. In the ensuing discussion, we will use x̃t = ⋃ j x̃ (j) t to denote the true maximiser of ϕ̃t – i.e. x̃ (j) t = argmaxz∈X (j) ϕ̃ (j) t (z).\nxt = ⋃ j x (j) t denotes the point chosen by Add-GP-UCB at the tth iteration. Recall that xt is ζ0t−1/2–optimal; I.e. ϕ̃t(x̃t)− ϕ̃t(xt) ≤ ζ0t−1/2.\nDenote p = ∑ j dj . πt denotes a sequence such that ∑ t π −1 t = 1. For e.g. when we use πt = π\n2t2/6 below, we obtain the rates in Theorem 5.\nIn what follows, we will construct discretisations Ω(j) on each group X (j) for the sake of analysis. Let ωj = |Ω(j)| and ωm = maxj ωj . The discretisation of the individual groups induces a discretisation Ω on X itself, Ω = {x = ⋃ j x (j) : x(j) ∈ Ω(j), j = 1, . . . ,M}. Let ω = |Ω| = ∏ j ωj . We first establish the following two lemmas before we prove Theorem 5.\nLemma 10. Pick δ ∈ (0, 1) and set βt = 2 log(ωmMπt/δ). Then with probability > 1− δ,\n∀t ≥ 1,∀x ∈ Ω, |f(x)− µt−1(x)| ≤ β1/2t M∑ j=1 σ (j) t−1(x (j)).\nProof. Conditioned on Dt−1, at any given x and t we have f(x(j)) ∼ N (µ(j)t−1(x(j)), σ (j) t−1j), ∀j = 1, . . .M . Using the tail bound, P(z > M) ≤ 12e −M2/2 for z ∼ N (0, 1) we have with probability > 1− δ/ωMπt,\n|f (j)(x(j))− µ(j)t−1(x(j))| σ\n(j) t−1(x\n(j)) > β\n1/2 t ≤ e−βt/2 =\nδ\nωmMπt .\nBy using a union bound ωj ≤ ωm times over all x(j) ∈ Ω(j) and then M times over all discretisations the above holds with probability > 1 − δ/πt for all j = 1, . . . ,M and x(j) ∈ Ω(j). Therefore, we have |f(x) − µt−1(x)| ≤ |f(x(j)) − µ\n(j) t−1(x (j))| ≤ β1/2t ∑ j σ (j) t−1(x (j)) for all x ∈ Ω. Now using the union bound on all t yields the result.\nLemma 11. The posterior mean µt−1 for a GP whose kernel κ(·, x) is L-Lipschitz satisfies, P ( ∀t ≥ 1 |µt−1(x)− µt−1(x′)| ≤ ( f(x∗) + η √ 2 log(πt/2δ) ) Lη−2t‖x− x′‖2 ) ≥ 1− δ.\nProof. Note that for given t, P ( yt < f(x∗) + η √ 2 log(πt/2δ) ) ≤ P ( t/η < √ 2 log(πt/2δ) ) ≤ δ/πt.\nTherefore the statement is true with probability > 1 − δ for all t. Further, ∆ η2I implies ‖∆−1‖op ≤ η−2 and |k(x, z)− k(x′, z)| ≤ L‖x− x′‖. Therefore\n|µt−1(x)− µt−1(x′)| = |Y >t−1∆−1(k(x,XT )− k(x′, XT )| ≤ ‖Yt−1‖2‖∆−1‖op‖k(x,Xt−1)− k(x′, Xt−1)‖2 ≤ ( f(x∗) + η √ 2 log(πt/2δ) ) Lη−2(t− 1)‖x− x′‖2.\nB.2.1. PROOF OF THEOREM 5\nProof. First note that by Assumption 2 and the union bound we have, P(∀i supx(j)∈X (j) |∂f (j)(x(j))/∂x (j) i | > J) ≤ diae −(J/b)2 . Since, ∂f(x)/∂x(j)i = ∂f (j)(x(j))/∂x (j) i , we have,\nP ( ∀i = 1, . . . , D sup\nx∈X ∣∣∣∂f(x) ∂xi ∣∣∣ > J) ≤ pae−(J/b)2 . By setting δ/3 = pae−J 2/b2 we have with probability > 1− δ/3,\n∀x, x′ ∈ X , |f(x)− f(x′)| ≤ b √ log(3ap/δ)‖x− x′‖1. (10)\nNow, we construct a sequence of discretisations Ω(j)t satisfying ‖x(j) − [x(j)]t]‖1 ≤ dj/τt ∀x(j) ∈ Ω (j) t . Here, [x (j)]t is the closest point to x(j) in Ω(j)t in an L2 sense. A sufficient discretisation is a grid with τt uniformly spaced points. Then it follows that for all x ∈ Ωt, ‖x − [x]t‖1 ≤ p/τt. Here Ωt is the discretisation induced on X by the Ω(j)t ’s and [x]t is the closest point to x in Ωt. Note that ‖x(j) − [x(j)]t‖2 ≤ √ dj/τt ∀x(j) ∈ Ω(j) and ‖x − [x]t‖2 ≤ √ p/τt. We will set τt = pt3–therefore, ωtj ≤ (pt3)d ∆ = ωmt. When combining this with (10), we get that with probability > 1 − δ/3,\n|f(x)− f([x])| ≤ b √\nlog(3ap/δ)/t3. By our choice of βt and using Lemma 10 the following is true for all t ≥ 1 and for all x ∈ X with probability > 1− 2δ/3,\n|f(x)− µt−1([x]t)| ≤ |f(x)− f([x]t)|+ |f([x]t)− µt−1([x]t)| ≤ b √ log(3ap/δ)\nt2 + β\n1/2 t M∑ j=1 σ (j) t−1([x (j)]t). (11)\nBy Lemma 11 with probability > 1− δ/3 we have, ∀x ∈ X , |µt−1(x)− µt−1([x]t)| ≤ L ( f(x∗) + η √ 2 log(3πt/2δ) ) √ pη2t2 . (12)\nWe use the above results to obtain the following bound on the instantaneous regret rt which holds with probability > 1− δ for all t ≥ 1,\nrt = f(x∗)− f(xt)\n≤ µt−1([x∗]t) + β1/2t M∑ j=1 σ (j) t−1([x (j) ∗ ]t)− µt−1([xt]t) + β1/2t M∑ j=1 σ (j) t−1([x (j) t ]t) +\n2b √ log(3ap/δ)\nt3\n≤ 2b √ log(3ap/δ)\nt3 + ζ0√ t + β 1/2 t  M∑ j=1 σ (j) t−1(x (j) t ) + M∑ j=1 σ (j) t−1([x (j) t ]t) + µt−1(xt)− µt−1([xt]t) ≤ 2b √ log(3ap/δ) t3 + L ( f(x∗) + η √ 2 log(πt/2δ) ) √ pη2t2 + ζ0√ t + β 1/2 t  M∑ j=1 σ (j) t−1(x (j) t ) + M∑ j=1 σ (j) t−1([x (j) t ]t)\n . (13) In the first step we have applied Equation (11) at x∗ and xt. In the second step we have used the fact that ϕ̃t([x∗]t) ≤ ϕ̃t(x̃t) ≤ ϕ̃t(xt) + ζ0t−1/2. In the third step we have used Equation (12).\nFor any x ∈ X we can bound σt(x)2 as follows,\nσt(x) 2 = η2η−2σt(x) 2 ≤ 1 log(1 + η−2) log ( 1 + η−2σt(x) 2 ) .\nHere we have used the fact that u2 ≤ v2 log(1 + u2)/ log(1 + v2) for u ≤ v and σt(x)2 ≤ κ(x, x) = 1. Write C1 = log\n−1(1 + η−2). By using Jensen’s inequality and Definition 3 for any set of T points {x1, x2, . . . xT } ⊂ X , T∑ t=1 M∑ j=1 σ (j) t (x (j)) 2 ≤MT T∑ t=1 M∑ j=1 σ (j) t (x (j)) 2 ≤ C1MT T∑ t=1 log ( 1 + η−2σt(x) 2 ) ≤ 2C1MTγT . (14)\nFinally we can bound the cumulative regret with probability > 1− δ for all T ≥ 1 by,\nRT = T∑ t=1 rt ≤ C2(a, b,D, L, δ) + ζ0 T∑ t=1 t−1/2 + β 1/2 T  T∑ t=1 M∑ j=1 σ (j) t−1(x (j) t ) + T∑ t=1 M∑ j=1 σ (j) t−1([x (j) t ]t)  ≤ C2(a, b,D, L, δ) + 2ζ0 √ T + √ 8C1βTMTγT .\nwhere we have used the summability of the first two terms in Equation (13). Here, for δ < 0.8, the constant C2 is given by,\nC2 ≥ b √ log(3ap/δ) + π2Lf(x∗)\n6 √ pη2 + Lπ3/2√ 12pδη ."
    } ],
    "references" : [ {
      "title" : "Using Confidence Bounds for Exploitationexploration Trade-offs",
      "author" : [ "Auer", "Peter" ],
      "venue" : "J. Mach. Learn. Res.,",
      "citeRegEx" : "Auer and Peter.,? \\Q2003\\E",
      "shortCiteRegEx" : "Auer and Peter.",
      "year" : 2003
    }, {
      "title" : "Batch Bayesian Optimization via Simulation Matching",
      "author" : [ "Azimi", "Javad", "Fern", "Alan", "Xiaoli Z" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Azimi et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Azimi et al\\.",
      "year" : 2010
    }, {
      "title" : "Algorithms for Hyper-Parameter Optimization",
      "author" : [ "Bergstra", "James S", "Bardenet", "Rémi", "Bengio", "Yoshua", "Kégl", "Balázs" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Bergstra et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Bergstra et al\\.",
      "year" : 2011
    }, {
      "title" : "A Tutorial on Bayesian Optimization of Expensive Cost Functions, with Application to Active User Modeling and Hierarchical Reinforcement Learning",
      "author" : [ "Brochu", "Eric", "Cora", "Vlad M", "de Freitas", "Nando" ],
      "venue" : null,
      "citeRegEx" : "Brochu et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Brochu et al\\.",
      "year" : 2010
    }, {
      "title" : "Convergence Rates of Efficient Global Optimization Algorithms",
      "author" : [ "Bull", "Adam D" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Bull and D.,? \\Q2011\\E",
      "shortCiteRegEx" : "Bull and D.",
      "year" : 2011
    }, {
      "title" : "Joint Optimization and Variable Selection of High-dimensional Gaussian Processes",
      "author" : [ "Chen", "Bo", "Castro", "Rui", "Krause", "Andreas" ],
      "venue" : "In Int’l Conference on Machine Learning,",
      "citeRegEx" : "Chen et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Chen et al\\.",
      "year" : 2012
    }, {
      "title" : "Exponential Regret Bounds for Gaussian Process Bandits with Deterministic Observations",
      "author" : [ "de Freitas", "Nando", "Smola", "Alex J", "Zoghi", "Masrour" ],
      "venue" : "In International Conference on Machine Learning,",
      "citeRegEx" : "Freitas et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Freitas et al\\.",
      "year" : 2012
    }, {
      "title" : "Learning Where to Attend with Deep Architectures for Image Tracking",
      "author" : [ "Denil", "Misha", "Bazzani", "Loris", "Larochelle", "Hugo", "de Freitas", "Nando" ],
      "venue" : "Neural Comput.,",
      "citeRegEx" : "Denil et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Denil et al\\.",
      "year" : 2012
    }, {
      "title" : "High-Dimensional Gaussian Process Bandits",
      "author" : [ "Djolonga", "Josip", "Krause", "Andreas", "Cevher", "Volkan" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Djolonga et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Djolonga et al\\.",
      "year" : 2013
    }, {
      "title" : "Additive gaussian processes",
      "author" : [ "Duvenaud", "David K", "Nickisch", "Hannes", "Rasmussen", "Carl Edward" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Duvenaud et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Duvenaud et al\\.",
      "year" : 2011
    }, {
      "title" : "Posterior consistency of Gaussian process prior for nonparametric binary regression",
      "author" : [ "Ghosal", "Subhashis", "Roy", "Anindya" ],
      "venue" : "Annals of Statistics,",
      "citeRegEx" : "Ghosal et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Ghosal et al\\.",
      "year" : 2006
    }, {
      "title" : "Bayesian Optimization for Synthetic Gene Design",
      "author" : [ "Gonzalez", "Javier", "Longworth", "Joseph", "James", "David", "Lawrence", "Neil" ],
      "venue" : "In NIPS Workshop on Bayesian Optimization in Academia and Industry,",
      "citeRegEx" : "Gonzalez et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Gonzalez et al\\.",
      "year" : 2014
    }, {
      "title" : "A Distribution Free Theory of Nonparametric Regression",
      "author" : [ "Györfi", "László", "Kohler", "Micael", "Krzyzak", "Adam", "Walk", "Harro" ],
      "venue" : "Springer Series in Statistics,",
      "citeRegEx" : "Györfi et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Györfi et al\\.",
      "year" : 2002
    }, {
      "title" : "Generalized Additive Models",
      "author" : [ "T.J. Hastie", "R.J. Tibshirani" ],
      "venue" : null,
      "citeRegEx" : "Hastie and Tibshirani,? \\Q1990\\E",
      "shortCiteRegEx" : "Hastie and Tibshirani",
      "year" : 1990
    }, {
      "title" : "Portfolio Allocation for Bayesian Optimization",
      "author" : [ "Hoffman", "Matthew D", "Brochu", "Eric", "de Freitas", "Nando" ],
      "venue" : "In Uncertainty in Artificial Intelligence,",
      "citeRegEx" : "Hoffman et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Hoffman et al\\.",
      "year" : 2011
    }, {
      "title" : "Automated Antenna Design with Evolutionary Algorithms",
      "author" : [ "G.S. Hornby", "A. Globus", "D.S. Linden", "J.D. Lohn" ],
      "venue" : "American Institute of Aeronautics and Astronautics,",
      "citeRegEx" : "Hornby et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Hornby et al\\.",
      "year" : 2006
    }, {
      "title" : "Lipschitzian Optimization Without the Lipschitz Constant",
      "author" : [ "D.R. Jones", "C.D. Perttunen", "B.E. Stuckman" ],
      "venue" : "J. Optim. Theory Appl.,",
      "citeRegEx" : "Jones et al\\.,? \\Q1993\\E",
      "shortCiteRegEx" : "Jones et al\\.",
      "year" : 1993
    }, {
      "title" : "Efficient global optimization of expensive black-box functions",
      "author" : [ "Jones", "Donald R", "Schonlau", "Matthias", "Welch", "William J" ],
      "venue" : "J. of Global Optimization,",
      "citeRegEx" : "Jones et al\\.,? \\Q1998\\E",
      "shortCiteRegEx" : "Jones et al\\.",
      "year" : 1998
    }, {
      "title" : "Bayesian Active Learning for Posterior Estimation",
      "author" : [ "Kandasamy", "Kirthevasan", "Schneider", "Jeff", "Póczos", "Barnabás" ],
      "venue" : "In International Joint Conference on Artificial Intelligence,",
      "citeRegEx" : "Kandasamy et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Kandasamy et al\\.",
      "year" : 2015
    }, {
      "title" : "Automatic gait optimization with gaussian process regression",
      "author" : [ "Lizotte", "Daniel", "Wang", "Tao", "Bowling", "Michael", "Schuurmans", "Dale" ],
      "venue" : "In in Proc. of IJCAI,",
      "citeRegEx" : "Lizotte et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Lizotte et al\\.",
      "year" : 2007
    }, {
      "title" : "Active Pointillistic Pattern Search",
      "author" : [ "Ma", "Yifei", "Sutherland", "Dougal J", "Garnett", "Roman", "Schneider", "Jeff G" ],
      "venue" : "In International Conference on Artificial Intelligence and Statistics, AISTATS,",
      "citeRegEx" : "Ma et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Ma et al\\.",
      "year" : 2015
    }, {
      "title" : "Adaptive MCMC with Bayesian Optimization",
      "author" : [ "Mahendran", "Nimalan", "Wang", "Ziyu", "Hamze", "Firas", "de Freitas", "Nando" ],
      "venue" : "In Artificial Intelligence and Statistics,",
      "citeRegEx" : "Mahendran et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Mahendran et al\\.",
      "year" : 2012
    }, {
      "title" : "Active Policy Learning for Robot Planning and Exploration under Uncertainty",
      "author" : [ "R. Martinez-Cantin", "N. de Freitas", "A. Doucet", "J. Castellanos" ],
      "venue" : "In Proceedings of Robotics: Science and Systems,",
      "citeRegEx" : "Martinez.Cantin et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Martinez.Cantin et al\\.",
      "year" : 2007
    }, {
      "title" : "Bayesian approach to global optimization and application to multiobjective and constrained problems",
      "author" : [ "J.B. Mockus", "L.J. Mockus" ],
      "venue" : "Journal of Optimization Theory and Applications,",
      "citeRegEx" : "Mockus and Mockus,? \\Q1991\\E",
      "shortCiteRegEx" : "Mockus and Mockus",
      "year" : 1991
    }, {
      "title" : "Application of Bayesian approach to numerical methods of global and stochastic optimization",
      "author" : [ "Mockus", "Jonas" ],
      "venue" : "Journal of Global Optimization,",
      "citeRegEx" : "Mockus and Jonas.,? \\Q1994\\E",
      "shortCiteRegEx" : "Mockus and Jonas.",
      "year" : 1994
    }, {
      "title" : "Active Learning of Model Evidence Using Bayesian Quadrature",
      "author" : [ "M. Osborne", "D. Duvenaud", "R. Garnett", "C. Rasmussen", "S. Roberts", "Z. Ghahramani" ],
      "venue" : "In Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "Osborne et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Osborne et al\\.",
      "year" : 2012
    }, {
      "title" : "A Bayesian model selection analysis of WMAP3",
      "author" : [ "Parkinson", "David", "Mukherjee", "Pia", "Liddle", "Andrew R" ],
      "venue" : "Physical Review,",
      "citeRegEx" : "Parkinson et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Parkinson et al\\.",
      "year" : 2006
    }, {
      "title" : "Gaussian Processes for Machine Learning. Adaptative computation and machine learning series",
      "author" : [ "C.E. Rasmussen", "C.K.I. Williams" ],
      "venue" : null,
      "citeRegEx" : "Rasmussen and Williams,? \\Q2006\\E",
      "shortCiteRegEx" : "Rasmussen and Williams",
      "year" : 2006
    }, {
      "title" : "Sparse Additive Models",
      "author" : [ "Ravikumar", "Pradeep", "Lafferty", "John", "Liu", "Han", "Wasserman", "Larry" ],
      "venue" : "Journal of the Royal Statistical Society: Series B (Statistical Methodology),",
      "citeRegEx" : "Ravikumar et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Ravikumar et al\\.",
      "year" : 2009
    }, {
      "title" : "Information Consistency of Nonparametric Gaussian Process Methods",
      "author" : [ "Seeger", "MW", "Kakade", "SM", "Foster", "DP" ],
      "venue" : "IEEE Transactions on Information Theory,",
      "citeRegEx" : "Seeger et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Seeger et al\\.",
      "year" : 2008
    }, {
      "title" : "Practical Bayesian Optimization of Machine Learning Algorithms",
      "author" : [ "Snoek", "Jasper", "Larochelle", "Hugo", "Adams", "Ryan P" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Snoek et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Snoek et al\\.",
      "year" : 2012
    }, {
      "title" : "Gaussian Process Optimization in the Bandit Setting: No Regret and Experimental Design",
      "author" : [ "Srinivas", "Niranjan", "Krause", "Andreas", "Kakade", "Sham", "Seeger", "Matthias" ],
      "venue" : "In International Conference on Machine Learning,",
      "citeRegEx" : "Srinivas et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Srinivas et al\\.",
      "year" : 2010
    }, {
      "title" : "Cosmological Constraints from the SDSS Luminous Red Galaxies",
      "author" : [ "M. Tegmark et al" ],
      "venue" : "Physical Review,",
      "citeRegEx" : "al,? \\Q2006\\E",
      "shortCiteRegEx" : "al",
      "year" : 2006
    }, {
      "title" : "On the Likelihood that one Unknown Probability Exceeds",
      "author" : [ "W.R. Thompson" ],
      "venue" : "Another in View of the Evidence of Two Samples. Biometrika,",
      "citeRegEx" : "Thompson,? \\Q1933\\E",
      "shortCiteRegEx" : "Thompson",
      "year" : 1933
    }, {
      "title" : "Rapid Object Detection using a Boosted Cascade of Simple Features",
      "author" : [ "Viola", "Paul A", "Jones", "Michael J" ],
      "venue" : "In Computer Vision and Pattern Recognition,",
      "citeRegEx" : "Viola et al\\.,? \\Q2001\\E",
      "shortCiteRegEx" : "Viola et al\\.",
      "year" : 2001
    }, {
      "title" : "Bayesian Optimization in High Dimensions via Random Embeddings",
      "author" : [ "Wang", "Ziyu", "Zoghi", "Masrour", "Hutter", "Frank", "Matheson", "David", "de Freitas", "Nando" ],
      "venue" : "In International Joint Conference on Artificial Intelligence,",
      "citeRegEx" : "Wang et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Wang et al\\.",
      "year" : 2013
    }, {
      "title" : "Making a Science of Model Search: Hyperparameter Optimization in Hundreds of Dimensions for Vision Architectures",
      "author" : [ "Yamins", "Daniel", "Tax", "David", "Bergstra", "James S" ],
      "venue" : "In International Conference on Machine Learning,",
      "citeRegEx" : "Yamins et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Yamins et al\\.",
      "year" : 2013
    }, {
      "title" : "η−2) and C2 is a constant depending on a, b, D, δ, L and η. Proof. Srinivas et al. (2010) bound the regret for exact maximisation of the GP-UCB acquisition φt. By following an analysis similar to our proof of Theorem",
      "author" : [ "T + C" ],
      "venue" : null,
      "citeRegEx" : "C2.,? \\Q2010\\E",
      "shortCiteRegEx" : "C2.",
      "year" : 2010
    }, {
      "title" : "The eigenvalues and eigenfunctions for the kernel are defined with respect to a base distribution on X . In the development of Theorem 8, Srinivas et al. (2010) draw nT samples from the uniform distribution on",
      "author" : [ "Seeger" ],
      "venue" : "X . Hence,",
      "citeRegEx" : "Seeger,? \\Q2008\\E",
      "shortCiteRegEx" : "Seeger",
      "year" : 2008
    }, {
      "title" : "argue that the uniform distribution still satisfies the required tail constraints and therefore",
      "author" : [ "case", "Srinivas" ],
      "venue" : null,
      "citeRegEx" : "case and Srinivas,? \\Q2010\\E",
      "shortCiteRegEx" : "case and Srinivas",
      "year" : 2010
    } ],
    "referenceMentions" : [ {
      "referenceID" : 30,
      "context" : "Gaussian process bandits and Bayesian optimisation (GPB/ BO) have been successfully applied in many applications such as tuning hyperparameters in learning algorithms (Snoek et al., 2012; Bergstra et al., 2011; Mahendran et al., 2012), robotics (Lizotte et al.",
      "startOffset" : 167,
      "endOffset" : 234
    }, {
      "referenceID" : 2,
      "context" : "Gaussian process bandits and Bayesian optimisation (GPB/ BO) have been successfully applied in many applications such as tuning hyperparameters in learning algorithms (Snoek et al., 2012; Bergstra et al., 2011; Mahendran et al., 2012), robotics (Lizotte et al.",
      "startOffset" : 167,
      "endOffset" : 234
    }, {
      "referenceID" : 21,
      "context" : "Gaussian process bandits and Bayesian optimisation (GPB/ BO) have been successfully applied in many applications such as tuning hyperparameters in learning algorithms (Snoek et al., 2012; Bergstra et al., 2011; Mahendran et al., 2012), robotics (Lizotte et al.",
      "startOffset" : 167,
      "endOffset" : 234
    }, {
      "referenceID" : 19,
      "context" : ", 2012), robotics (Lizotte et al., 2007; Martinez-Cantin et al., 2007) and object tracking (Denil et al.",
      "startOffset" : 18,
      "endOffset" : 70
    }, {
      "referenceID" : 22,
      "context" : ", 2012), robotics (Lizotte et al., 2007; Martinez-Cantin et al., 2007) and object tracking (Denil et al.",
      "startOffset" : 18,
      "endOffset" : 70
    }, {
      "referenceID" : 7,
      "context" : ", 2007) and object tracking (Denil et al., 2012).",
      "startOffset" : 28,
      "endOffset" : 48
    }, {
      "referenceID" : 35,
      "context" : "However, all such successes have been in low (typically < 10) dimensions (Wang et al., 2013).",
      "startOffset" : 73,
      "endOffset" : 92
    }, {
      "referenceID" : 36,
      "context" : "Expensive high dimensional functions occur in several problems in fields such as computer vision (Yamins et al., 2013), antenna design (Hornby et al.",
      "startOffset" : 97,
      "endOffset" : 118
    }, {
      "referenceID" : 15,
      "context" : ", 2013), antenna design (Hornby et al., 2006), computational astrophysics (Parkinson et al.",
      "startOffset" : 24,
      "endOffset" : 45
    }, {
      "referenceID" : 26,
      "context" : ", 2006), computational astrophysics (Parkinson et al., 2006) and biology (Gonzalez et al.",
      "startOffset" : 36,
      "endOffset" : 60
    }, {
      "referenceID" : 11,
      "context" : ", 2006) and biology (Gonzalez et al., 2014).",
      "startOffset" : 20,
      "endOffset" : 43
    }, {
      "referenceID" : 31,
      "context" : "Even current theoretical results suggest that GPB/ BO is exponentially difficult in high dimensions without further assumptions (Srinivas et al., 2010; Bull, 2011).",
      "startOffset" : 128,
      "endOffset" : 163
    }, {
      "referenceID" : 12,
      "context" : "Nonparametric regression is inherently difficult in high dimensions with known lower bounds depending exponentially in dimension (Györfi et al., 2002).",
      "startOffset" : 129,
      "endOffset" : 150
    }, {
      "referenceID" : 25,
      "context" : "GPB/ BO methods follow a family of GP based active learning methods which select the next experiment based on the posterior (Osborne et al., 2012; Ma et al., 2015; Kandasamy et al., 2015).",
      "startOffset" : 124,
      "endOffset" : 187
    }, {
      "referenceID" : 20,
      "context" : "GPB/ BO methods follow a family of GP based active learning methods which select the next experiment based on the posterior (Osborne et al., 2012; Ma et al., 2015; Kandasamy et al., 2015).",
      "startOffset" : 124,
      "endOffset" : 187
    }, {
      "referenceID" : 18,
      "context" : "GPB/ BO methods follow a family of GP based active learning methods which select the next experiment based on the posterior (Osborne et al., 2012; Ma et al., 2015; Kandasamy et al., 2015).",
      "startOffset" : 124,
      "endOffset" : 187
    }, {
      "referenceID" : 17,
      "context" : "In the GPB/ BO setting, common acquisition functions include Expected improvement (Mockus, 1994), probability of improvement (Jones et al., 1998), Thompson sampling (Thompson, 1933) and upper confidence bound (Auer, 2003).",
      "startOffset" : 125,
      "endOffset" : 145
    }, {
      "referenceID" : 33,
      "context" : ", 1998), Thompson sampling (Thompson, 1933) and upper confidence bound (Auer, 2003).",
      "startOffset" : 27,
      "endOffset" : 43
    }, {
      "referenceID" : 14,
      "context" : "Some literature studies variants, such as combining several acquisition functions (Hoffman et al., 2011) and querying in batches (Azimi et al.",
      "startOffset" : 82,
      "endOffset" : 104
    }, {
      "referenceID" : 1,
      "context" : ", 2011) and querying in batches (Azimi et al., 2010).",
      "startOffset" : 32,
      "endOffset" : 52
    }, {
      "referenceID" : 13,
      "context" : "In the GPB/ BO setting, common acquisition functions include Expected improvement (Mockus, 1994), probability of improvement (Jones et al., 1998), Thompson sampling (Thompson, 1933) and upper confidence bound (Auer, 2003). Of particular interest to us, is the Gaussian process upper confidence bound (GPUCB). It was first proposed and analysed in the noisy setting by Srinivas et al. (2010) and extended to the noiseless case by de Freitas et al.",
      "startOffset" : 126,
      "endOffset" : 391
    }, {
      "referenceID" : 5,
      "context" : "(2010) and extended to the noiseless case by de Freitas et al. (2012). Some literature studies variants, such as combining several acquisition functions (Hoffman et al.",
      "startOffset" : 48,
      "endOffset" : 70
    }, {
      "referenceID" : 5,
      "context" : "To our knowledge, most literature for GPB/ BO in high dimensions are in the setting where the function varies only along a very low dimensional subspace (Chen et al., 2012; Wang et al., 2013; Djolonga et al., 2013).",
      "startOffset" : 153,
      "endOffset" : 214
    }, {
      "referenceID" : 35,
      "context" : "To our knowledge, most literature for GPB/ BO in high dimensions are in the setting where the function varies only along a very low dimensional subspace (Chen et al., 2012; Wang et al., 2013; Djolonga et al., 2013).",
      "startOffset" : 153,
      "endOffset" : 214
    }, {
      "referenceID" : 8,
      "context" : "To our knowledge, most literature for GPB/ BO in high dimensions are in the setting where the function varies only along a very low dimensional subspace (Chen et al., 2012; Wang et al., 2013; Djolonga et al., 2013).",
      "startOffset" : 153,
      "endOffset" : 214
    }, {
      "referenceID" : 5,
      "context" : "general than the setting in Chen et al. (2012). Even though it does not contain the settings in Djolonga et al.",
      "startOffset" : 28,
      "endOffset" : 47
    }, {
      "referenceID" : 5,
      "context" : "general than the setting in Chen et al. (2012). Even though it does not contain the settings in Djolonga et al. (2013); Wang et al.",
      "startOffset" : 28,
      "endOffset" : 119
    }, {
      "referenceID" : 5,
      "context" : "general than the setting in Chen et al. (2012). Even though it does not contain the settings in Djolonga et al. (2013); Wang et al. (2013), unlike them, we still allow the function to vary along the entire domain.",
      "startOffset" : 28,
      "endOffset" : 139
    }, {
      "referenceID" : 30,
      "context" : "Using an additive structure is standard in high dimensional regression literature both in the GP framework and otherwise. Hastie & Tibshirani (1990); Ravikumar et al.",
      "startOffset" : 57,
      "endOffset" : 149
    }, {
      "referenceID" : 27,
      "context" : "Hastie & Tibshirani (1990); Ravikumar et al. (2009) treat the function as a sum of one dimensional components.",
      "startOffset" : 28,
      "endOffset" : 52
    }, {
      "referenceID" : 9,
      "context" : "Duvenaud et al. (2011) assume a sum of functions of all combinations of lower dimensional coordinates.",
      "startOffset" : 0,
      "endOffset" : 23
    }, {
      "referenceID" : 3,
      "context" : "Common techniques to maximise φt include grid search, Monte Carlo and multistart methods (Brochu et al., 2010).",
      "startOffset" : 89,
      "endOffset" : 110
    }, {
      "referenceID" : 3,
      "context" : "Common techniques to maximise φt include grid search, Monte Carlo and multistart methods (Brochu et al., 2010). In our work we use the Dividing Rectangles (DiRect) algorithm of Jones et al. (1993). While these methods are efficient in low dimensions they require exponential computation in high dimensions.",
      "startOffset" : 90,
      "endOffset" : 197
    }, {
      "referenceID" : 31,
      "context" : "Following Srinivas et al. (2010), we first bound the statistical difficulty of the problem as determined by the kernel.",
      "startOffset" : 10,
      "endOffset" : 33
    }, {
      "referenceID" : 31,
      "context" : "Srinivas et al. (2010) showed that the statistical difficulty of GPB/ BO is determined by the Maximum Information Gain as defined below.",
      "startOffset" : 0,
      "endOffset" : 23
    }, {
      "referenceID" : 31,
      "context" : "In contrast, for a D order kernel this is exponential (Srinivas et al., 2010).",
      "startOffset" : 54,
      "endOffset" : 77
    }, {
      "referenceID" : 29,
      "context" : "We use bounds on the eigenvalues of the SE and Matérn kernels from Seeger et al. (2008) and a result from Srinivas et al.",
      "startOffset" : 67,
      "endOffset" : 88
    }, {
      "referenceID" : 29,
      "context" : "We use bounds on the eigenvalues of the SE and Matérn kernels from Seeger et al. (2008) and a result from Srinivas et al. (2010) which bounds the information gain via the eigendecay of the kernel.",
      "startOffset" : 67,
      "endOffset" : 129
    }, {
      "referenceID" : 31,
      "context" : "Part of our proof uses ideas from Srinivas et al. (2010). We show that ∑ j βtσ (j) t−1(·) forms a credible interval for f(·) about the posterior mean μt(·) for an additive kernel in Add-GP-UCB.",
      "startOffset" : 34,
      "endOffset" : 57
    }, {
      "referenceID" : 9,
      "context" : "One could consider alternative lower order kernels – one candidate is the sum of all possible d order kernels (Duvenaud et al., 2011).",
      "startOffset" : 110,
      "endOffset" : 133
    }, {
      "referenceID" : 31,
      "context" : "Choice of βt: βt as specified by Theorems 5, usually tends to be conservative in practice (Srinivas et al., 2010).",
      "startOffset" : 90,
      "endOffset" : 113
    }, {
      "referenceID" : 32,
      "context" : "Data dependent prior: Our analysis assumes that we know the GP kernel of the prior. In reality this is rarely the case. In our experiments, we choose the hyperparameters of the kernel by maximising the GP marginal likelihood (Rasmussen & Williams, 2006) every Ncyc iterations. Initialisation: Marginal likelihood based kernel tuning can be unreliable with few data points. This is a problem in the first few iterations. Following the recommendations in Bull (2011) we initialise Add-GP-UCB (and GP-UCB) using Ninit points selected uniformly at random.",
      "startOffset" : 28,
      "endOffset" : 465
    }, {
      "referenceID" : 3,
      "context" : "Following, Brochu et al. (2010) we use DiRect to maximise φt, φ̃t.",
      "startOffset" : 11,
      "endOffset" : 32
    }, {
      "referenceID" : 3,
      "context" : "Following, Brochu et al. (2010) we use DiRect to maximise φt, φ̃t. We compare Add-GP-UCB against GP-UCB, random querying (RAND) and DiRect3. On the real datasets we also compare it to the Expected Improvement (GP-EI) acquisition function which is popular in BO applications and the method of Wang et al. (2013) which uses a random projection before applying BO (REMBO).",
      "startOffset" : 11,
      "endOffset" : 311
    }, {
      "referenceID" : 32,
      "context" : "Here we used Galaxy data from the Sloan Digital Sky Survey (SDSS). The task is to find the maximum likelihood estimators for a simulation based astrophysical likelihood model. Data and software for computing the likelihood are taken from Tegmark et al (2006). The software itself takes in only 9 parameters but we augment this to 20 dimensions to emulate the fact that in practical astrophysical problems we may not know the true parameters on which the problem is dependent.",
      "startOffset" : 14,
      "endOffset" : 259
    } ],
    "year" : 2016,
    "abstractText" : "Bayesian Optimisation (BO) is a technique used in optimising a D-dimensional function which is typically expensive to evaluate. While there have been many successes for BO in low dimensions, scaling it to high dimensions has been notoriously difficult. Existing literature on the topic are under very restrictive settings. In this paper, we identify two key challenges in this endeavour. We tackle these challenges by assuming an additive structure for the function. This setting is substantially more expressive and contains a richer class of functions than previous work. We prove that, for additive functions the regret has only linear dependence on D even though the function depends on all D dimensions. We also demonstrate several other statistical and computational benefits in our framework. Via synthetic examples, a scientific simulation and a face detection problem we demonstrate that our method outperforms naive BO on additive functions and on several examples where the function is not additive.",
    "creator" : "LaTeX with hyperref package"
  }
}