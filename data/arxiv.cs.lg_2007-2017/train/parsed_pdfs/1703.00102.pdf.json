{
  "name" : "1703.00102.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "SARAH: A Novel Method for Machine Learning Problems  Using Stochastic Recursive Gradient",
    "authors" : [ "LamM. Nguyen", "Jie Liu", "Katya Scheinberg", "Martin Takáč" ],
    "emails" : [ "guyen.mltd@gmail.com>,", "<jie.liu.2018@gmail.com>,", "<katyas@lehigh.edu>,", "<Takac.MT@gmail.com>." ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n70 3.\n00 10\n2v 2\n[ st\nat .M\nL ]\n3 J\nun 2\n01 7\nsive grAdient algoritHm (SARAH), as well as its practical variant SARAH+, as a novel approach to the finite-sum minimization problems. Different from the vanilla SGD and other modern stochastic methods such as SVRG, S2GD, SAG and SAGA, SARAH admits a simple recursive framework for updating stochastic gradient estimates; when comparing to SAG/SAGA, SARAH does not require a storage of past gradients. The linear convergence rate of SARAH is proven under strong convexity assumption. We also prove a linear convergence rate (in the strongly convex case) for an inner loop of SARAH, the property that SVRG does not possess. Numerical experiments demonstrate the efficiency of our algorithm."
    }, {
      "heading" : "1. Introduction",
      "text" : "We are interested in solving a problem of the form\nmin w∈Rd\n\n\n\nP (w) def =\n1\nn\n∑\ni∈[n]\nfi(w)\n\n\n\n, (1)\nwhere each fi, i ∈ [n] def= {1, . . . , n}, is convex with a Lipschitz continuous gradient. Throughout the paper, we assume that there exists an optimal solution w∗ of (1).\n1Department of Industrial and Systems Engineering, Lehigh University, USA. 2On leave at The University of Oxford, UK. All authors were supported by NSF Grant CCF-1618717. Katya Scheinberg was partially supported by NSF Grants DMS 13-19356, CCF-1320137 and CCF1618717. Correspondence to: Lam M. Nguyen <lamnguyen.mltd@gmail.com>, Jie Liu <jie.liu.2018@gmail.com>, Katya Scheinberg <katyas@lehigh.edu>, Martin Takáč <Takac.MT@gmail.com>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\nProblems of this type arise frequently in supervised learning applications (Hastie et al., 2009). Given a training set {(xi, yi)}ni=1 with xi ∈ Rd, yi ∈ R, the least squares regression model, for example, is written as (1) with fi(w) def = (xTi w−yi)2+ λ2 ‖w‖2, where ‖·‖ denotes the ℓ2-norm. The ℓ2-regularized logistic regression for binary classification is written with fi(w) def = log(1 + exp(−yixTi w)) + λ2 ‖w‖2 (yi ∈ {−1, 1}). In recent years, many advanced optimization methods have been developed for problem (1). While the objective function is smooth and convex, the traditional optimization methods, such as gradient descent (GD) or Newton method are often impractical for this problem, when n – the number of training samples and hence the number of fi’s – is very large. In particular, GD updates iterates as follows\nwt+1 = wt − ηt∇P (wt), t = 0, 1, 2, . . . .\nUnder strong convexity assumption on P and with appropriate choice of ηt, GD converges at a linear rate in terms of objective function valuesP (wt). However, when n is large, computing∇P (wt) at each iteration can be prohibitive. As an alternative, stochastic gradient descent (SGD)1, originating from the seminal work of Robbins and Monro in 1951 (Robbins & Monro, 1951), has become the method of choice for solving (1). At each step, SGD picks an index i ∈ [n] uniformly at random, and updates the iterate as wt+1 = wt − ηt∇fi(wt), which is up-to n times cheaper than an iteration of a full gradient method. The convergence rate of SGD is slower than that of GD, in particular, it is sublinear in the strongly convex case. The tradeoff, however, is advantageous due to the tremendous per-iteration savings and the fact that low accuracy solutions are sufficient. This trade-off has been thoroughly analyzed in (Bottou, 1998). Unfortunately, in practice SGD method is often too slow and its performance is too sensitive to the variance in the sample gradients ∇fi(wt). Use of mini-batches (averaging multiple sample gradients ∇fi(wt)) was used in (Shalev-Shwartz et al., 2007; 1We mark here that even though stochastic gradient is referred to as SG in literature, the term stochastic gradient descent (SGD) has been widely used in many important works of large-scale learning, including SAG/SAGA, SDCA, SVRG and MISO.\nCotter et al., 2011; Takáč et al., 2013) to reduce the variance and improve convergence rate by constant factors. Using diminishing sequence {ηt} is used to control the variance (Shalev-Shwartz et al., 2011; Bottou et al., 2016), but the practical convergence of SGD is known to be very sensitive to the choice of this sequence, which needs to be hand-picked.\nRecently, a class of more sophisticated algorithms have emerged, which use the specific finite-sum form of (1) and combine some deterministic and stochastic aspects to reduce variance of the steps. The examples of these methods are SAG/SAGA (Le Roux et al., 2012; Defazio et al., 2014), SDCA (Shalev-Shwartz & Zhang, 2013), SVRG (Johnson & Zhang, 2013; Xiao & Zhang, 2014), DIAG (Mokhtari et al., 2017), MISO (Mairal, 2013) and S2GD (Konečný & Richtárik, 2013), all of which enjoy faster convergence rate than that of SGD and use a fixed learning rate parameter η. In this paper we introduce a new method in this category, SARAH, which further improves several aspects of the existing methods. In Table 1 we summarize complexity and some other properties of the existing methods and SARAH when applied to strongly convex problems. Although SVRG and SARAH have the same convergence rate, we introduce a practical variant of SARAH that outperforms SVRG in our experiments.\nIn addition, theoretical results for complexity of the methods or their variants when applied to general convex functions have been derived (Schmidt et al., 2016; Defazio et al., 2014; Reddi et al., 2016; Allen-Zhu & Yuan, 2016; Allen-Zhu, 2017). In Table 2 we summarize the key complexity results, noting that convergence rate is now sublinear.\nOur Contributions. In this paper, we propose a novel algorithm which combines some of the good properties of existing algorithms, such as SAGA and SVRG, while aiming to improve on both of these methods. In particular, our algorithm does not take steps along a stochastic gradient direction, but rather along an accumulated direction using past stochastic gradient information (as in SAGA) and occasional exact gradient information (as in SVRG). We summarize the key properties of the proposed algorithm below.\n• Similarly to SVRG, SARAH’s iterations are divided into the outer loop where a full gradient is computed\nand the inner loop where only stochastic gradient is computed. Unlike the case of SVRG, the steps of the inner loop of SARAH are based on accumulated stochastic information. • Like SAG/SAGA and SVRG, SARAH has a sublinear rate of convergence for general convex functions,\nand a linear rate of convergence for strongly convex functions. • SARAH uses a constant learning rate, whose size is larger than that of SVRG. We analyze and discuss the\noptimal choice of the learning rate and the number of inner loop steps. However, unlike SAG/SAGA but similar to SVRG, SARAH does not require a storage\nof n past stochastic gradients. • We also prove a linear convergence rate (in the strongly convex case) for the inner loop of SARAH,\nthe property that SVRG does not possess. We show that the variance of the steps inside the inner loop goes to zero, thus SARAH is theoretically more stable and reliable than SVRG. • We provide a practical variant of SARAH based on the convergence properties of the inner loop, where\nthe simple stable stopping criterion for the inner loop is used (see Section 4 for more details). This variant shows how SARAH can be made more stable than SVRG in practice."
    }, {
      "heading" : "2. Stochastic Recursive Gradient Algorithm",
      "text" : "Now we are ready to present our SARAH (Algorithm 1).\nThe key step of the algorithm is a recursive update of the stochastic gradient estimate (SARAH update)\nvt = ∇fit(wt)−∇fit(wt−1) + vt−1, (2)\nfollowed by the iterate update:\nwt+1 = wt − ηvt. (3)\nFor comparison, SVRG update can be written in a similar way as\nvt = ∇fit(wt)−∇fit(w0) + v0. (4)\nAlgorithm 1 SARAH\nParameters: the learning rate η > 0 and the inner loop size m. Initialize: w̃0 Iterate: for s = 1, 2, . . . do w0 = w̃s−1 v0 = 1 n ∑n i=1 ∇fi(w0)\nw1 = w0 − ηv0 Iterate: for t = 1, . . . ,m− 1 do Sample it uniformly at random from [n] vt = ∇fit(wt)−∇fit(wt−1) + vt−1 wt+1 = wt − ηvt end for Set w̃s = wt with t chosen uniformly at random from {0, 1, . . . ,m}\nend for\nObserve that in SVRG, vt is an unbiased estimator of the gradient, while it is not true for SARAH. Specifically, 2\nE[vt|Ft] = ∇P (wt)−∇P (wt−1)+vt−1 6= ∇P (wt), (5)\nwhere 3 Ft = σ(w0, i1, i2, . . . , it−1) is the σ-algebra generated by w0, i1, i2, . . . , it−1; F0 = F1 = σ(w0). Hence, SARAH is different from SGD and SVRG type of methods, however, the following total expectation holds, E[vt] = E[∇P (wt)], differentiating SARAH from SAG/SAGA. SARAH is similar to SVRG since they both contain outer loops which require one full gradient evaluation per outer iteration followed by one full gradient descent step with a given learning rate. The difference lies in the inner loop, where SARAH updates the stochastic step direction vt recursively by adding and subtracting component gradients to and from the previous vt−1 (t ≥ 1) in (2). Each inner iteration evaluates 2 stochastic gradients and hence the total work per outer iteration isO(n+m) in terms of the number of gradient evaluations. Note that due to its nature, without running the inner loop, i.e.,m = 1, SARAH reduces to the GD algorithm."
    }, {
      "heading" : "3. Theoretical Analysis",
      "text" : "To proceed with the analysis of the proposed algorithm, we will make the following common assumptions.\nAssumption 1 (L-smooth). Each fi : R d → R, i ∈ [n], is L-smooth, i.e., there exists a constant L > 0 such that\n‖∇fi(w) −∇fi(w′)‖ ≤ L‖w − w′‖, ∀w,w′ ∈ Rd. 2 E[·|Ft] = Eit [·], which is expectation with respect to the random choice of index it (conditioned on w0, i1, i2, . . . , it−1). 3Ft also contains all the information of w0, . . . , wt as well as v0, . . . , vt−1.\nNote that this assumption implies that P (w) = 1 n ∑n i=1 fi(w) is also L-smooth. The following strong convexity assumption will be made for the appropriate parts of the analysis, otherwise, it would be dropped.\nAssumption 2a (µ-strongly convex). The function P : R\nd → R, is µ-strongly convex, i.e., there exists a constant µ > 0 such that ∀w,w′ ∈ Rd,\nP (w) ≥ P (w′) +∇P (w′)T (w − w′) + µ2 ‖w − w ′‖2.\nAnother, stronger, assumption of µ-strong convexity for (1) will also be imposed when required in our analysis. Note that Assumption 2b implies Assumption 2a but not vice versa.\nAssumption 2b. Each function fi : R d → R, i ∈ [n], is strongly convex with µ > 0.\nUnder Assumption 2a, let us define the (unique) optimal solution of (1) as w∗, Then strong convexity of P implies that\n2µ[P (w)− P (w∗)] ≤ ‖∇P (w)‖2, ∀w ∈ Rd. (6)\nWe note here, for future use, that for strongly convex functions of the form (1), arising in machine learning applications, the condition number is defined as κ def = L/µ. Furthermore, we should also notice that Assumptions 2a and 2b both cover a wide range of problems, e.g. l2-regularized empirical risk minimization problems with convex losses.\nFinally, as a special case of the strong convexity of all fi’s with µ = 0, we state the general convexity assumption, which we will use for convergence analysis.\nAssumption 3. Each function fi : R d → R, i ∈ [n], is convex, i.e.,\nfi(w) ≥ fi(w′) +∇fi(w′)T (w − w′), ∀i ∈ [n].\nAgain, we note that Assumption 2b implies Assumption 3, but Assumption 2a does not. Hence in our analysis, depending on the result we aim at, we will require Assumption 3 to hold by itself, or Assumption 2a and Assumption 3 to hold together, or Assumption 2b to hold by itself. We will always use Assumption 1.\nOur iteration complexity analysis aims to bound the number of outer iterations T (or total number of stochastic gradient evaluations) which is needed to guarantee that ‖∇P (wT )‖2 ≤ ǫ. In this case we will say that wT is an ǫ-accurate solution. However, as is common practice for stochastic gradient algorithms, we aim to obtain the bound on the number of iterations, which is required to guarantee the bound on the expected squared norm of a gradient, i.e.,\nE[‖∇P (wT )‖2] ≤ ǫ. (7)"
    }, {
      "heading" : "3.1. Linearly Diminishing Step-Size in a Single Inner Loop",
      "text" : "The most important property of the SVRG algorithm is the variance reduction of the steps. This property holds as the number of outer iteration grows, but it does not hold, if only the number of inner iterations increases. In other words, if we simply run the inner loop for many iterations (without executing additional outer loops), the variance of the steps does not reduce in the case of SVRG, while it goes to zero in the case of SARAH. To illustrate this effect, let us take a look at Figures 1 and 2.\nIn Figure 1, we applied one outer loop of SVRG and SARAH to a sum of 5 quadratic functions in a twodimensional space, where the optimal solution is at the origin, the black lines and black dots indicate the trajectory of each algorithm and the red point indicates the final iterate. Initially, both SVRG and SARAH take steps along stochastic gradient directions towards the optimal solution. However, later iterations of SVRG wander randomly around the origin with large deviation from it, while SARAH follows a much more stable convergent trajectory, with a final iterate falling in a small neighborhood of the optimal solution.\nIn Figure 2, the x-axis denotes the number of effective passes which is equivalent to the number of passes through all of the data in the dataset, the cost of each pass being equal to the cost of one full gradient evaluation; and y-axis represents ‖vt‖2. Figure 2 shows the evolution of ‖vt‖2 for SARAH, SVRG, SGD+ (SGD with decreas-\ning learning rate) and FISTA (an accelerated version of GD (Beck & Teboulle, 2009)) withm = 4n, where the left plot shows the trend over multiple outer iterations and the right plot shows a single outer iteration4. We can see that for SVRG, ‖vt‖2 decreases over the outer iterations, while it has an increasing trend or oscillating trend for each inner loop. In contrast, SARAH enjoys decreasing trends both in the outer and the inner loop iterations.\nWe will now show that the stochastic steps computed by SARAH converge linearly in the inner loop. We present two linear convergence results based on our two different assumptions of µ-strong convexity. These results substantiate our conclusion that SARAH uses more stable stochastic gradient estimates than SVRG. The following theorem is our first result to demonstrate the linear convergence of our stochastic recursive step vt.\nTheorem 1a. Suppose that Assumptions 1, 2a and 3 hold. Consider vt defined by (2) in SARAH (Algorithm 1) with η < 2/L. Then, for any t ≥ 1,\nE[‖vt‖2] ≤ [ 1− ( 2 ηL − 1 ) µ2η2 ] E[‖vt−1‖2]\n≤ [ 1− (\n2 ηL − 1\n) µ2η2 ]t E[‖∇P (w0)‖2].\nThis result implies that by choosing η = O(1/L), we obtain the linear convergence of ‖vt‖2 in expectation with the rate (1 − 1/κ2). Below we show that a better convergence rate can be obtained under a stronger convexity assumption.\nTheorem 1b. Suppose that Assumptions 1 and 2b hold. Consider vt defined by (2) in SARAH (Algorithm 1) with η ≤ 2/(µ+ L). Then the following bound holds, ∀ t ≥ 1,\nE[‖vt‖2] ≤ ( 1− 2µLηµ+L ) E[‖vt−1‖2]\n≤ ( 1− 2µLηµ+L )t E[‖∇P (w0)‖2].\nAgain, by setting η = O(1/L), we derive the linear convergence with the rate of (1 − 1/κ), which is a significant improvement over the result of Theorem 1a, when the problem is severely ill-conditioned."
    }, {
      "heading" : "3.2. Convergence Analysis",
      "text" : "In this section, we derive the general convergence rate results for Algorithm 1. First, we present two important Lemmas as the foundation of our theory. Then, we proceed to prove sublinear convergence rate of a single outer iteration when applied to general convex functions. In the end, we prove that the algorithm with multiple outer iterations has linear convergence rate in the strongly convex case.\n4In the plots of Figure 2, since the data for SVRG is noisy, we smooth it by using moving average filters with spans 100 for the left plot and 10 for the right one.\nWe begin with proving two useful lemmas that do not require any convexity assumption. The first Lemma 1 bounds the sum of expected values of ‖∇P (wt)‖2. The second, Lemma 2, bounds E[‖∇P (wt)− vt‖2]. Lemma 1. Suppose that Assumption 1 holds. Consider SARAH (Algorithm 1). Then, we have\nm ∑\nt=0\nE[‖∇P (wt)‖2] ≤ 2\nη E[P (w0)− P (w∗)] (8)\n+\nm ∑\nt=0\nE[‖∇P (wt)− vt‖2]− (1− Lη) m ∑\nt=0\nE[‖vt‖2].\nLemma 2. Suppose that Assumption 1 holds. Consider vt defined by (2) in SARAH (Algorithm 1). Then for any t ≥ 1,\nE[‖∇P (wt)− vt‖2] = t ∑\nj=1\nE[‖vj − vj−1‖2]\n− t ∑\nj=1\nE[‖∇P (wj)−∇P (wj−1)‖2].\nNow we are ready to provide our main theoretical results."
    }, {
      "heading" : "3.2.1. GENERAL CONVEX CASE",
      "text" : "Following from Lemma 2, we can obtain the following upper bound for E[‖∇P (wt) − vt‖2] for convex functions fi, i ∈ [n]. Lemma 3. Suppose that Assumptions 1 and 3 hold. Consider vt defined as (2) in SARAH (Algorithm 1) with η < 2/L. Then we have that for any t ≥ 1,\nE[‖∇P (wt)− vt‖2] ≤ ηL 2− ηL [ E[‖v0‖2]− E[‖vt‖2] ]\n≤ ηL 2− ηLE[‖v0‖ 2]. (9)\nUsing the above lemmas, we can state and prove one of our core theorems as follows.\nTheorem 2. Suppose that Assumptions 1 and 3 hold. Consider SARAH (Algorithm 1) with η ≤ 1/L. Then for any s ≥ 1, we have\nE[‖∇P (w̃s)‖2] ≤ 2\nη(m+ 1) E[P (w̃s−1)− P (w∗)]\n+ ηL\n2− ηLE[‖∇P (w̃s−1)‖ 2]. (10)\nProof. Since v0 = ∇P (w0) implies ‖∇P (w0)− v0‖2 = 0 then by Lemma 3, we can write\n∑m t=0E[‖∇P (wt)− vt‖2] ≤ mηL2−ηLE[‖v0‖2]. (11)\nHence, by Lemma 1 with η ≤ 1/L, we have ∑m\nt=0E[‖∇P (wt)‖2] ≤ 2ηE[P (w0)− P (w ∗)] + ∑m\nt=0E[‖∇P (wt)− vt‖2] (11) ≤ 2ηE[P (w0)− P (w ∗)] + mηL2−ηLE[‖v0‖ 2]. (12) Since we are considering one outer iteration, with s ≥ 1, then we have v0 = ∇P (w0) = ∇P (w̃s−1) (since w0 = w̃s−1), and w̃s = wt, where t is picked uniformly at random from {0, 1, . . . ,m}. Therefore, the following holds,\nE[‖∇P (w̃s)‖2] = 1m+1 ∑m t=0E[‖∇P (wt)‖2] (12)\n≤ 2η(m+1)E[P (w̃s−1)− P (w ∗)]\n+ ηL2−ηLE[‖∇P (w̃s−1)‖ 2].\nTheorem 2, in the case when η ≤ 1/L implies that\nE[‖∇P (w̃s)‖2] ≤ 2η(m+1)E[P (w̃s−1)− P (w ∗)]\n+ ηLE[‖∇P (w̃s−1)‖2].\nBy choosing the learning rate η = √\n2 L(m+1) (withm such\nthat √\n2 L(m+1) ≤ 1/L) we can derive the following con-\nvergence result,\nE[‖∇P (w̃s)‖2]\n≤ √\n2L m+1E[P (w̃s−1)− P (w∗) + ‖∇P (w̃s−1)‖2].\nClearly, this result shows a sublinear convergence rate for SARAH under general convexity assumption within a single inner loop, with increasing m, and consequently, we have the following result for complexity bound.\nCorollary 1. Suppose that Assumptions 1 and 3 hold. Consider SARAH (Algorithm 1) within a single outer iteration with the learning rate η = √\n2 L(m+1) wherem ≥ 2L−1 is\nthe total number of iterations, then ‖∇P (wt)‖2 converges sublinearly in expectation with a rate of √\n2L m+1 , and there-\nfore, the total complexity to achieve an ǫ-accurate solution defined in (7) is O(n+ 1/ǫ2).\nWe now turn to estimating convergence of SARAH with multiple outer steps. Simply using Theorem 2 for each of the outer steps we have the following result.\nTheorem 3. Suppose that Assumptions 1 and 3 hold. Consider SARAH (Algorithm 1) and define\nδk = 2 η(m+1)E[P (w̃k)− P (w ∗)], k = 0, 1, . . . , s− 1,\nand δ = max0≤k≤s−1 δk. Then we have\nE[‖∇P (w̃s)‖2]−∆ ≤ αs(‖∇P (w̃0)‖2 −∆), (13)\nwhere ∆ = δ (\n1 + ηL2(1−ηL)\n)\n, and α = ηL2−ηL .\nBased on Theorem 3, we have the following total complexity for SARAH in the general convex case.\nCorollary 2. Let us choose ∆ = ǫ/4, α = 1/2 (with η = 2/(3L)), and m = O(1/ǫ) in Theorem 3. Then, the total complexity to achieve an ǫ-accuracy solution defined in (7) is O((n+ (1/ǫ)) log(1/ǫ))."
    }, {
      "heading" : "3.2.2. STRONGLY CONVEX CASE",
      "text" : "We now turn to the discussion of the linear convergence rate of SARAH under the strong convexity assumption on P . From Theorem 2, for any s ≥ 1, using property (6) of the µ-strongly convex P , we have\nE[‖∇P (w̃s)‖2] ≤ 2η(m+1)E[P (w̃s−1)− P (w ∗)]\n+ ηL2−ηLE[‖∇P (w̃s−1)‖ 2]\n(6) ≤ (\n1 µη(m+1) + ηL 2−ηL\n)\nE[‖∇P (w̃s−1)‖2],\nand equivalently,\nE[‖∇P (w̃s)‖2] ≤ σm E[‖∇P (w̃s−1)‖2]. (14)\nLet us define σm def = 1µη(m+1) + ηL 2−ηL . Then by choosing η and m such that σm < 1, and applying (14) recursively, we are able to reach the following convergence result.\nTheorem 4. Suppose that Assumptions 1, 2a and 3 hold. Consider SARAH (Algorithm 1) with the choice of η andm such that\nσm def =\n1\nµη(m+ 1) +\nηL\n2− ηL < 1. (15)\nThen, we have\nE[‖∇P (w̃s)‖2] ≤ (σm)s‖∇P (w̃0)‖2.\nRemark 1. Theorem 4 implies that any η < 1/L will work for SARAH. Let us compare our convergence rate to that of SVRG. The linear rate of SVRG, as presented in (Johnson & Zhang, 2013), is given by\nαm = 1 µη(1−2Lη)m + 2ηL 1−2ηL < 1.\nWe observe that it implies that the learning rate has to satisfy η < 1/(4L), which is a tighter restriction than\nη < 1/L required by SARAH. In addition, with the same values of m and η, the rate or convergence of (the outer iterations) of SARAH is always smaller than that of SVRG.\nσm = 1 µη(m+1) + ηL 2−ηL = 1 µη(m+1) + 1 2/(ηL)−1\n< 1µη(1−2Lη)m + 1 0.5/(ηL)−1 = αm.\nRemark 2. To further demonstrate the better convergence properties of SARAH, let us consider following optimization problem\nmin 0<η<1/L σm, min 0<η<1/4L αm,\nwhich can be interpreted as the best convergence rates for different values of m, for both SARAH and SVRG. After simple calculations, we plot both learning rates and the corresponding theoretical rates of convergence, as shown in Figure 3, where the right plot is a zoom-in on a part of the middle plot. The left plot shows that the optimal learning rate for SARAH is significantly larger than that of SVRG, while the other two plots show significant improvement upon outer iteration convergence rates for SARAH over SVRG.\nBased on Theorem 4, we are able to derive the following total complexity for SARAH in the strongly convex case.\nCorollary 3. Fix ǫ ∈ (0, 1), and let us run SARAH with η = 1/(2L) and m = 4.5κ for T iterations where T = ⌈log(‖∇P (w̃0)‖2/ǫ)/ log(9/7)⌉, then we can derive an ǫ-accuracy solution defined in (7). Furthermore, we can obtain the total complexity of SARAH, to achieve the ǫ-accuracy solution, as O ((n+ κ) log(1/ǫ)) ."
    }, {
      "heading" : "4. A Practical Variant",
      "text" : "While SVRG is an efficient variance-reducing stochastic gradient method, one of its main drawbacks is the sensitivity of the practical performance with respect to the choice of m. It is know that m should be around O(κ),5 while it still remains unknown that what the exact best choice is. In this section, we propose a practical variant of SARAH as\n5 In practice, when n is large, P (w) is often considered as a regularized Empirical Loss Minimization problem with regularization parameter λ = 1\nn , then κ ∼ O(n).\nSARAH+ (Algorithm 2), which provides an automatic and adaptive choice of the inner loop sizem. Guided by the linear convergenceof the steps in the inner loop, demonstrated in Figure 2, we introduce a stopping criterion based on the values of ‖vt‖2 while upper-bounding the total number of steps by a large enoughm for robustness. The other modification compared to SARAH (Algorithm 1) is the more practical choice w̃s = wt, where t is the last index of the particular inner loop, instead of randomly selected intermediate index.\nAlgorithm 2 SARAH+\nParameters: the learning rate η > 0, 0 < γ ≤ 1 and the maximum inner loop size m. Initialize: w̃0 Iterate: for s = 1, 2, . . . do w0 = w̃s−1 v0 = 1 n ∑n i=1 ∇fi(w0)\nw1 = w0 − ηv0 t = 1 while ‖vt−1‖2 > γ‖v0‖2 and t < m do Sample it uniformly at random from [n] vt = ∇fit(wt)−∇fit(wt−1) + vt−1 wt+1 = wt − ηvt t = t+ 1 end while\nSet w̃s = wt end for\nDifferent from SARAH, SARAH+ provides a possibility of earlier termination and unnecessary careful choices of m, and it also covers the classical gradient descent when we set γ = 1 (since the while loop does not proceed). In Figure 4 we present the numerical performance of SARAH+ with different γs on rcv1 and news20 datasets. The size of the inner loop provides a trade-off between the fast sublinear convergence in the inner loop and linear convergence in the outer loop. From the results, it appears that γ = 1/8 is the optimal choice. With a larger γ, i.e. γ > 1/8, the iterates in the inner loop do not provide sufficient reduction, before another full gradient computation is required, while with γ < 1/8 an unnecessary number of inner steps is performed without gaining substantial progress. Clearly γ is another parameter that requires tuning, however, in our experiments, the performance of SARAH+ has been very robust with respect to the choices of γ and did not vary much from one data set to another.\nSimilarly to SVRG, ‖vt‖2 decreases in the outer iterations of SARAH+. However, unlike SVRG, SARAH+ also inherits from SARAH the consistent decrease of ‖vt‖2 in expectation in the inner loops. It is not possible to apply the same idea of adaptively terminating the inner loop of\nSVRG based on the reduction in ‖vt‖2, as ‖vt‖2 may have side fluctuations as shown in Figure 2."
    }, {
      "heading" : "5. Numerical Experiments",
      "text" : "To support the theoretical analyses and insights, we present our empirical experiments, comparing SARAH and SARAH+ with the state-of-the-art first-order methods for ℓ2-regularized logistic regression problems with\nfi(w) = log(1 + exp(−yixTi w)) + λ2 ‖w‖ 2,\non datasets covtype, ijcnn1, news20 and rcv1 6. For ijcnn1 and rcv1 we use the predefined testing and training sets, while covtype and news20 do not have test data, hence we randomly split the datasets with 70% for training and 30% for testing. Some statistics of the datasets are summarized in Table 3.\nThe penalty parameter λ is set to 1/n as is common practice (Le Roux et al., 2012). Note that like SVRG/S2GD and SAG/SAGA, SARAH also allows an efficient sparse implementation named “lazy updates” (Konečný et al., 2016). We conduct and compare numerical results of SARAH with SVRG, SAG, SGD+ and FISTA. SVRG (Johnson & Zhang, 2013) and SAG (Le Roux et al., 2012) are classic modern stochastic methods. SGD+ is SGD with decreasing learning rate η = η0/(k + 1) where k is the number of effective passes and η0 is some initial constant learning rate. FISTA (Beck & Teboulle, 2009) is the Fast Iterative Shrinkage-ThresholdingAlgorithm, wellknown as an efficient accelerated version of the gradient descent. Even though for each method, there is a theoretical safe learning rate, we compare the results for the best learning rates in hindsight.\nFigure 5 shows numerical results in terms of loss residuals\n6All datasets are available at http://www.csie.ntu. edu.tw/˜cjlin/libsvmtools/datasets/.\n(top) and test errors (bottom) on the four datasets, SARAH is sometimes comparable or a little worse than other methods at the beginning. However, it quickly catches up to or surpasses all other methods, demonstrating a faster rate of decrease across all experiments. We observe that on covtype and rcv1, SARAH, SVRG and SAG are comparable with some advantage of SARAH on covtype. On ijcnn1 and news20, SARAH and SVRG consistently surpass the other methods.\nIn particular, to validate the efficiency of our practical variant SARAH+, we provide an insight into how important the choices of m and η are for SVRG and SARAH in Table 4 and Figure 6. Table 4 presents the optimal choices of m and η for each of the algorithm, while Figure 6 shows the behaviors of SVRG and SARAH with different choices of m for covtype and ijcnn1, where m∗s denote the best choices. In Table 4, the optimal learning rates of SARAH vary less among different datasets compared to all the other methods and they approximate the theoretical upper bound for SARAH (1/L); on the contrary, for the other methods the empirical optimal rates can exceed their theoretical limits (SVRG with 1/(4L), SAG with 1/(16L), FISTA with 1/L). This empirical studies suggest that it is much easier to tune and find the ideal learning rate for SARAH. As observed in Figure 6, the behaviors of both SARAH and SVRG are quite sensitive to the choices ofm. With improper choices of m, the loss residuals can be increased considerably from 10−15 to 10−3 on both covtype in 40 effective passes and ijcnn1 in 17 effective passes for"
    }, {
      "heading" : "SARAH/SVRG.",
      "text" : ""
    }, {
      "heading" : "6. Conclusion",
      "text" : "We propose a new variance reducing stochastic recursive gradient algorithm SARAH, which combines some of the properties of well known existing algorithms, such as SAGA and SVRG. For smooth convex functions, we show a sublinear convergence rate, while for strongly convex cases, we prove the linear convergence rate and the computational complexity as those of SVRG and SAG. However, compared to SVRG, SARAH’s convergence rate constant is smaller and the algorithms is more stable both theoretically and numerically. Additionally, we prove the linear convergence for inner loops of SARAH which support the claim of stability. Based on this convergence we derive a practical version of SARAH, with a simple stopping criterion for the inner loops."
    }, {
      "heading" : "Acknowledgements",
      "text" : "The authors would like to thank the reviewers for useful suggestions which helped to improve the exposition in the paper."
    }, {
      "heading" : "A. Technical Results",
      "text" : "Lemma 4 (Theorem 2.1.5 in (Nesterov, 2004)). Suppose that f is convex and L-smooth. Then, for any w, w′ ∈ Rd,\nf(w) ≤ f(w′) +∇f(w′)T (w − w′) + L 2 ‖w − w′‖2, (16)\nf(w) ≥ f(w′) +∇f(w′)T (w − w′) + 1 2L ‖∇f(w)−∇f(w′)‖2, (17)\n(∇f(w) −∇f(w′))T (w − w′) ≥ 1 L ‖∇f(w)−∇f(w′)‖2. (18)\nNote that (16) does not require the convexity of f .\nLemma 5 (Theorem 2.1.11 in (Nesterov, 2004)). Suppose that f is µ-strongly convex and L-smooth. Then, for any w, w′ ∈ Rd,\n(∇f(w)−∇f(w′))T (w − w′) ≥ µL µ+ L ‖w − w′‖2 + 1 µ+ L ‖∇f(w)−∇f(w′)‖2. (19)\nLemma 6 (Choices ofm and η). Consider the rate of convergence σm in Theorem 4. If we choose η = 1/(θL) with θ > 1 and fix σm, then the best choice ofm is\nm∗ = 1\n2 (2θ∗ − 1)2κ− 1,\nwhere κ def = L/µ, with θ∗ calculated as:\nθ∗ = σm + 1 +\n√ σm + 1\n2σm .\nFurthermore, we require θ∗ > 1 + √ 2/2 for σm < 1."
    }, {
      "heading" : "B. Proofs",
      "text" : ""
    }, {
      "heading" : "B.1. Proof of Lemma 1",
      "text" : "By Assumption 1 and wt+1 = wt − ηvt, we have\nE[P (wt+1)] (16) ≤ E[P (wt)]− ηE[∇P (wt)T vt] + Lη2\n2 E[‖vt‖2]\n= E[P (wt)]− η\n2 E[‖∇P (wt)‖2] +\nη 2 E[‖∇P (wt)− vt‖2]−\n(\nη 2 − Lη\n2\n2\n)\nE[‖vt‖2],\nwhere the last equality follows from the fact aT b = 12 [ ‖a‖2 + ‖b‖2 − ‖a− b‖2 ] .\nBy summing over t = 0, . . . ,m, we have\nE[P (wm+1)] ≤ E[P (w0)]− η\n2\nm ∑\nt=0\nE[‖∇P (wt)‖2] + η\n2\nm ∑\nt=0\nE[‖∇P (wt)− vt‖2]− ( η 2 − Lη 2 2\n) m ∑\nt=0\nE[‖vt‖2],\nwhich is equivalent to (η > 0):\nm ∑\nt=0\nE[‖∇P (wt)‖2] ≤ 2\nη E[P (w0)− P (wm+1)] +\nm ∑\nt=0\nE[‖∇P (wt)− vt‖2]− (1− Lη) m ∑\nt=0\nE[‖vt‖2]\n≤ 2 η E[P (w0)− P (w∗)] +\nm ∑\nt=0\nE[‖∇P (wt)− vt‖2]− (1 − Lη) m ∑\nt=0\nE[‖vt‖2],\nwhere the last inequality follows since w∗ is a global minimizer of P ."
    }, {
      "heading" : "B.2. Proof of Lemma 2",
      "text" : "Note that Fj contains all the information of w0, . . . , wj as well as v0, . . . , vj−1. For j ≥ 1, we have\nE[‖∇P (wj)− vj‖2|Fj ] = E[‖[∇P (wj−1)− vj−1] + [∇P (wj)−∇P (wj−1)]− [vj − vj−1]‖2|Fj ] = ‖∇P (wj−1)− vj−1‖2 + ‖∇P (wj)−∇P (wj−1)‖2 + E[‖vj − vj−1‖2|Fj] + 2(∇P (wj−1)− vj−1)T (∇P (wj)−∇P (wj−1)) − 2(∇P (wj−1)− vj−1)TE[vj − vj−1|Fj] − 2(∇P (wj)−∇P (wj−1))TE[vj − vj−1|Fj ]\n= ‖∇P (wj−1)− vj−1‖2 − ‖∇P (wj)−∇P (wj−1)‖2 + E[‖vj − vj−1‖2|Fj],\nwhere the last equality follows from\nE[vj − vj−1|Fj ] (2)= E[∇fij (wj)−∇fij (wj−1)|Fj ] = ∇P (wj)−∇P (wj−1).\nBy taking expectation for the above equation, we have\nE[‖∇P (wj)− vj‖2] = E[‖∇P (wj−1)− vj−1‖2]− E[‖∇P (wj)−∇P (wj−1)‖2] + E[‖vj − vj−1‖2].\nNote that ‖∇P (w0)− v0‖2 = 0. By summing over j = 1, . . . , t (t ≥ 1), we have\nE[‖∇P (wt)− vt‖2] = t ∑\nj=1\nE[‖vj − vj−1‖2]− t ∑\nj=1\nE[‖∇P (wj)−∇P (wj−1)‖2]."
    }, {
      "heading" : "B.3. Proof of Lemma 3",
      "text" : "For j ≥ 1, we have\nE[‖vj‖2|Fj] = E[‖vj−1 − (∇fij (wj−1)−∇fij (wj))‖2|Fj]\n= ‖vj−1‖2 + E [ ‖∇fij (wj−1)−∇fij (wj)‖2 − 2η (∇fij (wj−1)−∇fij (wj)) T (wj−1 − wj)|Fj ]\n(18) ≤ ‖vj−1‖2 + E [ ‖∇fij (wj−1)−∇fij (wj)‖2 − 2Lη‖∇fij (wj−1)−∇fij (wj)‖ 2|Fj ] = ‖vj−1‖2 + ( 1− 2ηL )\nE[‖∇fij (wj−1)−∇fij (wj)‖2|Fj ] (2) = ‖vj−1‖2 + ( 1− 2ηL ) E[‖vj − vj−1‖2|Fj],\nwhich, if we take expectation, implies that\nE[‖vj − vj−1‖2] ≤ ηL 2− ηL [ E[‖vj−1‖2]− E[‖vj‖2] ] ,\nwhen η < 2/L.\nBy summing the above inequality over j = 1, . . . , t (t ≥ 1), we have t\n∑\nj=1\nE[‖vj − vj−1‖2] ≤ ηL 2− ηL [ E[‖v0‖2]− E[‖vt‖2] ] . (20)\nBy Lemma 2, we have\nE[‖∇P (wt)− vt‖2] ≤ t ∑\nj=1\nE[‖vj − vj−1‖2] (20) ≤ ηL 2− ηL [ E[‖v0‖2]− E[‖vt‖2] ] ."
    }, {
      "heading" : "B.4. Proof of Lemma 6",
      "text" : "With η = 1/(θL) and κ = L/µ, the rate of convergence αm can be written as\nσm (15) =\n1\nµη(m+ 1) +\nηL 2− ηL = θL µ(m+ 1) + 1/θ 2− 1/θ = ( κ m+ 1 ) θ + 1 2θ − 1 ,\nwhich is equivalent to\nm(θ) def = m = θ(2θ − 1) σm(2θ − 1)− 1 κ− 1.\nSince σm is considered fixed, then the optimal choice of m in terms of θ can be solved from minθ m(θ), or equivalently, 0 = (∂m)/(∂θ) = m′(θ), and therefore we have the equation with the optimal θ satisfying\nσm = (4θ ∗ − 1)/(2θ∗ − 1)2, (21)\nand by plugging it intom(θ) we conclude the optimalm:\nm∗ = m(K∗) = 1\n2 (2K∗ − 1)2κ− 1,\nwhile by solving for θ∗ in (21) and taking into account that θ > 1, we have the optimal choice of θ:\nθ∗ = σm + 1 +\n√ σm + 1\n2σm .\nObviously, for σm < 1, we require θ ∗ > 1 + √ 2/2."
    }, {
      "heading" : "B.5. Proof of Theorem 1a",
      "text" : "For t ≥ 1, we have\n‖∇P (wt)−∇P (wt−1)‖2 = ∥ ∥ ∥ 1\nn\nn ∑\ni=1\n[∇fi(wt)−∇fi(wt−1)] ∥ ∥ ∥ 2\n≤ 1 n\nn ∑\ni=1\n‖∇fi(wt)−∇fi(wt−1)‖2\n= E[‖∇fit(wt)−∇fit(wt−1)‖2|Ft]. (22)\nUsing the proof of Lemma 3, for t ≥ 1, we have\nE[‖vt‖2|Ft] ≤ ‖vt−1‖2 + ( 1− 2ηL )\nE[‖∇fit(wt−1)−∇fit(wt)‖2|Ft] (22) ≤ ‖vt−1‖2 + ( 1− 2ηL ) ‖∇P (wt)−∇P (wt−1)‖2 ≤ ‖vt−1‖2 + ( 1− 2ηL ) µ2η2‖vt−1‖2.\nNote that 1 − 2ηL < 0 since η < 2/L. The last inequality follows by the strong convexity of P , that is, µ‖wt − wt−1‖ ≤ ‖∇P (wt) − ∇P (wt−1)‖ and the fact that wt = wt−1 − ηvt−1. By taking the expectation and applying recursively, we have\nE[‖vt‖2] ≤ [ 1− ( 2 ηL − 1 ) µ2η2 ] E[‖vt−1‖2]\n≤ [ 1− (\n2 ηL − 1\n) µ2η2 ]t E[‖v0‖2]\n= [ 1− (\n2 ηL − 1\n) µ2η2 ]t E[‖∇P (w0)‖2]."
    }, {
      "heading" : "B.6. Proof of Theorem 1b",
      "text" : "We obviously have E[‖v0‖2|F0] = ‖∇P (w0)‖2. For t ≥ 1, we have\nE[‖vt‖2|Ft] (2)= E[‖vt−1 − (∇fit(wt−1)−∇fit(wt))‖2|Ft] (3) = ‖vt−1‖2 + E[‖∇fit(wt−1)−∇fit(wt)‖2 − 2η (∇fit(wt−1)−∇fit(wt))\nT (wt−1 − wt)|Ft] (19) ≤ ‖vt−1‖2 − 2µLηµ+L ‖vt−1‖ 2 + E[‖∇fit(wt−1)−∇fit(wt)‖2|Ft]− 2η · 1µ+LE[‖∇fit(wt−1)−∇fit(wt)‖\n2|Ft] = (1− 2µLηµ+L )‖vt−1‖ 2 + (1− 2η · 1µ+L )E[‖∇fit(wt−1)−∇fit(wt)‖ 2|Ft] ≤ (\n1− 2µLηµ+L ) ‖vt−1‖2, (23)\nwhere in last inequality we have used that η ≤ 2/(µ+ L). By taking the expectation and applying recursively, the desired result is achieved."
    }, {
      "heading" : "B.7. Proof of Theorem 3",
      "text" : "By Theorem 2, we have\nE[‖∇P (w̃s)‖2] ≤ 2\nη(m+ 1) E[P (w̃s−1)− P (w∗)] +\nηL\n2− ηLE[‖∇P (w̃s−1)‖ 2]\n= δs−1 + αE[‖∇P (w̃s−1)‖2] ≤ δs−1 + αδs−2 + · · ·+ αs−1δ0 + αs‖∇P (w̃0)‖2 ≤ δ + αδ + · · ·+ αs−1δ + αs‖∇P (w̃0)‖2 ≤ δ 1− α s\n1− α + α s‖∇P (w̃0)‖2\n= ∆(1 − αs) + αs‖∇P (w̃0)‖2 = ∆+ αs(‖∇P (w̃0)‖2 −∆),\nwhere the second last equality follows since\nδ 1− α = δ ( 2− ηL 2− 2ηL ) = δ ( 1 + ηL 2(1− ηL) ) = ∆.\nHence, the desired result is achieved."
    }, {
      "heading" : "B.8. Proof of Corollary 2",
      "text" : "Based on Theorem 3, if we would aim for an ǫ-accuracy solution, we can choose∆ = ǫ/4 and α = 1/2 (with η = 2/(3L)). To obtain the convergence to an ǫ-accuracy solution, we need to have δ = O(ǫ), or equivalently, m = O(1/ǫ). Then we\nhave\nE[‖∇P (w̃s)‖2] (13) ≤ ∆ 2 + 1 2 E[‖∇P (w̃s−1)‖2]\n≤ ∆ 2 + ∆ 22 + 1 22 E[‖∇P (w̃s−2)‖2] ≤ ∆ ( 1\n2 +\n1 22 + · · ·+ 1 2s\n)\n+ 1\n2s ‖∇P (w̃0)‖2\n≤ ∆+ 1 2s ‖∇P (w̃0)‖2.\nTo guarantee that E[‖∇P (w̃s)‖2] ≤ ǫ, it is sufficient to make 12s ‖∇P (w̃0)‖2 ≤ 34ǫ, or s = O(log(1/ǫ)). This implies the total complexity to achieve an ǫ-accuracy solution is (n+ 2m)s = O((n+ (1/ǫ)) log(1/ǫ))."
    }, {
      "heading" : "B.9. Proof of Corollary 3",
      "text" : "Based on Lemma 6 and Theorem 4, let us pick θ∗ = 2, i.e, then we have m∗ = 4.5κ − 1. So let us run SARAH with η = 1/(2L) andm = 4.5κ, then we can calculate σm in (15) as\nσm = 1\nµη(m+ 1) +\nηL 2− ηL = 1 [µ/(2L)](4.5κ+ 1) + 1/2 2− 1/2 < 4 9 + 1 3 = 7 9 .\nAccording to Theorem 4, if we run SARAH for T iterations where\nT = ⌈log(‖∇P (w̃0)‖2/ǫ)/ log(9/7)⌉ = ⌈log7/9(ǫ/‖∇P (w̃0)‖2)⌉ ≥ log7/9(ǫ/‖∇P (w̃0)‖2),\nthen we have\nE[‖∇P (w̃T )‖2] ≤ (σm)T ‖∇P (w̃0)‖2 < (7/9)T ‖∇P (w̃0)‖2 ≤ (7/9)log7/9(ǫ/‖∇P (w̃0)‖ 2)‖∇P (w̃0)‖2 = ǫ,\nthus we can derive (7). If we consider the number of gradient evaluations as the main computational complexity, then the total complexity can be obtained as\n(n+ 2m)T = O ((n+ κ) log(1/ǫ)) ."
    } ],
    "references" : [ {
      "title" : "Katyusha: The First Direct Acceleration of Stochastic Gradient Methods",
      "author" : [ "Allen-Zhu", "Zeyuan" ],
      "venue" : "Proceedings of the 49th Annual ACM on Symposium on Theory of Computing (to appear),",
      "citeRegEx" : "Allen.Zhu and Zeyuan.,? \\Q2017\\E",
      "shortCiteRegEx" : "Allen.Zhu and Zeyuan.",
      "year" : 2017
    }, {
      "title" : "Improved SVRG for Non-Strongly-Convex or Sum-of-Non-Convex Objectives",
      "author" : [ "Allen-Zhu", "Zeyuan", "Yuan", "Yang" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Allen.Zhu et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Allen.Zhu et al\\.",
      "year" : 2016
    }, {
      "title" : "A fast iterative shrinkagethresholding algorithm for linear inverse problems",
      "author" : [ "Beck", "Amir", "Teboulle", "Marc" ],
      "venue" : "SIAM J. Imaging Sciences,",
      "citeRegEx" : "Beck et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Beck et al\\.",
      "year" : 2009
    }, {
      "title" : "Online learning and stochastic approximations",
      "author" : [ "Bottou", "Léon" ],
      "venue" : null,
      "citeRegEx" : "Bottou and Léon.,? \\Q1998\\E",
      "shortCiteRegEx" : "Bottou and Léon.",
      "year" : 1998
    }, {
      "title" : "Optimization methods for large-scale machine learning",
      "author" : [ "Bottou", "Léon", "Curtis", "Frank E", "Nocedal", "Jorge" ],
      "venue" : null,
      "citeRegEx" : "Bottou et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Bottou et al\\.",
      "year" : 2016
    }, {
      "title" : "Better mini-batch algorithms via accelerated gradient methods",
      "author" : [ "Cotter", "Andrew", "Shamir", "Ohad", "Srebro", "Nati", "Sridharan", "Karthik" ],
      "venue" : "In NIPS, pp",
      "citeRegEx" : "Cotter et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Cotter et al\\.",
      "year" : 2011
    }, {
      "title" : "SAGA: A fast incremental gradient method with support for non-strongly convex composite objectives",
      "author" : [ "Defazio", "Aaron", "Bach", "Francis", "Lacoste-Julien", "Simon" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Defazio et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Defazio et al\\.",
      "year" : 2014
    }, {
      "title" : "The Elements of Statistical Learning: Data Mining, Inference, and Prediction",
      "author" : [ "Hastie", "Trevor", "Tibshirani", "Robert", "Friedman", "Jerome" ],
      "venue" : "Springer Series in Statistics,",
      "citeRegEx" : "Hastie et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Hastie et al\\.",
      "year" : 2009
    }, {
      "title" : "Accelerating stochastic gradient descent using predictive variance reduction",
      "author" : [ "Johnson", "Rie", "Zhang", "Tong" ],
      "venue" : "In NIPS, pp",
      "citeRegEx" : "Johnson et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Johnson et al\\.",
      "year" : 2013
    }, {
      "title" : "Mini-batch semi-stochastic gradient descent in the proximal setting",
      "author" : [ "Konečný", "Jakub", "Liu", "Jie", "Richtárik", "Peter", "Takáč", "Martin" ],
      "venue" : "IEEE Journal of Selected Topics in Signal Processing,",
      "citeRegEx" : "Konečný et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Konečný et al\\.",
      "year" : 2016
    }, {
      "title" : "Semi-stochastic gradient descent methods",
      "author" : [ "Konečný", "Jakub", "Richtárik", "Peter" ],
      "venue" : null,
      "citeRegEx" : "Konečný et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Konečný et al\\.",
      "year" : 2013
    }, {
      "title" : "A stochastic gradient method with an exponential convergence rate for finite training sets",
      "author" : [ "Le Roux", "Nicolas", "Schmidt", "Mark", "Bach", "Francis" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Roux et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Roux et al\\.",
      "year" : 2012
    }, {
      "title" : "Optimization with first-order surrogate functions",
      "author" : [ "Mairal", "Julien" ],
      "venue" : "In ICML, pp",
      "citeRegEx" : "Mairal and Julien.,? \\Q2013\\E",
      "shortCiteRegEx" : "Mairal and Julien.",
      "year" : 2013
    }, {
      "title" : "Introductory lectures on convex optimization : a basic course",
      "author" : [ "Nesterov", "Yurii" ],
      "venue" : null,
      "citeRegEx" : "Nesterov and Yurii.,? \\Q2004\\E",
      "shortCiteRegEx" : "Nesterov and Yurii.",
      "year" : 2004
    }, {
      "title" : "Stochastic variance reduction for nonconvex optimization",
      "author" : [ "Reddi", "Sashank J", "Hefny", "Ahmed", "Sra", "Suvrit", "Póczos", "Barnabás", "Smola", "Alexander J" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Reddi et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Reddi et al\\.",
      "year" : 2016
    }, {
      "title" : "A stochastic approximation method",
      "author" : [ "Robbins", "Herbert", "Monro", "Sutton" ],
      "venue" : "The Annals of Mathematical Statistics,",
      "citeRegEx" : "Robbins et al\\.,? \\Q1951\\E",
      "shortCiteRegEx" : "Robbins et al\\.",
      "year" : 1951
    }, {
      "title" : "Minimizing finite sums with the stochastic average gradient",
      "author" : [ "Schmidt", "Mark", "Le Roux", "Nicolas", "Bach", "Francis" ],
      "venue" : "Mathematical Programming,",
      "citeRegEx" : "Schmidt et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Schmidt et al\\.",
      "year" : 2016
    }, {
      "title" : "Stochastic dual coordinate ascent methods for regularized loss",
      "author" : [ "Shalev-Shwartz", "Shai", "Zhang", "Tong" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Shalev.Shwartz et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Shalev.Shwartz et al\\.",
      "year" : 2013
    }, {
      "title" : "Pegasos: Primal estimated sub-gradient solver for SVM",
      "author" : [ "Shalev-Shwartz", "Shai", "Singer", "Yoram", "Srebro", "Nathan" ],
      "venue" : "In ICML, pp",
      "citeRegEx" : "Shalev.Shwartz et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Shalev.Shwartz et al\\.",
      "year" : 2007
    }, {
      "title" : "Pegasos: Primal estimated sub-gradient solver for SVM",
      "author" : [ "Shalev-Shwartz", "Shai", "Singer", "Yoram", "Srebro", "Nathan", "Cotter", "Andrew" ],
      "venue" : "Mathematical Programming,",
      "citeRegEx" : "Shalev.Shwartz et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Shalev.Shwartz et al\\.",
      "year" : 2011
    }, {
      "title" : "Mini-batch primal and dual methods for SVMs",
      "author" : [ "Takáč", "Martin", "Bijral", "Avleen Singh", "Richtárik", "Peter", "Srebro", "Nathan" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Takáč et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Takáč et al\\.",
      "year" : 2013
    }, {
      "title" : "A proximal stochastic gradient method with progressive variance reduction",
      "author" : [ "Xiao", "Lin", "Zhang", "Tong" ],
      "venue" : "SIAM Journal on Optimization,",
      "citeRegEx" : "Xiao et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Xiao et al\\.",
      "year" : 2014
    } ],
    "referenceMentions" : [ {
      "referenceID" : 7,
      "context" : "Problems of this type arise frequently in supervised learning applications (Hastie et al., 2009).",
      "startOffset" : 75,
      "endOffset" : 96
    }, {
      "referenceID" : 19,
      "context" : "Using diminishing sequence {ηt} is used to control the variance (Shalev-Shwartz et al., 2011; Bottou et al., 2016), but the practical convergence of SGD is known to be very sensitive to the choice of this sequence, which needs to be hand-picked.",
      "startOffset" : 64,
      "endOffset" : 114
    }, {
      "referenceID" : 4,
      "context" : "Using diminishing sequence {ηt} is used to control the variance (Shalev-Shwartz et al., 2011; Bottou et al., 2016), but the practical convergence of SGD is known to be very sensitive to the choice of this sequence, which needs to be hand-picked.",
      "startOffset" : 64,
      "endOffset" : 114
    }, {
      "referenceID" : 6,
      "context" : "The examples of these methods are SAG/SAGA (Le Roux et al., 2012; Defazio et al., 2014), SDCA (Shalev-Shwartz & Zhang, 2013), SVRG (Johnson & Zhang, 2013; Xiao & Zhang, 2014), DIAG (Mokhtari et al.",
      "startOffset" : 43,
      "endOffset" : 87
    }, {
      "referenceID" : 16,
      "context" : "In addition, theoretical results for complexity of the methods or their variants when applied to general convex functions have been derived (Schmidt et al., 2016; Defazio et al., 2014; Reddi et al., 2016; Allen-Zhu & Yuan, 2016; Allen-Zhu, 2017).",
      "startOffset" : 140,
      "endOffset" : 245
    }, {
      "referenceID" : 6,
      "context" : "In addition, theoretical results for complexity of the methods or their variants when applied to general convex functions have been derived (Schmidt et al., 2016; Defazio et al., 2014; Reddi et al., 2016; Allen-Zhu & Yuan, 2016; Allen-Zhu, 2017).",
      "startOffset" : 140,
      "endOffset" : 245
    }, {
      "referenceID" : 14,
      "context" : "In addition, theoretical results for complexity of the methods or their variants when applied to general convex functions have been derived (Schmidt et al., 2016; Defazio et al., 2014; Reddi et al., 2016; Allen-Zhu & Yuan, 2016; Allen-Zhu, 2017).",
      "startOffset" : 140,
      "endOffset" : 245
    }, {
      "referenceID" : 9,
      "context" : "Note that like SVRG/S2GD and SAG/SAGA, SARAH also allows an efficient sparse implementation named “lazy updates” (Konečný et al., 2016).",
      "startOffset" : 113,
      "endOffset" : 135
    } ],
    "year" : 2017,
    "abstractText" : "In this paper, we propose a StochAstic Recursive grAdient algoritHm (SARAH), as well as its practical variant SARAH+, as a novel approach to the finite-sum minimization problems. Different from the vanilla SGD and other modern stochastic methods such as SVRG, S2GD, SAG and SAGA, SARAH admits a simple recursive framework for updating stochastic gradient estimates; when comparing to SAG/SAGA, SARAH does not require a storage of past gradients. The linear convergence rate of SARAH is proven under strong convexity assumption. We also prove a linear convergence rate (in the strongly convex case) for an inner loop of SARAH, the property that SVRG does not possess. Numerical experiments demonstrate the efficiency of our algorithm.",
    "creator" : "LaTeX with hyperref package"
  }
}