{
  "name" : "1206.6473.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Compositional Planning Using Optimal Option Models",
    "authors" : [ "David Silver", "Kamil Ciosek" ],
    "emails" : [ "d.silver@cs.ucl.ac.uk", "k.ciosek@cs.ucl.ac.uk" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "Classical planning algorithms make extensive use of temporal abstraction to construct high-level chunks of useful knowledge (Amarel, 1968; Sacerdoti, 1975; Korf, 1985; Laird et al., 1986). They are typically provided with a set of primitive planning operators as inputs. These are then composed together into macrooperators: open-loop sequences of planning operators. Macro-operators jump directly from an initial state to the outcome state that would result from following the sequence, without having to execute the intermediate operators. Macro-operators can themselves be composed together into more abstract operators, allowing planning to take place at a much more abstract level.\nAppearing in Proceedings of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright 2012 by the author(s)/owner(s).\nMacro-operators can be thought of as building blocks of knowledge, which can be combined together into more abstract knowledge. Powered by this knowledge, the path to the goal can often be found in a small number of high-level planning operations, even when the path is composed of thousands of primitive actions.\nIn Markov Decision Processes (MDPs), the outcome of an action may be stochastic. An open-loop sequence does not capture the contingencies that can arise as a result of each intermediate action. Instead, a closedloop policy, which maps states to actions, can respond to each particular situation as it arises. A closed-loop policy that is followed for some number of steps, and stops according to a termination condition that also depends on the state, is known as an option (Sutton et al., 1999). An option model describes the distribution of outcome states that would result from following the option (Sutton, 1995). Option models are the stochastic analogue of macro-operators: they jump directly from initial state to outcome, without having to execute the intermediate actions. Option models can also be composed together into more abstract option models (Precup et al., 1998). Option models thus provide basic building blocks for compositional knowledge in general MDPs.\nHowever, prior work on planning with options has been restricted to shallow hierarchies. Option models are either constructed from primitive actions, in an approach known as intra-option model learning; or they are used to compute a value function, in an approach known as inter-option (or SMDP) planning (Sutton et al., 1999). Although these steps are sometimes combined, they are typically combined in two stages: first constructing the option models without using them; and then using the option models without changing them. In both stages, the planning operators are fixed.\nIn this paper we focus explicitly on compositional planning : the multi-level composition of option models. Each option model is both constructed (intra-option) and used (inter-option). It is constructed from lowerlevel option models, so as to maximise progress towards a given subgoal. It may also be used to com-\npose higher-level option models. As soon as an option model has been created, it can be used to construct other option models. As a result, the set of planning operators improves dynamically, providing longer and more purposeful jumps as planning proceeds.\nOur approach is based on a major generalisation of the Bellman equation along four dimensions. First, we provide a recursive relationship between state probabilities as well as between rewards. Second, we compose over options rather than primitive actions. Third, we generalise from the overall goal of maximising total reward, to any given subgoal. Fourth, we optimise over termination conditions as well as policies.\nSeveral of these dimensions have been partially explored by prior work. First, Sutton et al. (1995, 1999, Section 5) developed a Bellman expectation equation for state probabilities, but this work was restricted to Markov reward processes without actions (Sutton, 1995) or to fixed policies without control (Sutton et al., 1999). We present a Bellman optimality equation for state probabilities in Markov decision processes, including actions and control. Second, Precup et al. (1998) provided Bellman equations for composing option models into policies, but not into options. Our framework constructs both policies and termination conditions, so that we can compose option models into other option models – a crucial step for compositional planning. Third, Sutton et al. (1999, Section 7) defined optimal options with respect to a given subgoal and termination condition, and suggested the existence of a corresponding Bellman optimality equation. We define this Bellman optimality equation, and also extend to the case when neither, either, or both the policy and termination condition are specified. No prior work has considered the state probabilities associated with Bellman optimality equations. Without knowledge of these state probabilities, it is not possible to jump directly to the outcome of an optimal option. Our approach to compositional planning is built directly on this knowledge, so as to build abstract macro-operators that can jump from one state directly to a distant state.\nThe Bellman optimality equation gives rise to important planning methods such as value iteration. Similarly, we use our generalised Bellman equation to derive a compositional planning algorithm, which simultaneously and recursively constructs the optimal option model for multiple subgoals, including the overall goal as a special case. We prove that this algorithm converges to optimal option models for all subgoals, including the optimal policy.\nThe options framework is agnostic about the source of the options, and does not commit to any particu-\nlar algorithm for their construction. However, several other approaches to hierarchical reinforcement learning have been proposed, based on samples from an unknown MDP. These architectures, including Dietterich’s MAXQ (2000), and Parr and Russell’s HAMs (1997; 2002), do construct the solution to one subproblem from the solution to other subproblems. However, these architectures are not directly applicable to compositional planning, where the MDP is known rather than sampled. By focusing on planning with known models, we develop a sound theoretical framework for compositional planning, based on the generalised Bellman equation. This work can be viewed as a bridge between the generality of options, and the compositional construction algorithms used by architectures such as MAXQ.\nWe illustrate our approach on two well-known benchmark problems: hierarchical path planning and the Tower of Hanoi. Both problems have been extensively studied using classical planning approaches. In both problems, planning directly with primitive operators (e.g. using value iteration) requires computation time that is exponential in the problem size, whereas algorithms based on compositions of macro-operators (e.g. Jonsson 2009) can solve these problems in polynomial time. Unfortunately, classical planning approaches do not generalise to stochastic planning problems. In contrast, our compositional planning algorithm can solve both deterministic and stochastic variants of these problems in a polynomial number of iterations."
    }, {
      "heading" : "2 Background",
      "text" : "An MDP is defined by a set of n states S, a set of actions A, action transition matrices P a and action reward vectors Ra for each action a ∈ A, and a discount factor 0 ≤ γ < 1. Each component of the action transition matrix P ass′ is the discounted probability of next state s′ given that action a was selected in state s, P ass′ = γ Pr(st+1 = s\n′ | st = s, at = a). Each component of the action reward vector Ras is the expected reward given that action a was selected in state s, Ras = E[rt+1 | st = s, at = a]. The discount factor can be viewed as a chance of exiting to an absorbing terminal state with probability 1− γ. The discounted probability P ass′ can be interpreted as the probability of reaching state s′ without exiting.\nA policy π(s, a) is the probability of selecting action a given state s, π(s, a) = Pr(at = a | st = s). The value function, V π(s), is the expected total reward from state s when following policy π, V π(s) = E[rt+1+ γrt+2 + ... | st = s, π]. The optimal value function V ∗(s) and optimal action value function Q∗(s, a) are the maximum achievable value and action value that\nCompositional Planning Using Optimal Option Models\ncan be achieved by any policy, V ∗(s) = max V π π(s). An optimal policy π∗(s, a) is any policy that achieves the optimal value function.\nThe optimal value function obeys a recursive relationship: the Bellman optimality equation, V ∗(s) = max R\na\na s + ∑ s′ P a ss′V ∗(s′). The optimal value func-\ntion is the unique fixed point of this equation, and can be found by turning the Bellman optimality equation into an iterative update, Vk+1(s) ← max R\na a s +∑\ns′ P a ss′Vk(s ′). This algorithm is called value iteration (Bellman, 1957). An option o = 〈π, β〉 is an extended behaviour or macro-action that combines a policy π(s, a) with a termination condition β(s) giving the probability that the option will stop in state s. We assume that options can be initiated from all states. Primitive actions are options: they can be represented by a policy that deterministically selects that action, and a termination condition that stops with probability 1. We denote the set of all policies by Π and the set of all termination conditions by B. An option model comprises an option transition matrix P o and an option reward vector Ro. Each component Ros is the expected total reward given that option o was executed from state s, Ros = E[rt+1 +γrt+2 + ...+γτ−1rt+τ | st = s, o], where τ is the random variable for the duration of option o. Each component P oss′ is the probability of terminating in state s′ given that option o was executed from state s, discounted by the total duration of the option, P oss′ = ∑∞ τ=1 γ τPr(τ, st+τ = s ′ | st = s, o). This can be interpreted as the probability of option o terminating in s′ without exiting."
    }, {
      "heading" : "3 Models",
      "text" : "Informally, a model is a stochastic mapping from state to state, combined with the reward accumulated along the way. Applying a model to a state results in a distribution over outcome states, and an expected reward. To compose models together, we apply a second model to this outcome distribution and expected reward, and arrive at a new state distribution and reward. We now formalise these ideas, following Sutton (1995).\nWe define a rasp (reward and state probabilities), [r| p ], to be a 1 × (1 + n) row vector, where n = |S|, r is a scalar reward, and p is a 1 × n row vector representing a discounted probability distribution p over states in S. We use ss to denote the deterministic rasp that is in state s with probability 1, and has a reward component of zero; we shorten to s when there is no ambiguity. Rasps are ordered by their reward components, [r1 | p1] ≤ [r2 | p2] if and only if r1 ≤ r2. A model is a transformation from rasp to rasp. For-\nmally, a model [ 1 0 R P ] is a 1 + n × 1 + n block matrix containing an n × 1 reward vector R and an n × n transition matrix P . This block matrix notation for models and block vector notation for rasps are known as homogeneous coordinates (Sutton, 1995). To compose two models together, we multiply their homogeneous coordinates,[\n1 0 R1 P1 ] [ 1 0 R2 P2 ] = [ 1 0 R1+P1R2 P1P2 ] (1)\nSimilarly, to compose a rasp and a model, we again multiply their homogeneous coordinates,\n[r | p] [\n1 0 R P\n] = [r + pR | pP ].\n(2)\nTo aid readability, homogenous matrices and vectors are denoted by boldface letters, e.g x, M and M for rasps, models and model sets respectively. Sequences of compositions are best understood by reading left to right, e.g. sAB is the model composition that starts in state s, applies model A and then applies model B."
    }, {
      "heading" : "3.1 Model Sets",
      "text" : "Models can represent the outcomes of actions, options and policies. An option model Oo ∈ O represents the outcome on termination of a corresponding option o ∈ O. It combines an option transition matrix with\nan option reward vector, Oo = [ 1 0 Ro P o ] . An action model Aa ∈ A represents the outcome of a prim-\nitive action a ∈ A, where Aa = [\n1 0 Ra P a\n] . Action\nmodels are option models, A ⊂ O, corresponding to options that terminate with probability 1. A policy model Ππ ∈ Π , where Ππ = [\n1 0 V π 0\n] , represents\nthe outcome of executing policy π forever. Policy models are also option models, Π ⊂ O, corresponding to options that terminate with probability 0. Finally, we define an identity model I corresponding to zero re-\nward and the identity transition matrix, I = [ 1 0 0 I ] ; this can be viewed as a null model without any discounting. Note that action/option/policy subscripts may be dropped when there is no ambiguity."
    }, {
      "heading" : "3.2 Value Models",
      "text" : "A value model V ∈ V , where V = [\n1 0 V 0\n] , has a\ntransition matrix of zero (i.e. it always exits) and a reward vector given by the components of V (s) as its\nreward vector (i.e. total reward before exiting). Policy models are value models, Π ⊂ V , where the reward vector contains the values V π(s), and the transition matrix is zero due to infinite discounting.\nValue models can be used to express several familiar value functions. A state value function V (s) can be represented by composition with the corresponding value model, sV; an action value function can be represented by sAV; and an inter-option value function (Sutton et al., 1999) can be represented by sOV.\nThe true value model G− = [ 1 0 V − 0 ] represents the overall goal of maximising total reward. It is defined to have a value function V −(s) that is a lower bound on the value function of all policies, V −(s) < V π(s),∀s ∈ S, π ∈ Π. This definition ensures that a termination condition of β(s) = 0 is always optimal, and that policy models dominate over terminating option models, with respect to the true value, sΠπG\n− = sΠπ = sO〈π,β〉Ππ ≥ sO〈π,β〉G−,∀s ∈ S, π ∈ Π, β ∈ B."
    }, {
      "heading" : "3.3 Expectation Models",
      "text" : "An expectation model Eρ(M) is the expected model under some distribution ρ(s, ·) over models. For example, an action expectation model Eπ(A) averages all action models Aa ∈ A according to policy π(s, a). Specifically, each row of Eπ(A) contains the expected rasp from state s after one action has been executed by π, Ea∼π(s,·)[sAa | s],\nEπ(A) =  1 0 Ea∼π(s,·)[sAa | s = s1]\n... Ea∼π(s,·)[sAa | s = sn]  (3) Composing a model with a deterministic rasp s picks out the row corresponding to state s,\nsEπ(A) = Ea∼π(s,·)[sAa | s] = ∑ a∈A π(s, a) sAa,∀s ∈ S (4)"
    }, {
      "heading" : "3.4 Maximising Models",
      "text" : "A max model max V V∈W maximises over a given set of value models W ⊆ V . Each reward component is the maximum value of sV from state s.\nmax V V∈W =  1 0\nmax sV V∈W,s=s1 0... max sV V∈W,s=sn\n (5)\nAn argmax model argmax MV M∈M maximises over the models in set M, with respect to value model V. Each\nrow of argmax MV M∈M is the rasp sM that maximises the value sMV from state s.\nargmax MV M∈M =  1 0 argmax sMV sM |M∈M,s=s1\n... argmax sMV sM |M∈M,s=sn\n (6)\nComposing an argmax model with a deterministic rasp s picks out the maximising row,\ns argmax MV M∈M = argmax sMV,∀s ∈ S sM |M∈M\n(7)"
    }, {
      "heading" : "4 Model Equations",
      "text" : "We now explore recursive relationships between compositions of models. For didactic purposes we begin with compositions of primitive actions into policy models, and develop a model equation that is analogous to the Bellman equation. We then extend this approach to compositions of option models into policy models; to compositions of action models into option models; and finally to compositions of option models into other option models. We provide proofs of unique fixed points in the supplementary material."
    }, {
      "heading" : "4.1 Action-Policy Model Composition",
      "text" : "We begin by rewriting the Bellman expectation equation as a model composition,\nV = Eπ(A)V (8)\nWe call this equation the action-policy model expectation equation. It rewrites the Bellman expectation equation in homogeneous coordinates. This equation has fixed point V = Ππ, i.e. composing the action expectation model Eπ(A) with policy model Ππ results in the same policy model Ππ. We also consider the model max AV A∈A that maximises the state-action value sAV from every state s. We can then rewrite the Bellman optimality equation in homogeneous coordinates,\nV = max AV A∈A\n(9)\nWe call this equation the action-policy model optimality equation.\nThe optimal policy model is the max model max Π Π∈Π over all policy models over the set of primitive actions, s max Π\nΠ∈Π = max s Π∈Π Π,∀s ∈ S. It is equivalent to the optimal value function. The optimal policy model V = max Π\nΠ∈Π is a fixed point of the action-policy model\noptimality equation."
    }, {
      "heading" : "4.2 Option-Policy Model Composition",
      "text" : "We now compose option models into a policy model. We assume we are given a base set of options Ω ⊆ O and a corresponding set of option models Ω ⊆ O. We consider the option expectation model Eπ(O) that averages the base option models Oo ∈ Ω according to hierarchical policy π(s, o) = Pr(o | s). Similarly to Equations 3 and 4, each row of Eπ(O) contains the expected rasp from state s after one option has been executed by π,\nsEπ(O) = Eo∼π(s,·)[sOo | s] = ∑ o∈Ω π(s, o) sOo,∀s ∈ S\n(10)\nThis gives the option-policy model expectation equation, with fixed point V = Ππ,\nV = Eπ(O)V (11)\nNext, we consider the model max OV O∈Ω that maximises the composed value sOV (the inter-option value function). This leads to the option-policy model optimality equation,\nV = max OV O∈Ω\n(12)\nGiven only a base set of option models Ω , which does not necessarily include all primitive actions, it is not in general possible to construct all policy models. Instead, we consider the hierarchical policy model set {Ππ | supp(π) ⊆ Ω}, which is the set of all policy models corresponding to hierarchical policies over Ω. The hierarchically optimal policy model max Π Ππ | supp(π)⊆Ω is the max model over this set; it is analogous to a hierarchically optimal value function (Dietterich, 2000), i.e. the best that can be achieved under the hierarchical constraints imposed by the choice of base options.1 The hierarchically optimal policy model V = max Π\nΠπ | supp(π)⊆Ω is the unique fixed point of the\noption-policy model optimality equation. If the base set includes all primitive actions, A ⊆ Ω , then all policy models can be represented and the hierarchically optimal policy model is the optimal policy model,\nmax Π Ππ | supp(π)⊆Ω = max Π Π∈Π ."
    }, {
      "heading" : "4.3 Action-Option Model Composition",
      "text" : "Primitive actions can also be composed together into option models, to give intra-option model learning.\n1Hierarchical optimality is a global optimality condition. In contrast, recursive optimality (Dietterich, 2000) is a weaker, local optimality condition that assumes all suboptions are fixed. Many hierarchical reinforcement learning algorithms achieve recursive optimality but not hierarchical optimality.\nThis requires a mechanism to incorporate option termination into model compositions.\nWe represent the termination condition β(s) by a termination model Eβ(I,M). This is an expectation model over {I,M} that selects each row from the identity model I with probability β(s), or from model M with probability 1− β(s),\nsEβ(I,M) = s (β(s)I + (1− β(s))M) ,∀s ∈ S (13)\nComposing an action model A with termination model Eβ(I,M) selects between A (termination) or AM (continuation). In particular, we consider the composition of expectation model Eπ(A) with termination model Eβ(I,M). This gives the action-option model expectation equation, with fixed point M = O〈π,β〉,\nM = Eπ(A)Eβ(I,M) (14)\nWe now consider the optimality of option models. We define optimality with respect to a subgoal value model G that represents the value on termination of the option, e.g. whether a given subgoal has been achieved. An optimal option model argmax OG\nO∈O is the argmax\nmodel, with respect to subgoal value model G, over all options, i.e. it maximises over both policies and termination conditions. We will consider option models that maximise over policies or termination conditions in a subsequent section.\nWe represent optimal termination by an argmax model over B ∈ {I,M}, which maximises the binary choice between termination, represented by identity model I, and continuation, represented by model M. For example, argmax ABG\nAB | B∈{I,M} either selects row s from action\nmodel A or from the composed model AM, depending on whether sAG (termination) or sAMG (continuation) gives more reward from state s. We only optimise over deterministic termination conditions, because an optimal deterministic termination condition must exist (analogous to optimal policies). We can now define the option-option model optimality equation, for which any optimal option model argmax OG\nO∈O is a fixed point,\nM = argmax ABG AB | A∈A,B∈{I,M}\n(15)"
    }, {
      "heading" : "4.4 Option-Option Model Composition",
      "text" : "We now present the most general case in which option models are composed into other option models. This combines intra-option model learning with interoption model learning, a key step towards our goal of compositional planning. As in option-policy model composition, we assume that we are given a base set Ω\nof options, and a corresponding set Ω of option models to compose together. As in action-option model composition, we consider termination conditions as well as policies. Combining these ideas together gives the option-option model expectation equation,\nM = Eπ(O)Eβ(I,M) (16)\nwith fixed point M = O〈π,β〉, and the option-option model optimality equation,\nM = argmax OBG OB | O∈Ω,B∈{I,M}\n(17)\nIt is not in general possible to construct all option models, due to limitations of the base set Ω . Instead, we consider the hierarchical option model set{ O〈π,β〉 | supp(π) ⊆ Ω,β ∈ B } , which is the set of option models O〈π,β〉 where π is restricted to options in Ω. The hierarchically optimal option model,\nargmax OG O〈π,β〉 | supp(π)⊆Ω,β∈B , is the argmax model over this set, with respect to subgoal value model G. A hierarchically optimal option model is a fixed point of the option-option model optimality equation."
    }, {
      "heading" : "4.5 Optimal β- and π-Option Models",
      "text" : "There are in fact two dimensions of optimality for option models: optimality of the policy π and optimality of the termination condition β. The previous sections dealt with jointly optimal option models, which maximise over both policies and termination conditions. We now consider option models that optimise just one of these two dimensions.\nAn optimal β-option model argmax OG O〈π,β〉 | π∈Π,β=β is the argmax model over the set of options with termination condition β, i.e. it maximises over policies for a given termination condition β. Similarly, an optimal π-option model argmax OG\nO〈π,β〉 | π=π,β∈B is the argmax model\nover the set of options with policy π, i.e. it maximises over termination conditions for a given policy π.\nWe can now define action-option model optimality equations for optimal β-option models, where the termination condition is given; and for optimal π-option models, where the policy is given,\nM = argmax AB̄G AB̄ | A∈A,B̄=Eβ(I,M)\n(18)\nM = argmax ĀBG ĀB | Ā=Eπ(A),B∈{I,M}\n(19)\nThese equations have respective fixed points: optimal β-option model M = argmax OG\nO〈π,β〉 | π∈Π,β=β , and optimal π-\noption model M = argmax OG O〈π,β〉 | π=π,β∈B . For option-option model composition of β-options, we restrict option models to elements of the hierarchical option model set that also match a given termination condition β, { O〈π,β〉 | supp(π) ⊆ Ω,β = β } . The hierarchically optimal β-option model is the argmax model over this restricted set, argmax OG\nO〈π,β〉 | supp(π)⊆Ω,β=β .\nThe option-option model optimality equations for βoptions and π-options respectively are,\nM = argmax OB̄G OB̄ | O∈Ω,B̄=Eβ(I,M)\n(20)\nM = argmax ŌBG ŌB | Ō=Eπ(O),B∈{I,M}\n(21)\nThe fixed points of these equations are the hierarchically optimal β-option model M = argmax OG\nO〈π,β〉 | supp(π)⊆Ω,β=β ;\nand the optimal π-option model M = argmax OG O〈π,β〉 | π=π,β∈B . Table 1 and 2 summarise the various model equations and their fixed points. In the supplementary material, we prove that each fixed point satisfies the corresponding equations, and furthermore that the subgoal value of each fixed point is unique."
    }, {
      "heading" : "5 Option-Option Model Iteration",
      "text" : "The Bellman optimality equation forms the basis of a wide variety of MDP planning algorithms (Sutton & Barto, 1998). Similarly, the model optimality equations can be used to derive a wide variety of MDP planning algorithms. In particular, the option-option model equations can be used to derive algorithms for compositional planning in MDPs. We focus here on a dynamic programming algorithm that uses the optionoption model optimality equation (Equation 17) as an iterative update. This algorithm, which we call optionoption model iteration (OOMI), can be viewed as a generalisation of value iteration to option models for multiple subgoals.\nWe assume that we are given a base set Ω of option models, and also m subgoal value models {G1, ...,Gm} for m different subgoals. At each iteration k, the algorithm updates a set of m option models Mk ={ Mk1 , ...,M k m } , containing one option model for every subgoal. Each option model is initialised to the true value model, M0g = G −. At each iteration k, for every subgoal j, option model Mk+1g is updated by the option-option model optimality equation (Equation 17). Maximisation is performed over the base set Ω and the current set of option models Mk,\nMk+1g ← argmax OBGg OB | O∈Ω∪Mk,B∈{I,Mkg}\n(22)\nOOMI imposes no explicit hierarchy: any option model may be composed with any other option model. When updating the option model Mg for subgoal value model Gg, all current option models are considered. In particular, the option model Mg itself is considered; this allows option models to be repeatedly squared, so that a single model may be efficiently applied as many times as required. As a result, even if OOMI is restricted to primitive actions, Ω = A, and only a single subgoal, G1 = G\n−, it may still converge in significantly fewer iterations than value iteration. We prove in the supplementary material that OOMI converges to a hierarchically optimal option model for each subgoal value model Gg. If OOMI includes the true value model in its set of value models, Gg = G\n−, then the corresponding option model Mg will converge to the hierarchically optimal policy model Π ∗Ω . Optionoption model iteration can similarly be extended to βoptions, where the termination condition is given; or π-options, where the policy is given, by using Equations 20 and 21 respectively as iterative updates. Finally, the option-option expectation model equation (Equation 16) can be used as the basis for an iterative update, analogous to policy iteration, that interleaves option evaluation with option improvement."
    }, {
      "heading" : "6 Empirical Results",
      "text" : "We illustrate our framework for compositional planning using two hierarchical MDPs: the Tower of Hanoi problem, and the Nine Rooms problem. The N -disc Tower of Hanoi problem has a discount factor is γ = 1, each action receives a reward of −1, and episodes terminate upon reaching the goal state (N discs stacked on right peg). The level-1 Nine Rooms gridworld is a 3 × 3 grid. The N -level Nine Rooms gridworld contains a 3× 3 grid of instances of level N − 1 problems; neighbouring instances are connected by a width 3N−2 doorway; and there is a single goal state in one corner. The discount factor is γ = 0.9, rewards are 1 in the goal state, and 0 elsewhere. We also use stochastic variants in which each action causes the intended move with probability 1−p, or with probability p randomly selects another legal move (Tower of Hanoi, p = 0.4), or remains in the current state (Nine Rooms, p = 0.05). For the Tower of Hanoi, we use m = 3N + 1 subgoal value models. This set includes the true value model G− and\na subgoal value model Gd,e = [ 1 0 V ond,e 0 ] for placing each disc d on top of each peg e. Each subgoal value function is defined by V ond,e (s) ∝ on(s, d, e), where the predicate on(s, d, e) has a value of 1 if disc d is on peg e in state s and 0 otherwise.2 For the Nine Rooms, we use 12(n− 1) subgoal value models. This set includes a subgoal value model Gl,j for each of the j ∈ [1, 12] doorways at each level l of the hierarchy. Each subgoal value function is defined by V doorwayl,j (s) ∝ in(s, l, j), where the predicate in(s, l, j) has a value of 1 if state s is in the jth level-l doorway, and 0 otherwise. In this problem, initiation sets were used to restrict the states considered to relevant doorways within the neighbourhood of the subgoal. All subgoal values are designed to be achieved “at any cost”, by choosing a large constant of proportionality. We use the primitive actions (moving a disc in Tower of Hanoi; moving N, E, S, W in Nine Rooms) as the base set Ω .\nWe compare three solution methods. Action-policy model iteration (APMI) is a one-level planning algorithm that plans over primitive action models. It iteratively applies the action-policy model optimality equation (Equation 9), and is equivalent to value iteration. Action-option-policy model iteration (AOPMI) is a two-level planning algorithm, with fixed planning operators. It first performs intra-option learning, constructing option models from primitive action models by iteratively applying the action-option model optimality equation (Equation 15). It then fixes the set of option models, and performs inter-option plan-\n2Results are qualitatively similar for other choices of subgoal, such as stacking or unstacking discs.\nning. Finally, it constructs a value function from option models, by iteratively applying the option-policy model optimality equation (Equation 12). Optionoption model iteration is the compositional planning algorithm (OOMI) described in Section 5. For each algorithm we measured the total number of iterations (applications of the corresponding model equation) required; and also the mean number of backups (updates to an individual state) to each state.3 The results are shown in Table 3. In larger instances of both problems, the compositional approach required significantly fewer iterations and backups than either flat planning (APMI) or two-level planning (AOPMI), where options are first created and then used. In the Tower of Hanoi, APMI and AOPMI required a number of iterations that grew exponentially with the number of discs, whereas OOMI required just 1 additional iteration per disc in the deterministic case, and 8 additional iterations per disc in the stochastic case. In the Nine Rooms, the total iterations for APMI and AOPMI again grows exponentially with the level, but polynomially for OOMI."
    }, {
      "heading" : "7 Conclusion",
      "text" : "The Bellman optimality equation has motivated the development of a wide variety of MDP planning algorithms. We have generalised the Bellman equation\n3Note, however, that each backup has a larger cost with model iteration, since a complete row must be updated.\nin several important dimensions, enabling an analogous variety of compositional planning algorithms. We have illustrated one such approach, using optionoption model iteration. This is the first MDP planning algorithm to dynamically create its own planning operators. These operators are composed together to give increasingly deep and purposeful jumps through state space. Like value iteration, option-option model iteration applies full-width backups over complete sweeps of the state space. In principle, the model equations could also be solved by sample backups over sample trajectories, leading to compositional algorithms for hierarchical reinforcement learning. In this paper we have focused on planning with table lookup models; however, similar to MAXQ (Dietterich, 2000), HAMs (Andre & Russell, 2002) or skills (Konidaris & Barto, 2009), substantial efficiency improvements may be generated when each option model is provided with its own state abstraction."
    } ],
    "references" : [ {
      "title" : "On representations of problems of reasoning about actions",
      "author" : [ "S. Amarel" ],
      "venue" : "Machine Intelligence,",
      "citeRegEx" : "Amarel,? \\Q1968\\E",
      "shortCiteRegEx" : "Amarel",
      "year" : 1968
    }, {
      "title" : "State abstraction for programmable reinforcement learning agents",
      "author" : [ "D. Andre", "S. Russell" ],
      "venue" : "In 18th National Conference on Artificial Intelligence,",
      "citeRegEx" : "Andre and Russell,? \\Q2002\\E",
      "shortCiteRegEx" : "Andre and Russell",
      "year" : 2002
    }, {
      "title" : "Dynamic Programming",
      "author" : [ "R. Bellman" ],
      "venue" : null,
      "citeRegEx" : "Bellman,? \\Q1957\\E",
      "shortCiteRegEx" : "Bellman",
      "year" : 1957
    }, {
      "title" : "Hierarchical reinforcement learning with the MAXQ value function decomposition",
      "author" : [ "T. Dietterich" ],
      "venue" : "Journal of Artificial Intelligence Research,",
      "citeRegEx" : "Dietterich,? \\Q2000\\E",
      "shortCiteRegEx" : "Dietterich",
      "year" : 2000
    }, {
      "title" : "The role of macros in tractable planning",
      "author" : [ "A. Jonsson" ],
      "venue" : "Journal of Artificial Intelligence Research,",
      "citeRegEx" : "Jonsson,? \\Q2009\\E",
      "shortCiteRegEx" : "Jonsson",
      "year" : 2009
    }, {
      "title" : "Efficient skill learning using abstraction selection",
      "author" : [ "G. Konidaris", "A. Barto" ],
      "venue" : "In 21st International Joint Conference on Artificial Intelligence,",
      "citeRegEx" : "Konidaris and Barto,? \\Q2009\\E",
      "shortCiteRegEx" : "Konidaris and Barto",
      "year" : 2009
    }, {
      "title" : "Learning to solve problems by searching for macrooperators",
      "author" : [ "R. Korf" ],
      "venue" : "Pitman Publishing,",
      "citeRegEx" : "Korf,? \\Q1985\\E",
      "shortCiteRegEx" : "Korf",
      "year" : 1985
    }, {
      "title" : "Chunking in SOAR: The anatomy of a general learning mechanism",
      "author" : [ "J. Laird", "P. Rosenbloom", "A. Newell" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Laird et al\\.,? \\Q1986\\E",
      "shortCiteRegEx" : "Laird et al\\.",
      "year" : 1986
    }, {
      "title" : "Reinforcement learning with hierarchies of machines",
      "author" : [ "R. Parr", "S. Russell" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Parr and Russell,? \\Q1997\\E",
      "shortCiteRegEx" : "Parr and Russell",
      "year" : 1997
    }, {
      "title" : "Theoretical results on reinforcement learning with temporally abstract options",
      "author" : [ "D. Precup", "R. Sutton", "S. Singh" ],
      "venue" : "In 10th European Conference on Machine Learning,",
      "citeRegEx" : "Precup et al\\.,? \\Q1998\\E",
      "shortCiteRegEx" : "Precup et al\\.",
      "year" : 1998
    }, {
      "title" : "A structure for plans and behavior",
      "author" : [ "E. Sacerdoti" ],
      "venue" : "PhD thesis, Stanford University,",
      "citeRegEx" : "Sacerdoti,? \\Q1975\\E",
      "shortCiteRegEx" : "Sacerdoti",
      "year" : 1975
    }, {
      "title" : "TD models: Modeling the world at a mixture of time scales",
      "author" : [ "R. Sutton" ],
      "venue" : "In 12th International Conference on Machine Learning,",
      "citeRegEx" : "Sutton,? \\Q1995\\E",
      "shortCiteRegEx" : "Sutton",
      "year" : 1995
    }, {
      "title" : "Reinforcement Learning: an Introduction",
      "author" : [ "R. Sutton", "A. Barto" ],
      "venue" : null,
      "citeRegEx" : "Sutton and Barto,? \\Q1998\\E",
      "shortCiteRegEx" : "Sutton and Barto",
      "year" : 1998
    }, {
      "title" : "Between MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement learning",
      "author" : [ "R. Sutton", "D. Precup", "S. Singh" ],
      "venue" : "Artificial Intelligence,",
      "citeRegEx" : "Sutton et al\\.,? \\Q1999\\E",
      "shortCiteRegEx" : "Sutton et al\\.",
      "year" : 1999
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Classical planning algorithms make extensive use of temporal abstraction to construct high-level chunks of useful knowledge (Amarel, 1968; Sacerdoti, 1975; Korf, 1985; Laird et al., 1986).",
      "startOffset" : 124,
      "endOffset" : 187
    }, {
      "referenceID" : 10,
      "context" : "Classical planning algorithms make extensive use of temporal abstraction to construct high-level chunks of useful knowledge (Amarel, 1968; Sacerdoti, 1975; Korf, 1985; Laird et al., 1986).",
      "startOffset" : 124,
      "endOffset" : 187
    }, {
      "referenceID" : 6,
      "context" : "Classical planning algorithms make extensive use of temporal abstraction to construct high-level chunks of useful knowledge (Amarel, 1968; Sacerdoti, 1975; Korf, 1985; Laird et al., 1986).",
      "startOffset" : 124,
      "endOffset" : 187
    }, {
      "referenceID" : 7,
      "context" : "Classical planning algorithms make extensive use of temporal abstraction to construct high-level chunks of useful knowledge (Amarel, 1968; Sacerdoti, 1975; Korf, 1985; Laird et al., 1986).",
      "startOffset" : 124,
      "endOffset" : 187
    }, {
      "referenceID" : 13,
      "context" : "A closed-loop policy that is followed for some number of steps, and stops according to a termination condition that also depends on the state, is known as an option (Sutton et al., 1999).",
      "startOffset" : 165,
      "endOffset" : 186
    }, {
      "referenceID" : 11,
      "context" : "An option model describes the distribution of outcome states that would result from following the option (Sutton, 1995).",
      "startOffset" : 105,
      "endOffset" : 119
    }, {
      "referenceID" : 9,
      "context" : "Option models can also be composed together into more abstract option models (Precup et al., 1998).",
      "startOffset" : 77,
      "endOffset" : 98
    }, {
      "referenceID" : 13,
      "context" : "Option models are either constructed from primitive actions, in an approach known as intra-option model learning; or they are used to compute a value function, in an approach known as inter-option (or SMDP) planning (Sutton et al., 1999).",
      "startOffset" : 216,
      "endOffset" : 237
    }, {
      "referenceID" : 11,
      "context" : "(1995, 1999, Section 5) developed a Bellman expectation equation for state probabilities, but this work was restricted to Markov reward processes without actions (Sutton, 1995) or to fixed policies without control (Sutton et al.",
      "startOffset" : 162,
      "endOffset" : 176
    }, {
      "referenceID" : 13,
      "context" : "(1995, 1999, Section 5) developed a Bellman expectation equation for state probabilities, but this work was restricted to Markov reward processes without actions (Sutton, 1995) or to fixed policies without control (Sutton et al., 1999).",
      "startOffset" : 214,
      "endOffset" : 235
    }, {
      "referenceID" : 2,
      "context" : "(1995, 1999, Section 5) developed a Bellman expectation equation for state probabilities, but this work was restricted to Markov reward processes without actions (Sutton, 1995) or to fixed policies without control (Sutton et al., 1999). We present a Bellman optimality equation for state probabilities in Markov decision processes, including actions and control. Second, Precup et al. (1998) provided Bellman equations for composing option models into policies, but not into options.",
      "startOffset" : 36,
      "endOffset" : 392
    }, {
      "referenceID" : 2,
      "context" : "These architectures, including Dietterich’s MAXQ (2000), and Parr and Russell’s HAMs (1997; 2002), do construct the solution to one subproblem from the solution to other subproblems.",
      "startOffset" : 31,
      "endOffset" : 56
    }, {
      "referenceID" : 2,
      "context" : "This algorithm is called value iteration (Bellman, 1957).",
      "startOffset" : 41,
      "endOffset" : 56
    }, {
      "referenceID" : 11,
      "context" : "We now formalise these ideas, following Sutton (1995).",
      "startOffset" : 40,
      "endOffset" : 54
    }, {
      "referenceID" : 11,
      "context" : "This block matrix notation for models and block vector notation for rasps are known as homogeneous coordinates (Sutton, 1995).",
      "startOffset" : 111,
      "endOffset" : 125
    }, {
      "referenceID" : 13,
      "context" : "A state value function V (s) can be represented by composition with the corresponding value model, sV; an action value function can be represented by sAV; and an inter-option value function (Sutton et al., 1999) can be represented by sOV.",
      "startOffset" : 190,
      "endOffset" : 211
    }, {
      "referenceID" : 3,
      "context" : "The hierarchically optimal policy model max Π Ππ | supp(π)⊆Ω is the max model over this set; it is analogous to a hierarchically optimal value function (Dietterich, 2000), i.",
      "startOffset" : 152,
      "endOffset" : 170
    }, {
      "referenceID" : 3,
      "context" : "In contrast, recursive optimality (Dietterich, 2000) is a weaker, local optimality condition that assumes all suboptions are fixed.",
      "startOffset" : 34,
      "endOffset" : 52
    }, {
      "referenceID" : 3,
      "context" : "In this paper we have focused on planning with table lookup models; however, similar to MAXQ (Dietterich, 2000), HAMs (Andre & Russell, 2002) or skills (Konidaris & Barto, 2009), substantial efficiency improvements may be generated when each option model is provided with its own state abstraction.",
      "startOffset" : 93,
      "endOffset" : 111
    } ],
    "year" : 2012,
    "abstractText" : "In this paper we introduce a framework for option model composition. Option models are temporal abstractions that, like macrooperators in classical planning, jump directly from a start state to an end state. Prior work has focused on constructing option models from primitive actions, by intra-option model learning; or on using option models to construct a value function, by inter-option planning. We present a unified view of intraand inter-option model learning, based on a major generalisation of the Bellman equation. Our fundamental operation is the recursive composition of option models into other option models. This key idea enables compositional planning over many levels of abstraction. We illustrate our framework using a dynamic programming algorithm that simultaneously constructs optimal option models for multiple subgoals, and also searches over those option models to provide rapid progress towards other subgoals.",
    "creator" : "LaTeX with hyperref package"
  }
}