{
  "name" : "1610.08696.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Learning Bound for Parameter Transfer Learning",
    "authors" : [ "Wataru Kumagai" ],
    "emails" : [ "kumagai@kanagawa-u.ac.jp" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n61 0.\n08 69\n6v 1\n[ st\nat .M\nL ]\n2 7\nO ct"
    }, {
      "heading" : "1 Introduction",
      "text" : "In traditional machine learning, it is assumed that data are identically drawn from a single distribution. However, this assumption does not always hold in real-world applications. Therefore, it would be significant to develop methods capable of incorporating samples drawn from different distributions. In this case, transfer learning provides a general way to accommodate these situations. In transfer learning, besides the availability of relatively few samples related with an objective task, abundant samples in other domains that are not necessarily drawn from an identical distribution, are available. Then, transfer learning aims at extracting some useful knowledge from data in other domains and applying the knowledge to improve the performance of the objective task. In accordance with the kind of knowledge that is transferred, approaches to solving transfer-learning problems can be classified into cases such as instance transfer, feature representation transfer, and parameter transfer (Pan and Yang (2010)). In this paper, we consider the parameter transfer approach, where some kind of parametric model is supposed and the transferred knowledge is encoded into parameters. Since the parameter transfer approach typically requires many samples to accurately learn a suitable parameter, unsupervised methods are often utilized for the learning process. In particular, transfer learning from unlabeled data for predictive tasks is known as self-taught learning (Raina et al. (2007)), where a joint generative model is not assumed to underlie unlabeled samples even though the unlabeled samples should be indicative of a structure that would subsequently be helpful in predicting tasks. In recent years, self-taught learning has been intensively studied, encouraged by the development of strong unsupervised methods. Furthermore, sparsity-based methods such as sparse coding or sparse neural networks have often been used in empirical studies of self-taught learning.\nAlthough many algorithms based on the parameter transfer approach have empirically demonstrated impressive performance in self-taught learning, some fundamental problems remain. First, the theoretical aspects of the parameter transfer approach have not been studied, and in particular, no learning bound was obtained. Second, although it is believed that a large amount of unlabeled data help to improve the performance of the objective task in self-taught learning, it has not been sufficiently clarified how many samples are required. Third, although sparsity-based methods are typically employed in self-taught learning, it is unknown how the sparsity works to guarantee the performance of self-taught learning.\nThe aim of the research presented in this paper is to shed light on the above problems. We first consider a general model of parametric feature mapping in the parameter transfer approach. Then, we newly formulate the local stability of parametric feature mapping and the parameter transfer learnability for this mapping, and provide a theoretical learning bound for parameter transfer learning algorithms based on the notions. Next, we consider the stability of sparse coding. Then we discuss the parameter transfer learnability by dictionary learning under the sparse model. Applying the learning bound for parameter transfer learning algorithms, we provide a learning bound of the sparse coding algorithm in self-taught learning.\nThis paper is organized as follows. In the remainder of this section, we refer to some related studies. In Section 2, we formulate the stability and the parameter transfer learnability of the parametric feature mapping. Then, we present a learning bound for parameter transfer learning. In Section 3, we show the stability of the sparse coding under perturbation of the dictionaries. Then, by imposing sparsity assumptions on samples and by considering dictionary learning, we derive the parameter transfer learnability for sparse coding. In particular, a learning bound is obtained for sparse coding in the setting of self-taught learning. In Section 4, we conclude the paper."
    }, {
      "heading" : "1.1 Related Work",
      "text" : "Approaches to transfer learning can be classified into some cases based on the kind of knowledge being transferred (Pan and Yang (2010)). In this paper, we consider the parameter transfer approach. This approach can be applied to various notable algorithms such as sparse coding, multiple kernel learning, and deep learning since the dictionary, weights on kernels, and weights on the neural network are regarded as parameters, respectively. Then, those parameters are typically trained or tuned on samples that are not necessarily drawn from a target region. In the parameter transfer setting, a number of samples in the source region are often needed to accurately estimate the parameter to be transferred. Thus, it is desirable to be able to use unlabeled samples in the source region.\nSelf-taught learning corresponds to the case where only unlabeled samples are given in the source region while labeled samples are available in the target domain. In this sense, self-taught learning is compatible with the parameter transfer approach. Actually, in Raina et al. (2007) where self-taught learning was first introduced, the sparse coding-based method is employed and the parameter transfer approach is already used regarding the dictionary learnt from images as the parameter to be transferred. Although self-taught learning has been studied in various contexts (Dai et al. (2008); Lee et al. (2009); Wang et al. (2013); Zhu et al. (2013)), its theoretical aspects have not been sufficiently analyzed. One of the main results in this paper is to provide a first theoretical learning bound in self-taught learning with the parameter transfer approach. We note that our setting differs from the environment-based setting (Baxter (2000), Maurer (2009)), where a distribution on distributions on labeled samples, known as an environment, is assumed. In our formulation, the existence of the environment is not assumed and labeled data in the source region are not required.\nSelf-taught learning algorithms are often based on sparse coding. In the seminal paper by Raina et al. (2007), they already proposed an algorithm that learns a dictionary in the source region and transfers it to the target region. They also showed the effectiveness of the sparse coding-based method. Moreover, since remarkable progress has been made in unsupervised learning based on sparse neural networks (Coates et al. (2011), Le (2013)), unlabeled samples of the source domain in self-taught learning are often preprocessed by sparsity-based methods. Recently, a sparse coding-based generalization bound was studied (Mehta and Gray (2013); Maurer et al. (2012)) and the analysis in Section 3.1 is based on (Mehta and Gray (2013))."
    }, {
      "heading" : "2 Learning Bound for Parameter Transfer Learning",
      "text" : ""
    }, {
      "heading" : "2.1 Problem Setting of Parameter Transfer Learning",
      "text" : "We formulate parameter transfer learning in this subsection. We first briefly introduce notations and terminology in transfer learning (Pan and Yang (2010)). Let X and Y be a sample space and a label space, respectively. We refer to a pair of Z := X × Y and a joint distribution P (x, y) on Z as a region. Then, a domain comprises a pair consisting of a sample space X and a marginal probability of P (x) on X and a task consists of a pair containing a label set Y and a conditional distribution P (y|x). In addition, let H = {h : X → Y} be a hypothesis space and ℓ : Z × Z → R≥0\nrepresent a loss function. Then, the expected risk and the empirical risk are defined by R(h) := Ez=(x,y)∼P [ℓ(z, h(x))] and R̂n(h) := 1n ∑n j=1 ℓ(yj, h(xj)), respectively. In the setting of transfer learning, besides samples from a region of interest known as a target region, it is assumed that samples from another region known as a source region are also available. We distinguish between the target and source regions by adding a subscript T or S to each notation introduced above, (e.g. PT , RS ). Then, the homogeneous setting (i.e., XS = XT ) is not assumed in general, and thus, the heterogeneous setting (i.e., XS 6= XT ) can be treated. We note that self-taught learning, which is treated in Section 3, corresponds to the case when the label space YS in the source region is the set of a single element.\nWe consider the parameter transfer approach, where the knowledge to be transferred is encoded into a parameter. The parameter transfer approach aims to learn a hypothesis with low expected risk for the target task by obtaining some knowledge about an effective parameter in the source region and transfer it to the target region. In this paper, we suppose that there are parametric models on both the source and target regions and that their parameter spaces are partly shared. Then, our strategy is to learn an effective parameter in the source region and then transfer a part of the parameter to the target region. We describe the formulation in the following. In the target region, we assume that YT ⊂ R and there is a parametric feature mapping ψθ : XT → Rm on the target domain such that each hypothesis hT ,θ,w : XT → YT is represented by\nhT ,θ,w(x) := 〈w, ψθ(x)〉 (1) with parameters θ ∈ Θ and w ∈ WT , where Θ is a subset of a normed space with a norm ‖ · ‖ and WT is a subset of Rm. Then the hypothesis set in the target region is parameterized as\nHT = {hT ,θ,w|θ ∈ Θ,w ∈ WT }.\nIn the following, we simply denote RT (hT ,θ,w) and R̂T (hT ,θ,w) by RT (θ,w) and R̂T (θ,w), respectively. In the source region, we suppose that there exists some kind of parametric model such as a sample distribution PS,θ,w or a hypothesis hS,θ,w with parameters θ ∈ Θ and w ∈ WS , and a part Θ of the parameter space is shared with the target region. Then, let θ∗S ∈ Θ and w∗S ∈ WS be parameters that are supposed to be effective in the source region (e.g., the true parameter of the sample distribution, the parameter of the optimal hypothesis with respect to the expected risk RS); however, explicit assumptions are not imposed on the parameters. Then, the parameter transfer algorithm treated in this paper is described as follows. Let N - and n-samples be available in the source and target regions, respectively. First, a parameter transfer algorithm outputs the estimator θ̂N ∈ Θ of θ∗S by using N -samples. Next, for the parameter\nw ∗ T := argmin w∈WT RT (θ∗S ,w)\nin the target region, the algorithm outputs its estimator\nŵN,n := argmin w∈WT R̂T ,n(θ̂N ,w) + ρr(w)\nby using n-samples, where r(w) is a 1-strongly convex function with respect to ‖ · ‖2 and ρ > 0. If the source region relates to the target region in some sense, the effective parameter θ∗S in the source region is expected to also be useful for the target task. In the next subsection, we regard RT (θ∗S ,w∗T ) as the baseline of predictive performance and derive a learning bound."
    }, {
      "heading" : "2.2 Learning Bound Based on Stability and Learnability",
      "text" : "We newly introduce the local stability and the parameter transfer learnability as below. These notions are essential to derive a learning bound in Theorem 1.\nDefinition 1 (Local Stability). A parametric feature mapping ψθ is said to be locally stable if there exist ǫθ : X → R>0 for each θ ∈ Θ and Lψ > 0 such that for θ′ ∈ Θ\n‖θ − θ′‖ ≤ ǫθ(x) ⇒ ‖ψθ(x)− ψθ′(x)‖2 ≤ Lψ‖θ − θ′‖.\nWe term ǫθ(x) the permissible radius of perturbation for θ at x. For samples Xn = {x1, . . .xn}, we denote as ǫθ(Xn) := minj∈[n] ǫθ(xj), where [n] := {1, . . . , n} for a positive integer n. Next, we formulate the parameter transfer learnability based on the local stability.\nDefinition 2 (Parameter Transfer Learnability). Suppose that N -samples in the source domain and n-samples Xn in the target domain are available. Let a parametric feature mapping {ψθ}θ∈Θ be locally stable. For δ̄ ∈ [0, 1), {ψθ}θ∈Θ is said to be parameter transfer learnable with probability 1 − δ̄ if there exists an algorithm that depends only on N -samples in the source domain such that, the output θ̂N of the algorithm satisfies\nPr [ ‖θ̂N − θ∗S‖ ≤ ǫθ∗S (X n) ] ≥ 1− δ̄.\nIn the following, we assume that parametric feature mapping is bounded as ‖ψθ(x)‖2 ≤ Rψ for arbitrary x ∈ X and θ ∈ Θ and linear predictors are also bounded as ‖w‖2 ≤ RW for any w ∈ W . In addition, we suppose that a loss function ℓ(·, ·) is Lℓ-Lipschitz and convex with respect to the second variable. We denote as Rr := supw∈W |r(w)|. Then, the following learning bound is obtained, where the strong convexity of the regularization term ρr(w) is essential.\nTheorem 1 (Learning Bound). Suppose that the parametric feature mapping ψθ is locally stable and an estimator θ̂N learned in the source region satisfies the parameter transfer learnability with\nprobability 1− δ̄. When ρ = LℓRψ √\n8(32+log(2/δ)) Rrn , the following inequality holds with probability\n1− (δ + 2δ̄):\nRT ( θ̂N , ŵN,n ) −RT (θ∗S ,w∗T )\n≤ LℓRψ ( RW √ 2 log(2/δ) + 2 √ 2Rr(32 + log(2/δ)) ) 1√ n + LℓLψRψ ∥∥∥θ̂N − θ∗S ∥∥∥\n+Lℓ √ LψRWRψ\n( Rr\n2(32 + log(2/δ))\n) 1 4\nn 1 4 √∥∥∥θ̂N − θ∗S ∥∥∥. (2)\nIf the estimation error ‖θ̂N − θ∗S‖ can be evaluated in terms of the number N of samples, Theorem 1 clarifies which term is dominant, and in particular, the number of samples required in the source domain such that this number is sufficiently large compared to the samples in the target domain."
    }, {
      "heading" : "2.3 Proof of Learning Bound",
      "text" : "We prove Theorem 1 in this subsection. In this proof, we omit the subscript T for simplicity. In addition, we denote θ∗S simply by θ ∗. We set as\nŵ ∗ n := argmin\nw∈W\n1\nn\nn∑\nj=1\nℓ(yj , 〈w, ψθ∗(xj)〉) + ρr(w).\nThen, we have\nRT ( θ̂N , ŵN,n ) −RT (θ∗,w∗)\n= E(x,y)∼P [ ℓ(y, 〈ŵN,n, ψθ̂N (x)〉) ] − E(x,y)∼P [ℓ(y, 〈ŵN,n, ψθ∗(x)〉)]\n+E(x,y)∼P [ℓ(y, 〈ŵN,n, ψθ∗(x)〉)] − E(x,y)∼P [ℓ(y, 〈ŵ∗n, ψθ∗(x)〉)] (3) +E(x,y)∼P [ℓ(y, 〈ŵ∗n, ψθ∗(x)〉)] − E(x,y)∼P [ℓ(y, 〈w∗, ψθ∗(x)〉)] .\nIn the following, we bound three parts of (3). First, we have the following inequality with probability 1− (δ/2 + δ̄):\nE(x,y)∼P [ ℓ(y, 〈ŵN,n, ψθ̂N (x)〉) ] − E(x,y)∼P [ℓ(y, 〈ŵN,n, ψθ∗(x)〉)]\n≤ LℓRWE(x,y)∼P [∥∥∥ψ\nθ̂N (x) − ψθ∗(x)\n∥∥∥ ]\n≤ LℓRW 1\nn\nn∑\nj=1\n∥∥∥ψ θ̂N (xj)− ψθ∗(xj) ∥∥∥+ LℓRWRψ\n√ 2 log(2/δ)\nn\n≤ LℓLψRW ∥∥∥θ̂N − θ∗ ∥∥∥+ LℓRWRψ √ 2 log(2/δ)\nn ,\nwhere we used Hoeffding’s inequality as the third inequality, and the local stability and parameter transfer learnability in the last inequality. Second, we have the following inequality with probability 1− δ̄:\nE(x,y)∼P [ℓ(y, 〈ŵN,n, ψθ∗(x)〉)] − E(x,y)∼P [ℓ(y, 〈ŵ∗n, ψθ∗(x)〉)] ≤ LℓE(x,y)∼P [|〈ŵN,n, ψθ∗(x)〉 − 〈ŵ∗n, ψθ∗(x)〉|] ≤ LℓRψ ‖ŵN,n − ŵ∗n‖2\n≤ LℓRψ\n√ 2LℓLψRW\nρ\n∥∥∥θ̂N − θ∗ ∥∥∥, (4)\nwhere the last inequality is derived by the strong convexity of the regularizer ρr(w) in the Appendix. Third, the following holds by Theorem 1 of Sridharan et al. (2009) with probability 1− δ/2:\nE(x,y)∼P [ℓ(y, 〈ŵ∗n, ψθ∗(x)〉)] − E(x,y)∼P [ℓ(y, 〈w∗, ψθ∗(x)〉)] = E(x,y)∼P [ℓ(y, 〈ŵ∗n, ψθ∗(x)〉) + ρr(ŵ∗n)]\n−E(x,y)∼P [ℓ(y, 〈w∗, ψθ∗(x)〉) + ρr(w∗)] + ρ(r(w∗)− r(ŵ∗n))\n≤ ( 8L2ℓR 2 ψ(32 + log(2/δ))\nρn\n) + ρRr.\nThus, when ρ = LℓRψ √\n8(32+log(2/δ)) Rrn , we have (2) with probability 1− (δ + 2δ̄)."
    }, {
      "heading" : "3 Stability and Learnability in Sparse Coding",
      "text" : "In this section, we consider the sparse coding in self-taught learning, where the source region essentially consists of the sample space XS without the label space YS . We assume that the sample spaces in both regions are Rd. Then, the sparse coding method treated here consists of a two-stage procedure, where a dictionary is learnt on the source region, and then a sparse coding with the learnt dictionary is used for a predictive task in the target region.\nFirst, we show that sparse coding satisfies the local stability in Section 3.1 and next explain that appropriate dictionary learning algorithms satisfy the parameter transfer learnability in Section 3.4. As a consequence of Theorem 1, we obtain the learning bound of self-taught learning algorithms based on sparse coding. We note that the results in this section are useful independent of transfer learning.\nWe here summarize the notations used in this section. Let ‖ · ‖p be the p-norm on Rd. We define as supp(a) := {i ∈ [m]|ai 6= 0} for a ∈ Rm. We denote the number of elements of a set S by |S|. When a vector a satisfies ‖a‖0 = |supp(a)| ≤ k, a is said to be k-sparse. We denote the ball with radius R centered at 0 by BRd(R) := {x ∈ Rd|‖x‖2 ≤ R}. We set as D := {D = [d1, . . . ,dm] ∈ BRd(1)\nm|‖dj‖2 = 1 (i = 1, . . . ,m)} and each D ∈ D a dictionary with size m. Definition 3 (Induced matrix norm). For an arbitrary matrix E = [e1, . . . , em] ∈ Rd×m, 1) the induced matrix norm is defined by ‖E‖1,2 := maxi∈[m] ‖ei‖2.\nWe adopt ‖ · ‖1,2 to measure the difference of dictionaries since it is typically used in the framework of dictionary learning. We note that ‖D− D̃‖1,2 ≤ 2 holds for arbitrary dictionaries D, D̃ ∈ D."
    }, {
      "heading" : "3.1 Local Stability of Sparse Representation",
      "text" : "We show the local stability of sparse representation under a sparse model. A sparse representation with dictionary parameter D of a sample x ∈ Rd is expressed as follows:\nϕD(x) := argmin z∈Rm\n1 2 ‖x−Dz‖22 + λ‖z‖1,\n1) In general, the (p, q)-induced norm for p, q ≥ 1 is defined by ‖E‖p,q := supv∈Rm,‖v‖p=1 ‖Ev‖q . Then, ‖ · ‖1,2 in this general definition coincides with that in Definition 3 by Lemma 17 of Vainsencher et al. (2011).\nwhere λ > 0 is a regularization parameter. This situation corresponds to the case where θ = D and ψθ = ϕD in the setting of Section 2.1. We prepare some notions to the stability of the sparse representation. The following margin and incoherence were introduced by Mehta and Gray (2013).\nDefinition 4 (k-margin). Given a dictionary D = [d1, . . . ,dm] ∈ D and a point x ∈ Rd, the k-margin of D on x is\nMk(D,x) := max I⊂[m],|I|=m−k min j∈I {λ− |〈dj ,x−DϕD(x)〉|} .\nDefinition 5 (µ-incoherence). A dictionary matrix D = [d1, . . . ,dm] ∈ D is termed µ-incoherent if 〈di,dj〉 ≤ µ/ √ d for all i 6= j.\nThen, the following theorem is obtained.\nTheorem 2 (Sparse Coding Stability). Let D ∈ D be µ-incoherent and λ ≤ 1. When\n‖D− D̃‖1,2 ≤ ǫk,D(x) := Mk,D(x)2λ\n64max{1, ‖x‖}4 , (5)\nthe following stability bound holds:\n‖ϕD(x) − ϕD̃(x)‖2 ≤ 4‖x‖2\n√ k\n(1− µk/ √ d)λ ‖D− D̃‖1,2.\nFrom Theorem 2, ǫk,D(x) becomes the permissible radius of perturbation in Definition 1.\nHere, we refer to the relation with the sparse coding stability (Theorem 4) of Mehta and Gray (2013), who measured the difference of dictionaries by ‖ · ‖2,2 instead of ‖ · ‖1,2 and the permissible radius of perturbation is given by Mk,D(x)2λ except for a constant factor. Applying the simple inequality ‖E‖2,2 ≤ √ m‖E‖1,2 for E ∈ Rd×m, we can obtain a variant of the sparse coding stability with the norm ‖ · ‖1,2. However, then the dictionary size m affects the permissible radius of perturbation and the stability bound of the sparse coding stability. On the other hand, the factor of m does not appear in Theorem 2, and thus, the result is effective even for a large m. In addition, whereas ‖x‖ ≤ 1 is assumed in Mehta and Gray (2013), Theorem 2 does not assume that ‖x‖ ≤ 1 and clarifies the dependency for the norm ‖x‖. In existing studies related to sparse coding, the sparse representation ϕD(x) is modified as ϕD(x)⊗ x (Mairal et al. (2009)) or ϕD(x) ⊗ (x − DϕD(x)) (Raina et al. (2007)) where ⊗ is the tensor product. By the stability of sparse representation (Theorem 2), it can be shown that such modified representations also have local stability."
    }, {
      "heading" : "3.2 Sparse Modeling and Margin Bound",
      "text" : "In this subsection, we assume a sparse structure for samples x ∈ Rd and specify a lower bound for the k-margin used in (5). The result obtained in this section plays an essential role to show the parameter transfer learnability in Section 3.4. Assumption 1 (Model). There exists a dictionary matrix D∗ such that every sample x is independently generated by a representation a and noise ξ as\nx = D∗a+ ξ.\nMoreover, we impose the following three assumptions on the above model.\nAssumption 2 (Dictionary). The dictionary matrix D∗ = [d1, . . . ,dm] ∈ D is µ-incoherent. Assumption 3 (Representation). The representation a is a random variable that is k-sparse (i.e., ‖a‖0 ≤ k) and the non-zero entries are lower bounded by C > 0 (i.e., ai 6= 0 satisfy |ai| ≥ C). Assumption 4 (Noise). The noise ξ is independent across coordinates and sub-Gaussian with parameter σ/ √ d on each component.\nWe note that the assumptions do not require the representation a or noise ξ to be identically distributed while those components are independent. This is essential because samples in the source and target domains cannot be assumed to be identically distributed in transfer learning.\nTheorem 3 (Margin Bound). Let 0 < t < 1. We set as\nδt,λ := 2σ\n(1− t) √ dλ exp\n( − (1− t) 2dλ2\n8σ2\n) +\n2σm√ dλ exp\n( −dλ 2\n8σ2\n)\n+ 4σk\nC √ d(1 − µk/ √ d) exp\n( −C 2d(1 − µk/ √ d)\n8σ2\n) +\n8σ(d− k)√ dλ exp\n( − dλ 2\n32σ2\n) . (6)\nWe suppose that d ≥ {(\n1 + 6(1−t)\n) µk }2\nand λ = d−τ for arbitrary 1/4 ≤ τ ≤ 1/2. Under Assumptions 1-4, the following inequality holds with probability 1− δt,λ at least:\nMk,D∗(x) ≥ tλ. (7)\nWe refer to the regularization parameter λ. An appropriate reflection of the sparsity of samples requires the regularization parameter λ to be set suitably. According to Theorem 4 of Zhao and Yu (2006)2), when samples follow the sparse model as in Assumptions 1-4 and λ ∼= d−τ for 1/4 ≤ τ ≤ 1/2, the representation ϕD(x) reconstructs the true sparse representation a of sample x with a small error. In particular, when τ = 1/4 (i.e., λ ∼= d−1/4) in Theorem 3, the failure probability δt,λ ∼= e− √ d on the margin is guaranteed to become sub-exponentially small with respect to dimension d and is negligible for the high-dimensional case. On the other hand, the typical choice τ = 1/2 (i.e., λ ∼= d−1/2) does not provide a useful result because δt,λ is not small at all."
    }, {
      "heading" : "3.3 Proof of Margin Bound",
      "text" : "We provide a sketch of the proof of Theorem 3. We denote the first term, the second term, and the sum of the third and fourth terms of (6) by δ1, δ2 and δ3, respectively. From Assumptions 1 and 3, a sample is represented as x = D∗a + ξ and ‖a‖0 ≤ k. Without the loss of generality, we assume that the first m− k components of a are 0 and the last k components are not 0. Since Mk,D∗(x) ≥ min\n1≤j≤m−k λ− 〈dj ,x−D∗ϕD(x)〉 = min 1≤j≤m−k λ− 〈dj , ξ〉 − 〈D∗⊤dj , a− ϕD(x)〉,\nit suffices to show that the following holds for an arbitrary 1 ≤ j ≤ m− k to prove Theorem 3: Pr[〈dj , ξ〉+ 〈D∗⊤dj , a− ϕD(x)〉 > (1− t)λ] ≤ δt,λ. (8)\nThen, (8) follows from the following inequalities:\nPr [ 〈dj , ξ〉 >\n1− t 2 λ\n] ≤ δ1, (9)\nPr [ 〈D∗⊤dj , a− ϕD(x)〉 >\n1− t 2 λ\n] ≤ δ2 + δ3. (10)\nThe inequality (9) holds since ‖dj‖ = 1 by the definition and Assumption 4. Thus, all we have to do is to show (10). We have\n〈D∗⊤dj , a− ϕD(x)〉 = 〈[〈d1,dj〉, . . . , 〈dm,dj〉]⊤, a− ϕD(x)〉 = 〈(1supp(a−ϕD(x)) ◦ [〈d1,dj〉, . . . , 〈dm,dj〉])⊤, a− ϕD(x)〉 ≤ ‖1supp(a−ϕD(x)) ◦ [〈d1,dj〉, . . . , 〈dm,dj〉]‖2‖a− ϕD(x)‖2,(11)\nwhere u ◦ v is the Hadamard product (i.e., component-wise product) between u and v, and 1A for a set A ⊂ [m] is a vector whose i-th component is 1 if i ∈ A and 0 otherwise. Applying Theorem 4 of Zhao and Yu (2006) and using the condition for λ, the following holds with probability 1− δ3:\nsupp(a) = supp(ϕD(x)). (12)\n2)Theorem 4 of Zhao and Yu (2006) is stated for Gaussian noise. However, it can be easily generalized to sub-Gaussian noise as in Assumption 4. Our setting corresponds to the case in which c1 = 1/2, c2 = 1, c3 = (log κ + log log d)/ log d for some κ > 1 (i.e., ed\nc3 ∼= dκ) and c4 = c in Theorem 4 of Zhao and Yu (2006). Note that our regularization parameter λ corresponds to λd/d in (Zhao and Yu (2006)).\nMoreover, under (12), the following holds with probability 1 − δ2 by modifying Corollary 1 of Negahban et al. (2009) and using the condition for λ:\n‖a− ϕD(x)‖2 ≤ 6 √ kλ\n1− µk√ d\n. (13)\nThus, if both of (12) and (13) hold, the right-hand side of (11) is bounded as follows:\n‖1supp(a−ϕD(x)) ◦ [〈d1,dj〉, . . . , 〈dm,dj〉]‖2‖a− ϕD(x)‖2\n≤ √ |supp(a− ϕD(x))|\nµ√ d\n6 √ kλ\n1− µk√ d\n= 6µk√ d− µk λ ≤ 1− t 2 λ,\nwhere we used Assumption 2 in the first inequality, (12) and Assumption 3 in the equality, and the condition for d in the last inequality. From the above discussion, the left-hand side of (10) is bounded by the sum of the probability δ3 that (12) does not hold and the probability δ2 that (12) holds but (13) does not hold."
    }, {
      "heading" : "3.4 Transfer Learnability for Dictionary Learning",
      "text" : "When the true dictionary D∗ exists as in Assumption 1, we show that the output D̂N of a suitable dictionary learning algorithm from N -unlabeled samples satisfies the parameter transfer learnability for the sparse coding ϕD. Then, Theorem 1 guarantees the learning bound in self-taught learning since the discussion in this section does not assume the label space in the source region. This situation corresponds to the case where θ∗S = D\n∗, θ̂N = D̂N and ‖ · ‖ = ‖ · ‖1,2 in Section 2.1. We show that an appropriate dictionary learning algorithm satisfies the parameter transfer learnability for the sparse coding ϕD by focusing on the permissible radius of perturbation in (5) under some assumptions. When Assumptions 1-4 hold and λ = d−τ for 1/4 ≤ τ ≤ 1/2, the margin bound (7) for x ∈ X holds with probability 1− δt,λ, and thus, we have\nǫk,D∗(x) ≥ t2λ3\n64max{1, ‖x‖}4 = Θ(d −3τ ).\nThus, if a dictionary learning algorithm outputs the estimator D̂N such that ‖D̂N −D∗‖1,2 ≤ O(d−3τ ) (14) with probability 1− δN , the estimator D̂N of D∗ satisfies the parameter transfer learnability for the sparse coding ϕD with probability δ̄ = δN + nδt,λ. Then, by the local stability of the sparse representation and the parameter transfer learnability of such a dictionary learning, Theorem 1 guarantees that sparse coding in self-taught learning satisfies the learning bound in (2).\nWe note that Theorem 1 can apply to any dictionary learning algorithm as long as (14) is satisfied. For example, Arora et al. (2015) show that, when k = O( √ d/ log d), m = O(d), Assumptions 1-4\nand some additional conditions are assumed, their dictionary learning algorithm outputs D̂N which satisfies ‖D̂N −D∗‖1,2 = O(d−M ) with probability 1− d−M ′ for arbitrarily large M,M ′ as long as N is sufficiently large."
    }, {
      "heading" : "4 Conclusion",
      "text" : "We derived a learning bound (Theorem 1) for a parameter transfer learning problem based on the local stability and parameter transfer learnability, which are newly introduced in this paper. Then, applying it to a sparse coding-based algorithm under a sparse model (Assumptions 1-4), we obtained the first theoretical guarantee of a learning bound in self-taught learning. Although we only consider sparse coding, the framework of parameter transfer learning includes other promising algorithms such as multiple kernel learning and deep neural networks, and thus, our results are expected to be effective to analyze the theoretical performance of these algorithms. Finally, we note that our learning bound can be applied to different settings from self-taught learning because Theorem 1 includes the case in which labeled samples are available in the source region."
    }, {
      "heading" : "A Appendix: Lemma for Proof of Theorem 1",
      "text" : "In this subsection, we omit the subscript T for simplicity. In addition, we denote θ∗S by θ∗ simply. We recall\nŵN,n := argmin w∈WT\n1\nn\nn∑\nj=1\nℓ(yj, 〈w, ψθ̂N (x)〉) + ρr(w),\nŵ ∗ n := argmin\nw∈W\n1\nn\nn∑\nj=1\nℓ(yj, 〈w, ψθ∗(xj)〉) + ρr(w).\nThe inequality (4) is obtained by the following lemma.\nLemma 1. The following holds with probability 1− δ̄:\n‖ŵN,n − ŵ∗n‖2 ≤ √\n2RWLℓLψ ρ\n∥∥∥θ̂N − θ∗ ∥∥∥. (15)\n[Proof] Let us define as\nf̂N,n(w) := 1\nn\nn∑\nj=1\nℓ(yj, 〈w, ψθ̂N (x)〉) + ρr(w),\nf̂∗n(w) := 1\nn\nn∑\nj=1\nℓ(yj, 〈w, ψθ∗(xj)〉) + ρr(w).\nIf\nf̂∗n(ŵ ∗ n) ≤ f̂N,n(ŵN,n),\nwe have the following with probability 1− δ̄: f̂N,n(ŵ ∗ n)− f̂N,n(ŵN,n) ≤ f̂N,n(ŵ∗n)− f̂∗n(ŵ∗n) + f̂∗n(ŵ∗n)− f̂N,n(ŵN,n)\n≤ f̂N,n(ŵ∗n)− f̂∗n(ŵ∗n)\n= 1\nn\nn∑\nj=1\nℓ(yj , 〈ŵ∗n, ψθ̂N (xj)〉)− 1\nn\nn∑\nj=1\nℓ(yj , 〈ŵ∗n, ψθ∗(xj)〉)\n≤ 1 n\nn∑\nj=1\nLℓ ∣∣∣〈ŵ∗n, ψθ̂N (xj)〉 − 〈ŵ ∗ n, ψθ∗(xj)〉 ∣∣∣\n≤ 1 n\nn∑\nj=1\nLℓRW ∥∥∥ψ\nθ̂N (xj)− ψθ∗(xj)\n∥∥∥\n≤ 1 n\nn∑\nj=1\nLℓRWLψ ∥∥∥θ̂N − θ∗ ∥∥∥\n= LℓRWLψ ∥∥∥θ̂N − θ∗ ∥∥∥ .\nSince f̂N,n is ρ-strongly convex and ŵN,n is its miniizer,\nf̂N,n(ŵ ∗ n)− f̂N,n(ŵN,n) ≥\nρ 2 ‖ŵ∗n − ŵN,n‖22.\nThus, we obtain (15).\nSimilarly, if\nf̂∗n(ŵ ∗ n) ≥ f̂N,n(ŵN,n),\nwe have the following with probability 1− δ̄: f̂∗n(ŵN,n)− f̂∗n(ŵ∗n) ≤ f̂∗n(ŵN,n)− f̂N,n(ŵN,n) + f̂N,n(ŵN,n)− f̂∗n(ŵ∗n)\n≤ f̂∗n(ŵN,n)− f̂N,n(ŵN,n)\n= 1\nn\nn∑\nj=1\nℓ(yj , 〈ŵN,n, ψθ∗(xj)〉)− 1\nn\nn∑\nj=1\nℓ(yj, 〈ŵN,n, ψθ̂N (xj)〉)\n≤ 1 n\nn∑\nj=1\nLℓ ∣∣∣〈ŵN,n, ψθ∗(xj)〉 − 〈ŵN,n, ψθ̂N (xj)〉 ∣∣∣\n≤ 1 n\nn∑\nj=1\nLℓRW ∥∥∥ψθ∗(xj)− ψθ̂N (xj) ∥∥∥\n≤ 1 n\nn∑\nj=1\nLℓRWLψ ∥∥∥θ̂N − θ∗ ∥∥∥\n= LℓRWLψ ∥∥∥θ̂N − θ∗ ∥∥∥ .\nSince f̂∗n is ρ-strongly convex and ŵ ∗ n is its minimizer,\nf̂∗n(ŵN,n)− f̂∗n(ŵ∗n) ≥ ρ 2 ‖ŵN,n − ŵ∗n‖22.\nThus, we obtain (15)."
    }, {
      "heading" : "B Appendix: Proof of Sparse Coding Stability",
      "text" : "The proof of Theorem 2 is almost the same as that of Theorem 1 in Mehta and Gray (2012). However, since a part of the proof can not applied to our setting, we provide the full proof of Theorem 2 in this section. Lemma 2. Let a ∈ Rm and E ∈ Rd×m. Then, ‖Ea‖2 ≤ ‖E‖1,2‖a‖1.\n[Proof]\n‖Ea‖2 = ‖ m∑\ni=1\naiei‖2 ≤ m∑\ni=1\n|ai|‖ei‖2 ≤ ‖E‖1,2 m∑\ni=1\n|ai| = ‖E‖1,2‖a‖1.\nLemma 3. The sparse representation ϕD(x) satisfies ‖ϕD(x)‖1 ≤ ‖x‖22/λ.\n[Proof] λ ‖ϕD(x)‖1 ≤ ‖x−DϕD(x)‖22 + λ ‖ϕD(x)‖1\n= min z∈Rm ‖x−Dz‖22 + λ ‖z‖1 ≤ ‖x‖22.\nLet a∗ and ã∗ respectively denote the solutions to the LASSO problems for the dictionary D and D̃:\na ∗ = argmin\nz∈Rm\n1 2 ‖x−Dz‖22 + λ‖z‖1,\nã ∗ = argmin\nz∈Rm\n1 2 ‖x− D̃z‖22 + λ‖z‖1.\nLet vD and vD̃ be the optimal values of the LASSO problems for the dictionary D and D̃:\nvD = min z∈Rm\n1 2 ‖x−Dz‖22 + λ‖z‖1 = 1 2 ‖x−Da∗‖22 + λ‖a∗‖1,\nv D̃ = min z∈Rm\n1 2 ‖x− D̃z‖22 + λ‖z‖1 = 1 2 ‖x− D̃ã∗‖22 + λ‖ã∗‖1.\nLemma 4 (Optimal Value Stability).\n|vD − vD̃| ≤ 1\n2 (2‖x‖2 + 1) ‖x‖22 ‖D− D̃‖1,2 λ .\n[Proof]\nv D̃ ≤ 1 2 ‖x− D̃a∗‖22 + λ‖a∗‖1\n= 1\n2 ‖x−Da∗ + (D− D̃)a∗‖22 + λ‖a∗‖1\n≤ 1 2 (‖x−Da∗‖22 + 2‖x−Da∗‖2‖(D− D̃)a∗‖2 + ‖(D− D̃)a∗‖22) + λ‖a∗‖1\n≤ 1 2 ‖x−Da∗‖22 + λ‖a∗‖1 + ‖x‖2\n( ‖x‖22‖D− D̃‖1,2\nλ\n) + 1\n2\n( ‖x‖22‖D− D̃‖1,2\nλ\n)2\n≤ vD + ( ‖x‖2 + 1\n2 ) ‖x‖22 λ ‖D− D̃‖1,2,\nwhere we used\n‖x−Da∗‖2 = √ ‖x−Da∗‖2 ≤ √ ‖x−Da∗‖2 + λ‖a∗‖1 ≤ √ ‖x‖22 = ‖x‖2.\nLemma 5 (Stability of Norm of Reconstructor). If ‖D− D̃‖1,2 ≤ λ, then ∣∣∣‖Da∗‖22 − ‖D̃ã∗‖22 ∣∣∣ ≤ (2‖x‖2 + 1) ‖x‖22 ‖D− D̃‖1,2\nλ .\nThe proof of Lemma 5 is the same as that of Lemma 11 in Mehta and Gray (2012).\nLemma 6. If ‖D− D̃‖1,2 ≤ λ, then ∣∣‖Da∗‖22 − ‖Dã∗‖22 ∣∣ ≤ ( 3‖x‖22 + 6‖x‖2 + 1 ) ‖x‖22\n‖D− D̃‖1,2 λ .\n[Proof] First, note that\n‖(D̃−D)ã∗‖2 ≤ ‖(D̃−D)‖1,2‖ã∗‖2 ≤ ‖x‖22 ‖D− D̃‖1,2\nλ\nand\n‖Dã∗‖2 ≤ ‖(D− D̃)ã∗‖2 + ‖D̃ã∗ − x‖2 + ‖x‖2\n≤ ‖x‖22 ‖D− D̃‖1,2\nλ + 2‖x‖2\n≤ (‖x‖2 + 2)‖x‖2, where we used Lemma 3. Then, we have\n∣∣∣‖Dã∗‖22 − ‖D̃ã∗‖22 ∣∣∣\n≤ 2 ∣∣∣〈Dã∗, (D̃−D)ã∗〉 ∣∣∣+ ‖(D̃−D)ã∗‖22 ≤ 2‖Dã∗‖2‖(D̃−D)ã∗‖2 + ‖(D̃−D)ã∗‖22 ≤ 2(‖x‖2 + 2)‖x‖2 ( ‖x‖22‖D− D̃‖1,2\nλ\n) + ( ‖x‖22‖D− D̃‖1,2\nλ\n)2\n≤ (3‖x‖2 + 4)‖x‖2 ( ‖x‖22‖D− D̃‖1,2\nλ\n) .\nCombining this fact with Lemma 5, we have∣∣‖Da∗‖22 − ‖Dã∗‖22 ∣∣\n≤ ∣∣∣‖Da∗‖22 − ‖D̃ã∗‖22 ∣∣∣+ ∣∣∣‖D̃ã∗‖22 − ‖Dã∗‖22 ∣∣∣ ≤ (2‖x‖2 + 1) ( ‖x‖22‖D− D̃‖1,2\nλ\n) + (3‖x‖2 + 4)‖x‖2 ( ‖x‖22‖D− D̃‖1,2\nλ\n)\n= ( 3‖x‖22 + 6‖x‖2 + 1 ) ‖x‖22 ‖D− D̃‖1,2 λ .\nLemma 7 (Reconstructor Stability). If ‖D− D̃‖1,2 ≤ λ, then\n‖Da∗ −Dã∗‖22 ≤ 2 ( 3‖x‖22 + 10‖x‖2 + 3 ) ‖x‖22 ‖D− D̃‖1,2 λ .\n[Proof] We set as ā∗ := 12 (a ∗ + ã∗). From the optimality of a∗, it follows that vD(a∗) ≤ vD(ā∗), that is, 1\n2 ‖x−Da∗‖22 + λ‖a∗‖1 ≤\n1 2 ‖x−Dā∗‖22 + λ‖ā∗‖1. (16)\nWe denote as ǫ := ‖D− D̃‖1,2, cx := (2‖x‖2 + 1) ‖x‖22 and c′x := ( 3‖x‖22 + 6‖x‖2 + 1 ) ‖x‖22.\nBy the convexity of the l1-norm, the RHS of (16) obeys:\n1\n2\n∥∥∥∥x−D ( a ∗ + ã∗\n2\n)∥∥∥∥ 2\n2\n+ λ ∥∥∥∥ a ∗ + ã∗\n2 ∥∥∥∥ 1\n≤ 1 2 ∥∥∥∥x− 1 2 (Da∗ +Dã∗) ∥∥∥∥ 2\n2\n+ λ\n2 ‖a∗‖1 +\nλ 2 ‖ã∗‖1\n= 1\n2\n( ‖x‖22 − 2 〈 x, 1\n2 (Da∗ +Dã∗)\n〉 + 1\n4 ‖Da∗ +Dã∗‖22\n) + λ\n2 ‖α‖1 +\nλ 2 ‖α̃‖1\n= 1\n2 ‖x‖22 −\n1 2 〈x,Da∗〉 − 1 2 〈x,Dã∗〉+ 1 8 (‖Da∗‖22 + ‖Dã∗‖22 + 2〈Da∗,Dã∗〉)\n+ λ\n2 ‖α‖1 +\nλ 2 ‖α̃‖1\n≤ 1 2 ‖x‖22 − 1 2 〈x,Da∗〉 − 1 2 〈x,Dã∗〉+ 1 4 ‖Da∗‖22 + 1 4 〈Da∗,Dã∗〉\n+ λ\n2 ‖α‖1 +\nλ 2 ‖α̃‖1 + c′x 8 ǫ λ\n≤ 1 2 ‖x‖22 − 1 2 〈x,Da∗〉 − 1 2 〈x,Dã∗〉+ 1 4 ‖Da∗‖22 + 1 4 〈Da∗,Dã∗〉\n+ 1 2 〈x−Da∗,Da∗〉+ 1 2 〈x− D̃ã∗, D̃ã∗〉+ c\n′ x\n8\nǫ\nλ\n≤ 1 2 ‖x‖22 − 1 2 〈x,Da∗〉 − 1 2 〈x,Dã∗〉+ 1 4 ‖Da∗‖22 + 1 4 〈Da∗,Dã∗〉\n+ 1 2 〈x,Da∗〉 − 1 2 ‖Da∗‖22 + 1 2 〈x, D̃ã∗〉 − 1 2 ‖Da∗‖22 + ( c′x 8 + cx 2 ) ǫ λ\n= 1\n2 ‖x‖22 −\n3 4 ‖Da∗‖22 + 1 4 〈Da∗,Dã∗〉+ ( c′ x + 4cx 8 ) ǫ λ .\nNow, taking the (expanded) LHS of (16) and the newly derived upper bound of the RHS of (16) yields the inequality:\n1 2 ‖x‖1 − 〈x,Da∗〉+ 1 2 ‖Da∗‖22 + λ‖a∗‖1\n≤ 1 2 ‖x‖22 − 3 4 ‖Da∗‖22 + 1 4 〈Da∗,Dã∗〉+ ( c′ x + 4cx 8 ) ǫ λ .\nReplacing λ‖a∗‖1 with 〈x−Da∗,Da∗〉 yields:\n−〈x,Da∗〉+ 1 2 ‖Da∗‖22 + 〈x−Da∗,Da∗〉\n≤ −3 4 ‖Da∗‖22 + 1 4 〈Da∗,Dã∗〉+ ( c′ x + 4cx 8 ) ǫ λ .\nHence,\n‖Da∗‖22 ≤ 〈Da∗,Dã∗〉+ c′x + 4cx\n2\nǫ λ .\nNow, note that\n‖Da∗ −Dã∗‖22 = ‖Da∗‖22 + ‖Dã∗‖22 − 2〈Da∗,Dã∗〉 ≤ ‖Da∗‖22 + ( ‖Da∗‖22 + c′x ǫ\nλ\n) + ( −2‖Da∗‖22 + (c′x + 4cx) ǫ\nλ\n)\n≤ 2(c′ x + 2cx)\nǫ λ .\nLemma 8. [Preservation of Sparsity] If\nMk(D,x) ≥ ( 1 +\n‖x‖2 λ\n) ‖x‖2‖D− D̃‖1,2 +\n√\n2 (3‖x‖22 + 10‖x‖2 + 3) ‖x‖22 ‖D− D̃‖1,2\nλ , (17)\nthen\n‖ϕD(x)− ϕD̃(x)‖0 ≤ k. (18)\n[Proof] In this proof, we denoteϕD(x) and ϕD̃(x) by a ∗ = [a∗1, . . . , a ∗ m] ⊤ and ã∗ = [ã∗1, . . . , ã ∗ m] ⊤, respectively. When D̃ = D, Lemma 8 obviously holds. In the following, we assume D̃ 6= D. Since Mk(D,x) > 0 from (17), there is a I ⊂ [m] with |I| = m− k such that for all i ∈ I:\n0 < Mk(D,x) ≤ λ− |〈dj ,x−DϕD(x)〉|. (19) To obtain (18), it is enough to show that a∗i = 0 and ã ∗ i = 0 for all i ∈ I. First, we show a∗i = 0 for all i ∈ I. From the optimality conditions for the LASSO (Asif and Romberg (2010), conditions L1 and L2), we have\n〈dj ,x−Da∗〉 = sign(a∗j )λ if a∗j 6= 0, |〈dj ,x−Da∗〉| < λ otherwise.\nNote that the above optimality conditions imply that if a∗j 6= 0 then |〈dj ,x−Da∗〉| = λ. (20) Combining (20) with (19), it holds that a∗i = 0 for all i ∈ I. Next, we show ã∗i = 0 for all i ∈ I. To do so, it is sufficient to show that\n|〈d̃i,x− D̃ã∗〉| < λ (21) for all i ∈ I. Note that\n|〈d̃i,x− D̃ã∗〉| = |〈di + d̃i − di,x− D̃ã∗〉| ≤ |〈di,x− D̃ã∗〉|+ ‖d̃i − di‖2‖x− D̃ã∗‖2 ≤ |〈di,x− D̃ã∗〉|+ ‖D̃−D‖1,2‖x‖2\nand\n|〈di,x− D̃ã∗〉| = |〈di,x− (D+ D̃−D)ã∗〉| ≤ |〈di,x−Dã∗〉|+ |〈di, (D̃−D)ã∗〉| ≤ |〈di,x−Dã∗〉|+ ‖D̃−D‖1,2‖ã∗‖1.\nHence,\n|〈d̃i,x− D̃ã∗〉| ≤ |〈di,x−Dã∗〉|+ ( 1 +\n‖x‖2 λ\n) ‖x‖2‖D− D̃‖1,2.\nNow,\n|〈di,x−Dã∗〉| = |〈di,x−Da∗ +Da∗ −Dã∗〉| ≤ |〈di,x−Da∗〉|+ |〈di,Da∗ −Dã∗〉| ≤ λ−Mk(D,x) + ‖Da∗ −Dã∗‖2\n≤ λ−Mk(D,x) +\n√\n2 (3‖x‖22 + 10‖x‖2 + 3) ‖x‖22 ‖D− D̃‖1,2\nλ , (22)\nwhere (22) is due to Lemma 7. Then, (21) is obtained by (17).\n[Proof of Theorem 2]\nFollowing by the notations of Mehta and Gray (2012), we denote ϕD(x) and ϕD̃(x) by z∗ and t∗, respectively. From (23) of Mehta and Gray (2012), we have\n(z∗ − t∗)⊤D⊤D(z∗ − t∗) ≤ (z∗ − t∗)⊤ ( (D̃⊤D̃−D⊤D)t∗ + 2(D− D̃)⊤x )\n= (z∗ − t∗)⊤(D̃⊤D̃−D⊤D)t∗ + 2(z∗ − t∗)⊤(D− D̃)⊤x. (23)\nWe evaluate the second term in (23) 3). We have the following by the definition of z∗:\n‖x− D̃t∗‖22 + λ‖t∗‖1 ≥ ‖x− D̃z∗‖22 + λ‖z∗‖1, and thus,\n2(z∗ − t∗)⊤D̃⊤x ≥ z⊤∗ D̃⊤D̃z∗ − t⊤∗ D̃⊤D̃t∗ + λ(‖z∗‖1 − ‖t∗‖1). Similarly, we have\n2(t∗ − z∗)⊤D⊤x ≥ t⊤∗ D⊤Dt∗ − z⊤∗ D⊤Dz∗ + λ(‖t∗‖1 − ‖z∗‖1). Summing up the above inequalities and multiplying −1, we obtain\n2(z∗ − t∗)⊤(D− D̃)⊤x ≤ −z⊤∗ D̃⊤D̃z∗ + t⊤∗ D̃⊤D̃t∗ − t⊤∗ D⊤Dt∗ + z⊤∗ D⊤Dz∗ = −z⊤∗ (D̃⊤D̃−D⊤D)z∗ + t⊤∗ (D̃⊤D̃−D⊤D)t∗ = (z∗ − t∗)⊤(D⊤D− D̃⊤D̃)z∗ − (z∗ − t∗)⊤(D̃⊤D̃−D⊤D)t∗ (24)\nWhen E := D− D̃, from (23) and (24), (z∗ − t∗)⊤D⊤D(z∗ − t∗)\n≤ (z∗ − t∗)⊤(D⊤D− D̃⊤D̃)z∗ ≤ |(z∗ − t∗)⊤(E⊤D̃+ D̃⊤E+E⊤E)z∗| ≤ |(z∗ − t∗)⊤E⊤D̃z∗|+ |(z∗ − t∗)⊤D̃⊤Ez∗|+ |(z∗ − t∗)⊤E⊤Ez∗| ≤ ‖E(z∗ − t∗)‖2‖D̃z∗‖2 + ‖D̃(z∗ − t∗)‖2‖Ez∗‖2 + ‖E(z∗ − t∗)‖2‖Ez∗‖2 ≤ (‖E‖1,2‖D̃‖1,2‖z∗‖1 + ‖D̃‖1,2‖E‖1,2‖z∗‖1 + ‖E‖1,2‖E‖1,2‖z∗‖1)‖z∗ − t∗‖1 ≤ ( ‖x‖22‖E‖1,2\nλ + ‖x‖22‖E‖1,2 λ + ‖x‖22‖E‖21,2 λ\n) √ k‖z∗ − t∗‖2\n≤ ( 4‖x‖22 λ ) ‖E‖1,2 √ k‖z∗ − t∗‖2, (25)\n3)The following bound in Mehta and Gray (2012) is not used in this paper:\n2(z∗ − t∗)⊤(D− D̃)⊤x ≤ 2‖D − D̃‖1,2 √ k‖z∗ − t∗‖2‖x‖2.\nwhere we used ‖E‖1,2 ≤ 2 in the last inequality. We note that the assumption (17) of Lemma 8 follows from (5). Then, since ‖z∗ − t∗‖0 ≤ k from Lemma 8, we have the following lower bound of (23) from the µ-incoherence of D:\n(z∗ − t∗)⊤D⊤D(z∗ − t∗) ≥ (1− µk/ √ d)‖z∗ − t∗‖22. (26)\nBy (25) and (26), we obtain\n‖z∗ − t∗‖2 ≤ 4‖x‖22\n√ k\n(1− µk/ √ d)λ ‖D− D̃‖1,2."
    }, {
      "heading" : "C Appendix: Proof of Margin Bound",
      "text" : "In this proof, we set as\nδ1 := 2σ\n(1− t) √ dλ exp\n( − (1− t) 2dλ2\n8σ2\n) ,\nδ2 := 2σm√ dλ exp\n( −dλ 2\n8σ2\n) ,\nδ′3 := 4σk\nC √ d(1 − µk/ √ d) exp\n( −C 2d(1 − µk/ √ d)\n8σ2\n)\nδ′′3 := 8σ(d− k)\ndλ exp\n( −d 2λ2\n32σ2\n) ,\nδ3 := δ ′ 3 + δ ′′ 3 .\nThen, δt,λ = δ1 + δ2 + δ3.\nThe column vectors for a µ-incoherent dictionary are in general position. Thus, a solution of LASSO for a µ-incoherent dictionary is unique due to Lemma 3 in Tibshirani et al. (2013).\nLemma 9. When a dictionary D is µ-incoherent, then the following bound holds for an arbitrary k-sparse vector b:\nb ⊤ D ⊤ Db ≥ ( 1− µk√\nd\n) ‖b‖22.\nRemark 1. We mention the relation with the k-incoherence of a dictionary, which is the assumption of the sparse coding stability in Mehta and Gray (2013). For k ∈ [m] and D ∈ D, the k-incoherence sk(D) is defined as\nsk(D) := (min{ςk(DΛ)|Λ ⊂ [m], |Λ| = k})2, where ςk(DΛ) is the k-th singular value of DΛ = [di1 , . . . ,dik ] for Λ = {i1, . . . , ik}. From Lemma 9, when a dictionary D is µ-incoherent, the k-incoherence of D satisfies\nsk(D) ≥ 1− µk√ d .\nThus, a µ-incoherent dictionary has positive k-incoherence when d > (µk)2. On the other hand, when k ≥ 2, if a dictionary D has positive k-incoherence sk(D), there is µ > 0 such that the dictionary is µ-incoherent.\nThe following notions are introduced in Zhao and Yu (2006). Let a be a k-sparse vector. Without loss of generality, we assume that a = [a1, . . . , ak, 0, . . . , 0]⊤. Then, we denote as D(1) = [d1, . . . ,dk] and D(2) = [dk+1, . . . ,dm]. Then, we define as Cij := 1dD(i) ⊤ D(j) for i, j ∈ {1, 2}. When a dictionary D is µ-incoherent and µk2/d < 1, C11 is positive definite due to Lemma 9 and especially invertible.\nDefinition 6 (Strong Irrepresentation Condition). There exists a positive vector η such that\n|C21C−111 sign(a)| ≤ 1− η, where sign(a) maps positive entry of a to 1, negative entry to −1 and 0 to 0, 1 is the (d − k) × 1 vector of 1’s and the inequality holds element-wise.\nThen, the following lemma is derived by modifying the proof of Lemma 2 of Zhao and Yu (2006).\nLemma 10 (Strong Irrepresentation Condition). When a dictionary D is µ-incoherent and d > µ(2k − 1) holds, the strong irrepresentation condition holds with η = (1− µ(2k − 1)/d)1. Lemma 11. Under Assuptions 1-4, when D is µ-incoherent and d > µ(2k−1), the following holds:\nPr [|supp(a− ϕD(x))| = k] ≥ 1− δ3.\n[Proof] The following inequality obviously holds:\nPr [|supp(a− ϕD(x))| = k] ≥ Pr [sign(a) = sign(ϕD(x))] .\nDue to Lemma 10 and Proofs of Theorems 3 and 4 in Zhao and Yu (2006), there exist sub-Gaussian random variables {zi}ki=1 and {ζi}d−ki=1 such that their variances are bounded as E[z2i ] ≤ σ2/d(1− µk/ √ d) ≤ σ2/d(1− µk/ √ d) and E[ζ2i ] ≤ σ2/d2 and\nPr [sign(a) = sign(ϕD(x))]\n≥ 1− k∑\ni=1\nPr [ |zi| ≥ √ d ( |ai| −\n√ kλ\n2(1− µk/ √ d)d\n)] − d−k∑\ni=1\nPr [ |ζi| ≥\n(1− µ(2k − 1)/d)λ 2 √ d\n] .\nWhen λ ≤ (1 − µk/ √ d)Cd/ √ k, the inequality |ai| −\n√ kλ\n2(1−µk/ √ d)d ≥ C/2 holds since |ai| ≥ C. Then, since 1− µ(2k − 1)/d ≥ 1/2 holds, we obtain\nPr [ |zi| ≥ √ d ( |ai| −\n√ kλ\n2(1− µk/ √ d)d\n)] ≤ Pr [ |zi| ≥ C √ d\n2\n] ≤ δ′3,\nPr [ |ζi| ≥\n(1− µ(2k − 1)/d)λ 2 √ d\n] ≤ Pr [ |ζi| ≥ λ4√d ] ≤ δ′′3 ,\nwhere we used that zi and ζi are sub-Gaussian. Thus the proof is completed.\nLemma 12. Let D be a dictionary. When ξ satisfies Assumption 4, the following holds:\nPr[λ ≥ 2‖D⊤ξ‖∞] ≤ 1− δ2,\n[Proof] Let ξ be a 1-dimensional sub-Gaussian with parameter σ/ √ d. Then, it holds that for t > 0\nPr [|ξ| > λ] ≤ σ√ dλ exp\n( −dλ 2\n2σ2\n) . (27)\nNote that 〈dj , ξ〉 is sub-Gaussian with parameter σ/ √ d because ‖dj‖2 = 1 for every j ∈ [m] and\ncomponents of ξ are independent and sub-Gaussian with parameter σ/ √ d. Thus,\nPr[λ < 2‖D⊤ξ‖∞] = Pr [ ∪mj=1{λ < 2|〈dj , ξ〉|} ] ≤ m∑\nj=1\nPr[λ < 2|〈dj, ξ〉|] ≤ δ2,\nwhere we used (27) in the last inequality.\nLemma 13. Under Assuptions 1-4, then\nPr [ ‖a− ϕD(x)‖2 ≤\n3 √ k\n(1− µk/ √ d) λ\n] ≥ 1− δ2 − δ3.\n[Proof] By Assumption 1, x = Da + ξ. We denote ϕD(x) by a∗ and a − a∗ by ∆. We have the following inequality by the definition of a∗:\n1 2 ‖x−Da∗‖22 + λ‖a∗‖1 ≤ 1 2 ‖x−Da‖22 + λ‖a‖1.\nSubstituting x = Da+ ξ, we have\n1 2 ‖D∆‖22 ≤ −〈D⊤ξ,∆〉+ λ(‖a‖1 − ‖a∗‖1)\n≤ ‖D⊤ξ‖∞‖∆‖1 + λ(‖a‖1 − ‖a∗‖1). (28)\nLet ∆k be the vector whose i-th component equals that of ∆ if i is in the support of a and equals 0 otherwise. In addition, let ∆⊥k = ∆−∆k. Using ∆ = ∆k +∆⊥k , we have\n‖a∗‖ = ‖a+∆⊥k +∆k‖1 ≥ ‖a‖1 + ‖∆⊥k ‖1 − ‖∆k‖1\nSubstituting the above inequality into (28), we have\n1 2 ‖D∆‖22 ≤ ‖D⊤ξ‖∞‖∆‖1 + λ(‖∆k‖1 − ‖∆⊥k ‖1)\nThe inequality λ ≥ 2‖D⊤ξ‖∞ holds with with probability 1 − δ2 due to Lemma 12, and then, the following inequality holds:\n0 ≤ 1 2 ‖D∆‖22 ≤ 1 2 λ(‖∆k‖1 + ‖∆⊥k ‖1) + λ(‖∆k‖1 − ‖∆⊥k ‖1).\nThus, ‖∆⊥k ‖1 ≤ 3‖∆k‖1 and 1\n2 ‖D∆‖22 ≤\n3 2 λ‖∆k‖1 − 1 2 λ‖∆⊥k ‖1 ≤ 3 2 λ‖∆k‖1 ≤ 3 2 λ √ k‖∆k‖2.\nThus, we have\n‖D∆‖22 ≤ 3λ √ k‖∆k‖2 ≤ 3λ √ k‖∆‖2.\nHere, ‖supp(∆)‖0 = k with probability 1− δ3 due to Lemma 11 and the following inequality holds by the µ-incoherence of the dictionary D:\n(1 − µk/ √ d)‖∆‖22 ≤ ‖D∆‖22,\nand thus,\n‖∆‖2 ≤ 3λ\n√ k\n(1− µk/ √ d) .\n[Proof of Theorem 3] From Assumption 1, an arbitrary sample x is represented as x = D∗a + ξ. Then,\n〈dj ,x−D∗ϕD(x)〉 = 〈dj , ξ +D∗(a− ϕD(x))〉 = 〈dj , ξ〉+ 〈D∗⊤dj , a− ϕD(x)〉.\nThen, we evaluate the probability that the first and second terms is bounded above by 1−t2 λ. We evaluate the probability for the first term. Since ‖dj‖ = 1 by the definition and ξ is drawn from a sub-Gaussian distribution with parameter σ2/ √ d, we have\nPr [ 〈dj , ξ〉 ≤\n1− t 2 λ\n] ≤ 1− δ1.\nWith probability 1− δ2 − δ3, the second term is evaluated as follows:\n〈D∗⊤dj , a− ϕD(x)〉 = 〈[〈d1,dj〉, . . . , 〈dm,dj〉]⊤, a− ϕD(x)〉 = 〈(1supp(a−ϕD(x)) ◦ [〈d1,dj〉, . . . , 〈dm,dj〉])⊤, a− ϕD(x)〉 ≤ ‖(1supp(a−ϕD(x)) ◦ [〈d1,dj〉, . . . , 〈dm,dj〉])⊤‖2‖a− ϕD(x)‖2 ≤ µ√\nd\n√ |supp(a− ϕD(x))|‖a− ϕD(x)‖2\n≤ 3µk (1 − µk/ √ d) √ d λ (29) ≤ 1− t 2 λ, (30)\nwhere we used Lemmas 11 and 13 in (29) and d ≥ (\n6µk\n(1−µk/ √ d)(1−t)\n)2 in (30). Thus, with probabil-\nity 1− (δ1 + δ2 + δ3) = 1− δt,λ, Mk,D∗(x) ≥ λ− 〈dj ,x−D∗ϕD(x)〉 ≥ tλ.\nWe note that d ≥ {(\n1 + 6(1−t)\n) µk }2 is enough to satisfy the condition d ≥ (\n6µk\n(1−µk/ √ d)(1−t)\n)2 .\nThus, the proof of Theorem 3 is completed."
    } ],
    "references" : [ {
      "title" : "Simple, efficient, and neural algorithms for sparse coding,",
      "author" : [ "S. Arora", "R. Ge", "T. Ma", "A. Moitra" ],
      "venue" : "arXiv preprint arXiv:1503.00778",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2015
    }, {
      "title" : "On the lasso and dantzig selector equivalence,",
      "author" : [ "M.S. Asif", "J. Romberg" ],
      "venue" : "Information Sciences and Systems (CISS),",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2010
    }, {
      "title" : "A model of inductive bias learning,",
      "author" : [ "J. Baxter" ],
      "venue" : "J. Artif. Intell. Res.(JAIR),",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2000
    }, {
      "title" : "An analysis of single-layer networks in unsupervised feature learning,",
      "author" : [ "A. Coates", "A.Y. Ng", "H. Lee" ],
      "venue" : "in International conference on artificial intelligence and statistics,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2011
    }, {
      "title" : "Self-taught clustering,",
      "author" : [ "W. Dai", "Q. Yang", "G.-R. Xue", "Y. Yu" ],
      "venue" : "Proceedings of the 25th international conference on Machine learning,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2008
    }, {
      "title" : "Building high-level features using large scale unsupervised learning,",
      "author" : [ "V. Q" ],
      "venue" : "in Acoustics, Speech and Signal Processing (ICASSP),",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2013
    }, {
      "title" : "Exponential Family Sparse Coding with Application to Self-taught Learning,",
      "author" : [ "H. Lee", "R. Raina", "A. Teichman", "A.Y. Ng" ],
      "venue" : "in IJCAI,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2009
    }, {
      "title" : "Supervised dictionary learning,” in Advances in neural information processing",
      "author" : [ "J. Mairal", "J. Ponce", "G. Sapiro", "A. Zisserman", "F.R. Bach" ],
      "venue" : null,
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2009
    }, {
      "title" : "Transfer bounds for linear feature learning,",
      "author" : [ "A. Maurer" ],
      "venue" : "Machine learning,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2009
    }, {
      "title" : "Sparse coding for multitask and transfer learning,",
      "author" : [ "A. Maurer", "M. Pontil", "B. Romera-Paredes" ],
      "venue" : "arXiv preprint arXiv:1209.0738",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2012
    }, {
      "title" : "A unified framework for highdimensional analysis of M -estimators with decomposable regularizers,",
      "author" : [ "S. Negahban", "B. Yu", "M.J. Wainwright", "P.K. Ravikumar" ],
      "venue" : "Advances in Neural Information Processing Systems,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2009
    }, {
      "title" : "A survey on transfer learning,",
      "author" : [ "S.J. Pan", "Q. Yang" ],
      "venue" : "Knowledge and Data Engineering, IEEE Transactions on,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2010
    }, {
      "title" : "Self-taught learning: transfer learning from unlabeled data,",
      "author" : [ "R. Raina", "A. Battle", "H. Lee", "B. Packer", "A.Y. Ng" ],
      "venue" : "Proceedings of the 24th international conference on Machine learning,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2007
    }, {
      "title" : "Fast rates for regularized objectives,",
      "author" : [ "K. Sridharan", "S. Shalev-Shwartz", "N. Srebro" ],
      "venue" : "Advances in Neural Information Processing Systems,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2009
    }, {
      "title" : "The lasso problem and uniqueness,",
      "author" : [ "R.J. Tibshirani" ],
      "venue" : "Electronic Journal of Statistics,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2013
    }, {
      "title" : "The sample complexity of dictionary learning,",
      "author" : [ "D. Vainsencher", "S. Mannor", "A.M. Bruckstein" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2011
    }, {
      "title" : "Robust and discriminative self-taught learning,",
      "author" : [ "H. Wang", "F. Nie", "H. Huang" ],
      "venue" : "Proceedings of The 30th International Conference on Machine Learning,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2013
    }, {
      "title" : "On model selection consistency of Lasso,",
      "author" : [ "P. Zhao", "B. Yu" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2006
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "References [1] S.",
      "startOffset" : 11,
      "endOffset" : 14
    }, {
      "referenceID" : 1,
      "context" : "[2] M.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 2,
      "context" : "[3] J.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 3,
      "context" : "[4] A.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 4,
      "context" : "[5] W.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 5,
      "context" : "[6] Q.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 6,
      "context" : "[7] H.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 7,
      "context" : "[8] J.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 8,
      "context" : "[9] A.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 9,
      "context" : "[10] A.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 10,
      "context" : "[13] S.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 11,
      "context" : "[14] S.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 12,
      "context" : "[15] R.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 13,
      "context" : "[16] K.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 14,
      "context" : "[17] R.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 15,
      "context" : "[18] D.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 16,
      "context" : "[19] H.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 17,
      "context" : "[20] P.",
      "startOffset" : 0,
      "endOffset" : 4
    } ],
    "year" : 2017,
    "abstractText" : "We consider a transfer-learning problem by using the parameter transfer approach, where a suitable parameter of feature mapping is learned through one task and applied to another objective task. Then, we introduce the notion of the local stability of parametric feature mapping and parameter transfer learnability, and thereby derive a learning bound for parameter transfer algorithms. As an application of parameter transfer learning, we discuss the performance of sparse coding in selftaught learning. Although self-taught learning algorithms with plentiful unlabeled data often show excellent empirical performance, their theoretical analysis has not been studied. In this paper, we also provide the first theoretical learning bound for self-taught learning.",
    "creator" : "LaTeX with hyperref package"
  }
}