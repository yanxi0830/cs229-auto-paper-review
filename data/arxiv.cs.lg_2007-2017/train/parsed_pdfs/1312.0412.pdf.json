{
  "name" : "1312.0412.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Practical Collapsed Stochastic Variational Inference for the HDP",
    "authors" : [ "Arnim Bleier" ],
    "emails" : [ "arnim.bleier@gesis.org" ],
    "sections" : [ {
      "heading" : "1 Background",
      "text" : "We begin by considering a model where each document d is a mixture θd of K discrete topicdistributions φk over a vocabulary of V terms. Let zdi ∈ {1, ..,K} denote the topic of the ith word wdi ∈ {1, .., V } in document d ∈ {1, .., D} and place Dirichlet priors on the parameters θd, φk. We have\nzdi | θd ∼ Discrete(θd) , θd ∼ Dirichlet(απ) , wdi | zdi, {φk} ∼ Discrete(φzdi) , φk ∼ Dirichlet(β) ,\nwhere π is the top-level distribution over topics, and α and β are concentration parameters. While the dimensionality of K is fixed in latent Dirichlet allocation (LDA), we want the model to determine the number of topics needed. Consequently we follow the assumptions made by the hierarchical Dirichlet process (HDP) [1] of a countable but infinite number of topics, of which only a finite number is used in the posterior. Our prior π is constructed by a truncated sick-breaking process [2],\nπk = π̄k k−1∏ l=1 (1− π̄l) , π̄k | γ ∼ Beta(1, γ) , π̄T = 1 , (1)\nwhere π̄ are the stick proportions. T is the truncation level and not the number of topics; if set to an appropriate level (i.e. T > K) the truncated stick-breaking process is a sufficient approximation of the Dirichlet process.\nIn the reminder of this paper we start by reviewing variational batch inference for the collapsed representation of the HDP. We then introduce our proposed stochastic updates. After that an early evaluation of our algorithm is presented. We conclude with a discussion of our work and its current limitations.\nar X\niv :1\n31 2.\n04 12\nv1 [\ncs .L\nG ]\n2 D\nec 2"
    }, {
      "heading" : "2 Practical Collapsed Variational Inference",
      "text" : "In this section we review practical batch collapsed variational Bayes inference (PCVB0) proposed by Sato et al. [3] which later will be the fundament of our stochastic inference. The collapsed representation of the HDP is achieved by marginalizing over θ and φ. If only zero-order information is used, the update for the variational distributions zdi over T possible topic assignments for each word is given by\nq(zdi = k) ∝ (n¬didk + απk) n¬dikwdi + β\nn¬dik. + V β . (2)\nWhere n¬didk is the number of times a word in document d has been assigned to topic k, and n ¬di kwdi is the number of times the term wdi has been assigned to topic k, in both cases excluding the current word di. Furthermore . is used in place of a variable to indicate that the sum over its values (i.e. nk. = ∑ w nkw) is taken. Teh’s [2] original variational Bayes inference required maintaining variance counts for απ. In a response Sato et al. [3] showed the usefulness of a lower-bound approximation for the number of tables in the Dirichlet process Chinese Restaurant representation. Leading to the update of the corpus-wide topic popularity\nπk = π̄k k−1∏ l=1 (1− π̄l) , π̄k = uk uk + vk , (3)\nuk = 1 + ∑ d E[1(ndk ≥ 1)] , vk = γ0 + T∑ l=k+1,d E[1(ndl ≥ 1)] , (4)\nwhere 1(.) is the indicator function and the expectation E[1(ndk ≥ 1)] = 1− ∏ i(1− q(zdi = k)). Moreover, using point estimates the updates for the hyper-parameters α and γ are\nα = ∑ d,k E[1(ndk ≥ 1)]∑\nd[Ψ(nd + α old)−Ψ(αold)]\n, (5)\nγ = T − 1∑T−1\nk=1 [Ψ(uk + vk)−Ψ(vk)] , (6)\nwith nd being the number of words in document d and Ψ(.) the digamma function."
    }, {
      "heading" : "3 Proposed Updates",
      "text" : "One of the main drawbacks of batch collapsed variational inference for the HDP are its high memory requirements. We propose to circumvent this. Following the ideas behind stochastic collapsed variational Bayesian inference (SCVB0) proposed by Foulds et al. [4] we potentially allow for data to arrive in a stream, but maintain the simplicity of the original PCVB0 schema.\nThe practical collapsed stochastic variational Bayes inference for the hierarchical Dirichlet process (PCSVB0), we propose, processes one word at a time, serially processing each word from all documents in turn. Suppose we have a guess of the current PCVB0 statistics. Next, we draw the ith word wdi of document d and compute its corresponding zdi via Equation (2). The expected number of times k appears in the document ndk, with respect to the current word, is ndq(zdi = k). Furthermore, the expected number of times nkw topic k is used by term wdi is nq(zdi = k) and zero for all other terms, with n being the total number of words in the corpus. As we process word by word we compute new zdi’s, but do not store them. Consequently, we cannot subtract the current word in Equation (2) and approximate the distribution by\nq(zdi = k) ∝ (ndk + απk) nkwdi + β\nnk. + V β . (7)\nWith the variational distribution zdi of the current word we are able to update the expected statistics for ndk and nkw via\nndk ← (1− ρdt )ndk + ρdtndq(zdi = k) , (8) nkw ← (1− ρct)nkw + ρctnq(zdi = k)1[wdi = w] , (9)\nand ρt is the step-size in update t. The parameters u and v for the the stick-breaking proportions required in the computation of Equation (3) and Equation (6) are updated after a document is processed. We re-order the sticks according to their sizes. The updates are\nuk ← (1− ρht )uk + ρht (1 +DE[1(ndk ≥ 1)]) , (10)\nvk ← (1− ρht )vk + ρht (γ +D T∑\nl=k+1\nE[1(ndl ≥ 1)]) . (11)\nFor the stochastic update of αwe again assume that our entire corpus consists of the single document d repeated D times, leading to\nα← (1− ρht )α+ ρht\n(∑T k=1 E[1(ndk ≥ 1)]\nΨ(nd + α)−Ψ(α)\n) . (12)\nThis suggests an iterative procedure, altering between approximating zdi and updating the expected count statistics word by word, and updating the global topic popularity along the hyper-parameters document by document.\nAlgorithm 1 PCSVB0 HDP inference 1: Initialize nkw, ndk, π, α, γ. 2: Set step-size schedule for ρdt , ρ c t and ρ h t .\n3: repeat 4: for each document d do 5: for each word i in d do 6: Compute q(zdi = k) (Equation 7). 7: Update ndk (Equation 8). 8: Update nkw (Equation 9). 9: end for\n10: Update πk (Equation 3 with 10,11). 11: Update γ and α (Equation 6,12). 12: end for 13: until stopping criterion is met."
    }, {
      "heading" : "4 Evaluation",
      "text" : "In this section we describe a first experimental analysis of the proposed PCSVB0 inference. We studied the predictive performance of the algorithm on the Associated Press (TREC-1) data. The dataset contains 398k tokens across 2250 documents, with a vocabulary size of 10932 unique terms. For the evaluation we compared the perplexity versus the number of documents seen for PCSVB0, SCVB0 and PCVB0. We trained the model on 80% of the documents. All held out documents were split; 70% of the tokens in each held out document were used to estimate the document parameters, the remaining 30% were used to compute the perplexity.\nWe used the step-size schedule ρt = s(τ+t)0.9 . For the update of nkw in iteration t the schedule ρ c t was parameterized with s = 10 and τ = 1000; the schedule for ndk ρdt was parameterized with s = 1 and τ = 10; and the schedule for the global stick-breaking weights and hyper-parameter\nupdates ρht was parameterized with s = 5 and τ = 100. The prior on the topic-simplex β was set for all algorithms to β = 0.01. The prior on the document-simplex for SCVB0 was set to α = 0.1. Furthermore, for PCSVB0 as well as PCVB0 a truncation-level of T = 200 was used."
    }, {
      "heading" : "5 Discussion",
      "text" : "We presented a collapsed stochastic variational inference algorithm for the HDP-LDA topic model. Our algorithm is based on the application of a practical lower bound approximation of the truncated stick-breaking process to a collapsed stochastic inference scheme and simpler to implement then other uncollapsed online variational inference algorithms for the HDP[5]. Initial small-scale experiments show promising improvements of PCSVB0 in predictive performance over existing algorithms, both in terms of the rate of convergence and the found optimum. Directions for future work are the application of so called ‘clumping’ in order to perform the update only for each distinct term per document, and then to scale the update by the number of its copies. Another direction is the usage of mini-batches. Such optimizations would improve wall-clock time per iteration and allow for a fair comparison with other online variational inference algorithms for the HDP."
    } ],
    "references" : [ {
      "title" : "Hierarchical Dirichlet Processes",
      "author" : [ "Y.W. Teh", "M. Jordan", "M. Beal", "D. Blei" ],
      "venue" : "Journal of the American Statistical Association",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2006
    }, {
      "title" : "Collapsed variational inference for HDP",
      "author" : [ "Y.W. Teh", "K. Kurihara", "M. Welling" ],
      "venue" : "Advances in Neural Information Processing Systems. pp",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2007
    }, {
      "title" : "Practical Collapsed Variational Bayes Inference for Hierarchical Dirichlet Process",
      "author" : [ "I. Sato", "K. Kurihara", "H. Nakagawa" ],
      "venue" : "Proceedings of the 18 ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. pp",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2012
    }, {
      "title" : "Stochastic Collapsed Variational Bayesian Inference for Latent Dirichlet Allocation",
      "author" : [ "J. Foulds", "L. Boyles", "C. Dubois", "P. Smyth", "M. Welling" ],
      "venue" : "Proceedings of the 19 ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. pp",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2013
    }, {
      "title" : "Online Variational Inference for the Hierarchical Dirichlet Processes",
      "author" : [ "C. Wang", "J. Paisley", "D. Blei" ],
      "venue" : "Proceedings of the 14 International Conference on Artificial Intelligence and Statistic",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2011
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "Consequently we follow the assumptions made by the hierarchical Dirichlet process (HDP) [1] of a countable but infinite number of topics, of which only a finite number is used in the posterior.",
      "startOffset" : 88,
      "endOffset" : 91
    }, {
      "referenceID" : 1,
      "context" : "Our prior π is constructed by a truncated sick-breaking process [2],",
      "startOffset" : 64,
      "endOffset" : 67
    }, {
      "referenceID" : 2,
      "context" : "[3] which later will be the fundament of our stochastic inference.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 1,
      "context" : "Teh’s [2] original variational Bayes inference required maintaining variance counts for απ.",
      "startOffset" : 6,
      "endOffset" : 9
    }, {
      "referenceID" : 2,
      "context" : "[3] showed the usefulness of a lower-bound approximation for the number of tables in the Dirichlet process Chinese Restaurant representation.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 3,
      "context" : "[4] we potentially allow for data to arrive in a stream, but maintain the simplicity of the original PCVB0 schema.",
      "startOffset" : 0,
      "endOffset" : 3
    } ],
    "year" : 2013,
    "abstractText" : "Recent advances have made it feasible to apply the stochastic variational paradigm to a collapsed representation of latent Dirichlet allocation (LDA). While the stochastic variational paradigm has successfully been applied to an uncollapsed representation of the hierarchical Dirichlet process (HDP), no attempts to apply this type of inference in a collapsed setting of non-parametric topic modeling have been put forward so far. In this paper we explore such a collapsed stochastic variational Bayes inference for the HDP. The proposed online algorithm is easy to implement and accounts for the inference of hyper-parameters. First experiments show a promising improvement in predictive performance. 1 Background We begin by considering a model where each document d is a mixture θd of K discrete topicdistributions φk over a vocabulary of V terms. Let zdi ∈ {1, ..,K} denote the topic of the i word wdi ∈ {1, .., V } in document d ∈ {1, .., D} and place Dirichlet priors on the parameters θd, φk. We have zdi | θd ∼ Discrete(θd) , θd ∼ Dirichlet(απ) , wdi | zdi, {φk} ∼ Discrete(φzdi) , φk ∼ Dirichlet(β) , where π is the top-level distribution over topics, and α and β are concentration parameters. While the dimensionality of K is fixed in latent Dirichlet allocation (LDA), we want the model to determine the number of topics needed. Consequently we follow the assumptions made by the hierarchical Dirichlet process (HDP) [1] of a countable but infinite number of topics, of which only a finite number is used in the posterior. Our prior π is constructed by a truncated sick-breaking process [2],",
    "creator" : "LaTeX with hyperref package"
  }
}