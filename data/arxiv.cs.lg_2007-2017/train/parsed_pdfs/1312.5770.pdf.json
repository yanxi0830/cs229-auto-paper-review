{
  "name" : "1312.5770.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Consistency of Causal Inference under the Additive Noise Model",
    "authors" : [ "Samory Kpotufe", "Eleni Sgouritsa", "Dominik Janzig", "Bernhard Schölkopf" ],
    "emails" : [ "SAMORY@TTIC.EDU", "ELENI.SGOURITSA@TUEBINGEN.MPG.DE", "DOMINIK.JANZIG@TUEBINGEN.MPG.DE", "BS@TUEBINGEN.MPG.DE" ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "Drawing causal conclusions for a set of observed variables given a sample from their joint distribution is a fundamental problem in science. Conditional-independence-based methods (Pearl, 2000; Spirtes et al., 2000) estimate a set of directed acyclic graphs, all entailing the same conditional independences, from the data. However, these methods can not distinguish between two graphs that entail the same set of conditional independences, the so-called Markov equivalent graphs. Consider for example the case of only two observed dependent random variables. Conditionalindependence-based methods can not recover the causal graph since X → Y and Y → X are Markov equivalent. An elegant basis for causal graphs is the framework of structural causal models (SCMs) (Pearl, 2000), where every observable is a function of its parents and an unobserved independent noise term. This allows us to formulate\nLong ArXiv Version.\nan assumption on function classes which lets us infer the causal direction in two-variable case.\nA special case of SCMs is the Causal Additive Noise Model (CAM) (Shimizu et al., 2006; Hoyer et al., 2009; Tillman et al., 2009; Peters et al., 2011a;b) which is given as follows: given two random variables X and Y , X is assumed to cause Y if (i) Y can be obtained as a function of X plus a noise term independent of X , but (ii) X cannot be obtained as a function of Y plus independent noise, then we infer that X causes Y . In this case, where (i) and (ii) hold simultaneously, the CAM is termed identifiable.\nInitial work on the CAM focused on establishing its theoretical soundness, i.e. understanding the class of distributions PX,Y for which the CAM is identifiable, i.e. for which (i) and (ii) hold simultaneously. Early work by (Shimizu et al., 2006) showed that the CAM is identifiable when the functional relationship Y = f(X) + η is linear, provided the independent noise η is not Gaussian. Later, Hoyer et al. (2009), Zhang & Hyvärinen (2009) and Peters et al. (2011a) showed that the CAM is identifiable more generally even if f is nonlinear, the main technical requirements being that the marginals PX , and Pη are absolutely continuous on R, with Pη having support R. Note that Zhang & Hyvärinen (2009) also introduces a generalization of the CAM termed post-nonlinear models. Further work by Peters et al. (2011b) showed how to reduce causal inference for a network of multiple variables under the CAM to the case of two variables X and Y discussed so far, by properly extending the conditions (i) and (ii) to conditional distributions instead of marginals. Thus, the soundness of the CAM being established by these various works, the next natural question is to understand the statistical behavior of the resulting estimation procedures on ar X iv :1 31 2.\n57 70\nv3 [\ncs .L\nG ]\n5 F\nfinite samples.\nCurrent insights into this last question are mostly empirical. Various works (Shimizu et al., 2006; Hoyer et al., 2009; Peters et al., 2011a) have successfully validated procedures based on the CAM (outlined in Section 1.1 below) on a mix of artificial and real-world datasets where the causal structure to be inferred is clear. However, on the theoretical side, it remains unclear whether these procedures can infer causality from samples in general situations where the CAM is identifiable. In the particular case where the functional relation between X and Y is linear, Hyvärinen et al. (2008) proposed a successful method shown to be consistent. In a recent Arxived result appearing after our initial submission, Buhlmann et al. (2013) showed the consistency of a maximum log-likelihood approach to causal inference under the multi-variable network extension of Peters et al. (2011b).\nWhile consistency has been shown for particular procedures, in this paper we are rather interested in general conditions under which common approaches, with various algorithmic instantiations, are consistent. We derive both algorithmic and distributional conditions for statistical consistency in general situations where the CAM is identifiable. The present work focuses on the case of two real variables, allowing us to focus on the inherent difficulties of achieving consistency with the common algorithmic approaches. These difficulties, described in Section 1.2 have to do with estimating the degree of independence between noise and input, while the noise is itself estimated from the input and hence is inherently dependent on the input."
    }, {
      "heading" : "1.1. Inference Methods Under the Additive Noise Model",
      "text" : "Causal inference methods under the Additive Noise Model typically follow the meta-procedure below. Assume f and g are the best functional fits under some risk, respectively Y ≈ f(X) and X ≈ g(Y ):\nFit Y as a function f(X), obtain the residuals ηY,f = Y − f(X), fit X as a function g(Y ), obtain the residuals ηX,g = X − g(Y ), decide X → Y if ηY,f ⊥ X but ηX,g 6⊥ Y , decide Y → X if the reverse holds true, abstain otherwise.\nInstantiations thus vary in the regression procedures employed for function fitting, and in the independence measures employed. Our analysis concerns procedures employing an entropy-based independence measure, which is cheaper than usual independence tests. These procedures vary in the regression and entropy estimators employed. They are presented in detail in Section 3."
    }, {
      "heading" : "1.2. Towards Consistency: Main Difficulties",
      "text" : "Assume (i) and (ii) hold so that X causes Y under the CAM. We want to detect this from sufficiently large finite samples. This is consistency in a rough sense.\nEstablishing consistency of the above meta-procedure faces many subtle difficulties. The above outlined algorithmic approach consists of four interdependent statistical estimation tasks, namely two regression problems and two independence-tests. Considered separately, the consistency of such estimation tasks is well understood, but in the present context the success of the independence tests is contingent on successful regression.\nThe main difficulty is that although we are observing X and Y , we are not observing the residuals ηY,f and ηX,g , but empirical approximations ηY,fn and ηX,gn obtained by estimating f and g as fn and gn on a sample of size n.\nFor now, consider just detecting that ηY,f , f unknown, is independent from X . A good estimator fn will ensure that fn and f are close, usually in an L2 sense (i.e. EX |fn(X)− f(X)|2 ≈ 0). Hence ηY,fn is close to ηY,f , but unfortunately this does not imply that ηY,fn ⊥ X if ηY,f ⊥ X . In fact it is easy to construct r.v.’s A,B,C such thatA ⊥ B, |B − C| < , for arbitrary , but C 6⊥ A. Thus, the estimate ηY,fn might be close to ηY,f , yet it might still appear dependent on X even if ηY,f is not. Complicating matters further, ηY,fn and ηY,f would only be close in an average sense (instead of close for every value of X) since fn and f are typically only close in an average sense (e.g. close in L2).\nNow consider the full causal discovery, i.e. consider also detecting that ηX,g depends on Y . To achieve consistency, the independence test employed must detect more dependence between ηX,gn and Y than between ηY,fn and X . This will depend on how the particular independence test is influenced by errors in the particular regression procedures employed, and the relative rates at which these various procedures converge.\nAs previously mentioned, we will consider a family of independence-tests based on comparing sums of entropies. We will handle the above difficulties and derive conditions for consistency by first understanding how the various estimated entropies converge as a function of regression convergence (L2 convergence).\nWe do not consider the question of finite-sample convergence rates for causal estimation under the CAM. In fact, it is not even clear whether it is generally possible to establish such rates. This is because it is generally possible that the Bayes best fits f(x) = E [Y |x] is smooth while g(y) = E [X|y] is not even continuous; yet it is well known that without smoothness or similar structural conditions, ar-\nbitrarily bad rates of convergence are possible in regression (see e.g. (Gyorfi et al., 2002), Theorem 3.1).\nHowever, along the way of deriving consistency, we analyze the convergence of various quantities, which appear to affect the finite-sample behavior of the meta-procedure. In particular the tails of the additive noise and the richness of the regression algorithms seem to have a strong effect on convergence. This is verified in controlled simulations. The theoretical details are discussed in Section 4."
    }, {
      "heading" : "2. Preliminaries",
      "text" : ""
    }, {
      "heading" : "2.1. Setup and Notation",
      "text" : "We letH and I denote respectively differential entropy, and mutual information (Cover et al., 1994). Given a density p we will at times use the (abuse of) notation H(p) when a r.v. is unspecified.\nThe distribution of a r.v. Z is denoted PZ , and its density when it exists is denoted pZ .\nThroughout the analysis we will be concerned with residuals from regression fits. We use the following notation. Definition 1. For a function f : R 7→ R, we consider either of the residuals: ηY,f , Y −f(X) and ηX,f , X−f(Y ).\nThe Causal Additive Noise Model is captured as follows: Definition 2 (CAM). Given r.v.’sX,Y , a function f : R 7→ R and a r.v. η, we write X f,η−−→ Y if the following holds:\n(i) PX,Y is generated as X ∼ PX , and Y = f(X) + η, where the noise r.v. η has 0 mean and η ⊥ X;\n(ii) for any g : R 7→ R, ηX,g , X − g(Y ) depends on X .\nWe write X → Y when f and η are clear from context."
    }, {
      "heading" : "3. Causal Inference Procedures",
      "text" : ""
    }, {
      "heading" : "3.1. Main Intuition",
      "text" : "Lemma 1. Consider any absolutely continuous jointdistribution PX,Y on X,Y ∈ R. For any two functions f, g : R 7→ R we have\nH(X) +H(ηY,f ) = H(Y ) +H(ηX,g)\n− {I(ηX,g, Y )− I(ηY,f , X)} .\nProof. By the chain rule of differential entropy we have\nH(X,Y ) = H(X) +H(Y |X) = H(X) +H(ηY,f |X) = H(X) +H(ηY,f )− I(ηY,f , X), similarly\nH(X,Y ) = H(Y ) +H(ηX,g)− I(ηX,g, Y ).\nEquate the two r.h.s above and rearrange.\nNote that whenever ηY,f ,⊥ X , we have I(ηY,f , X) = 0. Therefore, by the above lemma, if ηY,f ,⊥ X then CXY , H(X)+H(ηY,f ) is smaller thanCY X , H(Y )+H(ηX,g). This yields a measure of independence which is relatively cheap to estimate. In particular the test depends only on the marginal distributions of the r.v.’s X,Y and functional residuals, and does not involve estimating joint distributions or conditionals, as is implicit in most independence tests. We analyze a family of procedures based on this idea. This family is given in the next subsection."
    }, {
      "heading" : "3.2. Meta-Algorithm",
      "text" : "Let {(Xi, Yi)}n1 = {(x1, y1), . . . , (xn, yn)} be a finite sample drawn from PX,Y . Let Hn(X) and Hn(Y ) be respective estimators of H(X) and H(Y ) based on the sample {(Xi, Yi)}n1 .\nWe consider the following family of inference procedures:\nGiven an i.i.d sample {(Xi, Yi)}n1 from PX,Y , let fn be returned by an algorithm which fits Y as fn(X) and gn be returned by an algorithm which fitsX as gn(Y ). LetHn denote an entropy estimator. Given a threshold parameter τn\nn→∞−−−−→ 0: Decide X → Y if Hn(X) +Hn(ηY,fn) + τn ≤ Hn(Y ) +Hn(ηX,gn). Decide Y → X if Hn(Y ) +Hn(ηX,gn) + τn ≤ Hn(X) +Hn(ηY,fn). Abstain otherwise.\nThe analysis in this paper is carried with respect to the L2,PX and L2,PY functional norms defined as follows. Definition 3. For f : R 7→ R, and a measure µ on R, the L2,µ norm is given as ‖f‖2,µ = (∫ t f(t)2 dµ(t) )1/2 .\nWe assume the internal procedures fn, gn, Hn have the following consistency properties. Assumption 1. The internal procedures are consistent:\n• Suppose EY 2 < ∞. Let f(x) , E [Y |x]. Then ‖fn − f‖2,PX P−→ 0.\n• Suppose EX2 < ∞. Let g(y) , E [X|y]. Then ‖gn − g‖2,PY P−→ 0.\n• Suppose Z has bounded variance, and has continuous density pZ such that ∃T,C > 0, α > 1, ∀ |t| > T, pZ(t) ≤ C |t|−α. Then |Hn(Z)−H(Z)| P−→ 0.\nMany common nonparametric regression procedures (e.g. kernel, k-NN, Kernel-SVM, spline regressors) are consistent in the above sense (Gyorfi et al., 2002). Also the consistency of a variety of entropy estimators (e.g. plug-in entropy estimators) is well established (Beirlant et al., 1997)."
    }, {
      "heading" : "4. Technical overview of results",
      "text" : "We consider the following two versions of the above metaprocedure. The analysis (Section 5) is divided accordingly.\nDefinition 4 (Decoupled-estimation). fn and gn are learned on half of the sample {(Xi, Yi)}n1 , and the Hn (ηY,fn) andHn (ηX,gn) are learned on the other half of the sample (w.l.o.g. assume n is even). Hn(X) andHn(Y ) could be learned on either half or on the entire sample.\nDefinition 5 (Coupled-estimation). All fn, gn and entropies Hn are learned on the entire sample {(Xi, Yi)}n1 .\nOur most general consistency result (Theorem 1, Section 5.1) concerns decoupled-estimation. By decoupling regression and entropy estimations, we reduce the potential of overfitting, during entropy estimation, the generalization error of regression. This generalization error could be large if the regression algorithms are too rich (e.g. ERM over large functional classes). Our simulations show that, when the regression algorithm is too rich, the variance of the causal inference is large for coupled-estimation but remains low for decoupled-estimation (Fig. 1(a)). By decreasing the richness of the class (simulated by increasing the kernel bandwidth for a kernel regressor) the source of variance shifts to the sample size, and coupled-estimation (which estimates everything on a larger sample) becomes the better procedure and tends to converge faster (Fig. 1(b)).\nFor the consistency result of Theorem 1 we make no assumption on the richness of the regression algorithms, but simply assume that they converge in L2 (Assumption 1). The main technicality is to then show that entropies of residuals are locally continuous relative to the L2 metric in both causal and anticausal directions.\nFor coupled-estimation, the main difficulty is the following. Even though the entropy estimators are consistent for a fixed distribution, the distribution of the residuals change with fn and gn, thus with every random sample (this problem is alleviated by decoupling the estimation). However, if the richness of the regression algorithms is controlled, in other words if the set of potential fn and gn is not too rich, then the entropy estimate for residuals might converge. We show in Theorem 2 (Section 5.2) that if we employ kernel regressors with properly chosen bandwidths, and kernelbased entropy estimators with sufficiently smooth kernels, then the resulting method is consistent for causal inference.\nBoth consistency results of Theorem 1 and Theorem 2 rely on tail assumptions on the additive noise η (where X f,η−−→ Y ). We assume an exponentially decreasing tail for the more difficult case of coupled-estimation, but need only a mild assumption of polynomially decreasing tail in the case of decoupled-estimation. Note that it is common to assume that η has Gaussian tail, and our assumptions are milder in that respect.\nInterestingly, our analysis for Theorem 1 suggests that convergence of causal inference is likely faster if the noise η has faster decreasing tail (see Lemma 3). This is verified in our simulations where we vary the tail of η (Fig. 1(c))."
    }, {
      "heading" : "5. Analysis",
      "text" : ""
    }, {
      "heading" : "5.1. Consistency for Decoupled-estimation",
      "text" : "In this section we establish a general consistency result for the meta-procedure above. The main technicality consists of relating differential entropy of residuals to the L2-norms of residuals (i.e. to the error made in function estimation). We henceforth let Σ denote the Lebesgue measure.\nThe analysis in this section uses the following polynomial tail assumption on η. We note that Assumption 2 satisfies the idenfiability conditions of (Zhang & Hyvärinen, 2009).\nAssumption 2 (Tail). PX,Y is generated as follows: X\nf,η−−→ Y for some bounded function f , with bounded derivative on R. PX has bounded support, and both PX and Pη have densities pX , pη with bounded derivatives on R. Furthermore, we assume η has bounded variance, and pη satisfies, for some T > 0, C > 0, and α > 1:\n∀ |t| > T, pη(t) ≤ C |t|−α . (1)\nNote that, since the unknown target functions are assumed bounded, any consistent regressor can be appropriately truncated while maintaining consistency. We therefore have the following technical assumption on the regressors.\nAssumption 3. The regression procedures return bounded functions: limn→∞max {‖fn(t)‖∞ , ‖gn(t)‖∞} <∞. Theorem 1 (General consistency for decoupled-estimation). Suppose X f,η−−→ Y for some f, η, and PX,Y satisfies the tail Assumption 2. Suppose fn, gn, and Hn are consistent procedures satisfying Assumption 1 and 3. Let the meta-algorithm be decoupled as in Definition 4.\nThen the probability of correctly deciding X → Y goes to 1 as n→∞.\nTo prove the theorem, we have to understand how the estimated entropies converge as a function of the L2 error in regression estimation. We will proceed by bounding the distance between the densities pηY,f and pηY,f′ of the residuals of functions f and f ′ in terms of the L2 distance between f and f ′ (Lemma 3); this will then be used to bound the difference in the entropy of such residuals.\nGiven Assumption 2, the following lemma establishes some useful properties of the distribution PX,Y and of the distribution of certain residuals. It is easy to verify that under our assumptions, all distributions under consideration in the lemma are absolutely continuous.\nLemma 2 (Properties of induced densities). Suppose PX,Y satisfies Assumption 2 for some f, η, and α > 1. We then have the following: (i) pX,Y has a bounded gradient on R2, (ii) consider functions f ′, g : R 7→ R and suppose sup |f ′| and sup |g| are at most T0 for some T0; then there exists T ′ > 0 depending on T0, and C ′ > 0 such that ∀ |t| > T ′\n{ pX,Y (·, t), pX,Y (t, ·), pηY,f′ (t), pηX,g (t) } ≤ C ′ |t|−α .\nIn particular, the above holds for g(y) , E [X|Y = y].\nThe next lemma relates the density of residuals to the L2 distance between functions. Notice, as discussed in Section 4, that the Lemma suggests that the densities of residuals converge faster the sharper the tails of the noise η: the larger α, the sharper the bounds are in terms of the L2 distance between functions.\nLemma 3 (Density of residuals w.r.t. L2 distance). Suppose the joint distribution PX,Y satisfies Assumption 2 for some f, η and α > 1. Let g(y) , E[X|Y = y]. Consider functions f ′, g′ : R 7→ R. There exist a constant C ′′ such that for ‖f − f ′‖2,PX and (respectively) ‖g − g\n′‖2,PY sufficiently small, we have\nsup t∈R ∣∣∣pηY,f′ (t)− pηY,f (t)∣∣∣ ≤ C ′′ (‖f ′ − f‖2,PX)(α−1)/2α , and\nsup t∈R\n∣∣∣pηX,g′ (t)− pηX,g (t)∣∣∣ ≤ C ′′ (‖g′ − g‖2,PY )(α−1)/2α .\nProof. We start by bounding the difference between pηY,f′ (t) and pηY,f′ (t). We note that the same ideas can be used to bound the difference between pηX,g′ (t) and pηX,g (t), sinceX and Y are interchangeable in the analysis from this point on. This is because what follows does not depend on how PX,Y is generated, just on the properties of the induced distributions as stated in Lemma 2.\nWe will partition the space R as follows. First, let R> denote the set\n{ x : |f(x)− f ′(x)| > √ ‖f − f ′‖2,PX } .\nWe define the following interval U ⊂ R: let T ′ be defined as in Lemma 2, and τ > T ′; we have U , [−τ, τ ].\nFor any t ∈ R we have by writing residual densities in terms of the joint pX,Y (as in the proof of Lemma 2 in\nsupplementary appendix) that ∣∣∣pηY,f′ (t)− pη(t)∣∣∣\n= ∣∣∣∣∫ R (pX,Y (x, t+ f ′(x))− pX,Y (x, t+ f(x))) dx ∣∣∣∣ ≤\n∣∣∣∣∣ ∫ R\\D (pX,Y (x, t+ f ′(x))− pX,Y (x, t+ f(x))) dx ∣∣∣∣∣ (2)\n+ ∫ U\\R> |pX,Y (x, t+ f ′(x))− pX,Y (x, t+ f(x))| dx\n(3)\n+ ∣∣∣∣∫ R> (pX,Y (x, t+ f ′(x))− pX,Y (x, t+ f(x))) dx ∣∣∣∣ . (4)\nTo bound the first term (2), let yx denote either of t+f ′(x) or t+ f(x), we have by Lemma 2 that∫ ∞ τ pX,Y (x, yx) dx ≤ ∫ ∞ τ C ′x−α dx ≤ C ′ α− 1 τ−(α−1),\nso that the first term (2) is at most 2 C ′ α−1τ −(α−1).\nTo bound the second term (3) we recall that pX,Y has a bounded gradient on R2 (Lemma 2). Therefore there exists C0 such that for every x, y, ∈ R, pX,Y (x, y + ) differs from pX,Y (x, y) by at most C0 · | |. It follows that the second term (3) is at most∫ U\\R> C0 |f ′(x)− f(x)| dx ≤ 2τ · C0 √ ‖f − f ′‖2,PX .\nThe third term (4) is equal to\n|P (X ∈ R>, Y = t+ f ′(X))− P (X ∈ R>, Y = t+ f(X)) | ≤ PX(R>).\nWe next bound PX (R>) while noting that ‖f − f ′‖2,PX could be 0. Let > ‖f − f ′‖2,PX . By Markov’s inequality,\nPX { |f(X)− f ′(X)| > √ } ≤ ‖f − f ′‖1,PX√\n≤ ‖f − f ′‖2,PX√ .\nThus, consider a sequence of → ‖f − f ′‖2,PX , by Fatou’s lemma we have PX (R>) ≤ √ ‖f − f ′‖2,PX .\nCombining the above analysis we have that∣∣∣pηY,f′ (t)− pη(t)∣∣∣ ≤2 C ′α− 1τ−(α−1) + (1 + 2τ · C0) √ ‖f − f ′‖2,PX .\nNow, for ‖f − f ′‖2,PX sufficiently small, we can pick τ = O ( ‖f − f ′‖2,PX )−1/2α to get the result.\nAs previously noted we can use the same ideas as above to similarly bound ∣∣∣pηX,g′ (t)− pηX,g (t)∣∣∣ for all t ∈ R. It suffices to interchange X and Y in the above analysis.\nLemma 4. Let p1, p2 be two densities such that there exist T,C > 1 and α > 1, for all |t| > T , maxi∈[2] pi(t) < C |t|−α. Suppose supt∈R |p1(t)− p2(t)| < for some < min { 1/T 2, 1/(3e) } satisfying the further condition: ∀t > 1/ √ , t(α−1)/2 > ln t. We then have for sufficiently small\n|H(p1)−H(p2)| ≤ 18 √ ln(1/3 ) + 4Cα\nα− 1 (α−1)/4.\nProof. For simplicity of notation in what follows, let τ , 1/ √ . Let U , [−τ, τ ] and let U2> , {t ∈ U , p2(t) > 2 }. Define γ(u) = −u lnu for u > 0, and γ(0) = 0. We will use the fact that for the function γ(·) is increasing on [0, 1/e]. We have\nH(p1) = ∫ R\\U γ(p1(t)) dt+ ∫ U2> γ(p1(t)) dt\n+ ∫ U\\U2> γ(p1(t)) dt\n≤ ∫ R\\U γ(p1(t)) dt+ ∫ U2> p1(t) ln 1 p1(t) dt\n+ Σ (U \\ U2>) · γ(3 ), (5)\nsince for t ∈ U \\ U2> we have\np1(t) ≤ p2(t) + ≤ 3 ≤ 1/e.\nTo bound the first term of (5), notice that∫ ∞ τ γ(p1(t)) dt ≤ ∫ ∞ τ −Ct−α ln ( Ct−α ) dt\n≤ ∫ ∞ τ Cαt−α ln t dt\n≤ ∫ ∞ τ Cαt−(α+1)/2 dt ≤ 2Cα α− 1 τ−(α−1)/2,\nhence we have∫ R\\U γ(p1(t)) dt ≤ C ′τ−α ′ for C ′, α′ > 0.\nNext we bound the second term of (5) as follows:∫ U2> p1(t) ln 1 p1(t) dt\n≤ ∫ U2> (p2(t) + ) ln 1 p2(t)− dt\n= ∫ U2> p2(t) ln 1 p2(t)(1− /p2(t)) dt\n+ ∫ U2> ln 1 p2(t)− dt\n≤ H(p2) + ∫ U2> p2(t) ln 1 1− /p2(t) dt\n+ ∫ U2> ln 1 dt\n≤ H(p2) + ∫ U2> p2(t) ln(1 + 2 /p2(t)) dt + Σ (U2>) · γ( ) ≤ H(p2) + 2Σ (U2>) · + Σ (U2>) · γ( ).\nCombining all the above, we have\nH(p1) ≤H(p2) + 3Σ (U) · γ(3 ) + C ′τ−α ′\n=H(p2) + 18 √ ln(1/3 ) + C ′ α ′/2.\nNotice that p1 and p2 are interchangeable in the above argument. The result therefore follows.\nWe are now ready to prove the main theorem.\nProof of Theorem 1\nLet f(x) , E [Y |x] and g(y) , E [X|y]. By Lemma 1,\nH(X) +H (ηY,f ) > H(Y ) +H (ηX,g) + 8 , (6)\nfor some > 0.\nThus we detect the right direction X → Y if all quantities (a) |Hn (ηY,fn)−H (ηY,f )|, (b) |Hn (ηX,gn)−H (ηY,g)|, (c) |Hn(X)−H(X)|, and (d) |Hn(Y )−H(Y )|, are at most .\nBy assumption, (c) and (d) both tend to 0 in probability. The quantities (a) and (b) are handled as follows. We only show the argument for (a), as the argument for (b) is the same. We have:\n|Hn (ηY,fn)−H (ηY,f )| ≤ |Hn (ηY,fn)−H (ηY,fn)| + |H (ηY,fn)−H (ηY,f )| .\nNow Hn (ηY,fn) is consistent for fn fixed (it easy to check that PηY,fn satisfies the necessary conditions provided fn is bounded) and fn is learned on an independent sample from Hn, we have |Hn (ηY,fn)−H (ηY,fn)| P−→ 0.\nBy Lemma 3, convergence of fn i.e. ‖fn − f‖2,PX P−→ 0 implies supt ∣∣pηY,fn (t)− pηY,f (t)∣∣ P−→ 0; this in turn implies by Lemma 4 that |H (ηY,fn)−H (ηY,f )| P−→ 0.\nThus all quantities (a)-(d) are at most with probability going to 1."
    }, {
      "heading" : "5.2. Coupled Regression and Residual-entropy Estimation",
      "text" : "Here we consider a coupled version of the meta-algorithm where fn and gn are kernel regressors. This is described in the next subsection."
    }, {
      "heading" : "5.2.1. KERNEL INSTANTIATION OF THE META-ALGORITHM",
      "text" : "Regression: Although any kernel that is 0 outside a bounded region will work for the regression, we focus here (for simplicity) on the particular case where fn and gn are box-kernel regressors defined as follows (interchange X and Y to obtain gn(y)):\nfn(x) = 1\nnx,h n∑ i=1 Yi1{|Xi−x|<h}, (7)\nwhere nx,h = |i : |Xi − x| < h| , for a bandwidth h.\nEntropy estimation: Given a sequence = { i}ni=1, and a bandwidth σ, define pn, as follows:\npn, (t) = 1\nnh n∑ i=1 K ( i − t σ ) ,\nwhere ∫ R K(u) du = 1, ∣∣∣∣ dduK(u) ∣∣∣∣ <∞, and K(u) = 0 for |u| ≥ 1.\nLet Y,i = Yi − fn(Xi) and X,i = Xi − gn(Yi). The residual entropy estimators are defined as:\nHn (ηY,fn) , H (pn, Y ) and Hn (ηX,gn) , H (pn, X ) . (8)"
    }, {
      "heading" : "5.2.2. CONSISTENCY RESULT FOR COUPLED-ESTIMATION",
      "text" : "We abuse notation and use h and σ to denote the bandwidth parameters used to estimate either fn andHn (ηY,fn), or gn and Hn (ηX,gn). We make the distinction clear whenever needed.\nThe consistency result depends on the following quantities bounded in Lemma 5. Definition 6 (Expected average excess risk). Define Rn(fn) , E 1n ∑n i=1 |fn(Xi)− f(Xi)| and similarly\nRn(gn) , E 1n ∑n i=1 |gn(Yi)− g(Yi)|.\nWe assume in this section that the noise η has exponentially decreasing tail:\nDefinition 7. A r.v. Z has exponentially decreasing tail if there exists C,C ′ > 0 such that for all t > 0, P (|Z − EZ| > t) ≤ Ce−C′t.\nThe following consistency theorem hinges on properly choosing the bandwidths parameters h and σ. Essentially we want to choose h such that regression estimation is consistent, and we want to choose σ so as not to overfit regression error. If the bandwidth σ is too small relative to regression error (captured by Rn), then the entropy estimator (for the residual entropy) is only fitting this error. The conditions on σ in the Theorem are mainly to ensure that σ is not too small relative to regression error Rn.\nTheorem 2 (Coupled estimation). Suppose X f,η−−→ Y for some f, η, and suppose PX,Y satisfies Assumption 2, and η has exponentially decreasing tail. Let fn, gn, and Hn be defined as in Section 5.2.1, and let bothHn(X) andHn(Y ) be consistent as in Assumption 1.\nSuppose that : (i) For learning fn and Hn (ηY,fn), we use h = c1n\n−α for some c1 > 0 and 0 < α < 1, and σ = c2n−β for some c2 > 0 and 0 < β < min {(1− α)/4, α/2}. (ii) For learning gn and Hn (ηX,gn), h satisfies h → 0 and nh → ∞, and σ satisfies σ → 0, nσ → ∞, and σ = Ω(Rn(gn) −γ) for some 0 < γ < 1/2.\nThen the probability of correctly detecting X → Y goes to 1 as n→∞.\nThe theorem relies on Lemma 5 which bounds the errors Rn for both fn and gn. Suppose X\nf,η−−→ Y , then if f is smooth or continuously differentiable,Rn(fn)→ 0, and in fact we can obtain finite rates of convergence for Rn(fn), thus yielding advice on setting σ. The second part of the Lemma corresponds to this situation.\nHowever, as mentioned earlier in the paper introduction, a smooth f does not ensure that g(y) , g(X|y) is smooth or even continuous, so we do not have rates for Rn(gn). We can nonetheless show that Rn(gn) would generally converge to 0, which is sufficient for there to be proper settings for σ (i.e. σ larger than the error, but also tending to 0).\nWe note that the r.v.’s X and Y are interchangeable in this lemma since it does not assume X → Y . The proof is given in the supplemental appendix.\nLemma 5. Let fn be defined as in (7). Let f(x) , E [Y |x]. Suppose (i) EY 2 < ∞ and that f is bounded; h → 0 and nh→∞. Then E 1n ∑n 1 |fn(Xi)− f(Xi)| n→0−−−→ 0.\nSuppose further (ii) that PX has bounded support and that f is continuously differentiable; h = c1n−α for some c1 > 0 and 0 < α < 1.\nThen we have E 1n ∑n\n1 |fn(Xi)− f(Xi)| ≤ c2n−β , for β , min {(1− α)/2, α}.\nWe can now prove the theorem of this section.\nProof of Theorem 2\nLet ̄Y,i , Yi−f(Xi) and ̄X,i , Xi−g(Yi). Note that, under our conditions on σ both H (pn,̄Y ) and H (pn,̄X ) are respectively consistent estimators of H (ηY,f ) , H(η) and H (ηX,g) (see e.g. (Beirlant et al., 1997)). For any two densities p, p′ we write |p− p′| to denote supt |p(t)− p′(t)|.\nGiven the assumption that K has bounded derivative on R, there exists a constant cK such that\n|pn,̄Y − pn, Y | ≤ cK σ2 ·\n( 1\nn n∑ i=1 |fn(Xi)− f(Xi)| ) which implies\nE |pn,̄Y − pn, Y | 1/2 ≤\n√ cKRn(fn)\nσ ,\nand also\n|pn,̄X − pn, X | ≤ cK σ2 ·\n( 1\nn n∑ i=1 |gn(Yi)− g(Yi)| ) which also implies\nE |pn,̄X − pn, X | 1/2 ≤\n√ cKRn(gn)\nσ .\nThus by Lemma 5, we have E |pn,̄X − pn, X | 1/2 → 0, which in turn implies by Markov’s inequality that |pn,̄X − pn, X |\nP−→ 0. Now since PX has bounded support, both pn, X and pn,̄X have bounded support, and\nhence by Lemma 4 we have ∣∣H (p X,gn )−H (p̄X,g)∣∣ P−→ 0. Hence we also have |Hn (ηX,gn)−H(ηX,g)| P−→ 0.\nAgain by Lemma 5, we have that, for n sufficiently large, E |pn,̄Y − pn, Y |\n1/2 ≤ Cn−β/2 for some C > 0. Therefore by Markov’s inequality, we have P ( |pn,̄Y − pn, Y | ≥ √ Cn−β/4 ) → 0. Now, under the exponential tail assumption on the noise, all Yi samples are contained in a region of size C ′ log n with probability at least 1/n. Thus, since K is supported in [−1, 1], both pn,̄X and pn, Y are 0 outside a region of size C\n′′ log n. Let T be as in Lemma 4; for all n sufficiently large,√ Cn−β/4 < 1/(C ′′ log n)2 = 1/T 2. It follows by\nLemma 4 that ∣∣H (p Y,fn )−H (p̄Y,f )∣∣ P−→ 0, and hence that |Hn (ηY,fn)−H(ηY,f )| P−→ 0.\nThe rest of the proof is similar to that of Theorem 1 by calling on Lemma 1 and using the consistency of Hn(X) and Hn(Y )."
    }, {
      "heading" : "6. Final Remarks",
      "text" : "We derived the first consistency results for an existing family of procedures for causal inference under the Additive Noise Model. We obtained mild algorithmic requirements, and various distributional tail conditions which guarantee consistency. The present work focuses on the case of two r.v.s X and Y , which captures the inherent difficulties of consistency. We believe however that the insights developed should extend to the case of random vectors under corresponding tail conditions. The details however are left for future work.\nAnother interesting multivariate situation is that of a causal network of r.v.s. as in Peters et al. (2011b) dicussed earlier. Extending our consistency results to this particular multivariate case would primarily consist of extending our distributional tail conditions to the tails of distributions resulting from conditioning on appropriate sets of variables in the network. This is however a non-trivial extension as it involves, e.g. for the convergence of conditional entropies, some additional integration steps that have to be carefully worked out.\nA possible future direction of investigation is to understand under what conditions finite sample rates can be obtained for such procedures. For reasons explained earlier, we do not believe that this is possible without less general distributional assumptions."
    }, {
      "heading" : "A. Omitted figures from Section 4",
      "text" : "Some addtional experimental results were omitted in the main paper for space, and are given in Figure 2."
    }, {
      "heading" : "B. Omitted Proofs: Section 5.1",
      "text" : "Proof of Lemma 2. Note that, by assumption, both pX and pη are bounded. For any x, y ∈ R, we have\npX,Y (x, y) = pX(x) · pY |x(y) = pX(x) · pη(y − f(x)). (9)\ntherefore ddxpX,Y (x, y) is given by\nd\ndx pX(x) · pη(y − f(x))−\nd dx f(x) · d dx pη(y − f(x)).\nIt is clear that supx,y d dxpX,Y (x, y) < ∞. Similarly supx,y d dypX,Y (x, y) < ∞. Also, since pη is bounded, we have from (9) that for |t| sufficiently large, pX,Y (t, y) = 0 independent of y. Also, since f and pX are bounded, we have, independent of x, that for |t| sufficiently large, pX,Y (x, t) < C ′t−α for some C ′ > 0.\nNext the density of the residual ηY,f ′ of a function f ′ is easily obtained as follows for any t ∈ R.\npηY,f′ (t) = ∫ R pX(x) · pY |x(t+ f ′(x)) dx\n= ∫ R pX,Y (x, t+ f ′(x)) dx (10)\n= ∫ R pX(x) · pη(t+ f ′(x)− f(x)) dx.\nThus if f ′ is bounded, there exists T ′ > 0 such that for |t| > T ′,\npηY,f′ (t) ≤ C ′ |t|−α · ∫ R pX(x) dx = C ′ |t|−α ,\nwhere α is the same as for the bound on pη from the assumption.\nWe have similarly for any function g that,\npηX,g (t) = ∫ pY >0 pY (y) · pX|y(t+ g(y)) dy\n= ∫ R pX,Y (t+ g(y), y) dy. (11)\n= ∫ R pX(t+ g(y)) · pη(y − f(t+ g(y))) dy.\nIf g is bounded, then for t sufficiently large, pX(t+g(y)) = 0 for all y so pηX,g (t) = 0."
    }, {
      "heading" : "C. Omitted Proofs: Section 5.2",
      "text" : "We denote the random n-samples as Xn , {Xi}n1 and Y n , {Yi}n1 , throughout this section.\nWe bound Rn(fn) and Rn(gn) in Lemma 5 which makes use of Lemma 6. We note that the r.v.’s X and Y are interchangeable in the two lemmas 5 and 6, since they do not assume X → Y .\nLemma 6 ((Gyorfi et al., 2002)). For any positive f : R → R such that E f < ∞, there exists c0 such that EX,Xn { 1\nnX,h ∑ Xi:|Xi−X|<h f(Xi) } ≤ c0E f(X).\nProof of Lemma 5. We simply have to show that E 1n ∑n 1 |fn(Xi)− f(Xi)|\n2 n→∞−−−−→ 0 since by Hölder’s inequality and Jensen’s inequalities, for any φn(·),\nE 1\nn ∑ i |φn(Xi)| ≤E √ 1 n ∑ i |φn(Xi)|2\n≤ √ E 1\nn ∑ i |φn(Xi)|2.\nFor assumption (i), pick any > 0. We will show that for n sufficiently large, the above expectation is at most (7 + 3c0) , where c0 is as in Lemma 6. The further claim of assumption (ii) will be obtained along the way.\nFirst condition on Xn, fixing x = Xi for some Xi, and taking expectation with respect to the randomness in Y n , {Yi}n1 . We have by a standard bias-variance decomposition (see e.g. (Gyorfi et al., 2002)) that EY n|Xn |fn(x)− f(x)| 2\n≤ C nx,h + ∣∣∣∣∣∣ 1nx,h ∑\n|Xj−x|<h\nf(Xj)− f(x) ∣∣∣∣∣∣ 2 = Ax +Bx,\n(12)\nfor some C depending on the variance of Y .\nWe start with a bound on the first term of (12). Pick an interval S such that PX(R \\ S) < .\nConsider an (h/2)-cover Z of S such that for every z ∈ S, the interval [z − h/2, z + h/2] is contained in S. We can pick such a Z of size at most 2Σ(S)/h. Note that for any x ∈ [z − h/2, z + h/2], nx,h ≥ nz,h/2 ,\n|{Xi : |z −Xi| < h/2}|. We then have\n1\nn n∑ i=1 AXi = 1 n n∑ i=1 C nXi,h ( 1{Xi∈S} + 1{Xi /∈S} ) ≤ 1 n n∑ j=1 C nXi,h 1{Xi∈S} + 1 n n∑ i=1 1{Xi /∈S}\n≤ 1 n ∑ z∈Z ∑ Xi:|z−Xi|≤h/2 C nXi,h + 1 n n∑ i=1 1{Xi /∈S}\n≤ 1 n ∑ z∈Z C · nz,h/2 nz,h/2 + 1 n n∑ i=1 1{Xi /∈S}.\nTherefore by taking expectation over Xn and letting nh sufficiently large, we have\nE 1\nn n∑ i=1 AXi ≤ 2C · Σ(S) nh + PX(R \\ S) ≤ 2 .\nUnder assumption (ii), pick S larger than the support of PX , we have by the same equation above that for large n\nE 1\nn n∑ i=1 AXi ≤ 2C · Σ(S) nh = 2C · Σ(S) c1n1−α .\nWe now turn to the second term of (12). Under assumption (ii) the function f is Lipschitz continuous and we therefore have for some constant cf that Ax ≤ cfh2 = cfc1n−2α. Combining with the bound on Ax gives the result for assumption (i).\nFor assumption (i) we proceed as follows. It is well known that bounded uniformly continuous functions are dense in L2,PX for any PX . Therefore let f̃ be a bounded uniformly\ncontinuous function such that ∥∥∥f̃ − f∥∥∥\n2,PX < √ . Since\nh = h(n) → 0, we have sup|x,x′|<h ∣∣∣f̃(x)− f̃(x′)∣∣∣2 < for n sufficiently large. The second term of the r.h.s. of\nthe above equation (12) can then be bounded as follows. If nx,h = 1, then Bx = 0. Otherwise, if nx,h > 1, we have\nBx ≤ 3\nnx,h ∑ |Xj−x|<h (∣∣∣f(Xj)− f̃(Xj)∣∣∣2 + ∣∣∣f̃(Xj)− f̃(x)∣∣∣2 + ∣∣∣f̃(x)− f(x)∣∣∣2)\n≤3 + ∣∣∣f̃(x)− f(x)∣∣∣\n+ 3\nnx,h ∑ |Xj−x|<h ∣∣∣f(Xj)− f̃(Xj)∣∣∣2 ≤3 + ( 3\nnx,h + 1 ) ∣∣∣f̃(x)− f(x)∣∣∣ + 3\nnx,h − 1 ∑\n|Xj−x|<h,Xj 6=x ∣∣∣f(Xj)− f̃(Xj)∣∣∣2 . Therefore taking expectation over Xn, and applying Lemma 6 to the second term above for x = Xi, we have\nEBXi ≤3 + 2EX ∣∣∣f̃(X)− f(X)∣∣∣2\n+ 3c0EX ∣∣∣f̃(X)− f(X)∣∣∣2 = (5 + 3c0) ,\nso that E 1n ∑ iBXi ≤ (5 + 3c0) ."
    } ],
    "references" : [ {
      "title" : "Nonparametric entropy estimation: An overview",
      "author" : [ "Beirlant", "Jan", "Dudewicz", "Edward J", "Györfi", "László", "Van der Meulen", "Edward C" ],
      "venue" : "International Journal of Mathematical and Statistical Sciences,",
      "citeRegEx" : "Beirlant et al\\.,? \\Q1997\\E",
      "shortCiteRegEx" : "Beirlant et al\\.",
      "year" : 1997
    }, {
      "title" : "Cam: Causal additive models, high-dimensional order search and penalized regression",
      "author" : [ "P. Buhlmann", "J. Peters", "J. Ernest" ],
      "venue" : null,
      "citeRegEx" : "Buhlmann et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Buhlmann et al\\.",
      "year" : 2013
    }, {
      "title" : "Elements of information theory",
      "author" : [ "Cover", "Thomas M", "Thomas", "Joy A", "Kieffer", "John" ],
      "venue" : "SIAM Review,",
      "citeRegEx" : "Cover et al\\.,? \\Q1994\\E",
      "shortCiteRegEx" : "Cover et al\\.",
      "year" : 1994
    }, {
      "title" : "A Distribution Free Theory of Nonparametric Regression",
      "author" : [ "L. Gyorfi", "M. Kohler", "A. Krzyzak", "H. Walk" ],
      "venue" : null,
      "citeRegEx" : "Gyorfi et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Gyorfi et al\\.",
      "year" : 2002
    }, {
      "title" : "Nonlinear causal discovery with additive noise models",
      "author" : [ "Hoyer", "Patrik O", "Janzing", "Dominik", "JM Mooij", "Peters", "Jonas", "Schölkopf", "Bernhard" ],
      "venue" : "Proceedings of Advances in Neural Processing Information Systems,",
      "citeRegEx" : "Hoyer et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Hoyer et al\\.",
      "year" : 2009
    }, {
      "title" : "Causality: models, reasoning and inference, volume 29",
      "author" : [ "Pearl", "Judea" ],
      "venue" : null,
      "citeRegEx" : "Pearl and Judea.,? \\Q2000\\E",
      "shortCiteRegEx" : "Pearl and Judea.",
      "year" : 2000
    }, {
      "title" : "Causal inference on discrete data using additive noise models",
      "author" : [ "Peters", "Jonas", "Janzing", "Dominik", "Scholkopf", "Bernhard" ],
      "venue" : "Pattern Analysis and Machine Intelligence, IEEE Transactions on,",
      "citeRegEx" : "Peters et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Peters et al\\.",
      "year" : 2011
    }, {
      "title" : "Identifiability of causal graphs using functional models",
      "author" : [ "Peters", "Jonas", "Mooij", "Joris", "Janzing", "Dominik", "Schölkopf", "Bernhard" ],
      "venue" : "Proceedings of the 27th Conference on Uncertainty in Artificial Intelligence,",
      "citeRegEx" : "Peters et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Peters et al\\.",
      "year" : 2011
    }, {
      "title" : "A linear non-gaussian acyclic model for causal discovery",
      "author" : [ "Shimizu", "Shohei", "Hoyer", "Patrik O", "Hyvärinen", "Aapo", "Kerminen", "Antti" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "Shimizu et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Shimizu et al\\.",
      "year" : 2006
    }, {
      "title" : "Causation Prediction & Search 2e, volume 81",
      "author" : [ "Spirtes", "Peter", "Glymour", "Clark N", "Scheines", "Richard" ],
      "venue" : "MIT press,",
      "citeRegEx" : "Spirtes et al\\.,? \\Q2000\\E",
      "shortCiteRegEx" : "Spirtes et al\\.",
      "year" : 2000
    }, {
      "title" : "Nonlinear directed acyclic structure learning with weakly additive noise models",
      "author" : [ "Tillman", "Robert", "Gretton", "Arthur", "Spirtes", "Peter" ],
      "venue" : "Proceedings of Advances in Neural Processing Information Systems,",
      "citeRegEx" : "Tillman et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Tillman et al\\.",
      "year" : 2009
    }, {
      "title" : "On the identifiability of the post-nonlinear causal model",
      "author" : [ "Zhang", "Kun", "Hyvärinen", "Aapo" ],
      "venue" : "In Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence,",
      "citeRegEx" : "Zhang et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2009
    } ],
    "referenceMentions" : [ {
      "referenceID" : 9,
      "context" : "Conditional-independence-based methods (Pearl, 2000; Spirtes et al., 2000) estimate a set of directed acyclic graphs, all entailing the same conditional independences, from the data.",
      "startOffset" : 39,
      "endOffset" : 74
    }, {
      "referenceID" : 8,
      "context" : "Early work by (Shimizu et al., 2006) showed that the CAM is identifiable when the functional relationship Y = f(X) + η is linear, provided the independent noise η is not Gaussian.",
      "startOffset" : 14,
      "endOffset" : 36
    }, {
      "referenceID" : 4,
      "context" : ", 2006; Hoyer et al., 2009; Tillman et al., 2009; Peters et al., 2011a;b) which is given as follows: given two random variables X and Y , X is assumed to cause Y if (i) Y can be obtained as a function of X plus a noise term independent of X , but (ii) X cannot be obtained as a function of Y plus independent noise, then we infer that X causes Y . In this case, where (i) and (ii) hold simultaneously, the CAM is termed identifiable. Initial work on the CAM focused on establishing its theoretical soundness, i.e. understanding the class of distributions PX,Y for which the CAM is identifiable, i.e. for which (i) and (ii) hold simultaneously. Early work by (Shimizu et al., 2006) showed that the CAM is identifiable when the functional relationship Y = f(X) + η is linear, provided the independent noise η is not Gaussian. Later, Hoyer et al. (2009), Zhang & Hyvärinen (2009) and Peters et al.",
      "startOffset" : 8,
      "endOffset" : 851
    }, {
      "referenceID" : 4,
      "context" : ", 2006; Hoyer et al., 2009; Tillman et al., 2009; Peters et al., 2011a;b) which is given as follows: given two random variables X and Y , X is assumed to cause Y if (i) Y can be obtained as a function of X plus a noise term independent of X , but (ii) X cannot be obtained as a function of Y plus independent noise, then we infer that X causes Y . In this case, where (i) and (ii) hold simultaneously, the CAM is termed identifiable. Initial work on the CAM focused on establishing its theoretical soundness, i.e. understanding the class of distributions PX,Y for which the CAM is identifiable, i.e. for which (i) and (ii) hold simultaneously. Early work by (Shimizu et al., 2006) showed that the CAM is identifiable when the functional relationship Y = f(X) + η is linear, provided the independent noise η is not Gaussian. Later, Hoyer et al. (2009), Zhang & Hyvärinen (2009) and Peters et al.",
      "startOffset" : 8,
      "endOffset" : 877
    }, {
      "referenceID" : 4,
      "context" : ", 2006; Hoyer et al., 2009; Tillman et al., 2009; Peters et al., 2011a;b) which is given as follows: given two random variables X and Y , X is assumed to cause Y if (i) Y can be obtained as a function of X plus a noise term independent of X , but (ii) X cannot be obtained as a function of Y plus independent noise, then we infer that X causes Y . In this case, where (i) and (ii) hold simultaneously, the CAM is termed identifiable. Initial work on the CAM focused on establishing its theoretical soundness, i.e. understanding the class of distributions PX,Y for which the CAM is identifiable, i.e. for which (i) and (ii) hold simultaneously. Early work by (Shimizu et al., 2006) showed that the CAM is identifiable when the functional relationship Y = f(X) + η is linear, provided the independent noise η is not Gaussian. Later, Hoyer et al. (2009), Zhang & Hyvärinen (2009) and Peters et al. (2011a) showed that the CAM is identifiable more generally even if f is nonlinear, the main technical requirements being that the marginals PX , and Pη are absolutely continuous on R, with Pη having support R.",
      "startOffset" : 8,
      "endOffset" : 903
    }, {
      "referenceID" : 4,
      "context" : ", 2006; Hoyer et al., 2009; Tillman et al., 2009; Peters et al., 2011a;b) which is given as follows: given two random variables X and Y , X is assumed to cause Y if (i) Y can be obtained as a function of X plus a noise term independent of X , but (ii) X cannot be obtained as a function of Y plus independent noise, then we infer that X causes Y . In this case, where (i) and (ii) hold simultaneously, the CAM is termed identifiable. Initial work on the CAM focused on establishing its theoretical soundness, i.e. understanding the class of distributions PX,Y for which the CAM is identifiable, i.e. for which (i) and (ii) hold simultaneously. Early work by (Shimizu et al., 2006) showed that the CAM is identifiable when the functional relationship Y = f(X) + η is linear, provided the independent noise η is not Gaussian. Later, Hoyer et al. (2009), Zhang & Hyvärinen (2009) and Peters et al. (2011a) showed that the CAM is identifiable more generally even if f is nonlinear, the main technical requirements being that the marginals PX , and Pη are absolutely continuous on R, with Pη having support R. Note that Zhang & Hyvärinen (2009) also introduces a generalization of the CAM termed post-nonlinear models.",
      "startOffset" : 8,
      "endOffset" : 1140
    }, {
      "referenceID" : 4,
      "context" : ", 2006; Hoyer et al., 2009; Tillman et al., 2009; Peters et al., 2011a;b) which is given as follows: given two random variables X and Y , X is assumed to cause Y if (i) Y can be obtained as a function of X plus a noise term independent of X , but (ii) X cannot be obtained as a function of Y plus independent noise, then we infer that X causes Y . In this case, where (i) and (ii) hold simultaneously, the CAM is termed identifiable. Initial work on the CAM focused on establishing its theoretical soundness, i.e. understanding the class of distributions PX,Y for which the CAM is identifiable, i.e. for which (i) and (ii) hold simultaneously. Early work by (Shimizu et al., 2006) showed that the CAM is identifiable when the functional relationship Y = f(X) + η is linear, provided the independent noise η is not Gaussian. Later, Hoyer et al. (2009), Zhang & Hyvärinen (2009) and Peters et al. (2011a) showed that the CAM is identifiable more generally even if f is nonlinear, the main technical requirements being that the marginals PX , and Pη are absolutely continuous on R, with Pη having support R. Note that Zhang & Hyvärinen (2009) also introduces a generalization of the CAM termed post-nonlinear models. Further work by Peters et al. (2011b) showed how to reduce causal inference for a network of multiple variables under the CAM to the case of two variables X and Y discussed so far, by properly extending the conditions (i) and (ii) to conditional distributions instead of marginals.",
      "startOffset" : 8,
      "endOffset" : 1252
    }, {
      "referenceID" : 8,
      "context" : "Various works (Shimizu et al., 2006; Hoyer et al., 2009; Peters et al., 2011a) have successfully validated procedures based on the CAM (outlined in Section 1.",
      "startOffset" : 14,
      "endOffset" : 78
    }, {
      "referenceID" : 4,
      "context" : "Various works (Shimizu et al., 2006; Hoyer et al., 2009; Peters et al., 2011a) have successfully validated procedures based on the CAM (outlined in Section 1.",
      "startOffset" : 14,
      "endOffset" : 78
    }, {
      "referenceID" : 3,
      "context" : ", 2006; Hoyer et al., 2009; Peters et al., 2011a) have successfully validated procedures based on the CAM (outlined in Section 1.1 below) on a mix of artificial and real-world datasets where the causal structure to be inferred is clear. However, on the theoretical side, it remains unclear whether these procedures can infer causality from samples in general situations where the CAM is identifiable. In the particular case where the functional relation between X and Y is linear, Hyvärinen et al. (2008) proposed a successful method shown to be consistent.",
      "startOffset" : 8,
      "endOffset" : 505
    }, {
      "referenceID" : 1,
      "context" : "In a recent Arxived result appearing after our initial submission, Buhlmann et al. (2013) showed the consistency of a maximum log-likelihood approach to causal inference under the multi-variable network extension of Peters et al.",
      "startOffset" : 67,
      "endOffset" : 90
    }, {
      "referenceID" : 1,
      "context" : "In a recent Arxived result appearing after our initial submission, Buhlmann et al. (2013) showed the consistency of a maximum log-likelihood approach to causal inference under the multi-variable network extension of Peters et al. (2011b). While consistency has been shown for particular procedures, in this paper we are rather interested in general conditions under which common approaches, with various algorithmic instantiations, are consistent.",
      "startOffset" : 67,
      "endOffset" : 238
    }, {
      "referenceID" : 3,
      "context" : "(Gyorfi et al., 2002), Theorem 3.",
      "startOffset" : 0,
      "endOffset" : 21
    }, {
      "referenceID" : 2,
      "context" : "Setup and Notation We letH and I denote respectively differential entropy, and mutual information (Cover et al., 1994).",
      "startOffset" : 98,
      "endOffset" : 118
    }, {
      "referenceID" : 3,
      "context" : "kernel, k-NN, Kernel-SVM, spline regressors) are consistent in the above sense (Gyorfi et al., 2002).",
      "startOffset" : 79,
      "endOffset" : 100
    }, {
      "referenceID" : 0,
      "context" : "plug-in entropy estimators) is well established (Beirlant et al., 1997).",
      "startOffset" : 48,
      "endOffset" : 71
    }, {
      "referenceID" : 0,
      "context" : "For entropy estimation we employ a resubstitution estimate using a kernel density estimator tuned against log-likelihood (Beirlant et al., 1997) and for regression estimator we use kernel regression (KR).",
      "startOffset" : 121,
      "endOffset" : 144
    }, {
      "referenceID" : 0,
      "context" : "(Beirlant et al., 1997)).",
      "startOffset" : 0,
      "endOffset" : 23
    } ],
    "year" : 2014,
    "abstractText" : "We analyze a family of methods for statistical causal inference from sample under the socalled Additive Noise Model. While most work on the subject has concentrated on establishing the soundness of the Additive Noise Model, the statistical consistency of the resulting inference methods has received little attention. We derive general conditions under which the given family of inference methods consistently infers the causal direction in a nonparametric setting.",
    "creator" : "LaTeX with hyperref package"
  }
}