{
  "name" : "1602.02285.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "A Deep Learning Approach to Unsupervised Ensemble Learning",
    "authors" : [ "Uri Shaham", "Xiuyuan Cheng", "Omer Dror", "Ariel Jaffe", "Boaz Nadler", "Joseph Chang", "Yuval Kluger" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "In recent years, crowdsourcing applications gained significant popularity, and consequently much academic attention. At the same time, deep learning has become a major tool in machine learning and artificial intelligence, demonstrating impressive performance in several applications, including computer vision, speech recognition and natural language processing.\nThe goal of this paper is to show that deep learning methods can also be applied to the areas of crowdsourcing and unsupervised ensemble learning, and provide state-of-the-art results. In unsupervised ensemble learning, one is given the predictions of d classifiers on a set of n instances and the goal is to recover the true, unknown label of each instance. Dawid and Skene (1979) were among the first to consider such a setup. They assumed that the classifiers are conditionally independent given the true labels. We refer to this model as the DS model and also as the Conditional Independence model.\nDespite its simplicity, computing the maximum likelihood estimates of the classifiers’ accuracies and the true labels in the DS model is a non-convex optimization problem. In their paper, Dawid and Skene estimated these quantities by the EM algorithm, which is only guaranteed\nar X\niv :1\n60 2.\n02 28\n5v 1\n[ st\nat .M\nL ]\nto converge to a local optimum. In recent years, several authors developed computationally efficient spectral methods that are asymptotically consistent under the DS model, see Zhang et al. (2014); Parisi et al. (2014); Jain and Oh (2013); Jaffe et al. (2014) and references therein.\nThe model of Dawid and Skene relied on two key assumptions that typically do not hold in practice: (i) that classifiers make perfectly independent errors; and (ii) that these errors are uniformly distributed across all instances. To address the second issue above, several authors proposed richer models, that include parameters such as instance difficulty and varying skills of annotators across different regions of the input space, see for example Raykar et al. (2010), Whitehill et al. (2009) and Welinder et al. (2010).\nIn contrast, relatively few works considered relaxations of the conditional independence assumption: Platanios et al. (2014) proposed to estimate the accuracies of possibly dependent classifiers, via their agreement rates over classifier groups of different sizes. Donmez et al. (2010) proposed a model with pairwise interactions between all classifiers. Closest to our approach is the work of Jaffe et al. (2015), who assumed that some of the classifiers may be conditionally dependent, yet their dependency structure can be accurately described by a tree of depth 2.\nIn this manuscript, we propose a deep learning approach to unsupervised ensemble learning problems with possibly dependent classifiers, where the conditional independence assumption is strongly violated. We make the following contributions. First, we show that the DS model has an equivalent parametrization in terms of a Restricted Boltzmann Machine (RBM) with a single hidden node. Hence, under this model, the posterior probability of the true labels can be estimated from a trained RBM. Next, to tackle violations of conditional independence, we show how a RBM-based Deep Neural Net (DNN) can be applied to unsupervised ensemble learning, and propose a heuristic for determining the DNN architecture. Experimentally, we compare our approach to several state-of-the-art methods that are based on the conditional independence assumption and relaxations of it. We show that our DNN approach often performs better than the other methods on both simulated and real world datasets. Remarkably, we demonstrate that in some cases, while the raw representation of the data contains correlated features, the learned features in the last hidden layer are almost perfectly uncorrelated.\nThe structure of this manuscript is as follows: in Section 2 we give a formal definition of the problem. A brief background on RBMs is given in Section 3. In Section 4 we show how RBMs can be used to predict the true labels, under the assumption of conditional independence. In Section 5 we describe how to estimate the labels using a RBM-based DNN. Experimental results are reported in Section 6. The manuscript concludes with a brief summary in Section 7. Proofs appear in the appendix."
    }, {
      "heading" : "1.1 Notation",
      "text" : "Throughout this manuscript, X,H, Y are random variables, pθ, pλ are probability densities, parametrized by θ, λ, respectively. We think of pθ as the distribution generating the data and of pλ as the RBM model distribution. When the context is clear, we occasionally write p(x) as a shorthand for p(X = x). The dimensions of the input data and the sample size are denoted\nby d and n, respectively. We use σ(·) to denote the sigmoid function\nσ(z) = 1\n1 + e−z . (1)"
    }, {
      "heading" : "2 Problem Setup",
      "text" : "Let X ∈ {0, 1}d, Y ∈ {0, 1} be random variables. We refer to Y as the label of X. The pair (X,Y ) has a joint distribution, parametrized by θ and denoted by pθ(X,Y ), which is given by\npθ(X,Y ) = pθ(Y )pθ(X|Y ).\nThe joint distribution pθ(X,Y ) is not known to us, and neither are the marginals pθ(X), pθ(Y ). Let (x(1), y(1)), . . . , (x(n), y(n)) be n i.i.d samples from pθ(X,Y ). In unsupervised ensemble learning, we observe x(1), . . . , x(n) and the learning task is to recover y(1), . . . , y(n). In this application, the binary vector X = (X1, . . . , Xd)\nT contains the predictions of d classifiers or annotators on an instance, whose label Y is unobserved."
    }, {
      "heading" : "2.1 The Conditional Independence Model",
      "text" : "In their seminal paper, Dawid and Skene (1979), assumed that the conditional distribution pθ(X|Y ) factorizes, i.e.,\npθ(X|Y ) ≡ d∏ i=1 pθ(Xi|Y ). (2)\nEq. (2), also known as the conditional independence model, is depicted in Figure 1. It is fully parametrized by θ = ({ψi : i = 1, ..., d}, {ηi : i = 1, ..., d}, π), where\nψi = Pr(Xi = 1|Y = 1), ηi = Pr(Xi = 0|Y = 0), π = Pr(Y = 1).\nψi, ηi are often referred to as sensitivity and specificity, respectively. Under the interpretation of the Xi’s being classifiers, the sensitivity and specificity quantify the competence of the classifiers or annotators and the conditional independence assumption means that all d classifiers make independent errors.\nThe conditional independence model is often overly simplistic. In this manuscript we propose to apply deep learning techniques, specifically RBM-based DNNs, for unsupervised ensemble learning problems, where the conditional independence is not likely to hold. The following section gives essential background on RBMs, section 4 shows that a RBM with a single hidden node is equivalent to the conditional independence model, and section 5 presents our RBMbased DNN approach."
    }, {
      "heading" : "3 Restricted Boltzmann Machines",
      "text" : "A Restricted Boltzmann Machine (RBM) is an undirected bipartite graphical model, consisting of a set X of d visible binary random variables and a set H of m hidden binary random variables, arranged in two layers, which are fully connected to each other. An illustration of a RBM is depicted in Figure 2. A RBM is parametrized by λ = (W,a, b), where W is the weight matrix of\nthe connections between the visible and hidden units, and a, b are the bias vectors of the visible and hidden layers, respectively. Each configuration (X = x,H = h) of a RBM is associated with the following energy\nEλ(x, h) = −(aTx+ bTh+ xTWh) (3)\nwhich defines the probability of the configuration\npλ(X = x,H = h) = e−Eλ(x,h)\nZ , where Z ≡ ∑\nx,h e −Eλ(x,h) is the partition function. The bipartite structure of the RBM implies\nfactorial conditional probabilities pλ(X|H) = ∏ i pλ(Xi|H), pλ(H|X) = ∏ j pλ(Hj |X),\ngiven by\npλ(Xi = 1|H) = σ(ai +Wi.H) pλ(Hj = 1|X) = σ(bj +XTW.j),\nwhere σ(z) is the sigmoid function defined in equation (1), Wi. is the i-th row of W and W.j is its j-th column.\nGiven iid training data x(1), .., x(n) ∼ pθ(X), the RBM parameters λ = (W,a, b) are typically tuned to maximize the log-likelihood of the training data, where the likelihood that the RBM associates with a vector x is given by\npλ(X = x) = ∑ h pλ(X = x,H = h).\nA popular approach to learn the RBM parameters is via gradient-based optimization, where the gradients are approximated using contrastive divergence (Hinton et al., 2006; Bengio, 2009)."
    }, {
      "heading" : "4 RBM in the Conditional Independence Case",
      "text" : "In this section we show that given observed data x(1), . . . , x(n) ∈ {0, 1}d from the conditional independence model of Eq. (2), the posterior probabilities of the true, unknown labels y(1), . . . , y(n) can be consistently estimated via a RBM with a single hidden node.\nWe begin by showing that there is a bijective map from the parameters λ of a RBM with a single hidden node to the parameters θ of the conditional independence model, such that the joint distribution specified by the RBM is equivalent to that of the conditional independence model.\nLemma 4.1. The joint probability pλ(X = x,H = y) of a RBM with parameters λ = (a, b,W ) is equivalent to the joint probability pθ(X = x, Y = y) of a conditional independence model with parameters θ = ({ψi}, {ηi}, π) given by\nψi ≡ σ(ai +Wi), ηi ≡ 1− σ(ai) π ≡ ∑ x∈{0,1}d e aT x+b+xTW∑\nx∈{0,1}d ( eaT x + eaT x+b+xTW ) Furthermore, the map λ 7→ θ is a bijection.\nWe are now ready to prove the main result of this section, namely, that the posterior distribution of the true labels y(1), . . . , y(n) can be consistently estimated by a RBM with a single hidden node. To do so, we rely on a special case of a result proved by Chang (1996), that provides conditions under which the parameters of the conditional independence model are identifiable.\nLemma 4.2. Let x(1), ..., x(n) be observed data from the conditional independence model, specified by pθ. Assume that θ is such that for each i = 1, . . . , d, Xi is not independent of Y (i.e., each classifier is not just a random guess), and that d ≥ 3. Let λ̂MLE be a maximum likelihood parameter estimate of a RBM with a single hidden node. Then the RBM posterior probability pλ̂MLE(H = 1|X = x) converges to the true posterior pθ(Y = 1|X = x), as n→∞.\nRemark 4.3. The identifiability of the parameters is up to a single global 0/1 label flip. This means that one recovers either pθ(Y = y|X) or pθ(Y = 1−y|X). Assuming that on average, the Xi’s are more accurate than a random guess, this sign ambiguity can be resolved by comparing the predictions to the majority vote decision.\nRemark 4.4. Lemma 4.2 assumes that we found the MLE of the RBM parameters. Obtaining such a MLE is problematic for two main reasons. First, RBMs are typically trained to maximize a proxy for the likelihood, as the true likelihood is not tractable. Second, the RBM likelihood function is not concave, hence there are no guarantees that after training a RBM one obtains the maximum likelihood parameter λ̂MLE."
    }, {
      "heading" : "5 RBM-based Deep Neural Net",
      "text" : "In many practical settings, the variables X1, . . . , Xd are not conditionally independent. Fitting a conditionally independent model to such data may yield highly sub-optimal predictions for the true labels yi. To tackle this general case, we propose to train a RBM-based Deep Neural Net (DNN) and use it to estimate the posterior probabilities pθ(Y |X). In such a DNN, the hidden layer of each RBM is the input for the successive RBM. As suggested by Hinton et al. (2006), the RBMs are trained one at a time, bottom to top, i.e., the DNN is trained in a layer-wise fashion. Specifically, given training data x(1), . . . , x(n) ∈ {0, 1}d, we start by training the bottom RBM, and then obtain the first layer hidden representation of the data by sampling h(i) from the conditional RBM distribution pλ(H|X = x(i)). The vectors h(1), . . . , h(n) are then used as a training set for the second RBM and so on.\nIn the case considered in this manuscript, where the true label y is binary, the upper-most RBM in the DNN has a single hidden unit, from which the posterior probability pθ(Y |X) can be estimated. Such a DNN is depicted in Figure 3."
    }, {
      "heading" : "5.1 Motivation",
      "text" : "Deep learning algorithms have recently achieved state-of-the-art performance in a wide range of applications LeCun et al. (2015). While a rigorous theoretical understanding of deep nets is still lacking, many researchers believe that a key property in their success is their ability to disentangle factors of variation in the inputs; see for example Bengio et al. (2013), Tishby and Zaslavsky (2015), and Mehta and Schwab (2014). That is, as one moves through the net, the hidden units become less statistically dependent. We have seen in Section 4 that given a representation in which the units are independent conditional on the true label, a single node RBM gives a consistent estimation of the true label posterior probability. Propagating the data through several RBM layers can hence be seen as a processing of the data, which reduces the conditional dependence of the units while preserving most of the information on the true label Y . In Section 6 we will demonstrate cases where such decoupling does indeed happen in practice, i.e., although the original input variables Xi’s are not conditionally independent given the true label Y , after training, the units in the uppermost hidden layer are, remarkably,\napproximately conditionally independent. Thus, the assumptions of the conditional independence model apply (with respect to the uppermost hidden layer H last), and therefore one is able to consistently estimate the label posterior probability, Pr(Y |H last), as in Section 4.\nAnother motivation for using deep nets with several hidden layers for unsupervised ensemble learning is their rich expressive power. In our setting, we wish to approximate the posterior probability p(Y |X), which in general may be a complicated nonlinear function of X. When p(Y |X) cannot be accurately estimated by a RBM with a single hidden node (i.e., when the conditional independence assumption of Dawid and Skene does not hold), a better approximation may be obtained from a deeper network. Several works show that there exist functions that are significantly more efficiently represented by deeper networks, compared to shallower ones, where efficiency corresponds to the number of units. For example, Montufar et al. (2014) show that deep networks with piece-wise linear activations can represent functions with greater number of linear regions compared to shallow networks with the same number of units. In a recent work, Eldan and Shamir (2015) give an example for a radial function that can be efficiently computed by a 3-layer network, while requiring exponentially many units to be approximated accurately by a 2-layer network.\nFinally, we would like to emphasize that a RBM-based DNN is a discriminative model to estimate the posterior p(Y |X). In general, it may not correspond to any generative model Arora et al. (2015). Indeed, there is no guarantee that the marginal distributions implied by two adjacent RBMs match. Yet, it can be shown (see Appendix C) that stacking RBMs is a variational inference procedure assuming a specific class of data generation models. The nature of approximation of a top down generative model, where the data X is generated from a label Y , by a RBM-based DNN is explored in Appendix D."
    }, {
      "heading" : "5.2 Predicting the Label from a Trained DNN",
      "text" : "Given a trained DNN and a sample x ∼ pθ(X), the label y is estimated by propagating x through the network. Specifically, the units of each layer can be set by either (i) sampling from the conditional distribution given the layer below, i.e., hj ∼ pλ(hj |x), or (ii) by MAP estimate, setting each hidden unit hj = arg maxhj∈{0,1} pλ(hj |x). Since the first option is stochastic, one may propagate x through the net multiple times and average the outputs p(y|x) to obtain an approximation of E(Y |X = x). Experimentally, we found both options to be equally effective, while each option slightly outperforms the other in some cases."
    }, {
      "heading" : "5.3 Choosing the DNN Architecture",
      "text" : "The specific DNN architecture (i.e., number and sizes of layers) might have a dramatic effect on the quality of predictions. To determine the number of units in each layer we employed the following procedure: we first train a RBM with d hidden units. Next, we compute the singular value decomposition of the weight matrix W , and determine its rank (i.e., the number of sufficiently large singular values). Given that the rank is some m ≤ d, we re-train the RBM, setting the number of hidden units to be m. If m > 1, we add another layer on top of the current layer, and proceed recursively. The process stops when m = 1, so that the last layer of the DNN contains a single node. We refer to this method as the SVD approach. In our experiments, as a rule of thumb, we set m to be the minimal number of singular values (in descending order) whose cumulative sum is at least 95% of the total sum.\nThis method takes advantage of the co-adaptation of hidden units, which is a well known phenomenon in RBM training (see, for example, Hinton et al. (2012)). The term co-adaptation describes a situation where several hidden units tend to behave very similarly; this implies that the rank of the weight matrix might be small, although the number of hidden units may be larger."
    }, {
      "heading" : "6 Experimental Results",
      "text" : "In this section we compare the performance of the proposed DNN approach to several other approaches, and report experimental results obtained on four simulated data sets and eight real world data sets, from two different domains. All our datasets, as well as the scripts reproducing the reported results are publicly available at https://github.com/ushaham/RBMpaper. 1.\nSpecifically, we compare between the following unsupervised ensemble methods:\n• Vote. Majority voting, which is the maximum likelihood prediction, assuming that all classifiers are conditionally independent and have the same accuracy.\n• DS. Approximate maximum likelihood predictions under the Dawid and Skene model. Specifically, we use Spectral Meta Learner (Parisi et al., 2014), and Restricted Likelihood (Jaffe et al., 2014).\n1 Our scripts are based on the publicly available code in Hinton’s website http://www.cs.toronto.edu/\n~hinton/MatlabForSciencePaper.html.\n• CUBAM The method of Welinder et al. (2010), which assumes conditional independence, but allows the accuracy of each classifier to vary across different regions of the input domain.\n• L-SML Latent SML (Jaffe et al., 2015). This method relaxes the conditional independence assumption to a depth 2 tree model.\n• DNN The approach presented in this manuscript, with the depth and number of hidden units in each layer determined by the SVD approach, described in Section 5.3.\nFollowing Jaffe et al. (2015), the performance measure we chose is the balanced accuracy, given by ∑\nI{true label is 0 and predicted label is 0} 2 ∑ I{true label is 0}\n+\n∑ I{true label is 1 and predicted label is 1}\n2 ∑ I{true label is 1} ,\nwhere I{·} is the indicator function."
    }, {
      "heading" : "6.1 Simulated Datasets",
      "text" : "In this experiment we carefully generated four synthetic datasets, in order to demonstrate the performance of the DNN approach in several specific scenarios. In all four datasets the observed data is a n × d binary matrix, with input dimension d = 15 and sample size n = 10, 000. A detailed description of the datasets generation process is given in Appendix E.1.\n• CondInd A dataset where the conditional independence holds, and 10 of the 15 classifiers are in fact random guess.\n• Tree15-3-1 A dataset generated from a depth-2 tree with layer sizes 1,3,15. Every node in the intermediate layer is connected to five nodes in the bottom layer. This dataset is generated from the model considered by L-SML, and does not satisfy the conditional independence assumption, as is shown in Figure 6.\n• LayeredGraph15-5-5-1 A dataset generated from a depth-3 layered graph, with layer sizes 1,5,5,15. In this case, the conditional independence assumption does not hold, although in practice the amount of dependence in the data is not high (see Figure 11).\n• TruncatedGaussian. Here X = (1 + sign(Z))/2, where the r.v. Z follows a a mixture of two d-dimensional Gaussians with different means and same covariance matrix. The label Y indicates the specific Gaussian from which X is sampled. In this case, the data is highly dependent, as can be seen in Figure 11.\nThe results are summarized in Table 1. Along with the five unsupervised methods, the table also shows the accuracy of a supervised learner and the estimated accuracy of the Bayes-optimal\nclassifier. The supervised learner is a Multi Layer Perceptron (MLP) with two hidden layers of sizes 4 and 2, that was trained on a dataset with n = 10, 000 samples (independent of the test dataset). The Bayes-optimal approximated accuracy was computed on a sample of size 10, 000, with the true posterior probabilities of all 2d possible binary vectors estimated using a sample of size 106 from the corresponding model.\nOn all of the above datasets, the DNN always outperformed the majority vote rule and CUBAM. On the CondInd dataset, the DNN performs similarly to DS, and significantly better than the other methods. Despite being unsupervised, on this dataset both methods perform slightly better than the specific supervised learner we considered, and around the Bayes-optimal accuracy. The architecture determined by the SVD approach in this case is indeed a single RBM (with a single hidden node). The weight matrix of the RBM is shown in Figure 4, and corresponds to the fact that only the first five classifiers actually contain information about the true label in this dataset.\nFigure 5 shows the recovery of the true conditional independence model parameters {ψi, ηi} of a similar conditional independent dataset (however with no random guess classifiers) from a RBM with a single hidden node, using the map in Lemma 4.1.\nOn the Tree15-3-1 dataset, L-SML, which is tailored for data generated by a tree, outperforms the DNN. This result is expected, since it can be shown that the distribution of the bottom two layers of a tree cannot be parametrized as a RBM (see Appendix D). Still, the DNN performs significantly better than DS, CUBAM and majority vote, and not far from the supervised learner and the optimal Bayes classifier. Figure 6 shows the correlation matrix at the input and hidden layers, as well as the first layer weight matrix, demonstrating that the DNN captured the true data generation model. Consequently, the 3 hidden units are nearly conditionally uncorrelated given the label y.\nFigure 7 shows the cumulative proportion of the singular values on the condInd and Tree15-3-1 datasets, which explains the architecture determined by the SVD approach for both datasets.\nOn the LayeredGraph15-5-5-1 dataset, while outperforming the other methods, the DNN achieved accuracy close to the supervised learner and the Bayes optimal accuracy; however, the chosen DNN architecture is different from the one of the true data generation model.\nThe conditional independence assumption is strongly violated in the case of the TruncatedGaussian dataset. Here the DNN performs better than all other methods by a large margin."
    }, {
      "heading" : "6.2 Real-World Datasets",
      "text" : "In this section we experiment with two groups of datasets, from two different domains, as follows:\n• DREAM Three datasets from the DREAM mutation calling challenge Ewing et al. (2015); this challenge is an international effort to improve standard methods for identifying cancer-associated mutations and rearrangements in whole-genome sequencing data. The accuracy of current variant calling algorithms is not optimal due to sequencing errors, other experimental factors, parametric choices in each algorithm and preprocessing and\nfiltering decisions. Unsupervised ensemble learning of multiple variant callers is expected to provide more robust predictions. One of the goals of this challenge is to develop a state-of-the-art meta pipeline for somatic mutation detection, to output accurate as possible mutation calls associated with cancer. Specifically, we used three datasets, (S1, S2, S3) containing the predictions of classifiers that determine the presence or absence of of mutations in genome sequencing data. The data is available at (Ellrot, 2013). In S1, d = 124, n = 92, 362. In S2, d = 114, n = 70,561. In S3, d = 99, n = 78, 643.\n• Magic Forty datasets, which are constructed from the Magic dataset in the UCI repository, available at https://archive.ics.uci.edu/ml/datasets/MAGIC+Gamma+Telescope. This dataset contains n = 19, 020 instances with 11 attributes, which consists of physical measurements of gamma particles; the learning task is to classify each instance as background or high energy gamma rays. Each of the five datasets we constructed contains\nbinary predictions of d = 16 classifiers, obtained in the Weka machine learning software. The 16 classifiers belong to four groups: four random forest classifiers, three logistic trees classifiers, four SVM classifiers, and five naive Bayes classifiers. This setting is adopted from Jaffe et al. (2015). The group of SVM classifiers is highly correlated, as well as the group of Naive Bayes classifiers, as can be seen in Appendix E.2. Each of the forty datasets was obtained by predictions of the same classifiers, however trained on a different subset of the original Magic dataset (a random subset of size 500 each time).\nTable 2 shows the performance of the various methods on the DREAM datasets. As can be seen, the DNN and L-SML performs similarly on S1, while the former performs better on S3 and the latter on S2. The two methods outperform the majority vote rule, DS and CUBAM on all three datasets. Remarkably, the hidden representation on the S3 dataset is such that the units are perfectly uncorrelated, conditioned on the hidden label. This is shown in Figure 8.\nThe results on the Magic datasets are shown in Figure 9. On most of these datasets, the DNN outperforms all other methods, with a relatively large margin. On all forty datasets, the SVD approach yielded a 15-3-1 architecture.\nTo summarize our experiments, we observed that RBM-based DNN performs at least as well and often better than various other methods, on both simulated and real datasets, and that the SVD approach can serve as an effective tool for determination of the DNN architecture.\nWe remark that in our experiments, we observed that RBMs tend to be highly sensitive to hyper-parameter tuning (such as learning rate, momentum, regularization type and penalty), and these hyper-parameters need to be carefully tuned. To obtain a reasonable hyper-parameter setting we found it useful to apply the random configuration sampling procedure, proposed in (Bergstra and Bengio, 2012), and evaluate different models by average log-likelihood approximation, (see, for example, (Salakhutdinov and Murray, 2008) and the corresponding MATLAB scripts in (Salakhutdinov, 2010))."
    }, {
      "heading" : "7 Summary and Discussion",
      "text" : "We demonstrated how deep learning techniques can be used for unsupervised ensemble learning, and showed that the DNN approach proposed in this manuscript often performs at least as well and often better than state-of the art methods, especially when the conditional independence assumption made by Dawid and Skene (1979) does not hold.\nPossible directions for future research include extending the approach to multiclass problems, possible using Discrete RBMs Montúfar and Morton (2013), theoretical analysis of the SVD approach, and information theoretic analysis of the de-correlation, while preserving label information, that occurs while propagating data through a RBM-based DNN."
    }, {
      "heading" : "Acknowledgements",
      "text" : "The authors would like to thank George Linderman, Alex Cloninger, Tingting Jiang, Raphy Coifman, Sahand Negahban, Andrew Barron, Alex Kovner, Shahar Kovalsky, Maria Angelica Cueto, Jason Morton, and Brend Strumfels for their help."
    }, {
      "heading" : "A Proof of Lemma 4.1",
      "text" : "Proof. We will define θ so that for every x, y, pθ(Xi = xi|Y = y) = pλ(Xi = xi|H = y) and pθ(Y = y) = pλ(H = y).\nSince the weight matrix W has dimension d × 1 in this case, it is a vector, which we will denote as w. Recall that\npλ(Xi = 1|H = y) = σ(ai + wiy),\nhence we define ψi ≡ σ(ai + wi)\nand ηi ≡ 1− σ(ai).\nFinally, recall that\npλ(H = 1) =\n∑ x∈{0,1}d e\n−Eλ(x,1)∑ x∈{0,1}d, h∈{0,1} e −Eλ(x,h)\n=\n∑ x∈{0,1}d e\naT x+b+xTw∑ x∈{0,1}d, e aT x + eaT x+b+xTw ,\nwhere Eλ is the energy function given in equation (3), hence we set π ≡ ∑ x∈{0,1}d e aT x+b+xTw∑\nx∈{0,1}d, ( eaT x + eaT x+b+xTw ) . (4) To see that the map λ 7→ θ is 1:1, note that ai uniquely determines ηi, hence (ai, wi) uniquely determine (ψi, ηi). Lastly, rearranging equation (4) we get\nπ ∑\nx∈{0,1}d\n( ea T x + ea T x+b+wT x ) = ∑\nx∈{0,1}d ea T x+b+wT x\n⇒π ∑\nx∈{0,1}d ea T x = (1− π)eb ∑ x∈{0,1}d ea T x+wT x\n⇒eb = π 1− π\n∑ x∈{0,1}d e\naT x∑ x∈{0,1}d e aT x+wT x ,\nso that given (a,W ), π is uniquely determined by b. Showing that the map λ 7→ θ is a also subjective is straightforward. Hence it is a bijection."
    }, {
      "heading" : "B Proof of Lemma 4.2",
      "text" : "Proof. Since d ≥ 3 and for each i, Xi is not independent of Y , by Chang (1996), the parameter θ of the conditional independence model is identifiable. Since the map λ 7→ θ in Lemma 4.1\nis a bijection, there exists λ corresponding to θ, which is therefore identifiable as well. By the consistency property of the MLE (see, for example, (Casella and Berger, 2002)),\nlim n→∞ λ̂MLE = λ.\nSince pλ(H = 1|X) is continuous in θ, one obtains\npλ̂MLE(H = 1|X)→ pλ(H = 1|X).\nFinally, note that Lemma 4.1 implies, in particular, that under the map λ 7→ θ\npλ(H = 1|X) = pθ(Y = 1|X),\nwhich completes the proof."
    }, {
      "heading" : "C Stacking RBMs as a Variational Inference Procedure",
      "text" : "Variational inference is a common approach to tackle complicated probability estimation problems (see, for example, Bishop (2006); Fox and Roberts (2012), and a recent review Blei et al. (2016)). Specifically, let p be a target probability distribution that we want to approximate. In variational inference we define a family of approximate distributions D = {qα : α ∈ A}, and then perform optimization to find the member of D that is closest to p in Kullback-Leibler distance. A key idea is that the family D is flexible enough to contain a distribution close to p, yet simple enough to perform optimization over. For example, a popular choice is to take D as the collection of factorized distributions, i.e., of the form qα(X) = ∏ i qα(Xi). In this section, we motivate the use of RBM-based DNN by considering a specific data generation model, and showing that training a stack of RBMs on data generated by this model is in fact a variational inference procedure.\nThe generative model we consider is a two layer Deep Belief Network (DBN), which played an important role in the emergence of deep learning in 2006 Hinton et al. (2006). The DBN we consider generates data Y ∈ {0, 1}, H ∈ {0, 1}m, X ∈ {0, 1}d via the probability distribution\npθ(X,H, Y ) ≡ pθ1(X,H)pθ2(Y |H)\nwhere X,H form a RBM (parametrized by θ1). We observe data x(1) . . . x(n) from pθ(X) and our goal is to estimate the posterior pθ(y\n(i)|x(i)) for i = 1, . . . n. The posterior can be written as\npθ(Y |X) = Eh∼pθ1 (H|X)Pθ2(Y |H = h).\nCueto et al. (2010) showed that as long as m is not too large comparing to d, RBMs are locally identifiable, i.e., identifiable up to order and flips of hidden units (Jason Morton, personal communication). Therefore, when training a RBM with m hidden units on x(1) . . . x(n), by the consistency property of the MLE Casella and Berger (2002) the MLE θ̂1MLE will converge to\nthe true parameter θ1 as n → ∞. Hence, when n is large enough, the vectors h(i) obtained from the (trained) RBM are in fact samples from pθ1(H|X = x(i)).\nAt the next step, the vectors h(1) . . . h(n) are used to train a second RBM, with a single hidden node. Observe that in the data generation model considered in this section, pθ(H|Y ) does not factorize. The factorized distribution pλ(H|Y ) that minimizes KL(pθ2(H|Y )‖pλ(H|Y )) is given by\npλ(Hi|Y ) = pθ2(Hi|Y )\nBishop (2006) (Chapter 10). By Lemma 4.1, we know that the distribution pλ(H,Y ) = pθ(Y ) ∏ i pθ2(Hi|Y ) (5)\nis equivalent to a RBM. Finally, by Lemma 4.2, the distribution (5) is consistently estimated by a RBM trained on vectors h(1) . . . h(n), and is thus a variational inference procedure."
    }, {
      "heading" : "D Stacking RBMs as an Approximation for a Directed Top-",
      "text" : "Down Model\nAssume that the data is generated by a Markov chain Y → H → X, where Y ∈ {0, 1}, H ∈ {0, 1}m, X ∈ {0, 1}d. We further assume that the distributions pθ(X|H), pθ(H|Y ) factorize, i.e.,\npθ(X|H) = d∏ i=1 Pr(Xi|H) (6)\nand\npθ(H|Y ) = m∏ i=1 Pr(Hi|Y ), (7)\nand are given by RBM-like conditional distributions, i.e.,\npθ(Xi = 1|H) = σ (ai +Wi,·H) (8)\nand pθ(Hi = 1|Y ) = σ (bi + Ui,·Y ) . (9)\nHence the corresponding data generation probability is parametrized by θ = (π, a, b,W,U), where π = Pr(Y = 1).\nThis data generation process is depicted in Figure 10. The posterior probabilities pθ(Y |X) are given by\npθ(Y |X) = ∑\nH∈{0,1}m pθ(Y |H)pθ(H|X)\n= Eh∼pθ(H|X)pθ(Y |H = h).\nBy Section 4, we know that pθ(H,Y ) is equivalent to a RBM. Therefore, to accurately estimate the posterior, it suffices to approximate pθ(H|X).\nUnder the data generation model described in Figure 10 and equations (6)-(9), it is evident that the joint distribution pθ(X,H) cannot be parametrized as a RBM; indeed, pθ(H|X) does not factorize. Hence, training a RBM on samples from pθ(X), is a mean field approximation of pθ(H|X). The form of pθ(X,H) is shown in the following lemma.\nLemma D.1. Under the data generation model described in Figure 10 and equations (6)-(9), the joint distribution pθ(X,H) is given by\npθ(X,H) = exp ( aTX +XTWH + bTH ) Z(H)\nwhere\nZ(H) = 1∑\nX∈{0,1}d exp (a TX +XTWH)\n× ∑\nY ∈{0,1}\npθ(Y ) exp(H TUY )∑\nH′ exp (b TH ′ +H ′TUY )\nProof. By definition,\npθ(X,H) = ∑\nY ∈{0,1}\npθ(X,H, Y )\n= ∑\nY ∈{0,1}\np(Y )pθ(H|Y )p(X|H) (10)\nWriting\npθ(X|H) = exp\n( aTX +XTWH )∑ X′∈{0,1}d exp (a TX ′ +X ′TWH)\nand similarly\npθ(H|Y ) = exp\n( bTH +HTUY )∑ H′∈{0,1}m exp (b TH ′ +H ′TUY ) ,\nwe obtain\npθ(X|H)pθ(H|Y ) = exp ( aTX +XTWH + bTH +HTUY ) ( ∑\nX′ exp (aTX ′ +X ′TWH)) ( ∑ H′ exp (b TH ′ +H ′TUY )) . (11)\nPlugging equation (11) in equation (10) we get pθ(X,H) = exp ( aTX +XTWH + bTH ) × 1∑\nX′ exp (aTX ′ +X ′TWH) × ∑\nY ∈{0,1}\npθ(Y ) exp(H TUY )∑\nH′ exp (b TH ′ +H ′TUY )\nFrom lemma D.1 we see that pθ(H|X) is close to be factorizable if Z(H) is a approximately a log-linear function of H and pθ(X) is approximately a log-linear function of X."
    }, {
      "heading" : "E Datasets used for our experiments",
      "text" : "E.1 Simulated Dataset Generation Details\n• CondInd: the label Y was sampled from a Bernoulli(0.5) distribution; The specificity ηi and sensitivity ψi of the variables Xi, i = 1 . . . 5 were sampled uniformly from [0.5, 1]. The other ten Xi’s were random guesses, i.e., had specificity = sensitivity = 0.5.\n• Tree15-3-1: the label Y was sampled from a Bernoulli(0.5) distribution; each node in the intermediate and layer was generated from his parent with specificity and sensitivity sampled uniformly from [0.8, 1], and in the bottom layer with specificity and sensitivity sampled uniformly from [0.6, 1].\n• LayeredGraph15-5-5-1: Data is generated from a Layered Graph with four layers of dimensions 1,5,5,15, starting at the true label Y . Each layer in the graph is generated from the above layer, and the graph has sparse connectivity (about 30% of the edges exist). For every node i and parent j we sample specificity ψij and sensitivity ηij uniformly. Finally, the value at each node was calculated as the weighted sum of the probabilities of the node being 1 given the values of the nodes in the preceding layer, normalized by the sum over the edges. The label Y was sampled from a Bernoulli(0.5) distribution.\n• TruncatedGaussian: the label Y was sampled from a Bernoulli(0.5) distribution. One Gaussian had mean vector µ1 were each of the 15 coordinates was sampled uniformly. The other Gaussian had mean vector µ2 = −µ1. Both Gaussians had identical covariance matrix, with off diagonal entries of 0.5 and diagonal entries of 1.\nE.2 The Magic Datasets\nAn example for the correlation matrix of the 16 classifiers given the 0 class can be seen in Figure 12."
    } ],
    "references" : [ ],
    "referenceMentions" : [ ],
    "year" : 2016,
    "abstractText" : "We show how deep learning methods can be applied in the context of crowdsourcing and unsupervised ensemble learning. First, we prove that the popular model of Dawid and Skene, which assumes that all classifiers are conditionally independent, is equivalent to a Restricted Boltzmann Machine (RBM) with a single hidden node. Hence, under this model, the posterior probabilities of the true labels can be instead estimated via a trained RBM. Next, to address the more general case, where classifiers may strongly violate the conditional independence assumption, we propose to apply RBM-based Deep Neural Net (DNN). Experimental results on various simulated and real-world datasets demonstrate that our proposed DNN approach outperforms other state-of-the-art methods, in particular when the data violates the conditional independence assumption.",
    "creator" : "LaTeX with hyperref package"
  }
}