{
  "name" : "1610.06603.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Combinatorial Multi-Armed Bandit with General Reward Functions",
    "authors" : [ "Wei Chen", "Wei Hu", "Fu Li", "Jian Li", "Yu Liu", "Pinyan Lu" ],
    "emails" : [ "weic@microsoft.com.", "huwei@cs.princeton.edu.", "fuli.theory.research@gmail.com.", "lapordge@gmail.com." ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n61 0.\n06 60\n3v 3\n[ cs\n.L G\n] 1\nF eb\n2 01\n√ T )\ndistribution-independent regret, where T is the time horizon. We apply our results to the K-MAX problem and expected utility maximization problems. In particular, for K-MAX, we provide the first polynomial-time approximation scheme (PTAS) for its offline problem, and give the first Õ( √ T ) bound on the (1 − ǫ)approximation regret of its online problem, for any ǫ > 0."
    }, {
      "heading" : "1 Introduction",
      "text" : "Stochastic multi-armed bandit (MAB) is a classical online learning problem typically specified as a player againstm machines or arms. Each arm, when pulled, generates a random reward following an unknown distribution. The task of the player is to select one arm to pull in each round based on the historical rewards she collected, and the goal is to collect cumulative reward over multiple rounds as much as possible. In this paper, unless otherwise specified, we use MAB to refer to stochastic MAB.\nMAB problem demonstrates the key tradeoff between exploration and exploitation: whether the player should stick to the choice that performs the best so far, or should try some less explored alternatives that may provide better rewards. The performance measure of an MAB strategy is its cumulative regret, which is defined as the difference between the cumulative reward obtained by always playing the arm with the largest expected reward and the cumulative reward achieved by the learning strategy. MAB and its variants have been extensively studied in the literature, with classical results such as tight Θ(logT ) distribution-dependent and Θ( √ T ) distribution-independent upper and lower bounds on the regret in T rounds [19, 2, 1].\nAn important extension to the classical MAB problem is combinatorial multi-armed bandit (CMAB). In CMAB, the player selects not just one arm in each round, but a subset of arms or a combinatorial\n∗Microsoft Research, email: weic@microsoft.com. The authors are listed in alphabetical order. †Princeton University, email: huwei@cs.princeton.edu. ‡The University of Texas at Austin, email: fuli.theory.research@gmail.com. §Tsinghua University, email: lapordge@gmail.com. ¶Tsinghua University, email: liuyujyyz@gmail.com. ‖Shanghai University of Finance and Economics, email: lu.pinyan@mail.shufe.edu.cn.\n30th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain.\nobject in general, referred to as a super arm, which collectively provides a random reward to the player. The reward depends on the outcomes from the selected arms. The player may observe partial feedbacks from the selected arms to help her in decision making. CMAB has wide applications in online advertising, online recommendation, wireless routing, dynamic channel allocations, etc., because in all these settings the action unit is a combinatorial object (e.g. a set of advertisements, a set of recommended items, a route in a wireless network, and an allocation between channels and users), and the reward depends on unknown stochastic behaviors (e.g. users’ click through behaviors, wireless transmission quality, etc.). Therefore CMAB has attracted a lot of attention in online learning research in recent years [12, 8, 22, 15, 7, 16, 18, 17, 23, 9].\nMost of these studies focus on linear reward functions, for which the expected reward for playing a super arm is a linear combination of the expected outcomes from the constituent base arms. Even for studies that do generalize to non-linear reward functions, they typically still assume that the expected reward for choosing a super arm is a function of the expected outcomes from the constituent base arms in this super arm [8, 17]. However, many natural reward functions do not satisfy this property. For example, for the functionmax(), which takes a group of variables and outputs the maximum one among them, its expectation depends on the full distributions of the input random variables, not just their means. Function max() and its variants underly many applications. As an illustrative example, we consider the following scenario in auctions: the auctioneer is repeatedly selling an item to m bidders; in each round the auctioneer selects K bidders to bid; each of the K bidders independently draws her bid from her private valuation distribution and submits the bid; the auctioneer uses the first-price auction to determine the winner and collects the largest bid as the payment.1 The goal of the auctioneer is to gain as high cumulative payments as possible. We refer to this problem as the K-MAX bandit problem, which cannot be effectively solved in the existing CMAB framework.\nBeyond the K-MAX problem, many expected utility maximization (EUM) problems are studied in stochastic optimization literature [27, 20, 21, 4]. The problem can be formulated as maximizing E[u( ∑\ni∈S Xi)] among all feasible sets S, where Xi’s are independent random variables and u(·) is a utility function. For example, Xi could be the random delay of edge ei in a routing graph, S is a routing path in the graph, and the objective is maximizing the utility obtained from any routing path, and typically the shorter the delay, the larger the utility. The utility function u(·) is typically nonlinear to model risk-averse or risk-prone behaviors of users (e.g. a concave utility function is often used to model risk-averse behaviors). The non-linear utility function makes the objective function much more complicated: in particular, it is no longer a function of the means of the underlying random variables Xi’s. When the distributions of Xi’s are unknown, we can turn EUM into an online learning problem where the distributions of Xi’s need to be learned over time from online feedbacks, and we want to maximize the cumulative reward in the learning process. Again, this is not covered by the existing CMAB framework since only learning the means of Xi’s is not enough.\nIn this paper, we generalize the existing CMAB framework with semi-bandit feedbacks to handle general reward functions, where the expected reward for playing a super arm may depend more than just the means of the base arms, and the outcome distribution of a base arm can be arbitrary. This generalization is non-trivial, because almost all previous works on CMAB rely on estimating the expected outcomes from base arms, while in our case, we need an estimation method and an analytical tool to deal with the whole distribution, not just its mean. To this end, we turn the problem into estimating the cumulative distribution function (CDF) of each arm’s outcome distribution. We use stochastically dominant confidence bound (SDCB) to obtain a distribution that stochastically dominates the true distribution with high probability, and hence we also name our algorithm SDCB. We are able to show O(log T ) distribution-dependent and Õ( √ T ) distribution-independent regret bounds in T rounds. Furthermore, we propose a more efficient algorithm called Lazy-SDCB, which first executes a discretization step and then applies SDCB on the discretized problem. We show that Lazy-SDCB also achieves Õ( √ T ) distribution-independent regret bound. Our regret bounds are tight with respect to their dependencies on T (up to a logarithmic factor for distribution-independent bounds). To make our scheme work, we make a few reasonable assumptions, including boundedness, monotonicity and Lipschitz-continuity2 of the reward function, and independence among base arms. We apply our algorithms to the K-MAX and EUM problems, and provide efficient solutions with\n1We understand that the first-price auction is not truthful, but this example is only for illustrative purpose for the max() function.\n2The Lipschitz-continuity assumption is only made for Lazy-SDCB. See Section 4.\nconcrete regret bounds. Along the way, we also provide the first polynomial time approximation scheme (PTAS) for the offline K-MAX problem, which is formulated as maximizing E[maxi∈S Xi] subject to a cardinality constraint |S| ≤ K , where Xi’s are independent nonnegative random variables.\nTo summarize, our contributions include: (a) generalizing the CMAB framework to allow a general reward function whose expectation may depend on the entire distributions of the input random variables; (b) proposing the SDCB algorithm to achieve efficient learning in this framework with near-optimal regret bounds, even for arbitrary outcome distributions; (c) giving the first PTAS for the offline K-MAX problem. Our general framework treats any offline stochastic optimization algorithm as an oracle, and effectively integrates it into the online learning framework.\nRelated Work. As already mentioned, most relevant to our work are studies on CMAB frameworks, among which [12, 16, 18, 9] focus on linear reward functions while [8, 17] look into nonlinear reward functions. In particular, Chen et al. [8] look at general non-linear reward functions and Kveton et al. [17] consider specific non-linear reward functions in a conjunctive or disjunctive form, but both papers require that the expected reward of playing a super arm is determined by the expected outcomes from base arms.\nThe only work in combinatorial bandits we are aware of that does not require the above assumption on the expected reward is [15], which is based on a general Thompson sampling framework. However, they assume that the joint distribution of base arm outcomes is from a known parametric family within known likelihood function and only the parameters are unknown. They also assume the parameter space to be finite. In contrast, our general case is non-parametric, where we allow arbitrary bounded distributions. Although in our known finite support case the distribution can be parametrized by probabilities on all supported points, our parameter space is continuous. Moreover, it is unclear how to efficiently compute posteriors in their algorithm, and their regret bounds depend on complicated problem-dependent coefficients which may be very large for many combinatorial problems. They also provide a result on the K-MAX problem, but they only consider Bernoulli outcomes from base arms, much simpler than our case where general distributions are allowed.\nThere are extensive studies on the classical MAB problem, for which we refer to a survey by Bubeck and Cesa-Bianchi [5]. There are also some studies on adversarial combinatorial bandits, e.g. [26, 6]. Although it bears conceptual similarities with stochastic CMAB, the techniques used are different.\nExpected utility maximization (EUM) encompasses a large class of stochastic optimization problems and has been well studied (e.g. [27, 20, 21, 4]). To the best of our knowledge, we are the first to study the online learning version of these problems, and we provide a general solution to systematically address all these problems as long as there is an available offline (approximation) algorithm. The K-MAX problem may be traced back to [13], where Goel et al. provide a constant approximation algorithm to a generalized version in which the objective is to choose a subset S of cost at most K and maximize the expectation of a certain knapsack profit."
    }, {
      "heading" : "2 Setup and Notation",
      "text" : "Problem Formulation. We model a combinatorial multi-armed bandit (CMAB) problem as a tuple (E,F , D,R), where E = [m] = {1, 2, . . . ,m} is a set of m (base) arms, F ⊆ 2E is a set of subsets of E, D is a probability distribution over [0, 1]m, and R is a reward function defined on [0, 1]m × F . The arms produce stochastic outcomes X = (X1, X2, . . . , Xm) drawn from distribution D, where the i-th entry Xi is the outcome from the i-th arm. Each feasible subset of arms S ∈ F is called a super arm. Under a realization of outcomes x = (x1, . . . , xm), the player receives a reward R(x, S) when she chooses the super arm S to play. Without loss of generality, we assume the reward value to be nonnegative. Let K = maxS∈F |S| be the maximum size of any super arm. Let X(1), X(2), . . . be an i.i.d. sequence of random vectors drawn from D, where X(t) = (X\n(t) 1 , . . . , X (t) m ) is the outcome vector generated in the t-th round. In the t-th round, the player chooses a super arm St ∈ F to play, and then the outcomes from all arms in St, i.e., {X(t)i | i ∈ St}, are revealed to the player. According to the definition of the reward function, the reward value in the t-th round is R(X(t), St). The expected reward for choosing a super arm S in any round is denoted by rD(S) = EX∼D[R(X,S)].\nWe also assume that for a fixed super arm S ∈ F , the reward R(x, S) only depends on the revealed outcomes xS = (xi)i∈S . Therefore, we can alternatively express R(x, S) as RS(xS), where RS is a function defined on [0, 1]S .3\nA learning algorithm A for the CMAB problem selects which super arm to play in each round based on the revealed outcomes in all previous rounds. Let SAt be the super arm selected by A in the t-th round.4 The goal is to maximize the expected cumulative reward in T rounds, which is E [\n∑T t=1 R(X (t), SAt ) ] = ∑T t=1 E [ rD(S A t ) ] . Note that when the underlying distribution D is\nknown, the optimal algorithm A∗ chooses the optimal super arm S∗ = argmaxS∈F{rD(S)} in every round. The quality of an algorithm A is measured by its regret in T rounds, which is the difference between the expected cumulative reward of the optimal algorithm A∗ and that of A:\nRegAD(T ) = T · rD(S∗)− T ∑\nt=1\nE [ rD(S A t ) ] .\nFor some CMAB problem instances, the optimal super arm S∗ may be computationally hard to find even when the distribution D is known, but efficient approximation algorithms may exist, i.e., an α-approximate (0 < α ≤ 1) solution S′ ∈ F which satisfies rD(S′) ≥ α · maxS∈F{rD(S)} can be efficiently found given D as input. We will provide the exact formulation of our requirement on such an α-approximation computation oracle shortly. In such cases, it is not fair to compare a CMAB algorithm A with the optimal algorithm A∗ which always chooses the optimal super arm S∗. Instead, we define the α-approximation regret of an algorithm A as\nRegAD,α(T ) = T · α · rD(S∗)− T ∑\nt=1\nE [ rD(S A t ) ] .\nAs mentioned, almost all previous work on CMAB requires that the expected reward rD(S) of a super arm S depends only on the expectation vector µ = (µ1, . . . , µm) of outcomes, where µi = EX∼D[Xi]. This is a strong restriction that cannot be satisfied by a general nonlinear function RS and a general distribution D. The main motivation of this work is to remove this restriction.\nAssumptions. Throughout this paper, we make several assumptions on the outcome distribution D and the reward function R.\nAssumption 1 (Independent outcomes from arms). The outcomes from all m arms are mutually independent, i.e., for X ∼ D, X1, X2, . . . , Xm are mutually independent. We write D as D = D1 ×D2 × · · · ×Dm, where Di is the distribution of Xi.\nWe remark that the above independence assumption is also made for past studies on the offline EUM and K-MAX problems [27, 20, 21, 4, 13], so it is not an extra assumption for the online learning case.\nAssumption 2 (Bounded reward value). There exists M > 0 such that for any x ∈ [0, 1]m and any S ∈ F , we have 0 ≤ R(x, S) ≤ M . Assumption 3 (Monotone reward function). If two vectors x, x′ ∈ [0, 1]m satisfy xi ≤ x′i (∀i ∈ [m]), then for any S ∈ F , we have R(x, S) ≤ R(x′, S).\nComputation Oracle for Discrete Distributions with Finite Supports. We require that there exists an α-approximation computation oracle (0 < α ≤ 1) for maximizing rD(S), when each Di (i ∈ [m]) has a finite support. In this case, Di can be fully described by a finite set of numbers (i.e., its support {vi,1, vi,2, . . . , vi,si} and the values of its cumulative distribution function (CDF) Fi on the supported points: Fi(vi,j) = PrXi∼Di [Xi ≤ vi,j ] (j ∈ [si])). The oracle takes such a representation of D as input, and can output a super arm S′ = Oracle(D) ∈ F such that rD(S′) ≥ α ·maxS∈F{rD(S)}."
    }, {
      "heading" : "3 SDCB Algorithm",
      "text" : "3[0, 1]S is isomorphic to [0, 1]|S|; the coordinates in [0, 1]S are indexed by elements in S. 4Note that SAt may be random due to the random outcomes in previous rounds and the possible randomness\nused by A.\nAlgorithm 1 SDCB (Stochastically dominant confidence bound) 1: Throughout the algorithm, for each arm i ∈ [m], maintain: (i) a counter Ti which stores the\nnumber of times arm i has been played so far, and (ii) the empirical distribution D̂i of the observed outcomes from arm i so far, which is represented by its CDF F̂i\n2: // Initialization 3: for i = 1 to m do 4: // Action in the i-th round 5: Play a super arm Si that contains arm i 6: Update Tj and F̂j for each j ∈ Si 7: end for\n8: for t = m+ 1,m+ 2, . . . do 9: // Action in the t-th round\n10: For each i ∈ [m], let Di be a distribution whose CDF Fi is\nFi(x) =\n{ max{F̂i(x)− √\n3 ln t 2Ti , 0}, 0 ≤ x < 1 1, x = 1\n11: Play the super arm St ← Oracle(D), where D = D1 ×D2 × · · · ×Dm 12: Update Tj and F̂j for each j ∈ St 13: end for\nWe present our algorithm stochastically dominant confidence bound (SDCB) in Algorithm 1. Throughout the algorithm, we store, in a variable Ti, the number of times the outcomes from arm i are observed so far. We also maintain the empirical distribution D̂i of the observed outcomes from arm i so far, which can be represented by its CDF F̂i: for x ∈ [0, 1], the value of F̂i(x) is just the fraction of the observed outcomes from arm i that are no larger than x. Note that F̂i is always a step function which has “jumps” at the points that are observed outcomes from arm i. Therefore it suffices to store these discrete points as well as the values of F̂i at these points in order to store the whole function F̂i. Similarly, the later computation of stochastically dominant CDF Fi (line 10) only requires computation at these points, and the input to the offline oracle only needs to provide these points and corresponding CDF values (line 11).\nThe algorithm starts with m initialization rounds in which each arm is played at least once5 (lines 2- 7). In the t-th round (t > m), the algorithm consists of three steps. First, it calculates for each i ∈ [m] a distribution Di whose CDF Fi is obtained by lowering the CDF F̂i (line 10). The second step is to call the α-approximation oracle with the newly constructed distribution D = D1 × · · · ×Dm as input (line 11), and thus the super arm St output by the oracle satisfies rD(St) ≥ α·maxS∈F{rD(S)}. Finally, the algorithm chooses the super arm St to play, observes the outcomes from all arms in St, and updates Tj’s and F̂j ’s accordingly for each j ∈ St. The idea behind our algorithm is the optimism in the face of uncertainty principle, which is the key principle behind UCB-type algorithms. Our algorithm ensures that with high probability we have Fi(x) ≤ Fi(x) simultaneously for all i ∈ [m] and all x ∈ [0, 1], where Fi is the CDF of the outcome distribution Di. This means that each Di has first-order stochastic dominance over Di.6 Then from the monotonicity property of R(x, S) (Assumption 3) we know that rD(S) ≥ rD(S) holds for all S ∈ F with high probability. Therefore D provides an “optimistic” estimation on the expected reward from each super arm.\nRegret Bounds. We prove O(log T ) distribution-dependent and O( √ T log T ) distributionindependent upper bounds on the regret of SDCB (Algorithm 1).\n5Without loss of generality, we assume that each arm i ∈ [m] is contained in at least one super arm. 6We remark that while Fi(x) is a numerical lower confidence bound on Fi(x) for all x ∈ [0, 1], at the\ndistribution level, Di serves as a “stochastically dominant (upper) confidence bound” on Di.\nWe call a super arm S bad if rD(S) < α · rD(S∗). For each super arm S, we define ∆S = max{α · rD(S∗)− rD(S), 0}.\nLet FB = {S ∈ F | ∆S > 0}, which is the set of all bad super arms. Let EB ⊆ [m] be the set of arms that are contained in at least one bad super arm. For each i ∈ EB, we define\n∆i,min = min{∆S | S ∈ FB, i ∈ S}. Recall that M is an upper bound on the reward value (Assumption 2) and K = maxS∈F |S|. Theorem 1. A distribution-dependent upper bound on the α-approximation regret of SDCB (Algorithm 1) in T rounds is\nM2K ∑\ni∈EB\n2136\n∆i,min lnT +\n(\nπ2\n3 + 1\n)\nαMm,\nand a distribution-independent upper bound is\n93M √ mKT lnT + ( π2\n3 + 1\n)\nαMm.\nThe proof of Theorem 1 is given in Appendix A.1. The main idea is to reduce our analysis on general reward functions satisfying Assumptions 1-3 to the one in [18] that deals with the summation reward functionR(x, S) = ∑\ni∈S xi. Our analysis relies on the Dvoretzky-Kiefer-Wolfowitz inequality [10, 24], which gives a uniform concentration bound on the empirical CDF of a distribution.\nApplying Our Algorithm to the Previous CMAB Framework. Although our focus is on general reward functions, we note that when SDCB is applied to the previous CMAB framework where the expected reward depends only on the means of the random variables, it can achieve the same regret bounds as the previous combinatorial upper confidence bound (CUCB) algorithm in [8, 18].\nLet µi = EX∼D[Xi] be arm i’s mean outcome. In each round CUCB calculates (for each arm i) an upper confidence bound µ̄i on µi, with the essential property that µi ≤ µ̄i ≤ µi+Λi holds with high probability, for some Λi > 0. In SDCB, we use Di as a stochastically dominant confidence bound of Di. We can show that µi ≤ EYi∼Di [Yi] ≤ µi + Λi holds with high probability, with the same interval length Λi as in CUCB. (The proof is given in Appendix A.2.) Hence, the analysis in [8, 18] can be applied to SDCB, resulting in the same regret bounds.We further remark that in this case we do not need the three assumptions stated in Section 2 (in particular the independence assumption on Xi’s): the summation reward case just works as in [18] and the nonlinear reward case relies on the properties of monotonicity and bounded smoothness used in [8]."
    }, {
      "heading" : "4 Improved SDCB Algorithm by Discretization",
      "text" : "In Section 3, we have shown that our algorithm SDCB achieves near-optimal regret bounds. However, that algorithm might suffer from large running time and memory usage. Note that, in the t-th round, an arm i might have been observed t− 1 times already, and it is possible that all the observed values from arm i are different (e.g., when arm i’s outcome distribution Di is continuous). In such case, it takes Θ(t) space to store the empirical CDF F̂i of the observed outcomes from arm i, and both calculating the stochastically dominant CDF Fi and updating F̂i take Θ(t) time. Therefore, the worst-case space usage of SDCB in T rounds is Θ(T ), and the worst-case running time is Θ(T 2) (ignoring the dependence on m and K); here we do not count the time and space used by the offline computation oracle.\nIn this section, we propose an improved algorithm Lazy-SDCB which reduces the worst-case memory usage and running time to O( √ T ) andO(T 3/2), respectively, while preserving the O( √ T logT ) distribution-independent regret bound. To this end, we need an additional assumption on the reward function:\nAssumption 4 (Lipschitz-continuous reward function). There exists C > 0 such that for any S ∈ F and any x, x′ ∈ [0, 1]m, we have |R(x, S)−R(x′, S)| ≤ C‖xS − x′S‖1, where ‖xS − x′S‖1 = ∑\ni∈S |xi − x′i|.\nAlgorithm 2 Lazy-SDCB with known time horizon Input: time horizon T\n1: s ← ⌈ √ T ⌉ 2: Ij ← { [0, 1s ], j = 1\n( j−1s , j s ], j = 2, . . . , s\n3: Invoke SDCB (Algorithm 1) for T rounds, with the following change: whenever observing an outcome x (from any arm), find j ∈ [s] such that x ∈ Ij , and regard this outcome as js\nAlgorithm 3 Lazy-SDCB without knowing the time horizon 1: q ← ⌈log2 m⌉ 2: In rounds 1, 2, . . . , 2q, invoke Algorithm 2 with input T = 2q\n3: for k = q, q + 1, q + 2, . . . do 4: In rounds 2k + 1, 2k + 2, . . . , 2k+1, invoke Algorithm 2 with input T = 2k 5: end for\nWe first describe the algorithm when the time horizon T is known in advance. The algorithm is summarized in Algorithm 2. We perform a discretization on the distribution D = D1 × · · · ×Dm to obtain a discrete distribution D̃ = D̃1 × · · · × D̃m such that (i) for X̃ ∼ D̃, X̃1, . . . , X̃m are also mutually independent, and (ii) every D̃i is supported on a set of equally-spaced values { 1s , 2s , . . . , 1}, where s is set to be ⌈ √ T ⌉. Specifically, we partition [0, 1] into s intervals: I1 = [0, 1s ], I2 = ( 1 s , 2 s ], . . . , Is−1 = ( s−2 s , s−1 s ], Is = ( s−1 s , 1], and define D̃i as\nPr X̃i∼D̃i [X̃i = j/s] = Pr Xi∼Di\n[Xi ∈ Ij ] , j = 1, . . . , s.\nFor the CMAB problem ([m],F , D,R), our algorithm “pretends” that the outcomes are drawn from D̃ instead of D, by replacing any outcome x ∈ Ij by js (∀j ∈ [s]), and then applies SDCB to the problem ([m],F , D̃, R). Since each D̃i has a known support { 1s , 2s , . . . , 1}, the algorithm only needs to maintain the number of occurrences of each support value in order to obtain the empirical CDF of all the observed outcomes from arm i. Therefore, all the operations in a round can be done using O(s) = O( √ T ) time and space, and the total time and space used by Lazy-SDCB are O(T 3/2) and O( √ T ), respectively.\nThe discretization parameter s in Algorithm 2 depends on the time horizon T , which is why Algorithm 2 has to know T in advance. We can use the doubling trick to avoid the dependency on T . We present such an algorithm (without knowing T ) in Algorithm 3. It is easy to see that Algorithm 3 has the same asymptotic time and space usages as Algorithm 2.\nRegret Bounds. We show that both Algorithm 2 and Algorithm 3 achieve O( √ T logT ) distribution-independent regret bounds. The full proofs are given in Appendix B. Recall that C is the coefficient in the Lipschitz condition in Assumption 4.\nTheorem 2. Suppose the time horizon T is known in advance. Then the α-approximation regret of Algorithm 2 in T rounds is at most\n93M √ mKT lnT + 2CK √ T + ( π2\n3 + 1\n)\nαMm.\nProof Sketch. The regret consists of two parts: (i) the regret for the discretized CMAB problem ([m],F , D̃, R), and (ii) the error due to discretization. We directly apply Theorem 1 for the first part. For the second part, a key step is to show |rD(S)− rD̃(S)| ≤ CK/s for all S ∈ F (see Appendix B.1).\nTheorem 3. For any time horizon T ≥ 2, the α-approximation regret of Algorithm 3 in T rounds is at most\n318M √ mKT lnT + 7CK √ T + 10αMm lnT."
    }, {
      "heading" : "5 Applications",
      "text" : "We describe the K-MAX problem and the class of expected utility maximization problems as applications of our general CMAB framework.\nThe K-MAX Problem. In this problem, the player is allowed to select at most K arms from the set of m arms in each round, and the reward is the maximum one among the outcomes from the selected arms. In other words, the set of feasible super arms is F = { S ⊆ [m] ∣ ∣ |S| ≤ K }\n, and the reward function is R(x, S) = maxi∈S xi. It is easy to verify that this reward function satisfies Assumptions 2, 3 and 4 with M = C = 1.\nNow we consider the corresponding offline K-MAX problem of selecting at most K arms from m independent arms, with the largest expected reward. It can be implied by a result in [14] that finding the exact optimal solution is NP-hard, so we resort to approximation algorithms. We can show, using submodularity, that a simple greedy algorithm can achieve a (1 − 1/e)-approximation. Furthermore, we give the first PTAS for this problem. Our PTAS can be generalized to constraints other than the cardinality constraint |S| ≤ K , including s-t simple paths, matchings, knapsacks, etc. The algorithms and corresponding proofs are given in Appendix C.\nTheorem 4. There exists a PTAS for the offline K-MAX problem. In other words, for any constant ǫ > 0, there is a polynomial-time (1− ǫ)-approximation algorithm for the offline K-MAX problem.\nWe thus can apply our SDCB algorithm to the K-MAX bandit problem and obtain O(log T ) distribution-dependent and Õ( √ T ) distribution-independent regret bounds according to Theorem 1, or can apply Lazy-SDCB to get Õ( √ T ) distribution-independent bound according to Theorem 2 or 3.\nStreeter and Golovin [26] study an online submodular maximization problem in the oblivious adversary model. In particular, their result can cover the stochastic K-MAX bandit problem as a special case, and an O(K √ mT logm) upper bound on the (1 − 1/e)-regret can be shown. While the techniques in [26] can only give a bound on the (1 − 1/e)-approximation regret for K-MAX, we can obtain the first Õ( √ T ) bound on the (1− ǫ)-approximation regret for any constant ǫ > 0, using our PTAS as the offline oracle. Even when we use the simple greedy algorithm as the oracle, our experiments show that SDCB performs significantly better than the algorithm in [26] (see Appendix D).\nExpected Utility Maximization. Our framework can also be applied to reward functions of the form R(x, S) = u( ∑\ni∈S xi), where u(·) is an increasing utility function. The corresponding offline problem is to maximize the expected utility E[u( ∑\ni∈S xi)] subject to a feasibility constraint S ∈ F . Note that if u is nonlinear, the expected utility may not be a function of the means of the arms in S. Following the celebrated von Neumann-Morgenstern expected utility theorem, nonlinear utility functions have been extensively used to capture risk-averse or risk-prone behaviors in economics (see e.g., [11]), while linear utility functions correspond to risk-neutrality.\nLi and Deshpande [20] obtain a PTAS for the expected utility maximization (EUM) problem for several classes of utility functions (including for example increasing concave functions which typically indicate risk-averseness), and a large class of feasibility constraints (including cardinality constraint, s-t simple paths, matchings, and knapsacks). Similar results for other utility functions and feasibility constraints can be found in [27, 21, 4]. In the online problem, we can apply our algorithms, using their PTASs as the offline oracle. Again, we can obtain the first tight regret bounds on the (1 − ǫ)-approximation regret for any ǫ > 0, for the class of online EUM problems."
    }, {
      "heading" : "Acknowledgments",
      "text" : "Wei Chen was supported in part by the National Natural Science Foundation of China (Grant No. 61433014). Jian Li and Yu Liu were supported in part by the National Basic Research Program of China grants 2015CB358700, 2011CBA00300, 2011CBA00301, and the National NSFC grants 61033001, 61361136003. The authors would like to thank Tor Lattimore for referring to us the DKW inequality."
    }, {
      "heading" : "Appendix",
      "text" : ""
    }, {
      "heading" : "A Missing Proofs from Section 3",
      "text" : ""
    }, {
      "heading" : "A.1 Proof of Theorem 1",
      "text" : "We present the proof of Theorem 1 in four steps. In Section A.1.1, we review the L1 distance between two distributions and present a property of it. In Section A.1.2, we review the DvoretzkyKiefer-Wolfowitz (DKW) inequality, which is a strong concentration result for empirical CDFs. In Section A.1.3, we prove some key technical lemmas. Then we complete the proof of Theorem 1 in Section A.1.4."
    }, {
      "heading" : "A.1.1 The L1 Distance between Two Probability Distributions",
      "text" : "For simplicity, we only consider discrete distributions with finite supports – this will be enough for our purpose.\nLet P be a probability distribution. For any x, let P (x) = PrX∼P [X = x]. We write P = P1×P2× · · · × Pn if the (multivariate) random variable X ∼ P can be written as X = (X1, X2, . . . , Xn), where X1, . . . , Xn are mutually independent and Xi ∼ Pi (∀i ∈ [n]). For two distributions P and Q, their L1 distance is defined as\nL1(P,Q) = ∑\nx\n|P (x)−Q(x)|,\nwhere the summation is taken over x ∈ supp(P ) ∪ supp(Q). The L1 distance has the following property. It is a folklore result and we provide a proof for completeness.\nLemma 1. Let P = P1×P2×· · ·×Pn andQ = Q1×Q2×· · ·×Qn be two probability distributions. Then we have\nL1(P,Q) ≤ n ∑\ni=1\nL1(Pi, Qi). (1)\nProof. We prove (1) by induction on n.\nWhen n = 2, we have\nL1(P,Q) = ∑\nx\n∑\ny\n|P (x, y)−Q(x, y)|\n= ∑\nx\n∑\ny\n|P1(x)P2(y)−Q1(x)Q2(y)|\n≤ ∑\nx\n∑\ny\n(|P1(x)P2(y)− P1(x)Q2(y)|+ |P1(x)Q2(y)−Q1(x)Q2(y)|)\n= ∑\nx\nP1(x) ∑\ny\n|P2(y)−Q2(y)|+ ∑\ny\nQ2(y) ∑\nx\n|P1(x)−Q1(x)|\n= 1 · L1(P2, Q2) + 1 · L1(P1, Q1)\n=\n2 ∑\ni=1\nL1(Pi, Qi).\nHere the summation is taken over x ∈ supp(P1) ∪ supp(Q1) and y ∈ supp(P2) ∪ supp(Q2). Suppose (1) is proved for n = k − 1 (k ≥ 3). When n = k, using the results for n = k − 1 and n = 2, we get\nL1(P,Q) ≤ k−2 ∑\ni=1\nL1(Pi, Qi) + L1(Pk−1 × Pk, Qk−1 ×Qk)\n≤ k−2 ∑\ni=1\nL1(Pi, Qi) + L1(Pk−1, Qk−1) + L1(Pk, Qk)\n= k ∑\ni=1\nL1(Pi, Qi).\nThis completes the proof."
    }, {
      "heading" : "A.1.2 The DKW Inequality",
      "text" : "Consider a distribution D with CDF F (x). Let F̂n(x) be the empirical CDF of n i.i.d. samples X1, . . . , Xn drawn from D, i.e., F̂n(x) = 1n ∑n i=1 1{Xi ≤ x} (x ∈ R).7 Then we have: Lemma 2 (Dvoretzky-Kiefer-Wolfowitz inequality [10, 24]). For any ǫ > 0 and any n ∈ Z+, we have\nPr\n[\nsup x∈R\n∣ ∣ ∣ F̂n(x)− F (x) ∣ ∣ ∣ ≥ ǫ ] ≤ 2e−2nǫ2.\nNote that for any fixed x ∈ R, from the Chernoff bound we have Pr [∣ ∣ ∣ F̂n(x) − F (x) ∣ ∣ ∣ ≥ ǫ ]\n≤ 2e−2nǫ 2\n. The DKW inequality states a stronger guarantee that the Chernoff concentration holds simultaneously for all x ∈ R."
    }, {
      "heading" : "A.1.3 Technical Lemmas",
      "text" : "The following lemma describes some properties of the expected reward rP (S) = EX∼P [R(X,S)].\nLemma 3. Let P = P1 × · · · × Pm and P ′ = P ′1 × · · · × P ′m be two probability distributions over [0, 1]m. Let Fi and F ′i be the CDFs of Pi and P ′ i , respectively (i = 1, . . . ,m). Suppose each Pi (i ∈ [m]) is a discrete distribution with finite support.\n(i) If for any i ∈ [m], x ∈ [0, 1] we have F ′i (x) ≤ Fi(x), then for any super arm S ∈ F , we have rP ′(S) ≥ rP (S).\n(ii) If for any i ∈ [m], x ∈ [0, 1] we have Fi(x) − F ′i (x) ≤ Λi (Λi > 0), then for any super arm S ∈ F , we have\nrP ′(S)− rP (S) ≤ 2M ∑\ni∈S\nΛi.\nProof. It is easy to see why (i) is true. If we have F ′i (x) ≤ Fi(x) for all i ∈ [m] and x ∈ [0, 1], then for all i, P ′i has first-order stochastic dominance over Pi. When we change the distribution from Pi into P ′i , we are moving some probability mass from smaller values to larger values. Recall that the reward function R(x, S) has a monotonicity property (Assumption 3): if x and x′ are two vectors in [0, 1]m such that xi ≤ x′i for all i ∈ [m], then R(x, S) ≤ R(x′, S) for all S ∈ F . Therefore we have rP (S) ≤ rP ′ (S) for all S ∈ F .\nNow we prove (ii). Without loss of generality, we assume S = {1, 2, . . . , n} (n ≤ m). Let P ′′ = P ′′1 × · · · × P ′′m be a distribution over [0, 1]m such that the CDF of P ′′i is the following:\nF ′′i (x) =\n{\nmax{Fi(x)− Λi, 0}, 0 ≤ x < 1, 1, x = 1.\n(2)\nIt is easy to see that F ′′i (x) ≤ F ′i (x) for all i ∈ [m] and x ∈ [0, 1]. Thus from the result in (i) we have\nrP ′ (S) ≤ rP ′′(S). (3) 7We use 1{·} to denote the indicator function, i.e., 1{H} = 1 if an event H happens, and 1{H} = 0 if it\ndoes not happen.\nLet supp(Pi) = {vi,1, vi,2, . . . , vi,si} where 0 ≤ vi,1 < · · · < vi,si ≤ 1. Define PS = P1 × P2 × · · · ×Pn, and define P ′S and P ′′S similarly. Recall that the reward function R(x, S) can be written as RS(xS) = RS(x1, . . . , xn). Then we have\nrP ′′ (S)− rP (S) = ∑\nx1,...,xn\nRS(x1, . . . , xn)P ′′ S (x1, . . . , xn)−\n∑\nx1,...,xn\nRS(x1, . . . , xn)PS(x1, . . . , xn)\n= ∑\nx1,...,xn\nRS(x1, . . . , xn) · (P ′′S (x1, . . . , xn)− PS(x1, . . . , xn))\n≤ ∑\nx1,...,xn\nM · |P ′′S (x1, . . . , xn)− PS(x1, . . . , xn)|\n=M · L1(P ′′S , PS), where the summation is taken over xi ∈ {vi,1, . . . , vi,si} (∀i ∈ S). Then using Lemma 1 we obtain\nrP ′′(S)− rP (S) ≤ M · ∑\ni∈S\nL1(P ′′ i , Pi). (4)\nNow we give an upper bound on L1(P ′′i , Pi) for each i. Let Fi,j = Fi(vi,j), F ′′ i,j = F ′′ i (vi,j), and Fi,0 = F ′′ i,0 = 0. We have\nL1(P ′′ i , Pi) =\nsi ∑\nj=1\n|P ′′i (vi,j)− Pi(vi,j)|\n=\nsi ∑\nj=1\n∣ ∣(F ′′i,j − F ′′i,j−1)− (Fi,j − Fi,j−1) ∣ ∣\n=\nsi ∑\nj=1\n∣ ∣(Fi,j − F ′′i,j)− (Fi,j−1 − F ′′i,j−1) ∣ ∣ .\n(5)\nIn fact, for all 1 ≤ j < si, we have Fi,j − F ′′i,j ≥ Fi,j−1 − F ′′i,j−1. To see this, consider two cases:\n• If Fi,j < Λi, then we have Fi,j−1 ≤ Fi,j < Λi. By definition (2) we have F ′′i,j = F ′′i,j−1 = 0. Thus Fi,j − F ′′i,j = Fi,j ≥ Fi,j−1 = Fi,j−1 − F ′′i,j−1.\n• If Fi,j ≥ Λi, then by definition (2) we have Fi,j − F ′′i,j = Λi ≥ Fi,j−1 − F ′′i,j−1.\nTherefore (5) becomes\nL1(P ′′ i , Pi) =\nsi−1 ∑\nj=1\n( (Fi,j − F ′′i,j)− (Fi,j−1 − F ′′i,j−1) ) + ∣ ∣(1− 1)− (Fi,si−1 − F ′′i,si−1) ∣ ∣\n= Fi,si−1 − F ′′i,si−1 + ∣ ∣Fi,si−1 − F ′′i,si−1 ∣ ∣ = 2 (\nFi,si−1 − F ′′i,si−1 )\n≤ 2Λi,\n(6)\nwhere the last inequality is due to (2).\nWe complete the proof of the lemma by combining (3), (4) and (6):\nrP ′(S)− rP (S) ≤ rP ′′(S)− rP (S) ≤ M · ∑\ni∈S\nL1(P ′′ i , Pi) ≤ 2M\n∑\ni∈S\nΛi.\nThe following lemma is similar to Lemma 1 in [18]. We will use some additional notation:\n• For t ≥ m + 1 and i ∈ [m], let Ti,t be the value of counter Ti right after the t-th round of SDCB. In other words, Ti,t is the number of observed outcomes from arm i in the first t rounds.\n• Let St be the super arm selected by SDCB in the t-th round. Lemma 4. Define an event in each round t (m+ 1 ≤ t ≤ T ):\nHt = { 0 < ∆St ≤ 4M · ∑\ni∈St\n√\n3 ln t\n2Ti,t−1\n}\n. (7)\nThen the α-approximation regret of SDCB in T rounds is at most\nE\n[\nT ∑\nt=m+1\n1{Ht}∆St\n]\n+\n(\nπ2\n3 + 1\n)\nαMm.\nProof. Let Fi be the CDF of Di. Let F̂i,l be the empirical CDF of the first l observations from arm i. For m+ 1 ≤ t ≤ T , define an event\nEt = {\nthere exists i ∈ [m] such that sup x∈[0,1]\n∣ ∣ ∣ F̂i,Ti,t−1 (x) − Fi(x) ∣ ∣ ∣ ≥ √ 3 ln t\n2Ti,t−1\n}\n,\nwhich means that the empirical CDF F̂i is not close enough to the true CDF Fi at the beginning of the t-th round.\nRecall that we have S∗ = argmaxS∈F{rD(S)} and ∆S = max{α · rD(S∗)− rD(S), 0} (S ∈ F ). We bound the α-approximation regret of SDCB as\nRegSDCBD,α(T ) = T ∑\nt=1\nE [α · rD(S∗)− rD(St)] ≤ T ∑\nt=1\nE [∆St ]\n= E\n[\nm ∑\nt=1\n∆St\n]\n+ E\n[\nT ∑\nt=m+1\n1{Et}∆St\n]\n+ E\n[\nT ∑\nt=m+1\n1{¬Et}∆St\n]\n,\n(8)\nwhere ¬Et is the complement of event Et. We separately bound each term in (8).\n(a) the first term\nThe first term in (8) can be trivially bounded as\nE\n[\nm ∑\nt=1\n∆St\n]\n≤ m ∑\nt=1\nα · rD(S∗) ≤ m · αM. (9)\n(b) the second term\nBy the DKW inequality we know that for any i ∈ [m], l ≥ 1, t ≥ m+ 1 we have\nPr\n[\nsup x∈[0,1]\n∣ ∣ ∣ F̂i,l(x)− Fi(x) ∣ ∣ ∣ ≥ √ 3 ln t\n2l\n]\n≤ 2e−2l· 3 ln t2l = 2e−3 ln t = 2t−3.\nTherefore\nE\n[\nT ∑\nt=m+1\n1{Et} ] ≤ T ∑\nt=m+1\nm ∑\ni=1\nt−1 ∑\nl=1\nPr\n[\n∣ ∣ ∣ F̂i,j,l − Fi,j ∣ ∣ ∣ ≥ √ 3 ln t\n2l\n]\n≤ T ∑\nt=m+1\nm ∑\ni=1\nt−1 ∑\nl=1\n2t−3\n≤ 2m T ∑\nt=m+1\nt−2\n≤ π 2\n3 m,\nand then the second term in (8) can be bounded as\nE\n[\nT ∑\nt=m+1\n1{Et}∆St\n]\n≤ π 2\n3 m · (α · rD(S∗)) ≤\nπ2\n3 αMm. (10)\n(c) the third term\nWe fix t > m and first assume ¬Et happens. Let ci = √\n3 ln t 2Ti,t−1 for each i ∈ [m]. Since ¬Et happens, we have\n∣ ∣ ∣ F̂i,Ti,t−1(x) − Fi(x) ∣ ∣ ∣ < ci ∀i ∈ [m], x ∈ [0, 1]. (11)\nRecall that in round t of SDCB (Algorithm 1), the input to the oracle is D = D1 × · · · ×Dm, where the CDF Fi of Di is\nFi(x) =\n{\nmax{F̂i,Ti,t−1(x) − ci, 0}, 0 ≤ x < 1, 1, x = 1.\n(12)\nFrom (11) and (12) we know that Fi(x) ≤ Fi(x) ≤ Fi(x) + 2ci for all i ∈ [m], x ∈ [0, 1]. Thus, from Lemma 3 (i) we have rD(S) ≤ rD(S) ∀S ∈ F , (13) and from Lemma 3 (ii) we have\nrD(S) ≤ rD(S) + 2M ∑\ni∈S\n2ci ∀S ∈ F . (14)\nAlso, from the fact that the algorithm chooses St in the t-th round, we have\nrD(St) ≥ α ·max S∈F {rD(S)} ≥ α · rD(S∗). (15)\nFrom (13), (14) and (15) we have\nα · rD(S∗) ≤ α · rD(S∗) ≤ rD(St) ≤ rD(St) + 2M ∑\ni∈St\n2ci,\nwhich implies ∆St ≤ 4M ∑\ni∈St\nci.\nTherefore, when ¬Et happens, we always have ∆St ≤ 4M ∑ i∈St ci. In other words,\n¬Et =⇒ { ∆St ≤ 4M ∑\ni∈St\n√\n3 ln t\n2Ti,t−1\n}\n.\nThis implies\n{¬Et,∆St > 0} =⇒ { 0 < ∆St ≤ 4M ∑\ni∈St\n√\n3 ln t\n2Ti,t−1\n}\n= Ht.\nHence, the third term in (8) can be bounded as\nE\n[\nT ∑\nt=m+1\n1{¬Et}∆St\n]\n= E\n[\nT ∑\nt=m+1\n1{¬Et,∆St > 0}∆St\n] ≤ E [ T ∑\nt=m+1\n1{Ht}∆St\n]\n. (16)\nFinally, by combining (8), (9), (10) and (16) we have\nRegSDCBD,α(T ) ≤ E [ T ∑\nt=m+1\n1{Ht}∆St\n]\n+\n(\nπ2\n3 + 1\n)\nαMm,\ncompleting the proof of the lemma."
    }, {
      "heading" : "A.1.4 Finishing the Proof of Theorem 1",
      "text" : "Lemma 4 is very similar to Lemma 1 in [18]. We now apply the counting argument in [18] to finish the proof of Theorem 1.\nFrom Lemma 4 we know that it remains to bound E [\n∑T t=m+1 1{Ht}∆St\n]\n, where Ht is defined in (7).\nDefine two decreasing sequences of positive constants\n1 = β0 >β1 > β2 > . . .\nα1 > α2 > . . .\nsuch that limk→∞ αk = limk→∞ βk = 0. We choose {αk} and {βk} as in Theorem 4 of [18], which satisfy\n√ 6 ∞ ∑\nk=1\nβk−1 − βk√ αk ≤ 1 (17)\nand ∞ ∑\nk=1\nαk βk < 267. (18)\nFor t ∈ {m+ 1, . . . , T } and k ∈ Z+, let\nmk,t =\n{\nαk\n(\n2MK ∆St\n)2\nlnT ∆St > 0,\n+∞ ∆St = 0,\nand Ak,t = {i ∈ St | Ti,t−1 ≤ mk,t}.\nThen we define an event Gk,t = {|Ak,t| ≥ βkK},\nwhich means “in the t-th round, at least βkK arms in St had been observed at most mk,t times.”\nLemma 5. In the t-th round (m+ 1 ≤ t ≤ T ), if event Ht happens, then there exists k ∈ Z+ such that event Gk,t happens.\nProof. Assume that Ht happens and that none of G1,t,G2,t, . . . happens. Then |Ak,t| < βkK for all k ∈ Z+. Let A0,t = St and Āk,t = St \\Ak,t for k ∈ Z+∪{0}. It is easy to see Āk−1,t ⊆ Āk,t for all k ∈ Z+. Note that limk→∞ mk,t = 0. Thus there exists N ∈ Z+ such that Āk,t = St for all k ≥ N , and then we have St = ⋃∞ k=1 ( Āk,t \\ Āk−1,t )\n. Finally, note that for all i ∈ Āk,t, we have Ti,t−1 > mk,t. Therefore\n∑\ni∈St\n1 √\nTi,t−1 =\n∞ ∑\nk=1\n∑\ni∈Āk,t\\Āk−1,t\n1 √\nTi,t−1 ≤\n∞ ∑\nk=1\n∑\ni∈Āk,t\\Āk−1,t\n1 √ mk,t\n= ∞ ∑\nk=1\n∣ ∣Āk,t \\ Āk−1,t ∣ ∣\n√ mk,t\n= ∞ ∑\nk=1\n|Ak−1,t \\Ak,t|√ mk,t = ∞ ∑\nk=1\n|Ak−1,t| − |Ak,t|√ mk,t\n= |St|√ m1,t +\n∞ ∑\nk=1\n|Ak,t| ( 1 √ mk+1,t − 1√ mk,t )\n< K\n√ m1,t\n+ ∞ ∑\nk=1\nβkK\n(\n1 √ mk+1,t − 1√ mk,t\n)\n=\n∞ ∑\nk=1\n(βk−1 − βk)K√ mk,t .\nNote that we assume Ht happens. Then we have\n∆St ≤ 4M · ∑\ni∈St\n√\n3 ln t\n2Ti,t−1 ≤ 2M\n√ 6 lnT · ∑\ni∈St\n1 √\nTi,t−1\n< 2M √ 6 lnT · ∞ ∑\nk=1\n(βk−1 − βk)K√ mk,t = √ 6 ∞ ∑\nk=1\nβk−1 − βk√ αk ·∆St ≤ ∆St ,\nwhere the last inequality is due to (17). We reach a contradiction here. The proof of the lemma is completed.\nBy Lemma 5 we have\nT ∑\nt=m+1\n1{Ht}∆St ≤ ∞ ∑\nk=1\nT ∑\nt=m+1\n1{Gk,t,∆St > 0}∆St .\nFor i ∈ [m], k ∈ Z+, t ∈ {m+ 1, . . . , T }, define an event Gi,k,t = Gk,t ∧ {i ∈ St, Ti,t−1 ≤ mk,t}.\nThen by the definitions of Gk,t and Gi,k,t we have\n1{Gk,t,∆St > 0} ≤ 1\nβkK\n∑\ni∈EB\n1{Gi,k,t,∆St > 0}.\nTherefore T ∑\nt=m+1\n1{Ht}∆St ≤ ∑\ni∈EB\n∞ ∑\nk=1\nT ∑\nt=m+1\n1{Gi,k,t,∆St > 0} ∆St βkK .\nFor each arm i ∈ EB, suppose i is contained in Ni bad super arms SBi,1, SBi,2, . . . , SBi,Ni . Let ∆i,l = ∆SB i,l (l ∈ [Ni]). Without loss of generality, we assume ∆i,1 ≥ ∆i,2 ≥ . . . ≥ ∆i,Ni . Note that ∆i,Ni = ∆i,min. For convenience, we also define ∆i,0 = +∞, i.e., αk ( 2MK ∆i,0 )2 = 0. Then we have T ∑\nt=m+1\n1{Ht}∆St\n≤ ∑\ni∈EB\n∞ ∑\nk=1\nT ∑\nt=m+1\nNi ∑\nl=1\n1{Gi,k,t, St = SBi,l} ∆St βkK\n≤ ∑\ni∈EB\n∞ ∑\nk=1\nT ∑\nt=m+1\nNi ∑\nl=1\n1{Ti,t−1 ≤ mk,t, St = SBi,l} ∆i,l βkK\n= ∑\ni∈EB\n∞ ∑\nk=1\nT ∑\nt=m+1\nNi ∑\nl=1\n1\n{\nTi,t−1 ≤ αk ( 2MK\n∆i,l\n)2\nlnT, St = S B i,l\n}\n∆i,l βkK\n= ∑\ni∈EB\n∞ ∑\nk=1\nT ∑\nt=m+1\nNi ∑\nl=1\nl ∑\nj=1\n1\n{\nαk\n(\n2MK\n∆i,j−1\n)2\nlnT < Ti,t−1 ≤ αk ( 2MK\n∆i,j\n)2\nlnT, St = S B i,l\n}\n∆i,l βkK\n≤ ∑\ni∈EB\n∞ ∑\nk=1\nT ∑\nt=m+1\nNi ∑\nl=1\nl ∑\nj=1\n1\n{\nαk\n(\n2MK\n∆i,j−1\n)2\nlnT < Ti,t−1 ≤ αk ( 2MK\n∆i,j\n)2\nlnT, St = S B i,l\n}\n∆i,j βkK\n≤ ∑\ni∈EB\n∞ ∑\nk=1\nT ∑\nt=m+1\nNi ∑\nl=1\nNi ∑\nj=1\n1\n{\nαk\n(\n2MK\n∆i,j−1\n)2\nlnT < Ti,t−1 ≤ αk ( 2MK\n∆i,j\n)2\nlnT, St = S B i,l\n}\n∆i,j βkK\n≤ ∑\ni∈EB\n∞ ∑\nk=1\nT ∑\nt=m+1\nNi ∑\nj=1\n1\n{\nαk\n(\n2MK\n∆i,j−1\n)2\nlnT < Ti,t−1 ≤ αk ( 2MK\n∆i,j\n)2\nlnT\n}\n∆i,j βkK\n≤ ∑\ni∈EB\n∞ ∑\nk=1\nNi ∑\nj=1\n(\nαk\n(\n2MK\n∆i,j\n)2\nlnT − αk ( 2MK\n∆i,j−1\n)2\nlnT\n)\n∆i,j βkK\n=4M2K\n(\n∞ ∑\nk=1\nαk βk\n)\nlnT · ∑\ni∈EB\nNi ∑\nj=1\n(\n1 ∆2i,j − 1 ∆2i,j−1\n)\n∆i,j\n≤1068M2K lnT · ∑\ni∈EB\nNi ∑\nj=1\n(\n1 ∆2i,j − 1 ∆2i,j−1\n)\n∆i,j ,\nwhere the last inequality is due to (18).\nFinally, for each i ∈ EB we have Ni ∑\nj=1\n(\n1 ∆2i,j − 1 ∆2i,j−1\n)\n∆i,j = 1\n∆i,Ni +\nNi−1 ∑\nj=1\n1\n∆2i,j (∆i,j −∆i,j+1)\n≤ 1 ∆i,Ni +\n∫ ∆i,1\n∆i,Ni\n1\nx2 dx\n= 2 ∆i,Ni − 1 ∆i,1\n< 2\n∆i,min .\nIt follows that\nT ∑\nt=m+1\n1{Ht}∆St ≤ 1068M2K lnT · ∑\ni∈EB\n2\n∆i,min = M2K\n∑\ni∈EB\n2136\n∆i,min lnT. (19)\nCombining (19) with Lemma 4, the distribution-dependent regret bound in Theorem 1 is proved.\nTo prove the distribution-independent bound, we decompose ∑T t=m+1 1{Ht}∆St into two parts: T ∑\nt=m+1\n1{Ht}∆St = T ∑\nt=m+1\n1{Ht,∆St ≤ ǫ}∆St + T ∑\nt=m+1\n1{Ht,∆St > ǫ}∆St\n≤ ǫT + T ∑\nt=m+1\n1{Ht,∆St > ǫ}∆St , (20)\nwhere ǫ > 0 is a constant to be determined. The second term can be bounded in the same way as in the proof of the distribution-dependent regret bound, except that we only consider the case ∆St > ǫ. Thus we can replace (19) by\nT ∑\nt=m+1\n1{Ht,∆St > ǫ}∆St ≤ M2K ∑\ni∈EB,∆i,min>ǫ\n2136 ∆i,min lnT ≤ M2Km2136 ǫ lnT. (21)\nIt follows that T ∑\nt=m+1\n1{Ht}∆St ≤ ǫT +M2Km 2136\nǫ lnT.\nFinally, letting ǫ = √\n2136M2Km lnT T , we get\nT ∑\nt=m+1\n1{Ht}∆St ≤ 2 √ 2136M2KmT lnT < 93M √ mKT lnT .\nCombining this with Lemma 4, we conclude the proof of the distribution-independent regret bound in Theorem 1.\nAlgorithm 4 CUCB [8, 18] 1: For each arm i, maintain: (i) µ̂i, the average of all observed outcomes from arm i so far, and (ii)\nTi, the number of observed outcomes from arm i so far.\n2: // Initialization 3: for i = 1 to m do 4: // Action in the i-th round 5: Play a super arm Si that contains arm i, and update µ̂i and Ti. 6: end for\n7: for t = m+ 1,m+ 2, . . . do 8: // Action in the t-th round 9: µ̄i ← min{µ̂i + √\n3 ln t 2Ti , 1} ∀i ∈ [m] 10: Play the super arm St ← Oracle(µ̄), where µ̄ = (µ̄1, . . . , µ̄m). 11: Update µ̂i and Ti for all i ∈ St. 12: end for"
    }, {
      "heading" : "A.2 Analysis of Our Algorithm in the Previous CMAB Framework",
      "text" : "We now give an analysis of SDCB in the previous CMAB framework, following our discussion in Section 3. We consider the case in which the expected reward only depends on the means of the random variables. Namely, rD(S) only depends on µi’s (i ∈ S), where µi is arm i’s mean outcome. In this case, we can rewrite rD(S) as rµ(S), where µ = (µ1, . . . , µm) is the vector of means. Note that the offline computation oracle only needs a mean vector as input.\nWe no longer need the three assumptions (Assumptions 1-3) given in Section 2. In particular, we do not require independence among outcome distributions of all arms (Assumption 1). Although we cannot write D as D = D1 × · · · ×Dm, we still let Di be the outcome distribution of arm i. In this case, Di is the marginal distribution of D in the i-th component.\nWe summarize the CUCB algorithm [8, 18] in Algorithm 4. It maintains the empirical mean µ̂i of the outcomes from each arm i, and stores the number of observed outcomes from arm i in a variable Ti. In each round, it calculates an upper confidence bound (UCB) µ̄i of µi, Then it uses the UCB vector µ̄ as the input to the oracle, and plays the super arm output by the oracle. In the t-th round (t > m), each UCB µ̄i has the key property that\nµi ≤ µ̄i ≤ µi + 2 √ 3 ln t\n2Ti,t−1 (22)\nholds with high probability. (Recall that Ti,t−1 is the value of Ti after t − 1 rounds.) To see this, note that we have |µi − µ̂i| ≤ √\n3 ln t 2Ti,t−1 with high probability (by Chernoff bound), and then (22)\nfollows from the definition of µ̄i in line 9 of Algorithm 4.\nWe prove that the same property as (22) also holds for SDCB. Consider a fixed t > m, and let D = D1 × · · · ×Dm be the input to the oracle in the t-th round of SDCB. Let νi = EYi∼Di [Yi]. We can think that SDCB uses the mean vector ν = (ν1, . . . , νm) as the input to the oracle used by CUCB. We now show that for each i, we have\nµi ≤ νi ≤ µi + 2 √ 3 ln t\n2Ti,t−1 (23)\nwith high probability.\nTo show (23), we first prove the following lemma.\nLemma 6. Let P andP ′ be two distributions over [0, 1]with CDFs F andF ′, respectively. Consider two random variables Y ∼ P and Y ′ ∼ P ′.\n(i) If for all x ∈ [0, 1] we have F ′(x) ≤ F (x), then we have E[Y ] ≤ E[Y ′]. (ii) If for all x ∈ [0, 1] we have F (x)− F ′(x) ≤ Λ (Λ > 0), then we have E[Y ′] ≤ E[Y ] + Λ.\nProof. We have\nE[Y ] =\n∫ 1\n0\nxdF (x) = (xF (x)) ∣ ∣ 1 0 − ∫ 1\n0\nF (x) dx = 1− ∫ 1\n0\nF (x) dx.\nSimilarly, we have\nE[Y ′] = 1− ∫ 1\n0\nF ′(x) dx.\nThen the lemma holds trivially.\nNow we prove (23). According to the DKW inequality, with high probability we have\nFi(x) − 2 √ 3 ln t\n2Ti,t−1 ≤ Fi(x) ≤ Fi(x) (24)\nfor all i ∈ [m] and x ∈ [0, 1], where Fi is the CDF of Di used in round t of SDCB, and Fi is the CDF of Di. Suppose (24) holds for all i, x, then for any i, the two distributions Di and Di satisfy the two conditions in Lemma 6, with Λ = 2 √\n3 ln t 2Ti,t−1 ; then from Lemma 6 we know that\nµi ≤ νi ≤ µi + 2 √\n3 ln t 2Ti,t−1 . Hence we have shown that (23) holds with high probability.\nThe fact that (23) holds with high probability means that the mean of Di is also a UCB of µi with the same confidence as in CUCB. With this property, the analysis in [8, 18] can also be applied to SDCB, resulting in exactly the same regret bounds."
    }, {
      "heading" : "B Missing Proofs from Section 4",
      "text" : ""
    }, {
      "heading" : "B.1 Analysis of the Discretization Error",
      "text" : "The following lemma gives an upper bound on the error due to discretization. Refer to Section 4 for the definition of the discretized distribution D̃.\nLemma 7. For any S ∈ F , we have\n|rD(S)− rD̃(S)| ≤ CK\ns .\nTo prove Lemma 7, we show a slightly more general lemma which gives an upper bound on the discretization error of the expectation of a Lipschitz continuous function.\nLemma 8. Let g(x) be a Lipschitz continuous function on [0, 1]n such that for any x, x′ ∈ [0, 1]n, we have |g(x)− g(x′)| ≤ C‖x− x′‖1, where ‖x− x′‖1 = ∑n i=1 |xi − x′i|. Let P = P1 × · · · ×Pn be a probability distribution over [0, 1]n. Define another distribution P̃ = P̃1×· · ·× P̃n over [0, 1]n as follows: each P̃i (i ∈ [n]) takes values in { 1s , 2s , . . . , 1}, and\nPr X̃i∼P̃i [X̃i = j/s] = Pr Xi∼Pi\n[Xi ∈ Ij ] , j ∈ [s],\nwhere I1 = [0, 1s ], I2 = ( 1 s , 2 s ], . . . , Is−1 = ( s−2 s , s−1 s ], Is = ( s−1 s , 1]. Then\n∣ ∣ ∣ EX∼P [g(X)]− EX̃∼P̃ [g(X̃)] ∣ ∣ ∣ ≤ C · n\ns . (25)\nProof. Throughout the proof, we consider X = (X1, . . . , Xn) ∼ P and X̃ = (X̃1, . . . , X̃n) ∼ P̃ . Let vj = j s (j = 0, 1, . . . , s) and\npi,j = Pr[X̃i = vj ] = Pr[Xi ∈ Ij ] i ∈ [n], j ∈ [s].\nWe prove (25) by induction on n.\n(1) When n = 1, we have\nE[g(X1)] = ∑\nj∈[s],p1,j>0\np1,j · E [ g(X1) ∣ ∣X1 ∈ Ij ] . (26)\nSince g is continuous, for each j ∈ [s] such that p1,j > 0, there exists ξj ∈ [vj−1, vj ] such that\nE [g(X1)|X1 ∈ Ij ] = g(ξj)\nFrom the Lipschitz continuity of g we have\n|g(vj)− g(ξj)| ≤ C|vj − ξj | ≤ C|vj − vj−1| = C\ns .\nHence\n∣ ∣ ∣ E[g(X1)]− E[g(X̃1)] ∣ ∣ ∣ =\n∣ ∣ ∣ ∣ ∣ ∣ ∑\nj∈[s],p1,j>0\np1,j · E[g(X1)|X1 ∈ Ij ]− ∑\nj∈[s],p1,j>0\np1,j · g(vj)\n∣ ∣ ∣ ∣ ∣ ∣\n=\n∣ ∣ ∣ ∣ ∣ ∣ ∑\nj∈[s],p1,j>0\np1,j · g(ξj)− ∑\nj∈[s],p1,j>0\np1,j · g(vj)\n∣ ∣ ∣ ∣ ∣ ∣\n≤ ∑\nj∈[s],p1,j>0\np1,j · |g(ξj)− g(vj)|\n≤ ∑\nj∈[s],p1,j>0\np1,j · C\ns\n= C\ns .\nThis proves (25) for n = 1.\n(ii) Suppose (25) is correct for n = 1, 2, . . . , k − 1. Now we prove it for n = k (k ≥ 2). We define two functions on [0, 1]k−1:\nh(x1, . . . , xk−1) = EXk [g(x1, . . . , xk−1, Xk)]\nand\nh̃(x1, . . . , xk−1) = EX̃k [g(x1, . . . , xk−1, X̃k)].\nFor any fixed x1, . . . , xk−1 ∈ [0, 1], the function g(x1, . . . , xk−1, x) on x ∈ [0, 1] is Lipschitz continuous. Therefore from the result for n = 1 we have\n∣ ∣ ∣ h(x1, . . . , xk−1)− h̃(x1, . . . , xk−1) ∣ ∣ ∣ ≤ C\ns ∀x1, . . . , xk−1 ∈ [0, 1].\nThen we have ∣\n∣ ∣ E[g(X)]− E[g(X̃)]\n∣ ∣ ∣\n= ∣ ∣\n∣ EX1,...,Xk−1 [E[g(X)|X1, . . . , Xk−1]]− E[g(X̃)]\n∣ ∣ ∣\n= ∣ ∣\n∣ EX1,...,Xk−1 [h(X1, . . . , Xk−1)]− E[g(X̃)]\n∣ ∣ ∣\n≤ ∣ ∣\n∣ EX1,...,Xk−1 [h(X1, . . . , Xk−1)]− EX1,...,Xk−1 [h̃(X1, . . . , Xk−1)]\n∣ ∣ ∣\n+ ∣ ∣\n∣ EX1,...,Xk−1 [h̃(X1, . . . , Xk−1)]− E[g(X̃)]\n∣ ∣ ∣\n≤EX1,...,Xk−1 [∣ ∣ ∣ h(X1, . . . , Xk−1)− h̃(X1, . . . , Xk−1) ∣ ∣ ∣ ]\n+ ∣ ∣\n∣ EX1,...,Xk−1,X̃k\n[g(X1, . . . , Xk−1, X̃k)]− E[g(X̃)] ∣ ∣ ∣\n≤EX1,...,Xk−1 [ C\ns\n]\n+ ∣ ∣\n∣ EX̃k\n[ E[g(X1, . . . , Xk−1, X̃k)|X̃k]− E[g(X̃1, . . . , X̃k−1, X̃k)|X̃k] ]∣ ∣ ∣\n≤ C s + EX̃k [∣ ∣ ∣ E[g(X1, . . . , Xk−1, X̃k)|X̃k]− E[g(X̃1, . . . , X̃k−1, X̃k)|X̃k] ∣ ∣ ∣ ] = C\ns +\n∑\nj∈[s],pk,j>0\npk,j · ∣ ∣ ∣ E[g(X1, . . . , Xk−1, vj)]− E[g(X̃1, . . . , X̃k−1, vj)] ∣ ∣ ∣ .\n(27)\nFor any j ∈ [s], the function g(x1, . . . , xk−1, vj) on (x1, . . . , xk−1) ∈ [0, 1]k−1 is Lipschitz continuous. Then from the induction hypothesis at n = k − 1, we have\n∣ ∣ ∣ E[g(X1, . . . , Xk−1, vj)]− E[g(X̃1, . . . , X̃k−1, vj)] ∣ ∣ ∣ ≤ C(k − 1)\ns ∀j ∈ [s]. (28)\nFrom (27) and (28) we have\n∣ ∣ ∣ E[g(X)]− E[g(X̃)] ∣ ∣ ∣ ≤ C\ns +\n∑\nj∈[s],pk,j>0\npk,j · C(k − 1)\ns\n= C\ns + C(k − 1) s\n= Ck\ns .\nThis concludes the proof for n = k.\nNow we prove Lemma 7.\nProof of Lemma 7. We have\nrD(S) = EX∼D[R(X,S)] = EX∼D[RS(XS)] = EXS∼DS [RS(XS)],\nwhere XS = (Xi)i∈S and DS = (Di)i∈S . Similarly, we have\nrD̃(S) = EX̃S∼D̃S [RS(X̃S)].\nAccording to Assumption 4, the function RS defined on [0, 1]S is Lipschitz continuous. Then from Lemma 8 we have\n|rD(S)− rD̃(S)| = ∣ ∣ ∣ EXS∼DS [RS(XS)]− EX̃S∼D̃S [RS(X̃S)] ∣ ∣ ∣ ≤ C · |S| s ≤ C ·K s .\nThis completes the proof."
    }, {
      "heading" : "B.2 Proof of Theorem 2",
      "text" : "Proof of Theorem 2. Let S∗ = argmaxS∈F{rD(S)} and S̃∗ = argmaxS∈F{rD̃(S)} be the optimal super arms in problems ([m],F , D,R) and ([m],F , D̃, R), respectively. Suppose Algorithm 2 selects super arm St in the t-th round (1 ≤ t ≤ T ). Then its α-approximation regret is bounded as\nReg Alg. 2 D,α (T )\n=T · α · rD(S∗)− T ∑\nt=1\nE [rD(St)]\n=T · α (\nrD(S ∗)− rD̃(S̃∗)\n) + T ∑\nt=1\nE [rD̃(St)− rD(St)] + ( T · α · rD̃(S̃∗)− T ∑\nt=1\nE [rD̃(St)]\n)\n≤T · α (rD(S∗)− rD̃(S∗)) + T ∑\nt=1\nE [rD̃(St)− rD(St)] + Reg Alg. 1 D̃,α (T ).\nwhere the inequality is due to rD̃(S̃ ∗) ≥ rD̃(S∗).\nThen from Lemma 7 and the distribution-independent bound in Theorem 1 we have\nReg Alg. 2 D,α (T ) ≤ T · α ·\nCK s + T · CK s + 93M\n√ mKT lnT + ( π2\n3 + 1\n)\nαMm\n≤ 2 · CKT s\n+ 93M √ mKT lnT + ( π2\n3 + 1\n)\nαMm\n≤ 93M √ mKT lnT + 2CK √ T + ( π2\n3 + 1\n)\nαMm.\n(29)\nHere in the last two inequalities we have used α ≤ 1 and s = ⌈ √ T ⌉ ≥ √ T . The proof is completed."
    }, {
      "heading" : "B.3 Proof of Theorem 3",
      "text" : "Proof of Theorem 3. Let n = ⌈log2 T ⌉. Then we have 2n−1 < T ≤ 2n. If n ≤ q = ⌈log2 m⌉, then T ≤ 2m and the regret in T rounds is at most 2m · αM . The regret bound holds trivially.\nNow we assume n ≥ q + 1. Using Theorem 2, we have Reg\nAlg. 3 D,α (T )\n≤RegAlg. 3D,α (2n)\n=RegAlg. 2D,α (2 q) +\nn−1 ∑\nk=q\nReg Alg. 2 D,α (2 k)\n≤RegAlg. 2D,α (2m) + n−1 ∑\nk=q\nReg Alg. 2 D,α (2 k)\n≤ 2m · αM + n−1 ∑\nk=q\n( 93M √ mK · 2k ln 2k + 2CK √ 2k + ( π2\n3 + 1\n)\nαMm\n)\n≤ 2αMm+ ( 93M √ mK ln 2n−1 + 2CK ) · n−1 ∑\nk=1\n√ 2k + (n− 1) · ( π2\n3 + 1\n)\nαMm\n≤ ( 93M √ mK ln 2n−1 + 2CK ) · √ 2n√\n2− 1 +\n(\nπ2\n3 + 3\n)\n(n− 1) · αMm\n≤ ( 93M √ mK lnT + 2CK ) · √ 2T√ 2− 1 + ( π2 3 + 3 ) log2 T · αMm\nAlgorithm 5 Greedy-K-MAX 1: S ← ∅ 2: for i = 1 to K do 3: k ← argmaxj∈[m]\\S rD(S ∪ {j}) 4: S ← S ∪ {k} 5: end for Output: S\n≤ 318M √ mKT lnT + 7CK √ T + 10αMm lnT.\nC The Offline K-MAX Problem\nIn this section, we consider the offline K-MAX problem. Recall that we have m independent random variables {Xi}i∈[m]. Xi follows the discrete distribution Di with support {vi,1, . . . , vi,si} ⊂ [0, 1], and D = D1 × · · · ×Dm is the joint distribution of X = (X1, . . . , Xm). Let pi,j = Pr[Xi = vi,j ]. Define rD(S) = EX∼D[maxi∈S Xi] and OPT = maxS:|S|=K rD(S). Our goal is to find (in polynomial time) a subset S ⊆ [m] of cardinality K such that rD(S) ≥ α · OPT (for certain constant α).\nFirst, we show that rD(S) can be calculated in polynomial time given any S ⊆ [m]. Let S = {i1, i2, . . . , in}. Note that for X ∼ D, maxi∈S Xi can only take values in the set V (S) = ⋃\ni∈S supp(Di). For any v ∈ V (S), we have\nPr X∼D\n[\nmax i∈S Xi = v\n]\n= Pr X∼D [Xi1 = v,Xi2 ≤ v, . . . , Xin ≤ v] + Pr\nX∼D [Xi1 < v,Xi2 = v,Xi3 ≤ v, . . . , Xin ≤ v]\n+ · · · + Pr\nX∼D [Xi1 < v, . . . , Xin−1 < v,Xin = v].\n(30)\nSince Xi1 , . . . , Xin are mutually independent, each probability appearing in (30) can be calculated in polynomial time. Hence for any v ∈ V (S), PrX∼D [maxi∈S Xi = v] can be calculated in polynomial time using (30). Then rD(S) can be calculated by\nrD(S) = ∑\nv∈V (S)\nv · Pr X∼D\n[\nmax i∈S Xi = v\n]\nin polynomial time.\nC.1 (1− 1/e)-Approximation\nWe now show that a simple greedy algorithm (Algorithm 5) can find a (1 − 1/e)-approximate solution, by proving the submodularity of rD(S). In fact, this is implied by a slightly more general result [13, Lemma 3.2]. We provide a simple and direct proof for completeness.\nLemma 9. Algorithm 5 can output a subset S such that rD(S) ≥ (1− 1/e) ·OPT.\nProof. For any x ∈ [0, 1]m, let fx(S) = maxi∈S xi be a set function defined on 2[m]. (Define fx(∅) = 0.) We can verify that fx(S) is monotone and submodular:\n• Monotonicity. For any A ⊆ B ⊆ [m], we have fx(A) = maxi∈A xi ≤ maxi∈B xi = fx(B).\n• Submodularity. For any A ⊆ B ⊆ [m] and any k ∈ [m] \\ B, there are three cases (note that maxi∈A xi ≤ maxi∈B xi):\n(i) If xk ≤ maxi∈A xi, then fx(A ∪ {k})− fx(A) = 0 = fx(B ∪ {k})− fx(B).\n(ii) If maxi∈A xi < xk ≤ maxi∈B xi, then fx(A ∪ {k})− fx(A) = xk −maxi∈A xi > 0 = fx(B ∪ {k})− fx(B).\n(iii) If xk > maxi∈B xi, then fx(A ∪ {k}) − fx(A) = xk − maxi∈A xi ≥ xk − maxi∈B xi = fx(B ∪ {k})− fx(B).\nTherefore, we always have fx(A ∪ {k})− fx(A) ≥ fx(B ∪ {i})− fx(B). The function fx(S) is submodular.\nFor any S ⊆ [m] we have\nrD(S) =\ns1 ∑\nj1=1\ns2 ∑\nj2=1\n· · · sm ∑\njm=1\nf(v1,j1 ,...,vm,jm )(S)\nm ∏\ni=1\npi,ji .\nSince each set function f(v1,j1 ,...,vm,jm )(S) is monotone and submodular, rD(S) is a convex combination of monotone submodular functions on 2[m]. Therefore, rD(S) is also a monotone submodular function. According to the classical result on submodular maximization [25], the greedy algorithm can find a (1− 1/e)-approximate solution to maxS⊆[m],|S|≤K{rD(S)}."
    }, {
      "heading" : "C.2 PTAS",
      "text" : "Now we provide a PTAS for the K-MAX problem. In other words, we give an algorithm which, given any fixed constant 0 < ε < 1/2, can find a solution S of cardinality |K| such that rD(S) ≥ (1 − ε) · OPT in polynomial time. We first provide an overview of our approach, and then spell out the details later.\n1. (Discretization) We first transform each Xi to another discrete distribution X̃i, such that all X̃i’s are supported on a set of size O(1/ε2).\n2. (Computing signatures) For each Xi, we can compute from X̃i a signature Sig(Xi) which is a vector of size O(1/ε2). For a set S, we define its signature Sig(S) to be ∑\ni∈S Sig(Xi). We show that if two sets S1 and S2 have the same signature, their objective values are close (Lemma 12).\n3. (Enumerating signatures) We enumerate all possible signatures (there are polynomial number of them when treating ε as a constant) and try to find the one which is the signature of a set of size K , and the objective value is maximized."
    }, {
      "heading" : "C.2.1 Discretization",
      "text" : "We first describe the discretization step. We say that a random variable X follows the Bernoulli distribution B(v, q) if X takes value v with probability q and value 0 with probability 1− q. For any discrete distribution, we can rewrite it as the maximum of a set of Bernoulli distributions.\nDefinition 1. Let X be a discrete random variable with support {v1, v2, . . . , vs}(v1 < v2 < · · · < vs) and Pr[X = vj ] = pj . We define a set of independent Bernoulli random variables {Zj}j∈[s] as\nZj ∼ B ( vj , pj ∑\nj′≤j pj′\n)\n.\nWe call {Zj} the Bernoulli decomposition of Xi. Lemma 10. For a discrete distribution X and its Bernoulli decomposition {Zj}, maxj{Zj} has the same distribution with X .\nProof. We can easily see the following:\nPr[max j\n{Zj} = vi] = Pr[Zi = vi] ∏\ni′>i\nPr[Zi′ = 0]\n= pi ∑\ni′≤i pi′\n∏\nh>i\n(\n1− ph∑ h′≤h ph′\n)\nAlgorithm 6 Discretization 1: We first run Greedy-K-MAX to obtain a solution SG and let W = rD(SG). 2: for i = 1 to m do 3: Compute the Bernoulli decomposition {Zi,j}j of Xi. 4: for all Zi,j do 5: Create another Bernoulli variable Z̃i,j as follows: 6: if vi,j > W/ε then 7: Let Z̃i,j ∼ B ( W ε ,E[Zi,j ] ε W )\n(Case 1) 8: else 9: Let Z̃i,j = ⌊Zi,jεW ⌋εW (Case 2)\n10: end if 11: end for 12: Let X̃i = maxj{Z̃ij} 13: end for\n= pi ∑\ni′≤i pi′\n∏\nh>i\n∑\nh′≤h−1 ph′ ∑\nh′≤h ph′ = pi.\nHence, Pr[maxj{Zj} = vi] = Pr[X = vi] for all i ∈ [s].\nNow, we describe how to construct the discretization X̃i of Xi for all i ∈ [m]. The pseudocode can be found in Algorithm 6. We first run Greedy-K-MAX to obtain a solution SG. Let W = rD(SG). By Lemma 9, we know that W ≥ (1 − 1/e)OPT. Then we compute the Bernoulli decomposition {Zi,j}j of Xi. For each Zi,j , we create another Bernoulli variable Z̃i,j as follows: Recall that vi,j is the nonzero possible value of Zij . We distinguish two cases. Case 1: If vi,j > W/ε, then we let Z̃i,j ∼ B ( W ε ,E[Zi,j ] ε W )\n. It is easy to see that E[Z̃ij ] = E[Zij ]. Case 2: If vi,j ≤ W/ε, then we let Z̃i,j = ⌊Zi,jεW ⌋εW. We note that more than one Z̃ij’s may have the same support, and all Z̃ij ’s are supported on DS = {0, εW, 2εW, . . . ,W/ε}. Finally, we let X̃i = maxj{Z̃ij}, which is the discretization of Xi. Since X̃i is the maximum of a set of Bernoulli distributions, it is also a discrete distribution supported on DS. We can easily compute Pr[X̃i = v] for any v ∈ DS. Now, we show that the discretization only incurs a small loss in the objective value. The key is to show that we do not lose much in the transformation from Zi,j’s to Z̃i,j’s. We prove a slightly more general lemma as follows.\nLemma 11. Consider any set of Bernoulli variables {Zi ∼ B(ai, pi)}1≤i≤n. Assume that E[maxi∈[n] Zi] < cW, where c is a constant such that cε < 1/2. For each Zi, we create a Bernoulli variable Z̃i in the same way as Algorithm 6. Then the following holds:\nE[maxZi] ≥ E[max Z̃i] ≥ E[maxZi]− (2c+ 1)εW.\nProof. Assume a1 is the largest among all ai’s.\nIf a1 < W/ε, all Z̃i are created in Case 2. In this case, it is obvious to have that\nE[maxZi] ≥ E[max Z̃i] ≥ E[maxZi]− εW.\nIf a1 ≥ W/ε, the proof is slightly more complicated. Let L = {i | ai ≥ W/ε}. We prove by induction on n (i.e., the number of the variables) the following more general claim:\nE[maxZi] ≥ E[max Z̃i] ≥ E[maxZi]− εW− c ∑\ni∈L\nεaipi. (31)\nConsider the base case n = 1. The lemma holds immediately in Case 1 as E[Z1] = E[Z̃1].\nAssuming the lemma is true for n = k, we show it also holds for n = k + 1. Recall we have Z̃1 ∼ B(Wε , εE[Z1]/W). Thus\nE[max i≥1 Zi]− E[max i≥1 Z̃i] =a1p1 + (1− p1)E[max i≥2 Zi]− a1p1 − (1− εE[Z1]/W)E[max i≥2 Z̃i]\n≥(1− p1)E[max i≥2 Z̃i]− (1− εE[Z1]/W)E[max i≥2 Z̃i]\n=(εa1p1/W− p1)E[max i≥2 Z̃i] ≥ 0,\nwhere the first inequality follows from the induction hypothesis and the last from a1 ≥ W/ε. The other direction can be seen as follows:\nE[max i≥1 Z̃i]− E[max i≥1 Zi] =a1p1 + (1− εE[Z1]/W)E[max i≥2 Z̃i]− (a1p1 + (1− p1)E[max i≥2 Zi])\n≥(1 − εE[Z1]/W)E[max i≥2 Zi]− (1− p1)E[max i≥2 Zi]− εW − c ∑\ni∈L\\{1}\nεaipi\n≥(−εE[Z1]/W)E[max i≥2 Zi]− εW − c ∑\ni∈L\\{1}\nεaipi\n≥− εW − c ∑\ni∈L\nεaipi,\nwhere the last inequality holds since E[maxi≥2 Zi] ≤ cW. This finishes the proof of (31). Now, we show that ∑\ni∈L aipi ≤ 2W. This can be seen as follows. First, we can see from Markov inequality that Pr[maxZi > W/ε] ≤ cε. Equivalently, we have ∏\ni∈L(1− pi) ≥ 1− cε. Then, we can see that\nW ≥ ∑\ni∈L\nai ∏\nj<i\n(1 − pj)pi ≥ (1− cε) ∑\ni∈L\naipi ≥ 1\n2\n∑\ni∈L\naipi.\nPlugging this into (31), we prove the lemma.\nCorollary 1. For any set S ⊆ [m], suppose E[maxi∈S Xi] < cW, where c is a constant such that cε < 1/2. Then the following holds:\nE[max i∈S Xi] ≥ E[max i∈S X̃i] ≥ E[max i∈S Xi]− (2c+ 1)εW."
    }, {
      "heading" : "C.2.2 Signatures",
      "text" : "For each Xi, we have created its discretization X̃i = maxj{Z̃ij}. Since X̃i is a discrete distribution, we can define its Bernoulli decomposition {Yij}j∈[h] where h = |DS|. Suppose Yij ∼ B(jεW, qij). Now, we define the signature of Xi to be the vector Sig(Xi) = (Sig(Xi)1, . . . , Sig(Xi)h) where\nSig(Xi)j = min (⌊− ln (1 − qij) ε4/m ⌋ , ⌊ ln(1/ε4) ε4/m ⌋) · ε 4 m j ∈ [h].\nFor any set S, define its signature to be\nSig(S) = ∑\ni∈S\nSig(Xi).\nDefine the set SG of signature vectors to be all nonnegative h-dimensional vectors, where each coordinate is an integer multiple of ε4/m and at most m ln(1/ε4). Clearly, the size of SG is O ( ( mε−4 log(h/ε2) )h−1 ) = Õ(mO(1/ε 2)), which is polynomial for any fixed constant ε > 0\n(recall h = |DS| = O(1/ε2)). Now, we prove the following crucial lemma.\nLemma 12. Consider two sets S1 and S2. If Sig(S1) = Sig(S2), the following holds: ∣\n∣ ∣ ∣\nE[max i∈S1 X̃i]− E[max i∈S2 X̃i]\n∣ ∣ ∣ ∣ ≤ O(ε)W.\nAlgorithm 7 PTAS-K-MAX 1: U ← ∅ 2: for all signature vector sg ∈ SG do 3: Find a set S such that |S| = K and Sig(S) = sg 4: if rD(S) > rD(U) then 5: U ← S 6: end if 7: end for Output: U\nProof. Suppose {Yij}j∈[h] is the Bernoulli decomposition of X̃i. For any set S, we define Yk(S) = maxi∈S Yik (it is the max of a set of Bernoulli distributions). It is not hard to see that Yk(S) has a Bernoulli distribution B(kεW, pk(S)) with pk(S) = 1 − ∏\ni∈S(1 − qik). As Sig(S1) = Sig(S2), we have that\n|pk(S1)− pk(S2)| = | ∏\ni∈S1\n(1− qik)− ∏\ni∈S2\n(1− qik)|\n=\n∣ ∣ ∣ ∣ ∣ exp ( ∑\ni∈S1\nln(1− qik) ) − exp ( ∑\ni∈S2\nln(1− qik) )∣ ∣ ∣ ∣\n∣\n≤ 2ε4 ∀k ∈ [h].\nNoticing maxi∈S X̃i = maxk Yk(S), we have that ∣\n∣ ∣ ∣\nE[max i∈S1 X̃i]− E[max i∈S2 X̃i]\n∣ ∣ ∣ ∣ = ∣ ∣ ∣ ∣\nE[max k Yk(S1)]− E[max k Yk(S2)]\n∣ ∣ ∣ ∣\n≤W ε\n(\n∑\nk\n|pk(S1)− pk(S2)| )\n≤4hε3W = O(ε)W where the first inequality follows from Lemma 1.\nFor any signature vector sg, we associate to it a set of random variables {Bk ∼ B(kεW, 1 − e−sgk)}hk=1.8 Define the value of sg to be Val(sg) = E[maxk∈[h] Bk]. Corollary 2. For any feasible set S with Sig(S) = sg, |E[maxi∈S X̃i] − Val(sg)| ≤ O(ε)W. Moreover, combining with Corollary 1, we have that |E[maxi∈S Xi]− Val(sg)| ≤ O(ε)W."
    }, {
      "heading" : "C.2.3 Enumerating Signatures",
      "text" : "Our algorithm enumerates all signature vectors sg in SG. For each sg, we check if we can find a set S of size K such that Sig(S) = sg. This can be done by a standard dynamic program in Õ(mO(1/ε\n2)) time as follows: We use Boolean variable R[i][j][sg′] to represent whether signature vector sg′ ∈ SG can be dominated by i variables in set {X1, . . . , Xj}. The dynamic programming recursion is\nR[i][j][sg′] = R[i][j − 1][sg′] ∧R[i− 1][j − 1][sg′ − Sig(Xj)].\nIf the answer is yes (i.e., we can find such S), we say sg is a feasible signature vector and S is a candidate set. Finally, we pick the candidate set with maximum rD(S) and output the set. The pseudocode can be found in Algorithm 7.\nNow, we are ready to prove Theorem 4 by showing Algorithm 7 is a PTAS for the K-MAX problem.\nProof of Theorem 4. Suppose S∗ is the optimal solution and sg∗ is the signature of S∗. By Corollary 2, we have that |OPT− Val(sg∗)| ≤ O(ε)W.\n8 It is not hard to see the signature of maxk∈[h] Bk is exactly sg.\nAlgorithm 8 Online Submodular Maximization [26] 1: Let A1,A2, . . . ,AK be K instances of Exp3 2: for t = 1, 2, . . . do 3: // Action in the t-th round 4: for i = 1 to K do 5: Use Ai to select an arm at,i ∈ [m] 6: end for 7: Play the super arm St ← ⋃K i=1{at,i}\n8: for i = 1 to K do 9: Feed back ft( ⋃i j=1{at,j})− ft( ⋃i−1 j=1{at,j}) as the payoff Ai receives for choosing at,i\n10: end for 11: end for\nWhen Algorithm 7 is enumerating sg∗, it can find a set S such that Sig(S) = sg∗ (there exists at least one such set since S∗ is one). Therefore, we can see that\n|E[max i∈S Xi]− E[max i∈S∗ Xi]| ≤ |Val(sg∗)−max i∈S Xi|+ |Val(sg∗)− E[max i∈S∗ Xi]| ≤ O(ε)W.\nLet U be the output of Algorithm 7. Since W ≥ (1 − 1/e)OPT, we have rD(U) ≥ rD(S) = E[maxi∈S Xi] ≥ (1 −O(ε))OPT. The running time of the algorithm is polynomial for a fixed constant ε > 0, since the number of signature vectors is polynomial and the dynamic program in each iteration also runs in polynomial time. Hence, we have a PTAS for the K-MAX problem.\nRemark. In fact, Theorem 4 can be generalized in the following way: instead of the cardinality constraint |S| ≤ K , we can have more general combinatorial constraint on the feasible set S. As long as we can execute line 3 in Algorithm 7 in polynomial time, the analysis wound be the same. Using the same trick as in [20], we can extend the dynamic program to a more general class of combinatorial constraints where there is a pseudo-polynomial time for the exact version9 of the deterministic version of the corresponding problem. The class of constraints includes s-t simple paths, knapsacks, spanning trees, matchings, etc."
    }, {
      "heading" : "D Empirical Comparison between the SDCB Algorithm and Online",
      "text" : "Submodular Maximization on the K-MAX Problem\nWe perform experiments to compare the SDCB algorithm with the online submodular maximization algorithm in [26], on the K-MAX problem.\nOnline Submodular Maximization. First we briefly describe the online submodular maximization problem considered in [26] and the algorithm therein. At the beginning, an oblivious adversary sets a sequence of submodular functions f1, f2, . . . , fT on 2[m], where ft will be used to determine the reward in the t-th round. In the t-th round, if the player selects a feasible super arm St, the reward will be ft(St). This model covers the K-MAX problem as an instance: suppose X(t) = (X\n(t) 1 , . . . , X (t) m ) ∼ D is the outcome vector sampled in the t-th round, then the func-\ntion ft(S) = maxi∈S X (t) i is submodular and will determine the reward in the t-th round. We summarize the algorithm in Algorithm 8. It uses K copies of the Exp3 algorithm (see [3] for an introduction). For the K-MAX problem, Algorithm 8 achieves an O(K √ mT logm) upper bound on the (1− 1/e)-approximation regret.\nSetup. We set m = 9 and K = 3, i.e., there are 9 arms in total and it is allowed to select at most 3 arms in each round. We compare the performance of SDCB/Lazy-SDCB and the online submodular maximization algorithm on four different distributions. Here we use the greedy algorithm Greedy-K-MAX (Algorithm 5) as the offline oracle.\n9 In the exact version of a problem, we ask for a feasible set S such that total weight of S is exactly a given target value B. For example, in the exact spanning tree problem where each edge has an integer weight, we would like to find a spanning tree of weight exactly B.\nLet Xi ∼ Di (i = 1, . . . , 9). We consider the following distributions. For all of them, the optimal super arm is S∗ = {1, 2, 3}.\n• Distribution 1: All Di’s have the same support {0, 0.2, 0.4, 0.6, 0.8, 1}. For i ∈ {1, 2, 3}, Pr[Xi = 0] = Pr[Xi = 0.2] = Pr[Xi = 0.4] = Pr[Xi = 0.6] = Pr[Xi = 0.8] = 0.1 and Pr[Xi = 1] = 0.5. For i ∈ {4, 5, 6, . . . , 9}, Pr[Xi = 0] = 0.5 and Pr[Xi = 0.2] = Pr[Xi = 0.4] = Pr[Xi = 0.6] = Pr[Xi = 0.8] = Pr[Xi = 1] = 0.1. • Distribution 2: All Di’s have the same support {0, 0.2, 0.4, 0.6, 0.8, 1}. For i ∈ {1, 2, 3}, Pr[Xi = 0] = Pr[Xi = 0.2] = Pr[Xi = 0.4] = Pr[Xi = 0.6] = Pr[Xi = 0.8] = 0.1 and Pr[Xi = 1] = 0.5. For i ∈ {4, 5, 6, . . . , 9}, Pr[Xi = 0] = Pr[Xi = 0.2] = Pr[Xi = 0.4] = Pr[Xi = 0.6] = Pr[Xi = 0.8] = 0.12 and Pr[Xi = 1] = 0.4. • Distribution 3: All Di’s have the same support {0, 0.2, 0.4, 0.6, 0.8, 1}. For i ∈ {1, 2, 3}, Pr[Xi = 0] = Pr[Xi = 0.2] = Pr[Xi = 0.4] = Pr[Xi = 0.6] = Pr[Xi = 0.8] = 0.1 and Pr[Xi = 1] = 0.5. For i ∈ {4, 5, 6}, Pr[Xi = 0] = Pr[Xi = 0.2] = Pr[Xi = 0.4] = Pr[Xi = 0.6] = Pr[Xi = 0.8] = 0.12 and Pr[Xi = 1] = 0.4. For i ∈ {7, 8, 9}, Pr[Xi = 0] = Pr[Xi = 0.2] = Pr[Xi = 0.4] = Pr[Xi = 0.6] = Pr[Xi = 0.8] = 0.16 and Pr[Xi = 1] = 0.2. • Distribution 4: All Di’s are continuous distributions on [0, 1]. For i ∈ {1, 2, 3}, Di is the uniform distribution on [0, 1]. For i ∈ {4, 5, 6, . . . , 9}, the probability density function (PDF) of Xi is\nf(x) =\n{\n1.2 x ∈ [0, 0.5], 0.8 x ∈ (0.5, 1].\nThese distributions represent several different scenarios. Distribution 1 is relatively “easy” because the suboptimal arms 4-9’s distribution is far away from arms 1-3’s distribution, whereas distribution 2 is “hard” since the distribution of arms 4-9 is close to the distribution of arms 1-3. In distribution 3, the distribution of arms 4-6 is close to the distribution of arms 1-3’s, while arms 7-9’s distribution is further away. Distribution 4 is an example of a group of continuous distributions for which Lazy-SDCB is more efficient than SDCB.\nWe use SDCB for distributions 1-3, and Lazy-SDCB (with known time horizon) for distribution 4. Figure 1 shows the regrets of both SDCB and the online submodular maximization algorithm. We plot the 1-approximation regrets instead of the (1 − 1/e)-approximation regrets, since the greedy oracle usually performs much better than its (1 − 1/e)-approximation guarantee. We can see from Figure 1 that our algorithms achieve much lower regrets in all examples."
    } ],
    "references" : [ {
      "title" : "Minimax policies for adversarial and stochastic bandits",
      "author" : [ "Jean-Yves Audibert", "Sébastien Bubeck" ],
      "venue" : "In COLT,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2009
    }, {
      "title" : "Finite-time analysis of the multiarmed bandit problem",
      "author" : [ "Peter Auer", "Nicolo Cesa-Bianchi", "Paul Fischer" ],
      "venue" : "Machine learning,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2002
    }, {
      "title" : "The nonstochastic multiarmed bandit problem",
      "author" : [ "Peter Auer", "Nicolo Cesa-Bianchi", "Yoav Freund", "Robert E. Schapire" ],
      "venue" : "SIAM Journal on Computing,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2002
    }, {
      "title" : "A utility equivalence theorem for concave functions",
      "author" : [ "Anand Bhalgat", "Sanjeev Khanna" ],
      "venue" : "In IPCO,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2014
    }, {
      "title" : "Regret analysis of stochastic and nonstochastic multi-armed bandit problems",
      "author" : [ "Sébastien Bubeck", "Nicolò Cesa-Bianchi" ],
      "venue" : "Foundations and Trends in Machine Learning,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2012
    }, {
      "title" : "Combinatorial bandits",
      "author" : [ "Nicolo Cesa-Bianchi", "Gábor Lugosi" ],
      "venue" : "Journal of Computer and System Sciences,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2012
    }, {
      "title" : "Combinatorial pure exploration of multi-armed bandits",
      "author" : [ "Shouyuan Chen", "Tian Lin", "Irwin King", "Michael R. Lyu", "Wei Chen" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2014
    }, {
      "title" : "Combinatorial multi-armed bandit and its extension to probabilistically triggered arms",
      "author" : [ "Wei Chen", "Yajun Wang", "Yang Yuan", "Qinshi Wang" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2016
    }, {
      "title" : "Combinatorial bandits revisited",
      "author" : [ "Richard Combes", "M. Sadegh Talebi", "Alexandre Proutiere", "Marc Lelarge" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2015
    }, {
      "title" : "Asymptotic minimax character of the sample distribution function and of the classical multinomial estimator",
      "author" : [ "Aryeh Dvoretzky", "Jack Kiefer", "Jacob Wolfowitz" ],
      "venue" : "The Annals of Mathematical Statistics,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 1956
    }, {
      "title" : "The foundations of expected utility",
      "author" : [ "P.C. Fishburn" ],
      "venue" : "Dordrecht: Reidel,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 1982
    }, {
      "title" : "Combinatorial network optimization with unknown variables: Multi-armed bandits with linear rewards and individual observations",
      "author" : [ "Yi Gai", "Bhaskar Krishnamachari", "Rahul Jain" ],
      "venue" : "IEEE/ACM Transactions on Networking,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2012
    }, {
      "title" : "Asking the right questions: Model-driven optimization using probes",
      "author" : [ "Ashish Goel", "Sudipto Guha", "Kamesh Munagala" ],
      "venue" : "In PODS,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2006
    }, {
      "title" : "How to probe for an extreme value",
      "author" : [ "Ashish Goel", "Sudipto Guha", "Kamesh Munagala" ],
      "venue" : "ACM Transactions on Algorithms (TALG),",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2010
    }, {
      "title" : "Thompson sampling for complex online problems",
      "author" : [ "Aditya Gopalan", "Shie Mannor", "Yishay mansour" ],
      "venue" : "In ICML,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2014
    }, {
      "title" : "Matroid bandits: Fast combinatorial optimization with learning",
      "author" : [ "Branislav Kveton", "Zheng Wen", "Azin Ashkan", "Hoda Eydgahi", "Brian Eriksson" ],
      "venue" : "In UAI,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2014
    }, {
      "title" : "Combinatorial cascading bandits",
      "author" : [ "Branislav Kveton", "Zheng Wen", "Azin Ashkan", "Csaba Szepesvári" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2015
    }, {
      "title" : "Tight regret bounds for stochastic combinatorial semi-bandits",
      "author" : [ "Branislav Kveton", "Zheng Wen", "Azin Ashkan", "Csaba Szepesvári" ],
      "venue" : "In AISTATS,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2015
    }, {
      "title" : "Asymptotically efficient adaptive allocation rules",
      "author" : [ "Tze Leung Lai", "Herbert Robbins" ],
      "venue" : "Advances in applied mathematics,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 1985
    }, {
      "title" : "Maximizing expected utility for stochastic combinatorial optimization problems",
      "author" : [ "Jian Li", "Amol Deshpande" ],
      "venue" : "In FOCS,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2011
    }, {
      "title" : "Stochastic combinatorial optimization via poisson approximation",
      "author" : [ "Jian Li", "Wen Yuan" ],
      "venue" : "In STOC,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2013
    }, {
      "title" : "Combinatorial partial monitoring game with linear feedback and its applications",
      "author" : [ "Tian Lin", "Bruno Abrahao", "Robert Kleinberg", "John Lui", "Wei Chen" ],
      "venue" : "In ICML,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2014
    }, {
      "title" : "Stochastic online greedy learning with semi-bandit feedbacks",
      "author" : [ "Tian Lin", "Jian Li", "Wei Chen" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2015
    }, {
      "title" : "The tight constant in the dvoretzky-kiefer-wolfowitz inequality",
      "author" : [ "Pascal Massart" ],
      "venue" : "The Annals of Probability,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 1990
    }, {
      "title" : "An analysis of approximations for maximizing submodular set functions – I",
      "author" : [ "George L. Nemhauser", "Laurence A. Wolsey", "Marshall L. Fisher" ],
      "venue" : "Mathematical Programming,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 1978
    }, {
      "title" : "An online algorithm for maximizing submodular functions",
      "author" : [ "Matthew Streeter", "Daniel Golovin" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2008
    } ],
    "referenceMentions" : [ {
      "referenceID" : 18,
      "context" : "MAB and its variants have been extensively studied in the literature, with classical results such as tight Θ(logT ) distribution-dependent and Θ( √ T ) distribution-independent upper and lower bounds on the regret in T rounds [19, 2, 1].",
      "startOffset" : 226,
      "endOffset" : 236
    }, {
      "referenceID" : 1,
      "context" : "MAB and its variants have been extensively studied in the literature, with classical results such as tight Θ(logT ) distribution-dependent and Θ( √ T ) distribution-independent upper and lower bounds on the regret in T rounds [19, 2, 1].",
      "startOffset" : 226,
      "endOffset" : 236
    }, {
      "referenceID" : 0,
      "context" : "MAB and its variants have been extensively studied in the literature, with classical results such as tight Θ(logT ) distribution-dependent and Θ( √ T ) distribution-independent upper and lower bounds on the regret in T rounds [19, 2, 1].",
      "startOffset" : 226,
      "endOffset" : 236
    }, {
      "referenceID" : 11,
      "context" : "Therefore CMAB has attracted a lot of attention in online learning research in recent years [12, 8, 22, 15, 7, 16, 18, 17, 23, 9].",
      "startOffset" : 92,
      "endOffset" : 129
    }, {
      "referenceID" : 7,
      "context" : "Therefore CMAB has attracted a lot of attention in online learning research in recent years [12, 8, 22, 15, 7, 16, 18, 17, 23, 9].",
      "startOffset" : 92,
      "endOffset" : 129
    }, {
      "referenceID" : 21,
      "context" : "Therefore CMAB has attracted a lot of attention in online learning research in recent years [12, 8, 22, 15, 7, 16, 18, 17, 23, 9].",
      "startOffset" : 92,
      "endOffset" : 129
    }, {
      "referenceID" : 14,
      "context" : "Therefore CMAB has attracted a lot of attention in online learning research in recent years [12, 8, 22, 15, 7, 16, 18, 17, 23, 9].",
      "startOffset" : 92,
      "endOffset" : 129
    }, {
      "referenceID" : 6,
      "context" : "Therefore CMAB has attracted a lot of attention in online learning research in recent years [12, 8, 22, 15, 7, 16, 18, 17, 23, 9].",
      "startOffset" : 92,
      "endOffset" : 129
    }, {
      "referenceID" : 15,
      "context" : "Therefore CMAB has attracted a lot of attention in online learning research in recent years [12, 8, 22, 15, 7, 16, 18, 17, 23, 9].",
      "startOffset" : 92,
      "endOffset" : 129
    }, {
      "referenceID" : 17,
      "context" : "Therefore CMAB has attracted a lot of attention in online learning research in recent years [12, 8, 22, 15, 7, 16, 18, 17, 23, 9].",
      "startOffset" : 92,
      "endOffset" : 129
    }, {
      "referenceID" : 16,
      "context" : "Therefore CMAB has attracted a lot of attention in online learning research in recent years [12, 8, 22, 15, 7, 16, 18, 17, 23, 9].",
      "startOffset" : 92,
      "endOffset" : 129
    }, {
      "referenceID" : 22,
      "context" : "Therefore CMAB has attracted a lot of attention in online learning research in recent years [12, 8, 22, 15, 7, 16, 18, 17, 23, 9].",
      "startOffset" : 92,
      "endOffset" : 129
    }, {
      "referenceID" : 8,
      "context" : "Therefore CMAB has attracted a lot of attention in online learning research in recent years [12, 8, 22, 15, 7, 16, 18, 17, 23, 9].",
      "startOffset" : 92,
      "endOffset" : 129
    }, {
      "referenceID" : 7,
      "context" : "Even for studies that do generalize to non-linear reward functions, they typically still assume that the expected reward for choosing a super arm is a function of the expected outcomes from the constituent base arms in this super arm [8, 17].",
      "startOffset" : 234,
      "endOffset" : 241
    }, {
      "referenceID" : 16,
      "context" : "Even for studies that do generalize to non-linear reward functions, they typically still assume that the expected reward for choosing a super arm is a function of the expected outcomes from the constituent base arms in this super arm [8, 17].",
      "startOffset" : 234,
      "endOffset" : 241
    }, {
      "referenceID" : 19,
      "context" : "Beyond the K-MAX problem, many expected utility maximization (EUM) problems are studied in stochastic optimization literature [27, 20, 21, 4].",
      "startOffset" : 126,
      "endOffset" : 141
    }, {
      "referenceID" : 20,
      "context" : "Beyond the K-MAX problem, many expected utility maximization (EUM) problems are studied in stochastic optimization literature [27, 20, 21, 4].",
      "startOffset" : 126,
      "endOffset" : 141
    }, {
      "referenceID" : 3,
      "context" : "Beyond the K-MAX problem, many expected utility maximization (EUM) problems are studied in stochastic optimization literature [27, 20, 21, 4].",
      "startOffset" : 126,
      "endOffset" : 141
    }, {
      "referenceID" : 11,
      "context" : "As already mentioned, most relevant to our work are studies on CMAB frameworks, among which [12, 16, 18, 9] focus on linear reward functions while [8, 17] look into nonlinear reward functions.",
      "startOffset" : 92,
      "endOffset" : 107
    }, {
      "referenceID" : 15,
      "context" : "As already mentioned, most relevant to our work are studies on CMAB frameworks, among which [12, 16, 18, 9] focus on linear reward functions while [8, 17] look into nonlinear reward functions.",
      "startOffset" : 92,
      "endOffset" : 107
    }, {
      "referenceID" : 17,
      "context" : "As already mentioned, most relevant to our work are studies on CMAB frameworks, among which [12, 16, 18, 9] focus on linear reward functions while [8, 17] look into nonlinear reward functions.",
      "startOffset" : 92,
      "endOffset" : 107
    }, {
      "referenceID" : 8,
      "context" : "As already mentioned, most relevant to our work are studies on CMAB frameworks, among which [12, 16, 18, 9] focus on linear reward functions while [8, 17] look into nonlinear reward functions.",
      "startOffset" : 92,
      "endOffset" : 107
    }, {
      "referenceID" : 7,
      "context" : "As already mentioned, most relevant to our work are studies on CMAB frameworks, among which [12, 16, 18, 9] focus on linear reward functions while [8, 17] look into nonlinear reward functions.",
      "startOffset" : 147,
      "endOffset" : 154
    }, {
      "referenceID" : 16,
      "context" : "As already mentioned, most relevant to our work are studies on CMAB frameworks, among which [12, 16, 18, 9] focus on linear reward functions while [8, 17] look into nonlinear reward functions.",
      "startOffset" : 147,
      "endOffset" : 154
    }, {
      "referenceID" : 7,
      "context" : "[8] look at general non-linear reward functions and Kveton et al.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 16,
      "context" : "[17] consider specific non-linear reward functions in a conjunctive or disjunctive form, but both papers require that the expected reward of playing a super arm is determined by the expected outcomes from base arms.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 14,
      "context" : "The only work in combinatorial bandits we are aware of that does not require the above assumption on the expected reward is [15], which is based on a general Thompson sampling framework.",
      "startOffset" : 124,
      "endOffset" : 128
    }, {
      "referenceID" : 4,
      "context" : "There are extensive studies on the classical MAB problem, for which we refer to a survey by Bubeck and Cesa-Bianchi [5].",
      "startOffset" : 116,
      "endOffset" : 119
    }, {
      "referenceID" : 25,
      "context" : "[26, 6].",
      "startOffset" : 0,
      "endOffset" : 7
    }, {
      "referenceID" : 5,
      "context" : "[26, 6].",
      "startOffset" : 0,
      "endOffset" : 7
    }, {
      "referenceID" : 19,
      "context" : "[27, 20, 21, 4]).",
      "startOffset" : 0,
      "endOffset" : 15
    }, {
      "referenceID" : 20,
      "context" : "[27, 20, 21, 4]).",
      "startOffset" : 0,
      "endOffset" : 15
    }, {
      "referenceID" : 3,
      "context" : "[27, 20, 21, 4]).",
      "startOffset" : 0,
      "endOffset" : 15
    }, {
      "referenceID" : 12,
      "context" : "The K-MAX problem may be traced back to [13], where Goel et al.",
      "startOffset" : 40,
      "endOffset" : 44
    }, {
      "referenceID" : 0,
      "context" : ",m} is a set of m (base) arms, F ⊆ 2 is a set of subsets of E, D is a probability distribution over [0, 1], and R is a reward function defined on [0, 1] × F .",
      "startOffset" : 100,
      "endOffset" : 106
    }, {
      "referenceID" : 0,
      "context" : ",m} is a set of m (base) arms, F ⊆ 2 is a set of subsets of E, D is a probability distribution over [0, 1], and R is a reward function defined on [0, 1] × F .",
      "startOffset" : 146,
      "endOffset" : 152
    }, {
      "referenceID" : 0,
      "context" : "Therefore, we can alternatively express R(x, S) as RS(xS), where RS is a function defined on [0, 1] .",
      "startOffset" : 93,
      "endOffset" : 99
    }, {
      "referenceID" : 19,
      "context" : "We remark that the above independence assumption is also made for past studies on the offline EUM and K-MAX problems [27, 20, 21, 4, 13], so it is not an extra assumption for the online learning case.",
      "startOffset" : 117,
      "endOffset" : 136
    }, {
      "referenceID" : 20,
      "context" : "We remark that the above independence assumption is also made for past studies on the offline EUM and K-MAX problems [27, 20, 21, 4, 13], so it is not an extra assumption for the online learning case.",
      "startOffset" : 117,
      "endOffset" : 136
    }, {
      "referenceID" : 3,
      "context" : "We remark that the above independence assumption is also made for past studies on the offline EUM and K-MAX problems [27, 20, 21, 4, 13], so it is not an extra assumption for the online learning case.",
      "startOffset" : 117,
      "endOffset" : 136
    }, {
      "referenceID" : 12,
      "context" : "We remark that the above independence assumption is also made for past studies on the offline EUM and K-MAX problems [27, 20, 21, 4, 13], so it is not an extra assumption for the online learning case.",
      "startOffset" : 117,
      "endOffset" : 136
    }, {
      "referenceID" : 0,
      "context" : "There exists M > 0 such that for any x ∈ [0, 1] and any S ∈ F , we have 0 ≤ R(x, S) ≤ M .",
      "startOffset" : 41,
      "endOffset" : 47
    }, {
      "referenceID" : 0,
      "context" : "If two vectors x, x ∈ [0, 1] satisfy xi ≤ xi (∀i ∈ [m]), then for any S ∈ F , we have R(x, S) ≤ R(x, S).",
      "startOffset" : 22,
      "endOffset" : 28
    }, {
      "referenceID" : 0,
      "context" : "3 SDCB Algorithm [0, 1] is isomorphic to [0, 1]; the coordinates in [0, 1] are indexed by elements in S.",
      "startOffset" : 17,
      "endOffset" : 23
    }, {
      "referenceID" : 0,
      "context" : "3 SDCB Algorithm [0, 1] is isomorphic to [0, 1]; the coordinates in [0, 1] are indexed by elements in S.",
      "startOffset" : 41,
      "endOffset" : 47
    }, {
      "referenceID" : 0,
      "context" : "3 SDCB Algorithm [0, 1] is isomorphic to [0, 1]; the coordinates in [0, 1] are indexed by elements in S.",
      "startOffset" : 68,
      "endOffset" : 74
    }, {
      "referenceID" : 0,
      "context" : "We also maintain the empirical distribution D̂i of the observed outcomes from arm i so far, which can be represented by its CDF F̂i: for x ∈ [0, 1], the value of F̂i(x) is just the fraction of the observed outcomes from arm i that are no larger than x.",
      "startOffset" : 141,
      "endOffset" : 147
    }, {
      "referenceID" : 0,
      "context" : "Our algorithm ensures that with high probability we have Fi(x) ≤ Fi(x) simultaneously for all i ∈ [m] and all x ∈ [0, 1], where Fi is the CDF of the outcome distribution Di.",
      "startOffset" : 114,
      "endOffset" : 120
    }, {
      "referenceID" : 0,
      "context" : "We remark that while Fi(x) is a numerical lower confidence bound on Fi(x) for all x ∈ [0, 1], at the distribution level, Di serves as a “stochastically dominant (upper) confidence bound” on Di.",
      "startOffset" : 86,
      "endOffset" : 92
    }, {
      "referenceID" : 17,
      "context" : "The main idea is to reduce our analysis on general reward functions satisfying Assumptions 1-3 to the one in [18] that deals with the summation reward functionR(x, S) = ∑ i∈S xi.",
      "startOffset" : 109,
      "endOffset" : 113
    }, {
      "referenceID" : 9,
      "context" : "Our analysis relies on the Dvoretzky-Kiefer-Wolfowitz inequality [10, 24], which gives a uniform concentration bound on the empirical CDF of a distribution.",
      "startOffset" : 65,
      "endOffset" : 73
    }, {
      "referenceID" : 23,
      "context" : "Our analysis relies on the Dvoretzky-Kiefer-Wolfowitz inequality [10, 24], which gives a uniform concentration bound on the empirical CDF of a distribution.",
      "startOffset" : 65,
      "endOffset" : 73
    }, {
      "referenceID" : 7,
      "context" : "Although our focus is on general reward functions, we note that when SDCB is applied to the previous CMAB framework where the expected reward depends only on the means of the random variables, it can achieve the same regret bounds as the previous combinatorial upper confidence bound (CUCB) algorithm in [8, 18].",
      "startOffset" : 304,
      "endOffset" : 311
    }, {
      "referenceID" : 17,
      "context" : "Although our focus is on general reward functions, we note that when SDCB is applied to the previous CMAB framework where the expected reward depends only on the means of the random variables, it can achieve the same regret bounds as the previous combinatorial upper confidence bound (CUCB) algorithm in [8, 18].",
      "startOffset" : 304,
      "endOffset" : 311
    }, {
      "referenceID" : 7,
      "context" : ") Hence, the analysis in [8, 18] can be applied to SDCB, resulting in the same regret bounds.",
      "startOffset" : 25,
      "endOffset" : 32
    }, {
      "referenceID" : 17,
      "context" : ") Hence, the analysis in [8, 18] can be applied to SDCB, resulting in the same regret bounds.",
      "startOffset" : 25,
      "endOffset" : 32
    }, {
      "referenceID" : 17,
      "context" : "We further remark that in this case we do not need the three assumptions stated in Section 2 (in particular the independence assumption on Xi’s): the summation reward case just works as in [18] and the nonlinear reward case relies on the properties of monotonicity and bounded smoothness used in [8].",
      "startOffset" : 189,
      "endOffset" : 193
    }, {
      "referenceID" : 7,
      "context" : "We further remark that in this case we do not need the three assumptions stated in Section 2 (in particular the independence assumption on Xi’s): the summation reward case just works as in [18] and the nonlinear reward case relies on the properties of monotonicity and bounded smoothness used in [8].",
      "startOffset" : 296,
      "endOffset" : 299
    }, {
      "referenceID" : 0,
      "context" : "There exists C > 0 such that for any S ∈ F and any x, x ∈ [0, 1], we have |R(x, S)−R(x′, S)| ≤ C‖xS − xS‖1, where ‖xS − xS‖1 = ∑ i∈S |xi − xi|.",
      "startOffset" : 58,
      "endOffset" : 64
    }, {
      "referenceID" : 0,
      "context" : "Specifically, we partition [0, 1] into s intervals: I1 = [0, 1s ], I2 = ( 1 s , 2 s ], .",
      "startOffset" : 27,
      "endOffset" : 33
    }, {
      "referenceID" : 13,
      "context" : "It can be implied by a result in [14] that finding the exact optimal solution is NP-hard, so we resort to approximation algorithms.",
      "startOffset" : 33,
      "endOffset" : 37
    }, {
      "referenceID" : 25,
      "context" : "Streeter and Golovin [26] study an online submodular maximization problem in the oblivious adversary model.",
      "startOffset" : 21,
      "endOffset" : 25
    }, {
      "referenceID" : 25,
      "context" : "While the techniques in [26] can only give a bound on the (1 − 1/e)-approximation regret for K-MAX, we can obtain the first Õ( √ T ) bound on the (1− ǫ)-approximation regret for any constant ǫ > 0, using our PTAS as the offline oracle.",
      "startOffset" : 24,
      "endOffset" : 28
    }, {
      "referenceID" : 25,
      "context" : "Even when we use the simple greedy algorithm as the oracle, our experiments show that SDCB performs significantly better than the algorithm in [26] (see Appendix D).",
      "startOffset" : 143,
      "endOffset" : 147
    }, {
      "referenceID" : 10,
      "context" : ", [11]), while linear utility functions correspond to risk-neutrality.",
      "startOffset" : 2,
      "endOffset" : 6
    }, {
      "referenceID" : 19,
      "context" : "Li and Deshpande [20] obtain a PTAS for the expected utility maximization (EUM) problem for several classes of utility functions (including for example increasing concave functions which typically indicate risk-averseness), and a large class of feasibility constraints (including cardinality constraint, s-t simple paths, matchings, and knapsacks).",
      "startOffset" : 17,
      "endOffset" : 21
    }, {
      "referenceID" : 20,
      "context" : "Similar results for other utility functions and feasibility constraints can be found in [27, 21, 4].",
      "startOffset" : 88,
      "endOffset" : 99
    }, {
      "referenceID" : 3,
      "context" : "Similar results for other utility functions and feasibility constraints can be found in [27, 21, 4].",
      "startOffset" : 88,
      "endOffset" : 99
    }, {
      "referenceID" : 0,
      "context" : "References [1] Jean-Yves Audibert and Sébastien Bubeck.",
      "startOffset" : 11,
      "endOffset" : 14
    }, {
      "referenceID" : 1,
      "context" : "[2] Peter Auer, Nicolo Cesa-Bianchi, and Paul Fischer.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 2,
      "context" : "[3] Peter Auer, Nicolo Cesa-Bianchi, Yoav Freund, and Robert E.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 3,
      "context" : "[4] Anand Bhalgat and Sanjeev Khanna.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 4,
      "context" : "[5] Sébastien Bubeck and Nicolò Cesa-Bianchi.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 5,
      "context" : "[6] Nicolo Cesa-Bianchi and Gábor Lugosi.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 6,
      "context" : "[7] Shouyuan Chen, Tian Lin, Irwin King, Michael R.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 7,
      "context" : "[8] Wei Chen, Yajun Wang, Yang Yuan, and Qinshi Wang.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 8,
      "context" : "[9] Richard Combes, M.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 9,
      "context" : "[10] Aryeh Dvoretzky, Jack Kiefer, and Jacob Wolfowitz.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 10,
      "context" : "[11] P.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 11,
      "context" : "[12] Yi Gai, Bhaskar Krishnamachari, and Rahul Jain.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 12,
      "context" : "[13] Ashish Goel, Sudipto Guha, and Kamesh Munagala.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 13,
      "context" : "[14] Ashish Goel, Sudipto Guha, and Kamesh Munagala.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 14,
      "context" : "[15] Aditya Gopalan, Shie Mannor, and Yishay mansour.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 15,
      "context" : "[16] Branislav Kveton, Zheng Wen, Azin Ashkan, Hoda Eydgahi, and Brian Eriksson.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 16,
      "context" : "[17] Branislav Kveton, Zheng Wen, Azin Ashkan, and Csaba Szepesvári.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 17,
      "context" : "[18] Branislav Kveton, Zheng Wen, Azin Ashkan, and Csaba Szepesvári.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 18,
      "context" : "[19] Tze Leung Lai and Herbert Robbins.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 19,
      "context" : "[20] Jian Li and Amol Deshpande.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 20,
      "context" : "[21] Jian Li and Wen Yuan.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 21,
      "context" : "[22] Tian Lin, Bruno Abrahao, Robert Kleinberg, John Lui, and Wei Chen.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 22,
      "context" : "[23] Tian Lin, Jian Li, and Wei Chen.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 23,
      "context" : "[24] Pascal Massart.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 24,
      "context" : "[25] George L.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 25,
      "context" : "[26] Matthew Streeter and Daniel Golovin.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 9,
      "context" : "7 Then we have: Lemma 2 (Dvoretzky-Kiefer-Wolfowitz inequality [10, 24]).",
      "startOffset" : 63,
      "endOffset" : 71
    }, {
      "referenceID" : 23,
      "context" : "7 Then we have: Lemma 2 (Dvoretzky-Kiefer-Wolfowitz inequality [10, 24]).",
      "startOffset" : 63,
      "endOffset" : 71
    }, {
      "referenceID" : 0,
      "context" : "Let P = P1 × · · · × Pm and P ′ = P ′ 1 × · · · × P ′ m be two probability distributions over [0, 1].",
      "startOffset" : 94,
      "endOffset" : 100
    }, {
      "referenceID" : 0,
      "context" : "(i) If for any i ∈ [m], x ∈ [0, 1] we have F ′ i (x) ≤ Fi(x), then for any super arm S ∈ F , we have rP ′(S) ≥ rP (S).",
      "startOffset" : 28,
      "endOffset" : 34
    }, {
      "referenceID" : 0,
      "context" : "(ii) If for any i ∈ [m], x ∈ [0, 1] we have Fi(x) − F ′ i (x) ≤ Λi (Λi > 0), then for any super arm S ∈ F , we have rP ′(S)− rP (S) ≤ 2M ∑",
      "startOffset" : 29,
      "endOffset" : 35
    }, {
      "referenceID" : 0,
      "context" : "If we have F ′ i (x) ≤ Fi(x) for all i ∈ [m] and x ∈ [0, 1], then for all i, P ′ i has first-order stochastic dominance over Pi.",
      "startOffset" : 53,
      "endOffset" : 59
    }, {
      "referenceID" : 0,
      "context" : "Recall that the reward function R(x, S) has a monotonicity property (Assumption 3): if x and x are two vectors in [0, 1] such that xi ≤ xi for all i ∈ [m], then R(x, S) ≤ R(x, S) for all S ∈ F .",
      "startOffset" : 114,
      "endOffset" : 120
    }, {
      "referenceID" : 0,
      "context" : "Let P ′′ = P ′′ 1 × · · · × P ′′ m be a distribution over [0, 1] such that the CDF of P ′′ i is the following: F ′′ i (x) = { max{Fi(x)− Λi, 0}, 0 ≤ x < 1, 1, x = 1.",
      "startOffset" : 58,
      "endOffset" : 64
    }, {
      "referenceID" : 0,
      "context" : "(2) It is easy to see that F ′′ i (x) ≤ F ′ i (x) for all i ∈ [m] and x ∈ [0, 1].",
      "startOffset" : 74,
      "endOffset" : 80
    }, {
      "referenceID" : 17,
      "context" : "The following lemma is similar to Lemma 1 in [18].",
      "startOffset" : 45,
      "endOffset" : 49
    }, {
      "referenceID" : 0,
      "context" : "there exists i ∈ [m] such that sup x∈[0,1] ∣",
      "startOffset" : 37,
      "endOffset" : 42
    }, {
      "referenceID" : 0,
      "context" : "sup x∈[0,1] ∣",
      "startOffset" : 6,
      "endOffset" : 11
    }, {
      "referenceID" : 0,
      "context" : "∣ < ci ∀i ∈ [m], x ∈ [0, 1].",
      "startOffset" : 21,
      "endOffset" : 27
    }, {
      "referenceID" : 0,
      "context" : "(12) From (11) and (12) we know that Fi(x) ≤ Fi(x) ≤ Fi(x) + 2ci for all i ∈ [m], x ∈ [0, 1].",
      "startOffset" : 86,
      "endOffset" : 92
    }, {
      "referenceID" : 17,
      "context" : "4 Finishing the Proof of Theorem 1 Lemma 4 is very similar to Lemma 1 in [18].",
      "startOffset" : 73,
      "endOffset" : 77
    }, {
      "referenceID" : 17,
      "context" : "We now apply the counting argument in [18] to finish the proof of Theorem 1.",
      "startOffset" : 38,
      "endOffset" : 42
    }, {
      "referenceID" : 17,
      "context" : "We choose {αk} and {βk} as in Theorem 4 of [18], which satisfy √ 6 ∞ ∑",
      "startOffset" : 43,
      "endOffset" : 47
    }, {
      "referenceID" : 7,
      "context" : "Algorithm 4 CUCB [8, 18] 1: For each arm i, maintain: (i) μ̂i, the average of all observed outcomes from arm i so far, and (ii) Ti, the number of observed outcomes from arm i so far.",
      "startOffset" : 17,
      "endOffset" : 24
    }, {
      "referenceID" : 17,
      "context" : "Algorithm 4 CUCB [8, 18] 1: For each arm i, maintain: (i) μ̂i, the average of all observed outcomes from arm i so far, and (ii) Ti, the number of observed outcomes from arm i so far.",
      "startOffset" : 17,
      "endOffset" : 24
    }, {
      "referenceID" : 7,
      "context" : "We summarize the CUCB algorithm [8, 18] in Algorithm 4.",
      "startOffset" : 32,
      "endOffset" : 39
    }, {
      "referenceID" : 17,
      "context" : "We summarize the CUCB algorithm [8, 18] in Algorithm 4.",
      "startOffset" : 32,
      "endOffset" : 39
    }, {
      "referenceID" : 0,
      "context" : "Let P andP ′ be two distributions over [0, 1]with CDFs F andF , respectively.",
      "startOffset" : 39,
      "endOffset" : 45
    }, {
      "referenceID" : 0,
      "context" : "(i) If for all x ∈ [0, 1] we have F (x) ≤ F (x), then we have E[Y ] ≤ E[Y ].",
      "startOffset" : 19,
      "endOffset" : 25
    }, {
      "referenceID" : 0,
      "context" : "(ii) If for all x ∈ [0, 1] we have F (x)− F (x) ≤ Λ (Λ > 0), then we have E[Y ] ≤ E[Y ] + Λ.",
      "startOffset" : 20,
      "endOffset" : 26
    }, {
      "referenceID" : 0,
      "context" : "3 ln t 2Ti,t−1 ≤ Fi(x) ≤ Fi(x) (24) for all i ∈ [m] and x ∈ [0, 1], where Fi is the CDF of Di used in round t of SDCB, and Fi is the CDF of Di.",
      "startOffset" : 60,
      "endOffset" : 66
    }, {
      "referenceID" : 7,
      "context" : "With this property, the analysis in [8, 18] can also be applied to SDCB, resulting in exactly the same regret bounds.",
      "startOffset" : 36,
      "endOffset" : 43
    }, {
      "referenceID" : 17,
      "context" : "With this property, the analysis in [8, 18] can also be applied to SDCB, resulting in exactly the same regret bounds.",
      "startOffset" : 36,
      "endOffset" : 43
    }, {
      "referenceID" : 0,
      "context" : "Let g(x) be a Lipschitz continuous function on [0, 1] such that for any x, x ∈ [0, 1], we have |g(x)− g(x′)| ≤ C‖x− x‖1, where ‖x− x‖1 = ∑n i=1 |xi − xi|.",
      "startOffset" : 47,
      "endOffset" : 53
    }, {
      "referenceID" : 0,
      "context" : "Let g(x) be a Lipschitz continuous function on [0, 1] such that for any x, x ∈ [0, 1], we have |g(x)− g(x′)| ≤ C‖x− x‖1, where ‖x− x‖1 = ∑n i=1 |xi − xi|.",
      "startOffset" : 79,
      "endOffset" : 85
    }, {
      "referenceID" : 0,
      "context" : "Let P = P1 × · · · ×Pn be a probability distribution over [0, 1].",
      "startOffset" : 58,
      "endOffset" : 64
    }, {
      "referenceID" : 0,
      "context" : "Define another distribution P̃ = P̃1×· · ·× P̃n over [0, 1] as follows: each P̃i (i ∈ [n]) takes values in { 1s , 2s , .",
      "startOffset" : 53,
      "endOffset" : 59
    }, {
      "referenceID" : 0,
      "context" : "We define two functions on [0, 1]: h(x1, .",
      "startOffset" : 27,
      "endOffset" : 33
    }, {
      "referenceID" : 0,
      "context" : ", xk−1 ∈ [0, 1], the function g(x1, .",
      "startOffset" : 9,
      "endOffset" : 15
    }, {
      "referenceID" : 0,
      "context" : ", xk−1, x) on x ∈ [0, 1] is Lipschitz continuous.",
      "startOffset" : 18,
      "endOffset" : 24
    }, {
      "referenceID" : 0,
      "context" : ", xk−1 ∈ [0, 1].",
      "startOffset" : 9,
      "endOffset" : 15
    }, {
      "referenceID" : 0,
      "context" : ", xk−1) ∈ [0, 1] is Lipschitz continuous.",
      "startOffset" : 10,
      "endOffset" : 16
    }, {
      "referenceID" : 0,
      "context" : "According to Assumption 4, the function RS defined on [0, 1] is Lipschitz continuous.",
      "startOffset" : 54,
      "endOffset" : 60
    }, {
      "referenceID" : 0,
      "context" : ", vi,si} ⊂ [0, 1], and D = D1 × · · · ×Dm is the joint distribution of X = (X1, .",
      "startOffset" : 11,
      "endOffset" : 17
    }, {
      "referenceID" : 0,
      "context" : "For any x ∈ [0, 1], let fx(S) = maxi∈S xi be a set function defined on 2.",
      "startOffset" : 12,
      "endOffset" : 18
    }, {
      "referenceID" : 24,
      "context" : "According to the classical result on submodular maximization [25], the greedy algorithm can find a (1− 1/e)-approximate solution to maxS⊆[m],|S|≤K{rD(S)}.",
      "startOffset" : 61,
      "endOffset" : 65
    }, {
      "referenceID" : 25,
      "context" : "Algorithm 8 Online Submodular Maximization [26] 1: Let A1,A2, .",
      "startOffset" : 43,
      "endOffset" : 47
    }, {
      "referenceID" : 19,
      "context" : "Using the same trick as in [20], we can extend the dynamic program to a more general class of combinatorial constraints where there is a pseudo-polynomial time for the exact version9 of the deterministic version of the corresponding problem.",
      "startOffset" : 27,
      "endOffset" : 31
    }, {
      "referenceID" : 25,
      "context" : "D Empirical Comparison between the SDCB Algorithm and Online Submodular Maximization on the K-MAX Problem We perform experiments to compare the SDCB algorithm with the online submodular maximization algorithm in [26], on the K-MAX problem.",
      "startOffset" : 212,
      "endOffset" : 216
    }, {
      "referenceID" : 25,
      "context" : "First we briefly describe the online submodular maximization problem considered in [26] and the algorithm therein.",
      "startOffset" : 83,
      "endOffset" : 87
    }, {
      "referenceID" : 2,
      "context" : "It uses K copies of the Exp3 algorithm (see [3] for an introduction).",
      "startOffset" : 44,
      "endOffset" : 47
    }, {
      "referenceID" : 0,
      "context" : "• Distribution 4: All Di’s are continuous distributions on [0, 1].",
      "startOffset" : 59,
      "endOffset" : 65
    }, {
      "referenceID" : 0,
      "context" : "For i ∈ {1, 2, 3}, Di is the uniform distribution on [0, 1].",
      "startOffset" : 53,
      "endOffset" : 59
    } ],
    "year" : 2017,
    "abstractText" : "In this paper, we study the stochastic combinatorial multi-armed bandit (CMAB) framework that allows a general nonlinear reward function, whose expected value may not depend only on the means of the input random variables but possibly on the entire distributions of these variables. Our framework enables a much larger class of reward functions such as the max() function and nonlinear utility functions. Existing techniques relying on accurate estimations of the means of random variables, such as the upper confidence bound (UCB) technique, do not work directly on these functions. We propose a new algorithm called stochastically dominant confidence bound (SDCB), which estimates the distributions of underlying random variables and their stochastically dominant confidence bounds. We prove that SDCB can achieve O(log T ) distribution-dependent regret and Õ( √ T ) distribution-independent regret, where T is the time horizon. We apply our results to the K-MAX problem and expected utility maximization problems. In particular, for K-MAX, we provide the first polynomial-time approximation scheme (PTAS) for its offline problem, and give the first Õ( √ T ) bound on the (1 − ǫ)approximation regret of its online problem, for any ǫ > 0.",
    "creator" : "LaTeX with hyperref package"
  }
}