{
  "name" : "1303.4172.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Margins, Shrinkage, and Boosting",
    "authors" : [ "Matus Telgarsky" ],
    "emails" : [ "mtelgars@cs.ucsd.edu" ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "AdaBoost and related boosting algorithms greedily aggregate many simple predictors into a single accurate predictor (Freund & Schapire, 1997). One explanation for the efficacy of boosting is that it not only seeks aggregates with low empirical risk, but moreover that it prefers good margins, which leads to improved generalization (Schapire et al., 1997). Since AdaBoost does not attain maximum margins on general instances, a push was made to develop methods which carry such a guarantee (Rätsch & Warmuth, 2005; Shalev-Shwartz & Singer, 2008; Rudin et al., 2007).\nThis work shows that margin maximization may be achieved by scaling back the step size. The intuition for this result is simple (cf. Figure 1): when (equivalently) considered as steps in a coordinate descent procedure, the iterates, depicted as a path, approximate the path of constrained optima (for all possible choices of constraint). By scaling back the step size, the optimal path is more finely approximated.\nProceedings of the 30 th International Conference on Machine Learning, Atlanta, Georgia, USA, 2013. JMLR: W&CP volume 28. Copyright 2013 by the author(s).\nAs there have been many proposed step sizes for these methods, this manuscript will study four separate choices, deriving improved bounds for the more regularized choices. While it has been shown before that regularized step sizes have good generalization and asymptotically good margins (Zhang & Yu, 2005), this manuscript shows that straightforward step choices achieve these margins at rates matching explicitly margin-maximizing boosting methods.\nThis practice of scaling back weights was proposed by Friedman (2000, Section 5), who referred to it as a shrinkage scheme (Copas, 1983). This scheme is effective, and adopted in practice (see for instance Bradski (2000, Class CvGBTrees) and Pedregosa et al. (2011, Class GradientBoostingClassifier)); the purpose of this manuscript is to provide theoretical guarantees."
    }, {
      "heading" : "1.1. Outline",
      "text" : "After summarizing the main content, this introduction closes with connections to related work; thereafter, Section 2 recalls the core algorithm, defines the class of loss functions, and provides the four step sizes.\nar X\niv :1\n30 3.\n41 72\nv1 [\ncs .L\nG ]\n1 8\nM ar\n2 01\n3\nAs boosting is generally studied under the weak learning assumption (a separability condition), the dominant study in this manuscript is also under the condition of separability, and appears in Section 3. The first step is to show that shrinkage does not drastically change the rate of convergence of the empirical risk under these methods. The more involved study is on the topic of margins, and the final subsection compares these bounds to those of other methods.\nGeneral (potentially nonseparable) instances are discussed in Section 4. Once again, the first step is a convergence rate guarantee, which again matches those without shrinkage. This section also demonstrates that, under a certain decomposition of boosting problems, the algorithm is still achieving margins on a separable sub-component of the problem.\nThe manuscript closes with some discussion in Section 5. All proofs are relegated to appendices (in the supplementary material)."
    }, {
      "heading" : "1.2. Related Work",
      "text" : "Three close works proposed regularized line searches for boosting. First, Friedman (2000) gave the same scheme as is considered here (albeit with only the optimal line search); follow-up work has been mainly empirical, and the questions of convergence rates and margin guarantees do not appear in the literature. Second, Zhang & Yu (2005) also considered regularized line searches, but with a goal of proving consistency; margin maximization is proved as a byproduct, and the analogous results here hold under fewer conditions, and come with rates for the more stringent step sizes. A third work, due to Rätsch et al. (2001), also proves margin maximizing properties of regularized line searches, but again without rates.\nAs mentioned in the introduction, margin maximization properties of AdaBoost have received extensive study; an excellent survey of results with pointers to other literature is provided by Schapire & Freund (2012, Chapter 5). Amongst these, a crucial result, due to Rudin et al. (2004), provides a concrete input to AdaBoost which yields suboptimal margins (which is used in Section 3.3); that work also studies the evolution of these margins as a dynamical system, a topic which will reappear in Section 5.\nThe primary contribution of this manuscript is to exhibit margin maximization, thus a natural comparison is to other algorithms with this same guarantee, for instance the works of Rätsch & Warmuth (2005), ShalevShwartz & Singer (2008), and Rudin et al. (2007) (or again refer to Schapire & Freund (2012, Chapter 5,\nBibliographic Notes) for a more extensive summary). This manuscript will briefly compare with the methods of Shalev-Shwartz & Singer (2008), which subsume some earlier results and match the best guarantees, along with giving a simple, general, greedy scheme. The key distinction between previous work and the present work is firstly that the algorithmic modifications here are minor (in particular, the form of unregularized empirical risk minimization is unchanged), and that properties of an existing, widely used method are discerned (namely, the shrinkage procedure presented by Friedman (2000)).\nAs is standard in the above works, this manuscript is only concerned with convergence of empirical quantities.\nIn order to prove convergence rates, this work relies heavily on techniques due to Telgarsky (2012). In particular, the scheme to prove convergence rates of empirical risk, detailed properties of splitting out a hard core from a boosting instance (cf. Section 4), and the notion of relative curvature (cf. Section 2.1) are all due to Telgarsky (2012). The intent of the present manuscript is to establish margin properties, and in this regard it departs from Telgarsky (2012); by contrast, the convergence rates of empirical risk presented here are thus trivial, but included since they did not appear explicitly in the literature. It is worth mentioning that these methods produce bad constants when applied to the logistic loss; unfortunately, previous work also suffers in this case (for instance, the work of Collins et al. (2002) provided only convergence of empirical risk, and not rates)."
    }, {
      "heading" : "2. Algorithms and Notation",
      "text" : "First some basic notation. Let {(xi, yi)}mi=1 ⊆ X × {−1,+1} denote an m-point sample. Take H0 to denote the collection of weak learners; it is assumed that h ∈ H0 satisfies h(X ) ⊆ [−1,+1], and that H0 has some form of bounded complexity, meaning specifically that the set of vectors {(h(x1), . . . , h(xm)) : h ∈ H0} is finite; this for instance holds if there is a fixed finite set of outputs from H0, e.g., each h is binary. Consequently, let H = {hj}nj=1 denote the effective finite set of hypothesis, and collect the responses on the sample into a matrix A ∈ [−1,+1]m×n with Aij = −yihj(xi).\nBoosting finds a weighting λ ∈ Rn of H, which corresponds to a regressor x 7→ ∑n j=1 λjhj(x), and thus a binary classification rule after thresholding. The corresponding (l1 minimum) margin M(Aλ) over the sam-\nple with respect to λ is\nM(Aλ) := min i∈[m] −e>i Aλ ‖λ‖1 = min i∈[m]\nyi ∑n j=1 λjhj(xi)\n‖λ‖1 .\nLet γ denote the best (largest) achievable margin; equivalently (Shalev-Shwartz & Singer, 2008), γ is the weak learning rate (which justifies the choice of l1 margins):\nγ := max λ∈Rn ‖λ‖1=1 M(Aλ) = max λ∈Rn ‖λ‖1=1 min i∈[m] −e>i Aλ\n= min w∈∆m max j∈[n] ∣∣∣∣∣ m∑ i=1 wiyihj(xi) ∣∣∣∣∣ = minw∈∆m ‖A>w‖∞. When γ > 0, the instance is considered separable; classically, this condition is termed the weak learning assumption (Kearns & Valiant, 1989; Freund & Schapire, 1997)."
    }, {
      "heading" : "2.1. The Family of Loss Functions",
      "text" : "The class L will effectively be “functions similar to the exponential loss”. Some of this is for analytic convenience, but some of this appears to be essential, and thus a bit of motivation is appropriate.\nOptimization problems typically take advantage of curvature (e.g., strong convexity) to establish a convergence rate. The analysis here instead uses a relative form of curvature: it suffices for, say, the Hessian to not be too small relative to the gap between the current primal objective value and the primal optimum. In this sense, the exponential loss is ideal, as it is a fixed point of the differentiation operator.\nDefinition 2.1. Given a loss ` : R → R++ (where R++ denotes positive reals), let C`(z) ≥ 1 (with potentially C`(z) =∞) be the tightest positive constant so that, for every x ≤ z: C`(z)−1 ≤ exp(x)/`(i)(x) ≤ C`(z) for i ∈ {0, 1, 2} (the zeroth, first, and second derivatives). ♦\nSince C`(z) is defined to be the tightest constant, it follows that y ≤ z implies C`(y) ≤ C`(z).\nFrom here, the class of loss functions may be defined.\nDefinition 2.2. Let L contain all functions ` : R → R+ which are twice continuously differentiable, strictly convex, and have C`(z) < ∞ for all z ∈ R. Additionally, if limz→−∞ C`(z) = 1, then ` ∈ L∞. ♦\nCrucially, the two classes L and L∞ both contain the exponential and logistic losses.\nProposition 2.3. {x 7→ exp(x), x 7→ ln(1 + exp(x))} ⊆ L∞.\nAlgorithm 1 boost. Input: loss `, matrix A ∈ [−1,+1]m×n. Output: Weighting sequence {λt}∞t=0.\nInitialize λ0 := 0. for t = 1, 2, . . . : do\nChoose column (weak learner)\njt := arg max j\n|∇L(Aλt−1)>Aej |.\nSet descent direction vt ∈ {±ejt}, whereby\n∇L(Aλt−1)>Avt = −‖∇L(Aλt−1)>A‖∞.\nFind αt via line search. Update λt := λt−1 + αtvt.\nend for\nOne way to interpret this is to say “in the limit, logistic loss is the same as exponential loss”. Unfortunately, this treatment of the logistic loss ends up being quite unfair, in the sense that the bounds are not accurately representative of the behavior of the algorithm (see Section 3.3). It is, however, unclear how to better deal with the logistic loss.\nLastly, the relevant primal objective function may be defined.\nDefinition 2.4. Given ` ∈ L and vector z ∈ Rm, define L(z) := m−1 ∑m i=1 `(zi), whereby the primal optimization problem for boosting is to minimize L(Aλ) over the domain Rn. For convenience, define L̄A := infλ∈Rn L(Aλ). ♦"
    }, {
      "heading" : "2.2. Algorithm",
      "text" : "The algorithm appears in Algorithm 1. Before defining the various step sizes, two more definitions are in order.\nDefinition 2.5. For every t, define γt := ‖A>∇L(Aλt−1)‖∞/‖∇L(Aλt−1)‖1. (Note that 1 ≥ γt ≥ γ.) ♦\nAdditionally, rather than depending on parameter C`(z) for a carefully chosen z, the following definition suffices.\nDefinition 2.6. For t ≥ 1, define Ct := C`(` −1(mL(Aλt−1))). ♦\nThe significance of Ct is as follows. Since the algorithm itself is coordinate descent, and moreover since every line search will be shown to guarantee descent, every candidate λ considered in round t will satisfy L(Aλ) ≤ L(Aλt−1); thus, for every i ∈ [m],\n`(e>i Aλ) ≤ mL(Aλ) ≤ mL(Aλt−1), and so e>i Aλ ≤ `−1(mL(Aλt−1)), where the inverse is well-defined since ` is a bijection between R and R++ by definition of L (otherwise C`(z) =∞).\nThe collection of step sizes considered here are as follows, in order of least to most aggressive. Throughout these step sizes, ν ∈ (0, 1] will denote a shrinkage parameter.\nQuadratic upper bound. Rather than performing an optimal line search, i.e., rather than minimizing α 7→ L(A(λt−1 + αvt)), a quadratic upper bound of this univariate function may be minimized, which has a closed form solution (cf. the proof of Lemma 3.2). In particular, define the step\nsize αQt (ν) := νγt/C 4 t . This choice is pleasant algorithmically only when Ct is easy to compute (for instance, Ct = 1 for the exponential loss). In general, however, it is useful as an analytic aid, since most step sizes here can be lower bounded by it. This step size was introduced by Telgarsky (2012, Appendix D.3).\nWolfe. The Wolfe line search is a standard tool from nonlinear optimization (Nocedal & Wright, 2006, chapter 3), and for convex problems it may be implemented with binary search (Telgarsky, 2012, Appendix D.1). More precisely, this choice is a set of step sizes αWt (ν) satisfying two conditions. First, the step is explicitly disallowed from being too large:\nL(A(λt−1 + αvt)) ≤ L(Aλt−1)− α(1− ν/2)‖A>∇L(Aλt−1)‖∞.\n(2.7)\nSecond, the step should be approximately optimal (in terms of the line search problem):\n∇L(A(λt−1 + αvt))>Avt ≥ −(1− ν/4)‖∇L(Aλt−1)>A‖∞. (2.8)\n(Requiring the reverse inequality (with the right hand side negated) yields the Strong Wolfe Conditions, which are not necessary here.) In contrast to αQt (ν), the Wolfe step does not require knowledge of Ct, but will yield nearly identical bounds; in fact, computation of the Wolfe step requires only function evaluations, gradient evaluations, and knowledge of ν, A,vt, λt.\nAdaBoost. Following the scheme of AdaBoost, define αAt (ν) := ν 2 ln( 1+γt 1−γt ), where convention is\nfollowed and γt = 1 is ignored. Unfortunately,\neven though γt is loss-dependent, this step will only yield rates with the exponential loss. However, it will be instrumental in analyzing the fully optimizing step size, presented next. This step size was introduced with the original presentation of AdaBoost (Freund & Schapire, 1997), though the analysis here will rather follow a slightly later treatment (Schapire & Singer, 1999).\nOptimal. Let αOt (1) be a minimizer to α 7→ L(A(λt−1 +αvt)), which, as in the case of αAt (ν), is assumed to exist. For ν ∈ (0, 1), set αOt (ν) = ναOt (1). When A is binary and ` = exp, α O t (ν) =\nαAt (ν), though in general this is not true. This step size (with shrinkage!) was suggested by Friedman (2000) for use with the logistic loss.\nTo close, note that αQt (ν) and α O t (ν) have a simple relationship. Proposition 2.9. If A ∈ [−1,+1]m×n and ` ∈ L, then αQt (ν) ≤ αOt (ν)."
    }, {
      "heading" : "3. The Separable Case",
      "text" : "This section considers the setting of separability, meaning the weak learning assumption is satisfied (γ > 0). The three subsections respectively provide convergence rates in empirical risk, basic margin guarantees, and close with some discussion."
    }, {
      "heading" : "3.1. Convergence of Empirical Risk",
      "text" : "The basic guarantee is that all of these line search methods, for any loss in L and with arbitrary shrinkage, exhibit the same basic convergence rate as AdaBoost.\nTheorem 3.1. Let boosting matrix A with corresponding γ > 0 and shrinkage parameter ν ∈ (0, 1] be given. Given any ` ∈ L, any > 0, and iterates {λt}t≥0 consistent with αQt (ν), α W t (ν), α O t (ν), or α A t (ν) with ` = exp, then O( 1γ2 ln( 1 )) iterations suffice to ensure L(Aλt) ≤ , where the O(·) suppresses terms depending on C1 and ν.\nThe proof is in the appendix, but a basic discussion will appear here for each step size. The proofs are straightforward, as they should be: convergence analyses typically prove a bound for one step, and then iterate the bound. As such, taking 1/ν steps which are ν-factor as long as the original should do at least as well as the original (which is indeed the exhibited trade-off).\nFirst is the quadratic upper bound, which implicitly gives an upper bound for the optimal step as well. The\nproof follows a standard scheme from convex optimization of lower and upper bounding a potential function based on the gradient; the specifics use the relative curvature properties of L, and follow the analysis of Telgarsky (2012, Section 6.1, Appendix D).\nLemma 3.2. Consider the setting of Theorem 3.1, but with each step size αt satisfying α Q t (ν) ≤ αt ≤ αOt (ν). Then for any t > t0 ≥ 0,\nL(Aλt) ≤ L(Aλt0) exp\n( −ν(2− ν)\n2C6t0+1 t∑ i=t0+1 γ2i\n) .\nThe reason for the parameter t0 is to mitigate the horrendous dependence on Ct0 , which is potentially very large. In particular, consider ` ∈ L∞, meaning limz→−∞ C`(z) = 1. C1 may be quite bad, but convergence still happens. It follows that Ct → 1, and thus, by choosing some large t0, the bound provides that perhaps there is an initially slow convergence phase, but eventually it is very fast. That is to stay, Lemma 3.2 may be applied multiple times to give a more refined picture of the convergence, particularly in the case that ` ∈ L∞, which guarantees the constants are eventually near 1.\nNext, the Wolfe step size has a similar guarantee (and the analysis once again heavily relies on techniques due to Telgarsky (2012, 6.1, Appendix D)).\nLemma 3.3. Consider the setting of Theorem 3.1, but with αt ∈ αWt (ν). Then for any t > t0 ≥ 0,\nL(Aλt) ≤ L(Aλt0) exp\n( −ν(2− ν)\n8C6t0+1 t∑ i=t0+1 γ2i\n) .\n(The denominator blows up by a factor 4 due to extra halves introduced into the Wolfe conditions, specifically to adjust around the natural Wolfe parameters being within (0, 1) and not (0, 1].)\nLastly, consider αAt (ν). As in the statement of Theorem 3.1, this step size is only shown to work with the exponential loss. This may be an artifact of the analysis, however, which perhaps follows too closely the treatment of Schapire & Singer (1999), which only considers the exponential loss; for instance, a slightly modified step size can be used to show convergence with the logistic loss (Collins et al., 2002).\nLemma 3.4. Consider the setting of Theorem 3.1, but with αt ∈ αAt (ν) Then for any t > t0 ≥ 0,\nL(Aλt) ≤ L(Aλt0) t∏\ni=t0+1\nC3i\n( 1− ν\n2 γ2i\n) ."
    }, {
      "heading" : "3.2. Margin Maximization",
      "text" : "The margin rates here follow a simple pattern: the more regularized the step size, the faster the convergence to a good margin. While no lower bounds are presented, this is an interesting and intuitive correspondence (in particular, consistent with Figure 1). Unfortunately, the unconstrained step sizes only have asymptotic convergence (no rates), so the umbrella theorem for this subsection is also asymptotic.\nTheorem 3.5. Let boosting matrix A with corresponding γ > 0 and shrinkage parameter ν ∈ (0, 1] be given. Given any ` ∈ L∞, any > 0, and iterates {λt}t≥0 consistent with αQt (ν), α W t (ν), α A t (ν) with ` = exp, or αOt (ν) with binary A ∈ {−1,+1}m×n, then there exists T so that for all M(Aλt) ≥ γ − for all t ≥ T .\nIn contrast with the convergence rates of empirical risk (e.g., Theorem 3.1), the condition ` ∈ L∞ is made, rather than simply ` ∈ L (with improved constants when ` ∈ L∞). This can be interpreted to say: the analysis depends heavily upon the structure of the exponential loss. While this condition is likely unnecessary, on the other extreme it is important for the loss to be strictly convex; if for instance the hinge loss is used, then minimization can stop at any point achieving zero error, in particular at one with poor margin properties.\nReturning to task, the quadratic upper bound comes first.\nLemma 3.6. Suppose the setting of Theorem 3.5, but with αt = α Q t (ν). Additionally let t > t0 ≥ 0 be given with t ≥ 2C 6 1 ln(m)\nγ2ν(2−ν) (whereby all margins are nonnega-\ntive by Lemma 3.2). Then\nM(Aλt) ≥ γ (\n2− ν 2C6t0+1\n) − ln(c0)\ntνγ ,\nwhere\nc0 :=max { 1,mCt0+1L(Aλt0) exp ( ν(2− ν) 2C6t0+1 t0∑ i=1 γ2i )} .\nTo interpret this bound, first consider the simplifying case that ` = exp, whereby Ct = 1 for all t. Additionally taking t0 = 0, it follows that c0 = m, and the bound is simply\nM(Aλt) ≥ γ (\n1− ν 2\n) − ln(m)\ntνγ ;\nin particular,M(Aλt)→ γ as ν → 0 and tν →∞. For some other ` ∈ L∞, the denominator term C6t0+1 also\npresents an obstacle to establishing margin maximization; but note that t0 →∞ suffices, since it combines with ` ∈ L∞ via Theorem 3.1 to grant Ct0 → 1.\nThe proof of Lemma 3.6 does not have to work too hard, as the step size appears prominently in the convergence rate bound (cf. Lemma 3.2). As will be discussed in Section 3.3, the rate is nearly ideal.\nThe Wolfe search exhibits a similar rate.\nLemma 3.7. Suppose the setting of Theorem 3.5, but with αt = α W t (ν). Additionally let t > t0 ≥ 0 be given with t ≥ 8C 6 1 ln(m)\nγ2ν(2−ν) (whereby all margins are nonnega-\ntive by Lemma 3.3). Then M(Aλt) ≥ γ (\n2− ν 2C2t0+1\n) − 4C1 ln(c0)\ntνγ ,\nwhere\nc0 :=max { 1,mCt0+1L(Aλt0) exp ( (2− ν)γ 2C2t0+1 t0+1∑ i=1 αi )} .\nThe preceding two step choices, αQt (ν) and α W t (ν), had explicit regularization: the first stops as soon as the steepest matching quadratic turns upward, and the second refuses to go beyond a boundary (cf. eq. (2.7)).\nOn the other hand, the choices αAt (ν) and α O t (ν) are only constrained by the data. Recall that one way to derive αAt (ν) is in the case of binary A ∈ {−1,+1}m×n and ` = exp, where it is crucial that each weak learner is wrong on at least one example: this prevents steps from being too large. The techniques in the following proof follow those used in the margin bounds for regular AdaBoost (and are asymptotic there as well). It is worth noting that not only is this bound the worst, but the analysis is the trickiest.\nLemma 3.8. Consider the setting of Theorem 3.5, but now ` = exp and αt = α A t (ν). Then for any ∈ (0, γ], there exists T so that M(Aλt) ≥ γ − for all t ≥ T .\nSimilarly, αOt (ν) is only implicitly regularized. The condition that A ∈ {−1,+1}m×n prevents the negative, constraining examples from having too little influence.\nLemma 3.9. Consider the setting of Theorem 3.5, but now ` = exp, the matrix A is binary, and αt = α O t (ν). Then for any > 0, there exists T so that M(Aλt) ≥ γ − for all t ≥ T .\nThe above lemmas together provide the proof of Theorem 3.5. But before closing, note that while the results for the unconstrained step sizes were only asymptotic, it is possible to derive a rate for the more modest goal of margins closer to γ/3.\nProposition 3.10. Consider the setting of Theorem 3.5, but specialized with ` = exp and αt = α A t (ν). Let a target margin value θ < γ be given. If θ < γ/(1 + γ) (e.g., it suffices that θ < γ/2), then\n1\nm m∑ i=1 1 [ −eiAλt ‖λt‖1 < θ ] ≤ exp ( −tν(γ2 − θγ(2 + γ)) 2 ) .\nIn particular, if θ < γ/(2 + γ) (e.g., it suffices that θ < γ/3) and t > 2 ln(m)/(ν(γ2 − θγ(2 + γ))), then M(Aλt) ≥ θ.\nNote, of course, that this bound has the severe analytic artifact of demonstrating no benefit of shrinkage!"
    }, {
      "heading" : "3.3. Discussion",
      "text" : "To get a sense of these margin bounds, first recall Freund’s lower bound on boosting methods in the separable case, which states that Ω( 1γ2 ln( 1 τ )) iterations are necessary to achieve classification error τ > 0 (Freund, 1995, Section 2). Setting τ = 1/m, it follows that Ω(ln(m)/γ2) iterations are necessary to achieve any nonnegative margin. By comparison, with αWt (ν) and ` = exp, just 12 ln(m)/γ2 iterations with choice ν = 1/2 suffice to reach margin γ/2 (by Lemma 3.7). More generally, αWt (ν) reaches margin γ(1 − ν) with 8 ln(m)/(νγ)2 iterations (if step size αQt (ν) is used, then 2 ln(m)/(νγ)2 iterations suffice by Lemma 3.6).\nThe explicit margin-maximizing method of ShalevShwartz & Singer (2008) requires t ≥ 32 ln(m)/ 2 iterations to achieve margin γ − , where ∈ (0, γ). By comparison, converting the above multiplicative bound into an additive bound, step size αWt ( /γ) requires 8 ln(m)/ 2 iterations. While this bound is slightly better, the comparison is not fair, since αWt ( /γ) requires knowledge of γ in the choice of shrinkage parameter ν. (Pessimistically taking ν = gives an additive guarantee, but with a poor rate.) Consequently, it can be reasoned that shrinkage methods achieve excellent margins, but are best suited for multiplicative guarantees.\nAnother question is how accurately the bounds presented here depict the methods provided. As a brief sanity check, the methods may be run on a problem instance where AdaBoost demonstrably does not achieve maximum margins. The particular instance tested here is a binary matrix A ∈ {−1,+1}8×8 due to Rudin et al. (2004, Theorem 7); recall that AdaBoost, in the present notation (with A binary), corresponds to ` = exp and step size αAt (1) = α O t (1) (no shrinkage). Two plots are provided.\n1. Figure 2 is a sanity check, showing that ` = exp\nand αAt (1) = α O t (1) may not achieve maximum margins, but shrinkage overcomes this.\n2. Figure 3 demonstrates that the Wolfe search (with ` = exp) is indeed effective, but demanding higher accuracy comes at a price.\nThese plots will be discussed further in Section 5. Additional tests with this matrix demonstrated that the method of Shalev-Shwartz & Singer (2008) indeed performs a tiny bit worse than the Wolfe search, but of course one example is not terribly indicative. Perhaps most importantly, a test with the logistic loss showed that the bound is loose: the logistic loss performs well, and does not suffer a startup cost as indicated by the bounds."
    }, {
      "heading" : "4. The General Case",
      "text" : "The last technical contribution of this manuscript is to briefly consider the general case (which is potentially nonseparable). Similarly to the separable case, this section will establish convergence rates for empirical risk, margin guarantees, and briefly discuss the connection to existing margin maximizing methods. But first, it is necessary to discuss the structure of the general case, and in particular to develop what margins mean without separability.\nThis section hinges upon the following decomposition of a boosting instance. This decomposition partitions a boosting instance, specifically its examples {(xi, yi)}mi=1, into a hard subset H(A), and an easy subset H(A)c. The easy subset alone is separable, and thus margins will be measured there. Although the analysis will rely heavily on properties of this decom-\nposition due to Telgarsky (2012), the decomposition itself has appeared, with various guarantees, in numerous places (Goldreich & Levin, 1989; Impagliazzo, 1995; Mukherjee et al., 2011). The notation H(A) reflects the fact that this structure has no relation to the choice of ` ∈ L. Definition 4.1. (Cf. Telgarsky (2012, Definition 5.1, 5.7).) Given a boosting problem encoded in a matrix A ∈ Rm×n, a set of examples (rows) H(A) ⊆ [m] is a hard core for A (and the corresponding boosting problem) if it satisfies the following properties.\n• There exists a weighting λ̂ ∈ Rn with e>i Aλ̂ < 0 for i ∈ H(A)c and e>i Aλ̂ = 0 for i ∈ H(A).\n• Every weighting λ ∈ Rn with e>i Aλ < 0 for some i ∈ H(A) also has e>k Aλ > 0 for some k ∈ H(A).\nAdditionally, define a row-wise partition of A into matrices A0, A+, where A+ has the examples in H(A), and A0 has the examples in H(A) c. ♦\nThe second property provides that H(A) is difficult: positive margins on some examples force negative margins on others. On the other hand, the complement H(A)c is easy, and moreover can be solved without affecting H(A).\nProposition 4.2. (Cf. Telgarsky (2012, Proposition 5.8, Theorem 5.9).) For any A ∈ Rm×n, a hard core H(A) always exists, and is unique.\nWith the decomposition in place, the aforementioned guarantees may be stated. The first, as in the separable case, is convergence of empirical risk. There is hardly anything to do here; the groundwork from Sec-\ntion 3 can be plugged directly into existing techniques to generate this theorem (Telgarsky, 2012, Section 6).\nTheorem 4.3. Let general boosting matrix A be given (i.e., potentially γ = 0), along with shrinkage parameter ν ∈ (0, 1], any ` ∈ L, and target suboptimality > 0. Suppose step sizes {αt}t≥0 are consistent with αQt (ν), α W t (ν), α O t (ν), or α A t (ν) with ` = exp and A binary. Then O( 1 ) iterations suffice to reach suboptimality > 0.\nIf the instance is either separable (i.e., γ > 0 as in Section 3) or attains its minimizer (i.e., |H(A)| = m (Telgarsky, 2012, Theorem 5.5)), then the rate improves to O(ln( 1 )).\nLastly come the margin guarantees. As stated above, H(A)c, considered alone, is separable; note furthermore that the definition of hard core provides the existence of a weighting λ̂. which has positive margins over H(A)c, but abstains entirely over H(A). Consequently, an approximate minimizer to L(A·) can always add in a scaling of λ̂ and improve its empirical risk while simultaneously improving margins over H(A)c. Consequently, it is natural to expect the methods here to achieve positive margins over H(A)c. Note that the following result only shows that some positive margins are attained, and neither assert some sense under which they are maximal, nor does it provide rates.\nTheorem 4.4. Let general boosting matrix A be given with 1 ≤ |H(A)| ≤ m − 1 (i.e., the problem is neither separable, nor is the minimizer attainable). Let shrinkage parameter ν ∈ (0, 1] and any ` ∈ L∞ be given. Suppose step sizes {αt}t≥0 are consistent with αQt (ν), α W t (ν), α O t (ν) with ` = exp and binary A, or αAt (ν) with ` = exp and binary A., Then there exists γ̂ > 0 so that every example off the hard core (i.e., i ∈ H(A)c) has margin at least γ̂ for all large t.\nTo close, consider once again the comparison to explicit margin maximizing boosting methods as presented by Shalev-Shwartz & Singer (2008). There is no point in discussing the specific method discussed in Section 3.3, whose optimal objective value is exactly γ, which in this case is zero, and the method may happily quit without iterating. Indeed, a primary contribution of Shalev-Shwartz & Singer (2008) is not only to address this issue, but show how the same general boosting scheme can be instantiated for the aforementioned method, as well as methods with tolerance to nonseparability.\nIndeed, consider the “soft-margin” boosting method (Shalev-Shwartz & Singer, 2008), originally due to Warmuth et al. (2006), which, roughly speaking, has\na parameter controlling how many examples to give up on. This is in contrast to the methods here, which not only have a fixed data-dependant structure they try less hard on (the hard core H(A)), but moreover the particular margins achieved over the hard core are determined by the loss function ` ∈ L. It is of course worth mentioning that the margin analysis in the nonseparable case here is by comparison very incomplete, providing no rates and not even identifying exactly what positive margins are attained."
    }, {
      "heading" : "5. Discussion",
      "text" : "This manuscript immediately raises a number of questions. Perhaps foremost is the general question of the impact of margins on the efficacy of boosting. Although margins certainly provide an intuitive theory, it is still unclear how much they directly correlate with good algorithms (Reyzin & Schapire, 2006).\nNext, the bounds for the logistic loss are not tight. As there do not appear to be any more forgiving analyses of the logistic loss, the natural question is whether there are new techniques which provide a better characterization.\nLastly, Figure 2 shows a threshold effect: shrinkage 1 does not lead to the right margin, but 1/2 and smaller suffices to reach the maximum margin. (Indeed, experimentation reveals the threshold to be roughly 0.92.) It should be possible to clarify this behavior from the perspective of dynamical systems: smaller steps dodge bad attractors (Rudin et al., 2004; 2007)."
    }, {
      "heading" : "Acknowledgements",
      "text" : "The author thanks Daniel Hsu and the ICML reviewers for helpful comments and discussions. The author is also deeply indebted to Robert Schapire for numerous discussions, insight, and for suggesting study of the unconstrained step size (at the time, guarantees were only in place for the other choices!). This work was graciously supported by the NSF under grant IIS0713540."
    }, {
      "heading" : "A. Deferred Material from Section 2",
      "text" : "Proof of Proposition 2.3. There is nothing to show for exp, so consider `(x) = ln(1 + exp(x)), let z ∈ R be given, and let x ≤ z be arbitrary.\nConcavity grants ln(1 + exp(x)) ≤ exp(x). The lower bound can be checked in two stages. First, if x ≤ min{−1, z}, a Taylor expansion gives\nln(1 + ex) ≥ ex − sup ξ∈R\n1\n2(1 + ξ)2 e2x\n≥ ex (\n1− min{e z, e−1} 2\n) .\nOn the other hand, if −1 ≤ x ≤ z, then ex ≤ ez ln(1 + e−1)/ ln(1 + e−1) ≤ ez ln(1 + ex)/ ln(1 + e−1).\nNext, `′(x) = ex/(1 + ex), so `′(x) ≤ ex ≤ `′(x)(1 + ez). Similarly, `′′(x) = ex/(1 + ex)2, so `′′(x) ≤ ex ≤ `′′(x)(1 + ez)2.\nThe following lemma (and its proof) derive αQt (ν), establishes αQt (ν) ≤ αOt (ν), and gives the basic improvement due to one step satisfying α ∈ [αQt (ν), αOt (ν)].\nLemma A.1. Let boosting matrix A, shrinkage parameter ν ∈ (0, 1], and any ` ∈ L be given. For any iteration t, it holds that αQt+1(ν) ≤ αOt+1(ν). Furthermore, any step α ∈ [αQt+1(ν), αOt+1(ν)] satisfies\nL(A(λt + αvt+1)) ≤ L(Aλt) exp ( − ν(2− ν)γ2t+1\n2C6t+1\n) .\nProof. This analysis follows a scheme laid out by Telgarsky (2012, Appendix D.3). Let t denote any fixed iteration, and I denote the (possibly unbounded) interval\nI := {α ≥ 0 : L(A(λt + αvt+1)) ≤ L(Aλt)} ;\nby continuity of L and choice of vt+1, I is nonempty, with nonempty interior. By second order Taylor ex-\npansion, every α ∈ I satisfies\nL(A(λt + αvt+1)) ≤ L(Aλt) + αv>t+1A>∇L(Aλt)\n+ sup r∈I\nα2\n2 v>t+1A >∇2L(A(λt + rvt+1))Avt+1\n≤ L(Aλt)− α‖A>∇L(Aλt)‖∞\n+ α2\n2 sup r∈I\n1\nm m∑ i=1 `′′(e>i A(λt + rvt+1))A 2 ijt+1\n≤ L(Aλt)− α‖A>∇L(Aλt)‖∞\n+ C2t+1α 2\n2 sup r∈I\n1\nm m∑ i=1 `(e>i A(λt + rvt+1))A 2 ijt+1\n= L(Aλt)− α‖A>∇L(Aλt)‖∞ + C2t+1α 2\n2 L(Aλt)\n≤ L(Aλt)− α‖A>∇L(Aλt)‖∞ + C4t+1α 2\n2 ‖∇L(Aλt)‖1,\nwhich made use of `′′ ≤ Ct+1 exp ≤ C2t+1` along I, ` ≤ Ct+1 exp ≤ C2t+1`′ along I, |Aij | ≤ 1 (since elements of H are bounded in this way), and the definition of I (specifically r = 0 is the worst choice for r ∈ I). This final expression is a quadratic, whose minimizer must lie within I (since its second derivative exceeds that of L along this interval). Differentiating and setting to zero, the minimizer is\nI 3 ‖A >∇L(Aλt)‖∞\nC4t+1‖∇L(Aλt)‖1 = γt+1 C4t+1 = αQt+1(1).\nThis provides a derivation of the step αQt+1(ν), and also shows αQt+1(1) ≤ αOt+1(1). Plugging α Q t+1(ν) in for α in the above quadratic upper bound,\nL(A(λt + αvt+1)) ≤ L(Aλt)− ν(2− ν)γ2t+1‖∇L(Aλt)‖1\n2C4t+1 ≤ L(Aλt) ( 1− ν(2− ν)γ2t+1\n2C6t+1 ) ≤ L(Aλt) exp ( − ν(2− ν)γ2t+1\n2C6t+1\n) .\nProof of Proposition 2.9. This is the first part of Lemma A.1."
    }, {
      "heading" : "B. Deferred Material from Section 3",
      "text" : "B.1. Deferred Material from Section 3.1\nProof of Lemma 3.2. By Lemma A.1, for any t, L(A(λt + αvt+1)) ≤ L(Aλt) exp ( − ν(2− ν)γ2t+1\n2C6t+1\n) .\nNow let t0 ≤ t be given as in the desired statement, apply this bound t − t0 times, and use the fact that Ct+1 ≤ Ct.\nProof of Lemma 3.3. Let t denote any fixed iteration. Substituting c1 = 1 − ν/2, c2 = 1 − ν/4, and η = C2t+1 in a nearly identical guarantee for the Wolfe line search (Telgarsky, 2012, Proposition D.6) (where η is simply the biggest ratio between ` and `′′ in the current sublevel set) provides\nL(A(λt + αvt+1))\n≤ L(Aλt)− (1− ν/2)(ν/4)‖A>∇L(Aλt)‖2∞\n2C2t+1L(Aλt) ≤ L(Aλt) ( 1− ν(2− ν)γ2t+1\n8C6t+1 ) ≤ L(Aλt) exp ( − ν(2− ν)γ2t+1\n8C6t+1\n) .\nGiven t0 ≤ t, applying this bound t − t0 times and using Ct+1 ≤ Ct gives the result.\nNext, instead of directly proving Lemma 3.4, a more general lemma is given first, which will be useful later.\nLemma B.1. Consider the setting of Theorem 3.1, except now each step size αi satisfies\nαAi (ν)− τ ≤ αi ≤ αAi (ν) + τ\nfor some τ > 0. Then, given t ≥ t0,\nL(Aλt+1) ≤ L(Aλt0)\n· t+1∏\ni=t0+1\neτC4i 2 (1− γ2i )ν/2((1 + γi)1−ν + (1− γi)1−ν)\n≤ L(Aλt0) t+1∏\ni=t0+1\neτC4i 2 (1− γ2i )ν/2.\nProof. Fix an iteration t, and set wi = ` ′(e>i Aλt) and\nW = ∑ i wi ≤ mC2t+1L(Aλt). By convexity of exp(·),\nL(Aλt+1)\n≤ Ct+1 m m∑ i=1 exp(e>i Aλt+1)\n≤ C2t+1 m\n( W\nW ) m∑ i=1 wi exp ( 1 + e>i Avt+1 2 αt+1\n+ 1− e>i Avt+1\n2 (−αt+1)\n)\n≤ C2t+1 m W\n( 1− γt+1\n2 exp(αt+1) + 1 + γt+1 2\nexp(−αt+1) )\n≤ eτC4t+1\n2 L(Aλt)(1− γ2t+1)ν/2 · ( (1− γt+1)1−ν + (1 + γt+1)1−ν ) .\nTo simplify this expression, note that (·)1−ν is a concave function, and thus\n(1− γt+1)1−ν\n2 +\n(1 + γt+1) 1−ν\n2 ≤ (\n1− γt+1 2 + 1 + γt+1 2 )1−ν = 1.\nTo finish, given t ≥ t0, the result follows by t − t0 applications of these bounds.\nProof of Lemma 3.4. This follows by taking the second bound in Lemma B.1 with the choice τ = 0.\nProof of Theorem 3.1. The result follows from Lemma 3.2, Lemma 3.3, and Lemma 3.4 with the choice t0 = 0 and using Ct+1 ≤ Ct.\nB.2. Deferred Material from Section 3.2\nProof of Lemma 3.6. To start, note that\n‖λt+1‖1 = ∥∥∥∥∥ν t+1∑ i=1 viγiC −4 i ∥∥∥∥∥ 1 ≤ νC−41 t+1∑ i=1 γi ≤ ν t+1∑ i=1 γi.\nBy the form of L and the optimization guarantee in Lemma 3.2,\nmax k∈[m]\nexp(e>k Aλt+1)\n≤ m∑ i=1 exp(e>i Aλt+1) ≤ mCt+1L(Aλt+1)\n≤ mCt0+1L(Aλt0) exp\n( −ν(2− ν)\n2C6t0+1 t+1∑ i=t0+1 γ2i\n)\n= mCt0+1L(Aλt0) exp ( ν(2− ν) 2C6t0+1 t0∑ i=1 γ2i )\n· exp ( −ν(2− ν)\n2C6t0+1 t+1∑ i=1 γ2i\n)\n≤ c0 exp ( − (2− ν)\n2C6t0+1 t∑ i=1 νγ2i\n) ,\nwhere c0 is as in the statement. Since ln(·) is increasing, it follows that\nmax k∈[m] e>k Aλt+1 ≤ − (2− ν) 2C6t0+1 t∑ i=1 νγ2i + ln(c0).\nUsing the above bound on ‖λt‖1, since tγ ≤ ∑t+1 i=1 γt, and −e>k Aλt+1 is nonnegative by the lower bound on t,\nmin k∈[m] −e>k Aλt+1 ‖λt+1‖1 ≥ min k∈[m] −e>k Aλt+1 ν ∑t+1 i=1 γi\n≥ γ (\n2− ν 2C6t0+1 ) − ln(c0) ν ∑t+1 i=1 γi\n≥ γ (\n2− ν 2C6t0+1\n) − ln(c0)\n(t+ 1)νγ .\nProof of Lemma 3.7. Any step size αt+1 satisfying the Wolfe conditions will have lower bound\nαt+1 ≥ (1− (1− ν/4))‖A>L(Aλt)‖∞\nC2t L(Aλt)\n≥ (1− (1− ν/4))‖A >L(Aλt)‖∞ C4t ‖∇L(Aλt)‖1 = νγt+1 4C4t+1 ≥ νγ 4C41 ;\nindeed this expression appears in proofs demonstrating the improvement due to a single step of the Wolfe search, see for instance Telgarsky (2012, Proof of Proposition D.6, second to last line).\nAdditionally, note\n‖λt+1‖1 = ∥∥∥∥∥ t+1∑ i=1 αivi ∥∥∥∥∥ 1 ≤ t+1∑ i=1 αi.\nDirect from the first Wolfe condition (eq. (2.7)),\nL(Aλt+1) = L(A(λt + αt+1vt+1)) ≤ L(Aλt)− αt+1(1− ν/2)‖A>∇L(Aλt)‖∞\n≤ L(Aλt) ( 1− αt+1(1− ν/2)‖A >∇L(Aλt)‖∞\nL(Aλt) ) ≤ L(Aλt) ( 1− αt+1(2− ν)γt+1\n2C2t+1\n) .\nNow let t ≥ t0 be given as in the statement. Applying the above inequality t− t0 times,\nmax k∈[m]\nexp(e>k Aλt+1)\n≤ mCt+1L(Aλt+1)\n≤ mCt0+1L(Aλt0) exp\n( − (2− ν)γ\n2C2t0+1 t+1∑ i=t0+1 αi\n)\n≤ mCt0+1L(Aλt0) exp ( (2− ν)γ 2C2t0+1 t0∑ i=1 αi )\n· exp ( − (2− ν)γ\n2C2t0+1 t+1∑ i=1 αi\n)\n≤ c0 exp ( − (2− ν)γ\n2C2t0+1 t+1∑ i=1 αi\n) ,\nwhere c0 is as in the statement. Since ln(·) is increasing, it follows that\nmax k∈[m] e>k Aλt+1 ≤ − (2− ν)γ 2C2t0+1 t+1∑ i=1 αi + ln(c0).\nUsing the above lower bound on αi in terms of γi, and since all margins are nonnegative by the lower bound on t,\nmin k∈[m] −e>k Aλt+1 ‖λt+1‖1 ≥ min k∈[m] −e>k Aλt+1∑t+1 i=1 αi\n≥ γ (\n2− ν 2C2t0+1\n) − ln(c0)∑t+1\ni=1 αi ≥ γ (\n2− ν 2C2t0+1\n) − 4C 4 1 ln(c0)\n(t+ 1)νγ .\nThe remainder of this subsection provides proofs for αAt (ν) and α O t (ν), but uses some later material, most specifically the quantity Υν .\nLemma B.2. Consider the setting of Theorem 3.1, except now each step size αi satisfies\nαAi (ν)− τ ≤ αi ≤ αAi (ν) + τ\nfor some τ > 0. Let θ ∈ [0, γ) be given. Then, given t ≥ t0, m∑ i=1 1 [ −eiAλt+1 ‖λt+1‖1 < θ ]\n≤ mCt+1 exp(θ‖λt0‖1)L(Aλt0) t+1∏\ni=t0+1\n( eθτ (\n1 + γi 1− γi\n)θν/2\n· e τC4i 2 (1− γ2i )ν/2((1 + γi)1−ν + (1− γi)1−ν)\n) .\n≤ mCt+1 exp(θ‖λt0‖1)L(Aλt0) t+1∏\ni=t0+1\n( eθτ (\n1 + γi 1− γi\n)θν/2\n· eτC4i (1− γ2i )ν/2 ) .\nProof. To start,\nm∑ i=1 1 [ −eiAλt+1 ‖λt+1‖1 < θ ]\n= m∑ i=1 1 [θ‖λt+1‖1 + eiAλt+1 > 0] ≤ mCt+1 exp(θ‖λt+1‖1)L(Aλt+1).\nNext, note\n‖λt+1‖1 = ∥∥∥∥∥λt0 + t+1∑\ni=t0+1\nαivi ∥∥∥∥∥ 1\n≤ ‖λt0‖1 + t+1∑\ni=t0+1\n(τ + αAi (ν)).\nCombining these facts with the convergence bound from Lemma B.1, m∑ i=1 1 [ −eiAλt+1 ‖λt+1‖1 < θ ]\n≤ mCt+1 exp(θ‖λt0‖1)L(Aλt0) t+1∏\ni=t0+1\n( eθτ (\n1 + γi 1− γi\n)θν/2\n· e τC4i 2 (1− γ2i )ν/2((1 + γi)1−ν + (1− γi)1−ν)\n) .\nAs in the proof of Lemma B.1, (·)1−ν is concave, so the 1/2 may be pushed inside this last term to give\nthe vaguely simpler bound m∑ i=1 1 [ −eiAλt+1 ‖λt+1‖1 < θ ]\n≤ mCt+1 exp(θ‖λt0‖1)L(Aλt0) t+1∏\ni=t0+1\n( eθτ (\n1 + γi 1− γi\n)θν/2\n· eτC4i (1− γ2i )ν/2 ) .\nProof of Lemma 3.8. Set θ := γ − , whereby θ ∈ [0, γ). Invoking Lemma B.2 and simplifying terms via t0 = 0, Ci = 1, and τ = 0, then for any t,\nm∑ i=1 1 [ −eiAλt+1 ‖λt+1‖1 < θ ]\n≤ m t+1∏ i=1 (( 1 + γi 1− γi )θν/2 · 1\n2 (1− γ2i )ν/2((1 + γi)1−ν + (1− γi)1−ν)\n≤ m t+1∏ i=1\n(( 1 + γ\n1− γ )θν/2 · 1\n2 (1− γ2)ν/2((1 + γ)1−ν + (1− γ)1−ν)\n) ,\nwhere the replacement of γi by γ made use of the first part of Lemma B.6. Now, by the second part of Lemma B.6, this inner term is less than 1 iff θ < Υν(γ). By Theorem B.5, since θ < γ, there exists a ν sufficiently small that Υν(γ) > θ. Consequently, there exists a T so that this product is less than 1/m whenever t ≥ T , and the result follows.\nProof of Lemma 3.9. Set θ := γ − , whereby θ ∈ [0, γ). Since ` ∈ L∞, choose t0 large enough so that\nC8t0 <\n( 1 + γ\n1− γ\n) (γ−θ)ν 4\n.\nBy Lemma B.7, it follows that the optimal step size satisfies\nαAt (ν)− τ ≤ αOt (ν) ≤ αAt (ν) + τ\nwith τ = ν2 ln(C 4 t ). Combining this with the bound on Ct0 above,\ne2τC4t0 = C 8 t0 <\n( 1 + γ\n1− γ\n) (γ−θ)ν 4\n.\nPlugging this into the general margin bound in Lemma B.2 and additionally replacing γi with γ\nthanks to the first part of Lemma B.6, and finally setting θ′ := θ + (γ − θ)/2 = (θ + γ)/2 < γ, m∑ i=1 1 [ −eiAλt+1 ‖λt+1‖1 < θ\n] ≤ mCt+1 exp(θ‖λt0‖1)L(Aλt0)\n· t+1∏\ni=t0+1\n( eθτ (\n1 + γi 1− γi\n)θν/2\n· e τC4i 2 (1− γ2i )ν/2((1 + γi)1−ν + (1− γi)1−ν) ) ≤ mCt+1 exp(θ‖λt0‖1)L(Aλt0)\n· t+1∏\ni=t0+1\n( e2τC4t0+1 ( 1 + γ\n1− γ\n)θν/2\n· 1 2 (1− γ2)ν/2((1 + γ)1−ν + (1− γ)1−ν) ) ≤ mCt+1 exp(θ‖λt0‖1)L(Aλt0)\n· t+1∏\ni=t0+1\n(( 1 + γ\n1− γ\n)θ′ν/2\n· 1 2 (1− γ2)ν/2((1 + γ)1−ν + (1− γ)1−ν)\n) .\nBy the second part of Lemma B.6, the term within the product is less than one, and thus for all large t, this entire bound is less than 1, which gives the result.\nProof of Proposition 3.10. To start, note that, for any t ≥ 0,\n(1− γt)1−θ(1 + γt)1+θ = (1− γ2t )1−θ(1 + γt)2θ ≤ exp ( −γ2t (1− θ) + γt(2θ) ) = exp ( −γ2t + θγt(2 + γt) ) .\n(B.3)\nNext, since\nθ ≤ γ 1 + γ = 1 1 + 1/γ ≤ 1 1 + 1/γt = γt 1 + γt ,\nthen θ ≤ γ/(1 + γ) implies\nd\ndγt\n( −γ2t + θγt(2 + γt) ) = −2γt + 2θ(1 + γt)\n≤ −2γt + 2γt = 0.\nIn particular, the expression −γ2t + θγt(2 + γt) is decreasing in γ, and thus γt ≥ γ implies\n−γ2t + θγt(2 + γt) ≤ −γ2 + θγ(2 + γ),\nand consequently, combined with the bound in (B.3),\n(1− γt)1−θ(1 + γt)1+θ ≤ exp(−γ2 + θγ(2 + γ)).\nPlugging this into the simplified generic bound in Lemma B.2 with the specialization ` = exp, τ = 0, Ci = 1, and t0 = 0, it follows that\nm∑ i=1 1 [ −eiAλt+1 ‖λt+1‖1 < θ ]\n≤ m t+1∏ i=1 (( 1 + γi 1− γi )θν/2 (1− γ2i )ν/2 )\n≤ m exp ( −ν(t+ 1)\n2 (−γ2 + θγ(2 + γ))\n) .\nThe rest of the result follows by noting θ < γ/(2 + γ) implies −γ2 + θγ(2 + γ) < 0, whereby choices\nt > 2 ln(m)\nν(γ2 − θγ(2 + γ))\nexist, and plugging this all in to the above bound grants that M(Aλt) ≥ θ.\nProof of Theorem 3.5. For αAt (ν) and α O t (ν), Lemma 3.8 and Lemma 3.9 already state the results in the desired asymptotic form.\nFor the other two, since ` ∈ L∞, t0 can be chosen sufficiently large so that Ct0 is arbitrarily close to 1, whereby the bounds in Lemma 3.6 and Lemma 3.7 become sufficiently tight by taking ν small and tν large.\nB.2.1. The Quantity Υν\nDefinition B.4. Define\nΥν(γ) := 2 ν ln(2)− 2 ν ln((1 + γ)\n1−ν + (1− γ)1−ν)− ln(1− γ2) ln(1 + γ)− ln(1− γ) ;\nin the case that ν = 1, this quantity has been extensively studied in the context of AdaBoost’s margins (Rätsch & Warmuth, 2005; Rudin et al., 2004; Schapire & Freund, 2012) ♦\nThe basic properties of Υν are as follows.\nTheorem B.5. Suppose γ ∈ (0, 1).\n1. γ/2 ≤ Υν(γ) ≤ γ.\n2. limν↓0 Υν(γ) = γ.\nThe bounds γ/2 ≤ Υ1(γ) ≤ γ were known in the case that ν = 1 (cf. Rätsch & Warmuth (2005) and Schapire & Freund (2012, Bibliographic Notes, Chapter 5)).\nProof. (Item 1, subcase Υν(γ) ≥ γ/2.) To start, note that (·)1−ν is a concave function, whereby\n− 2 ν\nln ( (1 + γ)1−ν + (1− γ)1−ν ) = −2\nν ln\n( 2 ( 1\n2 (1 + γ)1−ν +\n1 2 (1− γ)1−ν )) ≥ −2 ν ln ( 2 (1) 1−ν ) = −2 ν ln (2) .\nIt follows that\nΥν(γ) ≥ − ln(1− γ2)\nln(1 + γ)− ln(1− γ) = Υ1(γ).\nNext recall the series expansion\nln(1 + z) = ∞∑ n=1 (−1)n+1 n zn\n(when |z| < 1). Plugging this in to the simplified form of Υ1(γ) and paying attention to cancellations in the numerator and denominator (odd and even terms, respectively),\nΥ1(γ) = − ∑∞ n=1 (−1)n+1 n (γ) n − ∑∞ n=1 (−1)n+1 n (−γ)\nn∑∞ n=1 (−1)n+1 n (γ) n − ∑∞ n=1 (−1)n+1 n (−γ)n\n= −2 ∑∞ n=1 (−1)2n+1 2n (γ) 2n\n2 ∑∞ n=1 (−1)2n−1+1 2n−1 (γ) 2n−1\n= γ ∑∞ n=1 1 2n (γ)\n2n∑∞ n=1 1 2n−1 (γ) 2n .\nTo finish, note that n ≥ 1 implies 1/(4n − 2) ≤ 1/(2n) ≤ 1/(2n− 1), and thus\nγ 2 = γ ∑∞ n=1 1 2(2n−1) (γ) 2n∑∞ n=1 1 2n−1 (γ) 2n\n≤ γ ∑∞ n=1 1 2n (γ)\n2n∑∞ n=1 1 2n−1 (γ) 2n\n≤ γ ∑∞ n=1 1 2n−1 (γ)\n2n∑∞ n=1 1 2n−1 (γ) 2n\n= γ.\nThat is to say, γ/2 ≤ Υ1(γ) ≤ γ, which combined with the above also gives Υν(γ) ≥ Υ1(γ) ≥ γ/2.\n(Item 1, subcase Υν(γ) ≤ γ.) By the power mean inequality (Steele, 2004, Equation 8.12),(\n1 + γ\n2 (1 + γ)−ν + 1− γ 2\n(1− γ)−ν )−1/ν\n≤ (1 + γ) 1+γ 2 (1− γ) 1−γ 2 .\nIt follows that\n− 2 ν ln\n( 1 + γ\n2 (1 + γ)−ν + 1− γ 2\n(1− γ)−ν )\n≤ (1 + γ) ln(1 + γ) + (1− γ) ln(1− γ).\nAs such,\nΥν(γ)\n≤ (1 + γ) ln(1 + γ) + (1− γ) ln(1− γ) ln(1 + γ)− ln(1− γ)\n+ − ln(1 + γ)− ln(1− γ) ln(1 + γ)− ln(1− γ)\n= γ ln(1 + γ)− γ ln(1− γ)\nln(1 + γ)− ln(1− γ) = γ.\n(Item 2.) Consider the (halved, negated) first term\nln((1 + γ)1−ν + (1− γ)1−ν)− ln(2) ν = ln(0.5(1 + γ)1−ν + 0.5(1− γ)1−ν)\nν .\nBy l’Hôpital’s rule,\nlim ν→0 ln(0.5(1 + γ)1−ν + 0.5(1− γ)1−ν) ν\n= lim ν→0 −(1 + γ)1−ν ln(1 + γ)− (1− γ)1−ν ln(1− γ) (1 + γ)1−ν + (1− γ)1−ν\n= −1 2 ((1 + γ) ln(1 + γ) + (1− γ) ln(1− γ)) .\nConsequently (recalling that this term was both halved and negated)\nlim ν→0\nΥν(γ) = (1 + γ) ln(1 + γ) + (1− γ) ln(1− γ)\nln(1 + γ)− ln(1− γ)\n+ − ln(1 + γ)− ln(1− γ) ln(1 + γ)− ln(1− γ)\n= γ.\nThe usefulness of Υν is captured in the following lemma.\nLemma B.6. Let ν ∈ (0, 1] and θ ∈ [0, 1] be given. The map\nγ 7→ ( 1 + γ\n1− γ\n)θν/2 (1− γ2)ν/2\n· ((1 + γ)1−ν + (1− γ)1−ν)\nis nonincreasing over [θ, 1]. Additionally, now taking γ to be fixed, θ < Υν(γ) iff\n1\n2\n( 1 + γ\n1− γ\n)θν/2 (1− γ2)ν/2\n· ((1 + γ)1−ν + (1− γ)1−ν) < 1.\nProof. Let f(γ) be the prescribed map. To establish f is nonincreasing, it will be shown that each element of the product f(γ) = g(γ)h(γ) is nonincreasing, where\ng(γ) :=\n( 1 + γ\n1− γ\n)θν/2 (1− γ2)ν/2\nh(γ) := (1 + γ)1−ν + (1− γ)1−ν .\nFirst, set ν′ := ν/2, and note\ng′(γ) = d\ndγ (1 + γ)ν\n′(1+θ)(1− γ)ν ′(1−θ)\n= ν′(1 + θ)(1 + γ)ν ′(1+θ)−1(1− γ)ν ′(1−θ)\n− ν′(1− θ)(1− γ)ν ′(1−θ)−1(1 + γ)ν ′(1+θ)\n= ν′(1 + γ)ν ′(1+θ)−1(1− γ)ν ′(1−θ)−1\n· ((1 + θ)(1− γ)− (1− θ)(1 + γ))\n= 2ν′(1 + γ)ν ′(1+θ)−1(1− γ)ν ′(1−θ)−1 (θ − γ) ,\nwhere this last term is nonpositive since θ ≤ γ. Consequently, g(γ) is nonincreasing.\nFor h(γ), note similarly that\nh′(γ) = (1− ν) ( (1 + γ)−ν − (1− γ)−ν ) =\n1− ν (1 + γ)ν(1− γ)ν ((1− γ)ν − (1 + γ)ν)\n≤ 0.\nTogether f(γ) = g(γ)h(γ) is nonincreasing in γ.\nFor the second statement, note that\n1 > 1\n2\n( 1 + γ\n1− γ\n)θν/2 (1− γ2)ν/2\n· ((1 + γ)1−ν + (1− γ)1−ν)\nis equivalent to\n0 > − ln(2) + ν 2 θ ln\n( 1 + γ\n1− γ\n) + ν\n2 ln(1− γ2)\n+ ln((1 + γ)1−ν + (1− γ)1−ν)\nis equivalent to\nθ < ln(2)− ν2 ln(1− γ 2)− ln((1 + γ)1−ν + (1− γ)1−ν) ν 2 ln ( 1+γ 1−γ\n) , where the last expression can be written θ < Υν(γ).\nB.2.2. Miscellaneous Technical Material\nLemma B.7. Suppose A ∈ {−1,+1}m×n is binary and ` ∈ L. Then 1 2 ln ( 1 + γt 1− γt ) − 1 2 ln(C4t ) ≤ αOt (1)\n≤ 1 2 ln ( 1 + γt 1− γt ) + 1 2 ln(C4t ).\nMore simply,∣∣αOt (ν)− αAt (ν)∣∣ ≤ ν2 ln (C4t ) . Proof. Choose s ∈ {±1} so that vt+1 = ejs for some ej . Then, by first order conditions on the optimal step size, and adopting shorthand notation where the summations take j fixed according to the preceding text, but i ∈ [m] may vary,\n0 = ∑ Aij<0 sAij` ′(e>i A(λt + sαt+1ej))\n+ ∑ Aij>0 sAij` ′(e>i A(λt + sαt+1ej))\n≤ C−1t+1 ∑ Aij<0 sAij exp(e > i Aλt) exp(sαt+1Aij)\n+ C1t+1 ∑ Aij>0 sAij exp(e > i Aλt) exp(sαt+1Aij)\n≤ exp(−sαt+1)C−2t+1 ∑ Aij<0 sAij` ′(e>i Aλt)\n+ exp(sαt+1)C 2 t+1 ∑ Aij>0 sAij` ′(e>i Aλt),\nwhich can be rearranged to yield\nsαt+1 ≥ 1\n2 ln\n( −sC−2t+1 ∑ Aij<0 Aij` ′(e>i Aλt)\nsC2t+1 ∑ Aij>0 Aij`′(e>i Aλt)\n)\n= 1\n2 ln\n(∑ Aij<0\n`′(e>i Aλt)∑ Aij>0 `′(e>i Aλt)\n) − 1\n2 ln(C4t+1).\nTo simplify further, note that sγt+1 = − ∑m i=1Aij` ′(e>i Aλt)\n‖∇L(Aλt)‖1\n=\n∑ Aij<0 `′(e>i Aλt)− ∑ Aij>0 `′(e>i Aλt)\n‖∇L(Aλt)‖1 ,\n1 =\n∑ Aij<0 `′(e>i Aλt) + ∑ Aij>0 `′(e>i Aλt)\n‖∇L(Aλt)‖1 ,\nwhich can be added and subtracted to yield∑ Aij>0\n`′(e>i Aλt)∑ Aij<0 `′(e>i Aλt) = 1− sγt+1 1 + sγt+1 ,\nwhereby\nsαt+1 ≥ 1\n2 ln ( 1 + sγt+1 1− sγt+1 ) − 1 2 ln(C4t+1).\nRepeating the steps above to prove a lower bound on αt+1, it also follows that\nsαt+1 ≤ 1\n2 ln ( 1 + sγt+1 1− sγt+1 ) + 1 2 ln(C4t+1).\nTo finish the first part of the result, it suffices to consider the cases s = +1 and s = −1 separately, which both lead to the desired pair of inequalities.\nFor the second guarantee, first note that αOt (ν) = ναOt (1) and α A t (ν) = να A t (1), and so recalling the form of αAt (1) and scaling the first guarantee by ν, it follows that\n|αOt (ν)− αAt (ν)| ≤ ν\n2 ln(C4t )."
    }, {
      "heading" : "C. Deferred Material from Section 4",
      "text" : "Proof sketch of Theorem 4.3. All the convergence rates developed by Telgarsky (2012, Section 6) stem from an inequality\nL(Aλt+1)− L̄A ≤ (L(Aλt)− L̄A) ( 1− ‖A >∇L(Aλt)‖2∞\ncL(Aλt)(L(Aλt)− L̄A)\n) ,\nwhere c > 0 is some constant independent of t (or improving with t, in which case the bound may be worsened by taking the choice for t = 0) (Telgarsky, 2012, Proposition 6.2, Proposition D.6). Exactly such a bound was provided for each line search in the proof of its respective optimization guarantee in the separable case (cf. Lemma 3.2, Lemma 3.3; no need to adjust Lemma 3.4, since ` = exp and A binary causes αAt (ν) = α O t (ν), and so Lemma 3.2 covers this case). Replacing c with the particulars for each step size will\nonly impact the final rates in Theorems 6.3, 6.6, and 6.12 by these constants. The only other thing to check is that ` ∈ G, the class of losses considered by Telgarsky (2012, Section 6); it can be checked directly that L ⊂ G.\nIn order to establish the margin properties, the following lemma is essential.\nLemma C.1. Consider the setting of Theorem 4.4. Then there exists T and γ̂ so that, for all t ≥ T ,\n‖A>∇L(Aλt)‖∞ L(Aλt)− L̄A ≥ γ̂.\nProof sketch. As discussed in the proof of Theorem 4.3, the results of Telgarsky (2012), which are superficially specialized to the Wolfe line search, carry over for the other line searches here with only a change of constants; consequently, those results carry over wholesale.\nTo start, let S be a compact cube containing all iterates, and let γ(A,S) be the corresponding generalized weak learning rate Telgarsky (2012, Definition 4.3).\nBy (Telgarsky, 2012, Theorem 5.9), L + ιim(A+) (i.e., the function which is L(y) when y = A+λ for some λ ∈ Rn, and ∞ otherwise) has compact level sets, and thus strict convexity of L grants a modulus of strong convexity c > 0 over S; furthermore, it holds for every t that\nL(A+λt)− L̄A\n≤ 1 2c ∥∥∥∇L(A+λt)− P1∇L(S)∩ker(A>+)(∇L(A+λt))∥∥∥21 , where P1∇L(S)∩ker(A>+) denotes the l1 projection onto ∇L(S) ∩ ker(A>+), the latter being the kernel (nullspace) of A>+ (Telgarsky, 2012, Lemma 6.8).\nNow choose T so that, for every t ≥ T ,\nL(A+λt)− L̄A ≤ L(Aλt)− L̄ ≤ 2c,\nwhich is possible by the convergence of {λt}∞t=1 (cf. Theorem 4.3 or (Telgarsky, 2012, Theorem 6.12)).\nUsing these facts, the definition of γ(A,S), the choice φ = exp, and the fact infλ L(A+λ) = infλ L(Aλ) = L̄A\n(Telgarsky, 2012, Theorem 5.9),\n‖A>∇L(Aλt)‖∞ L(Aλt)− L̄A\n≥ γ(A,S)\n( ‖∇L(Aλt)− P1S∩ker(A>)(∇L(Aλt))‖1\nL(Aλt)− L̄A\n)\n= γ(A,S) C2T C2T ( ‖∇L(A0λt)‖1 L(Aλt)− L̄A\n+ ‖∇L(A+λt)− P1S∩ker(A>+)(∇L(A+λt))‖1\nL(Aλt)− L̄A\n)\n≥ γ(A,S) C2T\n( L(A0λt) + √ 2c(L(A+λt)− L̄A)\nL(Aλt)− L̄A\n)\n≥ γ(A,S) C2T\n( L(A0λt)\nL(Aλt)− L̄A\n+\n√ (L(A+λt)− L̄A)(L(A+λt)− L̄A)\nL(Aλt)− L̄A\n)\n= γ(A,S)\nC2T .\nTo finish, set γ̂ := γ(A,S)/C2T .\nAnother technical lemma is helpful.\nLemma C.2. Consider the setting of Theorem 4.4. For each step size choice and B > 0, there exists TB so that for all t ≥ TB, ‖λt‖1 ≥ B.\nProof sketch. This follows from Theorem 4.3 and |H(A)| < m. In particular, choose any example i ∈ H(A)c; there exists > 0 so that 1m`(eiA\n>λ) < (which is a necessary condition for L(Aλ) < ) only when e>i Aλ ≤ −B‖e>i A‖∞, and so the result follows by combining this with Hölder’s inequality, namely the inequality −e>i Aλ ≤ ‖e>i A‖∞‖λ‖1; the optimality guarantee provides that this holds for all large t.\nIn order to proof the margin results, it is helpful to split into two cases, one being the Wolfe step sizes, the other being a generalization of the quadratic upper bound step sizes.\nLemma C.3. Consider the setting of Theorem 4.4, but with step sizes 0.5αQi (ν) ≤ αi ≤ 1.5α Q i (ν). Then there exists γ̂ > 0 and T so that, for all t ≥ T , all margins (over H(A)c) exceed γ̂.\nProof sketch. Consider the quadratic upper bound line search in Lemma 3.2 and its proof. It is unclear whether 0.5αQi (ν) or 1.5α Q i (ν) give a better step, due to the term ν. However, since αQi (1) is the minimizer,\nsymmetry grants that 0.5αQt (ν) is guaranteed to be a worse choice than anything in the specified interval. As such, plugging this in to the quadratic upper bound yields\nL(Aλt+1) ≤ L(Aλt)− c0γt+1‖A>∇L(Aλt)‖∞\n2 .\nfor some constant c0 > 0 depending on C1 and not on t.\nNow choose T1 according to Lemma C.1; by the above and Lemma C.1, for any t ≥ T1,\nL(Aλt+1)− L̄A\n≤ L(Aλt)− L̄A − γt+1c0‖A>∇L(Aλt)‖∞\n2 ≤ (L(Aλt)− L̄A) ( 1− γt+1c0‖A >∇L(Aλt)‖∞\n2(L(Aλt − L̄A) ) ≤ (L(Aλt)− L̄A) ( 1− γt+1c0γ̂\n2\n) ,\nwhich, after recursive application, provides\nL(Aλt+1)−L̄A ≤ (L(AλT1)−L̄A) exp\n( − γ̂c0\n2 t+1∑ i=T1+1 γi\n) .\nSince\n‖λt+1‖1 = ‖λT1 + t+1∑ i=T1 αivi‖1\n≤ ‖λT1‖1 + t+1∑\ni=T1+1\nαi\n≤ ‖λT1‖1 + 1.5ν t+1∑\ni=T1+1\nγi,\nit follows that\nL(Aλt+1)− L̄A ≤ (L(AλT1 − L̄A)) exp ( − γ̂c0\n3ν (‖λt+1‖1 − ‖λT1‖1)\n) .\nFor any iteration t, let bt ∈ {ei}m0i=1 index any example in [m] \\H(A) which achieves the worst margin (amongst elements off the hard core) for this iteration. Since the optimal error on this example is 0 (Telgar-\nsky, 2012, Theorem 5.9), for any t > T1,\nexp(b>t Aλt) = exp(b>t Aλt)− 0 ≤ mCT1(L(Aλt)− L̄A)\n≤ mCT1(L(AλT1 − L̄A)) exp ( − γ̂c0\n3ν (‖λt‖1 − ‖λT1‖1) ) = exp (−γ̂c0‖λt‖1/(3ν)) · Ct(L(AλT1 − L̄A)) exp(γ̂c0‖λT1‖1/(3ν))︸ ︷︷ ︸\n=:exp(r)\n.\nApplying ln and rearranging,\n−b>t Aλt ‖λt‖1 ≥ γ̂c0 3ν − r ‖λt‖1 .\nTo finish, by Lemma C.2, there exists T2 so that\n‖λi‖1 ≥ 6rν\nγ̂c0\nfor every i ≥ T2, and setting T := max{T1, T2} gives the desired result.\nLemma C.4. Consider the setting of Theorem 4.4, but specialized so that αi ∈ αWi (ν). Then there exists γ̂ > 0 and T so that, for all t ≥ T , all margins exceed γ̂.\nProof sketch. Choose T1 according to Lemma C.1, and let t ≥ T1 be arbitrary. Using Lemma C.1, and using the first Wolfe condition (eq. (2.7)) just as in the proof of Lemma 3.7,\nL(Aλt+1)− L̄A ≤ L(Aλt)− L̄A − αt+1(1− ν/2)‖A>∇L(Aλt)‖∞\n= (L(Aλt)− L̄A) ( 1− αt+1(1− ν/2)‖A >∇L(Aλt)‖∞\nL(Aλt)− L̄A ) ≤ (L(Aλt)− L̄A) (1− αt+1(1− ν/2)γ̂) ≤ (L(Aλt)− L̄A) exp (−αt+1(1− ν/2)γ̂)\nApplying this inequality recursively,\nL(Aλt+1)− L̄A\n≤ (L(AλT1 − L̄A)) exp\n( −(1− ν/2)γ̂\nt+1∑ i=T1 αi\n) .\nNote next, for any t0, that\n‖λt+1‖1 ≤ ‖λt0‖1 + t+1∑\ni=t0+1\nαi,\nwhereby\nL(Aλt+1)− L̄A ≤ (L(AλT1 − L̄A)) · exp (−(1− ν/2)γ̂(‖λt+1‖1 − ‖λt0‖1)) ,\nand the remainder of the proof proceeds just as for the quadratic upper bound (cf. Lemma C.3).\nProof sketch of Theorem 4.4. The case of αQt (ν) and αWt (ν) are handled by Lemma C.3 and Lemma C.4.\nNow consider the case of αAt (ν). Since γ = 0, Lemma C.6 grants the existence of a large T so that, for all t ≥ T , γt ≤ 0.1. Thus, by Lemma C.5, and considering t sufficiently large that Ct is almost 1, the problem reduces to the consideration of αQt (ν); in particular, the conditions to apply Lemma C.3, but now for the step αAt (ν), are satisfied. Note that this also handles the case αOt (ν), since, for α O t (ν) and α A t (ν), it was assumed that A is binary and ` = exp.\nC.1. Miscellaneous Technical Material\nLemma C.5. For any r ∈ [0, 1),\nr ≤ 1 2 ln\n( 1 + r\n1− r\n) ≤ r\n1− r .\nProof. Set g(r) := 12 ln((1 + r)/(1− r)). Note that\ng′(r) = (1− r2)−1 and g′′(r) = 2r (1− r2)2 .\nAs such, g is convex (along [0, 1)) and g′(0) = 1, thus g(r) ≥ r along [0, 1). The second part follows from concavity of ln(·):\n1 2 ln\n( 1 + r\n1− r\n) = 1\n2 ln\n( 1 + 2r\n1− r\n) ≤ 1\n2\n( 2r\n1− r\n) .\nLemma C.6. Under the conditions of Theorem 4.4, limt→∞ γt = 0.\nProof sketch. As discussed in the proof of Theorem 4.3, every step size provides a guarantee of the type\nL(Aλt+1) ≤ L(Aλt)− γ2tL(Aλt)\nc\nfor some c > 0 (independent of t). The result follows by rearranging this expression and using L(Aλt) ≥ L̄A > 0 (i.e., nonseparability) and L(Aλt − L(Aλt+1) → 0 (i.e., the convergence result, Theorem 4.3)."
    } ],
    "references" : [ {
      "title" : "Boosting a weak learning algorithm by majority",
      "author" : [ "Freund", "Yoav" ],
      "venue" : "Information and Computation,",
      "citeRegEx" : "Freund and Yoav.,? \\Q1995\\E",
      "shortCiteRegEx" : "Freund and Yoav.",
      "year" : 1995
    }, {
      "title" : "A decisiontheoretic generalization of on-line learning and an application to boosting",
      "author" : [ "Freund", "Yoav", "Schapire", "Robert E" ],
      "venue" : "J. Comput. Syst. Sci.,",
      "citeRegEx" : "Freund et al\\.,? \\Q1997\\E",
      "shortCiteRegEx" : "Freund et al\\.",
      "year" : 1997
    }, {
      "title" : "Greedy function approximation: A gradient boosting machine",
      "author" : [ "Friedman", "Jerome H" ],
      "venue" : "Annals of Statistics,",
      "citeRegEx" : "Friedman and H.,? \\Q2000\\E",
      "shortCiteRegEx" : "Friedman and H.",
      "year" : 2000
    }, {
      "title" : "A hard-core predicate for all one-way functions",
      "author" : [ "Goldreich", "Oded", "Levin", "Leonid" ],
      "venue" : "STOC, pp",
      "citeRegEx" : "Goldreich et al\\.,? \\Q1989\\E",
      "shortCiteRegEx" : "Goldreich et al\\.",
      "year" : 1989
    }, {
      "title" : "Hard-core distributions for somewhat hard problems",
      "author" : [ "Impagliazzo", "Russell" ],
      "venue" : "In FOCS, pp",
      "citeRegEx" : "Impagliazzo and Russell.,? \\Q1995\\E",
      "shortCiteRegEx" : "Impagliazzo and Russell.",
      "year" : 1995
    }, {
      "title" : "Cryptographic limitations on learning finite automata and boolean formulae",
      "author" : [ "Kearns", "Michael", "Valiant", "Leslie" ],
      "venue" : "STOC, pp",
      "citeRegEx" : "Kearns et al\\.,? \\Q1989\\E",
      "shortCiteRegEx" : "Kearns et al\\.",
      "year" : 1989
    }, {
      "title" : "The convergence rate of AdaBoost",
      "author" : [ "Mukherjee", "Indraneel", "Rudin", "Cynthia", "Schapire", "Robert" ],
      "venue" : "In COLT,",
      "citeRegEx" : "Mukherjee et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Mukherjee et al\\.",
      "year" : 2011
    }, {
      "title" : "Soft margins for adaboost",
      "author" : [ "G. Rätsch", "T. Onoda", "Müller", "K.-R" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Rätsch et al\\.,? \\Q2001\\E",
      "shortCiteRegEx" : "Rätsch et al\\.",
      "year" : 2001
    }, {
      "title" : "Efficient margin maximizing with boosting",
      "author" : [ "Rätsch", "Gunnar", "Warmuth", "Manfred" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Rätsch et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Rätsch et al\\.",
      "year" : 2005
    }, {
      "title" : "How boosting the margin can also boost classifier complexity",
      "author" : [ "Reyzin", "Lev", "Schapire", "Robert E" ],
      "venue" : "Proceedings of the 23rd International Conference on Machine Learning,",
      "citeRegEx" : "Reyzin et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Reyzin et al\\.",
      "year" : 2006
    }, {
      "title" : "The dynamics of AdaBoost: cyclic behavior and convergence of margins",
      "author" : [ "Rudin", "Cynthia", "Daubechies", "Ingrid", "Schapire", "Robert E" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Rudin et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Rudin et al\\.",
      "year" : 2004
    }, {
      "title" : "Analysis of boosting algorithms using the smooth margin function",
      "author" : [ "Rudin", "Cynthia", "Schapire", "Robert E", "Daubechies", "Ingrid" ],
      "venue" : "Annals of Statistics,",
      "citeRegEx" : "Rudin et al\\.,? \\Q2007\\E",
      "shortCiteRegEx" : "Rudin et al\\.",
      "year" : 2007
    }, {
      "title" : "Boosting: Foundations and Algorithms",
      "author" : [ "Schapire", "Robert E", "Freund", "Yoav" ],
      "venue" : null,
      "citeRegEx" : "Schapire et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Schapire et al\\.",
      "year" : 2012
    }, {
      "title" : "Improved boosting algorithms using confidence-rated predictions",
      "author" : [ "Schapire", "Robert E", "Singer", "Yoram" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Schapire et al\\.,? \\Q1999\\E",
      "shortCiteRegEx" : "Schapire et al\\.",
      "year" : 1999
    }, {
      "title" : "Boosting the margin: A new explanation for the effectiveness of voting methods",
      "author" : [ "Schapire", "Robert E", "Freund", "Yoav", "Barlett", "Peter", "Lee", "Wee Sun" ],
      "venue" : "In ICML, pp",
      "citeRegEx" : "Schapire et al\\.,? \\Q1997\\E",
      "shortCiteRegEx" : "Schapire et al\\.",
      "year" : 1997
    }, {
      "title" : "On the equivalence of weak learnability and linear separability: New relaxations and efficient boosting algorithms",
      "author" : [ "Shalev-Shwartz", "Shai", "Singer", "Yoram" ],
      "venue" : "In COLT, pp",
      "citeRegEx" : "Shalev.Shwartz et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Shalev.Shwartz et al\\.",
      "year" : 2008
    }, {
      "title" : "The Cauchy-Schwarz Master Class",
      "author" : [ "Steele", "J. Michael" ],
      "venue" : null,
      "citeRegEx" : "Steele and Michael.,? \\Q2004\\E",
      "shortCiteRegEx" : "Steele and Michael.",
      "year" : 2004
    }, {
      "title" : "A primal-dual convergence analysis of boosting",
      "author" : [ "Telgarsky", "Matus" ],
      "venue" : null,
      "citeRegEx" : "Telgarsky and Matus.,? \\Q2012\\E",
      "shortCiteRegEx" : "Telgarsky and Matus.",
      "year" : 2012
    }, {
      "title" : "Totally corrective boosting algorithms that maximize the margin",
      "author" : [ "Warmuth", "Manfred K", "Liao", "Jun", "Rätsch", "Gunnar" ],
      "venue" : "In ICML, pp",
      "citeRegEx" : "Warmuth et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Warmuth et al\\.",
      "year" : 2006
    }, {
      "title" : "Boosting with early stopping: Convergence and consistency",
      "author" : [ "Zhang", "Tong", "Yu", "Bin" ],
      "venue" : "The Annals of Statistics,",
      "citeRegEx" : "Zhang et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Zhang et al\\.",
      "year" : 2005
    }, {
      "title" : "The only other thing to check is that ` ∈ G, the class of losses considered by Telgarsky (2012",
      "author" : [ ],
      "venue" : null,
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2012
    }, {
      "title" : "Proof sketch. As discussed in the proof of Theorem 4.3, the results of Telgarsky (2012), which are superficially specialized to the Wolfe line search, carry over for the other line searches here with only a change",
      "author" : [ "≥ γ" ],
      "venue" : null,
      "citeRegEx" : "γ̂.,? \\Q2012\\E",
      "shortCiteRegEx" : "γ̂.",
      "year" : 2012
    } ],
    "referenceMentions" : [ {
      "referenceID" : 14,
      "context" : "One explanation for the efficacy of boosting is that it not only seeks aggregates with low empirical risk, but moreover that it prefers good margins, which leads to improved generalization (Schapire et al., 1997).",
      "startOffset" : 189,
      "endOffset" : 212
    }, {
      "referenceID" : 11,
      "context" : "Since AdaBoost does not attain maximum margins on general instances, a push was made to develop methods which carry such a guarantee (Rätsch & Warmuth, 2005; Shalev-Shwartz & Singer, 2008; Rudin et al., 2007).",
      "startOffset" : 133,
      "endOffset" : 208
    }, {
      "referenceID" : 7,
      "context" : "A third work, due to Rätsch et al. (2001), also proves margin maximizing properties of regularized line searches, but again without rates.",
      "startOffset" : 21,
      "endOffset" : 42
    }, {
      "referenceID" : 7,
      "context" : "A third work, due to Rätsch et al. (2001), also proves margin maximizing properties of regularized line searches, but again without rates. As mentioned in the introduction, margin maximization properties of AdaBoost have received extensive study; an excellent survey of results with pointers to other literature is provided by Schapire & Freund (2012, Chapter 5). Amongst these, a crucial result, due to Rudin et al. (2004), provides a concrete input to AdaBoost which yields suboptimal margins (which is used in Section 3.",
      "startOffset" : 21,
      "endOffset" : 424
    }, {
      "referenceID" : 7,
      "context" : "A third work, due to Rätsch et al. (2001), also proves margin maximizing properties of regularized line searches, but again without rates. As mentioned in the introduction, margin maximization properties of AdaBoost have received extensive study; an excellent survey of results with pointers to other literature is provided by Schapire & Freund (2012, Chapter 5). Amongst these, a crucial result, due to Rudin et al. (2004), provides a concrete input to AdaBoost which yields suboptimal margins (which is used in Section 3.3); that work also studies the evolution of these margins as a dynamical system, a topic which will reappear in Section 5. The primary contribution of this manuscript is to exhibit margin maximization, thus a natural comparison is to other algorithms with this same guarantee, for instance the works of Rätsch & Warmuth (2005), ShalevShwartz & Singer (2008), and Rudin et al.",
      "startOffset" : 21,
      "endOffset" : 850
    }, {
      "referenceID" : 7,
      "context" : "A third work, due to Rätsch et al. (2001), also proves margin maximizing properties of regularized line searches, but again without rates. As mentioned in the introduction, margin maximization properties of AdaBoost have received extensive study; an excellent survey of results with pointers to other literature is provided by Schapire & Freund (2012, Chapter 5). Amongst these, a crucial result, due to Rudin et al. (2004), provides a concrete input to AdaBoost which yields suboptimal margins (which is used in Section 3.3); that work also studies the evolution of these margins as a dynamical system, a topic which will reappear in Section 5. The primary contribution of this manuscript is to exhibit margin maximization, thus a natural comparison is to other algorithms with this same guarantee, for instance the works of Rätsch & Warmuth (2005), ShalevShwartz & Singer (2008), and Rudin et al.",
      "startOffset" : 21,
      "endOffset" : 881
    }, {
      "referenceID" : 7,
      "context" : "A third work, due to Rätsch et al. (2001), also proves margin maximizing properties of regularized line searches, but again without rates. As mentioned in the introduction, margin maximization properties of AdaBoost have received extensive study; an excellent survey of results with pointers to other literature is provided by Schapire & Freund (2012, Chapter 5). Amongst these, a crucial result, due to Rudin et al. (2004), provides a concrete input to AdaBoost which yields suboptimal margins (which is used in Section 3.3); that work also studies the evolution of these margins as a dynamical system, a topic which will reappear in Section 5. The primary contribution of this manuscript is to exhibit margin maximization, thus a natural comparison is to other algorithms with this same guarantee, for instance the works of Rätsch & Warmuth (2005), ShalevShwartz & Singer (2008), and Rudin et al. (2007) (or again refer to Schapire & Freund (2012, Chapter 5, Bibliographic Notes) for a more extensive summary).",
      "startOffset" : 21,
      "endOffset" : 906
    }, {
      "referenceID" : 7,
      "context" : "A third work, due to Rätsch et al. (2001), also proves margin maximizing properties of regularized line searches, but again without rates. As mentioned in the introduction, margin maximization properties of AdaBoost have received extensive study; an excellent survey of results with pointers to other literature is provided by Schapire & Freund (2012, Chapter 5). Amongst these, a crucial result, due to Rudin et al. (2004), provides a concrete input to AdaBoost which yields suboptimal margins (which is used in Section 3.3); that work also studies the evolution of these margins as a dynamical system, a topic which will reappear in Section 5. The primary contribution of this manuscript is to exhibit margin maximization, thus a natural comparison is to other algorithms with this same guarantee, for instance the works of Rätsch & Warmuth (2005), ShalevShwartz & Singer (2008), and Rudin et al. (2007) (or again refer to Schapire & Freund (2012, Chapter 5, Bibliographic Notes) for a more extensive summary). This manuscript will briefly compare with the methods of Shalev-Shwartz & Singer (2008), which subsume some earlier results and match the best guarantees, along with giving a simple, general, greedy scheme.",
      "startOffset" : 21,
      "endOffset" : 1101
    }, {
      "referenceID" : 7,
      "context" : "A third work, due to Rätsch et al. (2001), also proves margin maximizing properties of regularized line searches, but again without rates. As mentioned in the introduction, margin maximization properties of AdaBoost have received extensive study; an excellent survey of results with pointers to other literature is provided by Schapire & Freund (2012, Chapter 5). Amongst these, a crucial result, due to Rudin et al. (2004), provides a concrete input to AdaBoost which yields suboptimal margins (which is used in Section 3.3); that work also studies the evolution of these margins as a dynamical system, a topic which will reappear in Section 5. The primary contribution of this manuscript is to exhibit margin maximization, thus a natural comparison is to other algorithms with this same guarantee, for instance the works of Rätsch & Warmuth (2005), ShalevShwartz & Singer (2008), and Rudin et al. (2007) (or again refer to Schapire & Freund (2012, Chapter 5, Bibliographic Notes) for a more extensive summary). This manuscript will briefly compare with the methods of Shalev-Shwartz & Singer (2008), which subsume some earlier results and match the best guarantees, along with giving a simple, general, greedy scheme. The key distinction between previous work and the present work is firstly that the algorithmic modifications here are minor (in particular, the form of unregularized empirical risk minimization is unchanged), and that properties of an existing, widely used method are discerned (namely, the shrinkage procedure presented by Friedman (2000)).",
      "startOffset" : 21,
      "endOffset" : 1560
    }, {
      "referenceID" : 7,
      "context" : "A third work, due to Rätsch et al. (2001), also proves margin maximizing properties of regularized line searches, but again without rates. As mentioned in the introduction, margin maximization properties of AdaBoost have received extensive study; an excellent survey of results with pointers to other literature is provided by Schapire & Freund (2012, Chapter 5). Amongst these, a crucial result, due to Rudin et al. (2004), provides a concrete input to AdaBoost which yields suboptimal margins (which is used in Section 3.3); that work also studies the evolution of these margins as a dynamical system, a topic which will reappear in Section 5. The primary contribution of this manuscript is to exhibit margin maximization, thus a natural comparison is to other algorithms with this same guarantee, for instance the works of Rätsch & Warmuth (2005), ShalevShwartz & Singer (2008), and Rudin et al. (2007) (or again refer to Schapire & Freund (2012, Chapter 5, Bibliographic Notes) for a more extensive summary). This manuscript will briefly compare with the methods of Shalev-Shwartz & Singer (2008), which subsume some earlier results and match the best guarantees, along with giving a simple, general, greedy scheme. The key distinction between previous work and the present work is firstly that the algorithmic modifications here are minor (in particular, the form of unregularized empirical risk minimization is unchanged), and that properties of an existing, widely used method are discerned (namely, the shrinkage procedure presented by Friedman (2000)). As is standard in the above works, this manuscript is only concerned with convergence of empirical quantities. In order to prove convergence rates, this work relies heavily on techniques due to Telgarsky (2012). In particular, the scheme to prove convergence rates of empirical risk, detailed properties of splitting out a hard core from a boosting instance (cf.",
      "startOffset" : 21,
      "endOffset" : 1773
    }, {
      "referenceID" : 7,
      "context" : "A third work, due to Rätsch et al. (2001), also proves margin maximizing properties of regularized line searches, but again without rates. As mentioned in the introduction, margin maximization properties of AdaBoost have received extensive study; an excellent survey of results with pointers to other literature is provided by Schapire & Freund (2012, Chapter 5). Amongst these, a crucial result, due to Rudin et al. (2004), provides a concrete input to AdaBoost which yields suboptimal margins (which is used in Section 3.3); that work also studies the evolution of these margins as a dynamical system, a topic which will reappear in Section 5. The primary contribution of this manuscript is to exhibit margin maximization, thus a natural comparison is to other algorithms with this same guarantee, for instance the works of Rätsch & Warmuth (2005), ShalevShwartz & Singer (2008), and Rudin et al. (2007) (or again refer to Schapire & Freund (2012, Chapter 5, Bibliographic Notes) for a more extensive summary). This manuscript will briefly compare with the methods of Shalev-Shwartz & Singer (2008), which subsume some earlier results and match the best guarantees, along with giving a simple, general, greedy scheme. The key distinction between previous work and the present work is firstly that the algorithmic modifications here are minor (in particular, the form of unregularized empirical risk minimization is unchanged), and that properties of an existing, widely used method are discerned (namely, the shrinkage procedure presented by Friedman (2000)). As is standard in the above works, this manuscript is only concerned with convergence of empirical quantities. In order to prove convergence rates, this work relies heavily on techniques due to Telgarsky (2012). In particular, the scheme to prove convergence rates of empirical risk, detailed properties of splitting out a hard core from a boosting instance (cf. Section 4), and the notion of relative curvature (cf. Section 2.1) are all due to Telgarsky (2012). The intent of the present manuscript is to establish margin properties, and in this regard it departs from Telgarsky (2012); by contrast, the convergence rates of empirical risk presented here are thus trivial, but included since they did not appear explicitly in the literature.",
      "startOffset" : 21,
      "endOffset" : 2024
    }, {
      "referenceID" : 7,
      "context" : "A third work, due to Rätsch et al. (2001), also proves margin maximizing properties of regularized line searches, but again without rates. As mentioned in the introduction, margin maximization properties of AdaBoost have received extensive study; an excellent survey of results with pointers to other literature is provided by Schapire & Freund (2012, Chapter 5). Amongst these, a crucial result, due to Rudin et al. (2004), provides a concrete input to AdaBoost which yields suboptimal margins (which is used in Section 3.3); that work also studies the evolution of these margins as a dynamical system, a topic which will reappear in Section 5. The primary contribution of this manuscript is to exhibit margin maximization, thus a natural comparison is to other algorithms with this same guarantee, for instance the works of Rätsch & Warmuth (2005), ShalevShwartz & Singer (2008), and Rudin et al. (2007) (or again refer to Schapire & Freund (2012, Chapter 5, Bibliographic Notes) for a more extensive summary). This manuscript will briefly compare with the methods of Shalev-Shwartz & Singer (2008), which subsume some earlier results and match the best guarantees, along with giving a simple, general, greedy scheme. The key distinction between previous work and the present work is firstly that the algorithmic modifications here are minor (in particular, the form of unregularized empirical risk minimization is unchanged), and that properties of an existing, widely used method are discerned (namely, the shrinkage procedure presented by Friedman (2000)). As is standard in the above works, this manuscript is only concerned with convergence of empirical quantities. In order to prove convergence rates, this work relies heavily on techniques due to Telgarsky (2012). In particular, the scheme to prove convergence rates of empirical risk, detailed properties of splitting out a hard core from a boosting instance (cf. Section 4), and the notion of relative curvature (cf. Section 2.1) are all due to Telgarsky (2012). The intent of the present manuscript is to establish margin properties, and in this regard it departs from Telgarsky (2012); by contrast, the convergence rates of empirical risk presented here are thus trivial, but included since they did not appear explicitly in the literature.",
      "startOffset" : 21,
      "endOffset" : 2149
    }, {
      "referenceID" : 7,
      "context" : "A third work, due to Rätsch et al. (2001), also proves margin maximizing properties of regularized line searches, but again without rates. As mentioned in the introduction, margin maximization properties of AdaBoost have received extensive study; an excellent survey of results with pointers to other literature is provided by Schapire & Freund (2012, Chapter 5). Amongst these, a crucial result, due to Rudin et al. (2004), provides a concrete input to AdaBoost which yields suboptimal margins (which is used in Section 3.3); that work also studies the evolution of these margins as a dynamical system, a topic which will reappear in Section 5. The primary contribution of this manuscript is to exhibit margin maximization, thus a natural comparison is to other algorithms with this same guarantee, for instance the works of Rätsch & Warmuth (2005), ShalevShwartz & Singer (2008), and Rudin et al. (2007) (or again refer to Schapire & Freund (2012, Chapter 5, Bibliographic Notes) for a more extensive summary). This manuscript will briefly compare with the methods of Shalev-Shwartz & Singer (2008), which subsume some earlier results and match the best guarantees, along with giving a simple, general, greedy scheme. The key distinction between previous work and the present work is firstly that the algorithmic modifications here are minor (in particular, the form of unregularized empirical risk minimization is unchanged), and that properties of an existing, widely used method are discerned (namely, the shrinkage procedure presented by Friedman (2000)). As is standard in the above works, this manuscript is only concerned with convergence of empirical quantities. In order to prove convergence rates, this work relies heavily on techniques due to Telgarsky (2012). In particular, the scheme to prove convergence rates of empirical risk, detailed properties of splitting out a hard core from a boosting instance (cf. Section 4), and the notion of relative curvature (cf. Section 2.1) are all due to Telgarsky (2012). The intent of the present manuscript is to establish margin properties, and in this regard it departs from Telgarsky (2012); by contrast, the convergence rates of empirical risk presented here are thus trivial, but included since they did not appear explicitly in the literature. It is worth mentioning that these methods produce bad constants when applied to the logistic loss; unfortunately, previous work also suffers in this case (for instance, the work of Collins et al. (2002) provided only convergence of empirical risk, and not rates).",
      "startOffset" : 21,
      "endOffset" : 2508
    }, {
      "referenceID" : 18,
      "context" : "6). The explicit margin-maximizing method of ShalevShwartz & Singer (2008) requires t ≥ 32 ln(m)/ 2 iterations to achieve margin γ − , where ∈ (0, γ).",
      "startOffset" : 0,
      "endOffset" : 75
    }, {
      "referenceID" : 6,
      "context" : "position due to Telgarsky (2012), the decomposition itself has appeared, with various guarantees, in numerous places (Goldreich & Levin, 1989; Impagliazzo, 1995; Mukherjee et al., 2011).",
      "startOffset" : 117,
      "endOffset" : 185
    }, {
      "referenceID" : 18,
      "context" : "Indeed, consider the “soft-margin” boosting method (Shalev-Shwartz & Singer, 2008), originally due to Warmuth et al. (2006), which, roughly speaking, has a parameter controlling how many examples to give up on.",
      "startOffset" : 102,
      "endOffset" : 124
    }, {
      "referenceID" : 10,
      "context" : ") It should be possible to clarify this behavior from the perspective of dynamical systems: smaller steps dodge bad attractors (Rudin et al., 2004; 2007).",
      "startOffset" : 127,
      "endOffset" : 153
    }, {
      "referenceID" : 10,
      "context" : "in the case that ν = 1, this quantity has been extensively studied in the context of AdaBoost’s margins (Rätsch & Warmuth, 2005; Rudin et al., 2004; Schapire & Freund, 2012) ♦",
      "startOffset" : 104,
      "endOffset" : 173
    } ],
    "year" : 2013,
    "abstractText" : "This manuscript shows that AdaBoost and its immediate variants can produce approximate maximum margin classifiers simply by scaling step size choices with a fixed small constant. In this way, when the unscaled step size is an optimal choice, these results provide guarantees for Friedman’s empirically successful “shrinkage” procedure for gradient boosting (Friedman, 2000). Guarantees are also provided for a variety of other step sizes, affirming the intuition that increasingly regularized line searches provide improved margin guarantees. The results hold for the exponential loss and similar losses, most notably the logistic loss.",
    "creator" : "LaTeX with hyperref package"
  }
}