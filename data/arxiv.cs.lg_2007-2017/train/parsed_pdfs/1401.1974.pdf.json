{
  "name" : "1401.1974.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Bayesian Nonparametric Multilevel Clustering with Group-Level Contexts",
    "authors" : [ "Vu Nguyen", "Dinh Phung", "XuanLong Nguyen", "Svetha Venkatesh", "Hung Hai Bui" ],
    "emails" : [ "TVNGUYE@DEAKIN.EDU.AU", "DINH.PHUNG@DEAKIN.EDU.AU", "XUANLONG@UMICH.EDU", "SVETHA.VENKATESH@DEAKIN.EDU.AU", "BUI.H.HUNG@GMAIL.COM" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n40 1.\n19 74\nv4 [\ncs .L\nG ]\n2 9"
    }, {
      "heading" : "1. Introduction",
      "text" : "In many situations, content data naturally present themselves in groups, e.g., students are grouped into classes, classes grouped into schools, words grouped into documents, etc. Furthermore, each content group can be associated with additional context information (teachers of the class, authors of the document, time and location stamps).\nProceedings of the 31 st International Conference on Machine Learning, Beijing, China, 2014. JMLR: W&CP volume 32. Copyright 2014 by the author(s).\nDealing with grouped data, a setting known as multilevel analysis (Hox, 2010; Diez-Roux, 2000), has diverse application domains ranging from document modeling (Blei et al., 2003) to public health (Leyland & Goldstein, 2001).\nThis paper considers specifically the multilevel clustering problem in multilevel analysis: to jointly cluster both the content data and their groups when there is group-level context information. By context, we mean a secondary data source attached to the group of primary content data. An example is the problem of clustering documents, where each document is a group of words associated with grouplevel context information such as time-stamps, list of authors, etc. Another example is image clustering where visual image features (e.g. SIFT) are the content and image tags are the context.\nTo cluster groups together, it is often necessary to perform dimensionality reduction of the content data by forming content topics, effectively performing clustering of the content as well. For example, in document clustering, using bag-of-words directly as features is often problematic due to the large vocabulary size and the sparsity of the in-document word occurrences. Thus, a typical approach is to first apply dimensionality reduction techniques such as LDA (Blei et al., 2003) or HDP (Teh et al., 2006b) to find word topics (i.e., distributions on words), then perform document clustering using the word topics and the document-level context information as features. In such a cascaded approach, the dimensionality reduction step (e.g., topic modeling) is not able to utilize the context information. This limitation suggests that a better alternative is to perform context-aware document clustering and topic modeling jointly. With a joint model, one can expect to obtain improved document clusters as well as context-guided content topics that are more predictive of the data.\nRecent work has attempted to jointly capture word topics and document clusters. Parametric approaches (Xie & Xing, 2013) are extensions of the LDA (Blei et al., 2003) and require specifying the number of topics and clusters in advance. Bayesian nonparametric approaches including the nested Dirichlet process (nDP) (Rodriguez et al., 2008) and the multi-level clustering hierarchical Dirichlet Process (MLC-HDP) (Wulsin et al., 2012) can automatically adjust the number of clusters. We note that none of these methods can utilize context data.\nThis paper propose the Multilevel Clustering with Context (MC2), a Bayesian nonparametric model to jointly cluster both content and groups while fully utilizing grouplevel context. Using the Dirichlet process as the building block, our model constructs a product base-measure with a nested structure to accommodate both content and context observations. The MC2 model possesses properties that link the nested Dirichlet process (nDP) and the Dirichlet process mixture model (DPM) in an interesting way: integrating out all contents results in the DPM over contexts, whereas integrating out group-level context results in the nDP mixture over content variables. For inference, we provide an efficient collapsed Gibbs sampling procedure for the model.\nThe advantages of our model are: (1) the model automatically discovers the (unspecified) number of groups clusters and the number of topics while fully utilizing the context information; (2) content topic modeling is informed by group-level context information, leading to more predictive content topics; (3) the model is robust to partially missing context information. In our experiments, we demonstrate that our proposed model achieves better document clustering performances and more predictive word topics in realworld datasets in both text and image domains."
    }, {
      "heading" : "2. Related Background",
      "text" : "There have been extensive works on clustering documents in the literature. Due to limited scope of the paper, we only describe works closely related to probabilistic topic models. We note that standard topic models such as LDA (Blei et al., 2003) or its nonparametric Bayesian counter part, HDP (Teh et al., 2006b) exploits the group structure for word clustering. However these models do not cluster documents.\nAn approach to document clustering is to employ a twostage process. First, topic models (e.g. LDA or HDP) are applied to extract the topics and their mixture proportion for each document. Then, this is used as feature input to another clustering algorithm. Some examples of this approach include the use of LDA+Kmeans for image clustering (Xuan et al., 2011; Elango & Jayaraman, 2005) and\nHDP+Affinity Propagation for clustering human activities (Nguyen et al., 2013).\nA more elegant approach is to simultaneously cluster documents and discover topics. The first Bayesian nonparametric model proposed for this task is the nested Dirichlet Process (nDP) (Rodriguez et al., 2008) where documents in a cluster share the same distribution over topic atoms. Although the original nDP does not force the topic atoms to be shared across document clusters, this can be achieved by simply introducing a DP prior for the nDP base measure. The same observation was also made by (Wulsin et al., 2012) who introduced the MLC-HDP, a 3-level extension to the nDP. This model thus can cluster words, documents and document-corpora with shared topic atoms throughout the group hierarchy. Xie et al (Xie & Xing, 2013) recently introduced the Multi-Grain Clustering Topic Model which allows mixing between global topics and document-cluster topics. However, this is a parametric model which requires fixing the number of topics in advance. More crucially, all of these existing models do not attempt to utilize grouplevel context information."
    }, {
      "heading" : "Modelling with Dirichlet Process",
      "text" : "We provide a brief account of the Dirichlet process and its variants. The literature on DP is vast and we refer to (Hjort et al., 2010) for a comprehensive account. Here we focus on DPM, HDP and nDP which are related to our work.\nDirichlet process (Ferguson, 1973) is a basic building block in Bayesian nonparametrics. Let (Θ,B, H) be a probability measure space, and γ is a positive number, a Dirichlet process DP (γ,H) is a distribution over discrete random probability measure G on (Θ,B). Sethuraman (Sethuraman, 1994) provides an alternative constructive definition which makes the discreteness property of a draw from a Dirichlet process explicit via the stick-breaking representation: G = ∑∞\nk=1 βkδφk where φk iid ∼ H, k = 1, . . . ,∞ and\nβ = (βk) ∞ k=1 are the weights constructed through a ‘stickbreaking’ process βk = vk ∏ s<k (1− vs) with vk iid ∼ Beta (1, γ). It can be shown that ∑∞\nk=1 βk = 1 with probability one, and as a convention (Pitman, 2002), we hereafter write β ∼ GEM (γ).\nDue to its discrete nature, Dirichlet process has been widely used in Bayesian mixture models as the prior distribution on the mixing measures, each is associated with an atom φk in the stick-breaking representation of G above. A likelihood kernel F (·) is used to generate data xi | φk iid ∼ F (· | φk), resulting in a model known as the Dirichlet process mixture model (DPM), pioneered by the work of (Antoniak, 1974) and subsequently developed by many others. In section 3 we provide a precise definition for"
    }, {
      "heading" : "DPM.",
      "text" : "While DPM models exchangeable data within a single group, the Dirichlet process can also be constructed hierarchically to provide prior distributions over multiple exchangeable groups. Under this setting, each group is modelled as a DPM and these models are ‘linked’ together to reflect the dependency among them – a formalism which is generally known as dependent Dirichlet processes (MacEachern, 1999). One particular attractive approach is the hierarchical Dirichlet processes (Teh et al., 2006b) which posits the dependency among the group-level DPM by another Dirichlet process, i.e., Gj | α,G0 ∼ DP (α,G0) and G0 | γ,H ∼ DP (γ,H) where Gj is the prior for the j-th group, linked together via a discrete measure G0 whose distribution is another DP.\nYet another way of using DP to model multiple groups is to construct random measure in a nested structure in which the DP base measure is itself another DP. This formalism is the nested Dirichlet Process (Rodriguez et al., 2008), specifically Gj iid ∼ U where U ∼ DP (α× DP (γH)). Modeling Gj (s) hierarchically as in HDP and nestedly as in nDP yields different effects. HDP focuses on exploiting statistical strength across groups via sharing atoms φk (s), but it does not partition groups into clusters. This statement is made precisely by noting that P (Gj = Gj′ ) = 0 in HDP. Whereas, nDP emphasizes on inducing clusters on both observations and distributions, hence it partitions groups into clusters. To be precise, the prior probability of two groups being clustered together is P (Gj = Gj′ ) = 1 a+1 . Finally we note that this original definition of nDP in (Rodriguez et al., 2008) does not force the atoms to be shared across clusters of groups, but this can be achieved by simply introducing a DP prior for the nDP base measure, a modification that we use in this paper. This is made clearly in our definition for nDP mixture in section 3."
    }, {
      "heading" : "3. Multilevel Clustering with Contexts",
      "text" : ""
    }, {
      "heading" : "3.1. Model description and stick-breaking",
      "text" : "Consider data presented in a two-level group structure as follows. Denote by J the number of groups; each group j contains Nj exchangeable data points, represented by wj = { wj1, wj2, . . . , wjNj } . For each group j, the groupspecific context data is denoted by xj . Assuming that the groups are exchangeable, the overall data is {(xj ,wj)} J\nj=1. The collection {w1, . . . ,wJ} represents observations of the group contents, and {x1, . . . , xJ} represents observations of the group-level contexts.\nWe now describe the generative process of MC2 that generates a two-level clustering of this data. We use a grouplevel DP mixture to generate an infinite cluster model for\ngroups. Each group cluster k is associated with an atom having the form of a pair (φk, Q∗k) where φk is a parameter that generates the group-level contexts within the cluster and Q∗k is a measure that generates the group contents within the same cluster.\nTo generate atomic pairs of context parameter and measurevalued content parameter, we introduce a product basemeasure of the form H × DP(vQ0) for the group-level DP mixture. Drawing from a DP mixture with this base measure, each realization is a pair (θj , Qj); θj is then used to generate the context xj and Qj is used to repeatedly produce the set of content observations wji within the group j. Specifically,\nU ∼ DP (α(H × DP(vQ0)))where Q0 ∼ DP (ηS)\n(θj , Qj) iid ∼ U for each group j (1)\nxj ∼ F (.|θj), ϕji iid ∼ Qj , wji ∼ Y (.|ϕji)\nIn the above, H and S are respectively base measures for context and content parameters θj andϕji. The context and content observations are then generated via the likelihood kernels F (· | θj) and Y (· | ϕji). To simplify inference, H and S are assumed to be conjugate to F and Y respectively. The generative process is illustrated in Figure 1."
    }, {
      "heading" : "STICK-BREAKING REPRESENTATION",
      "text" : "We now derive the stick-breaking construction for MC2 where all the random discrete measures are specified by a distribution over integers and a countable set of atoms. The random measure U in Eq. (7) has the stick-breaking form:\nU =\n∞∑\nk=1\nπkδ(φk,Q∗k) (2)\nwhere π ∼ GEM (α) and (φk, Q∗k) iid ∼ H × DP (vQ0). Equivalently, this means φk is drawn i.i.d. from H and Q∗k drawn i.i.d. from DP (vQ0). Since Q0 ∼ DP (ηS), Q0 and Q∗k have the standard HDP (Teh et al., 2006b)\nstick-breaking forms: Q0 = ∑∞ m=1 ǫmδψmwhere ǫ ∼ GEM(η), ψm iid ∼ S; Q∗k = ∑∞ m=1 τk,mδψm where τ k = (τk1, τk2, . . .) ∼ DP (v, ǫ).\nFor each group j we sample the parameter pair (θj , Qj) iid ∼ U ; equivalently, this means drawing zj iid ∼ π and letting θj = φzj and Qj = Q ∗ zj . For the i-th content data within the group j, the content parameter ϕji is drawn iid ∼ Qj = Q∗zj ; equivalently, this means drawing lji iid ∼ τzj and letting ϕji = ψlji . Figure 1 presents the graphical model of this stick-breaking representation."
    }, {
      "heading" : "3.2. Inference and Polya Urn View",
      "text" : "We use collapsed Gibbs sampling, integrating out φk(s), ψm(s), π and τk (s). Latent variables z, l, ǫ and the hyperparameters α, v, η will be resampled. We only describe the key inference steps in sampling z, l and ǫ here and refer to Appendix A.2 for the rest of the details (including how to sample the hyper-parameters).\nSampling z. The required conditional distribution is p(zj = k | z−j , l,x, α,H) ∝\np (zj = k|z−j, α) p (xj |zj = k, z−j ,x−j , H)\n× p (lj∗|zj = k, l−j∗, z−j , ǫ, v)\nThe first term can be recognized as a form of the Chinese restaurant process (CRP). The second term is the predictive likelihood for the context observations under the component φk after integrating out φk. This can be evaluated analytically due to conjugacy of F and H . The last term is the predictive likelihood for the group content-index lj∗ = {lji|i = 1 . . .Nj}. Since lji | zj = k iid ∼ Mult (τ k) where τ k ∼ Dir (vǫ1, . . . , vǫM , ǫnew), the last term can also be evaluated analytically by integrating out τ k using the Multinomial-Dirichlet conjugacy property.\nSampling l. Let w−ji be the same set as w excluding wji, let w−ji(m) = {wj′i′ |(j′i′) 6= (ji) ∧ lj′i′ = m} and l−ij(k) = {lj′i′ |(j′i′) 6= (ji) ∧ zj′ = k}. Then p (lji = m | l−ji, zj = k, z−j, v,w, ǫ, S) ∝\np(wji|l, w−ji, S) p(lji = m|l−ji, zj = k, z−j, ǫ, v)\n=p (wji | w−ji(m), S) p (lji = m | l−ji(k), ǫ, v)\nThe first term is the predictive likelihood under mixture componentψm after integrating out ψm, which can be evaluated analytically due to the conjugacy of Y and S. The second term is in the form of a CRP similar to the one that arises during inference for HDP (Teh et al., 2006b).\nSampling ǫ. Sampling ǫ requires information from both z and l.\np (ǫ | l, z, v, η) ∝ p (l | ǫ, v, z, η)× p (ǫ | η) (3)\nUsing a similar strategy in HDP, we introduce auxiliary variables (okm), then alternatively sample together with ǫ:\np (okm = h | ·) ∝ Stirl (h, nkm) (vǫm)h, h = 0, 1, . . . , nkm\np (ǫ | ·) ∝ ǫη−1new\nM∏\nm=1\nǫ ∑ k okm−1\nm\nwhere Stirl (h, nkm) is the Stirling number of the first kind, nkm is the count of seeing the pair (zj = k, lji = m) : ∀i, j, and finally M is the current number of active content topics. It clear that okm can be sampled from a Multinomial distribution and ǫ from an (M + 1)-dim Dirichlet distribution."
    }, {
      "heading" : "POLYA URN VIEW",
      "text" : "Our model exhibits a Polya-urn view using the analogy of a fleet of buses, driving customers to restaurants. Each bus represents a group and customers on the bus are data points within the group. For each bus j, zj acts as the index to the restaurant for its destination. Thus, buses form clusters at their destination restaurants according to a CRP: a new bus drives to an existing restaurant with the probability proportional to the number of other buses that have arrived at that restaurant, and with probability proportional to α, it goes to a completely new restaurant.\nOnce all the buses have delivered customers to the restaurants, all customers at the restaurants start to behave in the same manner as in a Chinese restaurant franchise (CRF) process: customers are assigned tables according to a restaurant-specific CRP; tables are assigned with dishes ψm (representing the content topic atoms) according to a global franchise CRP. In addition to the usual CRF, at restaurant k, a single dessert φk (which represents the context-generating atom, drawing iid ∼ from H) will be served to all the customers at that restaurant. Thus, every customer on the same bus j will be served the same dessert φzj . We observe three sub-CRPs, corresponding to the three DP(s) in our model: the CRP at the dish level is due to the DP (ηS), the CRP forming tables inside each restaurant is due to the DP(vQ0), and the CRP aggregating buses to restaurants is due to the DP (α(H × DP(vQ0)))."
    }, {
      "heading" : "3.3. Marginalization property",
      "text" : "We study marginalization property for our model when either the content topics ϕji (s) or context topics θj (s) are marginalized out. Our main result is established in Theorem 9 where we show an interesting link to nested DP and DPM via our model.\nLet H be a measure over some measurable spaces (Θ,Σ). Let P be the set of all measures over (Θ,Σ), suitably endowed with some σ-algebra. Let G ∼ DP(αH) and\nθi iid ∼ G. The collection (θi) then follows the DP mixture distribution which is defined formally below.\nDefinition 1. (DPM) A DPM is a probability measure over Θn ∋ (θ1, . . . , θn) with the usual product sigma algebra Σn such that for every collection of measurable sets {(S1, . . . , Sn) : Si ∈ Σ, i = 1, . . . , n}:\nDPM(θ1 ∈ S1, . . . , θn ∈ Sn|α,H)\n=\n∫ n∏\ni=1\nG (Si)DP (dG | αH)\nWe now state a result regarding marginalization of draws from a DP mixture with a joint base measure. Consider two measurable spaces (Θ1,Σ1) and (Θ2,Σ2) and let (Θ,Σ) be their product space where Θ = Θ1×Θ2 and Σ = Σ1×Σ2. Let H∗ be a measure over the product space Θ = Θ1 ×Θ2 and let H1 be the marginal of H∗ over Θ1 in the sense that for any measurable set A ∈ Σ1, H1 (A) = H∗ (A×Θ2). Then drawing (θ(1)i , θ (2) i ) from a DP mixture with base measure αH and marginalizing out (θ(2)i ) is the same as drawing (θ(1)i ) from a DP mixture with base measure H1. Formally Proposition 2. Denote by θi the pair ( θ (1) i , θ (2) i ) , there holds\nDPM (\nθ (1) 1 ∈ S1, . . . , θ (1) n ∈ Sn | αH1\n)\n= DPM (θ1 ∈ S1 ×Θ2, . . . , θn ∈ Sn ×Θ2 | αH ∗)\nfor every collection of measurable sets {(S1, . . . , Sn) : Si ∈ Σ1, i = 1, . . . , n}.\nProof. see Appendix 7.\nNext we give a formal definition for the nDP mixture: ϕji iid ∼ Qj , Qj iid ∼ U , U ∼ DP(αDP(vQ0)), Q0 ∼ DP (ηS).\nDefinition 3. (nested DP Mixture) An nDPM is a probability measure over Θ ∑J j=1\nNj ∋ (ϕ11, . . . , ϕ1N1 , . . . , ϕJNJ ) equipped with the usual product sigma algebra ΣN1 × . . .×ΣNJ such that for every collection of measurable sets {(Sji) : Sji ∈ Σ, j = 1, . . . , J, i = 1 . . . , Nj}:\nnDPM(ϕji ∈ Sji, ∀i, j|α, v, η, S)\n=\n∫ ∫ \n\n\nJ∏\nj=1\n∫ Nj∏\ni=1\nQj (Sji)U (dQj)\n \n\n× DP (dU | αDP (vQ0))DP (dQ0 | η, S)\nWe now have the sufficient formalism to state the marginalization result for our model.\nTheorem 4. Given α,H and α, v, η, S, let θ = (θj : ∀j) and ϕ = (ϕji : ∀j, i) be generated as in Eq (7). Then, marginalizing out ϕ results in DPM (θ | α,H), whereas marginalizing out θ results in nDPM (ϕ|α, v, η, S).\nProof. We sketch the main steps, Appendix 9 provides more detail. Let H∗ = H1 × H2, we note that when either H1 or H2 are random, a result similar to Proposition 7 still holds by taking the expectation on both sides of the equality. Now let H1 = H and H2 = DP (vQ0) where Q0 ∼ DP(ηS) yields the proof for the marginalization of ϕ; let H1 = DP (vQ0) and H2 = H yields the proof for the marginalization of θ."
    }, {
      "heading" : "4. Experiments",
      "text" : "We first evaluate the model via simulation studies, then demonstrate its applications on text and image modeling using three real-world datasets. Throughout this section, unless explicitly stated, discrete data is modeled by Multinomial with Dirichlet prior, while continuous data is modeled by Gaussian (unknown mean and unknown variance) with Gaussian-Gamma prior."
    }, {
      "heading" : "4.1. Simulation studies",
      "text" : "The main goal is to investigate the posterior consistency of the model, i.e., its ability to recover the true group clusters, context distribution and content topics. To synthesize the data, we use M = 13 topics which are the 13 unique letters in the ICML string “INTERNATIONAL CONFERENCE MACHINE LEARNING”. Similar to (Griffiths & Steyvers, 2004), each topic ψm is a distribution over 35 words (pixels) and visualized as a 7 × 5 binary image. We generate K = 4 clusters of 100 documents each. For each cluster, we choose a set of topics corresponding to letters in the each of 4 words in the ICML string. The topic mixing distribution τk is an uniform distribution over the chosen topic letters. Each cluster is also assigned a context-generating univariate Gaussian distribution. These generating parameters are shown in Figure 2 (left). Altogether we have J = 400 documents; for each document we sample Nj = 50 words and a context variable xj drawing from the cluster-specific Gaussian.\nWe model the word wji with Multinomial and Gaussian for context xj . After 100 Gibbs iterations, the number of context and content topics (K = 4,M = 13) are recovered correctly: the learned context atoms φk and topic ψm are almost identical to the ground truth (Figure 2, right) and the model successfully identifies the 4 clusters of documents with topics corresponding to the 4 words in the ICML string.\nTo demonstrate the importance of context observation, we\nthen run LDA and HDP with only the word observations (ignoring context) where the number of topic of LDA is set to 13. As can be seen from Figure 2 (right), LDA and HDP have problems in recovering the true topics. They cannot distinguish small differences between the overlapping character topics (e.g M vs N, or I vs T). Further analysis of the role of context in MC2 is provided in Appendix A.3."
    }, {
      "heading" : "4.2. Experiments with Real-World Datasets",
      "text" : "We use two standard NIPS and PNAS text datasets, and the NUS-WIDE image dataset.\nNIPS contains 1,740 documents with vocabulary size 13,649 (excluding stop words); timestamps (1987-1999), authors (2,037) and title information are available and used as group-level context. PNAS contains 79,800 documents, vocab size = 36,782 with publication timestamp (915- 2005). For NUS-WIDE we use a subset of the 13-class animals 1 comprising of 3,411 images (2,054 images for training and 1357 images for testing) with off-the-shelf features including 500-dim bag-of-word SIFT vector and 1000-dim bag-of-tag annotation vector."
    }, {
      "heading" : "Text Modeling with Document-Level Contexts",
      "text" : "We use NIPS and PNAS datasets with 90% for training and 10% for held-out perplexity evaluation. We compare the perplexity with HDP (Teh et al., 2006b) where no grouplevel context can be used, and npTOT (Dubey et al., 2012) where only timestamp information can be used. We note that unlike our model, npTOT requires replication of document timestamp for every word in the document, which is somewhat unnatural.\nWe use perplexity score (Blei et al., 2003) on\n1downloaded from http://www.ml-thu.net/~jun/data/\nheld-out data as performance metric, defined as2 exp { − ∑J\nj=1 log p ( w test j | x\ntrain,wtrain ) / ( ∑\nj N test j\n)}\n.\nTo ensure fairness and comparable evaluation, only words in held-out data is used to compute the perplexity. We use univariate Gaussian for timestamp and Multinomial distributions for words, tags and authors. We ran collapsed Gibbs for 500 iterations after 100 burn-in samples.\nTable 1 shows the results where MC2 achieves significant better performance. This shows that group-level context information during training provide useful guidance for the modelling tasks. Regarding the informative aspect of group-level context, we achieve better perplexity with timestamp information than with titles and authors. This may be explained by the fact that 1361 authors (among 2037) show up only once in the data while title provides little additional information than what already in that abstracts. Interestingly, without the group-level context information, our model still predicts the held-out words better than HDP. This suggests that inducing partitions over documents simultaneously with topic modelling is beneficial.\nBeyond the capacity of HDP and npTOT, our model can induce clusters over documents (value of K in Table 1). Figure 3 shows an example of one such document cluster discovered from NIPS data with authors as context.\nOur proposed model also allows flexibility in deriving useful understanding into the data and to evaluate on its predictive capacity (e.g., who most likely wrote this article, which authors work in the same research topic and so on). Another possible usage is to obtain conditional distributions among context topics φk (s) and content topics ψm (s). For example if the context information is timestamp, the model immediately yields the distribution over time for\n2Appendix A.4 provides further details on how to derive this score from our model\na topic, showing when the topic rises and falls. Figure 4 illustrates an example of a distribution over time for a content topic discovered from PNAS dataset where timestamp was used as context. This topic appears to capture a congenital disorder known as Albinism. This distribution illustrates research attention to this condition over the past 100 years from PNAS data. To seek evidence for this result, we search the term “Albinism” in Google Scholar, using the top 50 searching results and plot the histogram over time in the same figure. Surprisingly, we obtain a very close match between our results and the results from Google Scholar as evidenced in the figure."
    }, {
      "heading" : "Image Clustering with Image-Level Tags",
      "text" : "We evaluate the clustering capacity of MC2 using contexts on an image clustering task. Our dataset is NUS-WIDE described earlier. We use bag-of-word SIFT features from each image for its content. Since each image in this dataset\ncomes with a set of tags, we exploit them as context information, hence each context observation xj is a bag-of-tag annotation vector.\nFirst we perform the perplexity evaluation for this dataset using a similar setting as in the previous section. Table 2 presents the results where our model again outperforms HDP even when no context (tags) is used for training.\nNext we evaluate the clustering quality of the model using the provided 13 classes as ground truth. We report performance on four well-known clustering evaluation metrics: Purity, Normalized Mutual Information (NMI), RandIndex (RI), and Fscore (detailed in (Rand, 1971; Cai et al., 2011)). We use the following baselines for comparison:\n• Kmeans and Non-negative Matrix Factorization (NMF)(Lee & Seung, 1999). For these methods, we need to specify the number of clusters in advance, hence we vary this number from 10 to 40. We then report the min, max, mean and standard deviation.\n• Affinity Propagation (AP) (Frey & Dueck, 2007): AP\nrequires a similarity score between two documents and we use the Euclidean distance for this purpose.\n• Hierarchical Dirichlet Process (HDP) + AP: we first run HDP using content observations, and then apply Affinity Propagation with similarity score derived from the symmetric KL divergence between the mixture proportions from two documents.\nFigure 5 shows the result in which our model consistently delivers highest performance across all four metrics. For purity and NMI, our model beats all by a wide margin.\nTo gain some understanding on the clusters of images induced by our model, we run t-SNE (Van der Maaten & Hinton, 2008), projecting the feature vectors (both content and context) onto a 2D space. For visual clarity, we randomly select 7 out of 28 clusters and display in Figure 6 where it can be seen that they are reasonably well separated.\nEffect of partially observed and missing data\nMissing and unlabelled data is commonly encountered in\npractical applications. Here we examine the effect of context observability on document clustering performance. To do so, we again use the NUS-WIDE 13-animal subset as described previously, then vary the amount of observing context observation xj with missing proportion ranges from 0% to 100%.\nTable 3 reports the result. We make two observations: a) utilizing context results in a big performance gain as evidenced in the difference between the top and bottom row of the table, and b) as the proportion of missing context starts to increase, the performance degrades gracefully up to 50% missing. This demonstrates the robustness of model against the possibility of missing context data."
    }, {
      "heading" : "5. Conclusion",
      "text" : "We have introduced an approach for multilevel clustering when there are group-level context information. Our MC2 provides a single joint model for utilizing group-level contexts to form group clusters while discovering the shared topics of the group contents at the same time. We provide a collapsed Gibbs sampling procedure and perform extensive experiments on three real-world datasets in both text and image domains. The experimental results using our model demonstrate the importance of utilizing context information in clustering both at the content and at the group level. Since similar types of contexts (time, tags, locations, ages, genres) are commonly encountered in many real-world data sources, we expect that our model will also be further applicable in other domains.\nOur model contains a novel ingredient in DP-based Bayesian nonparametric modeling: we propose to use a base measure in the form of a product between a context-generating prior H and a content-generating prior DP(vQ0). Doing this results in a new model with one marginal being the DPM and another marginal being the nDP mixture, thus establishing an interesting bridge between the DPM and the nDP. Our product base measure construction can be generalized to yield new models suitable for data presenting in more complicated nested group structures (e.g., more than 2-level deep)."
    }, {
      "heading" : "A. Appendix",
      "text" : "This note provides supplementary information for the main paper. It has three parts: a) the proof for the marginalization property of our proposed model, b) detailed derivations for our inference, and c) equations to show how the perplexity in the experiment was computed."
    }, {
      "heading" : "A.1. Proof for Marginalization Property (Theorem 4)",
      "text" : "We start with a proposition on the marginalization result for DPM with the product measure then move on the final proof for our proposed model."
    }, {
      "heading" : "A.1.1. MARGINALIZATION OF DPM WITH PRODUCT MEASURE",
      "text" : "Let H be a measure over some measurable space (Θ,Σ). Let P be the set of all measures over (Θ,Σ), suitably endowed with some σ-algebra. Let G ∼ DP(αH) be a draw from a Dirichlet process.\nLemma 5. Let S1 . . . Sn be n measurable sets in Σ. We form a measurable partition of Θ, a collection of disjoint measurable sets, that generate S1, . . . , Sn as follows. If S is a set, let S1 = S and S−1 = Θ\\S. Then S∗ = { ⋂n\ni=1 S ci i |ci ∈ {1,−1}} is a partition of Θ into a finite collection of disjoint measurable sets with the property that any Si can be written as a union of some sets in S∗. Let the element of S∗ be A1 . . . An∗ (note n∗ ≤ 2n). Then the expectation\nE G [G (S1) , . . . , G (Sn)] = (4)\n∫ n∏\ni=1\nG (Si)DP (dG | αH) (5)\ndepends only on α and H(Ai). In other words, the above expectation can be written as a function En(α,H(A1), . . . H(An∗)).\nIt is easy to see that since Si can always be expressed as the sum of some disjoints Ai, G (Si) can respectively be written as the sum of some G (Ai). Furthermore, by definition of a Dirichlet process, the vector (G (A1) , . . . , G (An∗)) distributed according to a finite Dirichlet distribution (αH (A1) , . . . , αH (An∗)), therefore the expectation E\nG [G (Si)] depends only on α and\nH (Ai) (s).\nDefinition 6. (DPM) A DPM is a probability measure over Θn ∋ (θ1, . . . , θn) with the usual product sigma algebra Σn such that for every collection of measurable sets {(S1, . . . , Sn) : Si ∈ Σ, i = 1, . . . , n}:\nDPM(θ1 ∈ S1, . . . , θn ∈ Sn|α,H) = (6) ∫\nG\nn∏\ni=1\nG (Si)DP (dG | αH)\nConsider two measurable spaces (Θ1,Σ1) and (Θ2,Σ2) and let (Θ,Σ) be their product space where Θ = Θ1 ×Θ2 and Σ = Σ1 × Σ2. We present the general theorem that states the marginal result from a product base measure.\nProposition 7. Let H∗ be a measure over the product spaceΘ = Θ1×Θ2. Let H1 be the marginal of H∗ over Θ1 in the sense that for any measurable set A ∈ Σ1, H1 (A) = H∗ (A×Θ2). Denote by θi the pair ( θ (1) i , θ (2) i ) , then:\nDPM (\nθ (1) 1 ∈ S1, . . . , θ (1) n ∈ Sn | αH1\n)\n= DPM (θ1 ∈ S1 ×Θ2, . . . , θn ∈ Sn ×Θ2 | αH ∗)\nfor every collection of measurable sets {(S1, . . . , Sn) : Si ∈ Σ1, i = 1, . . . , n}.\nProof. Since {(S1, . . . , Sn) : Si ∈ Σ1, i = 1, . . . , n} are rectangles, expanding the RHS using Definition 6 gives:\nRHS =\n∫\nG (S1 ×Θ2) . . .G (Sn ×Θ2) dDP(dG|α,H∗)\nLet Ti = Si × Θ2, the above expression is the expectation of ∏\niG(Ti) when G ∼ DP (αH ∗). Forming col-\nlection of the disjoint measurable sets T ∗ = (B1 . . . Bn∗) that generates Ti, then note that Bi = Ai × Θ2, and S∗ = (A1 . . . An∗) generates Si. By definition of H1, H1(Ai) = H ∗(Ai × Θ2) = H ∗(Bi). Using the Lemma 5 above, RHS = En(α,H∗(B1) . . .H∗(Bn∗)), while LHS = En(α,H1(A1) . . . H1(An∗)) and they are indeed the same.\nWe note that H∗ can be any arbitrary measure on Θ and, in general, we do not require H∗ to factorize as product measure."
    }, {
      "heading" : "A.1.2. MARGINALIZATION RESULT FOR OUR PROPOSED MODEL",
      "text" : "Recall that we are considering a product base-measure of the form H∗ = H ×DP(vQ0) for the group-level DP mixture. Drawing from a DP mixture with this base measure, each realization is a pair (θj , Qj); θj is then used to generate the context xj and Qj is used to repeatedly generate the\nset of content observations wji within the group j. Specifically,\nU ∼ DP (α(H × DP(vQ0))) where Q0 ∼ DP (ηS)\n(θj , Qj) iid ∼ U for j = 1, . . . , J (7)\nϕji iid ∼ Qj, for each j and i = 1, . . . , Nj\nIn the above, H and S are respectively base measures for context and content parameters θj and ϕji. We start with a definition for nested Dirichlet Process Mixture (nDPM) to proceed further.\nDefinition 8. (nested DP Mixture) An nDPM is a probability measure over Θ ∑ J j=1\nNj ∋ (ϕ11, . . . , ϕ1N1 , . . . , ϕJNJ ) equipped with the usual product sigma algebra ΣN1 × . . .×ΣNJ such that for every collection of measurable sets {(Sji) : Sji ∈ Σ, j = 1, . . . , J, i = 1 . . . , Nj}:\nnDPM(ϕji ∈ Sji, ∀i, j|α, v, η, S)\n=\n∫ ∫ \n\n\nJ∏\nj=1\n∫ Nj∏\ni=1\nQj (Sji)U (dQj)\n \n\n× DP (dU | αDP (vQ0))DP (dQ0 | η, S)\nWe now state the main marginalization result for our proposed model.\nTheorem 9. Given α,H and α, v, η, S, let θ = (θj : ∀j) and ϕ = (ϕji : ∀j, i) be generated as in Eq (7). Then, marginalizing out ϕ results in DPM (θ | α,H), whereas marginalizing out θ results in nDPM (ϕ|α, v, η, S).\nProof. First we make observation that if we can show Proposition 7 still holds when H1 is random with H2 is fixed and vice versa, then the proof required is an immediate corollary of Proposition 7 by letting H∗ = H1 × H2 where we first let H1 = H , H2 = DP (vQ0) to obtain the proof for the first result, and then swap the order H1 = DP (vQ0) , H2 = H to get the second result.\nTo see that Proposition 7 still holds when H2 is a random measure and H1 is fixed, we let the product base measure H∗ = H1 × H2 and further let µ be a prior probability measure for H2, i.e, H2 ∼ µ (·). Denote by θi the pair( θ (1) i , θ (2) i ) , consider the marginalization over H2:\n∫\nH2\nDPM (θ1 ∈ S1 ×Θ2, . . . , θn ∈ Sn ×Θ2 | α,H∗)µ (H2)\n=\n∫\nΣ2\nDPM (\nθ (1) 1 ∈ S1, . . . , θ (1) n ∈ Sn | α,H1\n)\n︸ ︷︷ ︸\nconstant w.r.t H2\nµ (H2)\n= DPM (\nθ (1) 1 ∈ S1, . . . , θ (1) n ∈ Sn | α,H1\n) ∫\nΣ2\nµ (H2)\n= DPM (\nθ (1) 1 ∈ S1, . . . , θ (1) n ∈ Sn | α,H1\n)\nWhen H1 is random and H2 is fixed. Let λ (·) be a prior probability measure for H1, ie., H1 ∼ λ (·). It is clear that Proposition 7 holds for each draw H1 from λ (·). This complete our proof."
    }, {
      "heading" : "A.1.3. ADDITIONAL RESULT FOR CORRELATION",
      "text" : "ANALYSIS IN NDPM\nWe now consider the correlation between ϕik and ϕjk′ for arbitrary i, j, k and k′, i.e., we need to evaluate:\nP (ϕik ∈ A1, ϕjk′ ∈ A2 | α, η, v, S)\nfor two measurable sets A1, A2 ∈ Σ by integrating out over all immediate random measures. We use an explicit stick-breaking representation for U where U ∼ DP (αDP (vQ0)) as follows\nU =\n∞∑\nk=1\nπkδQ∗ k\n(8)\nwhere π ∼ GEM (α) and Q∗k iid ∼ DP (vQ0). We use the notation δQ∗ k\nto denote the atomic measure on measure, placing its mass at measure Q∗k.\nFor i = j, we have:\nP (ϕik ∈ A1, ϕjk′ ∈ A2 | Q1, . . . , QJ) = Qi (A1)Qi (A2)\nSequentially take expectation over Qi and U :\n∫\nQi\nQi (A1)Qi (A2) dU (Qi) =\n∫\nQi\nQi (A1)Qi (A2) d\n( ∞∑\nk=1\nπkδQ∗ k\n)\n=\n∑\nk\nπk [Q ∗ k (A1)Q ∗ k (A2)]\n∫\nU\n∞∑\nk=1\nπk [Q ∗ k (A1)Q ∗ k (A2)] dDP (U | αDP (vQ0)) =\nE\n{ ∑\nk\nπk [Q ∗ k (A1)Q ∗ k (A2)]\n}\n=\n∑\nk\nE [πk]E [Q ∗ k (A1)Q ∗ k (A2)] =\nQ0 (A1 ∩ A2) +Q0 (A1)Q0 (A2)\nv (v + 1)\n( ∑\nk\nE [πk]\n)\n=\nQ0 (A1 ∩ A2) +Q0 (A1)Q0 (A2)\nv (v + 1)\nIntegrating out Q0 ∼ DP (vS) we get:\nP (ϕik ∈ A1, ϕjk′ ∈ A2 | α, v, η, S) =\nE Q0|η,S\n[ Q0 (A1 ∩A2) +Q0 (A1)Q0 (A2)\nv (v + 1)\n]\n=\n1\nv (v + 1)\n{\nS (A1 ∩A2) + S (A1 ∩ A2) + S (A1)S (A2)\nη (η + 1)\n}\n=\nS (A1 ∩ A2)\nv (v + 1) +\nS (A1 ∩ A2) + S (A1)S (A2)\nv (v + 1) η (η + 1)\nFor i 6= j, since Qi and Qj are conditionally independent given U , we get:\nP (ϕik ∈ A1, ϕjk′ ∈ A2 | Q1, . . . , QJ) = Qi (A1)Qj (A2)\nLet ak = Q∗k (A1) , bk = Q ∗ k (A2) and using Definition (8), integrating out U conditional on Q0 with the stick-breaking representation in Eq (8):\nP (ϕik ∈ A1, ϕjk′ ∈ A2 | vQ0) = (∫\nU\nQi (A1) dU\n)(∫\nU\nQj (A2) dU\n)\n=\nE\n[ ∑\nk\nπkQ ∗ k (A1)\n][ ∑\nk′\nπk′Q ∗ k′ (A2)\n]\n=\nE (π1a1 + π2a2 + . . .) (π1b1 + π2b2 + . . .) =\nE\n( ∑\nk\nπ2kakbk\n)\n+ E\n\n ∑\nk 6=k′\nπkπk′akbk′\n\n =\nAE\n( ∑\nk\nπ2k\n)\n+BE\n\n ∑\nk 6=k′\nπkπk′\n\n =\nA ∑\nk\nE [ π2k ] + B\n(\n1− ∑\nk\nE [ π2k ]\n)\nwhere\nA = E [akbk]\n= E [Q∗k (A1)Q ∗ k (A2)]\n= Q0 (A1 ∩ A2) +Q0 (A1)Q0 (A2)\nv (v + 1)\nand since Q∗k (s) are iid draw from DP (vQ0) we have:\nB = E [akbk′ ]\n= E [Q∗k (A1)Q ∗ k′ (A2)] = E [Q∗k (A1)]E [Q ∗ k′ (A2)]\n= Q0 (A1)Q0 (A2)\nLastly, since (π1, π2, . . .) ∼ GEM (α), using the property of its stick-breaking representation ∑\nk E [ π2k ] = 11+α . Put\nthings together we obtain the expression for the correlation of ϕik and ϕjk′ for i 6= j conditional on Q0 as:\nP (ϕik ∈ A1, ϕjk′ ∈ A2 | vQ0) =\nQ0 (A1 ∩ A2) +Q0 (A1)Q0 (A2)\n(1 + α) v (v + 1)\n+ α\n1 + α Q0 (A1)Q0 (A2)\n=\nQ0 (A1 ∩ A2)\n(1 + α) v (v + 1)\n+ αv (v + 1) + 1\n(1 + α) v (v + 1) Q0 (A1)Q0 (A2)\nNext, integrating out Q0 ∼ DP (vS) we get:\nP (ϕik ∈ A1, ϕjk′ ∈ A2 | α, v, η, S) =\nαv (v + 1) + 1\n(1 + α) v (v + 1) E [Q0 (A1)Q0 (A2)]\n+ E [Q0 (A1 ∩A2)]\n(1 + α) v (v + 1)\n=\nαv (v + 1) + 1\n(1 + α) v (v + 1)\nS (A1 ∩ A2) + S (A1)S (A2)\nη (η + 1)\n+ S (A1 ∩ A2)\n(1 + α) v (v + 1)"
    }, {
      "heading" : "A.2. Model Inference Derivations",
      "text" : "We provide detailed derivations for model inference with the graphical model displayed in Fig 1. The variables φk, ψm, π, τk are integrated out due to conjugacy property. We need to sample these latent variables z, l, ǫ and hyper parameters α, v, η. For convenience of notation, we denote z−j is a set of latent context variable z in all documents excluding document j, lj∗ is all of hidden variables lji in document j, and l−j∗ is all of l in other documents rather than document j-th.\nSAMPLING z\nSampling context index zj needs to take into account the influence of the corresponding context topics:\np(zj = k | z−j , l,x, α,H) ∝ p (zj = k | z−j , α) ︸ ︷︷ ︸\nCRP for context topic\n(9)\n× p (xj | zj = k, z−j ,x−j , H) ︸ ︷︷ ︸\ncontext predictive likelihood\n× p (lj∗ | zj = k, l−j∗, z−j , ǫ, v) ︸ ︷︷ ︸\ncontent latent marginal likelihood\nThe first term can easily be recognized as a form of Chinese Restaurant Process (CRP):\np (zj = k | z−j , α) =\n \n\nnk −j\nn∗ −j +α if kold α n∗ −j +α if knew\nwhere nk−j is the number of data zj = k excluding zj , and n∗−j is the count of all z, except zj .\nThe second expression is the predictive likelihood from the context observations under the context component φk. Specifically, let f (· | φ) and h (·) be respectively the density function for F (φ) and H , the conjugacy between F and H allows us to integrate out the mixture component parameter φk , leaving us the conditional density of xj under the mixture component k given all the context data items exclude xj :\np (xj | zj = k, z−j ,x−j , H) = ∫\nφk f (xj | φk)\n∏\nj′ 6=j,zj′=k\nf (xj′ | φk)h (φk) dφk\n∫\nφk\n∏\nj′ 6=j,zj′=k\nf (xj′ | φk) h (φk) dφk\n=f −xj k (xj)\nFinally, the last term is the contribution from the multiple latent variables of corresponding topics to that context. Since lji | zj = k iid ∼ Mult (τ k) where τ k ∼ Dir (vǫ1, . . . , vǫM , ǫnew), we shall attempt to integrate out τ k . Using the Multinomial-Dirichlet conjugacy property we proceed to compute the last term in Eq (9) as following:\np (lj∗ | zj = k, z−j , l−j∗, ǫ, v) =\n∫\nτk\np (lj∗ | τ k)\n(10)\n×p (τ k | {lj′∗ | zj′ = k, j ′ 6= j} , ǫ, v)dτ k\nRecognizing the term p (τ k | {lj′∗ | zj′ = k, j′ 6= j} , ǫ, v) is a posterior density, it is Dirichlet-distributed with the updated parameters\np (τ k | {lj′∗ | zj′ = k, j ′ 6= j}) (11)\n= Dir (\nvǫ1 + c −j k,1, . . . , vǫM + c −j k,M , vǫnew\n)\nwhere c−jk,m = ∑ j′ 6=j ∑Nj′\ni=1 I (lj′i = m, zj′ = k) is the count of topic m being assigned to context k excluding document j. Using this result, p (lj∗ | τ k) is a predictive likelihood for lj∗ under the posterior Dirichlet parameters\nτ k in Eq 11 and therefore can be evaluated to be:\np (lj∗ | zj = k, z−j , l−j∗, ǫ, v)\n=\n∫\nτk\np (lj∗ | τ k)\n× Dir (\nvǫ1 + c −j k,1, . . . , vǫM + c −j k,M , vǫnew\n)\ndτ k\n=\n∫\nτk\nM∏\nm=1\nτ c j k,m k,m × Γ ( ∑M m=1 ( vǫm + c −j k,m ))\n∏M m=1 Γ\n(\nvǫm + c −j k,m\n)\n×\nM∏\nm=1\nτ vǫm+c\n−j k,m −1\nk,m dτ k\n= Γ ( ∑M m=1 ( vǫm + c −j k,m ))\n∏M m=1 Γ\n(\nvǫm + c −j k,m\n)\n∫\nτk\nM∏\nm=1\nτ vǫm+c\n−j k,m +cj k,m −1\nk,m dτ k\n= Γ ( ∑M m=1 ( vǫm + c −j k,m ))\n∏M m=1 Γ\n(\nvǫm + c −j k,m\n)\n×\n∏M m=1 Γ\n(\nvǫm + c −j k,m + c j k,m\n)\nΓ ( ∑M\nm=1\n(\nvǫm + c −j k,m + c j k,m\n))\n= Γ ( ∑M m=1 ( vǫm + c −j k,m ))\nΓ ( ∑M\nm=1\n(\nvǫm + c −j k,m\n)\n+Nj\n)\n×\nM∏\nm=1\nΓ (\nvǫm + c −j k,m + c j k,m\n)\nΓ (\nvǫm + c −j k,m\n)\n=\n \n\nA = Γ(\n∑ m[vǫm+c −j\nk,m]) Γ( ∑ m [vǫm+ck,m]) ∏ m Γ(vǫm+ck,m) Γ(vǫm+c−jk,m) if k old\nB = Γ(\n∑ m vǫm)\nΓ( ∑\nm vǫm+Nj)\n∏\nm Γ(vǫm+cjk,m) Γ(vǫm)\nif k new\nnote that ǫ = (ǫ1, ǫ2, ...ǫM , ǫnew), here ǫ1:M = (ǫ1, ǫ2, ...ǫM ), when sampling zj we only use M active components from the previous iteration. In summary, the conditional distribution to sample zj is given as:\np (zj = k | z−j , l,x, α,H) ∝ {\nnk−j × f −xj k (xj)×A if k previousely used α× f −xji knew (xji)×B if k = knew\nImplementation note: to evaluate A and B, we make use of the marginal likelihood resulted from a MultinomialDirichlet conjugacy.\nSAMPLING l\nLet w−ji be the same set as w excluding wji, i.e w−ji = {wuv : u 6= j ∩ v 6= i}, then we can write\np (lji = m | l−ji, zj = k, v, w, S) ∝\n(12)\np (wji | w−ji, lji = m, ρ) ︸ ︷︷ ︸\ncontent predictive likelihood\n× p (lji = m | l−ji, zj = k, ǫm, v) ︸ ︷︷ ︸\nCRF for content topic\nThe first argument is computed as log likelihood predictive of the content with the component ψm\np (wji | w−ji, lji = m, ρ) =\n(13) ∫\nλm s (wji | λm)\n[ ∏\nu∈w−ji(m) y(u | λm)\n]\ns(λm)dλm ∫\nλm\n[ ∏\nu∈w−ji(m) y (u | λm)\n]\ns (λm) dλm\n, y−wjim (wji)\nAnd the second term is inspired by Chinese Restaurant Franchise (CRF) as:\np (lji = m | l−ji, ǫm, v) =\n{\nck,m + vǫm if mold\nvǫnew if m new\n(14)\nwhere ck,m is the number of data point |{lji|lji = m, zj = k, 1 ≤ j ≤ J, 1 ≤ i ≤ Nj}|. The final form to sample lji is given as:\np (lji = m | l−ji, zj = k, w, v, ǫ) ∝ {\n(ck,m + vǫm)× y −wji m (wji) if mis used previously vǫnew × y −wji m (wji) if m = mnew"
    }, {
      "heading" : "Sampling ǫ",
      "text" : "Note that sampling ǫ require both z and l.\np (ǫ | l, z, v, η) ∝ p (l | ǫ, v, z, η)× p (ǫ | η) (15)\nIsolating the content variables lkji generated by the same context zj = k into one group\nlkj = {lji : 1 ≤ i ≤ Nj , zj = k} the first term of 15 can be expressed following:\np (l | ǫ, v, z, η) =\nK∏\nk=1\n∫\nτk\np ( lk∗∗ | τk ) p (τk | ǫ) dτk\n=\nK∏\nk=1\nΓ(v)\nΓ (v + nk∗)\nM∏\nm=1\nΓ(vǫm + nkm)\nΓ(vǫm)\nwhere nk∗ = |{wji | zj = k, i = 1, ...Nj}| and nkm = |{wji | zj = k, lji = m, 1 ≤ j ≤ J, 1 ≤ i ≤ Nj , }|.\nLet ηr = η R , ηnew = R−MR η and recall that ǫ ∼ Dir (ηr, . . . , ηr, ηnew), the last term of Eq 15 is a Dirichlet density:\np (ǫ | η) =Dir\n\nη1, η2, ...ηM ︸ ︷︷ ︸\nM\n, ηnew\n\n\n= Γ(M × ηr + ηnew)\n[Γ(ηr)]Mηnew\nM∏\nm=1\nǫηr−1m ǫ ηnew−1 new\nUsing the result:\nΓ(vǫm + nkm)\nΓ(vǫm) =\nnkm∑\nokm=0\nStirl (okm, nkm) (vǫm)okm\nThus, Eq 15 becomes:\np (ǫ | l, z, v, η) =ǫηnew−1new\nK∏\nk=1\nΓ(v)\nΓ (v + nk∗)\n×\nM∏\nm=1\nǫηm−1m\nnkm∑\nokm=0\nStirl (okm, nkm) (vǫm) okm\n=ǫηnew−1new\nnkm∑\nokm=0\nK∏\nk=1\nΓ(v)\nΓ (v + nk∗)\n×\nM∏\nm=1\nǫηm−1m Stirl (okm, nkm) (vǫm) okm\np (ǫ,o | l, z, v, η) =ǫηnew−1new\nK∏\nk=1\nΓ(v)\nΓ (v + nk∗)\n×\nM∏\nm=1\nǫηm−1m Stirl (okm, nkm) (vǫm) okm\nThe probability of the auxiliary variable okm is computed as:\np(okm) =\nnkm∑\nokm=0\nStirl (okm, nkm) (vǫm)okm\nNow let o = (okm : ∀k,m) we derive the following joint distribution:\np (ǫ | o, l, z, v, η) = ǫηnew−1new\nM∏\nm=1\nǫ ∑ K okm+ηm−1\nm\nAs R → ∞, we have\np (ǫ | o, l, z, v, η) ∞ = ǫη−1new\nM∏\nm=1\nǫ ∑ K okm−1\nm\nFinally, we sample ǫ jointly with the auxiliary variable okm by:\np (okm = h | ·) ∝ Stirl (h, nkm) (vǫm)h, h = 0, 1, . . . , nkm\np(ǫ) ∝ ǫη−1new\nM∏\nm=1\nǫ ∑ K okm−1\nm"
    }, {
      "heading" : "Sampling hyperparameters",
      "text" : "In the proposed model, there are three hyper-parameters which need to be sampled : α, v and η.\nSAMPLING η\nUsing similar strategy and using technique from Escobar and West (Escobar & West, 1995), we have\np (M | η, u) = Stirl (M,u) ηM Γ (η)\nΓ (η + u)\nwhere u = ∑ m um with um = ∑\nK okm is in the previous sampling ǫ and M is the number of active content atoms. Let η ∼ Gamma (η1, η2). Recall that:\nΓ (η)\nΓ (η + u) =\n∫ 1\n0\ntη (1− t) u−1\n(\n1 + u\nη\n)\ndt\nthat we have just introduced an auxiliary variable t\np (t | η) ∝ tη (1− t) u−1 = Beta (η + 1, u)\nTherefore,\np (η | t) ∝ ηη1−1+M exp {−ηη2} × t η (1− t)\nu−1\n(\n1 + u\nη\n)\n= ηη1−1+M × exp {−η(η2 − log t)} × (1− t) u−1\n+ ηη1−1+M−1 exp {−η(η2 − log t)} × (1− t) u−1 u\n∝ ηη1−1+M exp {−η(η2 − log t)}\n+ uηη1−1+M−1 exp {−η(η2 − log t)}\n= πtGamma (η1 +M, η2 − log t) (16)\n+ (1− πt)Gamma (η1 +M − 1, η2 − log t)\nwhere πt satisfies this following equation to make the above expression a proper mixture density:\nπt 1− πt = η1 +M − 1 u (η2 − log t) (17)\nTo re-sample η, we first sample t ∼ Beta (η + 1, u), compute πt as in equation 17, and then use πt to select the correct Gamma distribution to sample η as in Eq. 16.\nSAMPLING α\nAgain sampling α is similar to Escobar et al (Escobar & West, 1995). Assuming α ∼ Gamma (α1, α2) with the auxiliary variable t:\np (t | α,K) ∝tα1 (1− t) J−1\np (t | α,K) ∝Beta (α1 + 1, J)\nJ : number of document\np (η | t,K) ∼πtGamma (α1 +K,α2 − log(t))\n+ (1− πt)Gamma (α1 +K − 1, α2 − log(t))\nwhere c, d are prior parameter for sampling η following Gamma distribution and πt1−πt = α1+K−1 J(α2−log t)\nSAMPLING v\nSampling v is similar to sampling concentration parameter in HDP (Teh et al., 2006b). Denote ok∗ = ∑\nm okm, where okm is defined previously during the sampling step for ǫ, nk∗ = ∑\nm nkm, where nkm is the count of |{lji | zji = k, lji = m}|. Using similar technique in (Teh et al., 2006b), we write:\np (o1∗, o2∗.., oK∗ | v, n1∗, ...nK∗) =\nK∏\nk=1\nStirl(nk∗, ok∗)α ok∗ 0\n× Γ(v)\nΓ (v + nk∗)\nwhere the last term can be expressed as\nΓ(v)\nΓ (v + nk∗) =\n1\nΓ(nk∗)\n∫ 1\n0\nbvk (1− bk) nk∗−1\n(\n1 + nk∗ v\n)\ndbk\nAssuming v ∼ Gamma (v1, v2), define the auxiliary variables b = (bk | k = 1, . . . ,K) , bk ∈ [0, 1] and t = (tk | k = 1, . . . ,K) , tk ∈ {0, 1} we have\nq (v, b, t) ∝ vv1−1+ ∑ k Mk exp {−vv1}\n×\nK∏\nk=1\nbvk (1− bk) Mk−1 ( Mk v )tk\nWe will sample the auxiliary variables bk, tk in accordance with v that are defined below:\nq(bk | v) =Beta (v + 1, ok∗)\nq (tk | .) =Bernoulli\n( ok∗/v\n1 + ok∗/v\n)\nq(v | .) =Gamma\n(\nv1 + ∑\nk\n(ok∗ − tk) , v2 − ∑\nk\nlog bk\n)"
    }, {
      "heading" : "A.3. Relative Roles of Context and Content Data",
      "text" : "Regarding the inference of the cluster index zj (Eq. 9), to obtain the marginal likelihood (the third term in Eq. 9) one has to integrate out the words’ topic labels lji. In doing so, it can be shown that the sufficient statistics coming from the content data toward the inference of the topic frequencies and the clustering labels will just be the empirical word frequency from each document. As each document becomes sufficiently long, the empirical word frequency quickly concentrates around its mean by the central limit theorem (CLT), so as soon as the effect of CLT kicks in, increasing document length further will do very little in improving this sufficient statistics.\nIncreasing the document length will probably not hurt, of course. But to what extent it contributes relative to the number of documents awaits a longer and richer story to be told.\nWe confirm this argument by varying the document length and the number of documents in the synthetic document and see how they affect the posterior of the clustering labels. Each experiment is repeated 20 times. We record the mean and standard deviation of the clustering performance by NMI score. As can be seen from Fig 7, using context observation makes the model more robust in recovering the true document clusters."
    }, {
      "heading" : "A.4. Perplexity Evaluation",
      "text" : "The standard perplexity proposed by Blei et al (Blei et al., 2003), used to evaluate the proposed model as following:\nperplexity ( wTest ) = exp\n{\n−\n∑JTest j=1 log p ( wTestj )\n∑JTest j=1 N Test j\n}\nDuring individual sampling iteration t, we utilize the important sampling approach (Teh et al., 2006a) to compute p (wTest). The posterior estimation of ψm in a MultinomialDirichlet case is defined below, note that it can be in other types of conjugacies (Gelman et al., 2003) (e.g. GaussianWishart, Binomial-Poisson):\nψtm,v = ntm,v + smooth\n∑V u=1 n t m,v + V × smooth\nτ tk,m = ck,m + vv × ǫm\n∑M m=1 (ck,m + vv × ǫm)\nwhere ntm,v is number of times a word v, v ∈ {1, ..., V } is assigned to context topic ψm in iteration t, and ck,m is the count of the set {wji | zj = k, lji = m, 0 ≤ j ≤ J, 0 ≤ i ≤ Nj}. There is a constant smooth parameter (Asuncion et al., 2009) that influence on the count, roughly set as 0.1. Supposed that we estimate zTestj = k and l Test ji = m, then the probability p ( wTestj ) is computed as:\np ( wTestj ) =\nNTestj∏\ni=1\n1\nT\nT∑\nt=1\nτ tk,mψ t m,wTest\nji\nwhere T is the number of collected Gibbs samples.\nMC2 on Synthetic Data\nJ: number of document. NJ: number of word per document. NMI: normalized mutual information."
    } ],
    "references" : [ {
      "title" : "Mixtures of Dirichlet processes with applications to Bayesian nonparametric problems",
      "author" : [ "C.E. Antoniak" ],
      "venue" : "The Annals of Statistics,",
      "citeRegEx" : "Antoniak,? \\Q1974\\E",
      "shortCiteRegEx" : "Antoniak",
      "year" : 1974
    }, {
      "title" : "On smoothing and inference for topic models",
      "author" : [ "A. Asuncion", "M. Welling", "P. Smyth", "Y.W. Teh" ],
      "venue" : "In Proceedings of the International Conference on Uncertainty in Artificial Intelligence,",
      "citeRegEx" : "Asuncion et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Asuncion et al\\.",
      "year" : 2009
    }, {
      "title" : "Latent Dirichlet allocation",
      "author" : [ "D.M. Blei", "A.Y. Ng", "M.I. Jordan" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Blei et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Blei et al\\.",
      "year" : 2003
    }, {
      "title" : "Locally consistent concept factorization for document clustering",
      "author" : [ "Cai", "Deng", "He", "Xiaofei", "Han", "Jiawei" ],
      "venue" : "Knowledge and Data Engineering, IEEE Transactions on,",
      "citeRegEx" : "Cai et al\\.,? \\Q2011\\E",
      "shortCiteRegEx" : "Cai et al\\.",
      "year" : 2011
    }, {
      "title" : "Multilevel analysis in public health research",
      "author" : [ "Diez-Roux", "Ana V" ],
      "venue" : "Annual review of public health,",
      "citeRegEx" : "Diez.Roux and V.,? \\Q2000\\E",
      "shortCiteRegEx" : "Diez.Roux and V.",
      "year" : 2000
    }, {
      "title" : "A non-parametric mixture model for topic modeling over time",
      "author" : [ "Dubey", "Avinava", "Hefny", "Ahmed", "Williamson", "Sinead", "Xing", "Eric P" ],
      "venue" : "arXiv preprint arXiv:1208.4411,",
      "citeRegEx" : "Dubey et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Dubey et al\\.",
      "year" : 2012
    }, {
      "title" : "Clustering images using the latent dirichlet allocation model",
      "author" : [ "Elango", "Pradheep K", "Jayaraman", "Karthik" ],
      "venue" : "University of Wisconsin,",
      "citeRegEx" : "Elango et al\\.,? \\Q2005\\E",
      "shortCiteRegEx" : "Elango et al\\.",
      "year" : 2005
    }, {
      "title" : "Bayesian density estimation and inference using mixtures",
      "author" : [ "M.D. Escobar", "M. West" ],
      "venue" : "Journal of the american statistical association,",
      "citeRegEx" : "Escobar and West,? \\Q1995\\E",
      "shortCiteRegEx" : "Escobar and West",
      "year" : 1995
    }, {
      "title" : "A Bayesian analysis of some nonparametric problems",
      "author" : [ "T.S. Ferguson" ],
      "venue" : "The Annals of Statistics,",
      "citeRegEx" : "Ferguson,? \\Q1973\\E",
      "shortCiteRegEx" : "Ferguson",
      "year" : 1973
    }, {
      "title" : "Clustering by passing messages between data",
      "author" : [ "B.J. Frey", "D. Dueck" ],
      "venue" : "points. Science,",
      "citeRegEx" : "Frey and Dueck,? \\Q2007\\E",
      "shortCiteRegEx" : "Frey and Dueck",
      "year" : 2007
    }, {
      "title" : "Bayesian data analysis",
      "author" : [ "Gelman", "Andrew", "Carlin", "John B", "Stern", "Hal S", "Rubin", "Donald B" ],
      "venue" : "CRC press,",
      "citeRegEx" : "Gelman et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Gelman et al\\.",
      "year" : 2003
    }, {
      "title" : "Finding scientific topics",
      "author" : [ "Griffiths", "Thomas L", "Steyvers", "Mark" ],
      "venue" : "Proceedings of the National academy of Sciences of the United States of America,",
      "citeRegEx" : "Griffiths et al\\.,? \\Q2004\\E",
      "shortCiteRegEx" : "Griffiths et al\\.",
      "year" : 2004
    }, {
      "title" : "Bayesian nonparametrics",
      "author" : [ "N.L. Hjort", "C. Holmes", "P. Müller", "S.G. Walker" ],
      "venue" : null,
      "citeRegEx" : "Hjort et al\\.,? \\Q2010\\E",
      "shortCiteRegEx" : "Hjort et al\\.",
      "year" : 2010
    }, {
      "title" : "Multilevel analysis: Techniques and applications",
      "author" : [ "Hox", "Joop" ],
      "venue" : null,
      "citeRegEx" : "Hox and Joop.,? \\Q2010\\E",
      "shortCiteRegEx" : "Hox and Joop.",
      "year" : 2010
    }, {
      "title" : "Learning the parts of objects by non-negative matrix factorization",
      "author" : [ "Lee", "Daniel D", "Seung", "H Sebastian" ],
      "venue" : null,
      "citeRegEx" : "Lee et al\\.,? \\Q1999\\E",
      "shortCiteRegEx" : "Lee et al\\.",
      "year" : 1999
    }, {
      "title" : "Multilevel modelling of health statistics",
      "author" : [ "Leyland", "Alastair H", "Goldstein", "Harvey" ],
      "venue" : null,
      "citeRegEx" : "Leyland et al\\.,? \\Q2001\\E",
      "shortCiteRegEx" : "Leyland et al\\.",
      "year" : 2001
    }, {
      "title" : "Dependent nonparametric processes",
      "author" : [ "S.N. MacEachern" ],
      "venue" : "In ASA Proceedings of the Section on Bayesian Statistical Science, pp",
      "citeRegEx" : "MacEachern,? \\Q1999\\E",
      "shortCiteRegEx" : "MacEachern",
      "year" : 1999
    }, {
      "title" : "Extraction of latent patterns and contexts from social honest signals using hierarchical dirichlet processes",
      "author" : [ "T.C. Nguyen", "D. Phung", "S. Gupta", "S. Venkatesh" ],
      "venue" : "IEEE International Conference on Pervasive Computing and Communications,",
      "citeRegEx" : "Nguyen et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Nguyen et al\\.",
      "year" : 2013
    }, {
      "title" : "Conditionally dependent Dirichlet processes for modelling naturally correlated data sources",
      "author" : [ "D. Phung", "X. Nguyen", "H. Bui", "T.V. Nguyen", "S. Venkatesh" ],
      "venue" : "Technical report,",
      "citeRegEx" : "Phung et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Phung et al\\.",
      "year" : 2012
    }, {
      "title" : "Poisson–Dirichlet and GEM invariant distributions for split-and-merge transformations of an interval partition",
      "author" : [ "J. Pitman" ],
      "venue" : "Combinatorics, Probability and Computing,",
      "citeRegEx" : "Pitman,? \\Q2002\\E",
      "shortCiteRegEx" : "Pitman",
      "year" : 2002
    }, {
      "title" : "Objective criteria for the evaluation of clustering methods",
      "author" : [ "Rand", "William M" ],
      "venue" : "Journal of the American Statistical association,",
      "citeRegEx" : "Rand and M.,? \\Q1971\\E",
      "shortCiteRegEx" : "Rand and M.",
      "year" : 1971
    }, {
      "title" : "The nested Dirichlet process",
      "author" : [ "A. Rodriguez", "D.B. Dunson", "A.E. Gelfand" ],
      "venue" : "Journal of the American Statistical Association,",
      "citeRegEx" : "Rodriguez et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Rodriguez et al\\.",
      "year" : 2008
    }, {
      "title" : "A constructive definition of Dirichlet priors",
      "author" : [ "J. Sethuraman" ],
      "venue" : "Statistica Sinica,",
      "citeRegEx" : "Sethuraman,? \\Q1994\\E",
      "shortCiteRegEx" : "Sethuraman",
      "year" : 1994
    }, {
      "title" : "A collapsed variational bayesian inference algorithm for latent dirichlet allocation",
      "author" : [ "Teh", "Yee W", "Newman", "David", "Welling", "Max" ],
      "venue" : "In Advances in neural information processing systems,",
      "citeRegEx" : "Teh et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Teh et al\\.",
      "year" : 2006
    }, {
      "title" : "Hierarchical Dirichlet processes",
      "author" : [ "Y.W. Teh", "M.I. Jordan", "M.J. Beal", "D.M. Blei" ],
      "venue" : "Journal of the American Statistical Association,",
      "citeRegEx" : "Teh et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Teh et al\\.",
      "year" : 2006
    }, {
      "title" : "Visualizing data using t-sne",
      "author" : [ "Van der Maaten", "Laurens", "Hinton", "Geoffrey" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Maaten et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Maaten et al\\.",
      "year" : 2008
    }, {
      "title" : "A hierarchical dirichlet process model with multiple levels of clustering for human eeg seizure modeling",
      "author" : [ "D. Wulsin", "S. Jensen", "B. Litt" ],
      "venue" : "Proceedings of the 29th International Conference on Machine learning,",
      "citeRegEx" : "Wulsin et al\\.,? \\Q2012\\E",
      "shortCiteRegEx" : "Wulsin et al\\.",
      "year" : 2012
    }, {
      "title" : "Integrating document clustering and topic modeling",
      "author" : [ "Xie", "Pengtao", "Xing", "Eric P" ],
      "venue" : null,
      "citeRegEx" : "Xie et al\\.,? \\Q2013\\E",
      "shortCiteRegEx" : "Xie et al\\.",
      "year" : 2013
    } ],
    "referenceMentions" : [ {
      "referenceID" : 2,
      "context" : "Dealing with grouped data, a setting known as multilevel analysis (Hox, 2010; Diez-Roux, 2000), has diverse application domains ranging from document modeling (Blei et al., 2003) to public health (Leyland & Goldstein, 2001).",
      "startOffset" : 159,
      "endOffset" : 178
    }, {
      "referenceID" : 2,
      "context" : "Thus, a typical approach is to first apply dimensionality reduction techniques such as LDA (Blei et al., 2003) or HDP (Teh et al.",
      "startOffset" : 91,
      "endOffset" : 110
    }, {
      "referenceID" : 2,
      "context" : "Parametric approaches (Xie & Xing, 2013) are extensions of the LDA (Blei et al., 2003) and require specifying the number of topics and clusters in advance.",
      "startOffset" : 67,
      "endOffset" : 86
    }, {
      "referenceID" : 21,
      "context" : "Bayesian nonparametric approaches including the nested Dirichlet process (nDP) (Rodriguez et al., 2008) and the multi-level clustering hierarchical Dirichlet Process (MLC-HDP) (Wulsin et al.",
      "startOffset" : 79,
      "endOffset" : 103
    }, {
      "referenceID" : 26,
      "context" : ", 2008) and the multi-level clustering hierarchical Dirichlet Process (MLC-HDP) (Wulsin et al., 2012) can automatically adjust the number of clusters.",
      "startOffset" : 80,
      "endOffset" : 101
    }, {
      "referenceID" : 2,
      "context" : "We note that standard topic models such as LDA (Blei et al., 2003) or its nonparametric Bayesian counter part, HDP (Teh et al.",
      "startOffset" : 47,
      "endOffset" : 66
    }, {
      "referenceID" : 17,
      "context" : ", 2011; Elango & Jayaraman, 2005) and HDP+Affinity Propagation for clustering human activities (Nguyen et al., 2013).",
      "startOffset" : 95,
      "endOffset" : 116
    }, {
      "referenceID" : 21,
      "context" : "The first Bayesian nonparametric model proposed for this task is the nested Dirichlet Process (nDP) (Rodriguez et al., 2008) where documents in a cluster share the same distribution over topic atoms.",
      "startOffset" : 100,
      "endOffset" : 124
    }, {
      "referenceID" : 26,
      "context" : "The same observation was also made by (Wulsin et al., 2012) who introduced the MLC-HDP, a 3-level extension to the nDP.",
      "startOffset" : 38,
      "endOffset" : 59
    }, {
      "referenceID" : 12,
      "context" : "The literature on DP is vast and we refer to (Hjort et al., 2010) for a comprehensive account.",
      "startOffset" : 45,
      "endOffset" : 65
    }, {
      "referenceID" : 8,
      "context" : "Dirichlet process (Ferguson, 1973) is a basic building block in Bayesian nonparametrics.",
      "startOffset" : 18,
      "endOffset" : 34
    }, {
      "referenceID" : 22,
      "context" : "Sethuraman (Sethuraman, 1994) provides an alternative constructive definition which makes the discreteness property of a draw from a Dirichlet process explicit via the stick-breaking representation: G = ∑∞ k=1 βkδφk where φk iid ∼ H, k = 1, .",
      "startOffset" : 11,
      "endOffset" : 29
    }, {
      "referenceID" : 19,
      "context" : "It can be shown that ∑∞ k=1 βk = 1 with probability one, and as a convention (Pitman, 2002), we hereafter write β ∼ GEM (γ).",
      "startOffset" : 77,
      "endOffset" : 91
    }, {
      "referenceID" : 0,
      "context" : "A likelihood kernel F (·) is used to generate data xi | φk iid ∼ F (· | φk), resulting in a model known as the Dirichlet process mixture model (DPM), pioneered by the work of (Antoniak, 1974) and subsequently developed by many others.",
      "startOffset" : 175,
      "endOffset" : 191
    }, {
      "referenceID" : 16,
      "context" : "Under this setting, each group is modelled as a DPM and these models are ‘linked’ together to reflect the dependency among them – a formalism which is generally known as dependent Dirichlet processes (MacEachern, 1999).",
      "startOffset" : 200,
      "endOffset" : 218
    }, {
      "referenceID" : 21,
      "context" : "This formalism is the nested Dirichlet Process (Rodriguez et al., 2008), specifically Gj iid ∼ U where U ∼ DP (α× DP (γH)).",
      "startOffset" : 47,
      "endOffset" : 71
    }, {
      "referenceID" : 21,
      "context" : "Finally we note that this original definition of nDP in (Rodriguez et al., 2008) does not force the atoms to be shared across clusters of groups, but this can be achieved by simply introducing a DP prior for the nDP base measure, a modification that we use in this paper.",
      "startOffset" : 56,
      "endOffset" : 80
    }, {
      "referenceID" : 5,
      "context" : ", 2006b) where no grouplevel context can be used, and npTOT (Dubey et al., 2012) where only timestamp information can be used.",
      "startOffset" : 60,
      "endOffset" : 80
    }, {
      "referenceID" : 2,
      "context" : "We use perplexity score (Blei et al., 2003) on downloaded from http://www.",
      "startOffset" : 24,
      "endOffset" : 43
    }, {
      "referenceID" : 5,
      "context" : "1 (−, 108) words npTOT (Dubey et al., 2012; Phung et al., 2012) 2491.",
      "startOffset" : 23,
      "endOffset" : 63
    }, {
      "referenceID" : 18,
      "context" : "1 (−, 108) words npTOT (Dubey et al., 2012; Phung et al., 2012) 2491.",
      "startOffset" : 23,
      "endOffset" : 63
    }, {
      "referenceID" : 3,
      "context" : "We report performance on four well-known clustering evaluation metrics: Purity, Normalized Mutual Information (NMI), RandIndex (RI), and Fscore (detailed in (Rand, 1971; Cai et al., 2011)).",
      "startOffset" : 157,
      "endOffset" : 187
    }, {
      "referenceID" : 2,
      "context" : "Perplexity Evaluation The standard perplexity proposed by Blei et al (Blei et al., 2003), used to evaluate the proposed model as following:",
      "startOffset" : 69,
      "endOffset" : 88
    }, {
      "referenceID" : 10,
      "context" : "The posterior estimation of ψm in a MultinomialDirichlet case is defined below, note that it can be in other types of conjugacies (Gelman et al., 2003) (e.",
      "startOffset" : 130,
      "endOffset" : 151
    }, {
      "referenceID" : 1,
      "context" : "There is a constant smooth parameter (Asuncion et al., 2009) that influence on the count, roughly set as 0.",
      "startOffset" : 37,
      "endOffset" : 60
    } ],
    "year" : 2014,
    "abstractText" : "We present a Bayesian nonparametric framework for multilevel clustering which utilizes grouplevel context information to simultaneously discover low-dimensional structures of the group contents and partitions groups into clusters. Using the Dirichlet process as the building block, our model constructs a product base-measure with a nested structure to accommodate content and context observations at multiple levels. The proposed model possesses properties that link the nested Dirichlet processes (nDP) and the Dirichlet process mixture models (DPM) in an interesting way: integrating out all contents results in the DPM over contexts, whereas integrating out group-specific contexts results in the nDP mixture over content variables. We provide a Polyaurn view of the model and an efficient collapsed Gibbs inference procedure. Extensive experiments on real-world datasets demonstrate the advantage of utilizing context information via our model in both text and image domains.",
    "creator" : "LaTeX with hyperref package"
  }
}