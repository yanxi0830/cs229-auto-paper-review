{
  "name" : "1309.2375.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Accelerated Proximal Stochastic Dual Coordinate Ascent for Regularized Loss Minimization",
    "authors" : [ "Shai Shalev-Shwartz", "Tong Zhang" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "We consider the following generic optimization problem associated with regularized loss minimization of linear predictors: Let X1, . . . , Xn be matrices in Rd×k (referred to as instances), let φ1, . . . , φn be a sequence of vector convex functions defined on Rk (referred to as loss functions), let g(·) be a convex function defined on Rd (referred to as a regularizer), and let λ ≥ 0 (referred to as a regularization parameter). Our goal is to solve:\nmin w∈Rd P (w) where P (w) =\n[ 1\nn n∑ i=1 φi(X > i w) + λg(w)\n] . (1)\nFor example, in ridge regression the regularizer is g(w) = 12‖w‖ 2 2, the instances are column vectors, and for every i the i’th loss function is φi(a) = 12(a− yi) 2, for some scalar yi.\nLet w∗ = argminw P (w) (we will later make assumptions that imply that w ∗ is unique). We say that w is -accurate if P (w) − P (w∗) ≤ . Our main result is a new algorithm for solving (1). If g is 1-strongly convex and each φi is (1/γ)-smooth (meaning that its gradient is (1/γ)-Lipschitz), then our algorithm finds, with probability of at least 1− δ, an -accurate solution to (1) in time\nO ( d ( n+ min { 1\nλ γ ,\n√ n\nλ γ\n}) log(1/ ) log(1/δ) max{1, log2(1/(λ γ n))} ) = Õ ( d ( n+ min { 1\nλ γ ,\n√ n\nλ γ\n})) .\nThis applies, for example, to ridge regression and to logistic regression with L2 regularization. The O notation hides constants terms and the Õ notation hides constants and logarithmic terms. We make these explicit in the formal statement of our theorems. ∗School of Computer Science and Engineering, The Hebrew University, Jerusalem, Israel †Department of Statistics, Rutgers University, NJ, USA ‡Baidu Inc., Beijing, China\nar X\niv :1\n30 9.\n23 75\nv2 [\nst at\n.M L\n] 8\nO ct\n2 01\n3\nIntuitively, we can think of 1λγ as the condition number of the problem. If the condition number is O(n) then our runtime becomes Õ(dn). This means that the runtime is nearly linear in the data size. This matches the recent result of Shalev-Shwartz and Zhang [25], Le Roux et al. [15], but our setting is significantly more general. When the condition number is much larger than n, our runtime becomes Õ(d √ n λ γ ). This significantly improves over the result of [25, 15]. It also significantly improves over the\nruntime of accelerated gradient descent due to Nesterov [18], which is Õ(dn √\n1 λ γ ).\nBy applying a smoothing technique to φi, we also derive a method that finds an -accurate solution to (1) assuming that each φi is O(1)-Lipschitz, and obtain the runtime\nÕ ( d ( n+ min { 1\nλ ,\n√ n\nλ\n})) .\nThis applies, for example, to SVM with the hinge-loss. It significantly improves over the rate dλ of SGD (e.g. [22]), when 1λ n.\nWe can also apply our results to non-strongly convex regularizers (such as the L1 norm regularizer), or to non-regularized problems, by adding a slight L2 regularization. For example, for L1 regularized problems, and assuming that each φi is (1/γ)-smooth, we obtain the runtime of\nÕ ( d ( n+ min { 1\nγ ,\n√ n\nγ\n})) .\nThis applies, for example, to the Lasso problem, in which the goal is to minimize the squared loss plus an L1 regularization term.\nTo put our results in context, in the table below we specify the runtime of various algorithms (while ignoring constants and logarithmic terms) for three key machine learning applications; SVM in which φi(a) = max{0, 1 − a} and g(w) = 12‖w‖ 2 2, Lasso in which φi(a) = 1 2(a − yi)\n2 and g(w) = σ‖w‖1, and Ridge Regression in which φi(a) = 12(a − yi) 2 and g(w) = 12‖w‖ 2 2. Additional applications, and a more detailed runtime comparison to previous work, are given in Section 5. In the table below, SGD stands for Stochastic Gradient Descent, and AGD stands for Accelerated Gradient Descent.\nProblem Algorithm Runtime\nSVM\nSGD [22] dλ AGD [17] dn √ 1 λ\nThis paper d ( n+ min{ 1λ , √ n λ } )\nLasso\nSGD and variants (e.g. [28, 27, 21]) d 2\nStochastic Coordinate Descent [23, 16] dn FISTA [18, 2] dn √ 1\nThis paper d ( n+ min{1 , √ n } )\nRidge Regression\nExact d2n+ d3 SGD [15], SDCA [25] d ( n+ 1λ ) AGD [18] dn √ 1 λ\nThis paper d ( n+ min{ 1λ , √ n λ} )\nTechnical contribution: Our algorithm combines two ideas. The first is a proximal version of stochastic dual coordinate ascent (SDCA).1 In particular, we generalize the recent analysis of [25] in two directions. First, we allow the regularizer, g, to be a general strongly convex function (and not necessarily the squared Euclidean norm). This allows us to consider non-smooth regularization function, such as the L1 regularization. Second, we allow the loss functions, φi, to be vector valued functions which are smooth (or Lipschitz) with respect to a general norm. This generalization is useful in multiclass applications. As in [25], the runtime of this procedure is Õ ( d ( n+ 1λγ )) . This would be a nearly linear time (in the size of the data) if 1λγ = O(n). Our second idea deals with the case 1 λγ n by iteratively approximating the objective function P with objective functions that have a stronger regularization. In particular, each iteration of our acceleration procedure involves approximate minimization of P (w) + κ2‖w− y‖ 2 2, with respect to w, where y is a vector obtained from previous iterates and κ is order of 1/(γn). The idea is that the addition of the relatively strong regularization makes the runtime of our proximal stochastic dual coordinate ascent procedure be Õ(dn). And, with a proper choice of y at each iteration, we show that the sequence of solutions\nof the problems with the added regularization converge to the minimum of P after √\n1 λγn iterations. This yields the overall runtime of d √\nn λγ .\nAdditional related work: As mentioned before, our first contribution is a proximal version of the stochastic dual coordinate ascent method and extension of the analysis given in Shalev-Shwartz and Zhang [25]. Stochastic dual coordinate ascent has also been studied in Collins et al. [3] but in more restricted settings than the general problem considered in this paper. One can also apply the analysis of stochastic coordinate descent methods given in Richtárik and Takáč [19] on the dual problem. However, here we are interested in understanding the primal sub-optimality, hence an analysis which only applies to the dual problem is not sufficient.\nThe generality of our approach allows us to apply it for multiclass prediction problems. We discuss this in detail later on in Section 5. Recently, [13] derived a stochastic coordinate ascent for structural SVM based on the Frank-Wolfe algorithm. Although with different motivations, for the special case of multiclass problems with the hinge-loss, their algorithm ends up to be the same as our proximal dual ascent algorithm (with the same rate). Our approach allows to accelerate the method and obtain an even faster rate.\nThe proof of our acceleration method adapts Nesterov’s estimation sequence technique, studied in Devolder et al. [7], Schmidt et al. [20], to allow approximate and stochastic proximal mapping. See also [1, 6]. In particular, it relies on similar ideas as in Proposition 4 of [20]. However, our specific requirement is different, and the proof presented here is different and significantly simpler than that of [20].\nThere have been several attempts to accelerate stochastic optimization algorithms. See for example [12, 11, 4] and the references therein. However, the runtime of these methods have a polynomial dependence on 1/ even if φi are smooth and g is λ-strongly convex, as opposed to the logarithmic dependence on 1/ obtained here. As in [15, 25], we avoid the polynomial dependence on 1/ by allowing more than a single pass over the data.\n1Technically speaking, it may be more accurate to use the term randomized dual coordinate ascent, instead of stochastic dual coordinate ascent. This is because our algorithm makes more than one pass over the data, and therefore cannot work directly on distributions with infinite support. However, following the convention in the prior machine learning literature, we do not make this distinction."
    }, {
      "heading" : "2 Preliminaries",
      "text" : "All the functions we consider in this paper are proper convex functions over a Euclidean space. We use R to denote the set of real numbers and to simplify our notation, when we use R to denote the range of a function f we in fact allow f to output the value +∞.\nGiven a function f : Rd → R we denote its conjugate function by\nf∗(y) = sup x [y>x− f(x)] .\nGiven a norm ‖ · ‖P we denote the dual norm by ‖ · ‖D where\n‖y‖D = sup x:‖x‖P=1\ny>x.\nWe use ‖·‖ or ‖·‖2 to denote the L2 norm, ‖x‖ = x>x. We also use ‖x‖1 = ∑\ni |xi| and ‖x‖∞ = maxi |xi|. The operator norm of a matrix X with respect to norms ‖ · ‖P , ‖ · ‖P ′ is defined as\n‖X‖P→P ′ = sup u:‖u‖P=1 ‖Xu‖P ′ .\nA function f : Rk → Rd is L-Lipschitz with respect to a norm ‖ · ‖P , whose dual norm is ‖ · ‖D, if for all a, b ∈ Rd, we have ‖f(a)− f(b)‖D ≤ L ‖a− b‖P . A function f : Rd → R is (1/γ)-smooth with respect to a norm ‖ · ‖P if it is differentiable and its gradient is (1/γ)-Lipschitz with respect to ‖ · ‖P . An equivalent condition is that for all a, b ∈ Rd, we have\nf(a) ≤ f(b) +∇f(b)>(a− b) + 1 2γ ‖a− b‖2P .\nA function f : Rd → R is γ-strongly convex with respect to ‖ · ‖P if\nf(w + v) ≥ f(w) +∇f(w)>v + γ 2 ‖v‖2P .\nIt is well known that f is γ-strongly convex with respect to ‖ · ‖P if and only if f∗ is (1/γ)-smooth with respect to the dual norm, ‖ · ‖D.\nThe dual problem of (1) is\nmax α∈Rk×n D(α) where D(α) =\n[ 1\nn n∑ i=1 −φ∗i (−αi)− λg∗ ( 1 λn n∑ i=1 Xiαi )] , (2)\nwhere αi is the i’th column of the matrix α, which forms a vector in Rk. We will assume that g is strongly convex which implies that g∗(·) is continuous differentiable. If we define\nv(α) = 1\nλn n∑ i=1 Xiαi and w(α) = ∇g∗(v(α)), (3)\nthen it is known that w(α∗) = w∗, where α∗ is an optimal solution of (2). It is also known that P (w∗) = D(α∗) which immediately implies that for all w and α, we have P (w) ≥ D(α), and hence the duality gap defined as P (w(α))−D(α) can be regarded as an upper bound on both the primal sub-optimality, P (w(α))−P (w∗), and on the dual sub-optimality, D(α∗)−D(α)."
    }, {
      "heading" : "3 Main Results",
      "text" : "In this section we describe our algorithms and their analysis. We start in Section 3.1 with a description of our proximal stochastic dual coordinate ascent procedure (Prox-SDCA). Then, in Section 3.2 we show how to accelerate the method by calling Prox-SDCA on a sequence of problems with a strong regularization. Throughout the first two sections we assume that the loss functions are smooth. Finally, we discuss the case of Lipschitz loss functions in Section 3.3.\nThe proofs of the main acceleration theorem (Theorem 3) is given in Section 4. The rest of the proofs are provided in the appendix."
    }, {
      "heading" : "3.1 Proximal Stochastic Dual Coordinate Ascent",
      "text" : "We now describe our proximal stochastic dual coordinate ascent procedure for solving (1). Our results in this subsection holds for g being a 1-strongly convex function with respect to some norm ‖ · ‖P ′ and every φi being a (1/γ)-smooth function with respect to some other norm ‖ · ‖P . The corresponding dual norms are denoted by ‖ · ‖D′ and ‖ · ‖D respectively.\nThe dual objective in (2) has a different dual vector associated with each example in the training set. At each iteration of dual coordinate ascent we only allow to change the i’th column of α, while the rest of the dual vectors are kept intact. We focus on a randomized version of dual coordinate ascent, in which at each round we choose which dual vector to update uniformly at random.\nAt step t, let v(t−1) = (λn)−1 ∑\niXiα (t−1) i and let w (t−1) = ∇g∗(v(t−1)). We will update the i-th dual variable α(t)i = α (t−1) i + ∆αi, in a way that will lead to a sufficient increase of the dual objective. For the primal problem, this would lead to the update v(t) = v(t−1)+(λn)−1Xi∆αi, and thereforew(t) = ∇g∗(v(t)) can also be written as\nw(t) = argmax w\n[ w>v(t) − g(w) ] = argmin\nw\n[ −w> ( n−1\nn∑ i=1 Xiα (t) i\n) + λg(w) ] .\nNote that this particular update is rather similar to the update step of proximal-gradient dual-averaging method (see for example Xiao [27]). The difference is on how α(t) is updated.\nThe goal of dual ascent methods is to increase the dual objective as much as possible, and thus the optimal way to choose ∆αi would be to maximize the dual objective, namely, we shall let\n∆αi = argmax ∆αi∈Rk\n[ − 1 n φ∗i (−(αi + ∆αi))− λg∗(v(t−1) + (λn)−1Xi∆αi) ] .\nHowever, for a complex g∗(·), this optimization problem may not be easy to solve. To simplify the optimization problem we can rely on the smoothness of g∗ (with respect to a norm ‖ · ‖D′) and instead of directly maximizing the dual objective function, we try to maximize the following proximal objective which is a lower bound of the dual objective:\nargmax ∆αi∈Rk\n[ − 1 n φ∗i (−(αi + ∆αi))− λ ( ∇g∗(v(t−1))>(λn)−1Xi∆αi + 1 2 ‖(λn)−1Xi∆αi‖2D′ )] = argmax\n∆αi∈Rk\n[ −φ∗i (−(αi + ∆αi))− w(t−1)>Xi∆αi − 1\n2λn ‖Xi∆αi‖2D′\n] .\nIn general, this optimization problem is still not necessarily simple to solve because φ∗ may also be complex. We will thus also propose alternative update rules for ∆αi of the form ∆αi = s(−∇φi(X>i w(t−1))−α (t−1) i ) for an appropriately chosen step size parameter s > 0. Our analysis shows that an appropriate choice of s still leads to a sufficient increase in the dual objective.\nIt should be pointed out that we can always pick ∆αi so that the dual objective is non-decreasing. In fact, if for a specific choice of ∆αi, the dual objective decreases, we may simply set ∆αi = 0. Therefore throughout the proof we will assume that the dual objective is non-decreasing whenever needed.\nThe theorems below provide upper bounds on the number of iterations required by our prox-SDCA procedure.\nTheorem 1. Consider Procedure Prox-SDCA as given in Figure 1. Let α∗ be an optimal dual solution and let > 0. For every T such that\nT ≥ ( n+ R2\nλγ\n) log (( n+ R2\nλγ\n) · D(α ∗)−D(α(0)) ) ,\nwe are guaranteed that E[P (w(T ))−D(α(T ))] ≤ . Moreover, for every T such that\nT ≥ ( n+ ⌈ R2\nλγ\n⌉) · ( 1 + log ( D(α∗)−D(α(0)) )) ,\nlet T0 = T − n− dR 2 λγ e, then we are guaranteed that E[P (w̄)−D(ᾱ)] ≤ .\nWe next give bounds that hold with high probability.\nTheorem 2. Consider Procedure Prox-SDCA as given in Figure 1. Let α∗ be an optimal dual solution, let D, P > 0, and let δ ∈ (0, 1).\n1. For every T such that\nT ≥ ⌈( n+ R2\nλγ\n) log ( 2(D(α∗)−D(α(0)))\nD\n)⌉ · ⌈\nlog2\n( 1\nδ\n)⌉ ,\nwe are guaranteed that with probability of at least 1− δ it holds that D(α∗)−D(α(T )) ≤ D.\n2. For every T such that\nT ≥ ⌈( n+ R2\nλγ\n) ( log ( n+ R2\nλγ\n) + log ( 2(D(α∗)−D(α(0)))\nP\n))⌉ · ⌈\nlog2\n( 1\nδ\n)⌉ ,\nwe are guaranteed that with probability of at least 1− δ it holds that P (w(T ))−D(α(T )) ≤ P .\n3. Let T be such that T ≥ ( n+ ⌈ R2\nλγ\n⌉) · ( 1 + ⌈ log ( 2(D(α∗)−D(α(0)))\nP\n)⌉) · ⌈\nlog2\n( 2\nδ\n)⌉ ,\nand let T0 = T − n − dR 2\nλγ e. Suppose we choose dlog2(2/δ)e values of t uniformly at random from T0 + 1, . . . , T , and then choose the single value of t from these dlog2(2/δ)e values for which P (w(t))−D(α(t)) is minimal. Then, with probability of at least 1−δ we have thatP (w(t))−D(α(t)) ≤ P .\nThe above theorem tells us that the runtime required to find an accurate solution, with probability of at least 1− δ, is\nO ( d ( n+ R2\nλγ\n) · log ( D(α∗)−D(α(0)) ) · log ( 1\nδ\n)) . (4)\nThis yields the following corollary.\nCorollary 1. The expected runtime required to minimize P up to accuracy is\nO ( d ( n+ R2\nλγ\n) · log ( D(α∗)−D(α(0)) )) .\nProof. We have shown that with a runtime of O ( d ( n+ R 2\nλγ\n) · log ( 2(D(α∗)−D(α(0))) )) we can find an\naccurate solution with probability of at least 1/2. Therefore, we can run the procedure for this amount of time and check if the duality gap is smaller than . If yes, we are done. Otherwise, we would restart the process. Since the probability of success is 1/2 we have that the average number of restarts we need is 2, which concludes the proof."
    }, {
      "heading" : "3.2 Acceleration",
      "text" : "The Prox-SDCA procedure described in the previous subsection has the iteration bound of Õ ( n+ R 2\nλγ\n) .\nThis is a nearly linear runtime whenever the condition number, R2/(λγ), is O(n). In this section we show how to improve the dependence on the condition number by an acceleration procedure. In particular, throughout this section we assume that 10n < R 2\nλγ . We further assume throughout this subsection that the regularizer, g, is 1-strongly convex with respect to the Euclidean norm, i.e. ‖u‖P ′ = ‖ · ‖2. This also implies that ‖u‖D′ is the Euclidean norm. A generalization of the acceleration technique for strongly convex regularizers with respect to general norms is left to future work.\nThe main idea of the acceleration procedure is to iteratively run the Prox-SDCA procedure, where at iteration t we call Prox-SDCA with the modified objective, P̃t(w) = P (w) + κ2‖w− y\n(t−1)‖2, where κ is a relatively large regularization parameter and the regularization is centered around the vector\ny(t−1) = w(t−1) + β(w(t−1) − w(t−2))\nfor some β ∈ (0, 1). That is, our regularization is centered around the previous solution plus a “momentum term” β(w(t−1) − w(t−2)).\nA pseudo-code of the algorithm is given in Figure 2. Note that all the parameters of the algorithm are determined by our theory.\nRemark 1. In the pseudo-code below, we specify the parameters based on our theoretical derivation. In our experiments, we found out that this choice of parameters also work very well in practice. However, we also found out that the algorithm is not very sensitive to the choice of parameters. For example, we found out that running 5n iterations of Prox-SDCA (that is, 5 epochs over the data), without checking the stopping condition, also works very well.\nThe main theorem is the following.\nTheorem 3. Consider the accelerated Prox-SDCA algorithm given in Figure 2.\n• Correctness: When the algorithm terminates we have that P (w(t))− P (w∗) ≤ .\n• Runtime:\n– The number of outer iterations is at most\n1 + 2\nη log(ξ1/ ) ≤ 1 +\n√ 8R2\nλγn\n( log ( 2R2\nλγn\n) + log ( P (0)−D(0) )) .\n– Each outer iteration involves a single call to Prox-SDCA, and the averaged runtime required by each such call is\nO ( dn log ( R2\nλγn\n)) .\nBy a straightforward amplification argument we obtain that for every δ ∈ (0, 1) the total runtime required by accelerated Prox-SDCA to guarantee an -accurate solution with probability of at least 1− δ is\nO ( d √ nR2\nλ γ log\n( R2\nλ γ n\n) ( log ( R2\nλγn\n) + log ( P (0)−D(0) )) log ( 1\nδ\n)) ."
    }, {
      "heading" : "3.3 Non-smooth, Lipschitz, loss functions",
      "text" : "So far we have assumed that for every i, φi is a (1/γ)-smooth function. We now consider the case in which φi might be non-smooth, and even non-differentiable, but it is L-Lipschitz.\nFollowing Nesterov [17], we apply a “smoothing” technique. We first observe that if φ is L-Lipschitz function then the domain of φ∗ is in the ball of radius L.\nLemma 1. Let φ : Rk → R be an L-Lipschitz function w.r.t. a norm ‖ · ‖P and let ‖ · ‖D be the dual norm. Then, for any α ∈ Rk s.t. ‖α‖D > L we have that φ∗(α) =∞.\nProof. Fix some α with ‖α‖D > L. Let x0 be a vector such that ‖x0‖P = 1 and α>x0 = ‖α‖D (this is a vector that achieves the maximal objective in the definition of the dual norm). By definition of the conjugate we have\nφ∗(α) = sup x [α> x− φ(x)]\n= −φ(0) + sup x\n[α> x− (φ(x)− φ(0))]\n≥ −φ(0) + sup x\n[α> x− L‖x− 0‖P ]\n≥ −φ(0) + sup c>0\n[α> (cx0)− L‖cx0‖P ]\n= −φ(0) + sup c>0 (‖α‖D − L) c =∞ .\nThis observation allows us to smooth L-Lipschitz functions by adding regularization to their conjugate. In particular, the following lemma generalizes Lemma 2.5 in [26].\nLemma 2. Let φ be a proper, convex, L-Lipschitz function w.r.t. a norm ‖ · ‖P , let ‖ · ‖D be the dual norm, and let φ∗ be the conjugate of φ. Assume that ‖ · ‖2 ≤ ‖ · ‖D. Define φ̃∗(α) = φ∗(α) + γ2‖α‖ 2 2 and let φ̃ be the conjugate of φ̃∗. Then, φ̃ is (1/γ)-smooth w.r.t. the Euclidean norm and\n∀a, 0 ≤ φ(a)− φ̃(a) ≤ γL2/2 .\nProof. The fact that φ̃ is (1/γ)-smooth follows directly from the fact that φ̃∗ is γ-strongly convex. For the second claim note that\nφ̃(a) = sup b\n[ ba− φ∗(b)− γ\n2 ‖b‖22\n] ≤ sup\nb [ba− φ∗(b)] = φ(a)\nand\nφ̃(a) = sup b\n[ ba− φ∗(b)− γ\n2 ‖b‖22\n] = sup\nb:‖b‖D≤L\n[ ba− φ∗(b)− γ\n2 ‖b‖22 ] ≥ sup\nb:‖b‖D≤L\n[ ba− φ∗(b)− γ\n2 ‖b‖2D\n] ≥ sup\nb:‖b‖D≤L [ba− φ∗(b)]− γ 2 L2\n= φ(a)− γ 2 L2 .\nRemark 2. It is also possible to smooth using different regularization functions which are strongly convex with respect to other norms. See Nesterov [17] for discussion."
    }, {
      "heading" : "4 Proof of Theorem 3",
      "text" : "The first claim of the theorem is that when the procedure stops we have P (w(t))−P (w∗) ≤ . We therefore need to show that each stopping condition guarantees that P (w(t))− P (w∗) ≤ .\nFor the second stopping condition, recall thatw(t) is an t-accurate minimizer of P (w)+ κ2‖w−y (t−1)‖2,\nand hence by Lemma 3 below (with z = w∗, w+ = w(t), and y = y(t−1)):\nP (w∗) ≥ P (w(t)) +Q (w∗;w(t), y(t−1))\n≥ P (w(t))− ρκ 2µ ‖y(t−1) − w(t)‖2 − (1 + ρ/µ) t .\nIt is left to show that the first stopping condition is correct, namely, to show that after 1 + 2η log(ξ1/ ) iterations the algorithm must converge to an -accurate solution. Observe that the definition of ξt yields that ξt = (1−η/2)t−1 ξ1 ≤ e−η(t−1)/2ξ1. Therefore, to prove that the first stopping condition is valid, it suffices to show that for every t, P (w(t))− P (w∗) ≤ ξt.\nRecall that at each outer iteration of the accelerated procedure, we approximately minimize an objective of the form\nP (w; y) = P (w) + κ\n2 ‖w − y‖2 .\nOf course, minimizing P (w; y) is not the same as minimizing P (w). Our first lemma shows that for every y, if w+ is an -accurate minimizer of P (w; y) then we can derive a lower bound on P (w) based on P (w+) and a convex quadratic function of w.\nLemma 3. Let µ = λ/2 and ρ = µ + κ. Let w+ be a vector such that P (w+; y) ≤ minw P (w, y) + . Then, for every z,\nP (z) ≥ P (w+) +Q (z;w+, y) ,\nwhere Q (z;w +, y) = µ\n2 ∥∥∥z − (y − ρµ(y − w+))∥∥∥2 − ρκ2µ‖y − w+‖2 − (1 + ρ/µ) . Proof. Denote\nΨ(w) = P (w)− µ 2 ‖w‖2 .\nWe can write 1\n2 ‖w‖2 = 1 2 ‖y‖2 + y>(w − y) + 1 2 ‖w − y‖2 .\nIt follows that\nP (w) = Ψ(w) + µ 2 ‖w‖2 = Ψ(w) + µ 2 ‖y‖2 + µ y>(w − y) + µ 2 ‖w − y‖2 .\nTherefore, we can rewrite P (w; y) as:\nP (w; y) = Ψ(w) + µ 2 ‖y‖2 + µ y>(w − y) + ρ 2 ‖w − y‖2 .\nLet w̃ = argminw P (w; y). Therefore, the gradient 2 of P (w; y) w.r.t. w vanishes at w̃, which yields\n∇Ψ(w̃) + µy + ρ(w̃ − y) = 0 ⇒ ∇Ψ(w̃) + µy = ρ(y − w̃) .\nBy the µ-strong convexity of Ψ we have that for every z,\nΨ(z) ≥ Ψ(w̃) +∇Ψ(w̃)>(z − w̃) + µ 2 ‖z − w̃‖2 .\nTherefore,\nP (z) = Ψ(z) + µ 2 ‖y‖2 + µ y>(z − y) + µ 2 ‖z − y‖2\n≥ Ψ(w̃) +∇Ψ(w̃)>(z − w̃) + µ 2 ‖z − w̃‖2 + µ 2 ‖y‖2 + µ y>(z − y) + µ 2 ‖z − y‖2 = P (w̃; y)− ρ 2 ‖w̃ − y‖2 +∇Ψ(w̃)>(z − w̃) + µ y>(z − w̃) + µ 2 ( ‖z − w̃‖2 + ‖z − y‖2 ) = P (w̃; y)− ρ\n2 ‖w̃ − y‖2 + ρ(y − w̃)>(z − w̃) + µ 2\n( ‖z − w̃‖2 + ‖z − y‖2 ) = P (w̃; y) + ρ\n2 ‖w̃ − y‖2 + ρ(y − w̃)>(z − y) + µ 2\n( ‖z − w̃‖2 + ‖z − y‖2 ) .\n2If the regularizer g(w) in the definition of P (w) is non-differentiable, we can replace∇Ψ(w̃) with an appropriate sub-gradient of Ψ at w̃. It is easy to verify that the proof is still valid.\nIn addition, by standard algebraic manipulations,\nρ 2 ‖w̃ − y‖2 + ρ(y − w̃)>(z − y) + µ 2 ‖z − w̃‖2 − (ρ 2 ‖w+ − y‖2 + ρ(y − w+)>(z − y) + µ 2 ‖z − w+‖2 ) = ( ρ(w+ − y)− ρ(z − y) + µ(w+ − z) )> (w̃ − w+) + ρ+ µ\n2 ‖w̃ − w+‖2\n= (ρ+ µ)(w+ − z)>(w̃ − w+) + ρ+ µ 2 ‖w̃ − w+‖2\n= 1\n2 ∥∥∥∥√µ(w+ − z) + ρ+ µ√µ (w̃ − w+) ∥∥∥∥2 − µ2 ‖z − w+‖2 − (ρ+ µ)22µ ‖w̃ − w+‖2 + ρ+ µ2 ‖w̃ − w+‖2\n≥ −µ 2 ‖z − w+‖2 − ρ(ρ+ µ) 2µ ‖w̃ − w+‖2 .\nSince P (·; y) is (ρ + µ)-strongly convex and w̃ minimizes P (·; y), we have that for every w+ it holds that ρ+µ\n2 ‖w̃ − w +‖2 ≤ P (w+; y) − P (w̃; y). Combining all the above and using the fact that for every w, y,\nP (w; y) ≥ P (w), we obtain that for every w+,\nP (z) ≥ P (w+) + ρ 2 ‖w+ − y‖2 + ρ(y−w+)>(z − y) + µ 2 ‖z − y‖2 −\n( 1 + ρ\nµ\n)( P (w+; y)− P (w̃; y) ) .\nFinally, using the assumption P (w+; y) ≤ minw P (w; y) + we conclude our proof.\nWe saw that the quadratic function P (w+) + Q (z;w+, y) lower bounds the function P everywhere. Therefore, any convex combination of such functions would form a quadratic function which lower bounds P . In particular, the algorithm (implicitly) maintains a sequence of quadratic functions, h1, h2, . . ., defined as follows. Choose η ∈ (0, 1) and a sequence y(1), y(2), . . . that will be specified later. Define,\nh1(z) = P (0) +QP (0)−D(0)(z; 0, 0) = P (0) + µ\n2 ‖z‖2 − (1 + ρ/µ)(P (0)−D(0)) ,\nand for t ≥ 1, ht+1(z) = (1− η)ht(z) + η(P (w(t+1)) +Q t+1(z;w(t+1), y(t))) .\nThe following simple lemma shows that for every t ≥ 1 and z, ht(z) lower bounds P (z).\nLemma 4. Let η ∈ (0, 1) and let y(1), y(2), . . . be any sequence of vectors. Assume that w(1) = 0 and for every t ≥ 1, w(t+1) satisfies P (w(t+1); y(t)) ≤ minw P (w; y(t)) + t+1. Then, for every t ≥ 1 and every vector z we have\nht(z) ≤ P (z) .\nProof. The proof is by induction. For t = 1, observe that P (0; 0) = P (0) and that for every w we have P (w; 0) ≥ P (w) ≥ D(0). This yields P (0; 0) −minw P (w; 0) ≤ P (0) − D(0). The claim now follows directly from Lemma 3. Next, for the inductive step, assume the claim holds for some t− 1 ≥ 1 and let us prove it for t. By the recursive definition of ht and by using Lemma 3 we have\nht(z) = (1− η)ht−1(z) + η(P (w(t)) +Q t(z;w(t), y(t−1))) ≤ (1− η)ht−1(z) + ηP (z) .\nUsing the inductive assumption we obtain that the right-hand side of the above is upper bounded by (1 − η)P (z) + ηP (z) = P (z), which concludes our proof.\nThe more difficult part of the proof is to show that for every t ≥ 1,\nP (w(t)) ≤ min w ht(w) + ξt .\nIf this holds true, then we would immediately get that for every w∗,\nP (w(t))− P (w∗) ≤ P (w(t))− ht(w∗) ≤ P (w(t))−min w ht(w) ≤ ξt .\nThis will conclude the proof of the first part of Theorem 3, since ξt = ξ1(1− η/2)t−1 ≤ ξ1 e−(t−1)η/2, and therefore, 1 + 2η log(ξ1/ ) iterations suffice to guarantee that P (w\n(t))− P (w∗) ≤ . Define\nv(t) = argmin w ht(w) .\nLet us construct an explicit formula for v(t). Clearly, v(1) = 0. Assume that we have calculated v(t) and let us calculate v(t+1). Note that ht is a quadratic function which is minimized at v(t). Furthermore, it is easy to see that for every t, ht is µ-strongly convex quadratic function. Therefore,\nht(z) = ht(v (t)) +\nµ 2 ‖z − v(t)‖2 .\nBy the definition of ht+1 we obtain that\nht+1(z) = (1− η)(ht(v(t)) + µ\n2 ‖z − v(t)‖2) + η(P (w(t+1)) +Q t+1(z;w(t+1), y(t))) .\nSince the gradient of ht+1(z) at v(t+1) should be zero, we obtain that v(t+1) should satisfy\n(1− η)µ(v(t+1) − v(t)) + ηµ ( v(t+1) − (y(t) − ρµ(y (t) − w(t+1))) ) = 0\nRearranging, we obtain\nv(t+1) = (1− η)v(t) + η(y(t) − ρµ(y (t) − w(t+1))) . (5)\nGetting back to our second phase of the proof, we need to show that for every t we have P (w(t)) ≤ ht(v (t)) + ξt. We do so by induction. For the case t = 1 we have\nP (w(1))− h1(v(1)) = P (0)− h1(0) = (1 + ρ/µ)(P (0)−D(0)) = ξ1 .\nFor the induction step, assume the claim holds for t ≥ 1 and let us prove it for t+ 1. We use the shorthands,\nQt(z) = Q t(z;w (t), y(t−1)) and ψt(z) = Qt(z) + P (w(t)) .\nLet us rewrite ht+1(v(t+1)) as\nht+1(v (t+1)) = (1− η)ht(v(t+1)) + ηψt+1(v(t+1))\n= (1− η)(ht(v(t)) + µ\n2 ‖v(t) − v(t+1)‖2) + ηψt+1(v(t+1)) .\nBy the inductive assumption we have ht(v(t)) ≥ P (w(t)) − ξt and by Lemma 3 we have P (w(t)) ≥ ψt+1(w (t)). Therefore,\nht+1(v (t+1)) ≥ (1− η)(ψt+1(w(t))− ξt +\nµ 2 ‖v(t) − v(t+1)‖2) + ηψt+1(v(t+1)) (6)\n= (1− η)µ\n2 ‖v(t) − v(t+1)‖2 + ηψt+1(v(t+1)) + (1− η)ψt+1(w(t))− (1− η)ξt .\nNext, note that we can rewrite\nQt+1(z) = µ 2 ‖z − y(t)‖2 + ρ(z − y(t))>(y(t) − w(t+1)) + ρ 2 ‖y(t) − w(t+1)‖2 − (1 + ρ/µ) t+1 .\nTherefore,\nηψt+1(v (t+1)) + (1− η)ψt+1(w(t))− P (w(t+1)) + (1 + ρ/µ) t+1 (7)\n= ηµ 2 ‖v(t+1) − y(t)‖2 + (1− η)µ 2 ‖w(t) − y(t)‖2 + ρ(ηv(t+1) + (1− η)w(t) − y(t))>(y(t) − w(t+1)) + ρ\n2 ‖y(t) − w(t+1)‖2\nSo far we did not specify η and y(t) (except y(0) = 0). We next set\nη = √ µ/ρ and ∀t ≥ 1, y(t) = (1 + η)−1(ηv(t) + w(t)) .\nThis choices guarantees that (see (5))\nηv(t+1) + (1− η)w(t) = η(1− η)v(t) + η2(1− ρ µ )y(t) + η2 ρ µ w(t+1) + (1− η)w(t)\n= w(t+1) + (1− η) [ ηv(t) +\nη2(1− ρµ) 1− η y(t) + w(t)\n]\n= w(t+1) + (1− η) [ ηv(t) − 1− η 2\n1− η y(t) + w(t) ] = w(t+1) + (1− η) [ ηv(t) − (1 + η)y(t) + w(t)\n] = w(t+1) .\nWe also observe that t+1 ≤ ηξt2(1+η−2) which implies that (1 + ρ/µ) t+1 + (1− η)ξt ≤ (1− η/2)ξt = ξt+1. Combining the above with (6) and (7), and rearranging terms, we obtain that\nht+1(v (t+1))− P (w(t+1)) + ξt+1 − (1− η)µ 2 ‖w(t) − y(t)‖2\n≥ (1− η)µ 2 ‖v(t) − v(t+1)‖2 + ηµ 2 ‖v(t+1) − y(t)‖2 − ρ 2 ‖y(t) − w(t+1)‖2 .\nNext, observe that ρη2 = µ and that by (5) we have\ny(t) − w(t+1) = η [ ηy(t) + (1− η)v(t) − v(t+1) ] .\nWe therefore obtain that\nht+1(v (t+1))− P (w(t+1)) + ξt+1 − (1− η)µ 2 ‖w(t) − y(t)‖2\n≥ (1− η)µ 2 ‖v(t) − v(t+1)‖2 + ηµ 2 ‖y(t) − v(t+1)‖2 − µ 2 ‖ηy(t) + (1− η)v(t) − v(t+1)‖2 .\nThe right-hand side of the above is non-negative because of the convexity of the function f(z) = µ2‖z − v(t+1)‖2, which yields\nP (w(t+1)) ≤ ht+1(v(t+1)) + ξt+1 − (1− η)µ\n2 ‖w(t) − y(t)‖2 ≤ ht+1(v(t+1)) + ξt+1 .\nThis concludes our inductive argument.\nProving the “runtime” part of Theorem 3: We next show that each call to Prox-SDCA will terminate quickly. By the definition of κ we have that\nR2\n(κ+ λ)γ = n .\nTherefore, based on Corollary 1 we know that the averaged runtime at iteration t is\nO ( dn log ( D̃t(α ∗)− D̃t(α(t−1)) η\n2(1+η−2)ξt−1\n)) .\nThe following lemma bounds the initial dual sub-optimality at iteration t ≥ 4. Similar arguments will yield a similar result for t < 4.\nLemma 5. D̃t(α ∗)− D̃t(α(t−1)) ≤ t−1 + 36κ\nλ ξt−3 .\nProof. Define λ̃ = λ + κ, f(w) = λ λ̃ g(w) + κ 2λ̃ ‖w‖2, and g̃t(w) = f(w) − κλ̃w >y(t−1). Note that λ̃ does not depend on t and therefore v(α) = 1\nnλ̃\n∑ iXiαi is the same for every t. Let,\nP̃t(w) = 1\nn n∑ i=1 φi(X > i w) + λ̃g̃t(w) .\nWe have P̃t(w (t−1)) = P̃t−1(w (t−1)) + κw(t−1)>(y(t−2) − y(t−1)) . (8)\nSince g̃∗t (θ) = maxw w>(θ + κ λ̃ y(t−1))− f(w) = f∗(θ + κ λ̃ y(t−1)) ,\nwe obtain that the dual problem is\nD̃t(α) = − 1\nn ∑ i φ∗i (−αi)− λ̃f∗(v(α) + κ λ̃ y(t−1))\nLet z = κ λ̃ (y(t−1) − y(t−2)), then, by the smoothness of f∗ we have\nf∗(v(α)+ κ\nλ̃ y(t−1)) = f∗(v(α)+\nκ λ̃ y(t−2) +z) ≤ f∗(v(α)+ κ λ̃ y(t−2))+∇f∗(v(α)+ κ λ̃ y(t−2))>z+ 1 2 ‖z‖2 .\nApplying this for α(t−1) and using w(t−1) = ∇g̃∗t−1(v(α(t−1))) = ∇f∗(v(α(t−1)) + κλ̃y (t−2)), we obtain\nf∗(v(α(t−1)) + κ λ̃ y(t−1)) ≤ f∗(v(α(t−1)) + κ λ̃ y(t−2)) + w(t−1)>z + 1 2 ‖z‖2 .\nIt follows that\n−D̃t(α(t−1)) + D̃t−1(α(t−1)) ≤ κw(t−1)>(y(t−1) − y(t−2)) + κ2\n2λ̃ ‖y(t−1) − y(t−2)‖2 .\nCombining the above with (8), we obtain that\nP̃t(w (t−1))− D̃t(α(t−1)) ≤ P̃t−1(w(t−1))− D̃t−1(α(t−1)) +\nκ2 2λ̃ ‖y(t−1) − y(t−2)‖2 .\nSince P̃t(w(t−1)) ≥ D̃t(α∗) and since λ̃ ≥ κ we get that\nD̃t(α ∗)− D̃t(α(t−1)) ≤ t−1 +\nκ 2 ‖y(t−1) − y(t−2)‖2 .\nNext, we bound ‖y(t−1) − y(t−2)‖2. We have\n‖y(t−1) − y(t−2)‖ = ‖w(t−1) − w(t−2) + β(w(t−1) − w(t−2) − w(t−2) + w(t−3))‖ ≤ 3 max\ni∈{1,2} ‖w(t−i) − w(t−i−1)‖ ,\nwhere we used the triangle inequality and β < 1. By strong convexity of P we have, for every i,\n‖w(i) − w∗‖ ≤\n√ P (w(i))− P (w∗)\nλ/2 ≤ √ ξi λ/2 ,\nwhich implies\n‖w(t−i) − w(t−i−1)‖ ≤ ‖w(t−i) − w∗‖+ ‖w∗ − w(t−i−1)‖ ≤ 2 √ ξt−i−1 λ/2 .\nThis yields the bound\n‖y(t−1) − y(t−2)‖2 ≤ 72ξt−3 λ .\nAll in all, we have obtained that\nD̃t(α ∗)− D̃t(α(t−1)) ≤ t−1 +\n36κ\nλ ξt−3 .\nGetting back to the proof of the second claim of Theorem 3, we have obtained that\nD̃t(α ∗)− D̃t(α(t−1))\nη 2(1+η−2)ξt−1\n≤ t−1η 2(1+η−2)ξt−1 + 36κξt−3 λ η 2(1+η−2)ξt−1 ≤ (1− η/2)−1 + 36κ2(1 + η −2)\nλη (1− η/2)−2\n≤ (1− η/2)−4 ( 1 + 72κ(1 + η−2)\nλη ) ≤ (1− η/2)−2 ( 1 + 36η−5 ) ,\nwhere in the last inequality we used η−2− 1 = 2κλ , which implies that 2κ λ (1 + η −2) ≤ η−4. Using 1 < η−5, 1− η/2 ≥ 0.5, and taking log to both sides, we get that\nlog\n( D̃t(α\n∗)− D̃t(α(t−1)) η\n2(1+η−2)ξt−1\n) ≤ 2 log(2) + log(37)− 5 log(η) ≤ 7 + 2.5 log ( R2\nλγn\n) .\nAll in all, we have shown that the average runtime required by Prox-SDCA(P̃t, η2(1+η−2)ξt−1, α (t−1)) is upper bounded by\nO ( dn log ( R2\nλγn\n)) ,\nwhich concludes the proof of the second claim of Theorem 3."
    }, {
      "heading" : "5 Applications",
      "text" : "In this section we specify our algorithmic framework to several popular machine learning applications. In Section 5.1 we start by describing several loss functions and deriving their conjugate. In Section 5.2 we describe several regularization functions. Finally, in the rest of the subsections we specify our algorithm for Ridge regression, SVM, Lasso, logistic regression, and multiclass prediction."
    }, {
      "heading" : "5.1 Loss functions",
      "text" : "Squared loss: φ(a) = 12(a− y) 2 for some y ∈ R. The conjugate function is\nφ∗(b) = max a ab− 1 2 (a− y)2 = 1 2 b2 + yb\nLogistic loss: φ(a) = log(1 + ea). The derivative is φ′(a) = 1/(1 + e−a) and the second derivative is φ′′(a) = 1\n(1+e−a)(1+ea) ∈ [0, 1/4], from which it follows that φ is (1/4)-smooth. The conjugate function is\nφ∗(b) = max a ab− log(1 + ea) = { b log(b) + (1− b) log(1− b) if b ∈ [0, 1] ∞ otherwise\nHinge loss: φ(a) = [1− a]+ := max{0, 1− a}. The conjugate function is\nφ∗(b) = max a ab−max{0, 1− a} = { b if b ∈ [−1, 0] ∞ otherwise\nSmooth hinge loss: This loss is obtained by smoothing the hinge-loss using the technique described in Lemma 2. This loss is parameterized by a scalar γ > 0 and is defined as:\nφ̃γ(a) =  0 a ≥ 1 1− a− γ/2 a ≤ 1− γ 1\n2γ (1− a) 2 o.w.\n(9)\nThe conjugate function is\nφ̃∗γ(b) =\n{ b+ γ2 b\n2 if b ∈ [−1, 0] ∞ otherwise\nIt follows that φ̃∗γ is γ strongly convex and φ̃ is (1/γ)-smooth. In addition, if φ is the vanilla hinge-loss, we have for every a that\nφ(a)− γ/2 ≤ φ̃(a) ≤ φ(a) .\nMax-of-hinge: The max-of-hinge loss function is a function from Rk to R, which is defined as:\nφ(a) = max j [cj + aj ]+ ,\nfor some c ∈ Rk. This loss function is useful for multiclass prediction problems. To calculate the conjugate of φ, let\nS = {β ∈ Rk+ : ‖β‖1 ≤ 1} (10)\nand note that we can write φ as φ(a) = max\nβ∈S ∑ j βj(cj + aj) .\nHence, the conjugate of φ is\nφ∗(b) = max a\n[ a>b− φ(a) ] = max\na min β∈S a>b−∑ j βj(cj + aj)  = min\nβ∈S max a a>b−∑ j βj(cj + aj)  = min β∈S −∑ j βjcj + ∑ j max aj aj(bj − βj)  . Each inner maximization over aj would be∞ unless βj = bj . Therefore,\nφ∗(b) = { −c>b if b ∈ S ∞ otherwise\n(11)\nSmooth max-of-hinge This loss obtained by smoothing the max-of-hinge loss using the technique described in Lemma 2. This loss is parameterized by a scalar γ > 0. We start by adding regularization to the conjugate of the max-of-hinge given in (11) and obtain\nφ̃∗γ(b) =\n{ γ 2‖b‖\n2 − c>b if b ∈ S ∞ otherwise\n(12)\nTaking the conjugate of the conjugate we obtain\nφ̃γ(a) = max b b>a− φ̃∗γ(b)\n= max b∈S b>(a+ c)− γ 2 ‖b‖2 = γ\n2 ‖(a+ c)/γ‖2 − γ 2 min b∈S ‖b− (a+ c)/γ‖2 (13)\nWhile we do not have a closed form solution for the minimization problem over b in the definition of φ̃γ above, this is a problem of projecting onto the intersection of the L1 ball and the positive orthant, and can be solved efficiently using the following procedure, adapted from [9].\nProject(µ)\nGoal: solve argminb ‖b− µ‖2 s.t. b ∈ Rk+, ‖b‖1 ≤ 1 Let: ∀i, µ̃i = max{0, µi} If: ‖µ̃‖1 ≤ 1 stop and return b = µ̃ Sort: let i1, . . . , ik be s.t. µi1 ≥ µi2 ≥ . . . ≥ µik Find: j∗ = max { j : j µ̃ij + 1− ∑j r=1 µ̃ir > 0\n} Define: θ = −1 + ∑j∗ r=1 µ̃ir Return: b s.t. ∀i, bi = max{µi − θ/j∗, 0}\nIt also holds that ∇φ̃γ(a) = argminb∈S ‖b − (a + c)/γ‖2, and therefore the gradient can also be calculated using the above projection procedure.\nNote that if φ being the max-of-hinge loss, then φ∗(b) + γ/2 ≥ φ̃∗γ(b) ≥ φ∗(b) and hence φ(a)− γ/2 ≤ φ̃γ(a) ≤ φ(a).\nObserve that all negative elements of a + c does not contribute to φ̃γ . This immediately implies that if φ(a) = 0 then we also have φ̃γ(a) = 0.\nSoft-max-of-hinge loss function: Another approach to smooth the max-of-hinge loss function is by using soft-max instead of max. The resulting soft-max-of-hinge loss function is defined as\nφγ(a) = γ log\n( 1 +\nk∑ i=1 e(ci+ai)/γ\n) , (14)\nwhere γ > 0 is a parameter. We have\nmax i [ci + ai]+ ≤ φγ(a) ≤ max i [ci + ai]+ + γ log(k + 1) .\nThe j’th element of the gradient of φ is\n∇jφγ(a) = e(cj+aj)/γ 1 + ∑k\ni=1 e (ci+ai)/γ\n.\nBy the definition of the conjugate we have φ∗γ(b) = maxa a >b − φγ(a). The vector a that maximizes the above must satisfy\n∀j, bj = e(cj+aj)/γ 1 + ∑k\ni=1 e (ci+ai)/γ\n.\nThis can be satisfied only if bj ≥ 0 for all j and ∑ j bj ≤ 1. That is, b ∈ S. Denote Z = ∑k i=1 e (ci+ai)/γ and note that\n(1 + Z)‖b‖1 = Z ⇒ Z = ‖b‖1 1− ‖b‖1 ⇒ 1 + Z = 1 1− ‖b‖1 .\nIt follows that\naj = γ(log(bj) + log(1 + Z))− cj = γ(log(bj)− log(1− ‖b‖1))− cj\nwhich yields\nφ∗γ(b) = ∑ j (γ(log(bj)− log(1− ‖b‖1))− cj) bj + γ log(1− ‖b‖1)\n= −c>b+ γ (1− ‖b‖1) log(1− ‖b‖1) +∑ j bj log(bj)  . Finally, if b /∈ S then the gradient of a>b−φγ(a) does not vanish anywhere, which means that φ∗γ(b) =∞. All in all, we obtain\nφ∗γ(b) =\n{ −c>b+ γ ( (1− ‖b‖1) log(1− ‖b‖1) + ∑ j bj log(bj) ) if b ∈ S\n∞ otherwise (15)\nSince the entropic function, ∑\nj bj log(bj) is 1-strongly convex over S with respect to theL1 norm, we obtain that φ∗γ is γ-strongly convex with respect to the L1 norm, from which it follows that φγ is (1/γ)-smooth with respect to the L∞ norm."
    }, {
      "heading" : "5.2 Regularizers",
      "text" : "L2 regularization: The simplest regularization is the squared L2 regularization\ng(w) = 1\n2 ‖w‖22 .\nThis is a 1-strongly convex regularization function whose conjugate is\ng∗(θ) = 1\n2 ‖θ‖22 .\nWe also have ∇g∗(θ) = θ .\nFor our acceleration procedure, we also use the L2 regularization plus a linear term, namely,\ng(w) = 1\n2 ‖w‖2 − w>z ,\nfor some vector z. The conjugate of this function is\ng∗(θ) = max w\n[ w>(θ + z)− 1\n2 ‖w‖2\n] = 1\n2 ‖θ + z‖2 .\nWe also have ∇g∗(θ) = θ + z .\nL1 regularization: Another popular regularization we consider is the L1 regularization,\nf(w) = σ ‖w‖1 .\nThis is not a strongly convex regularizer and therefore we will add a slight L2 regularization to it and define the L1-L2 regularization as\ng(w) = 1\n2 ‖w‖22 + σ′ ‖w‖1 , (16)\nwhere σ′ = σλ for some small λ. Note that\nλg(w) = λ\n2 ‖w‖22 + σ‖w‖1 ,\nso if λ is small enough (as will be formalized later) we obtain that λg(w) ≈ σ‖w‖1. The conjugate of g is\ng∗(v) = max w\n[ w>v − 1\n2 ‖w‖22 − σ′‖w‖1\n] .\nThe maximizer is also∇g∗(v) and we now show how to calculate it. We have\n∇g∗(v) = argmax w\n[ w>v − 1\n2 ‖w‖22 − σ′‖w‖1 ] = argmin\nw\n[ 1\n2 ‖w − v‖22 + σ′‖w‖1 ] A sub-gradient of the objective of the optimization problem above is of the form w − v + σ′z = 0, where z is a vector with zi = sign(wi), where if wi = 0 then zi ∈ [−1, 1]. Therefore, if w is an optimal solution then for all i, either wi = 0 or wi = vi− σ′sign(wi). Furthermore, it is easy to verify that if w is an optimal solution then for all i, if wi 6= 0 then the sign of wi must be the sign of vi. Therefore, whenever wi 6= 0 we have that wi = vi − σ′sign(vi). It follows that in that case we must have |vi| > σ′. And, the other direction is also true, namely, if |vi| > σ′ then setting wi = vi − σ′sign(vi) leads to an objective value whose i’th component is\n1\n2\n( σ′ )2 + σ′(|vi| − σ′) ≤ 1\n2 |vi|2 ,\nwhere the right-hand side is the i’th component of the objective value we will obtain by setting wi = 0. This leads to the conclusion that\n∇ig∗(v) = sign(vi) [ |vi| − σ′ ] + =\n{ vi − σ′sign(vi) if |vi| > σ′\n0 o.w.\nIt follows that g∗(v) = ∑ i sign(vi) [ |vi| − σ′ ] + vi − 1 2 ∑ i ( [ |vi| − σ′ ] + )2 − σ′ ∑ i [ |vi| − σ′ ] +\n= ∑ i [ |vi| − σ′ ] + ( |vi| − σ′ − 1 2 [ |vi| − σ′ ] + ) = 1\n2 ∑ i ([ |vi| − σ′ ] + )2 .\nAnother regularization function we’ll use in the accelerated procedure is\ng(w) = 1\n2 ‖w‖22 + σ′ ‖w‖1 − z>w . (17)\nThe conjugate function is\ng∗(v) = 1\n2 ∑ i ([ |vi + zi| − σ′ ] + )2 ,\nand its gradient is ∇ig∗(v) = sign(vi + zi) [ |vi + zi| − σ′ ] +"
    }, {
      "heading" : "5.3 Ridge Regression",
      "text" : "In ridge regression, we minimize the squared loss with L2 regularization. That is, g(w) = 12‖w‖ 2 and for every i we have that xi ∈ Rd and φi(a) = 12(a− yi) 2 for some yi ∈ R. The primal problem is therefore\nP (w) = 1\n2n n∑ i=1 (x>i w − yi)2 + λ 2 ‖w‖2 .\nBelow we specify Prox-SDCA for ridge regression. We use Option I since it is possible to derive a closed form solution to the maximization of the dual with respect to ∆αi. Indeed, since −φ∗i (−b) = −12b\n2 + yib we have that the maximization problem is\n∆αi = argmax b −1 2 (α (t+1) i + b) 2 + yi(α (t+1) i + b)− w (t−1)>xib− b2‖xi‖2 2λn\n= argmax b −1 a\n( 1 + ‖xi‖2\n2λn\n) b2 − ( α\n(t+1) i + w (t−1)>xi − yi ) b\n= − α\n(t+1) i + w (t−1)>xi − yi 1 + ‖xi‖ 2\n2λn\n.\nApplying the above update and using some additional tricks to improve the running time we obtain the following procedure.\nProx-SDCA((xi, yi)ni=1, , α (0), z) for solving ridge regression\nGoal: Minimize P (w) = 12n ∑n i=1(x > i w − yi)2 + λ ( 1 2‖w‖ 2 − w>z )\nInitialize v(0) = 1λn ∑n i=1 α (0) i xi, ∀i, ỹi = yi − x>i z Iterate: for t = 1, 2, . . . Randomly pick i\n∆αi = − α (t−1) i +v (t−1)>xi−ỹi 1+ ‖xi‖2 2λn α (t) i ← α (t−1) i + ∆αi and for j 6= i, α (t) j ← α (t−1) j\nv(t) ← v(t−1) + ∆αiλn xi Stopping condition:\nLet w(t) = v(t) + z Stop if 12n ∑n i=1 ( (x>i w (t) − yi)2 + (α(t)i + yi)2 − y2i ) + λw(t) > v(t) ≤\nThe runtime of Prox-SDCA for ridge regression becomes\nÕ ( d ( n+ R2\nλ\n)) ,\nwhere R = maxi ‖xi‖. This matches the recent results of [15, 25]. If R2/λ n we can apply the accelerated procedure and obtain the improved runtime\nÕ ( d √ nR2\nλ\n) ."
    }, {
      "heading" : "5.4 Logistic Regression",
      "text" : "In logistic regression, we minimize the logistic loss with L2 regularization. That is, g(w) = 12‖w‖ 2 and for every i we have that xi ∈ Rd and φi(a) = log(1 + ea). The primal problem is therefore3\nP (w) = 1\nn n∑ i=1 log(1 + ex > i w) + λ 2 ‖w‖2 .\nThe dual problem is\nD(α) = 1\nn n∑ i=1 (αi log(−αi)− (1 + αi) log(1 + αi))− λ 2 ‖v(α)‖2 ,\nand the dual constraints are α ∈ [−1, 0]n. Below we specify Prox-SDCA for logistic regression using Option III.\nProx-SDCA((xi)ni=1, , α (0), z) for logistic regression\nGoal: Minimize P (w) = 1n ∑n i=1 log(1 + e x>i w) + λ ( 1 2‖w‖ 2 − w>z )\nInitialize v(0) = 1λn ∑n i=1 α (0) i xi, and ∀i, pi = x>i z Define: φ∗(b) = b log(b) + (1− b) log(1− b) Iterate: for t = 1, 2, . . .\nRandomly pick i p = x>i w (t−1) q = −1/(1 + e−p)− α(t−1)i s = min ( 1, log(1+ep)+φ∗(−α(t−1)i )+pα (t−1) i +2q 2\nq2(4+ 1 λn ‖xi‖2) ) ∆αi = sq α (t) i = α (t−1) i + ∆αi and for j 6= i, α (t) j = α (t−1) j\nv(t) = v(t−1) + ∆αiλn xi Stopping condition:\nlet w(t) = v(t) + z Stop if 1n ∑n i=1 ( log(1 + ex > i w (t) ) + φ∗(−α(t−1)i ) ) + λw(t)>v(t) ≤\nThe runtime analysis is similar to the analysis for ridge regression.\n3Usually, the training data comes with labels, yi ∈ {±1}, and the loss function becomes log(1 + e−yix > i w). However, we can\neasily get rid of the labels by re-defining xi ← −yixi."
    }, {
      "heading" : "5.5 Lasso",
      "text" : "In the Lasso problem, the loss function is the squared loss but the regularization function is L1. That is, we need to solve the problem:\nmin w\n[ 1\n2n n∑ i=1 (x>i w − yi)2 + σ‖w‖1\n] , (18)\nwith a positive regularization parameter σ ∈ R+. Let ȳ = 12n ∑n i=1 y 2 i , and let w̄ be an optimal solution of (18). Then, the objective at w̄ is at most the objective at w = 0, which yields\nσ‖w̄‖1 ≤ ȳ ⇒ ‖w̄‖2 ≤ ‖w̄‖1 ≤ ȳ\nσ .\nConsider the optimization problem\nmin w P (w) where P (w) =\n1\n2n n∑ i=1 (x>i w − yi)2 + λ ( 1 2 ‖w‖22 + σ λ ‖w‖1 ) , (19)\nfor some λ > 0. This problem fits into our framework, since now the regularizer is strongly convex. Furthermore, if w∗ is an ( /2)-accurate solution to the problem in (19), then P (w∗) ≤ P (w̄) + /2 which yields [\n1\n2n n∑ i=1 (x>i w ∗ − yi)2 + σ‖w∗‖1\n] ≤ [ 1\n2n n∑ i=1 (x>i w̄ − yi)2 + σ‖w̄‖1\n] + λ\n2 ‖w̄‖22 + /2 .\nSince ‖w̄‖22 ≤ (ȳ/σ) 2, we obtain that setting λ = (σ/ȳ)2 guarantees that w∗ is an accurate solution to the original problem given in (18). In light of the above, from now on we focus on the problem given in (19). As in the case of ridge regression, we can apply Prox-SDCA with Option I. The resulting pseudo-code is given below. Applying the above update and using some additional tricks to improve the running time we obtain the following procedure.\nProx-SDCA((xi, yi)ni=1, , α (0), z) for solving L1 − L2 regression\nGoal: Minimize P (w) = 12n ∑n i=1(x > i w − yi)2 + λ ( 1 2‖w‖ 2 + σ′‖w‖1 − w>z )\nInitialize v(0) = 1λn ∑n i=1 α (0) i xi, and ∀j, w (0) j = sign(v (0) j + zj)[|v (0) j + zj | − σ′]+ Iterate: for t = 1, 2, . . . Randomly pick i\n∆αi = − α (t−1) i +w (t−1)>xi−yi 1+ ‖xi‖2 2λn α (t) i = α (t−1) i + ∆αi and for j 6= i, α (t) j = α (t−1) j v(t) = v(t−1) + ∆αiλn xi ∀j, w(t)j = sign(v (t) j + zj)[|v (t) j + zj | − σ′]+\nStopping condition: Stop if 12n ∑n i=1 ( (x>i w (t) − yi)2 − 2yiα(t)i + (α (t) i ) 2 ) + λw(t)>v(t) ≤\nLet us now discuss the runtime of the resulting method. Denote R = maxi ‖xi‖ and for simplicity, assume that ȳ = O(1). Choosing λ = (σ/ȳ)2, the runtime of our method becomes\nÕ ( d ( n+ min { R2\nσ2 ,\n√ nR2\nσ2\n})) .\nIt is also convenient to write the bound in terms of B = ‖w̄‖2, where, as before, w̄ is the optimal solution of the L1 regularized problem. With this parameterization, we can set λ = /B2 and the runtime becomes\nÕ ( d ( n+ min { R2B2 , √ nR2B2 })) .\nThe runtime of standard SGD is O(dR2B2/ 2) even in the case of smooth loss functions such as the squared loss. Several variants of SGD, that leads to sparser intermediate solutions, have been proposed (e.g. [14, 21, 27, 8, 10]). However, all of these variants share the runtime ofO(dR2B2/ 2), which is much slower than our runtime when is small.\nAnother relevant approach is the FISTA algorithm of [2]. The shrinkage operator of FISTA is the same as the gradient of g∗ used in our approach. It is a batch algorithm using Nesterov’s accelerated gradient technique. For the squared loss function, the runtime of FISTA is\nO ( dn √ R2B2 ) .\nThis bound is worst than our bound by a factor of at least √ n.\nAnother approach to solving (18) is stochastic coordinate descent over the primal problem. [21] showed that the runtime of this approach is\nO\n( dnB2 ) ,\nunder the assumption that ‖xi‖∞ ≤ 1 for all i. Similar results can also be found in [16]. For our method, the runtime depends on R2 = maxi ‖xi‖22. If R2 = O(1) then the runtime of our method is much better than that of [21]. In the general case, if maxi ‖xi‖∞ ≤ 1 then R2 ≤ d, which yields the runtime of\nÕ ( d ( n+ min { dB2 , √ ndB2 })) .\nThis is the same or better than [21] whenever d = O(n)."
    }, {
      "heading" : "5.6 Linear SVM",
      "text" : "Support Vector Machines (SVM) is an algorithm for learning a linear classifier. Linear SVM (i.e., SVM with linear kernels) amounts to minimizing the objective\nP (w) = 1\nn n∑ i=1 [1− x>i w]+ + λ 2 ‖w‖2 ,\nwhere [a]+ = max{0, a}, and for every i, xi ∈ Rd. This can be cast as the objective given in (1) by letting the regularization be g(w) = 12‖w‖ 2 2, and for every i, φi(a) = [1− a]+, is the hinge-loss.\nLet R = maxi ‖xi‖2. SGD enjoys the rate of O ( 1 λ ) . Many software packages apply SDCA and obtain\nthe rate Õ ( n+ 1λ ) . We now show how our accelerated proximal SDCA enjoys the rate Õ ( n+ √ n λ ) . This is significantly better than the rate of SGD when λ < 1/n. We note that a default setting for λ, which often works well in practice, is λ = 1/n. In this case, λ = /n 1/n.\nOur first step is to smooth the hinge-loss. Let γ = and consider the smooth hinge-loss as defined in (9). Recall that the smooth hinge-loss satisfies\n∀a, φ(a)− γ/2 ≤ φ̃(a) ≤ φ(a) .\nLet P̃ be the SVM objective while replacing the hinge-loss with the smooth hinge-loss. Therefore, for every w′ and w,\nP (w′)− P (w) ≤ P̃ (w′)− P̃ (w) + γ/2 .\nIt follows that if w′ is an ( /2)-optimal solution for P̃ , then it is -optimal solution for P . For the smoothed hinge loss, the optimization problem given in Option I of Prox-SDCA has a closed form solution and we obtain the following procedure:\nProx-SDCA((x1, . . . , xn), , α(0), z) for solving SVM (with smooth hinge-loss as in (9))\nDefine: φ̃γ as in (9) Goal: Minimize P (w) = 1n ∑n i=1 φ̃γ(x > i w) + λ ( 1 2‖w‖ 2 − w>z )\nInitialize w(0) = z + 1λn ∑n i=1 α (0) i xi Iterate: for t = 1, 2, . . . Randomly pick i\n∆αi = max ( −α(t−1)i , min ( 1− α(t−1)i , 1−x>i w(t−1)−γ α (t−1) i\n‖xi‖2/(λn)+γ )) α\n(t) i ← α (t−1) i + ∆αi and for j 6= i, α (t) j ← α (t−1) j\nw(t) ← w(t−1) + ∆αiλn xi Stopping condition:\nStop if 1n ∑n i=1 ( φ̃γ(x > i w (t))− α(t)i + γ 2 (α (t) i ) 2 ) + λw(t) > (w(t) − z) ≤\nDenote R = maxi ‖xi‖. Then, the runtime of the resulting method is\nÕ ( d ( n+ min { R2\nγ λ ,\n√ nR2\nγ λ\n})) .\nIn particular, choosing γ = we obtain a solution to the original SVM problem in runtime of\nÕ ( d ( n+ min { R2\nλ ,\n√ nR2\nλ\n})) .\nAs mentioned before, this is better than SGD when 1λ n."
    }, {
      "heading" : "5.7 Multiclass SVM",
      "text" : "Next we consider Multiclass SVM using the construction described in Crammer and Singer [5]. Each example consists of an instance vector xi ∈ Rd and a label yi ∈ {1, . . . , k}. The goal is to learn a matrix W ∈ Rd,k such that W>xi is a k’th dimensional vector of scores for the different classes. The prediction is the coordinate of W>xi of maximal value. The loss function is\nmax j 6=yi\n(1 + (W>xi)j − (W>xi)yi) .\nThis can be written as φ((W>xi)− (W>xi)yi) where\nφi(a) = max j [ci,j + aj ]+ ,\nwith ci being the all ones vector except 0 in the yi coordinate. We can model this in our framework as follows. Given a matrix M let vec(M) be the column vector obtained by concatenating the columns of M . Let ej be the all zeros vector except 1 in the j’th coordinate. For every i, let ci = 1 − eyi and let Xi ∈ Rdk,k be the matrix whose j’th column is vec(xi(ej − eyi)>). Then,\nX>i vec(W ) = W >xi − (W>xi)yi .\nTherefore, the optimization problem of multiclass SVM becomes:\nmin w∈Rdk\nP (w) where P (w) = 1\nn n∑ i=1 φi(X > i w) + λ 2 ‖w‖2 .\nAs in the case of SVM, we will use the smooth version of the max-of-hinge loss function as described in (13). If we set the smoothness parameter γ to be then an ( /2)-accurate solution to the problem with the smooth loss is also an -accurate solution to the original problem with the non-smooth loss. Therefore, from now on we focus on the problem with the smooth max-of-hinge loss.\nWe specify Prox-SDCA for multiclass SVM using Option I. We will show that the optimization problem in Option I can be calculated efficiently by sorting a k dimensional vector. Such ideas were explored in [5] for the non-smooth max-of-hinge loss.\nLet ŵ = w − 1λnXiα (t−1) i . Then, the optimization problem over αi can be written as\nargmax αi:−αi∈S\n(−c>i − ŵ>Xi)αi − γ\n2 ‖αi‖2 −\n1\n2λn ‖Xiαi‖2 . (20)\nAs shown before, if we organize ŵ as a d×k matrix, denoted Ŵ , we have thatX>i ŵ = Ŵ>xi− (Ŵ>xi)yi . We also have that\nXiαi = ∑ j vec(xi(ej − eyi)>)αi,j = vec(xi ∑ j αi,j(ej − eyi)>) = vec(xi(αi − ‖αi‖1eyi)>) .\nIt follows that an optimal solution to (20) must set αi,yi = 0 and we only need to optimize over the rest of the dual variables. This also yields,\n‖Xiαi‖2 = ‖xi‖2‖αi‖22 + ‖xi‖2‖αi‖21 .\nSo, (20) becomes:\nargmax αi:−αi∈S,αi,yi=0\n(−c>i − ŵ>Xi)αi − γ\n2 ‖αi‖22 −\n‖xi‖2\n2λn ‖αi‖22 −\n‖xi‖2\n2λn ‖αi‖21 . (21)\nThis is equivalent to a problem of the form:\nargmin a∈Rk−1+ ,β\n‖a− µ‖22 + Cβ2 s.t. ‖a‖1 = β ≤ 1 , (22)\nwhere\nµ = c>i + ŵ >Xi\nγ + ‖xi‖ 2\nλn\nand C = ‖xi‖2 λn\nγ + ‖xi‖ 2\nλn\n= 1\nγλn ‖xi‖2 + 1\n.\nThe equivalence is in the sense that if (a, β) is a solution of (22) then we can set αi = −a. Assume for simplicity that µ is sorted in a non-increasing order and that all of its elements are nonnegative (otherwise, it is easy to verify that we can zero the negative elements of µ and sort the non-negative, without affecting the solution). Let µ̄ be the cumulative sum of µ, that is, for every j, let µ̄j = ∑j r=1 µr. For every j, let zj = µ̄j − jµj . Since µ is sorted we have that\nzj+1 = j+1∑ r=1 µr − (j + 1)µj+1 = j∑ r=1 µr − jµj+1 ≥ j∑ r=1 µr − jµj = zj .\nNote also that z1 = 0 and that zk = µ̄k = ‖µ‖1 (since the coordinate of µ that corresponds to yi is zero). By the properties of projection onto the simplex (see [9]), for every z ∈ (zj , zj+1) we have that the projection of µ onto the set {b ∈ Rk+ : ‖b‖1 = z} is of the form ar = max{0, µr − θ/j} where θ = (−z + µ̄j)/j. Therefore, the objective becomes (ignoring constants that do not depend on z),\njθ2 + Cz2 = (−z + µ̄j)2/j + Cz2 .\nThe first order condition for minimality w.r.t. z is\n−(−z + µ̄j)/j + Cz = 0 ⇒ z = µ̄j\n1 + jC .\nIf this value of z is in (zj , zj+1), then it is the optimal z and we’re done. Otherwise, the optimum should be either z = 0 (which yields α = 0 as well) or z = 1.\na = OptimizeDual(µ,C)\nSolve the optimization problem given in (22) Initialize: ∀i, µ̂i = max{0, µi}, and sort µ̂ s.t. µ̂1 ≥ µ̂2 ≥ . . . ≥ µ̂k Let: µ̄ be s.t. µ̄j = ∑j i=1 µ̂i Let: z be s.t. zj = min{µ̄j − jµ̂j , 1} and zk+1 = 1 If: ∃j s.t. µ̄j1+jC ∈ [zj , zj+1]\nreturn a s.t. ∀i, ai = max { 0, µi − ( − µ̄j1+jC + µ̄j ) /j }\nElse: Let j be the minimal index s.t. zj = 1 set a s.t. ∀i, ai = max{0, µi − (−zj + µ̄j)/j} If: ‖a− µ‖2 + C ≤ ‖µ‖2\nreturn a Else:\nreturn (0, . . . , 0)\nThe resulting pseudo-codes for Prox-SDCA is given below. We specify the procedure while referring to W as a matrix, because it is the more natural representation. For convenience of the code, we also maintain in αi,yi the value of − ∑ j 6=yi αi,j (instead of the optimal value of 0).\nProx-SDCA((x1, y1)ni=1, , α, Z) for solving Multiclass SVM (with smooth hinge-loss as in (13))\nDefine: φ̃γ as in (13) Goal: Minimize P (W ) = 1n ∑n i=1 φ̃γ((W >xi)− (W>xi)yi) + λ ( 1 2vec(W ) >vec(W )− vec(W )>vec(Z) )\nInitialize W = Z + 1λn ∑n i=1 xiα > i Iterate: for t = 1, 2, . . . Randomly pick i Ŵ = W − 1λnxiα > i\np = x>i Ŵ , p = p− pyi , c = 1− eyi , µ = c+p γ+‖xi‖2/(λn) , C = 1 1+γλn/‖xi‖2 a = OptimizeDual(µ,C) αi = −a, αyi = ‖a‖1 W = Ŵ + 1λnxiα > i Stopping condition: let G = 0 for i = 1, . . . , n a = W>xi, a = a− ayi , c = 1− eyi , b = Project((a+ c)/γ) G = G+ γ2 (‖(a+ c)/γ‖ 2 − ‖b− (a+ c)/γ‖2) + c>α(t)i + γ 2 (‖α (t) i ‖2 − (α (t) i,yi )2)\nStop if G/n+ λvec(W )>vec(W − Z) ≤"
    }, {
      "heading" : "6 Experiments",
      "text" : "In this section we compare Prox-SDCA, its accelerated version Accelerated-Prox-SDCA, and the FISTA algorithm of [2], on L1 − L2 regularized loss minimization problems.\nThe experiments were performed on three large datasets with very different feature counts and sparsity, which were kindly provided by Thorsten Joachims (the datasets were also used in [24]). The astro-ph dataset classifies abstracts of papers from the physics ArXiv according to whether they belong in the astrophysics section; CCAT is a classification task taken from the Reuters RCV1 collection; and cov1 is class 1 of the covertype dataset of Blackard, Jock & Dean. The following table provides details of the dataset characteristics.\nDataset Training Size Testing Size Features Sparsity astro-ph 29882 32487 99757 0.08%\nCCAT 781265 23149 47236 0.16% cov1 522911 58101 54 22.22%\nThese are binary classification problems, with each xi being a vector which has been normalized to be ‖xi‖2 = 1, and yi being a binary class label of ±1. We multiplied each xi by yi and following [24], we employed the smooth hinge loss, φ̃γ , as in (9), with γ = 1. The optimization problem we need to solve is therefore\nmin w P (w) where P (w) =\n1\nn n∑ i=1 φ̃γ(x > i w) + λ 2 ‖w‖22 + σ‖w‖1 .\nIn the experiments, we set σ = 10−5 and vary λ in the range {10−6, 10−7, 10−8, 10−9}. The convergence behaviors are plotted in Figure 3. In all the plots we depict the primal objective as a function of the number of passes over the data (often referred to as “epochs”). For FISTA, each iteration involves a single pass over the data. For Prox-SDCA, each n iterations are equivalent to a single pass over the data. And, for Accelerated-Prox-SDCA, each n inner iterations are equivalent to a single pass over the data. For Prox-SDCA and Accelerated-Prox-SDCA we implemented their corresponding stopping conditions and terminate the methods once an accuracy of 10−3 was guaranteed.\nIt is clear from the graphs that Accelerated-Prox-SDCA yields the best results, and often significantly outperform the other methods. Prox-SDCA behaves similarly when λ is relatively large, but it converges much slower when λ is small. This is consistent with our theory. Finally, the relative performance of FISTA and Prox-SDCA depends on the ratio between λ and n, but in all cases, Accelerated-Prox-SDCA is much faster than FISTA. This is again consistent with our theory."
    }, {
      "heading" : "7 Discussion and Open Problems",
      "text" : "We have described and analyzed a proximal stochastic dual coordinate ascent method and have shown how to accelerate the procedure. The overall runtime of the resulting method improves state-of-the-art results in many cases of interest.\nThere are two main open problems that we leave to future research. Open Problem 1. When 1λγ is larger than n, the runtime of our procedure becomes Õ ( d √ n λγ ) . Is it\npossible to derive a method whose runtime is Õ ( d ( n+ √ 1 λγ )) ?\nOpen Problem 2. Our Prox-SDCA procedure and its analysis works for regularizers which are strongly convex with respect to an arbitrary norm. However, our acceleration procedure is designed for regularizers which are strongly convex with respect to the Euclidean norm. Is is possible to extend the acceleration procedure to more general regularizers?"
    }, {
      "heading" : "Acknowledgements",
      "text" : "The authors would like to thank Fen Xia for careful proof-reading of the paper which helped us to correct numerous typos. Shai Shalev-Shwartz is supported by the following grants: Intel Collaborative Research Institute for Computational Intelligence (ICRI-CI) and ISF 598-10. Tong Zhang is supported by the following grants: NSF IIS-1016061, NSF DMS-1007527, and NSF IIS-1250985."
    }, {
      "heading" : "A Proofs of Iteration Bounds for Prox-SDCA",
      "text" : "The proof technique follows that of Shalev-Shwartz and Zhang [25], but with the required generality for handling general strongly convex regularizers and smoothness/Lipschitzness with respect to general norms.\nWe prove the theorems for running Prox-SDCA while choosing ∆αi as in Option I. A careful examination of the proof easily reveals that the results hold for the other options as well. More specifically, Lemma 6 only requires choosing ∆αi = s(u (t−1) i −α (t−1) i ) as in (23), and Option III chooses s to optimize the bound on the right hand side of (25), and hence ensures that the choice can do no worse than the result of Lemma 6 with any s. The simplification in Option IV and V employs the specific simplification of the bound in Lemma 6 in the proof of the theorems.\nThe key lemma is the following:\nLemma 6. Assume that φ∗i is γ-strongly-convex. For any iteration t, let Et denote the expectation with respect to the randomness in choosing i at round t, conditional on the value of α(t−1). Then, for any iteration t and any s ∈ [0, 1] we have\nEt[D(α(t))−D(α(t−1))] ≥ s n [P (w(t−1))−D(α(t−1))]− ( s n )2 G(t) 2λ ,\nwhere\nG(t) = 1\nn n∑ i=1 ( ‖Xi‖2D→D′ − γ(1− s)λn s ) Et [ ‖u(t−1)i − α (t−1) i ‖ 2 D ] ,\nand −u(t−1)i = ∇φi(X>i w(t−1)). Proof. Since only the i’th element of α is updated, the improvement in the dual objective can be written as\nn[D(α(t))−D(α(t−1))] = ( −φ∗(−α(t)i )− λng ∗ ( v(t−1) + (λn)−1Xi∆αi )) − ( −φ∗(−α(t−1)i )− λng ∗ ( v(t−1) )) The smoothness of g∗ implies that g∗(v + ∆v) ≤ h(v; ∆v), where h(v; ∆v) := g∗(v) + ∇g∗(v)>∆v + 1 2‖∆v‖ 2 D′ . Therefore,\nn[D(α(t))−D(α(t−1))] ≥ ( −φ∗(−α(t)i )− λnh ( v(t−1); (λn)−1Xi∆αi )) ︸ ︷︷ ︸\nA\n− ( −φ∗(−α(t−1)i )− λng ∗ ( v(t−1) )) ︸ ︷︷ ︸\nB\n.\nBy the definition of the update we have for all s ∈ [0, 1] that\nA = max ∆αi −φ∗(−(α(t−1)i + ∆αi))− λnh\n( v(t−1); (λn)−1Xi∆αi ) ≥ −φ∗(−(α(t−1)i + s(u (t−1) i − α (t−1) i )))− λnh(v (t−1); (λn)−1sXi(u (t−1) i − α (t−1) i )). (23)\nFrom now on, we omit the superscripts and subscripts. Since φ∗ is γ-strongly convex, we have that\nφ∗(−(α+s(u−α))) = φ∗(s(−u)+(1−s)(−α)) ≤ sφ∗(−u)+(1−s)φ∗(−α)− γ 2 s(1−s)‖u−α‖2D (24)\nCombining this with (23) and rearranging terms we obtain that\nA ≥ −sφ∗(−u)− (1− s)φ∗(−α) + γ 2 s(1− s)‖u− α‖2D − λnh(v; (λn)−1sX(u− α))\n= −sφ∗(−u)− (1− s)φ∗(−α) + γ 2 s(1− s)‖u− α‖2D − λng∗(v)− sw>X(u− α)− s2‖X(u− α)‖2D′ 2λn ≥ −s(φ∗(−u) + w>Xu) + (−φ∗(−α)− λng∗(v))\n+ s\n2\n( γ(1− s)−\ns‖X‖2D→D′ λn\n) ‖u− α‖2D + s(φ∗(−α) + w>Xα).\nSince −u = ∇φ(X>w) we have φ∗(−u) + w>Xu = −φ(X>w), which yields\nA−B ≥ s [ φ(X>w) + φ∗(−α) + w>Xα+ ( γ(1− s)\n2 − s‖X‖2D→D′ 2λn\n) ‖u− α‖2D ] . (25)\nNext note that with w = ∇g∗(v), we have g(w) + g∗(v) = w>v. Therefore:\nP (w)−D(α) = 1 n n∑ i=1 φi(X > i w) + λg(w)− ( − 1 n n∑ i=1 φ∗i (−αi)− λg∗(v) )\n= 1\nn n∑ i=1 φi(X > i w) + 1 n n∑ i=1 φ∗i (−αi) + λw>v\n= 1\nn n∑ i=1 ( φi(X > i w) + φ ∗ i (−αi) + w>Xiαi ) .\nTherefore, if we take expectation of (25) w.r.t. the choice of i we obtain that\n1 s Et[A−B] ≥ [P (w)−D(α)]− s 2λn · 1 n n∑ i=1 ( ‖Xi‖2D→D′ − γ(1− s)λn s ) Et[‖ui − αi‖2D]︸ ︷︷ ︸\n=G(t)\n.\nWe have obtained that\nn s Et[D(α(t))−D(α(t−1))] ≥ [P (w(t−1))−D(α(t−1))]−\nsG(t)\n2λn . (26)\nMultiplying both sides by s/n concludes the proof of the lemma.\nEquipped with the above lemmas we are ready to prove Theorem 1 and Theorem 2.\nProof of Theorem 1. The assumption that φi is (1/γ)-smooth implies that φ∗i is γ-strongly-convex. We will apply Lemma 6 with\ns = n\nn+R2/(λγ) =\nλnγ\nR2 + λnγ ∈ [0, 1] .\nRecall that ‖Xi‖D→D′ ≤ R. Therefore, the choice of s implies that\n‖Xi‖2D→D′ − γ(1− s)λn s ≤ R2 − 1− s s/(λnγ) = R2 −R2 = 0 ,\nand hence G(t) ≤ 0 for all t. This yields,\nEt[D(α(t))−D(α(t−1))] ≥ s n (P (w(t−1))−D(α(t−1))) . (27)\nTaking expectation of both sides with respect to the randomness at previous rounds, and using the law of total expectation, we obtain that\nE[D(α(t))−D(α(t−1))] ≥ s n E[P (w(t−1))−D(α(t−1))] . (28)\nBut since (t−1)D := D(α ∗)−D(α(t−1)) ≤ P (w(t−1))−D(α(t−1)) andD(α(t))−D(α(t−1)) = (t−1)D − (t) D , we obtain that E[ (t)D ] ≤ ( 1− sn ) E[ (t−1)D ] ≤ ( 1− sn )t (0) D ≤ (0) D e − st n . Therefore, whenever t ≥ n\ns log(\n(0) D / D) =\n( n+ R 2\nλγ\n) log(\n(0) D / D) ,\nwe are guaranteed that E[ (t)D ] would be smaller than D. Using again (28), we can also obtain that\nE[P (w(t))−D(α(t))] ≤ n s E[D(α(t+1))−D(α(t))] = n s E[ (t)D − (t+1) D ] ≤ n s E[ (t)D ]. (29)\nSo, requiring E[ (t)D ] ≤ s n P we obtain an expected duality gap of at most P . This means that we should require\nt ≥ ( n+ R 2\nλγ\n) log((n+ R 2\nλγ ) · (0) D P ) ,\nwhich proves the first part of Theorem 1. Next, we sum the first inequality of (29) over t = T0 + 1, . . . , T to obtain\nE  1 T − T0 T∑ t=T0+1 (P (w(t))−D(α(t)))  ≤ n s(T − T0) E[D(α(T+1))−D(α(T0+1))].\nNow, if we choose w̄, ᾱ to be either the average vectors or a randomly chosen vector over t ∈ {T0 + 1, . . . , T}, then the above implies\nE[P (w̄)−D(ᾱ)] ≤ n s(T − T0) E[D(α(T+1))−D(α(T0+1))]\n≤ n s(T − T0) E[ (T0+1)D )] ≤ n s(T − T0) (0) D e − sT0 n .\nIt follows that in order to obtain a result of E[P (w̄)−D(ᾱ)] ≤ P , we need to have\nT0 ≥ n\ns log\n( n\n(0) D\ns(T − T0) P\n) .\nIn particular, the choice of T − T0 = ns and T0 = n s log( (0) D / P ) satisfies the above requirement.\nProof of Theorem 2. Define t0 = dns log(2 (0) D / D)e. The proof of Theorem 1 implies that for every t, E[ (t)D ] ≤ (0) D e − st n . By Markov’s inequality, with probability of at least 1/2 we have (t)D ≤ 2 (0) D e − st n . Applying it for t = t0 we get that (t0) D ≤ D with probability of at least 1/2. Now, lets apply the same argument again, this time with the initial dual sub-optimality being (t0)D . Since the dual is monotonically non-increasing, we have that (t0)D ≤ (0) D . Therefore, the same argument tells us that with probability of at least 1/2 we would have that (2t0)D ≤ D. Repeating this dlog2(1/δ)e times, we obtain that with probability of at least 1 − δ, for some k we have that (kt0)D ≤ D. Since the dual is monotonically non-decreasing, the claim about the dual sub-optimality follows.\nNext, for the duality gap, using (27) we have that for every t such that (t−1)D ≤ D we have\nP (w(t−1))−D(α(t−1)) ≤ n s E[D(α(t))−D(α(t−1))] ≤ n s D .\nThis proves the second claim of Theorem 2. For the last claim, suppose that at round T0 we have (T0) D ≤ D. Let T = T0 + n/s. It follows that if we choose t uniformly at random from {T0, . . . , T − 1}, then E[P (w(t)) − D(α(t))] ≤ D. By Markov’s inequality, with probability of at least 1/2 we have P (w(t)) − D(α(t)) ≤ 2 D. Therefore, if we choose log2(2/δ) such random t, with probability ≥ 1− δ/2, at least one of them will have P (w(t))−D(α(t)) ≤ 2 D. Combining with the first claim of the theorem, choosing D = P /2, and applying the union bound, we conclude the proof of the last claim of Theorem 2."
    } ],
    "references" : [ {
      "title" : "Estimate sequence methods: extensions and approximations",
      "author" : [ "Michel Baes" ],
      "venue" : "Institute for Operations Research, ETH, Zürich, Switzerland,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2009
    }, {
      "title" : "A fast iterative shrinkage-thresholding algorithm for linear inverse problems",
      "author" : [ "A. Beck", "M. Teboulle" ],
      "venue" : "SIAM Journal on Imaging Sciences,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2009
    }, {
      "title" : "Exponentiated gradient algorithms for conditional random fields and max-margin markov networks",
      "author" : [ "M. Collins", "A. Globerson", "T. Koo", "X. Carreras", "P. Bartlett" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2008
    }, {
      "title" : "Better mini-batch algorithms via accelerated gradient methods",
      "author" : [ "Andrew Cotter", "Ohad Shamir", "Nathan Srebro", "Karthik Sridharan" ],
      "venue" : "arXiv preprint arXiv:1106.4574,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2011
    }, {
      "title" : "On the algorithmic implementation of multiclass kernel-based vector machines",
      "author" : [ "K. Crammer", "Y. Singer" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2001
    }, {
      "title" : "Smooth optimization with approximate gradient",
      "author" : [ "Alexandre d’Aspremont" ],
      "venue" : "SIAM Journal on Optimization,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2008
    }, {
      "title" : "First-order methods of smooth convex optimization with inexact oracle",
      "author" : [ "Olivier Devolder", "Francois Glineur", "Yu. Nesterov" ],
      "venue" : "Technical Report 2011/2,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2011
    }, {
      "title" : "Efficient online and batch learning using forward backward splitting",
      "author" : [ "J. Duchi", "Y. Singer" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2009
    }, {
      "title" : "Efficient projections onto the l 1-ball for learning in high dimensions",
      "author" : [ "John Duchi", "Shai Shalev-Shwartz", "Yoram Singer", "Tushar Chandra" ],
      "venue" : "In Proceedings of the 25th international conference on Machine learning,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2008
    }, {
      "title" : "Composite objective mirror descent",
      "author" : [ "John Duchi", "Shai Shalev-Shwartz", "Yoram Singer", "Ambuj Tewari" ],
      "venue" : "In Proceedings of the 23rd Annual Conference on Learning Theory,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2010
    }, {
      "title" : "Optimal stochastic approximation algorithms for strongly convex stochastic composite optimization i: A generic algorithmic framework",
      "author" : [ "Saeed Ghadimi", "Guanghui Lan" ],
      "venue" : "SIAM Journal on Optimization,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2012
    }, {
      "title" : "Accelerated gradient methods for stochastic optimization and online learning",
      "author" : [ "Chonghai Hu", "Weike Pan", "James T Kwok" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2009
    }, {
      "title" : "Stochastic block-coordinate frank-wolfe optimization for structural svms",
      "author" : [ "S. Lacoste-Julien", "M. Jaggi", "M. Schmidt", "P. Pletscher" ],
      "venue" : "arXiv preprint arXiv:1207.4747,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2012
    }, {
      "title" : "Sparse online learning via truncated gradient",
      "author" : [ "J. Langford", "L. Li", "T. Zhang" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 2009
    }, {
      "title" : "A Stochastic Gradient Method with an Exponential Convergence Rate for Strongly-Convex Optimization with Finite Training Sets",
      "author" : [ "Nicolas Le Roux", "Mark Schmidt", "Francis Bach" ],
      "venue" : "arXiv preprint arXiv:1202.6258,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2012
    }, {
      "title" : "Efficiency of coordinate descent methods on huge-scale optimization problems",
      "author" : [ "Y. Nesterov" ],
      "venue" : "SIAM Journal on Optimization,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2012
    }, {
      "title" : "Smooth minimization of non-smooth functions",
      "author" : [ "Yurii Nesterov" ],
      "venue" : "Mathematical Programming,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2005
    }, {
      "title" : "Gradient methods for minimizing composite objective function",
      "author" : [ "Yurii Nesterov" ],
      "venue" : null,
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2007
    }, {
      "title" : "Iteration complexity of randomized block-coordinate descent methods for minimizing a composite function",
      "author" : [ "Peter Richtárik", "Martin Takáč" ],
      "venue" : "Mathematical Programming,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2012
    }, {
      "title" : "Convergence rates of inexact proximal-gradient methods for convex optimization",
      "author" : [ "Mark Schmidt", "Nicolas Le Roux", "Francis Bach" ],
      "venue" : "Technical Report arXiv:1109.2415,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2011
    }, {
      "title" : "Stochastic methods for l 1-regularized loss minimization",
      "author" : [ "S. Shalev-Shwartz", "A. Tewari" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2011
    }, {
      "title" : "Pegasos: Primal Estimated sub-GrAdient SOlver for SVM",
      "author" : [ "S. Shalev-Shwartz", "Y. Singer", "N. Srebro" ],
      "venue" : "In ICML,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2007
    }, {
      "title" : "Stochastic methods for l1 regularized loss minimization",
      "author" : [ "Shai Shalev-Shwartz", "Ambuj Tewari" ],
      "venue" : "In ICML,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2009
    }, {
      "title" : "Stochastic dual coordinate ascent methods for regularized loss minimization",
      "author" : [ "Shai Shalev-Shwartz", "Tong Zhang" ],
      "venue" : "arXiv preprint arXiv:1209.1873,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2012
    }, {
      "title" : "Stochastic dual coordinate ascent methods for regularized loss minimization",
      "author" : [ "Shai Shalev-Shwartz", "Tong Zhang" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2013
    }, {
      "title" : "Trading accuracy for sparsity in optimization problems with sparsity constraints",
      "author" : [ "Shai Shalev-Shwartz", "Nathan Srebro", "Tong Zhang" ],
      "venue" : "SIAM Journal on Optimization,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2010
    }, {
      "title" : "Dual averaging method for regularized stochastic learning and online optimization",
      "author" : [ "Lin Xiao" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2010
    }, {
      "title" : "On the dual formulation of regularized linear systems",
      "author" : [ "Tong Zhang" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 2002
    } ],
    "referenceMentions" : [ {
      "referenceID" : 24,
      "context" : "This matches the recent result of Shalev-Shwartz and Zhang [25], Le Roux et al.",
      "startOffset" : 59,
      "endOffset" : 63
    }, {
      "referenceID" : 14,
      "context" : "[15], but our setting is significantly more general.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 24,
      "context" : "This significantly improves over the result of [25, 15].",
      "startOffset" : 47,
      "endOffset" : 55
    }, {
      "referenceID" : 14,
      "context" : "This significantly improves over the result of [25, 15].",
      "startOffset" : 47,
      "endOffset" : 55
    }, {
      "referenceID" : 17,
      "context" : "It also significantly improves over the runtime of accelerated gradient descent due to Nesterov [18], which is Õ(dn √ 1 λ γ ).",
      "startOffset" : 96,
      "endOffset" : 100
    }, {
      "referenceID" : 21,
      "context" : "[22]), when 1 λ n.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 21,
      "context" : "SVM SGD [22] d λ AGD [17] dn √ 1 λ",
      "startOffset" : 8,
      "endOffset" : 12
    }, {
      "referenceID" : 16,
      "context" : "SVM SGD [22] d λ AGD [17] dn √ 1 λ",
      "startOffset" : 21,
      "endOffset" : 25
    }, {
      "referenceID" : 27,
      "context" : "[28, 27, 21]) d 2",
      "startOffset" : 0,
      "endOffset" : 12
    }, {
      "referenceID" : 26,
      "context" : "[28, 27, 21]) d 2",
      "startOffset" : 0,
      "endOffset" : 12
    }, {
      "referenceID" : 20,
      "context" : "[28, 27, 21]) d 2",
      "startOffset" : 0,
      "endOffset" : 12
    }, {
      "referenceID" : 22,
      "context" : "Stochastic Coordinate Descent [23, 16] dn FISTA [18, 2] dn √ 1",
      "startOffset" : 30,
      "endOffset" : 38
    }, {
      "referenceID" : 15,
      "context" : "Stochastic Coordinate Descent [23, 16] dn FISTA [18, 2] dn √ 1",
      "startOffset" : 30,
      "endOffset" : 38
    }, {
      "referenceID" : 17,
      "context" : "Stochastic Coordinate Descent [23, 16] dn FISTA [18, 2] dn √ 1",
      "startOffset" : 48,
      "endOffset" : 55
    }, {
      "referenceID" : 1,
      "context" : "Stochastic Coordinate Descent [23, 16] dn FISTA [18, 2] dn √ 1",
      "startOffset" : 48,
      "endOffset" : 55
    }, {
      "referenceID" : 14,
      "context" : "Ridge Regression Exact d2n+ d3 SGD [15], SDCA [25] d ( n+ 1 λ )",
      "startOffset" : 35,
      "endOffset" : 39
    }, {
      "referenceID" : 24,
      "context" : "Ridge Regression Exact d2n+ d3 SGD [15], SDCA [25] d ( n+ 1 λ )",
      "startOffset" : 46,
      "endOffset" : 50
    }, {
      "referenceID" : 17,
      "context" : "AGD [18] dn √ 1 λ",
      "startOffset" : 4,
      "endOffset" : 8
    }, {
      "referenceID" : 24,
      "context" : "1 In particular, we generalize the recent analysis of [25] in two directions.",
      "startOffset" : 54,
      "endOffset" : 58
    }, {
      "referenceID" : 24,
      "context" : "As in [25], the runtime of this procedure is Õ ( d ( n+ 1 λγ )) .",
      "startOffset" : 6,
      "endOffset" : 10
    }, {
      "referenceID" : 24,
      "context" : "Additional related work: As mentioned before, our first contribution is a proximal version of the stochastic dual coordinate ascent method and extension of the analysis given in Shalev-Shwartz and Zhang [25].",
      "startOffset" : 203,
      "endOffset" : 207
    }, {
      "referenceID" : 2,
      "context" : "[3] but in more restricted settings than the general problem considered in this paper.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 18,
      "context" : "One can also apply the analysis of stochastic coordinate descent methods given in Richtárik and Takáč [19] on the dual problem.",
      "startOffset" : 102,
      "endOffset" : 106
    }, {
      "referenceID" : 12,
      "context" : "Recently, [13] derived a stochastic coordinate ascent for structural SVM based on the Frank-Wolfe algorithm.",
      "startOffset" : 10,
      "endOffset" : 14
    }, {
      "referenceID" : 6,
      "context" : "[7], Schmidt et al.",
      "startOffset" : 0,
      "endOffset" : 3
    }, {
      "referenceID" : 19,
      "context" : "[20], to allow approximate and stochastic proximal mapping.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 0,
      "context" : "See also [1, 6].",
      "startOffset" : 9,
      "endOffset" : 15
    }, {
      "referenceID" : 5,
      "context" : "See also [1, 6].",
      "startOffset" : 9,
      "endOffset" : 15
    }, {
      "referenceID" : 19,
      "context" : "In particular, it relies on similar ideas as in Proposition 4 of [20].",
      "startOffset" : 65,
      "endOffset" : 69
    }, {
      "referenceID" : 19,
      "context" : "However, our specific requirement is different, and the proof presented here is different and significantly simpler than that of [20].",
      "startOffset" : 129,
      "endOffset" : 133
    }, {
      "referenceID" : 11,
      "context" : "See for example [12, 11, 4] and the references therein.",
      "startOffset" : 16,
      "endOffset" : 27
    }, {
      "referenceID" : 10,
      "context" : "See for example [12, 11, 4] and the references therein.",
      "startOffset" : 16,
      "endOffset" : 27
    }, {
      "referenceID" : 3,
      "context" : "See for example [12, 11, 4] and the references therein.",
      "startOffset" : 16,
      "endOffset" : 27
    }, {
      "referenceID" : 14,
      "context" : "As in [15, 25], we avoid the polynomial dependence on 1/ by allowing more than a single pass over the data.",
      "startOffset" : 6,
      "endOffset" : 14
    }, {
      "referenceID" : 24,
      "context" : "As in [15, 25], we avoid the polynomial dependence on 1/ by allowing more than a single pass over the data.",
      "startOffset" : 6,
      "endOffset" : 14
    }, {
      "referenceID" : 26,
      "context" : "Note that this particular update is rather similar to the update step of proximal-gradient dual-averaging method (see for example Xiao [27]).",
      "startOffset" : 135,
      "endOffset" : 139
    }, {
      "referenceID" : 0,
      "context" : "Let s = argmax s∈[0,1] [ −φi (−(α (t−1) i + sq))− sw (t−1)>Xiq − s2 2λn ‖Xiq‖2D′ ]",
      "startOffset" : 17,
      "endOffset" : 22
    }, {
      "referenceID" : 16,
      "context" : "Following Nesterov [17], we apply a “smoothing” technique.",
      "startOffset" : 19,
      "endOffset" : 23
    }, {
      "referenceID" : 25,
      "context" : "5 in [26].",
      "startOffset" : 5,
      "endOffset" : 9
    }, {
      "referenceID" : 16,
      "context" : "See Nesterov [17] for discussion.",
      "startOffset" : 13,
      "endOffset" : 17
    }, {
      "referenceID" : 0,
      "context" : "The conjugate function is φ∗(b) = max a ab− log(1 + e) = { b log(b) + (1− b) log(1− b) if b ∈ [0, 1] ∞ otherwise",
      "startOffset" : 94,
      "endOffset" : 100
    }, {
      "referenceID" : 8,
      "context" : "While we do not have a closed form solution for the minimization problem over b in the definition of φ̃γ above, this is a problem of projecting onto the intersection of the L1 ball and the positive orthant, and can be solved efficiently using the following procedure, adapted from [9].",
      "startOffset" : 281,
      "endOffset" : 284
    }, {
      "referenceID" : 14,
      "context" : "This matches the recent results of [15, 25].",
      "startOffset" : 35,
      "endOffset" : 43
    }, {
      "referenceID" : 24,
      "context" : "This matches the recent results of [15, 25].",
      "startOffset" : 35,
      "endOffset" : 43
    }, {
      "referenceID" : 13,
      "context" : "[14, 21, 27, 8, 10]).",
      "startOffset" : 0,
      "endOffset" : 19
    }, {
      "referenceID" : 20,
      "context" : "[14, 21, 27, 8, 10]).",
      "startOffset" : 0,
      "endOffset" : 19
    }, {
      "referenceID" : 26,
      "context" : "[14, 21, 27, 8, 10]).",
      "startOffset" : 0,
      "endOffset" : 19
    }, {
      "referenceID" : 7,
      "context" : "[14, 21, 27, 8, 10]).",
      "startOffset" : 0,
      "endOffset" : 19
    }, {
      "referenceID" : 9,
      "context" : "[14, 21, 27, 8, 10]).",
      "startOffset" : 0,
      "endOffset" : 19
    }, {
      "referenceID" : 1,
      "context" : "Another relevant approach is the FISTA algorithm of [2].",
      "startOffset" : 52,
      "endOffset" : 55
    }, {
      "referenceID" : 20,
      "context" : "[21] showed that the runtime of this approach is",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 15,
      "context" : "Similar results can also be found in [16].",
      "startOffset" : 37,
      "endOffset" : 41
    }, {
      "referenceID" : 20,
      "context" : "If R2 = O(1) then the runtime of our method is much better than that of [21].",
      "startOffset" : 72,
      "endOffset" : 76
    }, {
      "referenceID" : 20,
      "context" : "This is the same or better than [21] whenever d = O(n).",
      "startOffset" : 32,
      "endOffset" : 36
    }, {
      "referenceID" : 4,
      "context" : "7 Multiclass SVM Next we consider Multiclass SVM using the construction described in Crammer and Singer [5].",
      "startOffset" : 104,
      "endOffset" : 107
    }, {
      "referenceID" : 4,
      "context" : "Such ideas were explored in [5] for the non-smooth max-of-hinge loss.",
      "startOffset" : 28,
      "endOffset" : 31
    }, {
      "referenceID" : 8,
      "context" : "By the properties of projection onto the simplex (see [9]), for every z ∈ (zj , zj+1) we have that the projection of μ onto the set {b ∈ R+ : ‖b‖1 = z} is of the form ar = max{0, μr − θ/j} where θ = (−z + μ̄j)/j.",
      "startOffset" : 54,
      "endOffset" : 57
    }, {
      "referenceID" : 1,
      "context" : "In this section we compare Prox-SDCA, its accelerated version Accelerated-Prox-SDCA, and the FISTA algorithm of [2], on L1 − L2 regularized loss minimization problems.",
      "startOffset" : 112,
      "endOffset" : 115
    }, {
      "referenceID" : 23,
      "context" : "The experiments were performed on three large datasets with very different feature counts and sparsity, which were kindly provided by Thorsten Joachims (the datasets were also used in [24]).",
      "startOffset" : 184,
      "endOffset" : 188
    }, {
      "referenceID" : 23,
      "context" : "We multiplied each xi by yi and following [24], we employed the smooth hinge loss, φ̃γ , as in (9), with γ = 1.",
      "startOffset" : 42,
      "endOffset" : 46
    }, {
      "referenceID" : 24,
      "context" : "The proof technique follows that of Shalev-Shwartz and Zhang [25], but with the required generality for handling general strongly convex regularizers and smoothness/Lipschitzness with respect to general norms.",
      "startOffset" : 61,
      "endOffset" : 65
    }, {
      "referenceID" : 0,
      "context" : "Then, for any iteration t and any s ∈ [0, 1] we have Et[D(α)−D(α)] ≥ s n [P (w(t−1))−D(α(t−1))]− ( s n )2 G(t) 2λ ,",
      "startOffset" : 38,
      "endOffset" : 44
    }, {
      "referenceID" : 0,
      "context" : "By the definition of the update we have for all s ∈ [0, 1] that",
      "startOffset" : 52,
      "endOffset" : 58
    }, {
      "referenceID" : 0,
      "context" : "We will apply Lemma 6 with s = n n+R2/(λγ) = λnγ R2 + λnγ ∈ [0, 1] .",
      "startOffset" : 60,
      "endOffset" : 66
    } ],
    "year" : 2013,
    "abstractText" : "We introduce a proximal version of the stochastic dual coordinate ascent method and show how to accelerate the method using an inner-outer iteration procedure. We analyze the runtime of the framework and obtain rates that improve state-of-the-art results for various key machine learning optimization problems including SVM, logistic regression, ridge regression, Lasso, and multiclass SVM. Experiments validate our theoretical findings.",
    "creator" : "LaTeX with hyperref package"
  }
}