{
  "name" : "1403.3741.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Near-optimal Regret Bounds for Reinforcement Learning in Factored MDPs",
    "authors" : [ "Ian Osband", "Benjamin Van Roy" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n40 3.\n37 41\nv1 [\nst at\n.M L\n] 1\n5 M\nar 2\nAny learning algorithm over Markov decision processes (MDPs) will have worst-case regret Ω( √ SAT ) where T is the elapsed time and S and A are the cardinalities of the state and action spaces. In many settings of interest S and A may be so huge that it is impossible to guarantee good performance for an arbitrary MDP on any practical timeframe T . We show that, if we know the true system can be represented as a factored MDP, we can obtain regret bounds which scale polynomially in the number of parameters of the MDP, which may be exponentially smaller than S or A. Assuming an algorithm for approximate planning and knowledge of the graphical structure of the underlying MDP, we demonstrate that posterior sampling reinforcement learning (PSRL) and an algorithm based upon optimism in the face of uncertainty (UCRL-Factored) both satisfy near-optimal regret bounds."
    }, {
      "heading" : "1 Introduction",
      "text" : "The classic reinforcement learning problem considers an agent who must make sequential decisions within its environment while trying to maximize total reward accumulated over time [1, 2]. The environment is modeled as a Markov decision process (MDP) but the agent is uncertain of the true dynamics of the MDP. The agent must plan actions to maximize rewards based upon its imperfect knowledge, but also learns about its environment through experience. Efficient reinforcement learning manages this tradeoff between exploration and exploitation in such a way that the deviation from the optimal policy given perfect information is controlled.\nFactored MDPs [3] allow us to represent large structured MDPs compactly. A state is described by a selection of state variables, whose transitions can be represented by a dynamic Bayesian network (DBN) [4]. This is particularly beneficial when the transition of a state variable depends only on a small subset of other variables. For example, consider a large production line with m machines in sequence, each with K possible states. We write s = (s1, .., sm) with each si ∈ {1, ..,K}. It may be that, over a single time-step, machine i can only be influenced by the states of i − 1, i and i + 1. If so, any single si can still influence the entire system eventually, but the dimensionality of the learning problem is reduced exponentially from O(Km) to O(mK3).\nThere has been some success in establishing efficient reinforcement learning in factored MDPs (FMDPs). Kearns and Koller extend the E3 algorithm [5, 6] to exploit the DBN structure and obtain probably approximately correct (PAC) bounds with polynomial sample complexity. There are similar results available for the Rmax algorithm [7, 8] and even the greedy policy given an optimistic initialization [9]. These algorithms require the graph structure of the FMDP as a fixed prior. Some algorithms do seek to learn this structure from experience [10], but we will assume this structure is known.\nAnother form of efficiency guarantees for reinforcement learning are given by regret bounds. These bound the difference in accumulated rewards of a learning algorithm and the optimal policy over T steps [11]. Regret bounds naturally give rise to PAC bounds as a corollary, but also give a guarantee on the algorithm’s performance during the learning phase. Jaksch et al. [12] present UCRL2, which attains near-optimal regret of Õ(S √ AT ) with high probability. Recently Osband et al. [13] analyze PSRL, which also provides bounds on the expected regret of Õ(S √ AT ). Unlike the algorithms mentioned so far, PSRL does not use “optimism in the face of uncertainty” (OFU) to guide exploration, but instead the variance in posterior sampling. There has been no algorithm with efficient regret bounds in FMDPs so far.\nWe present two algorithms, PSRL and UCRL-Factored, with efficient regret bounds for FMDPs. These algorithms are described fully in Section 6. UCRL-Factored is a minor modification to UCRL2 that allows us to exploit the DBN structure while PSRL is unchanged. UCRL-Factored is guided by the OFU principle whereas PSRL is guided by posterior (also known as Thompson) sampling [14]. Posterior sampling has already been shown to perform extremely well in bandits [15, 16, 17] and non-factored MDPs [13]. We believe that posterior sampling will be simpler to implement, computationally cheaper and statistically more efficient than the optimistic alternative in FMDPs as well.\nBoth algorithms make use of approximate FMDP planner in internal steps. However, even where an FMDP is able to represented concisely, solving for the optimal policy may still be intractable in the most general case [18]. Our focus in this paper is upon the statistical aspect of the learning problem and like earlier discussions [5] we do not specify which computational methods are used. Our results serve as a reduction of the reinforcement learning problem to finding an approximate solution for a given FMDP. In many cases of interest, effective approximate planning methods for FMDPs do exist. Investigating and extending these methods are an ongoing subject of research [19, 20, 21, 22].\nWe believe that dimensionality reduction in large MDPs is essential for practical reinforcement learning. Factored MDPs are an approach with successful applications in many fields [3] but they are not the only one. There are regret bounds for continuous state spaces where the underlying reward and transition functions are known to belong to some function class, such as Höder continuous [23] or linear quadratic control [24]. These results are interesting, but each have some undesirable properties, the former has regret bounds which approach O(T ) for high dimensions and the latter retains an exponential dependence on the dimension. Perhaps the most popular approach in the literature is to assume the value function can be well-approximated by a low-dimensional (usually linear) representation of basis functions. Value-based approaches typically struggle to plan efficient exploration and so cannot obtain efficient learning guarantees, although there has been interesting progress in this field as well [25]."
    }, {
      "heading" : "2 Problem formulation",
      "text" : "We consider the problem of learning to optimize a random finite horizon MDP M = (S,A, RM , PM , τ, ρ) in repeated finite episodes of interaction. This is the same formulation as earlier work [13], which we reproduce here for completeness. S is the state space, A is the action space, RM (s, a) is a probability distribution over R when selecting action a while in state s, PM (s′|s, a) is the probability of transitioning to state s′ if action a is selected while at state s, τ is the time horizon, and ρ the initial state distribution. We define the MDP and all other random variables we will consider with respect to a probability space (Ω,F ,P).\nA deterministic policy µ is a function mapping each state s ∈ S and i = 1, . . . , τ to an action a ∈ A. For each MDP M = (S,A, RM , PM , τ, ρ) and policy µ, we define a value function\nV Mµ,i(s) := EM,µ\n\n\nτ ∑\nj=i\nR M (sj , aj)\n∣ ∣ ∣ si = s\n\n ,\nwhere R M (s, a) denotes the expected reward realized when action a is selected while in state s, and the subscripts of the expectation operator indicate that aj = µ(sj , j), and sj+1 ∼ PM (·|sj , aj) for j = i, . . . , τ . A policy µ is said to be optimal for MDP M if V Mµ,i(s) = maxµ′ V M µ′,i(s) for all s ∈ S and i = 1, . . . , τ . We will associate with each MDP M a policy µM that is optimal for M . The reinforcement learning agent interacts with the MDP over episodes that begin at times tk = (k− 1)τ +1, k = 1, 2, . . .. At each time t, the agent selects an action at, observes a scalar reward rt, and then transitions to st+1. If an agent follows a policy µ then when in state s at time t during episode k, it selects an action at = µ(s, t− tk). Let Ht = (s1, a1, r1, . . . , st−1, at−1, rt−1) denote the history of observations made prior to time t. A reinforcement learning algorithm is a deterministic sequence {πk|k = 1, 2, . . .} of functions, each mapping Htk to a probability distribution πk(Htk) over policies. At the start of the kth episode, the algorithm samples a policy µk from the distribution πk(Htk). The algorithm then selects actions at = µk(st, t− tk) at times t during the kth episode.\nWe define the regret incurred by a reinforcement learning algorithm π up to time T to be\nRegret(T, π,M∗) :=\n⌈T/τ⌉ ∑\nk=1\n∆k,\nwhere ∆k denotes regret over the kth episode, defined with respect to the MDP M ∗ by\n∆k := ∑\nS\nρ(s)(V M ∗ µ∗,1(s)− V M ∗ µk,1(s))\nwith µ∗ = µM ∗ and µk ∼ πk(Htk). Note that regret is not deterministic since it can depend on the random MDP M∗, the algorithm’s internal random sampling and, through the history Htk , on previous random transitions and random rewards. We will assess and compare algorithm performance in terms of regret and its expectation."
    }, {
      "heading" : "3 Factored MDPs",
      "text" : "To formalize our definition of a factored MDP we introduce some notation common to the literature [9].\nDefinition 1 ( Scope operation for factored sets X = X1 × ..×Xn). For any subset of indices Z ⊆ {1, 2, .., n} let us define the scope set X [Z] := ⊗\ni∈Z\nXi. Further, for any x ∈ X define\nthe scope variable x[Z] ∈ X [Z] to be the value of the variables xi ∈ Xi with indices i ∈ Z. For singleton sets Z we will write x[i] for x[{i}] in the natural way.\nLet PX ,Y be the set of functions mapping elements of a finite set X to probability mass functions over a finite set Y. PC,σX ,R will denote the set of functions mapping elements of a finite set X to σ-sub gaussian probability measures over (R,B(R)) with mean bounded in [0, C]. We will consider factored reward and factored transition functions which are drawn from within these families.\nDefinition 2 ( Factored reward functions R ∈ R ⊆ PC,σX ,R). The reward function class R is factored over X = X1 × .. × Xn with scopes Z1, ..Zl if and only if, for all R ∈ R, x ∈ X there exist functions {Ri ∈ PC,σX [Zi],R} l i=1 such that,\nE[r] = l ∑\ni=1\nE [ ri ]\nwhere the observed reward r ∼ R(x) is equal to ∑li=1 ri with each ri ∼ Ri(x[Zi]) and individually observed.\nDefinition 3 ( Factored transition functions P ∈ P ⊆ PX ,S ). The transition function class P is factored over X = X1 × ..× Xn and S = S1 × .. × Sm with scopes Z1, ..Zm if and only if, for all P ∈ P , x ∈ X , s ∈ S there exist some {Pi ∈ PX [Zi],Si}mi=1 such that,\nP (s|x) = m ∏\ni=1\nPi\n(\ns[i]\n∣ ∣ ∣ ∣ x[Zi] )\nA factored MDP (FMDP) is then defined to be an MDP with both factored rewards and factored transition functions. If we write X = S ×A, then an FMDP is fully characterized by the tuple\nM = ( {Si}mi=1; {Xi}ni=1; {ZRi }li=1; {Ri}li=1; {ZPi }mi=1; {Pi}mi=1; τ ; ρ ) ,\nwhere ZRi and Z P i are the scopes for the reward and transition functions respectively ⊆ {1, .., n} which refer to Xi. We assume that the size of all scopes |Zi| ≤ ζ ≪ n and factors |Xi| ≤ K so that the domains of Ri and Pi are of size at most Kζ ."
    }, {
      "heading" : "4 Results",
      "text" : "We present two algorithms, PSRL and UCRL-Factored with efficient regret bounds over factored MDPs. PSRL is guided by posterior sampling while UCRL-Factored uses optimism in the face of uncertainty. Full details of these algorithms are available in Section 6.\nOur first result shows that we can bound the expected regret of PSRL.\nTheorem 1 (Expected regret for PSRL in factored MDPs). Let M∗ be factored with graph structure G = (\n{Si}mi=1; {Xi}ni=1; {ZRi }li=1; {ZPi }mi=1; τ )\n. If φ is the distribution of M∗ and Ψ is the span of the optimal value function then we can bound the regret of PSRL:\nE [ Regret(T, πPSτ ,M ∗) ] ≤ 4 + 2 √ T + l ∑\ni=1\n{\n4(τC|X [ZRi ]|+ 1) + 8σ √ 2|X [ZRi ]|T log (4l|X [ZRi ]|kT ) }\n+E[Ψ] ( 1 + 4\nT − 4\n) m ∑\nj=1\n{\n4(τ |X [ZPj ]|+ 1) + 8 √ 2|X [ZPj ]||Sj |T log ( 4m|X [ZPj ]|kT )\n}\n(1)\nWe also show that using UCRL-Factored in a factored MDP we can bound the regret with high probability.\nTheorem 2 (High probability regret for UCRL-Factored in factored MDPs). Let M∗ be factored with graph structure G = (\n{Si}mi=1; {Xi}ni=1; {ZRi }li=1; {ZPi }mi=1; τ )\n. If D is the diameter of M∗, then for any M∗ can bound the regret of UCRL-Factored:\nRegret(T, πUCτ ,M ∗) ≤ CD\n√ 2T log(6/δ) + 2 √ T + l ∑\ni=1\n{\n4(τC|X [ZRi ]|+ 1) + 8σ √ 2|X [ZRi ]|T log (12l|X [ZRi ]|kT/δ) }\n+ CD\nm ∑\nj=1\n{\n4(τ |X [ZPj ]|+ 1) + 8 √ 2|X [ZPj ]||Sj |T log ( 12m|X [ZPj ]|kT/δ )\n}\n(2)\nwith probability at least 1− δ\nFor clarity, we present a symmetric problem instance for which we can produce a cleaner single-term upper bound. Let Q be shorthand for the structure G such that l + 1 = m, C = σ = 1, |Si| = |Xi| = K and |ZRi | = |ZPi | = ζ for all suitable i and write J = Kζ . In this case Ψ, D ≤ τ trivially.\nCorollary 1 (Clean bounds for PSRL in a symmetric problem). For an MDP with structure Q, if φ is the distribution of M∗ then we can bound the regret of PSRL:\nE [ Regret(T, πPSτ ,M ∗) ] ≤ 15mτ √ JKT log(2mJT ) (3)\nCorollary 2 (Clean bounds for UCRL-Factored in a symmetric problem). For an MDP with structure Q, then for any M∗ we can bound the regret of UCRL-Factored:\nRegret(T, πUCτ ,M ∗) ≤ 15mτ\n√\nJKT log(12mJT/δ) (4)\nwith probability at least 1− δ.\nThese simply follow from the theorems above with loose upper bounds upon constant and logarithmic factors. The derivations are available in the Appendix B. Both algorithms satisfy bounds of Õ(τm √ JKT ) whereas a Q-naive algorithm gives Õ(τ √ Jm/ζKmT ). We see that these new bounds are improved exponentially. These results are near optimal since for a factored MDP with m independent components with S states and A actions we obtain regret bounds Õ(mS √ AT ), which is close to the lower bound of Ω(m √ SAT )."
    }, {
      "heading" : "4.1 Interpretting regret bounds",
      "text" : "The bounds for PSRL and UCRL-Factored are qualitatively similar and share much of the same analysis. For each algorithm, the regret is Õ ( Ξ ∑m\nj=1\n√ |X [ZPj ]||Sj |T ) where Ξ is a measure of MDP connectedness, expected\nspan E[Ψ] for PSRL and scaled diameter CD for UCRL-Factored. The span of an MDP is defined Ψ(M∗) := maxs,s′∈S{VM ∗ µ∗,1(s) − VM ∗ µ∗,1(s ′)} which is the maximum difference in expected value of any two states under the optimal policy. The diameter of an MDP D(M∗) = maxs6=s′ minµ T µ s→s′ , where T µ s→s′ is the expected number of steps to get from s to s\n′ under policy µ. It is always the case that Ψ(M) ≤ CD(M), otherwise one might improve the optimal policy from s′ to follow simply by taking the fastest policy to the s with highest value. In some cases the span may be exponentially smaller than the diameter. In this sense PSRL satisfies a tighter bound.\nHowever, UCRL-Factored has stronger probabilistic guarantees than PSRL since its bounds hold with high probability for any MDP M∗ not just in expectation. There is an optimistic algorithm REGAL [26] which formally\nreplaces the UCRL2 D with Ψ and retains the high probability guarantees. However, no practical implementation of that algorithm exists, even when given access to an MDP planner. An analogous extension to the analysis of REGAL-Factored is possible.\nWe should also note that UCRL2 was designed to obtain regret bounds even in MDPs without episodic reset. This is accomplished by imposing artificial episodes which end whenever the number of visits to a state-action pair is doubled [12]. Using a similar modification, it is possible to extend UCRL-Factored to this setting without trouble and retain similar regret bounds. However, this doubling trick in PSRL does not retain provable regret bounds, since the episode length is no longer independent of the sampled MDP. Nevertheless, there has been good empirical performance using this method for non-factored MDPs without episodic reset in simulation [13]."
    }, {
      "heading" : "5 Confidence sets",
      "text" : "Our analysis will rely upon the construction of confidence sets based around the empirical estimates for the underlying reward and transition functions. These confidence sets are chosen so that at the beginning of any episode k the true and sampled functions are contained within the confidence set with high probability. We will then bound the deviation between the true function and elements in the confidence set by the maximal deviation within the confidence set. This technique is common to the literature and follows the same arguments as numerous previous papers on the subject [12, 26, 13].\nConsider a family of functions F ⊆ MX ,(Y,ΣY) which takes x ∈ X to a probability distribution over (Y,ΣY) measurable space. We will write this as MX ,Y unless we wish to stress the dependence on a particular σ-algebra which is not obvious.\nDefinition 4 (Set widths). Let X be a finite set, and let (Y,ΣY) be a measurable space. The width of a set F ∈ MX ,Y at x ∈ X with respect to a norm ‖ · ‖ is\nwF (x) := sup f,f∈F\n‖(f − f)(x)‖\nOur confidence set sequence {Ft ⊆ F : t ∈ N} is initialized with a set F . We adapt our confidence set to the observations yt ∈ Y which are drawn from the true function f∗ ∈ F at measurement points xt ∈ X so that yt ∼ f∗(xt). Each confidence set is then centered around an empirical estimate f̂t ∈ MX ,Y at time t, defined by\nf̂t(x) = 1\nnt(x)\n∑\nτ<t:xτ=x\nδyτ ,\nwhere nt(x) is the number of time x appears in (x1, .., xt−1) and δyt is the probability mass function over Y that assigns all probability to the outcome yt. If at any time t, nt(x) = 0 then we will let f̂t(x) be any arbitrary function ∈ MX ,Y . Our sequence of confidence sets depends on our choice of norm ‖ · ‖ and a non-decreasing sequence {dt : t ∈ N}. For each t, the confidence set is defined by:\nFt = Ft(‖ · ‖, xt−11 , dt) := { f ∈ F ∣ ∣ ∣\n∣\n‖(f − f̂t)(xi)‖ ≤ √\ndt nt(xi)\n∀i = 1, .., t− 1 } .\nWhere xt−11 is shorthand for (x1, .., xt−1) and we interpret nt(xi) = 0 as a null constraint which is satisfied ∀f ∈ F . The following result shows that we can bound the sum of confidence widths through time.\nTheorem 3 (Bounding the sum of widths). Let us write Fk for Ftk and associate times within episodes of length τ , t = tk + i for i = 1, .., τ and T = L× τ . For all finite sets X , measurable spaces (Y,ΣY), function classes F ⊆ MX ,Y with uniformly bounded widths wF (x) ≤ CF ∀x ∈ X and non-decreasing sequences {dt : t ∈ N}:\nL ∑\nk=1\nτ ∑\ni=1\nwFk(xtk+i) ≤ 4 ( τCF |X |+ 1 ) + 4 √ 2dT |X |T (5)\nProof. The proof follows from elementary considerations of nt(x) and the pigeonhole principle. We omit the details for brevity but refer the reader to Appendix A for a full derivation."
    }, {
      "heading" : "6 Algorithms",
      "text" : "Both algorithms require prior knowledge of G = ( {Si}mi=1; {Xi}ni=1; {ZRi }li=1; {ZPi }mi=1; τ )\n, the graphical structure of the FMDP. They also assume access to a “black box” that performs approximate dynamic programming for FMDPs. PSRL requires Γ(·, ǫ) which takes a single MDP M and output an ǫ-optimal policy for M . UCRLFactored requires Γ̃(·, ǫ) which takes in a family of MDPs M and outputs an ǫ-optimal with respect to the most optimistic M ∈ M. In general, it will be much more difficult to obtain an approximate solver Γ̃ than Γ.\nPSRL remains identical to earlier treatment [27, 13] provided G is encoded in the prior φ. UCRL-Factored is essentially UCRL2 [12] modified to exploit G in graph and episodic structure. We write Rit(dRit ) and Pjt (d Pj t ) as shorthand for these confidence sets Rit(|E[·]|, xt−11 [ZRi ], dRit ) and P it (‖ · ‖1, xt−11 [ZPj ], d Pj t ) generated from initial sets Ri1 = PC,σX [ZR i ],R and Pj1 = PX [ZPj ],Sj .\nAlgorithm 1 PSRL (Posterior Sampling)\n1: Input: Prior φ encoding G, t = 1 2: for episodes k = 1, 2, .. do 3: sample Mk ∼ φ(·|Ht) 4: compute µk = Γ(Mk, √\nτ/k) 5: for timesteps j = 1, .., τ do 6: sample and apply at = µk(st, j) 7: observe r1t , .., r l t and s 1 t+1, .., s m t+1 8: t = t+ 1 9: end for\n10: end for\nAlgorithm 2 UCRL-Factored (Optimism)\n1: Input: Graph structure G, confidence δ, t = 1 2: for episodes k = 1, 2, .. do 3: dRit = 4σ 2 log ( 4l|X [ZRi ]|k/δ ) for i = 1, .., l 4: d Pj t = 4|Sj | log ( 4m|X [ZPj ]|k/δ ) for j = 1, ..,m 5: Mk = {M |G, Ri ∈ Rit(dRit ), Pj ∈ Pjt (d Pj t ) ∀i, j} 6: compute µk = Γ̃(Mk, √\nτ/k) 7: for timesteps u = 1, .., τ do 8: sample and apply at = µk(st, u) 9: observe r1t , .., r l t and s 1 t+1, .., s m t+1\n10: t = t+ 1 11: end for\n12: end for\nThe parameters dRit and d Pj t are chosen to satisfy concentration inequalities so that the true MDP M ∗ lies within Mk for all k with high probability. Although PSRL makes no mention of confidence sets, Mk will also be useful in the analysis of PSRL."
    }, {
      "heading" : "7 Analysis",
      "text" : "We will now piece together the necessary analysis for our main results. First we recap the analysis of PSRL and UCRL2 which allow us to the regret to the bellman error. Next we show that, for factored MDPs, it is possible to bound this estimation error by the error in each factored component separately. From here we will use concentration inequalities upon the individual factors to show that, with high probability, the true MDP M∗ lies within Mk for all k. The final results will then be obtained through an application of Theorem 3."
    }, {
      "heading" : "7.1 From regret to Bellman error",
      "text" : "A key difficulty in providing regret bounds for reinforcement learning is that it depends upon the rewards of the optimal policy µ∗. For many reinforcement learning algorithms there is no clean way to relate the unknown optimal policy to the states and actions observed by the agent. Using the OFU principle, we can guarantee with high probability that the optimal rewards of the true MDP are upper bounded by the optimal rewards of the optimistic MDP [11]. In the case of posterior sampling, we make use of the posterior sampling lemma [17]\nLemma 1 (Posterior Sampling). If φ is the distribution of M∗ then, for any σ(Htk)-measurable function g,\nE[g(M∗)|Htk ] = E[g(Mk)|Htk ]. (6)\nNote that taking the expectation of (6) shows E[g(M∗)] = E[g(Mk)] through the tower property. We introduce the Bellman operator T Mµ , which for any MDP M = (S,A, RM , PM , τ, ρ), stationary policy µ : S → A and value function V : S → R, is defined by\nT Mµ V (s) := R M (s, µ(s)) + ∑\ns′∈S\nPM (s′|s, µ(s))V (s′).\nThis returns the expected value of state s where we follow the policy µ under the laws ofM , for one time step. The following lemma gives a concise form for the dynamic programming paradigm in terms of the Bellman operator.\nLemma 2 (Dynamic programming equation). For any MDP M = (S,A, RM , PM , τ, ρ) and policy µ : S × {1, . . . , τ} → A, the value functions V Mµ satisfy\nV Mµ,i = T Mµ(·,i)V Mµ,i+1 (7)\nfor i = 1 . . . τ , with V Mµ,τ+1 := 0.\nIn order to streamline our common analysis of PSRL and UCRL2 we will let M̃k refer generally to either the sampled MDP used in PSRL or the optimistic MDP chosen from Mk with associated near-optimal policy µ̃k). We will streamline our discussion of PM , RM , V Mµ,i and T Mµ by simply writing ∗ in place of M∗ or µ∗ and k in place of M̃k or µ̃k where appropriate; for example V ∗ k,i := V M∗ µ̃k,i . We will also write xk,i := (stk+i, µk(stk+i)).\nWe now break down the regret by adding and subtracting the imagined near optimal reward of policy µ̃K , which is known to the agent. For clarity of analysis we consider only the case of ρ(s′) = 1{s′ = s} but this changes nothing for our consideration of finite S.\n∆k = V ∗ ∗,1(s)− V ∗k,1(s) =\n( V kk,1(s)− V ∗k,1(s) ) + ( V ∗∗,1(s)− V kk,1(s) )\n(8)\nThe second term V ∗∗,1 − V ∗∗,1 relates the optimal rewards of the MDP M∗ to those near optimal for M̃k. We can bound this difference by √\n1/k for PSRL in expectation, and for UCRL-Factored with high probability. This follows for PSRL by Lemma 1, and for UCRL-Factored whenever the true MDP lies within the confidence set Mk by the principle of OFU and the approximate MDP planner Γ.\nWe decompose the first term through repeated applications of the dynamic programming equation,\n( V kk,1 − V ∗k,1 ) (stk+1) = τ ∑\ni=1\n( T kk,i − T ∗k,i ) V kk,i+1(stk+i) + τ ∑\ni=1\ndtk+1. (9)\nWhere dtk+i := ∑ s∈S\n{ P ∗(s|xk,i)(V ∗k,i+1 − V kk,i+1)(s) }\n− (V ∗k,i+1 − V kk,i+1)(stk+i). The second term captures the randomness in the transitions of the true MDP M∗ under policy µk. The expected value of (V ∗ k,i+1−V kk,i+1)(stk+i) given Htk+i is precisely ∑ s∈S { P ∗(s|xk,i)(V ∗k,i+1 − V kk,i+1)(s) } so that E[dtk+i] = 0 for all i. To obtain high probability bounds for UCRL-Factored we note that dtk+i martingale difference bounded by Ψk equal to the span of V kk,i. By the OFU principle we know that Ψk ≤ CD [12]. We apply the Azuma-Hoeffding inequality to say that:\nP\n(\nm ∑\nk=1\nτ ∑\ni=1\ndtk+i > CD √ 2T log(2/δ)\n)\n≤ δ (10)\nThe remaining term is the one step Bellman error of the imagined MDP M̃k. Crucially this term only depends on xk,i which are actually observed. We can now use the Hölder inequality to bound\nτ ∑\ni=1\n( T kk,i − T ∗k,i ) V kk,i+1(stk+i) ≤ τ ∑\ni=1\n|Rk(xk,i)− R ∗ (xk,i)|+\n1 2 Ψk‖P k(·|xk,i)− P ∗(·|xk,i)‖1 (11)"
    }, {
      "heading" : "7.2 Factorization decomposition",
      "text" : "So far our analysis has made no mention of the factorized structure of the MDP. We will now show how we can further bound equation (11) by the sums of errors in each factor of the reward and transition functions. It is quite clear that we may upper bound the deviations of R ∗ , R k by the sum of deviations of their factors using the triangle inequality. In fact, as we show in Lemma 3 we can also do this for the transition functions P ∗ and P k. This result really is the key insight for our results in this paper.\nLemma 3 (Bounding factored deviations). Let the transition function class P ⊆ PX ,S be factored over X = X1 × .. × Xn and S = S1 × .. × Sm with scopes Z1, ..Zm. Then, for any P, P̃ ∈ P we may bound their L1 distance by the sum of the differences of their factorizations:\n‖P (x)− P̃ (x)‖1 ≤ m ∑\ni=1\n‖Pi(x[Zi])− P̃i(x[Zi])‖1\nProof. In order to prove this lemma we begin with the simple claim that for any α1, α2, β1, β2 ∈ (0, 1]:\n|α1α2 − β1β2| = α2 ∣ ∣ ∣\n∣ α1 − β1β2 α2\n∣ ∣ ∣ ∣\n≤ α2 ( |α1 − β1|+ ∣ ∣ ∣\n∣ β1 − β1β2 α2\n∣ ∣ ∣ ∣ )\n≤ α2 |α1 − β1|+ β1 |α2 − β2|\nThis result also holds for any α1, α2, β1, β2 ∈ [0, 1], where including 0 can be verified case by case. We now consider the probability distributions p, p̃ over {1, .., d1} and q, q̃ over {1, .., d2}. We let Q = pqT , Q̃ = p̃q̃T be the joint probability distribution over {1, .., d1} × {1, .., d2}. Using the claim above we will be able to bound the L1 deviation ‖Q− Q̃‖1 by the deviations of their factors:\n‖Q− Q̃‖1 = d1 ∑\ni=1\nd2 ∑\nj=1\n|piqj − p̃iq̃j |\n≤ d1 ∑\ni=1\nd2 ∑\nj=1\nqj |pi − p̃i|+ p̃i|qj − q̃j |\n= ‖p− p̃‖1 + ‖q − q̃‖1\nApplying this result m times to the factored transition functions P and P̃ we recover our desired result."
    }, {
      "heading" : "7.3 Concentration guarantees for Mk",
      "text" : "We now want to show that the true MDP lies within Mk with high probability. Note that posterior sampling will also allow us to then say that the sampled Mk is within Mk with high probability too. In order to show this, we first present a concentration result for the L1 deviation of empirical probabilities.\nLemma 4 (L1 bounds for the empirical transition function). For all finite sets X , finite sets Y, function classes P ⊆ PX ,Y then for any x ∈ X , ǫ > 0:\nP\n( ‖P ∗(x)− P̂t(x)‖1 ≥ ǫ ) ≤ exp ( |Y| log(2)− nt(x)ǫ 2\n2\n)\nProof. This is a relaxation of the result proved by Weissman [28].\nLemma 4 ensures that for any x ∈ X , j = 1, ..,m P ( ‖P ∗j (x) − P̂jt(x)‖1 ≥ √ 2|Sj| nt(x) log ( 2 δ′ ) ) ≤ δ′. We then\ndefine d Pj tk = 2|Si| log\n(\n2/δ′k,j\n)\nwith δ′k,j = δ/(2m|X [ZPj ]|k2). Now using a union bound, together with the fact that\n∑∞ n=1 1/n 2 = π2/6 < 2:\nP\n(\nP ∗j ∈ Pjt (d Pj tk ) ∀k ∈ N, j = 1, ..,m\n)\n≥ 1− δ\nThe proof for sub σ-gaussian random variables follows from the definition and resultant tail bounds. ǫ ∈ R is a sub σ-gaussian random variable ⇐⇒ ∀t ∈ R, E [exp(tǫ)] ≤ exp ( σ2t2\n2\n)\n.\nLemma 5 (Tail bounds for sub σ-gaussian random variables). If {ǫi} are all independent and sub σ-gaussian then ∀β ≥ 0: P ( 1 n | ∑n i=1 ǫi| > β ) ≤ exp ( log(2)− nβ22σ2 ) .\nLemma 5 ensures that for any x ∈ X , i = 1, .., l P ( |R∗i (x)− R̂it(x)| ≥ √ 2σ2\nnt(x) log\n(\n2 δ′\n)\n)\n≤ δ′. A similar\nargument from here ensures that: P ( R ∗ i ∈ Rit(dRitk ) ∀k ∈ N, i = 1, .., l )\n≥ 1 − δ. This allows us to present our important result that\nP\n( M∗ ∈ Mk ∀k ∈ N ) ≥ 1− 2δ (12)"
    }, {
      "heading" : "7.4 Regret bounds",
      "text" : "We now have all the necessary intermediate results to complete our proof. We begin with the analysis of PSRL. Using equation (12) and the posterior sampling lemma we can say that P(M∗,Mk ∈ Mk∀k ∈ N) ≥ 1− 4δ. The contributions from near-optimal regret in planning function Γ are bounded by\n∑L k=1\n√ τ/k ≤ 2 √ T . From here\nwe take equation (11), Lemma 3 and Theorem 3 to say that for any δ > 0:\nE [ Regret(T, πPSτ ,M ∗) ]\n≤ 4δT + 2 √ T + l ∑\ni=1\n{\n4(τC|X [ZRi ]|+ 1) + 4 √ 2dRiT |X [ZRi ]|T }\n+ sup k=1,..,L\n( E[Ψk|Mk,M∗ ∈ Mk] ) × m ∑\nj=1\n{\n4(τ |X [ZPj ]|+ 1) + 4 √ 2d Pj T |X [ZPj ]|T\n}\nLet A = {M∗,Mk ∈ Mk}, since Ψk ≥ 0 and E[Ψk] = E[Ψ] via posterior sampling we can say that for all k:\nE[Ψk|A] ≤ P(A)−1E[Ψ] ≤ ( 1− 4δ k2\n)−1\nE[Ψ] =\n(\n1 + 4δ\nk2 − 4δ\n) E[Ψ] ≤ ( 1 + 4δ\n1− 4δ\n)\nE[Ψ].\nPlugging in the values of dRiT and d Pj T and setting δ = 1/T completes the proof of Theorem 1\nThe analysis of UCRL-Factored and Theorem 2 follows in a similar manner. We use equation (10) to bound the contributions of dt with probability 1− δ, and equation (12) to bound missed confidence regions. From here we take equation (11), Lemma 3 and Theorem 3 to say that, with probability ≥ 1− 3δ:\nRegret(T, πUCτ ,M ∗) ≤ CD\n√ 2T log(2/δ) + 2 √ T + l ∑\ni=1\n{\n4(τC|X [ZRi ]|+ 1) + 4 √ 2dRiT |X [ZRi ]|T }\n+ CD × m ∑\nj=1\n{\n4(τ |X [ZPj ]|+ 1) + 4 √ 2d Pj T |X [ZPj ]|T\n}\nand 2. The Corollaries 1 and 2 follow from simply plugging in these values in the symmetric case and upper bounding the constant and logarithmic terms. This is presented in more detail in Appendix B."
    }, {
      "heading" : "A Bounding the widths of confidence sets",
      "text" : "We present elementary arguments which culminate in a proof of Theorem 3.\nLemma 6 (Concentration results for √\ndT /nt(x)). For all finite sets X and any dT , ǫ ≥ 0:\nT ∑\nt=1\n1\n{\n√ dT /nt(xt) > h(dT , ǫ) } ≤ T ∑\nt=1\n1\n{\n√ dT /nt(xt) > ǫ } + |X |,\nWhere h(dT , ǫ) := √ dT ǫ2/(dT + ǫ2).\nProof. Let (xs1 , .., xsK ) be the largest subsequence of x T 1 such that\n√\ndT /nsi(xsi) ∈ (h(dT , ǫ), ǫ] ∀i. Now for any x ∈ X , let Tx = {si | xsi = x}. Suppose there exist two distinct elements σ, ρ ∈ Tx with σ < ρ so that nρ(x) ≥ nσ(x) + 1. We note that for any n ∈ R+, h(dT , √ dT /n) = √ dT /(n+ 1) so that:\nǫ ≥ √ dT /nσ(x) =⇒ h(dT , ǫ) ≥ √ dT /(nσ(x) + 1) ≥ √ dT /nρ(x)\nThis contradicts our assumption √\ndT /nρ(x) ∈ (h(d, ǫ), ǫ] and so we must conclude that |Tx| ≤ 1 for all x ∈ X . This means that (xs1 , .., xsK ) forms a subsequence of unique elements in X , the total length of which must be bounded by |X |.\nWe now provide a corollary of this result which allows for episodic delays in updating visit counts nt(x). We imagine that we will only update our counts every τ steps.\nCorollary 3 (Concentration results for √\ndT /ntk (x) in the episodic setting). Let us associate times within episodes of length τ , t = tk + i for i = 1, .., τ and T = M × τ . For all finite sets X and any dT , ǫ ≥ 0:\nM ∑\nk=1\nτ ∑\ni=1\n1\n{\n√\ndT /ntk (xtk+i) > h (τ)(dT , ǫ)\n} ≤ M ∑\nk=1\nτ ∑\ni=1\n1\n{\n√ dT /ntk (xtk+i) > ǫ } + 2τ |X |,\nWhere h(τ)(dT , ǫ) is the τ -fold composition of h(dT , ·) acting on ǫ. Proof. By an argument of visiting times similar to lemma 6 we can see that the worst case scenario for the episodic case ∑M\nk=1\n∑τ\ni=1 1\n{\n√\ndT /ntk (xtk+i) > h (τ)(dT , ǫ)\n}\nis to visit each x exactly τ − 1 times before the start of an episode, and then spend the entirety of the following episode within the state. Here we have upper bounded 2τ − 1 by 2τ and |X | − 1 by |X | to complete our result.\nIt will be useful to define notion of radius for each confidence set at each x ∈ X , rFt(x) := supf∈Ft ‖(f − f̂t)(x)‖. By the triangle inequality, we have wFt(x) ≤ 2rFt(x) for all x ∈ X . Lemma 7 (Bounding the number of large radii). Let us write Fk for Ftk and associate times within episodes of length τ , t = tk + i for i = 1, .., τ and T = M × τ . For all finite sets X , measurable spaces (Y,ΣY), function classes F ⊆ MX ,Y , non-decreasing sequences {dt : t ∈ N}, any T ∈ N and ǫ > 0:\nM ∑\nk=1\nτ ∑\ni=1\n1{rFk(xtk+i) > ǫ} < ( dT τǫ2 + 1 ) 2τ |X |\nProof. By construction of Ft and noting that dt is non-decreasing in t, we can say that rFk(xt) ≤ √\ndT /ntk(xt) for all t = 1, .., T so that\nM ∑\nk=1\nτ ∑\ni=1\n1{rFk(xt+k+1) > ǫ} ≤ M ∑\nk=1\nτ ∑\ni=1\n1\n{\n√ dT /ntk (xtk+i) > ǫ } .\nNow let g(ǫ) = √\ndT ǫ2/(dT − τǫ2) be the ǫ-inverse of h(τ)(dT , ǫ) such that g(h(τ)(dT , ǫ)) = ǫ. Applying Corollary 3 to our expression n times repeatedly we can say:\nM ∑\nk=1\nτ ∑\ni=1\n1\n{\n√ dT /ntk (xtk+i) > ǫ } ≤ M ∑\nk=1\nτ ∑\ni=1\n1\n{\n√\ndT /ntk (xtk+i) > g (n)(ǫ)\n}\n+ 2nτ |X |.\nWhere g(n)(ǫ) denotes the composition of g(·) n-times acting on ǫ. If we take n to be the lowest integer such that g(n)(ǫ) > √ dT /τ then, ∑M\nk=1\n∑τ\ni=1 1\n{\n√\ndT /ntk(xtk+i) > g (n)(ǫ)\n}\n≤ 2τ |X | so that the whole expression is bounded by (n+ 1) 2τ |X |. Note that for all N ∈ R+, g( √ dT /N) = √ dT /(N − τ ), if we write ǫ = √\ndT /N1 then n ≤ N1/τ = dTτǫ2 , which completes the proof.\nUsing these results we are finally able to complete our proof of Theorem 3 We first note that, via the triangle inequality ∑M\nk=1\n∑τ\ni=1 wFk (xtk+i) ≤ 2\n∑M\nk=1\n∑τ\ni=1 rFk(xtk+i). We streamline our notation by letting rk,i = rFk(xtk+i). Reordering\nthe sequence (r1,1, .., rM,τ ) → (ri1 , .., riT ) such ri1 ≥ .. ≥ riT we have that: M ∑\nk=1\nτ ∑\ni=1\nrFk(xtk+i) =\nT ∑\nt=1\nrit ≤ 1 + T ∑\ni=1\nrit1{rit ≥ T−1}.\nWe can see that rit > ǫ ≥ T−1 ⇐⇒ ∑T i=1 1{rit ≥ ǫ} ≥ t. From Lemma 7 this means that t ≤\n(\ndT τǫ2 + 1 ) 2τ |X |, so that ǫ ≤ √\n2|X|dT t−2τ |X| . This means that rit ≤ min{CF , √ 2|X|dT t−2τ |X| }. Therefore,\nT ∑\ni=1\nrit1{rit ≥ T−1} ≤ 2τCF |X |+ T ∑\nt=2τ |X|+1\n√\n2dT |X | t− τ |X |\n≤ 2τCF |X |+ ∫ T\n0\n√\n2dT |X | t dt\n≤ 2τCF |X |+ 2 √ 2dT |X |T\nWhich completes the proof of Theorem 3."
    }, {
      "heading" : "B Clean bounds for the symmetric problem",
      "text" : "We now provide concrete clean upper bounds for Theorems 1 and 2 in the simple symmetric case l + 1 = m, C = σ = 1, |Si| = |Xi| = K and |ZRi | = |ZPi | = ζ for all suitable i and write J = Kζ . For a non-trivial problem setting we assume that K ≥ 2, m ≥ 2, τ ≥ 2.\nFrom Section 7.4 we have that\nE\n[ Regret(T, πPSτ ,M ∗) ] ≤ 4 + 2 √ T +m { 4(τJ + 1) + 4 √ 8 log(4mJT 2/τ )JT }\n+ E[Ψ] ( 1 + 4\nT − 4\n) m { 4(τJ + 1) + 4 √ 8K log(4mJT 2/τ )JT }\nThrough looking at the constant term we know that the bounds are trivially satisfied for all T ≤ 56, from here we can certainly upper bound 4/(T − 4) ≤ 1/13. From here we can say that:\nE [ Regret(T, πPSτ ,M ∗) ] ≤ { 4 + 4m ( 1 + 14\n13 E[Ψ]\n) (τJ + 1) } + √ T { 2 + 4 √ 8J log(4mJT 2/τ ) + 4 √ 8JK log(4mJT 2/τ ) 14\n13 E[Ψ]\n}\n≤ 5 (1 +E[Ψ])mτJ + √ T { 12 √ J log(2mJT ) + 12E[Ψ] √ JK log(2mJT ) }\n≤ 5 (1 +E[Ψ])mτJ + 12m ( 1 +E[Ψ] √ K ) √ JT log(2mJT )\n≤ min(5mτ 2J, T ) + 12mτ √ JKT log(2mJT )\n≤ 15mτ √ JKT log(2mJT )\nWhere in the last steps we have used that Ψ ≤ τ and min(a, b) ≤ √ ab. We now repeat a similar procedure of upper bounds for UCRL-Factored, immediately replicating D by τ in our analysis to say that with probability ≥ 1− 3δ:\nRegret(T, πUCτ ,M ∗) ≤ τ\n√ 2T log(2/δ) + 2 √ T +m { 4(τJ + 1) + 4 √ 8 log(4mJT/δ)JT }\n+ τm { 4(τJ + 1) + 4 √ 8K log(4mJT/δ)JT }\n≤ (1 + τ )m4(τJ + 1) + √ T { τ √ 2 log(2/δ) + 2 +m4 √ 8 log(4mJT/δ)J + τm4 √ 8 log(4mJT/δ)JK } ≤ 5(1 + τ )mτJ + 12m(1 + τ √ K) √ JT log(4mJT/δ)\n≤ 15mτ √ JKT log(4mJT/δ)\nWhere in the last step we used a similar argument"
    } ],
    "references" : [ {
      "title" : "Optimal adaptive policies for Markov decision processes",
      "author" : [ "A.N. Burnetas", "M.N. Katehakis" ],
      "venue" : "Mathematics of Operations Research,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 1997
    }, {
      "title" : "Stochastic systems: estimation, identification and adaptive control",
      "author" : [ "P.R. Kumar", "P. Varaiya" ],
      "venue" : null,
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 1986
    }, {
      "title" : "Stochastic dynamic programming with factored representations",
      "author" : [ "Craig Boutilier", "Richard Dearden", "Moisés Goldszmidt" ],
      "venue" : "Artificial Intelligence,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2000
    }, {
      "title" : "Learning dynamic bayesian networks. In Adaptive processing of sequences and data structures, pages 168–197",
      "author" : [ "Zoubin Ghahramani" ],
      "venue" : null,
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 1998
    }, {
      "title" : "Efficient reinforcement learning in factored MDPs",
      "author" : [ "Michael Kearns", "Daphne Koller" ],
      "venue" : "In IJCAI,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 1999
    }, {
      "title" : "Near-optimal reinforcement learning in polynomial time",
      "author" : [ "M. Kearns", "S. Singh" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2002
    }, {
      "title" : "Model-based reinforcement learning in factored-state MDPs",
      "author" : [ "Alexander Strehl" ],
      "venue" : "In Approximate Dynamic Programming and Reinforcement Learning,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2007
    }, {
      "title" : "R-max-a general polynomial time algorithm for near-optimal reinforcement learning",
      "author" : [ "R.I. Brafman", "M. Tennenholtz" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2003
    }, {
      "title" : "Optimistic initialization and greediness lead to polynomial time learning in factored MDPs",
      "author" : [ "István Szita", "András Lőrincz" ],
      "venue" : "In Proceedings of the 26th Annual International Conference on Machine Learning,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2009
    }, {
      "title" : "Efficient structure learning in factored-state MDPs",
      "author" : [ "Alexander Strehl", "Carlos Diuk", "Michael Littman" ],
      "venue" : "In AAAI,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2007
    }, {
      "title" : "Reinforcement learning: A survey",
      "author" : [ "Leslie Pack Kaelbling", "Michael L Littman", "Andrew W Moore" ],
      "venue" : "arXiv preprint cs/9605103,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 1996
    }, {
      "title" : "Near-optimal regret bounds for reinforcement learning",
      "author" : [ "Thomas Jaksch", "Ronald Ortner", "Peter Auer" ],
      "venue" : "The Journal of Machine Learning Research,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2010
    }, {
      "title" : "More) Efficient Reinforcement Learning via Posterior Sampling",
      "author" : [ "Ian Osband", "Daniel Russo", "Benjamin Van Roy" ],
      "venue" : "Advances in Neural Information Processing Systems,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2013
    }, {
      "title" : "On the likelihood that one unknown probability exceeds another in view of the evidence of two samples",
      "author" : [ "W.R. Thompson" ],
      "venue" : null,
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 1933
    }, {
      "title" : "An empirical evaluation of Thompson sampling",
      "author" : [ "O. Chapelle", "L. Li" ],
      "venue" : "In Neural Information Processing Systems (NIPS),",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2011
    }, {
      "title" : "Thompson sampling: an asymptotically optimal finite time analysis",
      "author" : [ "E. Kauffmann", "N. Korda", "R. Munos" ],
      "venue" : "In International Conference on Algorithmic Learning Theory,",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2012
    }, {
      "title" : "Learning to optimize via posterior sampling",
      "author" : [ "D. Russo", "B. Van Roy" ],
      "venue" : "CoRR, abs/1301.2609,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2013
    }, {
      "title" : "Efficient solution algorithms for factored MDPs",
      "author" : [ "Carlos Guestrin", "Daphne Koller", "Ronald Parr", "Shobha Venkataraman" ],
      "venue" : "J. Artif. Intell. Res.(JAIR),",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2003
    }, {
      "title" : "Policy iteration for factored MDPs",
      "author" : [ "Daphne Koller", "Ronald Parr" ],
      "venue" : "In Proceedings of the Sixteenth conference on Uncertainty in artificial intelligence,",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2000
    }, {
      "title" : "Max-norm projections for factored MDPs",
      "author" : [ "Carlos Guestrin", "Daphne Koller", "Ronald Parr" ],
      "venue" : "In IJCAI,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2001
    }, {
      "title" : "Efficient solutions to factored MDPs with imprecise transition probabilities",
      "author" : [ "Karina Valdivia Delgado", "Scott Sanner", "Leliane Nunes De Barros" ],
      "venue" : "Artificial Intelligence,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2011
    }, {
      "title" : "Approximate linear programming for first-order MDPs",
      "author" : [ "Scott Sanner", "Craig Boutilier" ],
      "venue" : "arXiv preprint arXiv:1207.1415,",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2012
    }, {
      "title" : "Online regret bounds for undiscounted continuous reinforcement learning",
      "author" : [ "Ronald Ortner", "Daniil Ryabko" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2012
    }, {
      "title" : "Regret bounds for the adaptive control of linear quadratic systems",
      "author" : [ "Yasin Abbasi-Yadkori", "Csaba Szepesvári" ],
      "venue" : "Journal of Machine Learning Research-Proceedings Track,",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2011
    }, {
      "title" : "Generalization and exploration via randomized value functions",
      "author" : [ "Benjamin Van Roy", "Zheng Wen" ],
      "venue" : "arXiv preprint arXiv:1402.0635,",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2014
    }, {
      "title" : "Regal: A regularization based algorithm for reinforcement learning in weakly communicating MDPs",
      "author" : [ "P.L. Bartlett", "A. Tewari" ],
      "venue" : "In Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence,",
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 2009
    }, {
      "title" : "A Bayesian framework for reinforcement learning",
      "author" : [ "M. Strens" ],
      "venue" : "In Proceedings of the 17th International Conference on Machine Learning,",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2000
    } ],
    "referenceMentions" : [ {
      "referenceID" : 0,
      "context" : "1 Introduction The classic reinforcement learning problem considers an agent who must make sequential decisions within its environment while trying to maximize total reward accumulated over time [1, 2].",
      "startOffset" : 195,
      "endOffset" : 201
    }, {
      "referenceID" : 1,
      "context" : "1 Introduction The classic reinforcement learning problem considers an agent who must make sequential decisions within its environment while trying to maximize total reward accumulated over time [1, 2].",
      "startOffset" : 195,
      "endOffset" : 201
    }, {
      "referenceID" : 2,
      "context" : "Factored MDPs [3] allow us to represent large structured MDPs compactly.",
      "startOffset" : 14,
      "endOffset" : 17
    }, {
      "referenceID" : 3,
      "context" : "A state is described by a selection of state variables, whose transitions can be represented by a dynamic Bayesian network (DBN) [4].",
      "startOffset" : 129,
      "endOffset" : 132
    }, {
      "referenceID" : 4,
      "context" : "Kearns and Koller extend the E algorithm [5, 6] to exploit the DBN structure and obtain probably approximately correct (PAC) bounds with polynomial sample complexity.",
      "startOffset" : 41,
      "endOffset" : 47
    }, {
      "referenceID" : 5,
      "context" : "Kearns and Koller extend the E algorithm [5, 6] to exploit the DBN structure and obtain probably approximately correct (PAC) bounds with polynomial sample complexity.",
      "startOffset" : 41,
      "endOffset" : 47
    }, {
      "referenceID" : 6,
      "context" : "There are similar results available for the Rmax algorithm [7, 8] and even the greedy policy given an optimistic initialization [9].",
      "startOffset" : 59,
      "endOffset" : 65
    }, {
      "referenceID" : 7,
      "context" : "There are similar results available for the Rmax algorithm [7, 8] and even the greedy policy given an optimistic initialization [9].",
      "startOffset" : 59,
      "endOffset" : 65
    }, {
      "referenceID" : 8,
      "context" : "There are similar results available for the Rmax algorithm [7, 8] and even the greedy policy given an optimistic initialization [9].",
      "startOffset" : 128,
      "endOffset" : 131
    }, {
      "referenceID" : 9,
      "context" : "Some algorithms do seek to learn this structure from experience [10], but we will assume this structure is known.",
      "startOffset" : 64,
      "endOffset" : 68
    }, {
      "referenceID" : 10,
      "context" : "These bound the difference in accumulated rewards of a learning algorithm and the optimal policy over T steps [11].",
      "startOffset" : 110,
      "endOffset" : 114
    }, {
      "referenceID" : 11,
      "context" : "[12] present UCRL2, which attains near-optimal regret of Õ(S √ AT ) with high probability.",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 12,
      "context" : "[13] analyze PSRL, which also provides bounds on the expected regret of Õ(S √ AT ).",
      "startOffset" : 0,
      "endOffset" : 4
    }, {
      "referenceID" : 13,
      "context" : "UCRL-Factored is guided by the OFU principle whereas PSRL is guided by posterior (also known as Thompson) sampling [14].",
      "startOffset" : 115,
      "endOffset" : 119
    }, {
      "referenceID" : 14,
      "context" : "Posterior sampling has already been shown to perform extremely well in bandits [15, 16, 17] and non-factored MDPs [13].",
      "startOffset" : 79,
      "endOffset" : 91
    }, {
      "referenceID" : 15,
      "context" : "Posterior sampling has already been shown to perform extremely well in bandits [15, 16, 17] and non-factored MDPs [13].",
      "startOffset" : 79,
      "endOffset" : 91
    }, {
      "referenceID" : 16,
      "context" : "Posterior sampling has already been shown to perform extremely well in bandits [15, 16, 17] and non-factored MDPs [13].",
      "startOffset" : 79,
      "endOffset" : 91
    }, {
      "referenceID" : 12,
      "context" : "Posterior sampling has already been shown to perform extremely well in bandits [15, 16, 17] and non-factored MDPs [13].",
      "startOffset" : 114,
      "endOffset" : 118
    }, {
      "referenceID" : 17,
      "context" : "However, even where an FMDP is able to represented concisely, solving for the optimal policy may still be intractable in the most general case [18].",
      "startOffset" : 143,
      "endOffset" : 147
    }, {
      "referenceID" : 4,
      "context" : "Our focus in this paper is upon the statistical aspect of the learning problem and like earlier discussions [5] we do not specify which computational methods are used.",
      "startOffset" : 108,
      "endOffset" : 111
    }, {
      "referenceID" : 18,
      "context" : "Investigating and extending these methods are an ongoing subject of research [19, 20, 21, 22].",
      "startOffset" : 77,
      "endOffset" : 93
    }, {
      "referenceID" : 19,
      "context" : "Investigating and extending these methods are an ongoing subject of research [19, 20, 21, 22].",
      "startOffset" : 77,
      "endOffset" : 93
    }, {
      "referenceID" : 20,
      "context" : "Investigating and extending these methods are an ongoing subject of research [19, 20, 21, 22].",
      "startOffset" : 77,
      "endOffset" : 93
    }, {
      "referenceID" : 21,
      "context" : "Investigating and extending these methods are an ongoing subject of research [19, 20, 21, 22].",
      "startOffset" : 77,
      "endOffset" : 93
    }, {
      "referenceID" : 2,
      "context" : "Factored MDPs are an approach with successful applications in many fields [3] but they are not the only one.",
      "startOffset" : 74,
      "endOffset" : 77
    }, {
      "referenceID" : 22,
      "context" : "There are regret bounds for continuous state spaces where the underlying reward and transition functions are known to belong to some function class, such as Höder continuous [23] or linear quadratic control [24].",
      "startOffset" : 174,
      "endOffset" : 178
    }, {
      "referenceID" : 23,
      "context" : "There are regret bounds for continuous state spaces where the underlying reward and transition functions are known to belong to some function class, such as Höder continuous [23] or linear quadratic control [24].",
      "startOffset" : 207,
      "endOffset" : 211
    }, {
      "referenceID" : 24,
      "context" : "Value-based approaches typically struggle to plan efficient exploration and so cannot obtain efficient learning guarantees, although there has been interesting progress in this field as well [25].",
      "startOffset" : 191,
      "endOffset" : 195
    }, {
      "referenceID" : 12,
      "context" : "This is the same formulation as earlier work [13], which we reproduce here for completeness.",
      "startOffset" : 45,
      "endOffset" : 49
    }, {
      "referenceID" : 8,
      "context" : "3 Factored MDPs To formalize our definition of a factored MDP we introduce some notation common to the literature [9].",
      "startOffset" : 114,
      "endOffset" : 117
    }, {
      "referenceID" : 25,
      "context" : "There is an optimistic algorithm REGAL [26] which formally 4",
      "startOffset" : 39,
      "endOffset" : 43
    }, {
      "referenceID" : 11,
      "context" : "This is accomplished by imposing artificial episodes which end whenever the number of visits to a state-action pair is doubled [12].",
      "startOffset" : 127,
      "endOffset" : 131
    }, {
      "referenceID" : 12,
      "context" : "Nevertheless, there has been good empirical performance using this method for non-factored MDPs without episodic reset in simulation [13].",
      "startOffset" : 133,
      "endOffset" : 137
    }, {
      "referenceID" : 11,
      "context" : "This technique is common to the literature and follows the same arguments as numerous previous papers on the subject [12, 26, 13].",
      "startOffset" : 117,
      "endOffset" : 129
    }, {
      "referenceID" : 25,
      "context" : "This technique is common to the literature and follows the same arguments as numerous previous papers on the subject [12, 26, 13].",
      "startOffset" : 117,
      "endOffset" : 129
    }, {
      "referenceID" : 12,
      "context" : "This technique is common to the literature and follows the same arguments as numerous previous papers on the subject [12, 26, 13].",
      "startOffset" : 117,
      "endOffset" : 129
    }, {
      "referenceID" : 26,
      "context" : "PSRL remains identical to earlier treatment [27, 13] provided G is encoded in the prior φ.",
      "startOffset" : 44,
      "endOffset" : 52
    }, {
      "referenceID" : 12,
      "context" : "PSRL remains identical to earlier treatment [27, 13] provided G is encoded in the prior φ.",
      "startOffset" : 44,
      "endOffset" : 52
    }, {
      "referenceID" : 11,
      "context" : "UCRL-Factored is essentially UCRL2 [12] modified to exploit G in graph and episodic structure.",
      "startOffset" : 35,
      "endOffset" : 39
    }, {
      "referenceID" : 10,
      "context" : "Using the OFU principle, we can guarantee with high probability that the optimal rewards of the true MDP are upper bounded by the optimal rewards of the optimistic MDP [11].",
      "startOffset" : 168,
      "endOffset" : 172
    }, {
      "referenceID" : 16,
      "context" : "In the case of posterior sampling, we make use of the posterior sampling lemma [17] Lemma 1 (Posterior Sampling).",
      "startOffset" : 79,
      "endOffset" : 83
    }, {
      "referenceID" : 11,
      "context" : "By the OFU principle we know that Ψk ≤ CD [12].",
      "startOffset" : 42,
      "endOffset" : 46
    }, {
      "referenceID" : 0,
      "context" : "≤ α2 |α1 − β1|+ β1 |α2 − β2| This result also holds for any α1, α2, β1, β2 ∈ [0, 1], where including 0 can be verified case by case.",
      "startOffset" : 77,
      "endOffset" : 83
    } ],
    "year" : 2017,
    "abstractText" : "Any learning algorithm over Markov decision processes (MDPs) will have worst-case regret Ω( √ SAT ) where T is the elapsed time and S and A are the cardinalities of the state and action spaces. In many settings of interest S and A may be so huge that it is impossible to guarantee good performance for an arbitrary MDP on any practical timeframe T . We show that, if we know the true system can be represented as a factored MDP, we can obtain regret bounds which scale polynomially in the number of parameters of the MDP, which may be exponentially smaller than S or A. Assuming an algorithm for approximate planning and knowledge of the graphical structure of the underlying MDP, we demonstrate that posterior sampling reinforcement learning (PSRL) and an algorithm based upon optimism in the face of uncertainty (UCRL-Factored) both satisfy near-optimal regret bounds.",
    "creator" : "LaTeX with hyperref package"
  }
}