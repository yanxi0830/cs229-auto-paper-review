{
  "name" : "1201.2416.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Stochastic Low-Rank Kernel Learning for Regression",
    "authors" : [ "Pierre Machart", "Thomas Peel", "Sandrine Anthoine", "Liva Ralaivola", "Hervé Glotin" ],
    "emails" : [ "PIERRE.MACHART@LIF.UNIV-MRS.FR", "THOMAS.PEEL@LIF.UNIV-MRS.FR", "ANTHOINE@CMI.UNIV-MRS.FR", "LIVA.RALAIVOLA@LIF.UNIV-MRS.FR", "GLOTIN@UNIV-TLN.FR" ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "Our goal is to learn a kernel-based regression function, tackling at once two problems that commonly arise with kernel methods: working with a kernel tailored to the task at hand and efficiently handling problems whose size prevents the Gram matrix from being stored in memory. Though the present work focuses on regression, the material presented here might as well apply to classification.\nCompared with similar methods, we introduce two novelties. Firstly, we build conical combinations of rank-1\nAppearing in Proceedings of the 28 th International Conference on Machine Learning, Bellevue, WA, USA, 2011. Copyright 2011 by the author(s)/owner(s).\nNyström approximations, whose weights are chosen so as to serve the regression task – this makes our approach different from (Kumar et al., 2009) and (Suykens et al., 2002), which focus on approximating the full Gram matrix with no concern for any specific learning task. Secondly, to solve the convex optimization problem entailed by our modeling choice, we provide an original stochastic optimization procedure based on (Nesterov, 2010). It has the following characteristics: i) the computations of the updates are inexpensive (thanks to the designing choice of using rank1 approximations) and ii) the convergence is guaranteed. To assess the practicality and effectiveness of our learning procedure, we conduct a few experiments on benchmark datasets, which allow us to draw positive conclusions on the relevance of our approach.\nThe paper is organized as follows. Section 2 introduces some notation and our learning setting; in particular the optimization problem we are interested in and the rank1 parametrization of the kernel our approach builds upon. Section 3 describes our new stochastic optimization procedure, establishes guarantees of convergence and details the computations to be implemented. Section 4 discusses the hyperparameters inherent to our modeling as well as the complexity of the proposed algorithm. Section 5 reports results from numerical simulations on benchmark datasets."
    }, {
      "heading" : "2. Proposed Model",
      "text" : "Notation X is the input space, k : X×X → R denotes the (positive) kernel function we have at hand and φ : X → H refers to the mapping φ(x) := k(x, ·) from X to the reproducing kernel Hilbert space H associated with k. Hence,\nar X\niv :1\n20 1.\n24 16\nv1 [\ncs .L\nG ]\nk(x,x′)=〈φ(x), φ(x′)〉, with 〈·, ·〉 the inner product ofH.\nThe training set is L := {(xi, yi)}ni=1 ∈ (X × R)n, where yi is the target value associated to xi. K = (k(xi,xj))1≤i,j≤n ∈ Rn×n is the Gram matrix of k with respect to L. For m = 1, . . . , n, cm ∈ Rn is defined as:\ncm := 1√\nk(xm,xm) [k(x1,xm), . . . , k(xn,xm)]\n>."
    }, {
      "heading" : "2.1. Data-parameterized Kernels",
      "text" : "For m = 1, . . . , n, φ̃m : X → H̃m is the mapping:\nφ̃m(x) := 〈φ(x), φ(xm)〉 k(xm,xm) φ(xm). (1)\nIt directly follows that k̃m defined as, ∀x,x′ ∈ X ,\nk̃m(x,x ′) := 〈φ̃m(x), φ̃m(x′)〉 =\nk(x,xm)k(x ′,xm)\nk(xm,xm) ,\nis indeed a positive kernel. Therefore, these parameterized kernels k̃m give rise to a family (K̃m)1≤m≤n of Gram matrices of the following form:\nK̃m = (k̃m(xi,xj))1≤i,j≤n = cmc T m, (2)\nwhich can be seen as rank-1 Nyström approximations of the full Gram matrix K (Drineas & Mahoney, 2005; Williams & Seeger, 2001).\nAs studied in (Kumar et al., 2009), it is sensible to consider convex combinations of the K̃m if they are of very low rank. Building on this idea, we will investigate the use of a parameterized Gram matrix of the form:\nK̃(µ) = ∑ m∈S µmK̃m with µm ≥ 0, (3)\nwhere S is a set of indices corresponding to the specific rank-one approximations used. Note that since we consider conical combinations of the K̃m, which are all positive semi-definite, K̃(µ) is positive semi-definite as well.\nUsing (1), one can show that the kernel k̃µ, associated to our parametrized Gram matrix K̃(µ), is such that:\nk̃µ(x,x ′) = 〈φ(x), φ(x′)〉A = φ(x)>Aφ(x), (4)\nwith A : = ∑ m∈S µm φ(xm)φ(xm) > k(xm,xm) . (5)\nIn other words, our parametrization induces a modified metric in the feature space H associated to k. On a side note, remark that when S = {1 . . . , n} (i.e. all the columns are picked) and we have uniform weights µ, then K̃(µ) = KK>, which is a matrix encountered when working with the so-called empirical kernel map (Schölkopf et al., 1999).\nFrom now on, M denotes the size of S and m0 refers to the number of non-zero components of µ (i.e. it is the 0- pseudo-norm of µ)."
    }, {
      "heading" : "2.2. Kernel Ridge Regression",
      "text" : "Kernel Ridge regression (KRR) is the kernelized version of the popular ridge regression (Hoerl & Kennard, 1970) method. The associated optimization problem reads:\nmin w\n{ λ‖w‖2 +\nn∑ i=1\n(yi − 〈w, φ(xi)〉)2 } , (6)\nwhere λ > 0 is a regularization parameter.\nUsing I for the identity matrix, the following dual formulation may be considered:\nmax α∈Rn\n{ FKRR(α) := y\nTα− 1 4λ αT (λI +K)α\n} . (7)\nThe solutionα∗ of the concave problem (7) and the optimal solution w∗ of (6) are connected through the equality\nw∗ = 1\n2λ n∑ i=1 α∗i φ(xi),\nand α∗ can be found by setting the gradient of FKRR to zero, to give\nα∗ = 2(I + 1λK) −1y. (8)\nThe value of the objective function at α∗ is then:\nFKRR(α ∗) = yT (I + 1λK) −1y, (9)\nand the resulting regression function is given by:\nf(x) = 1\n2λ n∑ i=1 α∗i k(xi,x). (10)"
    }, {
      "heading" : "2.3. A Convex Optimization Problem",
      "text" : "KRR may be solved by solving the linear system (I + K λ )α = 2y, at a cost of O(n\n3) operations. This might be prohibitive for large n, even more so if the matrix I+ Kλ does not fit into memory. To cope with this possible problem, we work with K̃(µ) (3) instead of the Gram matrix K. As we shall see, this not only makes it possible to avoid memory issues but it also allows us to set up a learning problem where both µ and a regression function are sought for at once. This is very similar to the Multiple Kernel Learning paradigm (Rakotomamonjy et al., 2008) where one learns an optimal kernel along with the target function.\nTo set up the optimization problem we are interested in, we proceed in a way similar to (Rakotomamonjy et al., 2008). For m = 1, . . . , n, define the Hilbert space H̃′m as:\nH̃′m := { f ∈ H̃m ∣∣∣∣‖f‖H̃mµm <∞ } . (11)\nOne can prove (Aronszajn, 1950) that H̃ = ⊕ H̃′m is the\nRKHS associated to k̃ = ∑ µmk̃m. Mimicking the reasoning of (Rakotomamonjy et al., 2008), our primal optimization problem reads:\nmin {fm},µ { λ ∑ m∈S 1 µm ‖fm‖2H̃′m + n∑ i=1 (yi − ∑ m∈S fm(xi)) 2 } ,\ns.t. ∑ m∈S µm ≤ n1 , µm ≥ 0, (12)\nwhere n1 is a parameter controlling the 1-norm of µ. As this problem is also convex in µ, using the earlier results on the KRR problem, (12) is equivalent to:\nmin µ≥0 { max α yTα− 1 4λ αT (λI + K̃(µ))α } = min µ≥0 { yT (I + 1λK̃(µ)) −1y } s.t. ∑ m∈S µm ≤ n1. (13)\nFinally, using the equivalence between Tikhonov and Ivanov regularization methods (Vasin, 1970), we obtain the convex and smooth optimization problem we focus on:\nmin µ≥0\n{ F (µ) := yT (I+ 1λK̃(µ)) −1y + ν ∑ m µm } . (14)\nThe regression function f̃ is derived using (1), a minimizer µ∗ of the latter problem and the accompanying weight vector α∗ such that\nα∗ = 2 ( I + 1λK̃(µ ∗) )−1 y, (15)\n(obtained adapting (8) to the case K = K(µ∗)). We have:\nf̃(x) = 1\n2λ n∑ i=1 α∗i k̃(xi,x) = 1 2λ ∑ m∈S µ∗m n∑ i=1 α∗i k̃m(xi,x)\n= 1\n2λ ∑ m∈S α̃∗mk(xm,x), (16)\nwhere α̃∗m := µ ∗ m\nc>mα ∗√\nk(xm,xm) . (17)"
    }, {
      "heading" : "3. Solving the problem",
      "text" : "We now introduce a new stochastic optimization procedure to solve (14). It implements a coordinate descent strategy with step sizes that use second-order information."
    }, {
      "heading" : "3.1. A Second-Order Stochastic Coordinate Descent",
      "text" : "Problem (14) is a constrained minimization based on the differentiable and convex objective function F . Usual convex optimization methods (such as projected gradient descent, proximal methods) may be employed to solve this\nproblem, but they may be too computationally expensive if n is very large, which is essentially due to a suboptimal exploitation of the parametrization of the problem. Instead, the optimization strategy we propose is specifically tailored to take advantage of the parametrization of K̃(µ).\nAlgorithm 1 depicts our stochastic descent method, inspired by (Nesterov, 2010). At each iteration, a randomly chosen coordinate of µ is updated via a Newton step. This method has two essential features: i) using coordinate-wise updates of µ involves only partial derivatives which can be easily computed and ii) the stochastic approach ensures a reduced memory cost while still guaranteeing convergence.\nAlgorithm 1 Stochastic Coordinate Newton Descent Input: µ0 random. repeat\nChoose coordinate mk uniformly at random in S. Update : µk+1m = µ k m if m 6= mk and\nµk+1mk =argmin v≥0 ∂F (µk) ∂µmk (v−µkmk)+ 1 2 ∂2F (µk) ∂µ2mk (v−µkmk) 2,\n(18) until F (µk)− F (µk−M ) < F (µk−M )\nNotice that the Stochastic Coordinate Newton Descent (SCND) is similar to the algorithm proposed in (Nesterov, 2010), except that we replace the Lipschitz constants by the second-order partial derivatives ∂\n2F (µk) ∂µ2mk\n. Thus, we replace a constant step-size gradient descent by a the Newton-step in (18), which allows us to make larger steps.\nWe show that for the function F in (14), SCND does provably converge to a minimizer of Problem (14). First, we rewrite (18) as a Newton step and compute the partial derivatives: Proposition 1. Eq. (18) is equivalent to\nµk+1mk =\n{ ( µkmk − ∂F (µk) ∂µmk /∂ 2F (µk) ∂µ2mk ) + if ∂ 2F (µk) ∂µ2mk\n6=0 0 otherwise.\n(19)\nProof. (19) gives the optimality conditions for (18).\nProposition 2. The partial derivatives ∂ pF (µ) ∂µpm are:\n∂F (µ) ∂µm = −λ(y>K̃−1λ,µcm) 2 + ν, (20)\n∂pF (µ) ∂µpm = (−1)pp!λ(y>K̃−1λ,µcm) 2(c>mK̃ −1 λ,µcm) p−1,\nwith p ≥ 2 and K̃−1λ,µ := (λI + K̃(µ)) −1. (21)\nProof. Easy but tedious calculations give the results.\nTheorem 1 (Convergence). For any sequence {mk}k, the sequence {F (µk)}k verifies:\n(a) ∀k, F (µk+1) ≤ F (µk). (b) limk→∞ F (µk) = minµ≥0 F (µ).\nMoreover, if there exists a minimizer µ∗ of F such that the Hessian∇2F (µ∗) is positive definite then:\n(c) µ∗ is the unique minimizer of F . The sequence {µk} converges to µ∗: ||µk−µ∗||→0.\nSketch of proof. (a) Using that ∂ 3F (µ) ∂µ3m\n≤ 0 (see (20)), one shows that the Taylor series truncated to the second order: v → F (µ) + ∂F (µ)∂µm (vm−µm) + 1 2 ∂2F (µ) ∂µ2m\n(vm− µm)\n2, is a quadratic upper-bound of F that matches F and∇F at point µ (for any fixed m and µ). From this, the update formula (18) yields F (µk+1) ≤ F (µk).\n(b) First note that ||µk|| ≤ F (µ0) and extract a converging subsequence {µφ(k)}. Denote the limit by µ̂. Separating the cases where ∂\n2F (µ̂) ∂µ2m\nis zero or not, one shows that µ̂ satisfies the optimality conditions: 〈∇F (µ̂),v − µ̂〉 ≥ 0, ∀v ≥ 0. Thus µ̂ is a minimizer of F and we have limF (µk) = limF (µφ(k)) = F (µ̂) = minµ≥0 F (µ).\n(c) is standard in convex optimization."
    }, {
      "heading" : "3.2. Iterative Updates",
      "text" : "One may notice that the computations of the derivatives (20), as well as the computation of α∗, depend on K̃−1λ,µ. Moreover, the dependency in µ, for all those quantities, only lies in K̃−1λ,µ. Thus, a special care need be taken on how K̃−1λ,µ is stored and updated throughout.\nLet S+µ = {m ∈ S|µm > 0} and m0 = ‖µ‖0 = |S+µ |. Let C = [ci1 · · · cim0 ] be the concatenation of the cij ’s, for ij ∈ S+µ andD the diagonal matrix with diagonal elements µij , for ij ∈ S+µ . Remark that throughout the iterations the sizes of C and D may vary. Given (21) and using Woodbury formula (Theorem 2, Appendix), we have:\nK̃−1λ,µ = ( λI + CDC> )−1 = 1\nλ I − 1 λ2 CGC> (22)\nwith G := ( D−1 + 1\nλ C>C\n)−1 . (23)\nNote that G is a square matrix of order m0 and that an update on µ will require an update on G. Even though updating G−1, i.e. D−1 + 1λC\n>C, is trivial, it is more efficient to directly store and update G. This is what we describe now.\nAt each iteration, only one coordinate of µ is updated. Let p be the index of the updated coordinate, µold, Cold, Dold\nand Gold, the vectors and matrices before the update and µnew, Cnew, Dnew and Gnew the updated matrices/vectors. Let also ep bethe vector whose pth coordinate is 1 while other coordinates are 0. We encounter four different cases.\nCase 1: µoldp = 0 and µnewp = 0. No update needed:\nGnew = Gold. (24)\nCase 2: µoldp 6= 0 and µnewp 6= 0. Here, Cold = Cnew and\nD−1new = D −1 old + ∆pepe > p , where ∆p :=\n1 µnewp − 1 µoldp .\nThen, using Woodbury formula, we have: Gnew = ( G−1old + ∆pepe > p )−1 = Gold−\n∆p 1 + ∆pgpp gpg > p ,\n(25) with gpp the (p, p)th entry of Gold and gp its pth column.\nCase 3: µoldp 6= 0 and µnewp = 0. Here, S+µnew = S+µold \\ {p}. It follows that we have to remove cp from Cold to have Cnew. To get Gnew, we may consider the previous update formula when µnewp → 0 (that is, when ∆p → +∞). Note that we can use the previous formula because µp 7→ K̃−1λ,µ is well-defined and continuous at 0. Thus, as limµnewp →0 ∆p 1+∆pgpp = 1gpp , we have:\nGnew = ( Gold − 1\ngpp gpg\n> p ) \\{p} , (26)\nwhere A\\{p} denotes the matrix A from which the pth column and pth row have been removed.\nCase 4: µoldp = 0 and µnewp 6= 0. We have Cnew = [Cold cp ] . Using (23), it follows that\nGnew =\n( D−1old + 1 λC > oldCold\n1 λC > oldcp\n1 λc > p Cold\n1 µnewp + 1λc > p cp\n)−1\n=\n( G−1old\n1 λC > oldcp\n1 λc > p Cold 1 µnewp + 1λc > p cp )−1 = ( A v v> s ) ,\nwhere, using the block-matrix inversion formula of Theorem 3 (Appendix), we have:\ns =\n( 1\nµnewp +\n1 λ c>p cp − 1 λ2 c>p ColdGoldC > oldcp )−1 v = − s\nλ GoldC\n> oldcp (27)\nA = Gold + 1\ns vv>.\nAlgorithm 2 SLKL: Stochastic Low-Rank Kernel Learning inputs: L := {(xi, yi)}ni=1, ν > 0, M > 0, > 0. outputs: µ, G and C (yield (λI +K(µ))−1 from (22)).\ninitialization: µ(0) = 0. repeat\nChoose coordinate mk uniformly at random in S . Update µ(k) according to (19), by changing only the mk-th coordinate µkmk of µ (k):\n• compute the second order derivative\nh = λ(y>K̃−1λ,µcmk) 2(c>mkK̃ −1 λ,µcmk) ;\n• if h > 0 then\nµ(k+1)mk = max ( 0, µ(k)mk + λ(y>K̃−1λ,µcmk ) 2 − ν\nh\n) ;\nelse µ(k+1)mk = 0.\nUpdate G(k) and C(k) according to (24)-(27). until F (µk)− F (µk−M ) < F (µk−M )\nComplete learning algorithm. Algorithm 2 depicts the full Stochastic Low-Rank Kernel Learning algorithm (SLKL), which recollects all the pieces just described."
    }, {
      "heading" : "4. Analysis",
      "text" : "Here, we discuss the relation between λ and ν and we argue that there is no need to keep both hyperparameters. In addition, we provide a short analysis on the runtime complexity of our learning procedure."
    }, {
      "heading" : "4.1. Pivotal Hyperparameter λν",
      "text" : "First recall that we are interested in the minimizer µ∗λ,ν of constrained optimization problem (14), i.e.:\nµ∗λ,ν = argmin µ≥0 Fλ,ν(µ), (28)\nwhere, for the sake of clarity, we purposely show the dependence on λ and ν of the objective function Fλ,ν\nFλ,ν(µ) = y > ( I + K̃ ( µ λ ))−1 y + λν ∑ m µm λ , (29) We may name α∗λ,ν , α̃ ∗ λ,ν the weight vectors associated with µ∗λ,ν (see (15) and (17)). We have the following: Proposition 3. Let λ, ν, λ′, ν′ be strictly positive real numbers. If λν = λ′ν′ then\nµ∗λ′,ν′ = λ′ λ µ ∗ λ,ν , and f̃λ,ν = f̃λ′,ν′ ."
    }, {
      "heading" : "As a direct consequence:",
      "text" : "∀λ, ν ≥ 0, f̃λ,ν = f̃1,λν .\nProof. Suppose that we know µ∗λ,ν . Given the definition (29) of Fλ,ν and using λν = λ′ν′, we have\nFλ,ν(µ) = Fλ′,ν′ ( λ′ λ µ )\nSince the only constraint of problem (28) is the nonnegativity of the components of µ, it directly follows that λ′µ∗λ,ν/λ is a minimizer of Fλ′,ν′ (under these constraints), hence µ∗λ′,ν′ = λ ′µ∗λ,ν/λ.\nTo show f̃λ,ν = f̃λ′,ν′ , it suffices to observe that, according to the way α∗λ,ν is defined (cf. (15)),\nα∗λ′,ν′ = 2 ( I +K ( µ∗ λ′,ν′ λ′ ))−1 y\n= 2 ( I +K ( λ′\nλ µ∗λ,ν λ′ ))−1 y = α∗λ,ν ,\nand, thus, α̃∗λ′,ν′ = λ ′α̃∗λ,ν/λ. The definition (16) of f̃λ,ν then gives f̃λ,ν = f̃λ′,ν′ , which entails f̃λ,ν = f̃1,λν .\nThis proposition has two nice consequences. First, it says that the pivotal hyperparameter is actually the product λν: this is the quantity that parametrizes the learning problem (not λ or ν, seen independently). Thus, the set of regression functions, defined by the λ and ν hyperparameter space, can be described by exploring the set of vectors (µ∗1,ν)ν>0, which only depends on a single parameter. Second, considering (µ∗1,ν)ν>0 allows us to work with the family of objective functions (F1,ν)ν>0, which are well-conditioned numerically as the hyperparameter λ is set to 1."
    }, {
      "heading" : "4.2. Runtime Complexity and Memory Usage",
      "text" : "For the present analysis, let us assume that we pre-compute the M (randomly) selected columns c1, . . . , cM . If a is the cost of computing a column cm, the pre-computation has a cost of O(Ma) and has a memory usage of O(nM).\nAt each iteration, we have to compute the first and secondorder derivatives of the objective function, as well as its value and the weight vector α. Using (22), (20), (14) and (15), one can show that those operations have a complexity of O(nm0) if m0 is the zero-norm of µ.\nBesides, in addition to C, we need to store G for a memory cost of O(m20). Overall, if we denote the number of iterations by k, the algorithm has a memory cost of O(nM +m20) and a complexity of O(knm0 +Ma).\nIf memory is a critical issue, one may prefer to compute the columns cm on-the-fly and m0 columns need to be stored instead of M (this might be a substantial saving in terms of memory as can be seen in the next section). This improvement in term of memory usage implies an additive cost in the runtime complexity. In the worst case, we have\nto compute a new column c at each iteration. The resulting memory requirement scales as O(nm0 + m20) and the runtime complexity varies as O(k(nm0 + a))."
    }, {
      "heading" : "5. Numerical Simulations",
      "text" : "We now present results from various numerical experiments, for which we describe the datasets and the protocol used. We study the influence of the different parameters of our learning approach on the results and compare the performance of our algorithm to that of related methods."
    }, {
      "heading" : "5.1. Setup",
      "text" : "First, we use a toy dataset (denoted by sinc) to better understand the role and influence of the parameters. It consists in regressing the cardinal sine of the two-norm (i.e. x 7→ sin(‖x‖)/‖x‖) of random two-dimensional points, each drawn uniformly between−5 and +5. In order to have a better idea on how the solutions may or may not over-fit the training data, we add some white Gaussian noise on the target variable of the randomly picked 1000 training points (with a 10 dB signal-to-noise ratio). The test set is made of 1000 non-noisy independent instance/target pairs.\nWe then assess our method on two UCI datasets: Abalone (abalone) and Boston Housing (boston), using the same normalizations, Gaussian kernel parameters (σ denotes the kernel width) and data partition as in (Smola & Schölkopf, 2000). The United States Postal Service (USPS) dataset is used with the same setting as in (Williams & Seeger, 2001). Finally, the Modified National Institute of Standards and Technology (MNIST) dataset is used with the same preprocessing as in (Maji & Malik, 2009). Table 1 summarizes the characteristics of all the datasets we used.\nAs displayed in Algorithm 1, at each iteration k > M , we check if F (µk)−F (µk−M ) < F (µk−M ) holds. If so, we stop the optimization process. thus controls our stopping criterion. In the experiments, we set = 10−4 unless otherwise stated and we set λ to 1 for all the experiments and we run simulations for various values of ν and M . In order to assess the variability incurred by the stochastic nature of our learning algorithm, we run each experiment 20 times."
    }, {
      "heading" : "5.2. Influence of the parameters",
      "text" : ""
    }, {
      "heading" : "5.2.1. EVOLUTION OF THE OBJECTIVE",
      "text" : "We have established (Section 3) the convergence of our optimization procedure, under mild conditions. A question that we have not tackled yet is to evaluate its convergence rate. Figure 1 plots the evolution of the objective function on the sinc dataset. We observe that the evolutions of the objective function are impressively similar among the different runs. This empirically tends to assert that it is relevant to look for theoretical results on the convergence rate.\nA question left for future work is the impact of the random selection of the set of columns S on the reached solution.\n5.2.2. ZERO-NORM OF µ\nAs shown in Section 4.2, both memory usage and the complexity of the algorithm depend on m0. Thus, it is interesting to take a closer look at how this quantity evolves. Figure 2 and 3 experimentally point out two things. On the one hand, the number of active componentsm0 = ‖µ‖0 remains significantly smaller thanM . In other words, as long as the regularization parameter is well-chosen, we never have to store all of the cm at the same time. On the other hand, the solution µ∗ is sparse and ‖µ∗‖0 grows with M and diminishes with ν. A theoretical study on the dependence ofµ∗ andm0 inM and ν, left for future work, would\nbe all the more interesting since sparsity is the cornerstone on which the scalability of our algorithm depends."
    }, {
      "heading" : "5.3. Comparison to other methods",
      "text" : "This section aims at giving a hint on how our method performs on regression tasks. To do so, we compare the Mean Square Error (over the test set). In addition to our Stochastic Low-Rank Kernel Learning method (SLKL), we solve the problem with the standard Kernel Ridge Regression method, using the n training data (KRRn) and using onlyM training data (KRRM). We also evaluate the performance of the KRR method, using the kernel obtained with uniform weights on the M rank-1 approximations selected for SLKL (Unif ). The results are displayed in Table 2, where the bold font indicates the best low-rank method (KRRM, Unif or SLKL) for each experiment.\nTable 2 confirms that optimizing the weight vector µ is decisive as our results dramatically outperform those of Unif. As long as M < n, our method also outperforms KRRM. The explanation probably lies in the fact that our approximations keep information about similarities between the M selected points and the n − M others. Furthermore, our method SLKL achieves comparable performances (or even better on abalone) than KRRn, while finding sparse solutions. Compared to the approach from (Smola & Schölkopf, 2000), we seem to achieve lower test error on the boston dataset even for M = 128. On the abalone dataset, this method outperforms ours for every M we tried.\nFinally, we also compare the results we obtain on the USPS dataset with the ones obtained in (Williams & Seeger, 2001) (Nyst). As it consists in a classification task, we actually perform a regression on the labels to adapt our method, which is known to be equivalent to solving Fisher Discriminant Analysis (Duda & Hart, 1973). The performance achieved by Nyst outperforms ours. However, one may argue that the performance have a same order of magnitude and note that the Nyst approach focuses on the classification task, while ours was designed for regression."
    }, {
      "heading" : "5.4. Large-scale dataset",
      "text" : "To assess the scalability of our method, we ran experiments on the larger handwritten digits MNIST dataset, whose training set is made of 60000 examples. We used a Gaussian kernel computed over histograms of oriented gradients as in (Maji & Malik, 2009), in a “one versus all” setting. ForM=1000, we obtained classification error rates around 2% over the test set, which do not compete with state-ofthe-art results but achieve reasonable performance, considering that we use only a small part of the data (cf. the size of M ) and that our method was designed for regression.\nAlthough our method overcomes memory usage issues for such large-scale problems, it still is computationally intensive. In fact, a large number of iterations is spent picking coordinates whose associated weight remains at 0. Though those iterations do not induce any update, they do require computing the associated Gram matrix column (which is not stored as it does not weigh in the conic combination) as well as the derivatives of the objective function. The main focus of our future work is to avoid those computations, using e.g. techniques such as shrinkage (Hsieh et al., 2008)."
    }, {
      "heading" : "6. Conclusion",
      "text" : "We have presented an original kernel-based learning procedure for regression. The main features of our contribution are the use of a conical combination of data-based kernels and the derivation of a stochastic convex optimization procedure, that acts coordinate-wise and makes use of second-order information. We provide theoretical convergence guarantees for this optimization procedure, we depict the behavior of our learning procedure and illustrate its effectiveness through a number of numerical experiments carried out on several benchmark datasets.\nThe present work naturally raises several questions. Among them, we may pinpoint that of being able to establish precise rate of convergence for the stochastic optimization procedure and that of generalizing our approach to the use of several kernels. Establishing data-dependent generalization bounds taking advantage of either the one-norm constraint on µ or the size M of the kernel combination is of primary importance to us. The connection established between the one-norm hyperparameter ν and the ridge pa-\nrameter λ, in section 4, seems interesting and may be witnessed in (Rakotomamonjy et al., 2008). Although not been mentioned so far, there might be connections between our modeling strategy and boosting/leveraging-based optimization procedures. Finally, we plan on generalizing our approach to other kernel methods, noting that rank-1 update formulas as those proposed here can possibly be exhibited even for problems with no closed-form solution."
    }, {
      "heading" : "Acknowledgments",
      "text" : "This work is partially supported by the IST Program of the European Community, under the FP7 Pascal 2 Network of Excellence (ICT-216886-NOE) and by the ANR project LAMPADA (ANR-09-EMER-007)."
    }, {
      "heading" : "A. Matrix Inversion Formulas",
      "text" : "Theorem 2. (Woodbury matrix inversion formula (Woodbury, 1950)) Let n and m be positive integers, A ∈ Rn×n and C ∈ Rm×m be non-singular matrices and let U ∈ Rn×m and V ∈ Rm×n be two matrices. If C−1+V A−1U is non-singular then so is A+UCV and:\n(A+UCV )−1 = A−1−A−1U(C−1+V A−1U)−1V A−1.\nTheorem 3. (Matrix inversion with added column) Given m, integer and M ∈ R(n+1)×(n+1) partitioned as:\nM =\n( A b\nb> c\n) , where A ∈ Rn×n, b ∈ Rn and c ∈ R."
    }, {
      "heading" : "If A is non-singular and c− b>A−1b 6= 0, then M is nonsingular and the inverse of M is given by",
      "text" : "M−1 =\n( A−1 + 1kA −1bb>A−1 − 1kA −1b\n− 1kb >A−1 1k\n) , (30)\nwhere k = c− b>A−1b."
    } ],
    "references" : [ {
      "title" : "On the nyström method for approximating a gram matrix for improved kernel-based learning",
      "author" : [ "P. Drineas", "M. Mahoney" ],
      "venue" : "J. of Machine Learning Research,",
      "citeRegEx" : "Drineas and Mahoney,? \\Q2005\\E",
      "shortCiteRegEx" : "Drineas and Mahoney",
      "year" : 2005
    }, {
      "title" : "Pattern Classification and Scene Analysis",
      "author" : [ "Duda", "Richard O", "Hart", "Peter E" ],
      "venue" : null,
      "citeRegEx" : "Duda et al\\.,? \\Q1973\\E",
      "shortCiteRegEx" : "Duda et al\\.",
      "year" : 1973
    }, {
      "title" : "A dual coordinate descent method for largescale linear svm",
      "author" : [ "Hsieh", "C.-J", "Chang", "K.-W", "Lin", "Keerthi", "S. Sathiya", "S. Sundararajan" ],
      "venue" : "In Proceedings of the 25th International Conference on Machine Learning,",
      "citeRegEx" : "Hsieh et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Hsieh et al\\.",
      "year" : 2008
    }, {
      "title" : "Ensemble nyström method",
      "author" : [ "S. Kumar", "M. Mohri", "A. Talwalkar" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Kumar et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Kumar et al\\.",
      "year" : 2009
    }, {
      "title" : "Fast and accurate digit classification",
      "author" : [ "S. Maji", "J. Malik" ],
      "venue" : "Technical report, EECS Department, UC Berkeley,",
      "citeRegEx" : "Maji and Malik,? \\Q2009\\E",
      "shortCiteRegEx" : "Maji and Malik",
      "year" : 2009
    }, {
      "title" : "Efficiency of coordinate descent methods on hugescale optimization problems",
      "author" : [ "Y. Nesterov" ],
      "venue" : "Core discussion papers,",
      "citeRegEx" : "Nesterov,? \\Q2010\\E",
      "shortCiteRegEx" : "Nesterov",
      "year" : 2010
    }, {
      "title" : "Input space versus feature space in kernel-based methods",
      "author" : [ "B. Schölkopf", "S. Mika", "C.J.C. Burges", "P. Knirsch", "K.R. Müller", "G. Rätsch", "A.J. Smola" ],
      "venue" : "IEEE Transactions on Neural Networks,",
      "citeRegEx" : "Schölkopf et al\\.,? \\Q1999\\E",
      "shortCiteRegEx" : "Schölkopf et al\\.",
      "year" : 1999
    }, {
      "title" : "Sparse greedy matrix approximation for machine learning",
      "author" : [ "A.J. Smola", "B. Schölkopf" ],
      "venue" : "In International Conference on Machine Learning,",
      "citeRegEx" : "Smola and Schölkopf,? \\Q2000\\E",
      "shortCiteRegEx" : "Smola and Schölkopf",
      "year" : 2000
    }, {
      "title" : "Relationship of several variational methods for the approximate solution of ill-posed problems",
      "author" : [ "V.V. Vasin" ],
      "venue" : "Mathematical Notes,",
      "citeRegEx" : "Vasin,? \\Q1970\\E",
      "shortCiteRegEx" : "Vasin",
      "year" : 1970
    }, {
      "title" : "Using the nyström method to speed up kernel machines",
      "author" : [ "C. Williams", "M. Seeger" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "Williams and Seeger,? \\Q2001\\E",
      "shortCiteRegEx" : "Williams and Seeger",
      "year" : 2001
    }, {
      "title" : "Inverting modified matrices",
      "author" : [ "M.A. Woodbury" ],
      "venue" : "Technical report,",
      "citeRegEx" : "Woodbury,? \\Q1950\\E",
      "shortCiteRegEx" : "Woodbury",
      "year" : 1950
    } ],
    "referenceMentions" : [ {
      "referenceID" : 3,
      "context" : "Nyström approximations, whose weights are chosen so as to serve the regression task – this makes our approach different from (Kumar et al., 2009) and (Suykens et al.",
      "startOffset" : 125,
      "endOffset" : 145
    }, {
      "referenceID" : 5,
      "context" : "Secondly, to solve the convex optimization problem entailed by our modeling choice, we provide an original stochastic optimization procedure based on (Nesterov, 2010).",
      "startOffset" : 150,
      "endOffset" : 166
    }, {
      "referenceID" : 3,
      "context" : "As studied in (Kumar et al., 2009), it is sensible to consider convex combinations of the K̃m if they are of very low rank.",
      "startOffset" : 14,
      "endOffset" : 34
    }, {
      "referenceID" : 6,
      "context" : "all the columns are picked) and we have uniform weights μ, then K̃(μ) = KK>, which is a matrix encountered when working with the so-called empirical kernel map (Schölkopf et al., 1999).",
      "startOffset" : 160,
      "endOffset" : 184
    }, {
      "referenceID" : 8,
      "context" : "Finally, using the equivalence between Tikhonov and Ivanov regularization methods (Vasin, 1970), we obtain the convex and smooth optimization problem we focus on:",
      "startOffset" : 82,
      "endOffset" : 95
    }, {
      "referenceID" : 5,
      "context" : "Algorithm 1 depicts our stochastic descent method, inspired by (Nesterov, 2010).",
      "startOffset" : 63,
      "endOffset" : 79
    }, {
      "referenceID" : 5,
      "context" : "Notice that the Stochastic Coordinate Newton Descent (SCND) is similar to the algorithm proposed in (Nesterov, 2010), except that we replace the Lipschitz constants by the second-order partial derivatives ∂ F (μ) ∂μ2mk .",
      "startOffset" : 100,
      "endOffset" : 116
    }, {
      "referenceID" : 2,
      "context" : "techniques such as shrinkage (Hsieh et al., 2008).",
      "startOffset" : 29,
      "endOffset" : 49
    }, {
      "referenceID" : 10,
      "context" : "(Woodbury matrix inversion formula (Woodbury, 1950)) Let n and m be positive integers, A ∈ Rn×n and C ∈ Rm×m be non-singular matrices and let U ∈ Rn×m and V ∈ Rm×n be two matrices.",
      "startOffset" : 35,
      "endOffset" : 51
    } ],
    "year" : 2012,
    "abstractText" : "We present a novel approach to learn a kernelbased regression function. It is based on the use of conical combinations of data-based parameterized kernels and on a new stochastic convex optimization procedure of which we establish convergence guarantees. The overall learning procedure has the nice properties that a) the learned conical combination is automatically designed to perform the regression task at hand and b) the updates implicated by the optimization procedure are quite inexpensive. In order to shed light on the appositeness of our learning strategy, we present empirical results from experiments conducted on various benchmark datasets.",
    "creator" : "LaTeX with hyperref package"
  }
}