{
  "name" : "1512.01914.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : null,
    "authors" : [ ],
    "emails" : [ "zhang923@purdue.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "ar X\niv :1\n51 2.\n01 91\n4v 1\n[ cs\n.L G\n] 7\nBoltzmann machine, as a fundamental construction block of deep belief network and deep Boltzmann machines, is widely used in deep learning community and great success has been achieved. However, theoretical understanding of many aspects of it is still far from clear. In this paper, we studied the Rademacher complexity of both the asymptotic restricted Boltzmann machine and the practical implementation with single-step contrastive divergence (CD-1) procedure. Our results disclose the fact that practical implementation training procedure indeed increased the Rademacher complexity of restricted Boltzmann machines. A further research direction might be the investigation of the VC dimension of a compositional function used in the CD-1 procedure."
    }, {
      "heading" : "1 Introduction",
      "text" : "A restricted Boltzmann machine (RBM) is a generative graphical model that can learn a probability distribution over its set of inputs. Initially proposed by Smolensky [1986] for modeling cognitive process, it grew to prominence after successful application were found by Geoffrey Hinton and his collaborators [Hinton and Salakhutdinov, 2006, 2012; Salakhutdinov and Hinton, 2009]. As a building block for deep belief network (DBN) and deep Boltzmann machines (DBM), RBM is extremely useful for pre-training the data by projecting them to a hidden layer. Also, it is proved that by adding another layer on top of a RBM, the variational lower bound of the data likelihood can be increased [Hinton et al., 2006; Salakhutdinov and Hinton, 2012], which conveys the theoretical advantage of building multilayer RBMs.\nPre-training of the data by using a RBM is essentially a unsupervised learning process, in which no label of the data is provided. Instead, the training process is trying to maximize the data likelihood by finding a proper set of parameters of the RBM.\nHowever less attention has been given to the analysis of Rademacher complexity on RBMs. Rademacher complexity in the computational learning theory, measures richness of a class of realvalued functions with respect to a probability distribution. It can be regarded as a generalization of PAC-Bayes analysis. Its particular setting can help analysis of unsupervised learning algorithms,\n1\nrather than merely the prediction problems, given the hypothesis class is possibly infinite. Honorio [2012] also proved that discrete factor graphs, including Markov random fields, are Lipschitz continuous, which motivated this work to further investigate the properties of RBM.\nThe goal of this paper is trying to bound the Rademacher complexity for the likelihood of the RBM algorithm from a given training data set, with pre-assumptions that the model structure of the RBM is known (data dimensionality and number of hidden nodes)."
    }, {
      "heading" : "2 Preliminaries",
      "text" : "In the beginning of this section we introduce Lipschitz continuity.\nDefinition 1. Given the parameters Θ ∈ RM1×M2 , a differentiable function f(Θ) ∈ R is called Lipschitz continuous with respect to the lp-norm of Θ, if there exist a constant K ≥ 0 such that:\n(∀Θ1,Θ2)|f(Θ1)− f(Θ2)| ≤ K‖Θ1 −Θ2‖ (1)\nor equivalently:\n(∀Θ)‖ ∂f ∂Θ ‖ ≤ K (2)\nNext we introduce the Rademacher complexity.\nDefinition 2.\nDefinition 2.1. A random variable x ∈ {−1,+1} is called Rademacher ⇐⇒ P(x) ∼ Bernoulli(0.5).\nDefinition 2.2. The empirical Rademacher complexity of the hypothesis class H w.r.t a data set S = {z(1) . . . z(n)} is defined as:\nR̂s(F) = Eσ [\nsup h∈H\n(\n1 n\nn∑\ni=1\nσih(z (i))\n)]\n(3)\nAt the end of this section we give formal definition of the restricted Boltzmann machine.\nDefinition 3. The restricted Boltzmann machine is a two layer Markov Random Field, where the observed binary stochastic visible units x ∈ {0, 1}k have pairwise connections to the binary stochastic hidden units h ∈ {0, 1}m. There are no pairwise connections within the visible units, nor within the hidden ones. Restricted Boltzmann machine is a energy-based model, in which we define the energy for a state {x,h} as\nEnergy(x,h;θ) = −xTb− hTc− xTWh, (4)\nPage 2 of 11\nwhere θ = {c,b,W}, c ∈ Rm,b ∈ Rk, and W ∈ Rk×m. Hence, we can write the likelihood for an observation x as\npθ(x) =\n∑\nh\nexp{−Energy(x,h;θ)}\nZθ , (5)\nwhere\nZθ = ∑\nh\n∑\nx\nexp{−Energy(x,h;θ)}, (6)\nwhich is the partition function, used for normalization. The sum over h and x enumerate all the possible values for the visible units and the hidden ones. Our goal optimization is to maximize the log-likelihood (minimize the negative log-likelihood) of the model. For N data samples, we can write the log-likelihood as:\nln pθ(x) = ln{ ∑\nh exp{−Energy(x,h;θ)}} ︸ ︷︷ ︸\n1\n− lnZθ ︸ ︷︷ ︸\n2\n(7)"
    }, {
      "heading" : "3 Rademacher Complexity",
      "text" : "In this section, we provides a up-bound of the empirical Rademacher complexity for the likelihood of the restricted Boltzmann machine. Since part 2, the partition function of the restricted Boltzmann machine, of equation 7 is not depending on the data set. This part does not have any randomness and the Rademacher complexity of it is 0 by the definition. Thus, we can only focus on the Rademacher complexity of part 1 of equation 7. Denote Wj as the j-th column of the matrix W, cj as the j-th element of c, hj as the j-th element of h. By expanding part 1 of equation 7, we get\npart 1 = ln{ ∑\nh\nexp{−Energy(x,h;θ)}} (8)\n= ln{ ∑\nh1\n· · · ∑\nhm\nexp\n\nxTb+\nm∑\nj=1\nxTWjhj +\nm∑\nj=1\nhjcj\n\n} (9)\n= ln{ m∏\nj=1\n  ∑\nhj∈{0,1}\nexp\n xTb+ m∑\nj=1\nxTWjhj + m∑\nj=1\nhjcj\n\n\n\n} (10)\n=\nm∑\nj=1\nln [ exp(xTb) + exp(xTb+ xTWj + cj) ] (11)\nLemma 1. Let X = {x|x ∈ {0, 1}d}. Let F be the class of linear predictors, i.e.,\nF = {bTx|b ∈ Rd and ‖b‖1 ≤ B}. (12)\nWe have\nR̂s(F) ≤ B √ 2 ln (d)\nn (13)\nPage 3 of 11\nProof. Let S = {x(1) . . .x(n)} be a data set of n samples. Denote xj as the j-th element of x.\nR̂s(F) = Eσ [\nsup f∈F\n(\n1 n\nn∑\ni=1\nσif(x (i))\n)]\n(14)\n= 1\nn Eσ\n[\nsup f∈F\n( n∑\ni=1\nσib Tx(i)\n)]\n(15)\n= 1\nn Eσ\n[\nsup b:‖b‖1≤B\n(\nbT\n( n∑\ni=1\nσix (i)\n))]\n(16)\n= B\nn Eσ\n[\n‖ n∑\ni=1\nσix (i)‖∞\n]\n(17)\n= B\nn Eσ\n\n sup j∈{1...d}\n( n∑\ni=1\nσix (i)\n)\nj\n\n (18)\n≤ B √ 2 ln (d)\nn sup\nj∈{1...d}\n√ √ √ √ n∑\ni=1\n[\nx (i) j\n]2 (19)\n≤ B √ 2 ln (d)\nn\n√\nn‖x‖2∞ (20)\n= ‖x‖∞B √ 2 ln (d)\nn (21)\n= B\n√\n2 ln (d)\nn (22)\nEquation 17 uses Holder’s inequality when the equal sign is taken. inequality 19 uses Massart’s finite class lemma. Equation 22 is from the fact that x ∈ {0, 1}d. Therefore we proved inequality 13.\nRemark 1. Function\nφ(g) = ln (1 + exp(g)) (23)\nis 1-Lipschitz continuous for g ∈ R.\nProof. |∂φ(g)/∂g| = Sigmoid(g) ≤ 1.\nLemma 2. Let X = {x|x ∈ {0, 1}d},F be a class of linear predictors, i.e.,\nF = {bTx|b ∈ Rd}. (24)\nLet G be another class of linear predictors, i.e.,\nG = {wTx+ c|w ∈ Rd, c ∈ R}. (25)\nLet H be a function of F and G, written as\nH = {ln [exp (f(x)) + exp (f(x) + g(x))] |f ∈ F , g ∈ G}, (26)\nPage 4 of 11\nLet S = {x(1) . . .x(n)} be a data set of n samples. We have\nR̂s(H) ≤ R̂s(F) + R̂s(G) (27)\nProof.\nR̂s(H) = Eσ [\nsup h∈H\n(\n1 n\nn∑\ni=1\nσih(x (i))\n)]\n(28)\n= Eσ\n[\nsup f∈F ,g∈G\n(\n1 n\nn∑\ni=1\nσi ln [ exp ( f(xi) ) + exp ( f(xi) + g(xi) )]\n)]\n(29)\n= Eσ\n[\nsup f∈F ,g∈G\n(\n1 n\nn∑\ni=1\nσi ln [ exp ( f(xi) )] + ln [ 1 + exp ( g(xi) )]\n)]\n(30)\n≤ Eσ [\nsup f∈F\n(\n1 n\nn∑\ni=1\nσif(x i)\n)]\n+ Eσ\n[\nsup g∈G\n(\n1 n\nn∑\ni=1\nσi ln [ 1 + exp ( g(xi) )]\n)]\n(31)\n= Eσ\n[\nsup f∈F\n(\n1 n\nn∑\ni=1\nσif(x i)\n)]\n+ Eσ\n[\nsup g∈G\n(\n1 n\nn∑\ni=1\nσiφ(g(x (i)))\n)]\n(32)\n≤ R̂s(F) + R̂s(G) (33)\nIn inequality 31, the first part of it is exactly the Rademacher complexity of F by definition. The second part can be shown to be ≤ R̂s(G) by using Ledoux-Talagrand Contraction Lemma, combining with the results in Remark 1 that φ(g) is 1-Lipschitz continuous. Hence we proved inequality 27.\nRemark 2. Let X = {x|x ∈ {0, 1}d}. Let G be the class of linear predictors, i.e.,\nG = {wTx+ c|w ∈ Rd, c ∈ R and ‖w‖1 ≤ W}, (34)\nwhere Wj is the j-th column of W. Let S = {x(1) . . .x(n)} be a data set of n samples. We have\nR̂s(G) ≤ W √ 2 ln (d)\nn (35)\nPage 5 of 11\nProof.\nR̂s(G) = Eσ [\nsup g∈G\n(\n1 n\nn∑\ni=1\nσig(x (i))\n)]\n(36)\n= Eσ\n[\nsup g∈G\n(\n1 n\nn∑\ni=1\nσi(w Tx(i) + c)\n)]\n(37)\n≤ 1 n Eσ\n[\nsup w:‖w‖1≤W\n(\nwT\n( n∑\ni=1\nσix (i)\n))\n+ sup c\n( n∑\ni=1\nσi\n)]\n(38)\n= 1\nn Eσ\n[\nsup w:‖w‖1≤W\n(\nwT\n( n∑\ni=1\nσix (i)\n))]\n+ 1\nn Eσ\n[\nsup c\n( n∑\ni=1\nσi\n)]\n(39)\n(40)\nNotice that the first part of equation 39 can be bounded by W\n√\n2 ln (d)\nn by using the results in\nLemma 1, and the second part is exactly 0 by the definition of Rademacher complexity. Thus we proved inequality 35.\nTheorem 1. Let X = {x|x ∈ {0, 1}k},S = {x(1) . . .x(n)} be a data set of n samples. Given a restricted Boltzmann machine with k visible units and m hidden ones. For all the parameters θ = {c,b,W}, c ∈ Rm,b ∈ Rk, and W ∈ Rk×m, assuming b,W are bounded by spheres ‖b‖1 ≤ B, ‖W‖max = ∀j ‖Wj‖1 ≤ W, where Wj is the j-th column of W. We can bound the empirical Rademacher complexity for the likelihood of this restricted Boltzmann machine as:\nR̂s(ln pθ) ≤ m √ 2 ln (k)\nn (B +W ) . (41)\nProof. As we stated, we only consider equation 11 that has randomness and ignore the partition part of the log-likelihood. Using the notation in Lemma 2, we can write\nR̂s(ln pθ) = R̂s(\nm∑\nj=1\nHj) ≤ m∑\nj=1\nR̂s(Hj), (42)\nwhich is from the elementary properties of the Rademacher complexity. Knowing the fact that each Hj is of the same hypothesis space further provides us with m∑\nj=1\nR̂s(Hj) = mR̂s(H). (43)\nFrom Lemma 2 we have R̂s(H) ≤ R̂s(F) + R̂s(G). And from Lemma 1 we can directly bound R̂s(F) by B √ 2 ln (k)\nn . For R̂s(G), by using the results inRemark 2, we can bound it byW\n√\n2 ln (k)\nn .\nTogether with inequality 42 and equation 43, we proved this theorem.\nPage 6 of 11"
    }, {
      "heading" : "4 Rademacher Complexity with CD-1 Approximation",
      "text" : "Contrastive Divergence is an approximation of the log-likelihood gradient that has been found to be a successful update rule for training RBMs. The reason that we are applying contrastive divergence algorithm is that because the partition function is can be hardly estimated by enumerating all the possible values because the complexity will be in the order of exponential, nor the factorization trick we used for the numerator can be used. In order to approximate the partition function for all possible visible examples, a MCMC chain is created. First an example is sampled uniformly from the empirical training examples. Then a mean-field approximation is applied to obtain the values of hidden units (whose values are also binary): Rather than sample from the distribution of h, we use the values ∀ i, P (hi = 1) as the values to approximate the samples. After we obtain h̃, we have the distribution of x based on the current values of h (mean-field approximation) and parameters (W,b, c). We sample from this distribution to obtain a vector x̃ and use it to approximate the partition function. This procedure can also extended to more steps (CD-k, k steps). But experiments have shown that, even one step (CD-1) can yield a good performance for the model [Bengio, 2009].\nAfter using CD-1 algorithm, the Rademacher complexity of the second part of equation 7 is no more free of randomness, due to the fact that x̃ is a function of x. If we rewrite the second part as\nZθ ≈ ∑\nh\nexp{−Energy(x̃,h;θ)}, (44)\nRademacher complexity of this term is also depending on random variable x.\nTo simplify the procedure but without losing generality, instead of sampling x̃ from its distribution, we also use mean field approximation to obtain x̃. Also, we can write the energy function as\nEnergy(x,h;θ) = xTWh, (45)\nwhile ignore the bias term for simplicity.\nRemark 3. Using mean filed approximation, we can obtain\nh̃T = (sgm(xTW·1), . . . , sgm(x TW·m)), (46)\nwhere sgm() is the sigmoid function, and W·j is the j-th column of W.\nProof. P (h̃|x = 1) = exp{xTW1} ∑\nh\nexp{xTWh} =\nm∏ i=1 exp{xTW·i}\nm∏\ni=1\n∑\nhi\nexp{xTW·ihi}\nWith the fact that ∀i, j hi ⊥ hj |x,∀i, P (h̃i|x = 1) = exp{xTW·i}\n1 + exp{xTW·i} = sgm(xTW·i).\nRemark 4. Similar to Remark 3, we can obtain\nx̃T = (sgm(W1·h̃), . . . , sgm(Wk·h̃)), (47)\nwhere ∀vWv· is the v-th row of W.\nPage 7 of 11\nLemma 3. By using CD-1 Algorithm, and mean field approximation for both x̃ and h̃, we have part 2 of equation 7 as\nlnZθ =\nm∑\nj=1\nln\n[\n1 + exp{ k∑\ni=1\nWijsgm\n( m∑\nv=1\nWivsgm(x TW·v)\n) } ] , (48)\nwhere we use Wij to denote the element of i-th row and j-th column of matrix w.\nProof. (sketch) Using the results from Remark 3 and Remark 4, and the same factorization trick used before in equation 10, this can be shown easily.\nLemma 4. Let X = {x|x ∈ {0, 1}k}, Let T be a compositional function of x with parameters W, i.e.,\nT = {Wujsgm ( m∑\nv=1\nWuvsgm(x TW·v)\n)\n|W ∈ Rk×m, ∀u ∈ {1, . . . , k},∀j ∈ {1, . . . ,m}}, (49)\nand assuming W is bounded by spheres ‖W‖max = ∀j ‖W·j‖1 ≤ W, where W·j is the j-th column of W. Also Let S = {x(1) . . .x(n)} be a data set of n samples. We have\nR̂s(T ) ≤ W\n√\n2n ln |T | n\n(50)\nPage 8 of 11\nProof.\nR̂s(T ) = Eσ [\nsup tw∈T\n(\n1 n\nn∑\ni=1\nσitW(x (i))\n)]\n=⇒ (51)\nexp{Eσ [\ns sup tw∈T\n( n∑\ni=1\nσitW(x (i))\n)] } ≤ Eσ [\nexp{s sup tw∈T\n( n∑\ni=1\nσitW(x (i))\n) } ]\n(52)\n= sup tw∈T Eσ\n[ exp{s ( n∑\ni=1\nσitW(x (i))\n) } ]\n(53)\n≤ ∑\ntw∈T\nEσ\n[ exp{s ( n∑\ni=1\nσitW(x (i))\n) } ]\n(54)\n= ∑\ntw∈T\nEσ\n[ n∏\ni=1\nexp{s ( σitW(x (i)) ) } ]\n(55)\n= ∑\ntw∈T\nn∏\ni=1\nEσi\n[ exp{s ( σitW(x (i)) ) } ]\n(56)\n≤ ∑\ntw∈T\nn∏\ni=1\nexp{ 4s2W2uj\n8 } (57)\n= ∑\ntw∈T\nexp{ 4ns2W2uj\n8 } (58)\n≤ |T | sup tw∈T\nexp{ 4ns2W2uj\n8 } (59)\n= |T | exp{ns 2W 2\n2 } =⇒ (60)\nexp{Eσ [\ns sup tw∈T\n( n∑\ni=1\nσitW(x (i))\n)]\n} ≤ ln |T | s + nsW 2 2 =⇒ (61)\nexp{Eσ [\ns sup tw∈T\n( n∑\ni=1\nσitW(x (i))\n)]\n} ≤ W √ 2n ln |T | =⇒ (62)\nR̂s(T ) = Eσ [\nsup tw∈T\n(\n1 n\nn∑\ni=1\nσitW(x (i))\n)]\n≤ W √\n2n ln |T | n\n(63)\nInequality 52 uses Jensen’s inequality, and equation 56 uses the independence property of expectation. To obtain 57, we first notice that sgm() ∈ (0, 1), thus tw(xi) ∈ (0,Wuj) and σitw(xi) ∈ (−Wuj,Wuj), and then use Hoeffding’s Inequality. Inequality 59 uses our assumption that ‖W‖max ≤ W. By taking derivative of the RHS of 61 and set it to 0, we obtained\ns =\n√\n2 ln |T | nW hence we obtain equation 62. By dividing both sides by n we obtain equation 63.\nCorollary 1. Let X = {x|x ∈ {0, 1}k},S = {x(1) . . .x(n)} be a data set of n samples. Given a restricted Boltzmann machine with k visible units and m hidden ones, and it is trained by using\nPage 9 of 11\nCD-1 algorithm. For all the parameters θ = {W}, and W ∈ Rk×m, assuming W is bounded by spheres ‖W‖max = ∀j ‖W·j‖1 ≤ W, where W·j is the j-th column of W. Let T be a compositional function of x with parameters W, i.e.,\nT = {Wijsgm ( m∑\nv=1\nWivsgm(x TW·v)\n)\n|W ∈ Rk×m ∀j ∈ {1, . . . ,m}. (64)\nWe further assume the VC-dimension of T is V C(T ). We can bound the empirical Rademacher complexity for the likelihood of this restricted Boltzmann machine as:\nR̂s(ln pθ) ≤ W√ n\n( m √ 2 ln k + k √ 2V C(T ) ln(n+ 1) ) . (65)\nProof. Using the result of Remark 1, we can bound R̂s(logZθ) in equation 44 by R̂s(logZθ) ≤ R̂s( k∑\ni=1 Ti). Similar to equation 42, R̂s(\nk∑ i=1 Ti) ≤ k∑ i=1 R̂s(Ti). Knowing the fact that each Ti is from\nthe same hypothesis space further provides us with\nk∑\ni=1\nR̂s(Ti) = kR̂s(T ). (66)\nUsing the results in Lemma 4 we obtain R̂s(logZθ) ≤ Wk\n√\n2n ln |T | n . Then by Sauer-Shelah\nlemma, we know\nmax S\n|T (S)| ≤ (n+ 1)V C(T ). (67)\nTherefore we obtain\nR̂s(logZθ) ≤ Wk\n√\n2n ln |T | n\n≤ Wk √\n2V C(T )n ln(n+ 1) n\n(68)\nTogether with the results from Theorem 1, while ignoring the bias term, we proved this corollary."
    }, {
      "heading" : "5 Future Direction",
      "text" : "Can we get a tighter bound on it? Can we extend this results to multi-layer Boltzmann machines, like deep belief networks (DBN) or deep Boltzmann machines (DBM)? Is that possible to obtain the exact expression of the VC dimension of our constructed function T ?"
    } ],
    "references" : [ {
      "title" : "Learning deep architectures for ai",
      "author" : [ "Yoshua Bengio" ],
      "venue" : "Found. Trends Mach. Learn.,",
      "citeRegEx" : "Bengio.,? \\Q2009\\E",
      "shortCiteRegEx" : "Bengio.",
      "year" : 2009
    }, {
      "title" : "A better way to pretrain deep Boltzmann machines",
      "author" : [ "Geoffrey Hinton", "Ruslan Salakhutdinov" ],
      "venue" : "Advances in Neural Information,",
      "citeRegEx" : "Hinton and Salakhutdinov.,? \\Q2012\\E",
      "shortCiteRegEx" : "Hinton and Salakhutdinov.",
      "year" : 2012
    }, {
      "title" : "Reducing the dimensionality of data with neural networks",
      "author" : [ "Geoffrey E. Hinton", "R R Salakhutdinov" ],
      "venue" : "Science (New York, N.Y.),",
      "citeRegEx" : "Hinton and Salakhutdinov.,? \\Q2006\\E",
      "shortCiteRegEx" : "Hinton and Salakhutdinov.",
      "year" : 2006
    }, {
      "title" : "A fast learning algorithm for deep belief nets",
      "author" : [ "Geoffrey E. Hinton", "Simon Osindero", "Yee Whye Teh" ],
      "venue" : "Neural computation,",
      "citeRegEx" : "Hinton et al\\.,? \\Q2006\\E",
      "shortCiteRegEx" : "Hinton et al\\.",
      "year" : 2006
    }, {
      "title" : "Lipschitz parametrization of probabilistic graphical models",
      "author" : [ "Jean Honorio" ],
      "venue" : "Uncertainty in Artificial Intelligence",
      "citeRegEx" : "Honorio.,? \\Q2012\\E",
      "shortCiteRegEx" : "Honorio.",
      "year" : 2012
    }, {
      "title" : "Deep Boltzmann Machines",
      "author" : [ "Ruslan Salakhutdinov", "Geoffrey Hinton" ],
      "venue" : "Artificial Intelligence,",
      "citeRegEx" : "Salakhutdinov and Hinton.,? \\Q2009\\E",
      "shortCiteRegEx" : "Salakhutdinov and Hinton.",
      "year" : 2009
    }, {
      "title" : "An Efficient Learning Procedure for Deep Boltzmann Machines",
      "author" : [ "Ruslan Salakhutdinov", "Geoffrey Hinton" ],
      "venue" : "Neural Computation,",
      "citeRegEx" : "Salakhutdinov and Hinton.,? \\Q2012\\E",
      "shortCiteRegEx" : "Salakhutdinov and Hinton.",
      "year" : 2012
    }, {
      "title" : "Information processing in dynamical systems: Foundations of harmony theory",
      "author" : [ "Paul Smolensky" ],
      "venue" : "Parallel Distributed Processing Explorations in the Microstructure of Cognition,",
      "citeRegEx" : "Smolensky.,? \\Q1986\\E",
      "shortCiteRegEx" : "Smolensky.",
      "year" : 1986
    } ],
    "referenceMentions" : [ {
      "referenceID" : 5,
      "context" : "Initially proposed by Smolensky [1986] for modeling cognitive process, it grew to prominence after successful application were found by Geoffrey Hinton and his collaborators [Hinton and Salakhutdinov, 2006, 2012; Salakhutdinov and Hinton, 2009].",
      "startOffset" : 174,
      "endOffset" : 244
    }, {
      "referenceID" : 3,
      "context" : "Also, it is proved that by adding another layer on top of a RBM, the variational lower bound of the data likelihood can be increased [Hinton et al., 2006; Salakhutdinov and Hinton, 2012], which conveys the theoretical advantage of building multilayer RBMs.",
      "startOffset" : 133,
      "endOffset" : 186
    }, {
      "referenceID" : 6,
      "context" : "Also, it is proved that by adding another layer on top of a RBM, the variational lower bound of the data likelihood can be increased [Hinton et al., 2006; Salakhutdinov and Hinton, 2012], which conveys the theoretical advantage of building multilayer RBMs.",
      "startOffset" : 133,
      "endOffset" : 186
    }, {
      "referenceID" : 2,
      "context" : "Initially proposed by Smolensky [1986] for modeling cognitive process, it grew to prominence after successful application were found by Geoffrey Hinton and his collaborators [Hinton and Salakhutdinov, 2006, 2012; Salakhutdinov and Hinton, 2009].",
      "startOffset" : 22,
      "endOffset" : 39
    }, {
      "referenceID" : 4,
      "context" : "Honorio [2012] also proved that discrete factor graphs, including Markov random fields, are Lipschitz continuous, which motivated this work to further investigate the properties of RBM.",
      "startOffset" : 0,
      "endOffset" : 15
    }, {
      "referenceID" : 0,
      "context" : "But experiments have shown that, even one step (CD-1) can yield a good performance for the model [Bengio, 2009].",
      "startOffset" : 97,
      "endOffset" : 111
    } ],
    "year" : 2015,
    "abstractText" : "Boltzmann machine, as a fundamental construction block of deep belief network and deep Boltzmann machines, is widely used in deep learning community and great success has been achieved. However, theoretical understanding of many aspects of it is still far from clear. In this paper, we studied the Rademacher complexity of both the asymptotic restricted Boltzmann machine and the practical implementation with single-step contrastive divergence (CD-1) procedure. Our results disclose the fact that practical implementation training procedure indeed increased the Rademacher complexity of restricted Boltzmann machines. A further research direction might be the investigation of the VC dimension of a compositional function used in the CD-1 procedure.",
    "creator" : "LaTeX with hyperref package"
  }
}